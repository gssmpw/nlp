\subsection{Experiment Setup}
We conduct experiments across three distinct domains: card flip, bookshelf, and kitchen shown in Figure~\ref{fig:CPNP_tasks}. We compare against baselines in simulations of these environments and then show real-world results. Table~\ref{tab:skill_defn} shows the definitions of non-prehensile skills used in each domain. We include the definition of the prehensile skill in Appendix~\ref{Appendix:P_Skill}. 
\begin{table*}[ht]
\centering
\begin{tabular}{|c|c|cc|cc|}
\hline
\textbf{\begin{tabular}[c]{@{}c@{}}Domain\\ names\end{tabular}} & \textbf{Card flip}                                                                                    & \multicolumn{2}{c|}{\textbf{Bookshelf}}                                                                                                                                                                                                                                  & \multicolumn{2}{c|}{\textbf{Kitchen}}                                                                                                                                                                                                                                                                           \\ \hline
\textbf{NP skills}                                                 & $K_{\text{slide}}$                                                                                    & \multicolumn{1}{c|}{$K_{\text{topple}}$}                                                                                                      & $K_{\text{push}}$                                                                                                        & \multicolumn{1}{c|}{$K_{\text{sink}}$}                                                                                                     & $K_{\text{cupboard}}$                                                                                                                                              \\ \hline
$\phi(q,q')$                                                    & \begin{tabular}[c]{@{}c@{}}$q,q' \in \Qobj$, \\ $R_x(q) = R_x(q')$,\\ $R_y(q) = R_y(q')$\end{tabular} & \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}$q,q' \in \Qobj^{\text{(upper-shelf)}}$, \\ $R_x(q) = R_x(q')$,\\ $R_z(q) = R_z(q')$\end{tabular}} & \begin{tabular}[c]{@{}c@{}}$q,q' \in \Qobj^{\text{(lower-shelf)}}$, \\ $R_x(q) = R_x(q')$,\\ $R_y(q) = R_y(q')$\end{tabular} & \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}$q,q' \in \Qobj^{\text{(sink)}}$, \end{tabular}} & \begin{tabular}[c]{@{}c@{}}$q,q' \in \Qobj^{\text{(l-cupboard)}} \text{ or } \Qobj^{\text{(r-cupboard)}}$,\\ $R_x(q) = R_x(q')$,\\ $R_y(q) = R_y(q')$\end{tabular} \\ \hline
$\pi$                                                           & $\pi_{\text{slide}}$                                                                                  & \multicolumn{1}{c|}{$\pi_{\text{topple}}$}                                                                                                    & $\pi_{\text{push}}$                                                                                                      & \multicolumn{1}{c|}{$\pi_{\text{sink}}$}                                                                                                   & $\pi_{\text{cupboard}}$                                                                                                                                            \\ \hline
\end{tabular}
\caption{Non-prehensile manipulation skills trained using RL for each domain. The row $\phi(q,q')$ denotes the applicability checker for each skill. We write $q = \qobj$ and $\phi(q,q')$ instead of $\phi(s,q')$ with abuse of notation for brevity and clarity. Here, $R_x$, $R_y$ and $R_z$ denote the rotation matrices of pose $q$ with respect to $x,y$, and $z$ axes. For the card flip domain, slide skill is applicable if the desired pose $q'$ involves only translation and rotation wrt the z-axis from $q$. For the bookshelf domain, we have two sub-regions $\Qobj^{\text{(upper-shelf)}}$ and $\Qobj^{\text{(lower-shelf)}}$,  as shown in Figure~\ref{fig:region} (middle). To apply the topple or push skill, $q$ and $q'$ must belong to the same sub-region. Toppling skill is applicable only for orientation wrt the y-axis. The pushing skill is applicable for orientation wrt the z-axis and any translation within the lower shelf. For the kitchen domain, we have three sub-regions: $\Qobj^{\text{(sink)}}$, $\Qobj^{\text{(l-cupboard)}}$, and $\Qobj^{\text{(r-cupboard)}}$. $K_{\text{sink}}$ is applicable for any orientation or translation, as long as the object stays within the sink. $K_{\text{cupboard}}$ is applicable for an orientation wrt z-axis and any translation, as long as the object moves within a left or right cupboard. The last row, $\pi$, indicates different policies trained for different skills. The details of training are in Appendix~\ref{Appendix:NP_Skill}.}\label{tab:skill_defn}
\end{table*} 
\begin{figure*}[h]
\centering
\resizebox{\textwidth}{!}{
    \includegraphics{figures/region.png}
}
\caption{Regions for each domain. The world frame is shown in the bottom left of each figure, where the red line represents the $x$-axis, the green line represents the $y$-axis, and the blue line represents the $z$-axis.}\label{fig:region}
% \vspace{-1mm}
\end{figure*}

%In the card flip domain, the robot has to slide the card, flip it using pick-and-place, and slide it again to reach the goal. In the bookshelf domain, the robot must move a book from an upper shelf to the goal pose on a lower shelf. The book is initially surrounded by other books, so the robot needs to topple them to enable a grasp. Additionally, the lower shelf has a tight space, requiring the robot to push the book into place rather than directly placing it. In the kitchen domain, the robot's task is to move a cup, initially in an ungraspable pose inside the sink, to either the left or right cupboard.

%The object pose space $\Qobj$ for each domain is outlined in Table~\ref{table:exp_setup}. In the card flip domain, there are two regions: $R^\text{up}$ and $R^\text{down}$, which are stable card poses on a table with the card facing upward and downward respectively. In the bookshelf domain, there are two regions: \(R^\text{uppershelf}\), are poses of the book at the upper shelf where it is in between other books, and \(R^\text{lowershelf}\), which are stable poses on the lower shelf. In the kitchen domain, there are three regions: \( R^\text{sink} \), \( R^\text{r-cupboard} \), and \( R^\text{l-cupboard} \), which represent stable poses in the sink, right cupboard, and left cupboard, respectively. The detailed region descriptions are outlined in the Appendix \ref{Appendix:region}.



% The set of skills $\mathcal{K}$ for each domain is shown in Table~\ref{table:exp_setup}.

\iffalse
, as outlined in Definition~\ref{Def:Skill}. The skill consists of two components: (1) a goal-conditioned policy \( \pi \) and (2) an applicability checker \( \phi \). Every goal-conditioned policy \( \pi \) consists of two different policies: (1) a \textit{pre-contact} policy \( \pi_\text{pre} \), and (2) a \textit{post-contact} policy \( \pi_\text{post} \). Additionally, we define an applicability checker function of each skill in each domain in Definition~\ref{Def:phi_applicability}. The details of each policy, the applicability checker functions, and the space of stable object poses $\Qobj$ are described in Appendix~\ref{Appendix:region} and~\ref{Appendix:Skill_each_domain}.

% \noindent\hrulefill

% \noindent \textbf{Non-Prehensile (NP) Skill Definition} $K_{\text{NP}}$

% \begin{itemize}\label{Def:NP}
%     \item \textbf{Goal-conditioned Policy} \(\pi\)

%     \begin{itemize}

%         \item \(\pi = 
%             \begin{cases} 
%             \text{pre-contact policy, } \pi_\text{pre}(s.q_\text{obj}, q'_\text{obj}): Q_\text{obj} \times Q_\text{obj} \rightarrow Q_r\\
%             \text{post-contact policy, } \pi_\text{post}(s, q'_\text{obj}): S \times Q_\text{obj} \rightarrow A \\
%             \text{connector policy, } \pi_C(s, q'_r): S \times Q_r \rightarrow A

%             \end{cases}\)
%     \end{itemize}

%     \item \textbf{Region $R \subsetneq Q_\text{obj}$}
    
%     \item \textbf{Applicability checker} $\phi: Q_\text{obj} \times Q_\text{obj} \rightarrow \{0, 1\}$

%     \begin{itemize}
%         \item[] \underline{\textbf{Card Domain}}
%         \begin{itemize}
%             \item \(\phi_\text{slide}(q_\text{obj}, q'_\text{obj}) = 
%             \begin{cases} 
%             1, & \text{if } q'_\text{obj} \in R, \, q_\text{obj} \in R, \text{ and } \pi_\text{pre}(s.q_\text{obj}, q'_\text{obj}) \text{ is collision-free}, \\ 
%             0, & \text{otherwise.} 
%             \end{cases}\)
%         \end{itemize}
    

%         \item[] \underline{\textbf{Bookshelf Domain}}
%             \begin{itemize}
%                 \item \(\phi_\text{topple}(q_\text{obj}, q'_\text{obj}) = 
%                 \begin{cases} 
%                 1, & \text{if } q'_\text{obj} \in R, \, q_\text{obj} \in R, \text{ and } \pi_\text{pre}(s.q_\text{obj}, q'_\text{obj}) \text{ is collision-free}, \\ 
%                 0, & \text{otherwise.} 
%                 \end{cases}\)
%                 \item \(\phi_\text{push}(q_\text{obj}, q'_\text{obj}) = 
%                 \begin{cases} 
%                 1, & \text{if } q'_\text{obj} \in R, \, q_\text{obj} \in R, \text{ and } \pi_\text{pre}(s.q_\text{obj}, q'_\text{obj}) \text{ is collision-free}, \\ 
%                 0, & \text{otherwise.} 
%                 \end{cases}\)
%             \end{itemize}

%         \item[] \underline{\textbf{Kitchen Domain}}
%             \begin{itemize}
%                 \item \(\phi_\text{sink}(q_\text{obj}, q'_\text{obj}) = 
%                 \begin{cases} 
%                 1, & \text{if } q'_\text{obj} \in R, \, q_\text{obj} \in R, \text{ and } \pi_\text{pre}(s.q_\text{obj}, q'_\text{obj}) \text{ is collision-free}, \\ 
%                 0, & \text{otherwise.} 
%                 \end{cases}\)
%                 \item \(\phi_\text{l-cupboard}(q_\text{obj}, q'_\text{obj}) = 
%                 \begin{cases} 
%                 1, & \text{if } q'_\text{obj} \in R, \, q_\text{obj} \in R, \text{ and } \pi_\text{pre}(s.q_\text{obj}, q'_\text{obj}) \text{ is collision-free}, \\ 
%                 0, & \text{otherwise.} 
%                 \end{cases}\)
%                 \item \(\phi_\text{r-cupboard}(q_\text{obj}, q'_\text{obj}) = 
%                 \begin{cases} 
%                 1, & \text{if } q'_\text{obj} \in R, \, q_\text{obj} \in R, \text{ and } \pi_\text{pre}(s.q_\text{obj}, q'_\text{obj}) \text{ is collision-free}, \\ 
%                 0, & \text{otherwise.} 
%                 \end{cases}\)
%             \end{itemize}

%     \end{itemize}

% \end{itemize}

% \noindent\hrulefill


% \noindent\hrulefill

% \noindent \textbf{Prehensile (P) Skill Definition} $K_{\text{P}}$

% \begin{itemize}\label{Def:P}
%     \item \textbf{Goal-conditioned Policy $\pi$}
%     \begin{itemize}

%         \item \(\pi = 
%             \begin{cases} 
%             \text{pre-contact policy, } \pi_\text{pre}(s.q_\text{obj}, q'_\text{obj}): Q_\text{obj} \times Q_\text{obj} \rightarrow Q_r\\
%             \text{post-contact policy, } \pi_\text{post}(s, q'_\text{obj}): S \times Q_\text{obj} \rightarrow A \\
%             \text{connector policy, } \pi_C(s, q'_r): S \times Q_r \rightarrow A

%             \end{cases}\)
%     \end{itemize}

%     \item \textbf{Region $R = Q_\text{obj}$}

%     \item \textbf{Applicability checker} $\phi: S \times Q_\text{obj} \rightarrow \{0, 1\}$
%     \begin{itemize}
%         \item \(\phi(s, q'_\text{obj}) = 
%         \begin{cases} 
%         1, & \text{if } \pi_\text{pre}(s.q_\text{obj}, q'_\text{obj}) \text{ is collision-free}, \\ 
%         0, & \text{otherwise.} 
%         \end{cases}\)
%     \end{itemize}

% \end{itemize}
% \noindent\hrulefill

\begin{definition}[\textbf{Skill Definition}] \label{Def:Skill}
\noindent\hrulefill
\normalfont
% \noindent \textbf{Skill Definition} $K$

\begin{itemize}\label{Def:NP}
    \item \textbf{$K$'s goal-conditioned policy} \(K.\pi\)

    \begin{itemize}
        \item \(\pi  
            \begin{cases} 
            \text{pre-contact policy,} \\ 
            \quad \pi_\text{pre}(q_\text{obj}, q'_\text{obj}): Q_\text{obj} \times Q_\text{obj} \rightarrow Q_r, \\[5pt]
            
            \text{post-contact policy,} \\ 
            \quad \pi_\text{post}(s, q'_\text{obj}): S \times Q_\text{obj} \rightarrow A, \\[5pt]
    
            % \text{connector policy,} \\ 
            % \quad \pi_C(s, q'_r): S \times Q_r \rightarrow A.
            \end{cases}\)
    \end{itemize}

    % \item \textbf{Region $R \subsetneq Q_\text{obj}$}
    
    \item \textbf{$K$'s applicability checker} $K.\phi: Q_\text{obj} \times Q_\text{obj} \rightarrow \{0, 1\}$
\end{itemize}
\end{definition}

\noindent\hrulefill



% \noindent\hrulefill
\begin{definition}[\textbf{Applicability checker of each skill}] \label{Def:phi_applicability}
\noindent\hrulefill
\normalfont
% \noindent \textbf{Skill Definition} $K$

    % \item \textbf{Region $R \subsetneq Q_\text{obj}$}
    
    % \item \textbf{Applicability checker} $\phi: Q_\text{obj} \times Q_\text{obj} \rightarrow \{0, 1\}$
    
    % \noindent\hrulefill

    % \noindent\hrulefill
    % The applicability checker function for each skill in each domain.
    \begin{itemize}
        \item[]
        \item \underline{\textbf{Card Flip Domain}} $\mathcal{K}=\{K_{\text{NP}_\text{slide}}, K_\text{P}\}$
            \begin{itemize}
                \item \(K_{\text{NP}_\text{slide}}.\phi(q_\text{obj}, q'_\text{obj})\) \\
                \(\begin{cases} 
                1, & \text{if } q_\text{obj}, q'_\text{obj} \in \Qobj, R_x(\qobj) =R_x(q'_\text{obj}), \\ 
                   & \quad \text{and } R_y(\qobj) = R_y(q'_\text{obj}), \\ 
                0, & \text{otherwise.} 
                \end{cases}\)
                \item \(K_\text{P}.\phi(\qobj, q'_\text{obj}) \\
                \begin{cases} 
                1, & \text{if } q_\text{obj}, q'_\text{obj} \in \Qobj, \\
                &\quad \pi_\text{pre}(q_\text{obj}, q'_\text{obj}) \text{ is collision-free} \\
                & \quad \text{and not null} \\ 
                0, & \text{otherwise.} 
                \end{cases}\)
            \end{itemize}
        where \( R_x \) and \( R_y \) are notations for the \( x \) and \( y \) axes in the global frame.

        \item \underline{\textbf{Bookshelf Domain}} $\mathcal{K}=\{K_{\text{NP}_\text{topple}},K_{\text{NP}_\text{push}},K_{\text{P}}\}$
            \begin{itemize}
                \item \(K_{\text{NP}_\text{topple}}.\phi(q_\text{obj}, q'_\text{obj}) \\ 
                \begin{cases} 
                1, & \text{if } q_\text{obj}, q'_\text{obj} \in \Qobj^\text{upper-shelf}, R_x(\qobj) = R_x(q'_\text{obj}), \\ 
                & \quad \text{and } R_z(\qobj) = R_z(q'_\text{obj}), \\ 
                0, & \text{otherwise.} 
                \end{cases}\)
                \item \(K_{\text{NP}_\text{push}}.\phi(q_\text{obj}, q'_\text{obj}) \\
                \begin{cases} 
                1, & \text{if } q_\text{obj}, q'_\text{obj} \in \Qobj^\text{lower-shelf}, R_x(\qobj) = R_x(q'_\text{obj}), \\ 
                & \quad \text{and } R_y(\qobj) = R_y(q'_\text{obj}), \\ 
                0, & \text{otherwise.} 
                \end{cases}\)

                \item \(K_{\text{P}}.\phi(\qobj, q'_\text{obj}) \\
                \begin{cases} 
                1, & \text{if } q_\text{obj}, q'_\text{obj} \in \Qobj, \\
                &\quad \pi_\text{pre}(q_\text{obj}, q'_\text{obj}) \text{ is collision-free} \\
                & \quad \text{and not null} \\ 
                0, & \text{otherwise.} 
                \end{cases}\)
            \end{itemize}
        where \( R_z \) is a notation for the \( z \) axis in the global frame. $Q_{\text{obj}}^{\text{upper-shelf}} \subset \Qobj$ and $Q_{\text{obj}}^{\text{lower-shelf}} \subset \Qobj$ are subspaces of $\Qobj$. $Q_{\text{obj}}^{\text{upper-shelf}}$ is a space at the upper shelf, between other books where the target book is upright. $\Qobj^\text{lower-shelf}$ is a space at the lower shelf, where the book is on the lower shelf.

        \item \underline{\textbf{Kitchen Domain}} \\ $\mathcal{K}=\{K_{\text{NP}_\text{sink}},K_{\text{NP}_\text{l-cupboard}},K_{\text{NP}_\text{r-cupboard}},K_{\text{P}}\}$
            \begin{itemize}
                \item \(K_{\text{NP}_\text{sink}}.\phi(q_\text{obj}, q'_\text{obj})  \\
                \begin{cases} 
                1, & \text{if } q_\text{obj}, q'_\text{obj} \in \Qobj^\text{sink} \\ 
                0, & \text{otherwise.} 
                \end{cases}\)
                \item \(K_{\text{NP}_\text{l-cupboard}}.\phi(q_\text{obj}, q'_\text{obj}) \\
                \begin{cases} 
                1, & \text{if } q_\text{obj}, q'_\text{obj} \in \Qobj^\text{l-cupboard}, R_x(\qobj) = R_x(q'_\text{obj}), \\ 
                & \quad \text{and } R_y(\qobj) = R_y(q'_\text{obj}), \\ 
                0, & \text{otherwise.} 
                \end{cases}\)
                \item \(K_{\text{NP}_\text{r-cupboard}}.\phi(q_\text{obj}, q'_\text{obj}) \\
                \begin{cases} 
                1, & \text{if } q_\text{obj}, q'_\text{obj} \in \Qobj^\text{r-cupboard}, R_x(\qobj) = R_x(q'_\text{obj}), \\ 
                & \quad \text{and } R_y(\qobj) = R_y(q'_\text{obj}), \\ 
                0, & \text{otherwise.} 
                \end{cases}\)
                \item \(K_{\text{P}}.\phi(\qobj, q'_\text{obj}) \\
                \begin{cases} 
                1, & \text{if } q_\text{obj}, q'_\text{obj} \in \Qobj, \\
                &\quad \pi_\text{pre}(q_\text{obj}, q'_\text{obj}) \text{ is collision-free} \\
                & \quad \text{and not null} \\ 
                0, & \text{otherwise.} 
                \end{cases}\)
            \end{itemize}
        where $\Qobj^\text{sink} \subset \Qobj, \Qobj^\text{l-cupboard} \subset \Qobj,$ and $\Qobj^\text{r-cupboard} \subset \Qobj$ are the subspaces of $\Qobj$. $\Qobj^\text{sink}$ is the space that belongs to the sink, $\Qobj^\text{l-cupboard}$ is space that belongs to the left side of the cupboard, and $\Qobj^\text{r-cupboard}$ is the space that belongs the right side of the cupboard.
    \end{itemize}

\noindent\hrulefill
\end{definition}
\fi

%the skill library includes \(K_\text{P}\), which moves the card across regions (\(R^\text{up}\) and \(R^\text{down}\)), as well as two non-prehensile (NP) skills: \(K_{\text{NP}_\text{up}}\) and \(K_{\text{NP}_\text{down}}\), which manipulate the card within \(R^\text{up}\) and \(R^\text{down}\), respectively. In the bookshelf domain, the library consists of \(K_\text{P}\), which moves a book between the upper and lower shelves, \(K_{\text{NP}_\text{topple}}\), which topples a book on the upper shelf, and \(K_{\text{NP}_\text{push}}\), which pushes a book inward on the lower shelf. In the kitchen domain, the skill library includes \(K_\text{P}\), which moves a cup from the sink to the shelf, as well as \(K_{\text{NP}_\text{sink}}\), \(K_{\text{NP}_\text{leftshelf}}\), and \(K_{\text{NP}_\text{rightshelf}}\), which manipulate the cup within the sink, left shelf, and right shelf regions, respectively. The detailed skill descriptions and the training methods for the skills are outlined in Appendix.% ~\ref{Appendix:P_Skill} and~\ref{Appendix:NP_Skill}.


\subsection{Baselines}
We compare our method with the following baselines:

\begin{itemize}
    \item PPO \cite{schulman2017proximal}: An end-to-end RL policy that outputs actions directly, without utilizing a set of skills $\mathcal{K}$. The MDP definition and training hyperparameters are described in the Appendix~\ref{Appendix:baseline_PPO}.

    \item \texttt{MAPLE} \cite{nasiriany2022augmenting}: A state-of-the-art RL method for PAMDPs. The skills are used as parameterized actions. We use sparse rewards where we reward the robot when we achieve a goal, or when the high-level policy selects a feasible skill. To adapt the method to the PNP problem, we introduce several modifications. The details of these modifications, including the reward functions and hyperparameter configurations, are provided in Appendix~\ref{Appendix:baseline_MAPLE}.
    % Modifications to adapt to the PNP problem, the reward function and hyperparameters are provided in Appendix \ref{Appendix:baseline_MAPLE}.
        
    \item \texttt{Skill-RRT} (low-level action): Pure planner that executes a sequence of actions given by \texttt{Skill-RRT} in an open-loop manner.
    
    \item \texttt{Skill-RRT} (skill plan): Pure planner that executes the sequence of skill policies and their associate object poses as given by \texttt{Skill-RRT}. It executes the next skill policy in the plan if the current skill succeeds. It is semi-open-loop in that skills execute closed-loop policies but we do not change the high-level skill plan based on states.

    \item Ours: Diffusion policy trained with data from \skillrrt. Filtered data with replay success rate threshold $m=0.9$. The hyperparameters for diffusion policy is described in Appendix \ref{Appendix:imitation_learning}.
\end{itemize}
The summary of these baselines are shown in the first two columns of Table~\ref{table:main_exp}. 
% To adapt MAPLE to our PNP problem, we apply several modifications. First, we omit the affordance reward, which in MAPLE guides the high-level policy toward the desired manipulation region. Instead, we integrate the connector and skill, using a applicability checker $\applicabilitychecker$. The connector activates only feasible skills, thereby eliminating the need for an affordance reward. Additionally, we remove the explicit target location parameter, $x_\text{reach}$ used in MAPLE, computing it instead through the pre-contact policy $\pi_\text{pre}$. We also remove atomic primitives, low-level actions used to fill in gaps that cannot be fulfilled by skills, since our connectors are already trained to handle these gaps. Detailed description for training MAPLE can be found in Appendix~\ref{Appendix:baseline}.
% To adapt \texttt{MAPLE} to the PNP problem, we introduce several modifications. First, we remove the affordance reward, which in \texttt{MAPLE} guides the high-level policy toward the desired manipulation region. Instead, we integrate the connectors and skills, incorporating an applicability checker \(\applicabilitychecker\). The connector, which moves the robot to a state where the corresponding skill is applicable, is executed only when the predicted desired object pose is applicable (i.e., when the corresponding \(\applicabilitychecker\) holds true); otherwise, it is not executed. This replaces the need for an affordance reward.
% Furthermore, we replace the explicit initial end-effector position parameter, \(x_{\text{reach}}\), in \texttt{MAPLE} with the output of the pre-contact policy \(\pi_\text{pre}\). \(x_{\text{reach}}\) serves as an input to skills, specifying the initial end-effector position for skill execution. Instead, this position is now computed by the pre-contact policy, and the connector moves to the computed position before skill execution. 
% Additionally, we eliminate atomic primitives, low-level actions used to fill in gaps that cannot be fulfilled by skills, since our connectors are already trained to handle these gaps. A detailed description for training \texttt{MAPLE} can be found in Appendix~\ref{Appendix:baseline_MAPLE}.
For the PPO baseline, we use a total of 2.5B state-action pairs across all domains. For training the high-level policy in \texttt{MAPLE}, we use 0.27B, 0.33B, and 0.27B state-action pairs for the card flip, bookshelf, and kitchen domains, respectively. For our method, we collect 500 skill plans with a replay success rate threshold \( m = 0.9 \) in each domain, resulting in 0.0029B, 0.0028B, and 0.0032B state-action pairs for training our IL policy in card flip, bookshelf, and kitchen domain respectively.

% maple, ppo # of state-action pair
% ppo totally 2.5B state-action pair.
% MAPLE card flip 0.27B (low-level) 0.29M (high-level)
% MAPLE bookshelf 0.33B (low-level) 0.72M (high-level)
% MAPLE kitchen 0.27B (low-level) 0.29M (high-level)

%For \texttt{Skill-RRT} (low-level action), given an initial state and goal, we first store the low-level actions from \texttt{Skill-RRT}. Then, we re-execute the stored low-level actions starting from the same initial state and evaluate whether the object reaches the goal.  For \skillrrt (skill action), each skill in the skill plan is executed sequentially, moving to the next skill if the current skill satisfies its success condition.

% For diffusion policy distillation, we collect 500 skill plans with a replay success rate threshold \( m = 0.9 \), filtered from 13,900, 2900, and 4400 skill plan samples for the card flip, bookshelf, and kitchen domains, respectively. From the 500 skill plans over replay success rate 0.9, 2.9M, 2.8M, and 3.2M state-action pairs are used to train our IL policy for the card flip, bookshelf, and kitchen domains.

%For diffusion policy distillation, we collect 500 skill plans with a replay success rate threshold \( m = 0.9 \). This is after filtering out 13,400, 2,400, and 3,900 skill plans for the Card Flip, Bookshelf, and Kitchen domains, respectively, which failed to satisfy the replay success rate threshold $m=0.9$. 



% We evaluate the baselines using two metrics: (1) success rate, which is the number of successfully executed trajectories divided by the total number of problems solved, (2) Runtime, which measures the total time the algorithm takes to solve the problem. For planning-based algorithms, runtime includes both the planning and execution phases, while for other methods, it primarily corresponds to the execution phase.



\subsection{Result and Analysis}

In this paper, we make the following claims.

\begin{itemize}
    \item Claim 1: For long-horizon PNP problems with sparse rewards, leveraging a planner as opposed to pure RL achieves better success rates.
    % compare open-loop versus closed-loop
    \item Claim 2: Distilling a planner to a policy achieves better computation time and success rate than pure planning.
    %\item Claim 3: Combining planning (\texttt{Skill-RRT}) with imitation learning enhances execution efficiency and robustness by refining plans based on the current state, enabling faster and more successful skill execution.
    % split
\end{itemize}
To validate these claims, we evaluate the baselines using two metrics: (1) success rate, which is the number of successfully executed trajectories divided by the number of problem-solving attempts, and (2) computation time, which measures the total elapsed time for computing the entire sequence of actions. For \texttt{Skill-RRT}, the computation time includes both planning and inference times, as it involves using skill policies in addition to tree search. The planning and inference times of \texttt{Skill-RRT} are described in Appendix I.

\input{tables/baselines_exp}

% To support Claim 1, we compare our framework with MAPLE \cite{nasiriany2022augmenting}. As shown in Table \ref{table:main_exp}, MAPLE achieves a zero success rate in the card and kitchen domains. This outcome is caused by the difficulty of sampling a target object pose that is applicable for the next skill under sparse rewards. For example, in the card domain, the probability of MAPLE outputting the desired card pose at the end of the table through random actions is low, reducing the applicability of the P skill. In contrast, in the bookshelf domain, the P skill becomes applicable once the book is toppled, making skill chaining more easy.
% To support Claim 1, we compare our framework with MAPLE~\cite{nasiriany2022augmenting} and PPO. As shown in Table \ref{table:main_exp}, MAPLE achieves a success rate of zero in both the card flip and kitchen domains. This poor performance is mainly due to the challenge of predicting an object target pose that enables the successful execution of subsequent skills through random exploration. For instance, in the card domain, task success depends on releasing the gripper and moving it to the card without falling the card after completing the prehensile skill. However, the region where the subsequent non-prehensile skill can successfully execute is narrow, making it challenging for RL-based methods, which rely on random exploration, to find a viable solution without dense reward signals to guide the search.

\newcommand{\maple}{\texttt{MAPLE}}

Table~\ref{table:main_exp} shows the results.
To support Claim 1, we first compare our method with PPO~\cite{schulman2017proximal} and \texttt{MAPLE}~\cite{nasiriany2022augmenting}. PPO exhibits poor performance across all domains. Because this is a flat RL policy, it struggles with long-horizon problems that have sparse rewards, a perennial problem in RL. \maple{} also shows zero success rate in card flip and kitchen domains. While \maple{} is a PAMDP-based hierarchical RL method, it struggles in these domains due to the narrow passage problem~\cite{hsu1998finding}. For instance, in the card flip domain, the card can only be flipped near the edges of the table, a very small region in $\Qobj$. Additionally, even at the edge, to prevent the card from dropping during the transition from sliding to the prehensile skill, it must be positioned in a precise manner: it should be graspable yet remain sufficiently close to the table to avoid falling during the connection (See Appendix Figure~\ref{fig:card_y_position_histograms} for visual illustration).  Similarly, in the kitchen domain, the robot must position the cup such that the prehensile skill can be applied following the non-prehensile skill in the sink. But the cup poses for which you can apply prehensile skill is very narrow: it must be collision-free, and an inverse kinematics solution should exist for picking. For these reasons, most object subgoal pose exploration leads to failure for \maple{}, which ends up learning only locally optimal behaviors, such as bringing the card to the edge of the table but not executing the prehensile policy as it would often lead to dropping the card. For the bookshelf domain, where the robot simply has to topple the book to enable grasping, \maple{} shows 83\% success rate.


Pure planner, \skillrrt~(low-level action) achieves zero success rates across all domains, as its open-loop nature cannot respond to state transitions that are different from the plan. \skillrrt~(skill action) does better, because it reacts to unexpected transitions within skills. However, it tends to use risky desired object poses, such as sliding the card to the very edge, which, even with a slight difference in actions or state transitions, risks dropping the card.


In contrast to these approaches, our method achieves success rates above 93\% across all domains, thanks to the sophisticated exploration strategy in RRT combined with a data filtering mechanism that filters plans that involve risky object subgoals. Furthermore, because ours is a policy, we can achieve full closed-loop control, which can react to unexpected state transitions.




%In both domains, the range in which a successful transition between skills can occur is extremely narrow. In RL, if a skill with a specific desired object pose exhibits a high failure rate, the agent often learns to suppress its selection due to the reward-driven nature of policy optimization. This tendency can result in convergence to suboptimal policies and a fall into local minima, even though the skill with the desired object pose is essential for task completion, such as the sliding skill with the object pose near the edge. 


%. Specifically, to ensure task completion, the reward function would need to be designed in a way that sacrifices distance. As a result, PPO struggles to achieve high success rates in these domains.

%s presented in Table~\ref{table:main_exp}, 


% In contrast, our framework uses \skillrrt~to systematically explores the leading to higher success rates even in challenging scenarios where RL struggles due to issues with random exploration and credit assignment. Additionally, the bookshelf domain exhibits a broader region in which the subsequent skill could succeed. Specifically, the prehensile skill becomes feasible to execute if the topple skill is successful, enabling a higher success rate with the PAMDP method in this domain.

% In MAPLE, if a skill frequently fails, the agent often learns to avoid selecting that skill, leading to frequent cases of getting stuck in local minima, even though the skill is necessary for task success. In contrast, planning (\texttt{Skill-RRT}) continuously executes the failed skill along with a new desired object pose, even after failure. When successful, the new state is stored as a new node, and the search continues toward the goal. As a result, planning avoids local minima and maintains continuous exploration.
% bookshelf -> we can generate the high success rate plan 
% It is hard to sample target object pose that is feasible for the next skill with random exploration under sparse reward setting. (except bookshelf) 
% need shield -> bookshelf domain's characteristic
% 

% In RL, if a skill with a specific desired object pose exhibits a high failure rate, the agent often learns to suppress its selection due to the reward-driven nature of policy optimization. This tendency can result in convergence to suboptimal policies and a fall into local minima, even though the skill with the desired object pose is essential for task completion, such as the sliding skill with the object pose near the edge. 

%Exceptionally, the bookshelf domain allows for a wider range of successful skill transitions. Specifically, if the topple skill is performed successfully, the prehensile skill becomes applicable, leading to a higher success rate with the PAMDP method in this domain.

%To support claim 2, we compare our distillation policy with \texttt{Skill-RRT} (low-level actions). As shown in Table \ref{table:main_exp}, this approach achieves zero success rates across all domains, as its open-loop nature cannot respond to state transitions that are different from the plan. \skillrrt (skill action) does better, because it reacts to unexpected transitions within skills. However, it tends to use risky desired object poses, such as sliding the card to the very edge, which, even with a slight difference in actions or state transitions, risk dropping the card. In contrast, our method (\texttt{Skill-RRT} + IL) filters out skill plans with low replay success rates by replaying them multiple times and trains the distillation policy on trajectories generated from such robust skill plans. As a result, our method demonstrates higher success rates compared to simply executing skill plans.


% The poor performance is due to the contact-rich nature of our PNP tasks and simulation stochasticity of GPU-based simulation, which leads to increased dynamic uncertainty. Executing actions in an open-loop manner cannot actively respond to stochastic state transitions because it lacks state feedback, whereas our distillation policy can adapt its action to the current state immediately.

%To support Claim 3, we compare our method with \skillrrt (skill action). However, this method still fails to achieve high success rates. This is because there are risky target object poses where the skills do not succeed 100\%, even though these target poses succeed during the planning stage. For example, pushing a card to the very edge of a table might succeed during planning but is likely to result in the card falling off in execution. In contrast, our method (\texttt{Skill-RRT} + IL) filters out skill plans with low replay success rates by replaying them multiple times and trains the distillation policy on trajectories generated from such robust skill plans. As a result, our method demonstrates higher success rates compared to simply executing skill plans.

%Another benefit of distillation is that the distillation policy can solve tasks much faster than planning because it requires significantly less online computation time compared to planning. 

To validate our argument about computation time, we compare the total elapsed time for computing the sequence of actions between our method and \skillrrt{}. Note that only successful episodes are measured, as failure episodes, such as those involving object falling or timeout, result in extremely short or extremely long computation time. The results shown in Table \ref{table:main_exp} demonstrate that our distillation policy significantly reduces the computation time compared to a pure planner, \texttt{Skill-RRT}, as it eliminates expensive search procedure.

% Action Type: By executing the skill plan, if it fails, we cannot revise the skill with the current setting (as there is no function $\bets$ for termination the skill). Therefore, state-action imitation provides a better approach to address the problem.

% \textcolor{red}{result analysis, not yet}

\subsection{Real World Experiments}
We evaluate our distilled diffusion policy in real-world domains shown in Figure~\ref{fig:CPNP_tasks} by zero-shot transferring the policy. For each experiment, we use a set of 20 test problems consisting of different initial object poses, initial robot configurations, and goal object poses. The shapes of the real environments and objects are identical to those in the simulation. The policy achieves a success rate of over 80\% for all domains, as shown in Table \ref{table:real_world_result}. The main failure cases across domains include torque limit violations of the robot hardware caused by impacts between the robot and the object, and object dropping due to unstable object placement poses. Detailed setups for the real-world experiments are described in Appendix~\ref{Appendix:real_exp}.

\input{tables/real_world_result}

\subsection{Ablations of distillation policy architecture with its data filtering method} 

As described in Section~\ref{method:IL}, we train the distillation policy using the Diffusion Policy~\cite{chi2023diffusion} through imitation learning. For data collection, we use state-action pairs gathered by replaying skill plans whose success rate exceeds a threshold of \( m = 0.9 \). To understand the contribution of our choice of distillation policy architecture, IL algorithm, and and the data filtering method, we conduct ablation studies. The ablation study of the distillation policy architecture can be found in Appendix~\ref{Appendix:IL_ablation}, while the ablation study of the filtering method is presented in Appendix~\ref{Appendix:data_qual}.

