\begin{algorithm}[H]
\caption{State-Action Collection}\label{algo:Collect_data}
\begin{algorithmic}[1]
\State \textbf{Input:} Skill library ${\mathcal{O}} = \{o_{\text{NP}_1}, o_{\text{NP}_2}, \ldots, o_{\text{NP}_n}, o_{\text{Place}}\}$
\State \textbf{Initialize:} Counter $n \leftarrow 0$, dataset $\mathcal{D} \leftarrow \emptyset$
\While{$n < 500$}
    \State Randomly sample initial $q^\text{obj}_{\text{init}}$ and goal object pose $q^\text{obj}_{\text{goal}}$
    \State Generate skill plan $\tau_{\text{skill}} = \text{Skill-RRT}(q^\text{obj}_{\text{init}}, q^\text{obj}_{\text{goal}}, {\mathcal{O}})$
    \State Replay the skill plan $\tau_{\text{skill}}$ 400 times to obtain full trajectories $\tau_{\text{full}}^{(i)}$ for $i = 1, \ldots, 400$
    \State Compute replay success rate $r = \frac{1}{400} \sum \text{success}(\tau_{\text{full}}^{(i)})$
    \If{$r \geq 0.9$}
        \State \textcolor{red}{Select 30 successful trajectories randomly} from $\{\tau_{\text{full}}^{(i)} \mid \text{success}(\tau_{\text{full}}^{(i)}) = 1\}$
        \State $\mathcal{D} \leftarrow \mathcal{D} \cup \{\text{selected 30 successful trajectories}\}$
        \State $n \leftarrow n + 1$
    \EndIf
\EndWhile
\end{algorithmic}
\end{algorithm}
\begin{flushleft}
\footnotesize
\textbf{Algorithm \ref{algo:Collect_data} Explanation} â€” The algorithm collects a dataset of successful state-action trajectories. It begins by sampling random initial and goal states and generating a skill plan $\tau_{\text{skill}}$ using the Skill-RRT algorithm. This plan is executed 500 times to generate full trajectories, with the replay success rate $r$ computed as the ratio of successful trajectories. If $r$ is at least 0.9, 30 successful trajectories are randomly selected and added to the dataset $\mathcal{D}$. This process repeats until 1000 sets of successful trajectories are collected.
\end{flushleft}
