\section{Evaluation}


\begin{table}[t]
    \centering
    % \vspace{-0.1in}
    \scalebox{0.78}{
    % \begin{small}
        \begin{tabular}{lccc}
            \toprule
            \multirow{2}*{\textbf{MoE Models}} & \textbf{Parameters} & \textbf{Experts Per Layer} & \textbf{Num. of} \\
            & \textbf{(active / total)} & \textbf{(active / total)} & \textbf{Layers} \\
            \otoprule 
            \mixtral~\cite{jiang2024mixtral} & 12.9B / 46.7B & 2 / 8 & 32 \\
            % \hline
            \qwen~\cite{yang2024qwen2} & 2.7B / 14.3B & 4 / 60 & 24 \\
            \phimoe~\cite{abdin2024phi} & 6.6B / 42B & 2 / 16 & 32 \\
            \bottomrule 
        \end{tabular}
    % \end{small}
    }
    \caption{Characteristics of three \MoE models in evaluation.}
    \vspace{-0.2in}
    \label{table:eval-moe-models}
\end{table}








\subsection{Experimental Setup}
\label{subsec:eval-setup}


% \begin{figure*}[t]
%     \centering
%     \begin{subfigure}[t]{0.48\textwidth}
%         \centering
%         \includegraphics[width=.9\linewidth]{figs/eval-overall-lmsys.pdf}
%         \caption{Serving three \MoE models with LMSYS-Chat-1M dataset.}
%     \end{subfigure}
%     \begin{subfigure}[t]{0.48\textwidth}
%         \centering
%         \includegraphics[width=.9\linewidth]{figs/eval-overall-sharegpt.pdf}
%         \caption{Serving three \MoE models with ShareGPT dataset.}
%     \end{subfigure}
%     \caption{Overall performance of prefill and decode stages for \sys and other four baselines.}
%     \label{fig:eval-overall.pdf}
% \end{figure*}


\noindent \textbf{Testbed.}
We conduct all experiments on a six-GPU testbed, where each GPU is an NVIDIA GeForce RTX 3090 with 24 GB GPU memory. 
%
All GPUs are inter-connected using pairwise NVLinks and connected to the CPU memory using PCIe 4.0 with 32GB/s bandwidth. 
%
Additionally, the testbed has a total of 32 AMD Ryzen Threadripper PRO 3955WX CPU cores and 480 GB CPU memory.


\noindent \textbf{Models.}
We employ three popular \MoE-based \LLMs in our evaluation: \mixtral~\cite{jiang2024mixtral}, \qwen~\cite{yang2024qwen2}, and \phimoe~\cite{abdin2024phi}.
Table~\ref{table:eval-moe-models} describes the parameters, number of \MoE layers, and number of experts per layer for the three models.
Following the evaluation of existing works~\cite{song2024promoe}, we profile the models to set the optimal prefetch distance $d$ to three before evaluation.
% We set $d$ of \mixtral, \qwen, and \phimoe to \todo{$xxx$}, \todo{$xxx$}, and \todo{$xxx$}, respectively.


\noindent \textbf{Datasets and traces.}
We employ two real-world prompt datasets commonly used for \LLM evaluation: LMSYS-Chat-1M~\cite{zheng2023lmsys} and ShareGPT~\cite{sharegpt}.
%
For most experiments, we split the sampled datasets in a standard 7:3 ratio, where 70\% of the prompts' context data (\ie, semantic embeddings and expert maps) are stored in \sys's Expert Map Store, and 30\% of the prompts are used for testing. 
%
For online serving experiments, we empty the Expert Map Store and use real-world \LLM inference traces~\cite{patel2024splitwise,stojkovic2025dynamollm} released by Microsoft Azure to set input and generation lengths and drive invocations.

\noindent \textbf{Baselines.}
We compare \sys against four \SOTA \MoE serving baselines:
1) \textbf{MoE-Infinity}~\cite{xue2024moe} uses coarse-grained request-level expert activation patterns and synchronous expert prediction and prefetching for \MoE serving. 
We prepare the expert activation matrix collection for MoE-Infinity before evaluation for a fair comparison.
%
% However, the open-sourced MoE-Infinity codebase~\cite{moe-infinity-code} lacks some features described in its original paper, we had to modify
%y 
2) \textbf{ProMoE}~\cite{song2024promoe} employs a stride-based speculative expert prefetching approach for \MoE serving. Since the codebase of ProMoE is not open-sourced and requires training predictors for each \MoE model, we reproduced a prototype of ProMoE on top of MoE-Infinity in our best effort.
%
3) \textbf{Mixtral-Offloading}~\cite{eliseev2023fast} combines a layer-wise speculative expert prefetching and a \LRU-based expert cache. 
%
4) \textbf{DeepSpeend-Inference} employs an expert-agnostic layer-wise parameter offloading approach, which uses pure on-demand loading and does not support prefetching. 
%
We implement the offloading logic of DeepSpeed-Inference in the MoE-Infinity codebase and add an expert cache for a fair comparison.
We enable all baselines to serve \MoE models from HuggingFace Transformer~\cite{wolf2020huggingface}. 


\noindent \textbf{Metrics.}
Following the standard evaluation methodology of existing works~\cite{song2024promoe,xue2024moe,zhong2024distserve,agrawal2024taming} on \LLM serving, we report the performance of the prefill and decode stages separately. 
We measure Time-to-First-Token (TTFT) for the prefill stage and Time-Per-Output-Token (TPOT) for the decode stage.
Additionally, we also report other system metrics, such as expert hit rate and overheads, for detailed evaluation.


% \noindent \textbf{\sys's setting.}
% The hyperparameters of \sys containing the prefetch distance $d$ for each \MoE model, Expert Map Store capacity $C$, and Expert Cache memory limit $M$.
% For most experiments, we profile the \MoE models and set the prefetch distance $d$ to their optimal values. The Expert Map Store capacity $C$ is set to \todo{$xxx$} expert maps. We configure the Expert Cache memory limit to \todo{$xxx$} GB.
% The hyperparameter sensitivity is analyzed in \S\ref{subsec:eval-sensitivity}.


\begin{figure}[t]
  \centering
  \includegraphics[width=.95\linewidth]{figs/eval-overall-arxiv.pdf}
  \vspace{-0.15in}
  \caption{Overall performance of prefill and decode stages for \sys and other four baselines.}
  \vspace{-0.2in}
  \label{fig:eval-overall}
\end{figure}


\subsection{Overall Performance}
\label{subsec:eval-overall}



We first evaluate the performance of prefill and decode stages when running \sys and other baselines with the three \MoE models, where we measure Time-To-First-Token (TTFT) and Time-Per-Output-Token (TPOT) for each stage.
Note that the inference latency with expert offloading tends to be higher than no offloading due to two reasons: 
1) During inference, an excessive amount of parameters in \MoE models are loaded and offloaded, which prolongs the inference latency.
2) All baselines and \sys are implemented on top of the MoE-Infinity codebase~\cite{moe-infinity-code}, whose inference latency is inherently impacted by MoE-Infinity's implementation.
Nevertheless, comparing \sys and baselines is fair with the same experimental setup.

Figure~\ref{fig:eval-overall} shows the \TTFT, \TPOT, and expert hit rate of \sys and other four baselines when serving three \MoE models with LMSYS-Chat-1M and ShareGPT datasets, respectively.
DeepSpeed has both the worst \TTFT and \TPOT due to expert-agnostic offloading and lacking expert prefetching.
While Mixtral-Offloading, ProMoE, and MoE-Infinity perform better than DeepSpeed-Inference, they are underperformed by \sys because of coarse-grained offloading designs.
Compared to DeepSpeed-Inference, Mixtral-Offloading, ProMoE, and MoE-Infinity, our \sys reduces the average \TTFT by 44\%, 35\%, 33\%, 30\%, and reduces the average \TPOT by 70\%, 61\%, 55\%, 48\%, across three \MoE models.
%
% Figure~\ref{fig:eval-overall} also reports the expert hit rate of \sys and each baseline. 
For expert hit rate, Mixtral-Offloading achieves a higher hit rate than the other three baselines because of its synchronous speculative prefetching with a prefetch distance of 1. However, due to synchronous prefetching, its \TTFT and \TPOT are worse than others except DeepSpeed-Inference.
\sys improves the average expert hit rate by 147\%, 11\%, 34\%, and 63\% over DeepSpeed-Inference, Mixtral-Offloading, ProMoE, and MoE-Infinity, respectively.

% \begin{figure}[t]
%   \centering
%   \includegraphics[width=.9\linewidth]{figs/eval-overall-sharegpt.pdf}
%   % \vspace{-0.15in}
%   \caption{}
%   % \vspace{-0.25in}
%   \label{fig:eval-overall-sharegpt.pdf}
% \end{figure}




\subsection{Online Serving Performance}
\label{subsec:eval-online}


Except for the offline evaluation (\ie, Expert Map Store in full capacity before serving), we also evaluate \sys against other baselines in online serving settings.
We empty the Expert Map Store of \sys and the expert activation matrix collection of MoE-Infinity for the online serving experiment.
%
The request traces are derived from Azure \LLM inference traces~\cite{patel2024splitwise,stojkovic2025dynamollm}, with 64 requests randomly sampled to drive LMSYS-Chat-1M prompts for each \MoE model serving. 
To ensure consistency, \sys and all baselines input and generate the exact number of tokens specified in the traces.
%
Figure~\ref{fig:eval-online-serve} illustrates the CDF of end-to-end request latency across three \MoE models. The results demonstrate that \sys significantly reduces overall request latency compared to other baselines in online serving scenarios.


\begin{figure}[t]
  \centering
  \includegraphics[width=.95\linewidth]{figs/eval-online-serve-arxiv.pdf}
  \vspace{-0.15in}
  \caption{CDF of request latency for \MoE online serving.}
  \vspace{-0.2in}
  \label{fig:eval-online-serve}
\end{figure}



\subsection{Impact of Expert Cache Limits}



We measure the \TPOT of \sys and other baselines by limiting the expert cache memory budget to investigate their performance in the latency-memory trade-off (\S\ref{subsec:bg-latency-memory-tradeoff}).
We mainly focus on \TPOT to show the end-to-end performance impacted by varying cache limits.
Figure~\ref{fig:eval-cache-limit.pdf} shows the \TPOT of \sys and other four baselines when serving three \MoE models under different expert cache limits.
We gradually increase the GPU memory allocated for caching experts from 6 GB to 96 GB while employing the same experimental setting in \S\ref{subsec:eval-overall}.
Similarly, DeepSpeed-Inference has the worst \TPOT due to being expert-agnostic.
\sys consistently outperforms Mixtral-Offloading, ProMoE, and MoE-Infinity under varying expert cache limits.
Especially for limited GPU memory sizes (\eg, 6GB), \sys reduces the \TPOT by 32\%, 24\%, 18\%, and 18\%, compared to DeepSpeed-Inference, Mixtral-Offloading, ProMoE, and MoE-Infinity, across three \MoE models, respectively.
With fine-grained expert offloading, \sys significantly reduces the expert on-demand loading latency while maintaining a lower GPU memory footprint, therefore achieving a better spot in the latency-memory trade-off of \MoE serving.

% \subsection{Impact of Inference Batch Size}

\subsection{Ablation Study}
\label{subsec:eval-ablation}


% \begin{figure}[t]
%   \centering
%   \includegraphics[width=.95\linewidth]{figs/eval-expert-tracking.pdf}
%   % \vspace{-0.15in}
%   \caption{Expert hit rate of different expert pattern tracking approaches.}
%   % \vspace{-0.25in}
%   \label{fig:eval-expert-tracking}
% \end{figure}



We present the ablation study of \sys's design.


\textbf{Effectiveness of expert map search.}
One of \sys's key designs is the expert map, which tracks expert selection preferences in fine granularity.
We evaluate the effectiveness of the expert map against five expert pattern-tracking approaches as follows.
%
1) \textbf{Speculate}: speculative prediction used by Mixtral-Offloading~\cite{eliseev2023fast} and ProMoE~\cite{song2024promoe}, 
%
2) \textbf{Hit count}: request-level expert hit count used by MoE-Infinity~\cite{xue2024moe}, 
%
3) \textbf{Map (T)}: expert map with only trajectory similarity search,
4) \textbf{Map (T+S)}: expert map with both trajectory and semantic similarity search,
%
and
5) \textbf{Map (T+S+$\delta$)}: expert map with full features enabled, including trajectory and semantic similarity search (\S\ref{subsec:design-similarity-match}) and dynamic expert selection (\S\ref{subsec:design-expert-prefetch}).
%
We implement the above methods in \sys's Expert Map Matcher for a fair comparison.
Figure~\ref{fig:eval-expert-tracking} shows the expert hit rate of the above expert pattern tracking methods.
%
Speculative prediction is effective due to the widespread presence of residual connections in Transformer blocks. However, its effectiveness decreases drastically as prefetch distance increases~\cite{song2024promoe}.
%
The request-level expert activation count has the worst performance due to coarse granularity.
%
As features are incrementally restored to \sys's expert map, the expert hit rate gradually increases, demonstrating its effectiveness.

% \textbf{Effectiveness of asynchronous map matching.}




\begin{figure}[t]
  \centering
  \includegraphics[width=.9\linewidth]{figs/eval-cache-limit-arxiv.pdf}
  \vspace{-0.15in}
  \caption{Performance of \sys and other four baselines under varying expert cache limits.}
  \vspace{-0.1in}
  \label{fig:eval-cache-limit.pdf}
\end{figure}

\begin{figure}[!t]
    \centering
    \begin{subfigure}[t]{0.585\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figs/eval-expert-tracking.pdf}
        \caption{Expert pattern tracking approaches.}
        \label{fig:eval-expert-tracking}
    \end{subfigure}
    % \hspace{0.02in}
    \begin{subfigure}[t]{0.385\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figs/eval-prefetch-and-cache-arxiv.pdf}
        \caption{Prefetch and caching.}
        \label{fig:eval-prefetch-and-cache}
    \end{subfigure}
    \vspace{-0.1in}
    \caption{Ablation study of \sys.}
    \label{fig:eval-ablation}
    \vspace{-0.2in}
\end{figure}

\textbf{Effectiveness of expert prefetching and caching.}
We evaluate \sys's expert prefetching and caching against two caching algorithms:
1) \textbf{\LRU} used by Mixtral-Offloading~\cite{eliseev2023fast}
and 
2) \textbf{\LFU} used by MoE-Infinity~\cite{xue2024moe}.
%
Figure~\ref{fig:eval-prefetch-and-cache} depicts the expert hit rate of \sys and two baselines.
The results show that \LRU performs poorly in expert offloading scenarios. Though \LFU achieves a higher hit rate than \LRU, \sys surpasses both, achieving the highest expert hit rate.

\subsection{Sensitivity Analysis}
\label{subsec:eval-sensitivity}


\begin{figure}[t]
  \centering
  \includegraphics[width=.9\linewidth]{figs/eval-prefetch-distance.pdf}
  \vspace{-0.15in}
  \caption{Performance of \sys serving \MoE models with different prefetch distances.}
  \vspace{-0.1in}
  \label{fig:eval-prefetch-distance}
\end{figure}

% \begin{figure}[t]
%   \centering
%   \includegraphics[width=.9\linewidth]{figs/eval-store-capacity.pdf}
%   % \vspace{-0.15in}
%   \caption{Semantic and trajectory similarity lower bounds in \sys's serving with different Expert Map Store capacity.}
%   % \vspace{-0.25in}
%   \label{fig:eval-store-capacity}
% \end{figure}

\begin{figure}[t]
    \centering
    \begin{subfigure}[t]{0.55\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figs/eval-store-capacity.pdf}
        \caption{Expert Map Store capacity.}
        \label{fig:eval-store-capacity}
    \end{subfigure}
    % \hspace{0.02in}
    \begin{subfigure}[t]{0.435\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figs/eval-batch-size-arxiv.pdf}
        \caption{Inference batch size.}
        \label{fig:eval-batch-size}
    \end{subfigure}
    \vspace{-0.1in}
    \caption{Sensitivity analysis of \sys.}
    \vspace{-0.2in}
    \label{fig:eval-sensitivity}
\end{figure}


We analyze the sensitivity of three hyperparameters: prefetch distance of \MoE models, the capacity of Expert Map Store, and inference batch size.


\textbf{Prefetch distance of \MoE models.}
Figure~\ref{fig:eval-prefetch-distance} shows the \TTFT and \TPOT of \sys when serving three \MoE models with different prefetch distances.
%
We have demonstrated that the expert hit rate decreases when gradually increasing the prefetch distance (Figure~\ref{fig:bg-hit-distance}).
%
When the prefetch distance is small ($<3$), \sys cannot perfectly hide its system delay from the inference process, such as the map matching and expert prefetching, leading to the increase of inference latency.
%
With larger prefetch distances ($>3$), \sys has worse expert hit rates that also degrade the performance. 
Therefore, we set the prefetch distance $d$ to 3 for evaluating \sys.


\textbf{Capacity of Expert Map Store.}
We measure the mean semantic and trajectory similarity scores searched in \sys's expert map matching for \MoE model serving.
%
Figure~\ref{fig:eval-store-capacity} presents the mean semantic and trajectory similarity scores of \sys with different Expert Map Store capacity sizes.
%
Both semantic and trajectory similarity scores improve as the store capacity increases.
%
While the similarity scores exhibit a significant increase with capacities below 1K, further capacity expansion yields diminishing similarity gains. 
To minimize \sys's memory overhead, we set \sys's Expert Map Store capacity to 1K in evaluation.


\textbf{Inference batch size.}
We investigate the impact of inference batch size on \sys and three baselines using \mixtral with LMSYS-Chat-1M.
%
Figure~\ref{fig:eval-batch-size} presents the performance of \sys, Mixtral-Offloading, ProMoE, and MoE-Infinity as the batch size increases from one to four. \sys achieves the lowest \TTFT and \TPOT in most cases.


% \textbf{Inference batch size.}


% \subsection{Scalability}
% \label{subsec:eval-scalability}
% From one to six GPUs


\begin{figure}[t]
  \centering
  \includegraphics[width=.92\linewidth]{figs/eval-overhead-latency.pdf}
  \vspace{-0.15in}
  \caption{Latency breakdown of \sys's one inference iteration with three \MoE models.}
  \vspace{-0.1in}
  \label{fig:eval-overhead-latency.pdf}
\end{figure}





\subsection{System Overheads}
\label{subsec:eval-overhead}


\noindent \textbf{Latency overheads of \sys's operations.}
Figure~\ref{fig:eval-overhead-latency.pdf} shows the latency breakdown of one inference iteration in \sys when serving the three \MoE models.
We report any operations of \sys in \S\ref{subsec:eval-overall} that may incur a significant latency delay, including context collection, map matching, expert on-demand loading, expert prefetching, and map update after the iteration completes.
\qwen has lower end-to-end iteration latency than \mixtral and \phimoe because of significantly fewer parameters.
Note that expert prefetching, map matching, and map update tasks are executed asynchronously, aside from the inference process. Hence, they do not contribute to the end-to-end iteration latency.
Excluding three asynchronous tasks, the total delay incurred by other operations is consistently less than 30ms (5\% of the iteration) across three \MoE models, which is negligible compared to the inference latency.


\noindent \textbf{Memory overheads of \sys's Expert Map Store.}
Figure~\ref{fig:eval-overhead-memory.pdf} shows the CPU memory footprint of \sys's Expert Map Store when varying the store capacity from 1K to 32K maps.
The memory needed to store expert maps for \qwen is more than \mixtral and \phimoe because it has more experts per layer over the other two models, which increases the map shape.
Even for the largest capacity (32K), the Expert Map Store requires less than 200MB of memory to store the maps, which is trivial since modern GPU servers usually have abundant CPU memory (\eg, p4d.24xlarge on AWS EC2~\cite{aws-ec2} has over 1100 GB of CPU memory).
In the evaluation, \sys's map store capacity with 1K maps is sufficient for maintaining performance (\S\ref{subsec:eval-sensitivity}), resulting in minimal memory overhead.



\begin{figure}[t]
  \centering
  \includegraphics[width=.85\linewidth]{figs/eval-overhead-memory.pdf}
  % \vspace{-0.1in}
  \caption{CPU memory footprint of \sys's Expert Map Store with different capacity.}
  \vspace{-0.1in}
  \label{fig:eval-overhead-memory.pdf}
\end{figure}
