\section{Introduction}
\label{sec:intro}


\LLMs have achieved remarkable success in advancing \NLP research and transforming various applications, including content generation~\cite{dai2024neural,achiam2023gpt,brown2020language,radford2019language}, search and recommendation~\cite{lin2024data,zhao2024let}, and AI-assisted operations~\cite{nam2024using,li2024go,jiang2024lilac}.
%
Given the high training costs, modern \LLMs have returned to \MoE architectures~\cite{jiang2024mixtral,snowflake-arctic,yang2024qwen2,xai-grok,dai2024deepseekmoe,abdin2024phi} as their backbone implementations. Inside \MoE models, each \MoE layer comprises a gating network and a collection of experts, with only a subset of experts being activated during computation.
%
This sparse activation mechanism significantly reduces the number of \FLOPs, enabling \MoE-based \LLMs to achieve substantially lower training costs compared to dense \LLMs~\cite{dai2024deepseekmoe,yang2024qwen2,jiang2024mixtral}.


Despite the computational efficiency, \MoE models exhibit substantial memory inefficiency during the serving phase. Though certain model parameters remain inactive during inference, they must still reside in GPU memory to allow for potential future activation.
%
Expert offloading~\cite{xue2024moe,song2024promoe,eliseev2023fast,aminabadi2022deepspeed} has emerged as a promising strategy to address this issue, which predicts inactive experts and transfers them to CPU memory while retaining only the necessary experts in GPU memory, reducing the overall model memory footprint.


However, existing expert offloading solutions struggle to effectively balance the \textit{latency-memory trade-off} in \MoE serving. These approaches either suffer from high inference latency~\cite{aminabadi2022deepspeed,song2024promoe} or incur substantial model memory footprints~\cite{eliseev2023fast,xue2024moe}.
%
The key reason is that existing works track expert patterns and manage experts in \textit{coarse granularity}.
They fail to accurately identify and retain only the necessary experts in GPU memory during inference, resulting in frequent and costly on-demand expert loading~\cite{song2024promoe}, which severely degrades serving performance.


In this paper, we propose \sys, a \textit{fine-grained} expert offloading system that tames the latency-memory trade-off in \MoE serving.
%
To track and analyze \MoE models' expert selection behaviors in fine granularity, we propose a new data structure called \textit{expert map}, which records the iteration-level probability distributions output by the gate network.
\sys uses historical expert maps for comparing expert trajectory similarity to guide offloading.\footnote{In this paper, ``trajectory'' is defined as the collection of probability distributions over experts observed through layers.}
%
Apart from the expert map, \sys is designed to track fine-grained input semantic embeddings from individual request prompts processed by the \MoE model.
Given the collected semantic-based and trajectory-based information, \sys carefully searches the most accurate expert map for guiding expert prefetching, caching, and offloading through inference iterations.
%
In summary, we make the following contributions:
\begin{itemize}[noitemsep,topsep=0pt,parsep=0pt,partopsep=0pt]
    \item We design \sys, a \textbf{fine-grained} expert offloading system that achieves low inference latency while reducing model memory footprints.
    \item We propose a new data structure, expert map, that tracks fine-grained expert selection behaviors of \MoE models. \sys leverages input semantic embeddings to augment the expert map search for guiding expert offloading.
    \item We prototype \sys on top of HuggingFace Transformers~\cite{wolf2020huggingface} and deploy it on a six-GPU testbed. Extensive experiments with open-source \MoE models and real-world workloads show that \sys reduces inference latency by 47\% and improves expert hit rate by 36\% compared to state-of-the-art solutions.
\end{itemize}


