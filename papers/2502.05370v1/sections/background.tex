\section{Background and Motivation}
\label{sec:background}

% \begin{figure*}[t]
%   \centering
%   \includegraphics[width=0.7\linewidth]{figs/bg-moe-workflow-v2.pdf}
%   % \vspace{-0.15in}
%   \caption{Mixture-of-Experts (MoE) Large Language Model (LLM) serving workflows.}
%   % \vspace{-0.25in}
%   \label{fig:bg-moe-workflow}
% \end{figure*}

% \begin{figure}[t]
%   \centering
%   \includegraphics[width=0.2\linewidth]{figs/bg-trade-off.pdf}
%   % \vspace{-0.15in}
%   \caption{Design space in MoE-based LLM serving. 
%   % \hnote{(b) todo: add z-axis for granularity} \todo{Merge speculative to coarse-grained branch; remove moe refactor to related work section; Change figure b to three dimensions (add granularity as z axis}
%   }
%   % \vspace{-0.25in}
%   \label{fig:bg-trade-off}
% \end{figure}


\begin{figure*}[t]
    \centering
    \begin{subfigure}[t]{0.79\textwidth}
        \centering
        \includegraphics[width=.98\linewidth]{figs/bg-moe-workflow-v2.pdf}
        \caption{MoE-based LLM serving workflows.\protect\footnotemark}
        \label{fig:bg-moe-workflow}
    \end{subfigure}
    \begin{subfigure}[t]{0.2\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figs/bg-trade-off-arxiv.pdf}
        \caption{Trade-offs in MoE.}
        \label{fig:bg-trade-off}
    \end{subfigure}
    \vspace{-0.1in}
    \caption{Mixture-of-Experts (MoE) Large Language Model (LLM) serving.}
    \label{fig:bg-moe}
    \vspace{-0.1in}
\end{figure*}



\subsection{\LLM Serving}

% LLM two stages: prefill and decode
% prefill is compute bounded, decode is memory bounded
% prefill TTFT, decode TPS or TPOT

Unlike traditional \DL model inference, Large Language Model (LLM) serving consists of two consecutive stages: \textit{prefill} and \textit{decode}. Figure~\ref{fig:bg-moe-workflow} illustrates the two stages when an \LLM performs inference for a request prompt. 
% 
In the prefill stage, the \LLM first computes the intermediate \KV states of the prompt tokens, prefills the \KV cache~\cite{kwon2023efficient,liu2024cachegen,lee2024infinigen,zhong2024distserve,agrawal2024taming}, and then generates the first answer token. 
%
In the decode stage, the \LLM sequentially generates the answer to the prompt token-by-token in an auto-regressive manner, where tokens generated previously are used for generating the next token. 

The two stages have their own unique characteristics.  
%
The prefill stage only requires one \textit{iteration}, processing all tokens in parallel and generating the first answer token. 
%
The decode stage spans several iterations, generating one token per iteration until the answer is completed. 
% 
Due to the different characteristics of the two stages, recent studies~\cite{patel2024splitwise,zhong2024distserve} have identified that the prefill stage is compute-bounded, while the decode stage is considered memory-bounded. 
% 
Therefore, people typically quantify the serving performance of \LLM two stages using different metrics. 
%
For the prefill stage, \TTFT is commonly employed, which measures the latency from receiving the user request until generating the first answer token. 
%
For the decode stage, \TPS or \TPOT is used to measure the generation rate of \LLM serving.

\subsection{\MoE-based \LLM Serving}

% MoE: gate network and expert networks
% Training is more efficient, less computation, and higher performance than dense LLMs. 
% But suffer when serving -> inference still require loading the whole MoE -> large amount of memory -> lead to the latency-throughput trade-off in MoE LLM serving (next subsection)

By integrating \MoE layers in Transformer blocks~\cite{vaswani2017attention}, \MoE architectures~\cite{yuksel2012twenty} have emerged as a popular backbone for modern \LLMs, such as Mixtral~\cite{jiang2024mixtral}, Snowflake Arctic~\cite{snowflake-arctic}, and DeepSeek-MoE~\cite{dai2024deepseekmoe}. 
%
% \hao{\MoE architectures reduce the notorious large size of \LLMs without xxx.}
%
% \MoE layers are applied in the Transformer blocks~\cite{vaswani2017attention}, the default building block for modern \LLMs.
% 
Figure~\ref{fig:bg-moe-workflow} illustrates \MoE-based \LLMs' typical structures, where \FFN modules are replaced by \MoE layers. 
% 
Each \MoE layer consists of a gate network and a set of expert networks. Inside each Transformer block, the self-attention module first calculates the attentions~\cite{vaswani2017attention} based on input hidden states, and then the gate network determines which expert(s) to activate for computing the output representations. 
%
Compared to traditional dense \LLMs, \MoE-based \LLMs only activate a subset of parameters during training and inference, reducing computational overhead while delivering superior generation performance compared to dense \LLMs with a comparable number of parameters~\cite{jiang2024mixtral,snowflake-arctic,dai2024deepseekmoe,xai-grok,yang2024qwen2,abdin2024phi}.

% \begin{table}[t]
%     \centering
%     %\vspace{-0.1in}
%     \scalebox{0.78}{
%     % \begin{small}
%         \begin{tabular}{lcccc}
%             \toprule
%             \multirow{2}*{\textbf{MoE LLM}} & \textbf{Total} & \textbf{Inactive} & \textbf{Total} & \textbf{Inactive} \\
%             & \textbf{Param.} & \textbf{Param. (\%)} & \textbf{Mem.} & \textbf{Mem.  (\%)} \\
%             \otoprule 
%             \mixtral~\cite{jiang2024mixtral} & 46.7B & 33.8B (\todo{xx\%}) & 130GB & 94GB (\todo{xx\%}) \\
%             % \hline
%             DeepSeek-MoE~\cite{dai2024deepseekmoe} & 16.4B & 13.6B (\todo{xx\%}) & 40GB & 33GB (\todo{xx\%}) \\
%             % \hline
%             Arctic~\cite{snowflake-arctic} & 480B & 463B (\todo{xx\%}) & 640GB & 618GB (\todo{xx\%}) \\
%             % \hline
%             Grok-1~\cite{xai-grok} & 314B & 228B (\todo{xx\%}) & 500GB & 370GB (\todo{xx\%}) \\
%             % \hline
%             Qwen-MoE~\cite{yang2024qwen2} & 14.3B & 11.6B (\todo{xx\%}) & 36GB & 30GB (\todo{xx\%}) \\
%             \bottomrule
%         \end{tabular}
%     % \end{small}
%     }
%     \caption{Model parameters and memory footprints of five popular \MoE-based \LLMs.}
%     % \vspace{-0.2in}
%     \label{table:bg-moe-params}
% \end{table}

% However, \MoE \hl{architecture is a double-edged sword.}\hnote{not accurate, what is the fundamental reason for overheads?} 
Despite the benefits of saving training computations, \MoE-based \LLM serving still suffers from GPU memory inefficiency as \MoE inference requires loading all model parameters into GPU memory, including those inactive experts. 
%
% \todo{only show some stats, remove the table} Table~\ref{table:bg-moe-params} shows the model parameters and memory footprints of five popular \MoE-based \LLMs. 
For example, \mixtral~\cite{jiang2024mixtral} and DeepSeek-MoE~\cite{dai2024deepseekmoe} have 72\% and 83\% inactive parameters during inference due to the sparsity of expert activation in \MoE, leading to low memory efficiency and serving throughput. 
%
Therefore, to efficiently serve large \MoE models, we must seek a solution to the memory inefficiency inherited from \MoE architecture.

% \subsection{Problems of Existing Expert Offloading}
\subsection{Latency-Memory Trade-Off
% in Lossless \MoE-based LLM Serving
}
\label{subsec:bg-latency-memory-tradeoff}

% Why this trade-off?
% Because Decoder-only has more balanced routing, load balancing loss. Naively design offloading has bad performance

% How does this trade-off impact serving?

% What to do to tame this trade-off?
% Guided expert offloading

% \hnote{should we move this subsection to 2.3? \todo{exchange this section with section 2.3}}


\begin{figure}[t]
  \centering
  \includegraphics[width=.88\linewidth]{figs/bg-design-space-arxiv.pdf}
  \vspace{-0.1in}
  \caption{The design space of \MoE-based \LLM serving.}
  \vspace{-0.2in}
  \label{fig:bg-design-space}
\end{figure}


% \todo{First briefly describe the methodology of expert offloading}


Recently, a few studies have been proposed to improve \MoE-based \LLM serving efficiency.
Figure~\ref{fig:bg-design-space} describes the design space in \MoE serving.
%
% However, training an \MoE \LLM from scratch (or even fine-tuning) can cost days with many expensive GPUs~\cite{jiang2024mixtral,dai2024deepseekmoe,yang2024qwen2}. 
% 
Existing major studies can be categorized into two types: 
%
\textbf{Lossy serving} applies compression~\cite{li2023merge}, pruning~\cite{lee2024stun}, and quantization~\cite{kim2023mixture} techniques to the original \MoE models to reduce the serving memory requirements. However, this line of work achieves serving efficiency by sacrificing the generation quality. 
%
\textbf{Lossless serving} focuses on \textit{offloading} model weights (parameters~\cite{aminabadi2022deepspeed,ollama} or experts~\cite{eliseev2023fast,song2024promoe,xue2024moe}) that are sparsely utilized in temporal or spatial patterns from GPU memory to CPU memory, aiming to preserve reasonable inference latency. 
%
Specifically, expert offloading seeks to predict the activation of experts in advance, prefetching or caching only the necessary experts in GPU memory during inference.
%
We opt for lossless serving to design \sys because this line of methods avoids modifying models, hence assuring generation quality. 

However, existing offloading solutions cannot achieve an optimal spot in the latency-memory trade-off when serving \MoE-based \LLMs. 
Figure~\ref{fig:bg-trade-off} compares the performance (\ie, inference latency and memory footprint) of existing \SOTA offloading solutions, which either provide low inference latency but suffer from large memory footprint (\eg, No-offload and MoE-Infinity~\cite{xue2024moe}), or vice versa (\eg, ProMoE~\cite{song2024promoe}, Mixtral-Offloading~\cite{eliseev2023fast}, and DeepSpeed-Inference~\cite{aminabadi2022deepspeed}). 
% For example, no-offload and MoE-Infinity~\cite{xue2024moe} have low inference latency but high memory footprint, while ProMoE~\cite{song2024promoe}, Mixtral-offload~\cite{eliseev2023fast}, and DeepSpeed-Inference~\cite{aminabadi2022deepspeed} reduce memory footprint but incur high inference latency.

The key reason behind this dilemma is that \MoE-based decoder-only \LLMs have balanced expert routing~\cite{song2024promoe}, leaving existing solutions hard to find effective patterns for guiding expert offloading.
%
% Many expert offloading studies~\cite{eliseev2023fast,xue2024moe} focus on analyzing request-level \st{(which we consider coarse-grained)} expert activations yet can hardly find effective patterns for prediction. 
%
Existing research has identified two main reasons for this dilemma:
%
First, most \MoE-based \LLMs are decoder-only architectures, which exhibit uniform expert activation patterns and low expert access skewness compared to encoder-decoder \MoE \LLMs~\cite{song2024promoe,gupta2024lynx}. 
% \hnote{not aligned with observations in MoE-Infinity?}
%
Second, recent \MoE-based \LLMs are trained with a unique load balancing loss~\cite{jiang2024mixtral,snowflake-arctic,xai-grok,dai2024deepseekmoe,abdin2024phi}, which enforces gate networks to balance the tokens routed to each expert within the same \MoE layer, ensuring no experts are trivial throughout training.
% This balanced routing diminishes the predictability of coarse-grained expert patterns, thus making coarse-grained solutions ineffective.
This balanced routing diminishes the predictability of expert patterns, thus making existing solutions ineffective.
% Therefore, we argue that \textit{fine-grained} iteration-level information other than request-level should be employed to guide expert offloading in \MoE \LLM serving.


\begin{figure*}[t]
    \centering
    \begin{subfigure}[t]{0.31\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figs/bg-expert-heatmap-v1.pdf}
        \caption{Coarse-grained \vs fine-grained expert heatmaps for \mixtral. Heavier colors indicate more expert activations.}
        \label{fig:bg-expert-heatmap}
    \end{subfigure}
    \hspace{0.02in}
    \begin{subfigure}[t]{0.33\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figs/bg-entropy.pdf}
        \caption{Mean entropy per layer of three MoE models and two datasets for coarse-grained and fine-grained expert patterns. Higher entropy indicates lower predictability.}
        \label{fig:bg-entropy}
    \end{subfigure}
    \hspace{0.02in}
    \begin{subfigure}[t]{0.33\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figs/bg-entropy-iterate.pdf}
        \caption{Mean entropy per layer of three MoE models and two datasets through inference iterations. Aggregating expert patterns over iterations gradually diminishes predictability.}
        \label{fig:bg-entropy-iterate.pdf}
    \end{subfigure}
    % \vspace{-0.1in}
    \caption{Expert pattern and predictability analysis in coarse granularity (request-level) and fine granularity (iteration-level).}
    \label{fig:bg-expert-pattern-analysis}
    % \vspace{-0.1in}
\end{figure*}

\footnotetext{For simplicity, we only show one request prompt in one batch.}

\subsection{Existing MoE Offloading Solutions}

% \hnote{Maybe this section should be titled as ``Lossless MoE-based LLM serving,'' and talk about fine-grained and coarse-grained directly}



\begin{comment}
\begin{itemize}[noitemsep,topsep=0pt,parsep=0pt,partopsep=0pt]
    \item \textbf{Lossy serving} applies compression~\cite{li2023merge}, pruning~\cite{lee2024stun}, and quantization~\cite{kim2023mixture} techniques to the original \MoE models to reduce the serving memory requirements. However, this line of work achieves serving efficiency by sacrificing the generation quality. 
    % We do not compare \sys to lossy serving methods as \sys is designed to be lossless and guarantees generation quality.
    \item \noindent \textbf{Lossless serving} focuses on \textit{offloading} model weights (parameters~\cite{aminabadi2022deepspeed,ollama} or experts~\cite{eliseev2023fast,song2024promoe,xue2024moe}) that are sparsely utilized in temporal or spatial patterns from GPU memory to CPU memory, aiming to preserve reasonable inference latency. 
\end{itemize}
\end{comment}

% Some other works~\cite{hwang2024pre,du2024sida,cai2024read} propose refactoring the current \MoE architecture to achieve efficient serving, which requires retraining the \MoE models and thus impractical for model serving.
% However, training an \MoE-based \LLM from scratch (or even fine-tuning) can cost days with many expensive GPUs~\cite{jiang2024mixtral,dai2024deepseekmoe,yang2024qwen2}, which is impractical for model serving purpose.

%
% Concretely, lossless serving for \MoE \LLMs includes parameter offloading and expert offloading. 
% Parameter offloading~\cite{aminabadi2022deepspeed,ollama}, which transfers portions of model parameters or layers to CPU memory, has been vastly applied in traditional dense \LLM serving. 
% However, existing parameter offloading approaches are not tailored to \MoE \LLMs, thus resulting in unsatisfactory serving performance~\cite{song2024promoe,xue2024moe}. 
% 
% In contrast, \hl{expert offloading is a promising direction}\hnote{expert offloading is a part of parameter offloading, how to distinguish?} to reduce model memory footprint while maintaining inference latency~\cite{eliseev2023fast,song2024promoe,xue2024moe}. 
% 
% Since \MoE models naturally utilize a subset of experts in computations, expert offloading is theoretically more accurate and effective than expert-agnostic parameter offloading methods. 


% \todo{<Hao: should talk about ``coarse-grained'' here to prepare the claim for the dilemma. Also define and motivate ``fine-grained'' here.>} 
Existing expert offloading approaches~\cite{eliseev2023fast,xue2024moe} rely on \textbf{coarse-grained} expert patterns, which are inefficient for guiding offloading. 
% The coarse granularity is reflected in three aspects: 
% %
% 1) focusing on request-level patterns, only involving \textit{coarse-grained} aggregated information, 
% %
% 2) only recording binarized expert activations, failing to understand \textit{fine-grained} quantitative preferences of models' expert selection (\ie, probability distributions), 
% %
% and 3) solely relying on expert activation similarity to guide offloading (\eg, MoE-Infinity~\cite{xue2024moe}), overlooking the unique semantic characteristics of request prompts. 
%
We define coarse-grained information as the expert patterns collected at the request level, where information is aggregated over multiple iterations of a request prompt.
For example, MoE-Infinity~\cite{xue2024moe} tracks request-level expert activations.
Fine-grained information is defined as the expert patterns observed separately during each inference iteration.
%
Figure~\ref{fig:bg-expert-heatmap} shows examples of coarse-grained and fine-grained expert activation heatmaps for \mixtral~\cite{jiang2024mixtral}.
The heatmap records the expert activations across 32 \MoE layers, where each layer contains eight experts and activates two experts out of eight to compute representations. 
%
While fine-grained (iteration-level) heatmaps show clear expert activation patterns, the aggregated coarse-grained (request-level) heatmap diminishes predictability. 

% While existing works focus on leveraging coarse-grained expert patterns (\eg, MoE-Infinity~\cite{xue2024moe} tracks request-level expert activations), such coarse-grained expert information exhibits high uncertainty and unpredictability, leading to inefficient expert offloading.
%
To demonstrate this point, we analyze the Shannon entropy~\cite{shannon1948mathematical} of expert activations per \MoE layer for three popular \MoE models.
% \todo{more detailed descriptions about entropy, and why entropy indicates predict accuracy?}
Entropy is an essential metric to quantify the uncertainty and unpredictability of variables in information theory.
A balanced expert activation pattern (\eg, probability distribution $[0.25, 0.25, 0.25, 0.25]$ of four experts) results in a high entropy, which indicates the pattern is more unpredictable and hard to select experts.
%
Figure~\ref{fig:bg-entropy} presents the mean entropy computed per layer for three \MoE models (\mixtral~\cite{jiang2024mixtral}, \qwen~\cite{yang2024qwen2}, and \phimoe~\cite{abdin2024phi}) across two real-world datasets (LMSYS-Chat-1M~\cite{zheng2023lmsys} and ShareGPT~\cite{sharegpt}).
Coarse-grained expert patterns have significantly higher entropy than fine-grained patterns, meaning that expert patterns in coarse granularity can be less effective for predictions.
%
Figure~\ref{fig:bg-entropy-iterate.pdf} shows the mean entropy per layer through inference iterations.
While the entropy is low at the beginning of inference, it gradually increases through iterations due to aggregating expert activation information, thus becoming more unpredictable.

In contrast to coarse-grained expert offloading solutions, we argue that expert offloading should be carefully guided by \textbf{fine-grained} designs: analyzing iteration-level patterns, understanding models' expert selection preferences, and leveraging semantic characteristics of request prompts. 
% \hnote{we need examples, figures, more details about ``fine-grained''}


\subsection{Problems of Coarse-Grained Offloading}


% \begin{figure}[t]
%   \centering
%   \includegraphics[width=\linewidth]{figs/bg-coarse-vs-fine.pdf}
%   % \vspace{-0.15in}
%   \caption{Expert heatmaps and hit rates of running \mixtral serving with an example prompt. Heavier colors mean higher expert activations. Aggregating request-level heatmap diminishes iteration-level patterns, leading to low expert hit rates when guiding offloading.}
%   % \vspace{-0.25in}
%   \label{fig:bg-coarse-vs-fine}
% \end{figure}


% \begin{figure}[t]
%   \centering
%   \includegraphics[width=.9\linewidth]{figs/bg-entropy.pdf}
%   % \vspace{-0.15in}
%   \caption{Mean entropy per layer across three MoE models and two datasets for coarse-grained and fine-grained expert patterns. Higher entropy indicates lower predictability.}
%   % \vspace{-0.25in}
%   \label{fig:bg-entropy}
% \end{figure}

% \begin{figure}[t]
%   \centering
%   \includegraphics[width=.9\linewidth]{figs/bg-entropy-iterate.pdf}
%   % \vspace{-0.15in}
%   \caption{Cumulative entropy per layer across three MoE models and two datasets when inference iterates. Aggregating expert patterns over iterations gradually diminishes predictability.}
%   % \vspace{-0.25in}
%   \label{fzig:bg-entropy-iterate}
% \end{figure}

% \begin{figure}[t]
%   \centering
%   \includegraphics[width=.8\linewidth]{figs/bg-expert-heatmap.pdf}
%   % \vspace{-0.15in}
%   \caption{Coarse-grained \vs fine-grained expert heatmaps for \mixtral. Heavier colors indicate more expert activations.}
%   % \vspace{-0.25in}
%   \label{fig:bg-expert-heatmap}
% \end{figure}







\begin{figure}[t]
  \centering
  \includegraphics[width=.83\linewidth]{figs/bg-hit-distance.pdf}
  \vspace{-0.1in}
  \caption{Expert hit rates of coarse-grained and fine-grained expert offloading designs when serving \mixtral, \qwen, and \phimoe at different prefetch distances, respectively.}
  \vspace{-0.15in}
  \label{fig:bg-hit-distance}
\end{figure}

% Figure~\ref{fig:bg-coarse-vs-fine} shows the coarse-grained (\eg, MoE-Infinity~\cite{xue2024moe}) and fine-grained expert heatmaps and corresponding expert hit rates of running \mixtral serving with an example prompt. 
% % 
% The heatmap records the expert activations across 32 \MoE layers, where each layer contains eight experts and 
% % . Within each layer, \mixtral 
% activates two experts out of eight to compute representations. 
% %
% While fine-grained (iteration-level) expert heatmaps show clear activation patterns, the aggregated coarse-grained (request-level) heatmap diminishes useful patterns, becoming more balanced but less effective for guiding offloading. 

Existing coarse-grained expert offloading solutions exhibit three problems:

\noindent \textbf{1) Insufficient latency-memory trade-off.} Existing solutions prefetch and offload experts in coarse granularity, either heavily focusing on reducing inference latency but incurring large memory footprint~\cite{xue2024moe} or reducing memory footprint but severely increasing inference latency~\cite{aminabadi2022deepspeed,eliseev2023fast}.

\noindent \textbf{2) Low expert hit rates.} Existing solutions employ coarse-grained expert pattern tracking methods (\eg, Expert Activation Matrix in MoE-Infinity~\cite{xue2024moe}), which produce ineffective expert patterns for guiding offloading decisions, leading to low expert hit rates and high inference latency.

\noindent \textbf{3) Ignorance of \MoE models' and prompts' heterogeneity.} 
% \hao{\noindent \textbf{3) Ignorance of \MoE models' and prompts' heterogeneity.} }
Existing solutions largely ignore the unique characteristics of different \MoE models and input prompts and serve them in a one-fits-all manner~\cite{aminabadi2022deepspeed,eliseev2023fast,song2024promoe,xue2024moe}, which omits opportunities for fine-grained optimizations adaptive to heterogeneous models and prompts in \MoE serving.

% In contrast to existing coarse-grained expert offloading solutions, we argue that experts should be offloaded and managed in fine granularity. 
%
% \textit{First}, even though \MoE \LLMs exhibit unpredictability at the request level, we are the first to observe and leverage \hl{the fine-grained information hidden from \textit{iteration-level} expert activations} \hnote{unclear ``information'' here} to effectively guide expert offloading. 
% 
% \textit{Second}, existing expert offloading approaches, either coarse-grained~\cite{xue2024moe,eliseev2023fast} or speculative prediction~\cite{song2024promoe,eliseev2023fast}, cannot preserve satisfactory expert hit rate when the \textit{prefetch distance}~\cite{song2024promoe} becomes large. 
% 
% Prefetch distance refers to the distance between historical layers (that we can observe expert routing results) and the current layer to predict. The optimal prefetch distance of a \MoE model depends on the prediction overheads needed to hide from the inference process. 
% \hnote{still do not know the definition of ``fine-grained'' information here; we should distinguish ourselves here}
Figure~\ref{fig:bg-hit-distance} shows the expert hit rates of serving three popular \MoE-based \LLMs, \mixtral~\cite{jiang2024mixtral}, \qwen~\cite{yang2024qwen2}, and \phimoe~\cite{abdin2024phi} using LMSYS-Chat-1M dataset~\cite{zheng2023lmsys} with coarse-grained and fine-grained expert offloading designs at different prefetch distances, respectively. 
Prefetch distance refers to the number of layers ahead that a prefetch instruction is issued before the target layer activates its experts. 
% Different \MoE models with different input prompts have unique prefetch distances.
% The optimal prefetch distance of a \MoE model depends on the prediction overheads needed to hide from the inference process.
By leveraging fine-grained expert offloading, we can achieve significantly higher expert hit rates over coarse-grained methods and preserve better performance by adapting to varying prefetch distances.



