\section{\sys's Implementation}
\label{sec:implement}


We prototype \sys on top of Huggingface Transformers framework~\cite{wolf2020huggingface} using MoE-Infinity codebase~\cite{moe-infinity-code}.
% Since the open-sourced MoE-Infinity codebase~\cite{moe-infinity-code} lacks many features described in its original paper, such as we 
The implementation of \sys is described as follows.


\textbf{Expert Map Store} is implemented in Python using PyTorch~\cite{paszke2019pytorch} and NumPy~\cite{harris2020array} libraries.
We store all semantic embeddings and expert maps using \texttt{ndarrays} data structure for efficient array operations. The arrays are converted to tensors to compute similarity for expert map matching.


\textbf{Expert Map Matcher} is implemented in Python using PyTorch~\cite{paszke2019pytorch} and TorchMetrics~\cite{detlefsen2022torchmetrics} libraries.
We implement the pairwise computations, including similarity (\S\ref{subsec:design-similarity-match}) and redundancy (\S~\ref{subsec:design-expert-map-store}) scores, using the Cosine Similarity interfaces in TorchMetrics. 
We use the Python multithreading library to implement the asynchronous expert map matching and expert prefetching, where the threads share the same memory space with the Expert Map Store for efficient reading and writing.


\textbf{Expert Cache} is implemented in C++ based on MoE-Infinity codebase~\cite{moe-infinity-code}. 
% However, as the codebase lacks many features described in the original paper and contains some bugs, such as expert on-demand loading, we had to refactor the code.
The expert management in GPUs is implemented with the CUDA Runtime APIs~\cite{cuda-runtime-api}.
We implement the caching logic of \sys and fix critical bugs in the MoE-Infinity codebase to enable expert offloading.
Same with MoE-Infinity, \sys supports multi-GPU inference with expert parallelism, where the experts are mapped to different GPU devices for loading and offloading. We use a hash map to assign expert IDs to different GPUs and retrieve them during inference.
The expert assignment follows a round-robin manner to balance the overall GPU load.
Additionally, we use a multi-thread task pool in the GPU space to schedule and execute expert prefetching and on-demand loading tasks.