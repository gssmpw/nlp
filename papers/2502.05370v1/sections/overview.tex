\section{\sys's Overview} 

\subsection{Objectives and Challenges}

\sys is designed to achieve the following three goals: 

\textbf{Memory-efficient \MoE serving with minimal inference latency.}
We have demonstrated that existing expert offloading  solutions~\cite{eliseev2023fast,song2024promoe,xue2024moe} fail to tame the latency-memory trade-off in \MoE serving (\S\ref{subsec:bg-latency-memory-tradeoff}). 
% \hl{We aim to reduce the memory footprint of MoE LLMs while minimizing the inference latency.} \hnote{this sentence just repeats...}
We aim to achieve both low memory footprint and inference latency by proposing fine-grained expert offloading.

\textbf{Minimize expert miss due to mispredictions in expert prefetching.}
Expert prefetching, involving future expert activation predictions, is an essential step in expert offloading solutions. 
However, a recent study~\cite{song2024promoe} has shown that \textit{expert miss} due to mispredictions can cause high on-demand expert loading delay in inference.
We should minimize expert miss and mitigate mispredictions in expert offloading.


\textbf{Adapt to heterogeneous \MoE models and prompts.}
% Many works~\cite{hwang2024pre,du2024sida,cai2024read} propose to refactor the \MoE architecture to improve serving efficiency. However, this line of work cannot serve existing trained open-source \MoE \LLMs and requires costly retraining. 
% %
% We must develop a solution to be compatible with open-source \MoE \LLMs and can be easily integrated into existing \MoE \LLM serving systems. \hnote{does not seem to be a challenging goal}
%
\MoE inference can serve heterogeneous models~\cite{jiang2024mixtral,dai2024deepseekmoe,yang2024qwen2,snowflake-arctic,xai-grok} with varying prompts~\cite{zheng2023lmsys,sharegpt} in real-world scenarios.
%
While existing solutions handle different models and prompts with a one-fits-all design, we should design our expert offloading to adapt to the heterogeneity in \MoE serving.

We must address three critical challenges to realize the above objectives:

\textbf{How to maximize expert hit rate when prefetching and offloading experts?} 
% 
Expert hit rate directly relates to the inference latency. With more experts being hit, fewer experts need to be loaded on demand. 
We propose a fine-grained expert offloading solution to achieve a high expert hit rate.


% \textbf{How to mitigate expert miss when prefetching experts?}
% % 
% Existing studies~\cite{eliseev2023fast,song2024promoe,xue2024moe} largely ignore handling expert miss and only focus on improving the prediction accuracy of expert prefetching.
% In contrast, we propose to mitigate expert miss via similarity-aware selection when offloading experts.

\textbf{How to adapt to different \MoE models and prompts?}
Heterogeneous \MoE models and input prompts exhibit unique system and semantic characteristics.
We should craft our solution with fine-grained optimizations to enable adaptivity.


\textbf{How to avoid additional system overheads when managing experts?}
Our design must not introduce additional system overheads when serving existing \MoE \LLMs.
We apply a series of system optimizations in \sys to ensure serving efficiency and minimize additional overheads.



\begin{figure}[t]
  \centering
  \includegraphics[width=.9\linewidth]{figs/overview-arch-v2-arxiv.pdf}
  \vspace{-0.1in}
  \caption{\sys's architecture and workflow.} 
  \vspace{-0.1in}
  \label{fig:overview-arch.pdf}
\end{figure}

\subsection{Architecture and Workflow}

Figure~\ref{fig:overview-arch.pdf} describes the architecture and workflow of \sys, which consists of three main components: 

\begin{itemize}[noitemsep,topsep=0pt,parsep=0pt,partopsep=0pt]
    \item \textbf{Expert Map Store.} 
    We record \textit{expert maps}, a new data structure defined in \sys, to track \textit{fine-grained} expert activation patterns from historical request prompts. 
    expert maps provide nuance expert selection preferences over existing coarse-grained expert tracking methods (\eg, Expert Activation Matrix in MoE-Infinity~\cite{xue2024moe}).
    The Expert Map Store dynamically keeps the most useful and unique expert maps for real-time queries during inference.
    %
    \item \textbf{Expert Map Matcher.} 
    When a request prompt arrives, \sys searches the Expert Map Store for appropriate expert maps to guide expert prefetching before inference. 
    expert map search is guided by calculating similarity scores in two folds: \textit{semantic} and \textit{trajectory} similarity.
    %
    \item \textbf{Expert Cache.} 
    After receiving the matched expert maps, \sys prefetches experts from CPU memory to GPU memory for performing computations in inference. 
    \sys evicts and offloads low-priority expert weights to CPU memory if exceeding Expert Cache capacity.
\end{itemize}


% \sys categorizes \MoE layers into two types when prefetching and offloading experts: \textit{initial} and \textit{subsequent} layers.
% In an \MoE \LLM with $N$ \MoE layers, initial layers refer to $[1, k]$ layers, and subsequent layers are the $[k+1, N]$ layers.

\sys follows the five steps below to enable memory-efficient \MoE serving with minimal inference latency:

\noindent \textbf{Step {\Large \circled{\small 1}}: Inference context collection.} 
Before every inference iteration, \sys collects necessary \textit{contexts}, such as semantic embeddings and previous expert activation trajectories (\S\ref{subsec:design-expert-map}), and feeds them to the Expert Map Matcher for hybrid similarity matching.

\noindent \textbf{Step {\Large \circled{\small 2}}: Expert map similarity matching.} 
After receiving iteration-level contexts, the Expert Map Matcher finds and extracts the most similar expert maps by comparing the input context data with historical context data in the Expert Map Store (\S\ref{subsec:design-similarity-match}). 
% 
The matched expert maps are forwarded to the Expert Cache to guide expert prefetching and offloading decisions.

\noindent \textbf{Step {\Large \circled{\small 3}}: Guided expert prefetching and offloading.} 
We dynamically compute expert selection thresholds to determine which expert(s) to prefetch and offload in the \MoE model guided by the searched expert maps (\S\ref{subsec:design-expert-prefetch}). Then, \sys prefetches the expert weights from CPU to GPU memory and offloads cached experts from GPU to CPU when reaching the cache limit (\S\ref{subsec:design-expert-cache}).

\noindent \textbf{Step {\Large \circled{\small 4}}: Expert serving.} 
The whole inference process consists of one iteration in the Prefill stage and multiple iterations in the Decode stage. For each \MoE layer in every iteration, \sys directly serves the expert required by the gating network if the corresponding weights are available in the GPU memory (defined as an expert hit). Otherwise, \sys on-demand loads the expert weights from CPU to GPU to perform lossless serving (defined as an expert miss).


\noindent \textbf{Step {\Large \circled{\small 5}}: Expert map update.} 
\sys observes new expert maps produced after each iteration and updates them in the Expert Map Store (\S\ref{subsec:design-expert-map-store}). When reaching the store capacity (\eg, 1K expert maps), \sys deduplicates the Expert Map Store by identifying and dropping redundant expert maps to maintain diversity, maximizing the possibility of providing effective expert maps for any request prompts.

% When request prompts arrive, \sys's Expert Map Matcher immediately calculates the semantic similarity between the request prompts and historical prompts in the Expert Map Store to serve initial layers. \sys searches the expert maps with the highest semantic similarity scores to guide expert prefetching for initial \MoE layers. 
% After the inference iteration proceeds to deeper layers, we collect the trajectories from previous layers and feed them as input to \sys's Expert Map Matcher. \sys then searches the expert maps with the highest trajectory similarity scores.
% With the searched expert maps, \sys's Expert Cache is guided to prefetch the expert weights from CPU to GPU for inference and offload expert weights from GPU to CPU whenever cache eviction occurs.
% \sys performs the above expert prefetching, caching, and offloading process throughout every iteration until the whole inference is completed and answers generated by the \MoE \LLM.


\subsection{Problem Formulation}

We consider serving an \MoE-based \LLM with $L$ \MoE layers on a GPU cluster, where each \MoE layer has one gating network and $J$ experts. 
% 
The gating network of each layer selects top $K \in [1, J]$ experts for computation.
The \MoE model processes and generates answers for a workload consisting of $W$ unique request prompts.
%
Each request prompt $w \in [W]$ consists of multiple iterations processed during the prefill and decode stages, where $[W]$ is the request prompt collection.
Let $E^{(i)}_{l, j}$ denote the $j$-th expert at the $l$-th layer in the $i$-th iteration, where $l \in [L]$, $j \in [J]$, and $i \in [w]$.
During each iteration $i$, we can make at most $L \cdot J$ prefetching decisions. 
Let $E^i_{\textit{cache}}$ and $E^i_{\textit{activate}}$ denote the set of cached experts and the set of activated experts for Iteration $i$, respectively.
Hence, we represent the result of whether an expert $E^{(i)}_{l, j} \in E^{(i)}_{\textit{activate}}$ is hit (served by $E^{(i)}_{\textit{cache}}$) or miss (on-demand loading from CPU memory):
\begin{equation*}
  R^{(i)}_{l, j} = 
  \begin{cases}
    1, & \text{if \big{(}$E^{(i)}_{l, j} \in E^{(i)}_{\textit{activate}}\big{)} \bigwedge \big{(}E^{(i)}_{l, j} \notin E^{(i)}_{\textit{cache}}\big{)}$}, \\
    0, & \text{otherwise},
  \end{cases}
\end{equation*}
where $R^{(i)}_{l, j} = 1$ means $E^{(i)}_{l, j}$ is a miss.
% Let $M$ denote the total GPU memory available for caching expert weights, excluding not offloadable parameters in the \MoE model. 
Since all experts in an \MoE model are typically designed to have the same weight size, we assume experts' loading time $T_e$ and memory footprint $M_e$ are homogenous.\footnote{We only consider selective experts. Some MoE models, such as Qwen1.5-MoE-A2.7B, have a few always-on experts that are not offloadable.}
%
Therefore, the total on-demand loading latency $T$ is summed across all iterations for each expert during the inference process, \ie, $T := T_e \cdot \sum_{w \in [W]} \sum_{i \in [w]} \sum_{l \in [L]} \sum_{j \in [J]} R^{(i)}_{l,j}$.

Finally, employing the above definitions, we formulate the \MoE expert offloading as an \ILP optimization problem: 
\begin{align}
    \min_{\{E^{(i)}_{l, j}\}} &\Big{(} T_e \cdot \sum_{w \in [W]} \sum_{i \in [w]} \sum_{l \in [L]} \sum_{j \in [J]} R^i_{l,j} \Big{)} \notag \\
    \mathrm{s.t.~} 
    & |E^{(i)}_{\textit{cache}}| \leq L \cdot J,\quad \forall i \in [w], ~\forall w \in [W], \label{eq:constraint1} \\
    & |E^{(i)}_{\textit{activate}}| = L \cdot K,\quad \forall i \in [w], ~\forall w \in [W], \label{eq:constraint2} \\
    & |E^{(i)}_{\textit{cache}}| \cdot M_e \leq M,\quad \forall i \in [w], ~\forall w \in [W]. \label{eq:constraint3}
\end{align}
The objective is to minimize the on-demand loading latency (ideally $T = 0$ with perfect predictions) while limiting the total memory footprint of cached experts to satisfy the available GPU memory $M$.
%
Constraint~\ref{eq:constraint1} denotes the total number of prefetched experts should not exceed the total number of all experts in the \MoE model.
Constraint~\ref{eq:constraint2} represents the total number of activated experts, which must be the same as the total number of top $K$ experts summed across all $L$ layers.
Constraint~\ref{eq:constraint3} describes the total memory footprint of prefetched experts must be limited by the available GPU memory size.
%
Note that solving the \ILP problem is already NP-hard~\cite{cormen2022introduction}, while in reality, prefetching experts always have mispredictions that further complicate the problem. Therefore, we opt for a heuristic-based design for \sys.

% \hnote{@Hong, should we shrink this part?}