\section{Related Work}

% \noindent \textbf{\MoE-based \LLM serving systems.}
% Expert offloading: ProMoE, MoE-Infinity, Mixtral-offload, SwapMoE, Lina
% MoE quantization, compression, pruning: Compression of MoE, MERGE, THEN COMPRESS, 
% Refactoring MoE architecture: SiDA, Read-ME?
% Lynx?

\noindent \textbf{Lossless \MoE serving.}
Recent studies on lossless \MoE serving have been widely proposed. 
DeepSpeed inference~\cite{aminabadi2022deepspeed} offload layer-wise parameters without expert awareness.
Mixtral-Offloading~\cite{eliseev2023fast} employs \LRU expert caching and introduces speculative prediction to enable expert prefetching. 
MoE-Infinity~\cite{xue2024moe} proposes the request-level expert activation matrix to guide offloading in coarse granularity.
SwapMoE~\cite{swapmoe} maintains a set of critical experts in GPU memory and adjusts them based on workload changes to minimize offloading overhead.
ProMoE~\cite{song2024promoe} trains predictors per \MoE layer to achieve high speculative prediction accuracy and low inference latency. 
Lina~\cite{li2023accelerating} moves infrequently used experts to host memory and focuses more on distributed \MoE training. Liu \etal~\cite{liu2025optimizing} serves \MoE models on serverless computing by predicting expert patterns with black-box Bayesian techniques.
Unlike existing coarse-grained offloading solutions, \sys tracks fine-grained expert patterns from both trajectory and semantic aspects and outperforms \SOTA baselines.

\noindent \textbf{Lossy \MoE serving.}
Except for lossless offloading, other works also propose lossy \MoE serving.
Expert pruning~\cite{chen2022moe,kim2021moe} reduces memory usage by removing underutilized experts.
Knowledge distillation~\cite{rajbhandari2022kd,artetxe2022kd} produces compact sparse MoE models. 
ComPEFT~\cite{yadav2023compeft} demonstrates expert compression without accuracy loss, while MC-SMoE~\cite{li2023merge} further decomposes merged experts into low-rank and structurally sparse alternatives. 
Hobbit~\cite{tang2024hobbit} uses low precision to serve less-critical experts.
However, lossy serving may impact the generation quality and is orthogonal to \sys.

\noindent \textbf{\MoE refactorization.}
Some works propose to redesign and refactor the current \MoE architecture. 
Pre-gated MoE~\cite{hwang2024pre} utilizes preemptive expert selection to eliminate the sequential dependencies between expert selection and execution. 
SiDA-MoE~\cite{du2024sida} proposes a sparsity-inspired, data-aware inference system that decouples the expert routing from inference. 
READ-ME~\cite{cai2024read} refactors pre-trained dense \LLMs into specialized \MoE models.
Unlike the above works, \sys requires zero training to serve open-source \MoE models.

% \noindent \textbf{Optimizing MoE Serving on Limited Resources.}  
% Recent advancements in \MoE-based model have targeted improving memory efficiency and computational performance through system-level optimizations, algorithmic innovations, and architectural refactorings. 
% %
% At the system level, \textit{Mixtral-offloading}~\cite{mixtral2023} incorporates an \hao{\LRU} cache for the Mixtral MoE model and introduces a skip-based prediction method to support expert prefetching. 
% %
% \textit{SwapMoE}~\cite{swapmoe} maintains a set of important experts in GPU memory and dynamically adjusts them based on workload changes to minimize offloading overhead, while \textit{ProMoE}~\cite{promoe} uses a learned prediction method with \hl{high GOODPRED} to perform latency-oriented inference, achieving computational equivalence to the original model without compromising accuracy. 
% %
% \textit{Lina}~\cite{lina2023} transfers infrequently-used expert weights to host memory, while \textit{MoE-Infinity}~\cite{xue2024moe} leverages predictive expert pre-fetching \hl{based on xxx} \hnote{a general word} (\ie, Expert Activation Matrix) to optimize memory access patterns. 
% % 
% \hao{Unlike existing solutions}, \sys achieves lower latency by exploiting unique characteristics of \MoE to enable finer-grained expert prediction. \hnote{only lower latency?}

% On the algorithmic front, expert pruning~\cite{chen2022moe,kim2021moe} reduces memory usage by eliminating underutilized experts, knowledge distillation~\cite{rajbhandari2022kd,artetxe2022kd} produces compact sparse \MoE models and \textit{ComPEFT}~\cite{compeft} demonstrates expert compression without accuracy loss. \textit{MC-SMoE}~\cite{mcsmoe}
% further decomposes the merged experts into low-rank and structural sparse alternatives. \sys is orthogonal to these works, and as part of future work, we plan to integrate these algorithmic techniques to further improve performance and efficiency.
% % see LYNX



% % \textbf{\LLM serving with limited resources.}
% % Disk offload: FlexGen, CachedAttention, PowerInfer
% % LLM quantization, compression, pruning: FlexGen
% % Sparsity-aware serving: TODO
% \noindent \textbf{Optimizing Generic \LLM Serving on Limited Resources.}  
% \hl{Other works address this limitation in broader scenarios.}\hnote{this sentence has not much information...}
% %
% \textit{DeepSpeed-Inference}~\cite{deepspeed} and \textit{FlexGen}~\cite{flexgen} focus on offloading model weights and KV cache to host memory or storage. \textit{FastServe}~\cite{fastserve} and \textit{CachedAttention}~\cite{cachedattention} improve memory utilization and reduce latency by optimizing KV cache scheduling and offloading in multi-turn scenarios. 
% % 
% Further, \textit{PowerInfer}~\cite{powerinfer} and \textit{LLM in a Flash}~\cite{llminflash} exploit sparsity in \FFN computations to offload inactive weights, minimizing both memory usage and computational overhead. 

% On the algorithmic side,  sparsification~\cite{hoefler2021sparsification,frantar2023sparsification} and quantization approaches~\cite{kwon2022quantization,yao2022quantization,dettmers2022quantization} were proposed to reduce model size and computation by compressing weights to as low as 3 bits~\cite{frantar2022quantization} and activations to 8 bits~\cite{yao2022quantization}. 
% % 
% These methods perform well in dense models but overlook the sparse activation nature of MoE.


% % \noindent \textbf{Optimizing \LLM serving systems.}
% % Continuous batching: Orca
% % pagedattention: vLLM
% % Prefix caching: SGLang
% % PD disaggregation, chunked prefill: DistServe, Spliwise, MemServe, LoogServe, Tetrinfer

% \noindent \textbf{Optimizing \LLM Serving Systems.} 
% The advancements have centered around batching, cache management, and resource disaggregation of LLM inference. 
% %
% Orca~\cite{orca} enhances throughput via continuous batching and minimizes system idle time with iteration-level scheduling.
% %
% vLLM~\cite{vllm} introduces paged-attention for fine-grained KV cache management, while ChunkAttention~\cite{chunkattention} focuses on optimizing inference tasks with shared prompt prefixes. 
% %
% SGLang~\cite{sglang} achieves multi-level KV cache sharing using a tree-based \LRU structure. 
% %
% SARATHI~\cite{sarathi} proposes a chunked-prefill for long-context prefill. For disaggregating the two phases, solutions~\cite{splitwise,distserve,tetriinfer,loongserve,memserve} separate prefill and decode stages to avoid interference, enabling more efficient utilization.


% LLM inference serving is a rapidly developing field, with several recent works optimizing in batching, ....
% Orca [1] introduces continuous batching to increase throughput and proposes iterative-level scheduling to reduce bubbles.

% vLLM [2] proposes paged-attention for fine-grained KV cache management. vLLM and ChunkAttention [58] explore some simple reuse cases (e.g., system prompt sharing) but do not cover multi-level tree-structured sharing or LRU
% caching. 

% SGLang[3] first proposes treating the KV cache as a tree-based LRU cache, and the first solution that supports multi-level sharing, cache-aware scheduling, frontend-runtime co-scheduling, and distributed cases.

% To mitigate the impact of the long context, SARATHI [4]
% proposes chunked-prefill, splitting a prefill request into chunks and piggybacking decoding requests to overcome suboptimal prefill processing.

% SplitWise [5], DistServe [6], and TetriInfer [7] disaggregate two phases into different groups of GPUs to avoid interference, and LoongServe [8] takes a step further by enabling dynamic scaling. MemServe[8] takes a different approach by first abstracting out the MemPool component and then building disaggregated inference as a use case of MemPool.


