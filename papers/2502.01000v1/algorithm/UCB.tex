\vspace{-0.2cm}
\begin{algorithm}
  \caption{The MAB decision-making policy}
  \label{ucb}
  \begin{algorithmic}[1]
    \Require $\mathcal{D}_\mathcal{A}, \mathcal{D}_\mathcal{T}$: Auxiliary and target datasets
    \Require $f_\theta$: Parameterized model
    %\Require $M$: Best arm selection steps
    \Require $\alpha,\beta$: Decaying  and smoothing factors
    \State \textbf{Initialize} $f_{\theta_0}=\Theta_s$
    \State \textbf{Initialize} the information of each arm $a \in A$\\
    $\begin{array}{ll}
        \forall a \in A: & n_a = 1, \\
        &\nabla_{a} = \nabla_\theta \mathcal{L}(f_{\theta_0}, \mathcal{D}_a),
        \nabla_\mathcal{T} =\nabla_\theta\mathcal{L}(f_{\theta_0}, \mathcal{D}_\mathcal{T}),\\
        & \mathcal{R}^{PM}_{a,0} = -\mathcal{L}(f_{\theta_0},\mathcal{D}_a),
        \mathcal{R}^{PT}_{a,0} = 
        \cos(\nabla_{a}, \nabla_\mathcal{T}),\\
        %\frac{\nabla_a\cdot\nabla_\mathcal{T}}{||\nabla_a||_2 ||\nabla_\mathcal{T}||_2},\\
        & \hat{\mathcal{R}}_{a} = 0.5 \mathcal{R}^{PM}_{a,0} + 0.5 \mathcal{R}^{PT}_{a,0}
        %\frac{\cos(\nabla_\theta \mathcal{L}(f_{\theta_0}, \mathcal{D}_a),\nabla_\theta\mathcal{L}(f_{\theta_0}, \mathcal{D}_\mathcal{T}))}{{\mathcal{L}(f_{\theta_0}, \mathcal{D}_a)}}
        \end{array} $
    \For{$t = 1, 2, \dots, N$}
        \State Calculate the upper confidence bound for each arm
        \State $a^* = \arg\max_{a \in \mathcal{A}} \left(\hat{\mathcal{R}}_{a} + \sqrt{\frac{2 \ln t}{n_a}}\right)$
        \State Select the auxiliary dataset $\mathcal{D}_{a^*}$
        \State $n_{a^*} \leftarrow n_{a^*} + 1$

        \State $\nabla_\mathcal{T} \leftarrow\nabla_\theta\mathcal{L}(f_{\theta_{t-1}}, \mathcal{D}_\mathcal{T})$ 
        \State $\nabla_{a^*} \leftarrow \nabla_\theta \mathcal{L}(f_{\theta_{t-1}}, \mathcal{D}_{a^*})$
        \State Update model parameters w.r.t. $\nabla_\mathcal{T} + \nabla_{a^*}$
        \State Update the reward of the pulled arm
        \State $R_{a,t} = \alpha R^{PM}_{a,t} + (1-\alpha) R^{PT}_{a,t}$
        \State $\hat{R}_{a} \leftarrow (1 - \beta)\hat{R}_{a} + \beta R_{a,t}$
        \State Release memory $\nabla_{a^*} \leftarrow 0$
    \EndFor
    \State \textbf{end for}  \\
    \Return $f_\theta$
  \end{algorithmic}
  \vspace{-0.1cm}
\end{algorithm}
\vspace{-0.3cm}
