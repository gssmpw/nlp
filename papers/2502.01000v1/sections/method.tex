\vspace{-0.2cm}
\section{Methodology}
\vspace{-0.2cm}
In this section, we will elaborate on the proposed active and sequential domain adaptation (ASAP) framework, shown in Fig.~\ref{framework}.
First, we clarify the setting of few-shot domain adaptation with auxiliary datasets.
Then we formulate it as a multi-armed bandit (MAB) problem and describe how we solve it.


\vspace{-0.2cm}
\subsection{Problem definition}
\vspace{-0.1cm}
For domain adaptation problems, the network is usually first trained on an adequate source domain dataset $\mathcal{D}_\mathcal{S}$. 
We denote the pre-trained source model as $\Theta_s$.
Given a small quantity of data belonging to a target domain dataset $\mathcal{D}_\mathcal{T}=\{(x_i^t,y_i^t)\}_{i=1}^{m}$, the goal is to adapt $\Theta_s$ to achieve high performance on $\mathcal{D}_\mathcal{T}$ with access to a set of available auxiliary datasets $\mathcal{D}_\mathcal{A} = \{\mathcal{D}_{a_1},\mathcal{D}_{a_2},...,\mathcal{D}_{a_K}\}$.
For all $a \in A$, $\mathcal{D}_a$ is an individual auxiliary dataset.
% we actively and sequentially adapt $\Theta_s$ from $\mathcal{D}_\mathcal{S}$ to $\mathcal{D}_\mathcal{T}$ with auxiliary data. 
% $\mathcal{D}_\mathcal{T}\cup\mathcal{D}_\mathcal{A}$.

In this work, we formulate the auxiliary data selection problem in FSDA as a Markov decision process by adopting the multi-armed bandit (MAB) setting \cite{macready1998bandit}.
MAB has been used in sequential experiment design in active learning,
where the goal is to sequentially choose experiments to perform with the aim of maximizing some outcomes.
The MAB learning paradigm involves 
%a sequential decision-making process where 
an agent interacts with an environment over $N$ turns by following a policy $\pi$.
In our work, the environment consists of the target dataset $\mathcal{D}_\mathcal{T}$, the set of auxiliary datasets $\mathcal{D}_\mathcal{A}$, and the model $f_\theta$.
The agent learns a policy $\pi$ that defines the selection strategy over all $\mathcal{D}_a\in\mathcal{D}_\mathcal{A}$. 
%At each turn $t$, the agent pulls one of the environment’s $K$ arms, after which the environment feeds back a reward $R_t$. 
%Rewards for unplayed arms are not observed. 
%The goal of the agent is to adopt a policy $\pi$ that selects actions that lead to the largest cumulative reward over $N$ turns, $R=\sum_{t=1}^N R_t$. 
At each turn $t$, the agent selects one of the environment’s $K$ datasets $\mathcal{D}_a\in\mathcal{D}_\mathcal{A}$
to jointly trained with $\mathcal{D}_\mathcal{T}$.
The environment then updates the model $f_\theta$.
Accordingly, the agent receives a reward $R_{a,t}$ and uses it to update the policy $\pi$.
Rewards for unplayed arms are not observed. 
The goal of the agent is to adopt a policy $\pi$ that selects actions that lead to the largest cumulative reward over $N$ turns, $R=\sum_{t=1}^N R_t$. 
\input{figures/framework}
\vspace{-0.45cm}
\subsection{Deriving an efficient reward function}
\vspace{-0.1cm}
To ensure that the decision-making process adds minimal memory and computational overhead,
we derive rewards from the model's intrinsic information and the optimized losses, rather than relying on an external model or metric that requires extra training.
%\cite{chen2022auxiliary} or metric \cite{albalak2022feta} that requires extra training.
% In this work, we propose an efficient reward function to prioritize training on auxiliary datasets.
% This prioritized training maximizes auxiliaries's benefits to the target task while minimizing overfitting to the few-shot target data.
To achieve positive transfer during sequential adaptation, we design the reward with two considerations in mind: positive for model convergence and positive for joint training with the target task.

Formally, at turn $t$ for the auxiliary dataset $\mathcal{D}_a$
the reward of positive for model convergence is defined as:
\vspace{-0.1cm}
\begin{equation}
% \vspace{-0.1cm}
\label{PM}
    \mathcal{R}^{PM}_{a,t} = -\mathcal{L}_{a,t} = -\mathcal{L}(f_{\theta_t}, \mathcal{D}_a).
\end{equation}
Let $\nabla_{a} = \nabla_\theta \mathcal{L}(f_{\theta_t}, \mathcal{D}_a)$ be the auxiliary dataset gradient and $\nabla_\mathcal{T} =\nabla_\theta\mathcal{L}(f_{\theta_t}, \mathcal{D}_\mathcal{T})$ be the target dataset gradient,
% \begin{equation}
%     \nabla_{a} = \nabla_\theta \mathcal{L}(f_{\theta_t}, \mathcal{D}_a),\nabla_\mathcal{T} =\nabla_\theta\mathcal{L}(f_{\theta_t}, \mathcal{D}_\mathcal{T}),
% \end{equation}
we denote the reward of positive for joint training with $\mathcal{D}_\mathcal{T}$ as:
% \begin{equation}
%             R^{PT}_{a,t} = \cos(\nabla_a , \nabla_\mathcal{T}),
% \end{equation}
% where $\cos(\nabla_a,\nabla_\mathcal{T})$ is the cosine similarity between the gradients of the sampled auxiliary dataset and the target dataset:
\vspace{-0.1cm}
\begin{equation}
\vspace{-0.1cm}
\label{PT}
     \mathcal{R}^{PT}_{a,t} = \frac{\nabla_a\cdot\nabla_\mathcal{T}}{||\nabla_a||_2 ||\nabla_\mathcal{T}||_2}.
\end{equation}
Overall, at turn $t$ the reward of the auxiliary dataset $\mathcal{D}_a$ is defined as:
\vspace{-0.3cm}
\begin{equation}
\vspace{-0.1cm}
\label{reward}
    \mathcal{R}_{a,t} = \alpha \mathcal{R}^{PM}_{a,t} + (1 - \alpha) \mathcal{R}^{PT}_{a,t},
\end{equation}
where $\alpha$ is a time-variant weight that decreases with each selection turn.
Considering that $\mathcal{R}_{a,t}$ relies on the loss and gradients, which are intrinsic to the model, $\mathcal{R}_{a,t}$ is naturally training-efficient.
\vspace{-0.2cm}
\subsection{The decision-making policy}
\vspace{-0.2cm}
To optimize a decision-making policy, we propose to adopt a trace upper confidence bound (UCB) algorithm \cite{auer2002finite} %which is commonly used in MAB problems.
%In the UCB algorithm, 
where the agent greedily selects arms according to their upper confidence bound. 
%and we adapt it to our scenarios.
%The UCB algorithm was initially designed for stationary, normally distributed rewards. However, variants like \cite{ruan2021linear,wei2021nonstationary} have shown effectiveness in non-stationary, sub-Gaussian, and heavy-tailed distributions. To handle non-stationarity, 
%Here we adopt an exponential moving average \cite{wei2021nonstationary} for estimating the mean reward per arm.
Formally, after pulling the arm $a$ at turn $t$, the agent receives a observed reward $R_{a,t}$ and then calculate the estimated mean reward as:
\vspace{-0.1cm}
\begin{equation}
\vspace{-0.1cm}
    \hat{R}_{a} = (1-\beta) \hat{R}_{a} + \beta R_{a,t},
\vspace{-0.1cm}
\end{equation}
where $\beta$ is the smoothing factor \cite{wei2021nonstationary}.
Accordingly, we define the upper confidence bound based on Hoeffding’s inequality \cite{auer2002finite} for arm $a$ at turn $t$ being played $n_a$ times:
\vspace{-0.2cm}
 \begin{equation}
 \vspace{-0.2cm}
    UCB_{a,t} =
    \begin{cases}
        \infty,&\text{if } n_a = 0 \\
        \hat{R}_{a} + \sqrt{\frac{2\ln t}{n_a}},&\text{otherwise}.
    \end{cases}
\end{equation}
This allows us to balance the exploitation of arms with a high predicted reward and the exploration of areas with high uncertainty.
The proposed algorithm is shown in Algorithm.~\ref{ucb}.
\input{algorithm/UCB}


 
