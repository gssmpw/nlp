\vspace{-0.5cm}
\section{Experiments and Results}
\vspace{-0.2cm}
%The flexibility of our ASAP framework enables easy adaptation of foundation models to different medical image processing scenarios, facilitating comparative analyses and extending their application scopes.
To showcase the flexibility of our ASAP framework, we conduct extensive experiments on MRI and CT datasets covering various modalities and anatomical regions.
\vspace{-0.2cm}
\subsection{Datasets and Implementation Details}
\vspace{-0.1cm}
For MRI experiments, 
we construct the auxiliary datasets pool based on FeTS 2022 \cite{pati2021federated} 
(brain tumor segmentation) and iSeg2019 \cite{sun2021multi} (brain tissue segmentation). The auxiliary task pool consists of 30 datasets, each with a sample size exceeding 30.
%we use the collection of datasets FeTS 2022 \cite{pati2021federated} and iSeg-2019 \cite{sun2021multi} as the auxiliary datasets, covering brain tumor and brain tissue segmentation tasks.
%We select a total of 30 datasets, each with a sample size exceeding 30.
For the target datasets, we use two brain 3D MRI segmentation datasets:
the periventricular leukomalacia (PVL) dataset \cite{yang2021joint}, characterized by tissue reduction in periventricular and manually delineated on each slice of the patientâ€™s T2 MRI images, and the White Matter Hyperintensity (WMH) dataset \cite{kuijf2019standardized}
which segments white matter hyperintensities on FLAIR MRI images.
For CT experiments, we construct 30 datasets from TotalSegmentator (TOS) \cite{wasserthal2023totalsegmentator} as the auxiliary datasets, based on label diversity and density.
TOS is a whole-body-segmented 3D CT dataset that contains 117 main default tasks.
%We prevent data leakage by filtering out datasets that overlap with any target dataset.
For the target datasets, we experiment with vessel and liver segmentation tasks from MSD, a benchmark 3D CT dataset \cite{antonelli2022medical}.
% the Medical Segmentation Decathlon (MSD) dataset \cite{antonelli2022medical}.
For each auxiliary dataset, we use at most 30 training examples.
For each target task, all the experiments are conducted under the 1-way 3-shot scenario using 5-fold cross-validation.
We experimented with 5, 3, and 2 target samples, and found that using 3 samples yields satisfactory results in few-shot settings while also reducing the size requirements of the target dataset.
%, following the common few-shot learning setting of \cite{ouyang2022self}.
We implement all methods on pre-trained UNet\cite{ronneberger2015u}, SwinUNet \cite{cao2022swin}, and MambaUNet \cite{wang2024mamba}, from Flemme \cite{zhang2024flemme} following the pre-training settings of
MONAI \cite{cardoso2022monai}.
% For each kind of foundation model, we use consistent hyperparameters for all compared methods.
% The models are trained using a single A800 80GB GPU for a maximum of 400 epochs with a learning rate of 0.0003 and weight decay of 1e-7.
\vspace{-0.2cm}
\subsection{Performance evaluation}
\vspace{-0.1cm}
\input{table/ASAP}
\input{figures/result}
We compare our framework with state-of-the-art few-shot domain adaptation methods: %categorized into non-auxiliary and auxiliary-aided approaches:
1) direct fine-tuning (FT) the source model on the target dataset,
2) GMS \cite{yu2020gradient} identifies one best auxiliary dataset to aid the target based on gradient magnitude similarity,
3) a mixed-batch multi-task learning (MTL) framework \cite{graham2023one} utilizes all auxiliary data simutaneously,
4) a dynamic auxiliary learning (DAL) method \cite{albalak2024improving} adaptively samples the auxiliary data to jointly train with the target dataset based on gradient alignment.
We evaluate the target segmentation performance using the Dice score and the mean IoU. 
A quantitative analysis of model adaptation performance on MRI and CT datasets is detailed in Table~\ref{ASAP}.
% The CT experiment results are the average scores of four target segmentation tasks from MSD: liver, liver tumor, pancreas, and vessel.
The proposed ASAP framework outperforms all the baselines on all datasets, across modalities and anatomical regions. 
We also present the WMH and liver segmentation results on MambaUNet of different methods, clearly demonstrating
the enhancements our method brings to the target few-shot medical image
segmentation tasks, as shown in Fig~\ref{result}.
%Our method increases the improvement of GMS, the a-prior dataset selection method, to 17.16\%  on average of MRI and CT datasets, and dynamic auxiliary learning methods to 9.85\% with lower computation cost.

\vspace{-0.4cm}
\subsubsection{Effectiveness of exploring and exploiting}
\vspace{-0.1cm}
The a-priori dataset selection method GMS is inferior to ours because it relies solely on exploiting the relations determined prior to training, but never exploring, e.g., as observed in the vessel experiment on the right side of Table~\ref{ASAP}.
In contrast, the multi-task learning (MTL) framework continuously explores all auxiliary data but never exploits knowledge of their relation to the target, leading to unsatisfactory results, e.g., as observed in the WMH experiment on the left side of Table~\ref{ASAP}, with significantly increasing training time--up to 34 times longer than direct fine-tuning.
By balancing the trade-off between exploration and exploitation, our ASAP achieves a 24.39\% gain on WMH compared to MTL, and a 13.18\% gain on vessel segmentation compared to GMS in Dice scores.
%Compared to direct fine-tuning, the NAL experiments on WMH result in a 7.31\% decrease while the MTL experiments on WMH see a 14.83\% decrease in Dice scores. This indicates that not all auxiliary information benefits the target, highlighting the importance of proper task scheduling.
% Among auxiliary-aided methods, NAL is the least costly, as it selects one auxiliary dataset before training, but its performance is unsatisfactory. Exploring all auxiliary datasets in the search space in MTL significantly increases training time, 34x compared to direct FT, yielding only a 6.15\% gain in segmentation performance.
\vspace{-0.4cm}
\subsubsection{Effectiveness of the efficient reward function}
\vspace{-0.2cm}
Compared to static dataset selection methods GMS and MTL, DAL offers a relatively better solution by dynamically selecting the auxiliary data based on gradient alignment.
However, our reward function, with consideration of $\mathcal{R}^{PM}$ term, the reward of positive for model convergence, serves as a more effective guide to enable the model to 
converge faster and 
deliver superior performance.
Meanwhile, our reward function only relies on the losses and gradients, which are intrinsic to the model, making it naturally training-efficient: it took 15.28 hours to adapt the MambaUNet for the target task liver segmentation, compared to 75.03 hours in MTL, 15.97 hours in DAL, and 15.13 hours in GMS, the a-priori dataset selection method, with the same input size of 80 x 240 x 240 and batch size of 4.
As per our policy, we update only the selected arm's reward during training, which keeps the additional complexity stable, irrespective of the size of the auxiliary data pool. 
% \input{table/ASAP_MRI}
% \input{table/ASAP_CT}

\vspace{-0.4cm}
\subsubsection{Investigating the active and sequential training dynamics.}
\vspace{-0.2cm}
A closer look at the selected auxiliary tasks illustrates the active and sequential adaptation training mechanism, visualized in Fig~\ref{selection}.
We show the selected auxiliary datasets at different turns for target tasks WMH segmentation on MRI images and liver segmentation on CT images.
% In line with the search budget of the active learning strategy, our policy greedily pulls the arm based on the estimated upper confidence bound instead of updating the rewards of all arms at each turn.
Interestingly, the policy does not initially sample the task experientially similar to the target. Instead, it sequentially selects the auxiliary dataset that progressively aligns with the target.
Despite lacking access to the source domain, we can still effectively narrow the domain discrepancy by following this step-by-step knowledge acquisition, demonstrating the strength of the active and sequential domain adaptation framework.
\input{figures/selection}