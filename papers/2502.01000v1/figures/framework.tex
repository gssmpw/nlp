\begin{figure}
    \vspace{-0.1cm}
    \centering
    \includegraphics[width=\linewidth]{figures/f.pdf}
    \vspace{-0.5cm}
    \caption{Illustration of our active and sequential domain adaptation (ASAP) framework.
    The agent defines the policy $\pi$ that determines which arm to pull. The environment includes the auxiliary data pool $\mathcal{D}_\mathcal{A}$, the target dataset $\mathcal{D}_\mathcal{T}$, and the model $f_\theta$. At each turn $t$, ASAP executes the four shown steps.
    }
    %At each turn t, the agent pulls the arm according to its policy. The environment samples the selected auxiliary dataset $\mathcal{D}_a$ to jointly train with the target data. The model calculates losses and gradients, and then updates the parameters. A reward $R_{a,t}$ is calculated based on gradients and losses. The agent updates the policy based on $R_{a,t}$.}
    \label{framework}
    \vspace{-0.2cm}
\end{figure}