[
  {
    "index": 0,
    "papers": [
      {
        "key": "liu2024alora",
        "author": "Liu, Zequan and Lyn, Jiawen and Zhu, Wei and Tian, Xing",
        "title": "ALoRA: Allocating Low-Rank Adaptation for Fine-tuning Large Language Models"
      },
      {
        "key": "tian2024fanlora",
        "author": "Tian, Aaron and Zhao, Yi and Yin, Congrui and Zhu, Wei and Tian, Xing and Ge, Yi",
        "title": "FanLoRA: Fantastic LoRAs and Where to Find Them in Large Language Model Fine-tuning"
      },
      {
        "key": "zheng2024sca",
        "author": "Zheng, Huanran and Zhu, Wei and Wang, Xiaoling",
        "title": "SCA: Selective Compression Attention for Efficiently Extending the Context Window of Large Language Models"
      },
      {
        "key": "zhang2024milora",
        "author": "Zhang, Jingfan and Zhao, Yi and Chen, Dan and Tian, Xing and Zheng, Huanran and Zhu, Wei",
        "title": "MiLoRA: Efficient Mixture of Low-Rank Adaptation for Large Language Models Fine-tuning"
      },
      {
        "key": "Ding2022DeltaTA",
        "author": "Ning Ding and Yujia Qin and Guang Yang and Fu Wei and Zonghan Yang and Yusheng Su and Shengding Hu and Yulin Chen and Chi-Min Chan and Weize Chen and Jing Yi and Weilin Zhao and Xiaozhi Wang and Zhiyuan Liu and Haitao Zheng and Jianfei Chen and Yang Liu and Jie Tang and Juan Li and Maosong Sun",
        "title": "Delta Tuning: A Comprehensive Study of Parameter Efficient Methods for Pre-trained Language Models"
      },
      {
        "key": "Zhang2023LearnedAA",
        "author": "Yuming Zhang and Peng Wang and Ming Tan and Wei-Guo Zhu",
        "title": "Learned Adapters Are Better Than Manually Designed Adapters"
      },
      {
        "key": "zhu-tan-2023-spt",
        "author": "Zhu, Wei  and\nTan, Ming",
        "title": "{SPT}: Learning to Selectively Insert Prompts for Better Prompt Tuning"
      },
      {
        "key": "Cui2023UltraFeedbackBL",
        "author": "Ganqu Cui and Lifan Yuan and Ning Ding and Guanming Yao and Wei Zhu and Yuan Ni and Guotong Xie and Zhiyuan Liu and Maosong Sun",
        "title": "UltraFeedback: Boosting Language Models with High-quality Feedback"
      },
      {
        "key": "zheng2024nat4at",
        "author": "Zheng, Huanran and Zhu, Wei and Wang, Xiaoling",
        "title": "NAT4AT: Using Non-Autoregressive Translation Makes Autoregressive Translation Faster and Better"
      },
      {
        "key": "zhu2023acf",
        "author": "Zhu, Wei and Wang, Peng and Wang, Xiaoling and Ni, Yuan and Xie, Guotong",
        "title": "ACF: aligned contrastive finetuning for language and vision tasks"
      },
      {
        "key": "gao2023f",
        "author": "Gao, Xiangxiang and Zhu, Wei and Gao, Jiasheng and Yin, Congrui",
        "title": "F-PABEE: Flexible-patience-based Early Exiting for Single-label and Multi-label text Classification Tasks"
      },
      {
        "key": "zuo-etal-2022-continually",
        "author": "Zuo, Yuhui  and\nZhu, Wei  and\nCai, Guoyong GUET",
        "title": "Continually Detection, Rapidly React: Unseen Rumors Detection Based on Continual Prompt-Tuning"
      },
      {
        "key": "zhang-etal-2022-pcee",
        "author": "Zhang, Zhen  and\nZhu, Wei  and\nZhang, Jinfan  and\nWang, Peng  and\nJin, Rize  and\nChung, Tae-Sun",
        "title": "{PCEE}-{BERT}: Accelerating {BERT} Inference via Patient and Confident Early Exiting"
      },
      {
        "key": "sun-etal-2022-simple",
        "author": "Sun, Tianxiang  and\nLiu, Xiangyang  and\nZhu, Wei  and\nGeng, Zhichao  and\nWu, Lingling  and\nHe, Yilong  and\nNi, Yuan  and\nXie, Guotong  and\nHuang, Xuanjing  and\nQiu, Xipeng",
        "title": "A Simple Hash-Based Early Exiting Approach For Language Understanding and Generation"
      },
      {
        "key": "zhu-etal-2021-gaml",
        "author": "Zhu, Wei  and\nWang, Xiaoling  and\nNi, Yuan  and\nXie, Guotong",
        "title": "{GAML}-{BERT}: Improving {BERT} Early Exiting by Gradient Aligned Mutual Learning"
      },
      {
        "key": "Zhu2021MVPBERTMP",
        "author": "Wei Zhu",
        "title": "MVP-BERT: Multi-Vocab Pre-training for Chinese BERT"
      },
      {
        "key": "li-etal-2019-pingan",
        "author": "Li, Xiepeng  and\nZhang, Zhexi  and\nZhu, Wei  and\nLi, Zheng  and\nNi, Yuan  and\nGao, Peng  and\nYan, Junchi  and\nXie, Guotong",
        "title": "Pingan Smart Health and {SJTU} at {COIN} - Shared Task: utilizing Pre-trained Language Models and Common-sense Knowledge in Machine Reading Tasks"
      },
      {
        "key": "zhu2019panlp",
        "author": "Zhu, Wei and Zhou, Xiaofeng and Wang, Keqiang and Luo, Xun and Li, Xiepeng and Ni, Yuan and Xie, Guotong",
        "title": "Panlp at mediqa 2019: Pre-trained language models, transfer learning and knowledge distillation"
      },
      {
        "key": "zhu2019dr",
        "author": "Zhu, Wei and Ni, Yuan and Xie, Guotong and Zhou, Xiaofeng and Chen, Cai",
        "title": "The DR-KGQA system for automatically answering medication related questions in Chinese"
      },
      {
        "key": "zhou2019analysis",
        "author": "Zhou, Xiaofeng and Ni, Yuan and Xie, Guotong and Zhu, Wei and Chen, Cai and Wang, Tianhao and Pan, Zhigang",
        "title": "Analysis of the health information needs of diabetics in China"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "houlsby2019parameter",
        "author": "Houlsby, Neil and Giurgiu, Andrei and Jastrzebski, Stanislaw and Morrone, Bruna and De Laroussilhe, Quentin and Gesmundo, Andrea and Attariyan, Mona and Gelly, Sylvain",
        "title": "Parameter-efficient transfer learning for NLP"
      },
      {
        "key": "Rckl2020AdapterDropOT",
        "author": "Andreas R{\\\"u}ckl{\\'e} and Gregor Geigle and Max Glockner and Tilman Beck and Jonas Pfeiffer and Nils Reimers and Iryna Gurevych",
        "title": "AdapterDrop: On the Efficiency of Adapters in Transformers"
      },
      {
        "key": "Zhang2023LearnedAA",
        "author": "Yuming Zhang and Peng Wang and Ming Tan and Wei-Guo Zhu",
        "title": "Learned Adapters Are Better Than Manually Designed Adapters"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "Li2021PrefixTuningOC",
        "author": "Xiang Lisa Li and Percy Liang",
        "title": "Prefix-Tuning: Optimizing Continuous Prompts for Generation"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "lester2021power",
        "author": "Lester, Brian and Al-Rfou, Rami and Constant, Noah",
        "title": "The power of scale for parameter-efficient prompt tuning"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "Liu2022PTuningPT",
        "author": "Xiao Liu and Kaixuan Ji and Yicheng Fu and Weng Lam Tam and Zhengxiao Du and Zhilin Yang and Jie Tang",
        "title": "P-Tuning: Prompt Tuning Can Be Comparable to Fine-tuning Across Scales and Tasks"
      },
      {
        "key": "zhu2024iapt",
        "author": "Zhu, Wei and Tian, Aaron Xuxiang and Yin, Congrui and Ni, Yuan and Wang, Xiaoling and Xie, Guotong",
        "title": "IAPT: Instruction-Aware Prompt Tuning for Large Language Models"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "Liu2022FewShotPF",
        "author": "Haokun Liu and Derek Tam and Mohammed Muqeeth and Jay Mohta and Tenghao Huang and Mohit Bansal and Colin Raffel",
        "title": "Few-Shot Parameter-Efficient Fine-Tuning is Better and Cheaper than In-Context Learning"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "BenZaken2021BitFitSP",
        "author": "Elad Ben-Zaken and Shauli Ravfogel and Yoav Goldberg",
        "title": "BitFit: Simple Parameter-efficient Fine-tuning for Transformer-based Masked Language-models"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "BenZaken2021BitFitSP",
        "author": "Elad Ben-Zaken and Shauli Ravfogel and Yoav Goldberg",
        "title": "BitFit: Simple Parameter-efficient Fine-tuning for Transformer-based Masked Language-models"
      },
      {
        "key": "guo-etal-2021-parameter",
        "author": "Guo, Demi  and\nRush, Alexander  and\nKim, Yoon",
        "title": "Parameter-Efficient Transfer Learning with Diff Pruning"
      },
      {
        "key": "zhao-etal-2020-masking",
        "author": "Zhao, Mengjie  and\nLin, Tao  and\nMi, Fei  and\nJaggi, Martin  and\nSch{\\\"u}tze, Hinrich",
        "title": "Masking as an Efficient Alternative to Finetuning for Pretrained Language Models"
      },
      {
        "key": "zheng2024chimera",
        "author": "Zheng, Huanran and Zhu, Wei and Wang, Xiaoling",
        "title": "Chimera Model of Candidate Soups for Non-Autoregressive Translation"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "hu2021lora",
        "author": "Hu, Edward J and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu",
        "title": "Lora: Low-rank adaptation of large language models"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "aghajanyan-etal-2021-intrinsic",
        "author": "Aghajanyan, Armen  and\nGupta, Sonal  and\nZettlemoyer, Luke",
        "title": "Intrinsic Dimensionality Explains the Effectiveness of Language Model Fine-Tuning"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "hu2021lora",
        "author": "Hu, Edward J and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu",
        "title": "Lora: Low-rank adaptation of large language models"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "2023arXiv230318223Z",
        "author": "{Zhao}, Wayne Xin and {Zhou}, Kun and {Li}, Junyi and {Tang}, Tianyi and {Wang}, Xiaolei and {Hou}, Yupeng and {Min}, Yingqian and {Zhang}, Beichen and {Zhang}, Junjie and {Dong}, Zican and {Du}, Yifan and {Yang}, Chen and {Chen}, Yushuo and {Chen}, Zhipeng and {Jiang}, Jinhao and {Ren}, Ruiyang and {Li}, Yifan and {Tang}, Xinyu and {Liu}, Zikang and {Liu}, Peiyu and {Nie}, Jian-Yun and {Wen}, Ji-Rong",
        "title": "{A Survey of Large Language Models}"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "alpaca",
        "author": "Rohan Taori and Ishaan Gulrajani and Tianyi Zhang and Yann Dubois and Xuechen Li and Carlos Guestrin and Percy Liang and Tatsunori B. Hashimoto ",
        "title": "Stanford Alpaca: An Instruction-following LLaMA model"
      },
      {
        "key": "2023arXiv230514314D",
        "author": "{Dettmers}, Tim and {Pagnoni}, Artidoro and {Holtzman}, Ari and {Zettlemoyer}, Luke",
        "title": "{QLoRA: Efficient Finetuning of Quantized LLMs}"
      }
    ]
  }
]