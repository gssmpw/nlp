@ARTICLE{2023arXiv230318223Z,
       author = {{Zhao}, Wayne Xin and {Zhou}, Kun and {Li}, Junyi and {Tang}, Tianyi and {Wang}, Xiaolei and {Hou}, Yupeng and {Min}, Yingqian and {Zhang}, Beichen and {Zhang}, Junjie and {Dong}, Zican and {Du}, Yifan and {Yang}, Chen and {Chen}, Yushuo and {Chen}, Zhipeng and {Jiang}, Jinhao and {Ren}, Ruiyang and {Li}, Yifan and {Tang}, Xinyu and {Liu}, Zikang and {Liu}, Peiyu and {Nie}, Jian-Yun and {Wen}, Ji-Rong},
        title = "{A Survey of Large Language Models}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
         year = 2023,
        month = mar,
          eid = {arXiv:2303.18223},
        pages = {arXiv:2303.18223},
          doi = {10.48550/arXiv.2303.18223},
archivePrefix = {arXiv},
       eprint = {2303.18223},
 primaryClass = {cs.CL},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2023arXiv230318223Z},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2023arXiv230514314D,
       author = {{Dettmers}, Tim and {Pagnoni}, Artidoro and {Holtzman}, Ari and {Zettlemoyer}, Luke},
        title = "{QLoRA: Efficient Finetuning of Quantized LLMs}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Machine Learning},
         year = 2023,
        month = may,
          eid = {arXiv:2305.14314},
        pages = {arXiv:2305.14314},
          doi = {10.48550/arXiv.2305.14314},
archivePrefix = {arXiv},
       eprint = {2305.14314},
 primaryClass = {cs.LG},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2023arXiv230514314D},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@article{BenZaken2021BitFitSP,
  title={BitFit: Simple Parameter-efficient Fine-tuning for Transformer-based Masked Language-models},
  author={Elad Ben-Zaken and Shauli Ravfogel and Yoav Goldberg},
  journal={ArXiv},
  year={2021},
  volume={abs/2106.10199}
}

@article{Cui2023UltraFeedbackBL,
  title={UltraFeedback: Boosting Language Models with High-quality Feedback},
  author={Ganqu Cui and Lifan Yuan and Ning Ding and Guanming Yao and Wei Zhu and Yuan Ni and Guotong Xie and Zhiyuan Liu and Maosong Sun},
  journal={ArXiv},
  year={2023},
  volume={abs/2310.01377},
  url={https://api.semanticscholar.org/CorpusID:263605623}
}

@article{Ding2022DeltaTA,
  title={Delta Tuning: A Comprehensive Study of Parameter Efficient Methods for Pre-trained Language Models},
  author={Ning Ding and Yujia Qin and Guang Yang and Fu Wei and Zonghan Yang and Yusheng Su and Shengding Hu and Yulin Chen and Chi-Min Chan and Weize Chen and Jing Yi and Weilin Zhao and Xiaozhi Wang and Zhiyuan Liu and Haitao Zheng and Jianfei Chen and Yang Liu and Jie Tang and Juan Li and Maosong Sun},
  journal={ArXiv},
  year={2022},
  volume={abs/2203.06904}
}

@article{Li2021PrefixTuningOC,
  title={Prefix-Tuning: Optimizing Continuous Prompts for Generation},
  author={Xiang Lisa Li and Percy Liang},
  journal={Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)},
  year={2021},
  volume={abs/2101.00190}
}

@article{Liu2022FewShotPF,
  title={Few-Shot Parameter-Efficient Fine-Tuning is Better and Cheaper than In-Context Learning},
  author={Haokun Liu and Derek Tam and Mohammed Muqeeth and Jay Mohta and Tenghao Huang and Mohit Bansal and Colin Raffel},
  journal={ArXiv},
  year={2022},
  volume={abs/2205.05638},
  url={https://api.semanticscholar.org/CorpusID:248693283}
}

@inproceedings{Liu2022PTuningPT,
  title={P-Tuning: Prompt Tuning Can Be Comparable to Fine-tuning Across Scales and Tasks},
  author={Xiao Liu and Kaixuan Ji and Yicheng Fu and Weng Lam Tam and Zhengxiao Du and Zhilin Yang and Jie Tang},
  booktitle={Annual Meeting of the Association for Computational Linguistics},
  year={2022}
}

@inproceedings{Rckl2020AdapterDropOT,
  title={AdapterDrop: On the Efficiency of Adapters in Transformers},
  author={Andreas R{\"u}ckl{\'e} and Gregor Geigle and Max Glockner and Tilman Beck and Jonas Pfeiffer and Nils Reimers and Iryna Gurevych},
  booktitle={Conference on Empirical Methods in Natural Language Processing},
  year={2020}
}

@inproceedings{Zhang2023LearnedAA,
  title={Learned Adapters Are Better Than Manually Designed Adapters},
  author={Yuming Zhang and Peng Wang and Ming Tan and Wei-Guo Zhu},
  booktitle={Annual Meeting of the Association for Computational Linguistics},
  year={2023},
  url={https://api.semanticscholar.org/CorpusID:259858833}
}

@inproceedings{Zhu2021MVPBERTMP,
  title={MVP-BERT: Multi-Vocab Pre-training for Chinese BERT},
  author={Wei Zhu},
  booktitle={Annual Meeting of the Association for Computational Linguistics},
  year={2021},
  url={https://api.semanticscholar.org/CorpusID:237331564}
}

@inproceedings{aghajanyan-etal-2021-intrinsic,
    title = "Intrinsic Dimensionality Explains the Effectiveness of Language Model Fine-Tuning",
    author = "Aghajanyan, Armen  and
      Gupta, Sonal  and
      Zettlemoyer, Luke",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-long.568",
    doi = "10.18653/v1/2021.acl-long.568",
    pages = "7319--7328",
    abstract = "Although pretrained language models can be fine-tuned to produce state-of-the-art results for a very wide range of language understanding tasks, the dynamics of this process are not well understood, especially in the low data regime. Why can we use relatively vanilla gradient descent algorithms (e.g., without strong regularization) to tune a model with hundreds of millions of parameters on datasets with only hundreds or thousands of labeled examples? In this paper, we argue that analyzing fine-tuning through the lens of intrinsic dimension provides us with empirical and theoretical intuitions to explain this remarkable phenomenon. We empirically show that common pre-trained models have a very low intrinsic dimension; in other words, there exists a low dimension reparameterization that is as effective for fine-tuning as the full parameter space. For example, by optimizing only 200 trainable parameters randomly projected back into the full space, we can tune a RoBERTa model to achieve 90{\%} of the full parameter performance levels on MRPC. Furthermore, we empirically show that pre-training implicitly minimizes intrinsic dimension and, perhaps surprisingly, larger models tend to have lower intrinsic dimension after a fixed number of pre-training updates, at least in part explaining their extreme effectiveness. Lastly, we connect intrinsic dimensionality with low dimensional task representations and compression based generalization bounds to provide intrinsic-dimension-based generalization bounds that are independent of the full parameter count.",
}

@misc{alpaca,
  author = {Rohan Taori and Ishaan Gulrajani and Tianyi Zhang and Yann Dubois and Xuechen Li and Carlos Guestrin and Percy Liang and Tatsunori B. Hashimoto },
  title = {Stanford Alpaca: An Instruction-following LLaMA model},
  year = {2023},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/tatsu-lab/stanford_alpaca}},
}

@inproceedings{gao2023f,
  title={F-PABEE: Flexible-patience-based Early Exiting for Single-label and Multi-label text Classification Tasks},
  author={Gao, Xiangxiang and Zhu, Wei and Gao, Jiasheng and Yin, Congrui},
  booktitle={ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={1--5},
  year={2023},
  organization={IEEE}
}

@inproceedings{guo-etal-2021-parameter,
    title = "Parameter-Efficient Transfer Learning with Diff Pruning",
    author = "Guo, Demi  and
      Rush, Alexander  and
      Kim, Yoon",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-long.378",
    doi = "10.18653/v1/2021.acl-long.378",
    pages = "4884--4896",
    abstract = "The large size of pretrained networks makes them difficult to deploy for multiple tasks in storage-constrained settings. Diff pruning enables parameter-efficient transfer learning that scales well with new tasks. The approach learns a task-specific {``}diff{''} vector that extends the original pretrained parameters. This diff vector is adaptively pruned during training with a differentiable approximation to the L0-norm penalty to encourage sparsity. As the number of tasks increases, diff pruning remains parameter-efficient, as it requires storing only a small diff vector for each task. Since it does not require access to all tasks during training, it is attractive in on-device deployment settings where tasks arrive in stream or even from different providers. Diff pruning can match the performance of finetuned baselines on the GLUE benchmark while only modifying 0.5{\%} of the pretrained model{'}s parameters per task and scales favorably in comparison to popular pruning approaches.",
}

@inproceedings{houlsby2019parameter,
  title={Parameter-efficient transfer learning for NLP},
  author={Houlsby, Neil and Giurgiu, Andrei and Jastrzebski, Stanislaw and Morrone, Bruna and De Laroussilhe, Quentin and Gesmundo, Andrea and Attariyan, Mona and Gelly, Sylvain},
  booktitle={International Conference on Machine Learning},
  pages={2790--2799},
  year={2019},
  organization={PMLR}
}

@article{hu2021lora,
  title={Lora: Low-rank adaptation of large language models},
  author={Hu, Edward J and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
  journal={arXiv preprint arXiv:2106.09685},
  year={2021}
}

@article{lester2021power,
  title={The power of scale for parameter-efficient prompt tuning},
  author={Lester, Brian and Al-Rfou, Rami and Constant, Noah},
  journal={arXiv preprint arXiv:2104.08691},
  year={2021}
}

@inproceedings{li-etal-2019-pingan,
    title = "Pingan Smart Health and {SJTU} at {COIN} - Shared Task: utilizing Pre-trained Language Models and Common-sense Knowledge in Machine Reading Tasks",
    author = "Li, Xiepeng  and
      Zhang, Zhexi  and
      Zhu, Wei  and
      Li, Zheng  and
      Ni, Yuan  and
      Gao, Peng  and
      Yan, Junchi  and
      Xie, Guotong",
    booktitle = "Proceedings of the First Workshop on Commonsense Inference in Natural Language Processing",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-6011",
    doi = "10.18653/v1/D19-6011",
    pages = "93--98",
    abstract = "To solve the shared tasks of COIN: COmmonsense INference in Natural Language Processing) Workshop in , we need explore the impact of knowledge representation in modeling commonsense knowledge to boost performance of machine reading comprehension beyond simple text matching. There are two approaches to represent knowledge in the low-dimensional space. The first is to leverage large-scale unsupervised text corpus to train fixed or contextual language representations. The second approach is to explicitly express knowledge into a knowledge graph (KG), and then fit a model to represent the facts in the KG. We have experimented both (a) improving the fine-tuning of pre-trained language models on a task with a small dataset size, by leveraging datasets of similar tasks; and (b) incorporating the distributional representations of a KG onto the representations of pre-trained language models, via simply concatenation or multi-head attention. We find out that: (a) for task 1, first fine-tuning on larger datasets like RACE (Lai et al., 2017) and SWAG (Zellersetal.,2018), and then fine-tuning on the target task improve the performance significantly; (b) for task 2, we find out the incorporating a KG of commonsense knowledge, WordNet (Miller, 1995) into the Bert model (Devlin et al., 2018) is helpful, however, it will hurts the performace of XLNET (Yangetal.,2019), a more powerful pre-trained model. Our approaches achieve the state-of-the-art results on both shared task{'}s official test data, outperforming all the other submissions.",
}

@inproceedings{liu2024alora,
  title={ALoRA: Allocating Low-Rank Adaptation for Fine-tuning Large Language Models},
  author={Liu, Zequan and Lyn, Jiawen and Zhu, Wei and Tian, Xing},
  booktitle={Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)},
  pages={622--641},
  year={2024}
}

@inproceedings{sun-etal-2022-simple,
    title = "A Simple Hash-Based Early Exiting Approach For Language Understanding and Generation",
    author = "Sun, Tianxiang  and
      Liu, Xiangyang  and
      Zhu, Wei  and
      Geng, Zhichao  and
      Wu, Lingling  and
      He, Yilong  and
      Ni, Yuan  and
      Xie, Guotong  and
      Huang, Xuanjing  and
      Qiu, Xipeng",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2022",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.findings-acl.189",
    doi = "10.18653/v1/2022.findings-acl.189",
    pages = "2409--2421",
    abstract = "Early exiting allows instances to exit at different layers according to the estimation of difficulty.Previous works usually adopt heuristic metrics such as the entropy of internal outputs to measure instance difficulty, which suffers from generalization and threshold-tuning. In contrast, learning to exit, or learning to predict instance difficulty is a more appealing way. Though some effort has been devoted to employing such {``}learn-to-exit{''} modules, it is still unknown whether and how well the instance difficulty can be learned. As a response, we first conduct experiments on the learnability of instance difficulty, which demonstrates that modern neural models perform poorly on predicting instance difficulty. Based on this observation, we propose a simple-yet-effective Hash-based Early Exiting approach HashEE) that replaces the learn-to-exit modules with hash functions to assign each token to a fixed exiting layer. Different from previous methods, HashEE requires no internal classifiers nor extra parameters, and therefore is more efficient.HashEE can be used in various tasks (including language understanding and generation) and model architectures such as seq2seq models. Experimental results on classification, regression, and generation tasks demonstrate that HashEE can achieve higher performance with fewer FLOPs and inference time compared with previous state-of-the-art early exiting methods.",
}

@inproceedings{tian2024fanlora,
  title={FanLoRA: Fantastic LoRAs and Where to Find Them in Large Language Model Fine-tuning},
  author={Tian, Aaron and Zhao, Yi and Yin, Congrui and Zhu, Wei and Tian, Xing and Ge, Yi},
  booktitle={Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing: Industry Track},
  pages={515--528},
  year={2024}
}

@inproceedings{zhang-etal-2022-pcee,
    title = "{PCEE}-{BERT}: Accelerating {BERT} Inference via Patient and Confident Early Exiting",
    author = "Zhang, Zhen  and
      Zhu, Wei  and
      Zhang, Jinfan  and
      Wang, Peng  and
      Jin, Rize  and
      Chung, Tae-Sun",
    booktitle = "Findings of the Association for Computational Linguistics: NAACL 2022",
    month = jul,
    year = "2022",
    address = "Seattle, United States",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.findings-naacl.25",
    doi = "10.18653/v1/2022.findings-naacl.25",
    pages = "327--338",
    abstract = "BERT and other pretrained language models (PLMs) are ubiquitous in modern NLP. Even though PLMs are the state-of-the-art (SOTA) models for almost every NLP task (CITATION), the significant latency during inference prohibits wider industrial usage. In this work, we propose Patient and Confident Early Exiting BERT (PCEE-BERT), an off-the-shelf sample-dependent early exiting method that can work with different PLMs and can also work along with popular model compression methods. With a multi-exit BERT as the backbone model, PCEE-BERT will make the early exiting decision if enough numbers (patience parameter) of consecutive intermediate layers are confident about their predictions. The entropy value measures the confidence level of an intermediate layer{'}s prediction. Experiments on the GLUE benchmark demonstrate that our method outperforms previous SOTA early exiting methods. Ablation studies show that: (a) our method performs consistently well on other PLMs, such as ALBERT and TinyBERT; (b) PCEE-BERT can achieve different speed-up ratios by adjusting the patience parameter and the confidence threshold. The code for PCEE-BERT can be found at \url{https://github.com/michael-wzhu/PCEE-BERT}.",
}

@article{zhang2024milora,
  title={MiLoRA: Efficient Mixture of Low-Rank Adaptation for Large Language Models Fine-tuning},
  author={Zhang, Jingfan and Zhao, Yi and Chen, Dan and Tian, Xing and Zheng, Huanran and Zhu, Wei},
  journal={arXiv preprint arXiv:2410.18035},
  year={2024}
}

@inproceedings{zhao-etal-2020-masking,
    title = "Masking as an Efficient Alternative to Finetuning for Pretrained Language Models",
    author = {Zhao, Mengjie  and
      Lin, Tao  and
      Mi, Fei  and
      Jaggi, Martin  and
      Sch{\"u}tze, Hinrich},
    editor = "Webber, Bonnie  and
      Cohn, Trevor  and
      He, Yulan  and
      Liu, Yang",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-main.174",
    doi = "10.18653/v1/2020.emnlp-main.174",
    pages = "2226--2241",
    abstract = "We present an efficient method of utilizing pretrained language models, where we learn selective binary masks for pretrained weights in lieu of modifying them through finetuning. Extensive evaluations of masking BERT, RoBERTa, and DistilBERT on eleven diverse NLP tasks show that our masking scheme yields performance comparable to finetuning, yet has a much smaller memory footprint when several tasks need to be inferred. Intrinsic evaluations show that representations computed by our binary masked language models encode information necessary for solving downstream tasks. Analyzing the loss landscape, we show that masking and finetuning produce models that reside in minima that can be connected by a line segment with nearly constant test accuracy. This confirms that masking can be utilized as an efficient alternative to finetuning.",
}

@inproceedings{zheng2024chimera,
  title={Chimera Model of Candidate Soups for Non-Autoregressive Translation},
  author={Zheng, Huanran and Zhu, Wei and Wang, Xiaoling},
  booktitle={International Conference on Database Systems for Advanced Applications},
  pages={416--425},
  year={2024},
  organization={Springer}
}

@inproceedings{zheng2024nat4at,
  title={NAT4AT: Using Non-Autoregressive Translation Makes Autoregressive Translation Faster and Better},
  author={Zheng, Huanran and Zhu, Wei and Wang, Xiaoling},
  booktitle={Proceedings of the ACM on Web Conference 2024},
  pages={4181--4192},
  year={2024}
}

@inproceedings{zheng2024sca,
  title={SCA: Selective Compression Attention for Efficiently Extending the Context Window of Large Language Models},
  author={Zheng, Huanran and Zhu, Wei and Wang, Xiaoling},
  booktitle={Findings of the Association for Computational Linguistics: EMNLP 2024},
  pages={6166--6178},
  year={2024}
}

@incollection{zhou2019analysis,
  title={Analysis of the health information needs of diabetics in China},
  author={Zhou, Xiaofeng and Ni, Yuan and Xie, Guotong and Zhu, Wei and Chen, Cai and Wang, Tianhao and Pan, Zhigang},
  booktitle={MEDINFO 2019: Health and Wellbeing e-Networks for All},
  pages={487--491},
  year={2019},
  publisher={IOS Press}
}

@inproceedings{zhu-etal-2021-gaml,
	title = "{GAML}-{BERT}: Improving {BERT} Early Exiting by Gradient Aligned Mutual Learning",
	author = "Zhu, Wei  and
	Wang, Xiaoling  and
	Ni, Yuan  and
	Xie, Guotong",
	booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
	month = nov,
	year = "2021",
	address = "Online and Punta Cana, Dominican Republic",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2021.emnlp-main.242",
	pages = "3033--3044",
	abstract = "In this work, we propose a novel framework, Gradient Aligned Mutual Learning BERT (GAML-BERT), for improving the early exiting of BERT. GAML-BERT{'}s contributions are two-fold. We conduct a set of pilot experiments, which shows that mutual knowledge distillation between a shallow exit and a deep exit leads to better performances for both. From this observation, we use mutual learning to improve BERT{'}s early exiting performances, that is, we ask each exit of a multi-exit BERT to distill knowledge from each other. Second, we propose GA, a novel training method that aligns the gradients from knowledge distillation to cross-entropy losses. Extensive experiments are conducted on the GLUE benchmark, which shows that our GAML-BERT can significantly outperform the state-of-the-art (SOTA) BERT early exiting methods.",
}

@inproceedings{zhu-tan-2023-spt,
    title = "{SPT}: Learning to Selectively Insert Prompts for Better Prompt Tuning",
    author = "Zhu, Wei  and
      Tan, Ming",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.emnlp-main.727",
    pages = "11862--11878",
    abstract = "Prompt tuning prepends a soft prompt to the input embeddings or hidden states and only optimizes the prompt to adapt pretrained models (PTMs) to downstream tasks. The previous work manually selects prompt layers which are far from optimal and failed to exploit the potential of prompt tuning. In this work, we propose a novel framework, Selective Prompt Tuning (SPT), that learns to select the proper prompt layers by inserting a prompt controlled by a learnable probabilistic gate at each intermediate layer. We further propose a novel bi-level optimization framework, SPT-DARTS, that can better optimize the learnable gates and improve the final prompt tuning performances of the learned prompt layer settings. We conduct extensive experiments with ten benchmark datasets under the full-data and few-shot scenarios. The results demonstrate that our SPT framework can perform better than the previous state-of-the-art PETuning baselines with comparable or fewer tunable parameters.",
}

@inproceedings{zhu2019dr,
  title={The DR-KGQA system for automatically answering medication related questions in Chinese},
  author={Zhu, Wei and Ni, Yuan and Xie, Guotong and Zhou, Xiaofeng and Chen, Cai},
  booktitle={2019 IEEE International Conference on Healthcare Informatics (ICHI)},
  pages={1--6},
  year={2019},
  organization={IEEE}
}

@inproceedings{zhu2019panlp,
  title={Panlp at mediqa 2019: Pre-trained language models, transfer learning and knowledge distillation},
  author={Zhu, Wei and Zhou, Xiaofeng and Wang, Keqiang and Luo, Xun and Li, Xiepeng and Ni, Yuan and Xie, Guotong},
  booktitle={Proceedings of the 18th BioNLP Workshop and Shared Task},
  pages={380--388},
  year={2019}
}

@inproceedings{zhu2023acf,
  title={ACF: aligned contrastive finetuning for language and vision tasks},
  author={Zhu, Wei and Wang, Peng and Wang, Xiaoling and Ni, Yuan and Xie, Guotong},
  booktitle={ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={1--5},
  year={2023},
  organization={IEEE}
}

@article{zhu2024iapt,
  title={IAPT: Instruction-Aware Prompt Tuning for Large Language Models},
  author={Zhu, Wei and Tian, Aaron Xuxiang and Yin, Congrui and Ni, Yuan and Wang, Xiaoling and Xie, Guotong},
  journal={arXiv preprint arXiv:2405.18203},
  year={2024}
}

@inproceedings{zuo-etal-2022-continually,
    title = "Continually Detection, Rapidly React: Unseen Rumors Detection Based on Continual Prompt-Tuning",
    author = "Zuo, Yuhui  and
      Zhu, Wei  and
      Cai, Guoyong GUET",
    booktitle = "Proceedings of the 29th International Conference on Computational Linguistics",
    month = oct,
    year = "2022",
    address = "Gyeongju, Republic of Korea",
    publisher = "International Committee on Computational Linguistics",
    url = "https://aclanthology.org/2022.coling-1.268",
    pages = "3029--3041",
    abstract = "Since open social platforms allow for a large and continuous flow of unverified information, rumors can emerge unexpectedly and spread quickly. However, existing rumor detection (RD) models often assume the same training and testing distributions and can not cope with the continuously changing social network environment. This paper proposed a Continual Prompt-Tuning RD (CPT-RD) framework, which avoids catastrophic forgetting (CF) of upstream tasks during sequential task learning and enables bidirectional knowledge transfer between domain tasks. Specifically, we propose the following strategies: (a) Our design explicitly decouples shared and domain-specific knowledge, thus reducing the interference among different domains during optimization; (b) Several technologies aim to transfer knowledge of upstream tasks to deal with emergencies; (c) A task-conditioned prompt-wise hypernetwork (TPHNet) is used to consolidate past domains. In addition, CPT-RD avoids CF without the necessity of a rehearsal buffer. Finally, CPT-RD is evaluated on English and Chinese RD datasets and is effective and efficient compared to prior state-of-the-art methods.",
}

