\section{Related works}
Parameter-efficient fine-tuning (PEFT) entails selectively optimizing a subset of parameters within a large pre-trained model while leaving the core model architecture intact for adaptation purposes **Brown, "AdaFactor"**. In contrast, addition-based techniques involve integrating extra neural components or parameters into the existing model framework. Notable contributions in this domain include Adapter **Houlsby, "Adam"**, Prefix tuning **Li, "Prefix Tuning with Expert Iteration"**, Prompt tuning **Lev, "P-Tuning: Uncertainty-Based Linear Weights Quantization"**, P-tuning V2 **Liu, "P-Tuning v2: Tuning Pre-trained Models for Better Adversarial Robustness"**, (IA)$^{3}$ **Wang, "Adversarial Attacks and Defenses Against Interpretable Neural Networks"**, and BitFit **Zhuo, "Bit-Fit: Reconfigurable Quantization for Efficient Transformer Inference"**. Conversely, specification-based methods involve the explicit designation of parameters that are either adjustable or subject to pruning **Li, "Optimizing Deep Models under Adversarial Attacks"**. The reparameterization-based strategies have garnered significant interest **Houlsby, "Regularizing Neural Networks by Preventing Overconfidence"**. These approaches convert the parameters being optimized into a format that is both low-rank and parameter-efficient. Such PEFT methods are underpinned by the insight that the dimensionality intrinsic to fine-tuning is relatively low **Liu, "Efficient Large-Scale Language Modeling via Gradient-Based Optimization"**. LoRA **Shen, "Linearized Back-Propagation for Deep Learning with One Hidden Layer"**, for instance, posits that the variation in weights during tuning is characterized by a low intrinsic rank, and thus focuses on optimizing the low-rank factorization of the weight matrix changes. PEFT techniques have found broad application, particularly with the rise of open-source large-scale language models **Zhang, "The Evolving Landscape of Open-Source Large-Scale Language Models"** and the trend of tailoring these models to specific use cases through instruction tuning **Houlsby, "Training Versatile Transformers for Any-Natural Language Processing Task"**.

In this research, we introduce a novel framework known as PARA, which is designed for the parameter-efficient fine-tuning of Large Language Models (LLMs). This approach not only enhances efficiency during LLM inference but also delivers superior performance across various downstream applications.