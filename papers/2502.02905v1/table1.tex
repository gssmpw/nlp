\definecolor{rulecolor}{RGB}{0,71,171}
\definecolor{tableheadcolor}{gray}{0.92}
% Following is taken from Werner: http://tex.stackexchange.com/a/33761/3061
% and modified for my needs
%
% Command \topline consists of a (slightly modified)
% \toprule followed by a \heavyrule rule of colour tableheadcolor
% (hence, 2 separate rules)
\definecolor{rulecolor}{RGB}{0,71,171}
\definecolor{tableheadcolor}{gray}{0.92}

% Define custom rules and styles
\newcommand{\topline}{ %
        \arrayrulecolor{rulecolor}\specialrule{0.1em}{\abovetopsep}{0pt}%
        \arrayrulecolor{tableheadcolor}\specialrule{\belowrulesep}{0pt}{0pt}%
        \arrayrulecolor{rulecolor}}
\newcommand{\midtopline}{ %
        \arrayrulecolor{tableheadcolor}\specialrule{\aboverulesep}{0pt}{0pt}%
        \arrayrulecolor{rulecolor}\specialrule{\lightrulewidth}{0pt}{0pt}%
        \arrayrulecolor{white}\specialrule{\belowrulesep}{0pt}{0pt}%
        \arrayrulecolor{rulecolor}}
\newcommand{\bottomline}{ %
       \addlinespace[-0.8ex]%      
      \arrayrulecolor{white}\specialrule{\aboverulesep}{0pt}{0pt}%
        \arrayrulecolor{rulecolor} %
        \specialrule{\heavyrulewidth}{0pt}{\belowbottomsep}}%

% Custom section header command
\newcommand{\topmidheader}[2]{%
    \addlinespace[0.6ex]% Adds space before the header row
    \multicolumn{#1}{c}{\textsc{#2}}\\%
    \addlinespace[1ex]% Adds space after the header row
}
\newcommand{\midheader}[2]{%
    \addlinespace[-0.6ex]%
        \midrule\topmidheader{#1}{#2}}
\setlist[itemize]{noitemsep, topsep=0pt, partopsep=0pt, leftmargin=1em}
\renewcommand{\arraystretch}{0.8}

\begin{table}[!ht]
    \centering
    \begin{tabular}[t]{p{3.5cm} c p{3.5cm} p{6cm}} % Added 'p{6cm}' for wrapping text
        \topline
        \rowcolor{tableheadcolor}
        \textbf{Method} & \textbf{Original year} & \textbf{Essence} & \textbf{Pros \& Cons} \\
        \midtopline
        \topmidheader{4}{Evolutionary algorithms}
        Genetic algorithm (GA)  & 1973  & Evolutionary search based on natural selection principles &
        \vspace{-0.6em}\begin{itemize}
            \item Intuitive; robust to noisy and multi-modal problems
            \item May converge prematurely to suboptimal solutions
        \end{itemize}
        \\
        Particle swarm optimization (PSO)          & 1995 & Swarm intelligence inspired by birds' flocking behavior &\vspace{-0.6em}\begin{itemize}
            \item Efficient for continuous optimization problems
            \item Heavily depends on parameter tuning
        \end{itemize}\\
        Monte Carlo tree search (MCTS)       & 2006  & Probabilistic decision-making using random sampling in trees &\vspace{-0.6em}\begin{itemize}
            \item Balances exploration and exploitation
            \item Computationally intensive for large problem spaces
        \end{itemize}\\
        \midheader{4}{Adaptive and interactive learning methods}
        Bayesian optimization (BO)         & 1978   & Sequential inference for global optimization of black-box functions&\vspace{-0.6em}\begin{itemize}
            \item Adaptive and data-efficient
            \item Computationally intensive; heavily rely on choices on prior
        \end{itemize}\\
        Deep reinforcement learning (DQN, etc.)  & 2013 & Neural networks approximating reward functions &\vspace{-0.6em}\begin{itemize}
            \item Learns complex policies from raw data
            \item Inefficient training; hard hyperparameter tuning
        \end{itemize}\\
        \midheader{4}{Deep generative models}
        Variational autoencoder (VAE) & 2013      & Probabilistic latent space learning via variational inference&\vspace{-0.6em}\begin{itemize}
            \item Effective for generative modeling
            \item Variational assumption limits expressiveness
        \end{itemize}\\
        Generative adversarial network (GAN)  & 2014      & Adversarial learning between generator and discriminator&\vspace{-0.6em}\begin{itemize}
            \item Generates highly realistic data
            \item Training is unstable; prone to mode collapse.
        \end{itemize}\\
        Large language model (LLM)  & 2017      & Transformer-based pretraining for language understanding&\vspace{-0.6em}\begin{itemize}
            \item Exceptional in natural language tasks and versatile domains
            \item Requires enormous computational resources; biased output
        \end{itemize}\\
        Diffusion model  & 2020      & Progressive noise removal to generate data&\vspace{-0.6em}\begin{itemize}
            \item High-quality and stable outputs
            \item Slow generation process; requires careful tuning.
        \end{itemize}\\
        \bottomline
    \end{tabular}
    \caption{An incomplete list of classical algorithms and AI-based computational methods for materials inverse design.}
    \label{tab1}
\end{table}