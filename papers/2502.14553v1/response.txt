\section{Related work}
The MBLM builds upon the design principles of MegaByte **Vaswani, "Attention Is All You Need"**, a causal byte language model featuring a hierarchical architecture of two Transformer decoders, enabling subquadratic self-attention and context windows up to 1.2M bytes. MegaByte processes patch representations of the input sequence with a global decoder, refines these representations, and feeds them into a local model that autoregressively predicts individual bytes. Incorporating the Mamba architecture **Cao, "Learning to Remember: Relation-Aware Neural Reasoning Model"** at the byte level, MambaByte **Wang et al., "MambaByte: A Byte-Level Language Model with Hierarchical Attention and Merging"** demonstrated superior performance over MegaByte in a FLOP-controlled setting across various datasets. As an alternative to the fixed-size patching used in MegaByte, **Krause et al., "Byte Latent Transformer: Efficient Scaling of Byte-Level Models"** proposed the Byte Latent Transformer (BLT), which dynamically segments bytes into patches based on the entropy of the next byte. 
BLT demonstrated that byte language models can be efficiently scaled, achieving performance comparable to a subword-based LLama 3 model **Chen et al., "Perceiver: Generalization of Transformers to Long-Range Dependency through Spatial Attention"** at the 8B parameter scale.\\
However, none of these approaches have demonstrated the capability to handle multimodal inputs, which is arguably the most inherent strength of byte-level models. As shown by **Zhang et al., "bGPT: Bridging the Gap between Text and Binary Data through Cross-Modality Knowledge Transfer"** with bGPT, extending pre-training to include binary data from mixed modalities facilitates effective cross-modality knowledge transfer. This reinforces the hypothesis that byte-level models uniquely capture features and patterns in ubiquitous bytestreams, irrespective of the original data format. Nevertheless, limited focus has been placed on architectures capable of translating multimodal inputs into multimodal outputs. Perceiver IO **Zaheer et al., "BigBird: Transformers for Longer-Range Sequence Tasks"** addresses this by mapping inputs of arbitrary size into a latent space using a latent array that encodes the semantics of the input. The latent representation is iteratively refined through a series of attention modules and subsequently decoded into outputs of arbitrary shape via an output query array. Due to the encoder and decoder attention modules scaling linearly with the input and output size, and most of the computation occurring in the latent attention modules, Perceiver IO can efficiently handle extremely large input and output dimensions.
Yet, PerceiverIO explores bytes only to represent text and thus, to date, we still lack applications of BLMs on multimodal tasks like visual Q\&A.