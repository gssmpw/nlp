\section{Related Work}
\paragraph{Interpretability Methods in Language Models.} Interpretability paradigms for AI decision-making range from black-box techniques, which focus on input-output relationships, to internal analyses that delve into model mechanics \citep{bereska2024mechanistic}. Behavioral interpretability \citep{warstadt2020blimp,covert2021explaining,casalicchio2018visualizing} treats models as black boxes, examining robustness and variable dependencies, while attributional interpretability \citep{sundararajan2017axiomatic,smilkov2017smoothgrad,shrikumar2017learning} traces outputs back to individual input contributions. Concept-based interpretability \citep{belinkov2022probing,burns2023discovering,zou2023representation,yang2024makes,huimproving} explores high-level concepts within models' learned representations. In contrast, mechanistic interpretability \citep{bereska2024mechanistic} adopts a bottom-up approach, analyzing neurons, layers, and circuits to uncover causal relationships and precise computations, offering a detailed understanding of the model's internal operations.

\paragraph{Circuit Analysis.} Neural networks can be conceptualized as computational graphs, where circuits of linked features and weights serve as fundamental computational units \citep{bereska2024mechanistic}. Recent research has focused on dissecting models into interpretable circuits. Automated Circuit Discovery (ACDC) \citep{conmy2023towards} automates a large portion of the mechanistic interpretability workflow, but it is inefficient due to its recursive nature. \citet{syed2023attribution} introduced Edge Attribution Patching (EAP) to identify circuits for specific tasks, while \citet{hanna2024have} introduced EAP with integrated gradients(EAP-IG), which improves upon EAP by identifying more faithful circuits. Circuit analysis leverages key task-relevant parameters \citep{bereska2024mechanistic} and feature connections \citep{he2024dictionary} within the network to capture core computational processes and attribute outputs to specific components \citep{miller2024transformer}, bypassing the need to analyze the entire model. This approach maintains efficiency and scalability, offering a practical alternative for understanding model behavior.

\paragraph{Influence Function.} The influence function, initially a staple in robust statistics \citep{cook2000detection,cook1980characterizations}, has seen extensive adoption within machine learning since \citet{koh2017understanding} introduced it to the field. Its versatility spans various applications, including detecting mislabeled data, interpreting models, addressing model bias, and facilitating machine unlearning tasks. Notable works in machine unlearning encompass unlearning features and labels \citep{warnecke2021machine}, minimax unlearning \citep{liu2024certified}, forgetting a subset of image data for training deep neural networks \citep{golatkar2020eternal,golatkar2021mixed}, graph unlearning involving nodes, edges, and features. Recent advancements, such as the LiSSA method \citep{agarwal2017second,kwon2023datainf} and kNN-based techniques \citep{guo2021fastif}, have been proposed to enhance computational efficiency. Besides, various studies have applied influence functions to interpret models across different domains, including natural language processing \citep{han2020explaining} and image classification \citep{basu2021influence}, while also addressing biases in classification models \citep{wang2019repairing}, word embeddings \citep{brunet2019understanding}, and finetuned models \citep{chen2020multi}. Despite numerous studies on influence functions, we are the first to apply them to explain the thought process in language models (LMs) during reasoning tasks. We propose a new mechanistic interpretation framework, SICAF, to trace and analyze the reasoning strategies that language models (LMs) employ for complex tasks. Furthermore, compared to traditional neural networks, circuits contain only the most essential parameters of the model, significantly reducing the computational cost of calculating influence functions.