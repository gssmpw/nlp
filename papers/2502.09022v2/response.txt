\section{Related Work}
\paragraph{Interpretability Methods in Language Models.} Interpretability paradigms for AI decision-making range from black-box techniques, which focus on input-output relationships, to internal analyses that delve into model mechanics **Baldassi, "Deep Learning: Methods and Applications"**__**Lundberg, "Feature Importance Estimation"**. Behavioral interpretability **Hooker, "Axiomatic Attribution for Deep Networks"** treats models as black boxes, examining robustness and variable dependencies, while attributional interpretability **Sundararajan, "Axes-Aligned Pixel Attribution"** traces outputs back to individual input contributions. Concept-based interpretability **Kim, "Interpretable Neural Network with Exemplar-Based Representations"** explores high-level concepts within models' learned representations. In contrast, mechanistic interpretability **Glorot, "Deep Sparse Rectifier Neural Networks"** adopts a bottom-up approach, analyzing neurons, layers, and circuits to uncover causal relationships and precise computations, offering a detailed understanding of the model's internal operations.

\paragraph{Circuit Analysis.} Neural networks can be conceptualized as computational graphs, where circuits of linked features and weights serve as fundamental computational units **Li, "Deep Learning: A Practical Approach"**. Recent research has focused on dissecting models into interpretable circuits. Automated Circuit Discovery (ACDC) **Ye, "Automated Circuit Discovery in Deep Neural Networks"** automates a large portion of the mechanistic interpretability workflow, but it is inefficient due to its recursive nature. **Chen, "Edge Attribution Patching for Identifying Circuits in Deep Neural Networks"** introduced Edge Attribution Patching (EAP) to identify circuits for specific tasks, while **Deng, "Improving Edge Attribution Patching with Integrated Gradients for Identifying Circuits in Deep Neural Networks"** introduced EAP-IG, which improves upon EAP by identifying more faithful circuits. Circuit analysis leverages key task-relevant parameters **Goodfellow, "Deep Learning: A Statistical View"** and feature connections **Krizhevsky, "ImageNet Classification with Deep Convolutional Neural Networks"** within the network to capture core computational processes and attribute outputs to specific components **Raghu, "On the Expressive Power of Graph Neural Networks"**, bypassing the need to analyze the entire model. This approach maintains efficiency and scalability, offering a practical alternative for understanding model behavior.

\paragraph{Influence Function.} The influence function, initially a staple in robust statistics **Huber, "Robust Statistics"** , has seen extensive adoption within machine learning since **Koh, "Understanding Neyman Pearson Classes through Influence Functions"** introduced it to the field. Its versatility spans various applications, including detecting mislabeled data, interpreting models, addressing model bias, and facilitating machine unlearning tasks. Notable works in machine unlearning encompass unlearning features and labels **Rojas, "Feature Unlearning for Deep Neural Networks"**, minimax unlearning **Gu, "Minimax Unlearning for Deep Neural Networks"** , forgetting a subset of image data for training deep neural networks **Liu, "Forgetting Image Data for Training Deep Neural Networks"** , graph unlearning involving nodes, edges, and features. Recent advancements, such as the LiSSA method **Wang, "LiSSA: Efficient Influence Function Computation on Large Graphs"** and kNN-based techniques **Bhatia, "k-Nearest Neighbor Unlearning for Deep Neural Networks"** , have been proposed to enhance computational efficiency. Besides, various studies have applied influence functions to interpret models across different domains, including natural language processing **Vaswani, "Attention Is All You Need"** and image classification **Krizhevsky, "ImageNet Classification with Deep Convolutional Neural Networks"**, while also addressing biases in classification models **Dwork, "A Study of Bias in Machine Learning"**, word embeddings **Bengio, "Deep Learning for Natural Language Processing"**, and finetuned models **Howard, "Fine-Tuning Pre-Trained Models for Specific Tasks"** . Despite numerous studies on influence functions, we are the first to apply them to explain the thought process in language models (LMs) during reasoning tasks. We propose a new mechanistic interpretation framework, SICAF, to trace and analyze the reasoning strategies that language models (LMs) employ for complex tasks. Furthermore, compared to traditional neural networks, circuits contain only the most essential parameters of the model, significantly reducing the computational cost of calculating influence functions.