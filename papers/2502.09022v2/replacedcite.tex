\section{Related Work}
\paragraph{Interpretability Methods in Language Models.} Interpretability paradigms for AI decision-making range from black-box techniques, which focus on input-output relationships, to internal analyses that delve into model mechanics ____. Behavioral interpretability ____ treats models as black boxes, examining robustness and variable dependencies, while attributional interpretability ____ traces outputs back to individual input contributions. Concept-based interpretability ____ explores high-level concepts within models' learned representations. In contrast, mechanistic interpretability ____ adopts a bottom-up approach, analyzing neurons, layers, and circuits to uncover causal relationships and precise computations, offering a detailed understanding of the model's internal operations.

\paragraph{Circuit Analysis.} Neural networks can be conceptualized as computational graphs, where circuits of linked features and weights serve as fundamental computational units ____. Recent research has focused on dissecting models into interpretable circuits. Automated Circuit Discovery (ACDC) ____ automates a large portion of the mechanistic interpretability workflow, but it is inefficient due to its recursive nature. ____ introduced Edge Attribution Patching (EAP) to identify circuits for specific tasks, while ____ introduced EAP with integrated gradients(EAP-IG), which improves upon EAP by identifying more faithful circuits. Circuit analysis leverages key task-relevant parameters ____ and feature connections ____ within the network to capture core computational processes and attribute outputs to specific components ____, bypassing the need to analyze the entire model. This approach maintains efficiency and scalability, offering a practical alternative for understanding model behavior.

\paragraph{Influence Function.} The influence function, initially a staple in robust statistics ____, has seen extensive adoption within machine learning since ____ introduced it to the field. Its versatility spans various applications, including detecting mislabeled data, interpreting models, addressing model bias, and facilitating machine unlearning tasks. Notable works in machine unlearning encompass unlearning features and labels ____, minimax unlearning ____, forgetting a subset of image data for training deep neural networks ____, graph unlearning involving nodes, edges, and features. Recent advancements, such as the LiSSA method ____ and kNN-based techniques ____, have been proposed to enhance computational efficiency. Besides, various studies have applied influence functions to interpret models across different domains, including natural language processing ____ and image classification ____, while also addressing biases in classification models ____, word embeddings ____, and finetuned models ____. Despite numerous studies on influence functions, we are the first to apply them to explain the thought process in language models (LMs) during reasoning tasks. We propose a new mechanistic interpretation framework, SICAF, to trace and analyze the reasoning strategies that language models (LMs) employ for complex tasks. Furthermore, compared to traditional neural networks, circuits contain only the most essential parameters of the model, significantly reducing the computational cost of calculating influence functions.