\input{Tables/DNG_mp20_results}

\subsection{Benchmarks and datasets}\label{sec:benchmarks}

We use the following datasets to benchmark the OMG model: \textbf{\textit{perov-5}} \citep{castelli_new_2012}, a dataset of perovskites with 18,928 samples with five atoms per unit cell in which only lattice lengths and atomic types change; \textbf{\textit{MP-20}} \citep{jain_commentary_2013, xie_crystal_2022} from the Materials Project that contains 45,231 structures with a maximum of $N=20$ atoms per unit cell, and \mbox{\textbf{\textit{MPTS-52}}} \citep{baird_matbenchgenmetrics_2024} which is a time split of the Materials Project with 40,476 structures with up to $N=52$ atoms per unit cell and is typically the most difficult to learn.
We use the same 60-20-20 splits as \citet{xie_crystal_2022, jiao_crystal_2023, miller_flowmm_2024}.
%\textbf{\textit{carbon-24}} \cite{carbon_data_2020}, a dataset with 10,153 samples with only carbon atoms generated using AIRSS, with multiple structures corresponding to an $N$-atom unit cell;
For the \textbf{\textit{Alex-MP-20}} dataset \citep{zeni_generative_2025}, we used an 80-10-10 split constructed from MatterGen's 90-10 split, in which we removed 10\% of the training data to create a test dataset. This dataset contains 675,204 structures with 20 or fewer atoms per unit cell from the \textit{Alexandria} \citep{schmidt_largescale_2022, schmidt_dataset_2022} and \textit{MP-20} datasets.
We do not include the \textit{carbon-24} dataset \citep{carbon_data_2020} in our results, as match rate is ill-defined for this dataset; because all elements are carbon, it is not clear how many generated structures are unique and producing a structure that matches one in the reference dataset is trivial\footnote{Previous papers \citep{xie_crystal_2022, jiao_crystal_2023, miller_flowmm_2024} report match rate for \textit{carbon-24}, but they do not compare the generated structure to the entirety of the reference dataset; their results suggest the match tolerance is larger than the differences between the \textit{carbon-24} structures.}.
%\looseness=-1

\subsection{Performance metrics}

We assess the state-of-the-art performance of OMG's models using a variety of standard, refined, and contributed benchmarks.
For CSP (see Tab.~\ref{tab:CSP}), the match rate and root mean square distance (RMSD) of matched structures are computed between the test dataset and the generated structures using Pymatgen's \texttt{StructureMatcher} module \citep{ong_python_2013} with tolerances ($\texttt{stol}=0.5$, $\texttt{ltol}=0.3$, $\texttt{angletol}=10$).

For DNG (see Tab.~\ref{tab:DNG_eval}), metrics include validity (structural and compositional), coverage (recall and precision), and Wasserstein distances between distributions of properties including density $\rho$, number of unique elements $N$ (\textit{i.e.}, an $N$-ary material), and average coordination number by structure $\langle CN \rangle$.
We introduce the average coordination number benchmark in part due to the difficulty of generating symmetric structures; a structure's average coordination number is a useful fingerprint, and higher-coordinated structures tend to be more symmetric.

The previous DNG metrics are used during optimization of hyperparameters (see Appendix~\ref{app:hyperparameter}), with the best models on the validation set being used to calculate stability rate as well as the S.U.N. (stable, unique, and novel) rate (see Tab.~\ref{tab:DNG_stability}).
The stability-based metrics are calculated using MatterGen's code base \citep{mattergen_microsoft_2025}, in which the machine-learned interatomic potential MatterSim \citep{yang_mattersim_2024} is utilized for structural relaxation. 
This requires significantly less compute time in comparison to running DFT relaxations.
For all models with reported S.U.N. rates, we generated a set of 10,000 structures which are filtered to remove those with elements not handled by MatterSim.
Further details are provided in Appendix~\ref{app:hyp_metrics}. In addition to the stability and S.U.N rates, we also provide in Tab.~\ref{tab:DNG_eval} the average RMSD between the generated and relaxed structures, as well as the average energy above the convex hull. Figure~\ref{fig:ehull} compares the histograms of the energies above hull of the generated structures between OMG, DiffCSP, and FlowMM, and shows OMG's superior performance for the generation of stable structures.
%consistently generates lower-energy structures.

\begin{figure}[t]
   \centering
   \includegraphics[width=\columnwidth]{Figures/ehull_MMedit.pdf}
   \caption{Histogram of the computed energies above the convex hull for structures generated by FlowMM, DiffCSP, and OMG (linear interpolant). The OMG model consistently produces lower energy structures compared to FlowMM and DiffCSP. See Appendix~\ref{sun_calc} for calculation details.}
   \label{fig:ehull}
\end{figure}

We have refined the calculation of several metrics, and reported these for all models.
Previously reported match rates filtered the matched generated structures by their validity according to structural constraints on interatomic distances $r>0.5$\text{\AA} as well as SMACT composition rules \citep{davies_smact_2019} which screen structures by their electron valence and charge neutrality.
We note, however, that the datasets themselves contain invalid structures---for example, the \textit{MP-20} test dataset has $\sim 10$\% compositionally invalid structures. 
Thus, we argue that the removal of these invalid structures for computation of match rate and RMSE is not reasonable for assessing learning performance; we do, however, provide both match rates (with and without validation filtering).
%We additionally note that in prior work, the algorithm for computing match rate relies on a comparison between the structure from the reference test dataset (which provides the composition and number of elements $N$) and the generated structure; this is reasonable when a particular composition has only one or few structures in the materials dataset, which is not the case for \textit{carbon-24}.
%Therefore we add the option to `match all' between a given structure in the generated set to all structures in the reference (test) set, which we only employ for reporting match rates for the \textit{carbon-24} dataset.
%Thus, we contribute the correction of the previously reported highly depressed match rates for all models trained on \textit{carbon-24}.
%Correcting these low match rates is important, as they would otherwise suggest that the AIRSS technique used to create the \textit{carbon-24} dataset outperforms machine-learned models at generating correct structures.

% \subsection{Crystal structure prediction}

% For the CSP task, we report results in Table~\ref{tab:CSP}.
% \mm{Do we need a section for one line?}


% \subsection{Sequential vs. joint \textit{de novo} generation}

% We evaluate the performance of OMG on joint DNG in comparison to that of a two-step process in which composition is learned separately from structure, as seen in Fig.~\ref{fig:gen_viz}b. In the two-step process, we first train a Chemical Formula Prediction (CFP) model by modifying CSP Net (see Appendix \mm{add and ref here}) to generate compositions optimized for SMACT stability, distributional similarity to known $N$-ary structures, and novelty. The predicted compositions are then used as input for a pretrained CSP model, which generates the corresponding atomic configurations.

% \begin{figure}[tbp!]
%    \centering
%    \includegraphics[width=0.9\columnwidth]{Figures/figure_3_SI_MMedit.pdf}
%    \caption{Two avenues for performing \textit{de novo} generation of materials. The first uses two steps: a CFP model predicts compositions and then uses a CSP model to find accompanying stable structures.
%    The second trains a DNG model over cell, species, and fractional coordinates jointly.}
%    \label{fig:dng_schemes}
% \end{figure}

%\input{Tables/DNG_mp20_results}


