\subsection{Choice of interpolant}
\label{sec:inter}

In training OMG, we optimize the choice of the interpolating function that is used during training. 
\citet{albergo_stochastic_2023} introduced four interpolants of the form defined in Eq.~\ref{eq:linear_si}, each shaping the interpolation trajectory differently (see also Appendix~\ref{app:SI_interpolants} for further details). 

\input{Tables/CSP_main_results}

The \textbf{linear} interpolant defines a constant velocity trajectory from $x_0$ to $x_1$. When combined with an ODE sampling scheme and $\gamma=0$, this reproduces a particular choice of CFM. 
If the sampling scheme or $\gamma$ are changed, however, the use of linear interpolants become distinct from CFM.
The \textbf{trigonometric} interpolant prescribes trajectories with more curvature than the linear interpolant. 
% Both the linear and trigonometric interpolants ensure that both base and target distribution influence $\rho_t$ for all $t \in (0,1)$, maintaining smooth paths between the end-points.
The \textbf{encoder-decoder} interpolant first evolves samples from $\rho_0$ to follow an intermediate standard Gaussian distribution $\mathcal{N}(0,\bm{I})$ at time $t=0.5$, before mapping them to $\rho_1$ at $t=1$.
This approach has been found to interpolate more smoothly between distributions, potentially mitigating the formation of spurious features in the probability path at intermediate times \citep{albergo_stochastic_2023}.
Lastly, we consider the variance preserving \textbf{score-based diffusion (SBD)} interpolant.  When paired with an SDE sampling scheme, this interpolant is mathematically equivalent to an SBDM, but on the continuous time interval $[0,1]$. The SBD interpolant assumes that $\rho_0$ is a Gaussian, and unlike the previous three interpolants it involves no explicit latent variable; instead the $\alpha(t)x_0$ term takes on this role.
% We note that the conditional velocity field from the encoder-decoder interpolant between times $t=0.5$ and $t=1$ resembles that of the SBD interpolant between times $t=0$ and $t=1$; we emphasize, however, that the trained models are still learning fundamentally different velocity fields, and that these learned flows are likely not identical to the conditional fields. Additionally, the encoder-decoder and SBD interpolants have a particularly notable difference for our particular case of coupled stochastic interpolants for the modeling of joint distributions. Namely, for the encoder-decoder interpolant, the Gaussian-distributed coordinates at $t=0.5$ are conditioned on other coordinates that are halfway-interpolated at this point. Conversely, for SBD interpolation, the Gaussian distributed coordinates at $t=0$ are only conditioned on other random variables in the form of other coordinates since, at this point, all elements of $x_0$ are randomly distributed.
We note that the trajectory of the encoder-decoder interpolant between times $t=0.5$ and $t=1$ resembles that of the SBD interpolant between times $t=0$ and $t=1$. For the example of using the encoder-decoder interpolant only for the coordinates, however, we emphasize that the Gaussian-distributed coordinates at $t=0.5$ are conditioned on other coordinates that are halfway-interpolated at this point. Conversely, for SBD interpolation, the Gaussian distributed coordinates at $t=0$ are only conditioned on other random variables since, at this point, all elements of $x_0$ are randomly distributed.

We prioritize flexibility in learning the atomic coordinates, $\bm{X}$, as our findings indicate that learning drifts and denoisers which generate the positional distribution with high fidelity are more difficult than for the other two coordinates of interest.
As such, we evaluate the periodic versions of the four interpolants outlined above on learning the generation of atomic positions $\bm{X}$, but only use the linear interpolant with $\gamma=0$ for the lattice vectors $\bm{L}$.

\subsection{Equivariant representation of crystal structures}

Imposing inductive biases on the latent representation of the crystal structure can promote data efficiency and improve learning. 
The CSPNet architecture \citep{jiao_crystal_2023}, originally adopted in DiffCSP, is an equivariant graph neural network (EGNN) \citep{satorras_equivariant_2021} that produces a permutation- and rotation-equivariant, as well as translation-invariant representation of the crystal structures.

% previously was in Introduction
In the current OMG implementation, we employ CSPNet as an encoder that is trained from scratch. %ensuring O(3)-equivariance to capture lattice symmetries and periodic translational invariance of fractional coordinates.
%CSP Net builds upon an equivariant graph neural network architecture \cite{satorras_equivariant_2021}, ensuring O(3)-equivariance to capture lattice symmetries and periodic translational invariance of fractional coordinates.
% The input features to CSP Net combine atomic embeddings and sinusoidal positional encodings to encode atomic types and fractional coordinates, respectively.
The CSPNet architecture encodes atomic types using learnable atomic embeddings and represents fractional coordinates through sinusoidal positional encodings (see Appendix \ref{app:cspnet}). These features are processed through six layers of message-passing, after which the encoder produces the drift $b^\theta(t,x)$ of both the lattice and the fractional coordinates, as well as potentially predicting the denoiser $z^\theta(t,x)$. For DNG, the network must also predict $\log p_{1|t}^\theta(\bm{a}_1|x_{t})$. The resulting outputs inherently preserve the permutation, rotational, and translational symmetries embedded in CSPNet. 
%\looseness=-1

The output of CSPNet is invariant with respect to translations of the fractional coordinates in the input. Thus, one should, in principle, use a representation of the fractional coordinates that does not contain any information about translations. While this is straightforward in Euclidean space by removing the mean of the coordinates of the given structure, this cannot be done with periodic boundary conditions where the mean is not uniquely defined. We follow \citet{miller_flowmm_2024} and instead remove the center-of-mass motion when computing the ground-truth $\partial_tx(t,x_0,x_1,z)$ in Eq.~\ref{eq:loss_b}.
%\looseness=-1

Alternative EGNNs such as NequIP \citep{batzner_e3equivariant_2022}, M3GNet \citep{chen_universal_2022}, or MACE \citep{batatia_mace_2022} which have been widely used for the development of MLIPs can also serve as plug-and-play encoders within OMG's SI framework.  Integrating different architectures is a direction that we plan to explore in future iterations of the framework.

%\input{Tables/CSP_main_results}

\subsection{Comparison to other frameworks}

We report results for the CSP task in Tab.~\ref{tab:CSP} and for the DNG task in Tabs.~\ref{tab:DNG_eval} and \ref{tab:DNG_stability} for DiffCSP, FlowMM, and OMG. We detail in Sec.~\ref{sec:Experiments} how we improve the extant benchmarks used in the field and therefore recompute all CSP and DNG benchmarks.
In nearly all cases, we were able to generate structures using the DiffCSP and FlowMM source code which closely matched (within $\sim$1\%) the previously reported metrics in their respective manuscripts.
%In nearly all cases\footnote{FlowMM's model trained on \textit{carbon-24} using the reported hyperparameters exhibited a $\sim$3\% lower match rate (19.80\%) compared to the originally reported values.}, we were able to generate structures using the DiffCSP and FlowMM source code which closely matched (within $\sim$1\%) the previously reported metrics in their respective manuscripts. 
The observed differences can be attributed to the use of a newer version of SMACT composition rules\footnote{The SMACT Python library updated its default oxidation states with the release of version 3.0.} \citep{davies_smact_2019} and to natural fluctuations arising from model retraining.

Since the focus of this work is to assess our model's ability to learn unconstrained and unconditioned flows, we do not compare against symmetry-constrained generation methods \citep{ai4science_crystalgfn_2023, cao_space_2024, zhu_wycryst_2024, kazeev_wyckofftransformer_2024, jiao_space_2024}. Symmetry constraints can be incorporated in future extensions of the flexible OMG framework.

