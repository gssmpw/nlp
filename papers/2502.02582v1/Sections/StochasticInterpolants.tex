% Here, describe what stochastic interpolants are (attempt to continue from and not to overlap with portion in introduction)
\subsection{Stochastic Interpolants}

SIs provide a unifying mathematical framework for generative modeling, generalizing both SBDMs and CFM \citep{albergo_stochastic_2023}. 
The SI $x(t, x_0, x_1, z)$ bridges the base distribution $\rho_0$ with a target distribution $\rho_1$ by learning a time-dependent drift $b^\theta(t, x)$.
In this work, we focus on stochastic interpolants between $\rho_0$ and $\rho_1$ of the form:
\begin{equation}
    x_t \equiv x(t, x_0, x_1, z) = \alpha(t) x_0 + \beta(t) x_1 + \gamma(t) z.
\label{eq:linear_si}
\end{equation}
Here, $t\in[0,1]$ represents time and $(x_0, x_1)$ are paired samples drawn from $\rho_0$ and $\rho_1$, respectively.
The random variable $z$  is drawn from a standard Gaussian $\mathcal{N}(0, \bm{I})$ independently of $x_0$ and $x_1$. The functional forms of $\alpha$, $\beta$, and $\gamma$ are flexible, subject to few few constraints (see Appendix~\ref{app:SI_interpolants}). The inclusion of the latent variable $\gamma(t)z$ allows sampling of an ensemble of paths around the mean interpolant $I(t, x) = \alpha(t) x_0 + \beta(t) x_1$, and is theorized to improve generative modeling by promoting smoother and more regular learned flows \citep{albergo_stochastic_2023}.

For generative modeling, it is most important that the time-dependent density $\rho(t)$ of the stochastic process $x_t$ in Eq.~\ref{eq:linear_si} can also be realized either \emph{via} deterministic sampling through an ODE (derived from a transport equation) or stochastic sampling through an SDE (derived from a Fokker--Planck equation) only requiring $x_0\sim\rho_0$ (see Appendix~\ref{app:omg}).
For both ODE- and SDE-based sampling, the required drift term $b^\theta(t, x): [0,1] \times \mathbb{R}^d \rightarrow \mathbb{R}^d$ is learned by minimizing the loss function
\begin{equation}
    \begin{split}
        \mathcal{L}_b(\theta) = 
         &\, \mathbb{E}_{t, z, x_0, x_1} \\
         &\big[ |b^\theta(t, x_t)|^2 - 2\,\partial_t x(t, x_0, x_1, z) \cdot b^\theta(t, x_t) \big],
    \end{split}
\label{eq:loss_b}
\end{equation}
where the expectation is taken independently over $t\sim \mathcal{U}(0,1)$ with $\mathcal{U}(0,1)$ the uniform distribution between $0$ and $1$, $z\sim\mathcal{N}(0, \bm{I})$, $x_0\sim\rho_0$, and $x_1\sim\rho_1$.
For SDE-based sampling, an additional denoiser $z^\theta(t,x): [0,1] \times \mathbb{R}^d \rightarrow \mathbb{R}^d$  must be learned by minimizing an additional loss
\begin{equation}
    \mathcal{L}_z^\theta(\theta) = \mathbb{E}_{t, z, x_0, x_1} \left[ |z^\theta(t, x_t)|^2 - 2\,z^\theta(t, x_t) \cdot z \right].
\label{eq:loss_z}
\end{equation}
The drift term, along with the denoiser in the case of SDE-based sampling, enables the generation of samples from the target distribution \citep{albergo_stochastic_2023}.
For ODE-based sampling, $\gamma(t)=0$ is a possible choice. For SDE-based sampling, however, $\gamma(t)>0$ is required for all $t\in(0,1)$ (see Appendix~\ref{app:omg}).
%After training, sampling from the generative model can be carried out with standard ODE or SDE integrators to produce new samples from the target $\rho_1$. 

By appropriately selecting interpolation functions $\alpha$, $\beta$, $\gamma$ and choosing between deterministic (ODE) and stochastic (SDE) sampling schemes, the SI framework not only recovers CFM and SBDM as special cases but also enables the design of a broad class of novel generative models (see Appendix~\ref{app:SI_interpolants} for examples).
The strength of OMG's SI implementation for materials discovery lies in its ability to tune both the interpolation and sampling schemes, as illustrated in Fig.~\ref{fig:si_viz} for a pair of structures sampled from $\rho_0$ and $\rho_1$.
By systematically optimizing over this large design space, we achieve superior performance for CSP and DNG tasks across datasets, as discussed in Section~\ref{sec:Experiments}. 
%The SI framework provides superior potential for optimization during learning, which we leverage to achieve state-of-the-art results.
%This freedom to choose bespoke base distributions, interpolants, and sampling schemes is the driving force behind the performance of OMG.

\begin{figure}[t]
   \centering
   \includegraphics[width=\columnwidth]{Figures/figure_1_SI_MMedit_2_small.pdf}
   \caption{Visualization of the tunable components of the SI framework for bridging samples $x_0$ (gray particles) and $x_1$ (purple particles). \textbf{(a)} The choice of the interpolant changes the path of the time-dependent interpolation trajectory. \textbf{(b)} During inference, the learned drift term $b^\theta(t,x)$ and denoiser $z^\theta(t,x)$ generate new samples via standard ODE or SDE integration, here for a linear interpolant with $\gamma=\sqrt{0.07t(1-t)}$. \textbf{(c)} The inclusion of a latent variable $\gamma(t)z$ changes the interpolation path. \textbf{(d)} The function $\gamma(t, a) = \sqrt{at(1-t)}$ depends on $a$ that also influences the interpolation path.}
   \label{fig:si_viz}
\end{figure}

\subsection{Crystal representation and generation}

%\subsection{Particle birth and death}

A crystalline material is defined by its idealized repeat unit, or unit cell, which encodes its periodicity. 
In the OMG representation, a unit cell is described by separating the material's chemical composition---given by its atomic species $\bm{A} \in \mathbb{Z}_{>0}^N$, where $N$ is the number of atoms in the unit cell---from its structural representation---its fractional coordinates $\bm{X} \in [0, 1)^{3\times N}$ with periodic boundaries and lattice vectors $\bm{L} \in \mathbb{R}^{3 \times 3}$.
During training, all three components $\cb{\bm{A}, \bm{X}, \bm{L}}$ are considered simultaneously. We apply the SI framework only to the continuous structural representations $\cb{\bm{X}, \bm{L}}$ with loss functions defined in Eq.~\ref{eq:loss_b} and~\ref{eq:loss_z}, and use discrete flow matching (DFM) on the chemical species $\bm{A}$ \citep{gat_discrete_2024}. The number of atoms $N$ in the structure $x_0$ sampled from the base distribution $\rho_0$ is determined by the number of atoms in the corresponding structure $x_1$ sampled from the target distribution $\rho_1$.
% NOTE: this may change.
% Add a sentence here about how interpolation goes from "materials" drawn from 2 distributions
% 

\subsubsection{Atomic coordinates}
We specify the base distribution for the fractional coordinates $\bm{x} \in [0,1)$ for all $\bm{x} \in \bm{X}$ \textit{via} a uniform distribution (except for the score-based diffusion interpolant that requires a wrapped normal distribution $\rho_{0}(\bm{x})$ following the approach of \citet{jiao_crystal_2023}; see Section \ref{sec:inter}). 
For learning fractional coordinates, we implement a variety of periodic interpolants that connect the base to the target data distributions.
To satisfy periodic boundary conditions on the paths defined by the interpolants, we extend the SI framework to the surface of a four-dimensional torus in this paper.
Reminiscent of RFM~\citep{chen_flow_2024}, the linear interpolant on the torus traverses a path equivalent to the shortest-path geodesic which is always\footnote{The only exception being when two points are precisely half the box length apart. However, this case is not relevant for the given base distribution.} well-defined. Other interpolants, however, are more complex. In order to uniquely define them, we always define the interpolation with respect to the shortest-path geodesic.
That is, for interpolation between $\bm{x}_0$ and $\bm{x}_1$ with a periodic boundary at 0 and $1$, we first unwrap $\bm{x}_1$ to the periodic image $\bm{x}_1'$ which has the shortest possible distance from $\bm{x}_0$.
Following this, the interpolation between $\bm{x}_0$ and $\bm{x}_1'$ is computed given a choice of interpolant, and the traversed path is wrapped back into the boundary from $0$ to $1$. This approach is illustrated in Appendix~\ref{app:SI_PBC}.

\subsubsection{Lattice vectors}
To construct the base distribution for the cell vectors $\bm{L}$, we follow \citet{miller_flowmm_2024} and use an informed log-normal distribution $\rho_{0}(\bm{L})$ which is fitted to the distribution of cell vectors in each target dataset. This leverages the flexibility in the SI framework to choose arbitrary base distributions in contrast to SBDM which necessarily requires a Gaussian base distribution.

\subsubsection{Atomic species}

The discrete nature of chemical compositions $\bm{A}$ in atomic crystals requires a specialized approach for generative modeling. To address this, we implement discrete flow matching (DFM) \citep{campbell_generative_2024}.
In our implementation of the DFM framework, each atomic species $\bm{a} \in \bm{A}$ can take values in $\cb{1,2,\dots,100} \cup \cb{M}$; where $\cb{1-100}$ are atomic element numbers and $M$ is a masking token used during training.
The base distribution is defined as $\rho_0(\bm{a}) = \bk{M}^N$, meaning that initially all $N$ atoms are masked. As sampling progresses, the identities of the atoms evolve \emph{via} a continuous-time Markov Chain (CTMC), and are progressively unmasked to reveal valid atomic species. At $t=1$, all masked tokens are replaced.
To learn this process, we define a conditional flow $p_{t|1}(\bm{a}_t|\bm{a}_1$) that linearly interpolates in time from the fully masked state $\bm{a}_0$ toward $\bm{a}_1$ and thus yields the composition $\bm{a}_t$ of the interpolated structure $x_t$. Based on these conditional flows, a neural network is trained to approximate the denoising distribution $p_{1|t}^\theta(\bm{a}_1|x_t)$, which yields the probability for the composition $\bm{a}_1$ given the entire structure $x_t$, by minimizing a cross-entropy loss
\begin{equation}
    \mathcal{L}_{\mathrm{DFM}}(\theta) = \mathbb{E}_{t, x_{1}, x_{t}} \left[ \log p_{1|t}^\theta(\bm{a}_1|x_{t}) \right].
\label{eq:loss_dfm}
\end{equation}
%With this learned predictor, we can construct a rate matrix for the CTMC, $R_t(x_{\bm{A}, t}, i | x_{\bm{A}, 1})$, that dictates the evolution of element $i$ in the sequence at a particular time $t$ (see Appendix~\ref{app:SI_PBC}). 
In doing this, we are able to directly construct the marginal rate matrix $R^\theta_t(\bm{a}_t, i)$ for the CTMC that dictates the evolution of $\bm{a}_t$ at time $t$ to the next time step during generation (see Appendix~\ref{app:SI_DFM}). It is important to note that the learned probability path is a function of the entire atomic configuration $\cb{\bm{A}, \bm{X}, \bm{L}}$ which is necessary for the prediction of chemical composition from structure.

% \cz{In flowmm, analog bits are used for atomic types (chemical species), reviewer may have questions regarding the improvement of the performance: whether it is due to the Flow-CTMC or optimized SI paramaters?}

\subsection{Joint generation with stochastic interpolants}

For both CSP and DNG tasks, we seek to generate samples from a joint distribution over multiple coordinates. 
For DNG, this joint distribution $\rho_1$ encompasses all elements of a crystal unit cell.
For CSP we similarly model the joint distribution, $\rho_1$, but with atom types fixed to compositions sampled from the target dataset.
For both tasks, the total loss function is formulated as a weighted sum of the individual loss functions for each variable (see Appendix~\ref{app:loss}), and their relative weights are optimized (see Appendix~\ref{app:hyperparameter}).
We illustrate both types of models and their structure generation process in Fig.~\ref{fig:gen_viz}.

\begin{figure*}[t]
   \centering
   \includegraphics[width=\textwidth] {Figures/figure_2_SI_MMedit_2_small.pdf} %max width is 0.96
   \caption{Illustration of CSP and DNG tasks. (\textbf{a}) For CSP, the species $\bm{A}$ are fixed with known compositions from $t=0$. From this, we predict $\bm{X}$ and $\bm{L}$ from randomly sampled initial values. For DNG, we predict $(\bm{A}, \bm{X}, \bm{L})$ jointly. Our implementation of DFM initializes $\bm{A}$ as a sequence of masked particles that are unmasked through a series of discrete jumps to reveal a physically reasonable composition.
   (\textbf{b}) Two avenues for performing \textit{de novo} generation of materials. The first uses two steps: a CFP model predicts compositions and then uses a CSP model to find accompanying stable structures.
   The second trains a DNG model over cell, species, and fractional coordinates jointly as shown in (a).
   }
   \label{fig:gen_viz}
\end{figure*}

Additionally, for DNG, we consider a two-step process in which composition is learned separately from structure, as seen in Fig.~\ref{fig:gen_viz}b. In this approach, we first train a Chemical Formula Prediction (CFP) model (see Appendix \ref{app:cspnet}) to generate compositions optimized for SMACT stability \citep{davies_smact_2019}, similarity in the distribution of $N$-arity of known structures, as well as uniqueness and novelty. The predicted compositions are then used as input for a pretrained CSP model, which generates the corresponding atomic configurations.

% FOR ICLR, WE COMMENTED OUT THE FOLLOWING TWO PARAGRAPHS AND MOVED THEM TO THE APPENDIX!

\subsection{Data-dependent coupling}

SIs have been used with data-dependent couplings \citep{albergo_stochastic_2024}, where a coupling function $\nu(x_0, x_1)$ enables  biasing of $x_0$ based on the sampled $x_1$.
In OMG, we incorporate an optional data-dependent coupling that enforces an ordering (\textit{i.e.}, a permutation on the order of atomic elements within a structure) that produces the minimum fractional-coordinate distance between each particle pair $(\bm{x}_0^i, \bm{x}_1^i)$ from structures $x_0 \in \rho_0$ and $x_1 \in \rho_1$ (see Appendix~\ref{app:SI_MPD}). We find that the inclusion of this data-dependent coupling is optimal during hyperparameter tuning depending on the type of model: CSP models typically performed better without this coupling, but DNG models (see Tab.~\ref{tab:OMG_DNG_trends}) can benefit in certain cases from minimizing traveled distance \textit{via} permutation of elements.

\subsection{Velocity annealing}

Velocity annealing---rescaling the learned drift during generation to increase velocity over time as $b^\theta(t,x) \rightarrow (1+st)\,b^\theta(t,x)$ with $s$ as an hyperparameter during integration---has been empirically shown to improve performance in a number of studies that apply CFM to physical systems \citep{yim_fast_2023, bose_se3stochastic_2024, miller_flowmm_2024}.
For instance, \citet{miller_flowmm_2024} demonstrated that applying velocity annealing significantly improves performance in CSP and DNG benchmarks for materials. Motivated by these findings, we include velocity annealing in OMG as a tunable hyperparameter, while emphasizing that this technique lacks a formal theoretical justification within the mathematical frameworks underlying flow models and stochastic interpolants.