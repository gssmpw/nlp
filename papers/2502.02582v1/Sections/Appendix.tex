\section{Implementation details of stochastic interpolants} \label{app:SI}

\subsection{OMG Framework}
\label{app:omg}
Figures \ref{fig:training} and \ref{fig:integration} summarize the training and the integration pipeline of the OMG framework, respectively. Depending on the specific task, there are several stochastic interpolants at once. For CSP, one stochastic interpolant considers lattice vectors $\bm{L}$, and another one considers fractional coordinates $\bm{X}$. The model output of CSPNet (see Appendix \ref{app:cspnet}) depends on the full structural representation $\{\bm{A},\bm{X},\bm{L}\}$ and time $t$, where $\bm{A}$ are the atomic species. For the DNG task, we additionally use discrete flow matching for the atomic species $\bm{A}$ \citep{campbell_generative_2024}. 

During the numerical integration in the CSP task, $\bm{X}$ and $\bm{L}$ are integrated jointly while $\bm{A}$ is fixed. For DNG, $\bm{A}$ is evolved according to discrete flow matching \citep{campbell_generative_2024} (see Appendix \ref{app:SI_DFM}).
For the SDE sampling scheme in Fig.~\ref{fig:integration}, one chooses a time-dependent noise $\varepsilon(t)$ that only appears during integration and not during training. Also, $\gamma(t)$ has to be unequal zero in order to prevent the divergence in $1/\gamma(t)$. However, since $\gamma(t)$ necessarily vanishes at times $t=0$ and $t=1$ (see Appendix \ref{app:SI_interpolants}), one should choose a time-varying $\varepsilon(t)$ that vanishes near these endpoints \citep{albergo_stochastic_2023}.

\begin{figure}[htbp!]
   \centering
   \includegraphics[width=0.9\columnwidth]{Appendix_Figures/OMGArchitecture.pdf}
   \caption{Training pipeline of the OMG framework: A batch of structures is drawn from a dataset with target distribution $\rho_1$. Every structure $x_1\sim\rho_1$ is connected with a structure $x_0$ from the base distribution $\rho_0$ with stochastic interpolants that yield the interpolated structure $x_t=x(t, x_0, x_1, z)$ and the drift $b_t=\partial_t x_t$ at time $t\sim \mathcal{U}(0,1)$, possibly using a random variable $z\sim\mathcal{N}(0, \bm{I})$. The model CSPNet predicts $b^\theta_t= b^\theta(t,x_t)$ and $z^\theta_t=z(t, x_t)$ and its parameters are minimized based on the MSE losses in Eqs \ref{eq:loss_b} and \ref{eq:loss_z}.}
   \label{fig:training}
\end{figure}

\begin{figure}[htbp!]
   \centering
   \includegraphics[width=0.7\columnwidth]{Appendix_Figures/OMGArchitectureTwo.pdf}
   \caption{Numerical integration pipeline of the OMG framework: An initial structure $x_0$ from the base distribution $\rho_0$ is numerically integrated following either an ODE or an SDE based on the model predictions $b_t^\theta$ and $z_t^\theta$. For an SDE, one can choose a noise $\varepsilon(t)$ during integration.}
   \label{fig:integration}
\end{figure}

\subsection{Interpolant choice}\label{app:SI_interpolants}

We are concerned with interpolants with the form specified in Eq.~\ref{eq:linear_si}. Here, the following conditions must be met~\citep{albergo_stochastic_2023}:
\begin{equation}
    \alpha(0)=\beta(1)=1, \quad \alpha(1)=\beta(0)=\gamma(0)=\gamma(1)=0, \quad \gamma(t)>0 \,\forall  t \in (0,1)
\label{si_constraints}
\end{equation}
Under these constraints, the form of the SI is relatively flexible, and many different interpolants can be defined (see Tab.~\ref{tab:SIParams}). An additional advantage of SI is that the base distribution can be arbitrary as in CFM \citep{tong_improving_2024}. We find that particular forms of the stochastic interpolants result in better performance than others. 
To add to the flexibility of the SI framework, ODE or SDE integration can be used to sample from a trained SI model. We thoroughly explore the use of both of these techniques to investigate the performance of SI on different elements of the crystal unit cell.
\begin{table}[t!]
\caption{SI parameters from \citet{albergo_stochastic_2023}.}
\label{tab:SIParams}
\centering
\renewcommand{\arraystretch}{1.5} % Adjust row height
\begin{tabular}{@{}clccc@{}}
\toprule
& \textbf{Stochastic Interpolant} & $\alpha(t)$ & $\beta(t)$ & $\gamma(t)$ \\ 
\midrule
\multirow{3}{*}
    & linear & $1 - t$ & $t$ & $\sqrt{at(1-t)}$ \\ 
    {Arbitrary $\rho_0$} & trig & $\cos \frac{\pi}{2}t$ & $\sin \frac{\pi}{2}t$ & $\sqrt{at(1-t)}$ \\ 
    & enc-dec & $\cos^2(\pi t)1_{[0,\frac{1}{2})}(t)$ & $\cos^2(\pi t)1_{(\frac{1}{2},1]}(t)$ & $\sin^2(\pi t)$ \\ 
\midrule
\multirow{3}{*} 
    & linear & $1 - t$ & $t$ & $0$ \\ 
    {Gaussian $\rho_0$} & trig & $\cos \frac{\pi}{2}t$ & $\sin \frac{\pi}{2}t$ & $0$ \\ 
    & SBD & $\sqrt{1-t^2}$ & $t$ & $0$ \\ 
\midrule
    & Mirror & $0$ & $1$ & $\sqrt{at(1-t)}$ \\ 
\bottomrule
\end{tabular}
\end{table}
\subsection{Antithetic sampling}

As shown by \citet{albergo_stochastic_2023}, inspection of possible $\gamma$ terms and of Eq.~\ref{eq:loss_b} reveals that the loss function can become unstable around $t=0$ and $t=1$. To account for this, we implement antithetic sampling. This requires to compute the loss at both $x^+$ and $x^-$ where:
\begin{equation}
    x^+(t,x_0,x_1,z) = \alpha(t) x_0 + \beta(t) x_1 + \gamma(t) z,
\end{equation}
\begin{equation}
    x^-(t,x_0,x_1,z) = \alpha(t) x_0 + \beta(t) x_1 - \gamma(t) z.
\end{equation}

\subsection{Interpolation with periodic boundary conditions}\label{app:SI_PBC}

We show in Fig.~\ref{fig:PBC} how we implement periodic versions of interpolants in order to represent fractional coordinates in a unit cell with periodic boundary conditions. We emphasize that this procedure is important not only for the choice of interpolant, but also for the addition of the latent variable $\gamma(t) z$ which also moves the interpolation trajectory away from the geodesic.

\begin{figure}[th]
   \centering
   \includegraphics[width=\textwidth]{Appendix_Figures/app_PBC.pdf}
   \caption{Extending interpolants to incorporate periodic boundary conditions. \textbf{(a--b)} The path for a score based diffusion interpolant is calculated by first computing the shortest-path geodesic (blue) between the initial (green dot) and final positions (red dot). Next, the path of the interpolant moving the final position outside the bounding box is computed (green), and finally the path is wrapped back into the bounding box to produce the interpolant trajectory (orange). \textbf{(c)} The effect of adding a latent variable to any interpolant must be handled similarly to calculating the path of a non-linear interpolant. For a linear interpolant with a nonzero $\gamma$, we show samples of possible paths (blue) and their averaged path (orange) which collapses onto the path of the linear interpolant.}
   \label{fig:PBC}
\end{figure}

\subsection{DFM Details}\label{app:SI_DFM}
DFM allows for generative modeling of discrete sequences of tokens while respecting the discrete nature of the design space. As discussed, a parameterized neural network $p_{1|t}^\theta(x_1|x_t)$ is learned, which attempts to predict the final sequence from the current sequence. Borrowing from \citet{campbell_generative_2024}, we choose a conditional rate matrix $R_t(x_t,i|x_1)$ generating the conditional flow $p_{t|1}(x_t|x_1)$ of the form:
\begin{equation}
    R_t(x_t,i|x_1) = \frac{\mathrm{ReLU} \left( \partial_t p_{t|1}(i|x_1) - \partial_t p_{t|1}(x_t|x_1) \right)}{S \cdot p_{t|1}(x_t|x_1)},
\end{equation}
where $S$ is the number of possible tokens a sequence element can take on. This conditional rate matrix can be modified by including a term that introduces stochasticity in the form of a detailed balance rate matrix $R_t^{DB}$ by writing $R_t^\eta = R_t + \eta R_t^{DB}$. Here, \citep{campbell_generative_2024}:
\begin{equation}
    R_t^{DB}(i,j|x_1) = \eta \delta \{ i,x_1 \} \delta\{j,M\}+ \frac{\eta t}{1-t} \delta \{ i,M \} \delta \{ j,x_1 \},
\label{eq:rate_matr}
\end{equation}
where $M$ is the masking token. The parameter $\eta \in \mathbb{R}^{+}$ represents the level of stochasticity that only appears during generation. 

During generation, our objective is to compute $R^\theta_t(x_t,i)$ based on the learned distribution $p_{1|t}^\theta(x_1|x_t)$. Formally, we have
\begin{equation}
    R^\theta_t(x_t,i) = \mathbb{E}_{p_{1|t}^\theta(x_1|x_t)} \big[ R_t^\eta(x_t, i | x_1) \big]
\label{eq:uncond_rate_matr}
\end{equation}
In practice, \citet{campbell_generative_2024} show that we need not compute a full expectation, but rather, simply draw $x_1 \sim p_{1|t}^\theta(x_1|x_t)$, evaluate the conditional rate matrix, and perform an update of $x_t$ to $x_{t+\Delta t}$ with discrete time step $\Delta t$ directly from this by sampling $x_{t+\Delta t}$ according to
\begin{equation}
    p_{t+\Delta t | t}(x_{t+\Delta t} | x_1, x_t) = \delta \{ x_t, x_{t+\Delta t} \} + R_t^\eta(x_t,i|x_1) \Delta t.
\label{eq:discrete_euler}
\end{equation}

% FOR ICLR, COMMENT THE FOLLOWING BACK IN

%\subsection{Velocity annealing}

%Velocity annealing---rescaling the learned velocity field during generation to increase velocity over time as $b^\theta(t,x) \rightarrow (1+st)\,b^\theta(t,x)$ with $s$ as an hyperparameter during integration---has been empirically shown to improve performance in a number of studies that apply CFM to physical systems \citep{yim_fast_2023, bose_se3stochastic_2024, miller_flowmm_2024}.
%For instance, \citet{miller_flowmm_2024} demonstrated that applying velocity annealing significantly improves performance in CSP and DNG benchmarks for materials. Motivated by these findings, we include velocity annealing in OMG as a tunable hyperparameter, while emphasizing that this technique lacks a formal theoretical justification within the mathematical frameworks underlying flow models and stochastic interpolants.

%\subsection{Data-dependent coupling}

%SIs have been used with data-dependent couplings \citep{albergo_stochastic_2024}, where a coupling function $\nu(x_0, x_1)$ enables  biasing of $x_0$ based on the sampled $x_1$.
%In OMG, we incorporate an optional data-dependent coupling that enforces an ordering (\textit{i.e.}, a permutation on the order of atomic elements within a structure) that produces the minimum fractional-coordinate distance between each particle pair $(\bm{x}_0^i, \bm{x}_1^i)$ from structures $x_0 \in \rho_0$ and $x_1 \in \rho_1$. We find that the inclusion of this data-dependent coupling is optimal during hyperparameter tuning depending on the type of model: CSP models typically performed better without this coupling, but DNG models (see Tab.~\ref{tab:OMG_DNG_trends}) can benefit in certain cases from minimizing traveled distance \textit{via} permutation of elements.

\subsection{Permuting particle order to achieve minimum interpolated distance}\label{app:SI_MPD}

% \begin{table}[t]
% \caption{Optimally performing DNG trials from OMG. All models are trained with an ODE sampling scheme and ordered by decreasing species noise $\eta$. The values of whether the element ordering is permuted to minimize traveled distance is also included.}
% \label{tab:OMG_DNG_trends}
% \centering
% \resizebox{0.6\columnwidth}{!}{
% \begin{tabular}{ccc}
% \toprule
% \textbf{Pos. Interpolant} & \textbf{Min. dist. permute} & \textbf{Species noise} \\ \midrule
% Linear, $\gamma=0$ & True & 35.46 \\ 
% Linear, $\gamma=0$ & True & 31.29 \\ 
% Linear, $\gamma=0$ & True & 7.08 \\ 
% SBD, $\gamma=0$ & False & 1.95 \\ 
% EncDec, $\gamma=\sin^2{\pi t}$ & False & 0.85 \\ 
% Trig, $\gamma=\sqrt{1.19t(1-t)}$ & True & 0.81 \\ 
% \bottomrule
% \end{tabular}
% }
% \end{table}


We experiment with biasing the initial distribution such that we learn interpolant paths that are shorter on average than independently chosen $(x_0,x_1)$ pairs. 
Formally, our coupling is conditional on the sampled $(x_0,x_1)$ and is defined as
\begin{equation}
    \arg \min_{p} \sum_{i} d(p(x_{0}^i), x_{1}^i).
    %\arg \min_{p} \sum_{i} d(x_{0}^{p(i)}, x_{1}^i).
\end{equation}
Here, $d(\cdot, \cdot)$ is a distance metric which we define on a periodic manifold in fractional-coordinate space (\textit{i.e.}, a four-dimensional torus) and $p$ is some permutation function that permutes the discrete indices $i$. Under this coupling, we still sample $(x_0, x_1)$ independently but then bias the sampled $x_0$ to travel the minimum permutational distance necessary to reach the target structure. 
We show the performance of this method in Tab.~\ref{tab:OMG_DNG_trends} for our DNG models.
%We ablate the performance of these methods in (\textcolor{red}{some table with data}).

%We discuss the utility of these methods in Tab.~\ref{tab:OMG_DNG_trends} for our DNG models.

\section{Model architecture}
\subsection{Graph neural network} \label{app:cspnet}
We implement a message-passing GNN with CSPNet introduced in \citet{jiao_crystal_2023}:
\begin{equation}
    \bm{h}^i_{(0)} = \phi_{\bm{h}_{(0)}} (\bm{a}^i)
\end{equation}
\begin{equation}
\label{eq:message}
    \bm{m}_{(s)}^{ij} = \varphi_m \left( \bm{h}^i_{s-1}, \bm{h}^j_{s-1}, \bm{l}, \mathrm{SinusoidalEmbedding}(\bm{x}^j - \bm{x}^i) \right)
\end{equation}
\begin{equation}
    \bm{m}_{(s)}^i = \sum_{j=1}^N \bm{m}^{ij}_{(s)}
\end{equation}
\begin{equation}
    \bm{h}_{(s)}^i = \bm{h}_{(s-1)}^i + \varphi_h(\bm{h}_{(s-1)}, \bm{m}_{(s)}^i)
\end{equation}
\begin{equation}
    b_{\bm{x}} = \varphi_{\bm{x}} \left( \bm{h}^i_{(\mathrm{max \, s})} \right)
\end{equation}
\begin{equation}
    b_{\bm{l}} = \varphi_{\bm{l}} \left( \frac{1}{n} \sum_{i=1}^n \bm{h}^i_{(\mathrm{max \, s})} \right)
\end{equation}
Here, node embeddings, $\bm{h}$, are initialized as a function of the atom types, $\bm{a}$. Embeddings are then updated by a message passing scheme through a series of graph convolution layers. Messages are computed with a parameterized neural network, $\varphi_m$. Messages are computed from neighboring node embeddings as well as information about the lattice, $\bm{l}$, and distance between the fractional coordinates $\bm{x}$. 

For the CFP model that should only prediction compositions, we simply remove the input of the lattice $\bm{l}$ and the fractional coordinates $\bm{x}$ from the computation of the message in Eq.~\ref{eq:message}. This ensures that the output $p_{1|t}^\theta(\bm{a}_1|x_t)$ of CSPNet for the composition does not depend on lattice vectors or fractional coordinates, while preserving permutational equivariance.

\subsection{Loss function}
\label{app:loss}
With Equations~\ref{eq:loss_b},~\ref{eq:loss_z}, and~\ref{eq:loss_dfm}, we can construct a loss function for the modeling of our joint distribution of interest,
\begin{equation}
    \begin{split}
        \mathcal{L}(\theta) =
        &
        \mathbb{E}_{t, z, x_0, x_1} \big[ \\
        & \quad \, \lambda_{\bm{x}, b} \left[ |b_{\bm{x}}^\theta(t, x_t)|^2 - 2\partial_t x(t, x_0, x_1, z) \cdot b_{\bm{x}}^\theta(t, x_t) \right] +  \lambda_{\bm{x}, z} \left[ |z_{\bm{x}}^\theta(t, x_t)|^2 - 2z_{\bm{x}}^\theta(t, x_t) \cdot z \right] \\
        & + \lambda_{\bm{l}, b} \left[ |b_{\bm{l}}^\theta(t, x_t)|^2 - 2\partial_t x(t, x_0, x_1, z) \cdot b_{\bm{l}}^\theta(t, x_t) \right] + \lambda_{\bm{l}, z} \left[ |z_{\bm{l}}^\theta(t, x_t)|^2 - 2z_{\bm{l}}^\theta(t, x_t) \cdot z \right] \\
        & + \lambda_{\bm{a}} \left[ \log p_{1|t}^\theta(\bm{a}_1|x_{t}) \right]
        \big].
    \end{split}
\label{eq:loss_tot}
\end{equation}
where the $\lambda$ terms correspond to the relative weights of each term in the loss function. These weighting factors are hyperparameters that are included in our hyperparameter sweep.

\subsection{Hyperparameter optimization}
\label{app:hyperparameter}

Hyperparameter optimization was performed using the \texttt{Ray Tune} package \citep{liaw_tune_2018} in conjunction with the HyperOpt Python library \citep{bergstra_making_2013} for Bayesian optimization.
The tuned hyperparameters include both those relevant during training---the relative losses $\lambda$, the choice of stochastic interpolant for the atomic coordinates, the parameters for chosen $\gamma(t)$ (if necessary), the sampling scheme, the usage of data-dependent coupling, the batch size, and the learning rate---and during inference---the number of integration steps, the choice of the noises $\varepsilon(t)$ and $\eta$, and the magnitude of the velocity annealing parameter $s$ for both lattice vectors and atomic coordinates. 
Additionally, OMG provides the option to use the Niggli-reduced primitive cell \citep{grosse-kunstleve_numerically_2004, hjorthlarsen_atomic_2017} for training, though we did not include this in the hyperparameter tuning as preliminary tests suggested no difference in model performance on the \textit{MP-20} dataset.
% Further details are provided in Appendix~\ref{app:hyp_metrics}.

\section{Metrics for evaluating learned models} \label{app:hyp_metrics}

\subsection{Match rate and RMSE}

We include here a discussion on the tradeoff between match rate and RMSE, as we noticed this most strongly influences the \textit{perov-5} dataset. 
We show in Fig.~\ref{fig:RMSE_perov} how different interpolants for the atomic coordinates (trigonometric vs. linear) learn to match structures differently. 
For the linear case (which has a depressed match rate) the change in matching tolerance makes little difference; for the trigonometric interpolant it makes a far more significant difference and leads to a much higher match rate, suggesting that the trigonometric interpolant learns structures more reliably but less accurately.

\begin{figure}[th]
   \centering
   \includegraphics[width=0.7\textwidth]{Appendix_Figures/RMSE_comparison.pdf}
   \caption{We show here the effect of making matching more difficult by decreasing the length tolerance used by pymatgen's \texttt{StructureMatcher}. We plot the density of the normalized RMSE distributions from CSP models trained on the \textit{perov-5} dataset. We note that the curves for all generated structures and only valid generated structures overlap significantly.}
   \label{fig:RMSE_perov}
\end{figure}

\subsection{Validity metrics}

We noted that for the compositional validity metric, the version of the SMACT software \citep{davies_smact_2019} changed the computed values by several percent.
This impacts both the match rate (when filtered by valid structures) as well as the reported DNG compositional validity output.
As such, all values for all models were recomputed with the most up-to-date version (3.0) of the SMACT software.

\subsection{Calculation of S.U.N. rates}
\label{sun_calc}
Evaluation of DNG structures was performed using scripts provided by the developers of MatterGen \citep{mattergen_microsoft_2025}. A total of 10,000 structures were generated from each of OMG, DiffCSP, and FlowMM. These structures were then filtered to remove any that contained elements not supported by the MatterSim potential (version \texttt{MatterSim-v1.0.0-1M}) or the reference convex hull. These included heavy elements with atomic numbers $>$89, radioactive elements, and the noble gases (specifically: `Ac', `U', `Th', `Ne', `Tc', `Kr', `Pu', `Np', `Xe', `Pm', `He', `Pa'). Stability and novelty were computed with respect to the default dataset provided by MatterGen which contains 845,997 structures from the \textit{MP-20} \citep{jain_commentary_2013, xie_crystal_2022} and \textit{Alexandria} \citep{schmidt_dataset_2022, schmidt_largescale_2022} datasets. This provides a more challenging reference for computing novelty as each model was trained only on the $\sim$ 27,000 structures from the \textit{MP-20} training set.

\subsection{Structural and stability analysis of generated structures}

In Fig.~\ref{fig:ehull_omg}, we show the distribution of computed energies above the convex hull across various OMG models, showing best stability of generated structures for linear, encoder-decoder, trigonometric, and score-based diffusion (SBD) positional interpolants.


\begin{figure}[th]
   \centering
   \includegraphics[width=0.7\textwidth]{Appendix_Figures/ehull_omg_MMedit.pdf}
   \caption{Histogram of the computed energies above the convex hull for structures generated by four OMG DNG models trained using joint evolution for cell, coordinates, and species. We show that the linear and encoder-decoder positional interpolants are effective at generating more structures close to the convex hull. See Appendix~\ref{sun_calc} for calculation details.}
   \label{fig:ehull_omg}
\end{figure}

By evaluating the distribution of $N$-ary structures (Fig.~\ref{fig:viz_Nary}), the distribution of average coordination numbers (both by structure in Fig.~\ref{fig:viz_CN_struc} and by species in Fig.~\ref{fig:viz_CN_species}), as well as distribution of crystal systems (Fig.~\ref{fig:viz_crystalsystem}) which are related to a structure's Bravais lattice, we provide qualitative analysis for model performance.
Space groups (and thus crystal system) were determined using the \texttt{spglib} software \cite{togo_spglib_2024}.
%We show these metrics for FlowMM (Fig.~\ref{fig:viz_flowmm}), DiffCSP (Fig.~\ref{fig:viz_diffcsp}), OMG with the encoder-decoder interpolant (Fig.~\ref{fig:viz_omg_encdec}), and OMG with the linear interpolant (Fig.~\ref{fig:viz_omg_lin}).
Generally, OMG and DiffCSP showed the best performance in matching the distribution of average coordination number for each structure, particularly for high-coordinated structures.
DiffCSP performed most poorly among the four models on the $N$-ary structural similarity.
The average coordination number for species were best-matched by the two OMG models, and of these two the linear interpolant generated the distribution of crystal systems best.

\begin{figure}[th]
   \centering
   \includegraphics[width=\textwidth]{Appendix_Figures/Nary.pdf}
   \caption{Qualitative performance of the distribution of $N$-ary crystals for \textbf{(a)} Non-OMG models and \textbf{(b)} OMG models across structural benchmarks computed on generated structures and test set structures from the \textit{MP-20} dataset.}
   \label{fig:viz_Nary}
\end{figure}


\begin{figure}[th]
   \centering
   \includegraphics[width=\textwidth]{Appendix_Figures/avgCN_struc.pdf}
   \caption{Qualitative performance of the distribution of average coordination number by structure for \textbf{(a)} Non-OMG models and \textbf{(b)} OMG models across structural benchmarks computed on generated structures and test set structures from the \textit{MP-20} dataset.}
   \label{fig:viz_CN_struc}
\end{figure}

\begin{figure}[th]
   \centering
   \includegraphics[width=\textwidth]{Appendix_Figures/avgCN_species.pdf}
   \caption{Qualitative performance of the distribution of average coordination number by species (listed left to right in order of atomic number) for \textbf{(a)} Non-OMG models and \textbf{(b)} OMG models across structural benchmarks computed on generated structures and test set structures from the \textit{MP-20} dataset.}
   \label{fig:viz_CN_species}
\end{figure}

\begin{figure}[th]
   \centering
   \includegraphics[width=\textwidth]{Appendix_Figures/crystalsystem.pdf}
   \caption{Qualitative performance of the distribution of crystal system by structure for \textbf{(a)} Non-OMG models and \textbf{(b)} OMG models across structural benchmarks computed on generated structures and test set structures from the \textit{MP-20} dataset.}
   \label{fig:viz_crystalsystem}
\end{figure}


% \begin{figure}[th]
%    \centering
%    \includegraphics[width=0.9\textwidth]{Appendix_Figures/omg_flowmm.pdf}
%    \caption{Qualitative performance of FlowMM across structural benchmarks computed on generated structures and test set structures from the \textit{MP-20} dataset. \textbf{(a)} Distribution of $N$-ary crystals. \textbf{(b)} Distribution of crystal systems. \textbf{(c)} Distribution of average coordination number by structure. \textbf{(d)} Distribution of average coordination number by species (listed in order by atomic number).}
%    \label{fig:viz_flowmm}
% \end{figure}

% \begin{figure}[th]
%    \centering
%    \includegraphics[width=0.9\textwidth]{Appendix_Figures/omg_diffcsp.pdf}
%    \caption{Qualitative performance of DiffCSP across structural benchmarks computed on generated structures and test set structures from the \textit{MP-20} dataset. \textbf{(a)} Distribution of $N$-ary crystals. \textbf{(b)} Distribution of crystal systems. \textbf{(c)} Distribution of average coordination number by structure. \textbf{(d)} Distribution of average coordination number by species (listed in order by atomic number).}
%    \label{fig:viz_diffcsp}
% \end{figure}

% \begin{figure}[th]
%    \centering
%    \includegraphics[width=0.9\textwidth]{Appendix_Figures/omg_encdec.pdf}
%    \caption{Qualitative performance of OMG (encoder-decoder positional interpolant) across structural benchmarks computed on generated structures and test set structures from the \textit{MP-20} dataset. \textbf{(a)} Distribution of $N$-ary crystals. \textbf{(b)} Distribution of crystal systems. \textbf{(c)} Distribution of average coordination number by structure. \textbf{(d)} Distribution of average coordination number by species (listed in order by atomic number).}
%    \label{fig:viz_omg_encdec}
% \end{figure}

% \begin{figure}[th]
%    \centering
%    \includegraphics[width=0.9\textwidth]{Appendix_Figures/omg_lin.pdf}
%    \caption{Qualitative performance of OMG (linear positional interpolant) across structural benchmarks computed on generated structures and test set structures from the \textit{MP-20} dataset. \textbf{(a)} Distribution of $N$-ary crystals. \textbf{(b)} Distribution of crystal systems. \textbf{(c)} Distribution of average coordination number by structure. \textbf{(d)} Distribution of average coordination number by species (listed in order by atomic number).}
%    \label{fig:viz_omg_lin}
% \end{figure}

