%\input{Tables/DNG_mp20_results}

During hyperparameter optimization for CSP models, we observed a tradeoff between match rate and RMSE---which is only computed if matched---between the ideal and generated crystals.
By adjusting the relative cost between the position, cell, and species terms in the loss, we observe that the main learning challenge lies in the accurate prediction of the atomic coordinates, which always have a significantly higher relative weight in calculating the full loss function in Eq.~\ref{eq:loss_tot} in the appendix.
Unsurprisingly, we find that as the number of generated structures matching known compositions in the test dataset increases, the structural fidelity of these matches—quantified by the accompanying RMSE—also tends to increase. We quantify this tradeoff in Fig.~\ref{fig:RMSE_perov} in the appendix. However, match rate remains the primary metric of interest, as the generated structure can always be relaxed after generation if seeking minimum energy structures (i.e., at zero temperature).
%\looseness=-1

For the CSP task, OMG significantly outperforms previous approaches on all datasets. We highlight the particularly strong performance of the trigonometric interpolant in achieving a high match rate for the \textit{perov-5} dataset as shown in Tab.~\ref{tab:CSP}.
Unlike other datasets, \textit{perov-5} has a fixed number of atoms $N$ per unit cell and a fixed (cubic) cell with varying side lengths and identical fractional positions.
In contrast, in other datasets, \emph{no unique representation of the periodic repeat unit is imposed on flows}, meaning the model does not learn the invariance (or even equivariance) to the choice of periodic repeat unit.\footnote{Using Niggli reduction during learning to enforce a unique choice of unit cell on structures from our datasets is not sufficient for enforcing this invariance during generation of structures.} This likely contributes to the difficulty of unconstrained flow-based models in generating highly symmetric structures.
The \textit{perov-5} dataset presents a unique case where this invariance does not need to be learned,  making it an ideal benchmark for evaluating which positional interpolant can best learn structural relationship when the species are fixed.
The match rate achieved using the trigonometric interpolant in Tab.~\ref{tab:CSP} surpasses (by $1.6\times$) our best-performing linear interpolant model for \textit{perov-5}, which has a lower match rate of 51.86\% (without filtering invalid atoms) and an RMSE of 0.0757 (state-of-the-art for this metric).
% We posit that the trigonometric interpolant is particularly successful as it allows for particles to traverse flows that are more circuitous than the geodesic path imposed by the linear interpolant---which may be advantageous for the CSP task since the atomic identities are fixed to each particle as they flow along the learned velocity fields during generation.
It is possible that the superior performance of the trigonometric interpolant arises from its ability to generate more circuitous flow trajectories compared to the strictly geodesic paths imposed by the linear interpolant, akin to the reasoning behind using latent variables to enhance learning in SIs \citep{albergo_stochastic_2023}.
%\looseness=-1

%For \textit{de novo} generation, we evaluate the ability to generate stable, %novel, and unique (S.U.N.) structures in Tab.~\ref{tab:DNG_stability}.
%\mm{comment on the significance of low RMSD and low energy above hull in the DNG task}
%In Fig.~\ref{fig:ehull}, we show the distribution of computed energies above the convex hull across various models, demonstrating OMG's superior performance on stability metrics for generated structures.

Finally, we note trends in Tab.~\ref{tab:OMG_DNG_trends} among the best-performing models identified through hyperparameter optimization.
In particular, we observe a tradeoff between the choice of positional interpolant and the optimal level of `species noise' $\eta$ which sets the probability that an atom will change its identity if already in an unmasked state (see Appendix~\ref{app:SI_DFM}), with linear interpolants favoring higher species noise.
Additionally, we find a correlation between enabling element order permutation (to minimimize the distance between $x_0$ and $x_1$) and the use of geodesic (linear) interpolation paths,  with linear interpolants favoring minimum distance permutation.
Therefore, we conclude that DNG models built on linear interpolants likely benefit from targeted tuning strategies that are unnecessary for non-geodesic positional interpolants.

%\renewcommand{\arraystretch}{1.3}
\begin{table}[htbp]
\caption{Optimally performing DNG trials from OMG. All models are trained with an ODE sampling scheme and ordered by decreasing species noise $\eta$. The values of whether the element ordering is permuted to minimize traveled distance is also included.}
\label{tab:OMG_DNG_trends}
\centering
\resizebox{0.85\columnwidth}{!}{
\begin{tabular}{ccc}
\toprule
Positional & Min. dist. & Species \\ 
Interpolant & permutation & noise \\
\midrule
Linear, $\gamma=0$ & True & 35.46 \\ 
Linear, $\gamma=0$ & True & 31.29 \\ 
Linear, $\gamma=0$ & True & 7.08 \\ 
SBD, $\gamma=0$ & False & 1.95 \\ 
EncDec, $\gamma=\sin^2{\pi t}$ & False & 0.85 \\ 
Trig, $\gamma=\sqrt{1.19t(1-t)}$ & True & 0.81 \\ 
\bottomrule
\end{tabular}
}
\end{table}

% final paragraph
In conclusion, we demonstrate OMG's capabilities for learning the manifold of stable materials and generating novel structures.
We show state-of-the-art performance of SIs across nearly all benchmarks for both crystal structure prediction as well as \textit{de novo} generation of stable, novel, and unique structures.
In our discussion, we highlight several ways in which hyperparameters and interpolation paths influence learning, though a more systematic theoretical analysis is needed to fully understand why specific choices affect model performance. 
Future work can also extend the flexibility of OMG to additional interpolating functions.
We underscore the importance of flexible ML frameworks like OMG, which can adapt to different types of materials datasets by optimizing the generative model accordingly. 
Our work represents a key step forward in applications of machine-learning methods to materials discovery.

