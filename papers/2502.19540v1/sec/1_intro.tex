\section{Introduction}
\label{sec:intro}

\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.5\textwidth]{figures/teaser.pdf}
    \caption{\textbf{Illustration of the proposed component-wise contrastive objectives}. \modelname establishes two discriminative dictionaries at the part and object levels. Within the same semantic level, part/object components of the same classes are pulled closer ($\rightarrow\leftarrow$), while those of different classes are pushed apart ($\leftarrow\rightarrow$) (\ie, contrastive components). At the cross-semantic level, part components and their corresponding object components are pulled closer and vice versa (\ie logical constraints).
    }
    \label{fig:teaser}
\end{figure}

Human perception involves the ability to decompose an object into its semantically meaningful components (\ie, parts). For instance, when observing a dog, humans not only identify it as a dog but also simultaneously discover its head, torso, and other components, facilitating a more interpretable and resilient understanding of real-world scenarios. More specifically, humans can estimate the pose of a dog by considering the spatial arrangement of its parts, even in instances where some parts may be missing. This comprehensive perception enables individuals to make judgments about the potential actions of the observed object.

By contrast, emulating this innate human visual capability presents a big challenge for modern computer vision models. The predominant focus within the field has been on addressing semantic segmentation at the object level, with minimal attention given to intermediate part representations. Notable works~\cite{de2021part,michieli2020gmnet,li2022panoptic,michieli2022edge} in object parsing primarily extend algorithms designed for general segmentation, overlooking the fact that parts, being at a lower semantic level, can be captured more efficiently and interpretably through clustering. As a result, these works often adhere to frameworks tailored for object segmentation without incorporating specialized designs for handling parts. Moreover, even though certain studies~\cite{eslami2012generative,wang2015joint,he2023compositor} highlight the mutual benefit between object parsing and object segmentation, they typically treat these semantic levels separately, disregarding the logical relationship between them. Consequently, the optimization objectives for these two levels are disjoint, leading to sub-optimal predictions.

In this work, we propose \modelname, a general dictionary-based framework aimed at addressing these challenges. \modelname is built on top of an off-the-shelf cluster-based mask transformer, utilizing a set of dictionary components where each component is explicitly associated with a specific semantic class to facilitate the grouping of pixels belonging to that class. This enables \modelname to conduct inference in a straightforward parameter-free manner through nearest neighbor search on the pixel feature maps within the class dictionary. Taking this concept further, \modelname introduces a hierarchical formulation of dictionary components, aligning with the semantic hierarchy, which naturally forms the logical paths within the structure (\eg, bird-head $\rightarrow$ bird).
\modelname advances the learning of the above formulation through two simple yet effective targets: learning contrastive objectives for obtaining discriminative dictionary components and exploring logical relations for consistent predictions. 
Specifically, as depicted in Fig.~\ref{fig:teaser}, at each semantic level, \modelname employs a component-wise contrastive algorithm to pull closer the dictionary components withing the same class while pushing away those from different classes, thus a better-structured dictionary space is derived, ultimately improving the performance of object parsing. Then to model the cross-level logical relations, \modelname further contrasts the positive pair between dictionary component representing a particular part and its corresponding object dictionary components against the negative pairs involving the part component and all other object components. 
For further enhancement of logical constraints during testing, \modelname implements a post-processing function inspired by the principle that a pixel of a given part class must also be predicted as its corresponding object class. More precisely, \modelname enables this ability by calculating the logical path probability through multiplying the part-level similarity and object-level similarity. Subsequently, \modelname assigns each pixel with the class labels in the top-scoring path. This approach effectively captures the cross-level semantic information and corrects potential cross-level inconsistencies during inference. In summary, our contributions in this work include:
\begin{enumerate}
    \item We present \modelname, a versatile dictionary-based framework tailored for object parsing and can be integrated with various cluster-based mask transformers.
    \item We propose a component-wise contrastive learning method designed to enhance the learning of discriminative dictionary components and foster the development of a well-structured dictionary space.
    \item We introduce logical constraints for object parsing, leveraging inherent semantic hierarchy information to alleviate cross-level inconsistency.
    \item We validate the effectiveness of \modelname through extensive experiments on PartImageNet and Pascal-Part-108. The incorporation of the above modules notably improves performance on both the part and the object level.
\end{enumerate}



