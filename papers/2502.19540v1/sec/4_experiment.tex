\section{Experiments}
\label{sec:experiment}

\begin{table*}[t!]
\caption{PartImageNet \textit{val} set and Pascal-Part-108 \textit{test} set results. mIoU on parts and super-category, mAvg are reported. Reported results are averaged over 3 runs.
}
\centering
\subfloat[
\textbf{PartImageNet \textit{val} set results}
\label{tab:partimagenet}
]{
\centering
\begin{minipage}{0.59\linewidth}{\begin{center}
\tablestyle{4pt}{1.2}
\scalebox{0.8}{
\begin{tabular}{l|c|cc}
\multirow{2}{*}{method} & \multirow{2}{*}{backbone} & \multicolumn{2}{c}{mIoU} \\ \cline{3-4} 
 &  & \multicolumn{1}{c|}{Part} & Super-Category \\ \hline
DeepLabv3+~\cite{chen2018encoder} & ResNet-50~\cite{he2016deep} & \multicolumn{1}{c|}{60.57} & - \\
MaskFormer~\cite{cheng2021per} & ResNet-50~\cite{he2016deep} & \multicolumn{1}{c|}{60.34} & - \\
Compositor~\cite{he2023compositor} & ResNet-50~\cite{he2016deep} & \multicolumn{1}{c|}{61.44} & - \\
kMaX-DeepLab~\cite{yu2022k} & ResNet-50~\cite{he2016deep} & \multicolumn{1}{c|}{65.75} & 89.16 \\ 
\modelname & ResNet-50~\cite{he2016deep} & \multicolumn{1}{c|}{\textbf{67.83}} & \textbf{90.41} \\ \hline
SegFormer~\cite{xie2021segformer} & MiT-B2~\cite{xie2021segformer} & \multicolumn{1}{c|}{61.97} & - \\
MaskFormer~\cite{cheng2021per} & Swin-T~\cite{liu2021swin} & \multicolumn{1}{c|}{63.96} & - \\
Compositor~\cite{he2023compositor} & Swin-T~\cite{liu2021swin} & \multicolumn{1}{c|}{64.64} & - \\
kMaX-DeepLab~\cite{yu2022k} & ConvNeXt-T~\cite{liu2022convnet} & \multicolumn{1}{c|}{68.52} & 91.34 \\
\modelname & ConvNeXt-T~\cite{liu2022convnet} & \multicolumn{1}{c|}{\textbf{70.31}} & \textbf{92.65} \\
\end{tabular}
}
\end{center}}\end{minipage}
}
\subfloat[
\textbf{Pascal-Part-108 \textit{test} set results}
\label{tab:pascal}
]{
\centering
\begin{minipage}{0.39\linewidth}{\begin{center}
\tablestyle{4pt}{1.2}
\scalebox{0.8}{
\begin{tabular}{l|c|c}
method & Part mIoU & mAvg \\ \hline
SegNet~\cite{badrinarayanan2017segnet} & 18.6 & 20.8 \\
FCN~\cite{long2015fully} & 31.6 & 33.8 \\
DeepLab~\cite{chen2017deeplab} & 31.6 & 40.8 \\
DeepLabv3+~\cite{chen2018encoder} & 46.5 & 48.9 \\
BSANet~\cite{zhao2019multi} & 42.9 & 46.3 \\
GMNet~\cite{michieli2020gmnet} & 45.8 & 50.5 \\
FLOAT~\cite{singh2022float} & 48.0 & \textbf{53.0} \\ 
HSSN~\cite{li2022deep} & 48.3 & - \\
DeepLabv3+~\cite{chen2018encoder}+ LOGICSEG~\cite{li2023logicseg} & 49.1 & - \\ \hline 
kMaX-DeepLab~\cite{yu2022k} & 48.3 & 49.9 \\
\modelname & \textbf{49.8} & 52.0
\end{tabular}
}
\end{center}}\end{minipage}
}
\end{table*}

In this section, we first provide the experimental setup, followed by the main results on PartImageNet~\cite{he2022partimagenet} and Pascal-Part-108~\cite{michieli2020gmnet}. We conduct ablation studies on PartImageNet to demonstrate the effectiveness of our designs. We also provide visualizations to better understand \modelname.

\subsection{Experimental Setup}
\textbf{Datasets}\quad We conduct experiments on two popular object parsing benchmarks: PartImageNet~\cite{he2022partimagenet} and PASCAL-Part-108~\cite{michieli2020gmnet}. We provide the detailed statistics of each dataset and the class definitions below:
\begin{itemize}
    \item PartImageNet~\cite{he2022partimagenet} contains 24095 elaborately annotated general images from ImageNet~\cite{deng2009imagenet}, which are split into 20481/1206/2408 for \textit{train}/\textit{val}/\textit{test}. It is associated with 40 part classes, which are grouped into 11 object classes following the official class definition.
    \item Pascal-Part-108~\cite{michieli2020gmnet} expands upon the part definition introduced in Pascal-Part-58~\cite{chen2014detect}, providing a more intricate benchmark with finer part-level details. This extension maintains the original split of VOC~\cite{pascal-voc-2010} and encompasses a dataset of 10,103 images across 20 object classes and 108 part classes. Our experiments adhere to the original split, utilizing 4,998 images for training and 5,105 images for testing.
\end{itemize}

\noindent \textbf{Evaluation Metrics}\quad
We evaluate the performance of \modelname on the PartImageNet dataset~\cite{he2022partimagenet} using the mean Intersection over Union (mIoU) on both part and super-category levels. It's important to note that for PartImageNet, we choose to report performance on the super-category level because the parts in PartImageNet are defined within the context of super-category. The hierarchy of super-category is inherited for training \modelname on this dataset.
In the case of Pascal-Part-108, our evaluation includes reporting part mIoU, and additionally, we calculate the mAvg on the object level. The mAvg metric, as defined in the literature~\cite{zhao2019multi}, provides the average mIoU score of all parts belonging to an object. We refer the reader to FLOAT~\cite{singh2022float} for a detailed explanation of these metrics.

\noindent \textbf{Training details}\quad 
We implement \modelname based on the kMaX-DeepLab architecture~\cite{yu2022k}, utilizing its official PyTorch re-implementation codebase. To ensure a fair comparison, we adopt the training settings from kMaX-DeepLab. 
The backbone, pretrained on ImageNet~\cite{he2016deep,liu2022convnet}, followed a learning rate multiplier of 0.1. For regularization and augmentations, we incorporate drop path~\cite{huang2016deep} and random color jittering~\cite{cubuk2018autoaugment}. The optimizer used is AdamW~\cite{kingma2014adam,loshchilov2018decoupled} with a weight decay of 0.05.
Unless otherwise specified, we train all models with a batch size of 64 on a single A100 GPU, performing 40,000 iterations on PartImageNet~\cite{he2022partimagenet} and 10,000 iterations on Pascal-Part-108~\cite{chen2014detect}. The first 2,000 and 500 steps serve as the warm-up stage, where the learning rate linearly increases from 0 to $5 \times 10^{-4}$.
The training objective for \modelname includes the combination of kMaX-DeepLab's original losses and the proposed contrastive loss terms, as specified in Eq.~\ref{formula:contrast_p}, Eq.~\ref{formula:contrast_o} and Eq.~\ref{formula:contrast_cross}:
\begin{align*}
\mathcal{L} = &\lambda_{\text{kMaX}}\mathcal{L}_{\text{kMaX}} + \lambda_{p\_con}\mathcal{L}_{p\_con} + \\
&\lambda_{o\_con}\mathcal{L}_{o\_con} + \lambda_{logic}\mathcal{L}_{logic}.
\end{align*}
Here, $\mathcal{L}_{\text{kMaX}}$ represents the loss from kMaX-DeepLab~\cite{yu2022k}, and $\lambda_{\text{kMaX}}$ follows the official setting. The weights for the proposed loss terms are set to $\lambda_{p\_con} = 2$, $\lambda_{o\_con} = 2$, and $\lambda_{logic} = 1$.
\modelname uses the exact same number of part and object queries corresponding to the part and object classes in the dataset. Specifically, we set $P$ to 41 and 109, and $\widetilde{P}$ to 12 and 21 (with one additional learnable component for representing the background at both the part and object levels) in PartImageNet and Pascal-Part-108, respectively. This design enables a straightforward and highly interpretable inference process, using nearest neighbor search for parts and objects separately during inference. Afterward, we compute the top-scoring logical path and reassign the predicted classes based on that path.

\subsection{Main Results}
Our main results on the PartImageNet~\cite{he2022partimagenet} \textit{val} set and PASCAL-Person-Part~\cite{xia2017joint} \textit{test} set are summarized in Tab.~\ref{tab:partimagenet} and Tab.~\ref{tab:pascal}, respectively.

\noindent \textbf{PartImageNet \textit{val} set}\quad In Table~\ref{tab:partimagenet}, we present a comparison between \modelname and several classic segmentation models on the PartImageNet \textit{val} set. As a strong cluster-based mask transformer, kMaX-DeepLab~\cite{yu2022k} already surpasses previous works by a substantial margin. Particularly, with a ResNet-50~\cite{he2016deep} as the backbone, \modelname achieves a significant 2.08\% improvement in part mIoU over kMaX-DeepLab. Using the more powerful ConvNeXt-Tiny~\cite{liu2022convnet} as the backbone, \modelname further elevates the performance to 70.31 part mIoU, surpassing kMaX-DeepLab~\cite{yu2022k} with the same backbone by 1.79\% part mIoU. 
Notably, \modelname consistently enhances super-category segmentation results in comparison to kMaX-DeepLab. With ResNet-50 as the backbone, we observe an improvement of 1.25\%, while with ConvNeXt-Tiny, the enhancement reaches 1.31\%.

\noindent \textbf{Pascal-Part-108 \textit{test} set}\quad In Tab.~\ref{tab:pascal}, we summarize \modelname's performance on Pascal-Part-108 \textit{test} set against other methods. All the models utilize ResNet-101~\cite{he2016deep} as the backbone. As observed in the table, \modelname achieves the best performance, setting new state-of-the-art results with 49.8 part mIoU. Notably, \modelname outperforms the previous state-of-the-art method LOGISEG~\cite{li2023logicseg} and the strong baseline kMaX-DeepLab~\cite{yu2022k} by a substantial 0.7\% and 1.5\% in part mIoU, respectively. In terms of object segmentation, \modelname demonstrates a notable improvement over kMaX-DeepLab, achieving a substantial 2.1\% increase. This underscores \modelname's capability not only to refine parsing to a finer granularity but also to enhance overall segmentation quality.

\begin{figure*}[t!]
    \centering
    \includegraphics[width=\textwidth]{figures/visual.pdf}
    \caption{\textbf{Qualitative comparison for \modelname and kMaX-DeepLab on PartImageNet.} Note that \modelname produces much more accurate object parsing results with precise boundaries (\eg, row 1) and fewer missed detections (\eg, row 2 \& 3).}
    \label{fig:vis}
\end{figure*}

\subsection{Qualitative Results}
Fig.~\ref{fig:vis} depicts three representative visual results on PartImageNet. As seen, \modelname yields better object parsing results compared to kMaX-DeepLab~\cite{yu2022k} by yielding more accurate boundaries (\eg, row 1) and detecting parts that are missed by kMaX-DeepLab (\eg, row 2 \& 3). 

\subsection{Ablation Studies}
\label{subsec:ablation-studies}

\begin{table}[t]
% \small
\centering
\caption{Ablation study on individual module designs for \modelname on PartImageNet \textit{val} set. All models use ResNet-50~\cite{he2016deep}.}
% \setlength{\tabcolsep}{4mm}
\tablestyle{3pt}{1.0}
\begin{tabular}{l|cccc|c}
method & Dictionary & $\mathcal{L}_{p\_con}$ & $\mathcal{L}_{o\_con}$ & $\mathcal{L}_{logic}$ & Part mIoU \\ \hline
 & \XSolidBrush & \XSolidBrush & \XSolidBrush & \XSolidBrush & 65.75 \\
 & \checkmark & \XSolidBrush & \XSolidBrush & \XSolidBrush & 64.31 \\
 \modelname & \checkmark & \checkmark & \XSolidBrush & \XSolidBrush & 65.87 \\ 
 & \checkmark & \checkmark & \checkmark & \XSolidBrush & 66.53 \\
 & \checkmark & \checkmark & \checkmark & \checkmark & \textbf{67.83}    
\end{tabular}
\label{tab:moduleablate}
\end{table}



\noindent \textbf{Evaluating the Impact of Dictionary Components, Contrastive Components, and Logical Constraints}\quad In Table~\ref{tab:moduleablate}, we conduct ablation studies to assess the impact of our core design components on \modelname. Our findings reveal that simply adapting kMaX-DeepLab to our proposed dictionary-based mask transformer, by incorporating dictionary components, does not inherently enhance performance. In fact, the model's part mIoU on PartImageNet declines from 65.75 to 64.31. This drop is attributed to the insufficient discriminative power of the dictionary components, which, in the absence of contrastive loss supervision, leads to ambiguity during the nearest neighbor search among similar parts.
We then incorporate contrastive learning objectives into the dictionary-based mask transformer, resulting in a marked improvement of 2.22\% in part mIoU. Specifically, adding contrastive supervision on parts through $\mathcal{L}_{p\_con}$ brings a 1.56\% improvement, while additional contrast on object-level targets $\mathcal{L}_{o\_con}$ brings another 0.66\% improvement. Notably, this performance surpasses the baseline kMaX-DeepLab by 0.78\% in mIoU, supporting our hypothesis that cultivating a discriminative dictionary is crucial for the effective functioning of the dictionary-based mask transformer.
In the final phase of our ablation study, we integrate logical constraints into the model, which brings a notable 1.30\% improvement, establishing a new state-of-the-art performance on the PartImageNet \textit{val} set with ResNet-50.

\noindent \textbf{Impact of Memory Bank Size $S$}\quad Table~\ref{tab:memory} examines the effect of varying the size of the memory bank. A notable observation is the performance degradation when $S$ is reduced to 50. This decline suggests that a smaller memory bank size is inadequate in providing a sufficient number of samples for effective contrastive learning objectives. Conversely, expanding the memory bank size to 150 and 200 also results in a gradual decrease in performance. This decline could be attributed to the limited diversity of instances in the dataset. In such cases, an oversized memory bank may lead to redundancy in samples, which adversely affects the learning process for the dictionary components. This finding underscores the importance of optimizing the memory bank size to balance the need for sufficient sample diversity without introducing detrimental redundancy.

\noindent \textbf{Influence of the Number of Negative Samples $k$}\quad In Table~\ref{tab:negative}, we examine the influence of varying the number of negative samples, denoted as $k$. The findings illustrate a discernible trend: an insufficient number of negative samples corresponds to a decline in performance from 67.83 to 66.28 part mIoU. This suggests that a limited pool of negative samples may not provide sufficient challenge or diversity to effectively train the model. Conversely, excessively increasing the number of negative samples can also have detrimental effects. Specifically, an overabundance of negatives can lead to a scenario where the model's learning is dominated by 'easy' negatives, ultimately resulting in suboptimal performance.

\begin{table}[t!]
\caption{Ablation study on number of memory bank size $S$ and negative samples $k$ for \modelname with ResNet-50 as backbone on PartImageNet \textit{val} set.}
\centering
\subfloat[
\textbf{Ablation on number of memory bank size $S$}
\label{tab:memory}
]{
\centering
\begin{minipage}{0.49\linewidth}{\begin{center}
\tablestyle{3pt}{1.2}
\scalebox{1.0}{
\begin{tabular}{c|c}
    \# memory bank $S$ & Part mIoU \\ \hline
    50                 & 66.50 \\
    100                & \textbf{67.83} \\
    150                & 67.16 \\
    200                & 67.02  
\end{tabular}
}
\end{center}}\end{minipage}
}
\subfloat[
\textbf{Ablation on number of negative samples $k$}
\label{tab:negative}
]{
\centering
\begin{minipage}{0.49\linewidth}{\begin{center}
\tablestyle{3pt}{1.2}
\scalebox{1.0}{
\begin{tabular}{c|c}
    \# negative sample $k$ & Part mIoU \\ \hline
    50                & 66.28 \\
    100               & \textbf{67.83} \\
    200               & 66.40 \\
    all               & 65.74 
\end{tabular}
}
\end{center}}\end{minipage}
}
\end{table}


\begin{table}[h]
% \vspace{-3mm}
% \scriptsize
\centering
\caption{Performance of \modelname with different baselines using ResNet-50 as backbone on PartImageNet \textit{val} set.}
\begin{tabular}{l|cc}
\multirow{2}{*}{method} & \multicolumn{2}{c}{mIoU} \\ \cline{2-3} 
 & \multicolumn{1}{c|}{Part} & Super-Category \\ \hline
MaskFormer~\cite{cheng2021per} & \multicolumn{1}{c|}{60.34} & - \\
CoCal (MaskFormer) & \multicolumn{1}{c|}{63.52} & 86.67 \\ \hline
Mask2Former~\cite{cheng2022masked} & \multicolumn{1}{c|}{63.62} & 87.20 \\
CoCal (Mask2Former) & \multicolumn{1}{c|}{66.39} & 88.72
\end{tabular}
% \vspace{-6mm}
\label{tab:baseline}
\end{table}

\noindent \textbf{Generalizability of \modelname}\quad In Table~\ref{tab:baseline}, we evaluate the generalizability of \modelname using two baseline models. Incorporating \modelname into MaskFormer~\cite{cheng2021per} and Mask2Former~\cite{cheng2022masked} results in part mIoU improvements of 3.18 and 2.77, respectively. To be more specific, we change the cross-attention in MaskFormer to soft clustering attention~\cite{he2023compositor} in order to integrate with \modelname. Besides, we decrease the number of queries by changing Hungarian Matching to fixed matching mechanism. This experiment illustrates that \modelname can seamlessly integrate into various modern segmentation frameworks, consistently enhancing performance across different architectures.
