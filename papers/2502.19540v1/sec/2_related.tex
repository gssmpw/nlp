\section{Related Work}
\label{sec:related}

\subsection{Object Parsing}
The extensive literature on object parsing can be divided into single-object multi-part parsing~\cite{hariharan2015hypercolumns,xia2016zoom,chen2016attention,liang2018look,xia2017joint,nie2018mutual} and multi-object multi-part parsing~\cite{zhao2019multi,michieli2020gmnet,singh2022float,he2023compositor}. Single-object multi-part parsing has primarily focused on specific classes, such as humans~\cite{zhu2011max,liang2015deep,yamaguchi2012parsing}, animals~\cite{wang2015semantic}, and vehicles~\cite{eslami2012generative,lu2014parsing,song2017embedding}. While the methodologies addressing multi-object multi-part parsing mainly focus on employing top-down or coarse-to-fine strategies. 
Specifically, Singh et al.~\cite{singh2022float} proposed FLOAT, a factorized top-down parsing framework by first detecting the object followed with zooming in for obtaining higher quality part masks. On the contrary, He et al.~\cite{he2023compositor} introduced Compositor, a bottom-up architecture designed to iteratively learn objects by clustering pixels to derive parts. 
Recently, there are also explorations in the closely related area of panoptic part segmentation within the research community. Notable works such as~\cite{de2021part,li2022panoptic,verboeket2022hierarchical,li2023panopticpartformer++,alt2023efficientpps,muralidhara2024jppf} have delved into the semantic parsing of objects while also distinguishing parts between different instances. However, a common trend in these works, whether focused on semantic object parsing or panoptic part segmentation, involves extending standard segmentation models, often overlooking the nuanced semantic levels of parts. In contrast, \modelname takes a novel approach by focusing specifically on semantic object parsing. It redefines the paradigm of cluster-based mask transformers and introduces a novel dictionary-based framework meticulously tailored for object parsing.

\subsection{Cluster-based Mask Transformer}
With the recent progress in transformers \cite{carion2020end}, a new paradigm named mask classification~\cite{wang2020axial,wang2021max,cheng2021per,strudel2021segmenter,zhang2021k,cheng2022masked} has been proposed, where segmentation predictions are represented by a set of binary masks with its class label, which is generated through the conversion of object queries to mask embedding vectors followed by multiplying with the image features. The predicted masks are trained by Hungarian matching with ground truth masks. Thus the essential component of mask transformers is the decoder which takes object queries as input and gradually transfers them into mask embedding vectors. Recently, cluster-based mask transformers are introduced in \cite{yu2022cmt,yu2022k,liang2023clustseg}, which rethinks the design of the decoder by replacing the cross-attention with a k-means~\cite{lloyd1982least} attention. Building upon these explorations, \modelname introduces a global class dictionary and replaces the Hungarian matching with a fixed one-to-one matching, thereby establishing an interpretable dictionary-based framework for part segmentation.

\subsection{Contrastive Learning in Segmentation}
Contrastive learning~\cite{robinson2020contrastive,chen2020simple,khosla2020supervised,chen2020big,he2020momentum,chen2020improved,he2022masked} has emerged as a prominent technique in computer vision as an effective method for learning feature representation for self-supervised models. The core idea lies in contrasting similar (positive) data pairs against dissimilar (negative) pairs. Recently, Wang et al.~\cite{wang2021exploring} raise a pixel-to-pixel contrastive learning method for semantic segmentation, which enforces pixel embeddings belonging to the same semantic class to be more similar than embeddings from different classes. \cite{du2022weakly,li2023contrast,chen2023pipa,lv2023confidence,wang2023space,xie2023sepico} are built upon this concept, extending it to various segmentation domains. Motivated by these advancements, we propose a component-wise contrastive learning method tailored for modern cluster-based mask transformers, which effectively learns discriminative dictionary components within the clustering scheme.  

\subsection{Logical Constraints in Segmentation}
Few segmentation models~\cite{liang2018dynamic,xiao2018unified,wang2019learning,li2020deep,wang2020hierarchical,li2022deep,ke2022unsupervised,li2023logicseg} consider the implicit logic rules inherent in structured labels. While the majority of them are dedicated to human parsing, a few recent works~\cite{li2022deep,li2023logicseg} tackle the general segmentation in a flexible function and avoid incorporating label taxonomies into the network topology. Concretely, Li et al.~\cite{li2022deep} enhance the logical consistency by modeling the segmentation as a pixel-wise multi-label classification. Li et al.~\cite{li2023logicseg} exploit neuro-symbolic computing for grounding logical formulae onto data. In contrast to these efforts, \modelname introduces an object level on top of the part and models logical rules as a contrastive objective during training. 
