\section{Related Work}
In this section, we review the typical work related to this paper, including GFL, PFL, and Spectral GNNs.

\subsection{Graph Federated Learning}
GFL aims to utilize the distributed learning framework to collaboratively train GNNs while maintaining the privacy and security of the local graphs. Most of the existing GFL methods can be categorized into two types: optimization-based methods and model-based methods. On one hand, optimization-based methods focus on optimizing the collaboration strengths between clients, often leveraging conventional GNNs (**Konevcyn, "Graph Convolutional Networks"**). For instance, **McMahan et al., "FED-PUB"** determines client collaboration strengths according to the estimated similarities between subgraphs based on the outputs of local GCN-based models. Similarly, **Li et al., "FedGTA"** introduces the optimization-driven federated learning algorithm that adjusts the weight of each client's contribution based on the mixed moments of processed neighbor features. On the other hand, model-based methods focus on designing specific local models while using simple federation strategies (**McMahan et al., "FedAvg"**). For example, **Kottmann et al., "FedSage+"** builds on **Huang et al., "FedSage"** by introducing a missing neighbor generator to address missing links across local subgraphs, while **Huang et al.** combines GraphSAGE with FedAvg. To deal with the structure heterogeneity, **Wang et al., "AdaFGL"** introduces homophilous and heterophilous propagation modules for each client so that each client's model is adapted to the local graph structures. Besides, **Kottmann et al., "FedTAD"** enhances the knowledge transfer from the local models to the global model, alleviating the negative impact of unreliable knowledge caused by node feature heterogeneity. However, both optimization-based methods and model-based methods have performance limitations because they overlook the critical homophily heterogeneity, which leads to inconsistent spectral properties across different clients. Therefore, we naturally introduce the spectral GNN as the local model to mine graph spectral properties for GFL.

\subsection{Personalized Federated Learning}
Heterogeneity presents a fundamental challenge in GFL (**Konevcyn et al., "Graph Federated Learning"**). To deal with the heterogeneity, PFL methods (**Wang et al., "Personalized Federated Learning"**) have obtained increasing attention. Unlike **McMahan et al., "FedAvg,"** which aims to train a global model collaboratively, PFL methods aim to train a personalized model for each client. Most of existing PFL methods can be categorized as similarity-based methods (**Konevcyn et al., "Similarity-Based Personalized Federated Learning"**), local customization-based methods (**Li et al., "Local Customization-Based Personalized Federated Learning"**), and meta-learning-based methods (**Wang et al., "Meta-Learning-Based Personalized Federated Learning"**). Similarity-based methods first compute the inter-client similarities and then perform the weighted federation of local models for each client based on these similarities. Specifically, clients with larger similarity scores are assigned larger weights for federated aggregation. In contrast, local customization-based methods, such as **Li et al., "FedProx,"** incorporate a proximal term to customize personalized models for each client. Similarly, **Konevcyn et al., "FedALA,"** captures the desired information from the global model in an element-wise manner and then aggregates the local models. As an alternative, **Wang et al., "FedPer,"** focuses on federating the backbone weights while training a personalized classification layer locally on each client. Additionally, meta-learning-based methods, such as **Konevcyn et al., "Meta-Learning-Based Personalized Federated Learning,"** focus on discovering an initial shared model that can be efficiently adapted to each client, enabling the creation of personalized local models. Due to the simplicity and effectiveness of similarity-based methods, we concentrate on similarity-based PFL methods in this paper. However, similarity-based PFL methods only consider the similarities between pairwise clients, while ignoring the critical complementarities between clients. Therefore, according to our theoretical findings, we propose to pursue not only similarities between different clients but also complementarities between different clients.

\subsection{Spectral Graph Neural Networks}
Spectral GNNs represent a class of GNNs that operate by designing graph signal filters in the spectral domain (**Kottmann et al., "Spectral Graph Neural Networks"**). Specifically, they approximate filtering operations using polynomial bases of Laplacian eigenvalues (**Wang et al., "Polynomial Bases of Laplacian Eigenvalues"**). For example, **Huang et al., "ChebNet,"** employs a $K$-order truncated Chebyshev polynomial basis to implement a $K$-hop localized filtering. However, **Chien et al.,** argue that the depth of ChebNet is limited in practice due to the over-smoothing phenomenon. To address this issue, they propose a Generalized PageRank (GPR) GNN (**Chen et al., "Generalized PageRank GNN,"**) that adaptively learns the GPR weights to control the contribution of each propagation step. However, the above-mentioned spectral GNNs learn the graph signal filters without a clear constraint, which may lead to oversimplified or ill-posed filters. To overcome this problem, **Konevcyn et al., "BernNet,"** estimates the normalized Laplacian spectrum by a $K$-order Bernstein polynomial basis and learns the polynomial coefficients based on the observed graphs. Meanwhile, **Wang et al., "Expressive Power of Spectral GNNs,"** theoretically analyze the expressive power of spectral GNNs and find the advantage of orthogonal polynomial bases. Inspired by their theoretical findings, they propose JacobiConv (**Kottmann et al., "JacobiConv"**), which is based on the Jacobi polynomial basis due to its orthogonality and flexibility. Furthermore, **Huang et al., "OptBasisGNN,"** learns an orthogonal polynomial basis directly from the graph data. Nevertheless, the above-mentioned spectral GNNs can not deal with the varying homophily levels. To solve this problem, **Chen et al., "Universal Polynomial Bases,"** propose the universal polynomial bases. Inspired by this work, we not only introduce the spectral GNN as the local model to tackle the issue of homophily heterogeneity across different clients, but also propose a novel federated aggregation method from the perspective of polynomial bases.