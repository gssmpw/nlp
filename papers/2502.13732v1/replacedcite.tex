\section{Related Work}
In this section, we review the typical work related to this paper, including GFL, PFL, and Spectral GNNs.

\subsection{Graph Federated Learning}
GFL aims to utilize the distributed learning framework to collaboratively train GNNs while maintaining the privacy and security of the local graphs. Most of the existing GFL methods can be categorized into two types: optimization-based methods and model-based methods. On one hand, optimization-based methods focus on optimizing the collaboration strengths between clients, often leveraging conventional GNNs (\textit{e.g.}, Graph Convolutional Networks (GCN)____). For instance, FED-PUB____ determines client collaboration strengths according to the estimated similarities between subgraphs based on the outputs of local GCN-based models. Similarly, FedGTA____ introduces the optimization-driven federated learning algorithm that adjusts the weight of each client's contribution based on the mixed moments of processed neighbor features. On the other hand, model-based methods focus on designing specific local models while using simple federation strategies (\textit{e.g.}, FedAvg____). For example, FedSage+____ builds on FedSage____ by introducing a missing neighbor generator to address missing links across local subgraphs, while FedSage itself combines GraphSAGE____ with FedAvg. To deal with the structure heterogeneity, AdaFGL____ introduces homophilous and heterophilous propagation modules for each client so that each client's model is adapted to the local graph structures. Besides, FedTAD____ enhances the knowledge transfer from the local models to the global model, alleviating the negative impact of unreliable knowledge caused by node feature heterogeneity. However, both optimization-based methods and model-based methods have performance limitations because they overlook the critical homophily heterogeneity, which leads to inconsistent spectral properties across different clients. Therefore, we naturally introduce the spectral GNN as the local model to mine graph spectral properties for GFL.

\subsection{Personalized Federated Learning}
Heterogeneity presents a fundamental challenge in GFL____. To deal with the heterogeneity, PFL methods____ have obtained increasing attention. Unlike FedAvg, which aims to train a global model collaboratively, PFL methods aim to train a personalized model for each client. Most of existing PFL methods can be categorized as similarity-based methods____, local customization-based methods____, and meta-learning-based methods____. Similarity-based methods first compute the inter-client similarities and then perform the weighted federation of local models for each client based on these similarities. Specifically, clients with larger similarity scores are assigned larger weights for federated aggregation. In contrast, local customization-based methods, such as FedProx____, incorporate a proximal term to customize personalized models for each client. Similarly, FedALA____ captures the desired information from the global model in an element-wise manner and then aggregates the local models. As an alternative, FedPer____ focuses on federating the backbone weights while training a personalized classification layer locally on each client. Additionally, meta-learning-based methods, such as____, focus on discovering an initial shared model that can be efficiently adapted to each client, enabling the creation of personalized local models. Due to the simplicity and effectiveness of similarity-based methods, we concentrate on similarity-based PFL methods in this paper. However, similarity-based PFL methods only consider the similarities between pairwise clients, while ignoring the critical complementarities between clients. Therefore, according to our theoretical findings, we propose to pursue not only similarities between different clients but also complementarities between different clients. 

\subsection{Spectral Graph Neural Networks}
Spectral GNNs represent a class of GNNs that operate by designing graph signal filters in the spectral domain____. Specifically, they approximate filtering operations using polynomial bases of Laplacian eigenvalues____. For example, ChebNet____ employs a $K$-order truncated Chebyshev polynomial basis to implement a $K$-hop localized filtering. However, Chien~\textit{et al.}____ argue that the depth of ChebNet is limited in practice due to the over-smoothing phenomenon. To address this issue, they propose a Generalized PageRank (GPR) GNN (\textit{a.k.a.} GPR-GNN____) that adaptively learns the GPR weights to control the contribution of each propagation step. However, the above-mentioned spectral GNNs learn the graph signal filters without a clear constraint, which may lead to oversimplified or ill-posed filters. To overcome this problem, BernNet____ estimates the normalized Laplacian spectrum by a $K$-order Bernstein polynomial basis and learns the polynomial coefficients based on the observed graphs. Meanwhile, Wang \textit{et al.}____ theoretically analyze the expressive power of spectral GNNs and find the advantage of orthogonal polynomial bases. Inspired by their theoretical findings, they propose JacobiConv____, which is based on the Jacobi polynomial basis due to its orthogonality and flexibility. Furthermore, OptBasisGNN____ learns an orthogonal polynomial basis directly from the graph data. Nevertheless, the above-mentioned spectral GNNs can not deal with the varying homophily levels. To solve this problem, Huang \textit{et al.}____ propose the universal polynomial bases. Inspired by this work, we not only introduce the spectral GNN as the local model to tackle the issue of homophily heterogeneity across different clients, but also propose a novel federated aggregation method from the perspective of polynomial bases.