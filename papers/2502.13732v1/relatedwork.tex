\section{Related Work}
In this section, we review the typical work related to this paper, including GFL, PFL, and Spectral GNNs.

\subsection{Graph Federated Learning}
GFL aims to utilize the distributed learning framework to collaboratively train GNNs while maintaining the privacy and security of the local graphs. Most of the existing GFL methods can be categorized into two types: optimization-based methods and model-based methods. On one hand, optimization-based methods focus on optimizing the collaboration strengths between clients, often leveraging conventional GNNs (\textit{e.g.}, Graph Convolutional Networks (GCN)~\cite{kipf2017semisupervised}). For instance, FED-PUB~\cite{baek2023personalized} determines client collaboration strengths according to the estimated similarities between subgraphs based on the outputs of local GCN-based models. Similarly, FedGTA~\cite{li2023fedgta} introduces the optimization-driven federated learning algorithm that adjusts the weight of each client's contribution based on the mixed moments of processed neighbor features. On the other hand, model-based methods focus on designing specific local models while using simple federation strategies (\textit{e.g.}, FedAvg~\cite{mcmahan2017communication}). For example, FedSage+~\cite{NEURIPS2021_34adeb8e} builds on FedSage\cite{NEURIPS2021_34adeb8e} by introducing a missing neighbor generator to address missing links across local subgraphs, while FedSage itself combines GraphSAGE~\cite{NIPS2017_5dd9db5e} with FedAvg. To deal with the structure heterogeneity, AdaFGL~\cite{li2024adafgl} introduces homophilous and heterophilous propagation modules for each client so that each client's model is adapted to the local graph structures. Besides, FedTAD~\cite{zhu2024fedtad} enhances the knowledge transfer from the local models to the global model, alleviating the negative impact of unreliable knowledge caused by node feature heterogeneity. However, both optimization-based methods and model-based methods have performance limitations because they overlook the critical homophily heterogeneity, which leads to inconsistent spectral properties across different clients. Therefore, we naturally introduce the spectral GNN as the local model to mine graph spectral properties for GFL.

\subsection{Personalized Federated Learning}
Heterogeneity presents a fundamental challenge in GFL~\cite{ye2023heterogeneous}. To deal with the heterogeneity, PFL methods~\cite{MLSYS2020_1f5fe839, 9743558, Arivazhagan2019} have obtained increasing attention. Unlike FedAvg, which aims to train a global model collaboratively, PFL methods aim to train a personalized model for each client. Most of existing PFL methods can be categorized as similarity-based methods~\cite{baek2023personalized, li2023fedgta, wentao2025fediih}, local customization-based methods~\cite{MLSYS2020_1f5fe839, Arivazhagan2019, NEURIPS2020_f4f1f13c}, and meta-learning-based methods~\cite{chen2018federated, NEURIPS2020_24389bfe, 10485381}. Similarity-based methods first compute the inter-client similarities and then perform the weighted federation of local models for each client based on these similarities. Specifically, clients with larger similarity scores are assigned larger weights for federated aggregation. In contrast, local customization-based methods, such as FedProx~\cite{MLSYS2020_1f5fe839}, incorporate a proximal term to customize personalized models for each client. Similarly, FedALA~\cite{zhang2023fedala} captures the desired information from the global model in an element-wise manner and then aggregates the local models. As an alternative, FedPer~\cite{Arivazhagan2019} focuses on federating the backbone weights while training a personalized classification layer locally on each client. Additionally, meta-learning-based methods, such as~\cite{NEURIPS2020_24389bfe}, focus on discovering an initial shared model that can be efficiently adapted to each client, enabling the creation of personalized local models. Due to the simplicity and effectiveness of similarity-based methods, we concentrate on similarity-based PFL methods in this paper. However, similarity-based PFL methods only consider the similarities between pairwise clients, while ignoring the critical complementarities between clients. Therefore, according to our theoretical findings, we propose to pursue not only similarities between different clients but also complementarities between different clients. 

\subsection{Spectral Graph Neural Networks}
Spectral GNNs represent a class of GNNs that operate by designing graph signal filters in the spectral domain~\cite{wang2022powerful}. Specifically, they approximate filtering operations using polynomial bases of Laplacian eigenvalues~\cite{guo2023graph}. For example, ChebNet~\cite{defferrard2016convolutional} employs a $K$-order truncated Chebyshev polynomial basis to implement a $K$-hop localized filtering. However, Chien~\textit{et al.}~\cite{chien2021adaptive} argue that the depth of ChebNet is limited in practice due to the over-smoothing phenomenon. To address this issue, they propose a Generalized PageRank (GPR) GNN (\textit{a.k.a.} GPR-GNN~\cite{chien2021adaptive}) that adaptively learns the GPR weights to control the contribution of each propagation step. However, the above-mentioned spectral GNNs learn the graph signal filters without a clear constraint, which may lead to oversimplified or ill-posed filters. To overcome this problem, BernNet~\cite{he2021bernnet} estimates the normalized Laplacian spectrum by a $K$-order Bernstein polynomial basis and learns the polynomial coefficients based on the observed graphs. Meanwhile, Wang \textit{et al.}~\cite{wang2022powerful} theoretically analyze the expressive power of spectral GNNs and find the advantage of orthogonal polynomial bases. Inspired by their theoretical findings, they propose JacobiConv~\cite{wang2022powerful}, which is based on the Jacobi polynomial basis due to its orthogonality and flexibility. Furthermore, OptBasisGNN~\cite{guo2023graph} learns an orthogonal polynomial basis directly from the graph data. Nevertheless, the above-mentioned spectral GNNs can not deal with the varying homophily levels. To solve this problem, Huang \textit{et al.}~\cite{huanguniversal} propose the universal polynomial bases. Inspired by this work, we not only introduce the spectral GNN as the local model to tackle the issue of homophily heterogeneity across different clients, but also propose a novel federated aggregation method from the perspective of polynomial bases.