\documentclass[lettersize,journal]{IEEEtran}
\usepackage{amsmath,amsfonts}
\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{array}
\usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
\usepackage{textcomp}
\usepackage{stfloats}
\usepackage{url}
\usepackage{verbatim}
\usepackage{graphicx}
\usepackage{cite}
\usepackage{bm}

\usepackage{multirow}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{lineno,hyperref}

\newtheorem{thm}{Theorem}
\newtheorem{lem}[thm]{Lemma}
\newtheorem{prop}[thm]{Proposition}
\newtheorem{cor}{Corollary}
\newtheorem{conj}{Conjecture}[section]
\theoremstyle{definition}
\newtheorem{defn}{Definition}
\newtheorem{exmp}{Example}[section]
\newtheorem{rem}{Remark}
\usepackage{lineno,hyperref}


\hyphenation{op-tical net-works semi-conduc-tor IEEE-Xplore}
% updated with editorial comments 8/9/2021

\begin{document}

\title{Homophily Heterogeneity Matters in Graph Federated Learning: A Spectrum Sharing and Complementing Perspective (Appendix)}

\author{Wentao Yu,~\IEEEmembership{Student Member,~IEEE}
        % <-this % stops a space
%\thanks{This paper was produced by the IEEE Publication Technology Group. They are in Piscataway, NJ.}% <-this % stops a space
%\thanks{Manuscript received April 19, 2024; revised August 16, 2024.}
}

% The paper headers
\markboth{Journal of \LaTeX\ Class Files,~Vol.~14, No.~8, August~2024}%
{Shell \MakeLowercase{\textit{et al.}}: A Sample Article Using IEEEtran.cls for IEEE Journals}

% \IEEEpubid{0000--0000/00\$00.00~\copyright~2024 IEEE}
% Remember, if you use this you must call \IEEEpubidadjcol in the second
% column for its text to clear the IEEEpubid mark.

\maketitle

\begin{abstract}
In this appendix, we first provide the proofs for several theorems. After that, we present the details of constructing heterophily bases and the optimization process, respectively. Finally, we illustrate the implementation details of our experiments.
\end{abstract}


\section{Proofs}
\label{sec:Proofs}
In this section, we present the detailed proofs of our proposed theorems.

\subsection{Proof of Theorem 1}
\label{theorem_proportional_proof}
\begin{proof}
First, according to~\cite{huanguniversal}, the Laplacian frequency component of $\mathcal{G}_\mathrm{c}$ can be written as
\begin{equation}
\begin{aligned}
f(\bm{\Theta})&=\mathrm{Tr}(\frac{\bm{\Theta}^{\top} \mathbf{L}_\mathrm{c} \bm{\Theta}}{2})\\
&=\frac{\sum_{\langle i, j\rangle \in \mathcal{E}_\mathrm{c}} \Vert\bm{\theta}_i - \bm{\theta}_j \Vert^2_2 }{2\sum_{i \in \mathcal{V}_\mathrm{c}} \bm{\theta}_{i}^2d_i}.
\end{aligned}
\end{equation}
After several communication rounds of federation, it is reasonable to assume that similar client pairs tend to cluster together, while complementary client pairs exhibit a notable separation.

Second, inspired by~\cite{huanguniversal}, we assume the existence of a constant $\delta$ such that, when $S(i,j) \geq 0.5$, the condition $\Vert\bm{\theta}_i - \bm{\theta}_j\Vert_2 \leq t\delta$ holds for all $i, j \in \mathcal{V}_\mathrm{c}$, where $t \ll 1$ is a small constant. Conversely, when $S(i,j) < 0.5$, the condition $\Vert\bm{\theta}_i - \bm{\theta}_j\Vert_2 = r(\bm{\theta}_i, \bm{\theta}_j)\delta$ holds for all $i, j \in \mathcal{V}_\mathrm{c}$, where $r(\bm{\theta}_i, \bm{\theta}_j) \geq 1$ is a function parameterized by $\bm{\theta}_i$ and $\bm{\theta}_j$. Since $r^2(\bm{\theta}_i, \bm{\theta}_j)$ grows much faster than $t^2$, we can have $t^2 = o(r^2(\bm{\theta}_i, \bm{\theta}_j))$, where ``$o(\cdot)$'' denotes the little-o notation. 

Third, we can have
\begin{equation}
\begin{aligned}
f(\bm{\Theta})&=\frac{\sum_{\langle i, j\rangle \in \mathcal{E}_\mathrm{c}} \Vert\bm{\theta}_i - \bm{\theta}_j \Vert^2_2 }{2\sum_{i \in \mathcal{V}_\mathrm{c}} \bm{\theta}_{i}^2d_i}\\
&=\frac{t^2 \delta^2 r_s |\mathcal{E}_\mathrm{c}| + \sum_{\langle i, j\rangle \in \mathcal{E}_\mathrm{c}, S(i,j) < 0.5}r^2(\bm{\theta}_i,\bm{\theta}_j) \delta^2}{2 \sum_{i \in \mathcal{V}_c}\bm{\theta}_{i}^2d_i}\\
&=\frac{t^2 \delta^2 r_s |\mathcal{E}_\mathrm{c}|}{2 \sum_{i \in \mathcal{V}_\mathrm{c}}\bm{\theta}_{i}^2d_i} + \frac{\sum_{\langle i, j\rangle \in \mathcal{E}_\mathrm{c}, S(i,j) < 0.5}r^2(\bm{\theta}_i, \bm{\theta}_j) \delta^2}{2 \sum_{i \in \mathcal{V}_\mathrm{c}}\bm{\theta}_{i}^2d_i}\\
&=o(r^2(\bm{\theta}_i, \bm{\theta}_j)) + \frac{\sum_{\langle i, j\rangle \in \mathcal{E}_\mathrm{c}, S(i,j) < 0.5}r^2(\bm{\theta}_i, \bm{\theta}_j) \delta^2}{2 \sum_{i \in \mathcal{V}_\mathrm{c}}\bm{\theta}_{i}^2d_i}\\
&= o(r^2(\bm{\theta}_i, \bm{\theta}_j)) + \frac{|\{\langle i, j\rangle \in \mathcal{E}_\mathrm{c}: S(i,j) < 0.5\}|}{2|\mathcal{E}_\mathrm{c}|}\\
% &= o(r^2(\bm{\theta}_i, \bm{\theta}_j)) + \frac{1}{2} r_\mathrm{c}\\
&= \frac{1}{2} r_\mathrm{c} + o(r^2(\bm{\theta}_i, \bm{\theta}_j))\\
\end{aligned}
\label{theorem_proportional_proof_eq1}
\end{equation}
\end{proof}


\subsection{Proof of Theorem 2}
\label{theorem_equivalent_proof}
\begin{proof}
According to the theorem 4 in the main paper, the Laplacian Frequency Component $f(\bm{\Theta})$ can be written as
\begin{equation}
\begin{aligned}
f(\bm{\Theta})&=\mathrm{Tr}(\frac{\bm{\Theta}^{\top} \mathbf{L}_\mathrm{c} \bm{\Theta}}{2})\\
&=\frac{1}{4} \sum_{i=1}^M \sum_{j=1}^M \mathbf{W}_\mathrm{c}^{ij} \Vert \bm{\theta}_i - \bm{\theta}_j\Vert^2_2\\
&=\frac{1}{4} \sum_{\langle i, j\rangle \in \mathcal{E}_\mathrm{c}} \mathbf{W}_\mathrm{c}^{ij} \Vert \bm{\theta}_i - \bm{\theta}_j\Vert^2_2 .
\end{aligned}
\label{theorem_equivalent_proof_eq1}
\end{equation}
Since the measure of heterogeneity $H(\mathcal{G}_\mathrm{c})=\sum_{\langle i, j\rangle \in \mathcal{E}_\mathrm{c}} \mathbf{W}_\mathrm{c}^{ij} \Vert\bm{\theta}_i - \bm{\theta}_j\Vert^2_2 $, we can have $f(\bm{\Theta}) \propto H(\mathcal{G}_\mathrm{c})$. Therefore, we can conclude that the Laplacian frequency component of the federated collaboration graph $\mathcal{G}_\mathrm{c}$ is equivalent to the measure of heterogeneity in GFL.

\end{proof}


\section{Details of Constructing Heterophily Bases}
Recall that a fixed angle $\theta$ is formed between any pairs of heterophily bases to ensure the desired spectral property. To determine the value of $\theta$, Huang~\textit{et al.}~\cite{huanguniversal} empirically set $\theta=\frac{\pi}{2}(1-\hat{h}_m)$, where $\hat{h}_m$ is is the estimated homophily ratio on $\mathcal{G}_\mathrm{m}$. Therefore, the procedure of constructing heterophily bases is to manipulate the fixed angle $\theta$ between any pair of bases. First, we normalize the node feature matrix $\mathbf{X}_m$ by the Frobenius norm and treat it as the zeroth order of heterophily bases (\textit{i.e.}, $\mathbf{U}_m^0$). Second, we employ the orthonormal bases $[\mathbf{V}_m^0,\mathbf{V}_m^1,\cdots,\mathbf{V}_m^K]$ to assist in constructing the heterophily bases. Third, we use the three-term recurrence theorem~\cite{guo2023graph} to compute the orthonormal bases $[\mathbf{V}_m^0,\mathbf{V}_m^1,\cdots,\mathbf{V}_m^K]$. Finally, we update the heterophily bases by $\mathbf{U}_m^k\leftarrow\frac{\mathbf{U}_m^k+\mathbf{T}_m^k\mathbf{V}_m^k}{\Vert\mathbf{U}_m^k+\mathbf{T}_m^k\mathbf{V}_m^k\Vert_F}$, where $\mathbf{T}_m^k\leftarrow\sqrt{\left(\frac{(\mathbf{S}_m^{k-1})^{\top}\mathbf{U}_m^{k-1}}{k\cos\theta}\right)^2-\frac{(k-1)\cos\theta+1}{k}}$. According to~\cite{huanguniversal}, the procedure of constructing heterophily bases is exhibited in Algorithm~\ref{algorithm1}, where `$\leftarrow$' denotes the assignment operation. Moreover, the property of the heterophily bases is proved in~\cite{huanguniversal}, which confirms that the fixed angle between any pair of bases is $\theta$.

\begin{algorithm}[t]
\caption{Construct the Heterophily Bases}\label{algorithm1}

{\textbf{Input:}} Graph $\mathcal{G}_\mathrm{m}$; propagation matrix $\mathbf{P}_\mathrm{m}$; node feature matrix $\mathbf{X}_m$; the order of heterophily bases $K$; and the estimated homophily ratio $\hat{h}_m$.

{\textbf{Output:}} Heterophily bases $\mathbf{U}_m=[\mathbf{U}_m^0,\mathbf{U}_m^1,\cdots,\mathbf{U}_m^K]$.

\begin{algorithmic}[1]
\STATE $\mathbf{U}_m^0\leftarrow \frac{\mathbf{X}_m}{\Vert \mathbf{X}_m \Vert_F}$, $\mathbf{V}_m^0\leftarrow \mathbf{U}_m^0$, $\mathbf{V}_m^{-1}\leftarrow \mathbf{0}$, $\mathbf{S}_m^0\leftarrow \mathbf{U}_m^0$, $\theta \leftarrow \frac{\pi}{2}(1-\hat{h}_m)$;
\STATE \textbf{for} $k \leftarrow 1$ \textbf{to} $K$ \textbf{do}
\STATE \hspace{0.5cm} $\mathbf{V}_m^k\leftarrow \mathbf{P}_\mathrm{m}\mathbf{V}_m^{k-1}$;
\STATE \hspace{0.5cm} $\mathbf{V}_m^k\leftarrow \mathbf{V}_m^k - ((\mathbf{V}_m^k)^{\top}\mathbf{V}_m^{k-1})\mathbf{V}_m^{k-1} - ((\mathbf{V}_m^k)^{\top}\mathbf{V}_m^{k-2})\mathbf{V}_m^{k-2}$;
\STATE \hspace{0.5cm} $\mathbf{V}_m^k \leftarrow \frac{\mathbf{V}_m^k}{\Vert \mathbf{V}_m^k \Vert_F}$, $\mathbf{U}_m^k \leftarrow \frac{\mathbf{S}_m^{k-1}}{k}$, $\mathbf{S}_m^{k-1} \leftarrow \sum_{i=0}^{k-1}\mathbf{U}_m^i$;
\STATE \hspace{0.5cm} $\mathbf{T}_m^k\leftarrow\sqrt{\left(\frac{(\mathbf{S}_m^{k-1})^{\top}\mathbf{U}_m^{k-1}}{k\cos\theta}\right)^2-\frac{(k-1)\cos\theta+1}{k}}$;
\STATE \hspace{0.5cm} $\mathbf{U}_m^k\leftarrow\frac{\mathbf{U}_m^k+\mathbf{T}_m^k\mathbf{V}_m^k}{\Vert\mathbf{U}_m^k+\mathbf{T}_m^k\mathbf{V}_m^k\Vert_F}$, $\mathbf{S}_m^k\leftarrow \mathbf{S}_m^{k-1} + \mathbf{U}_m^k$.
\end{algorithmic}
\end{algorithm}

\section{Details of Optimization}
In this section, we first present the detailed optimization processes of three variables, respectively. Second, we summarize the main steps. Third, we analyze the computational complexity of the entire optimization process.

\subsection{Update \texorpdfstring{$\mathbf{R}$}{} with \texorpdfstring{$\mathbf{W}_c$}{} and \texorpdfstring{$\mathbf{S}$}{} Fixed}
Recall that we have to optimize the following problem, which can be written as
\begin{equation}
\begin{aligned}
&\operatorname*{min}_{\mathbf{O}} \mathrm{Tr}(\mathbf{O}^{\top} \mathcal{L}_\mathrm{c} \mathbf{O}) \Rightarrow \operatorname*{min}_{\mathbf{R}} \mathrm{Tr}(\mathbf{R}\mathbf{P}^{\top} \mathcal{L}_\mathrm{c} \mathbf{P}\mathbf{R})\\
&\mathrm{s.t.} \quad \mathbf{r}^{\top} \mathbf{1} = 1, r_l \geq 0, l=1,2, \cdots, d\\
& \quad \quad \mathbf{R} = \mathrm{diag}(\mathbf{r}).
\end{aligned}
\end{equation}
First, inspired by~\cite{nie2020self}, let us denote the matrix $\mathbf{E} \in \mathbb{R}^{d \times d} = \mathbf{P}^{\top} \mathcal{L}_\mathrm{c} \mathbf{P}$ and $e_i^*$ as the $i$-th diagonal element of matrix $\mathbf{E}$, we can obtain
\begin{equation}\label{eq_update1_1}
\begin{aligned}
&\operatorname*{min}_{\mathbf{R}}  \mathrm{Tr}(\mathbf{R}\mathbf{E}\mathbf{R}) \Rightarrow \operatorname*{min}_{r_i} \sum_{i=1}^d r_i^2 e_i^*\\
&\mathrm{s.t.} \quad \mathbf{r}^{\top} \mathbf{1} = 1, r_l \geq 0, l=1,2, \cdots, d\\
& \quad \quad \mathbf{R} = \mathrm{diag}(\mathbf{r}).
\end{aligned}
\end{equation}
Second, we employ the Lagrange multiplier method to solve Eq.~\eqref{eq_update1_1}. In the first step of employing the Lagrange multiplier method, we only consider the constraint $\mathbf{r}^{\top} \mathbf{1} = 1$. Therefore, the Lagrangian function of Eq.~\eqref{eq_update1_1} can be written as
\begin{equation}\label{eq_update1_2}
\mathcal{L}(\mathbf{r},z)= \sum_{i=1}^d r_i^2 e_i^* + z (\sum_{i=1}^d r_i - 1).
\end{equation}
In Eq.~\eqref{eq_update1_2}, we take the derivative with regrad to $r_i$ and set the derivative to zero for each $i$. Consequently, we can have
\begin{equation}\label{eq_update1_3}
\begin{aligned}
& \frac{ \partial \mathcal{L}(\mathbf{r},z) }{ \partial r_i } = 2r_i e_i^* + z = 0 \Rightarrow \\
& 2r_i e_i^* = -z \Rightarrow r_i = - \frac{z}{2e_i^*}.
\end{aligned}
\end{equation}
Third, we combine Eq.~\eqref{eq_update1_3} with the constraint $\mathbf{r}^{\top} \mathbf{1} = 1$. Therefore, we can have
\begin{equation}\label{eq_update1_4}
- \frac{ z }{ 2 }\sum_{i=1}^d \frac{1}{e_i^*} = 1 \Rightarrow z = - \frac{2}{\sum_{i=1}^d \frac{1}{e_i^*}}.
\end{equation}
After that, we take Eq.~\eqref{eq_update1_4} to Eq.~\eqref{eq_update1_3}. Therefore, we can have
\begin{equation}\label{eq_update1_5}
r_i = \frac{1}{e_i^* \sum_{i=1}^d \frac{1}{e_i^*}}.
\end{equation}
Next, we concentrate on the another constraint $r_l \geq 0$. We can find that the $i$-th diagonal element of matrix $\mathbf{E}$ can be calculated by $e_i^*=\mathbf{p}^{\top}_{[:,i]} \mathcal{L}_\mathrm{c} \mathbf{p}_{[:,i]}$, where $\mathbf{p}_{[:,i]} \in \mathbb{R}^{M \times 1}$ is the $i$-th column of $\mathbf{P}$, and $\mathbf{P} = [\mathbf{p}_1, \mathbf{p}_2, \cdots, \mathbf{p}_M]^{\top}$. Let us suppose the $\mathbf{p}_{[:,i]}$ as the matrix $\mathbf{O}$ in the theorem 1 of the main paper, we can have
\begin{equation}\label{eq_update1_6}
\begin{aligned}
&\mathbf{p}^{\top}_{[:,i]} \mathcal{L}_\mathrm{c} \mathbf{p}_{[:,i]}  \Rightarrow \mathrm{Tr}(\mathbf{p}^{\top}_{[:,i]} \mathcal{L}_\mathrm{c} \mathbf{p}_{[:,i]}) \Rightarrow\\
& \frac{1}{2} \sum_{j=1}^M \sum_{u=1}^M \Vert p_{[:,i], j} - p_{[:,i], u} \Vert_2^{2}w_{ju} .
\end{aligned}
\end{equation}
Since $\mathbf{W}_c$ is the fixed adjacency matrix of federated collaboration graph, we can find that $w_{jk} \geq 0$ in Eq.~\eqref{eq_update1_6}. Moreover, since $\frac{1}{2} \sum_{j=1}^M \sum_{u=1}^M \Vert p_{[:,i], j} - p_{[:,i], u} \Vert_2^{2} \geq 0$, we can have that $e_i^*=\mathbf{p}^{\top}_{[:,i]} \mathcal{L}_\mathrm{c} \mathbf{p}_{[:,i]} \geq 0$, which means that the constraint $r_i = \frac{1}{e_i^* \sum_{i=1}^d \frac{1}{e_i^*}} \geq 0$ is satisfied. In real-world applications, a small constant $\varepsilon \to 0 $ is typically added to $e_i^*$ to avoid $r_i \to \infty $.



\subsection{Update \texorpdfstring{$\mathbf{S}$}{} with \texorpdfstring{$\mathbf{W}_c$}{} and \texorpdfstring{$\mathbf{R}$}{} Fixed}
\label{Appendix_update2}
Since $\mathbf{W}_c$ and $\mathbf{R}$ are fixed, the optimization objective in Eq.~(6) of the main paper can be written as
\begin{equation}\label{eq_update2}
\begin{aligned}
&\operatorname*{min}_{\mathbf{S}} - \sum_{i=1}^{M} \sum_{j=1}^{M}\Vert \mathbf{S} \mathbf{q}_i - \mathbf{S} \mathbf{q}_j \Vert_2^{2}w_{ij}\\
&\mathrm{s.t.} \quad \mathbf{s}^{\top} \mathbf{1} = 1, s_l \geq 0, l=1,2, \cdots, d\\
& \quad \quad \mathbf{S} = \mathrm{diag}(\mathbf{s}).
\end{aligned}
\end{equation}
According to the theorem 1 of the main paper, we set the $\mathbf{o}_i = \mathbf{S} \mathbf{q}_i \in \mathbb{R}^{d \times 1} \Rightarrow \mathbf{O} = \mathbf{Q}\mathbf{S} $. Therefore, Eq.~\eqref{eq_update2} becomes
\begin{equation}
\begin{aligned}
&\operatorname*{min}_{\mathbf{S}} -\mathrm{Tr}(\mathbf{O}^{\top} \mathcal{L}_\mathrm{c} \mathbf{O}) \Rightarrow \operatorname*{min}_{\mathbf{S}} -\mathrm{Tr}(\mathbf{S}\mathbf{Q}^{\top} \mathcal{L}_\mathrm{c} \mathbf{Q}\mathbf{S})\\
&\mathrm{s.t.} \quad \mathbf{s}^{\top} \mathbf{1} = 1, s_l \geq 0, l=1,2, \cdots, d\\
& \quad \quad \mathbf{S} = \mathrm{diag}(\mathbf{s}).
\end{aligned}
\end{equation}
First, inspired by~\cite{nie2020self}, let us denote the matrix $\mathbf{N} \in \mathbb{R}^{d \times d} = \mathbf{Q}^{\top} \mathcal{L}_\mathrm{c} \mathbf{Q}$ and $n_i^*$ as the $i$-th diagonal element of matrix $\mathbf{N}$, we obtain
\begin{equation}\label{eq_update2_1}
\begin{aligned}
&\operatorname*{min}_{\mathbf{S}} - \mathrm{Tr}(\mathbf{S}\mathbf{N}\mathbf{S}) \Rightarrow \operatorname*{min}_{s_i} -\sum_{i=1}^d s_i^2 n_i^*\\
&\mathrm{s.t.} \quad \mathbf{s}^{\top} \mathbf{1} = 1, s_l \geq 0, l=1,2, \cdots, d\\
& \quad \quad \mathbf{S} = \mathrm{diag}(\mathbf{s}).
\end{aligned}
\end{equation}
Second, we employ the Lagrange multiplier method to solve Eq.~\eqref{eq_update2_1}. In the first step of employing the Lagrange multiplier method, we only consider the constraint $\mathbf{s}^{\top} \mathbf{1} = 1$. Therefore, the Lagrangian function of Eq.~\eqref{eq_update2_1} can be written as
\begin{equation}\label{eq_update2_2}
\mathcal{L}(\mathbf{s},z)= -\sum_{i=1}^d s_i^2 n_i^* + z (\sum_{i=1}^d s_i - 1).
\end{equation}
In Eq.~\eqref{eq_update2_2}, we take the derivative with regrad to $s_i$ and set the derivative to zero for each $i$. Consequently, we can have
\begin{equation}\label{eq_update2_3}
\begin{aligned}
& \frac{ \partial \mathcal{L}(\mathbf{s},z) }{ \partial s_i } = -2s_i n_i^* + z = 0 \Rightarrow \\
& 2s_i n_i^* = z \Rightarrow s_i = \frac{z}{2n_i^*}.
\end{aligned}
\end{equation}
Third, we combine Eq.~\eqref{eq_update2_3} with the constraint $\mathbf{s}^{\top} \mathbf{1} = 1$. Therefore, we can have
\begin{equation}\label{eq_update2_4}
\frac{ z }{ 2 }\sum_{i=1}^d \frac{1}{n_i^*} = 1 \Rightarrow z = \frac{2}{\sum_{i=1}^d \frac{1}{n_i^*}}.
\end{equation}
After that, we take Eq.~\eqref{eq_update2_4} to Eq.~\eqref{eq_update2_3}. Therefore, we can have
\begin{equation}\label{eq_update2_5}
s_i = \frac{1}{n_i^* \sum_{i=1}^d \frac{1}{n_i^*}}.
\end{equation}
Next, we concentrate on the another constraint $s_l \geq 0$. We can find that the $i$-th diagonal element of matrix $\mathbf{N}$ can be calculated by $n_i^*=\mathbf{q}^{\top}_{[:,i]} \mathcal{L}_\mathrm{c} \mathbf{q}_{[:,i]}$, where $\mathbf{q}_{[:,i]} \in \mathbb{R}^{M \times 1}$ is the $i$-th column of $\mathbf{Q}$, and $\mathbf{Q} = [\mathbf{q}_1, \mathbf{q}_2, \cdots, \mathbf{q}_M]^{\top}$. Let us suppose the $\mathbf{q}_{[:,i]}$ as the matrix $\mathbf{O}$ in the theorem 1 of the main paper, we can have
\begin{equation}\label{eq_update2_6}
\begin{aligned}
&\mathbf{q}^{\top}_{[:,i]} \mathcal{L}_\mathrm{c} \mathbf{q}_{[:,i]}  \Rightarrow \mathrm{Tr}(\mathbf{q}^{\top}_{[:,i]} \mathcal{L}_\mathrm{c} \mathbf{q}_{[:,i]}) \Rightarrow\\
& \frac{1}{2} \sum_{j=1}^M \sum_{u=1}^M \Vert q_{[:,i], j} - q_{[:,i], u} \Vert_2^{2}w_{ju} .
\end{aligned}
\end{equation}
Since $\mathbf{W}_c$ is the fixed adjacency matrix of federated collaboration graph, we can find that $w_{ju} \geq 0$ in Eq.~\eqref{eq_update2_6}. Moreover, since $\frac{1}{2} \sum_{j=1}^M \sum_{u=1}^M \Vert q_{[:,i], j} - q_{[:,i], u} \Vert_2^{2} \geq 0$, we can have that $n_i^*=\mathbf{q}^{\top}_{[:,i]} \mathcal{L}_\mathrm{c} \mathbf{q}_{[:,i]} \geq 0$, which means that the constraint $s_i = \frac{1}{n_i^* \sum_{i=1}^d \frac{1}{n_i^*}} \geq 0$ is satisfied. In real-world applications, a small constant $\varepsilon \to 0 $ is typically added to $n_i^*$ to avoid $s_i \to \infty $.

\subsection{Update \texorpdfstring{$\mathbf{W}_c$}{} with \texorpdfstring{$\mathbf{S}$}{} and \texorpdfstring{$\mathbf{R}$}{} Fixed}
Recall that we have to optimize the following problem, which can be written as
\begin{equation}\label{eq_update3_1}
\begin{aligned}
&\operatorname*{min}_{\mathbf{w}_i} \sum_{j=1}^{M} (\Vert \mathbf{R} \mathbf{p}_i - \mathbf{R} \mathbf{p}_j \Vert_2^{2}w_{ij}-\Vert \mathbf{S} \mathbf{q}_i - \mathbf{S} \mathbf{q}_j \Vert_2^{2}w_{ij} \\
&\quad \quad \quad \quad \quad \quad + \gamma w_{ij}^2)\\
&\mathrm{s.t.} \quad \quad \mathbf{w}_i^{\top} \mathbf{1} = 1, \quad w_{ij} \geq 0.
\end{aligned}
\end{equation}
First, inspired by~\cite{nie2020self}, we introduce the vector $\mathbf{t}_i$ with the $j$-th element as $t_{ij} = \Vert \mathbf{R} \mathbf{p}_i - \mathbf{R} \mathbf{p}_j \Vert_2^{2}-\Vert \mathbf{S} \mathbf{q}_i - \mathbf{S} \mathbf{q}_j \Vert_2^{2} $. Therefore, we can have
\begin{equation}\label{eq_update3_2}
\begin{aligned}
&\operatorname*{min}_{\mathbf{w}_i} \sum_{j=1}^{M} (t_{ij} w_{ij} + \gamma w_{ij}^2)\\
& \Rightarrow \operatorname*{min}_{\mathbf{w}_i} \sum_{j=1}^{M} ( w_{ij}^2 + \frac{1}{\gamma} t_{ij} w_{ij} +  \frac{1}{4 \gamma^2}t_{ij}^2)\\
& \Rightarrow \operatorname*{min}_{\mathbf{w}_i} \Vert \mathbf{w}_i + \frac{1}{2\gamma}\mathbf{t}_i \Vert_2^2\\
&\mathrm{s.t.} \quad \quad \mathbf{w}_i^{\top} \mathbf{1} = 1, \quad w_{ij} \geq 0.
\end{aligned}
\end{equation}
Second, we employ the Lagrange multiplier method to solve Eq.~\eqref{eq_update3_2}. Therefore, the Lagrangian function of Eq.~\eqref{eq_update3_2} can be written as
\begin{equation}\label{eq_update3_3}
\mathcal{L}(\mathbf{w}_i, \tau, \mathbf{b}_i)= k_l \Vert \mathbf{w}_i + \frac{1}{2\gamma}\mathbf{t}_i \Vert^2_2- \tau(\mathbf{w}_i^{\top} \mathbf{1} - 1) - \mathbf{b}_i^{\top}\mathbf{w}_i,
\end{equation}
where $\tau$ and $\mathbf{b}_i \geq 0$ are the Lagrange multipliers. In Eq.~\eqref{eq_update3_3}, $k_l$ is set to 0.5 according to~\cite{nie2020self}. Third, according to the Karush-Kuhn-Tucker (KKT) condition, we can have
\begin{equation}\label{eq_update3_4}
\begin{cases}
\forall j, \quad w_{ij}^* + \frac{t_{ij}}{2\gamma} - \tau^* - b^*_{ij} = 0 \\
\forall j, \quad w_{ij}^* b^*_{ij} = 0\\
\forall j, \quad w_{ij}^* \geq 0\\
\forall j, \quad b_{ij}^* \geq 0,& 
\end{cases}
\end{equation}
where $w_{ij}^*$ is the $j$-th element of $\mathbf{w}_{i}^*$. According to Eq.~\eqref{eq_update3_4}, we can have
\begin{equation}\label{eq_update3_5}
w_{ij}^* = -\frac{t_{ij}}{2\gamma} + \tau^* + b^*_{ij}.
\end{equation}
Third, we combine Eq.~\eqref{eq_update3_5} with the constraint $\mathbf{w}_i^{\top} \mathbf{1} = 1$. Therefore, we can have
\begin{equation}\label{eq_update3_6}
-\frac{1}{2\gamma} \mathbf{1}^{\top} \mathbf{t}_i + M \tau^* + \mathbf{1}^{\top} \mathbf{b}^*_i = 1
\end{equation}
and then
\begin{equation}\label{eq_update3_7}
\tau^* = \frac{1+\frac{1}{2\gamma} \mathbf{1}^{\top} \mathbf{t}_i - \mathbf{1}^{\top} \mathbf{b}^*_i}{M}.
\end{equation}
Combining Eq.~\eqref{eq_update3_7} with the first term of Eq.~\eqref{eq_update3_4}, we can have
\begin{equation}\label{eq_update3_8}
w_{ij}^* = \frac{1}{M} + \frac{1}{2M\gamma} \mathbf{1}^{\top} \mathbf{t}_i - \frac{\mathbf{1}^{\top} \mathbf{b}^*_i}{M} - \frac{t_{ij}}{2 \gamma} + b_{ij}^*
\end{equation}
and then
\begin{equation}\label{eq_update3_9}
\mathbf{w}_{i}^* = \frac{1}{M}(1 - \mathbf{1}^{\top}\mathbf{b}_i^*)\mathbf{1} - \frac{1}{2\gamma}(\mathbf{t}_i - \frac{1}{M}\mathbf{1}^{\top}\mathbf{t}_i\mathbf{1}) + \mathbf{b}_i^*,
\end{equation}
where $\mathbf{1}^{\top} \mathbf{t}_i$ is a constant. After that, we denote $\hat{b}_i^* = \frac{\mathbf{1}^{\top}\mathbf{b}^*_i}{M}$ and $h_{ij}=\frac{1}{M} - \frac{t_{ij}}{2\gamma} + \frac{\mathbf{1}^{\top}\mathbf{t}_i}{2M\gamma}$. Therefore, according to Eq.~\eqref{eq_update3_8}, $w_{ij}^*$ can be written as
\begin{equation}\label{eq_update3_10}
w_{ij}^* = h_{ij} + b_{ij}^* - \hat{b}_i^*.
\end{equation}
Meanwhile, we can have
\begin{equation}\label{eq_update3_11}
b_{ij}^* = w_{ij}^* - h_{ij} + \hat{b}_i^*.
\end{equation}
According to the second, thrid, and fourth terms of Eq.~\eqref{eq_update3_4}, we can infer that $w_{ij}^* = 0$ when $b_{ij}^* \geq 0$, and $b_{ij}^* = 0$ when $w_{ij}^* \geq 0$. Consequently, we can have
\begin{equation}\label{eq_update3_12}
w_{ij}^* = (h_{ij} - \hat{b}_i^*)_+
\end{equation}
and
\begin{equation}\label{eq_update3_13}
b_{ij}^* = (\hat{b}_i^* - h_{ij})_+.
\end{equation}
Based on Eq.~\ref{eq_update3_13}, we can have
\begin{equation}\label{eq_update3_14}
\hat{b}_i^* = \frac{1}{M}\sum_{j=1}^{M}(\hat{b}_i^* - h_{ij})_+.
\end{equation}
Then, we define a loss function and employ the Newton method to find the optimal $\hat{b}_i^*$. The loss function can be defined as
\begin{equation}\label{eq_update3_15}
\mathcal{L}(\hat{b}_i) = \frac{1}{M}\sum_{j=1}^{M}(\hat{b}_i - h_{ij})_+ - \hat{b}_i.
\end{equation}
Here we optimize $\hat{b}_i$ in an iterative manner. In the $t+1$-th iteration, the updated rule is defined as
\begin{equation}\label{eq_update3_16}
\hat{b}_i^{t+1} = \hat{b}_i^{t} - \mathcal{L}(\hat{b}_i^t) \cdot [\frac{ \partial \mathcal{L}(\hat{b}_i^t) }{ \partial \hat{b}_i^{t} }]^{-1}.
\end{equation}
If the loss function $\mathcal{L}(\hat{b}_i) \to 0$, we will obtain the optimal $\hat{b}_i^*$ and then the optimal $\mathbf{w}_{i}^*$. 


\subsection{Overview of Optimization Steps}
The main steps of our optimization process are summarized in Algorithm~\autoref{algorithm2}.


\begin{algorithm}[t]
\caption{Main Steps of the Optimization Process}\label{algorithm2}

{\textbf{Input:}} Pre-processed homophily bases $\mathbf{P} = [\mathbf{p}_1, \mathbf{p}_2, \cdots, \mathbf{p}_M]^{\top} \in \mathbb{R}^{M \times d}$; pre-processed heterophily bases $\mathbf{Q} = [\mathbf{q}_1, \mathbf{q}_2, \cdots, \mathbf{q}_M]^{\top} \in \mathbb{R}^{M \times d}$;
and the regularization parameter $\gamma$.

{\textbf{Output:}} The optimal adjacency matrix $\mathbf{W}_c$, the optimal self-attention vector $\mathbf{r}$, and the optimal self-attention vector $\mathbf{s}$.
\begin{algorithmic}[1]
\STATE {\textbf{repeat}}
\STATE \hspace{0.5cm} Update each element of $\mathbf{r}$ via Eq.~\eqref{eq_update1_5};
\STATE \hspace{0.5cm} Update each element of $\mathbf{s}$ via Eq.~\eqref{eq_update2_5};
\STATE \hspace{0.5cm} Update $\mathbf{W}_c$ by solving Eq.~\eqref{eq_update3_1} with Newton method via Eq.~\eqref{eq_update3_12}, Eq.~\eqref{eq_update3_15}, and Eq.~\eqref{eq_update3_16}.
\STATE {\textbf{until}} Convergence
\end{algorithmic}
\end{algorithm}

\subsection{Temporal Complexity Analysis of Algorithm~\autoref{algorithm2}}
\label{complexity_algorithm}
Here we concentrate on the temporal complexity of our optimization processes (\textit{i.e.}, Algorithm~\autoref{algorithm2}). Since the overall optimization problem is made up of three parts, the main temporal complexity comes from the following calculations. In Eq.~\eqref{eq_update1_1}, the temporal complexity of calculating $\mathbf{E} = \mathbf{P}^{\top} \mathcal{L}_\mathrm{c} \mathbf{P}$ is $\mathcal{O}(Md^2+M^2d)$. Similarly, in Eq.~\eqref{eq_update2_1}, the temporal complexity of calculating $\mathbf{N} = \mathbf{Q}^{\top} \mathcal{L}_\mathrm{c} \mathbf{Q}$ is $\mathcal{O}(Md^2+M^2d)$. In addition, in Eq.~\eqref{eq_update3_9}, the temporal complexity of calculating $\mathbf{w}_{i}^*$ is $\mathcal{O}(Md)$. In conclusion, the temporal complexity of Algorithm~\autoref{algorithm2} is $\mathcal{O}\big(Md \cdot [\max (M, d) + 1]\big)$. Consequently, we believe that our optimization method (\textit{i.e.}, Algorithm~\autoref{algorithm2}) is efficient in real-world applications, which is also validated by our experiments.

\section{Efficiency Analysis of Our Proposed FedGSP}
Here we analyze the spatial and temporal complexities of our proposed FedGSP on the client and server sides, respectively.

\subsection{Spatial Complexity}
\subsubsection{Client Side}
Since the local model deployed on each client is actually a spectral GNN (\textit{i.e.}, UniFilter), it means that the spatial complexity of our FedGSP on each client is determined by the UniFilter. In the UniFilter, there are $K$ homophily bases, $K$ heterophily bases, and one MLP layer. Therefore, the spatial complexity of UniFilter consists of three contents: inputs, homophily and heterophily bases, and the MLP layer. First, the spatial complexity of input node features and the adjacency matrix are $\mathcal{O}(n_m \times d)$ and $\mathcal{O}(n_m)$, respectively. Second, the spatial complexity of bases is $\mathcal{O}(K \times d^2)$. Third, the spatial complexity of the MLP layer is $\mathcal{O}(n_m \times d)$. Consequently, the total spatial complexity of our method on the client side is $\mathcal{O}( n_m \times d  + n _m + K \times d^2 + n_m \times d) \rightarrow \mathcal{O}( n_m \times d + K \times d^2)$.

\subsubsection{Server Side}
First, the spatial complexity of storing uploaded parameters of $M$ clients is related with the number of parameters of local models. Suppose that the number of parameters of local models is $K \times d^2$. Therefore, the spatial complexity is $\mathcal{O}(M \times K \times d^2)$. Second, the spatial complexity of optimizing collaboration graphs is $\mathcal{O}(K \times M^2)$ when considering there are $K$ orders of bases. Consequently, the total spatial complexity of our method on the server side is $\mathcal{O}( M \times K \times d^2  + K \times M^2) \rightarrow \mathcal{O}\big( M K \cdot (d^2 + M)\big)$.

\subsection{Temporal Complexity}
\subsubsection{Client Side}
Similarly, the temporal complexity of our method on each client is determined by the UniFilter, which is made up of $K$ bases and one MLP layer. First, the temporal complexity of $K$ homophily bases is $\mathcal{O}(K \times n_m^2 \times d + K \times n_m \times d^2)$. Second, according to~\cite{huanguniversal}, the temporal complexity of the construction of heterophily bases is $\mathcal{O}(K \times n_m)$. Therefore, the temporal complexity of heterophily bases is $\mathcal{O}(K \times n_m^2 \times d + K \times n_m \times d^2 + K \times n_m)$. Third, the temporal complexity of the MLP layer is $\mathcal{O}(n_m \times d^2)$. Consequently, the total temporal complexity of our method on the client side is $\mathcal{O}(K \times n_m^2 \times d + K \times n_m \times d^2 + K \times n_m + n_m \times d^2)$. After simplification, it can be written as $\mathcal{O}\big(n_m \cdot (K \times n_m \times d + K \times d^2 + K + d^2)\big)$.
\subsubsection{Server Side}
On the server, there are two operations, namely, optimization of collaboration strengths and federated aggregation for polynomial bases. First, according to the Section~\ref{complexity_algorithm}, the temporal complexity of optimizing collaboration strengths is $\mathcal{O}\big(M d \cdot [\max (M, d) + 1]\big)$. Second, the temporal complexity of separate aggregation is related with the number of clients and the number of parameters of local models. Suppose that the number of parameters of local models is $K \times d^2$. Therefore, the temporal complexity of separate aggregation is $\mathcal{O}(M \times K \times d^2)$. Consequently, the total temporal complexity of our method on the server side is $\mathcal{O}\big( M d \cdot [\max (M, d) + 1] + M \times K \times d^2\big) \rightarrow \mathcal{O}\big( M d \cdot [\max (M, d) + K \times d]\big)$.
        
\section{Implementation Details}
\label{implementation_details}
In this section, we provide the implementation details of our experiments, including the experimental platform, the dataset descriptions, the details of subgraph partitioning, the information of baseline methods, and our training details.

\subsection{Experimental Platform}
\label{experimental_platform}
All the experiments in our work are conducted on a Linux server with a 2.90 GHz Intel Xeon Gold 6326 CPU, 64 GB of RAM, and two NVIDIA GeForce RTX 4090 GPUs with 48GB of memory. Our proposed FedGSP is implemented via Python 3.8.8, PyTorch 1.12.0, and PyTorch Geometric (PyG) 2.3.0.

\subsection{Datasets}
\label{dataset_info}
To validate the effectiveness of our proposed FedGSP, we perform extensive experiments on eleven widely used benchmark datasets, namely, six homophilic and five heterophilic graph datasets. For the homophilic graph datasets, we choose four citation graphs: \textit{Cora}, \textit{CiteSeer}, \textit{PubMed}, and \textit{ogbn-arxiv}; two Amazon product graphs: \textit{Amazon-Computer} and \textit{Amazon-Photo}. For the heterophilic graph datasets, we select \textit{Roman-empire}, \textit{Amazon-ratings}, \textit{Minesweeper}, \textit{Tolokers}, and \textit{Questions}~\cite{platonov2023a}. The statistical information of the above benchmark datasets is described in Tab.~\ref{datasets_statistics}. Note that we use the Area Under the ROC curve (AUC) as the evaluation metric (higher values are better) for \textit{Minesweeper}, \textit{Tolokers}, and \textit{Questions} datasets, and use the accuracy as the evaluation metric (higher values are better) for other datasets.

\begin{table*}[t]
    \centering
    \caption{Statistical information of eleven used graph datasets.}
    \label{datasets_statistics}
    \begin{tabular}{cccccc}
    \hline
    Types                               & Datasets                      & \# Nodes & \# Edges  & \# Classes & \# Node Features \\ \hline
    \multirow{6}{*}{homophilic graph}   & \textit{Cora}                 & 2,708    & 5,429     & 7          & 1,433            \\
                                        & \textit{CiteSeer}             & 3,327    & 4,732     & 6          & 3,703            \\
                                        & \textit{PubMed}               & 19,717   & 44,324    & 3          & 500              \\
                                        & \textit{Amazon-Computer}      & 13,752   & 491,722   & 10         & 767              \\
                                        & \textit{Amazon-Photo}         & 7,650    & 238,162   & 8          & 745              \\
                                        & \textit{ogbn-arxiv}           & 169,343  & 2,315,598 & 40         & 128              \\ \hline
    \multirow{6}{*}{heterophilic graph} & \textit{Roman-empire}         & 22,662   & 32,927    & 18         & 300              \\
                                        & \textit{Amazon-ratings}       & 24,492   & 93,050    & 5          & 300              \\
                                        & \textit{Minesweeper}          & 10,000   & 39,402    & 2          & 7                \\
                                        & \textit{Tolokers}             & 11,758   & 519,000   & 2          & 10               \\
                                        & \textit{Questions}            & 48,921   & 153,540   & 2          & 301              \\ \hline
    \end{tabular}
    \end{table*}


In order to facilitate the division of datasets, a random sample of 20\% of nodes is selected for training purposes, 40\% for the purpose of validation, and 40\% for testing, with the exception of the \textit{ogbn-arxiv} dataset. Since \textit{ogbn-arxiv} dataset consists of a relatively large number of nodes in comparison to other datasets, a random sample of 5\% of the nodes is used for training, while the remaining half of the nodes are used for validation and testing, respectively.

\subsection{Subgraph Partitioning}
\label{subgraph_partitioning_detail}
Inspired by real-world requirements and following~\cite{baek2023personalized,wentao2025fediih}, we consider two subgraph partitioning settings: non-overlapping and overlapping. In the non-overlapping setting, $\cup_{m=1}^{M} \mathcal{V}_m=\mathcal{V}$ and $\mathcal{V}_m \cap \mathcal{V}_n=\emptyset $ for $\forall m \neq n \in \{1, 2, \cdots, M\}$, where $\mathcal{V}$ represents the node set of the global graph. Partitioning without this property is called overlapping. Here we present the details of how to partition the original graph into multiple subgraphs.

\subsubsection{Non-overlapping Partitioning}
First, if there are $M$ clients, the number of non-overlapping subgraphs to be generated is specified as $M$. Second, the METIS graph partitioning algorithm~\cite{karypis1997metis} is used to divide the original graph into $M$ subgraphs. In other words, the non-overlapping partitioning subgraph for each client is directly obtained by the output of the METIS algorithm.

\subsubsection{Overlapping Partitioning}
First, if there are $M$ clients, the number of overlapping subgraphs to be generated is specified as $M$. Second, the METIS~\cite{karypis1997metis} graph partitioning algorithm is used to divide the original graph into $\lfloor \frac{M}{5} \rfloor$ subgraphs. Third, in each subgraph generated by METIS, half of the nodes and their associated edges are randomly sampled. This procedure is performed five times to generate five different yet overlapped subgraphs. By doing so, the number of overlapping subgraphs is equal to the number of clients.

\subsection{Baseline Methods}
\label{baseline_methods_info}
We compare our proposed FedGSP with eleven baseline methods, which can be categorized into two groups. Concretely, we adopt one classic Federated Learning (FL) method (\textit{i.e.}, FedAvg~\cite{mcmahan2017communication}), two personalized FL methods (\textit{i.e.}, FedProx~\cite{MLSYS2020_1f5fe839} and FedPer~\cite{Arivazhagan2019}), three general GFL methods (\textit{i.e.}, GCFL~\cite{NEURIPS2021_9c6947bd}, FedGNN~\cite{wu2021fedgnn}, and FedSage+\cite{NEURIPS2021_34adeb8e}), and five personalized GFL methods (\textit{i.e.}, FED-PUB~\cite{baek2023personalized}, FedGTA~\cite{li2023fedgta}, AdaFGL~\cite{li2024adafgl}, FedTAD~\cite{zhu2024fedtad}, and FedIIH~\cite{wentao2025fediih}). Moreover, we perform experiments with local training, that is, training each client without federated aggregation. The detailed descriptions of these baseline methods are provided below.

\textbf{FedAvg} This method~\cite{mcmahan2017communication} represents one of the fundamental baseline methods in the field of FL. First, each client independently trains a model, which is subsequently transmitted to a server. Then, the server aggregates the locally updated models by averaging and transmits the aggregated model back to the clients.

\textbf{FedProx} This method~\cite{MLSYS2020_1f5fe839} is one of the personalized FL baseline methods. It customizes a personalized model for each client by adding a proximal term as a subproblem that minimizes weight differences between local and global models.

\textbf{FedPer} This method~\cite{Arivazhagan2019} is one of the personalized FL baseline methods. It only federates the weights of the backbone while training the personalized classification layer in each client.

\textbf{GCFL} This method~\cite{NEURIPS2021_9c6947bd} is one of the basic GFL methods. Specifically, GCFL is designed for vertical GFL (\textit{e.g.}, GFL for molecular graphs). In particular, it uses the bi-partitioning scheme, which divides a set of clients into two disjoint groups of clients based on the similarity of their gradients. This is similar to the mechanism proposed for image classification in clustered-FL~\cite{9174890}. Then, after partitioning, the model weights are shared only among clustered clients with similar gradients.

\textbf{FedGNN} This method~\cite{wu2021fedgnn} is one of the GFL baseline methods. It extends local subgraphs by exchanging node embeddings from other clients. Specifically, if two nodes in two different clients have exactly the same neighbors, FedGNN transfers the nodes with the same neighbors from other clients and expands them.

\textbf{FedSage+} This method~\cite{NEURIPS2021_34adeb8e} is one of the GFL baseline methods. It generates the missing edges between subgraphs and the corresponding neighbor nodes by using the missing neighbor generator. To train this neighbor generator, each client first receives node representations from other clients, and then computes the gradient of the distances between the local node features and the node representations of the other clients. After that, the gradient is sent back to the other clients, and this gradient is then used to train the neighbor generator.

\textbf{FED-PUB} This method~\cite{baek2023personalized} is one of the personalized GFL baseline methods. It estimates the similarities between the subgraphs based on the outputs of the local models that are given the same test graph. Then, based on the similarities, it performs a weighted averaging of the local models for each client. In addition, it learns a personalized sparse mask at each client in order to select and update only the subgraph-relevant subset of the aggregated parameters.

\textbf{FedGTA} This method~\cite{li2023fedgta} is one of the personalized GFL baseline methods. In this method, each client first computes topology-aware local smoothing confidence and mixed moments of neighbor features. They are then used to compute the inter-subgraph similarities, which are uploaded to the server along with the model parameters. Finally, the server is able to perform a weighted federation for each client.

\textbf{AdaFGL} This method~\cite{li2024adafgl} is one of the personalized GFL baseline methods. Actually, it is a decoupled two-step personalized approach. First, it uses standard multi-client federated collaborative training to acquire the federated knowledge extractor by aggregating uploaded models in the final round at the server. Second, each client performs personalized training based on the local subgraph and the federated knowledge extractor.

\textbf{FedTAD} This method~\cite{zhu2024fedtad} is one of the personalized GFL baseline methods. FedTAD designs a generator to generate a pseudo graph, which is subsequently employed for data-free knowledge distillation. By doing so, FedTAD enhances the reliable knowledge transfer from the local models to the global model, alleviating the negative impact of unreliable knowledge caused by node feature heterogeneity.

\textbf{FedIIH} This method~\cite{wentao2025fediih} is one of the personalized GFL baseline methods. FedIIH integrally models the inter- and intra- heterogeneity in GFL. On one hand, it characterizes the inter-heterogeneity from a multi-level global perspective, and thus it can properly compute the inter-subgraph similarities based on the whole distribution. On the other hand, it disentangles the subgraph into several latent factors, so that it can further consider the critical intra-heterogeneity.

\textbf{Local} This method is the non-FL baseline, where the model is trained only locally for each client, with no weight sharing.

\subsection{Training Details}
\label{training_details}
\subsubsection{Training Rounds and Epochs}
For the \textit{Cora}, \textit{CiteSeer}, \textit{PubMed}, \textit{Roman-empire}, \textit{Amazon-ratings}, \textit{Minesweeper}, \textit{Tolokers}, and \textit{Questions} datasets, we set the number of local training epochs and total rounds to 1 and 100, respectively. For several large-scale datasets, namely, \textit{Amazon-Computer}, \textit{Amazon-Photo}, and \textit{ogbn-arxiv}, we set the number of total rounds to 200. Note that the number of local epochs is set to 2 for the \textit{Amazon-Photo} and \textit{ogbn-arxiv} datasets, and to 3 for the \textit{Amazon-Computer} dataset. Finally, we report the test performance of all models at the best validation epoch, and the performance is measured by averaging across all clients in terms of node classification accuracies (or AUCs).

\subsubsection{Network Architectures}
For the experiments of all baseline methods, except FedSage+, FedGTA, FedTAD, FedIIH, and our proposed FedGSP, we use two layers of the Graph Convolutional Network (GCN)~\cite{kipf2017semisupervised} and a linear classifier layer as their network architectures. For the hyperparameter settings of baseline methods, we use the default settings given in their original papers. Because of the inductive and scalability advantages of GraphSAGE~\cite{NIPS2017_5dd9db5e}, FedSage+ uses GraphSAGE as the encoder and then trains a missing neighbor generator to handle missing links across local subgraphs. For FedIIH, we use the node feature projection layer of DisenGCN~\cite{pmlrv97ma19a} to obtain the node representations and a linear classifier layer (\textit{i.e.}, MLP) to perform node classifications. In contrast, FedGTA uses a Graph Attention Multi-Layer Perceptron (GAMLP)~\cite{35346783539121} as its backbone and a linear classifier layer to classify nodes. Note that GAMLP~\cite{35346783539121} is one of the scalable Graph Neural Network (GNN) models, which can capture the underlying correlations between different scales of graph knowledge. For our proposed FedGSP, we use the UniFilter to obtain the node representations and a linear classifier layer (\textit{i.e.}, MLP) to perform node classifications.

\bibliographystyle{IEEEtran}
\bibliography{mycite}




\end{document}


