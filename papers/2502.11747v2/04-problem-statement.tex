\section{Multi-Modal RAG for KI-VideoQA}
\label{sec:method}
In this work, we address the task of knowledge-intensive open-ended video question answering. Formally, given an input video $V$, a question $q$ about the video, and a corpus or knowledge source $\mathcal{K}$, the task is to answer the question. Note that in this task, questions cannot be merely answered by the given video; answering the question requires external knowledge. An example of this task is presented in Figure~\ref{fig:method_overview} (top left corner).

\paragraph{\textbf{Inputs}}
Let $V = [f_1, f_2, ..., f_t]$ be a video sequence of $t$ frames, where each frame $f_t \in \mathbb{R}^{h \times w \times 3}$ represents an RGB image with the height of $h$ and width of $w$ pixels. Each video is accompanied by subtitles ($S$) between the start and end timestamp for that video. Each question $q$ is a natural language sequence of $l$ tokens ${w_1, w_2, \cdots, w_l}$.

\paragraph{\textbf{Knowledge Sources}}
A knowledge source $\mathcal{K}$ is a collection of $m$ information items: $\mathcal{K} = \{k_1, k_2, ..., k_m\}$, where each information item may contain information needed to answer some questions. Each information item can be either textual (e.g., unstructured text documents) or visual (e.g., videos). This paper studies these and even multi-modal knowledge source situations which is a mixture of both textual and visual information items. 

\paragraph{\textbf{Output}}
Our work encompasses two distinct question answering tasks: (1) \textbf{Multiple Choice Questions}: Given a set of candidate answers $\mathcal{A} = {a^1, a^2, ..., a^K}$ where $K$ is typically equal to 4, the task is to select one correct answer $a^* \in \mathcal{A}$. (2) \textbf{Open-Ended Questions}: The expected output is a free-form text with the length of $p$ as the answer: $a = {y_1, y_2, \cdots, y_p}$, where each $y_i \in \mathcal{V}$ is a token from the vocabulary $\mathcal{V}$.


% \begin{enumerate}
%     \item \textbf{Multiple Choice Questions}: Given a set of candidate answers $\mathcal{A} = {a^1, a^2, ..., a^K}$ where $K$ is typically equal to 4, the task is to select one correct answer $a^* \in \mathcal{A}$. 
% %     The model outputs a probability distribution over the candidates:
% % \begin{equation}
% % P(a^k|\mathcal{V}, q, \mathcal{K}) \quad \text{for } k \in {1,...,K}
% % \end{equation}

%     \item \textbf{Open-Ended Questions}: The expected output is a free-form text with the length of $p$ as the answer: $a = {y_1, y_2, \cdots, y_p}$, where each $y_i \in \mathcal{V}$ is a token from the vocabulary $\mathcal{V}$.
% \end{enumerate}

Even though multiple choice questions for KI-VideoQA has been briefly explored in \citet{garcia2020knowit}, prior work has not studied retrieval-augmented solutions. Moreover, to the best of our knowledge, no prior work explored open-ended question answering in KI-VideoQA. 





% \subsection{Multi-Modal RAG for KI-VideoQA}
One approach to answer questions is to feed the given question and video (or its subtitle) to a VLM for answer generation. This approach has been studied in \cite{garcia2020knowit,wu2021transferring}. However, we believe that this approach cannot perform well for many knowledge-intensive questions. If the LLM has observed and learned the ``knowledge'' required for answering the question, this approach is likely to generate a correct answer, however, this is not always possible. For instance, this approach would not perform well in non-stationary data situations where new videos and knowledge sources have been produced after the VLM training. Therefore, this work presents the first attempt to apply ideas from the retrieval-augmented text generation (RAG) literature \cite{reml,rag} to KI-VideoQA.

In the following, we introduce a generic framework and discuss various implementations of it in each experiment. The framework, as depicted in Figure~\ref{fig:method_overview}, consists of three main components: query formulation, multi-modal knowledge retrieval, and a VLM for information synthesis and answer generation. The framework processes input in the form of a text question, video frames, and additional information about the video if available such as associated subtitles, utilizing external knowledge bases to generate accurate answers in both multiple-choice and open-ended settings. In the following, we describe various implementations of each of these components studied in this paper.


\subsection{Query Formulation}
The effectiveness of retrieval heavily depends on query formulation. We explore several query formulation strategies:
\begin{enumerate}[leftmargin=*]
\item \textbf{Question-only}: Using the question text $q$ as the query:
% \begin{equation}
% q_{raw} = q
% \end{equation}

\item \textbf{Question + Options}: For multiple choice questions, we can concatenate the question with all provided options as the query. This query formulation cannot be applied to open-ended answer generation, as options are not available to the model in this setting.
% concatenating the question with all possible options:
% \begin{equation}
%     q_{opt} = q \oplus \{a^1, a^2, ..., a^K\}
% \end{equation}
% where $\oplus$ denotes concatenation and $\{a^1, ..., a^K\}$ are the candidate answers.

\item \textbf{Question + Subtitle}: Enriching the query with the video subtitle, by concatenating the question with the video subtitle.
% \hamed{In your equation you mentioned subtitle at time $t$. What is $t$ here?} 
% relevant subtitle context:
% \begin{equation}
%     q_{sub} = q \oplus s_t
% \end{equation}
% where $s_t$ represents the subtitle at timestamp $t$.

\item \textbf{LLM-Based Query Rewriting}: Rewriting the question using a VLM that takes the video, subtitle and question, and is prompted to rewrite the question for higher quality retrieval. \footnote{Detailed prompts available in source code}
% Applying transformations to the base query using the inputs with a VLM:
% \begin{equation}
%     q_{trans} = T(q)
% \end{equation}
% where $T(\cdot)$ is the transformation that reformulates the query for better retrieval.

\end{enumerate}

\subsection{Multi-Modal Knowledge Retrieval}

Given a formulated query $q^*$ obtained from the last component, we perform retrieval over a diverse set of multi-modal knowledge sources as follows:
\begin{enumerate}[leftmargin=*]
\item \textbf{Subtitle Retrieval}: This retrieval model takes a collection of videos as a knowledge source and uses their subtitles to construct a text document for every video in the collection. Therefore, this knowledge source basically contains historical dialogues in the video collection and enables the system to leverage conversational context and spoken information that may not be visually apparent.

% \begin{equation}
% \mathcal{R}s = R_s(q*, \mathcal{K}_s)
% \end{equation}

% \begin{equation}
%     \mathcal{R}_c = R_c(q_*, \mathcal{K}_c)
% \end{equation}

\item \textbf{Video Caption Retrieval}: Developing effective video retrieval models is challenging and one approach is to turn the video into a textual description through video captioning. Therefore, each video will be represented by a single text document, automatically generated using a large vision-language model. We use Qwen2-VL-2B \cite{wang2024qwen2vl} in zero-shot setting for this purpose. These captions serve as an intermediate representation bridging visual and textual modalities. To provide an insight into what a video caption may contain, we provide one example in Figure~\ref{fig:method_overview} (bottom right corner).

% This corpus provides dense semantic descriptions of video content.

\item \textbf{Video Retrieval}: Knowledge retrieval can be done directly to the collection of videos. We do video retrieval using the generated captions and finding the corresponding video of the retrieved captions. This component helps in understanding visual context, actions, and temporal relationships that may be crucial for answering some question.


% \begin{equation}
%     \mathcal{R}_c = R_c(q_*, \mathcal{K}_c)
% \end{equation}

\end{enumerate}

Each retriever component operates independently and can be implemented using various retrieval architectures, such as sparse or dense retrieval models. Each returns a ranked list of information items with the highest relevance score, given the formulated query $q^*$. We implement and evaluate three distinct retrieval models:
\begin{itemize}[leftmargin=*]
    \item \textbf{BM25 \cite{robertson1994some}}: A sparse retrieval model rooted in classical probabilistic models. We use Elasticsearch's implementation of BM25. We use the default BM25 parameters (i.e., $b=0.75$ and $k_1=1.2$). This approach can only be employed for subtitle and video caption retrieval.

     \item \textbf{NV-Embed-v2 \cite{lee2024nv}}: State-of-the-art dense retrieval model which was ranked No. 1 on the Massive Text Embedding Benchmark \cite{muennighoff-etal-2023-mteb} as of January 5, 2025 with an impressive score of 72.31 across 56 text embedding tasks. The model is built on the Mistral-7B-v0.1 architecture and produces embeddings with a dimension of 4096. 
     % They used several new ideas: a two-staged instruction tuning method to enhance the accuracy of both retrieval and non-retrieval tasks, used Latent-attention mechanism to allows the LLM to attend to latent vectors, resulting in improved pooled embedding output. 
     % \hamed{it would be nice if you include more information about this. how many parameters? what is especial about it? for instance, a model that is trained using knowledge distillation on the x and y dataset.}
    
    \item \textbf{Stella \cite{zhang2024jasper}}: We use another state-of-the-art dense retrieval model, stella\_en\_400M\_v5.\footnote{Available at \url{https://huggingface.co/dunzhang/stella_en_400M_v5}.} This model only has 400M parameters and encodes queries and documents into 1024-dimensional dense vectors. Compared to other similar performing embedding models, both the number of parameters and encoded vector dimension are very small; for example, NV-Embed-v2 \cite{lee2024nv}, bge-en-icl \cite{li2024making}, and e5-mistral-7b-instruct \cite{wang2022text, wang2023improving, wang2024multilingual} have 7B parameters, and their vector dimensions are 4096. The deployment and application of these larger models in industry were hampered by their vector dimensions and numerous parameters, making it too slow for practical use. Stella uses an innovative distillation technique to achieve high performance while maintaining a smaller footprint. In our experiments we saw 11x speed improvement for a fixed batch size during encoding, compared to NV-Embed-v2.

% This makes it a compact yet powerful dense retrieval model that addresses the industry deployment challenges of large vector dimensions.
    
    % The model encodes queries and documents into 1024-dimensional dense vectors. Note that Stella has shown improved performance compared to commonly used dense retrieval models, such as DPR, ANCE, and TAS-B \cite{}. \hamed{I added the last sentence. Please find a good citation for it, so people don't question your choice.}
    
   
\end{itemize}

% For all retrieval models, we maintain a consistent evaluation setup. 


% \hamed{maybe better to highlight retrieval models and also the fact that we consider heterogenuous information at some point.}

\subsection{Augmented Answer Generation}

The final component is a VLM that generates the answer. It takes the original inputs (video, question, subtitle and options for the MCQ setting) and retrieved knowledge from each of the retrievers. The model utilizes heterogenuous information both from the input and retrieved knowledge. We utilize Qwen2-VL with two billion parameters \cite{wang2024qwen2vl} as our primary reader model, which serves as the foundation for answering both multiple choice and open-ended questions. Qwen2-VL processes the input video frames, question, and retrieved knowledge simultaneously to generate answers. For video frame processing, we sample frames uniformly at 1 FPS and encode them using Qwen2VL's native visual encoder. Each frame height and width is resized into 224 pixels before feeding to the VLM.
% \hamed{more information. what LLMs? any specific prompt? zero-shot or fine-tuning.}

\subsection{Fine-Tuning}
This section describes the method used for fine-tuning the Qwen2VL 2B model. The model is initialized using the pre-trained weights available on HuggingFace.\footnote{Available at \url{https://huggingface.co/Qwen/Qwen2-VL-2B-Instruct}.} We use the Adam optimizer with weight decay (AdamW) \cite{loshchilov2017decoupled} for fine-tuning with a learning rate of 1e-5. We use the cross-entropy loss function with a batch size of 1 due to the high memory requirement of processing videos by VLMs. To minimize the impact of gradient fluctuation, we update model parameters with 50 gradient accumulation step, resulting in an effective batch size of 50. The model uses Flash Attention 2 \cite{dao2023flashattention2fasterattentionbetter} for better acceleration and memory efficiency, especially when processing multiple videos. We use the training portion of each dataset for fine-tuning the model.


% \hamed{here describe your model training. loss funciton. optimization. etc.}

% Model: Qwen2VL
% Model Size: 2B parameters
% Pre-trained Model: The model is initialized with the pre-trained weights from Qwen/Qwen2-VL-2B-Instruct
% Optimizer: AdamW (Adam with Weight Decay)
% Learning Rate: 1e-5
% Loss Function: Cross-Entropy Loss
% Batch Size: 1 (due to the high memory requirements of processing images and videos)
% Gradient Accumulation Steps: 50 (Gradient accumulation is used to simulate a larger batch size by accumulating gradients over multiple forward passes before performing a backward pass and updating the model parameters. This helps in managing memory constraints while still allowing for effective training.)
% Flash Attention: The model uses flash\_attention\_2 for better acceleration and memory efficiency, especially when processing multiple images or videos.
% Training data: train partition of the datasets