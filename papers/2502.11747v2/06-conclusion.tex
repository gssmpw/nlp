\section{Conclusions and Future Directions}

% \hamed{future work: long videos / other domains beyond tv / embodid stuff / attribution}

This paper presents the first comprehensive study of multi-modal retrieval augmentation for knowledge-intensive video question answering. Our extensive experiments across two datasets demonstrate that retrieval augmentation significantly improves KI-VideoQA performance, with subtitle-based retrieval being particularly effective. The combination of subtitles with automatically generated video captions yields the best results. However, our work reveals important limitations in current video retrieval strategies. Directly retrieving video content tends to degrade performance compared to text-based approaches, highlighting the need for more sophisticated video retrieval techniques.

We intend to develop more efficient and effective video retrieval methods specifically designed for KI-VideoQA, as well as extending retrieval augmentation to longer-form videos beyond TV show clips. There are also opportunities to explore embodied question answering scenarios where physical interaction with the environment is required. Additionally, improving attribution and explanation capabilities could help users better understand how retrieved knowledge influences answers.