@inproceedings{Qu2021KIVQA,
author = {Qu, Chen and Zamani, Hamed and Yang, Liu and Croft, W. Bruce and Learned-Miller, Erik},
title = {Passage Retrieval for Outside-Knowledge Visual Question Answering},
year = {2021},
isbn = {9781450380379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3404835.3462987},
doi = {10.1145/3404835.3462987},
booktitle = {Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {1753–1757},
numpages = {5},
keywords = {dense retrieval, multi-modal, visual question answering},
location = {Virtual Event, Canada},
series = {SIGIR '21}
}

@inproceedings{Salemi2023ICTIR,
author = {Salemi, Alireza and Rafiee, Mahta and Zamani, Hamed},
title = {Pre-Training Multi-Modal Dense Retrievers for Outside-Knowledge Visual Question Answering},
year = {2023},
isbn = {9798400700736},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3578337.3605137},
doi = {10.1145/3578337.3605137},
booktitle = {Proceedings of the 2023 ACM SIGIR International Conference on Theory of Information Retrieval},
pages = {169–176},
numpages = {8},
keywords = {data generation, dense retrieval, multi-modal retrieval, pre-training, visual question answering},
location = {Taipei, Taiwan},
series = {ICTIR '23}
}

@inproceedings{Salemi2023MMFID,
author = {Salemi, Alireza and Altmayer Pizzorno, Juan and Zamani, Hamed},
title = {A Symmetric Dual Encoding Dense Retrieval Framework for Knowledge-Intensive Visual Question Answering},
year = {2023},
isbn = {9781450394086},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3539618.3591629},
doi = {10.1145/3539618.3591629},
booktitle = {Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {110–120},
numpages = {11},
keywords = {dense retrieval, knowledge distillation, multi-modal retrieval, visual question answering},
location = {Taipei, Taiwan},
series = {SIGIR '23}
}

@inproceedings{VisionTransformer,
title={An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
author={Alexey Dosovitskiy and Lucas Beyer and Alexander Kolesnikov and Dirk Weissenborn and Xiaohua Zhai and Thomas Unterthiner and Mostafa Dehghani and Matthias Minderer and Georg Heigold and Sylvain Gelly and Jakob Uszkoreit and Neil Houlsby},
booktitle={International Conference on Learning Representations},
year={2021},
}

@article{achiam2023gpt,
  title={Gpt-4 technical report},
  author={OpenAI},
  journal={arXiv preprint arXiv:2303.08774},
  year={2023}
}

@inproceedings{auer2007dbpedia,
  title={Dbpedia: A nucleus for a web of open data},
  author={Auer, S{\"o}ren and Bizer, Christian and Kobilarov, Georgi and Lehmann, Jens and Cyganiak, Richard and Ives, Zachary},
  booktitle={international semantic web conference},
  pages={722--735},
  year={2007},
  organization={Springer}
}

@inproceedings{bagad2023test,
  title={Test of time: Instilling video-language models with a sense of time},
  author={Bagad, Piyush and Tapaswi, Makarand and Snoek, Cees GM},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={2503--2516},
  year={2023}
}

@misc{bai2023qwenvl,
      title={Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond}, 
      author={Jinze Bai and Shuai Bai and Shusheng Yang and Shijie Wang and Sinan Tan and Peng Wang and Junyang Lin and Chang Zhou and Jingren Zhou},
      year={2023},
      eprint={2308.12966},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
}

@inproceedings{chen2024internvl,
  title={Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks},
  author={Chen, Zhe and Wu, Jiannan and Wang, Wenhai and Su, Weijie and Chen, Guo and Xing, Sen and Zhong, Muyan and Zhang, Qinglong and Zhu, Xizhou and Lu, Lewei and others},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={24185--24198},
  year={2024}
}

@article{chiang2023vicuna,
  title={Vicuna: An open-source chatbot impressing gpt-4 with 90\%* chatgpt quality},
  author={Chiang, Wei-Lin and Li, Zhuohan and Lin, Zi and Sheng, Ying and Wu, Zhanghao and Zhang, Hao and Zheng, Lianmin and Zhuang, Siyuan and Zhuang, Yonghao and Gonzalez, Joseph E and others},
  journal={See https://vicuna. lmsys. org (accessed 14 April 2023)},
  volume={2},
  number={3},
  pages={6},
  year={2023}
}

@article{fu2021violet,
  title={Violet: End-to-end video-language transformers with masked visual-token modeling},
  author={Fu, Tsu-Jui and Li, Linjie and Gan, Zhe and Lin, Kevin and Wang, William Yang and Wang, Lijuan and Liu, Zicheng},
  journal={arXiv preprint arXiv:2111.12681},
  year={2021}
}

@inproceedings{garcia2020knowit,
  title={KnowIT VQA: Answering knowledge-based questions about videos},
  author={Garcia, Noa and Otani, Mayu and Chu, Chenhui and Nakashima, Yuta},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={34},
  number={07},
  pages={10826--10834},
  year={2020}
}

@inproceedings{jang2017tgif,
  title={Tgif-qa: Toward spatio-temporal reasoning in visual question answering},
  author={Jang, Yunseok and Song, Yale and Yu, Youngjae and Kim, Youngjin and Kim, Gunhee},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={2758--2766},
  year={2017}
}

@inproceedings{kat,
    title = "{KAT}: A Knowledge Augmented Transformer for Vision-and-Language",
    author = "Gui, Liangke  and
      Wang, Borui  and
      Huang, Qiuyuan  and
      Hauptmann, Alexander  and
      Bisk, Yonatan  and
      Gao, Jianfeng",
    booktitle = "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jul,
    year = "2022",
    address = "Seattle, United States",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.naacl-main.70",
    doi = "10.18653/v1/2022.naacl-main.70",
    pages = "956--968",
}

@inproceedings{krisp,
  title={KRISP: Integrating Implicit and Symbolic Knowledge for Open-Domain Knowledge-Based VQA},
  author={Kenneth Marino and Xinlei Chen and Devi Parikh and Abhinav Kumar Gupta and Marcus Rohrbach},
  booktitle={2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2020},
  pages={14106-14116}
}

@article{lei2018tvqa,
  title={Tvqa: Localized, compositional video question answering},
  author={Lei, Jie and Yu, Licheng and Bansal, Mohit and Berg, Tamara L},
  journal={arXiv preprint arXiv:1809.01696},
  year={2018}
}

@article{li2023videochat,
  title={Videochat: Chat-centric video understanding},
  author={Li, KunChang and He, Yinan and Wang, Yi and Li, Yizhuo and Wang, Wenhai and Luo, Ping and Wang, Yali and Wang, Limin and Qiao, Yu},
  journal={arXiv preprint arXiv:2305.06355},
  year={2023}
}

@inproceedings{li2025llama,
  title={Llama-vid: An image is worth 2 tokens in large language models},
  author={Li, Yanwei and Wang, Chengyao and Jia, Jiaya},
  booktitle={European Conference on Computer Vision},
  pages={323--340},
  year={2025},
  organization={Springer}
}

@article{lin2023video,
  title={Video-llava: Learning united visual representation by alignment before projection},
  author={Lin, Bin and Ye, Yang and Zhu, Bin and Cui, Jiaxi and Ning, Munan and Jin, Peng and Yuan, Li},
  journal={arXiv preprint arXiv:2311.10122},
  year={2023}
}

@inproceedings{lu2018r,
  title={R-VQA: learning visual relation facts with semantic attention for visual question answering},
  author={Lu, Pan and Ji, Lei and Zhang, Wei and Duan, Nan and Zhou, Ming and Wang, Jianyong},
  booktitle={Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining},
  pages={1880--1889},
  year={2018}
}

@inproceedings{marino2019ok,
  title={Ok-vqa: A visual question answering benchmark requiring external knowledge},
  author={Marino, Kenneth and Rastegari, Mohammad and Farhadi, Ali and Mottaghi, Roozbeh},
  booktitle={Proceedings of the IEEE/cvf conference on computer vision and pattern recognition},
  pages={3195--3204},
  year={2019}
}

@inproceedings{mavex, 
title={Multi-Modal Answer Validation for Knowledge-Based VQA}, 
DOI={10.1609/aaai.v36i3.20174}, 
booktitle={Proceedings of the AAAI Conference on Artificial Intelligence}, 
author={Wu, Jialin and Lu, Jiasen and Sabharwal, Ashish and Mottaghi, Roozbeh}, 
year={2022}, 
pages={2712-2721} }

@inproceedings{radford2021learning,
  title={Learning transferable visual models from natural language supervision},
  author={Radford, Alec and others},
  booktitle={International conference on machine learning},
  pages={8748--8763},
  year={2021},
  organization={PMLR}
}

@article{sun2023eva,
  title={Eva-clip: Improved training techniques for clip at scale},
  author={Sun, Quan and Fang, Yuxin and Wu, Ledell and Wang, Xinlong and Cao, Yue},
  journal={arXiv preprint arXiv:2303.15389},
  year={2023}
}

@article{team2023gemini,
  title={Gemini: a family of highly capable multimodal models},
  author={Team Gemini},
  journal={arXiv preprint arXiv:2312.11805},
  year={2023}
}

@article{touvron2023llama,
  title={Llama: Open and efficient foundation language models},
  author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others},
  journal={arXiv preprint arXiv:2302.13971},
  year={2023}
}

@inproceedings{unifer,
author = {Guo, Yangyang and Nie, Liqiang and Wong, Yongkang and Liu, Yibing and Cheng, Zhiyong and Kankanhalli, Mohan},
title = {A Unified End-to-End Retriever-Reader Framework for Knowledge-Based VQA},
year = {2022},
isbn = {9781450392037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3503161.3547870},
doi = {10.1145/3503161.3547870},
booktitle = {Proceedings of the 30th ACM International Conference on Multimedia},
pages = {2061–2069},
numpages = {9},
keywords = {visual question answering, knowledge integration, modal fusion},
location = {Lisboa, Portugal},
series = {MM '22}
}

@article{wang2015explicit,
  title={Explicit knowledge-based reasoning for visual question answering},
  author={Wang, Peng and Wu, Qi and Shen, Chunhua and Hengel, Anton van den and Dick, Anthony},
  journal={arXiv preprint arXiv:1511.02570},
  year={2015}
}

@article{wang2017fvqa,
  title={Fvqa: Fact-based visual question answering},
  author={Wang, Peng and Wu, Qi and Shen, Chunhua and Dick, Anthony and Van Den Hengel, Anton},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  volume={40},
  number={10},
  pages={2413--2427},
  year={2017},
  publisher={IEEE}
}

@inproceedings{wu2016ask,
  title={Ask me anything: Free-form visual question answering based on knowledge from external sources},
  author={Wu, Qi and Wang, Peng and Shen, Chunhua and Dick, Anthony and Van Den Hengel, Anton},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={4622--4630},
  year={2016}
}

@article{wu2021transferring,
  title={Transferring domain-agnostic knowledge in video question answering},
  author={Wu, Tianran and Garcia, Noa and Otani, Mayu and Chu, Chenhui and Nakashima, Yuta and Takemura, Haruo},
  journal={arXiv preprint arXiv:2110.13395},
  year={2021}
}

@inproceedings{xiao2024can,
  title={Can i trust your answer? visually grounded video question answering},
  author={Xiao, Junbin and Yao, Angela and Li, Yicong and Chua, Tat-Seng},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={13204--13214},
  year={2024}
}

@article{xiao2024videoqa,
  title={VideoQA in the Era of LLMs: An Empirical Study},
  author={Xiao, Junbin and Huang, Nanxin and Qin, Hangyu and Li, Dongyang and Li, Yicong and Zhu, Fengbin and Tao, Zhulin and Yu, Jianxing and Lin, Liang and Chua, Tat-Seng and others},
  journal={arXiv preprint arXiv:2408.04223},
  year={2024}
}

@inproceedings{xu2017video,
  title={Video question answering via gradually refined attention over appearance and motion},
  author={Xu, Dejing and Zhao, Zhou and Xiao, Jun and Wu, Fei and Zhang, Hanwang and He, Xiangnan and Zhuang, Yueting},
  booktitle={Proceedings of the 25th ACM international conference on Multimedia},
  pages={1645--1653},
  year={2017}
}

@article{zhu2015building,
  title={Building a large-scale multimodal knowledge base system for answering visual queries},
  author={Zhu, Yuke and Zhang, Ce and R{\'e}, Christopher and Fei-Fei, Li},
  journal={arXiv preprint arXiv:1507.05670},
  year={2015}
}

