\section{Related Work}
This section reviews prior work on video understanding and question answering as well as knowledge-intensive visual QA.
% \hamed{note to self: decide later whether a section on RAG should be added or not.}

\paragraph{\textbf{Video Question Answering}}
VideoQA has evolved significantly from simple visual recognition to complex temporal and knowledge-based reasoning tasks. This evolution can be traced through three distinct stages \cite{xiao2024videoqa}. Initially, approaches combined CNNs for visual features with RNNs for text processing \cite{jang2017tgif, xu2017video}, focusing primarily on visual content understanding. The field then progressed to more sophisticated architectures like Vision Transformers \cite{VisionTransformer} and BERT-based models that leveraged cross-modal pre-training \cite{lei2018tvqa, fu2021violet} in the second stage.

The current era features VLMs that combine visual encoders like CLIP \cite{radford2021learning} or EVA-CLIP \cite{sun2023eva} with powerful language models such as LLaMA \cite{touvron2023llama} and Vicuna \cite{chiang2023vicuna}. Notable examples include VideoChat \cite{li2023videochat}, Video-LLaVA \cite{lin2023video}, LLaMA-VID \cite{li2025llama}, InternVL \cite{chen2024internvl}, and QwenVL \cite{bai2023qwenvl}. These models have made significant contributions, which demonstrate strong capabilities in video understanding and temporal reasoning. Commercial models like GPT-4V \cite{achiam2023gpt} and Gemini \cite{team2023gemini} have further pushed the boundaries, achieving near-human performance on some visual understanding tasks.

While these models excel at standard VideoQA tasks, recent studies \cite{xiao2024can} reveal concerning limitations, e.g., models can often answer questions correctly with irrelevant or even no video inputs, suggesting reliance on language priors rather than true visual understanding. Additionally, current models struggle with temporal reasoning and content ordering, highlighting the gap between model capabilities and human-like video comprehension \cite{bagad2023test}.

This work extends the current state-of-the-art VideoQA models to knowledge-intensive questions through retrieval augmentation, an area of research that is yet underexplored.

\paragraph{\textbf{Knowledge-Intensive Visual Question Answering}}
KI-VQA emerged to address the fundamental limitation of standard VQA systems--the constraint of inferring answers solely from training data. Given the finite nature of any training dataset, the knowledge scope of traditional VQA systems remains inherently limited. KI-VQA approaches aim to overcome this by incorporating external knowledge sources into the question answering process. Unlike this work, KI-VQA models mostly focus on images.

Early systems focused on common sense-enabled VQA, where models were augmented with basic world knowledge. These initial approaches either created specialized image-focused knowledge bases with template questions \cite{zhu2015building} or extracted information from general-purpose knowledge bases like DBpedia \cite{wu2016ask, auer2007dbpedia} to enhance VQA accuracy.
Several datasets have been introduced to study different aspects of knowledge integration.  KB-VQA \cite{wang2015explicit} and FVQA \cite{wang2017fvqa} focus on questions generated from templates or knowledge base facts. R-VQA \cite{lu2018r} studies relational fact reasoning, requiring models to understand relationships between entities. OK-VQA \cite{marino2019ok} contains free-form factoid questions without explicit knowledge annotations, requiring models to use an external knowledge base for answering natural language questions about images.

Most early KI-VQA datasets imposed significant constraints on question formulation, either through template-based generation or direct extraction from knowledge bases. This limited their ability to capture the complexity and diversity of real-world questions. KnowIT VQA \cite{garcia2020knowit} and KnowIT-X VQA \cite{wu2021transferring} datasets extended knowledge-based reasoning to video understanding. They were the first datasets to explore external knowledge questions in video content, with questions freely proposed by qualified workers to study both knowledge integration and temporal coherence. They are used in this research and are introduced in more detail in Section~\ref{sec:data}. While significant progress has been made in image-based KI-VQA \cite{Qu2021KIVQA,Salemi2023ICTIR,Salemi2023MMFID,mavex,krisp,kat,unifer}, extending these approaches to video understanding (i.e., KI-VideoQA) presents unique challenges that are understudied. For instance, it is still unclear what and how external information should be used for effective development of KI-VideoQA systems. This paper bridges these gaps.