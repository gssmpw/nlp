[
  {
    "index": 0,
    "papers": [
      {
        "key": "xiao2024videoqa",
        "author": "Xiao, Junbin and Huang, Nanxin and Qin, Hangyu and Li, Dongyang and Li, Yicong and Zhu, Fengbin and Tao, Zhulin and Yu, Jianxing and Lin, Liang and Chua, Tat-Seng and others",
        "title": "VideoQA in the Era of LLMs: An Empirical Study"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "jang2017tgif",
        "author": "Jang, Yunseok and Song, Yale and Yu, Youngjae and Kim, Youngjin and Kim, Gunhee",
        "title": "Tgif-qa: Toward spatio-temporal reasoning in visual question answering"
      },
      {
        "key": "xu2017video",
        "author": "Xu, Dejing and Zhao, Zhou and Xiao, Jun and Wu, Fei and Zhang, Hanwang and He, Xiangnan and Zhuang, Yueting",
        "title": "Video question answering via gradually refined attention over appearance and motion"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "VisionTransformer",
        "author": "Alexey Dosovitskiy and Lucas Beyer and Alexander Kolesnikov and Dirk Weissenborn and Xiaohua Zhai and Thomas Unterthiner and Mostafa Dehghani and Matthias Minderer and Georg Heigold and Sylvain Gelly and Jakob Uszkoreit and Neil Houlsby",
        "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "lei2018tvqa",
        "author": "Lei, Jie and Yu, Licheng and Bansal, Mohit and Berg, Tamara L",
        "title": "Tvqa: Localized, compositional video question answering"
      },
      {
        "key": "fu2021violet",
        "author": "Fu, Tsu-Jui and Li, Linjie and Gan, Zhe and Lin, Kevin and Wang, William Yang and Wang, Lijuan and Liu, Zicheng",
        "title": "Violet: End-to-end video-language transformers with masked visual-token modeling"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "radford2021learning",
        "author": "Radford, Alec and others",
        "title": "Learning transferable visual models from natural language supervision"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "sun2023eva",
        "author": "Sun, Quan and Fang, Yuxin and Wu, Ledell and Wang, Xinlong and Cao, Yue",
        "title": "Eva-clip: Improved training techniques for clip at scale"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "touvron2023llama",
        "author": "Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\\'e}e and Rozi{\\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others",
        "title": "Llama: Open and efficient foundation language models"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "chiang2023vicuna",
        "author": "Chiang, Wei-Lin and Li, Zhuohan and Lin, Zi and Sheng, Ying and Wu, Zhanghao and Zhang, Hao and Zheng, Lianmin and Zhuang, Siyuan and Zhuang, Yonghao and Gonzalez, Joseph E and others",
        "title": "Vicuna: An open-source chatbot impressing gpt-4 with 90\\%* chatgpt quality"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "li2023videochat",
        "author": "Li, KunChang and He, Yinan and Wang, Yi and Li, Yizhuo and Wang, Wenhai and Luo, Ping and Wang, Yali and Wang, Limin and Qiao, Yu",
        "title": "Videochat: Chat-centric video understanding"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "lin2023video",
        "author": "Lin, Bin and Ye, Yang and Zhu, Bin and Cui, Jiaxi and Ning, Munan and Jin, Peng and Yuan, Li",
        "title": "Video-llava: Learning united visual representation by alignment before projection"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "li2025llama",
        "author": "Li, Yanwei and Wang, Chengyao and Jia, Jiaya",
        "title": "Llama-vid: An image is worth 2 tokens in large language models"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "chen2024internvl",
        "author": "Chen, Zhe and Wu, Jiannan and Wang, Wenhai and Su, Weijie and Chen, Guo and Xing, Sen and Zhong, Muyan and Zhang, Qinglong and Zhu, Xizhou and Lu, Lewei and others",
        "title": "Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "bai2023qwenvl",
        "author": "Jinze Bai and Shuai Bai and Shusheng Yang and Shijie Wang and Sinan Tan and Peng Wang and Junyang Lin and Chang Zhou and Jingren Zhou",
        "title": "Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "achiam2023gpt",
        "author": "OpenAI",
        "title": "Gpt-4 technical report"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "team2023gemini",
        "author": "Team Gemini",
        "title": "Gemini: a family of highly capable multimodal models"
      }
    ]
  },
  {
    "index": 15,
    "papers": [
      {
        "key": "xiao2024can",
        "author": "Xiao, Junbin and Yao, Angela and Li, Yicong and Chua, Tat-Seng",
        "title": "Can i trust your answer? visually grounded video question answering"
      }
    ]
  },
  {
    "index": 16,
    "papers": [
      {
        "key": "bagad2023test",
        "author": "Bagad, Piyush and Tapaswi, Makarand and Snoek, Cees GM",
        "title": "Test of time: Instilling video-language models with a sense of time"
      }
    ]
  },
  {
    "index": 17,
    "papers": [
      {
        "key": "zhu2015building",
        "author": "Zhu, Yuke and Zhang, Ce and R{\\'e}, Christopher and Fei-Fei, Li",
        "title": "Building a large-scale multimodal knowledge base system for answering visual queries"
      }
    ]
  },
  {
    "index": 18,
    "papers": [
      {
        "key": "wu2016ask",
        "author": "Wu, Qi and Wang, Peng and Shen, Chunhua and Dick, Anthony and Van Den Hengel, Anton",
        "title": "Ask me anything: Free-form visual question answering based on knowledge from external sources"
      },
      {
        "key": "auer2007dbpedia",
        "author": "Auer, S{\\\"o}ren and Bizer, Christian and Kobilarov, Georgi and Lehmann, Jens and Cyganiak, Richard and Ives, Zachary",
        "title": "Dbpedia: A nucleus for a web of open data"
      }
    ]
  },
  {
    "index": 19,
    "papers": [
      {
        "key": "wang2015explicit",
        "author": "Wang, Peng and Wu, Qi and Shen, Chunhua and Hengel, Anton van den and Dick, Anthony",
        "title": "Explicit knowledge-based reasoning for visual question answering"
      }
    ]
  },
  {
    "index": 20,
    "papers": [
      {
        "key": "wang2017fvqa",
        "author": "Wang, Peng and Wu, Qi and Shen, Chunhua and Dick, Anthony and Van Den Hengel, Anton",
        "title": "Fvqa: Fact-based visual question answering"
      }
    ]
  },
  {
    "index": 21,
    "papers": [
      {
        "key": "lu2018r",
        "author": "Lu, Pan and Ji, Lei and Zhang, Wei and Duan, Nan and Zhou, Ming and Wang, Jianyong",
        "title": "R-VQA: learning visual relation facts with semantic attention for visual question answering"
      }
    ]
  },
  {
    "index": 22,
    "papers": [
      {
        "key": "marino2019ok",
        "author": "Marino, Kenneth and Rastegari, Mohammad and Farhadi, Ali and Mottaghi, Roozbeh",
        "title": "Ok-vqa: A visual question answering benchmark requiring external knowledge"
      }
    ]
  },
  {
    "index": 23,
    "papers": [
      {
        "key": "garcia2020knowit",
        "author": "Garcia, Noa and Otani, Mayu and Chu, Chenhui and Nakashima, Yuta",
        "title": "KnowIT VQA: Answering knowledge-based questions about videos"
      }
    ]
  },
  {
    "index": 24,
    "papers": [
      {
        "key": "wu2021transferring",
        "author": "Wu, Tianran and Garcia, Noa and Otani, Mayu and Chu, Chenhui and Nakashima, Yuta and Takemura, Haruo",
        "title": "Transferring domain-agnostic knowledge in video question answering"
      }
    ]
  },
  {
    "index": 25,
    "papers": [
      {
        "key": "Qu2021KIVQA",
        "author": "Qu, Chen and Zamani, Hamed and Yang, Liu and Croft, W. Bruce and Learned-Miller, Erik",
        "title": "Passage Retrieval for Outside-Knowledge Visual Question Answering"
      },
      {
        "key": "Salemi2023ICTIR",
        "author": "Salemi, Alireza and Rafiee, Mahta and Zamani, Hamed",
        "title": "Pre-Training Multi-Modal Dense Retrievers for Outside-Knowledge Visual Question Answering"
      },
      {
        "key": "Salemi2023MMFID",
        "author": "Salemi, Alireza and Altmayer Pizzorno, Juan and Zamani, Hamed",
        "title": "A Symmetric Dual Encoding Dense Retrieval Framework for Knowledge-Intensive Visual Question Answering"
      },
      {
        "key": "mavex",
        "author": "Wu, Jialin and Lu, Jiasen and Sabharwal, Ashish and Mottaghi, Roozbeh",
        "title": "Multi-Modal Answer Validation for Knowledge-Based VQA"
      },
      {
        "key": "krisp",
        "author": "Kenneth Marino and Xinlei Chen and Devi Parikh and Abhinav Kumar Gupta and Marcus Rohrbach",
        "title": "KRISP: Integrating Implicit and Symbolic Knowledge for Open-Domain Knowledge-Based VQA"
      },
      {
        "key": "kat",
        "author": "Gui, Liangke  and\nWang, Borui  and\nHuang, Qiuyuan  and\nHauptmann, Alexander  and\nBisk, Yonatan  and\nGao, Jianfeng",
        "title": "{KAT}: A Knowledge Augmented Transformer for Vision-and-Language"
      },
      {
        "key": "unifer",
        "author": "Guo, Yangyang and Nie, Liqiang and Wong, Yongkang and Liu, Yibing and Cheng, Zhiyong and Kankanhalli, Mohan",
        "title": "A Unified End-to-End Retriever-Reader Framework for Knowledge-Based VQA"
      }
    ]
  }
]