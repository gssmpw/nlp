\section{Related Work}
This section reviews prior work on video understanding and question answering as well as knowledge-intensive visual QA.
% \hamed{note to self: decide later whether a section on RAG should be added or not.}

\paragraph{\textbf{Video Question Answering}}
VideoQA has evolved significantly from simple visual recognition to complex temporal and knowledge-based reasoning tasks. This evolution can be traced through three distinct stages ____. Initially, approaches combined CNNs for visual features with RNNs for text processing ____, focusing primarily on visual content understanding. The field then progressed to more sophisticated architectures like Vision Transformers ____ and BERT-based models that leveraged cross-modal pre-training ____ in the second stage.

The current era features VLMs that combine visual encoders like CLIP ____ or EVA-CLIP ____ with powerful language models such as LLaMA ____ and Vicuna ____. Notable examples include VideoChat ____, Video-LLaVA ____, LLaMA-VID ____, InternVL ____, and QwenVL ____. These models have made significant contributions, which demonstrate strong capabilities in video understanding and temporal reasoning. Commercial models like GPT-4V ____ and Gemini ____ have further pushed the boundaries, achieving near-human performance on some visual understanding tasks.

While these models excel at standard VideoQA tasks, recent studies ____ reveal concerning limitations, e.g., models can often answer questions correctly with irrelevant or even no video inputs, suggesting reliance on language priors rather than true visual understanding. Additionally, current models struggle with temporal reasoning and content ordering, highlighting the gap between model capabilities and human-like video comprehension ____.

This work extends the current state-of-the-art VideoQA models to knowledge-intensive questions through retrieval augmentation, an area of research that is yet underexplored.

\paragraph{\textbf{Knowledge-Intensive Visual Question Answering}}
KI-VQA emerged to address the fundamental limitation of standard VQA systems--the constraint of inferring answers solely from training data. Given the finite nature of any training dataset, the knowledge scope of traditional VQA systems remains inherently limited. KI-VQA approaches aim to overcome this by incorporating external knowledge sources into the question answering process. Unlike this work, KI-VQA models mostly focus on images.

Early systems focused on common sense-enabled VQA, where models were augmented with basic world knowledge. These initial approaches either created specialized image-focused knowledge bases with template questions ____ or extracted information from general-purpose knowledge bases like DBpedia ____ to enhance VQA accuracy.
Several datasets have been introduced to study different aspects of knowledge integration.  KB-VQA ____ and FVQA ____ focus on questions generated from templates or knowledge base facts. R-VQA ____ studies relational fact reasoning, requiring models to understand relationships between entities. OK-VQA ____ contains free-form factoid questions without explicit knowledge annotations, requiring models to use an external knowledge base for answering natural language questions about images.

Most early KI-VQA datasets imposed significant constraints on question formulation, either through template-based generation or direct extraction from knowledge bases. This limited their ability to capture the complexity and diversity of real-world questions. KnowIT VQA ____ and KnowIT-X VQA ____ datasets extended knowledge-based reasoning to video understanding. They were the first datasets to explore external knowledge questions in video content, with questions freely proposed by qualified workers to study both knowledge integration and temporal coherence. They are used in this research and are introduced in more detail in Section~\ref{sec:data}. While significant progress has been made in image-based KI-VQA ____, extending these approaches to video understanding (i.e., KI-VideoQA) presents unique challenges that are understudied. For instance, it is still unclear what and how external information should be used for effective development of KI-VideoQA systems. This paper bridges these gaps.