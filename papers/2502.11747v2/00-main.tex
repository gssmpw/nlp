% \documentclass[sigconf,review,anonymous]{acmart}
% \usepackage[a-1b]{pdfx}
\documentclass[sigconf]{acmart}
\copyrightyear{2025}
\acmYear{2025}
\setcopyright{rightsretained}

%% These commands are specific for your submission.
\acmConference[SIGIR '25]{Proceedings of the 48th International ACM SIGIR Conference on Research and Development in Information Retrieval}{July 13--18, 2024}{Padua, Italy.}
\acmBooktitle{Proceedings of the 48th Int'l ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR '25), July 13--18, 2024, Padua, Italy}




\makeatother


%
% inline lists.  usage,
%    \begin{inlinelist}
%       \item first item,
%       \item second item, and
%       \item last item.
%    \end{inlinelist}
%
\usepackage{enumitem}
\newlist{inlinelist}{enumerate*}{1}
\setlist*[inlinelist,1]{%
  label=(\roman*),
}
\usepackage{subfigure}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{array}
\usepackage{makecell}
\usepackage{colortbl}
\usepackage{xcolor}
\usepackage{bbm}
\usepackage{arydshln}

\input{XX-notation}

\settopmatter{printacmref=false}



\title{Open-Ended and Knowledge-Intensive Video Question Answering}

% \title{Multi-Modal Retrieval Augmentation for Open-Ended and Knowledge-Intensive Video Question Answering}



\author{Md Zarif Ul Alam}
\affiliation{\institution{University of Massachusetts Amherst}
\country{United States}}
\email{zarifalam@cs.umass.edu}

\author{Hamed Zamani}
\affiliation{\institution{University of Massachusetts Amherst}
\country{United States}}
\email{zamani@cs.umass.edu}




\begin{document}



% \fancyhead{}

% \begin{abstract}
% While current video question answering systems perform well on some tasks requiring only direct visual understanding, they struggle with questions demanding knowledge beyond what is immediately observable in the video content. We refer to this challenging scenario as knowledge-intensive video question answering (KI-VideoQA), where models must retrieve and integrate external information with visual understanding to generate accurate responses. This work presents the first attempt to (1) study multi-modal retrieval-augmented generation for KI-VideoQA, and (2) go beyond multi-choice questions by studying open-ended questions in this task. Through an extensive empirical study of state-of-the-art retrieval and vision language models in both zero-shot and fine-tuned settings, we explore how different retrieval augmentation strategies can enhance knowledge integration in KI-VideoQA. We analyze three key aspects: (1) model's effectiveness across different information sources and modalities, (2) the impact of heterogeneous multi-modal context integration, and (3) model's effectiveness across different query formulation and retrieval result consumption. Our results suggest that while retrieval augmentation generally improves performance, its effectiveness varies significantly based on modality choice and retrieval strategy. Additionally, we find that successful knowledge integration often requires careful consideration of query formulation and optimal retrieval depth. Our exploration advances state-of-the-art accuracy for multiple choice questions by over 17.5\% on the KnowIT VQA dataset.
% % As a result of our analysis, we believe that KI-VideoQA systems require specialized approaches to multi-modal retrieval and context integration.
% % Finally, our evaluation in open-ended settings reveals important insights about Vision Language Model's ability to generate faithful and coherent responses while drawing on external knowledge.
% \end{abstract}


\begin{abstract}
Video question answering that requires external knowledge beyond the visual content remains a significant challenge in AI systems. While models can effectively answer questions based on direct visual observations, they often falter when faced with questions requiring broader contextual knowledge. To address this limitation, we investigate knowledge-intensive video question answering (KI-VideoQA) through the lens of multi-modal retrieval-augmented generation, with a particular focus on handling open-ended questions rather than just multiple-choice formats. Our comprehensive analysis examines various retrieval augmentation approaches using cutting-edge retrieval and vision language models, testing both zero-shot and fine-tuned configurations. We investigate several critical dimensions: the interplay between different information sources and modalities, strategies for integrating diverse multi-modal contexts, and the dynamics between query formulation and retrieval result utilization. Our findings reveal that while retrieval augmentation shows promise in improving model performance, its success is heavily dependent on the chosen modality and retrieval methodology. The study also highlights the critical role of query construction and retrieval depth optimization in effective knowledge integration. Through our proposed approach, we achieve a substantial 17.5\% improvement in accuracy on multiple choice questions in the KnowIT VQA dataset, establishing new state-of-the-art performance levels.
\end{abstract}

% \keywords{Video Question Answering; Retrieval Augmented Generation; Vision Language Models; Multi-Modal Question Answering}

% \begin{CCSXML}
% <ccs2012>
% <concept>
% <concept_id>10002951.10003317</concept_id>
% <concept_desc>Information systems~Information retrieval</concept_desc>
% <concept_significance>500</concept_significance>
% </concept>
% <concept>
% <concept_id>10010147.10010257</concept_id>
% <concept_desc>Computing methodologies~Machine learning</concept_desc>
% <concept_significance>500</concept_significance>
% </concept>
% </ccs2012>
% \end{CCSXML}

% \ccsdesc[500]{Information systems~Information retrieval}
% \ccsdesc[500]{Computing methodologies~Machine learning}

\maketitle

\input{01-intro}



\input{02-related-work}

\input{03-dataset}

\input{04-problem-statement}

% \section{Experimental Setup}
% \hamed{this can be a subsection if it's too brief.}

\input{05-results}

% \section{Key Findings and Conclusions}

\input{06-conclusion}

\bibliographystyle{ACM-Reference-Format}
\bibliography{XX-references}

\end{document}
