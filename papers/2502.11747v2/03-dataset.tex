\section{Datasets}
\label{sec:data}
To empirically study our research questions, we adopt two recent knowledge-intensive video question answering datasets that contain multi-choice questions. To the best of our knowledge, there is no publicly available dataset for generating free-form answers in response to knowledge-intensive questions about a video. Therefore, we construct another version of each dataset for free-form question answering tasks. In more detail, we simply use the textual description of the correct option as the reference (ground-truth) answer and do not provide the options to the QA models that are supposed to generate free-form answers. The datasets we adopt are KnowIT VQA \cite{garcia2020knowit} and KnowIT-X VQA \cite{wu2021transferring}. Both of them are constructed from popular TV sitcoms and feature questions that require integrating visual, textual, temporal, and external knowledge understanding.

\paragraph{\textbf{KnowIT VQA}}
KnowIT VQA \cite{garcia2020knowit} contains video clips from the TV show ``The Big Bang Theory'' and comprises 24,282 human-annotated question-answer pairs. The dataset was constructed from 207 episodes, resulting in 12,087 video clips of approximately 20 seconds each. Each clip is accompanied by subtitles and human-written questions that span different types: visual-based (22\%), textual-based (12\%), temporal-based (4\%), and knowledge-based (62\%). Importantly, each question-video pair includes an annotated knowledge snippet that captures the external information required to answer the question correctly. The models do not use this annotated knowledge snippet to simulate real-world question answering tasks over videos.

\paragraph{\textbf{KnowIT-X VQA}}
KnowIT-X VQA \cite{wu2021transferring} extends the methodology used by KnowIT VQA to another popular sitcom TV show, ``Friends''. It contains 21,412 question-video pairs from 202 episodes, divided into 12,176 video clips. Similar to its predecessor, data samples include video clips, subtitles, questions, four options (one correct), and an annotated snippet as the knowledge information.
% Similarly, the models in our experiments do not have access to this manually annotated knowledge snippet and are expected to find relevant knowledge from various sources that will be described in each experiment.


% \hamed{note to self: later check whether needed to mention information about retrieval corpora here.}

% \begin{table}[t]
% \caption{Statistics of the KnowIT and KnowIT-X datasets.}
% \label{tab:dataset_stats}
% % \small
% \begin{tabular}{lrr}
% \toprule
%  & \textbf{KnowIT} & \textbf{KnowIT-X} \\
% \midrule
% % \multicolumn{3}{l}{Dataset Scale} \\
% \# Episodes & 207 & 202 \\
% \# Scenes & 2,472 & 2,565 \\
% \# Short video clips & 12,087 & 12,176 \\
% \# Video-Question-Answer Tuples & 24,282 & 21,412 \\
% \midrule
% % \multicolumn{3}{l}{Average length (\# tokens)} \\
% Average question length & 7.49 & 7.65 \\
% Average subtitles length & 56.79 & 39.06 \\
% Average ground-truth answer length & 4.54 & 2.30 \\
% % Wrong Ans. & 4.13 & 2.06 \\ 
% \bottomrule
% \end{tabular}
% \end{table}