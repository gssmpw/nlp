\section{Results and Discussion}
In this section, we present our empirical results to study seven research questions. All experiments were conducted using 1 NVIDIA A100 GPU with 80GB memory. We use half-precision (fp16) computations to optimize memory usage and processing speed.

\paragraph{\textbf{Evaluation Metrics}} We evaluate our models using a wide range of metrics. Following prior work \cite{garcia2020knowit,wu2021transferring}, we use accuracy to measure model's effectiveness for multiple choice questions. For open-ended questions, we use ROUGE-1, ROUGE-L \cite{lin-2004-rouge}, BLEU-1, BLEU-4 \cite{papineni2002bleu}, and term overlap F1 (TO F1) \cite{Kong2013} for measuring various aspects of lexical overlap between the generated answer and the reference ground truth answer. We also use METEOR \cite{banerjee2005meteor} and BERTScore \cite{zhang2019bertscore} for measuring semantic similarity. This diverse set of metrics allows us to evaluate different aspects of answer quality, from lexical accuracy to semantic appropriateness. All metrics are computed on the test sets of each dataset.

To measure statistically significant improvements, we use a two-tailed paired t-test. Due to the binary nature of accuracy metric per query, we use McNemar test for this metric.
We report significant improvements where p-value is less than 0.05.
% \subsection{Evaluation}
% \hamed{we will decide about this section later, once we have a better understanding of the paper length. This can potentially be much shorter.}

% Given a test set $\mathcal{D} = {(v_i, q_i, a_i^*)}{i=1}^{|\mathcal{D}|}$, we evaluate our models using different metrics for both multiple choice questions (MCQ) and open-ended responses.
% \subsubsection{Multiple Choice Questions}
% For MCQ, we evaluate models in both zero-shot and fine-tuned settings using:
% \begin{equation}
% \text{Accuracy} = \frac{1}{D}\sum_{i=1}^D \mathbbm{1}\{a_i = a_i^*\}
% \end{equation}
% where $\mathbbm{1}\{\cdot\}$ is the indicator function and $a_i$ is the model's predicted answer. 


% \subsubsection{Open-ended Questions}
% For open-ended responses, we employ a comprehensive set of natural language generation metrics:
% \begin{itemize}
% \item \textbf{ROUGE scores}: We compute ROUGE-1 and ROUGE-L to evaluate lexical overlap and longest common subsequence between generated and reference answers:
% \begin{equation}
% \text{ROUGE-n} = \frac{\sum_{gram_n \in ref} Count_{match}(gram_n)}{\sum_{gram_n \in ref} Count(gram_n)}
% \end{equation}

% \item \textbf{METEOR}: To capture semantic similarity beyond exact matches, incorporating synonyms and paraphrases

% \item \textbf{BLEU scores}: We use both BLEU-1 and BLEU-4 to evaluate n-gram precision:
% \begin{equation}
%     \text{BLEU-n} = BP \cdot \exp(\sum_{i=1}^n w_i \log p_i)
% \end{equation}

% \item \textbf{F1 score}: To measure the balance between precision and recall in answer generation

% \item \textbf{BERTScore}: To capture semantic similarity using contextual embeddings
% \end{itemize}



\begin{table*}[t]
\centering
\caption{The results obtained by state-of-the-art VLMs for both multiple choice and open-ended questions on the KnowIT VQA dataset, under both zero-shot and fine-tuned settings. Current best reported performance on the KnowITVQA dataset is also shown in the table. Boldface indicates highest number in the column.}
\resizebox{1.8\columnwidth}{!}{%
\begin{tabular}{llccccccccc}
\toprule
\multirow{2}{*}{\textbf{Model}} & \multirow{2}{*}{\textbf{Setting}} & \multicolumn{1}{c}{\textbf{MCQ}} & & \multicolumn{7}{c}{\textbf{Open-Ended Questions}} \\
\cmidrule{3-3}\cmidrule{5-11}
 & & \textbf{\% Accuracy} & & \textbf{ROUGE-1} & \textbf{ROUGE-L} & \textbf{METEOR} & \textbf{BLEU-1} & \textbf{BLEU-4} & \textbf{TO F1} & \textbf{BERTScore} \\
\midrule
ROCK \cite{garcia2020knowit} & Fine-tuned & 65.20 && - & - & -& -& -& -& -   \\
\midrule 
\multirow{2}{*}{Qwen2VL 2B} & Zero-shot & 52.01 && 0.1266&	0.1156&	0.1471&	0.0882&	0.0163&	0.1086&	0.0777 \\
 & Fine-tuned & \textbf{67.34} && \textbf{0.3481}&	\textbf{0.3408}&	\textbf{0.2162}&	\textbf{0.3022}&	\textbf{0.0761}&	\textbf{0.2770}&	\textbf{0.4477} \\
\midrule 
\multirow{2}{*}{InternVL2-2B} & Zero-shot & 50.78 && 0.1450&	0.1345&	0.1548&	0.1027&	0.0200&	0.1265&	0.1177 \\
 & Fine-tuned & 67.17&& 0.3397&	0.3331&	0.2098&	0.2942&	0.0724&	0.2702&	0.4328 \\
\midrule 
GPT-4V & Zero-shot & 65.77&& 0.1327&	0.1225&	0.1685&	0.0813&	0.0172&	0.1313&	0.0493 \\ 
% \cline{4-10}
 % & Fine-tuned & x & \multicolumn{7}{c|}{x} \\
\bottomrule
\end{tabular}
}
\label{tab:vlm_results}
\end{table*}







\begin{table*}[t]
\centering
\caption{The results obtained by a retrieval augmented Qwen2VL 2B model \cite{wang2024qwen2vl} on the KnowIT VQA dataset using three diverse retrieval models (i.e., BM25, Stella, and NV-Embed-v2). We use three different knowledge sources for retrieval augmentation (i.e., video subtitles, automatically generated video captions, and videos themselves). Boldface indicates highest number in the column. Best performing zero-shot results are highlighted using underline. The superscript $^\triangle$ indicates improvement compared to the same VLM under the same experimental setting without retrieval augmentation. $^\blacktriangle$ means improvements are statistically significant.}
\resizebox{1.8\columnwidth}{!}{%
\begin{tabular}{llllllllllll}
\toprule
{\textbf{Retrieval}} & \textbf{Retrieval} & \multirow{2}{*}{\textbf{Setting}} & \multicolumn{1}{c}{\textbf{MCQ}} && \multicolumn{7}{c}{\textbf{Open-Ended Questions}} \\
\cmidrule{4-4}\cmidrule{6-12}
\textbf{Corpus} & \textbf{Model} &&  \textbf{\% Accuracy} & & \textbf{ROUGE-1} & \textbf{ROUGE-L} & \textbf{METEOR} & \textbf{BLEU-1} & \textbf{BLEU-4} & \textbf{TO F1} & \textbf{BERTScore} \\
\midrule
\multirow{6}{*}{\rotatebox{270}{Subtitles}} & \multirow{2}{*}{BM25}  & Zero-shot & 44.89 && 0.1282$^\triangle$ & 0.1169$^\triangle$ & 0.1391 & 0.0931$^\blacktriangle$ & 0.0180$^\blacktriangle$ & 0.1082 & 0.1061$^\blacktriangle$ \\
% \cline{4-12}
 & & Fine-tuned & 70.27$^\blacktriangle$  && 0.3586$^\triangle$ & 0.3501$^\triangle$ & 0.2260$^\blacktriangle$ & 0.3117$^\triangle$ & 0.0805$^\blacktriangle$ & 0.3020$^\blacktriangle$ & 0.4522$^\triangle$ \\
\cmidrule{2-12}
 & \multirow{2}{*}{Stella}  & Zero-shot & 47.81  && 0.1375$^\triangle$ & 0.1259$^\triangle$ & 0.1514$^\blacktriangle$ & 0.0992$^\blacktriangle$ & 0.0199$^\blacktriangle$ & 0.1183$^\blacktriangle$ & 0.1134$^\blacktriangle$ \\
% \cline{4-12}
 & & Fine-tuned & 72.34$^\blacktriangle$  && 0.3842$^\blacktriangle$ & 0.3763$^\blacktriangle$ & 0.2437$^\blacktriangle$ & 0.3320$^\blacktriangle$ & 0.0889$^\blacktriangle$ & 0.3245$^\blacktriangle$ & 0.4561$^\blacktriangle$ \\
\cmidrule{2-12}
 & \multirow{2}{*}{NV-Embed-v2}  & Zero-shot & 51.67  && \underline{0.1434}$^\blacktriangle$ & \underline{0.1316}$^\blacktriangle$ & \underline{0.1583}$^\blacktriangle$ & \underline{0.1009}$^\blacktriangle$ & \underline{0.0201}$^\blacktriangle$ & \underline{0.1280}$^\blacktriangle$ & \underline{0.1156}$^\blacktriangle$ \\
% \cline{4-12}
 & & Fine-tuned & \textbf{74.84$^\blacktriangle$}  && \textbf{0.3950$^\blacktriangle$} & \textbf{0.3862$^\blacktriangle$} & \textbf{0.2511$^\blacktriangle$} & \textbf{0.3429$^\blacktriangle$} & \textbf{0.0904$^\blacktriangle$} & \textbf{0.3394$^\blacktriangle$} & \textbf{0.4656$^\blacktriangle$} \\
\midrule

 \multirow{6}{*}{\rotatebox{270}{Video Captions}} & \multirow{2}{*}{BM25} & Zero-shot & 32.23 && 0.0868 & 0.0789 & 0.1009 & 0.0627 & 0.0112 & 0.0614 & 0.0192 \\
% \cline{4-12}
& & Fine-tuned & {66.71} & &  0.3428 & 0.3363 & 0.2149 & 0.2992 & 0.0753 & 0.2765 & 0.4421 \\
\cmidrule{2-12}
& \multirow{2}{*}{Stella} & Zero-shot & 40.83  & & 0.1012 & 0.0906 & 0.1163 & 0.0724 & 0.0128 & 0.0766 & 0.0563 \\
% \cline{4-12}
& & Fine-tuned & 65.90 & &  {0.3473} & 0.3415$^\triangle$ & {0.2200$^\triangle$} & {0.3050$^\triangle$} & {0.0767$^\triangle$} & {0.2795$^\triangle$} & {0.4449} \\
\cmidrule{2-12}
& \multirow{2}{*}{NV-Embed-v2} & Zero-shot & 40.03 & &  0.0930 & 0.0841 & 0.1098 & 0.0672 & 0.0119 & 0.0717 & 0.0449 \\
% \cline{4-12}
 & & Fine-tuned & 66.41  && 0.3421 & 0.3354 & 0.2150 & 0.2991 & 0.0762$^\triangle$ & 0.2749 & 0.4487$^\triangle$ \\
\midrule\midrule


\multirow{6}{*}{\rotatebox{270}{Videos}} & \multirow{2}{*}{BM25} & Zero-shot & \underline{53.16}$^\blacktriangle$ && 0.1264&	0.1151&	0.1466&	0.0878&	0.0161&	0.1085&	0.0761\\

% \cline{4-12}
 & & Fine-tuned & 65.81 && 0.3488$^\triangle$&	0.3407&	0.2165$^\triangle$&	0.3017&	0.0755&	0.2791$^\triangle$&	0.4477\\
\cmidrule{2-12}
& \multirow{2}{*}{Stella} & Zero-shot & 50.69&& 		0.1154&	0.1046&	0.1317&	0.0817&	0.0148&	0.0911&	0.1034$^\triangle$ \\
% \cline{4-12}
& & Fine-tuned & 63.78&	& 	0.3362&	0.3285 &	0.2049&	0.2875&	0.0716&	0.2648&	0.4350 \\
\cmidrule{2-12}
& \multirow{2}{*}{NV-Embed-v2} & Zero-shot & 50.65&	& 	0.1259&	0.1147&	0.1431&	0.0880&	0.0159&	0.0991&	0.0789$^\triangle$ \\
% \cline{4-12}
& & Fine-tuned & 58.95&& 		0.3302&	0.3220&	0.2016&	0.2820&	0.0700&	0.2558&	0.4322 \\
\bottomrule
\end{tabular}
}
\label{tab:diff_corpora}
\end{table*}














\begin{table*}[t]
\caption{The results obtained by a retrieval augmented Qwen2VL 2B model \cite{wang2024qwen2vl} using multiple heterogeneous corpora as knowledge sources on the KnowIT VQA dataset. NV-Embed-v2 (our best performing retrieval model) is used for information retrieval. Boldface indicates highest number in the column. Best performing zero-shot results are highlighted using underline. The superscript $^\blacktriangle$ indicates improvement compared to the same VLM under the same experimental setting without retrieval augmentation.}
\centering
\resizebox{1.8\columnwidth}{!}{%
\begin{tabular}{lllcccccccc}
\toprule
 \textbf{Retrieval} & \multirow{2}{*}{\textbf{Setting}} & \multicolumn{1}{c}{\textbf{MCQ}} & \multicolumn{7}{c}{\textbf{Open-Ended Questions}} \\
\cmidrule{3-3} \cmidrule{5-11}
\textbf{Corpora} & & \textbf{\% Accuracy} && \textbf{ROUGE-1} & \textbf{ROUGE-L} & \textbf{METEOR} & \textbf{BLEU-1} & \textbf{BLEU-4} & \textbf{TO F1} & \textbf{BERTScore} \\
\midrule
Subtitles \&   & Zero-shot & 46.67 && \underline{0.1225} & \underline{0.1117} & 0.1468 & \underline{0.0854} & \underline{0.0164}$^\triangle$ & \underline{0.1069} & \underline{0.0762} \\
% \cline{4-12}
Video Captions & Fine-tuned & \textbf{75.60}$^\blacktriangle$ && \textbf{0.3947}$^\blacktriangle$ & \textbf{0.3874}$^\blacktriangle$ & \textbf{0.2494}$^\blacktriangle$ & \textbf{0.3445}$^\blacktriangle$ & \textbf{0.0903}$^\blacktriangle$ & \textbf{0.3388}$^\blacktriangle$ & 0.4678$^\blacktriangle$ \\
\midrule
 Subtitles \&  & Zero-shot & \underline{56.92}$^\blacktriangle$&&		0.1219&	0.1106&	\underline{0.1513}$^\triangle$ &	0.0826&	0.0157&	0.1049&	0.0698 \\
Videos & Fine-tuned & 73.31$^\blacktriangle$&&		0.3855$^\blacktriangle$&	0.3775$^\blacktriangle$&	0.2419$^\blacktriangle$&	0.3363$^\blacktriangle$&	0.0873$^\blacktriangle$&	0.3267$^\blacktriangle$&	0.4653$^\blacktriangle$ \\
\midrule
 Videos \&  & Zero-shot & 40.61&	&	0.0981&	0.0882&	0.1155&	0.0681&	0.0117&	0.074&	0.0274 \\
% \cline{4-12}
Video Captions & Fine-tuned & 65.26&&		0.3363&	0.3294&	0.2080&	0.2931&	0.0727&	0.2668&	0.4417 \\
\midrule
 Subtitles \& Videos \& & Zero-shot & 48.50 & & 0.1089 & 0.0984 & 0.1316 & 0.0743 & 0.0136 & 0.0869 & 0.0483 \\
% \cline{4-12}
 Video Captions  & Fine-tuned & 73.78$^\blacktriangle$ && 0.3867$^\blacktriangle$ & 0.3788$^\blacktriangle$ & 0.2470$^\blacktriangle$ & 0.3375$^\blacktriangle$ & 0.0886$^\blacktriangle$ & 0.3289$^\blacktriangle$ & \textbf{0.4692}$^\blacktriangle$ \\
\bottomrule
\end{tabular}
}
\label{tab:heterogenuous}
\end{table*}






\begin{table*}[t]
\centering
\caption{The results obtained by a (retrieval augmented) Qwen2VL 2B model \cite{wang2024qwen2vl} for different retrieval corpora on the \textit{KnowIT-X} VQA dataset. NV-Embed-v2 (our best performing retrieval model) is used for information retrieval. Boldface indicates highest number in the column. Best performing zero-shot results are highlighted using underline.}
\resizebox{1.8\columnwidth}{!}{%
\begin{tabular}{llccccccccc}
\toprule
\textbf{Retrieval} & \multirow{2}{*}{\textbf{Setting}} & \multicolumn{1}{c}{\textbf{MCQ}} & & \multicolumn{7}{c}{\textbf{Open-Ended Questions}} \\
\cmidrule{3-3}\cmidrule{5-11}
\textbf{Corpus} & & \textbf{\% Accuracy} & & \textbf{ROUGE-1} & \textbf{ROUGE-L} & \textbf{METEOR} & \textbf{BLEU-1} & \textbf{BLEU-4} & \textbf{TO F1} & \textbf{BERTScore} \\
\midrule 
\multirow{3}{*}{None} & Zero-shot & 54.88 && 0.0995&	0.0952&	0.1431&	0.0768&	0.0177&	0.1115&	0.0534 \\
& Fine-tuned & 64.42&& 0.2894&	0.2870&	0.1719&	0.2483&	0.0664&	0.2608&	0.4989\\
& Transfer learning & 55.13&&		0.2894&	0.287&	0.1719&	0.2483&	0.0664&	0.2608&	0.4989\\\midrule

\multirow{3}{*}{Subtitles} & Zero-shot & 60.35 && 0.1268& 0.121&	0.1763&	0.0939&	0.0217&	0.1414&	0.0986 \\
& Fine-tuned & 74.29 && 0.5472&0.5423&	0.3646&	0.5051&	0.1559&	0.5083&	0.6419 \\
& Transfer learning & 70.71 && 0.4032&	0.399&	0.234&	0.3432&	0.0852&	0.3826&	0.5365 \\\midrule

\multirow{3}{*}{Video Captions} & Zero-shot & 48.40 && 0.0574&	0.0546&	0.0922&	0.048&	0.0104&	0.0583&	0.0016\\
& Fine-tuned & 62.29 && 0.4652&	0.4604&	0.3251&	0.4359&	0.1432&	0.4227&	0.6178 \\
& Transfer learning & 52.81 && 0.2386&	0.2368&	0.1447&	0.2031&	0.0544&	0.213&	0.4746\\\midrule

\multirow{3}{*}{Videos} & Zero-shot & 51.74 && 0.096&	0.0911&	0.1384&	0.076&	0.0176&	0.1017&	0.0561 \\
& Fine-tuned & 62.82 && 0.4591&	0.4541&	0.3093&	0.4265&	0.1357&	0.4157&	0.6067 \\
& Transfer learning & 57.6 && 0.2833&	0.28&	0.1624&	0.2421&	0.0628&	0.2572&	0.4786 \\\midrule

\multirow{3}{*}{Subtitles \& Video Captions} & Zero-shot & 49.80&& 0.0912&	0.0871&	0.1426&	0.0673&	0.0144&	0.1056&	0.0416 \\
& Fine-tuned & \textbf{74.44}&& \textbf{0.5479}&	\textbf{0.5426}&	\textbf{0.3693}&	\textbf{0.5084}&	\textbf{0.1570}&	\textbf{0.5147}&	\textbf{0.6479} \\
& Transfer learning & 68.58&& 0.4009&	0.3971&	0.2349&	0.3399&	0.0838&	0.3754&	0.5258 \\\midrule

\multirow{3}{*}{Subtitles \& Videos} & Zero-shot & 59.15&& 0.1016&	0.0968&	0.1564&	0.0726&	0.0155&	0.1196&	0.0522  \\
& Fine-tuned & 73.38&& 0.5387&	0.5333&	0.3625&	0.5007&	0.1554&	0.5030&	0.6364 \\
& Transfer learning & 70.04&& 0.3854&	0.3828&	0.2217&	0.3292&	0.0811&	0.3622&	0.5217 \\\midrule

\multirow{3}{*}{Videos \& Video Captions} & Zero-shot & 42.49&& 0.0513&	0.0487&	0.0857&	0.0431&	0.009&	0.0472&	0.0213 \\
& Fine-tuned & 60.50&& 0.4505&	0.4460&	0.3100&	0.4212&	0.1345&	0.4099&	0.5998 \\
& Transfer learning & 57.26&&	0.2366&	0.2347&	0.1403&	0.1987&	0.0523&	0.2126&	0.4629 \\\midrule

\multirow{3}{*}{Subtitles \& Videos \& Video Captions} & Zero-shot & 52.08&& 0.0756&	0.0718&	0.1231&	0.0558&	0.0113&	0.086&	0.0106 \\
& Fine-tuned & 72.19&& 0.5382&	0.5333&	0.3647&	0.5006&	0.1553&	0.5028&	0.6407 \\
& Transfer learning & 68.93&& 0.3866&	0.3818&	0.2262&	0.3341&	0.0818&	0.363&	0.5244 \\
\bottomrule
\end{tabular}
}
\label{tab:knowitx}
\end{table*}






\begin{table*}[t]
\centering
\caption{The results obtained by a retrieval augmented Qwen2VL 2B model \cite{wang2024qwen2vl} using video subtitles as the knowledge source on the KnowIT VQA dataset. NV-Embed-v2 (our best performing retrieval model) is used for information retrieval. Boldface indicates highest number in the column. Best performing zero-shot results are highlighted using underline.}
\resizebox{1.6\columnwidth}{!}{%
\begin{tabular}{llccccccccc}
\toprule
 \multirow{2}{*}{\textbf{Setting}} & \multirow{2}{*}{\textbf{k}} & \multicolumn{1}{c}{\textbf{MCQ}} & & \multicolumn{7}{c}{\textbf{Open-Ended Questions}} \\
\cmidrule{3-3}\cmidrule{5-11}
 & & \textbf{\% Accuracy} & & \textbf{ROUGE-1} & \textbf{ROUGE-L} & \textbf{METEOR} & \textbf{BLEU-1} & \textbf{BLEU-4} & \textbf{TO F1} & \textbf{BERTScore} \\
\hline
\multirow{5}{*}{{Zero-shot}} & 0 & 52.01 && 0.1266&	0.1156&	0.1471&	0.0882&	0.0163&	0.1086&	0.0777 \\
 & 2 & 54.21 & & 0.1527 & 0.1413 & 0.1658 & 0.1081 & 0.0226 & 0.1370 & 0.1248 \\
&{5}  & 54.60 && \underline{0.1536} & \underline{0.1426} & \underline{0.1689} & \underline{0.1082} & \underline{0.0228} & \underline{0.1375} & \underline{0.1290} \\
 &{10}  & 51.67 && 0.1434 & 0.1316 & 0.1583 & 0.1009 & 0.0201 & 0.1280 & 0.1156 \\
&{20} & \underline{55.23} && 0.1225 & 0.1127 & 0.1383 & 0.0876 & 0.0171 & 0.1042 & 0.0946 \\
\midrule

\multirow{5}{*}{{Fine-tuned}} & 0 & {67.34} && {0.3481}&	{0.3408}&	{0.2162}&	{0.3022}&	{0.0761}&	{0.2770}&	{0.4477} \\
 &{2}  & 70.73 && 0.3764 & 0.3686 & 0.239 & 0.3273 & 0.0861 & 0.3151 & 0.4632 \\
% \cline{4-13}
% \cline{5-13}
& {5}  & 74.54 & & 0.3978 & 0.3894 & 0.2525 & 0.3466 & 0.0920 & 0.3411 & \textbf{0.4785} \\
% \cline{4-13}
% \cline{5-13}
&{10}  & 74.84 && 0.3950 & 0.3862 & 0.2511 & 0.3429 & 0.0904 & 0.3394 & 0.4656 \\
% \cline{4-13}
% \cline{5-13}
& {20}  & \textbf{74.96} & & \textbf{0.4036} & \textbf{0.3962} & \textbf{0.2556} & \textbf{0.3502} & \textbf{0.0915} & \textbf{0.3518} & 0.4677 \\
\midrule
\end{tabular}
}
\label{tab:topk-comparison}
\end{table*}











\begin{table*}[t]
\centering
\caption{The results obtained by a retrieval augmented Qwen2VL 2B model \cite{wang2024qwen2vl} for four different query formulation strategies on the KnowIT VQA dataset. Video subtitles are used as the knowledge source. Boldface indicates highest number in the column. Best performing zero-shot results are highlighted using underline.}
\resizebox{1.8\columnwidth}{!}{%
\small
\begin{tabular}{llllllllllll}
\toprule
\multirow{2}{*}{\textbf{Query}} & \textbf{Retrieval} & \multirow{2}{*}{\textbf{Setting}} & \multicolumn{1}{c}{\textbf{MCQ}} && \multicolumn{7}{c}{\textbf{Open-Ended Questions}} \\
\cmidrule{4-4}\cmidrule{6-12}
 & \textbf{Model} &&  \textbf{\% Accuracy} & & \textbf{ROUGE-1} & \textbf{ROUGE-L} & \textbf{METEOR} & \textbf{BLEU-1} & \textbf{BLEU-4} & \textbf{TO F1} & \textbf{BERTScore} \\
\midrule

\multirow{6}{*}{\rotatebox{270}{question-only}} & \multirow{2}{*}{BM25} & Zero-shot & 32.91 && 0.1181 & 0.1076 & 0.1258 & 0.0875 & 0.0161 & 0.0908 & 0.1019 \\
 & & Fine-tuned & 67.01 && 0.3580 & 0.3498 & 0.2224 & 0.3105 & 0.0790 & 0.2878 & 0.4494 \\
\cmidrule{2-12}
 & \multirow{2}{*}{Stella} & Zero-shot & 44.30 && 0.1261 & 0.1155 & 0.1377 & 0.0924 & 0.0180 & 0.1003 & 0.1096 \\
 & & Fine-tuned & 70.61 && 0.3755 & 0.3671 & 0.2346 & 0.3243 & 0.0841 & 0.3156 & 0.4536 \\
\cmidrule{2-12}
 & \multirow{2}{*}{NV-Embed-v2} & Zero-shot & 44.64 && 0.1332 & 0.1217 & 0.1442 & 0.0964 & 0.0191 & 0.1044 & 0.1118 \\
 & & Fine-tuned & 70.81 && 0.3830 & 0.3761 & 0.2419 & 0.3335 & 0.0877 & 0.3235 & \textbf{0.4667} \\
\midrule
\multirow{6}{*}{\rotatebox{270}{question \& options}}& \multirow{2}{*}{BM25} & Zero-shot & 39.05 && - &- &- &- &- &- &-  \\
 & & Fine-tuned & 76.23 && - &- &- &- &- &- &- \\
\cmidrule{2-12}
 & \multirow{2}{*}{Stella} & Zero-shot & 46.71 && - &- &- &- &- &- &- \\
& & Fine-tuned & 73.90 && - &- &- &- &- &- &- \\
\cmidrule{2-12}
 & \multirow{2}{*}{NV-Embed-v2} & Zero-shot & 47.64 && - &- &- &- &- &- &- \\
 & & Fine-tuned & \textbf{76.75} && - &- &- &- &- &- &- \\
\midrule
\multirow{6}{*}{\rotatebox{270}{question \& subtitle}} & \multirow{2}{*}{BM25} & Zero-shot & 44.89 && 0.1282 & 0.1169 & 0.1391 & 0.0931 & 0.0180 & 0.1082 & 0.1061 \\
 & & Fine-tuned & 70.27 && 0.3586 & 0.3501 & 0.2260 & 0.3117 & 0.0805 & 0.3020 & 0.4522 \\
\cmidrule{2-12}
 & \multirow{2}{*}{Stella} & Zero-shot & 47.81& & 0.1375 & 0.1259 & 0.1514 & 0.0992 & 0.0199 & 0.1183 & 0.1134 \\
 & & Fine-tuned & 72.34 && 0.3842 & 0.3763 & 0.2437 & 0.3320 & 0.0889 & 0.3245 & 0.4561 \\
\cmidrule{2-12}
 & \multirow{2}{*}{NV-Embed-v2} & Zero-shot & \underline{51.67} && 0.1434 & 0.1316 & 0.1583 & 0.1009 & 0.0201 & 0.1280 & 0.1156 \\
 & & Fine-tuned & 74.84 & &\textbf{0.3950} & \textbf{0.3862} & \textbf{0.2511} & \textbf{0.3429} & \textbf{0.0904} & \textbf{0.3394} & 0.4656 \\
\midrule 
\multirow{6}{*}{\rotatebox{270}{question rewriting}} & \multirow{2}{*}{BM25} & Zero-shot & 36.63&& 0.1363&	0.1260 &	0.1484&	0.0999&	0.0217&	0.1153&	0.1108 \\
 & & Fine-tuned &  62.30 && 0.3506&	0.3446&	0.2220 &	0.3046&	0.0786&	0.2931&	0.4473\\
\cmidrule{2-12}
 & \multirow{2}{*}{Stella} & Zero-shot & 45.10 && \underline{0.1485} &	\underline{0.1373} &	\underline{0.1660} &	\underline{0.1058} &	\underline{0.0220} &	\underline{0.1287} &	\underline{0.1232} \\
 & & Fine-tuned & 65.01 && 0.3441&	0.3374&	0.2213&	0.2947&	0.0757&	0.2827&	0.4383 \\
\cmidrule{2-12}
 & \multirow{2}{*}{NV-Embed-v2} & Zero-shot & 47.39 && 0.1259&	0.1147&	0.1431&	0.0880 &	0.0159&	0.0991&	0.0789 \\
 & & Fine-tuned & 64.21 && 0.3409 &	0.3312 &	0.2190 &	0.2921 &	0.0760 &	0.2863 &	0.4323 \\

\bottomrule
\end{tabular}
}
\label{tab:diff_queries}
\end{table*}






\paragraph{\textbf{RQ1: How well do state-of-the-art vision language models perform in KI-VQA tasks?}}
To answer this question, we evaluate several state-of-the-art VLMs, including commercial models like GPT-4V \cite{achiam2023gpt} and open-source models like Qwen2VL \cite{wang2024qwen2vl} and InternVL2 \cite{wang2024intern2vl}. We study these models under different circumstances, including both multiple choice and open-ended questions and both zero-shot and fine-tuned settings for open-source VLMs.  The results are presented in Table~\ref{tab:vlm_results}. We also add the current best performance \cite{garcia2020knowit} in the table. Unsurprisingly, the results suggest that fine-tuning both open-source VLMs substantially improve their performance on the KnowIT VQA dataset. The results also suggest that the GPT-4V is the best performing zero-shot VLM for multiple choice question answering. However, the zero-shot InternVL2 model significantly performs better than GPT-4V in the open-ended setting, in terms of ROUGE-1, ROUGE-L, BLEU-1, BLEU-4, and BERTScore. That being said, the fine-tuned Qwen2VL model demonstrates the highest performance across all metrics. This model achieves an accuracy of over 67\% for multiple choice questions. Questions in this setting have four options, meaning that a random selector would achieve a 25\% accuracy. These results not only establish baseline performance for our research, but also enables us to better understand the strengths and limitations of VLMs in handling questions that require external knowledge beyond the video content.

Due to the strong fine-tuned performance of Qwen2VL in Table~\ref{tab:vlm_results}, we perform all retrieval augmentation experiments using this VLM. 



\paragraph{\textbf{RQ2: How does augmenting VLMs with retrieved textual knowledge impact the end-to-end KI-VQA performance?}}
To address this research question, we construct two text corpora for knowledge retrieval as described in Section~\ref{sec:method}. The first corpus consists of subtitles of all videos in the dataset. The second corpus was constructed through automatic captioning of the videos. Video captions often include deep semantic description of video content, serving as a proxy textual representation of the video. For each corpus, we employ three diverse and effective retrieval models (both term matching and dense retrieval models). See the retrieval model details in Section~\ref{sec:method}. In all experiments in this research question, 10 documents are retrieved for augmentation. We report the results in Table~\ref{tab:diff_corpora} (i.e., the top two sections related to subtitles and video captions). 
% , allowing us to evaluate the impact of past dialogues and narratives in the dataset.
Our investigation of subtitle-based retrieval augmentation reveals several important findings. The neural retriever NV-Embed-v2 consistently outperforms other approaches across all metrics, achieving 74.84\% MCQ accuracy and stronger open-ended response scores compared to BM25 and Stella.  Notably, the relatively low absolute scores on open-ended metrics (term overlap F1 scores ranging 0.3020-0.3394) highlight that generating accurate free-form responses remains challenging even with retrieval augmentation. This suggests that improving response generation quality, rather than just retrieval effectiveness, may be a crucial direction for future work.

% The superscript $\triangle$ indicates performance improvement compared to the results obtained by the same VLM without retrieval augmentation (i.e., Qwen2VL 2B results in Table~\ref{tab:vlm_results}).
% Therefore, as is evident from the results, retrieving from the subtitles corpus leads to statistically significant \hamed{?} performance gain in nearly all settings.

Analyzing the results from the video caption-based retrieval augmentation also highlights some important findings. First, augmenting with video captions underperform subtitles in every single experimental setting. The main reason for this observation is that the external knowledge required to answer most questions in the KnowIT dataset is not captured by the video captions. Another reason could be due to the failure of retrieval models in retrieving relevant video captions, compared to subtitles. The second main observation is that retrieval augmentation with video captions often performs even worse than the baseline results presented in Table~\ref{tab:vlm_results} (see Qwen2VL 2B results). For multiple choice questions, video caption-based augmentation deteriorates the results in every case. Underperforming the baseline model in this case means that the VLM cannot accurately and robustly disregard irrelevant information presented to them in retrieval augmentation. For open-ended questions, there are few cases with minor improvements which are negligible. Therefore, the nature of the textual knowledge source and the retrievability of information can substantially impact the behavior of multi-modal retrieval-augmented models in KI-VQA.


\paragraph{\textbf{RQ3: How does augmenting VLMs with retrieved multi-media knowledge (videos) impact the end-to-end KI-VQA performance?}}
VLMs have demonstrated different capabilities in integrating and consuming multi-media content. This research question help us better understand in what modality information needs to be presented to the current VLM technology for effective answer generation. In our experiments, VLMs consume videos frame-by-frame and frames are uniformly sampled at the rate of 1 frame per second. The model details are presented in Section~\ref{sec:method}. The last section of Table~\ref{tab:diff_corpora} reports the video-based retrieval augmentation results. It is worth noting that we observe that multi-modal retrieval models perform poorly for video retrieval, thus we use video subtitles to represents the videos for text-based retrieval; however, their corresponding videos are fed to the VLM for answer generation. 

Interestingly, video-based retrieval augmentation shows notably weaker performance compared to subtitle-based augmentation for open-domain questions.  It is worth highlighting that we could only use one retrieved video because of GPU memory availability. In the fine-tuned setting, even the best performing retriever (BM25) achieves only 65.81\% MCQ accuracy, which is significantly lower than subtitle-based retrieval (74.84\%) and slightly below caption-based retrieval (66.71\%). This performance gap is consistent across all open-ended metrics. However, video-based model demonstrate the strongest zero-shot results for multiple choice questions.

Given this observation, we believe that developing memory efficient VLMs that can consume more videos as context can potentially lead to the best performing multi-modal retrieval augmented systems. Moreover, since this model performs better under the zero-shot setting (compared to the baseline), future work can focus on improving fine-tuning optimizations for video context.




\paragraph{\textbf{RQ4: How does retrieval from heterogeneous information sources with multiple modalities (text and video) impact the end-to-end KI-VQA performance?}}
After examining individual modalities for retrieval augmentation, we investigate whether combining different knowledge sources can further enhance KI-VideoQA performance. We experiment with various combinations of our three knowledge corpora: subtitles (text), video captions (text), and video content (multi-media). For all experiments, we use NV-Embed-v2 as our retriever given its strong performance in single-modality settings, and employ subtitle-enriched queries.

Our results in Table~\ref{tab:heterogenuous} reveal several interesting patterns in multi-modal retrieval. The combination of subtitles and video captions leads to the best fine-tuning performance among all combinations in terms of nearly all metrics (except for BERTScore), reaching 75.6\% MCQ accuracy and the highest scores across all open-ended metrics. Comparing to the results presented in Table~\ref{tab:diff_corpora}, we observe that textual information from different sources (subtitles and captions) complement each other effectively, with captions potentially providing additional semantic context to the already available information in subtitles which is mostly about dialogues and narratives in the video.

Surprisingly, incorporating video retrieval consistently leads to performance degradation in fine-tuned settings. The subtitle-video combination performs worse than subtitle \& caption, and the video \& caption combination  shows the poorest performance (65.26\% MCQ accuracy). Even using all three modalities performs slightly worse than subtitle \& caption alone, achieving 73.78\% MCQ accuracy. 

These findings from fine-tuned experiments indicate that while combining different types of textual information (subtitles and captions) is beneficial, current video retrieval strategy might have introduced noise rather than helpful context. This suggests a need for better video retrieval techniques that can more effectively complement text-based knowledge sources. That said, the highest zero-shot result was achieved by the subtitles \& captions setting, suggesting that zero-shot VLMs can take advantage of multi-media context, but this advantage is lost after fine-tuning.


\paragraph{\textbf{RQ5: How does our empirical findings transfer to another dataset?}}

To assess the generalizability and transferability of our findings, we evaluate our approaches on the KnowIT-X dataset \cite{wu2021transferring}. We explore three experimental settings: zero-shot, fine-tuning, and transfer learning (where we use the model fine-tuned on KnowIT and evaluate it on KnowIT-X). Our results in Table~\ref{tab:knowitx} show several important patterns that largely align with our findings on the KnowIT dataset:

First, subtitle-based retrieval augmentation remains the most effective approach when only one of the knowledge sources are used for retrieval augmentation. When fine-tuned, subtitle retrieval achieves the highest performance (74.29\% MCQ accuracy and consistently stronger open-ended metrics) compared to video captions (62.29\%) and videos (62.82\%). This confirms our earlier observation about the importance of dialogue and narrative information captured in subtitles. Second, combining different knowledge sources shows similar patterns - the subtitle \& video caption combination achieves the strongest performance (74.44\% MCQ accuracy), while incorporating video retrieval tends to slightly degrade performance. This is consistent with our findings on the KnowIT dataset.

In terms of transfer learning, we observe that models pre-trained on the KnowIT dataset can transfer reasonably well to the KnowIT-X dataset. All metrics across MCQ and Open-Ended setting shows that it is significantly better than zero-shot performance but it has some performance degradation compared to the fine-tuned ones. For instance, with subtitle retrieval, transfer learning achieves 70.71\% MCQ accuracy compared to 60.35\% in zero-shot and 74.29\% with fine-tuning. The relatively strong transfer performance suggests that the knowledge integration strategies learned on one TV show can generalize to another, despite differences in content and style. 

% \hamed{write this when the results are ready.} 

\paragraph{\textbf{RQ6: How sensitive are the retrieval-augmented VLMs to the number of retrieved information?}}
To measure model sensitivity to the number of retrieved documents fed to the VLM, we performed an experiments by increasing the number of documents from 2 to 20 for both zero-shot and fune-tuned settings. In this experiments, for the sake of space, we solely focus on our best performing retrieval model NV-Embed-v2 and our best performing open-source VLM, i.e., Qwen2VL 2B. We use the subtitle-enriched questions as the search query. The results are presented in Table~\ref{tab:topk-comparison}. According to the table, the fine-tuned performance keeps increasing for all metrics except for BERTScore as we feed more retrieved documents to the VLM context. Thus the highest performance is achieved when 20 documents is used for augmentation. The model with 5 documents achieves the highest BERTScore. The zero-shot model has a very different behavior. The open-ended performance generally declines if we use more than five documents. This suggests that the zero-shot VLM is not well trained for taking advantage of more documents, since this behavior changes after fine-tuning. That said, the highest MCQ performance is achieved when 20 documents is used. However, the MCQ accuracy does not consistently increase with more documents, i.e., there is a drop in 10 documents. This demonstrates the unpredictable behavior of the zero-shot VLM in consuming long context.



\paragraph{\textbf{RQ7: How do different query formulations for knowledge retrieval impact the end-to-end performance in KI-VideoQA?}}
We study four different query formulations for KI-VideoQA as detailed in Section~\ref{sec:method}. This analysis was conducted on the video subtitles retrieval corpus, as it shows the strongest performance in our previous research questions. The results can be found in Table~\ref{tab:diff_queries}. For the MCQ task, the options-enriched query formulation proves to be the most effective strategy, achieving the highest accuracy across all retrievers (up to 76.747\% with NV-Embed-v2). This suggests that the additional context from answer options helps retrieve more relevant information. However, this advantage is limited to MCQ settings only.

% \hamed{this section will be edited once the results are complete.}
In open-ended scenarios, where $q_{opt}$ is not applicable, subtitle-enriched queries ($q_{sub}$) consistently outperform both raw questions ($q_{raw}$) and transformed queries ($q_{trans}$). With NV-Embed-v2, $q_{sub}$ achieves better scores across all metrics (e.g., ROUGE-1: 0.395 vs 0.383 for $q_{raw}$). Surprisingly, query transformation ($q_{trans}$) performs worst among all strategies in both MCQ and open-ended settings, suggesting that current transformation methods may be introducing noise rather than helpful context.
These findings indicate that while option-based queries are optimal for MCQ tasks, subtitle-enriched queries provide the best general-purpose strategy, especially for open-ended VideoQA where answer options are unavailable. The consistent performance patterns across different retrievers suggest these findings are robust to retrieval architecture choice. We also therefore use subtitle-enriched queries $q_{sub}$ for the rest of our experiments.










% -------------







% \subsection{RQ3}



% \subsection{RQ4}
% The results from caption-based retrieval show a pattern similar to subtitle retrieval, but with notably lower performance across all metrics. In the Fine-tuned setting, all retrievers achieve comparable MCQ accuracy (66.41-66.71\%), with BM25 performing marginally better than neural approaches. This minimal performance gap suggests that for captions, sophisticated neural retrievers offer little advantage over simple lexical matching.
% The substantial drop in performance compared to subtitle retrieval (about 8 percentage points in MCQ accuracy) indicates that captions alone may not capture the rich temporal and contextual information present in subtitles. This is particularly evident in open-ended metrics, where the highest F1 score (0.2795) is significantly lower than subtitle-based retrieval.
% Interestingly, while finetuning still provides substantial improvements (approximately 24-25 percentage points in MCQ accuracy), the convergence of all retrievers to similar performance levels suggests that the limiting factor may be the information content of captions rather than the retrieval method. This implies that for knowledge-intensive VideoQA, captions might be better used as a complementary source rather than a primary knowledge base.

% \subsection{RQ5}

% Video-based retrieval shows notably weaker performance compared to text-based approaches (subtitle and caption retrieval). One of the main reason might be that we could only use one retrieved video because of GPU memory availability. In the Fine-tuned setting, even the best performing retriever (BM25) achieves only 65.81\% MCQ accuracy, which is significantly lower than subtitle-based retrieval (74.84\%) and slightly below caption-based retrieval (66.71\%). This performance gap is consistent across all open-ended metrics.

% \subsection{RQ6}

% After examining individual modalities for retrieval augmentation, we investigate whether combining different knowledge sources can further enhance VideoQA performance. We experiment with various combinations of our three knowledge corpora: subtitles ($\mathcal{K}_s$), captions ($\mathcal{K}_c$), and video segments ($\mathcal{K}v$). For all experiments, we use NV-Embed-v2 as our retriever given its strong performance in single-modality settings, and employ subtitle-enriched queries ($q{sub}$).

% Our results reveal several interesting patterns in multimodal retrieval. The combination of subtitles and captions ($\mathcal{K}_s, \mathcal{K}_c$) achieves the best performance among all combinations, reaching 75.6\% MCQ accuracy and the highest scores across all open-ended metrics. This suggests that textual information from different sources (subtitles and captions) complement each other effectively, with captions potentially providing additional semantic context to the already available information in subtitles.

% Surprisingly, incorporating video retrieval ($\mathcal{K}_v$) consistently leads to performance degradation. The subtitle-video combination ($\mathcal{K}_s, \mathcal{K}_v$) performs worse than subtitle-caption, and the caption-video combination ($\mathcal{K}_c, \mathcal{K}_v$) shows the poorest performance (65.26\% MCQ accuracy). Even using all three modalities ($\mathcal{K}_s, \mathcal{K}_c, \mathcal{K}_v$) performs slightly worse than subtitle-caption alone, achieving 73.78\% MCQ accuracy.

% These findings indicate that while combining different types of textual information (subtitles and captions) is beneficial, current video retrieval strategy might have introduced noise rather than helpful context. This suggests a need for better video retrieval techniques that can more effectively complement text-based knowledge sources.

% \subsection{RQ7}

% Looking across all experiments, there is a consistent and significant gap between zero-shot and Fine-tuned performance. For single modality approaches, the performance differences are substantial:

% With subtitle retrieval, NV-Embed-v2 improves from 51.67\% to 74.84\% MCQ accuracy after finetuning, a 23.17 percentage point gain
% With caption retrieval, all retrievers show similar zero-shot performance around 40-42\%, improving to ~66\% after finetuning
% With video retrieval, models show slightly better zero-shot performance (50-53\%) but still benefit significantly from finetuning, reaching 58-65\% accuracy

% Interestingly, multimodal combinations show varying zero-shot capabilities. The subtitle-video combination achieves the highest zero-shot MCQ accuracy (56.92\%), suggesting that complementary information from different modalities may help with generalization. However, the caption-video combination performs poorly even in zero-shot (40.61\%), indicating that not all multimodal combinations are equally beneficial for generalization.
% These patterns persist across open-ended metrics, with substantial improvements from finetuning across all settings. This consistent performance gap suggests that while retrieval augmentation helps provide relevant knowledge, models require task-specific training to effectively utilize this information for answer generation.