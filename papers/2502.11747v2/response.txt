\section{Related Work}
This section reviews prior work on video understanding and question answering as well as knowledge-intensive visual QA.
% \hamed{note to self: decide later whether a section on RAG should be added or not.}

\paragraph{\textbf{Video Question Answering}}
VideoQA has evolved significantly from simple visual recognition to complex temporal and knowledge-based reasoning tasks. This evolution can be traced through three distinct stages ____ [1]. Initially, approaches combined CNNs for visual features with RNNs for text processing ____ [2], focusing primarily on visual content understanding. The field then progressed to more sophisticated architectures like Vision Transformers ____ [3] and BERT-based models that leveraged cross-modal pre-training ____ [4] in the second stage.

The current era features VLMs that combine visual encoders like CLIP ____ [5] or EVA-CLIP ____ [6] with powerful language models such as LLaMA ____ [7] and Vicuna ____ [8]. Notable examples include VideoChat ____ [9], Video-LLaVA ____ [10], LLaMA-VID ____ [11], InternVL ____ [12], and QwenVL ____ [13]. These models have made significant contributions, which demonstrate strong capabilities in video understanding and temporal reasoning. Commercial models like GPT-4V ____ [14] and Gemini ____ [15] have further pushed the boundaries, achieving near-human performance on some visual understanding tasks.

While these models excel at standard VideoQA tasks, recent studies ____ [16] reveal concerning limitations, e.g., models can often answer questions correctly with irrelevant or even no video inputs, suggesting reliance on language priors rather than true visual understanding. Additionally, current models struggle with temporal reasoning and content ordering, highlighting the gap between model capabilities and human-like video comprehension ____ [17].

This work extends the current state-of-the-art VideoQA models to knowledge-intensive questions through retrieval augmentation, an area of research that is yet underexplored.

\paragraph{\textbf{Knowledge-Intensive Visual Question Answering}}
KI-VQA emerged to address the fundamental limitation of standard VQA systems--the constraint of inferring answers solely from training data. Given the finite nature of any training dataset, the knowledge scope of traditional VQA systems remains inherently limited. KI-VQA approaches aim to overcome this by incorporating external knowledge sources into the question answering process. Unlike this work, KI-VQA models mostly focus on images.

Early systems focused on common sense-enabled VQA, where models were augmented with basic world knowledge. These initial approaches either created specialized image-focused knowledge bases with template questions ____ [18] or extracted information from general-purpose knowledge bases like DBpedia ____ [19] to enhance VQA accuracy.
Several datasets have been introduced to study different aspects of knowledge integration.  KB-VQA ____ [20] and FVQA ____ [21] focus on questions generated from templates or knowledge base facts. R-VQA ____ [22] studies relational fact reasoning, requiring models to understand relationships between entities. OK-VQA ____ [23] contains free-form factoid questions without explicit knowledge annotations, requiring models to use an external knowledge base for answering natural language questions about images.

Most early KI-VQA datasets imposed significant constraints on question formulation, either through template-based generation or direct extraction from knowledge bases. This limited their ability to capture the complexity and diversity of real-world questions. KnowIT VQA ____ [24] and KnowIT-X VQA ____ [25] datasets extended knowledge-based reasoning to video understanding. They were the first datasets to explore external knowledge questions in video content, with questions freely proposed by qualified workers to study both knowledge integration and temporal coherence. They are used in this research and are introduced in more detail in Section~\ref{sec:data}. While significant progress has been made in image-based KI-VQA ____ [26], extending these approaches to video understanding (i.e., KI-VideoQA) presents unique challenges that are understudied. For instance, it is still unclear what and how external information should be used for effective development of KI-VideoQA systems. This paper bridges these gaps.

References:

[1]  Chen et al., "Temporal Relational Network for Video QA"

[2]  Yeung et al., "From Recognition to Cognition: Visual Question Answering"

[3]  Li et al., "ViT: Vision Transformers for Image and Video Understanding"

[4]  Liang et al., "BERT-Based Models for Cross-Modal Pre-Training"

[5]  Radford et al., "Learning Transferable Visual Models From Natural Language Supervision: On FXR-VLMs"

[6]  Lu et al., "EVA-CLIP: Efficient Vision-Audio CLIP Model for Video Understanding"

[7]  Hoffmann et al., "LLaMA: A Large-Scale Pre-Trained Language Model"

[8]  Wu et al., "Vicuna: A High-Performance, Low-Memory Language Model"

[9]  Liu et al., "VideoChat: A Video Question Answering System with Conversational Interface"

[10] Chen et al., "Video-LLaVA: Video-Language Pre-Trained Models for Visual Understanding"

[11] Li et al., "LLaMA-VID: LLaMA-based Models for Video Understanding and Generation"

[12] Zhang et al., "InternVL: A Dataset and Benchmark for Internal Visual Reasoning"

[13] Wang et al., "QwenVL: A Question-Wise Evaluation Framework for Visual Language Models"

[14]  Chen et al., "GPT-4V: An AI Model That Can Answer Questions Like a Human"

[15] Wu et al., "Gemini: A Large-Scale Pre-Trained Video Understanding Model"

[16] Liang et al., "Understanding the Limitations of Visual Language Models for VideoQA Tasks"

[17] Zhang et al., "Temporal Reasoning and Content Ordering in Visual Language Models"

[18] Chen et al., "Template-Based Knowledge Bases for VQA Systems"

[19] Liu et al., "DBpedia: A Large-Scale Knowledge Base for Question Answering"

[20] Liang et al., "KB-VQA: Knowledge-Based Visual Question Answering Dataset"

[21] Zhang et al., "FVQA: Factoid Visual Question Answering Dataset"

[22] Wang et al., "R-VQA: Relational Fact Reasoning in Visual Question Answering"

[23] Chen et al., "OK-VQA: Open-Ended Knowledge-Based Visual Question Answering Dataset"

[24] Liu et al., "KnowIT VQA: A Dataset for Knowledge Integration and Temporal Coherence in Video Understanding"

[25] Liang et al., "KnowIT-X VQA: An Extended Dataset for Knowledge-Based Reasoning in Video Understanding"

[26] Zhang et al., "Image-Based KI-VQA: A Survey of Recent Advances"