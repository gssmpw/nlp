\section{Introduction}
With the rise of effective vision language models (VLMs), multi-modal question answering tasks have attracted considerable attention. Visual question answering (VQA) \cite{antol2015vqa, goyal2017making} is one of these tasks, where a question is asked about an image. In VQA, the questions can often be answered solely by processing the given image (e.g., `\texttt{what is the color of the table in the image?}'). In knowledge-intensive visual question answering (KI-VQA), on the other hand, researchers study more real-world questions where answering the question requires access to some external knowledge sources, for example, KI-VQA concerns with questions like `\texttt{when was this building [referring to the image] constructed?}'. The knowledge sources studied in KI-VQA are often in the form of structured (or semi-structured) data, such as knowledge bases \cite{marino2019ok,mavex,krisp,kat,unifer,wang2015explicit}. More recently, KI-VQA has been extended to unstructured knowledge sources (e.g., a large collection of text passages) by focusing on passage retrieval \cite{Qu2021KIVQA,Salemi2023ICTIR,Salemi2023MMFID}.

\citet{garcia2020knowit}  expanded KI-VQA to short videos as its visual component.  The task of knowledge-intensive video question answering (KI-VideoQA) concerns with answering questions in the context of a video, where extra information is necessary to accurately answer the question. An example of KI-VideoQA instances is provided in Figure~\ref{fig:method_overview} (top left corner). This task, which is the focus of this work, stands at the intersection of computer vision, information retrieval, and natural language processing. In more detail, KI-VideoQA extends KI-VQA to dynamic scenarios, such as the movement state of objects (e.g., `\texttt{slow}' or `\texttt{fast}'?, `\texttt{put down}' or `\texttt{take away}'?), action repetitions and their transitions over time \citep{jang2017tgif,jang2019video}. To the best of our knowledge, existing work on KI-VideoQA suffers from the following two shortcomings: (1) Prior work assumes that large (vision) language models contain the necessary external information required for answering KI-VideoQA tasks. (2) Prior work on KI-VideoQA  solely focuses on multiple choice questions, which limits the real-world applications of these systems.

We address these two shortcomings as follows: we introduce the first multi-modal retrieval-augmented generation pipeline for KI-VideoQA. This enables us to retrieve unstructured information (textual or visual) from one or more data collection(s) to provide the VLM with the necessary external knowledge. In addition, we extend the KI-VideoQA task to open-ended questions, where no options are provided and the model is expected to generate a free-form textual answer. For this purpose, we adopt and modify established KI-VideoQA benchmarks with multiple choice questions. % that are constructed based on two popular sitcom TV series--The Bing Bang Theory\footnote{\url{https://www.imdb.com/title/tt0898266/}} and Friends.\footnote{\url{https://www.imdb.com/title/tt0108778/}}

Based on these two main contributions, we study various implementations of the developed pipeline using current state-of-the-art retrieval and large VLMs in order to empirically study seven important research questions from an information retrieval perspective.
\begin{itemize}[leftmargin=*]
    \item \textbf{RQ1}: How well do state-of-the-art vision language models perform in KI-VQA tasks? 
\end{itemize}
To answer this question, we evaluate several state-of-the-art VLMs, including commercial models like GPT-4V \cite{achiam2023gpt} and open-source models like Qwen2VL \cite{wang2024qwen2vl} and Intern2VL \cite{wang2024intern2vl} on the KnowIT dataset \cite{garcia2020knowit}. We study these models under different circumstances, including both multiple choice and open-ended questions. We also explore the impact of fine-tuning on open-source VLMs. This research question not only establishes baseline results for our research, but also enables us to better understand the strengths and limitations of VLMs in handling questions that require external knowledge beyond the video content. 

\begin{figure*}[ht]
  % \begin{center}
  \centering
  \includegraphics[width=.9\linewidth]{images/method-cropped.pdf}
  \caption{An overview of our multi-modal retrieval augmentation pipeline for KI-VideoQA tasks.}
  \label{fig:method_overview}
  % \end{center}
\end{figure*}

Next, we focus on retrieval-augmented VLMs for KI-VQA. We first explore the impact of different knowledge sources that can be used for retrieval augmentation. In more detail, we address these questions:
\begin{itemize}[leftmargin=*]
    \item \textbf{RQ2}: How does augmenting VLMs with retrieved textual knowledge impact the end-to-end KI-VQA performance? 
    \item \textbf{RQ3}: How does augmenting VLMs with retrieved multi-media knowledge (videos) impact the end-to-end KI-VQA performance?
    \item \textbf{RQ4}: How does retrieval from heterogeneous information sources with multiple modalities (text and video) impact the end-to-end KI-VQA performance? 
\end{itemize}



To answer these research questions, we construct two text corpora for knowledge retrieval. The first corpus consists of subtitles of all videos in the dataset, allowing us to evaluate the impact of past dialogues and narratives in the dataset. We construct the second corpus by automatically generating video captions for each video in the dataset. Video captions often include deep semantic description of video content, serving as a proxy textual representation of the video.  We also explore the collection of all videos in the dataset as a multi-media corpus.  In our experiments, VLMs consume videos frame-by-frame and frames are uniformly sampled at the rate of 1 frame per second. Since these knowledge sources provide complementary information, we also conduct experiments on various combinations of these heterogeneous and multi-modal corpora. For each corpus, we explore three diverse and effective retrieval models, including sparse and dense retrieval models.
We further study:
\begin{itemize}[leftmargin=*]
    \item \textbf{RQ5}: How does our empirical findings transfer to another dataset?
\end{itemize}
By extending our analysis to the KnowIT-X dataset \cite{wu2021transferring}, we assess the generalizability and transferability of the behavior of the developed multi-modal retrieval augmented pipeline for KI-VideoQA. Our analysis also includes transfer learning experiments. 
We finally expand our analysis by exploring:
\begin{itemize}[leftmargin=*]
    \item \textbf{RQ6}: How sensitive are the retrieval-augmented VLMs to the number of retrieved information?
    \item \textbf{RQ7}: How do different query formulations for knowledge retrieval impact the end-to-end performance in KI-VideoQA?
\end{itemize}
In more detail, we explore four different query formulations with and without enriching the input question with the information extracted from the given video. 

In summary, for this underexplored and complex task that demands expertise across various disciplines, we find it crucial to conduct thorough empirical analyses. These analyses should address multiple facets of the problem, including the selection of knowledge sources, the retrieval model's capability to effectively retrieve information from these sources, the VLM's proficiency in utilizing the retrieved (multi-modal) context, and the formulation of effective queries for the task. Such a comprehensive approach lays the groundwork for developing robust multi-modal RAG systems for KI-VideoQA tasks. Besides these important lessons, our contributions advance state-of-the-art accuracy by over 17.5\% (from 65.2 to 76.7\%) for multiple choice questions on the KnowIT dataset.\footnote{Note that prior work only studied multiple choice questions.} We will soon release our implementation publicly for improving the reproducibility of the results.
% \footnote{The open-source repository is available at \url{https://anonymized\_url/}}












% ------------------
% \vfill
% \pagebreak



% Video Question Answering (VideoQA) is the task of answering visual content-related questions for videos. It stands at the intersection of computer vision, natural language processing, and knowledge reasoning. VideoQA extends image VQA~\citep{antol2015vqa, goyal2017making} to dynamic scenarios, such as the movement state of objects (e.g., ``\texttt{slow}'' or ``\texttt{fast}''?, ``\texttt{put down}'' or ``\texttt{take away}''?), action repetitions and their transitions over time \citep{jang2017tgif,jang2019video}. While traditional VideoQA focuses on answering questions about visual content directly observable in videos \cite{jang2017tgif,xu2017video}, knowledge-intensive VideoQA requires models to reason beyond the immediate visual and temporal information by incorporating external knowledge and context \cite{garcia2020knowit}.

% The emergence of Large Language Models (LLMs) has revolutionized the field of VideoQA, with state-of-the-art models achieving impressive performance on standard benchmarks \cite{xiao2024videoqa}. These models have evolved from traditional vision-language models (VLMs) like VIOLET \cite{fu2021violet} and TVQA \cite{lei2018tvqa} that relied on fine-tuning small language models (e.g., BERT, RoBERTa) to modern VLMs that leverage powerful frozen language models like LLaMA \cite{touvron2023llama}, Vicuna \cite{chiang2023vicuna}, or GPT-4 \cite{achiam2023gpt}. The progression has seen three distinct stages: first, CNN+RNN architectures that used convolutional networks for visual features and recurrent networks for language processing; second, Vision Transformer (ViT) and BERT-based models that benefited from self-supervised cross-modal pre-training; and finally, the current era of VLMs that combine visual encoders like CLIP \cite{radford2021learning} or EVA-CLIP \cite{sun2023eva} with large-scale language models \cite{xiao2024videoqa}. Vision Language Models have played a crucial role in this evolution, with models like BLIP-2 \cite{li2023blip}, LLaVA \cite{liu2023visualinstructiontuning}, and Video-LLaVA \cite{lin2023video} demonstrating strong capabilities in visual-language understanding through instruction tuning and visual adaptation of LLMs. Commercial models like GPT-4V and Gemini \cite{team2023gemini} have further pushed boundaries, achieving near-human performance on various visual understanding tasks. However, their ability to handle questions requiring extensive knowledge beyond the video content remains understudied, particularly in scenarios requiring temporal reasoning and knowledge integration across long video sequences.

% Knowledge-intensive VideoQA presents unique challenges compared to traditional VideoQA. Consider a question about a TV show character's motivation for certain actions - answering this requires not just understanding the immediate scene, but also knowledge about the character's history, relationships, and past events \cite{garcia2020knowit}. Similarly, questions about cultural references, technical terminology, or historical context in documentaries demand integration of external knowledge with video understanding.

% Retrieval augmentation has emerged as a promising approach to enhance LLMs' knowledge capabilities in text domains \cite{lewis2020retrieval}. However, its application to multimodal settings like VideoQA introduces new research questions: How should retrieval be performed over different modalities (subtitles, captions, video segments)? What are effective strategies for query formulation? How many retrieved items are optimal? How does retrieval augmentation impact zero-shot generalization?

% While VideoQA performance has seen remarkable improvements with LLMs, focusing solely on accuracy metrics for multiple-choice tasks may provide an incomplete or potentially misleading picture of these models' true capabilities. Open-ended VideoQA presents a more challenging and realistic test of a model's video understanding and knowledge integration abilities, as models cannot rely on multiple-choice answers as cues or exploit statistical patterns in answer options. Recent studies \cite{wang2023dataset} have shown that VLMs can often arrive at correct multiple-choice answers without truly understanding the content or by exploiting spurious correlations. They are also vulnerable to adversarial permutation in answer sets for
% multiple-choice prompting \cite{zong2023fool}. This raises critical questions about whether these models can generate faithful, coherent, and accurate responses when asked to answer questions in a free-form manner. Open-ended VideoQA also better reflects real-world applications where users expect natural, detailed responses rather than selections from pre-defined options. In this work, we conduct a comprehensive study of VLM's capabilities in open-ended knowledge-intensive VideoQA, examining their ability to generate accurate, relevant, and well-grounded responses while integrating knowledge beyond what is directly observable in the video. Understanding performance in this more challenging setting is crucial for developing more trustworthy and human-like video understanding systems.

% Here, we present a comprehensive empirical study of knowledge-intensive VideoQA through the lens of modern VLMs and retrieval augmentation. We investigate eight key research questions spanning baseline VLM performance (RQ1), query formulation strategies (RQ2), retrieval augmentation using different modalities (RQ3-5), multimodal context integration (RQ6), zero-shot capabilities (RQ7), and retrieval efficiency (RQ8).

% First, we are interested in establishing baseline performance of VLMs on knowledge-intensive VideoQA:

% \begin{itemize}
%     \item \textbf{RQ1}: How do VLMs perform on knowledge-intensive VQAs? 
%     % \hamed{Answer: Row 4 (baseline) + at least a couple more video LLMs}
% \end{itemize}

% To answer this question, we evaluate several state-of-the-art VLMs including commercial models like GPT-4V \cite{} and open-source models like Qwen2VL \cite{} and Intern2VL \cite{}. Our goal is to understand both their strengths and limitations in handling questions that require external knowledge beyond the video content.

% % We then investigate the effectiveness of different query formulations for knowledge retrieval in KI-VideoQA:

% \begin{itemize}
% \item \textbf{RQ2}: How do different query formulations for knowledge retrieval impact the end-to-end performance in KI-VideoQA?
% \end{itemize}

% \hamed{something here}

% % We also investigate different approaches to retrieval augmentation using various modalities:

% \begin{itemize}
%     \item \textbf{RQ3}: How does retrieval augmentation using video subtitles perform on knowledge-intensive VQAs? 
%     % \hamed{topk=10, query=question, different retrieval models (BM25, dense retrieval, ...)}

%     \item \textbf{RQ4}: How does retrieval augmentation using video captioning perform on knowledge-intensive VQAs? \hamed{repeat exact same setup as RQ2, but with video captions as the corpus.}

%     \item \textbf{RQ5}: How does mutli-modal augmentation with a retrieved video perform on knowledge-intensive VQAs? \hamed{repeat exact same setup as RQ2, but with video as the context.}
% \end{itemize}

% For these questions, we systematically evaluate different retrieval models (BM25, dense retrieval) and analyze how effectively they can enhance VLM's knowledge capabilities through different modalities. We maintain consistent experimental setups across modalities to enable fair comparisons. Building on these findings, we explore the integration of multiple modalities:

% \begin{itemize}
%     \item \textbf{RQ6}: How does retrieval augmentation with heterogeneous multi-modal context perform on knowledge-intensive VQAs? 
%     % \hamed{select your best retriever. just report the results for subtitle+caption, subtitle+video, subtitle+caption+video, caption+video}
%     % \hamed{some analysis on the queries that can benefit from multiple sources.}
% \end{itemize}

% This question examines different combinations of subtitles, captions, and video content to determine optimal multi-modal retrieval strategies.

% Finally, we investigate practical aspects of implementing retrieval augmentation:

% \begin{itemize}
%     \item \textbf{RQ7}: How do VLMs and their retrieval augmented variants perform in zero-shot settings?

%     \item \textbf{RQ8}: How many retrieved documents needed for effective knowledge-intensive video QA?
    
% \end{itemize}

% These questions address critical implementation considerations including query engineering, generalization capabilities, and efficiency tradeoffs. Our results provide insights about the effectiveness of retrieval augmentation for knowledge-intensive VideoQA and suggest promising directions for improving VLMs' ability to integrate external knowledge with video understanding. We find that while retrieval augmentation generally improves performance, the choice of modality, retrieval strategy, and query formulation significantly impacts effectiveness. Additionally, our analysis reveals important tradeoffs between retrieval depth and computational efficiency that must be considered for practical applications.
