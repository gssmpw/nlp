@inproceedings{antol2015vqa,
  title={Vqa: Visual question answering},
  author={Antol, Stanislaw and Agrawal, Aishwarya and Lu, Jiasen and Mitchell, Margaret and Batra, Dhruv and Zitnick, C Lawrence and Parikh, Devi},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={2425--2433},
  year={2015}
}

@inproceedings{goyal2017making,
  title={Making the v in vqa matter: Elevating the role of image understanding in visual question answering},
  author={Goyal, Yash and Khot, Tejas and Summers-Stay, Douglas and Batra, Dhruv and Parikh, Devi},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={6904--6913},
  year={2017}
}

@inproceedings{jang2017tgif,
  title={Tgif-qa: Toward spatio-temporal reasoning in visual question answering},
  author={Jang, Yunseok and Song, Yale and Yu, Youngjae and Kim, Youngjin and Kim, Gunhee},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={2758--2766},
  year={2017}
}

@article{jang2019video,
  title={Video question answering with spatio-temporal reasoning},
  author={Jang, Yunseok and Song, Yale and Kim, Chris Dongjoo and Yu, Youngjae and Kim, Youngjin and Kim, Gunhee},
  journal={International Journal of Computer Vision},
  volume={127},
  pages={1385--1412},
  year={2019},
  publisher={Springer}
}

@inproceedings{xu2017video,
  title={Video question answering via gradually refined attention over appearance and motion},
  author={Xu, Dejing and Zhao, Zhou and Xiao, Jun and Wu, Fei and Zhang, Hanwang and He, Xiangnan and Zhuang, Yueting},
  booktitle={Proceedings of the 25th ACM international conference on Multimedia},
  pages={1645--1653},
  year={2017}
}

@inproceedings{garcia2020knowit,
  title={KnowIT VQA: Answering knowledge-based questions about videos},
  author={Garcia, Noa and Otani, Mayu and Chu, Chenhui and Nakashima, Yuta},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={34},
  number={07},
  pages={10826--10834},
  year={2020}
}

@article{xiao2024videoqa,
  title={VideoQA in the Era of LLMs: An Empirical Study},
  author={Xiao, Junbin and Huang, Nanxin and Qin, Hangyu and Li, Dongyang and Li, Yicong and Zhu, Fengbin and Tao, Zhulin and Yu, Jianxing and Lin, Liang and Chua, Tat-Seng and others},
  journal={arXiv preprint arXiv:2408.04223},
  year={2024}
}

@inproceedings{Kong2013,
author = {Kong, Weize and Allan, James},
title = {Extracting query facets from search results},
year = {2013},
isbn = {9781450320344},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2484028.2484097},
doi = {10.1145/2484028.2484097},
booktitle = {Proceedings of the 36th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {93–102},
numpages = {10},
keywords = {multi-faceted query, query facet, semantic class extraction},
location = {Dublin, Ireland},
series = {SIGIR '13}
}

@inproceedings{rag,
author = {Lewis, Patrick and Perez, Ethan and Piktus, Aleksandra and Petroni, Fabio and Karpukhin, Vladimir and Goyal, Naman and K\"{u}ttler, Heinrich and Lewis, Mike and Yih, Wen-tau and Rockt\"{a}schel, Tim and Riedel, Sebastian and Kiela, Douwe},
title = {Retrieval-augmented generation for knowledge-intensive NLP tasks},
year = {2020},
isbn = {9781713829546},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
booktitle = {Proceedings of the 34th International Conference on Neural Information Processing Systems},
articleno = {793},
numpages = {16},
location = {Vancouver, BC, Canada},
series = {NIPS '20}
}

@inproceedings{reml,
author = {Zamani, Hamed and Diaz, Fernando and Dehghani, Mostafa and Metzler, Donald and Bendersky, Michael},
title = {Retrieval-Enhanced Machine Learning},
year = {2022},
isbn = {9781450387323},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3477495.3531722},
doi = {10.1145/3477495.3531722},
booktitle = {Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {2875–2886},
numpages = {12},
keywords = {knowledge grounding, memory augmentation, retrieval augmentation},
location = {Madrid, Spain},
series = {SIGIR '22}
}

@article{fu2021violet,
  title={Violet: End-to-end video-language transformers with masked visual-token modeling},
  author={Fu, Tsu-Jui and Li, Linjie and Gan, Zhe and Lin, Kevin and Wang, William Yang and Wang, Lijuan and Liu, Zicheng},
  journal={arXiv preprint arXiv:2111.12681},
  year={2021}
}

@article{lei2018tvqa,
  title={Tvqa: Localized, compositional video question answering},
  author={Lei, Jie and Yu, Licheng and Bansal, Mohit and Berg, Tamara L},
  journal={arXiv preprint arXiv:1809.01696},
  year={2018}
}

@article{touvron2023llama,
  title={Llama: Open and efficient foundation language models},
  author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others},
  journal={arXiv preprint arXiv:2302.13971},
  year={2023}
}

@article{chiang2023vicuna,
  title={Vicuna: An open-source chatbot impressing gpt-4 with 90\%* chatgpt quality},
  author={Chiang, Wei-Lin and Li, Zhuohan and Lin, Zi and Sheng, Ying and Wu, Zhanghao and Zhang, Hao and Zheng, Lianmin and Zhuang, Siyuan and Zhuang, Yonghao and Gonzalez, Joseph E and others},
  journal={See https://vicuna. lmsys. org (accessed 14 April 2023)},
  volume={2},
  number={3},
  pages={6},
  year={2023}
}

@article{achiam2023gpt,
  title={Gpt-4 technical report},
  author={OpenAI},
  journal={arXiv preprint arXiv:2303.08774},
  year={2023}
}

@inproceedings{radford2021learning,
  title={Learning transferable visual models from natural language supervision},
  author={Radford, Alec and others},
  booktitle={International conference on machine learning},
  pages={8748--8763},
  year={2021},
  organization={PMLR}
}

@article{sun2023eva,
  title={Eva-clip: Improved training techniques for clip at scale},
  author={Sun, Quan and Fang, Yuxin and Wu, Ledell and Wang, Xinlong and Cao, Yue},
  journal={arXiv preprint arXiv:2303.15389},
  year={2023}
}

@inproceedings{li2023blip,
  title={Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models},
  author={Li, Junnan and Li, Dongxu and Savarese, Silvio and Hoi, Steven},
  booktitle={International conference on machine learning},
  pages={19730--19742},
  year={2023},
  organization={PMLR}
}

@misc{liu2023visualinstructiontuning,
      title={Visual Instruction Tuning}, 
      author={Haotian Liu and Chunyuan Li and Qingyang Wu and Yong Jae Lee},
      year={2023},
      eprint={2304.08485},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2304.08485}, 
}

@article{lin2023video,
  title={Video-llava: Learning united visual representation by alignment before projection},
  author={Lin, Bin and Ye, Yang and Zhu, Bin and Cui, Jiaxi and Ning, Munan and Jin, Peng and Yuan, Li},
  journal={arXiv preprint arXiv:2311.10122},
  year={2023}
}

@article{team2023gemini,
  title={Gemini: a family of highly capable multimodal models},
  author={Team Gemini},
  journal={arXiv preprint arXiv:2312.11805},
  year={2023}
}

@article{lewis2020retrieval,
  title={Retrieval-augmented generation for knowledge-intensive nlp tasks},
  author={Lewis, Patrick and Perez, Ethan and Piktus, Aleksandra and Petroni, Fabio and Karpukhin, Vladimir and Goyal, Naman and K{\"u}ttler, Heinrich and Lewis, Mike and Yih, Wen-tau and Rockt{\"a}schel, Tim and others},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={9459--9474},
  year={2020}
}

@article{wang2023dataset,
  title={Dataset bias mitigation in multiple-choice visual question answering and beyond},
  author={Wang, Zhecan and Chen, Long and You, Haoxuan and Xu, Keyang and He, Yicheng and Li, Wenhao and Codella, Noel and Chang, Kai-Wei and Chang, Shih-Fu},
  journal={arXiv preprint arXiv:2310.14670},
  year={2023}
}

@article{zong2023fool,
  title={Fool your (vision and) language model with embarrassingly simple permutations},
  author={Zong, Yongshuo and Yu, Tingyang and Chavhan, Ruchika and Zhao, Bingchen and Hospedales, Timothy},
  journal={arXiv preprint arXiv:2310.01651},
  year={2023}
}

@article{li2023videochat,
  title={Videochat: Chat-centric video understanding},
  author={Li, KunChang and He, Yinan and Wang, Yi and Li, Yizhuo and Wang, Wenhai and Luo, Ping and Wang, Yali and Wang, Limin and Qiao, Yu},
  journal={arXiv preprint arXiv:2305.06355},
  year={2023}
}

@inproceedings{li2025llama,
  title={Llama-vid: An image is worth 2 tokens in large language models},
  author={Li, Yanwei and Wang, Chengyao and Jia, Jiaya},
  booktitle={European Conference on Computer Vision},
  pages={323--340},
  year={2025},
  organization={Springer}
}

@inproceedings{chen2024internvl,
  title={Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks},
  author={Chen, Zhe and Wu, Jiannan and Wang, Wenhai and Su, Weijie and Chen, Guo and Xing, Sen and Zhong, Muyan and Zhang, Qinglong and Zhu, Xizhou and Lu, Lewei and others},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={24185--24198},
  year={2024}
}

@inproceedings{xiao2024can,
  title={Can i trust your answer? visually grounded video question answering},
  author={Xiao, Junbin and Yao, Angela and Li, Yicong and Chua, Tat-Seng},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={13204--13214},
  year={2024}
}

@inproceedings{bagad2023test,
  title={Test of time: Instilling video-language models with a sense of time},
  author={Bagad, Piyush and Tapaswi, Makarand and Snoek, Cees GM},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={2503--2516},
  year={2023}
}

@article{zhu2015building,
  title={Building a large-scale multimodal knowledge base system for answering visual queries},
  author={Zhu, Yuke and Zhang, Ce and R{\'e}, Christopher and Fei-Fei, Li},
  journal={arXiv preprint arXiv:1507.05670},
  year={2015}
}

@inproceedings{wu2016ask,
  title={Ask me anything: Free-form visual question answering based on knowledge from external sources},
  author={Wu, Qi and Wang, Peng and Shen, Chunhua and Dick, Anthony and Van Den Hengel, Anton},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={4622--4630},
  year={2016}
}

@inproceedings{auer2007dbpedia,
  title={Dbpedia: A nucleus for a web of open data},
  author={Auer, S{\"o}ren and Bizer, Christian and Kobilarov, Georgi and Lehmann, Jens and Cyganiak, Richard and Ives, Zachary},
  booktitle={international semantic web conference},
  pages={722--735},
  year={2007},
  organization={Springer}
}

@article{wang2015explicit,
  title={Explicit knowledge-based reasoning for visual question answering},
  author={Wang, Peng and Wu, Qi and Shen, Chunhua and Hengel, Anton van den and Dick, Anthony},
  journal={arXiv preprint arXiv:1511.02570},
  year={2015}
}

@article{wang2017fvqa,
  title={Fvqa: Fact-based visual question answering},
  author={Wang, Peng and Wu, Qi and Shen, Chunhua and Dick, Anthony and Van Den Hengel, Anton},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  volume={40},
  number={10},
  pages={2413--2427},
  year={2017},
  publisher={IEEE}
}

@inproceedings{lu2018r,
  title={R-VQA: learning visual relation facts with semantic attention for visual question answering},
  author={Lu, Pan and Ji, Lei and Zhang, Wei and Duan, Nan and Zhou, Ming and Wang, Jianyong},
  booktitle={Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining},
  pages={1880--1889},
  year={2018}
}

@inproceedings{marino2019ok,
  title={Ok-vqa: A visual question answering benchmark requiring external knowledge},
  author={Marino, Kenneth and Rastegari, Mohammad and Farhadi, Ali and Mottaghi, Roozbeh},
  booktitle={Proceedings of the IEEE/cvf conference on computer vision and pattern recognition},
  pages={3195--3204},
  year={2019}
}

@article{wu2021transferring,
  title={Transferring domain-agnostic knowledge in video question answering},
  author={Wu, Tianran and Garcia, Noa and Otani, Mayu and Chu, Chenhui and Nakashima, Yuta and Takemura, Haruo},
  journal={arXiv preprint arXiv:2110.13395},
  year={2021}
}

@inproceedings{VisionTransformer,
title={An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
author={Alexey Dosovitskiy and Lucas Beyer and Alexander Kolesnikov and Dirk Weissenborn and Xiaohua Zhai and Thomas Unterthiner and Mostafa Dehghani and Matthias Minderer and Georg Heigold and Sylvain Gelly and Jakob Uszkoreit and Neil Houlsby},
booktitle={International Conference on Learning Representations},
year={2021},
}

@misc{bai2023qwenvl,
      title={Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond}, 
      author={Jinze Bai and Shuai Bai and Shusheng Yang and Shijie Wang and Sinan Tan and Peng Wang and Junyang Lin and Chang Zhou and Jingren Zhou},
      year={2023},
      eprint={2308.12966},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
}

@inproceedings{Qu2021KIVQA,
author = {Qu, Chen and Zamani, Hamed and Yang, Liu and Croft, W. Bruce and Learned-Miller, Erik},
title = {Passage Retrieval for Outside-Knowledge Visual Question Answering},
year = {2021},
isbn = {9781450380379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3404835.3462987},
doi = {10.1145/3404835.3462987},
booktitle = {Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {1753–1757},
numpages = {5},
keywords = {dense retrieval, multi-modal, visual question answering},
location = {Virtual Event, Canada},
series = {SIGIR '21}
}

@inproceedings{Salemi2023ICTIR,
author = {Salemi, Alireza and Rafiee, Mahta and Zamani, Hamed},
title = {Pre-Training Multi-Modal Dense Retrievers for Outside-Knowledge Visual Question Answering},
year = {2023},
isbn = {9798400700736},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3578337.3605137},
doi = {10.1145/3578337.3605137},
booktitle = {Proceedings of the 2023 ACM SIGIR International Conference on Theory of Information Retrieval},
pages = {169–176},
numpages = {8},
keywords = {data generation, dense retrieval, multi-modal retrieval, pre-training, visual question answering},
location = {Taipei, Taiwan},
series = {ICTIR '23}
}

@inproceedings{Salemi2023MMFID,
author = {Salemi, Alireza and Altmayer Pizzorno, Juan and Zamani, Hamed},
title = {A Symmetric Dual Encoding Dense Retrieval Framework for Knowledge-Intensive Visual Question Answering},
year = {2023},
isbn = {9781450394086},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3539618.3591629},
doi = {10.1145/3539618.3591629},
booktitle = {Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {110–120},
numpages = {11},
keywords = {dense retrieval, knowledge distillation, multi-modal retrieval, visual question answering},
location = {Taipei, Taiwan},
series = {SIGIR '23}
}


@inproceedings{kat,
    title = "{KAT}: A Knowledge Augmented Transformer for Vision-and-Language",
    author = "Gui, Liangke  and
      Wang, Borui  and
      Huang, Qiuyuan  and
      Hauptmann, Alexander  and
      Bisk, Yonatan  and
      Gao, Jianfeng",
    booktitle = "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jul,
    year = "2022",
    address = "Seattle, United States",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.naacl-main.70",
    doi = "10.18653/v1/2022.naacl-main.70",
    pages = "956--968",
}


@inproceedings{unifer,
author = {Guo, Yangyang and Nie, Liqiang and Wong, Yongkang and Liu, Yibing and Cheng, Zhiyong and Kankanhalli, Mohan},
title = {A Unified End-to-End Retriever-Reader Framework for Knowledge-Based VQA},
year = {2022},
isbn = {9781450392037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3503161.3547870},
doi = {10.1145/3503161.3547870},
booktitle = {Proceedings of the 30th ACM International Conference on Multimedia},
pages = {2061–2069},
numpages = {9},
keywords = {visual question answering, knowledge integration, modal fusion},
location = {Lisboa, Portugal},
series = {MM '22}
}


@inproceedings{krisp,
  title={KRISP: Integrating Implicit and Symbolic Knowledge for Open-Domain Knowledge-Based VQA},
  author={Kenneth Marino and Xinlei Chen and Devi Parikh and Abhinav Kumar Gupta and Marcus Rohrbach},
  booktitle={2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2020},
  pages={14106-14116}
}

@inproceedings{mavex, 
title={Multi-Modal Answer Validation for Knowledge-Based VQA}, 
DOI={10.1609/aaai.v36i3.20174}, 
booktitle={Proceedings of the AAAI Conference on Artificial Intelligence}, 
author={Wu, Jialin and Lu, Jiasen and Sabharwal, Ashish and Mottaghi, Roozbeh}, 
year={2022}, 
pages={2712-2721} }

@misc{wang2024qwen2vl,
      title={Qwen2-VL: Enhancing Vision-Language Model's Perception of the World at Any Resolution}, 
      author={Peng Wang and Shuai Bai and Sinan Tan and Shijie Wang and Zhihao Fan and Jinze Bai and Keqin Chen and Xuejing Liu and Jialin Wang and Wenbin Ge and Yang Fan and Kai Dang and Mengfei Du and Xuancheng Ren and Rui Men and Dayiheng Liu and Chang Zhou and Jingren Zhou and Junyang Lin},
      year={2024},
      eprint={2409.12191},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2409.12191}, 
}

@misc{wang2024intern2vl,
      title={Enhancing the Reasoning Ability of Multimodal Large Language Models via Mixed Preference Optimization}, 
      author={Weiyun Wang and Zhe Chen and Wenhai Wang and Yue Cao and Yangzhou Liu and Zhangwei Gao and Jinguo Zhu and Xizhou Zhu and Lewei Lu and Yu Qiao and Jifeng Dai},
      year={2024},
      eprint={2411.10442},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2411.10442}, 
}


@misc{dao2023flashattention2fasterattentionbetter,
      title={FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning}, 
      author={Tri Dao},
      year={2023},
      eprint={2307.08691},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2307.08691}, 
}


@article{lee2024nv,
  title={NV-Embed: Improved Techniques for Training LLMs as Generalist Embedding Models},
  author={Lee, Chankyu and Roy, Rajarshi and Xu, Mengyao and Raiman, Jonathan and Shoeybi, Mohammad and Catanzaro, Bryan and Ping, Wei},
  journal={arXiv preprint arXiv:2405.17428},
  year={2024}
}

@article{li2024making,
  title={Making text embedders few-shot learners},
  author={Li, Chaofan and Qin, MingHao and Xiao, Shitao and Chen, Jianlyu and Luo, Kun and Shao, Yingxia and Lian, Defu and Liu, Zheng},
  journal={arXiv preprint arXiv:2409.15700},
  year={2024}
}

@article{wang2022text,
  title={Text embeddings by weakly-supervised contrastive pre-training},
  author={Wang, Liang and Yang, Nan and Huang, Xiaolong and Jiao, Binxing and Yang, Linjun and Jiang, Daxin and Majumder, Rangan and Wei, Furu},
  journal={arXiv preprint arXiv:2212.03533},
  year={2022}
}

@article{wang2023improving,
  title={Improving text embeddings with large language models},
  author={Wang, Liang and Yang, Nan and Huang, Xiaolong and Yang, Linjun and Majumder, Rangan and Wei, Furu},
  journal={arXiv preprint arXiv:2401.00368},
  year={2023}
}

@article{wang2024multilingual,
  title={Multilingual e5 text embeddings: A technical report},
  author={Wang, Liang and Yang, Nan and Huang, Xiaolong and Yang, Linjun and Majumder, Rangan and Wei, Furu},
  journal={arXiv preprint arXiv:2402.05672},
  year={2024}
}

@article{zhang2024jasper,
  title={Jasper and Stella: distillation of SOTA embedding models},
  author={Zhang, Dun and others},
  journal={arXiv preprint arXiv:2412.19048},
  year={2024}
}

@inproceedings{robertson1994some,
  title={Some simple effective approximations to the 2-poisson model for probabilistic weighted retrieval},
  author={Robertson, Stephen E and Walker, Steve},
  booktitle={SIGIR’94: Proceedings of the Seventeenth Annual International ACM-SIGIR Conference on Research and Development in Information Retrieval, organised by Dublin City University},
  pages={232--241},
  year={1994},
  organization={Springer}
}

@inproceedings{muennighoff-etal-2023-mteb,
    title = "{MTEB}: Massive Text Embedding Benchmark",
    author = "Muennighoff, Niklas  and
      Tazi, Nouamane  and
      Magne, Loic  and
      Reimers, Nils",
    editor = "Vlachos, Andreas  and
      Augenstein, Isabelle",
    booktitle = "Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics",
    month = may,
    year = "2023",
    address = "Dubrovnik, Croatia",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.eacl-main.148/",
    doi = "10.18653/v1/2023.eacl-main.148",
    pages = "2014--2037",
    abstract = "Text embeddings are commonly evaluated on a small set of datasets from a single task not covering their possible applications to other tasks. It is unclear whether state-of-the-art embeddings on semantic textual similarity (STS) can be equally well applied to other tasks like clustering or reranking. This makes progress in the field difficult to track, as various models are constantly being proposed without proper evaluation. To solve this problem, we introduce the Massive Text Embedding Benchmark (MTEB). MTEB spans 8 embedding tasks covering a total of 58 datasets and 112 languages. Through the benchmarking of 33 models on MTEB, we establish the most comprehensive benchmark of text embeddings todate. We find that no particular text embedding method dominates across all tasks. This suggests that the field has yet to converge on a universal text embedding method and scale it up sufficiently to provide state-of-theart results on all embedding tasks. MTEB comes with open-source code and a public leaderboard at \url{https://github.com/embeddings-benchmark/mteb}."
}

@article{loshchilov2017decoupled,
  title={Decoupled weight decay regularization},
  author={Loshchilov, I},
  journal={arXiv preprint arXiv:1711.05101},
  year={2017}
}

@inproceedings{lin-2004-rouge,
    title = "{ROUGE}: A Package for Automatic Evaluation of Summaries",
    author = "Lin, Chin-Yew",
    booktitle = "Text Summarization Branches Out",
    month = jul,
    year = "2004",
    address = "Barcelona, Spain",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W04-1013/",
    pages = "74--81"
}

@inproceedings{papineni2002bleu,
  title={Bleu: a method for automatic evaluation of machine translation},
  author={Papineni, Kishore and Roukos, Salim and Ward, Todd and Zhu, Wei-Jing},
  booktitle={Proceedings of the 40th annual meeting of the Association for Computational Linguistics},
  pages={311--318},
  year={2002}
}

@article{rajpurkar2016squad,
  title={Squad: 100,000+ questions for machine comprehension of text},
  author={Rajpurkar, P},
  journal={arXiv preprint arXiv:1606.05250},
  year={2016}
}

@inproceedings{banerjee2005meteor,
  title={METEOR: An automatic metric for MT evaluation with improved correlation with human judgments},
  author={Banerjee, Satanjeev and Lavie, Alon},
  booktitle={Proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization},
  pages={65--72},
  year={2005}
}

@article{zhang2019bertscore,
  title={Bertscore: Evaluating text generation with bert},
  author={Zhang, Tianyi and Kishore, Varsha and Wu, Felix and Weinberger, Kilian Q and Artzi, Yoav},
  journal={arXiv preprint arXiv:1904.09675},
  year={2019}
}