%File: anonymous-submission-latex-2025.tex
\documentclass[letterpaper]{article} % DO NOT CHANGE THIS
\usepackage[submission]{aaai25}  % DO NOT CHANGE THIS
\usepackage{times}  % DO NOT CHANGE THIS
\usepackage{helvet}  % DO NOT CHANGE THIS
\usepackage{courier}  % DO NOT CHANGE THIS
\usepackage[hyphens]{url}  % DO NOT CHANGE THIS
\usepackage{graphicx} % DO NOT CHANGE THIS
\urlstyle{rm} % DO NOT CHANGE THIS
\def\UrlFont{\rm}  % DO NOT CHANGE THIS
\usepackage{natbib}  % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT
\usepackage{caption} % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT
\usepackage{multirow}
\frenchspacing  % DO NOT CHANGE THIS
\setlength{\pdfpagewidth}{8.5in} % DO NOT CHANGE THIS
\setlength{\pdfpageheight}{11in} % DO NOT CHANGE THIS
%
% These are recommended to typeset algorithms but not required. See the subsubsection on algorithms. Remove them if you don't have algorithms in your paper.
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{amsmath}


\usepackage{newfloat}
\usepackage{listings}
\usepackage{cases}
\DeclareCaptionStyle{ruled}{labelfont=normalfont,labelsep=colon,strut=off} % DO NOT CHANGE THIS
\lstset{%
	basicstyle={\footnotesize\ttfamily},% footnotesize acceptable for monospace
	numbers=left,numberstyle=\footnotesize,xleftmargin=2em,% show line numbers, remove this entire line if you don't want the numbers.
	aboveskip=0pt,belowskip=0pt,%
	showstringspaces=false,tabsize=2,breaklines=true}
\floatstyle{ruled}
\newfloat{listing}{tb}{lst}{}
\floatname{listing}{Listing}
%
% Keep the \pdfinfo as shown here. There's no need
% for you to add the /Title and /Author tags.
\pdfinfo{
/TemplateVersion (2025.1)
}

\setcounter{secnumdepth}{0} %May be changed to 1 or 2 if section numbers 
\title{Technical Appendix}


% REMOVE THIS: bibentry
% This is only needed to show inline citations in the guidelines document. You should not need it and can safely delete it.
\usepackage{bibentry}
% END REMOVE bibentry

\begin{document}

\maketitle

\appendix

\section{Algorithm}
In order to better understand CIER, we provide the algorithm of CIER. Algorithm~\ref{alg:train} shows the process of training, and Algorithm~\ref{alg:infer} shows the process of inference.
\begin{algorithm}[!h]
    \caption{Algorithm of Training}
    \label{alg:train}
    \renewcommand{\algorithmicrequire}{\textbf{Input:}}
    \renewcommand{\algorithmicensure}{\textbf{Output:}}
    \begin{algorithmic}[1]
        \REQUIRE A training dataset $D$ includes user $u$, item $i$, corresponding rating $r$, explanation $e$ and backbone words $w$
        %%input
        \ENSURE A trained CIER model    %%output
        \FOR{each batch $(u, i, r, e, w) \in D$}
            \STATE Soft rating $r_s = Smooth(r)$ (Eq. (5))
            \STATE Rating embeddings $s_r = Embedding(r_s)$ (Eq. (3))
            \STATE Predicted rating distribution $\hat{r_s} = LLM(u,i)$ (Eq. (1))
            \STATE Compute $\mathcal{L}_r(r,\hat{r_s})$ (Eq. (8))
            \STATE Generate a random number $n \in [0,1]$
            \STATE Calculate $P(t)$ based on the current training step number $t$ (Eq. (6))
            \IF {$n \leq P(t)$}
                \STATE Generated explanation $\hat{e}=LLM(u, i, s_r)$ (Eq. (4))
                \STATE Compute $\mathcal{L}_e(e,\hat{e})$ (Eq. (9))
            \ELSE
                \STATE Generated backbone word $\hat{w}=LLM(u, i, s_r)$ (Eq. (4))
                \STATE Compute $\mathcal{L}_e(w,\hat{w})$ (Eq. (9))
            \ENDIF
            \STATE Update model by minimizing $\mathcal{L}_r$ and $\mathcal{L}_e$ (Eq. (10))
        \ENDFOR        
        
    \end{algorithmic}
\end{algorithm}

\begin{algorithm}[!h]
    \caption{Algorithm of Inference}
    \label{alg:infer}
    \renewcommand{\algorithmicrequire}{\textbf{Input:}}
    \renewcommand{\algorithmicensure}{\textbf{Output:}}
    \begin{algorithmic}[1]
        \REQUIRE A user $u$ and an item $i$
        %%input
        \ENSURE A rating $\hat{r}$ and an explanation $\hat{e}$   %%output
        
        \STATE Predicted rating distribution $\hat{r_s} = LLM(u,i)$ (Eq. (1))

        \STATE Calculate the rating $\hat{r} = \sum_{x=1}^|r| r_{s,i} \dot x$ (Eq. (2))

        \STATE Rating embeddings $s_r = E(\hat{r})$ (Eq. (3))
            
        \STATE Generated explanation $\hat{e}=LLM(u, i, s_r)$ (Eq. (4))  
        
    \end{algorithmic}
\end{algorithm}
% \section{Appendix}
% \section{Implementation Details}
% All the experiments are conducted on an NVIDIA H800-80G GPU.
% We utilize the validation set to tune hyper-parameters for each dataset, and subsequently present the average evaluation metrics computed across 5 data splits on the testing set. 

% We loaded LLaMA2-7B from HuggingFace\footnote{https://huggingface.co/meta-llama/Llama-2-7b} as the backbone of our proposed model, utilizing BPE~\cite{BPE} for vocabulary construction. 
% To ensure fair comparison, we applied BPE to all baseline models and set the max explanation length to 20 BPE tokens.
% For CIER, we set the weight of rating prediction $\lambda$ to 0.1, and the probability of rating smoothing $\gamma$ to 0.2. 

% The CIER is optimized using the AdamW ~\cite{Adamw} optimizer with a hierarchical learning rate: $10^{-4}$ for the Lora module and $10^{-3}$ for the other components. The training epochs is set to 3 and the embedding size $d$ is set to 1024. For the hyperparameter setting of LoRA, $r$ is 4, lora\_alpha is 32, target\_modules is ["q\_proj", "k\_proj"] and lora\_dropout is 0.05.

% At the end of each epoch, we calculate the model's loss on the validation set. If the validation loss decreases, the model is saved.
\section{Comparing the results of using different PLMs as backbone.}
CIER does not strongly depend on a specific pre-trained language model. Table shows the effects of CIER on GPT-2, llama-v2-7b, and vicuna-7b-1.5. It can be found that on the larger dataset Yelp, the performance of gpt2 is not inferior to vicuna. On smaller datasets, gpt2 is difficult to match the 7b large-scale language model. LLM can better demonstrate its rich prior knowledge when the downstream dataset is smaller.
\begin{table}[h]
\small
    \begin{center}
        \resizebox{\linewidth}{!}{
            \begin{tabular}{l|cc|cc|cc}
                \hline
                \multirow{}{}&\multicolumn{2}{c|}{Yelp}&\multicolumn{2}{c|}{Amazon}&\multicolumn{2}{c}{TripAdvisor}\\
                \cline{2-7}
                &FMR$\uparrow$&R-L$\uparrow$&FMR$\uparrow$&R-L$\uparrow$&FMR$\uparrow$&R-L$\uparrow$\\
                \hline
                GPT-2&\textbf{8.87}&10.75&\textbf{12.49}&11.57&7,42&12.85\\
                Llama-v2-7b&8.71&\textbf{10.90}&12.45&11.70&\textbf{8.08}&13.40\\
                Vincuna-7b-1.5&8.70&10.76&12.42&\textbf{11.73}&7.97&\textbf{13.42}\\
                \hline
            \end{tabular}
        }
    \end{center}
    \caption{CIER's explainability and text quality on different PLMs.}
    \label{tab:plms}
\end{table}

\section{Comparing the results of different smoothing techniques}
The experimental results of applying different smoothing techniques on cier are shown in Table~\ref{tab:smooth}. When the rating prediction task is regarded as a classification task, the similarity between adjacent categories is high. Therefore, when smoothing, smoothing the rating to adjacent categories can bring better robustness to the model.
\begin{table}[h]
\small
    \begin{center}
        \resizebox{\linewidth}{!}{
            \begin{tabular}{l|cc|cc|cc}
                \hline
                \multirow{}{}&\multicolumn{2}{c|}{Yelp}&\multicolumn{2}{c|}{Amazon}&\multicolumn{2}{c}{TripAdvisor}\\
                \cline{2-7}
                &FMR$\uparrow$&R-L$\uparrow$&FMR$\uparrow$&R-L$\uparrow$&FMR$\uparrow$&R-L$\uparrow$\\
                \hline
                Hard rating&8.66&10.80&12.35&11.65&7.95&13.31\\
                Label Smoothing&8.63&10.87&12.33&11.61&8.01&13.36\\
                Gaussian smoothing&8.66&\textbf{10.91}&12.35&11.66&8.04&\textbf{13.40}\\
                Ours&\textbf{8.71}&10.90&\textbf{12.45}&\textbf{11.70}&\textbf{8.08}&\textbf{13.40}\\
                \hline
            \end{tabular}
        }
    \end{center}
    \caption{CIER's explainability and text quality on different smoothing techniques.}
    \label{tab:smooth}
\end{table}

\section{Instructions for using GPT-4 to automate evaluation of coherence}
Figure~\ref{fig:auto instruction} shows the instructions we designed for GPT-4 to enable it to automatically evaluate the coherence of predicted ratings and explanations. The instructions describe the rules for determining whether the ratings and explanations are coherent.
\begin{figure}[h]
    \centering
    \resizebox{\linewidth}{!}{
    \includegraphics{figures/auto instruction (2).pdf}
    }
    \caption{An instruction for GPT4 to perform automatic verification..}
    \label{fig:auto instruction}
\end{figure}
% 不同backbone的效果
\end{document}
