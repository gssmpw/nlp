%File: formatting-instructions-latex-2025.tex
%release 2025.0
\documentclass[letterpaper]{article} % DO NOT CHANGE THIS
\usepackage{aaai25}  % DO NOT CHANGE THIS
\usepackage{times}  % DO NOT CHANGE THIS
\usepackage{helvet}  % DO NOT CHANGE THIS
\usepackage{courier}  % DO NOT CHANGE THIS
\usepackage[hyphens]{url}  % DO NOT CHANGE THIS
\usepackage{graphicx} % DO NOT CHANGE THIS
\urlstyle{rm} % DO NOT CHANGE THIS
\def\UrlFont{\rm}  % DO NOT CHANGE THIS
\usepackage{natbib}  % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT
\usepackage{caption} % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT
\frenchspacing  % DO NOT CHANGE THIS
\setlength{\pdfpagewidth}{8.5in}  % DO NOT CHANGE THIS
\setlength{\pdfpageheight}{11in}  % DO NOT CHANGE THIS
%
% These are recommended to typeset algorithms but not required. See the subsubsection on algorithms. Remove them if you don't have algorithms in your paper.
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{amsmath}
\usepackage{multirow}
\usepackage{soul, color, xcolor}
\sethlcolor{blue}
%
% These are are recommended to typeset listings but not required. See the subsubsection on listing. Remove this block if you don't have listings in your paper.
\usepackage{newfloat}
\usepackage{listings}
\usepackage{cases}
\DeclareCaptionStyle{ruled}{labelfont=normalfont,labelsep=colon,strut=off} % DO NOT CHANGE THIS
\lstset{%
	basicstyle={\footnotesize\ttfamily},% footnotesize acceptable for monospace
	numbers=left,numberstyle=\footnotesize,xleftmargin=2em,% show line numbers, remove this entire line if you don't want the numbers.
	aboveskip=0pt,belowskip=0pt,%
	showstringspaces=false,tabsize=2,breaklines=true}
\floatstyle{ruled}
\newfloat{listing}{tb}{lst}{}
\floatname{listing}{Listing}
%
% Keep the \pdfinfo as shown here. There's no need
% for you to add the /Title and /Author tags.
\pdfinfo{
/TemplateVersion (2025.1)
}


\setcounter{secnumdepth}{0} 

\title{Coherency Improved Explainable Recommendation via Large Language Model}
\author{
    Shijie Liu\textsuperscript{\rm 1}\equalcontrib,
    Ruixing Ding\textsuperscript{\rm 1}\equalcontrib,
    Weihai Lu\textsuperscript{\rm 2}\equalcontrib,
    Jun Wang\textsuperscript{\rm 1},
    Mo Yu\textsuperscript{\rm 3},
    Xiaoming Shi\textsuperscript{\rm 1},
    Wei Zhang\textsuperscript{\rm 1}\thanks{Corresponding author.}
}
\affiliations{
    %Afiliations
    \textsuperscript{\rm 1}East China Normal University,~~~\\
    \textsuperscript{\rm 2}Peking University,~~~\\
    \textsuperscript{\rm 3}	WeChat AI, Tencent \\
    karrich128@gmail.com, zhangwei.thu2011@gmail.com
}

\iffalse
\title{My Publication Title --- Single Author}
\author {
    Author Name
}
\affiliations{
    Affiliation\\
    Affiliation Line 2\\
    name@example.com
}
\fi

\iffalse
\title{My Publication Title --- Multiple Authors}
\author {
    % Authors
    First Author Name\textsuperscript{\rm 1,\rm 2},
    Second Author Name\textsuperscript{\rm 2},
    Third Author Name\textsuperscript{\rm 1}
}
\affiliations {
    % Affiliations
    \textsuperscript{\rm 1}Affiliation 1\\
    \textsuperscript{\rm 2}Affiliation 2\\
    firstAuthor@affiliation1.com, secondAuthor@affilation2.com, thirdAuthor@affiliation1.com
}
\fi


% REMOVE THIS: bibentry
% This is only needed to show inline citations in the guidelines document. You should not need it and can safely delete it.
\usepackage{bibentry}
% END REMOVE bibentry

\begin{document}

\maketitle

\begin{abstract}
Explainable recommender systems are designed to elucidate the explanation behind each recommendation, enabling users to comprehend the underlying logic.
Previous works perform rating prediction and explanation generation in a multi-task manner. 
However, these works suffer from incoherence between predicted ratings and explanations.
To address the issue, we propose a novel framework that employs a large language model (LLM) to generate a rating, transforms it into a rating vector, and finally generates an explanation based on the rating vector and user-item information. 
Moreover, we propose utilizing publicly available LLMs and pre-trained sentiment analysis models to automatically evaluate the coherence without human annotations.
Extensive experimental results on three datasets of explainable recommendation show that the proposed framework is effective, outperforming state-of-the-art baselines with improvements of 7.3\% in explainability and 4.4\% in text quality. 
% The source code is available at \url{https://github.com/karrich/CIER}.

\end{abstract}
\begin{links}
\link{Code}{https://github.com/karrich/CIER}
\end{links}
\section{Introduction}

Recommendation systems provide personalized suggestions to maximize user engagement and satisfaction based on historical interactions and preferences~\cite{zhang2019deep}, showing significant potential and technological value.
Recently, to relieve the concerns regarding trustworthiness due to the inherent lack of transparency and explainability, 
 explainable recommendation systems have been introduced~\cite{ZhangC20,ZhangYWW22}. 
 These systems elucidate the rationale behind each recommendation, enabling users to comprehend the underlying logic. 
 This enhanced understanding empowers users to make informed decisions and fosters greater trust in the system's suggestions.

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/example_final.pdf}
    \caption{Explanations generated by NRT, PETER, and CER for an example from Amazon Movies.}
    \label{fig:example}
    
\end{figure}


Current works on explainable recommendation systems generate ratings and provide corresponding explanations~\cite{ni-etal-2017-estimating,DualLearning,ACL21-PETER,cheng-etal-2023-explainable}.
Specifically, the rating prediction and explanation generation modules are jointly learned in a multi-task learning manner, 
sharing a common hidden representation layer but having individual output layers.
Despite improvements in explanations, 
these methods suffer from incoherence between predicted ratings and explanations. 
As shown in Figure~\ref{fig:example}, NRT~\cite{NRT} and PETER~\cite{ACL21-PETER} generate inconsistent explanations.
This inconsistency arises because these two tasks only share hidden layer representation, and explanation generation does not explicitly include rating information.
To enhance the coherency, CER~\cite{ECAI23-CER} proposes explanation-based rating estimation, 
obtaining explanation embeddings through max pooling of generated text embeddings and minimizing the distance between the explanation and the corresponding rating vectors.
Despite the improved coherency, 
CER suffers from two issues: (1)
CER utilizes a small-sized transformer as the backbone, which limits the generative performance.
(2) CER struggles to enforce coherence due to poor sentence embedding, as it relies on max pooling of pre-trained word embeddings, which fails to capture rich contextual information~\cite{neelakantan2022text,wang2023improving}.
As such, CER fails to generate coherent explanations, as reflected in the figure.

Recently, the revolutionary progress in \textbf{l}arge \textbf{l}anguage \textbf{m}odels (LLM)~\cite{zeng2022glm,openai2023gpt4,touvron2023llama} has catalyzed substantial technological transformations in natural language generation and reshaped its foundation.
Inspired by LLMs, we propose using them as the backbone model to predict ratings and generate explanations for recommendation systems.
LLMs produce fluent and accurate ratings and explanations, addressing the first issue.
To tackle the second issue, we propose generating ratings and explanations in a pipeline manner, similar to next-token prediction, which is suitable for decoder-based LLMs.

Specifically, an LLM is fine-tuned with LoRA~\cite{lora} to predict ratings, which are subsequently transformed into rating vectors, while explanations are generated using both user and item information in conjunction with rating vectors.
The generation process utilizes the rating as input for the LLM, enhancing the coherency through its in-context learning capability. Meanwhile, training techniques such as rating smoothing, curriculum learning, and multi-task learning are employed to enhance performance, with experiments demonstrating their effectiveness.

Besides, coherency evaluation is crucial yet challenging.
Current methods can be divided into manual and automatic evaluations.
Manual evaluation, while effective, is labor-intensive and impractical at scale. 
To address this,
a study~\cite{ECAI23-CER} proposes using a binary classifier trained on manually annotated data for automatic evaluation.
Despite its high efficiency, this automated metric relies heavily on the quality and quantity of the annotated data, which is time-consuming and costly.
To overcome these limitations, we propose utilizing GPT-4~\cite{achiam2023gpt} and a pre-trained sentiment analysis model~\cite{sentiment} to assess coherency without additional manual annotations. GPT-4 excels in advanced natural language understanding, while the BERT-based pre-trained model is tailored for sentiment classification in product reviews, making both well-suited for our purposes. 

The main contributions are as follows:
\begin{itemize}
    \item To generate more coherent explanations, we propose a framework, named CIER (\textbf{C}oherency-\textbf{I}mproved \textbf{E}xplainable \textbf{R}ecommendation), which initially predicts a rating with LLMs and subsequently leverages the rating to generate an explanation. 

    \item For a more streamlined assessment of coherency between ratings and explanations, we propose to employ LLMs and pre-trained sentiment analysis models.

    \item 
    We conduct extensive experiments to demonstrate the effectiveness of the proposed framework against strong baselines, and experimental results show that training techniques can further improve the results.
    
\end{itemize}

\section{Related Work}

\subsection{Explainable Recommender Systems}
In recent years, more and more research has focused on how to provide good explanations for recommendations to enhance system effectiveness and user satisfaction. Various explanation styles include topical word clouds~\cite{cloud}, highlighted images~\cite{image}, knowledge graphs~\cite{KG}, and automatically generated textual explanations~\cite{ACL21-PETER}. The latter is of particular interest, as textual explanations are more easily comprehended by users, particularly non-expert users, and more informative than pre-defined templates. 

In this work, we focus on generating high-quality explanatory texts while providing accurate recommendations. Our proposed CIER framework aims to address the flaw of inconsistencies between recommendations and natural language explanations provided by existing methods~\cite{ACL21-PETER,NRT,TOIS23-PEPLER,ECAI23-CER,yang2021explanation,zhang2023triple,DualLearning}.



\subsection{LLMs for Explainable Recommendation}
With the advancement of natural language generation techniques, several studies have employed Recurrent Neural Networks (e.g., Long Short-Term Memory~\cite{lstm}, Gated Recurrent Unit~\cite{GRU}), unpretrained Transformer~\cite{transformer} and pre-trained language models (e.g., BERT~\cite{bert}) for generating explanations. Pre-trained large language models are initially introduced in PEPLER~\cite{TOIS23-PEPLER} to enhance the performance of explanation generation. Although PEPLER utilizes prompt-based transfer learning with GPT-2~\cite{Radford2019LanguageMA}, it fails to structure training data in a manner suitable for instruction tuning, thereby limiting the system's ability to produce high-quality explanations. 

Our proposed CIER framework is designed to harness the language capabilities of LLMs to advance the field of explainable recommender systems.



\subsection{Explainable Recommendation Evaluation Metrics}
Previous works mostly rely on perplexity and overlapping-based metrics such as Distinct-N~\cite{distinct-n}, Rouge score~\cite{rouge}, and BLEU score~\cite{bleu}, to evaluate against the ground truth explanations. However, none of these metrics assess how truthfully the generated explanations reflect the rating predictions.

The studies~\cite{ECAI23-CER,yang2021explanation} introduce some automatic methods for evaluating the consistency between predictions and explanations. However, the reliance on the manual rules and quality of annotations significantly impacts the effectiveness and reliability of the evaluation process, which also raises concerns about reproducibility. 
To address these limitations, we introduce a new automatic evaluation method that uses publicly available pre-trained language models to assess rating-explanation coherence.

\begin{figure*}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/model_final.pdf}
    % \vspace{-1em}
    \caption{The overview framework of CIER. (a) Rating Prediction: aiming to predict users' ratings of items based on LLM. (b) SR2WE: embedding the predicted soft rating into the LLM word embedding space. (c) Rating-Aware Explanation Generation: using the predicted ratings as context to generate explanations related to the ratings.}
    \label{fig:model}
\end{figure*}

\section{Methodology}
The overview of the proposed method CIER is depicted in Figure~\ref{fig:model}, with three modules, rating prediction, \textbf{s}oft \textbf{r}ating to \textbf{w}ord \textbf{e}mbedding (SR2WE), and explanation generation.
In what follows, we first provide the problem formulation, then introduce the details and training techniques of CIER, and finally describe the proposed automatic evaluation method for assessing the coherence.


\subsection{Problem Formulation}

Given a pair of user $u$ and item $i$, the objective is to jointly predict a rating $r_{u,i}$ and generate an explanation $E_{u,i}$ that justifies this rating. 
The rating $r_{u,i}$ is a score from 1 to 5 that reflects the user $u$'s preference towards the item $i$.
The explanation $E_{u,i}$ is a sequence of tokens from a predefined vocabulary $\mathcal{V}$ that provides a personalized verbalizer.  


\subsection{Proposed Method CIER}
\subsubsection{Rating Prediction}
The objective of the rating prediction task is to estimate the rating a user $u$ would give to an item $i$, similar to typical recommendation tasks. To construct a unified framework for both the rating prediction and explanation generation tasks, we employ LLaMA2-7B as the backbone for CIER and use a corresponding verbalizer~\cite{hu2021knowledgeable} specifically for the rating prediction component.
% The objective of the rating prediction task is to estimate the rating a user $u$ would give to an item $i$, similar to typical recommendation tasks. To unify rating prediction and explanation generation, we use LLaMA2-7B as the backbone for CIER and a verbalizer~\cite{hu2021knowledgeable} for the rating prediction component.


The verbalizer $V^r$ is a fixed mapping from numeric ratings to their word representations, defined as $V^r=$\{1: “1”, 2: “2”, 3: “3”, 4: “4”, 5: “5”\}. This design facilitates consistency in the rating prediction process.
The probability assigned by the model to each word in $V$ corresponds to the probability of each respective rating:
\begin{equation}
\begin{aligned}
\hat{r}_{u,i} = f([u, i, p_{1}, \ldots, p_{m}]),
\end{aligned}
\end{equation}
where $p$ represents the prompt, $f$ is the LLM, $m$ is the prompt length, and $\hat{r}_{u,i}$ is the predicted probability of each rating. %The specific  prompt is shown in Appendix.
Then the rating is obtained by weighted summation:
\begin{equation}
\begin{aligned}
\hat{r}_{score} = \sum_{x=1}^{|r|} \hat r_{u,i,x} \cdot x ~,
\end{aligned}
\end{equation}
where $|r|$ is the number of rating classes, $\hat r_{u,i,x}$ is the probability of rating $x$, and \( \sum_{x=1}^{|r|} \hat r_{u,i,x} = 1 \).

\subsubsection{Soft Rating to Word Embedding}

For a given rating, the hard rating embedding directly uses the corresponding word embedding in the verbalizer.
However, hard ratings have less information than soft ratings, so we try to embed soft-rating into the word embedding space, which is defined as follows:
\begin{equation}
\begin{aligned}
\mathbf{s}_{r_{u,i}} = \sum_{x=1}^{|r|} \hat r_{u,i,x} \cdot \text{Embedding}_{LLM}(V^r(x)) ~,
\end{aligned}
\end{equation}
where $\text{Embedding}_{LLM}$ is the word embedding layer of the LLM, and $V^r(x)$ is the corresponding word of rating $x$ in the verbalizer.
At this point, we have obtained the semantic representation of the predicted rating, which encapsulates the uncertainty and distribution features of user $u$'s preference towards item $i$.

\subsubsection{Rating-Aware Explanation Generation}
The rating-aware explanation generation module aims to generate an explanation based on given $u$, $i$, and $r_{u,i}$.

The process is formulated as follows:
\begin{equation}
\begin{aligned}
% r_{u,i} = f([u, i, e_{1}, \ldots, e_{\left|E_{u,v}\right|}]) ~,
E_{u,i} = f([u, i, {s}_{r_{u,i}}, p_{1}, \ldots, p_{j}]),
\end{aligned}
\end{equation}
where $p$ represents the prompt, $f$ is the LLM, $j$ is the prompt length, ${s}_{r_{u,i}}$ is the rating embedding from SR2WE module, and $E_{u,i}$ is the generated explanation.


\subsection{Training Techniques}
To balance efficiency and performance, we conduct Lora tuning for LLM. In addition, three training techniques are utilized in this work for better performance, i.e., rating smoothing, curriculum learning, and multi-task learning.
\subsubsection{Rating Smoothing}
Using a probability distribution over possible ratings to obtain the rating embedding in the inference phase offers several potential benefits.
However, training the model exclusively on ground-truth ratings introduces a notable disparity between the training phase and inference. 

To address this, we introduce a rating smoothing technique that is inspired by label smoothing but incorporates enhancements tailored to our specific scenario. Traditional label smoothing distributes probability across all categories, potentially diluting the model's sensitivity to user-specific ratings.
In rating prediction, adjacent ratings contain similar sentiments, so our proposed rating smoothing prevents over-smoothing by limiting the impact to ratings that are numerically adjacent to the ground truth ratings (called neighboring ratings).
Specifically, 
with a probability \(\gamma\), the original one-hot distribution of rating $r_{u,i}$ is transformed to:
\begin{equation}
\begin{aligned}
{r}_{u,i,x}^{\text{modified}} = 
\begin{cases} 
{1 - \alpha} & \text{if } x = r_{u,i} \\
\frac{\alpha}{k} & \text{if } x \in \mathcal{N}_{r_{u,i}}^k\\
0 & \text{others} \,,
\end{cases} 
\end{aligned}
\end{equation}
where \(\alpha \in [0, \frac{k}{k+1}]\), \(\mathcal{N}_{r_{u,i}}^k\) denotes the set of \(k\) neighboring ratings of $r_{u,i}$.
Regarding to the selection of the smoothing technique, various possibilities are explored and the proposed rating smoothing is intuitive and experimentally proven to be effective.% (more details in Technical Appendix). 

\subsubsection{Training With Curriculum Learning}

Previous methods struggle to capture explanatory keywords that reflect users' interests in explanations, showing low explainability. 
To address this problem, we introduce a keyword generation task to help the model identify item features (e.g. lobby, location) that the user cares about in explanations. 

Inspired by curriculum learning, we propose a training strategy that allows models to build foundational knowledge before tackling more intricate problems. 
Specifically, we devise a linear transition mechanism that dynamically adjusts the data allocation between the keyword generation and explanation generation tasks during training. 
The transition probability $P(t)$ represents the likelihood of the data point used for explanation generation task in batch $t$:
\begin{equation}
\begin{aligned}
P(t) = \frac{t}{T} ~,
\end{aligned}
\end{equation}
where $T$ denotes the total number of training batches.
During each batch, data points are probabilistically assigned to either task based on a random number 
$n$ generated from a uniform distribution over [0, 1].
The assignment is determined by comparing \( n \) with \( P(t) \):

\begin{subnumcases} {\label{weqn} \text{Task}(t) =}
Task_{\text{explanation}} & \text{if } $n < P(t)$ \\
Task_{\text{keyword}} & \text{if } $n \geq P(t)$\,.
\end{subnumcases}
The training process initially focuses on predicting the keywords of explanations, gradually shifting towards generating complete explanations.
This approach retains foundational knowledge while integrating the complexities of explanation generation. 
\begin{figure}[t!]
    \centering
    \includegraphics[width=\linewidth]{figures/curr_final.pdf}
    \vspace{-1em}
    \caption{Instructions and prompts for curriculum learning.}
    \label{fig:curriculum learning}
\end{figure}

Figure~\ref{fig:curriculum learning} shows the specific instructions and prompts used. During the keyword training process, ``explanation'' in the prompt will be replaced with ``keyword'', and the target will be replaced from a complete explanation to the key words in the explanation.

\subsubsection{Multi-Task Learning}

The cross-entropy loss (CE) is utilized as the loss function for rating prediction:
\begin{equation}
\begin{aligned}
\mathcal{L}_r = - \frac{1}{|\mathcal{T}|} \sum_{(u,i) \in \mathcal{T}} \sum_{x=1}^{|r|} r_{u,i,x} \log(\hat{r}_{u,i,x}) ~,
\end{aligned}
\end{equation}
where $\mathcal{T}$ denotes the training set and $r_{u,i,x}$ is the probability of the ground-truth rating being \( x \).
We use the Negative Log-Likelihood (NLL) as the loss function for the text (i.e., explanation or keyword) generation, computing the mean over user-item pairs in the training set.
\begin{equation}
    \begin{aligned}
        \mathcal{L}_e = \frac{1}{|\mathcal{T}|} \sum_{(u,i) \in \mathcal{T}} \frac{1}{|E_{u,i}|} \sum_{t=1}^{|E_{u,i}|} -\log c_{|S| - |E_{u,i}| + t}^{e_t} ~.
    \end{aligned}
\end{equation}
The probability $c_t^{e_t}$ is offset by $|S| - |E_{u,i}| + t$ positions because the
generated text is placed at the end of the sequence. 

We integrate rating prediction and text generation into a multi-task learning framework. The objective function is defined as follows: 
\begin{equation}
\begin{aligned}
\mathcal{J} = \min_{\Theta = \{\Theta_{Lora}, \Theta_{U}, \Theta_{I}\}} (\mathcal{L}_e + \lambda \mathcal{L}_r) ~,
\end{aligned}
\end{equation}
where $\Theta$ denotes all the trainable parameters in the model, including the parameters of Lora modules, i.e., $\Theta_{Lora}$, and the parameters of ID Embeddings, i.e., $\Theta_{U}$ and $\Theta_{I}$. The hyperparameter $\lambda$ is used to balance the learning between the explanation generation task and the rating prediction task. 
It is worth noting that these two tasks are performed in a pipeline manner like next-token prediction, thus suitable for decoder-based LLMs.

\subsection{Automatic Coherence Evaluation}
\label{automatic coherence}
To address heavy reliance on high-quality annotated data in the previous approach~\cite{ECAI23-CER}, we employ publicly available pre-trained language models, specifically GPT-4 and bert-base-multilingual-uncased-sentiment~\cite{sentiment}, to automatically assess the rating-explanation coherency. 
GPT-4 has recently demonstrated remarkable performance across various tasks, leading to its widespread use as an evaluator~\cite{10.5555/3666122.3666237,NEURIPS2023_ac662d74}. Meanwhile, the BERT-based model is specifically designed for sentiment analysis in product reviews, making it particularly suitable for our purposes.

For GPT-4, a prompt is utilized to provide clear guidelines on how sentiment should match each rating level and
an instruction is used to make it respond with ``Yes'' or ``No'' based on the coherency between ratings and explanations. 
The percentage of coherent rating-explanation pairs identified by GPT-4 serves as a performance metric. 
Specifically, the ``gpt-4o'' model is utilized to evaluate randomly sampled 500 predictions from each model. 

Bert-base-multilingual-uncased-sentiment is applied to predict the sentiment rating of explanations for all predictions. 
Given the influence of personalized factors on rating predictions and the individual biases across different datasets, coherency is defined as the predicted sentiment rating deviating by no more than one point from the given rating, defined as follows:
\begin{subnumcases}
{\text{Coherency} =}
1 & \text{if}  $| y - \hat{y}| \leq 1$ \\
0 & \text{otherwise}
\end{subnumcases}
where \( y \) represents the rating provided by explainable recommendation model, and  \( \hat{y} \) represents that from the sentiment classification model.

\section{Experiments}
\subsection{Experimental Setting}
\subsubsection{Dataset}
To validate the effectiveness of our method, we conducted experiments on three publicly available datasets and their splits~\cite{nete}. Each dataset is randomly divided into training, validation, and test sets in an 8:1:1 ratio five times. The three datasets are from TripAdvisor (hotel), Amazon (movies \& TV), and Yelp (restaurant). Each record in the dataset consists of a user ID, an item ID, a rating on a scale of 1 to 5, an explanation, and item features. The explanations are sentences extracted from user reviews. 
Features are attributes of items extracted from the explanation, e.g., \textit{lobby}, which represent aspects users care about, and we consider them as the keyword of explanations. 
The dataset statistics are shown in Table~\ref{tab:statistics}.
The available datasets and keyword extraction tools are provided by Sentires~\cite{SIGIR14-Sentires, Sentires}.

\begin{table}[t]
    \small
    \centering
    \begin{tabular}{l|r|r|r}
        \hline
        Datasets&Yelp&Amazon&TripAdvisor\\
        \hline
        \#users&27,147&7,506&9,765\\
        \#items&20,266&7,360&6,280\\
        \#records&1,293,247&441,783&320,023\\
        \#features&7,340&5,399&5,069\\
        \hline
    \end{tabular}
    % }
    \vspace{-0.5em}
    \caption{Statistics of the datasets.}
    \label{tab:statistics}
    %
\end{table}
\subsubsection{Evaluation Metrics}
To evaluate the performance of rating prediction, we utilize two commonly used metrics: Root Mean Square Error (\textbf{RMSE}) and Mean Absolute Error (\textbf{MAE}) to measure the deviation between predicted ratings and ground truth ratings.

For explanation performance, we measure the generated explanations from two main perspectives: text quality and explainability. For the text quality, we use \textbf{BLEU}~\cite{bleu} and \textbf{ROUGE}~\cite{rouge}, which are common metrics in natural language generation tasks.
Specifically, we use BLEU-1 and BLEU-4 metrics to evaluate the precision, the recall-scores of ROUGE-1 and ROUGE-2 to evaluate the recall, and the f1-score of ROUGE-L for comprehensive evaluation.
For the text explainability, we use additional indicators proposed by ~\cite{nete} to measure explainability: Feature Matching Ratio (\textbf{FMR}), Feature Coverage Ratio (\textbf{FCR}), Feature Diversity (\textbf{DIV}), and Unique Sentence Ratio (\textbf{USR}).

To measure the coherence between explanations and predicted ratings, we perform manual and automated evaluations. For manual evaluation, we follow CER~\cite{ECAI23-CER} to annotate the coherence with two independent human annotators. For automatic annotation, we use our proposed automatic evaluation method.

\subsubsection{Baselines}
\begin{table*}[ht!]
\small
    \begin{center}
            \begin{tabular}{l|cccc|ccccc}
                \hline
                \multirow{2}{*}&\multicolumn{4}{c|}{\textbf{Explainability}}&\multicolumn{5}{c}{\textbf{Text Quality}}\\
                \cline{2-10}
                &FMR$\uparrow$&FCR$\uparrow$&DIV$\downarrow$&USR$\uparrow$&B-1$\uparrow$&B-4$\uparrow$&R-1$\uparrow$&R-2$\uparrow$&R-L$\uparrow$\\
                \hline
                \hline
                &\multicolumn{9}{c}{Yelp}\\
                \hline
                NRT~\cite{NRT}&6.65&11.96&1.77&16.02&11.36&0.65&12.35&1.29&10.39\\
                Att2Seq~\cite{ATT2Seq}&7.08&14.24&1.72&17.65&11.47&0.69&12.46&1.35&10.40\\
                PETER~\cite{ACL21-PETER}&8.09&13.80&\underline{1.65}&8.47&9.68&0.62&11.63&1.26&10.24\\
                CER~\cite{ECAI23-CER}&8.05&15.00&\textbf{1.59}&9.67&10.03&0.65&11.72&1.29&10.27\\
                PEPLER~\cite{TOIS23-PEPLER}&8.11&21.04&1.73&20.32&10.94&0.67&12.05&1.36&10.41\\
                ERRA$^{\ast}$~\cite{cheng-etal-2023-explainable}&\textbackslash&\textbackslash&\textbackslash&\textbackslash&10.71&0.73&\textbackslash&1.36&10.82\\
                \hline
                CIER (Curriculum Learning)&\textbf{8.71}&\textbf{53.84}&1.67&\textbf{32.63}&\textbf{11.78}&\textbf{0.83}&\textbf{13.02}&\textbf{1.59}&\textbf{10.90}\\
                ~~~~~~~~~~~~~Two-Stage Learning&8.61&\underline{52.46}&1.67&{31.30}&\underline{11.62}&\underline{0.82}&\underline{12.89}&\underline{1.57}&\underline{10.85}\\

                ~~~~~~~~~~~~~Vanilla Learning&\underline{8.62}&52.19&1.70&\underline{32.48}&11.39&0.79&12.78&1.54&\underline{10.85}\\
                \hline
                \hline
                
                &\multicolumn{9}{c}{Amazon}\\
                \hline
                NRT~\cite{NRT}&11.13&5.67&2.38&14.57&12.62&0.89&13.82&1.88&11.24\\
                Att2Seq~\cite{ATT2Seq}&11.11&8.22&2.17&22.12&12.86&0.92&13.88&1.87&11.19\\
                PETER~\cite{ACL21-PETER}&11.60&9.12&2.20&13.30&12.38&1.00&13.45&1.94&11.29\\
                CER~\cite{ECAI23-CER}&11.47&10.25&2.09&14.72&12.02&1.02&13.23&1.92&11.05\\
                PEPLER~\cite{TOIS23-PEPLER}&11.88&34.07&2.26&24.87&12.57&1.03&13.83&1.92&11.31\\
                \hline
                CIER (Curriculum Learning)&\textbf{12.45}&\textbf{51.80}&\underline{2.08}&46.99&\textbf{13.55}&{1.15}&\textbf{14.61}&{2.09}&\textbf{11.70}\\

                ~~~~~~~~~~~~~Two-Stage Learning&\underline{12.21}&\underline{51.11}&\textbf{2.01}&\underline{50.26}&13.43&\underline{1.18}&14.50&\underline{2.12}&\underline{11.67}\\
                
                ~~~~~~~~~~~~~Vanilla Learning&{12.00}&50.84&\underline{2.08}&\textbf{50.92}&\underline{13.53}&\textbf{1.23}&\underline{14.58}&\textbf{2.13}&11.65\\
                \hline
                \hline
                &\multicolumn{9}{c}{TripAdvisor}\\
                \hline
                NRT~\cite{NRT}&5.76&14.15&3.09&18.29&14.85&0.96&15.07&1.98&12.24\\
                Att2Seq~\cite{ATT2Seq}&5.78&10.61&\underline{2.92}&10.33&15.16&0.97&15.17&1.97&12.22\\
                PETER~\cite{TOIS23-PEPLER}&6.47&13.72&{3.03}&9.60&15.97&1.04&15.94&2.25&12.64\\
                CER~\cite{ECAI23-CER}&6.97&12.99&3.14&9.18&15.59&1.09&15.89&2.19&12.75\\
                PEPLER~\cite{TOIS23-PEPLER}&7.36&19.91&3.35&{24.29}&15.06&1.02&14.92&2.03&12.21\\
                ERRA$^{\ast}$~\cite{cheng-etal-2023-explainable}&\textbackslash&\textbackslash&\textbackslash&\textbackslash&16.13&1.06&\textbackslash&2.15&13.17\\
                \hline
                CIER (Curriculum Learning)&\textbf{8.08}&\underline{36.99}&3.05&\underline{29.86}&\textbf{17.00}&\textbf{1.31}&\textbf{17.07}&\textbf{2.54}&\textbf{13.40}\\

                ~~~~~~~~~~~~~Two-Stage Learning&\underline{7.89}&\textbf{39.08}&3.00&\textbf{31.80}&\underline{16.54}&\underline{1.28}&\underline{16.70}&\underline{2.45}&\underline{13.33}\\
                
                ~~~~~~~~~~~~~Vanilla Learning&{7.73}&{36.60}&\textbf{2.86}&{27.63}&{16.45}&{1.25}&{16.66}&{2.40}&{13.31}\\
                \hline
                \hline

            \end{tabular}
    \end{center}
    \vspace{-1em}
    \caption{Results of explanation. B-1, B-4, R-1, R-2 and R-L represent the scores of BLUE-1, BLEU-4, ROUGE-1, ROUGE-2 and ROUGE-L, respectively. BLEU, ROUGE, FMR, FCR, and USR are presented as percentage (\%), while the others are absolute values. The best values in the table are represented in bold, and the second-best values are represented with underlines. Stars$^{\ast}$ indicate that the results of this method are from its paper.}
    \label{tab:explanation}
\end{table*}

\begin{table}[t]
\small
    \begin{center}
        \resizebox{\linewidth}{!}{
            \begin{tabular}{llc}
                \hline
                Method&Explanation&rating\\
                \hline
                Truth&swimming \textbf{pool} was small and shallow&1\\
                NRT&the bed was comfortable and the room was comfortable&3\\
                PETER&the hotel is a little dated but the rooms are very small&3\\
                CER&the resort is a bit dated but the hotel is a bit dated&1\\
                \textbf{CIER}&the \textbf{pool} is a bit small and the gym is a bit small&1\\
                \hline
            \end{tabular}
        }
    \end{center}
    \vspace{-1em}
    \caption{Example generated by CIER and baselines.}
    \label{tab:output}
\end{table}

To evaluate the explainability performance, we compare the following explanation methods:

\noindent\textbf{NRT}~\cite{NRT} utilizes GRU~\cite{GRU} to jointly predict ratings and generate explanations using user and item IDs as input. 

\noindent\textbf{Att2Seq} ~\cite{ATT2Seq} is an explanation generation model based on LSTM~\cite{lstm}. 

\noindent \textbf{PETER} ~\cite{ACL21-PETER} is a powerful multi-layer Transformer~\cite{transformer} model that simultaneously predicts ratings and generates explanations.
% It employs a context prediction task to effectively incorporate user and item information.

\noindent\textbf{CER} ~\cite{ECAI23-CER} proposes a module that estimates the discrepancy between predicted ratings and explanation-based ratings to enhance rating-explanation coherency. 
% To ensure the fairness of the experiment, we used PETER as the backbone of CER instead of PETER+.

\noindent\textbf{PEPLER} ~\cite{TOIS23-PEPLER} leverages the advanced capabilities of GPT-2 through prompt-based transfer learning and regularization loss. 

\noindent\textbf{ERRA} ~\cite{cheng-etal-2023-explainable} is a multi-layer Transformer with aspect enhancement and retrieval enhancement. Since the code is incomplete, we directly use its results in its paper.

For the evaluation of recommendation performance, in addition to NRT, PETER, and CER, we also use three traditional models as baselines:

 % \noindent\textbf{PMF} ~\cite{PMF} approximates user-item interaction matrices by decomposing them into lower-dimensional latent factor representations.
 
\noindent\textbf{SVD++} ~\cite{SVD++} integrates implicit feedback from users to enhance the latent factors.

\noindent\textbf{DeepCoNN} ~\cite{DeepCoNN} learns item properties and user behavior from review text.

\noindent\textbf{NARRE} ~\cite{NARRE}  applies the attention mechanism to the rating prediction task.

For evaluating coherence, we use PETER, CER, and CIER-M as baselines. CIER-M means that CIER masks the context (predicted ratings) when generating explanations.


\subsection{Implementation Details}
All the experiments are conducted on an NVIDIA H800 GPU.
We utilize the validation set to tune hyper-parameters for each dataset, and subsequently present the average evaluation metrics computed across 5 data splits on the testing set. 
We load LLaMA2-7B from HuggingFace as the backbone of our proposed model, utilizing BPE~\cite{BPE} for vocabulary construction. 
To ensure fair comparisons, we apply BPE to all baseline models and set the max explanation length to 20 BPE tokens.
% For CIER, we set the weight of rating prediction $\lambda$ to 0.1, and the probability of rating smoothing $\gamma$ to 0.2. 
% For CIER, set $\lambda$ to 0.1 by doing a grid search from the range [0.01, 0.1, 1.0, 10.0], and set $\gamma$ to 0.2 from the range [0.0, 0.2, 0.5, 0.8, 1.0].
For CIER, $\lambda$ is set to 0.1 and $\gamma$ to 0.2, selected through grid search over the ranges \([0.01, 0.1, 1.0, 10.0]\) and \([0.0, 0.2, 0.5, 0.8, 1.0]\), respectively.
The model is optimized using the AdamW ~\cite{Adamw} optimizer with hierarchical learning rates: $10^{-4}$ for the Lora module and $10^{-3}$ for the other components. The training epoch is set to 3 and the embedding size $d$ is set to 1024. 
At the end of each epoch, we calculate the model's loss on the validation set. If the validation loss does not decrease anymore, the model is saved.

\begin{table}[!t]
\normalsize
    \begin{center}
        \resizebox{\linewidth}{!}{
            \begin{tabular}{r|cc|cc|cc}
                \hline
                \multirow{2}{*}&\multicolumn{2}{c|}{Yelp}&\multicolumn{2}{c|}{Amazon}&\multicolumn{2}{c}{TripAdvisor}\\
                \cline{2-7}
                &R$\downarrow$&M$\downarrow$&R$\downarrow$&M$\downarrow$&R$\downarrow$&M$\downarrow$\\
                \hline
                % PMF&1.096&0.877&1.034&0.811&0.866&0.701\\
                SVD++&1.019&0.791&0.965&0.722&0.809&0.617\\
                
                DeepCoNN& 1.108& 0.883& 1.108& 0.881&0.888&0.683\\
                NARRE& 1.031& 0.811& 1.003& 0.780&0.817&0.622\\
                NRT&1.016&0.796&0.954&\underline{0.706}&\textbf{0.791}&\textbf{0.605}\\
                PETER&\underline{1.013}&\underline{0.783}&0.953&0.709&0.806&0.623\\
                CER&\underline{1.013}&0.787&\underline{0.952}&0.713&0.814&0.637\\
                CIER&\textbf{1.009}&\textbf{0.781}&\textbf{0.951}&\textbf{0.705}&\underline{ 0.797}&\underline{ 0.612}\\
                \hline
            \end{tabular}
        }
    \end{center}
    \vspace{-1em}
    \caption{The comparison of the recommendation performance of CIER and other baseline methods. “R” means RMSE and “M” means MAE.}
    \label{tab:recommendation}
\end{table}

\begin{table*}[!t]
\small
    \begin{center}
            \begin{tabular}{r|ccc|ccc|ccc}
                \hline
                 \multirow{2}{*}&\multicolumn{3}{c|}{GPT-4}&\multicolumn{3}{c|}{Sentiment-Bert}&\multicolumn{3}{c}{Human annotators}\\
                \cline{2-10}
                &Yelp&Amazon&TripAdvisor&Yelp&Amazon&TripAdvisor&Yelp&Amazon&TripAdvisor\\
                \hline
                PETER&87.2&79.6&87.0&69.2&69.6&80.3&62.0&63.0&84\\
                CER&88.8&80.0&90.4&70.1&70.6&80.7&65.6&66.0&82.5\\
                CIER-M&87.0&81.8&88.0&69.8&74.1&80.1&69.5&60.5&83.5\\
                CIER &\textbf{90.2}&\textbf{89.8}&\textbf{91.6}&\textbf{70.6}&\textbf{77.6}&\textbf{82.2}&\textbf{73.5}&\textbf{74.0}&\textbf{87.0}\\
                \hline
            \end{tabular}
    \end{center}
    \vspace{-1em}
    \caption{Results of coherence evaluation using GPT-4, BERT-based sentiment classification models and human annotations for explanations and prediction ratings of the selected methods.}
    \label{tab:auto}
\end{table*}
\subsection{Evaluation of Explanation}

The text quality and explainability of various explanation generation methods are presented in Table~\ref{tab:explanation}. 
In terms of text quality, our proposed CIER consistently outperforms the baselines on different datasets, demonstrating its effectiveness in generating high-quality sentences.
Table~\ref{tab:output} presents an example generated by the CIER model and some baselines.
By referring to the ground-truth explanation, CIER produces a more accurate explanation.

Regarding explainability, CIER consistently outperforms the baselines on FMR, FCR, and USR, indicating it effectively captures key information in explanatory texts. PEPLER and CIER with vanilla learning, while not explicitly optimized for explainability, demonstrate competitive performance. This can be attributed to their inherent text generation ability obtained by pre-training, enabling them to focus on the nuances and key information within explanations.




\subsection{Evaluation of Rating Prediction}
Evaluation of recommendation accuracy is shown in Table~\ref{tab:recommendation}. 
The experimental results indicate that the proposed method, leveraging an LLM backbone, exhibits strong recommendation performance across all datasets, especially excelling in larger datasets (i.e., Yelp and Amazon). In the smaller, sparser TripAdvisor dataset, while traditional models like NRT perform better, our method still outperforms other Transformer-based models (i.e., PETER and CER).


\subsection{Evaluation of Coherence}

The evaluation of the coherence between the explained and predicted ratings is shown in Table~\ref{tab:auto}. The manual annotation was performed by two volunteers who selected 100 data points from each dataset for the selected methods.
Before annotation, the agreement between the two instructed annotators was measured using the kappa coefficient on a random sample of 200 data points, resulting in a score of 0.918.

Our approach consistently maintains significant advantages in coherence. In particular, our method consistently outperforms CIER-M, suggesting that our approach of using predicted ratings to guide explanation generation allows the model to understand the relationship between ratings and explanations, thereby improving the relationship between explanations and predicted ratings.


\subsection{Effect of Keyword Generation Task}
To test the effect of our designed keyword generation task, we experimented with three different learning strategies:
\begin{itemize}
    \item[1)] \textbf{Vanilla Training},
    which involves training solely for rating prediction and explanation generation.
    While straightforward, it struggles to capture key explanatory words in explanations.
    \item[2)] \textbf{Two-Stage Training}, which involves the model first learning to generate keywords before shifting to explanation generation. 
    While this process helps build a solid foundation, it risks the model forgetting keyword generation knowledge.
    \item[3)] \textbf{Curriculum Learning (Ours)}, which employs a gradual transition from keyword generation to explanation generation.
    It reduces the risk of forgetting keyword generation knowledge and minimizes its negative impacts.
\end{itemize}

\begin{table}[t]
\small
    \begin{center}
        \resizebox{\linewidth}{!}{
            \begin{tabular}{l|cc|cc|cc}
                \hline
                \multirow{2}{*}&\multicolumn{2}{c|}{Yelp}&\multicolumn{2}{c|}{Amazon}&\multicolumn{2}{c}{TripAdvisor}\\
                \cline{2-7}
                &FMR$\uparrow$&R-L$\uparrow$&FMR$\uparrow$&R-L$\uparrow$&FMR$\uparrow$&R-L$\uparrow$\\
                \hline
                CIER&\textbf{8.71}&\textbf{10.90}&\textbf{12.45}&\textbf{11.70}&\textbf{8.08}&\textbf{13.40}\\
                \hline

                w/o RS&8.66&10.80&12.35&11.65&7.95&13.31\\
                
                w/o SR2WE&\underline{8.67}&\underline{10.86}&\underline{12.40}&\underline{11.68}&\underline{8.03}&\underline{13.37}\\
                w/o RA&8.58&10.72&12.36&11.59&7.92&13.35\\
                \hline
            \end{tabular}
        }
    \end{center}
    \vspace{-1em}
    \caption{Ablation analysis of explanation tasks. ``RS'' means rating smoothing, ``RA'' means rating-aware.}
    \label{tab:ablation}
\end{table}


All training processes consist of 3 epochs. For Two-Stage Training, the epochs are distributed in a ratio of 1:2 between the first and second stages.  
The experimental results are shown in Table~\ref{tab:explanation}. 
The two-stage training strategy fails to improve the explainability on the Yelp dataset, possibly due to its large size, which led to knowledge forgetting. Curriculum Learning strategy demonstrates the best performance across all datasets.
It makes the model effectively retain and utilize learned knowledge on keyword generation, resulting in more relevant and accurate explanations. 
However, curriculum learning does not achieve the best performance on the Amazon dataset, likely because 50\% of its keywords appear only once, 10\% more than that in the other datasets.
Thus it is harder to use keywords for generation.

\subsection{Ablation Study}

Table~\ref{tab:ablation} provides the results of the ablation experiments. After removing Rating Smoothing, both the explainability and text quality decline across all datasets.  

Moreover, removing the SR2WE and inserting the ratings from rating smoothing directly into the LLM prompts through the linear layer leads to a decrease in model performance, which indicates that the SR2WE module could better embed the ratings into the word vector space.

After disabling Rating-Aware generation, all indicators show a significant decline, indicating that explicit use of rating information is very beneficial for explanation generation.




\section{Conclusion}
In this paper, we introduce a novel method that utilizes LLMs as the backbone generation model, predicting ratings and explanations with some tailored training techniques. Additionally, we propose to employ LLMs and pre-trained sentiment analysis models to automatically evaluate the coherency between ratings and explanations. Extensive experimental results demonstrate that our approach outperforms the previous state-of-the-art approaches.

\section{Acknowledgments}
This work was supported in part by National Natural Science Foundation of China ( No. 62072182 and No. 92270119), Shanghai Institute of Artificial Intelligence for Education, and Key Laboratory of Advanced Theory and Application in Statistics and Data Science, Ministry of Education.

\bibliography{aaai25}

\appendix




\end{document}
