\section{Related Work}
\subsection{Explainable Recommender Systems}
In recent years, more and more research has focused on how to provide good explanations for recommendations to enhance system effectiveness and user satisfaction. Various explanation styles include topical word clouds~\cite{cloud}, highlighted images~\cite{image}, knowledge graphs~\cite{KG}, and automatically generated textual explanations~\cite{ACL21-PETER}. The latter is of particular interest, as textual explanations are more easily comprehended by users, particularly non-expert users, and more informative than pre-defined templates. 

In this work, we focus on generating high-quality explanatory texts while providing accurate recommendations. Our proposed CIER framework aims to address the flaw of inconsistencies between recommendations and natural language explanations provided by existing methods~\cite{ACL21-PETER,NRT,TOIS23-PEPLER,ECAI23-CER,yang2021explanation,zhang2023triple,DualLearning}.



\subsection{LLMs for Explainable Recommendation}
With the advancement of natural language generation techniques, several studies have employed Recurrent Neural Networks (e.g., Long Short-Term Memory~\cite{lstm}, Gated Recurrent Unit~\cite{GRU}), unpretrained Transformer~\cite{transformer} and pre-trained language models (e.g., BERT~\cite{bert}) for generating explanations. Pre-trained large language models are initially introduced in PEPLER~\cite{TOIS23-PEPLER} to enhance the performance of explanation generation. Although PEPLER utilizes prompt-based transfer learning with GPT-2~\cite{Radford2019LanguageMA}, it fails to structure training data in a manner suitable for instruction tuning, thereby limiting the system's ability to produce high-quality explanations. 

Our proposed CIER framework is designed to harness the language capabilities of LLMs to advance the field of explainable recommender systems.



\subsection{Explainable Recommendation Evaluation Metrics}
Previous works mostly rely on perplexity and overlapping-based metrics such as Distinct-N~\cite{distinct-n}, Rouge score~\cite{rouge}, and BLEU score~\cite{bleu}, to evaluate against the ground truth explanations. However, none of these metrics assess how truthfully the generated explanations reflect the rating predictions.

The studies~\cite{ECAI23-CER,yang2021explanation} introduce some automatic methods for evaluating the consistency between predictions and explanations. However, the reliance on the manual rules and quality of annotations significantly impacts the effectiveness and reliability of the evaluation process, which also raises concerns about reproducibility. 
To address these limitations, we introduce a new automatic evaluation method that uses publicly available pre-trained language models to assess rating-explanation coherence.

\begin{figure*}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/model_final.pdf}
    % \vspace{-1em}
    \caption{The overview framework of CIER. (a) Rating Prediction: aiming to predict users' ratings of items based on LLM. (b) SR2WE: embedding the predicted soft rating into the LLM word embedding space. (c) Rating-Aware Explanation Generation: using the predicted ratings as context to generate explanations related to the ratings.}
    \label{fig:model}
\end{figure*}