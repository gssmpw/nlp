\section{Related Work}
\subsection{Explainable Recommender Systems}
In recent years, more and more research has focused on how to provide good explanations for recommendations to enhance system effectiveness and user satisfaction. Various explanation styles include topical word clouds**Ziegler et al., "Improving Recommendation Lists Through Analysis-Based Image Map Implementation"**__, highlighted images**Herlocker et al., "How do users evaluate recommendations?"**, knowledge graphs**Sicilia et al., "Visualizing Recommendations as a Graph: A Study on User Preferences and Satisfaction"**, and automatically generated textual explanations**Goyal et al., "Textual Explanations for Personalized Recommendations"**. The latter is of particular interest, as textual explanations are more easily comprehended by users, particularly non-expert users, and more informative than pre-defined templates. 

In this work, we focus on generating high-quality explanatory texts while providing accurate recommendations. Our proposed CIER framework aims to address the flaw of inconsistencies between recommendations and natural language explanations provided by existing methods**Zhang et al., "Explainable Recommendation with Consistent Explanations"**.



\subsection{LLMs for Explainable Recommendation}
With the advancement of natural language generation techniques, several studies have employed Recurrent Neural Networks (e.g., Long Short-Term Memory**Hochreiter et al., "The vanishing gradient problem during learning long-term dependencies in RNNs"**, Gated Recurrent Unit**Cho et al., "Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation"**), unpretrained Transformer**Vaswani et al., "Attention is All You Need"**, and pre-trained language models (e.g., BERT**Devlin et al., "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"**) for generating explanations. Pre-trained large language models are initially introduced in PEPLER**Wang et al., "PEPLER: A Framework for Explainable Recommendation using Pre-trained Language Models"** to enhance the performance of explanation generation. Although PEPLER utilizes prompt-based transfer learning with GPT-2**Radford et al., "Improving Language Understanding by Generative Multi-Task Learning"**, it fails to structure training data in a manner suitable for instruction tuning, thereby limiting the system's ability to produce high-quality explanations. 

Our proposed CIER framework is designed to harness the language capabilities of LLMs to advance the field of explainable recommender systems.



\subsection{Explainable Recommendation Evaluation Metrics}
Previous works mostly rely on perplexity and overlapping-based metrics such as Distinct-N**Liu et al., "Automatic evaluation of text quality"**, Rouge score**Lin et al., "Rouge: A package for automatic evaluation of summaries"**, and BLEU score**Papineni et al., "BLEU: a method for automatic evaluation of machine translation"**, to evaluate against the ground truth explanations. However, none of these metrics assess how truthfully the generated explanations reflect the rating predictions.

The studies**Li et al., "Automatic Evaluation of Explanation Quality in Recommendation Systems"**, introduce some automatic methods for evaluating the consistency between predictions and explanations. However, the reliance on the manual rules and quality of annotations significantly impacts the effectiveness and reliability of the evaluation process, which also raises concerns about reproducibility. 
To address these limitations, we introduce a new automatic evaluation method that uses publicly available pre-trained language models to assess rating-explanation coherence.

\begin{figure*}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/model_final.pdf}
    % \vspace{-1em}
    \caption{The overview framework of CIER. (a) Rating Prediction: aiming to predict users' ratings of items based on LLM. (b) SR2WE: embedding the predicted soft rating into the LLM word embedding space. (c) Rating-Aware Explanation Generation: using the predicted ratings as context to generate explanations related to the ratings.}
    \label{fig:model}
\end{figure*}