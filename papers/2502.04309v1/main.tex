\documentclass[manuscript,screen]{acmart}
%\documentclass{article}
\usepackage{graphicx} % Required for inserting images
%\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{appendix}
\usepackage{centernot}
\newcommand{\R}{\mathbb{R}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\Prob}{\mathbb{P}}
\newcommand{\ind}{\mathbf{1}}
\newcommand\independent{\protect\mathpalette{\protect\independenT}{\perp}}
\def\independenT#1#2{\mathrel{\rlap{$#1#2$}\mkern2mu{#1#2}}}

\begin{document}

\title{Targeted Learning for Inference on Data Fairness}

\author{Alexander Asemota}
\email{asemota.alexander@gmail.com}
\orcid{0009-0007-4533-4636}
\affiliation{%
  \institution{University of California Berkeley}
  \city{Berkeley}
  \state{California}
  \country{USA}
}
\author{Giles Hooker}
\affiliation{%
  \institution{University of Pennsylvania}
  \city{Philadelphia}
  \state{Pennsylvania}
  \country{USA}}
\email{ghooker@wharton.upenn.edu}

\date{Janurary 2025}

%%
%% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
%% Please copy and paste the code instead of the example below.
%%
\begin{CCSXML}
<ccs2012>
   <concept>
       <concept_id>10002950.10003648.10003702</concept_id>
       <concept_desc>Mathematics of computing~Nonparametric statistics</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
   <concept>
       <concept_id>10010147.10010257</concept_id>
       <concept_desc>Computing methodologies~Machine learning</concept_desc>
       <concept_significance>300</concept_significance>
       </concept>
   <concept>
       <concept_id>10010405.10010455</concept_id>
       <concept_desc>Applied computing~Law, social and behavioral sciences</concept_desc>
       <concept_significance>300</concept_significance>
       </concept>
 </ccs2012>
\end{CCSXML}

\ccsdesc[500]{Mathematics of computing~Nonparametric statistics}
\ccsdesc[300]{Computing methodologies~Machine learning}
\ccsdesc[300]{Applied computing~Law, social and behavioral sciences}

%%
%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.
\keywords{Fairness, Targeted Learning, Nonparametric statistics, Inference}

\begin{abstract}
    Data and algorithms have the potential to produce and perpetuate discrimination and disparate treatment. As such, significant effort has been invested in developing approaches to defining, detecting, and eliminating unfair outcomes in algorithms. In this paper, we focus on performing statistical inference for fairness. Prior work in fairness inference has largely focused on inferring the fairness properties of a given predictive algorithm. Here, we expand fairness inference by evaluating fairness in the data generating process itself, referred to here as data fairness. We perform inference on data fairness using targeted learning, a flexible framework for nonparametric inference. We derive estimators demographic parity, equal opportunity, and conditional mutual information. Additionally, we find that our estimators for probabilistic metrics exploit double robustness. To validate our approach, we perform several simulations and apply our estimators to real data. 
\end{abstract}

\maketitle

\section{Introduction}

Over the past few decades, data and algorithms have become ever-present in society. Concerns about the use of data and algorithms for decision-making have risen in concert with the ascendancy of data-driven methods. Prior research has investigated problems ranging from privacy violations to explainability and accountability \cite{xai} \cite{privacy}. Here, we focus on fairness, which addresses discrimination and disparate treatment in data and algorithms \cite{fair_review}. Recently, fairness research has focused significantly on \textit{algorithmic fairness}, which aims to investigate algorithms for unfairness and intervene on algorithms to prevent unfair outcomes. Prior work has developed metrics for fairness, methods for fair variable selection, and mechanisms to correct unfair models \cite{fair_regression} \cite{feature}. 

Algorithmic fairness evolved out of concerns about the use of algorithms in high-stakes contexts, particularly those that have a history of disparity and discrimination. When training an algorithm on data from a discriminatory process, it is common for the algorithm to learn to replicate that discrimination, or ‘garbage in, garbage out’. Much work in algorithmic fairness focuses on detecting, evaluating, and correcting unfair algorithms under a garbage-in-garbage-out regime. However, little work focuses on assessing the extent to which the data-generating process itself is ‘garbage’, that is, the extent to which the data-generating process demonstrates unfairness. We use the term \textit{data fairness} to refer to concerns around fairness in the data-generating process.

Data fairness is often a quality we would like to investigate. For example, a university may want to understand if their admissions process is unfair for a particular group. Specifically, suppose the university uses application materials X to make admissions decisions Y. The university would like to know if decisions Y based on X demonstrate some violation of fairness. The analysis, then, is not a question of algorithmic fairness; there is no algorithm involved. Instead, it is a question of data fairness. Currently, there are few principled approaches to data fairness. A simple approach would be to compare Y across groups. However, this naive approach does not take into account information in X. Another approach is to train a model to predict Y from X, then use existing fairness inference methods. Existing inferential fairness methods, however, focus only on model-level inference without accounting for model uncertainty and therefore are inadequate at answering data-level questions. Some prior work has proposed methods that could be applied at the data-level, but these works largely focus on model-level inference \cite{fairtest} \cite{face}. 

In this paper, we propose using targeted learning to analyze and perform inference on data fairness. The remainder of this paper is structured as follows: In Section \ref{sec:tl}, we give a brief introduction to targeted learning, along with an example. In Section \ref{sec:inference}, we apply targeted learning to data fairness, deriving estimators for fairness metrics and a measure of association. In Section \ref{sec:sims}, we perform simulations to evaluate the properties of our derived estimators, and in Section \ref{sec:data}, we apply our estimators to two real-world datasets. Finally, in Section \ref{sec:discussion}, we discuss the implications of this work along with potential future work. 

\section{Targeted Learning}
\label{sec:tl}

Targeted learning (TL) is a framework for nonparametric inference that leverages modern machine learning methods while still maintaining desirable statistical properties. TL has mostly been used in causal inference, but it can be applied to a wide range of quantities of interest. Here, we give a gentle introduction to TL. This discussion is significantly inspired by \cite{hines} and \cite{modern}, see those references for a fuller introduction. 
\footnote{Here, we refer to all methods that follow the steps above as 'targeted learning'. Within this framework, Targeted Maximum Likelihood Estimation (also referred to as Targeted Minimum Loss-based Estimation) is one approach to constructing an estimator. For more details, see \cite{tl_book}. For a review of related methods, see \cite{tl_review}.}

Setting up and solving an inference problem using TL has four steps:
\begin{enumerate}
    \item Define an estimand
    \item Derive estimand's efficient influence function
    \item Construct an estimator
    \item Perform inference
\end{enumerate}

First, we must define an estimand. In general, an estimand is a mapping $\Psi(\cdot): \mathcal{M} \rightarrow \R$, where $\mathcal{M}$ is the space of distributions we are considering. 
\footnote{In this paper, we focus on the saturated or nonparametric setting. For more discussion regarding the non-saturaed/semi-parametric setting, see \cite{modern}}. 
The definition of 'estimand' is purposely broad to allow for flexibility in defining our object of interest. \cite{tl_book} argue that this flexibility allows for decreased inductive bias, since we can define an estimand that pertains exactly to the quantity we are interested in evaluating. Some common examples of estimands are the population mean $\E_P[X]$ and the conditional mean $\E_P[Y | X = x]$.

After defining our estimand, we need to derive its efficient influence function (EIF). The EIF is a function $\phi(O,P)$ to the real numbers that evaluates the effect of a perturbation $O$ on the value of the estimand $\Psi(P)$. Worded another way, the efficient influence function tells us how our estimand varies as we move across $\mathcal{M}$. We can leverage the EIF to build an efficient estimator, as we will demonstrate later. In the example below, we walk through deriving the EIF. For the population mean, the EIF is $\phi(x, P) = x - \E_P[X] = x - \Psi(P)$. For further intuition on the EIF and its use in estimation, see \cite{teaching_eif}.

Next, we can construct an estimator. We begin by considering a plug-in estimator, $\Psi(\hat{P})$, where $\hat{P}$ is estimated with some nonparametric model with iid data $O_i,\ldots O_n$. Since we do not make any assumptions about the true form of $P$ and since we may not have theoretical guarantees on the behavior $\hat{P}$, $\Psi(\hat{P})$ may be a biased estimate of $\Psi(P)$. Fortunately, the plug-in bias turns out to be exactly $-\frac{1}{n}\sum_{i=1}^n \phi(O_i, \hat{P})$. One way to obtain an unbiased estimator, then, is to set the bias to zero, and solve for the estimand. In the case of the population mean, this simply returns the sample mean, $\frac{1}{n}\sum_{i=1}^n X_i$. This approach to constructing an estimator is the 'estimating equations' approach. Other approaches include bias correction and targeted maximum likelihood estimation. 

Finally, we can go about performing inference. First, we split our data into a training set and an evaluation set. We fit our estimate $\hat{P}$ on the training set, then apply our estimator on the evaluation set. Finally, we quantify uncertainty using Wald-type confidence intervals; a 95\% confidence interval would take the form $\bar{\Psi}(\hat{P}) \pm 1.96 * \frac{\sigma\left(\phi(O,\hat{P})\right)}{\sqrt{n}}$.

Compared to traditional approaches to inference, targeted learning has few requirements and assumptions. In brief, we need to ensure that the error goes to zero at a fast enough rate. To do so, we remove plug-in bias when constructing our estimator and use sample splitting to avoid further bias, as discussed above. We also need to assume that our model learns the true function at a fast enough rate. In real-world problems, we typically fit an ensemble of models in hopes of capturing the true function. Finally, we need independent and identically distributed data, as is typical in inference. For a fuller accounting of the theory behind assumptions and requirements, see Appendix \ref{app:assume}. 


\subsection{An Example}

To illustrate the process of performing inference using TL, we walk through an example relevant to algorithmic fairness. Suppose we have a model $m(x)$ that predicts the probability of some binary outcome such that $m(x) \in [0,1]$. Furthermore, assume we use $m(x)$ to make binary predictions, $m_c(x) = \ind\{m(x) \geq c\}$. We are concerned that $m_c$ may exhibit disparity between groups $G = 0,1$, so we would like to evaluate the difference in the average of $m_c$ between groups. We choose to define our estimand as 
$$\Psi(P) = \Psi_1(P) - \Psi_0(P)$$
where $\Psi_g(P) = \E[m_c(X) | G = g]$. Next, we need to derive the efficient influence function. There are several ways to derive the EIF, but the most straightforward is the 'point mass contamination' approach. In this approach, we evaluate the effect of adding a point mass to $P$ on the value of the estimand. To do so, we define a family of distributions $P_t$ such that the density function takes the form
$$f_t(x,g) = t\ind_{\Tilde{x}, \Tilde{g}}(x,g) + (1-t)f_P(x,g)$$
where $t \in [0,1]$ and $\ind_{\Tilde{x}, \Tilde{g}}\{x,g\}$ is the Dirac delta function for the point $(\Tilde{x},\Tilde{g})$. Let $f_t(x)$ and $f_t(g)$ be similarly defined marginal densities, and let $f_t(x|g)$ be the density of $X|G$. Since our estimand is a sum, we can derive the components of the EIF separately for each part. Moreover, each part of the sum is largely equivalent, so deriving the EIF for $\Psi_1(P)$ also gives us the EIF for $\Psi_0(P)$.

To obtain the EIF for $\Psi_1(P)$, we evaluate the derivative $\frac{d\Psi_1(P_t)}{dt} \Bigr|_{t=0}$. This derivative tells us the rate $\Psi_1$ is changing when $P_t = P$. Intuitively, $\psi(x,g,P)$ is the direction in distribution space along which $Psi$ is most sensitive, allowing us to correct for bias in our estimation. Solving for the derivative, we find:
\begin{equation}
\begin{split}
    \frac{d\Psi_1(P_t)}{dt} \Bigr|_{t=0} & = \frac{d}{dt}\int m_c(x)f_t(x|1)dx \Bigr|_{t=0} \\
    & = \frac{d}{dt}\int m_c(x) \frac{f_t(x,1)}{f_t(1)}dx \\
    & = \int m_c(x) \left[\ind_{\Tilde{x}, \Tilde{g}}(x, 1) - f(x|1)\ind_{\Tilde{g}}(1)\right] dx \\
    & = \frac{\ind_{\Tilde{g}}(1)}{f(1)} \left[m_c(\Tilde{x}) - \Psi_1(P)\right]
\end{split}
\end{equation}
Therefore, our EIF is $\phi_1(x,g,P) = \frac{\ind_{g}(1)}{f(1)} \left[m_c(x) - \Psi_1(P)\right]$. To construct our estimator, we set the sample mean of the EIF to zero and solve for $\Psi_1(P)$. Our resulting estimator is
\begin{equation}
    \bar{\Psi}_1(\hat{P}) = \frac{1}{\frac{1}{n}\sum_{i=1}^n \ind\{g_i=1\}}\frac{1}{n}\sum_{i=1}^n \ind\{g_i=1\} m_c(x_i)
\end{equation}
Essentially, we take the mean of $m_c(x)$ where $G=1$ and weight it by $\Prob(G=1)$. Our estimate of $\Psi(P)$ therefore is $\bar{\Psi}(\hat{P}) = \bar{\Psi}_1(\hat{P}) - \bar{\Psi}_0(\hat{P})$. We then go about performing inference as described above, splitting our data into training and inference sets, fitting $\hat{P}$ on the training set, evaluating our estimator on the inference set, and building a Wald-type confidence interval:  $\bar{\Psi}(\hat{P}) \pm \frac{1.96}{\sqrt{n}} \sum_i \phi(x_i,g_i,\hat{P})^2$.

One last consideration is determining how to learn $\hat{P}$. The example above is simple in that there are no functions to estimate; $m_c(x)$ is a given model, and the empirical distributions of $X$ and $G$ are sufficient for the estimator above. More complex estimators, however, may require fitting models to describe parts of the ditribution. For example, in some estimators we may need an estimate of the conditional mean (e.g. $\E[Y|X=x]$), which will require fitting a regression or classification model. 

\section{Inference for Data Fairness}
\label{sec:inference}

A central benefit of targeted learning is its flexibility; for any inference problem, we begin by defining an estimand that is pertinent to the scientific question at hand. With this increased flexibility, we use targeted learning to build estimators for data fairness. Previous approaches to inference in algorithmic fairness have focused on quantifying uncertainty in the behavior of a specific model. The example above fits within the model-based approach to fairness inference. Model fairness often is the quality of interest, since we may have a specific model at hand that we want to understand. However, the model-based approach also limits us to a specific model, preventing inference on related models or the data itself. Inference on data fairness allows us to draw conclusions about the data as well as the potential implications on model fairness. We take two approaches to using TL for data fairness. First, we use TL to perform inference on fairness metrics at the data-level. Then, we propose investigating conditional associations between sensitive attributes and the outcome of interest. 

\subsection{Fairness Metrics}
Fairness metrics are a common approach to quantifying specific definitions of fairness in predictive models. As such, definitions of fairness metrics inherently condition on a given model. For example, demographic parity is defined as
$$\Prob(\hat{y} = 1 | G = 1) = \Prob(\hat{y} = 1 | G = 0)$$
where $\hat{y}$ is the output of a predictive model and $G$ denotes group membership. 

To move beyond a model-specific definition of fairness, we consider the decisions that would be made under the true distribution of $Y|X$. Let $D(x) = \Prob(Y=1|X=x)$. Specifically, we consider the Bayes optimal decision rule 
\begin{equation}
   D_c(x) = \ind\{D(x) \geq c\} 
\end{equation}
where $Y$ is a binary outcome and $c$ is our threshold, typically $\frac{1}{2}$. Notably, $D_c(x)$ is a property of the data-generating process, not a specific model. Therefore, inferences we make about $D_c(x)$ are about the data rather than a model. In practice, of course, we rarely know the true distribution of $Y|X$, so we must estimate it. TL gives us a framework to reason about how to perform estimation and how our estimation procedure may affect our inference. In this section, we only walk through estimands, EIFs, and estimators for demographic parity. The derivations for equal opportunity can be found in Appendix \ref{app:equal}. 

\subsubsection{Traditional Metrics}

Using the Bayes optimal decision rule $D_c(x)$ defined above, we can redefine demographic parity as 
\begin{equation}
    \E[D_c(X) | G = 1] = \E[D_c(X) | G = 0]
\end{equation}
such that our natural estimand is 
\begin{equation}
    \Psi_{DP}(P) = \E_P[D_c(X)|G=1] - \E_P[D_c(X)|G=0]
\end{equation}
As done in the example above, we can split our estimand into two parts, $\Psi_1(P)$ and $\Psi_0(P)$, and derive the EIF for only one. Doing so, we find that the EIF is
\begin{equation}
    \phi_1(x,g,P) = \frac{\ind\{g=1\}}{\Prob(G=1)}[D_c(x) - \Psi_1(P)]
\end{equation}
We also determine that our estimator takes the form
\begin{equation}
    \bar{\Psi}_1(\hat{P}) = \frac{1}{n*\hat{\Prob}(G=1)} \sum_{i=1}^n \ind\{g_i=1\}\hat{D}_c(x_i)
\end{equation}
where $\hat{\Prob}(G=1)$ is the empirical probability that $G=1$ and $\hat{D}_c(x)$ is a nonparametric estimate of $D_c(x)$. 

\subsubsection{Probabilistic Metrics}

In addition to considering the traditional definitions of demographic parity and equal opportunity, we consider probabilistic versions of these metrics. In brief, instead of looking at the probability that the conditional probability of $Y$ is greater than $c$, we can take the expectation of the conditional probability of $Y$. Probabilistic fairness metrics consequently lead to softer requirements than traditional metrics, and while they are often similar, the two approaches can sometimes result in different conclusions. In practice, we need to carefully consider which estimand is of interest. In particular, do we care about binary classifications or the average conditional probability?

For probabilistic demographic parity, our estimand is 
\begin{equation}
\begin{split}
    \Psi(P) &= \E[D(X) | G=1] - \E[D(X) | G=0] \\
\end{split}   
\end{equation}

so that instead of looking at the Bayes optimal rule, we directly evaluate the conditional distribution, $D(X) = Y|X$. Once again, we split our estimand in two to ease calculation of the EIF. The corresponding EIF is

\begin{equation}
    \phi_1(x,g,y,P) = \frac{1}{P(G=1)} [\pi(x)(\ind_y(1)-D(x)) + 
    \ind_g(1)(f(x) - \Psi_1(P)]
\end{equation}
where $\pi(x) = \Prob (G=1|X=x)$. Finally, we build our estimator, which takes the following form:

\begin{equation}
    \bar{\Psi}_1(\hat{P}) = \frac{1}{n*\hat{\Prob}(G=1)} 
    \sum_{i=1}^n \hat{\pi}(x_i)[\ind\{y_i=1\} - \hat{D}(x_i)] + \ind\{g_i=1\}\hat{D}(x_i)
\end{equation}
Note that the estimator for probabilistic demographic parity requires estimating two functions, $\pi(x)$ and $D(x)$, along with the marginal probability of $G$. 

\subsubsection{Double Robustness}
Taking a closer look at our estimator for probabilistic demographic parity, we find that we only need to estimate either $\pi(x)$ or $D(x)$ well for our estimator to perform well. Specifically, if either $\hat{\pi}(x) = \pi(x)$ or $\hat{D}(x) = D(x)$, then $E[\bar{\Psi}(\hat{P})] \approx \Psi(P)$.
\footnote{Our proof for double robustness can be found in Appendix \ref{app:proof}.}
This quality is referred to as double robustness, since our estimator is robust to misspecification in either model. Even if one model is misspecified, we should still get good estimates for the value of our estimand. Notably, we do not attain double robustness when we apply TL to model fairness, even for probabilistic model fairness. This is due to the fact that, conditional on the training data, $m(x)$ is independent of $P$. Since perturbations to $P$ occur (conceptually) after training, these perturbations do not affect $m(x)$. Consequently, although we need to account for the distribution of $G|X$ in probabilistic data fairness, we do not need to account for $G|X$ in probabilistic model fairness. 

Exploiting double-robustness for data fairness further demonstrates some of the important distinctions between model fairness and data fairness. In particular, data fairness requires additional estimation steps and further consideration of the relationships between $X$, $Y$, and $G$. Additionally, recovering a doubly robust estimator demonstrates one benefit of using TL; we can recover properties we otherwise may not realize we can leverage. 

\subsection{Conditional Associations}
In addition to applying TL to fairness metrics for data fairness, we construct an estimator to evaluate the association between two discrete variables conditioned on a set of other variables. When training a predictive model, there are a variety of concerns we may have about associations between the outcome $Y$ and group status $G$. Equal opportunity and demographic parity speak to a concern of $X$ leaking information about $G$ into our model. It's possible for $Y$ to be independent of $G$, but for $Y|X$ to be associated with $G$. On the other hand, we may notice an unconditional association between $Y$ and $G$, but believe that we can explain away the association by conditioning on $X$. In the latter case, we are seeking to justify our use of a given set of variables by the conditional association they entail. To evaluate conditional association, we propose using conditional mutual information (CMI), defined as
\begin{equation}
    \Psi(P) = \int \int \int p(y,g;x) \log \frac{p(y,g|x)}{p(y|x)p(g|x)}dydgdx
\end{equation}
which measures the KL-divergence between $P_{(Y,G)|X}$ and $P_{Y|X} \times P_{G|X}$. The smaller the value of $\Psi(P) > 0$, the more $X$ explains away the shared information between $Y$ and $G$. We derive our EIF and estimator as follows:

\begin{equation}
    \phi(y,g,x,P) = \log \frac{p(y,g|x)}{p(y|x)p(g|x)} - \Psi(P)
\end{equation}

\begin{equation}
    \bar{\Psi}(\hat{P}) = \frac{1}{n} \sum_{i=1}^n \log \frac{\hat{p}(y_i,g_i|x_i)}{\hat{p}(y_i|x_i)\hat{p}(g_i|x_i)}
\end{equation}

The form of our estimator leads to several practical constraints. Here, we decide to limit ourselves to discrete $Y$ and $G$, since estimating the conditional distribution of a continuous random variable is non-trivial. A classification algorithm is appropriate for the discrete case, and classification allows us to consider arbitrary $X$. To fit a plug-in, we need to learn the distributions of $(Y,G)|X$, $Y|X$, and $G|X$. We can take one of two modeling approaches, which we refer to as the single approach and the separate approach. In the single approach, we only estimate the distribution of $(Y,G)|X$. To obtain $Y|X$ and $G|X$, we simply marginalize out $G$ and $Y$, respectively. Since $Y$ and $G$ are discrete, this is straightforward, for example:
$$\Prob(Y=y|X=x) = \sum_{g}\Prob(Y=y,G=g|X=x)$$
In the separate approach, we fit separate models for each of $(Y,G)|X$, $Y|X$, and $G|X$. We consider both approaches in Section \ref{sec:sims}. In both the single and separate approaches, we calibrate the learned classifier(s) to maintain the properties of a true conditional distribution.

\section{Simulations}
\label{sec:sims}

To evaluate our approach to data fairness and some of the properties of our estimators, we perform several simulations. In particular, we investigate how coverage, error, and robustness interact with sample size. Our simulations largely follow the same template. We begin by specifying a data-generating process, then generate data accordingly. After the data are generated, we create an even train/test split to fit our models and perform inference. For each simulated dataset, we generate a 95\% Wald-type confidence interval with $\bar{\Psi}(\hat{P}) \pm 1.96 * \sigma\left(\phi(O,\hat{P})\right)/\sqrt{n}$. 

\subsection{Parity and Opportunity}

We design three simulation settings to assess our estimators for parity and opportunity. 

\begin{figure}
    \centering
    \includegraphics[width=0.7\linewidth]{figs/asymptotic.png}
    \caption{Plots for estimating demographic parity in simulation Setting 1. Each point represents an estimate from a simulated dataset. In both plots, the X-axis is sample size and the Y-axis is the estimated demographic parity. The dotted line represents the true value of demographic parity for the distribution. For the lower plot, the bands around each point represent the 95\% confidence interval.}
    \label{fig:asymptotic}
\end{figure}

In Setting 1, we evaluate our estimators in a scenario where $X \centernot \independent G$, implying some disparity and unequal opportunity in $Y|X$. We draw $X \in \R^5$ from a zero-mean multivariate normal distribution with $Cov(X_2, X_3) = - Cov(X_4, X_5) = 0.5$. We draw $G$ as a Bernoulli with $p=\frac{1}{2}$, and we add $G/2$ to $X_2$ and subtract $G/2$ from $X_5$. We then generate $Y|X$ with a logistic regression with interaction terms, specifically, 
$$\Prob(Y=1|X) = \mbox{logit}(-2X_1 +3X_2-4X_3 +3X_4-X_5+2X_2X_5 + X_3X_4).$$ 

In Setting 2, we evaluate our estimators when $X \independent G$. We draw $X$ and $G$ similarly as above, and $Y|X$ is generated with a logistic regression, that is $\Prob(Y=1|X) = \mbox{logit}(\beta^TX)$. However, the coefficients vary such that $\Prob(Y=1|X)$ is focused at the extremes when $G=1$, but unimodal when $G=0$. Specifically, when $G=0$, $\beta = 0.5 * \overrightarrow{\ind}$, and when $G=1$, $\beta = 2 * \overrightarrow{\ind}$. Nonetheless, $\Prob(Y=1|X)$ is centered at $\frac{1}{2}$ regardless of $G$, so we have parity and equal opportunity. 

In both Settings 1 and 2, we find that our estimators are conservative and have desirable error properties. Results for estimating demographic parity in Setting 1 can be found in Figure \ref{fig:asymptotic}. We maintain >95\% coverage across sample sizes, and error becomes negligible with a few thousand samples. Our simulations also reveal some of the similarities and differences between traditional and probabilistic fairness metrics. In our simulations, both types tended to have roughly the same value, but probabilistic metrics tended to have lower variance. This is largely due to the fact that probabilistic metrics operate on continuous values, while traditional metrics operate on binary values. 

Setting 3 assesses the double robustness of probabilistic fairness estimators. We generate $X$ as a multivariate normal as before, but we then generate $Y$ and $G$ as logistic regressions on the squared values of $X$. In particular, we define the following:
$$\Prob(Y=1|X) =\mbox{logit}(4X_1^2 + 2X_2^2 + X_3^2 - 3X_4^2 - 4X_5^2)$$
$$\Prob(G=1|X) =\mbox{logit}(X_1^2 + X_2^2 - X_3^2 - 2X_4^2 + X_5^2)$$
Consequently, a logistic model linear in $X$ is misspecified. To evaluate robustness, we fit either a logistic or gradient boosting model for $\Prob(Y|X)$ and $\Prob(G|X)$. Our setup results in four modeling scenarios: well-specified (both boosting), $\Prob(Y|X)$ misspecified, $\Prob(G|X)$ misspecified, and both misspecified. As seen in Figure \ref{fig:robust}, even when either model is misspecified, we achieve desirable coverage across sample sizes. 

\begin{figure}
    \centering
    \includegraphics[width=0.5\linewidth]{figs/robust_coverage.png}
    \caption{Heatmap demonstrating coverage in different scenarios and as sample size varies. For each cell in the heatmap, coverage is calculated over 100 simulations.}
    \label{fig:robust}
\end{figure}

\subsubsection{A Model-Based Estimate}

In addition to investigating properties of our estimators, we compare TL estimates to a naive model-based approach. If we only cared to perform model-based fairness inference, we may use a simple difference-in-means (i.e. mean prediction) t-test between groups. This approach treats each prediction as an iid sample from the distribution of predictions for each group. When we perform this t-test, we find that the point estimates are nearly the same as our data-based estimates, but the variances differ greatly. Specifically, TL estimates of variance are significantly larger than the t-test standard errors. This is desirable since we not only need to estimate the mean of a variable, but the conditional distribution as well. Here, TL estimates compensate for the estimation procedure and small sample bias that may occur.

\begin{figure}
    \centering
    \includegraphics[width=0.5\linewidth]{figs/naive_var.png}
    \caption{Line plot comparing targeted learning estimate of variance to a t-test estimate of variance.}
    \label{fig:naive}
\end{figure}

\subsection{CMI}

To assess our estimator for CMI, we perform a simulation that allows us to vary the conditional dependence between $Y$ and $G$. Additionally, we compare and contrast our TL estimator to a KNN estimator for CMI \cite{knncmi}. 

In our simulations, $X \in \R^3$ is simulated from a multivariate normal. $Y = (c*S + U + f_\beta(Z))/(c+2)$ and $G = (c*S + V + f_\beta(Z))/(c+2)$, where $c$ is a scalar weight, $S, U, V$ are uniform distributions, and $f_\beta(Z) =\mbox{logit}(\beta^TZ)$. The larger the value of $c$, the larger the CMI. For each value of $c$, we obtain a Monte Carlo estimate for the true CMI; these estimates can be found in Appendix \ref{app:cmi}. In our simulations, we vary both the sample size and $c$, and we estimate CMI using three methods: the KNN estimator (KNN), TL with the single approach (TL), and TL with separate approach (TL-sep). 

Figure \ref{fig:cmi} shows our results. First, we find that all estimators are biased, but this bias shifts depending on sample size and the value of $c$. The TL estimator performs uniformly better than TL-sep, so we only discuss the TL and KNN estimators moving forward. Generally, there is a monotonic relationship between $c$ and the bias. For TL, the bias approaches zero as $c$ increases but rarely exceeds zero across sample sizes. For KNN, the bias surpasses zero for smaller sample sizes, leading to a positive bias for larger values of $c$. The bias in these estimators for smaller values of $c$ can be explained somewhat by the true value of CMI. For example, when $c=0$ (i.e. when $X$ determines any dependence between $Y$ and $G$), the CMI is 0.0598. Both the TL and KNN estimators, however, estimate the CMI to be 0. While the TL and KNN estimates of CMI speak to our qualitative understanding of the dependence structure, the true CMI value diverges slightly from our expectations. With these simulation results in mind, it appears that our estimates of CMI can still be informative of the dependence structure, even though the estimates are biased. 

Next, we look at the coverage of the TL estimator. We find that coverage decreases as sample size increases and as $c$ decreases. In fact, coverage is zero for a significant portion of the simulation study space. Looking more closely, we find that poor coverage is due to bias and small sample variance when our estimates are close to zero. As the bias decreases and as our estimates move away from zero, coverage improves. Nonetheless, the poor coverage of our TL estimator makes it unsuitable for inference on CMI. 

Overall, we find that the TL estimator for CMI is biased, particularly when CMI is close to zero, and even for large sample sizes. Additionally, coverage is extremely poor, especially when $c$ is small and the sample size is large. However, our CMI estimator may be informative of the relationship between $X$, $Y$, and $G$, and performs similarly to or better than existing approaches to estimating CMI. 

\begin{figure}
    \centering
    \includegraphics[width=0.45\linewidth]{figs/cmi_error.png}
    \includegraphics[width=0.5\linewidth]{figs/cmi_coverage.png}
    \caption{Line plot and heatmap demonstrating error and coverage results for CMI. In the line plots, the X-axis is the value of c and the Y-axis is the error. The solid horizontal line represents and error of 0. For every combination of (c,estimator type, sample size), we perform 100 simulations. The heatmap shows coverage for the TL estimator as c and sample size varies.}
    \label{fig:cmi}
\end{figure}


\section{Data Analysis}
\label{sec:data}

We show how our approach to data fairness can be used in practice by applying our estimators to real data. We analyze the Adult-Income dataset and the Law School dataset, two datasets common in the fairness literature, and discuss the interpretations and implications of our results from a data fairness perspective. To perform our analysis, we split the data into a 60/40 train/test split, and train SuperLearners for our plug-in distributions. For both datasets, our analysis seeks to determine to what extent the distribution of $Y$ given $X$ violates definitions of fairness.

In addition to inferring the value of each metric, we construct a measure of variable importance to the value of each metric. In particular, we calculate a type of Shapley value to approximate the marginal contribution of a variable to the value of a given metric. To do so, we randomly sample permutations of the order of variables, and add each variable sequentially, evaluating the metric at each step. We then average the change for each variable across permutations. The variable importance measure provides some insight into which factors are most impactful to the metric value, and in practice variable importance may be useful to guide further inquiry or intervention. 

\begin{table}
    \centering
    \begin{tabular}{| l || c | c |}
    \hline
       Metric & Adult & Law\\
       \hline
       Parity  & 0.17 (0.16, 0.18) & 0.19 (0.14, 0.25)\\
       Prob. Parity  & 0.18 (0.17, 0.19) & 0.19 (0.14, 0.24)\\
       \hline
       Eq. Opp.  & 0.11 (-0.13, 0.36) & 0.10 (0.02, 0.17)\\
       Prob. Eq. Opp.  & 0.08 (0.03, 0.14) & 0.12 (0.06, 0.18)\\
       \hline
       CMI & -0.00 (-0.00, -0.00) & -0.00 (-0.00, -0.00)\\
       \hline
    \end{tabular}
    \caption{Inferential results from data analysis. Confidence intervals are contained in parentheses. Confidence level is 95\% for all analyses.}
    \label{tab:data_results}
\end{table}

\subsection{Adult-Income}

The Adult-Income dataset contains economic and demographic information on individuals living in the United States \cite{retiring} \cite{adult}. For our analysis, the outcome is $Y = \ind\{income > 50,000\}$ and $G = \ind\{is \ male\}$.

We find that both parity and probabilistic parity are violated, with confidence intervals bounded away from zero. Looking into the variable importance for these metrics, we can see that marital-status and relationship have the most significant impact on parity. These variables are highly related, so their mutual presence may potentially obscure a stronger effect. When we look into differences in income by relationship status, married individuals have significantly higher incomes than unmarried individuals. This discrepancy is due significantly to joint incomes rather than higher personal incomes. Additionally, we see that the probabilistic disparity is driven almost entirely by relationship/marital-status. The increased importance of these variables is likely due to certain values fully determining $G$ (e.g. 'Husband' implies $G=1$). Looking at the remaining features, we see that all variables increase disparity. 

Our inferences for equal opportunity have higher uncertainty than our results for parity. Both point estimates are positive, but only the confidence interval for probabilistic equal opportunity is bounded away from zero. Looking to the variable importance measures, we can see that some variables increase inequality (hours-per-week, workclass, marital-status) while others decrease it (capital-gain, capital-loss, education). The directionality of each variable importance is largely the same across both versions of equal opportunity. Finally, we find that income and sex are conditionally independent. Again, relationship status has the largest importance score. 

\subsection{Law School}

The Law School dataset contains educational, demographic, and economic information about law students \cite{lsac} \cite{data_review}. Here, our outcome of interest is $Y=\ind\{passed \ the \ bar\}$ and $G = \ind\{is \ white\}$. 

Our inferences for the Law School dataset are largely similar to our inferences for the Adult dataset, so we focus on conclusions that deviate. First, we notice that both versions of equal opportunity have confidence intervals bounded away from zero, and unlike the Adult dataset, the confidence intervals have similar lengths. This results from the difference in the rate of $Y=1$ across datasets (0.24 vs 0.89 in Adult and Law, respectively). Therefore, conditioning on $Y=1$ results in a much smaller sample in the Adult dataset. The variable importance measures indicate that LSAT score impacts metric values the most. Some remaining variables are strongly related, such as zfygpa (first-year GPA) and zgpa (cumulative GPA), so their importance may be somewhat underestimated. 

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figs/importance.png}
    \caption{Feature importance to fairness scores for both the Adult and Law school datasets. The first row contains feature importances for Adult, and the second row contains feature importances for Law school.}
    \label{fig:importance}
\end{figure}

\section{Discussion}
\label{sec:discussion}

In this paper, we propose using targeted learning to perform inference on data fairness. We derive estimators for demographic parity, equal opportunity, and conditional mutual information and discuss some desirable properties of these estimators. We conduct simulations that demonstrate the efficacy of our estimators for demographic parity and equal opportunity, but show some limitations with estimating CMI. We also demonstrate the application of our estimators on real-world data. 

As discussed previous, data fairness and algorithmic fairness are distinct, though related, concepts. The question remains, then, when should we perform inference on one as opposed to the other? There are some situations that are fairly straightforward. For example, if we care about the properties of a model, algorithmic fairness is the appropriate approach. However, there are edge cases that are less clear. For example, consider a human-in-the-loop decision-making system where a model makes a recommendation for the decision, but a human ultimately makes the final decision. Arguably, both algorithmic and data fairness are relevant in this case, and the choice between the two depends on the context and desired level of intervention. 

\subsection{Causal Inference for Data Fairness}

Though we have not addressed it directly, causality is an undertone throughout this paper. Our methodology is borrowed from causal inference, and the ideal data fairness claim is a causal one: $X$ causes unfair outcomes $Y$ in attribute $G$. However, in the context of fairness, determining causality is a difficult goal. To clarify our concerns, we first describe the potential outcome framework for causal inference, the approach used most often in statistics \cite{causal}. In the potential outcome framework, causal inference is framed as a sort of missing data problem, where, for a given individual, the missing datum is the outcome we would have observed if the individual had an alternative attribute (or treatment). To perform causal inference, then, we need to be able to imagine a world where an individual can feasibly attain each of the attributes of interest, without affecting other features. In a clinical trial, we can randomly assign some individuals to a new medication, so every individual feasibly could have received the treatment. In fairness, however, it is difficult, if not impossible, to imagine a world where a sensitive attribute (race, religion, gender, etc.) is an assignable quality divorced from an individual's lived experiences. 

Nevertheless, there are paths forward. Prior work has demonstrated the multitude of mediating pathways by which a sensitive attribute may affect a given outcome. Instead of performing causal inference on the attribute itself, it may be more reasonable to infer the causal effect of a given mediator. For example, race itself is immutable, but the perception of race can in some cases be intervened upon. Future work on causal inference in data fairness may benefit from causal inference approaches used in economics, sociology, and other fields \cite{shroud}. 

\subsection{Limitations and Future Work}

Our work has several important limitations that connect with potential future work. First, the fairness metrics we consider (demographic parity and equal opportunity) both operate on binary group status and binary outcomes. Although binary metrics are common in algorithmic fairness, focusing only on binary group status is limiting. Many attributes of interest have multiple categories or are continuous, and our estimators cannot accommodate these scenarios. Further work should develop tools for inference on data fairness that allow categorical and continuous attributes and outcomes. 

Next, as mentioned in Section \ref{sec:sims}, our estimator for CMI does not have desirable coverage or error properties for inference, but it appears that it can be informative of the dependence structure in a dataset. Since the focus of this paper is targeted learning applied to data fairness, we primarily considered inference rather than hypothesis testing. Given the properties of both the TL and KNN estimator, hypothesis testing may be a more appropriate approach for assessing conditional independence. Further work should explore hypothesis testing in the context of data fairness, particularly considering the tradeoffs between binary assessments of evidence and estimation. 

Finally, our methodology inherently assumes that fairness metrics are a desirable quantity to perform inference on. However, as prior work has discussed, satisfying a mathematical definition of fairness is a narrow and small success in the larger context in which decisions are being made. Data fairness should be one consideration among many others in decision-making evaluations. Moreover, though we provide a method for assessing data fairness, we do not provide any guidance on correcting for unfair decisions. The necessary changes to achieve data fairness will vary widely between situations, but future work should propose approaches to remedying data fairness violations. 

\bibliographystyle{plain}
\bibliography{refs}

\newpage
\appendix

\section{CMI Simulation}
\label{app:cmi}
\begin{table}[h]
    \centering
    \begin{tabular}{c|c}
        c & CMI \\ \hline
        0 & 0.0598 \\ 
        0.5 & 0.0735 \\ 
        1 & 0.1109 \\ 
        1.5 & 0.1712 \\ 
        2 & 0.2459 \\  
        2.5 & 0.3005 \\ 
        3 & 0.3443 \\ 
        3.5 & 0.3787 \\ 
        4 & 0.4063 \\ 
    \end{tabular}
    \caption{MC estimates for CMI for each value of c}
\end{table}

% \begin{table}[h]
%     \centering
%     \begin{tabular}{|c|c|c|c|c|c|c|c|c|c|}
%         \hline
%         c & 0 & 0.5 & 1 & 1.5 & 2 & 2.5 & 3 & 3.5 & 4\\ \hline
%         CMI & 0.0598 & 0.0735 & 0.1109 & 0.1712 & 0.2459 & 0.3005 & 0.3443 & 0.3787 & 0.4063 \\ \hline
%     \end{tabular}
%     \caption{Caption}
%     \label{tab:my_label}
% \end{table}


\section{Derivations for Equal Opportunity}
\label{app:equal}

For equal opportunity, 
\begin{equation}
    \Psi(P) = \E[D_c(X)|Y=1,G=1] - \E[D_c(X)|Y=1,G=0] 
\end{equation}

\begin{equation}
    \phi_1(x,g,y,P) = \frac{\ind_{y,g}(1,1)}{\Prob(Y=1,G=1)}(D_c(x) - \Psi(P))
\end{equation}

\begin{equation}
    \bar{\Psi}_1(\hat{P}) = \frac{1}{n*\hat{\Prob}(Y=1,G=1)} 
    \sum_{i=1}^n \ind_{y_i,g_i}(1,1) \hat{D}(x_i)
\end{equation}

For probabilistic equal opportunity,
\begin{equation}
    \Psi(P) = \E[D(X) | Y=1,G=1] - \E[D(X)|Y=1,G=1]
\end{equation}

\begin{equation}
    \phi_1(x,g,y,P) = \frac{1}{\Prob(Y=1,G=1)} [\rho_1(x)(\ind_y(1) - D(x)) + \ind_{y,g}(1,1)(D(x) - \Psi(P)]
\end{equation}
where $\rho_g(x) = \Prob(Y=1,G=g|X=x)$

\begin{equation}
    \bar{\Psi}_1(\hat{P}) = \frac{1}{n*\hat{\Prob}(Y=1,G=1)} 
    \sum_{i=1}^n \hat{\rho}_1(x_i)[\ind\{y_i=1\}-\hat{D}(x_i)] + \ind_{y_i,g_i}(1,1)\hat{D}(x_i)
\end{equation}

\section{Double Robustness Proof}
\label{app:proof}

We present a proof for double robustness in our estimator for probabilistic demographic parity. We only focus on the first summand from the estimator (i.e. $\bar{\Psi} _1(\hat{P})$ ), since the proof is the same for the second summand. Additionally, this proof can be extended to our estimator for probabilistic equal opportunity.

First, we define $\hat{p} = \frac{1}{n}\sum_{i=1}^n\ind\{g_i=1\}$ and $p = \Prob(G=1)$. Our estimand is
$$\Psi_1(P) = \E[D(X) | G=1]$$
Our estimator, as defined in Section \ref{sec:inference}, is
$$\bar{\Psi}_1(\hat{P}) = \frac{1}{n*\hat{p}}\sum_{i=1}^n\hat{\pi}(x_i)\ind\{y_i=1\} - \hat{\pi}(x_i)\hat{D}(x_i) + \ind\{g_i=1\}\hat{D}(x_i)$$
Next, we take the expectation and apply the tower property.
\begin{align}
    \E[\bar{\Psi}_1(\hat{P})] & = \E\left[\frac{1}{\hat{p}}\left(\hat{\pi}(X)\ind\{Y=1\} - \hat{\pi}(X)\hat{D}(X) + \ind\{G=1\}\hat{D}(X) \right)\right] \\
    & = \E\left[\E\left[\frac{1}{\hat{p}}\left(\hat{\pi}(X)\ind\{Y=1\} - \hat{\pi}(X)\hat{D}(X) + \ind\{G=1\}\hat{D}(X) \right) |X\right]\right] \\
    & = \E\left[\frac{1}{\hat{p}}\left(\hat{\pi}(X)D(X) - \hat{\pi}(X)\hat{D}(X) + \pi(X)\hat{D}(X) \right)\right]
\end{align}

Therefore, if either $\hat{\pi}(X) = \pi(X)$ or $\hat{D}(X) = D(X)$, then we have
\begin{align}
    \E[\bar{\Psi}_1(\hat{P})] &= \E\left[\frac{1}{\hat{p}}\pi(X)D(X) \right] \\
    & = \E\left[\frac{1}{p}\pi(X)D(X) \right] + \E\left[\left(\frac{1}{\hat{p}} -\frac{1}{p}\right)\pi(X)D(X) \right] \\
    & = \E\left[\frac{\Prob(G=1|X)\Prob(Y=1|X)}{\Prob(G=1)} \right] + \E\left[\left(\frac{1}{\hat{p}} -\frac{1}{p}\right)\pi(X)D(X) \right]\\
    & = \Psi_1(P) + \E\left[\left(\frac{1}{\hat{p}} -\frac{1}{p}\right)\pi(X)D(X) \right]
\end{align}
Finally, we evaluate the absolute error between $\E[\bar{\Psi}_1(\hat{P})]$ and $\Psi_1(P)$. We note that $max_x\{\pi(x)D(x)\} =1$
\begin{align}
    | \E[\bar{\Psi}_1(\hat{P})] - \Psi_1(P) | & = \bigg| \E\left[\left(\frac{1}{\hat{p}} -\frac{1}{p}\right)\pi(X)D(X) \right] \bigg| \\
    & \leq \bigg| \E\left[\frac{1}{\hat{p}} -\frac{1}{p} \right] \bigg|\\
    & = \bigg| \E\left[\frac{1}{\hat{p}} \right] - \frac{1}{p} \bigg|\\
    & = 0
\end{align}

\section{Targeted Learning Requirements and Assumptions}
\label{app:assume}

Although TL requires minimal assumptions, especially compared to classical inference methods, it is important to verify that we believe the relevant assumptions and meet the relevant requirements. We introduce some notation to simplify our discussion. 

The error in our estimate can be shown to be 
$$[ \Psi(\hat{P}) - \Psi(P)] = \Prob_n[\phi(O,P)] - \Prob_n[\phi(O,\hat{P})] + (\Prob_n - P)[\phi(O,\hat{P}) - \phi(O,P)] + R$$
where $\Prob_n$ is the empirical distribution of our data and 
$$R = [\Psi(\hat{P}) - \Psi(P)] + P[\phi(O,\hat{P})]$$

To use the central limit theorem for inference, we require that $[ \Psi(\hat{P}) - \Psi(P)] = \Prob_n[\phi(O,P)] + o_P(n^{-1/2})$. Therefore, the proceeding terms must each be at least $o_P(n^{-1/2})$; our further assumptions and requirements evolve from our goal to meet this rate. 

The plug-in bias $- \Prob_n[\phi(O,\hat{P})]$ is straightfoward to address. As mentioned and demonstrated above, we can use the esimtating equations approach (or any other TL bias elimination method) to 'zero out' the plug-in bias in our estimator. The plug-in bias then is equal to zero, so we meet the desired rate. 

The empirical process term $(\Prob_n - P)[\phi(O,\hat{P}) - \phi(O,P)]$ is similarly simple to deal with. We start by requiring that our estimate of the EIF is $\mathcal{L}_2$-consistent, that is 
$$\|\phi(O,\hat{P}) - \phi(O,P)\| \xrightarrow{P} 0$$
This requirement is easy to meet, since most machine learning methods are $\mathcal{L}_2$-consistent. Additionally, we need to control the bias involved in estimating $\hat{P}$ and performing inference on the same data. We can either only consider fitting functions that are 'simple enough' (i.e. Donsker) or simply decouple our estimation and inference by sample splitting. In practice, we more often choose the latter to avoid restricting our model space. By attaining $\mathcal{L}_2$-consistency and sample splitting, the empirical process term is $o_P(n^{-1/2})$.

Finally, we need to deal with the remainder term  $R$. Unfortunately, there is no way to bound the rate of $R$ in general, so the best we can do is require that $R$ is $o_P(n^{-1/2})$. This requirement takes different forms depending on the estimand at hand. For example, in causal inference, the doubly robust estimator involves training both an outcome model and a propensity score model. To achieve $R = o_P(n^{-1/2})$, the product of the convergence rates of these models must be $o_P(n^{-1/2})$ which implies that the models should individually achieve a rate of $o_P(n^{-1/4})$. Though this rate is slower, few models attain convergence rates near $o_P(n^{-1/4})$ in general. Fortunately, many common machine learning models can attain a $o_P(n^{-1/4})$ in specific circumstances. Consequently, one common approach is to fit an ensemble of models and hope that some model in the ensemble captures the true function well. Recently, the highly adaptive Lasso has become a more theoretically rigorous option, since it achieves a $o_P(n^{-1/4})$ convergence rate. However, the highly adaptive lasso can be computationally expensive, generating up to $n \cdot 2 ^{d-1}$ features before applying a lasso regression. 


\end{document}
