% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[review]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}

\usepackage{enumitem}
\usepackage{booktabs}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{tabularx}
\usepackage{multirow}
\usepackage{titling}
\usepackage{enumitem}

\usepackage[table,xcdraw]{xcolor}
  


\usepackage{array} % For defining custom column types
\usepackage{siunitx}
\usepackage{dcolumn}


\sisetup{
  table-space-text-post = ***,
}

\newcolumntype{L}{D{.}{.}{-3}}

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{When People are `Floods': Analyzing Dehumanizing Metaphors in Immigration Discourse with Large Language Models}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a separate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

\author{First Author \\
  Affiliation / Address line 1 \\
  Affiliation / Address line 2 \\
  Affiliation / Address line 3 \\
  \texttt{email@domain} \\\And
  Second Author \\
  Affiliation / Address line 1 \\
  Affiliation / Address line 2 \\
  Affiliation / Address line 3 \\
  \texttt{email@domain} \\}

%\author{
%  \textbf{First Author\textsuperscript{1}},
%  \textbf{Second Author\textsuperscript{1,2}},
%  \textbf{Third T. Author\textsuperscript{1}},
%  \textbf{Fourth Author\textsuperscript{1}},
%\\
%  \textbf{Fifth Author\textsuperscript{1,2}},
%  \textbf{Sixth Author\textsuperscript{1}},
%  \textbf{Seventh Author\textsuperscript{1}},
%  \textbf{Eighth Author \textsuperscript{1,2,3,4}},
%\\
%  \textbf{Ninth Author\textsuperscript{1}},
%  \textbf{Tenth Author\textsuperscript{1}},
%  \textbf{Eleventh E. Author\textsuperscript{1,2,3,4,5}},
%  \textbf{Twelfth Author\textsuperscript{1}},
%\\
%  \textbf{Thirteenth Author\textsuperscript{3}},
%  \textbf{Fourteenth F. Author\textsuperscript{2,4}},
%  \textbf{Fifteenth Author\textsuperscript{1}},
%  \textbf{Sixteenth Author\textsuperscript{1}},
%\\
%  \textbf{Seventeenth S. Author\textsuperscript{4,5}},
%  \textbf{Eighteenth Author\textsuperscript{3,4}},
%  \textbf{Nineteenth N. Author\textsuperscript{2,5}},
%  \textbf{Twentieth Author\textsuperscript{1}}
%\\
%\\
%  \textsuperscript{1}Affiliation 1,
%  \textsuperscript{2}Affiliation 2,
%  \textsuperscript{3}Affiliation 3,
%  \textsuperscript{4}Affiliation 4,
%  \textsuperscript{5}Affiliation 5
%\\
%  \small{
%    \textbf{Correspondence:} \href{mailto:email@domain}{email@domain}
%  }
%}

\begin{document}
\maketitle
\begin{abstract}
Metaphor is abundant in politics, and can shape public opinion and policy preferences. We develop a computational approach to measure metaphorical language, focusing on immigration discourse on social media. Grounded in qualitative social science research, we first identify seven concepts evoked in immigration discourse (such as \textsc{water} or \textsc{vermin}). We propose and evaluate a novel technique that leverages both word-level and document-level signals to measure metaphoricity with respect to these concepts. We then study the relationship between metaphor, political ideology, and user engagement in 200K U.S. tweets about immigration. Conservatives tend to use dehumanizing metaphors more than liberals, but this effect varies widely across concepts. Moreover, metaphor is associated with higher retweet counts, especially for tweets written by liberals.
Our work highlights the potential for computational methods to complement qualitative approaches in understanding subtle and implicit language in political discourse.




\end{abstract}

\section{Introduction}

Metaphor, communication and perception of one concept in terms of another, is abundant in political discourse. Deeply entrenched in culture and human cognition, metaphor structures how we understand the world around us \citep{lakoff1980metaphors}. In politics, metaphors are deployed consciously and subconsciously to structure our understanding of complex concepts in terms of more accessible everyday concepts \citep{burgers_figurative_2016}. Metaphors create conceptual mappings which emphasize some aspects of political issues while hiding others \citep{lakoff1980metaphors}, through which they can affect public attitudes and policy preference \citep{boeynaems_effects_2017}. 

Political metaphor research primarily focuses on politicians' speeches and mainstream news media \citep{charteris-black_britain_2006}. While social media is a popular source of political information across the globe, little is known about how ordinary people use metaphor and are affected by metaphor exposure on social media. We develop a computational approach for measuring and analyzing metaphor in immigration discourse on social media, specifically Twitter. We then use this methodology to study the relationship between immigration metaphors, political ideology, and user engagement.


Based on prior work on immigration, we first identify a set of seven relevant \textit{source domains}, the other concepts evoked by metaphor in discussions of immigration, such as \textsc{water} or \textsc{vermin}. These metaphors are dehumanizing, as they involve conceptual mappings to non-human entities. We then develop a method for measuring a social media post's metaphoricity with respect to these source domain concepts. Our method leverages both word-level and document-level signals to measure metaphor. Specifically, we use large language models (LLMs) to detect metaphorical words in conjunction with document embeddings to detect metaphorical associations in context. We rigorously evaluate this methodology on a new crowdsourced dataset of tweet pairs compared for metaphoricity with respect to each source domain concept. While we focus on immigration in the US, our methods require minimal manual annotation and can be readily applied to study metaphor in other political and cultural contexts. 


We use these methods to analyze metaphor in a dataset of 200K U.S. tweets about immigration with a specific focus on the relationship between metaphor and political ideology and ideology strength. While we find that conservatives tend to use dehumanizing metaphors more than liberals, this effect varies widely across concepts. As we find that liberals use more dehumanizing metaphors than expected, we additionally conduct a qualitative analysis highlighting various contexts in which liberals use such metaphors.

Finally, we leverage signals from ``favoriting'' and ``retweeting'' interactive behaviors to quantify relationships between metaphorical framing and user engagement. We observe null-to-negative effects of metaphor on the number of favorites. However, we find positive effects on the number of retweets. Interestingly, the effect on retweets is primarily driven by liberals. Taken together, our results suggest that even though liberals use dehumanizing metaphors of immigration less frequently, they are more susceptible to their effects. 


\section{Metaphorical Framing and Immigration}


% According to Conceptual Metaphor Theory, ``the essence of metaphor is understanding and experiencing one thing [concept] in terms of another'' \cite[p.~5]{lakoff1980metaphors}. Metaphor is not merely a superficial rhetorical device, but rather operates at a deeper cognitive level by structuring how we think about both everyday and complex concepts. Despite coming from a different disciplinary tradition, \cite{lakoff1980metaphors} offer a view of metaphor that is strikingly compatible with both the sociological conceptualization of framing \citep{goffman1974frame}, discussing how metaphors ``structure how we perceive, how we think, and what we do'' (p.4). Aligned with the political communication perspective on framing as selective emphasis \citep{Entman1993}, \cite{lakoff1980metaphors} describe how metaphors highlight certain aspects of concepts while downplaying or hiding others. Metaphor can also play a major role in politics: metaphors enable people to understand complex political issues in terms of more concrete everyday experiences \citep{burgers_figurative_2016}. Moreover, the conceptual mappings created and reinforced by metaphor establishes pathways for policy recommendations and political action \citep{lakoff1980metaphors}. \cite{burgers_figurative_2016} further establish how metaphors can fulfill the functions of framing defined by \cite{Entman1993}: problem definition, causal interpretation, moral evaluation, and treatment recommendation, and is well-aligned with the process model of framing \citep{Scheufele1999}. As an example, \cite{burgers_figurative_2016} demonstrate how the \textsc{natural disaster} metaphor of immigration \citep{charteris-black_britain_2006} accomplishes all framing functions: it leads to the interpretation that immigration is bad, immigrants cause harm, and immigration is difficult to control and thus requires even harsher restrictions. 

Scholars continue to disagree about the nature and strength of metaphorical framing effects. Much of this disagreement is due to the wide range of methodological traditions brought to metaphorical frame analysis \citep{boeynaems_effects_2017}. Critical discourse approaches focus on the relationship between natural discourse (e.g. metaphor use in actual newspapers) and real-world occurrences \citep{charteris-black_britain_2006}. Critical discourse approaches assert that discourse is not just shaped by society, but also plays an active role in constructing ideology and social realities; from this lens, metaphorical framing inherently has strong effects on social and political systems \citep{boeynaems_effects_2017}. In contrast, response-elicitation approaches, typically experiments, focus on quantifying the effect of metaphor exposure on individuals' cognition, attitudes, emotions, and opinions. Response-elicitation studies have shown mixed results \citep{boeynaems_effects_2017}. For example, \citet{blumenau_variable_2024} experimentally measure the effects of different rhetorical elements on participants' perceptions of message persuasiveness, and find that metaphor-based arguments are less persuasive on average compared to other rhetorical appeals.

This equivocality is exemplified in replication attempts of a well-known study by \cite{thibodeau_metaphors_2011}. In the original study, \cite{thibodeau_metaphors_2011} experimentally manipulate whether a news vignette discusses crime with \textsc{beast} or \textsc{virus} metaphors, and show that the \textsc{beast} metaphor increases support for harsher policies such as police enforcement. However, \cite{steen_when_2014} find that these effects do not replicate when adding a non-metaphorical control and a pre-exposure measure of political preference. Using the same scenario, \cite{reijnierse_how_2015} conduct two experiments with conflicting results. Echoing \cite{mio_metaphor_1997}, \cite{steen_when_2014} argues that we ought to not consider \textit{whether or not} metaphorical framing effects occur, but rather \textit{under what conditions} do they occur. Indeed, prior work has shown that metaphorical framing effects are moderated by many individual-level and contextual factors, such as content ambiguity, source domain (the concept evoked by the metaphor), political orientation, message source, and personality traits \citep{bosman_persuasive_1987,robins_metaphor_2000,kalmoe_fueling_2014,kalmoe_mobilizing_2019,panzeri_does_2021}.




\subsection{Immigration Metaphors and Political Ideology}

There is a large body of critical discourse scholarship focused on the metaphorical framing of immigration, particularly in news media. Metaphors have long been used in communication about immigration as they ``facilitate listeners' grasp of an external, difficult notion of society in terms of a familiar part of life'' \citep{ana_like_1999}. Metaphors of immigration and immigrants as \textsc{animals}, \textsc{vermin}, \textsc{objects}, \textsc{war}, and \textsc{natural disasters} have appeared in U.S. immigration restriction debates for well over a century \citep{obrien_indigestible_2003,card2022computational}. In their analysis of the UK Times newspaper from 1800-2018, \citet{taylor_metaphors_2021} find that \textsc{liquid} and \textsc{object} metaphors persisted throughout the entire period, while \textsc{animal}, \textsc{war}, and \textsc{weight} are more recent. Dehumanization of immigrants through metaphor is sometimes overt (e.g. calling immigrants \textit{animals}) \citep{ana_like_1999}, but is often subconscious as some metaphors are conventionalized in how we describe immigration (e.g. the \textsc{water} metaphor cued by talking about \textit{waves} of immigration) \citep{el2001metaphors,porto_water_2022}. Dehumanizing metaphors of immigrants in particular can play a role in licensing social discrimination and harsh punitive policies \citep{ana_like_1999,arcimaviciene_migration_2018}.


By emphasizing the perceived threat from immigrants, dehumanizing metaphors facilitate support for harsher and more restrictive immigration policies \citep{utych_how_2018}. If people use metaphor to promote particular policy positions and attitudes, we would expect that conservatives use more metaphorical language than liberals. Aligning with prior computational findings that Republican congresspeople use more dehumanizing metaphors than Democrats \citep{card2022computational} and that conservative Twitter users tend to frame immigrants as threats more than liberals do \citep{mendelsohn2021modeling}, we hypothesize that tweets from conservative authors will have more dehumanizing metaphors than tweets from liberal authors. 

However, prior corpus-assisted critical discourse analyses suggest a basis upon which to refute this hypothesis. Comparisons of left-leaning and right-leaning press reveal only minor, if any, differences in metaphor use in discussions of immigration in the Hungarian \citep{benczes_migrants_2022}, Spanish \citep{porto_water_2022}, and United States and European Union \citep{arcimaviciene_migration_2018} contexts. Some metaphors, particularly \textsc{water}, are so conventionalized and accepted as the ``natural'' way to talk about immigration that they appear across a wide range of news sources, and are even reinforced in counter-discourses by authors sympathetic to immigrants \citep{el2001metaphors}.

Metaphor use may also vary between ideological moderates and extremists. If dehumanizing metaphors of immigration are primarily used to communicate threat and advocate for stringent policies, we would expect to see higher usage among far-right authors compared to moderate conservatives. Moreover, extremists on both sides of the political spectrum use more negatively emotional language than moderates \citep{alizadeh2019psychology,frimer2019extremists}, with members of the far-left using even more negative language than members of the far-right \citep{frimer2019extremists}. Both political extremes experience higher amounts of fear and outgroup derogation than moderates \citep{van2015fear}. If metaphor is used to communicate vividness and emotion \citep{ortony1975metaphors}, then we may expect higher metaphor use among both the far-left and far-right compared to moderates. However, we would not expect to see differences across ideology strength if metaphors are primarily conventionalized.

Our hypotheses for the relationship between political ideology and dehumanizing metaphors of immigrants can be summarized as follows:

\begin{itemize}
    \item[H1:] Conservatives use more dehumanizing metaphors than liberals.
    \item[H2:] Far-right authors use more dehumanizing metaphors than moderate conservatives.
    \item[H3:] Far-left authors use more dehumanizing metaphors than moderate liberals.
\end{itemize}


\subsection{Metaphor Effects and Political Ideology}


Experimental research has shown that dehumanizing metaphors can affect attitudes towards immigrants and immigration policy preferences \citep{utych_how_2018,jimenez2021walls}. Exposure to the \textsc{animal} metaphor increases support for immigration restriction, with the effect mediated by emotions of anger and disgust \citep{utych_how_2018}. Exposure to the \textsc{water} metaphor increases support for a border wall between the United States and Mexico \citep{jimenez2021walls}. Metaphorically framing the United States as a \textsc{body} amplifies the effect of pathogenic contamination threat exposure on negative attitudes towards immigrants \citep{landau_dirt_2014}.

It is not clear if such effects hold when we consider real messages from a different domain (social media) use different outcome variables (user engagement as a low-commitment behavioral outcome). In a topic-agnostic study of politicians' Facebook posts, \citet{prabhakaran_how_2021} find that metaphor use is associated with higher audience engagement. We thus hypothesize that metaphor is positively associated with favoriting and retweeting behavior on Twitter. Favoriting and retweeting are related but distinct actions with diverse motivations \citep{boyd2010tweet,meier2014more}: while favoriting often signals alignment or endorsement, retweeting can be used for information amplification and may be either favorable or critical, especially when additional commentary is provided (via quote tweeting). 

As with other framing effects, metaphorical framing effects may be amplified or attenuated by a wide range of moderating variables. In the context of immigration, metaphors have been shown to interact with intergroup prejudices in affecting policy support \citep{marshall2018scurry,mccubbins_effects_2023}. For example, the strength of American in-group identification predicts support for stricter immigration policies, but only when participants are exposed to \textsc{vermin} metaphors \citep{marshall2018scurry}. Similarly, \citet{mccubbins_effects_2023} find that the dehumanizing \textsc{animal} and \textsc{vermin} metaphors both increase support for for-profit immigration detention centers, but only among participants with anti-Latino racial prejudice. \citet{mccubbins_effects_2023} further show that these metaphors have varied effects: exposure to the \textsc{animal} condition had enduring effects after several weeks, but not the \textsc{vermin} condition. 

We are interested in the potential moderating role of political ideology on metaphorical framing effects. Dehumanizing metaphors of immigration have policy entailments that are more aligned with political conservatives. Furthermore, conservatives have greater sensitivity to threat and disgust \citep{jost2003political,inbar2009conservatives}, which are evoked by these metaphors. One may thus expect to find stronger metaphorical framing effects among conservatives. However, prior work has also found liberals (and Democrats) to be more susceptible to metaphorical framing effects than conservatives (and Republicans) \citep{thibodeau_metaphors_2011,hart_riots_2018}. In the context of political protest and policing, \citet{hart_riots_2018} makes an argument for \textit{entrenchment}: conservatives have more fixed attitudes and opinions, ``while liberals formulate their views on a more context-dependent basis taking into account local information supplied by texts.'' In the context of immigration, \citet{lahav2012ideological} find that threat framing has much stronger effects on immigration policy attitudes for liberals compared to conservatives. 

Moreover, recent work has uncovered \textit{resistance to extreme metaphors} among conservatives (and Republicans). \citet{mccubbins_effects_2023} observe a backlash effect among Republicans: they became more opposed to immigrant detention centers when exposed to either dehumanizing metaphor. Several other studies have corroborated this backlash effect in the context of immigration. For example, \citet{hart_animals_2021} find that both \textsc{animal/disease} and \textsc{war} metaphors actually decrease support for anti-immigration sentiments and policies. \citet{boeynaems_attractive_2023} show that metaphorically framing refugees as \textsc{criminals} pushed right-wing populist voters' opinions \textit{away} from right-wing populist stances towards immigration. It appears that such ``extreme'' and overtly inflammatory metaphors are consciously recognized by audiences, which lead to greater scrutiny and ``thus make it easier for the metaphors and their implications to be rejected'' \citep{hart_animals_2021}. Due to liberals' sensitivity to metaphor and conservatives' resistance to extreme metaphor, liberals may be more susceptible to the effects of dehumanizing metaphors of immigration.

We consider favorite and retweet metrics as outcome variables. Our data includes the number of favorites and retweets that a tweet receives, but we unfortunately lack access to information about view counts, \textit{who} is exposed to a particular tweet, and \textit{who} engages with each tweet. To understand effects on audiences, we thus rely on information about the author's ideology and assumptions of homophily: audiences exposed to conservative (liberal) tweets are primarily conservative (liberal) \citep{barbera2015tweeting}. We summarize our hypotheses and research questions as follows:

\begin{itemize}[topsep=0pt,itemsep=0pt]
    \item[H4:] Higher metaphoricity is associated with higher favorite counts.
    \item[H5:] Higher metaphoricity is associated with higher retweet counts.
    \item[RQ1:] (How) does the relationship between metaphor and favorite counts differ between liberal and conservative tweets?
    \item[RQ2:] (How) does the relationship between metaphor and retweet counts differ between liberal and conservative tweets?
\end{itemize}






\input{tables/concepts}


\section{Metaphorical concepts}
We select seven dehumanizing concepts based on prior literature about metaphors in discussions of immigration, immigrants, and refugees (Table \ref{tab:concepts}). Three conceptual domains are related to living non-human organisms: \textsc{animal (beast)}, \textsc{vermin}, and \textsc{parasite}. Two domains are related to natural forces (\textsc{water (liquid)} and \textsc{physical pressure}). The final two concepts involve objectification (\textsc{commodity}) and organized violence (\textsc{war}).

While all of these metaphors liken immigrants to non-human entities, each creates a distinct logic about the perceived threat from immigrants and the plausible solutions to remedy such threats. For example, the \textsc{water} and \textsc{physical pressure} metaphors suggest that immigrants are a threatening force on a host country, which is in turn metaphorically represented as a container \citep{charteris-black_britain_2006}. Within these domains, \textit{reinforcement} of the container is a solution, which is metaphorically extended to more restrictive policy and border security \citep{charteris-black_britain_2006}. On the other hand, organism-related metaphors such as \textsc{vermin} and \textsc{parasite} create conceptual mappings through which the \textit{existence} of immigrants is a threat, thus making extermination and eradication plausible responses \citep{steuter_vermin_2010,musolff_metaphorical_2014,musolff_dehumanizing_2015}. Although \textsc{vermin} and \textsc{parasite} are closely related concepts, they diverge slightly in the nature of the perceived threat: vermin are characterized by their large quantity and capacity to spread disease \citep{steuter_vermin_2010}, while parasites are characterized by living off a host body at the expense of the host \citep{musolff_metaphorical_2014}. 

Based on these unique underlying logics, some metaphorical concepts may be seen at more extreme and blatantly dehumanizing than others (e.g. \textsc{vermin} vs \textsc{commodity}), but there is also heterogeneity within each of these categories (e.g. \textit{historical waves of immigration} and \textit{immigrants pouring in} are both instances of the \textsc{water} metaphor, but the former may be viewed as more conventional and neutrally-valenced).



\begin{figure}[htbp!]
    \centering
    \includegraphics[width=.5\columnwidth]{plots/metaphor_scoring.pdf}
    \caption{Metaphor processing schema showing both word-level measurement using large language models (left) and discourse-level measurement using contextual document embeddings (right). Given a tweet, these methods provide a metaphoricity score for each source domain concept of interest. We also propose and evaluate a combined score that takes the sum of both the word-level and discourse-level scores.}
    \label{fig:metaphor-processing}
\end{figure}





\section{Measuring Metaphors}

In order to address our research questions and hypotheses, we need a reliable and consistent methodology to measure metaphorical language at scale. While there is substantial prior NLP research on metaphor identification (see Appendix §\ref{app:computational} for more details), it primarily focuses on binary classification of whether or not an expression is metaphorical using existing benchmark datasets \citep{shutova_metaphor_2010,ge_survey_2023,steen2010method,gao_neural_2018,choi_melbert_2021}. Such models are less suitable for our goals: they are trained on neither political nor social media data and do not automatically identify relevant source domain concepts. Instead, we propose a novel approach inspired by \citet{brugman_metaphorical_2019}, who discuss how metaphorical frames can be analyzed at the word-level and the concept-level (focused on the ``broader logic of messaegs''). In their meta-analysis, \citet{brugman_metaphorical_2019} manually code experimental frame stimuli from prior studies as non-metaphorical, word-level metaphors, and concept-level metaphors. Overall, \cite{brugman_metaphorical_2019} find positive effects of metaphorical framing on beliefs and attitudes for both levels of analysis, but larger effects for concept-level metaphors. Drawing from this work, we propose two methods for automated metaphor processing: one at the word level and one at the discourse level, which corresponds to \cite{brugman_metaphorical_2019}'s idea of the concept level. These two methods are summarized in Figure \ref{fig:metaphor-processing}.




\subsection{Word-level Metaphor Processing}

We prompt large language models to identify metaphorical words and phrases and their corresponding conceptual source domains as follows:



\noindent \fbox{\begin{minipage}{\columnwidth}
\small{
For each metaphorical word in the tweet below, select the most relevant concept from the following list: 

[water, commodity, physical pressure, war, animal, vermin, parasite]

Respond with a JSON object where keys are metaphors and values are relevant concepts. If a metaphor is not related to any concept above, set its value to “none”. If there are no metaphors, output an empty JSON object. 

Tweet: [TWEET TEXT HERE]}
\end{minipage}}\\

For example, given the tweet \textit{immigrants are flooding into this country}, the LLM output is: \{flooding:\textsc{water}\}. We opt for this zero-shot setup due to initial budget constraints, but future work could explore in-context learning approaches. In order to encourage deterministic outputs, we prompt GPT-4-Turbo with temperature = 0. Our model evaluation and analyses use GPT-4-Turbo outputs from May 2024. 


We then calculate the word-level metaphoricity score for each concept based on the number of concept-relevant metaphorical expressions identified by the LLM (denoted here and in Figure \ref{fig:metaphor-processing} as $C(\textsc{concept})$). We normalize scores by the log-scaled length of the tweet ($C(\textsc{words})$) as follows: $LLM_{\textsc{concept}} = \frac{C(\textsc{concept})}{log(C(\textsc{words})+1)}$. In the example shown in Figure \ref{fig:metaphor-processing}, there is 1 \textsc{water} metaphor in a 6-word tweet, yielding a \textsc{water} metaphor score of $LLM_{\textsc{water}}=0.51$. By contrast, there are 0 \textsc{vermin} metaphors, so $LLM_{\textsc{vermin}}=0$.\footnote{We normalize by length under the intuition that a short tweet that primarily consists of a metaphor is more ``metaphorical'' than a long tweet that includes a metaphorical word. However, this intuition does not hold linearly. For example, an 8-word tweet containing 2 metaphors (.25) should be scored higher than a 40-word tweet containing 2 metaphors (.05), but would likely not appear to readers as 5x more metaphorical.}



\subsection{Discourse-level Metaphor Processing}

According to Conceptual Metaphor Theory, metaphor refers to understanding one concept in terms of another \citep{lakoff1980metaphors}. It is thus possible that a tweet can still be metaphorical insofar as it evokes a distinct conceptual domain, even if no individual words clearly belong to that domain. While automatic metaphor processing has primarily focused on lexical information, several computational scholars have argued that broader situational and discourse-level information is necessary to study metaphor production and comprehension \citep{mu2019learning,dankers_being_2020}.

In their study of dehumanizing language, \citet{mendelsohn2020framework} leverage the idea that geometric relationships between vector representations can capture associations between concepts. \citet{mendelsohn2020framework} estimated associations between target groups and the \textsc{vermin} metaphor by calculating the cosine similarity between word2vec vectors of target group labels and a \textsc{vermin} vector (calculated as a weighted average of vermin-related word representations). We adopt \citet{mendelsohn2020framework}'s approach with several adjustments. First, we use Transformer-based contextualized embeddings instead of static word embeddings, which enables us to calculate metaphorical associations for each document rather than single scores for the entire corpus. Second, we calculate associations between documents and concepts, rather than target group labels and concepts. We make this adjustment because many social media posts about political issues don't necessarily explicitly name the target group impacted by those issues. We expect this change to lead to an increase in recall for identifying dehumanizing metaphors. However, we also acknowledge the limitation that there may be a trade-off with precision, as our approach does not distinguish between different targets (e.g. in addition to dehumanizing metaphors targeting immigrants, we may capture metaphors targeting non-immigrant outpartisans).


We encode each tweet and source domain concept with SBERT using the pretrained \texttt{all-MiniLM-L6-v2} model \citep{reimers2019sentence}. The simplest approach for encoding concepts is to simply embed the concept name (e.g. the word \textit{water}). However, preliminary investigations revealed that just embedding the concept name emphasizes primarily literal usages of the word (e.g. tweets with the highest cosine similarity cued literal meanings of water, such as discussing migrants crossing the sea by boat). To encourage the model to identify metaphorical associations, we instead represent each concept as a set of ``carrier sentences''. Each carrier sentence resembles metaphorical usages of the concept but contain little additional semantic information (e.g. \textit{they flood in}; \textit{they hunt them down}). We manually construct 104 carrier sentences exclusively based on example words and sentences from prior literature (Table \ref{tab:concepts}). Each concept has between 8-22 associated carrier sentences.

We then calculate a tweet's discourse-level metaphoricity with respect to a particular source domain concept as the cosine similarity between the tweet and the average of the carrier sentences' SBERT representations, yielding continuous scores between 0 and 1. Considering the example in Figure \ref{fig:metaphor-processing}, the cosine similarity between the tweet's SBERT embedding and the average of the \textsc{water} concept's carrier SBERT embeddings is 0.21, while the cosine similarity with the \textsc{vermin} concept is 0.08.


\subsection{Combined Scoring}
We propose and evaluate a combined metaphor measure (SUM), which is simply the sum of the word-level (LLM) and discourse-level (EMB) scores for each conceptual domain. This measure places the most weight on the presence of metaphorical words (see score distributions in the Appendix Figures \ref{fig:dist-sbert}, \ref{fig:dist-gpt}, \ref{fig:dist-sum}). However, as metaphorical words are relatively sparse, the combined measure enables us to leverage discourse-level signals even in the absence of explicit metaphors.


\section{Data}

We use the Immigration Tweets Dataset from \citet{mendelsohn2021modeling}, which includes 2.6 million English-language tweets from 2018-2019 that contain a keyword related to immigration (e.g. \textit{immigration}, \textit{illegals}, \textit{migrants}). 4500 tweets were manually labeled for 27 frame categories including (a) 14 generic policy frames (e.g. \textit{Economic} and \textit{Security \& Defense}) \citep{boydstun2013identifying}, (b) 2 generic narrative frames (\textit{Episodic} and \textit{Thematic}) \citep{iyengar1991anyone}, and (c) 11 immigration-specific frames (e.g. \textit{Immigrants as victims of humanitarian crises} and \textit{Immigrants as threats to public order}) \citep{benson2013shaping}. RoBERTa models were trained on the manually-labeled data and used to automatically infer frame labels for the remaining tweets. 

The dataset further provide user engagement counts, tweet author metadata, inferred location of tweet authors at the country level and inferred political ideology based on social network-based models from earlier work \citep{compton2014geotagging,Barbera2015}. Using this information, we select a random sample of 200K tweets whose authors are based in the United States and have an associated ideology estimate. Ideology estimates are continuous values, where more negative scores reflect stronger liberal ideology and more positive scores reflect stronger conservative ideology. As our research questions involve comparing liberals and conservatives as groups, we opt to break down ideology estimates into two variables: (1) a binary liberal/conservative variable based on the sign of the ideology estimate, and (2) an \textit{ideology strength} variable that is the magnitude of the original estimate. 

\subsection{Sampling for Annotation}


The prevalence of metaphorical language with respect to each source domain concept is not known a-priori. Instead of randomly sampling tweets for human annotation, we thus adopt a stratified sampling approach using scores from a baseline model (\texttt{GPT-4-Turbo, Simple Prompt}) as a heuristic. 

For each concept $c$, we sample $n_c$ documents from all documents $D$. Let $h_c$ be the heuristic metaphoricity score with respect to $c$. $Q_{0,c} \in D$ is then the set of documents with $h_c = 0$. $Q_{i,c} \in D$ where $i = 1,2,...,k-1$ are the $k-1$ quantiles of documents with $h_c > 0$. The annotation sample for each concept is then:
$$S_c = Sample(Q_{0,c},\frac{n_c}{k}) \cup  \bigcup_{i=1}^{k-1}Sample(Q_{i,c},\frac{n_c}{k})$$

Using $k=5$ strata, we sample $n_c=200$ tweets for each source domain. We additionally sample 200 documents for \textit{domain-agnostic} metaphoricity, i.e., use of metaphorical language independently of any particular concept.



% The simplest evaluation design would involve providing human annotators with a randomly-selected set of tweets, asking them to assign each tweet a numerical score representing its metaphoricity with respect to a given concept, and calculating correlations with our models' scores. However, this approach presents two challenges. First, labeling metaphoricity is a subjective task that is shaped by the annotators' culture, cognition, and experiences. We would expect heterogeneity in annotations due to differing perceptions of metaphor. There could also be heterogeneity in how annotators approach numerical scales (e.g. even if two annotators share the same perceptions of metaphoricity, one may be biased towards the middle of the scale while the other is biased towards the extreme ends). To address this challenge, we construct a pairwise evaluation task: instead of requesting binary labels or numerical scores, we ask annotators to select which tweet out of a pair is most associated with a metaphorical concept. Comparisons between pairs or sets of items have been shown to produce more reliable labels for subjective NLP annotation tasks compared to numerical ratings \citep{kiritchenko2017best}.

% The second challenge is that we do not know a-priori how common metaphorical tweets are. If we randomly sample tweet pairs, it is possible that most pairs would have no relevance to a particular metaphor and that there would be no meaningful differences between the two tweets. We address this challenge by creating an intentionally biased evaluation setup, where we use the word-level (LLM) and discourse-level (EMB) metaphoricity scores as heuristics for sampling tweet pairs. 

% For each concept $C$, we sample tweet pairs as follows: 

% {
% \begin{enumerate}[noitemsep,nolistsep]
% \item Calculate $\text{EMB}_C$ scores for the full corpus using SBERT.
% \item Separate tweets into two strata based on $\text{EMB}_C$ score percentiles. The top stratum ($T_{\text{EMB}}^C$) includes tweets with $\text{EMB}_C$ scores in the top 2\%, and the bottom stratum includes tweets with scores in the bottom 98\% ($B_{\text{EMB}}^C$). 
% \item Randomly sample 800 tweets from the top stratum and 200 from the bottom stratum.
% \item Calculate $\text{LLM}_C$ scores for this sample of 1000 tweets using GPT-4-Turbo. We only calculate $\text{LLM}_C$ on this sample rather than the full corpus due to budget constraints.
% \item Separate tweets into two strata based on $\text{LLM}_C$ scores. The top stratum includes tweets with $\text{LLM}_C > 0$ ($T_{\text{LLM}}^C$), and the bottom includes tweets with  $\text{LLM}_C = 0$ ($B_{\text{LLM}}^C$).
% \item Create SBERT-based evaluation sample by randomly matching 150 tweets from $T_{\text{EMB}}^C$ with 150 tweets from $B_{\text{EMB}}^C$.
% \item Create GPT-based evaluation sample by randomly matching N tweets from $T_{\text{LLM}}^C$ with N tweets from $B_{\text{LLM}}^C$, where $N = min(150,|T_{\text{LLM}}^C|)$. 
% \end{enumerate}
% }

% From this procedure, we obtain 150 pairs of tweets that differ based on either discourse-level (EMB) or word-level (LLM) scores for each concept, with the exception of \textsc{parasite} with 50 pairs in the GPT-based evaluation sample. We additionally create an evaluation set of 150 tweet pairs for overall metaphoricity judgments, where one tweet contains at least one GPT-identified metaphor (associated with any conceptual domain) and the other tweet has none. In total, our evaluation dataset contains 2,150 tweet pairs.

% Intuitively, our strategy inverts a traditional machine learning evaluation setup. Instead of getting machine predictions on a human-curated evaluation dataset, we obtain human annotations for a machine-curated dataset. While our approach overcomes challenges of subjectivity and sparsity, it still has limitations. Most notably, our evaluation method cannot reliably measure \textit{recall} because it relies on a set of tweets already identified as highly metaphorical by the computational models. We do not know how well the models correctly retrieve truly highly-metaphorical documents. We encourage future work to expand our evaluation approach to measure recall. 


%We calculate discourse-level metaphoricity scores for all concepts for all 1.2M tweets using SBERT. For each concept, we sample 1000 tweets, 800 of which are drawn from the top 2\% of EMB scores, and 200 from the bottom 98\%. We then calculate word-level metaphoricity scores on this smaller sample using GPT-4-Turbo. This procedure yielded at least 150 tweets with non-zero word-level metaphoricity scores for each concept, except for \textsc{parasite} which has 50 tweets with GPT-identified metaphors.

%For each concept, we sample 150 pairs in which tweets differ based on SBERT scores (top 2\% vs. bottom 98\%) and 150 pairs that differ based on GPT scores (zero vs. at least one concept-relevant metaphor identified), with the exception of \textsc{parasite} with 50 pairs in the GPT-sampling condition. We refer to these biased sampling strategies as SBERT-based and GPT-based evaluation samples. We additionally create an evaluation set of 150 tweet pairs for overall metaphoricity judgments, in which GPT predicts that one tweet contains no metaphors and the other contains at least one metaphorical word associated with \textit{any} conceptual domain. In total, our dataset for evaluation contains 2,150 tweet pairs.

\subsection{Annotation for Evaluation}

\begin{itemize}
    \item motivate binary annotation. we believe this is a continuous underlying measure, but eliciting continuous judgments from humans is difficult. we thus treat this as a binary annotation task, and get continuous estimates from a large number of annotations
    \item created codebook
    \item two researchers familiar with the project and context used the codebook to label 80 tweets, ten for each concept and domain-agnostic metaphoricity. Reached agreement of 0.67 
    \item adapted the codebook for use by crowdworkers. Specifically, each task required annotators to label 20 tweets for metaphoricity with respect to a particular concept. So we had a slightly different set of instructions for each concept, and general instructions were tailored to particular concepts. 
    \item give tweet examples for zero, mid, and high metaphoricity for one concept
\end{itemize}

Each task required annotators to label 20 tweets for metaphoricity, either with respect to a particular concept $c$ or for use of any metaporical language (domain-agnostic). 

Participants are paid \$1.60 per task of 20 tweets. All annotators are based in the United States and have completed at least 200 prior tasks on Prolific with at least a 99\% approval rate. Prescreening and payment occurred through Prolific, but the annotation tasks are administered through Qualtrics. While we did not reject any responses on prolific, we filtered out labels from annotators who (1) completed the task in under 3 minutes and (2) provided the same response for all samples.  

% We recruit participants via Prolific to annotate tweet pairs, either for overall metaphoricity or with respect to a particular concept. Participants are randomly assigned to an annotation task containing 50 pairs of tweets and a single concept (or overall metaphoricity). When provided with a particular concept $C$, annotators are asked to select which tweet from a pair makes a stronger association between immigrants/immigration and $C$. For the overall metaphoricity tasks, participants are asked to select which tweet uses more metaphorical (non-literal) language. 

% Each tweet pair is annotated by 5-6 individuals, with a total of 196 unique annotators. Participants are paid \$4 per task of 50 tweet pairs, and the median time for task completion is 13 minutes. All annotators are based in the United States and have completed at least 200 prior tasks on Prolific with at least a 99\% approval rate. Prescreening and payment occurred through Prolific, but the annotation tasks are administered through Qualtrics.




% \input{tables/agreement}

% Interannotator agreement for this task is low, with an overall Krippendorff's $\alpha$ of 0.24 (see Table \ref{tab:agreement}, and Appendix Figure \ref{fig:agreement-person} for agreement distribution across annotators). There is some heterogenerity across concepts (Appendix Figures \ref{fig:agreement-overall}-\ref{fig:agreement-models}, with the highest agreement for \textsc{water} (0.33) and the lowest for \textsc{physical pressure} and \textsc{overall} (0.19). In light of this low agreement, we restrict our evaluation set to a subset of 1,377 tweet pairs for which there is agreement among at least 75\% of annotators (at least 4/5 or 5/6). 


% There are several possible sources for this low agreement: task complexity, annotator quality, and sampling issues. There is substantial subjectivity in determining if an expression is literal or not, particularly because many metaphors are conventionalized. We opt for a crowdsourcing annotation setup to collect data about perceived differences in metaphoricity across texts and simply task instructions accordingly. However, it is possible that laypeople often cannot consciously recognize metaphors, and may not notice that much of our conventional language for talking about immigration originates in other source domains. Due to the task's inherent subjectivity, it is difficult to diagnose issues with annotation quality. Nevertheless, we observe that some annotators completed the task far more quickly than expected, a small number of whom took under 5 seconds per tweet pair. While excluding these annotators does not substantially change our evaluation results, future annotation efforts ought to address this potential lack of annotator attention with minimum time requirements, shorter tasks, and mid-task attention checks.  


% Finally, we use SBERT and GPT scores as heuristics for sampling pairs. However, differences between tweets' scores may not accurately or meaningfully correspond to actual differences in metaphoricity (indeed, this annotation is necessary to evaluate the SBERT and GPT models). The lack of meaningful differences within pairs would lead to greater arbitrariness in annotators' selections, and thus lower interannotator agreement.


% \input{tables/full_sample_all_metaphor_eval}

% \begin{figure}[htbp!]
%     \centering
%     \includegraphics[width=.5\columnwidth]{plots/accuracy_by_model.pdf}
%     \caption{Accuracy by model across metaphors}
%     \label{fig:accuracy_by_metaphor}
% \end{figure} 





\section{Metaphor Processing Results}

The SUM model, which combines SBERT (discourse-level) and GPT (word-level) scores, achieves the highest performance on the full annotated evaluation set with an accuracy of 0.77 (Table \ref{tab:full_sample_all}). Because our evaluation task (a) consists of forced paired comparisons and not straightforward binary classification, and (b) cannot reliably measure recall, we use accuracy rather than F1 metrics to assess model performance. In this context, a model's accuracy refers to the proportion of tweet pairs for which the model gives a higher metaphoricity score to the tweet that annotators identify as more metaphorical.


The SUM model outperforms the other models across all metaphorical concepts (Figure \ref{fig:accuracy_by_metaphor}
). However, its accuracy varies across concepts, with the highest score for \textsc{water} (0.86) and the lowest for \textsc{commodity} (0.68). The model can readily distinguish which tweet from each pair uses more metaphorical language overall, with an accuracy of 0.95. We additionally compare models across subsets of tweet pairs that were sampled either based on SBERT or GPT scores. As expected, SBERT outperforms GPT for the evaluation sample generated based on SBERT and vice versa for the GPT-based evaluation sample (Figure \ref{fig:accuracy_by_sample}). 

\begin{figure}[htbp!]
    \centering
    \includegraphics[width=.3\columnwidth]{plots/sample_scoring_strategy_heatmap.pdf}
    \caption{Accuracy across different evaluation samples}
    \label{fig:accuracy_by_sample}
\end{figure}

We use the SUM model to predict metaphor scores for each concept on a set of 204K tweets randomly sampled from our data.\footnote{We use a large sample and not the entire dataset of 1.2M tweets due to OpenAI API costs and initial budget constraints.} Table \ref{tab:tweets} shows selected tweets with high metaphor scores, their corresponding concepts, and authors' ideology.


\input{tables/tweets.tex}


\section{Metaphor and political ideology}


Following the approach established in \citet{mendelsohn2021modeling}, we conduct a frame-building regression analysis to quantify the role of political ideology in metaphorical framing. Our first analysis compares metaphor usage between liberals and conservatives. Our second analysis focuses on the relationship between ideology strength and metaphor. 



\paragraph{Regression Setup} We fit a series of linear regression models where the dependent variables are z-scored normalized metaphor scores for each concept as well as overall metaphoricity. Fixed effects include ideology (liberal or conservative) and ideology strength (continuous; min = 0, max = 4.88, median = 1.48, 99\% = 3.33), and the interaction between ideology and ideology strength. We control for a set of message-level and author-level variables (e.g., tweet length, follower count). We additionally specify a set of models that include ten issue-generic policy frames (e.g., \textit{Security \& Defense}) that were detected by RoBERTa sufficiently well (F1 > 0.6) from \citep{mendelsohn2021modeling}. We also include interactions between ideology and issue-generic frames as fixed effects to account for the possibility that metaphor usage patterns within each issue-generic frame may vary between liberals and conservatives. See Appendix Tables \ref{tab:ideology-effect-frames} and \ref{tab:ideology-effect-no-frames} for all included variables in both sets of models. We visualize and discuss average marginal effects rather than regression coefficients for ease of interpretability, especially with interaction terms. All regression coefficients can be found in Appendix Tables \ref{tab:ideology-effect-frames} and \ref{tab:ideology-effect-no-frames}.





\paragraph{Results}
Figure \ref{fig:ideology-effect} shows the average marginal effect of conservative ideology for all concepts and overall metaphoricity after controlling for issue-generic frames (see Appendix Figure \ref{fig:ideology-effect-no-frame} for corresponding results from the set of models excluding issue-generic frames). Conservative ideology is significantly associated with higher use of all metaphors except for \textsc{war}. H1 is largely supported. 

At the same time, we observe substantial variation across concepts. Conservative ideology is most strongly predictive of the \textsc{vermin} metaphor and overall metaphor use by far. This may be partly driven by the expressions ``illegals'' and ``illegal aliens'' being frequently labeled as \textsc{vermin} metaphors by \texttt{gpt-4-turbo}.\footnote{We re-evaluated our metaphor processing models after removing these expressions from the GPT outputs but performance on our evaluation set dropped substantially.} While these terms are indeed metaphorical, they are also highly conventionalized in conservative discourse about immigration in the United States. Moreover, it remains an open question if these expressions specifically activate the \textsc{vermin} mapping.

Not only is conservative ideology predictive of metaphor use, ideology strength among conservatives is also associated with higher metaphor scores for all concepts. This supports H2. Ideology strength is not significantly associated with metaphor use among liberals. With the exception of a small but significant negative association with \textsc{vermin}, far-left authors are neither more nor less likely to use dehumanizing metaphors than moderates. H3 is thus not supported.

Our regression also facilitates analysis of the relationships between issue-generic policy frames, metaphor, and political ideology (Appendix Figures \ref{fig:frame-effect}, \ref{fig:overall-metaphoricity-all-vars}, and \ref{fig:ideology-by-frame}). We find that some issue-generic frames are strongly associated with particular metaphorical concepts (e.g., \textit{economic} for \textsc{commodity} and \textit{security} for \textsc{war}), and metaphors are more readily used with some issue-generic frames compared to others (e.g., \textit{security} is more metaphorical than \textsc{cultural identity}).


\begin{figure}[htbp!]
    \centering
    \includegraphics[width=.5\columnwidth]{plots/marginal_effects_by_ideology.pdf}
    \caption{Average marginal effect of conservative ideology on metaphor scores. ``Overall'' represents metaphoricity measurement independent of concept.}
    \label{fig:ideology-effect}
\end{figure}



\begin{figure}[htbp!]
    \centering
    \includegraphics[width=.5\columnwidth]{plots/marginal_effect_of_ideology_strength.pdf}
    \caption{Average marginal effect of ideology strength on metaphor scores among both conservatives (orange, lower bars) and liberals (blue, upper bars).}
    \label{fig:magnitude-effect}
\end{figure}


\section{How are liberals using metaphors?}

Although conservative ideology is more highly associated with the use of dehumanizing metaphors of immigrants overall, liberals still use these metaphors to a significant extent. 39.1\% of tweets in the full dataset are written by liberals, and 32.0\% of the tweets scoring in the highest 2\% for overall metaphoricity are from liberals. Out of the 2\% of tweets with the highest metaphor scores for \textsc{animal} and \textsc{pressure}, 38.9\% and 37.9\% are written by liberals, respectively. See Appendix Figure \ref{fig:percent-liberal} for the proportion of liberal tweets at varied metaphoricity score thresholds for all concepts. Some metaphors, namely \textsc{animal}, are far less ideologically-skewed than expected. We thus conduct a small qualitative analysis to investigate how and why liberals are using these metaphors, which seem much more aligned with conservative attitudes and policy preferences. Specifically, we draw the following themes from the 30 liberal tweets with the highest metaphor scores for each concept.

% # 39.1% of tweets in the dataset are from liberals
% # 32.0\% of overall tweets in the top 2 percent overall are from liberals
% # 38.9% of animal tweets in top 2 percent
% # 37.9% of pressure tweets


First and most straightforwardly, liberals sometimes embrace these metaphors. Examples include the liberal tweets shown for \textsc{commodity} and \textsc{water} in Table \ref{tab:tweets}, where immigrants are described as \textit{a source of money} and \textit{a wave}, respectively. Interestingly, the liberal author extends the \textsc{water} metaphor beyond conventional usages to celebrate immigrants. 

Second, liberals sometimes use dehumanizing metaphors in a sympathetic framing of immigrants, particularly to highlight humanitarian concerns. We observe this case the most frequently with the \textsc{animal} metaphor, potentially explaining the relatively weaker association between \textsc{animal} and conservative ideology. In our corpus, metaphorical references to holding \textit{kids in cages} at detention centers at the US-Mexico border are particularly prominent. One such example can be found in Table \ref{tab:tweets}, where the liberal author talks about ``caging innocent immigrant children'' and accusing their interlocutor of wanting to ``cage them like animals''. Other, more subtle, instances of the \textsc{animal} metaphor made references to \textit{feeding} and \textit{sheltering} immigrants; while these verbs are sometimes used to talk about humans, they deny agency to the recipient of the food and shelter and are thus more associated with animals \citep{tipler_agencys_2014}.

Third, liberal tweets with high metaphor scores often quote or paraphrase others. We sometimes observe this case with extreme metaphors that are so inflammatory that uttering these metaphors becomes a topic of discussion in and of itself, as exemplified by the liberal tweets with high \textsc{vermin} scores in Table \ref{tab:tweets}. One of the liberal tweets with a high \textsc{parasite} score invokes the metaphor to point out the irony of descendants of immigrants likening current immigrants to parasites. Such quotative tweets are not always explicitly critical, however, as can be seen in the liberal tweet with a high \textsc{pressure} score.

Finally, in line with research showing extreme out-party dehumanization \citep{martherus2021party}, we see a pattern of liberal tweets redirecting dehumanizing metaphors from immigrants to political opponents (especially Trump and his supporters). The liberal \textsc{war} tweet in Table \ref{tab:tweets} is one such example, where the metaphor is redirected from a war against immigrants to a war against ``right-wing forces''. The liberal \textsc{animal} tweet author similarly redirects the animal metaphor from immigrants towards their interlocutor, claiming that they need ``a straight jacket \& a muzzle''. Within our corpus, there is one particularly common sub-category that combines this case and straightforward dehumanization of immigrants: redirecting metaphors from immigrants as a population to Melania Trump, who is Donald Trump's wife and herself an immigrant. A liberal tweet with a high \textsc{parasite} score in Table \ref{tab:tweets}, for example, indirectly references Melania Trump as ``immigrant wifey'' and calls her a ``tick'' and ``blood sucker''.



\section{Metaphor and User Engagement}

We estimate metaphorical framing effects by measuring associations between metaphor scores and user engagement (favoriting and retweeting), which can be conceptualized two low-commitment behavioral outcomes. We also consider political ideology as a potential moderating variable. Unfortunately, we only have political ideology data for tweet authors and not audience members engaging with the tweets. This analysis thus rests on the assumption that a tweet's author and audience generally share the same political ideology \citep{barbera2015tweeting}. Specifically, we compare how associations between metaphoricity and user engagement differ based on whether the author is liberal or conservative.




\paragraph{Regression Setup} We again fit a series of linear regression models. This time, the dependent variables are the numbers of favorites and retweets (log-scaled) and the main independent variable of interest is the metaphor score based on the SUM model.\footnote{The SUM metaphor scores between concepts are weakly to moderately correlated, with Pearson correlations ranging from 0.08 (\textsc{pressure} \& \textsc{vermin}) to 0.42 (\textsc{animal} \& \textsc{parasite}), with a mean of 0.23. We thus fit separate models for each combination of metaphorical concept and engagement type to avoid multicollinearity issues.} Other fixed effects include ideology (liberal or conservative), interactions between ideology and metaphor score, ideology strength, issue-generic frames, and tweet-level and author-level control variables. As robustness checks, we additionally fit models without controlling for ideology or issue-generic frames. Because the results from these robustness checks are qualitatively similar, we focus on results from the full model in this section.


\begin{figure}[htbp!]
    \centering
    \includegraphics[width=.5\columnwidth]{plots/combined_frames_with_ideology_marginal_effects_on_engagement.pdf}
    \caption{Average marginal effects of metaphor scores on the number of favorites and retweets, log-scaled.}
    \label{fig:engagement}
\end{figure}

\paragraph{Results}

Figure \ref{fig:engagement} shows the average marginal effect of metaphor scores on favorite and retweet counts (see Appendix Figures \ref{fig:engagement-no-frame-no-ideology} and \ref{fig:engagement-no-frame} for results from models without issue-generic frames and ideology controls). Full regression coefficients can be found in the Appendix Tables \ref{tab:rt-frame} and \ref{tab:fav-frame} for favorites and retweets, respectively (see Tables \ref{tab:rt-no-frame}, \ref{tab:rt-no-frame-no-ideology}, \ref{tab:fav-no-frame}, and \ref{tab:fav-no-frame-no-ideology} for results from models without frame and ideology controls).

We observe mostly no significant relationship or significantly negative relationships between metaphor and favorites, with \textsc{war} being the only exception. Interestingly, the amount of overall metaphorical language use (``overall'') is the most strongly negatively predictive of favorites. H4 is thus not supported. On the other hand, we observe that high metaphoricity is associated with more retweets for five out of seven conceptual domains. This variation across concepts is in line with prior evidence that conceptual domains moderate metaphorical framing effects \citep{bosman_persuasive_1987}. H5 is mostly supported. 

We then separate tweets into those written by liberal and conservative authors to investigate the moderating role of ideology. We unfortunately only have the numbers of favorites and retweets and lack information about \textit{who} is engaging with this content. If assumptions of homophily hold, audiences engaging with tweets would most often share the same political ideology as the respective authors.

Addressing RQ1, we do not find significant associations between metaphor and favoriting behavior, and further do not identify consistent differences in this relationship among liberals and conservatives (Appendix Figure \ref{fig:favorite-ideology}). In contrast, Figure \ref{fig:retweet-ideology} shows that the positive association between metaphor and retweets is largely driven by liberals. For most conceptual domains, tweets with higher metaphor use get more retweets. Addressing RQ2, this effect is significantly stronger for liberal authors than conservatives. 




\begin{figure}[htbp!]
    \centering
    \includegraphics[width=.5\columnwidth]{plots/combined_frames_with_ideology_marginal_effects_on_retweets.pdf}
    \caption{Group-average marginal effects of metaphor scores on retweet counts (log-scaled) for liberals and conservatives.}
    \label{fig:retweet-ideology}
\end{figure}




\section{Discussion}

More than simply rhetorical decor, metaphors construct and reflect a deeper conceptual structuring of human experiences \citep{lakoff1980metaphors}, and are potentially important devices for political persuasion \citep{mio_metaphor_1997}. While the use of metaphor in politicians' speeches and mass media has been long-established \citep{charteris-black_britain_2006}, it remains unclear how metaphor is used in political discussions among ordinary people on social media. We do not know who uses metaphors on social media, how often and in what contexts, and why people select certain metaphors. We do not yet know precisely how metaphor affects online audiences' understanding of political information, attitudes, or policy preferences. We argue that this theoretical gap is largely due to a methodological one: identifying and quantifying metaphorical language at scale is a particularly challenging task. 

We develop a computational approach for processing metaphor that combines LLMs and document embeddings in order to leverage both word-level and discourse-level metaphorical cues. We evaluate our models by creating and annotating a new dataset of over 2000 pairs of tweets compared for metaphorical associations with respect to seven concepts, such as \textsc{water} and \textsc{animal}. We apply our approach to analyze the use of dehumanizing metaphors in over 200K immigration-related tweets, and specifically investigate the relationship between metaphor, political ideology, and user engagement.

We find that conservative ideology is associated with greater use of dehumanizing metaphors of immigrants. However, the strength of this association varies considerably across conceptual domains and is small-to-none for some concepts. Higher conservative usage suggests that metaphors are often used to reflect negative attitudes towards immigrants and in service of promoting stricter immigration policies. However, the variability in this association and appearance among liberals suggests a high degree of conventionalization in which such metaphors are accepted as ``natural'' \citep{el2001metaphors}.
Moreover, we find that ideology strength is positively associated with metaphor use for conservatives but there is no effect for liberals in either direction. Such asymmetrical effects suggest that conservatives and liberals use dehumanizing metaphors for different underlying reasons.

We further interrogate liberals' use of dehumanizing metaphors in a qualitative analysis of liberal tweets with the highest metaphor scores for each conceptual domain. We uncover three unanticipated patterns in how liberals use dehumanizing metaphors in immigration discourse: (1) in sympathetic framings of immigrants, (2) quoting (and potentially directly challenging) another person, and (3) redirecting metaphors to target political opponents, including immigrant members of the political outgroup. While these cases often do not include straightforwardly dehumanizing immigrants as a population, they could still tacitly endorse and reinforce the metaphors as permissible ways to think and talk about immigrants, potentially having unintentional ramifications for the treatment of immigrants in our society \citep{el2001metaphors}. Future research could expand our methodology to distinguish between such discursive contexts and address open questions about their prevalence and societal consequences.

%Even as our models can readily identify dehumanizing metaphors of immigrants, this qualitative investigation reveals how metaphor is interwoven through discursive and metadiscursive layers of political conversation, and raises a number of theoretical and methodological questions. Does automated metaphor processing need to distinguish between these cases, potentially requiring us to incorporate paraphrase and implied sentiment modules in conjunction with metaphor detection? Does such context moderate the effects of dehumanizing metaphors? Are the same conceptual mappings and their entailments activated by messages that use these metaphors, even if the message itself is not straightforwardly targeting immigrants, and may even be sympathetic towards immigrants? 


Lastly, we measure the relationship between metaphor and user engagement, and uncover surprising differences between the effects on favorite and retweet counts. Metaphor is associated with more retweets, but not favorites. In line with findings that liberals are more susceptible to effects of metaphor \cite{hart_riots_2018} and threat frames \citep{lahav2012ideological}, the association between metaphor and retweets is stronger among liberals than conservatives. There are a number of limitations in interpreting framing effects from this study. First is the lack of causality; while we control for many potential confounds in our regression analyses, we do not evaluate causal assumptions nor attempt to make causal claims. Second is the ambiguity surrounding people's motivations for favoriting and retweeting content \citep{meier2014more,boyd2010tweet}. We do not know why different patterns emerge for favoriting and retweeting. We conjecture that favoriting may be dampened by the negative emotional content conveyed with such metaphors, and retweeting may be enhanced by the desire to amplify information that communicates threats or presents compelling arguments against political opponents \citep{mendelsohn2021modeling}. The third limitation is that we only have data about how many favorites and retweets a post receives, and no data about \textit{who} is engaging with the content. In motivating and interpreting our findings, we assume that a tweet's author and audience share a political ideology \citep{barbera2015tweeting}. However, this is often not the case. For example, a tweet could be widely retweeted, but primarily among political opponents who criticize the original post. 

Beyond addressing these limitations, there are many opportunities for future work. The present research benefited from decades of literature about metaphor and framing in immigration discourse, and future research can assess the extent to which our methodology can be adapted to other contexts. We manually curate a set of relevant conceptual source domains from existing literature, which may not be readily available for new or lesser-studied issues. One may thus want to develop automated methods for \textit{political metaphor discovery} for situations in which the relevant source domain concepts are unknown. A potential direction could involve enhancing metaphor interpretation models that label conceptual mappings using external knowledge graph and lexical resources \citep{mao_metapro_2022}. Another avenue for future work could focus on combining NLP-based measurements of metaphorical language with large-scale framing experiments to precisely quantify the effects of metaphor on emotions, policy preferences, and attitudes towards different social groups. 


\section{Limitations}


\section{Ethical Considerations}

% Bibliography entries for the entire Anthology, followed by custom entries
%\bibliography{anthology,custom}
% Custom bibliography entries only
\bibliography{custom}

\appendix

\section{Example Appendix}
\label{sec:appendix}

\input{appendix}

\end{document}
