% Please add the following required packages to your document preamble:
% \usepackage{booktabs}
% \usepackage{graphicx}
\begin{table*}[htbp!]
\resizebox{\textwidth}{!}{%
\begin{tabular}{@{}llllllllll@{}}
\toprule
                                  & \multicolumn{9}{c}{Metaphoricity Classification Threshold (\% of annotators)}                                                                         \\ 
                                  & 10\%           & 20\%           & 30\%           & 40\%          & 50\%           & 60\%           & 70\%           & 80\%           & 90\%           \\ \midrule
SBERT                             & 0.608          & 0.618          & 0.625          & 0.625         & 0.638          & 0.65           & 0.678          & 0.675          & 0.647          \\
Llama3.1 + Simple                 & 0.653          & 0.657          & 0.661          & 0.665         & 0.673          & 0.68           & 0.717          & 0.728          & 0.748          \\
Llama3.1 + Simple + SBERT         & 0.684          & 0.692          & 0.702          & 0.702         & 0.713          & 0.719          & 0.76           & 0.77           & 0.781          \\
Llama3.1 + Descriptive            & 0.508          & 0.509          & 0.512          & 0.511         & 0.513          & 0.51           & 0.504          & 0.499          & 0.497          \\
Llama3.1 + Descriptive + SBERT    & 0.615          & 0.626          & 0.635          & 0.634         & 0.651          & 0.656          & 0.676          & 0.668          & 0.639          \\
GPT-4o + Simple                   & 0.661          & 0.682          & 0.681          & 0.691         & 0.703          & 0.706          & 0.726          & 0.753          & 0.782          \\
GPT-4o + Simple + SBERT           & 0.688          & 0.713          & 0.715          & 0.722         & 0.732          & 0.737          & 0.763          & 0.786          & 0.795          \\
GPT-4o + Descriptive              & 0.626          & 0.661          & 0.684          & 0.699         & 0.726          & 0.751          & 0.794          & 0.833          & 0.849          \\
GPT-4o + Descriptive + SBERT      & 0.677          & 0.709          & 0.731          & 0.742         & 0.771          & 0.796          & \textbf{0.847} & \textbf{0.869} & \textbf{0.866} \\
GPT-4-Turbo + Simple              & 0.688          & 0.66           & 0.643          & 0.64          & 0.65           & 0.642          & 0.673          & 0.679          & 0.724          \\
GPT-4-Turbo + Simple + SBERT      & \textbf{0.714} & 0.695          & 0.682          & 0.679         & 0.689          & 0.682          & 0.718          & 0.726          & 0.752          \\
GPT-4-Turbo + Descriptive         & 0.648          & 0.672          & 0.702          & 0.72          & 0.748          & 0.781          & 0.802          & 0.828          & 0.845          \\
GPT-4-Turbo + Descriptive + SBERT & 0.695          & \textbf{0.717} & \textbf{0.746} & \textbf{0.76} & \textbf{0.789} & \textbf{0.817} & 0.844          & 0.859          & 0.859          \\ \bottomrule
\end{tabular}%
}
\caption{ROC-AUC scores over all concepts for each model, prompt, and SBERT inclusion combination with ground-truth classification thresholds set at 10\% intervals. \texttt{(GPT-4-Turbo, Descriptive, SBERT)} has the highest performance for the 20-60\% range, and \texttt{(GPT-4o, Descriptive, SBERT)} has the highest performance for the 70-90\% range, but the difference between these two models is not significantly different.}
\label{tab:auc_threshold}
\end{table*}