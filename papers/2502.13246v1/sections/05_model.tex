\input{tables_feb2025/evaluation/auc_by_threshold}

\begin{figure}[t!]
    \centering
    \includegraphics[width=\columnwidth]{plots_feb2025/evaluation/auc30_sbert_effect.pdf}
    \caption{ROC-AUC scores at the 30\% threshold for each LLM and prompt combination, with and without adding discourse-level signal from SBERT. Adding discourse-level metaphorical associations improves performance across all LLMs and prompts.}
    \label{fig:sbert-effect}
\end{figure}

\section{Model Evaluation}

Since both the predicted and ground-truth metaphor scores are continuous, we evaluate models using Spearman correlation and ROC-AUC, applying varied classification thresholds to identify positive instances. 
%To more closely compare performance across models and concepts, 
In our main analysis, we focus on ROC-AUC values at the 30\% classification threshold (i.e., ``positive instances'' are tweets where at least 30\% of annotators judge it to be metaphorical) because this threshold creates the most balanced dataset. 
%Here, we focus on a subset of ROC-AUC results, and f
See Appendix ยง\ref{eval} for complete results.

Table \ref{tab:auc_threshold} and Figure \ref{fig:sbert-effect} show that the best performing models are \model{GPT-4o} and \model{GPT-4-Turbo} with \model{Descriptive} prompts and added \model{SBERT}. These models also have the highest performance across the majority of concepts (Appendix \ref{tab:auc30_concepts}). Notably, including discourse-level signals from adding in SBERT cosine similarity improves performance across all LLMs and prompt combinations.

We measure statistical significance with bootstrap tests (n=100).  \model{GPT-4o} and \model{GPT-4-Turbo} with \model{Descriptive} prompts and added \model{SBERT} outperform all other models, but are not significantly different from each other. We use \model{GPT-4o} for large-scale analysis because inference is 4x cheaper.

