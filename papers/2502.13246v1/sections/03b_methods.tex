

\section{Measuring Metaphors}

We propose a new approach that accounts for both word- and discourse-level metaphor. Even if a message does not borrow specific words from another conceptual domain, its broader logic could still implicitly evoke the metaphor \citep{brugman_metaphorical_2019}. While automatic metaphor processing has primarily focused on lexical information, several computational researchers have argued that broader situational and discourse-level information is necessary to study metaphor production and comprehension \citep{mu2019learning,dankers_being_2020}. We calculate separate word-level and discourse-level scores using LLMs and document embeddings, respectively, which are then combined to obtain a single \textit{metaphor score} for each source domain (Figure \ref{fig:metaphor-processing}).




\paragraph{Word-level Metaphor Processing}



We first prompt an LLM to identify and map metaphorical expressions to a source domain (or ``none'' if no concept applies). For example, in Figure \ref{fig:metaphor-processing}, the LLM outputs: \{flooding:\textsc{water}\}. We test two zero-shot prompts (ยง\ref{model-details}). The \texttt{Simple} prompt gives basic instructions and concept names. The \texttt{Descriptive} prompt also provides a definition of metaphor and brief concept descriptions. We evaluate three LLMs: \model{Llama3.1-70B}, \model{GPT-4-Turbo-2024-04-09}, and \model{GPT-4o-2024-08-06}.\footnote{All with temperature = 0.}



We calculate word-level scores for each concept as the number of identified metaphorical expressions ($C(\textsc{concept})$), normalized by log-scaled document length ($C(\textsc{words})$): LLM$_{\textsc{concept}} = \frac{C(\textsc{concept})}{log(C(\textsc{words})+1)}$. Figure \ref{fig:metaphor-processing} has 1 \textsc{water} metaphor in a 6-word tweet, yielding a \textsc{water} score (LLM$_{\textsc{water}}=0.51$). There are 0 \textsc{vermin} metaphors, so LLM$_{\textsc{vermin}}=0$.\footnote{We normalize by length since a short tweet that primarily consists of a metaphor is more ``metaphorical'' than a long tweet that includes a metaphorical word. However, this intuition does not hold linearly. For example, an 8-word tweet containing 2 metaphors (.25) is likely not 5x as  metaphorical as a 40-word tweet containing 2 metaphors (.05).}

\begin{figure}[t!]
    \centering
    \includegraphics[width=.9\columnwidth]{figures/metaphor_scoring.pdf}
    \caption{Our method involves calculating the rate of metaphorical words as detected by an LLM (left side), associations between documents and metaphorical concepts with cosine similarity (right side), and adding these measurements together (bottom).}
    \label{fig:metaphor-processing}
\end{figure}



\paragraph{Discourse-level Metaphor Processing}


In their study of dehumanizing language, \citet{mendelsohn2020framework} %show that geometric relationships between vector representations can capture associations between social groups and dehumanizing metaphors. They 
estimate associations between target groups and the \textsc{vermin} metaphor as the cosine similarity between word2vec vectors of group labels and an average of vermin-related word embeddings. We adopt this approach with several adjustments. First, we use Transformer-based contextualized embeddings 
%instead of static word embeddings
and thus calculate metaphorical associations per document rather than for the entire corpus. Second, we calculate associations between concepts and documents, rather than target group labels. Prior approaches require documents to contain mentions of a target group \citep{mendelsohn2020framework,card2022computational}, but many relevant social media posts about sociopolitical issues do not explicitly name the impacted social groups.%\footnote{We expect this change to lead to an increase in recall for identifying dehumanizing metaphors. However, we also acknowledge the limitation that there may be a trade-off with precision, as our approach does not distinguish between different targets (e.g. in addition to dehumanizing metaphors targeting immigrants, we may capture metaphors targeting non-immigrant outpartisans).}


We encode each tweet and source domain with SBERT using the \model{all-MiniLM-L6-v2} model \citep{reimers2019sentence}. 
%The simplest approach for encoding concepts is to simply embed the concept name (e.g. the word \textit{water}). However, 
Preliminary investigations reveal that just embedding the concept name (e.g. the word \textit{water}) overemphasizes literal usages (e.g. migrants crossing the sea). To encourage the model to identify metaphorical associations, we represent each concept as a set of ``carrier sentences'', which resemble metaphorical usages of the concept but contain little additional semantic information (e.g. \textit{they flood in}; \textit{they hunt them down}). We manually construct 104 carrier sentences based on examples from prior literature (Table \ref{tab:concepts}). Each concept has 8-22 carrier sentences (Table \ref{tab:sentences}). We calculate the discourse-level score with respect to a given concept (EMB$_{\textsc{concept}}$) as the cosine similarity between the tweet and the average of the carrier sentences' SBERT representations.%, yielding scores between 0 and 1. %In Figure \ref{fig:metaphor-processing}, EMB$_{\textsc{water}}=0.21$ and EMB$_{\textsc{vermin}}=0.08$.

We propose a combined score (SUM), simply the sum of the word- and discourse-level scores. SUM tends to put more weight on the presence of metaphorical words. As metaphorical words are relatively sparse, SUM can still leverage discourse-level signalstheir absence. Other combination strategies may achieve higher performance, but such explorations are left for future work.



