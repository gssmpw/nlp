\section{Data}

We use the Immigration Tweets Dataset from \citet{mendelsohn2021modeling}, which has 2.6 million English-language tweets from 2018-2019 that contain a keyword related to immigration (e.g. \textit{immigration}, \textit{illegals}, \textit{undocumented}). The dataset does not contain labels for metaphor, but does include inferred labels for several framing typologies. The dataset further provides data about user engagement and authors, including their inferred location at the country level and inferred political ideology based on social network-based models \citep{compton2014geotagging,Barbera2015}. We select a random sample of 400K tweets whose authors are based in the United States and have an associated ideology estimate and user engagement metrics. 



We create a new dataset for evaluation. Because the prevalence of metaphor is not known a-priori, we select documents by stratified sampling using a baseline model heuristic (\model{GPT-4-Turbo} with a \model{Simple} prompt) (ยง\ref{sampling}). We sample 200 documents for each of the seven source domains, and 200 documents for \textit{domain-agnostic} metaphor, i.e., metaphorical language independent of any particular concept, for a dataset of 1,600 documents.

Our approach considers metaphor on a continuous scale. However, eliciting continuous judgments from humans for a nuanced task such as metaphor identification is difficult. Instead, we collect binary judgments from many annotators and consider ``ground-truth'' labels to be the fraction of annotators who judge the document as metaphorical. We develop a codebook for identifying metaphor associated with each source domain (ยง\ref{annotation}). The codebook was pilot-tested by two authors, who independently labeled 80 tweets (10 per concept and 10 for domain-agnostic metaphor) and had inter-annotator agreement of 0.67 (Krippendorff's $\alpha$).

We recruit participants via Prolific to annotate all 1,600 documents. To simplify the task, participants are assigned to one source domain (or the domain-agnostic condition), provided with the relevant codebook portion, and asked to label 20 tweets with respect to their given concept. They are encouraged to make a binary judgment, but can select a third  ``don't know'' option if needed.\footnote{Participants are based in the United States and have completed at least 200 tasks with a $\geq$99\% approval rate. They are paid \$1.60 per task (\$16/hour). We do not reject any responses through Prolific, but filter out ``don't know'' labels and labels from annotators who (1) completed the full task in under three minutes, or (2) gave the same response for all documents.}

On average, we obtain eight annotations per tweet, and the mean metaphor score is 0.347 (see ยง\ref{annotation} for more detail about annotators and annotations). The overall inter-annotator agreement is 0.32 (Krippendorff's $\alpha$) and varies across concepts (Appendix Fig. \ref{fig:agreement}). While lower than between experts, this agreement is both expected and advantageous for our approach. As metaphor comprehension is closely tied to individual cognition, judgments are bound to vary widely across the population. Such heterogeneity reinforces that metaphor is not a clear binary, supporting our continuous measurement approach. Future cognitive science work could investigate why some documents and concepts elicit more disagreement than others.

