\section{Related Works}


Recent text-to-image diffusion models have demonstrated remarkable capabilities in generating diverse visual concepts \cite{rombach2022high}. While newer foundation models enhance text-image alignment through LLM-generated captions \cite{esser2024scaling, betker2023improving}, the fundamental challenge remains: text-conditioned generation is inherently under-determined, with multiple distinct outputs potentially satisfying the same prompt, making precise control challenging.

Prior work has explored various approaches to enhance generation control. One direction introduces additional conditioning modalities: spatial signals via adapters (ControlNet \cite{zhang2023addingconditionalcontroltexttoimage}), attention-based regional control \cite{chen2024trainingfreeregionalpromptingdiffusion}, and image-based conditioning for identity preservation \cite{ye2023ipadaptertextcompatibleimage, li2023photomaker, shi2024instantbooth} or style transfer via attention manipulation \cite{hertz2024stylealignedimagegeneration}. Another line of research focuses on refining existing images through disentangled text-driven editing \cite{brooks2023instructpix2pix, wu2024turboedit, deutch2024turboedittextbasedimageediting, xu2023infedit, brack2024ledits++, parmar2023zero}, where specific attributes are modified while preserving others, often using spatial masks. However, these text-driven approaches inherit the same under-determination challenges as the base models.

Notably, disentangled control has been more naturally achieved in GANs \cite{gans}, particularly through StyleGAN's low-dimensional latent space \cite{karras2019style}, which exhibits emergent disentanglement even at scale \cite{kang2023scaling}. This has enabled powerful latent space and image editing capabilities \cite{abdal2019image2stylegan, wu2021stylespace, abdal2021styleflow}. While diffusion models offer superior generation quality and diversity, they lack two key advantages of GANs' latent space: continuous, compositional edits and emergent disentanglement. This limitation prevents users from making serendipitous discoveries about visual variations captured in the training data, instead constraining them to variations they can explicitly describe through prompts.

Recent works have explored different approaches to discovering interpretable directions in diffusion models. The weights2weights method~\cite{dravid2024interpreting} learns a manifold of personalized model weights by fine-tuning individual LoRA adapters for each identity and applying PCA to discover a weight space that enables editing. While effective, this requires training separate models per instance. NoiseCLR~\cite{dalva2024noiseclr} learns text embeddings through contrastive learning on a data distribution, but the discovered directions can be arbitrary and lack semantic interpretability. Similarly, Liu et al. ~\cite{liu2023unsupervised} propose unsupervised decomposition of images into compositional concepts by learning multiple embeddings, but their approach often yields redundant or non-semantic directions. In contrast, our work directly learns a small set of semantically grounded sliders that decompose the distribution into interpretable and composable directions, enabling infinite creative variations through systematic exploration of the visual manifold.

\input{figures/method}


Concept Sliders~\cite{gandikota2023concept} addressed continuous control by leveraging LoRA adaptors~\cite{hu2021lora} to learn user-defined attributes. Our work complements this by tackling emergent disentanglement through self-supervised decomposition of the model's inherent variations into composable control dimensions, enabling systematic exploration of the model's creative capabilities.


