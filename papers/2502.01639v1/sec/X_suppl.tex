\clearpage
\appendix
\setcounter{page}{1}
\counterwithin{figure}{section}
\counterwithin{table}{section}
\counterwithin{equation}{section}

\maketitlesupplementary



\section{Principal Component Analysis}
Our analysis reveals that concepts frequently encountered in training data (e.g., "person") exhibit greater variation compared to concepts that are either less diverse or less common (e.g., "Van Gogh art" or "waterfalls"). We demonstrate this by analyzing the principal components for each concept through PCA visualization in Figure~\ref{fig:pca}. Notably, the 50th principal component for the "person" concept shows comparable variational magnitude to the 20th component of "waterfalls," highlighting the inherent variational differences across concepts. By discovering and uniformly sampling these variations, we effectively address the mode collapse problem in models, as shown in Figures~\ref{fig:conceptdiverse1} and~\ref{fig:conceptdiverse2}.
\input{sup-figures/pca}

\section{Effect of TimeStep during Inference}
The temporal application of sliders during inference significantly impacts both the precision and magnitude of image edit(Fig.~\ref{fig:timestep}) for SDXL-DMD SliderSpace. When sliders are applied at all timesteps during inference, we observe strong semantic and structural changes in the generated image. But applying the slider after a few steps helps preserve the image structure while still enabling controlled edits. This latter approach facilitates more precise editing, albeit with subtler semantic alterations that can be amplified by increasing the slider strength parameter.

% Add before the Interpretability & Control section

\subsection{Choice of Semantic Embeddings}
\label{sec:semantic}
While our primary implementation uses CLIP embeddings for semantic decomposition, SliderSpace is compatible with various semantic encoders. Our experiments with alternative embeddings like DINO-v2 and FaceNet demonstrate the framework's flexibility. As shown in Figure~\ref{fig:reb-concept}. DINO-v2 shows comparable overall performance to CLIP, with each encoder exhibiting different strengths across various concepts. For person-specific concepts, using FaceNet embeddings enables the discovery of fine-grained facial semantic directions as seen in Figure~\ref{fig:transfer}

The choice of encoder can be tailored to the target domain - CLIP for general concepts, DINO-v2 for certain visual attributes, and specialized encoders like FaceNet for domain-specific applications. This flexibility allows SliderSpace to adapt to different use cases while maintaining its core benefits of unsupervised discovery and semantic consistency.

\input{rebuttal-figures/concepts}

\subsection{Hyperparameter Analysis}
\label{sec:hyperparam}
We analyze the impact of two key hyperparameters in SliderSpace: the number of PCA directions and the LoRA rank. Our experiments reveal that increasing PCA directions improves both knowledge coverage and output diversity up to about 40 dimensions, after which returns diminish. With just 10 directions, SliderSpace matches the FID scores of 64 manually created Concept Sliders when evaluated against artistic style distributions. Regarding model architecture, we find that lower-rank adaptors (particularly rank-one) efficiently capture variations with a fixed training budget, outperforming higher-rank versions while maintaining better FID scores than Concept Sliders across different ranks.

This analysis guides our choice of using rank-one adapters with 40 PCA directions as the default configuration, offering an optimal balance between performance and computational efficiency.

\input{rebuttal-figures/hyperparams.tex}


\section{User Study}
We conducted user studies to evaluate SliderSpace's effectiveness through Amazon MTurk. For artistic evaluation (Sec~\ref{sec:art_exp}), participants compared two 9-image grids - one generated by SliderSpace using 3 random sliders per image, and another by our baselines. Both sets used identical base prompts: ``a building in a stunning landscape'' and ``a character in a scenic environment''. As shown in Fig~\ref{fig:user_art}, participants rated which grid exhibited greater artistic diversity and utility for art applications. For conceptual evaluation (Sec~\ref{sec:concept_exp}), participants compared image grids based on diversity, generative utility, and creativity (Fig~\ref{fig:user_concept}). Grid presentation order was randomized across all experiments.

\section{Qualitative Results}
\subsection{Art Exploration}
We identify the top-36 distinct art directions discovered by SDXL-DMD2 SliderSpace for the concept "artwork in the style of famous artist" in Figures~\ref{fig:artdmd1} and~\ref{fig:artdmd2}. Additionally, we showcase various combinations of SliderSpace samples in Figures~\ref{fig:artbuilding} and~\ref{fig:artcharacter}, where we randomly sample three sliders to generate images for both characters and buildings (used in our art experiments and user studies in Section~\ref{sec:art_exp}). The top 18 art styles discovered by SDXL-SliderSpace are presented in Figure~\ref{fig:artsdxl}.



\subsection{Diversity Enhancement}
We provide additional qualitative examples demonstrating how our generic diversity sliders mitigate mode collapse in distilled models. Our observations indicate that distilled models such as DMD2~\cite{dmd} tend to generate visually similar images for identical prompts, despite different random seed initializations. Through our trained diversity sliders, which are model-agnostic, we successfully counter mode collapse (Section~\ref{sec:diverse_exp}). As quantitatively validated in Table~\ref{tab:diverse}, the diversity SliderSpace significantly improves image variation, achieving FID scores comparable to the base model.

\subsection{Concept Decomposition}
We present qualitative examples of concept decomposition using the SDXL-DMD2~\cite{dmd} SliderSpace in Figures~\ref{fig:concept1}--\ref{fig:concept6}. Furthermore, we demonstrate SliderSpace's versatility across various models, including SDXL-Turbo~\cite{turbo} (Figures~\ref{fig:turbo1}, SDXL-Base~\cite{podell2023sdxl} (Figures~\ref{fig:artsdxl}), and the state-of-the-art transformer-based FLUX Schnell models (Figures~\ref{fig:intro} and~\ref{fig:flux}). We note that Claude3.5~\cite{claude} generated captions are not always accurate. For instance, in Figure~\ref{fig:concept1}, Claude annotates one of the sliders as ``Black Lab Technician'', but it is not visually distinct whether the slider is `lab technician' or a `scientist'.

\input{sec/6_ablations}

\input{sup-figures/concept_diverse1} 
\input{sup-figures/timestep_effect}

\input{sup-figures/art_individual}

\input{sup-figures/diverse}


\input{sup-figures/user_study}

\input{sup-figures/concept1}
\input{sup-figures/concept2}
\input{sup-figures/concept3}
\input{sup-figures/concept4}
\input{sup-figures/concept5}
\input{sup-figures/concept6}
\input{sup-figures/turbo_concept1}
% \input{author-kit-CVPR2025-v3-latex-/sup-figures/turbo_concept2}
% \input{author-kit-CVPR2025-v3-latex-/sup-figures/turbo_concept3}
% \input{author-kit-CVPR2025-v3-latex-/sup-figures/turbo_concept4}
\input{sup-figures/flux-concept}.




