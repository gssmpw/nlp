\documentclass[10pt,twocolumn,letterpaper]{article}
\usepackage[rebuttal]{cvpr}

% Include other packages here, before hyperref.
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}

% Import additional packages in the preamble file, before hyperref
\input{preamble}


% \usepackage[pagebackref,breaklinks,colorlinks,citecolor=cvprblue]{hyperref}
% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\definecolor{cvprblue}{rgb}{0.21,0.49,0.74}
\usepackage[pagebackref,breaklinks,colorlinks,allcolors=cvprblue]{hyperref}

\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}

% If you wish to avoid re-using figure, table, and equation numbers from
% the main paper, please uncomment the following and change the numbers
% appropriately.
%\setcounter{figure}{2}
%\setcounter{table}{1}
%\setcounter{equation}{2}

% If you wish to avoid re-using reference numbers from the main paper,
% please uncomment the following and change the counter value to the
% number of references you have in the main paper (here, 100).
%\makeatletter
%\apptocmd{\thebibliography}{\global\c@NAT@ctr 100\relax}{}{}
%\makeatother

\newcommand{\rone}[1]{\textcolor{blue}{(3WW8)}} 
\newcommand{\rtwo}[1]{\textcolor{purple}{(9cdw)}} 
\newcommand{\rthree}[1]{\textcolor{teal}{(FdWr)}} 
\makeatletter
\renewcommand{\paragraph}{%
  \@startsection{paragraph}{4}%
  {\z@}{1.25ex \@plus 1ex \@minus .2ex}{-1em}%
  {\normalfont\normalsize\bfseries}%
}
\makeatother

%%%%%%%%% PAPER ID  - PLEASE UPDATE
\def\paperID{7586} % *** Enter the Paper ID here
\def\confName{CVPR}
\def\confYear{2025}

\begin{document}

%%%%%%%%% TITLE - PLEASE UPDATE
\title{SliderSpace: Decomposing the Visual Capabilities of Diffusion Models}  % **** Enter the paper title here

\maketitle
\thispagestyle{empty}
\appendix

%%%%%%%%% BODY TEXT - ENTER YOUR RESPONSE BELOW
\small We thank the reviewers for their insightful feedback. We are encouraged that they found our work to be a significant advancement in generative models \rtwo{}, for discovering directions that reflect the true topology of the model's knowledge space \rtwo{}, and for its straightforward \rone{}, unsupervised approach in systematically uncovering a broader range of latent dimensions \rthree{}. We thank \rone{} \rtwo{} \rthree{} for noting that our framework maintains semantic consistency while providing controllable directions for image generation.

% \small We thank the reviewers for their insightful feedback. We are encouraged that they found our work to be a significant advance in generative models due to its innovative automated workflow \rone{}, for capturing the true topology of the model's knowledge space \rtwo{}, and for its improvement over prior work in systematic discovery of latent dimensions \rthree{}. We thank \rone{} \rtwo{} \rthree{} for noting that our framework provides fine-grained control while maintaining semantic consistency. 

%%%%%%%%% BODY TEXT - ENTER YOUR RESPONSE BELOW


\input{rebuttal-figures/hyperparams}
\noindent\textbf{\rone{} How does unsupervised slider discovery compare to manual discovery with Concept Sliders? }
\textit{With the same number of sliders, SliderSpace captures a much larger portion of the modelâ€™s diversity distribution, covering more dimensions of the knowledge topology}. In Fig.~\ref{fig:reb-hyperparam}, we compare our method to supervised Concept Sliders by training 64 art-specific sliders per concept using Claude-generated training prompts. Samples produced by SliderSpace have significantly better / lower FID (relative to the manually discovered artistic styles of ParrotZone). Thank you for the suggestion, this comparison highlights the significant advantage of unsupervised vs supervised discovery and we will add this discussion to the paper.

\noindent\textbf{\rtwo{}
 Impact of PCA Directions and Rank on SliderSpace:} 
Our analysis in Fig~\ref{fig:reb-hyperparam} reveals that increasing PCA directions improves both knowledge coverage and output diversity, up to about 40 dimensions. Importantly, we find that lower-rank adaptors (particularly rank-one) efficiently capture these variations with a fixed training budget. We note that performance of SliderSpace outperforms Concept Sliders irrespective of rank.
%while minimizing computational overhead. (The dotted line in the plot compares to a Concept Slider baseline using Claude-generated prompts for ``art'' concepts; our method achieves comparable or better results with significantly less computational cost.)

\noindent\textbf{\rtwo{} \rthree{} Can SliderSpace use encodings other than CLIP?}
Yes. We show both Dino-v2 and FaceNet embeddings as alternatives. First, our findings in Fig~\ref{fig:reb-concept} show that Dino-v2 excels for some concepts, while CLIP outperforms in others. In general, both encoders achieve similar overall performance, though they exhibit different strengths. Second, using FaceNet embeddings, we train a ``person'' SliderSpace in Fig~\ref{fig:reb-person} that captures many facial semantic directions.  We will discuss on the generality of SliderSpace to multiple encoders in the paper, and the trade-offs selecting different encoders enable.
%We will discuss these encoder-specific capabilities and trade-offs in our paper.
%offering superior performance in fine-grained visual attribute detection, while CLIP performs better at higher-level semantic concepts. 
\input{rebuttal-figures/person_examples}
\input{rebuttal-figures/concepts}


\noindent\textbf{\rone{} \rthree{} Can sliders trained on one concept be transferred to other?}
Yes. To demonstrate transferability, we train SliderSpace on the general concept ``person'' and discover interpretable semantic directions controlling appearance, accessories, age, and expressions (Fig~\ref{fig:reb-person}). We also show more semantic directions in Appendix D.3. These learned directions successfully transfer to related concepts like ``police'' and ``athlete'' without additional training, enabling slider reuse across related domains. Surprisingly, many sliders even transfer to out-of-domain concepts like ``dog''. We will discuss these findings in the paper.
% On the other hand, training a SliderSpace specific to ``police'' is not expensive and reveals additional concept directions for that specialized distribution (see Appendix Figure D.10).

\noindent\textbf{\rthree{} Can we optimize the computational resources needed for discovering SliderSpace?}
This is an important practical consideration. We note that SliderSpace represents a significant improvement over Concept Sliders, which requires approximately 8 hours to train 64 direction-specific sliders. Our speed-up comes from parallel direction discovery. As suggested by reviewer, by carefully optimizing the pipeline, including reduced PCA sample size and improved VRAM management, we could reduce the end-to-end training time from 2 hours to \textit{65 minutes} on A100 GPU. Future work is warranted to optimize for interactive response times.

\noindent\textbf{\rone{} \rtwo{} \rthree{} Typos and manuscript updates:}
Thank you, we will incorporate all suggested fixes to typo. As suggested by reviewers, we will simplify the related work and background to accommodate additional analysis. The revised manuscript will include the rebuttal experiments: hyperparameter sweep, concept slider comparisons, and ablations of other encoders like Dino and FaceNet as recommended by reviewers. We will add refereces to StyleCLIP and StyleGAN-NADA to discuss the CLIP direction disentanglement through orthogonality.

\end{document}