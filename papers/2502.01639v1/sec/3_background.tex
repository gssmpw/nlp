\section{Background}

\subsection{Latent Diffusion Models}

State-of-the-art text-to-image diffusion models~\cite{sd142022modelcard,sdv22022modelcard,podell2023sdxl, flux} often belong to the class of latent diffusion. Unlike traditional diffusion models that operate in pixel space, latent diffusion models work in a compressed latent space, offering significant computational advantages \cite{rombach2022high}. The diffusion modelling can be formalized as follows:

Let $\mathbf{x}_0$ be an initial image and $\mathbf{x}_T$ be pure Gaussian noise. The forward diffusion process gradually adds noise to the image. The generative process aims to reverse this diffusion, starting from $x_T$ and progressively denoising to reconstruct $\mathbf{x}_0$. At a timestep $t$ the model takes $\mathbf{x}_t$ as input and predicts a noise $\epsilon_t$ such that the next step $\mathbf{x}_{t-1}$:
\begin{equation}
\label{eq:xt}
\mathbf{x}_{t-1} \gets \frac{\mathbf{x}_t - \sqrt{1-\alpha_t}\epsilon_t}{\sqrt{\alpha_t}}
\end{equation}
We can estimate the final image $\Tilde{\mathbf{x}}_{0,t}$ by taking the same direction $\epsilon_t$ for remaining diffusion steps. This can be achieved by recursively applying the denoising from Equation~\ref{eq:xt} with the same direction. We label this ``Final Image Extrapolation'':
\begin{equation}
\label{eq:x0}
\Tilde{\mathbf{x}}_{0,t} \gets \frac{\mathbf{x}_t - \sqrt{1-\bar{\alpha_t}}\epsilon_t}{\sqrt{\bar{\alpha_t}}}.
\end{equation}
This enables us to visualize the final image $\Tilde{\mathbf{x}}_{0,t}$ that the diffusion model is planning of at each timestep $t$ without actually running denoising forward passes through all timesteps.

\subsection{LoRA: Low Rank Adaptors}
Low-rank adaptator (LoRA) \cite{hu2021lora} are a class of light-weight adaptors that are attachable to the weights of the model. Given a pre-trained model layer with weights $W_0 \in \mathbb{R}^{d \times k}$, LoRA decomposes the weight update $\Delta W$ as:

\begin{equation}
\Delta W = BA, \quad B \in \mathbb{R}^{d \times r}, A \in \mathbb{R}^{r \times k},
\end{equation}

\noindent where $r \ll \min(d,k)$ is a small rank that constrains the update to a low-dimensional subspace. This decomposition allows for efficient parameter updates and has shown success in various downstream tasks. However, the application of LoRA to unsupervised discovery of semantic directions in diffusion model weight space remains unexplored.