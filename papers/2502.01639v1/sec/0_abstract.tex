\begin{abstract}

We present SliderSpace, a framework for automatically decomposing the visual capabilities of diffusion models into controllable and human-understandable directions. Unlike existing control methods that require a user to specify attributes for each edit direction individually, SliderSpace discovers multiple interpretable and diverse directions simultaneously from a single text prompt. Each direction is trained as a low-rank adaptor, enabling compositional control and the discovery of surprising possibilities in the model's latent space. Through extensive experiments on state-of-the-art diffusion models, we demonstrate SliderSpace's effectiveness across three applications: concept decomposition, artistic style exploration, and diversity enhancement. Our quantitative evaluation shows that SliderSpace-discovered directions decompose the visual structure of model's knowledge effectively, offering insights into the latent capabilities encoded within diffusion models. User studies further validate that our method produces more diverse and useful variations compared to baselines. 
Our code, data and trained weights are available at \href{https://sliderspace.baulab.info/}{\textcolor[rgb]{0.21,0.49,0.74}{sliderspace.baulab.info}}

% We introduce SliderSpace, a novel framework for unsupervised discovery of continuous, diverse, and perceptually meaningful controls in text-to-image diffusion models. Unlike current control methods which rely on predefined attributes, SliderSpace automatiscally generates visually intuitive slider controls for any given prompt. This allows users to discover surprising possibilities in the model's latent space, and quickly explore the capabilities of a model. Driven by our novel unsupervised training, SliderSpace trains multiple low-rank adapters attached to cross-attention layers, each capturing a distinctive axis of visual variation for a given prompt. We demonstrate the efficacy and versatility of our method across various state-of-the-art text-to-image models, showcasing its applicability in concept decomposition, diversity enhancement, and artistic style exploration. Extensive experiments and user studies reveal that SliderSpace discovers directions which enhance diversity without harming semantic alignment, outperforming existing baselines.
% Our work not only facilitates fine-grained user control, but also opens new avenues for surfacing hidden capabilities within diffusion models.
%Code and pre-trained SliderSpace are available at [project URL].
\end{abstract}