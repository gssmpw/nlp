\section{Method}
\label{sec:method}
We present SliderSpace, a framework for decomposing a diffusion model's visual capabilities into semantically orthogonal control dimensions (Fig.~\ref{fig:intro}). Given a pre-trained text-to-image diffusion model $\theta$ and a prompt $c$, our goal is to discover $n$ independent directions that capture the principal modes of variation in the model's learned distribution.

\subsection{Problem Formulation}

Let $\mathcal{M}_\theta(c)$ denote the manifold of possible images that model $\theta$ can generate for prompt $c$. We aim to identify a set of controllable directions $\{\mathcal{T}_i\}_{i=1}^n$ that:
(1) span the major modes of variation in $\mathcal{M}_\theta(c)$,
(2) maintain semantic consistency across different initializations, and
(3) are mutually orthogonal in semantic space.

Building on recent advances in model adaptation~\cite{gandikota2023concept}, we formulate each control dimension as a LoRA adaptor~\cite{hu2021lora}, updating $\mathcal{T}_i$, where $i \in \{1, ..., n\}$. These lightweight adapters introduce targeted modifications to the model's cross-attention layers, enabling precise control over specific generative attributes. 
\subsection{SliderSpace: Unsupervised Visual Discovery}
SliderSpace discovery process consists of three key steps as depicted in Figure~\ref{fig:method}:

\paragraph{Distribution Sampling} 
First, we generate a diverse set of samples $\{x_j\}_{j=1}^m$ from $\mathcal{M}_\theta(c)$ by varying the random seed (for stability, $m\approx5000$). For each sample, we extract the estimated final image $\tilde{\mathbf{x}}_{0,t}$ at each timestep $t$ using Eq.~\ref{eq:x0}.

\paragraph{Semantic Decomposition}
We map each sample to a semantic embedding space, like CLIP, $\phi(\tilde{\mathbf{x}}_{0,t})$ and compute the principal components $V=\{v_i\}_{i=1}^n$ of the resulting distribution. These components represent orthogonal directions of maximal variation in semantic space:
\begin{equation}
    V = \text{PCA}(\{\phi(\tilde{x}_{0,t})\}_{j,t})
\end{equation}

\paragraph{Slider Training}
For each principal direction $v_i$, we train a corresponding adapter $\mathcal{T}_i$ to induce transformations that align with $v_i$ in semantic CLIP space. The training objective for each slider is:
\begin{equation}
\label{eq:contrast}
    \mathcal{L}_{\text{sliderspace}} = \sum_{i=1}^n 1 - \cos(\Delta\phi_i, v_i),
\end{equation}
where $\Delta\phi_i = \phi(\tilde{\mathbf{x}}_{0,t}^i) - \phi(\tilde{\mathbf{x}}_{0,t})$ represents the transformation induced by slider $i$ in embedding space and $\tilde{x}_{0,t}^i$ is the estimated final image with the slider $\mathcal{T}_i$ attached. This ensures that each slider's effect is semantically aligned with its corresponding principal direction while maintaining orthogonality with other sliders, building on established work showing that embedding differences encode semantic relationships~\cite{gal2022stylegan, patashnik2021styleclip}. 

Our formulation satisfies the three key requirements outlined in Section~\ref{sec:intro}. First, unsupervised discovery is achieved by deriving directions directly from the model's intrinsic variation through PCA in a semantic embedding space, without imposing predefined attributes or external supervision. Second, distribution consistency is enforced by our training objective $\mathcal{L}_{\text{sliderspace}}$, which ensures each slider's transformation maintains consistent direction in embedding space across different seeds and timesteps. Finally, semantic orthogonality is guaranteed through the PCA-based initialization of directions as each principal components are mutually orthogonal, ensuring each slider captures a distinct mode of variation. Together, these components enable the discovery of interpretable and reliable control dimensions that effectively decompose the model's learned distribution. In majority of this paper, we use CLIP~\cite{radford2021learning} as our primary semantic encoder.


\input{figures/concept}
\subsection{Interpretability \& Control}

SliderSpace provides a dual contribution: it serves as both a framework for discovering expressive dimensions of control and as a mechanism for decomposing the model's learned concept space into semantically meaningful components. Each adapter functions as a ``slider'' controlling a specific attribute of the generated image, enabling fine-grained manipulation of the output while maintaining semantics.

The resulting set of adapters provides valuable insights into the model's conceptual understanding, revealing nuanced semantic relationships that may not be immediately apparent from the text prompt alone.

Moreover, the low-rank structure of our adapters ensures computational efficiency and minimal memory overhead, making them particularly suitable for real-time interactive applications. This enables users to explore the concept space dynamically by modulating the influence of each adapter, facilitating intuitive creative control over the image generation process while maintaining the underlying semantic integrity of the original prompt.