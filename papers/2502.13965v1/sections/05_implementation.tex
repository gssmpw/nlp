\section{Implementation}
\label{sec:implementation}
\text{\name} is a multi-engine LLM inference serving system comprising a frontend, scheduler, and load balancer—totaling 5k lines of Python and CUDA/C++ code.

\vspace{1mm}
\noindent \textbf{Frontend.} \text{\name}’s frontend extends OpenAI’s Chat Completion and vLLM’s Python APIs~\cite{vllm, chat_completions} to provide a stateful interface that appears stateless to developers. Users simply import \text{\name}’s library into their Python applications, and upon program initialization, \text{\name} automatically issues a \texttt{start\_session} request to the backend. This operation returns a unique session identifier and creates a corresponding entry in the process table. Subsequent LLM calls are transparently annotated with the appropriate session, program, and thread IDs before being dispatched to the backend. When the program completes or encounters an error, \text{\name} invokes \texttt{end\_session}, removing the associated entry from the process table. As a research prototype, the current frontend lacks safeguards against user modification of the underlying package; addressing this limitation remains future work.

% \text{\name}'s frontend is an extention to OpenAI’s chat completion and vLLM’s Python APIs~\cite{vllm, chat_completions}, feeding stateful interactions to the backend while appearing stateless to users—developers simply import Agentix's package into their Python programs to call LLMs.
% When a program starts, \text{\name} automatically invokes \texttt{start\_session} calls to obtain stateful session IDs from the backend, which creates new records in the process table.
% Each subsequent LLM call from the program is transparently annotated  with program, thread, and session IDs, later parsed by the backend.
% When the program terminates or errors, \text{\name}'s frontend calls \texttt{end\_session}, whic removes the program's entry from the process table.
% As a research prototype, the frontend currently lacks safeguards against user tampering (i.e. modifying the package), for which we defer to future work.

\vspace{1mm}
\noindent \textbf{LLM Engine.} 
\text{\name} builds on vLLM v0.6.1~\cite{vllm}. To keep changes localized, we modify only the scheduler by integrating new policies (\textit{PLAS}, \textit{ATLAS}, and MLFQ) and memory swapping kernels for efficiency. This ensures straightforward experimentation and clear attribution of performance gains. The scheduler follows the algorithm described in the previous section (\S\ref{sec:solution}). We've also noticed in vLLM, each Key-Value (KV) block is transferred individually via \texttt{cudaMemcpyAsync}, creating small fragmented transfers that underutilize PCIe bandwidth and incur high overhead such as repeated DMA setups. To address this, we allocate a host buffer and consolidate all KV blocks into a single contiguous chunk, enabling one bulk transfer. The results are shown in the next section (\S\ref{sec:evaluation}).

% \text{\name} extends vLLM v0.6.1~\cite{vllm} with new scheduler policies (\textit{PLAS}, \textit{ATLAS}, and MLFQ) and a more efficient swapping kernel. Rather than modifying the entire system, these changes are carefully localized to the scheduling and memory management components, ensuring straightforward experimentation and clear attribution of performance gains.

% When a request arrives, the backend confirms its associated program and session identifiers against a centralized process table. This table tracks per-program and per-thread metadata, including service and waiting times, and provides a ready reference for debugging and post-hoc analysis. To streamline lookups, the scheduler maintains its own reference to these entries (§\ref{sec:program-priority}), avoiding excessive cross-component communication.

% In addition, we optimize vLLM’s GPU-CPU swapping procedure. In the baseline system, each Key-Value (KV) block is transferred individually via \texttt{cudaMemcpyAsync}, resulting in numerous small transfers that underutilize PCIe bandwidth and incur excessive context-switching overhead. Our modification batches these KV block indices into a single, bulk transfer operation per swap event. This approach achieves up to a 5× speedup in GPU-CPU data movement (§\ref{sec:evaluation}), allowing the improved scheduler policies to operate without being bottlenecked by memory transfer inefficiencies.

% \text{\name} implements its scheduler algorithm over a fork of vLLM v0.6.1~\cite{vllm}.
% Most of vLLM functionalities remain unchanged, except for the scheduler and swap kernel, which allows us to iterate fast and isolate performance improvements factors. The new scheduler options include \textit{PLAS}, \textit{ATLAS}, and MLFQ policies. When an LLM request arrives from the frontend, the backend verifies the included program and session IDs before processing the request; mismatches trigger immediate rejection. The process table is a centralized data structure containing total service time, waiting time, and thread metadata—covering all call sequences, their thread IDs, and parent relationships—obtained from the frontend. Although the scheduler does not directly depend on these details, they facilitate debugging and dynamic reconstruction of the program’s DAG to analyze waiting time and runtime distributions. The scheduler maintains its own copy of the process table for direct reference(\S\ref{{sec:program-priority}}). Lastly, we implement an efficient swap kernel to achieve up to 5x faster GPU-CPU swapping (\S\ref{sec:evaluation}). We observed that vLLM frequently invokes \texttt{cudaMemcpyAsync} for each KV block. Repeatedly issuing many small transfers reduces PCIe bandwidth utilization and context switching, leading to significant overheads. To address this, we modify vLLM's kernel to collect all block indices and copy them all at once in a single bulk transfer.

\vspace{1mm}
\noindent \textbf{Multi-engine.} vLLM currently lacks the ability to manage multiple LLM engines at the same time. To better evaluate our load balancing strategy, we built \texttt{AsyncMultiLLMEngine} atop of \texttt{AsyncLLMEngine}. Each LLM engine replica runs in a dedicated Python process, and a coordinating meta-engine manages these replicas via standard inter-process communication (IPC) primitives such as \texttt{mp.Queue} and \texttt{mp.Pipe}. When the meta-engine receives a request, it assigns the request to the appropriate replica, returning a future-like object to the frontend without blocking. The selected engine process executes the task asynchronously and sends the completed result back through the IPC channel. Upon receiving the result, the meta-engine resolves the future and provides the output to the frontend. This design allows multiple requests to be processed in parallel, with the meta-engine acting as a non-blocking coordinator that handles routing, resource assignment, and result collection.



% Upon receiving a new request from the frontend, the meta-engine examines the program’s identifier to determine if it has already been assigned to a specific engine. If no assignment exists, the least-loaded engine is selected, and the program-to-engine mapping is recorded. Subsequent requests from the same program are directed to this engine, ensuring data locality and reuse of cached state.

% After determining the target engine, the meta-engine submits the request asynchronously, returning a future-like object to the caller. The sub-engine processes the request and sends the result back to the meta-engine via IPC. The meta-engine then completes the future and returns the result to the frontend. This approach allows multiple requests to be handled concurrently, with the meta-engine serving as a coordinator that routes requests, manages assignments, and gathers results without blocking frontend operations.