
\section{Motivation}
\label{sec:motivation}
Today's AI agent infrastructure decouples LLM serving systems from agentic programs (\S\ref{sec:background}). As organizations shift from serving LLM queries to higher-level AI applications, LLM engines must optimize for program-level objectives, such as response times, or end-to-end latencies~\cite{lin2024parrotefficientservingllmbased}. Formally, a single-threaded program's end-to-end latency comprises three components: (1) \emph{waiting time}, the total queuing time of a program's LLM calls on the engine; (2) \emph{execution time}, the cumulative feedforward time of LLM calls; and (3) \emph{interceptions}, time spent waiting for external interrupts such as tool calls or human input. Since component (3) is unrelated to LLM serving, this section identifies problems and opportunities to reduce waiting (\S\ref{sec:wait_time_reduce}) and execution times (\S\ref{sec:execution_times_reduce}), subsequently addressed in the design of \text{\name}'s scheduling policies (\S\ref{sec:solution}).

\subsection{Program-level Wait Times}
\label{sec:wait_time_reduce}
\input{figures/hol}

Figure~\ref{fig:high_load_wait_times} shows that across various agentic workloads—from classic chatbots to ReAct and MCTS programs—the majority of a program's time is spent waiting as load increases. Hence, \text{\name} prioritizes reducing wait times, which not only improves program's latancies, but also increases LLM engine throughput. Faster call completions prompt programs to issue subsequent calls more quickly, increasing the arrival rate of LLM calls. Figure~\ref{fig:steady_state} illustrates steady-state behavior over a one-hour trace using LLaMA-3.1-8B~\cite{llama3} on a single A100-80GB GPU for entire chatbot conversations~\cite{sharegpt}. Compared to vLLM's first-come, first-served (FCFS) policy, \text{\name} consistently handles 10 additional concurrent LLM calls, offering more batching opportunities to improve throughput.

\vspace{2mm}
\noindent \textbf{Call-level Blocking.} The first challenge is LLM call-level \textit{head-of-line (HoL) blocking}. LLM calls with long decodes delay shorter ones, causing significant wait times~\cite{fastserve}. This issue is evident in serving engines like vLLM~\cite{vllm}, which wait for ongoing calls to finish decoding before scheduling new ones. HoL blocking is severe in our evaluated workloads with long-tailed distributions of decoding steps (Fig.~\ref{fig:trace-analysis}).

To measure blocking, Figure~\ref{fig:hol} measures the ratio of LLM requests' waiting time to execution time for Chatbot and MCTS workloads, as a function of output tokens. For FCFS policy, HoL blocking increases wait times for short LLM calls, increasing the ratio. Preemption, similar to how operating system schedulers interrupt long-running processes, mitigates HoL blocking by favoring shorter LLM calls. Figure~\ref{fig:hol} shows that Multi-Level Feedback Queue (MLFQ), a preemptive algorithm, leads to smaller ratios for short decodes. However, preemption without program-level statistics may not fully resolve the issue, as explained next.

\vspace{2mm}
\noindent \textbf{Program-level Blocking.} The second challenge is \emph{program-level HoL blocking}, where longer programs with many LLM requests delay shorter programs. Existing LLM schedulers are program-agnostic; they schedule individual LLM requests without considering their positions within the overall program, leading to suboptimal decisions. Our evaluation  shows a long-tailed distribution of LLM calls per program, which increases program-level blocking (\S\ref{sec:evaluation}).

To quantify program-level blocking, Figure~\ref{fig:hol} measures the ratio of programs' waiting time to execution time, with respect to number of LLM calls. For both workloads, FCFS and MLFQ incur higher ratios when the number of LLM calls is small, suggesting that short programs wait a long time. Due to this, preemptive scheduling policies, like MLFQ, may perform close to, or even worse, than FCFS (\S~\ref{sec:evaluation}). Without program-level context, MLFQ blindly prioritizes new LLM requests, leading to starvation of shorter programs when long programs' new LLM calls are prioritized.

\subsection{Program-level Execution Times}
\label{sec:execution_times_reduce}
\input{figures/prefix_cache}
A program's execution time largely depends on how efficiently the LLM engine manages the prefill and decoding phases. In agentic workloads, which often feature long, cumulative prefills, \text{\name} focuses on optimizing prefill performance. Specifically, significant portions of prefill computation can be eliminated through prefix caching. This technique stores and reuses relevant key-value (KV) cache entries—such as the system prompt—across LLM requests~\cite{lin2024parrotefficientservingllmbased,zheng2024sglangefficientexecutionstructured}.

\vspace{2mm}
\noindent \textbf{Data Locality.} Figure~\ref{fig:prefix_cache} illustrates the average cache-hit rate as a function of input length. The cache-hit rate is defined as the percentage of precomputed input tokens in the LLM engine's KV cache for an incoming LLM call. Notably, within a single program, cache-hit rates remain above 90\% across all input lengths, indicating that LLM calls within the same program share identical contexts. In contrast, when considering different programs, the cache-hit rate decays exponentially with input length, suggesting that programs only share the system prompt. These results suggest that LLM serving systems across engines should consider a program's \textit{data locality}, as much of its KV cache can be reused for future LLM requests.