\section{Evaluation}
\label{sec:evaluation}
In this section, we analyze representative agentic workloads, evaluate \text{\name}'s performance against state-of-the-art LLM serving systems, and ablate its design choices.

\subsection{Workloads}
\input{figures/trace_analysis}
\input{figures/end2end_single_engine}
\input{figures/end2end_single_tail}
Our real-world experiments evaluate \text{\name} over four representative agentic workloads, which widely vary in the number of decode tokens, prefill tokens, and the LLM calls (Fig.~\ref{fig:trace-analysis}).

\vspace{1mm}
\noindent \textbf{Chatbot Agent: ShareGPT~\cite{sharegpt}.} The ShareGPT dataset comprises of user-generated conversational inputs and outputs, typical for chatbot applications. The number of LLM calls follows a long-tailed distribution with a mean of 6.66 and a max of 80 (Fig.~\ref{fig:num-llm-call-analysis}). ShareGPT's conversational nature is evident in its decode-heavy calls, averaging 277 decode tokens versus 256 prefill tokens, where short prompts generate detailed responses (Fig.~\ref{fig:sharegpt-llm-call-analysis}). Our experiments replay entire conversations as a program rather than the first turn.

\vspace{1mm}
\noindent \textbf{ReAct Agent: BFCL~\cite{berkeley-function-calling-leaderboard}.}
The Berkeley Function Calling Leaderboard (BFCLv3) evaluates LLMs on multi-turn, multi-step tool-usage tasks.  Compared to ShareGPT, BFCL's LLM calls are less long-tailed, with a mean of 10.75 and a maximum of 70 calls per program (Fig.~\ref{fig:num-llm-call-analysis}). BFCL is prefill-heavy, averaging 735.06 tokens per call due to long system prompts and detailed tool signatures, while decodes are short, averaging 34.14 tokens (Fig.~\ref{fig:bfcl-llm-call-analysis}). BFCL thus encapsulates dynamic workflows that alternate between heavy prefills phases and short decodes with function calls.

\vspace{1mm}
\noindent \textbf{Monte Carlo Tree Search: LATS~\cite{zhou2024languageagenttreesearch}.} LATS workloads, derived from running MCTS on HotpotQA~\cite{yang2018hotpotqadatasetdiverseexplainable}, are computationally intensive and involve many parallel LLM calls. Each program instance contains on average 159.7 LLM calls—an order of magnitude more than ShareGPT or BFCL workloads (Fig.\ref{fig:num-llm-call-analysis}). Moreover, the prefill and decoding phase of each call averages 467.2 and 72.6 tokens respectively (Fig.~\ref{fig:lats-llm-call-analysis}). These distributions highlight MCTS’s inherently iterative, parallel nature, pushing LLM serving systems to handle large volumes of concurrent calls efficiently. 

\vspace{1mm}
\noindent \textbf{Mixed.} We combine all three workloads, sampling equally from each to ensure diversity. This workload stress tests \text{\name}'s performance across different program classes.

\vspace{1mm}
\noindent For our experiments, we synthesize a trace by randomly sampling programs, not LLM calls, from the above workloads and generating programs' arrivals using a Poisson process $\lambda$, following established methodologies~\cite{vllm, fastserve}. This approach ensures our setup closely reflects real-world scenarios.

\subsection{Experimental Setup}

\noindent \textbf{Models \& Testbed.} We evaluate on three models: LLaMA-3.1-8B, 70B and Falcon-180B, running on 1, 4, and 8 GPUs, respectively. Experiments are conducted on a GCP Compute Engine \texttt{a2-ultragpu-8g} instance with eight A100-SXM4-80GB GPUs connected via NVLink, 1360 GB host memory, PCIe-4.0×16, and 2 TB of disk space.

% Detailed model sizes and server configurations are shown in Table~\ref{tab:model_config}.
\vspace{1mm}
\noindent \textbf{Metrics.} Existing LLM serving systems focus on request-level metrics, such as Time-to-First-Token (TFTT) and Time-per-Output-Token (TPOT), also referred to as token latency~\cite{vllm, fastserve, nanoflow}. However, these metrics overlook end-to-end latency for agentic programs. To that end, we introduce program-level token latency, defined as the total program response time divided by the number of tokens generated\footnote{For multi-threaded programs, \textit{program-level} token latency is computed as the critical path response time divided by the total tokens across all threads.}. A high-throughput system for programs should retain low program-level latency during high request rates. For simplicity, we refer to our metric as \textit{latency} throughout the evaluation.

\vspace{1mm}
\noindent \textbf{Baselines.} Our evaluation considers three baselines. All baselines, including \text{\name}, use the same max batch size.
\begin{itemize}[itemsep=0pt, parsep=0pt, topsep=0pt, partopsep=0pt, leftmargin=*]
    \item \textbf{vLLM~\cite{vllm}.} vLLM is the state-of-the-art, high throughput LLM serving system that integrates continuous batching~\cite{orca} and PagedAttention~\cite{vllm} to reduce KV cache fragmentation. Its default scheduling policy is FCFS, which is application-unaware and suffers from call-level and program-level HoL blocking. We use vLLM v0.6.1.
    \item \textbf{vLLM-opt.} An optimized version of vLLM that enables chunk-prefill~\cite{chunk_prefill}, prefix-caching~\cite{zheng2024sglangefficientexecutionstructured, lin2024parrotefficientservingllmbased}, and multi-step scheduling. Based on vLLM's blogpost~\cite{vllm2024perfupdate}, it's performance closely matches SGLang~\cite{zheng2024sglangefficientexecutionstructured} and TensorRT~\cite{NVIDIATensorRT-LLM}.
    \item \textbf{MLFQ.} On top of vLLM-opt, it implements preemption via the Multi-Level Feedback Queue algorithm~\cite{fastserve}. This baseline ablates the impact of program and call-level blocking.
\end{itemize}

\subsection{End-to-End Single-Engine Performance}
In Figure~\ref{fig:end2end_single_engine}, we evaluate the end-to-end performance of \text{\name} against three baselines and four workloads: ShareGPT, BFCL, LATS, and Mixed. Across all workloads, \text{\name} consistently achieves the highest throughput given same token latency. Conversely, vLLM performs worst due to its lack of prefix caching, which results in expensive recomputation of cumulative state (Fig.~\ref{fig:prefix_cache}) for LLM calls in the same program. Across workloads, the relative performance between vLLM-opt, MLFQ, and \text{\name} varies.

The first two rows plot the latencies for single-threaded workloads, ShareGPT and BFCL. vLLM and vLLM-opt’s FCFS scheduling causes severe head-of-line (HoL) blocking, which increases latencies as arrival rates increase. In contrast, MLFQ, a preemptive algorithm, mitigates call-level HoL, improving throughput by 1.5× over vLLM-opt. However, at high load, it still suffers from program-level HoL. By employing PLAS to tackle both call- and program-level HoL, \text{\name} achieves up to 8× throughput of vLLM, twice that of vLLM-opt, and a 1.5× improvement over MLFQ under heavy load.

The third row presents results for the multi-threaded LATS workload. \text{\name} outperforms vLLM, MLFQ, and vLLM-opt by up to 5×, 2.5×, and 2×, respectively. Notably, MLFQ’s preemptive scheduling, which benefits single-threaded programs, is less effective in multi-threaded settings. By aggressively prioritizing shorter requests, MLFQ inadvertently disrupts threads in the same program, exacerbating program-level HoL blocking and stalling overall progress. \text{\name}’s ATLAS policy holistically optimizes resource allocation across all threads, maintaining balanced progress and sustaining high throughput under heavy multi-threaded workloads.

The fourth row of Figure~\ref{fig:end2end_single_engine} illustrates performance on mixed workloads. \text{\name} achieves up to 15× higher throughput than vLLM, 5.5× higher than MLFQ, and 4× higher than vLLM-opt. Since \text{\name} reduces program and call-level blocking, \text{\name} performs better as programs' heterogeneity, or the diversity of LLM calls and decode lengths, increases. 

% Since \text{\name} reduces program and call-level blocking, \text{\name} demonstrates robust, high-throughput performance even as workloads grow more complex and demanding.

\input{figures/end2end_multi_engine}
\vspace{1mm}
\noindent \textbf{Tail latency.}
Preemptive scheduling strategies can reduce average latency but risk increasing tail latency by starving long-running programs. Figure~\ref{fig:singe-engine-95} reports the 95\textsuperscript{th} (P95) and 99\textsuperscript{th} (P99) percentile latencies across different workloads on LLaMA-3.1-8B. For ShareGPT, MLFQ significantly improves average latency compared to vLLM-opt (Fig.~\ref{fig:end2end_single_engine}), but exhibits poor P95/99 tail latencies. In contrast, for BFCL, MLFQ outperforms vLLM-opt in both cases. In 7 of 8 scenarios, \text{\name} maintains consistently lower tail latencies than MLFQ and vLLM-opt and improves throughput by up to 1.7× for P95/99 tail latencies, demonstrating robust performance gains in both average and tail performance metrics.

% It's worth noting the impact of workload characteristics on the fairness of preemptive scheduling.
% A potential drawback for preemptive scheduling algorithms is that it can cause starvation of long programs and hurt tail latency. In figure~\ref{fig:singe-engine-95}, we present the 95\% and 99\% agent normalized latency comparison of all systems. By looking at ShareGPT, we can see bad tail latency performance with preemptive MLFQ. Despite achieving substantially better performance than vLLM-opt FCFS in average latency, it performs worse in P95 and similar in P99. It is important to note this is workload dependent. For workloads like BFCL with more balanced LLM call prefill and decode distribution, MLFQ is better than FCFS. But as we can see, regardless of workload characteristics, \text{\name} delivers consistently on-par or better results compared to the baselines. 
 
% In contrast, \text{\name} effectively mitigates resource contention by leveraging its PLAS scheduling strategy to balance workloads dynamically.
% We also evaluation the effectiveness of \text{\name} in offline usecases. We assume programs arrives all at once at the beginning. Figure~\ref{fig:offline-single-engine} shows the average makespan of submitted programs in \text{\name} compared to baselines. \text{\name} has the lowest makespan in all settings, achieving .

\subsection{End-to-End Multi-Engine Performance}   
To evaluate the effectiveness of \text{\name}'s data locality-aware load balancer (\S\ref{sec:load_balancer}), we compare it against two widely used load balancing strategies under identical scheduling policies (\textit{PLAS}, \textit{ATLAS}) for the sake of fairness:
\vspace{1mm}
\begin{itemize}[itemsep=0pt, parsep=0pt, topsep=0pt, partopsep=0pt, leftmargin=*] 
    \item \textbf{Round Robin.} Requests are assigned to engines in cyclic order—ensuring an even distribution of request counts—which is the default load-balancer policy for Kubernetes~\cite{kubernetes}. This strategy ignores data locality, resulting in costly KV cache misses and high recomputation overheads.
    \item \textbf{Least Used.} Requests are assigned to the engine with the lowest number of LLM calls in the system, effectively balancing engine workloads. However, like Round Robin, it neglects data locality and incurs frequent KV recomputations.
\end{itemize}

\vspace{1mm}

\noindent We conduct experiements using four replicas of LLaMA3.1-8B and two replicas of LLaMA3.1-70B with the ShareGPT and LATS workloads. The results, shown in Figure~\ref{fig:two-engine}, demonstrate the \text{\name}'s effectiveness in maintaining low average and tail latencies across all configurations. \text{\name} delivers up to 1.4× higher throughput compared to both baselines. The benefit is more pronounced in ShareGPT workload, where chat history reuse significantly amplifies KV-cache locality. These advantages become even more evident as the number of replicas increases, as a larger pool of engines reduces the likelihood of a request being routed to one with it's locality.


% to larger KV-cache reuse. It is also more significant when we utilize more replicas.

% This improvement demonstrates that a data locality-aware, program-centric distribution policy can significantly enhance system throughput and stability in multi-engine deployments, ensuring that adding replicas translates into meaningful performance improvements rather than diminishing returns.


% As we can see from results above, single engine is not enough to handle heavy and complex programs like MCTS with promising performance. Utilizing multiple engine replicas become the natural choice for handling higher program arrival rates. With multiple replicas, distribution of requests among programs is central for performance. In LLM inference process, data locality of KV cache is crucial for performance, leading to our integration of program-level load balancing policies. Here we explore common alternatives like round robin and load balancing at the LLM call granularity to compare their performance. Figure~\ref{fig:two-engine} shows the results. We can see that our policy performance 1.4x throughput compared to alternatives.

% To test the generality and scalability of \text{\name}, we evaluate its performance as the number of engine replicas increases. As shown in Figure~\ref{fig:scalability}, the system’s ability to maintain an agent-normalized latency near or below 1 scales effectively with the number of replicas. For each additional replica, the system can handle proportionally higher arrival rates before latency degradation sets in. This near-linear scalability underscores that \text{\name}, equipped with the PLAS/ATLAS scheduling policies and program-level load balancing, can effectively utilize added computational resources. It achieves horizontal scaling without being hamstrung by data locality overhead, proving itself as a robust and flexible solution for large-scale LLM serving deployments.
% We also conduct a scalability test to examine the generality of \text{\name}, equipped with PLAS/TLAS and program-level load balancing, in scaling to more engine replicas. As shown in figure~\ref{fig:scalability}, with increased number of replicas, it takes linearly higher number of arrival rates to exceed 1 in agent normalized latency, remarking \text{\name} as a viable option to horizontally scale the serving ability using more computation resources without overhead drawbacks from data locality.


\input{figures/end2end_multi_scalability}

\vspace{1mm}
\noindent \textbf{Scalability.} To evaluate the scalability of \text{\name}, we assess its performance as the number of engine replicas increases under various latency requirements, using the ShareGPT workload with the LLaMA3.1-8B model. Figure~\ref{fig:scalability} shows linear scaling in all cases. Leveraging program-level load balancing, \text{\name} effectively scales horizontally without data locality overhead, making it a robust solution for large-scale LLM deployments.



\subsection{Ablations}

We ablate \text{\name} over different scenarios, including offine batch inference, timing breakdown, and various design choices, such as the swap kernel. All experiments run LLaMA3.1-8B~\cite{llama3} over ShareGPT~\cite{sharegpt} and LATS~\cite{zhou2024languageagenttreesearch}.

\vspace{1mm}
\subsubsection{Offline inference.}
\input{figures/end2end_single_offline}
In offline scenarios that prioritize throughput over latency, large batches of programs are processed in bulk rather than interactively or in a streaming fashion. We consider a use case where all programs are submitted at the start. Figure~\ref{fig:offline-single-engine} presents the makespan of all programs across all systems using the ShareGPT dataset. \text{\name} consistently outperforms the baselines, decreasing the average makespan by 10-40\%. At 4000 programs, MLFQ fails to complete execution. By assigning all new requests to the highest-priority queue, it creates many active LLM requests, causing severe memory contention and frequent GPU-CPU swapping. This overwhelms system resources, resulting in Out-Of-Memory (OOM) errors despite a large swap space (>1.2TB).


\subsubsection{Timing Breakdown}
\input{figures/break_down}
Figure~\ref{fig:break_down} breaks down the time LLM calls spend in the LLM serving layer for \text{\name} and its corresponding baselines. Overall, \text{\name} achieves lower token latency for ShareGPT and LATS by reducing wait and swap times, attributed respectively to \text{\name}'s program-level scheduling policy and improved swap kernels. Due to higher scheduling costs for preemption, both \text{\name} and MLFQ attain higher scheduling times than vLLM-OPT's naive FCFS. Yet, \text{\name} still incurs lower scheduling overhead than MLFQ by incorporating program-level priorities and better distributing LLM calls efficiently across different priority queues. In contrast, MLFQ assigns new LLM calls to the highest-level priority queue by default; hence, a majority of LLM calls reside in high-priority queues, leading to large scheduling overheads.




\subsubsection{Comparison to Optimal Scheduling}
\input{figures/simulator}
Optimal scheduling policies like Shortest Remaining Processing Time (SRPT) assume complete knowledge of each program’s runtime—an unrealistic assumption in practice. Hence, we emulate clairvoyance with a simulator by exposing each program’s total LLM calls and decode steps a priori. The simulation only considers scheduling, where each continuous-batching step is identical. Under these simplified conditions, \text{\name} outperforms FCFS and other preemptive schedulers (e.g., Round Robin, MLFQ). Nevertheless, a noticable gap remains between \text{\name} and SRPT, showing that prior knowledge can significantly boost performance.

\input{figures/end2end_swap}

\vspace{1mm}
\subsubsection{Impact of Swapping Kernel} Preemptive scheduling increases active LLM calls in the system, incurring high GPU memory utilization. This leads to frequent GPU-CPU swaps for fetching relevant KV cache and significant swapping overheads at high request rates~\cite{fastserve}. \text{\name} mitigates this by batching parallel KV block transfers into a single operation—reducing swaps by up to 18x, swap times by 3-7x, and achieving 1.3x higher throughput than vLLM's implemented kernel (Fig.~\ref{fig:agentix_swap_sharegpt_8b}).

