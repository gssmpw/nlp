\begin{abstract}

Large language model (LLM) applications are evolving beyond simple chatbots into dynamic, general-purpose agentic programs, which scale LLM calls and output tokens to help AI agents reason, explore, and solve complex tasks. However, existing LLM serving systems ignore dependencies between programs and calls, missing significant opportunities for optimization.
Our analysis reveals that programs submitted to LLM serving engines experience long cumulative wait times, primarily due to head-of-line blocking at both the individual LLM request and the program.

To address this, we introduce \text{\name}, an LLM serving system that treats programs as first-class citizens to minimize their end-to-end latencies. \text{\name} intercepts LLM calls submitted by programs, enriching schedulers with program-level context. We propose two scheduling algorithms—for single-threaded and distributed programs—that preempt and prioritize LLM calls based on their programs' previously completed calls. Our evaluation demonstrates that across diverse LLMs and agentic workloads, \text{\name} improves throughput of programs by 4-15× at the same latency compared to state-of-the-art systems, such as vLLM.

\end{abstract}


% However, existing LLM serving systems schedule LLM requests without considering dependencies between programs and calls, overlooking substantial opportunities for performance optimization.
% Large language models (LLMs) are evolving beyond simple chatbot applications into highly dynamic and general-purpose agentic programs, which scale LLM calls to help AI agents reason, explore, and solve complex tasks. However, existing LLM serving systems schedule LLM calls without , missing significant opportunities for performance optimization. Our analysis reveals that programs submitted to LLM serving engines experience long cumulative wait times, primarily due to head-of-line blocking at both the individual LLM request and program level.

% Large language models (LLMs) are evolving beyond simple chatbots into dynamic, general-purpose agentic programs, which orchestrate multiple dependent LLM calls and external actions to solve complex, evolving tasks. However, current LLM serving systems overlook these program-level dependencies, often treating each LLM call in isolation. As a result, even short tasks may endure long cumulative wait times due to head-of-line blocking at both the request and program levels. For example, a program that iteratively searches and reasons about web content can spawn many dependent calls, all delayed by earlier long-running requests.

% We introduce Agentix, an LLM serving system that makes programs first-class citizens. Agentix intercepts each LLM call, tracking and leveraging program-level context—such as previously completed calls and the program’s overall execution graph—to guide scheduling decisions. We propose two algorithms for single-threaded and distributed agentic workloads, prioritizing calls from less “served” programs and thus mitigating head-of-line blocking. In our evaluation with diverse LLMs and realistic agentic workloads, Agentix seamlessly integrates with existing serving engines and consistently improves throughput by 4–15× at similar latencies compared to state-of-the-art systems like vLLM.