
\section{Related Work}
\label{sec:related_work}
\vspace{-1.5mm}
\noindent\paragraph{LLM Serving.} 
Orca~\cite{orca} introduces iteration-level scheduling that batches the LLM requests at the token level. vLLM~\cite{vllm} proposes PageAttention~\cite{vllm}
\vspace{-0.5mm}
\noindent\paragraph{LLM Serving Scheduler.} 



% \noindent\paragraph{Job Scheduling.} Cluster managers implement schedulers to orchestrate jobs and tasks over a fixed set of resources with a scheduling policy. First, a cluster scheduler decides when to run a job via a queueing policy, such as First Come First Serve (FCFS) \cite{yarn, spark, mesos}, Shortest Job First (SJF) \cite{alibaba}, Least Attained Service (LAS) \cite{tiresias}, or general priority-based policies \cite{kubernetes, slurm}. Second, a scheduler determines the placement strategy (i.e. which node) via a bin-packing policy, such as First-Fit, Best-fit \cite{slurm}, Worst-fit \cite{kubernetes, borg}. Bin-packing can also occur over single or multiple resource types \cite{drf, tetris, synergy, allox, optimus}.  Finally, schedulers may optimize for various objectives such as job completion time (JCT) \cite{tiresias, synergy}, throughput \cite{gavel, gandiva}, fairness \cite{drf, delay}, and makespan \cite{gavel, synergy}.
% \vspace{-0.5mm}
% \noindent\paragraph{Cluster autoscaling.} To orchestrate resources, cluster managers also implement cluster autoscalers to dynamically adjust cluster size via provisioned cloud instances. The default Kubernetes cluster autoscaler immediately scales cloud instances when tasks (i.e. pods) are pending due to insufficient cluster resources\cite{kubernetes}. Various cloud services, including managed instance groups ~\cite{awsautoscale,gcpautoscale} and custom Kubernetes solutions ~\cite{gke, karpenter}, scale by relying on rule-based mechanisms (e.g. CPU utilization $\geq$ $60\%$) and/or prediction-based methods to predict future bursts. Autoscalers may also optimize instance costs by right-sizing cloud instances ~\cite{kubecost} or managing multiple node pools~\cite{canap}. Other studies~\cite{seagull, sc11autoscaling} focus on cloud bursting for on-premise jobs, emphasizing on cost-efficient data migration and meeting deadlines.
% \vspace{-0.5mm}

% Orthogonally, there exists a large body of work on horizontal and vertical autoscaling, primarily focusing on resizing the number of tasks and task resource requirements per job. Works like Autopilot~\cite{rzadca2020autopilot}, Sinan~\cite{zhang2021sinan}, DS2~\cite{kalavri2018three}, and FIRM~\cite{qiu2020firm} either use direct feedback from workloads to maintain SLOs (e.g. latency or OOM rates) or use a model to predict resource needs. These approaches, which resubmit modified tasks to the scheduler and do not provision compute, are complementary to our work.

% In particular, the constant waiting policy generalizes cluster autoscalers ($C=0$). Existing auto-scaling policies schedule jobs on the cluster when available, and otherwise immediately provisions resources in the cloud when the cluster is full \cite{docker, kubernetes, slurm}.


% Other studies explore cloud bursting for on-premise jobs. For instance, Seagull \cite{seagull} performs cloud bursting for enterprise applications by calculating the cost-benefit tradeoff of each workload and opportunistically migrating VM state to the cloud. Likewise, \cite{sc11autoscaling} schedules workflows on the most cost-efficient instances to minimize cost while meeting deadlines. These instance selection methods from these studies could further enhance Starburst's cost savings.


% \section{Related Work}
% \label{sec:related_work}

% \noindent\paragraph{Waiting Policies.} Our work's closest neighbor is the Waiting Game \cite{waiting_game}, which introduces the concept of a waiting policy and investigates the optimal reserved cluster size that balances costs between reserved and on-demand instances. This research examines various selective and non-selective waiting policies, juggling between cost, reserved cluster provisioning, and Job Completion Time (JCT). One of their proposals, the Constant Wait policy (\S\ref{sec:naive_constant_waiting}), serves as one of our baselines. Our selective waiting policies shift CPU-jobs and jobs in extended queues to the cloud, which is different from the Waiting Game's strategy of moving short-run jobs to the cloud. As we've found that short jobs can account for 10-25\% of total costs, this approach doesn't suit our usecase. Additionally, they propose a balking waiting policy, which necessitates predicting job waiting times. To address this complex task, they utilized a DNN model to predict job waiting times \cite{good_wait}.

% \paragraph{Cluster Autoscaling.}

% There is a large body of work on autoscaling, primarily focusing on resizing workloads to utilize more resources. Works like Autopilot~\cite{rzadca2020autopilot}, Sinan~\cite{zhang2021sinan}, DS2~\cite{kalavri2018three}, and FIRM~\cite{qiu2020firm} either use direct feedback from workloads to maintain SLOs (e.g., latency or OOM rates) or use a model to predict resource needs. These approaches, which aim to prevent over or under-provisioning of the cluster, is orthagonal to our work.

% Other studies explore cloud bursting for on-premise jobs. For instance, Seagull \cite{seagull} performs cloud bursting for enterprise applications by calculating the cost-benefit tradeoff of each workload and opportunistically migrating VM state to the cloud. In contrast, Starburst uses a waiting mechanism to balance cost and job completion time. Likewise, \cite{sc11autoscaling} schedules workflows on the most cost-efficient instances to minimize cost while meeting deadlines. These instance selection methods from these studies could further enhance Starburst's cost savings.

% % \paragraph{DNN Scheduling}
% % Deployment of Deep Learning workloads has given rise to many different schedulers. Gandiva~\cite{gandiva} introduces scheduling mechanisms such as time-slicing, migration and bin-packing to maximize cluster utilization. Some works, such as Optimus~\cite{optimus}, assume job runtime is predictable and allocate resources to minimize JCT, while others, such as Tiresias~\cite{tiresias}, operate under partial job runtime information to minimize the average JCT by discretizing priorities in the job queue. Gavel~\cite{gavel} maximizes the goodput of jobs in a heterogeneous cluster by considering the relative performance differences between different resource types. Starburst \textcolor{red}{TODO - How is DNN scheduling related here?}