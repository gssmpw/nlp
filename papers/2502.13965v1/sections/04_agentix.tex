\section{\text{\name} Design}
\label{sec:solution}

We present \text{\name}'s overall architecture (\S\ref{sec:agentix_overview}) and then explore its two key components: (1) a program-aware scheduler (\S\ref{sec:agentix_policy}) designed to reduce both call-level and program-level blocking, and (2) a data locality–aware load balancer (\S\ref{sec:load_balancer}).

\subsection{Overview} \label{sec:agentix_overview}
\input{figures/architecture}
\text{\name} is a higher-level serving engine designed for agentic programs rather than individual LLM requests.
\text{\name} focuses on three primary objectives: (1) improving overall program's end-to-end latency, for users, (2) maximizing GPU utilization for providers, and (3) mitigating program starvation to improve fairness, measured via 95th and 99th percentile latencies.

 % While existing LLM engines prioritize request-specific metrics—such as Time-to-First-Token (TFTT) and Time-per-Output-Token (TPOT)—

\vspace{1mm} \noindent\textbf{Assumptions.} \text{\name} is non-clairvoyant; it assumes
no knowledge of program arrivals, the structure of executed workflows, or general workload distributions. When a program arrives, its execution DAG is initially unknown; \text{\name} dynamically constructs an internal representation (IR) as the program runs. This flexibility enables \text{\name} to generalize to any program that invokes LLM calls on the underlying engine. While prior work~\cite{lin2024parrotefficientservingllmbased}
submits static LLM applications to the engine, \text{\name} assumes users run general Python programs on their local machines, which invoke \text{\name}'s backend (\S~\ref{sec:implementation}).

\vspace{1.5mm} 
\noindent \textbf{Architecture.} Figure~\ref{fig:architecture} illustrates \text{\name}'s overall architecture. Unlike existing LLM engines, which assumes LLM calls are stateless, \text{\name} is stateful: programs execute from the user's local machine, establish a session with the \text{\name}, and issue LLM calls over time with an associated session ID. We further detail the low-level implementation in Section~\ref{sec:implementation}. When a session starts, \text{\name} adds a corresponding entry to a global process table (\S\ref{sec:agentix_policy}). This table tracks program metadata, including total service time, thread-level metadata, and waiting times across programs' LLM calls. Both the engine-level scheduler (\S\ref{sec:agentix_policy}) and stateful load balancer (\S\ref{sec:load_balancer}) leverage the table to schedule LLM calls for the next decoding batch and route LLM calls to an engine based on their program's data locality. %Finally, \text{\name}' custom memory manager implements better CPU-GPU swapping to mitigate swapping overheads.


\subsection{Program-Aware Scheduler}
\label{sec:agentix_policy}
\input{algorithms/agentix}

We present a general, efficient scheduler designed to minimize programs' response times, or end-to-end latencies, without a-priori knowledge. To mitigate head-of-line blocking at both the program and call levels, \text{\name} assigns priorities to calls based on program-level statistics (e.g., total accumulated runtime, \S\ref{sec:program-priority}) and dynamically preempts calls (\S\ref{sec:preemption}). The complete scheduling algorithm is shown in Alg.~\ref{algorithm:agentix}.

\subsubsection{Program-level Prioritization}
To implement program-level prioritization effectively, \text{\name} relies on a global process table that tracks essential program metrics, enabling more informed scheduling decisions across both single- and multi-threaded programs.
\label{sec:program-priority}

\vspace{1.5mm}
\noindent \textbf{Process Table.} Inspired by traditional operating systems, \text{\name} maintains a global process table that records the state of all running programs. When a new program arrives, \text{\name} adds a corresponding entry; when the program completes, this entry is removed. Each program entry in the process table tracks the following metrics:
\begin{itemize}[itemsep=0pt, parsep=0pt, topsep=0pt, partopsep=0pt, leftmargin=*]
    \item \textit{Service time:} For single-threaded programs, this is the cumulative execution time of all completed calls on the LLM engine's model executor. For multi-threaded programs, it is the longest observed critical path's execution time.
    \item \textit{Waiting time:} The time spent in the LLM engine's scheduler queue—used for anti-starvation.
    \item \textit{Engine ID(s):} The engine(s) that the program is currently running on—used for \text{\name}'s load-balancer. (\S~\ref{sec:load_balancer}).
    \item \textit{Threads Metadata:} Each thread corresponds to an active LLM call. Hence, we keep track of a program's active LLM calls and their individual arrival, waiting, and service times. 
    \item \textit{Most recent call arrival:} The last time a new LLM call arrived for this program—used for tracking stale programs.
    \item \textit{Most recent call completion:} The last time an LLM call finished—used for detecting long external interrupts.
\end{itemize}
When a program's LLM call completes, the table is updated accordingly. With the process table, the scheduler can reason about the global state of each program to schedule LLM calls.

\vspace{1.5mm}
\noindent \textbf{Single-Threaded Programs.}
\input{figures/dag_cp}
Scheduling policies like Shortest-Job-First (SJF) and Shortest-Remaining-Processing-Time (SRPT) minimize response times optimally in single- and multi-server settings~\cite{srpt_optimal, sjf_optimal}. However, these require exact knowledge of program runtimes, violating \text{\name}'s non-clairvoyance assumption. Instead, the Least-Attained-Service (LAS) algorithm~\cite{las}, widely used in information-agnostic settings such as data center networking~\cite{las_datacenter, coflow_dag} and deep learning clusters~\cite{tiresias}, offers a practical alternative.

We introduce \textit{Program-Level Attained Service}, or \textit{PLAS}, extending LAS to programs. For a single-threaded program, its service time is the total runtime of all prior completed LLM calls. Formally, if the $j$th LLM call $c_j$ with program ID of $c_{j}.id$ is submitted, \textit{PLAS} assigns a priority $p(c_j)$ to $c_j$ based on the sum of all runtimes, $t_k$, of all prior LLM calls with the same ID:

\vspace{-2mm}
\begin{equation}
\label{equation:plas}
p(c_j) = \sum_{\substack{k < j \\ c_{k}.id = c_{i}.id}} t_k
\vspace{-2mm}
\end{equation}

\noindent Here, large priority values mean lower priority. To reduce computation, the scheduler reads the program’s total service time from the process table (Line~\ref{alg:line11}). When an LLM call completes, its program’s total service time is updated (Line~\ref{alg:line3}). Thus, \textit{PLAS} naturally favors calls from programs that have received less total service, helping shorter programs finish earlier and reducing response times.

\vspace{1.5mm}
\noindent \textbf{Multi-Threaded Programs.} Unlike single-threaded programs, multi-threaded programs are modeled as dynamic DAGs of LLM calls. Unfortunately, a program’s completion time is dictated by the DAG's \emph{critical path}—the longest sequence of dependent calls from start to finish, illustrated in Figure~\ref{fig:dag_scheduling}. No matter how many parallel LLM calls an engine can process, the program only terminates when all calls along the critical path have finished. Furthermore, without considering critical paths, schedulers achieve sub-optimal completion times for programs; in Figure~\ref{fig:dag_scheduling}, the DAG's makespan increases from 11 to 14 units. 

To address this, we introduce \textit{Adaptive Thread-Level Attained Service (ATLAS)}, a pragmatic generalization of \textit{PLAS}, that prioritizes calls based on their service times along their programs' critical paths. \textit{ATLAS} aims to assign each newly arrived call \( c_j \) a priority \( p(c_j) \) based on the priorities and completed service times of its parents \(\mathcal{P}(c_j)\) in the same program:

\vspace{-5mm}
\begin{equation}
\label{equation:atlas}
p(c_j) = 
\begin{cases} 
0 & \text{if } c_j \text{ is root} \\
\max_{c_k \in \mathcal{P}(c_j)} \{ p(c_k) + t_k \} & \text{otherwise}
\end{cases}
\end{equation}

\noindent Here, \( t_k \) is the execution time of a parent call \( c_k \). By recursively combining parent priorities and runtimes, \( p(c_j) \) estimates the longest chain of accumulated service time leading to \( c_j \), providing a non-clairvoyant estimation of the critical path.

However, achieving both objectives—favoring short programs while also prioritizing the longest, critical-path threads—is nontrivial. To solve this, \textit{ATLAS} maintains a single scalar per program in its process table: the longest observed critical path.  Each active LLM call in a program inherits this value as its initial priority, and upon call completion, updates the scalar only if its own critical path is longer (Line~\ref{alg:line3}). This simple mechanism continuously refines the program’s critical path estimate without tracking dependencies between LLM calls. Consequently, \textit{ATLAS} favors programs and LLM calls with shorter critical paths, effectively approximating a Least-Attained-Service policy for dynamic DAGs. Furthermore, as all calls of a given program derive their priorities from the same entry, the scheduler naturally groups a program’s parallel calls, preventing straggler threads from delaying programs' completion.


\subsubsection{Preemptive Scheduling}
\input{figures/agentix_mlfq}
\label{sec:preemption}
\text{\name} assigns priorities to each LLM call based on their program's history. However, scheduling and preempting programs based on continuous priorities can degrade into worst-case round-robin scheduling~\cite{coflow_dag}, which performs worse than FCFS, and incur unnecessary context switches, including frequent KV-cache swaps between CPU and GPU~\cite{fastserve}. To avoid this, \text{\name} discretizes priorities into a finite set of queues, akin to multi-level feedback queues (MLFQ) in operating systems~\cite{coflow_dag, tiresias, arps_ostep_mlfq}.

\vspace{1.5mm}
\noindent \textbf{Multi-level Program-based Scheduling} \text{\name} bins and discretizes LLM calls' priorities into K queues ($Q_1, Q_2, \dots, Q_K$), where priorities decrease from $Q_1$ to $Q_K$. Each queue $Q_i$ covers a priority range $[Q_i^{lo}, Q_i^{hi})$, with $Q_1^{lo}=0$, $Q_K^{hi}=\infty$, and $Q_{i+1}^{lo}=Q_i^{hi}$.

In Figure~\ref{fig:agentix_mlfq}, when an LLM call arrives, \text{\name} looks up its program’s priority $p(c)$, based on the process table (\ding{172}, Line~\ref{alg:line11}). Unlike traditional MLFQ, where new calls all start at the highest priority queue $Q_1$, LLM calls are assigned to the $i$th queue based on discretized priorities, $p(c)\in[Q_i^{lo}, Q_i^{hi})$ (\ding{173}, Line~\ref{alg:line12}). Subsequently, calls receive the queue's time quantum and execute in FCFS order within their queue (Line~\ref{alg:line13},~\ref{alg:line36}).
Once a call exhausts its quantum, it is demoted to a lower priority queue (\ding{174}, Lines~\ref{alg:line21}-\ref{alg:line24}). If the call waits too long, \text{\name} employs anti-starvation mechanisms, described next (\ding{175}, Lines~\ref{alg:line25}-\ref{alg:line31}). Finally, when a call completes decoding, it updates the process table (Lines~\ref{alg:line17}-\ref{alg:line19}).

\vspace{1mm}
\noindent \textbf{Anti-Starvation.} Discrete prioritization, or MLFQ-style algorithms, incurs the starvation of long, low-priority programs~\cite{tiresias, fastserve, coflow_dag}. Simple anti-starvation techniques—such as promoting calls that have waited past a threshold—reduces \text{\name} to naive MLFQ, where long program's LLM calls, which are now in $Q_1$, interrupt short programs~\cite{fastserve, arps_ostep_mlfq}. Hence, we also utilize the process table to measure program-level starvation. Concretely, for a program $p$, \text{\name} promotes call $c$ to $Q_1$ if the ratio of total waiting time ($W_\text{total} = W_p + W_c$) to service time ($T_\text{total} = T_p + T_c$) exceeds a threshold $\beta$:
\vspace{-2mm}
\[ \frac{W_{\text{total}}}{T_{\text{total}}} \geq \beta \]
\vspace{-3mm}

\noindent Varying $\beta$ presents a trade off between programs' average response times and fairness. After promotion, only $W_c$ and $T_c$, or the calls' wait and run time, are set to zero, to ensure programs' threads, or active LLM calls, are likely all promoted together (Line~\ref{alg:unknown}). 

\vspace{1mm}
\noindent \textbf{Memory Management.} With preemptive scheduling, LLM engines must handle a large volume of concurrent LLM calls, leading to frequent GPU-CPU transfers as KV-cache blocks are repeatedly swapped to serve different requests~\cite{fastserve}. Prior work mitigates this swapping overhead by proactively swapping KV-cache for the next iteration of LLM requests while processing the current ones~\cite{fastserve}. However, \text{\name} is synchronous and requires real-time updates for each call's time quantum and the process table. Instead, \text{\name} employs two key optimizations to reduce both the frequency and overhead of GPU-CPU swapping respectively.

First, \text{\name} reduces total swaps by adopting multi-step scheduling, running the scheduler once every $N$ decoding steps rather than at every step. As some requests may complete early, our scheduler overprovisions queued requests already on the GPU, ensuring that new requests are immediately added when some requests finish before $N$ steps. Second, \text{\name} employs a more efficient GPU-CPU swap kernel. Instead of calling separate asynchronous transfers for each block, our kernel gathers all KV blocks into a contiguous buffer and transfers them in one operation—increasing PCIe bandwidth by reducing fragmentation, reducing per-block overhead, and lowering end-to-end swap latency (\S\ref{sec:implementation}).


\subsection{Load Balancer}
\label{sec:load_balancer}
\input{algorithms/data_locality_loadbalancer}
As agentic workloads scale, deploying multiple engine replicas is necessary. However, distributing requests without considering data locality yields suboptimal performance~\cite{preble}.

Our analysis for agentic workloads (§\ref{sec:execution_times_reduce}) highlights a critical distinction between short and long requests. Short requests below 2048 tokens achieve high cache hit rates ($\geq75\%$) across any engine, due to common system prompts. Enforcing data locality for these requests offers negligible gains and risks skewing engine utilization when large, parallel programs dominate specific engines. Thus, simply balancing short requests across the least-loaded engines preserves performance with minimal overhead. Conversely, longer requests are far more sensitive to their programs' data locality. Their substantial prefix overlap with a given program significantly reduces recomputation when consistently routed to the same engine, justifying occasional queuing delays.

While prior work relies on complex prefix trees to quantify data locality~\cite{preble}, our simple method dynamically routes short requests to the least-loaded engine and pins longer requests to their programs’ corresponding engines. Algorithm~\ref{algorithm:load_balancer} formalizes this approach, and our evaluation shows that \text{\name}’s load balancer improves both throughput and latency across heterogeneous workloads (\S\ref{sec:evaluation}).

% As agentic workloads scale and diversify, employing multiple engine replicas becomes necessary; for instance, a single engine can sustain only about 0.2 MCTS programs per second (§\ref{sec:evaluation}). However, simply distributing requests among replicas does not ensure optimal performance. Without data locality, requests may be routed to engines without the relevant Key-Value (KV) caches, incurring repeated and costly recomputations for prefills.

% Our analysis (§\ref{sec:execution_times_reduce}) indicates that requests differ significantly in their reliance on program-level locality. Smaller requests can achieve high cache hit rates regardless of consistent engine assignment. This is partly due to the substantial contribution of system prompts to their overall input length: when requests are shorter than 2k tokens, even inter-program scenarios yield over 80\% cache hit rates (Figure~\ref{fig:prefix_cache}). Under these conditions, enforcing strict program-level locality provides negligible benefit. Moreover, always applying program-level load balancing can skew utilization when many large programs dominate one engine. For these smaller requests, therefore, it is more efficient to assign them directly to the least-loaded engine, promoting balanced resource usage without imposing unnecessary overheads.

% By contrast, requests exceeding 2k tokens exhibit more extensive prefix overlap within the same program. These larger requests gain substantially from program-level locality, achieving higher cache hit rates and reducing recomputation costs when consistently routed to a single engine. Binding such requests to an established KV cache context is thus beneficial, even if it occasionally introduces short waiting times.

% This hybrid strategy leverages workload heterogeneity: smaller requests, which gain little from locality, are promptly dispatched to lightly loaded engines, while larger requests capitalize on program-affined cache reuse. Algorithm~\ref{algorithm:load_balancer} details this decision process. In §\ref{sec:evaluation}, we demonstrate that this threshold-driven approach improves both throughput and latency, showing that tailoring load balancing decisions to request characteristics is practical and effective in managing diverse LLM agent workloads.


