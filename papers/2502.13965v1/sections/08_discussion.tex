\section{Discussion \& Future Work}
\noindent \textbf{Graph Optimizations.} \text{\name} assumes no prior knowledge of a program’s execution DAG and dynamically constructs the graph as an internal representation (IR) during runtime. While full prior knowledge of a program’s execution is unrealistic, anticipating its immediate next steps can be practical—thereby enabling \textit{compiler optimizations} such as branch prediction and speculative execution, which enables future LLM calls to execute while prior calls are still completing. We defer such optimizations to future works.

\vspace{1.5mm}
\noindent \textbf{Post-Training.} Reasoning models, such as Deepseek-R1~\cite{deepseekai2025deepseekr1incentivizingreasoningcapability} and OpenAI's o1/o3 models~\cite{openai2025o1}, are post-trained via end-to-end reinforcement learning (RL) to optimize the thought process. To accelerate training, distributed RL systems alternate between distributed on-policy sampling and training to collect trajectories and perform policy gradient updates~\cite{sheng2024hybridflow,rllib}. With more effective scheduling, \text{\name} reduces the total makespan for batch sampling for each RL iteration, which immediately benefits distributed post-training systems.