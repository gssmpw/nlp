\section{Background \& Related Work}
\label{sec:background}

To detail relevant context for \text{\name}, we provide a brief overview of the emergent AI agent infrastructure and its applications, split between the LLM serving layer (\S~\ref{sec:llm_serving}) and higher-level agentic layer  (\S~\ref{sec:llm_agents}), as depicted in Figure~\ref{fig:agent_infra}.

\subsection{LLM Serving Layer}
\label{sec:llm_serving}
\noindent \textbf{LLM Inference Process.} Large language models (LLMs), which drive chatbots and AI applications, predominantly utilize the Transformer architecture~\cite{transformer}, including decoder-only models such as GPT, Claude, and LLaMA~\cite{gpt,llama,llama2,mistral}. For each request, LLM inference operates in two stages: the \textit{prefill} phase, which converts the input prompt into intermediate token states, and the \textit{decoding} phase, where new tokens are generated auto-regressively, one at a time, based on prior token sequences. To reduce computation, LLM serving systems leverage \textit{KV-cache}, which stores intermediate token states to accelerate token generation~\cite{orca, vllm}.

\vspace{2mm}
\noindent \textbf{LLM Serving.} LLM serving systems manage both the routing of LLM calls across engines and the execution of LLM calls within each engine (Fig.~\ref{fig:agent_infra}). Within an engine, recent innovations in LLM serving mirror concepts rooted in traditional operating systems (OS), such as memory management, kernel optimization, and scheduling~\cite{mei2024aiosllmagentoperating, sun2024llumnixdynamicschedulinglarge}. Existing solutions, such as vLLM, integrate virtual memory and paging techniques to reduce KV-cache fragmentation~\cite{vllm}, introduce shared memory to cache prefixes across LLM requests~\cite{lin2024parrotefficientservingllmbased,zheng2024sglangefficientexecutionstructured}, and manage cache hierarchies between GPU, CPU, and disk~\cite{nanoflow, flexgen, slora}. Other techniques improve GPU kernel implementations to accelerate self-attention~\cite{flashattention}, pipeline different operators~\cite{nanoflow}, and implement better tensor or pipeline parallelism~\cite{fastserve, alpaserve}. Finally, LLM engines can leverage better scheduling, such as binpacking prefills and decodes together~\cite{agrawal2024tamingthroughputlatencytradeoffllm} and preempting LLM requests~\cite{fastserve}, to improve response times. Across multiple LLM engines, serving systems employ load-balancing techniques like live migration~\cite{llumnix}, disaggregate prefills and decodes~\cite{mooncake}, construct prefix trees~\cite{preble}, and migrate KV caches across engines~\cite{lin2024parrotefficientservingllmbased} to meet request SLOs and improve tail latencies. Overall, the above techniques optimize for \textit{independent LLM requests}, equivalent to a function-call in a general program. Instead, \text{\name} focuses on program-level optimizations, particularly scheduling—akin to how traditional OSs manage entire processes across CPU cores.

\input{figures/agent-infra}
\subsection{Agentic Layer}
\label{sec:llm_agents}

\noindent \textbf{Agentic Programs.}
Above the LLM inference layer, developers build sophisticated \textit{agentic programs} to orchestrate interactions between agents, tools, and humans (Fig.~\ref{fig:agent_infra}). 
Specifically, this work focuses on LLM agents, defined as a tuple consisting of a system prompt specifying the agent's role and the LLM model class\footnote{LLM agents with identical system prompts but different models (e.g., LLaMA~\cite{llama}, Mistral~\cite{mistral}) are considered distinct\cite{mixtureofagents}.}. Similar to traditional OS processes and interrupts, agentic programs either interact directly with the LLM serving layer via LLM calls or engage in external interrupts—time spent outside an LLM engine. Specifically, agents can interact with tools to execute generic functions or external APIs, enabling control over environments such as databases, robotic systems, or the internet~\cite{schick2023toolformerlanguagemodelsteach, patil2023gorillalargelanguagemodel, yao2023webshopscalablerealworldweb,digirl,lmrobots,zhou2024llmenhanceddatamanagement}. Most importantly, agentic orchestration frameworks~\cite{openai_swarm}, such as LangChain~\cite{langchain, langgraph} and Autogen~\cite{wu2023autogenenablingnextgenllm}, provide developers with primitives to manage a program's control flow, determining when to execute agents, invoke tools, or request human input. Such primitives adhere to general programming semantics, including conditional statements, loops, error handling, and terminal conditions~\cite{zheng2024sglangefficientexecutionstructured, langgraph, wu2023autogenenablingnextgenllm, dspy}.
Finally, programs maintain a global history of outputs across agents, tools, and humans~\cite{lin2024parrotefficientservingllmbased,langgraph,memgpt,bufferofthoughts}. For instance, LLM-based chatbots accumulate messages between LLM agents' outputs and humans' inputs~\cite{openai2023gpt4}. Importantly, \text{\name} does not modify the program layer. Instead, it dynamically builds an internal state of the program’s execution graph (DAG) when the program runs, which is stored in a process table (\S\ref{sec:implementation}).

\input{figures/steady_state}
\vspace{2mm}
\noindent \textbf{Agentic Applications.} Beyond standard chatbots (Fig.~\ref{fig:chatbot_agent}), agentic applications, or instantiations of programs, automate or assist with complex tasks, including web or user-interface (UI) navigation (e.g. OpenAI's Operator)~\cite{OpenAIOperator, zhou2024webarenarealisticwebenvironment, gur2024realworldwebagentplanninglong,digirl}, resolving Github issues~\cite{jimenez2024swebench, yang2024sweagentagentcomputerinterfacesenable, wang2024openhandsopenplatformai}, solving IMO-level problems~\cite{deepmind_imo_silver_2024, kumarappan2024leanagentlifelonglearningformal}, fact-checking and summarizing claims from multiple sources (Fig.~\ref{fig:map_reduce_agent})~\cite{genspark2024, lin2024parrotefficientservingllmbased}, and enabling precise robotic control~\cite{rosser2024headsbetteronecollaborative}. Many applications scale inference time compute—the number of LLM calls and, correspondingly, total decode tokens—to improve their performance on complex tasks. These test-time methods include: step-by-step reasoning to decompose tasks~\cite{wei2023chainofthoughtpromptingelicitsreasoning, self-ask}, explicit thought injection to guide reasoning~\cite{yao2023reactsynergizingreasoningacting}, planning or searching to explore possible solutions~\cite{yao2023treethoughtsdeliberateproblem, Besta_2024, zhou2024languageagenttreesearch}, self-critique to evaluate actions~\cite{llmasjudge, stylus}, self-reflection to learn from failures~\cite{shinn2023reflexionlanguageagentsverbal, kumar2024traininglanguagemodelsselfcorrect}, and multi-agent collaboration~\cite{wu2023autogenenablingnextgenllm, societyofminds}. In particular, a single-threaded Reasoning and Acting (ReAct) agent, which combines chain-of-thought (CoT) techniques to efficiently act in an environment (Fig.\ref{fig:react_agent}), has recently been integrated on top of Deepseek-style (or o1-style) LLMs to enable automatic reasoning and tool calling~\cite{deepseekai2025deepseekr1incentivizingreasoningcapability, wang2024openhandsopenplatformai,RAGEN}. A multi-threaded program, Monte Carlo Tree Search (MCTS)~\cite{zhou2024languageagenttreesearch}, integrates parallel planning, self-critique, self-reflection, and multi-agent collaboration (Fig.~\ref{fig:mcts_agent}). Beyond MCTS, distributed programs may also incorporate best-of-N sampling, beam search, lookahead techniques, and genetic algorithms to explore and discover optimal solutions~\cite{chow2024inferenceawarefinetuningbestofnsampling, snell2024scalingllmtesttimecompute, davis2024networksnetworkscomplexityclass, lee2025evolvingdeeperllmthinking}. Given the probabilistic nature of LLMs, the breadth of inference-time techniques indicates that agentic programs and their applications exhibit three properties: (1) \textit{dynamic}, as different user prompts over the same program can yield entirely different execution patterns, (2) \textit{non-deterministic}, since the future is unknown, such as when a program decides to terminate, and (3) \textit{distributed}, with many programs leveraging parallel calls. Hence, \text{\name} is non-clairvoyant, operating with zero prior knowledge of programs' workloads or execution graphs.


% \vspace{2mm}

% \noindent \textbf{Challenges.}

\input{figures/program_wait_times}