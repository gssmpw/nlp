\section{Introduction}
\label{sec:intro}

% Humans generally rely on slower, more deliberate thinking to make better, more reliable decisions~\cite{Evans1984-EVAHAA, Kahneman:2011fj, 01670ebe-b3af-3829-a929-8f19529d1afb}. Analogously, 

Large language models (LLMs) as autonomous agents enhance their problem solving capabilities by scaling their inference computation—that is, increasing the number of output tokens or LLM calls~\cite{Evans1984-EVAHAA, Kahneman:2011fj, 01670ebe-b3af-3829-a929-8f19529d1afb, snell2024scalingllmtesttimecompute, chen2024llmcallsneedscaling, brown2024largelanguagemonkeysscaling}. With more calls and tokens, LLMs endow agents with improved reasoning~\cite{wei2023chainofthoughtpromptingelicitsreasoning, yao2023treethoughtsdeliberateproblem, yao2023reactsynergizingreasoningacting,deepseekai2025deepseekr1incentivizingreasoningcapability}, planning and search capabilities~\cite{zhou2024languageagenttreesearch, putta2024agentqadvancedreasoning}, self-reflection from prior experiences ~\cite{shinn2023reflexionlanguageagentsverbal, yu2024exactteachingaiagents, kumar2024traininglanguagemodelsselfcorrect}, and collaboration between multiple agents~\cite{du2023improvingfactualityreasoninglanguage, wu2023autogenenablingnextgenllm, zhuge2024languageagentsoptimizablegraphs}. These techniques enable agents to effectively navigate external environments via tools~\cite{schick2023toolformerlanguagemodelsteach, patil2023gorillalargelanguagemodel, yao2023reactsynergizingreasoningacting} and solve complex tasks, such as autonomously browsing the web~\cite{yao2023webshopscalablerealworldweb, zhou2024webarenarealisticwebenvironment, gur2024realworldwebagentplanninglong}, resolving GitHub issues~\cite{jimenez2024swebench, yang2024sweagentagentcomputerinterfacesenable, wang2024openhandsopenplatformai}, and proving difficult math problems~\cite{deepmind_imo_silver_2024, kumarappan2024leanagentlifelonglearningformal}. 

\input{figures/agentic_programs}

\input{figures/tutorial}

The rise of inference-time techniques and agentic applications signifies a shift from static, specialized LLM applications~\cite{langchain, lin2024parrotefficientservingllmbased} to highly dynamic, general \textit{agentic programs}~\cite{zheng2024sglangefficientexecutionstructured,wu2023autogenenablingnextgenllm, langgraph}. More precisely, an agentic program is a dynamic execution workflow, represented by a directed acyclic graph (DAG), that consists of LLM calls from one or more agents, and external interrupts, which include tool calls (i.e. external API calls), generic code execution, or human inputs (\S\ref{sec:background}). We assume that the LLM invocation pattern of programs emerges only at runtime, making it difficult to fully know or predict the entire graph in advance.

% \marquita{The preceding is a good intro., level-setting sentence. I suggest moving it to the beginning of the intro. What's there currently unpacks it.} 
Figure~\ref{fig:agentic_program} illustrates the highly dynamic nature of agentic programs with single and multi-threaded examples. Single-threaded programs vary in two dimensions: 1) the length of the program, which depends on the user prompt, and 2) the sequence of LLM calls and interrupts, determined by a program's control flow. For instance, both Chatbot and ReAct (Reasoning and Acting)~\cite{yao2023reactsynergizingreasoningacting} agents cycle between LLM calls and interrupts (human or tool call) and terminate based on a human or LLM's decision. (Fig.~\ref{fig:chatbot_agent},~\ref{fig:react_agent})~\cite{yao2023reactsynergizingreasoningacting}. Multi-threaded programs generally form DAGs. Both Map-Reduce, a classic multi-threaded program, and Monte Carlo Tree Search (MCTS) vary in the number of threads that fork and merge over time, where each thread may contain different sequences of LLM calls and interrupts (Fig.~\ref{fig:map_reduce_agent},~\ref{fig:mcts_agent}). In particular, MCTS is a widely used technique for search and planning for reasoning and web-based agents~\cite{zhou2024languageagenttreesearch, putta2024agentqadvancedreasoning, sequoia2024generative, muennighoff2025s1simpletesttimescaling}.

Existing LLM serving engines, like vLLM~\cite{vllm}, focus on optimizing individual LLM calls or static LLM applications~\cite{lin2024parrotefficientservingllmbased} by improving key-value (KV) cache efficiency~\cite{vllm, zheng2024sglangefficientexecutionstructured}, accelerating CUDA kernels~\cite{nanoflow,fastserve}, and better scheduling algorithms for LLM requests~\cite{fastserve, agrawal2024tamingthroughputlatencytradeoffllm}. However, these optimizations fail to account for the program-level context, such as the dependencies between LLM calls in the same program or program-level statistics, like total execution time. As a result, these systems often suffer from suboptimal end-to-end performance for complex programs—in particular, programs' end-to-end latencies (\S\ref{sec:motivation}).

Figure~\ref{fig:tutorial_example} illustrates a burst of two long programs (A, B) and two short programs (C, D) submitted to an LLM serving engine with a max batch size of 2 at t=0. Each program has one or more LLM calls with varying decoding lengths in Fig.~\ref{fig:program_runtimes}. 
Under a program-agnostic First-Come-First-Served (FCFS) policy, %employed in 
the default policy %setting
for vLLM~\cite{vllm}, long LLM calls block other calls from running, resulting in %incurring 
\textit{call-level} head-of-line (HoL) blocking, as shown in Fig.~\ref{fig:fcfs_example}. Program A and B's initial, long LLM calls execute first, delaying program C and D's execution until t=3,4. Repeated cases of HoL blocking result in a total waiting time of \textbf{18} units. To address this, preemptive scheduling, such as Multi-Level Feedback Queue (MLFQ)~\cite{fastserve}, reduces HoL blocking by preempting long LLM calls to let short calls execute. However, without program-level context, newer programs are repeatedly delayed by subsequent calls from older programs, incurring \textit{program-level} HoL blocking. In Fig.~\ref{fig:mlfq_example}, MLFQ successfully preempts program A and B's long calls to start executing C and D. However, MLFQ repeatedly prioritizes A and B's subsequent calls from t=6-12, which delays program D's execution. Consequently, MLFQ incurs the same wait time of \textbf{18} units as FCFS.

We present \text{\name}, an LLM inference system designed to run programs, not individual LLM calls. Inspired by OS schedulers for processes, our key idea is to prioritize LLM calls by the total execution time of their program's previously completed calls; LLM calls from long programs, which are unlikely to complete soon, are deprioritized, allowing shorter programs to complete first. In Fig.~\ref{fig:agentix_example}, short programs C and D are no longer blocked by subsequent LLM calls from long programs A and B, effectively eliminating HoL blocking and reducing the total wait time to \textbf{12} units. 

\text{\name} introduces a novel framework that leverages global, program-level statistics, such as program's cumulative execution time on an engine, to minimize waiting times and improve engine throughput. We propose two non-clairvoyant scheduling algorithms that assume no prior workload knowledge of programs: \textit{PLAS} (Program-Level Attained Service) for single-threaded programs and \textit{ATLAS} (Adaptive Thread-Level Attained Service) for multi-threaded programs represented as general, dynamic DAGs. \textit{PLAS} prioritizes LLM calls based on the current cumulative service, or execution times, of their source program.
% PLAS prioritizes LLM calls from programs that have consumed less total execution time, and hence short programs finish earlier,
Generalizing \textit{PLAS}, \textit{ATLAS} prioritizes LLM calls based on the maximum cumulative service time across all threads in the same program, which sorts calls based on their program's critical path~\cite{cpm}. Beyond reduced wait times, \textit{ATLAS} decreases program's makespans by prioritizing critical LLM calls that would otherwise block programs' progress (\S\ref{sec:solution}).

Programs comprised of tens to hundreds of LLM calls impose significant demands to the serving systems with a single LLM engine capable of handling only 0.2 programs per second for MCTS (\S\ref{sec:evaluation}). Hence, \text{\name} also routes programs' LLM calls across multiple engines. For agentic workloads, our key observation is that LLM calls within a program often share common prefixes and cumulative conversation states, while calls across programs typically share only the system prompt~\cite{preble}. To avoid recomputing the programs' KV-cache, \text{\name} respects a program's data locality by routing long calls to their programs' engines, while load-balancing shorter calls to other engines, where system prompts make up most of the input for shorter calls.

We implement a system prototype of \text{\name} as a layer on top of LLM serving engines, such as vLLM~\cite{vllm}, and expose a stateful API that allows users to establish persistent sessions with \text{\name}, unlike traditional stateless APIs~\cite{openai_chat_api}. We evaluate \text{\name} across different LLMs and four representative agentic workloads (\S\ref{sec:evaluation}). Our results show that \text{\name} improves throughput by 4-15x compared to state-of-the-art inference systems like vLLM~\cite{vllm}. Across engines, \text{\name} improves throughput by up to 1.5x over standard load-balancers.

\noindent In summary, the primary contributions of this paper are:
\begin{itemize}[itemsep=0pt, parsep=0pt, topsep=0pt, partopsep=0pt, leftmargin=*]
  \item This work is the first to formalize agentic programs as dynamic, non-deterministic DAGs of LLM calls and interrupts. (\S\ref{sec:background}) 
  \item \text{\name} utilizes program-level statistics to better inform its scheduler. \text{\name}'s non-clairvoyant scheduler requires only the cumulative service times of LLM calls within the same program. (\S\ref{sec:solution})
  \item \text{\name} leverages a simple load-balancing policy across multiple engines to balance data locality and KV-cache recomputation. (\S\ref{sec:solution})
  \item Our system is easily deployable, seamlessly integrates a stateful API with existing programming and agent frameworks, and  demonstrates significant throughput gains (\S\ref{sec:evaluation}).
\end{itemize}

% \item \marquita{\text{\name} dynamically builds an internal representation (IR) of the program’s execution graph as the program runs, rather than modifying the program layer. (\S\ref{sec:implementation})} %nice point somewhat hidden in section 2