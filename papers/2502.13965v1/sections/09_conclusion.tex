\section{Conclusion}
\label{sec:conclusion}
We present \text{\name}, a distributed LLM serving system designed for highly-dynamic and general programs, not individual LLM calls. \text{\name}'s key innovation is to leverage program-level statistics, such as the cumulative service times, to better prioritize and schedule LLM calls, thereby improving the end-to-end response times and throughput of programs. We propose two general scheduling algorithms—for single- and multi-threaded programs—and a locality-aware load balancer that effectively reduces programs' waiting and execution times. Our experiments demonstrate that \text{\name} improves throughput of programs by 4×–15× at the same latency compared to state-of-the-art systems like vLLM.

\subsection*{Acknowledgement} We thank Pravein Kannan, Diana Arroyo, and Marquita Ellis from IBM for their insightful discussion. We thank Google Deepmind for funding this project, providing AI infrastructure for us to run experiments. Sky Computing Lab is supported by gifts from Accenture, AMD, Anyscale, Google, IBM, Intel, Microsoft, Mohamed Bin Zayed University of Artificial Intelligence, Samsung SDS, SAP, Uber, and VMware. 