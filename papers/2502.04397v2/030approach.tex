\begin{figure*}[h!t]
\begin{center}
\centerline{\includegraphics[width=1.8\columnwidth]{figures/Figure2-icml_overview-v3.pdf}}
\vspace{-2mm}
\caption{\model is a general multimodal tokenizer of medical codes that can be integrated into any transformer-based model
or a system that requires tokenization. `X-attn' denotes a cross-attention module. }
\label{fig:approach-overview}
\end{center}
\vspace{-5mm}
\end{figure*}

\model is a multimodal medical tokenizer that leverages both text descriptions and relational contexts of medical codes. 
\model operates as a \textit{tokenization function} $f(\cdot)$ that maps a medical code $m \in \mathcal{M}$ to a sequence of elements $\mathcal{T}$ in the \textit{vocabulary} $\mathcal{V}$ with a size of $N$ by leveraging both its textual definition $\mathcal{D}(m)$ and a subgraph $\mathcal{G}(m)$ extracted from a biomedical knowledge graph $G$. Here, $\mathcal{M}$ is a set of 617,490 medical codes from eight medical coding systems: ICD-9, ICD-10-CM, ICD-10-PCS, SNOMED CT, ATC, NDC, CPT, and RxNORM.

\xhdr{Problem definition}
We formulate our problem as follows. Our goal is to train a multimodal tokenizer $f(\cdot)$ so that $\mathcal{T} = f(\mathcal{D}(m), \mathcal{G}(m))$, where $\mathcal{T} = [t_1, t_2, ... , t_T]$ and $t_{i} \in \mathcal{V}, 1 \leq i \leq T$. Then the generated $\mathcal{T}$ for medical code $m$ could be integrated to any EHR-based models $h(\cdot)$ and LMs models $p(\cdot)$ to perform predictive or generative tasks.

\xhdr{Overview}
Fig. \ref{fig:approach-overview} illustrates the architecture of \model, which takes both the medical code description and contextual knowledge from biomedical KGs as input. \model takes two steps, multimodal tokenization and token packing.

\subsection{Multimodal Tokenization}
Given a medical code $m$, paired with its description $t$ and its biological subgraph $G$, \model first adopts the text encoder, denoted as $\mathrm{E}_t$ and the graph encoder, denoted as $\mathrm{E}_g$, to generate two embeddings: the text semantic embedding $\mathbf{x}_{t} \in \mathbb{R}^{d_t}$ for $t$ and the graph-level embedding $\mathbf{x}_{g} \in \mathbb{R}^{d_g}$ for $G$. These embeddings are computed as $\mathbf{x}_{t} = \mathrm{E}_{t}(t)$ and $\mathbf{x}_{g} = \mathrm{E}_{g}(G)$ for $G$, where $\mathrm{E}_{t}$ and $\mathrm{E}_{g}$ represent the text and graph encoders, respectively. 

\xhdr{Modality-specific Embeddings} \model then adopts two linear projectors: $f_t : \mathbb{R}^{d_t} \rightarrow \mathbb{R}^{d}$ and $f_{g} : \mathbb{R}^{d_g} \rightarrow \mathbb{R}^{d}$, to generate modality-specific embeddings $\mathbf{e}_{t}^{s} \in \mathbb{R}^{d}$ and $\mathbf{e}_{g}^{s} \in \mathbb{R}^{d}$, respectively, where $\mathbf{e}_{t}^{s} = f_t(\mathbf{x}_{t})$, $\mathbf{e}_{g}^{s} = f_g(\mathbf{x}_{g})$, and $d$ is the dimension of specific embeddings.

\xhdr{Cross-modality Embeddings} 
Moreover, \model incorporates a cross-attention module to derive cross-modality embeddings $\mathbf{e}_{t}^{c} \in \mathbb{R}^{d}$ and $\mathbf{e}_{g}^{c} \in \mathbb{R}^{d}$, Specifically, the embedding $\mathbf{e}_{t}^{c}$ is computed as:
\begin{equation}
\mathbf{e}_{t}^{c} = \mathrm{softmax} \left( \frac{\mathbf{W}_q \mathbf{x}_{t} (\mathbf{W}_{k} \mathbf{x}_{g})^{T}}{\sqrt{d}} \right) (\mathbf{W}_{v} \mathbf{x}_{g})
\end{equation}

Similarly, the embedding $\mathbf{e}_{g}^{c}$ is given by:
\begin{equation}
\mathbf{e}_{g}^{c} = \mathrm{softmax} \left( \frac{\mathbf{W}_q \mathbf{x}_{g} (\mathbf{W}_{k} \mathbf{x}_{t})^{T}}{\sqrt{d}} \right) (\mathbf{W}_{v} \mathbf{x}_{t})
\end{equation}
where $\mathbf{W}_q, \mathbf{W}_k, \mathbf{W}_v \in \mathbb{R}^{d \times d}$ represents the query, key, and value weight matrix.

\xhdr{Tokenization}
After generating modality-specific and cross-modality embeddings, for each embedding, \model quantizes the embedding into $K$ tokens by querying a unified codebook $\mathbf{C} \in \mathbb{R}^{N \times d}$. The $K$ tokens are identified by the top $K$ nearest vectors in the codebook. 

In detail, for any modality-specific or cross-modality embedding $\mathbf{e}_{:}$, its quantized tokens $\mathcal{I}(\mathbf{e}_{:})$ is formulated by:
\begin{equation}
\label{k_tokens}
    \mathcal{I}(\mathbf{e}_{:}) = \mathrm{argmin}_{K}\left( \left\{\text{dist}(\mathbf{e}_{:}, \mathbf{C}_i)\right\}_{i=1}^{N} \right)
\end{equation}
where $\text{dist}(:, :)$ denotes the Euclidean distance, $| \mathcal{I}(\mathbf{e}_{:}) | = K$, and $\mathbf{C}_{i} = \mathbf{C}[i,:]$. Then \model assigns a weight to each token $k \in \mathcal{I}(\mathbf{e}_{:})$ based on the distance between \( \mathbf{e}_{:} \) and its corresponding vector \( \mathbf{C}_{k} = \mathbf{C}[k,:] \). These weighted tokens are then summed together to obtain the quantized vector for \( \mathbf{e}_{:} \), denoted as \( \mathbf{\hat{e}}_{:} \), which is given by:
\begin{equation}
    \mathbf{\hat{e}_{:}} = \sum_{k \in \mathcal{I}(\mathbf{e}_{:})} -\mathrm{Softmax}(\mathrm{dist}(\mathbf{e}_{:}, \mathbf{C}_{k})) * \mathbf{C}_{k}
\end{equation}
%
%introduce the loss here
Following vector quantization conventions, we employ a straight-through gradient estimator: $\mathbf{e}_{:} = \mathrm{sg}[\mathbf{e}_{:} - \mathbf{\hat{e}}_{:}] + \mathbf{\hat{e}}_{:}$ where $\mathbf{sg}[\cdot]$ denotes the stop-gradient operation. The codebook learning objective is $\mathcal{L}(\mathbf{e}_{:}, \mathbf{\hat{e}}_{:}) = \|\mathrm{sg}[\mathbf{\hat{e}}_{:}] - \mathbf{e}_{:}\|_2^2 + \alpha * \|\mathbf{\hat{e}}_{:} - \mathrm{sg}[\mathbf{e}_{:}]\|_2^2$, where $\alpha$ is a hyperparameter.

To preserve the distinctiveness of modality-specific and cross-modality embeddings, \model divides the entire codebook into three regions: a text-specific region, a graph-specific region, and a shared region, and then queries distinct regions of the codebook to generate their respective tokens and quantized vectors, which are represented by: ($\mathcal{I}(\mathbf{e}_{t}^{s})$, $\mathcal{I}(\mathbf{e}_{g}^{s})$, $\mathcal{I}(\mathbf{e}_{t}^{c})$, $\mathcal{I}(\mathbf{e}_{g}^{c})$) and (\( \mathbf{\hat{e}}_{t}^{s} \), \( \mathbf{\hat{e}}_{g}^{s} \), \( \mathbf{\hat{e}}_{t}^{c} \), \( \mathbf{\hat{e}}_{g}^{c} \)). Consequently, the codebook learning objective is rewritten by:
\begin{equation}
    \mathcal{L}_{C} = \mathcal{L}(\mathbf{e}_{t}^{s}, \mathbf{\hat{e}}_{t}^{s}) + \mathcal{L}(\mathbf{e}_{g}^{s}, \mathbf{\hat{e}}_{g}^{s}) + \mathcal{L}(\mathbf{e}_{t}^{c}, \mathbf{\hat{e}}_{t}^{c}) + \mathcal{L}(\mathbf{e}_{g}^{c}, \mathbf{\hat{e}}_{g}^{c})
\end{equation}

\subsection{Token Packing}
Unlike image-text paired data, where modalities have significant overlap, the two modalities (text and graph) of medical codes used in this work are more distinct yet complementary: the text focuses on clinical definitions, while the graph encodes domain-specific relationships not fully conveyed through text alone. Therefore, in addition to capturing shared information, it is crucial to extract modality-specific information during the tokenization process to ensure that the resulting tokens are highly informative.

Inspired by \cite{wang2024information}, we optimize the obtained tokens ($\mathcal{I}(\mathbf{e}_{t}^{s})$, $\mathcal{I}(\mathbf{e}_{g}^{s})$, $\mathcal{I}(\mathbf{e}_{t}^{c})$, $\mathcal{I}(\mathbf{e}_{g}^{c})$) by separately optimizing the shared and modality-specific information across these tokens and their corresponding quantized vectors. 

For shared information, given the tokens $\mathcal{I}(\mathbf{e}_{t}^{c})$ and $\mathcal{I}(\mathbf{e}_{g}^{c})$, the objective first is to adopt Kullbackâ€“Leibler (KL) divergence optimize them by ensuring their distance matrices \(\mathrm{dist}(\mathbf{e}_{t}^{c}, \mathbf{C})\) and \(\mathrm{dist}(\mathbf{e}_{g}^{c}, \mathbf{C})\) follow a similar distribution, as following:
\begin{equation}
    \mathcal{L}_{\text{KL}} = D_{\text{KL}}(\text{softmax}(\mbox{-}\mathrm{dist}(\mathbf{e}_{t}^{c}, \mathbf{C}))\parallel\text{softmax}(\mbox{-}\mathrm{dist}(\mathbf{e}_{g}^{c}, \mathbf{C}))) \nonumber
\end{equation}
Then optimize the quantized vectors \(\mathbf{\hat{e}}_{t}^{c} \), \( \mathbf{\hat{e}}_{g}^{c} \) to be highly informative about the other modality, while minimizing redundancy, as follows:
\begin{equation}
\mathbf{\hat{e}}_{t}^{c*} = \arg\max_{\mathbf{\hat{e}}_{t}^{c}} \left( I(\mathbf{\hat{e}}_{t}^{c}; \mathbf{e}_{g}^{c}) - \beta \cdot I(\mathbf{\hat{e}}_{t}^{c}; \mathbf{e}_{t}^{c} | \mathbf{e}_{g}^{c}) \right)
\end{equation}
\begin{equation}
\mathbf{\hat{e}}_{g}^{c*} = \arg\max_{\mathbf{\hat{e}}_{g}^{c}} \left( I(\mathbf{\hat{e}}_{g}^{c}; \mathbf{e}_{t}^{c}) - \beta \cdot I(\mathbf{\hat{e}}_{g}^{c}; \mathbf{e}_{g}^{c} | \mathbf{e}_{t}^{c}) \right)
\end{equation}
%
For specific information, given the tokens $\mathcal{I}(\mathbf{e}_{t}^{s})$ and $\mathcal{I}(\mathbf{e}_{g}^{s})$, the objective is to optimize these tokens by ensuring that the quantized vectors \( \mathbf{\hat{e}}_{t}^{s} \), \( \mathbf{\hat{e}}_{g}^{s} \) retain as much modality-specific information as possible, with minimal shared information, as follows:
\begin{equation}
    \mathbf{\hat{e}}_{t}^{s*} = \arg \max_{\mathbf{\hat{e}}_{t}^{s}} \left( I(\mathbf{\hat{e}}_{t}^{s}, \mathbf{\hat{e}}_{g}^{c}; \mathbf{e}_{t}^{s}) - \lambda \cdot I(\mathbf{e}_{t}^{s}; \mathbf{\hat{e}}_{g}^{c*}) \right)
\end{equation}
\begin{equation}
    \mathbf{\hat{e}}_{g}^{s*} = \arg \max_{\mathbf{\hat{e}}_{g}^{s}} \left( I(\mathbf{\hat{e}}_{g}^{s}, \mathbf{\hat{e}}_{t}^{c}; \mathbf{e}_{g}^{s}) - \lambda \cdot I(\mathbf{e}_{g}^{s}; \mathbf{\hat{e}}_{g}^{c*}) \right)
\end{equation}
%
Based on the derivation of \cite{wang2024information}, the common tokens could be optimized by the combination of InfoNCE loss between $\mathbf{\hat{e}}_{t}^{c}$ and $\mathbf{\hat{e}}_{g}^{c}$, and the alignment loss between $\mathbf{e}_{t}^{c}$ and $\mathbf{e}_{g}^{c}$. Then the loss for packing shared information across two modalities is formulated by: $\mathcal{L}_{token}^{c} = \mathcal{L}_{\mathrm{InfoNCE}}(\mathbf{\hat{e}}_{t}^{c}, \mathbf{\hat{e}}_{g}^{c}) + \mathcal{L}_{\mathrm{InfoNCE}}(\mathbf{\hat{e}}_{g}^{c}, \mathbf{\hat{e}}_{t}^{c}) - 2\beta \mathbb{E}_{\mathbf{e}_{t}^{c}, \mathbf{e}_{g}^{c}}(\mathbf{e}_{t}^{c} \cdot \mathbf{e}_{g}^{c})$. 
Additionally, the modality-specific tokens could be optimized by the combination of InfoNCE loss between $\mathbf{\hat{e}}_{t}^{c}$, $\mathbf{\hat{e}}_{g}^{c}$ and the quantized vectors of their augmented data, and the orthogonal loss between $\mathbf{\hat{e}}_{t}^{c}$, $\mathbf{\hat{e}}_{g}^{c}$, and $\mathbf{e}_{t}^{c}$, $\mathbf{e}_{g}^{c}$, respectively. Then the loss for packing specific information across two modalities is formulated by: $\mathcal{L}_{token}^{s} = \mathcal{L}_{\mathrm{InfoNCE}}(\mathbf{\hat{e}}_{t}^{c}, \mathbf{\tilde{e}}_{t}^{c}) + \lambda \mathcal{L}_{\mathrm{orthogonal}}(\mathbf{\hat{e}}_{t}^{c}, \mathbf{e}_{t}^{c}) + \mathcal{L}_{\mathrm{InfoNCE}}(\mathbf{\hat{e}}_{g}^{c}, \mathbf{\tilde{e}}_{g}^{c}) + \lambda \mathcal{L}_{\mathrm{orthogonal}}(\mathbf{\hat{e}}_{g}^{c}, \mathbf{e}_{g}^{c})$. 

Finally, we combine the shared and specific losses to form the overall token packing loss, $\mathcal{L}_{token} = \mathcal{L}_{token}^{c}+\mathcal{L}_{token}^{s}$, to optimize our obtained tokens, where $\beta$ and $\lambda$ are hyperparameters set to be equal. This approach enables \model to leverage both modality-shared and modality-specific information. 
% Further details are provided in Appendix~\ref{appendix_loss}.

\subsection{Training and Inference for \model}
\xhdr{Training Stage}
During the training stage, \model is trained by the sum of codebook loss $\mathcal{L}_{C}$, KL divergency loss $\mathcal{L}_{KL}$, loss for packing common and specific tokens across two modalities $\mathcal{L}_{token}$, where $\mathcal{L} = \mathcal{L}_{C} + \mathcal{L}_{KL} + \mathcal{L}_{token}$.

\xhdr{Inference Stage}
After pre-training, \model can be seamlessly integrated into any model or pipeline dealing with medical codes, providing unified medical tokens for downstream tasks.