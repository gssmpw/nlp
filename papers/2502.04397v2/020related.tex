\xhdr{Domain-specific Tokenizers} 
Tokenizers tailored for specific domains have been employed to process various types of data, including language~\cite{bpe,sentencepiece,wordpiece,Wang2024challenging,Minixhofer2024zeroshot}, images~\cite{ibot,vqgan,Yu2024difftok,Zha2024textok}, videos~\cite{Choudhury2024dontlook}, graphs~\cite{Perozzi2024graphtalk,vqgraph}, and molecular and material sciences~\cite{Fu2024moltok,Tahmid2024birna,Qiao2024mxdna}. While these tokenizers perform well within their respective domains, they are not directly applicable to medical codes, which contains specialized medical semantics. Medical codes reside in relation contexts and are accompanied by textual descriptions. Directly using the tokenizers for languages risks flattening the relationships among codes and failing to preserve the biomedical information. This will lead to fragmented tokenization of medical codes, resulting in loss of contextual information during encoding.
Meanwhile, visual tokenizer typically focus on local pixel-level relationships, which are insufficient for capturing the complex semantics inherent in medical codes. Graph tokenizers are designed to encode structured information from graphs into a discrete token, then enabling LLMs to process relational and topological knowledge effectively. However, graph tokenizers may suffer from information loss when applied to graphs in other domains, making them less flexible and efficient for large, dynamic, and cross-domain graphs. In contrast, our \model tokenizer explicitly incorporates the relevant medical semantics by integrating textual descriptions with graph-based relational contexts.


\xhdr{Vector-Quantized Tokenizers}
Tokenization strategies often vary according to the problem domain and data modality where recent work has highlighted the benefits of discrete tokenization~\cite{du2024role}. This process involves partitioning the input according to a finite set of tokens, often held in a \textit{codebook} (this concept is independent of medical coding despite the similar name), and the quantization process involves learning a mapping from input data to the optimal set of tokens according to a pre-defined objective such as reconstruction loss~\cite{van2017neural}. 

Recent work has highlighted the ability of vector quantized (VQ-based) tokenization to effectively compress semantic information\cite{gu2024rethinking}. This approach is particularly successful for tokenizing inputs with an inherent semantic structure such as graphs~\cite{yang2023vqgraph, wang2024learning}, speech~\cite{zeghidour2021soundstream, baevski2019vq}, and time~\cite{yu2021vector} as well as complex tasks like recommendation retrieval \cite{wang2024learnable, rajput2023recommender, sun2024learning} and image synthesis \cite{zhang2023regularized, yu2021vector}.

Another significant advantage to VQ-based tokenization is the natural integration of multiple modalities. By learning a shared latent space across modalities, each modality can jointly modeled using a common token vocabulary \cite{agarwal2025cosmos, yu2023language}. 
% I think there are probably better citations to use for the line above
TokenFlow leverages a dual-codebook design that allows for correlations across modalities through a dual encoder~\cite{qu2024tokenflow}.


\xhdr{Structured EHR, transformer-based, and foundation models} 
%
Structured EHR models leverage patient records to learn representations for clinical prediction and operational healthcare tasks. These models differ from medical LLMs~\cite{singhal2025toward,tu2024towards,singhal2023large}, which are typically trained on free-text clinical notes~\cite{jiang2023health} and biomedical literature rather than structured EHR data.  
%
BEHRT~\cite{li2020behrt} applies deep bidirectional learning to predict future medical events, encoding disease codes, age, and visit sequences using self-attention. TransformEHR~\cite{transform_ehr} adopts an encoder-decoder transformer with visit-level masking to pretrain on EHRs, enabling multi-task prediction. GT-BEHRT~\cite{gtbehrt} models intra-visit dependencies as a graph, using a graph transformer to learn visit representations before processing patient-level sequences with a transformer encoder.  
%
Other models enhance EHR representations with external knowledge. GraphCare~\cite{graphcare} integrates large language models and biomedical knowledge graphs to construct patient-specific graphs processed via a Bi-attention Augmented Graph Neural Network. Mult-EHR~\cite{mult_ehr} introduces multi-task heterogeneous graph learning with causal denoising to address data heterogeneity and confounding effects. ETHOS~\cite{ethos} tokenizes patient health timelines for transformer-based pretraining, achieving zero-shot performance.  
%
While these models focus on learning patient representations, \model serves a different role as a medical code tokenizer. It can be integrated into any structured EHR, transformer-based, or other foundation model, improving how medical codes are tokenized before being processed. Unlike these models, which rely on predefined tokenization schemes, \model optimizes the tokenization process itself.