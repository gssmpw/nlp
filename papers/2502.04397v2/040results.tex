\xhdr{Medical coding systems}
We collected a total of 617,490 medical codes from eight commonly used coding systems: ICD-9 \cite{world1988international}, ICD-10-CM \cite{fung2020new}, ICD-10-PCS \cite{averill2001development}, SNOMED CT \cite{donnelly2006snomed}, ATC \cite{miller1995new}, NDC \cite{palmer2006ndc}, CPT \cite{dotson2013cpt}, and RxNORM \cite{nelson2011normalized}, as shown in Table \ref{tab:code_systems}. These codes cover various events, including procedures, diagnoses, and medications. Each code is paired with a textual description from official documents and a subgraph from PrimeKG \cite{primekg}. Details for data pre-processing are available in Appendix \ref{appendix_dataset}.

\begin{table}[ht]
\centering
\footnotesize % or \scriptsize
\renewcommand{\arraystretch}{1.1}
\begin{tabular}{@{}l r @{\hspace{1.2cm}} l r@{}}
\toprule
\textbf{Code System} & \textbf{Count} & \textbf{Code System} & \textbf{Count}\\
\midrule
SNOMED       & 303,325 & ICD9           & 18,365 \\
ICD10-CM     & 81,184  & CPT            & 10,602 \\
RxNorm       & 81,151  & ATC            & 6,659  \\
ICD10-PCS    & 61,644  & NDC          & 54,560 \\
\bottomrule
\end{tabular}
\caption{Summary of the dataset's code systems distribution.}
\label{tab:code_systems}
\end{table}

\xhdr{Patient EHR datasets}
We used three publicly available EHR datasets: MIMIC-III \cite{mimiciii}, MIMIC-IV \cite{mimiciv}, and EHRShot \cite{wornow2023ehrshot}. MIMIC-III and MIMIC-IV are in-patient datasets with medical records for ICU patients, while EHRShot is a dataset containing longitudinal medical records that include both out-patients and ICU/ED patients. MIMIC datasets include NDC medications and ICD-9 / ICD-10 codes for diagnoses and procedures. In contrast, EHRShot mainly uses RxNorm codes for medications, SNOMED codes for diagnoses, and CPT, SNOMED, ICD-9, and ICD-10 codes for procedures. Table \ref{tab:stats-ehr} summarizes the statistics of three EHR datasets.

\begin{table}[h!t]
    \centering
    % Use \resizebox{\columnwidth}{!}{...} to force the table to fit the width of one column
    \resizebox{\columnwidth}{!}{%
    \begin{tabular}{lcccc}
        \toprule
         & \#patients & \#visits & \#visits/patient & \#events/patient \\
        \midrule
        MIMIC-III & 35,707 & 44,399 & 1.24 & 51.14 \\
        MIMIC-IV  & 123,488 & 232,263 & 1.88 & 70.33 \\
        EHRShot & 6,739 & 921,499 & 136.74 & 6182.17 \\
        \bottomrule
    \end{tabular}%
    }
    \caption{Statistics of EHR datasets.}
    \label{tab:stats-ehr}
\end{table}


\begin{table*}[h!t]
\centering
\begin{threeparttable}
\resizebox{\textwidth}{!}{%
\begin{tabular}{@{}lcccccccccccccc@{}}
\toprule
\multirow{2}{*}{\textbf{Model}} 
 & \multicolumn{2}{c}{\textbf{Task 1: MT$^{+}$}} 
 & \multicolumn{2}{c}{\textbf{Task 2: RA($<$15 days)$^{+}$}} 
 & \multicolumn{2}{c}{\textbf{Task 3: LOS$^{*}$}} 
 & \multicolumn{2}{c}{\textbf{Task 4: Pheno$^{\circ}$}} 
 & \multicolumn{2}{c}{\textbf{Task 5: DrugRec$^{\circ}$}}\\
\cmidrule(lr){2-3} \cmidrule(lr){4-5} \cmidrule(lr){6-7} \cmidrule(lr){8-9} \cmidrule(lr){10-11}
& MIMIC-III & MIMIC-IV 
& MIMIC-III & MIMIC-IV 
& MIMIC-III & MIMIC-IV 
& MIMIC-III & MIMIC-IV 
& MIMIC-III & MIMIC-IV\\ 
& AUPRC & AUPRC 
& AUPRC & AUPRC 
& AUPRC & AUPRC 
& AUPRC & AUPRC 
& AUPRC & AUPRC\\ 
\midrule
ETHOS
  & 0.617\,(0.010) & 0.282\,(0.001) 
  & 0.421\,(0.007) & 0.648\,(0.005) 
  & N/A & N/A 
  & N/A & N/A 
  & 0.104\,(0.008) & 0.131\,(0.005)\\

\rowcolor{blue!10}  % <--- Light built-in green highlight
+ \model
  & \textbf{0.634\,(0.020)} & \textbf{0.412\,(0.030)} 
  & \textbf{0.463\,(0.017)} & \textbf{0.690\,(0.007)} 
  & N/A & N/A 
  & N/A & N/A 
  & \textbf{0.170\,(0.014)} & \textbf{0.240\,(0.012)}\\
\hline

GT-BEHRT
  & 0.160\,(0.037) & 0.028\,(0.004)
  & 0.612\,(0.058) & 0.586\,(0.070)
  & 0.230\,(0.010) & 0.103\,(0.001)
  & 0.423\,(0.002) & 0.493\,(0.005)
  & 0.715\,(0.002) & 0.736\,(0.007)\\

\rowcolor{blue!10}
+ \model
  & \textbf{0.193\,(0.046)} & \textbf{0.034\,(0.005)}
  & \textbf{0.623\,(0.052)} & \textbf{0.609\,(0.064)}
  & \textbf{0.287\,(0.039)} & \textbf{0.114\,(0.003)}
  & \textbf{0.459\,(0.028)} & \textbf{0.512\,(0.006)}
  & \textbf{0.740\,(0.004)} & \textbf{0.783\,(0.010)}\\
\hline

MulT-EHR
  & 0.136\,(0.021) & 0.120\,(0.003)
  & 0.574\,(0.008) & 0.515\,(0.007)
  & 0.176\,(0.018) & 0.118\,(0.032)
  & 0.460\,(0.012) & 0.498\,(0.001)
  & 0.523\,(0.008) & 0.445\,(0.027)\\ 

\rowcolor{blue!10}
+ \model
  & \textbf{0.156\,(0.025)} & \textbf{0.141\,(0.013)}
  & \textbf{0.585\,(0.016)} & \textbf{0.565\,(0.002)}
  & \textbf{0.198\,(0.011)} & \textbf{0.136\,(0.030)}
  & \textbf{0.480\,(0.002)} & \textbf{0.504\,(0.001)}
  & \textbf{0.571\,(0.006)} & \textbf{0.465\,(0.003)}\\  
\hline

TransformEHR
  & 0.207\,(0.012) & 0.042\,(0.012)
  & 0.527\,(0.030) & 0.518\,(0.012)
  & 0.132\,(0.021) & 0.119\,(0.001)
  & 0.469\,(0.022) & 0.507\,(0.007)
  & 0.533\,(0.030) & 0.612\,(0.046)\\ 

\rowcolor{blue!10}
+ \model
  & \textbf{0.246\,(0.044)} & \textbf{0.058\,(0.007)}
  & \textbf{0.568\,(0.036)} & \textbf{0.525\,(0.017)}
  & \textbf{0.159\,(0.031)} & \textbf{0.121\,(0.002)}
  & \textbf{0.513\,(0.024)} & \textbf{0.518\,(0.012)}
  & \textbf{0.580\,(0.035)} & \textbf{0.661\,(0.092)}\\ 
\hline

BEHRT
  & 0.163\,(0.037) & 0.028\,(0.003)
  & 0.529\,(0.053) & 0.514\,(0.015)
  & 0.232\,(0.015) & 0.112\,(0.003)
  & 0.587\,(0.004) & 0.493\,(0.006)
  & 0.539\,(0.013) & 0.778\,(0.014)\\ 

\rowcolor{blue!10}
+ \model
  & \textbf{0.220\,(0.025)} & \textbf{0.032\,(0.006)}
  & \textbf{0.574\,(0.040)} & \textbf{0.515\,(0.005)}
  & \textbf{0.251\,(0.030)} & \textbf{0.137\,(0.004)}
  & \textbf{0.603\,(0.008)} & \textbf{0.504\,(0.006)}
  & \textbf{0.558\,(0.006)} & \textbf{0.792\,(0.007)}\\ 
\hline

\textit{Improvement (\%)} 
  & \textcolor{blue!70!black}{\textbf{+3.32\%}} 
  & \textcolor{blue!70!black}{\textbf{3.54\%}}
  & \textcolor{blue!70!black}{\textbf{3.00\%}} 
  & \textcolor{blue!70!black}{\textbf{2.46\%}} 
  & \textcolor{blue!70!black}{\textbf{3.13\%}} 
  & \textcolor{blue!70!black}{\textbf{1.40\%}}
  & \textcolor{blue!70!black}{\textbf{2.90\%}} 
  & \textcolor{blue!70!black}{\textbf{1.18\%}}
  & \textcolor{blue!70!black}{\textbf{4.10\%}}
  & \textcolor{blue!70!black}{\textbf{4.78\%}}\\ 
\bottomrule
\end{tabular}%
}
\begin{tablenotes}[para, flushleft]
\tiny
\item $+$: imbalanced binary classification; $*$: multi-class classification, macro-averaged; $\circ$: multi-label classification;
      N/A indicates that the model was not configured for this task.
\end{tablenotes}
\end{threeparttable}
\vspace{-1em}
\caption{The results of \model with all baseline models across five tasks on two in-patient datasets.}
\label{tab:mimic_combined}
\end{table*}

\xhdr{Baselines}
To evaluate \model for tasks related to medical codes, we consider two types of tokenizers and five models based on EHR as baselines.
The first type of tokenizer is text-based (e.g., bert-base-uncased \cite{DBLP:journals/corr/abs-1810-04805}), while the second is graph-based (e.g., VQGraph \cite{yang2024vqgraph}).
Five EHR-based models are ETHOS \cite{ethos}, GT-BEHRT \cite{gtbehrt}, MulT-EHR \cite{mult_ehr}, TransformEHR \cite{transform_ehr}, and BEHRT \cite{li2020behrt}. Details on implementation can be found in the Appendix~\ref{appendix_implementation}.

\xhdr{Evaluation setup}
%
We consider two evaluation setups:
%
\begin{itemize}[leftmargin=*,noitemsep,topsep=0pt]
\item \textbf{In-patient evaluation:} This setting combines the \model tokenizer with patient prediction models, using two in-patient datasets that include individuals admitted to a hospital. The evaluation encompasses five tasks: \circled{1} mortality prediction (MT), \circled{2} readmission prediction (RA), \circled{3} length-of-stay prediction (LOS), \circled{4} phenotype prediction (Pheno), and \circled{5} drug recommendation (DrugRec). The first three tasks focus on predicting a patient's future health status using their historical medical records. Phenotype prediction involves the identification of the phenotype of a patient's disease based on their medical history. We identified 24 phenotypes for diseases in MIMIC-III and MIMIC-IV, as follows \cite{harutyunyan2019multitask}. Drug recommendation aims to suggest appropriate medications for a patient, considering their historical medical records and the diseases identified during their current visit. For drug recommendation, we focus on five specific drug candidates, including Vancomycin, Levofloxacin, Heparin Sodium, Metoprolol, and Atorvastatin, rather than considering the entire range of available medications. AUPRC is adopted to evaluate the model's performance on the above classification tasks.
%
\item \textbf{Out-patient evaluation:} We evaluate \model together with patient prediction models on a dataset of patients who are not admitted to a hospital and consider two categories of tasks: \circled{1} Operational Outcomes (OO), and \circled{2} new diagnosis assignments (ND), following \cite{wornow2023ehrshot}. The OO includes MT, RA, and prolonged LOS. The new diagnosis assignments are used to predict the first diagnosis of a disease. Details of task definitions are in Appendix \ref{appendix_task_definitions}.
\end{itemize}

\subsection{\model tokenizer with in-patient EHR models}
%MIMIC III and MIMIC IV
Table \ref{tab:mimic_combined} presents the AUPRC values for each baseline and their integration with our \model for five tasks in two in-patient datasets. Compared to baselines that treat each medical code as an individual token, integrating our \model consistently improves performance across all five tasks, achieving an average improvement of 3.29\% on MIMIC-III and 2.67\% on MIMIC-IV. This improvement comes from more informative tokens generated by \model, which strengthen the EHR-based models. Among five tasks, \model demonstrates the most significant impact on drug recommendation tasks, highlighting the value of incorporating prior knowledge into our tokenizer.

To further assess the effectiveness of \model, we compare it against two tokenization methods: the text-based BERT tokenizer and the graph-based VQGraph tokenizer. Figure~\ref{fig:tokenizer} presents the performance of each tokenizer when integrated with a Transformer-based EHR model (TransformEHR) across five tasks on two in-patient datasets. \model consistently outperforms BERT and VQGraph in all tasks and datasets, demonstrating the superiority of its tokenization strategy.


\begin{table*}[h!t]
\centering
\renewcommand{\arraystretch}{1.0} % Ensure row height is the same
\tiny % Matches font size with inpatient table
\resizebox{\textwidth}{!}{%
\begin{tabular}{@{}lcccccccc@{}}
\toprule
\multirow{2}{*}{\textbf{Model}} & \multicolumn{3}{c}{\textbf{Task 1: Operational Outcomes (OO)}} & \multicolumn{4}{c}{\textbf{Task 2: Assignment of New Diagnoses (ND)}} \\
\cmidrule(lr){2-4} \cmidrule(lr){5-8} 
& Long LOS & RA (${<}$15 days) & MT & Hypertension & Hyperlipidemia & Pancreatic Cancer & Acute MI \\ 
& AUPRC & AUPRC & AUPRC & AUPRC & AUPRC & AUPRC & AUPRC \\ 
\midrule
ETHOS & NA & 0.079\,(0.017) & 0.102\,(0.018) & 0.166\,(0.020) & 0.155\,(0.031) & 0.056\,(0.006) & 0.093\,(0.011)\\
\rowcolor{blue!10} + \model & NA & \textbf{0.128\,(0.025)} & \textbf{0.339\,(0.010)} & \textbf{0.175\,(0.019)} & \textbf{0.163\,(0.025)} & \textbf{0.056\,(0.013)} & \textbf{0.104\,(0.017)}\\
\hline
GT-BEHRT & 0.714\,(0.021) & 0.115\,(0.012) & 0.239\,(0.012) & 0.303\,(0.018) & 0.239\,(0.007) & 0.044\,(0.008) & 0.015\,(0.008)\\
\rowcolor{blue!10} + \model & \textbf{0.739\,(0.025)} & \textbf{0.154\,(0.013)} & \textbf{0.444\,(0.015)} & \textbf{0.360\,(0.012)} & \textbf{0.441\,(0.005)} & \textbf{0.074\,(0.010)} & \textbf{0.031\,(0.015)}\\
\hline
MulT-EHR & 0.539\,(0.025) & 0.125\,(0.014) & 0.397\,(0.016) & 0.218\,(0.005) & 0.243\,(0.005) & 0.022\,(0.008) & 0.017\,(0.003)\\ 
\rowcolor{blue!10} + \model & \textbf{0.571\,(0.015)} & \textbf{0.188\,(0.021)} & \textbf{0.444\,(0.012)} & \textbf{0.226\,(0.006)} & \textbf{0.254\,(0.021)} & \textbf{0.037\,(0.015)} & \textbf{0.028\,(0.014)}\\  
\hline
TransformEHR & 0.652\,(0.023) & 0.197\,(0.016) & 0.344\,(0.030) & 0.376\,(0.018) & 0.305\,(0.021) & 0.053\,(0.006) & 0.025\,(0.006)\\ 
\rowcolor{blue!10} + \model & \textbf{0.675\,(0.018)} & \textbf{0.243\,(0.016)} & \textbf{0.379\,(0.034)} & \textbf{0.413\,(0.026)} & \textbf{0.333\,(0.018)} & \textbf{0.082\,(0.012)} & \textbf{0.052\,(0.017)}\\ 
\hline
BEHRT & 0.582\,(0.032) & 0.332\,(0.022) & 0.389\,(0.018) & 0.233\,(0.027) & 0.251\,(0.019) & 0.036\,(0.008) & 0.013\,(0.031)\\ 
\rowcolor{blue!10} + \model & \textbf{0.723\,(0.028)} & \textbf{0.397\,(0.036)} & \textbf{0.431\,(0.017)} & \textbf{0.287\,(0.018)} & \textbf{0.302\,(0.015)} & \textbf{0.057\,(0.012)} & \textbf{0.036\,(0.015)}\\ 
\hline
\textit{Improvement (\%)}
& \textcolor{blue!70!black}{\textbf{+5.52\%}} 
& \textcolor{blue!70!black}{\textbf{+5.24\%}} 
& \textcolor{blue!70!black}{\textbf{+11.32\%}} 
& \textcolor{blue!70!black}{\textbf{+3.30\%}} 
& \textcolor{blue!70!black}{\textbf{+6.00\%}} 
& \textcolor{blue!70!black}{\textbf{+1.90\%}} 
& \textcolor{blue!70!black}{\textbf{+1.76\%}}\\ 
\bottomrule
\end{tabular}%
}
\caption{The results of \model with all baseline models across two tasks on the EHRShot dataset.}
\label{tab:ehrshot}
\end{table*}

\begin{figure*}[h!t]
    \centerline{
    \includegraphics[width=0.6\textwidth]{figures/tokenizers.pdf}
    }
    \caption{The AUPRC values of three types of tokenizers on in-patient and out-patient datasets, where OO means Operational Outcomes and ND means assignment of new diagnoses.}
    \vspace{-2mm}
    \label{fig:tokenizer}
\end{figure*}

\subsection{\model tokenizer with out-patient EHR models}

Table \ref{tab:ehrshot} presents the AUPRC values for each baseline and its integration with \model across two task types on the out-patient HRShot dataset. The results reveal that our tokenizer has the most significant impact on mortality prediction in Operational Outcomes, achieving an average improvement of 11.30\%.  It also significantly improves the detection of new diagnoses of Acute MI, with an average improvement of 8.80\%. As shown in Fig. \ref{fig:tokenizer}, a comparison of three types of tokenizers further demonstrates the effectiveness of \model in integrating both graph and textual modalities. Additionally, when comparing performance across two in-patient datasets, we observe that \model is particularly beneficial for longitudinal data.

\subsection{Ablation studies}
In the ablation studies, to eliminate potential bias from different model architectures, we integrate \model with a vanilla Transformer-based model (e.g., TransformEHR) to examine the impact of the adopted modalities and the vocabulary size in \model on performance.
\begin{figure*}[h!t]
    \centerline{
    \includegraphics[width=1\textwidth]{figures/ablation_modality.pdf}
    }
    \vspace{-7mm}
    \caption{The AUPRC values obtained by removing the text and graph modalities across all tasks on two in-patient datasets and one out-patient dataset.}
    \label{fig:ablation_modality}
\end{figure*}

\xhdr{Effects of modalities on \model}
To evaluate the impact of the two modalities (text, graph) used in \model—medical code definitions and biological subgraphs derived from a biomedical knowledge graph—we assess its performance by removing the text and graph modalities separately. As shown in Fig. \ref{fig:ablation_modality}, \model, when leveraging both modalities, achieves the best performance across all tasks on three datasets. By comparing the performance of \model without the graph modality and \model without the text modality, we observe that both modalities contribute significantly to EHR-based prediction tasks. The graph modality benefits drug recommendation and new disease detection tasks, while the text modality proves essential for readmission prediction on MIMIC-III and operational outcomes in EHRShot. These findings emphasize the importance of incorporating the underlying information linked to medical codes.

\xhdr{Effect of codebook size $N$}
We further evaluate the impact of the codebook size on the performance of \model by training it with varying sizes and assessing its effectiveness across three distinct datasets integrated with TransformEHR. Fig.~\ref{fig:medqa} presents the results for various codebook sizes across all tasks on the three datasets. The performance trends observed on MIMIC-III and MIMIC-IV are quite consistent, demonstrating a clear pattern where increasing the codebook size enhances the model's performance. Specifically, the highest average performance is achieved when the codebook size is set to \( N = 12,000 \), indicating that this size strikes an optimal balance between sufficient coverage of the medical vocabulary and avoiding overfitting. 

In contrast, when analyzing the performance on EHRShot, a dataset consisting of patients with longer visit histories than those in MIMIC-III and MIMIC-IV, we observe that \model benefits from a larger codebook size. For EHRShot, the highest average performance is achieved when the codebook size is increased to \( N = 21,000 \). This suggests that for datasets with more extensive patient visit histories, a larger codebook may be more effective in capturing the underlying complexity of the medical information, thus improving the model's predictive capabilities. 

\subsection{Using \model tokenizer for medical QA}

\model demonstrates strong performance in EHR-based tasks, as shown in Tables \ref{tab:mimic_combined}-\ref{tab:ehrshot}. To further assess its capabilities, we explore its effectiveness in a generation task, specifically multiple-choice medical question answering (MedicalQA), where the goal is to select the correct answer to a given clinical question~\cite{singhal2023large}.
%
We evaluate whether \model enhances few-shot learning in MedicalQA by integrating its tokenized representations with a LLM (LLaMA3.1-8B~\cite{dubey2024llama3herdmodels}). \model-generated tokens are used as prefix tokens, which provide structured medical context before the main input, allowing the LLM to incorporate additional domain knowledge.

For this evaluation, we use the MedDDx dataset~\cite{su2024knowledge}, which contains questions at three difficulty levels: Basic, Intermediate, and Expert. The process consists of three steps: (1) Disease code mapping – Extract disease mentions from each question and retrieve their corresponding medical codes. (2) Tokenization via \model – Convert medical codes into structured tokens using \model. (3) Prefix Token Fine-Tuning – Fine-tune LLaMA3.1-8B using \model tokens as prefix inputs before the question text. We fine-tune the model on 617 intermediate-level questions and evaluate performance on 227 expert-level and 158 basic-level questions. The results in Figure~\ref{fig:medqa} show an accuracy improvement of 0.3\% on MedDDx-Basic and 2.3\% on MedDDx-Expert, suggesting that \model can enhance medical QA when used as a structured prefix representation.

\begin{figure}[h!t]
    \centerline{
    \includegraphics[width=0.5\textwidth]{figures/codebook_medqa.pdf}
    }
    \vspace{-4mm}
    \caption{A, The AUPRC values of \model with different codebook size $N$; B, The accuracy of LLaMA3.1-8B vs.~\model+LLaMA3.1-8B on two MedDDx medical QA datasets.}
    \label{fig:medqa}
\end{figure}
