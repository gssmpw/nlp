
\begin{figure}[!htbp]       
    \centerline{\includegraphics[width=0.8\linewidth]{figures/Figure1-Asset-1-vertical-v3.pdf}}
    \caption{\model is a multimodal tokenizer for medical codes that combines text embeddings from code descriptions with graph-based representations of dependencies from ontologies and terminologies. It is a general tokenizer that sheds light on how optimized tokenization benefits transformer models in medicine.}
    \label{fig:figure1}
    \vspace{-4mm}
\end{figure}

Electronic health records (EHRs) are the backbone of modern healthcare, capturing a person's health state with increasing precision across diverse modalities. Structured EHR data, encoded through standardized medical codes, support a wide range of applications, from personalized risk prediction~\cite{Goldstein2016risk,yu2024risk} and disease trajectory modeling~\cite{Jensen2017traj,Heumos2024ehrpy} to emulation of clinical trials~\cite{Katsoulakis2024digitaltwins,Kraljevic2024trial}. The cornerstone of structured EHRs is medical coding systems, which assign standardized alphanumeric codes to various aspects of patient health, including diseases, procedures, medications, and laboratory tests. These codes come from widely used terminologies such as ICD-9, ICD-10, SNOMED CT, CPT, and ATC, among others~\cite{foley1992comorbidities,world1988international,world2004international,donnelly2006snomed,dotson2013cpt,miller1995new}. Although essential for interoperability, medical codes introduce challenges for models, particularly in the tokenization process, which transforms structured EHR data into token sequences that foundation models can process.

Transformer-based models for structured EHRs~\cite{gtbehrt,transform_ehr,graphcare,ethos} rely on tokenizers to map raw data into discrete vocabulary items. However, standard tokenization strategies inherited from general-purpose language models fail to capture the complexity of medical codes, leading to six key challenges: (1) Scalability of medical vocabularies – Medical coding systems contain over 600,000 unique codes, far exceeding standard tokenizer capacities. Treating each code as a separate token leads to inefficient vocabulary expansion, increasing memory demands and fragmenting rare codes (e.g., splitting "ICD9: 250.0" into arbitrary subwords). (2) Loss of hierarchical and relational structure – Many coding systems encode structured dependencies, such as ATC codes, which classify drugs based on pharmacological and chemical properties~\cite{miller1995new}. Standard tokenizers, relying only on co-occurrence statistics, fail to capture hierarchical relationships, losing dependencies like disease co-occurrences and drug contraindications. (3) Redundancy across coding systems – Identical clinical concepts often appear under different codes across terminologies (e.g., ICD vs. SNOMED). Standard tokenization treats them as separate tokens, creating redundancy and complicating cross-system data integration. (4) Inefficiency in token storage – Expanding vocabulary sizes to accommodate medical codes results in bloated embedding tables that degrade computational efficiency, particularly for low-resource codes that appear infrequently but still require dedicated tokens. (5) Sparse and inconsistent usage – Many medical codes are rarely used or inconsistently documented, making it difficult for standard tokenizers to learn meaningful representations. Low-frequency codes suffer from poor embeddings, reducing performance on underrepresented conditions. (6) Lack of multimodal representations – Existing methods~\cite{graphcare,10.1145/3627673.3679582,xu-etal-2024-ram} treat medical codes as isolated textual tokens, discarding graph-based relationships that encode essential links between diagnoses, treatments, and medications. A robust tokenizer must integrate both textual and relational information to fully represent medical codes.

Several models attempt to enrich the representations of medical codes by incorporating external knowledge from LLM~\cite{graphcare,10.1145/3627673.3679582,xu-etal-2024-ram}. Methods like GraphCare and RAW-EHR prompt LLMs to generate structured knowledge triplets of medical codes. Although effective in specific tasks, these approaches suffer from limited generalizability, a heavy reliance on knowledge generated by LLM, and a lack of a unified framework for handling various medical coding systems. Despite advances in medical representation learning, a unified tokenizer that integrates textual and structured relational knowledge across coding systems remains an open challenge.

\xhdr{Present work}
%
We introduce \model 
%(\url{https://anonymous.4open.science/r/MedTok-8DEE})
, a multimodal medical code tokenizer that integrates textual descriptions and graph-based dependencies from biomedical ontologies (Figure~\ref{fig:figure1}). Unlike standard tokenization methods that treat medical codes as isolated textual tokens, \model captures both semantic meaning and structured relationships by encoding multiple modalities into a unified token space. \model operates in three stages. Multimodal encoding first extracts text embeddings from medical code descriptions and graph-based representations from biomedical knowledge graphs using separate encoders. Next, vector quantization maps both modalities into a shared token space, generating distinct text-informed and graph-informed token embeddings while preserving cross-modality interactions. Finally, optimization for expressivity ensures that token representations capture hierarchical relationships, semantic equivalence across different coding systems, and dependencies such as comorbidities and drug interactions.  

We integrate \model into five EHR models and evaluate it in clinical and operational tasks that span the inpatient (MIMIC-III, MIMIC-IV) and outpatient (EHRShot) settings. These tasks include disease prediction, operational outcome modeling, drug recommendation, patient risk stratification, and operational outcomes. Key contributions:  
%
\begin{itemize}[leftmargin=*,noitemsep,topsep=0pt]
\item Multimodal tokenization of medical codes -- \model tokenizer  jointly encodes both textual descriptions and graph-based representations of medical codes, enabling richer and structured embeddings.  
\item Improved cross-system generalization -- By incorporating ontological knowledge, \model bridges semantic gaps between different coding systems.  
\item Demonstrated performance gains -- Replacing standard EHR tokenizers with \model improves AUPRC by 4.10\% on MIMIC-III, 4.78\% on MIMIC-IV, and 11.30\% on EHRShot, with the largest gains in drug recommendation tasks. \model is a general purpose tokenizer that can be integrated into any transformer-based model or system that requires tokenization. Beyond EHR models, we demonstrate its applicability in medical question-answering systems, further highlighting the benefit of optimized tokenization of structured medical data.
\end{itemize}