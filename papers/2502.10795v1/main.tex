\documentclass[11pt,a4paper]{amsart}
\usepackage[foot]{amsaddr}
\setlength\marginparwidth{2cm}
\usepackage{ifxetex}
\ifxetex
  \usepackage[no-math]{fontspec}
\else
\fi
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsthm}
%\usepackage{cleveref}
\usepackage{fullpage}
\usepackage{microtype}
%\usepackage{charter}
\ifxetex % fix font incompatibility
  \usepackage[libertine]{newtxmath}
\else
  \usepackage{newtxmath}
\fi
\usepackage[tt=false]{libertine} %% tt is ugly 
\usepackage{caption}
\usepackage{tcolorbox}
\usepackage{bbm}
\usepackage{hyperref, color}
\hypersetup{colorlinks=true,citecolor=blue, linkcolor=blue, urlcolor=blue}
\usepackage[linesnumbered,boxed,ruled,vlined]{algorithm2e}
\usepackage{bm}
\usepackage{bbm}
\usepackage[numbers]{natbib}
\usepackage{xcolor}
\usepackage{enumerate} 
\usepackage{enumitem}
\usepackage{ragged2e}
\usepackage{tabularx}
\usepackage{array}
\usepackage{tablefootnote}
\usepackage{longtable}
\usepackage{hhline}
\usepackage{multirow}
\usepackage{cleveref}
%\usepackage{enumitem}
\newcolumntype{L}[1]{>{\raggedright\arraybackslash}p{#1}}
\newcolumntype{C}[1]{>{\centering\arraybackslash}m{#1}}
\newcolumntype{R}[1]{>{\raggedleft\arraybackslash}p{#1}}
\renewcommand\arraystretch{1.5}  
%\renewcommand\arraystretch{2}
\usepackage{makecell}
%\usepackage{ulem}
%\usepackage[T1]{fontenc}
\usepackage{footnote}
\usepackage{float}
\makesavenoteenv{tabular}


\renewcommand{\epsilon}{\varepsilon}



%\renewcommand{\todo}[1]{\typeout{TODO: \the\inputlineno: #1}\textbf{{\color{red}[[[ #1 ]]]}}}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{observation}[theorem]{Observation}
\newtheorem{claim}[theorem]{Claim}
\newtheorem*{claim*}{Claim}
\newtheorem{condition}[theorem]{Condition}
\newtheorem{example}[theorem]{Example}
\newtheorem{fact}[theorem]{Fact}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{Condition}{Condition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{remark}[theorem]{Remark}
\newtheorem*{remark*}{Remark}
\newtheorem{assumption}{Assumption}
\newcommand{\evnt}[1]{\mathsf{evnt}\left(#1\right)}
\newcommand{\toolarge}{dk\log\frac{n}{\delta}}
\newcommand{\repeattime}{\left\lceil\left( \frac{n}{\delta} \right)^{\frac{\eta}{10}} \log \frac{n}{\delta} \right\rceil }
\newcommand{\Rejection}{\textsf{RejectionSampling}}

%\def\Pr{\mathop{\mathbf{Pr}}\nolimits}
\def\Ex{\mathop{\mathbf{E}}\nolimits}

\renewcommand{\emptyset}{\varnothing}
\newcommand{\E}[1]{\mathbb{E}\left[{#1}\right]}
\renewcommand{\Pr}[2][]{ \ifthenelse{\isempty{#1}}
  {\mathop{\mathbf{Pr}}\left[#2\right]} {\mathop{\mathbf{Pr}}_{#1}\left[#2\right]} }

\newcommand{\norm}[1]{\left\Vert#1\right\Vert}
\newcommand{\abs}[1]{\left\vert#1\right\vert}
\newcommand{\set}[1]{\left\{#1\right\}}
\newcommand{\tuple}[1]{\left(#1\right)} 
\newcommand{\ceil}[1]{\lceil #1\rceil} 
\newcommand{\floor}[1]{\lfloor #1\rfloor}
\newcommand{\oq}{\overline{q}}
\newcommand{\BB}{{\bm B}}
\newcommand{\eps}{\varepsilon}
\newcommand{\tp}{\tuple}
% \renewcommand{\mid}{\;\middle\vert\;}
\newcommand{\cmid}{\,:\,}
\newcommand{\numP}{\ensuremath{\#\mathbf{P}}}
\renewcommand{\P}{\ensuremath{\mathbf{P}}}
\newcommand{\NP}{\ensuremath{\mathbf{NP}}}
\newcommand{\APX}{\ensuremath{\mathbf{APX}}}
\newcommand{\RP}{\ensuremath{\mathbf{RP}}}
\newcommand{\defeq}{:=}
\renewcommand{\d}{\,\-d}
\newcommand{\ol}{\overline}
\newcommand{\wt}{\text{wt}}
\newcommand{\alphab}{\boldsymbol{\alpha}}
\newcommand{\betab}{\boldsymbol{\beta}}
\newcommand{\rb}{{\bm r}}
\newcommand{\cb}{{\bm c}}
\newcommand{\lb}{C}
\newcommand{\ib}{{\boldsymbol{i}}}
\newcommand{\emm}{\mathrm{e}}
\newcommand{\sgn}{\mathrm{sgn}}
\newcommand{\ones}{{\bm 1}}
\newcommand{\Jb}{{\bm J}}
\newcommand{\transpose}[1]{#1^{\top}}
\newcommand{\holant}{{\mathrm{Holant}}}
\newcommand{\cvec}[2]{{\left[\begin{smallmatrix}#1\\#2\end{smallmatrix}\right]}}
\newcommand{\Tmatrix}{{\left(\begin{smallmatrix}1&1\\1&-1\end{smallmatrix}\right)}}
\newcommand{\odd}{\text{odd}}
\newcommand{\DTV}[2]{d_{\mathrm{TV}}\left({#1},{#2}\right)}
\newcommand{\Gap}{\ensuremath{\mathfrak{Gap}}}
\newcommand{\MCMCSingle}{\textsf{MCMCSingleVtx}}
\newcommand{\randomcol}{\mathsf{Randomcolor}}


\def\*#1{\bm{#1}} % Use \*A for \mathbf{A}
\def\+#1{\mathcal{#1}} % Use \+A for \mathcal{A}
\def\-#1{\mathrm{#1}} % Use \-A for \mathrm{A}
\def\=#1{\mathbb{#1}} % Use \=A for \mathbb{A}
\def\partD#1#2{\frac{\partial #1}{\partial #2}}

\def\MaxCut{\textnormal{\textsc{Max-Cut}}}
\def\MaxqCut{\textnormal{\textsc{Max-$q$-Cut}}}
\def\Mono{\operatorname{Mono}}
\def\Opt{\mathrm{Opt}}

\usepackage[textsize=tiny]{todonotes}
\newcommand{\wcytodo}[1]{\todo[color=blue!30]{wcy: #1}}
\usepackage{xifthen}


%\newcommand{\one}[1]{\mathbf{1}\left[#1\right]}

\makeatletter
\def\prob#1#2#3{\goodbreak\begin{list}{}{\labelwidth\z@ \itemindent-\leftmargin
                        \itemsep\z@  \topsep6\p@\@plus6\p@
                        \let\makelabel\descriptionlabel}
                \item[\it Name]#1
               \item[\it Instance]                #2
                \item[\it Output]#3
                \end{list}}
\makeatother


%commands added by wcy
\newcommand{\poly}{{\rm poly}}  
\newcommand{\basicsample}{\textnormal{\textsf{LB-Sample}}}
\newcommand{\mghinds}{\textnormal{\textsf{MarginIndSet}}}
\newcommand{\appmghinds}{\textnormal{\textsf{ApproxMarginIndSet}}}
\newcommand{\mghcol}{\textnormal{\textsf{Margincolouring}}}
\newcommand{\appmghcol}{\textnormal{\textsf{ApproxMarginColouring}}}
\newcommand{\resolve}{\textnormal{\textsf{Resolve}}}
\newcommand{\update}{\textnormal{\textsf{Update}}}
\newcommand{\suc}{\textnormal{\textsf{Suc}}}
\newcommand{\vbl}{\textnormal{\textsf{vbl}}}
\newcommand{\call}{\textnormal{\textsf{Call}}}
\newcommand{\appresolve}{\textnormal{\textsf{ApproxResolve}}}
\newcommand{\appresolverandom}{\textnormal{\textsf{ApproxResolve-Random}}}
\newcommand{\magenum}{\textnormal{\textsf{MarginEnum}}}
\newcommand{\config}{\mathsf{Boundary}}
\newcommand{\sample}{\mathsf{Sample}}
\newcommand{\appmghcoledge}{\textnormal{\textsf{ApproxMarginColouring-Set}}}
\newcommand{\pred}{\textnormal{\textsf{pred}}}
\newcommand{\Bern}{\textnormal{\textsf{Bern}}}
\newcommand{\ts}{\textnormal{\textsf{TS}}}
\newcommand{\talg}{T_\textnormal{\textsf{alg}}}
\newcommand{\lsample}{\textnormal{\textsf{LocalSample}}}
\newcommand{\berrace}{\textnormal{\textsf{BernoulliRace}}}
\newcommand{\berpower}{\textnormal{\textsf{BernoulliPower}}}
\newcommand{\marbf}{\textnormal{\textsf{MarginalBF}}}
\newcommand{\eval}{\textnormal{\textsf{Evaluate}}}
\newcommand{\sd}{\textnormal{\textsf{Seed}}}
\newcommand{\var}[1]{{{\vbl}}\left({#1}\right)}  
\newcommand{\DDS}{X}
\renewcommand{\defeq}{\triangleq}
\usepackage{enumitem}

\newcommand{\acvd}{\textnormal{\textsf{ApproximateCountingViaDerandomisation}}}
\newcommand{\ssms}{\textnormal{\textsf{SSMS}}}
\newcommand{\bdsplit}{\textnormal{\textsf{BD-SPLIT}}}
\newcommand{\ssmstr}{\textnormal{\textsf{SSMS-Truncated}}}
\newcommand{\bdsplittr}{\textnormal{\textsf{BD-SPLIT-Truncated}}}
\newcommand{\RC}{\pi_{RC}}
\newcommand{\Ising}{\pi_{Ising}}
\newcommand{\Joint}{\pi_{Joint}}
\newcommand{\joint}[1]{\overline{#1}}
\newcommand{\Pupjoint}{P^{E\uparrow}_{Joint}}
\newcommand{\Pdownjoint}{P^{E\downarrow}_{Joint}}
\newcommand{\JointEdrop}{\Joint^{E\downarrow}}
\newcommand{\PJointToIsing}{P_{J\rightarrow I}}
\newcommand{\PIsingToJoint}{P_{I\rightarrow J}}
\newcommand{\Pup}{P^{\uparrow}}
\newcommand{\Pdown}{P^{\downarrow}}
\newcommand{\Pdownup}{P^{\vee}}
\newcommand{\Pupdown}{P^{\wedge}}
\newcommand{\dist}{\mathrm{dist}}

\newcommand{\inner}[2]{\langle #1,#2\rangle}
\newcommand{\Diri}[2]{\ensuremath{\+E_{#1}\left(#2\right)}}
\newcommand{\Ent}[2]{\ensuremath{\textnormal{Ent}_{#1}\left(#2\right)}}
\newcommand{\KL}[2]{\ensuremath{D_{\textnormal{KL}}\left(#1\parallel#2\right)}}
\newcommand{\Var}[2]{\ensuremath{\textnormal{Var}_{#1}\left(#2\right)}}
\newcommand{\DF}[2]{\ensuremath{D_{f}\left(#1\parallel#2\right)}}
\newcommand{\chisq}[2]{\ensuremath{D_{\chi^2}\left(#1\parallel#2\right)}}

\newcommand{\mixingtime}[1]{\ensuremath{t_{\textnormal{mix}}(#1)}}
\newcommand{\pimin}{\ensuremath{\pi_{\textnormal{min}}}}
\newcommand{\randombit}{\basicsample}

\def\randseed{{\mathcal{R}}}
\def\coupler{{\emph{SpinCoupler}}}

\newbool{doubleblind}
\setbool{doubleblind}{false}

\ifdoubleblind
  \author{Author(s)}
\else

\author{Hongyang Liu, Chunyang Wang, Yitong Yin}
\address{State Key Laboratory for Novel Software Technology, New Cornerstone Science Laboratory, Nanjing University, 163 Xianlin Avenue, Nanjing, Jiangsu Province, 210023, China. \textnormal{E-mail: \texttt{liuhongyang@nju.edu.cn}, \texttt{wcysai@smail.nju.edu.cn}, \texttt{yinyt@nju.edu.cn}}}
\fi

\title{Local Gibbs sampling beyond local uniformity}

%\title{Local samplers for near-critical spin systems}
\date{}

%\address{State Key Laboratory for Novel Software Technology, Nanjing University, 163 Xianlin Avenue, Nanjing, Jiangsu Province, China. \textnormal{E-mails: \url{@smail.nju.edu.cn},  \url{yinyt@nju.edu.cn}}}

\begin{document}


\begin{abstract}
Local samplers are algorithms that generate random samples based on local queries to high-dimensional distributions, ensuring the samples follow the correct induced distributions while maintaining time complexity that scales locally with the query size. 
These samplers have broad applications, including deterministic approximate counting~\cite{HWY22c,feng2023towards}, sampling from infinite or high-dimensional Gibbs distributions~\cite{AJ22,HWY22a}, and providing local access to large random objects~\cite{amartya2020local}.

In this work, we present a local sampler for Gibbs distributions of spin systems whose efficiency does not rely on the ``local uniformity'' property, which imposes unconditional marginal lower bounds --- a key assumption required by all prior local samplers.
%
For fundamental models such as the Ising model, our algorithm achieves local efficiency in near-critical regimes, providing an exponential improvement over existing methods. 
Additionally, our approach is applicable to spin systems on graphs with unbounded degrees and supports dynamic sampling within the same near-critical regime.
\end{abstract}	

\maketitle

\section{Introduction}

Spin systems, which originated in statistical physics, are stochastic models characterized by local interactions.
These models have not only advanced our understanding of physical phenomena, such as phase transitions and criticality, 
but have also become central to machine learning and theoretical computer science, 
particularly in the study of sampling and inference problems in complex distributions.
%In particular, spin systems serve as powerful tools for studying sampling and inference problems in complex distributions. 



Let $q\ge 2$ be an integer. 
A $q$-spin system $\+S=(G=(V,E),\bm{\lambda}=(\lambda_v)_{v\in V},\bm{A}=(A_e)_{e\in E})$ is defined on a finite graph $G = ( V, E)$,
where each vertex is associated with an \emph{external field} $\lambda_v\in\mathbb{R}^q_{\geq 0}$, and each edge $e\in E$  is associated with an \emph{interaction matrix} $A_{e}\in\mathbb{R}^{q\times q}_{\geq 0}$. 
A configuration $\sigma \in [q]^V$ assigns a spin state from $[q]$ to each vertex $v \in V$. 

The \emph{Gibbs distribution} $\mu=\mu^{\+S}$ over all configurations $\sigma \in [q]^V$ is given by:
\[
\mu(\sigma)\defeq \frac{w(\sigma)}{Z}, \qquad w(\sigma)\defeq\prod\limits_{v\in V}\lambda_v(\sigma(v))\prod\limits_{e=(u,v)\in E}A_e(\sigma(u),\sigma(v)),
\]
where the the normalizing factor  $Z=\sum_{\sigma\in[q]^V}w(\sigma)$ is the \emph{partition function}.

%Given any $q$-spin system $\+S=(G=(V,E),(\bm{\lambda},\bm{A}))$ and let $\mu=\mu^{\+S}$. 

%A central inquiry within the study of spin systems involves sampling from the Gibbs distribution defined by these systems. For typical spin systems such as the hardcore models or the Ising model, a critical threshold defined by parameters of the system has been established, up to which sampling from the Gibbs distribution is \textbf{NP}-hard~\cite{SS14,galanis2016inapproximability}. Recent breakthroughs have shown that the generic Markov chain known as Glauber dynamics mixes rapidly up to hardness thresholds for certain spin systems such as the hardcore model and the Ising model ~\cite{ALO20, chen2020rapid, CLV21, chen2021rapid, anari2022entropic, CE22,CFYZ22}, which provides a complete characterization of computation phase transition that emerges in the problem of sampling from Gibbs distributions of such spin systems.
A central question in the study of spin systems is sampling from their associated Gibbs distributions. For well-known models such as the hardcore model and the Ising model, a critical threshold determined by the system's parameters has been identified, beyond which sampling from the Gibbs distribution becomes \textbf{NP}-hard~\cite{SS14,galanis2016inapproximability}. Recent breakthroughs have shown that the Glauber dynamics, a widely-used Markov chain, mixes rapidly up to these critical thresholds for specific spin systems, including the hardcore model and the Ising model~\cite{ALO20, chen2020rapid, CLV21, chen2021rapid, anari2022entropic, CE22,CFYZ22}. These results provide a comprehensive characterization of the computational phase transition inherent in sampling from the Gibbs distributions of such spin systems.

\subsection{Local sampling and local uniformity}
Recent research has increasingly focused on \emph{local} sampling techniques for high-dimensional Gibbs distributions~\cite{AJ22,anand23sphere,feng2023towards}.
In these algorithms, 
rather than directly drawing a global sample from the Gibbs distribution $\mu$,
a local sample is drawn according to the marginal distribution $\mu_{\Lambda}$ of $\mu$ on a subset $\Lambda$ of queried vertices,
ideally using a computational cost that scales near-linearly with the size of $\Lambda$.
The marginal distribution $\mu_{\Lambda}$ is defined as:
\[
\forall \tau\in[q]^\Lambda,\quad \mu_\Lambda(\tau)\triangleq\sum_{\sigma\in[q]^v:\sigma_\Lambda=\tau}\mu(\sigma).
\]
%Furthermore, for any subset of vertices $\Lambda\subseteq V$, let $\mu_{\Lambda}$ denote the marginal distribution of $\mu$ on $\Lambda$. 
Local sampling can, of course, solve global sampling by simply querying all vertices or using the auto-regressive sampler for self-reducible problems, as in~\cite{AJ22, HWY22a}. 
Beyond this, these local samplers offer the ability to ``scale down'' the sampling process, 
addressing the challenge of providing local access to large random objects~\cite{amartya2020local}, 
where sublinear computational costs are required for sublinear-size queries.
They also have led to the derandomization of Markov chain Monte Carlo (MCMC) methods~\cite{HWY22c, feng2023towards, anand24approximate}.


All current local samplers for the Gibbs distributions of the spin systems~\cite{AJ22,anand23sphere,feng2023towards} rely on the assumption of \emph{unconditional marginal lower bounds}, also known as the ``\emph{local uniformity}'' property.
This assumption requires that the marginal distribution of each vertex remains nearly identical across all neighboring configurations.
For Gibbs distributions with hard constraints, 
such as the hardcore model (i.e., graph independent sets) or solutions to constraint satisfaction problems, 
the local uniformity property holds in near-critical regimes, thus enabling the existence of local samplers~\cite{AJ22, HWY22a}.
However, for general spin systems under soft constraints, the condition of local uniformity may be excessively restrictive.

Consider, for example, the Ising model with edge activity $\beta>0$  and an arbitrary external field. 
The Ising model, introduced by Ising and Lenz~\cite{Ising1925Beitrag}, has been extensively studied in various fields.
Formally, it is a $2$-spin system with $A_e=\begin{pmatrix}
    \beta & 1\\
    1 & \beta\\
\end{pmatrix}$ at each edge $e\in E$ and arbitrary $\lambda_v$ at each $v\in V$.
Sampling from its Gibbs distribution can be achieved through Glauber dynamics, which mixes rapidly under the well-known ``uniqueness condition'':
\begin{equation}\label{eq:Ising-uniqueness-condition}
    \beta \in \left(\frac{\Delta-2}{\Delta},\frac{\Delta}{\Delta-2}\right),
\end{equation} 
where $\Delta$ is the maximum degree of the underlying graph.  Beyond this, either Glauber dynamics becomes torpidly mixing, or the sampling problem itself becomes intractable.
%
 In contrast, the requirement of unconditional marginal lower bounds for such models imposes a significantly stricter condition:
\begin{align}\label{eq:Ising-local-uniformity}
 \beta\in \left(\left(\frac{\Delta-1}{\Delta+1}\right)^{1/\Delta},\left(\frac{\Delta+1}{\Delta-1}\right)^{1/\Delta}\right),    
\end{align}
which is exponentially narrower compared to the uniqueness condition in~\eqref{eq:Ising-uniqueness-condition}.

This stark discrepancy raises a fundamental question: Do local samplers exist for such models in near-critical regimes? Or does local sampling inherently require a significantly more stringent critical condition compared to global sampling? 
%is local sampling inherently more challenging than Gibbs sampling, requiring significantly more stringent critical conditions?
%And more fundamentally, is local sampling inherently more challenging than global sampling for spin systems?
%Currently, this remains an open problem, leading to the following intriguing question:
%\begin{center}
%\emph{Is local sampling inherently more challenging than global sampling for spin systems?}
%\end{center}



\subsection{Our results}
In this paper, we address the aforementioned open question by designing a novel local sampler for spin systems under soft constraints,
showing that for these systems, local sampling is generally not significantly harder than global sampling.
Notably, our sampler is the first to achieve a near-critical regime for the important Ising model. 
Furthermore, our new local samplers imply efficient dynamic sampling for spin systems within improved regimes.

%\paragraph{\textbf{Spin system}}
%Before presenting our result, we first formally define spin systems. 



%For the ease of notation, we write $\bm{\lambda}=(\lambda_v)_{v\in V}$ and $\bm{A}=(A_e)_{e\in E}$, then a $q$-spin system can be defined solely by the tuple $(G=(V,E),\bm{\lambda},\bm{A})$.  

%We remark that in our definition, we do not require that $A_e$ is symmetric for each $e\in E$, which means we allow $A_{(u,v)}(x,y)\neq A_{(u,v)}(y,x)$ for some $(u,v)\in E,x\neq y\in [q]$. However, it should follow that $A_{(u,v)}(x,y)=A_{(v,u)}(y,x)$ for each $(u,v)\in E$ and each $x,y\in [q]$. We will specify the direction of $e=(u,v)$ for when we use the notation $A_e(x,y)$ when necessary.

We make the following assumption regarding the normalization and access model for spin systems.

\begin{assumption}[access model]\label{assumption:access-model}
Let $\+S=(G=(V,E),\bm{\lambda},\bm{A})$ be a $q$-spin system.
We assume:
\begin{itemize}
       \item For each $v\in V$, each neighbor $u\in N(v)$ can be accessed in $O(1)$ time.
    \item Each entry in every $\lambda_v$ and $A_e$ can be accessed in $O(1)$ time.
\end{itemize}
These can be achieved by storing $G$ as an adjacency list and representing $\lambda_v$ and $A_e$ as arrays.
\end{assumption}
%The normalization can be enforced without altering the Gibbs distribution.

\subsubsection{Local samplers for spin systems with bounded degrees}
Our fist general result provides a fast local sampler for $q$-spin systems that satisfy the following sufficient condition.

\begin{condition}[tractable regime for local sampler]\label{cond:main}
%\color{red} some condition (for example, $\bm{A}_{\min}/\bm{A}_{\max} \geq 1-\frac{c}{\Delta}$ for some constant $c$) \color{black} 
Let $\delta>0$ be a parameter, and $\+S=(G,\bm{\lambda},\bm{A})$ be a $q$-spin system on a graph $G=(V,E)$ with maximum degree $\Delta$.
The following condition holds:
\begin{itemize}
 \item (\textbf{Normalized}) All $\lambda_v$ and $A_e$ are normalized, i.e.,
\[
\forall v\in V,\quad \sum\limits_{c\in [q]}\lambda_v(c)=1\quad\text{ and }\quad\forall e\in E,\quad \max\limits_{i,j\in [q]}A_e(i,j)=1.
\]
 This normalization can be enforced without altering the Gibbs distribution.
 \item (\textbf{Permissive}) For every edge $e=(u,v)\in E$ and every pair of spin values $c_1,c_2\in [q]$,
    \[
    A_e(c_1,c_2) \geq \lb(\Delta,\delta)\defeq 1 - \frac{1-\delta}{2\Delta}.
    \]
\end{itemize}
\end{condition}

The following theorem presents our local sampler for spin systems with bounded degrees.
%for sampling from a marginal distribution $\mu_{\Lambda}$ in response to a query for a subset  $\Lambda\subseteq V$.
\begin{theorem}[local sampler for spin systems with bounded degrees]\label{theorem:local-sampler}
Assume \Cref{assumption:access-model} and \Cref{cond:main} hold for the input $q$-spin system $\+S=(G,\bm{\lambda},\bm{A})$.
There exists an algorithm that, 
given any query subset $\Lambda\subseteq V$, outputs a perfect sample $X\sim \mu^{\+S}_{\Lambda}$ in expected time $O\left(|\Lambda|\Delta\log q \right)$. 
% There exists an algorithm that, 
% for any $q$-spin system $\+S=(G,\bm{\lambda},\bm{A})$ on a graph $G=(V,E)$ with maximum degree $\Delta$ satisfying \Cref{cond:main}, 
% given any query subset $\Lambda\subseteq V$, outputs a perfect sample $X\sim \mu^{\+S}_{\Lambda}$ in expected time $O\left(|\Lambda|\Delta\log q \right)$. 
\end{theorem}

Next, we apply \Cref{theorem:local-sampler} to one of the most important spin systems with soft constraints: the Ising model. 
Recall the definition of the Ising model, which is a 2-spin system with an interaction matrix 
$$A_e=\begin{pmatrix}
    \beta & 1\\
    1 & \beta\\
\end{pmatrix}$$ 
at each edge $e\in E$ and an arbitrary external field $\lambda_v$ at each vertex $v\in V$.
Note that this standard definition of the Ising model does not satisfy the normalization condition in \Cref{cond:main} when $\beta>1$. 
However, we can transform such a (ferromagnetic) Ising model to satisfy this normalization condition,
by using $A_e/\beta$ as the interaction matrix, without altering its Gibbs distribution. 

Applying \Cref{theorem:local-sampler} then gives the following corollary.
% An Ising model $\+S=(G=(V,E),(\bm{\lambda},\beta))$ is a two-spin system whose interactions matrices (before normalization) are given by an 
% edge activity parameter $\beta$ such that 
% \[
% \forall e\in E, \quad A_e=\begin{pmatrix}
%     \beta & 1\\
%     1 & \beta\\
% \end{pmatrix}.
% \]
% Then, we have the following corollary for the Ising model. Note that the condition in \Cref{cor:ising} is asymptotically tight compared to the tree uniqueness condition in ~\eqref{eq:Ising-condition}.




\begin{comment}
To illustrate \Cref{cond:main}, we consider certain spin systems. An important special case of $q$-spin systems under extensive study is (symmetric) two-spin systems. Here, $q=2$ and the system is characterized by three parameters $\lambda,\beta,\gamma\geq 0$, such that
\[
\forall v\in V,\quad \lambda_{v}(c)=\begin{cases}\frac{\lambda}{1+\lambda}& c=1\\ \frac{1}{1+\lambda}& c=2 \end{cases}
\]
and 
\[
\forall e\in E, \quad A_e=\begin{pmatrix}
    \beta & 1\\
    1 & \gamma\\
\end{pmatrix}.
\]
Here, we remark that the $A_e$ presented above for two-spin systems is the interaction matrix before normalization. When $\max(\beta,\gamma)>1$, all parameters in $A_e$ should be multiplied by $1/\max(\beta,\gamma)$ according to \Cref{assumption:access-model}. A two-spin system can then be defined solely by the tuple $\+S=(G=(V,E),(\lambda,\beta,\gamma))$.



Among all two-spin systems, two particularly studied classes of two-spin systems are:
\begin{itemize}
    \item The hardcore model with fugacity $\lambda$, which corresponds to two-spin systems with $\beta=0$ and $\gamma=1$. Here, in the hardcore model, each configuration $\sigma\in [q]^V$ with $\mu(\sigma)>0$ corresponds to an independent set $I_{\sigma}\in G$ such that $\mu(I_{\sigma})\propto \lambda^{|I_{\sigma}|}$.
    \item The Ising model with edge activity $\beta$ and external field $\lambda$, which corresponds to two-spin systems with $\beta=\gamma>0$. Here in the Ising model, each configuration $\sigma\in [q]^V$ has probability $\mu(\sigma)\propto \beta^{m(\sigma)}\lambda^{n_{1}(\sigma)}$, where $m(\sigma)$ denotes the number of monochromatic edges in $\sigma$ and $n_{1}(\sigma)$ denotes the number of vertices set to $1$ in $\sigma$.
\end{itemize}  

However, our local sampler works in a much more unified sense as it also works up to near-critical regimes for the Ising model with arbitrary external fields: 
\end{comment}

\begin{corollary}[local Ising sampler]\label{cor:ising}
There exists an algorithm that, 
for any Ising model on a graph $G=(V,E)$ with maximum degree $\Delta$, arbitrary external field $\lambda_v$ at each $v\in V$,
and edge activity $\beta$ satisfying \begin{equation}\label{eq:Ising-condition}
    \beta \in\left(\frac{\Delta-0.5}{\Delta},\frac{\Delta}{\Delta-0.5}\right),
     \end{equation}
      outputs a perfect sample $X\sim \mu_{\Lambda}$ in expected time $O\left(|\Lambda|\Delta\right)$, 
      given any query subset $\Lambda\subseteq V$.  
\end{corollary}
The condition in~\eqref{eq:Ising-condition} falls within the same regime of $\left(1-\Theta\left(\frac{1}{\Delta}\right),1+\Theta\left(\frac{1}{\Delta}\right)\right)$ as the uniqueness condition in \eqref{eq:Ising-uniqueness-condition} and achieves an exponential improvement upon the local uniformity condition in \eqref{eq:Ising-local-uniformity}.

\begin{comment}
We remark that the result in \Cref{cor:ising} can easily be generalized for arbitrary two-spin systems $\+S=(G=(V,E),(\beta>0,\gamma>0,\lambda))$ with soft constraints in the regime of 
\[
\sqrt{\beta\gamma} \in\left(\frac{\Delta-0.5}{\Delta},\frac{\Delta}{\Delta-0.5}\right),
\]
with is also near-critical.
\end{comment}

% \paragraph{\textbf{Potts model}}
% Another spin system of interest is the Potts model, which characterizes the important combinatorial problem of proper $q$-colorings with soft constraints. The Potts model $\+S=(G=(V,E),(\bm{\lambda},\beta))$ with edge activity $\beta>0$ is a $q$-spin system such that all interaction matrices $A_e$ have diagonal entries $\beta$ and all off-diagonal entries equal to $1$ (before normalization). When $q=2$, the Potts model becomes exactly the Ising model. Our local sampler works for the Potts model under the same regime as \Cref{cor:ising}.


% \begin{corollary}[fast local sampler for the Potts model]\label{cor:potts}
% Given any Potts model $\+S=(G=(V,E),\bm{\lambda},\beta)$. Let $n=|V|$ and $\Delta$ be the maximum degree of $G$. If 
%      \begin{equation*}
%     \beta \in\left(\frac{\Delta-0.5}{\Delta},\frac{\Delta}{\Delta-0.5}\right),
%      \end{equation*}
%       then there exists an algorithm that, given any subset of vertices $\Lambda\subseteq V$, outputs a perfect sample $X\sim \mu^{\+S}_{\Lambda}$  within expected time $O\left(|\Lambda|\Delta\log q\right)$.  
% \end{corollary} 



%The proofs of \Cref{cor:ising} and \Cref{cor:potts} can be found in \Cref{sec:appendix-corollaries}. We then present \Cref{tab:lsample} to compare our results with the previous local samplers.
% Please add the following required packages to your document preamble:
% \usepackage{graphicx}
% Please add the following required packages to your document preamble:
% \usepackage{graphicx}
% Please add the following required packages to your document preamble:
% \usepackage{graphicx}

\begin{comment}
\begin{table}[H]
\centering
\resizebox{\columnwidth}{!}{%
\begin{tabular}{|c|c|c|c|}
\hline
  & hardcore model                                                                                & Ising model       & Potts model       \\ \hline
~\cite{AJ22} & \begin{tabular}[c]{@{}c@{}}$\lambda<\frac{1}{\Delta-1}$\\ expected time $O(|\Lambda|)$\end{tabular}   & not near-critical & not near-critical \\ \hline
~\cite{feng2023towards} & \begin{tabular}[c]{@{}c@{}}$\lambda<\frac{1}{\Delta-1}$\\ expected time $O(|\Lambda|)$\end{tabular} & not near-critical & not near-critical \\ \hline
\textbf{This work} &
  \begin{tabular}[c]{@{}c@{}}$\lambda<\frac{1}{\Delta}$\tablefootnote{We remark that our local sampler can easily adapt for hardcore model to achieve the $\lambda<\frac{1}{\Delta-1}$ regime and $O(|\Lambda|)$ expected running time. However, we compromised this small difference in the regime and running time as we want to present our result in a unified framework.}\\  expected time $O(|\Lambda|\Delta)$\end{tabular} &
  \begin{tabular}[c]{@{}c@{}}$\beta \in\left(\frac{\Delta-0.5}{\Delta},\frac{\Delta}{\Delta-0.5}\right)$\\  expected time $O(|\Lambda|\Delta)$\end{tabular} &
  \begin{tabular}[c]{@{}c@{}}$\beta \in\left(\frac{\Delta-0.5}{\Delta},\frac{\Delta}{\Delta-0.5}\right)$\\  expected time $O(|\Lambda|\Delta q)$\end{tabular} \\ \hline
\end{tabular}%
}
\caption{Comparison with previous local samplers}
\label{tab:lsample}
\end{table}

\end{comment}


\subsubsection{Local samplers for spin systems with unbounded degrees}
For the algorithms in \Cref{theorem:local-sampler} and \Cref{cor:ising},
the cost of answering each query is linear in the maximum degree $\Delta$.
This remains locally efficient for graphs where the maximum degree $\Delta$ is bounded by a constant.

For general graphs with potentially unbounded degrees, 
designing a local sampler that generates a sample $X\sim \mu_{\Lambda}$ within a cost of $\tilde{O}(|\Lambda|)$ seems impossible:
sampling from the marginal distribution $\mu_v$ requires access to the $A_e$ for all edges $e$ incident to $v$,
which alone incurs a time cost of $\Omega(\Delta)$.

%Note that our expected running time in \Cref{theorem:local-sampler} is linear with respect to $\Delta$, the maximum degree of the given graph $G$. When we assume that the given graph has a bounded degree, $\Delta$ can be viewed as a constant, and hence the sampler in \Cref{theorem:local-sampler} runs in time independent of the size of the graph. 

%We also investigate general cases where the graph has an unbounded degree, and we still wish for a local sampler with a running time sublinear with respect to the size of the graph in such cases. However, sublinear sampling in unbounded-degree graphs may not be possible without further assumptions due to the computational costs needed to even represent the marginal distribution, as shown in the following example.


% \begin{example}[computational costs for local samplers]
% Consider the spin system $\+S=(G=(V,E),\bm{A},\bm{\lambda})$ as follows:
% \begin{itemize}
%     \item The graph $G=(V,E)$ satisfies that $V=\{v_1,v_2,\dots,v_n\}$ and $E=\{(v_1,v_i)\}_{i=2}^{n}$, i.e., $G$ is a star graph centered at $v_1$;
%     \item For each $v\in V$, $\lambda_v(1)=\lambda_v(2)=\frac{1}{2}$;
%     \item For each $2\leq i\leq n$, let $e=(v_1,v_i)$, then 
%     \[
%     A_e({1,1})=A_e({1,2})=p_i,\quad A_e({2,1})=A_e({2,2})=1.
%     \]
% \end{itemize}
% Let $\mu=\mu^{\+S}$ be the distribution defined by this spin system, it is then easy to see that
% \[
% \mu(v_0=1)=\frac{\prod\limits_{i=2}^{n}p_i}{1+\prod\limits_{i=1}^{n}p_i},\quad \mu(v_0=2)= \frac{1}{1+\prod\limits_{i=1}^{n}p_i}.
% \]
% Note that it takes $\Omega(\Delta)=\Omega(n)$ time to calculate $\prod\limits_{i=2}^{n}p_i$, so it is hard to sample the marginal distribution of $v_1$ in sublinear time when only the parameters of the spin system is given.
% \end{example}

%Our second result shows that sublinear sampling is actually possible when we assume some precomputation is done with respect to the given spin system.

Nevertheless, efficient local sampling can be achieved on graphs with unbounded degrees through suitable preprocessing.

\begin{assumption}[preprocessing model]\label{assumption:additional-access-model}
Let $\delta>0$ be a parameter. 
%and $\+S=(G,\bm{\lambda},\bm{A})$ be a $q$-spin system on a graph $G=(V,E)$ with maximum degree $\Delta$ that satisfies \Cref{cond:main}.
Define $\lb\triangleq1-\frac{1-\delta}{2\Delta}$ as in \Cref{cond:main}.
We assume that the following values are precomputed and can be retrieved in $O(1)$ time:
\begin{itemize}
   \item $1-\lb^{i}$ and $\frac{1-\lb^{\lfloor i/2\rfloor }}{1-\lb^{i}}$ for each $0\leq i\leq \Delta$.
%   \item $\frac{1-\lb^{\lfloor i/2\rfloor }}{1-\lb^{i}}$ for each $0\leq i\leq \Delta$.
\end{itemize}
%where $\lb=\lb(\Delta,\delta)=1-\frac{1-\delta}{2\Delta}$ as in \Cref{cond:main}.
These values can be precomputed in $O(\Delta)$ in the unit-cost random-access machine (RAM) model.
\end{assumption}
%Here, the precomputation in \Cref{assumption:additional-access-model} can be done within time $O(\Delta^2)$ time in the word RAM model and $O(\Delta)$ in the unit cost RAM model, which is no larger than the input size of the spin system.

The following theorem presents our local sampler for general graphs with unbounded degrees.

\begin{theorem}[local sampler for spin systems with unbounded degrees]\label{theorem:sublinear-sampler}
Assume \Cref{assumption:access-model}, \Cref{assumption:additional-access-model}, and \Cref{cond:main} hold for the input $q$-spin system $\+S=(G,\bm{\lambda},\bm{A})$.
There exists an algorithm that, given any query subset $\Lambda\subseteq V$, outputs a perfect sample $X\sim\mu^\+S_{\Lambda}$ in expected time $O\left( |\Lambda|\log \Delta \log q\right)$. 
\end{theorem}


\subsubsection{Fast dynamic samplers with improved regimes}

Our local sampler also has a direct application to the problem of \emph{dynamic sampling} in $q$-spin systems under the same condition (\Cref{cond:main}).
In this setting, we aim to efficiently sample from a spin system that evolves dynamically over time, ensuring that each update incurs only a small incremental cost.

Formally, given a spin system $\+S=(G=(V,E),(\bm{\lambda},\bm{A}))$, we maintain a query set $\Lambda\subseteq V$, initially set to $\emptyset$.
Over time, we consider the following types of updates to both the query set and the spin system:

\begin{definition}[supported updates in the dynamic sampler]\label{definition:updates}
We consider types of updates as follows:
\begin{itemize}
    \item Modifying the query set, in one of two ways:
    \begin{itemize}
        \item \textbf{Adding query vertices}: Select $\Lambda'\subseteq V$ such that $\Lambda\cap \Lambda'=\emptyset$, and update $\Lambda\gets \Lambda\cup \Lambda'$.
        \item \textbf{Removing query vertices}: Select $\Lambda'\subseteq \Lambda$ and update $\Lambda\gets \Lambda\setminus \Lambda'$;
    \end{itemize}
    \item Modifying the parameters of the spin system $\+S$, in one of two ways:
    \begin{itemize}
        \item \textbf{Updating external fields}:  Modify $\lambda_v$ for some $v\in V$;
        \item \textbf{Updating interaction matrices}: Modify $A_e$ for some $e\in E$.
    \end{itemize}
\end{itemize}
\end{definition}
These updates provide significant generality: adding or removing vertices and edges in the spin system can be realized through appropriate modifications to external fields and interaction matrices.

Our result for dynamic sampling in spin systems is then presented as follows.

\begin{theorem}[dynamic sampling for spin systems]\label{theorem:dynamic-sampler}
Assume \Cref{assumption:access-model} and \Cref{cond:main} hold for both the current $q$-spin system $\+S=(G,\bm{\lambda},\bm{A})$ and the updated system $\+S'$ (if the spin system is updated),
and that an update, independent of the current sample, as defined in \Cref{definition:updates}, is performed.

Then, there exists a dynamic sampling algorithm that maintains a sample correctly distributed as $X\sim \mu_{\Lambda}$ for the current Gibbs distribution $\mu$ and the query set $\Lambda\subseteq V$, with incremental time cost for each update:
\begin{enumerate}[leftmargin=*]
    \item \textbf{Adding query vertices}: Expected $O(|\Lambda'|\Delta \log q)$ time where $\Lambda'$ is the set of added vertices; \label{item:thm-update-adding-query-vertices}
    \item \textbf{Removing query vertices}: Expected $O(|\Lambda'| \Delta \log q)$ time where $\Lambda'$ is the set of removed vertices; \label{item:thm-update-removing-query-vertices}
    \item \textbf{Updating external fields}: Expected $O(q\Delta)$ time; \label{item:thm-update-external-field}
    \item \textbf{Updating interaction matrices}: Expected $O(q^2+q\Delta)$ time. \label{item:thm-update-interaction-matrix}
\end{enumerate}
\end{theorem}

Note that Items \ref{item:thm-update-external-field} and \ref{item:thm-update-interaction-matrix} of \Cref{theorem:dynamic-sampler} already have input sizes of $\Omega(q)$ and $\Omega(q^2)$, respectively.


A prior related work is \cite{feng2019dynamic}, which considers the problem of dynamic sampling in general graphical models and can be applied to spin systems. 
Compared to their dynamic sampling algorithm, ours has the following advantages: 
\begin{itemize}
    \item Our algorithm can maintain local samples and handle local queries at a local cost, whereas the algorithm in \cite{feng2019dynamic} needs to maintain global samples.
    \item Our algorithm has an improved regime for the Ising and Potts models. The dynamic sampler in \cite{feng2019dynamic} works under the regime $\beta \in\left(\frac{\Delta-1/\alpha}{\Delta},\frac{\Delta}{\Delta-1/\alpha}\right)$, where $\alpha\approx 2.221\dots$, while ours works under $\beta \in\left(\frac{\Delta-0.5}{\Delta},\frac{\Delta}{\Delta-0.5}\right)$.
\end{itemize}

\begin{comment}

For the latter term, we show the improvement of our regimes for these typical models compared to ~\cite{feng2019dynamic} in the following \Cref{tab:dynamic}.

% Please add the following required packages to your document preamble:
% \usepackage{graphicx}
\begin{table}[H]
\centering
\resizebox{0.8\columnwidth}{!}{%
\begin{tabular}{|c|c|c|}
\hline
 &
 
  Ising model &
  Potts model \\ \hline
~\cite{feng2019dynamic} &
 \begin{tabular}[c]{@{}c@{}}$\beta \in\left(\frac{\Delta-1/\alpha}{\Delta},\frac{\Delta}{\Delta-1/\alpha}\right)$,\\ where $\alpha\approx 2.221\dots$\tablefootnote{Here $\alpha$ is the root of $\alpha=1+\frac{2}{1+\mathrm{exp}(-1/\alpha)}$.}\end{tabular}
  &
   \begin{tabular}[c]{@{}c@{}}$\beta \in\left(\frac{\Delta-1/\alpha}{\Delta},\frac{\Delta}{\Delta-1/\alpha}\right)$,\\ where $\alpha\approx 2.221\dots$\end{tabular} 
  \\ \hline
This work &
  $\beta \in\left(\frac{\Delta-0.5}{\Delta},\frac{\Delta}{\Delta-0.5}\right)$
   &  
    $\beta \in\left(\frac{\Delta-0.5}{\Delta},\frac{\Delta}{\Delta-0.5}\right)$
 \\ \hline
\end{tabular}%
}
\caption{Comparison with the dynamic sampler in ~\cite{feng2019dynamic}}
\label{tab:dynamic}
\end{table}

\end{comment}


%Still, the dynamic sampling algorithm in ~\cite{feng2019dynamic} has the advantage of being more generalized and robust against stronger adversaries\footnote{In our dynamic sampling algorithm, the complexity bound is based on the assumption that the updates are prepared in advance or independent with the outcomes of the sampler. However, the algorithm in ~\cite{feng2019dynamic} is efficient even when the updates are prepared online by an adversary, which can depend on the results of the sampler.}.

\subsection{Techniques and related work}
Previous works on local sampling algorithms include \cite{AJ22} and \cite{feng2023towards}, both of which rely on unconditional marginal lower bounds, i.e., the local uniformity property.
%
The work of \cite{AJ22} introduced a novel local sampling algorithm called ``\emph{lazy depth-first search}'' ({a.k.a.}~the A-J algorithm). 
To sample the spin of a vertex according to its correct marginal distribution, the algorithm first draws a random spin according to the unconditional marginal lower bounds, and with remaining probability, it recursively samples the spins of all neighboring vertices.
%
The algorithm in \cite{feng2023towards} takes a different approach, employing a backward deduction framework for Markov chains, referred to as ``\emph{coupling towards the past}'' (CTTP). 
Their method uses systematic Glauber dynamics combined with a grand coupling based on unconditional marginal lower bounds, 
allowing the spin of a vertex to be inferred via a convergent information-percolation process.
%
Despite their differences, the recursion style and sufficient conditions in \cite{AJ22} and \cite{feng2023towards} share similarities.
For a more in-depth comparison of the two algorithms, we refer the reader to Section 1.2 of \cite{feng2023towards}.


Our local sampler builds upon the coupling towards the past (CTTP) framework for Markov chains. 
Unlike the original CTTP algorithm in \cite{feng2023towards}, which relies on a default grand coupling based on unconditional marginal lower bounds, 
our approach introduces a new grand coupling for CTTP that eliminates this requirement.
This marks the first efficient local sampler that does not depend on unconditional marginal lower bounds.

We achieve this by first generalizing the CTTP framework through abstract ``\emph{marginal sampling oracles}'', 
which are procedures that sample from conditional marginal distributions, given oracle access to the neighborhood configuration.
With this abstraction, each implementation of a marginal sampling oracle may lead to a specific simulation of Glauber dynamics, or more precisely, to a specific grand coupling of the chain. 
We then design a novel implementation of the marginal sampling oracle using rejection sampling, 
leveraging the permissiveness of local constraints instead of relying on unconditional marginal lower bounds.
This gives us fast local samplers beyond the regime of local uniformity.

% Specifically, our new grand coupling is constructed from the concept of "marginal sampling oracles," a procedure that enables sampling from conditional marginal distributions, given oracle access to the neighborhood configuration. We demonstrate that, as long as a marginal sampling oracle exists with expected accesses to the neighborhood fewer than one, the CTTP framework based on this grand coupling terminates quickly, enabling the construction of an efficient local sampler. To achieve this, we implement the marginal sampling oracle using a rejection sampling procedure, leveraging soft constraints instead of directly applying unconditional marginal lower bounds, thereby resulting in fast local sampling beyond local uniformity.

%Specifically, we show that  
%which samples from the conditional marginal distribution $\mu^{\sigma}_{v}$ of a vertex $v$, given oracle access to the spin $\sigma(u)$ of its neighbors $u\in N(v)$ for a neighborhood configuration $\sigma\in [q]^{N(v)}$.
%This generalization provides significantly greater flexibility compared to the default grand coupling approach.
%
%To realize this, we implement a marginal sampling oracle via ``local rejection sampling'' procedure, 
%leading to an efficient local sampling algorithm in the desired regime.
%The key to our improved performance, particularly for the Ising model, is that our method directly exploits the marginal distribution rather than relying on unconditional lower bounds.




%\subsection{Related works}

%The most relevant works are the local and dynamic sampling algorithms introduced in~\cite{AJ22,feng2023towards,feng2019dynamic}, as discussed above.

Our local sampling algorithm also falls into the category of providing \emph{local access to large random objects}~\cite{amartya2020local,biswas2022local,morters2022sublinear}.
Given a $q$-spin system $\+S = (G = (V, E), \bm{\lambda}, \bm{A})$ and public random bits, our algorithm can generate consistent samples $X_{\Lambda}$ such that $X \sim \mu = \mu^{\+S}$ upon multiple queries of any subset of vertices $\Lambda \subseteq V$, using only a local number of probes for public random bits.

The design of our local sampler is based on the backward deduction of Glauber dynamics. We also show that local sampling algorithms can be achieved even for unbounded-degree graphs, assuming some precomputation with respect to the spin system. A related work of this is~\cite{anari2022entropic}, where they perform each update of the forward Glauber dynamics for the hardcore model and the Ising model in sublinear time, with precomputation assumed. However, their application and techniques differ from ours: They use separate algorithms for the hardcore and Ising models, while we propose a unified algorithm for all spin systems. Additionally, they employ Bernoulli factory algorithms to accelerate updates for the Ising model, which contrasts with our approach.


\subsection{Organization} 
The paper is organized as follows:

In \Cref{sec:prelim}, we introduce the necessary preliminaries.

In \Cref{sec:framework}, we present a generalized framework for backward deduction of Markov chains, showing how ``marginal sampling oracles'' with certain conditions can be transformed into fast local sampling algorithms.

In \Cref{sec:local}, we design a new marginal sampling oracle and apply it to obtain our local sampler, proving \Cref{theorem:local-sampler,theorem:sublinear-sampler}.

In \Cref{sec:dynamic}, we apply our local sampler to the dynamic sampling problem, proving \Cref{theorem:dynamic-sampler}.

In \Cref{sec:conclusions}, we summarize the contributions of the paper and outline potential future directions.




\section{Preliminaries}\label{sec:prelim}

\subsection{Markov chain basics}

Let $\Omega$ be a (finite) state space.
Let $(X_t)_{t = 1}^\infty$ be a Markov chain over the state space $\Omega$ with transition matrix $P$.
%Let $P:\Omega \times \Omega \to \mathbb{R}_{\geq 0}$ denote the transition matrix, i.e. for any $x,y \in \Omega$, $P(x,y) = \Pr[X_t = y \mid X_{t-1} = x]$. 
%We often use transition matrix $P$ to refer the corresponding Markov chain. 
A distribution $\pi$ over $\Omega$ is a \emph{stationary distribution} of $P$ if $\pi = \pi P$.
The Markov chain $P$ is \emph{irreducible} if for any $x,y \in \Omega$, there exists a timestamp $t$ such that $P^t(x,y) > 0$.
The Markov chain $P$ is \emph{aperiodic} if for any $x \in \Omega$, $\gcd\{t\mid P^t(x,x) > 0\} = 1$.
If the Markov chain $P$ is both irreducible and aperiodic, then it has a unique stationary distribution.
The Markov chain $P$ is \emph{reversible} with respect to the distribution $\pi$ if the following \emph{detailed balance equation} holds.
\begin{align*}
	\forall x, y \in \Omega,\quad \pi(x) P(x,y) = \pi(y)P(y,x),
\end{align*}
which implies $\pi$ is a stationary distribution of $P$.
The \emph{mixing time} of the Markov chain $P$ is defined by
\begin{align*}
	\forall \epsilon > 0, \quad T(P,\epsilon) \defeq \max_{X_0 \in \Omega} \max\{t \mid \DTV{P^t(X_0,\cdot)}{\mu} \leq \epsilon\},
\end{align*}
where the \emph{total variation distance} is defined by
\begin{align*}
\DTV{P^t(X_0,\cdot)}{\mu} \defeq \frac{1}{2}\sum_{y \in \Omega}\abs{P^t(X_0,y)-\mu(y)}.	
\end{align*}


\subsection{Systematic scan Glauber dynamics}\label{sec:sys-scan}


The \emph{systematic scan} Glauber dynamics is a generic way to sample from Gibbs distributions defined by spin systems. Given a $q$-spin system $\+S=(G=(V,E),\bm{\lambda},\bm{A})$. Let $n=|V|$ and assume an arbitrary ordering $V=\{v_0,v_1,\dots,v_{n-1}\}$, and let $T>0$ be some finite integer, the $T$-step systematic scan Glauber dynamics $\+P{(T)}=\+P^{\+S}(T)$
\begin{enumerate}
    \item starts with an arbitrary configuration $X_{-T}\in [q]^V$ satisfying $\mu(X_{-T})>0$ at time $t=-T$;
    \item at each time $-T< t\leq  0$,
    \begin{enumerate}
        \item picks the vertex $v= v_{i(t)}$ where $i(t)\triangleq t\mod n$, let $X_t(u)=X_{t-1}(u)$ for every $u\in V\setminus \{v\}$;
        \item resample $X_t(v)$ from the marginal distribution $\mu^{X_{t-1}}_{v}$ on $v$ conditioning on $X_{t-1}$ where
        \[
        \forall c\in [q],\quad \mu^{X_{t-1}}_{v}(c)=\mu^{X_{t-1}(N(v))}_{v}(c)\propto \lambda_v(c)\prod\limits_{e=(u,v)\in E}A_{e}(\sigma(u),c).
        \]
        Here, the first equality is due to the \emph{conditional independence} property of Gibbs distributions.
   \end{enumerate}
\end{enumerate}




The systematic scan Glauber dynamics is not a time-homogeneous Markov chain. However, by bundling $n$ consecutive updates together, we can obtain a time-homogeneous Markov chain, which is aperiodic and reversible, which is sufficient for us to apply the following theorem.
\begin{theorem}[\cite{levin2017markov}]\label{thm-convergence}
Let $\mu$ be a distribution with support $\Omega \subseteq [q]^V$. Let $(X_t)_{t =0}^\infty$ denote the systematic scan Glauber dynamics on $\mu$. If $(X_t)_{t =0}^\infty$ is irreducible over $\Omega$, it holds that
\begin{align*}
	\forall X_0 \in \Omega,\quad \lim_{t \to \infty}\DTV{X_t}{\mu} = 0.
\end{align*}
\end{theorem}



\section{Coupling towards the past without marginal lower bounds}\label{sec:framework}

Our local sampler is based on the Coupling Towards The Past (CTTP) framework recently introduced in \cite{feng2023towards}, 
which constructs a local sampler by evaluating multiple spin states from stationary Markov chains through backward deduction. 
Our framework generalizes the CTTP framework by replacing the default grand coupling, 
which uses unconditional marginal lower bounds, 
with grand couplings defined by arbitrary ``marginal sampling oracles''. 
This generalization allows us to design a specific marginal sampling oracle that leads to a local sampler beyond the regime of local uniformity.



% Our local sampler comes from the Coupling Towards The Past (CTTP) framework recently introduced in ~\cite{feng2023towards}. The CTTP framework constructs a local sampler by evaluating multiple spin states from stationary Markov chains through backward deduction. In fact, our framework generalizes the CTTP in ~\cite{feng2023towards} by changing the default grand coupling using unconditional marginal lower bounds into grand couplings defined by arbitrary ``marginal sampling oracles''. This generalization made possible our design of a specific ``marginal sampling oracle'' that led to a local sampler under an improved regime. 

%In this section, we will introduce this framework, as well as giving a few sufficient conditions that would imply 


\subsection{Marginal sampling oracles}
Before introducing the CTTP framework, we first define \emph{marginal sampling oracles}.
Due to the conditional independence property of Gibbs distributions, to sample from the (conditional) marginal distribution $\mu^{\sigma}_v$ for a vertex $v \in V$ and a configuration $\sigma \in [q]^{V \setminus {v}}$, it suffices to retrieve the spins of all neighbors, $\sigma(N(v))$.
A \emph{marginal sampling oracle} generalizes this concept by producing a marginal sample, given oracle access to the spin $\sigma(u)$ of each neighbor $u \in N(v)$.

%Before introducing the CTTP framework, we first define \emph{marginal sampling oracles}. Due to the conditional independence property of Gibbs distributions, to sample from the (conditional) marginal distribution $\mu^{\sigma}_v$ for a vertex $v\in V$ and some configuration $\sigma\in [q]^{V\setminus \{v\}}$, it is sufficient to only know the neighboring configuration $\sigma(N(v))$. A \emph{marginal sampling oracle} generalizes this concept by producing a marginal sample given only oracle access to $\sigma(u)$ for each $u\in N(v)$.  %These aim to realize the systematic scan Glauber dynamics, making it easier to apply backward deduction. 

\begin{definition}[marginal sampling oracle]\label{definition:locally-defined-grand-coupling}
Let $\mu$ be a distribution over $[q]^V$. For a variable $v \in V$, we define $\eval^\+O(v)$ as a procedure that
%\begin{itemize}
%    \item The procedure $\eval^\+O(v)$ 
    makes oracle queries to $\+O(u)$, which consistently returns a value $c_u \in [q]$ for each $u \in N(v)$, and outputs a value $c \in [q]$.
%\end{itemize}

We say that $\eval^\+O(v)$ is a \emph{marginal sampling oracle} at $v$ (with respect to $\mu$) if: 
\begin{itemize}
    \item for each $\sigma \in [q]^{N(v)}$, assuming $\+O(u)$ consistently returns $\sigma(u)$ for each $u \in N(v)$, the output of $\eval^\+O(v)$ is distributed exactly as $\mu^{\sigma}_v$.
\end{itemize}
\end{definition}

Recall the definition of systematic scan Glauber dynamics $\+P(T)$ in \Cref{sec:sys-scan}.
Using a marginal sampling oracle, the systematic scan Glauber dynamics can be simulated as follows.

\begin{definition}[simulation of systematic scan Glauber dynamics via a marginal sampling oracle]\label{definition:systematic-scan-coupling-realization}
The systematic scan Glauber dynamics $\+P(T)$  with respect to $\mu$  is simulated as:
\begin{enumerate}
    \item start with an arbitrary configuration $X_{-T}\in [q]^V$ satisfying $\mu(X_{-T})>0$ at time $t=-T$;
    \item at each time $-T< t\leq  0$,
    \begin{enumerate}
        \item pick the vertex $v= v_{i(t)}$ where $i(t)\triangleq t\mod n$, let $X_t(u)=X_{t-1}(u)$ for every $u\in V\setminus \{v\}$;
        \item let $\eval^\+O(v)$ be a marginal sampling oracle (w.r.t $\mu$) at $v$ where the oracle accesses $\+O(u)$ are replaced with $X_{t-1}(u)$ for each $u\in N(v)$, update $X_t(v)\gets \eval^\+O(v)$.\label{item:update}
    \end{enumerate}
\end{enumerate}
\end{definition}

\begin{remark}[grand coupling]\label{remark:implicit-grand-coupling}
In \Cref{definition:systematic-scan-coupling-realization}, the only randomness involved is within the subroutine $\eval^{\+O}(v)$. 
Notably, for any implementation of a marginal sampling oracle, 
\Cref{definition:systematic-scan-coupling-realization} specifies a simulation of systematic scan Glauber dynamics,
and implicitly defines a \emph{grand coupling} that couples the Markov chain across all possible initial configurations. 
To see this, consider pre-sampling all random variables used within $\eval^\+O(v_{i(t)})$ for each $t$, thereby defining the grand coupling.
%Then, the value returned by $\eval^\+O(v_{i(t)})$ is determined by the arguments to the oracle calls and their returned values.
\end{remark}


\subsection{Simulating stationary Markov chains using backward deduction}

We present the CTTP framework for constructing the local sampler, 
which is a backward deduction of the forward simulation described in \Cref{definition:systematic-scan-coupling-realization} (or equivalently, the grand coupling constructed in \Cref{remark:implicit-grand-coupling}).

Consider the systematic scan Glauber dynamics running from the infinite past toward time $0$, which is the limiting process of $\+P(T)$ as $T\to \infty$, denoted by $\+P(\infty)$. 
By \Cref{thm-convergence}, when $\+P(T)$ is irreducible, the state $X_0$ of this process is distributed exactly according to $\mu$. Our local sampler is then constructed by resolving the outcome $X_0(\Lambda)$, where $\Lambda$ is the queried set of vertices. For any $t\leq 0$ and $u\in V$, define \begin{equation}\label{eq:definition-pred}
\pred_t(u)\defeq \max \{t'\mid t'\leq t, v_{i(t')}=u\}
\end{equation}
as the last time, up to $t$, that vertex $u$ was updated.   The local sampler is presented in \Cref{Alg:lsample}.

%We may use backward deduction to resolve the update in $\+P(T)$ at some given time $t$. We note that by \Cref{item:update} of \Cref{definition:systematic-scan-coupling-realization}, the updated value at time $t$ only depends on the randomness used for the update at time $t$ and the states of its current neighbors $X_{t-1}(N(v))$. Then, the procedure is given as follows:

\begin{comment}
We assume that we have a
realization of a systematic scan Glauber dynamic $\left(X_t\right)_{-\infty<t\leq 0}$ that runs from the infinite past towards time $0$ based on some locally-defined grand coupling $f$ and a sequence of random reals $\{\+R_t\}_{-\infty<t\leq 0}$. 

Here in \Cref{Line:local-sampler-resolve} of \Cref{Alg:lsample}, $\resolve_f$ is a procedure that realizes the following functionality:
\begin{itemize}
    \item For each $-\infty<t\leq 0$, $\resolve_f(t)$ takes as input a timestamp $t$ and the locally-defined grand coupling $f$, and returns $X_t(v_{i(t)})$, the value $v_{i(t)}$ get updated to.
\end{itemize}
 So basically, in \Cref{Line:local-sampler-resolve}, we are recording the final values each vertex $v\in S$ gets updated to. $\resolve_f$ is formally defined as follows.
\end{comment}



\begin{algorithm}[H]
\caption{$\lsample(\Lambda;M)$} \label{Alg:lsample}
\SetKwInput{KwData}{Global variables}
\KwIn{A $q$-spin system $\+S=(G=(V,E),\bm{\lambda},\bm{A})$, a subset of variables $\Lambda\subseteq V$.}
\KwOut{A random value $X \in [q]^{\Lambda}$.}
\KwData{A mapping $M: \mathbb{Z}\to [q]\cup\{\perp\}$.}
$X\gets \emptyset, M\gets \perp^{\mathbb{Z}}$\label{Line:local-sampler-initialization}\;
\ForAll{$v\in \Lambda$}{
    $X(v)\gets \resolve(\pred_0(v);M)$\;\label{Line:local-sampler-resolve}
}
\Return $X$\;
\end{algorithm}

\Cref{Line:local-sampler-resolve} of \Cref{Alg:lsample} utilizes a procedure $\resolve$, formally presented as \Cref{Alg:resolve}, 
which takes as input a timestamp $t\leq 0$, and determines the outcome of the update at time $t$ of $\+P(\infty)$.

\begin{algorithm}[H]
\caption{$\resolve(t; M)$} \label{Alg:resolve}
\SetKwInput{KwData}{Global variables}
\KwIn{A $q$-spin system $\+S=(G=(V,E),\bm{\lambda},\bm{A})$, a timestamp $t\leq 0$.}
\KwOut{A random value $x\in [q]$}
\KwData{A mapping $M: \mathbb{Z}\to [q]\cup\{\perp\}$.}
\lIf{$M(t) \neq \perp$}{\Return $M(t)$\label{Line:resolve-finite-memoization}}
$M(t)\gets \eval^\+O(v_{i(t)})$, with $\+O(u)$ replaced by $\resolve(\pred_t(u);M)$ for each $u\in N(v_{i(t)})$\;\label{Line:resolve-sample}
\Return $M(t)$\; \label{Line:resolve-final-return}
\end{algorithm}

A global data structure $M$ is maintained within \Cref{Alg:lsample}, storing the resolved values $M(t)$ for updates at each time $t$.
It is initialized as $M=\perp^{\mathbb{Z}}$ in \Cref{Line:local-sampler-initialization}. 
This data structure $M$ is introduced to facilitate memoization:
the outcome of $\resolve(t)$ is evaluated only once, ensuring consistency across multiple calls for the same $t$. 
%Also, this memoization guarantees that the random seed $\+R_t$ at \Cref{Line:resolve-generate} will only be generated once for each timestamp $t$.  
For simplicity, we omit explicit references to $M$ and write $\lsample(\Lambda)$ and $\resolve(t)$ instead of $\lsample(\Lambda;M)$ and $\resolve(t;M)$.

\Cref{Line:resolve-sample} of \Cref{Alg:resolve} invokes a procedure $\eval^{\+O}(v_{i(t)})$,
which is abstractly defined in \Cref{definition:locally-defined-grand-coupling} but is not yet fully implemented.
Recall that the purpose of $\eval^{\+O}(v_{i(t)})$ is to infer the value to which the vertex $v=v_{i(t)}$ is updated in $\+P(\infty)$ at time $t$, which is distributed as $\mu^{X_{t-1}(N(v))}_v$ given access to $X_{t-1}(N(v))$. 
However, since \Cref{Alg:resolve} implements a backward deduction (as opposed to a forward simulation)  of the chain, the neighborhood configuration $X_{t-1}(N(v))$ at time $t$ is not available directly. 
To address this, the algorithm recursively applies \Cref{Alg:resolve} to infer the last updated value of each neighbor $u\in N(v)$ before time $t$ (as specified in \Cref{Line:resolve-sample} of \Cref{Alg:resolve}).

Formally, the subroutine $\eval(t)$ must satisfy the following local correctness condition:


\begin{condition}[local correctness of $\eval^\+O(v)$]\label{condition:local-correctness}
For each $v\in V$, the procedure $\eval^{\+O}(v)$ is a marginal sampling oracle at $v$, satisfying the requirement of \Cref{definition:locally-defined-grand-coupling}.
\end{condition}

%We remark that \Cref{Alg:resolve} is still well-defined when we set $T=\infty$, which can be imagined as corresponding to a Markov chain running from infinite past towards time $0$. In this case, $M$ can simply be initialized as $\perp^{\mathbb{Z}}$.  By \Cref{thm-convergence}, if $\+P(T)$ is irreducible, then the distribution of $X_0$ converges to $\mu$ as $T\to\infty$, agnostic of the initial state. Note that in this case, \Cref{Alg:resolve} does not necessarily terminate. 
Recall that \Cref{Alg:resolve} is designed to resolve the outcome of $\+P(\infty)$ at time $0$. However, the limiting process $\+P(\infty)$ is well-defined only if $\+P(T)$ is irreducible.  Additionally, we note that \Cref{Alg:resolve} does not necessarily terminate. 
Nonetheless, we provide a sufficient condition that ensures both the irreducibility of $\+P(T)$ and the termination of \Cref{Alg:resolve}. 



\begin{condition}[immediate termination of $\eval^{\+O}(v)$]\label{condition:immediate-termination}
   For each $v\in V$, let $\+E_v$ be the event that $\eval^{\+O}(v)$ terminates without making any calls to $\+O$. 
   Then, the following must hold:
    % \begin{itemize}
    %     \item Let $\+E_v$ be the event that $\eval^{\+O}(v)$ terminates without any calls of $\+O$, then 
        \[
        \Pr{\+E_v}>0.
        \]
    % \end{itemize}
\end{condition}

 %Our local sampler then takes as input a subset of variables $\Lambda\subseteq V$, and resolves the final value to which each variable gets updated up to time $0$ using $\resolve$.

We now establish the correctness of \Cref{Alg:lsample}, assuming Conditions \ref{condition:local-correctness} and \ref{condition:immediate-termination}.
\begin{lemma}[conditional correctness of \Cref{Alg:lsample}]\label{lemma:lsample-correctness}
   Assume that Conditions \ref{condition:local-correctness} and \ref{condition:immediate-termination} hold for $\eval^{\+O}(v)$.
   Then, for any $\Lambda \subseteq V$, \Cref{Alg:lsample} terminates with probability 1 and returns a random value $X \in [q]^{\Lambda}$ distributed according to $\mu_{\Lambda}$ upon termination. 
\end{lemma}
\Cref{lemma:lsample-correctness} is proved later in \Cref{sec:appendix-correctness}.
%
 It guarantees the termination and correctness of \Cref{Alg:lsample}, without addressing its efficiency.
 Next, we provide a sufficient condition for the efficiency of \Cref{Alg:lsample}.



\begin{condition}[condition for fast termination of $\eval^{\+O}(v)$]\label{condition:fast-termination}
Let $\delta > 0$ be a parameter.
 For each $v\in V$, and each $\sigma\in [q]^{N(v)}$ such that $\+O(u)$ consistently returns $\sigma(u)$ for all $u\in N(v)$,
 let $\+{T}^{\sigma}_v$ denote the total number of calls to $\+O(u)$ over all $u\in N(v)$. 
 Then, the following holds:
% \begin{itemize}
%     \item Let $\+{T}^{\sigma}_v$ denote the total number of calls to $\+O(u)$ over all $u\in N(v)$. Then,  
    \[
    \E{\+{T}^{\sigma}_v}\leq 1-\delta.
    \]
% \end{itemize}
\end{condition}

% We then conclude the subsection with the following lemma, which shows the efficiency of our local sampler assuming \Cref{condition:fast-termination}. 
% We remark that in \Cref{lemma:lsample-efficiency}, we upper bound the total number of recursive $\resolve$ calls instead of just the number of first calls for each $t\leq 0$.

We conclude this subsection with the following lemma, which establishes the efficiency of our local sampler under the assumption of \Cref{condition:fast-termination}.
Notably, it provides an upper bound on the total number of recursive $\resolve$ calls, rather than simply counting the number of initial calls for each $t \leq 0$.

\begin{lemma}[conditional efficiency of \Cref{Alg:lsample}]\label{lemma:lsample-efficiency}
Assuming \Cref{condition:fast-termination} holds, the expected total number of calls to $\resolve(t)$  within $\lsample(\Lambda)$ is $O(|\Lambda|)$.
\end{lemma}

\begin{proof}
The introduction of the map $M$ in \Cref{Line:resolve-finite-memoization} of $\resolve(t)$ is for memoization and only reduces the number of recursive calls. As a result, the expected running time of $\lsample(\Lambda)$ can be upper-bounded by the sum of the expected running times of $\resolve(\pred_0(v))$ for each $v \in \Lambda$. It remains to show that the expected running time of $\resolve(\pred_0(v))$ is $O(1)$ for each $v \in V$.

% Note that the introduction of the mapping $M$ in \Cref{Line:resolve-finite-memoization} of $\resolve(t)$ is for memoization and only reduces the number of recursive calls; hence the expected running time of $\lsample(\Lambda)$ can be upper bounded by the sum over the expected running time of $\resolve(\pred_0(v))$ for each $v\in \Lambda$. It remains to show that the expected running time of $\resolve(\pred_0(v))$ is $O(1)$ for each $v\in V$.
  
  % Again, by that the introduction of the mapping $M$ only reduces the number of recursive calls, the behavior of $\resolve(\pred_0(v))$ can be stochastically dominated by the following multitype Galton-Watson branching process:
  % \begin{itemize}
  %     \item Start with a root node labelled with $\pred_0(v)$ with depth $0$,
  %     \item for $i=0,1,\ldots$: for all current leaves labelled with some timestamp $t$ of depth $i$:
  %   \begin{itemize}
  %       \item Make an independent run of $\resolve(t)$, and for each timestamp $t'<t$ such that $\resolve(t)$ is directly recursively called, add a new node labeled with $t'$ as a child of $t$.
  %   \end{itemize}
  % \end{itemize}
As the mapping $M$ only reduces the number of recursive calls, the behavior of $\resolve(\pred_0(v))$ can be stochastically dominated by the following multitype Galton-Watson branching process: 
\begin{itemize} 
\item Start with a root node labeled with $\pred_0(v)$ at depth $0$. 
\item For each $i = 0, 1, \ldots$: for all current leaves labeled with some timestamp $t$ at depth $i$: 
\begin{itemize} 
    \item Perform an independent run of $\resolve(t)$, and for each timestamp $t' < t$ such that $\resolve(t)$ is directly recursively called, add a new node labeled with $t'$ as a child of $t$. 
\end{itemize} 
\end{itemize}
By \Cref{condition:fast-termination}, for any timestamp $t \leq 0$, the expected number of offspring of a node labeled $t$ is at most $1 - \delta$. Thus, applying the theory of branching processes, the expected number of nodes generated by this process is at most $\delta^{-1} = O(1)$. Therefore, the expected number of $\resolve(t)$ calls within $\lsample(\Lambda)$ is $O(|\Lambda|)$, completing the proof of the lemma. 
%  By \Cref{condition:fast-termination}, for any timestamp $t\leq 0$, the expected number of offspring of a node with label $t$ is $\leq 1-\delta$. Then, by leveraging the theory of branching processes, the expected number of nodes generated by this process is $\delta^{-1}=O(1)$. Hence, the expected number of $\resolve(t)$ calls within $\lsample(\Lambda)$ is $O(|\Lambda|)$, proving the lemma.
\end{proof}


\subsection{Conditional correctness of the local sampler}\label{sec:appendix-correctness}
We will prove \Cref{lemma:lsample-correctness}, which addresses the conditional correctness of the local sampler (\Cref{Alg:lsample}).
At a high level, the proof follows the same structure of the proof in~\cite{feng2023towards}.

First, we need to establish some basic components.
\begin{lemma}\label{lemma:lsample-correctness-irreducibility}
 Assume that both Conditions \ref{condition:local-correctness} and \ref{condition:immediate-termination} hold for $\eval^{\+O}(v)$, then $\+P(T)$ is irreducible.
\end{lemma}
\begin{proof}
    Recall that in \Cref{condition:immediate-termination}, for any $v\in V$ and $\sigma\in [q]^{N(v)}$, $\+E^{\sigma}_v$ denotes the event that $\eval^{\+O}(v)$ terminates without any calls to $\+O$, assuming that $\+O(u)$ consistently returns $\sigma(u)$ for each $u\in N(v)$. For any $v\in V$, let $c_v\in [q]$ be an arbitrary possible outcome of $\eval^{\+O}(v)$, conditioning on $\+E_v$ happens. Note that such $c_v$ always exists by \Cref{condition:immediate-termination}. Then combining with \Cref{condition:local-correctness}, we have 
\begin{equation}\label{eq:marginal-lower-bound}
   \min\limits_{\sigma \in [q]^{N(v)}}\mu^{\sigma}_v(c_v)>0. 
\end{equation}


Let $\tau\in [q]^V$ be the constant configuration where $\tau(v)=c_v$ for $t=\pred_0(v)$. By \eqref{eq:marginal-lower-bound} and the chain rule, we see that $\mu(\tau)>0$. Also following \eqref{eq:marginal-lower-bound}, any $\sigma\in [q]^V$ such that $\mu(\sigma)>0$ can reach $\tau$ through Glauber moves by changing some $\sigma(u)$ to $\tau(u)$ one at a time. Note that $\+P(T)$ is reversible, therefore, any $\sigma\in [q]^V$ such that $\mu(\sigma)>0$ can also be reached from $\tau$ and hence $\+P(T)$ is irreducible.
\end{proof}



For any finite $T>0$, we introduce the following finite-time version of \Cref{Alg:lsample}, presented as \Cref{Alg:lsample-finite}, which locally resolves the final state $X_0$ of $\+P(T)$. Note that the only difference between Algorithms \ref{Alg:lsample} and \ref{Alg:lsample-finite} is different initialization of the map $M$.

\begin{algorithm}[H]
\caption{$\lsample_T(\Lambda)$} \label{Alg:lsample-finite}
\SetKwInput{KwData}{Global variables}
\KwIn{a $q$-spin system $\+S=(G=(V,E),\bm{\lambda},\bm{A})$, a subset of variables $\Lambda\subseteq V$}
\KwOut{A random value $X \in [q]^{\Lambda}$}
\KwData{a map $M: \mathbb{Z}\to [q]\cup\{\perp\}$}
$X\gets \emptyset, M(t)\gets X_{-T}(v_{i(t)}) \text{ for each } t\leq -T,  M(t)\gets \perp \text{ for each } t> -T$\;
\ForAll{$v\in \Lambda$}{
    $X(v)\gets \resolve(\pred_0(v))$\;
}
\Return $X$\;
\end{algorithm}
 We then have the following lemma.

\begin{lemma}\label{lemma:lsample-correctness-convergence}
     Assume that both Conditions \ref{condition:local-correctness} and \ref{condition:immediate-termination} hold for $\eval^{\+O}(v)$. Then,
     \begin{enumerate}
         \item $\lsample(\Lambda)$ terminates with probability $1$;\label{item:lsample-correctness-convergence-1}
         \item $\lim\limits_{T\to \infty}\DTV{\lsample(\Lambda)}{\lsample_T(\Lambda)}=0$.\label{item:lsample-correctness-convergence-2}
     \end{enumerate}
\end{lemma}

\begin{proof}
We start with proving \Cref{item:lsample-correctness-convergence-1}. If suffices to show the termination of $\resolve(t_0)$ for any $t_0\leq 0$. Recall the event $\+E_v$ in \Cref{condition:immediate-termination}. For each $t\leq 0$, we similarly let $\+E_t$ denote the event that $\eval^{\+O}(v_{i(t)})$ within $\resolve(t)$ terminates without making any calls to $\+O$. We also define the event:
\[
\+B_t: \+E_{t'} \text{ happens for all }t'\in [t-n+1,t].
\]
We claim that if $\+{B}_t$ happens for some $t\leq t_0$,  then no recursive calls to $\resolve(t')$ would be incurred for any $t'\leq t-n$ within $\lsample(\Lambda)$. For the sake of contradiction, assume that a maximum $t^*\leq t-n$ exists such that $\resolve(t^*)$ is called. As $t^*\leq t-n<t_0$ , $\resolve(t^*)$ must be recursively called directly within another instance of $\resolve(t')$ (through $\eval^{\+O}(v_{i(t')})$) such that $t^*<t'$. Note that by \Cref{Alg:resolve}, the fact that $\eval^{\+O}(v_{i(t)})$ only make recursive calls to $\resolve(\pred_t(u))$ for some $u\in N(v_{i(t)})$ and \eqref{eq:definition-pred} we also have $t^*>t'-n$. We then have two cases:
\begin{enumerate}
    \item $t'\leq t-n$, this contradicts the maximality assumption for $t^*$.
    \item Otherwise $t'>t-n$. By $t^*\leq t-n$ and $t'<t^*+n$ we have $t'\in [t-n+1,t]$. 
      Also by the assumption that $\+{B}_t$ happens, we have $\+E_{t'}$ happens, therefore $\resolve(t')$ would have directly terminated without incurring any recursive call. This also leads to a contradiction and thus proves the claim.
\end{enumerate}

Let $p\defeq \min\limits_{t}\Pr{\+E_t}$, then $p>0$ by \Cref{condition:immediate-termination}. Note that by \Cref{condition:immediate-termination}, for any $t\leq t_0$, we have 
\[
\Pr{\+{B}_t}=\prod\limits_{t'=t-n+1}^{t}\Pr{\+E_{t'}}\geq (1-p)^n>0,
\]
where the first equality is by $\+E_{t}$ only depends on the randomness of procedure $\eval$, therefore all $\+E_{t'}$ are independent.

For any $L>0$, let $\+E_L$ be the event that there is a recursive call to $\resolve(t^*)$ where $t^* \le t_0-L n$.
By the claim above,
\begin{align*}
  \Pr{\+E_L} \le \Pr{\bigwedge\limits_{j=0}^{L-1}\tp{\neg \+{B}_{t_0-jn}}} = \prod\limits_{j=0}^{L-1}\Pr{\neg \+{B}_{t_0-jn}}\leq (1-p)^L,
\end{align*}
where the equality is again due to independence of $(\+E_t)_{t\leq t_0}$.
Consequently, with probability $1$, there is only a finite number of recursive calls, meaning that $\lsample(\Lambda)$ terminates with probability $1$. This establishes \Cref{item:lsample-correctness-convergence-1}. 


For any $t\le 0$, since $\resolve(t)$ terminates with probability $1$,
its output distribution is well-defined. Therefore, the output distribution of $\lsample(\Lambda)$ is well-defined. For any $\eps>0$, we choose a sufficiently large $L$ such that $(1-p)^L\le \eps$.
For any $T\ge Ln-t_0$, we couple $\lsample(\Lambda)$ with $\lsample_T(\Lambda)$ by pre-sampling all random variables used in $\eval^{\+O}(v_{i(t)})$ within $\resolve(t)$ for each $t\leq 0$. Here by \Cref{condition:local-correctness}, the coupling fails if and only if $\resolve(t')$ is recursively called within $\lsample(\Lambda)$ for some $t'\leq -T$, that is, $\+E_{L}$ happens. Therefore, by the coupling lemma, we have
\[
\DTV{\lsample(\Lambda)}{\lsample_T(\Lambda)}\leq \Pr{\+E_{L}}\leq (1-p)^L\le \eps,
\]
which proves \Cref{item:lsample-correctness-convergence-2} as we take $T\to \infty$.
\end{proof}

For any finite $T>0$ and $-T\leq t\leq 0$, we let $X_{T,t}$ be the state of $X_t$ in $\+P(T)$. The following lemma shows $\lsample_T$ indeed simulates $\+P(T)$.


\begin{lemma}\label{lemma:lsample-correctness-distribution}
  Assume that Conditions \ref{condition:local-correctness} and \ref{condition:immediate-termination} hold for $\eval^{\+O}(v)$. Then for any $\Lambda\subseteq V$, the value returned by $\lsample_T(\Lambda)$ is identically distributed as $X_{T,0}(\Lambda)$.
\end{lemma}

\begin{proof}
We maximally couple the value returned by each $\resolve(t)$ and $X_t(v_{i(t)})$ in $\+P(T)$ for each $t\leq T$ 
and claim that in this case, the value returned by each $\resolve(t)$ is exactly the same as $X_t(v_{i(t)})$ in $\+P(T)$; hence the lemma holds by the definition of $\lsample_T$ and \eqref{eq:definition-pred}.

We prove the claim by induction from time $-T$ to $0$. For each $-T\leq t<0$, let $v=v_{i(t)}$ and consider the value returned by $\resolve(\pred_t(u))$ for each $u\in N(v)$:
\begin{itemize}
    \item If $\pred_t(u)<-T$, then by \eqref{eq:definition-pred}, the value of $u$ is not updated up to time $t$ in $\+P(T)$, hence $X_t(u)=X_{-T}(u)$ and the value returned by $\resolve(\pred_t(u))$ is $X_{-T}(u)=X_t(u)$ by the initialization of $M$ in \Cref{Alg:lsample-finite}.
    \item Otherwise, $-T\leq \pred_t(u)<t$ by \eqref{eq:definition-pred} and $u\in N(v)$, so the value returned by $\resolve(\pred_t(u))$ is $X_{\pred_t(u)}=X_t(u)$ by the induction hypothesis. Hence by \Cref{Line:resolve-sample} of \Cref{Alg:resolve} and \Cref{condition:local-correctness}, both the distribution of $\resolve(t)$ and $X_t(v_{i(t)})$ is $\mu^{X_{t-1}(N(v))}(v)$ and hence can be perfectly coupled.
\end{itemize}
Hence, the claim holds, and the lemma is proved.
\end{proof}

We are now ready to prove \Cref{lemma:lsample-correctness}.

\begin{proof}[Proof of \Cref{lemma:lsample-correctness}]
By \Cref{item:lsample-correctness-convergence-1} of \Cref{lemma:lsample-correctness-convergence}, we have $\lsample(\Lambda)$ terminates with probability $1$ and its output distribution is well-defined. It remains to prove that the output distribution of $\lsample(\Lambda)$ is exactly $\mu_{\Lambda}$.

Note that the by \Cref{lemma:lsample-correctness-irreducibility}, we have $\+P(T)$ is irreducible and \Cref{thm-convergence} implies that
\begin{equation}\label{eq:dtv-convergence-irreducibility}
\lim_{T\to \infty}\DTV{\mu_{\Lambda}}{X_{T,0}(\Lambda)}=0,
\end{equation}

 Moreover, for any $T\geq 0$,

\begin{equation}\label{eq:triangle-inequality}
   \DTV{\lsample(\Lambda)}{\mu_{\Lambda}}\leq  \DTV{\lsample_T(\Lambda)}{\mu_{\Lambda}}+\DTV{\lsample_T(\Lambda)}{\lsample(\Lambda)},
\end{equation}
following from triangle inequality.

Overall, we have
    \begin{align*}
  &\DTV{\mu_{\Lambda}}{\lsample(\Lambda)} \\
  \le~&~\limsup_{T\rightarrow\infty}\DTV{\mu_{\Lambda}}{\lsample_T(\Lambda)} \tag{by \eqref{eq:triangle-inequality}}\\
  +~&~ \limsup_{T\rightarrow\infty}\DTV{\lsample_T(\Lambda)}{\lsample(\Lambda)}  \\
  =~&~\limsup_{T\rightarrow\infty}\DTV{\mu_{\Lambda}}{\lsample_T(\Lambda)} \tag{by \Cref{lemma:lsample-correctness-convergence}} \\
  \le~&~\limsup_{T\rightarrow\infty}\DTV{\mu_{\Lambda}}{X_{T,0}(\Lambda)} + \limsup_{T\rightarrow\infty}\DTV{X_{T,0}(\Lambda)}{\lsample_T(\Lambda)}   \tag{by \eqref{eq:triangle-inequality}}\\
   =~&~\limsup_{T\rightarrow\infty}\DTV{\mu_{\Lambda}}{X_{T,0}(\Lambda)}   \tag{by \Cref{lemma:lsample-correctness-distribution}}\\
   =~&~0, \tag{by \eqref{eq:dtv-convergence-irreducibility}}
\end{align*}
and the theorem follows.
\end{proof}


\begin{comment}
    
Nevertheless, we provide a sufficient condition that would also imply the efficiency of \Cref{lemma:lsample-correctness}.
\begin{condition}[Locally bounded recursive calls]\label{condition:locally-bounded-recursive-calls}
For the given locally-defined grand coupling $f$, $\eval(t)$ satisfies the following property for each $t\leq 0,r\in [0,1]$ and $v=v_{i(t)}$:
\begin{itemize}
    \item For each $u\in N(v)$, let $p_{t,v}$ be the maximum probability that $\resolve(\pred_t(u))$ is recursively called among all possible $X_{t-1}(N(v))$, then
    \[
    \sum\limits_{u\in N(v)} p_{t,v}\leq 1-\delta
    \]
    for some $\delta>0$.
\end{itemize}
\end{condition}


\begin{lemma}\label{lemma:lsample-efficiency}
Assume all Conditions \ref{condition:local-correctness}, \ref{condition:immediate-termination} and \ref{condition:locally-bounded-recursive-calls} hold, then the expected number of $\resolve(t)$ calls within $\lsample(\Lambda)$ is $O(|\Lambda|)$.
\end{lemma}
\begin{proof}
  Note that the introduction of the map $M$ in \Cref{Line:resolve-finite-memoization} of $\resolve(t)$ is for memoization and only reduces the number of recursive calls, hence the behavior of the recursive calls of $\resolve(t)$ can be stochastically dominated by a Galton-Watson process with expected offspring $\leq 1-\delta$ for each node, hence the lemma is proved.
\end{proof}



Here, the heart of designing such backward deduction algorithms is how to design $\sample(t)$ to achieve both Conditions \ref{condition:sample} and \ref{condition:bound-expected-number} simultaneously. In ~\cite{feng2023towards}, the idea is to use marginal lower bounds for early termination. However, for two-spin systems, relying on marginal lower bounds to satisfy \Cref{condition:bound-expected-number} leads to regimes that are nowhere near critical thresholds. As a comparison, we utilize the fact that marginal probabilities in two-spin systems are well-formed and apply the idea of efficient \emph{Bernoulli Factory Algorithms} to take advantage of this and hence reduce the number of recursive calls. Such an idea of applying Bernoulli Factory Algorithms for sampling from marginal distributions was also used previously by ~\cite{anari2022entropic} to improve the time needed for simulating the Glauber dynamics for the Ising model. 
\end{comment}

\begin{comment}

A sufficient condition that guarantees the termination of $\resolve(t)$ is given as follows.

\begin{condition}\label{condition:bound-expected-number}
For each $t<0$, $\sample(t)$ satisfies the following property:
\begin{itemize}
    \item Let $T_t$ denote the number of recursive calls of $\resolve$ within $\sample(t)$, then $\E{T_t}<1$.
\end{itemize}
\end{condition}




\begin{lemma}\label{lemma:backward-deduction-correctness}
Assume Conditions \ref{condition:sample} and \ref{condition:bound-expected-number} hold, then for any $t<0$, $\resolve(t;M,R)$ terminates with probability $1$. Moreover, upon termination, $\resolve(t;M,R)$ returns a sample $X\in [q]$ distributed exactly as $\mu_v$ where $v=v_{i(t)}$.
\end{lemma}
\begin{proof}
    Let $T=\max\limits_{-\infty<t\leq 0}\{\E{T_t}\}$, then by \Cref{condition:bound-expected-number} we have $T<1$.  Note that the introduction of the map $M$ in \Cref{Line:resolve-finite-memoization} of $\resolve(t)$ is for memoization and only reduces the number of recursive calls, hence the behavior of the recursive calls of $\resolve(t)$ can be stochastically dominated by a Galton-Watson process with expected offspring $<1$ for each node, and $\resolve(t)$ terminates almost surely by leveraging the theory of branching process. Combining with \Cref{lemma:backward-deduction-correctness-with-termination}, the lemma is proved.
\end{proof}

Here, the heart of designing such backward deduction algorithms is how to design $\sample(t)$ to achieve both Conditions \ref{condition:sample} and \ref{condition:bound-expected-number} simultaneously. In ~\cite{feng2023towards}, the idea is to use marginal lower bounds for early termination. However, for two-spin systems, relying on marginal lower bounds to satisfy \Cref{condition:bound-expected-number} leads to regimes that are nowhere near critical thresholds. As a comparison, we utilize the fact that marginal probabilities in two-spin systems are well-formed and apply the idea of efficient \emph{Bernoulli Factory Algorithms} to take advantage of this and hence reduce the number of recursive calls. Such an idea of applying Bernoulli Factory Algorithms for sampling from marginal distributions was also used previously by ~\cite{anari2022entropic} to improve the time needed for simulating the Glauber dynamics for the Ising model. 
\end{comment}
\input{algorithm-new}
\input{dynamic-sampling}

\section{Conclusions and Open Problems}\label{sec:conclusions}
In this paper, we design a new local sampling algorithm that goes beyond the use of the local uniformity property by generalizing and refining the framework of backward deduction of Markov chains, i.e., the ``coupling towards the past'' (CTTP) method. 
Our local sampling algorithm, in particular, is efficient in the near-critical regime of canonical spin systems such as the Ising model, providing an exponential improvement over previous works. 
Additionally, we show how our local algorithm can run in sublinear time on unbounded-degree graphs with precomputation. 
We also introduce new dynamic sampling algorithms based on our local sampler, improving the regimes of prior methods.


We leave the following open problems and directions for future work: 
\begin{itemize}
    \item While our local sampler performs well in near-critical regimes for spin systems using backward deduction of Glauber dynamics, it has been shown that forward simulation of Glauber dynamics mixes rapidly for the Ising model up to the uniqueness threshold. Can we improve the analysis of our current algorithm, design new local samplers that are efficient up to this critical threshold, or prove a lower bound showing that this is intractable for local sampling algorithms?
    %\item For general spin systems with unbounded-degree graphs, our design of sublinear-time local samplers required an additional $O(\Delta)$ factor with respect to the size of input as a cost of precomputing. Is it possible to precompute within time near-linear with respect to the input size and still obtain sublinear-time local samplers? 
    \item Local samplers, as introduced in~\cite{AJ22,feng2023towards}, have found a wide range of applications, as discussed in the introduction. Our work further shows that local samplers can also imply efficient dynamic sampling algorithms. We hope to see more applications of local samplers, particularly in the design of distributed and parallel algorithms. 
\end{itemize}

\newpage

\bibliographystyle{alpha}
\bibliography{references} 

\newpage
\appendix









\begin{comment}
\section{Proofs to corollaries}\label{sec:appendix-corollaries}

In this section, we prove three corollaries of the main theorem \Cref{theorem:local-sampler} in the introduction.


\begin{proof}[Proof of \Cref{cor:hardcore}]
    For hardcore models, we directly have
    \[
    b_{v,u,1}=0, \quad b_{v,u,2}=1;
    \]
    Hence for each $v\in V$, we have
    \begin{align*}
         &\frac{\sum\limits_{c\in[q]}\left(\lambda_v(c)\sum\limits_{e=(u,v)\in E}(1-b_{v,u,c})\right)}{\sum\limits_{c\in[q]}\left(\lambda_v(c)\prod\limits_{e=(u,v)\in E}A_e(\sigma(u),c)\right)}\\
         =&\frac{\lambda_v(1)\cdot |N(v)|}{\sum\limits_{c\in[q]}\left(\lambda_v(c)\prod\limits_{e=(u,v)\in E}A_e(\sigma(u),c)\right)} \\
         \leq&  \frac{\lambda_v(1)\cdot |N(v)|}{\lambda_v(2)\prod\limits_{e=(u,v)\in E}A_e(\sigma(u),2)} \\
          =&  \frac{\lambda_v(1)\cdot |N(v)|}{\lambda_v(2)} \\
        = &\lambda\cdot |N(v)|\\
        \leq & \lambda\cdot \Delta.
    \end{align*}
   Therefore when $\lambda<\frac{1}{\Delta}$, the condition in \Cref{cor:hardcore} is implied by \Cref{cond:main}. Then \Cref{cor:hardcore} is proved through \Cref{theorem:local-sampler}.
 \end{proof}

We then only prove \Cref{cor:potts}, and the proof of \Cref{cor:ising} follows as a special case when $q=2$.
\begin{proof}[Proof of \Cref{cor:potts}]
    We first assume that $\beta<1$. Note that for each $v\in V$ and $u\in N(v)$, we then have
    \[
        \forall c\in [q],\quad  b_{v,u,c}=\beta;
    \]
    Also, for each $v\in V$ and $\sigma\in [q]^{N(v)}$, let $n_1(\sigma)$ be the number of spins set to $1$ in $\sigma$, then we have 
    \[
    \forall c\in [q],\quad   \prod\limits_{e=(u,v)\in E}A_e(\sigma(u),c)\leq \beta^{|N(v)|}.
    \]
    Hence for each $v\in V$, we have
    \begin{align*}
         &\frac{\sum\limits_{c\in[q]}\left(\lambda_v(c)\sum\limits_{e=(u,v)\in E}(1-b_{v,u,c})\right)}{\sum\limits_{c\in[q]}\left(\lambda_v(c)\prod\limits_{e=(u,v)\in E}A_e(\sigma(u),c)\right)}\\
         \leq &\frac{\sum\limits_{c\in [q]}\left(\lambda_v(c)(1-\beta)\cdot |N(v)|\right)}{\sum\limits_{c\in[q]}\left(\lambda_v(c)\cdot \beta^{\Delta}\right)} \\
         \leq &\frac{(1-\beta)\cdot \Delta}{\beta^{\Delta}}.
    \end{align*}
    It is then easy to verify that when $1-\frac{1}{2\Delta}<\beta\leq 1$, we have $\frac{(1-\beta)\cdot \Delta}{\beta^{\Delta}}<1$. Also, the statements hold similarly for $1\leq \beta\leq 1-\frac{1}{2\Delta}$ after normalizing the interaction matrix, therefore the condition in \Cref{cor:ising} is implied by \Cref{cond:main}. Then \Cref{cor:ising} is proved through \Cref{theorem:local-sampler}.
 \end{proof}
\end{comment}





\clearpage


\end{document}
