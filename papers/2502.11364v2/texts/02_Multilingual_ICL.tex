\section{Background: Multilingual ICL}
\label{sec:multiling_icl}
In this section, we review the basic approaches of ICL with instruction-tuned LLMs (\cref{sec:multiling_icl:icl_instruction_tuning}) and introduce the multilingual prompting modes (\cref{sec:multiling_icl:modes}) that we evaluate in this work.
\subsection{ICL for Instruction-Tuned LLM} \label{sec:multiling_icl:icl_instruction_tuning}
Instruction-tuned LLMs \citep{instructGPT,cross_task_generalization, super_natural_instructions, ft_lm_are_zs_learners} generally possess the capability to follow task instructions (i.e., \textit{system prompt}), which are usually coupled with ICL to fully elicit their capability \interalia{cot}.
Formally, denote a training set by $\mathcal{D}_\text{train}=\left\{\left(q_i, a_i\right)\right\}_{i=1}^M$ and a test set $\mathcal{D}_\text{test}=\left\{\left(q_j, a_j\right)\right\}_{j=1}^{N}$ in the same domain, where $q_i$ is a task question (as model \textit{input}).
An ICL prompt for a test question $q_{\text{test}} \in \mathcal{D}_\text{test}$ has three core components:
(1) a system prompt $I_\text{sys}$ that describes the task and specifies the expected output format,
(2) $K$ sample input-output pairs ($K$-shot) from the training set $\left\{\left(q_k, a_k\right)\right\}_{k=1}^K \sim \mathcal{D}_\text{train}$ that provide in-context demonstrations, and
(3) a verbalizer $V$ mapping each ground truth label $a_i$ to a textual representation, which may also include reasoning steps \citep[i.e., chains of thoughts, or CoT in short;][]{cot}.
In summary, an ICL prompt for $q_{\text{test}}$ can be written as:
\begin{align}
  \text{prompt}_{q_\text{test}} & = I_\text{sys} \circ q_1 \circ V\left(a_1\right) \circ q_2 \circ V\left(a_2\right) \nonumber \\
                                & \quad \circ \cdots \circ q_K \circ V\left(a_K\right) \circ q_{\text{test}},
  \label{eq:icl_construction}
\end{align}
where $\circ$ is the string concatenation operator with a special end-of-turn $\left(\EOT\right)$ token as the delimiter.
The LLM with parameters $\vect{\theta}$, denoted as $p_{\vect{\theta}}$, then generates the response $\hat{a}_\text{test}$ given $\text{prompt}_{q_\text{test}}$:
$\hat{a}_\text{test} \sim p_{\vect{\theta}}\left(\text{prompt}_{q_\text{test}}\right)$.


\subsection{Multilingual Prompting Modes} \label{sec:multiling_icl:modes}
We extend our notations as follows to adapt to the multilingual settings.
A training set with $L$ languages is denoted by $\mathcal{D}_\text{train} = \left\{\mathcal{D}_\text{train}^{\text{lang}_1}, \ldots, \mathcal{D}_\text{train}^{\text{lang}_L} \right\}$, % e.g.,
% French split $\mathcal{D}_\text{train}^{\text{fr}}=\left\{\left(q_i^{\text{fr}}, a_i\right)\right\}_{i=1}^M$.
where the split $\mathcal{D}_\text{train}^{\text{lang}_\ell}$ consists of $M$ examples for any $\ell\in \{1, \ldots, L\}$.
The same applies to the test dataset $\mathcal{D}_\text{test}$, with each language-specific split consisting of $N$ examples.
Without further specification, we assume that the training examples at the same index are semantically equivalent across languages.

The ground-truth labels $a_i$ in quantitative LLM benchmarks \interalia{xcopa,gsm8k} are typically language-agnostic (such as numbers) or represented in a single word (such as Yes/No).
In such cases, the verbalizer is an identity.
For answers requiring reasoning steps, $V(a_i)$ is CoT in English, as there has been strong evidence that MLLMs perform better when generating English \interalia{mgsm, cross_lingual_prompting, not_all_language_are_created_equal}.
For the same reasons, $I_\text{sys}$ is always presented in English as well.

Following \citet{mega} and \citet{mgsm}, we evaluate MLLMs via several different prompting strategies (\textit{ICL modes}) in this work:

\begin{figure}[t]
  \centering
  \includegraphics[width=\linewidth]{figures/icl_modes_no_flag.pdf}
  \caption{Illustration of ICL modes by \cref{eq:icl_construction}. Assume $K=3$ and $M=10$. For the second datapoint of the test set (regardless of its language split, e.g., $q_\text{test}$ could be in Thai, Bengali, etc.), we first randomly generate $K=3$ indices from $\left\{1, \cdots, 10\right\}$, say $\left\{8,3,5\right\}$.  Next, we determine the languages of the $K=3$ demonstrations. For modes (a), (b), and (d), the language is uniformly specified. For mode (c), we randomly select $K=3$ languages, say $\left\{\text{en, de, fr}\right\}$. Then $\left\{(8, \text{en}), (3, \text{de}), (5, \text{fr})\right\}$ determines each demonstration.
  }
  \vspace{-5pt}
  \label{fig:icl_modes}
\end{figure}

\vspace{2pt}\noindent\textbf{The \english mode.} The $K$ demonstrations are always in English (\cref{fig:pull:english,fig:icl_modes}a).

\vspace{2pt}\noindent\textbf{The \monolingual mode(s).} The $K$ demonstrations are always presented in a single non-English high-resource language such as Chinese (\cref{fig:icl_modes}b).

\vspace{2pt}\noindent\textbf{The \multilingual mode.} \label{sec:icl:multilingual_mode}
From a predefined list of high-resources languages $\mathcal{L}_H$, $K$ languages are randomly selected, which, together with the sampled $K$ indices, determine the contents and languages of the $K$ demonstrations (\cref{fig:pull:multilingual,fig:icl_modes}c).

\vspace{2pt}\noindent\textbf{The \native mode.} The $K$ demonstrations are in the same language as the test question (\cref{fig:icl_modes}d).
