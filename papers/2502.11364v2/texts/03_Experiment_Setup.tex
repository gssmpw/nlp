\section{Experiment Setups}
\label{sec:setup}
% \subsection{Model}
\noindent\textbf{Models}.
We evaluate state-of-the-art instruction-tuned LLMs with about 8 billion parameters, which have officially claimed multilingual capabilities in model release: \textsf{Llama3-8B-Instruct}, \textsf{Llama3.1-8B-Instruct} \citep{llama3}, \textsf{Qwen2-7B-Instruct} \citep{qwen2}, \textsf{Qwen2.5-7B-Instruct} \citep{qwen2.5}, \textsf{Mistral-NeMo-12B-Instruct} \citep{mistral} and \textsf{Aya-Expanse-8B} \citep{aya-expanse}.
For additional references, we evaluate OpenAI closed-sourced commercial models, including \textsf{GPT3.5-turbo} \citep{gpt3.5} and \textsf{GPT4o-mini} \citep{gpt4o-mini}.
Detailed model cards can be found in \cref{app:model}.


\input{tables/dataset}

\vspace{3pt}
\noindent\textbf{Datasets.}
We evaluate the models using multilingual benchmarks from three distinct domains:
(1) \mgsm \cite{mgsm}, a benchmark of 250 grade-school math problems sampled from the English GSM8K \citep{gsm8k} and translated into 10 additional languages by expert native speakers.
(2) XCOPA \cite{xcopa}, a commonsense reasoning benchmark that extends the COPA dataset \citep{copa} to $11$ additional languages.\footnote{In this work, we merge the English COPA and XCOPA datasets, which we still refer to as \xcopa.}  (3) \xlwic \cite{xlwic}, a cross-lingual word-in-context understanding dataset spanning $13$ languages, where models are expected to tell whether a polysemous word retains the same meaning in two contexts.

\mgsm and \xcopa are \textit{parallel} where each corresponding datapoint across different language splits contains semantically equivalent content, allowing us to minimize semantic confounders in our experimental design.  \xlwic is language-specific and translation-variant thus naturally non-parallel. Dataset properties and examples are in \cref{tab:dataset,tab:dataset_additional_info}. Details of data curation are in \cref{app:dataset}.


\vspace{2pt}\noindent\textbf{Languages.}
Languages with large-scale digitized data resources on the web are known as \textit{high-resource languages} \citep[HRLs;][]{bender_rule}, which are exemplified by English, Spanish, and Chinese, among others.
In contrast, \textit{low-resource languages} (LRL) have scarce accessible data \cite{nllb}.
However, a universal standard for dichotomizing languages as either high- or low-resource has not been set \citep{bender_rule,state_and_fate_of_linguistic_diversity,survey_low_resource_nlp}.
Moreover, none of the models we evaluate has disclosed the language distributions in their training corpora.
As a workaround, we define our preset HRL list as the union of the 20 most frequent languages in \textsf{Llama2} \cite{llama2} and \textsf{PaLM} \cite{palm}, and classify languages out of the HRL list as LRLs.
Details of the preset HRL list can be found in \cref{app:lang}.


\vspace{2pt}\noindent\textbf{Prompts.} \label{sec:setup:prompts}
Following \citet{mgsm}, we use $K=6$ examples for demonstration for any test question.
For each multilingual dataset (\cref{tab:dataset}), we first sample $N$ index lists of length $K=6$ all at once, where the index range is $\left\{1, 2, \cdots, M\right\}.$
We allocate the $i$-th index list to the set of test questions $q_{\text{test}_i} = \left\{q_{\text{test}_i}^{\text{lang}_1}, q_{\text{test}_i}^{\text{lang}_2}, \cdots, q_{\text{test}_i}^{\text{lang}_L} \right\}$ for the same index $i$ across all $L$ language splits. The training-set index list, together with the specified languages,\footnote{
    For the \multilingual mode, we apply the same sampling procedure to generate $N$ HRL code lists of length $K=6$, drawing from the available HRLs in the dataset.
} jointly determines the content and language of the demonstration for each testing example (\cref{fig:icl_modes}).
This approach both ensures linguistic diversity for multilingual prompting and, whenever applicable, mitigates confounding factors that come with semantic inconsistency across examples.
All  \textit{interface} words (see \cref{tab:dataset}) are in the same language as the examples rather than in English.


\vspace{2pt}\noindent\textbf{Inference and metrics.}
Throughout this work, we use greedy decoding for inference, selecting the token with the highest probability at each step.
For \mgsm in need of CoT, we set the maximum token length to 500; for \xcopa and \xlwic, we set it to 10, as we expect the answers to be short.
We use exact match accuracy as our evaluation metric: for \mgsm, we extract the last numeral in the response. For \xcopa and \xlwic, we extract the label (\textit{expected output}) in the response (\cref{tab:dataset}).
