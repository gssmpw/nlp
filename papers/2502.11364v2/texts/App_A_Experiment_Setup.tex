\section{Experiment Setup} \label{app:setup}
\subsection{Model} \label{app:model}
\input{tables/model_card}
All model checkpoints we use and their properties are listed in \cref{tab:model_card}.


\subsection{Dataset} \label{app:dataset}
\input{tables/appendix/dataset_addtional_info}
After preprocessing, datapoints for each language split are stored in a single JSON file. \cref{tab:dataset_additional_info} summarizes the supported languages for each dataset and the sources from which they are obtained.


\paragraph{\mgsm} The original dataset consists of parallel datapoints across all language splits and training/test splits of the same size. Example datapoint and the Chat Template for few-shot demonstrations can be found in \cref{fig:template:mgsm}.



\paragraph{\xcopa} The $100$ datapoints in the training split of XCOPA are parallel to the last 100 datapoints in the English COPA development split. Therefore, we exclude the first $400$ datapoints from the development split of English COPA. The test splits of both datasets are parallel and contain the same number of datapoints. We then merge both into our \xcopa dataset. In \xcopa, there are two types of questions: ``cause'' and ``effect'', each corresponding to a distinct template, as shown in \cref{fig:template:xcopa:cause,fig:template:xcopa:effect}.


\begin{figure}[!ht]
    \centering
    \includegraphics[width=\linewidth]{figures/template_mgsm.pdf}
    \caption{An example of an English datapoint from \mgsm training set. When calling Chat Template API, \user role message is the ``question'' value, while \assistant role message is the ``answer'' value. Note that in the test set, the answer is null without exemplar CoT response. The correct numerical answer is stored in ``answer\_number''.}
    \label{fig:template:mgsm}
\end{figure}

\begin{figure}[!t]
    \centering
    \begin{subfigure}[b]{\columnwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/template_xcopa_cause.pdf}
        \captionsetup{skip=1pt}
        \caption{An example of ``cause'' datapoint.}
        \label{fig:template:xcopa:cause}
    \end{subfigure}

    \begin{subfigure}[b]{\columnwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/template_xcopa_effect.pdf}
        \captionsetup{skip=1pt}
        \caption{An example of ``effect'' datapoint.}
        \label{fig:template:xcopa:effect}
    \end{subfigure}

    % \captionsetup{skip=1pt}
    \caption{Examples of English datapoints from \xcopa training set. First, based on whether ``question'' is  ``cause'' or ``effect'', we fill the ``premise'', ``choice1'', and ``choice2'' values into one of the two predefined templates. The template's language is changeable as per the language split of the datapoint. Then we call the Chat Template API, \user role message is the filled template, while \assistant role message is the ``label'' value.}
    \label{fig:template:xcopa}
\end{figure}




\paragraph{\xlwic} This benchmark is designed to determine whether a specific word in a given language has the same meaning in two different sentences. As a result, the dataset is inherently non-parallel. Among all language splits, Estonian (et) contains the fewest datapoints, with $98$ in the training split and $390$ in the test split. For all other languages, we randomly subsample to match the size of the Estonian split to satisfy the demonstration sampling requirements outlined in \cref{sec:icl:multilingual_mode}. To leverage the attention mechanism of transformers \cite{transformer}, we add asterisks around the target word in both sentences to indicate that the LLM needs to disambiguate the meaning of that specific word. An example datapoint is shown in \cref{fig:template:xlwic}.

\begin{figure}[!t]
    \centering
    \includegraphics[width=\linewidth]{figures/template_xlwic.pdf}
    \caption{An example of English datapoint from \xlwic training set. We first fill the ``example\_1'', ``example\_2'', and ``target\_word'' values into the predefined templates. Asterisks * are surrounded around ``target\_word'' to draw the LLM's attention. The template's language is changeable as per the language split of the datapoint. Then we call the Chat Template API, \user role message is the filled template, while \assistant role message is ``Yes'' or ``No'' (``label'' is $1$ or $0$).}
    \label{fig:template:xlwic}
\end{figure}



\paragraph{\combined} \label{app:dataset:combined} Identifying specific neurons depends on the nature of the input corpus. Since language is inherently conjugate with the task, and our focus is on language-specific neurons rather than task-specific neurons, it is necessary to input all three datasets into the LLM to eliminate the confounding factor of the task domain. To balance the three datasets, we subsample their test splits to only $250$ datapoints each. To balance the number of language splits across datasets, we excluded two HRL splits, Korean (ko) and Dutch (nl), from the \xlwic dataset. This ensures that all three (sub-)datasets have $11$ languages for combination. Additionally, for \mgsm, we limit the answers to only include the final numeric result without the CoT reasoning. This approach ensures that the total number of datapoints and the overall token count are roughly the same across the original three datasets.


\paragraph{\flores} \label{app:dataset:flores} This machine translation benchmark comprises $3,001$ sentences extracted from English Wikipedia, spanning diverse topics and domains. These sentences were translated into $101$ languages by professional translators via a carefully controlled process. The Wikipedia domain is largely unrelated to the domains of the three datasets we evaluate (math, commonsense reasoning, and word disambiguation). Therefore, we choose \flores as our source pool of irrelevant sentences. Note that in this benchmark, each datapoint carries the same semantic meaning across all $101$ language splits. We do not want irrelevant sentences to affect the LLM's understanding of the original task excessively, thus we select sentences with word counts between $10$ and $15$ in the English split, introducing limited noise. \cis are drawn form the filtered \flores dataset. An example is provided in \cref{fig:flores}.

\begin{figure}[!t]
    \centering
    \includegraphics[width=\linewidth]{figures/flores_all_high_lang.pdf}
    \captionsetup{skip=1pt}
    \caption{A datapoint example from \flores of semantic-equivalent context-irrelevant sentences in all high resource languages we study in this work.}
    \label{fig:flores}
\end{figure}




\subsection{Language} \label{app:lang}
\subsubsection{High-Resource Language List} \label{app:lang:hrl_list}
\input{tables/top_20_lang}
To the best of our knowledge, we find two multilingual LLMs—\llamaTwo \cite{llama2} and \palm \cite{palm}—that publicly report the language distribution used during pretraining. The top 20 languages and their percentages for each model are listed in \cref{tab:llama2_top_20_lang} and \cref{tab:palm_top_20_lang}, respectively. We take the union of these languages to form what we consider a \textit{high-resource language list} (in terms of the richness in the LLM pretraining dataset), including the following $24$ languages:
\begin{equation}
    \begin{aligned}
        \mathcal{L}_{H} = & \{ \text{ar, ca, cs, da, de, en, es, fi, fr, id, it, ja, } \\
                          & \text{ko, nl, no, pl, pt, ru, sr, sv, tr, uk, vi, zh} \}.
    \end{aligned}
    \label{eq:high_lang_list}
\end{equation}

The high overlap between the top languages of the two LLMs further supports the rationale for applying this list to other LLMs.


\subsubsection{Languages We Evaluate}
\cref{tab:lang25} presents the union of languages supported by the three datasets we evaluated (\cref{sec:setup}, datasets). Based on whether a language appears in the high-resource language list, we categorized the $24$ languages into  HRL and LRL groups, with $13$ classified as HRL and $11$ as LRL. Among them, only English and Chinese are present in all three datasets.

It is worth highlighting that the $11$ LRLs span $7$ distinct writing systems and $6$ language families. This diversity suggests that when tokenizing inputs in these LRLs, they are unlikely to share common tokens with HRLs ($9$ out of $13$ use the Latin script). Consequently, this limits the MLLMs' ability to leverage shared subwords or similar syntax structures for cross-lingual transfer across LRLs.
\input{tables/language}
