\section{Conclusion and Discussion}
This work systematically analyzes multiple ICL strategies for MLLMs, and confirms that the presence of multiple languages is an effective strategy across multiple MLLMs.
This improvement is partially due to the inclusion of non-English languages in the prompting, and partially due to the in-topic demonstrations in non-English languages, which together strengthen the models' cross-lingual transfer capabilities, particularly the capability to process LRLs.
Our work echoes with \citet{revisit_primacy_of_english}---who suggest that HRLs other than English excel in the pretraining-finetuning framework---in the in-context learning framework, highlighting the importance of language inclusivity.

We are in agreement with \citet{nllb} and \citet{is_translation_all_you_need} that an ideal language-universal LLM should be equally capable in all languages. 
Beyond this belief, we found that non-English languages may better elicit the potential of MLLMs.
Although these observations remain in using HRLs for LRL processing, our results strongly support the call for greater research investment in enhancing MLLM capabilities for a broader range of languages.


\section*{Limitations}
This paper treats multilingual LLMs as black-box models, drawing the findings and conclusions based solely on their input-output behavior. Hence, we have not interpreted the internal mechanism of how multilingualism could affect MLLM's ``thinking'' process and its manifested performance. We have briefly touched on the impact of demonstrations in different languages on the MLLM performance. However, we do not conduct a thorough empirical analysis to identify which specific linguistic characteristics (e.g., writing systems, grammatical structures, or linguistic relatedness) contribute to the observed performance differences.

% \section*{Reproducibility}

