\section{ICL Mode Evaluation}
\subsection{\multilingual Prompts Surpass \english} \label{sec:icl_mode_eval:multilingual_vs_english}

\input{figures/vanilla_eval}
\input{tables/lrl_vanilla_eval}


\noindent\textbf{Results.}
We first compare the $6$-shot performance with \english, \multilingual and \native ICL modes on $6$ open-sourced MLLMs and $2$ commercial OpenAI models (\cref{fig:vanilla_eval_three_modes}), with \cref{tab:lrl_vanilla_eval} presenting the detailed performance of three selected MLLMs across various LRLs.
Overall, \multilingual mode outperforms \english mode, both for individual LRLs and on average.
In $18$ out of $24$ cases (\cref{fig:vanilla_eval_three_modes}), \multilingual mode achieves higher accuracy than \english one.
This phenomenon is evident even for \textsf{GPT4o-mini}, one of the currently strongest LLMs \cite{chatbot_arena}, and for HRLs as well (see \cref{tab:vanilla_eval:mgsm,tab:vanilla_eval:xcopa,tab:vanilla_eval:xlwic} in \cref{app:expt:vanilla} for HRL accuracies).
Extending the results of \citet{mgsm} that \multilingual mode generally outperforms \english mode for PaLM \citep{palm} and Codex \citep{codex}, our results confirm this trend is a general phenomenon across various MLLMs and datasets.

\vspace{2pt}\noindent\textbf{Outstanding performance of the \native mode and the practical unfeasibility.}
Admittedly, in $22$ out of $24$ comparisons, \native mode outperforms \multilingual mode (\cref{fig:vanilla_eval_three_modes}), which aligns with the machine-learning intuition that in-domain data, in terms of both genre and language, are more promising for model performance \citep{is_translation_all_you_need}. However, domain-specific datasets for LRLs are often difficult to obtain due to the scarcity of native speakers or professional translators \citep{nllb, nllb-200}; therefore, in practice, it is usually challenging to provide high-quality demonstrations in the same language and domain as the test question.
In contrast, annotations make the HRL-\multilingual mode more feasible in many scenarios.


\vspace{2pt}\noindent\textbf{Hypothesis Tests.} \label{sec:icl_mode_eval:hyp_test}
To verify whether the improvement is statistically significant, we conduct McNemar's test \cite{mcnemar}---the null hypothesis means no significant accuracy difference between the baseline (\english) and the compared mode.
Let $b$ denote the number of cases where the baseline is correct while the compared mode is incorrect, and $c$ denote the number of cases where the baseline mode is erroneous while the compared mode is correct.
We calculate the corrected version \cite{corrected_mcnemar} of the McNemar's statistic:
\begin{equation}
    \begin{aligned}
        \chi^2 = \frac{\left( |b-c| -1 \right)^2}{b+c},
    \end{aligned}
    \label{eq:mcnemar}
\end{equation}
which has a chi-squared distribution with one degree of freedom.
Significant $\chi^2$-test results provide strong evidence to reject the null hypothesis of no accuracy improvement.
Our results results (\cref{tab:lrl_vanilla_eval} and \cref{tab:hyp_test:vanilla_eval:mgsm,tab:hyp_test:vanilla_eval:xcopa,tab:hyp_test:vanilla_eval:xlwic} in \cref{app:expt:vanilla}) indicate that both \multilingual and \native modes significantly outperform the \english mode.



\input{figures/mono_vs_multilingual}




\subsection{Ablation Study: Non-English \monolingual Prompts Are Effective} \label{sec:icl_mode_eval:monolingual}

With the success of the \multilingual mode, we investigate whether the improvement comes from the introduction of multiple languages or simply from a single non-English HRL.
% We conduct further experiments to verify whether \monolingual in a single non-English HRL could achieve accuracy improvements over \english.
Specifically, we compare several \monolingual modes including \chinese (for all three datasets), \french, \japanese (both for \mgsm and \xlwic) and \italian (for \xcopa).
We select French and Italian because they are both European languages and thus share considerable subword overlap with English; in contrast, Chinese and Japanese exhibit little subword overlap with Latin-script languages, but there is a substantial overlap between the two due to their shared use of Chinese characters (or \textit{kanji}). This language selection allows us to analyze simultaneously the impact of the writing system (or \textit{subword overlap}) on ICL performance.

\vspace{2pt}\noindent\textbf{Results.} \label{sec:mono_vs_multilingual_results}
All HRL-\monolingual modes outperform \english in a considerable number of comparisons (\cref{fig:mono_vs_multi}).
This finding also holds for HRL evaluations (see \cref{tab:vanilla_eval:mgsm,tab:vanilla_eval:xcopa,tab:vanilla_eval:xlwic} in \cref{app:expt:vanilla} for raw accuracies of each language). Among all \monolingual modes, \chinese performs the best, matching \multilingual mode with 17 out of 21 comparisons when accuracy surpasses that of \english.
\japanese also frequently outperforms \english.
Extending the findings of \citet{revisit_primacy_of_english} that non-English languages are more effective than English in pretraining and fine-tuning based cross-lingual transfer, our results suggest that non-English languages, particularly those with non-Latin scripts, may be more effective under the prompting scheme as well.

However, the \multilingual mode exhibits stronger robustness, outperforming \chinese in 14 out of 21 LRL cases.
The same trend applies to HRLs.
The results of the hypothesis test further confirm the robustness: for both LRL and HRL splits, the \multilingual mode exhibits the highest number of significant results relative to other \monolingual modes (\cref{tab:hyp_test:vanilla_eval:mgsm,tab:hyp_test:vanilla_eval:xcopa,tab:hyp_test:vanilla_eval:xlwic} in \cref{app:expt:vanilla}).
Intuitively, we hypothesize that \multilingual mode functions like an ``average'' of individual HRL-\monolingual modes, making it most robust, and thus it outperforms \english most frequently and achieves the highest overall average accuracy. 

Following \citet{language_specific_neuron_msra}, we further identify ICL-mode-specific neurons and find that the neurons activated by \multilingual overlap most with those activated by \native among other modes. This further explains why \multilingual could achieve performance comparable to \native. See \cref{app:neuron} for details.




\subsection{Ablation Study: Merely Introducing New Language(s) Enhances ICL Performance} \label{sec:icl_mode_eval:cis}

\begin{figure}[!t]
    \centering
    \includegraphics[width=\linewidth]{figures/irr_sent.pdf}
    % 添加居中文本框
    \centering
    {\scriptsize
        \parbox{\linewidth}{%
            \centering
            English ICL Mode $+$ Multilingual Context-Irrelevant Sentences\\
            (\english$+\ $\cisMulti)
        }
    }
    \vspace{-18pt}
    \caption{Prepending multilingual \cis (\cisMulti) $\{s_i^{\text{lang}}\}_{i=1}^K \sim \mathcal{S}^{\text{lang}}$ to demonstrations $\{(q_i^{\text{en}}, a_i)\}_{i=1}^K \sim \mathcal{D}_\text{train}^{\text{en}}$ of \english ICL template illustrated in \cref{fig:icl_modes}a.}
    \label{fig:irr_sent_illustration}
    \vspace{-5pt}
\end{figure}



After showing that including non-English in the prompts can improve ICL performance, a natural follow-up question arises: does the gain come from the mere presence of non-English languages, or the interaction between the target language and the in-topic examples? 
To distinguish these two setups, we prepend a \textit{context-irrelevant sentence} (\cis) $s^{\text{lang}}$ before each ICL demonstration, which is unrelated to the current domain and can be in any language.
Analogous to the ICL modes introduced in \cref{sec:multiling_icl:modes}, CIS resembles the construction of those modes with the same sampling strategy (\cref{sec:setup}).
For example, based on the \english mode, we could prepend a set of multilingual \cis, which augments \cref{fig:icl_modes}a into \cref{fig:irr_sent_illustration}. We denote such setting by ``\english$+\ $\cisMulti''. This naming convention applies to other settings accordingly.


We use sentences from \flores \cite{flores101} as the source of our CIS, which provides parallel Wikipedia sentences in multiple languages, a fairly distant genre to all evaluation datasets.
We filter sentence-parallel sets with English word counts ranging from 10 to 15 as our sampling pool $\mathcal{S}^\text{lang}$, a small range compared to target datasets (\cref{tab:dataset}), to mitigate the risk of introducing too much noise.
Details of \flores can be found at \cref{app:dataset:flores}.

\vspace{2pt}\noindent\textbf{Results and analysis.}
We perform hypothesis testing as in \cref{sec:icl_mode_eval:hyp_test} to compare different \cis settings with \english$+\ $\cisEn (\cref{tab:irr_sent})---\english$+\ $\cisEn exhibit a small drop compared to \english only, indicating that our filtration effectively controls the negative impact of noise to a tolerable level.
We find that introducing a single non-English language generally improves ICL performance in most cases $\left(\frac{44}{54} \approx 82\%\right)$; however, the improvement is only statistically significant in $\frac{20}{44} \approx 45\%$ cases.
This reveals that simply introducing a language can lead to a modest improvement in MLLM performance, but it is more pronounced when in-topic demonstrations in another language (\monolingual modes) are incorporated.
More hypothesis test results for both LRL and HRL splits can be found in \cref{tab:hyp_test:cis:mgsm,tab:hyp_test:cis:xcopa,tab:hyp_test:cis:xlwic} in \cref{app:expt:cis}.


Introducing multiple languages (\cisMulti, \cref{fig:irr_sent_illustration}) is slightly more promising than \cis in a single language, with $\frac{16}{18}\approx 89\%$ of cases showing improvement, of which $\frac{9}{16}\approx 56\%$ are significant.
We further conduct experiments by prepending multilingual \cis to the \multilingual mode (\multilingual$+\ $\cisMulti, \cref{tab:irr_sent}). The performance of \multilingual$+\ $\cisMulti is not significantly lower than that of \english$+\ $\cisMulti; in more than half cases, it significantly outperforms \english$+\ $\cisMulti. This concludes that the significant improvement from \english mode to \multilingual mode is attributed to the incorporation of multiple languages with in-topic demonstrations.


\input{tables/irr_sent}







\subsection{Translation-Based Performance} \label{sec:icl_mode_eval:translation}
Mirroring the translation-training \interalia{xtreme} setup, existing work suggest that translating from LRLs into English and prompting with the translation results may yield better results \interalia{mega}.
While translation is not the main focus of our work, we conduct experiment to compare the performance of translation-based strategies for reference.

\vspace{2pt}\noindent\textbf{Strategies.}
We test two translation strategies for baseline comparison.
(1) \transEn: Translating test questions in other languages in the \english ICL mode (\cref{fig:icl_modes}a) into English.
(2) \transSource: Translating demonstrations in \english ICL mode (\cref{fig:icl_modes}a) into the source language of the current test question, which mirrors the \native mode (\cref{fig:icl_modes}d).
We use the Google Cloud Translation API for translation.\footnote{\url{https://cloud.google.com/translate/}}

\input{tables/translation}
\vspace{2pt}\noindent\textbf{Analysis.}
For LRLs, \transEn sometimes outperform the \native mode, while \transSource performs comparably to the \multilingual mode, but falls short of the \native mode performance (\cref{tab:translation}).
These results resonate with the phenomena that the translation quality of LRL$\rightarrow$En is generally higher than that of the reversed direction \cite{beyond_english_centric_multilingual_mt,flores101,nllb}.

For HRLs, however, \transEn underperforms to \native and, sometimes, even \multilingual (e.g., on \mgsm), suggestting that if a language is sufficiently well-trained, generating responses directly in that language is more effective than translating into English before inference.
These two findings highlight that MLLMs approach the ideal of being equally capable in HRLs \cite{is_translation_all_you_need}, but are still undertrained on LRLs, making translation into English a favored strategy.

In agreement with \citet{roles_of_english}, we would like to note that even if the task performance of translation-based strategies are the best, the ultimate goal of multilingualism is not just about optimizing task-specific performances.
A universal language model should be able to understand and generate text in all languages, instead of relying on specific language(s) as an intermediary.
On the other hand, due to the loss of semantic nuances, grammatical structures and cultural context, translation-based strategies may not be the best choice for tasks heavily reliant on language-specific nuances \cite{is_translation_all_you_need}.
