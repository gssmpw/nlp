\section{Related Work}
\noindent\textbf{Prompt engineering.} Instruction tuning aligns LLMs more closely with human instructions \cite{instructGPT,cross_task_generalization, ft_lm_are_zs_learners,general_language_assistant,super_natural_instructions, self_instruct}. Concurrently, numerous prompting strategies have been developed \cite{prompt_survey} and shown to consistently enhance the performance of instruction-tuned LLMs, such as in-context learning \cite{lm_are_few_shot_learner,rethink_demonstration} and chain-of-thought \cite{cot,lm_zero_shot_reasoner}. These prompting strategies are proven effective in multilingual tasks as well \cite{lm_few_shot_multilingual_learner,few_shot_learning_multilingual_lm, mgsm}.

\vspace{2pt}\noindent\textbf{Multilingual ICL.} 
For languages of templates, demonstrations and sample questions in native languages are conventionally inserted into a predefined English template \cite{few_shot_learning_multilingual_lm,polyglot_prompt,cross_lingual_prompting,not_all_language_are_created_equal,mega,plug}.  
\citet{roles_of_english} critiques this widespread misuse of English as the \textit{interface} language. \citet{cross_lingual_prompting,not_all_language_are_created_equal,plug} guide models to ``think'' and generate CoT in English, regardless of the input language, leading to improved performance for generation tasks compared to ``thinking'' in other language(s). \citet{sensitivity_prompt_design,impact_demonstration_multilingual_icl} highlight that models are sensitive to those templates.
For languages of demonstrations and test questions, \citet{mgsm,mega} conclude that in-language demonstrations outperform English demonstrations. \citet{do_llm_think_better_in_english,is_translation_all_you_need} suggest translating questions from LRLs into English can improve performance.
