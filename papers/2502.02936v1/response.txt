\section{Related Works}
% Done

\noindent\textbf{Multi-view multi-person 3D HPE.} 
% As mentioned in the introduction, multi-view multi-person 3D HPE frameworks typically are built from two stages. 
\red{These frameworks are normally built upon two stages.}
% The first stage detects 2D poses in every view. The second stage associates and selects 2D poses within the same target ID across views. 
We divide them into two categories: optimization-based and learning-based. 
Optimization-based methods mainly focus on the second stage.
Pioneers in this field, such as Li et al., "A 3D Pictorial Structure Model for Multi-View Multi-Person HPE" propose a 3D pictorial structure (3DPS) model for searching \red{corresponding} body joints from a common state space shared by all bodies. 
Zhang et al., "Geometric Energy Function for Multi-View Multi-Person HPE" associate 2D joints across views by optimizing the proposed geometric energy function, which includes the epipolar distance, PAF score Zhang et al., "Cross-View Joint Detection and Pose Estimation with Geometric Constraints" and tracking distance. 
% This optimization is done by Kruskal's greedy algorithm. 
Dong et al., "Affinity Matrices for Multi-View Multi-Person HPE" propose two affinity matrices for associating target bodies. These matrices measure the distance between 2D input pairs in terms of re-id visual features and geometric constraints. They adopt the 3DPS model to reconstruct the final motion. 
% The former____ is less flexible because the 2D detector cannot be changed, while the latter____ heavily relies on the person re-id techniques, which could increase computing complexity and fall into sub-optimal. 
Inspired by Li et al., "A 3D Pictorial Structure Model for Multi-View Multi-Person HPE", Zhou et al., "Graph-Based Clustering for Multi-View Multi-Person HPE" associate joint at part- and body-level. They build a skeletal graph, which includes all possible joint connections, and adopt a clustering algorithm to optimize the best connection.
Learning-based methods Zhang et al., "3D Convolutional Neural Networks for Multi-View Multi-Person HPE" propose to use 3D convolutional neural networks (CNNs) to locate the most likely 3D joint from a voxel-based 3D heatmap. Those techniques allow for an end-to-end \red{manner} where losses can be back propagated to the first stage. However, those voxel-based regressions have a significant computing cost due to their greedy space discretization. \red{Following the voxel representation, Ye et al., "Accelerating Multi-View Multi-Person HPE with 2D CNNs" accelerate the inference speed by utilizing 2D CNNs. Choudhury et al., "Kalman Filter and Spatial Gated Recurrent Units for Multi-View Multi-Person Pose Tracking and Forecasting" apply a Kalman filter and Spatial Gated Recurrent Units (GRUs) for pose tracking and forecasting. SelfPose3D suggests that combining synthetic root data with augmented 2D pseudo-pose data improves model accuracy. Despite their high accuracy, these methods lack generalizability and are unable to cope with changes in camera parameters. On the other hand,} Lin and Lee, "Plane-Sweep-Stereo-Based Network for Multi-View Multi-Person HPE" propose a plane-sweep-stereo-based network for regressing each joint's depth in every view. It \red{enables} real-time inference but requires fusing 3D poses from all views into the same coordinate, \red{which may result in duplicate poses}. 
% We argue that, from optimization to regression, the multi-stage framework's bottleneck is the quality of 2D detections.
We argue that, regardless of whether the method is optimization- or learning-based, the quality of 2D detections remains the bottleneck of the multi-stage framework.
Under heavy occlusions, 2D detections may have false target ID and joint type (see Fig.~\ref{fig_2d_detection}) in the first stage, resulting in the error-accumulated association in the second stage and ultimately leading to sub-optimal performance. In contrast, we propose the Joint Cloud which consists of all 3D joint candidates regardless of their target ID in order to preserve 2D detections in terms of their precise location. We then propose the JCSAT framework to select the potentially valid ones from redundancy with the trajectile, skeletal and angular cross-embedding. 
% All 2D information can be preserved as much as possible.
% This approach preserves as much 2D information as possible.

\noindent\textbf{Motion prior.} Recently, some research has been done to involve motion prior to 3D HPE tasks. Some studies Lin et al., "Predicting Limb Contact for Physically Plausible 3D Reconstruction" predict the contact of limbs to make the 3D reconstruction physically plausible. Some Wang et al., "Physics Simulation for Human Motion Prior" use physics simulation to avoid floor penetration. Some other works Xiao et al., "Estimating Extrinsic Camera Parameters from Projection Consistency" propose to estimate each camera's extrinsic parameters based on projection consistency across views. 
\red{Pose2UV focuses on human mesh recovery. It incorporates the human body mesh prior and the 2D pose prior to predict the corresponding SMPL-based Li et al., "Human Mesh Recovery with SMPL-Based Multi-Person Movements" multi-person movements. Notably, it introduces a large-scale multi-view multi-person motion dataset \textbf{3DMPB} with automatically annotated human target segmentation maps and SMPL data. In contrast, our datasets, BUMocap and BUMocap-X, are annotated manually.}
And we involve a weak prior which comes from the body's kinematic structure. We presume that the bone length should remain consistent throughout a motion sequence, and the structure should maintain a symmetrical form.

\noindent\textbf{Graph-based motion modelling.} Another line of work exemplifies the potential of graph models in human motion modelling. Yan et al., "Diverse Graph Convolution Kernels for Human Motion Modelling" focus on diverse designs of graph convolution kernels. Various works Wang et al., "Transformer-Based Networks for 3D HPE Tasks" propose implementing Transformer-based networks for 3D HPE tasks. Some Li et al., "Human Motion Modelling with Transformers" treat each input 2D joint as a token__ and apply Transformer__ \red{to} learn the projection between 2D and 3D. 
\red{POTR-3D proposes to utilize three Transformers to explore multi-person interactions. MTF-Transformer investigates the problem of calibration-free 3D HPE by using Transformers to parallelly predict camera extrinsic and motion.}
\red{Our previous work} Li et al., "Human Motion Modelling with Auto-Encoders" uses a Transformer-based Auto-encoder to complete the 3D motion.
In this work, we further explore the capability of the Transformer in 3D motion modelling. \red{The proposed model receives 3D joint candidates from the Joint Cloud, aggregates and selects them from redundancy, and then regresses these to produce the final motion.}

%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure*}[ht]
\centering
\includegraphics[width=1.0\linewidth]{figures/system_tvcg_v2.pdf}
% \setlength{\abovecaptionskip}{-4mm}
\caption{The proposed JCSAT pipeline for efficiently extracting kinematic skeletal structures from multi-view Joint Clouds. For every multi-view camera pair (e.g., $\alpha\gamma$), the Trajectory Encoder and Structure Encoder aggregate trajectile and skeletal features from the triangulated Joint Cloud in parallel. 
\red{POTR-3D} proposes to utilize three Transformers to explore multi-person interactions. 
\red{The proposed model receives 3D joint candidates from the Joint Cloud, aggregates and selects them from redundancy, and then regresses these to produce the final motion.}
\end{figure*}

%%%%%%%%%%%%%%%%%%%%%%%%

\begin{algorithm}
\caption{Joint Cloud Construction}\label{alg:joint_cloud_construction}
\begin{algorithmic}[1]
\Procedure{ConstructJointCloud}{$D, C$}
    \For{$n = 1$ to $N$ and $i = 1$ to $I$}
        \For{$(v', v'') \gets \dot{\mathbf{v}}$ and $(m', m'') \gets \dot{\mathbf{m}}$}
            \State $j^{i, \dot{\mathbf{m}}}_{n,\dot{\mathbf{v}}} \gets \tau (d_{i,m'}^{n,v'}, d_{i,m''}^{n,v''}, C_{v'}, C_{v''})$
        \EndFor
        \State $\mathbf{J}_n^m \gets $ Cluster$(\mathbf{J}_n, min(M_{n,1}, M_{n,2}, \cdots, M_{n,v}))$
    \EndFor
    \Return $\mathcal{J}$
\EndProcedure
\end{algorithmic}
\end{algorithm}