\section{Related Works}
% Done

\noindent\textbf{Multi-view multi-person 3D HPE.} 
% As mentioned in the introduction, multi-view multi-person 3D HPE frameworks typically are built from two stages. 
\red{These frameworks are normally built upon two stages.}
% The first stage detects 2D poses in every view. The second stage associates and selects 2D poses within the same target ID across views. 
We divide them into two categories: optimization-based and learning-based. 
Optimization-based methods mainly focus on the second stage.
Pioneers in this field, such as \cite{belagiannis20143d,belagiannis2014multiple} propose a 3D pictorial structure (3DPS) model for searching \red{corresponding} body joints from a common state space shared by all bodies. 
Zhang et al.~\cite{zhang20204d} associate 2D joints across views by optimizing the proposed geometric energy function, which includes the epipolar distance, PAF score~\cite{cao2019openpose} and tracking distance. 
% This optimization is done by Kruskal's greedy algorithm. 
Dong et al.~ \cite{dong2021fast} propose two affinity matrices for associating target bodies. These matrices measure the distance between 2D input pairs in terms of re-id visual features and geometric constraints. They adopt the 3DPS model to reconstruct the final motion. 
% The former~\cite{zhang20204d} is less flexible because the 2D detector cannot be changed, while the latter~\cite{dong2021fast} heavily relies on the person re-id techniques, which could increase computing complexity and fall into sub-optimal. 
Inspired by \cite{zhang20204d,dong2021fast}, Zhou et al.~\cite{zhou2022quickpose} associate joint at part- and body-level. They build a skeletal graph, which includes all possible joint connections, and adopt a clustering algorithm to optimize the best connection.
Learning-based methods~\cite{tu2020voxelpose,reddy2021tessetrack} propose to use 3D convolutional neural networks (CNNs) to locate the most likely 3D joint from a voxel-based 3D heatmap. Those techniques allow for an end-to-end \red{manner} where losses can be back propagated to the first stage. However, those voxel-based regressions have a significant computing cost due to their greedy space discretization. \red{Following the voxel representation, Ye et al.~\cite{ye2022faster} accelerate the inference speed by utilizing 2D CNNs. Choudhury et al.~\cite{choudhury2023tempo} apply a Kalman filter and Spatial Gated Recurrent Units (GRUs) for pose tracking and forecasting. SelfPose3D~\cite{srivastav2024selfpose3d} suggests that combining synthetic root data with augmented 2D pseudo-pose data improves model accuracy. Despite their high accuracy, these methods lack generalizability and are unable to cope with changes in camera parameters. On the other hand,} Lin and Lee~\cite{lin2021multi} propose a plane-sweep-stereo-based network for regressing each joint's depth in every view. It \red{enables} real-time inference but requires fusing 3D poses from all views into the same coordinate, \red{which may result in duplicate poses}. 
% We argue that, from optimization to regression, the multi-stage framework's bottleneck is the quality of 2D detections.
We argue that, regardless of whether the method is optimization- or learning-based, the quality of 2D detections remains the bottleneck of the multi-stage framework.
Under heavy occlusions, 2D detections may have false target ID and joint type (see Fig.~\ref{fig_2d_detection}) in the first stage, resulting in the error-accumulated association in the second stage and ultimately leading to sub-optimal performance. In contrast, we propose the Joint Cloud which consists of all 3D joint candidates regardless of their target ID in order to preserve 2D detections in terms of their precise location. We then propose the JCSAT framework to select the potentially valid ones from redundancy with the trajectile, skeletal and angular cross-embedding. 
% All 2D information can be preserved as much as possible.
% This approach preserves as much 2D information as possible.

\noindent\textbf{Motion prior.} Recently, some research has been done to involve motion prior to 3D HPE tasks. Some studies~\cite{rempe2021humor,huang2022neural} predict the contact of limbs to make the 3D reconstruction physically plausible. Some~\cite{gong2022posetriplet,maeda2022motionaug} use physics simulation to avoid floor penetration. Some other works~\cite{kolotouros2019learning,wandt2022elepose,wandt2021canonpose} propose to estimate each camera's extrinsic parameters based on projection consistency across views. 
\red{Pose2UV~\cite{huang2022pose2uv} focuses on human mesh recovery. It incorporates the human body mesh prior and the 2D pose prior to predict the corresponding SMPL-based~\cite{loper2015smpl} multi-person movements. Notably, it introduces a large-scale multi-view multi-person motion dataset \textbf{3DMPB} with automatically annotated human target segmentation maps and SMPL data. In contrast, our datasets, BUMocap and BUMocap-X, are annotated manually.}
And we involve a weak prior which comes from the body's kinematic structure. We presume that the bone length should remain consistent throughout a motion sequence, and the structure should maintain a symmetrical form.

\noindent\textbf{Graph-based motion modelling.} Another line of work exemplifies the potential of graph models in human motion modelling. Yan et al.~\cite{yan2018spatial} focus on diverse designs of graph convolution kernels. Various works~\cite{zheng20213d,zhu2021posegtac,li2022exploiting,jiang2022dual} propose implementing Transformer-based networks for 3D HPE tasks. Some~\cite{zheng20213d,zhu2021posegtac,li2022exploiting} treat each input 2D joint as a token~\cite{devlin2018bert} and apply Transformer~\cite{vaswani2017attention} \red{to} learn the projection between 2D and 3D. 
\red{POTR-3D~\cite{park2023towards} proposes to utilize three Transformers to explore multi-person interactions. MTF-Transformer~\cite{shuai2022adaptive} investigates the problem of calibration-free 3D HPE by using Transformers to parallelly predict camera extrinsic and motion.}
\red{Our previous work}~\cite{jiang2022dual} uses a Transformer-based Auto-encoder to complete the 3D motion.
In this work, we further explore the capability of the Transformer in 3D motion modelling. \red{The proposed model receives 3D joint candidates from the Joint Cloud, aggregates and selects them from redundancy, and then regresses these to produce the final motion.}

%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure*}[ht]
\centering
\includegraphics[width=1.0\linewidth]{figures/system_tvcg_v2.pdf}
% \setlength{\abovecaptionskip}{-4mm}
\caption{The proposed JCSAT pipeline for efficiently extracting kinematic skeletal structures from multi-view Joint Clouds. For every multi-view camera pair (e.g., $\alpha\gamma$), the Trajectory Encoder and Structure Encoder aggregate trajectile and skeletal features from the triangulated Joint Cloud in parallel. 
\red{The T-OTAP and S-OTAP modules select the optimal tokens along the joint candidate axis (joint candidates are constituents of the Joint Cloud, with each joint having multiple candidates triangulated from different camera view pairs) in a differentiable manner.
By traversing through all joints and frames, these candidate feature tokens are then blended element-wise into cross-embedding tokens. 
The cross-embedding tokens are then gathered across the axis of camera view pairs. A View Encoder extracts angular features based on which the subsequent V-OTAP module selects the most representative token for every joint across all frames. Ultimately, an MLP head module decodes the refined trajectory. The three OTAP modules enable comprehensive and efficient optimization of the entire system from end to end.}
%\red{Three Encoders with corresponding OTAP modules aggregate the features along the candidate axis and view-pair axis respectively.}
}
\label{fig:system}
\end{figure*}
%%%%%%%%%%%%%%%%%%%%%%%%

% TODO: 需要写OT的东西吗？
% \paragraph{Optimal transportation}.


% \setlength{\textfloatsep}{10pt}% Remove \textfloatsep
\begin{algorithm}[t]
\caption{Joint Cloud Construction}
\label{alg:Framwork}
\begin{algorithmic}
\Input
\Desc{$ \mathrm{D} $}{, 2D detections}
\Desc{$ \mathrm{C} $}{, Camera matrix}
\EndInput
\Output
\Desc{$\mathcal{J}$}{, Joint Cloud}
\EndOutput
\State %%%%%%%%%%%%%%%%%
\Function {Cluster }{$\mathbf{J}, M$}
\State $s \gets \mathbf{J}^{Hip}$
\State $c \gets \mathrm{KMeans}(s, M)$
% \For{$m = 1 \to M$} \Comment{Loop for each observed target}
% \For{$i = 1 \to I$} \Comment{Loop for each joint type}
\For{$m = 1 \to M$ and $i = 1 \to I$} 
\If {$dist(j^{i,m}, c^{m}) < \mathit{th}_i$} \Comment{Thresholding}
\State $\mathbf{J}^m \gets j^{i,m}$
\EndIf
\EndFor
\State \Return{$\mathbf{J}_m$}
\EndFunction
\State %%%%%%%%%%%%%%%%%
\Function {Construct Joint Cloud }{$ \mathrm{D}, \mathrm{C} $}
\For{$n = 1 \to N$ and $i = 1 \to I$} % \Comment{Loop for every frame, joint type}
\For{$(v', v'') \gets \dot{\mathbf{v}} $ and $(m', m'') \gets \dot{\mathbf{m}} $} \Comment{Eq.~\ref{eq.view_comb},~\ref{eq.id_comb}}
\State $j^{i, \dot{\mathbf{m}}}_{n,\dot{\mathbf{v}}} \gets \tau (d_{i,m'}^{n,v'}, d_{i,m''}^{n,v''}, \mathrm{C}_{v'}, \mathrm{C}_{v''})$ \Comment{Triangulate}
\EndFor
\State $\mathbf{J}_{n}^{m} \gets $ \Call{Cluster}{$\mathbf{J}_n, min(M_{n,1}, M_{n,2}, \cdots, M_{n,v})$}
\EndFor
\State \Return{$\mathcal{J}$}
\EndFunction
\end{algorithmic}
\end{algorithm}