% \subsection*{A}

\subsection{Discussion:\name works if all responses are from the same LM}
\input{tables/magpie-zoo}
In Section~\ref{sec:experiments_ultra}, we demonstrated how \name enables practitioners to refine model-generated responses for improved training outcomes. Beyond that, \name can optimize responses from a single generator. Using the Magpie-Zoo instruction set~\cite{xu2024strongermodelsstrongerteachers}, we sample 10 responses per instruction from Qwen2.5-72B-Instruct, select in-distribution responses with \name, to train a Mistral-v0.3-7B model. We also compare with training on top-3 reported datasets of Magpie-Zoo. As shown in Table~\ref{tab:magpie_zoo}, \name-selected responses further boost the performance. Practically, batch sampling of responses cause little latency~\cite{zhong2024distservedisaggregatingprefilldecoding,zhou2024survey}. One therefore can sample batches of responses from a single strong model and apply \name on top of it to find the response closest to the base model's distribution efficiently. 

\subsection{Discussion: Is In-Distribution A Silver Bullet?}
\label{sec:self_distillation}
The success of \name verifies our key hypothesis that matching SFT distribution to the base model benefits the performance. Yet, we argue that this cannot be pushed to the limit of SFT using data from the same distribution. 
By experimenting with training on solutions sampled from a trained version of the same base model, we notice a drastic performance drop from the original model as shown in Table~\ref{tab:self_distill_ultra}. 
\input{tables/self-distill}
We explored this using the \textbf{MATH} \cite{hendrycks2021measuringmathematicalproblemsolving} dataset. A base model was fine-tuned using solutions from stronger models like \textsc{Llama3.1-70B-Instruct} or GPT-3.5-Turbo-augmented \textbf{MetaMathQA} \cite{yu2024metamath}. This fine-tuned model then generated new solutions, which were used to further fine-tune another base model instance, simulating iterative on-policy tuning.
\input{tables/self-generate-math}
As shown in Table~\ref{tab:self_gen_math}, performance declined when solutions were sampled from the fine-tuned model itself. This decline stems from reduced solution diversity, leading to distributional drift and poorer generalization. 

These results validate our claim in \S\ref{sec:on_policy} that robust fine-tuning requires more than alignment with the base modelâ€”it needs complementary strategies to maintain response diversity and quality. 