\section{\name-Picking From Real-World SFT Datasets}
\label{sec:tulu_olmo}
In this section, we leverage the findings from the earlier experiments and demonstrate the effectiveness of \name to customize  training data for each base model by selecting from available datasets with overlapping instructions. 
Here, we do \emph{not} generate any new responses for the instructions; it only selects from existing ones.
We evaluate \name on the fully open dataset used in post-training phases of \textsc{Tulu-3}~\cite{lambert2024tulu3} and \textsc{Olmo-2}~\cite{olmo2025}. The details are presented below. 

\subsection{Data Mixture Details}
% \subsubsection{Tulu-Olmo Mixture}

\textsc{Tulu-3}~\cite{lambert2024tulu3} is a fully open-source collection of post-training recipes, including supervised fine-tuning and preference alignment data. \textsc{Olmo-2}~\cite{olmo2025} is a fully open-source language model. Both \textsc{Tulu-3} and \textsc{Olmo-2} use the same data mixture during the supervised fine-tuning stage, but different data mixtures and source models for generating preference data for different sizes of their models: Tulu-3-8B/70B and Olmo-2-7B/13B. 
To demonstrate the effectiveness of \name, we collected the overlapping instructions from both models and gather their corresponding responses.
From the preference data, we retained only the winning responses. We formed our candidate pool with those instructions with at least two distinct responses, resulting in a dataset of 350.4k unique instructions and about 1.03 million total instruction-response pairs for evaluation with \name.
We do \emph{not} apply further processing of these data or any filtering on top of  \name.
\subsection{Evaluation}

We evaluate on a set of commonly used benchmarks spanning over coding, math, knowledge and instruction-following. We evaluated on LeetCode~\cite{guo2024deepseekcoder}, MATH~\cite{hendrycks2021measuringmathematicalproblemsolving}, BigBenchHard(BBH)~\cite{suzgun2022bbh}, MMLU~\cite{hendrycks2021mmlu}, and AlpacaEval-V2~\cite{dubois2024alpacaevalv2}. LeetCode, MATH, BBH and MMLU are evaluated the in the same way as in~\cite{yuan2024eurus}, where we use zero-shot for MATH and MMLU, 3-shot example for BBH. We use the same AlpacaEval-v2 as in OpenInstruct.  
\subsection{Baselines}
% We extensively compare \name against multiple baselines to show the effectiveness of \name and study the effect of the level of distribution match on the final performance. 
\begin{itemize}
    \item \textbf{Responses From The Original SFT on The Subset.}
In this experiment, we keep the same set of instructions (350.4k) and pair each instruction with the response from the SFT mixture.
\item \textbf{Random Response Candidate From The Pool.} Randomly pairing each instruction with a candidate response for that instruction, which may come from either the SFT mixture or a winning response in the preference-learning data.
\item \textbf{Lowest-Probability Response.} Instead of taking the highest probability response to each instruction for the base model, we do the opposite on  and take the lowest for each. 
\item \textbf{Entire SFT Dataset.} We train on the entire SFT data mixture of Tulu with 939k instances. 
\item \textbf{All Available Responses For The Overlapping Instructions.} In this setting, we train on the entire candidate pool for all overlapping instructions, resulting in 103.6k instances. 
\item \textbf{S2L~\cite{yang2024s2l}.} S2L is a state-of-the-art unsupervised data selection baseline designed to select balanced subsets from large datasets by leveraging training loss trajectories. It first trains a small reference model within the same model family and record loss trajectory for each training example, then applies K-means clustering on the loss trajectory and samples equally from each. We use \textsc{Llama-3.2-1B} as reference for \textsc{Llama3.1-8B} and \textsc{Qwen2.5-0.5B} for \textsc{Qwen2.5-7B}. For \textsc{Mistral-v0.3-7B}, we use itself as reference since no smaller models in the family are available. For S2L, we select the same number of data from the same pool as \name. Further details in Appendix~\ref{app:s2l_details}.
\item \textbf{All data available for SFT mixture.} We train with all the available responses from all these datasets that share prompts with the SFT mixture. Which gives approximately 1.58 million (4.5 times thesize of \name-curated dataset). This is the union of the entire dataset and GRAPE's candidate pool.
% \item \textbf{DEITA~\cite{liu2024deita}.} DEITA is a model-independent data selection approach that automatically finds high-quality and diverse training data. 
% It requires training a scorer for quality and complexity each, with ChatGPT-annotated training examples, which are subsequently used for each instruction-response pair on quality and complexity. It then generates embeddings and applies diversity-driven filtering process. We used the released scorers to annotate the data and generate embeddings with \textsc{Mistral-7B}. \\ 
\end{itemize}
\input{tables/tulu-olmo}
% \paragraph{All Available Responses}\dylan{See If this can be finished}
\subsection{Results}

As shown in Table~\ref{tab:olmo-tulu}, models fine-tuned on responses selected by \name outperforms the strong baselines we constructed, especially the one that trains over all available data by significant margins across the 3 models. 
Remarkably, Using roughly 1/6 training computation (Tulu3-8B-SFT was trained for 2 epochs on 3 times of data), our performance exceeds that of \textsc{Tulu3-8B-SFT}. 
Also, \name outperforms state-of-the-art data-selection approaches like S2L, despite its simplicity and efficiency, further highlighting its effectiveness in diverse real-world scenarios and strengthening its overall practicability for large-scale instruction tuning and robust generalization.
Without the need to synthesize any new data, one can easily leverage established datasets sourced from the web to customize a dataset for each base model that yields better fine-tuning outcome. 

These results feature \name not only as an effective strategy to enhance performances, but a handy approach to improve fine-tuning efficiency. 

\subsection{Further Experiments}
\input{tables/openhermes}
We extend our evaluation using \textsc{OpenHermes-2.5}~\cite{OpenHermes}, a high-quality, open instruction-tuning dataset containing approximately 1 million distinct instructions. Following a similar strategy to the previous combo experiment, we source responses from additional datasets from ~\citet{open_hermes_preferences} and~\citet{huggingface2024openhermes}. For preference-based datasets, we include only the winning responses, consistent with the methodology in the Tulu-Olmo experiment. This process results in 575K unique instructions and 1.34 million total instruction-response pairs. 
As shown in Table~\ref{tab:hermes}, \name-selected data yields better performances too. 
The consistent improvement across models reaffirm our earlier observations: \name serves as an adaptive response selection mechanism that significantly enhances SFT performance that has wide applicability. 



