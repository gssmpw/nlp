\section{Background and Motivation}
\label{sec:motivation}
% SEC1 COMMENT
% \hao{this one is mainly about rl and pl so probably fits better after 3.2 where we get inspired RL. i would also trim this section since rl is not the main focus here}

% % SEC 2 COMMENT
% \hao{the argument that we should stay close to the pretrained checkpoint looks relevant (we should mention trust region method if we keep this dicussion).
% but might invite criticism why we did not compare to the regularization baselines.
% also, we didn't really measure how close our finetuned models are to the pretrained checkpoint, and there are just so many easier approaches to achieve the same goal.
% for these reasons i think this paragraph below does more harm than good.
% }
\paragraph{An analogy from Reinforcement Learning and Preference Learning}
% These observations are supported\hao{this is too strong since below are not sft anymore. `inspired' or `analogous' are more proper} 

The investigation of this work into the distribution match between the pre-trained LM and supervised fine-tuning (SFT) data is inspired by recent findings on policy optimization for LM alignment with RL~\cite{ouyang2022training} and preference learning~\cite{rafailov2023dpo,ethayarajh2024kto}. 
While the importance of matching training data distribution with the  policy has been well noted in both traditional RL~\cite{shi2023offlinereinforcementlearningonpolicy,fujimoto2018offpolicy,kumar2019offpolicy,peng2019offpolicy,wang2021criticregularizedregression,arora2023theory,jiang2016doublyrobustoffpolicyvalue,tang2010importancesampling} and LM settings~\cite{xiong2024iterative}, preference learning algorithms like DPO~\cite{rafailov2023dpo}, IPO~\cite{azar2023ipo} and KTO~\cite{ethayarajh2024kto} first emerged as off-policy algorithms. 
However, subsequent research has highlighted the performance gap between on-policy and off-policy training due to distribution shifts ~\cite{xu2024dposuperiorppollm,tang2024understandingperformancegaponline} and proposed various mitigation strategies ~\cite{zhuang2023bpo,zhou2024wpoenhancingrlhfweighted,zhang2024textbfplumimprovingcodelms,xiong2024iterative,guo2024onlineaifeedback}, showing that training models on data more closely aligned with their policy distribution can significantly improve performance, while failing to do so can yield sub-optimal policies or those that are harder to generalize. 
% The central premise of this work is that the responses on which we supervise the pre-trained language models should match the initial distribution of the pre-trained model. 
\paragraph{Hypothesis: Supervised Fine-tuning Benefits From Data That Better Matches Base Distribution}

The base distribution of pre-trained language models—shaped by extensive training on vast and diverse datasets—is inherently robust and generalizable. Therefore, during supervised fine-tuning phase, the pre-trained distribution should be carefully preserved~\cite{kumar2022finetuningdistortpretrainedfeatures,cohenwang2024askdistributionshiftpretraining,he2023preservingpretrained,yang2024selfdistillationbridgesdistributiongap,ding2023peft}, to best retain the knowledge and capabilities that emerge during pre-training~\cite{zhou2023lima}. 
If the proximity between the pre-trained distribution and the fine-tuning data is not maintained, the limited number of training examples available during SFT, compared to the vast scale of pre-training data, can increase the risk of distribution distortion. This misalignment can lead to issues such as catastrophic forgetting~\cite{aghajanyan2020betterfinetuningreducingrepresentational,yang2024selfdistillationbridgesdistributiongap} and the emergence of spurious correlations~\cite{feldman2021doeslearningrequirememorization}.

The central premise of our work is that by using responses closely aligned to the pre-trained distribution, we can minimize distribution shift during SFT and therefore achieve better data efficiency and stronger performance. 
% . This, in turn, leads to more efficient and effective model alignment.

% \hao{i feel that the para below should be weaved into the prev one, before we talk about preserving the pretrained distribution:}
% Conversely, if the proximity of the two distributions is not maintained, the limited number of training examples available during the supervised fine-tuning stage, compared to the vast scale of pre-training data, can increase the risk of distribution distortion. This can lead to issues such as catastrophic forgetting~\cite{aghajanyan2020betterfinetuningreducingrepresentational,yang2024selfdistillationbridgesdistributiongap} and the emergence of spurious correlations~\cite{feldman2021doeslearningrequirememorization}.


\paragraph{From On-Policy Alignment To Distribution-Aligned SFT}
\label{sec:on_policy}
We build on principles of on-policy alignment techniques with key distinctions tailored for SFT. Given that SFT reprsents a more primitive stage of post-training than preference learning, strictly sampling responses from the base model itself can introduce risks such as instability, bias reinforcement, knowledge stagnation, and overfitting~\cite{herel2024collapseselftrainedlanguagemodels,mobahi2020selfdistillationamplifiesregularizationhilbert,allenzhu2023understandingensembleknowledgedistillation}. To address this, we advocate for an approach that stays in-distribution while delivering effective supervision to the base model. To this end, we propose to gather and select responses from various sources, and select one that is closest to the target model's pretrained distribution, which we name \name. 
% % %%%%%%%%%%%%%%%%%%%
% % Original version
% % %%%%%%%%%%%%%%%%%%%
% \subsection{On-Policy Optimization Of Policies}
% \hao{this one is mainly about rl and pl so probably fits better after 3.2 where we get inspired RL. i would also trim this section since rl is not the main focus here}
% In the domain of Reinforcement Learning (RL), the catastrophic effects of distribution shifts caused by the mismatch between training data and the behavior policy to be optimized has been widely observed and studied~\cite{shi2023offlinereinforcementlearningonpolicy,fujimoto2018offpolicy,kumar2019offpolicy,peng2019offpolicy,wang2021criticregularizedregression,arora2023theory}. 

% On-policy RL algorithms offers advantages such as improved stability and reduced bias compared to off-policy methods, which sample state-action pairs from a different distribution~\cite{gu2017qpropsampleefficientpolicygradient,gu2017interpolatedpolicygradientmerging}. To address the challenges caused by distribution shifts in off-policy methods, techniques like replay buffers, target networks, and importance sampling (as well as more advanced sampling methods~\cite{jiang2016doublyrobustoffpolicyvalue,tang2010importancesampling}) are commonly employed to mitigate these negative effects.

% Recently, policy optimization algorithms- both RL and preference learning ~\cite{rafailov2023dpo,ethayarajh2024kto} algorithms, being intensively adopted in the context of language model fine-tuning. While on the RL side, the predominant proximal-policy optimization (PPO)\todo{cite} algorithm is on-policy, preference learning algorithms inspired by its success while aiming to reduce resource overheads, first emerged off-policy.  Objectives like Direct Preference Optimization (DPO) and Identity Preference Optimization (IPO) ~\cite{azar2023ipo} are empirically shown to exhibit sub-optimal performances when the distribution mismatch occurs~\cite{xu2024dposuperiorppollm,tang2024understandingperformancegaponline} between data and policy, leading to a series of investigations to device on-policy algorithms~\cite{zhuang2023bpo,zhou2024wpoenhancingrlhfweighted} for preference optimization.

% % ~\cite{zhuang2023bpo}
% % ~\cite{arora2023theory} % policy gradient

% % ~\cite{zhou2024wpoenhancingrlhfweighted} % wpo 
% \subsection{An Analogy In The Context Of Supervised Fine-tuning}
% \hao{the maths in this section doesn't add much since it is never used later and does not really help make any argument since these are all from prev works.
% i'd drop the maths
% }
% The potential negative impact of fine-tuning on pre-trained models has been discussed~\cite{kumar2022finetuningdistortpretrainedfeatures}. Under LM supervised fine-tuning set-up, the mis-match between the distribution of the responses in the SFT dataset $p_{\mathcal{D}}({y|x})$ and the pre-trained model's distribution $\pi_{\theta_0}(y|x)$ can also cause negative effects. 

% \hao{the argument that we should stay close to the pretrained checkpoint looks relevant (we should mention trust region method if we keep this dicussion).
% but might invite criticism why we did not compare to the regularization baselines.
% also, we didn't really measure how close our finetuned models are to the pretrained checkpoint, and there are just so many easier approaches to achieve the same goal.
% for these reasons i think this paragraph below does more harm than good.
% }
% If the the distance between these two distributions, \(
% D_{KL}(p_{\mathcal{D}} || \pi_{\theta_0})
% \) is large, then the best approximator of the distribution $p_{\mathcal{D}}$,$
% \theta_{\mathcal{D}}^* = \arg\min_{\theta} \mathbb{E}_{(x,y) \sim p_{\mathcal{D}}}\left[-\log \pi_{\theta}(y|x)\right],
% $ will be inevitably far away from the base parameters $\theta_0$. This will risk a high bias due to under-fiting induced by regularization that penalizes \( \|\theta - \theta_0\|\)or a large variance due to the large search space around $\theta_0$ given large \(\lVert \theta_{\mathcal{D}}^{*} - \theta_0 \lVert \)~\cite{information_geometry_applications,cover_thomas_information_theory}. 

% Further, larger distance in distribution can also make the fine-tuned model more prune to spurious correlations~\cite{feldman2021doeslearningrequirememorization} in the data, and cause catastrophic forgetting~\cite{aghajanyan2020betterfinetuningreducingrepresentational,yang2024selfdistillationbridgesdistributiongap} of the pre-trained knowledge. 

% Assume that the divergence between \(\pi_{\theta_0}\) and \(p_{\mathcal{D}}\) is large—e.g., \(D_{\mathrm{KL}}(\pi_{\theta_0}\,\|\, p_{\mathcal{D}})\) is large, then for \textit{any} single \(\theta\), it is impossible to achieve \textit{optimal} performance on both distributions simultaneously;
% \[
% \min_{\theta} 
% \bigl[
% \mathcal{L}_{\mathcal{D}}(\theta) + \mathcal{L}_{\text{orig}}(\theta)
% \bigr]
% \ge
% \min_{\theta} 
% \mathcal{L}_{\mathcal{D}}(\theta)
% +
% \min_{\theta} 
% \mathcal{L}_{\text{orig}}(\theta)
% - \mathcal{C},
% \]
% where $\mathcal{C}$ depends on how close \(p_{\mathcal{D}}\) is to \(q_{\text{orig}}\).
% \hao{this transition is very abrupt---before here we've been talking about staying close to the pretrained distribution but now suddenly change to RL. i didn't get the connection between them}
% Thus, inspired by the insights from on-policy RL and preference learning, we explore the possibility of overcoming the potential issues above by better aligning the response distribution in the SFT data with the pre-trained policy of each LM.


