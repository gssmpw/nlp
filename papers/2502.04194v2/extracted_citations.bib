@misc{chan2024balancingcosteffectivenesssynthetic,
      title={Balancing Cost and Effectiveness of Synthetic Data Generation Strategies for LLMs}, 
      author={Yung-Chieh Chan and George Pu and Apaar Shanker and Parth Suresh and Penn Jenks and John Heyer and Sam Denton},
      year={2024},
      eprint={2409.19759},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2409.19759}, 
}

@article{chen2023alpagasus,
  title={Alpagasus: Training a better alpaca with fewer data},
  author={Chen, Lichang and Li, Shiyang and Yan, Jun and Wang, Hai and Gunaratna, Kalpa and Yadav, Vikas and Tang, Zheng and Srinivasan, Vijay and Zhou, Tianyi and Huang, Heng and others},
  journal={arXiv preprint arXiv:2307.08701},
  year={2023}
}

@article{das2023deft,
  title={DEFT: Data Efficient Fine-Tuning for Large Language Models via Unsupervised Core-Set Selection},
  author={Das, Devleena and Khetan, Vivek},
  journal={arXiv preprint arXiv:2310.16776},
  year={2023}
}

@misc{du2023modsmodelorienteddataselection,
      title={MoDS: Model-oriented Data Selection for Instruction Tuning}, 
      author={Qianlong Du and Chengqing Zong and Jiajun Zhang},
      year={2023},
      eprint={2311.15653},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2311.15653}, 
}

@misc{gulcehre2023rest,
      title={Reinforced Self-Training (ReST) for Language Modeling}, 
      author={Caglar Gulcehre and Tom Le Paine and Srivatsan Srinivasan and Ksenia Konyushkova and Lotte Weerts and Abhishek Sharma and Aditya Siddhant and Alex Ahern and Miaosen Wang and Chenjie Gu and Wolfgang Macherey and Arnaud Doucet and Orhan Firat and Nando de Freitas},
      year={2023},
      eprint={2308.08998},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2308.08998}, 
}

@misc{guo2024onlineaifeedback,
      title={Direct Language Model Alignment from Online AI Feedback}, 
      author={Shangmin Guo and Biao Zhang and Tianlin Liu and Tianqi Liu and Misha Khalman and Felipe Llinares and Alexandre Rame and Thomas Mesnard and Yao Zhao and Bilal Piot and Johan Ferret and Mathieu Blondel},
      year={2024},
      eprint={2402.04792},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2402.04792}, 
}

@article{kang2024getmoreforless,
  title={Get more for less: Principled Data Selection for Warming Up Fine-Tuning in LLMs},
  author={Kang, Feiyang and Just, Hoang Anh and Sun, Yifan and Jahagirdar, Himanshu and Zhang, Yuanzhi and Du, Rongxing and Sahu, Anit Kumar and Jia, Ruoxi},
  journal={arXiv preprint arXiv:2405.02774},
  year={2024}
}

@inproceedings{li2024quantitytoqulality,
    title = "From Quantity to Quality: Boosting {LLM} Performance with Self-Guided Data Selection for Instruction Tuning",
    author = "Li, Ming  and
      Zhang, Yong  and
      Li, Zhitao  and
      Chen, Jiuhai  and
      Chen, Lichang  and
      Cheng, Ning  and
      Wang, Jianzong  and
      Zhou, Tianyi  and
      Xiao, Jing",
    editor = "Duh, Kevin  and
      Gomez, Helena  and
      Bethard, Steven",
    booktitle = "Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)",
    month = jun,
    year = "2024",
    address = "Mexico City, Mexico",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.naacl-long.421",
    doi = "10.18653/v1/2024.naacl-long.421",
    pages = "7602--7635",
}

@misc{li2024scar,
      title={SCAR: Efficient Instruction-Tuning for Large Language Models via Style Consistency-Aware Response Ranking}, 
      author={Zhuang Li and Yuncheng Hua and Thuy-Trang Vu and Haolan Zhan and Lizhen Qu and Gholamreza Haffari},
      year={2024},
      eprint={2406.10882},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2406.10882}, 
}

@inproceedings{li2024selective,
    title = "Selective Reflection-Tuning: Student-Selected Data Recycling for {LLM} Instruction-Tuning",
    author = "Li, Ming  and
      Chen, Lichang  and
      Chen, Jiuhai  and
      He, Shwai  and
      Gu, Jiuxiang  and
      Zhou, Tianyi",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2024",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.findings-acl.958",
    doi = "10.18653/v1/2024.findings-acl.958",
    pages = "16189--16211"
}

@misc{li2024superfilteringweaktostrongdatafiltering,
      title={Superfiltering: Weak-to-Strong Data Filtering for Fast Instruction-Tuning}, 
      author={Ming Li and Yong Zhang and Shwai He and Zhitao Li and Hongyu Zhao and Jianzong Wang and Ning Cheng and Tianyi Zhou},
      year={2024},
      eprint={2402.00530},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2402.00530}, 
}

@misc{liu2024provablymitigatingoveroptimizationrlhf,
      title={Provably Mitigating Overoptimization in RLHF: Your SFT Loss is Implicitly an Adversarial Regularizer}, 
      author={Zhihan Liu and Miao Lu and Shenao Zhang and Boyi Liu and Hongyi Guo and Yingxiang Yang and Jose Blanchet and Zhaoran Wang},
      year={2024},
      eprint={2405.16436},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2405.16436}, 
}

@article{mekala2024smaller,
  title={Smaller language models are capable of selecting instruction-tuning training data for larger language models},
  author={Mekala, Dheeraj and Nguyen, Alex and Shang, Jingbo},
  journal={arXiv preprint arXiv:2402.10430},
  year={2024}
}

@misc{miao2024aligningcodellmsdirectpreference,
      title={Aligning CodeLLMs with Direct Preference Optimization}, 
      author={Yibo Miao and Bofei Gao and Shanghaoran Quan and Junyang Lin and Daoguang Zan and Jiaheng Liu and Jian Yang and Tianyu Liu and Zhijie Deng},
      year={2024},
      eprint={2410.18585},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2410.18585}, 
}

@article{ouyang2022training,
  title={Training language models to follow instructions with human feedback},
  author={Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={27730--27744},
  year={2022}
}

@article{parkar2024selectllm,
  title={SelectLLM: Can LLMs Select Important Instructions to Annotate?},
  author={Parkar, Ritik Sachin and Kim, Jaehyung and Park, Jong Inn and Kang, Dongyeop},
  journal={arXiv preprint arXiv:2401.16553},
  year={2024}
}

@misc{rafailov2023dpo,
      title={Direct Preference Optimization: Your Language Model is Secretly a Reward Model}, 
      author={Rafael Rafailov and Archit Sharma and Eric Mitchell and Stefano Ermon and Christopher D. Manning and Chelsea Finn},
      year={2023},
      eprint={2305.18290},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{tajwar2024shoulduse,
  title={Preference fine-tuning of llms should leverage suboptimal, on-policy data},
  author={Tajwar, Fahim and Singh, Anikait and Sharma, Archit and Rafailov, Rafael and Schneider, Jeff and Xie, Tengyang and Ermon, Stefano and Finn, Chelsea and Kumar, Aviral},
  journal={arXiv preprint arXiv:2404.14367},
  year={2024}
}

@misc{wei2023magicoder,
      title={Magicoder: Source Code Is All You Need}, 
      author={Yuxiang Wei and Zhe Wang and Jiawei Liu and Yifeng Ding and Lingming Zhang},
      year={2023},
      eprint={2312.02120},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{xia2024less,
   title={{LESS}: Selecting Influential Data for Targeted Instruction Tuning},
   author={Xia, Mengzhou and Malladi, Sadhika and Gururangan, Suchin and Arora, Sanjeev and Chen, Danqi},
   booktitle={International Conference on Machine Learning (ICML)},
   year={2024}}

@misc{xu2023rethinkinginstructionqualitylift,
      title={Rethinking the Instruction Quality: LIFT is What You Need}, 
      author={Yang Xu and Yongqiang Yao and Yufan Huang and Mengnan Qi and Maoquan Wang and Bin Gu and Neel Sundaresan},
      year={2023},
      eprint={2312.11508},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2312.11508}, 
}

@inproceedings{xu2024dposuperiorppollm,
  author={Shusheng Xu and Wei Fu and Jiaxuan Gao and Wenjie Ye and Weilin Liu and Zhiyu Mei and Guangju Wang and Chao Yu and Yi Wu},
  title={Is DPO Superior to PPO for LLM Alignment? A Comprehensive Study},
  year={2024},
  cdate={1704067200000},
  url={https://openreview.net/forum?id=6XH8R7YrSk},
  booktitle={ICML},
}

@article{yang2024smalltolarge,
  title={SmallToLarge (S2L): Scalable Data Selection for Fine-tuning Large Language Models by Summarizing Training Trajectories of Small Models},
  author={Yang, Yu and Mishra, Siddhartha and Chiang, Jeffrey N and Mirzasoleiman, Baharan},
  journal={arXiv preprint arXiv:2403.07384},
  year={2024}
}

@article{zeng2024automatic,
  title={Automatic Instruction Evolving for Large Language Models},
  author={Zeng, Weihao and Xu, Can and Zhao, Yingxiu and Lou, Jian-Guang and Chen, Weizhu},
  journal={arXiv preprint arXiv:2406.00770},
  year={2024}
}

@misc{zhang2024selfexploringlanguagemodelsactive,
      title={Self-Exploring Language Models: Active Preference Elicitation for Online Alignment}, 
      author={Shenao Zhang and Donghan Yu and Hiteshi Sharma and Han Zhong and Zhihan Liu and Ziyi Yang and Shuohang Wang and Hany Hassan and Zhaoran Wang},
      year={2024},
      eprint={2405.19332},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2405.19332}, 
}

@misc{zhang2024textbfplumimprovingcodelms,
      title={$\textbf{PLUM}$: Improving Code LMs with Execution-Guided On-Policy Preference Learning Driven By Synthetic Test Cases}, 
      author={Dylan Zhang and Shizhe Diao and Xueyan Zou and Hao Peng},
      year={2024},
      eprint={2406.06887},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2406.06887}, 
}

