[
  {
    "index": 0,
    "papers": [
      {
        "key": "xu2023rethinkinginstructionqualitylift",
        "author": "Yang Xu and Yongqiang Yao and Yufan Huang and Mengnan Qi and Maoquan Wang and Bin Gu and Neel Sundaresan",
        "title": "Rethinking the Instruction Quality: LIFT is What You Need"
      },
      {
        "key": "xia2024less",
        "author": "Xia, Mengzhou and Malladi, Sadhika and Gururangan, Suchin and Arora, Sanjeev and Chen, Danqi",
        "title": "{LESS}: Selecting Influential Data for Targeted Instruction Tuning"
      },
      {
        "key": "chan2024balancingcosteffectivenesssynthetic",
        "author": "Yung-Chieh Chan and George Pu and Apaar Shanker and Parth Suresh and Penn Jenks and John Heyer and Sam Denton",
        "title": "Balancing Cost and Effectiveness of Synthetic Data Generation Strategies for LLMs"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "xu2024wizardlm",
        "author": "Can Xu and Qingfeng Sun and Kai Zheng and Xiubo Geng and Pu Zhao and Jiazhan Feng and Chongyang Tao and Qingwei Lin and Daxin Jiang",
        "title": "Wizard{LM}: Empowering Large Pre-Trained Language Models to Follow Complex Instructions"
      },
      {
        "key": "zeng2024automatic",
        "author": "Zeng, Weihao and Xu, Can and Zhao, Yingxiu and Lou, Jian-Guang and Chen, Weizhu",
        "title": "Automatic Instruction Evolving for Large Language Models"
      },
      {
        "key": "yu2024metamath",
        "author": "Longhui Yu and Weisen Jiang and Han Shi and Jincheng YU and Zhengying Liu and Yu Zhang and James Kwok and Zhenguo Li and Adrian Weller and Weiyang Liu",
        "title": "MetaMath: Bootstrap Your Own Mathematical Questions for Large Language Models"
      },
      {
        "key": "wei2023magicoder",
        "author": "Yuxiang Wei and Zhe Wang and Jiawei Liu and Yifeng Ding and Lingming Zhang",
        "title": "Magicoder: Source Code Is All You Need"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "xia2024less",
        "author": "Xia, Mengzhou and Malladi, Sadhika and Gururangan, Suchin and Arora, Sanjeev and Chen, Danqi",
        "title": "{LESS}: Selecting Influential Data for Targeted Instruction Tuning"
      },
      {
        "key": "chen2023alpagasus",
        "author": "Chen, Lichang and Li, Shiyang and Yan, Jun and Wang, Hai and Gunaratna, Kalpa and Yadav, Vikas and Tang, Zheng and Srinivasan, Vijay and Zhou, Tianyi and Huang, Heng and others",
        "title": "Alpagasus: Training a better alpaca with fewer data"
      },
      {
        "key": "parkar2024selectllm",
        "author": "Parkar, Ritik Sachin and Kim, Jaehyung and Park, Jong Inn and Kang, Dongyeop",
        "title": "SelectLLM: Can LLMs Select Important Instructions to Annotate?"
      },
      {
        "key": "li2024scar",
        "author": "Zhuang Li and Yuncheng Hua and Thuy-Trang Vu and Haolan Zhan and Lizhen Qu and Gholamreza Haffari",
        "title": "SCAR: Efficient Instruction-Tuning for Large Language Models via Style Consistency-Aware Response Ranking"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "chen2023alpagasus",
        "author": "Chen, Lichang and Li, Shiyang and Yan, Jun and Wang, Hai and Gunaratna, Kalpa and Yadav, Vikas and Tang, Zheng and Srinivasan, Vijay and Zhou, Tianyi and Huang, Heng and others",
        "title": "Alpagasus: Training a better alpaca with fewer data"
      },
      {
        "key": "parkar2024selectllm",
        "author": "Parkar, Ritik Sachin and Kim, Jaehyung and Park, Jong Inn and Kang, Dongyeop",
        "title": "SelectLLM: Can LLMs Select Important Instructions to Annotate?"
      },
      {
        "key": "li2024superfilteringweaktostrongdatafiltering",
        "author": "Ming Li and Yong Zhang and Shwai He and Zhitao Li and Hongyu Zhao and Jianzong Wang and Ning Cheng and Tianyi Zhou",
        "title": "Superfiltering: Weak-to-Strong Data Filtering for Fast Instruction-Tuning"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "kang2024getmoreforless",
        "author": "Kang, Feiyang and Just, Hoang Anh and Sun, Yifan and Jahagirdar, Himanshu and Zhang, Yuanzhi and Du, Rongxing and Sahu, Anit Kumar and Jia, Ruoxi",
        "title": "Get more for less: Principled Data Selection for Warming Up Fine-Tuning in LLMs"
      },
      {
        "key": "mekala2024smaller",
        "author": "Mekala, Dheeraj and Nguyen, Alex and Shang, Jingbo",
        "title": "Smaller language models are capable of selecting instruction-tuning training data for larger language models"
      },
      {
        "key": "xia2024less",
        "author": "Xia, Mengzhou and Malladi, Sadhika and Gururangan, Suchin and Arora, Sanjeev and Chen, Danqi",
        "title": "{LESS}: Selecting Influential Data for Targeted Instruction Tuning"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "yang2024smalltolarge",
        "author": "Yang, Yu and Mishra, Siddhartha and Chiang, Jeffrey N and Mirzasoleiman, Baharan",
        "title": "SmallToLarge (S2L): Scalable Data Selection for Fine-tuning Large Language Models by Summarizing Training Trajectories of Small Models"
      },
      {
        "key": "das2023deft",
        "author": "Das, Devleena and Khetan, Vivek",
        "title": "DEFT: Data Efficient Fine-Tuning for Large Language Models via Unsupervised Core-Set Selection"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "li2024quantitytoqulality",
        "author": "Li, Ming  and\nZhang, Yong  and\nLi, Zhitao  and\nChen, Jiuhai  and\nChen, Lichang  and\nCheng, Ning  and\nWang, Jianzong  and\nZhou, Tianyi  and\nXiao, Jing",
        "title": "From Quantity to Quality: Boosting {LLM} Performance with Self-Guided Data Selection for Instruction Tuning"
      },
      {
        "key": "du2023modsmodelorienteddataselection",
        "author": "Qianlong Du and Chengqing Zong and Jiajun Zhang",
        "title": "MoDS: Model-oriented Data Selection for Instruction Tuning"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "li2024selective",
        "author": "Li, Ming  and\nChen, Lichang  and\nChen, Jiuhai  and\nHe, Shwai  and\nGu, Jiuxiang  and\nZhou, Tianyi",
        "title": "Selective Reflection-Tuning: Student-Selected Data Recycling for {LLM} Instruction-Tuning"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "ouyang2022training",
        "author": "Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others",
        "title": "Training language models to follow instructions with human feedback"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "lee2024rlaif",
        "author": "Harrison Lee and Samrat Phatale and Hassan Mansoor and Kellie Ren Lu and Thomas Mesnard and Johan Ferret and Colton Bishop and Ethan Hall and Victor Carbune and Abhinav Rastogi",
        "title": "{RLAIF}: Scaling Reinforcement Learning from Human Feedback with {AI} Feedback"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "rafailov2023dpo",
        "author": "Rafael Rafailov and Archit Sharma and Eric Mitchell and Stefano Ermon and Christopher D. Manning and Chelsea Finn",
        "title": "Direct Preference Optimization: Your Language Model is Secretly a Reward Model"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "xu2024dposuperiorppollm",
        "author": "Shusheng Xu and Wei Fu and Jiaxuan Gao and Wenjie Ye and Weilin Liu and Zhiyu Mei and Guangju Wang and Chao Yu and Yi Wu",
        "title": "Is DPO Superior to PPO for LLM Alignment? A Comprehensive Study"
      },
      {
        "key": "tajwar2024shoulduse",
        "author": "Tajwar, Fahim and Singh, Anikait and Sharma, Archit and Rafailov, Rafael and Schneider, Jeff and Xie, Tengyang and Ermon, Stefano and Finn, Chelsea and Kumar, Aviral",
        "title": "Preference fine-tuning of llms should leverage suboptimal, on-policy data"
      },
      {
        "key": "zhang2024selfexploringlanguagemodelsactive",
        "author": "Shenao Zhang and Donghan Yu and Hiteshi Sharma and Han Zhong and Zhihan Liu and Ziyi Yang and Shuohang Wang and Hany Hassan and Zhaoran Wang",
        "title": "Self-Exploring Language Models: Active Preference Elicitation for Online Alignment"
      },
      {
        "key": "guo2024onlineaifeedback",
        "author": "Shangmin Guo and Biao Zhang and Tianlin Liu and Tianqi Liu and Misha Khalman and Felipe Llinares and Alexandre Rame and Thomas Mesnard and Yao Zhao and Bilal Piot and Johan Ferret and Mathieu Blondel",
        "title": "Direct Language Model Alignment from Online AI Feedback"
      },
      {
        "key": "liu2024provablymitigatingoveroptimizationrlhf",
        "author": "Zhihan Liu and Miao Lu and Shenao Zhang and Boyi Liu and Hongyi Guo and Yingxiang Yang and Jose Blanchet and Zhaoran Wang",
        "title": "Provably Mitigating Overoptimization in RLHF: Your SFT Loss is Implicitly an Adversarial Regularizer"
      },
      {
        "key": "zhang2024textbfplumimprovingcodelms",
        "author": "Dylan Zhang and Shizhe Diao and Xueyan Zou and Hao Peng",
        "title": "$\\textbf{PLUM}$: Improving Code LMs with Execution-Guided On-Policy Preference Learning Driven By Synthetic Test Cases"
      },
      {
        "key": "miao2024aligningcodellmsdirectpreference",
        "author": "Yibo Miao and Bofei Gao and Shanghaoran Quan and Junyang Lin and Daoguang Zan and Jiaheng Liu and Jian Yang and Tianyu Liu and Zhijie Deng",
        "title": "Aligning CodeLLMs with Direct Preference Optimization"
      },
      {
        "key": "gulcehre2023rest",
        "author": "Caglar Gulcehre and Tom Le Paine and Srivatsan Srinivasan and Ksenia Konyushkova and Lotte Weerts and Abhishek Sharma and Aditya Siddhant and Alex Ahern and Miaosen Wang and Chenjie Gu and Wolfgang Macherey and Arnaud Doucet and Orhan Firat and Nando de Freitas",
        "title": "Reinforced Self-Training (ReST) for Language Modeling"
      }
    ]
  }
]