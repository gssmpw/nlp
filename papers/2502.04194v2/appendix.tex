
\section{Parameter Distance}
We measure the L2 norm of model parameter difference between fine-tuned and  pre-trained checkpoints, as a signal of how much the distribution has drifted during SFT~\cite{information_geometry_applications,cover_thomas_information_theory}. We notice that training over well-matched distribution shifts the parameter less than training over those ill-matched. 

\begin{table}[h!]
    \centering
    \begin{tabular}{lccc}
        \toprule
        & \textbf{Mistral-7B-v0.3} & \textbf{Llama3.1-8B} & \textbf{Qwen2.5} \\
        \midrule
        \textbf{\name}  & 8.006 & 8.196 & 8.426 \\
        \textbf{Worst} & 8.029 & 8.202 & 8.467 \\
        \bottomrule
    \end{tabular}
    \caption{Performance comparison across different models}
    \label{tab:model_comparison}
\end{table}
% \section{Additional Results On Tulu3 / OLMO-v2 Alone.}

% To further break down the 

% \input{tables/tulu}
% \input{tables/olmo}


\section{Further Details On S2L}
\label{app:s2l_details}
This section details the experimental setup for two unsupervised data selection baseline, S2L applied to the joined pool of Tulu-3 and Olmo-v2 data. 

S2L, a state-of-the-art unsupervised data selection baseline, operates through two key steps: training a reference model to capture training dynamics and clustering the resulting trajectories to form a diverse, balanced subset of training data. The reference models used in our setup are specifically selected to enhance S2L’s performance, adhering to the theoretical underpinnings from the original paper that training dynamics remain consistent across models of varying sizes within the same family.

For our experiments, we train small reference models corresponding to the final target models. Specifically, we pair Llama-3.1-8B with Llama-3.2-1B, Qwen-2.5-7B with Qwen-2.5-0.5B, and Mistral-v0.3-7B with itself due to the lack of smaller models in the Mistral family. To minimize computational costs, LoRA is applied when training the Mistral reference model. \textbf{This choice of reference models are better compared to original S2L setup}, which employed a Pythia-70M proxy, thereby improving the fidelity of the selected subset.

Following S2L, the reference models are trained on a random 5\% subset of the dataset over four epochs. This reduced training requirement is justified by prior work, which demonstrates that only partial data is sufficient for the proxy model to learn meaningful training dynamics. During trajectory collection, we record the training loss of all examples at intervals of 500 iterations. The batch size and learning rate schedules are set as batch size of 128 and a learning rate warmup of 3\%, followed by a cosine decay to 2e-5.

We then perform K-means clustering using the Faiss library to efficiently partition the trajectory space into 100 clusters. The number of iterations is set to 20, and we use the Euclidean distance metric to ensure convergence to well-separated clusters. From each cluster, an equal number of examples are sampled to maintain a balanced subset distribution.

\section{Further Training Details}

We train our models on a 4-GPU Nvidia-GH200 node, with batch size 256 and micro batch size 2. 

\section{Further Ablations on UltraInteract. }
\input{tables/ablation}
\input{tables/self-distill-app}

\section{Additional Related Works On Model Dependent Data Selection Approaches}

\subsection{0. Notations}

\begin{enumerate}
    \item A training dataset $D = \{ x_i \}_{i=1}^N$ of size $N$; the final language model to be trained on the selected data $\theta$.
    \item We denote \textbf{the average cost of one forward pass} of model $\theta$ on a training example as $F_{\theta}$. As one backward pass is approximately the cost of two forward passes, \textbf{the average cost of one ``gradient pass''} (i.e., one forward + one backward) is thus $3F_{\theta}$.
    \item Another important source of computational cost in data selection comes from the training of additional models. We use $C(\theta, D, T)$ to denote the cost of training model $\theta$ on dataset $D$ for $T$ epochs (i.e., $N \cdot T$ examples are seen in total).
    \item Therefore, we unify the computational cost of most data selection approaches into two parts:
    \begin{enumerate}
        \item \textbf{The training of additional models}. For example, gradient-based influence requires training an additional model on part of the training dataset for $T$ epochs to obtain the checkpoints for gradient computation.
        \item \textbf{The computation of per-sample features}. For example, for each training example, gradient-based influence requires computing its gradient for each saved checkpoint, which means $T$ gradient passes are needed.
    \end{enumerate}
    \item Note that some algorithms may have additional computational costs other than the two parts above, such as clustering or a greedy algorithm for the final data selection. Since the two parts above constitute the majority of computation for almost all the data selection approaches, \textbf{we omit the other cost and only focus on these two.}
\end{enumerate}

\subsection*{1. TLDR: The Final Table}

For GRAPE, we assume that in the training dataset $D$, various responses to the same instruction are already available, thus no additional cost is incurred in the \textit{Response Collection} step of GRAPE. So the computational cost analysis of GRAPE under our framework is:

\begin{itemize}
    \item \textbf{Additional Training}: 0, as GRAPE directly evaluates data using the base model.
    \item \textbf{Per-sample conditional probability}: $NF_{\theta}$, as for a given target model $\theta$, we only need to compute conditional probability for each response (example) once.
\end{itemize}

The table below shows that our method, GRAPE, achieves superior performance with minimal computational cost compared with other model-based data selection approaches.

\begin{table}[h]
    \centering
    \begin{tabular}{@{}lcc@{}}
        \toprule
        & \textbf{Additional Training} & \textbf{Per-Sample Feature Computation} \\
        \midrule
        \textbf{GRAPE (ours)} & $0$ & $NF_{\theta}$ \\
        Gradient-based influence (LESS) & $C(\theta_{\text{lora}}, D_{\text{warmup}}, T)$ & $3T \cdot NF_{\theta}$ \\
        In-run gradient-based influence & $C(\theta, D, 1)$ & $0$ \\
        Gradient matching & $C(\theta_{\text{lora}}, D_{\text{warmup}}, T)$ & $3T \cdot NF_{\theta}$ \\
        Gradient norm & $m \cdot C(\theta, D, 1)$ & $3m \cdot NF_{\theta}$ \\
        Embedding-based & $0$ & $NF_{\theta}$ \\
        Simple uncertainty indicators & $0$ & $NF_{\theta}$ \\
        Perplexity & $C(\theta_{\text{ref}}, D_{\text{ref}}, 1)$ & $NF_{\theta_{\text{ref}}}$ \\
        Learnability & $C(\theta, D, 1)$ & $2 \cdot NF_{\theta}$ \\
        Loss trajectory (S2L) & $C(\theta_{\text{ref}}, D, T)$ & $T \cdot NF_{\theta_{\text{ref}}}$ \\
        \bottomrule
    \end{tabular}
    \caption{Computational cost comparison of data selection methods.}
\end{table}

\section*{2. Gradient-based Methods}

Gradients have long been an important source of information for training data selection, as they directly affect the whole optimization process of language models. Three kinds of model-based gradient-based data selection approaches have been proposed:

\begin{enumerate}
    \item Gradient-based influence
    \item Gradient matching
    \item Gradient norm
\end{enumerate}

\subsubsection*{2.1 Gradient-based Influence}

Gradient-based influence computes the pairwise influence scores between each pair of training and validation examples. Training data with the highest influence are selected, as training on them leads to the theoretically largest decrease in model loss on validation data. LESS \cite{xia2024less} formulates the pairwise influence scores as the cosine similarity between the gradients of training and validation data, and computes these gradient features using the following two steps:

\begin{enumerate}
    \item LoRA-train the final model on part of the whole training dataset, denoted as $D_{\text{warmup}}$, for $T$ epochs, and save the $T$ model checkpoints.
    \item For each data point, compute its LoRA gradient with each of the $T$ checkpoints, and later aggregate these $T$ gradients together in the cosine similarity expression.
\end{enumerate}

Therefore, the computational cost of gradient-based influence is:

\begin{itemize}
    \item \textbf{Additional training}: $C(\theta_{\text{lora}}, D_{\text{warmup}}, T)$.
    \item \textbf{Per-sample gradient for each checkpoint}: $NT \cdot 3F_{\theta} = 3 T \cdot NF_{\theta}$.
\end{itemize}

In order to reduce the cost incurred by per-sample gradient computation, recent work has developed \textit{in-run gradient-based influence} that directly computes the dot product between gradients without the need for separate gradient computations. However, this approach incorporates the dot product computations into the standard training process, which means in order to obtain pairwise influence scores for \textbf{the whole training set}, \textbf{a full training run} has to be done on all the training data. This incurs inefficiency when we do not actually need full dataset training. Moreover, the pairwise scores here only show the model’s \textbf{``dynamic preference''}: scores computed at the $t$-th iteration only reflect the model’s preference at this specific iteration. It is not theoretically guaranteed that these scores reflect the model’s preference from the beginning of training. Thus, the cost of in-run gradient-based influence is:

\begin{itemize}
    \item \textbf{Additional Training}: $C(\theta, D, 1)$.
    \item \textbf{Per-sample gradient}: 0.
\end{itemize}

\subsubsection*{2.2 Gradient Matching}

Gradient matching also requires per-sample gradients, but utilizes their information in a different way. It performs clustering based on these gradient features to group similar data, and then applies an iterative greedy selection algorithm. \textbf{In order to scale to LLM-level gradient computation and clustering, TAGCOS \cite{TAGCOS} completely follows the warmup training and gradient computation pipeline of LESS \cite{LESS}.} As the computational bottleneck here is still the gradient computation instead of clustering or iterative selection, \cite{TAGCOS} also shares the same computational cost as \cite{xia2024less}:

\begin{itemize}
    \item \textbf{Additional training}: $C(\theta_{\text{lora}}, D_{\text{warmup}}, T)$.
    \item \textbf{Per-sample gradient for each checkpoint}: $NT \cdot 3F_{\theta} = 3 T \cdot NF_{\theta}$.
\end{itemize}

\subsubsection*{2.3 Gradient Norm}

The $L_2$-norms of gradient vectors can also serve as effective indicators for data selection. \cite{GraNd} proposes GraNd, which obtains a utility score for each training point based on its gradient norm early in the training. More specifically, it starts from $m$ different model weight initializations, trains each model on the whole dataset to obtain per-sample gradient norms, and finally averages the $m$ gradient norms for each training point to obtain the final GraNd score. Therefore, the computational cost of GraNd is shown below:

\begin{itemize}
    \item \textbf{Additional training}: $m \cdot C(\theta, D, 1)$.
    \item \textbf{Per-sample gradient for each weight initialization}: $Nm \cdot 3F_{\theta} = 3m \cdot NF_{\theta}$.
\end{itemize}

\subsection*{3. Embedding-based Methods}

Embedding-based methods project the whole training set into an embedding space to quantify the information of each data point and their interactions. For model-based embedding-based selection methods, the embeddings are usually computed by the final model $\theta$ to align with its preference.

Under a supervised data selection setup where validation data representing target task distributions are available, \textbf{Representation-based Data Selection} (RDS; \cite{RDS1, RDS2}) computes the embedding similarity between training and validation data, and selects training points that are most similar to the target distribution in the embedding space.

For an unsupervised setup where only the embeddings of training data are accessible, \textbf{geometry-based coreset sampling} methods are widely used \cite{Coreset}. Grounded on the intuition that close samples in the embedding space often share similar properties, a \textbf{diverse} subset can be obtained by controlling the minimum distance between any two selected data points. Among them, using K-center greedy sampling to select embedding-based \textbf{facility locations} has been proven especially effective for instruction fine-tuning of LLMs \cite{FacilityLocations}.

These embedding-based approaches share similar computational costs: they do not need any additional model training and can directly extract useful per-sample embeddings using the last-layer hidden states of the pretrained final model $\theta$. Thus, their computational cost is shown below:

\begin{itemize}
    \item \textbf{Additional training}: 0.
    \item \textbf{Per-sample embedding computation}: $NF_{\theta}$.
\end{itemize}

\subsection*{4. LogProb-based Methods}

LogProb-based methods also directly utilize the target LLM to evaluate the utility of each training data point.

\subsubsection*{4.1 Simple Uncertainty-based Indicators}

Some simple model-based indicators inspired by the notion of uncertainty have been shown effective for a long time and recently extended to data selection for LLM instruction tuning \cite{Uncertainty1, Uncertainty2}. \cite{Uncertainty2} demonstrates the effectiveness of various indicators including \textbf{mean entropy, least confidence, mean margin}, etc. These simple indicators do not require additional training and can also be directly obtained with the pretrained final model $\theta$. Their computational cost is shown below:

\begin{itemize}
    \item \textbf{Additional training}: 0.
    \item \textbf{Per-sample per-token logits computation}: $NF_{\theta}$.
\end{itemize}

\subsubsection*{4.2 Perplexity (PPL)}

PPL is also a long-standing data selector and has been shown effective for LLM-scale data selection. Typically, a split of the training dataset, $D_{\text{ref}}$, is needed to train $\theta_{\text{ref}}$, a reference model that will be used to compute PPL for the whole training set.

A common approach is to use the final model $\theta$ as the reference model $\theta_{\text{ref}}$ to ensure the alignment in PPL patterns \cite{PPL1}, but prior work \cite{ankner2024perplexed} also shows that a reference model much smaller than the final model can also be an effective PPL-based data selector. The computational cost for PPL-based selection is shown below:

\begin{itemize}
    \item \textbf{Additional training}: $C(\theta_{\text{ref}}, D_{\text{ref}}, 1)$.
    \item \textbf{Per-sample PPL computation}: $NF_{\theta_{\text{ref}}}$.
\end{itemize}

\subsubsection*{4.3 Learnability}

In addition, \textbf{learnability \cite{Learnability1, Learnability2, Learnability3}} is a more effective metric than pure uncertainty or PPL, as it excludes uncertain but unlearnable points (e.g., noisy or less task-relevant) by considering the decrease in per-sample loss before and after the model is fully trained. More specifically, it trains the final model $\theta$ on the full training dataset to obtain a strong reference model $\theta_{\text{ref}}$, and then computes the difference of loss on each training example between $\theta$ and $\theta_{\text{ref}}$. In this way, it requires two forward passes for per-sample computation:

\begin{itemize}
    \item \textbf{Additional training}: $C(\theta, D, 1)$.
    \item \textbf{Per-sample learnability computation}: $2 \cdot NF_{\theta}$.
\end{itemize}

\subsubsection*{4.4 Loss Trajectory}

Moreover, logprob-based methods can also obtain finer-grained information from the \textbf{training dynamics} of LLMs. S2L \cite{yang2024s2l} obtains a feature vector for \textbf{each} training point by collecting their \textbf{training loss trajectories} over $T$-epoch training on a small reference model $\theta_{\text{ref}}$, and then applies K-means clustering to equally sample data points from each trajectory cluster. Prior work shows its superiority over other logprob-based indicators, but it also comes with significant computational cost:

\begin{itemize}
    \item \textbf{Additional training}: $C(\theta_{\text{ref}}, D, T)$. Here the choice of $\theta_{\text{ref}}$ is especially important, as prior work \cite{S2LRef} shows that reference models that come from the same model family as the final model tend to have similar loss trajectories of training data, so they can preserve more fidelity in their loss trajectory patterns.
    \item \textbf{Per-sample loss trajectory computation}: $T \cdot NF_{\theta_{\text{ref}}}$. Note that $T$ here is typically much larger than that in gradient-based influence computation, so the computational cost of this gradient-free approach can be even higher than gradient-based methods.
\end{itemize}
