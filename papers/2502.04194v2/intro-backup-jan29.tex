% \section{Introduction}
% Instruction tuning has emerged as a pivotal technique for enhancing large language models (LLMs) by aligning them with human intent through task-specific datasets. By fine-tuning on instruction-response pairs, this approach has unlocked remarkable generalization across unseen tasks, making LLMs versatile in real-world applications. 

% Crafting effective instruction-tuning datasets is a non-trivial challenge. Within this domain, up-scaling instruction tuning data by synthesizing more question-answer pairs with language models has been a predominant direction for improving SFT'ed model's performance. But this benefit will not extend forever. Research suggests that models trained on increasingly large collections is likely to stop improving or even starting to decline~\cite{alemohammad2024selfconsuming,gerstgrasser2024iscollapseinevitable,dohmatob2024strongmodelcollapse,shumailov2023curse,Hataya_2023_ICCV, martínez2023combining, martínez2023understanding, bohacek2023nepotistically, briesch2023large} because of the diversity decrease~\cite{padmakumar2024writing, guo2023curious} or distribution distortion~\cite{lebrun2021evaluating}.


% Therefore, effective instruction tuning requires more than expansion of training data size, it also demands a careful introspection into the quality of data, and its utility with respect to a base model~\cite{li2024quantitytoqulality,du2023modsmodelorienteddataselection}. 

% Prior studies have tailored instruction-tuning questions to the characteristics of the base model using approaches like curriculum learning~\cite{zhao2024preliminarystudyintrinsicrelationship,lee2024instructiontuninghumancurriculum,feng2023citinglargelanguagemodels} and targeted instruction expansion~\cite{du2023mods}. While these works emphasize aligning the training data with the model's capacity, or ``talent'', we argue that equal attention should be given to the model's inherent behavioral patterns — its ``tone''. 

% Current datasets often overlooks this aspect and prepares model-agnostic solutions, using solutions generated by more powerful models that could be very exotic to the model one wants to train.

% Moreover, the widely-adopted practice of pairing prompts with multiple responses for data augmentation as done in ~\citet{yu2024metamath,yuan2024eurus} may make the problem of distributional mismatches and spurious correlations even more pronounced, leading to diminishing returns or even performance degradation~\cite{li2024mugglemath,chan2024balancingcosteffectivenesssynthetic,setlur2024rlincorrectsyntheticdata}. 

% These issues highlight the need for targeted response selection strategies that align with the model’s inherent tendencies.

% Inspired by the success of on-policy learning in reinforcement learning and preference alignment~\cite{tajwar2024shoulduse,guo2024onlineaifeedback,setlur2024rlincorrectsyntheticdata,liu2024provablymitigatingoveroptimizationrlhf,chen2024optuneefficientonlinepreference}, we propose a approach to instruction tuning that tailors responses for each model's individual characteristics. Specifically, we hypothesize that responses closer to the base model's current distribution provide more effective training signals, aligning with the model's strengths while reducing the risks of distributional mismatches. By incorporating this distribution-aware perspective into dataset curation, we aim to unlock the untapped potential of aligning instruction data with the base model's unique characteristics.

% To realize this vision, we introduce a novel framework for response selection called \textbf{\name}, which stands for \textbf{G}enerating various candidate responses, \textbf{R}anking them based on their \textbf{A}lignment with the base model's \textbf{P}olicy, and leveraging this ranking to \textbf{E}nhance supervised fine-tuning performance.

% \name first samples solutions from a variety of strong models to from a diverse collection of responses, then ranks these responses based on their likelihood in the base model's distribution and take those that closely aligns with the base model to perform supervised fine-tuning. It ensures a good approximation of the distribution of ``golden'' answers while customizing for each base model to avoid drastic distribution shifts. 

% We validate our approach through comprehensive experiments, demonstrating consistent performance gains across diverse LLM architectures and tasks. Despite its simplicity, the framework achieves substantial improvements in generalization and robustness, offering a scalable solution to the challenges of instruction tuning.

% Through this work, we propose a shift from dataset scaling or feeding the same solutions to every model, to a more tailored, model-aware data curation on the response side. Our findings not only enhance the efficiency of instruction tuning but also pave the way for a deeper understanding of how LLMs interact with their training data, driving the development of more adaptable and high-performing models.
\section{Introduction}

\hao{this first paragraph is perhaps too well-known to have in the intro. i'd drop:}
% Supervised fine-tuning (SFT) has become a cornerstone for unlocking the full potential of large language models (LLMs), enabling them to generalize across diverse, unseen tasks\hao{this is a bit too strong. i'd say eliciting the capabilities of following instruction and solving various tasks}. By fine-tuning on instruction-response pairs, this process bridges the gap between pretrained models and their ability to align with human intent. 
Supervised fine-tuning (SFT) has become a cornerstone for unlocking the full potential of large language models (LLMs). However, crafting high-quality instruction datasets that effectively enhance model capabilities remains a significant challenge.

High-quality, large-scale supervised data is crucial for supervised instruction finetuning (SFT).s 


\hao{I added:
High-quality, large-scale supervised data is crucial for supervised instruction finetuning (SFT).s
}
It has become a well-established approach to collect SFT data by pairing each instruction with responses sampled from multiple LLMs that are usually stronger than the target LLM to be finetuned.\hao{check and cite}
\hao{maybe add a minor note about quality control through, e.g., rejection sampling}
% synthesizing more question-response pairs using stronger LLMs. 
% Recent studies reveal
\hao{this following sent is pretty vague. i;d drop}
While this has shown early promise, recent studies reveal a plateau effect, where performance stagnates or even declines as dataset size increases. 
This phenomenon arises due to issues such as diminishing diversity~\cite{padmakumar2024writing, guo2023curious} and distortion in the data distribution~\cite{lebrun2021evaluating}, which ultimately undermine the base model's performance and robustness~\cite{alemohammad2024selfconsuming, gerstgrasser2024iscollapseinevitable, shumailov2023curse,dohmatob2024strongmodelcollapse,Hataya_2023_ICCV,martínez2023combining,martínez2023understanding,bohacek2023nepotistically,briesch2023large}.



\hao{if we had addressed the comments in the paragraph above (that they are used to scale up the SFT data wjan2hich often results in issues), this would have been a nice transition.
}
Thus, effective instruction tuning often requires more than scaling up the data; 
it often needs  ``tailoring'' the data to the unique characteristics of the target model, which has been studied in the context of curriculum \hao{check}learning~\cite{zhao2024preliminarystudyintrinsicrelationship, lee2024instructiontuninghumancurriculum,feng2023citinglargelanguagemodels,setlur2024rlincorrectsyntheticdata} and \hao{we should clarify how this one differs from \name somewhere:}targeted data curation~\cite{du2023modsmodelorienteddataselection}.
% , aim to adapt instruction tuning to the model's capacity. 
\hao{i didn't get the `talent' vs. `tone' thing below. i doubt it that we actually need this sentence:}
While these approaches consider the model's strengths or ``talent,'' they often overlook the importance of aligning responses with the base model’s inherent behavioral tendencies or ``tone.'' 
Meanwhile, the principle of tailoring the data to the model has been a crucial ingredient for the later phases of LLM development, in the form of on-policy preference learning\hao{cite}, and on-policy/online reinforcement learning (RL)\hao{cite}.
% the importance of ensuring a closer alignment between training data and the behavior policy (of the model to be trained) has been extensively highlighted and utilized in research on reinforcement learning and preference learning
~\cite{xu2024dposuperiorppollm,tajwar2024shoulduse,zhang2024selfexploringlanguagemodelsactive,guo2024onlineaifeedback,liu2024provablymitigatingoveroptimizationrlhf,zhang2024textbfplumimprovingcodelms,miao2024aligningcodellmsdirectpreference,gulcehre2023rest,azar2023generaltheoreticalparadigmunderstand,tang2024understandingperformancegap,zhuang2023bpo,zhou2024wpoenhancingrlhfweighted}.

% This mismatch becomes particularly pronounced when responses are sampled from a very different distribution than the model's base policy, which could lead sub-optimal performance~\cite{li2024mugglemath, chan2024balancingcosteffectivenesssynthetic}. 

% On the other hand 

% To address these challenges, we propose a novel SFT response engineering framework, \name, designed to optimize instruction tuning by tailoring response selection to each base model's distribution. Unlike existing datasets that usually contains one-size-fits-all solutions sampled from one single best model without customization~\cite{yu2024metamath, yuan2024eurus,slimorca,OpenHermes}, \name curates responses that align closely with the base model's pretrained policy. By leveraging a distribution-aware approach, \name mitigates the risks of distribution drift and spurious correlations, while ensuring coverage of optimal solutions.
\hao{i reordered things below}
Inspired by these insights,
we hypothesize that SFT can similarly benefit from aligning data with the model,
the core idea behind \name.
For each instruction, \name {\bf g}athers and selects {\bf r}esponses from various LLMs,
and selects one that is closest to the target model's {\bf p}r\textbf{e}trained distribution.
This is achieved by calculating the perplexity of each response using the target model and selects the one with the lowest perplexity. 
% improved alignment between the base model's distribution and the distribution of the responses in the training data. 
% We propose \name which {\bf g}athers diverse {\bf r}esponses for each instruction from various sources and score them based on how well they {\bf a}lign with the base LLM’s {\bf p}r\textbf{e}trained distribution. 
% Thus, \name offers a surprisingly simple and efficient approach: it calculates the perplexity of each candidate response for a given instruction using the base model and selects the response with the lowest perplexity. 
Unlike existing datasets that usually contains one-size-fits-all responses for each instruction without customization~\cite{yu2024metamath, yuan2024eurus,slimorca,OpenHermes}, \name curates model-dependent SFT datasets that better mitigates the risks of distribution shift \hao{why does it fix the following issues?:} and spurious correlations, while posing minimum overhead for dataset collation. 

% By leveraging a distribution-aware approach, \name mitigates the risks of distribution drift and spurious correlations, while ensuring coverage of optimal solutions.

% Specifically, \name follows a two-step process: (1) collecting diverse responses from various strong LLMs to build a comprehensive candidate pool, and (2) ranking and selecting responses based on their likelihood under the base model’s distribution. This approach balances diversity and alignment, approximating the ``golden'' distribution while customizing the training data for the base model’s unique tendencies.

\name is simple and effective, and adds little overhead to the SFT process.
To \name's advantage, many existing datasets share overlapping instructions but contain different high-quality responses. One can therefore directly select, for each model, the best fit(s) among the off-the-shelf responses without needing to produce new responses. 
We first validate the  extensive controlled experiments on chain-of-thought reasoning scenario and demonstrate 
Through extensive experiments on stepwise reasoning tasks, we demonstrate the efficacy of \name. Notably, models fine-tuned with \name-selected responses outperform those trained on 3-times datasets and responses from the strongest models among all. This underscores the critical role of tailored data curation in optimizing SFT performance.

\todo{Talk about data selection set-up}

\hao{don't think we need this:}
We advocate for a model-aware approach to instruction tuning that tailors response selection to the base model. By shifting the focus from dataset scaling to strategic data alignment, \name advances the understanding of LLM training dynamics and sets a new benchmark for scalable, high-performance instruction tuning.
