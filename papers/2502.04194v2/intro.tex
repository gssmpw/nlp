
\section{Introduction}

% Supervised fine-tuning (SFT) has become a cornerstone for unlocking the full potential of large language models (LLMs), enabling them to generalize across diverse, unseen tasks\hao{this is a bit too strong. i'd say eliciting the capabilities of following instruction and solving various tasks}. By fine-tuning on instruction-response pairs, this process bridges the gap between pretrained models and their ability to align with human intent. 
% Supervised fine-tuning (SFT) has become a cornerstone for unlocking the full potential of large language models (LLMs)\hao{cite}. However, crafting high-quality instruction datasets that effectively enhance model capabilities remains a significant challenge.

High-quality, large-scale supervised data is crucial for supervised instruction finetuning (SFT;~\citealp{Dolly,openassist,zhao2024wildchat,zheng2024lmsyschatm}). A common practice of data collection involves sampling responses from strong language models, predominantly focusing on expanding the size of the dataset and improving the overall quality of the responses~\cite{sun2024principle, alpaca, wang-etal-2023-self-instruct, xu2024magpie, chen2024genqa}. However, recent research suggests that there is more complex dynamics involved~\cite{xu2024strongermodelsstrongerteachers}. A plateau effect in synthetic data scaling, where performance either stagnates or even declines as the size of the synthetic data increases beyond a certain point, has been widely observed.
This phenomenon arises due to issues such as diminishing diversity~\cite{padmakumar2024writing, guo2023curious} and distortion in the data distribution~\cite{lebrun2021evaluating}, which ultimately undermine the base model's performance and robustness~\cite{alemohammad2024selfconsuming, gerstgrasser2024iscollapseinevitable, shumailov2023curse,dohmatob2024strongmodelcollapse,Hataya_2023_ICCV,martínez2023combining,martínez2023understanding,bohacek2023nepotistically,briesch2023large}.


% \hao{I added:
% High-quality, large-scale supervised data is crucial for supervised instruction finetuning (SFT).s
% }
% It has become a well-established approach to collect SFT data by pairing each instruction with responses sampled from multiple LLMs that are usually stronger than the target LLM to be finetuned.\hao{check and cite}
% \hao{maybe add a minor note about quality control through, e.g., rejection sampling}
% synthesizing more question-response pairs using stronger LLMs. 
% Recent studies reveal
% \hao{this following sent is pretty vague. i;d drop}
% While this has shown early promise, recent studies reveal a plateau effect, where performance stagnates or even declines as dataset size increases. 
% This phenomenon arises due to issues such as diminishing diversity~\cite{padmakumar2024writing, guo2023curious} and distortion in the data distribution~\cite{lebrun2021evaluating}, which ultimately undermine the base model's performance and robustness~\cite{alemohammad2024selfconsuming, gerstgrasser2024iscollapseinevitable, shumailov2023curse,dohmatob2024strongmodelcollapse,Hataya_2023_ICCV,martínez2023combining,martínez2023understanding,bohacek2023nepotistically,briesch2023large}.


Thus, effective instruction tuning requires more than scaling up the data; 
it often needs  ``tailoring'' the data to the unique characteristics of the target model. Existing works focus on enhancing the model’s existing knowledge and capabilities~\cite{du2023modsmodelorienteddataselection} and optimizing the curriculum progression for instruction tuning \cite{zhao2024preliminarystudyintrinsicrelationship, lee2024instructiontuninghumancurriculum,feng2023citinglargelanguagemodels,setlur2024rlincorrectsyntheticdata}.
They typically prescribe questions for each model to learn solutions. However, it remains elusive which responses best suit a given base model for fine-tuning.
Meanwhile, tailoring the responses to the model has been a crucial ingredient for the success of later phases of LLM development, particularly through on-policy preference learning~\cite{tajwar2024shoulduse,zhang2024selfexploringlanguagemodelsactive,zhang2024textbfplumimprovingcodelms,miao2024aligningcodellmsdirectpreference,gulcehre2023rest,azar2023generaltheoreticalparadigmunderstand,tang2024understandingperformancegap,zhuang2023bpo}, and on-policy/online reinforcement learning~\cite{guo2024onlineaifeedback,liu2024provablymitigatingoveroptimizationrlhf,zhou2024wpoenhancingrlhfweighted}.

% , aim to adapt instruction tuning to the model's capacity. 
% \hao{i didn't get the `talent' vs. `tone' thing below. i doubt it that we actually need this sentence:}
% While these approaches consider the model's strengths or ``talent,'' they often overlook the importance of aligning responses with the base model’s inherent behavioral tendencies or ``tone.'' 
% Meanwhile, the principle of tailoring the data to the model has been a crucial ingredient for the later phases of LLM development, in the form of on-policy preference learning~\cite{tajwar2024shoulduse,zhang2024selfexploringlanguagemodelsactive,zhang2024textbfplumimprovingcodelms,miao2024aligningcodellmsdirectpreference,gulcehre2023rest,azar2023generaltheoreticalparadigmunderstand,tang2024understandingperformancegap,zhuang2023bpo,}, and on-policy/online reinforcement learning (RL)~\cite{guo2024onlineaifeedback,liu2024provablymitigatingoveroptimizationrlhf,zhou2024wpoenhancingrlhfweighted}.
% the importance of ensuring a closer alignment between training data and the behavior policy (of the model to be trained) has been extensively highlighted and utilized in research on reinforcement learning and preference learning

% This mismatch becomes particularly pronounced when responses are sampled from a very different distribution than the model's base policy, which could lead sub-optimal performance~\cite{li2024mugglemath, chan2024balancingcosteffectivenesssynthetic}. 

% On the other hand 

% To address these challenges, we propose a novel SFT response engineering framework, \name, designed to optimize instruction tuning by tailoring response selection to each base model's distribution. Unlike existing datasets that usually contains one-size-fits-all solutions sampled from one single best model without customization~\cite{yu2024metamath, yuan2024eurus,slimorca,OpenHermes}, \name curates responses that align closely with the base model's pretrained policy. By leveraging a distribution-aware approach, \name mitigates the risks of distribution drift and spurious correlations, while ensuring coverage of optimal solutions.
Inspired by these insights,
we hypothesize that SFT can similarly benefit from aligning data with the model,
the core idea behind \name.
For each instruction, \name gathers and selects response(s) from various sources that are closest to the target model's  pretrained distribution. 
This is achieved by calculating the probability of each response using the target model and selects the one with the highest probability(\S\ref{sec:methodology}).After obtaining these in-distribution responses, one can proceed with standard SFT without any modification to the training.
% improved alignment between the base model's distribution and the distribution of the responses in the training data. 
% We propose \name which {\bf g}athers diverse {\bf r}esponses for each instruction from various sources and score them based on how well they {\bf a}lign with the base LLM’s {\bf p}r\textbf{e}trained distribution. 
% Thus, \name offers a surprisingly simple and efficient approach: it calculates the perplexity of each candidate response for a given instruction using the base model and selects the response with the lowest perplexity. 
Unlike existing datasets that usually contains one-size-fits-all responses for each instruction without customization~\cite{yu2024metamath, yuan2024eurus,slimorca,OpenHermes}, \name curates model-dependent SFT datasets that better matches the base model's distribution, better mitigating the risks associated with distribution shift like spurious correlations and catastrophic forgetting, while posing minimum overhead.

% Specifically, \name follows a two-step process: (1) collecting diverse responses from various strong LLMs to build a comprehensive candidate pool, and (2) ranking and selecting responses based on their likelihood under the base model’s distribution. This approach balances diversity and alignment, approximating the ``golden'' distribution while customizing the training data for the base model’s unique tendencies.

To \name's advantage, many existing datasets share overlapping instructions but contain different high-quality responses, e.g.,
the instructions in Flan~\cite{longpre2023flan}, GSM-8K~\cite{cobbe2021trainingverifierssolvemath}, MATH~\cite{hendrycks2021measuringmathematicalproblemsolving},
and the post-training recipes that re-use SFT instructions for preference learning~\cite{lambert2024tulu3,olmo2025}.
Therefore, \name can directly select, for each model, the best fit(s) among the off-the-shelf responses without producing new responses, which we do in the experiments(\S~\ref{sec:tulu_olmo}).

We first validate our hypothesis through extensive controlled experiments on a reasoning dataset - UltraInteract-SFT~\cite{yuan2024eurus} and demonstrate the importance of supervising base models with in-distribution responses(\S\ref{sec:experiments_ultra}). We experimented on 4 popular pretrained LMs from Mistral~\cite{mistralai_codestral_2024}, Llama3.1~\cite{dubey2024llama3herdmodels} and Qwen2.5~\cite{hui2024qwen25} families. 
Notably, models fine-tuned with \name-selected responses outperform those trained on a 3$\times$ larger datasets up to $17.3\%$ absolute gain on average performances, and responses from the strongest model under consideration - \textsc{Llama3.1-405B-Instruct} by significant levels. 
We then experiment in realistic scenarios, collecting responses from post-training data for Tulu3 and Olmo-v2. We demonstrate the effectiveness of \name by outperforming training on all these available data that is 4.5 times the size of data curated by \name by up to 6.1\%.  Remarkably, \name allows finetuning a Llama3.1-8B base model to exceed the performance of Tulu-8B-SFT using 1/3 data and half number of epochs.

Our results prove \name as a simple but effective and scalable approach to improving supervised fine-tuning by aligning training distribution and that of the pre-trained checkpoint. 

% \hao{don't think we need this:}\dylan{removed}
% We advocate for a model-aware approach to instruction tuning that tailors response selection to the base model. By shifting the focus from dataset scaling to strategic data alignment, \name advances the understanding of LLM training dynamics and sets a new benchmark for scalable, high-performance instruction tuning.
