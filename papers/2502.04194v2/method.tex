\section{Methodology}
% \hao{since our method section is relative thin, i'd include some discussion on some of the design choices of \name, such as the use of PPL, top-1 vs. top-K, the overhead, or whatever is interesting
% }
\label{sec:methodology}
\begin{figure*}
    \centering
    \includegraphics[width=.76\linewidth]{figures/icml-main.pdf}
    \caption{An overview of \name. \name takes multiple off-the-shelf existing datasets and optionally generates new responses, finds overlapping instructions with multiple different responses and selects responses that align with the base model's distribution. This  dataset is then used for standard supervised fine-tuning.}
    \label{fig:overview}
\end{figure*}
% \hao{this section reads disconnected from the prev motivation section.
% would be nice to add some discussion how the above idea is reflected in the design of \name
% }
We introduce \name, a surprisingly simple yet effective methodology to enhance supervised fine-tuning (SFT) by customizing the training data for the base model. 
The key idea is to find responses among a candidate pool for each instruction \(x_i\) such that they align closely with the base model's pretrained distribution \(\pi_{\theta_0}\). 

As diagrammed in Figure~\ref{fig:overview}, \name consists of two main steps, followed by standard SFT:
\begin{enumerate}
    \item \textbf{Response Collection (\S\ref{sec:method_response_collection})} Collect a pool of high-quality candidate responses either from existing datasets or sampling from multiple LLMs.
    \item \textbf{Customization (\S\ref{sec:method_customization}):} For the target model to be finetuned \(\pi_{\theta_0}\), find the response(s), for each instruction, that are closest to the pretrained distribution of \(\pi_{\theta_0}\).
    % highest likelihood under the model's distribution. 
\end{enumerate}

% \hao{overhead needs to be discused}
% \hao{this one doesn't say much. i'd drop it and say something about \name does not modify the training}

\subsection{Collecting Responses from Existing Resources}
\label{sec:method_response_collection}
% \hao{i'd expand this discussion a bit.
% several points to cover: in open sft datasets, instructions have always been harder to collect than responses.
% as a result the same set of instructions have been used to collect different SFT datasets,
% and it would be great to give some concrete examples of this (maybe SFT data A includes code data X in its mixture, and so does SFT data B). some concrete percentage of overlapping among popular SFT datasets would be more convincing, or even a list/table.
% also, we should highlight that instructions are paired with multiple responses to scale up the data (seveal example) or for different training phases ( i'd bring up TULU/OLMO data we used).
% people also do this for different model families and generations.
% this ends up having a natural situation that one instruction with multiple responses become readily available resources
% %since different groups have different preferences
% }
For instruction-tuning of language models, high-quality instructions are more difficult to collect than responses~\cite{xu2024magpie,liu2024best}. 
Therefore, it is a common practice to reuse existing instruction-tuning prompts while generating diverse responses using various methods tailored to specific requirements. For instance, instructions from Flan~\cite{longpre2023flan}, OpenOrca~\cite{OpenOrca}, ShareGPT~\cite{vicuna2023}, and the training splits of GSM-8K~\cite{cobbe2021trainingverifierssolvemath}, MATH~\cite{hendrycks2021measuringmathematicalproblemsolving}, and CodeContests~\cite{alphacode} are frequently reused in datasets like Olmo~\cite{olmo2025}, Tulu~\cite{lambert2024tulu3}, OpenHermes~\cite{OpenHermes}, OpenOrca~\cite{OpenOrca}, MetaMath~\cite{yu2024metamath}, MathInstruct~\cite{yue2023mammoth}, UltraFeedback~\cite{cui2024ultrafeedback}, and UltraInteract~\cite{yuan2024eurus}, whether for SFT or preference learning. The solutions are generated using different models or follow varying styles depending on the specific needs. 
This naturally leads to a situation where a single instruction with multiple responses becomes a readily available resource. \name therefore leverages these pre-existing response candidates to tailor training dataset that better aligns with the base model's distribution. 
When such resources are unavailable or insufficient, practitioners can generate new responses and apply \name.
% \hao{maybe add here that when these resources are not readily available or do not meet the need for the practitioners, one can sampl new solutions, as we explore in the experiments}
% \hao{we shoul also emphasize that even when this setting, \name outperforms using all data}

Responses for the $i$-th instruction are collected from various datasets to form $\mathcal{R}_i =\{y{_i^j} : j = 1, \dots, J^{(i)}\}$, where $J^{(i)}$ denotes the number of responses collected for the $i$-th instruction.
\subsection{Customize Dataset For Models}
\label{sec:method_customization}
We then compute the conditional probability of each response \(\pi_{\theta_0}(y{_i^j}\mid x_i)\). We rank the responses based on the conditional probability and take those with the \textbf{highest} probability for each instruction.
% the most likely one(s). 
% We compute the perplexity for each $y$ 
% \begin{equation}
% \mathcal{R}_i^{\theta_0} = \operatorname{TopK}_{y_{ij} \in \tilde{\mathcal{R}}_i} \pi_{\theta_0}(y_{ij} \mid x_i),
% \end{equation}
% \[
% \text{PPL}(y^{ij}|x^i) = \exp\left(-\frac{1}{N} \sum_{t=1}^N \log \pi_{\theta_0}(y{_t^{ij}} \mid x^i, y{_{<t}^{ij}})\right)
% \]
% , 
% \begin{equation}
% \begin{align}
% \mathcal{R}_i^{\theta_0} = \operatorname{TopK}_{y_i^j \in \mathcal{R}_i} \pi_{\theta_0}(y_i^j \mid x^i)
% \end{align}
% % \end{equation}
% \hao{didn't understand this. i thought $K$ should be a hyperparam?}where \(K\) is the original number of responses associated with \(x_i\) in the dataset. 


% This ensures that the selected responses are both valid and well-aligned with the base model's distribution. 
% The dataset customized to \(\pi_{\theta_0}\) is then 
% $\{(x_i, y_i^j) \mid y_i^j\in\mathcal{R}_i\}$.

% \name operates can entirely on existing data collected from the web and does not require generating new samples. 
Notably, \name's selection process requires only a forward pass through the candidates and does not require gradient computation.
Its overhead is substantially lower than many model-based data selection algorithms~\cite{xia2024less,yang2024s2l,liu2024deita,zhao2021datasetcondensationgradientmatching,zhang2024tagcostaskagnosticgradientclustered,pan2024scalebioscalablebileveloptimization}.

% , \name requires only a single forward pass over examples and does not require gradient computation, making it way more efficient. This features \name as a highly efficient and scalable approach. 

Additionally, it is important to distinguish \name from the other preplexity-based data selection and curriculum planning methods~\cite{wu2024curriculumlearningqualitydrivendata,li2024superfilteringweaktostrongdatafiltering,liu2024letslearnstepstep}. Existing approaches focus on selecting \textbf{instructions} by using perplexity as a difficulty measure, which differs from \name that uses probability to select for each instruction in a fixed instruction set, responses that better matches with the base model's distribution. Our experiments in \S\ref{sec:tulu_olmo} demonstrate that low-probability responses with fixed set of instructions are detrimental to performance, further emphasizing the fundamental difference in the two processes. 

% ~\cite{ankner2024perplexed,antonello2022selectinginformativecontextsimproves,yin2024computeconstraineddataselection,lalor2020dynamicdataselectioncurriculum}. Under post-training context, these approaches typically concern with the 

% \subsection{Fine-Tuning on Refined Data}
% The refined dataset \(\tilde{\mathcal{D}}_{\theta_0}\) is used for supervised fine-tuning, optimizing the negative log-likelihood:
% \begin{equation}
% \mathcal{L}_{\text{SFT}} = \frac{1}{|\tilde{\mathcal{D}}_{\theta_0}|} \sum_{(q_i, \mathcal{R}_i^{\theta_0}) \in \tilde{\mathcal{D}}_{\theta_0}} \sum_{r_{ij} \in \mathcal{R}_i^{\theta_0}} -\log \pi_{\theta}(r_{ij} \mid q_i).
% \end{equation}
% This step ensures that the base model adapts to high-quality responses while retaining coherence with its original preferences.
