\section{Preliminary Experiments}
\label{sec:experiments_ultra}
In this section, we seek to empirically verify the hypothesis in \S\ref{sec:motivation}. To evaluate it under a controlled set-up, we consider using instructions from UltraInteract-SFT, and constrain within chain-of-thought reasoning (predominantly for coding, logic and math.) scenario. This allows us to control confounding factors like style and format while ensuring manageable data quality control during data collection. The insights from this experiment will inform the later more realistic setting in \S\ref{sec:tulu_olmo}.

% controlled but realistic setting 
% step-wise reasoning --> control the format, easy to verify results
% realistically plausible: math / coding. 
% generate data on our own -> control for other confounding factors 
% 


% In this section, we evaluate the effectiveness of \name. We describe the experimental setup, detail the response selection process, and analyze the results to validate our hypotheses in \S\ref{sec:motivation}. 

\subsection{Experimental Setup}
We detail our experiment settings below. 

\subsubsection{Training Data Curation} 
In this controlled experiment, we focus on chain-of-thought  reasoning~\cite{wei2022chainofthought,wang2024mathshepherd,luo2024improvemathematicalreasoninglanguage,cobbe2021trainingverifierssolvemath,li2023making,lightman2023letsverifystepstep}. Different models may follow different reasoning paths to solve a problem, while their final solutions can be easily verified.

% Apart from its importance as a research topic, step-wise reasoning~\cite{wei2022chainofthought,wang2024mathshepherd,luo2024improvemathematicalreasoninglanguage,cobbe2021trainingverifierssolvemath,li2023making,lightman2023letsverifystepstep} is well-suited for testing our approach. The difference of models' solution traces to the final solution is a natural manifestation of their varying inherent biases. 
\begin{figure}
    \centering
\includegraphics[width=.6\linewidth]{plots/loss_plot.pdf}
    \caption{The training loss curve of Llama3.1-8B on \textsc{Tulu-3}-\textsc{Olmo-2} combo. \textbf{Best} stands for training with highest-probability responses (\name-selected);   \textbf{Random} stands for randomly chosen responses for each instruction in the pool, \textbf{Worst} stands for lowest probability responses in the base model's distribution. Throughout training, \textbf{Best} has lower loss than \textbf{Random} than \textbf{Worst}.}
    \label{fig:overview}
\end{figure}


We use UltraInteract-SFT~\cite{yuan2024eurus}, which contains approximately $80,800$ unique instructions covering coding, math (chain-of-thought and program-aided) and logic reasoning domains , where each instruction is paired with varying numbers of different responses to contain a total $>280,000$ training examples. The responses in the dataset are strictly in step-wise format. 
For each instruction, we use \name to select the top-ranking responses, ensuring that the number of responses matches the original UltraInteract-SFT dataset for fair comparisons.

\paragraph{Response Generation} 
We collect responses from a diverse set of models of various sizes across model families, including \textsc{Mixtral-7x7B-Instruct}~\cite{jiang2024mixtralexperts}, \textsc{Codestral-22B}~\cite{mistralai2024codestral22b}, \textsc{Mistral-Small}~\cite{mistralai2024mistralsmallinstruct}, \textsc{Llama-3.1-70B-Instruct} and \textsc{Llama-3.1-405B-Instruct}~\cite{dubey2024llama3herdmodels}, and \textsc{Qwen2.5-72B-Chat}~\cite{yang2024qwen2technicalreport}, resulting in approximately 10x additional responses per instruction. The responses are then filtered based on the answers to ensure their validity following~\citet{yuan2024eurus}.
% \todo{Detail the exact prompting template.}
\begin{figure}
    \centering
\includegraphics[width=.6\linewidth]{plots/average_performance_plot.pdf}
    \caption{The average performance curve of Llama3.1-8B on \textsc{Tulu-3}-\textsc{Olmo-2} combo. \textbf{Best} stands for training with highest-likelihood responses (\name-selected);   \textbf{Random} stands for randomly chosen responses for each instruction in the pool. The average is taken over all items on table~\ref{tab:olmo-tulu} except AlpacaEval2.}
    \label{fig:overview}
\end{figure}
\subsubsection{Base Models} To demonstrate the generalizability of \name, we evaluate its performance across multiple LLMs, including \textsc{Llama-3.1-8B} and \textsc{Llama-3.2-3B} from \textsc{Llama-3}~\cite{grattafiori2024llama3herdmodels} family, \textsc{Mistral-7B}~\cite{jiang2023mistral7b} and \textsc{Qwen2.5-7B}~\cite{hui2024qwen25}.

\subsubsection{Evaluation}

We evaluate the model on coding and math reasoning benchmarks. For coding tasks, we consider HumanEval~\cite{chen2021humaneval}, MBPP~\cite{austin2021mbpp}, LeetCode~\cite{guo2024deepseekcoder}; for math datasets, we consider \textbf{MATH} dataset~\cite{hendrycks2021measuringmathematicalproblemsolving}, \textbf{GSM-Plus}~\cite{li2024gsmplus} and \textbf{TheoremQA}~\cite{chen2023theoremqatheoremdrivenquestionanswering} dataset. 
\textbf{HumanEval} and \textbf{MBPP} are natural-language-to-code benchmarks testing language models' ability to produce functionally correct programs. \textbf{LeetCode} contains interview-level programming problems that are more challenging. 
\textbf{MATH} contains high-school level math competition problems, whereas \textbf{GSM-Plus} is a more challenging variant of GSM-8k~\cite{cobbe2021trainingverifierssolvemath} 
and \textbf{Theorem-QA} contains complex math reasoning problems. 
\subsubsection{Baselines}
We compare \name{} with several baselines. Except for \textbf{Up-scaled Dataset} which is $3\times$ larger, the rest are controlled for the number of responses per-instruction to be the same as UltraInteract-SFT.
\begin{itemize}
\item \textbf{Original dataset.} Performing SFT over the original UltraInteract-SFT dataset as is. The original dataset also contains high-quality, verified responses.

We compare \name to directly using a standard, well-curated dataset. Responses from a powerful model are likely to represent an upper-bound quality of training data, and comparing against this baseline allows us to isolate whether further improvements stem from the strategic response selection performed by \name{}.
\item \textbf{Responses from the strongest model under consideration.} 
We use responses exclusively from the best model used to generate responses (in our case, \textsc{Llama3.1-405B-Instruct}). 

% These responses are used to curate a replica dataset, which serves as a baseline to evaluate the effectiveness of \name{}.
\item \textbf{Up-scaled Dataset.} The comparison with a larger dataset assesses how well selection improves instruction tuning compared to training on a larger set of diverse correct responses. Each instruction is paired with three times of randomly selected responses from the pool.\footnote{For example, if UltraInteract contains 3 different responses for instruction $x$, we end up including 9 for this setting.}
\end{itemize}
We train all models for 1 epoch with a learning rate of $10^{-5}$.

% \paragraph{Self-Distillation.} In this setting, we first train the base model on the original dataset to get $\pi_{\theta_\mathcal{D}}$, and sample from that model to create a replica that only contains \(
% \tilde{\mathcal{D}'} = \{(q_i, \mathcal{R}_i^{(\theta_\mathcal{D})})\}_{i=1}^{N}.
% \)

% \paragraph{Reward-Model-Based Selection.}
% We consider an alternative strategy to curate the dataset replica using reward values instead of distribution-match-based selection. Under this set-up, we compute the reward values for the filtered responses and take the highest-reward instances. We use \textsc{Tulu-v3-8B-RM}~\cite{lambert2024tulu3}. 

\input{tables/main}

\subsection{Results and Analysis}
% \textbf{Overall Performance.} 
Table~\ref{tab:main} summarizes the performance of \name{} across benchmarks.
Our approach consistently outperforms the various baselines across the board, including the original UltraInteract-SFT dataset. \name-selected solutions can outperform those directly sampled from the strongest model under consideration (\textsc{Llama3.1-405B-Instruct}) up to 13.8\% absolute improvement. 
This implies that customization for base models should be prioritized over identifying the presumably highest-quality responses. This verifies our central premise that being in-distribution with each base model is an important ingredient for the responses we supervise the base models on, to boosting downstream performance.
% Unlike existing approaches that train models to mimic fixed answers indiscriminately, \name aligns fine-tuning responses with the base model's inherent distribution. This tailored approach not only enhances fine-tuning performance but also prevents the risks associated with distribution drift, which can lead to performance collapse.
Furthermore, we demonstrate that merely adding more responses does not always lead to continuous improvement in model performance, which aligns with findings in prior studies~\cite{li2024quantitytoqulality,du2023modsmodelorienteddataselection}. By properly aligning with models' base distributions, \name outperforms those trained with 3x responses with at least 3.6\% and up to 17.3\% absolute improvement. These results reinforce the notion that scaling data without considering its alignment with the base model's initial distribution risks diminishing returns and, in some cases, even performance degradation.


% Moreover, our ablation studies confirm that random selection of responses yields suboptimal results compared to \nameâ€™s tailored selection strategy. This finding underscores the importance of thoughtful data curation over naive augmentation strategies.
