\section{Related Works}
\paragraph{Data Engineering For Instruction Tuning}

Data is central to the success of effective instruction tuning, ~\cite{xu2023rethinkinginstructionqualitylift,xia2024less,chan2024balancingcosteffectivenesssynthetic}, featuring both automated data synthesis~\cite{xu2024wizardlm,zeng2024automatic,yu2024metamath,wei2023magicoder} and selection~\cite{xia2024less,chen2023alpagasus,parkar2024selectllm,li2024scar}. Some selection approaches focus on high-quality data by leveraging LLMs~\cite{chen2023alpagasus,parkar2024selectllm,li2024superfilteringweaktostrongdatafiltering} or employing principled metrics~\cite{kang2024getmoreforless,mekala2024smaller,xia2024less}, while others, such as ~\citet{yang2024smalltolarge,das2023deft}, aim to identify diversity-optimized subsets for greater efficiency.

Another emerging trend is the customization of training data based on the characteristics of the base models. For instance, ~\citet{li2024quantitytoqulality,du2023modsmodelorienteddataselection} leverage the base model itself to select a subset of instructions, while ~\citet{li2024selective} introduce a teacher model to guide the selection process. However, these approaches primarily focus on identifying a re-weighted subset of questions for training. In contrast, \name has a different focus, where it aims to find a set of responses from the solution space that not only provide good coverage of the golden distribution but also align closely with the base model's policy.

\paragraph{On-Policy Methods For Language Model Alignment}

Recent advances in RLHF~\cite{ouyang2022training}, RLAIF~\cite{lee2024rlaif} and preference learning~\cite{rafailov2023dpo} research have identified the benefits of on-policy data for model alignment~\cite{xu2024dposuperiorppollm,tajwar2024shoulduse,zhang2024selfexploringlanguagemodelsactive,guo2024onlineaifeedback,liu2024provablymitigatingoveroptimizationrlhf,zhang2024textbfplumimprovingcodelms,miao2024aligningcodellmsdirectpreference,gulcehre2023rest}. However, these works primarily emphasize the preference-tuning stage, where models are refined to generate samples from an already reasonable policy. In contrast, less attention is given to the supervised fine-tuning phase, which starts with base models that are not yet well-prepared to generate high-quality answers that align sufficiently with the desired policy for optimization.


% \subsection{Data Personalization for Language Models}
% Data personalization, mostly for the purpose of curriculum learning, has gained popularity.