\section{Conclusion}
We present \name, a simple yet highly effective approach to improve supervised fine-tuning data building on our empirically-verified hypothesis that instruction tuning data shall better match the base model's distribution to optimize the training outcome. \name is surprisingly simple and efficient, where it customizes response data to supervise each model by taking highest-probability instances. We demonstrated the effectiveness of \name on multiple settings. Remarkably, \name can outperform not only SOTA data selection baselines, but also Tulu3-SFT by using a 1/3 subset of its SFT instructions, and performs better than training on 4.5 times data it selects from. We show \name as a highly scalable and simple approach can be adopted in multiple practical scenarios with minimal overhead.

% \section{Limitations}
% This work does not provide rigorous mathematical justifications to state why sampling from the fine-tuned $\pi_{\mathcal{C}}^{(\theta_1)}$ does not improve the performance. Future works should investigate and characterize this phenomenon. 

% In addition, \name does not select data for other post-training stages like RLHF.