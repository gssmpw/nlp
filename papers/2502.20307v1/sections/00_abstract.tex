\begin{abstract}

We present Mobius, a novel method to generate seamlessly looping videos from text descriptions directly without any user annotations, thereby creating new visual materials for the multi-media presentation. 
Our method repurposes the pre-trained video latent diffusion model for generating looping videos from text prompts without any training.  
During inference, we first construct a latent cycle by connecting the starting and ending noise of the videos. Given that the temporal consistency can be maintained by the context of the video diffusion model, we perform multi-frame latent denoising by gradually shifting the first-frame latent to the end in each step. As a result, the denoising context varies in each step while maintaining consistency throughout the inference process.
Moreover, the latent cycle in our method can be of any length. This extends our latent-shifting approach to generate seamless looping videos beyond the scope of the video diffusion model's context.
Unlike previous cinemagraphs, the proposed method does not require an image as appearance, which will restrict the motions of the generated results. Instead, our method can produce more dynamic motion and better visual quality.
We conduct multiple experiments and comparisons to verify the effectiveness of the proposed method, demonstrating its efficacy in different scenarios.
All the code will be made available.
% Video generation has a rapid development in recent years thanks to the text-to-video latent diffusion model. 
% The latent diffusion models are typically trained on the latent space of large-scale datasets with a limited temporal context.
% However, real-world videos do not have such temporal restrictions, and the dynamic of the content is represented by videos of different lengths.
% We present ShiftDiffusion, a training-free method to utilize the latent shifting in the pre-trained video diffusion model for longer video generation. 
% As a result, the denoised context varies in each step while maintaining overall consistency throughout the entire inference process.

% % As a result, these videos typically have different lengths.
% % which makes it hard to adapt the pre-trained model to longer video generation. 
% Thus, in this paper, we try to imitate the original window context of the denoising loop.
% In detail, since the latent modeling of the video context, we propose progressive latent shifting in the original text-to-image model. \jf{From the perspective of using short video generation model to simulate long video generation model, dynamic shift latents belongs to the window, which cleverly realizes the interframe coherence of long video.} On the other hand, position embedding is widely used in current video generation for temporal layer position. Directly using it for our task will cause the difference between training and inference. Thus, we propose a dynamic rope to imitate the original models. Based on the proposed technique, our method can generate longer coherent videos without training the original models. Besides, the proposed method enables multiple new applications, such as loop video generation, and multi-prompt generation.



%Latent diffusions are widely used in current text-to-video generation techniques. However, given a pre-trained text-to-video diffusion model, we are curious about how to adapt it for longer video generation, as it can only infer within a limited inference context. In this paper, we try to imitate the original window context of the denoising loop to wonder about our curiosity. In detail, 


%Latent diffusions are widely used in current text-to-video generation techniques. However, it can only infer with limited inference context. Given a pre-trained text-to-video diffusion model, we are curious about how to adapt it to the longer video generation. 
%Thus, in this paper, we try to imitate the original window context of the denoising loop. In detail, since the latent modeling of the video context, we propose progressive latent shifting in the original text-to-image model. \jf{From the perspective of using short video generation model to simulate long video generation model, dynamic shift latents belongs to the window, which cleverly realizes the interframe coherence of long video.} On the other hand, position embedding is widely used in current video generation for temporal layer position. Directly using it for our task will cause the difference between training and inference. Thus, we propose a dynamic rope to imitate the original models. Based on the proposed technique, our method can generate longer coherent videos without training the original models. Besides, the proposed method enables multiple new applications, such as loop video generation, and multi-prompt generation.


% \jianfei{need updating!}
% Current text-to-video~(T2V) diffusion models can generate high-quality video clips from text prompts. However, the length of the produced video is typically in seconds, and the models often fail to generate consistently longer videos under multiple prompts. Most previous longer video generation works are more focused on the consistency of the video and show less effect on the multiple actions. In this paper, the multi-prompt control and consistency for longer video generation are considered jointly. Thus, we propose ZeroMTP, a novel training-free method for multi-prompt longer video generation based on the pretrained single-prompt T2V model. In detail, for generating longer and consistent video in multiple prompts, we propose a novel noise sampling method, \ie, \textit{progressive latent displacement}, to handle the consistent changes in the longer generation. The code will be available soon. \xiaodong{seems too short}
\end{abstract}