\section{Related Work}
% \jianfei{have updated.}


\subsection{Cinemagraphs and Looped Video Generation}
Our task is similar to cinemagraphs, which aim to produce looping videos by manipulating an input video manually. However, the manual creation of cinemagraphs is a time-consuming process, even for professional artists.
Previously, learning-based methods faced difficulties in generating or editing an entire video. As a result, prior techniques only applied to specific patterns to create cinemagraphs, for example, water~\cite{holynski2021animating, mahapatra2023synthesizing, liao2013automated}, periodic pattern~\cite{endless_loops}, portrait~\cite{sadtalker, 10.1111/cgf.12147, bertiche2023blowing}, panoramic~\cite{agarwala2005panoramic, 10.1145/3144455}. As for representative work,
Endless Loops~\cite{endless_loops} utilizes CRF to compute loop shifts, and it can only work on the repeated pattern.  \cite{holynski2021animating} presents an image animation method to generate the moving water from a single image utilizing Eulerian motion fields. Text-to-cinemagraphs~\cite{mahapatra2023synthesizing} further extend it by the pre-trained text-to-image stable diffusion model. Several methods ~\cite{generative_image_dynamic, niu2024mofa, shi2024motion} present a two-stage framework to generate the video with trajectory control. However, they only work on certain object types or need manual trajectory design.
While LoopAnimate~\cite{loopanimate} employs multi-stage training and symmetric guidance to achieve looped generation, their generated results are too still.
Besides, we can naively utilize the generative frame interpolation methods~\cite{wang2024generative,xing2024tooncrafter} based on the video diffusion model for a generation by setting the same start and end keyframes. However, since the original frame interpolation model is not trained for cinemagraph, the generated results might also be still. Besides, these methods involve additional larger-scale training for generation, which might cause forgetting problems.
Differently, we directly generate the looped video from the text description, yet with better visual effects, such as the whole movement of the camera and object motion.

\input{figures/texs/PLS}

\subsection{Video Generation in Diffusion Model Era}
Due to the stabilizing training process of the Diffusion Model~\cite{ddim, ddpm}, video generation has had a big breakthrough in recent years. Eary works~\cite{imagen-video, make-a-video} directly generate high-resolution videos from cascade models of spatial and temporal layers directly in pixel space.
On the other hand, utilizing the pre-trained text-to-image models~\cite{ldm}, \ie, Stable Diffusion, as the base model, many works try to add additional layers to keep temporal consistency. \cite{videocrafter1, modelscope, magic-video} add temporal attention modules to the base model and train in an end-to-end fashion. Besides, \cite{animatediff} finds that training the models by temporal layers only has a better visual quality. \cite{videocrafter2} proposes a method to increase the visual qualities by a two-stage image and video joint training process. However, these methods only create a short video with limited motions, which restricts its applications in real-world cases. 
Besides the text-to-video diffusion model, new works also train image-to-video models for generation, which is also related to our task. For example, Stable Video Diffusion~\cite{svd} fine-tunes the text-to-video diffusion model with a high-quality data pipeline. DynamicCrafter~\cite{xing2025dynamicrafter} shares a similar idea and trains on the video diffusion model. ToonCrafter \cite{xing2024tooncrafter} and Generative image in-between \cite{wang2024generative} are further finetuning the image-to-video models for the generative frame interpolation. However, as we discussed before, directing utilizing the frame interpolation methods for our task might have issues with the too-still motion.
Recently, Sora \cite{sora} has made a big step in video generation via denoising transformers~(DiT~\cite{DiT}), showing the scalability and advantages. 
Thus, the more recent video generation methods~\cite{mochi, cogvideox,hunyuanvideo} are based on the DiT structure, which has better motion and temporal consistency than previous methods.  

Besides, since these pre-trained large diffusion models are trained from larger-scale datasets, we can repurpose these models for the new task without training. For instance, in the field of image/video editing, works such as Prompt to Prompt~\cite{prompt-to-prompt}, FateZero \cite{fatezero}, and MasaCtrl~\cite{masactrl} have achieved zero-shot editing through attention control. Meanwhile, there also contains some methods that have provided foundational discoveries for zero-shot editing~\cite{yu2023animatezero, freedom} and improving the performance without additional training~\cite{freeinit, freeu}. In this paper, we utilize the most popular open-sourced DiT-based video generation model, \ie, CogVideoX~\cite{cogvideox}, as the base model for looped video generation in a training-free manner. 

% Supported by larger-scale datasets and 3D VAE~\cite{zhao2024cvvae}, this model can generate results with higher image quality and slightly longer frames than previous methods.
% However, the videos generated by these methods still do not meet the demand for longer frames. In this paper, we aim to utilize the pre-trained T2V model for longer video generation in a training-free manner.
% More recently, the denoising transformer-based DiT~\cite{DiT} architecture has shown superior performance on this task. Powered by the larger-scale dataset and 3D VAE~\cite{zhao2024cvvae}, this model can generate longer results than previous methods. However, these methods only can generate short video clips with a single action. In this paper, we aim to utilize the pre-trained single prompt T2V model for the multi-prompt video generation in a training-free manner.


\subsection{Longer Video Generation in Diffusion Model}
\label{longer_video_generation}
Our looping videos can be considered an infinitely longer video generation. In current methods, due to the limited latent length in training the pre-trained text-to-video generation models, several methods are proposed to modify the denoising process of the original diffusion model for new purposes. For the longer video generation, Gen-L-Video~\cite{gen-l-video} uses the weighted sum of different short latent segments in the overlapping area to alleviate the inter-frame continuity issue. However, this method significantly increases the inference time and can lead to smooth transitions between frames. 
FreeNoise~\cite{freenoise} introduces a shuffled latent sequence design and uses attention-based weighting to maintain visual consistency in long videos. However, since the latent changes only occur in the shuffling, the resulting video motion may appear too static and is prone to out-of-memory~(OOM) errors. FIFO~\cite{fifo} uses diagonal denoising for long video generation, maintaining the consistency and coherence of the video. However, there is a training inference gap for reasoning at different noise levels, and it lacks global information modeling. 
Video-Infinity~\cite{video-infinity} uses distributed inference to facilitate global and local information interaction, achieving video consistency while accelerating inference. However, an important limitation is the need for multiple GPUs to run simultaneously, and the quality of generating longer videos is not very good. 
DiTCtrl~\cite{ditctrl} utilizes a mask-based attention-sharing mechanism to maintain semantics, as well as a latent mixing strategy to achieve smooth transitions between video frames. However, this also brings about a significant amount of additional computational costs. 
These longer video generation methods change the combination of latent in the test time to control the generated content in the diffusion process, which inspired our looping video generation from text directly. 

% Due to the limited length of the pre-trained text-to-video generation, there are also methods for generating multi-prompt long videos.
% Gen-L-Video~\cite{gen-l-video} employs weighted summation of latents from different short video clips in overlapping regions to mitigate inter-frame continuity issues. However, this method significantly increases inference time and encounters abrupt transitions between frames. 
% FreeNoise~\cite{freenoise} introduces a shuffled latent sequence design and utilizes attention-based weighting to maintain visual consistency in long videos. Yet, due to latent variations only occurring through shuffling, the resulting video motions may appear overly static and are prone to out-of-memory~(OOM) errors. 
% Video-Infinity~\cite{video-infinity} uses distributed inference to facilitate global and local information interaction, achieving video coherence while speeding up inference. However, a significant limitation is its requirement for multi-GPUs to run, and the quality of the generated videos will also be influenced. FIFO~\cite{fifo} uses diagonal denoising for long video generation, maintaining video consistency and coherence. However, the delay in text guidance affects its application in multi-prompt video generation. 
% %\jianfei{Different from the above methods, our approach xxx}%

% \xiaodong{combine loop video generation to the long video generation}


% \subsection{Zero-shot Model Modification}
% With the vigorous development of diffusion models in the fields of image and video generation, pre-trained text-to-image/video models~\cite{ldm,videocrafter2,animatediff} have shown advanced performance as foundational models, enabling the use of these pre-trained models for open-domain related tasks. For instance, in the field of image/video editing, works such as Prompt to Prompt~\cite{prompt-to-prompt}, FateZero~\cite{fatezero}, and MasaCtrl~\cite{masactrl} have achieved zero-shot editing through attention control. Meanwhile, zero-shot editing methods~\cite{yu2023animatezero, freedom, freeinit, freeu} have provided foundational discoveries for zero-shot editing and offered new perspectives for zero-shot long video generation. 
% Inspired by these editing methods, some approaches~\cite{gen-l-video, freenoise, fifo, video-infinity, ditctrl} have also utilized zero-shot methods to achieve long video generation. This paper also follows suit, designing a zero-shot long video generation method.
% \break

% \clearpage