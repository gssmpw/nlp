\section{Method}

% We aim to generate the looping video from the text prompt description directly. 

Given the text prompt, we design a training-free method for generating the looping video by shifting the noise in each inference step of the pre-trained video diffusion model, so that all the frames will be considered equally in the final generated video.
% Previous text-to-video diffusion models~\cite{animatediff, videocrafter1, videocrafter2, modelscope, cogvideox} could only generate video clips of 2-6 seconds. Our task is to generate a longer video with consistency. To achieve this goal, as shown on the right side of Figure~\ref{fig:PLS}, we utilize a pre-trained single-prompt text-to-video model~\cite{cogvideox} as the base model and design a new zero-shot inference method to generate longer videos with consistent subjects and coherence.
Below, we first introduce the basic paradigm of text-to-video to better understand our method in Sec.~\ref{sec:preliminary}. 
Then, we introduce the proposed \textit{Latent Shifting}, which iteratively transforms the position of the latent in each step~(Sec.~\ref{sec:PLS}). Since directly utilizing the looping latents will show artifacts when 3D VAE decoding, we design a frame-invariant decoding to decode looping video~(Sec.~\ref{sec:fivae}). Finally, we introduce the \textit{Rotary Position Encoding interpolation} to model global positional information when generating the longer looping videos in Sec.~\ref{sec:DyRoPE} and give some applications in Sec.~\ref{sec:application}, respectively.
% \xiaodong{first, we need to introduce our task, the whole pipeline.}

\subsection{Preliminary: Text-to-Video Latent Diffusion Model}
\label{sec:preliminary}
Taking one specific video diffusion model, \ie, CogVideoX~\cite{cogvideox}, as an example, we introduce the basic concepts and knowledge of the text-to-video latent diffusion model. 
Current large text-to-video models are all based on the latent diffusion model~\cite{ldm}. The latent diffusion model contains an auto-encoder $\mathcal{E}(\cdot)$, $\mathcal{D}(\cdot)$ for compressing the videos into the latent space. In the most advanced video diffusion models~\cite{sora, cogvideox, mochi}, the compression of videos in both the spatial and temporal domains represents the crucial factor for realizing better visual and temporal qualities. Then, following the Denoising Diffusion Probabilistic Models~\cite{ddpm}, for training, the input $F$ frame video clip $ \mathbf{\upsilon} \in \mathbb{R}^{F \times H \times W \times 3} $ with width $W$ and height $H$ is first converted to the latent space $\mathbf{z}_0$, where $\mathbf{z}_0 = \mathcal{E}(\mathbf{\upsilon}) = [z_0^1; ... ; z_0^f] \in \mathbb{R}^{f \times h \times w \times c}$. $h,w,f$ are the compressed height, width, and frame in the latent space, respectively. Then, the latent diffusion model $\epsilon_\theta$ is trained to denoise its perturbed version $\mathbf{z}_{t}$. For noise $\epsilon \sim \mathcal{N}(0, \mathbf{I})$, the time step of diffusion model $t \sim \mathcal{U}([1,...,T])$, the text prompt $c$, this denoising diffusion model is trained to minimize the following loss:
\begin{equation}
    % \mathcal{L}_{\mathrm{DiT}}=\mathbb{E}_{\mathbf{z}_{t},c,\epsilon,t}(||\epsilon-\epsilon_{\theta}(\mathbf{z}_{t},p,c,t)||_{2}^{2})
    \mathcal{L}=||\epsilon-\epsilon_{\theta}(\mathbf{z}_{t};c,t)||_{2}^{2}.
\end{equation}
Here, the denoising network $\epsilon_{\theta}$ is based on the DiT~\cite{DiT} architecture. 
% where the latents are first divided by the patch embedding. Then, by adding the position embedding~(Rotary Position Embedding~(ROPE~\cite{rope}) in CogVideoX) to the input token, a pure transformer-based network is utilized to denoise.

After training, giving any noise latents $[z_{t}^{1};...;z_{t}^f] \sim \mathcal{N}(0, \mathbf{I})$ for video generation, and a diffusion sampler $\Phi(\cdot)$, such as DDIM sampler~\cite{ddim}, the diffusion model generate the final clear video via an $T$-step iterative denoising, where $t$-th denoising step is expressed as:
\begin{equation}
    [z_{t-1}^{1}; ...; z_{t-1}^{f}] = \Phi([z_t^1; ...; z_t^f], t, c; \epsilon_{\theta} ),
\end{equation}
where $z_{t}^{i}$ denotes the latent of $i$-th frame at time step $t$. Notice that, the context length of the video diffusion model is restricted by the denoising network $\epsilon_{\theta}$, and each latent has the unchanged position when inference.

Finally, we could generate a video by the pre-trained latent decoder $\mathcal{D}(\cdot)$ of the 3D VAE as: 
$\mathbf{\upsilon}' = \mathcal{D}(\mathbf{z}_{0}')$. Notice that, since the 3D VAE of the video diffusion model supports both image and video generation, they usually treat the first frame differently in temporal compression. 


% the latent diffusion model is firstly transformed into a latent representation $z^f$ through the encoder $\mathcal{E}$ of the pre-trained VAE~\cite{vae}. Then, a neural network $\epsilon_{\theta}(.)$ is trained to remove the Gaussian noise $\epsilon_{t}$ at $t$ step added to $z_{t}^{f}$, which is defined as: 
% \begin{equation}
%     % \mathcal{L}_{\mathrm{DiT}}=\mathbb{E}_{\mathbf{z}_{t},c,\epsilon,t}(||\epsilon-\epsilon_{\theta}(\mathbf{z}_{t},p,c,t)||_{2}^{2})
%     \mathcal{L}=||\epsilon-\epsilon_{\theta}(\mathbf{z}_{t}^{f},PE,c,t)||_{2}^{2},
% \end{equation}
% where $z_{t}$ denotes the sample noise of $z_{f}$ at time $t$, $c$ is the conditional text prompt, and $PE$ represents the positional encoding added to the feature space of $z_{t}^{f}$ to enhance the model's spatiotemporal modeling capabilities. 

% After training, $\epsilon_{\theta}$ predicts each step of the added noise, so we can finally get the $z_{0}^{f'}$ from a random noise $z_{T}^{f}$. Finally, we could generate a video by the pre-trained latent decoder $\mathcal{D}$ of the VAE as: $V' = \mathcal{D}(z_{0}^{f'})$

% % $\epsilon_{\theta}(.)$ is based on the UNet-like structure or the Denoising Transformer~\cite{DiT}. 
% The proposed method utilizing the DiT-based text-to-video model, \ie, CogVideoX \cite{cogvideox} for $\epsilon_{\theta}(.)$ looping video generation, since it shows the state-of-the-art performance on the video generation.



% \if
% DiT, building on the latent diffusion model~\cite{ldm}, replaces its denoising network from a UNet~\cite{unet} to a transformer structure~\cite{DiT}, enhancing performance and scalability. Typically, given a video input $v$, it is transformed into a latent representation $z$ through a VAE~\cite{vae} encoder $E$. The model is trained to remove the Gaussian noise $\epsilon_{t}$ added to $z$, with the loss function defined as: 
% \begin{equation}
%     % \mathcal{L}_{\mathrm{DiT}}=\mathbb{E}_{\mathbf{z}_{t},c,\epsilon,t}(||\epsilon-\epsilon_{\theta}(\mathbf{z}_{t},p,c,t)||_{2}^{2})
%     \mathcal{L}_{\mathrm{DiT}}=||\epsilon-\epsilon_{\theta}(\mathbf{z}_{t},p,c,t)||_{2}^{2}
% \end{equation}
% where $\epsilon_{\theta}(.)$ represents the DiT network, usually a stack of DiT Blocks, $z_{t}$ denotes the sample noise of $z$ at time $t$, $c$ is the conditional information, typically text, and $p$ represents the positional encoding added to $z_{t}$ to enhance the model's spatiotemporal modeling capabilities.
% % After training the denoising network, the generated latents can be obtained by removing noise from random noise $\mathbf{z}_{T}=[z_{T}^{1};\ldots;z_{T}^{f}]\sim\mathcal{N}(0, I)$ over $T$ steps, where $f$ is the short video clip frame length. Each denoising step can be represented as:
% % \begin{equation}
% %     % \mathbf{z}_{t-1}=
% %     [z_{t-1}^1;...;z_{t-1}^f]=\Phi([z_t^1;...;z_t^f],t,{c};\epsilon_\theta),
% % \label{eq:sample}
% % \end{equation}
% % where $\Phi$ is a sampler like DDIM~\cite{ddim}. Finally, $\mathbf{z}_{0}$ can be transformed into pixel space through $\mathit{D}(\mathbf{z}_{0})$ to generate the video.
% \fi
% \\
% \paragraph{Rotary Position Embedding~(RoPE)}: 

% In fact, the RoPE encoding for the $m$ position can be rewritten in the following form:
% \begin{equation}
% \label{eq:rotary_angles}
% \begin{aligned}
%     \begin{pmatrix}\cos m\theta_0 & -\sin m\theta_0 & \cdots & 0 & 0\\ \sin m\theta_0 & \cos m\theta_0 & \cdots & 0 & 0\\ \ \vdots & \vdots & \ddots & \vdots & \vdots\\ 0 & 0 & \cdots & \cos m\theta_{d/2-1} & -\sin m\theta_{d/2-1}\\ 0 & 0 & \cdots & \sin m\theta_{d/2-1} & \cos m\theta_{d/2-1}\end{pmatrix}.
% \end{aligned}
% \end{equation}
% Here, the encoding involves pairwise rotation matrices, which is also where the term RoPE originates.



% \input{algorithm/Algorithm_PLS}
%
\subsection{Latent Shifting}
\label{sec:PLS}

The text-to-video diffusion model is trained on a multi-frame latent diffusion model, where multiple latents are sent into the denoising network for a generation. 
Since our looping video requires each frame to be considered as the first frame, we thus need to make each latent have the temporal consistency of the previous latent and the end latent. As shown in Figure.~\ref{fig:PLS}, we first build a cycle latent list for denoising by connecting the first frame latent and the last. Then, for each denoising step, we shift the first frame to the last to build a new multi-frame latents for generation. After multi-step denoising, we can maintain the whole temporal consistency of the entire video. 

Formally, given the inference context $f$ of a video diffusion model, we can generate the looping video which contains $N$ latents, where $N = n \times f$ and $n$ are the multiple factors for longer looping video generation. Firstly, we initialize all the latent as $[z_{T}^{1};...;z_{T}^N] \sim \mathcal{N}(0, \mathbf{I})$, then, for $t$-th denoising step, we shift the start point of the denoising context by $j = (t \times s) \mod N$, where $s$ is the skip step of each iteration.  Since we also need to maintain the $f$-frame inference restriction in the pre-trained diffusion model, the denoising step of this step can be formulated as:
\begin{equation}
    [z_{t-1}^{j}; ...; z_{t-1}^{j+f-1}] = \Phi([z_t^{j}; ...; z_t^{j+f-1}], t, c; \epsilon_{\theta} ),
\end{equation}
where $\Phi$ is a DDIM Sampler~\cite{ddim} as introduced before. When $j + f -1 > N $, our cycle list creates the denoising latents by concat the $[z_t^{j},...,z_t^{N}]$ and $[z_t^{1},...,z_t^{f-(N-j+1)}]$.

Our latent shifting algorithm utilizes the multi-frame denoising steps in the diffusion model and the temporal consistency denoising of the video diffusion model for looping video generation. Notice that, since this latent denoising method can be any length, our method can produce any length inference looping videos and can also be utilized in the longer video generation. 



% Given the inference context $C$ of a video diffusion model, we can generate the looping video with frame $N$, where $N = n \times C$. Firstly, we initialize all the frames 
% Firstly, we $[z_{t}^{1};...;z_{t}^f] \sim \mathcal{N}(0, \mathbf{I})$ 


% \jianfei{need updating.}
% The primary challenge we face lies in the length of latents. The key difference between short and long videos is the increased number of latent frames in long videos. As latents grow longer, relying solely on the information modeling from short-frame short video models becomes insufficient for capturing the long-range latent interactions in long videos. One simple approach could be directly overlapping and mixing latents, similar to the idea behind Gen-L-Video~\cite{gen-l-video}. This would involve performing inference at the junctions between short video segments, and then using weighted interpolation on the overlapping portions, resulting in a time cost of $2 * n - 1$. While this method somewhat alleviates the abrupt transitions in latent interactions, it inevitably leads to excessive time consumption and issues with transition smoothness in the video. To minimize unnecessary time expenditure while preserving latent interactions between long videos, we propose a $\mathbf{P}$rogressive $\mathbf{L}$atent $\mathbf{S}$hifting~($\mathbf{PLS}$)~method, which achieves a time cost of $n + 1$~(note that the Naïve cost is $n$) and effectively addresses the challenge of long latent interactions.

% Specifically, as the denoising timesteps progress, we dynamically divide the latent windows fed into the denoising network, allowing latents to exchange information across windows. As shown in Figure~\ref{fig:PLS}, using the PLS method, latents undergo denoising while subtly enabling global information interaction. Although each step involves local information exchange, the local window changes dynamically with each timestep, and ultimately, this results in an equivalent global information interaction. Formally, we show the overall progress of the
% proposed Progressive Latent Shifting in Algorithm.~\ref{alg:progressive_latent_shifting}
\input{figures/texs/VAE}

\subsection{Frame-Invariance Latent Decoding}
\label{sec:fivae}
To meet the demands of both text-to-video and text-to-image joint training, the latent compression of current state-of-the-art video generation models~\cite{cogvideox} does not compress each frame equally in the temporal dimension. In detail, CogVideoX employs a 3D VAE structure that compresses video frames both in spatial and temporal compression. 
% This leads to an issue: when converting the input image condition into latents, it is undesirable to apply the same 4× compression. 
However, the first latent frame employs a special encoding and does not do any compressions, while subsequent frames are encoded with the standard $4 \times$ compression for the motion similarity. 
In latent decoding, it utilizes the first three latent to generate the first night frames of video.
This inconsistent treatment of latent is inherently incompatible with our proposed latent shift method for looping video generation since we aim to produce a looping video in which each frame should be considered equally. If we directly utilize the original 3DVAE, it results in artifacts in the generated first frame due to the $4\times$ compression, as shown in Figure~\ref{fig:ablation}.
To mitigate this issue, we copy the last three latents and insert them before the first latent as redundant frames to counteract the special compression of the first frame. Then, in the generated video, we remove the redundant generated frames by the added latent.


% Specifically, we investigate the decoding process of the 3D VAE: for an odd number of latents, the first three latents are decoded together to produce 9 pixel frames; for an even number of latents, the first two latents are decoded together to produce 8 pixel frames. Subsequent frames are decoded in pairs, each pair generating 8 pixel frames. We observe that the pixel frames generated from pairs of latents are always decoded with a $4 \times$ expansion. Therefore, we copy the last few frames~(tail frames) and insert them before the first frame as redundant frames to counteract the special treatment of the first frame, normalizing it into a regular category. Finally, we remove the pixel frames generated by the redundant latents, effectively eliminating the 4× information overload in the first frame and achieving Frame-Invariance Latent Decoding, thereby removing artifacts.



\subsection{Rotary Position Embedding Interpolation}
\label{sec:DyRoPE}

CogVideoX~\cite{cogvideox} uses Rotary Position Embedding~(RoPE) to give positions in the attention model for denoising, which aims to achieve relative position encoding via absolute rotary position. However, if we directly utilize the original PoPE for our longer looping video generation task, the longer context does not match the original text-to-image model. 
% In large language models, the concept of position encoding was introduced to allow the transformer to model sequential information~(or positional information) in text.
% RoPE~\cite{rope} is a position encoding technique that , and it has been widely applied in current text-to-video generation models~\cite{cogvideox,mochi,hunyuanvideo}. 
To address this issue, we utilize a RoPE~\cite{rope} interpolation method for globally latent coding in the temporal dimension, inspired by the NTK-Aware interpolation in the longer context large language model~\cite{ntk_aware}. 
% Since we are only adding to the temporal dimension and there is no change in the pixel dimension of the video, we focus on making adjustments to the temporal RoPE encoding.

\input{figures/texs/DyRoPE}

 
Given the query vector at the $m$ position $q_{m}$ and the key vector at the $n$ position $k_{n}$ in the attention, RoPE introduces absolute positional information before calculating attention as follows:
\begin{equation}
\label{eq:qk_rope}
\begin{aligned}
    Q_{m}&=RoPE(q_{m},m)=q_{m}e^{im\theta},\\
    K_{n}&=RoPE(k_{n},n)=k_{n}e^{in\theta}.
\end{aligned}
\end{equation}
\\
Here, $\theta=\operatorname{diag}(\theta_{0},\cdots,\theta_{d/2-1})$ is a pre-define diagonal matrix, where $\theta_i=b^{-2(b-1)/d}$, with $b=10000$, and $d$ represents the vector dimension. Then, we perform an inner product calculation to obtain the attention weights $A_{m,n}$ as follows:
\begin{equation}
\label{eq:qk_product}
\begin{aligned}
    A_{m,n}=\mathrm{Re}[\left< Q_{m},K_{n} \right>]=\mathrm{Re}[\left< q_{m},k_{n} \right>e^{i(m-n)\theta}].
\end{aligned}
\end{equation}
The result can be transformed into a value related to $m-n$, thus achieving relative position encoding.

% PLS can achieve information interaction among the entire latents, but as mentioned above, what we want is to simulate the long video generation model. Therefore, we need the model to understand the positional relationships of the entire latents~(even if they are not input into the network simultaneously). However, the current RoPE approach defaults to encoding the position of each latent in the DiT network starting from zero, as shown in Fig~\ref{fig:DyRoPE}~(a). While relative positions are not encoded incorrectly, for the entire latents, the position encoding changes at each timestep as the PLS progresses. In other words, these position encodings only reflect local positioning and fail to capture global position encoding. To address this issue, we propose a globally dynamic RoPE approach~(DyRoPE), inspired by the NTK-Aware~\cite{ntk_aware} improvement to RoPE in LLMs. Since we are only adding to the temporal dimension and there is no change in the pixel dimension of the video, we focus on making adjustments to the temporal RoPE encoding.

% First, from Eq~\ref{eq:rotary_angles} and $\theta_i=b^{-2(i-1)/d}$, we can express the value for encoding the m-th position as follows for each dimension:
% \begin{equation}
% \label{eq:m_rotary_dim}
% \begin{aligned}
%     \left[\sin(\frac{m}{(b^{\frac{2}{d}})^0}),\cos(\frac{m}{(b^{\frac{2}{d}})^0}),...,\sin(\frac{m}{(b^{\frac{2}{d}})^{d/2-1}}),\cos(\frac{m}{(b^{\frac{2}{d}})^{d/2-1}})\right]
% \end{aligned}
% \end{equation}
% Let’s assume $\beta=b^{2/d}$, then we can obtain the base representation as follows:
% \begin{equation}
% \label{eq:b_base_dim}
% \begin{aligned}
%     \left[\sin(\frac{m}{\beta^0}),\cos(\frac{m}{\beta^0}),...,\sin(\frac{m}{\beta^{d/2-1}}),\cos(\frac{m}{\beta^{d/2-1}})\right]
% \end{aligned}
% \end{equation}
To extend the encoding for longer lengths, we scale the base $b$ as follows: 
\begin{equation}
\label{eq:b_base_dim}
\begin{aligned}
    b^{\prime}=b\cdot k^{d/(d-2)}
\end{aligned}
\end{equation}
% \jf{For the detailed proof, please refer to the appendix.}
Here, $b^{\prime}$ denotes the result after scaling, $k$ represents the multiple by which the video length increases, and $d$ indicates the dimension of the latents vector. Fig.~\ref{fig:DyRoPE} gives an illustration on how the RoPE-Interp. works. Since our core idea is to make each frame equal in the generation, we also try two different schemes to add the RoPE-Interp. to the features. The first one is the \textit{shifted RoPE-Interp.}, where RoPE changes along with the latents, and another is the \textit{fixed RoPE-Interp.}, where RoPE remains unchanged while the latents shift. We provide a more detailed comparison in the experiments. 
% \jianfei{As shown in Figure~\ref{fig:DyRoPE}~(b), with the increase of video frame length, we have extended RoPE accordingly. We also design two schemes: Shifted RoPE-Interp, where RoPE changes along with the latents, and Fixed RoPE-Interp, where RoPE remains unchanged while the latents shift}. 
% Experimental results show that this design leads to significant improvements in both video quality and coherence.
% As shown in Figure~\ref{fig:DyRoPE}~(b), for the increased video length, we also dynamically allocate its global positional encoding. \xiaodong{add two different methods for ROPE}, Experimental results also show that this design leads to significant improvements in both video quality and coherence.
% Solving this gives , and thus $\beta\lambda=(b\cdot k^{d/(d-2)})^{2/d}$. Through this approach, we have achieved the design of global RoPE encoding, as shown in Fig~\ref{fig:DyRoPE}~(b). Experimental results also show that this design leads to significant improvements in both video quality and coherence.

% \input{figures/texs/Application}
\subsection{More Application: Longer Video Generation}
\label{sec:application}
% \subsubsection{Multi-prompt long video generation}
% \label{sec:multiprompt_long_video_gen}
% \subsubsection{Looping video generation}
% \label{sec:looping_video_generation}
Longer video generation is an active research topic in current video generation since current text-to-video generation methods can only generate videos with limited context. The proposed latent shift naturally supports longer video inference beyond the training context by a non-cycle latent displacement. We utilize the same RoPE interpolation as we introduced before to correct the position of the latent. We give some examples in the supplemental videos.




% Usually, single prompt long video generation cannot meet the needs of more users for controllability.  In order to meet users' needs for controllable video operations, we introduce the control of multiple prompts. By switching between prompts, controllable video generation is achieved. For multi-prompt long video generation, we simply let the prompt change in the PLS implementation process, as shown in Fig~\ref{fig:Application}~(a). It is not difficult to find that this allows the latents at the junction guided by different prompts to absorb the alternating guidance from different prompts, thereby achieving good video consistency and text controllability.


% We give some examples in the experiments.



% Looping video generation is commonly used in many scenarios (such as dynamic wallpapers, movie plots, etc.), and it usually requires that the last frame of the video maintains excellent coherence with the first frame. Some existing methods typically require training to achieve looping video generation, which not only consumes a large amount of resources but also faces the problem of difficulty in collecting looping datasets. We have designed a looping sampling method that achieves zero-shot looping video generation. In simple terms, we connect the latents in sequence to form a ring, as shown in Fig~\ref{fig:Application}~(b). As the timestep progresses, we incrementally break one edge of the ring, turning it into a chain that is input into the network for denoising. In other words, during the denoising process, each frame of the latents may become the first frame. Moreover, this method is naturally compatible with PLS and can easily extend to looping long video generation.