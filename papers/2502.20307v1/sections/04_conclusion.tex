\section{Conclusion}
We represent a novel and innovative approach to generating seamlessly looping videos directly from text descriptions without the need for user annotations. This is achieved by repurposing a pre-trained text-to-video latent diffusion model with inference latent modification. In detail, considering each frame should be considered equally in the looping video, we construct a latent cycle and design latent shift to utilize the abilities of the video diffusion model's multi-frame latent denoising in each step, which further expands the scope of seamless looping video generation beyond the limitations of the video diffusion model's context. Besides, we introduce the frame-invariant latent decoding and RoPE-interpolation to further increase the performance.
% enable the generation of videos with consistent denoised context throughout the inference process. The ability to create latent cycles of any length further expands the scope of seamless looping video generation beyond the limitations of the video diffusion model's context.
Compared to previous cinemagraphs, Mobius has a distinct advantage as it does not rely on an image for appearance, thus allowing for more dynamic motion and enhanced visual quality in the generated videos. Through multiple experiments and comparisons, the effectiveness of this method has been verified across different scenarios even on the application of longer video generation task. 