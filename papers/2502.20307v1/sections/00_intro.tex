%---------------------------------------------
\section{Introduction}
\label{sec:intro}

Looping video, also called \textit{cinemagraph} in some research, aims to create a seamless looping video without ends via periodical motions. It is a unique way to share a specific moment's dynamics, which is popular as short videos and animated GIFs on social media, photo-sharing platforms, and screen savers\footnote{\url{cinemagraphs.com}} to create a better user experience. 
However, capturing these looping videos needs huge manual efforts, including the stabilization of the camera, manually annotating the moving object, selecting the animated frames, \etc. 

Previous efforts \cite{10.1111/cgf.12147, holynski2021animating, endless_loops, liao2013automated} make cinemagraphs from the given video or a single image animation. However, due to the difficulty of modeling open-world motion prior, these methods only focus on creating the looping video on the specific kinds, for example, water~\cite{holynski2021animating, mahapatra2023synthesizing, liao2013automated}, periodic pattern~\cite{endless_loops}, portrait~\cite{sadtalker, 10.1111/cgf.12147, bertiche2023blowing}, panoramic~\cite{agarwala2005panoramic, 10.1145/3144455}. 
Since the diffusion model provided universal generative priors for video, current frame interpolation methods~\cite{wang2024generative, loopanimate} can naturally produce the cinemagraph by setting the same beginning and end frames, however, the generated results in frame interpolation will often tend to generate still results in all frames.  
Besides, all current cinemagraph methods focus on simple motions with limited movement, whereas real-world videos are more complex.


% which is popular as the short videos and animated GIFs on social media, photo-sharing platforms, and screen savers. 


We define a new research problem beyond current cinemagraph synthesis which is directly generating the seamless looping video from text description. Different from previous methods which need tricks of a stable camera and only repeating some of the elements, our method aims to generate fully looping videos directly from the pre-trained text-to-video models, which will show more dynamic motions and natural visual effects, including the moving objects, the camera, \etc, by the generative prior. 
This is fully automatic and can generate videos which is unusual in real life. 
However, there are two key challenges in adapting it for our task. First, as the text-to-video diffusion model is trained on natural video, it remains unclear how to adapt it to our looping video generation. On the other hand, the current text-to-video generation model can only generate certain frames during inference. However, a short video might not provide a good representation of real-world dynamics. 
% We aim to utilize the pre-trained text-to-video diffusion model~\cite{cogvideox} as the base model.

Thus, we present Mobius to solve these problems in a training-free manner, where the key observation is that each frame should be considered equally important in the video final video. To this end, firstly, we propose a latent shift strategy in denosing. We construct a cycle utilizing all the noisy latents from the first frame to the end frame. Then, we shift its position by adding the first frame to the last to build the new noisy latent for denoising.
Thus, the video model maintains temporal consistency in each denoising step, and each video is equally considered.
For the generation in the longer context, the proposed latent shifting strategy naturally enables longer looping video generation by a longer denoising sequence cycle. 
However, if we directly generate the longer video utilizing this method, the generated results are also influenced by the inaccurate position embedding and frame-variant 3D VAE. Thus, we extend the rotary position embedding by an NTK-aware interpolation method inspired by the long context Large Language Model~\cite{ntk_aware} and propose a frame-invariance method for latent decoding.
Based on these modifications, the proposed method can directly utilize the pre-trained video diffusion model to generate high-quality cinemagraphs from text descriptions. Besides, we also show that the proposed latent shift can also work well for longer video generation tasks. Finally, the experiments demonstrate the qualitative and quantitative advantages of our approach. 
% We also extend the proposed method to two applications: single-longer video generation and multi-prompt video generation.

% In testing, it starts from multiple noises to generate the video via a multi-step denoising network via diffusion model. In looping videos, each frame should be considered as the first frame to construct the final video. Thus, we gradually shift the video latent in each denoising step to build a looping denoising process. Besides, since the current video generation process can only accept certain frames for generation, to extend the possibility of the proposed looping video, the proposed latent shifting strategy enables longer looping video generation seamlessly.

% To perform a looping video, each frame in the video should be also the first frame,











% \jianfei{have updated.}
% In the realm of text-to-video~(T2V) technology, current large diffusion models~\cite{animatediff, ldm, videocrafter1, videocrafter2,cogvideox} have demonstrated the ability to generate high-quality video clips based on text prompts. 
% The quality of video generation has been further enhanced with the use of the DiT network architecture~\cite{DiT}.
% Users can leverage these methods to create high-quality advertisements, generate short videos, \etc, easily from simple text prompts. However, existing models face certain limitations. Due to constraints in training resources and data, the generated videos are typically only 2-6 seconds long, which does not meet the demand for longer video generation.
% However, existing models face certain limitations since they are only trained in single prompt text-video pairs, and the GPU memory restricts the generation's length. Typically, the generated videos are only a few seconds long and often struggle to produce consistently longer videos when subjected to multiple action prompts.

% To address the issue of long video generation, several methods have been proposed to alleviate the limited context content problem of pre-trained video diffusion models, thereby achieving consistency in long videos. For example, Gen-L-Video~\cite{gen-l-video} shares the latents between each individual sampling in a zero-sampling manner. FreeNoise~\cite{freenoise} shuffles the previous latents to generate new longer content. FIFO~\cite{fifo} simulates a queue to process the latents at different noise levels iteratively for the generation of each frame, and so on.

% For example, Gen-L-Video~\cite{gen-l-video} shares the latents between each individual sampling in a zero-shot manner. FreeNoise~\cite{freenoise} shuffle the previous latents for new longer content generation, \etc. However, these methods have shown less impact on addressing multiple actions within the video.

% However, these methods typically regard the task of long video generation as splicing short videos, focusing only on the continuity between short video segments. Even if the coherence of the video is maintained, it can lead to unreasonable phenomena such as sudden camera shaking and abrupt changes in video flow rate. A key reason for this is that these methods do not consider the changes in global information, and each frame does not explicitly or implicitly obtain information from the entire video.

% Our key perspective is that we regard the pre-trained short video model as a part of the long video generation model, rather than treating long video as a concatenation of short videos. In other words, we believe that long videos require a dedicated long video generation model to achieve the best results. Our task has shifted from how to splice short videos to achieve long video generation to how to use short video models to simulate long video generation models.

% For simulating long video generation models, the greatest challenge is how to integrate global information while achieving inter-frame coherence, as well as how to extend short video generation models to conform to the inter-frame correspondence of long video generation models. For the former challenge, we observe that latents at the same noise level can be freely combined. Since diffusion models involve multiple inference steps, we can move the inference context window at different sampling steps for information interaction, thereby implicitly modeling global information through local information interaction. For the latter issue, we dynamically extend the positional encoding of the short video generation model to achieve the overall positional construction of the long video, thereby strengthening the inter-frame correspondence of the long video.

% Thus, we propose Progressive Latent Shifting~(PLS), a novel training-free method for long video generation based on pre-trained T2V models~\cite{videocrafter2}. This method effectively manages consistent changes that occur during the longer generation process by moving the context window at different denoising steps. Additionally, to enhance the overall positional relationships in long videos, we dynamically extend RoPE to model global positional information. We also compare our method with several state-of-the-art methods, and experiments demonstrate the qualitative and quantitative advantages of our approach. Given the scalability of PLS, we also showcase the implementation of related downstream tasks, such as looped video generation and multi-prompt long video generation.

Overall, the contribution can be summarized as:
\begin{itemize}
    \item We conduct a new research problem for the open-domain seamless looping video generation from text description using a pre-trained text-to-video diffusion model.
    \item We propose a latent shifting strategy to interactively denoise the latent in each step so that we can generate the looping video and it can be in arbitrary lengths.
    \item The detailed experiments show that the proposed method can achieve state-of-the-art performance on looping video generation and we also give the applications on longer video generation.
\end{itemize}
%---------------------------------------------
