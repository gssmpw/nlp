\section{Experiment}
\subsection{Settings and Implement Details}

\paragraph{Implementation details}
Our method is based on the pre-trained state-of-the-art open-source latent video diffusion model, CogVideoX-5B~\cite{cogvideox}. Notice that, we only modify the latent input of the diffusion model, our method might also work on any newly designed text-to-video latent diffusion models~\cite{mochi, hunyuanvideo}, without training.
% In our experiments, by inputting a single prompt, we can generate longer and more coherent videos. 
Each video has a resolution of 480x720, and the inference step is set to 50 following a standard DDIM sampling strategy. Other parameters are the same as the default settings of CogVideoX. To evaluate the proposed methods, we choose 140 prompts from VBench \cite{vbench} and EvalCrafter~\cite{evalcrafter} and use GPT~\cite{chatgpt} to expand them into more detailed descriptions. All the experiments are conducted on a single NVIDIA H100 GPU. Since we only add a temporal latent shift in each step denoising, the proposed method has a similar inference speed compared with direct generation.

\paragraph{Baseline}
Since there is no previous work for open-domain looping video generation from a text description, we majorly compare two generative interpolation methods and one method from the community. The first generative interpolation method is \textit{Svd-Interp.} from Generative Image Inbetween~\cite{wang2024generative}, which is trained on the stable video diffusion model~\cite{svd} for frame interpolation. The other generative interpolation is \textit{CogX-Interp.}\footnote{\url{https://github.com/feizc/CogvideX-Interpolation}}, which is also trained from the image-to-video model of the CogVideoX for frame interpolation. 
To compare, we consider the first frame of our generated results for the starting and ending key frames of the interpolation. Notice that these two methods are based on larger-scale training for frame interpolation. Our method generates the looping video from the text description directly.
\textit{Latent Mix} is a method to achieve this looping video, which has been reported on Github\footnote{https://github.com/THUDM/CogVideo/issues/149}, we compare with this method directly.


% Additionally, we compare with the \textbf{\textit{Naïve}} approach, which simply concatenates short videos generated directly without any additional operations.
\input{tabs/compare_loop}

\paragraph{Evaluation Metrics}
We report the MSE of the first frame and the last frame in the generated videos due to the looping video has the same first frame and the end frame. For the overall video quality, we utilize the widely used FVD~\cite{fvd} and CLIP Score~\cite{clip-score} for comparison. Besides, we give the overall video smoothness and dynamic score of the whole video from VBench~\cite{vbench}.

\input{figures/texs/Single_compare_solo}


\subsection{Comparison with Other Methods}
As introduced before, since current cinemagraph methods can not work on open-domain looping video generation, we compare our method with the state-of-the-art generative frame interpolation methods introduced in the baseline section. As shown in Fig.~\ref{fig:single_compare}, the baseline interpolation methods may produce still results or generate content that is far away from the start frame and the end frame. The latent mix method blending the initial and final latent may result in artifacts in the end frame. Differently, the proposed method can generate the same start and end frames without noticeable differences. Due to the page limitation, we give more examples in Fig.~\ref{fig:additional} and the supplementary video.

As for the numerical comparison, as shown in Tab.~\ref{tab:looping}, the proposed method shows a better visual quality and text-video alignment than previous methods. Besides, we also achieve a relatively higher score with both motion smoothness, video dynamic, and the MSE between the first frame and the last frame, which shows the advantage of the proposed methods. We argue that although the latent mix method gives much dynamic video, the generated content might not be a looping one according to the MSE between the first and the last frame. 
Evaluating the looping videos using current automatic evaluation metrics is also difficult, so we conduct a subjective user study to prove the proposed method's effectiveness further. 
In detail, we invite 23 participants to rank ten questions across three aspects, totaling 690 opinions under five different methods. Each participant will be asked to rank the overall visual quality of the video, the consistency of the video frames, and how dynamic the video is, on a scale of 5 to 1. Finally, we calculate the average score of these opinions. As shown in Table~\ref{tab:user_study}, our method outperforms others in visual quality, temporal quality, and video dynamic.



\input{tabs/user_study}



\subsection{Ablation Studies}
We have given the example in Fig.~\ref{fig:vae} to validate the effectiveness of the proposed frame-invariance latent decoding. Here, we give more ablation studies on the ROPE-Interp. and the skip step in our latent shifting. When performing a latent shift, we can shift the latent $s$ step for denoising, where a small step will be similar to the original inference. As shown in Fig.~\ref{fig:ablation}, when shifting the latent 6 steps in each denoising, the generated content is in a balance of the generated content and the motion. Differently, a small skip will show obversely artifacts. 


\input{figures/texs/ablation}


We also conduct experiments on the RoPE interpolation. In the method, we give two different ways to utilize the interpolated RoPE. As shown in Fig.~\ref{fig:rope}, the fixed RoPE-Interp performs well in our longer video looping generation, allowing each frame to be treated as the first frame during video generation, thereby achieving better looping results.
% which is more similar to the original diffusion model denoising, where the inputted latent has the same position as the model training.


\input{figures/texs/ablation_rope}


% \input{figures/texs/Compare_with_inference_time}
% \noindent\textbf{Time Efficiency  }
% We also conducted temporal efficiency experiments under CogVideoX, as shown in Figure~\ref{fig:compare_with_inference_time}, where the size of the legend also represents the length of different video frames. It can be seen that as the video frames continue to grow, the inference time of various methods is increasing. The Naïve method represents the baseline, that is, the lowest cost time. For Ditctrl, the latent mixing strategy infers too many intermediate latents for mixing, leading to a rapid increase in time cost as the frame length changes. However, our proposed method is only slightly higher than the baseline and much less than the time of Ditfctrl, proving its high temporal efficiency.


\subsection{Applications on Longer Video Generation}


% Since the proposed latent shifting can naturally work for longer video generation, we also compare our method for longer video generation, including \textit{Gen-L-Video}~\cite{gen-l-video} achieves video smoothness by blending the overlapping regions of latents. \textit{FreeNoise} \cite{freenoise} maintains subject consistency using a shuffled sequence of latents. \textit{FIFO}~\cite{fifo} generates each frame iteratively using a queue of latents at different noise levels. \textit{DitCtrl}~\cite{ditctrl} preserves video transitions through a KV-sharing mechanism and latent mixing strategies. 

% We conducted a visual comparison with the aforementioned baseline methods. 

Since the proposed latent shifting can naturally work for longer video generation, we also compare our method on longer video generation, where these baselines have been introduced in Section ~\ref{longer_video_generation} for details. 
% We conducted a visual comparison with the recent state-of-the-art  in the related work, please refer to Section ~\ref{longer_video_generation} for details.
% Each method has its own drawbacks,
As shown in Figure~\ref{fig:single_compare}, directly increasing the size of the latent causes video quality collapse. \textit{Gen-L-Video}~\cite{gen-l-video} produces overly smooth transitions in the background and excessive changes in the direction of the seagull. \textit{FreeNoise}~\cite{freenoise} tends to keep the seagull's orientation constant, the static nature of the image caused by latent shuffling is immediately apparent, and the phenomenon of the seagull having three legs also occurs. Although \textit{FIFO}~\cite{fifo} achieves better motion changes and video coherence, the issues of the seagull changing direction twice in a row and having three legs persist. \textit{DiTCtrl}~\cite{ditctrl} improves the seagull's orientation issue, but still has problems with the defective generation of the seagull's head in the first frame and the three-legged issue. In contrast, the proposed method maintains the seagull's orientation while ensuring coherent video motion. It does not exhibit the issue of the seagull having three legs, thereby achieving superior long video generation.
We give the full comparison in the supplementary video. As for the numerical comparison, we conduct the experiments on the same prompts of our looping video generation and calculate the main numerical results in Tab.~\ref{tab:single} utilizing the well-known metrics from previous studies~\cite{ditctrl,freenoise}.


\input{tabs/compare_with_others}

% \subsubsection{Multi-prompt long video generation}
% \subsubsection{Visual Comparison}
% \subsubsection{Quantitative Results}

% \subsubsection{longer video generation}
% \subsubsection{Visual Comparison}
% \subsubsection{Quantitative Results}



\subsection{Limitations}
Since our method is a training-free method based on the pre-trained video diffusion model, our motion prior might be influenced by the pre-trained video diffusion model. As shown in Fig.~\ref{fig:limitation}, we give the results of the successive frame of the generated illustration video. However, the generated dress might not be consistent in the generated results and does not show obvious movement. We argue that this is because of the issues of the motion prior in the pre-trained video diffusion model we use. A better latent diffusion model~\cite{hunyuanvideo, sora} might work better.


\input{figures/texs/limitation}



