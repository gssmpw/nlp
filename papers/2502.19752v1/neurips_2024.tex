\documentclass{article}


% if you need to pass options to natbib, use, e.g.:
\PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2024


% ready for submission
\usepackage[final]{neurips_2024}

\usepackage{hyperref}
\usepackage{url}  % DO NOT CHANGE THIS

% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
%\usepackage[preprint]{neurips_2024}


% to compile a camera-ready version, add the [final] option, e.g.:
%     \usepackage[final]{neurips_2024}


% to avoid loading the natbib package, add option nonatbib:
%    \usepackage[nonatbib]{neurips_2024}

\hypersetup{
colorlinks   = true, %Colours links instead of ugly boxes
urlcolor     = blue, %Colour for external hyperlinks
linkcolor    = blue, %Colour of internal links
citecolor   = blue %Colour of citations
}


\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors

%\usepackage[ruled,vlined]{algorithm2e}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\newcommand{\theHalgorithm}{\arabic{algorithm}}
\usepackage{algorithm}
%\usepackage{algorithmic}
\usepackage{algpseudocode}
\DeclareMathOperator*{\argmax}{\text{argmax}} %*****
\usepackage{caption} 
\captionsetup[table]{skip=10pt}
\usepackage{tabularx}
\usepackage{multirow}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}

\title{Probabilistic Federated Prompt-Tuning with\\ Non-IID and Imbalanced Data}


% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.


\author{%
  Pei-Yau Weng \\
  Washington State University \\
  \texttt{pei-yau.weng@wsu.edu} \\
  % examples of more authors
  \And
  Minh Hoang \\
  Princeton University \\
  \texttt{minhhoang@princeton.edu} \\
  \And
  Lam M. Nguyen \\
  IBM Research \\
  \texttt{lamnguyen.mltd@ibm.com} \\
  \And
  My T. Thai \\
  University of Florida \\
  \texttt{mythai@cise.uf.edu} \\
  \And
  Tsui-Wei Weng \\
  University of California San Diego \\
  \texttt{lweng@ucsd.edu} \\
  \And
  Trong Nghia Hoang\thanks{Corresponding authors: Pei-Yau Weng, Minh Hoang, Trong Nghia Hoang.}\\
  Washington State University\\
  \texttt{trongnghia.hoang@wsu.edu}
}

\begin{document}


\maketitle

\vspace{-6mm}
\begin{abstract}\vspace{-3mm}
Fine-tuning pre-trained models is a popular approach in machine learning for solving complex tasks with moderate data. However, fine-tuning the entire pre-trained model is ineffective in federated data scenarios where local data distributions are diversely skewed. To address this, we explore integrating federated learning with a more effective prompt-tuning method, optimizing for a small set of input prefixes to reprogram the pre-trained model's behavior. Our approach transforms federated learning into a distributed set modeling task, aggregating diverse sets of prompts to globally fine-tune the pre-trained model. We benchmark various baselines based on direct adaptations of existing federated model aggregation techniques and introduce a new probabilistic prompt aggregation method that substantially outperforms these baselines. Our reported results on a variety of computer vision datasets confirm that the proposed method is most effective to combat extreme data heterogeneity in federated learning.\vspace{-3mm} %We believe this is an important step towards leveraging pre-trained models in federated learning, bridging the gap between large, pre-trained models and their applications in private data settings.
\end{abstract}


\section{Introduction}
\label{sec:intro}

The proliferation of personal devices has transformed our data landscape into numerous federated systems with distinct resource constraints, data representations, distributions, and ownership. This has motivated the development of new machine learning (ML) paradigms that enable collaborative learning across different systems while respecting their data privacy. A prominent framework to substantiate this collaborative scheme is federated learning (FL), which allows multiple systems to train a common model without sharing their private data~\cite{DBLP:journals/corr/KonecnyMRR16,McMahan17,smith2017federated,DBLP:journals/corr/abs-1912-04977,DBLP:journals/corr/abs-1902-00146,pmlr-v97-yurochkin19a,NEURIPS2019_ecb287ff,NghiaUAI23a,NghiaUAI23b,NghiaAAAI19,MinhAAAI24,NghiaAIMagazine24}. 

Existing FL methods often assume that learning begins from scratch and does not build on prior expertise. On the other hand, fine-tuning pre-trained or foundation models~\cite{Bommasani2021FoundationModels} is an emerging paradigm for efficient generation of ML solutions. Almost all state-of-the-art models in natural language processing (NLP) are now fine-tuned versions of foundation models, such as BERT~\cite{devlin-etal-2019-bert}, BART~\cite{BART}, RoBERTa~\cite{RoBerta}, and T5~\cite{raffel2020exploring}. Likewise, many top performing vision models have also benefited from the generalization capability of foundation models, such as the vision transformer model~\cite{dosovitskiy2020image}, which was trained on large-scale and generic datasets such as ImageNet~\cite{deng2009imagenet}. Nonetheless, this fine-tuning practice has only recently been investigated in federated learning~\cite{nguyen2023begin,lu2023fedclip}.

Current research in this direction demonstrated various benefits of integrating fine-tuning into FL. For example, fine-tuning can utilize information better in decentralized data scenarios and thus improves the performance in various FL scenarios, mostly in NLP~\cite{FedBERT,zhang2023gptfl,kim-etal-2023-client,agarwal-etal-2023-practical}. Interestingly, it has been pointed out that even a simple initialization of local clients with a foundation model can prevent solution drift to some extent in large-scale scenarios with heterogeneous local data distributions~\cite{nguyen2023begin,lu2023fedclip,zhang2023gptfl}. This is also consistently observed in the results of our case studies in Fig.~\ref{fig:1}.

However, in FL environments that depend on frequent communication and synchronization of model updates across multiple devices, fine-tuning the entire pre-trained model is often infeasible due to limited local storage and communication bandwidth. Our study also reveals that full-model fine-tuning approaches fall short when the federated data are not only \textit{heterogeneous} but also \textit{imbalanced}. Our case studies in Fig.~\ref{fig:1} in particular show that when the local data are both heterogeneous and imbalance, federated learning with full-model adaptation suffers a huge performance drop, highlighting its instability in extreme data settings.
%it is crucial to note that the above benefits have mostly been observed when the entire large foundational models~\cite{lu2023fedclip} are fine-tuned or used to generate substantial amount of synthetic data~\cite{zhang2023gptfl}. In FL environments that depend on frequent communication and synchronization of client updates, fine-tuning the entire model is often infeasible due to limited (local) computation and communication bandwidth.

To address the resource bottleneck, several works in the recent federated learning literature have turned to a new class of parameter-efficient tuning method called prompt tuning. This method focuses on engineering cues, such as extra tokens which are appended to the input embedding in a transformer architecture. Such tokens or prompts provide beneficial context to performing the computational task, similar to how hints can be provided to assist puzzle solving. Local sets of prompts can then be aggregated or personalized using existing federated learning strategies. For example, the prior works of \cite{guo2022promptfl} and \cite{zhao2023fedprompt} use \textsc{FedAvg}~\cite{McMahan17} to aggregate the local prompts while \cite{Yang_2023_ICCV} integrates prompt-tuning into personalized federated learning, which helps generate client-specific sets of prompts that are well-customized to the corresponding local data distribution.
%\citet{lu2023fedclip} applies \textsc{FedAvg} to CLIP model

Although federated prompt-tuning approaches eliminate the need to update and communicate hundreds of millions of network weights, there is a substantial gap between the performance of these approaches in highly heterogeneous, data-imbalanced settings, and the upper-bound performance of fine-tuning in the centralized data setting, as shown in Fig.~\ref{fig:1}. This gap can be attributed to the fact that locally learned prompt sets are learned in arbitrary orders, which are generally unaligned across clients. That is, the same prompt position across different clients might encode different contextual information about the local data. A simple aggregation that disregards prompt alignment might attempt to combine prompts from different contexts and collapse into less informative prompts. This has not been explicitly modeled in prior federated prompt-tuning work.\vspace{-2.5mm}

\begin{figure}[h]
 \centering
  \includegraphics[width=0.7\columnwidth]{barplot.png}
%\vspace{-6mm}
  \caption{Test Accuracy ($\%$) achieved on the CIFAR-10 dataset by solving Eq.~\eqref{eq:FL_prompt} via centralizing data, using \textsc{FedAvg}, and using \textsc{FedProx} on (orange) full-model (FM) and (blue) prompt-tuning (PT) setups. The evaluation is performed under (left) a standard (non-extreme) heterogeneous data partition; and (right) an extremely imbalanced data partitioning scheme (see Section~\ref{sec:exp}).}
  \vspace{-2.5mm}
 \label{fig:1}
\end{figure}

To address this issue, we adopt a hierarchical probabilistic modeling approach to characterize both the generation and alignment of local prompts in this paper. We cast the server aggregation step as an alignment of local prompt sets. This alignment discovers and aggregates local prompts encoding similar contextual information into summarizing prompts. On the client side, we view each local prompt set as an independent sample drawn from a generative model parameterized by the global summarizing prompts. In this view, each local prompt can be seen as a probabilistic exploration initialized by a randomly selected summarizing prompt. The alignment between prompts can be characterized in terms of their association with the summarizing prompts, which can be inferred via learning the parameters of the above generative model. We summarize our main contributions below:

%Learning the defining parameters of this probabilistic model will enable the inference of the latent association between local and summarizing prompts, which characterize their alignment. 

%{\bf C1.} We inspect the potential of integrating FL into fine-tuning (Section~\ref{sec:prelim}). Our empirical studies show that such integration indeed improves the impact of fine-tuning effectively in federated scenarios with heterogeneous distributions of local data. Often, its achieved performance is quite close to the upper-limit performance of a state-of-the-art centralized model even when using a simple \textsc{FedAvg}~\cite{McMahan17} algorithm that was not designed to handle data heterogeneity. Such effect however subsides when local data classes become extremely imbalanced. It is even observed that in such situation, federated fine-tuning with the entire pre-trained model performs significantly worse than federated prompt-tuning.

{\bf C1.} We formulate the prompt summarizing procedure as a probabilistic set modeling task in which each local set is assumed to be an independent sample of a random point process. The alignment of similar prompts across different local sets can be set as part of the modeling parameterization, whose optimization can be interleaved with the optimization of the point process's parameters (Section~\ref{sec:model}).

%We develop a probabilistic method to explicitly model the generation and alignment of local prompt sets that are learned from a diverse range of imbalance datasets. Explicitly, we characterize the alignment between these prompt sets using a global set of summarizing prompts. Each summarizing prompt discovers and aggregates local prompts (across clients) that encode similar contextual information. Intuitively, the summarizing prompts encode all different aspects of context information across all clients, which can be leveraged to enable a more comprehensive fine-tuning of the pre-trained model (Section~\ref{sec:model}). 

%which formulates the prompt aggregation procedure as an unsupervised non-parametric clustering task (Section~\ref{sec:model}). 

{\bf C2.} We develop an algorithm to find the most probable association between the local and summarizing prompts. This association is viewed as a latent variable in the above generative model. Specifically, we cast its inference as a classical weighted bipartite matching task via an interesting observation that the association between the summarizing prompt and each local set of prompts participate linearly to the loss function of our proposed generative model (Section~\ref{sec:optimize}).

%An effective algorithm to solve this non-parametric clustering task is described in Section~\ref{sec:method}.

%The global fine-tuned model is synthesized via learning a succinct set of summarizing prompts, which aggregates similar local prompts while separating prompts that encode different concepts. Essentially, we formulate the prompt aggregation procedure as an unsupervised non-parametric clustering task, which learns the functional correspondence between existing and new prompts or recognizes new functional prompts. An effective algorithm to solve this non-parametric clustering task is described in Section~\ref{sec:method}.

{\bf C3.} We compare the performance of our method against various federated prompt-tuning baselines based on existing heterogeneous federated learning techniques to demonstrate its effectiveness. Our reported results on a variety of experiments and baselines demonstrate consistently that our method is most effective to combat data imbalance in extreme heterogeneous scenarios (Section~\ref{sec:exp}). \vspace{-2mm}

%A comprehensive review of the existing FL literature is also provided in Section~\ref{sec:related} to position of our contributions with respect to prior work.\vspace{-3mm}

%propose a new federated fine-tuning framework. In this framework, each local model is represented as a (learnable) set of re-programming prompts to a large, pre-trained model. Each prompt characterizes a distinct concept of fine-tuning. In each iteration, the global fine-tuned model is synthesized via learning a succinct set of summarizing prompts, which aggregates similar local prompts while separating prompts that encode different concepts (Section~\ref{sec:method}).

%Last, we develop a prompt summarizing model, which is able to detect new prompt concepts which are introduced across iterations, adapting the solution complexity non-parametrically to the observed data heterogeneity as needed. Essentially, we formulate the prompt aggregation procedure as an unsupervised non-parametric clustering task, which learns the functional correspondence between existing and new prompts or recognizes new functional prompts. An effective algorithm to solve this non-parametric clustering task is described in Section~\ref{sec:method}. Its effectiveness in federated settings with imbalanced local data distributions is demonstrated consistently across a variety of benchmark datasets in Section~\ref{sec:exp}.


\section{Related Work}
\label{sec:related}\vspace{-2mm}
Federated learning (FL) \cite{DBLP:journals/corr/KonecnyMRR16,McMahan17} is a collaborative framework that allows multiple parties to collaborate on training a common model without sharing their private data. In FL, the training data are distributed across $m$ clients. The $t$-th client owns a local, private dataset $D_t = \{(\mathbf{x}_{tm}, y_{tm})\}_{m=1}^{n_t}$ comprising $n_t$ data points. The goal is to fit a model $\mathbf{w}_\ast$ using all private datasets without centralizing them. That is, 
\begin{eqnarray}
\hspace{-9mm}\mathbf{w}_\ast \hspace{-2mm}&=&\hspace{-2mm} \underset{\mathbf{w}}{\arg\min}\left\{ L(\mathbf{w}) \triangleq \sum_{t=1}^m\left(\frac{n_t}{n}\right) \cdot L_t(\mathbf{w})\right\} \ , \ \text{where}\ \  L_t(\mathbf{w}) \ \triangleq\ \frac{1}{n_t} \sum_{m=1}^{n_t} \ell\big(\mathbf{x}_{tm}, y_{tm}; \mathbf{w}\big)
\hspace{-0mm} \label{eq:FL}
\end{eqnarray}
and $n = n_1 + n_2 + \ldots n_m$ denotes the total number of data points while $\ell(\mathbf{x}_{tm}, y_{tm}; \mathbf{w})$ denotes some loss function of choice. Clients collaborate via sharing their local models instead of data. Each client can run multiple local updates before sharing its local model for aggregation. This helps reduce the number of communication rounds while still preserving the convergence guarantee. \cite{McMahan17}~names this the \textsc{FedAvg} algorithm, which iterates between local optimization and global aggregation:
\begin{eqnarray}
\mathbf{w}^{(r)}_t &=& U\Big(\mathfrak{L}_t, \mathbf{w}^{(r-1)}_\ast\Big)\ \forall t \in [m] \ ,\quad \mathbf{w}^{(r)}_\ast \ \ = \ \ \sum_{t=1}^m \left(\frac{n_t}{n}\right) \cdot \mathbf{w}^{(r)}_t \ . \label{eq:local_global}
\end{eqnarray}
The local update routine $U(\mathfrak{L}_t, .)$ is typically standard gradient updates such as SGD or Adam. At the beginning of an iteration, the local weight is set to be the global estimate from the previous communication round. In practice, local data distributions tend to diverge which consequently causes the local updates in Eq.~\eqref{eq:local_global} to have different convergence points across clients. This is known as the solution drift phenomenon, which often decreases the performance of the federated model. Numerous approaches had been proposed to mitigate this phenomenon, which includes the following directions:

{\bf Client Regularization.} These methods focus on augmenting the local update strategies to prevent clients from drifting apart. For example, \textsc{FedProx}~\cite{pmlr-v139-li21h} introduces an $\ell_2$-regularization term to penalize updates that diverge the local model from the global model. \textsc{FedDyn}~\cite{feddyn_acar2021federated} uses a FL-adapted version of~\cite{shamir2014communication} on distributed optimization as the regularizer. \textsc{Scaffold}~\cite{pmlr-v119-karimireddy20a} attempts to correct the direction of local gradient updates at each client using their sum of gradients. \cite{MoonLiCVPR21} utilizes the similarity between model representations to correct local training via contrastive learning.

{\bf Server Regularization.} Another approach to prevent the drifting effect caused by heterogeneous local distributions is to replace the model average in Eq.~\eqref{eq:local_global} with a different mechanism of model aggregation. For example, \cite{NEURIPS2019_ecb287ff,pmlr-v97-yurochkin19a} decomposes client models into bags of neurons and performs a non-parametric clustering of neurons. Cluster centroids are used to synthesize an aggregated model for the next communication round. This approach bypasses the drifting phenomenon as the number of cluster is non-parametric and can be adjusted to accommodate new information patterns. In a similar vein, \cite{ChenICLR21} adopts a probabilistic perspective of model aggregation, sampling higher-quality global models and combining them via Bayesian model ensemble, leading to a more robust aggregation. \cite{Zhang_2022_CVPR} presents a data-free knowledge distillation methods for FL that help to train generators without compromising clients' data. \cite{kamp2021federated} redistributes each client's shared model to others for $b$ consecutive communication iterations, exposing it to various heterogeneous data sources before performing aggregation, curving their solution divergence.

{\bf Personalization.} Instead of learning a single, universal model, \cite{pmlr-v139-li21h,FallahNIPS20,pFedMeDinhNIPS20,yu2023multimodal,pmlr-v139-collins21a} seek to learn personalized models for all clients.  \cite{smith2017federated,yu2023multimodal} formulates this problem as multi-task learning, whereas \cite{FallahNIPS20} and \cite{pFedMeDinhNIPS20} respectively ground it in meta-learning and regularization frameworks. Several recent works impose a shared data representation across clients~\cite{pmlr-v139-collins21a,DBLP:journals/corr/abs-2003-13461,DBLP:journals/corr/abs-2001-01523,DBLP:journals/corr/abs-1912-00818,DBLP:journals/corr/abs-2002-05516}, which is optimized using existing FL techniques, and locally fine-tune a small classification head to achieve personalization. However, personalized models trained in this manner tend to only perform well on its corresponding local test set, and not on a comprehensive test set combining all local test data (see Section~\ref{sec:exp}).

Finally, most previous work assume that each FL client has sufficient data to adequately train its local model. This often contradicts real-world situations where data are scarce~\cite{ng2021federated,kamp2021federated,zhao2019multi,zhang2022fednilm,zhao2020network}. While data scarcity is not a new challenge of machine learning, it has not been thoroughly investigated in the context of FL. \cite{kamp2021federated}~points out that with limited, scarce data, local models often have bad qualities, and aggregating such models tend to result in poor global performance. Although fine-tuning pre-trained large models is an increasingly popular technique to combat data shortage, most existing FL works (including~\cite{kamp2021federated} which aims to address data scarcity) have not tried to leverage this resource. This motivates us to investigate prompt-tuning as a new FL paradigm.\vspace{-2mm}

\section{Probabilistic Federated Prompt-Tuning}
\label{sec:method}
First, Section~\ref{sec:prompt-tuning} provides a concise background on the standard prompt tuning technique in FL setting. Motivated by the result of our case study (see Fig.~\ref{fig:1}), Section~\ref{sec:model} further introduces a probabilistic federated prompt-tuning framework that aims to close up the performance gap between federated prompt-tuning and centralized full-model fine-tuning in data imbalance settings. Our framework characterizes each local model as a random set of prompts distributed by a hierarchical generative model. An effective optimization algorithm to learn these is then detailed in Section~\ref{sec:optimize}. An overall diagram featuring a bird-view of our framework is also provided in Fig.~\ref{fig:workflow}.\vspace{-4mm}

\begin{figure*}[ht!]
\centering
\begin{tabular}{c}
\includegraphics[width=0.99\textwidth]{pfpt.png}
\end{tabular}\vspace{-7mm}
\caption{Workflow of Probabilistic Federated Prompt Aggregation: (left) each client selects a subset of prompts from the global set of summarizing prompts using the prompt-selection mechanism adapted from~\cite{wang2022learning}, and fine-tune them using local data; and (right) the server collects all local prompt sets and updates the global summarizing prompts that aggregate similar local prompts. This is achieved by our proposed probabilistic federated prompt aggregation (\textsc{PFPT}) algorithm.}\vspace{-4mm}
\label{fig:workflow}
\end{figure*}

\subsection{Prompt-Tuning with Pre-Trained Model} 
\label{sec:prompt-tuning}
We use a pre-trained Vision Transformer~\cite{dosovitskiy2020image} in all our experiments. The pre-trained model is a composition $F_c \circ F_a \circ F_e$ where $F_c$ is the classification head, $F_a$ is the stack of attention blocks, and $F_e$ is the input embedding block. Keeping both $F_a$ and $F_e$ frozen, the local solution for each client $t$ can be represented as $F_{c,t} \circ F_a \circ (F_e \cup \boldsymbol{\omega})$ which applies a personalized prediction head on the output of the frozen attention block $F_a$ whose (set) inputs are the union of $F_e$ and a set $\boldsymbol{\omega}$ of reprogramming prompts~\cite{wang2022learning}. $\boldsymbol{\omega}$ is in turn a union of $k$ individual prompts $\boldsymbol{\omega} = \boldsymbol{\omega}_1 \cup \boldsymbol{\omega}_2 \cup \ldots \cup \boldsymbol{\omega}_k$. Each local solution is characterized by a tuple of $(F_{c,t}, \boldsymbol{\omega})$ comprising a personalized head $F_{c,t}$ and common prompt set $\boldsymbol{\omega}$. The FL framework in Eq.~\eqref{eq:FL} is now re-cast as\vspace{-1mm}
\begin{eqnarray}
\hspace{-4.5mm}\boldsymbol{\omega}_\ast \hspace{-3mm}&=&\hspace{-3mm} \arg\min_{\boldsymbol{\omega}}\left\{  \sum_{t=1}^m\left(\frac{n_t}{n}\right) \cdot L_t(\boldsymbol{\omega})\right\} \ \text{with}\  
L_t(\boldsymbol{\omega}) \triangleq\max_{F_{c,t}}\left\{\left(\frac{1}{n_t}\right) \cdot \sum_{m=1}^{n_t} \ell\Big(\mathbf{x}_{tm}, y_{tm}; F_{c,t}, \boldsymbol{\omega}\Big)\right\} \vspace{-1mm}\label{eq:FL_prompt} 
\end{eqnarray}
where $\ell(\mathbf{x}_{tm}, y_{tm}; F_{c,t}, \boldsymbol{\omega}) = \ell(\mathbf{x}_{tm}, y_{tm}; F_{c,t} \circ F_a \circ (F_e \cup \boldsymbol{\omega}))$ which makes explicit the leverage of the existing (pre-trained) expertises $(F_a, F_e)$. The optimal solution $\boldsymbol{\omega}_\ast$ of Eq.~\eqref{eq:FL_prompt} can be approximated using any of the existing federated learning algorithms, such as \textsc{FedAvg}~\cite{McMahan17} and \textsc{FedProx}~\cite{pmlr-v139-li21h}. %For this set of preliminary experiments, we will use the \textsc{FedAvg}~\cite{McMahan17} and \textsc{FedProx}~\cite{pmlr-v139-li21h} algorithm to demonstrate the difference performance characteristic of prompt-tuning and full-model adaptation across different data settings.

{\bf Remark.} There is an alternative approach to enabling light-weight federated fine-tuning. Instead of using prompts, approaches in this direction use a (learnable) adapter network that adapts the output of an intermediate (frozen) neural network segment before passing it to the rest of the (frozen) neural network. Local adapters across client can then be aggregated using \textsc{FedAvg}~\cite{lu2023fedclip}. This direction is however orthogonal to federated prompt-tuning, which is our main focus here. Further investigation into probabilistic methods for adapter aggregation would be a potential follow-up of our current work.\vspace{-2mm}

\subsection{Probabilistic Prompt Model}
\label{sec:model}
Intuitively, our federated prompt tuning framework is an analog to traditional FL on the space of prompts. Prior to each communication round, we suppose that each local client $t$, via prompt-tuning, has obtained a set of $n_t$ prompts $\boldsymbol{\omega}_t \triangleq \{\boldsymbol{\omega}_{t1}, \boldsymbol{\omega}_{t2}, \ldots, \boldsymbol{\omega}_{tn_t}\}$. As the participating clients upload their local prompt sets, the server aggregates the combined set and subsequently returns a set of $n$ summary prompts, denoted as $\boldsymbol{\Phi} \triangleq \{\boldsymbol{\phi}_i\}^n_{i=1}$. We will elaborate on this aggregation step in Section~\ref{sec:likelihood}. Similar to standard FL, this aggregated set is also distributed to all clients at the beginning of the next communication round. Each client then selects the most relevant prompts from this set for further fine-tuning. This sampling step follows a generative process described in Section~\ref{sec:prior}.


\subsubsection{Generative Model}
\label{sec:prior}
At the beginning of each communication round, each client constructs a set of $n_t$ prompt initializations given the server-broadcast summary $\boldsymbol{\phi}$. We model this construction by a random generative process. First, each local prompt initialization, $\boldsymbol{\omega}_{tk}$, is modeled as a sample drawn from a Gaussian with learnable neural parameterization,
\begin{eqnarray}
\boldsymbol{\omega}_{tk}&\sim& \mathbb{N}\Big(\boldsymbol{\psi}_{tk}, \mathrm{diag}\Big(\alpha(\boldsymbol{\psi}_{tk}; \boldsymbol{\gamma})\Big)\Big) \ , \vspace{-2mm}\label{eq:4.1}
\end{eqnarray}
where the diagonal covariance matrix are parameterized as the output of a neural net $\alpha$ (with weight $\gamma$) on the mean parameter $\boldsymbol{\psi}_{tk}$. The set of mean parameters $\boldsymbol{\psi}_t \triangleq \{\boldsymbol{\psi}_{tk}\}_{k=1}^{n_t}$ is in turn modeled as a random subset of the (server-broadcast) summarizing prompts $\boldsymbol{\phi}$, following a Bernoulli point process prior with a finite mixture as the base measure:
\begin{eqnarray}
\hspace{-2mm}Q_t(\boldsymbol{\psi}) &\sim& \mathrm{BeP}\left(\sum_{i=1}^n \sigma\Big(g\big(\boldsymbol{\phi}_i; \mathbf{w}\big)\Big) \cdot \mathbb{I}\Big(\boldsymbol{\psi} = \boldsymbol{\phi}_i\Big) \right) \ .  \label{eq:4.2}
\end{eqnarray}
Here, $\sigma$ denotes the sigmoid activation function, and $g$ is another deep-parameterized function parameterized by weight $\mathbf{w}$. By definition of the Bernoulli process, the sampled measure is given by:
\begin{eqnarray}
Q_t(\boldsymbol{\psi}) \hspace{-2mm} &=& \hspace{-2mm} \sum_{i=1}^n c_{i} \cdot \mathbb{I}\Big(\boldsymbol{\psi} = \boldsymbol{\phi}_i\Big) \ ,
%\quad \text{with}\quad \mathbb{P}_i(c_{i}) \ = \ \sigma\Big(g(\boldsymbol{\phi}_i; \mathbf{w})\Big)^{c_{i}} \cdot \left(1 - \sigma\Big(g(\boldsymbol{\phi}_i; \mathbf{w})\Big)\right)^{1 - c_{i}} \ ,
\label{eq:4.3}
\end{eqnarray}
where $c_{i} \in \{0,1\}$ is the outcome of a Bernoulli trial with bias $\sigma(g(\boldsymbol{\phi}_i; \mathbf{w}))$, that is:
\begin{eqnarray}
\mathbb{P}_i(c_{i}) &=& \sigma\Big(g(\boldsymbol{\phi}_i; \mathbf{w})\Big)^{c_{i}} \cdot \left(1 - \sigma\Big(g(\boldsymbol{\phi}_i; \mathbf{w})\Big)\right)^{1 - c_{i}} \ .
\end{eqnarray}
In layman term, this process simply means we will observe the result of a coin toss $c_i$ for every summarizing prompt $\boldsymbol{\phi}_i$. If this coin lands on head ($c_i = 1$), the summarizing prompt $\boldsymbol{\phi}_i$ will be included in the set of mean parameters. Vice versa, if this coin lands on tail, we will skip $\boldsymbol{\phi}_i$. Because of this, we remark that $n_t$ could be different from the number of prompts in the previous round.
%\noindent 

\subsubsection{Prompt Aggregation}
\label{sec:likelihood}
Given the above generative story, we can now describe our algorithm to find the set of summarizing prompts $\{\boldsymbol{\phi}_i\}_{i=1}^n$, which maximize the likelihood of observing local prompts $\{\boldsymbol{\omega}_t\}_{t=1}^m$. The resulting optimal set of summarizing prompts can then be used to re-program the pre-trained model, which correspond to the global fine-tuned solution. Assuming that each prompt captures a particular fine-tuning pattern or concept, summarizing the prompt sets characterizing the local solutions will allow local models to be aggregated on the concept level, naturally mitigating the solution drift effect caused by compounding impact of data heterogeneity and pre-trained weight interference. This is substantiated in the overall computation process below.

Let $z_{tk}^i \in \{0,1\}$ denotes whether the local prompt $\boldsymbol{\omega}_{tk}$ was sampled from a Gaussian centered at the $i$-th summarizing prompt $\boldsymbol{\phi}_i$. That is, the assignment variable $z_{tk}^i = 1$ if and only if there exists $k \in [n_t]$ such that $\boldsymbol{\psi}_{tk} = \boldsymbol{\phi}_i$. Each client $t$ can now be represented as $(\boldsymbol{\omega}_t, \mathbf{z}_t)$ where $\mathbf{z}_t = \{\mathbf{z}_t^i\}_{i=1}^n$, $\mathbf{z}_t^i = (z_{t1}^i, z_{t2}^i, \ldots, z_{tn_t}^i)$ and $\boldsymbol{\omega}_t = (\boldsymbol{\omega}_{t1}, \boldsymbol{\omega}_{t2}, \ldots, \boldsymbol{\omega}_{tn_t})$. 

Now, using the generative process specified in the previous section and all its defining parameters $\boldsymbol{\varphi} = (\mathbf{w}, \boldsymbol{\gamma}, \{\boldsymbol{\phi}_{i=1}^n\})$, the log likelihood of each client can be derived as follow,
\begin{eqnarray}
\log \mathbb{P}\left(\boldsymbol{\omega}_t, \mathbf{z}_t \mid \boldsymbol{\varphi}\right) &=& \log\mathbb{P}\left(\boldsymbol{\omega}_t \mid \mathbf{z}_t, \boldsymbol{\varphi}\right) \ +\  \log\mathbb{P}\left(\mathbf{z}_t \mid \boldsymbol{\varphi}\right) \ ,\label{eq:4.4}
\end{eqnarray}
where each of the summand on the right-hand side is computed below. First, given $(\mathbf{z}_t, \boldsymbol{\varphi})$,
\begin{eqnarray}
\hspace{-4mm}\mathbb{P}\Big(\boldsymbol{\omega}_t \mid \mathbf{z}_t, \boldsymbol{\varphi}\Big) \hspace{-3mm}&=&\hspace{-3mm} \prod_{k=1}^{n_t}\hspace{-0.5mm}\mathbb{N}\hspace{-0.5mm}\left(\boldsymbol{\omega}_{tk} \mid \boldsymbol{\psi}_{tk}, \mathrm{diag}\Big(\alpha\Big(\boldsymbol{\psi}_{tk}; \boldsymbol{\gamma}\Big)\Big) \right) \ \text{with}\ \boldsymbol{\psi}_{tk} \hspace{-0.5mm}=\hspace{-0.5mm} \Big(z_{tk}^1 \cdot \boldsymbol{\phi}_1 + \ldots + z_{tk}^n \cdot \boldsymbol{\phi}_n\Big) \label{eq:4.5}
\end{eqnarray}
Then, given $\boldsymbol{\varphi}$ and let $c_{ti} = z_{t1}^i + z_{t2}^i + \ldots + z_{tn_t}^i$,
\begin{eqnarray}
\mathbb{P}\Big(\mathbf{z}_t \mid \boldsymbol{\varphi}\Big) \hspace{-2mm}&=&\hspace{-2mm}\prod_{i=1}^n \Bigg(\sigma\Big(g(\boldsymbol{\phi}_i; \mathbf{w})\Big)^{c_{ti}}\Bigg) \ \times\ \prod_{i=1}^n\left( \Big(1 - \sigma\Big(g(\boldsymbol{\phi}_i; \mathbf{w})\Big)\Big)^{\big(1 - c_{ti}\big)}\right)\label{eq:4.6}
\end{eqnarray}
Eq.~\eqref{eq:4.5} follows from the Gaussian likelihood of local prompt in Eq.~\eqref{eq:4.1} with the additional identification form for $\boldsymbol{\psi}_{tk}$ which is computable if $\mathbf{z}_t$ is given. Eq.~\eqref{eq:4.6} on the other hand is the consequence of the Bernoulli process in Eq.~\eqref{eq:4.3}, which might not be immediately straight-forward. Its detailed derivation will be given in Appendix~\ref{app:a}.

\subsubsection{Overall Workflow}
\label{sec:workflow}
Using Eq.~\eqref{eq:4.5} and~\eqref{eq:4.6}, we have the following prompt aggregation formulation,
\begin{eqnarray}
\boldsymbol{\Phi}^\ast &=& \argmax_{\boldsymbol{\Phi}}\left\{\max_{\mathbf{w}, \boldsymbol{\gamma}, \mathbf{z}} \sum_{t=1}^m\log \mathbb{P}\Big(\boldsymbol{\omega}_t, \mathbf{z}_t \mid \mathbf{w},\boldsymbol{\gamma}, \boldsymbol{\Phi}\Big)\right\} \label{eq:4.7}
\end{eqnarray}
where $\boldsymbol{\Phi} = \{\boldsymbol{\phi}_i\}_{i=1}^n$, $\mathbf{z} = \{\mathbf{z}_t\}_{t=1}^m$, and $\boldsymbol{\gamma}$ is previously defined in Eq.~\eqref{eq:4.1}. The probability term on the right-hand side can be further expanded into computable terms using Eq.~\eqref{eq:4.4}, Eq.~\eqref{eq:4.5}, and Eq.~\eqref{eq:4.6}. Solving Eq.~\eqref{eq:4.7} is therefore the key step in our federated fine-tuning framework, whose overall flow is summarized in Alg.~\ref{alg:fed-prompt}. Our algorithm proceeds in multiple iterations. At each iteration, a set of $m$ clients is sampled. Each client selects the most relevant subset of summarizing prompts from the server using a mechanism adapted from continual visual prompt-tuning~\cite{wang2022learning}. The selected subset of summarizing prompts are then fine-tuned using local data (see lines $3$-$4$). The sets of fine-tuned prompts are subsequently uploaded to the server, which performs the aggregation via solving Eq.~\eqref{eq:4.7} (see line $6$). The aggregation step has two subtleties: (1) the total number of global prompts, $n = n_1 + \ldots + n_t$, is proportional to the total number of prompts across clients, making the entire algorithm non-parametric; and (2) once the optimal assignment parameters $\mathbf{z}$ are found via solving Eq.~\eqref{eq:4.7}, global prompts that were not assigned to any client will be removed (see lines $7$ and $8$). Intuitively, if the sampled clients are similar, their prompt sets will substantially overlap, which reduces the total number of distinct prompts selected by all clients. Otherwise, if the sampled clients are  dissimilar, there will be less overlapping and the union set of selected global prompts will expand, increasing the complexity of the resulting model to match the increased data heterogeneity.%\vspace{-3mm}

\begin{algorithm}[t]
\caption{Probabilistic Federated Prompt Tuning (PFPT)}
\label{alg:fed-prompt}
\textbf{input}: pre-trained model $F$, no. $\tau$ of iterations, no. $m$ of sampled clients per iteration\\
\textbf{output}: optimized set of prompts $\boldsymbol{\Phi}$
\begin{algorithmic}[1]
\State initialize global summarizing prompts $\boldsymbol{\Phi}$ 
\For{$s=1$ {\bfseries to} $\tau$}
\State sample $m$ clients with private datasets $\{D_t\}_{t=1}^m$
\For{$t=1$ {\bfseries to} $m$}       
\State $\boldsymbol{\omega}_t \leftarrow \mathrm{client}(F, D_t, \boldsymbol{\Phi})$ // local prompt-tuning -- optimizing $L_t(\omega_t)$ in Eq.~\eqref{eq:FL_prompt}
\EndFor
\State $\boldsymbol{\Phi}, \mathbf{z} \leftarrow \mathrm{server}(\{\boldsymbol{\omega}_t\}_{t=1}^m)$ // solving Eq.~\eqref{eq:4.7}
\State $\boldsymbol{\Phi} \leftarrow \Big\{\boldsymbol{\phi}_i \in \boldsymbol{\Phi} \mid \sum_{t=1}^m\sum_{k=1}^{n_t} z^i_{tk} > 0\Big\}$ // remove inactive prompts
\EndFor
\State\textbf{return} the set $\boldsymbol{\Phi}$ of optimal prompts
\end{algorithmic}%\vspace{-4mm}
\end{algorithm}%\vspace{-4mm}

\subsection{Optimization}
\label{sec:optimize}
Solving Eq.~\eqref{eq:4.7} above is however not trivial due to its mixed set of discrete/continuous variables. To sidestep this intractability, we instead formulate it as a bi-level optimization that alternates between two sub-tasks: (1)~optimizing $\mathbf{z}$ given $\mathbf{w}, \boldsymbol{\gamma}$ and $\boldsymbol{\Phi}$; and (2)~optimizing $\boldsymbol{\Phi}, \mathbf{w}, \boldsymbol{\gamma}$ given $\mathbf{z}$. Among which, solving sub-task (2) is straight-forward since it only requires being able to differentiate the expression in Eq.~\eqref{eq:4.5} and Eq.~\eqref{eq:4.6}, which is trivial if we use the current estimation of the (discrete) assignment parameters $\mathbf{z}$. On the other hand, solving sub-task (1) is seemingly prohibitive expensive because it involves optimizing the (discrete) assignment parameters $\mathbf{z}$. Fortunately, this can be mitigated by casting both Eq.~\eqref{eq:4.5} and Eq.~\eqref{eq:4.6} into a linear form with respect to the assignment parameter $\mathbf{z}$. Intuitively, it might be confusing why such linear form can be achieved at all given that $\mathbf{z}$ is input to a non-linear log probability function in Eq.~\eqref{eq:4.5} and Eq.~\eqref{eq:4.6}. However, we must recognize that for each client $t$, there is at most one local prompt from $t$ which can be associated with the $i^{\text{th}}$ summarizing prompt. This implies $\mathbf{z}_t^i \triangleq (z_{t1}^i, z_{t2}^i, \ldots, z_{tn_t}^i)$ is either a zero or one-hot vector. This is an important observation because a linear function of such zero or one-hot vector will remain linear regardless of any (non-linear) post-processing transformation. It is shown in Lemma~\ref{lem:1} below.
\begin{lemma}
\label{lem:1}
For any scalar function $g(\mathbf{r})$ and a binary vector $\boldsymbol{\zeta} = [\zeta_1, \zeta_2, \ldots, \zeta_n]$ such that $\zeta_i \in \{0,1\}$ and $\boldsymbol{\zeta}$ has at most one non-zero component, we have\vspace{-1.5mm}
\begin{eqnarray}
g\left(\sum_{i=1}^n\zeta_i \cdot \mathbf{r}_i\right) &=& \sum_{i=1}^n \Bigg(\zeta_i \cdot g(\mathbf{r}_i)\Bigg) \ +\ \Bigg(1 - \sum_{i=1}^n\zeta_i\Bigg) \cdot g(0)\label{eq:4.8}\vspace{-1mm}
\end{eqnarray}
with respect to any set $\{\mathbf{r}_i\}_{i=1}^n$ of valid inputs to $g(\mathbf{r})$.
\end{lemma}
\noindent The proof of Lemma~\ref{lem:1} is straight-forward. First, if there is no non-zero component, both sides of Eq.~\eqref{eq:4.8} evaluate to $g(0)$. Otherwise, suppose the only non-zero component appears at position $\kappa$, both sides of Eq.~\eqref{eq:4.8} will evaluate to $g(\mathbf{r}_\kappa)$. In both cases, Eq.~\eqref{eq:4.8} holds. Using Lemma~\ref{lem:1}, we can establish the desired linear forms for Eq.~\eqref{eq:4.6} and Eq.~\eqref{eq:4.7} which are its immediate consequences.

\begin{lemma}
\label{lem:2}
Let $\mathbb{P}(\boldsymbol{\omega}_t \mid \mathbf{z}_t, \boldsymbol{\varphi})$ defined as in Eq.~\eqref{eq:4.5}. Let $L_1(\mathbf{z}) = \sum_{t=1}^m \log \mathbb{P}(\boldsymbol{\omega}_t \mid \mathbf{z}_t, \boldsymbol{\varphi})$, considering $(\boldsymbol{\omega}_t,\boldsymbol{\varphi})$ as constants. We have
\begin{eqnarray}
L_1(\mathbf{z}) &=& \sum_{i=1}^n\sum_{t=1}^m\sum_{k=1}^{n_t} z_{tk}^i \cdot \log \mathbb{N}\left(\boldsymbol{\omega}_{tk}\hspace{-1mm}\mid\hspace{-1mm} \boldsymbol{\phi}_i, \mathrm{diag}\left(\alpha\Big(\boldsymbol{\phi}_i; \boldsymbol{\gamma}\Big)\right)\right)
\end{eqnarray}
which is linear in terms of the assignment parameter $\mathbf{z}$.
\end{lemma}

\begin{lemma}
\label{lem:3}
Let $\mathbb{P}(\mathbf{z}_t \mid \boldsymbol{\varphi})$ defined as in Eq.~\eqref{eq:4.6}. Let $L_2(\mathbf{z}) = \sum_{t=1}^m \log \mathbb{P}(\mathbf{z}_t \mid \boldsymbol{\varphi})$, considering $\boldsymbol{\varphi}$ as constants. Then, we have
\begin{eqnarray}
\hspace{-12mm}L_2(\mathbf{z}) \hspace{-2mm}&=&\hspace{-2mm}\sum_{i=1}^n\sum_{t=1}^m\sum_{k=1}^{n_t} z_{tk}^i \cdot \log \left(\frac{\sigma\Big(g(\boldsymbol{\phi}_i; \mathbf{w})\Big)}{1 - \sigma\Big(g(\boldsymbol{\phi}_i; \mathbf{w})\Big)}\right) \ +\ \sum_{i=1}^n\sum_{t=1}^m \log\left(1 - \sigma\Big(g(\boldsymbol{\phi}_i; \mathbf{w})\Big)\right)
\end{eqnarray}
which is linear in terms of the assignment parameter $\mathbf{z}$.
\end{lemma}

\noindent The detailed proof for Lemma~\ref{lem:2} and Lemma~\ref{lem:3} are deferred to Appendix~\ref{app:b}. Using these results, we can put together the overall optimization task for $\mathbf{z}$ while fixing the rest of the parameterization
\begin{eqnarray}
\mathbf{z}^\ast &=& \argmax_{\mathbf{z}} \left\{\sum_{t=1}^m \log \mathbb{P}\Big(\boldsymbol{\omega}_t, \mathbf{z}_t \mid \boldsymbol{\varphi}\Big)\right\} \ \ =\ \ \argmax_{\mathbf{z}} \Bigg\{L_1(\mathbf{z}) \ +\  L_2(\mathbf{z})\Bigg\} \label{eq:4.9}
\end{eqnarray}
which is a weighted linear optimization task. The second equality follows from Eq.~\ref{eq:4.4} and the results of Lemma~\ref{lem:2} and Lemma~\ref{lem:3}. Now, if we further choose to optimize $\mathbf{z}_t$ iteratively while fixing in addition $\mathbf{z}_{-t}$, Eq.~\eqref{eq:4.9} reduces to a weighted bipartite matching task, which can be solved effectively in $\mathbb{O}((\max_{t=1}^m n_t)^3)$ processing time using the Hungarian algorithm \cite{Kuhn55}. A detailed pseudo-code implementing the above scheme is deferred to Appendix~\ref{app:c}.\vspace{-2mm}

% {\bf Remark.} A similar weighted bipartite matching approach has been previously used to align neuron of feed-forward and convolutional nets in federated learning~\cite{pmlr-v97-yurochkin19a,NEURIPS2019_ecb287ff}. Our work can be considered a generalization of such prior work to federated prompt-tuning using a ViT backbone and a neural parameterization for the random point process prior over prompts~\ref{sec:prior}.

\section{Empirical Results}
\label{sec:exp}\vspace{-2mm}
This section presents our empirical studies on a variety of computer vision datasets, including CIFAR-10 and CIFAR-100~\cite{cifar10}, TinyImageNet~\cite{Ya2015} and a synthetic, diverse dataset created by pooling together the MNIST-M~\cite{MNIST-M}, Fashion-MNIST~\cite{FashionMNIst}, CINIC-10~\cite{Cinic-10} and MMAFEDB (available on Kaggle) datasets, which is referred to as the $4$-dataset.

Our experiments are conducted on two data settings: (a) Dirichlet-based heterogeneous partition following a previous setup in~\cite{pmlr-v97-yurochkin19a}; and (b) manual imbalanced data partition, as detailed below.%\vspace{-2mm}

\begin{table*}[t]
\begin{small}
\caption{Accuracy ($\%$) achieved on the CIFAR-10 dataset by \textsc{PFPT} and other baselines.}
\label{tab:3}
\centering
\resizebox{\textwidth}{!}{\begin{tabular}{|c|c|c|c|c|c|c|c|}
\hline
\ & \textsc{FedAvg-PT} & \textsc{FedProx-PT} & \textsc{Scaffold-PT} & \textsc{FedOpt-PT} & \textsc{PFedPG} & \textsc{GMM-PT} & \textsc{PFPT} (ours)\\ \hline
$\alpha = 0.5$ & $94.06 \pm 0.45$ & $94.26 \pm 0.34$ & $92.26 \pm 0.18$& $90.70 \pm 0.54$ & $88.00 \pm 0.22$ & $92.63 \pm 0.16$ & $\mathbf{94.39} \pm \mathbf{0.51}$\\  
$\alpha = 0.1$ & $93.05\pm 0.03$ & $93.05 \pm 0.23$ & $91.82 \pm 0.34$ & $88.20 \pm 0.39$ & $77.25 \pm 0.23$ & $92.68 \pm 0.28$ & $\mathbf{93.39} \pm \mathbf{0.22}$\\
\text{imbalance} & $91.21 \pm 0.16$ & $91.08 \pm 0.25$ & $87.06 \pm 1.27$ & $79.32 \pm 1.73$ & $43.16 \pm 0.27$ & $90.50 \pm 0.17$ & $\mathbf{91.45} \pm \mathbf{0.08}$\\
\hline 
\end{tabular}}
\end{small}
\end{table*}

\begin{table*}[h!]
\begin{small}
\caption{Accuracy ($\%$) achieved on the CIFAR-100 dataset by \textsc{PFPT} and other baselines.}
\label{tab:4}
\centering
\resizebox{\textwidth}{!}{\begin{tabular}{|c|c|c|c|c|c|c|c|}
\hline
\ & \textsc{FedAvg-PT} & \textsc{FedProx-PT} & \textsc{Scaffold-PT} & \textsc{FedOpt-PT} & \textsc{PFedPG-PT} & \textsc{GMM-PT} & \textsc{PFPT} (ours)\\ \hline
$\alpha = 0.5$ & $79.40 \pm 0.27$ & $79.21 \pm 0.31$ & $70.83 \pm 0.18$& $59.58 \pm 1.13$ & $43.47 \pm 0.39$ & $77.50 \pm 0.12$ & $\mathbf{80.24} \pm \mathbf{0.24}$\\  
$\alpha = 0.1$ & $74.50 \pm 0.46$ & $73.60 \pm 0.38$ & $69.94 \pm 0.63$ & $60.31 \pm 0.39$ & $29.46 \pm 0.80$ & $70.49 \pm 0.46$ & $\mathbf{75.08} \pm \mathbf{0.51}$\\
\text{imbalance} & $70.07 \pm 0.35$ & $70.67 \pm 0.25$ & $69.35 \pm 0.96$ & $58.86 \pm 0.77$ & $11.57 \pm 0.25$ & $66.95 \pm 0.46$ & $\mathbf{72.05} \pm \mathbf{0.93}$\\
\hline 
\end{tabular}}\vspace{-2mm}
\end{small}
\end{table*}


\begin{table*}[h!]
\begin{small}
\caption{Accuracy ($\%$) achieved on the TinyImageNet dataset by \textsc{PFPT} and other baselines.}\vspace{-2mm}
\label{tab:5}
\centering
\resizebox{\textwidth}{!}{\begin{tabular}{|c|c|c|c|c|c|c|c|}
\hline
\ & \textsc{FedAvg-PT} & \textsc{FedProx-PT} & \textsc{Scaffold-PT} & \textsc{FedOpt-PT} & \textsc{PFedPG} & \textsc{GMM-PT} & \textsc{PFPT} (ours)\\ \hline
$\alpha = 0.5$ & $86.38 \pm 0.18$ & $86.05 \pm 0.47$ & $78.70 \pm 0.33$& $61.55 \pm 0.28$ & $50.92 \pm 0.44$ & $84.73 \pm 0.20$ & $\mathbf{86.91} \pm \mathbf{0.14}$\\  
$\alpha = 0.1$ & $78.58 \pm 0.57$ & $79.19 \pm 0.28$ & $78.02 \pm 0.33$& $63.81 \pm 0.79$ & $34.34 \pm 0.10$ & $76.90 \pm 0.28$ & $\mathbf{82.31} \pm \mathbf{0.26}$\\  
imbalance & $75.21 \pm 0.73$ & $75.49 \pm 0.38$ & $76.88 \pm 0.45$& $64.33 \pm 0.61$ & $11.92 \pm 0.13$ & $73.85 \pm 0.36$ & $\mathbf{78.21} \pm \mathbf{1.25}$\\  
\hline %\vspace{-2mm}
\end{tabular}}\vspace{-5mm}
\end{small}
\end{table*}

\begin{table*}[t]
\begin{small}
\caption{Accuracy ($\%$) achieved on the synthetic $4$-dataset, which combines data from MNIST-M~\cite{MNIST-M}, Fashion-MNIST~\cite{FashionMNIst}, CINIC-10~\cite{Cinic-10}, and MMAFEDB$^{1}$, by \textsc{PFPT} and other baselines.}\vspace{-2mm}
\label{tab:7}
\centering
\resizebox{\textwidth}{!}{\begin{tabular}{|c|c|c|c|c|c|c|c|}
\hline
\ & \textsc{FedAvg-PT} & \textsc{FedProx-PT} & \textsc{Scaffold-PT} & \textsc{FedOpt-PT} & \textsc{PFedPG} & \textsc{GMM-PT} & \textsc{PFPT} (ours)\\ \hline
$\alpha = 0.5$ & $59.48 \pm 0.59$ & $59.50 \pm 0.70$ & $54.51 \pm 1.56$& $40.69 \pm 2.29$ & $30.54 \pm 0.50$ & $57.05 \pm 1.04$ & $\mathbf{76.89} \pm \mathbf{0.17}$\\  
$\alpha = 0.1$ & $48.74 \pm 0.63$ & $48.71 \pm 2.13$ & $46.65 \pm 3.17$& $43.51 \pm 0.78$ & $25.40 \pm 0.17$ & $46.82 \pm 2.80$ & $\mathbf{70.29} \pm \mathbf{0.32}$\\  
imbalance & $39.28 \pm 0.55$ & $39.65 \pm 1.89$ & $23.79 \pm 1.24$& $44.43 \pm 1.78$ & $22.54 \pm 0.32$ & $41.37 \pm 1.12$ & $\mathbf{62.23} \pm \mathbf{1.02}$\\  
\hline 
\end{tabular}}\vspace{-2mm}
\end{small}
\end{table*}
{\bf Heterogeneous Partition.} We partition the training data into $m$ subsets (for $m$ clients). Each client has observations of all classes but the distributions of classes across clients are different. We simulate this using a $\mathrm{Dirichlet}(\alpha \cdot \mathbf{1}_s)$ distribution over an $s$-dimensional simplex where $s$ is the number of classes and $\alpha$ is the concentration parameter. 

Our experiments are conducted with $\alpha = 0.1$ and $\alpha = 0.5$. For each client $r$, we drawn a sample $\mathbf{p}_r \sim \mathrm{Dirichlet}(\alpha \cdot \mathbf{1}_s)$ where $\mathbf{p}_{r,c} \in (0,1)$ specifies the percentage of examples in class $c$ to be assigned to client $r$. For the CIFAR-10, CIFAR-100, and TinyImageNet datasets, we set $m = 100$. For the $4$-dataset, we simulate $20$ partitions for each of the $4$ sub-dataset using $\mathrm{Dirichlet}(\alpha \cdot \mathbf{1}_s)$ with $s = 10, 10, 10$, and $7$ for MNIST-M, FASHION-MNIST, CINIC-10, and MMAFEDB. This amounts to a total of $80$ clients with diverse and heterogeneous data distribution.

{\bf Imbalance Partition.} For the CIFAR-10 ($10$-class), CIFAR-100 ($100$-class) and TinyImageNet ($200$-class) datasets, the training data of each client is set to be dominated by a particular subset of classes, which amounts to $10\%$ of the total number of classes. $99\%$ of the local dataset of each client is set to belong to a certain $10\%$ of the total number of classes. For our synthetic $4$-dataset, we partition each of the $4$ sub-dataset into $20$ subsets. Each subset is set so that $99\%$ of its data points belong to a single class. The remaining $1\%$ data of each class are then pooled together and evenly distributed among all clients. This amounts to $80$ clients with extremely imbalance local datasets.

The above schemes are applied only on the train partition of each dataset. We use the default train/test partition for CIFAR-10, CIFAR-100, and TinyImageNet. For the $4$-dataset, we sample $30$K data points from the default train partition of each of its $4$ sub-datasets. We also sample $2.5$K data points from the default test partition of each sub-dataset. This amounts to a synthetic dataset with $120$K data points in the train partition and $10$K data points in the test partition.

For each data setup on each dataset, we compare the performance of our probabilistic federated prompt-tuning (\textsc{PFPT}) algorithm with those of a representative set of state-of-the-art federated learning algorithms adapted to the prompt-tuning setting in Eq.~\eqref{eq:FL_prompt}, which include \textsc{FedAvg}~\cite{McMahan17}, \textsc{FedProx}~\cite{pmlr-v139-li21h}, \textsc{Scaffold}~\cite{pmlr-v119-karimireddy20a}, \textsc{FedOpt}~\cite{asad2020FedOpt} and \textsc{PFedPG}~\cite{Yang_2023_ICCV}. \textsc{PFedPG} is a personalization method that is originally measured based on how well its personalized models perform on their corresponding local test sets. In this context, the performance target is however set on a global test set so we use \textsc{PFedPG}'s common model for evaluation. We will refer to these adapted baselines as \textsc{FedAvg-PT}, \textsc{FedProx-PT}, \textsc{Scaffold-PT}, \textsc{FedOpt-PT}, and \textsc{PFedPG}. In addition, we also compare \textsc{PFPT} against a simple prompt clustering baseline, which replaces the prompt averaging of \textsc{FedAvg} by a Gaussian Mixture Model (\textsc{GMM}) clustering in which the cluster centroids are returned as the aggregated prompts. We refer to this baseline as \textsc{GMM-PT}. All results are averaged over $5$ independent runs and reported in Tables~\ref{tab:3} to~\ref{tab:7} below. The result of each run is evaluated on a global test set, which comprises the entire test partition.

\begin{figure*}[t]
\centering
\begin{tabular}{c}
\includegraphics[width=0.85\textwidth]{PFPT-visualize-touch-up.jpg}\vspace{-2mm}
\end{tabular}
\caption{t-SNE plots of 
the (learned) summarizing prompts of \textsc{PFPT} on CIFAR-100 over $120$ communication iterations with different heterogeneity settings. Yellow triangles denote the centroids of the t-SNE embeddings of the prompts. The dashed red line visualizes their trajectories.}\vspace{-1mm}
\label{fig:tsne_cifar100}
\end{figure*}

\begin{figure}[h!]
\centering
\begin{tabular}{c}
\includegraphics[width=0.8\textwidth]{cifar100-5.png}\vspace{-2mm}
\end{tabular}
\caption{Variations in CIFAR-100 global prompt pool size across $120$ communication rounds under different heterogeneity settings.}\vspace{-2mm}
\label{fig:poolsize_cifar100}
\end{figure}

\subsection{Non-IID Data with Locally Skewed Class Distributions} 
\label{sec:local-skew}
Table~\ref{tab:3} reports the performance of \textsc{PFPT} and various federated prompt-tuning baselines on the CIFAR-10 dataset. In all three settings (i.e., heterogeneous partitions with $\alpha=0.5$ and $0.1$, as well as the imbalance partition), \textsc{PFPT} consistently achieves the best performance. On average, the classification accuracy of our method improves by $0.3\%$ over the closest competitors, \textsc{FedAvg-PT} and \textsc{FedProx-PT}. While this gain seems modest, we note that the prompt tuning performance tends to fall off more significantly on several FL frameworks that were devised to counteract the effect of imbalanced data, such as \textsc{Scaffold-PT} and \textsc{FedOpt-PT}. In fact, the accuracy our method respectively improves by $2.7\%$ and $7.0\%$ over these baselines. This result clearly suggests that prompt tuning performance is more vulnerable to the heterogeneity challenge in FL, and thus requires a more specialized treatment. Finally, we observe that \textsc{PFedPG} achieves the worst performance among all baselines ($23.6\%$ worse than \textsc{PFPT} on average). This performance gap is especially pronounced on the imbalance setting. While \textsc{PFedPG} claimed to deal with both heterogeneity and prompt-tuning, this result is not surprising since it assumes that the local test sets are partitioned similarly to the local training sets, whereas our results are obtained on a holistic global test set.

We further repeat this experiment on the CIFAR-100 and TinyImageNet datasets, which are both more challenging than CIFAR-10 due to the increased number of classes (i.e., $100$ and $200$ classes respectively). The results of these experiments are reported in Table~\ref{tab:4} and~\ref{tab:5}. As expected, our method still consistently outperforms other baselines. However, the performance gap between \textsc{Pfpt} and other methods tends to widen proportionately to the increased difficulty of the dataset. For example, in the CIFAR-100 experiment, our method is $1.2\%$ better than the second best baseline (\textsc{FedAvg-PT}) and $47.6\%$ better than the worst baseline (\textsc{PFedPG}). 

In the TinyImageNet experiment, our method is $2.4\%$ better than \textsc{FedAvg-PT} and $50.1\%$ better than \textsc{PFedPG}. These results are therefore consistent with our findings in Table~\ref{tab:3}. Finally, Table~\ref{tab:7} records the prompt tuning performance of the same baselines on the most challenging scenario, which combines $4$ datasets from completely different vision domains. We observe a significant performance gap between \textsc{PFPT} and baselines that have been keeping up with its performance in earlier experiments, such as \textsc{FedAvg-PT} (now $20.6\%$ worse) and \textsc{FedProx-PT} (now $20.5\%$ worse). More interestingly, \textsc{GMM-PT}, which has performed on par with our method in previous experiments, observes a significant drop in performance (now $21.4\%$ worse). These extremely wide performance gaps are due to the fact that local prompts are more diverse (to account for highly heterogeneous tasks), which require a more refined alignment technique to facilitate effective aggregation.\vspace{-2mm}
\subsection{Non-IID Data with Globally Skewed Class Distributions}
\label{sec:global-skew}
In addition to the above setting which features non-IID data and locally skewed class distribution, we also evaluated the performance of our framework in a more adverse setting where the class distribution will remain skewed even if all local datasets are aggregated. That is, local datasets in this setting are non-IID draws from a long-tailed data distribution parameterized by an imbalance factor $\mathrm{IF} = (\max_c n_c) / (\min_c n_c)$ characterizing the ration between sample sizes ($n_c$) of the most frequent and least frequent classes~\cite{shang2022FEDIC,shang2022CReFF}. Such (global) long-tailed datasets are synthetically created from the original CIFAR-100 and ImageNet datasets following the data simulation in~\cite{Cao2019LearningID}. Following previous experiment setup in previous (non-ViT and no prompt-tuning) baseline methods FEDIC~\cite{shang2022FEDIC} and CReFF~\cite{shang2022CReFF}~\footnote{These federated fine-tuning baselines use full fine-tuning with ResNet as their backbone. Both baselines were not evaluated in our main setting in Section~\ref{sec:local-skew}.}, the resulting long-tailed datasets (CIFAR-100-LT and ImageNet-LT) are configured to have $\mathrm{IF} = 10, 50, 100$ for CIFAR-100 and $\mathrm{IF} = 1280/5$ for ImageNet. The reported results in Table~\ref{tab:g5} show that our method is also highly effective in this setting, achieving significantly higher accuracy than the current SOTA~\cite{shang2022FEDIC,shang2022CReFF} across all datasets and imbalance setups.\vspace{-2mm}

%~\footnote{These federated fine-tuning baselines use full fine-tuning with ResNet as their backbone. Both baselines were not evaluated in our main setting in Section~\ref{sec:local-skew}.}

\begin{table*}[h!]
\centering
%\begin{small}
\caption{Accuracy ($\%$) achieved on long-tailed datasets by \textsc{PFPT}, \textsc{CReFF}~\cite{shang2022CReFF} and \textsc{FEDIC}~\cite{shang2022FEDIC}.}\vspace{-2mm}
\label{tab:g5}
\begin{tabular}{|l|lll|c|}
\hline
\multicolumn{1}{|c|}{\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Non-IID ($\alpha = 0.1$)\end{tabular}}} & \multicolumn{3}{c|}{CIFAR-100-LT}                                 & \multicolumn{1}{c|}{\multirow{2}{*}{ImageNet-LT ($\mathrm{IF} \ =\ 1280 / 5$)}} \\ \cline{2-4}
\multicolumn{1}{|c|}{}                                                                         & \multicolumn{1}{l|}{$\mathrm{IF} \ \ =\ \  100$} & \multicolumn{1}{l|}{$\mathrm{IF} \ \ =\ \ 50$} & $\mathrm{IF} \ \ = \ \ 10$ & \multicolumn{1}{c|}{}                             \\ \hline
\textsc{FEDIC}                                                                                          & \multicolumn{1}{l|}{33.67}  & \multicolumn{1}{l|}{34.74} & 41.93 & 28.93                                             \\ \hline
\textsc{CReFF}                                                                                          & \multicolumn{1}{l|}{26.19}  & \multicolumn{1}{l|}{28.32} & 35.49 & 26.31                                             \\ \hline
\textsc{PFPT} (ours)                                                                                           & \multicolumn{1}{l|}{$\mathbf{60.74}$}  & \multicolumn{1}{l|}{$\mathbf{65.54}$} & $\mathbf{71.66}$ & $\mathbf{75.54}$                                             \\ \hline
\end{tabular}\vspace{-5mm}
%\end{small}
\end{table*}

\subsection{Prompt Convergence and Diversity}%\vspace{-2mm}
To generate insights regarding the learned prompts, we plot the $2$-dimensional t-SNE embeddings of all CIFAR-100 global prompts discovered by our method across $120$ communication rounds. This is repeated for the two heterogeneity and one data imbalance FL settings as well as the centralized data setting (Fig.~\ref{fig:tsne_cifar100}). The yellow triangles in each plot mark the corresponding centroids of the prompt embeddings (one per $10$ communication iterations). We also plot a best-fit spline curve (dashed and colored in red) through these centroids to visualize their update trajectory. In all settings, the distance between successive centroids consistently gets smaller as training progresses, which suggests that the learned prompts generally converge well. This is also corroborated by Fig.~\ref{fig:poolsize_cifar100}, which shows that the number of global prompts also converges over $120$ communication iterations. It is also observed that in the centralized learning scenario in which no heterogeneity is present, the convergence happens much faster. The prompts quickly converge within $10$ communication iterations. We also observe a gradual increase in the spread of the prompts with respect to their centroid. This suggests that the learned prompts are optimized by our method to capture the data diversity across participating clients. More interestingly, as we move from the standard heterogeneity settings (with $\alpha = 0.1$ and $\alpha = 0.5$) to the extremely heterogeneous setting with imbalance (local) data, the magnitude of this spread becomes larger, confirming the strong correlation between task heterogeneity and prompt diversity.\vspace{-2mm}

\section{Conclusion}
\label{sec:conclude}\vspace{-2mm}
Our paper presents a new and effective approach to address the challenges of prompt-tuning pre-trained models in federated data scenarios with diverse local data distributions. Our proposed approach is a hierarchical probabilistic framework that models server aggregation as a non-parametric alignment of locally learned prompt sets into summarizing prompts. In each subsequent communication round, local clients sample and perform local updates on relevant prompts from the previous set of summarizing prompts. In this manner, our approach bypasses the effect of solution drift and outperforms existing federated learning techniques (applied on prompt-tuning) on various computer vision datasets. The reported results emphasize the cost-efficiency and efficacy of our approach in combating data heterogeneity within extremely diverse federated scenarios. %We believe this research represents a significant step forward in leveraging pre-trained models in federated learning, providing a valuable contribution to bridging the gap between foundational models and their applications in private data settings.

\textbf{Acknowledgements and Disclosure of Funding.}
This work used GPU compute resource at SDSC through allocation CIS230391 from the Advanced Cyberinfrastructure Coordination Ecosystem: Services and Support (ACCESS) program~\cite{ACCESS-resource}, which is supported by U.S. National Science Foundation grants $\#$2138259, $\#$2138286, $\#$2138307, $\#$2137603, and $\#$2138296. My T. Thai acknowledges the support of National Science Foundation grants SCH-2123809 and III-2416606. T.-W. Weng is supported by National Science Foundation awards CCF-2107189, IIS-2313105, IIS-2430539, the Hellman Fellowship, and Intel Rising Star Faculty Award.

\bibliography{neurips_2024/neurips_2024}
\bibliographystyle{unsrtnat}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix

{\bf Code Release.}~Our experimental code is released and maintained at

\url{https://github.com/PeiYauWeng/PFPT}.

\section{Broader Statement of Impact}
\label{app:impact}
This research focuses on developing an effective prompt-tuning and summarizing algorithm for federated learning scenarios featuring a set of clients with diversely skewed local data. The mathematical approaches and insights developed in this paper will help bridge the gap between large, pre-trained models and their applications in private data settings with extremely skewed data distribution. While applications of our work to real data could result in ethical considerations, this is an indirect (and unpredictable) side-effect of our work. Our experimental work uses publicly available datasets to evaluate the performance of our algorithms; no ethical considerations are raised.

\iffalse
\section{Federated Fine-Tuning Workflow}
\label{app:d}
Fig.~\ref{fig:workflow} provides a bird-view of our probabilistic federated prompt-tuning framework. %This is visualized in two diagrams: (1) a server-client diagram which emphasizes on the macro interaction between server and clients, featuring the main steps in prompt summarizing and aggregation; and (2) a local-training diagram which features key steps of the local fine-tuning operation on a pre-trained ViT~\cite{dosovitskiy2020image} model backbone, such as how prompts are selected from the server and incorporated into the local model for further fine-tuning.

\begin{figure*}[ht!]
\centering
\begin{tabular}{c}
\includegraphics[width=0.99\textwidth]{plots/pfpt.png}
\end{tabular}
\caption{Workflow of Probabilistic Federated Prompt Aggregation: (left) each client selects a subset of prompts from the global set of summarizing prompts using the prompt-selection mechanism adapted from~\cite{wang2022learning}, and fine-tune them using local data; and (right) the server collects all local prompt sets and updates the global summarizing prompts that aggregate similar local prompts. This is achieved by our proposed probabilistic federated prompt aggregation (\textsc{PFPT}) algorithm.}
\label{fig:workflow}
\end{figure*}
\fi

\iffalse
\section{Parameter Complexity}
\label{app:e}
We also provide the parameter complexity of the baseline algorithms. For simplicity, the parameter complexity is measured by the total number of tunable parameters, and equivalently their corresponding ratio to the total number of parameters ($88,749,554$) in a prompted ViT~\cite{dosovitskiy2020image}. This is reported in Table~\ref{tab:e1} below. Our method expectedly requires tuning fewer parameters than $\textsc{PFEDPG}$ which has to maintain in addition a separated set of personalized prompts for each client. On the other hand, our method has to tune approximately twice the number of parameters of other baselines. However, this is a reasonable overhead given the significant performance improvement of our method over those baselines. 

\begin{table*}[ht]
\begin{small}
\caption{Parameter statistics: (top) the number of tunable parameters of each baseline algorithm; and (bottom) the corresponding ratio of tunable parameters to total number of parameters in the ViT~\cite{dosovitskiy2020image} backbone.} 
\label{tab:e1}
\centering
\resizebox{\textwidth}{!}{\begin{tabular}{|c|c|c|c|c|c|c|c|}
\hline
\ & \textsc{FedAvg-PT} & \textsc{FedProx-PT} & \textsc{Scaffold-PT} & \textsc{FedOpt-PT} & \textsc{PFedPG} & \textsc{GMM-PT} & \textsc{PFPT} \\ \hline
$\text{No. tunable params}$ & $525,322$ & $525,322$ & $525,322$& $525,322$ & $2,183,434$ & $525,322$ & $1,122,826$\\  
$\text{Percentage (\%)}$ & $0.5919\%$ & $0.5919\%$ & $0.5919\%$ & $0.5919\%$ & $2.4602\%$ & $0.5919\%$ & $1.2651\%$\\
\hline 
\end{tabular}}
\end{small}
\end{table*}
\fi

\section{Derivation of \texorpdfstring{Eq.~\eqref{eq:4.6}}{Eq. (10)}}
\label{app:a}
Note that $\mathbf{z}_t = \{\mathbf{z}_t^i\}_{i=1}^n$ which is a set of independent random vector given $\boldsymbol{\varphi}$. Hence,
\begin{eqnarray}
\mathbb{P}\left(\mathbf{z}_t \mid \boldsymbol{\varphi}\right) &=& \prod_{i=1}^n \mathbb{P}\left(\mathbf{z}_t^i \mid \boldsymbol{\varphi}\right) \ \ =\ \ \prod_{i=1}^n\prod_{k=1}^{n_t} \mathbb{P}\left(z_{tk}^i \mid \boldsymbol{\varphi}\right)  \label{eq:a1} 
\end{eqnarray}
where the last equality follows from the fact that $\mathbf{z}_t^i = (z_{t1}^i, z_{t2}^i, \ldots, z_{tn_t}^i)$ which is in turn a set of independent Bernoulli variables $z_{tk}^i \in \{0, 1\}$ indicating whether the local prompt $\boldsymbol{\omega}_{tk}$ was sampled from a Gaussian centered at the $i$-th summarizing prompt $\phi_i$. Furthermore, following the Bernoulli process definition in Eq.~\eqref{eq:4.3}, the $i$-th summarizing prompt $\phi_i$ is selected independently with probability $\sigma(g(\phi_i; \mathbf{w}))$ to sample the $k$-th local prompt $\boldsymbol{\omega}_{tk}$ via setting $\boldsymbol{\psi}_{tk} = \phi_i$ in Eq.~\eqref{eq:4.1}. Hence, 
\begin{eqnarray}
\mathbb{P}\left(z_{tk}^i \mid \boldsymbol{\varphi}\right) &=& \sigma\Big(g(\phi_i; \mathbf{w})\Big)^{z_{tk}^i} \cdot \left(1 - \sigma\Big(g(\phi_i; \mathbf{w})\right)\Big)^{1 - z_{tk}^i} \label{eq:a2}
\end{eqnarray}
Plugging Eq.~\eqref{eq:a2} into Eq.~\eqref{eq:a1} and using $c_{ti} = z_{t1}^i + z_{t2}^i + \ldots + z_{tn_t}^i$ result in Eq.~\eqref{eq:4.6}.

\section{Proof of Lemmas~\ref{lem:2} and~\ref{lem:3}}
\label{app:b}
{\bf Lemma~\ref{lem:2}.} To derive the result of Lemma~\ref{lem:2}, note that Eq.~\eqref{eq:4.5} implies
\begin{eqnarray}
\hspace{-21mm}\log\mathbb{P}\left(\boldsymbol{\omega}_t \mid \mathbf{z}_t, \boldsymbol{\varphi}\right) \hspace{-2mm}&=&\hspace{-2mm} \sum_{k=1}^{n_t} \log\mathbb{N}\left(\boldsymbol{\omega}_{tk} \ \Bigg|\ \sum_{i=1}^n z_{tk}^i \cdot \boldsymbol{\phi}_i,\mathrm{diag}\left(\alpha\left(\sum_{i=1}^n z_{tk}^i \cdot \boldsymbol{\phi}_i;\boldsymbol{\gamma}\right)\right)\right) \nonumber\\
\hspace{-2mm}&=&\hspace{-2mm} \sum_{k=1}^{n_t} g_k\left(\sum_{i=1}^n z_{tk}^i \cdot\boldsymbol{\phi}_i\right)  \ , \label{eq:b1}  
\end{eqnarray} 
where we define
\begin{eqnarray}
g_k\left(\mathbf{r}\right) &\triangleq& \log\mathbb{N}\left(\boldsymbol{\omega}_{tk} \ \Bigg | \ \mathbf{r}, \mathrm{diag}\Big(\mathbf{r}; \boldsymbol{\gamma}\Big)\right)  \ . \label{eq:b2}  
\end{eqnarray}
In addition, since $\sum_i z_{tk}^i = 1$ with $z_{tk}^i \in \{0,1\}$, Lemma~\ref{lem:1} implies
\begin{eqnarray}
g_k\left(\sum_{i=1}^n z_{tk}^i \cdot\boldsymbol{\phi}_i\right) &=& \sum_{i=1}^n z_{tk}^i \cdot g_k(\boldsymbol{\phi}_i) \ ,\label{eq:b3}   
\end{eqnarray}
which can be plugged into Eq.~\eqref{eq:b1} to arrive at
\begin{eqnarray}
\log\mathbb{P}\left(\boldsymbol{\omega}_t \mid \mathbf{z}_t, \boldsymbol{\varphi}\right) &=& \sum_{k=1}^{n_t}     g_k\left(\sum_{i=1}^n z_{tk}^i \cdot\boldsymbol{\phi}_i\right) \ \ =\ \ \sum_{k=1}^{n_t}\sum_{i=1}^n z_{tk}^i \cdot g_k(\boldsymbol{\phi}_i) \\ 
&=& \sum_{k=1}^{n_t}\sum_{i=1}^n z_{tk}^i \log\mathbb{N}\left(\boldsymbol{\omega}_{tk} \ \Big |\ \boldsymbol{\phi}_i, \mathrm{diag}\left(\boldsymbol{\phi}_i; \boldsymbol{\gamma}\right)\right) \ , \label{eq:b4}
\end{eqnarray}
where the last equality follows from the definition of $g_k$ in Eq.~\ref{eq:b3}. Finally, taking summation over $t = 1, 2, \ldots, n$ on both sides of Eq.~\eqref{eq:b4} and using the definition of $\mathfrak{L}_1$, we arrive at Lemma~\ref{lem:2}.

{\bf Lemma~\ref{lem:3}.} To derive the result of Lemma~\ref{lem:3}, we start with taking log on both sides of Eq.~\eqref{eq:4.6}, which results in
\begin{eqnarray}
\hspace{-8mm}\log\mathbb{P}\left(\mathbf{z}_t \mid \boldsymbol{\varphi}\right) &=& \sum_{i=1}^n c_{ti} \cdot \log\sigma\Big(g\left(\boldsymbol{\phi}_i; \mathbf{w}\right)\Big) + \sum_{i=1}^n \Big(1 - c_{ti}\Big)\cdot \log\Big(1 - \sigma\Big(g\left(\boldsymbol{\phi}_i; \mathbf{w}\right)\Big)\Big) \\
&=& \sum_{i=1}^n c_{ti} \cdot \log\left(\frac{\sigma\Big(g\left(\boldsymbol{\phi}_i; \mathbf{w}\right)\Big)}{1 - \sigma\Big(g\left(\boldsymbol{\phi}_i; \mathbf{w}\right)\Big)}\right) + \sum_{i=1}^n \log \Big(1 - \sigma\Big(g\left(\boldsymbol{\phi}_i; \mathbf{w}\right)\Big)\Big)\\
&=&\sum_{i=1}^n\sum_{k=1}^{n_t} z_{tk}^i \cdot \log\left(\frac{\sigma\Big(g\left(\boldsymbol{\phi}_i; \mathbf{w}\right)\Big)}{1 - \sigma\Big(g\left(\boldsymbol{\phi}_i; \mathbf{w}\right)\Big)}\right) + \sum_{i=1}^n \log \Big(1 - \sigma\Big(g\left(\boldsymbol{\phi}_i; \mathbf{w}\right)\Big)\Big) \label{eq:b5}
\end{eqnarray}
where the last equality is due to the fact that $c_{ti} = z_{t1}^i + z_{t2}^i + \ldots + z_{tn_t}^i$. Hence, taking the summation over $t = 1, 2, \ldots, m$ on both sides of Eq.~\eqref{eq:b5} and using the definition of $\mathfrak{L}_2$, we arrive at Lemma~\ref{lem:3}.

\section{Optimizing \texorpdfstring{Eq.~\eqref{eq:4.9}}{Eq. (13)} via Iterative Weighted Bipartite Matching}
\label{app:c}
For any given index $t$,  Eq.~\eqref{eq:4.9} can be rewritten as
\begin{eqnarray}
\hspace{-0mm}F\Big(\mathbf{z}_t; \mathbf{z}_{-t}\Big) \hspace{-2mm}&=&\hspace{-2mm} \sum_{i=1}^n\sum_{k=1}^{n_t} z_{tk}^i \cdot \left(\log\mathbb{N}\left(\boldsymbol{\omega}_{tk} \ \Big|\ \boldsymbol{\phi}_i, \mathrm{diag}\Big(\alpha\Big(\boldsymbol{\phi}_i; \boldsymbol{\gamma}\Big)\Big)\right) + \log\left(\frac{\sigma\Big(g\left(\boldsymbol{\phi}_i; \mathbf{w}\right)\Big)}{1 - \sigma\Big(g\left(\boldsymbol{\phi}_i; \mathbf{w}\right)\Big)}\right)\right) \nonumber\\
\hspace{-2mm}&+&\hspace{-2mm} G(\mathbf{z}_{-t}) \ =\ \sum_{i=1}^n\sum_{k=1}^{n_t} z_{tk}^i \cdot C_{tk}^i \ \ +\ \ G(\mathbf{z}_{-t}) \label{eq:c1}
\end{eqnarray}
where $G(\mathbf{z}_{-t})$ denotes the summation over the remaining terms in the definition of $L_1$ and $L_2$ that do not depend on $\mathbf{z}_t$, and 
\begin{eqnarray}
C_{tk}^i &\triangleq& \log\mathbb{N}\left(\boldsymbol{\omega}_{tk} \ \Big|\ \boldsymbol{\phi}_i, \mathrm{diag}\Big(\alpha\Big(\boldsymbol{\phi}_i; \boldsymbol{\gamma}\Big)\Big)\right) \ +\  \log\left(\frac{\sigma\Big(g\left(\boldsymbol{\phi}_i; \mathbf{w}\right)\Big)}{1 - \sigma\Big(g\left(\boldsymbol{\phi}_i; \mathbf{w}\right)\Big)}\right) \ .\label{eq:c2}
\end{eqnarray}
Thus, treating all but $\mathbf{z}_t$ as constants, we can solve for the optimal value of $\mathbf{z}_{-t}$ via solving the corresponding weighted bipartite matching. As we iterate over $t$, optimizing for $\mathbf{z}_t$ one at a time, this results in an iterative weighted bipartite matching approach, which is described below:
\begin{algorithm}[h!]
\caption{Iterative Weighted Bipartite Matching}
\label{alg:app-c}
\textbf{input}: generative parameters $\boldsymbol{\gamma}$, $\mathbf{w}$, $\boldsymbol{\omega}$ and $\boldsymbol{\phi}$\\
\textbf{output}: optimized set of values for $\mathbf{z}$
\begin{algorithmic}[1]
\State initialize  $\mathbf{z}$ randomly
\For{$t=1$ {\bfseries to} $m$}
\State compute the cost matrix $C_{tk}^i$ using Eq.~\eqref{eq:c2} and fixing $\mathbf{z}_{-t}$ as constant
\State $\mathbf{z}_t \leftarrow \argmax_{\mathbf{z}_t} F(\mathbf{z}_t; \mathbf{z}_{-t})$ // solve the weighted bipartite matching algorithm in Eq.~\eqref{eq:c2}
\EndFor
\State\textbf{return} the optimal set of alignment $\mathbf{z}$ 
\end{algorithmic}
\end{algorithm}

\begin{figure*}[h!]
\centering
\begin{tabular}{c}
\includegraphics[width=\textwidth]{PFPT-cropped-touch-up.jpg}\\
\includegraphics[width=\textwidth]{GMM-PT-cropped-touch-up.jpg}
\end{tabular}
\caption{t-SNE plots of the (learned) summarizing prompts of (top) our method \textsc{PFPT} and (bottom) \textsc{GMM-PT} over $120$ communication iterations with different heterogeneity settings. Yellow triangles denote the centroids of the t-SNE embeddings of the prompts. The dashed red lines visualize the trajectories of the centroids. The figure is best viewed with color.}
\label{fig:tsne_gmm_pfpt}
\end{figure*}

\section{Additional Ablation Studies}
\label{app:f}
Fig.~\ref{fig:tsne_gmm_pfpt} shows the learning trajectories of the summarizing prompts over $120$ communication iterations of our method $\textsc{PFPT}$ and $\textsc{GMM-PT}$, which is the vanilla baseline for modeling prompt alignment via clustering. Overall, the visual plots show that the learning trajectories of $\textsc{GMM-PT}$ appear less diverse than those of $\textsc{PFPT}$, hinting that the better performance of $\textsc{PFPT}$ can be attributed to its more diverse exploration over the prompt space, allowing it to contextualize information better into its prompt set. This appears consistent with the reported performance gap between $\textsc{GMM-PT}$ and $\textsc{PFPT}$ across a variety of datasets (see Table~\ref{tab:3}-Table~\ref{tab:7} in the main text).

\section{Additional Experiment Results}
In addition to the results reported in the main text, we have also conducted extra experiments comparing the performance of $\textsc{PFPT}$ (ours) with a more challenging set of baselines. These are the improved variants of the original baselines we used in the main text experiment. Each baseline is now improved with a client-specific prompt selection mechanism~\cite{wang2022learning}, which enables each client to selectively contextualize its local data into a subset of most relevant prompts. This prevents different information contexts from being collapsed into the same prompt. This modification improves the performance of the baselines substantially on the diverse $4$-dataset (Table~\ref{tab:g1}) while preserving their performance on the TinyImageNet dataset (Table~\ref{tab:g2}). However, it is observed that even in this more challenging setting, our $\textsc{PFPT}$ still outperforms all baselines on both datasets substantially, featuring an accuracy improvement up to $4.52\%$. Similar to our observations in the main text, the improvement is most pronouncing when client data becomes more diverse, which is the case with the $4$-dataset.

{\bf Remark.} We also consider this improvement to be a part of this work's contribution (although it is not the main contribution) since the incorporation of the prompt selection mechanism in \cite{wang2022learning} has also never been investigated in the existing literature of federated fine-tuning. The comparison results in this section also helps enrich our ablation studies of the key component of $\textsc{PFPT}$.

\begin{table*}[ht]
\begin{small}
\caption{Accuracy ($\%$) achieved on our synthetic $4$-dataset by our proposed \textsc{PFPT} algorithm and other baselines improved with a similar client-specific prompt selection mechanism. The symbol $+$ indicates the improved variant of the original baseline.}
\label{tab:g1}
\centering
\resizebox{\textwidth}{!}{\begin{tabular}{|c|c|c|c|c|c|}
\hline
\ & \textsc{FedAvg-PT+} & \textsc{FedProx-PT+} & \textsc{FedOpt-PT+} & \textsc{GMM-PT+} & \textsc{PFPT} \\ \hline
$\alpha = 0.5$ & $75.92$ & $76.85$ & $57.37$& $74.03$ & $\mathbf{76.89}$\\  
$\alpha = 0.1$ & $66.21$ & $67.77$ & $61.97$& $66.47$ & $\mathbf{70.29}$\\  
imbalance & $61.00$ & $61.75$ & $57.15$& $61.66$ & $\mathbf{62.23}$\\  
\hline 
\end{tabular}}
\end{small}
\end{table*}

\begin{table*}[ht]
\begin{small}
\caption{Accuracy ($\%$) achieved on the TinyImageNet dataset by our proposed \textsc{PFPT} algorithm and other baselines improved with a similar client-specific prompt selection mechanism. The symbol $+$ indicates the improved variant of the original baseline.}
\label{tab:g2}
\centering
\resizebox{\textwidth}{!}{\begin{tabular}{|c|c|c|c|c|c|}
\hline
\ & \textsc{FedAvg-PT+} & \textsc{FedProx-PT+} & \textsc{FedOpt-PT+} & \textsc{GMM-PT+} & \textsc{PFPT} \\ \hline
$\alpha = 0.5$ & $86.30$ & $85.78$& $60.52$ & $84.71$ & $\mathbf{86.91}$\\  
$\alpha = 0.1$ & $77.79$ & $75.46$ & $64.60$ & $76.35$ & $\mathbf{82.31}$\\  
imbalance & $74.25$ & $74.15$ & $64.24$& $74.17$ & $\mathbf{78.21}$\\  
\hline 
\end{tabular}}
\end{small}
\end{table*}

We also run experiments to compare the performance of our PFPT algorithm against another set of federated fine-tuning baselines that incorporate adapter-tuning approach into the FedAvg and FedProx backbones. The results are reported in Table~\ref{tab:g3}, which shows that the performance achieved by incorporating the adapter approach to fine-tuning in FedAvg and FedProx is only comparable to that of PFPT on CIFAR10. Since FedAvg and FedProx were consistently the best performing baselines (after PFPT) across all datasets, we believe that the comparison of FedAvg and FedProx configured with adapter-tuning is sufficient to demonstrate the advantage of PFPT over potential adapter-based FL approaches.

\begin{table*}[ht]
\begin{small}
\caption{Accuracy ($\%$) achieved on the all dataset by \textsc{FEDAVG} with Adapter-Tuning, \textsc{FEDPROX} with Adapter-Tuning, and our proposed \textsc{PFPT} algorithm.}
\label{tab:g3}
\centering
\resizebox{\textwidth}{!}{\begin{tabular}{|c|c|c|c|c|c|}
% Please add the following required packages to your document preamble:
% \usepackage{multirow}
% \usepackage[table,xcdraw]{xcolor}
\hline
Method & Setting & CIFAR10 & CIFAR100 & TinyImageNet & synthetic 4-datasets\\ \hline
& $\alpha = 0.5$ & {\color[HTML]{333333} 93.860.17} & {\color[HTML]{333333} 75.950.40} & {\color[HTML]{333333} 78.880.23} & {\color[HTML]{333333} 55.550.79} \\ \cline{2-6} 
& $\alpha = 0.1$ & {\color[HTML]{333333} 92.660.26}          & {\color[HTML]{333333} 65.040.68} & {\color[HTML]{333333} 57.620.80} & {\color[HTML]{333333} 30.584.67} \\ \cline{2-6} 
\multirow{-3}{*}{FEDAVG-Adapter} & imbalance & {\color[HTML]{333333} \textbf{92.330.26}} & {\color[HTML]{333333} 49.80.79}  & {\color[HTML]{333333} 40.901.34} & {\color[HTML]{333333} 10.868.94} \\ \hline
& $\alpha = 0.5$ & {\color[HTML]{333333} 93.690.20}          & {\color[HTML]{333333} 75.750.16} & {\color[HTML]{333333} 79.010.56} & {\color[HTML]{333333} 58.391.15} \\ \cline{2-6} 
& $\alpha = 0.1$ & {\color[HTML]{333333} 93.040.33}          & {\color[HTML]{333333} 64.590.82} & {\color[HTML]{333333} 58.620.56} & {\color[HTML]{333333} 32.921.34} \\ \cline{2-6} 
\multirow{-3}{*}{FEDPROX-Adapter} & imbalance & {\color[HTML]{333333} 92.130.13}          & {\color[HTML]{333333} 50.751.71} & {\color[HTML]{333333} 37.652.19} & {\color[HTML]{333333} 13.197.92} \\ \hline
& $\alpha = 0.5$ & \textbf{94.390.51} & \textbf{80.240.24} & \textbf{86.910.14} & \textbf{76.890.17} \\ \cline{2-6} 
& $\alpha = 0.1$ & \textbf{93.390.22} & \textbf{75.080.51} & \textbf{82.310.26} & \textbf{70.290.32} \\ \cline{2-6} 
\multirow{-3}{*}{PFPT} & imbalance & 91.450.08 & \textbf{72.050.93} & \textbf{78.211.25} & \textbf{62.231.02} \\ \hline 
\end{tabular}}
\end{small}
\end{table*}

\begin{table*}[h]
\begin{small}
\caption{Hyperparameter setting for all baselines and our PFPT}
\label{tab:g4}
\centering
\resizebox{\textwidth}{!}{\begin{tabular}{|c|c|c|c|c|c|c|c|}
\hline
Method & Setting & \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}Batch \\ size\end{tabular}} & \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}Communication\\ round\end{tabular}} & \begin{tabular}[c]{@{}l@{}}Eps. in \\ local training\end{tabular} & \begin{tabular}[c]{@{}l@{}}Optimizer \& \\ learning rate\end{tabular} & \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}Total\\ clients\end{tabular}} & \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}Sampled\\ clients\end{tabular}} \\ \hline
& $\alpha = 0.5$  & {\color[HTML]{333333} 16} & {\color[HTML]{333333} 120} & {\color[HTML]{333333} 5} & {\color[HTML]{333333} \begin{tabular}[c]{@{}l@{}}Adam: \\ beta=(0.9, 0.98), eps=1e-6\\ lr: 5e-4\end{tabular}} & \begin{tabular}[c]{@{}l@{}}CIFAR10: 100\\ CIFAR100: 100\\ TinyImageNet: 100\\ Synthetic 4-dataset: 80\end{tabular} & 10\\ \cline{2-8} 
\multirow{-3}{*}{\begin{tabular}[c]{@{}c@{}}FEDAVG-PT\\ FEDPROX-PT\\ FEDAVG-Adapter\\ FEDPROX-Adapter\\ FEDOPT-PT\\ PFEDPG-PT\\ GMM-PT\\ PFPT\end{tabular}} & $\alpha = 0.1$  & {\color[HTML]{333333} 16} & {\color[HTML]{333333} 120} & {\color[HTML]{333333} 5} & {\color[HTML]{333333} \begin{tabular}[c]{@{}l@{}}Adam: \\ beta=(0.9, 0.98), eps=1e-6\\ lr: 1e-4\end{tabular}} & \begin{tabular}[c]{@{}l@{}}CIFAR10: 100\\ CIFAR100: 100\\ TinyImageNet: 100\\ Synthetic 4-dataset: 80\end{tabular} & 10\\ \cline{2-8} 
 & imbalance & {\color[HTML]{333333} 16} & {\color[HTML]{333333} 120} & {\color[HTML]{333333} 5} & {\color[HTML]{333333} \begin{tabular}[c]{@{}l@{}}Adam: \\ beta=(0.9, 0.98), eps=1e-6\\ lr: 1e-4\end{tabular}} & \begin{tabular}[c]{@{}l@{}}CIFAR10: 100\\ CIFAR100: 100\\ TinyImageNet: 100\\ Synthetic 4-dataset: 80\end{tabular} & 10\\ \hline
& $\alpha = 0.5$ & {\color[HTML]{333333} 16} & {\color[HTML]{333333} 200} & {\color[HTML]{333333} 5} &{\color[HTML]{333333} \begin{tabular}[c]{@{}l@{}}SGD\\ lr: 5e-4\end{tabular}} & \begin{tabular}[c]{@{}l@{}}CIFAR10: 100\\ CIFAR100: 100\\ TinyImageNet: 100\\ Synthetic 4-dataset: 80\end{tabular} & 10\\ \cline{2-8} 
\multirow{-3}{*}{SCAFFOLD-PT} & $\alpha = 0.1$ & {\color[HTML]{333333} 16} & {\color[HTML]{333333} 200} & {\color[HTML]{333333} 5} & {\color[HTML]{333333} \begin{tabular}[c]{@{}l@{}}SGD\\ lr: 1e-4\end{tabular}} & \begin{tabular}[c]{@{}l@{}}CIFAR10: 100\\ CIFAR100: 100\\ TinyImageNet: 100\\ Synthetic 4-dataset: 80\end{tabular} & 10\\ \cline{2-8} 
 & imbalance & {\color[HTML]{333333} 16} & {\color[HTML]{333333} 200} & {\color[HTML]{333333} 5} & {\color[HTML]{333333} \begin{tabular}[c]{@{}l@{}}SGD\\ lr: 1e-4\end{tabular}} & \begin{tabular}[c]{@{}l@{}}CIFAR10: 100\\ CIFAR100: 100\\ TinyImageNet: 100\\ Synthetic 4-dataset: 80\end{tabular} & 10\\ \hline
\end{tabular}}
\end{small}
\end{table*}

\section{Hyperparameter Settings}

All experimented baselines use a batch size of 16 and are fine-tuned with 10 learnable prompts. All experiments are performed on a V100 GPU with 32GB GPU RAM. We employ the same hyperparameter setting in all baselines (except for SCAFFOLD-PT). The hyperparameter settings are all presented in Table~\ref{tab:g4}. SCAFFOLD-PT requires a larger learning rate since SCAFFOLD's variate control mechanism is designed to work with SGD. Hence, unlike other baselines, SCAFFOLD-PT is configured with SGD instead of Adam. Empirically, we find that a larger learning rate is required for SCAFFOLD-PT with SGD to reach its best performance in our setting. 
\label{sec:hyper}
% TODO


\iffalse
\section{Long-Tailed Experiment}
We also run long-tailed experiments to compare the performance of our \textsc{PFPT} algorithm against the prior method, FEDIC~\cite{shang2022FEDIC} and CReFF~\cite{shang2022CReFF}. All local data distributions and FL settings are configured to be the same for fair comparison: batch size=128 for CIFAR100-LT and 32 for  ImageNet-LT, No. total clients=20, No. online clients in each communication round=8, and No. epochs in local training=10. The results in Table~\ref{tab:g5} show that our method achieves higher accuracy than prior works substantially across all global imbalance settings with different imbalance factors (IF).
% Please add the following required packages to your document preamble:
% \usepackage{multirow}
\begin{table*}[!ht]
\caption{Accuracy ($\%$) achieved on both long-tailed cifar100 and ImageNet dataset by \textsc{PFPT}, \textsc{CReFF} and \textsc{FEDIC}.}
\label{tab:g5}
\centering
\begin{tabular}{|l|lll|l|}
\hline
\multicolumn{1}{|c|}{\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Non-IID\\$\alpha = 0.1$\end{tabular}}} & \multicolumn{3}{c|}{CIFAR100-LT}                                 & \multicolumn{1}{c|}{\multirow{2}{*}{ImageNet-LT}} \\ \cline{2-4}
\multicolumn{1}{|c|}{}                                                                         & \multicolumn{1}{l|}{IF=100} & \multicolumn{1}{l|}{IF=50} & IF=10 & \multicolumn{1}{c|}{}                             \\ \hline
FEDIC                                                                                          & \multicolumn{1}{l|}{33.67}  & \multicolumn{1}{l|}{34.74} & 41.93 & 28.93                                             \\ \hline
CReFF                                                                                          & \multicolumn{1}{l|}{26.19}  & \multicolumn{1}{l|}{28.32} & 35.49 & 26.31                                             \\ \hline
PFPT                                                                                           & \multicolumn{1}{l|}{$\mathbf{60.74}$}  & \multicolumn{1}{l|}{$\mathbf{65.54}$} & $\mathbf{71.66}$ & $\mathbf{75.54}$                                             \\ \hline
\end{tabular}
\end{table*}
\fi

\iffalse
\section{Full-model Tuning vs. Prompt-Tuning}
In traditional centralized learning setting, numerous studies have demonstrated that Prompt-Tuning exhibits superior performance in terms of computational efficiency and generalization across tasks~\cite{lester-etal-2021-power,lialin2023scaling,han2024facing,xu2023parameter}. 
%In contrast to full-model tuning, Prompt-Tuning necessitates only the parameters of prompts to be updated and stored. This implies that Prompt-Tuning is inherently more computationally efficient and utilizes less memory than full-model tuning. -- maybe avoid saying this because we actually have to compute all the gradients and do not save much in terms of memory?
For example, it was observed that Visual Prompt-Tuning (VPT) outperforms full-model tuning in scenarios where the pre-trained and downstream tasks have similar data distribution, but significant differences in terms of task objectives, such as adapting a classification model to solve a counting task~\cite{han2024facing}. Full-model tuning also carries the risk of overfitting when the downstream task has limited data. In contrast, Prompt-Tuning can circumvent this risk and preserve the model's generalization capabilities. 
However, it was shown that the total memory usage of Prompt-Tuning might exceed that of Full-model Tuning. This is because the prompt tokens are appended to the input embeddings located at the top layer of the model. As such, in order to update them, the optimizer still needs to compute all intermediate gradient terms during backpropagation, each now has slightly larger dimension due to the extra prompt tokens.
%Consequently, the parameters of the entire pre-trained model are involved in the gradient computation, resulting in a total memory usage that is considerably larger than what would be expected based on the total trainable parameters alone.

Nonetheless, this small overhead in memory usage is less pertinent in the context of federated learning. Instead, the effectiveness of Prompt-Tuning and Full-model Tuning are often assessed in terms of communication efficiency and privacy preservation. Here, communication efficiency refers to the number of parameters that are sent back and forth between clients and the central server per FL communication round. Since Prompt-Tuning only communicates the prompt tokens, it naturally has an advantage over Full-model Tuning, which has to communicate the entire set of model weights. Additionally, since the prompts constitute the complement of the partial information encoding of the new task, it follows that there is no inference attack capable of inferring privacy information by stealing only the weights or gradients of prompts during communication unless the pre-trained model is leaked. In other words, prompt tuning also prevents the entire pre-trained model from being compromised if it is privacy-sensitive. 
\fi

\section{Limitations}
Despite the clear advantage of PFPT in terms of communication bandwidth, as opposed to full-model FT and other forms of FL that typically have to communicate the entire architecture weights per round of communication, we foresee that PFPT will face some challenges when the pre-trained and downstream task domains differ (which goes even beyond the distribution shift setting that we investigate in this paper). Intuitively, a significant domain shift will likely require using more trainable prompts. Can we quantify this shift and bound the minimum number of prompts required to enable accurate adaptation? Although this question is beyond the scope of this paper, we will investigate and address it in future works, especially in the context of heterogeneous FL where each client operates in a different domain. We have also raised the issue of prompt tuning having to compute all intermediate gradients since the prompt tokes are appended at the top level of the pre-trained model. Going forward, we can explore new prompt placement strategies to reduce this memory consumption.
\label{sec:limit}
% TODO

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\clearpage
%\newpage
\section*{NeurIPS Paper Checklist}
\begin{enumerate}

\item {\bf Claims}
    \item[] Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope?
    \item[] Answer: \answerYes{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
    \item[] Justification: Our contributions can be found in Sections~\ref{sec:method} and~\ref{sec:exp}
    \item[] Guidelines:
    \begin{itemize}
        \item The answer NA means that the abstract and introduction do not include the claims made in the paper.
        \item The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. 
        \item The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. 
        \item It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. 
    \end{itemize}

\item {\bf Limitations}
    \item[] Question: Does the paper discuss the limitations of the work performed by the authors?
    \item[] Answer: \answerYes{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
    \item[] Justification: Please refer to Appendix~\ref{sec:limit}
    \item[] Guidelines:
    \begin{itemize}
        \item The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. 
        \item The authors are encouraged to create a separate "Limitations" section in their paper.
        \item The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.
        \item The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.
        \item The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.
        \item The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.
        \item If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.
        \item While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
    \end{itemize}

\item {\bf Theory Assumptions and Proofs}
    \item[] Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?
    \item[] Answer: \answerYes{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
    \item[] Justification: Our results mostly establish useful probabilistic equalities which do not require any assumptions. Complete proofs can be found in Appendices B, C, and D.
    \item[] Guidelines:
    \begin{itemize}
        \item The answer NA means that the paper does not include theoretical results. 
        \item All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.
        \item All assumptions should be clearly stated or referenced in the statement of any theorems.
        \item The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. 
        \item Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.
        \item Theorems and Lemmas that the proof relies upon should be properly referenced. 
    \end{itemize}

    \item {\bf Experimental Result Reproducibility}
    \item[] Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)?
    \item[] Answer: \answerYes{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
    \item[] Justification: Our experiment protocols are described in Section~\ref{sec:exp}. The detailed hyperparameter settings are fully specified in Appendix~\ref{sec:hyper}.
    \item[] Guidelines:
    \begin{itemize}
        \item The answer NA means that the paper does not include experiments.
        \item If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.
        \item If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. 
        \item Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.
        \item While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example
        \begin{enumerate}
            \item If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm.
            \item If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully.
            \item If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset).
            \item We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
        \end{enumerate}
    \end{itemize}


\item {\bf Open access to data and code}
    \item[] Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material?
    \item[] Answer: \answerYes{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
    \item[] Justification: All the used datasets are publicly available. Upon acceptance, we will release our code.
    \item[] Guidelines:
    \begin{itemize}
        \item The answer NA means that paper does not include experiments requiring code.
        \item Please see the NeurIPS code and data submission guidelines (\url{https://nips.cc/public/guides/CodeSubmissionPolicy}) for more details.
        \item While we encourage the release of code and data, we understand that this might not be possible, so No is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).
        \item The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (\url{https://nips.cc/public/guides/CodeSubmissionPolicy}) for more details.
        \item The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.
        \item The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.
        \item At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).
        \item Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
    \end{itemize}


\item {\bf Experimental Setting/Details}
    \item[] Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results?
    \item[] Answer: \answerYes{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
    \item[] Justification: Such details can be found in Section~\ref{sec:exp} and Appendix~\ref{sec:hyper}
    \item[] Guidelines:
    \begin{itemize}
        \item The answer NA means that the paper does not include experiments.
        \item The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them.
        \item The full details can be provided either with the code, in appendix, or as supplemental material.
    \end{itemize}

\item {\bf Experiment Statistical Significance}
    \item[] Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments?
    \item[] Answer: \answerYes{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
    \item[] Justification: We have provided error bars for all reported results. 
    \item[] Guidelines:
    \begin{itemize}
        \item The answer NA means that the paper does not include experiments.
        \item The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.
        \item The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).
        \item The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
        \item The assumptions made should be given (e.g., Normally distributed errors).
        \item It should be clear whether the error bar is the standard deviation or the standard error of the mean.
        \item It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96\% CI, if the hypothesis of Normality of errors is not verified.
        \item For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
        \item If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
    \end{itemize}

\item {\bf Experiments Compute Resources}
    \item[] Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments?
    \item[] Answer: \answerYes{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
    \item[] Justification: The detail regarding our compute resources are provided in Appendix~\ref{sec:hyper}
    \item[] Guidelines:
    \begin{itemize}
        \item The answer NA means that the paper does not include experiments.
        \item The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.
        \item The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. 
        \item The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper). 
    \end{itemize}
    
\item {\bf Code Of Ethics}
    \item[] Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics \url{https://neurips.cc/public/EthicsGuidelines}?
    \item[] Answer: \answerYes{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
    \item[] Justification: We have read the NeurIPS Code of Ethics and do not find our work violate any aspects of the code
    \item[] Guidelines:
    \begin{itemize}
        \item The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
        \item If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.
        \item The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
    \end{itemize}


\item {\bf Broader Impacts}
    \item[] Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed?
    \item[] Answer: \answerYes{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
    \item[] Justification: We provide a statement of impact in Appendix~\ref{app:impact}.
    \item[] Guidelines:
    \begin{itemize}
        \item The answer NA means that there is no societal impact of the work performed.
        \item If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.
        \item Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.
        \item The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
        \item The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
        \item If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
    \end{itemize}
    
\item {\bf Safeguards}
    \item[] Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)?
    \item[] Answer: \answerNA{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
    \item[] Justification: Our work do not create new dataset. We only use existing, publicly available datasets and their combination. We also do not create any new pre-trained NLP or vision models. We only used existing, publicly available pre-trained models.
    \item[] Guidelines:
    \begin{itemize}
        \item The answer NA means that the paper poses no such risks.
        \item Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. 
        \item Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.
        \item We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
    \end{itemize}

\item {\bf Licenses for existing assets}
    \item[] Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected?
    \item[] Answer: \answerYes{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
    \item[] Justification: We cite the source of all datasets and pre-trained models used in our experiments.
    \item[] Guidelines:
    \begin{itemize}
        \item The answer NA means that the paper does not use existing assets.
        \item The authors should cite the original paper that produced the code package or dataset.
        \item The authors should state which version of the asset is used and, if possible, include a URL.
        \item The name of the license (e.g., CC-BY 4.0) should be included for each asset.
        \item For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.
        \item If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, \url{paperswithcode.com/datasets} has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
        \item For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
        \item If this information is not available online, the authors are encouraged to reach out to the asset's creators.
    \end{itemize}

\item {\bf New Assets}
    \item[] Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets?
    \item[] Answer: \answerNA{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
    \item[] Justification: Our work does not release any new assets.
    \item[] Guidelines:
    \begin{itemize}
        \item The answer NA means that the paper does not release new assets.
        \item Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. 
        \item The paper should discuss whether and how consent was obtained from people whose asset is used.
        \item At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
    \end{itemize}

\item {\bf Crowdsourcing and Research with Human Subjects}
    \item[] Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? 
    \item[] Answer: \answerNA{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
    \item[] Justification: Our work does not involve crowdsourcing nor research with human subjects.
    \item[] Guidelines:
    \begin{itemize}
        \item The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
        \item Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. 
        \item According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. 
    \end{itemize}

\item {\bf Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects}
    \item[] Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained?
    \item[] Answer: \answerNA{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
    \item[] Justification: Our work does not involve crowdsourcing nor research with human subjects.
    \item[] Guidelines:
    \begin{itemize}
        \item The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
        \item Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper. 
        \item We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution. 
        \item For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.
    \end{itemize}

\end{enumerate}

\end{document}