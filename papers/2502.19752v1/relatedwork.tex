\section{Related Work}
\label{sec:related}\vspace{-2mm}
Federated learning (FL) \cite{DBLP:journals/corr/KonecnyMRR16,McMahan17} is a collaborative framework that allows multiple parties to collaborate on training a common model without sharing their private data. In FL, the training data are distributed across $m$ clients. The $t$-th client owns a local, private dataset $D_t = \{(\mathbf{x}_{tm}, y_{tm})\}_{m=1}^{n_t}$ comprising $n_t$ data points. The goal is to fit a model $\mathbf{w}_\ast$ using all private datasets without centralizing them. That is, 
\begin{eqnarray}
\hspace{-9mm}\mathbf{w}_\ast \hspace{-2mm}&=&\hspace{-2mm} \underset{\mathbf{w}}{\arg\min}\left\{ L(\mathbf{w}) \triangleq \sum_{t=1}^m\left(\frac{n_t}{n}\right) \cdot L_t(\mathbf{w})\right\} \ , \ \text{where}\ \  L_t(\mathbf{w}) \ \triangleq\ \frac{1}{n_t} \sum_{m=1}^{n_t} \ell\big(\mathbf{x}_{tm}, y_{tm}; \mathbf{w}\big)
\hspace{-0mm} \label{eq:FL}
\end{eqnarray}
and $n = n_1 + n_2 + \ldots n_m$ denotes the total number of data points while $\ell(\mathbf{x}_{tm}, y_{tm}; \mathbf{w})$ denotes some loss function of choice. Clients collaborate via sharing their local models instead of data. Each client can run multiple local updates before sharing its local model for aggregation. This helps reduce the number of communication rounds while still preserving the convergence guarantee. \cite{McMahan17}~names this the \textsc{FedAvg} algorithm, which iterates between local optimization and global aggregation:
\begin{eqnarray}
\mathbf{w}^{(r)}_t &=& U\Big(\mathfrak{L}_t, \mathbf{w}^{(r-1)}_\ast\Big)\ \forall t \in [m] \ ,\quad \mathbf{w}^{(r)}_\ast \ \ = \ \ \sum_{t=1}^m \left(\frac{n_t}{n}\right) \cdot \mathbf{w}^{(r)}_t \ . \label{eq:local_global}
\end{eqnarray}
The local update routine $U(\mathfrak{L}_t, .)$ is typically standard gradient updates such as SGD or Adam. At the beginning of an iteration, the local weight is set to be the global estimate from the previous communication round. In practice, local data distributions tend to diverge which consequently causes the local updates in Eq.~\eqref{eq:local_global} to have different convergence points across clients. This is known as the solution drift phenomenon, which often decreases the performance of the federated model. Numerous approaches had been proposed to mitigate this phenomenon, which includes the following directions:

{\bf Client Regularization.} These methods focus on augmenting the local update strategies to prevent clients from drifting apart. For example, \textsc{FedProx}~\cite{pmlr-v139-li21h} introduces an $\ell_2$-regularization term to penalize updates that diverge the local model from the global model. \textsc{FedDyn}~\cite{feddyn_acar2021federated} uses a FL-adapted version of~\cite{shamir2014communication} on distributed optimization as the regularizer. \textsc{Scaffold}~\cite{pmlr-v119-karimireddy20a} attempts to correct the direction of local gradient updates at each client using their sum of gradients. \cite{MoonLiCVPR21} utilizes the similarity between model representations to correct local training via contrastive learning.

{\bf Server Regularization.} Another approach to prevent the drifting effect caused by heterogeneous local distributions is to replace the model average in Eq.~\eqref{eq:local_global} with a different mechanism of model aggregation. For example, \cite{NEURIPS2019_ecb287ff,pmlr-v97-yurochkin19a} decomposes client models into bags of neurons and performs a non-parametric clustering of neurons. Cluster centroids are used to synthesize an aggregated model for the next communication round. This approach bypasses the drifting phenomenon as the number of cluster is non-parametric and can be adjusted to accommodate new information patterns. In a similar vein, \cite{ChenICLR21} adopts a probabilistic perspective of model aggregation, sampling higher-quality global models and combining them via Bayesian model ensemble, leading to a more robust aggregation. \cite{Zhang_2022_CVPR} presents a data-free knowledge distillation methods for FL that help to train generators without compromising clients' data. \cite{kamp2021federated} redistributes each client's shared model to others for $b$ consecutive communication iterations, exposing it to various heterogeneous data sources before performing aggregation, curving their solution divergence.

{\bf Personalization.} Instead of learning a single, universal model, \cite{pmlr-v139-li21h,FallahNIPS20,pFedMeDinhNIPS20,yu2023multimodal,pmlr-v139-collins21a} seek to learn personalized models for all clients.  \cite{smith2017federated,yu2023multimodal} formulates this problem as multi-task learning, whereas \cite{FallahNIPS20} and \cite{pFedMeDinhNIPS20} respectively ground it in meta-learning and regularization frameworks. Several recent works impose a shared data representation across clients~\cite{pmlr-v139-collins21a,DBLP:journals/corr/abs-2003-13461,DBLP:journals/corr/abs-2001-01523,DBLP:journals/corr/abs-1912-00818,DBLP:journals/corr/abs-2002-05516}, which is optimized using existing FL techniques, and locally fine-tune a small classification head to achieve personalization. However, personalized models trained in this manner tend to only perform well on its corresponding local test set, and not on a comprehensive test set combining all local test data (see Section~\ref{sec:exp}).

Finally, most previous work assume that each FL client has sufficient data to adequately train its local model. This often contradicts real-world situations where data are scarce~\cite{ng2021federated,kamp2021federated,zhao2019multi,zhang2022fednilm,zhao2020network}. While data scarcity is not a new challenge of machine learning, it has not been thoroughly investigated in the context of FL. \cite{kamp2021federated}~points out that with limited, scarce data, local models often have bad qualities, and aggregating such models tend to result in poor global performance. Although fine-tuning pre-trained large models is an increasingly popular technique to combat data shortage, most existing FL works (including~\cite{kamp2021federated} which aims to address data scarcity) have not tried to leverage this resource. This motivates us to investigate prompt-tuning as a new FL paradigm.\vspace{-2mm}