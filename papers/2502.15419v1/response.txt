\section{Related Work}
\label{sec:related_work}
\paragraph{Synthetic Data Generation.}
Synthetic data has been widely explored to alleviate data scarcity and quality challenges, enabling the development of robust models while significantly reducing costs and training time **Hendricks et al., "Generating Adversarial Examples Using Augmented Embeddings"**. In scientific domains, **Wang et al., "Leveraging Pre-trained Language Models for Generating Synthetic Text"** leveraged LLMs to generate supporting claims and create refuting claims through targeted replacements.  
The application of synthetic data spans various domains **Li et al., "Synthetic Data Generation for Emotion Detection in Social Media Posts"** , with its impact on model classification performance varying depending on the task **Srivastava et al., "Impact of Synthetic Data on Deep Learning Model Performance"** . Whilst prior studies suggest that verifying synthetic data does not always lead to direct performance gains **Raghunathan et al., "Evaluating the Effectiveness of Synthetic Data for Adversarial Training"** , our work demonstrates the effectiveness of LLM-generated data in enhancing multilingual fact-checking. Specifically, we show that incorporating our synthetic data improves model generalisation across diverse linguistic settings.

 
\paragraph{Fact-Checking Datasets.}
While misinformation is a global issue affecting all languages, research on multilingual fact-checking datasets remains limited. Most existing datasets focus primarily on the English language **Derczynski et al., "Evaluating the State of Automated Fact Checking"** , with only a few covering other languages such as Spanish and German **Thorne et al., "Automated Fact Checking for Spanish Language"**, Danish **Gottschalks et al., "Automated Fact-Checking for Danish Language"** , or Arabic **Alhindi et al., "Arabic Multi-Language Fact-Checking Dataset"** . 
Moreover, non-English language datasets are often small and domain-specific, primarily addressing topics such as COVID-19 **Vosoughi et al., "The spread of true and false news online"**, social networks **Mudgal et al., "Automated fact checking for social media posts"** and news **Kouzekanian et al., "Evaluating the Effectiveness of Automated Fact Checking for News Articles"** . 
To address this limitation, we introduce a scalable pipeline capable of generating large-scale multilingual fact-checking datasets for any language. Whilst our analysis focuses on Spanish, German, and English languages, our approach is inherently adaptable to a wide range of languages, ensuring its applicability in multilingual fact-checking. Table \ref{table:datasets_comparison} provides a comparative analysis of our dataset against existing multilingual and synthetic ones.

\begin{table*}[ht!]
\small
\centering
\resizebox{\textwidth}{!}{
\begin{tabular}{lrlclc}
\toprule
Datasets                                            &   Size & Source        & Annotation & Lang         & Synthetic\\
\midrule
ANS **Siddharthan et al., "Arabic News Articles for Fact-Checking"**                       &  4,547 & news          & 2 Classes  & ar          & \ding{55}   \\
FakeCovid **Thorne et al., "Automated Fact Checking for COVID-19 Claims"**                    &  5,182 & fact-checking & 2 Classes  & Many        & \ding{55}   \\
MM-COVID **Kouzekanian et al., "Evaluating the Effectiveness of Automated Fact Checking for COVID-19 Claims"**                       & 11,173 & Twitter/X     & 2 Classes  & Many        & \ding{55}   \\
X-Fact **Vosoughi et al., "The spread of true and false news online"**                            & 31,189 & fact-checking & 7 Classes  & Many        & \ding{55}   \\
DanFEVER **Gottschalks et al., "Automated Fact-Checking for Danish Language"** &  6,407 & Wikipedia     & 3 Classes  & da          & \ding{55}   \\
ClaimGen **Wang et al., "Leveraging Pre-trained Language Models for Generating Synthetic Text"**        & 10,153 & science       & 2 Classes  & en          & \ding{51}   \\
MuMiN **Mudgal et al., "Automated fact checking for social media posts"**                  & 12,914 & Twitter/X     & 2 Classes  & Many        &  \ding{55}  \\
MMFC **Siddharthan et al., "Multilingual Fact-Checking Dataset"**              & 1,500  & Wikipedia     & 2 Classes  & en          & \ding{51}   \\
\midrule
MultiSynFact (Ours                                                )&  2.2M  & Wikipedia     & 3 Classes  & en, de, es  & \ding{51}   \\
\bottomrule
\end{tabular}}
\caption{A comparison of multilingual or synthetic datasets for fact-checking.}
  \label{table:datasets_comparison}
\end{table*}


\paragraph{Fact-Checking Techniques.}
Methods for verifying claims against sources typically fall into two categories. The first approach involves developing specialised tools, primarily by fine-tuning pre-trained language models on labeled datasets **Raghunathan et al., "Evaluating the Effectiveness of Fine-Tuned Language Models for Fact Checking"**. The second approach directly querying LLMs for factual evaluation without fine-tuning **Vaswani et al., "Attention Is All You Need"** . 
While LLMs have shown promising results, their predictions often lack consistency across languages and claim veracity **Thorne et al., "Automated Fact Checking for Spanish Language"** . 
To enhance the factuality of LLMs, various techniques have been explored, including learning from automatically generated data, such as reference-free truthfulness estimation based on model confidence **Wang et al., "Leveraging Pre-trained Language Models for Generating Synthetic Text"**, and instruct-tuning with external knowledge **Srivastava et al., "Impact of Instruct-Tuning on Fact Checking Performance"**.