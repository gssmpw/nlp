\section{Related Work}
\label{sec:related_work}
\paragraph{Synthetic Data Generation.}
Synthetic data has been widely explored to alleviate data scarcity and quality challenges, enabling the development of robust models while significantly reducing costs and training time \cite{long-etal-2024-llms, goyal2024systematic, patel2024datadreamer}. In scientific domains, \citet{wright-etal-2022-generating} leveraged LLMs to generate supporting claims and create refuting claims through targeted replacements.  
The application of synthetic data spans various domains \cite{li2023synthetic, xu2024wizardlm, xu2024magpie}, with its impact on model classification performance varying depending on the task \cite{chan2024balancing}. Whilst prior studies suggest that verifying synthetic data does not always lead to direct performance gains \cite{li2023synthetic, yu2024metamath, chan2024balancing}, our work demonstrates the effectiveness of LLM-generated data in enhancing multilingual fact-checking. Specifically, we show that incorporating our synthetic data improves model generalisation across diverse linguistic settings.

 
\paragraph{Fact-Checking Datasets.}
While misinformation is a global issue affecting all languages, research on multilingual fact-checking datasets remains limited. Most existing datasets focus primarily on the English language \cite{thorne-etal-2018-fever, schuster-etal-2021-get}, with only a few covering other languages such as Spanish and German \cite{gupta2021x}, Danish \cite{norregaard-derczynski-2021-danfever}, or Arabic \cite{khouja-2020-stance}. 
Moreover, non-English language datasets are often small and domain-specific, primarily addressing topics such as COVID-19 \cite{li2020mmcovid, shahifakecovid}, social networks \cite{NielsenMcConville2022} and news \cite{gupta2021x}. 
To address this limitation, we introduce a scalable pipeline capable of generating large-scale multilingual fact-checking datasets for any language. Whilst our analysis focuses on Spanish, German, and English languages, our approach is inherently adaptable to a wide range of languages, ensuring its applicability in multilingual fact-checking. Table \ref{table:datasets_comparison} provides a comparative analysis of our dataset against existing multilingual and synthetic ones.

\begin{table*}[ht!]
\small
\centering
\resizebox{\textwidth}{!}{
\begin{tabular}{lrlclc}
\toprule
Datasets                                            &   Size & Source        & Annotation & Lang         & Synthetic\\
\midrule
ANS \cite{khouja-2020-stance}                       &  4,547 & news          & 2 Classes  & ar          & \ding{55}   \\
FakeCovid \cite{shahifakecovid}                    &  5,182 & fact-checking & 2 Classes  & Many        & \ding{55}   \\
MM-COVID \cite{li2020mmcovid}                       & 11,173 & Twitter/X     & 2 Classes  & Many        & \ding{55}   \\
X-Fact \cite{gupta2021x}                            & 31,189 & fact-checking & 7 Classes  & Many        & \ding{55}   \\
DanFEVER \cite{norregaard-derczynski-2021-danfever} &  6,407 & Wikipedia     & 3 Classes  & da          & \ding{55}   \\
ClaimGen \cite{wright-etal-2022-generating}        & 10,153 & science       & 2 Classes  & en          & \ding{51}   \\
MuMiN \cite{NielsenMcConville2022}                  & 12,914 & Twitter/X     & 2 Classes  & Many        &  \ding{55}  \\
MMFC \cite{bussotti-etal-2024-unknown}              & 1,500  & Wikipedia     & 2 Classes  & en          & \ding{51}   \\
\midrule
MultiSynFact (Ours                                                )&  2.2M  & Wikipedia     & 3 Classes  & en, de, es  & \ding{51}   \\
\bottomrule
\end{tabular}}
\caption{A comparison of multilingual or synthetic datasets for fact-checking.}
  \label{table:datasets_comparison}
\end{table*}


\paragraph{Fact-Checking Techniques.}
Methods for verifying claims against sources typically fall into two categories. The first approach involves developing specialised tools, primarily by fine-tuning pre-trained language models on labeled datasets \cite{thorne-etal-2018-fever, schuster-etal-2021-get, tianfine}. The second approach directly querying LLMs for factual evaluation without fine-tuning \cite{hu2024towards, shafayat2024multifact, 10.3389/frai.2024.1341697, singhal-etal-2024-multilingual}. 
While LLMs have shown promising results, their predictions often lack consistency across languages and claim veracity \cite{10.3389/frai.2024.1341697}. 
To enhance the factuality of LLMs, various techniques have been explored, including learning from automatically generated data, such as reference-free truthfulness estimation based on model confidence \cite{tianfine}, and instruct-tuning with external knowledge \cite{10317251}.