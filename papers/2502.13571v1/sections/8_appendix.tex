\appendix

\section{Time Complexity Analysis of HIM}

The time complexity of HIM primarily consists of the training time of the hyperbolic influence representation module and the selection time of the adaptive seed selection module.

The training time complexity of hyperbolic influence representation is $O( (|E|+|E_D|) \cdot n \cdot |\mathcal{N}| \cdot I )$, where $|E|$ is the number of edges in $G$, $|E_D| = \sum_{i=1}^{M}|E^i_D|$ is the number of all propagation edges,
$n$ is the number of dimensions, 
$\mathcal{N}$ is the number of negative samples,
and $I$ denotes the number of training epochs.

The time complexity of adaptive seed selection primarily arises from two parts: outside the loop and within the loop. 
Outside the loop, the main time cost is sorting $LDO$ for all nodes, with complexity $O(|V| \log |V|)$. Within the loop, finding neighbors and computing intersections each have a complexity of $O(|N_c|)$, and priority queue updates are $O( |\mathcal{C}| \log w)$. Consequently, ASW's time complexity is $O(|V| \log |V| + k \cdot (2|N_c|+ |\mathcal{C}| \log w))$.


\section{Experimental Setups}
\label{sec:setup}
\subsection{Implementation Detail}
The baselines are configured according to either public code repositories or the suggestions provided in the original papers.
We adopt the same evaluation scheme as the baseline DeepIM~\cite{ling2023icml} and DeepSN~\cite{hevapathige2024_DeepSN}.
In each dataset, we vary the value $k$ (or seed ratio) as [$1\%$, $5\%$, $10\%$] of total nodes for a given network.
For adaptive seed selection, we set the window size coefficient $\beta$ as $1.0$ for Cora-ML, and Power Grid, $\beta$ as $0.5$ for Facebook and GitHub, and $\beta$ as $0.1$ for YouToube.
For each experiment, similar to DeepIM and DeepSN, 
we evaluate chosen seed users through simulation under a diffusion model, running until the whole process is completed.
We record the average influence spread ratio over 100 rounds.
The default number of dimension $n$ is set to $64$.
The curvature parameter $\gamma$ is set to 1.
Further, we set the number of negative samples per positive sample as $5$.
All code is implemented in Python on a Ubuntu server equipped with a 10-core Intel(R) i9-10900X 3.70GHz CPU and one NVIDIA GTX 4090 GPU.