\section{Conclusion}\label{sec:conclusion}
In this work, we developed and tested two versions of the quantum-inspired optimization algorithms: TN-GEO and STN-GEO, for solving a generalized version of the multi knapsack problem in which all objects must be assigned to a knapsack. This problem is closely related to production and logistics applications. The two methods need different encodings of the problem: an integer formulation which can be used with standard TNs and a binary formulation which requires the TNs to be symmetric. We detail the implementation of both methods and discuss their limitations.

We observe the somewhat unexpected ability of TN-, or more specifically, MPS-generative models to generalize towards lower cost solutions. This feature is intriguing and deserves more investigation. We also found that TN- and STN-GEO performed comparably to simulated annealing, occasionally slightly under performing but significantly outperforming random search methods as the problem instances grow in complexity. As we varied the method's hyper-parameters, we noted that smaller bond dimension sizes (4-5) and smaller training epochs (1-3) often resulted in better performance, especially in smaller problem instances. This suggests that reduced truncation can help prevent over-fitting of the generative model, encourage exploration, and improve the algorithm's performance. Both quantum inspired algorithms proved effective in finding valid solutions within the tested optimization problems. However, our experiments show that GEO in general is a much more costly algorithm to use when time-to-solution metrics are considered.

We observed that numerical accuracy critically affects the preservation of symmetry in MPSs. In fact, we saw that one needs to carefully operate within the symmetry blocks when contracting and updating the tensors. Failing to do so, greatly reduces the quality of the solutions. Also, we found that the encoding of the problem also affects the solution quality. For example, we see that heuristically ordering the objects from most to least valuable, has a great impact in the solution quality. We think that this type of ordering reduces the necessity for the method to carry long-distance correlations. 

Our investigation also shows that as the size of the problem increases to industrially relevant dimensions, the effectiveness of the GEO method decreases on par to simulating annealing. In fact, we could not find a situation in which GEO produced better results than the standard simulated annealing method. We believe that this limitation is due to the fact that when the initial training set is created randomly, most of the cost evaluations will get heavily penalized by the softmax function, leading to extremely low probabilities. GEO, then struggles to generalize to better solutions. If an initial training dataset can be constructed with only valid solutions, however, we observed that GEO tended to converge quickly to the best solution given in the set, rather than generalize to a lower one. We attribute this behavior to 2 factors: the encoding and inability of MPSs to capture long-range correlations. If one would know, a priori, a problem encoding whose solution is known to be locally correlated, then we believe GEO with MPSs could show good performance. However, as the problem sizes grow, it is doubtful that such an encoding can be found. In our experiments, we tested heuristically ordering the objects from higher to lower weight for the encoding which showed the best results for small to medium problem instances. For larger instances, we saw no improvement with such a heuristic.

In future work, one could explore the behavior of GEO if the generative model is taken to be a quantum model. Such choice will increase the range of correlations which can be captured by the model, at the expense of possibly reducing the model's generalizability. It could be interesting to explore if there are quantum or classical generative models which exhibit large range correlations and also good generalization. Other possible avenues for exploration include introducing a mutation step to the GEO scheme \cite{gardiner2024} and using adaptive learning rate schemes to improve convergence. Additionally, feature maps \cite{stoudenmire2017supervisedlearningquantuminspiredtensor} could be incorporated to introduce non-linearities to the DMRG-like training. The use of compression layers \cite{meiburg2024generativelearningcontinuousdata} may also enable scaling to larger problem instances, particularly when the physical dimension (i.e., the number of knapsacks in the integer encoding) is large. Furthermore, comb tensor networks \cite{Chepiga_2019} could be utilized to extend the range of correlations that the generative model can represent and potentially reduce computational complexity, depending on the problem instance.
