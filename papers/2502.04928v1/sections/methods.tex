\section{Methods}

\begin{figure*}[!htbp]
    \centering
    \includegraphics[width=0.9\textwidth]{figures/01_geo_diagram.pdf}
    \caption{GEO pipeline. The process begins with the initial pairs of observations and their costs (step 0) and proceeds by selecting candidates for the training dataset (step 1) according to a selection strategy of choice. After selecting the candidates, the model computes a probability function $p(\mathbf{x})$ (step 2), followed by training the generative model (step 3). New samples are generated based on the trained model (step 4) and are then combined with the original training set (step 5). The updated training set is used for further iterations, continuing the loop for model refinement.}
    \label{fig:geo_diagram}
\end{figure*}

\subsection{Generative enhanced optimization}\label{sec:geo}
In this section, we discuss in detail the proposed TN-GEO pipeline closely following \cite{Alcazar2024,lopezpiqueres2023symmetric}. The outline of the algorithm is demonstrated in Fig. \ref{fig:geo_diagram}.
The aim is to solve a general optimization problem with equality constraints of the form 
\begin{equation}
  \min_\mathbf{x} c(\mathbf{x}),\,\, \text{s.t.  } A(\mathbf{x})= b  \;, \label{eq:optimization_problem}
\end{equation}
where $\mathbf{x}\in{\mathbb{N}}^L_0$, $c(\mathbf{x})$ is a scalar-valued cost function, $A(\mathbf{x})$ is a vector-valued function and $b$ is a vector. 

The main idea of the proposal is to use a generative model to generate new candidate solutions which may have better cost values with each iteration of the algorithm. The procedure is illustrated in Fig. \ref{fig:geo_diagram} and it works as follows: 

0) \textbf{Initialization}: A population $\mathcal {T}$ of candidate solutions, $\mathbf{x}$, encoding possible solutions of the optimization problem, are drawn w.r.t. a distribution of choice, e.g. uniformly drawn from the sample space or from a given set of samples which fulfill the constraints. Then the cost function of the optimization problem, $c(\mathbf{x})$, is evaluated for all $\mathbf{x}\in \mathcal {T}$. One can include in $c(\mathbf{x})$ the constraints of the problem as penalty terms. These constraints can be of any form and can thus be used to implement any business logic, which cannot be modeled by the equality constraints in Eq. \eqref{eq:optimization_problem}.

1) \textbf{Selection strategy}: If needed, a certain selection strategy is applied to filter out some of the candidates, and the population $\mathcal {T}$ is updated. This is done to improve the quality of the training data. The selection strategies that we use are detailed in Sec. \ref{sec:experiment_design}.

2) \textbf{Probability assignment}: Given all $\mathbf{x}\in \mathcal{T}$ and their cost function value $c(\mathbf{x})$, we assign each $\mathbf{x}$ a probability value via the softmax function
\begin{equation}
     p(\mathbf{x}) = \frac{e^{-\beta c(\mathbf{x})}}{\sum_{\mathbf{y}} e^{-\beta c(\mathbf{y})}},
\end{equation}
where $\beta$ is usually referred to as \emph{the inverse temperature} in the sense of the Boltzmann thermodynamic factor when $c(\mathbf{x})$ acts as the system energy. The idea of doing this is to give more weight to instances in which the cost is lower and conversely reduce the importance of values with higher cost. This biases the training of the generative model towards lower cost solutions. After this step, the data pairs ($\mathbf{x}$, $p(\mathbf{x})$) are used as input data to the generative model.

3) \textbf{Training generative model}: In general, the generative model, which is parametrized by a set of variables $\boldsymbol{\theta}$, can be any type of generative model, e.g., a deep neural network (GANs, VAEs, transformers, diffusion models, etc. \cite{9555209}), a quantum model (e.g., a VQC), or a tensor network. The generative model should learn the structure of the data, which leads to low cost solutions. This is similar to other gradient-free optimization methods e.g. evolutionary strategies \cite{hansen2019pycma}. The training happens in an internal loop (independent of the GEO loop) and it runs for a fixed number of iterations, or epochs, $N_e$, or until a sufficiently low training loss, $L(\boldsymbol{\theta})$ is achieved. Following \cite{Alcazar2024}, we use the negative log-likelihood (NLL) as the loss function for training the generative model, given by
\begin{equation}
    L(\boldsymbol{\theta}) = -\sum_{\mathbf{x}\in\mathcal T}p(\mathbf{x})\log\left(\mathbb{P}_{\boldsymbol\theta}(\mathbf{x})\right)\;,
\label{eq:nll_loss}
\end{equation}
where $\mathbb{P}_{\boldsymbol\theta}(\mathbf{x})$ is the sampling probability learned by the generator. We define the method for computing such probability in subsequent sections.

At the end of the training loop, the generator should be able to sample new solutions, $\mathbf{s}$, with probability $\mathbb{P}_{\boldsymbol{\theta}}(\mathbf{s})$. The intention is that the generator can generalize to produce solutions with lower cost $c(\mathbf{x})$. This has been shown empirically to be the case for situations in which the generator is given by a tensor network model \cite{Alcazar2024,banner2023quantuminspiredoptimizationindustrial}. It is extremely important at this stage not to let the training go so long as for the model to overfit and reproduce the training data exactly, leaving out any possibility of exploration of potential solutions outside of the training set. 
In this work, we train the generative model using a DMRG-inspired algorithm, which is described in Sec. \ref{sec:dmrg}.

4) \textbf{Sampling}: Once the generative model is trained, a new set of sampled solutions, $\mathcal{T}_s$ is generated by querying the generative model $N_s$ times. For all new samples $\mathbf{s}\in \mathcal{T}_s$ we compute the cost $c(\mathbf{s})$. We generate $\mathcal{T}_s$ via a method called perfect sampling, which is described in Sec. \ref{sec:perfect_sampling}. 
 
5) \textbf{Augmenting training set}: Then $\mathcal{T}_s$ is integrated to the original population $\mathcal T$ of possible solutions. Having combined the populations, we go back to step 1 (filtering the candidates), and repeat this procedure until a specified stopping criterion is reached. 
In this work, we stop the algorithm when the maximum number of iterations for the optimization loop has been reached.


\subsection{Matrix Product States (MPS)}\label{sec:mps}
    Tensor networks (TNs) are a powerful mathematical framework for representing and manipulating high-dimensional data, such as quantum states, images, and natural language. TNs can represent high-dimensional arrays, called tensors, as a network of interconnected lower-dimensional tensors. The nodes of the network correspond to the lower-dimensional tensors, and the edges correspond to the indices connecting them. The structure of the tensor network reflects the underlying structure of the data, such as the entanglement structure of a quantum state or the local correlations in an image. By exploiting this structure, we can represent high-dimensional data in a more efficient and compact way. There are many different types of tensor networks, each with its own set of rules and conventions. 
    Some of the most commonly used tensor networks include matrix product states (MPS), projected entangled pair states (PEPS) \cite{verstraete2004} and tree tensor networks (TTN) \cite{PhysRevA.74.022320}. 
    In this work we focus on MPS \cite{ORUS2014117}, also called tensor train (TT) \cite{Oseledets_2011}. 
    
Here, we introduce the TN formalism that is commonly used in quantum computing. Suppose we have a system of $L$ quantum particles with $d$ levels placed in a one dimensional spin lattice. Then the wave function $\ket\psi \in \mathbb{C}^{d^L}$ for the system can be expressed as an MPS, such that
\begin{align}
    & \ket{\psi} = \sum_{n_1 \dots n_L}A_{n_1, n_2, \dots, n_L} \ket{n_1 n_2 \dots n_L} \\
    & = \sum_{n_1 \dots n_L}\sum_{k_1 \dots k_{L-1}} T_{k_1}^{[1], n_1}T_{k_1 k_2}^{[2], n_2}\dots T_{k_{L-1}}^{[L], n_L} \ket{n_1 n_2 \dots n_L}.
\label{tn1}
\end{align}

This formulation decomposes the $L$-rank tensor $A$ into $L$ smaller tensors $T^{[i]}$, each with $2$ or $3$ legs (indices), which together form the MPS. This decomposition is illustrated in Fig. \ref{fig:mps_definition} using Penrose notation \cite{Penrose1971}. In Equation~\eqref{tn1}, the indices $k_1, k_2,\dots k_{L-1}$ represent the \emph{bond} between the adjacent spins, and $n_1, ..., n_L$ indicate the so-called \emph{physical} indices~\cite{SCHOLLWOCK201196}. Note that the first and the last tensors ($T_{k_1}^{[1],n_1}$ and $T_{k_{L-1}}^{[L],n_L}$) only have one subscript, as they both have only one bond. The tensors in between possess two bonds, each. The tensors with single subscript are represented with a vector, while the tensors with two subscripts are expressed as matrices.

\begin{figure*}[!htbp]
    \centering
    \includegraphics[]{figures/02_mps_diagram.pdf}
    \caption{Diagram of a Matrix Product State with $L=6$ elements.}
    \label{fig:mps_definition}
\end{figure*}

Apart from other benefits, MPS help represent a quantum state (wave function) with a lower number of complex scalars than the usual $d^L$, where $d$ is the dimension of each site (particle's Hilbert space) and $L$ is the number of sites. Note that for qubits, $d=2$. The total number of parameters required to express the quantum state in MPS is at most $L d \chi^2$. The MPS representation allows for an efficient description of the quantum state, as the number of parameters scales polynomially with the system size $L$, instead of exponentially as in the general case. This is useful for approximating many-body quantum states with limited entanglement classically. Note that any quantum state can be expressed as an MPS, but the MPS representation is not unique \cite{SCHOLLWOCK201196}. The maximum $\chi$ required to decompose an arbitrary tensor $A$ into a MPS exactly is $\chi\leq d^{L/2}$, but an approximation with a smaller $\chi$ is often sufficient.
Truncating the bond dimension can be used the limit tensor size and speed up computations, which is done in this work. 


\subsection{Enforcing cardinality constraints on MPS}\label{sec:symmetric_mps}

As suggested in \cite{lopezpiqueres2023symmetric}, we use symmetric tensors to represent quantum states that fulfill equality constraints as in the optimization problem of the form stated in Eq.~\eqref{eq:optimization_problem}. This paper focuses on cardinality-type constraints. In this section, we describe the case when the bits in a bitstring $\mathbf{x}$ must sum up to a certain number $b$. Thus $A(\mathbf{x})$ in Eq.~\eqref{eq:optimization_problem} becomes linear, i.e., $A(\mathbf{x}) = A\mathbf{x}$, with $A$ equal to the vector $[1,\ldots,1]$. 
Following~\cite{lopezpiqueres2023symmetric} and~\cite{PhysRevB.83.115125}, we briefly outline how symmetric tensors can be employed to enforce equality constraints. As a result MPS built from symmetric tensors will yield zero probability for all solutions which do not fulfill the constraint. 

The symmetry enforced here is the $U(1)$ symmetry, in general connected to the conservation of particle number. A symmetric tensor $T$ has only non-zero entries at index combinations which fulfill particle number conservation. It commutes with the action of the symmetry group and is decomposed into the direct sum over its components acting on the invariant subspaces, corresponding to a block-diagonal structure (\emph{Schur's lemma}).

\begin{figure}[!htbp]
    \centering
    \includegraphics[width=0.35\textwidth]{figures/03_binary_stn.pdf}
    \caption{Schematic of the charge conservation for a symmetric tensor network. We use arrows to show incoming and outgoing indices $k_i$, with possible values of the charge written in gray on top of each arrow. Dashed arrows display incoming particle number $b$ and outgoing $0$, which follows from Eq. \eqref{eq:charge_conservation}.}
    \label{fig:symmetric_TN}
\end{figure}

To enforce the cardinality constraint, the particle number is identified with the bit values and the total incoming particle number (charge) is set to $b$, as shown in Fig.~\ref{fig:symmetric_TN}. The physical indices $n_j$ of the tensors correspond to the bit values ($0$ or $1$) having dimension $2$, while the bond dimension of the contracted indices $k_j$ is determined by the possible values of the total particle number. For instance, the size of leg labeled $k_1$ is $2$, since the particle number can either be $b$ or $b-1$, depending on the value of $n_1$, while the leg labeled $k_2$ needs size $3$ to account for possible particle numbers $b, b-1, b-2$, depending on the values of $n_1, n_2$.
For tensor $T^{[j]}$, the particle number conservation reads
\begin{equation}\label{eq:charge_conservation}
    k_{j-1} = n_{j} + k_{j}.
\end{equation}
Each element of such a symmetric tensor is zero if the equality above is not fulfilled:
\begin{equation}\label{eq:symmetric_elements}
    T^{[j], n_j}_{k_{j-1},k_j}=T^{[j], n_j}_{k_{j-1},k_j}\delta_{k_{j-1},n_{j} + k_{j}},
\end{equation}
where $\delta_{i,j}$ is the Kronecker delta function. 
Consider an example when $b=1$. The elements of the corresponding symmetric MPS are shown in table \ref{tab:sym_mps}.
\begin{table}[h]
    \centering
    \begin{tabular}{|>{\centering\arraybackslash}m{1.8cm}|>{\centering\arraybackslash}m{2.8cm}|>{\centering\arraybackslash}m{2.8cm}|}
        
        \hline
        $l$ & $T^{[l],n_l=0}$ & $T^{[l],n_l=1}$ \\
        \hline
        $0$ 
        & $\begin{pNiceMatrix}[first-row,first-col]
    & 0 & 1 \\
    1 & 0 & * \\
    \end{pNiceMatrix}$ 
    & $\begin{pNiceMatrix}[first-row,first-col]
    & 0 & 1 \\
    1 & * & 0 \\
    \end{pNiceMatrix}$ \\
        \hline
        $1 \leq l \leq L-1$ 
        & $\begin{pNiceMatrix}[first-row,first-col]
    & 0 & 1 \\
    0 & * & 0 \\
    1 & 0 & * \\
    \end{pNiceMatrix}$
    & $\begin{pNiceMatrix}[first-row,first-col]
    & 0 & 1 \\
    0 & 0 & 0 \\
    1 & * & 0 \\
    \end{pNiceMatrix}$\\
        \hline
        $L$
        & $\begin{pNiceMatrix}[first-row,first-col]
    & 0  \\
    0 & *  \\
    1 & 0  \\
    \end{pNiceMatrix}$
    & $\begin{pNiceMatrix}[first-row,first-col]
    & 0  \\
    0 & 0  \\
    1 & *  \\
    \end{pNiceMatrix}$ \\
    \hline
    \end{tabular}
    \caption{Symmetric MPS elements for $b=1$ satisfying the constraints in Eq. \eqref{eq:symmetric_elements}. Each matrix corresponds to the $l$-th MPS element with a fixed physical index $n_l$ (either $0$ or $1$). The row and column indices (in blue) represent the bond indices of each MPS element: row indices are values of $k_{j-1}$ (incoming), and column indices are values of $k_j$ (outgoing). An asterisk $*$ denotes non-zero entries, which may be single scalars or entire blocks of larger dimension. When blocks appear, the  indices can be duplicated to maintain notation; for instance, if the block associated with row index $0$ has two rows, one might label them $0_A$ and $0_B$.}
    \label{tab:sym_mps}
\end{table}
Using that the incoming particle number is defined as $b$, the cardinality constraint directly follows:
\begin{equation}
    b = n_1 + k_{1} = n_1 + n_2 + k_{2} =\cdots= \sum_{j=1}^L n_j = \sum_{j=1}^L x_j,
\end{equation}
where $L$ denotes the number of tensors in the MPS. 
Together with $k_{N-1} = n_N$, the values of all bond indices can be determined if the physical indices $n_1,...,n_N$ are given~\cite{lopezpiqueres2023symmetric}. 

A concrete example of a symmetric MPS for the generalized knapsack problem with $b=[1,\ldots,1]^{\top}$, is given in the next section.

\subsection{Encoding the generalized multi-knapsack problem}\label{sec:knapsack}


The mulit-knapsack problem deals with the optimal assignment of $N$ objects to $M$ knapsacks. Each object, $i$, has a weight $w_{i}$ and a value $v_{ij}$ when assigned to knapsack $j$. In turn, each knapsack has a maximum capacity $m_j$. The goal is to maximize the value carried by all knapsacks while not exceeding their maximum capacities and including all objects. This problem is closely related to manufacturing setups in which one needs to minimize production costs (negative total value) when assigning (building) all products (objects) to all available manufacturing machines (knapsacks) while not exceeding their maximum capacities. In this analogy, the weights are the demanded number of products. In the following, we discuss the formulation of this problem in binary and integer types of encoding.
In this work, we consider two different types of encoding, binary and integer, as we find that encoding has an impact on the success of the overall method as shown in the results in Sec. \ref{sec:results}.

{\bf Binary encoding.}
Mathematically, we can formulate the knapsack problem as follows
    \begin{equation}\label{eq:knapsack_problem_formulation}
        \max_x \sum_{j=1}^{M}\sum_{i=1}^{N} v_{ij} x_{ij},
    \end{equation}
    subject to
    \begin{equation}\label{eq:knapsack_capacity_constraint_ineq}
        \sum_{i=1}^{N} w_{i} x_{ij} \leq m_j \hspace{2em}\forall j\;,
    \end{equation}
    and 
    \begin{equation}\label{eq:knapsack_assigment_constraint_eq}
        \sum_{j=1}^{M} x_{ij} = 1 \hspace{2em}\forall i\;,
    \end{equation}
where $\mathbf{x}=(x_{ij})$ consists of binary variables, which are 1 if the object $i$ is assigned to knapsack $j$ and 0 otherwise. Note that Eq.~\eqref{eq:knapsack_assigment_constraint_eq} forces all objects to be assigned to a knapsack, which is a generalization of the classical multi-knapsack problem.

This formulation can be mapped to Eq. \eqref{eq:optimization_problem} as follows. For the constraints we have
\begin{align*}
A_{ij} = \begin{cases}
    1,& \text{if } M(i-1)+1 \leq j \leq Mi, \\
    0,& \text{otherwise}
\end{cases}
\end{align*}
and $A(x)=Ax$, as well as $b= (1,\ldots,1)^\top$.
The objective function is defined as 
\begin{align*}
    c(\mathbf{x}) & = c_p \sum_{j=1}^{M}\max\left\{0, \sum_{i=1}^{N}(w_{i} x_{ij}-m_j)\right\} -\sum_{j=1}^{M}\sum_{i=1}^{N} v_{ij} x_{ij},
\end{align*}
where $c_p$ is a big enough penalty coefficient for including the inequality constraint in the objective.
When the assignment constraints from Eq. \eqref{eq:knapsack_assigment_constraint_eq} are fulfilled, the search space is reduced from $2^{MN}$ (all possible bitstrings) to $M^N$.
We represent these constraints using a symmetric MPS composed of 
$N\cdot M$ tensors, one for each binary variable corresponding to an object–knapsack pair. These tensors are grouped into 
$N$ segments, each of length 
$M$ and associated with one object. Within each segment, the bond dimension is 
$2$; between segments, it is $1$. As a result, the entire construction looks like 
$N$ small MPS arranged in a line, each encoding how a single object can be assigned across the 
$M$ knapsacks. These tensors are initialized according to Sec.~\ref{sec:symmetric_mps}. More specifically, we initialize each tensor as shown in Table \ref{tab:sym_mps} where $*$ is replaced with $1$.

An example of encoding cardinality constraints from Eq. \eqref{eq:knapsack_assigment_constraint_eq} into an MPS is shown in Fig. \ref{fig:sym_mps_example} for a knapsack problem with $N=2$ objects and $M=3$ knapsacks. As each object needs to be assigned once, the MPS needs to fulfill the following constraints:
\begin{align*}
        x_{11}+x_{12}+x_{13} &= 1, \\
        x_{21}+x_{22}+x_{23} &= 1, \\
\end{align*}
The incoming charge vector is given as ${b}=[1,1]$ and it changes based on the index of each physical leg.


\begin{figure}[!htbp]
    \centering
    \includegraphics[width=0.5\textwidth]{figures/04_binary_example.pdf}
    \caption{Schematic of the charge conservation in a MPS with binary encoding for a knapsack problem with $N=2$ objects and $M=3$ knapsacks. Each $x_{ij}$ is mapped to an index of the physical leg of $T^{[M(i-1)+j]}$, and the cardinality constraint reads $\sum_{j=1}^{M} x_{ij} = 1$. The incoming charge vector changes based on the physical leg index at each tensor, preserving the particle. The numbers $0$ and $1$ above the vertical leg correspond to physical index values, showing their influence on the charge vector by adjusting the charge value to their right accordingly. The indices and charge vectors in red demonstrate and example of assigning the $1$st object to the $2$nd knapsack, and the $2$nd object to the $3$rd knapsack. Black font denotes other possible indices and charge vectors unrelated to the example. Dashed arrows denote dimensions equal to $1$.
    }
    \label{fig:sym_mps_example}
\end{figure}
{\bf Integer encoding.}
We reformulate the problem from Eq. \eqref{eq:knapsack_problem_formulation}-\eqref{eq:knapsack_assigment_constraint_eq} in the following way:
\begin{equation}
    \max_y\sum_{i=1}^{N}v_{i,y_i}
\end{equation}
subject to
\begin{equation}\label{eq:knapsack_integer_ineq}
    \sum_{i=1}^{N}w_{i}\delta_{j, y_i}\leq m_{j},
\end{equation}
where $y_i\in\{1,...,M\}$ is the knapsack which contains object $i$.
The corresponding MPS consists of $N$ elements with physical dimension $M$, and does not require symmetry, since the cardinality constraints from Eq. \eqref{eq:knapsack_assigment_constraint_eq} are not needed anymore. Note that, in contrast to the binary formulation shown earlier, the physical legs of the tensors have dimension $d=M$. The MPS for the problem discussed above is illustrated in Fig. \ref{fig:nonsym_mps_example}.
\begin{figure}[!htbp]
    \centering
    \includegraphics[width=0.12\textwidth]{figures/05_integer_example.pdf}
    \caption{MPS with integer encoding for a knapsack problem with $N=2$ objects and $M=3$ knapsacks. Red color denotes an example of assigining the $1$st object to the $2$nd knapsack, and the $2$nd object to the $3$rd knapsack, i.e. $y=(2, 3)^\top$.}
    \label{fig:nonsym_mps_example}
\end{figure}

This formulation can be mapped to Eq. \eqref{eq:optimization_problem} as a minimization of the following cost function with no equality constraints:
\begin{align*}
    c(y) & = c_p \sum_{j=1}^{M}\max\left\{0, \sum_{i=1}^{N}(w_{i}\delta_{j, y_i}-m_j)\right\} - \sum_{i=1}^{N}v_{i,y_i},
\end{align*}
where $c_p$ is the penalty coefficient corresponding to the inequality constraint, Eq.~\eqref{eq:knapsack_integer_ineq}.

As an example of both types of encoding, consider the case where we have $N=2$ objects and $M=3$ knapsacks.
If the first object is assigned to the second knapsack, and the second object is assigned to the third knapsack, the corresponding encodings for the respective decision variables are as follows:
\begin{align*}
    (x_{ij})_{ij} =  
    \begin{pmatrix}
     0   & 1   & 0 \\
     0   & 0   & 1 \\
    \end{pmatrix}, 
    & {} &
    (y_i)_{i} = \begin{pmatrix} 2 \\ 3 \end{pmatrix}.
\end{align*}

The main difference between the two methods is the initialization of the MPS: binary encoding starts with a STN model (symmetric MPS) uniformly covering feasible samples, while integer encoding uses a random initialization of a TN model (random MPS). Both models can be trained in similar ways via the optimization procedure described in Sec. \ref{sec:dmrg}. Furthermore, both TN models will act as generators with the ability to sample potential solutions to the optimization problem. In this context, the free parameters of the TN models are then adjusted so that the model can learn the correlations present in the potential solutions. 

\subsection{DMRG-inspired training}\label{sec:dmrg} 

The density matrix renormalization group (DMRG) algorithm \cite{RevModPhys.77.259, PhysRevB.48.10345} was originally developed to optimize a MPS via a localized operation to manage the complexity associated with high-dimensional data and large tensor networks. Here, we present an algorithm inspired by DMRG aimed at minimizing the loss function in Equation~\eqref{eq:nll_loss}. This algorithm is favored for training generative models parameterized by TNs \cite{stoudenmire2016supervised}. Direct optimization of the entire MPS is computationally expensive, so this localized optimization approach helps to manage the complexity associated with high-dimensional data and large tensor networks.


There are two primary approaches in DMRG: one-site \cite{PhysRevB.72.180403} and two-site methods \cite{SCHOLLWOCK201196}. The one-site approach focuses on optimizing a single tensor at a time while keeping the rest of the MPS fixed. While this method is computationally cheaper, it can suffer from poor convergence and does not allow control of the bond dimension. The two-site approach, on the other hand, optimizes two adjacent MPS elements together, allowing the optimization process to capture the local correlations between them more effectively \cite{SCHOLLWOCK201196}. After the optimization, the combined tensor is decomposed back into two elements, preserving the MPS structure. This additional flexibility helps stabilize the optimization and ensures better control over the bond dimensions, leading to improved performance. For these reasons, we choose the two-site method for optimizing our TN generative models.
The algorithm we are using can be summarized in Algorithm \ref{alg:sweep}. 

\begin{algorithm}
\caption{Optimization Sweep}
\begin{algorithmic}[1]
\STATE \textbf{Input:} $\text{MPS}=\left\{T^{[j]}\right\}_{j=1}^l, \mathcal T$
\STATE \textbf{Output:} Optimized $\text{MPS}=\left\{T^{[j]}\right\}_j$

\STATE $\text{MPS} = \text{RIGHT\_CANONICAL}\left(\text{MPS}\right)$

\STATE \textbf{Left-to-right sweep:}
\FOR{$i = 1 \text{ to } (l - 1)$}
    \STATE $T^{[i]}, T^{[i+1]} =  \text{DMRG\_L2R\_UPDATE}\left(T^{[i]}, T^{[i+1]}, \mathcal T\right)$
    
\ENDFOR

\STATE \textbf{Right-to-left sweep:}
\FOR{$i = l \text{ to }2$}
    \STATE $T^{[i-1]}, T^{[i]} =  \text{DMRG\_R2L\_UPDATE}\left(T^{[i-1]}, T^{[i]}, \mathcal T\right)$
\ENDFOR

\STATE \textbf{return} $\text{MPS}=\left\{T^{[j]}\right\}_{j=1}^l$

\end{algorithmic}\label{alg:sweep}
\end{algorithm}

Here, $\text{RIGHT\_CANONICAL}\left(\left\{T^{[i]}\right\}_i\right)$ transforms the given MPS into the right-canonical \cite{SCHOLLWOCK201196} form vis SVD or QR decomposition, and normalizes it. The definition of the canonical form and the procedure are described in detail in Appendix \ref{appendix:right_canonical}. Both $\text{DMRG\_L2R\_UPDATE}$ and $\text{DMRG\_R2L\_UPDATE}$ use the same procedure, as in \cite{lopezpiqueres2023symmetric}, but adapted for our specific problem. The procedure involves contracting two MPS elements, updating the resulting tensor using the gradient descent, and decomposing the tensor via singular value decomposition (SVD).

Let us examine $\text{DMRG\_L2R\_UPDATE}\left(T^{[i]}, T^{[i+1]}, \mathcal{T}\right)$ procedure in detail, illustrating each step with figures. 

0. \textbf{Current setup:} At iteration $i$, the MPS is in a mixed canonical form centered at site $i$.

1. \textbf{Merging Tensors:}
   For the index $i$, indicating the canonical center of the MPS at this step, merge $T^{[i]}$ and $T^{[i+1]}$ by contracting their connecting leg $k_{i}$:
   \begin{align}
       T^{[i, i+1], n_i, n_{i+1}}_{k_{i-1}, k_{i+1}} := \sum_{k_{i}} T^{[i], n_i}_{k_{i-1}, k_{i}} \, T^{[i+1], n_{i+1}}_{k_{i}, k_{i+1}}.
   \end{align} 
The resulting tensor $T^{[i, i+1]}$ is symmetric, where the link charges are constrained by  $k_{i-1} + k_{i+1} = 1 - n_i - n_{i+1}$. 

   \begin{figure}[ht]
       \centering
       \includegraphics[]{figures/06_merged_tensor.pdf}
       \caption{Example of merging MPS elements $T^{[2]}$ and $T^{[3]}$ during $\text{DMRG\_L2R\_UPDATE}$. The green ($T^{[1]}$) and blue ($T^{[3-6]}$) colors indicate left- and right-canonical elements respectively, red ($T^{[2]}$) color corresponds to the canonical center at $i=2$.}
       \label{fig:merge_mps}
   \end{figure}

2. \textbf{Computing the Gradient:}
   Compute the gradient of the loss function with respect to $T^{[i, i+1]}$. The gradient of the NLL loss from Eq. \eqref{eq:nll_loss} is
   \begin{align}
       \frac{\partial L}{\partial T^{[i, i+1]}} = T^{[i, i+1]} - 2 \sum_{x \in \mathcal{T}} p(x) \frac{\Psi'(x)}{\Psi(x)},
    \label{eq:dLdM}
   \end{align}
   where $\Psi'(x) = \frac{\partial \Psi(x)}{\partial T^{[i, i+1]}}$. An example of computing the gradient in the form of a tensor diagram is shown in Fig. \ref{fig:grad_diagram}. The derivation of this expression is provided in the Appendix \ref{appendix:gradient}. 

3. \textbf{Updating the merged tensor:}
   Update $T^{[i, i+1]}$ using gradient descent:
   \begin{align}
       T^{[i, i+1]} \leftarrow T^{[i, i+1]} - \alpha \frac{\partial L}{\partial T^{[i, i+1]}},
   \end{align}
   where $\alpha$ is the learning rate. The symmetry of the merged tensor is preserved after the update.
   \begin{figure}[ht]
       \centering
       \includegraphics[]{figures/07_gradient_update.pdf}
       \caption{Updating the merged tensor at $i=2$.}
       \label{fig:grad_update}
   \end{figure}

4. \textbf{Decomposing via SVD:}
   Decompose the updated tensor $T^{[i, i+1]}$ via SVD. First, reshape $T^{[i, i+1]}$ into a matrix $\tilde T$ by combining the legs $k_{i-1}$ and $n_i$ into a single index, and $k_{i+1}$ and $n_{i+1}$ into another: $\tilde T_{dk_{i-1}+n_i,dk_{i+1}+n_{i+1}}=T^{[i, i+1],n_i, n_{i+1}}_{k_{i-1}, k_{i+1}}$, where $d$ is the dimension of the physical leg.
   
   \begin{figure}[ht]
       \centering
       \includegraphics[]{figures/08_svd_diagram.pdf}
       \caption{SVD of the reshaped merged tensor with truncation size $\chi$.}
       \label{fig:svd_diagram}
   \end{figure}

   Perform SVD of $\tilde{T}$ to obtain matrices $U$, $\Lambda$, and $V^\dagger$ such that:
   \begin{align}
       \tilde{T} = U \Lambda V^\dagger.
   \end{align}
   We then can truncate the matrices by a maximum bond dimension $\chi$: remove the columns, $j$, of $U$ for all $j>\chi$, remove rows, $i$, and columns, $j$, of $\Lambda$ for all $i>\chi, j>\chi$, remove rows, $i$, of $V^\dagger$ for all $i>\chi$. Finally, normalize the singular values $\Lambda\leftarrow\Lambda/\|\Lambda\|$.

5. \textbf{Updating the MPS:}
    Reshape $U$ and $\Lambda V^\dagger$ to update $T^{[i]}$ and $T^{[i+1]}$ as follows:
   \begin{align}
       T^{[i], n_i}_{k_{i-1}, k_{i}} &= U_{d k_{i-1} + n_i, k_{i}}, \\
       T^{[i+1], n_{i+1}}_{k_{i}, k_{i+1}} &= \left(\Lambda V^\dagger\right)_{k_{i}, d k_{i+1} + n_{i+1}},
   \end{align}
   where $d$ is the dimension of the physical leg.
   \begin{figure}[ht]
       \centering
       \includegraphics[]{figures/09_u_reshaped.pdf}
       \caption{Update of $T^{[i]}$ with reshaped $U$.}
       \label{fig:u_reshaped}
   \end{figure}
   \begin{figure}[ht]
       \centering
        \includegraphics[]{figures/10_lv_reshaped.pdf}
       \caption{Update of $T^{[i+1]}$ with reshaped $\Lambda V^\dagger$.}
       \label{fig:lv_reshaped}
   \end{figure}

\begin{figure*}[ht]
   \centering
   \includegraphics[]{figures/11_gradient_diagram.pdf}
   \caption{Example of computing the gradient of the NLL loss w.r.t. $T^{[i, i+1]}$ at $i=2$. We can see the charge conservation is preserved when all samples $x$ from the dataset $\mathcal T$ fulfill the cardinality constraint.}
   \label{fig:grad_diagram}
\end{figure*}

Note that the factorization of a $U(1)$ symmetric matrix via SVD preserves the symmetry \cite{PhysRevB.83.115125}, therefore the updated MPS elements remain symmetric. Morevover, the canonical center moves to $i+1$, and the norm of the MPS remains $Z=1$.


In a similar manner, $\text{DMRG\_R2L\_UPDATE}\left(T^{[i-1]}, T^{[i]}, \mathcal T\right)$ performs a similar update, except that the canonical center moves from $i$ to $i-1$ and step 5 is modified in the following way.

5. \textbf{Updating the MPS (for the right-to-left sweep):}
    Reshape $U\Lambda$ and $V^\dagger$ to update $T^{[i]}$ and $T^{[i+1]}$ as follows:
   \begin{align}
       T^{[i-1], n_{i-1}}_{k_{i-2}, k_{i-1}} & \leftarrow \left(U\Lambda\right)_{d k_{i-2} + n_{i-1}, k_{i-1}}, \\
       T^{[i], n_{i}}_{k_{i-1}, k_{i}} & \leftarrow V^\dagger_{k_{i-1}, d k_{i} + n_{i}}.
   \end{align}


\subsection{Perfect Sampling}\label{sec:perfect_sampling}

Once the TN model is trained, we need to generate samples from the generative model. For this, we describe the perfect sampling algorithm \cite{PhysRevB.85.165146}, which is designed to be efficient for sampling generative models parametrized by TNs. In this section we present a perfect sampling algorithm specifically designed for right-canonical MPS with unit norm. The objective of the algorithm is to sample configurations $\hat n = \{ \hat{n}_i \}_{i=1}^L$ according to the probability distribution defined by the MPS:
\begin{align}
    \mathbb{P}(\hat n)=\mathbb{P}(\hat n_1)\mathbb{P}(\hat n_2|\hat n_1)\cdots\mathbb{P}(\hat n_L|\hat n_1\ldots\hat n_{L-1})=|\Psi(\hat n)|^2,
\end{align}
where $\Psi(\hat n)$ is the MPS evaluated at the configuration $\hat{n}$:
\begin{align}
    \Psi(\hat n) = \sum_{k_1, k_2, \ldots, k_{L-1}}T^{[1],\hat n_1}_{k_1}T^{[2],\hat n_2}_{k_1,k_2}\cdots T^{[L],\hat{n}_L}_{k_{L-1}}.
\end{align}

1. \textbf{Initialization:} Start with the computation of the marginal probability for the first site $n_1\in\{1,\ldots,d\}$:
\begin{align}
    \mathbb{P}(n_1) = \sum_{k_1} T^{[1],n_1}_{k_1} \overline{T}^{[1],n_1}_{k_1}.
\end{align}
Sample $\hat n_1$ according to the obtained distribution. Initialize the projected part of the MPS $P_{k_1} = T^{[1], \hat{n}_1}_{k_1}$.

2. \textbf{Sequential Sampling:} For each subsequent site $j=1,\ldots,L-1$ repeat the following steps:
\begin{itemize}
    \item Compute conditional probabilities for all $n_j\in\{1,\ldots,d\}$:
    \begin{equation}
    \begin{split}
        &\mathbb{P}(n_j | \hat{n}_1, \dots, \hat{n}_{j-1}) =\\& \sum_{k_{j-1}, k_{j-1}'} \sum_{k_j} P_{k_{j-1}}\overline{P}_{k_{j-1}'} T^{[j], n_j}_{k_{j-1}, k_j}\overline{T}^{[j], n_j}_{k_{j-1}', k_j}.
    \end{split}
    \end{equation}
    \item Normalize the probabilities and sample $\hat n_j$ accordingly.
    \item Update the projected part $P$: \begin{align}
        P_{k_j} \leftarrow \sum_{k_{j-1}} P_{k_{j-1}} T^{[j], \hat{n}j}_{k_{j-1}, k_j}.
    \end{align}
\end{itemize}

3. \textbf{Final Site Sampling:} Compute conditional probabilities for the final site $n_L\in\{1,\ldots,d\}$:
\begin{align}
    &\mathbb{P}(n_L | \hat{n}_1, \dots, \hat{n}_{L-1}) =\\& \sum_{k_{L-1}, k_{L-1}'} P_{k_{L-1}}\overline{P}_{k_{L-1}'} T^{[L], n_L}_{k_{L-1}}\overline{T}^{[L], n_L}_{k_{L-1}'}.
\end{align}

The algorithm samples configurations exactly according to the probability distribution $p(\hat n)$, providing accurate statistical representations of the quantum state without approximation. By computing probabilities for all possible values $n_i\in\{1,\ldots,d\}$ at each step and exploiting the right-canonical form of the MPS, the computational complexity scales as $Ld\chi^2$, where $\chi$ is the bond dimension of the MPS.


\subsection{Experiment design}\label{sec:experiment_design} 

For our benchmarks, we create multiple instances of knapsack problems of different sizes, which we design in such a way that we know there is at least one valid solution. This is done to avoid the uncertainty in case the method does not find a feasible solution, i.e., whether the method did not find a feasible solution because a solution does not exist or because it found a local minimum that does not correspond to a feasible solution. We create these instances as follows
\begin{itemize}
    \item Choose a number of objects $N$, and a number of knapsacks $M$.
    \item Generate random arrays of values $v_{ij}\in\mathbb{N}$ and weights $w_{i}\in\mathbb{N}$ corresponding to the objects, $i$, and knapsacks, $j$, needed.
    \item Run an exact optimization solver to find the optimal solution. We use the Gurobi solver \cite{gurobi}. This is possible here, because we consider small problem instances. For larger problems this might be prohibitively expensive. 
    \item When a solution is found, a check is made to see if all objects were assigned to a knapsack. If so, the instance and the found solution are saved. If not, the unassigned objects are dropped from the problem instance and the reduced instance is solved again with Gurobi, whose result is saved together with the reduce problem instance.
\end{itemize}
 We create problems with $4$ to $58$ objects and $4$ to $10$ knapsacks, which lead to problems with search space sizes in the  range from $256$ to $10^{58}$ possible solutions.

In our tests, we run all TN- and STN-GEO experiments for a large combination of hyper-parameters. Our intention is to determine which combination of parameters yields the best results. We summarize our choices in Table \ref{table:hyper_parameters}.
\begin{table}[!h]
    \centering
    \begin{tabular}{|c|c|c|}
        \hline
        Parameter & Symbol & Values \\
        \hline
       Inverse temperature & $\beta$ & 0.1, 0.01, 0.001 \\
        \hline
        Learning rate & $\alpha$ & 0.001, 0.0001 \\
        \hline
        Number of epochs & $N_e$ & 1, 3, 5, 10 \\
        \hline
        Bond dimension & $\chi$ & 4, 8, 16, 32 \\
        \hline
        Number of samples & $N_s$ & $10\cdot M\cdot N$ \\
        \hline
    \end{tabular}
    \caption{GEO hyperparameters. We run our experiments for all combinations of the hyperparameters shown to identify the set of parameters which yield the best overall results.}
    \label{table:hyper_parameters}
\end{table}
Additionally, as discussed in Sec.~\ref{sec:geo}, we define a selection strategy to decide, which data is used for training of the generative model at each step.  In this work, we compare four selection strategies. Each optimization loop begins by combining the new population of samples $\mathcal T_s$ with the current set $\mathcal{T}$ and retaining only unique candidates. The subsequent filtering steps differ among the strategies: 
\begin{itemize} 
\item \textbf{All:} No additional filtering is applied. 
\item \textbf{Best:} Retain only the $|\mathcal{T}|$ samples with the lowest costs. 
\item \textbf{Symmetric:} Remove infeasible samples, i.e., those that do not satisfy the equality constraints. This strategy is useful when the symmetry is not perfectly preserved by the generative model itself (e.g. precision error during SVD or a non-symmetric model).
\item \textbf{Best Symmetric:} Remove infeasible candidates, then retain the $|\mathcal{T}|$ samples with the lowest costs. \end{itemize}

An example of how these strategies work is provided in Table \ref{table:selection_strategies} for a simple case of two knapsacks and two objects. Columns $x$ and $c(x)$ correspond to different candidates and the corresponding costs respectively. The rest of the columns contain \xmark\color{black}\,if a sample is removed from the set, and \cmark\color{black}\,otherwise, depending on the selection strategy.

\begin{table}[!h]
    \centering
    \begin{tabular}{|c|c|c|c|c|c|}
        \hline
        {$x$} & $c(x)$ & \textbf{all} & \textbf{best} & \textbf{symmetric} & \textbf{best symmetric} \\
        \hline
        $\color{ForestGreen}{\begin{pmatrix}
     0   & 1 \\
     0   & 1\\
    \end{pmatrix}}$ & ${-6}$ & \cmark & \cmark & \cmark & \cmark \\ 
        \hline
        $\color{ForestGreen}{\begin{pmatrix}
     1   & 0 \\
     1   & 0\\
    \end{pmatrix}}$ & ${-4}$ & \cmark & \xmark & \cmark & \cmark \\ 
        \hline
        $\color{ForestGreen}{\begin{pmatrix}
     0   & 1 \\
     1   & 0\\
    \end{pmatrix}}$ & ${-2}$ & \cmark & \xmark & \cmark & \xmark \\ \hline
        \color{ForestGreen}${\begin{pmatrix}
     1   & 0 \\
     0   & 1\\
    \end{pmatrix}}$ & ${-8}$ & \cmark & \cmark & \cmark & \cmark \\ \hline
        $\color{red}{\begin{pmatrix}
     1   & 1 \\
     1   & 1\\
    \end{pmatrix}}$ & ${-10}$ & \cmark & \cmark & \xmark & \xmark \\ \hline
        $\color{red}{\begin{pmatrix}
     0   & 0 \\
     0   & 0\\
    \end{pmatrix}}$ & ${0}$& \cmark & \xmark & \xmark & \xmark \\ \hline
    \end{tabular}
    \caption{Different selection strategies applied to a specific set of samples for the problem defined in Eq. \eqref{eq:optimization_problem} with $x\in\{0,1\}^{2\times2}$, $A(x)=(x_{11}+x_{12}, x_{21}+x_{22})^\top$, $b=(1 , 1)^\top$ and $c(x)=-3x_{11}-x_{12}-x_{21}-5x_{22}$. Feasible and infeasible candidates have green and red color respectively. The symbol {\cmark} means the sample remains in the training dataset, while {\xmark} means the sample is filtered out. The strategies \textbf{Best} and \textbf{Best Symmetric} choose 3 best costs that fit their requirements.}
    \label{table:selection_strategies}
\end{table}


Finally, for comparison, we also solve each problem instance via two other methods: random sampling and simulated annealing (SA). In both cases, we chose the number of samples to be the maximum number of cost function evaluations $n_f$ used in the GEO method per problem instance. Our GEO formulation samples only from a search space of $M^N$ elements (since we encode the assignment constraints directly in the structure of the TNs), therefore we adjust the random sampler and simulated annealer to do the same.
Without considering the assignment constraint, the search space for the binary encoding would be $2^{M\cdot N}$ and thus much larger than $M^N$.
We implement an ensemble-based SA algorithm where the size of the ensemble is kept as $10\cdot M\cdot N$. The SA protocol is iterated over $n_{\text{iter}}$ iterations such that the total number of samples equals to $n_f$. The initial temperature $T_{\text{initial}}$ is kept as half of the standard deviation of the initial ensemble. This choice of initial temperature is adapted from the work of Salamon~\emph{et. al}~\cite{Salamon1987FactsCA}. The final temperature $T_{\text{final}}$ is fixed to $1$, and the corresponding exponential cooling rate is computed as $\exp\left({\frac{1}{n_{\text{iter}}}}\cdot \ln\frac{T_{\text{final}}}{T_{\text{initial}}} \right)$. As for the acceptance criterion, we implement the Metropolis acceptance probability~\cite{Salamon1987FactsCA}. A solution candidate is always accepted to the new ensemble if its solution quality is better than the previous iteration. Solutions with poorer  quality are accepted with the metropolis acceptance probability. The best solution is logged over all the iterations and is reported as the final solution.
