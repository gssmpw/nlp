\section{Appendix}

\subsection{Canonical Matrix Product State}\label{appendix:right_canonical}

An important feature of the MPS representation is the ability to transform it into \emph{canonical form}. In this form, most tensors in the MPS become partial isometries, except for one special tensor called the \emph{canonical center}. This arrangement simplifies computational algorithms that update tensors one at a time, making complex calculations more manageable. 

When the MPS is in \emph{right-canonical form}, the tensors for positions $j\geq2$ satisfy the condition:
\begin{figure}[ht!]
  \centering
  \includegraphics[width=0.45\textwidth]{figures/right_canonical.pdf}\par
\end{figure}

In left-canonical form, for positions $j\leq L-1$, the tensors satisfy:

\begin{figure}[ht!]
  \centering
  \includegraphics[width=0.45\textwidth]{figures/left_canonical.pdf}\par
\end{figure}

In a \emph{mixed-canonical form}, there is a specific position $l$ called the canonical center where the tensor is arbitrary. Tensors to the left of this center (for $j<l$) satisfy the left-canonical condition, and tensors to the right (for $j>l$) satisfy the right-canonical condition.

An arbitrary MPS can be transformed into a canonical form using QR decomposition or SVD. Here, we describe how to get the right-canonical form using SVD. 

We are provided with pre-defined MPS for each site $i$ as $T^{[i]\hat{n}_i}$ where $\hat{n}_i\in \{0,1\}$ $\forall i$. We first need to form single site tensors for each $i$.
\begin{equation}
T^{[i]} = T^{[i] \hat{n}_1 } \oplus T^{[i] \hat{n}_2 } \hspace{2em} \forall i
\end{equation}

\begin{equation}
	T = T^{[1]} T ^{[2]} \dots T^{[n-1]}  T^{[n]}
\end{equation}

After obtaining the tensors for each site, we need to carry out singular value decomposition of the pair-wise contracted tensors, starting from the last site. Hence, the first step involves contracting the last and last but one tensors together to form $T^{[n-1, n]}$.

\begin{equation}
	T = T^{[1]} T ^{[2]} \dots \underbrace{ T^{[n-1]}  T^{[n]} }_\text{contract}
\end{equation}

The SVD on $T^{[n-1, n]}$ results in $U^{[n-1]}\cdot D^{[n-1]} \cdot V^{[n]}$, where $U^{[n-1]}$ and $V^{[n]}$ are unitary matrices and $D^{[n-1]}$ is a column vector (which later needs to be converted to a diagonal matrix).

\begin{equation}
	T = T^{[1]} T ^{[2]} \dots \underbrace{ T^{[n-1, n]} }_\text{SVD}
\end{equation}


\begin{equation}
	T = T^{[1]} T ^{[2]} \dots U^{[n-1]}\cdot D^{[n-1]} V^{[n]}
\end{equation}


$V^{[n]}$ now becomes the new tensor for the last site, while $U^{[n-1]}\cdot D^{[n-1]} $ now form the new tensor for site $n-1$.

\begin{equation}
	T = T^{[1]} T ^{[2]} \dots \underbrace{ U^{[n-1]}\cdot D^{[n-1]} }_{T^{[n-1]}} V^{[n]}
\end{equation}


\begin{equation}
	T = T^{[1]} T ^{[2]} \dots T^{[n-1]} V^{[n]}
\end{equation}

This process of pair-wise contraction, SVD is sequentially carried out till the first site. 

\begin{equation}
	T = T^{[1]} T ^{[2]} \dots \underbrace{T^{[n-2]}T^{[n-1]} }_\text{contract} V^{[n]}
\end{equation}


\begin{equation}
	T = T^{[1]} T ^{[2]} \dots \underbrace{T^{[n-2,n-1] }}_\text{SVD} V^{[n]}
\end{equation}


\begin{equation}
	T = T^{[1]} T ^{[2]} \dots  \dots \underbrace{ U^{[n-2]}\cdot D^{[n-2]} }_{T^{[n-2]}} V^{[n-1]} V^{[n]}
\end{equation}

\begin{equation}
	T = \underbrace{U^{[1]} \cdot D^{[1]}}_{T^{[1]}} V^{[2]} V^{[3]} \dots V^{[n-2]} V^{[n-1]} V^{[n]}
\end{equation}



\begin{equation}
	V^{[i]} \longrightarrow T^{[i]}, \hspace{1em} \forall i
\end{equation}


\subsection{Derivation of the gradient}
\label{appendix:gradient}

In this section, we derive the gradient $\frac{\partial L}{\partial T^{[i, i+1]}}$ from Eq. \eqref{eq:dLdM} of the negative log-likelihood (NLL) loss w.r.t. the merged tensor $T^{[i, i+1]}$:
\begin{equation}
    L = \sum_{x\in\mathcal T}p(x)\log(\mathbb{P}(x)),
\end{equation}
where $\mathbb{P}(x)$ is the probability of sampling $x$ from the MPS. More specifically,
\begin{equation}
    \mathbb{P}(x) = \frac{\left|\Psi(x)\right|^2}{Z}, \\
    Z=\sum_{x\in S}\left|\Psi(x)\right|^2,
\end{equation}

\begin{equation}
    x\sim \mathbb{P}(x) = \frac{\left|\Psi(x)\right|^2}{Z}, \\
    Z=\sum_{x\in S}\left|\Psi(x)\right|^2,
\end{equation}
where $S$ is the space of all possible samples (e.g. for the problem in \ref{fig:symmetric_TN}, $S=\{0,1\}^6$).

\begin{figure}[ht]
    \centering
    \includegraphics[]{figures/psi.pdf}
    \caption{Tensor diagram of $\Psi(x)$.}
    \label{fig:psi}
\end{figure}

Computing the derivative of the NLL function w.r.t. the merged tensor $T^{[i,i+1]}$ we get

\begin{align*}
    \frac{\partial L}{\partial T^{[i,i+1]}}  &= \sum_{x\in\mathcal T}p(x) \cdot\left(
    \frac{1}{\mathbb{P}(x)}\cdot\frac{\partial \mathbb{P}(x)}{\partial T^{[i,i+1]}}
    \right) \\
    &= \sum_{x\in\mathcal T}p(x)\cdot\left(\frac{Z}{\left|\Psi(x)\right|^2}\cdot\frac{2\Psi(x)\Psi'(x)Z-\left|\Psi(x)\right|^2 Z'}{Z^2}\right) \\
    &= \sum_{x\in\mathcal T}p(x)\cdot\left(\frac{Z}{Z'}-\frac{2\Psi'(x)}{\Psi(x)}\right) \\
    &= \frac{Z'}{Z} - 2\sum_{x\in\mathcal T}p(x)\frac{\Psi'(x)}{\Psi(x)},
\end{align*}
where $\Psi'(x)=\frac{\partial \Psi(x)}{\partial T^{[i,i+1]}}$ and $Z'=\frac{\partial Z}{\partial T^{[i,i+1]}}$.

\begin{figure}[ht]
    \centering
    \includegraphics[]{figures/psi_prime.pdf}
    \caption{Tensor diagram of $\frac{\partial\Psi(x)}{\partial T^{[2,3]}}$.}
    \label{fig:psi_prime}
\end{figure}

To simplify this equation further, we use the fact that the MPS has the canonical center at $i$, and that the MPS is normalized during the whole optimization process. Using these properties we conclude that $Z=1$ and $Z'=T^{[i,i+1]}$, which are visualized in Fig. \ref{fig:norm_mps}, \ref{fig:z_prime} respectively. 
\begin{figure}[ht]
    \centering
    \includegraphics[]{figures/norm_mps.pdf}
    \caption{Tensor diagram of $Z$ of a mixed canonical MPS.}
    \label{fig:norm_mps}
\end{figure}
\begin{figure}[ht]
    \centering
    \includegraphics[width=0.5\textwidth]{figures/z_prime.pdf}
    \caption{Tensor diagram of $\frac{\partial Z}{\partial T^{[2,3]}}$ of a mixed canonical MPS.}
    \label{fig:z_prime}
\end{figure}

Therefore, we are able to rewrite the gradient as
\begin{equation}\label{eq:dLdM_v2}
    \frac{\partial L}{\partial T^{[i,i+1]}} = T^{[i,i+1]} - 2\sum_{x\in\mathcal T}p(x)\frac{\Psi'(x)}{\Psi(x)}.
\end{equation}

\subsection{Why is the gradient symmetric?}
\label{appendix:gradient_symmetry}
Looking at Eq. \eqref{eq:dLdM_v2} and Fig. \ref{fig:grad_diagram}, it is sufficient to show that $\Psi'(x)$ is symmetric to prove that the gradient is symmetric as well.
Let's look at $\Psi'(x)$ as the gradient w.r.t. $T^{[2,3]}$ in more detail:
\begin{figure}[ht]
    \centering
    \includegraphics[]{figures/gradient_symmetry.pdf}
    \caption{Tensor diagram of $\frac{\partial \Psi(x)}{\partial T^{[2,3]}}$ of a mixed canonical symmetric MPS.}
    \label{fig:psi_prime_sym}
\end{figure}

from this illustration we get the following charge conservation:
\begin{align}
    n_{1,R}&=x_1 \\
    n_2&=x_2\\
    n_3&=x_3\\
    n_{3,R}&=x_4+x_5+x_6\\
\end{align}

When $x$ is feasible, $x_1+x_2+x_3+x_4+x_5+x_6=1$, we get the following equality:
\begin{equation}
    n_{1,R}+n_2+n_3+n_{3,R}=1,
\end{equation}
which corresponds exactly to the symmetry of the merged tensor $T^{[2,3]}$ (incoming flux $b=1$ and all legs are outgoing).

Therefore, as long as the samples $x$ in the training dataset $\mathcal T$ are feasible (fulfill the cardinality constraints), the gradient does not break the symmetry.
