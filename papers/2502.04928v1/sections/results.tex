\section{Results}\label{sec:results}

In this section, we present the results of using the TN- and STN-GEO solvers for the generalized multi-knapsack problem defined in Sec. \ref{sec:knapsack}. We compare the performance of binary and integer encoding for this problem. Both approaches use the same GEO optimization method; however, a block-wise implementation is employed when the MPS is symmetric. 

First we give a brief illustration of the GEO method in action for a simple problem instance, involving $N=7$ objects, $M=2$ knapsacks, an initial dataset of $N_s=14$ values with the same number of samples added to the training dataset at each iteration (selection strategy ``\textbf{all}''). Fig. \ref{fig:prob_dist_sym_4} shows the initial and final probability distributions of the samples after 50 GEO iterations, each containing one DMRG sweep. The samples are sorted by cost to situate higher-probability samples on the left. The reward is displayed as the negative cost function $-c(x)$.
This is done because we minimize the cost to maximize the reward. The vertical dashed line indicates where samples no longer fulfill the inequality constraints, and the inset zooms-in on the cost and probability distributions before the constraint violation point. Note that as the GEO iterations progress, the probability of sampling the optimal solution (left-most peak) increases, passing the probability of sampling that solution randomly. This shows the generalization capability of TN generative models.

\begin{figure*}[!htbp]
    \centering
    \includegraphics[width=1.\linewidth]{figures/12_distribution_example.pdf}
    \caption{This figure illustrates the optimization outcomes for a problem instance involving $2$ knapsacks and $7$ objects, resulting in a sample space of size $2^7=128$. Samples on the x-axis are sorted in decreasing order of the negative cost function $-c(x)$, shown as blue line. The gray distribution represents the probability of each sample generated from a randomly initialized MPS using integer encoding. After performing $50$ iterations of the GEO optimization algorithm, the green distribution shows the updated probabilities from the optimized MPS. The dashed horizontal black line indicates the probability of selecting a sample from a uniform distribution ($1/128$).
    The red vertical dashed line separates valid samples (fulfilling the inequality constraint) on the left from invalid samples on the right. The cost of valid samples looks like a flat line due to the scaling of the penalty, so the inset provides insight into the actual difference.}
    \label{fig:prob_dist_sym_4}
\end{figure*}

\subsection{Hyper-parameter study for TN and STN-GEO} 

In this section, we demonstrate the results of the exploration of the hyper-parameter space shown in section \ref{sec:experiment_design}. We run our implementation of the TN- and STN-GEO algorithm for the 60 knapsack problem instances created according to Sec.~\ref{sec:experiment_design}. We repeat every experiment 10 times per hyperparameter configuration and show the results of the \emph{best} performing choices in table \ref{tab:best_parameters_all} for binary and integer types of encoding. We defined as best performing the set of experiments which produce the highest ratio of found to optimal solutions within the 10 repetitions.
% In addition to the hyper-parameters, we show in both tables the maximum number of cost function evaluations that GEO needed to converge to a valid solution. The column $n_f$ displays the maximum number of function evaluations that was used when repeating the experiment with the optimization parameters from Table \ref{table:hyper_parameters} per scenario.

\begin{table*}[!h]
    \centering
    \begin{tabular}{|c|c||c|c|c|c|c|c|c||c|c|c|c|c|c|c|c|c|}
        \hline
        \multicolumn{2}{|c||}{Problem} & \multicolumn{7}{c||}{Integer encoding} & \multicolumn{7}{c|}{Binary encoding} \\
        \hline
        $M$ & $N$ & Selection & $\chi$ & $N_e$ &$\alpha$ & $\beta$ & $V$ &  $R$ & Selection & $\chi$ & $N_e$ &$\alpha$ & $\beta$ & $V$ &  $R$\\
        \hline \hline
6 & 6 & all & 4 & 3 & 0.0001 & 0.01 & 1.000 & 1.000 &best & 4 & 1 & 0.0001 & 0.01 & 1.000 & 1.000 \\ \hline
6 & 7 & best & 4 & 1 & 0.001 & 0.001 & 1.000 & 0.998 &all & 4 & 1 & 0.0001 & 0.1 & 1.000 & 1.000 \\ \hline
5 & 8 & best & 4 & 10 & 0.0001 & 0.1 & 1.000 & 1.000 &all & 4 & 1 & 0.0001 & 0.1 & 1.000 & 1.000 \\ \hline
2 & 19 & best & 4 & 5 & 0.0001 & 0.1 & 1.000 & 0.998 &all & 4 & 3 & 0.0001 & 0.1 & 1.000 & 1.000 \\ \hline
5 & 10 & best & 4 & 10 & 0.0001 & 0.01 & 0.700 & 0.962 &all & 4 & 1 & 0.0001 & 0.1 & 0.900 & 0.959 \\ \hline
5 & 11 & best & 4 & 1 & 0.0001 & 0.1 & 1.000 & 0.958 &all & 4 & 3 & 0.0001 & 0.001 & 0.600 & 0.959 \\ \hline
5 & 12 & best & 4 & 5 & 0.0001 & 0.001 & 1.000 & 0.950 &best & 32 & 10 & 0.001 & 0.1 & 0.100 & 0.964 \\ \hline
6 & 11 & best & 4 & 5 & 0.0001 & 0.1 & 1.000 & 0.957 &all & 4 & 5 & 0.0001 & 0.1 & 1.000 & 0.976 \\ \hline
6 & 12 & best & 4 & 3 & 0.0001 & 0.1 & 1.000 & 0.943 &best & 4 & 5 & 0.001 & 0.1 & 0.200 & 0.930 \\ \hline
5 & 14 & best & 4 & 5 & 0.0001 & 0.1 & 1.000 & 0.940 &all & 4 & 1 & 0.001 & 0.001 & 1.000 & 0.927 \\ \hline
6 & 13 & all & 32 & 5 & 0.0001 & 0.1 & 0.100 & 0.920 &best & 4 & 3 & 0.001 & 0.1 & 0.100 & 0.915 \\ \hline
5 & 15 & best & 4 & 5 & 0.0001 & 0.1 & 0.900 & 0.929 &all & 4 & 1 & 0.001 & 0.001 & 1.000 & 0.910 \\ \hline
5 & 16 & all & 32 & 10 & 0.0001 & 0.01 & 0.100 & 0.941 &best & 16 & 1 & 0.001 & 0.1 & 0.100 & 0.905 \\ \hline
5 & 17 & all & 4 & 10 & 0.0001 & 0.1 & 0.500 & 0.948 &all & 4 & 1 & 0.001 & 0.01 & 0.500 & 0.900 \\ \hline
5 & 18 & all & 16 & 3 & 0.001 & 0.1 & 0.100 & 0.909 &all & 4 & 1 & 0.001 & 0.1 & 0.100 & 0.929 \\ \hline
6 & 17 & best & 4 & 3 & 0.0001 & 0.1 & 0.100 & 0.909 &best & 4 & 1 & 0.001 & 0.01 & 0.200 & 0.839 \\ \hline
5 & 19 & best & 4 & 1 & 0.0001 & 0.1 & 1.000 & 0.892 &best & 4 & 1 & 0.001 & 0.1 & 0.800 & 0.880 \\ \hline
5 & 20 & all & 8 & 3 & 0.0001 & 0.01 & 0.100 & 0.903 &all & 32 & 3 & 0.001 & 0.01 & 0.100 & 0.884 \\ \hline
5 & 21 & all & 4 & 10 & 0.0001 & 0.001 & 0.300 & 0.873 &all & 32 & 3 & 0.001 & 0.1 & 0.100 & 0.901 \\ \hline
5 & 22 & all & 8 & 1 & 0.0001 & 0.01 & 0.100 & 0.884 &all & 4 & 1 & 0.001 & 0.01 & 0.400 & 0.875 \\ \hline
5 & 23 & best & 4 & 5 & 0.0001 & 0.01 & 0.400 & 0.883 &all & 8 & 5 & 0.001 & 0.1 & 0.100 & 0.886 \\ \hline
5 & 24 & all & 8 & 3 & 0.0001 & 0.01 & 0.100 & 0.936 &best & 4 & 1 & 0.001 & 0.1 & 0.200 & 0.906 \\ \hline
5 & 25 & best & 4 & 1 & 0.001 & 0.1 & 0.900 & 0.868 &all & 4 & 3 & 0.001 & 0.1 & 0.200 & 0.866 \\ \hline
6 & 23 & all & 4 & 10 & 0.001 & 0.01 & 0.100 & 0.893 &all & 8 & 3 & 0.001 & 0.1 & 0.900 & 0.865 \\ \hline
5 & 26 & all & 8 & 5 & 0.001 & 0.01 & 0.100 & 0.879 &best & 16 & 3 & 0.001 & 0.1 & 0.400 & 0.867 \\ \hline
6 & 24 & best & 4 & 3 & 0.0001 & 0.001 & 0.100 & 0.900 &best & 8 & 5 & 0.001 & 0.1 & 0.100 & 0.881 \\ \hline
5 & 27 & best & 8 & 10 & 0.0001 & 0.001 & 0.100 & 0.879 &all & 16 & 10 & 0.001 & 0.1 & 0.800 & 0.898 \\ \hline
6 & 25 & all & 4 & 10 & 0.001 & 0.01 & 0.100 & 0.882 &best & 16 & 10 & 0.001 & 0.1 & 0.300 & 0.892 \\ \hline
5 & 28 & all & 8 & 1 & 0.0001 & 0.1 & 0.200 & 0.892 &all & 8 & 10 & 0.001 & 0.1 & 0.100 & 0.901 \\ \hline
6 & 28 & best & 32 & 1 & 0.0001 & 0.001 & 0.100 & 0.887 &all & 4 & 1 & 0.001 & 0.1 & 0.100 & 0.844 \\ \hline
5 & 34 & best & 4 & 5 & 0.001 & 0.01 & 0.100 & 0.878 &all & 16 & 1 & 0.001 & 0.1 & 0.300 & 0.859 \\ \hline
6 & 31 & all & 32 & 5 & 0.001 & 0.01 & 0.100 & 0.870 &all & 16 & 10 & 0.001 & 0.1 & 0.700 & 0.857 \\ \hline
6 & 32 & best & 4 & 3 & 0.001 & 0.01 & 0.100 & 0.888 &all & 16 & 1 & 0.001 & 0.1 & 0.600 & 0.863 \\ \hline
10 & 25 & best & 8 & 1 & 0.001 & 0.1 & 0.100 & 0.893 &all & 16 & 1 & 0.001 & 0.1 & 0.700 & 0.877 \\ \hline
6 & 33 & all & 8 & 5 & 0.0001 & 0.001 & 0.100 & 0.893 &all & 4 & 5 & 0.001 & 0.1 & 0.100 & 0.869 \\ \hline
6 & 34 & all & 4 & 5 & 0.001 & 0.01 & 0.100 & 0.881 &best & 4 & 1 & 0.001 & 0.1 & 1.000 & 0.861 \\ \hline
5 & 38 & all & 16 & 10 & 0.001 & 0.1 & 0.100 & 0.862 &all & 16 & 3 & 0.001 & 0.1 & 0.800 & 0.865 \\ \hline
5 & 40 & all & 8 & 3 & 0.001 & 0.1 & 0.200 & 0.873 &all & 16 & 10 & 0.001 & 0.1 & 0.200 & 0.853 \\ \hline
10 & 28 & best & 4 & 3 & 0.001 & 0.01 & 0.100 & 0.863 &all & 4 & 5 & 0.001 & 0.01 & 0.100 & 0.877 \\ \hline
5 & 41 & best & 32 & 3 & 0.001 & 0.1 & 0.100 & 0.872 &all & 16 & 1 & 0.001 & 0.1 & 0.700 & 0.850 \\ \hline
6 & 37 & best & 8 & 10 & 0.0001 & 0.001 & 0.100 & 0.881 &best & 8 & 5 & 0.001 & 0.1 & 0.100 & 0.893 \\ \hline
5 & 42 & all & 32 & 1 & 0.001 & 0.01 & 0.100 & 0.906 &best & 4 & 1 & 0.0001 & 0.01 & 0.100 & 0.874 \\ \hline
5 & 43 & all & 32 & 3 & 0.0001 & 0.1 & 0.100 & 0.871 &all & 8 & 5 & 0.0001 & 0.1 & 0.200 & 0.868 \\ \hline
6 & 39 & all & 4 & 10 & 0.0001 & 0.01 & 0.100 & 0.877 &all & 16 & 1 & 0.001 & 0.1 & 0.300 & 0.879 \\ \hline
5 & 44 & all & 4 & 3 & 0.001 & 0.1 & 0.100 & 0.871 &best & 8 & 10 & 0.001 & 0.1 & 0.100 & 0.888 \\ \hline
5 & 45 & best & 16 & 10 & 0.001 & 0.001 & 0.100 & 0.879 &all & 16 & 10 & 0.001 & 0.1 & 1.000 & 0.879 \\ \hline
5 & 46 & best & 4 & 10 & 0.0001 & 0.1 & 0.200 & 0.885 &all & 16 & 10 & 0.001 & 0.1 & 0.700 & 0.871 \\ \hline
5 & 47 & all & 4 & 10 & 0.001 & 0.01 & 0.100 & 0.867 &best & 16 & 3 & 0.001 & 0.1 & 0.900 & 0.883 \\ \hline
10 & 34 & all & 4 & 3 & 0.0001 & 0.01 & 0.100 & 0.848 &all & 16 & 1 & 0.001 & 0.1 & 0.100 & 0.861 \\ \hline
6 & 46 & best & 4 & 5 & 0.001 & 0.1 & 0.100 & 0.862 &all & 8 & 5 & 0.001 & 0.1 & 0.200 & 0.862 \\ \hline
6 & 47 & best & 4 & 3 & 0.001 & 0.1 & 0.200 & 0.870 &best & 4 & 5 & 0.001 & 0.1 & 0.200 & 0.864 \\ \hline
6 & 48 & best & 4 & 5 & 0.0001 & 0.01 & 0.100 & 0.866 &all & 16 & 3 & 0.001 & 0.1 & 0.500 & 0.850 \\ \hline
6 & 50 & all & 8 & 5 & 0.0001 & 0.001 & 0.200 & 0.872 &best & 16 & 3 & 0.001 & 0.1 & 1.000 & 0.862 \\ \hline
10 & 39 & all & 4 & 3 & 0.0001 & 0.01 & 0.100 & 0.778 &all & 8 & 1 & 0.001 & 0.1 & 0.100 & 0.868 \\ \hline
6 & 51 & all & 4 & 5 & 0.0001 & 0.1 & 0.100 & 0.865 &all & 4 & 10 & 0.001 & 0.1 & 0.100 & 0.846 \\ \hline
6 & 52 & best & 4 & 5 & 0.0001 & 0.1 & 0.100 & 0.871 &best & 16 & 3 & 0.001 & 0.1 & 0.500 & 0.860 \\ \hline
6 & 53 & all & 4 & 1 & 0.001 & 0.01 & 0.100 & 0.862 &all & 8 & 1 & 0.001 & 0.01 & 0.100 & 0.869 \\ \hline
7 & 50 & best & 4 & 10 & 0.0001 & 0.01 & 0.100 & 0.848 &all & 8 & 5 & 0.001 & 0.1 & 0.100 & 0.864 \\ \hline
7 & 57 & best & 4 & 3 & 0.001 & 0.1 & 0.100 & 0.839 &all & 16 & 1 & 0.001 & 0.1 & 0.833 & 0.836 \\ \hline
7 & 58 & all & 4 & 3 & 0.0001 & 0.1 & 0.100 & 0.862 &best & 4 & 1 & 0.0001 & 0.001 & 0.100 & 0.836 \\ \hline        
\end{tabular}
    \caption{Best found hyper-parameter configurations for integer and binary encoding.}
    \label{tab:best_parameters_all}
\end{table*}

From this table, we observe that $\chi=4$ and $N_e=1$ or $3$ often lead to the best results. This pattern suggests that larger $\chi$ and $N_e$ lead to overfitting, where the model becomes too specialized to the training data, degrading its generalization. These trends are further supported by Figure~\ref{fig:heatmaps}, which illustrates how larger $\chi$ and $N_e$ negatively affect performance metrics. As shown in the heatmaps, larger truncation size $\chi$ and larger number of epochs $N_e$ lead to worse results in binary and integer encodings, both for the average proportion of valid solutions and for the ratio to the optimal cost. These observations can be attributed to overfitting: an increase in $N_e$ causes the model to train excessively on a fixed dataset before it is updated, while a larger MPS truncation size $\chi$ is also known to promote overfitting by increasing model capacity and complexity.

\begin{figure*}[ht]
   \centering
   \includegraphics[width=0.8\textwidth]{figures/heatmaps.pdf}
    \caption{Heatmaps displaying optimization performance across various parameter settings for integer and binary encoding schemes. The first row represents results for integer encoding, while the second row shows binary encoding. The first column indicates the average proportion of valid solutions found, averaged over all scenarios, for each combination of truncation size ($\chi$) and number of epochs ($N_e$). The second column illustrates the averaged ratio of these valid solutions to the known optimal solution cost.}
    \label{fig:heatmaps}
\end{figure*}

Finally, we have observed worse performance on small problem instances for larger values of $\alpha$ (e.g. $0.1$), which aligns with the intuition of gradient-based optimization. Therefore, we only performed the experiments on all problem instances for smaller values of $\alpha$, as discussed in Table~\ref{table:hyper_parameters}, and we did not observe any significant difference between the values of our choice. 

\subsection{Performance comparison of TN and STN-GEO random sampling and simulated annealing}
In this section, we compare the performance of TN- STN-GEO solver with respect to random sampling and simulated annealing. As stated in section \ref{sec:experiment_design}, we use the same number of function evaluations that were used in the GEO experiments. We present the results in Fig. \ref{fig:scenario_comparisons_full} in which we plot the performance of the optimization methods as a function of the search space size, via 2 metrics: the ratio of found valid solutions at convergence for 10 repetitions of the experiment, and the average ratio (over all convergent experiments) of the best found solution to the best solution found by the gurobi solver, as explained in section \ref{sec:experiment_design}.

We perform 10 runs of optimization for each set of the parameters from Table \ref{table:hyper_parameters}, and display the best result of each implementation in Fig. \ref{fig:scenario_comparisons_full}. Our comparative analysis of optimization methods reveals that both binary and integer encoding implementations are highly effective for smaller problem sizes, consistently yielding valid and near-optimal solutions. 

However, for larger problem instances, we observe a diminished average ratio to the optimal solution, on par with simulated annealing. We also see that the ability to generate valid solutions tends to decrease as the problem size increases.  Simulated annealing demonstrates a more consistent performance across the tested problem sizes and metrics, effectively finding valid and near-optimal solutions for most of the tested scenarios. Random search, included as a baseline, performs adequately on small problems but fails to find valid solutions as the problem size grows. For many of the randomly generated problem instances, we see that even random search can produce good results. Those are ``easy" instances of the knapsack problem which probably have a large space of valid solutions. However, note that there are problem instances, for which finding good solutions at random is very unlikely, even in moderately small sizes. These problems may be ``hard" instances of the multi knapsack problem, whose valid solution spaces are very small. For those instances, we see that GEO performs similarly to SA showing highly non-trivial performance. In Fig.~\ref{fig:hard_problems}, we plot the metric $R$ once more, focusing exclusively on the ``hard'' problem instances to enable a clearer comparison of the results.

\begin{figure*}[ht]
    \centering
    \includegraphics[width=1.\textwidth]{figures/comparison.pdf}
    \caption{Each method is run 10 times for every problem size (Integer and Binary also use different hyper-parameters from Table~\ref{table:hyper_parameters}),
selecting the configuration that maximizes the number of valid solutions at convergence (enforced via a penalty coefficient).
If multiple configurations yield the same number of valid solutions, we choose the one with the highest ratio to the optimal cost. Top plot: The average ratio of the best valid solution's cost to the known optimal, with error bars. Middle plot: The proportion of valid solutions (fulfilling both inequality and equality constraints) among all runs for each method. Bottom plot: The ``exploration ratio,'' computed as the maximum number of objective-function evaluations (across all parameter configurations)
divided by the problem size.
In all plots, the $x$-axis indicates the number of samples satisfying the cardinality constraint for each problem.}
    \label{fig:scenario_comparisons_full}
\end{figure*}

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.5\textwidth]{figures/mean_ratio_zero_keys.pdf}
    \caption{Subset of tested problem instances in which random sampling was unable to find a single valid solution. We see more clearly that both GEO implementations perform comparably to simulated annealing. Notably, for one problem instance, $N=39$ objects, $M=10$ knapsacks, we see that the binary implementation of GEO was able to find a good solution while the integer GEO does slightly worse and simulated annealing did not find a solution at all. We believe this to be an outlier behavior for simulated annealing}
    \label{fig:hard_problems}
\end{figure}

\subsection{Limitations and implementation recommendations of STN-GEO}\label{sec:limitations}

In this section we present the possible problems that may arise when using DMRG-inspired optimization (Sec. \ref{sec:dmrg}) of symmetric MPS for the generalized multi-knapsack problem (Sec. \ref{sec:knapsack}).

One limitation of symmetric MPS in our method stems from numerical errors introduced during the use of QR decomposition and SVD. These errors can accumulate over iterations, leading to the breaking of MPS symmetry. When symmetry is compromised, the probability of sampling values that violate cardinality constraints increases with each DMRG iteration. Moreover, symmetry breaking can also result from infeasible samples in the training dataset, which in turn introduce non-symmetric gradients (see Appendix \ref{appendix:gradient_symmetry}).

To address these issues, it is essential to perform SVD in a block-wise manner, \cite{PhysRevB.83.115125}, and to discard non-symmetric samples during the filtering step of the GEO iteration. Despite the added complexity in implementation to preserve symmetry, initializing the MPS symmetrically offers significant benefits for optimization convergence. This approach reduces the possible sample space from $2^{MN}$ to $M^N$, when compared to a randomly initialized MPS of the same size, thereby enhancing computational efficiency and improving convergence.

Additionally, while reducing bond dimension truncation in the MPS minimizes approximation errors, we observed that increasing the bond dimension often leads to overfitting, as demonstrated in the results of the previous section. This overfitting diminishes the model's generalization capabilities, indicating the necessity of balancing approximation accuracy with model complexity to achieve optimal performance.

 Finally, we recommend sorting the objects of the multi-knapsack problem by their weights ($w_{1}\geq w_2\geq...\geq w_{N}$) and the knapsacks by their capacities ($m_{1}\geq m_2\geq...\geq m_{M}$) before initiating the optimization procedure. The intuition is that arranging the MPS elements corresponding to knapsacks with the highest capacities next to each other helps to better capture correlations among them; the same principle applies to the objects.