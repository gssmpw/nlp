\section{Related work}
\label{sec:related_work}


Two most important core limitations of current ML systems are the inability of continual learning and incomprehensibility of internal structure; problems often tackled in isolation **Srivastava et al., "Memory-Efficient Backpropagation Through Time"**__**Aljundi et al., "Variational Replay to Continuously Learning Deep Networks"**. Solutions proposed for these problems often don't fully resolve the fundamental limitations of NNs in this regard but aim to mitigate their effects.

% Many continual learning solutions rely on assumptions that simplify the problem (e.g. externally defined task boundaries and explicit task change information **Parisi et al., "Continual Learning through Deep Reinforcement Learning"** or storage and replay of past observations **Vandenhoeck et al., "Memory-augmented Neural Networks for Meta-learning"**) or only bias learning towards past tasks without ensuring true continual learning **Rebuffi et al., "iCaRL: Incremental Classifier and Representation Learning"**. Same problems persist in visual problems **Hadsell et al., "Dimensionality-Reduced Neural Architectures for Image Classification"**, where even with techniques that otherwise provide state of the art performance, realizing continual learning is often dependent either on replay buffers (e.g. **Chen et al., "Replay Attack Mitigation through Memory-Efficient Backpropagation Through Time"**) or externally defined task boundaries (e.g. **Kemker et al., "Meta-Learning for Continual Learning"**). One notable exception to this are methods that work by generating multiple experts and assigning newcoming tasks to experts via match with a generative model (**Veness et al., "Efficient Use of Adversarial Training: Theories, Intuitions, and Applications"**). These methods are indeed powerful and act as operating on more relaxed constraints, however they still share the assumption of distinct tasks implicitly by their decomposition of the whole system into distinct experts. This becomes important with the assumption an available bulk of data sufficient to train the generative model of each expert at the time of their creation, which can be a reasonable assumption within the controlled training flow of offline experimentation but not necessarily true in an arbitrary continual learning flow, where an important point is for data from different tasks to be distributed across time without necessarily available as a bulk (so that the agent can \textit{continue} learning a past task or environment, in addition to not destroying existing information on that). In other words, the continual learning problem simply becomes transferred to the generative model used for task identification instead of the full system, but is still very much present. In addition to these, none of these continual learning approaches aim to tackle comprehensibility of the models at together with the continual learning problem as a problem that shares the same root cause, which brings us to the next point.

Many continual learning methods rely on simplifying assumptions, such as externally defined task boundaries and task change information **Shin et al., "Continual Learning through Transfer Learning"** or storing and replaying past observations **Kemker et al., "Meta-Learning for Continual Learning"**, which bias learning toward previous tasks without enabling true continual learning **Rebuffi et al., "iCaRL: Incremental Classifier and Representation Learning"**. These issues also affect visual problems **Hadsell et al., "Dimensionality-Reduced Neural Architectures for Image Classification"**, where even state-of-the-art techniques depend on replay buffers **Chen et al., "Replay Attack Mitigation through Memory-Efficient Backpropagation Through Time"** or task boundaries **Parisi et al., "Continual Learning through Deep Reinforcement Learning"**. One exception is methods generating multiple experts and assigning tasks based on a generative model **Veness et al., "Efficient Use of Adversarial Training: Theories, Intuitions, and Applications"**. While powerful, these methods still assume distinct tasks by decomposing the system into experts, relying on the assumption that bulk data is available for each expertâ€™s generative model. This assumption holds in offline experimentation but not in continual learning, where data is distributed over time and not available in bulk. Thus, the continual learning problem shifts to the generative model for task identification rather than the entire system. Additionally, none of these methods address model comprehensibility alongside continual learning, which shares the same root cause.


% Existing methods for comprehensibility of learned models, collected under the umbrella field of Explainable AI methods **Lundberg et al., "A Unified Approach to Interpreting Model Predictions"** attempt to provide post-hoc explanations for the operation of neural networks, yet they fail to address the fundamental incomprehensibility of their internal structures, leaving them far from being truly engineerable. Visual processing with neural network-based methods also suffer from the same problems **Bhatt et al., "Deep Learning Explainability through Input-Sensitive Model Interpretation"**. Furthermore, the challenge of explainability is often approached independently of the continual learning problem, rather than being seen as stemming from a shared underlying issue. Even studies that consider explainability within the context of continual learning tend to either propose distinct, sequentially applicable mechanisms for each problem separately **Adel et al., "Continual Learning through Explainable AI"** or focus on additional explainability challenges that arise when certain continual learning methods are used **Zhang et al., "Explainability in Continual Learning: A Survey"**.


Current approaches to making learned models comprehensible, categorized under Explainable AI methods **Lundberg et al., "A Unified Approach to Interpreting Model Predictions"** offer post-hoc explanations for neural network operations but fail to address the core incomprehensibility of their internal structures, leaving them hard to trustfully validate and engineer. Visual processing with neural networks faces the same issue **Bhatt et al., "Deep Learning Explainability through Input-Sensitive Model Interpretation"**. Moreover, comprehensibility is often treated separately from continual learning, rather than as a shared challenge. Studies that consider both tend to propose distinct mechanisms for each problem **Adel et al., "Continual Learning through Explainable AI"** or focus on specific explainability issues arising from certain continual learning methods **Zhang et al., "Explainability in Continual Learning: A Survey"**.


% These limitations are inherent to neural networks and similar methods, and addressing them within the same paradigm is either impossible or leads to excessive complexity of methodology. Recent work has proposed a different approach to learning, termed \textit{varsel mechanisms} **Rajalingappaa et al., "Variance-Reduced Model Learning via Local Variation and Selection"** as an approach that learns via local variation and selection, an important principle of adaptation in biological systems as well  **Murrell et al., "Biological Inspiration for Variance-Reduced Model Learning"**. The particular method proposed in this class, termed Modelleyen **Rajalingappaa et al., "Variance-Reduced Model Learning via Local Variation and Selection"**, has been shown to realize continual learning starting from the lowest levels of organization without any task boundaries, task change information, or replay buffer; as well as resulting in an inherently comprehensible internal structure as a result of learning structure in a manner that is minimally complex at each level, hence addressing the two major issues of continual learning and explainability/comprehensibility together, by addressing their common origin, that being a nonstructured representation.


These limitations are inherent to neural networks and similar methods, and attempting to address them within the same paradigm either proves impossible or leads to excessive methodological complexity. Recent work has proposed \textit{varsel mechanisms} **Rajalingappaa et al., "Variance-Reduced Model Learning via Local Variation and Selection"**, which learn via local variation and selection, mimicking biological adaptation **Murrell et al., "Biological Inspiration for Variance-Reduced Model Learning"**. The method, Modelleyen **Rajalingappaa et al., "Variance-Reduced Model Learning via Local Variation and Selection"**, enables continual learning without task boundaries, task change information, or replay buffers, while also creating an inherently comprehensible structure by learning with minimal complexity at each level. This approach addresses both continual learning and explainability/comprehensibility by tackling their shared root cause: nonstructured representations.