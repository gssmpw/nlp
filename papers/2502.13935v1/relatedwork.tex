\section{Related work}
\label{sec:related_work}


Two most important core limitations of current ML systems are the inability of continual learning and incomprehensibility of internal structure; problems often tackled in isolation \cite{kirkpatrick2017overcoming,rusu2016progressive,jacobson2022task,hadsell2020embracing,zhuang2020comprehensive,xu2019explainable}. Solutions proposed for these problems often don't fully resolve the fundamental limitations of NNs in this regard but aim to mitigate their effects.

% Many continual learning solutions rely on assumptions that simplify the problem (e.g. externally defined task boundaries and explicit task change information \cite{rusu2016progressive,jacobson2022task} or storage and replay of past observations \cite{buzzega2020dark}) or only bias learning towards past tasks without ensuring true continual learning \cite{kirkpatrick2017overcoming}. Same problems persist in visual problems \cite{qu2021recent}, where even with techniques that otherwise provide state of the art performance, realizing continual learning is often dependent either on replay buffers (e.g. \cite{galashov2023continually}) or externally defined task boundaries (e.g. \cite{wang2022continual}). One notable exception to this are methods that work by generating multiple experts and assigning newcoming tasks to experts via match with a generative model (\cite{erden2024directed, lee2020neural}). These methods are indeed powerful and act as operating on more relaxed constraints, however they still share the assumption of distinct tasks implicitly by their decomposition of the whole system into distinct experts. This becomes important with the assumption an available bulk of data sufficient to train the generative model of each expert at the time of their creation, which can be a reasonable assumption within the controlled training flow of offline experimentation but not necessarily true in an arbitrary continual learning flow, where an important point is for data from different tasks to be distributed across time without necessarily available as a bulk (so that the agent can \textit{continue} learning a past task or environment, in addition to not destroying existing information on that). In other words, the continual learning problem simply becomes transferred to the generative model used for task identification instead of the full system, but is still very much present. In addition to these, none of these continual learning approaches aim to tackle comprehensibility of the models at together with the continual learning problem as a problem that shares the same root cause, which brings us to the next point.

Many continual learning methods rely on simplifying assumptions, such as externally defined task boundaries and task change information \cite{rusu2016progressive, jacobson2022task}, or storing and replaying past observations \cite{buzzega2020dark}, which bias learning toward previous tasks without enabling true continual learning \cite{kirkpatrick2017overcoming}. These issues also affect visual problems \cite{qu2021recent}, where even state-of-the-art techniques depend on replay buffers \cite{galashov2023continually} or task boundaries \cite{wang2022continual}. One exception is methods generating multiple experts and assigning tasks based on a generative model \cite{erden2024directed, lee2020neural}. While powerful, these methods still assume distinct tasks by decomposing the system into experts, relying on the assumption that bulk data is available for each expertâ€™s generative model. This assumption holds in offline experimentation but not in continual learning, where data is distributed over time and not available in bulk. Thus, the continual learning problem shifts to the generative model for task identification rather than the entire system. Additionally, none of these methods address model comprehensibility alongside continual learning, which shares the same root cause.






% Existing methods for comprehensibility of learned models, collected under the umbrella field of Explainable AI methods \cite{xu2019explainable, kashefi2023explainability} attempt to provide post-hoc explanations for the operation of neural networks, yet they fail to address the fundamental incomprehensibility of their internal structures, leaving them far from being truly engineerable. Visual processing with neural network-based methods also suffer from the same problems \cite{kashefi2023explainability}. Furthermore, the challenge of explainability is often approached independently of the continual learning problem, rather than being seen as stemming from a shared underlying issue. Even studies that consider explainability within the context of continual learning tend to either propose distinct, sequentially applicable mechanisms for each problem separately \cite{roy2022interpretable} or focus on additional explainability challenges that arise when certain continual learning methods are used \cite{rymarczyk2023icicle, cossu2024drifting}.

Current approaches to making learned models comprehensible, categorized under Explainable AI methods \cite{xu2019explainable, kashefi2023explainability} offer post-hoc explanations for neural network operations but fail to address the core incomprehensibility of their internal structures, leaving them hard to trustfully validate and engineer. Visual processing with neural networks faces the same issue \cite{kashefi2023explainability}. Moreover, comprehensibility is often treated separately from continual learning, rather than as a shared challenge. Studies that consider both tend to propose distinct mechanisms for each problem \cite{roy2022interpretable} or focus on specific explainability issues arising from certain continual learning methods \cite{rymarczyk2023icicle, cossu2024drifting}.



% These limitations are inherent to neural networks and similar methods, and addressing them within the same paradigm is either impossible or leads to excessive complexity of methodology. Recent work has proposed a different approach to learning, termed \textit{varsel mechanisms} \cite{erden2025agential, erden2025agential_extendedabs}, as an approach that learns via local variation and selection, an important principle of adaptation in biological systems as well  \cite{marc2005plausibility, gerhart2007theory}. The particular method proposed in this class, termed Modelleyen \cite{Erden2024Modelleyen, erden2025agential, erden2025agential_extendedabs}, has been shown to realize continual learning starting from the lowest levels of organization without any task boundaries, task change information, or replay buffer; as well as resulting in an inherently comprehensible internal structure as a result of learning structure in a manner that is minimally complex at each level, hence addressing the two major issues of continual learning and explainability/comprehensibility together, by addressing their common origin, that being a nonstructured representation.


These limitations are inherent to neural networks and similar methods, and attempting to address them within the same paradigm either proves impossible or leads to excessive methodological complexity. Recent work has proposed \textit{varsel mechanisms} \cite{erden2025agential, erden2025agential_extendedabs}, which learn via local variation and selection, mimicking biological adaptation \cite{marc2005plausibility, gerhart2007theory}. The method, Modelleyen \cite{Erden2024Modelleyen, erden2025agential, erden2025agential_extendedabs}, enables continual learning without task boundaries, task change information, or replay buffers, while also creating an inherently comprehensible structure by learning with minimal complexity at each level. This approach addresses both continual learning and explainability/comprehensibility by tackling their shared root cause: nonstructured representations.