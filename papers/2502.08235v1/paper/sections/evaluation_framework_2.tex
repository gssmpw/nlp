
We analyze LRMs performance in agentic environments using SWE-bench Verified \cite{swebench_verified}, comparing reasoning models with their non-reasoning counterparts. Our study aims to answer the following research questions:
\begin{itemize}
    \item RQ1: Does overthinking affect agentic performance?
    \item RQ2: How does it impact different models?
    \item RQ3: Can we mitigate overthinking?
\end{itemize}

\subsection{Experimental setup}


\paragraph{OpenHands} To demonstrate how AI agents operate, we use the OpenHands framework \cite{wang2024openhandsopenplatformai}, which implements a complete agent-environment interaction cycle as illustrated in \cref{fig:figure3}. Through this framework, agents receive a set of tools to interact with their environment, along with examples of the proper usage of tools \cite{wang2024executablecodeactionselicit}. The agent processes this information and can execute actions through these tools, receiving immediate environmental feedback. This feedback is then incorporated into the agent's context, enabling \textbf{in-context learning} \cite{dong2024surveyincontextlearning} and \textbf{self-refinement} \cite{madaan2023selfrefineiterativerefinementselffeedback} through successive interactions. The framework supports both native function-calling capabilities \cite{openai_function_calling} and structured text output, adapting to different model architectures while maintaining a consistent interaction protocol. In this work, we leverage OpenHands' comprehensive instrumentation capabilities to systematically analyze how models balance the \textbf{\textit{Reasoning-Action Dilemma}}, revealing previously unexamined patterns in their interaction behavior.

\paragraph{SWE-Bench} Software engineering tasks present an ideal environment for studying agent behavior, as they require both sophisticated reasoning and continuous interaction with the environment \cite{jimenez2024swebenchlanguagemodelsresolve}. SWE-Bench captures this complexity by presenting agents with real-world software issues that demand multiple steps to resolve: agents must understand the problem, explore the codebase, reason about potential solutions, and validate their changes through testing \cite{yang2024sweagentagentcomputerinterfacesenable}. This multi-step nature creates a natural tension between reasoning and action, ideal for testing how models balance the \textbf{\textit{Reasoning-Action Dilemma}}. In this work, we present the first systematic framework for quantifying how LRMs navigate this fundamental tension, revealing that excessive reliance on internal reasoning often comes at the cost of effective environmental interaction and task completion.


\paragraph{Models Evaluated} To comprehensively study the phenomenon and influence of overthinking, we consider 19 models across multiple dimensions, including reasoning capabilities, model openness (proprietary vs. open-weight), model size, and function calling support. We evaluate both reasoning-optimized models as well as general-purpose language models. Our evaluation spans proprietary models (e.g., OpenAI o1, Claude Sonnet 3.5)~\cite{openai_learning_to_reason_2024,anthropic_claude_3_5} and open-weight alternatives (e.g., DeepSeek-R1, Qwen2.5) \cite{qwen2, qwen2.5, deepseekai2025deepseekr1incentivizingreasoningcapability} to ensure broad coverage. We also analyze models of varying scales, ranging from small (1.5B-14B) to large-scale models (32B-671B parameters) \cite{deepseek_reasoning_model}, to investigate whether model size influences overthinking tendencies. Additionally, we distinguish between models that natively support function calling (e.g., OpenAI o1, GPT-4o) \cite{openai_function_calling, openai_gpt4o_2024, openai_gpt4o_mini, openai_o1} and those that do not, which allows us to assess whether explicit function calling capabilities reduce overthinking compared to models that rely on prompt-based learning of tool usage. Further details on the models studied can be found in the~\autoref{apx:models},~\autoref{tab:model_comparison}.

\paragraph{Scaffoldings} Models are not able to directly execute code or edit files. So, we adopt CodeAct, an open-source single-agent scaffolding built within the OpenHands framework~\cite{openhands, qwq-32b-preview, openai_o1_mini, openai_o1_system_card_2024, deepseekai2025deepseekr1incentivizingreasoningcapability, sky_t1_2025}. Scaffolding provides a structured execution environment, allowing models to interact with SWE-bench in a controlled and consistent manner. We choose the single-agent approach as it maintains a unified reasoning process, ensuring full context retention throughout execution. In contrast, multi-agent scaffolds distribute tasks across multiple specialized agents that share an underlying model but operate with distinct prompts and action spaces~\cite{chen2024coderissueresolvingmultiagent,xia2024agentlessdemystifyingllmbasedsoftware, phan2024hyperagentgeneralistsoftwareengineering, allhands_single_agent_systems} which can introduce structural rigidity and lead to information loss during inter-agent communication~\cite{allhands_single_agent_systems}. Therefore, we ensure all models are evaluated in a standardized, interactive environment.



\paragraph{Overthinking Score Calculation} To ensure reliability and consistency, we employ Claude Sonnet 3.5 as the evaluation model and configure it with a temperature of 0 to enforce deterministic scoring, following the LLM-as-a-judge methodology~\cite{zheng2023judgingllmasajudgemtbenchchatbot}. Claude Sonnet 3.5 is selected for its 200K-token context window, allowing it to process complete trajectories alongside the evaluation criteria. Notably, the evaluator does not have access to the final issue resolution outcome, ensuring that the overthinking assessment remains independent of task success and thereby eliminating potential biases.