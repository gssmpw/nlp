\subsection{The Reasoning–Action Dilemma}
We observe that, in agentic decision-making tasks, LRMs constantly face the \emph{Reasoning–Action Dilemma} where they must navigate a fundamental trade-off between:
\begin{itemize}
    \item \textbf{Direct interaction with the environment}, where the model executes actions and receives feedback.
    \item \textbf{Internal reasoning}, where the model reasons over hypothetical outcomes before committing to an action.
\end{itemize}

Ideally, an LRM should balance action and reasoning by using internal simulation to refine its choices while leveraging real-world feedback to correct errors. For instance, when debugging a failing test case, a well-balanced model would hypothesize potential issues yet still execute the test opportunely to collect concrete failure signals. 

Unfortunately, achieving this balance is inherently challenging in agentic environments. On one hand, direct interaction with the environment is time and space (i.e. in-context memory is limited) consuming. On the other hand, prior research has demonstrated that LRMs exhibit significant vulnerability to knowledge insufficiency, where gaps in understanding can cascade into compounding errors throughout the reasoning process \cite{li2025searcho1agenticsearchenhancedlarge, zhong2024evaluationopenaio1opportunities, LingFLHLMS23, chia2024reasoningpathsoptimizationlearning}. Consequently, excessive simulation without sufficient external information can ultimately lead to failure. The situation is especially difficult for environments with limited interaction opportunities.

We observe that LRMs face a fundamental tension between incorporating environmental feedback and relying on internal reasoning chains, a challenge exacerbated by their prompt sensitivity \cite{openai_learning_to_reason_2024, deepseekai2025deepseekr1incentivizingreasoningcapability}. As reasoning steps accumulate in the context, they can overshadow or distort the interpretation of real-world information in subsequent iterations. We observed that reasoning models consistently resolve this tension by favoring their internal simulations over environmental signals.


\paragraph{Overthinking} To capture this potential failure mode in agentic settings, we define overthinking as the tendency of an LRM to \textit{rely excessively on internal reasoning} while failing to seek or integrate essential external feedback. Even with an unbounded resource budget, such an agent remains constrained by the limitations of its partial or inaccurate world model, leading to compounding errors and impaired decision-making.


\subsection{Manifestations of Overthinking}
\label{sec:manifestations}
Our investigation into impaired decision-making in AI agents draws from a detailed analysis of agent-environment interactions. These interactions are recorded in what we term trajectories. Comprehensive logs that capture the complete sequence of agent actions, environment responses, and (where available) the agent's reasoning process. As outlined in \cref{evaluation_framework}, we systematically analyzed these trajectories to understand patterns of \textbf{overthinking}.

While most trajectories include the agent's explicit reasoning process, those from the o1 family exclude these reasoning tokens \cite{openai_learning_to_reason_2024}. This limitation led us to focus our analysis on observable behaviors, which are the concrete actions agents take in response to environmental challenges.

Through this analysis, we identified three distinct patterns of \textbf{overthinking}: Analysis Paralysis, where agents become stuck in excessive planning; Premature Disengagement, where agents abandon tasks prematurely; and Rogue Actions, where agents seem to "get stressed" and generate multiple actions on the same iteration. These actions are exemplified in \cref{fig:manifestations}.

\paragraph{Analysis Paralysis} LRMs tend to shift their focus from immediate actions to elaborate future planning. They generate increasingly complex action sequences but \textit{struggle to execute} them systematically (\cref{fig:manifestations}a). Rather than addressing immediate errors, they construct intricate plans that often remain unexecuted, leading to a cycle of planning without progress.

\paragraph{Rogue Actions} 
We observe cases where agents deliberately generate chains of interdependent actions in a single step, \textit{without awaiting feedback from the environment} (\cref{fig:manifestations}b).
Despite their prior demonstrated awareness of step-by-step interaction requirements, models proceed to construct elaborate action sequences that presume the success of each preceding step, effectively substituting real environmental feedback with internal simulation.

\paragraph{Premature Disengagement} LRMs sometimes \textit{terminate tasks based solely on their internal simulation of the problem space}, either through direct abandonment or by delegating hypothetical action sequences (\cref{fig:manifestations}c). This illustrates how overreliance on internal reasoning can lead to decisions without environmental validation.


\subsection{Quantifying Overthinking}

\begin{figure}[t]
    \centering
    \includegraphics[width=1\linewidth,trim={0cm 0cm 0cm 0cm},clip]{paper/humaneval_vs_overthinking.pdf}
\caption{Validation of our automated overthinking detection methodology against expert human evaluators. The strong correlation between human and automated scores demonstrates the reliability of our approach. Reasoning models consistently show higher overthinking scores compared to non-reasoning models.}
    \label{fig:human_eval}
\end{figure}

\paragraph{Overthinking Score} To quantify overthinking behavior, we developed a systematic scoring method using an LLM-based evaluator. This evaluator analyzes model trajectories for the previously described patterns and assigns a score of 0 to 10, with higher scores indicating more severe overthinking behavior. Each score includes a detailed justification explaining which patterns were identified and their severity. The complete evaluation prompt and scoring criteria can be found in \cref{apx:prompt_overthinking}. 


To validate our LLM-based evaluator, we conduct an independent assessment where four expert annotators manually scored 20 randomly selected model traces, as shown in \cref{fig:human_eval}.
Using these standardized scores, we conduct a comprehensive statistical analysis to investigate the relationship between overthinking behavior and model performance and how overthinking affects LRMs compared to non-reasoning models. The tools used for the statistical analysis can be found in \cref{stat_framework}.

\paragraph{Overthinking prompt} We craft a prompt to systematically evaluate trajectories to detect overthinking behavior. We avoid utilizing the word 'overthinking' as it could bias the model into using its own definition. Instead, we base the prompt around the manifestations of overthinking defined in \cref{sec:manifestations} and the preference for internal reasoning chains over environmental interaction. 

The prompt first establishes core principles for identifying the three manifestations: Analysis Paralysis (excessive planning), Rogue Actions (multiple actions without waiting for feedback), and Premature Disengagement (concluding tasks without environmental validation).

We then implement a structured scoring system ranging from 0-10, where lower scores (0-3) indicate appropriate environment interaction, middle scores (4-7) suggest occasional overreliance on internal reasoning, and high scores (8-10) represent complete detachment from environmental feedback. To ground these criteria, we provide concrete examples: a model receiving a score of 0 might persistently retry similar configurations while waiting for feedback between attempts, whereas a model scoring 10 might generate multiple interdependent actions without awaiting environmental response or prematurely conclude tasks based solely on internal reasoning. The trajectory intentionally excludes information about whether the fix succeeded or failed, preventing the model from developing biases based on solution outcomes.