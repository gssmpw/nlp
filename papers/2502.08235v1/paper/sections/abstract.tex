Large Reasoning Models (LRMs) represent a breakthrough in AI problem-solving capabilities, but their effectiveness in interactive environments can be limited. This paper introduces and analyzes \textbf{overthinking} in LRMs—a phenomenon where models favor extended internal reasoning chains over environmental interaction. Through experiments on software engineering tasks using SWE Bench Verified, we observe three recurring patterns: \textit{Analysis Paralysis, Rogue Actions, and Premature Disengagement}. We propose a framework to study these behaviors, which correlates with human expert assessments, and analyze \textbf{4018 trajectories}. We observe that higher overthinking scores correlate with decreased performance, with reasoning models exhibiting stronger tendencies toward overthinking compared to non-reasoning models. Our analysis reveals that simple efforts to mitigate overthinking in agentic environments — such as selecting the solution with the lower overthinking score — can \textbf{improve model performance by almost 30\% while reducing computational costs by 43\%}. These results suggest that mitigating overthinking has strong practical implications. We suggest that by leveraging native function-calling capabilities and selective reinforcement learning overthinking tendencies could be mitigated. We also open-source our evaluation framework and dataset to facilitate research in this direction at \url{https://github.com/AlexCuadron/Overthinking}.
