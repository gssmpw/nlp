We generate and evaluate \textbf{3908 trajectories} using our evaluation methodology across all models.
We make publicly available every trajectory alongside their corresponding overthinking score and the reasoning behind this score.

Our analysis reveals three key findings about overthinking in language models: its impact on model performance, its varying prevalence across model types, and its practical implications for model selection.
Illustrated in \cref{fig:figure2}. We observe that overthinking consistently impacts performance across all evaluated models, with reasoning-optimized models showing higher overthinking tendencies than general-purpose ones as illustrated in \cref{fig:figure1}.

\subsection{Overthinking and Issue resolution} 

We observe a strong negative correlation between overthinking and performance on SWE-bench, as illustrated in \cref{fig:figure1}. Both reasoning and non-reasoning models show decreased performance as overthinking increases, though with notably different patterns.

\subsection{Overthinking and Model Type}
We make three key observations with regard to overthinking in reasoning and non-reasoning models. The results are presented in Figure~\ref{fig:figure1}.

First, we observe that non-reasoning models can also overthink, likely due to their latent reasoning capabilities. Recent studies suggest that non-reasoning models also exhibit reasoning abilities \cite{wei2023chainofthoughtpromptingelicitsreasoning, yao2023treethoughtsdeliberateproblem, chen2023program, kojima2023largelanguagemodelszeroshot}. 

Second, reasoning models exhibit significantly higher overthinking scores than non-reasoning models, as shown in \cref{tab:overthinking_comparison}. Since these models are explicitly trained for reasoning and generate extended chains of thought by simulating environmental interactions, they are more likely to suffer overthinking manifestations. 

\begin{table}[ht]
\centering
\begin{tabular}{lcccc}
\toprule
\textbf{Model} & \boldmath{$\beta_1$} & \boldmath{$R^2$} & \textbf{p-value} \\
\midrule
Reasoning      & -7.894 & 0.892 & 0.000 \\
Non-Reasoning  & -15.938 & 0.839 & 0.010 \\
\bottomrule
\end{tabular}
\caption{Regression Results for Reasoning and Non-Reasoning Models}
\label{tab:regression_results}
\end{table}

\begin{table}[ht]
\centering
\begin{tabular}{lc}
\toprule
\textbf{Measure} & \textbf{Value} \\
\midrule
Reasoning Models       & 3.505 $\pm$ 1.774 \\
Non-Reasoning Models   & 2.228 $\pm$ 0.751 \\
\bottomrule
\end{tabular}
\caption{Average Overthinking Scores for Reasoning and Non-Reasoning Models}
\label{tab:overthinking_scores}
\end{table}

Lastly, we also observe that non-reasoning models
that overthink suffer from severe degradation in issue
resolution, as indicated by the beta coefficients in \cref{tab:regression_results}. A lower beta coefficient corresponds to a more significant impact of overthinking on performance. We suspect that since non-reasoning models are not trained for reasoning, they are not capable of handling reasoning chains effectively, thus showing worse results.




\subsection{Overthinking and Model Size}
Our evaluation examines two model families across three size variants (32B, 14B, 7B): the non-reasoning Qwen2.5-Instruct and the reasoning R1-Distill-Qwen \cite{qwen2,qwen2.5,deepseekai2025deepseekr1incentivizingreasoningcapability}.

As illustrated in \cref{figure5}, our analysis suggests a negative correlation between model size and overthinking behavior. We hypothesize that smaller models struggle with environmental comprehension, causing them to rely more heavily on internal reasoning chains and increasing their tendency to \textbf{overthink}.

The relationship between model size and overthinking manifests differently across model types. As shown in \cref{tab:overthinking_comparison}, both reasoning and non-reasoning models show higher overthinking scores as their size decreases, with reasoning models consistently exhibiting greater susceptibility to overthinking. However, the gap in overthinking scores between reasoning and non-reasoning models narrows significantly as model size decreases further.
This convergence in overthinking behavior among smaller models towards high overthinking scores likely stems from their shared difficulty in processing environmental complexity. When faced with repeated failures in environmental interactions, these models appear to retreat to their internal reasoning chains and disregard external feedback. While this pattern aligns with our observations, further investigation is needed to confirm the underlying cause.

\begin{table}[ht]
\centering
\begin{tabular}{lccc}
\bottomrule
\textbf{Measure} & \textbf{Value} \\
\midrule
DS-R1 Family        & 6.700 $\pm$ 1.656 \\
Qwen2.5 Family      & 5.001 $\pm$ 1.732 \\
\bottomrule
\end{tabular}
\caption{Overthinking Score Comparison for R1 and Qwen2.5}
\label{tab:overthinking_comparison}
\end{table}






\begin{figure}[t]
    \centering
    \includegraphics[width=1\linewidth,trim={0cm 0cm 0cm 0cm},clip]{paper/size_matters.pdf}
    \vspace{-25pt}
    \caption{This graph showcases that both families suggest negative correlations between overthinking and model size. With reasoning and non-reasoning models showing close overthinking scores in their 7B and 14B counterparts}
    \label{figure5}
\end{figure}

\subsection{Overthinking and Token Usage}
Prior research has suggested that token usage can serve as an indicator for overthinking \cite{chen2024think23overthinkingo1like}. To investigate this relationship, we analyze the o1 model, manipulating its reasoning effort parameter between high and low settings, which directly influences the number of reasoning tokens used \cite{openai_chat_api}.

Our analysis reveals that o1 models with low reasoning effort demonstrate 35\% higher overthinking scores compared to their high-effort counterparts. As shown in \cref{tab:o1_model_comparison}, the difference in averaged overthinking scores between the two configurations is statistically significant, suggesting that increased token allocation might reduce overthinking in agentic contexts.

This finding challenges the perception that increased reasoning token usage correlates with overthinking as shown by some recent studies \cite{chen2024think23overthinkingo1like}. Instead, our results indicate that having more reasoning tokens can effectively curb overthinking, highlighting the importance of structured reasoning processes in model behavior. 
\begin{table}[ht]
\centering
\begin{tabular}{lc}
\toprule
\textbf{Measure} & \textbf{Value} \\
\midrule
o1 Low        & 2.774 $\pm$ 3.081 \\
o1 High       & 2.426 $\pm$ 2.880 \\
\bottomrule
\end{tabular}
\caption{Overthinking scores comparison between o1 model configurations with low and high reasoning effort settings}
\label{tab:o1_model_comparison}
\end{table}
\subsection{Overthinking and Context Window}
We analyze models across different context window sizes, ranging from 8K to 32K tokens. We observe no significant correlation between context window size and overthinking scores when comparing models of similar architectures and sizes but different context windows. For instance, comparing Qwen2.5-32B (32K context) with QwQ-32B (32K context) shows overthinking scores of 2.31 $\pm$ 0.42 and 2.28 $\pm$ 0.39 respectively (p $>$ 0.05).

We hypothesize that this lack of correlation may be because overthinking behaviors are more influenced by a model's architectural design and training approach rather than its context capacity. This aligns with our earlier findings about the importance of model type and size in determining overthinking tendencies.

\subsection{Practical Implications}

OpenAI showcased that reasoning models exhibit a disproportionate increase in computational costs relative to their performance gains \cite{arcprize2024oai}. Our experiments with SWE-bench Verified dataset confirm this observation: o1 with high reasoning effort achieves a 29.1\% resolution rate at \$1,400, while the low reasoning variant reaches 21.0\% at \$400 — a 3.5$\times$ cost difference for an 8.1 percentage point improvement in performance.

\paragraph{Metrics.} To address this efficiency gap, we computed the \textbf{(1) Pass@k}, which represents the percentage of tasks where at least one successful solution is found among K sampled trajectories, and \textbf{(2) Lowest Overthinking@K}, which selects the trajectory with the lowest overthinking score among K samples and reports the percentage of these selected trajectories that are successful. Pass@K evaluates the model’s ability to find any working solution (i.e., the upper bound for Lowest Overthinking@K), while Lowest Overthinking@K assesses our model's capability to identify the most promising solution as illustrated in \cref{fig:figure2}. The confidence intervals (CI) showcased were computed using Wilson score \cite{wallis2013binomial}

This method of selecting solutions based on overthinking scores yields impressive efficiency gains. By limiting to two samples with the lowest reasoning, we achieve a 27.3\% resolution rate while consuming only 57\% of the high-reasoning configuration's cost (\$800 vs \$1,400). Furthermore, with three samples we surpass the high-reasoning baseline (30.3\% vs 29.1\%) while still saving \$200 in computational costs. Our findings demonstrate that monitoring and controlling overthinking behavior is a highly effective strategy for optimizing both the performance and efficiency of language reasoning models in real-world applications.

