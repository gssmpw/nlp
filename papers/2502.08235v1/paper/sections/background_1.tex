In this section, we explore Large Reasoning Models (LRMs) and agentic environments,
where LRMs must balance sophisticated reasoning with practical actions. We examine how these models navigate environments requiring deep analytical thinking and concrete interactions. This leads to a fundamental dilemma between reasoning depth and actionâ€”a tension we will explore in \cref{section3}.


\subsection{Large Reasoning Models and Agentic Environments}
LRMs, defined as language models optimized through process reward models and test-time compute scaling \cite{xu2025largereasoningmodelssurvey}, represent an evolution beyond traditional LLMs through their focus on reliable step-by-step reasoning \cite{deepseekai2025deepseekr1incentivizingreasoningcapability,openai_o1}. These models achieve unprecedented performance through extended chain-of-thought reasoning \cite{wei2023chainofthoughtpromptingelicitsreasoning} and rigorous self-verification \cite{madaan2023selfrefineiterativerefinementselffeedback}. However, their extended focus on internal reasoning depth raises important questions about performance in interactive environments.

\begin{figure*}[t]
    \centering
    \includegraphics[width=1\textwidth,trim={0cm 4cm 0cm 1cm},clip]{paper/manifestations.pdf}
\caption{Three distinct patterns of overthinking behavior in LRM agent trajectories. (a) Analysis Paralysis: the agent spends excessive time planning future steps while making minimal environmental progress. (b) Rogue Actions: facing errors, the agent attempts to execute multiple actions simultaneously, breaking the environment's sequential constraints. (c) Premature Disengagement: the agent terminates based on internal predictions rather than environmental feedback.}
    \label{fig:manifestations}
\end{figure*}

\paragraph{Agency in AI Systems} While traditional AI defined agents broadly as entities that perceive and act upon their environment \cite{russell1995ai}, modern approaches view agency as a spectrum of capabilities \cite{zhang2024agenticinformationretrieval, kapoor2024aiagentsmatter}, emphasizing autonomous goal pursuit, natural language interfaces, and structured outputs like tool use \cite{yang2024sweagentagentcomputerinterfacesenable}. This framework has been particularly influential in software engineering, where various agent architectures \cite{ibm_swe_agents, aws_q_developer_agent, liu2024marscodeagentainativeautomated} have been developed to solve real-world GitHub issues \cite{jimenez2024swebenchlanguagemodelsresolve}. Our work examines how LRMs' distinctive reasoning capabilities affect their performance in these agentic environments.

