
\begin{figure}[t]
    \centering
    \includegraphics[width=1\linewidth,trim={0cm 0cm 0cm 0cm},clip]{paper/model_analysis_combined.pdf}
    \vspace{-25pt}
\caption{
Higher overthinking scores (tendency to favor internal reasoning over environmental feedback) correlate with lower issue resolution rates across all models. Reasoning models exhibit consistently higher overthinking tendencies, suggesting that excessive reliance on internal simulation impairs task performance. Model nomenclature: "FC" indicates native function calling capability, "DS" represents DeepSeek models, and suffixes o1\_high and o1\_low denote models with reasoning effort set to high and low respectively.
}
    \label{fig:figure1}
\end{figure}

 
\begin{figure}[t]
    \centering
    \includegraphics[width=1\linewidth,trim={0cm 0cm 0cm 0cm},clip]{paper/figure2.pdf}
\caption{OpenHands Execution Pipeline. 1) The system initializes by presenting the agent with the primary issue and previous action history. 2) The agent reaches a decision point -- 2a) Direct action formulation and execution, or 2b) Internal simulation of potential actions and outcomes, potentially leading to \textbf{overthinking}. 3) The chosen action is executed, generating environmental feedback which updates the event stream. This cycle continues until task completion.}
    \label{fig:figure3}
\end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[width=1\linewidth,trim={0cm 0cm 0cm 0cm},clip]{paper/pass_at_k_plot.pdf}
    \vspace{-25pt}
    \caption{
Performance comparison of Pass@k and Lowest Overthinking@k on SWE-bench Verified. Pass@k represents the success rate when considering k solutions, while Lowest Overthinking@k shows the success rate when selecting the solution with minimal overthinking from k samples. Using k=2 samples with low reasoning effort, we achieve a 27.3\% success rate while reducing computational costs by 43\% compared to high reasoning configurations. Increasing to k=3 further improves performance to 30.3\% surpassing the high configuration using 15\% less computational costs. The confidence intervals (CI) were computed using Wilson score \cite{wallis2013binomial}.}
    \label{fig:figure2}
\end{figure}

Large Reasoning Models (LRMs) \cite{ guan2025rstarmathsmallllmsmaster, xu2025largereasoningmodelssurvey}, such as OpenAI’s
o1 \cite{openai_o1_system_card_2024}, Alibaba’s QwQ \cite{qwq-32b-preview}, or Deepseek's R1 \cite{deepseekai2025deepseekr1incentivizingreasoningcapability} represent a breakthrough in large language models (LLMs). These advanced systems have fundamentally redefined AI’s problem-solving capabilities across various domains \cite{besta2025reasoning}. In particular, LRM's self-correction abilities enable them to achieve impressive scores in several benchmarks, such as AIME 2024 \cite{aops_aime_2024}, MMLU \cite{hendrycks2021measuringmassivemultitasklanguage}, or GPQA-Diamond \cite{rein2023gpqagraduatelevelgoogleproofqa} among others \cite{deepseekai2025deepseekr1incentivizingreasoningcapability, openai_o1_system_card_2024, openai_o1_mini, qwq-32b-preview, guan2025rstarmathsmallllmsmaster}.

Despite extensive analysis of LRMs in non-agentic environments, there remains a critical gap in understanding how LRMs perform in agentic environments \cite{smeyatsky2024agentic}, where models must simultaneously gather, retain, and act upon new information to complete their tasks \cite{zhang2024agenticinformationretrieval,yang2024sweagentagentcomputerinterfacesenable}. In this context, LRMs face a fundamental challenge: models must choose between engaging directly with their environment or relying on internal reasoning about potential actions and their hypothetical consequences, a challenge we define as the \textbf{\textit{Reasoning-Action Dilemma}}.

In this work, we present the first comprehensive empirical study of LRMs in agentic tasks at balancing the Reasoning-Action Dilemma, using real-world software engineering tasks as our experimental framework \cite{jimenez2024swebenchlanguagemodelsresolve, yang2024sweagentagentcomputerinterfacesenable}. We employ \emph{SWE-bench Verified} \cite{jimenez2024swebenchlanguagemodelsresolve, swebench_verified} as our benchmark, using the \emph{CodeAct} agent scaffolding \cite{wang2024executablecodeactionselicit} within the \emph{OpenHands} framework \cite{wang2024openhandsopenplatformai}. This setup creates a controlled environment where models must balance information gathering with reasoning chains while maintaining context across multiple interactions as illustrated in \autoref{fig:figure3}. A proper balance becomes critical as too much reliance on internal reasoning chains might lead to false assumptions about the environment.


We observe that LRMs exhibit a consistent pattern of favoring internal simulation over environmental interaction in the Reasoning-Action Dilemma, spending increasing amounts of time constructing elaborate chains of predicted actions rather than adapting to actual system responses, a phenomenon we define as \textbf{overthinking}.

To quantify \textbf{overthinking}, we develop and validate a systematic evaluation framework using LLM-as-a-judge \cite{zheng2023judgingllmasajudgemtbenchchatbot} that identifies three key patterns: Analysis Paralysis, Rogue Actions, and Premature Disengagement (\cref{fig:manifestations}). Our scoring system strongly correlates with human expert assessments (\cref{fig:human_eval}), confirming its reliability in measuring a model's tendency to favor internal simulation over environmental interaction. We applied this framework to analyze \textbf{4018 trajectories}, creating a comprehensive open-source dataset to advance research in balancing reasoning and action in agentic environments.

Statistical analysis reveals two distinct patterns in overthinking behavior. First, regression analysis demonstrates a significant negative correlation between overthinking and issue resolution rates for both reasoning and non-reasoning models (\cref{fig:figure1}), with the latter showing a steeper decline in performance as overthinking increases. Second, a direct comparison reveals that reasoning models consistently exhibit higher overthinking scores—nearly three times higher than non-reasoning models—with this difference being statistically significant as shown 
 afterward in \cref{tab:overthinking_scores}. These patterns suggest that while all models are susceptible to overthinking, reasoning models are particularly prone to this behavior.

Addressing overthinking yields substantial practical benefits. Running o1 with high reasoning effort achieves 29.1\% issue resolution but costs \$1,400, while the low reasoning variant reaches 21.0\% at 3.5$\times$ lower cost (\$400). Instead of using the expensive high-reasoning configuration, we found that generating two solutions with low reasoning effort (\$800 total) and selecting the one with a lower overthinking score achieves 27.3\% resolution rate (\cref{fig:figure2}). This simple strategy nearly matches the performance of high-reasoning configurations while reducing computational costs by 43\%, demonstrating that overthinking mitigation can dramatically improve the efficiency of LRMs in real-world applications.


Additionally, we suggest two potential approaches to mitigate overthinking in LRMs in agentic environments: native function-calling capabilities and selective reinforcement learning. Both approaches could significantly reduce overthinking while improving model performance, with function-calling models showing particularly promising results (\cref{fixoverthinking}). To facilitate further research into these solutions, we release our evaluation framework and dataset, enabling the broader research community to build upon these findings across different environments and architectures.
