
\subsection{Can native function calling affect overthinking?}

Our experimental analysis compares o1 model configurations with high reasoning effort, evaluating performance both with and without native function calling (FC) capabilities. The integration of FC capabilities yields substantial improvements, increasing the performance score from 29.1\% to 47.7\%, while simultaneously reducing the average overthinking score from 2.43 to 1.05 -- effectively mitigating the overthinking phenomenon.

However, benchmarking against BCFL \cite{berkeley-function-calling-leaderboard} reveals a more nuanced pattern, where the performance differential between FC and non-FC implementations of o1 in multi-turn environments shows a modest improvement from 36\% to 41\%. This comparatively smaller enhancement suggests that FC implementation alone cannot fully account for the dramatic performance improvements observed in our primary experiments.

\subsection{Why doesn't DeepSeek-R1-671B overthink?}
Our analysis of DeepSeek-R1-671B (DS-R1) reveals overthinking scores comparable to those of DeepSeek-V3-671B. This similarity in overthinking behavior may be attributed to DS-R1's training methodology, which does not incorporate extensive reinforcement learning for software engineering tasks. While DS-R1 maintains performance levels similar to DeepSeek-V3 on software engineering benchmarks \cite{deepseekai2025deepseekr1incentivizingreasoningcapability}, our findings suggest that the combination of limited RL training and substantial model scale (671B parameters) contributed to its controlled overthinking behavior.


\subsection{How to fix overthinking?}
\label{fixoverthinking}
While our algorithmic interventions demonstrate immediate practical benefits, they primarily address the symptoms rather than the root causes of overthinking. Our analysis suggests that more fundamental solutions might emerge from understanding how models learn to balance reasoning and environmental interaction. The success of function-calling architectures hints at the importance of explicit interaction training, while the effectiveness of limited reinforcement learning points to the role of training methodology.

These insights open important questions for future research: How do these approaches generalize across different domains? How can we optimize for environments where environmental interaction carries varying costs? Understanding these dynamics could help develop more robust solutions that prevent, rather than just mitigate, overthinking behaviors in large reasoning models.
