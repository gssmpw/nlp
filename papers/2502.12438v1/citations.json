[
  {
    "index": 0,
    "papers": [
      {
        "key": "jdc",
        "author": "Kum, Sangeun and Nam, Juhan",
        "title": "Joint detection and classification of singing voice melody using convolutional recurrent neural networks"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "st500",
        "author": "Wang, Jun-You and Jang, Jyh-Shing Roger",
        "title": "On the preparation and validation of a large-scale dataset of singing transcription"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "jong",
        "author": "Park, Jonggwon and Choi, Kyoyun and Oh, Seola and Kim, Leekyung and Park, Jonghun",
        "title": "Note-level singing melody transcription with transformers"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "musicyolo",
        "author": "Wang, Xianke and Tian, Bowen and Yang, Weiming and Xu, Wei and Cheng, Wenqing",
        "title": "{MusicYOLO: A vision-based framework for automatic singing transcription}"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "spleeter",
        "author": "Hennequin, Romain and Khlif, Anis and Voituret, Felix and Moussallam, Manuel",
        "title": "Spleeter: a fast and efficient music source separation tool with pre-trained models"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "av-svt",
        "author": "Gu, Xiangming and Zeng, Wei and Zhang, Jianan and Ou, Longshen and Wang, Ye",
        "title": "Deep audio-visual singing voice transcription based on self-supervised learning models"
      },
      {
        "key": "altamt",
        "author": "Gu, Xiangming and Ou, Longshen and Zeng, Wei and Zhang, Jianan and Wong, Nicholas and Wang, Ye",
        "title": "Automatic Lyric Transcription and Automatic Music Transcription from Multimodal Singing"
      },
      {
        "key": "rosvot",
        "author": "Ruiqi Li and Yu Zhang and Yongqi Wang and Zhiqing Hong and Rongjie Huang and Zhou Zhao",
        "title": "Robust Singing Voice Transcription Serves Synthesis"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "av-svt",
        "author": "Gu, Xiangming and Zeng, Wei and Zhang, Jianan and Ou, Longshen and Wang, Ye",
        "title": "Deep audio-visual singing voice transcription based on self-supervised learning models"
      },
      {
        "key": "altamt",
        "author": "Gu, Xiangming and Ou, Longshen and Zeng, Wei and Zhang, Jianan and Wong, Nicholas and Wang, Ye",
        "title": "Automatic Lyric Transcription and Automatic Music Transcription from Multimodal Singing"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "wav2vec",
        "author": "Baevski, Alexei and Zhou, Yuhao and Mohamed, Abdelrahman and Auli, Michael",
        "title": "wav2vec 2.0: A framework for self-supervised learning of speech representations"
      },
      {
        "key": "av-hubert",
        "author": "Shi, Bowen and Hsu, Wei-Ning and Lakhotia, Kushal and Mohamed, Abdelrahman",
        "title": "Learning audio-visual speech representation by masked multimodal cluster prediction"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "rosvot",
        "author": "Ruiqi Li and Yu Zhang and Yongqi Wang and Zhiqing Hong and Rongjie Huang and Zhou Zhao",
        "title": "Robust Singing Voice Transcription Serves Synthesis"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "quantization",
        "author": "Cemgil, Ali Taylan and Desain, Peter and Kappen, Bert",
        "title": "Rhythm quantization for transcription"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "nakamura_rhythm",
        "author": "Nakamura, Eita and Yoshii, Kazuyoshi and Sagayama, Shigeki",
        "title": "{Rhythm transcription of polyphonic piano music based on merged-output HMM for multiple voices}"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "weakly_attention",
        "author": "Nishikimi, Ryo and Nakamura, Eita and Fukayama, Satoru and Goto, Masataka and Yoshii, Kazuyoshi",
        "title": "Automatic singing transcription based on encoder-decoder recurrent neural networks with a weakly-supervised attention mechanism"
      },
      {
        "key": "beat_attention",
        "author": "Nishikimi, Ryo and Nakamura, Eita and Goto, Masataka and Yoshii, Kazuyoshi",
        "title": "End-to-end melody note transcription based on a beat-synchronous attention mechanism"
      },
      {
        "key": "CRNN_HSMM",
        "author": "Nishikimi, Ryo and Nakamura, Eita and Goto, Masataka and Yoshii, Kazuyoshi",
        "title": "{Audio-to-score singing transcription based on a CRNN-HSMM hybrid model}"
      },
      {
        "key": "ctc_score1",
        "author": "Deng, Tengyu and Nakamura, Eita and Yoshii, Kazuyoshi",
        "title": "{Audio-to-score singing transcription based on joint estimation of pitches, onsets, and metrical positions with tatum-level CTC loss}"
      },
      {
        "key": "ctc_score2",
        "author": "Deng, Tengyu and Nakamura, Eita and Nishikimi, Ryo and Yoshii, Kazuyoshi and others",
        "title": "{End-to-end singing transcription based on CTC and HSMM decoding with a refined score representation}"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "weakly_attention",
        "author": "Nishikimi, Ryo and Nakamura, Eita and Fukayama, Satoru and Goto, Masataka and Yoshii, Kazuyoshi",
        "title": "Automatic singing transcription based on encoder-decoder recurrent neural networks with a weakly-supervised attention mechanism"
      },
      {
        "key": "beat_attention",
        "author": "Nishikimi, Ryo and Nakamura, Eita and Goto, Masataka and Yoshii, Kazuyoshi",
        "title": "End-to-end melody note transcription based on a beat-synchronous attention mechanism"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "ctc_score1",
        "author": "Deng, Tengyu and Nakamura, Eita and Yoshii, Kazuyoshi",
        "title": "{Audio-to-score singing transcription based on joint estimation of pitches, onsets, and metrical positions with tatum-level CTC loss}"
      },
      {
        "key": "ctc_score2",
        "author": "Deng, Tengyu and Nakamura, Eita and Nishikimi, Ryo and Yoshii, Kazuyoshi and others",
        "title": "{End-to-end singing transcription based on CTC and HSMM decoding with a refined score representation}"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "CRNN_HSMM",
        "author": "Nishikimi, Ryo and Nakamura, Eita and Goto, Masataka and Yoshii, Kazuyoshi",
        "title": "{Audio-to-score singing transcription based on a CRNN-HSMM hybrid model}"
      }
    ]
  },
  {
    "index": 15,
    "papers": [
      {
        "key": "ctc_score1",
        "author": "Deng, Tengyu and Nakamura, Eita and Yoshii, Kazuyoshi",
        "title": "{Audio-to-score singing transcription based on joint estimation of pitches, onsets, and metrical positions with tatum-level CTC loss}"
      },
      {
        "key": "ctc_score2",
        "author": "Deng, Tengyu and Nakamura, Eita and Nishikimi, Ryo and Yoshii, Kazuyoshi and others",
        "title": "{End-to-end singing transcription based on CTC and HSMM decoding with a refined score representation}"
      }
    ]
  },
  {
    "index": 16,
    "papers": [
      {
        "key": "weakly_attention",
        "author": "Nishikimi, Ryo and Nakamura, Eita and Fukayama, Satoru and Goto, Masataka and Yoshii, Kazuyoshi",
        "title": "Automatic singing transcription based on encoder-decoder recurrent neural networks with a weakly-supervised attention mechanism"
      },
      {
        "key": "beat_attention",
        "author": "Nishikimi, Ryo and Nakamura, Eita and Goto, Masataka and Yoshii, Kazuyoshi",
        "title": "End-to-end melody note transcription based on a beat-synchronous attention mechanism"
      },
      {
        "key": "CRNN_HSMM",
        "author": "Nishikimi, Ryo and Nakamura, Eita and Goto, Masataka and Yoshii, Kazuyoshi",
        "title": "{Audio-to-score singing transcription based on a CRNN-HSMM hybrid model}"
      }
    ]
  },
  {
    "index": 17,
    "papers": [
      {
        "key": "weakly_attention",
        "author": "Nishikimi, Ryo and Nakamura, Eita and Fukayama, Satoru and Goto, Masataka and Yoshii, Kazuyoshi",
        "title": "Automatic singing transcription based on encoder-decoder recurrent neural networks with a weakly-supervised attention mechanism"
      }
    ]
  },
  {
    "index": 18,
    "papers": [
      {
        "key": "seq2seq",
        "author": "Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V",
        "title": "Sequence to sequence learning with neural networks"
      }
    ]
  },
  {
    "index": 19,
    "papers": [
      {
        "key": "s2s_amt",
        "author": "Awiszus, B Sc Maximilian",
        "title": "Automatic music transcription using sequence to sequence learning"
      },
      {
        "key": "s2s_asr",
        "author": "Chan, William and Jaitly, Navdeep and Le, Quoc and Vinyals, Oriol",
        "title": "Listen, attend and spell: A neural network for large vocabulary conversational speech recognition"
      }
    ]
  },
  {
    "index": 20,
    "papers": [
      {
        "key": "cs2s",
        "author": "Ullrich, Karen and van der Wel, Eelco",
        "title": "Music transcription with convolutional sequence-to-sequence models"
      }
    ]
  },
  {
    "index": 21,
    "papers": [
      {
        "key": "transformer",
        "author": "Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\\L}ukasz and Polosukhin, Illia",
        "title": "Attention is all you need"
      }
    ]
  },
  {
    "index": 22,
    "papers": [
      {
        "key": "vat",
        "author": "Miyato, Takeru and Maeda, Shin-ichi and Koyama, Masanori and Ishii, Shin",
        "title": "Virtual adversarial training: a regularization method for supervised and semi-supervised learning"
      }
    ]
  },
  {
    "index": 23,
    "papers": [
      {
        "key": "ctc",
        "author": "Graves, Alex and Fern{\\'a}ndez, Santiago and Gomez, Faustino and Schmidhuber, J{\\\"u}rgen",
        "title": "Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks"
      }
    ]
  },
  {
    "index": 24,
    "papers": [
      {
        "key": "jukebox",
        "author": "Dhariwal, Prafulla and Jun, Heewoo and Payne, Christine and Kim, Jong Wook and Radford, Alec and Sutskever, Ilya",
        "title": "Jukebox: A generative model for music"
      }
    ]
  },
  {
    "index": 25,
    "papers": [
      {
        "key": "vocano",
        "author": "Hsu, Jui-Yang and Su, Li",
        "title": "{VOCANO}: A Note Transcription Framework For Singing Voice In Polyphonic Music"
      },
      {
        "key": "ctc_note",
        "author": "Wang, Jun-You and Jang, Jyh-Shing Roger",
        "title": "Training a Singing Transcription Model Using Connectionist Temporal Classification Loss and Cross-Entropy Loss"
      },
      {
        "key": "notation1",
        "author": "Carvalho, Ralf Gunter Correa and Smaragdis, Paris",
        "title": "Towards end-to-end polyphonic music transcription: Transforming music audio directly to a score"
      },
      {
        "key": "notation2",
        "author": "Miguel A. Rom{\\'a}n and A. Pertusa and Jorge Calvo-Zaragoza",
        "title": "An End-to-end Framework for Audio-to-Score Music Transcription on Monophonic Excerpts"
      },
      {
        "key": "notation3",
        "author": "Rom{\\'a}n, Miguel A and Pertusa, Antonio and Calvo-Zaragoza, Jorge",
        "title": "A holistic approach to polyphonic music transcription with neural networks"
      }
    ]
  },
  {
    "index": 26,
    "papers": [
      {
        "key": "adt",
        "author": "Chih-Wei Wu and Alexander Lerch",
        "title": "Automatic Drum Transcription Using the Student-Teacher Learning Paradigm with Unlabeled Music Data"
      },
      {
        "key": "ssl_vocal",
        "author": "Keum, Sagneun and Lin, Jing-Hua and Su, Li and Nam, Juhan",
        "title": "Semi-supervised Learning Using Teacher-student Models for Vocal Melody Extraction"
      },
      {
        "key": "jdc_note",
        "author": "Kum, Sangeun and Lee, Jongpil and Kim, Keunhyoung Luke and Kim, Taehyoung and Nam, Juhan",
        "title": "Pseudo-label transfer from frame-level to note-level in a teacher-student framework for singing transcription from polyphonic music"
      }
    ]
  }
]