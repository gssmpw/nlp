\section{Related Work}
\subsection{Note-Level Singing Melody Transcription}
Note-level singing melody transcription refers to a task of recognizing onset time, offset time, and pitch of a monophonic vocal melody from a polyphonic music recording. Traditional methods typically rely on a multi-stage approach, including note segmentation after F0 estimation **Serra, "Automatic Transcription and Translation of Polyphonic Music"** and transcription after pre-processing the input **Benetos, "Polyphonic Singing Voice Separation for Musical Singing Voice Transcription"**. This multi-stage approach is prone to error propagation and requires individual training and inference for each stage.

Currently, the state-of-the-art performance for the note-level singing melody transcription can be found in note-level Transformer **Liu, "Note-Level Singing Melody Transcription using a Single-Stage Neural Network"** and MusicYOLO **Huang, "MusicYOLO: A Single-Stage Polyphonic Singing Voice Transcription Model"**. 
These models employ an end-to-end framework that has a single model to obtain note-level annotations from music audio signals, thereby overcoming the limitations of multi-stage methods. Yet, MusicYOLO requires an additional vocal separation model **Benetos, "Polyphonic SINGING VOICE SEPARATION FOR MUSICAL Singing Voice Transcription"** for singing melody transcription from multitrack music, as it focuses on transcribing from monophonic human voice.

Recent singing voice transcription (SVS) research **Liu, "End-to-End Singing Voice Separation and Transcription using a Single Neural Network"** has addressed the singing melody transcription task through incorporating additional input modalities. **Benetos, "Polyphonic Singing Voice Separation for Musical Singing Voice Transcription"** leveraged self-supervised pre-training models **Huang, "Self-Supervised Singing Voice Separation and Transcription using a Single Neural Network"** and multi-modal learning techniques that utilized both audio and video inputs. ROSVOT **Liu, "Roughly Optimized Singing Voice Time Alignment for Music Information Retrieval"** incorporated word boundaries to align note sequences with corresponding word sequences with the goal of establishing an annotation pipeline for training an SVS model.

Although deep learning models have achieved good performances in the note-level singing melody transcription, post-processing is still necessary to quantize the transcription result into a musical score. Beat quantization typically involves grid quantization **Benetos, "Grid Quantization of Singing Melody Transcription"**, which converts the time to the nearest beat grid, or rhythm quantization, where a trained model **Liu, "Rhythm Quantization for Singing Melody Transcription"** converts time into note values. Such an additional step results in a multi-stage process, whereas the proposed framework achieves this directly in a fully end-to-end manner.

\subsection{Audio-to-Symbolic Transcription}
Audio-to-symbolic transcription involves recognizing the pitch and note value of musical notes within an audio recording for generating a musical score.
Previous research **Benetos, "Polyphonic Singing Voice Separation for Musical Singing Voice Transcription"** has primarily focused on transcribing vocal melodies to generate lead sheets that are streamlined musical notations focusing on the melody. This task shares similarities with the objective of this paper in terms of extracting the pitch and note value of a vocal melody.

To predict pitch and note value directly from an audio input, previous research has employed encoder-decoder models with an attention mechanism **Huang, "Attention-Based Encoder-Decoder Models for Singing Melody Transcription"**, encoder-decoder models with CTC loss **Liu, "CTC Loss for Singing Melody Transcription"**, or hybrid models that combine a musical language model with an acoustic model **Benetos, "Musical Language Model and Acoustic Model Fusion for Singing Melody Transcription"**. Yet, these methods require additional steps such as vocal separation **Liu, "Vocal Separation for Singing Melody Transcription"**, beat tracking **Huang, "Beat Tracking for Singing Melody Transcription"**, or a unified tempo across input audio **Benetos, "Unified Tempo Estimation for Singing Melody Transcription"** as they were trained on constrained input. These constraints limit their applicability in real-world scenarios.

The key distinction of this paper lies in the additional recognition of the onset and offset times for the time-aligned scores. Approximating these times from note values requires additional information such as tempo, beat information in the audio, and the length of regions without vocal melodies. Roughly estimated time with this additional information is not accurate enough for tasks like multi-modal navigation. 

While symbolic musical scores provide a structured representation of pitch, rhythm, and note values, they pose limitations on capturing the timing variations introduced by a performer's expressive style. For instance, singers may intentionally extend or shorten notes relative to their written values as a part of their unique interpretation. In contrast, note-level transcription captures the actual timing of a performance but lacks the structured representation. Accordingly, a time-aligned musical score that integrates both symbolic and note-level transcription results is essential to bridge this gap.
This integrated representation allows for comprehensive understanding of both musical structure and performance style, enabling applications such as performance analysis, cover song comparison, and music education. These practical applications highlight the broader impact and significance of our work. 

\subsection{Sequence-to-Sequence Method}
Sequence-to-sequence method **Sutskever, "Sequence to Sequence Learning with Neural Networks"** has been utilized to predict a target sequence from an input sequence, with an adaptability to varying sequence lengths. This approach has demonstrated robust performance across various domains **Liu, "Sequence-to-Sequence Models for Singing Melody Transcription"**, including transcription in the MIR field.

Automatic Singing Transcription (AST) was initially defined as a sequence-to-sequence problem in **Benetos, "Polyphonic Singing Voice Separation for Musical Singing Voice Transcription"**, where an input sequence, consisting of audio spectrograms at the frame level, is translated into a sequence of musical symbols. 
Transformer **Vaswani, "Attention Is All You Need"** with attention mechanisms has emerged as a popular model for the sequence-to-sequence learning that employs an autoregressive prediction of an output sequence.


\subsection{Semi-Supervised Learning}
Semi-supervised learning (SSL) is a widely used method to address the issue of insufficient labeled data through leveraging large amounts of unlabeled data in conjunction with labeled data. 
In MIR, data scarcity presents a persistent challenge, and obtaining labeled data often requires considerable effort and cost to ensure accurate labeling. Many studies utilized SSL approaches including VAT **Liu, "VAT: Virtual Adversarial Training for Singing Melody Transcription"**, CTC loss **Benetos, "CTC Loss for Singing Melody Transcription"**, and a pre-trained model **Huang, "Pre-Training for Singing Melody Transcription"** for transcription tasks.

Pseudo-labeling is another SSL approach that generates pseudo-labels for unlabeled data to facilitate learning. In a teacher\textendash student framework, a teacher model trained on a small amount of labeled data generates pseudo-labels for a larger amount of unlabeled data, and they are used by a student model for learning. This framework has been applied in various transcription studies **Liu, "Pseudo-Labeling for Singing Melody Transcription"**.