\section{Related Work}
\subsection{Note-Level Singing Melody Transcription}
Note-level singing melody transcription refers to a task of recognizing onset time, offset time, and pitch of a monophonic vocal melody from a polyphonic music recording. Traditional methods typically rely on a multi-stage approach, including note segmentation after F0 estimation\cite{jdc} and transcription after pre-processing the input\cite{st500}. This multi-stage approach is prone to error propagation and requires individual training and inference for each stage.

Currently, the state-of-the-art performance for the note-level singing melody transcription can be found in note-level Transformer\cite{jong} and MusicYOLO\cite{musicyolo}. 
These models employ an end-to-end framework that has a single model to obtain note-level annotations from music audio signals, thereby overcoming the limitations of multi-stage methods. Yet, MusicYOLO requires an additional vocal separation model\cite{spleeter} for singing melody transcription from multitrack music, as it focuses on transcribing from monophonic human voice.

Recent singing voice transcription (SVS) research\cite{av-svt, altamt, rosvot} has addressed the singing melody transcription task through incorporating additional input modalities. \cite{av-svt, altamt} leveraged self-supervised pre-training models\cite{wav2vec, av-hubert} and multi-modal learning techniques that utilized both audio and video inputs. ROSVOT\cite{rosvot} incorporated word boundaries to align note sequences with corresponding word sequences with the goal of establishing an annotation pipeline for training an SVS model.

Although deep learning models have achieved good performances in the note-level singing melody transcription, post-processing is still necessary to quantize the transcription result into a musical score. Beat quantization typically involves grid quantization\cite{quantization}, which converts the time to the nearest beat grid, or rhythm quantization, where a trained model\cite{nakamura_rhythm} converts time into note values. Such an additional step results in a multi-stage process, whereas the proposed framework achieves this directly in a fully end-to-end manner.

\subsection{Audio-to-Symbolic Transcription}
Audio-to-symbolic transcription involves recognizing the pitch and note value of musical notes within an audio recording for generating a musical score.
Previous research \cite{weakly_attention, beat_attention, CRNN_HSMM, ctc_score1, ctc_score2} has primarily focused on transcribing vocal melodies to generate lead sheets that are streamlined musical notations focusing on the melody. This task shares similarities with the objective of this paper in terms of extracting the pitch and note value of a vocal melody.

To predict pitch and note value directly from an audio input, previous research has employed encoder-decoder models with an attention mechanism\cite{weakly_attention, beat_attention}, encoder-decoder models with CTC loss\cite{ctc_score1, ctc_score2}, or hybrid models that combine a musical language model with an acoustic model\cite{CRNN_HSMM}. Yet, these methods require additional steps such as vocal separation\cite{ctc_score1, ctc_score2}, beat tracking\cite{weakly_attention, beat_attention, CRNN_HSMM}, or a unified tempo across input audio\cite{weakly_attention} as they were trained on constrained input. These constraints limit their applicability in real-world scenarios.

The key distinction of this paper lies in the additional recognition of the onset and offset times for the time-aligned scores. Approximating these times from note values requires additional information such as tempo, beat information in the audio, and the length of regions without vocal melodies. Roughly estimated time with this additional information is not accurate enough for tasks like multi-modal navigation. 

While symbolic musical scores provide a structured representation of pitch, rhythm, and note values, they pose limitations on capturing the timing variations introduced by a performer's expressive style. For instance, singers may intentionally extend or shorten notes relative to their written values as a part of their unique interpretation. In contrast, note-level transcription captures the actual timing of a performance but lacks the structured representation. Accordingly, a time-aligned musical score that integrates both symbolic and note-level transcription results is essential to bridge this gap.
This integrated representation allows for comprehensive understanding of both musical structure and performance style, enabling applications such as performance analysis, cover song comparison, and music education. These practical applications highlight the broader impact and significance of our work. 

\subsection{Sequence-to-Sequence Method}
Sequence-to-sequence method\cite{seq2seq} has been utilized to predict a target sequence from an input sequence, with an adaptability to varying sequence lengths. This approach has demonstrated robust performance across various domains\cite{s2s_amt, s2s_asr} including transcription in the MIR field. 

Automatic Singing Transcription (AST) was initially defined as a sequence-to-sequence problem in \cite{cs2s}, where an input sequence, consisting of audio spectrograms at the frame level, is translated into a sequence of musical symbols. 
Transformer\cite{transformer} with attention mechanisms has emerged as a popular model for the sequence-to-sequence learning that employs an autoregressive prediction of an output sequence. 


\subsection{Semi-Supervised Learning}
Semi-supervised learning (SSL) is a widely used method to address the issue of insufficient labeled data through leveraging large amounts of unlabeled data in conjunction with labeled data. 
In MIR, data scarcity presents a persistent challenge, and obtaining labeled data often requires considerable effort and cost to ensure accurate labeling. Many studies utilized SSL approaches including VAT\cite{vat}, CTC loss\cite{ctc}, and a pre-trained model\cite{jukebox} for transcription tasks\cite{vocano, ctc_note, notation1, notation2, notation3}.

Pseudo-labeling is another SSL approach that generates pseudo-labels for unlabeled data to facilitate learning. In a teacher\textendash student framework, a teacher model trained on a small amount of labeled data generates pseudo-labels for a larger amount of unlabeled data, and they are used by a student model for learning. This framework has been applied in various transcription studies\cite{adt, ssl_vocal,jdc_note}.