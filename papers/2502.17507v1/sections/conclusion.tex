\section{Conclusion \& Future Work}
In this work, we made two contributions to the preference optimization literature. First, we showed that many DPO-style algorithms can be viewed as classification algorithms. In this classification framework, one may choose a specific classification label (soft or hard, using auxiliary information or not) and a specific loss to obtain a DPO-style algorithm. We believe that revealing this connection promotes a mutual transfer of ideas between the classification and preference optimization.

The classification framework also revealed that the underlying problem solved in DPO-style algorithms is under-specified and therefore susceptible to the collapse of both winner-loser probabilities. By endowing the DPO-style algorithms with our newly proposed constraints, we showed that the problem becomes specified, and thus, probability-collapse can be avoided.

A curious observation we made is that while we do want to avoid probability collapse, it is also not true that increasing the probability of the winner will always lead to the best final model. This means that the best performance is usually achieved by increasing the probability of some of the unseen answers. Thus, in future, it is interesting to do a more systematic study of the kinds of unseen responses for which DPO-style algorithms increase the probability during training.