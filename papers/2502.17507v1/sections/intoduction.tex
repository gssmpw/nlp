\section{Introduction}
The problem of ensuring that AI systems act in accordance with human preferences, also known as the {\em alignment} problem, has become a critical focus in machine learning. Reinforcement Learning from Human Feedback (RLHF) has emerged as a promising approach towards solving this problem~\citep{RLHF}. RLHF proceeds by first learning a reward model (RM) from a winner-loser preference dataset followed by employing standard RL algorithms to maximize the learned RM while keeping the model close to a reference model. 

Recent years have witnessed the emergence of algorithms that solve the two RLHF sub-problems as a single optimization problem. Chief among these methods is the Direct Preference Optimization (\texttt{DPO}) algorithm~\cite{DPO}. It proceeds by leveraging the closed-form solution of the RLHF objective and using the preference dataset to align the model, thus obviating the explicit reward-learning phase of RLHF. Since then, numerous extensions and successors of DPO have been proposed, highlighting the need for a deeper examination of this new class of algorithms in order to unify and connect their underlying principles, and to highlight and fix their potential pitfalls.

In this paper, we first present a classification framework that reformulates the family of DPO-style algorithms. The primary benefit of this framework is that it unifies and extends a variety of existing DPO-style algorithms. Most notably, with specific choices of classification labels and loss function, we recover the popular \texttt{DPO} \cite{DPO} and \texttt{IPO} \cite{IPO} algorithms, revealing them as special cases of our classification formulation. Moreover, unlike traditional RLHF approaches that rely solely on binary preference pairs, our classification framework can naturally incorporate richer information, such as ranked lists of preferences and auxiliary score information (e.g.,~ratings) about the responses. %The framework also allows us to easily propose new algorithms that have been under-explored in previous work.

We next leverage this classification perspective to investigate a peculiar behavior observed in several DPO-style algorithms: the decrease, or even collapse to zero, of the winner-loser probabilities. Note that prior work has documented this phenomenon~\cite{nemotron,ppo_vs_dpo,smaug,distilled_DPO,xiao2024caldpo,AIPO,sppo,APO}. Using our proposed classification framework, we pinpoint the root cause of this behavior: minimizing the loss in the DPO-style of algorithms only provides a single constraint for learning two probabilities, leaving the problem {\em under-constrained}. Leaning on this insight, we then propose a new set of constraints that can provably fix the under-specified nature of the DPO-style algorithms, thus effectively addressing the {\em probability-collapse} issue. These constraints are designed to control the movement of winner-loser probability mass between the reference and target policies. Our proposed algorithm, Constrained Controlled Classification DPO (\texttt{C-3DPO}), optimizes the \texttt{DPO} objective under these constraints.  

We evaluate the effectiveness of our set of constraint in enhancing preference alignment. Across two standard datasets and three with up to 13B parameters, \texttt{C-3DPO} outperforms vanilla \texttt{DPO} and several other baselines, delivering higher-quality final models when assessed holistically on the standard MT Benchmark~\cite{mt_bench}.
