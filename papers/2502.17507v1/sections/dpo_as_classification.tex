\section{A Classification View of DPO-style Algorithms}
%Direct Preference Optimization as Classification}
\label{sec:PO-C}

In this section, we show that DPO-style algorithms can be viewed as classification algorithms. The standard classification setting has three main ingredients.

\textbf{First}, we typically construct a classifier hypothesis space by defining a parametric form for probabilities assigned to each class, and then train the classifier to adjust these probabilities. We show that in DPO-style algorithms these probabilities are implicitly defined as follows:
%
\begin{equation}
p_{\theta}(x,y_w,y_l) \assign \textrm{softmax}\big(  r_{\theta}(x , y_w) , r_{\theta}(x , y_l)\big)\ ,
\label{eq:softmax_binary}
\end{equation}
%
where $r_{\theta}$ is the reward defined in \eqref{eq:standard_reward} having substituted $\pi^{*}$ with $\pi_{\theta}$. It is straightforward to see that under distribution $p_{\theta}$, the probability assigned to the {\em winner (preferred)} response $y_w$, denoted by $p^{ w}_{\theta}$, does not depend on the partition function $Z(x)$ and can be written as
%
\begin{equation} \label{pthetaw}
    p^{w}_{\theta}(x,y_w,y_l) := \frac{\big(\frac{\pi_{\theta}(y_w \mid x)}{\piRef(y_w \mid x)}\big)^{\beta}}{\big(\frac{\pi_{\theta}(y_{w} \mid x)}{\piRef(y_{w} \mid x)}\big)^{\beta} + \big(\frac{\pi_{\theta}(y_{l} \mid x)}{\piRef(y_{l} \mid x)}\big)^{\beta}} \ .
\end{equation}
%
%Whenever it is clear from the context, we will write $p^{w}_{\theta}$ instead of $p^{w}_{\theta}(x,y_w,y_l)$.
%The distribution $p_\theta$ in~\eqref{pthetaw} depends on $\pi_\theta$, $\beta$, and $\piRef$, and given that the answer must be in $\{y_w, y_l\}$.
%This distribution could be viewed as the probability of $y_w$ under the model $\pi_\theta$, $\beta$, and $\piRef$, and given that the answer must be in $\{y_w, y_l\}$. 
The probability assigned to the {\em loser (dispreferred)} response $y_l$, denoted by $p^{l}_{\theta}$, is defined similarly. %The distribution $p_\theta=(p_\theta^w,p_\theta^l)$ is over two responses $\{y_w, y_l\}$, and depends on $\pi_\theta$, $\beta$, and $\piRef$. 
Note that the distribution $p_\theta$ in~\eqref{eq:softmax_binary} is a function of $\pi_\theta$, $\beta$, and $\piRef$, and can be thought of as a generalization of the conditional probability of a response $y$ given that $y \in \{y_w, y_l\}$.

\textbf{Second}, in the standard classification setting, 
the dataset gives us access to labels, which we use to extract the probabilities associated with each class. To obtain these target probabilities, we simply use any distribution $p=(p^w , p^l)$ from the simplex $\Delta_{2}$, which is defined as the set of all vectors $p$ in $\mathbb{R}^{2}$ satisfying $p^w , p^l \geq 0$ and $p^w + p^l =1$. In the most basic case we just use the one-hot vector $(p^w , p^l) = (+1 , 0)$ akin to using {\em hard labels}. This indicates that we desire to always prefer the answer $y_w$ to $y_l$ when presented with a choice between the two. More generally, we can use {\em soft labels}, meaning that we put some non-zero weight behind each class. Soft labels have shown to be quite useful in %the context of 
classification~\cite{softLabels}, and by extension have been useful in preference optimization where we are less certain about the given preference $y_w \succ y_l$.

\textbf{Third}, to solve a classification problem, we often define a differentiable loss function measuring the discrepancy between a distribution $p_{\theta}$ in the hypothesis space and the target distribution $p$. We can use any standard loss $\mathcal L$ for classification leading to the optimization problem: $\min_{\theta} \sum_{\mathcal{D}}{\mathcal L}(p_{\theta},p)$. The loss function should be non-negative $\mathcal{L}(p_1, p_2) \geq 0$, and satisfy $\mathcal L(p_1 , p_2)= 0$ if and only if $p_1 = p_2$. A good example is the CE loss.

%Despite its simplicity, w
We now show that a large number of DPO-style algorithms can be viewed as specific instances of this classification framework. The generality arises from the ability to use {\bf (a)} hard or soft labels for the target distribution $p$ and {\bf (b)} different classification losses $\mathcal{L}$. To start we show that the popular \texttt{DPO} algorithm~\cite{DPO} can be recovered using hard labeling and the CE loss.

\begin{remark}[\texttt{DPO}]
Suppose that we use the CE loss and hard labels $p \assign (p^{w} , p^{l}) = (+1 , 0)$ in the classification framework above. Then, using \eqref{pthetaw}, we can write
%
\begin{align*}
{\mathcal L}\big(p_{\theta},\ p\big) &= -\left(p^w \log p^{w}_{\theta} + p^l \log p^{l}_{\theta}\right) \;\; = \; -\log p^{w}_{\theta} \\ %= \log  \frac{\big(\frac{\pi_{\theta}(y_w \mid x)}{\piRef(y_w \mid x)}\big)^{\beta}}{\big(\frac{\pi_{\theta}(y_{w} \mid x)}{\piRef(y_{w} \mid x)}\big)^{\beta} + \big(\frac{\pi_{\theta}(y_{l} \mid x)}{\piRef(y_{l} \mid x)}\big)^{\beta}}\\
&= -\log \sigma\left(\beta\log \frac{\pi_{\theta}(y_w | x)}{\piRef(y_w | x)} - \beta\log \frac{\pi_{\theta}(y_l |x)}{\piRef(y_l | x)}\right)\ ,
\end{align*}
%
which is exactly the \texttt{DPO} loss~\eqref{eq:DPO} summed over $\mathcal{D}$.
\end{remark}

Another popular DPO-style algorithm is \texttt{IPO}~\cite{IPO}. While the derivation of \texttt{IPO} in the original paper looks completely different than \texttt{DPO}, we now show that \texttt{IPO} can also be viewed as a specific instance of our classification framework. In contrast to \texttt{DPO}, to obtain \texttt{IPO} we use soft labeling in conjunction with a less common loss function which we present below.

\begin{remark}[\texttt{IPO}]
Suppose that we use the following loss
%
\begin{equation} \label{IPOloss}
    \mathcal{L}\big(p_{\theta},\ p\big) = \big(\log (p^{w}_{\theta}/p^{l}_{\theta}) - \log (p^{w}/p^{l})\big)^2\ ,
    % \mathcal{L}\big(p_{\theta},\ p\big) = \left(\log\frac{p^{w}_{\theta}}{p^{l}_{\theta}} - \log\frac{p^{w}}{p^{l}}\right)^2\ ,
\end{equation}
%
and soft labels $p \assign (p^{w}, p^{l}) = (\sigma(1/2) , \sigma(-1/2))$ in the above classification framework. We will then recover the \texttt{IPO} loss (Eq.~17 in~\citealt{IPO}). See Appendix \ref{Appendix:IPO} for more details.
\end{remark}

\citet{IPO} motivated \texttt{IPO} primarily as a mean to overcome the issue of vanishing KL penalty in the \texttt{DPO} algorithm. We highlight that \texttt{IPO} can hedge against this issue merely because it employs soft classification labels. Stated differently, to avoid the vanishing KL, we do not have to commit to the specific \texttt{IPO} loss. We can achieve this desirable property using soft classification labels and for instance, the CE loss, instead of the square loss in \texttt{IPO}. In this case, we obtain the Conservative DPO (\texttt{CDPO}) algorithm~\cite{CDPO} in which $p := (p^w,p^l) = (1-\varepsilon,\varepsilon)$ for $0 < \varepsilon < 1/2$ (see more details in Appendix \ref{Appendix:CDPO}). Similarly, to \texttt{IPO}, \texttt{CDPO} is free from the vanishing KL issue.

In Table~\ref{table_example_algorithms}, we show how several DPO-style algorithms can be formulated using our classification framework. 

\begin{table}[htb]
    \centering
    \begin{tabular}{|l||c||c|c|}
        \hline
        Data & Algorithm & Labels & Loss \\ \hline\hline
        \multirow{3}{*}{Pairs} & \texttt{DPO(BT)} & Hard & CE\\ \cline{2-4}
         & \texttt{CDPO} & Soft & CE \\ \cline{2-4}
         & \texttt{IPO} & Soft & Eq.~\eqref{IPOloss} \\ \hline\hline
        List (Sec.~\ref{subsec:Lists}) & \texttt{DPO(PL)} & Hard & CE \\ \hline\hline
         Auxiliary Info & \texttt{RPO} & Soft & CE \\ \cline{2-4}
         (Sec.~\ref{subsec:auxiliary}) & \texttt{Distilled DPO} & Soft & Eq.~\eqref{IPOloss} \\ \hline
    \end{tabular}
    \caption{Examples of DPO-style algorithms as classification. By \texttt{DPO(BT)} and \texttt{DPO(PL)}, we refer to the \texttt{DPO} algorithm with Bradley-Terry and Plackett-Luce (see Sec.~\ref{subsec:Lists}) models, respectively. More details on \texttt{RPO}~\cite{nemotron} and \texttt{Distilled DPO}~\cite{distilled_DPO} algorithms can be found in Sec.~\ref{subsec:auxiliary}.}
    \label{table_example_algorithms}
\end{table}

We now provide two straightforward extensions of the above classification framework that allow us to incorporate a larger set of existing DPO-style algorithms.

\subsection{List of Preferences}
\label{subsec:Lists} 

Our classification framework can be extended to work with lists, rather than pairs, of preferences. In particular, assume that we have $N$ responses for each prompt $x$, giving us a dataset of the form $\mathcal{D} = \{ (x,y_1, y_2,\ldots,y_N) \}.$ In this case, we can define a list version of the probability vector $p_{\theta}(x,y_1,\ldots, y_N)$ similarly to~\eqref{eq:softmax_binary}, 
%
%$$p_{\theta}(x,y_1,\ldots, y_N) \assign \textrm{softmax}\big(r_{\theta}(x , y_1),\ldots,r_{\theta}(x , y_N)\big)\ ,$$
%
together with a target distribution $p$.
%$p \in \Delta_{N}$ being any vector in the $N$-dimensional simplex. 
With this simple extension we can now incorporate existing DPO-style algorithms that work with lists. For instance, we can show that \texttt{DPO} with the Plackett-Luce model for preferences~\cite{DPO} can be captured in our classification framework, again using hard labels and the CE loss. See Appendix \ref{Appendix:DPO_PL} for a proof.

\subsection{Auxiliary Information}
\label{subsec:auxiliary}

A second important extension pertains to the definition of soft labels in our classification setting. So far we have only worked with soft labels that are fixed across the entire dataset, for instance, $p \assign (p^{w}, p^{l}) = (\sigma(1/2) , \sigma(-1/2))$ for all $(x, y_w, y_l)$ in \texttt{IPO}. These fixed labels are agnostic about any extra information we may have about our data triplets $(x , y_w , y_l)$. However, in some applications we may have access to some auxiliary scores, $s_w,s_l$, (e.g.,~ratings) associated with each response, which can then be used to enrich our soft labels. 

More formally, suppose now that our dataset is comprised of ${\mathcal D} = (x,y_w, s_w, y_l, s_l)$. To obtain the soft labels we can employ, for instance, $p = \text{softmax}(s_w, s_l)$. Combining this with the \texttt{IPO} loss~\eqref{IPOloss}, we recover \texttt{Distilled DPO} (Eq.~7 in~\citealt{distilled_DPO}). Using the same soft labels, but with the CE loss recovers \texttt{RPO} (see Sec.~3.3.2 in~\citealt{nemotron}). We provide more details on both algorithms in Appendix~\ref{Appendix:RPO}. These natural extensions further demonstrate that our classification framework is fairly general as well as sufficiently flexible to capture a large number of existing DPO-style algorithms.