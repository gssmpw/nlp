\section{Preliminaries} \label{sec:prelim}
%Background on DPO-style Algorithms}%irect Preference Optimization}

In this section, we present the key ingredients of preference optimization, which we will build upon in the subsequent sections. In this setting, we are given a dataset $\mathcal{D}$ of triplets $(x , y_w , y_l)$, where $x$ is a prompt, while $y_w$ and $y_l$ reflect our preference in choosing $y_w$ over $y_l$ conditioned on $x$. We are also given a reference $\piRef$ serving as a guardrail.

In RLHF, we first employ $\mathcal{D}$ to train a parameterized RM, $r_\phi$, and then use it to solve the following:
%
\begin{equation} \label{eq:standard_RLHF}
    \!\!\!\!\max_{\theta}\ \E{x}\Big[ \E{y \sim \pi_{\theta}}\big[r_\phi(x,y)\big] - \beta \mathbb{KL}\big(\pi_{\theta}(\cdot|x)||\piRef(\cdot|x)\big)\Big],
\end{equation}
%
where $\beta > 0$ is a hyper-parameter denoting the relative importance of reward maximization against ensuring a low deviation from $\piRef$. The RM is typically learned 
\iffalse
using the cross-entropy (CE) loss:
%
\begin{equation} \label{eq:reward-CE-loss}
\mathcal L(\phi,\mathcal D) := - \hspace{-0.2in}\sum_{(x,y_w,y_l)\in\mathcal D} \hspace{-0.1in} \log\sigma\big(r_\phi(x,y_w) - r_\phi(x,y_l)\big)\;,
\end{equation}
%
\fi
assuming that preferences follow the Bradley-Terry (BT) model:
%
\begin{equation}
p(y_w \succ y_l \mid x) = \sigma\big(r(x,y_w) - r(x,y_l)\big)\;,
\label{eq:standard_BT}    
\end{equation}
%
% \begin{eqnarray}
%     p(y_w \succ y_l \mid x) & = & \frac{\exp\big(r(x,y_w)\big)}{\exp\big(r(x,y_w)\big) + \exp\big(r(x,y_l)\big)} \nonumber \\
%     & = & \sigma\big(r(x,y_w) - r(x,y_l)\big)\ ,
%     \label{eq:standard_BT}
% \end{eqnarray}
where $\sigma(x) = 1/(1+\exp(-x))$ is the sigmoid function and $r$ is the latent reward of the annotator. Therefore, fine-tuning $\pi_\theta$ in the RLHF approach is split into two stages: reward-learning using the BT model, followed by a policy optimization using~\eqref{eq:standard_RLHF}.

More recently, a family of algorithms have emerged that solve the above two problems in a single stage, which is referred to as Direct Preference Optimization (DPO)-style algorithms. The key insight in DPO-style algorithms is that problem \eqref{eq:standard_RLHF} admits the following closed-form solution:
%
\begin{equation}
\pi^*(y | x) = \frac{\piRef(y | x)\exp\big(r(x,y)/\beta\big)}{Z(x)}\ ,
\label{eq:soft-MDP-optimal}
\end{equation}
%
where $Z(x)$ is the partition function and is generally intractable. We can rewrite~\eqref{eq:soft-MDP-optimal} as
%
\begin{equation} \label{eq:standard_reward}
r(x,y) = \beta\log \frac{Z(x)\pi^{*}(y | x)}{\piRef(y | x)}\ .
%r(x,y) = \beta\log Z(x)g_{\pi_{\textrm{ref}}}^{\pi^{*}}(x , y)\ ,
\end{equation}
%
%where throughout the paper we denote $g_{\mu}^{\pi}(x,y) = \frac{\pi(y \mid x)}{\mu(y \mid x)}$ for two probabilities $\pi$ and $\mu$.
Substituting $r(x,y)$ from~\eqref{eq:standard_reward}, $Z(x)$ cancels out, leading to the optimization problem solved by \texttt{DPO}:
%
\begin{equation} \label{eq:DPO}
\min_{\theta} \hspace{-0.15in}\sum_{(x, y_w, y_l) \in \mathcal{D}} \hspace{-0.2in}-\log \sigma\left(\beta\log \frac{\pi_{\theta}(y_w | x)}{\piRef(y_w | x)} - \beta\log \frac{\pi_{\theta}(y_l | x)}{\piRef(y_l | x)}\right)\;. 
\hspace{-0.1in}
\end{equation}
%  \max_{\theta} \hspace{-0.1in}\sum_{(x, y_w, y_l) \in \mathcal{D}} \hspace{-0.2in}\log \sigma\Big(\beta\log \frac{\pi_{\theta}(y_w \mid x)}{\piRef(y_w \mid x)} - \beta\log \frac{\pi_{\theta}(y_l \mid x)}{\piRef(y_l \mid x)}\Big)\ .
