\section{Derivations of \texttt{IPO} as a Classification Algorithm} 
\label{Appendix:IPO}

    We recall that \texttt{IPO} can be formulated as a classification problem with the soft labels $p \assign (p^{w}, p^{l}) = (\sigma(1/2) , \sigma(-1/2))$ and the loss given by
    \begin{equation} \label{IPO-Take2}
        \mathcal{L}\big(p_{\theta},\ p\big) = \left(\log\frac{p^{w}_{\theta}}{p^{l}_{\theta}} - \log\frac{p^{w}}{p^{l}}\right)^2\ .
    \end{equation}
    To show that we indeed recover the \texttt{IPO}, we first note that
    \begin{equation*}
        \frac{p^{w}}{p^{l}} = \frac{\sigma(1/2)}{\sigma(-1/2)} = \frac{1 + \exp(1/2)}{1 + \exp(-1/2)} = \exp(1/2)\ ,
    \end{equation*}
    where the second equality follows from the definition of the sigmoid $\sigma(x) = 1/(1 + \exp(-x))$. Moreover, using the definitions of $p^{w}_{\theta}$ (see \eqref{pthetaw}) and $p^{l}_{\theta}$, we obtain that 
    \begin{equation} \label{IPOfrac}
        \frac{p^{w}_{\theta}}{p^{l}_{\theta}} = \frac{\big(\frac{\pi_{\theta}(y_w \mid x)}{\piRef(y_w \mid x)}\big)^{\beta}}{\big(\frac{\pi_{\theta}(y_l \mid x)}{\piRef(y_l \mid x)}\big)^{\beta}} = \left(\frac{\pi_{\theta}(y_w | x)\piRef(y_l | x)}{\pi_{\theta}(y_l | x)\piRef(y_w | x)}\right)^{\beta}\ .
    \end{equation}
    Plugging these two developments to the loss in \eqref{IPO-Take2} yields
    \begin{equation*}
        \mathcal{L}\big(p_{\theta},\ p\big) = \left(\log\frac{p^{w}_{\theta}}{p^{l}_{\theta}} - \log\frac{p^{w}}{p^{l}}\right)^2 = \left(\beta\log \frac{\pi_{\theta}(y_w | x)\piRef(y_l | x)}{\pi_{\theta}(y_l | x)\piRef(y_w | x)} - \frac{1}{2}\right)^2 = \left(\log \frac{\pi_{\theta}(y_w | x)\piRef(y_l | x)}{\pi_{\theta}(y_l | x)\piRef(y_w | x)} - \frac{1}{2\beta}\right)^2\ ,
    \end{equation*} 
    which is exactly \texttt{IPO} (see Eq. (17) of~\citealt{IPO}).

\section{\texttt{CDPO}} \label{Appendix:CDPO}
    Recall the \texttt{CDPO} \cite{CDPO} loss is given for any triplet $(x , y_{w} , y_{l})$ by
    \begin{equation*}
        -(1 - \varepsilon)\log \sigma\left(\beta\log \frac{\pi_{\theta}(y_w | x)}{\piRef(y_w | x)} - \beta\log \frac{\pi_{\theta}(y_l | x)}{\piRef(y_l | x)}\right) - \varepsilon\log \sigma\left(\beta\log \frac{\pi_{\theta}(y_l | x)}{\piRef(y_l | x)} - \beta\log \frac{\pi_{\theta}(y_w | x)}{\piRef(y_w | x)}\right)\ ,
    \end{equation*}
    and then summed over all the triplets in the dataset ${\cal D}$. In order to see \texttt{CDPO} as a classification with the soft labels $p := (p^w,p^l) = (1-\varepsilon,\varepsilon)$ and the CE loss, we will use the following technical fact
    \begin{equation*}
        \sigma(\beta\log(a/b)) = \frac{1}{1 + \exp(-\beta\log(a/b))} = \frac{1}{1 + \frac{\exp{\log b^{\beta}}}{\exp{\log a^{\beta}}}} = \frac{1}{1 + \frac{b^{\beta}}{a^{\beta}}} = \frac{a^{\beta}}{a^{\beta} + b^{\beta}}.
    \end{equation*}
    Therefore, with $a = \frac{\pi_{\theta}(y_w|x)}{\piRef(y_w|x)}$ and $b = \frac{\pi_{\theta}(y_l|x)}{\piRef(y_l|x)}$ we get from \eqref{pthetaw} that $\sigma(\beta\log a/b) = p^{w}_{\theta}$. Similarly, we get that $\sigma(\beta\log(b/a)) = p^{l}_{\theta}$. Therefore, the \texttt{CDPO} loss can be written as follows
    \begin{equation*}
         -(1 - \varepsilon)\log p^{w}_{\theta} - \varepsilon \log p^{l}_{\theta} = -p^{w}\log p^{w}_{\theta} - p^{l}\log p^{l}_{\theta}\ ,
    \end{equation*}
    which is exactly the CE loss on the vectors $p_{\theta} = (p^{w}_{\theta} , p^{l}_{\theta})$ and $p = (p^w,p^l)$.

\section{\texttt{DPO (PL)}} \label{Appendix:DPO_PL}
    In this setting, we are given a dataset of the form $\mathcal{D} = \{ (x,y_1, y_2,\ldots,y_N) \}$. In order to recover, the \texttt{DPO} with Plackett-Luce~\cite{DPO}, we need to generalize the definition of the probability vector $p_{\theta}$ from pairs as in \eqref{pthetaw} to the following $N - 1$ subsets of the list, namely the first $N$, then the first $N-1$, then first $N-2$ and so on. More precisely, for any $1 \leq n < N$ we define 
    \begin{equation*}
        p_{\theta}(x,y_n, y_{n + 1} , \ldots, y_N) = \textrm{softmax}\big(\left(r_{\theta}(x,y_n), r_{\theta}(x,y_{n+1}), \ldots, r_{\theta}(x,y_N)\right)\big)\ .
    \end{equation*}
    In this case, the hard label vectors are defined, for any $1 \leq n < N$, by $p^{[n,N]} := (1 , 0 , \ldots , 0) \in \mathbb{R}^{N - n + 1}$. Now, using the CE loss we get the desired result as follows
    \begin{eqnarray*}
        -\sum_{n = 1}^{N - 1}\sum_{i = n}^{N} p_{i}^{[n,N]}\log p_{\theta}^{i}(x,y_n, y_{n + 1} , \ldots, y_N) & = & - \sum_{n = 1}^{N - 1} \log \frac{\exp\left(\beta\log \frac{Z(x)\pi_{\theta}(y_{n}|x)}{\piRef(y_{n}|x)}\right)}{\sum_{i = n}^{N} \exp\left(\beta\log \frac{Z(x)\pi_{\theta}(y_{i}|x)}{\piRef(y_{i}|x)}\right)}\\
        & = & - \sum_{n = 1}^{N - 1} \log \frac{\exp\left(\beta\log \frac{\pi_{\theta}(y_{n}|x)}{\piRef(y_{n}|x)}\right)}{\sum_{i = n}^{N} \exp\left(\beta\log \frac{\pi_{\theta}(y_{i}|x)}{\piRef(y_{i}|x)}\right)}\\
        & = & - \log \prod_{n = 1}^{N - 1} \frac{\exp\left(\beta\log \frac{\pi_{\theta}(y_{n}|x)}{\piRef(y_n|x)}\right)}{\sum_{i = n}^{N} \exp\left(\beta\log \frac{\pi_{\theta}(y_{i}|x)}{\piRef(y_{i}|x)}\right)}\ .
    \end{eqnarray*}

\section{\texttt{RPO} and \texttt{Distilled DPO}} \label{Appendix:RPO}
    As we discussed in Section 3.2, \texttt{RPO} can be reformulated as a classification with soft labels, which are defined by $p = \text{softmax}(s_w, s_l)$. Therefore, we immediately see that
    \begin{equation*}
        p^{w} = \frac{\exp(s_{w})}{\exp(s_{w}) + \exp(s_{l})} = \frac{1}{1 + \exp(-(s_{w} - s_{l}))}\ ,
    \end{equation*}
    and thus $p^{w} = \sigma(s_{w} - s_{l})$. Similarly, we get that $p^{l} = \sigma(-(s_{w} - s_{l}))$. Thus, \texttt{RPO} can be seen as a generalization of \texttt{CDPO} where the soft labels are given by a certain score and not fixed. To recover the \texttt{RPO} loss we denote $a = \beta\log \frac{\pi_{\theta}(y_w|x)}{\piRef(y_w|x)} - \beta\log \frac{\pi_{\theta}(y_l|x)}{\piRef(y_l|x)}$ and $b = s_w - s_l$. Then, we get that
    \begin{equation*}
        \sigma(b) \log \frac{\sigma(b)}{\sigma(a)} + (1-\sigma(b)) \log\frac{1-\sigma(b)}{1-\sigma(a)} = p^{w}\log\frac{p^{w}}{p_{\theta}^{w}} + p^{l}\log\frac{p^{l}}{p_{\theta}^{l}}\ .
    \end{equation*}
    By eliminating the constant terms (with respect to $\theta$) $p^{w}\log p^{w} + p^{l}\log p^{l}$, we indeed get the CE loss.

    To recover the \texttt{Distilled DPO} (Equation (7) of ~\citet{distilled_DPO}), we consider the soft labels $p = \text{softmax}(s_w, s_l)$ with the loss of \eqref{IPOloss}
    \begin{equation*}
        \mathcal{L}\big(p_{\theta},\ p\big) = \left(\log\frac{p^{w}_{\theta}}{p^{l}_{\theta}} - \log\frac{p^{w}}{p^{l}}\right)^2\ .
    \end{equation*}
    Indeed, it this case we have that
    \begin{equation*}
        \frac{p^{w}}{p^{l}} = \frac{\exp(s_{w})}{\exp(s_{w}) + \exp(s_{l})} \cdot \frac{\exp(s_{w}) + \exp(s_{l})}{\exp(s_{l})} = \frac{\exp(s_{w})}{\exp(s_{l})}\ ,
    \end{equation*}
    and therefore $\log (p^{w}/p^{l}) = s_{w} - s_{l}$. Combining this with \eqref{IPOfrac} yields the desired result.

\section{Proof of Lemma \ref{L:Technical}} \label{Appendix:Lemma}
    First, we recall the lemma.
    \technical*
    \begin{proof}
        First, we write
        \begin{equation*}
            a + b = a\left(\frac{a}{a + b}\right)^{-1}\ ,
        \end{equation*}
        which tanks to classical logarithmic rules yields that
        \begin{equation*}
            \log(a + b) = \log a - \log \frac{a}{a + b} = \log a - \log \frac{1}{1 + b/a}\ .
        \end{equation*}
        Using the definition of the sigmoid function we get
        \begin{equation*}
            \frac{1}{1 + b/a} = \frac{1}{1 + \exp(\log b - \log a)}= \sigma(\log a - \log b)\ ,
        \end{equation*}
        which proves the desired result.
    \end{proof}

\section{\texttt{C-3DPO} implementation details} \label{Appendix:c-3dpo_pseudo_code}
    We show implementation details of \texttt{C-3DPO-Log-$\ell_1$}, \texttt{C-3DPO-Log-$\ell_2$}, \texttt{C-3DPO-$\mathcal{I}$-$\ell_1$}, and \texttt{C-3DPO-$\mathcal{I}$-$\ell_2$} below. 

    \begin{verbatim}
def dpo_loss(pi_logps, ref_logps, yw_idxs, yl_idxs, beta, algo_name, reg_coeff):
    """
    pi_logps: policy logprobs, shape (B,)
    ref_logps: reference model logprobs, shape (B,)
    yw_idxs: preferred completion indices in [0, B-1], shape (T,)
    yl_idxs: dispreferred completion indices in [0, B-1], shape (T,)
    beta: temperature controlling strength of KL penalty
    Each pair of (yw_idxs[i], yl_idxs[i]) represents the indices of a single
    preference pair.
    """
    pi_yw_logps,  pi_yl_logps =  pi_logps[yw_idxs],  pi_logps[yl_idxs]
    ref_yw_logps, ref_yl_logps = ref_logps[yw_idxs], ref_logps[yl_idxs]
    pi_logratios  = pi_yw_logps - pi_yl_logps
    ref_logratios = ref_yw_logps - ref_yl_logps
    losses = -F.logsigmoid(beta * (pi_logratios - ref_logratios))
    if algo_name == 'c3dpo_log_l1':
        reguralized_losses_without_square = \
            pi_yw_logps + pi_yl_logps - ref_yw_logps - ref_yl_logps
        reguralized_losses = (reguralized_losses_without_square) ** 2
        losses = losses + reg_coeff * reguralized_losses
    elif algo_name == 'c3dpo_log_l2':
        reguralized_losses = F.l1_loss(
            pi_yw_logps + pi_yl_logps,
            ref_yw_logps + ref_yl_logps
        )
        losses = losses + reg_coeff * reguralized_losses
    elif algo_name == 'c3dpo_i_l1':
        reguralized_losses_without_square = \
            pi_yw_logps - F.logsigmoid(pi_yw_logps - pi_yl_logps) - \
                (ref_yw_logps - F.logsigmoid(ref_yw_logps - ref_yl_logps))
        reguralized_losses = (reguralized_losses_without_square) ** 2
        losses = losses + reg_coeff * reguralized_losses
    elif algo_name == 'c3dpo_i_l2':
        reguralized_losses = F.l1_loss(
            pi_yw_logps - F.logsigmoid(pi_yw_logps - pi_yl_logps),
            ref_yw_logps - F.logsigmoid(ref_yw_logps - ref_yl_logps)
        )
        losses = losses + reg_coeff * reguralized_losses
    rewards = beta * (pi_logps - ref_logps).detach()
    return losses, rewards
    \end{verbatim}

\section{\textrm{Ultrafeedback Binarized} Claude 3.5 Sonnet v2 win rate prompt and hyperparameters} 
 \label{Appendix:win_rates_prompt}
    In this section we include the prompt used to generate win rates for the \textrm{Ultrafeedback Binarized} experiments. We use Claude Sonnet 3.5 v2 (AWS Bedrock model ID anthropic.claude-3-5-sonnet-20241022-v2:0) to generate win rates. We set the max\_tokens to 1024, temperature to 0, and used default value 0.999 for top\_p and top\_k disabled.
    \begin{verbatim}
For the following query to a chatbot, which response is more helpful?

Query: <prompt>

Response A:
<one of the responses>

Response B:
<the other response>

FIRST provide a one-sentence comparison of the two responses and explain
which you feel is more helpful. SECOND, on a new line, state only "A" or "B"
to indicate which response is more helpful. Your response should use the format:
Comparison: <one-sentence comparison and explanation>
More helpful: <"A" or "B">
    \end{verbatim}

\section{\textrm{Reddit TL;DR} Claude 3.5 Sonnet v2 win rate prompt and hyperparameters} 
 \label{Appendix:win_rates_prompt_tldr}
    In this section we include the prompt used to generate win rates for the \textrm{Reddit TL;DR} experiments. We use Claude Sonnet 3.5 v2 (AWS Bedrock model ID anthropic.claude-3-5-sonnet-20241022-v2:0) to generate win rates. We set the max\_tokens to 1024, temperature to 0, and used default value 0.999 for top\_p and top\_k disabled.
    \begin{verbatim}
Which of the following summaries does a better job of summarizing the most
important points in the given forum post, without including unimportant or
irrelevant details? A good summary is both precise and concise.
Post: {prompt}
Summary A:
{baseline_response}
Summary B:
{generated_response}
FIRST provide a one-sentence comparison of the two summaries, explaining 
which you prefer and why. SECOND, on a new line, state only "A" or "B" to 
indicate your choice. Your response should use the format:
Comparison: <one-sentence comparison and explanation>
Preferred: <"A" or "B">
    \end{verbatim}
\clearpage

\section{Additional win rates analysis of \texttt{C-3DPO} with \textrm{Zephyr-7B-SFT} aligned on \textrm{Ultrafeedback Binarized}} \label{win_rates_across_10_inferences_7b_ultrafeedback}
\begin{figure*}[h]
    \centering
    \includegraphics[width=\textwidth]{figures/c3dpo_ultrafeedback_zp7b_win_rates.png}
    \caption{Win rates comparison of \textrm{Zephyr-7B-SFT} aligned on \textrm{Ultrafeedback Binarized} using \texttt{DPO} and \texttt{C-3DPO}. The first 4 plots show win rates of \texttt{C-3DPO} at individual checkpoints across 10 different inference runs. The last plot shows mean and standard error of win rates across all checkpoints and all inference runs.}
    \label{fig:ultrafeedback_zp7b_win_rates}
\end{figure*}
Figure \ref{fig:ultrafeedback_zp7b_win_rates} shows win rates comparison between \textrm{Zephyr-7B-SFT} aligned with \texttt{DPO} and \texttt{C-3DPO}. We align \textrm{Zephyr-7B-SFT} following~\citet{rasul2024preference} using \texttt{DPO}, \texttt{C-3DPO-Log-$\ell_1$}, \texttt{C-3DPO-Log-$\ell_2$}, \texttt{C-3DPO-$\mathcal{I}$-$\ell_1$}, and \texttt{C-3DPO-$\mathcal{I}$-$\ell_2$} for one epoch, all \texttt{C-3DPO} use hyper-parameter $\lambda=2\times 10^{-4}$. With each checkpoint, we generate responses using test split of \textrm{Ultrafeedback Binarized} using hyperparameters max\_tokens=1000, temperature=1.0, top\_p=0.9, top\_k=50. Different from the head to head setting, we ask Claude to compare the generated response directly with the preferred response in the dataset. The win rates and standard errors are calculated based on 10 different inference runs. 

% \section{Log probability of preferred and rejected sequences with different $\lambda$} \label{logps_tune_lambda_7b_ultrafeedback}
% \begin{figure*}[h]
%     \centering
%     \includegraphics[width=\textwidth]{figures/c3dpo_ultrafeedback_zp7b_tune_lambda.png}
%     \caption{Log probability of \textrm{Zephyr-7B-SFT} aligned on \textrm{Ultrafeedback Binarized} using \texttt{DPO} and \texttt{C-3DPO-Log-$\ell_1$} with different $\lambda$ values. A stronger $\lambda$ mitigates probability collapose.}
%     \label{fig:ultrafeedback_zp7b_tune_lambda}
% \end{figure*}
% We align \textrm{Zephyr-7B-SFT} on \textrm{Ultrafeedback Binarized} using \texttt{C-3DPO-Log-$\ell_1$} with different $\lambda$ values. Figure \ref{fig:ultrafeedback_zp7b_tune_lambda} shows log probability changes of preferred and rejected sequences. Stronger $\lambda$ values, e.g. $5 \times 10^{-0}$ and $5 \times 10^{-1}$ mitigate the probability collapse behavior of \texttt{DPO}. 