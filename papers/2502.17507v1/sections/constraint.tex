\section{Constrained Controlled Classification for DPO (\texttt{C-3DPO})}
\label{sec:algos} 

We now present a general family of constraints to fix the under-specified nature of the DPO-style algorithms described in Section~\ref{sec:DPO-Pitfall}. These constraints are on the probability mass of the winner-loser pair and we use them to control how much this mass changes from the reference policy, $\piRef$, to target policy $\pi_\theta$. We then show how these constraints can be incorporated into any DPO-style loss function and propose our algorithms which we refer to as Constrained Controlled Classification for Direct Preference Optimization (\texttt{C-3DPO}).  

The constraint, with respect to an arbitrary function $\varphi : \mathbb{R} \rightarrow \mathbb{R}$, takes the general form:
%
\begin{align}
\label{P:Cons}
\varphi\big(\pi_{\theta}(y_w|x)\big) &+ \varphi\big(\pi_{\theta}(y_l|x)\big) \\
&= \varphi\big(\piRef(y_w|x)\big) + \varphi\big(\piRef(y_l|x)\big)\ . \nonumber 
\end{align} 
%
%To gain some intuition, suppose that $\varphi(x) =x$. 
Note that the right-hand-side is fixed during training, so the two probabilities on the left-hand-side cannot move in the same direction.

We now generalize this intuition by showing that when the constraint function $\varphi$ is monotonic and is added to the solution characterization of DPO-style algorithms~\eqref{DPO_solution_characterization}, then we can control the direction of the movement of probability mass for winner-loser pairs.

\begin{proposition}
    Let $\varphi : \mathbb{R} \rightarrow \mathbb{R}$ be a monotonic function and assume that~\eqref{P:Cons} holds.
    %\begin{equation} \label{P:Cons}
    %    \varphi\big(\pi_{\theta}(y_w|x)\big) + \varphi\big(\pi(y_l|x)\big) = \varphi\big(\piRef(y_w|x)\big) + \varphi\big(\piRef(y_l|x)\big)\ .
    %\end{equation}
    Then, $\pi_{\theta^{*}}(y_w|x) > \piRef(y_w|x)$ and $\pi_{\theta^{*}}(y_l|x) < \piRef(y_l|x)$.
\end{proposition}

\begin{proof}
For simplicity we assume that $\varphi$ is monotonically increasing (the same arguments can be easily applied to the monotonically decreasing case). From~\eqref{DPO_solution_characterization}, we may write
%
\begin{equation*}
\frac{\pi_{\theta^{*}}(y_w|x)}{\piRef(y_w|x)} = \sqrt[\beta]{\frac{1-\varepsilon}{\varepsilon}} \cdot \frac{\pi_{\theta^{*}}(y_l|x)}{\piRef(y_l|x)}\ .    
\end{equation*}
%
Since $\varepsilon < 1/2$, we have that $(1 - \varepsilon)/\varepsilon > 1$, and therefore
%$\frac{\pi_{\theta^{*}}(y_w|x)}{\piRef(y_w|x)} > \frac{\pi_{\theta^{*}}(y_l|x)}{\piRef(y_l|x)}$, and so:
%
\begin{equation}
 \pi_{\theta^{*}}(y_w|x) > \frac{\pi_{\theta^*}(y_l|x)}{\piRef(y_l|x)} \cdot \piRef(y_w|x) \ .
    \label{eq:base}
\end{equation}
%
We will show that $\pi_{\theta^*}(y_l|x)<\piRef(y_l|x)$ by contradiction. To this end, we assume that $\pi_{\theta^*}(y_l|x)\geq\piRef(y_l|x)$. 
%Then, we have that $\frac{\pi_{\theta^*}(y_l|x)}{\piRef(y_l|x)}\geq 1$ which combined with (\ref{eq:base}) means that 
From \eqref{eq:base}, we immediately obtain that $\pi_{\theta^*}(y_w|x) > \piRef(y_w|x)$. Applying the constraint $\varphi$ to the following two inequalities:
%
\begin{equation*}
    \pi_{\theta^*}(y_w|x) > \piRef(y_w|x) \quad \text{and} \quad \pi_{\theta^*}(y_l|x) \geq \piRef(y_l|x) \ ,
\end{equation*}
%\begin{eqnarray*}
%    \pi_{\theta^*}(y_w|x)&>&\piRef(y_w|x) \\
%    \pi_{\theta^*}(y_l|x)&\geq&\piRef(y_l|x) \ ,
%\end{eqnarray*}
%
and using its monotonicity, we obtain
%
\begin{eqnarray*}
    \varphi\big(\pi_{\theta^{*}}(y_w|x)\big)&>&\varphi\big(\piRef(y_w|x)\big) \\
    \varphi\big(\pi_{\theta^{*}}(y_l|x)\big)&\geq&\varphi\big(\piRef(y_l|x)\big).
\end{eqnarray*}
%
%when $\varphi$ is monotonically increasing, or
%\begin{eqnarray*}
%    \varphi(\pi_{\theta^*}(y_w|x))&<&\varphi(\piRef(y_w|x)) \\
%    \varphi(\pi_{\theta^*}(y_l|x))&\leq&\varphi(\piRef(y_l|x)),
%\end{eqnarray*}
%when $\varphi$ is monotonically decreasing. 
By adding both sides of the above two inequalities, we get a contradiction to \eqref{P:Cons}. Thus, proving that $\pi_{\theta^*}(y_l|x)<\piRef(y_l|x)$. Similarly, we can prove that $\pi_{\theta^{*}}(y_w|x)>\piRef(y_w|x)$, which concludes the proof.
\end{proof}

We just proved that any monotonic constraint function $\varphi$ guarantees that the learned policy, $\pi_{\theta^*}$, assigns a higher (lower) probability to the winner (loser) response than the one assigned to it by the reference policy $\piRef$. Thus, the resulting algorithm avoids the undesirable behavior of probability collapse for the winner response. It is now natural to ask what specific $\varphi$ we should use in the context of this constraint. We present two interesting candidates next. 

\subsection{Logarithmic Constraint $\;\varphi(x) \assign \log x$}
\label{subsec:Log-constraint}

Our first choice is to employ the logarithmic constraint:
%
\begin{eqnarray}
\log\left(\pi_{\theta}(y_w|x)\right) + \log\left(\pi_{\theta}(y_l|x)\right) = \nonumber \\
&& \hspace{-1.7in} \log\left(\piRef(y_w|x)\right) + \log\left(\piRef(y_l|x)\right)\ ,
\label{eq:constraint_phi_log}
\end{eqnarray}
%
which is nice to work with empirically in light of the fact that all terms are in the log-space. Moreover, these log probabilities are already computed in \texttt{DPO}, which would help with efficient implementation of the corresponding \texttt{C-3DPO} algorithm. 

Rather than using hard constraints, it is easier to compute the deviation from the constraint using either $\ell_1$ or $\ell_2$ norm, and then add it as a regularizer to the original DPO-style loss with a regularization parameter, $\lambda$, that trades-off the relative importance of the two terms. 
%to add the deviation from the constraint to the loss as a regularizer using either $\ell_1$ or $\ell_2$ norm. We then add this regularizer to the original DPO-style loss with a hyper-parameter ($\mu$) that trades-off the relative importance of the two losses. 
Equipping the \texttt{DPO} loss~\eqref{eq:DPO} with the logarithmic constraint~\eqref{eq:constraint_phi_log}, we obtain the following loss for \texttt{C-3DPO}:
%
\begin{align}
&\min_{\theta}\;\sum_{\mathcal{D}} \; -\log \sigma\left(\beta\log\frac{\pi_{\theta}(y_w|x)}{\piRef(y_w|x)} - \beta\log\frac{\pi_{\theta}(y_l|x)}{\piRef(y_l|x)}\right) \nonumber \\ 
&\;\;+\lambda\left(\log\frac{\pi_{\theta}(y_w|x)}{\piRef(y_w|x)} + \log\frac{\pi_{\theta}(y_l|x)}{\piRef(y_l|x)}\right)^{2}\ .
\label{eq:C2DPO-log}
\end{align}
%
In contrast to the hard constraint, in this case we do not necessarily force the winner (loser) probability to go up (down). Rather, we impose a penalty when the learner violates the constraint. Notice also that we added the penalty term to the original loss of \texttt{DPO} for simplicity, but in principle, the penalty term can be added to any DPO-style loss covered in our classification framework.

Further, we can show that employing the logarithmic constraint~\eqref{eq:constraint_phi_log} has a meaningful RLHF interpretation. Recall that~\citet{DPO} defined 
%
\begin{equation}
\label{eq:implicit-reward}
 \hat r_{\theta}(x,y) := \beta\log\big(\pi_{\theta}(y|x)/\piRef(y|x)\big),\;\forall y\in\mathcal Y,  
\end{equation}
%
as {\em implicit reward} learned during DPO training. Using this notation, we can rewrite the objective~\eqref{eq:C2DPO-log} simply as
%
\begin{equation*}
-\log \sigma\big( \hat r_{\theta}(x , y_w) - \hat r_{\theta}(x , y_l)\big) + \frac{\lambda}{\beta^{2}}\big( \hat r_{\theta}(x , y_w) +  \hat r_{\theta}(x , y_l)\big)^{2}\ .
\end{equation*}
%
Under $\varphi(x) = \log x$, we solve the original RLHF problem akin to \texttt{DPO}, but we also incentivize that the sum of the implicit rewards for the winner and loser to stay around zero. It follows that the constraint regularizes the implicit rewards so as to avoid rewards that are {\bf (a)} very large and {\bf (b)} have the same sign. These two properties cannot co-exist when employing $\varphi(x) = \log x$, since doing so would yield a large magnitude inside the square and ultimately a large magnitude in the second piece of the loss. Intuitively this can hedge against probability collapse, because in the case of collapse both implicit rewards are large in magnitude and also both have negative sign, which the constraint will highly penalize.


\subsection{Identity Constraint $\;\varphi(x) \assign x$}
\label{subsec:Identity-constraint}

A second interesting choice would be to simply use the identity constraint:
%
\begin{equation}
    \pi_{\theta}(y_w|x) + \pi_{\theta}(y_l|x)  = \piRef(y_w|x) + \piRef(y_l|x)\ .
    \label{eq:constraint_phi_identity}
\end{equation}
%
While~\eqref{eq:constraint_phi_identity} is also a plausible constraint, at first glance it is unclear how to implement it since the constraint is no longer in the log-space and is specified in terms of raw probabilities. Working with raw probabilities is prone to numerical underflow issues, thus, we would like to derive a constraint which is equivalent to~\eqref{eq:constraint_phi_identity} and operates in the log-space. To do so, we make use of the following lemma whose proof is reported in Appendix~\ref{Appendix:Lemma}.

\begin{restatable}[]{lemma}{technical} \label{L:Technical}
For any two numbers $a$ and $b$, we have
%
\begin{equation*}
\log(a + b) = \log a - \log\sigma(\log a - \log b)\ . 
\end{equation*}
%
%where $\sigma$ is the sigmoid function $\sigma(x) = 1/(1+ \exp(-x))$.
\end{restatable}
Applying $\log$ to both sides of~\eqref{eq:constraint_phi_identity}, we obtain
%
\begin{equation*}
\log\big(\pi_{\theta}(y_w|x) + \pi_{\theta}(y_l|x)\big)  = \log\big(\piRef(y_w|x) + \piRef(y_l|x)\big)\ ,
\end{equation*}
%
which can be rewritten using Lemma~\ref{L:Technical} as
%
\begin{align}
\label{eq:v2}
\log\big(&\pi_{\theta}(y_w|x)\big) - \log\sigma\left(\log\frac{\pi_{\theta}(y_w|x)}{\pi_\theta(y_l|x)}\right) \\  
=\; &\log\big(\piRef(y_w|x)\big) - \log\sigma\left(\log\frac{\piRef(y_w|x)}{\piRef(y_l|x)}\right)\ . \nonumber
\end{align}
%
Moving from~\eqref{eq:constraint_phi_identity} to~\eqref{eq:v2}, we have rewritten the constraint entirely in the log-space, thus, avoiding numerical issues, and similar to the logarithmic constraint in Section~\ref{subsec:Log-constraint}, allowing a straightforward implementation of the corresponding \texttt{C-3DPO} algorithm. Equipping the \texttt{DPO} loss~\eqref{eq:DPO} with the logarithmic constraint~\eqref{eq:v2}, we obtain the following loss for \texttt{C-3DPO}: 
%
\begin{align}
% &\min_{\theta}\;\sum_{\mathcal{D}} \; -\log \sigma\big(\hat r(x,y_w) - \hat r(x,y_l)\big) \nonumber \\ 
% &\;\;+\lambda\left(\hat r(x,y_w)/\beta + \log\sigma\left(\log\frac{\piRef(y_w|x)}{\piRef(y_l|x)}\right) \right. \nonumber \\
% &\;\; \left. - \log\sigma\left(\log\frac{\pi_{\theta}(y_w|x)}{\pi_\theta(y_l|x)}\right)\right)^{2}\ .
&\min_{\theta}\;\sum_{\mathcal{D}} \; -\log \sigma\left(\beta\log\frac{\pi_{\theta}(y_w|x)}{\piRef(y_w|x)} - \beta\log\frac{\pi_{\theta}(y_l|x)}{\piRef(y_l|x)}\right) \nonumber \\ 
&\;\;+\lambda\left(\log\frac{\pi_{\theta}(y_w|x)}{\piRef(y_w|x)} + \log\sigma\left(\log\frac{\piRef(y_w|x)}{\piRef(y_l|x)}\right) \right. \nonumber \\
&\;\; \left. - \log\sigma\left(\log\frac{\pi_{\theta}(y_w|x)}{\pi_\theta(y_l|x)}\right)\right)^{2}\ .
\label{eq:C-3DPO-log}
\end{align}

We conclude the section by noting that unlike $\varphi(x)=\log x$, deriving an RLHF interpretation under $\varphi(x)= x$ is subtle. That said, an interesting property under $\varphi(x)= x$ is that the winner probability can increase only by an amount equal to $\piRef(y_l|x)$. This means that we will not put the entire probability mass behind $y_w$. Thus, this constraint can serve as an additional guardrail to maintain a stochastic $\pi_\theta$. 
%response distribution.