\section{Experiments}
\begin{figure*}[h]  % 't' places it at the top of the column
    \centering
    \includegraphics[width=0.9\textwidth, trim={0pt 15pt 0pt 0pt},clip]{figures/c3dpo_head2head.pdf}
    \caption{Head to head win-rate for four different implementations of \texttt{C-3DPO} against \texttt{DPO}. For each individual plot, we take the prompts from the held-out test set of \textrm{Ultrafeedback Binarized} and generate responses from the \texttt{C-3DPO} and \texttt{DPO} model. Then we present the prompt and two responses to Anthropic Claude Sonnet-3.5-v2 and ask which of the two answers is more helpful and honest. We then compute the win rate. Results are averaged over 10 different random seeds.}
    \label{fig:single_column}
\end{figure*}
\iffalse
\begin{figure*}[h]  % 't' places it at the top of the column
   \centering
   \begin{subfigure}{0.4\textwidth}
       \centering
       \includegraphics[width=\textwidth]{figures/radar_uf_7b/1.png}
       %\caption{Subfigure 1}
   \end{subfigure}
   \begin{subfigure}{0.4\textwidth}
       \centering
       \includegraphics[width=\textwidth]{figures/radar_uf_7b/2.png}
       %\caption{Subfigure 2}
   \end{subfigure}
   \begin{subfigure}{0.18\textwidth}
   \vspace{-200pt}
       \includegraphics[width=\textwidth]{figures/radar_uf_7b/table-7B.png}
       %\caption{Subfigure 2}
   \end{subfigure}
   \caption{A comparison between \texttt{C-3DPO} and baselines. (Left) The two variants of \texttt{C-3DPO-Log} better align \textrm{Zephyr-7b-SFT} relative to vanilla \texttt{DPO}. (Right) \texttt{C-3DPO} variants also improve upon existing baselines that are designed to mitigate the collapse issue of \texttt{DPO}.}
   \label{fig:uf_7b_radar}
\end{figure*}
\fi

\begin{figure*}[h]  % 't' places it at the top of the column
   \centering
   \begin{subfigure}{\textwidth}
       \centering
       \includegraphics[width=.85\textwidth]{figures/radar_uf_7b/7b.pdf}
       %\caption{Subfigure 1}
   \end{subfigure}
   \caption{A comparison between \texttt{C-3DPO} and baselines. (Left) The two variants of \texttt{C-3DPO-Log} better align \textrm{Zephyr-7b-SFT} relative to vanilla \texttt{DPO}. (Right) \texttt{C-3DPO} variants also improve upon existing baselines that are designed to mitigate the collapse issue of \texttt{DPO}.}
   \label{fig:uf_7b_radar}
\end{figure*}

In this section, we empirically evaluate \texttt{C-3DPO} across 2 standard datasets and 3 initial checkpoints.
\subsection{\textrm{Ultrafeedback Binarized}}
The original \textrm{Ultrafeedback} dataset comprises 64k prompts, each paired with four completions from various models. To construct \textrm{Ultrafeedback-Binarized}, GPT-4 assigns scores based on criteria such as helpfulness. \textrm{UltraFeedback\_binarized} was then derived by selecting the highest-scoring completion as $y_w$ and a randomly selected as $y_l$.

\begin{figure}[h]  % 't' places it at the top of the column
   \centering
   \begin{subfigure}{0.225\textwidth}
       \centering
       \includegraphics[width=\textwidth]{figures/c3dpo_training_losses_chosen.png}
       %\caption{Subfigure 1}
   \end{subfigure}
   \begin{subfigure}{0.225\textwidth}
       \centering
       \includegraphics[width=\textwidth]{figures/c3dpo_training_losses_rejected.png}
       %\caption{Subfigure 2}
   \end{subfigure}
   \caption{A comparison between \texttt{C-3DPO} and baselines with the GPT-J model on the TL;DR dataset. }
   \label{fig:training_losses}
\end{figure}

Following~\citet{rasul2024preference}, we align \textrm{Zephyr-7B-SFT} by performing various preference optimization algorithms on this initial checkpoint. To holistically evaluate the final model, ~\citet{rasul2024preference} used MT-Bench, a multi-turn benchmark that uses GPT-4 to judge modelsâ€™ performance in $8$ different categories: Writing, Roleplay, Reasoning, Math, Coding, Extraction, STEM, and Humanities. In addition to MT-Bench, we also present the head to head win rate of \texttt{C-3DPO} against \texttt{DPO} by using 200 prompts from the held-out test set of \textrm{Ultrafeedback-Binarized}. We compute the win-rate by asking Anthropic's Claude Sonnet-3.5-v2 which response is more helpful. The exact prompt used for Claude is in Appendix~\ref{Appendix:win_rates_prompt}.

\begin{figure}[h]  % 't' places it at the top of the column
   \centering
   \begin{subfigure}{0.435\textwidth}
       \centering
       \includegraphics[width=\textwidth]{figures/radar_uf_13b/13b.pdf}
       %\caption{Subfigure 1}
   \end{subfigure}
   \caption{Comparison between \texttt{C-3DPO} and \texttt{DPO} for aligning the Olmo-13B-SFT model using the Ultrafeedback-Binarized dataset.}
   \label{fig:uf_13b_radar}
\end{figure}
\begin{figure*}[h]  % 't' places it at the top of the column
   \centering
    \includegraphics[width=0.9\textwidth, trim={0pt 5pt 0pt 0pt},clip]{figures/c3dpo_tldr.pdf}
   \caption{Comparing \texttt{C-3DPO} and the baseline on the TL;DR dataset. The first four plots show the win rates of checkpoints across the first epoch against the preferred human summaries. The last figure shows the win rates of all checkpoints at different temperatures.}
   \label{fig:tl;dr}
\end{figure*}

Recall from Section~\ref{sec:algos} that we proposed two candidates for the constraint function $\varphi$, namely the logarithmic and identity functions. Moreover, notice from~(\ref{eq:C-3DPO-log}) that we measure the deviation from the constraint using an $\ell_2$ penalty. Alternatively, we can measure this deviation using an $\ell_1$ penalty. Therefore, together, we have four specific implementations of \texttt{C-3DPO}, namely: \texttt{C-3DPO-Log-$\ell_1$}, \texttt{C-3DPO-Log-$\ell_2$}, \texttt{C-3DPO-$\mathcal{I}$-$\ell_1$}, and \texttt{C-3DPO-$\mathcal{I}$-$\ell_2$}. In Appendix~\ref{Appendix:c-3dpo_pseudo_code}, we present a pseudo-code for each of these 4 variations. In Figure~\ref{fig:single_column}, we show the head to head win rate of each of these 4 implementations against \texttt{DPO}. For all 4 implementations we used the hyper-parameter $\lambda=2\times 10^{-4}$. While we did not tune $\lambda$ for each of the 4 implementations separately, doing so may increase the gap between \texttt{C-3DPO} and \texttt{DPO}. 

From this result, it is clear that \texttt{C-3DPO-Log} is resulting in better alignment. Therefore, we use the two final checkpoints from \texttt{C-3DPO-Log} for MT-Bench evaluation. In Figure~\ref{fig:uf_7b_radar} we show the MT-Bench evaluation for \texttt{C-3DPO} against vanilla \texttt{DPO}, as well as related DPO-style algorithms that discuss the collapse of probabilities in \texttt{DPO} and aim to mitigate it, such as \texttt{Cal-DPO}~\cite{xiao2024caldpo}, \texttt{SPPO}~\cite{sppo}, and \texttt{DPOP}~\cite{smaug}. \texttt{C-3DPO} is the most competitive algorithm.


We then use \texttt{C-3DPO-Log} on a larger initial checkpoint, namely the \textrm{Olmo-13B-SFT} model from AllenAI~\cite{olmo20242olmo2furious}. We use the same Ultrafeedback-Binarized dataset to align this initial SFT model, and compare \texttt{C-3DPO-Log} against \texttt{DPO}. In Figure~\ref{fig:uf_13b_radar}, we again see the superiority of \texttt{C-3DPO-Log} against vanilla \texttt{DPO} suggesting that these improvements may scale to larger models.

\subsection{\textrm{Reddit TL;DR}}
We then evaluate our proposed method on a summarization task with human-assigned scores for pairs of summaries. For this purpose, we employ the \textrm{Reddit TL;DR} dataset from \citet{reddit}, specifically focusing on the posts under relationships and relationship\_advice subreddits. We follow \citet{odpo} in creating the dataset and include pairs of summaries where one received a higher quality score than the other. The training and validation dataset has 572 and 102 rows respectively, each containing one Reddit post and four to twenty-four user generated summaries and their corresponding scores. During training and testing, we select the highest-scoring summary as $y_w$ and a randomly selected alternative as $y_l$.

We align the GPT-J developed by \citet{gpt-j} with vanilla \texttt{DPO} as well as \texttt{C-3DPO} algorithms. Specifically, we first run one SFT epoch with the \textrm{Reddit TL;DR} dataset on the GPT-J checkpoint, where we maximize the probability of generating the winner summary, and then take the SFT checkpoint for subsequent alignment experiments. We evaluate the final checkpoints by computing win rates against $y_w$ following \citet{DPO, odpo}. We use Claude Sonnet 3.5 v2 as a judge to determine which summary is more concise and precise, and provide prompt used for Claude in the Appendix \ref{Appendix:win_rates_prompt_tldr}. As shown in Figure \ref{fig:tl;dr}, our proposed method improves upon the baseline when using higher temperatures. 

