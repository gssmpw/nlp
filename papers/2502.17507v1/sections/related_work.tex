\section{Related Work}
\label{sec:related-work}

A few recent papers have presented a unifying perspective on DPO-style algorithms. Notably,~\citet{tang2024generalized} presented a generalization of \texttt{DPO} where different supervised learning losses are applied to the difference of implicit rewards, $\big(\hat r(x,y_w) - \hat r(x,y_l)\big)/\beta$.  
%log ratio difference $\log\frac{\pi_{\theta}(y_w|x)}{\piRef(y_w|x)} - \log\frac{\pi_{\theta}(y_l|x)}{\piRef(y_l|x)}$. 
In contrast, we make a clear connection between DPO-style algorithms and classification, and also extend our results to lists and auxiliary information. A second notable example was to generalize from KL to any f-divergence when measuring the discrepancy between the target and reference model~\cite{han2024f}. We also note that the first ingredient of our classification framework, namely $\pi_{\theta}$, was used by~\citet{sharifnassab2024soft} to propose a soft version of \texttt{DPO}.

As mentioned in Section~\ref{sec:DPO-Pitfall}, earlier works studied the undesirable decrease of the winner probability during training. These works explained this phenomenon from different angles, and thus, proposed different losses to address it. Here we provide a brief overview of a number of these results, especially those that we experimentally compare against. \citet{smaug} proposed \texttt{DPOP} which addresses the phenomenon by adding the penalty term $\max\big(0,-\hat r(x,y_w)/\beta\big)$ within the log-sigmoid of the \texttt{DPO} loss~\eqref{eq:DPO}. \texttt{DPOP} can also be viewed as \texttt{DPO} with a modified BT model. In this sense, it has similarities with the \texttt{$\alpha$-DPO} loss~\cite{AIPO}.

\citet{xiao2024caldpo} attributed the undesirable behavior to the contrastive loss of \texttt{DPO} not being {\em scale-calibrated}, i.e.,~ignoring the absolute values of implicit rewards, $\hat r(x,y_w)$ and $\hat r(x,y_l)$. They address this by constraining the implicit rewards to a scale that matches the ground-truth rewards $r(x,y)$. Thus, in their proposed loss, \texttt{Cal-DPO}, they add the square-loss $\big(\hat r(x,y) - r(x,y)\big)^2,\;y\in\{y_w,y_l\}$ to the DPO loss (without $\beta$). Of course, since they do not have access to the ground-truth reward, they replace it with $1/2$ and $-1/2$ for $y_w$ and $y_l$, respectively. A loss very similar to \texttt{Cal-DPO} was proposed by \citet{sppo} and they named it \texttt{SPPO}. It is simply \texttt{Cal-DPO} without the \texttt{DPO} loss.  Finally,~\citet{APO} proposed \texttt{APO}, which offer fine-grained control over the implicit rewards.