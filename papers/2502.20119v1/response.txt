\section{Related work}
A few works in the past have explored stroke-based methods for interactive generations. The work of Ivan Sutherland **Sutherland, "Sketchpad: A Man-Machine Graphical Communication System"** is the first to investigate interactive interfaces for freehand drawing. A pencil rendering technique is presented in **Szeliski, "Image Analysis and Rendering"** through observation models to simulate artists and illustrators. Chen et al. **Chen, Wang, and Williams, "Sketching by Hand with Digital Pencil"** investigated a method for portrait drawing based on composite sketching. An input image is divided into several layers in \cite{li2003feature} to render the intensity of each stacked layer. Though these methods can generate sketches, they fail to offer the drawing process and only produce the final result.

Humans create art through a stroke-by-stroke mechanism rather than pixel-wise operations. Towards this, Fu et al. **Fu, Bagnell, and Szeliski, "DeepReflectiveSketching: A Deep Neural Network for Interactive Sketching"** presented an algorithm that leverages human-drawn line drawings in order to extract stroke order and then animate the sketch. In particular, they proposed a method to estimate drawing order from static line drawings by applying conventional principles of drawing order.

This approach utilizes Hamiltonian graph minimization and an energy function to determine stroke order. While the method is efficient in smaller search spaces, it becomes more complex as the number of strokes increases. The most computationally demanding step involves finding Hamiltonian paths on k-nn graphs, with running times ranging from a few seconds to 2 to 5 minutes. This depends on the value of k, the number of significant lines, and the structure of the k-nn graphs. The method is limited to line art images with clearly defined lines or curves, excluding those with shading, texture, or complex geometric sketches that are difficult to distinguish. The inputs are also assumed to be relatively clean and free of hatching strokes. Also, they establish that the order of detail strokes is less crucial, allowing them to use a simpler strategy rather than the computationally intensive one required for significant lines.

In comparison, our proposed work operates on complex images with a large number of strokes.

Liu et al. addressed the problem of simulating the process of observational drawing, focusing on how people draw lines when sketching a given 3D model. They presented a multiphase drawing framework in which drawing actions are ordered by phases: posture phase, primitive phase, contour phase, and details phase. The lines within these phases are organized at three levels: phase-by-phase, part-by-part, and finally, stroke-by-stroke. To measure the information gained between previously drawn strokes and the target drawing as ground truth, they build a graph similar to **Prim, "Shortest Connection Networks and Some of Their Extensions with Applications to Network Synthesis and Other Fields"** adopting the greedy Prim's minimum spanning tree algorithm. However, this method cannot be extended to complex images with a large number of strokes similar to ____.

Further, an RNN-based method, Sketch-RNN **Ha and Eck, "A Neural Representation of Sketch Drawings"**, is explored on a human-drawn image to construct stroke-based drawings of common objects. It mainly utilizes the pen-state information of the digitally drawn sketch to learn stroke sequences. However, this approach uses only simple hand-drawn objects (QuickDraw) with few strokes and does not scale to real paintings. Moreover, QuickDraw is prone to sampling noise due to highly correlated temporal sequences and suffers from limited capacity as presented in ____.

Zheng **Zheng, Zhang, and Lai, "StrokeNet: A Deep Learning Model for Stroke-Based Chinese Character Writing"** presented a StrokeNet that can generate a sequence of strokes toward Chinese character writing.

However, the generated sequence and their strokes are far from human writing. To progress in AI-assisted creative sketching, Songwei et al. **Song, Li, and Yang, "Part-based Generative Adversarial Networks for Creative Sketching"** introduced Creative Birds and Creative Creatures datasets, where they proposed a part-based GAN to predict suggestions for partial sketches by generating novel part compositions.

Though this method generates compositional parts, it does not go with human creative flow construction. Further, Yonggang et al. \cite{qi2021sketchlattice} introduced an alternative sketch representation based on the lattice structure over a 2D plane towards the sketch manipulation task. All these methods investigate the simple single object categories and do not consider complex natural or art data.

To tackle natural images, various works **Xu, Lu, and Zhang, "Photo-to-Sketch: A Deep Learning Approach for Scene Understanding"** presented a photo-to-sketch method to convert scene to sketch by different levels of abstraction. CLIPasso **Sarkar et al., "CLIPasso: Image-to-Image Translation with Conditional Batch Normalization"** presents a photo-to-sketch method to convert a single object image to a sketch by different levels of abstraction. Here, sketches are derived from a set of B\'{e}zier curves and the number of strokes defines the level of abstraction.

CLIPascene **Sarkar et al., "CLIPascene: Image-to-Image Translation with Conditional Batch Normalization"** is an extension of CLIPasso, where it extends a single object category to a scene. Though these methods produce vector curves, they are very sparse and not suitable for faithful sketching due to limited details.

Tong et al. **Tong, Liu, and Zhang, "Drawing Process for Image-to-Pencil Sketches"** introduced the drawing process for image-to-pencil sketches by drawing one stroke at a time. At first, they established a parameter-controlled pencil stroke generation mechanism based on the pixel-scale statistical results of some real pencil drawings and then exploited a framework to guide stroke arrangement on the canvas. Here, they determine stroke using central pixel gray value, line width, and line length. And, they use an Edge Tangent Flow (ETF) vector field to guide the direction of the stroke.

However, the ETFs do not have inherent sequence order and do not enable the natural drawing process. Further, the representation of pencil lines is one form of sketching and does not support a wide variety of complex data.

Few works \cite{liu2021paint, huang2019learning} explored Reinforcement Learning (RL) based mechanisms where the objective is to predict a set of B\'{e}zier curves through rendering to minimize the difference between the rendered image and the target. Even though these methods can generate high-quality paintings, they generate random curves on canvas and do not hold any inherent sequence.

Specifically, Hung et al. **Hung et al., "Deep Deterministic Policy Gradient for Oil Painting"** employed the Deep Deterministic Policy Gradient (DDPG) algorithm to train a neural agent for oil painting.

However, their Deep Reinforcement Learning (DRL) approach faces limitations due to its requirement for a large number of parameters, constraining the network input size to 128x128 images. This constraint limits the generation of fine-grained details. In contrast, our algorithm does not impose any restrictions on input image size and is capable of generating high-quality, detailed results.