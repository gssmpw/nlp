\section{Experiments}
In this section, we present the data collection and pre-processing, implementation details, and experimental results of the proposed approach. 

\subsection{Data Collection and Implementation Details}
\subsubsection{Dataset}
To the best of our knowledge, there are no existing datasets that demonstrate the stroke-by-stroke evolution of visual art from the initial sketch to the final painting. The closest available dataset for ordered stroke sequences is QuickDraw \cite{ha2017neural}, which consists of simple hand-drawn vector sketches of common objects, created in an online game where players are to draw specific objects within 20 seconds. However, these sketches do not align with our objective of generating the evolution of complex real-world artworks.

For our purposes, we curate a sample dataset from WikiArt \cite{saleh2015large} that includes a diverse range of artworks by renowned artists. In particular, we sampled 500 artworks from various artists to evaluate the effectiveness of the proposed method. Additionally, to investigate the proposed method in diverse settings, we harvested 90 sketch images and 70 RGB images, which include line art, face sketches, and natural images. We randomly sampled face sketches from FS2K-SDE Dataset \cite{dai2023sketch}, line art sketches from \cite{lineartweb}, and natural images from \cite{tong2021sketch}. 

\subsubsection{Implementation details}
To obtain sketches from paintings and natural images, we leverage the line drawing method \cite{chan2022learning} that trained on sampled COCO dataset using CLIP features. Further, to attain a vector image of the input sketch or image, we convert pixel image into vector curves through SVG conversion via the vectorizing tool \cite{vectwebsite}. We impose no restrictions on the dimensions of image inputs or the number of strokes within each image. And, the proximity distance is treated as a hyper-parameter. Here, the number of clusters and the number of strokes per cluster are determined based on this proximity distance. We found that setting proximity distance to approximately 
$\max(Input_{width}, Input_{height})/8$ yields compact clusters that provide a good balance between the number of clusters and the number of strokes per cluster.

\begin{figure}[!t]
    \centering
    \includegraphics[width=\textwidth]{Samples/Fig5_Evolution_WikiArt.pdf}
    \caption{Sketch \& Paint stroke evolution sequences on WikiArt samples.}
    \label{fig:wikiart_results}
\end{figure}

 
\subsection{Results}
We extensively test our algorithm on inputs with varying degrees of complexity and structure. Since there are no formal quantitative measures to gauge the valid stroke sequence evolution on input images, we principally assess the effectiveness of the proposed model qualitatively.

\subsubsection{Results on WikiArt}
 Figure \ref{fig:wikiart_results} shows some examples of stroke-by-stroke ordering on the WikiArt dataset. From this figure, we can observe that the proposed algorithm successfully composes stroke sequence evolution from sketch to paint. Additionally, it effectively handles images with varied resolutions, intricate details, numerous strokes, and diverse color palettes. 


\begin{figure}[!t]

    \centering
    \includegraphics[width=\textwidth]{Samples/Fig6_Evolution_DiverseData.pdf}   
    \caption{Demonstration of stroke evolution on various other input data types such as (A-B) Simple line art \cite{lineartweb} (C-D) Face sketches sampled from FS2K-SDE \cite{dai2023sketch}, (E-F) Natural images from \cite{tong2021sketch}.}
    \label{fig:aaai_results}
\end{figure}

To evaluate the robustness of the proposed method, we further apply it to other forms of data such as line art, face sketches, and natural images. Figure \ref{fig:aaai_results} presents sampled sequences from these diverse input images. The results demonstrate that the predicted stroke sequence order closely mirrors a pragmatic drawing process, regardless of the input type. Specifically, the algorithm produces plausible drawing sequences for less complex images like line art (Figure \ref{fig:aaai_results}, A-B) and face sketches (Figure \ref{fig:aaai_results}, C-D). These predicted sequences follow a logical and intuitive order, closely mirroring the natural progression an artist is likely to adopt. As seen in Figure \ref{fig:aaai_results} (E-F), our method can also effectively interpret natural images that are complex in terms of resolution, detail, and stroke count. From all the results presented above, we can infer that our algorithm can comprehend a variety of input images and produce a pragmatic drawing process. Dynamic examples of drawing evolution, from sketch to painting across various inputs, can be viewed at \cite{youtube_video}. 


\subsubsection{Comparison with other methods}
In this section, we analyze the effectiveness of our algorithm relative to other state-of-the-art methods. Figure \ref{fig:comparison} illustrates the comparative evolution of our method against prominent techniques \cite{tong2021sketch, liu2021paint}. 
For a fair assessment, we qualitatively compare only our image-to-sketch translation with VectorFlow’s \cite{tong2021sketch} image-to-pencil translation, and only our colored paint stroke sequence with Paint Transformer’s \cite{liu2021paint} paint sequence. 
In other words, to ensure fair comparability, we omit color sequencing for the VectorFlow comparison and sketch sequencing for the Paint Transformer comparison. From Figure \ref{fig:comparison}, we can infer that our proposed method can provide systematic sequencing rather than projecting strokes in random order as in \cite{liu2021paint,tong2021sketch}.

\begin{figure}[!t]
    \centering
    \includegraphics[width=\textwidth]{Samples/Fig7_Comparison_RelatedWork.pdf}
    \caption{Qualitative comparison of our method over Vector Flow \cite{tong2021sketch} and Paint Transformer \cite{liu2021paint} (We only include the relevant corresponding portions of generated sequence from our method).} 
    \label{fig:comparison}
\end{figure}

\begin{figure}[!t]
    \centering
    \includegraphics[width=\textwidth]{Samples/Fig8_UserSurvey.pdf}
    \caption{User-survey results for quality evaluation of our method, Paint \& Sketch, in comparison with  Vector Flow (A) \cite{tong2021sketch} and Paint Transformer (B). \cite{liu2021paint} }
    \label{fig:user_survey}
\end{figure}

Additionally, we conducted an initial user study with limited participants (5), each evaluating 5 image generations with VectorFlow \cite{tong2021sketch} and 3 image generations with PaintTransformer \cite{liu2021paint} along with corresponding generations through our method. Participants rated the systems on naturalness, user engagement, stroke quality, and overall experience, with scores ranging from 1 to 10. The survey asked the participants about how accurately the system emulated a pragmatic drawing process, how engaged they felt during the interaction, what the quality of stroke texture and tone was, and what their overall satisfaction was. The bar chart depicting the average user study results is shown in Figure \ref{fig:user_survey}. These results demonstrate that our method provides a more engaging and satisfying user experience than other related methods.

\subsubsection{Limitations} The proposed approach has some known limitations: (1) The method depends upon the quality of line drawing for sketch generation. Hence, it may fail to extract contour lines when the image is dominated by black-intensity regions. (2) There is no mechanism that identifies and learns from the generated sequences that are better aligned to human drawing processes. (3) The paint strokes are coarse and typically not in the form of strokes from any particular drawing medium such as painting brushes.