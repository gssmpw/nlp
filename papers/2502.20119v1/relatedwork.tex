\section{Related work}
A few works in the past have explored stroke-based methods for interactive generations. The work of Ivan Sutherland \cite{sutherland1964sketch} is the first to investigate interactive interfaces for freehand drawing. A pencil rendering technique is presented in \cite{sousa1999observational} through observation models to simulate artists and illustrators. Chen \textit {et al.} \cite{chen2004example} investigated a method for portrait drawing based on composite sketching. An input image is divided into several layers in \cite {li2003feature} to render the intensity of each stacked layer. Though these methods can generate sketches, they fail to offer the drawing process and only produce the final result. 

Humans create art through a stroke-by-stroke mechanism rather than pixel-wise operations. Towards this, Fu \textit{et al.} \cite{fu2011animated} presented an algorithm that leverages human-drawn line drawings in order to extract stroke order and then animate the sketch. In particular, they proposed a method to estimate drawing order from static line drawings by applying conventional principles of drawing order. 
This approach utilizes Hamiltonian graph minimization and an energy function to determine stroke order. While the method is efficient in smaller search spaces, it becomes more complex as the number of strokes increases. The most computationally demanding step involves finding Hamiltonian paths on k-nn graphs, with running times ranging from a few seconds to 2 to 5 minutes. This depends on the value of k, the number of significant lines, and the structure of the k-nn graphs. The method is limited to line art images with clearly defined lines or curves, excluding those with shading, texture, or complex geometric sketches that are difficult to distinguish. The inputs are also assumed to be relatively clean and free of hatching strokes. Also, they establish that the order of detail strokes is less crucial, allowing them to use a simpler strategy rather than the computationally intensive one required for significant lines. 
In comparison, our proposed work operates on complex images with a large number of strokes.

Liu et al. addressed the problem of simulating the process of observational drawing, focusing on how people draw lines when sketching a given 3D model. They presented a multiphase drawing framework in which drawing actions are ordered by phases: posture phase, primitive phase, contour phase, and details phase. The lines within these phases are organized at three levels: phase-by-phase, part-by-part, and finally, stroke-by-stroke. To measure the information gained between previously drawn strokes and the target drawing as ground truth, they build a graph similar to \cite{fu2011animated} adopting the greedy Prim's minimum spanning tree algorithm. However, this method cannot be extended to complex images with a large number of strokes similar to \cite{fu2011animated}. Further, an RNN-based method, Sketch-RNN \cite{ha2017neural}, is explored on a human-drawn image to construct stroke-based drawings of common objects. It mainly utilizes the pen-state information of the digitally drawn sketch to learn stroke sequences. However, this approach uses only simple hand-drawn objects (QuickDraw) with few strokes and does not scale to real paintings. Moreover, QuickDraw is prone to sampling noise due to highly correlated temporal sequences and suffers from limited capacity as presented in \cite{das2020beziersketch}. 

Zheng \cite{zheng2018strokenet} presented a StrokeNet that can generate a sequence of strokes toward Chinese character writing. 
However, the generated sequence and their strokes are far from human writing. To progress in AI-assisted creative sketching, Songwei \textit{et al.} \cite{ge2020creative} introduced Creative Birds and Creative Creatures datasets, where they proposed a part-based GAN to predict suggestions for partial sketches by generating novel part compositions. 
Though this method generates compositional parts, it does not go with human creative flow construction. Further, Yonggang \textit{et al.} \cite {qi2021sketchlattice} introduced an alternative sketch representation based on the lattice structure over a 2D plane towards the sketch manipulation task. All these methods investigate the simple single object categories and do not consider complex natural or art data.

To tackle natural images, various works \cite{vinker2022clipasso,vinker2023clipascene} presented a photo-to-sketch method to convert scene to sketch by different levels of abstraction. CLIPasso \cite{vinker2022clipasso} presents a photo-to-sketch method to convert a single object image to a sketch by different levels of abstraction. Here, sketches are derived from a set of B\'{e}zier curves and the number of strokes defines the level of abstraction. 
CLIPascene \cite{vinker2023clipascene} is an extension of CLIPasso, where it extends a single object category to a scene. Though these methods produce vector curves, they are very sparse and not suitable for faithful sketching due to limited details. Tong et al. \cite{tong2021sketch} introduced the drawing process for image-to-pencil sketches by drawing one stroke at a time.  At first, they established a parameter-controlled pencil stroke generation mechanism based on the pixel-scale statistical results of some real pencil drawings and then exploited a framework to guide stroke arrangement on the canvas. Here, they determine stroke using central pixel gray value, line width, and line length. And, they use an Edge Tangent Flow (ETF) vector field to guide the direction of the stroke. However, the ETFs do not have inherent sequence order and do not enable the natural drawing process. Further, the representation of pencil lines is one form of sketching and does not support a wide variety of complex data.

Few works \cite {liu2021paint, huang2019learning} explored Reinforcement Learning (RL) based mechanisms where the objective is to predict a set of B\'{e}zier curves through rendering to minimize the difference between the rendered image and the target. Even though these methods can generate high-quality paintings, they generate random curves on canvas and do not hold any inherent sequence. Specifically, Hung et al. \cite{huang2019learning} employed the Deep Deterministic Policy Gradient (DDPG) algorithm to train a neural agent for oil painting. 
However, their Deep Reinforcement Learning (DRL) approach faces limitations due to its requirement for a large number of parameters, constraining the network input size to 128x128 images. This constraint limits the generation of fine-grained details. In contrast, our algorithm does not impose any restrictions on input image size and is capable of generating high-quality, detailed results.