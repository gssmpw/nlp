\color{black}

\section{Affinity Entropy}\label{appx:affinity_entropy}

Consider defining a modified Tsallis entropy $H^p_a$ with temperature parameter $p \in (0, 1]$ as:
\begin{align}
    H^p_a(\vx) &= \frac{1}{p} \Big[ 1 - \vz^\top \vz \Big] = \frac{1}{p} \Big[ 1 - \sum_i (U^{(p)}_i \vx)^{p+1} \Big] \label{eqn:modtsallis}
\end{align}
where $\vz = (U^{(p)} \vx)^{\frac{p+1}{2}}$.
Note that this definition recovers the standard definition of Tsallis entropy when $U^{(p)}$ is the identity matrix.

\begin{remark}
$U^{(p)}_{ij} \ge 0$ for all entries for $H^p_a$ to be real-valued.
\end{remark}
$U^{(p)}_{ij}$ must be non-negative for every $i, j$, otherwise, there exists $\vx = \ve_j$ where $\ve_j$ is a standard-basis vector such that $U^{(p)}_i \vx < 0$ and $(U^{(p)}_i \vx)^{p+1}$ is not real for $p \in (0,1)$.

\begin{remark}
The $(p+1)$-norm of each column of $U^{(p)}$ must be less than or equal to $1$ for $H^p_a$ to be non-negative for any $\vx \in \Delta$.
\end{remark}

We need $\vz^\top \vz \le 1$ for $p \in (0,1]$ and any $\vx \in \Delta$. Equivalently, we require $(\vz^\top \vz)^{\frac{1}{p+1}} \le 1$ for $p \in (0,1]$.

Note $(\vz^\top \vz)^{\frac{1}{p+1}} = \big( \sum_i (U^{(p)}_i \vx)^{p+1} \big)^{\frac{1}{p+1}} = ||U^{(p)} \vx||_{p+1}$. Therefore, we require
\begin{align}
    1 &\ge \sup_{\vx \in \Delta} ||U^{(p)} \vx||_{p+1}
    \\ &= \sup_{||\vx||_1 = 1} ||U^{(p)} \vx||_{p+1} \quad \text{for} \quad U^{(p)} \ge 0
    \\ &= ||U^{(p)}||_{1,p+1}
    \\ &= \max_{j=1} ||U^{(p)}_{\cdot,j}||_{p+1} \quad \text{by~\citet{drakakis2009calculation}}.
\end{align}

\begin{remark}
Among all admissible $U^{(p)}$, defining $U^{(p)}$ such that its columns have exactly unit $(p+1)$-norm achieves $\min_{U^{(p)}} \min_{\vx \in \Delta} H^a_p(\vx)$.
\end{remark}

This follows from the previous remark and is desireable for the sake of defining a ``tight'' definition of entropy. Intuitively, by the conditions set thus far, $U^{(p)} = \mathbf{0}$ is admissible. Yet, this gives a loose definition of entropy where $H^p_a = \sfrac{1}{p}$. It turns out that this intuition is required in the limit as $p \rightarrow 0$.

\begin{remark}
$U^{(p)}$ must be precisely column stochastic for $H^p_a$ to remain finite in the limit of $p \rightarrow 0$.
\end{remark}
In the limit $p \rightarrow 0$, the denominator of $H^p_a$ goes to zero, therefore, by L'Hôpital's rule, the numerator must as well. The numerator goes to $z^\top z = \sum_i U^{(p)}_i x = \mathbf{1}^\top U^{(p)} x$. Therefore,
\begin{align}
    \forall x \in \Delta^{d-1} \quad 1 - \mathbf{1}^\top U^{(p)} x  &= 0.
\end{align}
Finite distributions only obey a single equality constraint, that is $x^\top \mathbf{1} = 1$, therefore it must be the case that $\mathbf{1}^\top U^{(p)} = \mathbf{1}^\top$, i.e., $U^{(p)}$ is column stochastic.

\begin{remark}
$H^p_a$ is concave in $\vx$.
\end{remark}
Let $y_i = U^{(p)}_i \vx$. Then each element of the sum, $y_i^{p+1}$ is a convex function in $y_i$, which itself is a linear transformation on $\vx$. Therefore, $\sum_i (U^{(p)}_i \vx)^{p+1}$ is convex in $\vx$. Hence $H^p_a$ is concave in $\vx$.

\begin{remark}
The gradients $\nabla_{\vx} H^p_a$ are well-defined.
\end{remark}

Recall~\eqref{eqn:modtsallis}, then:
\begin{align}
    \frac{\partial H^p_a}{\partial x_j} &= -\frac{p+1}{p} \sum_i (U^{(p)}_i \vx)^p U^{(p)}_{ij}
    \\ \nabla_{\vx} H^p_a &= -\frac{p+1}{p} (U^{(p)})^\top (U^{(p)} \vx)^p
\end{align}
which is well-defined for any choice of $U^{(p)}_{ij} \ge 0$ for all $i, j$.

\begin{remark}
$H^p_a$ is well-defined in the limit as $p \rightarrow 0$, i.e., Shannon \emph{affinity} entropy is well-defined.
\end{remark}

It is known that Shannon entropy can be recovered from Tsallis entropy in the limit as $p \rightarrow 0$. We repeat that derivation here and use L'Hôpital's rule. The derivative of the denominator is $1$, hence we find the limit is given by the finite derivative of the numerator:
\begin{align}
    \frac{d [pH^p_a]}{dp} &= -\frac{d}{dp} \Big[ \sum_i y_i^{p+1} \Big]
    \\ &= -\frac{d}{dp} \Big[ \sum_i e^{(p+1)\log(y_i)} \Big]
    \\ &= -\sum_i \big(\log(y_i) + (p+1) \frac{1}{y_i} \frac{d y_i}{dp} \big) e^{(p+1)\log(y_i)}.
\end{align}

In the limit $p \rightarrow 0$, the derivative evaluates to
\begin{align}
    \frac{d [pH^p_a]}{dp} &= -\sum_i \Big[ e^{(p+1)\log(y_i)} \log(y_i) \Big]\Big\vert_{p=0} - (p+1) \sum_i \Big[ \frac{1}{y_i} \frac{d y_i}{dp} e^{(p+1)\log(y_i)} \Big]\Big\vert_{p=0}
    \\ &= -\sum_i y_i \log(y_i) - \sum_i \frac{d y_i}{dp}\Big\vert_{p=0}
    \\ &= S(y) - \sum_i \frac{d y_i}{dp}\Big\vert_{p=0}.
\end{align}

\begin{remark}
Let $K$ be a similarity matrix between actions with non-negative entries with positive column-sums. Then $U^{(p)} = K \texttt{diag}\big( 1 / (\mathbf{1}^\top K^{p+1})^{1/(p+1)} \big)$ satisfies the conditions stated above for $U^{(p)}$.
\end{remark}

\begin{remark}
Under the above choice of $U^{(p)}$, Shannon \emph{affinity} entropy $S_a = H^{p\rightarrow0}_a$ can be derived as:
\begin{align}
    S_a(\vx) &= S(U^{(0)}\vx) - \sum_j \Big[ \log(\sum_{i} K_{ij}) - \sum_{i} U^{(0)}_{ij} \log(K_{ij}) \Big] x_j.
\end{align}
\end{remark}

The necessary $y_i$ term can be rewritten and its derivative (evaluated at $p=0$) can be derived as follows:

\begin{align}
    y_i &= U^{(p)}_i \vx = \sum_j \frac{K_{ij}}{(\sum_{i'} K_{i'j}^{p+1})^{\frac{1}{p+1}}} x_j
    \\ &= \sum_j K_{ij} x_j (\sum_{i'} K_{i'j}^{p+1})^{-\frac{1}{p+1}}
    \\ &= \sum_j K_{ij} x_j e^{-\frac{1}{p+1} \log(\sum_{i'} K_{i'j}^{p+1})}
    \\ &= \sum_j K_{ij} x_j e^{-\frac{1}{p+1} \log\big(\sum_{i'} e^{(p+1) \log(K_{i'j})}\big)}
    \\ \frac{d y_i}{dp} &= \sum_j K_{ij} x_j e^{-\frac{1}{p+1} \log\big(\sum_{i'} e^{(p+1) \log(K_{i'j})}\big)} \Big[ \frac{1}{(p+1)^2} \log\big(\sum_{i'} e^{(p+1) \log(K_{i'j})}\big)
    \\ &- \frac{1}{p+1} \frac{1}{\sum_{i'} e^{(p+1) \log(K_{i'j})}} \sum_{i'} \log(K_{i'j}) e^{(p+1) \log(K_{i'j})} \Big]
    \\ &= \sum_j K_{ij} x_j (\sum_{i'} K_{i'j}^{p+1})^{-\frac{1}{p+1}} \Big[ \frac{1}{(p+1)^2} \log(\sum_{i'} K_{i'j}^{p+1})
    \\ &- \frac{1}{p+1} \frac{1}{\sum_{i'} K_{i'j}^{p+1}} \sum_{i'} \log(K_{i'j}) K_{i'j}^{p+1} \Big]
    \\ &= \sum_j \Big[ \frac{1}{(p+1)^2} \log(\sum_{i'} K_{i'j}^{p+1}) - \frac{1}{p+1} \sum_{i'} (U^{(p)}_{i'j})^{p+1} \log(K_{i'j}) \Big] U^{(p)}_{ij} x_j
    \\ \frac{d y_i}{dp}\Big\vert_{p=0} &= \sum_j \Big[ \log(\sum_{i'} K_{i'j}) - \sum_{i'} U^{(0)}_{i'j} \log(K_{i'j}) \Big] U^{(0)}_{ij} x_j
\end{align}
where we define $K_{ij} \log(K_{ij}) = 0$ if $K_{ij} = 0$ (which implies $(U^{(p)}_{ij})^{p+1} \log(K_{ij}) = 0$ if $K_{ij} = 0$.

Plugging this back into the second term in the formula for Shannon \emph{affinity} entropy, we find

\begin{align}
    \sum_i \frac{d y_i}{dp}\Big\vert_{p=0} &= \sum_i \sum_j \Big[ \log(\sum_{i'} K_{i'j}) - \sum_{i'} U^{(0)}_{i'j} \log(K_{i'j}) \Big] U^{(0)}_{ij} x_j
    \\ &= \sum_j \Big[ \log(\sum_{i'} K_{i'j}) - \sum_{i'} U^{(0)}_{i'j} \log(K_{i'j}) \Big] x_j \sum_i U^{(0)}_{ij}
    \\ &= \sum_j \Big[ \log(\sum_{i'} K_{i'j}) - \sum_{i'} U^{(0)}_{i'j} \log(K_{i'j}) \Big] x_j
\end{align}
completing the claim.

\begin{remark}
In the case of duplicate strategies (clones), the maximizers of $H^p_a$ form precisely the set of distributions which arbitrarily distribute a mass of $\frac{1}{C}$ across each of the $C$ sets of clones.
\end{remark}

Consider the case of exact clones, i.e., $K$ is block diagonal (w.l.o.g.) with blocks of ones. Let there be $C$ clone groups each of size $n_c$ for $c \in \{1, \ldots, C\}$. Let $c(i)$ map an action $i$ to its clone set. In this case, it can be shown that $U^{(p)}_{ij} = n_{c(i)}^{-\frac{1}{p+1}}$ if $c(i)=c(j)$, otherwise $U^{(p)}_{ij} = 0$. Note that the gradient of entropy w.r.t. $\boldsymbol{x}$ must be proportional to the ones vector for $\boldsymbol{x}$ to be a maximizer in the interior of the simplex. Let $\boldsymbol{x} = [\frac{1}{C} \boldsymbol{x}_{1}, \ldots, \frac{1}{C} \boldsymbol{x}_{C}]$ with each $\boldsymbol{x}_{c} \in \mathbb{R}^{n_c}$ w.l.o.g. We will show that the set of maximizers of $H^p_a$ is necessarily the set of $\boldsymbol{x}$ where each $\boldsymbol{x}_{c} \in \Delta^{n_c - 1}$. For $\boldsymbol{x}$ to be a maximizer, the gradient must be equal to the ones vector multiplied by a scalar $-d \in \mathbb{R}$:

\begin{align}
    \forall j \,\, \frac{\partial H^p_a(\boldsymbol{x})}{\partial x_j} &= -\frac{p+1}{p} \sum_i (U^{(p)}_i \boldsymbol{x})^p U^{(p)}_{ij}
    \\ &= -\frac{p+1}{p} \sum_i (\sum_k U^{(p)}_{ik} x_k)^p U^{(p)}_{ij}
    \\ &= -\frac{p+1}{p} \sum_i \big( \frac{1}{C} n_{c(i)}^{-\frac{1}{p+1}} \mathbf{1}^\top \boldsymbol{x}_{c(i)} \big)^p U^{(p)}_{ij}
    \\ &= -\frac{p+1}{p} n_{c(j)} \big( \frac{1}{C} n_{c(j)}^{-\frac{1}{p+1}} \mathbf{1}^\top \boldsymbol{x}_{c(j)} \big)^p n_{c(j)}^{-\frac{1}{p+1}}
    \\ &= -\frac{p+1}{p} n_{c(j)} n_{c(j)}^{-\frac{p+1}{p+1}} \big(\frac{1}{C} \mathbf{1}^\top \boldsymbol{x}_{c(j)} \big)^p
    \\ &= -\frac{p+1}{p} \big(\frac{1}{C} \mathbf{1}^\top \boldsymbol{x}_{c(j)} \big)^p = -d. \label{eqn:maxentgrad}
\end{align}

We also require $\boldsymbol{x} \in \Delta$, which implies
\begin{align}
    x_j &\ge 0 \implies x_{c(j)} \ge \mathbf{0}
    \\ 1 &= \sum_j x_j = \sum_c \frac{1}{C} \mathbf{1}_{n_c}^\top \boldsymbol{x}_{c}
    \\ &= C\Big(\frac{dp}{p+1}\Big)^{1/p} = d^{1/p}C\Big(\frac{p}{p+1}\Big)^{1/p} \implies d = C^{-p} \Big(\frac{p+1}{p}\Big).
\end{align}

Finally, we know from~\eqref{eqn:maxentgrad}
\begin{align}
    (\frac{1}{C} \mathbf{1}^\top \boldsymbol{x}_{c(j)})^p &= \frac{dp}{p+1} = C^{-p}
    \\ \implies \mathbf{1}^\top \boldsymbol{x}_{c(j)} &= 1
\end{align}
proving the claim.

\begin{remark}
In the case of duplicate strategies (clones), the maximizers of $H^p_a$ achieve an entropy value which is equal to the Tsallis entropy of the system with clones removed.
\end{remark}

If we evaluate the max entropy distribution we find
\begin{align}
    H^p_a(\boldsymbol{x}) &= \frac{1}{p} \Big[ 1 - \sum_i (U^{(p)}_i \boldsymbol{x})^{p+1} \Big]
    \\ &= \frac{1}{p} \Big[ 1 - \sum_i \big( \frac{1}{C} n_{c(i)}^{-\frac{1}{p+1}} \mathbf{1}^\top \boldsymbol{x}_{c(i)} \big)^{p+1} \Big]
    \\ &= \frac{1}{p} \Big[ 1 - \sum_c n_c \big( \frac{1}{C} n_{c}^{-\frac{1}{p+1}} \mathbf{1}^\top \boldsymbol{x}_{c} \big)^{p+1} \Big]
    \\ &= \frac{1}{p} \Big[ 1 - \sum_c n_c \big( \frac{1}{C} n_{c}^{-\frac{1}{p+1}} \big)^{p+1} \Big]
    \\ &= \frac{1}{p} \Big[ 1 - \sum_c n_c n_c^{-1} \big( \frac{1}{C} \big)^{p+1} \Big]
    \\ &= \frac{1}{p} \Big[ 1 - \sum_c \big( \frac{1}{C} \big)^{p+1} \Big]
\end{align}
which is precisely the Tsallis entropy of the uniform distribution over $C$ distinct clones.

\color{black}
