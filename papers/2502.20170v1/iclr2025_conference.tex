
\documentclass{article} %
\usepackage{sgamex}
\usepackage{iclr2025_conference,times}


\input{math_commands}

\usepackage{hyperref}
\usepackage{url}
\usepackage{graphicx}
\usepackage{svg}
\usepackage{enumitem}

\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{setspace}

\usepackage{rotating}

\usepackage{todonotes} %
\newcommand{\liusiqi}[1]{\todo[inline,color=red!20!white]{\textbf{Siqi:} #1}}
\newcommand{\imgemp}[1]{\todo[inline,color=blue!20!white]{\textbf{Ian:} #1}}
\newcommand{\marris}[1]{\todo[inline,color=orange!20!white]{\textbf{Luke:} #1}}
\newcommand{\georgios}[1]{\todo[inline,color=orange!20!white]{\textbf{Luke:} #1}}


\newcommand{\koh}{{\tt king-of-the-hill}\xspace}
\newcommand{\geminipro}{{\tt gemini-1.5-pro-api-0514}\xspace}
\newcommand{\geminiflash}{{\tt gemini-1.5-flash-api-0514}\xspace}
\newcommand{\gpto}{{\tt gpt-4o-2024-05-13}\xspace}
\newcommand{\gptt}{{\tt gpt-4-turbo-2024-04-09}\xspace}
\newcommand{\arenahard}{{\tt arena-hard-v0.1}\xspace}
\newcommand{\npgs}{$N$-player general-sum\xspace}
\newcommand{\tpzs}{$2$-player zero-sum\xspace}


\title{Re-evaluating Open-ended Evaluation of \\ Large Language Models}


\author{Siqi Liu\thanks{Equal contribution.}\,\,, Ian Gemp\footnotemark[1]\,\,, Luke Marris, Georgios Piliouras, Nicolas Heess, Marc Lanctot \\
Google DeepMind\\
London, UK \\
\texttt{\{liusiqi,imgemp,marris,gpil,heess,lanctot\}@google.com}
}


\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy %
\begin{document}


\maketitle

\begin{abstract}
Evaluation has traditionally focused on ranking candidates for a specific skill. Modern generalist models, such as Large Language Models (LLMs), decidedly outpace this paradigm. Open-ended evaluation systems, where candidate models are compared on user-submitted prompts, have emerged as a popular solution. Despite their many advantages, we show that the current Elo-based rating systems can be susceptible to and even reinforce biases in data, intentional or accidental, due to their sensitivity to redundancies. To address this issue, we propose evaluation as a 3-player game, and introduce novel game-theoretic solution concepts to ensure robustness to redundancy. We show that our method leads to intuitive ratings and provide insights into the competitive landscape of LLM development.
\end{abstract}

\section{Introduction}

We can only improve what we measure, yet measuring the performance of Large Language Models (LLMs) has become an elusive endeavor owing to their breadth and depth of capabilities. Real-world benchmarks are costly to curate, increasingly requiring feedback from human domain experts \citep{hendrycks2021measuring, rein2023gpqa}. Synthetic benchmarks can help, but their relevance to real-world performance is less clear \citep{zhang-etal-2024-bench, hsieh2024ruler}. An even more vexing challenge of static benchmarks is that of test set contamination, a phenomenon difficult to prevent despite efforts \citep{golchin2024time,balloccu2024leak,palavalli2024taxonomydatacontaminationlarge}. Enumerating skills of interests with narrowly defined static benchmarks seems to be an uphill battle from the outset, as frontier models become generally capable.

An emerging trend in LLM evaluation is therefore to rely on open-ended evaluation systems, a notable example being the LMSYS Chatbot Arena \citep{chiang2024chatbot}. 
In such a system, users submit prompts of interest, with each model assigned an Elo score \citep{elo1978rating} based on how they compare to each other on all prompts. In contrast to static benchmarks, this open-ended approach enjoys liveness, diversity and scale, lending itself to become an important reference in LLM development. Despite an intuitive sense of progress, issues around redundancy, bias and quality of crowdsourced data have been raised~\citep{chiang2024chatbot, ahuja2023mega, arenahard2024}. Several recent studies reverted back to centralized curation for quality~\citep{alpaca, lee2024holistic, white2024livebench}. Increasing commercial efforts have been invested in private and proprietary evaluation too.




Perhaps this tension between quality and open-endedness is to be expected in LLM evaluation. Biases, redundancies and quality issues in the prompt distribution can affect Elo ratings, as they reflect performance {\em on average}.
This along with other identified deficiencies of the Elo system~\citep{balduzzi2018re, bertrand2023limitations, lanctot2023evaluating} raise crucial questions for LLM development: how does an Elo-based open-ended evaluation system affect model development today, and how can we mitigate its drawbacks, if any, in the future? In this paper, we provide an empirical simulation-based investigation of the former and lean on game theory for a solution to the latter.


The connection between evaluation and game theory needs unpacking. Consider a set of agents and a set of tasks, a naive approach to evaluation would rank agents by their average performance over tasks, propagating biases and redundancies in the task set. A game-theoretic approach~\citep{balduzzi2018re}, would be to consider evaluation as an {\em agent-vs-task} game where the {\em agent} ({\em task}) player chooses one of its agents (tasks) and is rewarded (penalized) by the agent's performance on the task. This game-theoretic perspective accomplishes two goals simultaneously. First, it lets the evaluation system designers express their goals in players' objectives: here, \citet{balduzzi2018re} evaluates agents under {\em adversarial} task selection. Second, a game-theoretic equilibrium decides which actions are played during evaluation: quality and redundancies in players' action sets do not matter. It is in this sense that game theory is well suited for evaluation systems that are open-ended.

Applying game theory to LLM evaluation however has its own challenges. Indeed, the decision of \citet{balduzzi2018re} in comparing agents under {\em adversarial} task selection was guided by theoretical benefits. In \tpzs games, approximating a Nash equilibrium (NE, \cite{nash1950non}) is computationally tractable. NEs are also interchangeable in this setting as playing any NE guarantees zero exploitability. Beyond this setting, both benefits are lost: approximating NEs is computationally hard in the worst case \citep{daskalakis2006note} and despite recent progress important challenges remain \citep{gemp2022sample, gemp2024approximating}.  Equilibrium selection in this generalised setting remains a long-standing challenge too \citep{harsanyi1988general, rinott2000number}. For instance, driving on either side of the road is an equilibrium, but it is unclear which equilibrium should be used for evaluation. Past attempts at game-theoretic evaluation have therefore been restricted to the \tpzs settings when LLM evaluation calls for at least 3 players ({e.g., \em model-vs-model-vs-prompt}). 



In this paper, we make several contributions that lead up to our equilibrium rating framework:
\begin{enumerate}
    \item We show, via a simulated example (Section~\ref{sec:illustrative}), the risk of models specializing in a few skills, at the expense of others, as they maximise their Elo ratings. Similarly, popular practice in prompt selection further reinforces this trend;
    \item We introduce novel equilibrium solution concepts for \npgs games that are unique and clone-invariant, a pre-requisite for our equilibrium rating method (Section~\ref{sec:method});%
    \item We show our method scales to a real-world LLM evaluation dataset (Section~\ref{sec:exact_clone_invariance}) and provide ratings that are invariant to redundancy and correspond to our intuition in the sense of risk-dominance \citep{harsanyi1988general}, with empirical evidence (Appendix~\ref{app:fragile_equilibria});
    \item We provide examples of analyzing these equilibrium structures of the game, drawing insights into the competitive landscape of LLM evaluation (Section~\ref{sec:interpreting}).
\end{enumerate}





\subsection{Elo rating improvement path: a simulated example} \label{sec:illustrative}

With models continually improving their Elo ratings in systems such as LMSYS Chatbot Arena \citep{li2024crowdsourced}, it is worth asking if higher Elo scores translate to meaningful progress across skills of interest. This is difficult to answer from real-world data: we cannot replicate LLM development at scale nor can we disentangle factors driving model development besides maximizing leaderboard ratings. A synthetic example can provide insights in a controlled setting.


Consider $S$ orthogonal skills of interests, $M$ models and $P$ prompts with each prompt a probability vector $\vp \in \Delta^S$ over the skills and each model a vector $\vm \in {\sR}^S_+$, representing its competencies in each skill.
We can then define the utility of selecting model $\vm_i$ when compared to model $\vm_j$ on prompt $\vp_k$, as $u_m(\vp_k, \vm_i, \vm_j) = \vp_k^T (\vm_i - \vm_j)$ with $i, j \in [M]$ and $k \in [K]$.
A less common but equally valid question is what should be the utility, if any, for selecting a prompt.
We follow a similar definition as \citet{arenahard2024} and define the utility in choosing prompt $\vp_k$ as $u_p(\vp_k, \vm_i, \vm_j) = |u_m(\vp_k, \vm_i, \vm_j)|$. The {\em separability} of a prompt is then $\frac{1}{M^2} \sum_{ij}{u_p(\vp_k, \vm_i, \vm_j)}$, consistent with the prompt selection criterion used in offline benchmarks such as \arenahard.

We now observe how this system evolves with rating-maximizing players. Consider two settings: a) the ``initial prompts'' setting where the set of prompts is fixed but the set of models expands; and b) the ``additional prompts'' setting where prompt and model players alternate to introduce new prompts and models. We use a simple evolutionary process for our simulation (see Appendix~\ref{app:simulated_model_prompt_improvement} for pseudocode). Let $P_t$ and $M_t$ be the number of prompts and models at iteration $t$ and $P_0$, $M_0$ the number of initial prompts and models sampled from $\mathrm{Dirichlet}(\vone_{1:S})$. We introduce a model at each iteration which is a sum of improvement vectors sampled from $\mathrm{Dirichlet}(\vone_{1:S})$, such that the new addition receives the highest rating according the rating method used (i.e. Elo or our equilibrium-based method). In the ``additional prompts'' setting, a best-of-64 prompt is added at each iteration, selected by their separability when Elo ratings are used, and by their equilibrium ratings otherwise.

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{figures/elo_vs_equilibrium_ratings.pdf}
    \caption{\figleft We simulate the effect of the rating method on model development with users submitting highly rated models (and prompts) iteratively. \figcenter We show how model and prompt skill entropy evolves under different rating methods over 32 trials. \figright We show an example sequence of models and prompts maximising their respective ratings. Darker indicates higher value.}
    \label{fig:elo_vs_equilibrium}
\end{figure}







Figure~\ref{fig:elo_vs_equilibrium}~\figcenter shows our findings. Let $H(\bar{\vp}_t)$, $H(\bar{\vm}_t)$ be the prompt and model skill entropy at iteration $t$ with $\bar{\vp}_t = \frac{1}{P_t} \sum^{P_t}_i \vp_i$ and $\bar{\vm}_t = \frac{1}{M_t} \sum^{M_t}_i \vm_i$ and $H$ the Shannon entropy. The Elo rating method leads to a consistent decline in skill entropy: the sequence of models improve along specific skill dimensions that are over-represented in the fixed set of initial prompts (dashed). Adding prompts with high separability further reinforces this trend in both model and prompt skill entropy (solid). 
We offer an intuitive explanation. Improvement on the Elo ratings or the separability metric reflects improvements against the {\em average}. At iteration $t$, the expected utility to model $\vm_i$ is given by $u_m(\bar{\vp}_t, \vm_i, \bar{\vm}_t)$ with its gradient defined by $\bar{\vp}_t$ --- improving on the most prevalent skill in $\bar{\vp}_t$ therefore leads to the steepest ascent in model utility.
Similarly, the gradient for a prompt vector $\vp_k$ is defined by the absolute deviation of the model vectors along each skill dimension $\frac{1}{M_t} \sum^t_i | \vm_i - \bar{\vm}_t |$. Prompts that target the skill dimension with the highest ``spread'' averaged across all model pairs are therefore the most highly rated.
Figure~\ref{fig:elo_vs_equilibrium}~\figright illustrates this phenomenon from a single trial. The Elo-maximising sequence of models specialises in skill $1$ due to prompt redundancy while the separability-maximising sequence of prompts remains focused on skill 1, at the expense of others.

The underlying challenge, one that we address, is to propose a practical rating method that compares models and prompts in a way that is intuitive and robust to redundancies. Figure~\ref{fig:elo_vs_equilibrium}~\figcenter suggests that maximising equilibrium ratings preserves skill entropy. Indeed, Figure~\ref{fig:elo_vs_equilibrium} \figright shows models focusing on different skills across iterations. As prompts are no longer measured against the {\em average} model pairs, they also remain diverse. In both cases, ratings are computed at a game-theoretic equilibrium distribution, instead of the uniform. We now present our equilibrium rating framework.





\section{Background}
\label{sec:background}

\paragraph{Normal-form game}

A normal-form game is a tuple $(N, \gA, u)$ where $N$ is a finite set of players $N = \{1, \ldots, n\}$ indexed by $i$, a tuple of strategy (action) sets $\gA = (\gA_1, \ldots, \gA_n)$, and a tuple of utility functions $u = (u_1, \ldots, u_n)$ with $\gA_i$ and $u_i: \gA \rightarrow \mathbb{R}$ being player $i$'s strategy set and utility respectively. Let $\va \in \gA = (a_1, \ldots, a_n)$ with $a_i \in \gA_i$ for all $i$ denote a strategy \emph{profile}. We allow strategy profiles to be selected randomly according to a distribution $\vx \in \Delta(\gA)$ over {\it joint} actions. Let $x_i$ denote the marginal distribution over player $i$'s strategy set $\gA_i$, i.e., $\vx$ with all players $j \ne i$ marginalized out. Likewise, let $x_{-i}$ denote the distribution with player $i$ marginalized out. We call $\vx$ a \emph{pure} strategy if it places all mass on a single action profile and \emph{mixed} otherwise. Each player's utility function is naturally extended to randomized strategy profiles by considering its expected value $u_i(\vx) = \E_{\va \sim \vx}[u_i(\va)]$. Similarly, let $u_i(x_i, x_{-i}) = \E_{\substack{a_i \sim x_i \\ a_{-i} \sim x_{-i}}}[u_i(\va)]$.


\paragraph{Coarse Correlated Equilibrium (CCE) and Nash Equilibrium (NE)}
An equilibrium is a strategy profile $\vx$ from which no player has an incentive to unilaterally deviate.
Define player $i$'s incentive to deviate to $x'_i \in \Delta(\gA_i)$ unilaterally as $\text{regret}_i(x'_i, \vx) = u_i(x'_i, x_{-i}) - u_i(\vx)$, where $\Delta(\gA_i)$ is the simplex over $\gA_i$.
Then, player $i$'s maximum regret for deviating from $\vx$, is defined as:
\begin{equation} \label{eq:deviation_gain}
    \text{regret}_i(\vx) = \max_{x'_i \in \Delta(\gA_i)} \Big[ \text{regret}_i(x'_i, \vx) \Big] = \max_{x'_i \in \Delta(\gA_i)} \Big[ u_i(x'_i, x_{-i}) \Big] - u_i(\vx).
\end{equation}

The profile $\vx$ is an approximate Coarse Correlated Equilibrium~\citep{aumann1974subjectivity, aumann1987correlated} ($\epsilon$-CCE) iff $\forall i, \text{regret}_i(\vx) \le \epsilon$. If $\vx$ can be factorized into player marginals such that players cannot correlate, i.e., $\vx = \bigtimes_{i=1}^N x_i$, then $\vx$ is also an $\epsilon$-NE. NEs are a subset of CCEs.


\paragraph{Equilibrium Selection}
Games can have many equilibria. Additional criteria are often introduced to make their selection unique. The set of CCEs is always convex, and so any strictly convex objective function such as negative Shannon entropy can used to select a unique equilibrium.

In contrast, the set of NEs need not be convex, however, several solutions have been proposed to solve for unique Nash equilibria in general-sum games~\citep{harsanyi1988general}. The LLE was originally defined by~\citet{mckelvey1995quantal} along with their introduction of quantal response (logit) equilibria (QREs) which satisfy the following fixed point equation for all players $i \in N$:
\begin{align}
    x_i &= \softmax(\frac{1}{\tau} \nabla_{x_i} u_i) \label{eqn:qre_fp}
\end{align}
where $\nabla_{x_i} u_i$ is the gradient of $u_i$ w.r.t. $x_i$. QREs are defined by a temperature parameter $\tau$ and can be interpreted as the Nash equilibria of a game with payoffs perturbed by Gumbel$(0, \tau)$ noise. Computing the LLE involves tracing a continuum of QREs, starting at temperature $\tau = \infty$ (corresponding to the uniform strategy profile) and ending at the LLE in the limit of $\tau = 0$. The LLE is unique in all games except a $0$-measure set~\citep{mckelvey1995quantal,goeree2003risk}. Another reason to solve for an LLE is that it falls into the family of homotopy methods \citep{herings2010homotopy}, which were shown to select {\em risk-dominant} equilibria in some general settings, a Nobel prize winning result of \citet{harsanyi1988general}. Empirically, LLEs have also been shown to approximate human play in games \citep{mckelvey1995quantal,goeree2003risk}.

\section{Method} \label{sec:method}

We now describe our rating method in terms of gamification, equilibrium solving and its selection. In gamification, we endow prompt and model players with utility functions, partly inspired by prior works, such that actions played at an equilibrium reflect our intuition. We note that our specific gamification defines an \npgs game where equilibrium solving and selection requires more careful consideration. For equilibrium solving, we build on existing methods for approximating NEs and CCEs, reformulated to accommodate entropy-based techniques that select unique equilibria and explain why ratings derived from these equilibria remain vulnerable to manipulation in the face of redundant actions. We then propose a family of algorithms based on a novel kernelized entropy that select unique equilibria yet are also robust to redundant actions. Finally, for a given equilibrium solution $\vx$, we define the rating of an action $a_i$ to be $\text{regret}_i(a_i, \vx)$.






\subsection{Gamification: Evaluation via a Game Between Models and Prompts}
\label{sec:gamification}


We study a 3-player general-sum game in our experiments. Consider a {\em prompt} player with $a_p \in \gA_p$ the set of prompts, a {\em king} player and a {\em rebel} player each with actions $a_m \in \gA_m$ the set of models. Let $u_k(a_p, a_m, a'_m) \in \{-1, -\sfrac{1}{2}, 0, +\sfrac{1}{2}, +1\}$ be the utility function to the king player representing a preference towards king model response $a_m$ over the rebel model response $a'_m$ on a prompt $a_p$. The prompt player is rewarded for separating the models, with $u_p(a_p, a_m, a'_m) = |u_k(a_p, a_m, a'_m)|$. The rebel player receives $u_r(a_p, a_m, a'_m) = -u_k(a_p, a_m, a'_m)$ except for when $a_m = a'_m$ in which case $u_r(a_p, a_m, a'_m) = -1$. This asymmetry discourages the same model being played by both model players deterministically with a prompt player indifferent over its actions. We refer to this game as \koh as it favours the king player, leaving the rebel player to mount its best resistance without relying on some of the best models that the king player may choose. We refer to the king player ratings as the model ratings in our results. %

Given a collection of prompts and models, the utility function can be tabulated with $|\gA_p| \times |\gA_m|^2$ pairwise preference ratings. 
We query a \geminipro judge for preference ratings similar to \citet{zheng2023judging, verga2024replacing, dubois2024length, dubois2024alpacafarm, chiang2023can, liu2023g}. We caveat that our results could therefore suffer from {\em self-preference} \citep{panickssery2024llm} and should {\em not} be viewed as an objective assessment of frontier LLMs.


\subsection{Equilibrium Solving}\label{sec:eq_solving}

For an instance of the evaluation game, we can compute different equilibrium solutions $\vx$ which then define ratings. Here we present two options as they are unique, scalable and lead to intuitive, invariant ratings when combined with a selection criteria that we describe in Section~\ref{sec:selection}.

\paragraph{Nash Equilibrium (NE)}

While LLE computation~\citep{turocy2005dynamic} is typically formulated as solving a differential equation that evolves the temperature $\tau$ towards $0$ while obeying the logit constraint in Equation~\eqref{eqn:qre_fp}, i.e., $x_i = \softmax(\frac{1}{\tau} \nabla_{x_i} u_i)$ for all $i$, this is also equivalent to satisfying the constraint $x_i = \argmax_{z_i \in \Delta} u_i(z_i, x_{-i}) + \tau S(x_i)$ where $S(x_i)$ is the Shannon entropy of $x_i$. In this work, we choose another condition
\begin{equation}
    x_i = \argmax_{z_i \in \Delta} \Big\{ u_i^{\tau}(z_i, x_{-i}) \myeq u_i(z_i, x_{-i}) - \tau \KL(z_i || t_i) \Big\}
    \label{eq:lle_strategy}
\end{equation}
which is equivalent in the case where the target strategy $t_i$ is set to player $i$'s uniform strategy. Using this definition of $u_i^{\tau}(z_i, x_{-i})$, we can define a loss function as in~\citep{gemp2022sample} such that $\argmin_{\boldsymbol{x}} \gL^{\tau}(\boldsymbol{x})$ is a QRE at temperature $\tau$:
\begin{align}
    \gL^{\tau}(\boldsymbol{x}) &= \sum_i u_i^{\tau}(\br_i, x_{-i}) - u_i^{\tau}(x_i, x_{-i}) \label{eq:qre_loss}
\end{align}
where player $i$'s best response $\br_i = \softmax(\frac{1}{\tau} \nabla_{x_i} u_i + \log(t_i))$.
By annealing $\tau$ from a high value and successively re-solving for the global minimum of $\gL^{\tau}$, we can approximately trace the QRE continuum to the LLE. In Section~\ref{sec:selection}, we explore non-uniform $t_i$ to achieve clone-invariance.



\paragraph{Coarse Correlated Equilibrium (CCE)} Solving for a unique CCE is computationally easier than NE as the problem is convex (Equation~\eqref{eq:deviation_gain}). Therefore any strictly convex function can be used to uniquely select an equilibrium. For example, maximum entropy would be a suitable default criterion following the principle of maximum entropy. However, as we show in Section~\ref{sec:selection}, a different target formulation is necessary for clone-invariance. As such, we opt for maximum relative entropy to a target joint $t=\bigtimes_{i=1}^n t_i$ to allow for non-uniform target joint distributions. A number of off-the-shelf solvers \citep{domahidi2013_ecos} and frameworks \citep{diamond2016_cvxpy} can be used to compute solutions to this problem. We used a particularly efficient dual space gradient based algorithm described in Appendix~\ref{sec:mrecce} for scaling.

\subsection{Invariant Equilibrium Selection} \label{sec:selection}

There may be many NEs and CCEs
\citep{mclennan1999generic, sturmfels2002solving, mclennan2005expected}. Some equilibria exhibit sparse or heavily skewed strategy profiles (see examples in Appendix~\ref{app:fragile_equilibria}). Intuitively, these equilibria are {\em risky} in the sense of {\em risk dominance}: playing one such equilibrium when other players do not would be a costly mistake. Our goal is to propose a selection procedure that along with our equilibrium solving algorithms, approximates a clone-invariant equilibrium.

Shannon entropy plays a key role in several equilibrium selection approaches, however, its definition is vulnerable to redundancy in games. Consider a game with $2$ distinct actions $A$ and $B$ per player and introduce $b - 1$ clones of $B$ into player $1$'s action set. The maximum entropy strategy for player $1$ in the new game is uniform across their actions with mass $\frac{1}{1 + b}$ on each, but this induces a distribution that places $\frac{b}{1 + b}$ cumulative mass on the cloned action $B$.
From Section~\ref{sec:eq_solving}, the maximum Shannon entropy profile defines the precise starting point for tracing the path of QREs towards the LLE. This starting point is sensitive to clones. Hence, if we compute the LLE using the uniform distribution in this new game, we will effectively start from the $(A, B)$ mixed-strategy $(\frac{1}{1 + b}, \frac{b}{1 + b})$ rather than the desired mixed-strategy $(\sfrac{1}{2}, \sfrac{1}{2})$; hence, will not necessarily arrive at the LLE of the original game.





\textbf{Desired properties.} A clone-invariant entropy definition should be:
\begin{description}
\item[P1.] Real-valued, finite, and non-negative for any distribution $x$;
\item[P2.] Have a well-defined gradient for any $x$ in the interior of the simplex;
\item[P3.] Its maximizers should form a convex set. In the case of duplicate strategies (clones), the maximizers should form precisely the set of distributions which arbitrarily distribute a mass of $\frac{1}{c}$ across each of the $c$ sets of clones. In addition, they should achieve an entropy value which is equal to the entropy of the system with clones removed;
\item[P4.] Amenable to efficient estimation and flexible to re-interpretation of redundancy. \label{item:p4}
\end{description}

Note \textbf{P3.} resolves the issue with Shannon entropy that we highlighted above. \textbf{P1} is necessary for a reasonable measure of information content. \textbf{P2} is necessary for gradient-based optimization, and \textbf{P4} is practically helpful for efficient implementation and adaptation to bespoke game settings. We now introduce \emph{affinity} entropy $H_a^p: \Delta \rightarrow \mathbb{R}$, a generalized Tsallis entropy~\citep{tsallis1988possible} that recognises similar or redundant strategies. Its derivation from the above axioms can be found in Appendix~\ref{appx:affinity_entropy}.
\begin{definition}[Affinity Entropy $H_a^p$]
\begin{align}
    H^p_a(\vx) &= \frac{1}{p} \Big[ 1 - \mathbf{1}^\top (U^{(p)} \vx)^{p+1} \Big] %
\end{align}
with entropic-index parameter $p \in (0, 1]$, $U^{(p)} = K\Lambda_p^{-1}$, and $K$ a similarity kernel with entries in $[0, 1]$ with $1$ indicating two strategies are clones, and $\Lambda_p$ a diagonal matrix containing the $(p+1)$-norms of the columns of $K$ on its diagonal.
\end{definition}

\begin{theorem}
Affinity entropy $H_a^p$ satisfies all desiderata \textbf{P1-P4}.
\end{theorem}


In experiments, we define a similarity kernel $K^{(i)}$ for each player $i$ with entries $K^{(i)}_{\alpha \beta}$ with %
\begin{align}
    D^{(i)}_{\alpha \beta} &= \mathbb{E}_{\va \sim U(\gA)}[\big( u_i(\alpha, a_{-i}) - u_i(\beta, a_{-i}) \big)^2] \label{eqn:dissimilarity}
    \\ K^{(i)}_{\alpha \beta} &= \texttt{exp}(-D^{(i)}_{\alpha \beta} / (2 \sigma)^2) \label{eqn:similarity}
\end{align}
where $D$ measures the strategic \emph{dis}-similarity between player $i$'s strategies $\alpha$ and $\beta$ and $K$ is simply a radial basis function (RBF) kernel under the metric $D$. Note $D^{(i)}_{\alpha \beta}$ is zero iff two strategies $\alpha$ and $\beta$ achieve exactly the same utility for player $i$ irrespective of the actions chosen by other players in the game. It should also be clear from the definition how one might Monte-Carlo estimate $D$.
To select for an NE or a CCE, we set $t = \argmax H_a^{p=1}(x)$ in Equation~\eqref{eq:lle_strategy} and Equation~\eqref{eq:mrecce} respectively.





\section{Results} \label{sec:results}

We use the same hyper-parameters for equilibrium solving in all results (see Appendix~\ref{app:solver_params}). For evaluation on real-world prompts, we consider the \arenahard dataset with 500 prompts, selected to separate frontier LLMs, as well as responses from many candidate LLMs. We consider responses from 17 LLMs in particular and queried \geminipro for 8 pairwise preference ratings on each prompt for each model pair. See Appendix~\ref{app:arena_hard_data} for more details.

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{figures/equilibrium_prompts_omni.pdf}
    \caption{We inspect the model improvement path induced by NE ratings as shown in Figure~\ref{fig:elo_vs_equilibrium}~\figright. \figleft shows the sequence of additional prompts added at each iteration. Each prompt is the best-of-64 samples according to their NE ratings. \figcenter shows the sequence of prompt player NEs. Each row defines a distribution over prompts. \figright shows the equilibrium-weighted prompt skills and the sequence of king player models. Recall prompts and models are non-negative vectors over skills, darker indicates higher focus or capability in each skill.}
    \label{fig:equilibrium_prompts}
\end{figure}

\subsection{Equilibrium rating improvement path: a simulated example}


Recall from Figure~\ref{fig:elo_vs_equilibrium} that contrary to the Elo improvement path, maximizing equilibrium ratings led to models (and prompts) improving across skills. We inspect the equilibrium improvement path and offer our interpretation. 
Figure~\ref{fig:equilibrium_prompts}~\figright shows that the shifts in focus between skills by the model player coincides with transitions in the NE prompts, or prompts weighted by their NE strategies (shown in Figure~\ref{fig:equilibrium_prompts}~\figcenter). Similarly, to gain support under an NE, new prompts must highlight a skill dimension along which equilibrium models are better differentiated (Figure~\ref{fig:equilibrium_prompts}~\figleft). In sum, equilibrium prompts separate equilibrium models. This dynamic encourages exploration of new skill dimensions and incentivises models to be well-rounded across skills.




\subsection{Invariant Evaluation} \label{sec:exact_clone_invariance}

We now turn to \arenahard and show that candidate LLMs' equilibrium ratings are invariant to redundancies when their Elo ratings are not. In this experiment, we will introduce prompts targeted at bringing down the rating of a certain action (in this case, model). Specifically, let $\bar{\vu}_k(a_k) = \frac{1}{|\gA_m|} \sum_{a_r} u_k(\cdot, a_k, a_r)$ be the vector of expected king player payoffs when playing action $a_k$ against a randomly chosen rebel model on each prompt. We can then sample prompts adversarial  to $a_k$ from $\softmax(-\lambda \bar{\vu}_k(a_k))$ and add them to the prompt set. Figure~\ref{fig:exact_clone_invariance_with_shannon} reports the king model rankings under different methods with $a_k = \texttt{\geminipro}$ and $\lambda = 10$. 

Our first observation is that without redundant adversarial prompts, our proposed equilibrium rankings of LLMs are fairly consistent with their Elo rankings, with a few models moving up or down one or two positions. This deserves attention. Out of a multiplicity of equilibria, the NE and CCE we selected led to rankings that correspond to our intuition. Indeed, we show in Appendix~\ref{app:fragile_equilibria} that the NE we select is risk-dominant among 128 mixed-strategy NEs of this game.
Second, the Elo ratings can be arbitrarily influenced by redundancy, with the top-ranked model falling through the ranks. Equilibrium rankings remain invariant. In fact, while we lose the invariance guarantee with {\em near} redundant prompts, we show models' equilibrium rankings to degrade gracefully in Appendix~\ref{app:noisy_clone_invariance}.
Third, the CCE ratings show the top-3 models to tie for the first place: correlating models with prompts affects the competitive landscape which we inspect in Section~\ref{sec:interpreting}. 
Lastly, solving for a unique equilibrium is not sufficient for invariant ratings. We show in Figure~\ref{fig:exact_clone_invariance_with_shannon}~\figright that using Shannon's entropy for tracing the QRE continuum or for selecting a max-entropy CCE would not lead to invariant ratings. 
For completeness, we provide a detailed breakdown of our equilibrium ratings in terms of action ratings and marginals for each player in Appendix~\ref{app:noisy_clone_invariance}. %

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{figures/clone_invariance_exact_with_shannon.pdf}
    \caption{We introduce an increasing number of redundant copies of prompts adversarial to \geminipro and show model rankings under each method. Models at the same rank are grouped in grey and ordered alphabetically. \figright We show equilibrium rankings under NE(-a) and CCE(-a) selected using Shannon's entropy instead of the affinity entropy. Dotted lines connecting different rating panels indicate continuity in the labeling. For instance, \geminipro consistently ranks first under our NE and CCE ratings, despite the introduction of up to 500 redundant adversarial prompts. However, its ranking suffered significantly under the Elo ratings as soon as 250 adversarial prompts have been introduced.}
    \label{fig:exact_clone_invariance_with_shannon}
\end{figure}

\subsection{Interpreting Equilibrium Solutions}
\label{sec:interpreting}

Besides rankings, the equilibrium solutions can surface interpretable insights. We share two examples using NE and CCE solutions respectively from ratings shown in Section~\ref{sec:exact_clone_invariance}.

\paragraph{Nash Equilibrium Prompts} We have shown that equilibrium ratings are intuitive and invariant to redundancy. A follow-up question is which actions are highly-rated and which actions affect other players' ratings (i.e., with positive support at the NE). 

Recall that the prompt player utility $u_p(a_p, a_k, a_r) = |u_k(a_p, a_k, a_r)|$ reflects the extent to which a prompt separates the pair of responses from models $a_k$ and $a_r$. The prompt player's equilibrium rating is then $\text{regret}(a_p, \vx) = \E_{\substack{a_k \sim x_k \\ a_r \sim x_r}} u_p(a_p, a_k, a_r)$ with $x_k$, $x_r$ the NE strategies of the king and rebel player respectively. By definition, prompts that are highly rated under NE ratings separate models played at the NE. In other words, while the Elo ratings reflect the strength of an action on average, equilibrium ratings reflect the strength of actions at the selected equilibrium. 

We can now illustrate these phenomena using the same game investigated in the second columns of Figure~\ref{fig:exact_clone_invariance_with_shannon}, with 250 redundant prompts added to the game. First, we show in Figure~\ref{fig:ne_ratings_and_support_samples}~\figtop the {\em king-vs-rebel} payoff matrices induced by 6 sample prompts, with increasing equilibrium prompt ratings. Prompts with low ratings tend to fail to differentiate performant models (i.e. top-left block of each heatmap). Second, we can ask which prompts should we expect to have support at an equilibrium. Figure~\ref{fig:ne_ratings_and_support_samples}~\figbottom shows that empirically, highly rated prompts are played more often at the equilibrium we select. This implies that the model ratings are heavily influenced by a small subset of prompts that separate frontier models. We note that this correlation is not guaranteed, following our discussion in Section~\ref{sec:exact_clone_invariance} on redundant actions. Indeed, our final observation is that prompts that are clones with other prompts tend to receive lower probability mass than their ratings would have required. In fact, since we have introduced 250 redundant prompts explicitly, we can highlight in gray prompts that are indeed redundant --- many of these prompts enjoy high ratings, but significantly lower mass. In other words, equilibrium ratings reflect quality of an action in isolation while equilibrium mass further takes into account redundancy of an action with respect to other actions. This observation is even clearer in games studied in Appendix~\ref{app:toy_games}-\ref{app:vulnerability}.


\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{figures/ne_ratings_and_support_samples.pdf}
    \caption{
    Highly rated prompts generally have high support under the NE. Redundant prompts (gray bands) receive identical ratings but notably lower support. In sum, equilibrium ratings reflect separability of each prompt with respect to the model equilibrium strategies in isolation, whereas equilibrium support of each prompt further accounts for its redundancy with respect to other prompts. \figtop We show the {\em king-vs-rebel} payoffs induced by example prompts. Green indicates king-player winning and red losing. Highly rated prompts tend to discriminate between strong models (top-left corners). \figbottom We show the NE supports and ratings of all prompts, ordered by their NE ratings.}
    \label{fig:ne_ratings_and_support_samples}
\end{figure}


\paragraph{Marginal rating contribution by co-player action}
With ratings derived from underlying equilibria, we can decompose the rating of each action into a sum of marginal contributions from each co-player's actions. Recall from Equation~\eqref{eq:deviation_gain} that the rating of an action $a_i$ is its $\text{regret}_i(a_i, \vx) = u_i(a_i, x_{-i}) - u_i(\vx)$. We can decompose the rating of player $i$'s action $a_i$ into a weighted sum of each of player $j$'s contributions, with $\delta(a_i, a_j, \vx) = x_j(a_j) \left[u_i(a_i, a_j, x_{-i,-j}) - u_i(a_j, x_{-j})  \right]$ the marginal contribution of $a_j$ to $a_i$'s equilibrium rating. Note that $\text{regret}(a_i, \vx) = \sum_{a_j} \delta(a_i, a_j, \vx)$. 
The rating of $a_i$ describes the {\em cost} incurred by player $i$ if it were to deviate to play $a_i$ from an equilibrium $\vx$. $\delta(a_i, a_j, \vx)$ therefore explains $a_j$'s contribution to player $i$'s decision to not deviate.

Recall from Figure~\ref{fig:exact_clone_invariance_with_shannon} where several models tied for the first place under the CCE profile but are fully differentiated under NE. We can now leverage the marginal contribution analysis to understand the mechanism underlying this phenomenon. Figure~\ref{fig:cce_rating_contribution} shows the CCE king model ratings decomposed from the perspective of the rebel player. In other words, we ask which rebel models contribute most positively or negatively to each king model's CCE rating. For clarity of presentation, we focus on the top 5 models and we group rebel models into families of models if they share the same naming prefix. The contribution of each family of model is therefore the sum of the contribution by models within each family $\gF$ or $\sum_{a_r \in \gF} \delta(a_k, a_r, \vx)$ with $a_k$ a king model and $a_r$ a rebel model.

We make several remarks. First, all 3 top-ranked king models benefit the most when compared against rebel models in their own model family: the GPT family \citep{achiam2023gpt} of models contribute positively to the ratings of \gpto and \gptt. Similarly, \geminiflash, the only other model in the Gemini family \citep{team2023gemini}, improves \geminipro's rating the most. We speculate that this can be a result of model developers selecting models to release based on favourable comparisons to their earlier or smaller models. Second, all top-ranked models remain vulnerable to open-weight models such as the Mistral \citep{jiang2023mistral} and Llama \citep{dubey2024llama3herdmodels} families of models. More fine-grained analysis may shed light on the prompts on which these losses tend to occur. 

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{figures/cce_rating_contribution.pdf}
    \caption{
    The CCE joint distribution can surface insights in the comparison data. Each bar represents a model family $\gF$ and its width corresponds to $\sum_{a_r \in \gF} \delta(a_k, a_r, \vx)$ with $a_k$ a king player model choice and $a_r$ a rebel model belonging to the family $\gF$. A model's family is determined by its model name prefix. For brevity, we show the king model rating breakdown for the top 5 out of 17 models.
    Interestingly, we observe that the 3 top-ranked king models tend to do well against other models in its own family. The Gemma family of models, improves the rating of \geminipro but detract from the ratings of GPT models. Mistral and Llama family of models, on the other hand, surface weaknesses in all 3 top-ranked models, contributing negatively to top-ranked king models' ratings.
    }
    \label{fig:cce_rating_contribution}
\end{figure}

We caveat that our results are in part derived from the preference ratings of a \geminipro model and may not reflect the true dynamics of real-world LLM development. Nevertheless, the interpretability offered by the game-theoretic equilibria further distinguishes game-theoretic evaluation from prior works to be discussed in the Section~\ref{sec:related}. 








\section{Related Works} \label{sec:related}

There is a rich body of literature studying rating methods with applications in Chess, Go, Tennis and video games. One family of probabilistic methods follows the Bradley-Terry model and predicts pairwise win probabilities from ratings. A widely used example is Elo \citep{elo1978rating} with extensions Bayes-Elo, mElo and Elo-MMR \citep{coulom2008whole, balduzzi2018re, ebtekar2021elo, vadori2024ordinal} capturing temporal variation, cyclicality and ordinal ranks in data. Elo ratings can typically be efficiently solved as regression problems, although their ratings are vulnerable to redundancy. A separate line of work draws from Social Choice (or \emph{Voting}) Theory (SCT, \citet{sen1977social, lanctot2023evaluating}),
which also studies {\em independence of clones}: rankings should be invariant to redundant candidates (e.g., LLM models) being added.
However, invariance to redundancy in votes (e.g., prompts) is in direct opposition to the spirit of social choice theory. In this sense, SCT provides partial (one-sided) clone invariance, which we argue is insufficient for open-ended, LMSYS-style evaluation. Finally, game-theoretic evaluation has been previously studied in \citet{balduzzi2018re} and \citet{marris2022_game_theoretic_rating} where full clone-invariance is guaranteed in the \tpzs setting. Our method generalises the approaches in these works to \npgs settings, with practical equilibrium solving and selection algorithms based on our novel affinity entropy definition. Other approaches have been concurrently developed that avoid the equilibrium selection dilemma, and hence obviate the use of entropy~\citep{marris2025deviationratings}.

\section{Conclusions} \label{sec:conclusion}

We studied the effect of maximizing Elo ratings in the context of open-ended evaluation and showed that its sensitivity to redundancy could bias model (and prompt) selection. We then proposed an equilibrium rating framework, with practical equilibrium solving and selection algorithms that can scale to real-world LLM evaluation. We show our method to provide intuitive and robust rankings of models (and prompts), with interpretable structures.

We see several exciting future directions. First, although our methods can scale to tens of thousands of prompts and tens of models on commodity hardware, scaling further would be challenging. Tabulating the evaluation payoff tensor with pairwise preference ratings can be costly too. 
Research into alternative solution concepts, or how we could leverage their equilibrium structure for analysis (e.g. prompt and model pruning) is also promising. Finally, while we target LLM evaluation in particular, our methodology can be applied more generally to other domains. For instance, our rating methods could evaluate multi-modal model generation capabilities \citep{jiang2024genaiarenaopenevaluation} or analysing game dynamics for video game development \citep{pendurkar2023bilevel}.































































\bibliography{iclr2025_conference}
\bibliographystyle{iclr2025_conference}

\newpage
\appendix


\section{Computing the Maximum Relative Entropy CCE}
\label{sec:mrecce}

A maximum relative entropy CCE, that minimises the distance of the log-joint, $\log(x(a))$, to a target log-joint, $t(a) \in \mathbb{R}^{|\gA|}$, can be computed using gradient descent. We formulate the problem in dual space \citep{marris2022_turbo_arxiv} with dual parameters, $\alpha_i(a'_i) \in \mathbb{R}_+^{|\gA_i|} ~ \forall i$, defined as functions, $\alpha_i(a'_i) = \softplus(\theta_i(a'_i)) ~ \forall i$, of learned parameters, $\theta(a'_i) \in \mathbb{R}^{|\gA_i|} ~ \forall i$. Let $l_\theta(a)$ be a logit term used to construct the loss function.
\begin{align}
    l_\theta(a) = - \sum_i \sum_{a'_i} \alpha_p(a'_i) \left[ u_i(a'_i, a_{-i}) - u_i(a) \right] + t(a)
    \label{eq:mrecce}
\end{align}
Minimizing a loss function, $\min_\theta L_\theta$,  converges to optimal dual variables, $\alpha^*_i(a'_i) = \softplus(\theta^*_i(a'_i)) ~ \forall i$ with $ L_\theta = \log \left[ \sum_a \exp \left[ l_\theta(a) \right] \right]$.
The loss is convex, deterministic, and unconstrained. Therefore many optimization algorithms are suitable. The primal joint can be simply recovered from the optimal logit term $x_\theta(a) = \softmax\left[ l_{\theta^*}(a) \right]$.

\input{affinity_entropy}
\input{affinity_kernel}

\section{Experiments}

\subsection{Simulated model and prompt improvement path} \label{app:simulated_model_prompt_improvement}

Algorithm~\ref{alg:evo_improvement} describes our simulated model and prompt improvement procedure. At each iteration, we add a new prompt and a model following an evolutionary procedure. We require all prompts to be probability distributions over skill dimensions. We model for a transitive dimension for models by representing each model vector as a sum of probability vectors over skills. A new model is added to the set of models $\gA_m$ if and only if it becomes top-ranked according to the rating function $r$. A new prompt is added as long as it is the best-of-$P'$ sampled prompts and does not have to be top-ranked.

\begin{algorithm}
\centering
\caption{Evolutionary model and prompt selection procedure \label{alg:evo_improvement}}
\begin{algorithmic}[1]
\State Let $K$ be the number of orthogonal skill dimensions.
\State Let $r: \gA_p \times \gA_m \rightarrow \vr_p, \vr_m$ be a rating function assigning a scalar rating to each action.
\State Let $P_0$, $M_0$ be the number of initial prompts and models.
\State Let $P'$, $M'$ be the number of sampled candidate prompts and models at each iteration.
\State
\State $\gA^0_p \sim \textrm{Dirichlet}(\vone_{1:K}, P_0)$ \Comment{$P_0$ sampled initial prompts.}
\State $\gA^0_m \sim \textrm{Dirichlet}(\vone_{1:K}, M_0)$ \Comment{$M_0$ sampled initial models.}
\State
\For{$t \in [1, \dots]$}
\If {additional prompts} \Comment{If adding new prompts.}
    \State $\gA'_p \sim \textrm{Dirichlet}(\vone_{1:K}, P')$  \Comment{Sampling $P'$ candidate prompts.}
    \State $\vr_p, \_ \gets r(A'_p \cup A_p, A_m)$
    \State $\gA_p \gets \gA_p \cup \{ A'_p[\arg \max \vr_p[:P'] ] \}$  \Comment{Add best-of-$P'$ prompt.}
\EndIf
\State $\gA'_m \gets \vzero$
\While {true}
    \State $\Delta_m \gets \textrm{Dirichlet}(\vone_{1:K}, M')$ \Comment{Sampling $M'$ model improvement vectors.}
    \State $\_, \vr_m \gets r(A_p, \{ \gA'_m + \Delta_m \} \cup A_m)$ \Comment{Evaluate improved candidate models.}
    \State $\gA'_m \gets \gA'_m + \Delta_m[\arg \max \vr_m[:M']]$
    \If {$\arg \max \vr_m[:M'] = \arg \max \vr_m$}
        \State $\gA_m = \{ \gA'_m[\arg \max \vr_m] \} \cup \gA_m$  \Comment{Add a new top-ranked model.}
        \State \textbf{break}
    \EndIf
\EndWhile
\EndFor
\end{algorithmic}
\end{algorithm}

\subsection{Equilibrium-solving hyper-parameters}
\label{app:solver_params}

We use the same set of hyper-parameters for all our experiments. For affinity-entropy $H^p_a(x)$, we use $p=1$ and set kernel variance to $\expnumber{1}{-6}$. To solve for a max affinity-entropy distribution we use gradient descent. The max affinity-entropy distribution is then used in NE and CCE solving.

For NE solving using LLE approximation, we initialize temperature $\tau = 1.0$ which is annealed exponentially with a decay rate of $0.95$ every $250$ gradient updates if and only if the exploitability in the annealed game $\gL^\tau(\vx)$ (Equation~\eqref{eq:qre_loss}) is at most $\expnumber{1}{-5}$. We set the terminal temperature to $\tau = \expnumber{1}{-2}$. We early terminate the equilibrium solving if we have found an $\epsilon-$NE with $\epsilon = \expnumber{1}{-3}$.
For CCE solving, the optimization problem is convex and we minimize Equation~\ref{eq:mrecce} directly.
For gradient descent, we use an Adam optimizer \cite{kingma2014adam} with a fixed learning rate $\expnumber{1}{-2}$ for all steps (maximizing affinity-entropy and equilibrium solving).

\subsection{The \arenahard evaluation data}
\label{app:arena_hard_data}

We evaluate our method on the \arenahard dataset \citep{arenahard2024} with 500 prompts and 17 competing models. The set of prompts as well as model responses are downloaded from LMSYS data repository (\url{https://huggingface.co/spaces/lmsys/arena-hard-browser}), with the exception of \geminipro and {\tt gemini-1.5-flash-api-0514}. As we need to tabulate the payoff tensor for all model pairs, we sampled 8 preference ratings using \geminipro for each model pair, with 4 samples for each permutation to account for potential position bias of the LLM rater. Pairwise model utility is averaged over all ratings samples. %

\subsection{Risk-dominant equilibria}
\label{app:fragile_equilibria}
Our \koh evaluation game admits a multitude of Nash equilibria, among them 80 are pure-strategy NEs (see Table~\ref{tab:pure_ne}). Additionally, we computed 128 mixed-strategy NEs with exploitability at most $\expnumber{1}{-2}$ that each derives a distinct set of ratings. In particular, one of the 128 mixed-strategy NEs is pre-computed by our NE solving and selection procedure by tracing the QRE continuum, which we refer to as the $0$-th equilibrium, or $\vx^0$. 

\begin{table}[]
\centering
\caption{Prompt and king actions that each define 16 pure-strategy Nash equilibria --- any rebel action except the model played by the king player is a pure-strategy NE.\label{tab:pure_ne}}
\begin{small}
\begin{tabular}{c|c}
{\bf Prompt} & {\bf King}   \\ \hline
``Can you implement a python tool that is intended to ru...'' & gemini-1.5-pro-api-0514 \\
``Hi. I have this URL which I can paste in my Microsoft ...'' & gemini-1.5-pro-api-0514 \\
``Please provide a simple RESPONSE to the following PROM...'' & claude-3-5-sonnet-20240620 \\
``Take on the rol eof an Gherkin expert. Can you improve...'' & claude-3-5-sonnet-20240620 \\
``Write a small python function that get all the links o...'' & gemini-1.5-flash-api-0514  \\ \hline
\end{tabular}%
\end{small}
\end{table}

A longstanding challenge in game theory is that of equilibrium selection. Suppose that every player knows that there are many equilibria in the game, each player must confront the following question during play: out of all equilibria, which equilibrium strategy should I play and relatedly, which equilibrium would each of my co-players play? This is critical, as miscoordinating could lead to arbitrarily bad outcome, despite each player playing one of its equilibrium strategies. For instance, everyone driving on the right or left hand side of the road are two valid equilibria, but miscoordinating would be devastating.

It is for this reason that the notion of risk-dominance of \citet{harsanyi1988general} is critically important: the Nobel-prize winning theorem suggests that players would each iterate on their prior beliefs over which equilibria its co-players would play and choose the one that is the least {\em risky} when players {\em miscoordinate} under such priors. Here, we show empirically that our solution concept leads to risk-dominant equilibria as suggested by \citet{herings2010homotopy}. To do so, we simultaneously minimize the exploitability of several profiles in parallel with a regularizer that maximizes the $L_2$ rating differences between any two profiles by gradient descent as in \cite{liu2024nfgtransformer}. This yields an additional 127 NEs with exploitability at most $\expnumber{1}{-2}$ that we analyze in Figure~\ref{fig:risk_dominance_analysis}. 

Figure~\ref{fig:risk_dominance_analysis}~\figtop shows the 128 mixed-strategy NEs with distinct model ratings. Figure~\ref{fig:risk_dominance_analysis}~\figcenter shows the expected payoffs to player $i$ when it plays its $p$-th equilibrium strategy $x^p_i$ when other players uniformly choose one of theirs, or $\E_{q \sim \pi_{u}} \left[ u_i(x^p_i, x^q_{-i}) \right]$ with $\pi_u$ a uniform distribution over 128 equilibria. In yellow, we show the sum of per-player expected payoffs. We confirm that many NEs are indeed {\em risky}, as their stability relies heavily on all players coordinating on the same equilibrium. Figure~\ref{fig:risk_dominance_analysis}~\figbottom takes things one step further and follows the intuition of risk dominance more closely. Starting from a uniform prior belief over player $i$'s choice of equilibria, $\pi^0_i = \pi_u$, each player iterate their believes over other players' choices of equilibrium based on the expected payoff of them playing each equilibrium. 

Specifically, we let 
\begin{align}
    \pi^{t+1}_i = \softmax \Big( \log \pi^t_i + \eta \E_{\substack{\forall j \neq i \\ t(j) \sim \pi_j}} \left[ \vu_i(\dots, x^{t(i-1)}_{i-1}, x^{t(i+1)}_{i+1}, \dots) \right] \Big)
\end{align}
with $\eta = \expnumber{1}{-2}$ the step-size and we compute the expected payoffs to player $i$ when playing its $k$-th equilibrium at $T = 10,000$ as
\begin{align}
\E_{\substack{\forall j \neq i \\ e(j) \sim \pi^T_{j}}} \left[ u_i(\dots, x^{e(i-1)}_{i-1}, x^k_i, x^{e(i+1)}_{i+1}, \dots) \right]
\end{align}

Ordered by the sum of expected payoffs for all players, we observe that the Nash equilibrium our procedure selects (equilibrium $\vx^0$) is the least risky among 128 mixed-strategy NEs of the game, without any player being particularly worse off than others even when players miscoordinate.

\begin{sidewaysfigure}
    \centering
    \includegraphics[width=1.0\textwidth]{figures/risk_dominance_analysis.pdf}
    \caption{From top to bottom: a) we show the distinct king player action ratings derived from 128 mixed-strategy NEs of the \koh game. All NEs have exploitability at most $\epsilon \le \expnumber{1}{-2}$; b) we show the expected payoff to each player under uniform priors over their 128 equilibria; yellow circles show the sum of expected per-player payoffs; c) we show the same analysis as in b) but the expectation is taken under optimized equilibrium priors. Equilibrium 0 (rightmost) is the LLE our NE solving procedure select. }
    \label{fig:risk_dominance_analysis}
\end{sidewaysfigure}

\subsection{Invariant Evaluation} \label{app:noisy_clone_invariance}

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{figures/clone_invariance_noisy.pdf}
    \caption{We introduce an increasing number of redundant copies of prompts adversarial to \geminipro with noise sampled from $\textrm{Uniform}(-0.01, 0.01)$ applied to their payoffs. Equilibrium ratings with a clone invariant selection procedure degrades gracefully to noisy redundancy while the Elo ratings become incrementally skewed. Models at the same rank (with an absolute rating difference at most $\expnumber{1}{-4}$) are grouped in grey and ordered alphabetically. We caveat that the specific rankings reported are subject to the LLM preference model used which in this case may exhibit a self-preference to the Gemini family of models.}
    \label{fig:noisy_clone_invariance}
\end{figure}

We show in Figure~\ref{fig:noisy_clone_invariance} the effect of introducing {\em near} redundant adversarial prompts on the equilibrium ratings. While our invariant property is limited to exact clones, our results show that our approach results in rankings that degrade gracefully in this approximate case, even with 1,000 adversarial prompts. The Elo rating system suffers from such bias in data similarly as in the exact case Figure~\ref{fig:exact_clone_invariance_with_shannon}.

In Figure~\ref{fig:rating_breakdown} we provide a detailed breakdown of our NE and CCE ratings results (without redundant adversarial prompts). We show the actions of each player ranked by their equilibrium ratings and by their support under the equilibrium marginal distribution.

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{figures/rating_breakdown.pdf}
    \caption{We show actions of each player ranked by their rating and equilibrium support under NE \figtop and CCE \figbottom profiles respectively.}
    \label{fig:rating_breakdown}
\end{figure}













\end{document}
