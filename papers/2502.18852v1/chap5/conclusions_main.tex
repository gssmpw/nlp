\chapter{Discussion and Conclusions}
\textbf{In this chapter, DNN refers to ``FWI as a Data-Driven DNN'' and RNN as ``Theory-Guided RNN as an Analogue of FWI''. The first section presents a critique to the work, addresses possible pitfalls, identifies areas of potential improvement and suggests directions for future research. This is followed by a final section which presents concluding remarks for this dissertation.}

\section{Critique, Limitations and Future Work}
\subsection{Inversion Paradigm}
DNN, RNN and FWI are three different frameworks which fall within different parts of the inversion spectrum. On one end there is the data-driven approach for DNN, on the other there is purely deterministic geophysics with classical FWI, and in between there is theory-guided RNN.

% Within the DNN approach, the inversion component is purely data-oriented and the data generator is based on geophysics. This out-performs classical FWI inversion for deeper and over-thrust areas since it is not bound by the deterministic forward-modelling physical constraints from ray-tracing. Figure~\ref{fig:fwi_ray_tracing} shows a sample of shots through the Marmousi model. Ray-paths below the high velocity at depth 2-2.5km do not arrive at the receivers at the surface for the current model offsets. Acquisition geometry controlling the offset is a hard-limit within FWI. \citet{Morgan2009} considers large-offsets to be fundamental for successful FWI. Moreover, this hinders seismic imaging in and around salt bodies since, by definition, only one-way ray-paths are considered \citep{Jones2014} - See Figures~\ref{fig:fwi_poor_illum}. 

% \begin{figure}[ht!]
%     \centering
%     \includegraphics[width=0.95\textwidth]{27_real_data_06_ray_tracing_marm1_v2-eps-converted-to.pdf}
%     \caption[Ray-tracing coverage within forward modelling]{Ray-tracing coverage within forward modelling derived from deterministic geophysics.}
%     \label{fig:fwi_ray_tracing}
% \end{figure}

% \begin{figure}[ht!]
%     \centering
%     \includegraphics[width=0.95\textwidth]{fwi_poor_illumination.png}
%     \caption[Typical ray-paths and Reflection point counts for classical FWI.]{Typical ray-paths and Reflection point counts for area at salt flank that has poor illumination due to deterministic inversion. Adapted from \cite{Jones2014}.}
%     \label{fig:fwi_poor_illum}
% \end{figure}

Within the DNN approach, the inversion component is data-oriented and the data generator is based on geophysics. If data-generators are to include information that might not be deterministically available within a seismic survey, the reconstruction process could invert for this information to \cite{Lewis2017}'s work. A similar approach has already been applied by \cite{Bubba2019} for brain CT-scans (Figure~\ref{fig:fwi_gitta}). These types of data-driven models could be pre-cursors for deterministic models as has been done in the work of \cite{Araya-Polo2018}. Figure~\ref{fig:dnn_initial_fwi} shows inversion for classical FWI without and with DNN as a priori model. The latter approach produces more layer continuity and better imaging at depth. This comes with relatively no extra overhead cost since the DNN would be a pre-trained network. Furthermore, \cite{Grohs2019} analytically determine the upper bounds for the approximation and generalization power for a NN. This could be implemented and provided as a caveat with data-driven inversion by providing confidence maps for the inversion.

\begin{figure}[ht!]
    \centering
    \includegraphics[width=0.8\textwidth]{gitta.png}
    \caption[Data-driven inversion for CT-scans.]{Inversion for brain CT-scan, with area of interest. Classical inversion via Filtered Back Projection has areas of poor-illumination due to limited-angle, whilst the data-driven approach offers better inversion. Adapted from \cite{Bubba2019}.}
    \label{fig:fwi_gitta}
\end{figure}

\begin{figure}[ht!]
    \centering
    \includegraphics[width=0.95\textwidth]{22_extra_plot_01_prior_model-eps-converted-to.pdf}
    \caption[Improved FWI result with DNN as initial model]{Improved FWI result with DNN as initial model.}
    \label{fig:dnn_initial_fwi}
\end{figure}

Theory-guided inversion inherits advantages and problems from either part of the spectrum. It faces challenges of cycle-skipping and local-minima, whilst it benefits from the use of automatic differentiation to calculate the gradient. This reduces development time as it avoids the need to manually implement and verify the adjoint state method. Furthermore, being at the intersection of physics and computer science, it is inherently strengthened by contributions from two communities of researchers. This opens up possibility of considering other deep learning techniques such as dropout or other acyclic-graph architectures such as directed acyclic graphs \citep{Bogaerts2020}.

In classical FWI, the wavefield at the final time step is affected by the wavefield during the initial time steps. Back-propagation must occur over the entire sequence of time steps for theory-guided RNN. Application of back-propagation through thousands of layers is not a typical application in deep learning applications and automatic differentiation is not designed to efficiently handle such situations. Strategies common to other FWI frameworks to reduce memory requirements could be translated into the field. Examples would include not saving the wavefield at every time step \citep{Nguyen2015}, applying compression to the wavefield time slices \citep{Boehm2015,Kalita2017}, saving wavefields to disk rather then memory \citep{Shen2015}, and regenerating wavefields during back-propagation rather than storing them \citep{Malcolm2016, Yao2020}. Data-driven inversion is gradually proving itself to be a field of its own. An account for some of the main contributions is provided by \cite{Arridge2019}. It is up to the end user to decide to which level of confidence they are willing to base their inversion on data.

\subsection{Training Datasets for Real Data}\label{ref:sec_disc_development_dataset}
The DNN requires a global model for a real world problem. Consider as example a data generator trained on data that is limited to 3 layers and inversion is carried out for a system made for more than 3 layers. The inversion process will start degradation as shown in Figure~\ref{fig:dnn_discussion_missing_layer}. Conv1D-Adadelta DNN was trained for 30 epochs for a maximum 3 layers for velocity ranging from 1450\si{ms^{-1}} to 5000\si{ms^{-1}}. The first two columns are inversions for velocity profiles within the generator limit for the number of layers. These inversions are of good quality as expected from previous results. For more layers in the velocity profile, the inversion tries to generalize for these profiles but misses some components. The inversion is still able to identify some layers, but not to the same resolution as if trained on those layers. The large velocity contrasts are inverted correctly as shown for the 6 layer velocity profile. Similarly, the geophysics models allowed within the problem need to be considered. Figure~\ref{fig:dnn_discussion_Inclusion_Inversion} illustrates inversion for a large velocity of 6000\si{ms^{-1}} and velocity inversion respectively. Given that these models were not included in the development dataset, the inversion process would never be able to invert for these velocity types correctly. On the other hand, the DNN framework is robust to different noise levels. Figure~\ref{fig:dnn_discussion_Noise_Sensitivity} showcases the inversion for pink noise contaminated data at different levels. Pink noise contamination was chosen due to its prevalence in the low frequency limit \citep{Randall2009}, thus making it more suited to assess the effect on the low frequency component of FWI. The inversion remains relatively unaltered before 25\% and then starts degradation. This inversion process could be used as a de-noising technique given the correct data-generator. 

\clearpage
\begin{figure}[ht!]
    \centering
    \subbottom[The first two columns are inversions for velocity profiles within the generator limit for the number of layers. For more layers, the inversion tries to generalize for these profiles but misses some components. The inversion is still able to identify some layers, but not to the same resolution as if trained on those layers.\label{fig:dnn_discussion_missing_layer}]{\includegraphics[width=0.9\textwidth]{28_multi_layer_01_missing_extra_layer_01_Missing_Layer-eps-converted-to.pdf}}
    \subbottom[Large velocity inclusions and Velocity inversion would be missed as well.\label{fig:dnn_discussion_Inclusion_Inversion}]{\includegraphics[width=0.9\textwidth]{28_multi_layer_01_missing_extra_layer_02_Inclusion_Inversion_Layer-eps-converted-to.pdf}}
    \subbottom[Robustness to noise up to 25\% pink noise contamination.\label{fig:dnn_discussion_Noise_Sensitivity}]{\includegraphics[width=0.9\textwidth]{28_multi_layer_01_missing_extra_layer_03_Noise_Sensitivity-eps-converted-to.pdf}}
	\caption[Development dataset directly influences the quality of inversion.]{Conv1D-Adadelta DNN trained for 30 epochs for a maximum 3 layers for velocity ranging from 1450\si{ms^{-1}} to 5000\si{ms^{-1}}. The quality of the development dataset directly influence the quality of DNN inversion. A: Missing layer. B: Different geophysical models. C: Noise sensitivity. }
\end{figure}

\clearpage
Modelling techniques and transform spaces could provide an alternative approach for the DNN. \cite{Jozinovic2020} use Wigner-Ville distributions for pseudo-spectral representations of the seismograms for prediction of intensity measurements of ground shaking, or other representations such as Recurrence Plots \citep{Kamphorst1987}, Markov Transition Fields \citep{Wang2015} and Gramian Angular Fields \citep{Wang2015} - See Figure~\ref{fig:alt_representations_trace}.

\begin{figure}[ht!]
    \centering
    \includegraphics[width=0.95\textwidth]{21_2d_patch_options-eps-converted-to.pdf}
    \caption[Alternative representations for a trace.]{Alternative representations for a trace.}
    \label{fig:alt_representations_trace}
\end{figure}

For theory-guided RNN, excluding part of the data from training for use as a development dataset is standard practice in deep learning, but not within classical FWI. For a real-world problem, the size of the seismic dataset relative to the model parameters generally has fewer data samples and could potentially prove problematic. Hyper-parameter tuning for the optimal parameters for RNN demonstrate that practice can result in convergence to a good model, yet this does not prove a similar result is achievable when using the entire dataset. 

% NOISE SENSITIVITY OF RNN
% **** INVERT RNN WITH PINK NOISE LEVELS

\subsection{Forward Modelling and Multiples}
All shots considered within the forward problem for either FWI and RNN framework were within the water column for the Marmousi model. This implies that receiver data have surface-related multiples, together with all other inter-layer multiple components. Undergoing forward problem solving with and without multiples is a decision that the literature is still unable to resolve. 

Multiples travel longer paths and are reflected at small angles in contrast to the primaries and are able to illuminate shadow zones where primary reflections cannot reach \citep{Bergen2019}. Inclusion of these wavefield components can lead to improvement within the inversion process as multiples can contain more subsurface information compared to primary and diving wave \citep{Komatitsch2002}. \cite{Bleibinhaus2009} investigated these effects of surface scattering in FWI and concluded that velocity models resulting from neglecting the free surface in the inversion show artifacts and suffered from a loss of resolution. \cite{Liu2020} employ a combination of lower-order multiple as the source and the higher-order multiple to invert, whilst \cite{Zhang2013} transform each hydrophone into a virtual point source with a time history equal to that of the recorded data to help their inversion and are able to produce methods utilizing multiples to improve velocity updates. 

\cite{Hicks2001} and \cite{Operto2006} show how traditional FWI would become unstable when inverting with free surface related waves. This said, removing of multiples introduce additional processing steps which are subject to error and could lead to removal of signal. The consensus is that the choice of multiple inclusion is per different use case. Indeed, the work presented could be revisited for the sensitivity of multiples within the inversion.

\subsection{Implications of Data Volume and Computational Power}
More data is directly correlated with better modelling for NN frameworks, and this ability is limited by the resources available. Similar to classical FWI, computational power is a limitation within the frameworks presented. This was already identified within the RNN approach with the limit from the Graphical Processing Unit RAM, constraining the model size and batch processing. A larger batch-size for RNN processing would intuitively imply that the optimization is less likely to get stuck with local minimum and reduce the probability of cycle-skipping. Workaround for this could be multi-Graphical Processing Unit systems, such as NVIDIA's DGX station\footnote{\url{https://www.nvidia.com/en-us/data-center/dgx-systems/}$\textsuperscript{\ref{ref:footnote_no_affiliation}}$} and  Lambda Lab's Vector station\footnote{\url{https://lambdalabs.com/gpu-workstations/}$\textsuperscript{\ref{ref:footnote_no_affiliation}}$}, or cloud computing such as Amazon Web Services \footnote{\url{https://aws.amazon.com/nvidia/}$\textsuperscript{\ref{ref:footnote_no_affiliation}}$} and Google Cloud\footnote{\url{https://cloud.google.com/gpu/}$\textsuperscript{\ref{ref:footnote_no_affiliation}}$}\footnote{\label{ref:footnote_no_affiliation}These are just samples of resources and there is no affiliation.}. This cost on memory requirements for \acp{NN} is a common issue with solving optimization of large-scale neural networks \citep{Bottou2018} and efforts have been made into mitigating this via alternating gradient direction methods and Bregman iteration training methods \citep{Boyd2011, Taylor2016}.


% what does difference in data volume imply for DNN. See latest Istituto Nazionale di Geofisica e Vulcanologia run


\subsection{Maturity of the Frameworks}
The NN framework results presented in this work should be considered within the context of maturity of the technique. As indicated in the literature review, FWI was originally an academic pursuit and initial inversions results were naive. Figure~\ref{fig:gauthier_fwi} shows results for FWI inversion obtained by \citet{Gauthier1986} for a circular model. Inverting for 8 shots for 5 iterations, with 400 receiver locations, the model is able to recover some structure within the velocity - Figure~\ref{fig:gauthier_fwi_legacy}. Inverting a similar model within a modern FWI framework would produce better global inversion as shown in Figure~\ref{fig:gauthier_fwi_modern}. The legacy inversion by Gauthier et al. took approximately 1 hour to execute on a CRAY-1S supercomputer \citep{Kolodzey1981}, whilst modern FWI was run on a personal computer for 2 minutes. This is a major uplift, which is backed by thirty-fives years of advances in hardware, computational modelling, mathematics, geophysics and optimization theory. Taking a similar time consideration for the NN framework, the potential for data-driven pseudo-spectral approach is still in infancy, yet able to produce improvements to a mature technique.

\begin{figure}[!ht]
	\centering
	\subbottom[Initial circular model.]{\includegraphics[width=0.32\textwidth]{gauthier_FWI_initial.pdf}}
    \subbottom[Legacy FWI from 1986.\label{fig:gauthier_fwi_legacy}]{\includegraphics[width=0.32\textwidth]{gauthier_FWI_invert.pdf}}
    \subbottom[Modern FWI.\label{fig:gauthier_fwi_modern}]{\includegraphics[width=0.32\textwidth]{gauthier_FWI_camembert.pdf}}
	\caption[Comparison between inversion for legacy FWI and modern FWI.]{Comparison between inversion results for legacy FWI and modern FWI for a circular model. 35 years of advances in hardware, computational modelling, mathematics, geophysics and optimization theory produce major uplift at the fraction of time.}
	\label{fig:gauthier_fwi}
\end{figure}

Both frameworks are still to be implemented to higher dimensions. The DNN approach would follow similarly to the work by \cite{Liu2020}. 2D receivers are modelled for an arbitrary velocity models and 2D convolutional architectures are trained to invert from the seismic data to the velocity model. Naturally, the next step would be 3D geometry. In the case of theory-guided approaches, addition of this axis is expected to provided better imaging.

% \subsection{higher dimensions}
% \subsection{2d, 3d  equivalent for learned direct}

% \begin{figure}[ht!]
%     \centering
%     \includegraphics[width=0.95\textwidth]{dnn_2d.png}
%     \caption[Outline for 2d DNN]{Outline for 2d DNN}
%     \label{fig:dnn_higher_dimensions}
% \end{figure}


\subsection{Other Areas of Deep Learning}
The intersection of Deep Learning and FWI is just starting to flourish and offers ample opportunity for new research avenues. Alternative architectures are readily available to be implemented. Some examples for DNN are shown in Figure~\ref{fig:CNN_history}. Transformers \citep{Vaswani2017} are currently state-of-the-art sequence learners and \cite{Shalova2020} developed the initial theoretical work for implementation for FWI. Alternatively, Fourier Recurrent Units \citep{Zhang2018} could be used within theory-guided FWI. These units stabilize training gradients along the temporal dimension with Fourier basis functions and would potentially resolve the gradient anomaly within our work.

The technique of Transfer Learning within Deep Learning has just starting to be considered in geophysics. Indeed, given that DNN generators are stochastic global engines, a pre-trained NN for a survey could easily be employed on another seismic survey with minimal training. \cite{Siahkoohi2019} successfully used the probability distributions of nearby surveys around an area of interest to fine-tune a pre-trained Generative Adversarial Network and be able to map low-cost, low-fidelity solutions to remove surface-related multiples and ghost from shot records and numerical dispersion from receivers.

\cite{Yadav2015} presented the topic of solving differential equations directly using NN architectures and bypassing the finite-difference approach. This framework has been found to be advantageous since (i) the solutions obtained are integrable and differentiable, thus have better interpretability, (ii) solutions are highly generalized and preserves accuracy despite very few points, and (iii) no modification is needed for different kinds of boundary conditions. Indeed, \cite{Zhu2020} developed a neural-network-based full waveform inversion method that integrates deep neural networks with FWI by representing the velocity model with a generative neural network. The velocity model generated by NN is input to conventional FWI partial differential equation solvers. The gradients of both the NN and PDEs are calculated using automatic differentiation, which back-propagates gradients through the acoustic/elastic PDEs and NN layers to update the weights and biases of the generative neural network \citep{Zhu2020}.

% % Differentiable Programming Yann Lecun
% % 	Optimizer teaching another optimizer

% Moreover, the work presented in this dissertation can be extended to \ac{RTM}. 
% % Same problem can be extended to RTM
% % 	See here: https://colab.research.google.com/drive/1BgQM5VGgyFp7Q--pAJX-vGb2bW9mcbM8

\section{Conclusion}
Data-driven and theory-guided approaches for FWI have been reviewed with a comprehensive study of literature. 
The pseudo-spectral approach via deep learning framework was identified to be lacking in any previous work and this proved to be an opportunity for development. FWI was re-cast within a DNN framework for both a data-driven and a theory-guided based formulation. Both approaches were developed theoretically, qualitatively assessed on synthetic data and tested on the Marmousi dataset. 

Elements within a classical FWI framework were shown to be substitutable with DNN components. The base architecture for the network was set to be an hour-glass neuron design. This is representative of multi-scale FWI and modern DNN approaches. Two data-generators were used to validate the framework on multi-layer models. This was tested for normalization and concluded not applicable. Fully connected layers, 1D and 2D convolutions, VGG and ResNet type architectures for Adagrad, Adadelta, RMSprop and Adam optimizer were quantitatively evaluated for computational hours, DNN training performance, validation and learning rate performance, and inversion RMSE. The best performing architecture-loss combination was identified as Conv1D-Adadelta. Conv1D architecture ranked the highest in all tests, whilst the differences in optimizer were superficial. The choice of architecture was the most important aspect as choosing a different loss optimizer would not result in deterioration of the result.

The Conv1D-Adadelta network was trained for inversion of the Marmousi model by using a 20-50 layer generator, with velocities ranging from 1450\si{ms^{-1}} to 5000 \si{ms^{-1}}. It was trained for 30 epochs, at 1,000,000 and 100,000 trace per epoch per training and testing dataset respectively. Analysis of the network kernel and bias distribution for the training and velocity and trace update per epoch confirmed that the framework is a learning process. Most of the update happen within the first few epochs, whilst additional epochs refined the inverted velocity. Multi-scale 3.5Hz classical FWI with Sobolev space norm regularization was compared to this DNN inversion. Inversion performance in shallow sections was equally good for either classical FWI or DNN approach. DNN framework performs better for deeper and over-thrust areas since DNNs are not bound by forward-modelling physical constraints from ray-tracing.

Theory-guided RNN as an analogue of FWI was implemented for 2D experiments and different wavefield components compared to an analytical 2D Green's function and time implementation. Based on these results, RNN Time is able to model the wavefield within a maximum 0.06 error tolerance and 1.74\% RPE. RNN Freq is overall more accurate with 0.05 error tolerance and 1.449\% RPE. Assessment on the gradients indicates how the adjoint state and RNN Freq gradients in general overestimate finite difference calculation, whilst RNN Time under-estimates it with an infinitesimal error. RNN Freq produced a perturbation on the onset of the gradient which was attributed to modelling artefact and could be mitigated in future versions of this approach. Based on the model size and compute available, the ideal loss was Adam with a learning rate of 2 and batch size of 1. Model batch size proved to be a limitation for practical implementations, yet RNN is computationally more efficient than the classical FWI presented in this work. RNN freq provides more stable convergence and is better performant. Overall, RNN frameworks are able to identify faults, but amplitudes are not fully inverted properly.

From the comparative analysis of the NN approaches, DNN was better performing. DNN networks recovered more of the velocity contrast, whilst RNN was better at edge definition. RNN was more suited for the shallow and depth sections due to the cleaner receiver residuals. This was only valid in the middle section of the model with the most coverage from ray-tracing. Indeed, RNN approaches had some leakage on the edges.

A critique addressing benefits and limitations of these two approaches was also included. The impact of the shift in the inversion paradigm was reviewed for both approaches. Data-driven should be considered within global approximation approaches and has potential to be used as \emph{a priori} to deterministic FWI. Research which can be easily translated was addressed in the form of probabilistic maps for the inversions and analytical accuracy upper bounds per iteration. The RNN approach benefits from the wider community of active researchers. The reduction in development time is a direct integration from Computer Science to geophysics. Vice-versa, Deep Learning frameworks can adopt strategies common to FWI. 

Data-driven inversion is still at the very early stages of development, and more research opportunity is still available. However, to truly assess the applicability and relevance of these frameworks, these approaches will have to be applied to real data in the future. The DNN model generators were shown to work within the boundaries of their parameters. Extra layers, velocity inversions and inclusions could be missed altogether if the network is not prepared correctly. Application of pre-trained networks is relatively easy and thus different geophysical model hypothesis could be assessed quickly. In either case, the DNN is robust to noise and future work would involve implementation as a de-noising techniques. Alternative modelling methods and transform spaces were also provided as alternative approaches for DNN. 

The forward modelling approach used through this work was critiqued for the use of multiples. Whether to use or not to use multiples within forward modelling is model dependent and should be evaluated for RNN Freq. Similar to classical FWI, computational power was identifiable as a limitation within these DNN frameworks. Although this is currently a limitation, it will not be in the near future due to the relative quick development of GPUs. A corollary to the whole approach was addressed in the form of the maturity of the approach. 35 years of advances applied to these frameworks would be expected to yield very good results. Finally, other areas of DNN that can be applied to FWI were presented. Alternative architectures such as Transformers and use of Fourier Recurrent Units are readily available. Potential of transfer learning and solving differential equations using NN were presented as future directions of research for these frameworks.