\chapter[Theoretical Tools]{Additional Theoretical Tools}\label{sec:app_theory}
% \section{Universal Approximation Theorem}\label{sec:app_theory_Universal_Approximation_Theorem}
\section[Difference between Time and Frequency FWI]{Difference between Time and Frequency FWI}\label{sec:app_theory_Differences_in_numerical_implementation_between_Time_and_Frequency_FWI}
To illustrate the differences within numerical implementations between time and frequency FWI, let us consider the 1D finite difference formulation of the acoustic wave equation given by:
\begin{equation}\label{eq:wave_equation_2}
    \frac{1}{c(x)^2} \frac{\partial^2 p(x,t)}{\partial t^2} - \nabla^2p(x,t) = s(x,t).
\end{equation}
Following from \cite{Igel2016}, Equation~\ref{eq:wave_equation_2} is re-arranged and re-written in dense notation as:
% \begin{equation}\label{eq:wave_equation_2_dense}
%     \frac{1}{c^2}\partial_t^2p - \partial_x^2p=s
% \end{equation}
% Re-arranging Equation~\ref{eq:wave_equation_2_dense} and since $s$ is a scalar yields:
\begin{equation}\label{eq:wave_equation_derivs}
    \partial_t^2p = c^2\partial_x^2p + s.
\end{equation}
Discretize space and time with constant increments $dx$ and $dt$ such that
\begin{align}
    x_j &= jdx, \qquad j=\left[0,l_{max}\right]\\
    t_n &= ndt, \qquad t=\left[0,n_{max}\right].
\end{align}
\begin{figure}[ht!]
    \centering
    \includegraphics[width=0.3\linewidth]{4_point_stencil.png}
    \caption[Space-Time FD discretization stencil for the 1D acoustic wave equation.]{Space-Time FD discretization stencil for the 1D acoustic wave equation. The $x$-axis corresponds to space and the $y$-axis to time. The open circle denotes the point $p(x_j,t_{n+1})$ to which the state of the pressure field is extrapolated. From \cite{Igel2016}.}
    \label{fig:4_point_stencil}
\end{figure}
Implementing a FD stencil as shown in Figure~\ref{fig:4_point_stencil}, the derivatives approximated via central finite differences and Equation~\ref{eq:wave_equation_derivs} can be written as:
\begin{equation}
    \frac{p_j^{n+1} - 2p_j^{n}+p_j^{n-1}}{dt^2} = c_j^2\left[ \frac{p_j^{n+1} - 2p_j^{n}+p_j^{n-1}}{dx^2} \right] + s_j^n,
\end{equation}
where the upper index corresponds to time discretization and the lower index corresponds to spatial discretization. Pressure at location $n+1$ based on current location $n$ and previous $n-1$ location is given by:
\begin{equation}\label{eq:wave_equation_time_step}
    p_j^{n+1} = c_j^2\left[p_j^{n+1} - 2p_j^{n}+p_j^{n-1} \right] + 2p_j^n-p_j^{n-1}+ dt^2s_j^n.
\end{equation}
Given a source wavelet $s_j^n$ at location $j$ and time $n$, an initial wavefield $p_j^0$ and initial conditions such that everything is at rest at time $t=0$, all components on the right-hand side of Equation~\ref{eq:wave_equation_time_step} are known and can be used to propagate particle motion through an acoustic medium.

Considering the Fourier pseudo-spectral implementation \citep{Gazdag1981}, Fourier theory seeks to approximate a given function by a finite sum over some $N$ orthogonal basis functions $\Phi_i$, namely:
\begin{equation}
    f(x)\approx \sum_{i=0}^N a_i \Phi_i (x),
\end{equation}
where $a_i$ are the Fourier coefficients. The main benefit of using this approximation is that given the discrete Fourier transform of functions defined on a regular grid, exact machine precision derivatives up to the Nyquist wavenumber $k_N=\frac{\pi}{dx}$ can be calculated. In particular, it can be shown that the Fourier transform operator $\mathcal{F}$ can obtain the exact $n^{th}$ derivate as:
\begin{equation}\label{eq:fourier_deriv}
    f^n(x)=\mathcal{F}^{-1} \left[(ik)^n\mathcal{F}\left[f(x)\right]\right].
\end{equation}
Substituting Equation~\ref{eq:fourier_deriv} in the $2^{nd}$ order spatial derivative of the 1D acoustic wave equation in Equation~\ref{eq:wave_equation_derivs} gives:
\begin{equation}\label{eq:2nd_derivs}
    \partial_x^2p_j^n=\mathcal{F}^{-1}\left[ (ik)^2 P_\nu^n \right]=\mathcal{F}^{-1}\left[-k^2 P_\nu^n\right],
\end{equation}
where $P_\nu^n$ is the discrete complex wavenumber spectrum at time $n$. As such, the main overall error in the numerical solution comes from only the time integration scheme. Substituting Equation~\ref{eq:2nd_derivs} in Equation~\ref{eq:wave_equation_derivs}, discretize on a FD stencil and rearranging yields:
\begin{equation}
    p_j^{n+1}=dt^2 \left[c_j^2 \mathcal{F}^{-1}\left[k^2P_\nu^n\right]+s_j^n\right]+2p_j^n-p_j^{n-1}.
\end{equation}
\section{Activation Functions}\label{sec:app_theory_Alternative_Activation_Functions}
Below are popular activations functions based on \cite{Goodfellow2016}. These are represented graphically in Figure~\ref{fig:alternate_activation_functions}.
\begin{itemize}
    \item {\bfseries Binary Step}: $f(x)=\begin{cases} 
        1, & \forall x \geq 0 \\  
        0, & \forall x < 0  
        \end{cases} $. This a simple switch on-off type function ideal for a binary classification. It is more theoretical than practical as data classifications tasks usually classify into multiple classes. Furthermore, a binary step function has a vanishing gradient, thus making it not usable with back-propagation. 
	\item {\bfseries Linear}: $f(x)=\alpha x,\forall x\in\mathbb{R}$. This is proportional to the input $x$, based on the scalar $\alpha$. The gradient is constant at $\alpha$, resulting in a linear transformation.
    \item {\bfseries Sigmoid}: $f(x)=\frac{1}{1+e^{-x}}$. This is widely used function since it is a non-linear smooth continuously differentiable function. The non-linearity enables neurons to approximate non-linear functions. The gradient is always positive, however it still has a vanishing gradient problem as $f^{'}(x)$ approaches zero for larger $x$. Furthermore, the sigmoid function is non-symmetric and connectivity between neurons is not necessarily always positive. Mitigation is achieved via scaling of the sigmoid, resulting in the Tanh function.
    \item {\bfseries Tanh}: $f(x)= \frac{e^{x}-e^{-x}}{e^{x}+e^{-x}}$. This function caries the same properties of non-linearity, smoothness and continuous differentiability of the sigmoid function, however it has the added benefit of being symmetric over the origin. This mitigates the problem of having all values of the same sign. Nonetheless, Tanh still retains the vanishing gradient problem as the function is flat with small gradients for large $x$.
    \item {\bfseries Softmax}: $f(x_i)=\frac{e^{x_i}}{\sum_{j=1}^{n}e^{x_j}}$. This function is the sigmoid function extended to multiple classes and provides a probability distribution over the predicted classes. 
	\item {\bfseries ReLU}: $f(x)=max(0, x)$. The Rectified Linear Unit is non-linear, allowing easy back-propagation of errors. When employed on a network of neurons, the negative component of the function is converted to zero and the neuron is deactivated, thus introducing sparsity with the network and making it efficient and easy for computation.
\end{itemize}

\begin{figure}[!ht]
	\centering
	\subbottom[Binary Step]{\includegraphics[width=0.32\textwidth]{act_fun_binary.png}}
    \subbottom[Linear]{\includegraphics[width=0.32\textwidth]{act_fun_linear.png}}
    \subbottom[Sigmoid]{\includegraphics[width=0.32\textwidth]{act_fun_sigmoid.png}} 

    \subbottom[Tanh]{\includegraphics[width=0.32\textwidth]{act_fun_tanh.png}}
    \subbottom[ReLU]{\includegraphics[width=0.32\textwidth]{act_fun_Relu.png}}
	\caption[Activation functions $f(x)$ and their derivative $f^{'}(x)$.]{Activation functions $f(x)$ and their derivative $f^{'}(x)$.}        
	\label{fig:alternate_activation_functions}
\end{figure}

\section{Loss Optimizers}\label{sec:app_theory_Loss_Optimizers}
To simplify notation in this section, consider objective function $\mathcal{J}(\Theta)$ parameterized by model parameters $\Theta\in\mathbb{R}^d$ to optimize gradient $\nabla_\Theta\mathcal{J}(\Theta)$ with respect to the model parameters for a learning rate $\eta$. This can be recast as:
\begin{equation}
    \Theta\equiv\Theta-\eta\nabla_\Theta\mathcal{J}(\Theta).
\end{equation}
\subsection{Adagrad}\label{sec:app_theory_Adagrad}
The Adaptive Gradient or Adagrad algorithm \citep{Duchi2011} performs learning rate updates with training based on the frequency of distribution in the data. It performs smaller updates (smaller learning rates) on parameters associated with frequently occurring features and larger updates vice-versa. 

As Adagrad uses a different learning rate for every parameter $\Theta_i$ at time step $t$, the gradient of the objective function is given by
\begin{equation}
    g_t=\sum_{i}\nabla_{\Theta_{t}} \mathcal{J}(\Theta_{t,i}) .
\end{equation}
Model updates at time $t+1$ become:
\begin{equation}\label{eq:loss_optimizer_adagrad}
    \Theta_{t+1}=\Theta_{t}-\frac{\eta}{\sqrt{G_t+\epsilon}} g_t,
\end{equation}
where $G_t$ is the sum of squares of the gradients up to time step $t$ and $\epsilon$ is a stabilizing term to avoid division by zero. This optimizer eliminates the need to manually tune the learning rate. However, the accumulation of the squared gradients in the denominator keeps growing with every update and will eventually cause the learning rate to shrink up to the point the algorithm is no longer able to acquire additional knowledge \citep{Ruder2016}.

\subsection{Adadelta}\label{sec:app_theory_Adadelta}
Adadelta \citep{Zeiler2012} extends Adagrad and aims to avoid the continual decay of the learning rates by using a decaying average of past gradients to some fixed size $w$. Equation~\ref{eq:loss_optimizer_adagrad} becomes
\begin{equation}
    \Theta_{t+1}=\Theta_t-\frac{\text{RMS}\left[\Delta\Theta\right]_{t-1}}{\text{RMS}\left[g\right]_t}g_t,
\end{equation}
where $RMS\left[g\right]_t$ is the Root Mean Squared for the gradient, $RMS\left[\Delta\Theta\right]_{t-1}=\sqrt{E\left[\Delta\Theta^2\right]_{t-1}+\epsilon}$ is the Root Mean Squared of parameter updates. Since $RMS\left[\Delta\Theta\right]_{t}$ is unknown, it is approximated until the previous time step $t-1$. This algorithm removes the need for a default learning rate.

\subsection{RMSprop}\label{sec:app_theory_RMSprop}
Similar to Adadelta, RMSprop \citep{Hinton2012} attempts to resolve Adagrad’s vanishing learning rates by maintaining a moving average of the square of gradients and divides the gradient by the root of this average. Equation~\ref{eq:loss_optimizer_adagrad} becomes:
\begin{align}
    E\left[g^2\right]_t &= 0.9E\left[g^2\right]_{t-1}+0.1g_t^2 \\
    \Theta_{t+1} &= \Theta_{t}-\frac{\eta}{\sqrt{E\left[g^2\right]_t+\epsilon}}g_t.
\end{align}

\subsection{Adam}\label{sec:app_theory_Adam}
Adaptive Moment Estimation or Adam \citep{Kingma2014} computes adaptive learning rates for each parameter based on estimations of first-order and second-order moments. 
The algorithm updates exponential moving averages of the gradient $(\hat{m}_t)$ and the squared gradient $(\hat{v}_t)$ by
\begin{align}
    \hat{m}_t &= \frac{m_t}{1-\beta_1^t} \\
    \hat{v}_t &= \frac{v_t}{1-\beta_2^t},
\end{align}
where the hyper-parameters $\beta_1, \beta_2 \in [0, 1)$ control the exponential decay rates of these moving averages. Equation~\ref{eq:loss_optimizer_adagrad} for Adam becomes:
\begin{equation}
    \Theta_{t+1}=\Theta_t - \frac{\eta}{\sqrt{\hat{v}_t}+\epsilon}\hat{m}_t.
\end{equation}

\section{Regularization}\label{sec:app_theory_Regularization}
DNN regularization is available via two strategies: (i) functional-based, and (ii) NN architecture-based. 
\subsection{Functional-based}\label{sec:app_theory_Functional-based_NN_regularization}
These type of schemes update the cost function (Equation~\ref{eq:dnn_cost_function_J}) with a regularization term, namely:
\begin{equation}
    \hat{\mathcal{J}} = \mathcal{J} + \alpha \Omega,
\end{equation}
where $\mathcal{J}$ is the original cost function, $\Omega$ is the regularization term or norm penalty and $\alpha\in\left[0,\inf\right)$ is a hyperparameter that weights the relative contribution of $\mathcal{J}$ to $\Omega$. The most commonly used norm penalty is $L^2-norm$ defined as:
\begin{equation}
    \Omega_{L^{2}}=\frac{1}{2}\left|\left|\boldsymbol{w}\right|\right|_2^2,
\end{equation}
where $\boldsymbol{w}$ are the weights throughout the DNN. This lends itself from the broader subject of regularization theory and was previously identified for FWI in §~\ref{sec:theory_regularization_fwi}. Alternative names for this style of regularization are Ridge Regression or Tikhonov regularization.

\subsection{Architecture-based}\label{sec:app_theory_NN_architecture-based_regularization}
DNN can be regularized via alterations to the NN architecture through (i) Dropout, (ii) Data augmentation, and (iii) Early Stopping.
\subsubsection{Dropout}
Dropout is a very simple ensemble-based technique \citep{Srivastava2014}. It is the process of allow only some parts of the network to be updated. Figure~\ref{fig:dropout} illustrates all sub-networks that can be formed from a simple NN that allow weight update within an epoch. Either of these sub-networks is randomly considered and trained at a given epoch and inherently remove nodal dependencies \citep{Srivastava2014}. 
\begin{figure}[ht!]
    \centering
    \includegraphics[width=0.5\linewidth]{dropout.png}
    \caption[All possible sub-networks produced from dropout.]{All 16 possible sub-networks are produced from a simple base network. A large proportion of these do not have sufficient input-to-output connections and are ignored. For wide and deeper DNNs, these types of sub-networks become insignificant since it would be unlikely to drop all paths from inputs to outputs. If such an occurrence does happen, it would only effect a single epoch and this would be mitigated with long enough training. From \cite{Goodfellow2016}.}
    \label{fig:dropout}
\end{figure}

\subsubsection{Data Augmentation}
The best way to make a DNN model generalize better for “unseen” data is to train on more data \citep{Goodfellow2016}. The amount of data available in real-world application is limited and it is reasonably straight forward to create new “fake” data. This is achieved via transformations of the original $x$ input and mapping to the correct $y$ \citep{Lee2018}. \cite{Ekbatani2017} and \cite{Le2017} show successful training of a DNN using synthetic created input-output pairs in practice.

\subsubsection{Early Stopping}
When DNN model complexity is sufficient to represent over-fitting, it is likely that the training error decreases steadily over time, but validation set error begins to rise again and makes the model worse off with any additional iteration \citep{Yao2007}. Early stopping is the technique which terminates training at a given epoch $\mathcal{E}^n$ if the errors on the validation sets at $\mathcal{E}^{n+1}$ are greater than those of $\mathcal{E}^n$. Early stopping regularization is the most widely used regularization technique as it is unobtrusive to the learning dynamics for DNN \citep{Caruana2000}.

\section{CNN Building Blocks}\label{sec:app_theory_CNN_Building_Blocks}
This section reviews the elements for CNNs referenced through-out the dissertation.
\subsection{Convolutional Layer}\label{sec:app_theory_Convolutional_Layer}
A convolutional layer differs from a normal fully connected layer through the introduction of convolutions across the neurons. The window across which convolutions are applied is referred to as a kernel or filter. Mathematically, the convolutional operation on an input image tensor $I_c$ for the $l^{th}$ layer is given by:
\begin{equation}
    f_l^k(p,q)=\sum_c\sum_{x,y} i_c(x,y) e_l^k(u,v),
\end{equation}
where $i_c (x,y)$ is an element of the input image tensor, $e_l^k (u,v)$ is the index of the $k^{th}$ convolutional kernel $k_l$, and the output or feature-map of the $k^{th}$ convolutional kernel is given by $F_l^k=\left[f_l^k(1,1),\cdots,f_l^k(P,Q)\right]$ with $P$ and $Q$ being the total number of rows and columns in $I_C$. A graphical representation of the convolutional operation is given in Figure~\ref{fig:CNN_calculation}.
\begin{figure}[ht!]
    \centering
    \includegraphics[width=0.7\linewidth]{CNN_calculation.png}
    \caption[Example of 2D convolution.]{Example of 2D convolution.}
    \label{fig:CNN_calculation}
\end{figure}

\subsection{Pooling Layer}\label{sec:app_theory_Pooling_Layer}
Once a feature-map is extracted from a convolutional layer, the exact location of one element becomes less important as long as its relative position to others is preserved \citep{Khan2020}. Sub-regions of interest can thus be inferred via reduction of dimensionality \citep{Lee2016}. There are two main types mechanisms, max and min pooling. Max pooling picks the maximum value and min pooling picks the minimum value from the assigned region. Figure~\ref{fig:CNN_calculation_max_pooling} illustrates an example of max pooling.
\begin{figure}[ht!]
    \centering
    \includegraphics[width=0.4\linewidth]{CNN_calculation_max_pooling.png}
    \caption[Example of 2D max-pooling.]{Example of 2D max-pooling.}
    \label{fig:CNN_calculation_max_pooling}
\end{figure}

Either of these pooling operations support feature extraction which is invariant to translational shifts and small distortions \citep{Ranzato2007}. These invariant features regulate the complexity of the network and reduce over-fitting. Alternative formulations include average \citep{Boureau2010}, L2 \citep{Wang2012}, overlapping and spatial pyramid pooling \citep{He2015}.

\subsection{Batch Normalization}\label{sec:app_theory_Batch_Normalization}
Input distributions to the layers in deep network may vary when weights are updated during training. Batch Normalization is used to counteract the internal covariance shift in the distribution of hidden unit values by standardising the input to zero mean and unit variance for each batch \citep{Ioffe2015}. Batch normalization for a feature-map $F_l^k$ is given by
\begin{equation}
    \mathcal{N}_l^k=\frac{\mathcal{F}_l^k - \mu_B}{\sqrt{\sigma_B^2+\epsilon}},
\end{equation}
where $\mathcal{N}_l^k$ is the normalized feature map, $\mathcal{F}_l^k$ is the input feature map, $\mu_B$ is the mean, $\sigma_B^2$ is the variance for a batch and $\epsilon$ is added for numerical stability \citep{Ioffe2015}.

\section{Common CNN Architectures}\label{sec:app_theory_Comparison_of_Common_CNN_Architectures}
\subsection{AlexNet}\label{sec:app_theory_AlexNet}
AlexNet \citep{Krizhevsky2012} is considered as the first deep neural network with 8 layers, compared to its predecessor LeNet with 5 layers \citep{LeCun1990}. In particular, it has large filters $(11\times 11, 5\times 5)$ at the initial layers, uses dropout during training, ReLU to improve convergence rates and overlapping sub-sampling and local response normalization to improve generalization. The basic architectural blueprint is given in Figure~\ref{fig:dnn_arch_comparison_alexnet}.

\subsection{VGG}\label{sec:app_theory_VGG}
VGG \citep{Simonyan2014} is a rather simplistic, homogenous network. However, it is 19 layers deep. It stacks $(3\times 3)$ filters to reduce the computational complexity, uses max pooling after the convolutional layers and padding is applied to maintain the spatial resolution. The only problem associated with VGG network are the 138 million trainable parameters, which make it computationally expensive for network training and deploying on system with low resources. The architectural blueprint is given in Figure~\ref{fig:dnn_arch_comparison_vgg}.

\subsection{ResNet}\label{sec:app_theory_ResNet}
ResNet \citep{He2016} is a 152-layers deep network shown in Figure~\ref{fig:dnn_arch_comparison_ResNet}. ResNet is 20 times deeper than AlexNet and 8 times deeper VGG. Pivotal to this network is the concept of a residual block, shown in Figure~\ref{fig:dnn_arch_comparison_resnet_identity}. This introduces the “identity map” or “skip connection” that skips one or more layers. This propagates the error deep through the network layers and allows it to learn how to minimize the residual.

\begin{figure}[ht!]
	\centering
	\subbottom[AlexNet: Five convolutional and three fully connected layers.\label{fig:dnn_arch_comparison_alexnet}]{\includegraphics[angle=-90, origin=c, width=0.31\textwidth, trim={8.6cm 5cm 8.6cm 5cm}, clip]{AlexNet_VGG_ResNet_AlexNet.pdf}}\quad
    \subbottom[VGG: Five stacked conv. layers with max pooling and 3 fully connected layers.\label{fig:dnn_arch_comparison_vgg}]{\includegraphics[angle=-90, origin=c, width=0.31\textwidth, trim={2cm 4cm 1.7cm 4cm}, clip]{AlexNet_VGG_ResNet_VGG.pdf}}\quad
    \subbottom[ResNet: 152 layer deep network with residual blocks.\label{fig:dnn_arch_comparison_ResNet}]{\includegraphics[angle=-90, origin=c, width=0.31\textwidth, trim={2cm 4cm 1.7cm 4cm}, clip]{AlexNet_VGG_ResNet_ResNet.pdf}}
    
    \subbottom[Residual block introduces an “identity map” or “skip connection” which skips one or more layers.\label{fig:dnn_arch_comparison_resnet_identity}]{\includegraphics[width=0.5\textwidth, trim={10.5cm 6.4cm 10.5cm 6.5cm}, clip]{AlexNet_VGG_ResNet_ResNetIdentity.pdf}}
	\caption[Activation functions $f(x)$ and their derivative $f^{'}(x)$.]{Architectural blueprints for (a) AlexNet, (b) VGG, (c) ResNet and (d) Residual Block for ResNet. The numbers in each layer in (a)-(c) indicate the number of trainable parameters. Adapted from \cite{Krizhevsky2012}, \cite{Simonyan2014} and \cite{He2016}.}        
	\label{fig:dnn_architecture_Comparison}
\end{figure}
\clearpage
\section{RNN Graphs and Unfolding}\label{sec:app_theory_Graphs_and_Unfolding}
RNNs are capable of learning sequences representing dynamic temporal behaviour \citep{Rumelhart1986}. These can be categorized into two broad classes: (i) finite impulse with directed acyclic graphs, and (ii) infinite impulse with directed cyclic graphs \citep{Sun2019}. A finite impulse RNN is a directed acyclic graph that can be unrolled and replaced with a strictly feed-forward NN, whilst an infinite impulse RNN is a directed acyclic graph which cannot be unrolled \citep{Sherstinsky2020}. 

When doing wavefield modelling, the wavefield at time $t$ is derived from components of the previous time $t-1$ and not future time $t+1$. Given this directionality in wave propagation, forward modelling can be mapped to a finite impulse directed acyclic graph. ``Unfolding'' of a RNN is a useful technique to visualize directed acyclic graphs. Figure~\ref{fig:RNN_Unfolding} shows part of an unfolded RNN. 

\begin{figure}[ht!]
	\centering
	\includegraphics[width=0.98\linewidth]{RNN_unfolding.png}
	\caption[RNNs can be represented as directed acyclic graphs]{RNNs can be represented as directed acyclic graphs. Chuck of the NN $A$ looks at some input $x_t$ and outputs a value $y_t$. A loop allows information to be passed from one step of the network to the next. Bias weights are omitted for clarity. Adapted from \cite{Olah2015}.}
	\label{fig:RNN_Unfolding}
\end{figure}

\section{LSTM Components}\label{sec:app_theory_Individual_RNN_Components}
\subsection{Forget gate}
The forget gate uses a sigmoid function to decide what information should be passed between hidden states. Values from this gate range between 0 and 1, indicating the level of information to be forgotten.

\subsection{Input gate}
The input gate uses a sigmoid activation function, accepts the previous hidden state and current input and decides which values will be updated. The current input and previous hidden state are passed into the tanh function to squeeze values between -1 and 1 and get a potential new candidate.
 	
\subsection{Cell state}
The cell state acts as a mechanism to transfers information through the sequence. This enables information from earlier time steps to be available at later time steps, thus reducing the effects of vanishing gradient. The preservation of gradient information by LSTM is illustrated in Figure~\ref{fig:LSTM_gradient}.

\begin{figure}[ht!]
    \centering
    \includegraphics[width=0.8\linewidth]{LSTM_gradient.png}
    \caption[LSTM preserves the gradient to deeper layers of a NN.]{``Preservation of gradient with LSTM structure. The shading of the nodes indicate the influence of the inputs at a particular point in time. The black nodes indicate maximum sensitive and the white nodes are entirely insensitive. The state of the input, forget, and output gates are displayed below, to the left and above the hidden layer respectively. In this example, all gates are either entirely open (‘O’) or closed (‘—’)." From \cite{Graves2012}.}
    \label{fig:LSTM_gradient}
\end{figure}     
 
\subsection{Output gate}
The output gate determines the next hidden state. It 
 uses a sigmoid activation on the current state and previous hidden state, and multiples this new cell state with a tanh to decide which part of the data should be pushed forward through the sequence.

\section[Equivalence of Automatic Differentiation]{Automatic Differentiation and Adjoint State}\label{sec:app_results_equivalence_AD_adjoint}
Following from \cite{Richardson2018}, consider the 1D scalar wave equation consisting of only one shot with one receiver. Using cost function in Equation~\ref{eq:dnn_cost_function_J}, the gradient with respect to one wavefield time step is
\begin{equation}
    \frac{\partial J}{\partial \mathbf{u}_t} = \frac{\partial J}{\partial \mathbf{u}_t} + \frac{\partial J}{\partial \mathbf{u}_{t+1}}\frac{\partial \mathbf{u}_{t+1}}{\partial \mathbf{u}_t}+\frac{\partial J}{\partial \mathbf{u}_{t+2}}\frac{\partial \mathbf{u}_{t+2}}{\partial \mathbf{u}_t},
\end{equation}
where $\frac{\partial \mathcal{J}}{\partial \mathbf{u}_t}$ indicates the row vector of partial derivatives of $\mathcal{J}$ with respect to elements of $\mathbf{u}_t$, while the wave speed $\mathbf{c}$ and the wavefields from other time steps $\mathbf{u_{t' \neq t}}$ are held constant. $\frac{\partial \mathcal{J}}{\partial \mathbf{u}_{t+1}}$ includes all changes to $\mathcal{J}$ caused by changes of the wavefield at a previous $\mathbf{u}_{t+1}$ and a later time step $\mathbf{u}_{t+1}$.

Considering Automatic differentiation, the gradient of the cost function with respect to the wave speed is obtained by chaining different derivates at different time steps via the chain rule. Namely, 
\begin{equation}
        \frac{\partial \mathcal{J}}{\partial \mathbf{c}} = \sum_{t=1}^{N_t}\frac{\partial \mathcal{J}}{\partial \mathbf{u}_t}\frac{\partial\mathbf{u}_t}{\partial \mathbf{c}}
        \label{eqn:accum_djdc}.
\end{equation}
Consider $N_t=4$ as an example. The steps to calculate the gradient of $\mathcal{J}$ would be
\begin{equation}
        \frac{\partial \mathcal{J}}{\partial \mathbf{u}_4}=\frac{\partial \mathcal{J}}{\partial \mathbf{u}_4},
\end{equation}
\begin{equation}
        \frac{\partial \mathcal{J}}{\partial \mathbf{u}_3}=\frac{\partial \mathcal{J}}{\partial \mathbf{u}_3}+\frac{\partial \mathcal{J}}{\partial \mathbf{u}_{4}}\frac{\partial \mathbf{u}_{4}}{\partial \mathbf{u}_3},
\end{equation}
\begin{equation}
        \frac{\partial \mathcal{J}}{\partial \mathbf{u}_2}=\frac{\partial \mathcal{J}}{\partial \mathbf{u}_2}+\frac{\partial \mathcal{J}}{\partial \mathbf{u}_{3}}\frac{\partial \mathbf{u}_{3}}{\partial \mathbf{u}_2}+\frac{\partial \mathcal{J}}{\partial \mathbf{u}_{4}}\frac{\partial \mathbf{u}_{4}}{\partial \mathbf{u}_2},
\end{equation}
\begin{equation}
        \frac{\partial \mathcal{J}}{\partial \mathbf{u}_1} =\frac{\partial \mathcal{J}}{\partial \mathbf{u}_1}+\frac{\partial \mathcal{J}}{\partial \mathbf{u}_{2}}\frac{\partial \mathbf{u}_{2}}{\partial \mathbf{u}_1}+\frac{\partial \mathcal{J}}{\partial \mathbf{u}_{3}}\frac{\partial \mathbf{u}_{3}}{\partial \mathbf{u}_1}.
\end{equation}
The required partial derivatives are given by
\begin{equation}
        \frac{\partial \mathcal{J}}{\partial \mathbf{u}_t}= 2(\delta_{x_r}^T\mathbf{u}_t - d_t)\delta_{x_r}^T,
\end{equation}
\begin{equation}
        \frac{\partial \mathbf{u}_{t+1}}{\partial \mathbf{u}_t}= \mathbf{c}^2 \Delta_t^2 \mathbf{D_x}^2 + 2,
\end{equation}
\begin{equation}
\frac{\partial \mathbf{u}_{t+2}}{\partial \mathbf{u}_t} = -1,
\end{equation}
\begin{equation}
        \frac{\partial\mathbf{u}_t}{\partial \mathbf{c}}= 2 \mathbf{c} \Delta_t^2 \left( \mathbf{D_x}^2 \mathbf{u}_{t-1} - \mathbf{f}_{t-1}\right).
\end{equation}
Substituting into Equation \ref{eqn:accum_djdc} yields
\begin{align}
        \frac{\partial \mathcal{J}}{\partial \mathbf{c}} &= \sum_{t=1}^{N_t} \left(2(\delta_{x_r}^T\mathbf{u}_t - d_t)\delta_{x_r}^T + \frac{\partial \mathcal{J}}{\partial \mathbf{u}_{t+1}}(\mathbf{c}^2 \Delta_t^2 \mathbf{D_x}^2 + 2)
        - \frac{\partial \mathcal{J}}{\partial \mathbf{u}_{t+2}}\right) \\
        &\qquad\times 2 \mathbf{c} \Delta_t^2 \left( \mathbf{D_x}^2 \mathbf{u}_{t-1} - \mathbf{f}_{t-1}\right)\nonumber\\
        &= \sum_{t=1}^{N_t} \left(\mathbf{c}^2 \Delta_t^2 \left(\mathbf{D_x}^2\frac{\partial \mathcal{J}}{\partial \mathbf{u}_{t+1}} + \frac{2}{\mathbf{c}^2 \Delta_t^2} (\delta_{x_r}^T\mathbf{u}_t - d_t)\delta_{x_r}^T\right)\right. \\
        &\left.\qquad + 2\frac{\partial \mathcal{J}}{\partial \mathbf{u}_{t+1}}- \frac{\partial \mathcal{J}}{\partial \mathbf{u}_{t+2}}\right)2 \mathbf{c} \Delta_t^2 \left( \mathbf{D_x}^2 \mathbf{u}_{t-1} - \mathbf{f}_{t-1}\right)\\
        &= \sum_{t=1}^{N_t} -F^{-1}\left(\frac{2}{\mathbf{c}^2 \Delta_t^2}(\delta_{x_r}^T\mathbf{u}_t - d_t)\delta_{x_r}^T\right)
        2 \mathbf{c} \Delta_t^2 \left( \mathbf{D_x}^2 \mathbf{u}_{t-1} - \mathbf{f}_{t-1}\right),
\end{align}
where $F^{-1}(\mathbf{g}_t)$ is wave propagation backward in time of the source amplitude $\mathbf{g}_t$. Recognize that the wave equation can be used to express the factor on the right as the second time derivative of the forward propagated source wavefield,
\begin{align}
        \frac{\partial \mathcal{J}}{\partial \mathbf{c}} &=\sum_{t=1}^{N_t}-F^{-1}\left(\frac{2}{\mathbf{c}^2 \Delta_t^2} (\delta_{x_r}^T\mathbf{u}_t - d_t)\delta_{x_r}^T\right)\frac{2\Delta_t^2}{\mathbf{c}}\frac{\partial^2\mathbf{u}_{t-1}}{\partial t^2}\\
        &=\sum_{t=1}^{N_t}-F^{-1}\left(2 (\delta_{x_r}^T\mathbf{u}_t - d_t)\delta_{x_r}^T\right)\frac{2}{\mathbf{c}^3}\frac{\partial^2\mathbf{u}_{t-1}}{\partial t^2},
\end{align}
where the linearity of wave propagation is used to take factors out of the source amplitude of the left term. The result is the same equation that is used in the adjoint state method.