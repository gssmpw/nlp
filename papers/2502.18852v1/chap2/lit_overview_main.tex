\chapter{Literature Review}
\textbf{In this chapter the main literature concerning FWI is discussed in the context of pseudo-spectral approaches, with focus on pointing out the general research trends in this area. \acp{NN} are then presented with respect to inverse problems, and their development history discussed. Both these components are put together to identify applications within geophysics, with emphasis on velocity inversion approaches.}

\section{Full Waveform Inversion}\label{sec:lit_rev_fwi}
\ac{FWI} tries to derive the best velocity model and other lithologic properties (as density, anelastic absorption and anisotropy) of the Earth’s subsurface to be consistent with recorded data. An exhaustive search for this ideal model is almost impossible and methods for finding an optimal one describing the data space are necessary. There are two main categories for dealing with this problem: (i) global optimization methods, and (ii) direct solving through linearisation.

Global optimization methods use stochastic processes to try and find the global minimum of the misfit function \citep{Torn1989, Sen1995}. Three most well-known cases of global methods are Monte Carlo methods \citep{Press1968, Biswas2017}, genetic algorithm \citep{Gerstoft1994, Parker1999, Tran2012} and simulated annealing \citep{Rothman1985, Pullammanappallil1994, Tran2011}. Global optimization methods are all very dependent on a fast forward modelling algorithm as they require large amounts of forward modelling calculations. As computers are getting faster and better, the necessity of keeping the parametrization simple might decline. However, current solutions to the seismic inverse problem have to resort to local optimization. The next section reviews direct solving through linearisation for FWI.

% Global methods try to find global minimum of the misfit function. These are stochastic in nature and use global information about the misfit surface to perform model updates \citep{Torn1989, Sen1995}. Three most well-known cases of global methods are:
% \begin{enumerate}
% 	\item \textbf{Monte Carlo methods}: These are pure random search methods in which models are drawn stochastically from the total model space, forward modelled and the model with lowest cost function is utilized \citep{Press1968, Biswas2017}.
% 	\item \textbf{Genetic Algorithm methods}: These are based on analogues of biological evolution where a relatively small selection of models is chosen from the model space \citep{Holland1975}. The best models of these parent selection form new child models by cross-over and mutation of the parameters describing the model. The children will then replace the weakest models in the selection. By such iterative steps, the selection of models will gather towards an optimal model \citep{Gerstoft1994, Parker1999, Tran2012}.
% 	\item \textbf{Simulated Annealing methods}: These are based on analogues of physical annealing processes modelled in statistical mechanics \citep{Kirkpatrick1983, Geman1984}. Model parameters are randomly perturbed and updated model evaluated and assessed whether to be accepted or not. Only better models are propagated forward and the possibility of a perturbation being accepted decreases. If convergence exists, the model will iteratively become better \citep{Rothman1985, Pullammanappallil1994, Tran2011}.
% \end{enumerate}

% Global optimization methods are all very dependent on a fast forward modelling algorithm as they require large amounts of forward modelling calculations. As computers are getting faster and better, the necessity of keeping the parametrization simple might decline. However, current solutions to the seismic inverse problem have to resort to local optimization.

\subsection[FWI as Local Optimization]{Formulation of FWI as Local Optimization}\label{sec:fwi_local_opt}
The concept of local optimization for \ac{FWI} was introduced in the 1980’s. \citet{Lailly1983} and \citet{Tarantola1984a} cast the exploding-reflector concept of \citet{Claerbout1971, Claerbout1976} as a local optimization problem which aims to minimise in a least-squares sense the misfit between recorded and modelled data \citep{Virieux2009}. The problem is set in the time-domain as follows: set a forward propagation field to model the observed data, back propagate the misfit between the modelled and the observed data, cross-correlate both fields at each point in space to derive a correction, and do least squares minimization of the residuals iteratively. This outline forms the basis of this technique to this day.

\cite{Gauthier1986} numerically demonstrate a local optimization FWI approach using two-dimensional synthetic data examples. A single diffracting point on a homogeneous model was used to illustrate the importance of proper sampling of the subsurface. Furthermore, this model was used to show that the free surface adds an extra complexity to the problem and increases the non-linearity of the inversion. FWI with or without free-surface multiple modelling is an active area of research to this day \citep{Komatitsch2002,Bergen2019}. 
% A homogeneous model with two point diffractors of equal amplitudes was used to demonstrate the importance of preconditioning the gradient to accelerate the convergence and correctly recover amplitudes. Another synthetic model with a circular perturbation of a diameter larger than the wavelength showed the importance of the accuracy in the starting model. When the starting model falls within a global minimum, the predicted data converges to a true solution, otherwise the inversion results were cycle-skipped and convergence occurs within local minima.

\subsection{Applications in Time and Frequency}
One of the pioneering applications of FWI was presented by \cite{Bunks1995} and is represented in Figure~\ref{fig:first_practical_fwi}. They showed better imaging using a hierarchical multi-scale approach on the Marmousi synthetic model. This strategy initially inverts for low-frequency components where there are fewer local minima, and those that exist are sparser than if for higher frequencies. However, decomposing by scale did not resolve issues of source estimation, source bandwidth and noise \citep{Bunks1995}. In the 1990s, Pratt and his associates proposed FWI via the pseudo-spectral domain \citep{Pratt1990a, Pratt1990b, Pratt1991}. The initial application was to cross-hole data utilizing a finite difference approach and an elastic wave propagator to facilitate the modelling of multi-source data. This was extended to wide-aperture seismic data by \cite{Pratt1996}. Analytically, the time- and frequency-domain problems are equivalent \citep{Virieux2009}. Initial attempts of pseudo-spectral FWI include application to the Marmousi model \citep{Sirgue2004} and land seismic dataset \citep{Operto2004}.

\begin{figure}[!ht]
	\centering
	\subbottom[Section from real Marmousi velocity model.]{\includegraphics[width=0.45\textwidth]{BUNKS_true_2}}
	\subbottom[Best estimate using multiscale FWI.]{\includegraphics[width=0.45\textwidth]{BUNKS_best_guess_2}}
	\caption[First practical application of FWI using the Marmousi model]{First practical application of FWI using the Marmousi model. This shows significant improvements for the FWI results as presented by \cite{Bunks1995}.}        
	\label{fig:first_practical_fwi}
\end{figure}

\subsection{Beyond Academic Experiments}\label{sec:lit_rev_beyond_academic_exp}
Theoretically, two-dimensional inversion is only able to explain out-of-plane events by mapping them into in-plane artefacts \citep{Morgan2009}. This meant that FWI restricted to purely academic pursuits \citep{Sirgue2009} and full potential could only be realized if extended to three-dimensions. The first 3D frequency-domain algorithms where developed by \citet{Warner2007} on synthetic datasets, however these used low initial frequencies that are not normally present in real data \citep{Morgan2013}. Examples of this application are demonstrated by \cite{Sirgue2007}, \cite{Ali2007} and \cite{Operto2007}. \cite{Warner2008c} presented the first 3D real data application to a shallow North Sea survey. This improved the resolution of shallow high-velocity channels that resulted in uplifts upon migration. 
% Near-offset, steep reverse faults that segmented the target horizon were clearly identifiable on migrated sections which otherwise were not visible from conventional velocity models.
In Figure~\ref{fig:fwi_prestack_improvements}, \citet{Sirgue2009} demonstrated successful FWI results for a 3D dataset of the Valhall field, Norway. They inverted wide-azimuth ocean-bottom cable data using a sequence of low frequency bands to generate high-resolution velocity models. The updated velocity model demonstrated a network of shallow high-velocity channels and a gas-filled fracture extension from a gas cloud which was not previously identifiable \citep{Sirgue2010}.

\begin{figure}[!ht]
	\centering
	\subbottom[Velocitiy model. Top: Conventional, Bottom: FWI.]{\includegraphics[width=0.45\textwidth]{SIRGUE_2010_valhall_FWI_a}}
	\subbottom[Pre-stack depth migrated section. Top: Conventional, Bottom: FWI.]{\includegraphics[width=0.45\textwidth]{SIRGUE_2010_valhall_FWI_b}}%
	\caption[Improvements in velocity model and pre-stack depth migrated images obtained through FWI over the Valhall field.]{Improvements in velocity model and pre-stack depth migrated images obtained through FWI over the Valhall field. The FWI updated velocity model demonstrated a network of shallow high-velocity channels and a gas-filled fracture extension from a gas cloud which was not previously identifiable in conventional tomography. The impact is evident in the migrated sections, which show more continues events in otherwise poorly illuminated area. Adapted from \cite{Sirgue2009} and \cite{Sirgue2010}.}        
	\label{fig:fwi_prestack_improvements}
\end{figure}

\citet{Plessix2010} show results from the application of full waveform inversion to ocean bottom data recorded in the Gulf of Mexico with near-ideal long-offset and wide-azimuth. Their approach was anisotropic and assumed vertical transversely isotropic media with fixed Thomsen’s parameters. The model had better imaging of dips and produced flatter common image gathers in the deep part of the model \citep{Plessix2010}. \cite{Wang2016} developed 3D waveform inversion for orthorhombic media in the acoustic approximation using pseudo-spectral methods. This was found to be stable and produced kinematic accurate pure-mode primary wavefields with an acceptable computational cost. \cite{Xie2017} applied orthorhombic full-waveform inversion for imaging wide-azimuth ocean-bottom-cable data. The results had better azimuthal and polar direction-dependent wave imaging which significantly improved fault imaging -  See Figure~\ref{fig:ortho_fwi}.

A re-occurring theme within this section is the creation of a better approximation to the wavefield propagation within the subsurface; 1D to 2D to 3D discretization, acoustic to anisotropic to elastic to orthorhombic wavefield modelling, with each additional dimension of information resulting in more numerical and computer intensive algorithms \citep{Kumar2012}. Even though computing power has increased dramatically, making FWI more productive, the underlying algorithms are only improving incrementally. Indeed, the next generation of experiments will require changes to acquisition geometry to allow for full-bandwidth and multi-azimuth reconstruction of the wavefield \citep{Morgan2016}.

\begin{figure}[!ht]
	\centering
	\subbottom[PSDM stack and CDP gather with original orthorhombic model from tomography.]{\includegraphics[width=0.9\textwidth]{XIE_initial}}
	\subbottom[PSDM stack and CDP gather with orthorhombi FWI update.]{\includegraphics[width=0.9\textwidth]{XIE_fwi}}
	\caption[Imaging improvements obtained through orthorhombic imaging.]{Imaging improvements obtained through orthorhombic imaging. This produces sharp truncations and clearer faults as highlighted by the red dashed ovals, as well as better focussed gathers. Adapted from \cite{Xie2017}.}        
	\label{fig:ortho_fwi}
\end{figure}

\section{Deep Neural Networks}

\subsection{Neural Networks for Inverse Problems}
The mathematical formulation of FWI falls under the more general class of variational inverse problems \citep{Tanaka2003}. The aim is to find a function which is the minimal or the maximal value of a specified functional \citep{Dadvand2006}. Indeed, inverse problems attempt to reconstruct an image $x \in X \subseteq \mathbb{R}^d$ from a set of measurements $y \in Y \subseteq \mathbb{R}^m$ of the form
\begin{equation}\label{eq:inverse_theory}
	y=\Gamma(x)+\epsilon
\end{equation}
where $\Gamma: X \mapsto Y, \Gamma \in \mathbb{R}^{m\times d}$ is the discrete operator and $\epsilon \in Y \subseteq \mathbb{R}^{m}$ is the noise. \ac{NN} within Machine Learning can be considered to be a set of algorithms of non-linear functional approximations under weak assumptions \citep{Oktem2018}. Namely, when applied to inverse problems, Equation~\ref{eq:inverse_theory} can be re-phrased as the problem of reconstructing a non-linear mapping $\Gamma_\theta^{\dagger}:Y \mapsto X$ satisfying the pseudo-inverse property
\begin{equation}\label{eq:learned_inverse}
	\Gamma_\theta^{-1}(y) \approx x
\end{equation}
where observations $y\in Y$ are related to $x\in X$ as in Equation~\ref{eq:inverse_theory}, and $\theta$ represents the parametrization of pseudo-inverse by the \ac{NN} learning \citep{Adler2017a}. The loss function defined in Equation~\ref{eq:learned_inverse} is dependent on the type of training data, which is dependent on the learning approach \citep{Adler2017b}.  There are two main classes of learning in Machine Learning: (i) Supervised, and (ii) Unsupervised.

% \subsection{Supervised learning}
In supervised learning, training data are independent distributed random pairs with input $x\in X$ and labelled output $y \in Y$ \citep{Vito2005}. Estimating $\theta$ for Equation~\ref{eq:learned_inverse} can be formulated as minimizing a loss function $\mathcal{J}(\theta)$ which has the following structure \citep{Adler2017a}:
\begin{equation}\label{eq:supervised_learning}
	\mathcal{J}(\theta) := \mathcal{D}(\Gamma_\theta^{-1}(\boldsymbol{y}), \boldsymbol{x})
\end{equation}
where $\mathcal{D}$ is a distance function quantifying the quality of the reconstruction and $\Gamma_\theta^{-1}:Y\mapsto X$ is the pseudo-inverse to be learned \citep{Adler2017a}. A common metric for the distance function is the sum of squared distances, resulting in:
\begin{equation}
\mathcal{J}(\theta) := \left|\left|\Gamma_\theta^{-1}(y)- x\right|\right|_X^2
\end{equation}
Approaching the inverse problem directly via this approach amounts to learn $\Gamma_\theta^{-1}:Y\mapsto X$ from data such that it approximates an inverse of $\Gamma$. In particular, this has successful applications in medical imaging \citep{Xu2012, Lucas2018}, signal processing \citep{Rusu2017, Dokmanic2016} and regularization theory \citep{Meinhardt2017, Romano2016, DelosReyes2017}.

% \subsection{Unsupervised learning}
In unsupervised learning, there exist no input-output labelled pairs and the training data is solely elements of $y\in Y$. The \ac{NN} is required to learn both the forward problem and inverse problem \citep{Andrychowicz2016}. The loss functional for unsupervised learning is given as:
\begin{equation}
	\mathcal{J}(\theta) := \mathcal{L}\left(\Gamma\left(\Gamma_\theta^{-1}(y)\right),x \right)+\mathcal{S}\left(\Gamma_\theta^{-1}(g)\right)
\end{equation}
where $\mathcal{L}:Y\times X \mapsto\mathbb{R}$ is a suitable affine transformation of the data and $\mathcal{S}:X\mapsto \mathbb{R}$ is the regularization function. Main applications of this learning are to inherent structure and have been proven successful in exploratory data analysis applications such as clustering \citep{Sever2015,Gerdova2002} and dimension reduction \citep{Dolenko2015}.

% \begin{figure}[!ht]
% 	\centering
% 	\subbottom[Supervised learning overview]{\includegraphics[width=0.72\textwidth]{Learning_Type_Supervised.png}}
% 	\subbottom[Unupervised learning overview]{\includegraphics[width=0.72\textwidth]{Learning_Type_Unsupervised.png}}%
% 	\caption[Supervised and unsupervised learning types.]{Supervised and unsupervised learning types. Adapted from \cite{Ma2018}.}        
% 	\label{fig:learning_types}
% \end{figure}

\subsection{Evolution of Neural Networks}\label{sec:evo_NN}
The remaining literature review is restricted to supervised learning approaches using NN as these are more suited for velocity inversion. For a complete review, \cite{Lippmann1987} and \cite{Chentouf1997} provide further detail. 
% This section proceeds with the development of feed-forward networks, overview of \ac{DNN} landscape and architecture type (§\ref{sec:dnn_arch}) and finally investigate applications to Geophysical applications (§\ref{sec:app_geophysics}).

\subsubsection{Early Neural Nets and the Perceptron}\label{sec:early_nn}
The basic ideas of \ac{NN} date back to the 1940’s and were initially devised by \citet{McCulloch1943} when trying to understand how to map the inner workings of a biological brain into a machine. From a biological aspect, neurons in the brain are interconnected via nerve cells that are involved in the processing and transmitting of chemical and electrical signals \citep{McCulloch1943}. 
% A simplified schematic of the biological brain is illustrated in Figure~\ref{fig:brain_neuron}.

% \begin{figure}[ht!]
% 	\centering
% 	\includegraphics[width=0.7\linewidth]{brain_neuron_2}
% 	\caption[This is the short caption for List of Figures]{Schematic of the biological brain neuron. The main components are the Axons (transmission lines), Dendrites (receptors) and the neuron cell body. These are linked together from previous neurons to form a neural network. From \cite{Preetham2016}.}
% 	\label{fig:brain_neuron}
% \end{figure}

Early \acp{NN} with rudimentary architectures did not learn \citep{McCulloch1943} and the notion of self-organized learning only came about in 1949 by \citet{Hebb1949}. \cite{Rosenblatt1958} extended this idea of learning and proposed the first and simplest neural network – the McCullock-Pitts-Perceptron. As shown in Figure~\ref{fig:perceptron}, this consists of a single neuron with weights and an activation function. The weights are the learned component and determine the contribution of either input $x$ to the output $y$. The activation function $\sigma$ adds a non-linear transform, allowing the neuron to decide if the input is relevant for the paired output. Without an activation function, the neuron would be equivalent to a linear regressor \citep{Minsky2017}. \cite{Rosenblatt1958} used this fundamental architecture to reproduce a functional mapping that classifies patterns that are linearly separable. This machine was an analogue computer that was connected to a camera that used 20×20 array of cadmium sulphide photocells to produce a 400-pixel image. Shown in Figure~\ref{fig:perceptron_mk1}, the McCullock-Pitts-Perceptron had a patch-board that allowed experimentation with different combinations of input features wired up randomly to demonstrate the ability of the perceptron to learn \citep{Hecht-Nielsen1990, Bishop2006}.

\begin{figure}[ht!]
	\centering
	\includegraphics[width=0.75\linewidth]{perceptron_v2.png}
	\caption[The Single Neuron Perceptron]{The Single Neuron Perceptron. The input values are multiplied by the weights. If the weighted sum of the product satisfies the activation function, the perceptron is activated and ``fires'' a signal. Adapted from \cite{Rosenblatt1958}.}
	\label{fig:perceptron}
\end{figure}

\begin{figure}[ht!]
	\begin{minipage}[c]{0.4\textwidth}
		\centering
		\includegraphics[width=\textwidth]{Mark_I_perceptron}
	\end{minipage}\hfill
	\begin{minipage}[c]{0.55\textwidth}
		\caption[Mark I Perceptron Machine]{The Mark I Perceptron Machine was the first machine used to implement the Perceptron algorithm. The machine was connected to a camera that used 20×20 array of cadmium sulphide photocells to produce a 400-pixel image. To the right is a patch-board that allowed experimentation with different combinations of input features. This was usually wired up randomly to demonstrate the ability of the perceptron to learn. Adapted from \cite{Hecht-Nielsen1990, Bishop2006}.}
		\label{fig:perceptron_mk1}
	\end{minipage}
\end{figure}

Rosenblatt’s perceptron was the first application of supervised learning \citep{Russell2008}. However, \citet{Minsky1969} highlight limitations to the applications of a single perceptron. They also point out that Rosenblatt’s claims that the ``perceptron may eventually be able to learn, make decisions, and translate languages'' were exaggerated. There was no other follow ups on this work by Minsky and Papert, and research on perceptron-style learning machines practically halted \citep{Minsky2017}. 

\subsubsection{Back-Propagation and Hidden Layers}
Efficient error back-propagation in NN networks were described in Linnainmaa’s master thesis \citep{Linnainmaa1970}. This minimizes the errors through gradient descent in the parameter space \citep{Hadamard1907} and allows for explicit minimization of the cost function. Back-propagation permits \acp{NN} to learn complicated multidimensional functional mappings \citep{Dreyfus1973}. 

The back-propagation formulation lends itself from major developments in dynamic programming throughout the 1960s and 1970s \citep{Kelley1960, Bryson1961, Linnainmaa1976}. A simplified derivation using the chain rule was derived by \cite{Dreyfus1973} and the first NN-specific application was described by \cite{Werbos81}. It was until the mid-1980s that \cite{Rumelhart1986} made back-propagation mainstream for \acp{NN} through the numerical demonstration of internal representations of the hidden layer. Hidden layers reside in-between input and output layers of the NN.

Back-propagation was no panacea and additional hidden layers did not offer empirical improvements \citep{Schmidhuber2015}. \cite{Kolmogoro1956}, \cite{Hecht-Nielsen1989} and \cite{Hornik1989} pursued development of back-propagation encouraged by the Universal Approximation Theorem. Namely, this theorem states that if enough hidden units are used in a NN layer, this can approximate any multivariate continuous function with arbitrary accuracy \citep{Hecht-Nielsen1989}.
% Numerous improvements to back-propagation have been proposed; least-squares/Gauss-Newton/Levenberg-Marquardt methods \citep{Gauss1809,Newton1687, Levenberg1944}, quasi-newton/Broyden-Fletcher-Goldfarb-Shanno (BFGS) methods \citep{Broyden1965, Fletcher1963}, Partial BFGS \citep{Battiti1992}, conjugate gradient \citep{Hestenes1952}. A full historic recollection of improvements to NN have been described by  \cite{Montavon2012}. 
Although back-propagation theoretically allows for deep problems, it was shown not to work on practical problems \citep{Schmidhuber2015}.

\subsubsection{The Vanishing Gradient and Renaissance of Machine Learning}
The major milestone in NN came about in 1991. Hochreiter’s thesis identified that deep \acp{NN} suffer from the vanishing or exploding gradient problem \citep{Hochreiter1991}. Gradients computed by back-propagation become very small or very large with added layers, causing convergence to halt or introduce unstable update steps. Solutions proposed to address this challenge included batch normalization \citep{Ioffe2015}, Hessian-free optimisations \citep{Moller1993, Schraudolph2002, Martens2010}, random weight assignment
\citep{Hochreiter1996}, universal sequential search \citep{Levin1973} and weight pruning \citep{LeCun1990}. 

Prior to 2012, NN were apparently an academic pursuit. This changed when AlexNet \citep{Krizhevsky2012} won the ImageNet \citep{Russakovsky2015} visual object recognition by a considerable margin. AlexNet used a deep architecture consisting of eight layers \citep{Krizhevsky2015} and was the only entry employing NN in 2012. All submissions in subsequent years were NN-based \citep{Singh2015} and in 2015, \acp{NN} surpassed human performance in visual object recognition for the first time \citep{Russakovsky2015} - see Figure~\ref{fig:evolution_imagenet}. AlexNet is undoubtedly a pivotal event that ignited the renaissance in interest around deep learning.

\begin{figure}[ht!]
	\centering
	\includegraphics[width=0.9\linewidth]{evolution_imagenet}
	\caption[Evolution of accuracy for the ImageNet challenge.]{Evolution of the accuracy for the ImageNet challenge \citep{Krizhevsky2012}. AlexNet won ImageNet in 2012 with 16.4\% error in accuracy. With each year of the competition, the accuracy has been increasing. Sources for these accuracies are Clarifia \citep{Zeiler2014}, VGG-16 \citep{Simonyan2014}, GoogleLeNet-19 \citep{Szegedy2014}, ResNet-152 \citep{He2016}, GoogleLeNet-v4 \citep{Szegedy2016} and SENet \citep{Hu2017}.}
	\label{fig:evolution_imagenet}
\end{figure}

\subsection{Deep Neural Network Architecture Landscape}\label{sec:dnn_arch}
According to \citet{Patterson2017}, three of the most common major architectures are (i) Neural Network, (ii) \ac{CNN}, and (iii) \ac{RNN}. 

As discussed in §~\ref{sec:early_nn}, neural networks are non-linear models inspired by the neural architecture of the brain in biological systems. A typical neural network is known as a multi-layer perceptron and consists of a series of layers, composed of neurons and their connections \citep{Goodfellow2016}. 

\acp{CNN} are regularized version of MLPs with convolution operations in place of general matrix multiplication in at least one of the layers \citep{LeCun1989}. These types of networks find their motivation from work by \citet{Hubel1959, Hubel1962}. Inspired by this work, \citet{Fukushima1982} introduced convolutional layers and downsampling layers, \citet{Zhou1988} developed max pooling layers and \cite{LeCun1990} used back-propagation to derive the kernel coefficients for convolutional layers. The architecture of the NN used by LeCun et al. is known as LeNet5 and is shown in Figure~\ref{fig:CNN_arch} for the classification of hand-written digits. This essentially laid the foundations for modern CNNs. \cite{LeCun2010} gives a comprehensive history up to 2010 and a more recent review is available by \cite{Khan2020}.

\begin{figure}[ht!]
	\centering
	\includegraphics[width=0.95\linewidth]{LeNet5.png}
	\caption[LeNet5 CNN architecture to classify handwritten characters.]{LeNet5 - LeCun et al.’s (1990) CNN architecture used to classify handwritten digits. This consists of two sets of convolutional and max pooling layers, followed by a flattening convolutional layer, then two fully-connected layers and finally a soft-max classifier.}
	\label{fig:CNN_arch}
\end{figure}

\acp{RNN} are in the family of feed-forward neural networks that have recurrent connections or allow for parameter sharing \citep{Lang1988}. These recurrent connection allow for input activations to pass from the hidden nodes in previous states to influence the current input \citep{Waibel1989, Lang1990}. 
% This is known as unrolling a RNN and is shown in Figure~\ref{fig:RNN_arch}.
% \clearpage
% \begin{figure}[ht!]
% 	\centering
% 	\includegraphics[width=0.95\linewidth]{unrolled_RNN.png}
% 	\caption[An unrolled RNN]{An unrolled RNN showing the recurrent connections between RNN cells $A$ for some input $x_t$ and outputs value $h_t$. From \citet{Olah2015}.}
% 	\label{fig:RNN_arch}
% \end{figure}
\ac{LSTM} networks are one of the most commonly used variations of RNNs \citep{Patterson2017}. These were introduced by \citet{Hochreiter1997} and add the concept of memory gates or cells \citep{Graves2012, Gers1999}. These gates allow for information to be accessible across different time-steps and thus attenuates the vanishing gradient problem present with most RNN models \citep{Patterson2017}.

% \subsubsection[Artificial Neural Networks]{Artificial Neural Networks}
% As introduced and discussed in §~\ref{sec:early_nn}, ANNs are non-linear models inspired by the neural architecture of the brain in biological systems. A typical neural network is known as a multi-layer perceptron and consists of a series of layers, composed of neurons and their connections \citep{Goodfellow2016}.
% \subsubsection[Unsupervised Pre-trained Networks]{Unsupervised Pre-trained Networks}

% There are three specific architectures within UPNs:
% \begin{itemize}
% 	\item Autoencoders
% 	\item \acp{DBN}
% 	\item \acp{GAN}
% \end{itemize}

% Autoencoders are a variant of neural networks which have an extra bias for calculating the error of reconstruction of the original output \citep{Patterson2017}. They have been part of the historical landscape of NN since inception by \citet{LeCun1987}. Autoencoders consist of two parts, an encoder function and a reconstruction decoder \citep{Goodfellow2016}  – see Figure~\ref{fig:autoencoder_arch}. These NN are trained to prioritize aspects of the input data and learn useful properties and underlying patterns \citep{LeCun1987}. Indeed, the main use case for these \acp{NN} are for unsupervised feature extraction as the NN only uses the original input for learning weights rather than backpropagation \citep{Bourlard1988}. Modern autoencoders have generalized the initial idea of an encoder-decoder pair beyond deterministic functions to stochastic mappings \citep{Hoffman2013}. 


% \begin{figure}[ht!]
% 	\centering
% 	\includegraphics[width=0.6\linewidth]{DNN_Landscape_0_AutoEncoder.png}
% 	\caption[General structure of an autoencoder]{General structure of an autoencoder mapping an input to a reconstruction through an internal representation.}
% 	\label{fig:autoencoder_arch}
% \end{figure}

% \acp{DBN} are built using layers of \acp{RBM} \citep{Smolensky1986} for the pre-train phase and a feed-forward network for the fine-tune phase \citep{Hinton2006, Hinton2007} - see Figure~\ref{fig:RBM_arch}. Boltzmann machines were originally introduced as a general connection approach to learn arbitrary probabilistic distributions over binary vectors \citep{Fahlman1983, Ackley1985, Hinton1984}. RBMs are similar to ANNs, yet some connections between any two visible units or any two hidden units is not allowed (hence the name Restricted”) - see Figure~\ref{fig:DBM_arch}.

% \begin{figure}[!ht]
% 	\centering
% 	\subbottom[RBM architecture showing no direct interactions between any two visible units or between any two hidden units. On the contrary, a general Boltzmann machine may have arbitrary connections. Adapted from \cite{Goodfellow2016}.\label{fig:RBM_arch}]{\includegraphics[width=0.6\textwidth]{DNN_Landscape_1_RBMs_RBM}}
% 	\newline
% 	\subbottom[DBN architecture showing stacking of RBM layers. Adapted from \cite{Kizrak2017}.\label{fig:DBM_arch}]{\includegraphics[width=0.95\textwidth]{DNN_Landscape_1_RBMs_DBM}}
% 	\caption[General structure of DBNs.]{General structure of DBNs.}        
% 	\label{fig:DBN_arch_overview}
% \end{figure}

% \acp{GAN} are network types which use unsupervised learning to train two models in parallel (a discriminator and generator network) in a game theoretic scenario of a zero-sum game using data distributions \citep{Goodfellow2014}. The generator network generates samples while the discriminator evaluates the output (Figure~\ref{fig:GAN_arch}). Typically, the generative network learns to map from a latent space to a data distribution of interest, while the discriminative network distinguishes candidates produced by the generator from the true data distribution. The generative network's training objective is to increase the error rate of the discriminative network \citep{Goodfellow2014, Luc2016}.

% \begin{figure}[ht!]
% 	\centering
% 	\includegraphics[width=0.6\linewidth]{DNN_Landscape_2_GANs.png}
% 	\caption[General structure of a GAN]{General structure of a GAN. Adapted from \cite{Goodfellow2014}.}
% 	\label{fig:GAN_arch}
% \end{figure}

% \subsubsection[Convolutional Neural Networks]{Convolutional Neural Networks}
% \acp{CNN} are a specialized kind of neural network for processing data that has a known grid-like topology \citep{Goodfellow2016}. These are regularized version of MLPs with convolution operations in place of general matrix multiplication in at least one of their layers \citep{LeCun1989}. These types of networks find their motivation from work by Hubel and Wiesel in the 1950s and 1960s on visual cortexes of cats and monkeys \citep{Hubel1959, Hubel1962}. Inspired by this work, \citet{Fukushima1982} introduced the two basic types of layers in CNNs: convolutional layers and downsampling layers. Zhou \& Chellappa introduced the concept of max pooling \cite{Zhou1988} and \cite{LeCun1990} utilized back-propagation to learn the convolution kernel coefficients directly from images of hand-written numbers - see Figure~\ref{fig:CNN_arch}. This essentially laid the foundations for modern CNNs. For a comprehensive history up to 2010, readers are guided to \cite{LeCun2010} and a more recent review is available by \cite{Khan2020}.

% \begin{figure}[ht!]
% 	\centering
% 	\includegraphics[width=0.9\linewidth]{LeNet5.png}
% 	\caption[LeNet5 - LeCun et al.'s (1990) CNN architecture used to classify handwritten and machine-printed characters.]{LeNet5 - LeCun et al.’s ( 1990) CNN architecture used to classify handwritten and machine-printed characters. LeNet-5 architecture consists of two sets of convolutional and max pooling layers, followed by a flattening convolutional layer, then two fully-connected layers and finally a soft-max classifier.}
% 	\label{fig:CNN_arch}
% \end{figure}

% \subsubsection[Recurrent Neural Networks]{Recurrent Neural Networks}
% \acp{RNN} are in the family of feed-forward neural networks that add the concept of recurrent connections or parameter sharing \citep{Lang1988}. At each time-step of sending input through a RNN, nodes receiving input along recurrent edges receive input activations from the current input vector and from the hidden nodes in the network’s previous state \citep{Waibel1989}. The output is computed from the hidden state at the given time-step. The previous input vector at the previous time step can influence the current output at the current time-step through the recurrent connections \citep{Lang1990} - see Figure~\ref{fig:RNN_arch}.

% \ac{LSTM} networks are the most commonly used variation of RNNs. LSTM networks were introduced by \citet{Hochreiter1997}. The critical component of the LSTM is the addition of memory gates or cells \citep{Graves2012, Gers1999}. The contents of the memory cell are modulated by input and forget gates. Assuming that both of these gates are closed, the contents of the memory cell will remain unmodified between one time-step and the next. The gating structure allows information to be retained across many time-steps, and consequently also allows gradients to flow across many time-steps. This allows the LSTM model to overcome the vanishing gradient problem that occurs with most Recurrent Neural Network models.

% \begin{figure}[ht!]
% 	\centering
% 	\includegraphics[width=0.9\linewidth]{unrolled_RNN.png}
% 	\caption[An unrolled RNN showing the recurrent connections between RNN cells $A$ for some input $x_t$ and outputs value $h_t$. From \citet{Olah2015}.]{An unrolled RNN showing the recurrent connections between RNN cells $A$ for some input $x_t$ and outputs value $h_t$. From \citet{Olah2015}.}
% 	\label{fig:RNN_arch}
% \end{figure}

\subsection{Not Just Algorithms}
Apart from Machine Learning algorithms, re-interest in DNN has led to software architectures that allow for quick development. The most common include Tensorflow \citep{Abadi2015}, Keras \citep{Chollet2015}, PyTorch \citep{Paszke2017}, Caffe \citep{Jia2014}) and Deeplearning4j \citep{Nicholson2016}. These types of frameworks are facilitating interdisciplinarity between Machine Learning and geophysics. Indeed, \cite{Richardson2018} employed DNN architecture within Tensorflow to solve for FWI. Utilizing a common DNN optimizer - Adam - he shows in Figure~\ref{fig:state_of_art_adam} how the cost function converged quicker in the inversion process as compared to conventional methods in FWI. 
\begin{figure}[ht!]
	\centering
	\includegraphics[width=0.95\linewidth]{adam_convergence.png}
	\caption[Faster convergence of Adam cost function.]{Adam cost function shown to converge much more rapidly than conventional Stochastic Gradient Descent and L-BFGS-B for FWI. From \cite{Richardson2018}.}
	\label{fig:state_of_art_adam}
\end{figure}

\section{Neural Networks in Geophysics}\label{sec:app_geophysics}
Machine Learning techniques have been utilised across different geophysical applications. Some notable examples include geo-dynamics \citep{Shahnas2018}, geology \citep{Reading2015}, seismology \citep{Shimshoni1998}, paleo-climatology \citep{Dowla1996}, climate change \citep{Anderson2018} and hydrogeology \citep{Hamshaw2018}. Unsupervised algorithms have also been investigated by \citet{Kohler2010} for pattern recognitions of wavefield patterns with minimal domain knowledge. Other geophysical application include seismic deconvolution \citep{Wang1992, CaldeOn-Macias1997}, tomography \citep{Nath1999}, first-break picking \citep{Murat1992}, trace editing \citep{McCormack1993a}, electricity \citep{Poulton1992}, magnetism \citep{Zhang1997}, shear-wave splitting \citep{Dai1994}, event classification \citep{Romeo1994}, petrophysics \citep{Downton2018} and noise attenuation \citep{Li2018a, Halpert2018}. 

\subsection{Legacy Velocity Inversion}
More specific to velocity estimation, the first published investigation for the use of NN was a RNN by \citet{Michaels1992}. Their network architecture represented all components in an elastic FWI experiment with a seismic source, the propagation media and an imaging response. Figure~\ref{fig:Block_RNN_FWI} shows a block diagram representation for their network. The neural column consisted of two 1-layer neuron columns, one for particle displacement and another for particle velocity. 

\begin{figure}[ht!]
	\begin{minipage}[c]{0.55\textwidth}
		\centering
		\includegraphics[width=\textwidth]{RNN_FWI.png}
	\end{minipage}\hfill
	\begin{minipage}[c]{0.4\textwidth}
		\caption[Block diagram for RNN system by Michaels \& Smith (1992)]{Block diagram for RNN system by \citet{Michaels1992}. The difference between a signal and the internal neural signals along a neural column are processed to provide a training signal that modifies neuron weights.}
		\label{fig:Block_RNN_FWI}
	\end{minipage}
\end{figure}

% \begin{figure}[ht!]
% 	\centering
% 	\includegraphics[width=0.6\linewidth]{RNN_FWI.png}
% 	\caption[Block diagram for RNN system by Michaels \& Smith (1992)]{Block diagram for RNN system by \citet{Michaels1992}. The difference between a signal and the internal neural signals along a neural column are processed to provide a training signal that modifies neuron weights.}
% 	\label{fig:Block_RNN_FWI}
% \end{figure}

\clearpage
\cite{Roth1994} published the first application of NN which estimated 1D velocity functions from shot gathers from a single layer NN in 1994. Figure~\ref{fig:first_NN_FWI} shows their NN architecture. This accepted synthetic common shot gathers from a single source as input and used to compute corresponding 1D large-scale velocity models. The training set used for learning consisted of 450 synthetic models built up of eight strata with constant layer thickness over a homogeneous half-space. Their network was able to approximate a true velocity model sufficiently to act as a starting model for further seismic imaging algorithms. The inferred velocity profiles of the unseen data provided 80\% accuracy levels, and although the network was stable for noise contained data, it was not robust against strong correlated noise. Nonetheless, this investigation sets up NN as possible candidates to solve non-trivial inverse problems.

\begin{figure}[ht!]
	\centering
	\includegraphics[width=0.95\linewidth]{first_NN_FWI_edited.png}
	\caption[Architecture for first NN application to FWI.]{Architecture for first NN application to FWI. This was a very shallow NN with 1 hidden layer and non-symmetric input-output neurons. Adapted from \cite{Roth1994}.}
	\label{fig:first_NN_FWI}
\end{figure}

NNs are not solely limited to creating initial models to FWI. \citet{Langer1996} show how NNs can invert for parameters related to the seismic source and propagation medium using a similar single hidden layer architecture as R{\"o}th \& Tarantola. The difference in approach was two-fold; the NN employed a single seismogram as input as opposed to whole shot gather and pseudo-spectral data was used for training rather than time waveforms directly. The use of a single waveform did allow for improved results, however the use of pseudo-spectral data was instrumental. Transformed NN inference had better accuracies than the conventional time approach. Motivation to use pseudo-spectral data followed the work of \citet{Falsaperla1996} where they identified how the introduced sparsity within pseudo-spectral domain facilitated the learning process for the NN and was more robust to noise.

\subsection{Data-Driven Approaches to Velocity Estimation}
The terminology of data-driven geophysics is not a novel-one. This was first introduced in the literature by \citet{Schultz1994} when estimating rock properties directly from seismic-data through statistical techniques. However, conceptually, this is similar to the deconvolution process within a seismic processing flow \citep{Robinson1957,Robinson1967}. Namely, a filter is derived via autocorrelations and applied as a deconvolution operator \citep{Webster1978}. The term has only recently found a re-invigorated interest. Some modern applications of data-driven geophysical processes include dictionary learning \citep{NazariSiahsar2017}, time series analysis \citep{Wu2018a}, fault identification \citep{Mangalathu2020}, and reservoir characterization \citep{Schakel2014}.

Twenty-one years after \citet{Michaels1992}, \cite{Lewis2017} employed DNN architecture to learn prior models for seismic FWI. Their data driven approach at estimating initial models was applied to salt body reconstruction by learning the probability of salt geo-bodies and use this to regularize the FWI objective function. \citet{Araya-Polo2018} utilised DNN architecture and inverted for 2D high-velocity profiles. For the training process, they generated thousands of random 2D velocity models with up to four faults in them, at various dip angles and positions. Each model had three to eight layers, with velocities ranging from 2000 to 4000 \si{ms^{-1}}, with layer velocity increasing with depth. The DNN architecture is not defined in their paper, however when applied to unseen data with and without salt anomalies, their results achieved accuracies well above 80\% for both cases. This was used to obtain a low-wavenumber starting model then passed to traditional FWI as an initial model. \cite{Wu2018} proposed a convolutional-based network called ``InversionNet'' to directly map the raw seismic data to the corresponding seismic velocity model for a simple fault model with flat or curved subsurface layers. More recently, \cite{Li2019} extended this further and developed a DNN framework called ``SeisInvNet'' to perform the end-to-end velocity inversion mapping with enhanced single-trace seismic data as the input in time domain. 

\subsection{Wave Physics as an Analogue Recurrent Neural Network}
Recently, \citet{Raissi2019} and \citet{Hughes2019} derived a function between the physical dynamics of wave phenomena and RNNs. In their work they propose physics-informed neural networks that are trained to solve supervised learning tasks while respecting the laws of physics described by general non-linear partial differential equations. Fundamental to their approach is the ability for DNNs to be universal function approximators. Within this formulation, \cite{Raissi2019} are able to solve non-linear problems without the need to compute a priori assumptions, perform linearisation or employ local time-stepping. Under this new paradigm in modelling, \citet{Raissi2019} show how back-propagation is used ``to differentiate neural networks with respect to their input coordinates and model parameters to obtain physics-informed neural networks. Such \acp{NN} are constrained to respect any symmetries, invariances, or conservation principles originating from the physical laws that govern the observed data, as modelled by general time-dependent and non-linear partial differential equations''. In particular, following up from this work, \cite{Sun2019} recast the forward modelling problem in FWI into a deep learning network by recasting the acoustic wave propagation into a RNN framework. Figure~\ref{fig:RNN_FWI_Marmousi} shows velocity inversion results from \cite{Sun2019} applied to the Marmousi velocity model. These theory-guided inversions still suffer from cycle-skipping, local-minima and high computational cost \citep{Sun2019}. Recent research suggests that Stochastic Gradient Descent algorithms have the capacity to escape local minima to a certain extent \citep{Sun2019}. 

% \begin{figure}[ht!]
% 	\centering
% 	\includegraphics[width=0.9\linewidth]{RAISSI_Wave_physics_analog_RNN.png}
% 	\caption[Solution to Schr{\'o}dinger's equation using RNN.]{Top: Predicted solution for the Schr{\'o}dinger equation $|h(t,x)|$ along with the initial and boundary training data.\newline Bottom: Comparison of the predicted and exact solutions corresponding to the three temporal snapshots depicted by the dashed vertical lines in the top panel. The relative L2 error is  $1.97\times10^{-3}$. From \citet{Raissi2019}.}
% 	\label{fig:RNN_physics}
% \end{figure}

\begin{figure}[ht!]
	\centering
	\includegraphics[width=0.98\linewidth]{SUN_RNN_FWI.png}
	\caption[The inversion of Marmousi velocity model using RNN forward modelling.]{The inversion of Marmousi velocity model using RNN forward modelling framework with Adam algorithm optimizer. (a) True Marmousi. (b) 25th iteration. (c) 50th iteration (d) 100th iteration. From \cite{Sun2019}.}
	\label{fig:RNN_FWI_Marmousi}
\end{figure}