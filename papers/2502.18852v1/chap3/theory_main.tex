\chapter{Theoretical Considerations}

\textbf{This chapter reviews and derives the key theoretical approaches used in this dissertation. The first section introduces a classical FWI formulation and summarizes the local optimization problem as a flowchart of elements. Using a combination of ANNs, CNNs and RNNs present in  DNN architectures, FWI is recast as a simulation within a deep learning framework. The next two sections introduce the theoretical setups of data-driven and theory-guided pseudo-spectral FWI and associated theory which can be used to solve FWI as a Deep Learning problem. For a more in-depth assimilation, Appendix~\ref{sec:app_theory} provides complementary material.}

\section{FWI as Local Optimization}\label{sec:theory_theory_fwi_as_local_optimization}
\cite{Lailly1983} and \cite{Tarantola1984a} recast the migration imaging principle introduced by \cite{Claerbout1971} as a local optimization problem \citep{Virieux2009}. For an anisotropic medium, particle motion is based on the wave equation given by:
\begin{equation}\label{eq:wave_equation}
	\frac{1}{c(\boldsymbol{x})^2}\frac{\partial^2p(\boldsymbol{x},t)}{\partial t^2} - \nabla^2 p(\boldsymbol{x},t) = s(\boldsymbol{x},t),
\end{equation}
where $p(\boldsymbol{x},t)$ is the pressure wavefield, $c(\boldsymbol{x})$ is the acoustic p-wave velocity and $s(\boldsymbol{x},t)$ is the source. This can be expressed as a linear operator and solved numerically.

After forward modelling the physical system through the data, the objective is to minimize the difference between the observed data and the modelled data. The difference or misfit between the two datasets is known as the misfit-, objective- or cost- function $\phi$. The most common cost function is given by the $L_2-norm$ of the data residuals:
\begin{equation}\label{eq:l2_norm}
	\phi(x)= \frac{1}{2}\left|\left| \boldsymbol{y}_{\boldsymbol{obs}} - \boldsymbol{y}_{\boldsymbol{cal}}(x) \right|\right|_D^2 =\frac{1}{2}\Delta\boldsymbol{y}^\dagger\Delta\boldsymbol{y},
\end{equation}
where $D$ indicates the data domain given by $n_s$ sources and $n_r$ receivers, $\dagger$ is the transpose and $\boldsymbol{y}_{\boldsymbol{obs}},\boldsymbol{y}_{\boldsymbol{cal}}$ are the observed and calculated data respectively. The misfit function $\phi$ can be minimized with respect to the model parameters $y$ by setting the gradient to zero: 
\begin{equation}
	\nabla\phi=\frac{\partial\phi}{\partial y} = 0.
\end{equation}
To solve for the misfit function, FWI utilizes a linearised and iterative optimisation scheme. Based on the Born approximation in scattering theory \citep{Born1980}, consider the first model calculated to be $\boldsymbol{x}_0$. After the first pass via forward modelling, the model needs to be updated by the model parameter perturbation $\Delta \boldsymbol{x}_0$. This newly updated model is then used to calculate the next update and the procedure continues iteratively until the computed mode is close enough to the true model based on a residual threshold criterion. At each iteration $k$, the misfit function $\phi(\boldsymbol{x}_k )$ is calculated from model $\boldsymbol{x}_{k-1}$ of the previous iteration giving:
\begin{equation}\label{eq:misfit_next_step}
	\phi(\boldsymbol{x}_k)=\phi(\boldsymbol{x}_{k-1} + \Delta\boldsymbol{x}_k).
\end{equation}
Assuming that the model perturbation is small enough with respect to the model, Equation~\ref{eq:misfit_next_step} can be expanded via Taylor expansions up to second orders as:
\begin{equation}\label{eq:misfit_taylor_expansion}
	\phi(\boldsymbol{x}_k) = \phi(\boldsymbol{x}_{k-1}) + \delta\boldsymbol{x}^T\frac{\partial\phi}{\partial x}+\frac{1}{2}\delta\boldsymbol{x}^T\frac{\partial^2\phi}{\partial x^2}\delta\boldsymbol{x}.
\end{equation}
Taking the derivative of Equation~\ref{eq:misfit_taylor_expansion} and minimizing to determine the model update leads to:
\begin{equation}\label{eq:hessian}
	\partial\boldsymbol{x}\approx-\boldsymbol{H}^{-1}\nabla_x\phi,
\end{equation}
where $\boldsymbol{H}=\frac{\partial^2\phi}{\partial x^2}$ is the Hessian matrix and $\nabla_x\phi$ the gradient of misfit function evaluated at $\boldsymbol{x}_0$. The Hessian matrix is symmetric and represents the curvature trend of the misfit function.
\subsection{Model Update}\label{sec:theory_model_update}
Several methods can be utilised for updating for the next iteration. Newton methods update the model directly \citep{Newton1687}, whilst Gauss-Newton use Hessian approximations \citep{Tarantola1984}. The latter is referred to as the step-length \citep{Menke1989} and Equation~\ref{eq:hessian} becomes:
\begin{equation}
	\partial x \approx -\alpha\nabla_x\phi,
\end{equation}
where $\alpha$ is the step-length parameter and the magnitude of $\alpha$ is derived via first perturbing the initial model $\boldsymbol{x}_0$ by a differential change in the direction opposite to the gradient.
\subsection{Regularization}\label{sec:theory_regularization_fwi}
FWI often turns out to be an ill-posed problem given the incomplete parameter space measured in the field. Artefacts and over-fitting might be introduced in the model due to noise or high frequency component of the data \citep{Sirgue2004}. 
Well-posedness can be imposed on the model through \emph{a priori} information \citep{Asnaashari2013}. This is introduced into the misfit function (Equation~\ref{eq:l2_norm}) with the addition of Tikhonov $L_2-norm$ regularization \citep{Tikhonov1977}:
\begin{equation}
	\phi(x)=\frac{1}{2} \left|\left|\boldsymbol{y}_{\boldsymbol{obs}}-\boldsymbol{y}_{\boldsymbol{cal}}\left(\boldsymbol{x}\right)\right|\right|_D^2  + \frac{1}{2}\lambda\left|\left|\boldsymbol{x}-\boldsymbol{x}_{\boldsymbol{\emph{a priori}}}\right|\right|_M^2,
\end{equation}
where $\left|\left|\boldsymbol{x}-\boldsymbol{x}_{\boldsymbol{prior}}\right|\right|_M^2$ is the regularization term and $\lambda$ is the regularisation parameter which controls the trade-off between data and model residuals. Namely, the regularization parameter, $\lambda$, gives relative weight to model optimization term with respect to data optimisation term and acts as a smoother on the modelling \citep{Tikhonov1963}. 

\subsection{Difference between Time and Pseudo-spectral FWI}
Forward modelling for FWI can be done in the pseudo-spectral or time-domain. In case of an attenuating medium, the pseudo-spectral domain is the preferred method as frequency dependent attenuation (quality factor $Q$) is represented by the imaginary component of the velocity \citep{Pratt1999}. Frequency domain application requires the solution of a linear system of equations by a factorization method \citep{Sirgue2004}. This improves the chance for the inversion to locate the global minimum \citep{Sirgue2004}, however it scales poorly with the size of the problem \citep{Operto2007} or else requires assumptions on the physical propagation of the wave equation \citep{BenHadjAli2007}. Time-domain approach has a simpler implementation \citep{Vigh2008}, however it is highly more sensitive to cycle skipping \citep{Vigh2008} and prone to problematic low-wavenumber estimation for the gradient \citep{Sirgue2004}. Time-domain approaches either consider $Q$ to be constant or utilise some relaxation mechanism, thus not fully representing the physics of the underlying problem \citep{Blanch1995}. Either approach has been successfully applied to production datasets, both with merits in different environments. For time implementation reference is made to \cite{Vigh2008}, \cite{Vigh2010}, \cite{Liu2011} and \cite{Cai2015}, and for frequency to \cite{Ben-Hadj-Ali2008}, \cite{Operto2015}, \cite{Plessix2009}. Differences in numerical implementation between time and frequency FWI given in Appendix~\ref{sec:app_theory_Differences_in_numerical_implementation_between_Time_and_Frequency_FWI}.
\subsection{FWI Algorithm Summary}
Excluding considerations related to the practical implementation of FWI, the local optimization iterative scheme for FWI described in the previous section can be summarised in Algorithm~\ref{algo:fwi_local_optimization} and schematic is illustrated in Figure~\ref{fig:workflow_FWI}.

\begin{algorithm}
Choose an initial model $\boldsymbol{x}_0$ and source wavelet $s(\boldsymbol{x})$.\\
For each source, the forward problem $\Gamma:X\mapsto Y$ is solved everywhere in the model space to get a predicted wavefield $\boldsymbol{y}_i$. This is sampled at receivers $r(\boldsymbol{x})$.\\
At every receiver, data residuals are calculated between the modelled wavefield $\boldsymbol{y}_i$ and the observed data $\boldsymbol{y}_{\boldsymbol{obs}}$.\\
Data residuals are back-propagated to produce a residual wavefield.\\
For each source location, the misfit function $\phi(\boldsymbol{x})$ is applied for the observed data and back-propagated residual wavefield to generate the gradient $\nabla_\phi$ required at every point in the model.\\
The gradient is scaled based on the step-length $\alpha$, applied to the starting model and an updated model is obtained $\boldsymbol{x}_{i+1}$.\\
The process is iteratively repeated from Step 2 until a convergence criterion is satisfied.
\caption{FWI as Local Optimization}
\label{algo:fwi_local_optimization}
\end{algorithm}

\begin{figure}[ht]
	\centering
	\includegraphics[width=0.9\linewidth]{fwi_workflow.png}
	\caption[Schematic FWI workflow solved as an iterative optimization process.]{Schematic FWI workflow solved as an iterative optimization process.}
	\label{fig:workflow_FWI}
\end{figure}

\section[FWI as a Data-Driven DNN]{FWI as a Data-Driven DNN}\label{sec:theory_FWI_as_Learned_Direct_Approx}
When applied to inverse problems, \acp{NN} can simulate the non-linear functional of the inverse problem $\Gamma^{-1}:Y\mapsto X$. That is, using a NN, a non-linear mapping can be learned which will minimize
\begin{equation}
	\left|\left|\boldsymbol{x}-g_\Theta(\boldsymbol{y})\right|\right|^2,
\end{equation}
where $g_\Theta$ is the learned formulation of the inverse problem functional $\Gamma^{-1}$, and $\Theta$ the large data set of pairs $(\boldsymbol{x},\boldsymbol{y})$ used for the learning process \citep{Lucas2018}.

\subsection{Artificial Neuron, Perceptron and Multi-Layer Perceptron}
The most elementary component in a NN is a neuron. This receives excitatory input and sums the result to produce an output or activation \citep{Raschka2017}. For a given artificial neuron, consider $n$ inputs with signals $\boldsymbol{x}$ and weights $\boldsymbol{w}$. The output $\boldsymbol{y}$ of a neuron from all input signals is given by:
\begin{equation}
	y=\sigma\left( b + \sum_{j=0}^{n}w_{j}x_j \right),
\end{equation}
where $\sigma$ is the activation function and $b$ is a bias term enabling the activation functions to shift about the origin. Popular activations functions include Binary Step, Linear, Sigmoid, Tanh, Softmax and ReLu functions \citep{Goodfellow2016}. The ReLu or Rectified linear unit is the most widely used activation function. It is non-linear, allowing easily back-propagation of errors. When employed on a network of neurons, the negative component of the function is converted to zero and the neuron is deactivated, introducing sparsity with the network and making it efficient and easy for computation. More information on the other functions is provided in Appendix~\ref{sec:app_theory_Alternative_Activation_Functions}.

\subsection{Feed-forward Architectures and Deep Networks}
The architecture of a NN refers to the number of neurons, their arrangement and connectivity. When a single neuron, the result is a Perceptron. Stacking multiple layers of the simple neurons and fully connecting all the inputs results in the MLP or fully connected layer. Within MLP, the input from the initial nodes $\boldsymbol{x}$ is connected to a hidden layer of neurons and the information is passed layer by layer through the hidden layers until reaching the output layer. Figure~\ref{fig:MLP_2_layersI} shows a fully connected MLP consisting of 2 hidden layers. The output of the units $\boldsymbol{y}$  is the weighted sum of the input units and the application of a non-linear element-wise function \citep{Lucas2018}.

\begin{figure}[ht!]
	\centering
	\includegraphics[width=0.98\linewidth]{mlp.png}
	\caption[Fully connected NN with 2 hidden layers.]{Fully connected NN with 2 hidden layers. 
	% The activation function of the $k^{th}$ output neuron in layer $l$ is defined as $y_k^l=\sigma\left(b_k^l+\sum_{j=0}^{m}w_{kj}^l x_j^{l-1} \right)$, where $\sigma$ is the chosen activation function. All weights $w$ and bias $b$ are learned during the training phase. 
	Adapted from \cite{Lucas2018}.}
	\label{fig:MLP_2_layersI}
\end{figure}

When designing a NN, the depth of the network and the width of each layer is key. A network with a single hidden layer is sufficient to fit the training set \citep{Goodfellow2016}. However, deeper NN have better performance metrics (Figure~\ref{fig:deep_NN_advantage_METRICS}) and are able to generalize better to different non-linear functions (see Figure~\ref{fig:deep_NN_advantage_REGULARIZE}), yet harder to optimize \citep{Haykin2009}. Indeed, \cite{Montufar2014} showed that a shallow NN equivalent in terms of performance metrics to a DNN could require an exponential number of hidden units.

\begin{figure}[!ht]
	\centering
	\subbottom[Deeper networks have been empirically shown to improve the accuracy, since deeper networks are able to achieve higher test accuracies. From \cite{Goodfellow2015}.\label{fig:deep_NN_advantage_METRICS}]{\includegraphics[width=0.9\textwidth]{effect_of_depth.png}}
	\subbottom[Visual representation of the improved generalization for deep NNs over shallow NNs. The blue and red are the different weight and bias combinations for all the units in a network and the green line represents the decision function explaining the optimal choice for the weights and biases. The dotted grey line represents the line of symmetry which if appled will result in a simpler decision function able to explain more of the the weights and biases.
	\newline \textit{Left:} A shallow NN requires a complicated decision boundary to be able to explain the optimal wieght and bias combinations.
	\newline \textit{Centre:} A simpler boundary decision can be obtained when considering the axis of symmetry. This extra degree of freedom is achievable within a NN by having an additional hidden layer.
	\newline \textit{Right:} An even simpler, almost linear, decision boundary is obtained if an additional axis of symmetry (degree of freedom) is considered, thus giving a deeper network the ability to generalize better. From \cite{Montufar2014}.\label{fig:deep_NN_advantage_REGULARIZE}]{\includegraphics[width=0.98\textwidth]{deep_generalization.png}}
	\caption{Advantages of a deep NN over a shallow NN.}        
	\label{fig:deep_NN_advantage}
\end{figure}

\subsection{A Universal Approximation Framework}\label{sec:theory_UAT}
A fundamental attribute to all deep feed-forward NN is that these provide a universal approximation framework \citep{Hornik1989}. In particular, the Universal Approximation Theorem as per \citet{Leshno1993} states that a ``feed-forward network with a linear output layer and at least one hidden layer with an appropriate activation function can approximate any Borel measurable function\footnote{\citet{Stephens2006} state that ``The real-valued function $f$ defined with domain $\mathcal{E}\subset\Omega$, for measurable space $(\Omega,\mathcal{F})$, is Borel measurable with respect to $\mathcal{F}$ if the inverse image of set $\mathcal{B}$, defined as $f^{-1} (\mathcal{B})\equiv\left\{\omega\in\mathcal{E}:f(\omega)\in\mathcal{B}\right\}$ is an element of $\sigma$-algebra $\mathcal{F}$, for all Borel sets $\mathcal{B}$ of $\mathbb{R}$''.} from one finite-dimensional space to another with any desired non-zero amount of error, provided that the network has sufficient hidden layers.''
\clearpage

\subsection{Back-Propagation and Learning with DNN}
When training a DNN, the forward propagation through the hidden layers from input $\boldsymbol{x}$ to output $\boldsymbol{y}$ needs to be measured for its misfit. In terms of regression modelling, the most commonly used cost function is the Sum of Squared Error, defined as:
\begin{equation}\label{eq:dnn_cost_function_J}
	\mathcal{J}\equiv\frac{1}{2}\sum_{j=1}^{\mathcal{J}}\left(\boldsymbol{y}^i-\boldsymbol{y}_{true}\right)^2,
\end{equation}
where $\boldsymbol{y}_{true}$ is the labelled true datasets and $\boldsymbol{y}^{i}$ is the output from the $i^{th}$ pass forward pass through the network. 

\begin{figure}[ht!]
	\centering
	\includegraphics[width=0.9\linewidth]{BP.png}
	\caption[DNN schematic]{Schematic of a DNN with input $\boldsymbol{x}$, $L$ layers, $\boldsymbol{a}^l$ activation function at layer $l$ and weights $\boldsymbol{w}^l$. From \cite{Hallstrom2016}.}
	\label{fig:DNN_schematic}
\end{figure}
Consider the schematic for a fully connected DNN shown in Figure~\ref{fig:DNN_schematic}. Applying forward propagation through the layers of neurons:
\begin{alignat}{3}
&\text{Input sum of neuron $k$ in layer $l$} \qquad && z_k^l &&=b_k^l+\sum_{j}w_{kj}^l a_j^{l-1} \label{eq:input_sum_neurons_k}\\
&\text{Activation function on layer $l$}\qquad &&a_k^l &&=\sigma\left(z_k^l\right)\\
&\text{Input sum of neuron $m$ in layer $l+1$}\qquad &&z_m^{l+1}&&=b_m^l+\sum_{k}w_{mk}^{l+1} a_k^l \label{eq:input_sum_neurons_m}.
\end{alignat}
The objective is to minimize the function $\mathcal{J}$ with respect to the weights. Employing the chain rule, the derivative with respect to a single weight in layer $l$ is given as:
\begin{equation}
	\frac{\partial \mathcal{J}}{\partial w_{kj}^l} = 
	\frac{\partial \mathcal{J}}{\partial z_k^l}\frac{\partial z_k^l}{\partial w_{kj}^l} =
	\frac{\partial \mathcal{J}}{\partial a_k^l}\frac{\partial a_k^l}{\partial z_{k}^l}\frac{\partial z_k^l}{\partial w_{kj}^l}.
	\label{eq:dnn_cost_function}
\end{equation}
Substituting Equations~\ref{eq:input_sum_neurons_k}-\ref{eq:input_sum_neurons_m} in Equation~\ref{eq:dnn_cost_function} yields
\begin{equation}
	\frac{\partial \mathcal{J}}{\partial w_{kj}^l} = 
	\left( \sum_{m} 	\frac{\partial \mathcal{J}}{\partial z_m^{l+1}}\frac{\partial z_m^{l+1}}{\partial a_l^k}  \right)
	\frac{\partial a_k^l}{\partial z_{k}^l}\frac{\partial z_k^l}{\partial w_{kj}^l} =
	\left( \sum_{m}\frac{\partial \mathcal{J}}{\partial z_m^{l+1}} w_{mk}^{l+1} \right) \sigma^{'}\left( z_k^l\right) a_j^{l-1}.
	\label{eq:dnn_cost_function_substitute}
\end{equation}
The error signal of a neuron $k$ in layer $l$ is defined as the total error when the input sum of the neuron is changed and is given by:
\begin{equation}
	\delta_k^l\equiv\frac{\partial\mathcal{J}}{z_k^l}.
\end{equation}
Substituting Equations~\ref{eq:input_sum_neurons_k}-\ref{eq:input_sum_neurons_m} in Equation~\ref{eq:dnn_cost_function_substitute} results in a recursive formulation for the error given by 
\begin{equation}\label{eq:mlp_error}
	\delta_k^l=\left(\sum_{m}\delta_m^{l+1}w_{mk}^{l+1} \right)\sigma^{'}(z_k^l).
\end{equation}
Estimation of the error of the neurons in the final layer $L$ can be a sequential calculation of the error from the previous layer, until all error signals within the network are thus computed. The only derivative to be calculated is the derivative of the cost function $\sigma^{'}$, which is the output of the network. Bias $b_k^l$ for $l^{th}$ layer is a weight to be optimized and the error signal for the bias is given by:
\begin{equation}
	\frac{\partial\mathcal{J}}{\partial b_k^L}=\frac{\partial\mathcal{J}}{\partial z_k^l}\underbrace{\frac{\partial z_k^l}{\partial b_k^l}}_{1} = \delta_k^l.
\end{equation}
Hence, error signals for all neurons in the network can be recursively calculated throughout the network and the derivative of the cost function with respect to all the weights can also be calculated. Training of the DNN is achieved via gradient descent algorithm, referred to as Delta Rule in the machine learning community \citep{Sutton1988}. A small fraction of the derived derivative is subtracted from the weight and updated weight is given as:
\begin{equation}
	w_{kj}^l = w_{kj}^l-\eta\delta_k^l,
\end{equation}
where $\eta$ is a small scalar referred to the learning rate which controls how much of an adjusting is applied to the DNN with respect to the loss gradient. This is synonymous to the model update in FWI presented in §~\ref{sec:theory_model_update}. Equivalence of back-propagation and the classical adjoint method is numerically shown in Appendix~\ref{sec:app_results_equivalence_AD_adjoint}.

\subsection{Optimizing the Loss Function}\label{sec:theory_optimizing_the_loss_function}
Updating of the error gradient in a steepest gradient descent manner might be conceptually straightforward to understand. However, a major drawback with this algorithm is the high risk of getting stuck in local minima \citep{Fletcher1987}. This is an active area of research and \cite{Ruder2016} does an extensive review of optimizers. Four of the most widely used are (i) Adagrad, (ii) Adadelta, (iii) Adam and (iv) RMSprop. Difference in the implementation for these optimizers is given in Appendix~\ref{sec:app_theory_Loss_Optimizers}.

\subsection{Generalization of DNN for “unseen” Data}\label{sec:theory_generalization}
Central to DNN is how to make the network able to perform well not just on the training data, but also on new “unseen” inputs. DNN resolve this via two regularization strategies: (i) functional-based, and (ii) NN architecture-based regularizations. These are schemes which update the cost function. On the other hand, architecture-based regularization are alterations to the NN architecture in the form of (i) Dropout, (ii) Data augmentation, and (iii) Early Stopping. Further information on these is available in Appendix~\ref{sec:app_theory_Regularization}. Either of these are essential for DNN as shown in Figure~\ref{fig:DNN_regularization}.

\begin{figure}[ht!]
	\centering
	\includegraphics[width=0.9\linewidth]{goldstein_loss_regularization_axis.png}
	\caption[NN loss function without and with regularization]{NN loss function surface without (Left) and with (Right) regularization. Two random vectors $\left(\delta,\eta\right)$ are chosen on the parameter space and values of the loss computed. Vertical axes are in logarithmic scale to show dynamic range. It is highly evident that without regularization, the loss function is susceptible to local minima and at risk to converge to sub-optimal levels. On the other hand, regularization promotes flatness and prevent chaotic behaviour. Adapted from \cite{Li2017}.}
	\label{fig:DNN_regularization}
\end{figure}

\subsection{Convolutional Neural Networks}
CNNs are feed forward multi-layered networks with layers consisting of convolutions and linear transformers able to perform multiple transformations \citep{LeCun2010}. Apart from Dropout, Activation functions and Fully Connected Layers components, CNNs introduce (i) Convolutional, (ii) Pooling, and (iii) Batch Normalization layers. Convolutional layers add convolutions across the neurons. Pooling layers are dimension reducing layers applied after convolutional layers since location of elements within feature-maps become less important as long as their position relative to others is preserved \citep{Khan2020}. Batch Normalization shifts the covariance of the distribution of hidden units to zero mean and unit variance for each batch \citep{Ioffe2015}, ensuring smooth gradient descent and regularization \citep{Santurkar2018}. Further details on the specific building blocks for CNNs is available in Appendix~\ref{sec:app_theory_CNN_Building_Blocks}.

\subsection{Neural Network Architecture Types}\label{sec:theory_NN_arch_types}
Combining the components introduced in the previous sections, different DNN architectures can be developed. The simplest form for a network would be MLP. This is considered to be a rudimental and basic network. An improvement would be the inclusion of convolutional, max-pooling and batch normalization layers in either one-dimension (1D) or two-dimensions (2D). One such architecture is AlexNet \citep{Krizhevsky2012}. Khan et al.’s 2020 survey provides a detailed overview of the different CNN architectures and how they evolved over time. In particular, two architectures which are considered state-of-the-art as a consensus in the literature are VGG \citep{Simonyan2014} and ResNet \citep{He2016IEEE}. These are only three of a myriad of architectures (Figure~\ref{fig:CNN_history}). More information on these architectures is available in Appendix~\ref{sec:app_theory_Comparison_of_Common_CNN_Architectures}.
\begin{figure}[ht!]
	\centering
	\includegraphics[width=0.98\linewidth]{CNN_history.png}
	\caption[Historic overview of CNN architectures]{Historic overview of CNN architectures, highlighting timeline position for AlexNet, VGG and ResNet. Adapted from \cite{Khan2020}.}
	\label{fig:CNN_history}
\end{figure}

\clearpage
\subsection{Outline for Solving FWI with Data-Driven DNNs}
Utilizing NN architecture and formulae, training of a DNN for FWI can be summarized as in Algorithm~\ref{algo:fwi_dnn}. Schematic of the learning process for DNN is given Figure~\ref{fig:workflow_DNN}.

\begin{algorithm}
Setup a deep architecture from those shown in §~\ref{sec:theory_NN_arch_types}, with regularization measures.\\
Initialise the set of weights $\boldsymbol{w}^l$  and biases $\boldsymbol{b}^l$.\\
Forward propagate through the network connections to calculate input sums and activation function for all neurons and layers.\\
Calculate the error signal for the final layer $\delta^L$ by choosing an appropriate differentiable activation function.\\
Back propagate the errors for all neurons in each $l$ layer $\delta^l$.\\
Differentiate the cost function with respect to biases $\left(\frac{\partial\mathcal{J}}{\partial\boldsymbol{b}^l}\right)$\\
Differentiate the cost function with respect to weights $\left(\frac{\partial\mathcal{J}}{\partial\boldsymbol{w}^l}\right)$\\
Update weights $\boldsymbol{w}^l$ via gradient descent.\\
Recursively repeat from Step 3 until the desired convergence criterion is met.
	\caption{FWI as a data-driven DNN}
	\label{algo:fwi_dnn}
\end{algorithm}

\begin{figure}[ht!]
	\centering
	\includegraphics[width=0.9\linewidth]{DNN_workflow.png}
	\caption[Schematic of DNN workflow]{Schematic of DNN workflow solved via supervised learning with input-output pairs $(\boldsymbol{x}, \boldsymbol{y})$. This is analogous to the FWI algorithm previously shown in Figure~\ref{fig:workflow_FWI}, with the difference that feed forward operator $g_\Theta:X\mapsto Y$ is a learned function.}
	\label{fig:workflow_DNN}
\end{figure}

\section[RNN as an Analogue of FWI]{Theory-Guided RNN as an Analogue of FWI}
Feed forward NNs presented in §~\ref{sec:theory_FWI_as_Learned_Direct_Approx} do not allow for cyclical/recurrent connections between neurons. If this condition is relaxed, we obtain RNNs. This difference is illustrated in Figure~\ref{fig:MLP_vs_RNN}. Although this might seem trivial, recurrent connections allow a ``memory'' of previous inputs to persist in the network’s state \citep{Graves2012}. Through equivalence results of Universal Approximation Theorem for MLP, \cite{Hammer2000} prove that a ``RNN with sufficient number of hidden units can approximate any measurable sequence-to-sequence mapping to arbitrary accuracy''.

\begin{figure}[ht!]
	\centering
	\includegraphics[width=0.9\linewidth]{MLP_vs_RNN.png}
	\caption[Difference between MLP and RNN architectures.]{Difference between MLP and RNN architectures are Red and Blue cyclical connections. Red are intra-layer connections, whereas Blue are across layers.}
	\label{fig:MLP_vs_RNN}
\end{figure}

\subsection{Forward Pass}
The forward pass of a RNN is identical to that of a MLP, except that the current input and the hidden layer activations from the previous time-step contribute to the current activation function. 

Consider a sequence $x$ of length $T$ and a RNN with input units $I$, hidden units $H$ and output units $K$. Let $x_i^t$ be the input $i$ at time-step $t$, $a_j^t$
the network output to unit $j$ at time-step $t$ and $b_j^t$ the activation of unit $j$ at time-step $t$. For the hidden units we have,
\begin{equation}
	a_h^t=b_h^0+\sum_{i=1}^{I} w_{ih} x_i^t +\sum_{h^{'}=1}^{H}w_{h^{'}h} b_{h^{'}}^{t-1},
\end{equation}
where $b_h^0$ is the initial bias term. \cite{Zimmermann2006} show how initial non-zero bias improve RNN stability and performance. This term is set to always zero to simplify the equation notation. Non-linear, differentiable activation functions $\sigma$ are applied similarly to MLP to get:
\begin{equation}\label{eq:RNN_activation_function}
	b_h^t=\sigma_h \left(a_h^t\right)=\sigma_h \left(\sum_{i=1}^{I}w_{ih} x_i^t +\sum_{h^{'}=1}^{H} w_{h^{'} h} b_{h^{'}}^{t-1} \right).
\end{equation}
Starting at $t = 1$, recursively applying Equation~\ref{eq:RNN_activation_function} will result in the complete sequence of hidden activations. The output $y_k^t$ at the $k^{th}$ neuron at time $t$ is thus given by:
\begin{equation}
	y_k^t=\sum_{h=1}^{H}w_{hk} b_h^t.
\end{equation}
% Or more concisely written as:
% \begin{equation}
% 	y^t=W^t\cdot b^t	
% \end{equation}
\subsection{Backward Pass}
Given that components of the forward pass and the loss function definition are synonymous with those of MLPs, the remaining components are the derivatives with respect to the model weights. There are two widely used algorithms for this: (i) Real Time Recurrent Learning \citep{Robinson1987}, and (ii) Back-Propagation Through Time \citep{Werbos1988}. Back-Propagation Through Time will be considered going forward. Implementation of Real Time Recurrent Learning is given by \cite{Williams1989} or more recently by \cite{Haykin2009}.

Like standard back propagation, Back-Propagation Through Time consists of repeated application of the chain rule. The only difference is that for RNN, the loss function $\mathcal{J}$ is now dependent on the activation of the hidden layer at the next time-step. Equation~\ref{eq:mlp_error} can be thus re-written as:
\begin{equation}\label{eq:rnn_error}
	\delta_h^t = \frac{\partial\mathcal{L}}{\partial a_j^t}= \theta^{'}(a_h^t)\left(\sum_{k=1}^{K}\delta_k^{t}w_{mk} + \sum_{h^{'}=1}^{H}\delta_{h^{'}}^{t+1}w_{hh^{'}} \right).
\end{equation}
The complete sequence of partial derivate terms can be calculated by starting at time $t=T$ and recursively applying Equation~\ref{eq:rnn_error}. Summing over the whole sequence to get network weights results in:
\begin{equation}
	\frac{\partial\mathcal{L}}{\partial w_{ij}} = \sum_{t=1}^{T}\frac{\partial\mathcal{L}}{\partial a_j^t}\frac{\partial a_j^t}{\partial w_{ij}} = \sum_{t=1}^{T}\delta_j^tb_i^t.
\end{equation}
\subsection{Vanishing Gradient}
Standard RNN architectures suffer from the vanishing gradient problem \citep{Hochreiter1991}. Namely, the sensitivity of deeper neurons either decays or blows up exponentially as it passes through the recurrent connections \citet{Graves2012}. This is schematically illustrated in Figure~\ref{fig:RNN_vanishing_gradient}. Attempts to solve for this included gradient-descent variants \citep{Pearlmutter1989, Williams1989, Elman1990, Fahlman1991, Schmidhuber1992a}, explicitly introduced time delays or time constants \citep{Lang1990,Plate1993,Lin1996,Mozer1992}, simulated annealing and discrete error propagation \citep{Bengio1994}, hierarchical sequence compression \citep{Schmidhuber1992} and Kalman filtering \citep{Puskorius1994}. A comprehensive list is available within \cite{Hochreiter1997}.

\begin{figure}[ht!]
	\centering
	\includegraphics[width=0.8\linewidth]{RNN_vanishing_gradient.png}
	\caption[Vanishing gradient problem for RNNs.]{``Vanishing gradient problem for RNNs. The shading of the nodes indicates their sensitivity to the inputs at time one. The darker the shade, the greater the influence. The sensitivity decays over time as new inputs overwrite the activations of the hidden layer, and the network forgets the first inputs.'' From \cite{Graves2012}.}
	\label{fig:RNN_vanishing_gradient}
\end{figure}

\subsection[Long Short-Term Memory]{Long Short-Term Memory (LSTM)}
Apart from the subset of algorithms able to handle the vanishing gradient problem in the previous section, \cite{Hochreiter1997} introduce a modified architecture type known as \ac{LSTM}. This NN introduces a set of recurrent connections known as memory blocks (the input, output and forget gates) and a cell state. Figure~\ref{fig:LSTM_module_RNN} shows a standard RNN with a single tanh layer. Figure~\ref{fig:LSTM_module_LSTM} shows the LSTM chain structure but with the additional four interaction layers. Mathematical detail for each of these components is given in Appendix~\ref{sec:app_theory_Individual_RNN_Components}.

As before, $w_{ij}$ is the weight from unit $i$ to unit $j$, $a_j^t$ is the network input to unit $j$ at time step $t$ and $b_j^t$ is the activation of unit $j$ at time step $t$. Given the LSTM memory blocks, the subscripts $\iota$, $\zeta$ and $\omicron$ refer to the input, forget and output gate respectively, the subscript $c$ refers to one of the memory cells $C$ and $s_c^t$ is the state of cell $c$ at time $t$. 

\begin{figure}[!ht]
	\centering
	\subbottom[The repeating module in a standard RNN contains a single layer.\label{fig:LSTM_module_RNN}]{\includegraphics[width=0.7\textwidth]{LSTM_module_RNN.png}}
	\subbottom[A LSTM memory block has an additional three gates – Input, Output and Forget Gate (red) and a cell state block (blue).\label{fig:LSTM_module_LSTM}]{\includegraphics[width=0.7\textwidth]{LSTM_module_LSTM.png}}
	\caption[Comparison between RNN and LSTM blocks.]{Comparison between RNN and LSTM blocks. Adapted from \cite{Olah2015}.}        
	\label{fig:RNN_LSTM_blocks}
\end{figure}

\subsection{Network Training}
The theory reviewed for MLP optimizers in §~\ref{sec:theory_optimizing_the_loss_function} and regularization in §~\ref{sec:theory_generalization} remain unchanged for LSTMs.

\subsection{LSTM as a Substitute for Wave Propagation}
Revisiting the discretized FD stencil for wave propagation given as,
\begin{equation}
	p_j^{n+1}=\partial t^2 \left[c_j^2 \mathcal{F}^{-1} \left[k^2 \mathcal{P}_{\nu}^n\right]+s_j^n\right]+2p_j^n-p_j^{n-1},
\end{equation}
it is clear how pressure wave $p$ and source impulse $s$ at current time step $n$ are not affected by the future values $n+1$, but only dependent on the previous state of pressure at $n-1$. This is, by definition, a finite impulse with directed acyclic graph under graph theory definitions \citep{Thulasiraman2011}. With slight modification to the LSTM blueprint in Figure~\ref{fig:LSTM_module_LSTM}, a Deep Learning architecture supporting forward modelling can be cast as a LSTM cell that considers the pressure wave at time $n-1$, produces the modelled shot record at current time $n$ and stores this in memory for the next step $n+1$. Measuring all outputs at each moment in time would equal to the measurements of the wavefield locally at a geophone. This LSTM architecture is shown in Figure~\ref{fig:LSTM_unrolled} as an unrolled graph and Figure~\ref{fig:LSTM_FWI_components} as the building block components within an LSTM. 

The inputs to the LSTM cell are the source term at current time $s^t$, the wavefield at current $u_t$ and previous time step $u_{t-1}$ stored in memory of the LSTM. These wavefields are combined together with untrainable modelling operator $\omega$ and constants $-1$ and $-2$ to replicate the incremental time stepping in forward modelling. Deciding to model in time is equivalent to setting $\omega$ to the Laplacian, whereas setting it to calculate pseudo-spectral second order derivates will lead to pseudo-spectral wavefield modelling. The trainable velocity-related parameter $v^2 \Delta t^2$ is applied to get the current modelled wavefield $u^{t+1}$. This is stored in memory, passed to the forget gate and receiver location discretization $\delta_{x_{r}}$ applied to get the predicted outputs $d^{t+1}$. To train the velocity parameters, seismic shot records are provided as labelled data for training.
\clearpage
\begin{figure}[!ht]
	\centering
	\subbottom[Unrolled form of acyclic graph of LSTM for FWI.\label{fig:LSTM_unrolled}]{\includegraphics[width=0.7\textwidth]{LSTM_unrolled.png}}
	\subbottom[Modified LSTM cell block supporting of forward modelling.\label{fig:LSTM_FWI_components}]{\includegraphics[width=0.9\textwidth]{LSTM_FWI_components.png}}
	\caption[Recasting of forward modelling within an LSTM]{Recasting of forward modelling of FWI within an LSTM deep learning framework. Adapted from \cite{Sun2019}.}        
	\label{fig:LSTM_FWI}
\end{figure}

\subsection{Outline for Solving FWI with a RNN}
Physics-informed RNNs for FWI is identical to classical FWI, apart from the forward modelling component which is done within an LSTM framework. This is summarized by Algorithm~\ref{algo:FWI_RNN} and schematic shown in Figure~\ref{fig:workflow_RNN}.
\begin{algorithm}
Choose an initial model $\boldsymbol{x}_0$ and source wavelet $s(\boldsymbol{x})$.\\
For each source, solve the forward problem through LSTM time-stepping to get a predicted wavefield $\boldsymbol{y}_i$. This is sampled at receivers $r(\boldsymbol{x})$.\\
At every receiver, data residuals are calculated between the modelled wavefield $\boldsymbol{y}_i$ and the observed data $\boldsymbol{y}_{\boldsymbol{obs}}$.\\
Data residuals are back-propagated to produce a residual wavefield.\\
For each source location, the misfit function $\phi(\boldsymbol{x})$ is applied for the observed data, regularized and back-propagated through residual wavefield to generate the gradient $\nabla_\phi$ required at every point in the model.\\
The gradient is scaled based on loss optimization function, applied to the starting model and an updated model is obtained $\boldsymbol{x}_{i+1}$.\\
The process is iteratively repeated from Step 2 until a convergence criterion is satisfied.
	\caption{FWI as RNN Implementation}
	\label{algo:FWI_RNN}
\end{algorithm}

\begin{figure}[ht!]
	\centering
	\includegraphics[width=0.9\linewidth]{RNN_workflow.png}
	\caption[Schematic of RNN workflow]{Schematic of RNN workflow. The orange components are modified from the original FWI workflow in Figure~\ref{fig:workflow_FWI}.}
	\label{fig:workflow_RNN}
\end{figure}