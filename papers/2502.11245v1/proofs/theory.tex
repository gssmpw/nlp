In this section, we start by summarizing the prerequisite material.  We begin by recalling a central Lemma proven in \citep{marconato2023not} that allows us to write the condition for likelihood-optimal models in terms of $\valpha$. 
To ease of comparison, 
let $p (\vC \mid \vX) := \vf(\vX)$ and $p(\vY \mid \vX) := (\vomega \circ \vf) (\vX)$ denote the conditional distributions defined by the CBM $(\vf, \vomega) \in \calF \times \Omega$, and:
\[
    p(\vY \mid \vG) := \bbE_{\vx \sim p^*(\vX \mid \vG)} [ p (\vY \mid \vx) ]
\]
Also, we consider the following joint distributions:
\[
    p^*(\vX, \vY) = p^*(\vY \mid \vX) p^*(\vX), \quad p(\vX, \vY) = p(\vY \mid \vX) p^*(\vX)
\]


\begin{lemma}[\citep{marconato2023not}]
    \label{lemma:abstraction-from-lh}
    It holds that:
    (\textit{i})  The true risk of $p$ can be upper bounded as follows:
    \begin{align}
    \textstyle
        \bbE_{(\vx, \vy) \sim p^*(\vX , \vY)} [
            \log p(\vy \mid \vx)
        ] 
        &= - \KL[ p^*(\vX; \vY) \ \| \ p(\vX, \vY)  ] - H[ p^*(\vX, \vY) ]
        ] 
        \\
        &\leq
        \bbE_{\vg \sim p(\vG)} \big(
            - \KL [ p^*(\vY \mid \vg; \BK) \ \| \ p(\vY \mid \vg) ] - H [ p^*(\vY \mid \vg; \BK) ]
        \big)
    \label{eq:dpl-upper-bound}
    \end{align}
    where 
    \KL is the Kullback-Leibler divergence and H is the Shannon entropy.  Moreover, under \cref{assu:concepts,assu:labels}, $p(\vY \mid \vX; \BK)$ is an optimum of the LHS of \cref{eq:dpl-upper-bound} if and only if $p(\vY \mid \vG)$ is an optimum of the RHS.
    (\textit{ii}) There exists a bijection between the deterministic concept distributions $p(\vC \mid \vX)$ that, for each $\vg \in \mathrm{supp}(\vG)$, are constant over the support of $p^*(\vX \mid \vg)$ apart for zero-measure subspaces $\calX^0 \subset \mathsf{supp}{( p^*(\vX \mid \vg))}$
    and the deterministic distributions of the form $p(\vC \mid \vG)$.
\end{lemma}

Here, point (i) connects the optima of the likelihood $p (\vY \mid \vX)$ and the optima of $p (\vY \mid \vG)$. This implies that, under \cref{assu:concepts,assu:labels}, knowing the optima of $p (\vY \mid \vG)$ informs us about optimal CBMs $p (\vY \mid \vX)$.
Notice that, to a single $\valpha(\vG) = p (\vC \mid \vG)$ might correspond to multiple choices of $\vf \in \calF$. The choice is unique if, by point (ii), we consider deterministic $p(\vC \mid \vG)$ showing that they are into one-to-one correspondence with $p (\vC \mid \vX)$ that are almost constant in the support. 

Limiting to those $\valpha$'s in point (ii), we can find a simple way to describe optima for $p (\vY \mid \vG)$. 

\begin{lemma}[Deterministic optima of the likelihood]
    \label{lemma:deterministic-optima}
    Under \cref{assu:concepts,assu:labels}, 
    for all CBMs $(\vf, \vomega) \in \calF \times \Omega$ where $\vf:\vx \mapsto \vc$ is constant over the support of $p^*(\vX \mid \vg)$ apart for a zero-measure subspace $\calX^0 \subset \mathsf{supp}{( p^*(\vX \mid \vg))}$, for all $\vg \in \calG$, it holds that the optima of the likelihood for $p(\vY \mid \vG)$ are obtained when:
    \[
        \forall \vg \in \mathsf{supp}(p^*(\vG)), \; (\vbeta \circ \valpha)(\vg) = \vbeta^* (\vg)
    \]
    where $\valpha (\vg) :=  \bbE_{\vx \sim p^*(\vX \mid \vg)} [ \vf(\vx) ]  \in \Vset{\calA} $ and $\vbeta (\vc) := \vomega{( \Ind{\vC = \vc}) } \in \Vset{\calB} $. 
\end{lemma}

\begin{proof}
    Notice that by \cref{lemma:abstraction-from-lh}, it holds that $\valpha \in \Vset{\calA}$ is a deterministic conditional distribution. 
    Since $\vf: \vx \to \vc$ is constant over all $\vx \sim p^*(\vX \mid \vg)$ for a fixed $\vg \in \calG$,
     it holds that $\vf(\vx) = \valpha (\vg)$
    apart for some $\vx$ in a measure-zero subspace $\calX^0 \subset \mathsf{supp}{( p^*(\vX \mid \vg))}$,
   therefore we get:
    \begin{align}
        \bbE_{\vx \sim p^*(\vX \mid \vg) } [ \vomega (\vf (\vx))] &= \vomega( \valpha(\vg)) \\
        &= (\vbeta \circ \valpha) (\vg)
        \label{eq:relation-f-alpha}
    \end{align}
    where in the last line we substituted $\vomega$ with $\vbeta$ since $\valpha$ is deterministic. For $p (\vY \mid \vG)$, the necessary and sufficient condition for maximum log-likelihood is:
    \[
        \forall \vg \in \mathsf{supp}(p^*(\vG)), \; p(\vY \mid \vg) = \vbeta^* (\vg)
    \]
    and substituting \cref{eq:relation-f-alpha} we get:
    \[
        \forall \vg \in \mathsf{supp}(p^*(\vG)), \; (\vbeta \circ \valpha) (\vg) = \vbeta^* (\vg)
    \]
    showing the claim.
\end{proof}





\subsection{Equivalence relation given by Definition \ref{def:intended-semantics}}


We start by proving that \cref{def:intended-semantics} defines an equivalence relation. 

\begin{proposition}
    \cref{def:intended-semantics} defines an equivalence relation (denoted as $\sim$) between couples $(\valpha, \vbeta), (\valpha', \vbeta') \in \calA \times \calB$, where $(\valpha, \vbeta) \sim (\valpha', \vbeta')$ if there exist a permutation $\pi:[k] \to [k]$ and $k$ element-wise invertible functions $\psi_1, \ldots, \psi_k$ such that:
    \begin{align}
        &\forall \vg \in \calG, \; \valpha (\vg) = (\vP_\pi \circ \vpsi \circ \valpha') (\vg) \\
        &\forall \vc \in \calC, \; \vbeta (\vc) = ( \vbeta' \circ \vpsi^{-1} \circ \vP^{-1}_\pi  ) (\vc)
    \end{align}
    where $\vP_\pi: \calC \to \calC$ is the permutation matrix induced by $\pi$ and $\vpsi(\vc) := (\psi_1(c_1), \ldots, \psi_k(c_k))$.
\end{proposition}

\begin{proof}

    To the aim of the proof, it is useful to analyze how $\vP_\pi$ and $\vpsi$ relate. Consider for any $\vc \in \calC$:
    \begin{align}
        (\vP_\pi \circ \vpsi)(\vc) 
        &= \vP_\pi ( \psi_1(c_1), \ldots, \psi_k(c_k) ) \\
        &= ( \psi_{\pi(1)}(c_{\pi(1)}), \ldots, \psi_{\pi(k)}(c_{\pi(k)}) )
    \end{align}
    Denoting with $\vpsi_\pi(\vc) := ( \psi_{\pi(1)}(c_1 ), \ldots, \psi_{\pi(k)}(c_k) )$, we have that:
    \[
        (\vP_\pi \circ \vpsi) (\vc) = (\vpsi_\pi \circ \vP_\pi) (\vc) \label{eq:psi-pi-transform}
    \]
    From this expression, notice it holds for all $\vc \in \calC$:
    \begin{align}
        (\vP_\pi \circ \vpsi) (\vc) &= (\vP_\pi \circ \vpsi \circ \vP^{-1}_{\pi} \circ \vP_\pi )(\vc) \\
        (\vpsi_\pi \circ \vP_\pi) (\vc) &= \big( (\vP_\pi \circ \vpsi \circ \vP^{-1}_{\pi}) \circ \vP_\pi \big) (\vc)
    \end{align}
    so that we can equivalently write $\vpsi_\pi:= \vP_\pi \circ \vpsi \circ \vP^{-1}_{\pi}$.
    

    \textbf{Reflexivity}. This follows by choosing $\vP_\pi = \mathrm{id}$ and similarly $\psi_i = \mathrm{id}$ for all $i \in [k]$.

    \textbf{Symmetry}. We have to prove that $(\valpha, \vbeta) \sim (\valpha', \vbeta') \implies (\valpha', \vbeta') \sim (\valpha, \vbeta)$. To this end, we start by taking the expression for $\valpha$ in terms of $\valpha'$. Consider for any $\vg \in \calG$:
    \begin{align}
        \valpha(\vg) &= (\vP_\pi \circ \vpsi \circ \valpha' )(\vg) \\
                     &= (\vpsi_\pi \circ \vP_\pi \circ  \valpha' )(\vg)
    \end{align}
    where in the last step we used \cref{eq:psi-pi-transform}. By first inverting $\vpsi_\pi$ and then $\vP_\pi$, we obtain that:
    \[
        \valpha'(\vg) = (\vP_\pi^{-1} \circ \vpsi_\pi^{-1} \circ \valpha)(\vg)
    \]
    showing the reflexivity of $\valpha$. With similar steps, we can show that a similar relation also holds for $\vbeta'$, that is for all $\vc \in \calC$: 
    \begin{align}
        \vbeta' (\vc)  = (\vbeta \circ \vpsi_\pi \circ \vP_\pi) (\vc)
    \end{align}

    \textbf{Transitivity}. We have to prove that if $(\valpha, \vbeta) \sim (\valpha', \vbeta')$ and $(\valpha', \vbeta') \sim (\valpha^\dagger, \vbeta^\dagger) $ then also $(\valpha, \vbeta) \sim (\valpha^\dagger, \vbeta^\dagger)$. We start from the expression of $\valpha$ and $\valpha'$, where we have that $\forall \vg \in \calG$:
    \begin{align}
        \valpha(\vg) &= (\vP_\pi \circ \vpsi \circ \valpha' )(\vg) \\
        \valpha'(\vg) &= (\vP_{\pi'} \circ \vpsi' \circ \valpha^\dagger)(\vg)
    \end{align}
    We proceed by substituting the expression of $\valpha'$ in $\valpha$ to obtain:
    \begin{align}
        \valpha(\vg) &= (\vP_\pi \circ \vpsi \circ \vP_{\pi'} \circ \vpsi' \circ \valpha^\dagger)(\vg) \\
        &= (\vP_\pi \circ \vP_{\pi'} \circ \vP_{\pi'}^{-1} \circ \vpsi \circ \vP_{\pi'} \circ \vpsi' \circ \valpha^\dagger)(\vg) 
        \tag{Composing $\vP_\pi$ with the identity $\vP_{\pi'} \circ \vP_{\pi'}^{-1}$}
        \\
        &= (\vP_\pi \circ \vP_{\pi'} \circ \vpsi_{{\pi'}^{-1}} \circ \vpsi' \circ \valpha^\dagger)(\vg) 
        \tag{Using that $\vpsi_{{\pi'}^{-1}} := \vP_{\pi'}^{-1} \circ \vpsi \circ \vP_{\pi'}$.} \\
        &= (\vP_{\pi^\dagger} \circ \vpsi^\dagger \circ \valpha^\dagger) (\vg)
    \end{align}
    where we defined $\vP_{\pi^\dagger} := \vP_\pi \circ \vP_{\pi'}$ and $\vpsi^\dagger := \vpsi_{{\pi'}^{-1}} \circ \vpsi'$. 
    With similar steps, we obtain that $\vbeta$ can be related to $\vbeta^\dagger$ as:
    \[
        \vbeta (\vc) =  (\vbeta^\dagger \circ {\vpsi^{\dagger}}^{-1} \circ \vP_{\pi^\dagger}^{-1}) (\vc)
    \]
    for all $\vc \in \calC$. This shows the equivalence relation of \cref{def:intended-semantics}.
\end{proof}
 

We now prove how solutions with intended semantics (\cref{def:intended-semantics}) relate to the optima of the log-likelihood:

\begin{lemma}[Intended semantics entails optimal models]
    \label{lemma:intended-semantics-optima}
    If a pair $(\valpha, \vbeta) \in \calA \times \calB$ possesses the intended semantics (\cref{def:intended-semantics}), it holds:
    \[
        \forall \vg \in \calG, \; (\vbeta \circ \valpha) (\vg) = \vbeta^* (\vg)
    \]
\end{lemma}

\begin{proof}
    We make use of the fact that by \cref{def:intended-semantics} it holds:
    \begin{align}
        \valpha(\vg) &=
            (\vP_\pi \circ \vpsi  \circ \mathrm{id})(\vg)%
        \label{eq:aligned-concepts-supp}
        \\
        \vbeta(\vc) &= 
            (\vbeta^* \circ 
            \vpsi^{-1} \circ \vP_\pi^{-1} ) (\vc)
        \label{eq:aligned-knowledge-supp}
    \end{align}
    Therefore, combining \cref{eq:aligned-concepts-supp,eq:aligned-knowledge-supp} we get for all $\vg \in \calG$:
    \begin{align}
        (\vbeta \circ \valpha)(\vg) 
            &= (\vbeta^* \circ
            \vP_\pi^{-1} \circ \vpsi^{-1} \circ 
            \vpsi \circ \vP_\pi)(\vg) \\
            &= (\vbeta^* \circ
            \vP_\pi^{-1} \circ \vP_\pi)(\vg) \\
            &= \vbeta^* (\vg)
    \end{align}
    yielding the claim.
\end{proof}





\subsection{Proof of Theorem \ref{thm:count-jrss} and Corollary \ref{cor:count-rss}}



\begin{theorem}
    Let $s\calG := \bigcup_{i=1}^k \{|\calG_i|\}$ be the set of cardinalities of each concept $G_i \subseteq \vG$ and $ms\calG$ denote the multi-set 
    $ms\calG := \{ (|\calG_i|, m(|\calG_i|), \; i \in [k] \} $, where $m(\bullet)$ denotes the multiplicity of each element of $s\calG$. 
    Under \cref{assu:concepts,assu:labels}, the number of deterministic JRSs $(\valpha, \vbeta) \in \Vset{\calA} \times \Vset{\calB}$ amounts to:
    \begin{align}
        \textstyle
        &
        \sum_{(\valpha, \vbeta) \in \Vset{\calA} \times \Vset{\calB}} 
        \Ind{
             \bigwedge_{\vg \in \supp(\vG)}
            (\vbeta \circ \valpha)(\vg)
                =
                \vbeta^* (\vg)
        } - C[\calG]
        \label{eq:jrs-count-app}
    \end{align}  
    where $C[\calG]$ is the total number of pairs with the intended semantics, given by
    $
        C[\calG] := \prod_{\xi \in s\calG} m(\xi)! \times \prod_{i=1}^k |\calG_i|!
    $.
    
\end{theorem}

\begin{proof}
    Since we are considering pairs $(\valpha, \vbeta) \in \Vset{\calA} \times \Vset{\calB}$, we can stick to the fact that $\valpha$ defines a function $\valpha:\calG \to \calC$ and that, similarly, $\vbeta$ defines a function $\vbeta: \calC \to \calY$. In this case, $\vbeta \circ \valpha: \calG \to \calY$ is a map from ground-truth concepts to labels.
    We start by \cref{def:intended-semantics} and consider the pairs that attain maximum likelihood by \cref{lemma:deterministic-optima}: 
    \[
        \forall \vg \in \mathsf{supp}(p^*(\vG)), \; (\vbeta \circ \valpha) (\vg) = \vbeta^* ( \vg)
        \label{eq:maxlikelihood-supp}
    \]
    where, since $\valpha \in \Vset{\calA}$ and is deterministic, we can substitute $\vomega$ with $\vbeta$. Since both $\Vset{\calA}$ and $\Vset{\calB}$ are countable, we can convert this to a total count of pairs that attain maximum likelihood:
    \[
        \sum_{(\valpha, \vbeta) \in \Vset{\calA} \times \Vset{\calB}} 
        \Ind{
            \bigwedge_{\vg \in \mathsf{supp} (p^*(\vG)) } (\vbeta \circ \valpha) ( \vg) = \vbeta^* (\vg)
        } \label{eq:count-jrss-app}
    \]
    where, for each pair $(\valpha, \vbeta) \in \Vset{\calA} \times \Vset{\calB}$ the sum increases by one if the condition of \cref{eq:maxlikelihood-supp} is satisfied.
    
    Among these optimal pairs, there are those possessing the intended semantics by \cref{lemma:intended-semantics-optima}, which consists of possible permutations of the concepts and element-wise invertible transformations of the concept values. We start by evaluating the number of possible element-wise invertible transformations for each subset of concepts $\calG_i \subset \calG$. 

    Each concept $c_i$ has values in the subset $\calG_i$, for a  total of $|\calG_i|$ values. The overall number of possible invertible transformations is then given by the number of possible permutations, resulting in a total of $|\calG_i|!$ maps. 
    This means that the total number of element-wise invertible transformations is given by:
    \[
        \prod_{i=1}^k |\calG_i|!
    \]

    Next, we consider what permutation of the concepts are possible. To this end, consider the set and the multi-set comprising all different $\calG_i$ cardinalities given by:
    \[
        s\calG := \bigcup_{i=1}^k \{ |\calG_i| \} , 
        \quad
        ms\calG := \{( |\calG_i|, m(|\calG_i|): \; i \in [k]  \}
    \]
    where $m(\bullet)$ counts how many repetitions are present.
    When the multi-set contains repeated numbers, \ie when different $c_i$ and $c_j$ have the same cardinality, we have to account a possible permutation of the concept indices. Therefore, for each element $\xi \in s\calG$ we have that the total number of permutations of concepts amount to:
    \[
        \mathrm{perm}(\calG) :=
        \prod_{ \xi \in s\calG} m(\xi)!
    \]
    Finally, the total number of pairs possessing the intended semantics amount to $ \mathrm{perm}(\calG) \times \prod_{i=1}^k |\calG_i|!$, resulting that the total number of deterministic JRSs are:
    \[
        \sum_{(\valpha, \vbeta) \in \Vset{\calA} \times \Vset{\calB}} 
        \Ind{
            \bigwedge_{\vg \in \mathsf{supp} (p^*(\vG)) } (\vbeta \circ \valpha) ( \vg) = \vbeta^* (\vg)
        } - \mathrm{perm}(\calG) \times \prod_{i=1}^k |\calG_i|!
    \]
    yielding the claim.
\end{proof}


\countrss*

\begin{proof}
    The proof follows immediately by replacing $\Vset{\calB}$ with $\{\vbeta^*\}$, allowing only for the ground-truth inference function. In this context, by following similar steps of \cref{thm:count-jrss}, we arrive at a similar count to \cref{eq:count-jrss-app}, that is:
    \[
        \sum_{(\valpha, \vbeta) \in \Vset{\calA} \times \{ \vbeta^*\}
        } 
        \Ind{
            \bigwedge_{\vg \in \mathsf{supp} (p^*(\vG)) } (\vbeta \circ \valpha) ( \vg) = \vbeta^* (\vg)
        } \label{eq:count-rss-app}
    \]
    In the context of RSs, the only pair possessing the intended semantics is $(\id, \vbeta^*)$, which has to subtracted from \cref{eq:count-rss-app}. This shows the claim.
\end{proof}

\subsection{Proof of Theorem \ref{thm:implications}}

\thmimplications*






\begin{proof}
    Consider a pair $(\valpha, \vbeta) \in \calA \times \calB$, where both $\valpha$ and $\vbeta$ can be non-deterministic conditional probability distributions. 
    \textbf{Step} (i).
    We begin by first recalling \cref{lemma:abstraction-from-lh}. Under \cref{assu:concepts,assu:labels},
    a CBM $(\vf, \vomega) \in \calF \times \Omega$ is optimal, \ie it attains maximum likelihood, if it holds that: 
    \[
        \forall \vg \in \mathsf{supp}(p^*(\vG)), \;  \bbE_{\vx \sim p^*(\vX \mid \vg)} [(\vomega \circ \vf) (\vx)] = \vbeta^* (\vg)
    \]
    Notice that, according to \cref{assu:concepts,assu:labels}, it also holds that the labels must be predicted with probability one to attain maximum likelihood, that is:
    \[
        \forall \vg \in \calG, \; \max \vbeta^* (\vg) = 1
    \]
    Now consider a pair $(\valpha, \vbeta) \in \calA \times \calB$.
    By \cref{lemma:abstraction-from-lh} (i), it holds that to obtain maximum likelihood, the learned knowledge $\vbeta: \calC \to \Delta_\calY$ must be deterministic:
    \[
        \max \vbeta (\vc) =1
    \]
    which implies that, necessarily, to have optimal $\vbeta$ the space $\calB$ restricts to the set $\Vset{\calB}$, so any learned knowledge $\vbeta \in \Vset{\calB}$. 
    This shows that optimal pairs $(\valpha, \vbeta)$ are in $\calA \times \Vset{\calB}$.
    

    \textbf{Step} (ii). In this step, we make use of \cref{assu:monotonic} and consider only inference functions $\vomega$ with that property. 
    We start by considering a couple $(\va' , \vb') \in \Vset{\calA} \times \Vset{\calB}$ that attain maximum likelihood, that is according to \cref{thm:count-jrss}:
    \[
        \forall \vg \in \mathsf{supp} (p^*(\vG)), \; (\vb' \circ \va') (\vg) = \vbeta^* (\vg)
    \]
    Now, we have to prove that, given $\vb' \in \Vset{\calB}$,
    there are no $\valpha \in \calA \setminus \Vset{\calA}$ that can attain maximum likelihood
    for at least one possible choice of $\vomega' \in \Omega$,
    such that for all $\vc \in \calC$ it holds $\vomega'( \Ind{\vC = \vc} ) = \vb'(\vc)$. 
    Since $\calA$ is a simplex, we can always construct a non-deterministic conditional probability distribution $\valpha \in \calA$ from a convex combination of the vertices $\va_i \in \Vset{\calA}$, \ie for all $\vg \in \calG$ it holds:
    \[
        \valpha( \vg ) = \sum_{\valpha_i \in \Vset{\calA}} \lambda_i \va_i (\vg)
    \]
    Consider another $\va'' \neq \va' \in \Vset{\calA}$ and consider an arbitrary convex combination that defines a non-deterministic $\valpha \in \calA$:
    \[
    \label{eq:alpha-convex}
        \forall \vg \in \calG, \; \valpha (\vg) := \lambda \va'(\vg) + (1- \lambda) \va'' (\vg)
    \]
    where $\lambda \in (0,1)$. 
    When the deterministic JRSs count (\cref{eq:jrs-count}) equals to zero, we know that by \cref{thm:count-jrss} only $(\va', \vb')$ attains maximum likelihood, whereas $(\va'', \vb')$ is not optimal. 
    Therefore, there exists at least one $\hat \vg \in \mathsf{supp} (p^*(\vG)) $ such that
    \[
        (\vbeta' \circ \va')(\hat \vg) \neq (\vbeta' \circ \va'') (\hat \vg)
    \]
    We now have to look at the form of $\vf$ that can induce such a non-deterministic $\valpha = \lambda \va' + (1-\lambda) \va''$. 
    Recalling, the definition of $\valpha$  (\cref{eq:def-alpha}) we have that:
    \begin{align}
        \valpha (\vg) 
        &= \bbE_{\vx \sim p^*(\vX \mid \vg)} [ \vf(\vx) ]  \\
        &=  \lambda \va'(\vg) + (1-\lambda) \va'' (\vg)
    \end{align}
    For this $\valpha$, there are two possible kinds of $\vf \in \calF$ that can express it.

    (1) In one case, we can have $\vf: \bbR^n \to \Vset{\Delta_\calC}$ -- mapping inputs to ``hard'' distributions over concepts.  Since $\valpha$ is not a ``hard'' distribution (provided $\lambda \in (0, 1)$), $\vf(\vx)$ must be equal to $ \va'(\vg)$ for a fraction $\lambda$ of the examples $\vx \in \mathsf{supp} ( p^*(\vX \mid \vg ) )$ and to $\va''(\vg)$  for a fraction of $1-\lambda$. %
    In that case, it holds that there exist a subspace of non-vanishing measure $\calX'' \subset \mathsf{supp} ( p^*(\vX \mid \hat \vg) )$ such that $\vf(\vx) = \va''(\hat \vg)$. Therefore,
    for all $\vx \in \calX''$ we have that
    $ (\vb' \circ \va'')(\vx) =
    (\vomega \circ \va'')(\hat \vg) \neq \vbeta^*(\hat \vg)$, and such an $\vf$ is suboptimal.

    (2) The remaining option is that $\vf: \bbR^n \to \Delta_\calC$, excluding mapping to the vertices almost everywhere in $\mathsf{supp} ( p^*(\vX \mid \hat \vg) ) $. In light of this, we rewrite $\vf$ as follows:
    \[
        \forall \vx \in \mathsf{supp} ( p^*(\vX \mid \hat \vg) ), \; \vf(\vx) = \sum_{\vc \in \calC} p(\vc \mid \vx) \Ind{\vC = \vc}
    \]
    where $p(\vc \mid \vx) := \vf(\vx)_{\vc}$. Let $\hat \vc' := \va'(\hat \vg)$ and $\hat \vc'' := \va''(\hat \vg)$.  But $\valpha$ is a convex combination of $\va'$ and $\va''$, hence the function $\vf$ must attribute non-zero probability mass only to the concept vectors $\hat \vc'$ and $ \hat \vc''$. Hence, we rewrite it as:
    \[
        \forall \vx \in \calX \subseteq \mathsf{supp} ( p^*(\vX \mid \hat \vg) ), \; \vf(\vx) = p(\hat \vc' \mid \vx) \Ind{\vC = \hat \vc'} + p(\hat \vc'' \mid \vx) \Ind{\vC = \hat \vc''}
    \]
    where $\calX$ has measure one. 
    Notice that in this case it holds $\argmax_{\vy \in \calY} \vb' (\hat \vc')_{\vy} \neq \argmax_{\vy \in \calY} \vb' (\hat \vc'')_{\vy}$.
    By \cref{assu:monotonic} we have that, for any choice of $\lambda \in (0,1)$ it holds that for all $\vx \in \calX $:
    \begin{align}
        \max \vomega ( 
            p(\vc' \mid \vx) \Ind{\vC = \vc'} + p(\vc'' \mid \vx) \Ind{\vC = \vc''} 
            ) 
        &< \max ( \argmax_{\vy \in \calY} \vb' (\hat \vc')_{\vy},\argmax_{\vy \in \calY} \vb' (\hat \vc'')_{\vy}  ) 
        \tag{Using the condition from \cref{assu:monotonic}}
        \\
        &= 1 \label{eq:last-step}
    \end{align}
    where the last step in \cref{eq:last-step} follows from the fact that $\vb' \in \Vset{\calB}$, so giving point-mass conditional probability distributions. 

    (1) and (2) together show that any $\valpha \in \calA \setminus \Vset{\calA}$ constructed as a convex combination of $\va'$ and an $\va'' \neq \va'$ cannot attain maximum likelihood. Hence, 
    the optimal pairs restrict to $(\valpha, \vbeta) \in \Vset{\calA} \times \Vset{\calB}$ and since the count of JRSs (\cref{eq:jrs-count}) is zero, all $(\valpha, \vbeta) \in \calA \times \calB$ that are optimal also possess the intended semantics. This concludes the proof.    
\end{proof}














