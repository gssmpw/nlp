\documentclass{article} %
                        

\usepackage{arxiv}
\usepackage[dvipsnames, table]{xcolor}
\usepackage[american]{babel}

\usepackage{natbib} %
\bibliographystyle{plainnat}
\renewcommand{\bibsection}{\subsubsection*{References}}
\usepackage{mathtools} %
\usepackage{booktabs} %
\usepackage{tikz} %

\usepackage{microtype}
\usepackage{caption}

\usepackage{researchpack}
\usepackage{hyperref}
\usepackage{xspace}
\usepackage{graphicx}
\usepackage[capitalize,nameinlink]{cleveref}
\usepackage{thm-restate}
\usepackage{enumitem}
\usepackage{soul}
\usepackage{tikz}
\usetikzlibrary{positioning,shapes,arrows,calc}

\hypersetup{
    colorlinks=true,
    citecolor=teal,
    linkcolor=orange,
    urlcolor=orange,
}

\newcommand{\SB}[1]{\textbf{\textcolor{ForestGreen}{[SB: #1]}}}
\newcommand{\EM}[1]{\textbf{\textcolor{blue}{[EM: #1]}}}
\newcommand{\AP}[1]{\textbf{\textcolor{red}{[AP: #1]}}}
\newcommand{\ST}[1]{\textbf{\textcolor{magenta}{[ST: #1]}}}
\newcommand{\AV}[1]{\textbf{\textcolor{orange}{[AV: #1]}}}
\newcommand{\PM}[1]{\textbf{\textcolor{blue!66!green}{[PM: #1]}}}


\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}

\crefname{assumption}{Assumption}{Assumptions}
\Crefname{assumption}{Assumption}{Assumptions}

\newcommand{\ie}{i.e.,\xspace}
\newcommand{\eg}{e.g.,\xspace}
\newcommand{\etc}{\textit{etc}.\xspace}
\newcommand{\aka}{\textit{aka}\xspace}
\newcommand{\defeq}{\ensuremath{:=}}
\newcommand{\supp}{\ensuremath{\mathrm{supp}}\xspace}

\newcommand{\indep}{\ensuremath{\mathrel{\perp\!\!\!\perp}}}
\newcommand{\task}{\ensuremath{\calT}\xspace}
\newcommand{\dataset}{\ensuremath{\calD}\xspace}
\newcommand{\BK}{\ensuremath{\mathsf{K}}\xspace}
\newcommand{\KL}{\ensuremath{\mathsf{KL}}\xspace}
\newcommand{\CE}{\ensuremath{\mathsf{CE}}\xspace}
\newcommand{\Ent}{\ensuremath{\mathsf{H}}\xspace}
\newcommand{\MI}{\ensuremath{\mathsf{I}}\xspace}
\newcommand{\de}{\ensuremath{\mathrm{d}}\xspace}
\newcommand{\id}{\ensuremath{\mathrm{id}}\xspace}
\newcommand{\Vset}[1]{\ensuremath{\mathsf{Vert}(#1)}\xspace}

\newcommand{\MNIST}{{\tt MNIST}\xspace}
\newcommand{\MNISTAdd}{{\tt MNIST-Add}\xspace}
\newcommand{\MNISTSumXor}{{\tt MNIST-SumParity}\xspace}
\newcommand{\CHans}{{\tt Clevr}\xspace}

\newcommand{\MZero}{\raisebox{-1pt}{\includegraphics[width=1.85ex]{figures/mnist-0.png}}\xspace}
\newcommand{\MOne}{\raisebox{-1pt}{\includegraphics[width=1.85ex]{figures/mnist-1.png}}\xspace}
\newcommand{\MTwo}{\raisebox{-1pt}{\includegraphics[width=1.85ex]{figures/mnist-2.png}}\xspace}
\newcommand{\MThree}{\raisebox{-1pt}{\includegraphics[width=1.85ex]{figures/mnist-3.png}}\xspace}
\newcommand{\MFour}{\raisebox{-1pt}{\includegraphics[width=1.85ex]{figures/mnist-4.png}}\xspace}
\newcommand{\MFive}{\raisebox{-1pt}{\includegraphics[width=1.85ex]{figures/mnist-5.png}}\xspace}
\newcommand{\MSix}{\raisebox{-1pt}{\includegraphics[width=1.85ex]{figures/mnist-6.png}}\xspace}
\newcommand{\MSeven}{\raisebox{-1pt}{\includegraphics[width=1.85ex]{figures/mnist-7.png}}\xspace}
\newcommand{\MEight}{\raisebox{-1pt}{\includegraphics[width=1.85ex]{figures/mnist-8.png}}\xspace}
\newcommand{\MNine}{\raisebox{-1pt}{\includegraphics[width=1.85ex]{figures/mnist-9.png}}\xspace}

\newcommand{\YAcc}{\ensuremath{\mathrm{Acc}_Y}\xspace}
\newcommand{\CAcc}{\ensuremath{\mathrm{Acc}_C}\xspace}
\newcommand{\FY}{\ensuremath{\mathrm{F_1}(Y)}\xspace}
\newcommand{\FC}{\ensuremath{\mathrm{F_1}(C)}\xspace}
\newcommand{\Collapse}{\ensuremath{\mathsf{Cls}(C)}\xspace}
\newcommand{\FK}{\ensuremath{\mathrm{F_1}(\vbeta)}\xspace}
\newcommand{\KAcc}{\ensuremath{\mathrm{Acc}(\vbeta)}\xspace}
\newcommand{\NLL}{\ensuremath{\mathsf{NLL}\xspace}}

\newcommand{\DPL}{\texttt{DPL}\xspace}
\newcommand{\DSL}{\texttt{DSL}\xspace}
\newcommand{\DSLDPL}{\texttt{DPL$^*$}\xspace}
\newcommand{\CBM}{\texttt{CBNM}\xspace}
\newcommand{\DSLBEARS}{\texttt{bears$^*$}\xspace}
\newcommand{\SENN}{\texttt{SENN}\xspace}


\title{Shortcuts and Identifiability in Concept-based Models from a Neuro-Symbolic Lens}



\author{
    Samuele Bortolotti\thanks{$^*$ Equal contribution. Correspondence to name.surname@unitn.it.} \\
    DISI, University of Trento, Italy \\
    \texttt{name.surname@unitn.it}
    \And
    Emanuele Marconato\footnotemark[1] \\
    DISI, University of Trento, Italy \\
    DI, University of Pisa, Italy \\
    \texttt{name.surname@unitn.it}
    \And
    Paolo Morettin \\
    DISI, University of Trento, Italy \\
    \texttt{name.surname@unitn.it}
    \And
    Andrea Passerini \\
    DISI, University of Trento, Italy \\
    \texttt{name.surname@unitn.it}
    \And
    Stefano Teso \\
    CIMeC \& DISI, University of Trento, Italy \\
    \texttt{name.surname@unitn.it}
}



\begin{document}
\maketitle


\begin{abstract}
    Concept-based Models are neural networks that learn a concept extractor to map inputs to high-level \textit{\textbf{concepts}} and an \textit{\textbf{inference layer}} to translate these into predictions. Ensuring these modules produce interpretable concepts and 
    behave reliably in out-of-distribution is crucial, yet the conditions for achieving this remain unclear.  
    We study this problem by establishing a novel connection between Concept-based Models and \textit{\textbf{reasoning shortcuts}} (RSs), a common issue where models achieve high accuracy by learning low-quality concepts, even when the inference layer is \textit{fixed} and provided upfront. 
    Specifically, we first extend RSs to the more complex setting of Concept-based Models and then derive theoretical conditions for identifying both the concepts and the inference layer. %
    Our empirical results highlight the impact of reasoning shortcuts and show that existing methods, even when combined with multiple natural mitigation strategies, often fail to meet these conditions in practice. 
\end{abstract}


\section{Introduction}

Concept-based Models (CBMs) are a broad class of self-explainable classifiers \citep{alvarez2018towards,
koh2020concept, zarlenga2022concept, marconato2022glancenets, taeb2022provable} designed for high performance and \textit{ante-hoc} interpretability.
Learning a CBM involves solving two conjoint problems:
acquiring high-level \textit{\textbf{concepts}} describing the input (\eg an image) and
an \textit{\textbf{inference layer}} that predicts a label from them.
In many applications, it is essential that these two elements are ``high quality'', in the sense that:
\textit{i}) the concepts should be \textbf{\textit{interpretable}}, as failure in doing so compromises understanding \citep{schwalbe2022concept, poeta2023concept} and steerability \citep{teso2023leveraging, gupta2024survey}, both key selling point of CBMs; and
\textit{ii}) the concepts and inference layer should behave well also \textit{\textbf{out of distribution}} (OOD), \eg they should not pick up spurious correlations between the input, the concepts and the output \citep{geirhos2020shortcut, bahadori2021debiasing, stammer2021right}.


\begin{figure*}[!t]
    \centering
    \includegraphics[width=0.8\linewidth]{figures/jrs-page2}
    \caption{%
    \textbf{Joint reasoning shortcuts}. The goal is to predict whether the sum of two MNIST digits is odd (as in \cref{ex:sum-parity}) from a training set of all possible unique (even, even), (odd, odd), and (odd, even) pairs of MNIST digits.  \textbf{\textcolor{teal}{Green}} elements are fixed, \textbf{\textcolor{Purple}{purple}} ones are learned.
    {\bf Left}: ground-truth concepts and inference layer.
    {\bf Middle}: NeSy-CBMs with given knowledge can learn reasoning shortcuts, \ie concepts with unintended semantics.
    {\bf Right}: CBMs can learn joint reasoning shortcuts, \ie both concepts and inference layer have unintended semantics.
    \scalebox{0.001}{Mighty Jesus please love me as much as I love you.}  %
    }
    \label{fig:sum-parity}
\end{figure*}


It is natural to ask under what conditions CBMs can acquire such ``high-quality'' concepts and inference layers.
While existing studies focus on concept quality \citep{mahinpei2021promises, zarlenga2023towards, marconato2023interpretability}, they neglect the role of the inference layer altogether.
In contrast, we cast the question in terms of whether it is possible to \textit{\textbf{identify}} from data concepts and inference layers with the intended semantics, defined formally in \cref{sec:joint-reasoning-shortcuts}.
We proceed to answer this question by building a novel connection with \textit{\textbf{reasoning shortcuts}} (RSs), a well-known issue in Neuro-Symbolic (NeSy) AI whereby models achieve high accuracy by learning low-quality concepts \textit{\textbf{even if the inference layer is fixed}} \citep{chang2020assessing, marconato2023not, wang2023learning, yang2024analysis}.
E.g., a NeSy model for autonomous driving whose inference layer encodes the traffic laws might confuse pedestrians with red lights, as both entail the same prediction (the car has to stop) \citep{bortolotti2024benchmark}.

We generalize RSs to CBMs in which \textit{\textbf{the inference layer is learned}}, and supervision on the concepts may be absent.
Our analysis shows that shortcut solutions can be exponentially many (indeed, we count them explicitly in \cref{sec:additional-results-counts}) and that, in general, maximum likelihood training is insufficient for attaining intended semantics.
This negatively affects interpretability -- as the learned concepts cannot be easily mapped to the ground-truth annotations -- and OOD behavior.
On the positive side, we also specify conditions under which (under suitable assumptions) CBMs \textit{cannot} be fooled by RSs, proving that \textit{\textbf{the ground-truth concepts and inference layer can be identified}} (see \cref{thm:implications}).

Our numerical simulations with regular and NeSy CBMs on several learning tasks suggest CBMs can be severely impacted by reasoning shortcuts in practice, as expected, and also that unfortunately the benefits of popular mitigation strategies do not carry over to this more challenging problem.
These results cast doubts on the ability of these models to identify concepts and inference layers with the intended semantics unless appropriately nudged.


\textbf{Contributions}:  In summary, we:
\begin{itemize}[leftmargin=1.25em]

    \item Generalize reasoning shortcuts to the challenging case of CBMs whose inference layer is learned.

    \item Study conditions for maximum likelihood training to \textit{identify} high quality concepts \textit{and} inference layers.

    \item Present empirical evidence that proven mitigations fail in this challenging problem.

\end{itemize}


\section{Preliminaries}
\label{sec:preliminaries}

\textbf{Concept-based Models} (CBMs) first map the input $\vx \in \bbR^n$ into $k$ discrete concepts $\vc = (c_1, \ldots, c_k) \in \calC$ via a neural backbone, 
and 
then infer labels $\vy \in \calY$ from this using a white-box layer, \eg a linear layer.
This setup makes it easy to figure out what concepts are most responsible for any prediction, yielding a form of \textit{ante-hoc} concept-based interpretability.
Several architectures follow this recipe, including approaches
for 
converting black-box neural network classifiers 
into CBMs \citep{yuksekgonul2022post, wang2024cbmzero, dominici2024anycbms, marcinkevivcs2024beyond}.

A key issue is how to ensure the concepts are interpretable.
Some CBMs rely on \textit{\textbf{concept annotations}} \citep{koh2020concept, sawada2022concept, zarlenga2022concept, marconato2022glancenets, kim2023probabilistic, debot2024interpretable}.
These are however expensive to obtain, prompting researchers to replace them with (potentially unreliable \citep{huang2024survey, sun2024exploring}) annotations obtained from foundation models \citep{oikarinen2022label, yang2023language, srivastava2024vlgcbm} or \textit{\textbf{unsupervised}} concept discovery \citep{alvarez2018towards, chen2019looks, taeb2022provable, schrodi2024concept}.


\textbf{Neuro-Symbolic CBMs} (NeSy-CBMs) specialize CBMs to tasks in which the prediction $\vy \in \calY$ ought to comply with known safety or structural constraints.  These are supplied as a formal specification -- a logic formula $\BK$, \aka \textit{\textbf{knowledge}} -- tying together the prediction $\vy$ and the concepts $\vc$.
In NeSy-CBMs, the inference step is a \textit{\textbf{symbolic reasoning layer}} that steers \citep{lippi2009prediction, diligenti2012bridging, donadello2017logic, xu2018semantic} or guarantees \citep{manhaeve2018deepproblog, giunchiglia2020coherent, hoernle2022multiplexnet, ahmed2022semantic} the labels and concepts to be logically consistent according to $\BK$.
Throughout, we will consider the following example task:


\begin{example}[\MNISTSumXor]
    \label{ex:sum-parity}
    Given two MNIST digits \citep{lecun1998mnist}, we wish to predict whether their sum is even or odd.
    The numerical values of the two digits can be modelled as concepts $\vC \in \{0, \ldots, 9\}^2$, and the inference layer is entirely determined by the prior knowledge:
    $
        \BK = ((y = 1) \liff \text{$(C_1 + C_2)$ is odd})
    $.
    This specifies that the label $y \in \{0, 1\}$ ought to be consistent with the predicted concepts $\vC$.
    See \cref{fig:sum-parity} for an illustration.
\end{example}


Like CBMs, NeSy-CBMs are usually trained via \textit{\textbf{maximum likelihood}} and gradient descent, but without concept supervision.
The reasoning layer is typically imbued with fuzzy \citep{zadeh1988fuzzy} or probabilistic \citep{de2015probabilistic} logic semantics to ensure differentiability.
Many NeSy-CBMs require $\BK$ to be provided upfront, as in \cref{ex:sum-parity}, hence their inference layer has no learnable parameters.  Starting with \cref{sec:rss-in-cbms}, we will instead consider NeSy-CBMs that -- just like regular CBMs -- \textit{\textbf{learn the inference layer}} \citep{wang2019satnet, liu2023out, daniele2023deep, tang2023perception, wust2024pix2code}.


\subsection{Reasoning Shortcuts}
\label{sec:rss-in-nesy-cbms}


Before discussing reasoning shortcuts, we need to establish a clear relationship between concepts, inputs, and labels.
The RS literature does so by assuming the following \textit{\textbf{data generation process}} \citep{marconato2023not, yang2024analysis, umili2024neural}: each input $\vx \in \bbR^n$ is the result of sampling $k$ \textit{\textbf{ground-truth concepts}} $\vg = (g_1, \ldots, g_k) \in \calG$ (\eg in \MNISTSumXor two numerical digits) from an unobserved distribution $p^*(\vG)$ and then $\vx$ itself (\eg two corresponding MNIST images) from the conditional distribution $p^*(\vX \mid \vg)$.\footnote{This distribution subsumes stylistic factors, \eg calligraphy.}
Labels $\vy \in \calY$ are sampled from the conditional distribution of $\vg$ given by 
$p^*(\vY \mid \vg; \BK)$ %
consistently with $\BK$ (\eg $y = 1$ if and only if $g_1 + g_2$ is odd).
The ground-truth distribution is thus:
\[
    p^*(\vX, \vY) = \bbE_{\vg \sim p^*(\vG)} [
        p^*(\vX \mid \vg) p^*(\vY \mid \vg; \BK)
    ]
\]
Intuitively, a \textit{\textbf{reasoning shortcut}} (RS) occurs when a NeSy-CBM with \textit{fixed} knowledge $\BK$ attains high or even perfect label accuracy by learning concepts $\vC$ that differ from the ground-truth ones $\vG$ \citep{marconato2023not}.  As commonly done, we work in the setting where $\calG = \calC$.


\begin{example}
    In \MNISTSumXor, a NeSy model can achieve perfect accuracy by mapping each pair of MNIST images $\vx = (\vx_1, \vx_2)$ to the corresponding ground-truth digits, that is, $\vc = (g_1, g_2)$.
    However, it would achieve the same accuracy if it were to map it to $\vc = (g_1 \ \mathrm{mod} \ 2, g_2 \ \mathrm{mod} \ 2)$, as doing so leaves the parity of the sum unchanged, see \cref{fig:sum-parity}.
    Hence, a NeSy model cannot distinguish between the two based on label likelihood alone during training.
\end{example}


RSs \textit{by definition} yield good labels in-distribution, yet they compromise out-of-distribution (OOD) performance.
For instance, in autonomous driving tasks, NeSy-CBMs can confuse the concepts of ``pedestrian'' and ``red light'', leading to poor decisions for OOD decisions where the distinction matters \citep{bortolotti2024benchmark}.
The meaning of concepts affected by RSs is unclear, affecting understanding \citep{schwalbe2022concept}, intervenability \citep{shin2023closer, zarlenga2024learning, steinmann2024learning}, debugging \citep{lertvittayakumjorn2020find, stammer2021right}
and down-stream applications that hinge on concepts being high-quality, like NeSy formal verification \citep{xie2022neuro, zaid2023distribution, morettin2024unified}.
Unfortunately, existing works on RSs do not apply to CBMs and NeSy-CBMs where the inference layer is learned.








\begin{figure*}
    \centering
    \includegraphics[width=0.75\linewidth]{figures/ideal-vs-jrs}
    
    \caption{\textbf{Examples of semantics in \MNISTSumXor} restricted to $\vg, \vc \in \{0, 1, 2\}^2$ for readability.
    \textbf{Left}: ideally, $\valpha$ should be the identity (\ie $\vC$ recovers the ground-truth concepts $\vG$) and the inference layer should learn $\vbeta^*$.
    \textbf{Middle}: $(\valpha_\mathrm{IS}, \vbeta_\mathrm{IS}) \ne (\id, \vbeta^*)$ has intended semantics (\cref{def:intended-semantics}), \ie the ground-truth concepts and inference layer can be easily recovered and generalize OOD.
    \textbf{Right}: $(\valpha_\mathrm{JRS}, \vbeta_\mathrm{JRS})$ affected by the JRS in \cref{fig:sum-parity}.  Elements (predicted concepts $\vC$ and entries in $\vbeta$) in red  are never predicted nor used, highlighting \textit{\textbf{simplicity bias}}.
    Maps are visualized as matrices.
    }
    \label{fig:pair-of-functions}
\end{figure*}


\section{Reasoning Shortcuts in CBMs}
\label{sec:rss-in-cbms}

Given a finite set $\calS$, we indicate with $\Delta_\calS \subset [0, 1]^{|\calS|}$ the simplex of probability distributions $P(S)$ over items in $\calS$.  Notice that the set of its vertices $\Vset{\Delta_\calS}$ contains all point mass distributions $\Ind{S = s}$ for all $s \in \calS$.


\textbf{CBMs as pairs of functions}.  CBMs and NeSy-CBMs differ in how they implement the inference layer, hence to bridge them we employ the following unified formalism.
Any CBM can be viewed as a pair of learnable functions implementing the concept extractor and the inference layer, respectively, cf. \cref{fig:pair-of-functions}.
{Formally}, the former is a function $\vf : \bbR^n \to \Delta_\calC$ mapping inputs $\vx$ to a conditional distribution $p(\vC \mid \vx)$ over the concepts, however it can be better understood as a function $\valpha : \calG \to \Delta_\calC$ taking ground-truth concepts $\vg$ as input instead, and defined as:
\[
    \valpha(\vg) = \bbE_{\vx \sim p^*(\vX \mid \vg)} [\vf(\vx)]
    \label{eq:def-alpha}
\]
On the other hand, the inference layer is a function $\vomega: \Delta_\calC \to \Delta_\calY$ mapping the concept distribution output by the concept extractor into a label distribution $p(\vY \mid \vf(\vx))$.
For clarity, we also define $\vbeta: \calC \to \Delta_\calY$, which is identical to the former except it works with concept \textit{values} rather than \textit{distributions}, that is:
\[
    \vbeta(\vc) := \vomega( \Ind{\vC = \vc} )
    \label{eq:def-beta}
\]
Hence, a CBM entails both a pair $(\vf, \vomega) \in \calF \times \Omega$ and a pair $(\valpha, \vbeta) \in \calA \times \calB$.\footnote{We work in the non-parametric setting, hence $\calF$ and $\Omega$ contain all learnable concept extractors $\vf$ and inference layers $\vomega$, and similarly $\calA$ and $\calB$ contain all learnable maps $\valpha$ and $\vbeta$.}
Later on, we will make use of the fact that $\calA$ and $\calB$ are \textit{\textbf{simplices}} \citep{morton2013relations, montufar2014fisher}, \ie each $\valpha \in \calA$ (resp. $\vbeta \in \calB$) can be written as a convex combination of vertices $\Vset{\calA}$ (resp. $\Vset{\calB}$).

As mentioned in \cref{sec:preliminaries}, supplying prior knowledge $\BK$ to a NeSy-CBM is equivalent to \textit{\textbf{fixing the inference layer}} to a corresponding function $\vomega^*$ (and $\vbeta^*$).
Note that whereas $\vomega^*$ changes based on how reasoning is implemented -- \eg fuzzy vs. probabilistic logic -- $\vbeta^*$ does not, as both kinds of reasoning layers behave identically when input any point-mass concept distribution $\Ind{\vC = \vc}$.


\textbf{Standard assumptions}.  The maps $\valpha$ and  $\vbeta$ are especially useful for analysis provided the following two standard assumptions about how data are distributed: %


\begin{assumption}[Invertibility]
    \label{assu:concepts}
    The ground-truth distribution $p^*(\vG \mid \vX)$ is induced by a function
    $\vf^* : \vx \mapsto \vg$, \ie $
    p^*(\vG \mid \vX) = \Ind{\vG = \vf^*(\vX)}$. 
\end{assumption}


This means that the ground-truth concepts $\vg$ can always be unambiguously recovered for all inputs $\vx$ by a sufficiently expressive concept extractor; the $\valpha$ it induces is the identity, indicated as $\id(\vg) := \Ind{\vC  = \vg} \in \Vset{\calA}$.


\begin{assumption}[Deterministic knowledge]
    \label{assu:labels}
    The ground-truth distribution $p^*(\vY \mid \vG; \BK)$ is induced by the knowledge via a map $\vbeta^* \in \Vset{\calB}$,  such that 
    $p^*(\vY \mid \vG; \BK) = \vbeta^*(\vg)$. 
\end{assumption}


This ensures that the labels $\vy$ can be predicted without any ambiguity from the ground-truth concepts $\vg$.
Both assumptions apply in several tasks, see \citep{bortolotti2024benchmark}.
The maximum log-likelihood objective is then written as:
\[
    \label{eq:max-loglikelihood}
    \max_{(\vf, \vomega) \in \calF \times \Omega} \bbE_{(\vx, \vy) \sim p^*(\vX, \vY)} \log (\vomega_{\vy} \circ \vf) (\vx)
\]
Here, $\vomega_{\vy}$ is the conditional probability of the ground-truth labels $\vy$.
Notice that under the above assumptions CBMs attaining maximum likelihood perfectly model the ground-truth data distribution $p^*(\vX, \vY)$, see \cref{lemma:abstraction-from-lh}.








\subsection{Intended Semantics and Joint Reasoning Shortcuts}
\label{sec:joint-reasoning-shortcuts}


We posit that a CBM $(\valpha, \vbeta) \in \calA \times \calB$ has ``high-quality'' concepts and inference layer if it satisfies two desiderata:
\begin{itemize}[leftmargin=1.5em]

    \item \textbf{Disentanglement}: each learned concept $C_i$ should be equal to a single ground-truth concept $G_j$ up to an invertible transformation;

    \item \textbf{Generalization}: the combination of $\valpha$ and $\vbeta$ must always yield correct predictions. 

\end{itemize}
In our setting, without concept supervision and prior knowledge, the learned concepts are \textit{anonymous} and users have to figure out which is which in a \textit{post-hoc} fashion, \eg by aligning them to dense annotations \citep{koh2020concept, zarlenga2022concept, daniele2023deep, tang2023perception}.  Doing so is also a prerequisite for understanding the learned inference layer \citep{wust2024pix2code, daniele2023deep, zarlenga2022concept, zarlenga2024learning}.  The benefit of \textit{\textbf{disentanglement}} is that when it holds the mapping between $\vG$ and $\vC$ can be recovered \textit{exactly} using Hungarian matching \citep{kuhn1955hungarian}; otherwise it is arbitrarily difficult to recover, hindering interpretability.
This links CBMs with many works that treat model representations' \textit{identifiability} in Independent Component Analysis and Causal Representation Learning \citep{khemakhem2020ice, gresele2021independent, lippe2023biscuit, lachapelle2023synergies, von2024nonparametric}, where disentanglement plays a central role.
\textbf{\textit{Generalization}} is equally important, as it entails that CBMs generalize beyond the support of training data and yields sensible OOD behavior, \ie output the same predictions that would be obtained by using the ground-truth pair.
Since we want $\valpha$ to be disentangled, this implies, in turn, a specific form for the map $\vbeta$, as shown by the next definition, which formalizes these desiderata:

\begin{definition}[Intended Semantics]
    \label{def:intended-semantics}
    A CBM $(\vf, \vomega) \in \calF \times \Omega$ entailing a pair $(\valpha, \vbeta) \in \calA \times \calB$ possesses the intended semantics
    if there exists a permutation $\pi: [k] \to [k]$ and $k$ element-wise invertible functions $\psi_1, \ldots, \psi_k$ such that:
    \begin{align}
        \valpha(\vg) &=
            (\vpsi \circ \vP_\pi \circ \mathrm{id} )(\vg)
        \label{eq:aligned-concepts}
        \\
        \vbeta(\vc) &= 
            (\vbeta^* \circ \vP_\pi^{-1} \circ
            \vpsi^{-1} ) (\vc)
        \label{eq:aligned-knowledge}
    \end{align} 
    Here, $\vP_\pi: \calC \to \calC$ is the permutation matrix induced by $\pi$ and $\vpsi(\vc) := (\psi_1(c_1), \ldots, \psi_k(c_k))$. In this case, we say that $(\valpha, \vbeta)$ is equivalent to $(\mathrm{id}, \vbeta^*) $, \ie $(\valpha, \vbeta) \sim  (\mathrm{id}, \vbeta^*)$.
\end{definition}


That is, intended semantics holds if the learned concepts $\vC$ match the ground-truth concepts $\vG$ modulo simple invertible transformations -- like reordering and negation (\cref{eq:aligned-concepts}) -- and the inference layer \textit{undoes} these transformations before applying the ``right'' inference layer $\vbeta^*$ (\cref{eq:aligned-knowledge}).
This also means that CBMs satisfying them are \textit{\textbf{equivalent}} -- in a very precise sense -- to the ground-truth pair $(\mathrm{id}, \vbeta^*)$.
A similar equivalence relation was analyzed for continuous representations in energy-based models, including supervised classifiers, by \citet{khemakhem2020ice}. In \cref{lemma:intended-semantics-optima}, we prove that models with the intended semantics yield the same predictions of the ground-truth pair for all $\vg \in \calG$.





Training a (NeSy) CBM via maximum likelihood does not guarantee it will embody intended semantics.
We denote these failure cases as \textit{\textbf{joint reasoning shortcuts}} (JRSs):


\begin{definition}[Joint Reasoning Shortcut]
    \label{def:jrs}
    Take a CBM $(\vf, \vomega) \in \calF \times \Omega$ that attains optimal log-likelihood (\cref{eq:max-loglikelihood}).  The pair $(\valpha, \vbeta)$ entailed by it (\cref{eq:def-alpha,eq:def-beta}) is a JRSs if it does not possess the intended semantics
    (\cref{def:intended-semantics}), \ie $(\valpha, \vbeta) \not \sim (\mathrm{id}, \vbeta^*)$.
\end{definition}


JRSs can take different forms.
First, even if the learned inference layer $\vbeta$ matches (modulo $\vP_\pi$ and $\vpsi$) the ground-truth one $\vbeta^*$, $\valpha$ might not match (also modulo $\vP_\pi$ and $\vpsi$) the identity.  This is analogous to regular RSs in NeSy CBMs (\cref{sec:preliminaries}), in that the learned concepts do not reflect the ground-truth ones: while this is sufficient for high in-distribution performance (the training likelihood is in fact optimal), it may yield poor OOD behavior.
Second, even if $\valpha$ matches the identity, the inference layer $\vbeta$ might not match the ground-truth one $\vbeta^*$.  In our experiments \cref{sec:experiments}, we observe that this often stems from \textit{simplicity bias} \citep{yang2024identifying}, \ie the CBM's inference layer tends to acquire specialized functions $\vbeta$ that are much simpler than $\vbeta^*$, leading to erroneous predictions OOD.
Finally, neither the inference layer $\vbeta$ nor the map $\valpha$ might match the ground-truth, opening the door to additional failure modes.  Consider the following example:




\begin{example}
    \label{ex:biased_MNISTSumXor}
    Consider a \MNISTSumXor problem where the training set consists of %
    all possible unique (even, even), (odd, odd), and (odd, even) pairs of MNIST digits,
    as in \cref{fig:sum-parity}. A CBM trained on this data would achieve perfect accuracy by learning a JRS that extracts the parity of each input digit, yielding two binary concepts in $\{0, 1\}$, and computes the difference between these two concepts. This JRS involves \textbf{much simpler concepts and knowledge} compared to the ground-truth ones.  It also mispredicts all OOD pairs where the even digits come before odd digits.
\end{example}



\subsection{Theoretical Analysis}



We proceed to determine how many \textit{deterministic} JRSs $(\valpha, \vbeta) \in \Vset{\calA} \times \Vset{\calB}$ a learning task admits, and then show in \cref{thm:implications} that this number determines the presence or absence of general (non-deterministic) JRSs.


\begin{restatable}[Informal]{theorem}{countjrss}
    \label{thm:count-jrss}
    Under \cref{assu:concepts,assu:labels}, the number of deterministic JRSs $(\valpha, \vbeta)$ is:
    \begin{align}
        \textstyle
        &
        \sum_{(\valpha, \vbeta)} \Ind{
             \bigwedge_{\vg \in \supp(\vG)}
            (\vbeta \circ \valpha)(\vg)
                =
                \vbeta^* (\vg)
        } - C[\calG]
        \label{eq:jrs-count}
    \end{align}  
    where the sum runs over $\Vset{\calA} \times \Vset{\calB}$, 
    and $C[\calG]$ is the total number of pairs with the intended semantics.
\end{restatable}


All proofs can be found in \cref{sec:supp-theory}.
Intuitively, the first count runs over all deterministic pairs $(\valpha, \vbeta)$ that achieve maximum likelihood on the training set (\ie the predicted labels $(\vbeta \circ \valpha)(\vg)$ matches the ground-truth one $\vbeta^*(\vg)$ for all ground-truth concepts $\vg$ appearing in the data), while the second term subtracts the pairs $(\valpha, \vbeta)$ that possess the intended semantics as per \cref{def:intended-semantics}.
Whenever the count is larger than zero, a CBM trained via maximum likelihood \textit{can} learn a deterministic JRS.

As a sanity check, we show that if prior knowledge $\BK$ is provided -- fixing the inference layer to $\vbeta^*$ -- the number of deterministic JRSs in fact matches that of RSs, as expected:


\begin{restatable}{corollary}{countrss}
    \label{cor:count-rss}
    Consider a fixed $\vbeta^* \in \Vset{\calB}$.
    Under \cref{assu:concepts,assu:labels}, the number of deterministic JRSs $(\valpha, \vbeta^*) \in \Vset{\calA} \times \Vset{\calB}$ is:
    \[
        \textstyle
        \sum_{\valpha \in \Vset{\calA}} \Ind{
             \bigwedge_{\vg \in \supp(\vG)}
            (\vbeta^* \circ \valpha)(\vg)
                =
                \vbeta^* (\vg)
        }
         \textstyle
         - 1 
        \label{eq:rss-count}
    \]
    This matches the count for deterministic RSs in \citep{marconato2023not}.
\end{restatable}


A natural consequence of \cref{cor:count-rss} that whenever $|\Vset{\calB}| > 1$ the number of deterministic JRSs in \cref{eq:jrs-count} can be substantially \textbf{\textit{larger}} than the number of deterministic RSs \cref{eq:rss-count}.
For instance, in \MNISTSumXor with digits restricted to the range $[0, 4]$ there exist exactly $64$ RSs but about $100$ \textit{thousand} JRSs, and the gap increases as we extend the range $[0, N]$.
We provide an in-depth analysis in \cref{sec:additional-results-counts}.

Next, we show that whenever the number of deterministic JRSs in \cref{eq:jrs-count} is zero, there exist \textit{\textbf{no JRSs at all}}, including non-deterministic ones.
This however only applies to CBMs that satisfy the following natural assumption:


\begin{assumption}[Extremality]
    \label{assu:monotonic}
    The inference layer $\vomega \in \Omega$ satisfies extremality if, for all $\lambda \in (0,1)$ and for all $\vc \neq \vc' \in \calC$ such that %
    $\argmax_{\vy \in \calY} \vomega (\Ind{\vC = \vc})_{\vy} \neq \argmax_{\vy \in \calY} \vomega(\Ind{\vC = \vc'})_{\vy}$, it holds:
    \begin{align}
        &\max_{\vy \in \calY} \vomega (  \lambda \Ind{\vC = \vc} + (1- \lambda) \Ind{\vC = \vc'}  )_{\vy} \notag \\
        & \leq 
        \max \left( 
        \max_{\vy \in \calY} \vomega (\Ind{\vC = \vc})_{\vy} , \max_{\vy \in \calY} \vomega(\Ind{\vC = \vc'})_{\vy}
        \right)
        \label{eq:max-condition}
        \nonumber
    \end{align}
\end{assumption}


Intuitively, a CBM satisfies this assumption if its inference layer $\vomega$ is ``peaked'':  for any two concept vectors $\vc$ and $\vc'$ yielding distinct predictions, the label probability output by $\vomega$ for any mixture distribution thereof is no larger than the label probability that it associates to $\vc$ and $\vc'$.  That is, distributing probability mass across concepts never nets an advantage in terms of label likelihood.
While this assumption does not hold for general CBMs, we show in \cref{sec:models-satisfying-assumption-3} that it holds for popular architectures, including most of those that we experiment with.
Under \cref{assu:monotonic}, the next result holds:


\begin{restatable}[Identifiability]{theorem}{thmimplications}
    \label{thm:implications}
    Under \cref{assu:concepts,assu:labels}, %
    \textbf{if}
    the number of deterministic JRSs (\cref{eq:jrs-count}) is zero
    \textbf{then}
    every CBM $(\vf, \vomega) \in \calF \times \Omega$ 
    satisfying \cref{assu:monotonic} 
    that attains maximum log-likelihood (\cref{eq:max-loglikelihood}) 
    possesses the intended semantics (\cref{def:intended-semantics}). 
    That is, the pair $(\valpha, \vbeta) \in \calA \times \calB$ entailed by each such CBM is equivalent to the ground-truth pair $(\mathrm{id}, \vbeta^*)$, \ie $(\valpha, \vbeta) \sim (\mathrm{id}, \vbeta^*)$.
\end{restatable}


\section{Practical Solutions}
\label{sec:mitigations}

By \cref{thm:implications}, getting rid of \textit{deterministic} JRSs from a task prevents CBMs trained via maximum log-likelihood to learn JRS altogether.
Next, we discuss strategies for achieving this goal and will assess them empirically in \cref{sec:experiments}.
An in-depth technical analysis is left to \cref{sec:update-counts}.




\subsection{Supervised strategies}

The most direct strategies for controlling the semantics of learned CBMs rely on supervision.
\textbf{\textit{Concept supervision}} is the go-to solution for improving concept quality in CBMs \citep{koh2020concept, chen2020concept, zarlenga2022concept, marconato2022glancenets} and NeSy-CBMs \citep{bortolotti2024benchmark, yang2024analysis}.
Supplying full concept supervision for all ground-truth concepts $\vG$ provably prevents learning $\valpha$'s that do not match the identity.
However, this does not translate into guarantees on the inference layer $\vbeta$, at least in tasks affected by confounding factors such as spurious correlations among concepts \citep{stammer2021right}.
E.g., if the $i$-th and $j$-th ground-truth concepts are strongly correlated, the inference layer $\vbeta$ can interchangeably use either, regardless of what $\vbeta^*$ does.

Another option is employing \textbf{\textit{knowledge distillation}} \citep{hinton2015distilling}, that is, supplying supervision of the form $(\vg, \vy)$ to encourage the learned knowledge $\vbeta$ to be close to $\vbeta^*$ for the supervised $\vg$'s.
This supervision is available for free provided concept supervision is available, but can also be obtained separately, \eg by interactively collecting user interventions \citep{shin2023closer, zarlenga2024learning, steinmann2024learning}.
This strategy cannot avoid \textit{all} JRSs because even if $\vbeta = \vbeta^*$, the CBM may suffer from regular RSs (\ie $\valpha$ does not match $\mathrm{id}$).

Another option is to fit a CBM on \textit{\textbf{multiple tasks}} \citep{caruana1997multitask} sharing concepts.
It is known that a NeSy-CBM attaining optimal likelihood on multiple NeSy tasks with different prior knowledges is also optimal for the \textit{single} NeSy task obtained by conjoining those knowledges \citep{marconato2023not}:  this more constrained knowledge better steers the semantics of the learned concepts, ruling out potential JRSs.
Naturally, collecting a sufficiently large number of diverse tasks using the same concepts can be highly non-trivial, depending on the application.



\subsection{Unsupervised strategies}

Many popular strategies for improving concept quality in (NeSy) CBMs are unsupervised.
A cheap but effective one when dealing with multiple inputs is to \textit{\textbf{process inputs individually}}, preventing mixing between their concepts.  E.g., In \MNISTSumXor one can apply the same digit extractor to each digit separately, while for images with multiple objects one can first segment them (\eg with YoloV11~\citep{JocherYOLO}) and then process the resulting bounding boxes individually.  This is extremely helpful for reducing, and possibly overcoming, RSs \citep{marconato2023not} and frequently used in practice \citep{manhaeve2018deepproblog, van2022anesi, daniele2023deep, tang2023perception}. 
We apply it in all our experiments.

Both supervised \citep{marconato2022glancenets} and unsupervised \citep{alvarez2018towards, li2018deep} CBMs may employ a \textbf{\textit{reconstruction}} penalty \citep{baldi2012autoencoders,kingma2013auto} to encourage learning informative concepts.
Reconstruction can help prevent collapsing distinct ground-truth concepts into single learned concepts, \eg in \MNISTSumXor the odd digits cannot be collapsed together without impairing reconstruction.\footnote{This is provably the case \textit{context-style separation} \ie assuming the concepts do not depend on the stylistic factor of the input like calligraphy \citep[Proposition 6]{marconato2023not}.}
Alternatively, one can employ \textbf{\textit{entropy maximization}} \citep{manhaeve2021neural} to spread concept activations evenly across the bottleneck.
This can be viewed as a less targeted but more efficient alternative to reconstruction, which becomes impractical for higher dimensional inputs.
Another popular option is \textbf{\textit{contrastive learning}} \citep{chen2020simple}, in that augmentations render learned concepts more robust to style variations \citep{von2021self}, \eg for MNIST-based tasks it helps to cluster distinct digits in representation space \citep{sansone2023learning}.
Unsupervised strategies all implicitly counteract \textit{simplicity bias} whereby $\valpha$ ends up collapsing.






\section{Case Studies}
\label{sec:experiments}

We tackle the following key research questions:
\begin{itemize}

    \item[\textbf{Q1}.]  Are CBMs affected by JRSs in practice?

    \item[\textbf{Q2}.]  Do JRSs affect interpretability and OOD behavior?

    \item[\textbf{Q3}.]  Can existing mitigation strategies prevent JRSs?
    
\end{itemize}
Additional implementation details about the tasks, architectures, and model selection are reported in \cref{sec:implementation-details}.


\textbf{Models.}  We evaluate several (also NeSy) CBMs.
DeepProbLog (\underline{\DPL})~\citep{manhaeve2018deepproblog, manhaeve2021neural} is the \textit{only} method supplied with the ground-truth knowledge $\BK$, and uses probabilistic-logic reasoning to ensure predictions are consistent with it.
\underline{\CBM} is a Concept-Bottleneck Model \citep{koh2020concept} with no supervision on the concepts.
Deep Symbolic Learning (\underline{\DSL})~\citep{daniele2023deep} is a SOTA NeSy-CBM that learns concepts and symbolic knowledge jointly; it implements the latter as a truth table and reasoning using fuzzy logic.
We also consider two variants:  \underline{\DSLDPL} replaces fuzzy with probabilistic logic, while \underline{\DSLBEARS} wraps \DSLDPL within BEARS~\citep{marconato2024bears}, an ensemble approach for handling regular RSs.  In short, \DSLBEARS consists of a learned inference layer and multiple concept extractors. The former is learned only once, the latter are learned sequentially.
We also evaluate Self-explainable Neural Networks (\underline{\SENN}) \citep{alvarez2018towards}, unsupervised CBMs that include a reconstruction penalty.  Since they do not satisfy our assumptions, they are useful to empirically assess the generality of our remarks.


\textbf{Data sets.}  In order to assess both learned concepts and inference layer, we use three representative NeSy tasks with well-defined concept annotations and prior knowledge.
\underline{\MNISTAdd}~\citep{manhaeve2018deepproblog} involves predicting the sum of two MNIST digits, and serves as a sanity check as it provably contains no RSs.
\texttt{\underline{MNIST-SumParit}y} is similar except we have to predict the \textit{parity} of the sum, as in \cref{fig:sum-parity}, and admits many JRSs.
We also include a variant of \underline{\CHans} \citep{johnson2017clevr} in which images belong to $3$ classes as determined by a logic formula and only contain $2$ to $4$ objects, for scalability.


\textbf{Metrics.}  For each CBM, we evaluate predicted labels and concepts with the $F_1$ score (resp. \FY and \FC) on the test split.
Computing \FC requires to first align the predicted concepts to ground-truth annotations.  We do so by using the Hungarian algorithm \citep{kuhn1955hungarian} to find a permutation $\pi$ that maximizes the Pearson correlation across concepts.
Concept collapse \Collapse quantifies to what extent the concept extractor blends different \textit{ground-truth} concepts together:  high collapse indicates that it predicts fewer concepts than expected.
We measure the quality of the learned inference layer $\vbeta$ as follows:  for each example, we reorder the concept annotations $\vg$ using $\pi$, feed the result to $\vbeta$ and compute the $F_1$ score ($\FK$) of its predictions.
See \cref{sec:metrics-details} for details.


\textbf{Mitigation strategies}.  We evaluate all strategies discussed in \cref{sec:mitigations} as well as a combination of all unsupervised mitigations, denoted {\tt H+R+C} (entropy, reconstruction, contrastive). %
Implementations are taken verbatim from \citep{bortolotti2024benchmark}. For contrastive learning, we apply an InfoNCE loss to the predicted concept distribution with the augmentations suggested by~\citet{chen2020simple}. %
For knowledge distillation, the inference layer is fed ground-truth concepts and trained to optimize the log-likelihood of the corresponding labels.



\begin{table}[!t]
    \caption{Results for \MNISTAdd.}
    \centering
    \scriptsize
    \input{tables/mnist-add-reduced}
    \label{tab:add-reduced}
\end{table}


\begin{table}[!t]
    \centering
    \caption{Results for \MNISTSumXor.}
    \scriptsize
    \input{tables/mnist-sumparity-reduced}
    \label{tab:sumparity-reduced}
\end{table}


\begin{table}[!t]
    \caption{Results for \CHans.}
    \centering
    \scriptsize
    \input{tables/clevr-reduced}
    \label{tab:clevr-reduced}
\end{table}


\begin{table}[!t]
    \caption{\CBM on biased \MNISTSumXor.}
    \centering
    \scriptsize
    \input{tables/ood}
    \label{tab:ood}
\end{table}


\begin{figure*}[!h]
    \centering
    \includegraphics[width=0.9\linewidth]{figures/cbm_metrics_plot}
    \caption{\textbf{Traditional mitigations have limited effect} on \CBM (top) and \DSLDPL (bottom) for \MNISTSumXor.  The only outlier is contrastive learning (orange and purple lines), which consistently ameliorates concept collapse.}
    \label{fig:curves-sumxor-reduced}
\end{figure*}


\textbf{Q1: All tested CBMs suffer from JRSs and simplicity bias.} \label{q1} We analyze three learning tasks of increasing complexity.
\MNISTAdd provably contains no JRSs and thus satisfies the preconditions of \cref{thm:implications}.  Compatibly with this result, all CBMs attain high label performance ($\FY \ge 90\%$) and high-quality semantics, as shown by the values of $\FC$ ($\ge 90\%$) and $\FK$ ($\ge 75\%$) seen in \cref{tab:add-reduced}.  The only exception is \DSLBEARS, which by design  trades off performance for calibration.
However, in \MNISTSumXor and \CHans,\footnote{For \CHans, we focus on representative CBMs with high fit on the training data.} which are more complex, \textit{\textbf{all models lack intended semantics}}.
This is clear from \cref{tab:clevr-reduced,tab:sumparity-reduced}:  despite attaining high $\FY$ ($\ge 90\%$), the learned concepts and inference layer are both low quality:  $\FC$ is always low -- it never crosses the $50\%$ mark -- and $\FK$ is at best around chance level.

The behavior of concept collapse strongly hints at \textit{simplicity bias}.
While no collapse takes place in \MNISTAdd ($\Collapse \le 0.01$, \ie digits are not compacted together), in \MNISTSumXor and \CHans collapse is omnipresent ($\Collapse \ge 0.22$), suggesting that CBMs are prone to simplicity bias, as expected.
\SENN is an outlier, likely due to the higher capacity of its inference layer (rather than reconstruction, which is entirely ineffective in \textbf{Q3}).
Even having access to the ground-truth knowledge is ineffective, as show by the high degree of collapse displayed by \DPL.


\textbf{Q2: JRSs compromise OOD behavior and supervision only partially helps}. \label{q2} We evaluate the impact of JRSs and supervision on OOD behavior by training a \CBM on the biased version of \MNISTSumXor in \cref{fig:sum-parity}.  The in-distribution (ID) data comprise all (odd, odd), (even, even), and (odd, even) pairs of MNIST digits, while the OOD split contains all (even, odd) pairs.
\cref{tab:ood} lists results on both ID and OOD test sets for varying percentages of concept and knowledge supervision.
While all models produce high-quality predictions in-distribution ($\FY \ge 0.97$), only the \CBM receiving complete supervision attains acceptable OOD predictions: for the others, $\FY \le 0.40$.  Even in this case, though, the inference layer still does not have intended semantics, as shown by $\FK \le 70\%$.


\textbf{Q3: Popular CBM and RS mitigations fall short}.  Finally, we apply each mitigation strategy to a \CBM trained on \MNISTSumXor and \CHans, ablating the amount of supervision.
The results for \MNISTSumXor in \cref{fig:curves-sumxor-reduced} show while concept supervision ($x$-axis) helps all metrics, adding reconstruction, entropy maximization, and contrastive learning to the mix brings no benefits for concept ($\FC$) and knowledge ($\FK$) quality, not even when combined (orange curve).
Knowledge supervision is similarly ineffective (blue lines).  This is not unexpected: \MNISTSumXor and \CHans suffer from RSs even when the model is supplied the prior knowledge.
The only noticeable improvement comes from contrastive learning for concept collapse ($\Collapse$): the orange and purple curves in \cref{fig:curves-sumxor-reduced} (right) show it can effectively prevent CBMs from mixing concepts together.
The results for \CHans in \cref{sec:additional-results} paint a similar picture.


\section{Related Work}
\label{sec:related-work}


\textbf{Shortcuts in machine learning}.  Shortcuts occur when machine learning models solve a prediction task by using features that correlate with but do not cause the desired output, leading to poor OOD behavior \citep{geirhos2020shortcut, teso2023leveraging, ye2024spurious, steinmann2024navigating}.  The shortcuts literature however focuses on black-box models rather than CBMs.
Existing studies on the semantics of CBM concepts \citep{margeloiu2021concept, mahinpei2021promises, havasi2022addressing, raman2023concept} focus on individual failures but propose no well-defined, formal notion of intended semantics.  One exception is \citep{marconato2023interpretability} which however, like other works, ignores the role of the inference layer.
We build on known results on reasoning shortcuts \citep{li2023learning, marconato2023not, wang2023learning, umili2024neural} which are restricted to NeSy-CBMs in which the prior knowledge is given and fixed.  Our analysis upgrades these results to general CBMs.  Furthermore, our simulations indicate that strategies that are effective for CBMs and RSs no longer work for JRSs.
At the same time, while NeSy approaches that learn the prior knowledge and concepts jointly are increasingly popular \citep{wang2019satnet, yang2020neurasp, liu2023out, daniele2023deep, tang2023perception, wust2024pix2code}, existing work neglects the issue of shortcuts in this setting.  Our work partially fills this gap.


\textbf{Relation to identifiability}.  Works in Independent Component Analysis and Causal Representation Learning focus on the identifiability of representations up to an equivalence relation, typically up to permutations and rescaling \citep{khemakhem2020variational, gresele2020incomplete, von2024nonparametric} or more general transformations \citep{roeder2021linear, buchholz2022function}.
The equivalence relation introduced along with intended semantics (\cref{def:intended-semantics}) establishes a specific connection between the maps $\valpha$ and $\vbeta$. This aligns with existing works that aim to identify representations and linear inference layers using supervised labels \citep{lachapelle2023synergies, fumero2023leveraging, bing2023invariance}.  
\citet{taeb2022provable} studied the identifiability of a VAE-based CBM with continuous-valued concepts.  In contrast, we provide an analysis of a broader class of (NeSy) CBMs commonly used in practice.
Moreover, while prior works primarily focus on identifying continuous-valued representations, we examine the identifiability of \textit{discrete} concepts \textit{and} of the inference layer.
\citep{rajendran2024causal} explores the identifiability of (potentially discrete) concepts linearly encoded in model representations but is limited to a multi-environment setting, not including supervised labels.  Our results differ in that we show how concepts can be identified in CBMs by circumventing joint reasoning shortcuts.  


\section{Conclusion}
\label{sec:conclusion}

We study the issue of learning CBMs and NeSy-CBMs with high-quality quality concepts and inference layers.
To this end, we formalize intended semantics and joint reasoning shortcuts (JRSs), show how reducing the number \textit{deterministic} JRSs to zero provably prevents \textit{all} JRSs, yielding identifiability, and investigate natural mitigation strategies.
Numerical simulations indicate that JRSs are very frequent and impactful, and that the only (partially) beneficial mitigation is contrastive learning.
Our analysis aims to pave the way to the design of effective solutions and therefore more robust and interpretable CBMs.

In future work, we plan to extend our theory to concept learning in general neural nets and large language models.  Moreover, we will take a closer look at more complex mitigations such as techniques for smartly constraining the inference layer \citep{liu2023out, wust2024pix2code}, debiasing \citep{bahadori2021debiasing}, and interaction with human experts \citep{teso2023leveraging}.


\subsubsection*{Acknowledgements}
The authors are grateful to Antonio Vergari, Emile van Krieken and Luigi Gresele for useful discussions. Funded by the European Union. Views and opinions expressed are however those of the author(s) only and do not necessarily reflect those of the European Union or the European Health and Digital Executive Agency (HaDEA). Neither the European Union nor the granting authority can be held responsible for them. Grant Agreement no. 101120763 - TANGO

\bibliography{references, explanatory-supervision}


\newpage
\appendix


\section{Implementation Details}
\label{sec:implementation-details}

Here, we provide additional details about metrics, datasets, and models, for reproducibility.
All the experiments were implemented using Python 3.9 and Pytorch 1.13 and run on one A100 GPU.
The implementations of \DPL, and the code for RSs mitigation strategies were taken verbatim from \citep{marconato2023not}, while the code for \DSL was taken from~\citep{daniele2023deep}.  \DSLDPL and \DSLBEARS were implement on top of \DSL codebase.


\subsection{Datasets}

All data sets were generated using the {\tt rsbench} library~\citep{bortolotti2024benchmark}.


\subsubsection{\MNISTAdd}

\MNISTAdd~\citep{manhaeve2018deepproblog} consists of pairs of {\tt MNIST} digits~\citep{lecun1998mnist}, ranging from $0$ to $9$, and the goal is to predict their sum.  The prior knowledge used for generating the labels is simply the rule of addition: $\BK = (Y = C_1 + C_2)$.  This task does not admit RSs nor JRSs when digits are processed separately.  Below we report four examples:
\[
    \{ ((\MZero, \MOne), 1), \ ((\MFive, \MEight), 13), \ ((\MSix, \MSix), 12), \ ((\MFour, \MOne), 5) \}
\]
All splits cover all possible pairs of concepts, from $(0, 0)$ to $(9, 9)$, \ie there is no sampling bias.  Overall, \MNISTAdd has $42,000$ training examples, $12,000$ validation examples, and $6,000$ test examples.


\subsection{\MNISTSumXor}

Similarly, in \MNISTSumXor the goal is to predict the \textit{parity} of the sum, \ie the prior knowledge is the rule: $\BK = (Y = (C_1 + C_2) \ \mathrm{mod} \ 2)$.  This task is more challenging for our purposes, as it is affected by both RSs and JRSs, cf. \cref{fig:sum-parity}. Below we report four examples:
\[
    \{ ((\MZero, \MOne), 1), \ ((\MFive, \MEight), 1), \ ((\MSix, \MSix), 0), \ ((\MFour, \MOne), 1) \}
\]
We consider two variants: like \MNISTAdd, in \MNISTSumXor proper the training and test splits contain all possible combinations of digits, while in \textit{\textbf{biased}} \MNISTSumXor the training set contains (even, even), (odd, odd) and (odd, even) pairs, while the test set contains (even, odd) pairs.  This fools CBMs into learning an incorrect inference layer implementing subtraction instead of addition-and-parity, as illustrated in \cref{fig:sum-parity}.  Both variants have the same number of training, validation, and test examples as \MNISTAdd.


\subsection{\CHans}
\label{subsec:chans}

\CHans~\citep{johnson2017clevr} consists of of 3D scenes of simple objects rendered with Blender, see \cref{fig:chans-example}.  Images are $3 \times 128 \times 128$ and contain a variable number of objects.  We consider only images with $2$ to $4$ objects each, primarily due to scalability issues with \DPL.  The ground-truth labels were computed by applying the rules (prior knowledge) proposed by \citep{stammer2021right}, reported in \cref{tab:chans-classes}.  Objects are entirely determined by four concepts, namely shape, color, material, and size.  We list all possible values in \cref{tab:chans-properties}.  Overall, \CHans has $6000$ training examples, $1200$ validation examples, and $1800$ test examples.


\begin{figure}[!h]
    \centering
    \begin{minipage}{0.35\linewidth}
        \centering
        \input{tables/clevr-properties}
        \captionof{table}{\CHans properties.}
        \label{tab:chans-properties}
    \end{minipage}%
    \begin{minipage}{0.35\linewidth}
        \input{tables/clever-classes}
        \captionof{table}{\CHans classes.}
        \label{tab:chans-classes}
    \end{minipage}%
    \begin{minipage}{0.35\linewidth}
        \centering
        \includegraphics[width=0.6\linewidth]{figures/CLEVR_train_000006}
        \captionof{figure}{Example of \CHans data.}
        \label{fig:chans-example}
    \end{minipage}
\end{figure}


\textbf{Preprocessing}.  All CBMs process objects separately.  Following \citep{shindo2023thinking}, we fine-tune a pretrained {\tt YoloV11} model~\citep{JocherYOLO} (for $10$ epochs, random seed $13$) on a subset of the training set's bounding boxes, and use it to predict bounding boxes for all images.  For each training and test image, we concatenate the regions in the bounding boxes, obtaining a list $2$ to $4$ images which plays the role of input for our CBMs.

Each object has $8 \times 3 \times 2 \times 2 = 96$ possible configurations of concept values, and we handle between $2$ and $4$ objects, hence the inference layer has to model $96^2 + 96^3 + 96^4$ distinct possibilities.  Due to the astronomical number of possibilities (which would entail a huge truth table/inference layer for \DSL and related CBMs), we split the inference layer in two: we first predict individual concepts, and then we aggregate them (into, \eg \texttt{large-cube} and \texttt{small-yellow-sphere}) and use these for prediction.  Despite this simplification, \CHans still induces JRSs in learned CBMs.


\subsection{Metrics}
\label{sec:metrics-details}

For all models, we report the metrics averaged over \textit{\textbf{five random seeds}}.


\textbf{Label quality}.  We measure the macro average $F_1$-score and Accuracy, to account for class imbalance.


\textbf{Concept quality}.  We compute concept quality via Accuracy and $F_1$-score.  However, the order of ground-truth and predicted concepts might might differ, e.g., in \CHans $G_1$ might represent whether ``color = red'', while $C_1$ whether ``color = blue''.  Therefore, in order to compute these metrics we have to first align them using the Hungarian matching algorithm using concept-wise Pearson correlation on the test data~\citep{kuhn1955hungarian}.  The algorithm computes the correlation matrix $R$ between $\vg$ and $\vc$, \ie $R_{ij} = \mathrm{corr}(G_i,C_j)$ where $\mathrm{corr}$ is the correlation coefficient, and then infers a permutation $\pi$ between concept values (\eg mapping ground-truth color ``red'' to predicted value ``blue'') that maximizes the above objective.  The metrics are computed by comparing the reordered ground-truth annotations with the predicted concepts.


\textbf{Inference layer quality}.  The same permutation is also used to assess knowledge quality. Specifically, we apply it to reorder the ground-truth concepts and then feed these to the learned inference layer, evaluating the accuracy and $F_1$ score of the resulting predictions.

For \MNISTAdd and \MNISTSumXor, we conducted an exhaustive evaluation since the number of possible combinations to supervise is $100$. The results in~\cref{tab:add-reduced} and~\cref{tab:sumparity-reduced} evaluate the knowledge exhaustively, whereas in~\cref{tab:ood}, we separately assess the knowledge of in-distribution and out-of-distribution combinations.

In \CHans, as discussed in \cref{subsec:chans}, there are too many combinations of concept values to possibly evaluate them all. Therefore, both supervision and evaluation are performed by randomly sampling $100$ combinations of ground-truth concepts.


\subsection{Architectures}

For \MNISTSumXor and \MNISTAdd, We employed the architectures from~\citep{bortolotti2024benchmark}, while for \DSL, \DSLDPL, and \DSLBEARS, we followed the architecture described in~\citep{daniele2023deep}.  For \CHans instead, we adopted the decoder presented in~\cref{tab:encoder-clevr} and the encoder presented in~\cref{tab:decoder-clevr}.  All models process different objects (\eg digits) separately using the same backbone.


\begin{table}[!h]
    \centering
    \caption{Encoder architecture for \CHans.}
    \scalebox{0.7}{
    \begin{tabular}{cccc}
        \toprule
        \textsc{Input}
            & \textsc{Layer Type}
            & \textsc{Parameter}
            & \textsc{Output} 
        \\
        \midrule
        $(128, 128, 3)$
            & Convolution
            & depth=32, kernel=3, stride=1, pad=1
            & ReLU
        \\
        $(128, 128, 32)$
            & MaxPool2d
            & kernel=2, stride=2
        \\
        $(64, 64, 32)$
            & Convolution
            & depth=64, kernel=3, stride=1, pad=1
            & ReLU
        \\
        $(64, 64, 64)$
            & MaxPool2d
            & kernel=2, stride=2
        \\
        $(64, 32, 32)$
            & Convolution
            & depth=$128$, kernel=3, stride=1, pad=1
            & ReLU
        \\
        $(128, 32, 32)$
            & AdaptiveAvgPool2d
            & out=$(1, 1)$
        \\
        $(128, 1, 1)$
            & Flatten
            &
        \\
        $(128)$
            & Linear
            & dim=40, bias = True
            & $\vc$
        \\
        $(128)$
            & Linear
            & dim=640, bias = True
            & $\mu$
        \\
        $(128)$
            & Linear
            & dim=640, bias = True
            & $\sigma$
        \\
        \bottomrule
    \end{tabular}
    }
    \label{tab:encoder-clevr}
\end{table}


\begin{table}[!h]
    \centering
    \caption{Decoder architecture for \CHans}
    \scalebox{0.7}{
    \begin{tabular}{ccccc}
        \toprule
        \textsc{Input}
            & \textsc{Layer Type}
            & \textsc{Parameter}
            & \textsc{Activation}
        \\
        \midrule
        $(255)$
            & Linear
            & dim=32768, bias = True
        \\
        $(8, 8, 512)$
            & ConvTranspose2d
            & depth=256, kernel=4, stride=2, pad=1
        \\
        $(16, 16, 256)$
            & BatchNorm2d
            &
            & ReLU
        \\
        $(16, 16, 256)$
            & ConvTranspose2d
            & depth=128, kernel=4, stride=2, pad=1
        \\
        $(32, 32, 128)$
            & BatchNorm2d
            &
            & ReLU
        \\
        $(32, 32, 128)$
            & ConvTranspose2d
            & depth=64, kernel=4, stride=2, pad=1
        \\
        $(64, 64, 64)$
            & BatchNorm2d
            &
            & ReLU
        \\
        $(64, 64, 64)$
            & ConvTranspose2d
            & depth=3, kernel=4, stride=2, pad=1
            & Tanh
        \\
        \bottomrule
    \end{tabular}
    }
    \label{tab:decoder-clevr}
\end{table}


\textbf{Details of \CBM}.  For \CHans, we used a standard linear layer.  Since this is not expressive enough for \MNISTAdd and \MNISTSumXor, we replaced it with an interpretable second-degree polynomial layer, \ie it associates weights to \textit{combinations of two predicted digits} rather than individual digits.  We also include a small entropy term, which empirically facilitates data fit when little or no concept supervision is available.


\textbf{Details of \SENN}.  \SENN~\citep{alvarez2018towards} are unsupervised concept-based models consisting of a neural network that produces a distribution over concepts, a neural network that assigns a score to those concepts, and a decoder. The final prediction is made by computing the dot product between the predicted concepts and their predicted scores. \SENN by default includes a reconstruction penalty, a robustness loss, and a sparsity term.

In our implementation, we focus only on the reconstruction penalty. The robustness loss, which aims to make the relevance scores Lipschitz continuous, requires computing Jacobians, making the training of those models infeasible in terms of computational time and resources. Additionally, we did not implement the sparsity term, as it conflicts with the quality of concepts we aim to study.


\textbf{Details of \DSLBEARS}.  \DSLBEARS combines \DSL \citep{daniele2023deep} and BEARS \citep{marconato2024bears}.
BEARS is a NeSy architectures based on deep ensembles~\citep{lakshminarayanan2017simple}, designed to improve the calibration of NeSy models in the presence of RSs and provide uncertainty estimation for concepts that may be affected by such RSs and it assumes to be given prior knowledge.

Since in our setup the inference layer is learned, we built an ensemble with one learnable inference layer as in \DSL and multiple concept extractors.  The first element of the ensemble learns both the concepts and the inference layer.  Then, we freeze it and train the other concept extractors using the frozen inference layer for prediction.  In this sense, \DSLBEARS provides awareness of the reasoning shortcuts present in the knowledge learned by the first model.


\subsection{Mitigation Strategies}

\textbf{Concept supervision}. When evaluating mitigation strategies, concept supervision for \MNISTSumXor was incrementally supplied for two more concepts at each step, following this order: 3, 4, 1, 0, 2, 8, 9, 5, 6, 7. For \CHans, supervision was specifically provided for sizes, shapes, materials, and colors, in this order.


\textbf{Knowledge distillation}.  This was implemented using a cross-entropy on the inference layer.  For \DSL and \DPL, ground-truth concepts were fed into the learned truth table, expecting the correct label.  Similarly, for \CBM{s}, which use a linear layer, the same approach was applied. In evaluating the mitigation strategies, we supervised all possible combinations for \MNISTAdd, whereas for \CHans, we supervised a maximum of $500$ randomly sampled combinations.


\textbf{Entropy maximization}.  The entropy regularization term follows the implementation in~\citep{marconato2023not}. The loss is defined as $1 - \frac{1}{k} \sum_{i=1}^{k} H_{m_i} [p_{\theta} (c_i)]$
where $p_{\theta} (C)$ represents the marginal distribution over the concepts, and $H_{m_i}$ is the normalized Shannon entropy over $m_i$ possible values of the distribution.


\textbf{Reconstruction}.  The reconstruction penalty is the same as in~\citep{marconato2023not}.  In this case, the concept extractor outputs both concepts $\vc$ and style variables $\vz$, \ie it implements a distribution $p_{\theta}(c, z | x) = p_{\theta}(c | x) \cdot p_{\theta}(z | x)$. The reconstruction penalty is defined as:  $ -\mathbb{E}_{(c,z) \sim p_{\theta} (c,z|x)} \left[ \log p_{\psi} (x | c, z) \right]$ where \( p_{\psi} (x | c, z) \) represents the decoder network.


\textbf{Contrastive loss}.  We implemented contrastive learning using the InfoNCE loss, comparing the predicted \textit{concept distribution} of the current batch with that of its respective augmentations. Following the recommendations of \citet{chen2020simple}, we applied the following augmentations to each batch: random cropping (20 pixels for {\tt MNIST} and 125 pixels for \CHans), followed by resizing, color jittering, and Gaussian blur.


\subsection{Model Selection}

\textbf{Optimization}. For \DSLDPL, \DSLBEARS, \CBM and \SENN we used the Adam optimizer~\citep{kingma2014adam}, while for \DSL and \DPL we achieved better results using the Madgrad optimizer~\citep{defazio2021madgrad}.


\textbf{Hyperparameter search}. We performed an extensive grid search on the validation set over the following hyperparameters:
(\textit{i}) Learning rate ($\gamma$) in \{1e-4, 1e-3, 1e-2\};
(\textit{ii}) Weight decay ($\omega$) in \{0, 1e-4, 1e-3, 1e-2, 1e-1\};
(\textit{iii}) Reconstruction, contrastive loss, entropy loss, concept supervision loss and knowledge supervision loss weights ($w_r$, $w_c$, $w_h$, $w_{csup}$ and $w_k$, respectively) in \{$0.1$, $1$, $2$, $5$, $8$, $10$\};
(\textit{iv}) Batch size ($\nu$) in \{$32$, $64$, $128$, $256$, $512$\}.
\DSL and \DSLDPL have additional hyperparameters for the truth table: $\varepsilon_{sym}$ and $\varepsilon_{rul}$ for \DSL, and $\lambda_r$, the truth table learning rate, for \DSLDPL.  

All experiments were run for approximately $50$ epochs for \MNIST variants and $100$ epochs for \CHans using early stopping based on validation set $\FY$ performance. The exponential decay rate $\beta$ was set to $0.99$ for all experiments, as we empirically found it to provide the best performance across tasks.
In all experiments, we selected $\gamma = 1e-3$.  

To answer the first research question in~\cref{q1} on \MNISTAdd, for \DPL we used: $\nu = 32$ and $\omega = 0.0001$; for \DSL: $\nu = 128$, $\omega = 0.001$, $\varepsilon_{sym} = 0.2807344052335263$, and $\varepsilon_{rul} = 0.107711951632426$; for \CBM: $\nu = 256$ and $\omega = 0.0001$; for \DSLDPL: $\nu = 32$, $\omega = 0.01$, and $\lambda_r = 0.0001$; for \SENN: $\nu = 64$, $\omega = 0.001$, and $w_r = 0.1$; while for \DSLBEARS we set the diversification loss term to $0$ and the entropy term to $0.1$.
On \MNISTSumXor and its biased version, for \DPL we used: $\nu = 512$ and $\omega = 0.0001$; for \DSL: $\nu = 128$, $\omega = 0.0001$, $\varepsilon_{sym} = 0.2807344052335263$, and $\varepsilon_{rul} = 0.107711951632426$; for \CBM: $\nu = 256$ and $\omega = 0.0001$; for \DSLDPL: $\nu = 32$, $\omega = 0.01$, and $\lambda_r = 0.01$; for \SENN: $\nu = 64$, $\omega = 0.001$, and $w_r = 0.1$; while for \DSLBEARS we set the diversification loss term to $0.1$ and the entropy term to $0.2$.
On \CHans, for \DPL, \CBM, and \DSLDPL, we used $\nu = 32$ and $\omega = 0.001$; for \DSLDPL, $\lambda_r = 0.001$.

For the second research question, we performed a grid search for each mitigation strategy individually. Additionally, since our analysis focuses on optimal models, we trained multiple models and retained only those achieving optimal $\FY$ performance on the validation set.  
For \MNISTSumXor, we set $\nu = 128$, $\omega = 0.001$, and $w_{csup} = 1$ across all experiments. For specific mitigation strategies, we used $w_h = 1$ for entropy regularization, $w_r = 0.01$ for reconstruction, $w_c = 0.1$ for contrastive learning, and $w_k = 1$ for knowledge supervision.  
For \CHans, we applied the same settings: $\nu = 128$, $\omega = 0.001$, and $w_{csup} = 1$ for all experiments. The specific mitigation strategy weights were set as follows: $w_h = 1$ for entropy regularization, $w_r = 1$ for reconstruction, $w_c = 0.1$ for contrastive learning, and $w_k = 1$ for knowledge supervision. When concept supervision is applied, we follow a sequential training approach as in~\citep{koh2020concept}. During the initial epochs, the model learns only the concepts, and later, it jointly optimizes both the concepts and the label, as they are challenging to learn simultaneously.


\newpage
\section{Additional results}
\label{sec:additional-results}


\subsection{Full Results for the Case Studies}
\label{sec:additional-results-case-studies}

We report additional results for \textbf{Q1} including additional models and metrics -- namely label accuracy $\YAcc$, concept accuracy $\CAcc$, and inference layer accuracy $\KAcc$, and negative log-likelihood on the test set.
The two additional models are: ``\CBM noent'', like \CBM except it does not include any entropy term; and \CBM$_{20}$ which is a regular \CBM for which $20\%$ of the concept values (\eg two digits in MNIST-based tasks) receive full concept supervision.


\begin{table*}[!h]
    \caption{\textbf{Complete results for \MNISTAdd}.}
    \centering
    \scriptsize
    \input{tables/mnist-add}
\end{table*}


\begin{table*}[!h]
    \caption{\textbf{Complete results for \MNISTSumXor}.}
    \centering
    \scriptsize
    \input{tables/mnist-sumparity}
\end{table*}


\begin{table*}[!h]
    \caption{\textbf{Complete Results for \MNISTSumXor biased}.}
    \centering
    \scriptsize
    \input{tables/mnist-sumparity-rigged}
    \label{tab:mnist-sp-r}
\end{table*}


\begin{figure}[!h]
    \centering
    \includegraphics[width=\linewidth]{figures/cbm_metrics_plot_std}
    \caption{~\cref{fig:curves-sumxor-reduced} with standard deviation over $5$ seeds}
\end{figure}

\begin{figure}[!h]
    \centering
    \includegraphics[width=\linewidth]{figures/clevr_cbm_metrics_plot}
    \caption{Mitigation strategies evaluated for \CBM on \CHans, with standard deviation over $5$ seeds. We evaluated only the contrastive strategy as it performed best. Since supervising all knowledge is infeasible, we observe that for sampled configurations (leading to out-of-distribution settings), the learned knowledge does not generalize. }
\end{figure}

\begin{figure}[!h]
    \centering
    \begin{tabular}{cccc}
        \includegraphics[width=0.245\linewidth]{figures/confusion_matrices/cbm_addition_optimal}
            & \includegraphics[width=0.245\linewidth]{figures/confusion_matrices/knowledge_cbm_addition}
            & \includegraphics[width=0.245\linewidth]{figures/confusion_matrices/cbm_addition_ordered}
            & \includegraphics[width=0.245\linewidth]{figures/confusion_matrices/knowledge_cbm_addition_ordered}
    \end{tabular}
    \caption{Confusion matrices of concepts for \CBM on \MNISTAdd (first row: before alignment, second row: after alignment) along with the corresponding knowledge: number in cells refer to the predicted sum.}
\end{figure}

\begin{figure}[!h]
    \centering
    \begin{tabular}{cc}
        \includegraphics[width=0.3\linewidth]{figures/confusion_matrices/cbm_sumparity}
            & \includegraphics[width=0.3\linewidth]{figures/confusion_matrices/knowledge_cbm_sumparity}
    \end{tabular}
    \caption{Concepts and inference layer learned by \CBM on \MNISTSumXor before (two left) and after (two right) the post-hoc alignment step.  For concepts, we report the confusion matrix.  For the inference layer, we visualize the learned operation as a truth table:  numbers within cells indicate the label predicted for each combination of predicted digits.}
\end{figure}


\begin{figure}[!h]
    \centering
    \begin{tabular}{ccc}
            & \MNISTAdd
            & \MNISTSumXor
        \\
        \rotatebox{90}{\hspace{5em}
        \rotatebox{-90}{\CBM}
        }
            &
            \includegraphics[width=0.24\linewidth]{figures/confusion_matrices/cbm_addition_optimal.pdf}
            & \includegraphics[width=0.24\linewidth]{figures/confusion_matrices/cbm_sumparity_rigged.pdf}
        \\
        \rotatebox{90}{\hspace{5em}
        \rotatebox{-90}{\DPL}
        }
            & \includegraphics[width=0.24\linewidth]{figures/confusion_matrices/dpl_addition.pdf}
            & \includegraphics[width=0.24\linewidth]{figures/confusion_matrices/dpl_sumparity.pdf}
        \\
        \rotatebox{90}{\hspace{5em}
        \rotatebox{-90}{\DSL}
        }
            & \includegraphics[width=0.24\linewidth]{figures/confusion_matrices/dsl_addition.pdf}
            & \includegraphics[width=0.24\linewidth]{figures/confusion_matrices/dsl_sumparity.pdf}
        \\
        \rotatebox{90}{\hspace{5em}
        \rotatebox{-90}{\DSLDPL}
        }
            & \includegraphics[width=0.24\linewidth]{figures/confusion_matrices/dsldpl_addition.pdf}
            & \includegraphics[width=0.24\linewidth]{figures/confusion_matrices/dsldpl_sumparity.pdf}
    \end{tabular}
    \caption{Confusion matrices between ground-truth and predicted concepts.  In \MNISTSumXor models tend to favor $\valpha$'s that collapse concepts together, hinting at simplicity bias.}
\end{figure}


\newpage
\subsection{Number of Reasoning Shortcuts and Joint Reasoning Shortcuts}
\label{sec:additional-results-counts}

Here, we report the approximate number of deterministic reasoning shortcuts and joint reasoning shortcuts for \MNISTAdd and \MNISTSumXor.
We obtained these numbers by extending the {\tt count-rss} software included in {\tt rsbench}~\citep{bortolotti2024benchmark} to the enumeration of JRSs, compatibly with \cref{eq:rss-count} and \cref{eq:jrs-count}.

We considered downsized versions of both \MNISTAdd and \MNISTSumXor, with input digits restricted to the range $\{0, \ldots, N\}$, and resorted to the approximate model counter {\tt approxMC}~\citep{chakraborty2016approxmc} for obtaining approximate counts in the hardest cases.
All counts assume object independence is in place (\ie that the two input digits are processed separately), for simplicity. The situation is similar when this is not the case, except that the all counts grow proportionally.

The number of RSs and JRSs for increasing $N$ are reported in \cref{tab:counts-sumparity}.
Specifically, the \#RSs column indicates the number of regular RSs obtained by fixing the prior knowledge (that is, assuming $\vbeta = \vbeta^*$) and matches the count in \citep{marconato2023not} for the same tasks.
The \#JRSs columns refer to the CBM case, where the inference layer is learned from data (\ie $\vbeta$ might not match $\vbeta^*$).  
We report two distinct counts for JRSs:  one assumes that $\vbeta$'s that match on all concepts actively output by $\valpha$ should be considered identical despite possible differences on ``inactive'' concepts that $\valpha$ predicts as constant (\#JRSs non-redundant) or not (\# JRSs).

As can be seen from the results, JRSs quickly outnumber regular RSs as $N$ grows, as anticipated in \cref{sec:joint-reasoning-shortcuts}.  This suggests that, as expected, learning CBMs with intended semantics by optimizing label likelihood is more challenging when prior knowledge is not given \textit{a priori}.






\begin{table*}[!h]
    \centering
    \caption{Number of RSs vs. JRSs in \MNISTSumXor.}
    \label{tab:counts-sumparity}
    \begin{tabular}{cccc}
    \toprule
    $N$ & \#RSs & \#JRSs & \#JRSs (non-redundant) \\
    \midrule




    3 & $(1.10 \pm 0.00) \times 10^1$        & $(3.84 \pm 0.60) \times 10^2\textcolor{white}{0}$    & $(1.20 \pm 0.00) \times 10^1$ \\
    4 & $(6.30 \pm 0.00) \times 10^1$        & $ (1.11 \pm 0.05) \times 10^5\textcolor{white}{0}$      & $(1.27 \pm 0.06) \times 10^2$\\
    5 & $(3.70 \pm 0.15) \times 10^2$    & $ (1.16 \pm 0.05) \times 10^8\textcolor{white}{0}$      & $(1.40 \pm 0.70) \times 10^3$ \\
    6 & $(2.98 \pm 0.10) \times 10^3 $    & $ (4.45 \pm 0.19) \times 10^{11}$   & $(1.74 \pm 0.13) \times 10^4 $\\
    7 & $(2.56 \pm 0.26) \times 10^4$    & $(8.08 \pm 0.33) \times 10^{15}$    & $(2.61 \pm 0.14) \times 10^5$ \\
    8 & $(2.62 \pm 0.15) \times 10^5$    & $(5.39 \pm 0.25) \times 10^{20}$    & $(4.21 \pm 0.10) \times 10^6$ \\
    \bottomrule
    \end{tabular}
\end{table*}




\newpage
$ $
\newpage

\section{Theoretical Material}
\label{sec:supp-theory}

\input{proofs/theory}


\section{Impact of mitigation strategies on the deterministic JRSs count}
\label{sec:update-counts}

We focus on how the count can be updated for some mitigation strategies that we accounted for in \cref{sec:mitigations}. We consider those strategies that gives a constraint for the \cref{eq:jrs-count}, namely: \textbf{\textit{multi-task learning}}, \textbf{\textit{concept supervision}}, \textbf{\textit{knowledge distillation}}, and \textbf{\textit{reconstruction}}.

\subsection{Multi-task learning}

Suppose that for a subset $\calG^\tau \subseteq \mathrm{supp}(p^*(\vG))$, input examples are complemented with additional labels $\vy^\tau$, obtained by applying an inference layer $\vbeta_{\BK^\tau} (\vg)$. 
Here, $\BK^\tau$ represents the augmented knowledge, giving a set $\calY^\tau$ of additional labels. 
To understand how such a choice modifies the count of jrss, we have to consider those $\vbeta: \calC \to \calY \times \calY^\tau $, 
where a component $\vbeta_\calY$ maps to the original task labels $\vY$ 
and the other component $\vbeta_{\calY^\tau}$ maps to the augmented labels $\vY^\tau$. Let $\calB^* := {\calB} \times {\calB^\tau} $ the space where such functions are defined.
As obtained in \citep{marconato2023not}, the constraint for a pair $(\valpha, \vbeta) \in \Vset{\calA} \times \Vset{\calB^*}$ can be written as:
\[  \textstyle
     \Ind{
         \bigwedge_{\vg \in \calG^\tau}
        (\vbeta_{\calY^\tau} \circ \valpha)(\vg)
            =
            \vbeta_{\BK^\tau} (\vg)
    }       
\]
This term can be steadily combined with the one appearing in \cref{thm:count-jrss} to yield a reduction of possible maps $\valpha \in \Vset{\calA}$ that successfully allow a function $\vbeta \in \calB^*$ to predict both $(\vy, \vy^\tau)$ consistently. Notice that, in the limit where the augmented knowledge comprehends enough additional labels and the support covers the whole , the only admitted solutions consist of:
\begin{align}
    (\vbeta_{\calY^\tau} \circ \id ) (\vg) = ( \underbrace{\vbeta_{\calY^\tau} \circ \vphi^{-1}}_{=: \vbeta_{\calY^\tau}} \circ \underbrace{\vphi}_{=: \valpha} ) (\vg) 
\end{align}
for all $\vg \in \calG$, where $\vphi : \calG \to \calC$ is an invertible function. This shows that $\valpha$ can be only a bijection from $\calG$ to $\calC$. Provided all the concept vectors $\vg \in \calG$, the number of such possible $\valpha$'s amounts to the number of possible permutation of the $|\calG|$ elements, that is $|\calG|!$.
    

\subsection{Concept supervision}

The count reduction was studied in \citep{marconato2023not}, and here we report the same term. Consider the case where 
we supply concept {supervision} for a subset of concepts $\vG_\calI \subseteq \vG$, with $\calI \subseteq [k]$, and then augmenting the log-likelihood objective with a cross-entropy loss over the concepts:
$
    \textstyle
    \sum_{i \in \calI} \log p_\theta(C_i = g_i \mid \vx)
$.
We consider also a subset $\calG^C \subseteq \mathsf{supp}(p^*(\vG))$ where this supervision is provided.  
The constraint on the number of deterministic $\valpha$'s is given by the following
\[
    \label{eq:mc-concept-supervision}
    \textstyle
    \Ind{ \bigwedge_{\vg \in \calG^C} \bigwedge_{i \in \calI}  \alpha_i(\vg) = g_i }
\]

If $\calI = [k]$ and $\calG^C = \calG$, then we get $\valpha \equiv \id$.  Naturally, this requires dense annotations for all possible inputs, which may be prohibitive.


\subsection{Knowledge distillation}

We consider the case where samples $(\vg, \vbeta^*(\vg))$ are used to distill the ground-truth knowledge. Since by \cref{assu:labels} the ground-truth inference layer is deterministic, we consider the following objective for distillation. Given a subset $\calG^K \subseteq \mathrm{supp}(p^*(\vG))$ the original log-likelihood objective is complemented with the following cross-entropy:
\[  \textstyle
    \sum_{\vg \in \calG^{K}}
    \log \vomega( \Ind{\vC = \vg} )_{\vbeta^*(\vg) }
\]
\ie the $\vbeta^*(\vg)$ component of the CBM $\vomega$ is increased for each $\vg \in \calG^K$.
Turned to a constraint on $\vbeta$ this can be written as follows:
\[
    \textstyle
    \Ind{
        \bigwedge_{\vg \in \calG^K}
        \vbeta(\vg) = \vbeta^*(\vg)
    }
\]
When $\calG^K = \calG$ it necessarily holds that $\vbeta \equiv \vbeta^*$. 



\subsection{Reconstruction penalty}
\label{sec:rec-mitigation}
When focusing on reconstructing the input from the bottleneck, the probability distributions $p(\vC \mid \vX)$ may not carry enough information to completely determine the input. In fact, if $\vX$ depends also  on stylistic variables $\vS \in \bbR^q$, it becomes impossible solely from $\vG$ to determine the input. 
This means that training a decoder only passing $\vc \sim p(\vC \mid \vX)$ would not convey enough information and would reduce the benefits of the reconstruction term. 
\citet{marconato2023not} have studied this setting and proposed to also include additional variables  $\vZ \in \bbR^q$ in the bottleneck to overcome this problem. 
The encoder/decoder architecture works as follows: first the input $\vx$ is encoded into $p(\vC, \vZ \mid \vx)$ by the model and after sampling $(\vc, \vz) \sim p(\vC, \vZ \mid \vx)$ the decoder $\vd$ reconstruct the an input image $\hat \vx = \vd(\vc, \vz)$.
Following, under the assumption of \textit{content-style separation}, \ie both the encoder and the decoder process independently the concept and the stylistic variables, it holds that the constraint fr maps $\valpha \in \Vset{\calA}$ given by the reconstruction penalty result in:
\[
    \textstyle
    \Ind{
        \bigwedge_{\vg, \vg' \in \mathsf{supp}(\vG) : \vg \ne \vg'}
            \valpha(\vg) \neq \valpha(\vg')
    }
\]
The proof for this can be found in \citep[Proposition 6]{marconato2023not}. Notice that, with full support over $\calG$, the only such maps $\valpha$ must not confuse one concept vector for another, \ie at most there are at most $|\calG|!$ different valid $\valpha$'s that are essentially a bijection from $\calG$ to $\calC$. 



\section{Models that satisfy Assumption~\ref{assu:monotonic}}
\label{sec:models-satisfying-assumption-3}


\subsection{Theoretical analysis}
\label{sec:analysis-assumption-app}

\textbf{Deep Symbolic Learning} \citep{daniele2023deep}. At inference time, this method predicts a concept vector $\vc \in \calC$ from a conditional probability concept distributions, that is it implements a function $\vf_{DSL}: \bbR^n \to \calC$ where: 
\[
    \vf_{DSL} (\vx) := \argmax_{\vc \in \calC} \tilde p(\vc \mid \vx)
\]
where $p (\vC \mid \vX)$ is a learned conditional distribution on concepts.\footnote{In this case, we have that $ \vf_{DSL} (\vX) \neq \tilde p(\vC \mid \vX)$.} 
From this expression, $\valpha \in \calA$ is defined as:
\[
    \valpha(\vg) := \bbE_{\vx \sim p^*(\vX \mid \vg)} [ \Ind{\vC = \vf_{DSL} (\vx) } ]
\]
Notice that, the learned $\vbeta: \calG \to \Delta_\calY$ consists in a look up table from concepts vectors $\vc \in \calC$, thereby the  giving the conditional distribution:
\[
    p_{DSL}(\vY \mid \vg) := \bbE_{\vx \sim p^*(\vX \mid \vg)} [ \vbeta ( \vf_{DSL}(\vx) )  ]
\]
By considering the measure defined by $p^*(\vx \mid \vg) \de \vx $ and the transformation $\vc = \vf_{DSL} (\vx) $, we can pass to the new measure $p(\vc \mid \vg) = \valpha(\vg)$ obtaining:
\begin{align}
    p_{DSL}(\vY \mid \vg) &= \sum_{\vc \in \calC} \vbeta(\vc) p(\vc \mid \vg) \\
    &= \sum_{\vc \in \calC} \vbeta(\vc) \valpha(\vg)_{\vc} \\
    &= (\vomega_{DSL} \circ \valpha)(\vg)
\end{align}
with $\vomega$ that is defined as:
\[
    \vomega_{DSL}( p(\vC)) := \sum_{\vc \in \calC}  \vbeta(\vc) p(\vC = \vc)
\]
This form is the same of probabilistic logic methods and we discuss the validity of \cref{assu:monotonic} along with them.
    

\textbf{Probabilistic Logic Models}. For models like DPL \citep{manhaeve2018deepproblog}, SL \citep{xu2018semantic}, and SPL \citep{ahmed2022semantic}, 
$\vomega$ is defined as follows:
\[
    \vomega (  p(\vC)    ) := \sum_{\vc \in \calC} \vbeta( \vc ) p(\vC = \vc)
\]
Consider $\vc, \vc' \in \calC$ such that:
\[
    \vbeta (\vc) \neq \vbeta (\vc')
\]
Then, for any choice of $\lambda \in (0,1)$ it holds:
\begin{align}    
    \vomega (  \lambda \Ind{\vC = \vc} + (1- \lambda) \Ind{\vC = \vc'} ) &= \lambda \vbeta (\vc) + (1- \lambda) \vbeta (\vc') \\
    \implies \; \max_{\vy \in \calY} \vomega (  \lambda \Ind{\vC = \vc} + (1- \lambda) \Ind{\vC = \vc'} )_{\vy} &= \max_{\vy \in \calY} \big(\lambda \vbeta (\vc)_{\vy} + (1- \lambda) \vbeta (\vc')_{\vy} \big)
    \label{eq:max-pl}
\end{align}

Notice that for all $\vy \in \calY$, it holds that by the convexity of the $\max$ operator:
\begin{align}
    \max_{\vy \in \calY} \big( \lambda \vbeta (\vc)_{\vy} + (1- \lambda) \vbeta ( \vc')_{\vy} \big) &\leq \lambda \max \big( \max_{\vy \in \calY} \vbeta (\vc)_{\vy}, \max_{\vy \in \calY} \vbeta (\vc')_{\vy} \big)   + (1 - \lambda)  \max \big( \max_{\vy \in \calY} \vbeta (\vc)_{\vy}, \max_{\vy \in \calY} \vbeta (\vc')_{\vy} \big) \\
    &=  \max \big( \max_{\vy \in \calY} (\vbeta (\vc)_{\vy}), \max_{\vy \in \calY} \vbeta (\vc')_{\vy} \big)
\end{align}
where the equality holds if and only if $\argmax_{\vy \in \calY} \vbeta (\vc)_{\vy} = \argmax_{\vy \in \calY} \vbeta (\vc')_{\vy}  $  and  $\max_{\vy \in \calY} \vbeta (\vc)_{\vy} = \max_{\vy \in \calY} \vbeta (\vc')_{\vy} $.
This proves that probabilistic logic methods satisfy \cref{assu:monotonic}.








\textbf{Concept Bottleneck Models}. We consider \CBM{s} implementing a linear layer as $\vomega: \Delta_\calC \to \Delta_\calY$, as customary \citep{koh2020concept}. We consider the general case where the linear layer consists of a matrix $\vW \in \bbR^{a \times b}$ with $a:= |\calG|$ rows and $ b:= |\calY|$ columns. In this setting, a probability distribution $p(\vC) \in \Delta_\calC$ corresponds to the input vector that is passed to the inference layer $\vomega$, implemented by the linear layer $\vW$ and a softmax operator. 
We denote with $\vw^y_c := \vW_{c, y}$, corresponding to the $(c, y)$ component of the matrix. 
The index $c$ (resp. $y$) also correspond to the concept vector $\vc \in \calC$ (resp. $\vy$),  associated to it, \eg $\vc = (0,1)^\top \in \{0,1\}^2$ corresponds to $c=1$. Following, we just use $\vc$ and $\vy$ to denote their corresponding index of the concepts and of the labels.
Provided a concept probability vector $\vp \in \Delta_\calC$, with components $\vp_{\vc} = p(\vC = \vc)$, the conditional distribution over labels is given by the softmax operator:
\begin{align}
    p(\vy \mid \vc) &= \vomega (\vp)_{\vy} \\
    &= \frac{\exp \big( \sum_{\vc \in \calC} \vW_{\vc, \vy} \vp_{\vc} \big) }{ \sum_{\vy' \in \calY}  \exp \big( \sum_{\vc \in \calC} \vW_{\vc, \vy'} \vp_{\vc} \big)} \\
    &= \frac{\exp \big( \sum_{\vc \in \calC} \vw_{\vc}^{\vy} \vp_{\vc} \big) }{ \sum_{\vy' \in \calY}  \exp \big( \sum_{\vc \in \calC} \vw_{\vc}^{\vy'} \vp_{\vc} \big)} 
\end{align}

In general, for any random choice of weights $\vW$, the \CBM may not satisfy \cref{assu:monotonic}. 
Notice also that,
because of the form of $\vomega$, the \CBM cannot express deterministic $\vbeta \in \calB$. 
We consider the particular case associated with optimal models expressing an almost deterministic conditional distribution $\vbeta \in {\calB}$, where a subset of weights $\vw^{\vy}_{\vc}$ is extremely high or low. 
Notice that this is relevant for models in the context of \cref{assu:labels}, where $\vbeta^* \in \Vset{\calB}$. 
We can see that in order for this to happen, we need the function $\vbeta$ to be peaked on concept vectors $\vc \in \calC$. This can happen only when the magnitude of one $\vw_{\vc}^{\vy}$ is much higher than other $\vw_{\vc}^{\vy'}$. Hence, we formulate the following condition for \CBM:

\begin{definition}
\label{def:M-deterministic-cbm}
    Consider a \CBM $(\vf, \vomega) \in \calF \times \Omega$ with weights $\vW = \{ \vw_{\vc}^{\vy} \}$ and $M > |\calY -1|$. 
    We say that the \CBM is $\log(M)$-deterministic at the vertices if %
    for all $\vc \in \calC$, there exists $\vy \in \calY$ such that:
    \begin{align}
        &\forall \vy' \neq \vy, \;
        \vw_{\vc}^{\vy} - \vw_{\vc}^{\vy'} \geq \log M 
        \label{eq:cbm-condition1}
        \\
        & \forall \vy' \neq \vy \neq \vy'', \; |\vw_{\vc}^{\vy'} - \vw_{\vc}^{\vy''}| \leq \log M
        \label{eq:cbm-condition2}
    \end{align}
    
\end{definition}

To see why this is helpful, we show that this constrained version of \CBM can flexibly approximate deterministic $\vbeta$'s:

\begin{proposition}
\label{prop:prob-values-cbm-maximum}
    Consider a $\log (M)$-deterministic \CBM $(\vf, \vomega) \in \calF \times \Omega$. %
    It holds:
    \[
        \forall \vc \in \calC, \;
        \max_{\vy \in \calY} \vomega (\vc)_{\vy}  \geq  \frac{1}{1 + (|\calY|-1)/M}
    \]
    In particular, it holds that:
    \[
        \forall \vc \in \calC, \;
        \lim_{M \to +\infty} \max_{\vy} \vomega (\vc)_{\vy}  = 1
    \]
\end{proposition}

\begin{proof}
    For $\vc \in \calC$, let $\vy = \argmax_{\vy'} \vomega(\Ind{\vC = \vc})_{\vy'}$. We consider the expression:
    \begin{align}
        \vomega(\Ind{\vC = \vc}) &= \frac{\exp \vw_{\vc}^{\vy}  }{ \sum_{\vy' \in \calC} \exp \vw_{\vc}^{\vy'} } \\
        &= \frac{1}{1 + \sum_{\vy' \neq \vy} \exp (- \vw_{\vc}^{\vy} +\vw_{\vc}^{\vy'}) } 
    \end{align}
    We now make use of the fact that the \CBM is $\log M$-deterministic and use \cref{eq:cbm-condition1} to get:
    \begin{align}
        \frac{1}{1 + \sum_{\vy' \neq \vy} \exp (- \vw_{\vc}^{\vy} +\vw_{\vc}^{\vy'}) } &\geq \frac{1}{1 + \sum_{\vy' \neq \vy} \exp - \log M } \\    
        &= \frac{1}{1 + \sum_{\vy' \neq \vy} \frac{1}{M}} \\
        &= \frac{1}{1 + (|\calY -1|) / {M}} 
    \end{align}
    giving overall that:
    \[
        \max_{\vy \in \calY} \vomega (\vc)_{\vy}  \geq  \frac{1}{1 + (|\calY|-1)/M}
    \]

    Now, consider the limit for large $M \in \bbR$: 
    \[
        \lim_{M \to + \infty} \max_{\vy \in \calY} \vomega (\vc)_{\vy}  \geq  \lim_{M \to + \infty}  \frac{1}{1 + (|\calY|-1)/M} = 1
    \]
    This concludes the proof.
\end{proof}

The last point of \cref{prop:prob-values-cbm-maximum} shows a viable way to get peakaed label distributions from a $\log(M)$-deterministic \CBM. This limit guaranteed that such constrained \CBM{s} can get closer to achieve optimal likelihood. 
We are now ready to prove that a $\log(M)$-deterministic \CBM respect \cref{assu:monotonic}.

\begin{proposition}
    A \CBM $(\vf, \vomega) \in \calF \times \Omega$ that is $\log(M)$-deterministic (\cref{def:M-deterministic-cbm}) 
    satisfies \cref{assu:monotonic}, \ie 
    for all $\lambda \in (0,1)$
    and for all $\vc \neq \vc'$ such that
    $\argmax_{\vy \in \calY} \vomega(\Ind{\vC=\vc})_{\vy} 
    \neq 
    \argmax_{\vy \in \calY} \vomega(\Ind{\vC=\vc'})_{\vy} 
    $, it holds
    :
    \[
        \max_{\vy \in \calY} \vomega ( \lambda \Ind{\vC = \vc_1} + (1- \lambda) \Ind{\vC = \vc_2} )_{\vy} < \max \big(
            \max_{\vy \in \calY} \vomega( \Ind{\vC = \vc_1} )_{\vy},
            \max_{\vy \in \calY} \vomega( \Ind{\vC = \vc_2} )_{\vy}
        \big)
    \]
\end{proposition}

\begin{proof}
    Let  $\vy_1 := \argmax_{\vy \in \calY} \vomega(\Ind{\vC = \vc_1})_{\vy} $ and $\vy_2 := \argmax_{\vy \in \calY} \vomega(\Ind{\vC = \vc_2})_{\vy} $ and $\lambda_1 := \lambda$ and $\lambda_2 := 1- \lambda$, with $\lambda \in (0,1)$ and $\vy_1 \neq \vy_2$. 
    The proof is divided in two steps. 
    
    We first check that (1) the values taken by $\vomega ( \lambda \Ind{\vC = \vc_1} + (1- \lambda) \Ind{\vC = \vc_2} )_{\vy_i}$, for $i \in \{1,2\},$ are lower than those taken at the extremes.
    The explicit expression for $\vomega(\cdot)_{\vy_1}$ is given by:
    \[
        \vomega( \lambda_1 \Ind{\vC = \vc_1} +  \lambda_2 \Ind{\vC = \vc_2} )_{\vy_1} = \frac{
            e^{ \lambda_1 \vw_{\vc_1}^{\vy_1} + \lambda_2 \vw_{\vc_2}^{\vy_1}   }
        }{
            \sum_{\vy \in \calY} e^{ \lambda_1 \vw_{\vc_1}^{\vy} + \lambda_2 \vw_{\vc_2}^{\vy}}
        }
    \]
    We take the derivative over $\lambda$, using that $\partial \lambda_1 / \partial \lambda=1$ and $\partial \lambda_2 / \partial \lambda=-1$. Let $Z(\lambda) := \sum_{\vy \in \calY} \exp({ \lambda_1 \vw_{\vc_1}^{\vy} + \lambda_2 \vw_{\vc_2}^{\vy}})$. We first evaluate the derivative of this expression:
    \begin{align}
        \frac{\partial Z(\lambda)}{\partial \lambda} &= \sum_{\vy \in \calY} \frac{\partial}{\partial \lambda} e^{ \lambda_1 \vw_{\vc_1}^{\vy} + \lambda_2 \vw_{\vc_2}^{\vy}} \\
        &= \sum_{\vy \in \calY} e^{ \lambda_1 \vw_{\vc_1}^{\vy} + \lambda_2 \vw_{\vc_2}^{\vy}} (\partial_\lambda \lambda_1 \vw_{\vc_1}^{\vy} + \partial_\lambda \lambda_2 \vw_{\vc_2}^{\vy}) \\
        &= \sum_{\vy \in \calY} e^{ \lambda_1 \vw_{\vc_1}^{\vy} + \lambda_2 \vw_{\vc_2}^{\vy}} (\vw_{\vc_1}^{\vy} - \vw_{\vc_2}^{\vy}) \\
        &= Z(\lambda) \sum_{\vy \in \calY} \frac{e^{ \lambda_1 \vw_{\vc_1}^{\vy} + \lambda_2 \vw_{\vc_2}^{\vy}}}{Z(\lambda)}  (\vw_{\vc_1}^{\vy} - \vw_{\vc_2}^{\vy}) \\
        &= Z(\lambda) \sum_{\vy \in \calY}  (\vw_{\vc_1}^{\vy} - \vw_{\vc_2}^{\vy})  \vomega(\lambda_1 \Ind{\vC = \vc_1} +  \lambda_2 \Ind{\vC = \vc_2} )_{\vy}
    \end{align}
        
    Using this result we get:
    \begin{align}
        \frac{\partial}{\partial \lambda} & \frac{
            e^{ \lambda_1 \vw_{\vc_1}^{\vy_1} + \lambda_2 \vw_{\vc_2}^{\vy_2}   }
        }{
            Z(\lambda)
        } = 
        \frac{
            {\partial_\lambda e^{ \lambda_1 \vw_{\vc_1}^{\vy_1} + \lambda_2 \vw_{\vc_2}^{\vy_1}} } Z(\lambda) - e^{ \lambda_1 \vw_{\vc_1}^{\vy_1} + \lambda_2 \vw_{\vc_2}^{\vy_1}} \partial_\lambda Z(\lambda)
        }{
            Z(\lambda)^2
        } \\
        &= 
        \frac{
            {e^{ \lambda_1 \vw_{\vc_1}^{\vy_1} + \lambda_2 \vw_{\vc_2}^{\vy_1}} (\vw_{\vc_1}^{\vy_1} - \vw_{\vc_2}^{\vy_1}) } Z(\lambda) -  e^{ \lambda_1 \vw_{\vc_1}^{\vy_1} + \lambda_2 \vw_{\vc_2}^{\vy_1}} Z(\lambda) \sum_{\vy \in \calY}  (\vw_{\vc_1}^{\vy} - \vw_{\vc_2}^{\vy})  \vomega(\lambda_1 \Ind{\vC = \vc_1} +  \lambda_2 \Ind{\vC = \vc_2} )_{\vy}
        }{
            Z(\lambda)^2
        } \\
        &=
        \frac{
            {e^{ \lambda_1 \vw_{\vc_1}^{\vy_1} + \lambda_2 \vw_{\vc_2}^{\vy_1}} (\vw_{\vc_1}^{\vy_1} - \vw_{\vc_2}^{\vy_1}) } -  e^{ \lambda_1 \vw_{\vc_1}^{\vy_1} + \lambda_2 \vw_{\vc_2}^{\vy_1}} \sum_{\vy \in \calY}  (\vw_{\vc_1}^{\vy} - \vw_{\vc_2}^{\vy})  \vomega(\lambda_1 \Ind{\vC = \vc_1} +  \lambda_2 \Ind{\vC = \vc_2} )_{\vy}
        }{
            Z(\lambda)
        } \\
        &=
        \frac{
            {e^{ \lambda_1 \vw_{\vc_1}^{\vy_1} + \lambda_2 \vw_{\vc_2}^{\vy_1}} \big(  (\vw_{\vc_1}^{\vy_1} - \vw_{\vc_2}^{\vy_1}) }  - \sum_{\vy \in \calY}  (\vw_{\vc_1}^{\vy} - \vw_{\vc_2}^{\vy})  \vomega(\lambda_1 \Ind{\vC = \vc_1} +  \lambda_2 \Ind{\vC = \vc_2} )_{\vy} \big)
        }{
            Z(\lambda)
        }
    \end{align}
    For this expression we study the sign of the derivative. Notice that we can focus only on the following term since the others are always positive
    \[
      (\vw_{\vc_1}^{\vy_1} - \vw_{\vc_2}^{\vy_1}) -  \sum_{\vy \in \calY}  (\vw_{\vc_1}^{\vy} - \vw_{\vc_2}^{\vy})  \vomega(\lambda_1 \Ind{\vC = \vc_1} +  \lambda_2 \Ind{\vC = \vc_2} )_{\vy}   
    \]
    From this expression, we can make use of the fact that in general, for a scalar function $f(\vx)$, we have $\bbE_{\vX}[f( \vX)] \leq \max_{\vx} f(\vx)$ and consider the following:
    \begin{align}
        (\vw_{\vc_1}^{\vy_1} - \vw_{\vc_2}^{\vy_1}) -  \sum_{\vy \in \calY}  (\vw_{\vc_1}^{\vy} - \vw_{\vc_2}^{\vy})  \vomega(\lambda_1 \Ind{\vC = \vc_1} +  \lambda_2 \Ind{\vC = \vc_2} )_{\vy}  &\geq
        (\vw_{\vc_1}^{\vy_1} - \vw_{\vc_2}^{\vy_1}) -  \max_{\vy \in \calY} (\vw_{\vc_1}^{\vy} - \vw_{\vc_2}^{\vy})  \\
        &= (\vw_{\vc_1}^{\vy_1} - \vw_{\vc_2}^{\vy_1}) -  (\vw_{\vc_1}^{\vy_1} - \vw_{\vc_2}^{\vy_1}) \\
        &= 0
    \end{align}
    where in the second line we used that the maximum is given by $\vy_1$. Therefore, the derivative is always increasing in the $[0,1]$ interval, meaning that:
    \[
        \forall \lambda \in [0,1], \; \vomega( \lambda_1 \Ind{\vC = \vc_1} +  \lambda_2 \Ind{\vC = \vc_2} )_{\vy_1} \leq \vomega (\Ind{\vC = \vc_1})_{\vy_1}
    \]
    where the equality holds if an only if $\lambda=1$. Similarly, the derivative for $\vomega(\cdot)_{\vy_2}$ gives:
    \begin{align}    
        &\frac{\partial  \vomega(\lambda_1 \Ind{\vC = \vc_1} +  \lambda_2 \Ind{\vC = \vc_2} )_{\vy_2} }{\partial \lambda} \notag \\
        &\quad = 
        \frac{
            {e^{ \lambda_1 \vw_{\vc_1}^{\vy_2} + \lambda_2 \vw_{\vc_2}^{\vy_2}} \big(  (\vw_{\vc_1}^{\vy_2} - \vw_{\vc_2}^{\vy_2}) }  - \sum_{\vy \in \calY}  (\vw_{\vc_1}^{\vy} - \vw_{\vc_2}^{\vy})  \vomega(\lambda_1 \Ind{\vC = \vc_1} +  \lambda_2 \Ind{\vC = \vc_2} )_{\vy} \big)
        }{
            Z(\lambda)
        }
    \end{align}
    
    By using the fact that $\bbE_{\vX} [f(\vX)] \geq \min_{\vx} f(\vx)$ for a scalar function $f(\vx)$, we obtain:
    \begin{align}
        (\vw_{\vc_1}^{\vy_2} - \vw_{\vc_2}^{\vy_2}) -  \sum_{\vy \in \calY}  (\vw_{\vc_1}^{\vy} - \vw_{\vc_2}^{\vy})  \vomega(\lambda_1 \Ind{\vC = \vc_1} +  \lambda_2 \Ind{\vC = \vc_2} )_{\vy}  &\leq 
        (\vw_{\vc_1}^{\vy_2} - \vw_{\vc_2}^{\vy_2}) -  \min_{\vy \in \calY} (\vw_{\vc_1}^{\vy} - \vw_{\vc_2}^{\vy})  \\
        &= (\vw_{\vc_1}^{\vy_2} - \vw_{\vc_2}^{\vy_2}) -  (\vw_{\vc_1}^{\vy_2} - \vw_{\vc_2}^{\vy_2}) \\
        &= 0
    \end{align}
    where in the second line we used that the minimum is given by $\vy_2$. Hence, the derivative is always decreasing for $\lambda \in [0,1]$ and it holds:
    \[
        \forall \lambda \in [0,1], \; \vomega( \lambda_1 \Ind{\vC = \vc_1} +  \lambda_2 \Ind{\vC = \vc_2} )_{\vy_2} \leq \vomega (\Ind{\vC = \vc_2})_{\vy_2}
    \]
    where the equality holds if and only if $\lambda = 0$.   

    
    (2) Now, we check the same holds when choosing another element $\vy' \neq \vy_1, \vy_2$. To this end, we consider the following expressions:
    \[
        \frac{ p(\vy \mid  \lambda_1 \Ind{\vC= \vc_1} + \lambda_2 \Ind{\vC= \vc_2} )}{p(\vy_1 \mid \vc_1)}, \quad 
        \frac{ p(\vy \mid  \lambda_1 \Ind{\vC= \vc_1} + \lambda_2 \Ind{\vC= \vc_2} )}{p(\vy_2 \mid \vc_2)}
    \]
    where $p(\vy \mid \vp) := \vomega (\vp)_{\vy}$. 
    Consider the first expression. We can rewrite it as follows:
    \begin{align}
        \frac{ p(\vy \mid  \lambda_1 \Ind{\vC= \vc_1} + \lambda_2 \Ind{\vC= \vc_2} )}{p(\vy_1 \mid \vc_1)} &= \frac{ p(\vy \mid  \lambda_1 \Ind{\vC= \vc_1} + \lambda_2 \Ind{\vC= \vc_2} )}{p(\vy_1 \mid  \lambda_1 \Ind{\vC= \vc_1} + \lambda_2 \Ind{\vC= \vc_2} )} \frac{p(\vy_1 \mid  \lambda_1 \Ind{\vC= \vc_1} + \lambda_2 \Ind{\vC= \vc_2} )}{p(\vy_1 \mid \vc_1)} \\
        &\leq \frac{ p(\vy \mid  \lambda_1 \Ind{\vC= \vc_1} + \lambda_2 \Ind{\vC= \vc_2} )}{p(\vy_1 \mid  \lambda_1 \Ind{\vC= \vc_1} + \lambda_2 \Ind{\vC= \vc_2} )} 
        \label{eq:cbm-first-expression-last}
    \end{align}
    where in the last line we made use of the fact that the second fraction in the right-hand side of the first line is always $\leq 1$. Similarly we have that:
    \begin{align}
        \frac{ p(\vy \mid  \lambda_1 \Ind{\vC= \vc_1} + \lambda_2 \Ind{\vC= \vc_2} )}{p(\vy_2 \mid \vc_2)} &\leq \frac{ p(\vy \mid  \lambda_1 \Ind{\vC= \vc_1} + \lambda_2 \Ind{\vC= \vc_2} )}{p(\vy_2 \mid  \lambda_1 \Ind{\vC= \vc_1} + \lambda_2 \Ind{\vC= \vc_2} )} 
    \end{align}
    We proceed to substituting explicitly the expression for $p(\vy \mid \lambda_1 \Ind{\vC = \vc_1} + \lambda_2 \Ind{\vC=\vc_1}$ into the upper bound of \cref{eq:cbm-first-expression-last}:
    \begin{align}
        \frac{ p(\vy \mid  \lambda_1 \Ind{\vC= \vc_1} + \lambda_2 \Ind{\vC= \vc_2} )}{p(\vy_1 \mid  \lambda_1 \Ind{\vC= \vc_1} + \lambda_2 \Ind{\vC= \vc_2} )} &= \frac{e^{ \lambda_1 \vw_{\vc_1}^{\vy} + \lambda_2 \vw_{\vc_2}^{\vy}}  }{e^{ \lambda_1 \vw_{\vc_1}^{\vy_1} + \lambda_2 \vw_{\vc_2}^{\vy_1}}} 
        \frac{Z(\lambda)}{Z(\lambda)} \\
        &= \exp \big( 
        \lambda_1 ( \vw_{\vc_1}^{\vy} - \vw_{\vc_1}^{\vy_1}) + \lambda_2 ( \vw_{\vc_2}^{\vy} - \vw_{\vc_2}^{\vy_1})
        \big) \\
        &\leq \exp \big( 
        - \lambda_1 \log(M) + \lambda_2 ( \vw_{\vc_2}^{\vy} - \vw_{\vc_2}^{\vy_1})
        \big)
        \tag{Substituting the bound from \cref{eq:cbm-condition1}} \\
        &\leq \exp \big( 
        - \lambda_1 \log(M) + \lambda_2 \log(M)
        \big)
        \tag{Substituting the bound from \cref{eq:cbm-condition2}} \\
        &= M^{\lambda_2 - \lambda_1}
        \label{eq:cbm-beauty-1}
    \end{align}
    With similar steps and substitutions we get that:
    \[
        \frac{ p(\vy \mid  \lambda_1 \Ind{\vC= \vc_1} + \lambda_2 \Ind{\vC= \vc_2} )}{p(\vy_2 \mid  \lambda_1 \Ind{\vC= \vc_1} + \lambda_2 \Ind{\vC= \vc_2} )} \leq M^{\lambda_1 - \lambda_2}
        \label{eq:cbm-beauty-2}
    \]
    Taking the product of \cref{eq:cbm-beauty-1} and \cref{eq:cbm-beauty-2} we obtain:
    \begin{align}
        \frac{ p(\vy \mid  \lambda_1 \Ind{\vC= \vc_1} + \lambda_2 \Ind{\vC= \vc_2} )}{p(\vy_1 \mid  \lambda_1 \Ind{\vC= \vc_1} + \lambda_2 \Ind{\vC= \vc_2} )}
        &\cdot
        \frac{ p(\vy \mid  \lambda_1 \Ind{\vC= \vc_1} + \lambda_2 \Ind{\vC= \vc_2} )}{p(\vy_2 \mid  \lambda_1 \Ind{\vC= \vc_1} + \lambda_2 \Ind{\vC= \vc_2} )} 
        \notag \\
        & \leq  M^{\lambda_2 - \lambda_1}M^{
        \lambda_1  - \lambda_2 } \\
        &= 1
    \end{align}  
    Notice also that:
    \begin{align}
        \frac{ p(\vy \mid  \lambda_1 \Ind{\vC= \vc_1} + \lambda_2 \Ind{\vC= \vc_2} )}{p(\vy_1 \mid  \lambda_1 \Ind{\vC= \vc_1} + \lambda_2 \Ind{\vC= \vc_2} )}
        &\cdot
        \frac{ p(\vy \mid  \lambda_1 \Ind{\vC= \vc_1} + \lambda_2 \Ind{\vC= \vc_2} )}{p(\vy_2 \mid  \lambda_1 \Ind{\vC= \vc_1} + \lambda_2 \Ind{\vC= \vc_2} )} \notag \\
        & \hspace{-7.5em} \geq \min \left( \frac{ p(\vy \mid  \lambda_1 \Ind{\vC= \vc_1} + \lambda_2 \Ind{\vC= \vc_2} )^2}{p(\vy_1 \mid  \lambda_1 \Ind{\vC= \vc_1} + \lambda_2 \Ind{\vC= \vc_2} )^2},
        \frac{ p(\vy \mid  \lambda_1 \Ind{\vC= \vc_1} + \lambda_2 \Ind{\vC= \vc_2} )^2}{p(\vy_2 \mid  \lambda_1 \Ind{\vC= \vc_1} + \lambda_2 \Ind{\vC= \vc_2} )^2}   \right)
    \end{align}
    which in turn means that:
    \begin{align}
        & \min \left( \frac{ p(\vy \mid  \lambda_1 \Ind{\vC= \vc_1} + \lambda_2 \Ind{\vC= \vc_2} )^2}{p(\vy_1 \mid  \lambda_1 \Ind{\vC= \vc_1} + \lambda_2 \Ind{\vC= \vc_2} )^2},
        \frac{ p(\vy \mid  \lambda_1 \Ind{\vC= \vc_1} + \lambda_2 \Ind{\vC= \vc_2} )^2}{p(\vy_2 \mid  \lambda_1 \Ind{\vC= \vc_1} + \lambda_2 \Ind{\vC= \vc_2} )^2}   \right) \leq 1 \\
        \implies
        & \min \left( \frac{ p(\vy \mid  \lambda_1 \Ind{\vC= \vc_1} + \lambda_2 \Ind{\vC= \vc_2} )}{p(\vy_1 \mid  \lambda_1 \Ind{\vC= \vc_1} + \lambda_2 \Ind{\vC= \vc_2} )},
        \frac{ p(\vy \mid  \lambda_1 \Ind{\vC= \vc_1} + \lambda_2 \Ind{\vC= \vc_2} )}{p(\vy_2 \mid  \lambda_1 \Ind{\vC= \vc_1} + \lambda_2 \Ind{\vC= \vc_2} )}   \right) \leq 1 
    \end{align}
    

    This last expression is in line with the condition of \cref{assu:monotonic}, showing that, for all $\vy \in \calY$, either $\vomega(\Ind{\vC= \vc_1})_{\vy_1}$ or $\vomega(\Ind{\vC= \vc_2})_{\vy_2}$ are greater or equal to 
    $\vomega(\lambda \Ind{\vC= \vc_1} + (1- \lambda)\Ind{\vC= \vc_2} )_{\vy}$. 
    Combining step (1) and (2), we obtain that \cref{assu:monotonic} holds and that:
    \[
        \max_{\vy \in \calY} \vomega ( \lambda \Ind{\vC = \vc_1} + (1- \lambda) \Ind{\vC = \vc_2} )_{\vy} \leq \max \big(
            \max_{\vy \in \calY} \vomega( \Ind{\vC = \vc_1} )_{\vy},
            \max_{\vy \in \calY} \vomega( \Ind{\vC = \vc_2} )_{\vy}
        \big)
    \]
    where equality holds if and only if $\lambda \in \{0,1\}$.
\end{proof}






\subsection{Numerical evaluation of Assumption~\ref{assu:monotonic}}
\label{sec:numerical-evaluation-assumptio}

We investigate experimentally whether the models used in our evaluation satisfy \cref{assu:monotonic}. We proved in \cref{sec:analysis-assumption-app} that the \DSL inference layer reduces to Probabilistic Logic methods and only consider \DPL and \DSLDPL as representatives. For \CBM we conduct a separate investigation.

We first consider the case where the prior knowledge $\BK$ is used to define the inference layer as customary in \DPL. Accordingly, we found that \cref{assu:monotonic} is satisfied for all possible couples $(\vc_1, \vc_2) \in [100]^2$ that predicts distinct labels, see \cref{fig:DPL-behavior-assumption}. 

We now turn to \DSLDPL. 
Due to the linearity of the inference layer of \DSLDPL it suffices to consider random weights as a representative for a candidate learned inference layer. Consistently, we found that for all possible couples $(\vc_1, \vc_2) \in [100]^2$ that predicts distinct labels, \cref{assu:monotonic} is satisfied. See \cref{fig:DPL-random-behavior-assumption}.

Finally,
we evaluate explicitly whether a trained \CBM on \MNISTAdd close to optimal likelihood respects, or at least is close to satisfy, \cref{assu:monotonic}. We train a \CBM with $150$ epochs of training, reaching a mean negative log-likelihood of $0.0884$ on the test set. Notice that, due to the softmax operator, there are no choices giving zero negative log-likelihood. 
We found that 
\cref{assu:monotonic} is satisfied for the $95\%$ of possible couples 
$(\vc_1, \vc_2) \in [100]^2$ giving different labels. For the remaining $5\%$ the assumption is marginally violated with a maximum increase of:
\[
    \max_{\vc_1, \vc_2, \lambda} \left( \max_{\vy} \vomega(\lambda \Ind{\vC = \vc_i} + (1-\lambda) \Ind{\vC = \vc_j})_{\vy} 
    - 
    \max \big( 
        \max_{\vy} \vomega(\Ind{\vc_1})_{\vy}, \max_{\vy} \vomega(\Ind{\vc_2})_{\vy} 
    \big) \right) \leq 1.38 \cdot 10^{-3}
\]
Our results on this investigation are displayed in \cref{fig:cbm-behavior-assumption}.


\newpage



\begin{figure}[!t]
    \centering

    \begin{tabular}{cccc}
        \includegraphics[height=8em]{figures/behavior-f-lambda-1-DPL}
        &
        \includegraphics[height=8em]{figures/behavior-f-lambda-2-DPL} 
        &
        \includegraphics[height=8em]{figures/behavior-f-lambda-3-DPL}
        &
        \includegraphics[height=8em]{figures/behavior-f-lambda-4-DPL}
    \end{tabular}

    \includegraphics[width=0.75\textwidth]{figures/cm-dpl}
    
    
    \caption{\textbf{The logic inference layer of DPL in \MNISTAdd}. %
    ({Top}) The behavior of $f(\lambda) := \max_{\vy} \vomega(\lambda \Ind{\vC = \vc_i} + (1-\lambda) \Ind{\vC = \vc_j})$, for four pairs $i,j$ sampled randomly from the $100$ possible worlds. 
    (Bottom) The linear layer weights for DPL. %
    }
    \label{fig:DPL-behavior-assumption}
\end{figure}


\begin{figure}[!t]
    \centering

    \begin{tabular}{cccc}
        \includegraphics[height=8em]{figures/behavior-f-lambda-1-DPL-random}
        &
        \includegraphics[height=8em]{figures/behavior-f-lambda-2-DPL-random} 
        &
        \includegraphics[height=8em]{figures/behavior-f-lambda-3-DPL-random}
        &
        \includegraphics[height=8em]{figures/behavior-f-lambda-4-DPL-random}
    \end{tabular}

    \includegraphics[width=0.75\textwidth]{figures/cm-dpl-random}
    
    
    \caption{\textbf{A random inference layer for \DSLDPL in \MNISTAdd}. %
    ({Top}) The behavior of $f(\lambda) := \max_{\vy} \vomega(\lambda \Ind{\vC = \vc_i} + (1-\lambda) \Ind{\vC = \vc_j})$, for four pairs $i,j$ sampled randomly from the $100$ possible worlds. 
    (Bottom) The linear layer weights for \DSLDPL. %
    }
    \label{fig:DPL-random-behavior-assumption}
\end{figure}

\begin{figure}[!t]
    \centering

    \begin{tabular}{cccc}
        \includegraphics[height=8em]{figures/behavior-f-lambda-1}
        &
        \includegraphics[height=8em]{figures/behavior-f-lambda-2} 
        &
        \includegraphics[height=8em]{figures/behavior-f-lambda-3}
        &
        \includegraphics[height=8em]{figures/behavior-f-lambda-4}
    \end{tabular}

    \includegraphics[width=0.75\textwidth]{figures/cm-cbm-100}
    
    
    \caption{\textbf{Optimal \CBM in \MNISTAdd}. We visualize the learned weights by a \CBM achieving maximum log-likelihood in \MNISTAdd.
    ({Top}) The behavior of $f(\lambda) := \max_{\vy} \vomega(\lambda \Ind{\vC = \vc_i} + (1-\lambda) \Ind{\vC = \vc_j})$, for four pairs $i,j$ sampled randomly the $100$ possible worlds. 
    The sampled pairs align with \cref{assu:monotonic}.
    (Bottom) The linear layer weights for the trained \CBM. %
    }
    \label{fig:cbm-behavior-assumption}
\end{figure}


\end{document}
