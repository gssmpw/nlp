\section{Related Work}
\label{sec:related-work}


\textbf{Shortcuts in machine learning}.  Shortcuts occur when machine learning models solve a prediction task by using features that correlate with but do not cause the desired output, leading to poor OOD behavior \citep{geirhos2020shortcut, teso2023leveraging, ye2024spurious, steinmann2024navigating}.  The shortcuts literature however focuses on black-box models rather than CBMs.
Existing studies on the semantics of CBM concepts \citep{margeloiu2021concept, mahinpei2021promises, havasi2022addressing, raman2023concept} focus on individual failures but propose no well-defined, formal notion of intended semantics.  One exception is \citep{marconato2023interpretability} which however, like other works, ignores the role of the inference layer.
We build on known results on reasoning shortcuts \citep{li2023learning, marconato2023not, wang2023learning, umili2024neural} which are restricted to NeSy-CBMs in which the prior knowledge is given and fixed.  Our analysis upgrades these results to general CBMs.  Furthermore, our simulations indicate that strategies that are effective for CBMs and RSs no longer work for JRSs.
At the same time, while NeSy approaches that learn the prior knowledge and concepts jointly are increasingly popular \citep{wang2019satnet, yang2020neurasp, liu2023out, daniele2023deep, tang2023perception, wust2024pix2code}, existing work neglects the issue of shortcuts in this setting.  Our work partially fills this gap.


\textbf{Relation to identifiability}.  Works in Independent Component Analysis and Causal Representation Learning focus on the identifiability of representations up to an equivalence relation, typically up to permutations and rescaling \citep{khemakhem2020variational, gresele2020incomplete, von2024nonparametric} or more general transformations \citep{roeder2021linear, buchholz2022function}.
The equivalence relation introduced along with intended semantics (\cref{def:intended-semantics}) establishes a specific connection between the maps $\valpha$ and $\vbeta$. This aligns with existing works that aim to identify representations and linear inference layers using supervised labels \citep{lachapelle2023synergies, fumero2023leveraging, bing2023invariance}.  
\citet{taeb2022provable} studied the identifiability of a VAE-based CBM with continuous-valued concepts.  In contrast, we provide an analysis of a broader class of (NeSy) CBMs commonly used in practice.
Moreover, while prior works primarily focus on identifying continuous-valued representations, we examine the identifiability of \textit{discrete} concepts \textit{and} of the inference layer.
\citep{rajendran2024causal} explores the identifiability of (potentially discrete) concepts linearly encoded in model representations but is limited to a multi-environment setting, not including supervised labels.  Our results differ in that we show how concepts can be identified in CBMs by circumventing joint reasoning shortcuts.