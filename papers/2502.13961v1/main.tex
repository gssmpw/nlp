\documentclass{article}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsmath, amssymb, amsthm, amsfonts}       % blackboard math symbols
\usepackage{nicefrac, xfrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{bm}
\usepackage{enumitem}
\usepackage{graphicx}
\usepackage{natbib}
\usepackage{geometry}
\usepackage{authblk,textcomp}
\usepackage{wrapfig}
\usepackage{caption}
\captionsetup[figure]{font=small}
\usepackage{subfigure}  % For using \subfigure (deprecated, consider subcaption instead)
\usepackage[cal=euler]{mathalfa}
\usepackage{libertine}
\usepackage{mathtools}
\usepackage{parskip}
\usepackage{subcaption}
\DeclarePairedDelimiter\ceil{\lceil}{\rceil}
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}
\makeatletter
\newcommand{\otherlabel}[2]{\protected@edef\@currentlabel{#2}\label{#1}}
\makeatother

\usepackage[export]{adjustbox}
% Page Layout
\geometry{
 a4paper,
 left=20mm,
 right=20mm,
 top=20mm,
}

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{proposition}{Proposition}
\newtheorem{definition}{Definition}
\newtheorem{corollary}{Corollary}
\newtheorem{conjecture}{Conjecture}
\newtheorem{assumption}{Assumption}
\newtheorem{remark}{Remark}
\newtheorem{example}{Example}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{bbm}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}


%%% CUSTOM [TO-CHECK-COMPATIBILITY]
\usepackage{commath}
% \usepackage{xfrac}
% \usepackage{enumerate}
\newcommand{\vDelta}{\mathbf{\Delta}}
\def\hR{\widehat{\mathcal{R}}}

\renewcommand{\vec}[1]{\bm{#1}}
\newcommand\Et[1]{\mathbb{E}_t\left[#1\right]}

% \newcommand{\change}[1]{\textcolor{black}{ #1}} # Uncomment once changes are over
\newcommand{\change}[1]{\textcolor{purple}{ #1}}

\hypersetup{pdfauthor={IdePHICS},pdftitle={HiearchyLearning},%
            colorlinks, linktocpage=true, pdfstartpage=1, pdfstartview=FitV,%
    breaklinks=true, pdfpagemode=UseNone, pageanchor=true, pdfpagemode=UseOutlines,%
    plainpages=false, bookmarksnumbered, bookmarksopen=true, bookmarksopenlevel=1,%
    hypertexnames=true, pdfhighlight=/O,%
    urlcolor=orange, linkcolor=blue, citecolor=blue
        }

% Alternative Assumption
\newtheorem{assumptionalt}{Assumption}[assumption]

%% NEEDED %%
\DeclareMathOperator{\eff}{eff}
% \DeclarePairedDelimiterX{\abs}[1]\lvert\rvert{\ifblank{#1}{\: \cdot \:}{#1}}
\newcommand{\polylog}[1]{\mathrm{polylog}(#1)}
\usepackage{algorithm}
\usepackage{algorithmic}
\newcommand{\lz}[1]{{\sffamily\normalsize\bfseries {\underline{\color{orange}LZ:}} } {\color{orange} #1}}
\usepackage{tikz}
% \DeclarePairedDelimiter{\norm}{\lVert}{\rVert}
\usetikzlibrary{trees, positioning, calc}
\usetikzlibrary{trees, positioning}
\input{macros}
% \newtheorem{assumption}[theorem]{Assumption}
\usepackage[font=small,labelfont=bf]{caption}



\title{The Computational Advantage of Depth: Learning High-Dimensional Hierarchical Functions with Gradient Descent}

\author[1,2]{Yatin Dandi}
\author[1]{Luca Pesce}
\author[2]{Lenka Zdeborov\'a}
\author[1]{Florent Krzakala}

% new official EPFL format
\affil[1]{\small Ecole Polytechnique F\'{e}d\'{e}rale de Lausanne, 
Information, Learning and Physics Laboratory. CH-1015 Lausanne, Switzerland.}
\affil[2]{\small 
Ecole Polytechnique F\'{e}d\'{e}rale de Lausanne,
Statistical Physics of Computation Laboratory. CH-1015 Lausanne, Switzerland.}
\date{}
\begin{document}


\maketitle

\begin{abstract}%
 Understanding the advantages of deep neural networks trained by gradient descent (GD) compared to shallow models remains an open theoretical challenge.  
 While the study of multi-index models with Gaussian data in high dimensions has provided analytical insights into the benefits of GD-trained neural networks over kernels, the role of depth in improving sample complexity and generalization in GD-trained networks remains poorly understood. 
In this paper, we introduce a class of target functions (single and multi-index Gaussian hierarchical targets) that incorporate a hierarchy of latent subspace dimensionalities. This framework enables us to analytically study the learning dynamics and generalization performance of deep networks compared to shallow ones in the high-dimensional limit. Specifically, our main theorem shows that feature learning with GD reduces the effective dimensionality, transforming a high-dimensional problem into a sequence of lower-dimensional ones. This enables learning the target function with drastically less samples than with shallow networks. While the results are proven in a controlled training setting, we also discuss more common training procedures and argue that they learn through the same mechanisms.  These findings open the way to further quantitative studies of the crucial role of depth in learning hierarchical structures with deep networks.
\end{abstract}


\vspace{-0.3cm}
\section{Introduction}
\vspace{-0.1cm}
Understanding the computational benefits of deep neural networks over their shallow counterparts is a central question in modern machine learning theory \citep{sejnowski2020unreasonable,zhang2021understanding}. While shallow models can approximate any complex functions \cite{cybenko1989approximation}, deep networks almost universally exhibit remarkable advantages in practice \citep{lecun2015deep,adlam2023kernel}. While there has been much progress in approximation theory on the advantage of depth (see e.g. \cite{mhaskar2016deep,pmlr-v49-telgarsky16,mhaskar2017and,poggio2017and} and reference therein), the dynamics of learning with gradient descent is a more complex question. A fundamental open problem  is thus:
\begin{center}
\textit{
Can one quantify the computational advantage of deep models trained with gradient-based methods with respect to shallow models in some analyzable setting?
} 
\end{center} 
One line of work on GD-based methods in deep networks leading to interesting results is in the setting of deep \textit{linear} network ---see e.g. \cite{saxe2013exact,ji2018gradient,arora2018convergence,lee2019wide,ghorbani2021linearized}.
While deep linear networks offer valuable insights into nonlinear learning dynamics, their simplicity renders them insufficient to capture the complexity of hierarchical feature learning.

Another popular line of research is to study the dynamics of gradient-based methods learning multi-index functions with shallow models \citep{BenArous2021,ba2020generalization,ghorbani2020neural,bietti2022learning,abbe2023sgd,troiani2024fundamental}. 
% Such existing solvable models predominantly fall into two categories: kernel and random feature models \citep{loureiro2021learning,mei_generalization_2022,mei2022generalization}, and multi-index functions learned with shallow models \citep{ba2020generalization,ghorbani2020neural,bietti2022learning,abbe2023sgd,troiani2024fundamental}. 
Multi-index functions provide a rich class of targets, but their efficient learnability by shallow two-layer networks \citep{arnaboldi2024repetita,lee2024neural} undermines their utility as benchmarks for understanding the computational advantages of depth. This motivates the following consideration: 
\begin{center}
\textit{
What is the natural model of targets to consider for understanding the emergent computational advantage of depth when training with gradient-based methods?
} 
\end{center} 
The present paper addresses both these questions. To answer the latter,  we introduce a class of target functions designed to probe the hierarchical structure and computational potential of deep networks. 
%These functions, which we shall refer to as  
These {\it Multi-Index Gaussian-Hierachical Target} (MIGHT) functions encapsulate a hierarchy of latent subspaces with varying dimensionalities. We then proceed to answer the former 
interrogative by analyzing the learning dynamics of multi-layer neural networks on such targets, providing a characterization of the computational advantages afforded by depth. We show how depth enables a hierarchical decomposition of tasks, reducing the effective dimensionality at each layer, and leading to a quantifiable improvement in sample complexity over shallow models.

\begin{figure*}[t]
\centering
\includegraphics[width=0.7\linewidth]{figs/SightMight.pdf}
\vspace{-2cm}
\caption{\textbf{SIGHT and MIGHT targets:} Illustration of Single and Multi Index Gaussian Hierarchical Targets, i.e., SIGHT in eq.~\eqref{eq:3layer_target-reduced} and MIGHT in eq.~\eqref{eq:3layer_target_might}. {\bf Left: A SIGHT function.} Here we first go from ${\bf x} \in {\mathbb R}^d$ to ${\bf z} \in {\mathbb R}^{d^{\varepsilon}}$. After applying the polynomial transformation pointwise (not shown), this is projected to create a scalar $h^\star \in {\mathbb R}$. One can then output the label $y=g^\star(h^\star)$. {\bf Right: A MIGHT function.} Again, we go from ${\bf x} \in {\mathbb R}^d$ to ${\bf z} \in {\mathbb R}^{d^{\varepsilon}}$. After applying the polynomial transformation pointwise, we finally projecte on two values $h_{4,1}^\star$ and $h_{4,2}^\star$, from which we create $y$ as a two-index function $y=g^\star(h_{4,1}^\star,h_{4,2}^\star)$.
     \label{fig:app:sandm_targets}}
\end{figure*}

\vspace{-0.3cm}
\section{Hierarchical Targets and Main Results}
\label{main:def:targer}

\vspace{-0.1cm}
\subsection{Single-Index Gaussian Hierarchical Targets}
\vspace{-0.1cm}
Our simpler setting, where the task ---using Gaussian i.i.d. data $\{\vec x_\mu\}_{\mu =1}^n \in \mathbb R^{n \times d}$--- to learn the following Single-Index Gaussian Hierarchical Target (SIGHT) function class that we write in three equivalent forms as:
\begin{align}
\label{eq:3layer_target}
f^\star(\vec{x}) &=  g^\star\left(
\frac{\vec{a}^{\star^\top} \, 
P_k\left(W^\star \vec{x}\right) }{\sqrt{d^{\varepsilon_1}}} \right),\,\, {\vec x \in \mathbb{R}^d},\,\\
&= g^\star\left(
\frac{\vec{a}^{\star^\top}  
P_k\left(
\vec z^\star
\right)} {\sqrt{d^{\varepsilon_1}}} \right),\,\,
{\vec z^\star = W^\star \vec x \in  \mathbb R^{d^{\eps_1}}},
\label{eq:3layer_target-reduced}
\\
&= g^\star\left(h^\star \right),\,\, %h^\star \in \mathbb{R}
 h^\star =  \vec{a}^\star \cdot 
{P_k\left( {\bf z}^\star \right) }/{\sqrt{d^{\varepsilon_1}}} 
\,  \in {\mathbb R}\,.
\end{align}
Here $P_k$ is a fixed polynomial applied component-wise, and $d^{\epsilon_1}$ denotes the dimensionality of the {\it second-layer features} (non-linear features) in the intermediate layer, which we choose to be  $1 > \epsilon_1 > 0$. The {\it first-layer} features (linear features) are $ {\bf z}^\star = W^\star \vec{x}$, where $W^\star\!\in\! \mathbb{R}^{d^{\epsilon_1}\!\times\!\,d}$ has  orthonormal unit vectors as rows, and ${\bf a}^* \in \mathbb R^{d^{\varepsilon_1}}$ is choosen randomly from a Gaussian distribution. We refer to the variable $h^*$ as the \textit{index} in the name of the class.  
This construction, a generalization of the hidden manifold model \citep{goldt2020modeling}, is motivated by the compositional structure present in real-world functions and by the analysis carried over by  \cite{wang2023learning,nichani2024provable}. The strictly decreasing 
dimensionality of the features across depth allows us to avoid the pitfall of the original hidden manifold model \citep{goldt2020modeling} that turns out to be equivalent to a Gaussian linear target \citep{goldt2022gaussian,hu2022universality,montanari2022universality}.  

\vspace{-0.3cm}
\subsection{Multi-Index Gaussian Hierarchical Targets}
\vspace{-0.1cm}
A simple generalization of the above construction is to include many non-linear features, leading to Multi-Index Gaussian Hierarchical Targets (MIGHT) defined as:
\begin{align}
\label{eq:3layer_target_might}
f^\star(\vec{x}) =  g^\star\left( h^\star_{1}(\vec{x}),\ldots, h^\star_{r}(\vec{x})\right),
\end{align}
where
\begin{equation}
\label{eq:non_linear_feature_def_might}
    h^\star_m(\vec{x}) = \frac{1}{\sqrt{d^{\epsilon_{1}}}} {\bf a}^{\star \top}_m P_{k,m}\left(W_m^\star \vec{x}\right),\, m=1,\ldots r\,,
\end{equation}
with now  $r$ directions, each with their own  layer weights (${\bf a}_m$ and $W^\star_m$), and 
polynomials ($P_{k,m}$).
we illustrate the functions considered in this paper graphically. We first illustrate the SIGHT (\ref{eq:3layer_target}) and MIGHT (\ref{eq:3layer_target_might}) in Fig.~\ref{fig:app:sandm_targets}.
\vspace{-0.3cm}


\begin{figure*}[t]
\centering
\includegraphics[width=0.7\linewidth]{figs/DeepSightMight.pdf}
\vspace{-2cm}
\caption{\textbf{Deep SIGHT and MIGHT:} Illustration of deep target functions. {\bf Left: A SIGHT function with depth $L=3$.} Here we first go from ${\bf x} \in {\mathbb R}^d$ to ${\bf h}_1 \in {\mathbb R}^{d^{\varepsilon_1}}$. After applying the polynomial transformation pointwise (not shown), we now divide ${\bf h}_1$ into $d^{\varepsilon_2}$ blocks of sizes $d^{\varepsilon_2-\varepsilon_1}$. Each of these blocks is projected to create one of the components of ${\bf h}_1 \in {\mathbb R}^{d^{\varepsilon_2}}$. After another polynomial transformation (not shown)  we finally project to a single value $h_3^\star$. We can then output the label $y=g^\star(h_3^\star)$. {\bf Right: A MIGHT function with depth $L=4$.} Again, we go from ${\bf x} \in {\mathbb R}^d$ to ${\bf h}_1 \in {\mathbb R}^{d^{\varepsilon_1}}$. After applyging the polynomial transformation pointwise (not shown), we now divide ${\bf h}_1$ into $d^{\varepsilon_2}$ blocks of sizes $d^{\varepsilon_1-\varepsilon_2}$. Each of these blocks is projected to create one of the components of ${\bf h}_2\in {\mathbb R}^{d^{\varepsilon_2}}$. We repeat this operation: we further divide ${\bf h}_2$ into $d^{\varepsilon_3}$ blocks of sizes $d^{\varepsilon_2-\varepsilon_3}$ and each of these blocs is projected to create one of the components of ${\bf h}_3 \in {\mathbb R}^{d^{\varepsilon_3}}$. After another polynomial transformation (not shown) we finally project on two values $h_{4,1}^\star$ and $h_{4,2}^\star$ and create $y$ as a two-index function $y=g^\star(h_{4,1}^\star,h_{4,2}^\star)$.
     \label{fig:app:deep_targets}}
\end{figure*}


\subsection{Deep Multi-Index Hierarchical Targets}
\vspace{-0.1cm}
Finally, we define the {\it deep} version of MIGHTs as
%(see also App.~\ref{sec:app:plots}) as:
%
\begin{equation}
    \label{eq:target_def_deep}
    f^\star(\vec{x}) =  g^\star\left( h^\star_{L,1}(\vec{x}),\ldots,h^\star_{L,r}(\vec{x})\right),
\end{equation}
with Gaussian data $\{\vec x_\mu\}_{\mu =1}^n \!\in\!\mathbb R^d$, and where each features ${\bf h}^\star_{\ell}(\vec{x})\!\in\!{\mathbb R}^{d^\varepsilon_\ell}$ are recursively defined as:
\begin{equation}
    \label{eq:non_linear_feature_def}
    {h}^\star_{\ell,m}(\vec{x}) = \frac{1}{\sqrt{d^{\epsilon_{\ell-1}-\epsilon_{\ell}}}}\vec a^{\star^\top}_{\ell,m} P_{k, m, \ell}\left({\bf h}^\star_{\ell-1,\{1+(m-1)d^{\epsilon_{\ell-1}-\epsilon_{\ell}},\ldots, md^{\epsilon_{\ell-1}-\epsilon_{\ell}}  \}}(\vec{x})\right), 
\end{equation}
with $\ell = 1 \cdots L$, $m = 1\cdots d^{\varepsilon_{\ell}}$ and where ${\bf a}^\star_{\ell,m} \in \mathbb{R}^{d^{\epsilon_{\ell-1}-\epsilon_{\ell}}}$  acts on the $m_{th}$ block of the previous layer feature $\vec h^\star_{\ell-1}(\vec{x})$ (each of them being of size $d^{\epsilon_{\ell-1}-\epsilon_{\ell}}$). Again $P_{k,m, \ell}$ are fixed polynomials for $\ell\!=\!1, \cdots, L\!-\!1$; $d^{\epsilon_{\ell}}$ denotes the dimensionality of the features at layer $\ell$, which we choose to be strictly decreasing across depth, i.e., $1 > \epsilon_1 \!>\! \epsilon_2 \!>\! \cdots \!>\! \epsilon_{L-1} \!>0$, with ${\bf h}^\star_L \in \mathbb{R}^r$ being finite-dimensional. For clarity, we illustrate the deep version of SIGHT and MIGHT functions in Fig.~\ref{fig:app:deep_targets}, where the tree structure of the deep version of these target functions is apparent.


%{\color{red}LZ: What follows is very obscure. Is it needed?} 
% For the sake of simplicity, we restrict ourselves to the connection weights $A^\star$ possessing a ``tree-like" structure, and not mi
% \begin{equation}
% \label{eq:tree_like_connection}
%     A^\star_\ell = \begin{pmatrix}
%         a^\star_{\ell,1} & 0 & \cdots & 0\\
%         0 &  a^\star_{\ell,2} & \cdots & 0\\
%         \vdots \\
%         0 &  0 & \cdots & a^\star_{\ell,d^{\epsilon_{\ell}}}\\
%     \end{pmatrix},
% \end{equation}
% {\color{red}LZ: Why this dimension, do not understand.} 
This "tree-like" construction of $f^\star(\vec{x})$ ensures that for any layer index $\ell \in 1, \cdots, L$,  the hidden features 
$h^\star_{\ell, m}(\vec{x})$ remain independent for different index $m \in 1, \cdots, d^{\varepsilon_{\ell}}$.  Finally, the $1^{\rm st}$-layer features are defined as \looseness=-1
\begin{equation}
\label{eq:linear_feature}
     {\bf h}^\star_{1}(\vec{x})= {\bf z}^\star = W^\star \vec{x},
\end{equation}
where $W^\star\!\in\! \mathbb{R}^{d^{\epsilon_1}\!\times\!\,d}$ has  orthonormal unit vectors as rows. By explicitly incorporating multiple levels of non-linear feature transformations, each associated with a progressively reduced latent dimensionality, it models the deep hierarchical structure is a feature of complex real-world tasks, see e.g. \citep{mallat1999wavelet,lecun2015deep,mossel2016deep,cagnetta2024towards, sclocchi2024probinglatenthierarchicalstructure}. 
%Moreover, the tree-like structure of the connection weights, combined with the independence of hidden features at each layer, ensures the analytical tractability while preserving the expressive power of the hierarchy.

\vspace{-0.3cm}
\subsection{Learning Model} 
\vspace{-0.1cm}
We now consider learning SIGHT and MIGHT functions $f^\star(\vec{x})$ through an $L$-layer neural network, that is a standard multi-layer perceptron:
\begin{equation}
\hat{f}_\theta(\vec{x}) = b_L+\vec{w}_L^\top \sigma({\bf b}_{L-1}+W_{L-1} \cdots \sigma({\bf b}_1+ W_1(\vec{x} ))),
\end{equation}
where $\theta$ denotes the ensemble of trainable parameters $\{{\bf b}_\ell,W_\ell,\, \ell =1 \cdots L\}$. The hidden layer weights have dimension $W_\ell \in \mathbb{R}^{p_{\ell} \times p_{\ell-1}}$ for $\ell \in \{2,\cdots, L-1\}$ with readout layer $W_L \in \mathbb{R}^{p_L}$ and first layer $W_1 \in \mathbb{R}^{p_1 \times d}$, and the biases ${\bf b}_\ell$ are in $\mathbb{R}^{p_\ell}$. We shall consider Empirical Risk Minimization (ERM) of the square loss $\hat {\mathcal{R}}(\{\vec x_\mu\}) = \sum_{\mu =1}^n \left(f^*({\bf x}_{\mu}) - \hat f_{\theta}({\bf x}_{\mu}) \right)^2$ with gradient descent.
% \lz{We are using $p_l$ both for the polynom name and for the with of each layer. CHANGE one of them!!!!! }

\vspace{-0.3cm}
\subsection{Main Results in a Nutshell} 
\vspace{-0.1cm}
The backbone of our results is the analysis of the asymptotic performance of learning SIGHT and MIGHT functions using multi-layer networks trained with Gradient Descent on Gaussian data, as both $n$ (the number of data) and $d$ (the dimension of the data) grow to infinity. We unveil a series of sharp thresholds in the sample complexity ratio $\kappa = \frac{\log n}{\log d}$ where neural networks learn the target with increasing accuracy. To summarize:\looseness=-1
\begin{itemize}
[noitemsep,leftmargin=1em,wide=0pt]
    \item Our targets offer a solvable playground to unveil the computational advantage of deep networks over shallow ones. The learning mechanism can be viewed as the reduction of the ``effective dimension" in which networks trained on $f^\star(\vec{x})$  successively reduces the dimensionality of the search space:
    \vspace{-0.1cm}
    \begin{equation}
    d^{\epsilon_1} \rightarrow d^{\epsilon_2} \rightarrow d^{\epsilon_3}, \cdots, \rightarrow r.\label{eq:dim_red}
      \end{equation}
     Depth acts as a progressive filter that {\it distills} data into lower-dimensional representations (a coarse-graining mechanism akin to renormalization in physics), enabling efficient learning of subsequent layers.     
  %  \item We focus our mathematical analysis on the layer-wise training of three-layer networks, where each layer is trained sequentially and independently. Under specific universality and isotropy assumptions, we establish a precise result on the recovery of the second-layer features $\vec{h}^\star_2$ given the first-layer outputs, and subsequently the third-layer features $\vec{h}^\star_3$. Our findings show that this hierarchical recovery requires a sample complexity of the form $\tilde{O}(d^{k\epsilon_1 + \epsilon_2})$. This result offers a rigorous demonstration of how depth enables effective dimensionality reduction in the MIGHT framework.
  \item We focus the rigorous analysis in the paper on the case of shallow SIGHT functions (eq.~\eqref{eq:3layer_target}) learned by $3-$layer networks, where each layer is trained sequentially and independently. We prove that a three-layer network trained in a layer-wise fashion can learn a SIGHT function $f^\star(\vec{x})$ efficiently. Specifically, the network first recovers $W^\star$ using $\tilde{O}(d^{\epsilon_1 + 1})$ samples, then reconstructs $h^*$ with $\tilde{O}(d^{k \epsilon_1})$ samples (with $k$ denoting the degree of $P_k$, in case $k \epsilon_1< 1+\epsilon_1$ both happen at $1+\epsilon_1$), and finally fits $f^\star$ as a function of $h^\star$ using only $\tilde{O}(1)$ samples. This sample complexity aligns with predictions from the dimension-reduction/coarse-graining perspective, where earlier layers successively reduce the effective dimensionality of the learning problem.
  %and represents a strict improvement over both a 2-layer network and a 3-layer network where only the last two layers are trained. 
  We also present additional results for deeper targets and networks.
%\item The mechanism behind the dimensional reduction  is captured mathemactially through two key training paradigms: one that updates the Conjugate Kernel in a layerwise fashion, and another where only the first layer undergoes training via a Neural Tangent Kernel update. These findings provide insights into the mechanisms underlying these advantages, drawing connections to random feature models and the consequent dimensionality reductions. 
    \item We further explore the problem through numerical simulations using more realistic training procedures than those covered by the theorems. Our results suggest that the dimensionality reduction mechanism remains broadly applicable. Notably, we illustrate such a phenomenon using 3-layer networks training all the layers jointly with standard backpropagation. %To further hammer on the advantage of the 3-layer architecture, we also show that it outperforms 2-layer ones even {\it when only the first layer is trained}. Indeed we show that the second layerâ€™s features can still adapt via an evolving neural tangent kernel mechanism, and the networks outperform shallow two-layer ones, which illustrates the architectural nature of their superiority. 
    We provide the code of our simulations at \href{https://github.com/IdePHICS/ComputationalDepth}{https://github.com/IdePHICS/ComputationalDepth}.\looseness=-1
\end{itemize}
\vspace{-0.3cm}
\subsection{Related Works}
\vspace{-0.1cm}
\paragraph {Random Feature Models ---} A key attribute enabling the effectiveness of neural networks is their ability to adjust to low-dimensional features present in the training data. However, interestingly, much of the current theoretical understanding of neural networks comes from studying their lazy regime, where features are not learned during training. One of the most pre-eminent examples of such ``fixed-features'' regimes are Random Feature (RF) models, initially introduced as a computationally efficient approximation to kernel methods by \cite{rahimi2007random}, they have gained attention as models of two-layer neural networks in the lazy regime. One of the main motivations is their sharp generalization guarantees in the high-dimensional limit \citep{gerace2020generalisation,goldt_gaussian_2021,mei2022generalization, Mei2023,xiao2022precise,defilippis2024dimension}. As mentioned, however, the performance of such methods, and of any kernel method in general, is limited. A fundamental theorem in \cite{mei2022generalization} states that only a polynomial approximation up to degree $\kappa_{\rm RF}$ of any target $f^*$, with $\kappa_{\rm RF} = {\rm min} (\kappa_1,\kappa_2)$ when learning with $n=d^{\kappa_1}$ data and $p=d^{\kappa_2}$ features. While even shallow networks can surpass these limitations \citep{ghorbani2021linearized,ba2020generalization,dandi2024random}, this relation for $\kappa_{\rm RF}$ plays a fundamental role in our analysis. 

\paragraph{Multi-index Models ---}
Despite the theoretical successes in describing fixed feature methods, the holy grail of machine learning theory remains a rigorous description of network adaptation to low-dimensional features. A popular model to study such low-dimensional structure in the learning performance is the  {\it multi-index model}. For this class of target (denoted as $f^\star_{MI}$), the input datum $\vec x$ is projected on a $r-$dimensional subspace $W^\star = \{\vec{w}^\star_j, j \in 1 \cdots r\}$ and the input-output relation depend solely on a non-linear map $g^\star$ of these $r$ (linear) features : 
\begin{align}
    f^\star_{\rm MI}(\vec x) = g^\star(\vec{x}^\top \vec{w}^\star_1, \dots,\vec{x}^\top \vec{w}^\star_r)
\end{align}
While the information theoretical performance is well understood \cite{barbier2019optimal,aubin2018committee}, there has been intense theoretical scrutiny to characterize the sample complexity needed to learn multi-index models with shallow models. On the one hand, kernel methods can only learn a polynomial approximation \citep{mei2022generalization}; on the other hand, the situation in neural networks appears more complicated at first as the hardness of a given $f^\star_{\rm MI}$ has been characterized by the ``information'' and ``leap'' exponents \cite{BenArous2021, abbe2022merged, dandi2024twolayer, damian2024computational, dandi2024benefits,arnaboldi2024repetita, lee2024neural, bietti2023learning, simsek2024learning, arous2024stochastic}.
It was shown, however, that simple modification of vanilla Stochastic Gradient Descent (SGD), such as Extra-Gradient methods or Sharpness Aware Minimizers, are able to attain sample complexity corresponding to Statistical Query (SQ) lower bound \citep{arnaboldi2024repetita, lee2024neural}, and are essentially optimal up to polylog factors in the dimension \citep{damian2024computational,troiani2024fundamental}. A motivation of the present work is to go beyond such limitations.
% \subsection*{Benefit of reusing batches} We are able to show that the sample complexity can be reduced up to $n = O(d_{\rm eff}^{k})$ by reusing batches. The result in Fig.~\ref{fig:online/offline} exeplifies this for $d_{\rm eff} = \sqrt{d}$ and $k = 3$
%     \begin{enumerate}
%         \item We first present a rigorous analysis based on matrix concentration arguments for the functional single-index model, i.e. $r=1$. The analysis is based on linearization arguments on the dynamics of the preactivations. 
%         \item We present heuristic arguments for the functional multi-index case ($r>1$). 
%         \item The above sample complexity matches what one would expect from Information Theoretic arguments (simple parameter counting). 
%     \end{enumerate}
% \subsection*{General Depth Hierarchical Targets} We extend recursively the above argument to deeper networks using the hierarchical tree structure over multiple layers. 


\begin{wrapfigure}{rt}{0.5\textwidth}
\centering
{\includegraphics[width=0.95\linewidth]{figs/MainFig_v4.jpg}}
\caption{An illustration of the phase transitions in learning SIGHT according to the main Theorem~\ref{thm:main_theorem} denoting the computational advantage of depth for two different target model: \textbf{(a)} generic shallow SIGHT function of the form in eq.~\eqref{eq:3layer_target} and \textbf{(b)} the particular example in eq.~\eqref{main-example}. All methods learn functions of increasing complexity with more data, but three layer nets do so more efficiently than two layer ones, which are themselves more efficient than kernels.
\label{mainfig}
}
\end{wrapfigure}

\paragraph{3-Layers networks  ---} Substantial effort has been devoted to investigating the approximation advantages conferred by deeper neural network architectures \citep{pmlr-v49-telgarsky16,eldan2016power, safran2022optimization}. However, it remains unclear how these approximation gaps translate into sample complexity ones for neural networks when trained through gradient descent. An important step towards the role of depth in neural networks has been carried over by  \cite{wang2023learning,nichani2024provable}, who proved separation results between the test performance of 2 \& 3 layer networks. More precisely, \cite{wang2023learning}  proved that 3-layer architectures with a fixed first layer can learn a target function of the form $g^\star(\vec{x}^\top A \vec{x})$ in $n = \tilde{O}(d^4)$ samples through a single-gradient step on the second layer, where $\vec{x} \in \mathbb{R}^d$ and $A \in \mathbb{R}^{d \times d}$. In contrast, 2-layer networks require a super-polynomial number of samples in terms of the degree of $g^\star$. \cite{nichani2024provable} subsequently improved the sample complexity to $\tilde{O}(d^2)$ and generalized the result to functions of $p_{th}$-order polynomials. 
\cite{fu2024learning} further extended these results to learning multiple-nonlinear features. 
Here, we go beyond these results to prove stronger separation results by analyzing fully trained networks without a fixed first layer.


\paragraph{Universality ---} 
    A crucial role in our analysis is played by the asymptotic Gaussianity of $\vec {h}^\star_{\ell}(\vec{x})$ which leads to a simplified description of how dependencies on  $\vec {h}^\star_{\ell}(\vec{x})$ propagate to lower-level features. Such a property
    is a crucial component of the analysis in  \cite{nichani2024provable,wang2023learning}. Specifically, \cite{nichani2024provable,wang2023learning} showed that the projection of $g^\star(\langle \text{He}_k(\vec{x}), A \rangle)$ on degree-$k$ Hermite polynomials lies along the non-linear feature $\langle \text{He}_k(\vec{x}), A \rangle$ while $g^\star$ has vanishing projections on lower degree terms. 
    We generalize these results to describe the projections on all degree components.


\paragraph{Coarse-graining ---} The dimensionality reduction we describe is closely related to the concept of learning features across different scales. This idea has been explored in the context of machine learning through connections with the renormalization group \cite{wilson1971renormalization} in physics, where each scale corresponds to a distinct set of features. Such techniques have inspired studies of deep neural networks \citep{mehta2014exact,li2018neural,marchand2022wavelet}.  Here, we present a concrete example of such a coarse-graining mechanism, illustrating how hierarchical structures can be analyzed explicitly.


% We provide the sample complexity for online training algorithms as $n = O(d_{\rm eff}^{2k})$. This result generalizes the findings of \cite{nichani2024provable}, which focus on a fixed polynomial degree $k = 2$. 
% {\bf FK ? ?? what does this means?????}
% Our result follows from a direct computation:
% \[
% \vec{x}^\top A \vec{x} = \sum_{i \in [d]} \lambda_i \vec{x}_i^\top \vec{v}_i \vec{v}_i^\top \vec{x}_i,
% \]
% where $g^\star(\vec{x}^\top \vec{v}_i) = (\vec{x}^\top \vec{v}_i)^2$. Notably, the authors of \cite{nichani2024provable} do not consider the training of the first layer.
% {\bf Sufficient statistics --} 
% {\bf FK : Why do we need this in the main?!?!?! }
% Starting with the pivotal in the $90'$s, approach with summary statistics to study the learning behaviour of neural networks// Recently, revival with hardness exponents // We perform a leap forward correctly identifying the order paraemters for deep targets: overlap in function space between $h(\vec x)$ and $h^\star(\vec x)$. The main result will deal with the dynamics of this object under gradient descent updates. Below, the definition of the order parameters:


\section{Heuristic argument underlying the main results}
\label{sec:heuristic}

Before presenting the main technical results, we describe here a heuristic argument describing the narrative behind the results. 
%Understanding the full learning dynamics of deep neural networks remains an open and formidable challenge in machine learning theory. 
For concreteness, we focus here on learning a shallow SIGHT function \eqref{eq:3layer_target} as a first step toward a broader understanding. For concreteness, we will discuss the following example (later used in Fig.~\ref{fig:gen_error_fig1}):
\begin{equation}
\label{main-example}
f^\star(\vec{x}) =  \tanh\left(
\frac{\vec{a}^{\star^\top} \, 
P_3\left(W^\star \vec{x}\right) }{\sqrt{d^{\varepsilon_1=1/2}}} \right)
\end{equation}
with a polynomial $P_3(x)={\rm He}_2(x)+{\rm He}_3(x)$ (the second and third Hermite polynomials), $\epsilon_1=1/2$, and discuss the performance of different learning architectures, highlighting the dimensionality reduction due to feature learning. The learning dynamics for general SIGHT (eq.~\eqref{eq:3layer_target}) and the particular example above (eq.~~\eqref{main-example}) are illustrated in Fig.~\ref{mainfig} respectively in the top and bottom panel.

\begin{enumerate}[noitemsep,leftmargin=1em,wide=0pt]
\item[a)] {\bf Kernel methods, or random feature models}, can only learn a polynomial approximation of degree $\kappa$ in the Hermite basis of $f^\star$ if $n = O(d^\kappa)$ \citep{mei2022generalization}. This is a strong limitation that leads to poor performance as the learning method is not sensitive to the presence of relevant low-dimensional structure, but rather only to the degree of the target. 
%{\color{red}LZ: Here it starts to be unclear, what exactly is quadratic? Be more precise in what you want to stay here. I could not follow the next two sentences.} 
In the example \eqref{main-example}, the lowest (Hermite) polynomial order is quadratic in ${\bf x}$ (as can be seen by expanding the $\tanh$): learning it thus requires $n=O(d^2)$ samples of data for a kernel method to beat random performance. Learning the cubic approximation would requires  $n=O(d^3)$ samples, etc. The corresponding thresholds are sketched in orange in Fig.~\ref{mainfig}. 

\item[b)] We now turn to {\bf two layer net} of the form (we do not write explicitly the additional biases for clarity) with a number of neurons $p$ at least of order $\Theta(d^{k\epsilon_1+\delta})$
\begin{equation}
\hat{f}_\theta(\vec{x}) = \vec{w}_2^\top \sigma(W_1(\vec{x}))\,
\end{equation}
Thanks to feature learning, such architecture should perform better: Indeed, for $W_1$ to learn the $d \times d^{\eps_1}$ first-layer feature matrix $W^\star$, we need {\it at least} $n=O(d\times d^{\eps_1})$ data. If $n \gg d^{1+\eps_1}$, we thus expect that $W_1$ correlates with $W^\star$. Intuitively, $W_1$ is then close to a noisy random rotation of  $W^\star$and behaves roughly as $W_1 \approx Z_1 W^\star + Z_2$ (with $Z_1$ and $Z_2$ are essentially random matrices). The two-layer neural net thus now  behaves as:
\begin{equation}
\hat{f}_\theta(\vec{x}) \approx \vec{w}_2^\top \sigma\left(
Z_1 {\vec z}^\star  + Z_3 \right)\,.
\end{equation}
Fitting now the outer weights $\vec{w}_2$ leads, once again, to a random feature model, but now applied to the target  eq.~\eqref{eq:3layer_target-reduced} seen as a function of ${\bf z}$ instead of eq~\eqref{eq:3layer_target} seen as a function of ${\bf x}$. This leads to an effective Random Feature model with respect to {\it the lower dimensional vector} $\{{\vec z}^\star \in {\mathbb R}^{d^{\eps_1}}\}$. Thanks to this dimensional reduction from dimension $d$ to the effective one $d^{\eps_1}$, we just need  $n={(d^{\varepsilon_1})}^\kappa$ samples of data to now fit a $\kappa-$th degree polynomial approximation of $f^\star$. This is a drastic improvement.\looseness=-1

Coming back to the example: with  $n=O(d^{1+\epsilon_1=1.5})$, $\kappa=1.5$, data samples a two-layer net learns the first layer representation $W^\star$, leading to a dimensionality reduction from $d$ to $\sqrt{d}$. From $n=O(d^{3 \epsilon_1 = 1.5})$, $\kappa=1.5$, we are also able to fit a (Hermite) polynomial approximation of degree $3$ of the target viewed as a function of $\vec z$. The next order in the expansion of \eqref{main-example} is power $6$ in $\vec z$, and thus will be fitted at $\kappa=3$.

\item[c)] We now finally consider {\bf a three-layer neural networks}, with width $p_2=p_1 = \Theta(d^{k\epsilon_1+\delta})$:
\begin{equation}
\hat{f}_\theta(\vec{x}) = \vec{w}_3^\top \sigma(W_{2} \sigma(W_1\vec{x}))\,.
\end{equation}
We still expect that $W_1$ learns the first-layer features  $W^\star$ when $n \gg d^{1+\eps_1}$, at which point:
\begin{equation}
\hat{f}_\theta(\vec{x}) \approx \vec{w}_3^\top \sigma(
W_{2}  \sigma\left(
Z_1 \vec z^\star + Z_3)\right)
\end{equation}
However, contrary to the previously depicted shallow case, three-layer networks can further approximate $h^\star$ by updating the second layer. With each power of $d^{\eps_1}$ we expect to be fit an power approximation of $h^\star$ and, in particular, with $ n = O(d^{k\eps_1})$, we expect the second layer preactivation $h_2(\vec x) = W_2 \sigma(W_1 \vec x)$ to correlate compeltly with the ($k-$polynomial) features $h^\star$. Therefore, denoting again $Z_4, Z_5$ as random matrices, a 3-layer network now acts as:
\begin{equation}
\hat{f}_\theta(\vec{x}) \approx \vec{w}_3^\top \sigma\left(Z_4 h^\star + Z_5\right),
\end{equation} 
Fitting now $\vec w_3$ leads to a random feature model on the {\it scalar} $h$, which can be fitted perfectly with any growing number of samples $n$. In other words, through successive coarse-graining from $d^{\rm eff}\!=\!d\!\to\!\sqrt{d}\!\to\!1$, we have reduced the dimension from a diverging one ($d$) to a finite one. 

Note that generalization error as plotted in Fig.~\ref{mainfig} can jump for two reasons as $n$ increases: either because of a reduction of the dimension $d_{\rm eff}$, or because of an increase of polynomial fitting power within this dimension. The phenomenology is a bit simpler in the particular example \eqref{main-example}, where the advantage of a three-layer net is considerable: for $n = O(d^{1.5})$, the network learns to represent the non-linear features $h^\star$ directly, and thus can learn the entire function. \looseness=-1
\end{enumerate}

%\vspace{-0.3 cm}
While such parameter counting sounds reasonable,  this heuristic may fail for general data distributions, as high-degree polynomials may localize on low-dimensional structures. However, for Gaussian and spherical measures, hypercontractivity ensures that such polynomials remain delocalized [Lemma \ref{lem:hyper}]. Our analysis relies on proving that such a property holds under feature learning, and even for deep non-linear hidden features. This delocalization is crucial for matrix concentration arguments, as it enables the approximation of eigenfunctions via random projections \cite{mei_generalization_2022}.

This scenario, illustrated in Fig.~\ref{mainfig}, extends {\it mutatis mutandis} to generic deep multi-layer MIGHT functions, where a sequence of transitions emerges progressively across the layers. 
%$d^{\epsilon_1} \rightarrow d^{\epsilon_2} \rightarrow d^{\epsilon_3}, \cdots, \rightarrow r$. 
Consider for instance the following hierarchical target function from eq.~\eqref{eq:non_linear_feature_def} (see also Fig.~\ref{fig:app:deep_targets}):
    \begin{equation}
  f^\star(\vec{x}) =  \tanh\left(\frac{\vec{a}^{\star^\top}
P_{k'}\left({\bf h}^\star_{2}\right) }{\sqrt{d^{\varepsilon_2}}} \right),\,        {h}^\star_{2,m} = \left(\frac{\vec{a}_{2,m}^{\star^\top} 
P_k\left({\bf h}^\star_{1}=W^\star {\bf x}\right)_{\{1+(m-1)d^{\varepsilon_1-\varepsilon_2},\ldots,md^{\varepsilon_1-\varepsilon_2}\}} }{\sqrt{d^{\varepsilon_1-\varepsilon_2}}}\right)\,.\nonumber
    \end{equation}
In this case we expect a reduction from $d\!\to\!d^{\varepsilon_1}\!\to\!d^{\varepsilon_2}\!\to\!1$. The first one arises at  $n=O(d^{\epsilon_1+1})$ when learning $W^\star$, then at $n=O(d^{k\varepsilon_1+\varepsilon_2})$ (to learn all the $d^{\varepsilon_2}$ polynomials, each of them requiring $d^{k\varepsilon_1}$ data) and finally at $n=O(d^{k' \varepsilon_2})$ to learn the activation in the $\tanh$ (a single $k'$ polynomial in dimension $d^{\varepsilon_2}$). Note that while these must proceed in this order, some of these jumps can happen at the same value of $\kappa$. For instance, if $k'\varepsilon_2\!<\!k\varepsilon_1+\varepsilon_2$, then the last two jumps arise simultaneously. 
    
%    f^\star(\vec{x}) =  \tanh\left({\vec{a}_2^{\star^\top}
%p_3\left({\bf h}^\star_{(2)}\right) }/{\sqrt{d^{\varepsilon_2=1/4}}} \right)$, with ${\bf h}^\star_{(2)} = \left({\vec{a}_2^{\star^\top} 
%p_3\left({\bf h}^\star_{(1)}\right) }/{\sqrt{d^{\varepsilon_1=1/2}}}\right)$
%and $h^\star_1=.   
   

%FK large enough width (d>n)


\section{Main Theoretical Results}
\label{sec:main_theorems}
% So far the {\it degree counting} analysis is based on information theoretic arguments, computing how many parameters are needed to learn the function. 
 We now turn to the main part of our results that describe learning of the SIGHT and MIGHT function classes with deep neural networks trained by gradient descent. We present a rigorous analysis of gradient-based Empirical Risk Minimization (ERM). 
 %As is known through the analysis of multi-index functions, this analysis is not a negligible detail. 
Since a complete rigorous analysis of gradient descent in deep networks is extremely challenging -- and hitherto elusive-- we first present a rigorous description for the SIGHT target of eq.~\eqref{eq:3layer_target} under a specific deep-learning schedule. This approach enables us to provide precise theorems that capture the hierarchical learning process.
We analyze the following training procedure:
\vspace{-0.2 cm}
%and formally justify the heuristic picture outlined above:
%\lz{The following training procedure needs to be written as concisely as possible. Remove ALL the comments about why and relation to the proofs. Just specify the training about which we have proofs so that anyone can grasp it readily. Ideally state in an order in which you would state it in the code, e.g. staring with initialiation etc.}
\looseness=-1
\begin{itemize}[noitemsep,leftmargin=1em,wide=0pt]
 \item {\bf Initilization:} The parameters of the model $ \hat{f}_\theta(\vec{x}) = \vec{w}_3^\top \sigma(W_{2} \sigma(W_1\vec{x}))$ are initialized as $W_{1,i} \sim U(\mathcal{S}^{d-1}(1))$ for $i \in [p_1]$, $W_2 = \mathbf{I}_{p_1}$, and $w_{3,i}= 1$ for $i \in [p]$.
    \item \textbf{Layer-wise training}: (i) We first perform a pre-determined number $T_1$ of gradient updates on the first layer $W_1$ on independent batches of data for each step \cite{dandi2024twolayer}. (ii) Subsequently, we re-initialize the second layer $W_2$ do a single large gradient step. (iii) Finally, we update $\vec{w}_3$ through ridge regression. %(equivalent to optimizing $\vec{w}_3$ through gradient descent on squared loss with an appropriately chosen step size). 
     Layer-wise training procedures are a common simplifying assumption in the analysis of two-layer networks \citep{damian2022neural,abbe2023sgd,dandi2024twolayer}. %A complete analysis of the joint training remains open even for two-layer networks except for training of layers at differing time scales \cite{berthier2024learning,bietti2022learning}.\looseness=-1
  \item \textbf{Neuron-wise spherical projections}: While updating the first layer parameters $W_1$, we utilize spherical-gradient and project each neuron onto the unit sphere. Such spherical projections are commonly utilized in the literature on two-layer networks \citep{BenArous2021, abbe2023sgd}. 
   % \item \textbf{Single-index hierarchical targets} ($r=1$): For our rigorous result, we restrict ourselves to the setting of a single non-linear feature i.e. targets of the form:
%\begin{equation}\label{def:sing-ind}
%      f^\star(\vec{x})=  g^\star\left(h^\star(\vec{x})\right),
%    \end{equation}
%    where $h^\star(\vec{x}) = \frac{\vec{a}^\star \cdot 
%p_k\left( W^\star\vec{x} \right)}{\sqrt{d^{\varepsilon_1}}}$.

%The above model suffices to illustrate the dimension-reduction phenomenon described in 
 % Equation \ref{eq:dim_red} while simplifying our analysis since $h_2(\vec{x})$ is required to recover a single non-linear feature $h^\star(\vec{x})$. 
%    {\color{red} LZ: I find the explanation below of the training procedure unclear. Can we stick to the essential points?}
    \item \textbf{Pre-conditioning of gradient for the second layer}: 
    %Since the analysis of gradient descent with the same batch size on a diverging number of updates is quite challenging, we instead reduce the number of required updates 
    We use a pre-conditioning of the gradient step --- broadly used in various optimization schedules (e.g.~Adam \cite{kingma2014adam})--- using the sample-covariance of the features as preconditioning matrix, i.e., 
    $$\Delta W_2 = -\eta (\frac{1}{n}\sigma(W_1 X^\top)^\top (\sigma(W_1 X)^\top)^{-1} \nabla_{W_2} \mathcal{L}$$
    %Let $\vec h_2(\vec{x}) = W_2\sigma(W_1 \vec{x})$ denote the pre-activations output by the second layer. 
    Through the feature map $ \vec{x} \rightarrow \sigma(W_1 \vec{x})$, the updates of $W_2$ in parameter space translate to updates to  $h_2(\vec{x})$ in function space.
    %Indeed (see Appendix \ref{app:pre-cond}) due to the spectral bias of the kernel defined by the feature maps $\sigma(W_1 \vec{x})$, the gradient $\nabla_{W_2}\mathcal{L}$ with respect to $W_2$ is dominated by directions corresponding to low-degree updates to $ \vec h_2(\vec{x})$, with the components along degree-$k$ functions decaying as $\frac{1}{d^{k \epsilon_1}}$. This decay necessitates $\Theta(d^{k \epsilon_1})$ gradient updates on $W_2$ for $\vec h_2(\vec{x})$ to recover degree-$k$ components of $h^\star(\vec{x})$ for under the optimal sample-complexity of $\Theta(d^{k \epsilon_1})$. 
    Without such pre-conditioning, online SGD leads to a worse sample complexity of $\Theta(d^{2k \epsilon_1})$ as in the single-step analysis of \cite{nichani2024provable}.\looseness=-1 
\end{itemize}
%
With this algorithm, we can now study gradient descent and demonstrate the learning of a class of SIGHT function. The theorem will assume the following conditions:
\begin{itemize}[noitemsep,leftmargin=1em,wide=0pt]
 \item \textbf{Uniform weighting}: We set  $\{a^\star_{i}=1$  for all $i \in 1 \cdots d^{\varepsilon_1}\}$: This operation ensures isotropic dependence along all components, simplifying the analysis. While $a^\star_{i}=1$ is a particular choice of target weights, the training algorithm of the model is agnostic to this choice and we, therefore, obtain sample-complexity expected for a general non-linear feature of the form ${\vec{a}^{\star^\top} 
P_k\left( W^\star \vec{x} \right)}/{\sqrt{d^{\varepsilon_1}}}$.
\item \textbf{Information exponent}: We shall indeed require that the information exponent \cite{BenArous2021,abbe2022merged,abbe2023sgd} of $g^\star(\cdot)$ is $1$ and  that of $P_k(\cdot)$ is $2$. This condition is necessary, as gradient descent (without repetition) has a drastic worst complexity for exponents larger than $2$. 
%
\begin{assumption}\label{ass:target}
Let $z \sim \mathcal{N}(0,1)$ denote a standard normal variable. We assume that $\Ea{g^\star(z)z} \neq 0$, $\Ea{P_k(z)\text{He}_2(z)} \neq 0$. We further assume that $\sigma$ is analytic, satisfies $\sigma'(0)\neq 0$ and there exist constants $L_1, L_2 \in \mathbb{R}^+$ such that $\abs{\sigma(x)} \leq L_1+L_2\abs{x}^4$.
\end{assumption}

We further require the activations of the neural net $\sigma(\cdot)$ to be sufficiently expressive and satisfies certain alignment conditions:
\begin{assumption}\label{ass:act}
    $\sigma:\mathbb{R} \rightarrow \mathbb{E}$ is analytic, non-polynomial and satisfies: 
    $$
    \Ea{\sigma(z)\text{He}_j(z)} \neq 0,
    $$
    for all $1 < j \leq k$,  $$\Ea{\sigma(\sigma(z))\text{He}_2(z)}\Ea{P_k(z)\text{He}_2(z)} > 0$$
and 
\begin{equation}\label{eq:sigsig}
    \Ea{\sigma(\sigma(z))z}=0
\end{equation}
\end{assumption}
 The last two conditions ensure that all neurons in $W_1$ recover spherical projections of $W^\star$. In the absence of the above conditions, we still expect recovery of $W^\star$ but with anisotropy across neurons. Such an anisotropy is expected to complicate the subsequent analysis.

The next assumption, however, is only a technical one that arises only because we used $\{a^\star_{i}=1\}$. It could be relaxed by taking Gaussian values, or by performing more gradient steps on $W_2$, but this would complicate the proof. We discuss this in detail in App.~\ref{sec:app:main_proof}:
\begin{assumption}\label{ass:bad}
$\Eb{z \sim \mathcal{N}(0,1)}{g^\star(z)\text{He}_j(z)} = 0$ for $1 < j \leq k$
\end{assumption}

\end{itemize}

Under the above assumptions, our main result now establishes hierarchical learning for the target of the form \eqref{eq:3layer_target}
 by a three-layer network  $f^\star(\vec{x})$ by first recovering $W^\star$ through the first-layer $W_1$, next recovering $h^\star(\vec{x})$ through the second layer pre-activations $\vec h_2(\vec{x}) = W_2 \sigma(W_1 \vec x)$ and finally fitting $f^\star(\vec{x})$ upon training the last layer $\vec w_3$:
 \begin{theorem}[Informal]
\label{thm:main_theorem}
Let $f^\star(\vec{x})$ be as in Eq.~\eqref{eq:3layer_target} with $\epsilon_1 > 0$ and consider a three-layer model:
\begin{eqnarray}
    \hat{f}_\theta(\vec{x}) = \vec{w}_3^\top \sigma(b_2 + W_{2}\sigma(W_1\vec{x}+b_1)),
\end{eqnarray}
with $W_1 \in \mathbb{R}^{p_1 \times d}$,  $W_2 \in \mathbb{R}^{p_2 \times p_1}, w_3 \in \mathbb{R}^{p_3}$.
Under Ass.~\ref{ass:target}, for any $0 < \delta < \delta' < 1$, there exists a scale parameter $\epsilon > 0$ and time-steps $T_1 = \mathcal{O}(\mathrm{polylog} d)$ such that with batch-size $n_1 = \Theta(d^{\epsilon_1+1+\delta}), n_2 = \Theta(d^{k\epsilon_1+\delta})$ and $p_2=p_1 = \Theta(d^{k\epsilon_1+\delta'})$, the following holds with high probability as $d \rightarrow \infty$:
\begin{enumerate}[noitemsep,leftmargin=1em,wide=0pt]
%    \item $T_1$ steps of projected SGD on $W_1$ with neuron-wise projections on sphere of radius $\epsilon$ and step-size $\eta= \mathcal{O}(\sqrt{d^{k\epsilon_1}})$ on independent batches of size $n_1$ results in $W_1$ learning random projections along $W^\star_1,\cdots, W^\star_r$ upto error $o_d(1)$. Concretely, there exists a sequence of random matrices $Z \in \mathbb{R}^{p_1 \times d^{\epsilon_1}}$ with independent rows sampled uniformly on the unit sphere i.e $z_i \sim U(\mathcal{S}(1))$:
%    \begin{equation}
%    \label{eq:thmW}
%        \frac{1}{\epsilon} W_1 = Z (W^\star_1,\cdots, W^\star_r)^\top +o_d(1)
%    \end{equation}
\item $T_1$ steps of neuron-wise spherical SGD on correlation-loss applied to $W_1$ with step-size $\eta= O(\sqrt{p_2}\sqrt{d^{\epsilon_1}})$ on independent batches of size $n_1$ results in $W_1$ learning random projections along $W^\star$ upto error $o_d(1)$. Concretely, there exists a sequence of random matrices $Z \in \mathbb{R}^{p_1 \times d^{\epsilon_1}}$ with independent rows sampled uniformly on the unit sphere i.e $z_i \sim U(\mathcal{S}(1))$:
    \begin{equation}
    \label{eq:thmW}
        W_1 =  Z (W^\star) +o_d(1),
    \end{equation}
    where the $o_d(1)$ error is in operator norm.
    \item Subsequently, upon reinitializing $W_2$, using step size $\eta_2 = \Theta(\sqrt{p_2})$ and regularization strength $\lambda = \Theta(1)$ such that a single pre-conditioned gradient step on correlation loss using an independent size $n_2$ results in $\vec{h}_2(\vec{x})=W_2\sigma(W_1 \vec{x}) \in \mathbb{R}^{p_2}$ learning $h^\star$ upto error $o_d(1)$:
    \begin{equation}
    \label{eq:thmH}
    \begin{split}
        \vec h_2(\vec{x}) &= c h^\star(\vec{x})+o_d(1),
    \end{split}
    \end{equation}
    where $c \neq 0$ denotes a constant and the $o_d(1)$ error is w.r.t the metric induced by $L_2(\mathcal{N}(\vec{0}, I_d))$.
    
    \item Upon training $W_1,W_2$ as above, updating $W_3$ with ridge-regression on $\Theta(d^{\delta})$ samples results in $W_3^\top\sigma(W_2\sigma(W_1 \vec{x}))$ approximating $f^\star(\vec{x})$ upto error $o_d(1)$.
\end{enumerate}
\end{theorem}
The details of the initialization projections and pre-conditioning steps are provided in Appendix~\ref{app:pre-cond}. The condition $p_2=p_1$ is again solely to simplify the analysis and we expect the results to hold for $p_2 = \Theta(d^{\delta}), \Theta(d^{k\epsilon_1+\delta})$.

Since each row of $W^\star_j$ contains $d$ parameters, the complexity $n_1 \approx \Theta(d^{\epsilon_1+1})$ matches the total number of parameters in $W^\star_1, \cdots, W^\star_r$, and is therefore expected to be the information-theoretic scaling of the sample-complexity required for the (strong) recovery of $W^\star_1, \cdots, W^\star_r$. Similarly, the complexity $n_2 = \Theta(d^{k\epsilon_1})$ is the expected minimum sample-complexity required for the strong recovery of a degree-$k$ functions on a $d^{\epsilon_1}$-dim. space.




\paragraph{Sketch of the proof idea ---} We provide the  steps to prove the above result in Appendix \ref{sec:app:main_proof}, and present here a short sketch, highlighting the most important steps:

\noindent (i) \textbf{Composition of Hermite decompositions}: Building upon \cite{wang2023learning}, we utilize the asymptotic Gaussianity of $h^\star(\vec{x})$ to relate
    the Hermite decomposition of $f^\star(\vec{x})$ to the one of $h^\star(\vec{x})$.
    
  \noindent  (ii) \textbf{Low-dimensional dynamics for $W_1$}: Using the compositional Hermite-decomposition above, following \citep{BenArous2021, arnaboldi2023high,abbe2023sgd}, we show that the evolution of $W_1$ during the training of the first layer can be described through an effective dynamics on the overlaps $W_1 (W^\star)^\top$. Unlike, however, their single/multi-index analysis, 
    the diverging dimensionality of $W^\star, W_1$ that appear in our approach, as well as the later use of the updated weights $W_2$, require a careful control over the error terms. Concretely, we show that the components of $W_1$ along $W^\star$, as well as the error terms, maintain isotropy and hypercontractivity through the dynamics.

\noindent (iii) \textbf{Function-space decomposition of the $2^{\rm nd}$-layer pre-activations}: 
    A Gradient steps on $W_2$ extract statistics in features-space $\sigma(W_1 \vec{x})$. Similar to \citep{wang2023learning,nichani2024provable,fu2024learning}, we show that these statistics appear in the updates for the pre-activations $h_2(\vec{x})$ as projections of a perturbed version of $f^\star$ on the conjugate Kernel defined by the first-layer:
    \begin{equation}
        \Delta \vec h_2(\vec{x}) \approx c \sigma(W_1 \vec{x})^\top(\frac{1}{n}\sigma(W_1 X^\top)^\top (\sigma(W_1 X)^\top)^{-1}\sigma(W_1 X)f^\star(X),
\end{equation}\label{eq:gram-matrix projection}
    where $X \in \mathbb{R}^{n \times d}$ denotes the batch of data utilized in a gradient step and $c>0$ denotes a constant.

\begin{wrapfigure}{rt}{0.5\textwidth}
\vspace{-0.8cm}
{\includegraphics[width=0.95\linewidth]{figs/ModNewShiftedGenErrManyd2x2LogTrue.pdf}}
\caption{\textbf{Numerical simulation:}  Generalization error versus $\kappa = \frac{\log{n}}{\log{d}}$ for  $f^\star(\vec{x}) =  \tanh\left(
3 {\vec{a}^{\star^\top} \, 
P_3\left(W^\star \vec{x}\right) }/{\sqrt{d^{\varepsilon_1=1/2}}} \right)$ with different training protocols: \textbf{(Top)} kernel ridge regression (orange points) only beat the random performance (purple solid line) starting from $n=d + (d-1)d/2$, and is limited to quadratic approximation (orange line). $2$-layer net (green points), instead, starts to learn at $\kappa=1.5$ (black vertical dashed line) and can beat the quadratic limit (asymptotics is given by the green line). 3-layer net trained with layerwise training (blue markers) not only learn at $\kappa=1.5$ (vertical line). but also surpasses the best possible 2-layer net error, illustrating the advantage of depth;
\textbf{(Bottom)} comparison of layerwise training (blue) with joint training (red) of all the layers of a 3-layer net with standard backpropagation.}
    \label{fig:gen_error_fig1} 
\vspace{-2em}
\end{wrapfigure}


 \noindent  (iv) \textbf{Concentration of the sample-covariance matrix}: 
   In light of $(iii)$, the recovery of features in $h_2(\vec{x})$ depends on the feature matrix $\sigma(W_1 X)$ being able to approximate and span the relevant functional subspace, which requires both sufficiently many samples and sufficiently many neurons. Building on the matrix-concentration analysis of \cite{mei2022generalization}, we show that the projections onto the $\sigma(W_1 X)$ up to degree-$k$ functions can be well approximated as long as $n,p_1 = \Theta(d^{k\epsilon_1+\delta})$. Low-degree eigenfunctions concentrate faster since they span lower-dimensional subspaces.

\paragraph{From SIGHT to MIGHT  ---}  While we expect similar results to hold in generality, the theorem is only fully proven for the class of target in eq.~\eqref{eq:3layer_target}. While a complete proof for MIGHT is a difficult task, we discuss additional ($r>1$ and $\ell >1$)  results in this and subsequent paragraphs.

We first remark that part $(i)$ of Theorem \ref{thm:main_theorem}, that is the weak-recovery of $W^\star$, holds for arbitrary~$r$, and thus for MIGHT functions $f^\star$ (and not only SIGHT ones). Establishing rigorously part $(ii)$ for $r>1$ is difficult and involves technical hurdles relating to the control in the Gaussian approximation of $h^\star$. These difficulties are not new, and similar to the ones on standard single and multi-index problems, that we describe in Appendix \ref{app:multiple_layers}.

MIGHT functions are interesting in illustrating the role of the information exponent in Ass.~\ref{ass:target}. It is easy to design counterexamples, for instance, the parity problem with $y={\rm sign}(h^\star_1 h^
\star_2 h^\star_3)$  violates Ass.~\ref{ass:target}. We illustrate some of these numerically in Appendix~\ref{sec:numerics} (See Fig.~\ref{fig:parity_stair_comparison}). We believe, however, that with reusing batches, the information exponent could be replaced with the much permissive generative one \cite{dandi2024benefits,lee2024neural,arnaboldi2024repetita}.  SIGHT and MIGHT functions are indeed generalizations of the multi-index functions, and the properties of the latter such as information \cite{BenArous2021} and generative exponents \cite{damian2024computational}, and the notion of trivial, easy and hard directions \cite{troiani2024fundamental}) should translate to the former.  

\paragraph{From MIGHT to Deeper MIGHT  ---} Depth introduces more difficulties for rigorous studies, but our mathematical analysis can be extended for more general constructions. By the tree-like hierarchical construction of features (Eq. \eqref{eq:non_linear_feature_def}) for general depth, the components ${\bf h}^\star_{\ell}(\vec{x})$ remain independent and asymptotically Gaussian. Generalizing Thm.~\ref{thm:main_theorem} for $L\ge3$ in its full-generality requires however not only an extension of part $(ii)$ of Thm.~\ref{thm:main_theorem} to $r>1$, but also a careful control over the non-asymptotic rates for the tails of $\vec h^\star_{\ell}(\vec{x})$ and the associated kernels. 

We instead prove a weaker, but useful, result corresponding to the hierarchical weak recovery of a single non-linear feature at a general level of depth: 
\begin{theorem}[Informal]\label{thm:multi-layer}
Let $L \in \mathbb{N}$ be arbitrary and let $f^\star(\vec{x})$ denote a target as in Eq.~\eqref{eq:target_def_deep} with $r=1$. Consider a student model of the form 
$\hat{f}_\theta(\vec{x}) = \vec{w}_L^\top \sigma(W_{L-1} \sigma( W h^\star_{L-1}(\vec{x})))$ with $W \in \mathbb{R}^{p \times d^{\epsilon_{L-2}
}}$ having rows independently sampled as $w_i \sim U(\mathcal{S}_{d^{\epsilon_{L-2}}}(1))$
i.e a model with the ${(L-2)}_{th}$ layer having perfectly recovered $h^\star_{L-1}(\vec{x})$.
Then, with $\eta =\Theta(\sqrt{p_{L-1}})$, after a single step of pre-conditioned SGD on $W_{L-1}$ with batch-size $\Theta(d^{{k\epsilon_{\ell-2}}+\delta})$, the pre-activations $ h_{L-1}(\vec{x}) \coloneqq W_{L-1}\sigma( W h^\star_{L-1}(\vec{x}))$ satisfy, for a constant $c>0$:
\begin{equation}
   h_{L-1}(\vec{x}) = c h^\star_{L}(\vec{x})\vec{w}_L + o_d(1),
\end{equation}
\label{th:multi}
\end{theorem}
\vspace{-0.3 cm}


Note that the above is not a direct consequence of the central limit theorem applied to $h^\star_{L-1}(\vec{x})$ since the update to $W_{L-1}$ involves matrices with diverging dimensions. Instead, we exploit the exact independence of components of $h^\star_{L-1}(\vec{x})$ and the hyper-contractivity of the Gaussian measure to relate the sample covariance of features $\sigma(W h^\star_{L-2}(\vec{x}))$ to those of the equivalent Gaussian inputs. We refer to App. \ref{app:multiple_layers} for the full proof.



\section{Numerical Illustrations}
While our theorems provide a rigorous control of learning with a particular, well-conditioned, training procedure, we want here to move as far as possible from the theoretical setting, and instead look at realistic training routines with mini-batch updates, finite (and rather low) dimensional examples, using multi-pass (instead of a single one) for the second layer, etc. to show that the essence of the phenomenon obeys a similar picture to the one predicted by our theory. 
\begin{wrapfigure}{rt}{0.5\textwidth}
\vspace{-0.3cm}
 \centering
 {\includegraphics[width=0.9\linewidth]{figs/CosSimAllTogether.pdf}}
 \caption{\textbf{Visualizing Feature Learning:} The overlaps $M_h, M_W$ (Def.~\ref{def:sufficient_stat}), respectively on the top and bottom panel, as a function of the sample complexity $\kappa = \frac{\log n}{\log d}$ for three-layer networks trained with the protocol described in Theorem~\ref{thm:main_theorem} (blue circles) and standard backpropagation (red squares). Following Theorem~\ref{thm:main_theorem}, the behavior sharply changes around $\kappa = 1.5$ (vertical dashed line) where feature learning in both layers arises (same setting as in Fig.~\ref{fig:gen_error_fig1}).}
     \label{fig:theorem_illustration}
     \vspace{-1.5cm}
 \end{wrapfigure}

For concreteness, we consider $f^\star(\vec{x}) =  \tanh\left(
 \frac{3\vec{a}^{\star^\top} \, 
P_3\left(W^\star \vec{x}\right) }{\sqrt{d^{\varepsilon_1=1/2}}} \right)$, a similar example as discussed in Section~\ref{sec:main_theorems} and with, again, a polynomial $P_{k=3}$ with second and third Hermite polynomials. We show simulations in Fig.~\ref{fig:gen_error_fig1} (and refer to Appendix~\ref{sec:numerics} for details) and discuss here the more salient observations: 


%
\noindent \textbf{(i)} First we compare the performance of kernel methods with those of a two-layer network. On the one hand, the former method should be able to fit the quadratic part of the target function as soon as $n=O(d^2)$ \citep{mei_generalization_2022}. This is well observed, with a  double descent peak when the number of data hits the number of features in a quadratic kernel, i.e. $n_{\rm peak}=d(d-1)/2 + d + 1$. On the other hand, two-layer networks are capable of recovering  $W^\star$ when $n=O(d^{1.5})$, therefore improving the test performance to quadratic and cubic fit when $\kappa \ge 1.5$.



\noindent \textbf{(ii)} We then train a three-layer network, with a {\bf layerwise} approach resembling the procedure in Thm~\ref{thm:main_theorem}, where we train every layer in order, (first $W_1$, then $W_2$, etc.). We do not, however, follow the restrictions of the theorem and just perform a standard gradient descent (no reinitializing, no projection, using minibatch, etc.). Not only does the method starts to learn when $n>d^{1.5}$ but {\bf it outperforms the 2-layer baseline} in agreement with Thm.~\ref{thm:main_theorem}. 
%This is a clear illustration of the advantage of $3-$ layers networks  over $2-$layers ones.


\noindent \textbf{(iii)} Lastly, we consider the standard training procedure ---refered to as {\bf joint training}-- with   backpropagation through the network with mini-batch gradient descent. The routine performs similarly to the layerwise approach, illustrating the generality of the dimensionality reduction beyond the assumptions of Thm.~\ref{thm:main_theorem}.





\paragraph{Visualizing Feature Learning ---} 
We now show that this enhanced generalization performance is due to feature learning. Indeed, the key result in Thm~\ref{thm:main_theorem} refers to the ability of three-layer networks to perform hierarchically dimensionality reduction through feature learning. To probe the quality of the learned representations, we shall introduce the ``overlaps'' (or order parameters).  
\begin{definition}
\label{def:sufficient_stat}
The order parameters for $3-$layer networks are the matrices $M_W \in \mathbb{R}^{p_1 \times rd^{\varepsilon_1}}$ and $M_h \in \mathbb{R}^{p_2 \times r}$ (with $\vec z \sim \mathcal{N}(0,I_d)$)
\begin{align}
\label{eq:sufficient_stat}
    M_W &= \frac{W_1W^\star}{\norm{W_1}_2}, \\
    M_h &= \frac{\mathbb{E}[\vec h(\vec{z}) \vec h^\star(\vec z)]}{\sqrt{\mathbb{E}[\vec h(\vec{z})^2]}}. \,\, 
\end{align}
\end{definition}
The behavior of these quantities as a function of the sample complexity $\kappa$ is portrayed in Fig.~\ref{fig:theorem_illustration}. Since we do not follow the strong prescription of Thm.~\ref{thm:main_theorem}, and are working with a low dimensional example, we do not expect a sharp $0/1$ transition as in the idealized scenario, but instead, the components along $W^\star$ to occupy a $\Theta(1)$ fraction (but not full) of the norm of $W_1$.  This is well obeyed (Figure \ref{fig:theorem_illustration}) and the predicted crossover at $\kappa=1.5$ is clearly observed in both layerwise and joint training.

 
\paragraph{Conclusion---}
We introduced a theoretical framework for understanding the computational advantages of deep neural networks over shallow models when learning high-dimensional hierarchical functions, where depth facilitates a progressive reduction of effective dimensionality. We hope our paper will spark interest in these directions.

\subsection{Acknowledgement}
We thank Alex Damian, Denny Wu, Zhichao Wang, Bruno Loureiro, Yue Lu, Jason Lee, Theodor Misiakiewicz, Eshaan Nichani for insightful discussions. We acknowledge funding from the Swiss National Science Foundation grants SNFS SMArtNet (grant number 212049),  OperaGOST (grant number 200021 200390) and DSGIANGO.

\bibliographystyle{plainnat}
\bibliography{refs}

\appendix

\newpage 
% \section{SIGHT, MIGHT and Depth}\label{sec:app:plots}
% For clarity, we illustrate the deep version of SIGHT and MIGHT functions (defined in Section \ref{main:def:targer}) in Fig.~\ref{fig:app:deep_targets}, where the tree structure of the deep version of these target functions is apparent.


\section{Proofs of the main Results}\label{sec:app:main_proof}

\subsection{Proof Sketch}

We prove each of the three parts of Theorem \ref{thm:main_theorem} in succession. We outline the proof for each of these parts below:

\textbf{Part $(i)$}:
\begin{enumerate}
    \item The asymptotic composition of Hermite polynomials allows us to decompose the Hermite decomposition of $f^\star(\vec{x})$ along Hermite polynomials applied to $W^\star \vec{x}$.
    \item The leading order term in the Hermite-decomposition  $f^\star(\vec{x})$ lies along $\text{He}_2(W^\star \vec{x})$, which contributes a linear drift to the dynamics of each neuron in $W_1$, with the direction of the drift for neuron $i$ given by $u^\star_i = W^\star (W^\star)^\top w^0_i$, i.e the initial direction of $w_i$'s projection onto $W^\star$.
    \item We show that the neuron $w^{(t)}$ remains approximately isotropic w.r.t the rows of $W^\star$
    \item Under the above isotropy and due to $d^{\epsilon_1} >> 1$, we show that the above linear term dominates throughput the weak-recovery and subsequent states of the dynamics.
    \item As a consequence, each neuron in $W_1$ evolves primarily along $u^\star_i$, with the noise controlled through the choice of batch-size. A stopping-time based analsyis  
    then yields $w^{(t)}_i \rightarrow u^\star_i$.
    \item For subsquent use in part $(ii)$ however, we require finer control over the distribution of $w^{(t)}_i$ and its residual terms. 
    \item Inductively, we show that the distribution of $w^{(t)}_i$ conditioned on a suitable stopping-time is approximately $U(S_1(d))$ and maintains hypercontractivity.
\end{enumerate}

\textbf{Part $(ii)$}:
\begin{enumerate}
    \item Through results established in Part $(i)$, we show that distribution of the updated weights $W_1$ approximately maintains hypercontractivity for the eigenfunctions of the random-features Kernel associated to the features $\sigma(XW_1^\top)$. This ensures the concentration of the associated sample covariances. 
    \item Upon establishing concentration and spherical approximation along the subspace corresponding to $W^\star$, through an analysis similar to \cite{mei2022generalization}, we show that the feature matrix $Z=\sigma(XW^\top)$ contains $\Theta(d^k)$ spikes with diverging eigenvalues and an isotropic bulk with eigenvalues $O(1)$.
    \item  Under $n,p_2 \gg \Theta(d^{k\epsilon_1})$, we show that these spikes suffice for the pre-conditioned update 
    $$-\eta (\frac{1}{n}\sigma(W_1 X^\top)^\top (\sigma(W_1 X)^\top)^{-1} \nabla_{W_2} \mathcal{L},$$ to approximate $f^\star(x)$ upto degree $k$-components. As a result, we obtain the recovery of $h^\star(\vec{x})$ through $h^2(\vec{x})$
\end{enumerate}
\textbf{Part $(iii)$}: Finally, fitting the target $f^\star(x)$ upon training $\vec{w}_3$ follows through universality of the random features Kernel associated with $\sigma(\cdot)$ and perturbation of the Kernel regression operators.
\subsection{Preliminaries}


% Next, consider a random-features Kernel with $w_i \sim U(\mathcal{S}(\sqrt{d}))$. We show that assuming $\sigma(0)=0$, $K(\vec{x}, \vec{x}')$ can be approximately diagonalized along $\norm{\vec{x}}\times \phi_{l,k}(\vec{x})$

\subsubsection{Stochastic Domination}

Throughout the analysis, much of our probabilistic error bounds will take the following form, which are standard for functions of random variables with finite Orlicz-norm such as sub-Gaussian/sub-Exponential random variables:
\begin{equation}
     \Pr{\left[\abs{X}_d \geq C \frac{(\log d)^k}{d^m} \right]} \leq e^{-c (\log d)^m} , 
\end{equation}
for some constants $m >1, k> 0, c > 0$. A slightly weaker form of the bound takes the form: 
\begin{equation}
     \Pr{\left[\abs{X}_d \geq C \frac{1}{d^{m-\delta}}\right]} \leq  \frac{1}{d^{k}}, 
\end{equation}
for any $\delta > 0$ and $k \in \mathbb{N}$.
To concisely represent such bounds, we use the following notation:
\begin{definition}\label{def:stoch-dom}[Stochastic dominance \citep{lu2022equivalence}]
    We say that a sequence of real or complex random variables $X_d$ in a normed space is stochastically dominated by another sequence $Y_d$ in the same space if for all $\epsilon > 0$ and $k$, the following holds for large enough $d$:
    \begin{equation}\label{eq:stoch_dom}
        \Pr[\norm{X}_d > d^{\epsilon_1}{\norm{Y}}_d]  \leq d^{-k}.
\end{equation}

We denote the above relation through the following notation:
\begin{equation}
    X = \mathcal{O}_{\prec}(Y).
\end{equation}
Through a union bound, we obtain that $\mathcal{O}_{\prec}$ is closed under addition, multiplication, i.e $X_1 = O_{\prec}(Y_1)$ and $X_2 = O_{\prec}(Y_2)$ imply that:
\begin{equation}
X_1+X_2 = O_{\prec}(Y_1+Y_2), 
\end{equation}
and:
\begin{equation}
X_1X_2 = O_{\prec}(Y_1Y_2), 
\end{equation}
Furthermore, due to the flexibility of setting an arbitrarily large $k$ in Eq.~\eqref{eq:stoch_dom}, we observe that stochastic dominance is closed under unions of polynomially many events in $d$. 


We will often exploit this while taking unions over $p=\mathcal{O}(d)$ neurons and $n=\mathcal{O}(d)$ samples. Furthermore, $\prec$ absorbs polylogarithmic factors i.e:
\begin{equation}
     X = \mathcal{O}_{\prec}(Y) \implies X = \mathcal{O}_{\prec}(\polylog d Y),
\end{equation}
subsumes exponential tail bounds of the form:
\begin{equation}
    \Pr[X_d > t Y_d]  \leq e^{-t^\alpha},
\end{equation}
for some $\alpha >0$, as well as polynomial tails of arbitrarily large degree:
\begin{equation}
     \Pr[X_d > t Y_d]  \leq \frac{C_k}{t^k},
\end{equation}
for some sequence of constants $C_k$ dependent on $k$.
\end{definition}

The above bounds directly translate to the following control over moments:
\begin{proposition}
    Let $X_d,Y_d$ denote two sequences of random variables with:
    \begin{equation}
        X = \mathcal{O}_{\prec}(Y),
    \end{equation}
    then for any $q \in \mathbb{N}$ and $\delta > 0$:
    \begin{equation}
        \Ea{\norm{X}^p}^{1/p} \leq d^{\delta} \Ea{\norm{Y}^p}^{1/p}
    \end{equation}
\end{proposition}

\begin{proposition}
    The above proposition follows directly through the following decomposition:
    \begin{equation}
        \Ea{\norm{Y}^p}^{1/p} = \Ea{\norm{Y}^p \mathbf{1}_{\norm{X} \leq d^{\delta} \norm{Y}}}^{1/p}+ \Ea{\norm{Y}^p \mathbf{1}_{\norm{X} > d^{\delta} \norm{Y}}}^{1/p},
    \end{equation}
    where $\mathbf{1}$ denotes the indicator function. Using the property $\mathbb{E}[Z]=\int_{s=0}^\infty \Pr[Z>s]ds$, the second term is bounded by $\frac{1}{d^{k}}$ for any $k$ and large enough $d$.
\end{proposition}

\textbf{Asymptotic notation:}
In light of the above proposition, throughout the subsequent sections, we use the notation $\tilde{\mathcal{O}}$ to denote deterministic asymptotic bounds upto factors $d^\delta$ for arbitrarily small $\delta > 0$ i.e:
\begin{equation}
    f(d) = \tilde{\mathcal{O}}(g(d)),
\end{equation}
if for any $\delta > 0$, $f(d) \leq d^{\delta} g(d)$ for large enough $d$

Through a standard application of the Lindeberg exchange technique \cite{Chatterjee_2006,van2014probability}, we further have the following useful estimate:

\begin{lemma}[Non-asymptotic CLT -bound]\label{lem:lind} 
Let $X_1, \dots, X_n \in \mathbb{R}$ be $n$ i.i.d random variables satisfying $X_i = O_{\prec}(1)$. Then, for any  function pseudo-lipschitz function $q:\mathbb{R} \rightarrow \mathbb{R}$ of order $k$ and any $\delta > 0$:
\begin{equation}
    \abs{\Ea{q(\frac{1}{\sqrt{d}}(\sum_{i=1}^d X_i)}-\Eb{z\sim \mathcal{N}(0,1)}{q(z}} \leq c_1\sqrt{d}\abs{\Ea{X}} + c_2 \abs{\Ea{X}^2_1-1}+ \frac{c_3}{d^{3/2-\delta}}\Ea{\abs{X}^3},
\end{equation}
where $c_1,c_2$ denote constants dependent only on $q$.
\end{lemma}

\subsubsection{Orthogonal Polynomials and Spherical Harmonics}\label{sec:spher_harm}

\textbf{Hermite Polynomials}:

\begin{definition}[Hermite decomposition]\label{def:hermite}
    Let $f: \dR^m \to \dR$ be a function that is square integrable w.r.t the Gaussian measure. There exists a family of tensors $(C_j(f))_{j\in\dN}$ such that $C_j(f)$ is of order $j$ and for all $\vec{x} \in \dR^m$,
    \begin{equation}
        f(\vec{x}) = \sum_{j \in \dN} \langle C_j(f), \cH_j(\vec{x}) \rangle
\label{eq:hermite_expansion}
    \end{equation}
    where $\cH_j(\vec{x})$ is the $j$-th order Hermite tensor \citep{grad_1949_note}.
\end{definition}

\textbf{Gegenbauer and Associated Laguerre polynomials
}
Let $\vec{w} \sim U(\mathcal{S}^{d-1}(\sqrt{d})$ denote a random variable distributed uniformly on the sphere in $\mathbb{R}^d$ of radiues $\sqrt{d}$.  Let $\mu_d$ denote the associated push-forward measure of the projection $\sqrt{d}\langle \vec{w}, \vec{e}_1 \rangle$. The Gegenbauer polynomials $Q^d_\ell(\cdot)$ \cite{ghorbani2020neural} for $\ell \in \mathbb{N}$ form an orthonormal basis w.r.t $L^2(\mu_d)$ with $Q^d_\ell(\cdot)$ being a polynomial of degree $\ell$. Therefore, for any $f \in L^2(\mu_d)$ and $v \in \mathbb{R}^d$ with $\norm{v}=1$, the following decomposition exists:
\begin{equation}
    f(\sqrt{d} \langle\vec{v},\vec{w}\rangle) = \sum_{k=0}^\infty \nu_{d,k} Q^d_k(\sqrt{d} \langle\vec{v},\vec{w}\rangle)
\end{equation}


Next, suppose that $\vec{x} \sim \mathcal{N}(\mathbf{0}, \mathbf{I}_d)$. Let $\tau_d$ denote the associated pushforward measure of $\norm{x}^2$. Then, the associated Laguerre polynomials $l^d_k(\cdot)$ form an orthonormal basis w.r.t $\tau_d$ \citep{arfken2011mathematical}.


\textbf{Spherical Harmonics}
Recall that any inner-product Kernel can be diagonalized w.r.t $L_2(U(\mathcal{S}^{d-1}(\sqrt{d})))$ along the basis of spherical Harmonics $\{Y_{\ell, k}\}_{\ell \in [B(d,k)], k \in \N} \}$, where $B(d,k)$ denotes the number of spherical harmonics of degree $k$, satisfying $B(d,k) = \Theta(d^k)$:
\begin{equation}
    K(\vec{x}, \vec{x}') = \sum_{k=0}^\infty \lambda_k \sum_{l =1}^{n_k} Y_{l,k}(\vec{x})Y_{l,k}(\vec{x}'),
\end{equation}
where $\lambda_k$ denotes the eigenvalue of $K$ w.r.t the $k$-degree spherical harmonics $Y_{l,k}(\vec{x})$. \citep{ghorbani2021linearized}.

The Spherical Harmonics are related to the Gegenbauer polynomials through the following identity:

\begin{proposition}\label{prop:gegen_harm}
For any $\vec{w}_1, \vec{w}_2 \sim U(\mathcal{S}^{d-1}(\sqrt{d}))$:
\begin{equation}
    Q^d_k(\vec{w}_1, \vec{w}_2) = \frac{1}{B(d,k)}  \sum_{\ell=1}^{B(d,k)} Y_{\ell, k}(\vec{w}_1)Y_{\ell, k}(\vec{w}_2)
\end{equation}
\end{proposition}

We next recall that the Gaussian measure $\mathcal{N}(\vec{0}, \mathbf{I}_d)$ admits the following tensor product decomposition:

\begin{equation}
    \mathcal{N}(\vec{0}, \mathbf{I}_d) = \chi^2(\norm{x}^2) \otimes U(\mathcal{S}^{d-1}(\sqrt{d})),
\end{equation}
where $U(\mathcal{S}^{d-1}(\sqrt{d}))$ denotes the uniform measure on sphere of radius $\sqrt{d}$

The above tensor product decomposition naturally relates the Hermite orthonormal basis w.r.t the Gaussian measure against the product of radial functions and Gegenbauer polynomials. In particular, we have the following relation:
\begin{proposition}
    For any $k \in \mathbb{N}$, the $k_{th}$-degree Hermite polynomial 
    lies in the subspace spanned by functions of the form:
    \begin{equation}
f(\frac{\norm{x}^2-1}{\sqrt{d^{\epsilon_1}}})Y_{\ell,j}(\sqrt{d}\vec{x}/\norm{x}),
    \end{equation}
with $0 < j \leq k$.
\end{proposition}

\begin{proof}
    Recall that $Y_{\ell,j}(\sqrt{d}\vec{x}/\norm{x})$ are homogenous polynomials of degree $j$. Upon restriction to the sphere of radius $\norm{x}$, $\text{He}_k(\vec{x})$ is a polynomial of degree at-most $k$. Therefore, by Fubini's theorem, we obtain:
    \begin{equation}
        \Ea{f(\frac{\norm{x}^2-1}{\sqrt{d^{\epsilon_1}}})Y_{\ell,j}(\sqrt{d}\vec{x}/\norm{x})\text{He}_k(\vec{x})}=0,
    \end{equation}
for $j > k$.
\end{proof}
\begin{proposition}
    For any $k>2$ and polynomial $q(x)$:
    \begin{equation}    \Ea{\frac{\frac{1}{\sqrt{d^{\epsilon_1}}}n \sum_{i=1}^{\sqrt{d^{\epsilon_1}}}\langle w^\star_i, \vec{x}\rangle^2-1}{\sqrt{d^{\epsilon_1}}}  q(\frac{1}{\sqrt{d^{\epsilon_1}}}n \sum_{i=1}^{\sqrt{d^{\epsilon_1}}}\text{He}_k(\langle w^\star_i, \vec{x}\rangle))} = \mathcal{O}(\frac{1}{\sqrt{d^{\epsilon_1}}})
    \end{equation}
\end{proposition}
\begin{proof}
    The above is a direct consequence of Lemma \ref{lem:lind} applied to the random variables $(\text{He}_2(\langle w^\star_i, \vec{x}\rangle), \text{He}_k(\langle w^\star_i, \vec{x}\rangle)) \in \mathbb{R}^2$, whose higher-moments are bounded by Gaussian hypercontractivity (Lemma \ref{lem:hyper}).
\end{proof}

We utilize Gegenbauer polynomials and spherical Harmonics primarily due to the absence of results on eigenvectors of inner-product Kernel matrices under polynomial scalings. This is also the primary bottleneck towards the extension of our theory to multiple layers. Essentially, our analysis relies on showing the concentration of the sample-covariance matrix to the population covariance matrix along the degree-$k$ components. 

\subsubsection{Spectral Norm of a tensor}
\begin{definition}
    For a symmetric positive-definite tensor $T \in \mathbb{R}^{d \otimes k}$ of order $k$, we define the spectral norm of $T$ as follows:
    \begin{equation}
        \norm{T}_2 = \sup_{x \in \mathbb{R}^d, \norm{x}=1} \abs{\langle x^{\otimes k}, T\rangle}
    \end{equation}
\end{definition}

\subsubsection{Hermite-tensors and Gaussian-inner Products}

We denote by $\text{He}_k$ for $k \in \mathbb{N}$ the normalized Hermite-polynomials forming an orthonormal basis w.r.t $L^2(\gamma)$. For any $f \in L^2(\gamma)$, we have:
\begin{equation}
    f(z) = \sum_{k=0}^\infty \mu_k \text{He}_k(z). 
\end{equation}

The Hermite tensors result in the following generalization of the above decomposition:
\begin{proposition}\label{prop:Hermite-tens}
Let $\gamma_m$ denote the $m$-dimensional Gaussian measure.
For any $f, g: \mathbb{R}^m \rightarrow \mathbb{R}$
    $\in  \ell^2(\dR^m, \gamma_m)$, let $C_k(f)$ denote the $k_{th}$-order Hermite-tensor, defined as:
    \begin{equation}
       C_k(f) \coloneqq \Eb{z \sim \gamma_m}{f (\vec{z})\operatorname{He}_k(\vec{z})} = \Eb{z \sim \gamma_m}{\nabla^k f(z)},
    \end{equation}
where $\operatorname{He}_k(\vec{z})$ denotes the $k_{th}$-order Hermite tensor on $\mathbb{R}^m$.
Then:
\begin{equation}\label{eq:app:hermite_scalar}
    \langle f, g \rangle_\gamma = \sum_{k \in \dN} \langle C_k(f), C_k(g) \rangle.
\end{equation}
\end{proposition}

\subsubsection{Compact Self-Adjoint Operators}

We collect here the following well-known properties of bounded linear operators on a Hilbert space $L^2(\mu)$ \citep{axler2020measure}:
\begin{proposition}
    Let $A:L^2(\mu,\Omega) \rightarrow L^2(\mu,\Omega)$ denote a bounded-linear operator on a hilbert space $L^2(\mu)$. Then:
    \begin{enumerate}
        \item If $A$ is compact, self-adjoint then $A$ can be diagonalized along a countable-basis of eigenvectors.
        \item Suppose that $\mu$ is $\sigma$-finite, then any integral operator $I(x,y): \omega \times \omega \rightarrow \mathbb{R}$ with $\norm{I(x,y)}_{L^2(\mu) \times L^2(\mu)}< \infty$ is compact
        \item For a symmetric integral operator $\norm{I(x,y)}_{L^2(\mu) \times L^2(\mu)}< \infty$:
        \begin{equation}
            \int I(x,y) d\mu(\omega \times \omega) = \sum_{k} \lambda_k,
        \end{equation}
        where $\{\lambda_k\}$ denote the eigenvalues associated with $I(\cdot, \cdot)$.
    \end{enumerate}
\end{proposition}

\subsubsection{Concentration in Orlicz-spaces}

\begin{definition}
   For any $\alpha \in \dR$, define $\psi_\alpha(x) = e^{x^\alpha} - 1$. The \emph{Orlicz norm} for   a real random variable $X$; $\norm{X}_{\psi_\alpha}$ is defined as
     \begin{equation}
     \norm{X}_{\psi_\alpha} = \inf \left\{t > 0\::\: \dE\left[ \psi_\alpha\left(\frac{|X|}{t} \right)\right] \leq 1\right\}
     \end{equation}
 \end{definition}

Random variables exhibiting suitable bounds on orlicz norms of finite-order exhibit the following concentration inequality:

\begin{theorem}[Theorem 6.2.3 in \cite{ledoux2013probability}] \label{thm:app:orlicz_sum}
     Let $X_1, \dots, X_n$ be $n$ independent random variables with zero mean and second moment $\dE X_i^2 = \sigma_i^2$. Then,
     \begin{equation}
         \norm{\sum_{i=1}^n X_i}_{\psi_\alpha} \leq K_\alpha \log(n)^{1/\alpha} \left(\sqrt{\sum_{i=1}^n \sigma_i^2} + \max_{i}\norm{X_i}_{\psi_\alpha} \right)
     \end{equation}
 \end{theorem}
 
\subsection{Useful Preliminary Results}

A central result underlying our analysis for part $(ii)$ of Theorem \ref{thm:main_theorem}, based on \cite{mei_generalization_2022} is the following matrix-concentration bound for matrices with independent heavy-tailed rows:

\begin{lemma}[Theorem 5.48 in \cite{vershynin2010introduction}]\label{lem:mat_conc}
    Let $A \in \mathbb{R}^{p \times n}$ be a random matrix with independent rows $a_i \in \mathbb{R}^n$ with covariance $\Ea{a_ia_i^\top}= \Sigma_a$ and $\Ea{\max_{i \leq p} \norm{a}^2_i} \leq m(d)$. Then:
    \begin{equation}\label{eq:mat_con_bound}
        \Ea{\norm{\frac{1}{n}A^\top A-\Sigma_a}} \leq \operatorname{max}(\norm{\Sigma_a} \delta, \delta^2),
    \end{equation}
where $\delta = C\sqrt{m \frac{\log (\min(n,p)}{p}}$.
\end{lemma}
\begin{lemma}[Weyl's inequality]
    For any $A,B \in \mathbb{R}^{m \times n}$, for all $i \in \mathbb{N}$ with $i \leq \min(m,n)$:
    \begin{equation}
        \abs{\sigma_i(A)-\sigma_i(B)} \leq \norm{A-B}
    \end{equation}
\end{lemma}

\begin{lemma}[Resolvent Identity]\label{lem:diff_inv}
Let, $A, B \in \R^{p \times p}$ be two invertible matrices, then: 
   \begin{equation}
       A^{-1}-B^{-1} = A^{-1}(B-A)B^{-1}.
   \end{equation}
\end{lemma}


Our next central tool that will be utilized frequently throughout our analysis is the hypercontractivity w.r.t the Gaussian measure:

\begin{lemma}[Gaussian Hypercontractivity, Proposition 5.48. in \cite{aubrun2017alice}]\label{lem:hyper}
For any polynomial $q:\mathbb{R}^d \rightarrow \mathbb{R}$ of degree $k$ and any $p \in \mathbb{N}, p \geq 2$:
    \begin{equation}
        \norm{q(z)}_{p,\gamma^d} \leq (p-1)^k \norm{q(z)}_{2,\gamma^d},
    \end{equation}
where $\gamma^d$ denotes the standard Gaussian measure on $\mathbb{R}^d$ and $\norm{q(z)}_{p,\gamma}$ denotes the $p$-norm:
\begin{equation}
   \norm{q(z)}_{p,\gamma} \coloneqq \Eb{z \sim \gamma}{\abs{q(z)}^p}^{\frac{1}{p}}
\end{equation}
\end{lemma}

\begin{proposition}\label{lem:stoch-dom-mean}
Let $z \sim \mathcal{N}(0,\mathbf{I}_d)$ denote a $d$-dimensional Gaussian vectors. Suppose that $X_1,\cdots, X_k$ denote i.i.d random variables obtained by applying a fixed polynomial of degree $p \in \mathbb{N}$. Then:
\begin{equation}
    \frac{1}{\sqrt{k}}(\sum_{i=1}^k X_i) = \mathcal{O}_\prec(\sqrt{k\abs{\Ea{X}^2}+k(k-1)\abs{\Ea{X}}^2}).
\end{equation}
\end{proposition}
\begin{proof}
Since $q(z) = \frac{1}{\sqrt{k}}(\sum_{i=1}^k X_i)$ is a polynomial in $z$ with finite degree $p$, Lemma \ref{lem:hyper} implies that its higher-order moments are bounded as $\norm{q(x)}_p \leq C_p \norm{q(x)}_2$. The result then follows by noting that:
\begin{equation}
\Ea{q(z)^2}^{1/2}=\sqrt{\abs{k\Ea{X}^2}+k(k-1)\abs{\Ea{X}}^2}
\end{equation}
\end{proof}


\begin{lemma}[Discrete Gronwall]\label{lem:gronwall}

Let $a_t, b_t, c^t_1, c^t_2$ be  non-negative sequences satisfying:
\begin{equation}
    a_{t+1} \geq a_t+ c^t_1 a_t + c^t_2 b_t,
\end{equation}

then, for any $t \in \mathbb{N}$:
\begin{equation}
    a_{t+1} \geq \prod_{s=1}^t (1+c^s_1) a_0 + \sum_{j=1}^{t-1}  \prod_{s=1}^j (1+c^s_1) c_2 b_j
\end{equation}
We analogously have the corresponding upper bound i.e
\begin{equation}
    a_{t+1} \leq a_t+ c^t_1 a_t + c^t_2 b_t,
\end{equation}
implies that:
\begin{equation}
    a_{t+1} \leq \prod_{s=1}^t (1+c^s_1) a_0 + \sum_{j=1}^{t-1}  \prod_{s=1}^j (1+c^s_1) c_2 b_j
\end{equation}
    
\end{lemma}


\subsection{Full Algorithm}

We describe the full algorithmic routine used in Theorem \ref{thm:main_theorem} in Algorithm \ref{alg:layerwise}.

\begin{algorithm}[h]
\caption{Layer-Wise Training for a Three-Layer Network}
\label{alg:layerwise}
\begin{algorithmic}

\STATE \textbf{Input:} Training data $\mathcal{D}$, mini-batch sizes $n_1,n_2$, 
       learning rates $\eta_1,\eta_2$, ridge regularization $\lambda$, iteration steps $T_{1}$.

\STATE \textbf{Initialize:}
\STATE \quad $W^{(1)}_i \ \overset{\text{i.i.d}}{\sim} \ 
U(\mathcal{S}^{d-1}(1))$ for $i \in [p_1]$
\STATE \quad $W^{(2)} \leftarrow \mathbf{I}_{p_2 \times d}$
\STATE \quad $(b^{(1)}, b^{(2)})  \leftarrow \mathbf{0}_{p_1 \times p_2}$.

\vspace{6pt}
\STATE \textbf{Layer 1 updates (correlation loss with spherical projections):}
\STATE $\hat{f}(\vec{x})\coloneqq \hat{f}_\theta(\vec{x}) = \vec{w}_3^\top \sigma(W_{2} \sigma(W_1\vec{x}))$
\STATE $\mathcal{L}:\mathbb{R}^d \times \mathbb{R} \rightarrow \mathbb{R} \gets \mathcal{L}(\vec{x}, y) \coloneqq - y\hat{f}(\vec{x})$ \textbf{(Set loss $\mathcal{L}$ to correlation loss)}
\FOR{$t = 1$ to $T_1$}
   \STATE Sample mini-batch $X, \mathbf{y} \subset \mathcal{D}$ of size $n_1$
   \STATE \textbf{For each neuron $j$ in layer 1:}
   \STATE \quad $\tilde{W^{(1)}_{j}} \leftarrow W^{(1)}_{j} \;-\; \eta_1 \,\nabla_{W^{(1)}_{j}} \mathcal{L}(X,y) (\mathbf{I}_d-(W^{(1)}_{j})(W^{(1)}_{j})^\top)$
   \STATE \quad $W^{(1)}_{j} \leftarrow \frac{1}{\norm{\tilde{W^{(1)}_{j}}}}\tilde{W^{(1)}_{j}}$
\ENDFOR

\vspace{6pt}
\STATE \textbf{Fix layer 1, update layer 2:}
\STATE \textbf{Re-initialize $W^{(2)} \rightarrow \mathbf{0}_{p_2 \times p_1}$}
   \STATE Sample mini-batch $X, \vec y \subset \mathcal{D}$ of size $n_2$
   \STATE $W_2 \leftarrow \left(\frac{1}{n}\sigma(W_1 X^\top)^\top (\sigma(W_1 X)^\top\right)^{-1} \nabla_{W_2} \mathcal{L}$


\vspace{6pt}
\STATE \textbf{Fix layers 1,2, solve for $W^{(3)}$ via ridge regression:}
  \STATE Sample mini-batch $X, \mathbf{y} \subset \mathcal{D}$ of size $n_3$
\STATE \textbf{Form design matrix $H$:}
\STATE \quad \textbf{For each} $(x,y)$ in $\mathcal{D}$:
\STATE \quad \quad $h_{1} \leftarrow \sigma\bigl(W^{1} x + b^{(1)}\bigr)$
\STATE \quad \quad $h_{2} \leftarrow \sigma\bigl(W^{2} h_{1} + b^{(2)}\bigr)$
\STATE \quad \quad $H_{(x,:)} \leftarrow [\,h_{2}\,]^\top,\quad Y_x \leftarrow y$

\STATE \textbf{Solve:}
\STATE \quad $W^{(3)} \leftarrow \bigl(H^\top H + \lambda I\bigr)^{-1}\,H^\top Y$
\end{algorithmic}
\end{algorithm}


\subsection{Leveraging Asymptotic Gaussianity}\label{sec:uni}

A crucial property of the non-linear feature $h^\star(\vec{x})$ that we leverage is its asymptotic Gaussianity, not only w.r.t their marginals but w.r.t propagation to the lower-level features. Specifically, building on \cite{wang2023learning}, we show that the high Hermite-degree functions of $h^\star_m(\vec{x})$ do not propagate projections along low Hermite-degree functions of $W^\star \vec{x}$. To show this, we provide an inductive proof inspired by the combinatorial approach developed in  \cite{wang2023learning}, wherein the (entropically) dominant contributions in $(h^\star_m(\vec{x}))^k$ arise from terms having the lowest degrees in $\operatorname{He}_j(\langle w^\star, \vec{x}\rangle)$. 

\begin{proposition}\label{prop:hermite_comp}
For any $\epsilon_1 > 0$, and $k\in \mathbb{N}$, let $h_\star(\vec{x})$ for $\vec{x} \in \mathbb{R}^d$ denote a non-linear feature of the form:
\begin{equation}
    h_\star(\vec{x}) = \frac{1}{\sqrt{d^{\epsilon_1}}} \sum_{i=1}^{d^{\epsilon_1}} \operatorname{P}_k(\langle w^\star_i, \vec{x}\rangle),
\end{equation}
where $\operatorname{P}_k$ denote polynomials of degree $k$ satisfying $\Eb{z \sim \mathcal{N}(0,1)}{\operatorname{P}_k(z)} = 0$ and $\Eb{z \sim \mathcal{N}(0,1)}{\operatorname{P}_k(z)z} = 0$

Denote by $S$ the set of indices in $[d^{\epsilon_1}]$ and by $\Gamma_m(S)$ the set of all $m$-permutations in $S$ consisting of distinct values. 

Then, the following holds for any $m \in \mathbb{N}$:
\begin{equation}\label{eq:herm_comp}
\operatorname{He}_m(h_\star(\vec{x})) = \frac{1}{m\sqrt{d^{m\epsilon_1}}}\sum_{s \in \Gamma_m(S)} \prod_{s_i} \operatorname{P}_k(\langle w^\star_{s_i}, \vec{x}\rangle) + r_m(\vec{x}) = \mathcal{O}_{\prec}(1),
\end{equation}
    where $r_m(\vec{x})$ satisfies:
    \begin{enumerate}[noitemsep,leftmargin=1em,wide=0pt]
    \item \begin{equation}
        r_m(\vec{x}) = \mathcal{O}_{\prec}(\frac{1}{\sqrt{d^{\epsilon_1}}}).
    \end{equation}
    \item For any $k \in \mathbb{N}$ and $v \in \mathbb{R}^d$:
\begin{equation}\label{eq:tens_bound}
        \norm{\Ea{ C_k(r_m(\vec{x}))\text{He}_{k-1}(\langle \vec{v}, \vec{x} \rangle) \vec{x}}}_2 = \tilde{\mathcal{O}}(\frac{1}{\sqrt{d^{\epsilon_1}}} (\max_{i \in \sqrt{d^{\epsilon_1}}}\abs{\langle \vec{w}^\star_i, \vec{v}\rangle})^{k-1}), \end{equation}
    for some $\tilde{\delta}> 0$,
\end{enumerate}
where recall that $\tilde{\mathcal{O}}$ subsumes factors of the form $d^\delta$ for arbitrarily small $\delta > 0$.

The above set of properties characterize, in particular the projections onto Hermite-polynomials of $\vec{x}$  of non-linear functions applied to $h^\star(\vec{x})$
\end{proposition}
% For $m \in \mathbb{N}$, let $\mathcal{P}_m$ denote the projection onto the subspace spanned by degree-$k$ Hermite-polynomials. 

\begin{proof}
    The proof proceeds by induction. Similar to \cite{wang2023learning}, the central idea is to utilize the fact that the Hermite-degree is additive for products of terms dependent on orthogonal subspaces. The entropically-dominant terms in $\operatorname{He}_p(h_\star(\vec{x}))$ arise from products of $\langle w^\star_i, \vec{x}\rangle, \langle w^\star_j, \vec{x}\rangle$ for $i \neq j$ contributing a leading Hermite-degree of $dk$.

We show inductively that Equation \ref{eq:herm_comp} holds for any $m \in \mathbb{N}$.

The base case $m=1$ holds trivially. Suppose that the statement holds for some $m \in \mathbb{N}$. Recall that the (normalized) Hermite polynomials satisfy the following recursion:
\begin{equation}
     \operatorname{He}_{m+1}(x) = x  \sqrt{\frac{m}{m+1}}\operatorname{He}_{m}(x) - m \sqrt{\frac{m-1}{m+1}} \operatorname{He}_{m-1}(x).
\end{equation}

Applying the above relation with $x=h_\star(\vec{x})$ yields:
\begin{equation}
\operatorname{He}_{m+1}(h_\star(\vec{x})) =  \sqrt{\frac{m}{m+1}}(\frac{1}{\sqrt{d^{\epsilon_1}}} \sum_{i=1}^{d^{\epsilon_1}}) \operatorname{P}_k(z) \operatorname{He}_{m}(h_\star(\vec{x}))- m \sqrt{\frac{m-1}{m+1}} \operatorname{He}_{m-1}(h_\star(\vec{x}))
\end{equation}

The induction hypothesis on $ \operatorname{He}_{m}(h_\star(\vec{x}))$, then implies:

\begin{equation}
\operatorname{He}_m(h_\star(\vec{x})) = \sqrt{\frac{m}{m+1}} \frac{1}{\sqrt{d^{\epsilon_1}}}\sum_{i=1}^{d^{\epsilon_1}}\operatorname{P}_k(\langle w^\star_i, \vec{x}\rangle)(\frac{1}{\sqrt{m d^{m\epsilon_1}}}\sum_{s \in \Gamma_m(S)} \prod_{s_i} \operatorname{P}_k(\langle w^\star_{s_i}, \vec{x}\rangle)+ 
r_m(\vec{x}))  - m \sqrt{\frac{m-1}{m+1}} \operatorname{He}_{m-1}(h_\star(\vec{x}))
\end{equation}

The first term splits into two components depending on whether $i \in s$ or $i \notin s$:
\begin{equation}
\begin{split}
\operatorname{He}_m(h_\star(\vec{x})) &= \sum_{i=1}^{d^{\epsilon_1}}( \operatorname{P}_k(\langle w^\star_i, \vec{x}\rangle))^2) (\frac{1}{\sqrt{(m +1)d^{(m+1)\epsilon_1}}}\sum_{s \in \Gamma_{m-1}(S \backslash i)} \prod_{s_i} \operatorname{P}_k(\langle w^\star_{s_i}, \vec{x}\rangle) 
) \\&+(\frac{1}{\sqrt{(m+1)d^{(m+1)\epsilon_1}}}\sum_{s \in \Gamma_{m+1}(S)} \prod_{s_i} \operatorname{P}_k(\langle w^\star_{s_i}, \vec{x}\rangle)) - m \sqrt{\frac{m-1}{m+1}} \operatorname{He}_{m-1}(h_\star(\vec{x}))+ \mathcal{O}_\prec(\frac{1}{\sqrt{d^{\epsilon_1}}}),
\end{split}
\end{equation}
where we used that $\sum_{i=1}^{d^{\epsilon_1}} \frac{1}{\sqrt{d^{\epsilon_1}}}\operatorname{P}_k(\langle w^\star_i, \vec{x}\rangle) r_m(\vec{x})) = \mathcal{O}_\prec(\frac{1}{\sqrt{d^{\epsilon_1}}})$ through the closure under-muliplication of $\mathcal{O}_\prec(\cdot)$ and Lemma \ref{lem:stoch-dom-mean}. The second term is exactly the desired expression for $\operatorname{He}_m(h^\star(\vec{x}))$ in Equation \ref{eq:herm_comp}.

Next, we rewrite the first term as:
\begin{equation}
\underbrace{\sum_{i=1}^{d^{\epsilon_1}}(\frac{1}{\sqrt{(m+1)d^{(m+1)\epsilon_1}}}\sum_{s \in \Gamma_{m-1}(S/i)} \prod_{s_i} \operatorname{P}_k(\langle w^\star_{s_i}, \vec{x}\rangle)}_{T_1} +  \underbrace{\sum_{i=1}^{d^{\epsilon_1}}\frac{1}{\sqrt{(m+1)d^{(m+1)\epsilon_1}}} ((\operatorname{P}_k(\langle w^\star_i, \vec{x}\rangle))^2-1)\\ \sum_{s \in \Gamma_{m-1}(S/i)} \prod_{s_i} \operatorname{P}_k(\langle w^\star_{s_i}, \vec{x}\rangle)}_{T_2}.
\end{equation}

By the induction hypothesis, $T_1$ cancels with $-m \sqrt{\frac{m-1}{m+1}}  \operatorname{He}_{m-1}(h_\star(\vec{x}))$ upto an error $\mathcal{O}_\prec(\frac{1}{\sqrt{d^\epsilon}})$. It remains to show that $T_2$ is stochastically dominated as $\mathcal{O}_\prec(\frac{1}{\sqrt{d^\epsilon}})$. To achieve this, we note by Gaussian hypercontractivity (Lemma \ref{lem:hyper}), it suffices to bound the second-moment of $T_2$. We have:
\begin{align*}
    \Ea{T_2^2} &= \frac{1}{d^{(m+1)\epsilon_1}}\sum_{i=1}^{d^{\epsilon_1}} ((\operatorname{P}_k(\langle w^\star_i, \vec{x}\rangle)^2-1) \sum_{s \in \Gamma_{m-1}(S/i)} \prod_{s_i} \operatorname{P}_k(\langle w^\star_{s_i}, \vec{x}\rangle))^2\\
    &+ \frac{1}{d^{(m+1)\epsilon_1}}\sum_{i\neq j=1}^{d^{\epsilon_1}} \prod_{k=l}(\operatorname{P}_k(\langle w^\star_k, \vec{x}\rangle)^3-\operatorname{P}_k(\langle w^\star_k, \vec{x}\rangle))\sum_{s \in \Gamma_{m-2}(S/(i,j))} (\prod_{s_i} \operatorname{P}_k(\langle w^\star_{s_i}, \vec{x}\rangle)^2),
\end{align*}
where in the last line we used the fact that the cross-terms vanish for terms with $\operatorname{He}_k(\langle w^\star_{s_i}, \vec{x}\rangle)$ appearing once. The desired bound is obtained by noting that by the inductive hypothesis:
\begin{equation}
\sum_{s \in \Gamma_{m-1}(S/i)} \prod_{s_i} \operatorname{He}_k(\langle w^\star_{s_i}, \vec{x}\rangle) = \mathcal{O}_{\prec}(\sqrt{d^{(m-1)\epsilon_1}}).
\end{equation} Therefore the first term contributes $d^{\epsilon_1}$ terms of order $\mathcal{O}_{\prec}(d^{(m-1)\epsilon_1})$ while the second term consists of $d^{2\epsilon_1}$ terms of order $\mathcal{O}_{\prec}(d^{(m)\epsilon_1})$. Therefore, both the terms are entropically sub-dominant compared to the factor $\frac{1}{d^{(m+1)\epsilon_1}}$, yielding:
\begin{equation}
    T_2 = \mathcal{O}_\prec (\frac{1}{\sqrt{d^{\epsilon_1}}})
\end{equation}
It remains to show statement $(ii)$ (Equation \ref{eq:tens_bound}). We first consider the residual term:
\begin{equation}
    \sum_{i=1}^{d^{\epsilon_1}} \frac{1}{\sqrt{d^{\epsilon_1}}}\operatorname{P}_k(\langle w^\star_i, \vec{x}\rangle) r_m(\vec{x}))
\end{equation}
Recall that for any $v \in \mathbb{R}^d$ and any $r(\vec{x})$:
\begin{equation}
    \Ea{\langle \nabla^k r(\vec{x}) v^{\otimes k}\rangle} = \Ea{r(\vec{x})\text{He}_k(\langle \vec{x} ,\vec{v})}.
\end{equation}
Therefore, by induction and the closure of stochastic domination under multplication, the above term satisfies $(ii)$. 

For the remaining term $T_2$, $(ii)$ holds by noting that by Proposition \ref{prop:Hermite-tens}, for each $i \in \sqrt{d^{\epsilon_1}}$, $\norm{\Ea{ C_k(r_m(\vec{x}))\text{He}_{k-1}(\langle \vec{v}, \vec{x} \rangle) \langle \vec{x}, w^\star_i \rangle}}_2$ is a polynomial in $\{\langle w^\star_i, v \rangle\}_{i \in \sqrt{d}^{\epsilon_1}}$ of degree at-least $k-1$. Since $\{\langle \vec{x}, w^\star_i \rangle\}$ are orthonormal functions:
\begin{equation}
\sum_{i=1}^{\sqrt{d^{\epsilon_1}}}\norm{\Ea{ C_k(r_m(\vec{x}))\text{He}_{k-1}(\langle \vec{v}, \vec{x} \rangle) \langle \vec{x}, w^\star_i \rangle}}^2_2 \leq \norm{\Ea{ C_k(r_m(\vec{x}))\text{He}_{k-1}(\langle \vec{v}, \vec{x} \rangle)}}^2 = \tilde{\mathcal{O}}(\frac{1}{d^{\epsilon_1}})
\end{equation}

% Therefore by the condition on $v$, each summand in $\Ea{T_2 \text{He}_k(\langle v, \vec{x} \rangle)}$
% is of order $\mathcal{O}(\frac{1}{\sqrt{d^{\epsilon-\delta}}})$ smaller than the summands in $T_2$, ensuring $(ii)$ holds.
\end{proof}
\subsection{Feature Learning by the First Layer}

In this section, we analyze the dynamics of $W_1$ (part $(i)$ of Theorem \ref{thm:main_theorem}). In fact, for subsequent usage in the dynamics of $W_2, w_3$, we require a stronger characterization of $(i)$ of Theorem \ref{thm:main_theorem}.
To state the precise result, we first set up the required notation. Let $\mathcal{D}_t = \{X_t,\vec{y}_t\}$ denote the batch of samples at time-step $t$ for $t \in \mathbb{N}$. Observe that under the correlation loss, and with $W_2=\mathbb{I}$, each neuron $w_i$ for $i \in [p]$ evolves independently. In-fact, the dynamics is equivalent to that of a two-layer network with modified activation $\tilde{\sigma}=\sigma(\sigma(\cdot \rangle))$

Therefore, the gradient descent dynamics on $W_1$ defines a stochastic mapping:
\begin{equation}
    w^0 \rightarrow w^{(t)},
\end{equation}
applied to a random variable $w^0 \sim U(\mathcal{S}^{d-1}(1))$.

Let $\{\mathcal{F}_t\}_{t \in \mathbb{N}}$ denote the filtration generated by $\mathcal{D}_1, \mathcal{D}_2, \cdots$. Let $U^\star \in \mathbb{R}^d$ denote the subspace spanned by the teacher weights $W^\star$. Define $u^\star \coloneqq \frac{P_{U^\star}w_0}{\norm{P_{U^\star}w_0}}$ to be the unit-vector along $P_{W^\star}w_0$. Our analysis proceeds by establishing the following:
\begin{enumerate}
    \item The dynamics of $w^{(t)}$ is dominated by drift along the initial direction $u^\star$.
    \item The overlap of $w^{(t)}$ along $W^\star$ grows linearly upto reaching a threshold $\kappa >0$ and subsequently $w^{(t)}$ reaches overlap $\eta$ in a constant number of iterations.
    \item The distribution of $w^{(t)}$ maintains isotropy and regularity of tails.
\end{enumerate}

Let $\kappa > 0$ be fixed
We introduce the following hitting time:
\begin{equation}
    \tau_\kappa \coloneqq \{\inf t: \abs{\langle u^\star, w^{(t)}\rangle \geq 1-\kappa}\}.
\end{equation}

The random variable $w_\kappa$ then admits a regular conditional distribution w.r.t $\mathcal{F}_t$ $\mu_\eta(|\mathcal{F}_t)$ \citep{klenke2013probability}.

Suppose that each neuron for $i \in [p_1]$ in Algorithm \ref{alg:layerwise} is stopped at $\tau_\eta$ as defined above. Then 
 
Let $e_1, \cdots e_{d-d^{\epsilon_1}}$ denote a fixed basis for the complement of $W^\star$.
The main result of this section establishes points $i-iii$ described above and constitutes the formal statement for part $(i)$ of Theorem \ref{thm:main_theorem}:
\begin{theorem}\label{thm:main_pt}
For any $0 < \kappa < 1$, let $\mu_\kappa(\cdot|X_1,X_2,\cdots)$ denote the regular conditional measure over $w^\tau_\kappa$ conditioned on the sequence of datasets $X_1,X_2,\cdots$ associated with the natural filtration $\mathcal{F}_t$. Then, for any $k \in \mathbb{N}$, there exists a sequence of ``high-probability" events $\mathcal{E} \in \cup_{t \geq 1}\{\mathcal{F}_t\}$ such that:
\begin{enumerate}
    \item $\Pr[\mathcal{E}] \geq 1-\frac{C_k}{d^{k}}$ for some $C_k>0$ and large enough $d$.
    \item For any $X_1,X_2,\cdots \in \mathcal{E}$, the random variable $w \sim \mu_\eta(\cdot|X_1,X_2,\cdots)$, satisfies the following with probability $1-Ce^{-C\log d^2}$ as $d \rightarrow \infty$:

   \begin{equation}
       w^\tau_\kappa=\kappa^
       +u^\star+u_\perp + v,
   \end{equation}
 where $\kappa^+ \geq \kappa$ and:
 \begin{enumerate}
     \item $u_\perp \in U^\star, v \in U^\star_\perp$.
     \item $\norm{u_\perp}= \mathcal{O}_\prec(\frac{1}{d^\delta})$.
     
\item \begin{equation}
     \sup_{i \in d^{\epsilon_1}} \abs{\langle w^\tau_\kappa, w^\star_i \rangle} = O_\prec(\frac{1}{\sqrt{d^{\epsilon_1}}}).
 \end{equation}
 and
 \begin{equation}
     \sup_{j \in [d-d^{\epsilon_1}]} \abs{\langle w^\tau_\kappa, e_j \rangle} = O_\prec(\frac{1}{\sqrt{d}}).
 \end{equation}
 \item For any (deterministic) $ w^\star \in U^\star$:
 \begin{equation}
     \abs{\langle w^\star, w^\tau_\kappa \rangle} = O_\prec(\frac{1}{\sqrt{d^{\epsilon_1}}}),
 \end{equation}
 and for any $w_\perp \in U^\star_\perp$:
 \begin{equation}
     \abs{\langle w_\perp, w^\tau_\kappa \rangle} = O_\prec(\frac{1}{\sqrt{d}}).
 \end{equation}
 \item \begin{equation}
     \norm{v} - \abs{\langle w^0, v \rangle} = \mathcal{O}_\prec(\frac{1}{d^\delta}),
 \end{equation}
 where $w^0 \sim U(\mathcal{S}^{d-1}(1))$ denotes the initialization of the neuron.
\end{enumerate}
\end{enumerate}
\end{theorem}

Properties $(c)$ stipulates that  $w^\tau_{\kappa}$ remains approximately isotropic with well-behaved tails along a fixed basis of $U^\star$ and its complement. This is important for ensuring concentration of well-behaved functions of $w^\tau_{\kappa}$ in part $(ii)$. Maintaining this property throughout the dynamics further leads to a control over the higher-order terms.

\begin{corollary}\label{cor:moment`_cont}
Let $w^\tau_\eta$ be as defined in Theorem \ref{thm:main_pt}. Then, $\exists \ \delta > 0$ and choice of step-size $\eta=\tilde{\eta}\sqrt{d^{\epsilon_1}}$ for some $\tilde{\eta}>0$ such that:
\begin{equation}
\norm{\Ea{w^\tau_\kappa (w^\tau_\kappa)^\top}-\kappa\frac{1}{\sqrt{d^{\epsilon_1}}}-\sqrt{1-\kappa^2}\frac{1}{\sqrt{d-d^{\epsilon_1}}}} =\mathcal{O}(\frac{1}{d^\delta})
\end{equation}
\end{corollary}
% Our analysis follows the drift+ martingale approach similar to \cite{BenArous2021}. The factor $\frac{1}{\sqrt{d^{\epsilon_1}}}$ is responsible for the increased sample complexity from $\mathcal{O}(d \log d)$ to $\mathcal{O}(d^{\epsilon_1+1}d \log d)$.
\subsection{Form of the Update}

Under the initialization $W_2=\mathbf{I}_p, b_1, b_2 = \mathbf{0}, w_3 =\mathbf{0}$,  for any $i \in [p_1]$, the update to any neuron $w$ in $W_1$ can be expressed as:
\begin{equation}\label{eq:update}
\begin{split}
    \tilde{w}^{t} &= w^{t} + (\mathbf{I}_d-(\vec{w}_t)(\vec{w}_t)^\top)\eta \frac{1}{n}\sum_{i=1}^n f^\star(\vec{x}_i)\tilde{\sigma'}(\langle w^{t}, \vec{x}_i\rangle)\vec{x}_i
    \\&= w^{t+1}=\frac{1}{{\norm{\tilde{w}^{t}}}_2}\tilde{w}^{t}
\end{split}
\end{equation}

In what follows, we denote the gradient update as:
\begin{equation}
   g^t \coloneqq \frac{1}{n}\sum_{i=1}^n \sigma f^\star(\vec{x}^t_i)\tilde{\sigma'}((\langle w^{t}, \vec{x}_i\rangle)\langle w^{(t)}, \vec{x}^t_i\rangle))\vec{x}^t_i,
\end{equation}

and its corresponding spherical version as:
\begin{equation}
    g^t_\perp \coloneqq g^t(\mathbb{I}-(w^{(t)})(w^{(t)})^\top),
\end{equation}
where we recall that $\norm{w^{(t)}}=1$ by the spherical constraint.

Applying the Hermite decomposition to $f^\star(\vec{x})$ and utilizing the composition of Hermite-coefficients established in Proposition \ref{prop:hermite_comp} results in the following expansion for the gradient:    \begin{lemma}\label{lem:app:grad_exp}
    Let $C_{k}^\star$  for $k \in \mathbb{N}$ denote the $k_{th}$-order Hermite tensor of $f^\star(
    z)$ and let $\{c_k\}_{k=1}^\infty$ be the Hermite coefficients of $\tilde{\sigma}(\cdot)$. Then:
    \begin{equation}\label{eq:update}
        \dE[\vec{g}^\perp] = (\mathbf{I}_d-(\vec{w}_t)(\vec{w}_t)^\top)\frac{1}{\sqrt{p}}\left(\sum_{k = 0}^\infty c_{k+1}\,  C_{k+1}^\star \times_{1 \dots k} (\vec{w}^t)^{\otimes k} \right)
    \end{equation}
\end{lemma}
\begin{proof}
The above is a direct consequence of Stein's Lemma applied to $\dE[\vec{g}^t]$:
  \begin{align*}
        \dE\left[ \vec{x} \tilde{\sigma'}(\langle \vec{w}, \vec{x} \rangle) f^\star(\vec{x})\right] &= \dE\left[ \nabla_{\vec{x}}\sigma'(\langle \vec{w}, \vec{x} \rangle) f^\star(\vec{x})\right] + \dE\left[ \tilde{\sigma'}(\langle \vec{w}, \vec{x} \rangle) \nabla_{\vec{x}}f^\star(\vec{x})\right] \\
        &= \vec{w} \dE\left[ \tilde{\sigma''}(\langle \vec{w}, \vec{z} \rangle) f^\star(\vec{z})\right] + \dE\left[ \sigma'(\langle \vec{w}, \vec{x} \rangle) \nabla_{\vec{x}}f^\star(\vec{x})\right]. 
    \end{align*}
The first term vanishes under the orthogonal projection $(\mathbf{I}_d-(w^{(t)})(w^{(t)})^\top)$ while the second term results in Equation \ref{eq:update}.
\end{proof}

Combining the above with the recursive Hermite decomposition of $f^\star(\vec{x})$ through Proposition \ref{prop:hermite_comp} 
yields the following form for the expected updates:

\begin{proposition}\label{prop:gen}
Let $\{\mu_k\}_{k=1}^\infty$, $\{c_k\}_{k=1}^\infty$,  $\{c^\star_k\}_{k=1}^\infty$ denote the Hermite coefficients of $g^\star, \tilde{\sigma}, P^k_\star$ respectively. Then:
    \begin{equation}\label{eq:gt_decomp}
    \begin{split}
        &\Ea{g^\perp_t} = (\mathbf{I}_d-(\vec{w}_t)(\vec{w}_t)^\top)\Bigl(\frac{1}{\sqrt{d^{\epsilon_1}}} c_2 c^\star_2 \mu_1  v^\top (W^\star)^\top(W^\star) w + \mu_1 \sum_{j=3}^k c^\star_j c_j  \frac{1}{\sqrt{d^{\epsilon_1}}} \sum_{i=1}^{d^{\epsilon_1}} (\langle w^\star_i, w \rangle)(\langle w^\star_i, v \rangle)\\ &+ \sum_{m={k+1}}^\infty \mu_m c_m \frac{1}{\sqrt{md^{m\epsilon_1}}}\sum_{s \in \Gamma(S,m), j \in [k]^s} \prod_{i=1}^{m-1} c^\star_{j_i} (\langle w^\star_{s_i}, w \rangle)^{j_1} \langle w^\star_{s_m}, v \rangle+ \sum_{m=1}^\infty c_m\Ea{r_m(x)\tilde{\sigma}'(\langle w,\vec{x} \rangle)(\langle\vec{x},v\rangle}\Bigr),
    \end{split}
    \end{equation}
where $r_m(x)$ denotes the remainder for the degree-$m$ Hermite term in Equation \ref{eq:herm_comp}.
\end{proposition}

\begin{proof}
    Proposition \ref{prop:hermite_comp} applied to $f^\star(\vec{x})$ yields:
    \begin{equation}
        f^\star(\vec{x}) = \sum_{m=1}^\infty \mu_m \frac{1}{\sqrt{m d^{m\epsilon_1}}}\sum_{s \in \Gamma_m(S)} \prod_{s_i} \operatorname{P}_k(\langle w^\star_{s_i}, \vec{x}\rangle) + \sum_{m=1}^\infty \mu_m r_ m(\vec{x}).
    \end{equation}

Next, by expanding $\operatorname{P}_k(\langle w^\star_{s_i}, \vec{x}\rangle)=\sum_{j=1}^k c^\star_k \text{He}_j(\langle w^\star_{s_i}, \vec{x}\rangle)$, the first term in the RHS can be further decomposed as:
\begin{equation}
    \sum_{m={k+1}}^\infty \mu_m \frac{1}{\sqrt{d^{m\epsilon_1}}}\sum_{s \in \Gamma_m(S)} \prod_{s_i} \operatorname{P}_k(\langle w^\star_{s_i}, \vec{x}\rangle) = \sum_{m={k+1}}^\infty \mu_m \frac{1}{\sqrt{d^{m\epsilon_1}}}\sum_{s \in \Gamma(S,m), j \in [k]^s} \prod_{i=1}^{m} c^\star_{j_i} \text{He}_{j_1}(\langle w^\star_{s_i}, w \rangle).
\end{equation}
Equation \ref{eq:gt_decomp} then follows by noting that any term of the form $\prod_{i=1}^{m} c^\star_{j_i} \text{He}_{j_1}(\langle w^\star_{s_i}, w \rangle)$ appears in $C^\star_\ell$ with $\ell=\sum_{i=1}^m j_i$.
\end{proof}

The magnitude of the gradient updates is bounded through the following Lemma:
\begin{proposition}\label{eq:grad-bounds}
Let $g^t_{\perp,i} \coloneqq \sigma f^\star(\vec{x})\sigma'(\sigma'(\langle w, \vec{x}\rangle))\vec{x}(I-\frac{1}{\epsilon^2}ww^\top)$ denote the spherical gradient for neuron $i$ at time-step $t$.
Then $g_\perp$ satisfies:
\begin{enumerate}
    \item
    \begin{equation}\label{eq:grad-norm}
        \norm{g^t_{\perp}}^2 = \norm{\Ea{g^t_{\perp}}}^2 +\mathcal{O}_\prec(\frac{1}{d^{\epsilon_1+\delta}}),
    \end{equation}
    \item For any $v \in \mathbb{R}^d$ with $\norm{v}=1$:
    \begin{equation}\label{eq:grad-proj}
        \langle g^t_\perp, v\rangle- \Ea{\langle g^t_\perp, v\rangle} = \mathcal{O}_\prec(\frac{1}{\sqrt{n}_1}) = \mathcal{O}_\prec(\frac{1}{\sqrt{d^{1+\delta}}\sqrt{d^{\epsilon_1}}})
    \end{equation}
    \item \begin{equation}\label{eq:g_perp_conc}
\norm{P_{U^\star}g^t_\perp}- \Ea{\norm{P_{U^\star_\perp}g^t}}  =\mathcal{O}_\prec(\frac{1}{\sqrt{d^{1+\delta}}})
    \end{equation}
\end{enumerate}
\end{proposition}
\begin{proof}
    We begin by writing:
    \begin{equation}
    \norm{g^t_{\perp}}^2= \norm{ \Ea{g^t_{\perp}}}^2+\norm{g^t_{\perp}-\Ea{g^t_{\perp}}}^2.
    \end{equation}
By the assumption on $\sigma$, the composed activation $\tilde{\sigma}$ and its derivatives are polynomially bounded. Therefore, applying standard concentration results for for independent random variables with bounded orlicz norm (Theorem \ref{thm:app:orlicz_sum}), we obtain:
\begin{equation}\label{eq:g_conc}
    g^t_{\perp}-\Ea{g^t_{\perp}} = \mathcal{O}_\prec(\sqrt{\frac{d}{n}}) = \mathcal{O}_\prec(\frac{1}{\sqrt{d^{\delta+\epsilon_1}}})
\end{equation}
which yields Equation \ref{eq:grad-norm}.

Applying the same result (Theorem \ref{thm:app:orlicz_sum}) to projections of $g^t_\perp$ along $v$ and $P_{U^\star}$ then yields Equations \ref{eq:grad-proj} and \ref{eq:g_perp_conc}.
\end{proof}


\subsection{Initial Overlaps}

Before proceeding with the analysis, we collect the following result on the concentration of the initial overlaps along $W^\star$ for the first-layer neurons at initialization:

\begin{lemma}\label{lem:init_over}
For $w \sim \mathcal{N}(0,\frac{1}{d}\mathbf{I})$,
\begin{equation}
   \abs{\sqrt{d^{1-\epsilon_1}}(\norm{P_U^\star w}_2-1} =  \mathcal{O}_{\prec} (\frac{1}{\sqrt{d^{\epsilon_1}}})
\end{equation}
\end{lemma}

\begin{proof}
    Since $w^0 \sim U(\mathcal{S}^{d-1}(1))$, the squared overlap norm $\frac{d}{d^{\epsilon_1}}\norm{W^\star w^0_{1,i}}^2_2$ is an average over $d^{\epsilon_1}$ sub-exponential random variables. Therefore, a standard application of Bernstein's inequality \citep{vershynin2010introduction} yields an error probability of $1-ce^{-\log d^2}$. The proposition then follows through a union bound over the $p_1$ neurons.
\end{proof}

\subsection{Difference Inequality}

Let ${P}^\star_{U^\star} \coloneqq  (W^\star)^\top (W^\star)$ denote the projector onto the subspace spanned by $W^\star$. For $i \in [p_1]$, define $u^\star_i$ to be the unit-vector along the projection of $w^0_i$ along $W^\star$:

\begin{equation}
    u^\star \coloneqq\frac{1}{\norm{W^\star w^{(0)}_i}} P^\star_{U^\star} w^0
\end{equation}

Further define $m^t = \langle u^\star, w^{(t)}_{1,i} \rangle$  and $m^t_{\perp}= \norm{(P^\star_{U^\star}-u^\star (u^\star)^\top)w^{(t)}}$, denoting the projections of $w^{(t)}$ along $u^\star_i$ and its complement in the span of $W^\star$ and:
\begin{equation}
  m^t_\times  \coloneqq \sqrt{d^{\epsilon_1}}\max_{i \in \sqrt{d^{\epsilon_1}}}\abs{\langle w^{(t)}, w^\star_i \rangle}.
\end{equation}

Additionally, we track the residual component in $w^{(t)}$ lying in $U^\star_\perp$ but orthogonal to $w_0$:
\begin{equation}
    r^t_\perp \coloneqq (\mathbb{I}-w^{(0)}(w^{(0)})^\top)P_{U^\star_\perp} w^{(t)}
\end{equation}

Our analysis relies on showing that the dynamics of $w^{(t)}$ is dominated by a linear drift along $u^\star$. This requires a control over the following additional terms:
\begin{enumerate}
    \item Residual linear drift along $U^\star_\perp$: This term is controlled through a bound on $m^t_\perp$.
    \item Contributions from higher-order terms: These are controlled through a bound on $m^t_\times$.
    \item Noise in the gradient updates: This is supressed through the choice of batch-size $n=\Theta(d^{1+\epsilon_1+\delta})$
\end{enumerate}

Recall that $n_1 = \Theta(d^{k\epsilon_1+\delta})$.
Let $\tilde{\delta}$ be any arbitrary value satisfying  $0 < \tilde{\delta} < \delta $
For any $\eta > 0$, we define the following stopping times:
\begin{eqnarray}
    \tau^+_\kappa = \inf (t: \abs{m^t} \geq \kappa)
\end{eqnarray}

\begin{eqnarray}
    \tau^{-}_{\tilde{\delta}}= \inf (t: \abs{m^t} \leq d^{-\tilde{\delta}} \min(m^t_\perp, m^t_\times, \frac{1}{\sqrt{d^{1-\epsilon_1}}}))
\end{eqnarray}

% \begin{eqnarray}
%     \tau^\perp_{\tilde{\delta}}= \inf (t: m^t \leq d^{\tilde{\delta}} m^t_\perp)
% \end{eqnarray}

% \begin{eqnarray}
% \tau^\times_{\tilde{\delta}}= \inf(t:\max_{j \in d^{\epsilon_1 k}}  \abs{w_i, w^\star_j}\geq \frac{\abs{m}_t}{d^{\frac{\epsilon_1}{2}-\tilde{\delta}}}).
% \end{eqnarray}

The stopping time $\tau^+_\kappa$ simply accounts for the overlap reaching the desired value $\kappa$. The stopping time $\tau^{-}_{\tilde{\delta}}$ ensures that for $t \leq \tau^{-}_{\tilde{\delta}}$, the three residual
contributions in $g^t_\perp$ listed above, namely the drift along $U^\star_\perp$, higher-order terms and the gradient noise remain supressed.

% orthogonal contribution $m^t_\perp$ remains negligble compared to $m_t$, thereby ensuring that $m_t$'s dynamics remains approximately unaffected. Similarly, the time $  \tau^\times_{\tilde{\delta}}$ allows bounding the higher-order terms in the dynamics.



Note that by definition, $m^0_{i,\perp}=0$ while $m^0_i =\frac{1}{\sqrt{d^{\epsilon_1}}}$ by Lemma \ref{lem:init_over} . While both $m^0_{i,\perp}, m^0_i$ grow exponentially, we will show that for any $0 < \tilde{\delta} < \delta $, there exists a small enough $\tilde{\eta}$ such that with step size $\eta= \tilde{\eta}d^{\epsilon_1}$, $\tau_{\kappa}^+ >  \tau^{\perp}_{\tilde{\delta}}, \tau^{\perp}_{\tilde{\delta}}$ with high-probability.

\begin{proposition}\label{prop:diff_ineq}
Let $c= \mu_1 c^\star_2 \tilde{c}_2$ and $\eta=\tilde{\eta}\sqrt{d^{\epsilon_1}p_2}$
Define $\tau^\star \coloneqq \tau_{\kappa}^+ \land \tau^{-}_{\tilde{\delta}}$
For any $\tilde{\delta} < \delta_\perp < \delta, \kappa$ and $k \in \mathbb{N}$, and any constants $c^+_m,c^+_\perp, c^+_\times, c^-_m,c^-_\perp, c^-_\times$ such that $c^-_m < c < c^+_m$,  $c^-_\perp < c < c^+_\perp$,  $c^-_\times < c < c^+_\times$ there exists constant $C_1,C_2,C_3,C_4$ and $\tilde{\eta}$ such that for large enough $d$: 
\begin{enumerate}
    \item 
\begin{equation} \label{eq:overlap}        
\mathbb{P}(m_{t+1} \geq m^t + \tilde{\eta} c^-_m m_t - \tilde{\eta} c^+_m m^2_t -\tilde{\eta}^2 C_1 m^3_t
\ \forall t < \tau^\star) \geq 1-\frac{1}{d^{k}}
    \end{equation}
\begin{equation} \label{eq:overlap_l}        
\mathbb{P}(m_{t+1} \leq m^t + \tilde{\eta} c^+_m m_t - \tilde{\eta} c^-_m m^2_t
\ \forall t < \tau^\star) \geq 1-\frac{1}{d^{k}}
    \end{equation}
    \item \begin{equation}\label{eq:m_perp}         \mathbb{P}(m^{t+1}_{\perp} \leq \left(m^t_{\perp} + \tilde{\eta} c^+_{\perp} m^{t}_{\perp}-\tilde{\eta} c^-_{\perp} m^tm^{t}_{\perp}+\tilde{\eta}C_2 (m^t_\times)+\tilde{\eta}\frac{C_3}{\sqrt{d^{1-\epsilon_1+\delta_\perp}}},
    \ \forall t < \tau^\star) \right) \geq 1-\frac{1}{d^{k}}
    \end{equation}
    \item 
    
    \begin{equation} 
    \mathbb{P}(m^t_{\times} \leq m^t_{\times} + \tilde{\eta} c^-_{\times} m^t_{\times}-\tilde{\eta} c^+_{\times} m^t_{\times}m^t+\tilde{\eta}\frac{C_4}{\sqrt{d^{1+\delta_\perp}}}, \ \forall t < \tau^\star) \geq 1-\frac{1}{d^{k}}
    \end{equation}
\end{enumerate}
\end{proposition}

Before establishing the above proposition, we first show how it implies Theorem \ref{thm:main_pt}

\subsection{Proof of Theorem \ref{thm:main_pt}}

Suppose that $t \leq \tau^\star$. 
Applying a union-bound to $(i)$ in Proposition \ref{prop:diff_ineq} then implies that with probability at-least $t(1-\frac{1}{d}^k)$:
\begin{equation}\label{eq:mt_upd}
    m_{t+1} \geq m^t + \tilde{\eta} c^-_m m_t - \tilde{\eta} c^+_m m^2_t -\tilde{\eta}^2 C_1 m^3_t. 
\end{equation}
Since $t < \tau^+_\kappa$ we have $m_t \leq \kappa$ and thus $\tilde{\eta}^2 C_1 m^3_t < \tilde{\eta}^2 C_1 \kappa m^2_t$. Equation \ref{eq:mt_upd} then implies:
 \begin{equation}
    m^{t+1} \geq (1+\tilde{\eta} c^-_m - (\tilde{\eta} c^+_m+\tilde{\eta}^2 C_1 \kappa)m^t)m^t, 
\end{equation}
which inductively implies the following intermediate bound:
\begin{equation}\label{eq:int_bound}
    m^{t+1} \geq \prod_{s=1}^t(1+\tilde{\eta} c^-_m - (\tilde{\eta} c^+_m+\tilde{\eta}^2 C_1 \kappa) m^s)m^0, 
\end{equation}
 
Since $m_t^2 \leq \kappa m_t$, we further obtain:
\begin{equation}\label{eq:mt_upd}
    m_{t+1} \geq m^t +(\tilde{\eta} c^+_m(1-\kappa)-\tilde{\eta}^2 C_1 \kappa^2) m^t
\end{equation}

Therefore,  $\forall t < \tau^\star$:

\begin{equation}\label{eq:main_bound}
    m^{t}\geq (1+\tilde{\eta} c^+_m(1-\kappa)-\tilde{\eta}^2 C_1 \kappa^2)^t m^{0}.
\end{equation}
By Lemma \ref{lem:init_over}, we have:
\begin{equation}\label{eq:perp_upd}
    \abs{m^{0}} = \frac{1}{d^{1/2(1-\epsilon_1)}}+\mathcal{O}_{\prec}(\frac{1}{d}),
\end{equation}

Since for small enough $\tilde{\eta}$, $c^+_m(1-\kappa)-\tilde{\eta}^2 C_1 \kappa^2 > 0$, Equation \ref{eq:main_bound} implies:
\begin{equation}
    \tau^\star \leq c_{\kappa,\epsilon}\log d, 
\end{equation}
for some constant $c_{\kappa,\epsilon}>0$.

Next, consider the orthogonal component $m^{t}_{\perp}=0$. Note that $m^{0}_{\perp}=0$ by definition. Part $(ii)$ of Proposition \ref{prop:diff_ineq} along with the discrete Gronwall inequality (Lemma \ref{lem:gronwall}) implies:

\begin{align*}
   m^{t}_{\perp} &\leq \sum_{j=1}^{t-1} \prod_{s=1}^j (1+\tilde{\eta}c^+_{\perp}-\tilde{\eta}c^-_{\perp}m^s)(\tilde{\eta}C_2 (m^t_\times)^2+\tilde{\eta}\frac{C_3}{\sqrt{d^{1-\epsilon_1+\delta_\perp}}})\\
   & \leq \sum_{j=1}^{t-1} \prod_{s=1}^j (1+\tilde{\eta}c^+_{\perp}-\tilde{\eta}c^-_{\perp}m^s)(\tilde{\eta}C_2 \frac{1}{d^{2\tilde{\delta}}}(m^t)^2+\tilde{\eta}\frac{C_3}{\sqrt{d^{1-\epsilon_1+\delta_\perp}}}),
\end{align*}
where we used that $m^t_\times \leq \frac{1}{d}^{\tilde{\delta}}$ since $t \leq \tau^-_{\tilde{\delta}}$.

Our goal next is to compare the above bound against the lower-bound given by Equation \ref{eq:int_bound}. Since $\abs{\log(1+a)-\log(1+b)} \leq \abs{a-b}$ for $a, b >0$, we have:
\begin{equation}
    \frac{(1+\tilde{\eta} c^-_m - (\tilde{\eta} c^+_m+\tilde{\eta}^2 C_1 \kappa) m^s)}{(1+\tilde{\eta}c^+_{\perp}-\tilde{\eta}c^-_{\perp}m^s)} \leq \exp(\tilde{\eta} \abs{c^-_m-c^+_\perp}+\abs{c^+_m-c-_\perp}+\tilde{\eta}^2C_1\kappa))
\end{equation}

Therefore, we obtain the corresponding bound:

\begin{equation}\label{eq:m_perp_bound}
     m^{t}_{\perp} \leq t\exp(\tilde{\eta} \abs{c^-_m-c^+_\perp}+\abs{c^+_m-c-_\perp}+\tilde{\eta}^2C_1\kappa)) \left(\frac{1}{d^{2\tilde{\delta}}}C_2 m_0+\frac{C_3}{\sqrt{d^{1-\epsilon_1+\delta_\perp}}})\right)
\end{equation}

For any $0 < \delta_\perp < \tilde{\delta}$, we may set $\abs{c^+_m-c^-_{\perp}}, \abs{c^-_m-c^+_{\perp}}$ and $\tilde{\eta}$ small enough so that for any $t \leq c_{\kappa, \epsilon} \log d$:
\begin{equation}
   (t\tilde{\eta} \abs{c^-_m-c^+_\perp}+\abs{c^+_m-c-_\perp}+\tilde{\eta}^2C_1\kappa) + \log C_2 <\delta_{\perp}-\tilde{\delta},
\end{equation}
Implying:
\begin{equation}
m^{t}_{\perp} \leq \frac{m_t}{d^{\tilde{\delta}}}
\end{equation}

Similarly by setting $\abs{c^+_m-c^-_{\times}}, \abs{c^-_m-c^+_{\times}}$ small enough enough we have by part (iii) in Proposition \ref{prop:diff_ineq}:

\begin{equation}
     m^{t}_{\times} \leq \frac{m_t}{d^{\tilde{\delta}}}
\end{equation}
Therefore, while the dynamics of $m^{t},m^{t}_{\perp}, m^{t}_{\times}$ evolves at arbitrarily close rates. The initial advantage in $m^{t}$ over $m^{t}_{\perp}, m^t_\times$ through initalization ensures that the hitting time for $m^{t}$ arrives first, ensuring that:
\begin{equation}
     \tau_{\kappa} < \tau^-_{\tilde{\delta}}
\end{equation}

By the definition of $\tau^\star$, this establishes all claims in Theorem \ref{thm:main_theorem} apart from $(f)$.
 To obtain $f$, note that by the form of the updates, $r^t_\perp$ is updated solely through the gradient noise $g^t_{\perp}-\Ea{g^t_{\perp}}$ and normalization. Therefore, Lemma \ref{eq:grad-bounds} implies that:
\begin{equation}
    r^t_\perp = \mathcal{O}_\prec(t \frac{1}{d^{\sqrt{\delta}}}).
\end{equation}

\subsubsection{The conditioning input set}
To complete the proof of Theorem \ref{thm:main_pt}, it remains to specify the high-probability set $\mathcal{E}_{\kappa,\tilde{\delta}}$.
For any $\tilde{\delta} > 0$ and $\kappa \in \mathbb{R}$, consider the event:
\begin{equation}
    \mathcal{E}_{\kappa,\tilde{\delta}} :\cap_{t \leq \tau^+_\kappa} \left[\abs{\langle g^t, v\rangle- \Ea{\langle g^t, v\rangle}} \geq \frac{d^{\tilde{\delta}}}{d^{1+\epsilon +\delta}}\right]
\end{equation}
Equations \ref{eq:grad-proj}, \ref{eq:g_conc} and a union bound imply that for any $k \in \mathbb{N}$:
\begin{equation}
   \Pr[ \mathcal{E}_{\kappa,\tilde{\delta}}] \geq 1-\frac{1}{d^k},
\end{equation}
where the probability is w.r.t the joint measure over $w_0, X_1,\cdots X_n$

By the law of total expectation:
\begin{equation}
     \Pr[  \mathcal{E}^\complement_{\kappa,\tilde{\delta}}] \geq  \frac{1}{d^k} \Ea{\mathbf{1}[\Pr[ \mathcal{E}^\complement_{\kappa,\tilde{\delta}}\vert \{X_i\}_{i \in \mathbb{N}}]\geq \frac{1}{d^k}},
\end{equation}
implying, for any $k>\tilde{k} \in \mathbb{N}$:
\begin{equation}
    \Ea{\mathbf{1}[\Pr[  \mathcal{E}^\complement_{\kappa,\tilde{\delta}}\vert \{X_i\}_{i \in \mathbb{N}}]\geq \frac{1}{d^k}} \leq \frac{1}{d^k},
\end{equation}
taking the interesection over $k \in \mathbb{N}$ the required conditioning set $\mathcal{E}$ in Theorem \ref{thm:main_pt}.




% Note: throughout, we set $\eta=\mathcal{O}(1)$ so that the interaction term can be controlled under small $\epsilon$.



\subsection{Proof of Proposition \ref{prop:diff_ineq}}

Consider the ``effective" activation:
\begin{equation}
    \tilde{\sigma}(x) \coloneqq \sigma'(\sigma'(x))
\end{equation}
Let $\tilde{c}_2= \Eb{z \sim \mathcal{N}(0,1)}{\tilde{\sigma}(z)\text{He}_2(z)}$. Define the constant $c=\mu_2 c^\star_1 \tilde{c}_2$. By assumption \ref{ass:bad}, $c>0$.


\textbf{part $(i)$}: Applying Proposition \ref{prop:gen}, we obtain the following decomposition for the update $g^\perp_t$:

 \begin{equation}
    \begin{split}
        g^\perp_t &= (\mathbf{I}-w^{(t)} (w^{(t)})^\top)\frac{1}{\sqrt{d^{\epsilon_1}}} \mu_1 c^\star_2 \tilde{c}_2 P_{U^\star}w^{(t)} \\&+ (\mathbf{I}-w^{(t)}(w^{(t)})^\top)\underbrace{\sum_{j=3}^k \mu_1 c^\star_jc_j  \frac{1}{\sqrt{d^{\epsilon_1}}} \sum_{i=1}^{d^{\epsilon_1}} (\langle w^\star_i, w \rangle)^j w^\star_i +\sum_{m={k+1}}^\infty \mu_m c_m \frac{1}{\sqrt{md^{m\epsilon_1}}} \sum_{s \in \Gamma(S,m), j \in [k]^s} \prod_{i=1}^{m-1} c^\star_{j_i} (\langle w^\star_{s_i}, w \rangle)^{j_1} w^\star_{s_m}}_{\Delta_1}\\&+ (\mathbf{I}-w^{(t)}(w^{(t)})^\top)\underbrace{\sum_{m=1}^\infty\Ea{\mu_m r_m(x)\tilde{\sigma}'(\langle w,\vec{x} \rangle)\vec{x}}}_{\Delta_2} \\& + (\mathbf{I}-w^{(t)}(w^{(t)})^\top)\underbrace{g^\perp_t-\Ea{g^\perp_t}}_{\Delta_3} 
    \end{split}
    \end{equation}
Since $m_t = \langle w^{(t)} , u^\star\rangle$, $\norm{w^t}=1$ the first term simplifies to:
\begin{equation}\label{eq:m_t_bound}
    \frac{1}{\sqrt{d^{\epsilon_1}}} \mu_1 c^\star_2 \tilde{c}_2 (u^\star)^\top P_{U^\star}w^{(t)}-\frac{1}{\sqrt{d^{\epsilon_1}}} \mu_1 c^\star_2 \tilde{c}_2 \langle w^{(t)} , u^\star\rangle w^{(t)}P_{U^\star}w^{(t)}\\
    = cm_t-cm^2_t.
\end{equation}

Next, for $\Delta_1$, we separately consider the component along $w^\star_i$ for each $i \in \sqrt{d^{\epsilon_1}}$:
\begin{equation}
    \langle \Delta_1, w^\star_i \rangle = \sum_{j=3}^k \mu_1 c^\star_jc_j  \frac{1}{\sqrt{d^{\epsilon_1}}} (\langle w^\star_i, w \rangle)^j + \mu_m c_m \frac{1}{\sqrt{md^{m\epsilon_1}}} \sum_{s \in \Gamma(S,m), s_m=i, j \in [k]^s} \prod_{i=1}^{m-1} c^\star_{j_i} (\langle w^\star_{s_i}, w \rangle)^{j_1}
\end{equation}

% Since $t \leq  \tau^{-}_{\tilde{\delta}}$, e
Each term of the form $\langle w^\star_{s_i}, w \rangle$ is uniformly bounded as $\frac{m^t_\times}{\sqrt{d^{\epsilon_1}}}$. Since $\abs{\Gamma(S,m), s_m=i} = \Theta(d^{(m-1)\epsilon_1})$, we obtain:
\begin{equation}
    \abs{\langle \Delta_1, w^\star_i \rangle} \leq \frac{1}{\sqrt{d^{\epsilon_1}}}\sum_{j \in \mathbb{N}}c_j(m^t_{\times})^j
\end{equation}
for some constants $c_j$ with $\sup_j \abs{c_j} < \infty$. Therefore a geometric-series bound (applicable since $m^t_{\times} < 1$) yields:
\begin{equation}
     \abs{\langle \Delta_1, w^\star_i \rangle} \leq \frac{C}{\sqrt{d^{\epsilon_1}}} m^t_{\times},
\end{equation}
for some contant $C>0$. Summing the above bound over $i \in \sqrt{d^{\epsilon_1}}$, results in the bound:
\begin{equation}\label{eq:bound_1}
    \norm{\Delta_1}_2 \leq \frac{C}{\sqrt{d^{\epsilon_1}}} m^t_{\times}
\end{equation}

Next, for $\Delta_2$, we first apply the Hermite decomposition of $\tilde{\sigma}'$ to obtain:
\begin{equation}
\Ea{r_m(x)\tilde{\sigma}'(\langle w,\vec{x} \rangle)\vec{x}} = \sum_{j=1}^\infty \tilde{c}_j 
\Ea{r_m(x)\text{He}_{j-1}(\langle w,\vec{x} \rangle)\vec{x}} 
\end{equation}

By Assumption \ref{ass:act}, $\tilde{c}_1=0$ while $(ii)$ in Proposition \ref{prop:hermite_comp} implies that the terms corresponding to $j>2$ are bounded as $\frac{1}{\sqrt{d^{j\epsilon_1}}}(m^t_{\times})^j$

We therefore obtain:
\begin{equation}
    \norm{\Delta_2} \leq \frac{\tilde{C} m^t_{\times}}{\sqrt{d^{\epsilon_1}}},
\end{equation}
for some constant $\tilde{C} > 0$. 
Lastly, Lemma \ref{lem:app:grad_exp} implies that:
\begin{equation}\label{eq:bound_delta}
    \abs{\langle \Delta_3, u^\star \rangle} = \mathcal{O}_\prec(\frac{1}{d^{k\epsilon+\delta}}).
\end{equation}
Since $t < \tau^{-}_{\tilde{\delta}}$, the above bounds on $\Delta_1, \Delta_2,\Delta_3$ can be absorbed within arbitrarily small constants compared to $m$:
\begin{equation}
    \abs{\langle \Delta_1, u^\star \rangle}+ \abs{\langle \Delta_2, u^\star \rangle}+ \abs{\langle \Delta_3, u^\star \rangle} \leq C m_t,
\end{equation}
for arbitrarily small constant $C>0$.


This results in the bound:
\begin{equation}\label{eq:part_1}
    m^{t+1} \geq \frac{m^t + c \frac{\eta}{\sqrt{d^{\epsilon1}}}m^t - \eta\tilde{c}(m^t)^2}{\sqrt{1+\eta^2\norm{g^t}^2}},
\end{equation}
where $\tilde{c} \leq c+\tilde{\epsilon}$ for arbitrarily small $\tilde{\epsilon}$.
Next, we use the inequality $\sqrt{1+t}^{-1} \geq (1-\frac{t}{2})$ for $t\geq 0$ to obtain:
\begin{equation}
\begin{split}
(\sqrt{1+\eta^2{\norm{g^t}^2}})^{-1}
\geq 1 - \frac{\eta^2}{2}\norm{g^t}^2)\\
\geq 1-(\frac{1}{2} \eta^2 c^2_g m^t_2),
\end{split}
\end{equation}
where in the last line we applied the control over the squared gradient norm in Lemma \ref{eq:grad-bounds} and $c_g < c$ can again be set arbitrarily close to $c$. Combining with Equation \ref{eq:part_1} yields part $(i)$.

Next, for part $(ii)$, introduce the operator:
\begin{equation}
  P_{U^\perp} \coloneqq (\mathbb{I}-(u^\star)(u^\star)^\top)P_{U^\star},
\end{equation}
corresponding to the projection onto the orthogonal complement of $u^\star$ in $U^\star$. Let $u^t_\perp \coloneqq (\mathbb{I}-(u^\star)(u^\star)^\top)P_{U^\star} w^{(t)}$.

 \begin{equation}
    \begin{split}
        g_t P_{U^\perp} &= \frac{(\mathbf{I}-w^{(t)} (w^{(t)})^\top)}{\sqrt{d^{\epsilon_1}}} \mu_1 c^\star_2 \tilde{c}_2 P_{U^\perp}(W^\star)^\top(W^\star) w^t + (\mathbf{I}-w^{(t)} (w^{(t)})^\top)P_{U^\perp}\Delta_1+(\mathbf{I}-w^{(t)} (w^{(t)})^\top)P_{U^\perp}\Delta_2\\&+(\mathbf{I}-w^{(t)} (w^{(t)})^\top)P_{U^\perp}\Delta_3
    \end{split}
    \end{equation}

Since $\norm{P_{U^\perp}w^t} = m^t_\perp$ and $\norm{P_{U^\star}w^t} = m^t$ , the first term simplifies to:
\begin{align*}
    \norm{\frac{(\mathbf{I}-w^{(t)} (w^{(t)})^\top)}{\sqrt{d^{\epsilon_1}}} \mu_1 c^\star_2 \tilde{c}_2 P_{U^\perp}(W^\star)^\top(W^\star) w^t} &\leq \frac{\mu_1 c^\star_2 \tilde{c}_2}{\sqrt{d^{\epsilon_1}}}\norm{P_{U^\perp}(W^\star)^\top(W^\star) w^t}-\frac{\mu_1 c^\star_2 \tilde{c}_2}{\sqrt{d^{\epsilon_1}}}\norm{(w^{(t)} (w^{(t)})^\top)P_{U^\perp}(W^\star)^\top(W^\star) w^t}\\
    &=cm^t_\perp - c m^tm^t_\perp.
\end{align*}

By Equation \ref{eq:bound_1}, we have:
\begin{equation}
\norm{(\mathbf{I}-w^{(t)} (w^{(t)})^\top) P_{U^\perp}\Delta_1} \leq \norm{\Delta_1} \leq \frac{C}{\sqrt{d^{\epsilon_1}}} m^t_{\times}, 
\end{equation}
for some constant $C>0$.

Similarly, by Equation \ref{eq:delta_2_b}, we obtain a bound:
\begin{equation}
   \norm{(\mathbf{I}-w^{(t)} (w^{(t)})^\top) P_{U^\perp}\Delta_2} \leq \frac{\tilde{C}}{\sqrt{d^{\epsilon_1}}} m^t_{\times}.
\end{equation}
The above combine to result in the term $\tilde{\eta}C_2 (m^t_\times)$ in Equation \ref{eq:perp_upd}. Finally, by Equation \ref{eq:g_perp_conc} in Proposition \ref{eq:grad-bounds}, $\Delta_3 P_{U^\perp}$ is bounded as:
\begin{equation}
    \norm{(\mathbf{I}-w^{(t)} (w^{(t)})^\top)P_{U^\perp}\Delta_3}=\mathcal{O}_\prec(\frac{1}{\sqrt{d^{1-\epsilon_1+\tilde{\delta}}}}) 
\end{equation}, 
yielding the last term in Equation \ref{eq:perp_upd}.


Analogously, part $(iii)$ follows by considering the terms $\langle w^\star_i, \Delta_j\rangle$, $i \in \sqrt{d^{\epsilon_1}}$ for $j=1,2,3$ for $(iii)$ respectively, with the bound on $\norm{g^t}^2$ remaining the same.



\subsection{Feature Learning by the Second Layer}


To motivate our setup for the training of $W_2$, we start with a heurestic discussion of the dynamics of gradient updates in the absence of pre-conditioning and projections. The presentation of formal results towards the proof of part $(ii)$ of Theorem \ref{thm:main_theorem} starts from Section \ref{sec:update_struc}.

\subsection{Updates in Feature Space and Projection Onto the Kernel}

The gradient update for a neuron $W_{i,2}, i \in [p]$ in the second layer has the following form:
\begin{equation}
   W^{t+1}_{i,2} = W^{t}_{i,2} - \eta\nabla_{W^{t}_2} \mathcal{L} = W^{t}_{i,2} + \eta \sum_{\mu=1}^n (f^\star(\vec{x}_\mu) - \hat{f}(\vec{x}))(\vec{x}_\mu) W_{i,3} \sigma'(\langle W^{t}_{i,2}, \sigma(W_1(\vec{x}))\rangle)\sigma(W_1(\vec{x}_\mu)) \in \R^{p_1}.
\end{equation}
Under the approximation $\hat{f}(\vec{x}) \approx 0$, the updated pre-activation out of at a fixed input $\vec{x}$ are thus given by:
\begin{equation}\label{eq:sec_layer_update}
\begin{split}
\langle W^{t}_{i,2}, \sigma(W_1(\vec{x}))\rangle &\approx \langle w^{(t)}_{i,2}, \sigma(W_1(\vec{x}))\rangle  + \eta \langle \sum_{\mu=1}^n f^\star(\vec{x}_\mu) a_i \sigma'(\langle W^{t}_{i,2}, \sigma(W_1(\vec{x}_\mu))\rangle)\sigma(W_1(\vec{x}_\mu)),\sigma(W_1(\vec{x}_\mu))\rangle \rangle\\
&= \langle W^{t}_{i,2}, \sigma(W_1(\vec{x}))\rangle  + \eta \sum_{\mu=1}^n f^\star(\vec{x}_\mu) a_i \sigma'(\langle W^{t}_{i,2}, \sigma(W_1(\vec{x}_\mu))\rangle)\langle \sigma(W_1(\vec{x}_\mu)),\sigma(W_1(\vec{x}))\rangle \rangle.
\end{split}
\end{equation}

Letting $h_{2,i}^t(\vec{x}) \coloneqq \langle w^{(t)}_{i,2}, \sigma(W_1(\vec{x}))\rangle$, we obtain:

\begin{equation}\label{eq:pre-ac-update}
    h^{t+1}_{2,i}(\vec{x}) \approx h^{t}_{2,i}(\vec{x}) + \eta W_{3,i}Z(\vec{x})^\top Z^\top (f^\star(X)\sigma'(h^{t}_{2,i}(X)),
\end{equation}
where $Z(\vec{x})$ denotes the feature-mapping $\sigma(W_1(\vec{x}))$.

We see that in the limit $n, p_1 \rightarrow \infty$, the above update results in a projection of $f^\star$ on the following Kernel (integral operator):
\begin{equation}
    K_1(\vec{x},\vec{x'}) = \Eb{\vec{w} \sim \mu_1}{\sigma(\langle \vec{w}, \vec{x} \rangle)\sigma(\langle \vec{w}, \vec{x}' \rangle)},
\end{equation}

where $\mu_1$ denotes the distribution of the rows of $W_1$ obtained upon feature learning in part $(i)$.


\subsection{The Role of Preconditioning}\label{app:pre-cond}
In light of Equation \ref{eq:pre-ac-update}, we obtain a dynamics of the form:
\begin{equation}
    h^{t+1}_{2,i}(\vec{x}) \approx h^{t}_{2,i}(\vec{x}) + \eta W_{3,i}K_1 f^\star(x) \sigma'(h^{t}_{2,i}(x)) + \text{noise},
\end{equation}
where $K_1 f^\star(x) \sigma'(h^{t}_{2,i}(x))$ denotes the projection of $f^\star(x) \sigma'(h^{t}_{2,i}(x))$ onto the Kernel $K_1$. Through a central limit theorem-based heurestic, we expect the noise to be of order $\mathcal{O}(\frac{1}{\sqrt{n}}+\frac{1}{\sqrt{p}})$ \cite{nichani2024provable}. However, the decay in $K_1$'s spectrum, entails that the degree-$k$ components in $K_1 f^\star(x) \sigma'(h^{t}_{2,i}(x))$ are of order $\mathcal{O}(\frac{1}{d^{k}})$. Comparing $\mathcal{O}(\frac{1}{\sqrt{n}}+\frac{1}{\sqrt{p}})$ and $\mathcal{O}(\frac{1}{d}^{k})$, one expects a sample-complexity of $d^{2k}$ for recovering $h^\star(\vec{x})$ through a single (non-preconditioned) gradient step. For quadratic features, this is precisely the sample-complexity obtained in \cite{nichani2024provable}.


A possible way to get around the additional sample complexity would be to re-use a single batch of size $\mathcal{O}(d^{ k_\epsilon})$ for up to $\mathcal{O}(d^{ k_\epsilon})$ steps, ensuring that the projection on the Kernel is well-approximated at each step while the number of steps are enough for the dynamics described by Equation \ref{eq:pre-ac-update} to approximate ridge-regression, which effectively has the same effect as pre-conditioning through the removal of the learned components. However, analyses of gradient descent with the re-use of batches for a large number of iterations is expected to be challenging due to the accumulation of additional correlations and memory terms \cite{dandi2024benefits}.

Therefore, to allow a simplified ``online" analysis we opt to include additional pre-conditioning in the updates, which effectively removes the extra $\frac{1}{d^{k_\epsilon}}$ factor from Equation \ref{eq:pre-ac-update}.

\subsection{Main Result for part $(ii)$: Recovery of $h^\star(\vec{x})$}

\begin{theorem}\label{thm:main_pt2}
Let $W^1_2$ denote the updated layer $2$ weights after a single pre-conditioned gradient step with batch-size $n_2$, with initialization $W^0_2 = \mathbf{0}_{p_2 \times p_1}$ as in Algorithm \ref{alg:layerwise}:
\begin{equation}
    W^1_2 =  -\eta \left(\frac{1}{n}\sigma(W_1 X^\top)^\top (\sigma(W_1 X)^\top\right)^{-1} \nabla_{W_2} \mathcal{L}
\end{equation},
where $W_1 \in \mathbb{R}^{p_1 \times d}$ denotes the updated weight matrix with independent rows obtained as per Theorem \ref{thm:main_pt}. The updated pre-activations $h^1_2(\vec{x})=W^1_2\sigma(W_1 \vec{x})$ then satisfy:
\begin{equation}
    h^1_2(\vec{x})= \eta w_3 \sigma'(0)  h^\star (\vec{x})+ r(\vec{x}),
\end{equation}
where $r(\vec{x})$ satisfies:
\begin{equation}
    r(\vec{x}) = \mathcal{O}_\prec(\frac{1}{\sqrt{d^{\epsilon_1}}}).
\end{equation}
\end{theorem}


\subsection{Structure of the Pre-conditioned Update}\label{sec:update_struc}

Let $Z$ denote the feature matrix $Z=\sigma(X W_1^\top)$ applied to an independent data-matrix $X \in \mathbb{R}^{n_2 \times d}$ using the updated weights $W_1$ obtained in part $(i)$. Throughout the section, we assume that the threshold parameter $\kappa>0$ in Theorem \ref{thm:main_pt} is fixed to some dimension-independent value and occasionally consider the limit $\kappa \rightarrow 0$ (but after $d \rightarrow 0$). Denote by $Z(\vec{x})$ the same mapping applied to a fixed point $\vec{x} \in \mathbb{R}^d$.

The proposition below expresses a pre-conditioned gradient update on $W_2$ as a "Kernel-ridge regression like" update to $h^t_2(\vec{x})$. 

\begin{proposition}\label{prop:2update}
    Suppose that $W_2$ is re-initialized to $\mathbf{0}$. The updated pre-activations $h^t_2(\vec{x})$ satisfy for $i \in [p_2]$:
    \begin{equation}
    h^{1}_{2,i}(\vec{x}) =\eta w_{3,i}Z(\vec{x})^\top (\frac{1}{n}Z^\top Z)^{-1} Z^\top (f^\star(X))\sigma'(0).
    \end{equation}
\end{proposition}


\subsection{Decomposition into Radial and Spherical Kernels}

Let $l^d_k, Q^d_k$ for $k \in \mathbb{N}$ denote the associated Laguerre and Gegenbauer polynomials in dimension $d$. Recall that $U^\star$ denotes the span of $W^\star$.

From Theorem \ref{thm:main_pt}, for any $i \in [p_1]$, the updated neuron $w^1_i$ can be decomposed as:
\begin{equation}
    \vec{w}^1_i = \vec{u}_i + \vec{v}_i,
\end{equation}
where $\norm{\vec{w}^1_i}=1$, $u_i \in U^\star$ and $v_i \in U^\star_\perp$. 

For any $\vec{x} \in \mathbb{R}^d$, denote by $\vec{x}^\star, \vec{x}^\perp$, its components along $U^\star$ and $U^\star_\perp$ respectively. Let $r^\star, r_\perp$ denote $\norm{\vec{x}^\star}, \norm{\vec{x}^\perp}$ respectively.

Next, we decompose the inner-product $\langle w_i, x\rangle$  as:
\begin{equation}
  \langle w_i, x\rangle=  \norm{\vec{x}^\star} \langle \vec{u}, \frac{\vec{x}^\star}{\norm{\vec{x}^\star}}\rangle + \norm{\vec{x}^\perp} \langle \vec{v}, \frac{\vec{x}_\perp}{\norm{\vec{x}_\perp}}\rangle.
\end{equation}

By the Gaussianity of $\vec{x}$, the random variables $\norm{\vec{x}^\star}, \langle \vec{u}, \frac{\vec{x}^\star}{\norm{\vec{x}^\star}}\rangle,  \norm{\vec{x}^\perp}, \langle \vec{v}, \frac{\vec{x}_\perp}{\norm{\vec{x}_\perp}}\rangle$ are mutually independent. Therefore, $\langle w, x\rangle$ admits an orthonormal basis given by the tensor product of associated Laguerre and Gegenbauer polynomials.


By expanding $\sigma$ along the bases of associated Laguerre and Gegenbauer polynomials, the activation $\sigma(\langle w, x\rangle +b)$ can then be decomposed as:
\begin{equation}\label{eq:main_sigma_decomp}
\begin{split}
    &\sigma(\langle w_i, x\rangle +b)\\ &= \sum_{k_1,j_1,k_2,j_2=0}^\infty a^d_{j_1,k_1,j_2,k_2}(b, \norm{u_i}, \norm{v_i}) l^{d^{\epsilon_1}}_{j_1}(r^\star)l^{d-d^{\epsilon_1}}_{j_2}(r_\perp)Q^{d^{\epsilon_1}}_{k_1}(d^{\epsilon_1} \langle \vec{u}, \frac{\vec{x}^\star}{\norm{\vec{x}^\star}}\rangle) Q_{k_2}^{d-d^{\epsilon_1}}(d-d^{\epsilon_1} \langle \vec{v}, \frac{\vec{x}_\perp}{\norm{\vec{x}_\perp}} \rangle),
\end{split}
\end{equation}
where $r^\star=\frac{\norm{x}^2_\star-1}{\sqrt{d}}$ and $r^\perp = \frac{\norm{x}^2_\perp-1}{\sqrt{d}}$ and $l^{d}_k, P^d_k$ denote the associated Laguerre and Gegenbauer polynomials in dimension $d$ respectively.

\begin{proposition}\label{eq:rad_coeff}
For all $k \in \mathbb{N}$:
    \begin{equation}
       \lim_{\kappa \rightarrow 0} \lim_{d \rightarrow \infty} a^d_{j,0}(b) \sqrt{d^{\epsilon_1}}^k = \mu_k(b)
    \end{equation}
\end{proposition}
\begin{proof}
Let $d_\perp \coloneqq d- d^{\epsilon_1}$, then:
    \begin{equation}
         \sigma(\langle w_i, x\rangle +b) = \sigma((\sqrt{1+\frac{\norm{x}^2_{\star}-d^{\epsilon_1}}{d^{\epsilon_1}}}) \sqrt{d^\epsilon_1}\langle u, \frac{x^\star}{\norm{x}^\star}\rangle +\sqrt{1+\frac{\norm{x}^2_{\perp}-d_\perp}{d_\perp})}\sqrt{(d_\perp)}\langle v, \frac{x_\perp}{\norm{x_\perp}}+b)
    \end{equation}
Subsequently, the result follows through the Taylor expansion $\sqrt{1+z}= 1+\frac{z}{2}+o(z^2)$ w.r.t $r^\star$, while noting that $\sqrt{d^\epsilon_1} \langle u, \frac{x^\star}{\norm{x}^\star}\rangle \rightarrow \mathcal{N}(0,\tilde{\kappa})$ and  $\sqrt{(d_\perp)}\langle v, \frac{x_\perp}{\norm{x_\perp}} \rightarrow \rangle \rightarrow \mathcal{N}(0,1-\tilde{\kappa})$ by Theorem \ref{thm:main_pt} for some $\tilde{\kappa} > \kappa$
\end{proof}

In light of the above decomposition, we introduce the following sequence of radial Kernels, with $k_1=k_2=j_2=0$:
\begin{equation}\label{eq:rad_kern}
    K^d_k(\vec{x}_1, \vec{x}_2) = \sum_{j=0}^\infty \sum_{j'=0}^\infty \Ea{a^d_{j,k,0,0}(b, \norm{u_i}, \norm{v_i})a^d_{j',k,0,0}(b, \norm{u_i}, \norm{v_i})} l^{d^{\epsilon_1}}_j(r^\star_1)l^{d^{\epsilon_1}}_{j'}(r^\star_2)
\end{equation}

\begin{proposition}
Under Assumption \ref{ass:target}, $h^\star_{1,2}$, $K^d_0$ admits eigenfunctions $\phi_{1,d}, \phi_{2,d}$ with associated eigenvalues $\lambda_1=\Theta(1), \lambda_2=\Theta(\frac{1}{\sqrt{d^{\epsilon_1}}})$ such that $r^\star(\vec{x})= \frac{1}{\sqrt{d^{\epsilon_1}}}(\norm{\vec{x}^\star}^2-1)$  satisfies:
\begin{equation}\label{eq:r_comp}
r^\star(\vec{x}) = \mathcal{P}_{\operatorname{span}\{\phi_{1,d}, \phi_{2,d}\}}r^\star(\vec{x})+\mathcal{O}(\frac{1}{\sqrt{d^{\epsilon_1}}}).
\end{equation}
\end{proposition}
where $\mathcal{P}$ denotes projection in $L^2(\mu(\vec{x}))$.

\begin{proof}
By proposition \ref{eq:rad_coeff},
  $a^d_k$ converge a.s to deterministic limits $a^d_k$ as $d \rightarrow \infty$.
   We obtain the following limiting expression for $K_{k}(\vec{x}, \vec{x}')$:
   \begin{equation}
   \begin{split}
       K_{0}(\vec{x}, \vec{x}') &= \Eb{b}{a_0a_0} +\frac{1}{d^{\epsilon_1}} \Eb{b}{a_0a_1}l_1(r_1)l_1(r_2) +\frac{1}{\sqrt{d^{\epsilon_1}}} \Eb{b}{a_0a_1}l_0(r^\star_1)l_1(r^\star_2)\\&+\frac{1}{\sqrt{d^{\epsilon_1}}} \Eb{b}{a_0a_1}l_1(r^\star_1)l_0(r^\star_2)+\cdots
    \end{split}
   \end{equation}
   Note that:
   \begin{equation}
       \Ea{l_0(r^\star_1)K_{0}(\vec{x}, \vec{x}') l_0(r^\star_2)} = \Eb{b}{a^2_0} = \Theta(1),
   \end{equation}
   for any $\delta > 0$, while:
   \begin{equation}
        \Ea{l_1(r^\star_1)K_{0}(\vec{x}, \vec{x}') l_1(r^\star_2)} = \frac{1}{d^{\epsilon_1}}\Eb{b}{a^2_1} = \Theta(\frac{1}{d^{\epsilon_1}})
   \end{equation}
   
   Therefore, by the variational characterization of eigenvalues for compact self-adjoint operators \citep{axler2020measure}, we obtain:
   \begin{equation}
       \abs{\lambda_1(K_{0}(\vec{x}, \vec{x}'))- \Eb{b}{a_0(b)a_0(b)}} = \mathcal{O}_\prec(\frac{1}{d^{k \epsilon_1}})
   \end{equation}
   implying:
   \begin{equation}
       \lambda_1(K_0(\vec{x}, \vec{x}')) = \Theta_d(1),
   \end{equation}
   and:
   \begin{equation}
       \lambda_2(K_{0}(\vec{x}, \vec{x}')) = \Theta_d(\frac{1}{d^{ \epsilon_1}}).
   \end{equation}

Analogously, since $\sigma$ is analytic, with probability $1$, $a_{j,0} \neq 0, \forall j \in \mathbb{N}$, we obtain that the $j_{th}$ eigenvalue for $K_{0}(\vec{x}, \vec{x}')$ satisfies:
\begin{equation}
    \lambda_j(K_{0}(\vec{x}, \vec{x}')) = \Theta_d(\frac{1}{d^{\frac{j}{2}\epsilon_1}}).
\end{equation}

Equation \ref{eq:r_comp} then follows by noting that:
\begin{equation}
    \Ea{r^\star(\vec{x})K_{0}(\vec{x}, \vec{x}')r^\star(\vec{x'})} = \Theta_d(\frac{1}{d^{ \epsilon_1}})
\end{equation}
   % we note that the eigenfunctions $\phi_{1}, \phi_{2}$ are also the eigenfunctions for the original Kernel $K(\vec{x}, \vec{x}')$
\end{proof}

\subsection{Decomposition of the Feature Matrix}

For $j_i,k_i \in \mathbb{N}$, let $\theta^d_{j_1,k_1,j_2,k_2}$ denote the eigenfunctions of the radial Kernel $K^d_k$ for $k \in \mathbb{N}$,  defined as (Generalizing Equation \ref{eq:rad_kern}):
\begin{equation}\label{eq:rad_kern_2}
    K^d_{k_1,k_2}(\vec{x}_1, \vec{x}_2) = \sum_{j_1,j'_1}^\infty \sum_{j'_2,j'_2}^\infty \Ea{a^d_{j_1,k_1, j_2,k_2}a^d_{j'_1,k_1, j'_2,k_2}} l^{d^{\epsilon_1}}_{j_1}(r^\star_1)l^{d-d^{\epsilon_1}}_{j_2}(r^\perp_1)l^{d^{\epsilon_1}}_{j'_1}(r^\star_2)l^{d-d^{\epsilon_1}}_{j'_2}(r^\perp_2)
\end{equation}

Analogously, let $\kappa^d_{j_1,k_1,j_2,k_2}$ denote the eigenfunctions of the associated companion Kernel defined on the weights:
\begin{equation}\label{eq:rad_kern_dual}
    \mathcal{K}^d_{k_1,k_2}(\vec{w}_1, \vec{w}_2) = \sum_{j=0,j'=0}^\infty a^d_{j,k_1, j',k_2}(\vec{w}_1)a^d_{j,k_1,j',k_2}(\vec{w}_2)
\end{equation}
Define:
\begin{equation}
    \psi_{j_1,j_2,k_1,k_2}(\vec{x})=\theta^d_{j_1,k_1,j_2,k_2}(r^\star,r_\perp) Y_k(\vec{x}^\star/r^\star)Y_k(\vec{x}_\perp/r_\perp).
\end{equation}

And for the conjugate:
\begin{equation}
    \phi_{j_1,j_2,k_1,k_2}(\vec{w})=\kappa^d_{j_1,k_1,j_2,k_2}(b,\norm{u},\norm{v}) Y_k(\vec{u}/\norm{u})Y_k(\vec{v}/\norm{v}).
\end{equation}

With a slight abuse of notation, we denote $\theta^d_{j,0,0,0}$ and $\kappa^d_{j,0,0,0}$ by $\theta^d_{j}$ and $\kappa^d_{j}$ respectively. These correspond to eigenvalues for the zeroth-order radial Kernel along $U^\star$.

We partition the indices $j_1,j_2,k_1,k_2$ into three disjoint sets:
\begin{align*}
    \mathcal{S}_1 &= \{j_1,j_2,k_1,k_2:k_1=j_2=k_2=0,j_2 \leq 2\} \cup \{j_1,j_2,k_1,k_2:j_1=j_2=k_2=0,k_1 \leq k\}\\
    \mathcal{S}_2 &= \{j_1,j_2,k_1,k_2:j_2,2j_1+2j_2/\epsilon_1+k_1+k_2/\epsilon_1\} - \mathcal{S}_1\\
     \mathcal{S}_3 &= \{j_1,j_2,k_1,k_2, \in \mathbb{N}^4\} - (\mathcal{S}_1 \cup \mathcal{S}_2).
\end{align*}
The above partitioning is motivated as follows:
\begin{enumerate}
    \item  $\mathcal{S}_1$ corresponds the set of eigenfunctions whose projections can be approximated via $Z$ with $n_2, p_2 = \Theta(d^{k\epsilon_1})$ and are relevant towards learning $f^\star(\vec{x})$.
    \item $\mathcal{S}_2$ corresponds to the set of eigenfunctions whose projections can be approximated by $Z$ but do not contribute to the learning of $f^\star(\vec{x})$.
    \item $\mathcal{S}_3$ corresponds to the high-degree set of eigenfunctions for which the number of samples, neurons $n_2,p_2$ are insufficient towards being approximated through $Z$. 
\end{enumerate}


Expressing Equation \ref{eq:main_sigma_decomp} in matrix form and applying Proposition \ref{prop:gegen_harm}, we obtain the following decomposition:
\begin{equation}\label{eq:mat_decomp}
\begin{split}
    Z &= \sum_{j=1}^{2k} \Theta^d_{j}(\vec{r}^\star)D^r_j\mathfrak{K}^d_{j}(\vec{b}, \vec{\norm{u}}) + \sum_{j=1}^{k}Y_j(U)D^{\mathcal{S}_1}_j Y_j(X^\star)\\ &+ \sum_{j_1,j_2,k_1,k_2 \in \mathcal{S}_2} \Psi_{j_1,j_2,k_1,k_2}(X)D^{\mathcal{S}_2}_{j_1,j_2,k_1,k_2}\Phi_{j_1,j_2,k_1,k_2}(W) \\&+ \sum_{j_1,j_2,k_1,k_2 \in \mathcal{S}_3} \Psi_{j_1,j_2,k_1,k_2}(X)D^{\mathcal{S}_3}_{j_1,j_2,k_1,k_2} \Phi_{j_1,j_2,k_1,k_2}(W),
\end{split}
\end{equation}
where $D^r_j, D^s_j$ denote diagonal matrices with entries $(b^d_j)^2, (a^d_j)^2$ respectively. We denote the above three-components corresponding to $\mathcal{S}_1, \mathcal{S}_2, \mathcal{S}_3$ as $Z_1, Z_2, Z_3$ respectively.

\subsection{Approximation of Eigenfunctions}

Let $M=\abs{S_1} \cup \abs{S_2}$. Since $B(d,k) = \Theta(d^k)$ (section \ref{sec:spher_harm}), we obtain $M=\Theta(d^{k\epsilon})$. The above partitioning of eigenfunctions translates to a ``spike"+bulk structure for $Z$, with the spikes arising from components corresponding to $\mathcal{S}_1, \mathcal{S}_2$ allowing the reconstruction of the corresponding eigenfunctions through the sample-covariance. The higher-degree components $\mathcal{S}_3$, on the other hand, coalesce into a bulk. These properties are summarized in the following proposition, which constitutes the central result of this section:

\begin{proposition}\label{prop:orth_decomp}
\begin{enumerate}
    \item \begin{equation}\label{eq:conc_x}
        \norm{\frac{1}{p}\sum_{i=1}^p \psi_{\mathcal{S}_1\cup \mathcal{S}_2}(\vec{x}) \psi_{\mathcal{S}_1 \cup \mathcal{S}_2}(\vec{x})^\top - \mathbb{I}_M} = O_\prec(1)
    \end{equation}
    \item \begin{equation}\label{eq:conc_w}
        \norm{\frac{1}{n}\sum_{i=1}^n \phi_{\mathcal{S}_1 \cup \mathcal{S}_2}(\vec{w}) \phi_{\mathcal{S}_1 \cup \mathcal{S}_2}(\vec{w})^\top - \mathbb{I}_M} = O_\prec(1)
    \end{equation}
    \item
    \begin{equation}
        Z_{3} Z_{3}^\top = c_d \mathbb{I}_d + o_d(1). 
    \end{equation}
\end{enumerate}
    
\end{proposition}
\begin{proof}
Equation \ref{eq:conc_x} is a direct consequence of Lemma \ref{lem:mat_conc} and the hyper-contractivity of the spherical measure. Equation \ref{eq:conc_w} however, requires additional control over the error in $\vec{w}$.

We start with showing that the covariance is well-approximated in expectation
 Let $v \in \mathbb{R}^n, \norm{v}=1$ denote an arbitrary fixed unit vector. Then:
    \begin{equation}        v^\top\Ea{\phi_{\mathcal{S}_1 \cup \mathcal{S}_2}(\vec{w}) \phi_{\mathcal{S}_1 \cup \mathcal{S}_2}(\vec{w})^\top - \mathbb{I}_M}v = \Ea{\sum_{s \in _{\mathcal{S}_1 \cup \mathcal{S}_2}} v^2_s \phi^2_i(\vec{w})}-1,
    \end{equation}
    since $\psi^2_i$ are uniformly lipschitz on $S_d$, applying a taylor expansion on $\vec{w}$ around $u^\star$ yields:
    \begin{equation}
       \Ea{\sum_{s \in \mathcal{S}_1 \cup \mathcal{S}_2} v^2_s \psi^2_s(\vec{w})} =   \Ea{\sum_{s \in \mathcal{S}_1 \cup \mathcal{S}_2} v^2_s \psi^2_s(\tilde{\vec{w}})}+\mathcal{O}(\frac{1}{d^{\delta}})
    \end{equation}
    where we used that $h_v(\vec{w})=\Ea{\sum_{s \in _{\mathcal{S}_1 \cup \mathcal{S}_2}} v^2_s \phi^2_i(\vec{w})}$ is an even polynomial in $\vec{w}$. Therefore, $\Ea{\nabla h_v(\vec{w})}=0$ while $\norm{\Ea{\nabla^2 h_v(\vec{w})}} \leq C$ for some constant $C>0$. Corollary \ref{cor:moment`_cont} then ensures that the second order-term is bounded as $\mathcal{O}(\frac{1}{d^{\delta}})$. Taking supremum over $v$ for $\norm{v}=1$, we obtain:
    \begin{equation}
    \norm{\Ea{\phi_{\mathcal{S}_1 \cup \mathcal{S}_2}(\vec{w}) \phi_{\mathcal{S}_1 \cup \mathcal{S}_2}(\vec{w})^\top - \mathbb{I}_M}} = \mathcal{O}(\frac{1}{d^{\delta}}),
    \end{equation}
for some $\delta > 0$.
    We move on to establishing the concentration of $\Phi_{\mathcal{S}_1}(\vec{w})$. By Equation 28 in \cite{misiakiewicz2022spectrum}, spherical harmonics $Y_{m,k}$ of degree $k \in \mathbb{N}$ admit a basis with the following representing along the cartersian coordinates:
 \begin{equation}\label{eq:harmonic_cartesian}
Y_{\alpha}(\mathbf{w}) = C_{\alpha}^{1/2} h_{\alpha}(w_1, w_2) \prod_{j=1}^{d-2} \left\{ \left( w_1^2 + \dots + w_{d-j+1}^2 \right)^{\alpha_j/2} \tilde{Q}_{\alpha_j}^{(d_j)} \left( \frac{w_{d-j+1}}{\sqrt{w_1^2 + \dots + w_{d-j+1}^2}} \right) \right\},
\end{equation}
where $\alpha \in \mathbb{N}^{d}$ contains at-most $\ell$-non-zero entries. Therefore,
$Y_{\alpha}(\mathbf{w})$ is a polynomial in at-most $\ell$ coordinates in $\vec{w}$ along with the $\ell$ projection norms $r_j = \sqrt{\sum_{i=1}^j w_j^2}$. Applying part $ii,c$ of Theorem \ref{thm:main_pt} then implies that:
 \begin{equation}
     \sup_{i \in d^{\epsilon_1}} \abs{\frac{1}{i}\sum_{j=1}^i(\langle w^\tau_\kappa, w^\star_i \rangle)^2} = \mathcal{O}_\prec(\frac{1}{d^{\epsilon_1}}).
 \end{equation}
 Subsequently:
\begin{equation}
    \abs{Y^{d^{\epsilon_1}}_{\alpha}(\vec{u}_i)-Y^{d^{\epsilon_1}}_{\alpha}(\tilde{\mathbf{u}^\star_i})} = \mathcal{O}_{\prec}(\frac{1}{d^{\delta}}).
\end{equation}
and:
\begin{equation}
    \abs{Y_{\alpha}(\vec{u}_i)-Y_{\alpha}(\tilde{\mathbf{u}^\star_i})} = \mathcal{O}_{\prec}(\frac{1}{d^{\delta}}).
\end{equation}

Taking a union bound over the $\Theta(d^{k\epsilon_1})$ values of  $\alpha$  yields:
\begin{equation}
    \Ea{\max_{\alpha}{Y^2_{\alpha}(\mathbf{w}})} = \tilde{\mathcal{O}}(1).
\end{equation}
For the radial components recall that $\norm{u}, \norm{v} = \mathcal{O}_\prec(1)$ while the radial eigenfunctions are continuous.

Setting $\tilde{\delta} < \delta, \delta'$ and recalling that, $n_2=\Theta(d^{k\epsilon_1+\delta}), p_2= \Theta(d^{k\epsilon_1+\delta'})$ while $\abs{\mathcal{S}_1\cup \mathcal{S}_2} = \Theta(d^{k\epsilon})$, we may apply Lemma \ref{lem:mat_conc} to obtain:
\begin{equation}\label{eq:conc}
\norm{\phi_{\mathcal{S}_1 \cup \mathcal{S}_2}(\vec{w}) \phi_{\mathcal{S}_1}(\vec{w})^\top-\Ea{ \phi_{\mathcal{S}_1 \cup \mathcal{S}_2}(\vec{w}) \phi_{\mathcal{S}_1}(\vec{w})^\top}} = \mathcal{O}_{\prec}(\frac{1}{d^{\delta}})
\end{equation}
where we absorbed the $d^{\tilde{\delta}}$ factor into the $\frac{1}{p}$ factor in the bound in Lemma \ref{lem:mat_conc} (Equation \ref{eq:mat_con_bound}).

The proof of $(iii)$ similarly follows from Propositions 4, 8 in \cite{mei_generalization_2022}. We outline the central steps. First, via the expansion of $\sigma(\cdot)$ given by Equation \ref{eq:main_sigma_decomp}, for any $\vec{x}$, $\Psi_{\mathcal{S}_3}(\vec{x})^\top \Phi_{\mathcal{S}_3}(\vec{w})$ can be expressed 
as $\sigma(\langle \vec{w}, \vec{x} \rangle+b)-\Psi_{\mathcal{S}_1\cup \mathcal{S}_2}(\vec{x})^\top \Phi_{\mathcal{S}_1\cup \mathcal{S}_2}(\vec{w})$. Through Equation \ref{eq:harmonic_cartesian}, $\Psi_{\mathcal{S}_3}(\vec{x})^\top \Phi_{\mathcal{S}_3}(\vec{w})$ therefore depends on $\vec{w}$ only through a finite number of coordinates in $U^\star$.
Analogous to Equation \ref{eq:conc} above, applying Lemma \ref{lem:mat_conc} and using $p>>n$, we obtain that:
\begin{equation}
    \norm{Z_3Z_3^\top - G_3(X,X)}=\mathcal{O}_{\prec}(\frac{1}{d^{\delta}})
\end{equation}
where $G_3(X,X)$ denotes the gram-matrix associated to the Kernel:
\begin{equation}
    K_3(\vec{x},\vec{x'}) = \sum_{s \in \mathcal{S}_3} \lambda_s\psi_s(\vec{x})\psi_s(\vec{x}').
\end{equation},
applied to the data-matrix $X \in \mathbb{R}^{n \times d}$.

The gram-matrix $G_3(X^\star,X^\star)$ now corresponds exactly to the spherical distribution on $U^\star$, with decay identical to the case of spherical data in \cite{mei2022generalization}. Therefore, proposition $8$ in \cite{mei2022generalization} applies, resulting in the bound:
\begin{equation}
    \norm{G_3(X^\star,X^\star)-c_d\mathbf{I}}=\mathcal{O}_{\prec}(\frac{1}{d^{\delta}})
\end{equation}
for some constant $c>0$.
% \textbf{($iii)$}:
% By the heavy-tailed matrix-concentration inequality (Lemma \ref{lem:mat_conc}, we first show that the gram matrix is well approximated by the Kernel:
% \begin{equation}
%     \norm{Z_{3}Z_{3}^\top- G}= \mathcal{O}(\frac{1}{d^{k\epsilon_1}})
% \end{equation}

% Let $G_{\perp}$ denote the corresponding off-diagonal matrix:
% \begin{equation}
%     G_{\perp}(i,j) =  K_{\mathcal{S}_3} \mathbf{1}_{i \neq j}
% \end{equation}

% The contribution of the above matrix is controlled through the following proposition:
% \begin{proposition}
%     \begin{equation}
%         \Ea{\norm{ G_{\perp}(i,j)}} = \mathcal{O}(\frac{1}{d^{\frac{\epsilon_1}{2}}})
%     \end{equation}
% \end{proposition}
% \begin{proof}
% We have:
% \begin{equation}
%    (n(n-1)) \Ea{G^2_{\perp}(i,j)} = \mathcal{O}(\lambda(G)^2).
% \end{equation}
% Since the operator norm is dominated by the Frobenius norm, we obtain:
% \begin{align*}
%     \norm{G_{\perp}(i,j)} \leq \norm{G_{\perp}(i,j)}_F &\leq \sqrt{(n(n-1)) \Ea{G^2_{\perp}(i,j)}}\\
%     & \leq \mathcal{O}(\frac{1}{d^{\frac{\epsilon_1}{2}}})
% \end{align*}
% \end{proof}

\end{proof}

\subsection{Properties of the Feature-covariance Matrix}

Having established Proposition \ref{prop:orth_decomp} and the concentration of the top eigenvectors, the setting of $Z$ is now reduced to the spike + ``bulk" structure in the proof of Theorem $1$ in \cite{mei_generalization_2022} with $\Theta(d^{k\epsilon_1})$ spikes arising from the eigenfunctions $\mathcal{S}_1, \mathcal{S}_2$ corresponding to near-identity sample-covariances and a
remaining bulk with uniformly-bounded operator norm. Therefore, the proofs of Propositions $6,7$ in \cite{mei_generalization_2022}, based on perturbation inequalities for singular values, singular vectors, result in the following estimates for $Z$:

% The above decomposition translates to a precise description of the spectral decomposition of $Z$, following the decomposition utilized in Proposition 6 of \cite{mei_generalization_2022}:

\begin{proposition}\label{prop:cov_mat_struct}
$Z$ admits a singular value decomposition 
\begin{equation}
    Z = U_1S_1V_1^\top + U_2S_2V_2^\top +U_3S_3V_3^\top,
\end{equation}
such that:
\begin{enumerate}
    \item  $\sigma_{\text{min}}(S_1 \cup S_2) = \Theta_d(\frac{1}{d^{k\eps_1}})$
    \item $\norm{S_3-c_3\mathbf{I}} = o_{d,p}(1)$,
    for some constant $c_3>0$.
    \item $\Psi_{S_1 \cup S_2}^\top U_3=o_d(\sqrt{n_2})$ and $\Phi_{S_1 \cup S_2}^\top V_3=o_d(\sqrt{p_2})$
    \end{enumerate}
\end{proposition}








The proof of the above Proposition follows directly through Proposition 6 in \cite{mei_generalization_2022}. The above result exactly charaterizes the projections of 
functions onto pre-conditioned features:
\begin{proposition}\label{prop:proj_app}
    For any $g:\mathbb{R}^d \rightarrow \mathbb{R}$ such that the projections onto radial components of degree $>2$ are $o_d(1)$:
    \begin{equation}
 Z(\vec{x})(\frac{1}{n}Z^\top Z)^{-1} Z^\top g(X) = \mathcal{P}_{S_1}  g(\vec{x}) + \mathcal{O}_\prec(\frac{1}{d^{\delta'-\delta}}).
    \end{equation}
\end{proposition}

The proof of part $(ii)$ of Theorem \ref{thm:main_theorem} is then completed by showing that under Assumption \ref{ass:target}, the projection onto $\mathcal{S}_1$ is exactly along $h^\star(\vec{x})$:

\begin{proposition}
    Under Assumption \ref{ass:target}:
    \begin{equation}
        \mathcal{P}_{S_1}  g(\vec{x}) = \mu_1 h^\star(\vec{x})+o_d(1).
    \end{equation}
\end{proposition}
\begin{proof}
By Assumption \ref{ass:bad} and the composition of Hermite decompositions (Lemma \ref{prop:hermite_comp}), the non-vanishing terms along the radial component $\norm{x}^2-1$ consists of total input-degree-$2$ and $2k$ while the remaining terms on the complement of $h^\star_\ell$ have degree at least $3(k+1) > k$. $\mathcal{S}_1$ therefore consists exactly of the subspace with effective degree $k$.
\end{proof}

% Next, we show that $\sigma_{\min}(\Psi \Psi^\top) = \Theta(\frac{1}{d^{2k\epsilon_1}})$ and thus $\norm{(\Psi \Psi^\top)^{-1}} =\mathcal{O}(d^{2k\epsilon_1})$. Moreover, restricted to the $> k$ degree part, the norm of $(\Psi \Psi^\top)^{-1}$ is $\mathcal{O}(1)$, while the norm of the $> k$ degree part of $\Psi^\top (f^\star(X)-\hat{f}(X))\sigma'(h^{t}_{2,i}(X)$ is $\mathcal{O}(\frac{1}{d^{(2(k+1)\epsilon_1}})$

% Finally, the errors in the projections along the degree $\leq k$ components are of order $\mathcal{O}(\frac{1}{\sqrt{p_1}}+\frac{1}{\sqrt{n}})$ which can be supressed w.r.t the initial values of the projections.

% We split the dynamics into components along the linear directions $\frac{1}{\sqrt{r}} \sum_{\ell=1}^r h^\star_l$, an orthogonal basis along $h^\star_l$, and the linear features $W^\star \vec{x}$.

% \begin{proposition}
%     There exists $\eta > 0$, $\epsilon > 0$ such that the pre-activations after a single step satisfy for all $i$ outside a vanishing subset:
%     \begin{equation}
%         h^{t+1}_{2,i}(\vec{x}) = cw_{3,i}\frac{1}{\sqrt{r}} \sum_{\ell=1}^r h^\star_l + o_d(1),
%     \end{equation}
% for some constant $c(\eta, \epsilon) > 0$.
% \end{proposition}
% The proof splits into the following major components:
% \begin{enumerate}
%     \item Every update to $W_2$ along relevant eigenvectors of the gram-matrix translates to an update on $h_2(\vec{x})$ towards $h^\star(\vec{x})$. 
% \end{enumerate}

% For any $i \in \mathcal{S}_{\eta_1,\eta_2}$, define:
% \begin{equation}
%     \psi_{i, \leq k}(\vec{x}) = [\text{He}_1(W^\star \vec{x})^\top, \text{He}_2(W^\star \vec{x})^\top, \cdots, \text{He}_k(W^\star \vec{x})^\top].
% \end{equation}




% \subsection{Repulsion terms}

% Next, we show that the presence of the repulsion terms ensures that the components along the symmetric directions and $W^\star \vec{x}$ do not explode:
% \begin{lemma}
%     $\exists M >0$ such that if $\abs{\langle h^\ell_i , h^\star \rangle} > M$, then $\Delta \abs{\langle h^\ell_i , h^\star \rangle} < 0$.
% \end{lemma}




\subsection{Proof of Proposition \ref{prop:proj_app}}

Let $\hat{g}(\vec{x})=  Z(\vec{x})^\top(ZZ^\top)^{-1} Z^\top g(X)$. Proposition \ref{prop:proj_app} is equivalent to $ \norm{\hat{g}(\vec{x}-g(\vec{x})}^2_2 = o_d(1)$. Expanding, we obtain:
\begin{equation}
    \norm{\hat{g}(\vec{x})-\mathcal{P}_k g(\vec{x})}^2_2 = \norm{g(\vec{x})}^2 - 2 \langle \hat{g}(\vec{x}),\mathcal{P}_{\mathcal{S}_1} g(\vec{x})\rangle + \norm{ \hat{g}(\vec{x})^2}.
\end{equation}

It therefore suffices to show that:
\begin{equation}
    \langle \hat{g}(\vec{x}), \mathcal{P}_{\mathcal{S}_1} g(\vec{x})\rangle \rightarrow \norm{\mathcal{P}_{\mathcal{S}_1} g(\vec{x})}^2,
\end{equation}
and:
\begin{equation}
    \norm{\hat{g}(\vec{x})^2} \rightarrow \norm{\mathcal{P}_{\mathcal{S}_2} g(\vec{x})}^2.
\end{equation}

Let $g_{\mathcal{S}_1}$ denote the vector with components:
\begin{equation}
    g_{\mathcal{S}_1} \coloneqq [\Ea{g(\vec{x})\Psi_{s}(\vec{x})}:s \in \mathcal{S}_1],
\end{equation}
Let $\Lambda_{\leq 2, k}$ denote the diagonal matrix with the corresponding eigenvalues.

Then the above terms can be expressed as:
\begin{equation}\label{eq:term_1}
     \langle \hat{g}(\vec{x}), \mathcal{P}_k g(\vec{x})\rangle  = g_{\mathcal{S}_1} D_{\mathcal{S}_1}\Phi_{\mathcal{S}_1}^\top  (\frac{1}{n} Z{Z}^\top)^{-1}Z^\top g(X),
\end{equation}
and:
\begin{equation}\label{eq:term_2}
     \norm{\hat{g}(\vec{x})^2}  = g(X)^\top  Z(\frac{1}{n} Z{Z}^\top)^{-1}\Sigma (\frac{1}{n} Z{Z}^\top)^{-1}Z^\top g(X),
\end{equation}
where $\Sigma$ denotes the feature covariance:
\begin{equation}
    \Sigma = \frac{1}{p_2} \Ea{Z(\vec{x})Z(\vec{x)})^\top} = \frac{1}{p_2}\sum_{j_1,j_2,k_1,k_2} \Phi_{j_1,j_2,k_1,k_2}(W) \Phi_{j_1,j_2,k_1,k_2}(W)^\top
\end{equation}
To compute the above terms, we use Proposition \ref{prop:cov_mat_struct} to estimate certain intermediate quantities similar to Proposition 7 in \cite{mei_generalization_2022}: 
\begin{proposition}
Under the setup of Theorem \ref{thm:main_theorem}, with the decomposition of eigenfunctions specified by Equation \ref{eq:mat_decomp} :
\begin{enumerate}
    \item 
    \begin{equation}\label{eq:prop_1}
        \psi^\top_{\mathcal{S}_1} Z(\frac{1}{n}Z^\top Z)^{-1} \phi_{\mathcal{S}_1}D_{\mathcal{S}_1} = \mathbb{I}_{m_1}+\mathcal{O}_\prec(1)
    \end{equation}
    \begin{equation}\label{eq:prop_2}
D_{\mathcal{S}_1}\phi^\top_{\mathcal{S}_1} Z(\frac{1}{n}Z^\top Z)^{-1} Z^\top \frac{1}{p_2}f_{\mathcal{S}_3} = \mathcal{O}_\prec(1)
    \end{equation}
    \begin{equation}\label{eq:prop_3}
\norm{\psi^\top_{\mathcal{S}_1} Z(\frac{1}{n}Z^\top Z)^{-1} \phi_{\mathcal{S}_1}D_{\mathcal{S}_1}} = \mathcal{O}_\prec(\frac{1}{\sqrt{n}})
    \end{equation}
    
    \item \begin{equation}\label{eq:prop_4}
        \frac{n}{p_2}\norm{\Sigma_{\mathcal{S}_3}}=o_d(1)
    \end{equation}
    \item \begin{equation}\label{eq:prop_5}
    \norm{\Psi_{\mathcal{S}_2} f^\star(X)}=o_d(1),
\end{equation}
    
\end{enumerate}
\end{proposition}
 

% While Proposition \ref{prop:cov_mat_struct} implies that:
% \begin{equation}
%      \norm{\mathcal{P}_{S_3}f^\star(\vec{x}) \Phi_{S_3}} = \mathcal{O}_d(\frac{1}{\sqrt{d^{k\epsilon_1}}})
% \end{equation}

Under the above proposition, the terms given by Equations \ref{eq:term_1}, \ref{eq:term_2} simplify as follows:
\begin{align*}
g_{\mathcal{S}_1} D_{\mathcal{S}_1}\Phi_{\mathcal{S}_1}^\top  (\frac{1}{n}Z{Z}^\top)^{-1}Z^\top g(X) &= g_{\mathcal{S}_1} D_{\mathcal{S}_1}\Phi_{\mathcal{S}_1}^\top  (\frac{1}{n} Z{Z}^\top)^{-1}Z_{1,2}^\top g_{1,2}(X)\\&+g_{\mathcal{S}_1} D_{\mathcal{S}_1}\Phi_{\mathcal{S}_1}^\top  (\frac{1}{n} Z{Z}^\top)^{-1}Z_{1,2}^\top g_3(X)+g_{\mathcal{S}_1} D_{\mathcal{S}_1}\Phi_{\mathcal{S}_1}^\top  (\frac{1}{n} Z{Z}^\top)^{-1}Z^\top_3 g(X) 
\end{align*}
By Equation \ref{eq:prop_1}, the first term converges to $\norm{\mathcal{P}_{\mathcal{S}_1} g(\vec{x})}^2$ while the other two terms are bounded as $\mathcal{O}_\prec(\frac{1}{d^{\delta}})$ by Equations \ref{eq:prop_2},\ref{eq:prop_3} respectively.

Similarly,
\begin{align*}
    g(X)^\top  Z(\frac{1}{n} Z{Z}^\top)^{-1}\Sigma (\frac{1}{n} Z{Z}^\top)^{-1}Z^\top g(X) &= g(X)^\top  Z(\frac{1}{n} Z{Z}^\top)^{-1}\Sigma_{1,2} (\frac{1}{n} Z{Z}^\top)^{-1}Z^\top g(X)\\&+g(X)^\top  Z(\frac{1}{n} Z{Z}^\top)^{-1}\Sigma_{3} (\frac{1}{n} Z{Z}^\top)^{-1}Z^\top g(X)
\end{align*}

By Equation \ref{eq:prop_3}, the second term is bounded as $\mathcal{O}_\prec(1)$. This completes the proof of part $(ii)$ of Theorem \ref{thm:main_pt}.
% Express $\Psi^j_t(\vec{x})$ as:
% \begin{equation}
%     \Psi^j_t = \sum_{k=1}^\lambda \lambda_k \phi_k(\vec{x}) \phi_k(\vec{w})
% \end{equation}






% \subsection{Fitting the third Layer}

% We proceed with a standard analysis based on Lemma 11 and Corollary 4 in \cite{damian2022neural}. We utilize the asymptotic normality of $h^\star_1(\vec{x}), \cdots, h^\star_r(\vec{x})$ along with the normality of $W^\star \vec{x}$ 

% \subsection{Characterization of the spiked Gram Matrix}

% We begin by expanding $\psi^1(\vec{x})$:
% \begin{equation}
%     \sigma(\langle \vec{w},\vec{x} \rangle) = \sum_{k=1}^\infty c_k(\norm{w}) \text{He}_k(\langle \frac{\vec{w}}{\norm{w}}, \vec{x} \rangle),
% \end{equation}
% where $\text{He}_k$ denote the Hermite polynomials. The crucial observations then are:
% \begin{enumerate}
%     \item In the Gram matrix (upon averaging over $w$), $ \text{He}_k(\langle \frac{\vec{w}}{\norm{w}}, \vec{x} \rangle)$ contribute to terms only upto degree $k$.
%     \item $\text{He}_k(\langle \frac{\vec{w}}{\norm{w}}, \vec{x} \rangle)$ are othonormal w.r.t the $L_2$-space induced by $\vec{x}$.
%     \item The contributions from $\text{He}_k$ are of order $\mathcal{O}(\frac{1}{d^k})$.
% \end{enumerate}
% Therefore, Proposition \ref{prop:matrix_conc} applies. 

% The only remaining issue is that $\text{He}_k(\langle \frac{\vec{w}}{\norm{x}}, \vec{x} \rangle)$ are not orthogonal w.r.t $\vec{w}$ for even-$k$ (They remain orthogonal for odd-$k$).
% Therefore, even in expectation, the covariance of $\text{He}_k(\langle \frac{\vec{w}}{\norm{w}}, \vec{x} \rangle)$ is not identity. (This is an issue even when $\norm{w}=1$)
% We show however, that for even $k$, this only results in spurious spikes that can be safely ignored. These spikes can allow quicker learning of certain norms but for random $a^\star$ any projection on such norms can be neglected.




% \begin{lemma}
% Under assumptions..., we have, for all ${i \in [N(d)]} U_{d, > > (d)}(\btheta_i, \btheta_i)$:
% % \begin{equation}
% %     U_{d, > (d)}(\vec{\theta}_i, \vec{\theta}_i) =~ O_{d, P}(N(d)^\delta) \cdot \mathbb{E}_{\bf \theta}[U_{d, > \evN (d)}(\bf \theta, \bf \theta)]
% % \end{equation}
% \end{lemma}



\subsection{Proof of part $(iii)$: Fitting the Target}

Upon the completion of part $(ii)$, 
the second-layer pre-activations $h_2(\vec{x})=W_2\sigma(W_1 \vec{x})$ are approximately equivalent to those of a random feature-mapping applied to the scalar input $h^\star(\vec{x})$, with the random weights of the feature mapping given by $\tilde{w}=cw_3$, with $c=\eta\sigma'(0)$ as in Proposition \ref{prop:2update}. Hence, we introduce the Kernel $K(\cdot, \cdot):\mathbb{R}^2 \rightarrow \mathbb{R}$:
\begin{equation}\label{eq:def_K}
    K(z_1,z_2) \coloneqq \Eb{w\sim \mathcal{N}(0,1)}{\sigma(c w z_1+b)\sigma(c w z_2+b)}.
\end{equation}

For $Z \in \mathbb{R}^{n}$, we further denote by $K(Z,Z)$ the corresponding gram-matrix $K(Z,Z) \in \mathbb{R}^{n \times n}$, with entries
\begin{equation}
    K(Z,Z)_{i,j} =  K(z_i,z_j). 
\end{equation}

Let $\mathcal{H}_K$ denote the RKHS corresponding to the Kernel $K$. Since the moments of $h^\star(\vec{x})$ are uniformly bounded in $d$, we obtain:

\begin{proposition}\label{prop:krr}[\cite{caponnetto2007optimal}]
For any $\delta > 0$, and large enough $d$, $\exists$ constants $c,C$ such that with $\lambda=\Theta(\sqrt{n})$:
    \begin{equation} \norm{k(h^\star,H^\star)(\frac{1}{n}K(H^\star,H^\star)+\lambda \mathbb{I})^{-1} \frac{1}{\sqrt{n}}g^\star(H^\star)-g^\star(h^\star)}^2_2-\inf_{f \in \mathcal{H}_K}\left[\norm{f-g^\star(h^\star)}^2_2 + \lambda \norm{f}_{\mathcal{H}_K}\right]\leq C\frac{N_K(\lambda)\log (\frac{1}{\delta})^c}{n},
    \end{equation}
    where $H^\star \in \mathbb{R}^{N}$ contains independent samples $h^\star(\vec{x})$, and $N_K(\lambda)$ denotes the ``effective-dimension":
    \begin{equation}
        N_K(\lambda) = \operatorname{Tr}[(K+\lambda)^{-1}K],
    \end{equation}
    which admits the following trivial bound:
    \begin{equation}
        N_K(\lambda) \leq \frac{\operatorname{Tr}[K]}{\lambda}
    \end{equation}
\end{proposition}


We next translate the above bound into generalization error through a control of the approximation error term.

Note that the uniform bounds on the moments of $h^\star(\vec{x})$ and Markov's inequality, for any $\epsilon > 0$, $\exists R_\epsilon > 0$ such that for large enough $d$:
\begin{equation}
    \Pr[h^\star(\vec{x}) \notin B_{R_\epsilon}] \leq \epsilon
\end{equation}

Next, define the following class of functions:
\begin{equation}
   \mathcal{F}_\epsilon = \{f \in \mathcal{H}_K: \text{supp}(f) \in B_{R_\epsilon}\}.
\end{equation}

Then:
\begin{equation}
\inf_{f \in \mathcal{H}_K}\left[\norm{f-g^\star(h^\star)}^2_2 + \lambda \norm{f}_{\mathcal{H}_K}\right] \leq \inf_{ f \in \mathcal{F}_\epsilon}\left[\norm{f-g^\star(h^\star)}^2_2 + \lambda \norm{f}_{\mathcal{H}_K}\right]
\end{equation}

Restricted to the compact set $B_{R_\epsilon}$, universality of random feature Kernels with non-polynomial, polynomially-bounded activations \cite{sun2018approximation} implies that for any $\epsilon > 0$, $\exists f_\epsilon \in \mathcal{H}_K$ such that:
\begin{equation}
    \inf_{ f \in \mathcal{F}_\epsilon}\left[\norm{f-g^\star(h^\star)\mathbf{1}_{h^\star \in B_{R_\epsilon}}}^2_2 + \lambda \norm{f}_{\mathcal{H}_K}\right] \leq \epsilon + \lambda \norm{f_\epsilon}^2_{\mathcal{H}_K}.
\end{equation}

Therefore, by setting $\lambda$ small enough such that:
\begin{equation}
     \lambda_\epsilon \norm{f_\epsilon}^2_{\mathcal{H}_K} \leq \epsilon,
\end{equation}
we otbain:
\begin{equation}
   \inf_{ f \in \mathcal{F}_\epsilon}\left[\norm{f-g^\star(h^\star)}^2_2 + \lambda \norm{f}_{\mathcal{H}_K}\right] \leq 2\epsilon+\norm{g^\star(h^\star)\mathbf{1}_{h^\star \notin B_{R_\epsilon}}}^2_2.
\end{equation}
By Cauchy-Shwartz and the uniform bound on $\Ea{g^\star(h^\star)^2}$, the last term in the RHS is bounded by $C\epsilon$ for some cosntant $C>0$. 

Subsequently, we may set $n$ in Proposition \ref{prop:krr} large enough such that:
\begin{equation}
    C\frac{\operatorname{Tr}[K]\log (\frac{1}{\delta})^c}{\lambda n} \leq \epsilon,
\end{equation}
Implying that for small enough $\lambda(\epsilon)$ and large enough $n(\epsilon,\delta)$, with probability $1-\delta$:
\begin{equation} \norm{k(h^\star,H^\star)(K(H^\star,H^\star)+\lambda \mathbb{I})^{-1} g^\star(H^\star)-g^\star(h^\star)}^2_2\leq C\epsilon,
\end{equation}
for some constant $C>0$.

Now, returning to the true features $h^2(\vec{x})$, it remains to combine the above estimate with the concentration of the gram-matrix to the associated Kernel. This is established similar to the proof of Proposition \ref{prop:orth_decomp} through Lemma \ref{lem:mat_conc}. 

Note that the above argument does not yield the dependence of $\lambda, n$ on $\epsilon$. Such an explicit dependence requires finer control on the approximation, source terms. For such an analysis, we refer to the explicit rademacher complexity based bounds for ReLU activation in \cite{damian2022neural}.


% Next, since $h_{2,i}(\vec{x})$ for $i\in [p_2]$ satisfies:
% \begin{equation}
%     h_{2,i}(\vec{x}) = c w_3 h^\star(\vec{x}) + \mathcal{O}_{\prec}(1).
% \end{equation}
% For a fixed $N \in \mathbb{N}$, let $H \in \mathbb{R}^{N \times p_2}$ denote the matrix of second-layer pre-activations.


% For any fixed $n,\lambda$, the error $
% \mathcal{O}_\prec(\frac{1}{d^{\delta}})$ in part $(ii)$ combined with the Lindeberg estimate (Lemma \ref{lem:lind}) suffices to obtain arbitrarily small error for $\hat{f}(\vec{x})$, thus proving $iii$. 

We remark that more quantitave estimates can be obtained through rademacher-complexity based analysis for specification activations such as Relu \citep{damian2022neural}.


\subsection{Relaxing Assumption \ref{ass:bad}}
\label{sec:app:ass_bad}
In this section, we adress the requirement of Assumption \ref{ass:bad} and steps towards relaxing it. Assumption \ref{ass:bad} simplifies our analysis by ensuring that $\mathcal{P}_{\mathcal{S}_1} f^\star(\vec{x})$ is exactly $h^\star(\vec{x})$ arising from the first-order Hermite coefficient of $g^\star(\vec{x})$. In general, however, the degree-$k$ approximation of $f^\star(\vec{x})$ may contain additional components involving higher-degree dependence on $h^\star(\vec{x})$. For instance, if $g^\star(\vec{x})$ has a non-zero second-order Hermite coeffient, then Lemma \ref{prop:hermite_comp} implies that $\operatorname{He}_2(h^\star(\vec{x}))$ can be decomposed into components of Hermite degree $4,4,\cdots, 2k$. Therefore, if $k\geq 4$, gradient updates to $W_2$ result in $h_2(\vec{x}) \approx c (h^\star + \text{higher order components})$. While ideally one would hope that the learning of such additional components would only help towards fitting $f^\star(\vec{x})$ by $w_3$, this would require the second-layer pre-activations to disentangle $h^\star$ and the remaining components i.e. to specialize across non-linear features. Analysis of such a specialization remains challenging due to the reasons described in Appendix \ref{sec:heuristic}. Therefore, relaxing Assumption \ref{ass:bad} requires going beyond the single-spike ($r=1$) non-linear feature learning.

Additionally, as we saw through the decomposition of the activation into radial and spherical arguments
(Equation \ref{eq:main_sigma_decomp}), the radial components exhibit slower-decay w.r.t the degree. Therefore, $d^{k \epsilon}$ samples, neurons suffice towards learning degree-$k$ components on $\frac{1}{\sqrt{d^{\epsilon_1}}}\norm{x^\star}^2$ which correspond to degree $2k$ components on $\vec{x}$. We believe this to be an artifact of our choice $a^\star=1$, which leads to a special dependence along the radial component. Going beyond the isotropic $a^\star=1$ setting is however, challenging due to our reliance on diagonalization of the associated Kernel along a fixed basis.

% \subsection{Necessity of updating the first layer}

% We show that solely updating the second layer with $n,p = \mathcal{O}(d^{3-\epsilon})$ fails to learn $\langle \vec{w}^\star, \vec{z} \rangle$ with a finite number of gradient steps.

% As shown in \cite{nichani2024provable}, the gradients in second and deeper layers have weaker signal due to the non-isometry of the random features. The sample efficiency can be boosted by directly updating the second layer through ridge regression. However, this fails to achieve a correlation with $\langle\vec{w}^\star, \vec{x}\rangle$

% \subsection{Backward-Feature correction}

% Now, consider the target:
% \begin{equation}
%     f^\star(\vec{x}) = (\langle \vec{w}^*, \vec{x} \rangle) + He_3(\langle \vec{w}^*, \vec{x} \rangle)^3 (\vec{x}^\top A \vec{x}-\Ea{\vec{x}^\top A \vec{x}}) + He_5(\vec{x}^\top A \vec{x}-\Ea{\vec{x}^\top A \vec{x}})(\langle \vec{w}^*_2, \vec{x} \rangle),  
% \end{equation}

% The first layer learns $\vec{w}^*_2$ only after the first layer has learned $\vec{w}^*$ and the second layer has learned $\vec{x}^\top A \vec{x}$

% \subsection{Gradient step on the second layer and role of the feature Kernel}

% We first perform a gradient step on $W^{i}_2$ with batch-size and neurons $p=\mathcal{O}(d^\kappa),\mathcal{O}(d^\kappa)$. The role of $\kappa> 0$ will become apparent later. 



% Let $\mathcal{H}_K$ denote the RKHS defined by $K$. We obtain:
% \begin{equation}\label{eq:Kernel-update}
%     \langle W^{1}_{i,2}, \sigma(W_1(\vec{x}))\rangle \approx \langle W^{0}_{i,2}, \sigma(W_1(\vec{x}))\rangle+ \eta \Eb{\vec{x}'}{f^\star(\vec{x}') a_i \sigma'(\langle W^{0}_{i,2}, \sigma(W^{0}_1(\vec{x}'))\rangle) K(\vec{x}',\vec{x})}
% \end{equation}


% The above observation underlies the analysis in \cite{nichani2024provable} who showed that the right choice of $K$ can allow the updates to the second and third layer to efficiently fit certain target functions.

% The advantage of introducing the Kernel $K$ is that instead of considering $W^{1}_{i,2}$ as a vector in $\R^{p_1}$, we interpret it as parameterizing a function $\Psi: \R^d \rightarrow \R$ defined as:
% \begin{equation}
%     \psi_{W^{1}_{i,2}}(\vec{x}) =  \langle W^{1}_{i,2}, \sigma(W^{0}_1(\vec{x}))\rangle
% \end{equation}

% In the limit $n, p_1 \rightarrow \infty$, by Riesz representation theorem, each neuron $W^{i}_2$ can be mapped to an element of $\mathcal{H}_K$ \citep{bach2023learning}.




\section{Deeper networks: Proof of Theorem \ref{thm:multi-layer}}\label{app:multiple_layers}

In this section, we discuss the main challenges that arise when extending the arguments for proving the main Theorem~\ref{thm:main_theorem} to MIGHT functions ($r>1$) and general depth targets ($\ell >3$). 

By the independence and asymptotic Gaussianity of the features $\vec h^\star_{\ell}(\vec{x})$ we expect the above result to extend to a general number of layers. However, proving such a result in its full-generality requires accounting for the non-asymptotic rates for the tails of $\vec h^\star_{\ell}(\vec{x})$ and the associated kernels.

Instead, we prove a weaker result corresponding to the hierarchical weak-recovery of a single non-linear feature at a general level of depth, given by Theorem \ref{thm:multi-layer}.



% \begin{theorem}[Informal]
% Let $f^\star(\vec{x})$ be a target as in Equation \ref{eq:target_def_deep} with $r=1$. Consider a student model of the form 
% $\hat{f}_\theta(\vec{x}) = \vec{w}_L^\top \sigma(W_{L-1} \sigma( W h^\star_{L-2}(\vec{x})))$ with $W \in \mathbb{R}^{p \times d^{\epsilon_{L-2}
% }}$ having rows independently sampled as $w_i \sim U(\mathcal{S}(1))$
% i.e a model with the ${L-2}_{th}$ layer having perfectly recovered $h^\star_{L-2}(\vec{x})$.
% Then, a single  step of pre-conditioned SGD on $W_{L}$ with batch-size $\Theta(d^{{k\epsilon_\ell}+\delta})$ results in $h_{L-1}(\vec{x}) \coloneqq W_{L-1} \sigma( W h^\star_{L-1}(\vec{x}))$ recovering $h^\star_{L-1}(\vec{x})$.
% \label{th:multi}
% \end{theorem}

The central tool underlying our proof is a propagation of hyper-contractivity through the layers:


\begin{proposition}[Propagation of Hyper-contractivity]\label{prop:hyper_prop}
    Let $f:\mathbb{R} \rightarrow \mathbb{R}$ be a polynomial of finite-degree $k$. Then, for any $\ell \in \mathbb{N}$:
    \begin{enumerate}
        \item $h^\star_\ell(\vec{x})= \mathcal{O}_\prec(1)$.
        \item For any $\delta>0$: $\Ea{\abs{h^\star_\ell(\vec{x})}} = \mathcal{O}(\frac{1}{\sqrt{d^{\epsilon_\ell-\delta}}})$
        \item $\Ea{\norm{h^\star_\ell(\vec{x})h^\star_\ell(\vec{x})^\top-\mathbb{I}_{p_\ell}}}=\mathcal{O}(\frac{1}{\sqrt{d}}).$
        
    \end{enumerate}
\end{proposition}
\begin{proof}
    The proof proceeds by induction. For $\ell=1$, the statements hold by Gaussian-hypercontractivity (Lemma \ref{lem:hyper}) since $\text{He}_k$ for distinct $w^\star_i$ are uncorrelated, zero-mean random variables and thus $h^\star_2(\vec{x})$ has all moments of bounded order.
    
    Suppose the statements hold for some $\ell \in \mathbb{N}$. Applying Lemma \ref{lem:lind}, we obtain:
    \begin{equation}\label{eq:prop-bound}
\Ea{\abs{\text{He}_k(h^\star_\ell)}} = \mathcal{O}(\frac{d^{\delta}}{\sqrt{d^{\epsilon_\ell}}})
    \end{equation},
    for any $\delta > 0$.
    Subsequently, applying \ref{lem:stoch-dom-mean} leads to the following propagation of tails:
    \begin{align*}
         {h}^\star_{\ell+1,m}(\vec{x}) &= \frac{1}{\sqrt{d^{\epsilon_{\ell-1}-\epsilon_{\ell}}}}\vec a^{\star^\top}_{\ell,m} P_{k, m, \ell}\left({\bf h}^\star_{\ell-1,\{1+(m-1)d^{\epsilon_{\ell-1}-\epsilon_{\ell}},\ldots, md^{\epsilon_{\ell-1}-\epsilon_{\ell}}  \}}(\vec{x})\right)\\
         &=\frac{1}{\sqrt{d^{\epsilon_{\ell-1}-\epsilon_{\ell}}}} \sum_{i=1}^{\sqrt{d^{\epsilon_{\ell-1}-\epsilon_{\ell}}}} \mathcal{O}_\prec(1) = \mathcal{O}_\prec(1),
    \end{align*}
where we used the bound $h^\star_{\ell}(\vec{x})=\mathcal{O}_\prec(1)$ by the induction hypothesis. By Lemma \ref{lem:stoch-dom-mean} and Equation \ref{eq:prop-bound}, for any $\delta > 0$, we obtain that:
\begin{equation}
    \Ea{\abs{h^\star_{\ell+1,m}(\vec{x})}}= \mathcal{O}(\sqrt{d^{\epsilon_{\ell}-\epsilon_{\ell+1}}} \frac{1}{\sqrt{d}^\epsilon_{\ell}}) = \mathcal{O}(\frac{1}{\sqrt{\epsilon_{\ell+1}}}).
\end{equation}
\end{proof}

The above proposition establishes that the hidden features $h^\star_\ell(\vec{x})$ maintain errors in means $\mathcal{O}_\prec(\frac{1}{\sqrt{d}^{\epsilon_\ell}})$ and preserve tails of the form $\mathcal{O}_\prec(1)$.
 Theorem \ref{thm:multi-layer} then follows by noting that the above error bounds suffice for Proposition \ref{prop:orth_decomp} to hold for the feature-matrix $\sigma(Wh^\star_{L-1}(\vec{x}))$. Concretely, $h^\star_\ell(\vec{x})= \mathcal{O}_\prec(1)$ ensures that Lemma \ref{lem:mat_conc} applies while the errors in means, covariances suffice for the expected covariance of spherical harmonics to converge to $\mathbf{I}$.
We again write:
\begin{equation}
    \sigma(Wh^\star_{L-1}(\vec{x})) = \Psi_{\mathcal{S}_1}(h^\star_{L-1}(\vec{x}))\Phi(W)_{\mathcal{S}_1}^\top + \Psi_{\mathcal{S}_3}(h^\star_{L-1}(\vec{x}))\Phi(W)_{\mathcal{S}_3}^\top,
\end{equation}

Unlike Proposition \ref{prop:orth_decomp} that involved approximations in $W$, the above decomposition involves approximating $h^\star_{L-1}(\vec{x})$ through equivalent Gaussian-inputs $\vec{x}$. The proof follows that of Proposition \ref{prop:orth_decomp}, with
Proposition \ref{prop:hyper_prop} implying that:
\begin{equation}
    \norm{\Psi_{\mathcal{S}_1}(h^\star_{L-1}(\vec{x}))\Psi_{\mathcal{S}_1}(h^\star_{L-1}(\vec{x}))^\top - \mathbb{I}_{M}} = \mathcal{O}_\prec(\frac{1}{\sqrt{d^{\epsilon_1}}}).
\end{equation}
% However, proving such a result requires accounting for the non-asymptotic rates for the tails of $\vec h^\star_{\ell}(\vec{x})$ and the associated kernels. We conjecture that, under the push-forward measure induced by the features $\vec h^\star_{\ell}(\vec{x})$, the associated conjugate kernel is isotropic w.r.t the degree-$k$ components for any $k \in \mathbb{N}$. Under this conjecture, we show that layer-wise training inductively recovers features of increasing levels hierarchically.
% \begin{proof}
%     The proof of the above result consists of the following steps:
%     \begin{enumerate}
%         \item We first show that under the equivalent Gaussian model with $h^\star_{L-1}$ replaced by $z \sim \mathcal{N}(0,I_{d^\epsilon_{L-2}}$, for $n>>p$, the projection operation: 
%         \begin{equation}
%              \Delta h_{L_1}(\vec{z}) = \eta \sigma(W_{L-1} \vec{z})^\top\sigma(W_{L-2} \vec{Z}) \sigma'(h_{L-2}(\vec{Z})) \odot f^\star(\vec{Z})
%         \end{equation}
%         can be approximated along the basis of matrices of the form:
%         \begin{equation}
%      \text{He}_k(W^\top W),
%         \end{equation}
%         corresponding to components along increasing degrees. Such a decomposition underlies the general equivalence principle behind inner-product Kernels in polynomial regimes, developed recently in \cite{lu2022equivalence}.

%         \item Since $h^\star_{L-1}(\vec{x})$ are independent and identically distributed, there exists a sequence of orthonormal polynomials bases: $q^1_d, q^2_d, \cdots$ w.r.t the pushforward measure induced by the components of $h^\star_{L-1}(\vec{x})$. This results in the following sequence of decompositions:
%         \begin{equation}
%             \sigma ( \langle w_i, h^\star_{L-1}(\vec{x})\rangle = \sum_{k=0}^\infty c_k\sum_{S \in \binom{d}{k}} w_{s_i}\cdots w_{s_k} q^1_d(h_i)\cdots q^1_d(h_k)
%         \end{equation}.
%         \item The resulting covariance matrix can be decomposed along the gram matrix of the tensor product vectors $w_i^{\otimes k}$.
%         \item By symmetry, the number of coefficients is finite and $c_k \rightarrow \mu_k$, where $\mu_k$ denote the Hermite coefficients.
%         \item This shows that the sample-covariance matrices in operator norm converge to the Gaussian equivalent approximations.
%         \item The concentration of sample-covariance matrices follows through hyper-contractivity.
%     \end{enumerate}
% \end{proof}

\section{Details on the Numerical Investigation}
\label{sec:numerics}
In this section, we provide additional insights into the numerical illustrations presented in the main text. We refer to \href{https://github.com/IdePHICS/ComputationalDepth}{https://github.com/IdePHICS/ComputationalDepth} for the code.


\begin{figure*}[t]
\centering
\subfigure[Reinitializing]{\includegraphics[width=0.49\linewidth]{figs/reinit_check1.pdf}}
\subfigure[Without reinitializing]{\includegraphics[width=0.49\linewidth]{figs/reinit_check2.pdf}}
\caption{\textbf{Reinitialization of subsequent layers:} The plots compare the generalization error achieved by two variants of the layerwise procedure in Theorem~\ref{thm:main_theorem}. The left panel illustrates a routine with reinitialization of the subsequent layers against a procedure where this assumption is relaxed in the right panel. There is no substantial difference between the two algorithms when looking at the generalization performance.}
    \label{fig:app:reinit_check}
\end{figure*}


\begin{figure*}[t]
\centering
\subfigure[Fresh batch layerwise]{\includegraphics[width=0.49\linewidth]{figs/reuse_check_noreuse.pdf}}
\subfigure[Reuse batch layerwise]{\includegraphics[width=0.49\linewidth]{figs/reuse_check_reuse.pdf}}
\caption{\textbf{Reuse of the same data batch over layers:} The plots compare the generalization error achieved by two variants of the layerwise procedure in Theorem~\ref{thm:main_theorem}. The left panel illustrates a routine without using the same batch of data for different layers of training, while on the right this assumption is relaxed by always holding constant the total number of samples seen for every layer. There is no substantial difference between the two algorithms when looking at the generalization performance.}
    \label{fig:app:fresh_batch_check}
\end{figure*}

\subsection{Shallow methods} We illustrate in Fig.~\ref{fig:gen_error_fig1} the performance of two shallow methods: kernels (orange) and two-layer networks (green). At stake with three-layer architectures (red and blue), shallow methods are not able to perform non-linear feature learning, hence resulting in suboptimal performance. Below, we provide additional clarifications on these methods.

\paragraph{Kernel methods --} We consider a quadratic kernel $k(\vec x_1, \vec x_2) = (\vec x_1^\top, \vec x_2)^2 + (\vec x_1^\top, \vec x_2) + c = \vec \varphi_{\rm quad}^\top (\vec x_1) \vec \varphi_{\rm quad}(\vec x_2) $ that is an optimal choice among kernel mappings in the data regime explored ($n = o_d(d^{2+\delta})$), as follows by the asymptotics results in  \cite{mei_generalization_2022}. The feature map $\varphi_{\rm quad}$ is not learned, therefore we refer to kernel methods as ``fixed feature'' methods. The lack of feature learning, and therefore adaptation to the relevant low-dimensional subspaces present in the SIGHT target $f^\star$, results in a large error value achieved by the best possible kernel methods (signaled with an orange solid line in Fig.~\ref{fig:gen_error_fig1}) that serve as a lower bound for the simulations (shown as orange points). This bound coincides with the best quadratic approximation of the target as shown by \cite{mei_generalization_2022}. The figure shows also neatly the presence of the double descent peak when the number of data equals the dimension of the feature space, sometimes called the interpolation peak: $n_{\rm peak}=d(d-1)/2 + d + 1$; this is illustrated by a vertical orange dashed line in the left section of  Fig.~\ref{fig:gen_error_fig1}.


\paragraph{Two-layer networks -- } Two-layer networks are able, on the other hand, to capture linear features in the SIGHT target $f^\star$ (denoted $W^\star$ in eq.~\eqref{eq:3layer_target}). This is exemplified in Fig.~\ref{fig:gen_error_fig1} by the green points, with a net decrease in the test error with respect to kernel methods (orange ones). The generalization error shows a transition around the expected $\kappa = 1.5$, where Theorem~\ref{thm:main_theorem} predicts that the linear features $W^\star$ are recovered (shown in the illustration by a vertical black line). However, we observe that two-layer networks in this setting cannot surpass the green solid line, corresponding to the best quartic approximation of the target. This is explained by the fact that, although partial dimensionality reduction has been achieved $d \to d^{\varepsilon_1}=\sqrt{d}$, two-layer networks  are still performing random features in a $\sqrt{d}-$dimensional space. Therefore, with $n\simeq p =O(d^2) = O(\sqrt{d}^4)$ samples and neurons, we can fit the best quartic approximation of the target \citep{mei_generalization_2022}. 

\subsection{Three layer networks}
The results portrayed in Fig.~\ref{fig:gen_error_fig1} show a stark contrast between two and three-layer networks, with the latter surpassing the best possible performance for a shallow network (green solid line) thanks to the presence of non-linear feature learning. 

We consider two training routines: a) the layerwise procedure, resembling Theorem~\ref{thm:main_theorem} and algorithmically described in Alg.~\ref{alg:layerwise}; b) training using backpropagation and vanilla regularized gradient descent for all the layers jointly. 

\paragraph{Remark on Algorithm~\ref{alg:layerwise} --} Throughout this section we will consider a 
slight generalization of the routine in Alg.~\ref{alg:layerwise}: we will update the second layer weights reusing a single batch of size $\mathcal{O}(d^{k\epsilon})$ for up to $\mathcal{O}(d^{ k\epsilon})$ steps instead of using a single gradient step with preconditioning. We refer to Sec.~\ref{app:pre-cond} for discussion on the difficulties of analyzing rigorously such routine. 

Moreover, we do not follow all the theoretical prescriptions needed to prove rigorously the results and included in Alg.~\ref{alg:layerwise}. The goal of Figures~\ref{fig:app:reinit_check} and~\ref{fig:app:fresh_batch_check} is to exemplify the capability of lifting some of the theoretically needed assumptions. Respectively, in Fig.~\ref{fig:app:reinit_check} we analyze the presence of reinitialization of subsequent layers, and in Fig.~\ref{fig:app:fresh_batch_check} we consider the presence of shared batches across layers. In both cases, we do not observe a stark difference between the two settings. 

\begin{figure*}[t]
\centering
\subfigure[Layerwise training]{\includegraphics[width=0.49\linewidth]{figs/layerwise_training.pdf}}
\subfigure[Joint Training]{\includegraphics[width=0.49\linewidth]{figs/joint_training.pdf}}
\caption{\textbf{Training/Validation loss:} The plots illustrate the behavior of the training and validation losses as a function of the iteration time. It shows respectively on the left the layerwise training procedure inspired by Theorem~\ref{thm:main_theorem} (Alg.~\ref{alg:layerwise}), while on the right standard joint training using backpropagation.}
    \label{fig:app:multiple_layers}
\end{figure*}
Finally, we plot the training and validation loss curves that guided our analysis (See Fig.~\ref{fig:app:multiple_layers}).

% \subsection{Heurestic analyses for $r>1$}
%%% RETRIEVEFLAG %%%
% By considering the Hermite decomposition of $P_k$, we may decompose $h^\star(\vec{x})$ into constituent components of differing Hermite-degrees:
% \begin{equation}
%     h^\star(\vec{x}) = \sum_{j=1}^k c_j h^\star_{j}(\vec{x}),
% \end{equation}
% where $c_j$ for $j \in [N]$ denote the Hermite coefficients of $P_k$ and $h^\star_{j}(\vec{x}) = \frac{1}{\sqrt{d^\epsilon}} \sum_{i=1}^{\sqrt{d^\epsilon_1}} \text{He}_j(\langle W^\star_{i},\vec{x} \rangle)$

% We note that Equation \ref{eq:pre-ac-update} further describes the evolution of the functional overlaps $u^t_{i,j} \coloneqq \Ea{h^{t}_{2,i}(\vec{x}) h^\star_{j}(\vec{x})}$:

% \begin{equation}
%     u^{t+1}_{i,j} \approx u^{t}_{i,j} + \eta W_{3,i} \Ea{h^\star_{j,m}(\vec{x}) Z(\vec{x})^\top} Z^\top (f^\star(X)\sigma'(h^{t}_{2,i}(X))
% \end{equation}
% For $n,p_1=\Theta(d^{k\epsilon_1})$, we expect that the term
% $\Ea{h^\star_{i,j}(\vec{x}) \psi(\vec{x})^\top} Z^\top f^\star(X)\sigma'(h^{t}_{2,i}(X)))$ effectively projects the ``perturbed target" $f^\star(\vec{x})\sigma'(h^{t}_{2,i}(\vec{x}))$ upto the Hermite-degree $k$ eigenspaces of the Kernel $K_1$.

% \begin{equation}\label{eq:effective-update}
%     u^{t}_{i,j} = u^{t}_{i,j} + \eta \frac{c}{d^{k_\epsilon}}  u^{t}_{i,j} + \eta\times \text{noise}, 
% \end{equation}

% The factor $\frac{c}{d^{k_\epsilon}}$ is a manifestation of the ill-conditioning of the landscape along higher Hermite-degree components. Concretely, $\frac{c}{d^{k_\epsilon}}$ is the scaling of the eigenvalue of $K_1$ corresponding to the Hermite-degree $k$ eigenspace. Since $u^{0}_{i,j,m} \approx \frac{1}{\sqrt{d^{k_\epsilon}}}$, we expect the dynamics in Equation \ref{eq:effective-update} to require timesteps $T=\mathcal{O}(d^{k_\epsilon} \polylog d)$ to achieve overlap $u^{t}_{i,j,m} = \mathcal{O}(1)$. However, recall that each step itself requires a sample-complexity of $\mathcal{O}(d^{k_\epsilon})$ for the corresponding projections on $K_1$ to be well-approximated by the feature-covariance projection $\psi(\vec{x})^\top \Psi^\top$. This raises the total sample complexity to $\mathcal{O}(d^{2 k_\epsilon})$ instead of the expected sample-complexity of $\mathcal{O}(d^{ k_\epsilon})$. 

% By the random choice of $W_2$, we expect  that at initialization, w.h.p as $d \rightarrow \infty$, $h^2_i(\vec{x})$ contains $\Theta(\frac{1}{d}^{k\epsilon_1})$ overlap along the degree-$k$ components.

% % Finally, the errors in the projections along the degree $\leq k$ components are of order $\mathcal{O}(\frac{1}{\sqrt{p_1}}+\frac{1}{\sqrt{n}})$ which can be supressed w.r.t the initial values of the projections.

% We split the dynamics into components along the linear directions $\frac{1}{\sqrt{r}} \sum_{\ell=1}^r h^\star_l$, an orthogonal basis along $h^\star_l$, and the linear features $W^\star \vec{x}$.
% Subsequently, instead of keeping track of the entire function $h_2(\vec{x})$, we may consider a stochastic process over its projections on $h^\star_1(\vec{x}), \cdots, h^\star_r(\vec{x})$ and $W^\star_1(\vec{x}), \cdots, W^\star_r(\vec{x})$.

% Let $Z = \sigma(W X^\top)$ denote the activation matrix of the first layer subsequent to its training. Let $\vec{x} \sim \mathcal{N}(0, \mathbf{I})$ be an independent sample and denote by $\psi(\vec{x})$ the corresponding first-layer activation. The update to the second-layer pre-activations $h_{2,i}(\vec{x})$ for the $i_{th}$ neuron for $i \in [p_1]$ under a single pre-conditioned SGD step is given by:
% \begin{equation}
%     h^{t+1}_{2,i}(\vec{x}) = h^{t}_{2,i}(\vec{x}) - \eta W_{3,i}\psi(\vec{x})^\top (Z Z^\top)^{-1} Z^\top (f^\star(X)-\hat{f}(X))\sigma'(h^{t}_{2,i}(X)).
% \end{equation}
% Therefore, projections of the form $\Ea{h^{t+1}_{2,i}(\vec{x}) \phi(\vec{x})}$ result in left multiplication of $(\Psi \Psi^\top)^{-1}$ along fixed vectors. 

\subsection{Visualizing Feature Learning}
In this section, we complement the observations made in Figure~\ref{fig:theorem_illustration}. The striking capability of a three-layer network, as unveiled by our hierarchical construction, is to perform non-linear feature learning (equivalent to having a $M_h >0$ as the input dimension diverges in the bottom panel of Fig.~\ref{fig:theorem_illustration}). Fig.~\ref{fig:theorem_illustration} uses the trained weights and illustrates the behavior of the sufficient statistics for different values of $\kappa$, showing a transition around the theoretically predicted value of $\kappa = 1.5$. We use $1000$ random samples to estimate the expectation defining the non-linear overlap $M_h$.


We exemplify in Fig.~\ref{fig:time_sufficient_stats} the ``dual'' plot of Fig.~\ref{fig:theorem_illustration} by showing the evolution in time of the sufficient statistics $M_W, M_h$ for two different values of $\kappa = \frac{\log n}{\log d}$. The plot shows that when $\kappa < 1.5$ (the critical threshold) feature learning is impossible, as it is reflected by the overlaps attaining the random guess value. On the other hand for $\kappa > 1.5 $ the overlaps grow far from the random initialization performance. 

Additionally, we illustrate the evolution in time of the overlaps under the learning of MIGHT functions (eq.~\eqref{eq:3layer_target_might}) in Fig.~\ref{fig:parity_stair_comparison}. The figure exemplifies the necessity of Assumption~\ref{ass:target} that refers to the generalization of the information exponents \cite{BenArous2021, damian2024computational} of the multi-index target literature to the present hierarchical setting.


\subsection{Hyperparameters}
In every figure showing sufficient statistics or generalization errors, we average over $20$ different seeds and plot the median. The regularization strengths for the different layers are optimized with standard hyperparameter sweeping for every value of $\kappa$ plotted, while the other hyperparameters are considered fixed. More precisely, we fix:
\begin{enumerate}
    \item First hidden layer size: $p_1 = \mathrm{int}(n_{max}^{1-\delta})$, with $n_{max}$ the maximal $n$ probed in the respective plot and $\delta = 0.1$
    \item Second hidden layer size: $p2 = 600$. 
    \item Hidden layer size for two-layer network: $p = \mathrm{int}(p1/25)$
    \item Learning rates: while the orders of magnitude for the different learning rates as a function of $d$ are provided in Alg.~\ref{alg:layerwise} for layerwise training we use fixed prefactor $\mathrm{lr}_1 = 1, \mathrm{lr}_2 = 2$. Concerning joint training we use instead for all the three layers all the prefactors equal to $0.2$.
    \item Minibatch size: $n_b = \mathrm{int}(\frac{7n}{10})$, with $n = d^\kappa$.
    \item Iteration time: we follow the prescriptions of Theorem~\ref{thm:main_theorem} iterating for $T_1 = O(\mathrm{polylog}(d))$ steps and $T_2 = O(d^{1.5})$ steps. In the numerical implementation we consider for layerwise training $T_1 = \mathrm{int(15 \log d)}, T_2 = \mathrm{int}(5d^{1.5})$. On the other hand, for standard training using backpropagation, we iterate jointly all the layers for $T_2$ steps.
\end{enumerate}



\begin{figure*}[t]
\centering
\subfigure[$\kappa = 1.2$]{\includegraphics[width=0.49\linewidth]{figs/CosSimAllTogetherTimeNmax2.pdf}}
% \subfigure[$\kappa = 1.7$]{\includegraphics[width=0.33\linewidth]{figs/CosSimAllTogetherTime3Nmax4.pdf}}
\subfigure[$\kappa = 2$]{\includegraphics[width=0.49\linewidth]{figs/CosSimAllTogetherTime.pdf}}
\caption{\textbf{Visualizing Feature Learning:} The plot shows the evolution of the $\ell 2$ norm of the overlaps (Definition~\ref{def:sufficient_stat}) as a function of the training time $t$ for two different values of $\kappa = \frac{\log n}{\log d}$, respectively $\kappa = 1.2$ on the left and $\kappa = 2$ on the right. Different training methods are illustrated with different colors: in blue the layerwise training (Alg.~\ref{alg:layerwise}), in red standard joint training using backpropagation.}
    \label{fig:time_sufficient_stats}
\end{figure*}

\begin{figure*}[t]
\centering
\subfigure[$g^\star(h^\star_1, h^\star_2, h^\star_3)= \mathrm{sign}(h^\star_1h^\star_2h^\star_3)$]{\includegraphics[width=0.49\linewidth]{figs/MI2.pdf}}
\subfigure[$g^\star(h^\star_1, h^\star_2)= h^\star_1 + h^\star_1 h^\star_2 $]{\includegraphics[width=0.49\linewidth]{figs/MI1.pdf}}
\caption{\textbf{Easy and Hard Features:} The plot shows the evolution of the $\ell 2$ norm of the overlaps (Definition~\ref{def:sufficient_stat}) as a function of the training time $t$ for two different values MIGHT functions $f^\star (\vec x) = g^\star (\{h^\star_l\}_{l=1}^r)$ (See eq.~\eqref{eq:3layer_target_might}).  Different training methods are illustrated with different colors: in blue the layerwise training (Alg.~\ref{alg:layerwise}), in red standard backpropagation. The overlap component along different directions ($h^\star_l, l = 1 \cdots r$) are signaled with different markers.}
    \label{fig:parity_stair_comparison}
\end{figure*}

\end{document}
