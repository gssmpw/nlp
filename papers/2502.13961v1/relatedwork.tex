\section{Related Works}
\vspace{-0.1cm}
\paragraph {Random Feature Models ---} A key attribute enabling the effectiveness of neural networks is their ability to adjust to low-dimensional features present in the training data. However, interestingly, much of the current theoretical understanding of neural networks comes from studying their lazy regime, where features are not learned during training. One of the most pre-eminent examples of such ``fixed-features'' regimes are Random Feature (RF) models, initially introduced as a computationally efficient approximation to kernel methods by \cite{rahimi2007random}, they have gained attention as models of two-layer neural networks in the lazy regime. One of the main motivations is their sharp generalization guarantees in the high-dimensional limit \citep{gerace2020generalisation,goldt_gaussian_2021,mei2022generalization, Mei2023,xiao2022precise,defilippis2024dimension}. As mentioned, however, the performance of such methods, and of any kernel method in general, is limited. A fundamental theorem in \cite{mei2022generalization} states that only a polynomial approximation up to degree $\kappa_{\rm RF}$ of any target $f^*$, with $\kappa_{\rm RF} = {\rm min} (\kappa_1,\kappa_2)$ when learning with $n=d^{\kappa_1}$ data and $p=d^{\kappa_2}$ features. While even shallow networks can surpass these limitations \citep{ghorbani2021linearized,ba2020generalization,dandi2024random}, this relation for $\kappa_{\rm RF}$ plays a fundamental role in our analysis. 

\paragraph{Multi-index Models ---}
Despite the theoretical successes in describing fixed feature methods, the holy grail of machine learning theory remains a rigorous description of network adaptation to low-dimensional features. A popular model to study such low-dimensional structure in the learning performance is the  {\it multi-index model}. For this class of target (denoted as $f^\star_{MI}$), the input datum $\vec x$ is projected on a $r-$dimensional subspace $W^\star = \{\vec{w}^\star_j, j \in 1 \cdots r\}$ and the input-output relation depend solely on a non-linear map $g^\star$ of these $r$ (linear) features : 
\begin{align}
    f^\star_{\rm MI}(\vec x) = g^\star(\vec{x}^\top \vec{w}^\star_1, \dots,\vec{x}^\top \vec{w}^\star_r)
\end{align}
While the information theoretical performance is well understood \cite{barbier2019optimal,aubin2018committee}, there has been intense theoretical scrutiny to characterize the sample complexity needed to learn multi-index models with shallow models. On the one hand, kernel methods can only learn a polynomial approximation \citep{mei2022generalization}; on the other hand, the situation in neural networks appears more complicated at first as the hardness of a given $f^\star_{\rm MI}$ has been characterized by the ``information'' and ``leap'' exponents \cite{BenArous2021, abbe2022merged, dandi2024twolayer, damian2024computational, dandi2024benefits,arnaboldi2024repetita, lee2024neural, bietti2023learning, simsek2024learning, arous2024stochastic}.
It was shown, however, that simple modification of vanilla Stochastic Gradient Descent (SGD), such as Extra-Gradient methods or Sharpness Aware Minimizers, are able to attain sample complexity corresponding to Statistical Query (SQ) lower bound \citep{arnaboldi2024repetita, lee2024neural}, and are essentially optimal up to polylog factors in the dimension \citep{damian2024computational,troiani2024fundamental}. A motivation of the present work is to go beyond such limitations.
%