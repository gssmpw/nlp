@article{nichani2024provable,
  title={Provable guarantees for nonlinear feature learning in three-layer neural networks},
  author={Nichani, Eshaan and Damian, Alex and Lee, Jason D},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}
@article{arous2024stochastic,
  title={Stochastic gradient descent in high dimensions for multi-spiked tensor PCA},
  author={Arous, G{\'e}rard Ben and Gerbelot, C{\'e}dric and Piccolo, Vanessa},
  journal={arXiv preprint arXiv:2410.18162},
  year={2024}
}
@misc{scholkopf2002learning,
  title={Learning with kernels: support vector machines, regularization, optimization, and beyond},
  author={Sch{\"o}lkopf, B},
  year={2002},
  publisher={The MIT Press}
}

@article{kingma2014adam,
  title={Adam: A method for stochastic optimization},
  author={Kingma, Diederik P and Ba, Jimmy},
  journal={arXiv preprint arXiv:1412.6980},
  year={2014}
}



@article{frye2012spherical,
  title={Spherical harmonics in p dimensions},
  author={Frye, Christopher and Efthimiou, Costas J},
  journal={arXiv preprint arXiv:1205.3548},
  year={2012}
}
@article{van2014probability,
  title={Probability in high dimension},
  author={Van Handel, Ramon},
  journal={Lecture Notes (Princeton University)},
  volume={2},
  number={3},
  pages={2--3},
  year={2014}
}
@book{aubrun2017alice,
  title={Alice and Bob meet Banach},
  author={Aubrun, Guillaume and Szarek, Stanis{\l}aw J},
  volume={223},
  year={2017},
  publisher={American Mathematical Soc.}
}
@article{gross1975logarithmic,
  title={Logarithmic sobolev inequalities},
  author={Gross, Leonard},
  journal={American Journal of Mathematics},
  volume={97},
  number={4},
  pages={1061--1083},
  year={1975},
  publisher={JSTOR}
}

@book{o2014analysis,
  title={Analysis of boolean functions},
  author={O'Donnell, Ryan},
  year={2014},
  publisher={Cambridge University Press}
}
@article{Chatterjee_2006,
   title={A generalization of the Lindeberg principle},
   volume={34},
   ISSN={0091-1798},
   DOI={10.1214/009117906000000575},
   number={6},
   journal={The Annals of Probability},
   publisher={Institute of Mathematical Statistics},
   author={Chatterjee, Sourav},
   year={2006},
   month=nov }
@inproceedings{li2018algorithmic,
  title={Algorithmic regularization in over-parameterized matrix sensing and neural networks with quadratic activations},
  author={Li, Yuanzhi and Ma, Tengyu and Zhang, Hongyang},
  booktitle={Conference On Learning Theory},
  pages={2--47},
  year={2018},
  organization={PMLR}
}

@book{boucheron,
    author = {Boucheron, Stéphane and Lugosi, Gábor and Massart, Pascal},
    title = {Concentration Inequalities: A Nonasymptotic Theory of Independence},
    publisher = {Oxford University Press},
    year = {2013},
    month = {02},
    isbn = {9780199535255},
    doi = {10.1093/acprof:oso/9780199535255.001.0001}
}

@article{poggio2017and,
  title={Why and when can deep-but not shallow-networks avoid the curse of dimensionality: a review},
  author={Poggio, Tomaso and Mhaskar, Hrushikesh and Rosasco, Lorenzo and Miranda, Brando and Liao, Qianli},
  journal={International Journal of Automation and Computing},
  volume={14},
  number={5},
  pages={503--519},
  year={2017},
  publisher={Springer}
}

@book{axler2020measure,
  title={Measure, integration \& real analysis},
  author={Axler, Sheldon},
  year={2020},
  publisher={Springer Nature}
}
@book{bach2024learning,
  title={Learning theory from first principles},
  author={Bach, Francis},
  year={2024},
  publisher={MIT press}
}

@article{misiakiewicz2022spectrum,
  title={Spectrum of inner-product kernel matrices in the polynomial regime and multiple descent phenomenon in kernel ridge regression},
  author={Misiakiewicz, Theodor},
  journal={arXiv preprint arXiv:2204.10425},
  year={2022}
}

@book{kato2013perturbation,
  title={Perturbation theory for linear operators},
  author={Kato, Tosio},
  volume={132},
  year={2013},
  publisher={Springer Science \& Business Media}
}

@article{sun2018approximation,
  title={On the approximation properties of random ReLU features},
  author={Sun, Yitong and Gilbert, Anna and Tewari, Ambuj},
  journal={arXiv preprint arXiv:1810.04374},
  year={2018}
}

@article{mhaskar2016deep,
  title={Deep vs. shallow networks: An approximation theory perspective},
  author={Mhaskar, Hrushikesh N and Poggio, Tomaso},
  journal={Analysis and Applications},
  volume={14},
  number={06},
  pages={829--848},
  year={2016},
  publisher={World Scientific}
}

@book{klenke2013probability,
  title={Probability theory: a comprehensive course},
  author={Klenke, Achim},
  year={2013},
  publisher={Springer Science \& Business Media}
}


@article{fu2024learning,
  title={Learning Hierarchical Polynomials of Multiple Nonlinear Features with Three-Layer Networks},
  author={Fu, Hengyu and Wang, Zihao and Nichani, Eshaan and Lee, Jason D},
  journal={arXiv preprint arXiv:2411.17201},
  year={2024}
}

@article{lu2022equivalence,
  title={An Equivalence Principle for the Spectrum of Random Inner-Product Kernel Matrices with Polynomial Scalings},
  author={Lu, Yue M and Yau, Horng-Tzer},
  journal={arXiv preprint arXiv:2205.06308},
  year={2022}
}

@article{dubova2023universality,
  title={Universality for the global spectrum of random inner-product kernel matrices in the polynomial regime},
  author={Dubova, Sofiia and Lu, Yue M and McKenna, Benjamin and Yau, Horng-Tzer},
  journal={arXiv preprint arXiv:2310.18280},
  year={2023}
}

@article{vershynin2010introduction,
  title={Introduction to the non-asymptotic analysis of random matrices},
  author={Vershynin, Roman},
  journal={arXiv preprint arXiv:1011.3027},
  year={2010}
}

@inproceedings{montanari2022universality,
  title={Universality of empirical risk minimization},
  author={Montanari, Andrea and Saeed, Basil N},
  booktitle={Conference on Learning Theory},
  pages={4310--4312},
  year={2022},
  organization={PMLR}
}
@article{hu2022universality,
  title={Universality laws for high-dimensional learning with random features},
  author={Hu, Hong and Lu, Yue M},
  journal={IEEE Transactions on Information Theory},
  volume={69},
  number={3},
  pages={1932--1964},
  year={2022},
  publisher={IEEE}
}


@article{simsek2024learning,
  title={Learning Gaussian Multi-Index Models with Gradient Flow: Time Complexity and Directional Convergence},
  author={Simsek, Berfin and Bendjeddou, Amire and Hsu, Daniel},
  journal={arXiv preprint arXiv:2411.08798},
  year={2024}
}


@inproceedings{eldan2016power,
  title={The power of depth for feedforward neural networks},
  author={Eldan, Ronen and Shamir, Ohad},
  booktitle={Conference on learning theory},
  pages={907--940},
  year={2016},
  organization={PMLR}
}


@InProceedings{pmlr-v49-telgarsky16,
  title = 	 {benefits of depth in neural networks},
  author = 	 {Telgarsky, Matus},
  booktitle = 	 {29th Annual Conference on Learning Theory},
  pages = 	 {1517--1539},
  year = 	 {2016},
  editor = 	 {Feldman, Vitaly and Rakhlin, Alexander and Shamir, Ohad},
  volume = 	 {49},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Columbia University, New York, New York, USA},
  month = 	 {23--26 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v49/telgarsky16.pdf},
  abstract = 	 {For any positive integer k, there exist neural networks with Θ(k^3) layers, Θ(1) nodes per layer, and Θ(1) distinct parameters which can not be approximated by networks with O(k) layers unless they are exponentially large — they must possess Ω(2^k) nodes. This result is proved here for a class of nodes termed \emphsemi-algebraic gates which includes the common choices of ReLU, maximum, indicator, and piecewise polynomial functions, therefore establishing benefits of depth against not just standard networks with ReLU gates, but also convolutional networks with ReLU and maximization gates, sum-product networks, and boosted decision trees (in this last case with a stronger separation: Ω(2^k^3) total tree nodes are required). }
}


@inproceedings{safran2022optimization,
  title={Optimization-based separations for neural networks},
  author={Safran, Itay and Lee, Jason},
  booktitle={Conference on Learning Theory},
  pages={3--64},
  year={2022},
  organization={PMLR}
}

@inproceedings{goldt2022gaussian,
  title={The gaussian equivalence of generative models for learning with shallow neural networks},
  author={Goldt, Sebastian and Loureiro, Bruno and Reeves, Galen and Krzakala, Florent and M{\'e}zard, Marc and Zdeborov{\'a}, Lenka},
  booktitle={Mathematical and Scientific Machine Learning},
  pages={426--471},
  year={2022},
  organization={PMLR}
}


@inproceedings{gerace2020generalisation,
  title={Generalisation error in learning with random features and the hidden manifold model},
  author={Gerace, Federica and Loureiro, Bruno and Krzakala, Florent and M{\'e}zard, Marc and Zdeborov{\'a}, Lenka},
  booktitle={International Conference on Machine Learning},
  pages={3452--3462},
  year={2020},
  organization={PMLR}
}
@article{defilippis2024dimension,
  title={Dimension-free deterministic equivalents for random feature regression},
  author={Defilippis, Leonardo and Loureiro, Bruno and Misiakiewicz, Theodor},
  journal={arXiv preprint arXiv:2405.15699},
  year={2024}
}
@article{xiao2022precise,
  title={Precise learning curves and higher-order scalings for dot-product kernel regression},
  author={Xiao, Lechao and Hu, Hong and Misiakiewicz, Theodor and Lu, Yue and Pennington, Jeffrey},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={4558--4570},
  year={2022}
}

@article{jacot2018neural,
  title={Neural tangent kernel: Convergence and generalization in neural networks},
  author={Jacot, Arthur and Gabriel, Franck and Hongler, Cl{\'e}ment},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}

@article{lecun2015deep,
  title={Deep learning},
  author={LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
  journal={nature},
  volume={521},
  number={7553},
  pages={436--444},
  year={2015},
  publisher={Nature Publishing Group UK London}
}
@article{sejnowski2020unreasonable,
  title={The unreasonable effectiveness of deep learning in artificial intelligence},
  author={Sejnowski, Terrence J},
  journal={Proceedings of the National Academy of Sciences},
  volume={117},
  number={48},
  year={2020},
  publisher={National Acad Sciences}
}

@article{chizat2019lazy,
  title={On lazy training in differentiable programming},
  author={Chizat, Lenaic and Oyallon, Edouard and Bach, Francis},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@article{dandi2024random,
  title={A Random Matrix Theory Perspective on the Spectrum of Learned Features and Asymptotic Generalization Capabilities},
  author={Dandi, Yatin and Pesce, Luca and Cui, Hugo and Krzakala, Florent and Lu, Yue M and Loureiro, Bruno},
  journal={arXiv preprint arXiv:2410.18938},
  year={2024}
}

@inproceedings{mhaskar2017and,
  title={When and why are deep networks better than shallow ones?},
  author={Mhaskar, Hrushikesh and Liao, Qianli and Poggio, Tomaso},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={31},
  number={1},
  year={2017}
}
@article{aubin2018committee,
  title={The committee machine: Computational to statistical gaps in learning a two-layers neural network},
  author={Aubin, Benjamin and Maillard, Antoine and Krzakala, Florent and Macris, Nicolas and Zdeborov{\'a}, Lenka and others},
  journal={Advances in Neural Information Processing Systems},
  volume={31},
  year={2018}
}
@article{barbier2019optimal,
  title={Optimal errors and phase transitions in high-dimensional generalized linear models},
  author={Barbier, Jean and Krzakala, Florent and Macris, Nicolas and Miolane, L{\'e}o and Zdeborov{\'a}, Lenka},
  journal={Proceedings of the National Academy of Sciences},
  volume={116},
  number={12},
  pages={5451--5460},
  year={2019},
  publisher={National Acad Sciences}
}
@misc{mallat1999wavelet,
  title={A wavelet tour of signal processing},
  author={Mallat, Stephane},
  year={1999},
  publisher={Academic Press}
}
@article{kalimeris2019sgd,
  title={Sgd on neural networks learns functions of increasing complexity},
  author={Kalimeris, Dimitris and Kaplun, Gal and Nakkiran, Preetum and Edelman, Benjamin and Yang, Tristan and Barak, Boaz and Zhang, Haofeng},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}
@article{mossel2016deep,
  title={Deep learning and hierarchal generative models},
  author={Mossel, Elchanan},
  journal={arXiv preprint arXiv:1612.09057},
  year={2016}
}
@article{cagnetta2024towards,
  title={Towards a theory of how the structure of language is acquired by deep neural networks},
  author={Cagnetta, Francesco and Wyart, Matthieu},
  journal={arXiv preprint arXiv:2406.00048},
  year={2024}
}
@article{mehta2014exact,
  title={An exact mapping between the variational renormalization group and deep learning},
  author={Mehta, Pankaj and Schwab, David J},
  journal={arXiv preprint arXiv:1410.3831},
  year={2014}
}

@article{wilson1971renormalization,
  title={Renormalization group and critical phenomena. II. Phase-space cell analysis of critical behavior},
  author={Wilson, Kenneth G},
  journal={Physical Review B},
  volume={4},
  number={9},
  pages={3184},
  year={1971},
  publisher={APS}
}
@article{li2018neural,
  title={Neural network renormalization group},
  author={Li, Shuo-Hui and Wang, Lei},
  journal={Physical review letters},
  volume={121},
  number={26},
  pages={260601},
  year={2018},
  publisher={APS}
}
@article{marchand2022wavelet,
  title={Wavelet conditional renormalization group},
  author={Marchand, Tanguy and Ozawa, Misaki and Biroli, Giulio and Mallat, St{\'e}phane},
  journal={arXiv preprint arXiv:2207.04941},
  year={2022}
}
@inproceedings{ba2020generalization,
  title={Generalization of two-layer neural networks: An asymptotic viewpoint},
  author={Ba, Jimmy and Erdogdu, Murat and Suzuki, Taiji and Wu, Denny and Zhang, Tianzong},
  booktitle={International conference on learning representations},
  year={2020}
}
@inproceedings{damian2022neural,
  title={Neural networks can learn representations with gradient descent},
  author={Damian, Alexandru and Lee, Jason and Soltanolkotabi, Mahdi},
  booktitle={Conference on Learning Theory},
  pages={5413--5452},
  year={2022},
  organization={PMLR}
}
@article{ghorbani2021linearized,
  title={Linearized two-layers neural networks in high dimension},
  author={Ghorbani, Behrooz and Mei, Song and Misiakiewicz, Theodor and Montanari, Andrea},
  journal={The Annals of Statistics},
  volume={49},
  number={2},
  year={2021}
}


@article{ghorbani2020neural,
  title={When do neural networks outperform kernel methods?},
  author={Ghorbani, Behrooz and Mei, Song and Misiakiewicz, Theodor and Montanari, Andrea},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={14820--14830},
  year={2020}
}


@article{bach2023learning,
  title={Learning Theory from First Principles},
  author={Bach, Francis},
  year={2023}
}
@article{mei2022generalization,
  title={Generalization error of random feature and kernel methods: hypercontractivity and kernel matrix concentration},
  author={Mei, Song and Misiakiewicz, Theodor and Montanari, Andrea},
  journal={Applied and Computational Harmonic Analysis},
  volume={59},
  pages={3--84},
  year={2022},
  publisher={Elsevier}
}

@article{arous2021online,
  title={Online stochastic gradient descent on non-convex losses from high-dimensional inference},
  author={Arous, Gerard Ben and Gheissari, Reza and Jagannath, Aukosh},
  journal={Journal of Machine Learning Research},
  volume={22},
  number={106},
  pages={1--51},
  year={2021}
}

@article{wang2023learning,
  title={Learning hierarchical polynomials with three-layer neural networks},
  author={Wang, Zihao and Nichani, Eshaan and Lee, Jason D},
  journal={arXiv preprint arXiv:2311.13774},
  year={2023}
}

@book{holmes2012introduction,
  title={Introduction to perturbation methods},
  author={Holmes, Mark H},
  volume={20},
  year={2012},
  publisher={Springer Science \& Business Media}
}

@article{goldt2020modeling,
  title     = {Modeling the influence of data structure on learning in neural networks: The hidden manifold model},
  author    = {Goldt, Sebastian and M{\'e}zard, Marc and Krzakala, Florent and Zdeborov{\'a}, Lenka},
  journal   = {Physical Review X},
  volume    = {10},
  number    = {4},
  pages     = {041044},
  year      = {2020},
  publisher = {APS}
}

@article{loureiro2021learning,
  title={Learning curves of generic features maps for realistic datasets with a teacher-student model},
  author={Loureiro, Bruno and Gerbelot, Cedric and Cui, Hugo and Goldt, Sebastian and Krzakala, Florent and Mezard, Marc and Zdeborov{\'a}, Lenka},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={18137--18151},
  year={2021}
}

@inproceedings{goldt_gaussian_2021,
  title     = {The Gaussian equivalence of generative models for learning with shallow neural networks},
  author    = {Goldt, Sebastian and Loureiro, Bruno and Reeves, Galen and Krzakala, Florent and Mezard, Marc and Zdeborova, Lenka},
  year      = 2022,
  booktitle = {Proceedings of the 2nd Mathematical and Scientific Machine Learning Conference},
  pages     = {426--471}
}

@article{mei_generalization_2022,
  title   = {The Generalization Error of Random Features Regression: Precise Asymptotics and the Double Descent Curve},
  author  = {Mei, Song and Montanari, Andrea},
  year    = 2022,
  journal = {Communications on Pure and Applied Mathematics},
  volume  = 75,
  number  = 4,
  pages   = {667--766},
  eprint  = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/cpa.22008}
}

@article{Mei2023,
  title    = {Generalization error of random feature and kernel methods: Hypercontractivity and kernel matrix concentration},
  journal  = {Applied and Computational Harmonic Analysis},
  volume   = {59},
  pages    = {3-84},
  year     = {2022},
  note     = {Special Issue on Harmonic Analysis and Machine Learning},
  issn     = {1063-5203},
  doi      = {https://doi.org/10.1016/j.acha.2021.12.003},
  author   = {Song Mei and Theodor Misiakiewicz and Andrea Montanari},
  keywords = {Random features, Kernel methods, Generalization error, High dimensional limit}
}

@article{rahimi2007random,
  title={Random features for large-scale kernel machines},
  author={Rahimi, Ali and Recht, Benjamin},
  journal={Advances in neural information processing systems},
  volume={20},
  year={2007}
}


@article{BenArous2021,
  author  = {Gerard {Ben Arous} and Reza Gheissari and Aukosh Jagannath},
  title   = {Online stochastic gradient descent on non-convex losses from high-dimensional inference},
  journal = {Journal of Machine Learning Research},
  year    = {2021},
  volume  = {22},
  number  = {106},
  pages   = {1--51},
}

@article{troiani2024fundamental,
  title={Fundamental limits of weak learnability in high-dimensional multi-index models},
  author={Troiani, Emanuele and Dandi, Yatin and Defilippis, Leonardo and Zdeborov{\'a}, Lenka and Loureiro, Bruno and Krzakala, Florent},
  journal={arXiv preprint arXiv:2405.15480},
  year={2024}
}
@article{cybenko1989approximation,
  title={Approximation by superpositions of a sigmoidal function},
  author={Cybenko, George},
  journal={Mathematics of control, signals and systems},
  volume={2},
  number={4},
  pages={303--314},
  year={1989},
  publisher={Springer}
}

@article{berthier2024learning,
  title={Learning time-scales in two-layers neural networks},
  author={Berthier, Rapha{\"e}l and Montanari, Andrea and Zhou, Kangjie},
  journal={Foundations of Computational Mathematics},
  pages={1--84},
  year={2024},
  publisher={Springer}
}

@article{bietti2022learning,
  title={Learning single-index models with shallow neural networks},
  author={Bietti, Alberto and Bruna, Joan and Sanford, Clayton and Song, Min Jae},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={9768--9783},
  year={2022}
}

@inproceedings{abbe2023sgd,
  title={Sgd learning on neural networks: leap complexity and saddle-to-saddle dynamics},
  author={Abbe, Emmanuel and Adsera, Enric Boix and Misiakiewicz, Theodor},
  booktitle={The Thirty Sixth Annual Conference on Learning Theory},
  pages={2552--2623},
  year={2023},
  organization={PMLR}
}

@inproceedings{abbe2022merged,
  title        = {The merged-staircase property: a necessary and nearly sufficient condition for sgd learning of sparse functions on two-layer neural networks},
  author       = {Abbe, Emmanuel and Boix-Adsera, Enric  and Misiakiewicz, Theodor},
  booktitle    = {Conference on Learning Theory},
  pages        = {4782--4887},
  year         = {2022},
  organization = {PMLR}
}

@article{arora2018convergence,
  title={A convergence analysis of gradient descent for deep linear neural networks},
  author={Arora, Sanjeev and Cohen, Nadav and Golowich, Noah and Hu, Wei},
  journal={arXiv preprint arXiv:1810.02281},
  year={2018}
}

@article{lee2019wide,
  title={Wide neural networks of any depth evolve as linear models under gradient descent},
  author={Lee, Jaehoon and Xiao, Lechao and Schoenholz, Samuel and Bahri, Yasaman and Novak, Roman and Sohl-Dickstein, Jascha and Pennington, Jeffrey},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}
@article{ji2018gradient,
  title={Gradient descent aligns the layers of deep linear networks},
  author={Ji, Ziwei and Telgarsky, Matus},
  journal={arXiv preprint arXiv:1810.02032},
  year={2018}
}
@article{grad_1949_note,
  author    = {Grad, Harold},
  journal   = {Communications on Pure and Applied Mathematics},
  title     = {Note on {N}-dimensional hermite polynomials},
  year      = {1949},
  issn      = {1097-0312},
  number    = {4},
  pages     = {325--330},
  volume    = {2},
  copyright = {Copyright © 1949 Wiley Periodicals, Inc., A Wiley Company},
  doi       = {10.1002/cpa.3160020402},
  language  = {en},
}

@book{ledoux_1991_probability,
  author     = {Ledoux, Michel and Talagrand, Michel},
  publisher  = {Springer-Verlag},
  title      = {Probability in {Banach} {Spaces}: {Isoperimetry} and {Processes}},
  year       = {1991},
  isbn       = {9780387520131},
  note       = {Google-Books-ID: juC1QgAACAAJ}
}

@article{saxe2013exact,
  title={Exact solutions to the nonlinear dynamics of learning in deep linear neural networks},
  author={Saxe, Andrew M and McClelland, James L and Ganguli, Surya},
  journal={arXiv preprint arXiv:1312.6120},
  year={2013}
}
@inproceedings{arnaboldi2023high,
  title={From high-dimensional \& mean-field dynamics to dimensionless odes: A unifying approach to sgd in two-layers networks},
  author={Arnaboldi, Luca and Stephan, Ludovic and Krzakala, Florent and Loureiro, Bruno},
  booktitle={The Thirty Sixth Annual Conference on Learning Theory},
  pages={1199--1227},
  year={2023},
  organization={PMLR}
}
@article{adlam2023kernel,
  title={Kernel regression with infinite-width neural networks on millions of examples},
  author={Adlam, Ben and Lee, Jaehoon and Padhy, Shreyas and Nado, Zachary and Snoek, Jasper},
  journal={arXiv preprint arXiv:2303.05420},
  year={2023}
}
@article{zhang2021understanding,
  title={Understanding deep learning (still) requires rethinking generalization},
  author={Zhang, Chiyuan and Bengio, Samy and Hardt, Moritz and Recht, Benjamin and Vinyals, Oriol},
  journal={Communications of the ACM},
  volume={64},
  number={3},
  pages={107--115},
  year={2021},
  publisher={ACM New York, NY, USA}
}

@article{dandi2024twolayer,
  author  = {Yatin Dandi and Florent Krzakala and Bruno Loureiro and Luca Pesce and Ludovic Stephan},
  title   = {How Two-Layer Neural Networks Learn, One (Giant) Step at a Time},
  journal = {Journal of Machine Learning Research},
  year    = {2024},
  volume  = {25},
  number  = {349},
  pages   = {1--65}
}
@article{damian2024computational,
  title={The Computational Complexity of Learning Gaussian Single-Index Models},
  author={Damian, Alex and Pillaud-Vivien, Loucas and Lee, Jason D and Bruna, Joan},
  journal={arXiv preprint arXiv:2403.05529},
  year={2024}
}

@article{bietti2023learning,
  title={On learning gaussian multi-index models with gradient flow},
  author={Bietti, Alberto and Bruna, Joan and Pillaud-Vivien, Loucas},
  journal={arXiv preprint arXiv:2310.19793},
  year={2023}
}

@article{arnaboldi2024repetita,
  title={Repetita iuvant: Data repetition allows sgd to learn high-dimensional multi-index functions},
  author={Arnaboldi, Luca and Dandi, Yatin and Krzakala, Florent and Pesce, Luca and Stephan, Ludovic},
  journal={arXiv preprint arXiv:2405.15459},
  year={2024}
}

@article{lee2024neural,
  title={Neural network learns low-dimensional polynomials with SGD near the information-theoretic limit},
  author={Lee, Jason D and Oko, Kazusato and Suzuki, Taiji and Wu, Denny},
  journal={arXiv preprint arXiv:2406.01581},
  year={2024}
}

@article{dandi2024benefits,
  title={The Benefits of Reusing Batches for Gradient Descent in Two-Layer Networks: Breaking the Curse of Information and Leap Exponents},
  author={Dandi, Yatin and Troiani, Emanuele and Arnaboldi, Luca and Pesce, Luca and Zdeborov{\'a}, Lenka and Krzakala, Florent},
  journal={arXiv preprint arXiv:2402.03220},
  year={2024}
}

@misc{sclocchi2024probinglatenthierarchicalstructure,
      title={Probing the Latent Hierarchical Structure of Data via Diffusion Models}, 
      author={Antonio Sclocchi and Alessandro Favero and Noam Itzhak Levi and Matthieu Wyart},
      year={2024},
      eprint={2410.13770},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@book{bai2010spectral,
  title={Spectral analysis of large dimensional random matrices},
  author={Bai, Zhidong and Silverstein, Jack W},
  volume={20},
  year={2010},
  publisher={Springer}
}

@book{arfken2011mathematical,
  title={Mathematical methods for physicists: a comprehensive guide},
  author={Arfken, George B and Weber, Hans J and Harris, Frank E},
  year={2011},
  publisher={Academic press}
}
@article{rudi2017generalization,
  title={Generalization properties of learning with random features},
  author={Rudi, Alessandro and Rosasco, Lorenzo},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}
@article{caponnetto2007optimal,
  title={Optimal rates for the regularized least-squares algorithm},
  author={Caponnetto, Andrea and De Vito, Ernesto},
  journal={Foundations of Computational Mathematics},
  volume={7},
  pages={331--368},
  year={2007},
  publisher={Springer}
}