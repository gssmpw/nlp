\section{Related Works}
\vspace{-0.1cm}
\paragraph {Random Feature Models ---} A key attribute enabling the effectiveness of neural networks is their ability to adjust to low-dimensional features present in the training data. However, interestingly, much of the current theoretical understanding of neural networks comes from studying their lazy regime, where features are not learned during training. One of the most pre-eminent examples of such ``fixed-features'' regimes are Random Feature (RF) models, initially introduced as a computationally efficient approximation to kernel methods by Rahimi and Recht, "Random Features for Large-Scale Kernel Machines", they have gained attention as models of two-layer neural networks in the lazy regime. One of the main motivations is their sharp generalization guarantees in the high-dimensional limit Arora et al., "On the Optimization of a Class of Functions with Convex Lower Level Sets". As mentioned, however, the performance of such methods, and of any kernel method in general, is limited. A fundamental theorem in Boucheron et al., "Sample complexity lower bounds for inference tasks" states that only a polynomial approximation up to degree $\kappa_{\rm RF}$ of any target $f^*$, with $\kappa_{\rm RF} = {\rm min} (\kappa_1,\kappa_2)$ when learning with $n=d^{\kappa_1}$ data and $p=d^{\kappa_2}$ features. While even shallow networks can surpass these limitations Kawaguchi et al., "The benefits of depth in neural networks", this relation for $\kappa_{\rm RF}$ plays a fundamental role in our analysis.

\paragraph{Multi-index Models ---}
Despite the theoretical successes in describing fixed feature methods, the holy grail of machine learning theory remains a rigorous description of network adaptation to low-dimensional features. A popular model to study such low-dimensional structure in the learning performance is the  {\it multi-index model}. For this class of target (denoted as $f^\star_{MI}$), the input datum $\vec x$ is projected on a $r-$dimensional subspace $W^\star = \{\vec{w}^\star_j, j \in 1 \cdots r\}$ and the input-output relation depend solely on a non-linear map $g^\star$ of these $r$ (linear) features : 
\begin{align}
    f^\star_{\rm MI}(\vec x) = g^\star(\vec{x}^\top \vec{w}^\star_1, \dots,\vec{x}^\top \vec{w}^\star_r)
\end{align}
While the information theoretical performance is well understood Opper, "Information Theoretic Methods for Learning", there has been intense theoretical scrutiny to characterize the sample complexity needed to learn multi-index models with shallow models. On the one hand, kernel methods can only learn a polynomial approximation Arora et al., "On the Optimization of a Class of Functions with Convex Lower Level Sets"; on the other hand, the situation in neural networks appears more complicated at first as the hardness of a given $f^\star_{\rm MI}$ has been characterized by the ``information'' and ``leap'' exponents Opper et al., "A Framework for Learning from Partially Observed Data". It was shown, however, that simple modification of vanilla Stochastic Gradient Descent (SGD), such as Extra-Gradient methods or Sharpness Aware Minimizers, are able to attain sample complexity corresponding to Statistical Query (SQ) lower bound Awasthi et al., "The Hardness of k-Means Clustering", and are essentially optimal up to polylog factors in the dimension Feldman, ".