@article{BenArous2021,
  author  = {Gerard {Ben Arous} and Reza Gheissari and Aukosh Jagannath},
  title   = {Online stochastic gradient descent on non-convex losses from high-dimensional inference},
  journal = {Journal of Machine Learning Research},
  year    = {2021},
  volume  = {22},
  number  = {106},
  pages   = {1--51},
}

@article{Mei2023,
  title    = {Generalization error of random feature and kernel methods: Hypercontractivity and kernel matrix concentration},
  journal  = {Applied and Computational Harmonic Analysis},
  volume   = {59},
  pages    = {3-84},
  year     = {2022},
  note     = {Special Issue on Harmonic Analysis and Machine Learning},
  issn     = {1063-5203},
  doi      = {https://doi.org/10.1016/j.acha.2021.12.003},
  author   = {Song Mei and Theodor Misiakiewicz and Andrea Montanari},
  keywords = {Random features, Kernel methods, Generalization error, High dimensional limit}
}

@inproceedings{abbe2022merged,
  title        = {The merged-staircase property: a necessary and nearly sufficient condition for sgd learning of sparse functions on two-layer neural networks},
  author       = {Abbe, Emmanuel and Boix-Adsera, Enric  and Misiakiewicz, Theodor},
  booktitle    = {Conference on Learning Theory},
  pages        = {4782--4887},
  year         = {2022},
  organization = {PMLR}
}

@article{arnaboldi2024repetita,
  title={Repetita iuvant: Data repetition allows sgd to learn high-dimensional multi-index functions},
  author={Arnaboldi, Luca and Dandi, Yatin and Krzakala, Florent and Pesce, Luca and Stephan, Ludovic},
  journal={arXiv preprint arXiv:2405.15459},
  year={2024}
}

@article{arous2024stochastic,
  title={Stochastic gradient descent in high dimensions for multi-spiked tensor PCA},
  author={Arous, G{\'e}rard Ben and Gerbelot, C{\'e}dric and Piccolo, Vanessa},
  journal={arXiv preprint arXiv:2410.18162},
  year={2024}
}

@article{aubin2018committee,
  title={The committee machine: Computational to statistical gaps in learning a two-layers neural network},
  author={Aubin, Benjamin and Maillard, Antoine and Krzakala, Florent and Macris, Nicolas and Zdeborov{\'a}, Lenka and others},
  journal={Advances in Neural Information Processing Systems},
  volume={31},
  year={2018}
}

@inproceedings{ba2020generalization,
  title={Generalization of two-layer neural networks: An asymptotic viewpoint},
  author={Ba, Jimmy and Erdogdu, Murat and Suzuki, Taiji and Wu, Denny and Zhang, Tianzong},
  booktitle={International conference on learning representations},
  year={2020}
}

@article{barbier2019optimal,
  title={Optimal errors and phase transitions in high-dimensional generalized linear models},
  author={Barbier, Jean and Krzakala, Florent and Macris, Nicolas and Miolane, L{\'e}o and Zdeborov{\'a}, Lenka},
  journal={Proceedings of the National Academy of Sciences},
  volume={116},
  number={12},
  pages={5451--5460},
  year={2019},
  publisher={National Acad Sciences}
}

@article{bietti2023learning,
  title={On learning gaussian multi-index models with gradient flow},
  author={Bietti, Alberto and Bruna, Joan and Pillaud-Vivien, Loucas},
  journal={arXiv preprint arXiv:2310.19793},
  year={2023}
}

@article{damian2024computational,
  title={The Computational Complexity of Learning Gaussian Single-Index Models},
  author={Damian, Alex and Pillaud-Vivien, Loucas and Lee, Jason D and Bruna, Joan},
  journal={arXiv preprint arXiv:2403.05529},
  year={2024}
}

@article{dandi2024benefits,
  title={The Benefits of Reusing Batches for Gradient Descent in Two-Layer Networks: Breaking the Curse of Information and Leap Exponents},
  author={Dandi, Yatin and Troiani, Emanuele and Arnaboldi, Luca and Pesce, Luca and Zdeborov{\'a}, Lenka and Krzakala, Florent},
  journal={arXiv preprint arXiv:2402.03220},
  year={2024}
}

@article{dandi2024random,
  title={A Random Matrix Theory Perspective on the Spectrum of Learned Features and Asymptotic Generalization Capabilities},
  author={Dandi, Yatin and Pesce, Luca and Cui, Hugo and Krzakala, Florent and Lu, Yue M and Loureiro, Bruno},
  journal={arXiv preprint arXiv:2410.18938},
  year={2024}
}

@article{dandi2024twolayer,
  author  = {Yatin Dandi and Florent Krzakala and Bruno Loureiro and Luca Pesce and Ludovic Stephan},
  title   = {How Two-Layer Neural Networks Learn, One (Giant) Step at a Time},
  journal = {Journal of Machine Learning Research},
  year    = {2024},
  volume  = {25},
  number  = {349},
  pages   = {1--65}
}

@article{defilippis2024dimension,
  title={Dimension-free deterministic equivalents for random feature regression},
  author={Defilippis, Leonardo and Loureiro, Bruno and Misiakiewicz, Theodor},
  journal={arXiv preprint arXiv:2405.15699},
  year={2024}
}

@inproceedings{gerace2020generalisation,
  title={Generalisation error in learning with random features and the hidden manifold model},
  author={Gerace, Federica and Loureiro, Bruno and Krzakala, Florent and M{\'e}zard, Marc and Zdeborov{\'a}, Lenka},
  booktitle={International Conference on Machine Learning},
  pages={3452--3462},
  year={2020},
  organization={PMLR}
}

@article{ghorbani2021linearized,
  title={Linearized two-layers neural networks in high dimension},
  author={Ghorbani, Behrooz and Mei, Song and Misiakiewicz, Theodor and Montanari, Andrea},
  journal={The Annals of Statistics},
  volume={49},
  number={2},
  year={2021}
}

@inproceedings{goldt_gaussian_2021,
  title     = {The Gaussian equivalence of generative models for learning with shallow neural networks},
  author    = {Goldt, Sebastian and Loureiro, Bruno and Reeves, Galen and Krzakala, Florent and Mezard, Marc and Zdeborova, Lenka},
  year      = 2022,
  booktitle = {Proceedings of the 2nd Mathematical and Scientific Machine Learning Conference},
  pages     = {426--471}
}

@article{lee2024neural,
  title={Neural network learns low-dimensional polynomials with SGD near the information-theoretic limit},
  author={Lee, Jason D and Oko, Kazusato and Suzuki, Taiji and Wu, Denny},
  journal={arXiv preprint arXiv:2406.01581},
  year={2024}
}

@article{mei2022generalization,
  title={Generalization error of random feature and kernel methods: hypercontractivity and kernel matrix concentration},
  author={Mei, Song and Misiakiewicz, Theodor and Montanari, Andrea},
  journal={Applied and Computational Harmonic Analysis},
  volume={59},
  pages={3--84},
  year={2022},
  publisher={Elsevier}
}

@article{rahimi2007random,
  title={Random features for large-scale kernel machines},
  author={Rahimi, Ali and Recht, Benjamin},
  journal={Advances in neural information processing systems},
  volume={20},
  year={2007}
}

@article{simsek2024learning,
  title={Learning Gaussian Multi-Index Models with Gradient Flow: Time Complexity and Directional Convergence},
  author={Simsek, Berfin and Bendjeddou, Amire and Hsu, Daniel},
  journal={arXiv preprint arXiv:2411.08798},
  year={2024}
}

@article{troiani2024fundamental,
  title={Fundamental limits of weak learnability in high-dimensional multi-index models},
  author={Troiani, Emanuele and Dandi, Yatin and Defilippis, Leonardo and Zdeborov{\'a}, Lenka and Loureiro, Bruno and Krzakala, Florent},
  journal={arXiv preprint arXiv:2405.15480},
  year={2024}
}

@article{xiao2022precise,
  title={Precise learning curves and higher-order scalings for dot-product kernel regression},
  author={Xiao, Lechao and Hu, Hong and Misiakiewicz, Theodor and Lu, Yue and Pennington, Jeffrey},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={4558--4570},
  year={2022}
}

