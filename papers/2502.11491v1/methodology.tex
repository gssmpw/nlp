As shown in Figure~\ref{fig:framework}, the entire algorithm is divided into three steps:

\begin{enumerate}
    \item \textbf{Condition and Aim Recognition}: Prompt the LLM to understand the known conditions and the solving aims of the question.
    \item \textbf{Ontology-Guided Reverse Thinking Reasoning}:  Use the Reverse Thinking Reasoning method to construct label reasoning paths on the knowledge graph ontology.
    \item \textbf{Guided Answer Mining}: Use the label reasoning paths to guide querying and prompt the LLM to generate the final answer.
\end{enumerate}

\subsection{Aim and Condition Recognition}

\begin{figure}[htbp]
  \centering
  \includegraphics[width=\columnwidth]{extract_prompt.pdf}  % 设置图片宽度，保证其占用一栏
  \caption{Prompt template for condition and aim recognition.}  % 图片标题
  \label{fig:extract_prompt}  % 图片引用标签
\end{figure}

This step extracts the condition entities \( \mathcal{C}_E = \{c_1, c_2, \dots, c_n\} \), labels of condition entities \( \mathcal{C}_L = \{cl_1, cl_2, \dots, cl_n\} \), aim entities \( \mathcal{A}_E = \{a_1, a_2, \dots, a_n\} \), and labels of aim entities \( \mathcal{A}_L = \{al_1, al_2, \dots, al_n\} \) from the question by prompting LLM. 

Condition is defined as the known key information in the question, while Aim is defined as the content the user wants to query through the question. 

We provide the LLM with a Label List of the knowledge graph, prompting the LLM to first extract \( \mathcal{C}_E \) and \( \mathcal{A}_E \), and then assign labels to the respective entities. The main content of the prompt template is shown in Figure~\ref{fig:extract_prompt}, and the complete content of the prompt can be found in Appendix~\ref{sec:PROMPTS}.

\subsection{Ontology-Guided Reverse Thinking Reasoning}

Knowledge graph reasoning differs from document reasoning in that its data is structured, making the effective use of structural information particularly important \cite{Thambi2022TowardsIT}. We propose for the first time the use of a knowledge graph ontology (KG ontology) to construct label reasoning paths, thereby guiding KG queries to enhance the reasoning ability of LLM with knowledge graphs. 

The way we construct paths on KG ontology is Reverse Thinking Reasoning. The constructed path is named as label reasoning path \( \mathcal{R}_P \), or abstract reasoning path.

\subsubsection{Step I. Construct the Neighbor Label Dictionary}

The ontology of the knowledge graph consists of several relation-defined triples. For each label \( l_i \) in a triple, we collect all other labels \( l_k, l_{k+1}, \dots \) that appear in the same relation-defined triple.

To express this, we introduce a function \( \mathcal{N}(l_i) \), which denotes the set of labels \( l_k \) that appear in the same triple as \( l_i \):

\begin{equation}
\mathcal{N}(l_i) = \{ l_k \mid (l_i, \text{relationship}, l_k) \in \mathcal{G} \}
\end{equation}
where \( \mathcal{G} \) represents the set of all triples in the knowledge graph. Then, we construct a neighbor label dictionary, denoted as \( \mathcal{D} \), where \( l_i \) is the key, and \( \mathcal{N}(l_i) \) is the value associated with it:

\begin{equation}
\mathcal{D} = \{ l_i : \mathcal{N}(l_i) \}
\end{equation}

For example, given the following relationships in the knowledge graph ontology: \( l_1 \) → \( l_2 \), \( l_1 \) → \( l_3 \), and \( l_3 \) → \( l_1 \), the neighbor label dictionary \( \mathcal{D} \) will be:

\[
\mathcal{D} =
\left\{
\begin{array}{ll}
l_1 & : [l_2, l_3], \\
l_2 & : [l_1], \\
l_3 & : [l_1]
\end{array}
\right.
\]

\subsubsection{Step II. Construct the Reverse Reasoning Tree}

First, since the length of \( \mathcal{A}_L \) may be greater than 1, we create a virtual root node and add all aim labels \( \mathcal{A}_L = \{ al_1, al_2, \dots, al_m \} \) as its child nodes.

Then, we recursively traverse all child nodes, querying the neighbor label dictionary \( \mathcal{D} \) to add all neighboring labels \( \mathcal{N}(l_i) \) as child nodes of each current node. 

This process continues recursively until the maximum recursion depth \( \text{max\_pop} \) is reached. The maximum recursion depth is determined based on the number of hops of the question. 

The reverse reasoning tree, denoted as \( \mathcal{T} \), is built as a recursive structure where the nodes represent labels from the knowledge graph, and the edges represent the relationships between them.

\subsubsection{Step III. Prune By Conditions}

Starting from the root node, we perform a depth-first search (DFS) and record the current path.

When a leaf node is reached, we check if any node in the path matches the condition labels \( \mathcal{C}_L \). The pruning is performed as follows:

\begin{itemize}
    \item If the path contains no condition label nodes, the entire path is removed.
    \item If the path contains condition label nodes, only the last condition node and its preceding nodes are retained, while the subsequent nodes are deleted.
\end{itemize}

The pruning algorithm is recursively applied to each child node, using a copy of the path to avoid contaminating the original path. The output is the tree after pruning by conditions, denoted as \( \mathcal{T}_{\mathcal{\text{Condition}}} \).

\begin{algorithm}[htbp]
\caption{Prune Paths by Conditions}
\KwIn{$\mathcal{R}_P \leftarrow \text{all label reasoning paths by DFS}$, $\mathcal{C}_L \leftarrow \text{condition labels}$}
\KwOut{$\mathcal{T}_{\mathcal{\text{Condition}}}$}

\SetKwFunction{FMain}{PrunePathsByConditions}
\SetKwProg{Fn}{Function}{:}{}
\Fn{\FMain{$\mathcal{R}_P$, $\mathcal{C}_L$}}{
    $\mathcal{T}_{\mathcal{\text{Condition}}} \gets \emptyset$\;
    
    \ForEach{$path \in \mathcal{R}_P$}{
        $condition\_indices \gets \left[ i \mid node_i \in path \text{ and } node_i \in \mathcal{C}_L \right]$\;
        
        \If{$condition\_indices \neq \emptyset$}{
            $last\_condition\_index \gets \text{last element of } condition\_indices$\;
            $\mathcal{T}_{\mathcal{\text{Condition}}}.\text{append}(path[:last\_condition\_index + 1])$\;
        }
    }
    \KwRet{$\mathcal{T}_{\mathcal{\text{Condition}}}$}\;
}
\end{algorithm}

\subsubsection{Step IV. Prune Cycle Sub-paths}

Due to the possibility of bidirectional relationships between two labels, cycles may exist in the reasoning paths. 

A depth-first search (DFS) is performed on \( \mathcal{T}_{\text{Condition}} \), adding the current node's name to the visited set \( \text{visited} \). The pruning algorithm is recursively called for each child node of the current node. If the current node's name already exists in the visited set \( \text{visited} \), the edge between the current node and its parent is removed, effectively eliminating the cycle. During backtracking, the current node's name is removed from the visited set so that other paths can access it. The output is the tree after pruning cycles, denoted as \( \mathcal{T}_{\text{Cycle}} \).

\subsubsection{Step V. Prune By Semantics}

As shown in Figure~\ref{fig:framework}, after pruning by conditions and cycles, interference paths such as ``\( \text{team} \rightarrow \text{conference} \rightarrow \text{venue} \)'' may still exist. To remove these irrelevant paths, semantic information is used for pruning. 

A depth-first search (DFS) is performed on all paths of \( \mathcal{T}_{\text{Cycle}} \), and the paths are reversed to forward paths. These paths, together with the problem, are input into LLM. The model is prompted using a template to output the paths that are beneficial for answering the question. The main content of the prompt template is shown in Figure~\ref{fig:filter_prompt}, and the complete content can be found in Appendix~\ref{sec:PROMPTS}.

\begin{figure}[H]
  \centering
  \includegraphics[width=\columnwidth]{filter_prompt.pdf}  % 设置图片宽度，保证其占用一栏
  \caption{Prompt template for prune by semantics with LLM.}  % 图片标题
  \label{fig:filter_prompt}  % 图片引用标签
\end{figure}

\subsection{Guided Answer Mining}

Through Ontology-Guided Reverse Thinking Reasoning, abstract reasoning paths for solving the problem are obtained. They are then used to guide the forward knowledge graph query process to collect entity reasoning paths. 

A tree structure is used to store the results of each query step. The process is driven by traversing the abstract path, which consists of a sequence of labels. For each reasoning path, the first node is a condition node, and all entities that satisfy the label of this node are added as child nodes to the tree. Then, for each of these child nodes, the next label in the abstract path is used to query the neighboring entities of the current entity. Only those neighbors whose label matches the next label in the abstract path are retained and added as children of the current child node. This process continues iteratively, following the order of labels in the abstract path.

\input{tabel1_baseline}

If there are many neighboring entities satisfying the next label, and the number exceeds the limit, top\_k neighboring entities are randomly selected and added to the tree. This process is recursively applied until the reasoning path is fully traversed.

After the forward entity tree is built, a depth-first search (DFS) is performed to collect all entity paths, which are then input into LLM along with the problem to generate the final answer. The complete content of the prompt can be found in Appendix~\ref{sec:PROMPTS}.



