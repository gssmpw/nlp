\subsection{Datasets}
To evaluate the performance of ORT on knowledge graph question and answer tasks, we conducted experiments on two multi-hop datasets (CWQ \cite{Talmor2018TheWA} and WebQSP \cite{Devlin2019BERTPO}). The questions in both datasets cover various domains, including people, places, events, etc. Due to the complexity of the questions, traditional question answering systems and search-based engines often struggle to provide valuable knowledge. FreeBase \cite{Bollacker2008FreebaseAC} serves as the background knowledge graph for both datasets, containing approximately 88 million entities, 20,000 relationships, and 126 million triples.

Similar to the datasets used in ROG, we extracted 3,531 question-answer pairs from the CWQ dataset as the test set, which includes 2,294,264 triples and 4,726 relationships. We also extracted 1,628 question-answer pairs from the WebQSP dataset as the test set, which includes 2,277,228 triples and 5,051 relationships. For details, see Table~\ref{tabel:introduce}.
\input{tabel5_dataset}

\subsection{Metrics}
\textbf{Accuracy} is the ratio of the number of correct predictions to the total number of predictions. The formula is as follows:
\begin{equation}
\text{Accuracy} = \frac{\sum_{i=1}^{N} \mathbb{I}(\hat{y}_i \in A_{\text{gold},i})}{N}
\end{equation}

\textbf{Hit@1} is whether the most probable prediction among the model's multiple outputs contains the ground truth. If yes, the Hit@1 score is 1; otherwise, the score is 0. Because our method has only one output, there is no need to select the prediction with the highest probability. For example, consider the question “What religion does India follow?” The correct answer is "Hinduism," and the model’s predicted answers are "Christianity, Hinduism, Islam." In this case, since "Hinduism" appears in the model's predicted answers and it is the correct answer, the Hit@1 score is 1. The formula is as follows:
\begin{equation}
Hit@1 = \mathbb{I}(\exists \hat{y}_i \in A_{\text{gold}})
\end{equation}

\textbf{Precision} is the ratio of the number of correct predictions to the total number of predictions. The formula is as follows:
\begin{equation}
Precision = \frac{\sum_{i=1}^{N} \mathbb{I}(\hat{y}_i \in A_{\text{gold},i})}{N_{\text{pred}}}
\end{equation}

\textbf{Recall} measures how many of the standard answers the model can correctly predict. The calculation method is the same as that of accuracy.

\textbf{F1 score} is the harmonic mean of precision and recall. The formula is as follows:
\begin{equation}
F1 = \frac{2 \times Precision \times Recall}{Precision + Recall}
\end{equation}

\subsection{Baselines}
Baselines are grouped into 12 baseline methods into 5 categories: 1) \textbf{Embedding-based methods}, 2) \textbf{Retrieval-augmented methods}, 3) \textbf{LLM}, 4) \textbf{LLM+KGs (Fine-tuned)}, and 5) \textbf{LLM+KGs (non-Fine-tuned)}. The detailed information for each baseline is as follows:

\textbf{Embedding-based methods}  
\begin{itemize}
    \item KV-MEM \cite{Miller2016KeyValueMN} employs a key-value memory network to store triples and performs multi-hop reasoning by iterating operations over memory.  
    \item NSM\cite{He2021ImprovingMK} uses a sequential model to mimic the multi-hop reasoning process.
\end{itemize}

\textbf{Retrieval-augmented methods}  
\begin{itemize}
    \item GraftNet \cite{Sun2018OpenDQ}  retrieves relevant subgraphs from knowledge graphs with entity linking.  
    \item SR+NSM \cite{Zhang2022SubgraphRE} introduces a relation-path retrieval mechanism to fetch subgraphs for multi-hop reasoning.  
    \item SR+NSM+E2E \cite{Zhang2022SubgraphRE} further adopts an end-to-end training strategy to jointly train the retrieval and reasoning modules of SR+NSM.
\end{itemize}

\textbf{LLM methods}  
\begin{itemize}
    \item GPT-4 is a large language model developed by OpenAI, renowned for its excellent performance across a wide range of natural language processing tasks.  
    \item DeepSeek-v3 \cite{DeepSeekAI2024DeepSeekV3TR} is an advanced model designed for deep reasoning and retrieval-augmented tasks, focusing on domain-specific knowledge extraction.  
    \item Qwen-max is a large model optimized for multilingual and multi-task learning, known for its strong capabilities in both generative and analytical tasks.
\end{itemize}

\textbf{LLM+KGs (Fine-tuned) methods}  
\begin{itemize}
    \item KD-COT \cite{Wang2023KnowledgeDrivenCE} retrieves relevant knowledge from KGs to formulate faithful reasoning plans for LLMs.  
    \item ROG \cite{Luo2023ReasoningOG} combines knowledge graphs (KGs) and large language models (LLMs) to achieve reliable and interpretable reasoning through a planning-retrieval-reasoning framework.
\end{itemize}

\textbf{LLM+KGs (non-Fine-tuned) methods}  
\begin{itemize}
    \item KG Retriever aims to find the shortest path between each pair of question entities, and then retrieves the final prompt from the KG to guide the LLM in answering the question. The key difference between MindMap and KG Retriever is that they do not use diverse multiple pieces of evidence in the LLM, nor do they ORT the evidence sources.  
    \item MindMap \cite{wen-etal-2024-mindmap} integrates knowledge graphs (KGs) and large language models (LLMs) by using KGs to provide explicit knowledge and reasoning paths, enhancing the LLM's reasoning ability and transparency while revealing the thought process of the LLM.
\end{itemize}
