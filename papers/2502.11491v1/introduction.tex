LLMs have made significant achievements in natural language processing, excelling in tasks such as semantic understanding \cite{Raiaan2024ARO}, text generation \cite{shen-etal-2024-heart}, machine translation \cite{hu-etal-2024-gentranslate}, dialogue systems \cite{Zhang2019DIALOGPTL}, sentiment analysis \cite{Devlin2019BERTPO}, and text summarization \cite{Basyal2023TextSU}. 

\begin{figure}[t]
  \centering
  \includegraphics[width=\columnwidth]{introduce.pdf}  % 设置图片宽度，保证其占用一栏
  \caption{Example of previous methods and ORT. Traditional methods are limited to entity-centric reasoning through vector matching and path collection. In contrast, ORT enables ontology-aware reasoning by identifying conceptual intents, constructing reverse-label reasoning paths, and guiding targeted traversal to the correct answers.}  % 图片标题
  \label{fig:introduce}  % 图片引用标签
\end{figure}

\begin{figure*}[t]  % 'h'表示图片插入位置
  \centering
  \includegraphics[width=1\textwidth]{framework.pdf}  % 设置图片宽度，并替换为图片文件名
  \caption{The overall framework of ORT. Starting from a question, the LLM is used to identify conditions and aims in the question, along with their corresponding labels. Using the aim label as the root node, the system iteratively queries related labels on the knowledge graph ontology until the backward max-hop limit is reached. Paths that do not contain condition labels, paths after the last condition label in each sequence, and loops are then pruned. The reasoning paths are used as guidance to query the knowledge graph, and the LLM summarizes the entity paths to derive the final answer.}  % 图片标题
  \label{fig:framework}  % 图片引用标签
\end{figure*}

The rapid development of LLMs has sparked interest in combining LLMs with knowledge graphs to improve KGQA performance \cite{Hu2024GRAGGR}. Existing approaches typically adopt two paradigms. \textit{Fine-tuning methods} like LPKG~\cite{wang-etal-2024-learning-plan} and RoG~\cite{Luo2023ReasoningOG}. However, creating high-quality training data is resource-intensive \cite{Cao2023InstructionMI}. Additionally, knowledge graphs are highly structured data, and when faced with questions that have not been fine-tuned, the quality is difficult to guarantee \cite{Jiang2024KGFITKG}. \textit{Embedding + Search methods} such as MindMap~\cite{wen-etal-2024-mindmap} and Think-on-Graph~\cite{Sun2023ThinkonGraphDA} rely on entity embeddings and graph traversal, but do not handle conceptual targets absent in KG entities. As shown in Figure ~\ref{fig:introduce}, ``stadium'' is a concept, not an entity in the knowledge graph, so ``Embedding + Search Strategy'' can only find paths between ``1995 Rugby World Cup'' and ``Ireland Team'' and their neighbors, but cannot reach ``Ellis Park Stadium''.

In this paper, we propose a novel method named Ontology-guided Reverse Thinking (ORT). Our approach begins by extracting not only known entities from the question but also its underlying aims using LLMs, where these aims are represented as entity labels. Building upon these labels, we establish a reasoning framework that combines both ontological structures and reverse thinking principles. Specifically, we construct a reasoning tree that originates from the identified aims and progresses toward the known conditions, effectively creating label reasoning paths with the knowledge graph ontology. This reverse-oriented approach incorporates path pruning, eliminating unnecessary branches during the reasoning process. The refined reasoning paths then guide targeted knowledge queries in the knowledge graph, followed by using LLMs to aggregate knowledge and generate answers. This integrated methodology enables precise knowledge retrieval while minimizing interference from irrelevant information, ultimately enhancing the accuracy of the LLM's responses.

The experimental results demonstrate that our method significantly improves the answer coverage and quality of LLM KGQA. Compared to direct responses from LLMs, our method achieves a Hit@1 improvement of at least 25.43\% and an F1 score improvement of at least 25.82\%. As a plug-and-play approach, it greatly enhances the efficiency of LLM KGQA.

In summary, our main contributions include: 1) We first introduce a human-like problem-solving approach for KGQA: Ontology-Guided Reverse Thinking. 2) As a plug-and-play method, we enable LLMs to efficiently understand the structure of the knowledge graph. 3) Experimental results demonstrate a significant improvement in the LLM's KGQA ability, achieving state-of-the-art among the models studied.