\subsection{Experimental Setup}

\paragraph{Benchmarks}

We conducted experiments on two widely used KGQA datasets: WebQuestionSP (WebQSP) \cite{Yih2016TheVO} and ComplexWebQuestions (CWQ) \cite{Talmor2018TheWA}. Both datasets are constructed by extracting data from the Freebase knowledge graph. In our experiments, we follow RoG \cite{Luo2023ReasoningOG} to construct knowledge graphs for WebQSP and CWQ. More details can be found in Appendix~\ref{sec:EXPERIMENT DETAILS}. 

\begin{figure}[t]
  \centering
  \includegraphics[width=\columnwidth]{Question_hops.pdf}  % 设置图片宽度为两栏宽
  \caption{Statistics of the question hops in WebQSP and CWQ.}  % 图片标题
  \label{fig:question hops}  % 图片引用标签
\end{figure}

\begin{figure*}[t]
  \centering
  \includegraphics[width=\textwidth]{table2-v2.pdf}  % 设置图片宽度为两栏宽
  \caption{Comparison of LLM vs. LLM+Our Method on WebQSP and CWQ Datasets. The left side is Hit@1 (\%), and the right side is F1 (\%).}  % 图片标题
  \label{fig:LLM Improvement}  % 图片引用标签
\end{figure*}

\paragraph{Evaluation Metrics}

Following previous work \cite{Luo2023ReasoningOG, Zhang2022SubgraphRE, Li2024SimpleIE, Tan2024PathsoverGraphKG}, we use Hit@1 and F1 as evaluation metrics. We also provide detailed results for accuracy, precision, and recall in Appendix~\ref{sec:ADDTIONAL RESULTS}. Hit@1 refers to selecting the top-1 prediction and checking whether the true answer is included. If yes, the score is 1; otherwise, it is 0. This measures the answer coverage. Since the prediction may contain invalid content, the F1 score is also used as an evaluation metric to balance precision and recall.

\paragraph{Baselines}

We compared a total of five categories of baselines, including:
\begin{itemize}
    \item \textbf{Embedding-based}: KV-Mem \cite{Miller2016KeyValueMN} and NSM \cite{He2021ImprovingMK};
    \item \textbf{Retrieval-based}: GraphNet \cite{Sun2018OpenDQ} and SR \cite{Zhang2022SubgraphRE};
    \item \textbf{LLM}: GPT-4, DeepSeek-v3 \cite{DeepSeekAI2024DeepSeekV3TR}, and Qwen-max;
    \item \textbf{Fine-tuned LLM knowledge graph reasoning methods (KGR FT)}: KD-CoT \cite{Wang2023KnowledgeDrivenCE} and RoG \cite{Luo2023ReasoningOG};
    \item \textbf{Non-fine-tuned LLM knowledge graph reasoning methods (KGR w/o FT)}: MindMap \cite{wen-etal-2024-mindmap} and KG-Retriever.
    % and Think-on-Graph (ToG) \cite{Sun2023ThinkonGraphDA}.
\end{itemize}
Details of these baselines can be found in Appendix~\ref{sec:EXPERIMENT DETAILS}.

\subsection{ORT Achieves SOTA}

As shown in Table~\ref{table:baseline}, ORT achieves state-of-the-art performance on both WebQSP and CWQ. The base LLM for all LLM+KGs(non-Fine-tuned) methods including our method shown in Table~\ref{table:baseline} is DeepSeek-v3.

Compared to small-scale models based on embedding or retrieval, on WebQSP, Hit@1 improves by 20\% to 42.7\%, and F1 improves by 7.7\% to 37.3\%; on CWQ, Hit@1 improves by 22.7\% to 54.5\%, and F1 improves by 15.5\% to 46.9\%. 

ORT also outperforms partially \textit{KGR FT} methods such as KD-CoT and RoG. This demonstrates that our method not only enhances question-answering performance but also reduces costs, improves model scalability, and enhances adaptability to different knowledge graphs. 

Besides, compared to pure LLM and \textit{KGR w/o FT} methods, ORT also achieves significant improvements, which will be discussed later.

Furthermore, as shown in Figure ~\ref{fig:question hops}, WebQSP primarily focuses on single-hop questions, with 65.49\% requiring only one hop and no questions exceeding three hops, whereas CWQ contains more complex multi-hop questions, with 20.75\% requiring three or more hops. The inferior performance of current methods on CWQ compared to WebQSP further underscores the limitations of existing approaches in addressing multi-hop reasoning tasks.

\subsection{Soars LLM's KGQA Ability}

As shown in Figure~\ref{fig:LLM Improvement}, we conducted comparative experiments on three LLMs: GPT-4o, DeepSeek-v3, and Qwen-Max. Compared to direct answers, LLMs using ORT led to more than a 25\% improvement in Hit@1 and F1 scores across both datasets. 

On WebQSP, Hit@1 of the three LLM respectively improved by 25.7\%, 25.3\%, and 29.14\%, and F1 score increased by 28.23\%, 27.96\%, and 31.69\%. On CWQ, Hit@1 improved by 27.23\%, 31.79\%, and 31.45\%, and F1 score increased by 25.82\%, 28.83\%, and 28.30\%. Details can be found in Table~\ref{tabel:llm-improvement}.

This not only demonstrates that ORT effectively enhances the performance of LLMs on KGQA tasks but also highlights that ORT is not limited to a specific LLM. It serves as a universal enhancement strategy and can be directly used for improving LLM performance. It has the potential to become an effective, convenient, and important tool in such a domain.

\input{tabel2_llm_improvement}

\subsection{ORT Outperforms Peers}

% ORT effectively addresses two key shortcomings of previous \textit{KGR w/o FT} methods: "Embedding + Search Strategy" \cite{wen-etal-2024-mindmap} and over-reliance on LLM to generate reasoning paths \cite{Chen2024ANP}. 

\input{tabel3_non-fine-tuned_LLM_KGR}

ORT achieves better performance compared to other methods of the same type. Using the same base LLM (GPT-4, DeepSeek-v3, and Qwen-max), ORT was compared with MindMap and KG Retriever, as shown in Table~\ref{tabel:non-fine-tuned-llm-kgr}. On WebQSP, ORT achieved an average improvement of 26.56\% in Hit@1 and 26.27\% in F1 over MindMap across the three base models. On CWQ, ORT outperformed MindMap with an average improvement of 20.18\% in Hit@1 and 16.86\% in F1. 

\subsection{Ablation Study}
\input{tabel4_ablation}
Through ablation study, we aim to demonstrate the effectiveness and necessity of Reverse Thinking Reasoning, Knowledge Graph Structure-Based Reasoning, and Rule-Guided Reasoning. We designed it in three parts:
\begin{enumerate}
    \item \textbf{w/o LLM Filter}: In this part, we removed using LLMs for pruning based on the semantics of questions and paths.
    \item \textbf{Trace Forward}: We additionally designed a forward reasoning algorithm, which collects reasoning paths starting from the conditions and iterating towards the goals.
    \item \textbf{w/o Rules}: In this part, label reasoning paths are not constructed, and instead, LLM directly generates answers.
\end{enumerate}

The experimental results are shown in Table~\ref{tabel:ablation}, including Hit@1, F1, Precision, and Recall. Precision measures the proportion of correct predictions among all predicted results, while Recall measures the proportion of correct predictions identified from all ground truth instances.

\paragraph{w/o LLM Filter Analysis}As seen in Table~\ref{tabel:ablation}, on WebQSP, Hit@1 and Precision decreased, but Recall and F1 improved. To ensure the reliability of predictions and reduce hallucinations, we still choose to retain the use of LLM to filter abstract paths.

\paragraph{Trace Forward Analysis}To contrast with Trace Back, we designed the Trace Forward method, which performs forward reasoning starting from the conditions. The basic pipeline is to extract the conditions from the question, and then iteratively perform a breadth-first search from several conditions on the KG ontology to construct a reasoning tree. A depth-first search from the conditions outputs all paths, which, along with the question, are given to LLM to filter abstract paths that semantically match. Then, abstract paths are used to retrieve entity paths from the knowledge graph, which are passed to LLM to generate the final answer. 

The potential drawback of this method is that it may collect irrelevant abstract paths, and overly depends on LLM to generate reasoning paths. As seen in Table~\ref{tabel:ablation}, on WebQSP, Trace Forward's Hit@1 decreased by 11.61\%, and F1 decreased by 13.10\%. One CWQ, Trace Forward's Hit@1 decreased by 12.18\%, and F1 decreased by 13.33\%. However, this method still performed better than MindMap, which highlights the necessity of utilizing the KG ontology and using abstract paths to guide knowledge retrieval.

\paragraph{w/o Rules Analysis}Finally, to demonstrate the necessity of Rule-Guided Reasoning, we conducted experiments without rule guidance, i.e., directly using LLM to generate answers without generating abstract paths. The experimental results clearly show that this method's performance significantly decreased.