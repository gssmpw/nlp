\paragraph{Small-scale models for KGQA}

Small-scale methods for knowledge graph question answering (KGQA) can be divided into two categories: embedding-based and retrieval-based methods. Embedding-based methods, such as KV-Mem \cite{Miller2016KeyValueMN} and NSM \cite{He2021ImprovingMK}, represent entities and relations in a low-dimensional vector space, performing well on simple, single-hop queries. However, they struggle with complex, multi-hop queries due to difficulty in capturing intricate path information. To address this, retrieval-based models like GraphNet \cite{Sun2018OpenDQ} and SR \cite{Zhang2022SubgraphRE} construct subgraphs or paths for reasoning, showing improvements in multi-hop tasks by better leveraging structural relationships. Yet, both methods are limited by incomplete utilization of the full structural information in the knowledge graph.


\paragraph{Fine-tuning LLMs for KGQA}

In recent years, the rapid development of large language models (LLMs) has sparked interest in combining LLMs with knowledge graphs to improve KGQA performance. Models like RoG \cite{Luo2023ReasoningOG}, KD-CoT \cite{Wang2023KnowledgeDrivenCE}, UniKGQA \cite{Jiang2022UniKGQAUR}, and DeCAF \cite{Yu2022DecAFJD} have demonstrated impressive results by fine-tuning LLMs to generate reasoning paths and produce answers. These models excel at tackling complex KGQA tasks, where multi-hop reasoning is required. However, fine-tuning LLMs often demands vast computational resources and labeled datasets, making it challenging to scale these methods for practical, real-world applications.

\paragraph{Non-fine-tuning LLMs for KGQA}

Some recent approaches focus on methods that utilize LLMs for KGQA without requiring additional training. MindMap \citet{wen-etal-2024-mindmap} is one such method that extracts entities from the query and performs a breadth-first search in the knowledge graph to generate reasoning paths. Additionally, \citet{Chen2024ANP} proposes a model that feeds all the relations in the knowledge graph to the LLM to help generate relational paths. Although these methods can avoid the high computational costs associated with fine-tuning, they often suffer from a lack of deep understanding of the underlying structure of the knowledge graph, which can lead to the generation of lower-quality reasoning paths.