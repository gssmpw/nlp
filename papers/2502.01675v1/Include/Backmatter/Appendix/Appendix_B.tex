\chapter{\textcolor{black}{Edge Network optimization}}\label{app: EN_ib}

In this section the mathematical solution of the optimization problem \eref{eq: EN_ib initial opt problem} in \sref{sec: EN_ib} reported below:

\begin{mini}|s|[0]
    {\mathbf{\Psi}(t)}{\lim_{T \to +\infty}\; \frac{1}{T} \sum_{t=1}^T  \mathbb{E}[P^{tot}(t)] }
    {}{}
    \addConstraint{\lim_{T \to +\infty}\; \frac{1}{T} \sum_{t=1}^T  \mathbb{E}[D_k^{tot}(t)] \leq D_k^{avg}\qquad \forall k }{}
    \addConstraint{ \lim_{T \to +\infty}\; \frac{1}{T} \sum_{t=1}^T  \mathbb{E}[G_k(t)] \leq G_k^{avg}\qquad \forall k }{}
    \addConstraint{0 \leq f_k(t) \leq f_k^{max} \qquad \forall k,t }{}
    \addConstraint{0 \leq R_k(t) \leq R_k^{max}(t) \qquad \forall k,t }{}
    \addConstraint{\beta_k(t) \in \mathcal{B}_k  \qquad \forall k,t}{}
    \addConstraint{0 \leq f^{es}(t) \leq f_{es}^{max} \qquad \forall t}{}
    \addConstraint{f_k^{es}(t) \geq 0 \quad \forall k,t}, \qquad {\sum_{k=1}^K f_k^{es}(t) \leq f_c(t)  \quad \forall t,}{}
\end{mini}

These virtual queues associated to the long-term delay and evaluation metric constraints, $T_k(t)$ and $U_k(t)$ respectively are introduced as follows \cite{Neely2010Lyapunov}:
\begin{align}
    T_k(t+1) &= \max [0,T_k(t) + \varepsilon_k(D_k^{tot}(t) - D_k^{avg})] \\
    U_k(t+1) &= \max [0,U_k(t) + \nu_k(G_k(t) - G_k^{avg})],  
\end{align}
where $\epsilon_k$ and $ \nu_k $ are the learning rate for the update of the virtual queues. 

Based on these virtual queues is possible to define the \textit{Lyapunov function} $L(\mathbf{\Theta}(t))$ as:
\begin{equation}
    L(\mathbf{\Theta}(t)) = \frac{1}{2} \sum_{k=1}^K T_k^2(t) + U_k^2(t),
    \tag{\ref{eq: EN_ib Lyapunov function}}
    \label{app: EN_ib Lyapunov function}
\end{equation}
where $\mathbf{\Theta}(t) = [\{T_k(t)\}_k, \{U_k(t)\}_k]$ is the vector composed by all the virtual queues at time $t$. The idea is to use this Lyapunov function to satisfy the constraints on $D_k^{avg}$ and $G_k^{avg}$ by enforcing the stability of $L(\mathbf{\Theta}(t))$. 

To this scope it is introduced the so called \textit{drift-plus-penalty function}:
\begin{align}
    \Delta(\Theta(t)) &= \mathbb{E}\left[L({\Theta}(t+1))-L({\Theta}(t))+V\cdot P^{tot}(t)  \;\Big|\; \Theta(t)\right] \\
    &=\mathbb{E}\left[\;\sum_{k=1}^K \frac{T_k^2(t+1)-T_k^2(t)}{2} +  \frac{U_k^2(t+1)-U_k^2(t)}{2} +V\cdot P^{tot}(t)\;\; \Big|\;\; \Theta(t)\right]\\
    &= \mathbb{E}\left[\;\sum_{k=1}^K \Delta_{T_k} +  \Delta_{U_k} +V\cdot P^{tot}(t) \;\; \Big|\;\; \Theta(t)\right],
    \label{app: EN_ib drift plus penalty}
\end{align}
where, starting from a generic virtual queue evolving as 
$H(t+1) = \max [0,H(t) +h(t) - \Bar{h}]$ the quantity $\Delta_H$ is defined as follows:
\begin{align*}
    \Delta_H &= \frac{H^2(t+1)-H^2(t)}{2} = \frac{\max [0,(H(t) +h(t) - \Bar{h})^2]-H^2(t)}{2} \\
   &\leq   \frac{(h(t) - \Bar{h})^2}{2} + H(t)[h(t)-\Bar{h}].
\end{align*} 

By applying the same upper bound to $\Delta_{T_k}$ it is possible to obtain:
\begin{align}
    \Delta_{T_k} &= \frac{T_k^2(t+1)-T_k^2(t)}{2} = \frac{\max [0,(T_k(t) + \nu_k(D_k^{tot}(t) - D_k^{avg}))^2]-T_k^2(t)}{2} \\
    &\leq   \nu_k^2\frac{(D_k^{tot}(t) - D_k^{avg})^2}{2} + \nu_k T_k(t)[D_k^{tot}(t) - D_k^{avg}] \\
    &\leq \nu_k^2\frac{(D_k^{max} - D_k^{avg})^2}{2}  + \nu_k T_k(t)[D_k(t) - D_k^{avg}],
    \label{app: EN_ib delta U_k}
\end{align}
where $D_k^{max}(t)$ is the maximum delay allowed for the $k$-th \gls{ed}.

By applying the same reasoning to $\Delta_{U_k}$ it is possible to obtain:
\begin{equation}
    \Delta_{U_k} \leq \nu_k^2\frac{(G_k^{max} - G_k^{avg})^2}{2}  + \nu_k U_k(t)[G_k(t) - G_k^{avg}],
    \label{app: EN_ib delta U_k}
\end{equation}
where $G_k^{max}(t)$ is the maximum value allowed for the evaluation metric for the $k$-th \gls{ed}.

Substituting now \eref{app: EN_ib delta U_k} and \eqref{app: EN_ib delta U_k} inside \eref{app: EN_ib drift plus penalty} and rearranging the terms it is possible to obtain:

\begin{align}
    \Delta_p(\Theta(t)) &\leq
    \sum_{k=1}^K \Bigg{[} \nu_k^2\frac{(D_k^{max} - D_k^{avg})^2}{2} + \nu_k^2\frac{(G_k^{max}(t) - G_k^{avg})^2}{2}  \Bigg{]}  \\ &\;\;\;
    + \mathbb{E} \Bigg{[}\;\sum_{k=1}^K \Big{[} - \varepsilon_k Z_k(t)Q_k^{avg} - \nu_k S_k(t)G_k^{avg}   + \Big|\;\; \Theta(t) \Bigg{]} \\ &\;\;\; + \mathbb{E} \Bigg{[}\;\sum_{k=1}^K \Big{[} \varepsilon_k Z_k(t)Q_k^{tot}(t)  +  \nu_k S_k(t)G_k(t) \Big{]} + V\cdot P^{tot}\;\; \Big|\;\; \Theta(t) \Bigg{]}, 
\end{align}
where some constants that have been taken out of the expected value (first line), while others even if within the expected value do not depend on the optimization parameters (second line).

Pivoting therefore on the Lyapunov optimization it is possible to neglect all these terms. Moreover, it is possible to remove the expected value to obtain the following per-slot optimization:

\begin{mini}|s|[0]
    {\mathbf{\Psi}(t)}{\sum_{k=1}^K \bigg[ \frac{\epsilon_kT_k(t)N_k(t)}{R_k(t)} + \frac{\epsilon_kT_k(t)W_k(t)}{f_k(t)\rho_k } + \frac{\epsilon_kT_k(t)W_{max}^{es}}{f_k^{es}(t) \rho_k^{es}}+}{}{} \breakObjective{\qquad +  \frac{B_k N_0}{h_k(t)} {\rm exp} \left(\frac{R_k(t) ln(2)}{B_k} \right) + V \Gamma_k \eta_k (f_k(t))^3 +}{}{} \breakObjective{\;+  V \eta (f_c(t))^3 + \nu_k U_k(t)G_k(t)\bigg]}{}{}
    \addConstraint{\mathbf{\Psi}(t) \in \mathcal{T}(t),}{}
    \label{eq: EN_ib per-slot opt problem structure}
\end{mini}
where $\mathcal{T}(t)$ indicates the space of possible solutions given by the constraints on the optimization variables. 

at this point it is possible to split the problem for the resource allocation at the \gls{ed} and at the \gls{es}.

\section{Edge Device optimization}\label{app: EN_ib ed opt}
The sub-problem for the \gls{ed} as defined in \eref{eq: EN_ib per-slot opt ed} can be split in two further sub-problems for the transmission rate $R_k(t)$ and the clock frequency $f_k(t)$.

\subsection*{Transmission rate optimal solution}
The sub-problem associated to the transmission rate $R_k(t)$ can be defined as follows:
\begin{mini}|s|[0]
    {R_k(t)}{\frac{\epsilon_kT_k(t)N_k(t)}{R_k(t)} +  V \frac{B_k N_0}{h_k(t)} {\rm exp} \left(\frac{R_k(t) ln(2)}{B_k} \right) }{}{}
    \addConstraint{0 \leq R_k(t) \leq R_k^{max}(t)}{} 
\end{mini}

To simplify the notation, define:
\[
A = \epsilon_k T_k(t) N_k(t), \quad B = V \dfrac{B_k N_0}{h_k(t)}, \quad C = \dfrac{\ln(2)}{B_k}.
\]

Computing the derivative of the objective function $J(R_k(t))$ with respect to $R_k(t)$and set it to zero it is possible to obtain:
\[
\frac{dJ}{dR_k(t)} = -\dfrac{A}{[R_k(t)]^2} + B C \exp\left( C R_k(t) \right) = 0.
\]

By defining Let $x = C R_k(t)$ and $d = \dfrac{A C}{B}$ the derivative can be rearranged as:
\[
x e^{\frac{x}{2}} = \sqrt{d}.
\]

Fortunately, there is an exact solution to this problem and it is based on the \textit{Lambert W function}. By applying the definition and substituting back all the terms it is possible to obtain the final solution:
\begin{equation}
    R_k^*(t) = \frac{2 B_k}{ln(2)}\; W\! \!\left(\sqrt{\frac{\epsilon_k T_k(t)\; ln(2)\; h_k(t)N_k(t)\; }{4 B_k^2\;V \;N_0}}\right)\; \Biggr|_0^{R_k^{max}(t)}
\end{equation}

\subsection*{Clock frequency optimal solution}
The sub-problem associated to the transmission rate $R_k(t)$ can be defined as follows:
\begin{mini}|s|[0]
    {f_k(t)}{\frac{\epsilon_k T_k(t)W_k(t)}{f_k(t)\rho_k } +  V \Gamma_k \eta_k (f_k(t))^3 }{}{}
    \addConstraint{0 \leq f_k(t) \leq f_k^{max}}{} 
\end{mini}

To simplify the notation define:
\[
A = \dfrac{\epsilon_k T_k(t) W_k(t)}{\rho_k}, \quad B = V \Gamma_k \eta_k
\]

Computing the derivative of the objective function $J(f_k(t))$ with respect to $f_k(t)$ and set it to zero it is possible to obtain:
\[
\frac{dJ}{df_k(t)} = -\dfrac{A}{[f_k(t)]^2} + 3B [f_k(t)]^2 = 0
\]

After multiply both sides by $[f_k(t)]^2$, rearranging the terms and substituting back  $A$ and $B$ the final solution is:
\[
f_k(t) = \left( \dfrac{A}{3B} \right)^{1/4} \; \Biggr|_0^{f_k^{max}} \implies f_k^* (t) = \sqrt[4]{\frac{\epsilon_k T_k(t) W_k(t)}{3 V \Gamma_k \eta_k \rho_k} }\; \Biggr|_0^{f_k^{max}},
\]


\section{Edge Server optimization}\label{app: EN_ib es opt}


\begin{mini}|s|[0]
    {\{f_f^{es}(t)\}_k, f_c(t)}{\sum_{k=1}^K \frac{\epsilon_k T_k(t)W_{max}^{es}}{f_k^{es}(t)\rho_k^{es}} + V \eta (f_c(t))^3 }{}{}
    \addConstraint{0 \leq f_c(t) \leq f_c^{max} }{}
    \addConstraint{f_k^{es}(t) \geq 0 \quad \forall k}, \qquad {\sum_{k=1}^K f_k^{es}(t) \leq f_c(t)}{}
\end{mini}

Define:
\[
A_k = \dfrac{ \epsilon_k T_k(t) W_{\text{max}}^{es} }{ \rho_k^{es} }, \quad B = V \eta, \quad S = \sum_{k=1}^K \sqrt{ A_k }
\]


The objective function becomes:
\[
J(\{f_k^{es}(t)\}_k,\ f_c(t)) = \sum_{k=1}^K \dfrac{A_k}{f_k^{es}(t)} + B [f_c(t)]^3
\]

As a first step it is possible to define the associated Lagrangian $L$ of the sub-problem with respect to  $f_k^{es}(t)$ given $f_c(t)$ as:
\[
L = \sum_{k=1}^K \dfrac{A_k}{f_k^{es}(t)} + \lambda \left( \sum_{k=1}^K f_k^{es}(t) - f_c(t) \right)
\]

By deriving it and isolating with respect to $f_k^{es}(t)$ it is possible to obtain:

Solve for $f_k^{es}(t)$:
\[
    \frac{\partial L}{\partial f_k^{es}(t)} = -\dfrac{A_k}{[f_k^{es}(t)]^2} + \lambda = 0  \implies [f_k^{es}(t)]^2 = \dfrac{A_k}{\lambda} \implies f_k^{es}(t) = \sqrt{ \dfrac{A_k}{\lambda} }
\]

Apply the coupling constraint on $f_c(t)$ and by solving for $\lambda$ it is possible to identify:
\[
\sum_{k=1}^K f_k^{es}(t) = \dfrac{1}{\sqrt{\lambda}} \sum_{k=1}^K \sqrt{ A_k } = f_c(t) \implies \sqrt{\lambda} = \dfrac{ S }{ f_c(t) } \implies \lambda = \left( \dfrac{ S }{ f_c(t) } \right)^2
\]

Therefore:
\[
f_k^{es}(t) = \dfrac{ \sqrt{ A_k } }{ S } f_c(t)
\]

This term can now be substituted back into the objective function that is then derived with respect to $f_c(t)$ and set to zero as:

\[
J(f_c(t)) = \dfrac{ S^2 }{ f_c(t) } + B [f_c(t)]^3 \implies \frac{dJ}{df_c(t)} = - \dfrac{ S^2 }{ [f_c(t)]^2 } + 3 B [f_c(t)]^2 = 0
\]

By solving for $f_c(t)$, substituting back the expressions of $A$, $B$ and $S$ and applying the constraints it is possible to obtain the final solution:
\[
    f_c^*(t) = \left[ \left( \dfrac{ S^2 }{ 3 B } \right)^{1/4} \right]_0^{f_c^{\text{max}}}  = \frac{\sqrt{\sum_{k=1}^K \sqrt{\frac{\epsilon_k T_k(t)W_{max}^{es}}{\rho_k^{es}}}}}{\sqrt[4]{3V\eta}} \; \Biggr|_0^{f_{c}^{max}}
\]


Therefore, for every $k$:
\[
f_k^{es}(t) = \dfrac{ \sqrt{ A_k } }{ S } f_c^*(t) = f_k^{es*}(t) = \frac{\sqrt{\frac{\epsilon_k T_k(t)W_{max}^{es}}{\rho_k^{es}}}}{\sqrt{\sum_{k=1}^K \sqrt{\frac{\epsilon_k T_k(t)W_{max}^{es}}{\rho_k^{es}}}}\sqrt[4]{3V\eta} }
\]
