\chapter{Semantic Image Coding Using
Masked Vector Quantization}\label{ch: SQGAN}
The content of this chapter is entirely based on the following publication:
\begin{quotation}
    \noindent \textit{\textbf{\large SQ-GAN: Semantic Image Coding Using
Masked Vector Quantization}}\\
    \textit{Francesco Pezone, Sergio Barbarossa, Giuseppe Caire}
\end{quotation}

\section{Introduction}
In this chapter will be introduced the \gls{sqgan}, another \gls{ssm}-based image compression algorithms designed to address some limitations of the model proposed in the previous chapter. \\

Before continuing it is important to clearly define what constitutes the semantic relevant information in this context. The assumption that will be made is that the semantic relevant information consists of the following elements: (i) the reconstructed \gls{ssm} should preserve as much of the information of $\s$ as possible\footnote{In \cref{ch: SPIC} the lossless reconstruction of the \gls{ssm} was required. Now this constraint is relaxed allowing the lossy compression of the \gls{ssm}.}; (ii) the reconstructed image should retain as much of the \gls{ssm} as possible; (iii)  the overall
characteristics of the objects, such as colors and details, should be preserved; and (iv) some classes are more important than others and should be reconstructed better than the rest.\footnote{In \cref{ch: SPIC} this was optional and not required in the classic \gls{spic}, but only in the \gls{cspic}. In this chapter this is the default.}

The \gls{sqgan} introduces several innovations tailored to enhance semantic image compression. Firstly, the integration of semantic information into the compression process is achieved by modifying the \gls{maskvqvae} architecture. This modification involves the development of a \gls{samm}, which selectively prioritizes latent vectors associated with semantically relevant regions. By conditioning the masking process on the \gls{ssm}, the \gls{samm} ensures that critical classes such as "traffic signs" and "traffic lights" receive higher values of relevance score, thereby improving their reconstruction.

Secondly, to improve the efficiency of the image encoding, the \gls{sqgan} introduces the  \gls{spe}  that leverages the \gls{ssm} to assign different weights depending on the class of the objects. This facilitates the compression and preservation of details in significant regions of the image while reducing the focus on non-relevant regions.

Additionally, the \gls{sqgan} incorporates a multi-step adversarial training with a specifically designed Semantic-Aware Discriminator for image reconstruction. This discriminator is designed to reduce the focus of the discriminator on non-relevant regions to focus on the most important ones.

Finally, to address the challenge of underrepresented but crucial classes within the dataset, it is introduced the Semantic Relevant Classes Enhancement data augmentation technique. This novel augmentation method increases the prevalence of semantically important classes, such as "traffic signs" and "traffic lights", during the training phase. By augmenting the dataset with additional instances of these critical classes, the model exposure increases,  improving its ability to accurately reconstruct them, thereby enhancing the robustness and effectiveness of the compression process.\\

This chapter is structured as follows. In \sref{sec: SQGAN model description} will be introduced the architecture of the \gls{sqgan} and the details of the sub-networks $G_\s$ and $G_\x$ with the proposed \gls{spe} and the \gls{samm}. In \sref{sec: SQGAN training} will be discussed the multi-step training process of the \gls{sqgan} with the proposed Semantic Relevant Classes Enhancement data augmentation technique and the new Semantic-Aware Discriminator. In \sref{sec: SQGAN numerical results} will be shown the results and performances of the \gls{sqgan}.
\section{Model Architecture}\label{sec: SQGAN model description}
\begin{figure}[!t]
    \centering
    \includegraphics[width=0.9\textwidth]{Figures/SQ-GAN/Schema_Semantic_MQGAN.pdf}
    \caption[\acrshort{sqgan} architecture scheme]{Overview of the generator of the \acrshort{sqgan} architecture. The generator is composed of two sub-networks: $G_\s$ in beige and $G_\x$ in green. They are responsible for the compression and reconstruction of the \acrshort{ssm} and the image, respectively.}
    %[Scheme of the proposed SQ-GAN]{Sceme illustrating the architecture of the proposed SQ-GAN. It is composed of two sub-network ($G_\s$ and $G_\x$) responsible for the compression and reconstruction of the image and the SSM. The SAMM blocks are responsible for selecting only the most relevant latent vectors based on a relevance score.}
    \label{fig: SQGAN Scheme masked Sementic VQ-GAN}
\end{figure}
The \gls{sqgan} architecture depicted in \fref{fig: SQGAN Scheme masked Sementic VQ-GAN} consist of a main network $G$ composed of two sub-networks, $G_\x$ (green) and $G_\s$ (beige). They are responsible for the compression and reconstruction of the image $\x$ and the \gls{ssm} $\s$, respectively.\footnote{The presence of the apex/subscripts $\x$ and $\s$ will be used to refer to the specific pipeline.} Each sub-network consists of an encoder and a decoder pair: $E_\x$-$D_\x$ for the image and $E_\s$-$D_\s$ for the \gls{ssm} plus other additional blocks specific for the semantic vector quantization.

The encoder $E_\x$ processes the image $\x$ in conjunction with the \gls{ssm} $\s$  and the \gls{spe} producing a latent tensor $\z^\x$ with dimensions $C \times H_{16} \times W_{16}$. Similarly, the encoder $E_\s$ maps the \gls{ssm} $\s$ to a latent tensor $\z^\s$ of the same shape. Here, $C$ denotes the number of channels, while $H_{16} = \frac{H}{16}$ and $W_{16} = \frac{W}{16}$ represent the spatial dimensions reduced by a factor of 16.\footnote{In general the following notation if applied: $H_{n} = \frac{H}{n}$ and $W_{n} = \frac{W}{n}$.} 

To enhance compression efficiency while preserving semantic relevance, the \gls{samm} is integrated into both sub-networks. The \gls{samm} selectively prioritizes latent vectors based on their association with semantically important regions conditioned on the \gls{ssm}. As for the \gls{amm}, only $N_\x = m_\x \cdot K$ and $N_\s = m_\s \cdot K$ latent vectors are retained for the image and \gls{ssm}, respectively. The masking fractions $m_\x$ and $m_\s$ can be dynamically adjusted to control the compression level, allowing the model to operate at different compression rates.

This section will describe the architecture of the generator $G$ of the \gls{sqgan} in detail. Every block is discussed separately, starting from the encoder $E_\s$ and $E_\x$.

\subsection{Semantic Encoder}\label{sec: SQGAN semantic Encoder}
The first part of the sub-network $G_\s$ consists of the encoder block $E_\s$. The encoder is used to map the $n_c \times H \times W$ \gls{ssm} $\s$ to a latent tensor $\z^\s=E_\s(\s)$ with shape $C \times H_{16} \times W_{16}$, where $C$ is the number of channels and $H_{16}$ and $W_{16}$ are the final height and width.\\
$E_\s$ is formed by a repeated sequence of two \glspl{resblock} and one down-scaling layer, that will be referred to as \gls{resblockdown}. The average pooling down-scaling layer is used to halve the height $H$ and width $W$ of the \gls{ssm} after every iteration. After the first \gls{resblockdown}, the intermediate latent representation will have a shape of $\frac{H}{2}=H_{2}$ and $\frac{W}{2}=W_{2}$. By applying the \gls{resblockdown} for a total of 4 times the final latent tensor will have a spatial shape of $H_{16} \times W_{16}$. \\
The value of 4 consecutive \glspl{resblockdown} has been chosen to achieve a good balance between compression and detail retention. This is an important hyperparameter that strongly influences the results. Changing to 3 or 5, namely to $H_{8} \times W_{8}$ or $H_{32} \times W_{32}$, will change the performances of the whole architecture, causing poor compression in the first case and poor semantic preservation in the second. After an extensive ablation study it was concluded that $H_{16} \times W_{16}$ achieved the best performance.\\
%A total if 3 consecutive \glspl{resblockdown} will guarantee better reconstruction quality paying the price of higher \glspl{bpp}. On the other hand, the 5 \glspl{resblockdown} will compress more at the expense of the reconstruction quality. For this reason, after training the model with a different number of consecutive \glspl{resblockdown} it was concluded that 4 represent the best trade-off between compression and reconstruction.\\
The output of the last \gls{resblockdown} is used in the final multi-head self-attention layer of the encoder.\\
% This concludes the description of the semantic encoder $E_\s$. A block composed of four consecutive \gls{resblockdown} and one multi-head self-attention layer.  This block takes as input the \gls{ssm} $\s$ and outputs a latent tensor $\z^\s = E_\s(\s)$. The output shape of $\z^\s$ is $C \times H_{16} \times W_{16}$, where $C$ is a hyperparameter usually set to $C=256$.

\subsection{Image Encoder}\label{sec: SQGAN image Encoder}
\begin{figure}[!t]
    \centering
    \includegraphics[width=0.75\textwidth]{Figures/SQ-GAN/Schema_Masker_SQGAN_vs_MQGAN.pdf}
    \caption[\acrshort{samm} architecture scheme]{Architectural diagram of the \acrshort{amm} as in \cite{Huang2023MaskedVQ-VAE} (LEFT), and the proposed \acrshort{samm} employing the \acrshort{spade} layer to introduce the \acrshort{ssm} conditioning (RIGHT).}
    %{Comparison between AMM on the left and the proposed SAMM on the right. The strength of the SAMM is the conditioning on the SSM $\s$ via the \gls{spade} layer.}
    \label{fig: SQGAN SemAdaptiveMask module vs old}
\end{figure}
At the same time, the image $\x$ is processed by the image encoder $E_\x$. This block is responsible for mapping the image $\x$ to the latent representation $\z^\x$ and it is conditioned by the \gls{ssm} $\s$. 
The structure of the encoder $E_\x$ is the same as the one of the encoder $E_\s$.
The main difference lies in the conditioning process. In fact, not all the parts of the image $\x$ have the same semantic meaning and relevance: for example  pedestrians are more important than sky. To help the encoder $E_\x$ in giving different importance to different parts, the image $\x$ is modified before being inserted in the encoder.\\
The proposed method draws inspiration from the version of the \gls{pe} introduced by Dosovitskiy et al. in transformer networks for vision tasks \cite{Dosovitskiy2021ViT}. By subdividing the images into $16 \times 16$ they made possible to assign a specific \gls{pe} vector to every patch. In this way the transformer network was able to  correctly interpret the relative positions of the different patches in the frame.

In this work, the implementation of the \gls{pe} as proposed in \cite{Dosovitskiy2021ViT} is not needed. Unlike transformer architectures, the spatial correlations are preserved in convolution-based architectures. However, the idea of giving different weights to different semantic regions of the frame is used to improve the overall performances.
For this reason in this work is proposed a variation of the \gls{pe}, referred to as \gls{spe}. The goal of the \gls{spe} is to provide the encoder $E_\x$ with a suitable transformation of $\s$ that can be used to influence the feature extraction process.\\
To obtain this \gls{spe}, the \gls{ssm} $\s$ is processed by a two layer \gls{cnn}. This network is designed to take into account the semantic classes of adjacent pixels and provide a meaningful, learnable transformation of $\s$. At the same time the image $\x$ is processed by a one layers \gls{cnn} called $InputNet_\x$. Similar to the classical \gls{pe}, the \gls{spe} is added to the output of the $InputNet_\x$ as:
\begin{equation}
    \h_{SemPE}(\x, \s) = InputNet_\x(\x) + SemPE(\s).
    \label{eq: SQGAN SemPE}
\end{equation}
This new augmented representation of the original image $\x$ can now be processed by the encoder $E_\x$ and transformed into the latent tensor $\z^\x = E_\x(\h_{SemPE}(\x, \s))$. 
The encoding process of $\x$ is now complete and the latent tensor $\z^\x$ has the same shape as $\z^\s$, $C \times H_{16} \times W_{16}$. 

\subsection{Semantic Conditioned Adaptive Mask Module}\label{sec: SQGAN SemAdaptiveMask}
After the encoding process, the image $\x$ and the \gls{ssm} $\s$ have been transformed to the respective latent tensors $\z^\x$ and $\z^\s$, of shape $C \times H_{16} \times W_{16}$. These tensors can be interpreted as being composed of $K=H_{16} \times W_{16}$ latent vectors in $C$ dimensions. The next step consist in selecting only the most relevant of these vectors. This selection is influenced by the relevance masking fraction $m_\x$ and $m_\s$. These values represent the fraction $N_\x=m_\x\cdot K$ and $N_\s=m_\s \cdot K$of the total $K$ latent vectors that will be selected by the proposed \gls{samm}.\\
The following discussion will consider only the latent tensor $\z^\x$ and the pipeline $G_\x$ to simplify the analysis since the process introduced in this section, in \sref{sec: SQGAN quantization} and in \sref{sec: SQGAN ADM} are exactly the same for both sub-networks.

The \gls{samm} is the proposed variation of the previously discussed \gls{amm}, see \sref{sec: GM mqvae}. The idea is to assign to everyone of the $K$ latent vectors a relevance score and then select only the fraction $m_\x$ of those with the highest relevance score. In the proposed \gls{samm} the relevance score is conditioned on the \gls{ssm} $\s$ and the masking fraction $m_\x$ can be adjusted dynamically. This allows the network to use the same weights to compress images at different levels of compression. The differences in architecture between \gls{amm} and \gls{samm} are shown in \fref{fig: SQGAN SemAdaptiveMask module vs old}.\\
While the classic \gls{amm}, on the left, is more suitable for general purpose applications, the \gls{samm} on the right is designed to take into account the semantic class of the different regions of the image. The new \gls{samm} is able to enforce this information thanks to the \gls{spade} normalization layer \cite{Park2019SPADE} used to replace all the classic normalization layers in the \gls{amm} to obtain a conditioned normalization based on the \gls{ssm}.\\
The \gls{samm} is a network that takes as input the latent tensor $\z^\x$ and the \gls{ssm} $\s$ and outputs a relevance score $s_k^\x \in [0,1]$ for each latent vector $\z_k^\x$. Of all the $\z_k^\x$ vectors composing $\z^\x$, only the $N_\x$ with the highest scores are selected. The final step involves the multiplication of the selected latent vectors by their respective relevance scores. This is done to allow the backpropagation to flow through the \gls{samm} and train its parameters, as discussed in \sref{sec: GM mqvae}.
%the process of selecting the elements with the highest relevance score is not differentiable and without the product it would be impossible to train the \gls{samm}.\\
%This completes the masking process where, starting from the output of the encoder, a list of the most relevant $N_\x$ latent vectors is obtained alongside with a list with the relative position of these vectors in the latent tensor $\z^\x$.\\

\subsection{Quantization and Compression}\label{sec: SQGAN quantization}
After the the most relevant $N_\x$ latent vectors have been selected the next step consist of the vector quantization and data compression. All the non-relevant $K-N_\x$ latent vectors are automatically dropped and never used again.\\
The vector quantization process is the same as described in \sref{sec: GM mqvae}. A learnable codebook $\C_\x$ is used to find the closest codeword $\e_j^\x$ to the selected score-scaled relevant vector $\z_k^{'\x}= \z_k^\x \times s_k^\x$ and replace it with the associated quantization index $e_j^\x$. In this application it was chosen to use $J=1024$ codewords of dimensionality $C=256$.\\
In the classical \gls{maskvqvae} this would have marked the end of this process. In fact, the only purpose of the vector quantization was to avoid redundancy and improve the final image quality. In this case however, this is additionally used to compress the data.\\
The compression of the quantization indices is performed by entropy coding. Every one of the $N_\x$ selected $e_j^\x$ is located in the respective location in a $H_{16} \times W_{16}$ shaped array. To each of the remaining empty $K-N_\x$ elements of the array is assigned an additional index $e_0^\x$ that is specific for the non-relevant vectors. This additional index brings the total possible amount of quantization indices to $J+1=1025$.\\
The index $e_0^\x$ appears with probability $1-\frac{N_\x}{K}$, while the other indices $e_j^\x$ appear with probability $\frac{N_\x}{K} \alpha_j$ for some $\alpha_j \geq 0$. Generally this probability is unknown and depends on the specific image $\x$ to be encoded. However, using the fact that entropy is maximized by the uniform probability, it is possible to upper-bound the entropy of the index sequence (in bits per index) by letting $\alpha_j = \frac{1}{J}$.
Therefore, the entropy coding rate for the index sequence is upper-bounded by:
\begin{equation}
    R_\x = h_2\!\left(\frac{N_\x}{K}\right) + log_2(J)\frac{N_\x}{K} ,
\end{equation}
where $h_2(p) = -(1-p)log_2(1 - p) - p log_2(p)$ is the binary entropy function. 
Since the index sequence length is $K$, the number of bits necessary to represent such a sequence is therefore upper-bounded by:
\begin{equation}
    B_\x = K R_\x = K\cdot h_2\!\left(\frac{N_\x}{K}\right) + log_2(J)\cdot N_\x . 
\end{equation}
However, in this work this value is further approximated. To maintain a linear relationship between the number of bits and the number of relevant latent vectors the condition $h_2(p) \leq 1$ is used. By substituting $h_2(p)=1$ the final amount of bits is expressed as follows:
\begin{equation}
    B_\x = K + log_2(J) \cdot N_\x = K(1+10\cdot m_\x),
\end{equation}
where $J=1024$. This corresponds to overestimating the bits required for compression and intentionally decreasing the performances of the proposed \gls{sqgan} only to simplify future analysis when linearity will be preferred (\sref{sec: EN_nn}). However, as will be pointed out in \sref{sec: SQGAN numerical results} the \gls{sqgan} is constantly outperforming classical image compression algorithms even by overestimating the require compression bits.\\   
By considering the same approach for the $N_s$ relevant vectors in $\z_\s$ and normalizing by the total number of pixels $H \times W$, it is possible to express the total amount of \gls{bpp} as a function of $m_\x$ and $m_\s$ as follows:
\begin{equation}
    BPP = \frac{B_\x + B_\s}{H \times W} = BPP_\x + BPP_\s = \frac{1}{256}[10(m_\x + m_\s) + 2].\footnotemark
    \label{eq: SQGAN BPP}
\end{equation}
\footnotetext{In the reminder of this chapter when referring to the \gls{bpp} it will be intended the total \gls{bpp} unless when differently specified.}
\subsection{Tensor Reconstruction and Adaptive De-Masking Module}\label{sec: SQGAN ADM}
At the decoder the quantization indices are rearranged in the $H_{16} \times W_{16}$ shaped array and copies of the codebooks $\C_\x$ and $\C_\s$ are stored at the receiver end. The following discussion will again consider only the sub-network $G_\x$, but the same identical approach is applied to $G_\s$.\\
The non-relevant elements encoded with index $e_0^\x$ are replaced with a learnable codeword $\bM_\x$ learned during training.\\
This new tensor composed of $\e_j^\x$'s and $\bM_\x$'s is now ready to be processed by the \gls{adm}. This step is the same as the one introduced in the \gls{maskvqvae} \cite{Huang2023MaskedVQ-VAE} and is used to gradually let the information flow from the relevant $\e_j^\x$ to the non-relevant placeholder $\bM_\x$. In fact, this additional codeword contains no information about $\x$. Being available only at the receiver, and being the same for all the non-relevant latent vectors will negatively affect the performances of the decoder in reconstructing $\x$. The \gls{adm} with the direction-constrained self-attention is actively reducing these unwanted effects.\\
The output $\hat{\z}^\x$ of the \gls{adm} is then used as input of the decoder $D_\x$, the same being true for $\hat{\z}^\s$.\\
From now on the flows for the two sub-networks $G_\x$ and $G_\s$ will diverge again. The next subsection will discuss the decoding process of the \gls{ssm} $\s$.

\subsection{Semantic Decoder}\label{sec: SQGAN semantic Decoder}
After the \gls{adm} has correctly processed the received relevant quantization indices it is time to decode the latent tensor to obtain the reconstruction $\hat{\s}$.\\
The structure of the decoder $D_\s$ is very similar to the mirrored version of the encoder $E_\s$. The first input layer is composed of the multi-head self-attention layer, after which the series of \glspl{resblockup} is placed. Every \gls{resblockup} is composed of two consecutive \glspl{resblock} and one up-scaling layer. The up-scaling is performed by copying the value of one element in the up-scaled $2\times2$ patch. The goal is to gradually transform the latent tensor $\hat{\z}^\s$ from a spatial shape of $H_{16} \times W_{16}$ back to the original spatial shape of $H \times W$, the same as the \gls{ssm} $\s$. This decoding and up-scaling process is completed by using four consecutive \glspl{resblockup}. \\
The final reconstructed \gls{ssm} $\hat{\s}$ is obtained by applying the argmax operator to assign any pixel to a specific semantic class.\\
The reconstructed \gls{ssm} $\hat{\s}$ can now be used to condition the reconstruction $\hat{\x}$ of the image $\x$. 

\subsection{Image Decoder}\label{sec: SQGAN image Decoder}
The reconstruction of $\x$ is obtained by applying the decoder $D_\x$ to $\hat{z}^\x$  and to the reconstructed \gls{ssm} $\hat{\s}$. This decoder has a structure similar to the mirrored version of the encoder $E_\x$, with some key conceptual differences. The input  multi-head self-attention layer remains the same. At the same time the sequence of four consecutive \gls{resblockup} is modified to incorporate the conditioning via the \gls{ssm}. The modifications focus on the normalization layers within the \glspl{resblock}, replacing every normalization layer with the \gls{spade} layer, as depicted in \fref{fig: GM resblock spade}. Without such conditioning, the decoder $D_\x$ will require much more computational power, longer training and bigger latent representation to have a similar level of \gls{ssm} retention. \\
%This ends the decoding process with the reconstruction of the image $\hat{\x}$. \\

%This end the discussion of the proposed \gls{sqgan} architecture, a model composed of different blocks that process the input $\x$ and $\s$ in two interconnected pipelines. The encoders $E_\x$ (\sref{sec: SQGAN image Encoder}) and $E_\s$ (\sref{sec: SQGAN semantic Encoder}) are the first blocks of these pipelines. They are responsible for the mapping of the input data in the respective latent tensors. From this point the two parallel pipeline are composed of the same blocks. The \gls{samm} are used to select the relevant latent vectors (\sref{sec: SQGAN SemAdaptiveMask}), then they are vector quantized, and the quantization indeces are compressed and send them to the receiver (\sref{sec: SQGAN quantization}). At the receiver the relevant quantization indices are restructured using a copy of the codebooks and the final latent tensor is processed (\sref{sec: SQGAN ADM}). After the information has flown from the relevant to the non-relevant latent vectors it is time of reconstructing $\hat{\x}$ (\sref{sec: SQGAN image Decoder}) and $\hat{\s}$ (\sref{sec: SQGAN semantic Decoder}).\\
%The interconnectivity in between parts in the \gls{sqgan} is one of the strength of this architecture but could also represent a problem if not controlled properly during training. Being composed of so many specialized parts could lead to some bad results if trained as a single mono-block model. In the next section the training process of the \gls{sqgan} will be discussed.

\section{Training and Inference}\label{sec: SQGAN training}
In the previous section, the architecture of the  \gls{sqgan} was presented as being composed of two interconnected sub-networks, $G_\x$ and $G_\s$, forming the single network $G$. This section focuses on the training process specifically designed for this architecture. 

The training of the entire network $G$ as a monolithic entity is not feasible due to the \gls{sqgan}'s structural complexity, which makes such an approach inefficient and impractical. The two sub-networks are in fact intended to reconstruct objects in distinct domains—the image domain and the \gls{ssm} domain—each requiring different loss functions. By using a single loss function for the whole process the performances will decrease and none of the sub-networks will perform at its best. Additionally, the masking fractions $m_\s$ and $m_\x$ are considered variable rather than fixed as in \cite{Huang2023MaskedVQ-VAE}. This allows  the \gls{sqgan} to compress images and \glspl{ssm} at various compression levels but also increases the training complexity. \\

To effectively train  the \gls{sqgan}, a multi-step approach consisting of three stages is employed: (i) train $G_\s$ using the original $\s$, (ii) train $G_\x$ with the original $\x$ and $\s$ and (iii) fine-tune the entire network $G$ using the original $\x$, original $\s$, and the reconstructed $\hat{\s}$ by freezing $G_\s$'s parameters and only fine-tuning $G_\x$'s parameters. 

The necessity of this multi-step approach arises from the distinct objectives and dependencies of the sub-networks. Initially training $G_\s$ ensures that the \gls{ssm} is accurately reconstructed independently of the rest. At the same time training $G_\x$ with the original $\x$ and $\s$ allows it to learn the conditional dependencies required for image reconstruction based on a reliable \gls{ssm}. However, since $G_\x$ is later required to be conditioned by the reconstructed $\hat{\s}$ rather than the original $\s$, the fine-tuning step is crucial to adapt $G_\x$ to mitigate the imperfections of $\hat{\s}$ that are not present in $\s$.

In addition to the multi-step training approach, another crucial challenge is the identification and preservation of semantically relevant information. To address this problem the training process has been further improved by proposing and incorporating the Semantic Relevant Classes Enhancement data augmentation technique and the Semantic-Aware discriminator network. The idea of these proposed approaches is to emphasize the importance of semantically relevant classes (i.e. "traffic signs", "traffic lights", "pedestrians" etc.), while preventing non-relevant classes (i.e. "sky", "vegetation" and "street") from being too dominant.

This section is divided as follows. \sref{sec: SQGAN Data Augmentation} will present the data augmentation process discussing commonly used techniques and the proposed Semantic Relevant Classes Enhancement method. \sref{sec: SQGAN training G_s} will focus on the training of $G_\s$ while \sref{sec: SQGAN training G_x} will focus on the training of $G_\x$ with the introduction of the proposed Semantic-Aware Discriminator. At the end \sref{sec: SQGAN training G} will focus on the final fine-tuning process. 


\subsection{Data Augmentation}\label{sec: SQGAN Data Augmentation}
\begin{figure}[!t]
    \centering
    \adjustbox{valign=t}{\includegraphics[width=0.45\textwidth]{Figures/SQ-GAN/traffic_sign_nonagum_image.png}}%
    \hspace{5mm}
    \adjustbox{valign=t}{\includegraphics[width=0.45\textwidth]{Figures/SQ-GAN/traffic_sign_agum_image.png}}\\[1mm]
    \adjustbox{valign=t}{\includegraphics[width=0.45\textwidth]{Figures/SQ-GAN/traffic_sign_nonagum_ssm.png}}%
    \hspace{5mm}
    \adjustbox{valign=t}{\includegraphics[width=0.45\textwidth]{Figures/SQ-GAN/traffic_sign_agum_ssm.png}}
    \caption[Data Augmentation for \acrshort{sqgan}]{Effect of combined data augmentation techniques, including rotation, cropping and the proposed Semantic Relevant Classes Enhancement. On the left, the original image and \gls{ssm} and on the right the resulting augmented version.}
    \label{fig: SQGAN Data augmentation}
\end{figure}


This section introduces a novel data augmentation technique, the  Semantic Relevant Classes Enhancement, designed to solve the problem of underrepresented but semantically relevant classes in datasets. The core idea lies in the assumption that some specific classes, i.e. "traffic signs" and "traffic lights", are crucial in applications like autonomous driving but unfortunately underrepresented. In fact they often occupy a small portion of the frame and are not always present in every frame. Enhancing their representation is fundamental for improving the model's ability to efficiently reconstruct these critical classes.

This new data augmentation technique is fundamentally different from other proposed through the years \cite{Konushin2021dataAug1, Jockel2021dataAug2}. Prior methods often focus on swapping existing objects---for example, replacing a "turn left" sign with a "stop" sign---or employ complex \gls{nn} architectures to introduce new objects into images. These approaches can lead to increased computational costs or fail to adequately address the under-representation of critical classes. In contrast, the proposed technique is designed to be straightforward, efficient, and fast.

Below will be described the data augmentation pipeline used in this work and the effects are shown in \fref{fig: SQGAN Data augmentation}. At first the implementation of the commonly used techniques will be discussed, then the proposed data augmentation method.
\begin{itemize}[label={}]
    \item{\textbf{Random Rotation}:} With probability $p=0.5$, each pair $(\x,\s)$ is rotated by a random angle between $[-5^\circ,5^\circ]$. This small range prevents unrealistic scenarios, e.g. a pedestrian walking at a 45 degrees angle. This is performed to help the model generalize slight variations in camera orientation.
    
    \item{\textbf{Random Cropping}:} With probability $p=0.5$, this technique selects a random portion of the frame of the pair $(\x,\s)$. Unlike standard random cropping, the selected portion of the frame is constrained to avoid regions too close to the top or the bottom. This is because the top and bottom areas are often dominated by non-relevant classes such as "sky", "vegetation", or "street". By focusing on the central regions where relevant objects like "traffic signs" and "traffic lights" are more likely to appear, the model is encouraged to pay attention to smaller details in relevant classes. 
    
    \item{\textbf{Random Color Manipulation}:} With probability $p=0.3$, the brightness and saturation of the image $\x$ are altered. This simulates overexposed or underexposed images, such as those seen when a vehicle enters or exits a tunnel, helping the model deal with challenging lighting conditions.
    
    \item{\textbf{Random Semantic Relevant Classes Enhancement}:} This is the novel data augmentation technique specifically designed in this work to address the under-representation of "traffic signs" and "traffic lights". The process is based on the use of mini-batches of pairs $(\x,\s)$.

    For each image in a mini-batch, the \gls{ssm} is used to identify all instances of "traffic signs" and "traffic lights" present in that image. The main idea is to augment each image and its corresponding \gls{ssm} by adding more instances of these critical classes. This is achieved by copying "traffic signs" and "traffic lights" from other images within the same mini-batch and pasting them into the current image and \gls{ssm}. By increasing the presence of these objects, the model will be more exposed to them during training.

    The process begins by collecting all "traffic signs" and "traffic lights" from the images and \glspl{ssm} in the current mini-batch. For each image in the mini-batch, a random number $n$ between $0$ and $25$ is selected, representing the number of objects to add. Then, $n$ objects are randomly chosen from the collected set and are carefully placed into the image and its \gls{ssm}. Placement is done in such a way to avoid overlapping with existing instances of the same classes or with the other relevant class.
\end{itemize}

The final augmented pair $(\x,\s)$ is represented in \fref{fig: SQGAN Data augmentation} with the original pair on the left and the data augmented version on the right. Other than the cropping, rotation and color correction it is interesting to focus on the increase in "traffic signs" (yellow), and "traffic lights" (orange) in the augmented $\x$ and $\s$.


\subsection{Training of $G_\s$}\label{sec: SQGAN training G_s}
\begin{figure}[!t]
    \centering
    \includegraphics[width=1\textwidth]{Figures/SQ-GAN/G_ssm.pdf}
    \caption[\acrshort{sqgan} training pipeline of the  \acrshort{ssm} generator]{Schematic representation of the semantic generator network $G_\s$ training pipeline.}
    \label{fig: SQGAN Gen_sem sqgan}
\end{figure}
After data augmentation, the training of the sub-network $G_\s$ can be implemented, as depicted in \fref{fig: SQGAN Gen_sem sqgan}. The training is based on an adversarial approach typical of \glspl{vqgan}. The input and output tensors of $G_\s$ have a shape of $n_c \times H \times W$, representing the one-hot encoded \glspl{ssm}. The loss function is designed as follows:
\begin{equation}
    \Loss_{\text{SQ-GAN}}^\s = \lambda_{\text{GAN}} \Loss_{\text{GAN}} + \lambda_{\text{WCE}} \Loss_{\text{WCE}}  + \lambda_{\text{vq}} \Loss_{\text{vq}} + \lambda_{\text{commit}} \Loss_{\text{commit}},
\end{equation}
In contrast to the classic \gls{vqgan} loss function discussed in \eref{eq: GM vq-gan total_loss}, the loss function adopted here uses the weighted cross-entropy loss $ \Loss_{\text{WCE}}$ as the reconstruction loss, rather than the \gls{l2} or the perceptual loss. This choice is motivated by the nature of the \gls{ssm}.

To emphasize the importance of semantically relevant classes over non-relevant ones, the weight factors $ \Loss_{\text{WCE}}$ are set as follows: 

\vspace{0.2cm}
\begin{tabular}{ll}
    \textbullet \;\; $\text{w} = 1$  & for the relevant classes "traffic signs" and "traffic lights". \\
    \textbullet \;\; $\text{w} = 0.85$  & for the classes "people" and "rider". \\
    \textbullet \;\; $\text{w} = 0.20$  & for the non-relevant classes "sky" and "vegetation". \\
    \textbullet \;\; $\text{w} = 0.50$  & for all the other classes.
\end{tabular}\\

These weight assignments encourage the model to focus more on reconstructing the relevant classes, thus guiding the \gls{samm} to prioritize the selection of latent vectors containing information about these classes.

The other important part involved in training is the discriminator network \cite{isola2017image2image}. It is composed of a convolutional layer, batch normalization layer and a leaky ReLU \cite{Bing2015Rectified} repeated 3 times and followed by the last convolutional layer that maps the output to a single number. This value is the output of the discriminator used to classify the \gls{ssm} as real or fake.

During training, the masking fraction $m_\s$ is varied randomly, selected from a set of values ranging from 5\% to 100\% with and expected value of 35\%. This approach allows the model to learn to compress the \gls{ssm} at various levels of compression.

The sub-network $G_\s$ is trained using the Adam optimizer \cite{Kingma2015Adam} with a learning rate of $10^{-4}$ and a batch size of 8. The training is conducted for 200 epochs with early stopping to prevent overfitting.


\subsection{Training of $G_\x$}\label{sec: SQGAN training G_x}
\begin{figure}[!t]
    \centering
    \includegraphics[width=1\textwidth]{Figures/SQ-GAN/G_img.pdf}
    \caption[\acrshort{sqgan} training pipeline of the image generator]{Schematic representation of the image generator network $G_\x$ training pipeline.}
    \label{fig: SQGAN Gen_img sqgan}
\end{figure}
The training of the sub-network $G_\x$ follows a similar approach to that of $G_\s$. At this stage, $G_\x$ is trained using the original image $\x$ and the original \gls{ssm} $\s$. The training pipeline is illustrated in \fref{fig: SQGAN Gen_img sqgan}, highlighting that the true \gls{ssm} $\s$ is used to condition the decoder.\footnote{This is in contrast with the functioning of the network $G$ where the reconstructed \gls{ssm} $\hat{\s}$ is used to condition the discriminator.}

An adversarial approach typical of \glspl{vqgan} is employed for training, with the loss function defined as:
\begin{equation}
    \Loss_{\text{SQ-GAN}}^\x = \lambda_{\text{GAN}} \Loss_{\text{GAN}} + \lambda_{WL_2} \Loss_{WL_2}  + \lambda_{\text{perc}} \Loss_{\text{perc}} + \lambda_{\text{vq}} \Loss_{\text{vq}} + \lambda_{\text{commit}} \Loss_{\text{commit}},
\end{equation}
The weighted $l_2$ loss $\Loss_{WL_2}$ adjusts the importance of different semantic classes as follows:

\vspace{0.2cm}
\begin{tabular}{ll}
    \textbullet \;\; $\text{w} = 1$    & for the relevant classes "traffic signs" and "traffic lights". \\
    \textbullet \;\; $\text{w} = 0.55$ & for the classes "people" and "rider". \\
    \textbullet \;\; $\text{w} = 0$    & for the non-relevant classes "sky" and "vegetation". \\
    \textbullet \;\; $\text{w} = 0.15$ & for all other classes.
\end{tabular}\\


Among all the choices of weights the most interesting is the one concerning the weights for "sky" and "vegetation", set to zero. This choice has been made to force the network not to consider the pixel-by-pixel reconstruction of these classes. In fact, the real color of the sky and trees in the image are semantically non-relevant. 

The perceptual loss is instead the part that ensures that the reconstructed image maintains visual similarity to the original, preventing unrealistic alterations such as an unnatural sky color.

Another important improvement is in the adversarial loss $\mathcal{L}_{\text{GAN}}$ and involves the discriminator network. In this work a new version of the discriminator is proposed to avoid the focus on non-relevant regions of the image.

\subsubsection{Semantic-Aware Discriminator}
\begin{quote}
The discriminator $D_{disc}^\x$ plays a crucial role in adversarial training by determining whether an image reconstructed by $G_\x$ is real or fake. It achieves this by classifying images between real and fake images, and is trained on both $\x$ and $\hat{\x}$  to identify the characteristics that make an image appear authentic. However, a potential drawback is that the discriminator might focus on non-relevant parts of the image to influence the classification. For instance, it might prioritize vegetation details, and classify images as real only if the leaves on the trees have a certain level of detail. This will force the generator $G_\x$ to reconstruct images with better vegetation details to fool the discriminator. Unfortunately, this is not optimal for the structure of the \gls{sqgan}. Giving more importance to the vegetation will decrease the importance of other classes, thus causing the \gls{samm} module  to select the wrong latent vectors as relevant.

In recent years various articles have proposed ways to modify the discriminator to introduce various conditioning. For example, \cite{Oluwasanmi2020condDiscr} conditioned the discriminator on the \gls{ssm} to improve \gls{sseg} retention, while \cite{Chen2020ssd} enforced the discriminator to focus on high-frequency components. However, these methods do not adequately address the issue at hand. 

To achieve the desired performance, it is essential to adjust the discriminator to minimize its focus on non-relevant regions. To this scope it is possible to consider these key observations about the discriminator's behavior:
\begin{itemize}
    \item For a trained discriminator $D_{disc}$ and two images $\x$ and $\y$, $D_{disc}(\x)$ is likely similar to $D_{disc}(\y)$ if both images originate from the same data distribution, i.e., $p_{\x} = p_{\y}$. However, $D_{disc}(\x) = D_{disc}(\y)$ is not guaranteed unless the images are identical or indistinguishable by the discriminator.
    \item If $p_{\y}$ differs from $p_{\x}$, $D_{disc}(\y)$ will likely differ from $D_{disc}(\x)$. The output difference is primarily influenced by the aspects of $p_{\y}$ that deviate from $p_{\x}$.
    \item The greater the difference between $p_{\x}$ and $p_{\y}$, the higher the uncertainty in predicting $D_{disc}(\y)$ based on $D_{disc}(\x)$.
\end{itemize}
Based on these insights, a technique is proposed to reduce the discriminator's focus on non-relevant semantic classes.

The approach consist in artificially modifying the reconstructed image $\hat{\x}$ before it is evaluated by the discriminator. This modification aims to minimize the differences in non-relevant regions between $\x$ and $\hat{\x}$, bringing the data distribution of these regions of $\hat{\x}$ closer to the real distribution of $\x$. For example, if the generator reconstructs a tree with dark green leaves when in reality they are light green, the color in $\hat{\x}$ will be artificially shifted toward light green. 

This artificial editing is performed by considering the residual between the real and reconstructed images, defined as:
\begin{equation}
    \br = \x - \hat{\x}.
\end{equation}
By masking this pixel-wise difference between the two images and adding back a fraction of the residual to $\hat{\x}$ it is possible to obtain the new image:
\begin{equation}
    \hat{\x}_{rel} = \hat{\x} + \w_{rel} \odot \br,
\end{equation}
where $\w_{rel}$ is the re-scaling relevance tensor with the following values:

\vspace{0.2cm}
\begin{tabular}{ll}
    \textbullet \;\; $\w_{rel} = 0.90$    & for the class "sky". \\
    \textbullet \;\; $\w_{rel} = 0.80$ & for the class "vegetation". \\
    \textbullet \;\; $\w_{rel} = 0.40$    & for the class "street". \\
    \textbullet \;\; $\w_{rel} = 0$ & for the other classes.
\end{tabular}\\

This means that, for instance, 80\% of the difference between the shades of green leaves is removed before presenting the image to the discriminator. The modified vegetation in $\hat{\x}_{rel}$ will appear much closer to the real light green in $\x$, thus reducing the discriminator's focus on this non-relevant region.

It is important to acknowledge that this approach negatively impacts the generator's ability to accurately reproduce these specific non-relevant classes. Nevertheless, the perceptual loss still considers the entire image, guiding the generator to reconstruct the "sky," "vegetation," and "streets" to maintain overall realism.
\end{quote}
The proposed approach is a simple yet effective way to reduce the discriminator's focus on non-relevant regions. Alongside the weighted \gls{l2} loss these techniques allow the generator to focus on the relevant parts of the image.

Similarly to $G_\s$, the sub-network $G_\x$ is trained for different masking fractions $m_\x$. These values are selected from a finite set ranging from $5\%$ to $100\%$ with an expected value of $35\%$.
The structure of the discriminator $D_{disc}^\x$ is the same as the one used in the training of $G_\s$ and the training is performed using the Adam optimizer with a learning rate of $10^{-4}$ with batch size of $8$. The model is trained for $200$ epochs with early stopping.


\subsection{Fine-tuning of $G$}\label{sec: SQGAN training G}

The final step involves fine-tuning the entire network $G$ by freezing the parameters of $G_\s$ and updating only those of $G_\x$. This fine-tuning addresses the scenario where the original \gls{ssm} $\s$ is unavailable at the receiver, that is when the \gls{sqgan} is used for \gls{sc}.

This fine-tuning is essential since the quality of $\hat{\s}$ may not perfectly match that of the original $\s$, as it is influenced by the masking fraction $m_\s$.

Fine-tuning allows the model to adapt to variations in the quality of $\hat{\s}$ by training $G_\x$ with the reconstructed \gls{ssm}. This ensures that $G_\x$ can effectively reconstruct images based on $\hat{\s}$ despite its imperfections.

The loss function and the Semantic-Aware Discriminator $D_{disc}^\x$ remain identical to those used in the training of $G_\x$. Fine-tuning is conducted over 100 epochs with early stopping to prevent overfitting. During each iteration, both masking fractions $m_\x$ and $m_\s$ are randomly selected from the same distribution. The Adam optimizer is employed with a learning rate of $10^{-4}$ and a batch size of 8.



\section{Results} \label{sec: SQGAN numerical results}
This section will focus on the performances of the proposed \gls{sqgan} evaluated using the 500 pairs of images and \glspl{ssm} from the validation set of the Cityscape dataset. \footnote{More details on the dataset can e found in \sref{sec: SPIC results}}

This chapter is structured as follows. In \sref{sec: SQGAN result samm} the latent vector selection performed by the \gls{samm} will be discussed to show which are the parts of the $\x$ and $\s$ that are considered semantically relevant. Additionally, this section will show how varying the masking fractions $\m_\x$ and $\m_\s$ affects the final reconstruction. In \sref{sec: SQGAN result comparison} the performances of the proposed \gls{sqgan} will be compared with classical compression algorithms like \gls{bpg} and \gls{jpeg2000}.

\subsection{SAMM Performance}\label{sec: SQGAN result samm}
\begin{figure}[!t]
    \centering
    \adjustbox{valign=t}{\includegraphics[width=0.45\textwidth]{Figures/SQ-GAN/real_image.png}}%
    \hspace{5mm}
    \adjustbox{valign=t}{\includegraphics[width=0.45\textwidth]{Figures/SQ-GAN/masked_image.png}}\\[1mm]
    \adjustbox{valign=t}{\includegraphics[width=0.45\textwidth]{Figures/SQ-GAN/real_ssm.png}}%
    \hspace{5mm}
    \adjustbox{valign=t}{\includegraphics[width=0.45\textwidth]{Figures/SQ-GAN/masked_ssm.png}}
    \caption[Visual representation of the effect of the \acrshort{samm} module]{Visual representation of the latent tensor selection of the \acrshort{samm} projected in the image and \acrshort{ssm} domain. In both cases the masking has been fixed to $m_\x=m_\s=0.20$ and the region considered semantically relevant are shown on the right.}
    \label{fig: SQGAN masked images}
\end{figure}

\begin{figure}[!t]
    \centering
    % Text boxes above columns
    \begin{tabular}{>{\centering\arraybackslash}m{0.24\textwidth} 
                    @{\hspace{1mm}}>{\centering\arraybackslash}m{0.24\textwidth} 
                    @{\hspace{1mm}}>{\centering\arraybackslash}m{0.24\textwidth} 
                    @{\hspace{1mm}}>{\centering\arraybackslash}m{0.24\textwidth}}
        Original & 
        $m_\x=0.95, \;m_\s=0.15$ & 
        $m_\x=0.55, \;m_\s=0.55$ & 
        $m_\x=0.15, \;m_\s=0.95$ \\
    \end{tabular}
    \\% Adjust space between text and images

    % Images with corresponding SSM overlayed on the top row
    \begin{tabular}{>{\centering\arraybackslash}m{0.24\textwidth} 
                    @{\hspace{1mm}}>{\centering\arraybackslash}m{0.24\textwidth} 
                    @{\hspace{1mm}}>{\centering\arraybackslash}m{0.24\textwidth} 
                    @{\hspace{1mm}}>{\centering\arraybackslash}m{0.24\textwidth}}
        \begin{tikzpicture}
            \node[anchor=south west,inner sep=0] (image1) at (0,0) {\includegraphics[width=\linewidth]{Figures/SQ-GAN/real_image.png}};
            \node[anchor=south east,inner sep=0] at (image1.south east) {\includegraphics[width=0.45\linewidth]{Figures/SQ-GAN/real_ssm.png}};
        \end{tikzpicture} & 
        \begin{tikzpicture}
            \node[anchor=south west,inner sep=0] (image4) at (0,0) {\includegraphics[width=\linewidth]{Figures/SQ-GAN/masking/img_x95_s15.png}};
            \node[anchor=south east,inner sep=0] at (image4.south east) {\includegraphics[width=0.45\linewidth]{Figures/SQ-GAN/masking/intern_ssm_x95_s15.png}};
        \end{tikzpicture} & 
        \begin{tikzpicture}
            \node[anchor=south west,inner sep=0] (image3) at (0,0) {\includegraphics[width=\linewidth]{Figures/SQ-GAN/masking/img_x55_s55.png}};
            \node[anchor=south east,inner sep=0] at (image3.south east) {\includegraphics[width=0.45\linewidth]{Figures/SQ-GAN/masking/intern_ssm_x55_s55.png}};
        \end{tikzpicture} & 
        \begin{tikzpicture}
            \node[anchor=south west,inner sep=0] (image2) at (0,0) {\includegraphics[width=\linewidth]{Figures/SQ-GAN/masking/img_x15_s95.png}};
            \node[anchor=south east,inner sep=0] at (image2.south east) {\includegraphics[width=0.45\linewidth]{Figures/SQ-GAN/masking/intern_ssm_x15_s95.png}};
        \end{tikzpicture} \\

        % new row for SSM
        \includegraphics[width=\linewidth]{Figures/SQ-GAN/real_ssm.png} & 
        \includegraphics[width=\linewidth]{Figures/SQ-GAN/masking/ssm_x95_s15.png} & 
        \includegraphics[width=\linewidth]{Figures/SQ-GAN/masking/ssm_x55_s55.png} & 
        \includegraphics[width=\linewidth]{Figures/SQ-GAN/masking/ssm_x15_s95.png} \\
    \end{tabular}
    
    \caption[Visual comparison between different results at different masking fractions]{Visual comparison between the same image and \acrshort{ssm} at different masking reactions $m_\x$ and $m_\s$. The original image and \acrshort{ssm} are shown on the left. The upper row shows the reconstructed $\hat{\x}$ and the generated \gls{ssm} using the \gls{sota} \gls{ssmodel} INTERN-2.5 \cite{Wang2022internimage}. The bottom row shows the reconstructed \gls{ssm} $\hat{\s}$. All pairs $(\hat{\x}, \hat{\s})$ are obtained at $0.05$\gls{bpp}.}
    \label{fig: SQGAN visual result changing masking}
\end{figure}
The \gls{sqgan} adopt various techniques to force the model to correctly reconstruct the relevant regions of the image. The weighted loss function based on the semantic classes, the Semantic-Aware Discriminator $D_{disc}^\x$, and the Semantic Relevant Classes Enhancement data augmentation have been designed with this idea in mind. Their scope is to guide the \gls{samm} in identifying and selecting the relevant latent vectors $\z_k^\x$ and $\z_k^\s$. By prioritizing certain regions over other the \gls{samm} learns to assign the truly relevant latent vectors a higher score. 

Since the selection is performed on the latent tensor, the effect of the \gls{samm} has to be projected from the latent tensor to the image and \gls{ssm} domain. These projections are  illustrated in \fref{fig: SQGAN masked images}.\footnote{The projection showed in figure \ref{fig: SQGAN masked images} has been realized by pivoting on the convolutional nature of the two encoders that allow the preservation of the spatial correlation between the latent tensor and the original frame.} 
In the left column the original $\x$ and $\s$ are represented, while the right column shows which are the regions associated to the latent vectors that the \gls{samm} consider more relevant. In this analysis the masking fractions have been fixed to $m_\x=m_\s=0.20$.

It is immediately evident that $G_\x$ and $G_\s$ consider different regions as relevant. The \gls{samm} in $G_\s$ focuses on regions with the most change in semantic classes. The street, building and sky requires very few dedicated latent vectors to be represented. On the contrary the parts of the \gls{ssm} containing relevant classes are strongly  preferred. 

In a similar way the \gls{samm} in $G_\x$ shows a strong preference for relevant classes like cars and people. However, it also focuses on areas previously ignored, such as the street and buildings. In fact, thanks to the conditioning on the \gls{ssm}, the sub-network $G_\x$ knows the location of every object and their shape and can focus on different aspects like colors and textures. For this reason the streets and sky will require some latent vectors to reconstruct the colors correctly. However, most of the latent vectors are selected from the truly relevant regions.

In both cases the \gls{samm} tends to prefer regions that contain more semantically relevant objects. This is a direct effect of the various techniques adopted to train the model as described in \sref{sec: SQGAN training}. \\

Another important analysis examines the effects of different masking fractions on the output. In fact increasing $m_\x$ and $m_\s$ is expected to increase the overall quality of $\hat{\x}$ and $\hat{\s}$ respectively, while decreasing is expected to do the opposite. \fref{fig: SQGAN visual result changing masking} shows visually how masking fractions influences the reconstructed outputs.

The column on the left shows the original $\x$ and $\s$. The other columns show on top the reconstructed $\hat{\x}$ and the generated \gls{ssm} obtained form $\hat{\x}$ via the INTERN-2.5 \gls{ssmodel} \cite{Wang2022internimage}. The bottom row shows the reconstructed $\hat{\s}$. All the pairs of $\hat{\x}$ and $\hat{\s}$ have been reconstructed, fixing the compression level to a total amount of $0.05$\gls{bpp} and by letting the masking fractions $m_\x$ and $m_\s$ vary.

The first consideration is the reconstructed $\hat{\x}$. It is evident that the presence of an object in $\hat{\x}$ is influenced more by the quality of $\hat{\s}$ and thus on $m_\s$  than on $m_\x$. The lower the amount of object and detail in $\hat{\s}$ the lower will the amount that $\hat{\x}$ is able to retain be. This is clearly be seen by comparing the $\hat{\s}$ on the bottom row and the generated \gls{ssm} in the upper row. If $\hat{\s}$ does not contain some detail, that same detail is not present in the generated \gls{ssm}.\\
The other aspect is the influence of $m_\x$ on the quality. As expected the increase of $m_\x$ tends to improve the overall image quality especially on the non-relevant details. The relevant details are reconstructed and prioritized even for low values of $m_\x$, while the windows of the building are better reconstructed when $m_\x=0.95$. This is important since it shows that the model is able to first reconstruct relevant classes and then improve the non-relevant.

The other consideration is on the reconstructed $\hat{\s}$. It is possible to see that the change in masking fractions $m_\s$ influences the reconstruction of the \gls{ssm} up to a certain value. Both the reconstructed $\hat{\s}$ obtained at $m_\s=0.55$ and $m_\s=0.95$ look in fact almost identical.\\
This behavior can be seen in \fref{fig: SQGAN miou vs m_s} where the \gls{miou} between $\s$ and $\hat{\s}$ is reported as a function of $m_\s$. For very low values of $m_\s$ the performances are not optimal. As soon as $m_\s \geq 0.20$, the model is able to reconstruct $\hat{\s}$ with a decent level of semantic retention. To give a term of comparison the value of $m_\s =0.20$  correspond to a compression of $BPP_\s=0.011$\gls{bpp}. This means that at this low value of \gls{bpp} the model is already able to preserve a lot of details in the \gls{ssm}. After the value of $m_\s=0.55$ (equivalent to $BPP_\s=0.025$\gls{bpp}), there is no further advantage in increasing the masking fraction. After this value, all the relevant latent vectors characterizing the \gls{ssm} have been used. Adding other vectors will only increase the amount of redundant information, thus not improving the reconstructed \gls{ssm}. This is the opposite of what was discussed about $m_\x$, where even for values close to 1 the improvements were still visible on the final output.
\begin{figure}[!t]
    \centering
    \begin{minipage}[b]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Figures/SQ-GAN/plots/mIoU_mIoU_vs_m_s.pdf}
        \caption[\acrshort{ssm} retention as a function of the masking fraction]{\acrshort{ssm} retention  evaluated between the true $\s$ and the reconstructed $\hat{\s}$ with the \gls{miou} metric as a function of the masking fraction $m_\s$. As the masking fraction $m_\s$ increases the network $G_\s$ is able to better reconstruct the \gls{ssm}. However, the increase of performances reaches a plateau from $m_\s \geq 0.2$ ($BPP_\s=0.011$\gls{bpp}).}
        \label{fig: SQGAN miou vs m_s}
    \end{minipage}
    \hfill
    \begin{minipage}[b]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Figures/SQ-GAN/plots/LPIPS_3d.png}
        \caption[\acrshort{lpips} as a function of the masking fractions]{\acrshort{lpips} evaluated between $\x$ and $\hat{\x}$ as the masking fractions $m_\s$ e $m_\x$ vary.}
        \label{fig: SQGAN lpips 3d plot}
    \end{minipage}
\end{figure}

Similar to the result in \fref{fig: SQGAN miou vs m_s} it is interesting to see how both $m_\x$ and $m_\s$ influence $\hat{\x}$. For this scope in \fref{fig: SQGAN lpips 3d plot} the \gls{lpips} evaluated on $\hat{\x}$ is reported. The effects of both masking fractions are clear. As expected, the performances along the $m_\s$ axis are influencing the output only for values of $m_\s \leq 0.55$. The influence on $m_\x$ is instead along the whole range from 0 to 1. This is perfectly explaining the visual results analysed in \fref{fig: SQGAN visual result changing masking}.

The results discussed in this section help to understand how the model performs as a function of the masking fractions $m_\x$ and $m_\s$. However, the most important part is to understand how the model is performing in comparison with other data compression techniques. This will be discussed in the next section.

\subsection{Visual Results and Comparisons with Classical Approaches}\label{sec: SQGAN result comparison}
\begin{figure}[!t]
    \centering
    % Text boxes above columns
    \begin{tabular}{>{\centering\arraybackslash}m{0.24\textwidth} 
                    @{\hspace{1mm}}>{\centering\arraybackslash}m{0.24\textwidth} 
                    @{\hspace{1mm}}>{\centering\arraybackslash}m{0.24\textwidth} 
                    @{\hspace{1mm}}>{\centering\arraybackslash}m{0.24\textwidth}}
        Original & 
        $0.019$\textit{BPP} & 
        $0.038$\textit{BPP} & 
        $0.078$\textit{BPP} \\
    \end{tabular}
    \\% Adjust space between text and images

    % Images with corresponding SSM overlayed
    \begin{tabular}{>{\centering\arraybackslash}m{0.24\textwidth} 
                    @{\hspace{1mm}}>{\centering\arraybackslash}m{0.24\textwidth} 
                    @{\hspace{1mm}}>{\centering\arraybackslash}m{0.24\textwidth} 
                    @{\hspace{1mm}}>{\centering\arraybackslash}m{0.24\textwidth}}
        \begin{tikzpicture}
            \node[anchor=south west,inner sep=0] (image1) at (0,0) {\includegraphics[width=\linewidth]{Figures/SQ-GAN/real_image.png}};
            \node[anchor=south east,inner sep=0] at (image1.south east) {\includegraphics[width=0.45\linewidth]{Figures/SQ-GAN/real_ssm.png}};
        \end{tikzpicture} & 
        \begin{tikzpicture}
            \node[anchor=south west,inner sep=0] (image2) at (0,0) {\includegraphics[width=\linewidth]{Figures/SQ-GAN/comparisons/img/image_0194.png}};
            \node[anchor=south east,inner sep=0] at (image2.south east) {\includegraphics[width=0.45\linewidth]{Figures/SQ-GAN/comparisons/img/ssm_0194.png}};
        \end{tikzpicture} & 
        \begin{tikzpicture}
            \node[anchor=south west,inner sep=0] (image3) at (0,0) {\includegraphics[width=\linewidth]{Figures/SQ-GAN/comparisons/img/image_0389.png}};
            \node[anchor=south east,inner sep=0] at (image3.south east) {\includegraphics[width=0.45\linewidth]{Figures/SQ-GAN/comparisons/img/ssm_0389.png}};
        \end{tikzpicture} & 
        \begin{tikzpicture}
            \node[anchor=south west,inner sep=0] (image4) at (0,0) {\includegraphics[width=\linewidth]{Figures/SQ-GAN/comparisons/img/image_0780.png}};
            \node[anchor=south east,inner sep=0] at (image4.south east) {\includegraphics[width=0.45\linewidth]{Figures/SQ-GAN/comparisons/img/ssm_0780.png}};
        \end{tikzpicture} \\
        % new row for BPG
        \begin{tikzpicture}
            \node[anchor=south west,inner sep=0] (image5) at (0,0) {\includegraphics[width=\linewidth]{Figures/SQ-GAN/real_image.png}};
            \node[anchor=south east,inner sep=0] at (image5.south east) {\includegraphics[width=0.45\linewidth]{Figures/SQ-GAN/real_ssm.png}};
        \end{tikzpicture} & 
        % Empty slot for (row 2, col 2) 
        & 
        \begin{tikzpicture}
            \node[anchor=south west,inner sep=0] (image7) at (0,0) {\includegraphics[width=\linewidth]{Figures/SQ-GAN/comparisons/bpg/image_0383.png}};
            \node[anchor=south east,inner sep=0] at (image7.south east) {\includegraphics[width=0.45\linewidth]{Figures/SQ-GAN/comparisons/bpg/ssm_0383.png}};
        \end{tikzpicture} & 
        \begin{tikzpicture}
            \node[anchor=south west,inner sep=0] (image8) at (0,0) {\includegraphics[width=\linewidth]{Figures/SQ-GAN/comparisons/bpg/image_0785.png}};
            \node[anchor=south east,inner sep=0] at (image8.south east) {\includegraphics[width=0.45\linewidth]{Figures/SQ-GAN/comparisons/bpg/ssm_0785.png}};
        \end{tikzpicture} \\
    \end{tabular}
    
    \caption[Visual comparison between \acrshort{bpg} and \acrshort{sqgan}]{Visual comparison at different compression rates between the proposed \acrshort{sqgan} (TOP) and the classical \acrshort{bpg} (BOTTOM). The \glspl{ssm} shown are generated from $\hat{\x}$ via the \acrshort{sota} \acrshort{ssmodel} INTERN-2.5 \cite{Wang2022internimage}. The proposed model is able to reconstruct images with higher semantic retention and lower values of \acrshort{bpp} compared with \acrshort{bpg}. The \acrshort{bpg} algorithm is not able to compress images at lower values than $0.038$ \acrshort{bpp}, thus the comparison is limited to $0.038$ and $0.078$ \acrshort{bpp}.}
    \label{fig: SQGAN visual comparison sqgan bpg}
\end{figure}
In this section the results of the proposed \gls{sqgan} are compared with the classical compression algorithm \gls{bpg} and \gls{jpeg2000}. Before numerically presenting the performances in term of semantic and classical metrics, it is important to visually see the differences. The visual comparison between \gls{bpg} and \gls{sqgan} is shown in \fref{fig: SQGAN visual comparison sqgan bpg}. \\

The top row represent the reconstructed image $\hat{\x}$ obtained with the proposed \gls{sqgan} and the associated generated \gls{ssm} via the INTERN-2.5 \gls{ssmodel}. The bottom row is showing the reconstructed image obtained by using the \gls{bpg} algorithms and the associated generated \gls{ssm}. 

As a first examination it is noticable that the \gls{sqgan} can reach lower values of \gls{bpp}. The classic \gls{bpg} cannot compress images at \gls{bpp} lower than $0.038$.

The other observation is that while the classical \gls{bpg} is using precious resources to reconstruct the windows of the buildings, the \gls{sqgan} is focusing on the relevant parts. This is shown in the amount of semantic retention of the generated \gls{ssm}. The person riding the bicycle is still visible and correctly identified by the \gls{ssmodel} even if the windows of the building are reconstructed with less detailed. On the contrary the \gls{bpg} is trying to reconstruct the window and the person riding the bicycle with the same level of detail.\\
As a matter of comparison, to obtain a level of semantic retention similar to the one obtained by the \gls{sqgan} at $0.038$\gls{bpp} for the \gls{bpg} algorithm almost $0.280$\gls{bpp} are required.\\

\begin{figure}[!t]
    \centering
    \begin{subfigure}[t]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Figures/SQ-GAN/plots/FID_FID_vs_BPP.pdf}
        \caption*{(a)} % Caption under image without adding to list
    \end{subfigure}%
    \hspace{5mm}
    \begin{subfigure}[t]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Figures/SQ-GAN/plots/lpips_lpips_vs_BPP.pdf}
        \caption*{(b)}
    \end{subfigure}
    
    \vspace{5mm} % Add space between rows

    \begin{subfigure}[t]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Figures/SQ-GAN/plots/mIoU_internimage_mIoU_internimage_vs_BPP.pdf}
        \caption*{(c)}
    \end{subfigure}%
    \hspace{5mm}
    \begin{subfigure}[t]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Figures/SQ-GAN/plots/psnr_psnr_vs_BPP.pdf}
        \caption*{(d)}
        \label{fig: SQGAN psnr vs bpp}
    \end{subfigure}

    \caption[Performance comparison between the \acrshort{sqgan} and classical compression algorithms]{Performance comparison between \acrshort{bpg}, \acrshort{jpeg2000} and \acrshort{sqgan} in term of semantic metrics and the classic pixel-by-pixel \acrshort{psnr}.}
    %[Comparison performances between \acrshort{sqgan} and classical compression algorithms]{Performance comparisons between \gls{sqgan} and classical compression algorithms. The non-semantic metric \gls{psnr} (d) is the only metric where \gls{bpg} performs better. In all the semantic-related metrics assessing image quality, such as \gls{fid} (a), \gls{lpips} (b), and \gls{miou} (c), the proposed model outperforms classical algorithms.}
    \label{fig: SQGAN all metrics comparison}
\end{figure}
These improvements can additionally be evaluated via comparison metrics as in \fref{fig: SQGAN all metrics comparison}. The figure shows the comparison between \gls{sqgan} and classical image compression algorithms in terms of \gls{fid}, \gls{lpips}, \gls{psnr} and \gls{miou}. The first 3 metrics are evaluated between the original $\x$ and the reconstructed $\hat{\x}$, while the \gls{miou} is evaluated between $\s$ and the \gls{ssm} generated via the \gls{sota} INTERN-2.5 \gls{ssmodel}.

The first important advantage is that the proposed \gls{sqgan} is able to compress at a lower level of \gls{bpp}. Moreover, on semantic metrics like \gls{fid}, \gls{lpips}, and \gls{miou}  the \gls{sqgan} consistently outperforms classical algorithms at a fraction of the \gls{bpp}. The only metric where \gls{bpg} performs better is the \gls{psnr}. This is not a surprise since this is a classical pixel-by-pixel metric that does not consider the overall visual quality but only the distance in the pixel domain. However, in a \gls{sc} framework, not all pixels have the same importance. Performing better on pixel-by-pixel metrics and not on semantic relevant metrics is not an advantage. For example, at $0.038$\gls{bpp} the \gls{bpg} algorithm can reconstruct images with a \gls{psnr} of 26, but it fails to preserve the \gls{ssm}. As shown in \fref{fig: SQGAN visual comparison sqgan bpg}, while \gls{bpg} reconstructs the windows of buildings better, only the first two cars are detected and the rest of the objects are lost.

Overall, the proposed \gls{sqgan} demonstrates exceptional performances. The model compresses images at very low \gls{bpp} while preserving their semantic content. It outperforms classical algorithms by learning to select relevant parts of the image focusing on key elements and considering less relevant parts only when asked to.\\

In summary, the proposed \gls{sqgan} achieves remarkable \gls{sc} image compression by effectively balancing low \gls{bpp} and high semantic retention. The integration of the \gls{samm} and \gls{spe} allows for selective prioritization of semantically relevant regions, ensuring that critical classes such as "traffic signs" and "traffic lights" are accurately reconstructed. The introduction of the Semantic-Aware Discriminator further enhances the model's ability to focus on important semantic details while minimizing attention on less relevant areas. Additionally, the Semantic Relevant Classes Enhancement data augmentation technique plays a crucial role in addressing the challenge of underrepresented classes, thereby improving the overall robustness and effectiveness of the compression process.

Experimental results on the Cityscapes dataset demonstrate that \gls{sqgan} consistently outperforms classical compression algorithms like \gls{bpg} and \gls{jpeg2000} across various metrics, including \gls{miou}, \gls{fid} and \gls{lpips}. These findings highlight \gls{sqgan}'s superior capability in preserving semantic content while maintaining efficient compression rates. The visual comparisons further corroborate the quantitative results, showcasing \gls{sqgan}'s ability to retain essential semantic details even at lower \gls{bpp} levels.

These advancements establish \gls{sqgan} as a potent tool for applications requiring both high compression efficiency and meticulous preservation of semantic information. Building on the \gls{sqgan}, the following chapter explores its application in \gls{goc} Resource Allocation in Edge Networks.