\chapter{\textcolor{black}{Semantic-Preserving Image Coding based on Conditional Diffusion Models}}\label{ch: SPIC}

The content of this chapter is entirely based on the following publications:
\begin{quotation}
    \noindent \textit{\textbf{\large Semantic-Preserving Image Coding based on Conditional Diffusion Models}}\\
    \textit{Francesco Pezone, Osman Musa, Giuseppe Caire, Sergio Barbarossa}

    \vspace{0.1cm}
    \noindent \textit{\textbf{\large C-SPIC: Class-Specific Semantic-Preserving Image Coding with Residual Enhancement for Accurate Object Recovery}}\\
    \textit{Francesco Pezone, Osman Musa, Giuseppe Caire, Sergio Barbarossa}
\end{quotation}

\section{Introduction}
After having discussed the overall idea behind the concept of \gls{sc} and some of the most important generative architectures, it is possible to merge these two worlds. In this chapter the challenge of high-quality image reconstruction is addressed. This is performed via the development of a framework based on \glspl{ddpm} that can serve as a viable alternative to classical image compression algorithms. \\

Before continuing any further it is important to define what constitutes the semantically relevant information in this context. The assumption that will be made in this chapter is that the semantic relevant information consists of the following elements: (i) the lossless reconstruction of the \gls{ssm}; (ii) the reconstructed image should retain as much of the \gls{ssm} as possible; (iii) the overall characteristics of the objects such as colors and details should be preserved; and (iv), if required, certain semantic classes will be considered semantically relevant information, necessitating their high-quality reconstruction regardless of the quality of the rest of the image.

According to these assumptions, this chapter introduces the \gls{spic} framework, a modular semantic image communication scheme based on the generating power of \glspl{ddpm}. The \gls{spic} framework consist of a transmitter and receiver side. At the transmitter the image $\x$ is processed to generate the \gls{ssm} $\s$ and a low-quality (coarse) version $\co$ before sending them to the receiver. The \gls{ssm} is send losslessly while the coarse lossy. At the receiver they are used as input of the proposed  \gls{semcore} to reconstruct the full-resolution image $\hat{\x}$ similar to $\x$.\\

Prominent recent models have exploited the use of the  \gls{ssm} to guide the image reconstruction process. For example, Isola et al. \cite{isola2017image2image} introduced the pix2pix model, a conditional \gls{gan} that uses the \gls{ssm} as input to generate an image that is able to preserve the \gls{ssm} content.

Wang et al. \cite{Wang2018HighRes} addressed the challenge of image reconstruction by proposing a more efficient variant of a pix2pix model. Instead of generating the image solely from the \gls{ssm}, they introduced a multi-step approach that integrates features extracted from the original image. This method was able to achieve incredible reconstruction capabilities to produce an image $\hat{\x}$ more similar to $\x$ in term of colour and texture, while preserving the same level of \gls{ssm} as for the pix2pix model.

Recently, \gls{ddpm} \cite{Ho2020ddpm} have demonstrated remarkable results in image synthesis \cite{Dhariwal2021DDPM_beat_GAN, Nichol2021Improved_DDPM}. Building upon the works of Isola et al. and Wang et al., in \cite{Wang2022SISDM} it was introduced a \gls{ddpm} exploiting the conditioning on the \gls{ssm}. This conditioning was enforced by the extensive use of the \gls{spade} normalization layer \cite{Park2019SPADE}. While being able to produce images with a high level of \gls{ssm} preservation, even this model presents a limitation on other aspects of reconstruction.

A common limitation to the cited models is the oversight of the original image. By considering only the \gls{ssm} as input, the resulting image may be visually appealing but every time an image is produced it will look differently in colors and textures, even if the \gls{ssm} is the same. The sole constraint of these algorithms is the preservation of the \gls{ssm}, which is not enough to preserve all the other semantically relevant information discussed before.

Classical image compression algorithms, such as \gls{bpg} \cite{Bellard2017BPG} and \gls{jpeg2000}, approach the problem from a different perspective. These algorithms are designed to reduce the file size of images by exploiting redundancies in the data, employing techniques like discrete cosine transforms and wavelet transforms. While they are effective at maintaining a certain level of visual quality at lower bit rates, they primarily focus on optimizing classical metrics like \gls{psnr}  rather than the semantic content. Consequently, they may indiscriminately discard information that is crucial for semantic interpretation, such as fine details in specific regions or features important for tasks like object recognition or segmentation. This lack of semantic awareness can lead to a degradation of semantically relevant information, which is detrimental in applications where understanding the content of the image is as important as its visual appearance.

An alternative approach to enhancing image quality is through \gls{sr} models, which aim to reconstruct \gls{hr} images from their \gls{lr} counterparts. Techniques like \cite{Dong2015SRCNN, Lim2017EDSR, Bruna2016SuperResolution, Rombach2022SR_CVPR} utilize various architectures, from classical \gls{dnn} to more advanced \gls{ddpm}, to learn the mapping between \gls{lr} and \gls{hr} image pairs. While \gls{sr} models can effectively enhance details and improve visual quality, they often  operate under the assumption that the input is a uniformly down-scaled version of the original image, which may not hold true in practical scenarios involving complex degradation or compression artifacts. As a result, while they can improve the overall appearance of an image, they may not adequately restore semantically significant details required for tasks that rely on precise content understanding.\\

 
In this context is inserted the proposed \gls{spic} framework. Although slightly suboptimal compared to conventional approaches in terms of the overall rate-distortion curve, the proposed method allows for a significantly better reconstruction when considering the combination of the multiple criticalities of other models.

What is more, thanks to the modular approach and the doubly conditioned \gls{semcore}  this framework is able to be easily modified without the need of further fine-tuning or re-training. This chapter proposes additionally to the \gls{spic} also an improved version, referred to as \gls{cspic}. This variation is introduced to overcome some limitations of the \gls{spic} framework in reconstructing small and detailed objects.\\

This chapter is organized as follows: In \sref{sec: SPIC spic}, the basic modular structure of \gls{spic} is introduced, focusing on the transmitter and receiver architectures and the proposed \gls{semcore} model. In \sref{sec: SPIC residuo}, the \gls{cspic} is introduced as a modified version of the \gls{spic} to allow better reconstruction of specific details, without requiring fine-tuning or re-training of the \gls{semcore} model. In \sref{sec: SPIC results}, the results of the proposed architectures are compared with classical compression algorithms such as \gls{bpg} and \gls{jpeg2000}.

\section{Semantic-Preserving Image Coding (SPIC)}\label{sec: SPIC spic}

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{Figures/SPIC/schema_spic.pdf}
    \caption[\acrshort{spic} architecture scheme]{Overview of the \acrshort{spic} architecture. At the transmitter side the \acrshort{ssm} $\s$ and the coarse image $\co$ are extracted from the original image and compressed with classical off-the-shelf compression algorithms. At the receiver it is employed the \acrshort{semcore} that leverages both $\s$ and $\co$ for high-fidelity semantic-relevant image recovery even at low BPP.}
    \label{fig: SPIC spic_scheme}
\end{figure}

In this section, the basic modular structure of the \gls{spic} framework is introduced, as illustrated in \fref{fig: SPIC spic_scheme}. The architecture comprises a transmitter and a receiver, which will be discussed separately.

\subsection{Transmitter Side}\label{sec: SPIC spic_transmitter}

As shown in \fref{fig: SPIC spic_scheme}, the transmitter consists of two separate pipelines designed to extract the \gls{ssm} $\mathbf{s}$ and the coarse image $\mathbf{c}$ from the input image $\mathbf{x}$.

The design aims to keep the system simple while allowing for future improvements and modifications. By adopting a modular approach, it is possible to integrate multiple components without the need for fine-tuning or re-training the model.

\begin{itemize}[label={}]
    \item \textbf{\gls{ssm} Pipeline:} As discussed at the beginning of the chapter, one of the fundamental semantic information that has to be available at the receiver is the \gls{ssm}, transmitted in a lossless way.

    In this work, the \gls{ssm} $\s$ is extracted directly from the original image $\x$ using an out-of-the-shelf pre-trained \gls{sota} \gls{ssmodel}. The INTERN-2.5 model \cite{Wang2022internimage} is chosen for this task due to its high performance in semantic segmentation. However, the modular structure of the \gls{spic} allows other choices to be made.

    After generating the \gls{ssm}, the next step is lossless compression. The output RGB-colored \gls{ssm} is mapped to grayscale by assigning each class a specific shade of gray. This grayscale \gls{ssm} is then compressed using the lossless compression algorithm \gls{flif} \cite{Sneyers2016FLIF}. This algorithm ensures efficient encoding of the \gls{ssm} with an average of 0.112 \gls{bpp}. \\

    \item \textbf{Coarse Image Pipeline:} This step is fundamental in compressing all other semantically relevant information about the original image that are not contained in the \gls{ssm}. Examples include object details and patterns, colors, and more.

    The process starts with an initial down-scaling. The image $\x$ is transformed from its original resolution of $256 \times 512$ pixels to $64 \times 128$ pixels. This operation is performed by averaging the pixel values in corresponding patches.

    Thanks to the down-scaling it is possible to preserve a general "overview" of the original image $\x$ with some of the most semantically relevant information about color, patterns, etc. by using only a fraction of the total number of pixels.

    Unlike the \gls{ssm}, perfect reconstruction of the coarse image at the receiver is not required. Therefore, a lossy compression algorithm can be employed. In this work, the \gls{bpg} algorithm is used.

    This algorithm can compress the coarse image $\co$ at different resolutions, measured in terms of \gls{bpp}, providing a lot of flexibility. In fact, the value at which $\co$ is compressed will affect the level of detail preservation and, consequently, the quality of the final reconstructed image $\hat{\x}$.
\end{itemize}

After this pre-processing, both the encoded $\s$ and $\co$ are sent to the receiver.

\subsection{Semantic-Conditioned Super-Resolution Diffusion Model (SemCoRe)}\label{sec: SPIC semcore}

At the receiver, the incoming encoded $\s$ and $\co$ are decoded using their corresponding algorithms.

Beyond this, the receiver consists solely of the proposed \gls{semcore} block. Its purpose is to reconstruct an image $\hat{\x}$ that preserves all the semantic information of $\x$. \\

The architecture of the proposed \gls{semcore} is based on the work of Wang et al. \cite{Wang2022SISDM}. They introduced an \gls{ssm}-conditioned \gls{ddpm} capable of generating visually appealing images with high levels of \gls{ssm} preservation but colors and texture completely different from the original image. In this thesis, thanks to the additional conditioning on the coarse image this problem has been mitigated. The architecture of the proposed \gls{semcore} is as follows:

\begin{itemize}[label={}]
    \item {\textbf{Encoder}:} The encoder is the part of the network responsible for the extraction of the most important features. As already introduced in \sref{sec: GM unet} and \sref{sec: GM ddpm_advanced_techniques}, the conditioning in a \gls{unet} is strongly dependent on the task and since the scope of the encoder is to extract as much information as possible from the input, it is useful to use the conditioning on the coarse $\co$ at this stage. Specifically, the input tensor has a shape of $6 \times 256 \times 512$ pixels. The first three channels correspond to the RGB values of the noisy image $\x_t$ at time-step $t$, and the last three channels correspond at every iteration to the same RGB values of the up-scaled coarse image $\co$\footnote{The up-scaling is performed via linear interpolation to restore the original shape of $256 \times 512$ pixels. This is done to reverse the down-scaling performed at the transmitter side.}. This concatenated input allows the encoder to process both images simultaneously and capture important details from the coarse image at every step of the diffusion process. 
    
    The structure of the encoder $E$ consist of an initial convolutional layer that map the input tensor of shape $6 \times 256 \times 512$ to a tensor of shape $512 \times 256 \times 512$. This is followed by a repeated alternating series of two time-conditioned \glspl{resblock}, depicted in \fref{fig: GM resblock with time}, a multi-head self-attention layer with 8 heads and a down-scaling layer. This sequence is repeated 2 times. After an additional time-conditioned \glspl{resblock} and a multi-head self-attention layer with 8 heads are employed to conclude the encoding phase. 
    
    At the end of the encoding phase the tensor $\z = E(\x_t, \co)$ has a shape of $256 \times 64 \times 128$.\\
    \item {\textbf{Bottleneck}:} The bottleneck is that part of the \gls{unet} at the beginning of the up-scaling phase. For this reason it is important to start the conditioning process on the \gls{ssm} to enforce its structure. This is performed by the use of the \gls{spade} layer introduced in the \gls{resblock}. In this case the \gls{resblock} is obtained by merging the time-conditioned version in \fref{fig: GM resblock with time} with the one that uses the \gls{ssm} conditioning in \fref{fig: GM resblock spade} \cite{Wang2022SISDM}. In this way the \gls{unet} will be able to influence the denoising on the current time-step and enforce the structure of the \gls{ssm}. This \gls{resblock} will be referred to as Diff-Dec \gls{resblock}.
    
    The structure of the bottleneck is composed by one Diff-Dec \gls{resblock} followed by a multi-head self-attention layer with 8 heads and another Diff-Dec \gls{resblock} and the output shape is preserved.
    \item {\textbf{Decoder}:} The scope of the decoder is to produce the denoised version of $\x_t$ in a way that is coherent with the \gls{ssm}. Its structure is composed of an initial Diff-Dec \gls{resblock} followed by a multi-head self-attention layer with 8 heads. Then there is a sequence of two Diff-Dec \gls{resblock}, a multi-head self-attention layer with 8 heads and a linear up-scaling. This sequence is repeated 2 times. The final block of the decoder consist in a  Diff-Dec \gls{resblock} followed by a convolutional layer. 
    
    At the end the output correspond to the prediction $\bepsilon_0$ of shape $3 \times 256 \times 512$ to be removed from $\x_t$ to obtain the estimate of $\x_0\equiv \x$.
\end{itemize}

After that the \gls{unet} structure of the \gls{semcore} has been defined it is possible to focus on the diffusion step. 
As introduced in \sref{sec: GM ddpm training_inference}, the training consist of $T=1000$ diffusion steps. At inference, to save computational resources, only a reduced number of diffusion steps is performed, $T=20$.

On top of that, to better enforce the preservation of the \gls{ssm} the training is performed by applying the \gls{cfg} technique discussed in \sref{sec: GM ddpm_advanced_techniques} illustrated in \fref{fig: GM schema_cfg}.

By combining the dual conditioning approach and advanced training procedures, the \gls{semcore} model is able to produce \gls{sr} images of the coarse $\co$, that are visually close to the original. In this way the relevant semantic information is better preserved.


\section{Class-Specific SPIC (C-SPIC)}\label{sec: SPIC residuo}

\begin{figure}[!t]
    \centering
    \includegraphics[width=\textwidth]{Figures/SPIC/schema_spic_residuo_brutto.pdf}
    \caption[\acrshort{cspic}$^w$ architecture scheme]{Overview of the \acrshort{cspic}$^w$ architecture. At the transmitter after the generation and compression of $\s$ and  $\co$ the \acrshort{semcore} is used to evaluate the intermediate reconstructed image $\tilde{\x}$ and the relevant residual $\br$. This residual is processed and compacted to preserve only information about some relevant classes before being send to the receiver. At the receiver the $\tilde{\x}$ is evaluated starting from $\s$ and $\co$ and then the relevant residual is processed and de-compacted before adding it back to produce the final reconstructed image $\hat{\x}$.}
    %[C-SPIC scheme with residual]{Overview of the proposed C-SPIC architecture with residual.}
    \label{fig: SPIC cspic with residual scheme}
\end{figure}

Even if the \gls{spic} framework is already able to reconstruct images that are more semantically relevant and visually appealing compared to classical compression algorithms and other \gls{dnn}-based models, it still lacks one important detail.
In one of the first stages of \gls{spic}, the original image $\x$ is down-scaled to produce the coarse image $\co$. Although this down-scaling allows for a more efficient compression of the data, some specific small details are inevitably lost.

This phenomenon can be neglected in many cases but not in some other. For example in an urban scenario where knowing only that  the color of a traffic sign is blue is not always enough. By down-scaling the image to produce the coarse, some information about colors are preserved, but the small details  inside the traffic sign might be lost.

To address this limitation, the \gls{spic} framework is further modified to consider some specific object as semantically relevant, thus improving their reconstruction. This modified version is referred to as \gls{cspic} and is implemented without any need for fine-tuning or re-training of the \gls{semcore} model.\\

The idea is based on the \gls{dsslic} framework proposed in \cite{Akbari2019DSSLIC}. In their work, they introduced a framework to improve the reconstruction quality of images. Instead of evaluating the model only at the receiver, the model is evaluated at both ends to produce an intermediate reconstruction $\tilde{\x}$. At the transmitter side, the difference between the original image and the intermediate reconstruction is evaluated, compressed and sent to the receiver. This quantity is referred to as residual, $\br = \x - \tilde{\x}$. At this point at the receiver, after reconstructing the intermediate representation $\tilde{\x}$, the residual is added back. This will improve the quality of the final reconstructed image $\hat{\x}$.\\

A similar idea is implemented and proposed in this section, but with a substantial difference: the residual is considered only for the semantically relevant classes and not for the whole frame. This change allows for significant savings in terms of \gls{bpp} compared to the transmission of the full residual and this modification will not affect the reconstruction of the remaining semantic information.

This section is divided into two parts. In \sref{sec: SPIC residual dsslic}, a first version named \gls{cspic}$^w$ is introduced as a variation of the \gls{dsslic} framework. The apex $^w$ is used to specify that the residual is produced and transmitted. In \sref{sec: SPIC image dsslic}, another version referred to as \gls{cspic}$^{w/o}$ is presented. This variation is introduced to overcome the computational complexity of \gls{cspic}$^w$. In fact, to obtain the residual $\br$ it is fundamental to use the \gls{semcore} even at the transmitter side. This can be feasible in some applications but not doable in all those in which the transmitter has a low computational power. The \gls{cspic}$^{w/o}$ is designed to have the same computational complexity as for the classic \gls{spic}, and allow the same performances as \gls{cspic} with an increase of at most 10\% of the total \gls{bpp}.

\subsection{C-SPIC with Residual (C-SPIC$^w$)}\label{sec: SPIC residual dsslic}

The scheme of the proposed \gls{cspic}$^w$ framework is depicted in \fref{fig: SPIC cspic with residual scheme}.

Differently from the base version of \gls{spic} shown in \fref{fig: SPIC spic_scheme}, the structure of both the transmitter and receiver consist now of multiple elements. This is absolutely necessary  to improve the reconstruction capabilities of certain relevant semantic classes. 

The \gls{cspic}$^w$ is composed as follows:

\subsubsection{Transmitter Side}\label{sec: SPIC cspic_with transmitter}

The initial processing of the original image $\x$ is the same as discussed for \gls{spic} in \sref{sec: SPIC spic_transmitter}, the coarse image $\co$ and the \gls{ssm} $\s$ being produced and transmitted to the receiver using the same approach.

Differently from before, the coarse image and \gls{ssm} are now processed by the \gls{semcore} directly at the transmitter to output the intermediate reconstructed image $\tilde{\x}$. This image is then used to produce the general residual $\br = \x - \tilde{\x}$ of the whole image.

However, since only certain semantic classes are considered as relevant and require higher reconstruction performance, the residual is processed as depicted in \fref{fig: SPIC cspic residual pipeline} undergoing three different steps:

\begin{enumerate}
    \item \textbf{Masking:} First, a binary mask $\m$ is obtained from the \gls{ssm} $\s$. Only the pixels corresponding to the semantically relevant classes are set to 1, and the others to 0. This binary mask $\m$ is used to perform an element-wise multiplication with the residual $\br$, obtaining $\br_m = \br \odot \m$, as shown in \fref{fig: SPIC cspic residual pipeline}(a). This step effectively filters out the residual information for non-relevant classes.

    \item \textbf{Compacting:} This is the step that allows to transform the residual from the representation in \fref{fig: SPIC cspic residual pipeline}(a) to (b). 
    
    The invertible algorithm $g$ for compacting the residual, and removing the empty regions is described in detail in \aref{app: SPIC compacting} and only requires the \gls{ssm} to be performed. The advantage of using the compacted representation lies in the fact that the residual can be represented with fewer bits because of the smaller shape of compacted version. For instance, from its original shape of $256 \times 512$ pixels, the reshaped $g(\br_m)$ has an average shape of $40 \times 40$ pixels when considering traffic signs, $30 \times 40$ pixels when considering traffic lights, and $80 \times 40$ pixels when considering pedestrians. This represents a significant reduction in data size, allowing for lower amount of \gls{bpp} in compression.

    \item \textbf{Color Adjusting:} The final step involves re-scaling the pixel values. Since the residual is a difference between two images, the values of each RGB component are in the range $[-255, 255]$. To make these values compatible with the \gls{bpg} compression algorithm, they are re-scaled into the range $[0, 255]$. This results in the final processed residual $\br'$, as shown in \fref{fig: SPIC cspic residual pipeline}(c).
\end{enumerate}

The final processed residual $\br'$ can now be compressed in a lossy manner using the \gls{bpg} algorithm and sent to the receiver.

\begin{figure}[!t]
    \centering
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Figures/SPIC/improved/Residual_masked.png}
        \caption*{(a) Residual $\br_m = \br \odot \m$ after masking}
    \end{subfigure}%
    \begin{subfigure}[b]{0.24\textwidth}
        \centering
        \raisebox{-\height}{\includegraphics[height=\heightof{\includegraphics[width=2.04\textwidth]{Figures/SPIC/improved/Residual_masked.png}}]{Figures/SPIC/improved/Residual_masked_compacted.png}}
        \caption*{(b) Compacted $g(\br_m)$}
    \end{subfigure}
    \begin{subfigure}[b]{0.24\textwidth}
        \centering
        \raisebox{-\height}{\includegraphics[height=\heightof{\includegraphics[width=2.04\textwidth]{Figures/SPIC/improved/Residual_masked.png}}]{Figures/SPIC/improved/Residual_masked_compacted_8bitcolour.png}}
        \caption*{(c) Final $\br'$}
    \end{subfigure}%

    \caption[Residual processing pipeline in the \acrshort{cspic}]{Processing pipeline of the residual $\br$ in the \acrshort{cspic}: (a) the residual isis initially masked using the binary mask $\m$, (b) the function $g(\cdot)$ is applied to compact the relevant regions and reduce empty spaces, and (c) the pixel values are re-scaled to an 8-bit representation.}
    \label{fig: SPIC cspic residual pipeline}
\end{figure}

\subsubsection{Receiver Side}\label{sec: SPIC cspic_with receiver}

At the receiver, the \gls{ssm} $\s$ and coarse image $\co$ are processed as in the \gls{spic} framework: they are decoded and used as conditioning inputs to the \gls{semcore} model to produce the same intermediate reconstructed image $\tilde{\x}$ as at the transmitter.

The processing of the residual involves reversing the transformations applied at the transmitter:

\begin{enumerate}
    \item \textbf{Reverse Color Adjusting:} The received and decoded residual $\tilde{\br}'$ is first processed to adjust the values of the pixels back into the original range of $[-255, 255]$.

    \item \textbf{De-compacting:} Since the \gls{ssm} $\s$ is transmitted losslessly the compacting algorithm is reversed and the original shape of the residual is restored. This operation is referred to as $g^{-1}$ and described in \aref{app: SPIC compacting}.

    \item \textbf{Masking:} The residual is then multiplied element-wise with the mask $\m$ to ensure that only the relevant semantic classes are considered.
\end{enumerate}

Finally, the masked reconstructed residual is added back to the intermediate reconstructed image $\tilde{\x}$ to obtain the final reconstructed image $\hat{\x}$. In this way $\hat{\x}$ will contain enhanced details for the specific semantic classes considered as relevant. The quality of these details will be influenced by the compression level at which the residual has been encoded.

\subsection{C-SPIC without Residual (C-SPIC$^{w/o}$)}\label{sec: SPIC image dsslic}

In scenarios where the transmitter cannot perform the computationally intensive task of evaluating the \gls{semcore} model, an alternative approach is necessary.

To address this, the \gls{cspic}$^{w/o}$ is introduced as a variant of the \gls{cspic}$^w$ and the scheme is depicted in \fref{fig: SPIC cspic without scheme}. This new implementation significantly reduces the computational requirements at the transmitter side while preserving similar reconstruction quality. The draw back is the increase of up to 10\% of the required \gls{bpp}, this may or may not represent a problem depending on the communication infrastructure.

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{Figures/SPIC/schema_spic_residuo.pdf}
    \caption[\acrshort{cspic}$^{w/o}$ architecture scheme]{Overview of the \acrshort{cspic}$^{w/o}$ architecture. At the transmitter after the generation and compression of $\s$ and  $\co$ the original image is directly used to extract the information about relevant classes. After masking and compacting, the transformation $\h'$ is send to the receiver. At the receiver the intermediate $\tilde{\x}$ is evaluated starting from $\s$ and $\co$ by using the \acrshort{semcore} model and then the semantically relevant parts are replaced with the original extracted from the received $\tilde{\h}'$.} 
    %[C-SPIC scheme without residual]{Overview of the proposed C-SPIC architecture without residual.}
    \label{fig: SPIC cspic without scheme}
\end{figure}

\subsubsection{Transmitter Side}\label{sec: SPIC cspic_no transmitter}

In this variant, the transmitter's structure remains similar to that discussed in \sref{sec: SPIC cspic_with transmitter}, with one crucial difference: the residual is not computed but instead the original image $\x$ is used directly to extract the relevant details as follows:

\begin{enumerate}
    \item \textbf{Masking:} The original image $\x$ is multiplied element-wise with the binary mask $\m$ obtained from the \gls{ssm} $\s$. This results in an image where only the relevant semantic classes are preserved, and the rest of the pixels are zeroed out.

    \item \textbf{Compacting:} The function $g(\cdot)$ is applied to the masked image to compact the relevant regions.

    \item \textbf{Color Adjusting (Optional):} To reduce the \gls{bpp} a color correction step can be applied. This involves modifying the RGB values of the pixels that have been set to zero  to match the mean values of the relevant pixels. This can slightly improve the compression efficiency.
\end{enumerate}

The resulting compacted image $\h'$ is then compressed using a lossy compression algorithm like \gls{bpg} and transmitted to the receiver.

While this approach avoids the need for the transmitter to compute $\tilde{\x}$ using the \gls{semcore} model, the amount of \gls{bpp} required to compress $\h'$ are higher compared to $\br'$. This is a trade-off that has to be considered when choosing between \gls{cspic}$^w$ and \gls{cspic}$^{w/o}$.

\subsubsection{Receiver Side}

The receiver processes the \gls{ssm} $\s$ and coarse image $\co$ as before, using them to generate the intermediate reconstructed image $\tilde{\x}$ via the \gls{semcore} model.

The received compacted image $\h'$ is decoded and then processed as follows:

\begin{enumerate}
    \item \textbf{De-compacting:} The inverse function $g^{-1}(\cdot)$ is applied to restore the compacted image to its original dimensions, using the \gls{ssm}.

    \item \textbf{Masking:} The de-compacted masked image is multiplied element-wise with the mask $\m$ to ensure that only the relevant semantic classes are considered and $\h_m$ is obtained.
\end{enumerate}

Finally, the relevant regions from the de-compacted image are substituted into the intermediate reconstructed image $\tilde{\x}$ to obtain the final reconstructed image $\hat{\x}$:

\[\hat{\x} = \tilde{\x} \;\circv\; \h_m\]

Here, the symbol $\circv$ denotes the substitution of pixel values in $\tilde{\x}$ with those in $\h_m$ when the value of the mask $\m$ is 1.\\


\section{Results}\label{sec: SPIC results}

This section provides a comprehensive analysis of the performance of the proposed frameworks, \gls{spic} and \gls{cspic}. The evaluation is conducted on the Cityscapes dataset \cite{Cordts2016Cityscapes}, although the frameworks are applicable to any dataset from which a \gls{ssm} with fewer than 255 semantic classes can be extracted.

The Cityscapes dataset contains 5000 pairs of images recorded in 50 German cities from the interior of a moving vehicle. Each pair consists of an RGB image $\x$ of the street scene and the associated \gls{ssm} $\s$. The images have a resolution of $1024\times2048$ pixels and are divided into 2975 pairs for training, 500 for validation, and 1525 for testing. Both the images and \glspl{ssm} are considered RGB images.

The \gls{ssm} in the Cityscapes dataset comprises multiple classes. In most applications, including this one, only $C_\s=19+1$ classes are used. The first 19 classes refer to actual semantic categories like "car", "person", etc., while the last one is the "null class", representing all objects that do not belong to any other class (e.g. the silhouette of the recording vehicle). Before being input to the network, the images and \gls{ssm}s are down-scaled to $256\times512$ pixels. This re-scaling is performed by averaging the RGB values in the same patch for the image while considering the  nearest-neighbours interpolation for the \gls{ssm}.

The true \gls{ssm} provided with the dataset is used only during training. At inference time, the \gls{ssm} is generated by an out-of-the-shelf \gls{sota} \gls{ssmodel}. The INTERN-2.5 model \cite{Wang2022internimage} is chosen for its remarkable ability to extract \glspl{ssm} from images, though other options can be adopted without affecting the results, thanks to the modularity of the approach.

An important metric used in this section is the \gls{bpp}, evaluated in this context as follows:

\begin{equation} \text{BPP} = \frac{B_{\x} + B_{\s} + B_{\br'}}{256\times512}, \end{equation}

where $B_{\x}$ is the number of bits used to lossy encode the image, $B_{\s}$ is the number of bits used to losslessly encode the \gls{ssm}, $B_{\br'}$ is the number of bits used to lossy encode the residual and $256\times512$ is the total number of pixels in the original image. The first two terms are always present in both \gls{spic} and \gls{cspic}. The term associated to the residual is instead present only in the \gls{cspic} framework and can refer to either the compacted masked residual $\br'$ or the compacted masked image $\h'$.\\

This section is divided into two parts. In \sref{sec: SPIC results spic}, the performance of \gls{spic} is compared to classical compression algorithms like \gls{bpg} and \gls{jpeg2000}. In \sref{sec: SPIC results c-spic}, the results for \gls{cspic} are discussed, highlighting the visual improvement in reconstructing semantically relevant classes.

\subsection{SPIC}\label{sec: SPIC results spic}
\begin{figure}[t]
    \centering
    % Titles
    \begin{subfigure}{\textwidth}
        \centering
        \begin{minipage}{0.32\textwidth}
            \centering
            \textbf{BPG} ($0.176$ BPP)
        \end{minipage}
        \hfill
        \begin{minipage}{0.32\textwidth}
            \centering
            \textbf{Original}
        \end{minipage}
        \hfill
        \begin{minipage}{0.32\textwidth}
            \centering
            \textbf{SPIC} ($0.166$ BPP)
        \end{minipage}
    \end{subfigure}
    
    % Top row of images
    \begin{subfigure}{\textwidth}
        \centering
        \begin{tikzpicture}
            \node[anchor=south west,inner sep=0] (frame1) at (0,0) {\includegraphics[width=.32\textwidth]{Figures/SPIC/Classico/bpg40.png}};
            \node[anchor=south west,inner sep=0] (frame2) at (frame1.south east) {\includegraphics[width=.32\textwidth]{Figures/SPIC/Classico/original_512.png}};
            \node[anchor=south west,inner sep=0] (frame3) at (frame2.south east) {\includegraphics[width=.32\textwidth]{Figures/SPIC/Classico/our_bpg35.png}};
            
            % Add \gls{ssm} overlays
            \begin{scope}[x={(frame1.south east)},y={(frame1.north west)}]
                \node[anchor=south east,inner sep=0] at (1,0) {\includegraphics[width=.13\textwidth]{Figures/SPIC/Classico/bpg40_SSM.png}};
            \end{scope}
            \begin{scope}[x={(frame2.south east)},y={(frame2.north west)}]
                \node[anchor=south east,inner sep=0] at (1,0) {\includegraphics[width=.13\textwidth]{Figures/SPIC/Classico/original_512_SSM.png}};
            \end{scope}
            \begin{scope}[x={(frame3.south east)},y={(frame3.north west)}]
                \node[anchor=south east,inner sep=0] at (1,0) {\includegraphics[width=.13\textwidth]{Figures/SPIC/Classico/our_bpg35_SSM.png}};
            \end{scope}
        \end{tikzpicture}
    \end{subfigure}
    
    % Bottom row of images with trimming
    \begin{subfigure}{\textwidth}
        \centering
        \begin{tikzpicture}
            \node[anchor=south west,inner sep=0] (frame4) at (0,0) {\includegraphics[width=.32\textwidth, trim=190 100 90 40, clip]{Figures/SPIC/Classico/bpg40.png}};
            \node[anchor=south west,inner sep=0] (frame5) at (frame4.south east) {\includegraphics[width=.32\textwidth, trim=190 100 90 40, clip]{Figures/SPIC/Classico/original_512.png}};
            \node[anchor=south west,inner sep=0] (frame6) at (frame5.south east) {\includegraphics[width=.32\textwidth, trim=190 100 90 40, clip]{Figures/SPIC/Classico/our_bpg35.png}};
            
            % Add \gls{ssm} overlays with different trimming
            \begin{scope}[x={(frame4.south east)},y={(frame4.north west)}]
                \node[anchor=south east,inner sep=0] at (1,0) {\includegraphics[width=.13\textwidth, trim=190 100 90 40, clip]{Figures/SPIC/Classico/bpg40_SSM.png}};
            \end{scope}
            \begin{scope}[x={(frame5.south east)},y={(frame5.north west)}]
                \node[anchor=south east,inner sep=0] at (1,0) {\includegraphics[width=.13\textwidth, trim=760 400 360 160, clip]{Figures/SPIC/Classico/original_512_SSM.png}};
            \end{scope}
            \begin{scope}[x={(frame6.south east)},y={(frame6.north west)}]
                \node[anchor=south east,inner sep=0] at (1,0) {\includegraphics[width=.13\textwidth, trim=190 100 90 40, clip]{Figures/SPIC/Classico/our_bpg35_SSM.png}};
            \end{scope}
        \end{tikzpicture}
    \end{subfigure}
    \caption[Visual comparison between \acrshort{bpg} and \acrshort{spic}]{Visual comparison between the original image and \acrshort{ssm} (CENTER), the image compressed with \acrshort{bpg} at $0.176$ \acrshort{bpp} with the associated generated \acrshort{ssm} (LEFT), and the image obtained with the proposed \acrshort{spic} framework at $0.166$ \acrshort{bpp} and the associated generated \acrshort{ssm} (RIGHT). At similar values of \acrshort{bpp} the proposed approach preserves semantically relevant details better than \acrshort{bpg}. The person riding the bicycle is clearly detected by the generated \acrshort{ssm}.}
    \label{fig: SPIC Visual_comparison_OUR_vs_BPG}
\end{figure}

An advantage of the proposed model is its capability to retain semantic information while providing a good trade-off between overall image quality and compression rate. Several existing compression algorithms and \gls{sr} models often reconstruct visually pleasing images. However, a closer inspection reveals a significant drawback: the degradation of semantic content. This effect is amplified and particularly evident as the size of semantically relevant objects within the image diminishes. For larger foreground objects the level of detail and \gls{ssm} retention is good enough. However, as the object size shrinks, conventional models falter, failing to accurately process the image and preserve details.

This aspect can be observed by examining \fref{fig: SPIC Visual_comparison_OUR_vs_BPG} where the proposed \gls{spic} is compared to the classical \gls{bpg} algorithm at similar \gls{bpp} values.

The upper row shows the reconstructed images $\hat{\x}$, while the lower row displays the associated \glspl{ssm}. For the original image is reported the original \gls{ssm}, while for the other two columns, the \glspl{ssm} shown are the ones generated form $\hat{\x}$ via the INTERN-2.5 semantic segmentation model.

As can be seen, the overall visual quality of the images is very similar. In some details, such as the shape of the windows, the \gls{bpg} algorithm performs even better. However, when attention is focused on the generated \gls{ssm}, the results are completely different. The image reconstructed using \gls{bpg} fails to retain detailed information about small objects. An example is the person on the bicycle, shown in detail in the bottom row. The \gls{bpg} algorithm fails to correctly compress and reconstruct the rider in $\hat{\x}$, causing the \gls{ssmodel} to not detect the rider. However, in the case of the \gls{spic} framework, by conditioning the reconstruction of the image on the original \gls{ssm}, the preservation of the semantic information is guaranteed. This can be visually appreciated in the generated \gls{ssm}.
\begin{figure}[!t]
    \centering
    % Titles Row
    \begin{subfigure}{\textwidth}
        \centering
        \begin{minipage}{0.333\textwidth}
            \centering
            \textbf{SOTA SR model} \cite{Rombach2022SR_CVPR}
        \end{minipage}%
        \begin{minipage}{0.333\textwidth}
            \centering
            \textbf{SPIC}
        \end{minipage}%
        \begin{minipage}{0.333\textwidth}
            \centering
            \textbf{\gls{ssm}-based model} \cite{Wang2022SISDM}
        \end{minipage}
    \end{subfigure}
    
    % Images Row
    \begin{subfigure}{\textwidth}
        \centering
        \begin{subfigure}{0.333\textwidth}
            \centering
            \includegraphics[width=\linewidth, trim=200 110 180 70, clip]{Figures/SPIC/SR/SOTA_comparison_SR.png}
            \caption*{}
        \end{subfigure}%
        \begin{subfigure}{0.333\textwidth}
            \centering
            \includegraphics[width=\linewidth, trim=200 110 180 70, clip]{Figures/SPIC/SR/SOTA_comparison_OUR.png}
            \caption*{}
        \end{subfigure}%
        \begin{subfigure}{0.333\textwidth}
            \centering
            \includegraphics[width=\linewidth, trim=200 110 180 70, clip]{Figures/SPIC/SR/SOTA_comparison_SISDM.png}
            \caption*{}
        \end{subfigure}
    \end{subfigure}
    \vspace{-1cm}
    \caption[Visual comparison with Super-Resolution and SSM conditioned models]{Visual comparison between: the \acrshort{sota} \acrshort{sr} model \cite{Rombach2022SR_CVPR} (LEFT), the proposed \acrshort{spic} framework (CENTER), and the \acrshort{ssm} conditioned model \cite{Wang2022SISDM} (RIGHT).}
    \label{fig: SPIC super_res_comparison}
\end{figure}

Not only do classical compression approaches present these problems, but issues also arise when analyzing \gls{sr} models. All the \gls{sr} models work under a very strict condition: the input coarse image $\co$ must already contain sufficient details. To better explain this concept, \fref{fig: SPIC super_res_comparison} illustrates a comparison between the reconstruction capabilities of the \gls{sota} \gls{sr} model proposed in \cite{Rombach2022SR_CVPR} (left) and the proposed \gls{spic} framework (middle).

The main drawback of \gls{sr} models is that they are designed to enhance the quality of images that contain minor defects. Nonetheless, when the starting image lacks relevant details, the model is unable to infer what the output image should look like. The result is that the \gls{sota} \gls{sr} model fails to reconstruct the three pedestrians between the two cars. This failure is due to the very low-resolution input image, which does not contain enough details. Removing objects is not the only problem. The model begins adding pedestrians near the sidewalk or on the street when detecting another object that cannot correctly identify. This happens due to the low-resolution coarse image, where objects vaguely resembling pedestrians are wrongly considered as belonging to that class.

Conditioning only on the low-resolution image does not yield good results in terms of object reconstruction. The other extreme is conditioning solely on the \gls{ssm}. In \fref{fig: SPIC super_res_comparison}, the result of this conditioning is shown in the right image. This image has been reconstructed by conditioning only on the \gls{ssm} using the model proposed in \cite{Wang2022SISDM}. In this case, the pedestrians are placed in the correct positions, as are all the other elements in the scene. Still, the drawback is that the resulting image is completely different from the original: the colors and traffic signs have changed. This is because this family of models is agnostic to any other semantic information beside the \gls{ssm}.

In this scenario, \gls{spic} manages to balance all these factors. With this framework, it is possible to transmit the original \gls{ssm} losslessly to the receiver and at the same time reconstruct an overall good-looking image able to preserve all the most important semantic details, thanks to the conditioning on the \gls{ssm} itself.
\begin{figure}[!t]
    \centering
    \begin{subfigure}[t]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth, trim=5 10 40 20, clip]{Figures/SPIC/Classico/miou_vs_bpp_combined.pdf}
        \caption*{(a)} % Caption under image without adding to list
    \end{subfigure}%
    \begin{subfigure}[t]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth, trim=5 10 40 20, clip]{Figures/SPIC/Classico/fid_vs_bpp_combined.pdf}
        \caption*{(b)}
    \end{subfigure}
    \caption[Performance comparison between the \acrshort{spic} and classical compression algorithms]{Performance comparison between \acrshort{bpg}, \acrshort{jpeg2000} and \acrshort{spic} evaluated in terms of \acrshort{miou} and \acrshort{fid}.}
    \label{fig: SPIC spic metrics comparison}
\end{figure}

The performances of the \gls{spic} framework are shown in \fref{fig: SPIC spic metrics comparison}.\footnote{All the performances are evaluated by averaging the results on 500 images.} On the left is shown the plot associated with the concept of \gls{ssm} retention. This quantity is evaluated by the \gls{miou} between the original \gls{ssm} and the one generated from $\hat{\x}$ via the INTERN-2.5 \gls{ssmodel}.

The black dotted vertical line, positioned at $0.112$ \gls{bpp}, represents the \gls{bpp} required for the lossless compression of the \gls{ssm}. The blue point represents the \gls{ssm} retention of the proposed \gls{spic} framework, evaluated at a \gls{bpp} given by the sum of the \gls{bpp} necessary for the lossless encoding of the \gls{ssm} and the lossy encoding of the coarse image. The green and magenta curves represent the performances achieved with \gls{bpg} and \gls{jpeg2000} compression methods.

The red dotted horizontal line represents the upper bound achievable with the INTERN-2.5 \gls{ssmodel}. This means that this model is not able to generate \glspl{ssm} that perform better than an \gls{miou} of $0.84$.

The results are clear: the \gls{spic} framework outperforms the other methods. The \gls{bpg} can achieve \gls{miou} performances close to what the \gls{spic} can achieve at $0.17$\gls{bpp} only when the compression is in the order of $1$ \gls{bpp}.

The other comparison, on the right, is on the \gls{fid} metric. Even in this case, the proposed \gls{spic} is able to outperform the other two methods.

Overall, the reconstructed image is visually more appealing and can better preserve the \gls{ssm}, as visually shown in \fref{fig: SPIC Visual_comparison_OUR_vs_BPG}.

\subsection{C-SPIC}\label{sec: SPIC results c-spic}

While the proposed \gls{spic} is able to outperform classical compression algorithms both visually and on different semantic metrics, it can still be improved by introducing the \gls{cspic} framework.

In \fref{fig: SPIC detail traffic sign}, a detail of a "Parking" traffic sign is shown extracted from reconstructed images at similar \gls{bpp} values. The image on the right is the result of the classical \gls{bpg} compression algorithm at $0.176$ \gls{bpp}. It is possible to see that the traffic sign is blue and vaguely resembles a "P", although it is not very clear. In the middle, at $0.166$ \gls{bpp}, is the reconstruction obtained with the \gls{spic} framework. The traffic sign is blue, but the "P" has been completely removed.

Finally, the left image, obtained with the proposed \gls{cspic}$^w$ at $0.171$ \gls{bpp}, is clearly superior in terms of preservation of meaning. The sign is blue, and the "P" is clearly visible. By using only $0.004$ more \gls{bpp} than the \gls{spic} framework, the traffic sign is now perfectly reconstructed.
\footnote{The exact same visual result obtained with the \gls{cspic}$^w$ at $0.171$ \gls{bpp} can be achieved with the \gls{cspic}$^{w/o}$ at $0.173$ \gls{bpp}. Due to the small difference in terms of metrics between the two approaches, the comparison are evaluated only between the \gls{cspic}$^w$ and classical compression algorithms. The choice of \gls{cspic}$^{w/o}$ over \gls{cspic}$^w$ is only a matter of computational constraints at the transmitter and if the 10\% increase in \gls{bpp} can really make the difference.}
\begin{figure}[!t]
    \centering
    % Titles Row
    \begin{subfigure}{\textwidth}
        \centering
        \begin{minipage}{0.333\textwidth}
            \centering
            \textbf{BPG} (0.176 BPP)
        \end{minipage}%
        \begin{minipage}{0.333\textwidth}
            \centering
            \textbf{SPIC} (0.166 BPP)
        \end{minipage}%
        \begin{minipage}{0.333\textwidth}
            \centering
            \textbf{C-SPIC$^w$} (0.171 BPP)
        \end{minipage}
    \end{subfigure}
    
    % Images Row
    \begin{subfigure}{\textwidth}
        \centering
        \begin{subfigure}{0.333\textwidth}
            \centering
            \includegraphics[width=\linewidth, trim=330 140 50 40, clip]{Figures/SPIC/Classico/bpg40.png}
            \caption*{}
        \end{subfigure}%
        \begin{subfigure}{0.333\textwidth}
            \centering
            \includegraphics[width=\linewidth, trim=330 140 50 40, clip]{Figures/SPIC/Classico/our_bpg35.png}
            \caption*{}
        \end{subfigure}%
        \begin{subfigure}{0.333\textwidth}
            \centering
            \includegraphics[width=\linewidth, trim=330 140 50 40, clip]{Figures/SPIC/improved/our_bpg35_trafficsignsres30.png}
            \caption*{}
        \end{subfigure}
    \end{subfigure}
    \vspace{-1cm}
    \caption[Visual comparison between \acrshort{bpg}, \acrshort{spic} and \acrshort{cspic}$^w$ on the "traffic sign" class]{Visual comparison of the traffic sign "P"  compressed with the \acrshort{bpg} algorithm at 0.176 \gls{bpp} (LEFT), the proposed \acrshort{spic} framework at 0.166 \acrshort{bpp}(CENTER), and the \gls{cspic}$^w$ at 0.171 \acrshort{bpp} (RIGHT).}
    %[Visual comparison for improved SPIC]{Visual comparison of the parking traffic sign detail between the image compressed with the \gls{bpg} algorithm at 0.176 \gls{bpp} (LEFT), the proposed \gls{spic} framework at 0.166 \gls{bpp}(CENTER), and the \gls{cspic}$^w$ with enhanced traffic signs at 0.171 \gls{bpp} (RIGHT).}
    \label{fig: SPIC detail traffic sign}
\end{figure}
\begin{figure}[!t]
    \centering
    \begin{subfigure}[t]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth, trim=5 10 40 20, clip]{Figures/SPIC/improved/psnr_vs_bpp_combined.pdf}
        \caption*{(a)}
    \end{subfigure}%
    \begin{subfigure}[t]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth, trim=5 10 40 20, clip]{Figures/SPIC/improved/accuracy_vs_bpp_combined.pdf}
        \caption*{(b)}
    \end{subfigure}

    \caption[Performance comparison between the \acrshort{cspic}$^w$ and classical compression algorithms]{Performance comparison between \acrshort{bpg}, \acrshort{jpeg2000} and \acrshort{cspic}$^w$ on the "traffic sign" class evaluated in term of masked \acrshort{psnr} and traffic signs classification accuracy.}
    \label{fig: SPIC cspic metrics comparison}
\end{figure}

In \fref{fig: SPIC cspic metrics comparison}, some numerical comparisons are presented. The color scheme is similar to that discussed in the previous section, with the red dotted horizontal line representing the upper bound for accuracy in traffic sign classification, as described in \sref{sec: GM evaluation metrics}. The black dotted vertical line again represents the amount of \gls{bpp} required to losslessly encode the \gls{ssm} and is placed at $0.112$ \gls{bpp}.

The performances of the proposed \gls{cspic}$^w$ are evaluated multiple times and reported with three different shades of blue. The difference between the three versions lies in the level of compression of the coarse image $\co$. In the \gls{cspic}$^w$, both the coarse image and the residual can be compressed at different levels. Since the effects of varying the quality of the coarse image were presented in \fref{fig: SPIC spic metrics comparison}, in this case, the quality of the coarse is fixed to three different values, and the quality of the residual is varied. The coarse image is compressed at these values of \gls{bpp}: $0.007$, $0.054$, and $0.183$. In the plot, the different lines are identified by different subscript, e.g. \gls{cspic}$_{0.007}^w$ indicates that the coarse has been compressed at $0.007$ \gls{bpp}.

The results show that, as soon as the artifacts of the lossy compression of the residual decreases, the proposed method outperforms the comparison approaches both on a classical metric like \gls{psnr}\footnote{The \gls{psnr} is evaluated only on the portion of the image containing the traffic signs.} and on the traffic sign classification accuracy.

These results are even more interesting when considering that the overall image still preserves the same qualities described in the \gls{spic} framework.\\


In conclusion, the \gls{spic} framework, along with its enhanced variant \gls{cspic}, represents a significant advancement in the field of \gls{sc} for image compression. By employing a modular architecture that distinctly handles the \gls{ssm}, the coarse image and the reconstruction, the \gls{spic} framework effectively balances compression efficiency with the preservation of essential semantic information. The introduction of the \gls{semcore} model at the receiver side enables high-fidelity image reconstruction, ensuring that semantically relevant details are maintained even at low \gls{bpp} levels.

The \gls{cspic} extension further refines this approach by incorporating a residual enhancement mechanism tailored for specific semantic classes. This enhancement ensures that smaller and more detailed objects, which are crucial for accurate semantic interpretation, are reconstructed with higher fidelity without imposing a significant additional \gls{bpp} overhead. Experimental results on the Cityscapes dataset clearly demonstrate that both \gls{spic} and \gls{cspic} outperform classical compression algorithms such as \gls{bpg} and \gls{jpeg2000} in terms of \gls{miou} and \gls{fid}, underscoring their superior capability in preserving semantic content.

These findings establish a robust foundation for further exploration and optimization in gls{sc}-based image compression algorithms and will be considered in the next chapter.
