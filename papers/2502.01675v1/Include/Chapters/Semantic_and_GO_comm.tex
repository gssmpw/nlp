\chapter{\textcolor{black}{Foundations of Semantic and Goal-Oriented Communication}}
\label{ch: SEMCOM}
\thispagestyle{plain}
The content of this chapter is based on the current state of the art and on the contributions at the core of \sref{sec: SEMCOM sem_gen}, \sref{sec: SEMCOM sem_go} and the implementation of the \gls{ib} principle in the context of \gls{goc}. These contributions are based on the following publications:
\begin{quotation}
\noindent \textit{\textbf{\large Goal-Oriented Communication for Edge Learning based on the Information Bottleneck}}\\
\textit{Francesco Pezone, Sergio Barbarossa, Paolo Di Lorenzo}

\vspace{0.1cm}
\noindent \textit{\textbf{\large Semantic and Goal-Oriented Communications}}\\
\textit{Sergio Barbarossa, Francesco Pezone}

\vspace{0.1cm}

\noindent \textit{\textbf{\large Semantic Communications based on Adaptive Generative Models and Information Bottleneck}}\\
\textit{Sergio Barbarossa, Danilo Comminiello, Eleonora Grassucci, Francesco Pezone, Stefania Sardellitti, Paolo Di Lorenzo}
\end{quotation}

 

\section{Introduction}
In 1983, during an interview, the famous physicist Richard Feynman was asked why two magnets repel each other. Faced with this apparently simple question, one of the greatest physicists who ever lived took the opportunity to illustrate an important lesson: answering a "why" question is not easy at all! One of the first assumptions is that the questioner and the respondent share some common knowledge. It is useless if the questioner is a 5-year-old and the answer involves concepts of quantum mechanics. Another assumption is that there must be a point at which the question is considered answered. It is always possible to respond with another "why" question; it's the classic game that kids love to playâ€”keep asking "why?". For this reason, Richard Feynman ultimately told the interviewer, \textit{"I'm not going to be able to give you an answer to why magnets attract or repel, except to tell you that they do."}\\

This anecdote is a simple example of how communication can sometimes be difficult. Two interlocutors can be in the same room and talk for hours, but if what they say is not understood by the other, or if it does not satisfy the other's curiosity, then the communication is not effective.\\
These concepts are not new in the field of \gls{it}.  It was in 1953 when Weaver suggested that the broad subject of communication can be divided into three main levels \cite{WARREN1953semantic}:
\begin{itemize}[{label={--}}]
    \item \textbf{Syntactic level}: \textit{How accurately can the symbols of communication be transmitted? (The technical problem.)}
    \item \textbf{Semantic level}: \textit{How precisely do the transmitted symbols convey the desired meaning? (The semantic problem.)}
    \item \textbf{Effectiveness level}: \textit{How effectively does the received meaning affect conduct in the desired way? (The effectiveness problem.)}
\end{itemize}

\section{Syntactic Level}
The syntactic level is one of the most studied in the field of communication. It refers to the technical problem of how to transmit symbols to guarantee a correct reconstruction at the receiver end. This level is based on the work proposed by Shannon in 1948 \cite{Shannon1948Communication}. At the time, there was a lack of a mathematical framework to understand and optimize the communication process. Telephone networks and radio transmissions were becoming more popular, and engineers were struggling with issues related to signal noise, bandwidth, and the capacity of communication channels. Shannon proposed a mathematical model to describe the communication process, and his contribution completely changed the way communication is approached.

The theory proposed by Shannon is still the basis of many modern communication systems. Building on his work, researchers have developed a vast number of communication strategies, such as sophisticated forms of error detection and correction \cite{Mercier2010errorcorrection}, multiple-input multiple-output (MIMO) communications \cite{Jensen2016MIMO}, mitigation of multi-user interference \cite{Yang2022interference}, etc.

At the same time, new network and communication infrastructures are being developed at an incredible pace. Technologies like 4G and 5G are now part of the daily life of billions of people, and 6G is on the horizon \cite{Saad2019Vision6G}. Unfortunately, the rate of improvement of physical devices and communication infrastructures is subject to physical limitations. All the players in telecommunications spend billions of dollars every year to access finite resources like bandwidth. The management of these resources is a complex task, and the optimization of the communication process is a never-ending challenge. For this reason, the syntactic level alone might not be sufficient anymore.

The subtle problem is that, from theory, it is known that even with the best possible compression algorithm, there is a limit to the number of bits at which a piece of data can be compressed. This limit depends on its entropy, also referred to as Shannon entropy. It is not possible to perform better than this. This means that if the idea is to reconstruct the exact sequence of symbols, the best that can be achieved in terms of compression is given by its entropy. However, if the idea is to convey the meaning of a piece of data regardless of the form, this can be potentially achieved at values lower than the entropy of the original data. This is the idea behind the semantic level of communication.

\section{Semantic Communication (Semantic Level)}\label{sec: SEMCOM sem}
On the semantic level, the way the message is reconstructed is not relevant as long as the \textit{semantic information} is preserved. The term semantic information refers to the information that is conveyed by the data and is relevant to the receiver, allowing the receiver to understand the message without reconstructing it symbol by symbol.

Multiple works have proposed formal theories concerning \gls{sc} \cite{Bao2011SemEntropy, Gunduz2024SemTheory, Carnap1954SemCommTheory}. This thesis will present \gls{sc} a more intuitive and high level way. It is in fact possible to consider any given piece of data $\x$ as composed of two parts: 
\begin{itemize}[{label={}}]
    \item \textbf{Syntactic component} $V$: This quantity refers to the subset $V=\{v_i\}$ of the symbols $v_i \in \mathcal{V}$ used to represent the data $\x$ in its original domain. Here, $\mathcal{V}$ denotes the alphabet of the symbols.
    \item \textbf{Semantic component} $S$: This quantity refers to the subset $S=\{s_i\}$ of the semantic information/meanings $s_i \in \mathcal{S}$ selected from the semantic alphabet $\mathcal{S}$ and associated with the data $\x$. 
\end{itemize}
Only by having access to both components is it possible to fully describe the data $\x$. In fact, without the semantic component $S$, the data $\x$ is just a sequence of symbols $V$ that can be processed only at a symbolic level. Without the syntactic component $V$, the data $\x$ itself will not exist. However, to fully describe the data $\x$ in a semantic way, the semantic component $S$ is the most important part. Once $S$ is defined, it is possible to associate it with multiple symbolic components $V$.

To clarify the concept, it is possible to consider $\x$ as an image of an urban environment. The symbolic component $V$ will be composed of the sequence of RGB pixel values $v_i$ of the image. The semantic component $S$ will instead be associated with the more abstract semantic information $s_i$ contained in the image. Some examples might be accessing if there is a pedestrian crossing the street, if a car is driving too close, or if the right window on the fifth floor is open or not.

From $S$, it is possible to derive multiple symbolic components. For example if $S$ represents only the semantic meaning \textit{"the traffic light is red"}, there are countless possible configurations $V$ of the RGB values $v_i$ that represent an image with the same semantic meaning.

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{Figures/Semantic_communication/Sem_comm_scheme.png}
    \caption[Semantic Communication Scheme]{Semantic Communication scheme.}
    \label{fig: SEMCOM semantic_comm_scheme}
\end{figure}

In fact, in the context of \gls{sc}, only the semantic information matters. Two pieces of data $\x=(V,S)$ and $\hat{\x}=(U,T)$ are defined as \textit{semantically equivalent}, represented with $\x \longleftrightarrow \hat{\x}$, if and only if $S=T$. This means that as long as the semantic components are the same, the data can be considered the same on a semantic level. No assumption is made about the syntactic components $V$ and $U$, which might be completely different from one another.

This idea is shown in \fref{fig: SEMCOM semantic_comm_scheme}, where the data $\x=(V,S)$ is compressed and transmitted to the receiver. The syntactic component $V$ is transformed via a Semantic-Based Source Encoder to produce a representation defined as $\z=\Phi(V|S,\theta)$. This representation is composed of semantic symbols $z_i\in \mathcal{Z}$ selected from an alphabet $\mathcal{Z}$ to convey the semantic information $s_i$. Each $z_i$ is selected on the basis of $v_i$, the semantic information $s_i$, and the \gls{kb} $\theta$.

The \gls{kb} $\theta$ refers to a set of facts, rules, constraints, etc., specific to the semantic context at hand. It is used to connect the syntactic symbols $v_i$ to the semantic symbols $z_i$ associated with the semantic information $s_i$ and vice versa. This is a fundamental part of the \gls{sc} framework since only if the \gls{kb} is shared between the transmitter and the receiver can \gls{sc} happen. Ideally, $\theta$ and $\theta'$ should be the same at both ends, even if they could potentially differ within some margins.

The semantic symbols $\z$ can now be forwarded to the syntactic level. In fact, \gls{sc} relies on the syntactic communication to encode, transmit, and decode every semantic symbol $z_i$. Without the syntactic level and the use of conventional compression techniques, \gls{sc} would not be possible.

At the receiver end, the received transformation $\hat{\z}$ can be used to reconstruct a semantically equivalent representation of the data $\x$. The reconstruction is influenced by the \gls{kb} $\theta'$ and is performed by a specialized semantic decoder (represented by a generative model in \fref{fig: SEMCOM semantic_comm_scheme}) to reconstruct $\hat{\x}=(U,S)$.

To clarify this concept, consider again the example of the image of the street environment. Assume that the semantic component $S$ is composed only of the semantic information $s_1$ that the traffic light is red. Also, consider that the \gls{kb} $\theta$ knows that the data refers to images of a street environment, and the \gls{kb} $\theta'$ knows that the images refer to a street environment on a cloudy day. In this case, the Semantic-Based Source Encoder will process all the RGB pixels $v_i$, and the output transformation might be represented as a single sentence $\z=$ \textit{"There is a red traffic light"} composed of the semantic symbols, characters, $z_i$ associated with $s_1$. This string is encoded, transmitted, and at the receiver decoded by the syntactic level to obtain the string $\hat{\z}=$ \textit{"There ys a red trafgic licht."} The transmission introduced some errors that the syntactic level was not able to remove.

The string $\hat{\z}$ can now be used as input to a text-to-image generative model. This model will process $\hat{\z}$, and the \gls{kb} $\theta'$ will be used to give more context about the fact that the images are from a street environment on a cloudy day. By exploiting this additional context, the model will be able to correct the syntactic errors in $\hat{\z}$. The output of the model will be an image $\hat{\x}=(U,S)$ that is coherent with the semantic information contained in $S$.

Of course, the syntactic component $U$ of the image $\hat{\x}$ will not be the same as the $V$ of the original image $\x$, since there are countless combinations of pixel values that can represent the same semantic information. Nonetheless, $\x$ and $\hat{\x}$ can still be considered semantically equivalent, thus the communication happend in a semantically lossless way.

As introduced in the example, it is possible at the syntactic level to have the introduction of some errors on the semantic symbols $z_i$. In general the correction of such errors is demanded to the syntactic level. However, while this process can fail from time to time, the additional semantic decoder at the receiver can help correct these errors on a semantic level. The assumption is that the errors introduced by the syntactic level might not impact the preservation of the semantic information $s_i$. In the previous example, the two sentences \textit{"There is a red traffic light"} and \textit{"There ys a red trafgic licht"} present multiple errors that the text-to-image model employed at the receiver is fortunately able to correct. In fact, differently to other works that employ end-to-end architecture to incorporate the channel in the training phase \cite{Gunduz2019DeepJSCC, Felix2018OFDM}, this will consider a modular approach. All the three levels of communication will be considered separately and the possible errors introduced at the syntactic level will be corrected by the generative model at the semantic level. 

As shown in \fref{fig: SEMCOM semantic_comm_scheme}, in this thesis the \gls{sc} framework is designed to be based on the presence of generative models at the receiver. In fact, generative models are useful tools to extract and work with semantic information from the data and use them to produce semantically equivalent reconstructions.

\subsection{Semantic Communication Based on Generative Models}\label{sec: SEMCOM sem_gen}
Generative models have become increasingly popular in the field of machine learning due to their exceptional ability to model complex data distributions and generate new data samples that retain the essence of the original content. These models have achieved remarkable results in various tasks such as image \gls{sr} \cite{Ledig2017Photo}, denoising \cite{Vincent2010Stacked}, image-to-audio translation \cite{Zhou2017Visual}, 3D synthesis \cite{Wu2016Learning} and more.

The fundamental principle of generative modeling involves designing a model that can learn the underlying distribution of the data $\x$. This distribution is then used to produce representative samples similar to the data contained in the training dataset. In learning the distribution of the data, the model captures the semantic information $s_i \in S$ contained in it. For instance, when a generative model is trained to generate human faces, it starts by learning how to identify simple patterns (features), like horizontal and vertical lines, rounded objects, or abrupt changes in colors. Progressively, as the model processes the input data through its hidden layers, more complex semantic features will be identified and considered. Instead of simple patterns, the model will focus on structures like eyes, nose, mouth, etc. and progressively incorporate more complex features \cite{Karras2019Style, Zeiler2014Visualizing}.

This way, the model is able to obtain a hidden representation $\z$ of the data that contains all the semantic information $s_i$ about faces. By using these hidden representations, the model can generate new samples that are coherent with the original data.

One important advantage of employing generative models is their flexibility. These models are able to reconstruct a plausible $\hat{\x}$ in an incremental way. If the received semantic information increases, then the model will be able to reconstruct data that are more semantically close to the original one, with higher \textit{semantic similarity}. By semantic similarity is intended any metric that is able to capture the differences between the semantic content of data. This will be discussed in detail in \sref{sec: GM evaluation metrics} where various semantic metrics will be introduced.

This concept of flexibility provided by generative models is depicted in \fref{fig: SEMCOM generative_model_channel}.

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{Figures/Semantic_communication/Sem_genAI/Channel_Gen_AI.png}
    \caption[Semantic reconstruction performances as channel quality varies]{Impact of varying channel conditions on semantic reconstruction and how the generative model adapt to these changes. As the channel quality improves, the generative model is able to receive more semantic symbols $\z_i$ and eventually reach semantically equivalence between $\x$ and $\hat{\x}$.}
    \label{fig: SEMCOM generative_model_channel}
\end{figure}

Suppose that the original data $\x$ is the picture of the dog in the top-right corner \cite{Lexthehead2024Alex}. Associated with this picture is the semantic meaning $S$ produced with the  LLaVA-v1.5-13B model \cite{Liu_2024_CVPR} and reported under $\x$. In a real scenario the transmission might not always be possible. In fact, the channel might be noisy, the bandwidth might be limited or the transmission might be too expensive.

In these cases, it is important to have a communication system that is able to adapt to the channel conditions. To this end, suppose that the syntactic level is based on a successive refinement approach \cite{Tian2008SuccRefinement}. In this way, the receiver will be able to decode more semantic symbols $z_i$ as the channel conditions improve. If the channel performances are very poor, it might be possible to transmit only one semantic symbol. This $z_1$=\textit{"a dog"} can be inserted as input in the SDXL generative model \cite{Podell2023SDXL} to produce the bottom-left image in \fref{fig: SEMCOM generative_model_channel}. This will already be enough to generate a realistic image of a dog, but the semantic similarity with the original one might be very low.

As soon as the channel conditions improve, the generative model will be able to receive more semantic symbols $z_i$ and generate images that are increasingly semantically similar to the original one. If the channel conditions are very good, then it is possible to transmit all the semantic symbols $z_i$, and the generative model will be able to generate an image that is semantically equivalent to the original one.

However, the realization of this adaptive scenario is not trivial. The selection of the semantic symbols $z_i$ to be transmitted is a complex task related to the effectiveness level of communication.

\section{Goal-Oriented Communication (Effectiveness Level)} \label{sec: SEMCOM go}
The last level of communication proposed by Weaver is the effectiveness level. This level is related to the effect of the received message on the receiver end. In other words, it is related to the ability of the message to induce the desired behavior in the receiver. In this context, the idea is to transmit not all the information, but only those that are strictly relevant to the fulfillment of a certain goal. For this reason, the communication is also referred to \gls{goc}.

This level of communication can be considered as a higher level that can directly orchestrate the behavior of lower (semantic and syntactic) levels and how the communication infrastructure process the data and control the different connected components \cite{Zhang2022goaloriented, Kountouris2024goal}.

The scheme of the \gls{goc} is depicted in \fref{fig: SEMCOM go_comm_scheme}.

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{Figures/Semantic_communication/G_O_comm_scheme.png}
    \caption[Goal-Oriented Communication Scheme]{Goal-Oriented Communication Scheme.}
    \label{fig: SEMCOM go_comm_scheme}
\end{figure}

Consider a scenario where the transmitter can observe some data $\x$, and the communication is happening to perform a certain task at the receiver end. The nature of this task can range from classification to parameter estimation. It can be constrained by power consumption or bandwidth limitations. In this context consider $\y$ as the data that contains the necessary information to perform a certain task at the receiver.

The idea of \gls{goc} is to compress and transform $\x$ to obtain a representation $\z=\Phi(\x)$ that is able to preserve all the information that $\x$ has on $\y$.

Additionally, the level of compression can be dynamically adapted to external factors that depend on the communication infrastructure. An example is in the context of \glspl{en} where multiple \glspl{ed} communicate to the same \gls{es}. It might happen that the communication infrastructure is at capacity but an additional \gls{ed} has to be inserted. In this case, the \gls{goc} paradigm is required to optimize the resource in the \gls{en}. This might cause some \glspl{ed} to compress the data more to reduce their impact on the network and allow all the \glspl{ed} to communicate with the \gls{es}.

This is just one of the possible examples of how \gls{goc} can be used to adapt to external factors. The dynamic adaptation of the network will be discussed in \cref{ch: Goal_oriented}, where the \gls{goc} framework will be introduced in the context of \glspl{en} and the resource optimization process presented.

In this section, the focus will be posed on one of the possible approaches to obtain a representation $\z$ that is effective.

\subsection{Goal-Oriented Communication Based on Information Bottleneck} \label{sec: SEMCOM ib}

In \gls{goc}, the idea is to identify the transformation $\z=\Phi(\x)$ so that $\z$ contains the same level of relevant information that $\x$ has on $\y$, and at the same time, $\z$ is maximally compressed.

When a transformation $\z$ satisfies these two conditions, it is said to be a \textit{minimal sufficient statistic} of $\x$ with respect to $\y$ \cite{Cover2006IT}. This concept can be expressed in terms of the mutual information between the terms as:
\begin{equation}
    I(\x; \z) = \min_{\w: I(\w; \y) = I(\x; \y)} I(\x; \w).
    \label{eq: SEMCOM min_suff_stat}
\end{equation}
Minimizing the mutual information $I(\x; \z)$ ensures that the transformation $\z$ is as compressed as possible. Simultaneously, the constraint $I(\z; \y) = I(\x; \y)$ guarantees that $\z$ preserves all the information that $\x$ originally had regarding $\y$.
The advantage of transmitting this transformation $\z$ is that the receiver will be able to perform the task as well as if $\x$ was transmitted but with a potentially high advantage in terms of transmitted bits.

Unfortunately, the process of identifying the minimal sufficient statistic is not an easy task. For this reason in \cite{Strinati20216G} was proposed, and further extended in \cite{Shao2021learning},  the use of the \gls{ib} method \cite{Tishby1999IB} to perform \gls{goc}. The \gls{ib} is used to loosen the constraints described in \eref{eq: SEMCOM min_suff_stat} and is represented as follows:
\begin{equation}
    \min_{\Phi=p(\z|\x)} I(\x; \z) - \beta I(\z; \y).
    \label{eq: SEMCOM ib_problem}
\end{equation}
The new formulation is a trade-off between the compression capabilities of $\Phi$ and the information that $\z$ is able to retain about $\y$. In the \gls{ib} problem the transformation function $\Phi=p(\z|\x)$ is represented in a probabilistic way and not deterministic. The parameter $\beta$ is a non-negative parameter used to explore the trade-off between the two mutual information where $I(\z; \y)$ is referred to as \textit{relevance} while $I(\x; \z)$ as \textit{complexity}.

For low values of $\beta$, the \gls{ib} problem will prefer compression over performance. This translates into a transformation $\z$ that has very low complexity but, unfortunately, also the performance will be negatively impacted, and the effectiveness of the communication will be lower. At high values of $\beta$, the transformation $\z$ will be obtained by preferring the maximization of the relevance. This will inevitably improve the effectiveness of the communication as well as the complexity of the transformation, resulting in more bits to be transmitted. The relationship between the two quantities is reported in \fref{fig: SEMCOM gib_tradeoff}.

\begin{figure}
    \centering
    \includegraphics[width=0.85\textwidth]{Figures/Semantic_communication/gib_curve.png}
    \caption[Information Bottleneck Trade-Off in the Relevance-Complexity plane]{Trade-off representation of the \acrshort{ib} problem in the Relevance-Complexity plane \cite{Zaidi2020IB}. The blue line represents the optimal solutions for different values of $\beta$ while the red line represents the limit of relevance given by all the information that $\x$ has on $\y$. The optimal solution is achieved in the case of  \acrshort{gib} where the projection matrix $\mathbf{A}$ is selected as in \eref{eq: SEMCOM Matrice_A}.}
    \label{fig: SEMCOM gib_tradeoff}
\end{figure}

In the plot, it is possible to identify a feasible region where the solution will be found. The optimal solutions for any given value of $\beta$ are represented by the points on the blue boundary between the two regions. As $\beta$ increases, the feasible region approaches the red line that represents the upper-bound of the relevance given by all the information that $\x$ has on $\y$, formally $I(\x;\y)$.

This problem is not trivial to solve in general. However, there are two cases in which it is possible to be solved: (i) the case of discrete random variables that admits a solution achievable via an iterative algorithm based on the Blahut-Arimoto algorithm \cite{Tishby1999IB}, and (ii) the case where $\x$ and $\y$ are jointly Gaussian \cite{Chechik2004GIB}.\\

In this work is presented the application of this second case, also referred to as the \gls{gib}, in the context of \gls{goc}. In fact, this case is helpful to better understand how the \gls{ib} works but also represents one of the cases where the solution is optimal. For this reason in this section will be recalled the main results of the \gls{gib} while in \sref{sec: EN_ib} it will be presented the integration in the \gls{en}.

Denote by $\x\sim \mathcal{N}(\mathbf{0}, \Sigma_{X})$ and $\y\sim \mathcal{N}(\mathbf{0}, \Sigma_{Y})$ two centered multivariate jointly Gaussian vectors of dimension $d_\x$ and $d_\y$, respectively. Also, let $\Sigma_{XY}$ represent the cross-covariance between $\x$ and $\y$. Under these conditions, the optimal encoding rule $\Phi$ is a linear transformation \cite{Chechik2004GIB} expressed as:
\begin{equation}
    \z = \Phi(\x)  = \mathbf{A} \x + \boldsymbol{\xi},
\end{equation}
where $\boldsymbol{\xi} \sim  \mathcal{N}(\mathbf{0}, \Sigma_{\xi})$ is a Gaussian noise, statistically independent of $(\x, \y)$.

In this configuration, Chechik et al. \cite{Chechik2004GIB} were able to evaluate the optimal transformation matrix $\mathbf{A}$ as a function of the trade-off parameter $\beta$. The structure of $\mathbf{A}$ is given by:
\begin{equation}
\mathbf{A} = \left\{\begin{matrix}
            [\mathbf{0}^T;...;\mathbf{0}^T] & 0 \leq \beta \leq \beta_1^c\\
            [\alpha_1\mathbf{v}_1^T; \mathbf{0}^T;...;\mathbf{0}^T] & \beta_1^c < \beta \leq \beta_2^c\\
            [\alpha_1 \mathbf{v}_1^T;\alpha_2\mathbf{v}_2^T;\mathbf{0}^T;\ldots;\mathbf{0}^T] & \beta_2^c < \beta \leq \beta_3^c\\
            \vdots\\
            [\alpha_1 \mathbf{v}_1^T;\alpha_2\mathbf{v}_2^T;\ldots;\alpha_{n_{\beta}}\mathbf{v}_{n_{\beta}}^T] & \beta_{n_{\beta}}^c < \beta 
            \end{matrix}\right.
\label{eq: SEMCOM Matrice_A}
\end{equation}
where $\mathbf{v}_i$ represent the left eigenvectors of the matrix $\bSigma_{X/Y}\,\bSigma_X^{-1}$ sorted by their corresponding ascending eigenvalues $\lambda_i$, for all $i=1, \ldots, n_{\beta}$; also, $\beta_i^c = \frac{1}{1-\lambda_i}$ denote the critical values of $\beta$ at which the number of components varies, $\alpha_i=\sqrt{\frac{\beta(1-\lambda_i)-1}{\lambda_i r_i}}$, and $r_i = \mathbf{v}_i^T\Sigma_{X} \mathbf{v}_i$,  for all $i=1, \ldots, n_{\beta}$. The value $n_{\beta}$ represent the highest index $i$ such that $\lambda_i \leq \frac{\beta-1}{\beta}$. This condition guarantees that the solution found is a global optimum.

The structure of the matrix $\mathbf{A}$ is very peculiar. For certain values of the trade-off parameter $\beta < \beta_1^c$, the preference for compression over performance is so high that the transformation $\z$ is the null vector and no information is sent to the receiver. As $\beta$ starts to increase, more importance is given to the performance, and $\z$ starts to populate with elements.

Because of the design of the \gls{gib}, it is possible to express the mutual information $I(\x; \z)$ and $I(\z; \y)$ in closed form as a function of $\beta$ as follows:
\begin{align}
    &I(\x; \z)=\frac{1}{2} \sum_{i=1}^{n_{\beta}} \log_2\left((\beta-1)\frac{1-\lambda_i}{\lambda_i}\right),\\
    &I(\z; \y)=I(x; z)-\frac{1}{2} \sum_{i=1}^{n_{\beta}}\log_2\left(\beta(1-\lambda_i)\right).
\end{align}
Since the solution of the \gls{gib} problem presented in \cite{Chechik2004GIB} is optimal, by representing these values on the relevance-complexity plane in \fref{fig: SEMCOM gib_tradeoff}, the trade-off will lie on the blue boundary between the feasible regions.

The interesting advantage of using the \gls{gib}, or more generally the \gls{ib}, is that the performance can be controlled by the trade-off parameter $\beta$. As will be discussed in \cref{ch: Goal_oriented}, this parameter allows communication to be adjusted based on external factors that might influence it. When multiple \glspl{ed} are connected to the same \gls{es}, a feedback mechanism from the \gls{es} to each \gls{ed} can influence and force it to adjust the value of $\beta$. By modifying the complexity of the transformation and its relevance, this approach becomes a powerful tool for facilitating the optimization of communication.

\section{Semantic-Goal-Oriented Communication} \label{sec: SEMCOM sem_go}
After discussing the three levels of communication, it is interesting to consider merging them all. As already discussed in the previous section and depicted in \fref{fig: INTRO semantic_communication}, the effectiveness level can work in synergy with the other levels. In this section, the structure of the so-called \gls{sgoc} will be discussed, as illustrated in \fref{fig: SEMCOM sem_go_comm_scheme}.

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{Figures/Semantic_communication/G_O_SEM_comm_scheme.png}
    \caption[Semantic-Goal-Oriented Communication Scheme]{Semantic-Goal-Oriented Communication Scheme.}
    \label{fig: SEMCOM sem_go_comm_scheme}
\end{figure}

This type of communication is based on the premise that the semantic information $S$ is the most critical component of the data $\x$, and its preservation, along with power optimization, is the ultimate goal. Recalling \fref{fig: SEMCOM generative_model_channel}, even after the semantic level has successfully produced the semantic symbols $z_i$, the channel conditions may not be adequate to transmit all these symbols.

In such cases, \gls{goc} interacts with the semantic and syntactic levels, as well as the network transmitting the semantic symbols, to orchestrate the communication process. This orchestration involves selecting an appropriate subset of semantic symbols $z_i$ that are sufficient for the specific task and current network conditions.

By adaptively choosing which semantic symbols to transmit, the system ensures that the most relevant information is conveyed. This approach maintains the effectiveness of the communication by focusing on transmitting the semantic content that is most crucial for the receiver's task.

The practical implementation of this concept will be presented in \sref{sec: EN_nn}, where the model introduced in \cref{ch: SQGAN} will be integrated into a \gls{goc} framework.




