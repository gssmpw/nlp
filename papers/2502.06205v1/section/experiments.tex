\section{Experiments}


\subsection{Experimental Setup}

\textbf{Datasets.} To comprehensively evaluate our \modelname, we experiment on both single-hop datasets including Natural Questions (NQ)~\citep{KwiatkowskiPRCP19}, PopQA~\citep{mallen-etal-2023-trust}, and TriviaQA (TQA)~\citep{joshi-etal-2017-triviaqa}, as well as multi-hop datasets including 2WikiMultiHopQA (2Wiki)~\citep{ho-etal-2020-constructing}, Musique~\citep{trivedi-etal-2022-musique}, and HotpotQA (HQA)~\citep{yang-etal-2018-hotpotqa}.
For each dataset, we only use 6000 randomly sampled questions instead of the full training set.

\input{section/table/main_table}

\textbf{Baseline.}
We compare our method against a diverse set of baselines, including
(1) \textbf{Direct}: Directly answer questions without retrieval.
(2) \textbf{Standard RAG}: The standard retrieval-augmented method retrieves documents based on the question.
(3) \textbf{Retriever Fine-tuning Method}: REPLUG~\citep{ShiMYS0LZY24}.
(4) \textbf{LLM Fine-tuning Method}: Self-RAG~\citep{AsaiWWSH24}, InstructRAG~\citep{abs-2406-13629}, and Auto-RAG~\citep{auto-rag}.
(5) \textbf{Intermediate module Method}: Reranker~\citep{gte}, QueryRewrite~\citep{MaGHZD23}, SKR~\citep{WangLSL23}, and SlimPLM~\citep{TanD0GFW24}.

\textbf{Implementation Details.}
Following~\citet{AsaiWWSH24}, we construct our retrieval system using the 2018 Wikipedia dump~\citep{yang-etal-2018-hotpotqa} as the knowledge source and use contriever-msmarco~\citep{IzacardCHRBJG22} as our dense retriever.
We utilize Qwen2-72B-Instruct~\citep{qwen2} as fixed LLM server, while Qwen2-0.5B or Qwen2-1.5B is trained as candidate lightweight proxy for efficient edge deployment.
In the warm-up phase, we collect 4 solutions for each question with Qwen2-72B-Instruct.
We use a learning rate of 4e-5, with 3 epochs and a batch size of 512.
For the RL phase, we set learning rate of 5e-7 for policy model and 5e-6 for value model with a batch size of 1024 and maximal depth of 13.
% For the RL phase, we adopt our tree-based sampling and credit assignment mechanism for end-to-end training.
More details please refer to Appendix~\ref{app:imple_details}.

\subsection{Main Results}
We report the performance on both single-hop and multi-hop datasets in Table~\ref{tab:main_result}.
\textbf{First}, our \modelname consistently outperforms various baselines across different datasets, achieving superior average performance of 62.08\% and 63.53\% with lightweight proxies of only 0.5B and 1.5B parameters, respectively.
This demonstrates the effectiveness of our proxy-centric alignment approach in bridging the gap between the retriever and the LLM.
\textbf{Second}, compared to single-hop datasets, our method yields particularly notable gains in challenging multi-hop reasoning tasks. Specifically, \modelname achieves significant improvements on multi-hop datasets (2Wiki +15.5\%, HQA +13.2\%, Musique +12.9\%), while maintaining strong performance on single-hop tasks (NQ +3.5\%, PopQA +3.9\%, TQA +3.2\%).
This significant performance gain suggests that, even without training original RAG system, our \modelname effectively enhances the coordination between the retriever and LLM, which is particularly crucial for addressing complex multi-hop tasks.
\textbf{Third}, although retriever fine-tuning method~\citep{ShiMYS0LZY24} requires fewer tuned parameters, it does not overcome the limitations of standard RAG systems in handling complex cognitive and multi-hop reasoning tasks.
Both LLM fine-tuning and intermediate module methods show promising results, but are constrained by either large tuning parameters (7B/72B) or inconsistent performance across different reasoning datasets.
In contrast, our \modelname achieves consistent improvements across all datasets with only 0.5B/1.5B additional parameters, demonstrating both efficiency and effectiveness in enhancing RAG systems.

\subsection{Analysis of Plug-and-Play Proxy}
\input{section/table/ood_table}
In this section, we conduct a comprehensive investigation of performance across three out-of-distribution (OOD) dimensions, including OOD datasets, retrieval systems, and LLM servers. 
This analysis aims to demonstrate that our proxy is a plug-and-play module with superior generalization capabilities. 
In Table~\ref{tab:ood_results}, we assess its modularity and generalization by introducing two recent and challenging OOD datasets: FreshQA (FQA)~\citep{VuI0CWWTSZLL24} and MultiHop-RAG (M-RAG)~\citep{multihop_rag}. 
Additionally, we replace the retriever with the Google search engine~\citep{schmidt2014google} and experiment with different LLM servers.

\textbf{First}, LLM fine-tuning approaches exhibit notably inferior performance compared to the standard RAG. This significant degradation suggests that directly fine-tuning LLMs, while potentially effective for specific tasks, may compromise the inherent generalization capabilities and lead to subpar OOD performance.
\textbf{Second}, while intermediate-module methods maintain competitive performance, their focus on optimizing individual tasks may compromise their robustness.
In contrast, our \modelname holistically optimizes all communication tasks of the entire RAG pipeline through multi-agent collaboration, effectively aligning the retriever and LLM while preserving their inherent generalization capabilities.
This enables \modelname to achieve superior generalization across all OOD settings, consistently outperforming existing approaches with a large margin, 4.22\% over the best performing baseline on average.
\textbf{Finally}, even all three dimensions are OOD, \modelname exhibits robust performance across different LLM servers (Qwen2-72B, Qwen2-7B, Llama3.3-70B, and GPT-4o-mini), with consistent improvements ranging from 1.7\% to 5.6\%.
This platform-agnostic performance demonstrates the plug-and-play capability of our method, enabling seamless integration with various retrievers and LLM servers without requiring any modifications.


\subsection{Ablation Study}
% \begin{figure}[t]
\begin{wraptable}{r}{8cm}
\vspace{-4em}
    \begin{center}
    \includegraphics[width=\linewidth]{images/ablation_rl.pdf}
    % \vspace{-2em}
    \caption{Ablation Study.}
    \vspace{-2em}
    \label{fig:ablation_rl}
    \end{center}
% \end{figure}
\end{wraptable}

\textbf{Ablation on Training Paradigm.}
To thoroughly evaluate the effectiveness of different components in our training process, we conduct comprehensive ablation studies across six in-domain datasets. Specifically, we examine the following variants:
(1) ``w/o Tree-structured Rollout'': A variant without the tree-structured rollout and Monte Carlo credit assignment, meaning that we directly optimize each agent using the system-level reward (a single trajectory).
(2) ``w/o RL'': The performance in the supervised warm-up phase.
(3) ``SOTA Baseline'': The strongest baseline of each dataset.

The experimental results reveal several key findings. 
\textbf{First}, removing the tree-structured rollout (and Monte Carlo credit assignment) leads to unstable performance during the RL phase, occasionally degrading below the supervised warm-up model. 
This degradation can be attributed to the direct use of system-level rewards as supervised signals for all agents, which fails to accurately assess individual agent contributions and may mask detrimental actions within successful trajectories.
In contrast, our Monte Carlo credit assignment mechanism enables reward allocation in probabilistic expectation through tree-structured exploration, ensuring that each agent receives appropriate feedback for its specific actions.
\textbf{Second}, comparing with the supervised warm-up model (``w/o RL''), our \modelname achieves substantial improvements across all datasets.
This performance boost demonstrates that end-to-end RL optimization effectively aligns the behaviors of multiple agents towards the system-level objectives, going beyond the limitations of supervised learning that only optimizes for local agents.
The improvement is particularly significant on challenging datasets like 2Wiki (+1.6\%), Musique (+4.5\%), PopQA (+1.7\%), and TQA (+2.0\%), where sophisticated coordination among agents is crucial for task success.

\input{section/table/ablation_strategy}
\textbf{Ablation on Collaborative Strategy.}
To better understand the effectiveness of our collaborative strategy, we conduct ablation studies on OOD datasets by forcing \modelname to consistently use a fixed strategy for all questions, as shown in Table~\ref{tab:ablation_stategy}.
Notably, \texttt{[No Retrieval]} only utilizes the inherent knowledge of LLMs exhibit the lowest performance, which is suited for addressing straightforward problems.
We observe that the \texttt{[Planning]} strategy consistently achieves the best performance among other strategies, which is reasonable given its more sophisticated reasoning process and higher inference cost.
Moreover, even our simple \texttt{[Retrieval]} strategy significantly outperforms other baselines shown in Table~\ref{tab:ood_results}, demonstrating the effectiveness of the retrieval-filter capability in our \modelname.


\subsection{Detailed Analysis}

\begin{figure}[t]
    \centering
    \begin{minipage}{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{images/inference_time.pdf}
        \caption{Performance and Efficiency Comparison.}
        \label{fig:inference_time}
    \end{minipage}
    \hfill
    \begin{minipage}{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{images/acc_steps.pdf}
        \caption{Average Accuracy of \modelname during RL training.}
        \label{fig:acc_steps}
    \end{minipage}
\end{figure}


% \begin{figure}[t]
%     \centering
%     \includegraphics[width=0.5\linewidth]{images/inference_time.pdf}
%     % \vspace{-2em}
%     \caption{Performance and Efficiency Comparison.}
%     % \vspace{-1em}
%     \label{fig:inference_time}
% \end{figure}

\textbf{Inference Efficiency Analysis.}
To investigate the inference efficiency of \modelname, we compare both the performance and inference cost across different methods, as illustrated in Figure \ref{fig:inference_time}.
Our approach achieves superior performance (+9.2\% for in-domain and +4.2\% for OOD scenarios) while maintaining a reasonable inference time of 4.8s per question.
Although slightly slower than the Standard RAG method (3.6s), \modelname yields significant performance gains across both in-domain and out-of-generation evaluations.
Furthermore, \modelname outperforms most methods, such as AutoRAG~\citep{auto-rag} and SlimPLM~\citep{TanD0GFW24}, in both efficiency and effectiveness.
This demonstrates that \modelname achieves an optimal balance between performance and computational efficiency.

% \begin{figure}
%     \centering
%     \includegraphics[width=0.5\linewidth]{images/acc_steps.pdf}
%     % \vspace{-2em}
%     \caption{Average Accuracy of \modelname during RL training.}
%     % \vspace{-1em}
%     \label{fig:acc_steps}
% \end{figure}
\textbf{Training Dynamics in RL.}
Figure~\ref{fig:acc_steps} depicts the average performance trajectory of our \modelname across six in-domain benchmarks throughout the RL training process.
The results demonstrate consistent and stable improvement in accuracy for both model (\modelname-1.5B and \modelname-0.5B) during RL training. 
The final accuracy of \modelname-1.5B (63.53\%) surpasses that of \modelname-0.5B (62.08\%), suggesting that model capacity plays a role in the ultimate performance ceiling.
Both models exhibit rapid improvement during the initial training phase and eventually outperform the SFT model.
This significant improvement highlights the effectiveness of our tree-structured multi-agent optimization framework in optimizing multi-agent collaborative systems over time.


\subsection{Additional Results}
We conduct additional analyses on \modelname-ICL performance, RL training dynamics, and inference depth distribution.
Detailed results and discussions are presented in Appendix~\ref{app:additional_experiments}.
