\section{Preliminaries}

Before introducing \modelname, we first review the preliminaries of cooperative multi-agent reinforcement learning (MARL), where multiple agents work collaboratively to accomplish given tasks.
A cooperative MARL problem is generally formalized as a cooperative stochastic game, represented by a tuple $\langle\mathcal{N}, \{\mathcal{S}^i\}_{i \in \mathcal{N}}, \{\mathcal{A}^i\}_{i\in \mathcal{N}},\mathcal{T},\mathcal{R}\rangle$, where:
\begin{itemize}[topsep=1pt, partopsep=1pt, leftmargin=12pt, itemsep=-1pt]
\item $\mathcal{N}=\{1,2,...,n\}$ is the set of agents in the system. % represents
\item $\mathcal{S}^i$ is the state space of agent $i$, where each agent maintains its agent-specific state information.
\item $\mathcal{A}^i$ is the action space of agent $i$, defining the joint action space $\mathcal{A}:=\mathcal{A}^1 \times \cdots \times \mathcal{A}^n$.
\item $\mathcal{T}: \{\mathcal{S}^i\}_{i\in \mathcal{N}} \times \mathcal{A} \rightarrow \{\mathcal{S}^i\}_{i\in \mathcal{N}}$ is the deterministic state transition function, specifying how the states of agents update given their joint action $\bm{a} \in \mathcal{A}$.
\item $\mathcal{R}: \{\mathcal{S}^i\}_{i\in \mathcal{N}} \times \mathcal{A} \times \mathcal{N} \rightarrow \mathbb{R}$ is the system-level reward that measures the overall task completion, where 1 indicates success and 0 otherwise.
\end{itemize}
In this cooperative setting, all agents share the same system-level reward $\mathcal{R}$ and work together to accomplish the task. Each agent follows its policy $\pi^i: \mathcal{S}^i \rightarrow \mathcal{A}^i$ to select actions based on its local observations. A trajectory $(\{s_0^i\}_{i\in \mathcal{N}}, \bm{a}_0, \{s_1^i\}_{i\in \mathcal{N}}, \bm{a}_1, ...)$ represents the sequence of agent states and joint actions, where $s_t^i \in \mathcal{S}^i$ is the state of agent $i$ at time step $t$, and $\bm{a}_t = \{a^i_t\}_{i\in\mathcal{N}} \in \mathcal{A}$ is the joint action at time step $t$ under the joint policy $\pi = (\pi^1, ..., \pi^n)$.

% The goal of cooperative MARL is to find optimal policies $\pi^i: \mathcal{S}^i \rightarrow \Delta(\mathcal{A}^i)$ for each agent $i$ that maximize the system-level reward:
% \begin{equation}
%     J(\pi) = \mathbb{E}_{\tau \sim \pi}[R(\tau)]
% \end{equation}
% where $\tau = (\{s_0^i\}_{i\in \mathcal{N}}, \bm{a}_0, \{s_1^i\}_{i\in \mathcal{N}}, \bm{a}_1, ...)$ represents a trajectory of agent states and joint actions, with $s_t^i \in \mathcal{S}^i$ being the state of agent $i$ at time step $t$, and $\bm{a}_t = \{a^i_t\}_{i\in\mathcal{N}} \in \mathcal{A}$ being the joint action at time step $t$ under the joint policy $\pi = (\pi^1, ..., \pi^n)$.

% \begin{itemize}
% \item $R: \{\mathcal{S}^i\}_{i\in \mathcal{N}} \times \mathcal{A} \rightarrow \mathbb{R}$ is the system-level reward that measures the overall task completion, where 1 indicates successful task completion and 0 otherwise. This reward is shared by all agents in the system.
% \end{itemize}



% While all agents share the same team reward $R(\tau)$, effectively learning from such a sparse team reward requires proper credit assignment, which is the main focus of this work.
