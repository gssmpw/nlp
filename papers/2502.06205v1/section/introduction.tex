\section{Introduction}
% which requires either adapting the retriever to match the LLM's inherent preferences or optimizing the LLM to effectively master the retriever's behavior.

Recent advances in retrieval-augmented generation (RAG) for large language models (LLMs) have demonstrated remarkable capabilities in various tasks~\citep{claude,gpt_4o,llama3,qwen2,gemma,AsaiWWSH24,abs-2406-13629}, empowering LLMs to acquire up-to-date or domain-specific knowledge while mitigating hallucinations~\citep{abs-2312-10997,FanDNWLYCL24,abs-2406-08116}.
The effectiveness of RAG systems, however, hinges on the alignment\footnote{Here, alignment refers to the functional coordination between retrievers and LLMs, rather than human preference alignment.} between the retriever and the LLM—an inherently challenging goal as these components are typically developed independently without co-training. 
This lack of co-training can result in semantic mismatch and suboptimal interactions: retrievers may fail to provide information tailored to the LLM’s needs, while LLMs may struggle to generate effective queries or seamlessly incorporate retrieved content.
% This alignment is particularly challenging as these two components are typically trained independently without awareness of each other's characteristics---retrievers may not capture the specific types of information that LLMs can effectively utilize, while LLMs struggle with both formulating effective queries and properly incorporating the retrieved content into their generations.

Existing approaches address this misalignment through three main strategies: (1) fine-tuning retrievers to align with LLM preferences, (2) optimizing LLMs to adapt to retriever behavior, and (3) introducing intermediate modules to bridge the gap between them~\citep{MaGHZD23,ShiMYS0LZY24,AsaiWWSH24,abs-2406-13629,auto-rag,abs-2407-02485}.
Despite progress, these methods face notable challenges: fine-tuning retrievers often requires carefully curated data and may not be feasible for commercial search engines~\citep{schmidt2014google,webgpt}, while optimizing LLMs is resource-intensive and risks compromising their original capabilities~\citep{abs-2409-10102}. 
Approaches that introduce intermediate modules to avoid modifying either the retriever or the LLM primarily focus on optimizing individual tasks, such as query rewriting or document reranking~\citep{MaGHZD23,WangLSL23,TanD0GFW24}. 
However, optimizing a single task in isolation often leads to suboptimal results, as the effectiveness of RAG systems relies on the cohesive interaction and collaboration among multiple components throughout the entire pipeline~\citep{FanDNWLYCL24,abs-2409-10102}.

In human search behavior, the process typically involves an iterative back-and-forth process of proposing search queries and reviewing documents until the correct response is either found in the retrieved documents or emerges in the person’s mind. 
Similarly, LLMs can emulate this process by taking on multiple roles within a search pipeline: proposing search queries, reviewing documents, deciding when to terminate retrieval, and generating the final response, among other tasks.
However, assigning all these tasks to LLMs results in numerous calls, leading to high computational costs, especially for complex questions. 
To address this, it is desirable to design a compact proxy capable of handling most tasks, while reserving the most challenging ones to LLMs—such as planning the overall roadmap and generating the final response. % — query decomposition

% To address this, it is desirable to design a compact proxy capable of handling most tasks, while delegating only the most challenging ones to LLMs—such as planning the overall roadmap and generating the final response.

% In practice, human users naturally demonstrate an effective approach when working with retrievers and LLMs. 
% Particularly, they instinctively employ various coordinated strategies, such as determining retrieval necessity for different tasks, iteratively refining search queries, carefully filtering and synthesizing relevant information, effectively bridging the gap between retrievers and LLMs.
% We term this process as ``human-guided alignment'', which reveals two crucial insights: (1) effective RAG alignment can be achieved through an intelligent intermediary without modifying the original components, and (2) the alignment process requires holistic optimization across the entire pipeline rather than isolated optimization of individual components.
% In contrast, human users, when interfacing with retrievers and LLMs, naturally employ a holistic approach, dynamically combining various strategies such as determining retrieval necessity, reformulating queries, and filtering documents to bridge the gap between retrievers and LLMs.
% This human-guided alignment demonstrates the effectiveness of having an intelligent intermediary that can adaptively coordinate between retrievers and LLMs without modifying either component.
% This adaptive and holistic planning ability enables humans to make interconnected decisions throughout the RAG pipeline without necessitating modifications to the retriever or LLM.
% Notably, these approaches overlook a crucial insight: human users, when interfacing with retrievers and LLMs, naturally employ various strategies such as query reformulation and context filtering to bridge the gap between the retriever's behaviors and LLM's requirements, without modifying either model.

% Inspired by this human-guided alignment paradigm, we propose \modelname, a lightweight yet effective proxy-centric alignment framework that introduces an intelligent proxy to mediate between retrievers and LLMs while preserving their independence and generalizability. % original capabilities.
Therefore, we propose \modelname, a proxy-centric alignment framework that employs a lightweight yet effective proxy to facilitate seamless communication between retrievers and LLMs without modifying them or compromising their original capabilities. 
As illustrated in Figure~\ref{fig:framework}, \modelname integrates a lightweight multi-agent collaborative system within a single proxy model, where multiple agents work in a human-like manner to assist the entire RAG pipeline. 
To optimize this proxy, we employ multi-agent reinforcement learning (MARL) for end-to-end training, treating the retrievers and LLMs as part of the environment. 
To address the key challenge of optimizing multiple agents with distinct tasks, we introduce a tree-structured rollout mechanism and Monte Carlo credit assignment to improve reward distribution among different agents.
Experimental results demonstrate that the RL-trained proxy achieves robust performance on both in-domain and out-of-distribution datasets, even with unseen retrievers and LLMs, highlighting its plug-and-play modularity and superior generalization capability.
Our contributions are summarized as follows:
\begin{itemize}[topsep=1pt, partopsep=1pt, leftmargin=12pt, itemsep=-1pt]
    \item We propose \modelname, a novel proxy-centric alignment framework that bridges the gap between retrievers and LLMs through a lightweight multi-agent system, enabling seamless integration without modifying existing RAG components.
    \item We design an efficient multi-agent collaborative system within a single proxy model that emulates human-like search behavior, where specialized agents handle different aspects of the RAG pipeline while maintaining computational efficiency.
    \item We develop an innovative training approach combining MARL with a tree-structured rollout mechanism and Monte Carlo credit assignment, effectively addressing the challenge of optimizing multiple agents towards the system-level performance in an end-to-end manner.    
    \item Extensive experiments demonstrate that \modelname achieves superior performance across diverse datasets and exhibits strong generalization capability with unseen retrievers and LLMs, validating its effectiveness as a plug-and-play solution for RAG systems.
\end{itemize}

\begin{figure*}[ht]
    \centering
    \includegraphics[width=\linewidth]{images/proxy_framework.pdf}
    % \vspace{-2em}
    % \caption{Overall framework of \modelname. (up left) In human-guided alignment, the necessary human cognitive capabilities for interaction with the RAG system. (up right) To simulate human-guided alignment, we implement proxy-centric alignment using a multi-agent system within a lightweight proxy with collaborative strategies. (bottom) The pipeline we end-to-end optimizing the multi-agent system.}
    \caption{Overall framework of \modelname. (Upper left) Essential cognitive capabilities required for effective RAG system interaction in human-guided alignment. (Upper right) Our proxy-centric alignment simulates these human-like interaction through a lightweight multi-agent system with collaborative strategies. (Bottom) The end-to-end optimization pipeline for our multi-agent system.}
    % \vspace{-0.8em}
    % (up left) 人类与retrieval和llm交互的形式来做rag所需要的能力。
    % (up right) 我们将人类的交互过程用proxy来模拟
    % (bottom) rollout + reward assignment + optimization
    \label{fig:framework}
\end{figure*}










