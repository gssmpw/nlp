\newpage
\appendix
\onecolumn

% \section{Multiple agents}
% The action space $\mathcal{A}^r$ is defined as $\mathcal{A}^r = \{$\texttt{[No Retrieval]},\texttt{[Retrieval]<query>}, \texttt{[Planning]}$\}$, where \texttt{<query>} represents the space of possible queries.


% The action space $\mathcal{A}^d$ is defined as $\mathcal{A}^d = \{\texttt{Retrieval} \times \mathcal{Q}\} \cup \{\texttt{LLM}\}$, where $\mathcal{Q}$ represents the space of possible subqueries.

% \section{Discussions}

% \subsection{Future Work}

% \subsection{Limitations}
% sft warm-up  from zero

\section{More Implementation Details}\label{app:imple_details}

In this section, we provide a comprehensive implementation details of our proposed method. For additional insights and more intricate details, we refer the reader to our supplementary materials.


\subsection{RL Training Process}\label{app:ppo_details}
Having obtained the credit rewards that reflect each agent's contribution, we develop an optimization framework to guide end-to-end training across all agents. % encourage collaborative behaviors among agents
The key idea is to use these credit signals for optimizing the collaborative behavior of the entire system.
The optimization objective for our multi-agent system can be formulated as maximizing the expected credit rewards:
\begin{equation}
    \mathcal{J}(\theta) = \mathbb{E}_{\tau \sim \pi_\theta}\left[\sum_{i\in \mathcal{N}}\sum_{t} r_{\text{credit}}(s^i_t, a^i_t)\right]
\end{equation}
% We optimize this objective using Proximal Policy Optimization (PPO)~\citep{SchulmanWDRK17}.
% Since each agent's action is a sequence of tokens, we optimize this objective using Proximal Policy Optimization (PPO)~\citep{SchulmanWDRK17} and decompose the optimization at the token level~\citep{Ouyang0JAWMZASR22}. Specifically, we define:
Since each agent's action is a sequence of tokens, we decompose this optimization using Proximal Policy Optimization (PPO)~\citep{SchulmanWDRK17,abs-2312-01058,ZhuDW24} as follows:
\begin{equation}
    \mathcal{L}_{\text{\modelname}} = \sum_{i\in\mathcal{N}} \mathcal{L}_{\text{PPO}}^i(\theta, \phi)
\end{equation}
Specifically, for each agent $i$, we define:
\begin{equation}
    \mathcal{L}_{\text{CLIP}}^i(\theta)
    = \mathbb{E}_{\tau \sim \pi_\theta}\Big[\sum_{t}\sum_{m} \min\big(r_{t,m}^i(\theta)\hat{A}_{t,m}^i, \text{clip}(r_{t,m}^i(\theta), 1-\epsilon, 1+\epsilon)\hat{A}_{t,m}^i\big)\Big]
\end{equation}
% \begin{equation}
%     \mathcal{L}_{\text{CLIP}}(\theta)
%     = \mathbb{E}_{\tau \sim \pi_\theta}\Big[\sum_{i\in \mathcal{N}}\sum_{t}\sum_{m} \min\big(r_{t,m}^i(\theta)\hat{A}_{t,m}^i, \text{clip}(r_{t,m}^i(\theta), 1-\epsilon, 1+\epsilon)\hat{A}_{t,m}^i\big)\Big]
% \end{equation}
where $r_{t,m}^i(\theta) = \frac{\pi_\theta(a_{t,m}^i|s_{t,m}^i)}{\pi_{\theta_{\text{old}}}(a_{t,m}^i|s_{t,m}^i)}$ is the probability ratio, $s_{t,m}^i$ represents the concatenation of current state and the first $m-1$ tokens in the action sequence for agent $i$ at time step $t$, and $a_{t,m}^i$ denotes its $m$-th token.
We compute the advantage estimate using GAE~\citep{SchulmanMLJA15}: $\hat{A}_{t,m}^i = \sum_{l=0}^{M-m-1}(\gamma\lambda)^l\delta_{t,m+l}^i$, where $M$ is the token length of the action sequence.
% We compute the advantage estimate using GAE~\citep{SchulmanMLJA15}: $\hat{A}_{t,m}^i = \sum_{l=0}^{M-m-1}(\gamma\lambda)^l\delta_{t,m+l}^i$, where $M$ is the token length of the action sequence, and $\delta_{t,m}^i = r_{\text{token}}(s_{t,m}^i, a_{t,m}^i) + \gamma V_\phi(s_{t,m+1}^i) - V_\phi(s_{t,m}^i)$ is the TD-error.
% The advantage estimate $\hat{A}_{t,m}^i$ is computed using Generalized Advantage Estimation (GAE):
% \begin{equation}
%     \hat{A}_{t,m}^i = \delta_{t,m}^i + (\gamma\lambda)\delta_{t,m+1}^i + ... + (\gamma\lambda)^{M-m+1}\delta_{t,M-1}^i
% \end{equation}
% where $M$ is the token length of the corresponding response sequence, and $\delta_{t,m}^i = r_{\text{token}}(s_{t,m}^i, a_{t,m}^i) + \gamma V_\phi(s_{t,m+1}^i) - V_\phi(s_{t,m}^i)$ is the TD-error.
% The token-level reward $r_{\text{token}}$ incorporates both the tree credit and a KL penalty term:
% \begin{equation}
%     r_{\text{token}}(s_{t,m}^i, a_{t,m}^i) = \begin{cases}
%         r_{\text{credit}}(s_t^i, a_t^i) -\beta \text{KL}(m) & m = M \\
%         -\beta \text{KL}(m) & \text{otherwise}
%     \end{cases}
% \end{equation}
% where $\text{KL}(m) = \log \frac{\pi_{\theta_{\text{old}}}(a_{t,m}^i|s_{t,m}^i)}{\pi_{\theta_{\text{ref}}}(a_{t,m}^i|s_{t,m}^i)}$, and $\beta$ is a hyperparameter that controls the strength of the KL penalty.

To estimate state values across the multi-agent system, we employ a centralized state-value function $V_\phi$ that takes each agent's state $s_{t,m}^i$ as input. The value function is optimized to minimize the mean squared error~\citep{LoweWTHAM17}:
\begin{equation}
    \mathcal{L}_V^i(\phi) = \mathbb{E}_{\tau \sim \pi_\theta}\left[\sum_{t}\sum_{m} (V_\phi(s_{t,m}^i) - \hat{G}_{t,m}^i)^2\right]
\end{equation}
where $\hat{G}_{t,m}^i = \hat{A}_{t,m}^i + V_\phi(s_{t,m}^i)$ is the empirical return.
The final optimization objective combines the policy and value losses:
\begin{equation}
    \mathcal{L}_{\text{PPO}}^{i}(\theta, \phi) = \mathcal{L}_{\text{CLIP}}^{i}(\theta) + c_v \mathcal{L}_V^i(\phi)
\end{equation}
where $c_v$ controls the weight of the value loss. This joint objective enables end-to-end training of both policy and value networks across all agents.
% Therefore, for multi-agent reinforcement learning~\citep{abs-2312-01058,ZhuDW24}, we have the final training objective:
% \begin{align}
%     \mathcal{L}_{\text{\modelname}} &= \sum_{i\in\mathcal{N}} \mathcal{L}_{\text{PPO}}^i(\theta, \phi) \nonumber \\
%     &= sum_{i\in\mathcal{N}} \Big(
%         \mathcal{L}_{\text{CLIP}}^{i}(\theta) + c_v \mathcal{L}_V^i(\phi)
%     \Big)
%     &= \sum_{i\in \mathcal{N}} \Big( \mathbb{E}_{\tau \sim \pi_\theta}\Big[\sum_{t}\sum_{m} \min\big(r_{t,m}^i(\theta)\hat{A}_{t,m}^i, \text{clip}(r_{t,m}^i(\theta), 1-\epsilon, 1+\epsilon)\hat{A}_{t,m}^i\big)\Big]
%     + \nonumber \\
%     &\mathbb{E}_{\tau \sim \pi_\theta}\left[\sum_{t}\sum_{m} (V_\phi(s_{t,m}^i) - \hat{G}_{t,m}^i)^2\right] \nonumber \\
%     &= \mathbb{E}_{\tau \sim \pi_\theta}\Big[\sum_{i\in \mathcal{N}}\sum_{t}\sum_{m} \min\big(r_{t,m}^i(\theta)\hat{A}_{t,m}^i, \text{clip}(r_{t,m}^i(\theta), 1-\epsilon, 1+\epsilon)\hat{A}_{t,m}^i\big)\Big]
% \end{align}


% In cooperative multi-agent system, we employ a centralized state-value function $V_\phi$ shared across all agents to estimate the value of each agent's state $s_{t,m}^i$. This value function is optimized to minimize the mean squared error between its predictions and the empirical returns across all agents:
% \begin{equation}
%     L_V(\phi) = \mathbb{E}_{\tau \sim \pi_\theta}\left[\sum_{i\in \mathcal{N}}\sum_{t}\sum_{m} (V_\phi(s_{t,m}^i) - \hat{G}_{t,m}^i)^2\right]
% \end{equation}
% where $\hat{G}_{t,m}^i = \hat{A}_{t,m}^i + V_\phi(s_{t,m}^i)$ is the empirical return.
% The final optimization objective combines the policy and value losses:
% \begin{equation}
%     L(\theta, \phi) = L^{\text{CLIP}}(\theta) - c_v L_V(\phi)
% \end{equation}
% where $c_v$ is a coefficient for the value loss.
% This overall optimization is performed in an end-to-end manner across all agents simultaneously.


\subsection{Implementation Details}
\textbf{Supervised Warm-up Phase:} 
We utilize \texttt{Llama-Factory}~\citep{zheng2024llamafactory} as our training framework for the initial supervised fine-tuning phase. The detailed hyper-parameters for this phase are presented in Table~\ref{tab:sft_params}.
\input{section/table/sft_table}

\textbf{Reinforcement Learning Phase:} 
For the RL training phase, we adopt \texttt{OpenRLHF}~\citep{hu2024openrlhf} as our primary training framework, coupled with \texttt{VLLM}~\citep{kwon2023efficient} inference engine. The complete set of RL training hyper-parameters is detailed in Table~\ref{tab:rl_params}. To initialize both the policy and value models, we leverage the model obtained after one epoch of supervised fine-tuning, with the language model head replaced by a value head for the value model.
\input{section/table/rl_table}

% \textbf{Inference Phase\footnote{We have released all the code in the Supplementary Material.}:}
% We construct our retrieval server using the 2018 Wikipedia dump~\citep{yang-etal-2018-hotpotqa} as the knowledge source and use contriever-msmarco~\citep{IzacardCHRBJG22} as our dense retriever.
% We also adopt the \href{https://docs.sglang.ai/}{\texttt{SGLang}}\footnote{\url{https://docs.sglang.ai/}} as the LLM server to support different model, such as Qwen2-72B-Instruct~\citep{qwen2}, Llama3.3-70B-Instruct~\citep{llama3}.
% Our inference code support two efficient inference engine: \texttt{SGLang} and \texttt{VLLM}.

\textbf{Inference Phase\footnote{The complete implementation code is available in the Supplementary Material.}:}
For the deployment of our system, we establish a comprehensive infrastructure that integrates multiple components:

\begin{itemize}[topsep=1pt, partopsep=1pt, leftmargin=12pt, itemsep=-1pt]
    \item \textbf{Retriever Server:} We construct our retrieval server using the 2018 Wikipedia dump~\citep{yang-etal-2018-hotpotqa} as the primary knowledge source. We employ contriever-msmarco~\citep{IzacardCHRBJG22} as our dense retriever for efficient and effective document retrieval. Our inference code also supports Google search engine~\citep{schmidt2014google} as the retriever server.
    
    \item \textbf{LLM Service:} We integrate \href{https://docs.sglang.ai/}{\texttt{SGLang}}\footnote{\url{https://docs.sglang.ai/}} as our LLM server, which provides compatibility with various state-of-the-art language models, including Qwen2-72B-Instruct~\citep{qwen2} and Llama3.3-70B-Instruct~\citep{llama3}. Moreover, we also support GPT series models\footnote{We use \texttt{gpt-4o-mini-2024-07-18} in our out-of-generalization experiments.}.
    
    \item \textbf{Inference Optimization:} Our implementation supports two high-performance inference engines: \texttt{SGLang} and \texttt{VLLM}, allowing users to optimize for different deployment scenarios and hardware configurations.
\end{itemize}

This modular architecture ensures both flexibility in model selection and efficiency in deployment, while maintaining robust performance across different configurations.

% \textbf{Other Details in our \modelname.}


\subsection{Dataset Details}
\input{section/table/data_statistics}
\textbf{In-domain Datasets.}
As shown in Table~\ref{tab:data_statistics}, we conduct extensive in-domain experiments on three single-hop and three multi-hop datasets.
For each dataset, we randomly sampled 6,000 instances as the training set, with sampling ratios detailed in Table~\ref{tab:data_statistics}. Overall, we utilize only 8\% of the original data as the training set.
For the in-domain test sets, we randomly sampled 1,000 instances as the test set.

\input{section/table/ood_statistics}

\textbf{Out-of-generalization Datasets.}
To comprehensively evaluate the plug-and-play capability of our \modelname in out-of-distribution generalization scenarios, we introduce two recent challenging datasets: FreshQA~\citep{VuI0CWWTSZLL24} and Multihop-RAG~\citep{multihop_rag}. 
The statistics of the OOD datasets are summarized in Table~\ref{tab:ood_statistics}.

% \subsection{Overall Algorithm}
% In this section, we present the inference process of \modelname for reference, as shown in the Algorithm~\ref{alg:infer_alg}.

% \input{section/algorithm/inference_algorithm}


\section{Instructions and State Transition Function}\label{app:instructions}
\subsection{Instructions for Each Agent}
In this section, we details the state space and action space fo each agent in our \modelname.

\textbf{Reasoning Router.}
The Reasoning Router agent operates with state space $\mathcal{S}^1=\{q\}$, where $q$ represents the input question.
This agent is responsible for determining whether retrieval is necessary for the given question and assessing the question complexity when retrieval is needed.
For a question that does not require retrieval, this agent outputs \texttt{[No Retrieval]}.
If the retrieval is needed, the agent outputs one of the following actions based on the complexity of the question $q$: for simple questions requiring retrieval: \texttt{[Retrieval]<query content>}, initiating a single-pass retrieval-filter loop, where \texttt{<query content>} defines the space of possible queries;
for complex questions: \texttt{[Planning]}, triggering the multi-step reasoning strategy.
The specific examples are as follows:
\input{section/prompts/reasoning_router}

\textbf{Information Filter.}
The state space of Information Filter consists of the question $q$, the retrieved documents, and the current objective (if in \texttt{[Planning]} mode), i.e., $\mathcal{S}^2=\{q, \text{retrieved documents}\}$ for single-pass strategy (\texttt{Retrieval<query content}), or $\mathcal{S}^2=\{q, \text{retrieved documents}, \text{current objective}\}$ for multi-step reasoning strategy (\texttt{[Planning]}).
\input{section/prompts/information_filter}


\textbf{Decision Maker.}
The Decision Maker agent operates with state space $\mathcal{S}^3=\{q, \text{Accumulated Documents}, \text{Roadmap}\}$.
Based on the current state, this agent outputs one of two possible actions: \texttt{[Retrieval]<subquery content>} (requesting additional retrieval-filtering loop through the sub-query) or \texttt{[LLM]} (passing all accumulated documents to LLM for generating the final answer).
\input{section/prompts/decision_maker}


\subsection{State Transition Function}\label{app:transition_details}
Given a state $s_t^i$ and an action $a_t^i$ in each agent $i\in \mathcal{N}$, the transition function $\mathcal{T}$ in our framework is deterministic.
Based on the three collaborative strategies introduced in Section~\ref{sec:strategy}, the state transitions are defined as follows:

\textbf{Direct Answering Strategy} (\texttt{[No Retrieval]}): In this strategy, the LLM directly generates the answer without retrieval, resulting in no state transitions between agents.

\textbf{Single-pass Strategy} (\texttt{[Retrieval]<query content>}): This strategy involves a state transition between the Reasoning Router and Information Filter agents:
\begin{equation}
    \mathcal{T}: \mathcal{S}^1=\{q\} \times \mathcal{A}=\{\texttt{[Retrieval]<query content>}\} \xrightarrow{\text{retrieval}} \mathcal{S}^2 = \{q, \text{retrieved documents}\}
\end{equation}
where $\mathcal{S}^1$ represents the initial state with the question $q$, and $\mathcal{S}^2$ represents the state for the Information Filter agent after retrieval. The Information Filter is responsible for filtering the helpful documents based on $\mathcal{S}^2$.


\textbf{Multi-Step Reasoning Strategy} (\texttt{[Planning]}): This strategy involves multiple state transitions in a cyclic manner:


\begin{itemize}[topsep=1pt, partopsep=1pt, leftmargin=12pt, itemsep=-1pt]
    \item Reasoning Router $\rightarrow$ Decision Maker: %Given the LLM-generated roadmap after \texttt{[Planning]}, we have the state $\mathcal{S}^3=\{q, \text{Accumulated Documents}, \text{Roadmap}\}$ in Decision Maker agent.
    \begin{equation}
        \mathcal{T}: \mathcal{S}^1=\{q\} \times \mathcal{A}=\{\texttt{[Planning]}\} \xrightarrow{\text{\texttt{[planning]}}} \mathcal{S}^3 = \{q, \text{Accumulated Documents}, \text{Roadmap}\},
    \end{equation}
    where the roadmap is generated by the LLM and the accumulated documents is empty for initial step.
    \item Decision Maker $\rightarrow$ Information Filter: %Given the current step's objective and retrieved documents based on the \texttt{[Retrieval]<subquery content>} outputted by the Decision Maker, we have the state $\mathcal{S}^2=\{q, \text{retrieved documents}, \text{current objective}\}$ in the Information Filter agent.
    \begin{align}
        \mathcal{T}: \mathcal{S}^3=\{q, \text{Accumulated Documents}, \text{Roadmap}\} \times \mathcal{A}=\{\texttt{[Retrieval]<subquery content>}, \nonumber \\ \text{current objective}\} \xrightarrow{\text{retrieval}} \mathcal{S}^2 = \{q, \text{retrieved documents}, \text{current objective}\},
    \end{align}
    where the current objective is generated by the Decision Maker agent in $\mathcal{S}^3$.
    \item Information Filter $\rightarrow$ Decision Maker: % Given the filtered documents, we need to update the exisiting documents (accumulated documents) in the state of Decision Maker agent: $\mathcal{S}^3=\{q, \text{New Accumulated Documents}, \text{Roadmap}\}$.
    \begin{align}
        \mathcal{T}: \mathcal{S}^2=\{q, \text{retrieved documents}, \text{current objective}\} \times \mathcal{A}=\{\text{Selected Documents}\} \nonumber \\ \xrightarrow{\text{filter}} \mathcal{S}^3_{\text{new}} = \{q, \text{Updated Accumulated Documents}, \text{Roadmap}\}.
    \end{align}
\end{itemize}

This retrieval-filter loop between the Decision Maker agent and the Information Filter agent continues until the Decision Maker outputting \texttt{[LLM]} or a termination condition is met. The state transitions in our \modelname are deterministic and well-defined, ensuring consistent behavior across the multi-agent system.


\section{Additional Experimental Results}\label{app:additional_experiments}

\subsection{Comparative Analysis of \modelname-ICL and \modelname-RL}
\input{section/table/icl_table}
In this section, we further conduct a comprehensive comparison between \modelname-ICL (Qwen2-72B-Instruct) and \modelname-RL (Qwen2-1.5B), where \modelname-ICL (Qwen2-72B-Instruct) is used to generate seed data through rejection sampling in our supervised warm-up phase.
Our experimental results, as presented in Table~\ref{tab:icl}, reveal several important findings. First, \modelname-ICL demonstrates remarkable performance, surpassing all baseline methods across different datasets (as shown in Table~\ref{tab:main_result} and Table~\ref{tab:ood_results}). 
This result validates the effectiveness of our framework design, where collaboration among multiple agents enables effective alignment of the LLM and the retriever.
However, this approach faces practical limitations due to substantial inference overhead from multiple LLM queries, making it less suitable for efficient responses and edge deployment.

To address these limitations, we introduce a compact proxy that significantly reduces computational requirements while maintaining framework effectiveness.
Our analysis reveals that while \modelname-ICL performs well overall, it may not achieve optimal performance on more challenging tasks (e.g., 2WikiMultiHopQA, HotpotQA, and Musique).
Through reinforcement learning, we further optimize individual agent capabilities, leading to substantial improvements on the complex tasks.
In conclusion, our proxy-centric alignment framework demonstrates strong performance across both variants. While \modelname-ICL showcases the framework's effectiveness through few-shot learning, \modelname-RL offers practical advantages through reduced computational requirements and enhanced performance on challenging tasks. Our \modelname-RL successfully aligns the retriever and LLM without modifying either component, while facilitating edge deployment and robust performance across diverse scenarios.

\subsection{More Analysis in RL}
% strategy ratio
% depth distribution
\begin{figure}[h]
    \centering
    \includegraphics[width=0.5\linewidth]{images/strategy.pdf}
    \caption{Strategy Ratio in RL training process.}
    \label{fig:strategy_ratio}
\end{figure}

\textbf{Strategy Ratio during RL Training Process.}
As introduced in the Section~\ref{sec:strategy}, our \modelname incorporates three distinct strategies: Direct Answering Strategy (\texttt{[No Retrieval]}), Single-pass Strategy (\texttt{[Retrieval]<query content>}), and Multi-Step Reasoning Strategy (\texttt{[Planning]}), each designed for different question complexities. 
Figure~\ref{fig:strategy_ratio} reveals how \modelname dynamically adapts its strategy selection during the RL training process.

The evolution of strategy ratios shows a clear trend: the Multi-Step Reasoning Strategy gradually dominates the decision space, stabilizing at approximately 60-70\%, while the Single-pass Strategy decreases to around 30\%. The Direct Answering Strategy maintains a consistent but low ratio of about 5\%. This distribution pattern offers several insights into our framework's learning behavior:
\textbf{First}, the limited use of Direct Answering Strategy aligns with our experimental findings in Table~\ref{tab:main_result}, confirming that solely relying on the model's inherent knowledge is insufficient for complex question-answering tasks.
\textbf{Second}, the substantial proportion of Single-pass Strategy usage demonstrates our \modelname's ability to identify scenarios where simple external information retrieval suffices.
\textbf{Most notably}, the increasing preference for Multi-Step Reasoning Strategy indicates that our \modelname recognizes the importance of multi-step reasoning in handling complex queries effectively.
These learned ratios demonstrate that our framework effectively develops a balanced strategy selection mechanism.
By dynamically choosing appropriate strategies based on question complexity, our \modelname achieves a balance between computational efficiency and reasoning capability, making it well-suited for real-world applications.

% Figure~\ref{fig:strategy_ratio} illustrates the evolution of strategy selection ratios throughout the RL training process, revealing interesting patterns in how our \modelname learns to adaptively choose different strategies in RL.
% We observe that the Multi-Step Reasoning Strategy gradually becomes dominant, reaching and maintaining approximately 60-70\% of all decisions, while the Single-pass Strategy steadily decreases to around 30\%. The Direct Answering Strategy consistently maintains a low ratio of about 5\%.
% This distribution aligns with our intuition about efficient question-answering: 
% for many questions, Direct Answering Strategy (\texttt{[No Retrieval]}) is more efficient; however, relying solely on the modelâ€™s inherent knowledge remains challenging, as shown in Table~\ref{tab:main_result}.
% The significant proportion of Single-pass Strategy (\texttt{[Retrieval]<query content>}) indicates that our framework learns to identify cases where external information is necessary.
% The utilization of Multi-Step Reasoning Strategy (\texttt{[Planning]}) steadily increases, indicating that multi-step reasoning is essential for improving the performance of RAG systems on complex problems.
% These learned ratios demonstrate that our framework effectively develops a balanced strategy selection mechanism, avoiding the computational overhead of unnecessary retrievals while maintaining the capability to handle complex queries when needed. This adaptive behavior contributes to both efficiency and effectiveness in real-world applications.

\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{images/depth_distribution.pdf}
    \caption{Depth Distribution in Test set.}
    \label{fig:depth_dist}
\end{figure}

\textbf{Depth Distribution.}
Figure~\ref{fig:depth_dist} presents the depth distribution of reasoning processes across different datasets, revealing distinct patterns that align with the inherent complexity of each task. We observe three clear categories of reasoning depth requirements:
\textbf{(1) Simple Complexity (Depth 3-5):} Datasets like NaturalQuestions, PopQA, and TriviaQA show concentrated distributions around depths 3-5, indicating that most questions in these datasets can be effectively addressed with the Direct Answering Strategy (\texttt{[No Retrieval]}) and Single-pass Strategy (\texttt{[Retrieval]<query content>}). 
This aligns with the nature of these datasets, which primarily contain straightforward factual questions.
\textbf{(2) Mixed Complexity:} HotpotQA and 2WikiMultiHopQA exhibit multiple peaks in the depth distribution, with notable concentrations around depths 3-4 and depths 9-15, indicating a diverse range of question complexity. This bimodal distribution suggests that while some questions require simple reasoning steps, others need more complex reasoning chains.
\textbf{(3) Complex Complexity:} Musique displaies broader distributions with significant density at higher depths (9-13), particularly pronounced in their rightward skew. 
Musique's distribution is notably spread across higher depths, consistent with its design for multi-step reasoning questions.

These distributions validate our framework's adaptive capability in handling queries of varying complexity. The framework naturally adjusts its reasoning depth based on task requirements, demonstrating efficient resource utilization while maintaining the ability to perform deep reasoning when necessary.


\section{Additional Prompts}
In this section, we supplement additional prompts based on Appendix~\ref{app:instructions}.


\subsection{Roadmap}
In the multi-step reasoning strategy, we introduce an LLM-generated roadmap as high-level guidance for our proxy. 
The specific prompt and example are as follows:

\input{section/prompts/prompt_roadmap}

\subsection{Evaluation}
In our experiments, we found that traditional evaluation metrics such as Exact Match (EM) are often inaccurate, as they strictly require identical generated answers.
To address this issue, following previous work~\citep{ZhengC00WZL0LXZ23,VuI0CWWTSZLL24}, we leverage an LLM to assess answer correctness by comparing the predicted answer with the ground truth.
The specific example is as follows:

\input{section/prompts/evaluation}