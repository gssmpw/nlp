\section{Cooperative Multi-agent System}
% Inspired by human-guided alignment (Figure~\ref{fig:framework}, left), we propose a proxy-centric alignment paradigm, named \modelname, that reformulates RAG as a collaborative multi-agent system, where multiple agents cooperatively optimize the RAG pipeline without modifying the retriever or LLM. In this section, we first elaborate on the role of each agent (Section~\ref{sec:multi_agents}) and then,  introduce our tree-based credit assignment mechanism (Figure~\ref{fig:framework}, right), which effectively distribute system-level rewards to individual agents, enabling efficient multi-agent optimization in RL (Section 3.2).
In this section, we elaborate on the role of each agent (Section~\ref{sec:multi_agents}) with their collaborative strategies (Section~\ref{sec:strategy}). % along

\subsection{Multiple Agents}\label{sec:multi_agents}
Inspired by human behavior mentioned in the Introduction, we design three specialized agents—Reasoning Router, Information Filter, and Decision Maker—to facilitate communication between the retriever and the LLM, as illustrated in Figure~\ref{fig:framework}. 
These agents operate as distinct roles within a single lightweight proxy using targeted instructions, collaboratively managing various aspects of the RAG pipeline. 
This unified design ensures efficient coordination while maintaining simplicity for edge deployment. 
Formally, we define each agent as follows:

This proxy plays the role of \textbf{Reasoning Router} agent to determine the optimal reasoning strategy for a given question from a high-level perspective. 
Given the current state (the question), it selects actions using a maximum two-step operation:
\begin{enumerate}[topsep=1pt, partopsep=1pt, leftmargin=12pt, itemsep=-1pt]
    \item \textbf{Decide Retrieval Necessity}: If the agent outputs \texttt{[No Retrieval]},  the question is directly processed by the LLM, leveraging its internalized knowledge.
    \item \textbf{Determine Question Complexity}: If the agent outputs \texttt{[Retrieval]}, the agent also evaluates whether the question requires complex reasoning. 
\end{enumerate}

\textit{For simple questions}, the agent continues to generate a single \texttt{<query content>} and interacts with the retriever to obtain documents. 
The retrieved documents are then processed by the \textbf{Information Filter} agent to extract relevant, LLM-friendly content. 
Finally, the LLM uses the filtered documents to generate an answer to the question.

\textit{For complex questions}, the agent outputs \texttt{[Planning]}, which will trigger a multi-step reasoning strategy that requires coordination with multiple agents. 
Further details on this strategy will be introduced later.

% \begin{itemize}[topsep=1pt, partopsep=1pt, leftmargin=12pt, itemsep=-1pt]
%     \item \texttt{[No Retrieval]}: Direct answering strategy that leverages only the LLM's inherent knowledge.
%     \item \texttt{[Retrieval]<query content>}: Single-pass strategy that generates an query for one-time retrieval and filtering.
%     \item \texttt{[Planning]}: Multi-step reasoning strategy that coordinates iterative retrieval and reasoning processes.
% \end{itemize}

This proxy plays the role of \textbf{Information Filter} agent to process and filter retrieved information for identifying content suitable for LLMs. 
Its state space includes the question, the retrieved documents, and the current reasoning objective (if operating in \texttt{[Planning]} mode). 
Based on the given state, the agent selects an action to analyze and filter relevant documents using the following structured format:
\begin{tcolorbox}[boxsep=0mm,left=2.5mm,right=2.5mm]
Thought: $<$Analysis of each documents$>$\\
Action: [$<$Selected document IDs$>$]
\end{tcolorbox}

This proxy plays the role of \textbf{Decision Maker} agent to determine the optimal action within the \texttt{[Planning]} strategy based on the current state. 
Its state space includes the question, the LLM-generated roadmap, and the accumulated documents from the reasoning history. 
Given the current state, the agent selects an action to evaluate progress and decide the next operation, using the following structured format:
\begin{tcolorbox}[boxsep=0mm,left=2.5mm,right=2.5mm]
Thought: $<$Analysis of current progress and objective$>$\\
Action: \{\texttt{[Retrieval]<subquery content>} (Continue with retrieval-filter loop), or \texttt{[LLM]} (Pass to LLM for answering)\}% No Retrieval
\end{tcolorbox}

\subsection{Collaborative Strategy}\label{sec:strategy}

With the three specialized agents defined, we now describe how they collaborate to efficiently handle different types of questions. 
Their coordination follows a structured workflow, enabling adaptive and multi-granular information processing through various strategies, as detailed below.

The \textbf{Direct Answering Strategy} and \textbf{Single-pass Strategy} have already been introduced in the definition of the \textbf{Reasoning Router} agent, corresponding to the agent outputs \texttt{[No Retrieval]} and \texttt{[Retrieval]<query content>}, respectively.

% \textbf{Direct Answering Strategy} (\texttt{[No Retrieval]}): For questions within LLM’s knowledge scope, the Reasoning Router directly routes them to LLM without retrieval operations. 
% This strategy efficiently handles general knowledge questions by leveraging LLM’s inherent capabilities.

% \textbf{Single-pass Strategy} (\texttt{[Retrieval]}):
% For questions requiring specific external information, this strategy performs a single round of retrieval-filter process. The workflow consists of two steps: (1) the Reasoning Router first generates an optimized query for targeted retrieval; (2) the Information Filter analyzes and selects LLM-friendly documents, and passes them to LLM for answer generation.

\textbf{Multi-Step Reasoning Strategy} corresponds to the \texttt{[Planning]} output by \textbf{Reasoning Router} agent. 
Designed to address complex questions requiring a high-level roadmap from LLM and multiple retrieval-filter loops, this strategy enables iterative information gathering and reasoning through the following three phases:
\begin{enumerate}[topsep=1pt, partopsep=1pt, leftmargin=12pt, itemsep=-1pt]
    \item \textbf{Generate Roadmap}: The LLM decomposes the complex question into a structured set of subgoals, providing high-level guidance for the proxy.
    \item \textbf{Iterative Retrieval-filter Loop}: Guided by the roadmap, the \textbf{Decision Maker} evaluates the current progress, determines the next objective, and generates subqueries for the retrieval-filter loop. 
    This process is carried out in coordination with the \textbf{Information Filter} and continues iteratively until the \textbf{Decision Maker} determines that the accumulated documents contain sufficient information to address all subgoals.
    \item \textbf{Final Answer}: All accumulated information is passed to LLM for answer generation.
\end{enumerate}
Notably, generating the roadmap is the only role that is not played by the proxy in our communication pipeline; however, the LLM is invoked only once in the \texttt{[Planning]} strategy, minimizing computational overhead. 
Additionally, it is important to note that the number of retrieval-filter loops may not directly correspond to the number of subgoals, as a single retrieval might address multiple subgoals or require multiple attempts for a single subgoal.

% \textbf{Multi-Step Reasoning Strategy} (\texttt{[Planning]}):
% For complex questions requiring decomposition or multiple retrieval steps, we posit that these problems may demand deeper cognitive reasoning or span across multiple documents. Therefore, the proxy initially instructs the LLM to create a plan that encompasses multiple subgoals. 
% Based on these subgoals, the proxy generates subquery, executes an external retrieval process, and filters out irrelevant documents. 
% This process is repeated iteratively until the proxy determines that the accumulated retrieved documents contain sufficient information to address all subgoals. 
% It is important to note that the number of retrieval attempts may not correspond directly to the number of subgoals, due to the fact that once retrieval may solve multiple subgoals or ont solve even one subgoal.

Through these three strategies, our multi-agent system adaptively handles questions of varying complexity. 
The Reasoning Router automatically selects the most appropriate strategy based on the characteristics of each question: the Direct Answering Strategy provides immediate responses for general knowledge, the Single-pass Strategy efficiently retrieves information for fact-based questions, and the Multi-Step Strategy addresses complex questions through guided iterative reasoning. 
This hierarchical approach ensures optimal resource utilization by aligning computational effort with question complexity while potentially maintaining high response quality.
Next, the key focus is to optimize the proxy to learn the knowledge boundaries of the LLM and master the specific capabilities of each agent.

\section{Multi-Agent Proxy Optimization}\label{sec:tree_opt}
Since the final answer generated by the LLM is straightforward to evaluate as the system-level reward, it is intuitive to employ reinforcement learning to optimize the proxy. 
However, each agent within the proxy serves as an intermediate module, responsible for only a partial trajectory of the RAG pipeline.
This makes it difficult to define agent-level rewards~\citep{GuinetODC24}. 
For example, a high-quality generated query might still result in a low system-level reward due to poor subsequent document filtering. 
To tackle this challenge, we propose a tree-structured rollout approach for robust on-policy learning, utilizing deterministic rollout in the early stages and stochastic rollout in later stages.

% Training multiple agents in RAG pipeline presents unique challenges. The core difficulty lies in the evaluation of intermediate decisions: designing reliable local metrics for individual agents (e.g., query quality or filtering accuracy) is not only challenging but may also conflict with the ultimate goal of generating high-quality answers~\citep{GuinetODC24}.
% To address these challenges, we propose a tree-based multi-agent optimization framework that enables precise credit assignment (Section~\ref{sec:tree_credit}) and coordinated end-to-end training across all agents (Section~\ref{sec:optimization}).


% The effectiveness of our multi-agent system relies on the coordinated optimization of all agents towards the system-level objective. However, training multiple agents in the RAG pipeline presents unique challenges.
% The core difficulty lies in the evaluation and optimization of intermediate decisions: designing reliable local metrics for individual agents (e.g., query quality or filtering accuracy) is not only challenging but may also not align with, or even conflict with, the ultimate goal of generating high-quality answers~\citep{GuinetODC24}.
% For instance, a seemingly optimal query might retrieve information that is less effective for the final answer generation.
% Therefore, this section presents our end-to-end optimization of the multi-agent system. We first introduce a tree-based credit assignment mechanism that distributes system-level rewards across different agents based on their expected contribution (Section~\ref{sec:tree_credit}). Then, we detail our training process that enables coordinated optimization of multiple agents to achieve better retriever-LLM alignment (Section~\ref{sec:optimization}).
% To effectively optimize our multi-agent system, we propose a tree-based optimization framework that enables precise credit assignment and coordinated training of all agents. Our framework focuses on directly optimizing system-level performance while avoiding the pitfalls of local metrics. Specifically, we first introduce a tree-based credit assignment mechanism that distributes rewards based on agents' expected contributions (Section~\ref{sec:tree_credit}), followed by a coordinated end-to-end optimization (Section~\ref{sec:optimization}).

\subsection{Credit Assignment}\label{sec:tree_credit}
To avoid the sparse reward in traditional single trajectory rollout, we propose the tree-structured rollout for credit assignment, which distributes system-level rewards across agents to mitigate the high variance of local rewards for each agent.
The core idea is to evaluate each agent's contribution by forcing the Reasoning Router to explore all possible reasoning strategies during the rollout for each question.
% Our framework focuses on directly optimizing system-level performance while avoiding the pitfalls of local metrics.   based on their expected contribution
% This approach enables precise reward distribution while maintaining alignment with the system-level objective.

\textbf{Deterministic Rollout.} Given a question $q$, we begin by forcing the Reasoning Router to explore both \texttt{[No Retrieval]} and \texttt{[Retrieval]}. 
For the \texttt{[Retrieval]} branch, we further force the agent to explore simple reasoning by directly generating \texttt{<query content>} and complex reasoning through \texttt{[Planning]}.
As shown in Figure~\ref{fig:framework}, we deterministically construct a decision tree during the first stage of the rollout.
Current tree, with a depth of 2, enables simultaneous exploration of multiple reasoning paths, providing a comprehensive understanding of how each decision impacts the final outcome.

\textbf{Stochastic Rollout.} Once the overall reasoning strategy is confirmed, the subsequent rollout employs sampling to expand the decision tree. 
For each non-terminal node, we randomly sample $K(t)$ candidate actions from the proxy $\pi$ for the $i$-th agent at depth $t$.\footnote{The time step $t$ is reused as the depth of the tree. Since nodes are expanded layer by layer in our implementation, the depth corresponds to the time step.}
Specifically, the tree is expanded using the following children (actions):
%
\begin{align}
    \{a^i_{t,k}\}_{k=1}^{K(t)} &\sim \pi(\cdot|s^i_t, \text{instruction}_i) \\
    K(t) &= \begin{cases}
        2, & \text{if } t \leq 4 \\
        1, & \text{if } t > 4
    \end{cases}
\end{align}
%
where $\text{instruction}_i$ refers to the task-specific instruction for the $i$-th agent, and $K(t)$ represents the dynamic branching factor at depth $t$, balancing exploration and computational efficiency. 
Each sampled action $a^i_{t,k}$ triggers a state transition governed by:
%
\vspace{-0.5em}
\begin{equation}
    s^j_{t+1,k} = \mathcal{T}(s^i_t, a^i_{t,k}),
\end{equation}
%
where $j\in \mathcal{N}$ denotes the next agent in the predefined strategy (Section~\ref{sec:strategy}).\footnote{The transition function \(\mathcal{T}\) is deterministic and involves state updates. See Appendix~\ref{app:transition_details} for more details.} % , such as changes in documents
By recursively applying this sampling process until leaf nodes are reached, we construct a decision tree containing multiple trajectories $\tau$:
%
\begin{equation}
    \tau = \{(s^i_t, a^i_{t,k}, s^i_{t+1,k})\}_{i\in \mathcal{N},t,k}
\end{equation}
%
where each leaf node includes the final system-level reward ($R=1$ for success and $R=0$ for failure).

\textbf{Monte Carlo Credit Assignment.} Instead of constructing a single trajectory rollout for each question, we create a \textit{tree-structured rollout} containing multiple trajectories. 
This structure enables us to trace how individual decisions impact the system-level outcome. 
For each agent-generated node $(s^i_t, a^i_t)$, we compute its credit reward based on the expected system-level reward:
%
\begin{equation}\label{eq:mc_est}
     r_{\text{credit}}(s^i_t, a^i_t) = \mathbb{E}_{\tau \sim \pi_\theta}[R(\tau) | s^i_t,a^i_t]  \approx \frac{\sum_{l \in L(s^i_t, a^i_t)} R_l}{|L(s^i_t, a^i_t)|},
\end{equation}
%
where $L(s^i_t, a^i_t)$ denotes the set of leaf nodes reachable from $(s^i_t, a^i_t)$, and $R_l$ is the final reward of leaf node $l$.

Our proposed credit assignment mechanism offers several key advantages over a single trajectory rollout:
(1) Our rollout thoroughly explores all possible strategies for each question, generating numerous intermediate training examples for each agent. 
(2) Most importantly, while directly redistributing a single system-level reward to agent nodes in a single trajectory is challenging, our approach accurately estimates the reward of each agent node in a probabilistic expectation using the tree-structured rollout.

% This formulation effectively measures the average impact of each decision on the final system performance by considering all possible future trajectories.
% This tree-based credit assignment mechanism has several advantages:
% (1) It provides a principled way to evaluate each action's contribution by considering all possible downstream outcomes, as shown in the expectation formulation.
% (2) The credit rewards naturally align with the system-level objective since they are derived directly from the final rewards.
% (1) it naturally accounts for all possible downstream outcomes of an action, probabilistically reflecting the effectiveness of each action in terms of its contribution to successful task completion.
% (2) it ensures that the optimization objective of each agent aligns with the ultimate goal in the multi-agent system.


\subsection{Training Method}\label{sec:optimization}
For each sampled tree, we disassemble it into individual nodes and group them by their corresponding agents, as illustrated in Figure~\ref{fig:framework}.
The token sequence within each node, along with its corresponding reward computed via Eq~(\ref{eq:mc_est}), is added to the replay buffer for Proximal Policy Optimization (PPO)~\citep{SchulmanWDRK17}. 
The overall training objective follows the standard PPO framework but incorporates multi-agent aggregation. 
%
\begin{equation}
    \mathcal{L}_{\text{\modelname}} = \sum_{i\in\mathcal{N}} \mathcal{L}_{\text{PPO}}^i,
\end{equation}
%
where the loss $\mathcal{L}_{\text{PPO}}$ can represent either value loss or policy loss. 
Details of the loss functions are provided in the Appendix~\ref{app:ppo_details}.
Following the PPO recipe in \citet{Ouyang0JAWMZASR22}, the PPO for LLMs typically requires an initialization from the supervised fine-tuning model.
Therefore, we employ our tree-structured rollout with rejection sampling~\citep{rft} to collect seed data and use cross-entropy loss for the supervised warm-up phase.
