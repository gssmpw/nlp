\section{Related Work}

\textbf{Retrieval-Augmented Generation.}
Retrieval-augmented generation (RAG) has emerged as a crucial technique for enhancing LLMs' capabilities by incorporating external knowledge sources~\citep{abs-2312-10997,abs-2407-02485}.
Recent studies have highlighted the importance of aligning the retriever with the LLM to achieve superior performance~\citep{FanDNWLYCL24,RQ-RAG}.
This alignment can be approached through three main strategies: retriever fine-tuning methods~\citep{ShiMYS0LZY24}, LLM fine-tuning methods~\citep{AsaiWWSH24,abs-2406-13629,auto-rag}, and intermediate modules methods~\citep{MaGHZD23,WangLSL23,TanD0GFW24}.
However, these methods often face practical limitations, such as focusing on local optimization, the inability to align with commercial search engines, and the substantial computational costs of LLM optimization.
Different from previous work, we introduce a lightweight, proxy-centric alignment framework that implements alignment without modifying either the retriever or LLM while optimizing the entire RAG pipeline holistically.

\textbf{Multi-agent Systems.}
Multi-agent systems have recently garnered increasing attention, especially in the context of complex task-solving and decision-making~\citep{GuoCWCPCW024,pmlr-v235-zhang24au}.
A major challenge in multi-agent frameworks is credit assignment—determining each agent's contribution to the overall system performance—which becomes particularly crucial in multi-agent reinforcement learning~\citep{abs-2312-01058,ZhuDW24}.
In our work, we propose Monte Carlo credit assignment mechanism to distribute system-level rewards to each agent in the form of expectations, enabling effective coordination within agents.

% However, existing multi-agent frameworks often face challenges in appropriate credit assignment among agents, especially in multi-agent reinforcement learning~\citep{abs-2312-01058,ZhuDW24}.

% Multi-agent systems have gained increasing attention in AI research, particularly for complex task solving and decision-making \citep{multiagent1, multiagent2}. In the context of language models, recent work has explored using multiple specialized agents to decompose and solve complex tasks \citep{autogen, agent1}. These approaches typically rely on role-specific agents with predefined responsibilities, coordinated through various communication protocols. However, existing multi-agent frameworks often face challenges in efficient coordination and reward attribution among agents. Our work advances this field by implementing a lightweight multi-agent system within a single proxy model, coupled with a novel tree-based credit assignment mechanism for precise reward allocation. This design enables effective collaboration while maintaining computational efficiency and clear accountability for each agent's contributions.