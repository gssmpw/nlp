\section{Related Work}
Research has consistently shown that LLMs, such as GPT-based models, encode and perpetuate biases from their training data \cite{bender2021dangers, bolukbasi2016man}. Studies have documented biases in gendered language \cite{caliskan2017semantics}, racial and ethnic stereotypes \cite{sheng2021societal}, and disparities in sentiment and lexical framing \cite{gehman2020realtoxicityprompts}. These biases can influence LLM-driven decision-making, leading to discriminatory or unequal outcomes in domains such as hiring, healthcare, and content moderation. However, fewer studies have examined how bias in LLMs affects robot caregiving interactions, an area where fairness and inclusivity are particularly critical.

The field of HRI has increasingly focused on ensuring that robots interact ethically and equitably with diverse users \cite{lee2022configuring,seaborndiversity,seaborn2023not,winkle202315,korpan2023trust}. Prior research has explored biases in robot perception, demonstrating that users may assign gender, race, and personality traits to robots based on their design and behavior \cite{guidi2022ambivalent,bartneck2018robots}. Studies on caregiving robots have highlighted concerns that robots may reinforce stereotypes about gendered caregiving roles \cite{wessel2021gender,moradbakhti2023counter} or exhibit racial and cultural insensitivity in cross-cultural care settings \cite{vernon2024african,hundt2022robots}. One recent study intentionally incorporated participant's nationality, mental and physical condition in LLM prompts to generate dialogue in a human-robot interaction but did not consider the potential biases introduced by the LLM \cite{grassi2024enhancing}.

Recent research showed that LLM-generated responses for HRI tasks such as facial emotion expression, trust evaluation, proximity preference, and security risk evaluation were biased and discriminatory when prompted with similar labels as the ones examined here \cite{azeem2024llm}. This paper's analysis complement's their work by examining bias in LLM generation in the robot caregiving context. Existing approaches to measuring bias in LLM-generated text include lexical analysis, sentiment analysis, syntactic complexity analysis, and embedding-based similarity metrics \cite{gallegos2024bias}. In this study, we apply statistical and computational methods to analyze disparities in LLM-generated caregiving descriptions, identifying patterns of bias across demographic groups.