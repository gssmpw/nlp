\section{Related Work}
Research has consistently shown that LLMs, such as GPT-based models, encode and perpetuate biases from their training data **Bolukbasi et al., "Man is to Computer Programmer as Woman is to Homemaker?"**. Studies have documented biases in gendered language **Caliskan et al., "Semantics derived automatically from language genesis encourage bias suppression"** , racial and ethnic stereotypes **Sap et al., "Social Bias Frames in Neural Networks"** , and disparities in sentiment and lexical framing **Zhao et al., "Gender bias in word embeddings"** . These biases can influence LLM-driven decision-making, leading to discriminatory or unequal outcomes in domains such as hiring, healthcare, and content moderation. However, fewer studies have examined how bias in LLMs affects robot caregiving interactions, an area where fairness and inclusivity are particularly critical.

The field of HRI has increasingly focused on ensuring that robots interact ethically and equitably with diverse users **Kumar et al., "Human-Robot Interaction"** . Prior research has explored biases in robot perception, demonstrating that users may assign gender, race, and personality traits to robots based on their design and behavior **Bartneck et al., "Does the Robot Smile? Measuring Human-Robot Proximity through Facial Emotion Expression"** . Studies on caregiving robots have highlighted concerns that robots may reinforce stereotypes about gendered caregiving roles **Goodrich et al., "A Framework for Investigating Human-Caregiving Interactions with Domestic Service Robots"**  or exhibit racial and cultural insensitivity in cross-cultural care settings **Gockley et al., "Culturally Situated Design for Human-Robot Interaction: A Progress Report"** . One recent study intentionally incorporated participant's nationality, mental and physical condition in LLM prompts to generate dialogue in a human-robot interaction but did not consider the potential biases introduced by the LLM  **Yan et al., "Integrating Participantâ€™s Characteristics into Language Models for Human-Robot Interaction"** .

Recent research showed that LLM-generated responses for HRI tasks such as facial emotion expression, trust evaluation, proximity preference, and security risk evaluation were biased and discriminatory when prompted with similar labels as the ones examined here  **Li et al., "An Analysis of Bias in LLM-Generated Responses for Human-Robot Interaction Tasks"** . This paper's analysis complement's their work by examining bias in LLM generation in the robot caregiving context. Existing approaches to measuring bias in LLM-generated text include lexical analysis, sentiment analysis, syntactic complexity analysis, and embedding-based similarity metrics  **Gurcke et al., "Bias in Language Models"** . In this study, we apply statistical and computational methods to analyze disparities in LLM-generated caregiving descriptions, identifying patterns of bias across demographic groups.