


\section{Method: Kernel Divergence Score}
\label{sec:methods}
In this section, we present our method, {Kernel Divergence Score}, which leverages information among samples within the model's embedding space to establish a more nuanced contamination scoring mechanism. In a nutshell, we assess changes in the kernel matrix of sample embeddings before and after fine-tuning, capturing how the relationships between samples evolve as a result of fine-tuning.


Our approach is motivated by the fact that \emph{fine-tuning affects the embedding relationships involving unseen samples more significantly than those involving seen samples}. For seen samples, the model has already been exposed to similar data during pretraining, leading to minimal shifts in their embedding relationships. In contrast, unseen samples experience more pronounced changes, as the fine-tuning process adjusts the model to better align with the benchmark dataset. By quantifying these changes using the Kernel Divergence Score, we can provide a reliable and granular measure of dataset contamination.

\noindent\textbf{Kernel similarity matrix.}
A kernel similarity matrix captures the relationships among data samples, providing fine-grained information on their distribution. Formally, let $Z\in \mathbb{R}^{n\times d}$ represent the embeddings of $n$ samples in the dataset $\mathcal{D}$, where $Z_i \in \mathbb{R}^{1\times d}$ is the normalized embedding of $i$-th sample extracted from the pre-trained LLM $\mathcal{M}$. We define the kernel matrix $\Phi(Z) \in \mathbb{R}^{n\times n}$ based on the Radial Basis Function (RBF) kernel:
\begin{equation*}
    \begin{split}
        \Phi(Z)_{i,j} &= \text{exp}(-\gamma||Z_i - Z_j||_2^2),
    \end{split}
\end{equation*}
where $\Phi(Z)_{i,j}$ is the kernel similarity between samples $Z_i$ and $Z_j$, and $\gamma$ controls the kernel bandwidth.  The kernel matrix captures the pairwise relationships between all samples in the dataset, with values ranging from 0 to 1. An entry close to 1 indicates high similarity (small distance), while a value close to 0 indicates low similarity (large distance). The matrix $\Phi(Z)$ is both symmetric and positive semidefinite. 

\input{B_Figures/kernel_decomp}

\noindent\textbf{Leveraging the effect of fine-tuning.} Our proposed Kernel Divergence Score is based on the kernel matrix before and after fine-tuning. 
% Inspired by the work of \citet{zhang2024fine}, we utilize the impact of supervised fine-tuning in our scoring approach.
%Specifically, we assess the change in the kernel matrix before and after fine-tuning.
%Our intuition is that training will affect the relationships involving unseen samples more significantly than those involving seen samples.
Formally, let $Z' \in \mathbb{R}^{n \times d}$ represent the embeddings \emph{after} supervised fine-tuning on dataset $\mathcal{D}$, where $Z_i' \in \mathbb{R}^{1 \times d}$.
%Both sets of embeddings are normalized to mitigate the effect of embedding magnitude.
Accordingly, we can derive the kernel similarity matrix as:
\begin{equation}
\label{eq:rbf}
    \begin{split}
        \Phi(Z')_{i,j} &= \text{exp}(-\gamma||Z_i' - Z_j'||_2^2).
    \end{split}
\end{equation}
Then, we define \textbf{Kernel Divergence} as
\begin{equation}
    \frac{1}{E} \sum_{i,j=1}^n  \bigg\vert\Phi(Z)_{i,j} \log \frac{\Phi(Z)_{i,j}}{\Phi(Z')_{i,j}} \bigg\vert,
    \label{eq:score}
\end{equation}
where $E = \sqrt{\sum_{i,j} \Phi(Z)_{i,j}}$ is a normalizer.
%The rationale for this design choice will be further discussed in Section~\ref{sec:component}.
When $\gamma=1$, our score in Eq.~\eqref{eq:score} can be equivalently written as
\begin{equation}
\label{eq:score_decomp}
    \frac{1}{E} \sum_{i,j=1}^n \underbrace{\text{exp}(-||Z_i - Z_j||_2^2)}_{\substack{\text{(1) Soft gating for originally} \\ \text{closely-related samples}}} \; \underbrace{\bigg\vert ||Z_i' - Z_j'||_2^2 - ||Z_i - Z_j||_2^2 \bigg\vert}_{\text{(2) Change in distance before and after SFT}}.
\end{equation}
\paragraph{Interpretation of kernel divergence.} This function quantifies how fine-tuning changes the pairwise distances between samples, weighted by their original similarity. Specifically, the second term $\big\vert||Z_i' - Z_j'||_2^2 - ||Z_i - Z_j||_2^2\big\vert$ measures the absolute change in the squared Euclidean distance between embedding pairs caused by fine-tuning. For unseen samples, fine-tuning tends to create new meaningful relationships or significantly alter their embeddings, making their contribution to the score larger. 
%The function can be viewed as a soft counter which aggregates the effect of supervised fine-tuning~(SFT) for each pair of samples that were initially closely related.
The first exponential term acts as a soft gating function, assigning a higher weight to pairs of samples that were originally closer to each other. By incorporating this term, the score prioritizes the impact of fine-tuning on pairs that were initially similar, highlighting cases where fine-tuning induces significant changes in their relationships. Overall, a larger fraction of unseen examples (or smaller $\lambda$) can elevate the kernel divergence more significantly. Because we want the scoring function $S(\mathcal{D},\mathcal{M})$ to be positively correlated with the contamination rate $\lambda$, we define our final scoring function to be the negation of kernel divergence:
\begin{equation}
\label{eq:final_score}
S(\mathcal{D}, \mathcal{M}) = -  \frac{1}{E} \sum_{i,j=1}^n  \bigg\vert\Phi(Z)_{i,j} \log \frac{\Phi(Z)_{i,j}}{\Phi(Z')_{i,j}} \bigg\vert,
\end{equation}
which is expected to be larger as the contamination $\lambda$ grows.
%Presumably, as shown in our prior observation that unseen samples tend to cluster together, kernel entries of unseen sample pairs will be greater compared to other pair entries.


\noindent\textbf{Visual demonstration of score components.}
To provide a concrete understanding of the role of each component in our Kernel Divergence Score~(Eq.~\eqref{eq:score} or Eq.~\eqref{eq:score_decomp}), we present the kernel matrix for each component in Figure~\ref{fig:k_decomp}. 
We use $n=100$ samples with a contamination rate $\lambda=0.4$, meaning 40\% of the samples are seen during model pre-training. The left panel illustrates the soft gate from Eq.~\eqref{eq:score_decomp}, or the kernel similarity matrix using the pre-trained embedding. The middle panel captures the change of pairwise embedding distance after supervised fine-tuning, and the right panel shows the resulting element-wise score matrix after the Hadamard product of the two.
As hypothesized, the middle panel suggests that relationships involving unseen samples are more significantly altered by fine-tuning. By multiplying the soft gate~(left panel), unseen sample pairs contribute more to the overall score. Additional examples with varying contamination rates are provided in Appendix~\ref{apdx:more_kernels}.