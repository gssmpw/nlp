\section*{Impact Statement}
The broader impact of this work lies in its potential to significantly improve the reliability, transparency, and fairness of large language model evaluation. By enabling the identification and quantification of contaminated datasets, our approach ensures that reported performance metrics are more trustworthy and reflective of a model's true generalization capabilities. This contributes to a more rigorous benchmarking process, fostering fair and meaningful comparisons across different models and architectures.
Furthermore, the insights gained from this work can inform better practices for dataset curation. This not only reduces the risk of inflated performance results but also enhances the utility of benchmarks as tools for guiding research and development.




\section*{Acknowledgement}
The authors would like to thank Shawn Im and Seongheon Park for their valuable comments on the manuscript. This work is supported by the AFOSR Young Investigator Program under award number FA9550-23-1-0184, National Science Foundation (NSF) Award No. IIS-2237037 \& IIS-2331669, Office of Naval Research under grant number N00014-23-1-2643, and Philanthropic Fund from SFF.