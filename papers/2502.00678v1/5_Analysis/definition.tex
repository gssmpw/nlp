
\noindent\textbf{Definition 3.} \textit{(\textbf{Fine-tuned Score Deviation}) is the difference of scores before and after supervised fine-tuning, averaged across samples:}
\begin{equation}
    \frac{1}{n}\sum_{i=1}^n S(\mathbf{x}_i; \theta) - S(\mathbf{x}_i; \theta'),
\end{equation}
\textit{where $\mathbf{x}_i$ is the $i$-th sample in the dataset, $S(\cdot;\cdot)$ is an existing scoring function~(e.g., Min-K\% or Perplexity Score), and $\theta, \theta'$ are models before and after fine-tuning, respectively.}~\cite{zhang2024fine}


\input{B_Figures/req2}
In comparison to the detector-based scoring methods, we evaluate our Kernel Divergence Score, in addition to alternative scoring approaches that alsp leverage the fine-grained information of kernels.
In all their definitions below, let $Z, Z' \in \mathbb{R}^{n \times d}$ be the embeddings before and after supervised fine-tuning, respectively, where $Z_i, Z_i' \in \mathbb{R}^{1 \times d}$ are row vectors.

\noindent\textbf{Definition 5.} \textit{(\textbf{KDE KL Divergence}) is the Kullback-Leibler divergence of the Kernel Density Estimator~(KDE):}
\begin{equation}
    \text{KLD}\bigg(\frac{1}{E}\sum_{i=1}^n\Phi(Z)_{i,j} \; \bigg\vert \; \frac{1}{E'}\sum_{i=1}^n\Phi(Z')_{i,j}\bigg),
\end{equation}
\textit{where $\Phi(\cdot)$ is the RBF kernel, $Z, Z'$ are normalized model embeddings, $E = \sum_{i,j} \Phi(Z)_{i,j}$ and $E' = \sum_{i,j} \Phi(Z')_{i,j}$.}

\noindent\textbf{Definition 6.} \textit{(\textbf{Maximum Mean Discrepancy$^2$}) is the squared value of the maximum mean discrepancy~(MMD):}
\begin{equation}
    \text{MMD}(Z, Z') = \sup_{f \in \mathcal{H}}\bigg(\mathbb{E}_{Z}[f(Z)] - \mathbb{E}_{Z'}[f(Z')]\bigg),
\end{equation}
\textit{where $\mathcal{H}$ is the reproducing kernel Hilbert space~(RKHS). In practice, }
\begin{equation}
    \mathbb{E}_Z[k(Z_i, Z_i)] + \mathbb{E}_{Z'}[k(Z_i', Z_i')] - 2 \mathbb{E}_{Z,Z'}[k(Z_i, Z_i')]
\end{equation}
\textit{is computed as the MMD$^2$, where $k$ is the kernel function.}~\cite{gretton2012kernel}


\noindent\textbf{Definition 7.} \textit{(\textbf{Centered Kernel Alignment}) is defined as}
\begin{equation}
    1 - \frac{||Z^\top Z'||_\text{F}^2}{||Z^\top Z||_\text{F} ||Z'^\top Z'||_\text{F}},
\end{equation}
\textit{where $Z, Z'$ are zero-centered embeddings.}~\cite{kornblith2019similarity}



\noindent\textbf{Definition 8.} \textit{(\textbf{Dot-product Kernel MSE}) is the mean squared error of paired dot-product kernels:}
\begin{equation}
    \frac{1}{n^2}||Z Z^\top - Z' Z'^\top||_\text{F}^2,
\end{equation}
\textit{where $n$ is the number of samples.}

\noindent\textbf{Definition 9.} \textit{(\textbf{Cosine Similarity Kernel MSE}) is the mean squared error of paired cosine similarity kernels.}
\begin{equation}
    \frac{1}{n^2}\bigg\Vert\frac{Z Z^\top}{||Z||_2^2} - \frac{Z' Z'^\top}{||Z'||_2^2}\bigg\Vert_\text{F}^2,
\end{equation}
\textit{where $n$ is the number of samples.}


\noindent\textbf{Definition 10.} \textit{(\textbf{RBF Kernel MSE}) is the mean squared error of paired radial basis function~(RBF) kernels.}
\begin{equation}
    \frac{1}{n^2}||\Phi(Z) - \Phi(Z')||_\text{F}^2,
\end{equation}
\textit{where $n$ is the number of samples, $\Phi$ is the RBF kernel, and $Z, Z'$ are normalized model embeddings.}

We subtract the original Centered Kernel Alignment~(CKA) value from 1 to ensure that the measure's trend aligns with the contamination rate trends observed in other kernel-based baselines.
Note that this does not affect the general property of the measure.
Further implementation details and hyperparameters for each baseline are provided in Appendix~\ref{}.
