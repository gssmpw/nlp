



\section{Ablation Studies}
\label{sec:analysis}



In this section, we conduct an in-depth ablation to understand various design choices of our kernel divergence score. 
\input{5_Analysis/ablation}
\input{5_Analysis/design_choices}




\section{Discussions}
\label{sec:discussion}
\input{5_Analysis/discussions}

\paragraph{Temporal shift problems of MIA benchmarks.}
Recent studies have expressed concerns regarding the temporal shift issues in existing Membership Inference Attack~(MIA) benchmarks~\cite{duan2024membership,das2024blind,maini2024llm}.
Notably, datasets such as WikiMIA, BookMIA, and ArxivTection have been identified as susceptible to temporal cues, which can inadvertently simplify the membership inference task.
This simplification arises because models can exploit temporal information to distinguish between seen versus unseen data, leading to a potential overestimation of detection performance.

To ensure the robustness of our approach and mitigate potential biases introduced by temporal shifts, we conducted evaluations using 500 samples from six subsets of the Pile dataset~\cite{gao2020pile}. 
The subsets include text data from various sources, including expository proses~(Wikipedia), academic papers~(PhilPapers), emails~(Enron), news articles~(HackerNews),  web-scraped data~(Pile-CC), and user-contributed questions and answers~(StackExchange).
For each subset, the `train' set is regarded as seen data, while the `val' set serves as unseen data.
We mix these two sets according to varying contamination rates to assess our model's performance under different conditions. 
This methodology provides a rigorous assessment, ensuring that our model does not exploit temporal cues. 
As presented in Table~\ref{tab:pile}, the Spearman and Pearson correlation coefficients are both near 1.0, averaging at 0.944 and 0.948, respectively. 
These findings demonstrate that our method reliably scores contamination levels without relying on temporal shifts.


\input{A_Tables/pile}
\input{A_Tables/other_models}



\paragraph{Extension to various model families.}
We extend our evaluation to diverse models to demonstrate the versatility of our approach. 
As presented in Table~\ref{tab:other_models}, our approach consistently exhibits near-perfect correlation values across all models tested. 
These findings underscore the robustness and applicability of our method across diverse model families.


\paragraph{Computational cost.}

Our kernel divergence score  involves a fine-tuning step to obtain two kernel matrices, followed by the computation of our scoring function.
Given a dataset with $N$ samples, the fine-tuning step operates with a time complexity of $O(c_1\cdot N)$, where $c_1$ is a constant influenced by factors such as average sample length, batch size, and model dimension.
The computation of the KDS score has complexity $O(c_2\cdot N^2)$, due to the quadratic nature of kernel matrix operations.
In practice, the latency overhead caused by scoring is minimal, as these operations are highly optimized through vectorized computations.
As demonstrated in Table~\ref{tab:time}, the latency measured in seconds for each dataset confirms the efficiency and scalability of our approach. In Appendix~\ref{apdx:inthewild}, we confirm that our method can be applied to real-world benchmark datasets, where we employ our method across 11 diverse and widely used benchmarks, most of which have sizes around a few hundred to thousand samples. 

\input{A_Tables/time}