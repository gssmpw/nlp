
\noindent\textbf{What's the impact of fine-tuning on the score?}
As described in Eq.~\eqref{eq:score_decomp}, the Kernel Divergence Score comprises two key components: \textbf{(1)} the kernel similarity matrix $\Phi(Z)$, which serves as a soft gating mechanism, and \textbf{(2)} the change in pairwise distance, which captures the effects of supervised fine-tuning. 
The roles of these two components are qualitatively illustrated in the left and middle panels of Figure~\ref{fig:k_decomp}.
To further elucidate their individual contributions, we conducted an ablation study, with results summarized in Table~\ref{tab:ablation} (top). The study evaluates the impact of removing each component. 

Specifically, ablating component (1) involves omitting the soft gating mechanism, thereby utilizing the average of $\big\vert||Z_i' - Z_j'||_2^2 - ||Z_i - Z_j||_2^2\big\vert$ as the contamination score. 
Conversely, ablating component (2) is equivalent to disregarding the effects of supervised fine-tuning, relying solely on the kernel similarity matrix $\Phi(Z)$ before fine-tuning, with the average kernel entry value serving as the contamination score.
The results indicate that removing the soft gating mechanism (Component 1) leads to a marginal decline in performance. 
This suggests that while the gating mechanism enhances the score's reliability, the majority of the information is derived from the differential effects observed before and after fine-tuning. 
% On the other hand, eliminating the supervised fine-tuning component significantly may have random effects, depending on 
Indeed, removing the supervised fine-tuning component (Component 2) significantly degrades the performance, highlighting the critical role of fine-tuning in amplifying the kernel's ability to measure dataset contamination levels.

\input{A_Tables/ablation}



\paragraph{Our method is not sensitive to the choice of kernel function.}
The Kernel Divergence Score employs the RBF kernel to compute differences after supervised fine-tuning.
However, an important question arises: how robust is the scoring performance to variations in the kernel function?
To address this, we evaluate our method using alternative kernel functions, including Euclidean pairwise distance, cosine similarity, and dot-product similarity, as shown in Table~\ref{tab:ablation} (bottom).
For the Euclidean pairwise distance and cosine similarity kernels, we replace $\Phi(Z)$ in Eq.~\eqref{eq:final_score} with the respective kernel computations.
We add 1 to the cosine similarity kernel matrix to enforce non-negative entries.
For the dot-product similarity kernel, we compute the mean squared error of the kernel matrices before and after fine-tuning, as the kernel entries may take negative values, making them incompatible with the logarithmic operation used in our scoring function.

The results indicate that scoring performance remains consistent across different kernel functions. 
This suggests that the effectiveness of the Kernel Divergence Score lies in its ability to leverage structural information from kernel representations, rather than being dependent on the specific choice of the kernel. 
These findings highlight the versatility of our kernel-based approach in quantifying contamination levels across diverse settings.




\paragraph{How does the kernel bandwidth $\gamma$ impact the performance?} 
In an RBF kernel, the bandwidth parameter $\gamma$ controls the sharpness of the kernel's entry distribution.
In our approach, we employed the median  heuristic~\cite{garreau2017large}, which sets $\gamma$ as the inverse of the median pairwise distance.
To examine the effect of $\gamma$ on contamination scoring,  we evaluate different bandwidth values $\{0.001, 0.01, 0.1, 1.0, 10.0\}$, as shown in Table~\ref{tab:gamma}.
The results indicate that scoring performance is largely invariant to the choice of $\gamma$.
This behavior is expected, considering that Eq.~\eqref{eq:score_decomp} with arbitrary $\gamma$ is 
\begin{equation}
    \frac{1}{E} \sum_{i,j=1}^n \, \underbrace{\gamma \;\text{exp}(-u_{i,j})^\gamma}_{\text{monotonicity preserved}} \; \big\vert u_{i,j}' - u_{i,j} \big\vert,
\end{equation}
where $u_{i,j} = ||Z_i - Z_j||_2^2$ and $u_{i,j}' = ||Z_i' - Z_j'||_2^2$.
The effect of $\gamma$ is limited to a constant multiplicative factor on the overall scores, and to a power factor that controls the sharpness of the soft gate.
This does not influence their relative trends across varying contamination rates.
This invariance underscores the robustness of our Kernel Divergence Score to the choice of bandwidth parameter.
However, setting an excessively large  value~(\textit{e.g.} $\gamma = 10.0$) may cause numerical errors and degrade scoring performance.
\input{A_Tables/gamma}


\input{B_Figures/location}
\paragraph{What's the impact of embedding extraction location?}
In our main experiments, we use the output embeddings from the final layer of the LLM to compute KDS. 
To further analyze the impact of embedding location, we evaluate the Spearman and Pearson correlation using embeddings extracted from different layers of the model, as shown in Figure~\ref{fig:location} (top).
Our results reveal that the strongest correlation is observed in the latter layers, indicating that these layers contain the most information relevant to dataset contamination. This suggests that the latter layers of the LLM, which are typically fine-tuned to align with specific tasks or datasets, are more sensitive to the effects of contamination compared to earlier layers.
%Furthermore, Figure~\ref{fig:location} (bottom) shows that the average coefficient of variation tends to diminish towards the final layers.
These findings support the selection of final layer embeddings for kernel computation, as they provide an informative basis for assessing dataset contamination.




\paragraph{How does SFT configuration impact the performance?}
Table~\ref{tab:training_config} presents the Spearman and Pearson correlation coefficients for contamination scores under different SFT training configurations on the WikiMIA dataset. 
The configurations vary in terms of the optimization method (Stochastic Gradient Descent vs. Batch Gradient Descent) and the number of fine-tuning epochs (1 vs. 4).
The results show that stochastic GD significantly outperforms batch GD, suggesting that the finer-grained updates introduced by SGD enhance the sensitivity of the Kernel Divergence Score to dataset contamination. 
On the other hand, increasing the number of fine-tuning epochs does not necessarily improve scoring performance.
This may be attributed to the repeated exposure of the model to the same samples during training, which could obscure the distinction between seen and unseen samples. 
Overall, training with one epoch using SGD leads to the best performance. 

\input{A_Tables/training_config}
