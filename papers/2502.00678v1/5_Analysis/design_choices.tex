% \subsection{Design Choices}
% \label{sec:design_choices}



% We analyze the impact of design choices: (1) embedding extraction location, and (2) SFT configurations.
% % In this section, we demonstrate what effects these design choices have.





% All configurations exhibit near-perfect Spearman and Pearson correlations, indicating a robust monotonic relationship between the contamination scores and the contamination ratios $\lambda$. 
% This highlights the reliability of the Kernel Divergence Score across varying training setups. The highest correlations are observed with stochastic gradient descent. 
% This suggests that SGD introduces finer-grained updates that enhance the sensitivity of the Kernel Divergence Score to dataset contamination. 
% Overall, these results underscore the effectiveness of our method across different training configurations for reliable quantification of contamination.

%Our Kernel Divergence Score utilizes the difference between kernels before and after supervised fine-tuning. 
%To eliminate the influence of the data ordering presented during a single epoch of stochastic gradient descent (SGD), we accumulated gradients for all samples and updated the model once per epoch. 
% This approach ensures fair comparisons across different baselines and multiple runs.
% In Table~\ref{tab:training_config}, we demonstrate the impact of various training configurations.
% Increasing the number of parameter updates via SGD may lead to improved compliance with the Coherence requirement.
% Interestingly, however, multiple epochs of batch gradient descent may degrade performance.
% One possible explanation is that multiple epochs make the boundary between seen and unseen samples less distinct.
% \textcolor{blue}{it's better to repeat the experiments multiple times and report the mean and std, for the results to be reliable. }


% \textcolor{blue}{can go into appendix if we run out of space}



% \paragraph{What's the role of the normalization factor?}
% Recall that in our Kernel Divergence Score, we define the normalizer $E$ as the square root of the sum of entries in the kernel matrix.
% We employ this square-root normalizer because, despite the second-order nature of Eq.~\eqref{eq:score_decomp}, the sum of kernel entries reveals a linear relationship with varying data subset sizes, as shown in Figure~\ref{fig:normalizer} (Appendix).
% A linear fit to the data yields an $R^2$ value of 0.9766, confirming the linearity of this relationship. 
% Therefore, to mitigate the influence of dataset size and prevent over-penalizing the score scale, we utilize the square-root normalizer.
% \textcolor{blue}{This doesn't seem to be as critical as other ablations. Consider moving to the appendix if we run out of space.}
% \FC{Will be important if we discuss dataset size, as the normalizer is the key to consistent score magnitude across datasets. But if we're not discussing dataset size, this is not very important.}