\section{Introduction}
\label{sec:introduction}
When a large language model (LLM) performs remarkably well on a benchmark, can you confidently attribute its success to true generalization---or is it simply a reflection of what the model has already seen during pre-training? The reality is, we often donâ€™t know. Beneath the surface of those impressive performance scores lies a critical vulnerability: \emph{dataset contamination}, a phenomenon where evaluation datasets overlap with the pretraining data of the model~\cite{golchintime}. This overlap artificially inflates reported performance metrics, obscures true generalization capabilities, and raises critical concerns about the reliability of benchmark evaluations. This brings us to a pressing and underexplored question: \emph{\textbf{How can we quantify the degree of dataset contamination}?} 

Addressing this question is crucial to ensuring that performance evaluations genuinely reflect a model's ability to generalize to unseen data, rather than benefiting from overlap with pretraining data. To formalize the problem, we aim to develop a scoring function $S: (\mathcal{D}, \mathcal{M}) \rightarrow \mathbb{R},$ that takes a benchmark dataset $\mathcal{D}$ as input and produces a score indicative of its relative contamination level with respect to the given model $\mathcal{M}$. A higher score corresponds to a greater contamination level. Such a score is valuable because researchers can use it to rank multiple benchmarks and prioritize the less contaminated ones, enabling more informed comparisons and reliable evaluation. For the score to be reliable, the scoring function must satisfy two essential properties:  \emph{monotonicity}, which ensures that the score exhibits a positive correlation with the contamination level, and \emph{consistency}, which means that the score remains stable across independently sampled subsets with the same contamination rate.

\input{B_Figures/intro}

To quantify dataset contamination, we introduce the \textbf{Kernel Divergence Score} (KDS), which computes the divergence of the kernel similarity matrix of sample embeddings before and after fine-tuning on the benchmark dataset. By analyzing changes in the kernel similarity matrix, KDS captures how fine-tuning reshapes the embeddings for seen and unseen data, providing a more holistic and nuanced perspective on dataset contamination.
%Unlike traditional methods that rely on instance-level properties, KDS leverages the relational information embedded within the dataset, capturing the impact of fine-tuning on kernel entries that encode inter-sample relationships. 
This approach is motivated by the fact that fine-tuning has a more significant effect on the embeddings of unseen samples, which the model must adapt to, while seen samples exhibit minimal changes due to prior exposure during pre-training. Furthermore, as the proportion of unseen samples increases, their cumulative effect on the kernel divergence score becomes more pronounced. By quantifying these changes, KDS can provide a reliable and interpretable measure of dataset contamination, with scores that proportionally reflect the level of contamination. 


To evaluate KDS, we perform extensive experiments, systematically controlling contamination ratios across multiple datasets.  
Our results demonstrate that KDS achieves near-perfect correlation with contamination levels, generally outperforming existing baselines across multiple datasets.
Additionally, we show that KDS is robust to design choices, including kernel functions, kernel bandwidth, and the extraction location of embeddings.
Overall, KDS provides stable scores across diverse scenarios, enabling researchers to reliably identify benchmarks based on contamination levels. We summarize our {contributions} as follows: 


\vspace{-2mm}
\begin{itemize}[leftmargin=*]
    \item We propose \textit{Kernel Divergence Score}, a reliable dataset-level scoring function for quantifying benchmark contamination. To the best of our knowledge, we are the first to leverage the fine-grained information of kernels for scoring contamination levels.
    \item We validate Kernel Divergence Score through extensive experiments on controlled contamination scenarios, showing strong performance over existing baselines. 
    \item We perform comprehensive ablations to analyze the impact of various design choices. Further practical discussions are presented, providing deeper insights into our kernel-based approach.
\end{itemize}
