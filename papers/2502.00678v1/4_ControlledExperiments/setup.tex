\subsection{Experimental Setup}
\label{sec:es}

\input{A_Tables/req1}


\paragraph{Dataset and model.}
Our controlled experiment aims to evaluate the extent to which each method satisfies the Monotonicity and Consistency requirements. 
For this purpose, we utilize three popular pre-training data detection benchmarks, WikiMIA~\cite{shidetecting}, BookMIA~\cite{shidetecting}, and ArxivTection~\cite{duarte2024decop}, each comprising samples labeled as `seen' or `unseen'.
Among the models compatible with the datasets, we select Mistral-7B-Instruct-v0.2~\cite{jiang2023mistral} for our main evaluation. We present additional results for other model families in Section~\ref{sec:discussion}.


\paragraph{Baselines.}
We consider various baseline methods for comprehensive evaluation.
Specifically, we consider Zlib~\cite{carlini2021extracting}, Perplexity Score~\cite{li2023estimating}, Min-K\%~\cite{shidetecting}, Min-K\%++~\cite{zhang2024min}, Fine-tuned Score Deviation~(FSD;~\citet{zhang2024fine}), which evaluate the likelihood of exposure for every sample independently. The overall contamination score of the dataset $S(\mathcal{D}, \mathcal{M})$ is then quantified by averaging these instance-wise scores. In addition, we consider the dataset-level approach that assesses the contamination of a dataset as a whole by examining statistical or distributional patterns that differentiate seen vs unseen datasets. This approach provides a more holistic view of contamination, capturing aggregate characteristics that are not discernible at the individual example level. Specifically, we consider the Sharded Rank Comparison Test~(SRCT; \citet{orenproving}), the latest dataset-level detection method that identifies datasets showing significant variability in likelihood values across different sample orderings.
For each baseline, we adjusted their score sign so that higher scores indicate more contamination~(\textit{i.e.}, bigger $\lambda).$ We include the detailed definition of each baseline in Appendix~\ref{app:baseline}.


\paragraph{Experimental details.}
For each dataset, we evaluate scoring performances on different contamination rates.
For integrity across experiments, we fix the data subset size to 700 for WikiMIA and ArxivTection, and 4000 for BookMIA.
Then, we run each dataset five times for robust evaluation, each with differently sampled subsets.
For methods that require fine-tuning~(\textit{e.g} our Kernel Divergence Score and Fine-tuned Score Deviation~\cite{zhang2024fine}), we train the model for 1 epoch using stochastic gradient descent with a batch size of 4.
Furthermore, in determining the bandwidth parameter $\gamma$ in the RBF kernel~(Eq.~\eqref{eq:rbf}), we utilize the median heuristic~\cite{garreau2017large}.
Further implementation details are in Appendix~\ref{apdx:details}.