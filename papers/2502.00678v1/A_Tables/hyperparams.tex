
\begin{table}[h]
    \centering
    \setlength{\tabcolsep}{8pt}
    \caption{\textbf{Supervised Fine-tuning Configurations and Hyperparameters.}}
\vspace{3mm}
\resizebox{0.6\linewidth}{!}{
\begin{tabular}{l | c c c}
\toprule
 \textbf{Hyperparameter} & WikiMIA & BookMIA & ArxivTection \\
\midrule
    LoRA Dimension & \multicolumn{3}{c}{8} \\
    LoRA $\alpha$ & \multicolumn{3}{c}{32} \\
    LoRA Dropout & \multicolumn{3}{c}{0.1} \\
    LoRA Target Modules & \multicolumn{3}{c}{query, value projection layers} \\
    SGD Learning Rate & \multicolumn{3}{c}{0.0001} \\
    Batch GD Learning Rate & \multicolumn{3}{c}{0.01} \\
    Batch Size & \multicolumn{3}{c}{4} \\
\midrule
    Dataset Size & 700 & 4000 & 700 \\
\bottomrule
\end{tabular}
}
\label{tab:hyperparam}
\end{table}
