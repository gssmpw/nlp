\section{Further Experimental Details}
\label{apdx:details}
In this section, we append further experimental details and provide formal definitions of the baselines evaluated in the manuscript.

\subsection{Implementation Details}
For supervised fine-tuning, we utilize Low-rank Adaptation~\cite{hulora}.
In Table~\ref{tab:hyperparam}, we disclose detailed LoRA configurations and other training hyperparameters used for supervised fine-tuning.

\input{A_Tables/hyperparams}


\subsection{Baseline Definitions}
\label{app:baseline}
Here, we provide formal definitions for each baseline compared in Table~\ref{tab:req1}.


\noindent\textbf{Definition 1.} \textit{(\textbf{Zlib Score}) is the negated ratio of the log perplexity and the zlib compression size:}
\begin{equation}
    -\frac{1}{n}\sum_{i=1}^n \frac{ -\frac{1}{|\mathcal{T}_i|}\sum_{x_j \in \mathcal{T}_i} \log P_\theta(x_j | x_{<j})}{\text{Zlib}(\mathbf{x}_i).\text{size}},
\end{equation}
\textit{where $\mathcal{T}_i$ is the set of tokens from sample $i$.}~\cite{carlini2021extracting}

\noindent\textbf{Definition 2.} \textit{(\textbf{Perplexity Score}) is the negated average perplexity across samples:}
\begin{equation}
    -\frac{1}{n}\sum_{i=1}^n \text{exp}\bigg(-\frac{1}{|\mathcal{T}_i|}\sum_{x_j \in \mathcal{T}_i} \log P_\theta(x_j | x_{<j})\bigg),
\end{equation}
\textit{where $\mathcal{T}_i$ is the set of tokens from sample $i$.}~\cite{li2023estimating}


\noindent\textbf{Definition 3.} \textit{(\textbf{Min-K\% Score}) is the negated mean probability from bottom-$k\%$ tokens averaged across samples:}
\begin{equation}
    -\frac{1}{n \cdot |\mathcal{K}_i|}\sum_{i=1}^n \sum_{x_j \in \mathcal{K}_i} \log P_\theta(x_j | x_{<j}),
\end{equation}
\textit{where $\mathcal{K}_i$ is the set of bottom-$k\%$ tokens from sample $i$.}~\cite{shidetecting}

\noindent\textbf{Definition 4.} \textit{(\textbf{Min-K\%++ Score}) is the negated mean normalized probability from bottom-$k\%$ tokens averaged across samples:}
\begin{equation}
    -\frac{1}{n \cdot |\mathcal{K}_i|}\sum_{i=1}^n \sum_{x_j \in \mathcal{K}_i} \frac{\log P_\theta(x_j | x_{<j}) - \mu_{x_{<j}}}{\sigma_{x_{<j}}},
\end{equation}
\textit{where $\mathcal{K}_i$ is the set of bottom-$k\%$ tokens from sample $i$, $\mu_{x_{<j}} = \mathbb{E}_{z\sim p(\cdot | x_{<j})} [\log p(z | x_{<j})]$ is the expected log probability over the vocabulary of the model, and $\sigma_{x_{<j}} = \sqrt{\mathbb{E}_{z\sim p(\cdot | x_{<j})} [(\log p(z | x_{<j}) - \mu_{x_{<j}})^2]}$ is the standard deviation.}~\cite{zhang2024min}

Following the general guideline from \citet{shidetecting}, we take the bottom 20\% tokens for the Min-K\% Score and Min-K\%++ Score.

\noindent\textbf{Definition 5.} \textit{(\textbf{Fine-tuned Score Deviation}) is the difference of scores before and after supervised fine-tuning, averaged across samples:}
\begin{equation}
    \frac{1}{n}\sum_{i=1}^n S(\mathbf{x}_i; \theta) - S(\mathbf{x}_i; \theta'),
\end{equation}
\textit{where $\mathbf{x}_i$ is the $i$-th sample in the dataset, $S(\cdot;\cdot)$ is an existing scoring function~(e.g., Min-K\% or Perplexity Score), and $\theta, \theta'$ are models before and after fine-tuning, respectively.}~\cite{zhang2024fine}


\noindent\textbf{Definition 6.} \textit{(\textbf{Sharded Rank Comparison Test}) is the difference between the log likelihood of the canonical dataset sample ordering from the mean over shuffled sample orderings, averaged across dataset shards:}
\begin{equation}
    \frac{1}{r} \sum_{k=1}^r \bigg[ \log P([x_i^{(k)}]_{i=1}^n) - \frac{1}{|\frak{S}|} \sum_{\sigma \in \frak{S}} \log P([x_{\sigma(i)}^{(k)}]_{i=1}^n) \bigg],
\end{equation}
\textit{where $r$ is the number of shards, $\frak{S}$ is the set of sample permutations, and $[x_i^{(k)}]_{i=1}^n$ is the sequence of samples $x_1, x_2, \ldots, x_n$ in $k$-th shard of the dataset.}~\cite{orenproving}