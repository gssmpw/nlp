\section{A Comprehensive Benchmark Leakage Evaluation In-the-wild}
\label{apdx:inthewild}
In this section, we provide an extensive evaluation result across large language model and benchmark dataset pairs, based on our Kernel Divergence Score.

\input{A_Tables/inthewild}
\subsection{Benchmark Dataset Details}
Here, we list the full names of the benchmark dataset code in Table~\ref{tab:inthewild}.
We also provide dataset details and the specific splits used for evaluation.

\begin{itemize}[leftmargin=*]
    \item \textbf{BlQ} is the BoolQ dataset~\cite{clark2019boolq}. It contains binary-choice questions asking about the given passage. We evaluated the validation split that consists of 3270 samples.
    \item \textbf{HSg} is the HellaSwag dataset~\cite{zellers2019hellaswag}. It is a commonsense natural language inference benchmark asking the model to choose an option that best finishes the given sentence. We evaluated the validation split that consists of 10042 samples.
    \item \textbf{OBQ} is the OpenBookQA dataset~\cite{OpenBookQA2018}. It is a 4-way multiple-choice question answering benchmark that requires multi-step reasoning, use of additional common and commonsense knowledge, and rich text comprehension. We evaluated the test split of the `main' subset, which consists of 500 samples.
    \item \textbf{MNLI} is the Multi-genre NLI dataset~\cite{N18-1101}. It is a crowd-sourced collection of sentence pairs annotated with textual entailment information, covering various spoken and written text. We evaluated the `validation\_matched' split that consists of 9815 samples.
    \item \textbf{TFQ} is the TruthfulQA dataset~\cite{lin2022truthfulqa}. It consists of question and answer pairs, some of which are correct and others are factually incorrect. Questions are designed in a way that humans may answer falsely due to a false belief or misconception. We evaluated the validation split of the `generation' subset, which consists of 817 samples.
    % \item \textbf{NaQ} is the Natural Questions dataset~\cite{47761}. It contains questions that require reading an entire Wikipedia article that may or may not contain the answer to the question. We evaluate the validation split of the `default' subset, which contains 7830 samples.
    \item \textbf{PIQ} is the Physical Interaction Question-answering dataset~\cite{Bisk2020}. It contains questions that require physical commonsense reasoning based on everyday situations. We evaluate the test split of the `plain\_text' subset, which consists of 3084 samples. 
    \item \textbf{MPP} is the Professional Psychology subset of the MMLU dataset~\cite{hendryckstest2021}. It contains multiple-choice questions that require expert knowledge in psychology. We evaluate the test split, which consists of 798 samples.
    \item \textbf{MPL} is the Professional Law subset of the MMLU dataset~\cite{hendryckstest2021}. It contains multiple-choice questions that require expert knowledge in law. We evaluate the test split, which consists of 1101 samples.
    \item \textbf{MHP} is the Highschool Psychology subset of the MMLU dataset~\cite{hendryckstest2021}. It contains multiple-choice questions that require highschool-level knowledge in psychology. We evaluate the test split, which consists of 544 samples.
    \item \textbf{GSM} is the Grade School Math 8K dataset~\cite{cobbe2021gsm8k}. It contains linguistically diverse grade school math word problems that require multi-step reasoning. Solutions generally involve calculating a series of arithmetic operations. We evaluate the test split of the `main' subset, which consists of 1319 samples.
    \item \textbf{MTH} is the MATH-500 dataset~\cite{lightmanlet}. It contains a subset of 500 problems from the MATH benchmark, which requires the model to provide a numerical answer to the question. We evaluate the test split, which consists of 500 samples.
\end{itemize}


\subsection{Implementation Details}
The implementation details are generally identical to the setup used in our experiments on the WikiMIA, BookMIA, and ArxivTection datasets.
We employ 1 epoch of stochastic gradient descent for supervised fine-tuning, with the same LoRA configurations reported in Table~\ref{tab:hyperparam}.

Furthermore, in choosing the target model for evaluation, we tried to choose the latest instruction-tuned version of each model family and size.
For instance, the most recent version of 1B and 3B models of the Llama model was 3.2, while it was 3.1 for the 8B model.
In such a case, we select the latest version, respectively.


\subsection{Results}
Evaluation results are provided in Table~\ref{tab:inthewild}.
For better comparison, we min-max scale the scores across benchmark datasets, and their ranks are provided in parentheses.
Please note that the scores are \textit{not} to be compared across models.
Overall, the MMLU~\cite{hendryckstest2021}, Math500~\cite{lightmanlet}, OBQA~\cite{OpenBookQA2018}, and TruthfulQA~\cite{lin2022truthfulqa} datasets showed higher contamination scores, and the two natural language inference datasets, HellaSwag~\cite{zellers2019hellaswag} and MNLI~\cite{N18-1101}, received lowest scores.