\section{Extended Literature Review}
\label{apdx:relwork}

Here, we provide a more descriptive review of previous works on membership inference attack~(MIA)~\cite{shokri2017membership,truex2019demystifying}, as it is related to the objective of our work in quantifying leakage~(\textit{i.e.}, contamination) in datasets~\cite{magar2022data,xu2024benchmark,balloccu2024leak}.

\paragraph{Early MIA approaches.} 
Membership inference attacks aim to determine whether a specific data sample was part of a model's training dataset. 
Early approaches primarily utilized metrics derived from model outputs to achieve this. 
For instance, \citet{salem2019ml} employed output entropy as a distinguishing factor, observing that models often exhibit lower entropy (\textit{i.e.}, higher confidence) for training samples compared to non-training ones. 
Similarly, \citet{liu2019socinf} focused on the model's confidence scores, noting that higher confidence levels could indicate a sample's presence in the training set, and \citet{carlini2022membership} proposed a likelihood ratio-based approach.
Beyond output-based metrics, researchers have explored the impact of training dynamics on MIAs. 
\citet{yeom2018privacy} investigated the use of loss values, finding that models tend to produce lower loss for training samples, making loss a viable metric for membership inference.
Additionally, \citet{liu2023gradient} introduced a gradient-based approach, leveraging the observation that the gradients of training samples differ from those of non-training samples, thereby serving as an effective indicator for membership inference.


\paragraph{Challenges of MIA on Large Language Models.}
While MIAs have been effective against traditional machine learning models, applying them to large language models (LLMs) presents unique challenges.
Recent studies have highlighted the difficulty of performing MIAs on LLMs. 
For instance, \citet{duan2024membership} found that MIAs often barely outperform random guessing when applied to LLMs. 
This limited success is primarily due to the vast scale of LLM training datasets and the relatively brief exposure of each sample during training--typically only one epoch--resulting in minimal memorization of individual data points. 
Additionally, the inherent overlap of n-grams between seen and unseen text samples complicates the distinction between seen and unseen data. 
This overlap creates a fuzzy boundary, making it challenging for MIAs to accurately infer membership. 
Furthermore, \citet{meeus2024sok} identified that distribution shifts between collected member and non-member datasets can lead to misleading MIA performance evaluations. 
They demonstrated that significant distribution shifts might cause high MIA performance, not due to actual memorization by the model, but because of these shifts. 
When controlling for such shifts, MIA performance often drops to near-random levels.

\paragraph{MIA on Large Language Models.}
Despite challenges, numerous studies have endeavored to apply membership inference attacks~(MIA) for large language models~(LLMs). 
Building on classical appraoches~\cite{yeom2018privacy, carlini2022membership}, researchers have introduced a range of innovative approaches tailored to LLMs.
Perplexity-based methods have been utilized, as demonstrated by \citet{li2023estimating} and \citet{carlini2021extracting}, who leveraged perplexity as a key metric to infer membership. 
Similarly, likelihood-based strategies have been explored, with \citet{shidetecting} and \citet{zhang2024min} employing likelihood scores to distinguish between seen and unseen samples effectively.
Other studies have extended traditional metric-based approaches to the LLM domain~\cite{duan2024membership,xie2024recall,zhang2024min,mattern2023membership,ye2024data}, while \citet{zhang2024fine} further expanded the scope by investigating the influence of fine-tuning on various membership-related scores.
In addition to individual sample-level techniques, set-level detection methods have been introduced to identify contamination across broader datasets. 
For example, \citet{orenproving}, \citet{zhang2024pacost}, and ~\citet{golchintime} enabled a more holistic assessment of dataset contamination.
Building on these foundations, our work introduced the Kernel Divergence Score, a novel method for evaluating contamination levels. 
This approach capitalizes on the differential effect of fine-tuning on embedding kernel similarity matrices, offering a unique perspective on contamination detection and addressing key limitations of existing methods.