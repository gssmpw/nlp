\section{Limitations and Future Work}
\label{apdx:limits}

In this work, we introduced the Kernel Divergence Score as a method to quantify dataset contamination levels. Compared to sample-wise scoring methods, our kernel divergence score leverages kernels that require quadratic computation. 
As shown in Table~\ref{tab:time}, kernel computation itself is not a significant burden in practice.
However, it can become a bottleneck in terms of time and space as dataset size increases substantially. 
To mitigate this computational challenge, we can reduce the amount of computation by adopting local kernel approaches~\cite{segata2010fast}, and by sequentially computing and accumulating the kernel entries to avoid out-of-memory errors.
% While our approach effectively measures relative differences in contamination rates across  datasets, its calibration across diverse models remains an open challenge. 
% This limitation suggests the need for a scoring function that operates independently of particular models.
Another promising direction for future research is the application of Positive-Unlabeled~(PU) learning~\cite{elkan2008learning}, which focuses on constructing classifiers using only positive and unlabeled data.
In the context of dataset contamination detection, PU learning could help in estimating the distribution of contaminated data when only a subset of positive (\textit{i.e.}, contaminated) examples is available.
Additionally, exploring kernel calibration methods may enhance the robustness of KDS across different models. 
By investigating these directions, future work can aim to develop a more universally applicable contamination scoring mechanism, thereby improving the reliability and comparability of contamination assessments across various machine learning models and datasets.

