\section{Related Works}

Data contamination~\cite{magar2022data,xu2024benchmark,balloccu2024leak}, also known as benchmark leakage, poses a significant challenge in the evaluation of LLMs~\cite{zhou2023don,duan2024membership}.
To mitigate this problem, one line of research focuses on ``decontaminating" datasets by introducing controlled perturbations to reduce overlap with evaluation~\cite{yang2023rethinking}.
Another line explores methods for detecting contaminated datasets or identifying samples seen during LLM training. 
Membership inference attack (MIA) techniques~\cite{shokri2017membership,truex2019demystifying} have been employed to classify individual data as seen or unseen~\cite{yeom2018privacy,salem2019ml,mattern2023membership}, with many recent studies specifically targeting LLMs for pre-training data detection~\cite{carlini2021extracting,shidetecting,zhang2024min,xie2024recall,li2023estimating,ye2024data}.
In addition, set-level detection methods have been introduced to identify contamination at a broader dataset level~\cite{orenproving,zhang2024pacost,golchintime}.
Building on this foundation, our work introduced a novel approach, Kernel Divergence Score, to scoring contamination levels using information derived from embedding kernel similarity matrices. 
\emph{An expanded literature review of MIA is in Appendix~\ref{apdx:relwork}}.
%Our method provides a reliable and granular approach to quantify contamination.

% \textcolor{blue}{Let's double check on the literature, cited extensively.}