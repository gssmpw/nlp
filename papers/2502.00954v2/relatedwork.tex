\section{Related Work}
\subsection{3D Scene Update} 

The evolving nature of 3D environments requires continuous scene updates for effective understanding, particularly in autonomous driving \cite{yurtsever2020survey} and robotic navigation \cite{wong2000scene}. A fundamental approach to scene updating involves reconstructing the entire scene from scratch. Advances in 3D reconstruction, such as multi-view stereo \cite{seitz2006comparison}, depth sensor-based \cite{zollhofer2014real}, and volumetric methods \cite{newcombe2011kinectfusion}, have improved reconstruction fidelity. But current methods struggle to balance accuracy, efficiency, and scalability \cite{niessner2013real}, making reconstruction primarily suitable for situations involving major changes. Recently, radiance-based 3D scene editing approaches, such as NeRF and 3D Gaussian Splatting (3DGS), have emerged as more efficient alternatives for incremental scene updates. NeRF-based methods \cite{liu2021editing,kania2022conerf} were limited to basic object edits and struggled with complex, cluttered scenes \cite{ye2025gaussian}. 3DGS-based methods enable finer control over scene content, including geometry \cite{huang2024sc,waczynska2024games,ye2025gaussian}, texture \cite{chen2024gaussianeditor}, and lighting \cite{gao2025relightable}. However, they struggle to disentangle these components and require costly re-optimization, limiting editability and efficiency \cite{wu2024recent}. 

Therefore, accurate and efficient 3D scene updates remain a challenge, making real-time scene acquisition not always feasible. This highlights the need for hypothetical reasoning.

\subsection{3D Visual Question Answering}
3D Visual Question Answering (3D VQA), a key task for evaluating 3D reasoning, has advanced with the rise of Vision-Language Models (VLMs) \cite{hong20233d, liu2024visual, anthroptic2024claude3.5sonnet}. By integrating vision encoders with Large Language Models (LLMs) \cite{peng2023instruction,bai2023qwen}, VLMs enable multimodal perception of 3D scenes using inputs like top-view maps and point clouds. As models advance, there is growing interest in developing more comprehensive benchmarks to better assess their 3D reasoning capabilities. Initial 3DQA dataset \cite{ye20213d}, derived from ScanNet \cite{dai2017scannet}, introduced 6K manually annotated QA pairs for scene-level reasoning. ScanQA \cite{azuma2022scanqa} expanded this with 41K QA pairs using automated question generation and human refinement. Qian et al. \cite{qian2024nuscenes} further extended 3D VQA to autonomous driving scenarios, offering domain-specific challenges. SQA3D \cite{ma2022sqa3d} introduced “situated reasoning,” requiring models to contextualize answers based on an agent’s position and orientation. MRSA \cite{linghu2024multi} and SPARTUN3D \cite{zhang2024spartun3d} further scaled SQA3D, adding multimodal inputs such as images for richer situational context. 

Unlike previous benchmarks, where answers are derived fully from the given scene, Hypo3D hypothetically applies a change to the scene and derives answers from the changed scene, increasing the hallucination risk.