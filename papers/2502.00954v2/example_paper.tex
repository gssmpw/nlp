%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables
\usepackage[table]{xcolor} 

% Hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025}.
\usepackage{hyperref}

% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025}

% For theorems and such:
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% Additional useful packages:
\usepackage{longtable}
\usepackage{multirow}
\usepackage{enumitem}
\usepackage{placeins} % For float barriers
\usepackage{adjustbox}

% Cleveref for enhanced referencing:
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}

\definecolor{SciGreen}{RGB}{34, 139, 34}  % Forest green, a deeper green for clarity
\definecolor{SciRed}{RGB}{178, 34, 34} 

% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Hypothetical Reasoning in 3D}

\begin{document}

\twocolumn[
\icmltitle{Hypo3D: Exploring Hypothetical Reasoning in 3D}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Ye Mao}{equal}
\icmlauthor{Weixun Luo}{equal}
\icmlauthor{Junpeng Jing$^\dag$}{}
\icmlauthor{Anlan Qiu}{}
\icmlauthor{Krystian Mikolajczyk}{} \\
\vskip 0.05in
Imperial College London
\vskip 0.02in
\url{https://matchlab-imperial.github.io/Hypo3D}{}
\end{icmlauthorlist}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

\icmlcorrespondingauthor{Junpeng Jing}{j.jing23@imperial.ac.uk}

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{figure*}[t]
    \centering
    \includegraphics[width=1.0\linewidth]{figure/fig1.pdf}
        \vspace{-2.0em}
    \caption{Overview of the Hypo3D benchmark. \textcircled{1} Examples of five context change types. \textcircled{2} Sample questions, including scale-based and direction-based questions requiring spatial reasoning, as well as semantic questions, all of which have open-ended answers. \textcircled{3} The radar chart highlights a notable performance gap between models and humans, especially in direction-based questions.}
    \label{fig:1}
    \vspace{-1.0em}
\end{figure*}
   
\begin{abstract}
The rise of vision-language foundation models marks an advancement in bridging the gap between human and machine capabilities in 3D scene reasoning. Existing 3D reasoning benchmarks assume real-time scene accessibility, which is impractical due to the high cost of frequent scene updates. To this end, we introduce \textit{Hypothetical 3D Reasoning}, namely Hypo3D, a benchmark designed to evaluate models' ability to reason without access to real-time scene data. Models need to imagine the scene state based on a provided change description before reasoning. Hypo3D is formulated as a 3D Visual Question Answering (VQA) benchmark, comprising 7,727 context changes across 700 indoor scenes, resulting in 14,885 question-answer pairs. An anchor-based world frame is established for all scenes, ensuring consistent reference to a global frame for directional terms in context changes and QAs. Extensive experiments show that state-of-the-art foundation models struggle to reason in hypothetically changed scenes. This reveals a substantial performance gap compared to humans, particularly in scenarios involving movement changes and directional reasoning. Even when the context change is irrelevant to the question, models often incorrectly adjust their answers. 
\end{abstract}

\section{Introduction}\label{intro}
\begin{flushright}
\textit{“Imagination is more important than knowledge."} \\[0.1cm]
\text{-- ALBERT EINSTEIN}
\end{flushright}
Artificial General Intelligence (AGI) aims to replicate the full spectrum of human cognitive abilities \cite{goertzel2014artificial,rayhan2023artificial}. Reasoning, a core cognitive ability, is a major research focus. The emergence of vision-language foundation models \cite{hong20233d,liu2024visual,wang2024qwen2,fu2024scene} has marked a significant step toward narrowing the gap between human and machine reasoning, enabling progress from text summarization \cite{lewis2019bart,zhang2020pegasus} to complex scene understanding \cite{huang2023embodied,driess2023palm}.

These models are typically evaluated under the assumption that all \textbf{\textit{knowledge}} about perception is immediately accessible during reasoning. 
For instance, in the evaluation of model performance in 3D scene reasoning, existing benchmarks \cite{ye20213d,azuma2022scanqa,ma2022sqa3d,linghu2024multi,zhang2024spartun3d} presuppose real-time availability of scene data (e.g., point clouds). Yet, this assumption does not always hold, as real-world scenes are dynamic, and maintaining up-to-date 3D scenes is challenging. Unlike 2D image capture, 3D scene collection demands specialized equipment, extended scanning times, and a complex reconstruction process for accurate geometric representation \cite{niessner2013real,daneshmand20183d}. Thus, immediate perceptual knowledge for reasoning is unavailable in many real-world cases. \textbf{\textit{Imagination}}, rooted in prior knowledge, allows humans to overcome such limitations by deducing missing details and approximating reality. Even without direct visual input, humans can mentally simulate changes in a scene and reason about the changed scene in their minds. This ability, known as mental imagery \cite{pylyshyn2002mental,moulton2009imagining}, is crucial to human intelligence and raises a critical question: \textit{Can current foundation models employ imagination to fill perceptual knowledge gaps and enhance reasoning?}

In response to this question, we propose a new reasoning concept: hypothetical reasoning. It evaluates models' ability to reason 
without immediate perceptual knowledge, requiring models to formulate reasonable hypotheses to bridge knowledge gaps. As an initial attempt, this study focuses specifically on hypothetical reasoning in 3D scenes, referred to as Hypo3D. As shown in Figs. \ref{fig:1} and \ref{fig:2}, the Hypo3D task follows a 3D visual question-answering format but cannot be solved using the given scene alone. Instead, models need to rely on an accessible context change description to imagine the current scene state after the change and adjust their answers accordingly. Based on the Hypo3D task, we constructed a dataset comprising 7,727 context changes and 14,885 question-answer pairs across 700 indoor scenes. As shown in Fig. \ref{fig:1}, these context changes span five categories: (1) Movement Change, involving geometric transformations like translation or rotation; (2) Removal Change, taking away objects; (3) Attribute Change, modifying object properties such as color and state; (4) Addition Change, introducing new objects; and (5) Replacement Change, substituting existing objects with new ones. 

The questions in the dataset range from simple proximity and relative position queries to intricate path reasoning and navigation tasks, broadly categorized into three types: (1) scale-based, focusing on proximity and size relationships; (2) direction-based, requiring reasoning about directional terms; and (3) semantic, highlighting scene semantics with minimal spatial reasoning. In constructing direction-based questions, we observe that existing 3D reasoning datasets reveal ambiguities in defining directional terms in 3D. Early datasets \cite{ye20213d,azuma2022scanqa} employ object-centric definitions, which leads to confusion when dealing with symmetrical or amorphous objects, such as round tables or cushions. Recent datasets \cite{ma2022sqa3d,linghu2024multi,zhang2024spartun3d} define directions relative to the observer. This strategy struggles to describe global directional relationships and introduces inconsistencies when multiple observers are involved. Towards this, Hypo3D establishes a world frame anchored to reference objects in each scene, ensuring that all directional terms are defined relative to a consistent global reference frame.

Extensive experiments on ten foundation models reveal a substantial performance gap between models and humans on the Hypo3D task, particularly in movement changes and directional reasoning. Surprisingly, closed-source models (e.g., GPT-4o \cite{openai2024gpt4o}) do not outperform open-source alternatives. Furthermore, the models frequently hallucinate context changes, modifying their answers even when those changes are irrelevant to the posed question. Notably, their performance consistently degrades when required to imagine a scene after changes before reasoning, as compared to directly reasoning with the provided scene. These findings highlight a key limitation of current models: the inability to leverage imagination to infer missing perceptual knowledge and reason hypothetically.

\begin{figure*}[ht]
    \centering
    \includegraphics[width=1.0\linewidth]{figure/fig2.pdf}
    \vspace{-1em}
    \caption{Example of hypothetical reasoning in a 3D scene. Given a 3D scene and an anchor-based frame description (Scene Orientation), models first align the scene to the specified frame. Then, based on a context change description and a question, models hypothetically modify the aligned scene and answer questions about the changed scene. Various models (e.g., LLMs, 2D VLMs, 3D VLMs) can tackle this task using corresponding scene representations, including scene captions, top-view maps, point clouds, and egocentric RGB-D videos.}
    \label{fig:2}
\end{figure*}

\section{Related Work}
\subsection{3D Scene Update} 

The evolving nature of 3D environments requires continuous scene updates for effective understanding, particularly in autonomous driving \cite{yurtsever2020survey} and robotic navigation \cite{wong2000scene}. A fundamental approach to scene updating involves reconstructing the entire scene from scratch. Advances in 3D reconstruction, such as multi-view stereo \cite{seitz2006comparison}, depth sensor-based \cite{zollhofer2014real}, and volumetric methods \cite{newcombe2011kinectfusion}, have improved reconstruction fidelity. But current methods struggle to balance accuracy, efficiency, and scalability \cite{niessner2013real}, making reconstruction primarily suitable for situations involving major changes. Recently, radiance-based 3D scene editing approaches, such as NeRF and 3D Gaussian Splatting (3DGS), have emerged as more efficient alternatives for incremental scene updates. NeRF-based methods \cite{liu2021editing,kania2022conerf} were limited to basic object edits and struggled with complex, cluttered scenes \cite{ye2025gaussian}. 3DGS-based methods enable finer control over scene content, including geometry \cite{huang2024sc,waczynska2024games,ye2025gaussian}, texture \cite{chen2024gaussianeditor}, and lighting \cite{gao2025relightable}. However, they struggle to disentangle these components and require costly re-optimization, limiting editability and efficiency \cite{wu2024recent}. 

Therefore, accurate and efficient 3D scene updates remain a challenge, making real-time scene acquisition not always feasible. This highlights the need for hypothetical reasoning.

\subsection{3D Visual Question Answering}
3D Visual Question Answering (3D VQA), a key task for evaluating 3D reasoning, has advanced with the rise of Vision-Language Models (VLMs) \cite{hong20233d, liu2024visual, anthroptic2024claude3.5sonnet}. By integrating vision encoders with Large Language Models (LLMs) \cite{peng2023instruction,bai2023qwen}, VLMs enable multimodal perception of 3D scenes using inputs like top-view maps and point clouds. As models advance, there is growing interest in developing more comprehensive benchmarks to better assess their 3D reasoning capabilities. Initial 3DQA dataset \cite{ye20213d}, derived from ScanNet \cite{dai2017scannet}, introduced 6K manually annotated QA pairs for scene-level reasoning. ScanQA \cite{azuma2022scanqa} expanded this with 41K QA pairs using automated question generation and human refinement. Qian et al. \cite{qian2024nuscenes} further extended 3D VQA to autonomous driving scenarios, offering domain-specific challenges. SQA3D \cite{ma2022sqa3d} introduced “situated reasoning,” requiring models to contextualize answers based on an agent’s position and orientation. MRSA \cite{linghu2024multi} and SPARTUN3D \cite{zhang2024spartun3d} further scaled SQA3D, adding multimodal inputs such as images for richer situational context. 

Unlike previous benchmarks, where answers are derived fully from the given scene, Hypo3D hypothetically applies a change to the scene and derives answers from the changed scene, increasing the hallucination risk.
 \section{Hypo3D Benchmark}

\begin{figure*}[t]
    \centering
    \includegraphics[width=1.0\linewidth]{figure/data_genereation.pdf}
     \vspace{-1em}
      \caption{Dataset Generation Pipeline. The Hypo3D collection pipeline consists of five stages: Stage \textcircled{1} curates scenes (50 hours per person), Stage \textcircled{2} defines world frames (10 h/p), Stages \textcircled{3} and \textcircled{4} collect context changes and QA descriptions from human annotators (thousands of hours) and LLM, and Stage \textcircled{5} conducts grammar checks and filters data based on semantic similarity. (25 h/p).}
      \vspace{-1em}
    \label{fig:3}
\end{figure*}

This section first defines the Hypo3D task and explains how humans, assisted by LLMs, generate high-quality context changes and QAs. It then presents the dataset statistics.

\subsection{Task Definition}
A task instance in Hypo3D is formulated as a tuple ⟨\( S \),  \( F \),  \( c \),  \( q \)⟩, where  \( S \) denotes the scene representation;  \( F \) specifies the world frame defined by anchor objects in  \( S \) that standardizes all directional terms in the task; c describes a context change to be applied to the scene; and q denotes a question. The task is to first rotate \( S \) into \( \tilde{S} \) so that the anchor object is located in the orientation described in \( F \), followed by computing an answer \( a \) to \( q \) after applying $c$ to \( \tilde{S} \). Fig. \ref{fig:2} provides an example of hypothetical reasoning in a 3D scene.

\subsection{Dataset Generation Pipeline}
As depicted in Fig. \ref{fig:3}, the Hypo3D benchmark was constructed through a rigorously designed five-stage pipeline. The initial Stage \textcircled{1} involves a comprehensive 50-hour manual curation process to collect and standardize diverse 3D scene representations from established datasets. This is followed by Stage \textcircled{2}, a meticulous 10-hour manual annotation phase where each scene is assigned an anchor-based world frame to ensure spatial consistency. The core data generation occurs in Stages \textcircled{3} and \textcircled{4}, where we implemented a hybrid approach combining human expertise with LLMs to generate context changes and question-answer pairs. These critical stages demanded substantial human oversight, with over 1,000 hours dedicated to human refinement to guarantee exceptional data quality and contextual coherence. Finally, the pipeline concludes with Stage \textcircled{5}, a 25-hour post-processing phase that employs quality control measures to eliminate redundancies while maintaining grammatical precision.

\textbf{Scene Curation.}
Each 3D scan $S$ in the Hypo3D benchmark can be represented through multiple modalities to facilitate reasoning, including point cloud, RGB-D video, top-view map, and scene caption. The point clouds and RGB-D videos are directly curated from the original scene datasets. Two variants of top-view maps are generated: non-semantic and semantic. Non-semantic maps are generated by positioning a simulated orthographic camera above the scene to produce photorealistic top-down renderings. Semantic maps augment non-semantic ones by incorporating hoverable semantic labels, with each label precisely positioned at the centroid of the bounding box corresponding to its object. An example semantic map can be seen in Fig. \ref{fig:2}. Unlike their non-semantic counterparts, semantic maps reduce object recognition errors in models, prioritizing the evaluation of hypothetical reasoning performance. Scene captions are textual descriptions detailing the attributes of objects and their relationships within the scenes.

\textbf{Anchor-Based World Frame Annotation.}
The same scene in 3D can be captured from different viewpoints in any orientation. Thus, it does not have a clearly defined, fixed reference frame. Here, we defined the world coordinate frame textually based on the locations of anchor objects in each scene, such as: “\textit{The desk is located to the left of the scene}”. When selecting anchor object candidates, visual prominence is prioritized over the extent to which they cover multiple sides of the scene. This is because, conventionally, defining a single primary orientation (e.g., “front”) is sufficient to infer the remaining orientations (“left”, “right”, “back”) using the right-hand rule \cite{hamilton2008lectures}, ensuring a consistent and unambiguous spatial frame.

\textbf{Context Change Preparation.} 
The Hypo3D benchmark defines five distinct types of object-level context changes, illustrated in Fig. \ref{fig:1}. For a scene \( S \) with an object list \( O = \{o_1, o_2, \dots, o_n\} \), each change type is defined as follows:
\vspace{-0.7em}
\begin{enumerate}[label=(\arabic*), noitemsep]
    \item \textit{\textbf{Movement changes}} relocate objects \( O_m \subseteq O \) to new positions or orientations within the scene \( S \).
    \item \textit{\textbf{Removal changes}} eliminate objects \( O_r \subseteq O \) from \( S \), resulting in an updated object list \( \hat{O} = O \setminus O_r \).
    \item \textit{\textbf{Attribute changes}} modify properties of a subset of objects \( O_{att} \subseteq O \), such as color, material, or state.
    \item \textit{\textbf{Addition changes}} introduce new objects \( O_a\) into \( S \), updating the object list to \( \hat{O} = O \cup O_a \).
    \item \textit{\textbf{Replacement changes}} replace objects \( O_{rp} \subseteq O \) with new objects \( O_a\), resulting in an updated object list \( \hat{O} = (O \setminus O_{rp}) \cup O_a \).
\end{enumerate}
\vspace{-0.7em}

As illustrated in Fig. \ref{fig:3} \textcircled{3}, human annotators and GPT-4o initially generate raw context changes uniformly to ensure data diversity. Human data is collected via crowdsourcing on the CloudResearch platform.\footnote{CloudResearch, Connect Platform, \url{https://www.cloudresearch.com}.} For each scene aligned to the world frame $\tilde{S}$ with an associated object list $O$, annotators create a set of distinct descriptions $C$ for a specified change category, following the rules below:

\vspace{-0.7em}
\begin{enumerate}[label=(\arabic*), noitemsep]
    \item Each object $o_i \in O$ referenced in change $c_i \in C$ must have a uniquely specified location if it appears multiple times in the scene $\tilde{S}$.
    \item Each context change $c_i$ must be spatially feasible within the layout of $\tilde{S}$.
    \item Each \( c_i \) must be independent, meaning it is derived solely from the original scene \( \tilde{S} \) and does not rely on any version of \( \tilde{S} \) modified by \( c_j \), where \( i \neq j \).
\end{enumerate}
\vspace{-0.7em}
% (1) if an object $o_i \in O$ referenced in $c_i \in C$ appears multiple times in the scene, its location should be uniquely specified; (2) each $c_i$ must be spatially feasible within the layout of $\tilde{S}$; and (3) each $c_i$ must be independent of the others, meaning it is contextually relevant to $\tilde{S}$ itself and not derived from other changed scenes $\tilde{S}_{c_j}$, where $i \neq j$. 


For GPT-4o, context changes are generated using the semantic top-view map of $\tilde{S}$ and a textual prompt that follows the same criteria as those provided to human annotators. The detailed prompts and human guidelines for raw change generation are provided in Appendix \ref{B}.

Finally, each raw change $c$ generated by humans and GPT-4o is edited by an independent group of human reviewers, producing a refined version $c'$.

\begin{figure*}[h]
    \centering
    \includegraphics[width=1.0\linewidth]{figure/fig4.pdf}
    \vspace{-1.6em}
    \caption{Dataset Statistics. \textcircled{1} Word cloud representing context change descriptions. \textcircled{2} Frequency distribution of context change types across 7,727 instances. \textcircled{3} Distribution of question types across change categories, with question frequency consistently highest for scale-based, then direction-based, and finally semantic.}
    
    \label{fig:4}
\end{figure*}

\textbf{Question-Answer Preparation.} Raw questions are initially generated by GPT-4o, as illustrated in Fig. \ref{fig:3} \textcircled{4}. Seven prompt templates, each corresponding to a specific question type, are designed for each type of context change. Each template includes the object list \( O \) in the scene \( S \), the context change \( c \), example questions, and a question type description (e.g., ask for the position of the changed object relative to other objects in the scene). In total, eleven unique question types are evenly distributed across the context change types (see Appendix \ref{B.2}). For each context change \( c \), 21 raw questions are generated, denoted as \( Q_c \).

Human reviewers then refine \( Q_c \) to produce a new question set \( Q_c' \). During this process, only a small portion of raw questions are retained. Specifically, 91\% of the questions are filtered out, and the remaining questions undergo additional editing to strictly ensure \( Q_c' \) satisfies the following criteria: (1) each \( q \in Q_c' \) can only be answered by combining \( S \) and \( c \), as neither is sufficient on its own; (2) the answer to each $q$ must be potentially impacted by the change $c$; (3) each $q$ has a unique and unambiguous answer; and (4) answers cannot be inferred from commonsense knowledge (e.g., a bed is larger than a pillow). Finally, all questions in \( Q_c' \) are annotated with concise human-provided answers $A_{c}'$ and reclassified into general types to evaluate models based on their reasoning capabilities: scale-based, direction-based, and semantic questions, as shown in Fig. \ref{fig:1}. Scale-based questions assess spatial reasoning related to proximity and size perception, direction-based questions focus on orientation understanding, and semantic questions evaluate the model's ability to interpret object attributes with minimal spatial reasoning. Notably, questions can belong to multiple types if they require diverse reasoning.

\textbf{Post-processing.} To ensure data diversity, SBERT \cite{reimers2019sentence} is used to remove semantically similar descriptions. It encodes the refined context changes \( C' \) and questions \( Q' \), producing embeddings \( E_{C'} \) and \( E_{Q'} \), respectively. Context changes and questions are filtered by excluding pairs with cosine similarity \( \text{Sim}(e_i, e_j) > 0.8 \) for \( e_i, e_j \in E_{C'} \), and \( \text{Sim}(e_k, e_v) > 0.8 \) for \( e_k, e_v \in E_{Q'} \). The remaining context changes, questions, and corresponding answers are further refined for grammatical accuracy using GPT4-Turbo, resulting in descriptions for the final dataset \( \tilde{C} \), \( \tilde{Q} \), and \( \tilde{A} \).


\subsection{Statistics}
The Hypo3D benchmark comprises 700 unique scenes, with 500 sourced from the ScanNet \cite{dai2017scannet} dataset and 200 from the 3RScan \cite{wald2019rio} dataset, randomly sampled from their respective sources. The dataset includes 7,727 context changes and 14,885 question-answer pairs. On average, a context change description contains 13.62 words, a question description contains 13.69 words, and an answer contains 1.28 words. The word cloud in Fig. \ref{fig:4} \textcircled{1} highlights that the most frequent words in context change descriptions are verbs representing change actions, such as ``moved," ``removed," and ``replaced". Fig. \ref{fig:4} \textcircled{2} illustrates the distribution of context change types, with movement changes being the most frequent, as they often result in more pronounced scene layout rearrangements. The bar charts in Fig. \ref{fig:4} \textcircled{3} show that scale-based and direction-based questions, which require more spatial reasoning, constitute a larger portion of the dataset compared to semantic questions.

\section{Experiments}
\subsection{Evaluation Protocol} 
\textit{Exact Match (EM)} and \textit{Partial Match (PM)} are the metrics used for evaluation. EM measures the percentage of model-predicted answers that exactly match the ground-truth answers. PM quantifies the percentage of overlapping words between the predicted answers and the ground truth. For the same model, PM is typically higher than EM, as it accounts for partial correctness. When computing EM and PM, both the predicted and ground-truth answers are normalized by lowercasing, removing punctuation, and aligning semantically similar terms (e.g., ``left" and ``west").

\setlength{\tabcolsep}{9pt} 
\begin{table*}[ht]
\centering
\caption{EM and PM accuracy of ten foundation models and human evaluators on Hypo3D. The highest model performance for each type of context change is in \textbf{bold}, while the best-performing model within each family is \underline{underlined}.}
\vskip 0.05in
\renewcommand{\arraystretch}{1.2}
\begin{adjustbox}{width=\textwidth}
\begin{tabular}{l|cc|cc|cc|cc|cc|cc}
\toprule
\toprule
\multirow{2}{*}{\textbf{Model}} & \multicolumn{2}{c|}{\textbf{Movement}} 
& \multicolumn{2}{c|}{\textbf{Removal}} 
& \multicolumn{2}{c|}{\textbf{Attribute}} 
& \multicolumn{2}{c|}{\textbf{Addition}} 
& \multicolumn{2}{c|}{\textbf{Replacement}} 
& \multicolumn{2}{c}{\textbf{Overall}} \\ 
& \multicolumn{1}{c}{EM} & PM & EM & PM & EM & PM & EM & PM & EM & PM & EM &PM\\ \hline
\rowcolor{gray!30} \multicolumn{13}{l}{\textit{LLM (Scene Caption)}}\\ Llama-3.2 3B & 25.31 & 28.37 & 29.85 & 33.65 & 24.95 & 29.59 & 26.78 & 30.78 & 23.75 & 27.68 & 26.08 & 29.91 \\
GPT-4o API (Text)  & 35.76 & 38.66 & 36.88 & 41.71 & 34.05 & 39.58 & 39.74 & 43.28 & 31.33 & 35.24 & \underline{35.54} & \underline{39.65}\\
\hline
\rowcolor{gray!30} \multicolumn{13}{l}{\hspace{0pt}\textit{2D VLM (Non-Semantic Top-View Map)}}\\
 Qwen2-VL 7B & 29.23 & 35.08 & 30.71 & 34.69 & 29.04 & 33.94 & 31.48 & 35.17 & 28.41 & 33.10 & 29.68 & 34.47 \\
Qwen2-VL 72B & 33.02 & 37.38 & 33.88 & 37.57 & 33.48 & 37.62 & 35.95 & 40.29 & 30.66 & 34.64 & 33.39 & 37.51\\
LLaVA-OV 7B  & 30.34 & 34.17 & 29.81 & 33.24 & 31.37 & 36.13 & 33.12 & 35.64 & 28.41 & 31.81 & 30.62 & 34.34\\
LLaVA-OV 72B & 36.46 & 39.83 & 36.45 & 40.22 & 35.70 & 40.46 & 39.64 & 42.25 & 33.83 & 37.85 & \underline{36.38} & \underline{40.13}\\
Claude 3.5 Sonnet API & 17.49 & 30.24 & 19.90 & 27.34 & 22.96 & 33.47 & 22.90 & 31.61 & 20.35 & 27.70 & 20.42 & 30.29 \\
GPT-4o API& 34.49 & 37.69 & 32.85 & 36.53 & 31.23 & 35.38 & 38.09 & 40.70 & 30.04 & 33.22 & 33.58 & 36.75\\
\hline
\rowcolor{gray!30} \multicolumn{13}{l}{\textit{2D VLM (Semantic Top-View Map)}}\\
Qwen2-VL 7B & 31.26 & 36.41 & 38.09 & 41.90 & 34.83 & 39.41 & 37.64 & 41.41 & 31.86 & 36.62 & 34.40 & 38.91 \\
Qwen2-VL 72B & 38.42 & 42.56 & \textbf{47.36} & \textbf{51.05} & 46.76 & 51.10 & 47.63 & 50.87 & \textbf{44.43} & 48.78 & 44.25 & 48.25\\
LLaVA-OV 7B  & 33.32 & 36.80 & 34.34 & 37.84 & 34.98 & 39.50 & 38.96 & 41.98 & 33.93 & 38.33 & 34.81 & 38.60 \\
LLaVA-OV 72B & 39.39 & 42.99 & 43.44 & 46.87 & 44.57 & 49.37 & 46.12 & 49.06 & 44.10 & 48.18 & 43.01 & 46.83\\
Claude 3.5 Sonnet API & 30.92 & 42.98 & 40.26 & 48.54 & 42.29 & \textbf{52.72} & 43.16 & 51.59 & 43.28 & \textbf{50.73} & 38.86 & 48.65 \\
GPT-4o API& \textbf{40.77} & \textbf{43.79} & 47.36 & 50.40 & \textbf{47.42} & 51.39 & \textbf{50.59} & \textbf{53.77} & 44.24 & 47.68 & \underline{\textbf{45.50}} & \underline{\textbf{48.82}} \\
\hline
\rowcolor{gray!30} \multicolumn{13}{l}{\textit{3D VLM (RGB-D Video, Point Cloud)}}\\
LEO 7B & 14.40 & 22.96 & 18.54 & 22.82 & 14.35 & 21.56 & 14.64 & 24.83 & 11.76 & 19.50 & 14.83 & 22.40 \\
LLaVA-3D 7B & 31.63 & 35.11 & 30.60 & 33.91 & 31.60 & 36.16 & 33.67 & 36.70 & 30.42 & 34.16 & \underline{31.56} & \underline{35.23} \\
\hline
\hline
\textbf{Human} & 95.00 & 96.00 & 93.00 & 95.00 & 93.00 & 94.83 & 89.00 & 90.67  & 85.00 & 86.00 & 91.00 & 92.50 \\
\bottomrule
\end{tabular}
\end{adjustbox}
\label{tab:1}
\end{table*}

\subsection{Baselines}
A total of ten foundation models were evaluated in a zero-shot setting on the Hypo3D benchmark, grouped into: (1) LLMs, (2) 2D VLMs, and (3) 3D VLMs. The evaluated LLMs, Llama3.2 (3B) \cite{dubey2024llama} and GPT-4o \cite{openai2024gpt4o}, utilize textual scene captions from the ScanRefer \cite{chen2020scanrefer} and SceneVerse \cite{jia2025sceneverse} datasets to represent 3D scenes. The 2D VLMs consist of open-source models Qwen2-VL (7B, 72B) \cite{wang2024qwen2} and LLaVA-OV (7B, 72B) \cite{li2024llava}, alongside closed-source models GPT-4o \cite{openai2024gpt4o} and Claude 3.5 Sonnet\cite{anthroptic2024claude3.5sonnet}\footnote{We use GPT-4o-08-16 and Claude 3.5 Sonnet-10-22.}, both employing semantic and non-semantic top-view maps. The 3D VLMs assessed include LEO (7B) \cite{huang2023embodied}, which encodes 3D scenes using egocentric 2D images with 3D point clouds, and LLaVA-3D (7B) \cite{zhu2024llava}, which represents 3D scenes through multi-view images. 

In addition to scene inputs, all models receive the anchor-based world frame, hypothetical context changes applied to the scenes, the question, and task guidance as part of the textual prompt. Specific prompt templates for each model are provided in Appendix \ref{C.2}. As shown in Fig. \ref{fig:2}, The task guidance generally consists of three steps: (1) Rotate the scene to align with the provided world frame, (2) Imagine the scene after the specified context change, and (3) Answer the question based on the modified scene. 

\subsection{Results and Discussions}
\textbf{Human-Model Performance Gap.}  
To manage the high costs of human evaluation, we sampled 50 scenes and 250 context changes with 50 questions per change type for assessment. To avoid contamination, human evaluators were excluded from benchmark annotation and provided only 10 QA pairs for task familiarization. As shown in Table \ref{tab:1}, human performance exceeds 85\% in EM across all change types, though it doesn't reach 100\% due to the open-ended nature of Hypo3D questions. Human performance is slightly lower for addition and replacement changes, as these introduce new objects, causing confusion with existing ones.

In contrast, foundation models perform significantly worse, with the top-performing model, GPT-4o, achieving under 50\% overall in both EM and PM metrics, even with a semantic top-view map input. Notably, it lags 45.5\% behind human EM performance. Furthermore, models show significant performance bias across different question types, as highlighted by the distinct non-overlapping regions in the radar chart in Fig. \ref{fig:5}. In comparison, humans achieve over 90\% accuracy across all question types, demonstrating consistently strong performance.

\textbf{LLMs \textit{vs.} 2D VLMs \textit{vs.} 3D VLMs.}
Table \ref{tab:1} shows that 2D VLMs outperform other foundation models across all change types. Their performance improves significantly with semantic top-view maps that provide explicit object labels compared to non-semantic maps. When using non-semantic maps, most 2D VLMs perform worse than the text-only version of GPT-4o, highlighting the impact of image recognition errors on 2D VLM performance. 

For open-source VLMs like Qwen2-VL and LLaVA-OV, larger model sizes yield better results. Closed-source models (GPT-4o and Claude 3.5 Sonnet), despite their reputation for superior performance, do not maintain this advantage on the Hypo3D task. They excel with semantic maps but struggle on non-semantic maps, where the open-source LLaVA-OV 72B delivers the best performance. This pattern aligns with findings by \citet{li2024topviewrs}, where closed-source models particularly struggled on unlabeled top-view maps. 

Interestingly, 3D VLMs, despite receiving the richest geometric information, do not demonstrate a clear advantage over 2D VLMs or LLMs. Notably, the LEO model struggles with the instruction following, often failing to interpret task guidance and achieving the lowest EM score (14.83\%). LLaVA-3D, although outperforming all other 7B 2D VLMs when using a non-semantic map, lacks a larger model size variant to fully showcase its potential in this task. See Appendix \ref{C.4} for additional qualitative results illustrating the performance gap between models.

\subsection{Analyses and Insights}
Our key insights are as follows: Models face significant difficulties in reasoning about hypothetically changed scenes compared to static ones, particularly with movement and replacement changes. Direction-based questions that require scene orientation also pose challenges, with performance deteriorating in the absence of a defined world frame. Most models exhibit severe hallucinations, often altering their answers even when the context changes are irrelevant to the questions. All 2D VLM results presented in Tables \ref{tab:2}, \ref{tab:3}, and \ref{tab:4} are derived from the semantic top-view map.

\textbf{Insight 1}: \textit{Models struggle with hypothetical movement and replacement changes.} 

The bolded values in Table \ref{tab:1} indicate the highest EM and PM accuracy achieved by models across different context changes. Movement changes show the lowest performance, scoring 9.82\% lower in EM and 9.98\% lower in PM compared to the best-performing addition changes. This outcome highlights the models' difficulty in handling changes that heavily reconfigure the scene's spatial layout and alter inter-object spatial relationships. Another finding is that replacement changes perform worse than both removal and addition changes, likely because they involve both object removal and addition simultaneously, making them more challenging than handling either change individually. For example, the replacement change ``\textit{a cup is replaced with a phone}'' can be interpreted as first removing the cup and then adding the phone in its place. 

\textbf{Insight 2}: \textit{Models struggle with direction-based questions.} 

The radar chart in Fig. \ref{fig:5} shows EM results across different question types, revealing that most models, except LEO, perform better on semantic questions than spatial ones. A more distinct pattern can be observed in Fig. \ref{fig:14}. When using a semantic map where object labels are provided, leading models like Qwen2-VL 72B and GPT-4o achieve over 60\% EM, indicating that semantic questions are less challenging when models correctly identify objects. Within spatial questions, models struggle more with direction-based questions compared to scale-based ones. Even with scene input aligned to the world frame, performance on direction-based questions remains low (see Appendix \ref{C.3}), suggesting models struggle more with orientation understanding than with size and proximity reasoning.

\begin{figure}[t]
    \centering
    \includegraphics[width=1.0\linewidth]{figure/fig5.pdf}
    \vspace{-1em}
    \caption{Model and human EM performance across question types. Humans consistently achieve strong performance, whereas models struggle, particularly with direction-based questions.}
    \label{fig:5}
    \vspace{-1em}
\end{figure}

\textbf{Insight 3}: \textit{Anchor-based frame definition improves orientation understanding.} 

Table \ref{tab:2} presents model results on 3,495 pure direction-based questions from Hypo3D under three conditions: without a world frame (w/o frame), using the camera view as the frame (w. camera), and using the anchor-based frame (w. anchor). The camera view is an image that captures the $X$ side of the room from its center, where $X \in \{\text{left, right, front, back}\}$. The results indicate that not all frame definition methods can be effectively interpreted by current models. Only our anchor-based definition method consistently improves the performance of 2D and 3D VLMs, whereas using the camera view as a frame reduces the performance. See Appendix \ref{C.2} for details on inference templates across different settings.

\begin{table}[t]
    \caption{Comparison of model performance on directional questions using no frame, camera view frame, and anchor object frame. \textcolor{SciRed}{Red} is lower, and \textcolor{SciGreen}{green} is higher, compared to w/o frame.}
    \label{sample-table}
    \vskip 0.05in
    \centering
    \begin{adjustbox}{width=1.0\linewidth}
    \begin{tabular}{l|cc|cc|cc} 
        \toprule
        \multirow{2}{*}{\textbf{Model}} & \multicolumn{2}{c|}{\textbf{w/o frame}} 
                               & \multicolumn{2}{c|}{\textbf{w. camera}} 
                               & \multicolumn{2}{c}{\textbf{w. anchor (ours)}} \\ 
        & EM & PM & EM & PM & EM & PM  \\
        \midrule
        Llama 3.2 3B & 9.7 & 18.55 & - & - & \textcolor{SciGreen}{13.36} & \textcolor{SciGreen}{21.52}  \\ 
        Qwen2-VL 72B & 20.54 & 30.61 & \textcolor{SciRed}{19.97} & \textcolor{SciRed}{30.14}& \textcolor{SciGreen}{22.03} & \textcolor{SciGreen}{33.63} \\ 
        GPT-4o API& 18.45 & 28.36 & \textcolor{SciRed}{18.20} & \textcolor{SciRed}{28.04} & \textcolor{SciGreen}{19.37} & \textcolor{SciGreen}{29.48} \\ 
        LLaVA-3D 7B & 15.57 & 25.57 & - & - & \textcolor{SciRed}{15.31} & \textcolor{SciGreen}{25.78} \\ 
        \bottomrule 
    \end{tabular}
    \end{adjustbox}
\label{tab:2}
\vspace{-1em}
\end{table}

\textbf{Insight 4}: \textit{Reasoning in hypothetically changed scenes is more challenging than in unchanged scenes.}

250 context changes and corresponding question pairs were sampled from Hypo3D to validate this insight. Each pair was annotated with two answers: one based on the unchanged scene and the other based on the hypothetically changed scene. Table \ref{tab:3} presents model performance for answering questions under both conditions. Most models, except Llama-3.2 3B, exhibit a consistent performance drop in EM and PM accuracy when reasoning in changed scenes compared to the unchanged scene. This finding addresses the core research question discussed in Sec. \ref{intro}, showing that the imagination capability required for hypothetical reasoning is lacking in current foundation models.

\setlength{\tabcolsep}{4pt} 
\begin{table}[t]
\centering
\caption{Comparison of model performance when using and not using context change, where the changes \textbf{affect} the answer.}
\label{sample-table}
\vskip 0.05in
\begin{adjustbox}{width=1.0\linewidth}
\begin{tabular}{l|cc|cc} 
\toprule
\multirow{2}{*}{\textbf{Model}} & \multicolumn{2}{c|}{\textbf{w/o change}} & \multicolumn{2}{c}{\textbf{w. change}} \\
& \multicolumn{1}{c}{EM} & \multicolumn{1}{c|}{PM} & \multicolumn{1}{c}{EM} & \multicolumn{1}{c}{PM} \\
\midrule
LLaMA-3.2 3B           & 19.00 & 23.25 & 20.50 (\textcolor{SciGreen}{+1.50}) & 24.50 (\textcolor{SciGreen}{+1.25}) \\ 
Qwen2-VL 72B           & 37.00 & 41.50 & 31.50 (\textcolor{SciRed}{-5.50}) & 36.00 (\textcolor{SciRed}{-5.50}) \\
GPT-4o API           & 38.00 & 40.25 & 33.00 (\textcolor{SciRed}{-5.00}) & 36.00 (\textcolor{SciRed}{-4.25}) \\
Claude 3.5 Sonnet API      & 33.00 & 39.75 & 29.00 (\textcolor{SciRed}{-4.00}) & 35.50 (\textcolor{SciRed}{-4.25}) \\ 
LLaVA-3D 7B            & 27.00 & 31.00 & 20.50 (\textcolor{SciRed}{-6.50}) & 24.00 (\textcolor{SciRed}{-7.00}) \\
\bottomrule
\end{tabular}
\end{adjustbox}
\label{tab:3}
\vspace{-1.0em}
\end{table}

\begin{table}[h]
\caption{Comparison of model performance when using and not using context change, where the changes \textbf{do not affect} the answer.}
\centering
\label{sample-table}
\vskip 0.05in
\begin{adjustbox}{width=1.0\linewidth}
\begin{tabular}{l|cc|cc} 
\toprule
\multirow{2}{*}{\textbf{Model}} & \multicolumn{2}{c|}{\textbf{w/o change}} & \multicolumn{2}{c}{\textbf{w. change}} \\
& EM & PM & EM & PM \\
\midrule
LLaMA-3.2 3B & 27.50 & 31.42 & 29.00 (\textcolor{SciGreen}{+1.50}) & 33.25 (\textcolor{SciGreen}{+1.83}) \\ 
Qwen2-VL 72B & 56.50 & 60.17 & 51.50 (\textcolor{SciRed}{-5.00}) & 55.17 (\textcolor{SciRed}{-5.00}) \\ 
GPT-4o API & 57.00 & 60.00 & 52.50 (\textcolor{SciRed}{-4.50}) & 56.92 (\textcolor{SciRed}{-3.08}) \\ 
Claude 3.5 Sonnet API & 52.50 & 59.00 & 49.00 (\textcolor{SciRed}{-3.50}) & 53.25 (\textcolor{SciRed}{-5.75}) \\ 
LLaVA-3D 7B & 37.50 & 40.17 & 37.00 (\textcolor{SciRed}{-0.50}) & 40.17 (0.00) \\
\bottomrule
\end{tabular}
\end{adjustbox}
\label{tab:4}
\vspace{-1em}
\end{table}

\textbf{Insight 5}: \textit{Models hallucinate when changes are irrelevant.}

Previous results indicate that models struggle to understand how context changes influence answers. Here, we show that they also struggle to ignore irrelevant context. To test this, we constructed 250 new context change-QA triplets, separate from the benchmark. In these triplets, the context changes have no impact on the answers to the questions. For example, the context change can be ``\textit{The object is moved from A to B}", while the question asks, ``\textit{What is the color of the object?}" Models were evaluated both with and without the context change description. Ideally, they should provide consistent answers, as object movement does not affect its color. However, as shown in Table \ref{tab:4}, all 2D and 3D VLMs exhibit performance degradation when context changes are introduced. Notably, while 2D VLMs achieve the highest performance on Hypo3D tasks in previous evaluations, they also exhibit more severe hallucinations.

\section{Conclusion}
In this paper, we introduce the Hypo3D task to investigate the hypothetical reasoning ability of foundation models in 3D. This task challenges models to imagine scene changes before 3D reasoning, demanding robust reasoning without real-time scene access. To standardize directional term definitions in 3D, Hypo3D employs an anchor-based world frame for each scene. Extensive experiments on ten foundation models, including LLMs, 2D VLMs, and 3D VLMs, reveal that all models struggle with the Hypo3D task, especially when handling movement changes and directional reasoning questions. These models exhibit severe hallucinations, altering answers even when the context changes do not affect the questions. These findings confirm that current models struggle to simulate scene changes without direct observation. We hope this study inspires further research into strengthening foundation models' hypothetical reasoning to narrow the gap with human cognitive abilities.

% \section*{Impact Statement}
% The impact of our study is multifaceted. First, while hypothetical reasoning is motivated by challenges in handling frequent updates in 3D scenes, it is crucial for foundation models across modalities, including 3D, 2D, and text. It enables models to simulate perceptual outcomes internally without requiring physical action. This capability is especially valuable in scenarios where obtaining sensory output is technologically constrained or costly. Second, Hypo3D, the first 3D VQA benchmark, standardizes directional relationships using a fixed world frame, enabling unified representations. Finally, we will release prompt templates, WebUI source code for data collection, 3D scene representations, and reasoning scripts for model evaluation, empowering researchers to develop customized hypothetical reasoning datasets and ensuring result reproducibility.
\nocite{langley00}
% \clearpage
\bibliography{example_paper}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\onecolumn
\appendix



\section{Limitation}
One limitation is relying solely on text to describe context changes, which may lack precision for complex scenarios. Future work will incorporate multimodal approaches, such as images or egocentric videos, for more accurate and complementary representation. Additionally, our dataset focuses on hypothetical reasoning in indoor scenes, with plans to extend to outdoor environments. Lastly, Hypo3D addresses object-level changes (e.g., modifying specific objects); future work will explore scene-level changes involving significant layout rearrangements.

\section{Benchmark Details} \label{B}
\subsection{Human Collection via Crowdsourcing} 

Half of the raw context changes were collected using the crowdsourcing platform CloudResearch, following a multi-stage procedure facilitated by a dedicated WebUI designed for collecting specific types of changes. Annotators were recruited from English-speaking countries, including the United States, Australia, Canada, Ireland, New Zealand, and the United Kingdom, to ensure linguistic consistency. Annotators were compensated at a rate of $\$1.00$ for every five change descriptions derived from 3D scenes. Fig. \ref{fig:6} outlines the annotation protocol, specifying requirements for accurate descriptions of moved object locations and ensuring that changed objects are not repeated across submissions.

\subsection{LLM Collection}\label{B.2}
GPT-4o is integral to both the context change and question collection processes in Hypo3D. For context change collection, each type of change is generated using dedicated prompt templates. Fig. \ref{fig:7} highlights the templates specifically designed for capturing addition changes.

For the question collection, eleven unique question types are designed, encompassing categories such as proximity, size-based recognition, and path reasoning, as outlined in Table \ref{tab:5}. Fig. \ref{fig:8} and \ref{fig:9} illustrate the prompt templates for generating questions related to proximity and relative positions, respectively. Each context change is associated with seven specific question types, and the distribution of question types across different changes is detailed in Table \ref{tab:6}. While some context changes share the same question types, they are provided with different prompt templates and example questions for diverse generations. Once collected, the questions are categorized into broader, coarse-grained categories based on the primary capability required to answer them, such as scale-based, direction-based, and semantic questions.

\begin{figure*}[h]
    \centering
    \includegraphics[width= 0.7\linewidth]{figure/human_context_guideline.pdf}
     \vspace{-1em}
      \caption{Guidelines for movement change collection in crowdsourcing.}
    \label{fig:6}
\end{figure*}

\setlength{\tabcolsep}{9pt} 
\begin{table*}[h]
\raggedleft
\caption{Complete list of question types, each with a representative example.}
\vskip 0.05in
\renewcommand{\arraystretch}{1.2}
\begin{adjustbox}{width=1.0\textwidth}
\begin{tabular}{l|l}
\toprule
\toprule
\textbf{Proximity} & \textit{Which direct distance is shorter: from the nightstand to the window or to the box?}\\ \hline
\textbf{Direction-based Recognition} & \textit{What item is directly positioned below the shelf now?} \\ \hline
\textbf{Size-based Recognition} &   \textit{What is the largest item remaining on the bed now?}\\\hline
\textbf{Functionality}& \textit{What item in the room now provides storage functionality similar to the removed shelf?}\\\hline
\textbf{Counting} & \textit{What is the current count of chairs next to the table?}\\\hline
\textbf{Navigation}& \textit{Which direction should you move from the plant to reach the new chair?} \\\hline
\textbf{Placement Height}& \textit{Is the box positioned higher or lower than the keyboards?} \\\hline
\textbf{Relative Position} & \textit{What is the relative position of the armchair to the footstool now?} \\\hline
\textbf{Attribute} &  \textit{How many different colors of the table are in the room now?} \\\hline
\textbf{Path Reasoning} & \textit{Is the direct path from the trash can's new location to the fire alarm obstructed by any objects?} \\\hline
\textbf{Situational Reasoning} &  \textit{Are you closer to the repositioned trash can or the copier when you're next to the door?} \\
\hline
\bottomrule
\end{tabular}
\end{adjustbox}
\label{tab:5}
\end{table*}

\setlength{\tabcolsep}{3pt} 
\begin{table*}[h]
\centering
\caption{Distribution of question types across different context changes.}
\vskip 0.05in
\renewcommand{\arraystretch}{1.2}
\begin{adjustbox}{width=1.0\textwidth}
\begin{tabular}{c|c|c|c|c}
\toprule
\toprule\multicolumn{1}{c|}{\textbf{Movement}}
& \multicolumn{1}{c|}{\textbf{Removal}}
& \multicolumn{1}{c|}{\textbf{Attribute}}
& \multicolumn{1}{c|}{\textbf{Addition}} 
& \multicolumn{1}{c}{\textbf{Replacement}} \\
\hline
Proximity & Situational Reasoning & Proximity & Proximity & Proximity \\
Direction-based Recognition & Direction-based Recognition & Direction-based Recognition & Direction-based Recognition & Direction-based Recognition \\
Size-based Recognition & Size-based Recognition & Size-based Recognition & Size-based Recognition & Size-based Recognition
\\
Relative Position & Size Fitness & Relative Position & Relative Position & Relative Position\\
Path Reasoning & Functionality & Functionality & Functionality & Functionality \\
Placement Height & Navigation & Navigation & Placement Height  & Attribute \\ 
Counting & Counting & Counting & Counting & Counting\\
\hline
\bottomrule
\end{tabular}
\end{adjustbox}
\label{tab:6}
\end{table*}

\begin{figure*}[h]
    \centering
    \includegraphics[width=0.9\linewidth]{figure/addition_change_generation.pdf}
     \vspace{-1em}
      \caption{Prompt template for addition change generation.}
    \label{fig:7}
\end{figure*}

\begin{figure*}[h]
    \centering
    \includegraphics[width=.9\linewidth]{figure/proximity_question_generation.pdf}
     \vspace{-1em}
      \caption{Prompt template for generating raw proximity questions based on context changes and the object list of the 3D scene.}
    \label{fig:8}
\end{figure*}

\begin{figure*}[h]
    \centering
    \includegraphics[width=.9\linewidth]{figure/rel_question_generation.pdf}
     \vspace{-1em}
      \caption{Prompt template for generating raw relative position questions based on context changes and the object list of the 3D scene.}
    \label{fig:9}
\end{figure*}

\begin{figure*}[h]
    \centering
    \includegraphics[width=0.7\linewidth]{figure/fig6.png}
     \vspace{-1em}
      \caption{Question distribution in Hypo3D.}
    \label{fig:10}
\end{figure*}

% \clearpage
\subsection{Data Statistics}
Fig. \ref{fig:10} demonstrates that the questions in our benchmark begin with various start words, such as “what”, “which”, “is”, “how”, and “when”. None of these words dominate the dataset, highlighting the balanced and diverse nature of our dataset.

\section{Experiments}
\subsection{Model Hyperparameter Settings}
Our experiments primarily used the default inference hyperparameters for zero-shot models, as detailed in Table \ref{tab:7}. For Claude 3.5 Sonnet, the maximum new token parameter was set to 40, reduced from its default value due to the model's tendency to generate lengthy responses, even when instructed to be concise.

\setlength{\tabcolsep}{9pt} 
\begin{table*}[h]
\centering
\caption{Inference hyperparameter settings for the baseline models.}
\vskip 0.05in
\renewcommand{\arraystretch}{1.2}
\begin{adjustbox}{width=0.5\textwidth}
\begin{tabular}{l|c|c}
\toprule
\toprule
\multirow{1}{*}{\textbf{Model}} &  \multicolumn{1}{c|}{\textbf{Max New Tokens}} & \multicolumn{1}{c}{\textbf{Temperature}} \\
\hline
Llama3.2 3B& 32 & 0.6 \\
Qwen2-VL 7B \& 72B & 128 & 0.01\\
LLaVA-OV 7B \& 72B  & 128 & 0.7 \\
GPT-4o API& 1024 & 1.0 \\
Claude 3.5 Sonnet API & 40 & 1.0 \\
LLaVA-3D 7B & 512 & 0.2 \\
LEO 7B & 256 & 1.0 \\
\bottomrule
\bottomrule
\end{tabular}
\end{adjustbox}
\label{tab:7}
\end{table*}


\begin{figure*}[h]
    \centering
    \includegraphics[width=0.9\linewidth]{figure/hypo3d_reason.pdf}
     \vspace{-1em}
      \caption{Prompt template for the main hypothetical reasoning experiments, with differences between prompts for 3D VLMs, 2D VLMs, and LLMs underlined.}
    \label{fig:11}
\end{figure*}

\begin{figure*}[h]
    \centering
    \includegraphics[width=0.9\linewidth]{figure/without_axis.pdf}
     \vspace{-1em}
      \caption{Prompt template for evaluating model performance without using a world frame.}
    \label{fig:12}
\end{figure*}

\begin{figure*}[h]
    \centering
    \includegraphics[width=0.9\linewidth]{figure/reason_without_change.pdf}
     \vspace{-1em}
     \caption{Prompt template for evaluating models in static scenes without context changes.}
    \label{fig:13}
\end{figure*}


\setlength{\tabcolsep}{7pt} 
\begin{table*}[h]
\centering
\caption{Complete results of foundation models and human evaluators on Hypo3D.}
\renewcommand{\arraystretch}{1.1}
\begin{adjustbox}{width=\textwidth}
\begin{tabular}{cc|ccc|ccc|ccc|ccc|ccc|ccc|ccc|ccc|ccc|ccc|c}
\toprule
\hline
 {\multirow{2}{*}{\textbf{Model}}} & \multicolumn{1}{c|}{\multirow{2}{*}{Metric}}  & \multicolumn{3}{c|}{\textbf{Movement}} & \multicolumn{3}{c|}{\textbf{Removal}} & \multicolumn{3}{c|}{\textbf{Attribute}} & \multicolumn{3}{c|}{\textbf{Addition}} & \multicolumn{3}{c|}{\textbf{Replacement}} & \multirow{2}{*}{\textbf{Overall}} \\ 
 &\multicolumn{1}{c|}{} & \multicolumn{1}{c}{Scale.} & \multicolumn{1}{c}{Dire.} & \multicolumn{1}{c|}{Sem.} & \multicolumn{1}{c}{Scale.} & \multicolumn{1}{c}{Dire.} & \multicolumn{1}{c|}{Sem.} & \multicolumn{1}{c}{Scale.} & \multicolumn{1}{c}{Dire.} & \multicolumn{1}{c|}{Sem.} & \multicolumn{1}{c}{Scale.} & \multicolumn{1}{c}{Dire.} & \multicolumn{1}{c|}{Sem.} & \multicolumn{1}{c}{Scale.} & \multicolumn{1}{c}{Dire.} & \multicolumn{1}{c|}{Sem.} &  \\ \hline
 \rowcolor{gray!30} \multicolumn{18}{l}{\textit{LLM (Scene Caption)}} \\
\multirow{2}{*}{Llama 3.2 3B}
 &EM& 29.62 & 15.89 & 30.94 & 35.29 & 14.14 & 30.54 & 26.50 & 13.49 & 32.23 & 30.26 & 12.49 & 32.12 & 26.20 & 10.96 & 33.08 & 26.08 \\
&\cellcolor{gray!15}PM& \cellcolor{gray!15}31.43 & \cellcolor{gray!15}20.26 & \cellcolor{gray!15}31.18 &\cellcolor{gray!15} 37.40 &\cellcolor{gray!15} 21.37 &\cellcolor{gray!15} 32.23 &\cellcolor{gray!15} 31.03 & \cellcolor{gray!15}20.97 &\cellcolor{gray!15} 32.48 &\cellcolor{gray!15} 34.15 &\cellcolor{gray!15} 17.90 &\cellcolor{gray!15} 32.54 & \cellcolor{gray!15}28.90 & \cellcolor{gray!15}17.14 & \cellcolor{gray!15}34.54 & \cellcolor{gray!15}29.91 \\ 
\multirow{2}{*}{GPT-4o API (Text) }
 &EM& 45.36 & 21.88 & 37.17 & 44.01 & 20.49 & 37.66 & 35.03 & 21.52 & 45.85 & 45.31 & 19.08 & 50.00 & 36.41 & 15.23 & 42.54 & 35.54 \\ 
 &\cellcolor{gray!15}PM& \cellcolor{gray!15}47.20 & \cellcolor{gray!15}26.01 &\cellcolor{gray!15} 37.17 &\cellcolor{gray!15} 46.59 &\cellcolor{gray!15} 29.67 &\cellcolor{gray!15} 40.32 &\cellcolor{gray!15} 39.43 &\cellcolor{gray!15} 30.74 &\cellcolor{gray!15} 47.54 &\cellcolor{gray!15} 48.65 &\cellcolor{gray!15} 24.44 &\cellcolor{gray!15} 50.84 &\cellcolor{gray!15} 39.55 &\cellcolor{gray!15} 20.85 &\cellcolor{gray!15} 44.65 &\cellcolor{gray!15} 39.65 \\ \hline
 \rowcolor{gray!30} \multicolumn{18}{l}{\textit{2D VLM (Semantic Top-View Map)}}\\
\multirow{2}{*}{Qwen2-VL 7B} &EM& 34.82 & 19.96 & 35.49 & 33.73 & 17.27 & 35.76 & 29.14 & 15.25 & 40.53 & 35.50 & 14.49 & 37.99 & 30.29 & 13.00 & 41.54 & 29.85 \\
  &\cellcolor{gray!15}PM& \cellcolor{gray!15}39.75 & \cellcolor{gray!15}25.75 &\cellcolor{gray!15} 35.49 &\cellcolor{gray!15} 35.38 &\cellcolor{gray!15} 25.73 &\cellcolor{gray!15} 37.00 & \cellcolor{gray!15}33.15 & \cellcolor{gray!15}24.16 &\cellcolor{gray!15} 41.50 & \cellcolor{gray!15}37.55 &\cellcolor{gray!15} 20.55 & \cellcolor{gray!15}39.39 & \cellcolor{gray!15}31.90 & \cellcolor{gray!15}20.35 &\cellcolor{gray!15} 45.15 & \cellcolor{gray!15}34.47 \\
\multirow{2}{*}{Qwen2-VL 72B}
 &EM& 39.75 & 21.33 & 38.13 & 37.34 & 18.41 & 40.03 & 33.47 & 19.94 & 47.18 & 39.41 & 18.02 & 44.13 & 33.14 & 15.03 & 42.29 & 33.39 \\ 
   & \cellcolor{gray!15} PM& \cellcolor{gray!15} 42.18 & \cellcolor{gray!15} 27.06 & \cellcolor{gray!15} 38.13 & \cellcolor{gray!15} 38.71 & \cellcolor{gray!15}26.72 & \cellcolor{gray!15}41.09 & \cellcolor{gray!15}36.54 & \cellcolor{gray!15}27.14 & \cellcolor{gray!15}47.79 & \cellcolor{gray!15}43.11 & \cellcolor{gray!15}23.97 & \cellcolor{gray!15}44.41 & \cellcolor{gray!15}34.19 & \cellcolor{gray!15}22.29 & \cellcolor{gray!15}43.66 & \cellcolor{gray!15}37.51 \\
 \multirow{2}{*}{LLaVA-OV 7B}
 &EM& 36.35 & 20.34 & 36.93 & 32.61 & 16.13 & 38.29 & 33.24 & 20.29 & 39.53 & 38.75 & 13.31 & 40.50 & 31.76 & 14.06 & 39.80 & 30.62 \\
&\cellcolor{gray!15}PM& \cellcolor{gray!15}38.90 & \cellcolor{gray!15}24.99 & \cellcolor{gray!15}36.93 & \cellcolor{gray!15}33.81 & \cellcolor{gray!15}23.29 & \cellcolor{gray!15}40.32 & \cellcolor{gray!15}37.30 & \cellcolor{gray!15}29.07 & \cellcolor{gray!15}40.03 & \cellcolor{gray!15}\cellcolor{gray!15}40.02 & \cellcolor{gray!15}17.84 & \cellcolor{gray!15}41.48 & \cellcolor{gray!15}32.97 & \cellcolor{gray!15}19.69 & \cellcolor{gray!15}42.04 & \cellcolor{gray!15}34.34 \\ 
\multirow{2}{*}{LLaVA-OV 72B}
 &EM& 43.53 & 24.20 & 41.01 & 40.90 & 20.68 & 41.77 & 38.61 & 23.70 & 43.69 & 43.84 & 19.67 & 48.32 & 36.24 & 17.26 & 47.76 & 36.38 \\ 
  &\cellcolor{gray!15}PM& \cellcolor{gray!15}44.73 & \cellcolor{gray!15}29.38 & \cellcolor{gray!15}41.01 &\cellcolor{gray!15} 42.17 & \cellcolor{gray!15}29.13 & \cellcolor{gray!15}42.88 & \cellcolor{gray!15}42.43 &\cellcolor{gray!15} 32.38 &\cellcolor{gray!15} 44.24 &\cellcolor{gray!15} 45.10 &\cellcolor{gray!15} 25.03 &\cellcolor{gray!15} 48.32 &\cellcolor{gray!15} 37.78 &\cellcolor{gray!15} 24.43 &\cellcolor{gray!15} 49.13 &\cellcolor{gray!15} 40.13 \\
  \multirow{2}{*}{Claude 3.5 Sonnet}
 &EM& 21.28 & 8.65 & 26.62 & 18.64 & 11.48 & 29.27 & 26.17 & 12.90 & 26.91 & 25.76 & 7.54 & 33.24 & 24.33 & 7.37 & 26.87 & 20.42 \\ 
  &\cellcolor{gray!15}PM& \cellcolor{gray!15}35.11 &\cellcolor{gray!15} 21.70 &\cellcolor{gray!15} 29.02 &\cellcolor{gray!15} 23.88 &\cellcolor{gray!15} 22.20 &\cellcolor{gray!15} 35.23 &\cellcolor{gray!15} 33.41 & \cellcolor{gray!15}27.60 &\cellcolor{gray!15} 36.99 &\cellcolor{gray!15} 32.77 &\cellcolor{gray!15} 18.67 &\cellcolor{gray!15} 38.69 &\cellcolor{gray!15} 28.60 &\cellcolor{gray!15} 17.70 &\cellcolor{gray!15} 32.21 &\cellcolor{gray!15} 30.29 \\
  \multirow{2}{*}{GPT-4o API}
 &EM& 43.98 & 21.16 & 34.53 & 37.66 & 16.41 & 37.82 & 32.77 & 18.01 & 40.53 & 45.90 & 17.90 & 38.83 & 34.29 & 13.19 & 40.80 & 33.58 \\ 
  &\cellcolor{gray!15}PM& \cellcolor{gray!15}45.08 & \cellcolor{gray!15}25.24 &\cellcolor{gray!15} 34.53 & \cellcolor{gray!15}38.71 &\cellcolor{gray!15} 24.08 &\cellcolor{gray!15} 38.82 &\cellcolor{gray!15} 35.77 &\cellcolor{gray!15} 25.02 &\cellcolor{gray!15} 40.97 & \cellcolor{gray!15}47.52 & \cellcolor{gray!15}21.97 &\cellcolor{gray!15} 39.39 & \cellcolor{gray!15}35.62 &\cellcolor{gray!15} 18.66 & \cellcolor{gray!15}42.04 & \cellcolor{gray!15}36.75 \\ \hline
  \rowcolor{gray!30} \multicolumn{18}{l}{\textit{2D VLM (Non-Semantic Top-View Map)}}\\
\multirow{2}{*}{Qwen2-VL 7B}
&EM& 37.25 & 22.06 & 37.17 & 39.78 & 20.11 & 53.01 & 35.36 & 19.77 & 48.67 & 40.89 & 19.32 & 52.23 & 33.47 & 17.65 & 47.51 & 34.40 \\
  &\cellcolor{gray!15}PM& \cellcolor{gray!15}40.83 &\cellcolor{gray!15} 28.04 &\cellcolor{gray!15} 37.17 &\cellcolor{gray!15} 41.58 & \cellcolor{gray!15}28.25 & \cellcolor{gray!15}53.80 & \cellcolor{gray!15}38.74 &\cellcolor{gray!15} 27.76 &\cellcolor{gray!15} 50.11 &\cellcolor{gray!15} 42.71 &\cellcolor{gray!15} 25.80 &\cellcolor{gray!15} 54.47 &\cellcolor{gray!15} 35.32 &\cellcolor{gray!15} 24.31 &\cellcolor{gray!15} 52.36 &\cellcolor{gray!15} 38.91 \\
\multirow{2}{*}{Qwen2-VL 72B}
&EM& 45.81 & 25.05 & 46.52 & 50.50 & 27.13 & 63.13 & 47.29 & 29.74 & 68.27 & 51.81 & 24.85 & 64.80 & 47.76 & 26.96 & 63.43 & 44.25 \\
  &\cellcolor{gray!15}PM& \cellcolor{gray!15}48.01 & \cellcolor{gray!15}30.63 & \cellcolor{gray!15}46.52 & \cellcolor{gray!15}51.85 & \cellcolor{gray!15}35.56 &\cellcolor{gray!15} 63.61 & \cellcolor{gray!15}50.06 & \cellcolor{gray!15}37.45 &\cellcolor{gray!15} 68.94 &\cellcolor{gray!15} 53.81 &\cellcolor{gray!15} 30.51 &\cellcolor{gray!15} 64.80 &\cellcolor{gray!15} 48.75 &\cellcolor{gray!15} 34.79 &\cellcolor{gray!15} 64.68 &\cellcolor{gray!15} 48.25 \\
 \multirow{2}{*}{LLaVA-OV 7B}
&EM& 39.30 & 22.70 & 39.81 & 37.97 & 20.49 & 41.61 & 37.81 & 22.05 & 41.53 & 43.99 & 19.91 & 49.72 & 37.71 & 18.72 & 44.53 & 34.81 \\
&\cellcolor{gray!15}PM&\cellcolor{gray!15} 41.04 &\cellcolor{gray!15} 27.44 & \cellcolor{gray!15}39.81 &\cellcolor{gray!15} 39.09 &\cellcolor{gray!15} 28.27 &\cellcolor{gray!15} 43.04 &\cellcolor{gray!15} 41.22 & \cellcolor{gray!15}30.11 &\cellcolor{gray!15} 43.05 &\cellcolor{gray!15} 44.88 &\cellcolor{gray!15} 25.62 &\cellcolor{gray!15} 51.96 &\cellcolor{gray!15} 38.57 &\cellcolor{gray!15} 26.22 &\cellcolor{gray!15} 47.51 &\cellcolor{gray!15} 38.60 \\
\multirow{2}{*}{LLaVA-OV 72B}
 &EM& 46.93 & 27.11 & 47.00 & 48.69 & 26.94 & 51.42 & 48.33 & 29.97 & 56.48 & 51.22 & 25.68 & 57.26 & 49.80 & 27.06 & 56.22 & 43.01 \\
 &\cellcolor{gray!15}PM& \cellcolor{gray!15}48.02 & \cellcolor{gray!15}32.95 &\cellcolor{gray!15} 47.00 &\cellcolor{gray!15} 49.74 &\cellcolor{gray!15} 35.09 &\cellcolor{gray!15} 52.06 &\cellcolor{gray!15} 51.65 &\cellcolor{gray!15} 38.83 &\cellcolor{gray!15} 56.89 &\cellcolor{gray!15} 52.39 &\cellcolor{gray!15} 31.80 &\cellcolor{gray!15} 57.68 &\cellcolor{gray!15} 50.73 &\cellcolor{gray!15} 34.11 &\cellcolor{gray!15} 58.33 &\cellcolor{gray!15} 46.83 \\ 
 \multirow{2}{*}{Claude 3.5 Sonnet}
&EM& 36.87 & 19.87 & 49.64 & 42.02 & 22.96 & 59.02 & 45.50 & 25.10 & 57.81 & 47.68 & 21.08 & 64.80 & 51.76 & 25.02 & 55.72 & 38.86 \\ 
 &\cellcolor{gray!15}PM&\cellcolor{gray!15} 48.39 &\cellcolor{gray!15} 32.67 &\cellcolor{gray!15} 50.60 &\cellcolor{gray!15} 47.64 &\cellcolor{gray!15} 34.49 &\cellcolor{gray!15} 65.11 &\cellcolor{gray!15} 51.01 &\cellcolor{gray!15} 39.29 &\cellcolor{gray!15} 71.23 &\cellcolor{gray!15} 52.64 &\cellcolor{gray!15} 34.28 & \cellcolor{gray!15}69.27 & \cellcolor{gray!15}54.22 &\cellcolor{gray!15} 35.97 &\cellcolor{gray!15} 61.82 & \cellcolor{gray!15}48.65 \\
 \multirow{2}{*}{GPT-4o API}
&EM& 49.51 & 25.52 & 51.32 & 52.18 & 26.47 & 61.71 & 50.21 & 29.15 & 64.95 & 58.52 & 27.44 & 61.73 & 50.29 & 25.41 & 60.70 & 45.50 \\ 
 &\cellcolor{gray!15}PM& \cellcolor{gray!15}50.62&\cellcolor{gray!15} 30.21 &\cellcolor{gray!15} 51.32 &\cellcolor{gray!15} 53.13 &\cellcolor{gray!15} 33.35 & \cellcolor{gray!15}62.18 &\cellcolor{gray!15} 52.63 &\cellcolor{gray!15} 35.94 &\cellcolor{gray!15} 65.70 &\cellcolor{gray!15} 60.16 &\cellcolor{gray!15} 33.33 &\cellcolor{gray!15} 62.15 & \cellcolor{gray!15}51.14 & \cellcolor{gray!15}31.59 &\cellcolor{gray!15} 61.82 &\cellcolor{gray!15} 48.82 \\
 \hline
 \rowcolor{gray!30} \multicolumn{18}{l}{\textit{3D VLM (RGB-D Video, Point Cloud)}}\\
\multirow{2}{*}{LEO 7B}
&EM& 21.13 & 7.79 & 7.91 & 27.62 & 2.28 & 10.44 & 16.22 & 3.46 & 19.10 & 20.66 & 2.00 & 8.94 & 16.24 & 1.36 & 10.45 & 14.83 \\
&\cellcolor{gray!15}PM& \cellcolor{gray!15}30.05 & \cellcolor{gray!15}14.54 & \cellcolor{gray!15}10.07 & \cellcolor{gray!15}31.60 & \cellcolor{gray!15}5.80 & \cellcolor{gray!15}17.01 & \cellcolor{gray!15}24.91 & \cellcolor{gray!15}7.60 & \cellcolor{gray!15}26.99 & \cellcolor{gray!15}32.12 & \cellcolor{gray!15}10.66 &\cellcolor{gray!15}13.83 & \cellcolor{gray!15}23.70 & \cellcolor{gray!15}8.63 & \cellcolor{gray!15}15.67 & \cellcolor{gray!15}22.40
 \\
 \hline
\multirow{2}{*}{LLaVA-3D 7B}
 &EM& 38.03 & 21.63 & 35.49 & 35.60 & 17.36 & 32.91 & 34.84 & 17.77 & 37.54 & 39.34 & 17.55 & 37.99 & 33.55 & 15.52 & 43.28 & 31.56 \\
 &\cellcolor{gray!15}PM& \cellcolor{gray!15}39.38 &\cellcolor{gray!15} 26.86 &\cellcolor{gray!15} 35.49 & \cellcolor{gray!15}36.74 &\cellcolor{gray!15} 25.04 &\cellcolor{gray!15} 33.57 &\cellcolor{gray!15} 38.72 &\cellcolor{gray!15} 26.22 &\cellcolor{gray!15} 38.18 &\cellcolor{gray!15} 40.44 &\cellcolor{gray!15} 24.03 &\cellcolor{gray!15} 38.13 &\cellcolor{gray!15} 34.88 &\cellcolor{gray!15} 22.34 &\cellcolor{gray!15} 44.15 &\cellcolor{gray!15} 35.23 \\
\hline
\bottomrule
\end{tabular}
\end{adjustbox}
\label{tab:8}
\end{table*}

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.6\linewidth]{figure/fig7.pdf}
    \vspace{-1em}
    \caption{Model and human SBERT scores across question types. Models struggle the most with direction-based questions, followed by scale-based and semantic questions.}
    \label{fig:14}
    \vspace{-1em}
\end{figure}

\begin{figure*}[ht]
    \centering
    \includegraphics[width=1.0\linewidth]{figure/example.pdf}
     \vspace{-1em}
      \caption{Qualitative Results. The changed object described in the context change is highlighted with a \textcolor{SciRed}{red} bounding box. Model outputs are shown in \textcolor{SciGreen}{green} for correct and \textcolor{SciRed}{red} for incorrect predictions.  Results indicate that while models struggle with most examples, 2D VLMs are more likely to provide partially correct answers.}
    \label{fig:15}
\end{figure*}

\clearpage
\setlength{\tabcolsep}{9pt} 
\begin{table*}[ht]
\centering
\caption{Comparison of model performance in direction-based questions on non-aligned versus aligned top-view maps. The results show no significant improvement, and in some cases, a decline in performance with aligned maps.}
\vskip 0.05in
\renewcommand{\arraystretch}{1.2}
\begin{adjustbox}{width=0.5\textwidth}
\begin{tabular}{l|cc|cc}
\toprule
\toprule
\multirow{2}{*}{\textbf{Model}} &  \multicolumn{2}{c|}{\textbf{Non-aligned}} 
& \multicolumn{2}{c}{\textbf{Aligned}} \\
& \multicolumn{1}{c}{EM} & PM & EM & PM \\ \hline
\rowcolor{gray!30} \multicolumn{5}{l}{\hspace{0pt}\textit{2D VLM (Non-Semantic Top-View Map)}}\\
 Qwen2-VL 7B & 16.70 & 23.93 & \textcolor{SciRed}{16.60} &\textcolor{SciRed}{23.40}\\
Qwen2-VL 72B & 19.21 & 25.95 & \textcolor{SciGreen}{19.99} & \textcolor{SciGreen}{27.08}\\
LLaVA-OV 7B  & 17.91 & 24.08 & \textcolor{SciGreen}{18.86} & \textcolor{SciGreen}{25.29}\\
LLaVA-OV 72B & 21.97 & 28.81 & \textcolor{SciRed}{21.69} & \textcolor{SciRed}{28.25}\\
GPT-4o API& 17.95 & 23.64 & \textcolor{SciGreen}{18.64} & \textcolor{SciGreen}{24.69} \\
\hline
\rowcolor{gray!30} \multicolumn{5}{l}{\textit{2D VLM (Semantic Top-View Map)}}\\
Qwen2-VL 7B & 20.22 & 27.18 & \textcolor{SciGreen}{21.94}& \textcolor{SciGreen}{29.17}\\
Qwen2-VL 72B & 26.77 & 33.65 & \textcolor{SciGreen}{33.95} & \textcolor{SciGreen}{43.28}\\
LLaVA-OV 7B  & 21.28 & 27.82 & \textcolor{SciGreen}{23.77} & \textcolor{SciGreen}{31.64} \\
LLaVA-OV 72B & 27.60 & 34.74 & \textcolor{SciGreen}{31.93} & \textcolor{SciGreen}{40.77}\\
GPT-4o API& 26.57 & 32.67 & \textcolor{SciGreen}{32.38} & \textcolor{SciGreen}{42.53} \\
\bottomrule
\end{tabular}
\end{adjustbox}
\label{tab:9}
\vspace{-1em}
\end{table*}

\subsection{Reasoning Prompts} \label{C.2}
The prompt template used for the main results in Table \ref{tab:1} is shown in Fig. \ref{fig:11}. The only difference between prompts for 3D VLMs, 2D VLMs, and LLMs is how the 3D scene is introduced, tailored to their specific scene representation formats, while all other parts remain consistent to ensure a fair comparison.

For the experiment in Table \ref{tab:2}, which assesses the effectiveness of the world frame, the prompt templates in Fig. \ref{fig:12} were used to evaluate model performance without a frame description.

Experiments in Tables \ref{tab:3} and \ref{tab:4} evaluate model performance in scenarios where the context change description is not given, with the exact prompt template provided in Fig. \ref{fig:13}.

\subsection{More Quantitative Results}\label{C.3}
We present the complete main results for each context change type and question type in Table \ref{tab:8}. 

The radar chart in Fig. \ref{fig:14} evaluates model performance across different question types using the SBERT metric, which measures cosine similarity between the text embeddings of predicted and ground-truth answers. These embeddings are generated using the SBERT model \cite{reimers2019sentence}. The results clearly show that most models struggle the most with directional questions, perform better on scale-based questions, and achieve their best performance on semantic questions.

Table \ref{tab:9} further highlights that, even when the top-view map is physically aligned with the world frame (i.e., no mental alignment required), model performance on direction-based questions shows no significant improvement, particularly for non-semantic maps. This indicates that current foundation models still face notable challenges in orientation understanding.

\subsection{More Qualitative Results}\label{C.4}
The qualitative results of model performance across various context change types are shown in Fig. \ref{fig:15}. Although models answer most questions incorrectly, 2D VLMs (LLaVA-OV 72B, GPT-4o) are more likely to provide partially correct answers, suggesting a relatively better capability for hypothetical reasoning.

\end{document}