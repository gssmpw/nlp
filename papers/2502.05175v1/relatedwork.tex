\section{Related work}
\label{sec:related_work}
\vspace{-0.5em}

\inlineheading{Few-view reconstruction}
Our problem setting is not few-view, but we highlight the differences here.
These methods focus on deleting reconstruction artifacts \cite{warburg2023nerfbusters,sabour2024spotlesssplats,goli2024bayes}, using sparse-view losses \cite{niemeyer2022regnerf}, leveraging depth and normal priors \cite{TurkuRMSRK2025}, or using generative models \cite{wu2024reconfusion,LiuCKTT2024,KantSVGRTG2023}, to complete sparse captures.
In contrast, our goal is to complete more realistic casual captures, with more input images, and to look outside of the captured training images.

\inlineheading{Multi-view generative models}
Many single-view to single new-view models exist where an input image is known and a target image is unknown \cite{liu2023zero,sargent2023zeronvs,seo2024genwarp,tewari2023diffusion}.
A few methods have increased the input context to multiple input images but still generate just one output view \cite{wu2024reconfusion,jin2024lvsm}.
Even fewer methods have increased both the number of inputs and the number of outputs.
CAT3D \cite{gao2024cat3d} uses 1 or 3 input images and generates 7 or 5 images, but never goes beyond a total sequence size of 8.
It also remains closed-source, which limits its impact.
We emphasize the importance of large sequence sizes in order to fit the entire casual capture in the context to make new content consistent with the observations.
Some video models have been fine-tuned for camera control \cite{he2024cameractrl,wang2024motionctrl,VanHWOSLTDZV2024} or use geometry conditioning \cite{yu2024viewcrafter,liu2024reconx,muller2024multidiff,MVDiffusion}, but these models generate smooth temporal videos, so can neither condition on the entire capture nor generate the many well-distributed views typical for the 3D reconstruction setting.

\inlineheading{LRMs conditioned on cameras}
Large reconstruction models (LRMs) that predict 3D have become popular to directly predict Gaussians \cite{xu2024grm,ZiwenTZBLHFX2024}.
Most methods assume camera poses as input, which may come from traditional methods like COLMAP \cite{schonberger2016structure} or newer data-driven methods \cite{wang2024dust3r,leroy2024grounding}.
LRMs are excellent at predicting pixel-aligned geometry but cannot inpaint unobserved areas of the scene.
Furthermore, they rely on camera poses, which may be unknown in casual captures.
We present a unified model for both tasks, such that when the camera is unknown, we can perform the ``uncalibrated scene completion" task of making a camera fly-through of the scene from a set of sparse unposed photos.
We model our camera pose prediction inspired by other data-driven approaches \cite{zhang2024cameras}, but use a raymap latent space, with ray origins and directions instead of Pl√ºcker coordinates.

\inlineheading{3D inpainting}
Current 3D inpainting methods such as SPIn-NeRF \cite{mirzaei2023spin} or NeRFiller \cite{weber2024nerfiller} rely on using 2D inpainting models within the NeRF 3D reconstruction framework to complete scenes \cite{ShihMHCCK2024}.
Most methods \cite{CaoYFWX2024,ChenCSXZ2024,LinKHLMKYT2024,ChenLP2024} focus on the SPIn-NeRF dataset, which is forward-facing and has much less camera motion than a typical casual capture.
NeRFiller has more challenging camera movement, so we consider this dataset for experiments.
However, none of these methods are conditioned on camera views when inpainting.
This makes it impossible for these methods to complete scenes with large unknown content.
This is because some of the generated views will be completely unknown, and without having context of the existing scene, it is unclear how to fill in the image.
In contrast, our approach is camera-pose conditioned.

\begin{figure*}[t]
\centering
\includegraphics[width=\linewidth]{figures/model-samples_small.pdf}
\vspace{-15px}
\caption{\label{fig:model_samples}%
    \textbf{Model samples.}
    Here we show generations from our model.
    For this setting, we provide pose input for all images.
    The top rows indicates which pixels are known, with yellow indicating unknown regions.
    The middle rows show the inpainted images after passing the entire sequence of size 16 (top rows) into the model for 24 denoising steps.
    The bottom rows show the ground truth, but note that this is not necessarily the only correct solution if the newly generated pixels are unobserved according to the masks.
    Notice that in the top example, the generations are self-consistent but different than the GT, which is entirely plausible.
    \vspace{-1em}
}
\end{figure*}

\vspace{-5px}