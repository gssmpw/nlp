\clearpage
\appendix
\setcounter{page}{1}
% \maketitlesupplementary
\setcounter{figure}{0}
\renewcommand{\thefigure}{A.\arabic{figure}}
\setcounter{table}{0}
\renewcommand{\thetable}{A.\arabic{table}}
\section{Appendix}
\label{sec:appendix}

Here we provide more information about our paper. There is additional info at our project page, \url{https://ethanweber.me/fillerbuster/}.

\section{More Results}

Please see the website for interactive results where we show more results in the Nerfbusters and Nerfiller datasets. We also have a new experiment in a section called \textbf{``Flexible Conditioning and Generation"} where we illustrate how our model can adapt to different number of input frames. We show results for this on the LERF dataset~\cite{kerr2023lerf}.

\begin{figure*}[t]
\centering
\includegraphics[width=\linewidth]{figures/model-samples-full-output_small.pdf}\vspace{-0.5em}
\caption{\label{fig:model_sample}%
    \textbf{Recovering missing information from a multi-view sequence.}
    Our model is flexible in that it can condition on any available information and recover the missing regions.
    We mark the conditioning as ``Masked images'', ``Masked origins'', and ``Masked directions''.
    The yellow regions are where information is not known.
    Given these conditions, we can recover all the missing information in the ``Inpainted'' rows.
    Notice that we are recovering fully unknown images, unknown poses, partial images, and we are generating two fully new images and poses at the same time (far right).
    The ground truth ``GT'' rows are provided as reference from the original capture, but the model only needs to follow the GT when the input provides the appropriate information. Our model does not require a ``GT'' reference and instead the model can be used to complete casually captured scenes where there is no reference, as shown in our paper.
    The origins and directions are all within the cube $[-1, 1]^3$ so can be visualized in RGB space.
}\vspace{-1em}
\end{figure*}


\section{Training Recipe}

Here we provide more training details about our model. Some of the data we use are as follows:
\begin{itemize}
  \item \textbf{ScanNet++~\cite{yeshwanth2023scannet++}:}
  $\sim$500 captures. This dataset is a collection of indoor scenes. We use their high-quality DSLR images data for training our model.

  \item \textbf{Shutterstock3D:}
  $\sim$2M 3D assets from Shutterstock. For the meshes, we render them from 24 views sampled on a sphere using Blender and the Cycles physically-based path tracer, similar to Objaverse \cite{deitke2022objaverse,deitke2023n}.

  \item \textbf{Shutterstock2D:}
  $\sim$400M image-text pairs from Shutterstock.
  These images are aesthetically pleasing. We use these images since they have text information and can help learn the long-tail of information that may not be present in our other datasets.
\end{itemize}

\noindent
We combine these datasets with various other multi-view data available to us. We train our model for multi-view image and raymap denoising.
We include single-view data in the training mix because we are training the model from scratch and wish to learn more concepts outside of of the more specific multi-view data.
50\% of the time, the model samples multi-view data, and the other 50\% of the time, the model samples single-view data.

\inlineheading{Multi-view data sampling}
We implement our multi-view dataloader by leveraging the Nerfstudio \cite{tancik2023nerfstudio} framework.
We use custom Nerfstudio DataParsers for each dataset type and train NeRFs on a subset of the captures with the Nerfacto \cite{tancik2023nerfstudio} method.
After confirming that our image crops and rays are sampled properly, we can confidently use the dataset for training our multi-view diffusion model.
Within each of these multi-view datasets, we uniformly sample frames with one of 5 strides: 1, 2, 4, 8, or completely random (\ie no stride).
We sample random crops within each image, and with 10\% chance we center the crop.
With 10\% chance, we drop out conditioning.
75\% of the time, we train for image denoising, and 25\% of the time, we train for raymap denoising.

\inlineheading{Single-view data sampling}
Our single-view data from Shutterstock2D is treated as a single-sequence set with a text prompt and unknown camera ray conditioning.
In this case, we mask out the raymaps as conditioning and we also mask out the loss for the noise prediction, to not penalize the predictions where we do not have ground truth.
We use cross-attention with text embeddings.
In practice, we only use the text to control coarse signals like like brightening the generation by using the word ``bright".

\inlineheading{Mixed-precision training}
We train with \texttt{bfloat16} to reduce memory requirements and speed up training.
To stabilize training, we found it was important to perform LayerNorm in \texttt{float32}, and to normalize keys and queries before attention operations~\cite{esser2024scaling}.

\inlineheading{Hyperparameters}
We use flow matching and logit normal sampling \cite{esser2024scaling}.
Our learning rate is constant at $10^{-4}$.
For multi-view training at 256\textsuperscript{2} resolution, we use a batch size of 4 sequences, and a sequence size of 10 for each GPU.
For single-view training at 256\textsuperscript{2} resolution, we use a batch size of 52 images, where in this case the sequence size is 1.
We train with 64 A100 GPUs across 8 nodes for 1M steps which takes roughly a month.
Finally, we train for an additional 100K steps with varying image resolution.
To implement this, we assign each GPU to a specific resolution with uniform probability and tune the batch size and sequence size to fit within the GPU memory. We use the sequence sizes \{20, 20, 10, 5, 2\} for resolutions \{64\textsuperscript{2}, 128\textsuperscript{2}, 256\textsuperscript{2}, 512\textsuperscript{2}, 1024\textsuperscript{2}\}, respectively.


\section{Method Details}

\subsection{Image and Raymap VAE}

We train two separate VAEs from scratch for compressing images and raymaps into latent representations.
We use the same convolutional architecture \cite{rombach2022high} for both VAEs, each with 4 down blocks and 4 up blocks.
For the image VAE, the output channels for each down block are 128, 256, 512, and 512.
We use 3 input channels and 16 dimensions for the latent dimension.
For the raymap VAE, we reduce the down block channel dimensions by a factor of 4, leading to dimensions: 32, 64, 128, and 128.
The raymap VAE takes as input 6 channels and has a latent dimension of 16.
For the raymap VAE, we remove group norm since we noticed it produced spot artifacts in the corner, consistent with previous findings \cite{MovieGen2024}, and we modified the padding in the network to use ``replicate'' padding.
The image VAE has 84M parameters and the raymap VAE has 5M parameters. The image VAE trains on all our data, while the raymap VAE trains on only our multi-view data.

\subsection{Model Architecture}

We provide an overview figure for our model in the main paper.
In this document, we provide a more detailed figure of our model in \cref{fig:model_architecture_all}.
Please see the figure and its caption for more details.
We also provide a model sample in \cref{fig:model_sample} showing a combination of known images, missing images, missing poses, partial images, and where both images and poses are unknown, resulting in unconditional generations.

\begin{figure*}[t]
\centering
\includegraphics[width=\linewidth,trim=0 200 0 140,clip]{figures/model-architecture-all.pdf}
\caption{\label{fig:model_architecture_all}%
    \textbf{\method{} model architecture.}
    Here we show the full model architecture from input (top) to output (bottom).
    We use the same notation as the main paper, where $I$ denotes images, $R$ denotes raymaps, and $\mathcal{M}$ denotes masks indicating where we know information or not.
    We inject various conditioning time and text conditioning into the model, as shown on the sides of the transformer.
    Timestep conditioning is necessary because our model is a diffusion transformer.
    Text, however, is optional.
    We include it to jointly train on our single-view image collection, which has text annotations, whereas the multi-view sequences are always trained with an empty string as the text prompt.
    The ``Ada LayerNorm Continuous'' is normalization with scale and shift modulation from the continuous time conditioning.
}
\vspace{10em}
\end{figure*}


\subsection{Raymap Prediction Discussion}

We solve for camera poses with a MultiDiffusion-style approach \cite{muller2024multidiff,weber2024nerfiller}, where we use a smaller sequence size as input to the model and average predictions in order to predict a larger context size.
We found this approach to give higher quality results for raymap recovery compared to passing in all images together in the same forward-pass.
However, this finding is not true for generating image content.
For the image prediction task, it's actually better to pass in all images in the same forward-pass.
We suspect this finding is because relative camera pose estimation will be unaffected by cameras that do not look in the same areas, but image generation will still be influenced by all context in the scene.

\subsection{Inference Speed}

We provide inference speeds in \cref{tab:inference_speeds}.
We report this on a NVIDIA A100.
Here we sample for 50 steps, but in practice one could sample for 24 steps and achieve similar quality results, while halving the inference time. Our model is on the order of seconds to generate a handful of multi-view images, which makes it useful to use with open-sourced reconstruction frameworks~\cite{tancik2023nerfstudio}.

\begin{table}[H]
\caption{\label{tab:inference_speeds}%
    \textbf{Inference time of our model.} 
    Measured for 50 denoising steps, including time for VAE encoding and decoding.
}
\centering
\small
\begin{tabular}{lrrrr}
\toprule
Resolution              & 4 Images & 8 Images & 16 Images & 32 Images \\ \midrule
256$\times$256          & 9 sec & 9 sec & 12 sec & 23 sec \\
512$\times$512          & 12 sec & 23 sec & 53 sec & 2 min \\
\bottomrule
\end{tabular}
\vspace{-1em}
\end{table}

\section{Evaluation Details}

\subsection{CAT3D-Sequence-Size Baseline}

For this baseline, we choose 3 images from a casual capture and use them for conditioning.
Then, we generate 20 rounds of inpaints, generating 6 new views each time to reach a total of 120 new generations, the same number that we generate with our method that uses larger sequence sizes.
The 6 target views are sampled at a random elevation and height on a cylinder.
In practice, CAT3D \cite{gao2024cat3d} uses 5 new views, but we use 6 to ease comparison in our implementation.

\subsection{Relative Rotation Accuracy Metric}

We use LoFTR~\cite{sun2021loftr} (outdoor weights) and Kornia~\cite{riba2020kornia} to recover the relative rotation between frames sampled within 1 second of each other (in our 10-second videos).
We sample 20 pairs per capture and report the average Relative Rotation Accuracy for our baselines.


\subsection{Nerfbusters Dataset}

We use the training splits of the Nerfbusters dataset \cite{weber2024nerfiller}.
Nerfbusters was captured with a training and evaluation video, but we only consider the training video, since it mimics the casual capture of an inexperienced photogrammetry user.


\subsection{NeRFiller Dataset}

We obtain the NeRFiller dataset and camera paths from the NeRFiller project \cite{weber2024nerfiller}.
We exclude the ``backpack'' capture for our analysis because it failed to reconstruct with Splatfacto~\cite{tancik2023nerfstudio,ye2024gsplat}. We suspect this is because the ``backpack'' capture is a forward-facing scene originally from SPIn-NeRF \cite{mirzaei2023spin}.


\section{Normal Regularization}

Our normal regularization loss $\mathcal{L}_\text{align} = \|\mathrm{sg}(N_\text{r}) - N_\text{d}\|^{2}_{2} + \|\mathrm{sg}(N_\text{r}) - N_\text{d}\|^{2}_{2}$ aligns 3D Gaussian normals with depth-derived normals. Specifically, $N_\text{r}$ are rendered normals from 3D Gaussians, oriented towards the camera, and $N_\text{d}$ are normals derived from rendered depth maps by backprojecting rendered depth and computing cross products for nearby points. We found that applying this regularization at the start of training can prevent the Gaussians from moving around gracefully during the start of optimization, however, if we start the regularization after 10K steps when the initial structure settles, we can significantly improve geometry. \cref{fig:normal_regularization} shows our results with and without normal regularization.
Note that we also use a TV loss to help create smoother surfaces rather than jagged ones, but it has little effect on the geometry overall.

\begin{figure}[H]
\centering
\includegraphics[width=\linewidth]{figures/normal-regularization_small_small.pdf}
\caption{\textbf{Normal regularization.} We find that aligning depth-derived normals with 3D Gaussian Splatting normals helps improve our reconstructions.}
\label{fig:normal_regularization}
\end{figure}