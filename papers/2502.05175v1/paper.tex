\begin{figure}
\twocolumn[{%
\renewcommand\twocolumn[1][]{#1}
\maketitle
\centering
\vspace{-2em}
\includegraphics[width=\textwidth]{figures/teaser_small.pdf}
\caption{\label{fig:teaser}%
    \textbf{Completing casual captures.}
    \method takes an incomplete casual capture which has many images (left) and conditions on these to create many consistent novel views, shown on the right with arrows. The original images and the new ones enable novel-view synthesis (right) that is much more complete compared to vanilla Gaussian Splatting trained on only the incomplete casual capture (left).
    % The project page is \url{https://ethanweber.me/fillerbuster/}.
    % \vspace{1em}
}
}]
\end{figure}

\begin{abstract}%
We present \method\footnote{Project page at \url{https://ethanweber.me/fillerbuster/}}, a method that completes unknown regions of a 3D scene by utilizing a novel large-scale multi-view latent diffusion transformer.
Casual captures are often sparse and miss surrounding content behind objects or above the scene.
Existing methods are not suitable for handling this challenge as they focus on making the known pixels look good with sparse-view priors, or on creating the missing sides of objects from just one or two photos.
In reality, we often have hundreds of input frames and want to complete areas that are missing and unobserved from the input frames.
Additionally, the images often do not have known camera parameters.
Our solution is to train a generative model that can consume a large context of input frames while generating unknown target views and recovering image poses when desired.
We show results where we complete partial captures on two existing datasets.
We also present an uncalibrated scene completion task where our unified model predicts both poses and creates new content.
Our model is the first to predict many images and poses together for scene completion.
\end{abstract}


\vspace{0.25em}
\section{Introduction}
\label{sec:introduction}
\vspace{-5px}

Photogrammetry has been around for decades \cite{Sturm2011} but only recently has become mainstream with novel-view synthesis techniques becoming high fidelity, such as NeRF \cite{mildenhall2021nerf} and Gaussian Splatting \cite{kerbl20233d}.
Widely used apps like Polycam \cite{polycam} or Flythroughs \cite{Flythroughs} mean that everyday people can go out and easily capture content.
Many such captures are done casually, which means the data is collected rather quickly and may miss large portions of the scene where the camera never looked.
Sometimes, the capture is just a handful of sparse photos, which makes obtaining camera poses challenging.

Reconstructing casually captured scenes is challenging because there is missing content to complete and it is not predictable where the missing content will be from capture to capture.
In contrast, the object-centric setting is much simpler as one can assume a canonical coordinate frame and sample missing views looking inward on a sphere.
Instead, we highlight the challenges of scenes and focus on this more general setting, where the input camera poses can be incredibly diverse.
Our goal is to fill in the missing information to enable an immersive view of the scene that feels complete, and where the rendered content can go beyond what is seen in training images, as illustrated in \cref{fig:teaser}.
To address this problem setting, we propose \method for recovering unknown 3D information from casually captured content.
This content may be casual videos, where a user quickly scans their phone through a scene, or this may be a sparse set of photos with unknown poses, e.g. from a vacation.
Given this data capture as input, our unified model can jointly complete the unobserved content and recover poses.

To improve casual captures, our key insight is to jointly model the image and camera distribution of existing casual captures by using a multi-view aware diffusion model.
Our approach is made possible by the large influx of captured data being recorded and uploaded online.

We design our model to handle a large and variable number of input and output frames.
This is in contrast to existing generative novel-view-synthesis (NVS) methods that are typically autoregressive, meaning the next generations are conditioned on previous generations.
More specifically, our problem setting is very different from the common settings of
(1) generation from text only (no images) \cite{HoelleCOJN2023,WangLWBLSZ2023,ShiWYLLY2024}
(2) from just one image \cite{liu2023zero,SargeLSHYZCLFSW2024}, or
(3) from two images with the goal of interpolating between them \cite{jin2024lvsm,yu2024viewcrafter}.
We present an overview of related work in \cref{fig:problem_setting}.
Our model can take in many images and camera poses, \eg 50 images, and the user can specify which content is known, which is unknown, and which should be completed.
The model can also take in partially complete images, unlike the current paradigm of assuming the input images are fully intact and known \cite{liu2023zero,gao2024cat3d}.
Concretely, we train a large-scale diffusion transformer with a flow-matching loss for inpainting in latent space \cite{rombach2022high}, conditioned on known images and camera poses (represented as raymaps) with the task of recovering the missing content, as illustrated in \cref{fig:model_architecture}.

We demonstrate our problem setting and the usefulness of our \method model on multiple tasks.
First, we show our model can complete casual captures by hallucinating large unknown regions.
Second, we introduce the task of ``uncalibrated scene completion", where the goal is to recover both the image poses and completed novel views.
Notably, we perform both tasks with our unified model.
Third, we show the multi-view inpainting task on the NeRFiller dataset \cite{weber2024nerfiller}, where we surpass prior work in quality and consistency.
Finally, we present an ablation of our modeling decisions and show our model's ability to gracefully handle different numbers of input images.

\begin{figure}[t]
\centering
\includegraphics[width=\linewidth]{figures/problem-setting.pdf}
\caption{\label{fig:problem_setting}%
    \textbf{Problem setting.}
    We illustrate our problem setting with respect to a non-exhaustive set of related work.
    Many works focus on scene synthesis (left) where one generates data from text or from a single image.
    Similarly many tackle novel-view synthesis (bottom) to synthesize new views of the input image content.
    Fewer works focus on scene completion where the task is to complete missing content in captures (top right).
}
\end{figure}


\begin{figure*}[t]
\centering
\includegraphics[width=\textwidth]{figures/model-architecture_small.pdf}
\caption{\label{fig:model_architecture}%
    \textbf{Model overview.}
    \method is trained on a large collection of multi-view images and poses (top and bottom of stacked images, respectively), which makes it useful for completing casual captures at inference time.
    More specifically, we are interested in four primary uses of the model:
    (1) conditioning on known images which have pose,
    (2) predicting new views where poses are provided,
    (3) predicting partial images where some pixels are known, or
    (4) recovering the camera poses when its unknown.
    Our model is a latent DiT trained to jointly model images and poses for any mixture of the input.
    In practice, our poses are 6-channel raymaps encoding ray origins and directions.
    \looseness-1
    \vspace{-1.7em}
}
\end{figure*}


\section{Related work}
\label{sec:related_work}
\vspace{-0.5em}

\inlineheading{Few-view reconstruction}
Our problem setting is not few-view, but we highlight the differences here.
These methods focus on deleting reconstruction artifacts \cite{warburg2023nerfbusters,sabour2024spotlesssplats,goli2024bayes}, using sparse-view losses \cite{niemeyer2022regnerf}, leveraging depth and normal priors \cite{TurkuRMSRK2025}, or using generative models \cite{wu2024reconfusion,LiuCKTT2024,KantSVGRTG2023}, to complete sparse captures.
In contrast, our goal is to complete more realistic casual captures, with more input images, and to look outside of the captured training images.

\inlineheading{Multi-view generative models}
Many single-view to single new-view models exist where an input image is known and a target image is unknown \cite{liu2023zero,sargent2023zeronvs,seo2024genwarp,tewari2023diffusion}.
A few methods have increased the input context to multiple input images but still generate just one output view \cite{wu2024reconfusion,jin2024lvsm}.
Even fewer methods have increased both the number of inputs and the number of outputs.
CAT3D \cite{gao2024cat3d} uses 1 or 3 input images and generates 7 or 5 images, but never goes beyond a total sequence size of 8.
It also remains closed-source, which limits its impact.
We emphasize the importance of large sequence sizes in order to fit the entire casual capture in the context to make new content consistent with the observations.
Some video models have been fine-tuned for camera control \cite{he2024cameractrl,wang2024motionctrl,VanHWOSLTDZV2024} or use geometry conditioning \cite{yu2024viewcrafter,liu2024reconx,muller2024multidiff,MVDiffusion}, but these models generate smooth temporal videos, so can neither condition on the entire capture nor generate the many well-distributed views typical for the 3D reconstruction setting.

\inlineheading{LRMs conditioned on cameras}
Large reconstruction models (LRMs) that predict 3D have become popular to directly predict Gaussians \cite{xu2024grm,ZiwenTZBLHFX2024}.
Most methods assume camera poses as input, which may come from traditional methods like COLMAP \cite{schonberger2016structure} or newer data-driven methods \cite{wang2024dust3r,leroy2024grounding}.
LRMs are excellent at predicting pixel-aligned geometry but cannot inpaint unobserved areas of the scene.
Furthermore, they rely on camera poses, which may be unknown in casual captures.
We present a unified model for both tasks, such that when the camera is unknown, we can perform the ``uncalibrated scene completion" task of making a camera fly-through of the scene from a set of sparse unposed photos.
We model our camera pose prediction inspired by other data-driven approaches \cite{zhang2024cameras}, but use a raymap latent space, with ray origins and directions instead of Plücker coordinates.

\inlineheading{3D inpainting}
Current 3D inpainting methods such as SPIn-NeRF \cite{mirzaei2023spin} or NeRFiller \cite{weber2024nerfiller} rely on using 2D inpainting models within the NeRF 3D reconstruction framework to complete scenes \cite{ShihMHCCK2024}.
Most methods \cite{CaoYFWX2024,ChenCSXZ2024,LinKHLMKYT2024,ChenLP2024} focus on the SPIn-NeRF dataset, which is forward-facing and has much less camera motion than a typical casual capture.
NeRFiller has more challenging camera movement, so we consider this dataset for experiments.
However, none of these methods are conditioned on camera views when inpainting.
This makes it impossible for these methods to complete scenes with large unknown content.
This is because some of the generated views will be completely unknown, and without having context of the existing scene, it is unclear how to fill in the image.
In contrast, our approach is camera-pose conditioned.

\begin{figure*}[t]
\centering
\includegraphics[width=\linewidth]{figures/model-samples_small.pdf}
\vspace{-15px}
\caption{\label{fig:model_samples}%
    \textbf{Model samples.}
    Here we show generations from our model.
    For this setting, we provide pose input for all images.
    The top rows indicates which pixels are known, with yellow indicating unknown regions.
    The middle rows show the inpainted images after passing the entire sequence of size 16 (top rows) into the model for 24 denoising steps.
    The bottom rows show the ground truth, but note that this is not necessarily the only correct solution if the newly generated pixels are unobserved according to the masks.
    Notice that in the top example, the generations are self-consistent but different than the GT, which is entirely plausible.
    \vspace{-1em}
}
\end{figure*}

\vspace{-5px}
\section{Method}
\label{sec:method}
\vspace{-5px}

We first explain our model's details for jointly modeling image completion and poses (\cref{sec:model}), and then explain how to use \method for casual scene completion with our model being helpful for 3D reconstruction (\cref{sec:multi-view-completion}).

\subsection{\method Model}\label{sec:model}

We propose a latent diffusion transformer that denoises multiple input images and calibrated camera poses with masks indicating known and unknown regions.
There are $N$ elements in a sequence with images $I_i \in \mathbb{R}^{H \times W \times 3}$, raymaps $R_i \in \mathbb{R}^{H \times W \times 6}$ with origin and direction per pixel, valid image masks $\mathcal{M}^\text{I}_i \in \mathbb{R}^{H \times W}$, and valid ray masks $\mathcal{M}^\text{R}_i \in \mathbb{R}^{H \times W}$, where 1 indicates known conditioning information and 0 indicates unknown pixels.
Our goal is to predict all images and raymaps given only the known information, i.e., $p(I,R \mid I \odot \mathcal{M}^\text{I}, R \odot \mathcal{M}^\text{R})$.
We use ``sequence'' to refer to multiple images and cameras from the same capture, and ``sequence size'' for how many images are denoised together.

\inlineheading{Model architecture}
%
The architecture is designed for latent inpainting \cite{rombach2022high}, taking in any combination of known and unknown images and raymaps, and predicting the missing values.
We use a DiT architecture \cite{peebles2023scalable} and train with the flow matching objective \cite{lipman2022flow}.
We train separate VAEs for images and poses encoded as raymaps, where $\mathcal{E}^\text{I}$ denotes the image encoder and $\mathcal{E}^\text{R}$ denotes the raymap encoder.
Both encoders compress the spatial resolution by a factor of 8$\times$ and output a $d$-dimensional representation.
We set $d=16$ for both encoders.
Let $z^\text{I}_{i} = \mathcal{E}^\text{I}(I_{i})$ and $z^\text{R}_{i} = \mathcal{E}^\text{R}(R_{i})$ denote the compressed latent image and raymap, respectively.
Let $\mathcal{D}$ denote a downscaling operation that reduces the spatial resolution by the same factor as the encoders.
We add noise to $z_{i}$ as $\tilde{z}_{i,t} = (1 - t) z + t \epsilon$, then prepare the sequence as
%
\begin{align}
\begin{split}
s_{i,t} = {} & \tilde{z}^\text{I}_{i,t} \oplus \mathcal{E}^\text{I}(I_{i} \odot \mathcal{M}_{i}^\text{I}) \oplus \mathcal{D}(\mathcal{M}_{i}^\text{I}) \oplus {} \\
 & \tilde{z}^\text{R}_{i,t} \oplus \mathcal{E}^\text{R}(R_{i} \odot \mathcal{M}_{i}^\text{R}) \oplus \mathcal{D}(\mathcal{M}_{i}^\text{R}) \text{,}
\label{eq:sequence_equation}
\end{split}
\end{align}
%
where $\oplus$ denotes concatenation of the noisy latents, known image and ray latents, and the masks themselves.
The noisy sequence $s_{t} \in \mathbb{R}^{N \times H \times W \times (4d+2)}$ is patchified, and positionally embedded (described later), then passed through the transformer model $\mathcal{F}$ to predict the denoised latent images and raymaps as $\{z^\text{I}, z^\text{R}\} = \mathcal{F}(s)$.
Our VAEs have a convolutional architecture \cite{rombach2022high} and train with KL \cite{kingma2013auto}, adversarial \cite{goodfellow2014generative}, and L1 reconstruction losses.
Our transformer architecture is ``DiT-L/2" \cite{peebles2023scalable} with a latent patch size of 2$\times$2 and 24 layers of multi-head self-attention.
Our model only has 650M parameters – small enough to fit on most GPUs and fast enough to use with open-source 3D reconstruction tools.

\inlineheading{Raymap coordinate convention}
%
%
Raymaps comprise per-pixel ray origins and world-space unit directions.
At training time, we randomly choose one camera from our sequence to be at the origin and oriented upright.
We also randomly rotate and rescale the cameras for augmentation, and ensure that origins are always within the cube $[-1, 1]^3$.
%

\inlineheading{Masking out regions}
%
During training, we mask out information from the images and/or the raymaps; the task is to predict the denoised sequence from partial noise.
We apply masking at the pixel level before VAE encoding (\cref{eq:sequence_equation}) to enable precise control over which pixels are known or unknown.
We use a mixed masking strategy: some images are known, some are unknown, and some are partially unknown with randomly rotated rectangles, as illustrated in \cref{fig:model_samples}.
We dropout image and raymap masking with a 10\% chance to enable classifier-free guidance \cite{ho2022classifier}.

\inlineheading{Token positional embeddings}
%
We use two forms of positional embeddings to enable varying sequence lengths to be generated at inference time.
\textit{1) 2D layout embeddings} encode the layout of the image with fixed sinusoidal embeddings.
\textit{2) Index embeddings} are more unique for our setting, where we add an unordered index descriptor to each token coming from the same image.
More specifically, the full sequence $s$ is first patchified and projected into patches $p$.
It is then augmented with positional embeddings as $p' = \psi_\text{2D}(p) + \psi_\text{Idx}(p)$, where $\psi_\text{2D}$ is sinusoidal embeddings to encode the 2D layout of each patch within the image itself \cite{vaswani2017attention}, and $\psi_\text{Idx}$ to encode which index in the sequence the patch is from.
During training, $\psi_\text{Idx}(p)$ randomly samples a frequency for each image and then adds that value to each patch of the same image.
During inference, the frequencies are chosen with uniform spacing and applied in the same way so each image has a unique identifier.
This helps support generating longer sequences at inference time beyond the training lengths, which we show in \cref{sec:model-design}.
Prior multi-view diffusion transformer models do not incorporate this, and we show that this is useful for generating longer sequences.

\inlineheading{Training and inference details}
%
We train our model \textit{from scratch} on a collection of datasets including
ScanNet++ \cite{yeshwanth2023scannet++}
and a corpus of Shutterstock data including 2D images and 3D asset renderings.
We train our final model on 64 A100 GPUs for approximately a month.
We first train at 256$\times$256 resolution for 1M iterations, and then fine-tune for 100K iterations with resolutions varying from 64$\times$64 to 1024$\times$1024 with sequence lengths between 20 and 2, depending on how many images fit in GPU memory for a given resolution.
See our appendix for additional details.
For inference, we apply classifier-free guidance (CFG) by dropping out both image and raymap conditioning for an unconditional prediction.
We use spatially varying CFG weights of 7 for the unknown regions and 1.1 for the known regions to avoid saturation artifacts since the task of copying the conditioning is much easier than predicting new information \cite{blattmann2023stable}.
\looseness-1


\subsection{Multi-View Scene Completion}\label{sec:multi-view-completion}

Here we explain how to use our model to complete scenes.

\begin{figure*}[t]
\centering
\includegraphics[width=\linewidth]{figures/completing-casual-captures_small.pdf}\vspace{-0.5em}
\caption{\label{fig:completing_casual_captures}%
    \textbf{Completing casual captures.}
    Here we demonstrate our ability to complete casual captures from the training splits of the Nerfbusters dataset \cite{warburg2023nerfbusters}.
    On the left, we show the input captures and some representative images.
    3DGS (Splatfacto) cannot add missing details so the capture remains incomplete.
    Our CAT3D baseline conditions on 3 images and generates 6 images at a time, so it cannot produce consistent content.
    \method conditions on 16–40 images to generate 24 novel views, and obtains the most consistent results.
}\vspace{-1em}
\end{figure*}

\inlineheading{Variable sequence lengths}
%
Our index embedding enables changing the sequence length at inference time.
We leverage this property to generate many images at the same time for inpainting incomplete scenes, since NeRF and Gaussian splatting typically require many views to create a scene.


\inlineheading{Multi-view inpainting for scene completion}
%
We complete scenes by generating novel views and adding them to our existing dataset, then optimizing 3DGS \cite{kerbl20233d}.
We avoid the need for an SDS-like optimization approach~\cite{poole2022dreamfusion,haque2023instruct} because our model can generate many consistent images with a large sequence size.
To complete scenes with large camera movement, we add new views to the scene that look in all directions.
We first inpaint many ($\sim$25) ``anchor'' frames, and then condition on these frames to generate more novel views, as in CAT3D \cite{gao2024cat3d}.
The key difference is that we can handle much larger sequence sizes than CAT3D, which operates on at most 3 images for conditioning.
Furthermore, unlike CAT3D, we can also complete scenes with partial masks.
To complete these scenes, we inpaint the images themselves and update the dataset with the new pixels.

\inlineheading{Normal regularization}
%
We find that regularizing Gaussian splat geometry towards the end of optimization can help improve results. Specifically, we apply a total-variation smoothness loss on rendered normals \cite{fridovich2022plenoxels}, and we also align our depth-derived surface normals with rendered normals (similar to \citet{verbin2024nerf} but using Gaussians instead of NeRF).
This second loss is $\mathcal{L}_\text{align} = \|\mathrm{sg}(N_\text{r}) - N_\text{d}\|^{2}_{2} + \|\mathrm{sg}(N_\text{r}) - N_\text{d}\|^{2}_{2}$, where $N_\text{r}$ are rendered normals from 3D Gaussians, oriented towards the camera, and $N_\text{d}$ are normals derived from rendered depth maps.
We apply our normal regularizations after the initial geometry has taken form, at approximately 10K steps.
Please see the appendix for more details.

\vspace{-7px}
\section{Evaluation}
\vspace{-8px}
We first show our casual scene capture completion results on the Nerfbusters dataset, and then demonstrate the ``uncalibrated scene completion'' task on data captured ourselves.
Next, we show results on the NeRFiller dataset, where we surpass prior work in quality and consistency.
Finally, we evaluate our model design choices.
Note that we choose to use 3D Gaussian splatting \cite{kerbl20233d} for our reconstruction experiments rather than NeRF \cite{mildenhall2021nerf} because 3DGS is fast to train and thus gaining popularity among casual capture users.


\subsection{Completing Casually Captured Scenes}
\label{sec:completing_casual_captures}
\vspace{-5px}

\inlineheading{Setting}
%
Here we show results for completing casually captured scenes.
We choose the Nerfbusters dataset \cite{warburg2023nerfbusters} for this setting because it mimics the casual captures of an inexperienced user.
Our goal is to take these partial captures and to complete them – either by completing geometry or adding context to the capture.
We compare the following methods:
\begin{enumerate*}[(1)]
\item \textit{3DGS} (Splatfacto \cite{tancik2023nerfstudio}, which uses the gsplat library~\cite{ye2024gsplat}, with no inpainting),
\item \textit{NeRFiller} \cite{weber2024nerfiller} (NeRFiller inpainting, which we note is not suitable for this setting where the new views do not have partial masks),
\item \textit{CAT3D-sequence-size} (ours with CAT3D-sized conditioning, where we condition on 3 images and generate 6 images a time, further described in the appendix), and
\item \textit{\method} (our complete method, where we condition on 16 views and generate 24 images at a time).
\end{enumerate*}
We perform multiple rounds of inpainting to reach $\sim$100 new views that are added to the scene. We show this procedure in \cref{fig:camera_sampling}, where we sample cameras on a cylinder looking at random directions.
Unfortunately, CAT3D \cite{gao2024cat3d} is not open-sourced, so we cannot compare with it directly. 

\begin{figure}[t]
\centering
\includegraphics[width=\linewidth]{figures/camera-sampling_small.pdf}
\caption{\label{fig:camera_sampling}%
    \textbf{Novel-view sampling.}
    We start with a casual capture (top left) and condition on 16 of the images to generate 24 anchor views simultaneously (top right).
    We then condition on the casual capture and anchors to densify views (bottom).
    We repeat the dense stage for multiple rounds to reach $\sim$100 novel views in total.
    \vspace{-2em}
}
\end{figure}

\inlineheading{Results}
%
We show qualitative results in \cref{fig:completing_casual_captures}.
We find that \textit{3DGS} cannot add any additional detail, leading to large unknown regions when rendering novel views away from the training images.
Naïvely adapting \textit{NeRFiller} to this challenging setting fails drastically because the inpainting is not conditioned on pose and are therefore random, adding random colors to the scene.
\textit{CAT3D-sequence-size} is more consistent but introduces artifacts due to the limited context size.
Our proposed method \textit{Fillerbuster}, with large sequence sizes for conditioning and generation, of the most consistent.
We design a new metric, to evaluate this task since we do not have ground-truth when hallucinating novel scene content.
We are inspired by previous work that measures reconstruction error \cite{fridman2024scenescape} or distance to epipolar lines \cite{muller2024multidiff,yu2023long}.
Specifically, we render a novel-view camera trajectory and estimate the poses between nearby frames.
We use off-the-shelf correspondences \cite{sun2021loftr} and classical methods \cite{hartley1997defense} to obtain a relative rotation, which we compare with the ground truth.
We report relative rotation accuracy in \cref{fig:completing_casual_captures_metrics} and find the qualitative results to be consistent with camera-pose estimation accuracy.

\begin{figure}[t]
\centering
\includegraphics[width=\linewidth]{figures/relative-rotation-accuracy.pdf}\vspace{-0.5em}
\caption{\label{fig:completing_casual_captures_metrics}%
    \textbf{Completing casual captures metrics.}
    We report relative rotation accuracy for nearby frames in a novel-view video.
    We use off-the-shelf correspondences \cite{sun2021loftr}
    to estimate camera rotation and compare with the ground truth.
    \method{} produces the most consistent videos from a pose-estimation perspective.
    \vspace{-1em}
}
\end{figure}


\subsection{Uncalibrated Scene Completion}

Here we consider the task of scene completion, starting from a collection of 16 unposed photos.
We show that our unified image-and-pose model supports such casual captures by predicting camera poses and then generating a fly-through of the scene, completing unknown content where it's missing.

\begin{figure*}[t]
\centering
\includegraphics[width=\linewidth]{figures/uncalibrated-scene-completion_small.pdf}
\vspace{-15px}
\caption{\label{fig:uncalibrated_scene_completition}%
    \textbf{Uncalibrated scene completion.}
    We capture some scenes with an iPhone 14 Pro and run our framework. We start from 16 uncalibrated and unposed images (left), and we use our model to both predict camera pose (middle) and generate completed views (right).
    We show our predicted cameras in red compared and unknown views we will sample in black.
    Our cameras are plausible and useful for conditioning on to generate new views.
    We show just 4 views here and the full videos on the project page.
}
\vspace{-15px}
\end{figure*}


\inlineheading{Setting}
%
Given the set of images, we can denoise the raymaps conditioned only on the images.
We use joint denoising tiling with a window size of 8 images and average 8 times per denoising step.
This is similar to MultiDiffusion \cite{bar2023multidiffusion} or NeRFiller's joint denoising \cite{weber2024nerfiller} (see appendix).
Then, we solve for the pinhole camera parameters that match backprojected rays to the denoised rays, taking only 5 seconds to converge for 16 images.
Next, we condition on our predicted rays to generate novel views to complete the scenes.
We create a camera path by fitting a 2D ellipse to our posed images and point cameras inward.
Here we use a sequence size of 48: 16 input images with generated poses, plus 32 generated images with specified poses, but note that this decision is flexible.

\begin{figure}[t]
\centering
\includegraphics[width=\linewidth]{figures/nerfiller-dataset-results_small.pdf}
\vspace{-15px}
\caption{\label{fig:nerfiller_results}%
    \textbf{NeRFiller dataset novel-views.}
    We complete NeRFiller scenes \cite{weber2024nerfiller} with higher quality and control than their method.
}
\vspace{-1em}
\end{figure}


\inlineheading{Results}
%
Our joint modeling of poses and images is convenient because we do not rely on external structure-from-motion; instead, our unified model can handle both tasks gracefully.
We show qualitative results of our video poses in \cref{fig:uncalibrated_scene_completition}.
Please see our project page for video results.

\begin{table}[t]
\caption{\label{tab:completing_regions}%
    \textbf{Completing masked 3D regions.} 
    On the NeRFiller dataset \cite{weber2024nerfiller}, we report novel-view synthesis metrics where we compare the rendered images with the inpainted images.
    In parentheses, we report numbers without using our normal regularizations.
    No normal regularization lets the network cheat to explain inconsistencies, leading to slightly improved but misleading metrics.
    Overall, we find \method is much more consistent than NeRFiller.
}
\centering
\small
\begin{tabular}{lrrr}
\toprule
Method              & PSNR $\uparrow$ & SSIM $\uparrow$ & LPIPS $\downarrow$ \\ \midrule
NeRFiller             & 25.57 (25.94) & 0.89 (0.88) & 0.182 (0.194) \\
\method                & \textbf{29.60} (30.65) & \textbf{0.92} (0.93) & \textbf{0.096} (0.069) \\
\bottomrule
\end{tabular}
\vspace{-1em}
\end{table}


\subsection{Completing Masked 3D Regions}

\Cref{fig:nerfiller_results} shows results where we inpaint scenes from the NeRFiller dataset and compare against the NeRFiller method \cite{weber2024nerfiller}.
For both NeRFiller and \method, we inpaint 32 equally-spaced training images and then train 3D Gaussian splatting with our normal regularization.
Notably, unlike in NeRFiller, we do not use depth supervision or iterative dataset updates.
We instead inpaint once at the start of training to directly assess multi-view inpainting quality, regardless of any SDS-style optimizations that encourage consistency.
We also report reconstruction metrics in \cref{tab:completing_regions} by comparing our 32 inpainted images with the final renderings from the same 32 viewpoints.
Our method is more consistent than NeRFiller, with and without our normal regularization.

\input{tables/model_design}


\subsection{Model Design Ablations}
\label{sec:model-design}

We evaluate our model choices with the Nerfstudio dataset \cite{tancik2023nerfstudio} because it consists of 10 well-captured static scenes that look in all directions.
In \cref{tab:model_design}, we report novel-view synthesis metrics for 256$\times$256 resolution images for varying sequence lengths.
For each scene, we randomly sample a sequence of size $N$ image crops of this resolution.
We condition on $N/4$ full crops and $N/4$ partial crops, and generate all the missing information (see \cref{fig:model_samples} for examples).
We repeat this procedure 50 times for sequences of length $N \!\in\! \{8,16,32\}$, and report averaged metrics.
Each model is trained from scratch for 100K iterations with sequences of $N \!=\! 8$ images.
Inspired by \citet{esser2024scaling}, we also report validation losses (``VAL'') on these samples for 20 equally-spaced time steps.
This metric captures generation quality unlike the other metrics that evaluate reconstruction.

\begin{figure}%[H]
\centering
\includegraphics[width=\linewidth]{figures/ablations_small.pdf}
\caption{\label{fig:qualitative_model_ablations}%
    \textbf{Qualitative results for model ablations}.
    We provide pose for all images and perform completion in the unknown regions (yellow).
    Without index embeddings, the model fails to reason about which image the tokens are coming from, so the results are patchy and blurry.
    Without training for pose prediction, the generations are worse than training for pose prediction (\method (100K)).
    The last rows show our final model and GT for reference.
}
\vspace{-10px}
\end{figure}

\Cref{tab:model_design} compares the following ablations (see \cref{fig:qualitative_model_ablations} for visual examples):
\textit{``no-index-emb"} does not use index embeddings and instead relies on the raymaps to understand the token relationships.
This makes the task harder and the model performs worse.
\textit{``fixed-index-emb"} uses a fixed number of index embeddings, preventing it from generalizing to more than 8 images.
To go beyond 8 images, more index embeddings are introduced, which this model variant cannot handle.
Notice the low PSNR of this setting for 16-views.
\textit{``no-poses"} does not denoise raymaps, and interestingly, we find that when it does not learn to predict camera pose, the model performs worse at image generation, indicating that image and pose predictions are complementary tasks.
Finally, \textit{``random-poses"} randomizes the poses instead of forcing them to be upright with one camera at the origin.
Our final model is trained for much longer and is shown at the bottom of the table, obtaining the best ``VAL'' results.
We note that the metrics vary from 8-views to 32-views because more conditioning and more unknowns are introduced, and reconstruction metrics are imperfect for assessing the models.

\vspace{-10px}
\section{Conclusion}

Many 3D casual captures are missing content because the camera does not look everywhere.
To recover these missing details, we present \method{}, a large-scale multi-view diffusion model, to complete missing regions or recover camera poses when they are not available.
We show our model is useful at completing casual captures, and we introduce an ``uncalibrated scene completion'' task where we generate novel missing content from unposed images from our own mobile captures.
We also outperform NeRFiller on its setting, where partial views are known and there are masks to complete.
Lastly, we present important model design decisions to enable large sequence conditioning and generation.

The area of scene completion is incredibly exciting and there are many avenues to explore beyond what is presented here.
For example, choosing camera paths can be challenging because cameras should not be sampled inside of objects or behind walls.
For the Nerfbusters dataset, we sample on a cylinder (\cref{fig:camera_sampling}), but this may not be suitable for more catered settings, such as looking under a desk or behind a corner.
Consequently, a method that predicts where to sample next could be valuable.
We also note that our model's generations become worse when the generated cameras are very far away from the conditioning views, \eg when we step back too far from the input views.
Incorporating more diverse training data, or specifically rendering distant viewpoints in simulations, may be useful.
We proposed an initial step towards the challenging problem of casual capture scene completion. We expect the results will further improve with a larger model, bigger compute budget, and more diverse training data.

\vspace{1em}
\inlineheading{Acknowledgements}
We would like to thank
Timur Bagautdinov,
Jin Kyu Kim,
Julieta Martinez,
Su Zhaoen,
Rawal Khirodkar,
Nir Sopher,
Nicholas Dahm,
Alexander Richard,
Bob Hansen,
Stanislav Pidhorskyi,
Tomas Simon,
David McAllister,
Justin Kerr,
Frederik Warburg,
Riley Peterlinz,
Evonne Ng,
Aleksander Holynski,
Artem Sevastopolsky,
Tobias Kirschstein,
Chen Guo,
Nikhil Keetha,
Ayush Tewari,
Changil Kim,
Lorenzo Porzi,
Corinne Stucker,
Katja Schwarz, and
Julian Straub
for helpful discussions, technical support, and/or sharing relevant \mbox{knowledge}.