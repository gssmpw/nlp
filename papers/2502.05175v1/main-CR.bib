% Encoding: UTF-8

@String{AAAI           = {AAAI}}
@String{ACCV           = {ACCV}}
@String{ACCVW          = {Proceedings of ACCV Workshops}}
@String{AFRIGRAPH      = {Proceedings of the International Conference on Computer Graphics, Virtual Reality, Visualisation and Interaction in Africa (Afrigraph)}}
@String{APGV           = {Proceedings of the Symposium on Applied Perception in Graphics and Visualization}}
@String{apr            = {April}}
@String{aug            = {August}}
@String{BMVC           = {BMVC}}
@String{CAe            = {CAe}}
@String{CAG            = {Computers \& Graphics}}
@String{CGA            = {IEEE Comput. Graph. Appl.}}
@String{CGF            = {Comput. Graph. Forum}}
@String{CGI            = {Proceedings of Computer Graphics International (CGI)}}
@String{CHI            = {CHI}}
@String{CORL           = {Proceedings of the Conference on Robot Learning (CoRL)}}
@String{CVIU           = {Comput. Vision Image Understanding}}
@String{CVM            = {Computational Visual Media}}
@String{CVMP           = {CVMP}}
@String{CVPR           = {CVPR}}
@String{CVPRW          = {CVPR Workshops}}
@String{DAGM           = {Pattern Recognition (Proceedings of the DAGM Symposium)}}
@String{dec            = {December}}
@String{ECCV           = {ECCV}}
@String{ECCVW          = {ECCV Workshops}}
@String{EGSR           = {Proceedings of the Eurographics Symposium on Rendering}}
@String{EGWR           = {Proceedings of the Eurographics Workshop on Rendering}}
@String{ETRA           = {ETRA}}
@String{feb            = {February}}
@String{FG             = {Proceedings of the International Conference on Automatic Face and Gesture Recognition (FG)}}
@String{GCPR           = {Pattern Recognition (Proceedings of the German Conference on Pattern Recognition)}}
@String{GI             = {Graphics Interface}}
@String{GRAPHITE       = {Proceedings of the International Conference on Computer Graphics and Interactive Techniques in Australasia and South East Asia (GRAPHITE)}}
@String{HPG            = {Proceedings of High Performance Graphics (HPG)}}
@String{I3D            = {Proceedings of the Symposium on Interactive 3D Graphics and Games (I3D)}}
@String{ICASSP         = {ICASSP}}
@String{ICCP           = {ICCP}}
@String{ICCV           = {ICCV}}
@String{ICCVW          = {ICCV Workshops}}
@String{ICIP           = {ICIP}}
@String{ICLR           = {ICLR}}
@String{ICME           = {ICME}}
@String{ICML           = {ICML}}
@String{ICPR           = {ICPR}}
@String{ICRA           = {ICRA}}
@String{IJCV           = {International Journal of Computer Vision}}
@String{IROS           = {IROS}}
@String{ISM            = {Proceedings of the International Symposium on Multimedia (ISM)}}
@String{ISMAR          = {ISMAR}}
@String{jan            = {January}}
@String{JCGT           = {Journal of Computer Graphics Techniques (JCGT)}}
@String{jul            = {July}}
@String{jun            = {June}}
@String{LNCS           = {Lecture Notes in Computer Science}}
@String{mar            = {March}}
@String{may            = {May}}
@String{MULTIMEDIA     = {ACM Multimedia}}
@String{NeurIPS        = {NeurIPS}}
@String{NeurIPSW       = {NeurIPS Workshops}}
@String{NIPS           = {NIPS}}
@String{NIPSW          = {NIPS Workshops}}
@String{nov            = {November}}
@String{NPAR           = {Proceedings of the International Symposium on Non-Photorealistic Animation and Rendering (NPAR)}}
@String{oct            = {October}}
@String{PAMI           = {TPAMI}}
@String{Proc3DIM       = {Proceedings of the International Workshop on 3D Digital Imaging and Modeling (3DIM)}}
@String{Proc3DIMPVT    = {Proceedings of the International Conference on 3D Imaging, Modeling, Processing, Visualization and Transmission (3DIMPVT)}}
@String{Proc3DPVT      = {Proceedings of the International Symposium on 3D Data Processing, Visualization, and Transmission (3DPVT)}}
@String{Proc3DTV       = {Proceedings of the 3DTV Conference}}
@String{Proc3DV        = {3DV}}
@String{PROCAMS        = {Proceedings of the International Workshop on Projector-Camera Systems (PROCAMS)}}
@String{ProcCGIT       = {Proceedings of the ACM on Computer Graphics and Interactive Techniques}}
@String{ProcCVM        = {Proceedings of Computational Visual Media (CVM)}}
@String{ProcEG         = {Comput. Graph. Forum}}
@String{ProcEGSR       = {Comput. Graph. Forum}}
@String{ProcPG         = {Comput. Graph. Forum}}
@String{ProcSGP        = {Comput. Graph. Forum}}
@String{ProcSIGASIA    = {ACM Trans. Graph.}}
@String{ProcSIGGRAPH   = {ACM Trans. Graph.}}
@String{ProcSIGGRAPHCG = {Computer Graphics (Proceedings of SIGGRAPH)}}
@String{ProcSPIE       = {Proceedings of SPIE}}
@String{RSS            = {Proceedings of Robotics: Science and Systems (RSS)}}
@String{SAP            = {Proceedings of the Symposium on Applied Perception (SAP)}}
@String{SBIM           = {Proceedings of the Symposium on Sketch-Based Interfaces and Modeling (SBIM)}}
@String{SCA            = {Proceedings of the Symposium on Computer Animation (SCA)}}
@String{sep            = {September}}
@String{SIGAsiaConf    = {SIGGRAPH Asia}}
@String{SIGGRAPH       = {SIGGRAPH}}
@String{SIGGRAPHConf   = {SIGGRAPH}}
@String{TAP            = {ACM Trans. Appl. Percept.}}
@String{TCSVT          = {IEEE Transactions on Circuits and Systems for Video Technology}}
@String{TIP            = {IEEE Transactions on Image Processing}}
@String{TOG            = {ACM Trans. Graph.}}
@String{TPCG           = {Proceedings of Theory and Practice of Computer Graphics (TPCG)}}
@String{TVCG           = {TVCG}}
@String{UIST           = {UIST}}
@String{VMV            = {Proceedings of the Vision, Modeling, and Visualization Workshop (VMV)}}
@String{VR             = {IEEE VR}}
@String{VR3DUI         = {IEEE VR}}
@String{VRST           = {VRST}}
@String{VRW            = {Proceedings of IEEE Virtual Reality Workshops}}
@String{WACV           = {WACV}}
@String{WACVW          = {Proceedings of WACV Workshops}}
@String{WSBIM          = {Proceedings of the Workshop on Sketch-Based Interfaces and Modeling (SBIM)}}
@String{WSCG           = {Proceedings of the International Conference in Central Europe on Computer Graphics, Visualisation and Computer Vision (WSCG)}}

@Unpublished{MovieGen2024,
  author    = {{The Movie Gen team @ Meta}},
  title     = {{Movie Gen}: A Cast of Media Foundation Models},
  note      = {\href{https://arxiv.org/abs/2410.13720}{arXiv:2410.13720}},
  year      = {2024},
  _author   = {Adam Polyak and Amit Zohar and Andrew Brown and Andros Tjandra and Animesh Sinha and Ann Lee and Apoorv Vyas and Bowen Shi and Chih-Yao Ma and Ching-Yao Chuang and David Yan and Dhruv Choudhary and Dingkang Wang and Geet Sethi and Guan Pang and Haoyu Ma and Ishan Misra and Ji Hou and Jialiang Wang and Kiran Jagadeesh and Kunpeng Li and Luxin Zhang and Mannat Singh and Mary Williamson and Matt Le and Matthew Yu and Mitesh Kumar Singh and Peizhao Zhang and Peter Vajda and Quentin Duval and Rohit Girdhar and Roshan Sumbaly and Sai Saketh Rambhatla and Sam Tsai and Samaneh Azadi and Samyak Datta and Sanyuan Chen and Sean Bell and Sharadh Ramaswamy and Shelly Sheynin and Siddharth Bhattacharya and Simran Motwani and Tao Xu and Tianhe Li and Tingbo Hou and Wei-Ning Hsu and Xi Yin and Xiaoliang Dai and Yaniv Taigman and Yaqiao Luo and Yen-Cheng Liu and Yi-Chiao Wu and Yue Zhao and Yuval Kirstain and Zecheng He and Zijian He and Albert Pumarola and Ali Thabet and Artsiom Sanakoyeu and Arun Mallya and Baishan Guo and Boris Araya and Breena Kerr and Carleigh Wood and Ce Liu and Cen Peng and Dimitry Vengertsev and Edgar Schonfeld and Elliot Blanchard and Felix Juefei-Xu and Fraylie Nord and Jeff Liang and John Hoffman and Jonas Kohler and Kaolin Fire and Karthik Sivakumar and Lawrence Chen and Licheng Yu and Luya Gao and Markos Georgopoulos and Rashel Moritz and Sara K. Sampson and Shikai Li and Simone Parmeggiani and Steve Fine and Tara Fowler and Vladan Petrovic and Yuming Du},
  abstract  = {We present Movie Gen, a cast of foundation models that generates high-quality, 1080p HD videos with different aspect ratios and synchronized audio. We also show additional capabilities such as precise instruction-based video editing and generation of personalized videos based on a user’s image. Our models set a new state-of-the-art on multiple tasks: text-to-video synthesis, video personalization, video editing, video-to-audio generation, and text-to-audio generation. Our largest video generation model is a 30B parameter transformer trained with a maximum context length of 73K video tokens, corresponding to a generated video of 16 seconds at 16 frames-per-second. We show multiple technical innovations and simplifications on the architecture, latent spaces, training objectives and recipes, data curation, evaluation protocols, parallelization techniques, and inference optimizations that allow us to reap the benefits of scaling pre-training data, model size, and training compute for training large scale media generation models. We hope this paper helps the research community to accelerate progress and innovation in media generation models.},
  arxiv     = {2410.13720},
  _url       = {https://go.fb.me/MovieGenResearchVideos},
}

@InProceedings{BarroMTHMS2021,
  author    = {Jonathan T. Barron and Ben Mildenhall and Matthew Tancik and Peter Hedman and Ricardo Martin-Brualla and Pratul P. Srinivasan},
  title     = {{Mip-NeRF}: A Multiscale Representation for Anti-Aliasing Neural Radiance Fields},
  booktitle = ICCV,
  year      = {2021},
  abstract  = {The rendering procedure used by neural radiance fields (NeRF) samples a scene with a single ray per pixel and may therefore produce renderings that are excessively blurred or aliased when training or testing images observe scene content at different resolutions. The straightforward solution of supersampling by rendering with multiple rays per pixel is impractical for NeRF, because rendering each ray requires querying a multilayer perceptron hundreds of times. Our solution, which we call "mip-NeRF" (à la "mipmap"), extends NeRF to represent the scene at a continuously-valued scale. By efficiently rendering anti-aliased conical frustums instead of rays, mip-NeRF reduces objectionable aliasing artifacts and significantly improves NeRF's ability to represent fine details, while also being 7% faster than NeRF and half the size. Compared to NeRF, mip-NeRF reduces average error rates by 16% on the dataset presented with NeRF and by 60% on a challenging multiscale variant of that dataset that we present. mip-NeRF is also able to match the accuracy of a brute-force supersampled NeRF on our multiscale dataset while being 22x faster.},
  doi       = {10.1109/ICCV48922.2021.00580},
  _url       = {https://jonbarron.info/mipnerf/},
}

@InProceedings{BarroMVSH2022,
  author    = {Jonathan T. Barron and Ben Mildenhall and Dor Verbin and Pratul P. Srinivasan and Peter Hedman},
  title     = {Mip-{NeRF} 360: Unbounded Anti-Aliased Neural Radiance Fields},
  booktitle = CVPR,
  year      = {2022},
  abstract  = {Though neural radiance fields (NeRF) have demonstrated impressive view synthesis results on objects and small bounded regions of space, they struggle on "unbounded" scenes, where the camera may point in any direction and content may exist at any distance. In this setting, existing NeRF-like models often produce blurry or low-resolution renderings (due to the unbalanced detail and scale of nearby and distant objects), are slow to train, and may exhibit artifacts due to the inherent ambiguity of the task of reconstructing a large scene from a small set of images. We present an extension of mip-NeRF (a NeRF variant that addresses sampling and aliasing) that uses a non-linear scene parameterization, online distillation, and a novel distortion-based regularizer to overcome the challenges presented by unbounded scenes. Our model, which we dub "mip-NeRF 360" as we target scenes in which the camera rotates 360 degrees around a point, reduces mean-squared error by 54% compared to mip-NeRF, and is able to produce realistic synthesized views and detailed depth maps for highly intricate, unbounded real-world scenes.},
  arxiv     = {2111.12077},
  doi       = {10.1109/CVPR52688.2022.00539},
  _url       = {https://jonbarron.info/mipnerf360/},
}

@Unpublished{BartrNXLKALX2024,
  author    = {Edward Bartrum and Thu Nguyen-Phuoc and Chris Xie and Zhengqin Li and Numair Khan and Armen Avetisyan and Douglas Lanman and Lei Xiao},
  title     = {{ReplaceAnything3D:Text}-Guided {3D} Scene Editing with Compositional Neural Radiance Fields},
  note      = {\href{https://arxiv.org/abs/2401.17895}{arXiv:2401.17895}},
  year      = {2024},
  abstract  = {We introduce ReplaceAnything3D model (RAM3D), a novel text-guided 3D scene editing method that enables the replacement of specific objects within a scene. Given multi-view images of a scene, a text prompt describing the object to replace, and a text prompt describing the new object, our Erase-and-Replace approach can effectively swap objects in the scene with newly generated content while maintaining 3D consistency across multiple viewpoints. We demonstrate the versatility of ReplaceAnything3D by applying it to various realistic 3D scenes, showcasing results of modified foreground objects that are well-integrated with the rest of the scene without affecting its overall integrity.},
  arxiv     = {2401.17895},
  _url       = {https://replaceanything3d.github.io/},
}

@Unpublished{BlattDKMKLLEVLJR2023,
  author    = {Andreas Blattmann and Tim Dockhorn and Sumith Kulal and Daniel Mendelevitch and Maciej Kilian and Dominik Lorenz and Yam Levi and Zion English and Vikram Voleti and Adam Letts and Varun Jampani and Robin Rombach},
  title     = {Stable Video Diffusion: Scaling Latent Video Diffusion Models to Large Datasets},
  note      = {\href{https://arxiv.org/abs/2311.15127}{arXiv:2311.15127}},
  year      = {2023},
  abstract  = {We present Stable Video Diffusion - a latent video diffusion model for high-resolution, state-of-the-art text-to-video and image-to-video generation. Recently, latent diffusion models trained for 2D image synthesis have been turned into generative video models by inserting temporal layers and finetuning them on small, high-quality video datasets. However, training methods in the literature vary widely, and the field has yet to agree on a unified strategy for curating video data. In this paper, we identify and evaluate three different stages for successful training of video LDMs: text-to-image pretraining, video pretraining, and high-quality video finetuning. Furthermore, we demonstrate the necessity of a well-curated pretraining dataset for generating high-quality videos and present a systematic curation process to train a strong base model, including captioning and filtering strategies. We then explore the impact of finetuning our base model on high-quality data and train a text-to-video model that is competitive with closed-source video generation. We also show that our base model provides a powerful motion representation for downstream tasks such as image-to-video generation and adaptability to camera motion-specific LoRA modules. Finally, we demonstrate that our model provides a strong multi-view 3D-prior and can serve as a base to finetune a multi-view diffusion model that jointly generates multiple views of objects in a feedforward fashion, outperforming image-based methods at a fraction of their compute budget. We release code and model weights at https://github.com/Stability-AI/generative-models.},
  arxiv     = {2311.15127},
  _url       = {https://github.com/Stability-AI/generative-models},
}

@InProceedings{CaoYFWX2024,
  author    = {Chenjie Cao and Chaohui Yu and Yanwei Fu and Fan Wang and Xiangyang Xue},
  title     = {{MVInpainter}: Learning Multi-View Consistent Inpainting to Bridge {2D} and {3D} Editing},
  booktitle = NeurIPS,
  year      = {2024},
  abstract  = {Novel View Synthesis (NVS) and 3D generation have recently achieved prominent improvements. However, these works mainly focus on confined categories or synthetic 3D assets, which are discouraged from generalizing to challenging in-the-wild scenes and fail to be employed with 2D synthesis directly. Moreover, these methods heavily depended on camera poses, limiting their real-world applications. To overcome these issues, we propose MVInpainter, re-formulating the 3D editing as a multi-view 2D inpainting task. Specifically, MVInpainter partially inpaints multi-view images with the reference guidance rather than intractably generating an entirely novel view from scratch, which largely simplifies the difficulty of in-the-wild NVS and leverages unmasked clues instead of explicit pose conditions. To ensure cross-view consistency, MVInpainter is enhanced by video priors from motion components and appearance guidance from concatenated reference key&value attention. Furthermore, MVInpainter incorporates slot attention to aggregate high-level optical flow features from unmasked regions to control the camera movement with pose-free training and inference. Sufficient scene-level experiments on both object-centric and forward-facing datasets verify the effectiveness of MVInpainter, including diverse tasks, such as multi-view object removal, synthesis, insertion, and replacement. The project page is https://ewrfcas.github.io/MVInpainter/.},
  arxiv     = {2408.08000},
  _url       = {https://ewrfcas.github.io/MVInpainter/},
}

@InProceedings{ChenXETG2024,
  author    = {Anpei Chen and Haofei Xu and Stefano Esposito and Siyu Tang and Andreas Geiger},
  title     = {{LaRa}: Efficient Large-Baseline Radiance Fields},
  booktitle = ECCV,
  year      = {2024},
  abstract  = {Radiance field methods have achieved photorealistic novel view synthesis and geometry reconstruction. But they are mostly applied in per-scene optimization or small-baseline settings. While several recent works investigate feed-forward reconstruction with large baselines by utilizing transformers, they all operate with a standard global attention mechanism and hence ignore the local nature of 3D reconstruction. We propose a method that unifies local and global reasoning in transformer layers, resulting in improved quality and faster convergence. Our model represents scenes as Gaussian Volumes and combines this with an image encoder and Group Attention Layers for efficient feed-forward reconstruction. Experimental results demonstrate that our model, trained for two days on four GPUs, demonstrates high fidelity in reconstructing 360&deg radiance fields, and robustness to zero-shot and out-of-domain testing.},
  arxiv     = {2407.04699},
  _url       = {https://apchenstu.github.io/LaRa/},
}

@InProceedings{ChenLP2024,
  author    = {Honghua Chen and Chen Change Loy and Xingang Pan},
  title     = {{MVIP-NeRF}: Multi-view {3D} Inpainting on {NeRF} Scenes via Diffusion Prior},
  booktitle = CVPR,
  year      = {2024},
  _month     = jun,
  _pages    = {5344-5353},
  abstract  = {Despite the emergence of successful NeRF inpainting methods built upon explicit RGB and depth 2D inpainting supervisions these methods are inherently constrained by the capabilities of their underlying 2D inpainters. This is due to two key reasons: (i) independently inpainting constituent images results in view-inconsistent imagery and (ii) 2D inpainters struggle to ensure high-quality geometry completion and alignment with inpainted RGB images. To overcome these limitations we propose a novel approach called MVIP-NeRF that harnesses the potential of diffusion priors for NeRF inpainting addressing both appearance and geometry aspects. MVIP-NeRF performs joint inpainting across multiple views to reach a consistent solution which is achieved via an iterative optimization process based on Score Distillation Sampling (SDS). Apart from recovering the rendered RGB images we also extract normal maps as a geometric representation and define a normal SDS loss that motivates accurate geometry inpainting and alignment with the appearance. Additionally we formulate a multi-view SDS score function to distill generative priors simultaneously from different view images ensuring consistent visual completion when dealing with large view variations. Our experimental results show better appearance and geometry recovery than previous NeRF inpainting methods.},
  _url       = {https://openaccess.thecvf.com/content/CVPR2024/html/Chen_MVIP-NeRF_Multi-view_3D_Inpainting_on_NeRF_Scenes_via_Diffusion_Prior_CVPR_2024_paper.html},
}

@InProceedings{ChenCSXZ2024,
  author    = {Jiafu Chen and Tianyi Chu and Jiakai Sun and Wei Xing and Lei Zhao},
  title     = {Single-Mask Inpainting for Voxel-based Neural Radiance Fields},
  booktitle = ECCV,
  year      = {2024},
  abstract  = {3D inpainting is a challenging task in computer vision and graphics that aims to remove objects and fill in missing regions with a visually coherent and complete representation of the background. A few methods have been proposed to address this problem, yielding notable results in inpainting. However, these methods haven’t perfectly solved the limitation of relying on masks for each view. Obtaining masks for each view can be time-consuming and reduces quality, especially in scenarios with a large number of views or complex scenes. To address this limitation, we propose an innovative approach that eliminates the need for per-view masks and uses a single mask from a selected view. We focus on improving the quality of forward-facing scene inpainting. By unprojecting the single 2D mask into the NeRFs space, we define the regions that require inpainting in three dimensions. We introduce a two-step optimization process. Firstly, we utilize 2D inpainters to generate color and depth priors for the selected view. This provides a rough supervision for the area to be inpainted. Secondly, we incorporate a 2D diffusion model to enhance the quality of the inpainted regions, reducing distortions and elevating the overall visual fidelity. Through extensive experiments, we demonstrate the effectiveness of our single-mask inpainting framework. The results show that our approach successfully inpaints complex geometry and produces visually plausible and realistic outcomes.},
}

@Unpublished{ChenGXWYRWLLL2024,
  author    = {Junsong Chen and Chongjian Ge and Enze Xie and Yue Wu and Lewei Yao and Xiaozhe Ren and Zhongdao Wang and Ping Luo and Huchuan Lu and Zhenguo Li},
  title     = {{PixArt-$\Sigma$}: Weak-to-Strong Training of Diffusion Transformer for {4K} Text-to-Image Generation},
  note      = {\href{https://arxiv.org/abs/2403.04692}{arXiv:2403.04692}},
  year      = {2024},
  abstract  = {In this paper, we introduce PixArt-\Sigma, a Diffusion Transformer model~(DiT) capable of directly generating images at 4K resolution. PixArt-\Sigma represents a significant advancement over its predecessor, PixArt-\alpha, offering images of markedly higher fidelity and improved alignment with text prompts. A key feature of PixArt-\Sigma is its training efficiency. Leveraging the foundational pre-training of PixArt-\alpha, it evolves from the `weaker' baseline to a `stronger' model via incorporating higher quality data, a process we term "weak-to-strong training". The advancements in PixArt-\Sigma are twofold: (1) High-Quality Training Data: PixArt-\Sigma incorporates superior-quality image data, paired with more precise and detailed image captions. (2) Efficient Token Compression: we propose a novel attention module within the DiT framework that compresses both keys and values, significantly improving efficiency and facilitating ultra-high-resolution image generation. Thanks to these improvements, PixArt-\Sigma achieves superior image quality and user prompt adherence capabilities with significantly smaller model size (0.6B parameters) than existing text-to-image diffusion models, such as SDXL (2.6B parameters) and SD Cascade (5.1B parameters). Moreover, PixArt-\Sigma's capability to generate 4K images supports the creation of high-resolution posters and wallpapers, efficiently bolstering the production of high-quality visual content in industries such as film and gaming.},
  arxiv     = {2403.04692},
  _url       = {https://pixart-alpha.github.io/PixArt-sigma-project/},
}

@Unpublished{ChenHYOH2024,
  author    = {Junyi Chen and Di Huang and Weicai Ye and Wanli Ouyang and Tong He},
  title     = {Where Am {I} and What Will {I} See: An Auto-Regressive Model for Spatial Localization and View Prediction},
  note      = {\href{https://arxiv.org/abs/2410.18962}{arXiv:2410.18962}},
  year      = {2024},
  abstract  = {Spatial intelligence is the ability of a machine to perceive, reason, and act in three dimensions within space and time. Recent advancements in large-scale auto-regressive models have demonstrated remarkable capabilities across various reasoning tasks. However, these models often struggle with fundamental aspects of spatial reasoning, particularly in answering questions like "Where am I?" and "What will I see?". While some attempts have been done, existing approaches typically treat them as separate tasks, failing to capture their interconnected nature. In this paper, we present Generative Spatial Transformer (GST), a novel auto-regressive framework that jointly addresses spatial localization and view prediction. Our model simultaneously estimates the camera pose from a single image and predicts the view from a new camera pose, effectively bridging the gap between spatial awareness and visual prediction. The proposed innovative camera tokenization method enables the model to learn the joint distribution of 2D projections and their corresponding spatial perspectives in an auto-regressive manner. This unified training paradigm demonstrates that joint optimization of pose estimation and novel view synthesis leads to improved performance in both tasks, for the first time, highlighting the inherent relationship between spatial awareness and visual prediction.},
  arxiv     = {2410.18962},
  _url       = {https://sotamak1r.github.io/gst/},
}

@InProceedings{ChenQLLL2024,
  author    = {Jiahao Chen and Yipeng Qin and Lingjie Liu and Jiangbo Lu and Guanbin Li},
  title     = {{NeRF-HuGS}: Improved Neural Radiance Fields in Non-static Scenes Using Heuristics-Guided Segmentation},
  booktitle = CVPR,
  year      = {2024},
  _month     = jun,
  _pages    = {19436-19446},
  abstract  = {Neural Radiance Field (NeRF) has been widely recognized for its excellence in novel view synthesis and 3D scene reconstruction. However their effectiveness is inherently tied to the assumption of static scenes rendering them susceptible to undesirable artifacts when confronted with transient distractors such as moving objects or shadows. In this work we propose a novel paradigm namely "Heuristics-Guided Segmentation" (HuGS) which significantly enhances the separation of static scenes from transient distractors by harmoniously combining the strengths of hand-crafted heuristics and state-of-the-art segmentation models thus significantly transcending the limitations of previous solutions. Furthermore we delve into the meticulous design of heuristics introducing a seamless fusion of Structure-from-Motion (SfM)-based heuristics and color residual heuristics catering to a diverse range of texture profiles. Extensive experiments demonstrate the superiority and robustness of our method in mitigating transient distractors for NeRFs trained in non-static scenes. Project page: https://cnhaox.github.io/NeRF-HuGS/},
  _url       = {https://cnhaox.github.io/NeRF-HuGS/},
}

@Unpublished{ChenMCWPT2024,
  author    = {Yutong Chen and Marko Mihajlovic and Xiyi Chen and Yiming Wang and Sergey Prokudin and Siyu Tang},
  title     = {{SplatFormer}: Point Transformer for Robust {3D} Gaussian Splatting},
  note      = {\href{https://arxiv.org/abs/2411.06390}{arXiv:2411.06390}},
  year      = {2024},
  abstract  = {3D Gaussian Splatting (3DGS) has recently transformed photorealistic reconstruction, achieving high visual fidelity and real-time performance. However, rendering quality significantly deteriorates when test views deviate from the camera angles used during training, posing a major challenge for applications in immersive free-viewpoint rendering and navigation. In this work, we conduct a comprehensive evaluation of 3DGS and related novel view synthesis methods under out-of-distribution (OOD) test camera scenarios. By creating diverse test cases with synthetic and real-world datasets, we demonstrate that most existing methods, including those incorporating various regularization techniques and data-driven priors, struggle to generalize effectively to OOD views. To address this limitation, we introduce SplatFormer, the first point transformer model specifically designed to operate on Gaussian splats. SplatFormer takes as input an initial 3DGS set optimized under limited training views and refines it in a single forward pass, effectively removing potential artifacts in OOD test views. To our knowledge, this is the first successful application of point transformers directly on 3DGS sets, surpassing the limitations of previous multi-scene training methods, which could handle only a restricted number of input views during inference. Our model significantly improves rendering quality under extreme novel views, achieving state-of-the-art performance in these challenging scenarios and outperforming various 3DGS regularization techniques, multi-scene models tailored for sparse view synthesis, and diffusion-based frameworks.},
  arxiv     = {2411.06390},
  _url       = {https://sergeyprokudin.github.io/splatformer/},
}

@InProceedings{ChenZXZVCC2024,
  author        = {Yuedong Chen and Chuanxia Zheng and Haofei Xu and Bohan Zhuang and Andrea Vedaldi and Tat-Jen Cham and Jianfei Cai},
  title         = {{MVSplat360}: Feed-Forward 360 Scene Synthesis from Sparse Views},
  booktitle     = NeurIPS,
  year          = {2024},
  abstract      = {We introduce MVSplat360, a feed-forward approach for 360° novel view synthesis (NVS) of diverse real-world scenes, using only sparse observations. This setting is inherently ill-posed due to minimal overlap among input views and insufficient visual information provided, making it challenging for conventional methods to achieve high-quality results. Our MVSplat360 addresses this by effectively combining geometry-aware 3D reconstruction with temporally consistent video generation. Specifically, it refactors a feed-forward 3D Gaussian Splatting (3DGS) model to render features directly into the latent space of a pre-trained Stable Video Diffusion (SVD) model, where these features then act as pose and visual cues to guide the denoising process and produce photorealistic 3D-consistent views. Our model is end-to-end trainable and supports rendering arbitrary views with as few as 5 sparse input views. To evaluate MVSplat360's performance, we introduce a new benchmark using the challenging DL3DV-10K dataset, where MVSplat360 achieves superior visual quality compared to state-of-the-art methods on wide-sweeping or even 360° NVS tasks. Experiments on the existing benchmark RealEstate10K also confirm the effectiveness of our model. The video results are available on our project page: https://donydchen.github.io/mvsplat360.},
  arxiv         = {2411.04924},
  _url           = {https://donydchen.github.io/mvsplat360},
}

@InProceedings{DengTLGSW2024,
  author    = {Boyang Deng and Richard Tucker and Zhengqi Li and Leonidas Guibas and Noah Snavely and Gordon Wetzstein},
  title     = {Streetscapes: Large-scale Consistent Street View Generation Using Autoregressive Video Diffusion},
  booktitle = SIGGRAPH,
  year      = {2024},
  abstract  = {We present a method for generating Streetscapes-long sequences of views through an on-the-fly synthesized city-scale scene. Our generation is conditioned by language input (e.g., city name, weather), as well as an underlying map/layout hosting the desired trajectory. Compared to recent models for video generation or 3D view synthesis, our method can scale to much longer-range camera trajectories, spanning several city blocks, while maintaining visual quality and consistency. To achieve this goal, we build on recent work on video diffusion, used within an autoregressive framework that can easily scale to long sequences. In particular, we introduce a new temporal imputation method that prevents our autoregressive approach from drifting from the distribution of realistic city imagery. We train our Streetscapes system on a compelling source of data-posed imagery from Google Street View, along with contextual map data-which allows users to generate city views conditioned on any desired city layout, with controllable camera poses. Please see more results at our project page at https://boyangdeng.com/streetscapes.},
  arxiv     = {2407.13759},
  _url       = {https://boyangdeng.com/streetscapes},
}

@Unpublished{ElataKOFS2024,
  author    = {Noam Elata and Bahjat Kawar and Yaron Ostrovsky-Berman and Miriam Farber and Ron Sokolovsky},
  title     = {Novel View Synthesis with Pixel-Space Diffusion Models},
  note      = {\href{https://arxiv.org/abs/2411.07765}{arXiv:2411.07765}},
  year      = {2024},
  abstract  = {Synthesizing a novel view from a single input image is a challenging task. Traditionally, this task was approached by estimating scene depth, warping, and inpainting, with machine learning models enabling parts of the pipeline. More recently, generative models are being increasingly employed in novel view synthesis (NVS), often encompassing the entire end-to-end system. In this work, we adapt a modern diffusion model architecture for end-to-end NVS in the pixel space, substantially outperforming previous state-of-the-art (SOTA) techniques. We explore different ways to encode geometric information into the network. Our experiments show that while these methods may enhance performance, their impact is minor compared to utilizing improved generative models. Moreover, we introduce a novel NVS training scheme that utilizes single-view datasets, capitalizing on their relative abundance compared to their multi-view counterparts. This leads to improved generalization capabilities to scenes with out-of-domain content.},
  arxiv     = {2411.07765},
  _url       = {https://arxiv.org/abs/2411.07765},
}

@Unpublished{FengDXNABZ2024,
  author    = {Haiwen Feng and Zheng Ding and Zhihao Xia and Simon Niklaus and Victoria Abrevaya and Michael J. Black and Xuaner Zhang},
  title     = {Explorative Inbetweening of Time and Space},
  note      = {\href{https://arxiv.org/abs/2403.14611}{arXiv:2403.14611}},
  year      = {2024},
  abstract  = {We introduce bounded generation as a generalized task to control video generation to synthesize arbitrary camera and subject motion based only on a given start and end frame. Our objective is to fully leverage the inherent generalization capability of an image-to-video model without additional training or fine-tuning of the original model. This is achieved through the proposed new sampling strategy, which we call Time Reversal Fusion, that fuses the temporally forward and backward denoising paths conditioned on the start and end frame, respectively. The fused path results in a video that smoothly connects the two frames, generating inbetweening of faithful subject motion, novel views of static scenes, and seamless video looping when the two bounding frames are identical. We curate a diverse evaluation dataset of image pairs and compare against the closest existing methods. We find that Time Reversal Fusion outperforms related work on all subtasks, exhibiting the ability to generate complex motions and 3D-consistent views guided by bounded frames. See project page at https://time-reversal.github.io.},
  arxiv     = {2403.14611},
  _url       = {https://time-reversal.github.io},
}

@InProceedings{GaoHHBMSBP2024,
  author    = {Ruiqi Gao and Aleksander Holynski and Philipp Henzler and Arthur Brussee and Ricardo Martin-Brualla and Pratul Srinivasan and Jonathan T. Barron and Ben Poole},
  title     = {{CAT3D}: Create Anything in {3D} with Multi-View Diffusion Models},
  booktitle = NeurIPS,
  year      = {2024},
  abstract  = {Advances in 3D reconstruction have enabled high-quality 3D capture, but require a user to collect hundreds to thousands of images to create a 3D scene. We present CAT3D, a method for creating anything in 3D by simulating this real-world capture process with a multi-view diffusion model. Given any number of input images and a set of target novel viewpoints, our model generates highly consistent novel views of a scene. These generated views can be used as input to robust 3D reconstruction techniques to produce 3D representations that can be rendered from any viewpoint in real-time. CAT3D can create entire 3D scenes in as little as one minute, and outperforms existing methods for single image and few-view 3D scene creation. See our project page for results and interactive demos at https://cat3d.github.io.},
  arxiv     = {2405.10314},
  _url       = {https://cat3d.github.io},
}

@InProceedings{GoliRSJT2024,
  author    = {Lily Goli and Cody Reading and Silvia Sellán and Alec Jacobson and Andrea Tagliasacchi},
  title     = {Bayes' Rays: Uncertainty Quantification for Neural Radiance Fields},
  booktitle = CVPR,
  year      = {2024},
  abstract  = {Neural Radiance Fields (NeRFs) have shown promise in applications like view synthesis and depth estimation, but learning from multiview images faces inherent uncertainties. Current methods to quantify them are either heuristic or computationally demanding. We introduce BayesRays, a post-hoc framework to evaluate uncertainty in any pre-trained NeRF without modifying the training process. Our method establishes a volumetric uncertainty field using spatial perturbations and a Bayesian Laplace approximation. We derive our algorithm statistically and show its superior performance in key metrics and applications. Additional results available at: https://bayesrays.github.io.},
  arxiv     = {2309.03185},
  _url       = {https://bayesrays.github.io/},
}

@InProceedings{GuTLSTLR2023,
  author    = {Jiatao Gu and Alex Trevithick and Kai-En Lin and Josh Susskind and Christian Theobalt and Lingjie Liu and Ravi Ramamoorthi},
  title     = {{NerfDiff}: Single-image View Synthesis with {NeRF}-guided Distillation from {3D}-aware Diffusion},
  booktitle = ICML,
  year      = {2023},
  abstract  = {Novel view synthesis from a single image requires inferring occluded regions of objects and scenes whilst simultaneously maintaining semantic and physical consistency with the input. Existing approaches condition neural radiance fields (NeRF) on local image features, projecting points to the input image plane, and aggregating 2D features to perform volume rendering. However, under severe occlusion, this projection fails to resolve uncertainty, resulting in blurry renderings that lack details. In this work, we propose NerfDiff, which addresses this issue by distilling the knowledge of a 3D-aware conditional diffusion model (CDM) into NeRF through synthesizing and refining a set of virtual views at test time. We further propose a novel NeRF-guided distillation algorithm that simultaneously generates 3D consistent virtual views from the CDM samples, and finetunes the NeRF based on the improved virtual views. Our approach significantly outperforms existing NeRF-based and geometry-free approaches on challenging datasets, including ShapeNet, ABO, and Clevr3D.},
  arxiv     = {2302.10109},
  _url       = {https://jiataogu.me/nerfdiff/},
}

@Unpublished{HoS2021,
  author    = {Jonathan Ho and Tim Salimans},
  title     = {Classifier-Free Diffusion Guidance},
  note      = {\href{https://arxiv.org/abs/2207.12598}{arXiv:2207.12598}},
  year      = {2021},
  abstract  = {Classifier guidance is a recently introduced method to trade off mode coverage and sample fidelity in conditional diffusion models post training, in the same spirit as low temperature sampling or truncation in other types of generative models. Classifier guidance combines the score estimate of a diffusion model with the gradient of an image classifier and thereby requires training an image classifier separate from the diffusion model. It also raises the question of whether guidance can be performed without a classifier. We show that guidance can be indeed performed by a pure generative model without such a classifier: in what we call classifier-free guidance, we jointly train a conditional and an unconditional diffusion model, and we combine the resulting conditional and unconditional score estimates to attain a trade-off between sample quality and diversity similar to that obtained using classifier guidance.},
  arxiv     = {2207.12598},
  _url       = {https://openreview.net/pdf?id=qw8AKxfYbI},
}

@InProceedings{HoelleBMNTRZN2024,
  author    = {Lukas Höllein and Aljaž Božič and Norman Müller and David Novotny and Hung-Yu Tseng and Christian Richardt and Michael Zollhöfer and Matthias Nießner},
  title     = {{ViewDiff}: {3D}-Consistent Image Generation with Text-to-Image Models},
  booktitle = CVPR,
  year      = {2024},
  _month     = jun,
  _pages    = {5043-5052},
  abstract  = {3D asset generation is getting massive amounts of attention inspired by the recent success on text-guided 2D content creation. Existing text-to-3D methods use pretrained text-to-image diffusion models in an optimization problem or fine-tune them on synthetic data which often results in non-photorealistic 3D objects without backgrounds. In this paper we present a method that leverages pretrained text-to-image models as a prior and learn to generate multi-view images in a single denoising process from real-world data. Concretely we propose to integrate 3D volume-rendering and cross-frame-attention layers into each block of the existing U-Net network of the text-to-image model. Moreover we design an autoregressive generation that renders more 3D-consistent images at any viewpoint. We train our model on real-world datasets of objects and showcase its capabilities to generate instances with a variety of high-quality shapes and textures in authentic surroundings. Compared to the existing methods the results generated by our method are consistent and have favorable visual quality (-30% FID -37% KID).},
  _url       = {https://lukashoel.github.io/ViewDiff/},
}

@InProceedings{HoelleCOJN2023,
  author    = {Lukas Höllein and Ang Cao and Andrew Owens and Justin Johnson and Matthias Nießner},
  title     = {{Text2Room}: Extracting Textured {3D} Meshes from {2D} Text-to-Image Models},
  booktitle = ICCV,
  year      = {2023},
  abstract  = {We present Text2Room, a method for generating room-scale textured 3D meshes from a given text prompt as input. To this end, we leverage pre-trained 2D text-to-image models to synthesize a sequence of images from different poses. In order to lift these outputs into a consistent 3D scene representation, we combine monocular depth estimation with a text-conditioned inpainting model. The core idea of our approach is a tailored viewpoint selection such that the content of each image can be fused into a seamless, textured 3D mesh. More specifically, we propose a continuous alignment strategy that iteratively fuses scene frames with the existing geometry to create a seamless mesh. Unlike existing works that focus on generating single objects or zoom-out trajectories from text, our method generates complete 3D scenes with multiple objects and explicit 3D geometry. We evaluate our approach using qualitative and quantitative metrics, demonstrating it as the first method to generate room-scale 3D geometry with compelling textures from only text as input.},
  arxiv     = {2303.11989},
  _url       = {https://lukashoel.github.io/text-to-room/},
}

@Unpublished{HongJSHYLK2024,
  author    = {Sunghwan Hong and Jaewoo Jung and Heeseong Shin and Jisang Han and Jiaolong Yang and Chong Luo and Seungryong Kim},
  title     = {{PF3plat}: Pose-Free Feed-Forward {3D Gaussian} Splatting},
  note      = {\href{https://arxiv.org/abs/2410.22128}{arXiv:2410.22128}},
  year      = {2024},
  abstract  = {We consider the problem of novel view synthesis from unposed images in a single feed-forward. Our framework capitalizes on fast speed, scalability, and high-quality 3D reconstruction and view synthesis capabilities of 3DGS, where we further extend it to offer a practical solution that relaxes common assumptions such as dense image views, accurate camera poses, and substantial image overlaps. We achieve this through identifying and addressing unique challenges arising from the use of pixel-aligned 3DGS: misaligned 3D Gaussians across different views induce noisy or sparse gradients that destabilize training and hinder convergence, especially when above assumptions are not met. To mitigate this, we employ pre-trained monocular depth estimation and visual correspondence models to achieve coarse alignments of 3D Gaussians. We then introduce lightweight, learnable modules to refine depth and pose estimates from the coarse alignments, improving the quality of 3D reconstruction and novel view synthesis. Furthermore, the refined estimates are leveraged to estimate geometry confidence scores, which assess the reliability of 3D Gaussian centers and condition the prediction of Gaussian parameters accordingly. Extensive evaluations on large-scale real-world datasets demonstrate that PF3plat sets a new state-of-the-art across all benchmarks, supported by comprehensive ablation studies validating our design choices.},
  arxiv     = {2410.22128},
  _url       = {https://cvlab-kaist.github.io/PF3plat/},
}

@Unpublished{JinJTZBZLSX2025,
  author        = {Haian Jin and Hanwen Jiang and Hao Tan and Kai Zhang and Sai Bi and Tianyuan Zhang and Fujun Luan and Noah Snavely and Zexiang Xu},
  title         = {{LVSM}: A Large View Synthesis model with Minimal {3D} Inductive Bias},
  note          = {\href{https://arxiv.org/abs/2410.17242}{arXiv:2410.17242}},
  year          = {2024},
  abstract      = {We propose the Large View Synthesis Model (LVSM), a novel transformer-based approach for scalable and generalizable novel view synthesis from sparse-view inputs. We introduce two architectures: (1) an encoder-decoder LVSM, which encodes input image tokens into a fixed number of 1D latent tokens, functioning as a fully learned scene representation, and decodes novel-view images from them; and (2) a decoder-only LVSM, which directly maps input images to novel-view outputs, completely eliminating intermediate scene representations. Both models bypass the 3D inductive biases used in previous methods---from 3D representations (e.g., NeRF, 3DGS) to network designs (e.g., epipolar projections, plane sweeps)---addressing novel view synthesis with a fully data-driven approach. While the encoder-decoder model offers faster inference due to its independent latent representation, the decoder-only LVSM achieves superior quality, scalability, and zero-shot generalization, outperforming previous state-of-the-art methods by 1.5 to 3.5 dB PSNR. Comprehensive evaluations across multiple datasets demonstrate that both LVSM variants achieve state-of-the-art novel view synthesis quality, delivering superior performance even with reduced computational resources (1-2 GPUs). Please see our anonymous website for more details: https://lvsm-web.github.io/},
  arxiv         = {2410.17242},
  _url           = {https://haian-jin.github.io/projects/LVSM/},
}

@InProceedings{KantSVGRTG2023,
  author    = {Yash Kant and Aliaksandr Siarohin and Michael Vasilkovsky and Riza Alp Guler and Jian Ren and Sergey Tulyakov and Igor Gilitschenski},
  title     = {{iNVS}: Repurposing Diffusion Inpainters for Novel View Synthesis},
  booktitle = SIGAsiaConf,
  year      = {2023},
  abstract  = {We present a method for generating consistent novel views from a single source image. Our approach focuses on maximizing the reuse of visible pixels from the source image. To achieve this, we use a monocular depth estimator that transfers visible pixels from the source view to the target view. Starting from a pre-trained 2D inpainting diffusion model, we train our method on the large-scale Objaverse dataset to learn 3D object priors. While training we use a novel masking mechanism based on epipolar lines to further improve the quality of our approach. This allows our framework to perform zero-shot novel view synthesis on a variety of objects. We evaluate the zero-shot abilities of our framework on three challenging datasets: Google Scanned Objects, Ray Traced Multiview, and Common Objects in 3D. See our webpage for more details: https://yashkant.github.io/invs/},
  arxiv     = {2310.16167},
  _url       = {https://yashkant.github.io/invs/},
}

@InProceedings{KantSWVQRGGTG2024,
  author    = {Yash Kant and Aliaksandr Siarohin and Ziyi Wu and Michael Vasilkovsky and Guocheng Qian and Jian Ren and Riza Alp Guler and Bernard Ghanem and Sergey Tulyakov and Igor Gilitschenski},
  title     = {{SPAD}: Spatially Aware Multi-View Diffusers},
  booktitle = CVPR,
  year      = {2024},
  _month     = jun,
  _pages    = {10026-10038},
  abstract  = {We present SPAD a novel approach for creating consistent multi-view images from text prompts or single images. To enable multi-view generation we repurpose a pretrained 2D diffusion model by extending its self-attention layers with cross-view interactions and fine-tune it on a high quality subset of Objaverse. We find that a naive extension of the self-attention proposed in prior work (e.g. MVDream) leads to content copying between views. Therefore we explicitly constrain the cross-view attention based on epipolar geometry. To further enhance 3D consistency we utilize Pl ?ucker coordinates derived from camera rays and inject them as positional encoding. This enables SPAD to reason over spatial proximity in 3D well. Compared to concurrent works that can only generate views at fixed azimuth and elevation (e.g. MVDream SyncDreamer) SPAD offers full camera control and achieves state-of-the-art results in novel view synthesis on unseen objects from the Objaverse and Google Scanned Objects datasets. Finally we demonstrate that text-to-3D generation using SPAD prevents the multi-face Janus issue.},
  _url       = {https://yashkant.github.io/spad/},
}

@Article{KerblKLD2023,
  author    = {Bernhard Kerbl and Georgios Kopanas and Thomas Leimkühler and George Drettakis},
  title     = {{3D Gaussian} Splatting for Real-Time Radiance Field Rendering},
  journal   = ProcSIGGRAPH,
  year      = {2023},
  volume    = {42},
  number    = {4},
  pages     = {139:1--14},
  _month     = jul,
  abstract  = {Radiance Field methods have recently revolutionized novel-view synthesis of scenes captured with multiple photos or videos. However, achieving high visual quality still requires neural networks that are costly to train and render, while recent faster methods inevitably trade off speed for quality. For unbounded and complete scenes (rather than isolated objects) and 1080p resolution rendering, no current method can achieve real-time display rates.

We introduce three key elements that allow us to achieve state-of-the-art visual quality while maintaining competitive training times and importantly allow high-quality real-time (≥ 100 fps) novel-view synthesis at 1080p resolution.

First, starting from sparse points produced during camera calibration, we represent the scene with 3D Gaussians that preserve desirable properties of continuous volumetric radiance fields for scene optimization while avoiding unnecessary computation in empty space; Second, we perform interleaved optimization/density control of the 3D Gaussians, notably optimizing anisotropic covariance to achieve an accurate representation of the scene; Third, we develop a fast visibility-aware rendering algorithm that supports anisotropic splatting and both accelerates training and allows realtime rendering. We demonstrate state-of-the-art visual quality and real-time rendering on several established datasets.},
  doi       = {10.1145/3592433},
  _url       = {https://repo-sam.inria.fr/fungraph/3d-gaussian-splatting/},
}

@InProceedings{LeeKJ2024,
  author    = {Inhee Lee and Byungjun Kim and Hanbyul Joo},
  title     = {Guess The Unseen: Dynamic {3D} Scene Reconstruction from Partial {2D} Glimpses},
  booktitle = CVPR,
  year      = {2024},
  _month     = jun,
  _pages    = {1062-1071},
  abstract  = {In this paper we present a method to reconstruct the world and multiple dynamic humans in 3D from a monocular video input. As a key idea we represent both the world and multiple humans via the recently emerging 3D Gaussian Splatting (3D-GS) representation enabling to conveniently and efficiently compose and render them together. In particular we address the scenarios with severely limited and sparse observations in 3D human reconstruction a common challenge encountered in the real world. To tackle this challenge we introduce a novel approach to optimize the 3D-GS representation in a canonical space by fusing the sparse cues in the common space where we leverage a pre-trained 2D diffusion model to synthesize unseen views while keeping the consistency with the observed 2D appearances. We demonstrate our method can reconstruct high-quality animatable 3D humans in various challenging examples in the presence of occlusion image crops few-shot and extremely sparse observations. After reconstruction our method is capable of not only rendering the scene in any novel views at arbitrary time instances but also editing the 3D scene by removing individual humans or applying different motions for each human. Through various experiments we demonstrate the quality and efficiency of our methods over alternative existing approaches.},
  _url       = {https://snuvclab.github.io/gtu/},
}

@InProceedings{LeroyCR2024,
  author    = {Vincent Leroy and Yohann Cabon and Jérôme Revaud},
  title     = {Grounding Image Matching in {3D} with {MASt3R}},
  booktitle = ECCV,
  year      = {2024},
  abstract  = {Image Matching is a core component of all best-performing algorithms and pipelines in 3D vision. Yet despite matching being fundamentally a 3D problem, intrinsically linked to camera pose and scene geometry, it is typically treated as a 2D problem. This makes sense as the goal of matching is to establish correspondences between 2D pixel fields, but also seems like a potentially hazardous choice. In this work, we take a different stance and propose to cast matching as a 3D task with DUSt3R, a recent and powerful 3D reconstruction framework based on Transformers. Based on pointmaps regression, this method displayed impressive robustness in matching views with extreme viewpoint changes, yet with limited accuracy. We aim here to improve the matching capabilities of such an approach while preserving its robustness. We thus propose to augment the DUSt3R network with a new head that outputs dense local features, trained with an additional matching loss. We further address the issue of quadratic complexity of dense matching, which becomes prohibitively slow for downstream applications if not carefully treated. We introduce a fast reciprocal matching scheme that not only accelerates matching by orders of magnitude, but also comes with theoretical guarantees and, lastly, yields improved results. Extensive experiments show that our approach, coined MASt3R, significantly outperforms the state of the art on multiple matching tasks. In particular, it beats the best published methods by 30% (absolute improvement) in VCRE AUC on the extremely challenging Map-free localization dataset.},
  arxiv     = {2406.09756},
  _url       = {https://arxiv.org/abs/2406.09756},
}

@InProceedings{LeroyRLW2024,
  author    = {Vincent Leroy and Jerome Revaud and Thomas Lucas and Philippe Weinzaepfel},
  title     = {Win-Win: Training High-Resolution Vision Transformers from Two Windows},
  booktitle = ICLR,
  year      = {2024},
  abstract  = {Transformers have become the standard in state-of-the-art vision architectures, achieving impressive performance on both image-level and dense pixelwise tasks. However, training vision transformers for high-resolution pixelwise tasks has a prohibitive cost. Typical solutions boil down to hierarchical architectures, fast and approximate attention, or training on low-resolution crops. This latter solution does not constrain architectural choices, but it leads to a clear performance drop when testing at resolutions significantly higher than that used for training, thus requiring ad-hoc and slow post-processing schemes. In this paper, we propose a novel strategy for efficient training and inference of high-resolution vision transformers. The key principle is to mask out most of the high-resolution inputs during training, keeping only N random windows. This allows the model to learn local interactions between tokens inside each window, and global interactions between tokens from different windows. As a result, the model can directly process the high-resolution input at test time without any special trick. We show that this strategy is effective when using relative positional embedding such as rotary embeddings. It is 4 times faster to train than a full-resolution network, and it is straightforward to use at test time compared to existing approaches. We apply this strategy to three dense prediction tasks with high-resolution data. First, we show on the task of semantic segmentation that a simple setting with 2 windows performs best, hence the name of our method: Win-Win. Second, we confirm this result on the task of monocular depth prediction. Third, we further extend it to the binocular task of optical flow, reaching state-of-the-art performance on the Spring benchmark that contains Full-HD images with an order of magnitude faster inference than the best competitor.},
  arxiv     = {2310.00632},
  _url       = {https://arxiv.org/abs/2310.00632},
}

@InProceedings{LinKHLMKYT2024,
  author    = {Chieh Hubert Lin and Changil Kim and Jia-Bin Huang and Qinbo Li and Chih-Yao Ma and Johannes Kopf and Ming-Hsuan Yang and Hung-Yu Tseng},
  title     = {Taming Latent Diffusion Model for Neural Radiance Field Inpainting},
  booktitle = ECCV,
  year      = {2024},
  abstract  = {Neural Radiance Field (NeRF) is a representation for 3D reconstruction from multi-view images. Despite some recent work showing preliminary success in editing a reconstructed NeRF with diffusion prior, they remain struggling to synthesize reasonable geometry in completely uncovered regions. One major reason is the high diversity of synthetic contents from the diffusion model, which hinders the radiance field from converging to a crisp and deterministic geometry. Moreover, applying latent diffusion models on real data often yields a textural shift incoherent to the image condition due to auto-encoding errors. These two problems are further reinforced with the use of pixel-distance losses. To address these issues, we propose tempering the diffusion model's stochasticity with per-scene customization and mitigating the textural shift with masked adversarial training. During the analyses, we also found the commonly used pixel and perceptual losses are harmful in the NeRF inpainting task. Through rigorous experiments, our framework yields state-of-the-art NeRF inpainting results on various real-world scenes. Project page: https://hubert0527.github.io/MALD-NeRF},
  arxiv     = {2404.09995},
  _url       = {https://hubert0527.github.io/MALD-NeRF},
}

@InProceedings{LinLLY2024,
  author    = {Shanchuan Lin and Bingchen Liu and Jiashi Li and Xiao Yang},
  title     = {Common Diffusion Noise Schedules and Sample Steps are Flawed},
  booktitle = WACV,
  year      = {2024},
  abstract  = {We discover that common diffusion noise schedules do not enforce the last timestep to have zero signal-to-noise ratio (SNR), and some implementations of diffusion samplers do not start from the last timestep. Such designs are flawed and do not reflect the fact that the model is given pure Gaussian noise at inference, creating a discrepancy between training and inference. We show that the flawed design causes real problems in existing implementations. In Stable Diffusion, it severely limits the model to only generate images with medium brightness and prevents it from generating very bright and dark samples. We propose a few simple fixes: (1) rescale the noise schedule to enforce zero terminal SNR; (2) train the model with v prediction; (3) change the sampler to always start from the last timestep; (4) rescale classifier-free guidance to prevent over-exposure. These simple changes ensure the diffusion process is congruent between training and inference and allow the model to generate samples more faithful to the original data distribution.},
  arxiv     = {2305.08891},
  _url       = {https://arxiv.org/abs/2305.08891},
}

@InProceedings{LinDZY2024,
  author    = {Youtian Lin and Zuozhuo Dai and Siyu Zhu and Yao Yao},
  title     = {Gaussian-Flow: {4D} Reconstruction with Dynamic {3D Gaussian} Particle},
  booktitle = CVPR,
  year      = {2024},
  abstract  = {We introduce Gaussian-Flow, a novel point-based approach for fast dynamic scene reconstruction and real-time rendering from both multi-view and monocular videos. In contrast to the prevalent NeRF-based approaches hampered by slow training and rendering speeds, our approach harnesses recent advancements in point-based 3D Gaussian Splatting (3DGS). Specifically, a novel Dual-Domain Deformation Model (DDDM) is proposed to explicitly model attribute deformations of each Gaussian point, where the time-dependent residual of each attribute is captured by a polynomial fitting in the time domain, and a Fourier series fitting in the frequency domain. The proposed DDDM is capable of modeling complex scene deformations across long video footage, eliminating the need for training separate 3DGS for each frame or introducing an additional implicit neural field to model 3D dynamics. Moreover, the explicit deformation modeling for discretized Gaussian points ensures ultra-fast training and rendering of a 4D scene, which is comparable to the original 3DGS designed for static 3D reconstruction. Our proposed approach showcases a substantial efficiency improvement, achieving a $5\times$ faster training speed compared to the per-frame 3DGS modeling. In addition, quantitative results demonstrate that the proposed Gaussian-Flow significantly outperforms previous leading methods in novel view rendering quality. Project page: https://nju-3dv.github.io/projects/Gaussian-Flow},
  arxiv     = {2312.03431},
  _url       = {https://nju-3dv.github.io/projects/Gaussian-Flow},
}

@InProceedings{LingSTZXWYGYLLSAMKKHZBB2024,
  author    = {Lu Ling and Yichen Sheng and Zhi Tu and Wentian Zhao and Cheng Xin and Kun Wan and Lantao Yu and Qianyu Guo and Zixun Yu and Yawen Lu and Xuanmao Li and Xingpeng Sun and Rohan Ashok and Aniruddha Mukherjee and Hao Kang and Xiangrui Kong and Gang Hua and Tianyi Zhang and Bedrich Benes and Aniket Bera},
  title     = {{DL3DV-10K}: A Large-Scale Scene Dataset for Deep Learning-based {3D} Vision},
  booktitle = CVPR,
  year      = {2024},
  abstract  = {We have witnessed significant progress in deep learning-based 3D vision, ranging from neural radiance field (NeRF) based 3D representation learning to applications in novel view synthesis (NVS). However, existing scene-level datasets for deep learning-based 3D vision, limited to either synthetic environments or a narrow selection of real-world scenes, are quite insufficient. This insufficiency not only hinders a comprehensive benchmark of existing methods but also caps what could be explored in deep learning-based 3D analysis. To address this critical gap, we present DL3DV-10K, a large-scale scene dataset, featuring 51.2 million frames from 10,510 videos captured from 65 types of point-of-interest (POI) locations, covering both bounded and unbounded scenes, with different levels of reflection, transparency, and lighting. We conducted a comprehensive benchmark of recent NVS methods on DL3DV-10K, which revealed valuable insights for future research in NVS. In addition, we have obtained encouraging results in a pilot study to learn generalizable NeRF from DL3DV-10K, which manifests the necessity of a large-scale scene-level dataset to forge a path toward a foundation model for learning 3D representation. Our DL3DV-10K dataset, benchmark results, and models will be publicly accessible at https://dl3dv-10k.github.io/DL3DV-10K/.},
  arxiv     = {2312.16256},
  _url       = {https://dl3dv-10k.github.io/DL3DV-10K/},
}

@InProceedings{LipmaCBNL2023,
  author    = {Yaron Lipman and Ricky T. Q. Chen and Heli Ben-Hamu and Maximilian Nickel and Matt Le},
  title     = {Flow Matching for Generative Modeling},
  booktitle = ICLR,
  year      = {2023},
  abstract  = {We introduce a new paradigm for generative modeling built on Continuous Normalizing Flows (CNFs), allowing us to train CNFs at unprecedented scale. Specifically, we present the notion of Flow Matching (FM), a simulation-free approach for training CNFs based on regressing vector fields of fixed conditional probability paths. Flow Matching is compatible with a general family of Gaussian probability paths for transforming between noise and data samples -- which subsumes existing diffusion paths as specific instances. Interestingly, we find that employing FM with diffusion paths results in a more robust and stable alternative for training diffusion models. Furthermore, Flow Matching opens the door to training CNFs with other, non-diffusion probability paths. An instance of particular interest is using Optimal Transport (OT) displacement interpolation to define the conditional probability paths. These paths are more efficient than diffusion paths, provide faster training and sampling, and result in better generalization. Training CNFs using Flow Matching on ImageNet leads to consistently better performance than alternative diffusion-based methods in terms of both likelihood and sample quality, and allows fast and reliable sample generation using off-the-shelf numerical ODE solvers.},
  arxiv     = {2210.02747},
  _url       = {https://arxiv.org/abs/2210.02747},
}

@Unpublished{LiuSWWSYZD2024,
  author    = {Fangfu Liu and Wenqiang Sun and Hanyang Wang and Yikai Wang and Haowen Sun and Junliang Ye and Jun Zhang and Yueqi Duan},
  title     = {{ReconX}: Reconstruct Any Scene from Sparse Views with Video Diffusion Model},
  note      = {\href{https://arxiv.org/abs/2408.16767}{arXiv:2408.16767}},
  year      = {2024},
  abstract  = {Advancements in 3D scene reconstruction have transformed 2D images from the real world into 3D models, producing realistic 3D results from hundreds of input photos. Despite great success in dense-view reconstruction scenarios, rendering a detailed scene from insufficient captured views is still an ill-posed optimization problem, often resulting in artifacts and distortions in unseen areas. In this paper, we propose ReconX, a novel 3D scene reconstruction paradigm that reframes the ambiguous reconstruction challenge as a temporal generation task. The key insight is to unleash the strong generative prior of large pre-trained video diffusion models for sparse-view reconstruction. However, 3D view consistency struggles to be accurately preserved in directly generated video frames from pre-trained models. To address this, given limited input views, the proposed ReconX first constructs a global point cloud and encodes it into a contextual space as the 3D structure condition. Guided by the condition, the video diffusion model then synthesizes video frames that are both detail-preserved and exhibit a high degree of 3D consistency, ensuring the coherence of the scene from various perspectives. Finally, we recover the 3D scene from the generated video through a confidence-aware 3D Gaussian Splatting optimization scheme. Extensive experiments on various real-world datasets show the superiority of our ReconX over state-of-the-art methods in terms of quality and generalizability.},
  arxiv     = {2408.16767},
  _url       = {https://liuff19.github.io/ReconX},
}

@InProceedings{LiuWHTZV2023,
  author    = {Ruoshi Liu and Rundi Wu and Basile Van Hoorick and Pavel Tokmakov and Sergey Zakharov and Carl Vondrick},
  title     = {Zero-1-to-3: Zero-shot One Image to {3D} Object},
  booktitle = ICCV,
  year      = {2023},
  abstract  = {We introduce Zero-1-to-3, a framework for changing the camera viewpoint of an object given just a single RGB image. To perform novel view synthesis in this under-constrained setting, we capitalize on the geometric priors that large-scale diffusion models learn about natural images. Our conditional diffusion model uses a synthetic dataset to learn controls of the relative camera viewpoint, which allow new images to be generated of the same object under a specified camera transformation. Even though it is trained on a synthetic dataset, our model retains a strong zero-shot generalization ability to out-of-distribution datasets as well as in-the-wild images, including impressionist paintings. Our viewpoint-conditioned diffusion approach can further be used for the task of 3D reconstruction from a single image. Qualitative and quantitative experiments show that our method significantly outperforms state-of-the-art single-view 3D reconstruction and novel view synthesis models by leveraging Internet-scale pre-training.},
  arxiv     = {2303.11328},
  _url       = {https://zero123.cs.columbia.edu/},
}

@InProceedings{LiuWHSYZCLL2024,
  author    = {Tianqi Liu and Guangcong Wang and Shoukang Hu and Liao Shen and Xinyi Ye and Yuhang Zang and Zhiguo Cao and Wei Li and Ziwei Liu},
  title     = {{MVSGaussian}: Fast Generalizable Gaussian Splatting Reconstruction from Multi-View Stereo},
  booktitle = ECCV,
  year      = {2024},
  abstract  = {We present MVSGaussian, a new generalizable 3D Gaussian representation approach derived from Multi-View Stereo (MVS) that can efficiently reconstruct unseen scenes. Specifically, 1) we leverage MVS to encode geometry-aware Gaussian representations and decode them into Gaussian parameters. 2) To further enhance performance, we propose a hybrid Gaussian rendering that integrates an efficient volume rendering design for novel view synthesis. 3) To support fast fine-tuning for specific scenes, we introduce a multi-view geometric consistent aggregation strategy to effectively aggregate the point clouds generated by the generalizable model, serving as the initialization for per-scene optimization. Compared with previous generalizable NeRF-based methods, which typically require minutes of fine-tuning and seconds of rendering per image, MVSGaussian achieves real-time rendering with better synthesis quality for each scene. Compared with the vanilla 3D-GS, MVSGaussian achieves better view synthesis with less training computational cost. Extensive experiments on DTU, Real Forward-facing, NeRF Synthetic, and Tanks and Temples datasets validate that MVSGaussian attains state-of-the-art performance with convincing generalizability, real-time rendering speed, and fast per-scene optimization.},
  arxiv     = {2405.12218},
  _url       = {https://mvsgaussian.github.io/},
}

@InProceedings{LiuCKTT2024,
  author    = {Xinhang Liu and Jiaben Chen and Shiu-hong Kao and Yu-Wing Tai and Chi-Keung Tang},
  title     = {{Deceptive-NeRF/3DGS:} Diffusion-Generated Pseudo-Observations for High-Quality Sparse-View Reconstruction},
  booktitle = ECCV,
  year      = {2024},
  abstract  = {Novel view synthesis via Neural Radiance Fields (NeRFs) or 3D Gaussian Splatting (3DGS) typically necessitates dense observations with hundreds of input images to circumvent artifacts. We introduce Deceptive-NeRF/3DGS to enhance sparse-view reconstruction with only a limited set of input images, by leveraging a diffusion model pre-trained from multiview datasets. Different from using diffusion priors to regularize representation optimization, our method directly uses diffusion-generated images to train NeRF/3DGS as if they were real input views. Specifically, we propose a deceptive diffusion model turning noisy images rendered from few-view reconstructions into highquality photorealistic pseudo-observations. To resolve consistency among pseudo-observations and real input views, we develop an uncertainty measure to guide the diffusion model's generation. Our system progressively incorporates diffusion-generated pseudo-observations into the training image sets, ultimately densifying the sparse input observations by 5 to 10 times. Extensive experiments across diverse and challenging datasets validate that our approach outperforms existing state-of-the-art methods and is capable of synthesizing novel views with super-resolution in the few-view setting.},
  arxiv     = {2305.15171},
  _url       = {https://xinhangliu.com/deceptive-nerf-3dgs},
}

@InProceedings{LiuZH2024,
  author    = {Xi Liu and Chaoyi Zhou and Siyu Huang},
  title     = {{3DGS}-Enhancer: Enhancing Unbounded {3D} Gaussian Splatting with View-consistent {2D} Diffusion Priors},
  booktitle = NeurIPS,
  year      = {2024},
  abstract  = {Novel-view synthesis aims to generate novel views of a scene from multiple input images or videos, and recent advancements like 3D Gaussian splatting (3DGS) have achieved notable success in producing photorealistic renderings with efficient pipelines. However, generating high-quality novel views under challenging settings, such as sparse input views, remains difficult due to insufficient information in under-sampled areas, often resulting in noticeable artifacts. This paper presents 3DGS-Enhancer, a novel pipeline for enhancing the representation quality of 3DGS representations. We leverage 2D video diffusion priors to address the challenging 3D view consistency problem, reformulating it as achieving temporal consistency within a video generation process. 3DGS-Enhancer restores view-consistent latent features of rendered novel views and integrates them with the input views through a spatial-temporal decoder. The enhanced views are then used to fine-tune the initial 3DGS model, significantly improving its rendering performance. Extensive experiments on large-scale datasets of unbounded scenes demonstrate that 3DGS-Enhancer yields superior reconstruction performance and high-fidelity rendering results compared to state-of-the-art methods. The project webpage is https://xiliu8006.github.io/3DGS-Enhancer-project.},
  arxiv     = {2410.16266},
  _url       = {https://xiliu8006.github.io/3DGS-Enhancer-project},
}

@Article{MildeSTBRN2022,
  author    = {Ben Mildenhall and Pratul P. Srinivasan and Matthew Tancik and Jonathan T. Barron and Ravi Ramamoorthi and Ren Ng},
  title     = {{NeRF}: Representing Scenes as Neural Radiance Fields for View Synthesis},
  journal   = {Communications of the ACM},
  year      = {2022},
  volume    = {65},
  number    = {1},
  pages     = {99--106},
  _issn      = {0001-0782},
  abstract  = {We present a method that achieves state-of-the-art results for synthesizing novel views of complex scenes by optimizing an underlying continuous volumetric scene function using a sparse set of input views. Our algorithm represents a scene using a fully connected (nonconvolutional) deep network, whose input is a single continuous 5D coordinate (spatial location (x, y, z) and viewing direction (θ, ϕ)) and whose output is the volume density and view-dependent emitted radiance at that spatial location. We synthesize views by querying 5D coordinates along camera rays and use classic volume rendering techniques to project the output colors and densities into an image. Because volume rendering is naturally differentiable, the only input required to optimize our representation is a set of images with known camera poses. We describe how to effectively optimize neural radiance fields to render photorealistic novel views of scenes with complicated geometry and appearance, and demonstrate results that outperform prior work on neural rendering and view synthesis.},
  doi       = {10.1145/3503250},
}

@InProceedings{MildeSTBRN2020,
  author    = {Ben Mildenhall and Pratul P. Srinivasan and Matthew Tancik and Jonathan T. Barron and Ravi Ramamoorthi and Ren Ng},
  title     = {{NeRF}: Representing Scenes as Neural Radiance Fields for View Synthesis},
  booktitle = ECCV,
  year      = {2020},
  abstract  = {We present a method that achieves state-of-the-art results for synthesizing novel views of complex scenes by optimizing an underlying continuous volumetric scene function using a sparse set of input views. Our algorithm represents a scene using a fully-connected (non-convolutional) deep network, whose input is a single continuous 5D coordinate (spatial location $(x,y,z)$ and viewing direction $(\theta, \phi)$) and whose output is the volume density and view-dependent emitted radiance at that spatial location. We synthesize views by querying 5D coordinates along camera rays and use classic volume rendering techniques to project the output colors and densities into an image. Because volume rendering is naturally differentiable, the only input required to optimize our representation is a set of images with known camera poses. We describe how to effectively optimize neural radiance fields to render photorealistic novel views of scenes with complicated geometry and appearance, and demonstrate results that outperform prior work on neural rendering and view synthesis. View synthesis results are best viewed as videos, so we urge readers to view our supplementary video for convincing comparisons.},
  arxiv     = {2003.08934},
  doi       = {10.1007/978-3-030-58452-8_24},
  _url       = {http://www.matthewtancik.com/nerf},
}

@InProceedings{MirzaADKBGL2023,
  author    = {Ashkan Mirzaei and Tristan Aumentado-Armstrong and Konstantinos G. Derpanis and Jonathan Kelly and Marcus A. Brubaker and Igor Gilitschenski and Alex Levinshtein},
  title     = {{SPIn-NeRF}: Multiview Segmentation and Perceptual Inpainting with Neural Radiance Fields},
  booktitle = CVPR,
  year      = {2023},
  abstract  = {Neural Radiance Fields (NeRFs) have emerged as a popular approach for novel view synthesis. While NeRFs are quickly being adapted for a wider set of applications, intuitively editing NeRF scenes is still an open challenge. One important editing task is the removal of unwanted objects from a 3D scene, such that the replaced region is visually plausible and consistent with its context. We refer to this task as 3D inpainting. In 3D, solutions must be both consistent across multiple views and geometrically valid. In this paper, we propose a novel 3D inpainting method that addresses these challenges. Given a small set of posed images and sparse annotations in a single input image, our framework first rapidly obtains a 3D segmentation mask for a target object. Using the mask, a perceptual optimizationbased approach is then introduced that leverages learned 2D image inpainters, distilling their information into 3D space, while ensuring view consistency. We also address the lack of a diverse benchmark for evaluating 3D scene inpainting methods by introducing a dataset comprised of challenging real-world scenes. In particular, our dataset contains views of the same scene with and without a target object, enabling more principled benchmarking of the 3D inpainting task. We first demonstrate the superiority of our approach on multiview segmentation, comparing to NeRFbased methods and 2D segmentation approaches. We then evaluate on the task of 3D inpainting, establishing state-ofthe-art performance against other NeRF manipulation algorithms, as well as a strong 2D image inpainter baseline},
  arxiv     = {2211.12254},
  _url       = {https://spinnerf3d.github.io/},
}

@InProceedings{MuelleSRPRNK2024,
  author    = {Norman Müller and Katja Schwarz and Barbara Rössle and Lorenzo Porzi and Rota Bulò, Samuel and Matthias Nießner and Peter Kontschieder},
  title     = {{MultiDiff}: Consistent Novel View Synthesis from a Single Image},
  booktitle = CVPR,
  year      = {2024},
  _month     = jun,
  _pages    = {10258-10268},
  abstract  = {We introduce MultiDiff a novel approach for consistent novel view synthesis of scenes from a single RGB image. The task of synthesizing novel views from a single reference image is highly ill-posed by nature as there exist multiple plausible explanations for unobserved areas. To address this issue we incorporate strong priors in form of monocular depth predictors and video-diffusion models. Monocular depth enables us to condition our model on warped reference images for the target views increasing geometric stability. The video-diffusion prior provides a strong proxy for 3D scenes allowing the model to learn continuous and pixel-accurate correspondences across generated images. In contrast to approaches relying on autoregressive image generation that are prone to drifts and error accumulation MultiDiff jointly synthesizes a sequence of frames yielding high-quality and multi-view consistent results -- even for long-term scene generation with large camera movements while reducing inference time by an order of magnitude. For additional consistency and image quality improvements we introduce a novel structured noise distribution. Our experimental results demonstrate that MultiDiff outperforms state-of-the-art methods on the challenging real-world datasets RealEstate10K and ScanNet. Finally our model naturally supports multi-view consistent editing without the need for further tuning.},
  _url       = {https://sirwyver.github.io/MultiDiff/},
}

@InProceedings{PooleJBM2023,
  author    = {Ben Poole and Ajay Jain and Jonathan T. Barron and Ben Mildenhall},
  title     = {{DreamFusion}: Text-to-{3D} using {2D} Diffusion},
  booktitle = ICLR,
  year      = {2023},
  abstract  = {Recent breakthroughs in text-to-image synthesis have been driven by diffusion models trained on billions of image-text pairs. Adapting this approach to 3D synthesis would require large-scale datasets of labeled 3D data and efficient architectures for denoising 3D data, neither of which currently exist. In this work, we circumvent these limitations by using a pretrained 2D text-to-image diffusion model to perform text-to-3D synthesis. We introduce a loss based on probability density distillation that enables the use of a 2D diffusion model as a prior for optimization of a parametric image generator. Using this loss in a DeepDream-like procedure, we optimize a randomly-initialized 3D model (a Neural Radiance Field, or NeRF) via gradient descent such that its 2D renderings from random angles achieve a low loss. The resulting 3D model of the given text can be viewed from any angle, relit by arbitrary illumination, or composited into any 3D environment. Our approach requires no 3D training data and no modifications to the image diffusion model, demonstrating the effectiveness of pretrained image diffusion models as priors.},
  arxiv     = {2209.14988},
  _url       = {https://dreamfusion3d.github.io/},
}

@Unpublished{PrabhWTHGPB2023,
  author    = {Kira Prabhu and Jane Wu and Lynn Tsai and Peter Hedman and Dan B Goldman and Ben Poole and Michael Broxton},
  title     = {{Inpaint3D}: {3D} Scene Content Generation using {2D} Inpainting Diffusion},
  note      = {\href{https://arxiv.org/abs/2312.03869}{arXiv:2312.03869}},
  year      = {2023},
  abstract  = {This paper presents a novel approach to inpainting 3D regions of a scene, given masked multi-view images, by distilling a 2D diffusion model into a learned 3D scene representation (e.g. a NeRF). Unlike 3D generative methods that explicitly condition the diffusion model on camera pose or multi-view information, our diffusion model is conditioned only on a single masked 2D image. Nevertheless, we show that this 2D diffusion model can still serve as a generative prior in a 3D multi-view reconstruction problem where we optimize a NeRF using a combination of score distillation sampling and NeRF reconstruction losses. Predicted depth is used as additional supervision to encourage accurate geometry. We compare our approach to 3D inpainting methods that focus on object removal. Because our method can generate content to fill any 3D masked region, we additionally demonstrate 3D object completion, 3D object replacement, and 3D scene completion.},
  arxiv     = {2312.03869},
  _url       = {https://inpaint3d.github.io/},
}

@InProceedings{RaistLMMWZKWHWNLGYD2023,
  author    = {Alexander Raistrick and Lahav Lipson and Zeyu Ma and Lingjie Mei and Mingzhe Wang and Yiming Zuo and Karhan Kayan and Hongyu Wen and Beining Han and Yihan Wang and Alejandro Newell and Hei Law and Ankit Goyal and Kaiyu Yang and Jia Deng},
  title     = {Infinite Photorealistic Worlds using Procedural Generation},
  booktitle = CVPR,
  year      = {2023},
  abstract  = {We introduce Infinigen, a procedural generator of photorealistic 3D scenes of the natural world. Infinigen is entirely procedural: every asset, from shape to texture, is generated from scratch via randomized mathematical rules, using no external source and allowing infinite variation and composition. Infinigen offers broad coverage of objects and scenes in the natural world including plants, animals, terrains, and natural phenomena such as fire, cloud, rain, and snow. Infinigen can be used to generate unlimited, diverse training data for a wide range of computer vision tasks including object detection, semantic segmentation, optical flow, and 3D reconstruction. We expect Infinigen to be a useful resource for computer vision research and beyond. Please visit https://infinigen.orgfor videos, code and pre-generated data.},
  arxiv     = {2306.09310},
  _url       = {https://infinigen.org},
}

@InProceedings{RombaBLEO2022,
  author    = {Robin Rombach and Andreas Blattmann and Dominik Lorenz and Patrick Esser and Björn Ommer},
  title     = {High-Resolution Image Synthesis with Latent Diffusion Models},
  booktitle = CVPR,
  year      = {2022},
  abstract  = {By decomposing the image formation process into a sequential application of denoising autoencoders, diffusion models (DMs) achieve state-of-the-art synthesis results on image data and beyond. Additionally, their formulation allows for a guiding mechanism to control the image generation process without retraining. However, since these models typically operate directly in pixel space, optimization of powerful DMs often consumes hundreds of GPU days and inference is expensive due to sequential evaluations. To enable DM training on limited computational resources while retaining their quality and flexibility, we apply them in the latent space of powerful pretrained autoencoders. In contrast to previous work, training diffusion models on such a representation allows for the first time to reach a near-optimal point between complexity reduction and detail preservation, greatly boosting visual fidelity. By introducing cross-attention layers into the model architecture, we turn diffusion models into powerful and flexible generators for general conditioning inputs such as text or bounding boxes and high-resolution synthesis becomes possible in a convolutional manner. Our latent diffusion models (LDMs) achieve a new state of the art for image inpainting and highly competitive performance on various tasks, including unconditional image generation, semantic scene synthesis, and super-resolution, while significantly reducing computational requirements compared to pixel-based DMs. Code is available at https://github.com/CompVis/latent-diffusion.},
  arxiv     = {2112.10752},
  _url       = {https://github.com/CompVis/latent-diffusion},
}

@InProceedings{RonenLG2023,
  author    = {Tomer Ronen and Omer Levy and Avram Golbert},
  title     = {Vision Transformers With Mixed-Resolution Tokenization},
  booktitle = CVPRW,
  year      = {2023},
  _month     = jun,
  _pages    = {4613-4622},
  abstract  = {Vision Transformer models process input images by dividing them into a spatially regular grid of equal-size patches. Conversely, Transformers were originally introduced over natural language sequences, where each token represents a subword - a chunk of raw data of arbitrary size. In this work, we apply this approach to Vision Transformers by introducing a novel image tokenization scheme, replacing the standard uniform grid with a mixed-resolution sequence of tokens, where each token represents a patch of arbitrary size. Using the Quadtree algorithm and a novel saliency scorer, we construct a patch mosaic where low-saliency areas of the image are processed in low resolution, routing more of the model's capacity to important image regions. Using the same architecture as vanilla Vision Transformers, our Quadformer models achieve substantial accuracy gains on image classification when controlling for the computational budget. Code and models are publicly available at https://github.com/TomerRonen34/mixed-resolution-vit.},
  _url       = {https://github.com/TomerRonen34/mixed-resolution-vit},
}

@Unpublished{SadatHW2024,
  author    = {Seyedmorteza Sadat and Otmar Hilliges and Romann M. Weber},
  title     = {Eliminating Oversaturation and Artifacts of High Guidance Scales in Diffusion Models},
  note      = {\href{https://arxiv.org/abs/2410.02416}{arXiv:2410.02416}},
  year      = {2024},
  abstract  = {Classifier-free guidance (CFG) is crucial for improving both generation quality and alignment between the input condition and final output in diffusion models. While a high guidance scale is generally required to enhance these aspects, it also causes oversaturation and unrealistic artifacts. In this paper, we revisit the CFG update rule and introduce modifications to address this issue. We first decompose the update term in CFG into parallel and orthogonal components with respect to the conditional model prediction and observe that the parallel component primarily causes oversaturation, while the orthogonal component enhances image quality. Accordingly, we propose down-weighting the parallel component to achieve high-quality generations without oversaturation. Additionally, we draw a connection between CFG and gradient ascent and introduce a new rescaling and momentum method for the CFG update rule based on this insight. Our approach, termed adaptive projected guidance (APG), retains the quality-boosting advantages of CFG while enabling the use of higher guidance scales without oversaturation. APG is easy to implement and introduces practically no additional computational overhead to the sampling process. Through extensive experiments, we demonstrate that APG is compatible with various conditional diffusion models and samplers, leading to improved FID, recall, and saturation scores while maintaining precision comparable to CFG, making our method a superior plug-and-play alternative to standard classifier-free guidance.},
  arxiv     = {2410.02416},
  _url       = {https://arxiv.org/abs/2410.02416},
}

@InProceedings{SargeLSHYZCLFSW2024,
  author    = {Kyle Sargent and Zizhang Li and Tanmay Shah and Charles Herrmann and Hong-Xing Yu and Yunzhi Zhang and Eric Ryan Chan and Dmitry Lagun and Li Fei-Fei and Deqing Sun and Jiajun Wu},
  title     = {{ZeroNVS}: Zero-Shot 360-Degree View Synthesis from a Single Real Image},
  booktitle = CVPR,
  year      = {2024},
  abstract  = {We introduce a 3D-aware diffusion model, ZeroNVS, for single-image novel view synthesis for in-the-wild scenes. While existing methods are designed for single objects with masked backgrounds, we propose new techniques to address challenges introduced by in-the-wild multi-object scenes with complex backgrounds. Specifically, we train a generative prior on a mixture of data sources that capture object-centric, indoor, and outdoor scenes. To address issues from data mixture such as depth-scale ambiguity, we propose a novel camera conditioning parameterization and normalization scheme. Further, we observe that Score Distillation Sampling (SDS) tends to truncate the distribution of complex backgrounds during distillation of 360-degree scenes, and propose "SDS anchoring" to improve the diversity of synthesized novel views. Our model sets a new state-of-the-art result in LPIPS on the DTU dataset in the zero-shot setting, even outperforming methods specifically trained on DTU. We further adapt the challenging Mip-NeRF 360 dataset as a new benchmark for single-image novel view synthesis, and demonstrate strong performance in this setting. Our code and data are at http://kylesargent.github.io/zeronvs/},
  arxiv     = {2310.17994},
  _url       = {https://kylesargent.github.io/zeronvs/},
}

@InProceedings{SeoFSNMHLKM2024,
  author    = {Junyoung Seo and Kazumi Fukuda and Takashi Shibuya and Takuya Narihira and Naoki Murata and Shoukang Hu and Chieh-Hsin Lai and Seungryong Kim and Yuki Mitsufuji},
  title     = {{GenWarp}: Single Image to Novel Views with Semantic-Preserving Generative Warping},
  booktitle = NeurIPS,
  year      = {2024},
  abstract  = {Generating novel views from a single image remains a challenging task due to the complexity of 3D scenes and the limited diversity in the existing multi-view datasets to train a model on. Recent research combining large-scale text-to-image (T2I) models with monocular depth estimation (MDE) has shown promise in handling in-the-wild images. In these methods, an input view is geometrically warped to novel views with estimated depth maps, then the warped image is inpainted by T2I models. However, they struggle with noisy depth maps and loss of semantic details when warping an input view to novel viewpoints. In this paper, we propose a novel approach for single-shot novel view synthesis, a semantic-preserving generative warping framework that enables T2I generative models to learn where to warp and where to generate, through augmenting cross-view attention with self-attention. Our approach addresses the limitations of existing methods by conditioning the generative model on source view images and incorporating geometric warping signals. Qualitative and quantitative evaluations demonstrate that our model outperforms existing methods in both in-domain and out-of-domain scenarios. Project page is available at https://GenWarp-NVS.github.io/.},
  arxiv     = {2405.17251},
  _url       = {https://GenWarp-NVS.github.io},
}

@InProceedings{ShenCGXMWF2024,
  author    = {Yuan Shen and Duygu Ceylan and Paul Guerrero and Zexiang Xu and Niloy J. Mitra and Shenlong Wang and Anna Frühstück},
  title     = {{SuperGaussian}: Repurposing Video Models for {3D} Super Resolution},
  booktitle = ECCV,
  year      = {2024},
  abstract  = {We present a simple, modular, and generic method that upsamples coarse 3D models by adding geometric and appearance details. While generative 3D models now exist, they do not yet match the quality of their counterparts in image and video domains. We demonstrate that it is possible to directly repurpose existing (pretrained) video models for 3D super-resolution and thus sidestep the problem of the shortage of large repositories of high-quality 3D training models. We describe how to repurpose video upsampling models, which are not 3D consistent, and combine them with 3D consolidation to produce 3D-consistent results. As output, we produce high quality Gaussian Splat models, which are object centric and effective. Our method is category agnostic and can be easily incorporated into existing 3D workflows. We evaluate our proposed SuperGaussian on a variety of 3D inputs, which are diverse both in terms of complexity and representation (e.g., Gaussian Splats or NeRFs), and demonstrate that our simple method significantly improves the fidelity of the final 3D models. Check our project website for details: http://supergaussian.github.io},
  arxiv     = {2406.00609},
  _url       = {https://supergaussian.github.io},
}

@InProceedings{ShiWYLLY2024,
  author    = {Yichun Shi and Peng Wang and Jianglong Ye and Mai Long and Kejie Li and Xiao Yang},
  title     = {{MVDream}: Multi-view Diffusion for {3D} Generation},
  booktitle = ICLR,
  year      = {2024},
  abstract  = {We propose MVDream, a multi-view diffusion model that is able to generate geometrically consistent multi-view images from a given text prompt. By leveraging image diffusion models pre-trained on large-scale web datasets and a multi-view dataset rendered from 3D assets, the resulting multi-view diffusion model can achieve both the generalizability of 2D diffusion and the consistency of 3D data. Such a model can thus be applied as a multi-view prior for 3D generation via Score Distillation Sampling, where it greatly improves the stability of existing 2D-lifting methods by solving the 3D consistency problem. Finally, we show that the multi-view diffusion model can also be fine-tuned under a few shot setting for personalized 3D generation, i.e. DreamBooth3D application, where the consistency can be maintained after learning the subject identity.},
  arxiv     = {2308.16512},
  _url       = {https://mv-dream.github.io/},
}

@InProceedings{ShihMHCCK2024,
  author    = {Meng-Li Shih and Wei-Chiu Ma and Aleksander Holynski and Forrester Cole and Brian L. Curless and Janne Kontkanen},
  title     = {{ExtraNeRF}: Visibility-Aware View Extrapolation of Neural Radiance Fields with Diffusion Models},
  booktitle = CVPR,
  year      = {2024},
  abstract  = {We propose ExtraNeRF, a novel method for extrapolating the range of views handled by a Neural Radiance Field (NeRF). Our main idea is to leverage NeRFs to model scene-specific, fine-grained details, while capitalizing on diffusion models to extrapolate beyond our observed data. A key ingredient is to track visibility to determine what portions of the scene have not been observed, and focus on reconstructing those regions consistently with diffusion models. Our primary contributions include a visibility-aware diffusion-based inpainting module that is fine-tuned on the input imagery, yielding an initial NeRF with moderate quality (often blurry) inpainted regions, followed by a second diffusion model trained on the input imagery to consistently enhance, notably sharpen, the inpainted imagery from the first pass. We demonstrate high-quality results, extrapolating beyond a small number of (typically six or fewer) input views, effectively outpainting the NeRF as well as inpainting newly disoccluded regions inside the original viewing volume. We compare with related work both quantitatively and qualitatively and show significant gains over prior art.},
  arxiv     = {2406.06133},
  _url       = {https://shihmengli.github.io/extranerf-website/},
}

@Unpublished{ShrirTLR2024,
  author    = {Jaidev Shriram and Alex Trevithick and Lingjie Liu and Ravi Ramamoorthi},
  title     = {{RealmDreamer}: Text-Driven {3D} Scene Generation with Inpainting and Depth Diffusion},
  note      = {\href{https://arxiv.org/abs/2404.07199}{arXiv:2404.07199}},
  year      = {2024},
  abstract  = {We introduce RealmDreamer, a technique for generation of general forward-facing 3D scenes from text descriptions. Our technique optimizes a 3D Gaussian Splatting representation to match complex text prompts. We initialize these splats by utilizing the state-of-the-art text-to-image generators, lifting their samples into 3D, and computing the occlusion volume. We then optimize this representation across multiple views as a 3D inpainting task with image-conditional diffusion models. To learn correct geometric structure, we incorporate a depth diffusion model by conditioning on the samples from the inpainting model, giving rich geometric structure. Finally, we finetune the model using sharpened samples from image generators. Notably, our technique does not require video or multi-view data and can synthesize a variety of high-quality 3D scenes in different styles, consisting of multiple objects. Its generality additionally allows 3D synthesis from a single image.},
  arxiv     = {2404.07199},
  _url       = {https://realmdreamer.github.io/},
}

@Unpublished{StrauDSYSN2024,
  author    = {Julian Straub and Daniel DeTone and Tianwei Shen and Nan Yang and Chris Sweeney and Richard Newcombe},
  title     = {{EFM3D}: A Benchmark for Measuring Progress Towards {3D} Egocentric Foundation Models},
  note      = {\href{https://arxiv.org/abs/2406.10224}{arXiv:2406.10224}},
  year      = {2024},
  abstract  = {The advent of wearable computers enables a new source of context for AI that is embedded in egocentric sensor data. This new egocentric data comes equipped with fine-grained 3D location information and thus presents the opportunity for a novel class of spatial foundation models that are rooted in 3D space. To measure progress on what we term Egocentric Foundation Models (EFMs) we establish EFM3D, a benchmark with two core 3D egocentric perception tasks. EFM3D is the first benchmark for 3D object detection and surface regression on high quality annotated egocentric data of Project Aria. We propose Egocentric Voxel Lifting (EVL), a baseline for 3D EFMs. EVL leverages all available egocentric modalities and inherits foundational capabilities from 2D foundation models. This model, trained on a large simulated dataset, outperforms existing methods on the EFM3D benchmark.},
  arxiv     = {2406.10224},
  _url       = {https://arxiv.org/abs/2406.10224},
}

@InProceedings{Sturm2011,
  author    = {Peter Sturm},
  title     = {A historical survey of geometric computer vision},
  booktitle = {International Conference on Computer Analysis of Images and Patterns (CAIP)},
  year      = {2011},
  abstract  = {This short paper accompanies an invited lecture on a historical survey of geometric computer vision problems. It presents some early works on image-based 3D modeling, multi-view geometry, and structure-from-motion, from the last three centuries. Some of these are relatively well known to photogrammetrists and computer vision researchers whereas others seem to have been largely forgotten or overlooked. This paper gives a very brief summary of an ongoing historical study.},
  doi       = {10.1007/978-3-642-23672-3_1},
}

@InProceedings{SunSWBZ2021,
  author    = {Jiaming Sun and Zehong Shen and Yuang Wang and Hujun Bao and Xiaowei Zhou},
  title     = {{LoFTR}: Detector-Free Local Feature Matching with Transformers},
  booktitle = CVPR,
  year      = {2021},
  abstract  = {We present a novel method for local image feature matching. Instead of performing image feature detection, description, and matching sequentially, we propose to first establish pixel-wise dense matches at a coarse level and later refine the good matches at a fine level. In contrast to dense methods that use a cost volume to search correspondences, we use self and cross attention layers in Transformer to obtain feature descriptors that are conditioned on both images. The global receptive field provided by Transformer enables our method to produce dense matches in low-texture areas, where feature detectors usually struggle to produce repeatable interest points. The experiments on indoor and outdoor datasets show that LoFTR outperforms state-of-the-art methods by a large margin. LoFTR also ranks first on two public benchmarks of visual localization among the published methods.},
  arxiv     = {2104.00680},
  _url       = {https://zju3dv.github.io/loftr/},
}

@InProceedings{SunZGWHCR2024,
  author    = {Su Sun and Cheng Zhao and Yuliang Guo and Ruoyu Wang and Xinyu Huang and Yingjie Victor Chen and Liu Ren},
  title     = {Behind the Veil: Enhanced Indoor {3D} Scene Reconstruction with Occluded Surfaces Completion},
  booktitle = CVPR,
  year      = {2024},
  _month     = jun,
  _pages    = {12744-12753},
  abstract  = {In this paper we present a novel indoor 3D reconstruction method with occluded surface completion given a sequence of depth readings. Prior state-of-the-art (SOTA) methods only focus on the reconstruction of the visible areas in a scene neglecting the invisible areas due to the occlusions e.g. the contact surface between furniture occluded wall and floor. Our method tackles the task of completing the occluded scene surfaces resulting in a complete 3D scene mesh. The core idea of our method is learning 3D geometry prior from various complete scenes to infer the occluded geometry of an unseen scene from solely depth measurements. We design a coarse-fine hierarchical octree representation coupled with a dual-decoder architecture i.e. Geo-decoder and 3D Inpainter which jointly reconstructs the complete 3D scene geometry. The Geo-decoder with detailed representation at fine levels is optimized online for each scene to reconstruct visible surfaces. The 3D Inpainter with abstract representation at coarse levels is trained offline using various scenes to complete occluded surfaces. As a result while the Geo-decoder is specialized for an individual scene the 3D Inpainter can be generally applied across different scenes. We evaluate the proposed method on the 3D Completed Room Scene (3D-CRS) and iTHOR datasets significantly outperforming the SOTA methods by a gain of 16.8% and 24.2% in terms of the completeness of 3D reconstruction. 3D-CRS dataset including a complete 3D mesh of each scene is provided at project webpage.},
  _url       = {https://openaccess.thecvf.com//content/CVPR2024/html/Sun_Behind_the_Veil_Enhanced_Indoor_3D_Scene_Reconstruction_with_Occluded_CVPR_2024_paper.html},
}

@Unpublished{SunCLCDZW2024,
  author    = {Wenqiang Sun and Shuo Chen and Fangfu Liu and Zilong Chen and Yueqi Duan and Jun Zhang and Yikai Wang},
  title     = {{DimensionX}: Create Any {3D} and {4D} Scenes from a Single Image with Controllable Video Diffusion},
  note      = {\href{https://arxiv.org/abs/2411.04928}{arXiv:2411.04928}},
  year      = {2024},
  abstract  = {In this paper, we introduce \textbfDimensionX, a framework designed to generate photorealistic 3D and 4D scenes from just a single image with video diffusion. Our approach begins with the insight that both the spatial structure of a 3D scene and the temporal evolution of a 4D scene can be effectively represented through sequences of video frames. While recent video diffusion models have shown remarkable success in producing vivid visuals, they face limitations in directly recovering 3D/4D scenes due to limited spatial and temporal controllability during generation. To overcome this, we propose ST-Director, which decouples spatial and temporal factors in video diffusion by learning dimension-aware LoRAs from dimension-variant data. This controllable video diffusion approach enables precise manipulation of spatial structure and temporal dynamics, allowing us to reconstruct both 3D and 4D representations from sequential frames with the combination of spatial and temporal dimensions. Additionally, to bridge the gap between generated videos and real-world scenes, we introduce a trajectory-aware mechanism for 3D generation and an identity-preserving denoising strategy for 4D generation. Extensive experiments on various real-world and synthetic datasets demonstrate that DimensionX achieves superior results in controllable video generation, as well as in 3D and 4D scene generation, compared with previous methods.},
  arxiv     = {2411.04928},
  _url       = {https://chenshuo20.github.io/DimensionX/},
}

@InProceedings{TangZCWF2023,
  author    = {Shitao Tang and Fuyang Zhang and Jiacheng Chen and Peng Wang and Yasutaka Furukawa},
  title     = {{MVDiffusion}: Enabling Holistic Multi-view Image Generation with Correspondence-Aware Diffusion},
  booktitle = NeurIPS,
  year      = {2023},
  abstract  = {This paper introduces MVDiffusion, a simple yet effective multi-view image generation method for scenarios where pixel-to-pixel correspondences are available, such as perspective crops from panorama or multi-view images given geometry (depth maps and poses). Unlike prior models that rely on iterative image warping and inpainting, MVDiffusion concurrently generates all images with a global awareness, encompassing high resolution and rich content, effectively addressing the error accumulation prevalent in preceding models. MVDiffusion specifically incorporates a correspondence-aware attention mechanism, enabling effective cross-view interaction. This mechanism underpins three pivotal modules: 1) a generation module that produces low-resolution images while maintaining global correspondence, 2) an interpolation module that densifies spatial coverage between images, and 3) a super-resolution module that upscales into high-resolution outputs. In terms of panoramic imagery, MVDiffusion can generate high-resolution photorealistic images up to 1024$\times$1024 pixels. For geometry-conditioned multi-view image generation, MVDiffusion demonstrates the first method capable of generating a textured map of a scene mesh. The project page is at https://mvdiffusion.github.io.},
  arxiv     = {2307.01097},
  _url       = {https://mvdiffusion.github.io/},
}

@InProceedings{TewarYCRTDFS2023,
  author    = {Ayush Tewari and Tianwei Yin and George Cazenavette and Semon Rezchikov and Joshua B. Tenenbaum and Frédo Durand and William T. Freeman and Vincent Sitzmann},
  title     = {Diffusion with Forward Models: Solving Stochastic Inverse Problems Without Direct Supervision},
  booktitle = NeurIPS,
  year      = {2023},
  abstract  = {Denoising diffusion models are a powerful type of generative models used to capture complex distributions of real-world signals. However, their applicability is limited to scenarios where training samples are readily available, which is not always the case in real-world applications. For example, in inverse graphics, the goal is to generate samples from a distribution of 3D scenes that align with a given image, but ground-truth 3D scenes are unavailable and only 2D images are accessible. To address this limitation, we propose a novel class of denoising diffusion probabilistic models that learn to sample from distributions of signals that are never directly observed. Instead, these signals are measured indirectly through a known differentiable forward model, which produces partial observations of the unknown signal. Our approach involves integrating the forward model directly into the denoising process. This integration effectively connects the generative modeling of observations with the generative modeling of the underlying signals, allowing for end-to-end training of a conditional generative model over signals. During inference, our approach enables sampling from the distribution of underlying signals that are consistent with a given partial observation. We demonstrate the effectiveness of our method on three challenging computer vision tasks. For instance, in the context of inverse graphics, our model enables direct sampling from the distribution of 3D scenes that align with a single 2D input image.},
  arxiv     = {2306.11719},
  _url       = {https://diffusion-with-forward-models.github.io/},
}

@InProceedings{TurkuRMSRK2025,
  author    = {Matias Turkulainen and Xuqian Ren and Iaroslav Melekhov and Otto Seiskari and Esa Rahtu and Juho Kannala},
  title     = {{DN}-Splatter: Depth and Normal Priors for {Gaussian} Splatting and Meshing},
  booktitle = WACV,
  year      = {2025},
  abstract  = {3D Gaussian splatting, a novel differentiable rendering technique, has achieved state-of-the-art novel view synthesis results with high rendering speeds and relatively low training times. However, its performance on scenes commonly seen in indoor datasets is poor due to the lack of geometric constraints during optimization. We extend 3D Gaussian splatting with depth and normal cues to tackle challenging indoor datasets and showcase techniques for efficient mesh extraction, an important downstream application. Specifically, we regularize the optimization procedure with depth information, enforce local smoothness of nearby Gaussians, and use the geometry of the 3D Gaussians supervised by normal cues to achieve better alignment with the true scene geometry. We improve depth estimation and novel view synthesis results over baselines and show how this simple yet effective regularization technique can be used to directly extract meshes from the Gaussian representation yielding more physically accurate reconstructions on indoor scenes. Our code will be released in https://github.com/maturk/dn-splatter.},
  arxiv     = {2403.17822},
  _url       = {https://github.com/maturk/dn-splatter},
}

@InProceedings{VanHWOSLTDZV2024,
  author    = {Van Hoorick, Basile and Rundi Wu and Ege Ozguroglu and Kyle Sargent and Ruoshi Liu and Pavel Tokmakov and Achal Dave and Changxi Zheng and Carl Vondrick},
  title     = {Generative Camera Dolly: Extreme Monocular Dynamic Novel View Synthesis},
  booktitle = ECCV,
  year      = {2024},
  abstract  = {Accurate reconstruction of complex dynamic scenes from just a single viewpoint continues to be a challenging task in computer vision. Current dynamic novel view synthesis methods typically require videos from many different camera viewpoints, necessitating careful recording setups, and significantly restricting their utility in the wild as well as in terms of embodied AI applications. In this paper, we propose $\textbfGCD$, a controllable monocular dynamic view synthesis pipeline that leverages large-scale diffusion priors to, given a video of any scene, generate a synchronous video from any other chosen perspective, conditioned on a set of relative camera pose parameters. Our model does not require depth as input, and does not explicitly model 3D scene geometry, instead performing end-to-end video-to-video translation in order to achieve its goal efficiently. Despite being trained on synthetic multi-view video data only, zero-shot real-world generalization experiments show promising results in multiple domains, including robotics, object permanence, and driving environments. We believe our framework can potentially unlock powerful applications in rich dynamic scene understanding, perception for robotics, and interactive 3D video viewing experiences for virtual reality.},
  arxiv     = {2405.14868},
  _url       = {https://gcd.cs.columbia.edu/},
}

@InProceedings{VaswaSPUJGKP2017,
  author    = {Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
  title     = {Attention Is All You Need},
  booktitle = NIPS,
  year      = {2017},
  abstract  = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
  arxiv     = {1706.03762},
  _url       = {https://arxiv.org/abs/1706.03762},
}

@InProceedings{VerbiSHMASB2024,
  author    = {Dor Verbin and Pratul P. Srinivasan and Peter Hedman and Ben Mildenhall and Benjamin Attal and Richard Szeliski and Jonathan T. Barron},
  title     = {{NeRF}-Casting: Improved View-Dependent Appearance with Consistent Reflections},
  booktitle = SIGAsiaConf,
  year      = {2024},
  abstract  = {Neural Radiance Fields (NeRFs) typically struggle to reconstruct and render highly specular objects, whose appearance varies quickly with changes in viewpoint. Recent works have improved NeRF's ability to render detailed specular appearance of distant environment illumination, but are unable to synthesize consistent reflections of closer content. Moreover, these techniques rely on large computationally-expensive neural networks to model outgoing radiance, which severely limits optimization and rendering speed. We address these issues with an approach based on ray tracing: instead of querying an expensive neural network for the outgoing view-dependent radiance at points along each camera ray, our model casts reflection rays from these points and traces them through the NeRF representation to render feature vectors which are decoded into color using a small inexpensive network. We demonstrate that our model outperforms prior methods for view synthesis of scenes containing shiny objects, and that it is the only existing NeRF method that can synthesize photorealistic specular appearance and reflections in real-world scenes, while requiring comparable optimization time to current state-of-the-art view synthesis models.},
  arxiv     = {2405.14871},
  _url       = {https://dorverbin.github.io/nerf-casting/},
}

@InProceedings{VoynoHAFC2024,
  author    = {Andrey Voynov and Amir Hertz and Moab Arar and Shlomi Fruchter and Daniel Cohen-Or},
  title     = {Curved Diffusion: A Generative Model With Optical Geometry Control},
  booktitle = ECCV,
  year      = {2024},
  abstract  = {State-of-the-art diffusion models can generate highly realistic images based on various conditioning like text, segmentation, and depth. However, an essential aspect often overlooked is the specific camera geometry used during image capture. The influence of different optical systems on the final scene appearance is frequently overlooked. This study introduces a framework that intimately integrates a text-to-image diffusion model with the particular lens geometry used in image rendering. Our method is based on a per-pixel coordinate conditioning method, enabling the control over the rendering geometry. Notably, we demonstrate the manipulation of curvature properties, achieving diverse visual effects, such as fish-eye, panoramic views, and spherical texturing using a single diffusion model.},
  arxiv     = {2311.17609},
  _url       = {https://anylens-diffusion.github.io/},
}

@Unpublished{WangA2024,
  author    = {Hengyi Wang and Lourdes Agapito},
  title     = {{3D} Reconstruction with Spatial Memory},
  note      = {\href{https://arxiv.org/abs/2408.16061}{arXiv:2408.16061}},
  year      = {2024},
  abstract  = {We present Spann3R, a novel approach for dense 3D reconstruction from ordered or unordered image collections. Built on the DUSt3R paradigm, Spann3R uses a transformer-based architecture to directly regress pointmaps from images without any prior knowledge of the scene or camera parameters. Unlike DUSt3R, which predicts per image-pair pointmaps each expressed in its local coordinate frame, Spann3R can predict per-image pointmaps expressed in a global coordinate system, thus eliminating the need for optimization-based global alignment. The key idea of Spann3R is to manage an external spatial memory that learns to keep track of all previous relevant 3D information. Spann3R then queries this spatial memory to predict the 3D structure of the next frame in a global coordinate system. Taking advantage of DUSt3R's pre-trained weights, and further fine-tuning on a subset of datasets, Spann3R shows competitive performance and generalization ability on various unseen datasets and can process ordered image collections in real time. Project page: \url https://hengyiwang.github.io/projects/spanner},
  arxiv     = {2408.16061},
  _url       = {https://hengyiwang.github.io/projects/spanner},
}

@InProceedings{WangLCCR2024,
  author    = {Shuzhe Wang and Vincent Leroy and Yohann Cabon and Boris Chidlovskii and Jerome Revaud},
  title     = {{DUSt3R}: Geometric {3D} Vision Made Easy},
  booktitle = CVPR,
  year      = {2024},
  abstract  = {Multi-view stereo reconstruction (MVS) in the wild requires to first estimate the camera parameters e.g. intrinsic and extrinsic parameters. These are usually tedious and cumbersome to obtain, yet they are mandatory to triangulate corresponding pixels in 3D space, which is the core of all best performing MVS algorithms. In this work, we take an opposite stance and introduce DUSt3R, a radically novel paradigm for Dense and Unconstrained Stereo 3D Reconstruction of arbitrary image collections, i.e. operating without prior information about camera calibration nor viewpoint poses. We cast the pairwise reconstruction problem as a regression of pointmaps, relaxing the hard constraints of usual projective camera models. We show that this formulation smoothly unifies the monocular and binocular reconstruction cases. In the case where more than two images are provided, we further propose a simple yet effective global alignment strategy that expresses all pairwise pointmaps in a common reference frame. We base our network architecture on standard Transformer encoders and decoders, allowing us to leverage powerful pretrained models. Our formulation directly provides a 3D model of the scene as well as depth information, but interestingly, we can seamlessly recover from it, pixel matches, relative and absolute camera. Exhaustive experiments on all these tasks showcase that the proposed DUSt3R can unify various 3D vision tasks and set new SoTAs on monocular/multi-view depth estimation as well as relative pose estimation. In summary, DUSt3R makes many geometric 3D vision tasks easy.},
  arxiv     = {2312.14132},
  _url       = {https://dust3r.europe.naverlabs.com/},
}

@Unpublished{WangXLCT2024,
  author    = {Shuaixian Wang and Haoran Xu and Yaokun Li and Jiwei Chen and Guang Tan},
  title     = {{IE-NeRF}: Inpainting Enhanced Neural Radiance Fields in the Wild},
  note      = {\href{https://arxiv.org/abs/2407.10695}{arXiv:2407.10695}},
  year      = {2024},
  abstract  = {We present a novel approach for synthesizing realistic novel views using Neural Radiance Fields (NeRF) with uncontrolled photos in the wild. While NeRF has shown impressive results in controlled settings, it struggles with transient objects commonly found in dynamic and time-varying scenes. Our framework called \textitInpainting Enhanced NeRF, or \ours, enhances the conventional NeRF by drawing inspiration from the technique of image inpainting. Specifically, our approach extends the Multi-Layer Perceptrons (MLP) of NeRF, enabling it to simultaneously generate intrinsic properties (static color, density) and extrinsic transient masks. We introduce an inpainting module that leverages the transient masks to effectively exclude occlusions, resulting in improved volume rendering quality. Additionally, we propose a new training strategy with frequency regularization to address the sparsity issue of low-frequency transient components. We evaluate our approach on internet photo collections of landmarks, demonstrating its ability to generate high-quality novel views and achieve state-of-the-art performance.},
  arxiv     = {2407.10695},
  _url       = {https://arxiv.org/abs/2407.10695},
}

@InProceedings{WangWZX2024,
  author    = {Yuxin Wang and Qianyi Wu and Guofeng Zhang and Dan Xu},
  title     = {{GScream}: Learning {3D} Geometry and Feature Consistent Gaussian Splatting for Object Removal},
  booktitle = ECCV,
  year      = {2024},
  abstract  = {This paper tackles the intricate challenge of object removal to update the radiance field using the 3D Gaussian Splatting. The main challenges of this task lie in the preservation of geometric consistency and the maintenance of texture coherence in the presence of the substantial discrete nature of Gaussian primitives. We introduce a robust framework specifically designed to overcome these obstacles. The key insight of our approach is the enhancement of information exchange among visible and invisible areas, facilitating content restoration in terms of both geometry and texture. Our methodology begins with optimizing the positioning of Gaussian primitives to improve geometric consistency across both removed and visible areas, guided by an online registration process informed by monocular depth estimation. Following this, we employ a novel feature propagation mechanism to bolster texture coherence, leveraging a cross-attention design that bridges sampling Gaussians from both uncertain and certain areas. This innovative approach significantly refines the texture coherence within the final radiance field. Extensive experiments validate that our method not only elevates the quality of novel view synthesis for scenes undergoing object removal but also showcases notable efficiency gains in training and rendering speeds.},
  arxiv     = {2404.13679},
  _url       = {https://w-ted.github.io/publications/gscream},
}

@InProceedings{WangLWBLSZ2023,
  author    = {Zhengyi Wang and Cheng Lu and Yikai Wang and Fan Bao and Chongxuan Li and Hang Su and Jun Zhu},
  title     = {{ProlificDreamer}: High-Fidelity and Diverse Text-to-{3D} Generation with Variational Score Distillation},
  booktitle = NeurIPS,
  year      = {2023},
  abstract  = {Score distillation sampling (SDS) has shown great promise in text-to-3D generation by distilling pretrained large-scale text-to-image diffusion models, but suffers from over-saturation, over-smoothing, and low-diversity problems. In this work, we propose to model the 3D parameter as a random variable instead of a constant as in SDS and present variational score distillation (VSD), a principled particle-based variational framework to explain and address the aforementioned issues in text-to-3D generation. We show that SDS is a special case of VSD and leads to poor samples with both small and large CFG weights. In comparison, VSD works well with various CFG weights as ancestral sampling from diffusion models and simultaneously improves the diversity and sample quality with a common CFG weight (i.e., $7.5$). We further present various improvements in the design space for text-to-3D such as distillation time schedule and density initialization, which are orthogonal to the distillation algorithm yet not well explored. Our overall approach, dubbed ProlificDreamer, can generate high rendering resolution (i.e., $512\times512$) and high-fidelity NeRF with rich structure and complex effects (e.g., smoke and drops). Further, initialized from NeRF, meshes fine-tuned by VSD are meticulously detailed and photo-realistic. Project page: https://ml.cs.tsinghua.edu.cn/prolificdreamer/},
  arxiv     = {2305.16213},
  _url       = {https://ml.cs.tsinghua.edu.cn/prolificdreamer/},
}

@InProceedings{WeberHJSSKK2024,
  author    = {Ethan Weber and Aleksander Hołyński and Varun Jampani and Saurabh Saxena and Noah Snavely and Abhishek Kar and Angjoo Kanazawa},
  title     = {{NeRFiller}: Completing Scenes via Generative {3D} Inpainting},
  booktitle = CVPR,
  year      = {2024},
  abstract  = {We propose NeRFiller, an approach that completes missing portions of a 3D capture via generative 3D inpainting using off-the-shelf 2D visual generative models. Often parts of a captured 3D scene or object are missing due to mesh reconstruction failures or a lack of observations (e.g., contact regions, such as the bottom of objects, or hard-to-reach areas). We approach this challenging 3D inpainting problem by leveraging a 2D inpainting diffusion model. We identify a surprising behavior of these models, where they generate more 3D consistent inpaints when images form a 2$\times$2 grid, and show how to generalize this behavior to more than four images. We then present an iterative framework to distill these inpainted regions into a single consistent 3D scene. In contrast to related works, we focus on completing scenes rather than deleting foreground objects, and our approach does not require tight 2D object masks or text. We compare our approach to relevant baselines adapted to our setting on a variety of scenes, where NeRFiller creates the most 3D consistent and plausible scene completions. Our project page is at https://ethanweber.me/nerfiller.},
  arxiv     = {2312.04560},
  _url       = {https://ethanweber.me/nerfiller},
}

@Article{YangLFLXZST2024,
  author    = {Chen Yang and Sikuang Li and Jiemin Fang and Ruofan Liang and Lingxi Xie and Xiaopeng Zhang and Wei Shen and Qi Tian},
  title     = {{GaussianObject}: Just Taking Four Images to Get A High-Quality {3D} Object with {Gaussian} Splatting},
  journal   = ProcSIGASIA,
  year      = {2024},
  note      = {\href{https://arxiv.org/abs/2402.10259}{arXiv:2402.10259}},
  abstract  = {Reconstructing and rendering 3D objects from highly sparse views is of critical importance for promoting applications of 3D vision techniques and improving user experience. However, images from sparse views only contain very limited 3D information, leading to two significant challenges: 1) Difficulty in building multi-view consistency as images for matching are too few; 2) Partially omitted or highly compressed object information as view coverage is insufficient. To tackle these challenges, we propose GaussianObject, a framework to represent and render the 3D object with Gaussian splatting, that achieves high rendering quality with only 4 input images. We first introduce techniques of visual hull and floater elimination which explicitly inject structure priors into the initial optimization process for helping build multi-view consistency, yielding a coarse 3D Gaussian representation. Then we construct a Gaussian repair model based on diffusion models to supplement the omitted object information, where Gaussians are further refined. We design a self-generating strategy to obtain image pairs for training the repair model. Our GaussianObject is evaluated on several challenging datasets, including MipNeRF360, OmniObject3D, and OpenIllumination, achieving strong reconstruction results from only 4 views and significantly outperforming previous state-of-the-art methods.},
  arxiv     = {2402.10259},
  _url       = {https://gaussianobject.github.io/},
}

@Unpublished{YeLXLPYP2024,
  author    = {Botao Ye and Sifei Liu and Haofei Xu and Xueting Li and Marc Pollefeys and Ming-Hsuan Yang and Songyou Peng},
  title     = {No Pose, No Problem: Surprisingly Simple {3D} Gaussian Splats from Sparse Unposed Images},
  note      = {\href{https://arxiv.org/abs/2410.24207}{arXiv:2410.24207}},
  year      = {2024},
  abstract  = {We introduce NoPoSplat, a feed-forward model capable of reconstructing 3D scenes parameterized by 3D Gaussians from \textitunposed sparse multi-view images. Our model, trained exclusively with photometric loss, achieves real-time 3D Gaussian reconstruction during inference. To eliminate the need for accurate pose input during reconstruction, we anchor one input view's local camera coordinates as the canonical space and train the network to predict Gaussian primitives for all views within this space. This approach obviates the need to transform Gaussian primitives from local coordinates into a global coordinate system, thus avoiding errors associated with per-frame Gaussians and pose estimation. To resolve scale ambiguity, we design and compare various intrinsic embedding methods, ultimately opting to convert camera intrinsics into a token embedding and concatenate it with image tokens as input to the model, enabling accurate scene scale prediction. We utilize the reconstructed 3D Gaussians for novel view synthesis and pose estimation tasks and propose a two-stage coarse-to-fine pipeline for accurate pose estimation. Experimental results demonstrate that our pose-free approach can achieve superior novel view synthesis quality compared to pose-required methods, particularly in scenarios with limited input image overlap. For pose estimation, our method, trained without ground truth depth or explicit matching loss, significantly outperforms the state-of-the-art methods with substantial improvements. This work makes significant advances in pose-free generalizable 3D reconstruction and demonstrates its applicability to real-world scenarios. Code and trained models are available at https://noposplat.github.io/.},
  arxiv     = {2410.24207},
  _url       = {https://noposplat.github.io/},
}

@Unpublished{YeLKTYPSYHTK2024,
  author    = {Vickie Ye and Ruilong Li and Justin Kerr and Matias Turkulainen and Brent Yi and Zhuoyang Pan and Otto Seiskari and Jianbo Ye and Jeffrey Hu and Matthew Tancik and Angjoo Kanazawa},
  title     = {gsplat: An Open-Source Library for {Gaussian} Splatting},
  note      = {\href{https://arxiv.org/abs/2409.06765}{arXiv:2409.06765}},
  year      = {2024},
  abstract  = {gsplat is an open-source library designed for training and developing Gaussian Splatting methods. It features a front-end with Python bindings compatible with the PyTorch library and a back-end with highly optimized CUDA kernels. gsplat offers numerous features that enhance the optimization of Gaussian Splatting models, which include optimization improvements for speed, memory, and convergence times. Experimental results demonstrate that gsplat achieves up to 10% less training time and 4x less memory than the original implementation. Utilized in several research projects, gsplat is actively maintained on GitHub. Source code is available at https://github.com/nerfstudio-project/gsplatunder Apache License 2.0. We welcome contributions from the open-source community.},
  arxiv     = {2409.06765},
  _url       = {https://github.com/nerfstudio-project/gsplat},
}

@InProceedings{YeshwLND2023,
  author    = {Chandan Yeshwanth and Yueh-Cheng Liu and Matthias Nießner and Angela Dai},
  title     = {{ScanNet}++: A High-Fidelity Dataset of {3D} Indoor Scenes},
  booktitle = ICCV,
  year      = {2023},
  abstract  = {We present ScanNet++, a large-scale dataset that couples together capture of high-quality and commodity-level geometry and color of indoor scenes. Each scene is captured with a high-end laser scanner at sub-millimeter resolution, along with registered 33-megapixel images from a DSLR camera, and RGB-D streams from an iPhone. Scene reconstructions are further annotated with an open vocabulary of semantics, with label-ambiguous scenarios explicitly annotated for comprehensive semantic understanding. ScanNet++ enables a new real-world benchmark for novel view synthesis, both from high-quality RGB capture, and importantly also from commodity-level images, in addition to a new benchmark for 3D semantic scene understanding that comprehensively encapsulates diverse and ambiguous semantic labeling scenarios. Currently, ScanNet++ contains 460 scenes, 280,000 captured DSLR images, and over 3.7M iPhone RGBD frames.},
  arxiv     = {2308.11417},
  _url       = {https://cy94.github.io/scannetpp/},
}

@Unpublished{YuKJJHSX2024,
  author    = {Sihyun Yu and Sangkyung Kwak and Huiwon Jang and Jongheon Jeong and Jonathan Huang and Jinwoo Shin and Saining Xie},
  title     = {Representation Alignment for Generation: Training Diffusion Transformers Is Easier Than You Think},
  note      = {\href{https://arxiv.org/abs/2410.06940}{arXiv:2410.06940}},
  year      = {2024},
  abstract  = {Recent studies have shown that the denoising process in (generative) diffusion models can induce meaningful (discriminative) representations inside the model, though the quality of these representations still lags behind those learned through recent self-supervised learning methods. We argue that one main bottleneck in training large-scale diffusion models for generation lies in effectively learning these representations. Moreover, training can be made easier by incorporating high-quality external visual representations, rather than relying solely on the diffusion models to learn them independently. We study this by introducing a straightforward regularization called REPresentation Alignment (REPA), which aligns the projections of noisy input hidden states in denoising networks with clean image representations obtained from external, pretrained visual encoders. The results are striking: our simple strategy yields significant improvements in both training efficiency and generation quality when applied to popular diffusion and flow-based transformers, such as DiTs and SiTs. For instance, our method can speed up SiT training by over 17.5$\times$, matching the performance (without classifier-free guidance) of a SiT-XL model trained for 7M steps in less than 400K steps. In terms of final generation quality, our approach achieves state-of-the-art results of FID=1.42 using classifier-free guidance with the guidance interval.},
  arxiv     = {2410.06940},
  _url       = {https://sihyun.me/REPA},
}

@Unpublished{YuXYHLHGWST2024,
  author    = {Wangbo Yu and Jinbo Xing and Li Yuan and Wenbo Hu and Xiaoyu Li and Zhipeng Huang and Xiangjun Gao and Tien-Tsin Wong and Ying Shan and Yonghong Tian},
  title     = {{ViewCrafter}: Taming Video Diffusion Models for High-fidelity Novel View Synthesis},
  note      = {\href{https://arxiv.org/abs/2409.02048}{arXiv:2409.02048}},
  year      = {2024},
  abstract  = {Despite recent advancements in neural 3D reconstruction, the dependence on dense multi-view captures restricts their broader applicability. In this work, we propose \textbfViewCrafter, a novel method for synthesizing high-fidelity novel views of generic scenes from single or sparse images with the prior of video diffusion model. Our method takes advantage of the powerful generation capabilities of video diffusion model and the coarse 3D clues offered by point-based representation to generate high-quality video frames with precise camera pose control. To further enlarge the generation range of novel views, we tailored an iterative view synthesis strategy together with a camera trajectory planning algorithm to progressively extend the 3D clues and the areas covered by the novel views. With ViewCrafter, we can facilitate various applications, such as immersive experiences with real-time rendering by efficiently optimizing a 3D-GS representation using the reconstructed 3D points and the generated novel views, and scene-level text-to-3D generation for more imaginative content creation. Extensive experiments on diverse datasets demonstrate the strong generalization capability and superior performance of our method in synthesizing high-fidelity and consistent novel views.},
  arxiv     = {2409.02048},
  _url       = {https://drexubery.github.io/ViewCrafter/},
}

@InProceedings{YuXZLYWYZXLCCH2023,
  author    = {Xianggang Yu and Mutian Xu and Yidan Zhang and Haolin Liu and Chongjie Ye and Yushuang Wu and Zizheng Yan and Chenming Zhu and Zhangyang Xiong and Tianyou Liang and Guanying Chen and Shuguang Cui and Xiaoguang Han},
  title     = {{MVImgNet}: A Large-scale Dataset of Multi-view Images},
  booktitle = CVPR,
  year      = {2023},
  abstract  = {Being data-driven is one of the most iconic properties of deep learning algorithms. The birth of ImageNet drives a remarkable trend of "learning from large-scale data" in computer vision. Pretraining on ImageNet to obtain rich universal representations has been manifested to benefit various 2D visual tasks, and becomes a standard in 2D vision. However, due to the laborious collection of real-world 3D data, there is yet no generic dataset serving as a counterpart of ImageNet in 3D vision, thus how such a dataset can impact the 3D community is unraveled. To remedy this defect, we introduce MVImgNet, a large-scale dataset of multi-view images, which is highly convenient to gain by shooting videos of real-world objects in human daily life. It contains 6.5 million frames from 219,188 videos crossing objects from 238 classes, with rich annotations of object masks, camera parameters, and point clouds. The multi-view attribute endows our dataset with 3D-aware signals, making it a soft bridge between 2D and 3D vision.

We conduct pilot studies for probing the potential of MVImgNet on a variety of 3D and 2D visual tasks, including radiance field reconstruction, multi-view stereo, and view-consistent image understanding, where MVImgNet demonstrates promising performance, remaining lots of possibilities for future explorations.

Besides, via dense reconstruction on MVImgNet, a 3D object point cloud dataset is derived, called MVPNet, covering 87,200 samples from 150 categories, with the class label on each point cloud. Experiments show that MVPNet can benefit the real-world 3D object classification while posing new challenges to point cloud understanding.

MVImgNet and MVPNet will be publicly available, hoping to inspire the broader vision community.},
  arxiv     = {2303.06042},
  _url       = {https://gaplab.cuhk.edu.cn/projects/MVImgNet/},
}

@InProceedings{YueDETL2024,
  author    = {Yuanwen Yue and Anurag Das and Francis Engelmann and Siyu Tang and Jan Eric Lenssen},
  title     = {Improving {2D} Feature Representations by {3D}-Aware Fine-Tuning},
  booktitle = ECCV,
  year      = {2024},
  abstract  = {Current visual foundation models are trained purely on unstructured 2D data, limiting their understanding of 3D structure of objects and scenes. In this work, we show that fine-tuning on 3D-aware data improves the quality of emerging semantic features. We design a method to lift semantic 2D features into an efficient 3D Gaussian representation, which allows us to re-render them for arbitrary views. Using the rendered 3D-aware features, we design a fine-tuning strategy to transfer such 3D awareness into a 2D foundation model. We demonstrate that models fine-tuned in that way produce features that readily improve downstream task performance in semantic segmentation and depth estimation through simple linear probing. Notably, though fined-tuned on a single indoor dataset, the improvement is transferable to a variety of indoor datasets and out-of-domain datasets. We hope our study encourages the community to consider injecting 3D awareness when training 2D foundation models. Project page: https://ywyue.github.io/FiT3D.},
  arxiv     = {2407.20229},
  _url       = {https://ywyue.github.io/FiT3D},
}

@Unpublished{ZhangHHJDCSY2024,
  author    = {Junyi Zhang and Charles Herrmann and Junhwa Hur and Varun Jampani and Trevor Darrell and Forrester Cole and Deqing Sun and Ming-Hsuan Yang},
  title     = {{MonST3R}: A Simple Approach for Estimating Geometry in the Presence of Motion},
  note      = {\href{https://arxiv.org/abs/2410.03825}{arXiv:2410.03825}},
  year      = {2024},
  abstract  = {Estimating geometry from dynamic scenes, where objects move and deform over time, remains a core challenge in computer vision. Current approaches often rely on multi-stage pipelines or global optimizations that decompose the problem into subtasks, like depth and flow, leading to complex systems prone to errors. In this paper, we present Motion DUSt3R (MonST3R), a novel geometry-first approach that directly estimates per-timestep geometry from dynamic scenes. Our key insight is that by simply estimating a pointmap for each timestep, we can effectively adapt DUST3R's representation, previously only used for static scenes, to dynamic scenes. However, this approach presents a significant challenge: the scarcity of suitable training data, namely dynamic, posed videos with depth labels. Despite this, we show that by posing the problem as a fine-tuning task, identifying several suitable datasets, and strategically training the model on this limited data, we can surprisingly enable the model to handle dynamics, even without an explicit motion representation. Based on this, we introduce new optimizations for several downstream video-specific tasks and demonstrate strong performance on video depth and camera pose estimation, outperforming prior work in terms of robustness and efficiency. Moreover, MonST3R shows promising results for primarily feed-forward 4D reconstruction.},
  arxiv     = {2410.03825},
  _url       = {https://monst3r-project.github.io/},
}

@Article{ZhangLWWL2024,
  author    = {Jingbo Zhang and Xiaoyu Li and Ziyu Wan and Can Wang and Jing Liao},
  title     = {{Text2NeRF}: Text-Driven {3D} Scene Generation with Neural Radiance Fields},
  journal   = TVCG,
  year      = {2024},
  volume    = {30},
  number    = {12},
  pages     = {7749--7762},
  _month     = dec,
  _issn      = {1941-0506},
  abstract  = {Text-driven 3D scene generation is widely applicable to video gaming, film industry, and metaverse applications that have a large demand for 3D scenes. However, existing text-to-3D generation methods are limited to producing 3D objects with simple geometries and dreamlike styles that lack realism. In this work, we present Text2NeRF, which is able to generate a wide range of 3D scenes with complicated geometric structures and high-fidelity textures purely from a text prompt. To this end, we adopt NeRF as the 3D representation and leverage a pre-trained text-to-image diffusion model to constrain the 3D reconstruction of the NeRF to reflect the scene description. Specifically, we employ the diffusion model to infer the text-related image as the content prior and use a monocular depth estimation method to offer the geometric prior. Both content and geometric priors are utilized to update the NeRF model. To guarantee textured and geometric consistency between different views, we introduce a progressive scene inpainting and updating strategy for novel view synthesis of the scene. Our method requires no additional training data but only a natural language description of the scene as the input. Extensive experiments demonstrate that our Text2NeRF outperforms existing methods in producing photo-realistic, multi-view consistent, and diverse 3D scenes from a variety of natural language prompts. Our code and model are available at https://github.com/eckertzhang/Text2NeRF.},
  arxiv     = {2305.11588},
  doi       = {10.1109/TVCG.2024.3361502},
  _url       = {https://eckertzhang.github.io/Text2NeRF.github.io/},
}

@InProceedings{ZhaoT2024,
  author    = {Qitao Zhao and Shubham Tulsiani},
  title     = {Sparse-view Pose Estimation and Reconstruction via Analysis by Generative Synthesis},
  booktitle = NeurIPS,
  year      = {2024},
  abstract  = {Inferring the 3D structure underlying a set of multi-view images requires solving two co-dependent tasks – accurate 3D reconstruction requires precise camera poses, and predicting camera poses relies on (implicitly or explicitly) modeling the underlying 3D. The classical framework of analysis by synthesis casts this inference as a joint optimization seeking to explain the observed pixels, and recent instantiations typically learn expressive 3D representations (e.g., Neural Fields) with gradient-descent-based pose refinement of off-the-shelf pose estimates. However, given a sparse set of observed views, the observations may not provide sufficient direct evidence to obtain complete and accurate 3D. Moreover, large errors in pose estimation may not be easily corrected and can further degrade the inferred 3D. To allow robust 3D reconstruction and pose estimation in this challenging setup, we propose a method that adapts this analysis-by-synthesis approach by: a) including novel-view-synthesis-based generative priors in conjunction with photometric objectives to improve the quality of the inferred 3D, and b) explicit reasoning about outliers and a discrete search and continuous optimization-based strategy to correct them. We validate our framework across real-world and synthetic datasets in conjunction with several off-the-shelf pose estimation systems as initialization. We find that it significantly improves the base systems’ pose accuracy while yielding high-quality 3D reconstructions that outperform the results from current unposed multi-view reconstruction baselines.},
}

@InProceedings{ZhuFJW2024,
  author    = {Zehao Zhu and Zhiwen Fan and Yifan Jiang and Zhangyang Wang},
  title     = {{FSGS}: Real-Time Few-shot View Synthesis using {Gaussian} Splatting},
  booktitle = ECCV,
  year      = {2024},
  abstract  = {Novel view synthesis from limited observations remains an important and persistent task. However, high efficiency in existing NeRF-based few-shot view synthesis is often compromised to obtain an accurate 3D representation. To address this challenge, we propose a few-shot view synthesis framework based on 3D Gaussian Splatting that enables real-time and photo-realistic view synthesis with as few as three training views. The proposed method, dubbed FSGS, handles the extremely sparse initialized SfM points with a thoughtfully designed Gaussian Unpooling process. Our method iteratively distributes new Gaussians around the most representative locations, subsequently infilling local details in vacant areas. We also integrate a large-scale pre-trained monocular depth estimator within the Gaussians optimization process, leveraging online augmented views to guide the geometric optimization towards an optimal solution. Starting from sparse points observed from limited input viewpoints, our FSGS can accurately grow into unseen regions, comprehensively covering the scene and boosting the rendering quality of novel views. Overall, FSGS achieves state-of-the-art performance in both accuracy and rendering efficiency across diverse datasets, including LLFF, Mip-NeRF360, and Blender. Project website: https://zehaozhu.github.io/FSGS/.},
  arxiv     = {2312.00451},
  _url       = {https://zehaozhu.github.io/FSGS/},
}

@Unpublished{ZiwenTZBLHFX2024,
  author    = {Chen Ziwen and Hao Tan and Kai Zhang and Sai Bi and Fujun Luan and Yicong Hong and Li Fuxin and Zexiang Xu},
  title     = {Long-{LRM}: Long-sequence Large Reconstruction Model for Wide-coverage {Gaussian} Splats},
  note      = {\href{https://arxiv.org/abs/2410.12781}{arXiv:2410.12781}},
  year      = {2024},
  abstract  = {We propose Long-LRM, a generalizable 3D Gaussian reconstruction model that is capable of reconstructing a large scene from a long sequence of input images. Specifically, our model can process 32 source images at 960x540 resolution within only 1.3 seconds on a single A100 80G GPU. Our architecture features a mixture of the recent Mamba2 blocks and the classical transformer blocks which allowed many more tokens to be processed than prior work, enhanced by efficient token merging and Gaussian pruning steps that balance between quality and efficiency. Unlike previous feed-forward models that are limited to processing 1~4 input images and can only reconstruct a small portion of a large scene, Long-LRM reconstructs the entire scene in a single feed-forward step. On large-scale scene datasets such as DL3DV-140 and Tanks and Temples, our method achieves performance comparable to optimization-based approaches while being two orders of magnitude more efficient. Project page: https://arthurhero.github.io/projects/llrm},
  arxiv     = {2410.12781},
  _url       = {https://arthurhero.github.io/projects/llrm},
}

@Comment{jabref-meta: databaseType:bibtex;}

@Comment{jabref-meta: saveOrderConfig:specified;author;false;editor;false;year;true;}
