
\section{Introduction}



%As robotics advances, the potential for robots to assist people in various domains, such as manufacturing \cite{sauppe_social_2015, terzioglu_designing_2020}, domestic assistance \cite{babel_step_2022, schneiders_domestic_2021, chatterjee_usage_2024}, healthcare \cite{holland_service_2021, ahn_healthcare_2015}, and education \cite{belpaeme_social_2018, turan_effect_2020} is becoming increasingly evident.

As robotics advances, the potential for robots to assist people in various domains, such as manufacturing \cite{sauppe_social_2015, terzioglu_designing_2020}, domestic assistance \cite{babel_step_2022, schneiders_domestic_2021, chatterjee_usage_2024} is becoming increasingly evident.  One significant application of robotics is in human-robot teaming, which involves collaboration between humans and robotic systems working together to perform joint activities \cite{mingyue_ma_human-robot_2018}. In human-robot teaming, robots must behave and communicate effectively to maintain alignment within the team \cite{chakraborti_ai_2017}.
%Robots have demonstrated the ability to perform a wide range of tasks, such as assembly \cite{wang_see_2020, su_effects_2023, bernardo_interactive_2016}.
 However, as robots become more integrated into our daily lives, ensuring the reliability of these systems is a pressing concern \cite{honig_understanding_2018, desai_effects_2012}. Robot errors are inevitable, much like human errors, due to the inherent uncertainty of the world and the need to make decisions and act in real-time.
If these failures are not managed appropriately, they can negatively impact task success, human safety, trust, and perceptions of the robot’s intelligence \cite{ schaefer_meta-analysis_2016, salem_would_2015, sebo_i_2019, lei_should_2021}. 
These factors are crucial because the degree to which people trust robots influences their willingness to collaborate with them, which is essential for establishing effective human-robot teams \cite{breazeal_social_2016, rossi_matter_2023, huang_anticipatory_2016}. 
However, trust can fluctuate over time; it tends to increase when robots perform well but might drop rapidly when they inevitably make errors. 
In addition, the type of failure (i.e. at the motion execution or task planning level), might significantly affect trust, and robots that demonstrate awareness of their errors show potential in restoring trust, as the saying goes, ‘a fault confessed is half forgiven.’
By focusing on moments when human interactions deviate from expected patterns, strategies can be identified to make these interactions more robust. 


\begin{figure}[t]
  \centering
  \includegraphics[width=\linewidth]{Photos/AoIs.pdf} 
  \caption{Different areas of interest in the experiment.
}
  \label{fig:Robot}
\end{figure}



One strategy to enhance human-robot interaction is modelling the user's reactions to robot failures. This user model can be inferred from various signals \cite{ rossi_user_2017}, such as the user’s social cues during the period of the failure \cite{rabinowitz_machine_2018}. One of those social and non-verbal cues is a person’s eye gaze \cite{wachowiak_analysing_2022}, which plays a crucial role in conveying attention \cite{velichkovsky_social_2021, fang_dual_2021}, intentions \cite{velichkovsky_social_2021, fang_dual_2021, rubies_enhancing_2022}, and emotional states \cite{velichkovsky_social_2021, huang_using_2015}. Eye gaze has been leveraged in human-robot interactions to enhance the robot's ability to comprehend and to anticipate human actions \cite{fang_dual_2021, mwangi_dyadic_2018}. Research has also shown that people exhibit consistent gaze patterns while performing specific activities, \cite{johansson_eye-hand_2001, land_what_2001, matthis_gaze_2018}, making it possible to model these patterns. However, there is a lack of studies on accurate gaze patterns in response to robot failures, which could potentially aid robots in recovering from their failures.

  

This study examines the impact of robot failures on users' perception and gaze behaviour. Utilising a within-between experimental design, we analysed the effects of failure types, failure timing, and failure acknowledgement in a sample of 27 participants. Participants engaged in a collaborative task involving four Tangram puzzles, during which the robot was programmed to fail. Each participant experienced all combinations of failure type and timing (within-subjects), with one group exposed to the robot acknowledging its failures and the other group receiving no acknowledgement (between-subjects). Our results indicate that users' gaze patterns during failure events differ significantly from those observed during times of no failure. Furthermore, these gaze behaviours are highly dependent on the type of failure.



%Yet, we are unaware of studies on the role of gaze, how it user's gaze behavior change in response to these failures and its potential as an indicator of failure. 



%robots with fixed behaviours struggle to adapt to the dynamic and unpredictable nature of human interactions \cite{aronson_gaze_2018}. 

%This limitation often causes robots to fail in meeting user expectations, making it challenging to integrate robots into daily routines and achieve seamless real-time collaboration \cite{breazeal_social_2016, rossi_matter_2023, huang_anticipatory_2016}. 


%By focusing on moments when human interactions deviate from expected patterns, key opportunities can be identified to make these interactions more robust. 
%This, in turn, enables robots to reliably adapt to human behaviour and improve human-robot collaboration \cite{inceoglu_fino-net_2021}.




%However, trust is not a constant; it increases when robots perform well and can rapidly diminish when robots inevitably make mistakes [9]. It also highly depended on the type of failure and 

%It is necessary to investigate how trust violations and trust repairs influence perceptions of trust and people’s willingness to continue working with an imperfect robotic teammate so that mitigating strategies may be developed.


%their negative efects on task success, safety, and human trust and willingness to work together will snowball and require more time and efort to recover from [16, 24, 34].

%The degree to which people trust robots impacts their willingness to work with them and is key to establishing collaborative human-robot teams. 


%Robot errors are inescapable and perennial in complex human-robot collaboration; these errors are mostly unexpected and come from a multitude of sources ranging from mechanical malfunctions (e.g., failure to close the gripper when attempting to grasp) to uncertain perceptions and reasoning (e.g., errors in intent recognition) to shifts in the environment (e.g., new physical constraints emerge). If not managed appropriately, their negative efects on task success, safety, and human trust and willingness to work together will snowball and require more time and efort to recover from [16, 24, 34].

%[robots make failure]


%The degree to which people trust robots impacts their willingness to work with them and is key to establishing collaborative human-robot teams [9]. However, trust is not a constant; it increases when robots perform well and can rapidly diminish when robots inevitably make mistakes [9]. It is necessary to investigate how trust violations and trust repairs influence perceptions of trust and people’s willingness to continue working with an imperfect robotic teammate so that mitigating strategies may be developed.

%A person’s level of trust of an automated system is a key factor that influences their use of that system

%Failures influence willingness to interact with a robot [2]. Despite ongoing technological advancements, failures are difficult to eliminate \cite{tsarouhas_mission_2016, honig_expect_2021}. Human–robot interaction (HRI) studies note types of robotic failures and varying severity levels as impactful [5–7].

%Yet, we are unaware of studies on the role of PeR in how robotic failures are perceived and experienced.

%after timing and sayig different aspects of failure
%Yet, we are unaware of studies on the role of gaze, how it user's gaze behavior change in response to these failures and its potential as an indicator of failure. 


% Increasing robots’ awareness of their errors – which we define as actions occurring “not as planned” [1] – is a promising way to improve human-robot interactions [2, 3].


 %Assistive robots, in the context of human-robot collaboration, are starting to evolve from autonomous tools into collaborative partners, augmenting human capabilities [8]. For these robots to be productive teammates, a relationship built on shared trust is imperative [9]. However, human collaborator trust can be easily damaged by unexpected robot errors. In addition, robot errors negatively impact safety and task performance. As a result, people will be less willing to work with robots [5, 23].


%In the evolving landscape of robotics, the potential of robots to augment human capabilities across a diverse range of applications—from assistive technologies \cite{belpaeme_social_2018} and collaborative manufacturing \cite{sauppe_social_2015} to domestic robots \cite{stiefelhagen_natural_2004} —is increasingly recognised. robots have been very successful at performing specific tasks, such as assembly tasks \textcolor{red}{\textbf{[CITE MORE]}} \cite{su_effects_2023}, and have also been effective in reducing social isolation in non-industrial settings \textcolor{red}{\textbf{[CITE]}}. As robots become more integrated into our daily lives, ensuring the reliability of these systems is a pressing concern \cite{honig_understanding_2018} \textcolor{red}{\textbf{[CITE MORE]}}. Traditional models with fixed behaviors struggle to adapt to the dynamic and unpredictable nature of human interactions \cite{aronson_gaze_2018}. This often causes social robots to fail in meeting user expectations, making it challenging to integrate robots into daily routines and achieve seamless real-time collaboration \cite{breazeal_social_2016, rossi_matter_2023, huang_anticipatory_2016}. By focusing on moments when human interactions deviate from expected patterns, we can identify key opportunities to enhance human-robot interaction (HRI) \textcolor{red}{\textbf{[CITE]}}. Addressing these deviations is essential for developing robots that can reliably adapt to human behavior and create stronger partnerships with humans \cite{inceoglu_fino-net_2021}. As human-robot interactions become more commonplace, the impact of these system failures on the trust between humans and their robotic counterparts cannot be underestimated \cite{morales_interaction_2019, rossi_matter_2023, schaefer_meta-analysis_2016, salem_would_2015}. 



 
  %Due to technical constraints and the diverse nature of users, current HRI systems still experience frequent errors. System failures can negatively impact the trust between humans and their robotic counterparts \cite{ schaefer_meta-analysis_2016, salem_would_2015, sebo_i_2019}. Therefore, an autonomous robot should be capable of recognizing its mistakes and quickly recovering from them. 

  

%An autonomous robot should be capable of recognizing its mistakes and quickly recovering from them \textcolor{red}{\textbf{[Recovering from failure]}}. This ability can be developed by modeling human reactions to failures. The user model can be inferred from various signals, such as the user’s social cues during the period of the failure \cite{rabinowitz_machine_2018, rossi_user_2017}. One of those social and non-verbal cues is a person’s eye gaze \cite{wachowiak_analysing_2022}, which plays a crucial role in conveying focus \cite{velichkovsky_social_2021}, intentions \cite{fang_dual_2021, rubies_enhancing_2022}, and emotional states \cite{velichkovsky_social_2021, huang_using_2015}. The communicative value of eye gaze in HRI has been acknowledged, underscoring its potential to improve the understanding and predictability of human actions \cite{fang_dual_2021, mwangi_dyadic_2018}. People exhibit consistent gaze patterns while performing particular activities such as walking \cite{matthis_gaze_2018}, manipulating objects \cite{johansson_eye-hand_2001, land_what_2001}, or controlling robots \cite{aronson_eye-hand_2018}. 




%This paper aims to provide insights into how robot failures affect users’ perception and gaze behavior during the failure. Our hypothesis is that people’s responses to failures vary based on the type and timing of the failure. Additionally, if the robot shows awareness of its own failure, it will influence the person’s perception and their gaze pattern in subsequent failures. We applied these analyses in a mixed-methods study design (failure type × failure timing × acknowledgment of failure), involving 27 participants. Participants were instructed to collaboratively solve four Tangram puzzles, which is a common dissection puzzle. The robot was programmed to fail during each puzzle, incorporating all combinations of failure type and timing for each participant. Half of the participants were exposed to the robot acknowledging its failures in all puzzles, while the other half were not exposed to any acknowledgment.


