
\documentclass{article}

\pdfoutput=1

\usepackage{microtype}
\usepackage{graphicx}
\usepackage{booktabs} %

\usepackage{hyperref}

\newcommand{\theHalgorithm}{\arabic{algorithm}}

\usepackage[accepted]{icml2025}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

\usepackage[capitalize,noabbrev]{cleveref}

\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

\usepackage[textsize=tiny]{todonotes}

\newcommand{\td}[1]{\todo{#1}}%

\usepackage{caption}
\usepackage{subcaption}
\graphicspath{{./fig/}}

\usepackage[inline]{enumitem}

\newcommand{\cites}[1]{\citeauthor{#1}'s~\citeyear{#1}}%
\setcitestyle{square}

\icmltitlerunning{%
All You Need for Counterfactual Explainability %
Is Principled and Reliable Estimate of Aleatoric and Epistemic Uncertainty%
}

\begin{document}

\newlength{\oldtextfloatsep}\setlength{\oldtextfloatsep}{\textfloatsep}

\twocolumn[
\icmltitle{%
All You Need for Counterfactual Explainability \\%
Is Principled and Reliable Estimate of Aleatoric and Epistemic Uncertainty%
}

\begin{icmlauthorlist}
\icmlauthor{Kacper Sokol}{eth,lmu}%
\icmlauthor{Eyke H{\"u}llermeier}{lmu}%
\end{icmlauthorlist}

\icmlaffiliation{eth}{Department of Computer Science, ETH Zurich, Switzerland}
\icmlaffiliation{lmu}{Institute of Informatics, LMU Munich, Germany}

\icmlcorrespondingauthor{Kacper Sokol}{kacper.sokol@inf.ethz.ch}

\icmlkeywords{Explainability, Interpretability, Counterfactuals, Machine Learning}

\vskip 0.3in
]

\printAffiliationsAndNotice{}

\begin{abstract}
This position paper argues that, to its detriment, transparency research overlooks many foundational concepts of artificial intelligence. %
Here, we focus on uncertainty quantification -- in the context of ante-hoc interpretability and counterfactual explainability -- showing how its adoption could address key challenges in the field. %
First, we posit that uncertainty and ante-hoc interpretability offer complementary views of the same underlying idea; %
second, we assert that uncertainty provides a principled unifying framework for counterfactual explainability. %
Consequently, inherently transparent models can benefit from human-centred explanatory insights -- like counterfactuals -- which are otherwise missing. %
At a higher level, integrating artificial intelligence fundamentals into transparency research promises to yield more reliable, robust and understandable predictive models. %
\end{abstract}




\section{Uncertainty and Transparency}\label{sec:intro}%

Artificial intelligence (AI) models can achieve impressive results across many domains, %
but their deployment is often stymied by their opaqueness, unreliability and lack of robustness~\citep{rudin2019stop}. %
Consequently, two paradigms have emerged to alleviate such issues: %
\emph{ante-hoc} interpretability envisages building inherently transparent models whose functioning adheres to domain-specific constraints, and %
\emph{post-hoc} explainability offers tools that elucidate the operation of predictive models through independent explanatory mechanisms~\citep{sokol2020explainability}. %
Additionally, uncertainty quantification has been proposed to improve the accountability of AI by looking beyond predictive performance; %
it aims to provide truthful representation of models' \emph{aleatoric} -- inherent to data generating processes -- and \emph{epistemic} -- due to model non-uniqueness -- uncertainty~\citep{hullermeier2021aleatoric}. %

But methods for reliably estimating uncertainty %
remain fairly underdeveloped, with such considerations often being neglected. %
Moreover, while post-hoc approaches are at the forefront of \emph{human-centred} explainable AI (XAI) %
-- enabling diverse audiences, both with and without technical expertise, to peer inside predictive models~\citep{miller2019explanation} -- %
these techniques are usually unable to faithfully capture the operation of AI systems, offering misleading insights~\citep{rudin2019stop}. %
As a result, ante-hoc interpretable models are preferred in (high stakes) real-world applications %
even though %
their functioning remains largely opaque to non-technical stakeholders, who nonetheless tend to be their primary users~\citep{sokol2023reasonable}. %


In this position paper \textbf{we argue that %
AI transparency techniques are largely oblivious to various notions of uncertainty %
despite %
the two being fundamentally interconnected}. %
More broadly, \textbf{we assert that XAI research tends to neglect the rich tapestry of foundational AI concepts}, %
which can lead to reinventing what already exists; %
admittedly, this sentiment is more true for post-hoc than ante-hoc approaches since the latter draw upon decades of work on classic AI models~\citep{rudin2022interpretable}. %
We %
support these arguments by %
demonstrating that connecting AI transparency and uncertainty quantification can address many open challenges in XAI. %
Specifically, we use %
the example of \emph{counterfactual explanations} %
given that they are considered the gold standard of human-centred XAI~\citep{miller2019explanation}. %

To this end, we first review relevant topics in Section~\ref{sec:related}: %
(\S\ref{sec:related:exp})~counterfactual explainability, (\S\ref{sec:related:uq})~uncertainty quantification and (\S\ref{sec:related:exp+uq})~their intersection. %
Next, in Section~\ref{sec:methodology}, we present our arguments. %
We begin by (\S\ref{sec:methodology:challenges})~summarising open challenges: %
misguided or overlooked modelling assumptions; %
ad-hoc fixes and unreliable proxies used in lieu of principled uncertainty handling; %
overemphasis on crisp classification and inadequate XAI evaluation practice; %
dominance of neural models applied to unstructured data leading to neglect of inherently transparent AI; and %
largely missing human-centred perspective in ante-hoc interpretability. %

Building upon these observations, \textbf{we posit that: %
(\S\ref{sec:methodology:ante})~uncertainty quantification and ante-hoc interpretability are fundamentally two facets of the same concept; and %
(\S\ref{sec:methodology:cf})~uncertainty quantification provides a principled unifying framework for generating state-of-the-art counterfactuals}. %
Consequently, \textbf{we argue that making inherently interpretable models uncertainty-aware not only improves their reliability and robustness but also provides a systematic pathway for integration of human-centred explanatory insights such as counterfactuals}. %
We then (\S\ref{sec:methodology:anti})~review alternative perspectives, thereby strengthening our position. %
We conclude in Section~\ref{sec:conclusion} by summarising our arguments and outlining future research directions. %

\section{Background}\label{sec:related}%

Let us first overview %
latest research in \emph{counterfactual explainability}~(\S\ref{sec:related:exp}) and \emph{uncertainty quantification}~(\S\ref{sec:related:uq}) %
as well as work at the \emph{intersection of these two fields}~(\S\ref{sec:related:exp+uq}). %

\subsection{Counterfactual Explainability}\label{sec:related:exp}%

Counterfactuals capture hypothetical situations where the outcome of interest is different (usually, more desirable) than what has been observed. %
They are considered the gold standard of \emph{human-centred} XAI given their strong social sciences foundations, natural familiarity to lay and expert audiences, and regulatory compliance~\citep{wachter2017counterfactual,miller2019explanation,guidotti2022counterfactual}. %
In their simplest form, they are generated by minimising the distance in the feature space between the current instance and a hypothetical data point that is assigned the desired prediction, %
i.e., finding the most \emph{similar} (distance-wise) instance of the selected class. %
This process optimises for the following properties: %
\vspace{-\topsep-\parskip-\partopsep}%
\begin{description}[itemsep=-0.0pt]%
  \item[Validity] guarantees that the counterfactual instance is classified by the explained model with the desired class. %
  \item[Similarity] places the counterfactual instance as close as possible to the factual data point in the feature space (according to the chosen distance metric), making it highly relevant and easy to reach. %
  \item[Sparsity] minimises the number of features changed between the factual and counterfactual instances, enhancing human comprehensibility of the explanation. %
\end{description}
\vspace{-\topsep-\partopsep}%

In practice, however, this strategy often yields counterfactuals that are perceptually closer to \emph{adversarial examples}~\citep{goodfellow2014explaining} than \emph{meaningful explanations}, i.e., the recommended change is either unrealistic or carries no intrinsic meaning~\citep{freiesleben2022intriguing}. %
As a solution, %
a collection of new \emph{social}, \emph{technical} and \emph{sociotechnical} desiderata was proposed~\citep{pawelczyk2020learning,delaney2021uncertainty,schut2021generating,guidotti2022counterfactual}: %
\vspace{-\topsep-\parskip-\partopsep}%
\begin{description}[itemsep=-0.0pt]%
  \item[Plausibility] requires the counterfactual state to come from the data manifold, thus be achievable in real life. %
  \item[Connectedness] extends \emph{plausibility} by ensuring that the counterfactual instance is supported by (i.e., is close in the feature space to) another data point %
  assigned to the counterfactual class (with high probability) by the explained model %
  such that there exists a direct line (or a sequence of steps through known data points) between the two that does not cross a decision boundary. %
  \item[Discriminativeness] makes the counterfactual instance \emph{unambiguous}, i.e., clearly \emph{distinguishable} from similar instances (refer back to \emph{similarity}) that are not of the counterfactual class, to %
  avoid \emph{confusing} humans with the like of \emph{adversarial examples}. %
  \item[Robustness] prevents \emph{realistic} data shifts and model changes from invalidating counterfactual explanations. %
  \item[Stability] ensures that the neighbours of the explained data point receive \emph{similar} (or identical) counterfactuals. %
  \item[Actionability] guarantees that counterfactuals can be implemented in real life, as some features may be immutable, e.g., ethnicity, while others must obey monotonicity, e.g., age, %
  simultaneously maintaining the congruity of these changes given that they may be incompatible. %
\end{description}
\vspace{-\topsep-\partopsep}%

\begin{figure*}%
  \centering
  \begin{subfigure}[t]{0.31\textwidth}
      \centering
      \includegraphics[scale=0.405]{ucf-fundamental}%
      \caption{%
          Visualisation of the three \emph{fundamental} counterfactual desiderata. %
          Explanation~A is \textbf{invalid}; explanation~B lacks \textbf{sparsity} in comparison to explanation~C as the former requires changing two features whereas the latter only one; explanation~D lacks \textbf{similarity} when compared to explanation~C given that it is farther away. %
          \label{fig:counterfactual:basic}}%
  \end{subfigure}
  \quad
  \begin{subfigure}[t]{0.62\textwidth + 1em} %
      \centering
      \includegraphics[scale=0.405]{ucf-extended}%
      \caption{%
          Visualisation of the six \emph{extended} counterfactual desiderata. %
          Explanation~A is \textbf{implausible} as it lies outside of the data manifold; explanation~B lacks \textbf{connectedness} as its closes neighbour is classified negative; explanation~C is not \textbf{discriminative} as it lies in a region where classes overlap; explanations~A, B \& C may lack \textbf{robustness} as more data points are collected to train the model and it converges; all the explanations are considered \textbf{stable} given the simplicity of this example; finally, if feature $x_1$ (horizontal axis) is assumed actionable but feature $x_2$ (vertical axis) not, only explanation~D is \textbf{actionable} as the other counterfactuals require changing both features $x_1$ and $x_2$. %
          \label{fig:counterfactual:extended}}
  \end{subfigure}
  \caption{%
  Visualisation of the (\subref{fig:counterfactual:basic}) fundamental -- \emph{validity}, \emph{similarity} and \emph{sparsity} -- and (\subref{fig:counterfactual:extended}) extended -- \emph{plausibility}, \emph{connectedness}, \emph{discriminativeness}, \emph{robustness}, \emph{stability} and \emph{actionability} -- properties of human-centred counterfactual explanations. %
  The question mark indicates the explained instance and the check mark represents a counterfactual data point. %
  \label{fig:counterfactual}}%
\end{figure*}

\begin{figure*}%
  \centering
    \begin{subfigure}[t]{0.310\textwidth}
      \centering
      \includegraphics[scale=0.405]{zcf-group}%
      \caption{%
          Generating a \textbf{diverse} (\emph{representative} and \emph{minimal}) collection of counterfactuals helps %
          to satisfy disparate needs of different explainees. %
          Explanation \textbf{availability} is not a problem in this example, especially that the depicted linear model is near-optimal and unlikely to change significantly. %
      }\label{fig:cf:group}%
  \end{subfigure}
  \quad
  \begin{subfigure}[t]{0.310\textwidth}
      \centering
      \includegraphics[scale=0.405]{zcf-both}%
      \caption{%
          A counterfactual explanation -- captured by the green vector -- communicates the entire feature set change as a single step; %
          its path-based alternative -- depicted by the blue segments -- splits it into three simpler steps. %
          The latter approach becomes particularly useful for high-dimensional data. %
      }\label{fig:cf:cl+rec}%
  \end{subfigure}
  \quad
  \begin{subfigure}[t]{0.310\textwidth}
      \centering
      \includegraphics[scale=0.405]{zcf-space}%
      \caption{%
          The \textbf{feasibility} of the shown path-based counterfactuals (which also entails their \emph{plausibility}, \emph{connectedness} and \emph{stability}) is ensured by constructing them upon pre-existing instances. %
          Accounting for the spatial properties allows to capture the paths' \emph{affinity}, \emph{branching} and \emph{divergence}. %
      }\label{fig:cf:path}%
  \end{subfigure}
  \caption{%
  Visualisation of counterfactuals in relation to %
  (\subref{fig:cf:group}) explanation groups and model selection; %
  (\subref{fig:cf:cl+rec}) path-based explainability; and %
  (\subref{fig:cf:path}) geometry of the feature space density. %
  The question mark indicates the explained instance, the star symbolises an intermediate step of a counterfactual path, and the check mark represents a counterfactual data point. %
  }\label{fig:cf}%
\end{figure*}

\begin{figure*}%
  \centering
  \begin{subfigure}[t]{0.62\textwidth}%
      \centering
      \includegraphics[scale=0.405]{uq-a}%
      \caption{The optimal model may remain \emph{aleatorically} uncertain about a prediction, e.g., the question mark, because of the intrinsic class overlap. This type of uncertainty cannot be (easily) reduced. Note, however, that collecting additional information (rather than observations), e.g., an extra feature, could allow separating the overlapping instances in the added dimension at the expense of increased \emph{epistemic} uncertainty.\label{fig:uncertainty:aleatoric}} %
  \end{subfigure}
  \quad
  \begin{subfigure}[t]{0.31\textwidth + 1em}
      \centering
      \includegraphics[scale=0.405]{uq-e}%
      \caption{The lack of knowledge about the optimal model may result in \emph{epistemic} uncertainty about a prediction, e.g., the question mark. This type of uncertainty can usually be reduced by collecting more data -- see Panel~(\subref{fig:uncertainty:aleatoric}).\label{fig:uncertainty:epistemic}}%
  \end{subfigure}
  \caption{Demonstration of (\subref{fig:uncertainty:aleatoric}) \emph{aleatoric} and (\subref{fig:uncertainty:epistemic}) \emph{epistemic} uncertainty %
  for a two-dimensional toy data set with the model class restricted to linear classifiers~\citep{hullermeier2021aleatoric}.\label{fig:uncertainty}}%
\end{figure*}

Notably, all these properties -- illustrated in Figure~\ref{fig:counterfactual} -- pertain to the counterfactual instance itself. %
Additional desiderata apply to \emph{collections} of such explanations, used because a single counterfactual is unlikely to satisfy the distinct needs of different explainees~\citep{sokol2020one}. %
In this context, \textbf{diversity} ensures that a set of counterfactuals is \emph{representative} of the available explanations while remaining \emph{minimal}. %
From the \emph{model selection} perspective, counterfactual \textbf{availability} can also be considered~\citep{kanamori2024learning}; %
it maximises the number of individuals for whom at least one acceptable explanation can be generated, %
which is important when multiple models (from a single class) with comparable performance exist~\citep{sokol2024cross}. %

\begin{figure*}[t]%
    \centering
    \begin{subfigure}[t]{0.485\textwidth}
        \centering
        \includegraphics[scale=0.405]{uq-lin}%
        \caption{Given the \emph{linear} model class, the point represented by the question mark will be classified as positive with high certainty (despite the lack of data observed in its neighbourhood that could directly support this prediction).}\label{fig:model:linear}%
    \end{subfigure}
    \quad
    \begin{subfigure}[t]{0.485\textwidth}
        \centering
        \includegraphics[scale=0.405]{uq-quad}%
        \caption{When using the \emph{quadratic} model class -- which is more flexible and expressive than linear models -- the point represented by the question mark cannot be classified as positive or negative with high certainty (unless more data are collected).}\label{fig:model:quadratic}%
    \end{subfigure}
    \caption{Demonstration of how uncertainty quantification depends on the assumed model class for (\subref{fig:model:linear}) \emph{linear} and (\subref{fig:model:quadratic}) \emph{quadratic} classifiers using a two-dimensional toy data set~\citep{hullermeier2021aleatoric}.}\label{fig:model}%
\end{figure*}

A prominent extension of counterfactuals %
builds %
action sequences %
guiding an explainee step-by-step to the chosen outcome~\citep{poyiadzi2020face}. %
Here, additional desiderata are introduced to formalise the properties of the \emph{path} connecting the factual and counterfactual instances. %
\textbf{Feasibility} entails building such a \emph{link} by following pre-existing data points, thus ensuring \emph{plausibility}, \emph{connectedness} and \emph{stability}. %
\emph{Affinity}, \emph{branching} and \emph{divergence} imbue these paths with \emph{spatial awareness} %
by capturing the geometry of the feature space density~\citep{sokol2023navigating}. %
These desiderata account for the order in which feature changes need to be implemented, group counterfactuals based on the (directional) similarity of their paths, and differentiate incompatible explanations, %
hence offer a richer perspective on counterfactual \emph{diversity} and \emph{actionability}. %
These extra properties are shown in Figure~\ref{fig:cf}. %

Adhering to the desiderata listed above allows to retrieve state-of-the-art counterfactuals without modelling the causal structure of the data generating processes~\citep{karimi2022survey}, which in real life is often infeasible.%
\footnote{%
The causal perspective is outside of this paper's scope, but %
properties such as \emph{actionability} and \emph{plausibility} can act as its proxy.%
} %
Such explanations, nonetheless, may still lack reliability, hence be unsuitable for high stakes domains, if they do not fully account for the inner workings of the underlying predictive model~\citep{rudin2019stop}. %
In XAI, a distinction is made between \emph{ante-} and \emph{post-hoc} techniques. %
Post-hoc methods generate explanations using mechanisms that operate independently of predictive models, %
which makes them portable but also prevents them from accurately capturing how these models make decisions. %
Ante-hoc interpretability, in contrast, is achieved by building inherently transparent models that adhere to domain-specific constraints, %
which guarantees their reliability, but %
often at the expense of limiting their comprehensibility to AI experts, with the explanatory needs of other stakeholders left unaddressed~\citep{sokol2023reasonable}. %
While counterfactuals are ideal for this purpose, most relevant explainers are post-hoc~\citep{guidotti2022counterfactual}, leaving this problem unresolved. %

\subsection{Uncertainty Quantification}\label{sec:related:uq}%

In addition to transparency, %
other critical properties of data-driven models -- such as their fairness, robustness, reliability, trustworthiness and accountability -- have gained prominence %
with the proliferation of AI. %
The foundational aspect of these desiderata is \emph{predictive uncertainty} %
given its crucial role in enhancing AI systems' awareness of their limitations~\citep{hullermeier2021aleatoric}. %
Primarily, this involves distinguishing between \emph{aleatoric} uncertainty, which arises due to the randomness inherent to the data generating process, and \emph{epistemic} uncertainty, which stems from the learning algorithm's ignorance of the true underlying model. %

Aleatoric uncertainty is \emph{irreducible} in the sense that more observations of a given phenomenon will not improve our ability to predict its outcome; %
a prototypical example is a coin toss -- no matter how many data points we collect, we cannot reliably predict how the coin will land next given the intrinsic variability of this process. %
In certain cases, however, collecting more information (as opposed to more observations), e.g., an extra feature, can help to alleviate aleatoric uncertainty, e.g., by separating thus far overlapping data points in the added dimension. %
Such an approach reduces aleatoric uncertainty often at the expense of its epistemic counterpart as fitting a model in higher dimensions is more difficult and requires a larger data set. %
Epistemic uncertainty, on the other hand, is \emph{reducible} as additional observations of a given phenomenon can help us to more reliably predict its outcome; %
revisiting the coin toss example, the larger our set of observations is, the more precisely we can estimate the coin's bias. %
Figure~\ref{fig:uncertainty} demonstrates these two types of uncertainty for a two-dimensional toy example. %

Methods to reliably quantify aleatoric and epistemic uncertainty, such as \emph{calibration}, are still in early stages of development~\citep{silva2023classifier}. %
By and large, we lack AI models that are truly uncertainty-aware, which curtails the use of this technology in high stakes domains. %
Despite often being overlooked, uncertainty estimation is \emph{never} assumption-free. %
It is influenced by the underlying, and sometimes implicit, data modelling assumptions, among which the model class choice is particularly consequential. %

Notably, the degree of predictive uncertainty is determined by the flexibility of the model class -- see Figure~\ref{fig:model}. %
More restrictive families of functions result in lower uncertainty and vice versa, with those deemed universal approximators, e.g., neural networks, impacted the most~\citep{hullermeier2021aleatoric}. %
Other, model-specific, assumptions also influence uncertainty quantification. %
For example, linear models -- see Figure~\ref{fig:model:linear} -- presuppose that the farther a data point is placed from the decision boundary, the less uncertain its prediction is. %
This holds even if such an instance comes from a sparse data region. %
While this property is inherent to linear models, its consequences may be undesirable and necessitate special handling, e.g., tweaking the uncertainty estimates post-hoc to convey low confidence outside of the data manifold~\citep{perello2016background}. %

In addition to aleatoric and epistemic, various other sources of uncertainty exist~\citep{hullermeier2021aleatoric}. %
\emph{Model uncertainty} comes from incorrect model class choice, which is common in practice; %
it cannot be easily detected and addressed, %
but when the model class is highly expressive (or considered a universal approximator), this type of uncertainty is inconsequential. %
\emph{Approximation uncertainty} arises when the model learnt from data does not align with the true or optimal model; %
it contributes to epistemic uncertainty and is particularly pronounced for highly expressive model classes and sparse training data. %
In general, uncertainty quantification is premised on the \emph{closed/stable world assumption}, violating which makes this process considerably more difficult~\citep{gigerenzer2023psychological}. %
Relevant examples are %
non-stationary data generating processes or data distribution shifts, %
emergence of new and disappearance of known target classes, and %
noisy or imprecise data features (e.g., due to measurement errors). %

Another challenge comes from %
representing \emph{lack of knowledge}. %
The common approach of modelling total ignorance with the uniform distribution is unable to distinguish between %
\emph{precise} knowledge of an inherently random process, e.g., a fair coin toss, %
and \emph{complete lack} of knowledge about it, e.g., a possible bias of the coin. %
Overcoming this problem requires working with \emph{second-order uncertainty} given that a single distribution cannot capture uncertain knowledge~\citep{shafer1976mathematical,dubois1988possibility,walley1991statistical,smets1994transferable}. %

\subsection{Uncertainty and Counterfactual Explainability}\label{sec:related:exp+uq} %

Integrating uncertainty quantification with XAI has recently gained interest, albeit with much left to be explored~\citep{bhatt2021uncertainty}. %
Notably, reliable uncertainty estimation remains a key challenge in building ante-hoc interpretable models~\citep{rudin2022interpretable}. %
More broadly, explaining why an AI system is uncertain can help humans to understand its limitations and act to reduce its uncertainty; %
conversely, presenting the uncertainty of an explanation can improve humans' ability to make responsible AI-aided decisions. %

In this space, %
\citet{antoran2020getting} proposed a counterfactual explainer that shows how to reduce uncertainty of a prediction. %
\citet{delaney2021uncertainty} used uncertainty to evaluate the quality of %
counterfactuals, expanding the \emph{validity} desideratum from crisp to probabilistic classification. %
To this end, they proposed \emph{trust scores}, which link (epistemic) uncertainty to counterfactual \emph{plausibility}; %
others adapted pre-existing outlier detection methods for this purpose~\citep{romashov2022baycon}. %
Similarly, \citet{thiagarajan2022training} %
designed an explainer that %
generates high-confidence counterfactual data points by %
minimising their uncertainty. %

\citet{duell2024quce} %
expanded the notion of counterfactual \emph{plausibility} by %
computing uncertainty of the vector connecting the factual and counterfactual instances. %
\citet{kanamori2024learning} explored %
the implicit link between epistemic uncertainty and \emph{model multiplicity} -- a phenomenon where a group of models has comparable predictive performance despite intrinsic differences (see Figures~\ref{fig:uncertainty:epistemic} and \ref{fig:model}), sometimes called the Rash\=omon effect of statistics~\citep{breiman2001statistical,rudin2024amazing}; %
they did so in the context of %
counterfactual \emph{robustness} and \emph{availability}. %
\citet{schut2021generating} directly coupled counterfactual explainability and uncertainty %
by connecting aleatoric and epistemic uncertainty respectively to %
counterfactual discriminativeness (which they call \emph{unambiguity}) and plausibility (called \emph{realism}) %
to generate high-quality explanations. %


\section{Building Bridges: Uncertainty-aware XAI}\label{sec:methodology}%

Despite latent connections between uncertainty and various forms of AI interpretability and explainability, (\S\ref{sec:methodology:challenges})~many such links remain unexplored with the underlying challenges unaddressed %
-- XAI is thus largely uncertainty-unaware. %
This section focuses %
on two critical gaps at the intersection of these fields. %
One, (\S\ref{sec:methodology:ante})~%
we argue that (reliable) uncertainty quantification and ante-hoc interpretability are tightly coupled and synergistic %
since the former reinforces AI transparency and accountability while %
the latter provides well-defined model forms benefiting the former. %
Making ante-hoc interpretable models uncertainty-aware additionally %
creates a systematic pathway for adoption of human-centred developments from across XAI, %
expanding the comprehensibility of such AI systems to non-technical stakeholders, e.g., %
through counterfactual insights %
as we demonstrate next. %



Two, (\S\ref{sec:methodology:cf})~%
we argue that uncertainty quantification offers a \emph{principled unifying framework} for generating state-of-the-art (non-causal) counterfactuals, superseding many ad-hoc proxies and criteria currently used to this end. %
Of particular relevance are %
the distinction between aleatoric and epistemic uncertainty, which allows to overcome %
the limitations imposed by employing its aggregate measure, %
and the properties of the entire path connecting factual and counterfactual instances, rather than of the latter point alone. %
We finish this section by (\S\ref{sec:methodology:anti})~reviewing alternative perspectives. %

\subsection{Open Challenges}\label{sec:methodology:challenges}


Section~\ref{sec:related:exp+uq} showed that uncertainty has primarily been applied, albeit often indirectly, to evaluate counterfactuals and, too a much lesser extent, guide their generation. %
Most %
explainers \emph{overlook} data modelling assumptions that are foundational to robust uncertainty quantification and use \emph{ad-hoc} fixes when uncertainty estimates are unavailable. %
They employ proxies like %
operating within \emph{previously observed data points}, %
\emph{measuring distance} to the nearest instance, %
adapting pre-existing \emph{outlier detection} methods, %
approximating the data manifold in \emph{latent space}, %
or defining \emph{custom metrics} such as the trust scores. %
While these approaches help to ensure \emph{plausibility}, \emph{connectedness} and \emph{actionability} -- as well as other desiderata -- they rely on surrogate measures of uncertainty %
that can be unreliable, e.g., when they are incompatible with the explained class of models. %

XAI's blindness to uncertainty %
thus not only undermines the \emph{truthfulness} of explanations, but more critically it side-steps the challenge of %
building high \emph{quality} models AI~\citep{schut2021generating}. %
The latter is symptomatic of a broader problem, where the quality of models and their explanations tends to be assessed independently (especially for post-hoc methods)~\citep{sokol2024what}. %
Another consequence is that most (counterfactual) explainers operate on \emph{crisp} predictions, %
which prevents them from relying on the (relative) uncertainty of each class, possibly exacerbating explanation \emph{ambiguity}~\citep{sokol2020limetree}. %
More broadly, XAI rarely ventures beyond %
\emph{deterministic} models; while it sometimes considers \emph{first-order} probabilistic predictions, %
\emph{second-order} uncertainty remains alien~\citep{wood2024model}. %

Moreover, research at the intersection of XAI and uncertainty quantification is predominantly concerned with differentiable as well as neural model classes applied to images and text, overlooking other areas of AI despite the prevalence of tabular data in real-life applications~\citep{shwartz2022tabular}. %
Consequently, (ante-hoc) inherently interpretable models -- which tend to offer superior performance for structured data~\citep{rudin2019stop} -- are neglected. %
They not only lack methods %
to imbue them with reliable aleatoric and epistemic uncertainty estimates~\citep{rudin2022interpretable}, %
but also %
that can produce human-centred explanatory insights (e.g., counterfactuals) into their operation. %
Since ante-hoc interpretability in itself is often insufficient to engender understanding in non-technical stakeholders, such techniques are of paramount importance~\citep{sokol2023reasonable}. %


\subsection{Uncertainty Quantification and Ante-hoc Interpretability are Fundamentally Intertwined}\label{sec:methodology:ante}%


Given their technological nature, %
AI models %
should not be accepted based on the \emph{trust} they engender, %
e.g., after prolonged interactions, %
but rather due to their operational \emph{reliability} and \emph{robustness}~\citep{ryan2020ai}. %
Their sound technical design and functioning are especially important in high stakes domains~\citep{rudin2019stop}. %
These principles are embodied by %
(ante-hoc) \emph{interpretability} and \emph{uncertainty quantification}, %
which allow humans to develop correct understanding of models' \emph{capabilities} and \emph{limitations}~\citep{schut2021generating}, leading to \emph{accountability} and \emph{trust}~\citep{tomsett2020rapid}. %
While uncertainty is generally used to capture the \emph{quality} of AI models and their predictions, %
it is important not to overlook its decomposition into the aleatoric and epistemic parts %
given that they convey fundamentally distinct information. %

Based on our review of %
uncertainty and %
ante-hoc interpretability, %
we posit that %
these concepts %
are not only complementary~\citep{schut2021generating} %
but rather constitute different views of the same underlying idea -- %
with the former being necessary for a strong notion of the latter, and the latter enhancing the former. %
While \citet{rudin2022interpretable} have briefly noted the link between these two concepts, there is much left to be explored. %
Recall that ante-hoc interpretability offers %
inherently robust, accountable and transparent AI models %
by constraining their form~\cite{rudin2019stop}. %
Specifying the modelling assumptions explicitly, in turn, allows for %
reliable uncertainty quantification and decomposition %
given how sensitive these processes can be (see Figure~\ref{fig:model}). %
This \emph{shared foundation} is critical as %
post-hoc (and model-agnostic) uncertainty estimation can yield %
incorrect %
insights due to its possible incompatibility with (implicit) data modelling assumptions -- %
akin to how post-hoc explainers may not %
truthfully capture the functioning of AI models. %

Current XAI research, nonetheless, often overlooks the quality (including predictive power) of a model as well as the importance and implications of a model class choice; %
this is especially true for post-hoc methods but also affects, albeit to a much lesser extent, ante-hoc interpretability. %
For example, consider the fundamental differences in the decision boundary shape learnt by linear models and decision trees, and their impact on predictions and uncertainty estimates. %
For the former, these properties rely largely on the distance from the decision boundary; whereas for the latter, %
the relation is more nuanced and depends on the hyper-rectangle partition of the feature space as well as the quantity and class distribution of the training data used to learn it. %
Reducing epistemic uncertainty of models considered to be universal approximators is particularly challenging exactly because of their inherent expressiveness. %
Here, a popular solution is to introduce strong model form regularisation -- an approach that bears the hallmarks of ante-hoc interpretability. %

A slight modification to a model class %
-- e.g., bounding the hyper-rectangles of a decision tree from all sides -- %
could yield drastically different predictions and uncertainty estimates. %
Similarly, some model classes capture the training data distribution, while other do not, leading to different handling of instances located in sparse data regions. %
While post-processing techniques can enhance a particular model class in this respect~\citep{perello2016background}, %
such an approach introduces asymmetry between the modelling assumptions and properties of the final predictions. %
If the latter are used in downstream tasks, this may be undesirable; %
more broadly, it signals that another model class fulfilling the desired requirements ought to be considered instead~\citep{rudin2024amazing}. %

Notably, %
access to reliable uncertainty estimates promises to %
enable generation of a broad range of explanatory insights; %
we show this for counterfactuals below (\S\ref{sec:methodology:cf}). %
Guiding this process with other, ad-hoc mechanisms %
may yield incorrect explanations, %
and would be especially harmful, and counterproductive, for ante-hoc interpretable models. %
In essence, such explainers need to compensate for the shortcomings or lack of (native) uncertainty estimates post-hoc, %
which not only increases their complexity but also tends to introduce (implicit) assumptions that are likely incompatible %
with the operation of many AI models, %
thus degrading explanation quality. %
Uncertainty-guided approaches that produce human-centred explanatory insights are of particular relevance %
for ante-hoc interpretable models %
given their incomprehensibility to non-technical stakeholders, which likely harms their real-life adoption. %

{
\begin{figure}[!t]%
  \centering
  \begin{subfigure}[t]{0.2375\textwidth}
      \centering
      \includegraphics[width=\textwidth]{example-linear}%
      \caption{%
        Linear model.
      }\label{fig:example-p:linear}%
  \end{subfigure}
  \hfill%
  \begin{subfigure}[t]{0.2375\textwidth}
      \centering
      \includegraphics[width=\textwidth]{example-linear+bg}%
      \caption{%
        Linear model with manifold. %
      }\label{fig:example-p:linear+bg}%
  \end{subfigure}
  \caption{%
    Illustration of uncertainty-driven instance- (green) and path-based (blue) counterfactuals for %
    (\subref{fig:example-p:linear})~a standard linear model and %
    (\subref{fig:example-p:linear+bg})~one enhanced with a background class~\citep{perello2016background}. %
    In the former case, following the direction perpendicular to the decision boundary is the optimal approach; %
    in the latter case, the counterfactual vector and path are skewed since epistemic uncertainty additionally captures the data manifold shape. %
  }\label{fig:example-p}%
\end{figure}
}

\subsection{Uncertainty Quantification is a Unifying Framework for Counterfactual Explainability}\label{sec:methodology:cf}

Given the appeal of counterfactuals, %
improving their retrieval is crucial. %
We posit that uncertainty quantification is uniquely suited to this end and provides a principled unifying framework for counterfactual explainability, subsuming the desiderata listed in Section~\ref{sec:related:exp}. %
Specifically, %
expressing them through constraints on \emph{aleatoric} and \emph{epistemic uncertainty}, instead of its aggregate measure, %
and working with the \emph{path-based} conceptualisation of counterfactuals, rather than the counterfactual instances alone, %
offer many benefits. %

Chief among them is \emph{explanation consistency across models} (whether from the same or different model classes) given that modelling assumptions are directly accounted for in explanation generation. %
This is more desirable than explanation consistency with respect to an explainer (applied to distinct model classes) %
since its operational characteristics are unlikely to be compatible with a broad range of unique modelling assumptions. %
Optimal, in terms of uncertainty, explanations may thus necessarily be dissimilar in absolute terms across distinct modelling scenarios -- e.g., different counterfactual instances -- but at the same time consistent with regard to their high-level desiderata as shown in Figure~\ref{fig:example-p}. %
Consequently, building models that are uncertainty-aware, reliable and robust because they are fundamentally suitable for a particular application appears more important than creating ``universal'', thus overly complex, explainers. %

\paragraph{Similarity \& Sparsity}%
These observations offer an alternative perspective on counterfactual \emph{similarity}, allowing us to compare explanations to each other through their uncertainty in addition to their separation in the feature space as well as distance from the explained instance. %
Otherwise, the classic interpretation of counterfactual \emph{similarity} and \emph{sparsity} positions these properties as complementary to the notion of uncertainty, hence they can be implemented independently of it, %
e.g., by minimising the Manhattan and Euclidean distance metrics. %
For path-based explanations, these two properties should additionally account for the quantity and size of individual steps as well as the number of features affected by each segment; %
note that these characteristics %
can, to a degree, be reflected in uncertainty estimates, e.g., when the data manifold shape captures feature (inter)dependence. %

\paragraph{Validity \& Actionability}%
More directly, %
aleatoric and epistemic uncertainty estimates %
offer a comprehensive perspective on the \emph{validity} of (instance-based) explanations. %
Going beyond crisp classification allows to better differentiate explanations, %
which can be especially insightful when dealing with more than two classes, where the interplay of their individual probabilities may be nuanced~\citep{sokol2020limetree}. %
Furthermore, lack of validity due to aleatoric uncertainty clearly communicates different information about an explanation than when the uncertainty is of epistemic nature. %
For example, qualifying the validity of a counterfactual though its \emph{actionability} usually requires confining it to features that are deemed \emph{mutable} and accounting for the \emph{directionality}, \emph{monotonicity} and \emph{rate} of their change as well as between-feature \emph{compatibility} and \emph{ratio}. %
However, uncertainty estimates -- their epistemic component in particular -- can offer useful insights with this regard. %
These validity and actionability perspectives can be easily extended to %
path-based counterfactuals by inspecting uncertainty of the points constituting their individual steps. %
A desirable uncertainty profile of a counterfactual path can thus be characterised by %
  low epistemic uncertainty along its entirety, %
  low aleatoric uncertainty towards its end, and %
  decision boundary crossings in regions of increased aleatoric uncertainty or a balanced mixture of both uncertainty types. %


\paragraph{Robustness \& Stability}%
Quantifying the variability of aleatoric and epistemic uncertainty within a hyper-sphere %
placed around the factual and counterfactual instances, or %
slid along the vector or path linking them, %
can further improve explanation \emph{robustness} and \emph{stability}, e.g., by reducing its sensitivity to model shifts. %
It can also alleviate the adverse effects of noisy or imprecise human implementation of counterfactuals, which may be out of their control~\citep{xuan2024perfect}. %
Avoiding %
sharp changes in per-class uncertainty measured along the vector or path and %
ensuring that it obeys monotonicity constraints such that a decision boundary between any two classes is only crossed once %
can be highly beneficial as well. %


\paragraph{Feasibility, Plausibility \& Connectedness}%
\emph{Validity} expressed in terms of (epistemic) uncertainty also implicitly captures %
explanation \emph{feasibility}; %
as a bonus, this approach relaxes the strong assumption that \emph{atypical} observations are impossible, instead quantifying their \emph{improbability}. %
\emph{Plausibility} and \emph{connectedness} can be implemented through uncertainty like this as well -- in lieu of direct reliance on data observed before -- %
which also %
improves explanation \emph{privacy}~\citep{small2023counterfactual}. %


\paragraph{Discriminativeness}%
\emph{Validity} viewed through (aleatoric) uncertainty %
additionally %
captures and improves %
explanation \emph{discriminativeness} %
given that low aleatoric uncertainty around a counterfactual %
curbs its \emph{ambiguity}. %
Notably, the uncertainty perspective naturally generalises this property, allowing to explicitly differentiate %
counterfactual instances from all the other classes and not only the factual one. %
\emph{Discriminativeness} reinforces explanation \emph{connectedness} too, making counterfactuals not only \emph{appealing} but also \emph{realistic}. %

\paragraph{Diversity, Spatiality \& Availability}%
\emph{Similarity} naturally lends itself to generating \emph{diverse} counterfactuals. %
Additionally, path-based explanations determine the feature change order, offering another mechanism for \emph{diversity} as well as implementation of \emph{spatially-aware} desiderata. %
For example, uncertainty can indicate steps where paths are likely to \emph{branch} (aleatoric) and robust decision boundary crossings (epistemic). %
\emph{Availability} can be viewed via a similar lens. %


\subsection{Alternative Views}\label{sec:methodology:anti}




Reliable uncertainty quantification is undoubtedly challenging, and in some cases even impossible. %
Using it as the foundation of XAI may %
thus curtail the progress of this field, at least short-term. %
Embracing this perspective will also likely %
require a fundamental shift %
in how explanations are formalised -- e.g., they might become probabilistic~\citep{chau2023explaining}, %
which can be seen in Figure~\ref{fig:model:linear} for a counterfactual placed in the span of possible models (top-right) -- %
increasing the technical complexity of XAI to the detriment of lay explainees. %
With this in mind, %
for many (low stakes) domains, crisp (ante-hoc interpretable) modelling may be sufficient, making uncertainty estimation redundant. %
However, simply considering it can catalyse important insights even if %
it is ultimately deemed unnecessary. %
When AI models are provided as inaccessible oracles, i.e., black boxes, our perspective is also inapplicable; %
here, uncertainty proxies and post-hoc explainability are largely unavoidable. %

Our strong focus on ante-hoc interpretability is rooted in its rich history of success and reported superiority for structured data; while the latter claim can be challenged~\citep{wang2021hybrid}, dismissing inherently transparent AI altogether without further evidence seems premature. %
One can additionally argue that %
using auxiliary explanatory mechanisms to inspect ante-hoc interpretable models inadvertently breaks this property. %
But in our case this position can be easily refuted since the focus remains on reliable data modelling, with counterfactuals simply being a by-product of access to uncertainty estimates. %
Crucially, counterfactual explainability can be viewed through frameworks other than uncertainty quantification, most notably causality. %
While this perspective promises more reliable explanations and is compatible with ante-hoc interpretability, causal modelling is often unrealistic in practice as mentioned earlier. %


\section{Conclusion and Future Work}\label{sec:conclusion}%

In this paper we argued that %
\emph{uncertainty quantification} and \emph{ante-hoc interpretability} are fundamentally intertwined; %
recognising this synergistic relation promises to benefit comprehensibility of predictive models %
as well as improve their aleatoric and epistemic uncertainty. %
We also asserted that uncertainty quantification offers a sound \emph{unifying framework} for \emph{counterfactual explainability}; %
this connection facilitates rigorous generation of reliable counterfactuals for different model classes. %
Consequently, we observed that %
counterfactual explainers tend to be overcomplicated since they attempt to \emph{compensate for various shortcomings} of AI models. %
Grounding these tools in uncertainty could allow them to remain relatively simple, %
instead shifting the engineering efforts towards \emph{building more principled predictive models} %
that are ante-hoc interpretable and uncertainty-aware. %
Together, our two tenets opened a pathway for \emph{bringing human-centred explanatory insights}, like counterfactuals, \emph{to ante-hoc interpretable models}, whose comprehensibility %
has thus far been largely limited to technical experts. %

Given their prevalence, %
this paper focused on counterfactuals, %
nonetheless in future work we will expand our uncertainty-centred perspective to other explanation types. %
Since our approach is premised on access to reliable aleatoric and epistemic uncertainty estimates, %
we will also investigate %
latest \emph{uncertainty quantification} and \emph{probability calibration} methods, %
focusing on ante-hoc interpretable models given their strong connection to this topic. %
We additionally plan to examine the role that other \emph{types} and \emph{sources} of uncertainty play in AI transparency, %
driving further exploration of uncertainty-aware XAI; %
in particular, we will look into second-order uncertainty %
and suitable (probabilistic) formalisations of explanations. %
Such a broad perspective can, among others, improve the \emph{robustness} and \emph{stability} of predictions and explanations, %
benefiting (high stakes) domains like healthcare, %
where decisions are often made based on incomplete and uncertain information. %
More broadly, we will study other fundamental AI concepts that are overlooked in XAI research. %


\section*{Acknowledgements}
This research was supported by the 2024 DAAD \emph{Postdoc-NeT-AI -- Short-term Research Stay} programme. %


\bibliography{uxai}
\bibliographystyle{icml2025}

\end{document}
