
\documentclass{article}

\usepackage{microtype}
\usepackage{graphicx}
\usepackage{booktabs} %
\usepackage{multirow}
\usepackage{enumitem}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage[makeroom]{cancel}


\usepackage{thm-restate}
\newcommand{\theHalgorithm}{\arabic{algorithm}}


\usepackage[preprint]{style/icml2025}


\def\loose{\looseness=-1}



\usepackage{amssymb}%
\usepackage{pifont}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{xspace}


\usepackage[final, colorlinks=true, allcolors=blue, pagebackref=true]{hyperref}
\usepackage[capitalize,noabbrev]{cleveref}

\newcommand\tara[1]{\noindent{\color{purple} {\bf \fbox{Tara}} {\it#1}}}
\newcommand\joey[1]{\noindent{\color{blue} {\bf \fbox{Joey}} {\it#1}}}
\newcommand\alex[1]{\noindent{\color{orange} {\bf \fbox{Alex}} {\it#1}}}
\newcommand\charlie[1]{\noindent{\color{red} {\bf \fbox{Charlie}} {\it#1}}}
\newcommand\leon[1]{\noindent{\color{blue} {\bf \fbox{Leon}} {\it#1}}}

\newcommand{\std}[1]{\textcolor{black}{\scriptsize{$\pm #1$}}}

\input{math_commands}


\renewcommand*{\figureautorefname}{Fig.}
\renewcommand*{\tableautorefname}{Tab.}
\newcommand{\propositionautorefname}{Prop.}
\makeatletter
\renewcommand*{\appendixautorefname}{\S\@gobble}
\renewcommand*{\sectionautorefname}{\S\@gobble}
\renewcommand*{\subsectionautorefname}{\S\@gobble}
\renewcommand*{\subsubsectionautorefname}{\S\@gobble}
\makeatother

\newcommand{\ie}{\textit{i.e.}}
\newcommand{\eg}{\textit{e.g.}}

\usepackage[createShortEnv,conf={no link to proof, text link},commandRef=autoref]{proof-at-the-end}



\renewcommand*{\backrefalt}[4]{%
    \ifcase #1 \footnotesize{(Not cited.)}%
    \or        \footnotesize{(Cited on page~#2)}%
    \else      \footnotesize{(Cited on pages~#2)}%
    \fi}

\declaretheorem[name=Theorem,numberwithin=section]{thm}
\declaretheorem[name=Proposition,numberwithin=section]{prop}
\declaretheorem[name=Lemma,numberwithin=section]{lem}


\hypersetup{
	colorlinks=true,       %
	linkcolor=blue,        %
	citecolor=blue,        %
	filecolor=magenta,     %
	urlcolor=blue         
}


\usepackage[textsize=tiny]{todonotes}


\icmltitlerunning{Scalable Equilibrium Sampling with Non-Equilibrium Boltzmann Generators}

\begin{document}

\twocolumn[
\icmltitle{Scalable Equilibrium Sampling with Sequential Boltzmann Generators}



\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist} 
\icmlauthor{Charlie B. Tan}{equal,oxford}
\icmlauthor{Avishek Joey Bose}{equal,oxford,mila}
\icmlauthor{Chen Lin}{oxford}
\icmlauthor{Leon Klein}{berlin}
\icmlauthor{Michael M. Bronstein}{oxford}
\icmlauthor{Alexander Tong}{mila,udem}
\end{icmlauthorlist}

\icmlaffiliation{mila}{Mila -- Qu\'ebec AI Institute}
\icmlaffiliation{udem}{Universit\'e de Montr\'eal}
\icmlaffiliation{oxford}{University of Oxford}
\icmlaffiliation{berlin}{Freie Universit√§t Berlin}

\icmlcorrespondingauthor{Charlie B. Tan}{charlie.tan@cs.ox.ac.uk}
\icmlcorrespondingauthor{Avishek Joey Bose, Alexander Tong}{bosejoey,alexander.tong@mila.quebec}

\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]



\printAffiliationsAndNotice{\icmlEqualContribution} %

\begin{abstract}

\looseness=-1
Scalable sampling of molecular states in thermodynamic equilibrium is a long-standing challenge in statistical physics. Boltzmann generators tackle this problem by pairing powerful normalizing flows with importance sampling to obtain statistically independent samples under the target distribution. In this paper, we extend the Boltzmann generator framework and introduce \namelong (\nameshort) with two key improvements. The first is a highly efficient non-equivariant Transformer-based normalizing flow operating directly on all-atom Cartesian coordinates. In contrast to equivariant continuous flows of prior methods, we leverage exactly invertible non-equivariant architectures which are highly efficient both during sample generation and likelihood computation. As a result, this unlocks more sophisticated inference strategies beyond standard importance sampling. More precisely, as a second key improvement we perform inference-time scaling of flow samples using annealed Langevin dynamics which transports samples toward the target distribution leading to lower variance (annealed) importance weights which enable higher fidelity resampling with sequential Monte Carlo. \nameshort achieves state-of-the-art performance w.r.t.\ all metrics on molecular systems, demonstrating the first equilibrium sampling in Cartesian coordinates of tri, tetra, and hexapeptides that were so far intractable for prior Boltzmann generators.





\end{abstract}

\section{Introduction}
\label{sec:introduction}

\begin{figure}[t]
    \includegraphics[width=\linewidth]{figures/Energy_contour_ABGen.pdf}
    \vspace{-10pt}
    \caption{
        \looseness=-1 \nameshort uses inference time non-equilibrium transport to move initial proposal samples from a normalizing flow. %
    }
    \vspace{-10pt}
    \label{fig:visual_abstract}
\end{figure}

\looseness=-1
The simulation of molecular systems at the all-atom resolution is of central interest in understanding complex natural processes. These include important biophysical processes such as protein-folding~\citep{noe2009constructing,lindorff2011fast}, protein-ligand binding~\citep{buch2011complete}, and formation of crystal structures~\citep{parrinello1980crystal,matsumoto2002molecular}, whose understanding can aid in problems that range from long-standing global health challenges to efficient energy storage~\citep{Deringer_2020}. 

\looseness=-1
The dominant paradigm for molecular simulation involves running Markov Chain Monte Carlo (MCMC) or Molecular Dynamics (MD) whereby the equations of motion are integrated with finely discretized time steps. However, such molecular systems often exist in thermodynamic equilibrium by remaining for long time horizons in metastable states before rapidly transitioning to another metastable state. Such metastable states are captured in the minima of a complex energy landscape, associated with the molecular system's equilibrium (Boltzmann) distribution at a given temperature. 
Unfortunately, drawing uncorrelated samples from such metastable states via traditional MD or MCMC methods is prohibitively computationally expensive, requiring long simulation steps with small updates on the order of femtoseconds $1\text{fs} =10^{-15}\text{s}$, as transitions are rare events due to the presence of high-energy barriers between well-separated metastable states~\citep{wirnsberger2020targeted}. 

\looseness=-1
An alternative approach is to enhance sampling efficiency by leveraging powerful generative models such as normalizing flows~\citep{dinh2016density,rezende2015variational} trained on existing biased datasets, to produce approximate samples which can then be reweighted via importance sampling to follow the desired Boltzmann distribution. Such models, called {\em Boltzmann generators} (BG)~\citep{noe2019boltzmann}, allow faster sampling through amortization as generation is significantly cheaper computationally than running MD or MCMC. Despite their appeal, it remains challenging for existing BGs to generate uncorrelated samples in their native Cartesian coordinates from the energy modes of the Boltzmann distribution for larger molecular systems at the scale of small peptides (2 amino acids)~\citep{klein2023equivariant,midgley2023se}. The principal drawback inhibiting scalability stems from the lack of expressive equivariant architectures that are also exactly invertible~\citep{bose2021equivariant,midgley2023se}, or the present over-reliance on simple $\text{E}(n)$-GNN's~\citep{satorras2021n} based equivariant vector fields used in the design of continuous-time normalizing flows~\citep{chen_neural_2018}. As a result, even the most performant BGs suffer from low overlap with the target Boltzmann distribution, leading to poor sampling efficiency during importance sampling. 

\begin{table}[htb]
    \vspace{-5pt}
    \centering
    \caption{\looseness=-1 \small Overview of the properties of models for sampling from target distributions with (possibly biased data).}
\resizebox{1\columnwidth}{!}{
\begin{tabular}{l|cccccc}
\toprule
Method  & Use $\mathcal{E}(x)$ & Exact Likelihoods & Use Data & Transport\\
\midrule
DEM~\citep{akhound2024iterated}                 & \cmark & \xmark & \xmark & \xmark \\
NETS~\citep{albergo_nets_2024}                  & \cmark & \cmark & \xmark & \cmark \\
BG~\citep{noe2019boltzmann}                 & \cmark & \cmark & \cmark & \xmark \\
\nameshort (ours) & \cmark & \cmark & \cmark & \cmark\\
\bottomrule
\end{tabular}
}
    \label{tab:summary}
\end{table}

\looseness=-1
\xhdr{Present work}
In this paper, we introduce \namelong (\nameshort) a novel extension to the existing Boltzmann generator framework. \nameshort makes progress on the scalability of Boltzmann generators in Cartesian coordinates along two complementary axes:  (1) scalable pre-training of softly $\sethree$-equivariant proposal normalizing flows in BGs;  and (2) inference time scaling of proposal flow samples and their importance weights under fast non-equilibrium processes, e.g. such as Langevin dynamics. The final result yields higher quality generated samples with reduced weight redundancy through SMC reweighting and thus allowing for the computation of important observable quantities such as free energy differences between metastable states of $\mu_{\text{target}}(x)$. 



\looseness=-1
Our proposed approach \nameshort scales up proposal normalizing flows in BG's by following recent advances in atomistic generative modeling, e.g.\ AlphaFold 3~\citep{abramson2024accurate}. In particular, we opt to remove the rigid $\sethree$-equivariance as an explicit architectural inductive bias in favor of softly enforcing it through simpler and more efficient data augmentation. To further improve samples and their importance weights---a crucial step in the real-world application of BGs---we perform inference scaling by designing a target-informed non-equilibrium process. More precisely, we define an interpolation between the proposal flow energy distribution (i.e., negative log density of samples) and the known target Boltzmann energy. Crucially, simulating samples at inference via the transport dynamics can be coupled to an equivalent time evolution of importance weights, converting naturally to the well-established technique of Sequential Monte Carlo (SMC) \citep{doucet2001sequential} in continuous time. As a result, \nameshort can easily improve over the simple one-step importance sampling methodology used in existing BGs. We summarize the different aspects of our proposed \nameshort in comparison to other learned samplers and Boltzmann generators in~\cref{tab:summary}.

\looseness=-1
We instantiate \nameshort using exactly invertible architectures by utilizing a modernized \emph{non-equivariant} Transformer architecture as the backbone and use best-in-class models in TarFlow~\citep{zhai2024normalizing}. 
We demonstrate that exactly invertible architectures, because of fast and exact log-likelihood computations, benefit from inference-scaling. We emphasize this is in stark contrast to continuous normalizing flows that power prior SOTA Boltzman generators which require both simulation of the 2nd order divergence operator and differentiating through an ODE solver. Furthermore, we demonstrate that enforcing equivariance softly along with appropriate normalization strategies enables us to stably scale the size of proposal flows in \nameshort. On a theoretical front, we study the added bias of common numerical tricks in the literature such as thresholding, and propose an automatic scheme to find the optimal thresholding parameter.
Empirically, we observe \nameshort achieve state-of-the-art results across all metrics, and due to the enhanced computational efficiency far outperform continuous BG's on all datasets. In particular, \nameshort is the first method to solve tripeptides, tetrapeptides, hexapeptides, and makes progress towards equilibrium sampling of decapeptides in Cartesian coordinates while past BG methods were intractable beyond dipeptides.


\section{Background and preliminaries}
\label{sec:background}
\looseness=-1
We are interested in drawing statistically independent samples from the target Boltzmann distribution $\mu_{\text{target}}$, with partition function $\gZ$, defined over $\R^{n \times 3}$:
\begin{equation*}
    \mu_{\text{target}}(x) \propto \exp\left( \frac{-\E(x)}{k_{\text{B}}T} \right), \gZ = \int_{\mathbb{R}^d} \exp\left( \frac{-\E(x)}{k_{\text{B}}T} \right) dx.
\end{equation*}
\looseness=-1
The Boltzmann distribution is defined for a system and includes the Boltzmann constant $k_{\text{B}}$, and is specified for a given temperature $T$. Additionally, the potential energy of the system $\gE: \R^{n \times 3} \to \R$ and its gradient $\nabla \gE$ can be evaluated at any point $x \in \R^{n \times 3}$, but the exact density $\mu_{\text{target}}(x)$ is not available as the partition function $\gZ$ associated to the Boltzmann distribution in general is intractable to evaluate. 

\looseness=-1
In this paper, unlike pure sampling-based settings, we are afforded access to a small biased dataset of $N$ samples $\gD = \{ x^i \}_{i=1}^{N}$, provided as an empirical distribution $p_{\gD}$. Consequently, it is possible to perform an initial learning phase that fits a generative model $p_{\theta}$, with parameters $\theta$, to $p_{\gD}$---e.g. by minimizing the forward KL $\KL(p_{\gD} || p_{\theta})$---to act as a proposal distribution that can be corrected. 

\subsection{Normalizing Flows}
\label{sec:normalizing_flows}
\looseness=-1
A key desirable property needed for the correction of a trained generative model $p_{\theta}$ on a biased dataset $\gD$ is the ability to extract an exact likelihood $p_{\theta}(x)$. Normalizing flows~\citep{dinh2016density,rezende2015variational} represent exactly such a model class as they learn to transform an easy-to-sample base density to a desired target density using a parametrized diffeomorphism. More formally, given a sample from a (prior) base density $x_0 \sim p_0$ and a diffeomorphism $f_{\theta}: \R^{n \times 3} \to \R^{n \times 3}$ that maps the initial sample to $x_1 = f_{\theta}(x_0)$. We can obtain an expression for the log density of $x_1$ via the classical change of variables, %
\begin{equation}
    \log p_1(x_1) = \log p_0(x_0) - \log \det \left | \frac{\partial f_{\theta} (x_0)}{\partial x_0}\right |.
    \label{eqn:change_of_variables}
\end{equation}
\looseness=-1
In~\eqref{eqn:change_of_variables} above the $\log \det | \cdot |$ term corresponds to the Jacobian determinant of $f_{\theta}$ evaluated at $x_0$. 
Optimizing \eqref{eqn:change_of_variables} is the maximum likelihood objective for training normalizing flows and results in $f_{\theta}$ learning $p_1 \approx p_{\text{data}}$.
There are multiple ways to construct the (flow) map $f_{\theta}$. Perhaps the most popular approach is to consider the flow to be a composition of a finite number of elementary diffeomorphisms $f_{\theta} = f_M \circ f_{M-1} \dots \circ f_1$, resulting in the change in log density to be: $ \log p_1(x_1) = \log p_0(x_0) - \sum^M_{i=1} \log \left | \partial f_{i, \theta} (x_{i-1})/ \partial x_{i-1} \right |$. We note that the construction of each $f_{i, \theta},  i\in [M]$ is motivated such that both the inverse $f^{-1}_{i, \theta}(x)$ and Jacobian $\partial f_{i, \theta} (x) / \partial x $ are computationally cheap to compute.


\looseness=-1
\xhdr{Continuous Normalizing Flows}
In the limit of the infinite elementary diffeomorphisms, a normalizing transforms into a continuous normalizing flow (CNF)~\citep{chen_neural_2018}. Formally, a \emph{flow} is a one-parameter time-dependent diffeomorphism $\psi_{t}:[0,1] \times \R^{n \times 3} \to \R^{n \times 3}$ that is the solution to the following ordinary differential equation (ODE): $ \frac{d}{dt} \psi_{t}(x) = u_t \left(\psi_{t}(x) \right)$, with initial conditions $\psi_0(x_0) = x_0$, for a time-dependent vector field $u_t:[0,1]\times\R^{n \times 3}\to\R^{n \times 3}$. It is often desirable to construct the target flow by associating it to a designated \emph{probability path} $p_t: [0, 1] \times \sP(\R^{n \times 3}) \to \sP(\R^{n \times 3})$ which is a time-indexed interpolation in probability space between two distributions $p_0, p_1 \in \sP(\R^{n \times 3})$. 
In such cases, the flow $\psi_t$ is said to generate $p_t$ if it pushes forward $p_0$ to $p_1$ by following $u_t$---i.e. $p_t = [\psi_t]_\# (p_0)$.
As $\psi_t$ is a valid flow and satisfies an ODE the change in log density can be computed using the instantaneous change of variables:
\begin{equation}
    \log p (x_1) = \log p(x_0) - \int^1_0 \nabla \cdot u_t (x_t) dt,
    \label{eqn:instantaneous_change_of_variable}
\end{equation}
where $x_t = \psi_t(x_0)$ and $\nabla \cdot$ is the divergence operator. 


\looseness=-1
A CNF can then be viewed as a neural flow that seeks to learn a designated target flow $\psi_t$ for all time $t\in [0,1]$. 
The most scalable way to train CNFs is to employ a flow-matching learning framework~\citep{liu_rectified_2022,albergo_building_2023,lipman_flow_2022,tong_conditional_2023}. Specifically, flow-matching regresses a learnable vector field of a CNF $f_{t, \theta}(t, \cdot): [0,1] \times \R^{n \times 3} \to \R^{n \times 3}$ to the target vector field $u_t(x_t)$ associated to the flow $\psi_t$. In practice, it is considerably easier to regress against a target \emph{conditional} vector field $u_t (x_t | z)$---which generates the conditional probability path $p_t(x_t|z)$---as we do not have closed form access to the (marginal) vector field $u_t$ which generates $p_t$. The conditional flow-matching (CFM) objective can then be stated as a simple simulation-free regression,
\begin{equation}
\gL_{\rm CFM}(\theta) = \mathbb{E}_{t, q(z), p_t(x_t | z)} \|f_{t,\theta}(t, x_t) - u_t(x_t | z)\|_2^2.
\label{eqn:CFM}
\end{equation}
\looseness=-1
The conditioning distribution $q(z)$ can be chosen from any valid coupling, for instance, the independent coupling $q(z)= p(x_0) p(x_1)$. 
We highlight that~\eqref{eqn:CFM} allows for greater flexibility in $f_{t, \theta}$ as there is no exact invertibility constraint. To generate samples and their corresponding log density according to the CNF we may solve the following flow ODE numerically with initial conditions $x_0 = \psi_0(x_0)$ and $c = \log p_0 (x_0)$, which is the log density under the prior: 
\begin{equation}
    \frac{d}{dt} 
    \begin{bmatrix}
        \psi_{t, \theta}(x_t) \\
        \log p_t (x_t)
    \end{bmatrix} = 
    \begin{bmatrix}
        f_{t, \theta}(t, x_t) \\
        -\nabla \cdot f_{t, \theta}(t, x_t)
    \end{bmatrix}.
    \label{eqn:cnf_and_log_prob_ode}
\end{equation}

\subsection{Boltzmann generators}
\label{sec:boltzmann_generators}


\looseness=-1
A Boltzmann generator~\citep{noe2019boltzmann} $\mu_{\theta}$ pairs a normalizing flow as the proposal generative model $p_{\theta}$, which is then corrected to obtain i.i.d. samples under $\mu_{\text{target}}$ using importance sampling. More precisely, as normalizing flows are exact likelihood models, BG's first draw $K$ independent samples $x^i \sim p_{\theta}(x), i\in [K]$ and compute the corresponding importance weights for each sample $w(x^i) = \exp\left(\frac{-\gE(x^i)}{k_BT} \right)/ p_{\theta}(x^i)$. Leveraging the collection of importance weights we can compute a Monte-Carlo approximation to any test function $\phi(x)$ of interest under $\mu_{\text{target}}$ using self-normalized importance sampling as follows:
\begin{equation*}
     \mathbb{E}_{\mu_{\text{target}}(x)}[ \phi(x)] = \mathbb{E}_{p_\theta} [\phi(x) \bar{w}(x)] \approx \frac{\sum_{i=1}^K w(x^i) \phi(x^i)}{\sum_{i=1}^K w(x^i)}.
\end{equation*}
\looseness=-1
In addition, computing importance weights also enables resampling the pool of samples according to the collection of normalized importance weights $W = \{\bar{w}(x^i) \}_{i=1}^K$. 






\section{\namelong}
\label{sec:method}
\looseness=-1
We now present \nameshort which extends and improves over classical Boltzmann generators by adding a non-equilibrium sampling process that leads to higher-quality samples with reduced redundancy. We begin by identifying the key limitation in current BG's 
as importance sampling with a suboptimal proposal. Indeed, while the self-normalized importance sampling estimator is consistent, its' fidelity is highly dependent on the quality of the actual proposal $p_{\theta}$. In fact, the optimal proposal distribution is proportional to the minimizer of the variance of $\phi(x^i)\mu_{\text{target}}(x^i)$~\citep{mcbook}. Unfortunately, since $p_{\theta}$ within a BG framework is trained on a biased dataset $\gD$ the importance weights computed typically exhibit large variance---resulting in a small effective sample size (ESS).\footnote{ESS is defined as: $\text{ESS} = 1/ \sum^K_i (\bar{w}(x^i))^2$.} 


\looseness=-1
We address the need for more flexible proposals in~\S\ref{sec:scaling_training} with modernized scalable training recipes for normalizing flows. 
In~\S\ref{sec:jarzynski} we outline our novel application of non-equilibrium sampling with Sequential Monte Carlo \citep{doucet2001sequential} that powers our inference scaling algorithm to transport proposal samples and their importance weight towards the metastable states of $\gE(x)$.  We term the overall process of combining a pre-trained Boltzmann generator with inference scaling through annealing: \namelong. 


\looseness=-1
\xhdr{Symmetries of molecular systems}
The energy function $\gE(x)$ in a molecular system using classical force fields is known to be invariant under global rotations and translation\footnote{In classical force fields, in contrast to semi-empirical ones, the correct ordering of the topology is needed for energy evaluation.}, which corresponds to the group $\sethree \cong \sothree \ltimes (\R^3, +)$. Unfortunately, $\sethree$ is a non-compact group which does not allow for defining a prior density $p_0(x_0)$ on $\R^{n\times 3}$. Equivariant generative models circumvent this issue by defining a mean-free prior which is a projection of a Gaussian prior $\gN(0, I)$ onto the subspace $\R^{(n-1) \times 3}$~\citep{garcia2021n}. Thus pushing forward a mean free prior with an equivariant flow provably leads to an invariant proposal $p_1(x_1)$~\citep{kohler2020equivariant,bose2021equivariant}. We next build BG's by departing from exactly equivariant maps by instead considering soft-equivariance which opens up the usage of more scalable and efficient architectures.



\subsection{Scaling training of Boltzmann generators}
\label{sec:scaling_training}
\looseness=-1
To improve proposal flows in \nameshort we favor scalable architectural choices that are more expressive than exactly equivariant ones. We motivate this choice by highlighting that many classes of normalizing flow models are known to be universal density approximators~\citep{teshima2020coupling,lee2021universal}. Thus, expressive enough non-equivariant flows \emph{can learn to approximate any equivariant map}.


\looseness=-1
\xhdr{Soft equivariance}
We instantiate \nameshort with a state-of-the-art TarFlow~\citep{zhai2024normalizing} which is based on Blockwise Masked Autoregressive Flow~\citep{papamakarios2017masked} based on a causal Vision Transformer (ViT)~\citep{alexey2020image} modified for molecular systems where patches are over the particle dimension. Since the data comes mean-free we further normalize the data to a standard deviation of one. Combined, this allows us to scale both the depth and width of the models stably as there is no tension between a hard equivariance constraint and the invertibility of the network. 

\cut{
We now summarize the main architectural differences between prior BG's:
\begin{enumerate}[label=\textbf{(D\arabic*)},left=0pt,nosep]
\item Time encoding in CNF's using Fourier features.
\item Position encoding using RoPE.
\item Normalizing data to be mean-free with std one.
\end{enumerate}
\looseness=-1
While prior continuous BG's~\citep{klein2023equivariant,klein2023timewarp,klein2024transferable} choose to simply concatenate the time as a scalar input we encode time using Fourier features (\textbf{D1}). Improving over past BG's that omitted explicit positional information we utilize Rotary positional embeddings (RoPE)~\citep{su2024roformer} (\textbf{D2}). Paired with data normalization to an std of one (\textbf{D3}), our proposed modification allows us to scale both wider and deeper transformer architectures for molecular systems on Cartesian coordinates. For example, DiT based CNFs in \nameshort can be scaled to $\approx 15 \times$ the size of equivariant EGNN architectures found in prior BG's~\citep{klein2024transferable} with comparable throughput. 
}

\looseness=-1
We include a series of strategies to improve training of non-equivariant flows by softly enforcing $\sethree$-equivariance. First, we softly enforce equivariance to global rotations through data augmentation by sampling random rotations $R \in \sothree$ and applying them to data samples $R\circ x_1 \sim p_1(x_1)$. Secondly, as the data is mean-free and has $(n-1) \times d$ degrees of freedom we lift the data dimensionality back to $n$ by adding noise to the center of mass. This allows us to easily train with a non-equivariant prior distribution such as the standard normal $p_0 = \gN(0,I)$. The next proposition outlines the family of permissible noise.

\begin{mdframed}[style=MyFrame2]
\begin{restatable}{proposition}{propcom}
\label{prop:com}
\looseness=-1
Given an $\sethree$-invariant $\mu_{\text{target}}(x)$ and the noise-adjusted distribution $\mu'_{\text{target}}(x)$. Consider the decomposition of a data sample into its constituent mean-free component, $\tilde{x}$ and center of mass $c \in \R^3$,  $x = \tilde{x} + c$, where $c \sim \mu(c)$ and $\mu(c)$ is $\sothree$-invariant. Then $\mu_{\text{target}}(\tilde{x}) = \mu'_{\text{target}}(\tilde{x})$ if $\mu'_{\text{target}}(x) = \mu(\tilde{x})\mu (\|c\|)$.
\end{restatable}
\end{mdframed}
\looseness=-1
We prove~\cref{prop:com} in~\S\ref{app:propcom}, which tells us that any noise distribution that acts on the norm of the center of mass does not operationally change the target. As a result, we choose to add small amounts of Gaussian noise $c \sim \gN (0, \sigma)$ to the center of mass of a given data sample. The impact of this noise is that during reweighting we must account for $\mu(\| c\|)$ which follows a $\chi(3)$ distribution. Consequently, we must adjust the model energy to account for the impact of CoM noise during reweighting as follows: 
\begin{equation}
    \log p^c_{\theta}(x) = \log p_{\theta}(x) - \left(\log\left(\frac{\| c^2\|}{\sigma^3}\right) + \frac{\| c \|^2}{2\sigma^2} + C\right),
    \label{eqn:com_adjust}
\end{equation}
where $C =  - \log\left(\sqrt{2}\Gamma\left(\frac{3}{2}\right)\right)$ and $\Gamma$ is the gamma function. 



\subsection{Inference time scaling of Boltzmann generators}
\label{sec:jarzynski}
\looseness=-1
Given a trained BG with proposal flow $p_{\theta}$, the simple importance sampling estimator suffers from a large variance of importance weights as the dimensionality and complexity of $\mu_{\text{target}}(x)$ grows in large molecular systems. We aim to address this bottleneck by proposing an inference time scaling algorithm that anneals samples $x^i \sim p_{\theta}(x)$---and corresponding unnormalized importance weights $w(x^i)$---in a continuous manner towards $\mu_{\text{target}}$. %



\xhdr{Improving samples through non-equilibrium transport}
\looseness=-1
We leverage a class of methods that fall under non-equilibrium sampling to improve the base proposal flow samples. One of the simplest instantiations of this idea is to use annealed Langevin dynamics with reweighting through a continuous-time variant of Annealed Importance Sampling (AIS) \citep{neal2001annealed}. Concretely, we consider the following SDE that drives proposal samples towards metastable states of the Boltzmann target:
\begin{equation}
    \label{eqn:langevin_sde}
     d x_{\tau} = - \epsilon_{\tau} \nabla \E_{\tau}(x_{\tau }) d \tau  + \sqrt{2 \epsilon_{\tau}}dW_{\tau},
\end{equation}
\looseness=-1
where $\epsilon_{\tau} \geq 0$ is a time-dependent diffusion coefficient and $W_{\tau}$ is the standard Wiener process. We distinguish $\tau$, from $t$ used in the context of training $p_{\theta}$, as the time variable that evolves initial proposal samples at $\tau=0$ towards the target at $\tau=1$. The energy interpolation $\E_t$ is a design choice, and we opt for a simple linear interpolant $\E_t = (1-\tau) \E_0 + \tau \E_1$, and set $\gE_0(x) = -\log p_{\theta}(x)$. We highlight that unlike past work in pure sampling~\citep{mate2023learning,albergo_nets_2024} which use the prior energy $\E_0(x) = -\log p_0(x)$, our design affords the significantly more informative proposal given by the pre-trained normalizing flow $p_{\theta}$. As such, there is often no need for \emph{additional learning} during this step which we view as extending the inference capabilities of the original Boltzmann generator $\mu_{\theta}(x)$.

\looseness=-1
To compute test functions for the transported samples, and thus reweighting, we use a well-known and celebrated result known as Jarzynski's equality that enables the calculation of equilibrium statistics from non-equilibrium processes. We recall the main result, originally derived in \citet{jarzynski1997nonequilibirum}, and recently re-derived in continuous-time in the context of learning to sample 
by \citet{vargas2024transport,albergo_nets_2024} that makes explicit the time evolution of the new importance weights.

\begin{mdframed}[style=MyFrame2]
\begin{proposition}[\citet{albergo_nets_2024}]
\label{prop:nets}
Let $(x_{\tau}, w_{\tau})$ solve the coupled system of SDE / ODE
\begin{align*}
    d x_{\tau} &= - \epsilon_{\tau} \nabla \E_{\tau}(x_{\tau }) d \tau  + \sqrt{2 \epsilon_{\tau}}dW_{\tau} \\
    d \log w_{\tau} &= - \partial_{\tau} \gE_{\tau}(x_{\tau}) d \tau \quad \text{with } x_0 \sim p_{\theta},  w_0 = 0
\end{align*}
then for any test function $\phi: \mathbb{R}^d \to \mathbb{R}$ we have
\begin{equation}
    \int_{\mathbb{R}^d} \phi(x) p_{\tau}(x) dx = \frac{\mathbb{E} [ w_{\tau} \phi(x_{\tau})]}{\mathbb{E}[w_{\tau}]}
\end{equation}
and
\begin{equation}
    \gZ_{\tau}/ \gZ_1 = \mathbb{E}[e^{w_{\tau}}] \quad (\text{Jarzynski's Equality})
\end{equation}
\end{proposition}
\end{mdframed}
\looseness=-1
The final samples $x_{\tau=1}$ can then be reweighted according to final importance weights $w_{\tau=1}$ that have lower variance than simple importance sampling in conventional BGs. It is crucial to highlight that through inference-time scaling, we never need to utilize the high-variance importance weights under the prior $p_0(x_0)$, and instead the proposal $p_{\theta}(x_0)$ acts as a new prior for the annealing process. It is precisely this learned proposal distribution that $d\log w_{\tau}$ accounts for within the parlance of Annealed Importance Sampling. To evolve the Langevin SDE we require, 
\begin{equation*}
    \nabla \gE_{\tau}(x_{\tau}) = (1-\tau) \nabla(-\log p_{\theta}(x_{\tau})) + \tau \nabla \left(\frac{\E(x_{\tau})}{k_BT} \right),
\end{equation*}
\looseness=-1
which requires efficient gradient computation through the log-likelihood estimation under the normalizing flow $p_{\theta}$ as given by~\eqref{eqn:change_of_variables}. This presents the first point of distinction between finite flows and CNF's. The former class of flows trained using~\eqref{eqn:change_of_variables} gives fast exact likelihoods---especially for our scalable non-equivariant TarFlow model. In contrast, CNF's must simulate~\eqref{eqn:cnf_and_log_prob_ode} and differentiate through an ODE solver to compute $\nabla \log p_{\theta} (x_{\tau})$ for each step of the Langevin SDE in~\eqref{eqn:langevin_sde}.
As a result, a TarFlow proposal is considerably cheaper to simulate and reweight with AIS than a CNF. In~\S\ref{app:alternate_paths} we present an alternate interpolant that does not require the proposal distribution during sampling which is appealing when only samples are needed but at the cost of more expensive computation of log weights. These paths are of interest in the setting of Boltzmann emulators and other generative models and are of independent interest but are not considered further in the context of \nameshort.

\cut{
\looseness=-1
\xhdr{Proposal free Langevin dynamics}
We can also modify the Langevin SDE in~\eqref{eqn:langevin_sde} to include an additional drift term $\nu_{\tau}(x_{\tau}) \in \R^d$ as follows:
\begin{equation*}
    d x_{\tau} = - \epsilon_{\tau} \nabla \gE_t(x_{\tau }) d \tau + \nu_{\tau} ( x_{\tau}) d \tau + \sqrt{2 \epsilon_{\tau}}dW_{\tau}.
\end{equation*}
\looseness=-1
Under perfect drift $\nu_{\tau}(\tau)$ the log weights do not change and there is no need for correction. For imperfect drift the corresponding coupled ODE time-evolution of log-weights $d \log w_{\tau}$ needed to apply AIS was derived in NETS~\citep[Proposition 3]{albergo_nets_2024}:
\begin{equation*}
     d w_{\tau} = \nabla \cdot \nu_{\tau} ( x_{\tau}) d\tau - \nabla \E_{\tau}(x_{\tau}) \cdot \nu_{\tau}( x_{\tau}) d\tau - \partial_{\tau} \gE_{\tau}(x_{\tau}) d\tau.
\end{equation*}
In contrast to learning a drift as done in NETS~\citep{albergo_nets_2024} we now illustrate that a judicious choice of $\nu_{\tau}(x_{\tau})$ eliminates the need to compute the gradient of log-likelihood under the proposal. For instance, we can choose $\nu_{\tau}(x_{\tau}) = \epsilon_{\tau} \nabla \E_{\tau}(x_{\tau}) - \epsilon_{\tau} \nabla \left( \frac{\E (x_{\tau})}{k_BT}\right)$, which by straightforward calculation gives the following SDE:
\begin{align}
     d x_{\tau} & = - \epsilon_{\tau} \nabla \gE_t(x_{\tau }) d \tau + \nu_{\tau} ( x_{\tau}) d \tau + \sqrt{2 \epsilon_{\tau}}dW_{\tau} \nonumber \\
     &=  - \epsilon_{\tau} \nabla \left( \frac{\E (x_{\tau})}{k_BT}\right) d\tau  + \sqrt{2 \epsilon_{\tau}}dW_{\tau}.
     \label{eqn:proposal_free_SDE}
\end{align}
\looseness=-1
This new SDE greatly simplifies the simulation of samples $x_{\tau}$ as it is independent of the proposal energy $\nabla \gE_0(x_{\tau}) = -\nabla \log p_{\theta}(x_{\tau})$. However, the log weights ODE still requires the computation of the gradient of the proposal energy. The form of~\eqref{eqn:proposal_free_SDE} suggests the possibility of massively parallel simulation schemes under the TarFlow and but critically not the CNF where the computation of the log weights is remains expensive due to the need to compute the divergence operator in~\eqref{eqn:cnf_and_log_prob_ode}. Furthermore, while recent advances in divergence-free density estimation via the It$\hat{\text{o}}$ density estimator~\citep{skreta2024superposition,karczewski2024diffusion} might appear attractive we show that the log density under this estimator is necessarily biased and may limit the fidelity of self-normalized importance sampling incurs non-negotiable added bias. For ease of presentation, we present this theoretical investigation in appendix~\S\ref{app:ito_filtering} and characterize the added bias in~\cref{prop:weights_bias}. In totality, this limits the application of continuous BG's to only the conventional IS setting, unlike finite flows like TarFlow which can benefit from non-equilibrium transport and AIS.
}

\looseness=-1
To further enable a reduced computational footprint we propose a strategy that eliminates the forward evolution of the initial proposal that already obtain high energy. Specifically, we can simulate a large number of samples via~\eqref{eqn:proposal_free_SDE} and threshold using an energy threshold $\gamma >0$, and evaluate the log weights of promising samples. We justify our strategy by first remarking a lower bound to the log partition function of $\mu_{\text{target}}$ using a Monte Carlo estimate,
\begin{align}
    \log \gZ &= \log \sE_{x \sim p_{\theta}(x)} \left[ \frac{\exp\left(\frac{-\E(x)}{k_BT}\right)}{p_{\theta}(x)} \right] \nonumber \\
    & \geq \sE_{x \sim p_{\theta}(x)} \left[\frac{-\E(x)}{k_BT} -\log p_{\theta}(x) \right] = \log \hat{\gZ}.
    \label{eqn:logZhat}
\end{align}
Plugging this estimate in the definition of the target Boltzmann distribution we get an upper bound,
\begin{align*}
   \log \mu_{\text{target}}(x) 
                            & \leq  \log \left(\frac{-\E(x)}{k_BT} \right) - \log \hat{\gZ} .
\end{align*}
\looseness=-1
An upper bound on $\mu_{\text{target}}(x)$ allows us to threshold samples using the energy function, $\gE(x) > \gamma$, of the target. Formally, this corresponds to truncating the target distribution $\hat{\mu}_{\text{target}}(x):= \sP\left(\mu_{\text{target}}(x) \geq \frac{\gamma}{\log \hat{\gZ}}\right)$ which places zero mass on high energy conformations. Correcting flow samples with respect to this truncated target introduces an additional bias into the self-normalized importance sampling estimate, which precisely corresponds to the difference in total variation distance between the two distributions $\text{TV} (\hat{\mu}_{\text{target}}, \mu_{\text{target}})$. We prove this result using an intermediate result in \cref{lemma:tvd_truncation} included in~\cref{app:proofs}. 

\looseness=-1
Our next theoretical result provides a prescriptive strategy of setting an appropriate threshold $\gamma$ as a function of the number of samples $K$ and effective sample size under $\hat{\mu}_{\text{target}}(x)$.

\begin{mdframed}[style=MyFrame2]
\begin{restatable}{proposition}{propenergythreshold}
\label{prop:energybias}
\looseness=-1
Given an energy threshold $\gE(x) > \gamma$, for $\gamma > 0$ large and the resulting truncated target distribution $\hat{\mu}_{\text{target}}(x):= \sP\left( \mu_{\text{target}}(x) \geq \frac{\gamma}{\log \hat{\gZ}} \right)$. Further, assume that the density of unnormalized importance weights w.r.t.\ to $\hat{\mu}_{\text{target}}$ is square integrable $(\hat{w}(x))^2 < \infty$. Given a tolerance $\rho = 1/ \text{ESS}$ and bias of the original importance sampling estimator in total variation $b= \text{TV}(\mu_{\theta}, \mu_{\text{target}})$, then the $\gamma$-truncation threshold with $K$-samples for $\text{TV}(\mu_{\theta}, \hat{\mu}_{\text{target}})$ is:
\begin{equation}
    \gamma \geq \frac{1}{\lambda} \log \left( \frac{K b}{12 \rho \sE[\exp(-\lambda X)]} \right) + \log \hat{\gZ}.
\end{equation}
\end{restatable}
\end{mdframed}
\looseness=-1
The proof for~\cref{prop:energybias} is located in~\S\ref{app:energybias}. \Cref{prop:energybias} allows us to appropriately set a energy threshold $\gamma$ as a function of tolerance $\rho$ that depends on ESS. In practice, this allows us to negotiate the amount of acceptable bias when dropping initial samples that obtain high-energy before any further AIS correction. Moreover, this gives a firmer theoretical foundation to existing practices of thresholding high-energy samples~\citep{midgley2022flow,midgley2023se}.

\looseness=-1
Analogous to thresholding based on $\gE(x)$, we can also threshold by the probability under the proposal flow with truncation $\hat{p}_{\theta}(x) := \sP(p_{\theta}(x) \geq \delta)$, for small $\delta >0$. Essentially, this thresholding filters low probability samples under the model prior to any importance sampling. The additional bias incurred by performing such thresholding is theoretically analyzed in~\cref{prop:isbias} and presented in~\S\ref{app:isbias}. 

\begin{algorithm}[H]
\caption{\nameshort Sampling}
\label{alg:abgen}
\begin{algorithmic}[1]
\REQUIRE \# particles $K$, \# annealed distributions $N$, Energy annealing schedule $\mathcal{E}_\tau(x_{\tau})$
\STATE $x_0 \sim \mathcal{E}_0(x_0); \quad \Delta \gets 1 / N$
\FOR{$i = 1$ to $N$}
    \STATE $x_{\tau+\Delta} \gets x_\tau - \epsilon_\tau \nabla \mathcal{E}_\tau(x_\tau) d \tau + \sqrt{2 \epsilon_\tau} d W_\tau$
    \STATE $\log w_{\tau + \Delta} \gets \log w_\tau - \partial_\tau \mathcal{E}(x_\tau) d \tau$ \\
    \STATE $\tau \gets \tau + \Delta$
    \IF{$ \text{ESS} < \text{ESS}_\text{threshold}$}
        \STATE $x_\tau \gets \textsc{Resample}(x_\tau, w_\tau)$ 
        \STATE $w_\tau \gets 0$
    \ENDIF
\ENDFOR
\end{algorithmic}
\end{algorithm}
\vspace{-15pt}

\section{Experiments}
\label{sec:experiments}

\begin{figure*}[t!]
    \vspace{-8pt}
    \captionsetup[subfigure]{aboveskip=-1pt,belowskip=-1pt}
    \begin{subfigure}{0.19\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/generated_samples/al2_gen.png}
        \caption{\nameshort ALDP}
        \label{fig:gen_aldp}
    \end{subfigure}
     \begin{subfigure}{0.19\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/generated_samples/al3_gen.png}
        \caption{\nameshort AL3}
        \label{fig:al3_gen}
    \end{subfigure}
     \begin{subfigure}{0.19\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/generated_samples/al4_gen.png}
        \caption{\nameshort AL4}
        \label{fig:al4_gen}
    \end{subfigure}
    \begin{subfigure}{.19\linewidth}
    \centering
        \includegraphics[width=\linewidth]{figures/generated_samples/al6_gen.png}
        \caption{\nameshort AL6}
        \label{fig:al6_gen}
    \end{subfigure}
    \begin{subfigure}{.19\linewidth}
    \centering
        \includegraphics[width=\linewidth]{figures/generated_samples/chignolin.png}
        \caption{\nameshort Chignolin}
        \label{fig:al10_gen}
    \end{subfigure}
    \caption{ \small  Generated samples from \nameshort on molecular systems ranging from 2 to 10 amino acids.}
       
    \vspace{-10pt}
    \label{fig:generated_samples}
\end{figure*}

\begin{figure*}[t]
    \captionsetup[subfigure]{aboveskip=-1pt,belowskip=-1pt}
    \begin{subfigure}{0.24\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/eacf_ad2/clip_0.2/22_eacf_0.002_density.png}
        \caption{$\sethree$-EACF on AL2}
        \label{fig:ecnf_energy}
    \end{subfigure}
     \begin{subfigure}{0.24\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/ecnf/22_ecnf_density.png}
        \caption{ECNF on AL2}
        \label{fig:ecnf_improved_energy}
    \end{subfigure}
     \begin{subfigure}{0.24\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/ecnf/22_ecnf++_density.png}
        \caption{ECNF++ on AL2}
        \label{fig:eacf_02_energy}
    \end{subfigure}
    \begin{subfigure}{.24\linewidth}
    \centering
        \includegraphics[width=\linewidth]{figures/density_v2/22_SBG_density.png}
        \caption{\nameshort on AL2}
        \label{fig:al2_tarflow_energy}
    \end{subfigure}
    \caption{ \small  Energies of samples generated with different methods on alanine dipeptide (ALDP).}
       
    \label{fig:al2_energy_figs}
\end{figure*}


\looseness=-1
We evaluate \nameshort on small peptides using classical force-fields as the energy function with exact experimental setups described in~\S\ref{app:exp_details}. To generate samples and their corresponding weights we follow~\cref{alg:abgen} with resampling (lines 6-9) which is run on initial proposal samples and is equivalent to performing SMC~\citep{doucet2001sequential}. 


\looseness=-1
\xhdr{Datasets}
We consider small peptides composed of varying numbers of alanine amino acids, with some systems additionally incorporating an acetyl group and an N-methyl group. We investigate alanine systems of up to $6$ amino acids. 
All datasets are generated from a single MD simulation in implicit solvent using a classical force field. For each system, the first 100ps is used for training, the next 20ps for validation, and the remainder serves as the test set. Therefore, some metastable states may not be represented in the training set. An exception is alanine dipeptide, for which we use the dataset from~\citet{klein2024transferable}. 
In addition to the alanine systems, we also investigate the significantly larger protein Chignolin, consisting of $10$ amino acids generated with the Anton supercomputer in~\citet{lindorff2011fast}.
We provide additional dataset details in~\S\ref{app:dataset_details}.

\looseness=-1
\xhdr{Baselines}
For baselines, we train prior state-of-the-art equivariant Boltzmann generators. Specifically, we use $\sethree$-augmented coupling flow~\citep{midgley2023se} as the exactly invertible and equivariant architecture and the equivariant ECNF employed in Transferrable Boltzmann Generators~\citep{klein2024transferable}. We also include an improved equivariant CNF (ECNF++) as a stronger baseline, see~\S\ref{app:ecnf_implementation_details} for full details, which uses improved flow matching loss, improved data normalization, a larger network, improved learning rate schedule, and optimizer. 
We note that ECNF is equivariant to E(3) and hence generates samples of both global chiralities, which we resolve by applying a flip transformation as in \citet{klein2024transferable}, for further details see \S\ref{app:additional_results}.

\looseness=-1
\xhdr{Metrics}
We report interatomic distances as a normalized density between the ground truth data, 
and initial proposal samples, as well as the energy histogram of the system for ground truth, initial proposal samples, transported samples, and the reweighted energy histogram. We also include Ramachandran plots~\cite{ramachandran1963stereochemistry} for each molecular system studied that visualizes dihedral angles' distribution for the ground truth data distribution and the generated samples. We include additional quantitative metrics that provide a finer grained evaluation of each method. Concretely, we compute the ESS, Wasserstein-1 distance on the energy distribution, and the Wasserstein-2 distance of the dihedral angles used in the Ramachandran plot. 




\subsection{Results}
\label{sec:main_results}


\looseness=-1
We evaluate \nameshort and our chosen baselines on alanine dipeptide (ALDP), trialanine (AL3), alanine tetrapeptide (AL4), and hexaalanine (AL6) with quantitative metrics summarized in~\cref{tab:aldp_results} and~\cref{tab:main_results_wide} and generated samples in~\cref{fig:generated_samples}. 
In \nameshort @ 10k we generate 10k samples and directly report metrics on these samples. In SBG @100k we generate 100k samples and subsample to 10k after SMC to computl directly comparable metrics. For $\sethree$-EACF we retrain this baseline on our more challenging version of ALDP and observe that performance degrades substantially at the selected $0.2\%$ weight clipping threshold (c.f.~\S\ref{app:additional_results} for higher clipping thresholds). Furthermore,  we find that on ALDP, our improved ECNF++ baseline obtains a $177\%$ relative improvement in ESS over the previous SOTA ECNF from~\citet{klein2024transferable}. Importantly, we observe \nameshort is the best method on the Wasserstein-1 energy distance $\gE$-$\gW_1$ and Wasserstein-2 distance on dihedreal angles $\sT$-$\gW_2$. As \nameshort involves resamples on a finite set of points, we observe that the higher number of particles (100k) results in consistently improved $\gE$-$\gW_1$ and $\sT$-$\gW_2$. These results are further substantiated in~\cref{fig:al2_energy_figs} which depicts the energy histograms of \nameshort in relation to the ground truth energy of the system and depicts a near perfect overlap.

\looseness=-1
For tripeptides, tetrapeptides, and hexapeptides we remark that the $\sethree$-EACF baseline is too computationally expensive and thus does not scale (c.f.~\cref{tab:training_times}). Consequently, we report metrics for our improved ECNF, ECNF++, and \nameshort. We observe that ECNF fails to learn effectively on the tri and tetrapeptides with $\gE$-$\gW_1$ exploding over $10^{4}$, while our improved ECNF++ is orders of magnitude better.
We highlight that \nameshort is the best method across all metrics, and in particular, we highlight that the improvements are more prominently driven by inference time scaling of proposal samples as observed in~\cref{fig:al3_energy_figs},~\cref{fig:al4_energy_figs}, and~\cref{fig:al6_energy_figs}. As reweighted samples under \nameshort show extremely high overlap with the ground truth $\mu_{\text{target}}(x)$, we argue that \nameshort successfully solves these molecular systems in comparison to prior BG's. We also report in~\S\ref{app:rama_plots} the Ramachandran plots for each method. Finally, we include additional ablations such as the utility of CoM energy adjustment in~\cref{app:additional_results}.




\looseness=-1
\xhdr{Inference scaling}
To illustrate the scalability of \nameshort in relation to other methods we plot in~\cref{fig:computational_cost_in_hours} the log-scale inference time for each dataset. In particular, for inference, we include the time to generate and reweight $10k$ samples. We observe an almost exponential scaling of ECNF as the size of the peptide grows, while \nameshort is dramatically faster at inference. We also plot $\sT$-$\gW_2$ metric on AL3 as a function of Langevin timestep granularity which shows a monotonic decrease as the number of samples increase for the center of mass adjusted energy, demonstrating a dimension for inference-time scaling of \nameshort not present in standard Boltzmann generators. Furthermore, the standard energy function does not give monotonic improvement with increasing granularity, demonstrating the improvement achieved by the center of mass adjusted energy.

\begin{figure}[t]
\begin{subfigure}{0.49\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/Inference_time.png}
\end{subfigure}
\begin{subfigure}{0.49\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/Torus_wasserstein_vs_timesteps_main.png}
\end{subfigure}

 \caption{\small \textbf{Left:} Time in hours for sampling and reweighing 10k points. \textbf{Right:} $\sT$-$\gW_2$ on AL3 as a function of Langvein timestep granularity for both standard and center of mass adjusted proposal energy functions.} 

 \label{fig:computational_cost_in_hours}
\end{figure}

\begin{table}[ht!]
\caption{\small Quantitative results on ALDP. $^*$Indicates resampled ESS.}
\label{tab:aldp_results}
\resizebox{1\linewidth}{!}{
\begin{tabular}{@{}lccccccc}
    \toprule
    Datasets $\rightarrow$ & \multicolumn3c{Alanine dipeptide (ALDP)}   \\
    \cmidrule(lr){2-4}
    Algorithm $\downarrow$ & ESS $\uparrow$ & $\gE$-$\gW_1$ $\downarrow$ & $\sT$-$\gW_2$ $\downarrow$   \\
    \midrule

    $\sethree$-EACF
    & $<10^{-4}$
    & 3.31
    & 2.92 \\

    ECNF
    & 0.084 & 0.984 & 0.391    \\
    ECNF++ (Ours) & 0.260 $\pm$ 0.043 & 0.796 $\pm$ 0.081 & 0.293 $\pm$ 0.017 \\
    \midrule 

\nameshort (Ours) @10k &  0.897$^*$ $\pm$ 0.167 & 0.536 $\pm$ 0.367 & 0.502 $\pm$ 0.108 \\


\nameshort (Ours) @100k & 0.894$^*$ $\pm$ 0.172 & \textbf{0.466 $\pm$ 0.380} & \textbf{0.226 $\pm$ 0.029} \\



    \bottomrule
    \end{tabular}
}
\vspace{-15pt}
\end{table}

\begin{table*}[ht!]
\caption{\small Quantitative results on trialanine, alanine tetrapeptide, and hexapeptide. $^*$Indicates ESS after resampling. }
\label{tab:main_results_wide}
\resizebox{1\linewidth}{!}{
\begin{tabular}{@{}lccccccccccccccc}
    \toprule
    Datasets $\rightarrow$ & \multicolumn3c{Tripeptide (AL3)} & \multicolumn3c{Tetrapeptide (AL4) } & \multicolumn3c{Hexapeptide (AL6)}  \\
    \cmidrule(lr){2-4}\cmidrule(lr){5-7}\cmidrule(lr){8-10}\cmidrule(lr){11-13}
    Algorithm $\downarrow$ & ESS $\uparrow$ & $\gE$-$\gW_1$ $\downarrow$ & $\sT$-$\gW_2$ $\downarrow$ & ESS $\uparrow$& $\gE$-$\gW_1$ $\downarrow$ & $\sT$-$\gW_2$ $\downarrow$ & ESS $\uparrow$ & $\gE$-$\gW_1$$\downarrow$ & $\sT$-$\gW_2$ $\downarrow$ \\
    \midrule
    ECNF & $<10^{-4}$ & $ > 10^4$ & 4.241 &  $<10^{-4}$ &  $ >10^{4}$  & 4.902& - & - & - \\
    ECNF++ (Ours)   
                & 0.036 $\pm$ 0.027 & 1.759 $\pm$ 0.788 & 1.177 $\pm$ 0.145
                & 0.123 $\pm$ 0.006 & 4.229 $\pm$ 1.284 & 2.082 $\pm$ 0.005
                & 0.015 \comment{$\pm$ 0.003} & 8.954 \comment{$\pm$ 0.646} & 4.315 \comment{$\pm$ 0.018}
                \\




    \midrule 




\nameshort (Ours) @10k &  0.876$^*$ $\pm$ 0.215 & 1.456 $\pm$ 0.739 & 1.303 $\pm$ 0.039 & 0.901$^*$ $\pm$ 0.059 & \textbf{1.517 $\pm$ 0.387} & 2.080 $\pm$ 0.028 & 0.995$^*$ & 2.321 & 3.697 \\



\nameshort (Ours) @100k &  0.907$^*$ $\pm$ 0.151 & \textbf{1.314 $\pm$ 0.409} & \textbf{0.948 $\pm$ 0.068} & 0.886$^*$ $\pm$ 0.087 & 1.920 $\pm$ 0.251 & \textbf{1.841 $\pm$ 0.047} & 0.989$^*$ & \textbf{0.305} & \textbf{3.326} \\




    \bottomrule
    \end{tabular}
}
\vspace{-5pt}
\end{table*}

\begin{figure}[t]
    \captionsetup[subfigure]{aboveskip=-1pt,belowskip=-1pt}
     \begin{subfigure}{0.49\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/ecnf/33_ecnf++_density.png}
        \caption{ECNF++ on AL3}
        \label{fig:al3_ecnf_energy}
    \end{subfigure}
    \begin{subfigure}{.49\linewidth}
    \centering
         \includegraphics[width=\linewidth]{figures/density_v2/33_SBG_density.png}
        \caption{\nameshort on AL3}
        \label{fig:al3_tarflow_energy}
    \end{subfigure}
    \vspace{-10pt}
    \caption{ \small  Energy plots for trialanine (AL3).}
    \label{fig:al3_energy_figs}
\end{figure}


\begin{figure}[t]
    \captionsetup[subfigure]{aboveskip=-1pt,belowskip=-1pt}
     \begin{subfigure}{0.49\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/ecnf/42_ecnf++_density.png}
        \caption{ECNF++ on AL4}
        \label{fig:al4_ecnf_energy}
    \end{subfigure}
    \begin{subfigure}{.49\linewidth}
    \centering
          \includegraphics[width=\linewidth]{figures/density_v2/42_SBG_density.png}
        \caption{\nameshort on AL4}
        \label{fig:al4_tarflow_energy}
    \end{subfigure}
    \vspace{-10pt}
    \caption{ \small  Energy plots for tetrapeptide (AL4).}
    \vspace{-10pt}
    \label{fig:al4_energy_figs}
\end{figure}

\begin{figure}[t]
    \captionsetup[subfigure]{aboveskip=-1pt,belowskip=-1pt}
     \begin{subfigure}{0.49\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/ecnf/63_ecnf++_density.png}
        \caption{ECNF++ on AL6}
        \label{fig:al6_ecnf_energy}
    \end{subfigure}
    \begin{subfigure}{.49\linewidth}
    \centering
          \includegraphics[width=\linewidth]{figures/density_v2/63_SBG_density.png}
        \caption{\nameshort on AL6}
        \label{fig:al6_tarflow_energy}
    \end{subfigure}
    \vspace{-10pt}
    \caption{ \small  Energy plots for hexapeptide (AL6).}
       
    \label{fig:al6_energy_figs}
\end{figure}





\subsection{Scaling to decapeptides}
\label{sec:decapeptides}
\looseness=-1
We now apply \nameshort to a decapeptide in Chignolin. As no other baseline can scale to this molecule we report energy histograms and distance plots for \nameshort only in~\cref{fig:al10_energy_and_distance_figs}. We observe success of \nameshort at matching the interatomic distance distribution. We additionally observe imperfect but reasonable proposal samples under the energy distribution whose energies are greatly improved after reweighting. Evidently the equilibrium distribution is not perfectly sampled as evidenced by the lack of density at the lowest energy states and large peaks in the reweighted distributions. Nevertheless, our application of \nameshort to Chignolin represents a significant step towards the scalability of BG's that previously struggled on even tripeptides like AL3 as found in the ECNF $\E$-$\gW_1$ result in~\cref{tab:main_results_wide}.

\begin{figure}[!htb]
    \captionsetup[subfigure]{aboveskip=-1pt,belowskip=-1pt}
     \begin{subfigure}{0.49\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/chignolin_distances.png}
        \vspace{0mm}
        \caption{\small Interatomic Distance}
        \label{fig:al10_ecnf_energy}
    \end{subfigure}
    \begin{subfigure}{.49\linewidth}
    \centering
        \includegraphics[width=\linewidth]{figures/chignolin_energies.png}
          \vspace{0mm}
        \caption{\small Energy histogram}
        \label{fig:al10_tarflow_energy}
    \end{subfigure}
    \vspace{-10pt}
    \caption{ \small Interatomic distance and energy plots for Chignolin.}
    \label{fig:al10_energy_and_distance_figs}
\end{figure}









\section{Related work}
\label{sec:related_work}
\looseness=-1
Boltzmann generators (BGs) \citep{noe2019boltzmann} have been applied to both free energy estimation \citep{wirnsberger2020targeted, rizzi2023multimap, schebek2024efficient} and molecular sampling. Initially, BGs relied on system-specific representations, such as internal coordinates, to achieve relevant sampling efficiencies \citep{noe2019boltzmann, kohler2021smooth, midgley2022flow, kohler2023rigid, dibak2021temperature}. However, these representations are generally not transferable across different systems, leading to the development of BGs in Cartesian coordinates \citep{klein2023equivariant, midgley2023se,klein2024transferable}. While this improves transferability, they are currently limited in scalability, struggling to extend beyond dipeptides. Scaling to larger systems typically requires sacrificing exact sampling from the target distribution  \citep{jing2022torsional,abdin2023pepflow,jing2024alphafold,lewis2024scalable}, which often includes coarse-graining. 
An alternative to direct sampling from $\mu_{\text{target}}(x)$ is to generate samples iteratively by learning large steps in time \citep{schreiner2023implicit, fu2023simulate, klein2023timewarp, diez2024boltzmann, jing2024generative, daigavane2024jamun}.


\section{Conclusion}
\label{sec:conclusion}
\looseness=-1
In this paper, we introduce \nameshort an extension to the Boltzmann generator framework that scales inference through the use of non-equilibrium transport. Unlike past BG's in \nameshort, we scale training using a non-equivariant transformer-based TarFlow architecture with soft equivariance penalties to $6$ peptides. In terms of limitations, using non-equilibrium sampling as presented in \nameshort does not enjoy easy application to CNFs due to expensive simulation, which limits the use of modern flow matching methods in a \nameshort context. Considering hybrid approaches that mix CNFs through distillation to an invertible architecture or consistency-based objectives is thus a natural direction for future work. Finally, considering other classes of scalable generative models such as autoregressive ones which also permit exact likelihoods is also a ripe direction orf future work.



\section{Impact Statement}
\label{sec:impact_statement}

\looseness=-1
This work studies amortized sampling from Boltzmann densities, a problem of general interest in machine learning and AI4Science that
arises both in pure statistical modeling and in applications. We highlight the training Boltzmann generators on molecular tasks are in turn applicable to drug and material discovery. While we do not foresee immediate negative impacts of our advances in this area,
we encourage due caution when scaling up to prevent their potential misuse.

\input{acknowledgements}

\bibliography{clean}
\bibliographystyle{style/icml2025}

\appendix
\onecolumn

\section{Alternate Paths}
\label{app:alternate_paths}

\subsection{Proposal free Langevin dynamics}
\looseness=-1
We can also modify the Langevin SDE in~\eqref{eqn:langevin_sde} to include an additional drift term $\nu_{\tau}(x_{\tau}) \in \R^d$ as follows:
\begin{equation*}
    d x_{\tau} = - \epsilon_{\tau} \nabla \gE_t(x_{\tau }) d \tau + \nu_{\tau} ( x_{\tau}) d \tau + \sqrt{2 \epsilon_{\tau}}dW_{\tau}.
\end{equation*}
\looseness=-1
Under perfect drift $\nu_{\tau}(\tau)$ the log weights do not change and there is no need for correction. For imperfect drift the corresponding coupled ODE time-evolution of log-weights $d \log w_{\tau}$ needed to apply AIS was derived in NETS~\citep[Proposition 3]{albergo_nets_2024}:
\begin{equation*}
     d w_{\tau} = \nabla \cdot \nu_{\tau} ( x_{\tau}) d\tau - \nabla \E_{\tau}(x_{\tau}) \cdot \nu_{\tau}( x_{\tau}) d\tau - \partial_{\tau} \gE_{\tau}(x_{\tau}) d\tau.
\end{equation*}
In contrast to learning a drift as done in NETS~\citep{albergo_nets_2024} we now illustrate that a judicious choice of $\nu_{\tau}(x_{\tau})$ eliminates the need to compute the gradient of log-likelihood under the proposal. For instance, we can choose $\nu_{\tau}(x_{\tau}) = \epsilon_{\tau} \nabla \E_{\tau}(x_{\tau}) - \epsilon_{\tau} \nabla \left( \frac{\E (x_{\tau})}{k_BT}\right)$, which by straightforward calculation gives the following SDE:
\begin{align}
     d x_{\tau} & = - \epsilon_{\tau} \nabla \gE_t(x_{\tau }) d \tau + \nu_{\tau} ( x_{\tau}) d \tau + \sqrt{2 \epsilon_{\tau}}dW_{\tau} \nonumber \\
     &=  - \epsilon_{\tau} \nabla \left( \frac{\E (x_{\tau})}{k_BT}\right) d\tau  + \sqrt{2 \epsilon_{\tau}}dW_{\tau}.
     \label{eqn:proposal_free_SDE}
\end{align}
\looseness=-1
This new SDE greatly simplifies the simulation of samples $x_{\tau}$ as it is independent of the proposal energy $\nabla \gE_0(x_{\tau}) = -\nabla \log p_{\theta}(x_{\tau})$. However, the log weights ODE still requires the computation of the gradient of the proposal energy. The form of~\eqref{eqn:proposal_free_SDE} suggests the possibility of massively parallel simulation schemes under a regular normalizing flow and a CNF. However, due to simulatio the log weights remains expensive for CNFs due to the need to compute the divergence operator in~\eqref{eqn:cnf_and_log_prob_ode}. Furthermore, while recent advances in divergence-free density estimation via the It$\hat{\text{o}}$ density estimator~\citep{skreta2024superposition,karczewski2024diffusion} might appear attractive we show that the log density under this estimator is necessarily biased and may limit the fidelity of self-normalized importance sampling incurs non-negotiable added bias. For ease of presentation, we present this theoretical investigation in appendix~\S\ref{app:ito_filtering} and characterize the added bias in~\cref{prop:weights_bias}. In totality, this limits the application of continuous BG's to only the conventional IS setting, unlike finite flows like TarFlow which can benefit from non-equilibrium transport and AIS.

\section{Proofs}
\label{app:proofs}

\subsection{Proof of~\cref{prop:com}}
\label{app:propcom}

\begin{mdframed}[style=MyFrame2]
\propcom*
\end{mdframed}

\begin{proof}
We start by noting $x = \tilde{x} + c$ and thus we can construct the target as a marginalization over $c$
\begin{align}
    \mu_{\text{target}}(\tilde{x}) =  \int \mu_{\text{target}}(\tilde{x}, c) dc = \int  \mu_{\text{target}}(\tilde{x} | c) \mu(c) dc 
\end{align}

Now select $\mu(c) = \mu(\| c\|) \mu(\phi) \mu(\psi)$ which gives:
\begin{equation}
    \int  \mu_{\text{target}}(\tilde{x} | c) \mu(c) dc  = \int  \mu_{\text{target}}(\tilde{x} | c)\mu(\| c\|) \mu(\phi) \mu(\psi) dc. 
\end{equation}
\looseness=-1
But the target distribution is $\sethree$-invariant and thus this results in the following result,
\begin{equation}
    \int  \mu_{\text{target}}(\tilde{x} | c) \mu(\|c \|) dc =  \int  \mu_{\text{target}}(\tilde{x}) \mu(\|c \|) dc = \mu'_{\text{target}}(x).
\end{equation}
\end{proof}

\subsection{Proof of~\cref{lemma:tvd_truncation}}
\label{app:lemma_truncation}

We first prove a useful lemma that computes the total variation distance between the original distribution of the normalizing flow $p_{\theta}$ and the truncated distribution $\hat{p}_{\theta}$ before proving the propositions.

\begin{mdframed}[style=MyFrame2]
\begin{lemma}
Let $p_{\theta}$ be a generative and denote $\hat{p}_{\theta}(x)$ the $\delta$-truncated distribution such that $\hat{p}_{\theta}(x): = \sP( p_{\theta}(x) \geq \delta)$, for a small $\delta > 0$. Define the constant $\beta  = \sP(p_{\theta}(x) < \delta)$ as the event where the truncation occurs. Then the total variation distance between the generative model and its truncated distribution is $ \text{TV}(p_{\theta}, \hat{p}_{\theta}) = \beta$.
\label{lemma:tvd_truncation}
\end{lemma}
\end{mdframed}


\begin{proof}
We begin by first characterizing the total variation distance between flow after correction with importance sampling $p(x)$ with truncated distribution $\hat{p}(x)$. Recall that the truncated distribution is defined as follows:

\begin{equation}
    \hat{p}(x): = \sP( p(x) \geq \delta)  = \frac{p(x) \mathbb{I}\{ p(x) \geq \delta\}}{\int  \mathbb{I}\{ p(x) \geq \delta\} p(x) dx},
\end{equation}
where $\mathbb{I}$ is the indicator function. Denote the events $\alpha = \sP(X \geq \delta)$ and $\beta = \sP(X < \delta)$ for the random variance $X \sim p(x)$. Clearly, $\alpha + \beta =1$ and $\alpha =\int  \mathbb{I}\{ \mu(x) \geq \delta\} p(x) dx $. Now consider the total variation distance between these two distributions:

\begin{equation}
    \text{TV}(p, \hat{p}) =  \sup_{\phi \in \Phi } \left| \sE_{x \sim p(x)}[\phi(x) ]- \sE_{\hat{x} \sim \hat{p}(x)} [\phi(\hat{x})]  \right| = \frac{1}{2} \int | p(x) - \hat{p}(x)| dx.
\end{equation}
\looseness=-1
where $\Phi = \{ \phi: \| \phi\|_{\infty} \leq 1\}$.
Next we break up the event space into two regions $R_1$ and $R_2$ which correspond to the events $p(x) < \delta$ and $p(x) \geq \delta$ respectively. Now consider the total variation distance in the region $R_1$ whereby construction $\hat{p}(x) = 0$, 
\begin{equation}
 \frac{1}{2} \int_{R_1} | p(x) - \hat{p}(x)| dx = \frac{1}{2}\int_{R_1} p(x) dx = \frac{\beta}{2}.
\end{equation}
A similar computation on $R_2$ gives,
\begin{equation}
 \frac{1}{2} \int_{R_2} | p_{\theta}(x) - \hat{p}_{\theta}(x)| dx =   \frac{1}{2} \int_{R_2} \left| p(x) - \frac{p(x)}{\alpha} \right| dx = \frac{1}{2}\int_{R_2} p(x) \left | 
 1 - \frac{1}{\alpha}\right| dx = \frac{\alpha (\frac{1}{\alpha} -1)}{2} = \frac{\beta}{2},
\end{equation}
where we exploited the fact that $\hat{p_{\theta}}(x) = \frac{p_{\theta}(x)}{\alpha}$ in the first equality and that $\alpha = \int_{R_2} p_{\theta}(x) dx$ in the second equality. Combining these results we get the full total variation distance:
\begin{equation}
     \text{TV}(p, \hat{p}) =  \frac{1}{2} \int | p(x) - \hat{p}(x)| dx = \frac{1}{2} \int_{R_1} | p(x) - \hat{p}(x)| dx + \frac{1}{2} \int_{R_2} | p(x) - \hat{p}(x)| dx = \beta.
\end{equation}
Thus the $\text{TV}(p, \hat{p}) = \beta$ and $0$ in the trivial case where $\alpha = 1$ and the truncated distribution are the same.
\end{proof}

\subsection{Proof of~\cref{prop:energybias}}
\label{app:energybias}

\begin{mdframed}[style=MyFrame2]
\propenergythreshold*
\end{mdframed}

\begin{proof}
We start by recalling a well-known result stating the bias of self-normalized importance sampling found in~\citet[Theorem 2.1]{agapiou2017importance} using $K$ samples from the proposal $\mu(x)$:
\begin{equation}
    \sup_{\| \phi \|_{\infty} \leq 1} \left| \sE \left[ \mu_{\theta}^K (\phi) -\mu_{\text{target}} (\phi)  \right] \right| \leq \frac{12 \rho}{K}, \quad \rho \approx \frac{K}{\text{ESS}} = \frac{K\sum^K_jw(x^j)^2 }{\left(\sum_i^K w(x^i)\right)^2} 
\end{equation} 
where the terms $\mu_{\theta}^K(\phi) = \sum_i^K \bar{w}(x^i) \phi(x^i)$ is the self-normalized importance estimator of $\mu_{\text{target}}$ with samples drawn according to $x^i \sim p_{\theta}(x)$ and $\|\phi(x)\| \leq 1 $ is a bounded test function.

\looseness=-1
By truncating using an energy threshold $\E(x) < \gamma$, for a large $\gamma > 0$, we truncate the support of $\mu_{\text{target}}(x)$ by cutting off low probability regions that constitute high-energy configurations. More precisely, we have $\hat{\mu}_{\text{target}}:= \sP\left(\mu_{\text{target}}(x) \geq \frac{\gamma}{\log \hat{\gZ}}\right)$, where $\log \hat{\gZ}$ is as defined in~\eqref{eqn:logZhat}. Note that $\hat{\mu}_{\text{target}}(x)$ is absolutely continuous w.r.t. to $\mu_{\text{target}}$ as the support is contained up to modulo measure zero sets. The importance sampling error incurred by using $\hat{\mu}_{\text{target}}$ can be bounded as follows:
\begin{align}
    \sup_{\| \phi \|_{\infty} \leq 1} \left| \sE \left[ \mu_{\theta}^K (\phi) -\mu_{\text{target}} (\phi)  \right] \right| & \leq   \sup_{\| \phi \|_{\infty} \leq 1} \left| \sE \left[ \mu_{\theta}^K (\phi) -\hat{\mu}_{\text{target}} (\phi)  \right]  \right| +  \sup_{\| \phi \|_{\infty} \leq 1} \left| \sE \left[ \hat{\mu}_{\text{target}}(\phi) - \mu_{\text{target}}(\phi)\right] \right| \\
    & \leq \frac{12 \hat{\rho}}{K} + \beta_1 \\
    & \leq \frac{12 \rho}{K} + \beta_1.
\end{align}
\looseness=-1
The first inequality follows from the triangle inequality. Here we note that $\hat{\rho}$ is the ESS which corresponds to using importance weights computed with respect to the truncated target $\hat{\mu}_{\text{target}}$ rather than $\mu_{\text{target}}$. The constant $\beta_1 = \text{TV}(\hat{\mu}_{\text{target}}, \mu_{\text{target}})$ and follows from an application of \cref{lemma:tvd_truncation}. Further, note that $\rho \geq \hat{\rho}$ since ESS must increase---and thereby $\hat{\rho}$ decreases---as the distributional overlap between the two distributions decreases.
Now observe, $\beta_1 = \sP\left(X < \frac{\gamma}{\log \hat{\gZ}}\right)$, where samples follow the law $X \sim \hat{\mu}_{\text{target}}(x)$. Then a 
 direct application of Chernoff's inequality gives us $\sP\left(X < \frac{\gamma}{\log \hat{\gZ}}\right) = \beta_1 \leq \exp\left(\frac{\lambda \gamma}{\log \hat{\gZ}}\right) \sE[\exp\left(-\lambda X\right)]$. Thus the additional bias incurred is, 
\begin{equation}
    \sup_{\| \phi \|_{\infty} \leq 1} \left| \sE \left[ \hat{\mu}_{\theta}^K (\phi) -\mu_{\text{target}} (\phi)  \right] \right| \leq \frac{12 \rho}{K} + \beta_1 \leq \frac{12 \rho}{K} +  \exp\left(\frac{\lambda \gamma}{\log \hat{\gZ}}\right) \sE[\exp(-\lambda X)].
\end{equation}
Where the term $\sE[\exp(-\lambda X)]$ is the moment generating function. Setting $b:= \text{TV}(\mu^K_{\theta}, \mu_{\text{target}})$, then we have
\begin{equation}
   \gamma \geq \frac{1}{\lambda} \log \left( \frac{K b}{12 \rho \sE[\exp(-\lambda X)]} \right) + \log \hat{\gZ}.
\end{equation}
\end{proof}

\subsection{Proof of \cref{prop:isbias}}
\label{app:isbias}


\begin{mdframed}[style=MyFrame2]
\begin{restatable}{proposition}{propbias}
\label{prop:isbias}
\looseness=-1
Assume that the density of the model $p_{\theta}$ after importance sampling $\mu_{\theta}$ is absolutely continuous with respect to the target $\mu_{\text{target}}$. Further, assume that the density of unnormalized importance weights is square integrable $(w(x))^2 < \infty$. Given a tolerance $\rho = 1/ \text{ESS}$ of the original importance sampling estimator under $\mu_{\theta}$ and bias of the importance sampling estimator in total variation $b= \text{TV}(\mu_{\theta}, \mu_{\text{target}})$, then the $\delta$-truncation for the truncated distribution $\hat{p}_{\theta}(x) := \sP(p_{\theta}(x) \geq \delta)$ threshold with $K$-samples is:
\begin{equation}
    \delta \geq \frac{1}{\lambda} \log \left( \frac{K b}{12 \rho \sE[\exp(-\lambda X)]} \right).
\end{equation}
\end{restatable}
\end{mdframed}
\begin{proof}
\looseness=-1
We aim to bound the total variation distance $\text{TV}( \hat{\mu}_{\theta}^K, \mu_{\text{target}})$ of using the truncated distribution $\sP(p_{\theta}(x) > \delta)$ by again recalling the bias of self-normalized importance sampling using $K$ samples from $\mu_{\theta}(x)$:
\begin{equation}
    \sup_{\| \phi \|_{\infty} \leq 1} \left| \sE \left[ \mu_{\theta}^K (\phi) -\mu_{\text{target}} (\phi)  \right] \right| \leq \frac{12 \rho}{K}, \quad \rho \approx \frac{K}{\text{ESS}} = \frac{K\sum^K_jw(x^j)^2 }{\left(\sum_i^K w(x^i)\right)^2} 
\end{equation} 

where the terms $\mu_{\theta}^K(\phi) = \sum_i^K \bar{w}(x^i) \phi(x^i)$ is the self-normalized importance estimator of $\mu_{\text{target}}$ with samples drawn according to $x^i \sim p_{\theta}(x)$ and $\|\phi(x)\| \leq 1 $ is a bounded test function. We next characterize the error introduced by using the truncated distribution $\hat{p}_{\theta}$ for importance sampling in place of $p_{\theta}$ by first defining the truncated $K$-sample self-normalized importance estimator $\hat{\mu}_{\theta}^K (\phi)= \sum^K_j \bar{w}(x^j)\phi(x^j)$, where $x^j \sim \hat{p}_{\theta}(x)$. Specifically, we bound the total variation distance:
\begin{align}
    \text{TV}(\mu_{\theta}, \hat{\mu}_{\theta}) &=  \sup_{\| \phi \|_{\infty} \leq 1} \left| \sE \left[ \mu_{\theta}^K (\phi) - \hat{\mu}_{\theta}^K (\phi)  \right] \right| \\
    &=   \sup_{\| \phi \|_{\infty} \leq 1} \left| \sE_{x^i \sim p_{\theta}} \left[ \sum^K_{i=1}\bar{w}(x^i) \phi(x^i) \right]  - \sE_{x^j \sim \hat{p}_{\theta}} \left[\sum^K_{j=1} \bar{w}(x^j)\phi(x^j)  \right] \right| \\
   & = \frac{1}{2}\left( \sE_{x^i \sim p_{\theta}} \left[\sum^K_{i=1} \bar{w}(x^i) \right]  - \sE_{x^j \sim \hat{p}_{\theta}} \left[ \sum^K_{j=1} \bar{w}(x^j)  \right] \right)
\end{align}
Here in the second equality, we used the fact that the test function is bounded $|| \phi \||_{\infty} \leq 1$ 
Next, we apply~\cref{lemma:tvd_truncation} and leverage the fact that the self-normalized weights are also bounded and achieve a bound on the total variation distance,
\begin{align}
    \text{TV}(\mu, \hat{\mu}) & =  \frac{1}{2}\left( \sE_{x^i \sim p_{\theta}} \left[ \sum^K_{i=1} \bar{w}(x^i) \right]  - \sE_{x^j \sim \hat{p}_{\theta}} \left[\sum^K_{j=1} \bar{w}(x^j)  \right] \right) \\
    & = \beta_2,
\end{align}

where $\beta_2$ is the probability mass $\sP(X< \delta)$ when $X \sim p_{\theta}(x)$. Like previously, the overall error can be bounded using the triangle inequality
\begin{align}
    \sup_{\| \phi \|_{\infty} \leq 1} \left| \sE \left[ \mu_{\theta}^K (\phi) -\mu_{\text{target}} (\phi)  \right] \right| & \leq   \sup_{\| \phi \|_{\infty} \leq 1} \left| \sE \left[ \hat{\mu}^K_{\theta}(\phi) - \mu_{\text{target}}(\phi)\right] \right|  +  \sup_{\| \phi \|_{\infty} \leq 1} \left| \sE \left[ \mu_{\theta}^K (\phi) -\hat{\mu}^K_{\theta} (\phi)  \right]  \right|  \\
    & \leq \frac{12 \hat{\rho}}{K} + \beta_2 \\
     & \leq \frac{12 \rho}{K} + \beta_2.
\end{align}

\looseness-1
Where the last inequality follows from the same logic as in~\cref{prop:energybias} where ESS goes up after truncation and therefore $\rho > \hat{\rho}$.
A direct application of Chernoff's inequality gives us $\sP(X < \delta) = \beta_2 \leq \exp(\lambda \delta) \sE[\exp(-\lambda X)]$ where we used the moment generating function of $p_{\theta}(x)$. Thus the additional bias incurred is, 
\begin{equation}
    \sup_{\| \phi \|_{\infty} \leq 1} \left| \sE \left[ \mu_{\theta}^K (\phi) -\mu_{\text{target}} (\phi)  \right] \right| \leq \frac{12 \rho}{K} + \beta_2 \leq \frac{12 \rho}{K} +  \exp(\lambda \delta) \sE[\exp(-\lambda X)].
\end{equation}

Setting $b:= \text{TV}(\mu_{\theta}, \mu_{\text{target}})$ as the bias, then we have
\begin{equation}
   \delta \geq \frac{1}{\lambda} \log \left( \frac{K b}{12 \rho \sE[\exp(-\lambda X)]} \right).
\end{equation}

\end{proof}



\section{It√¥ Filtering}
\label{sec:proofs}

\subsection{Flow Matching SDE}
\label{app:flow_matching_sde}
As shown in~\citet{domingo2024adjoint} we can write Flow Matching with Gaussian conditional paths and Diffusion models under a unified SDE framework given a reference flow:
\begin{equation}
    x_t = \beta_t x_0 + \alpha_t x_1, 
\end{equation}
where $(\alpha_t)_{t \in [0,1]}, (\beta_t)_{t \in [0,1]}$ are functions such that $\alpha_0 = \beta_1 =0$ and $\alpha_1 = \beta_0 = 1$. In the specific case of flow matching with linear interpolants that we consider we have:
\begin{equation}
    x_t = (1-t) x_0 + t x_1.
\end{equation}
The unified SDE for both flow matching and continuous-time diffusion models as introduced in~\citet{domingo2024adjoint} is then:
\begin{equation}
    dx_t = \kappa_t x + \left( \frac{\sigma_t^2}{2} + \eta_t\right) \mathfrak{s}(x_t, t) + \sigma_t dW_t, \quad \kappa_t = \frac{\dot{\alpha}_t}{\alpha_t}, \eta_t = \beta_t\left( \frac{\dot{\alpha}_t}{\alpha_t}\beta_t - \dot{\beta}_t\right)
\end{equation}
where $\mathfrak{s}(x_t, t)$ is the score function estimated by the diffusion model. Thus the flow matching SDE is:
\begin{equation}
        d x_t = \left(2 f_{t, \theta}(t, x_t) - \frac{x_t}{t}\right) dt + \sigma_t d W_t, \quad \sigma_t = \sqrt{\left(2(1-t)t\right)} 
        \label{eqn:flow_matching_sde}
\end{equation}
In fact, the Stein score can be estimated from the output of a velocity field and vice-versa:
\begin{equation}
    \nabla \log p_t (x_t) = \frac{t f_{t, \theta}(t, x_t) - x_t}{1 - t}, \quad f_{t, \theta}(t, x_t) = \frac{x_t + (1-t)\nabla \log p_t(x_t) }{t}
    \label{eqn:score_as_velocity}
\end{equation}
Rewriting \eqref{eqn:flow_matching_sde} in terms of the score function we get,
\begin{align}
    dx_t &= \frac{x_t}{t} + \sigma^2_t \nabla \log p_t (x_t) + \sigma_t dW_t.
    \label{eqn:flow_matching_sde_with_score}
\end{align}



\subsection{It√¥ filtering}
\label{app:ito_filtering}


\begin{mdframed}[style=MyFrame2]
\begin{restatable}{proposition}{weightsbias}
\label{prop:weights_bias}
\looseness=-1
Assume that the density of the model $p_{\theta}$ after importance sampling $\mu_{\theta}$ is absolutely continuous with respect to the target $\mu_{\text{target}}$. Further, assume that the density of unnormalized importance weights is square integrable $(w(x))^2 < \infty$. Let $r(x_0)$ be the It√¥ density estimator for $\log p_0(x_0)$ of the flow matching SDE:
\begin{equation}
     dx_t = \frac{x_t}{t} + \sigma^2_t \nabla \mathfrak{s}_{\theta} (t, x_t) + \sigma_t dW_t, \quad \sigma_t =  \sqrt{\left(2(1-t)t\right)}.
\end{equation}
 Given $\rho = 1/ \text{ESS}$, and $\zeta >0$ which is the weight clipping threshold. Then the additional bias of using the It$\hat{\text{o}}$ density estimator for importance sampling $\hat{\mu}_{r, \theta}$ with clipping is:
\begin{equation}
          \sup_{\| \phi \|_{\infty} \leq 1} \left| \sE \left[ \mu_{r, \theta}^K (\phi) -\mu_{\text{target}} (\phi)  \right] \right| \leq \frac{12 \rho}{K} + \beta_3 + \beta_4,
\end{equation}
where $\beta_3 = \text{TV}(\mu_{r, \theta}, \mu_{\theta})$ and $\beta_4 = \text{TV}(\mu_{r, \theta}, \hat{\mu}_{r, \theta})$.

\end{restatable}
\end{mdframed}

We now recall It√¥'s lemma which states that for a stochastic process,

\begin{equation}
    dx_t = f_t(t, x_t) + g_t dW_t,
\end{equation}
and a smooth function $h: \R \times \R^d \to \R$ the variation of $h$ as a function of the stochastic SDE can be approximated using a Taylor approximation:
\begin{equation}
    d h(t, x_t) = \left ( \frac{\partial}{\partial t} h(t, x_t) + \frac{\partial}{\partial x} h(t, x_t)^T f_t(t, x_t) + \frac{1}{2} \sigma_t^2 \Delta_x h(t, x_t) \right ) dt + \sigma_t \frac{\partial}{\partial x} h(t, x_t) d W_t.
\end{equation}
where $\Delta_x$ is the Laplacian. We will use It\^o's Lemma with $h(t, x_t) := \log p_t(x_t)$ to obtain the It\^o density estimator~\citep{skreta2024superposition,karczewski2024diffusion} but for flow models
\begin{equation}
    d \log p_t(x_t) = \left ( \frac{\partial}{\partial t} \log p_t(x_t) + \frac{\partial}{\partial x} \log p_t(x_t)^T f(t, x_t) + \frac{1}{2} \sigma_t^2 \Delta_x \log p_t(x_t) \right ) dt + \sigma_t \frac{\partial}{\partial x} \log p_t(x_t) d W_t,
\end{equation}
To solve for the change in density over time we can start from the log version of the Fokker-Plank equation:
\begin{equation}
    \frac{\partial}{\partial t} \log p_t(x) = - \nabla \cdot (f(t, x)) + \frac{1}{2} \sigma_t^2 \Delta_x \log p_t(x) - \nabla_x \log p_t(x)^T \left ( f(t, x) - \frac{1}{2} \sigma_t^2 \nabla_x \log p_t(x) \right )
\end{equation}

in the general case we end with:
\begin{equation}\label{eq:general}
    d \log p_t(x_t) = \left ( - \nabla \cdot \left ( f(t, x_t) - \sigma_t^2 \nabla_x \log p_t(x_t) \right ) + \frac{1}{2} \sigma_t^2 \| \nabla_x \log p_t(x_t) \|^2 \right ) dt + \sigma_t \nabla_x \log p_t(x_t)^T d W_t.
\end{equation}

We now apply this to the flow-matching SDE~\eqref{eqn:flow_matching_sde_with_score} written in terms of the score function. In particular, we have,
\begin{align}
    d \log p_t(x_t) &= \left ( - \nabla \cdot \left (\sigma^2_t \nabla_x \log p_t(x_t) + \frac{x_t}{t} - \sigma_t^2 \nabla_x \log p_t(x_t) \right ) + \frac{1}{2} \sigma_t^2 \| \nabla_x \log p_t(x_t) \|^2 \right ) dt  \nonumber\\
    & + \sigma_t \nabla_x \log p_t(x_t)^T d W_t \nonumber \\
    d \log p_t(x_t) &= \left ( - d / t + \frac{1}{2} \sigma_t^2 \| \nabla_x \log p_t(x_t) \|^2 \right ) dt + \sigma_t \nabla_x \log p_t(x_t)^T d W_t.
    \label{eqn:flow_matching_ito_density}
\end{align}
The above equation makes an implicit assumption that we have access to the actual ground truth score function of $\nabla \log_t(x_t)$ rather than the estimated one $\mathfrak{s}_{\theta}$, expressed via the vector field as in~\eqref{eqn:score_as_velocity}. When working with imperfect score estimates we have the following SDE:
\begin{align}
    dx_t &= \frac{x_t}{t} + \sigma^2_t \nabla \mathfrak{s}_{\theta} (t, x_t) + \sigma_t dW_t.
    \label{eqn:flow_matching_sde_with_score_estimate}
\end{align}

The score estimation error causes a discrepancy in $\log p_t(x_t)$ estimates
whose error is captured in the theorem from~\citet{karczewski2024diffusion}[Theorem 3]:

\begin{equation}
   \log r_0(x_0) = \log p_0(x_0)  + Y
\end{equation}
where $\log r_0$ is the bias of the log density starting at time $t=0$ of the auxiliary process that does not track $x_t$ correctly due to the estimation error of the score. Also, $Y$ is a random variable such that that bias of $r_0$ is given by:

\begin{equation}
    \sE[Y] = \underbrace{\frac{1}{2}\sE_{t\sim \gU(0,1), x_t \sim p_t(x_t)} \left[ \sigma^2_t||\mathfrak{s}_{\theta}(t, x_t) - \nabla \log p_t(x_t) ||^2 \right] }_{\geq 0}
\end{equation}




Thus the It√¥ density estimator forms an upper bound to the true log density, i.e. $r_0 (x_0)\geq \log p_0 (x_0)$. This allows us to form an upper bound on the normalized log weights as an expectation,
\begin{align*}
    \sE_{x_0 \sim p_{\theta}(x_0)}[\log \bar{w}(x_0)] &=  \sE_{x_0 \sim p_{\theta}(x_0)} \left[-\frac{\gE(x_0)}{k_BT} - \log p_0 (x_0) - C \right]\\
                & \leq \sE_{x_0 \sim p_{\theta}(x_0)} \left[-\frac{\gE(x_0)}{k_BT} - r_0 (x_0) \right],
\end{align*}
where $C$ is a constant. We define $ \log \bar{w}_r(x_0):= -\frac{\gE(x_0)}{k_BT} - r_0 (x_0)$ as the new normalized importance weights, module constants. 
We can now compute the additional bias of self-normalized importance sampling estimator $\mu^K_{r, \theta}$
\begin{align}
    \text{TV}(\mu_{r, \theta}, \mu_{\theta}) &=  \sup_{\| \phi \|_{\infty} \leq 1} \left| \sE \left[ \mu_{r, \theta}^K (\phi) - \mu_{\theta}^K (\phi)  \right] \right| \\
    &=   \sup_{\| \phi \|_{\infty} \leq 1} \left| \sE_{x^i \sim p_{\theta}} \left[ \sum^K_{i=1}\bar{w}_r(x^i) \phi(x^i) \right]  - \sE_{x^j \sim p_{\theta}} \left[\sum^K_{j=1} \bar{w}(x^j)\phi(x^j)  \right] \right| \\
   & = \frac{1}{2}\left( \sE_{x^i \sim p_{\theta}} \left[\sum^K_{i=1} \bar{w}_r(x^i) \right]  - \sE_{x^j \sim p_{\theta}} \left[ \sum^K_{j=1} \bar{w}(x^j)  \right] \right) \\
   &= \frac{1}{2}\left( \sE_{x^i \sim p_{\theta}} \left[\sum^K_{i=1} \exp\left( \frac{1}{2}\sE_{t\sim \gU(0,1), x_t \sim p_t(x_t)} \left[ \sigma^2_t||\mathfrak{s}_{\theta}(t, x_t) - \nabla \log p_t(x_t) ||^2 \right] \right)  \right] \right) \\
   & := \beta_3
\end{align}

The total bias is then

\begin{equation}
    \sup_{\| \phi \|_{\infty} \leq 1} \left| \sE \left[ \mu_{r, \theta}^K (\phi) -\mu_{\text{target}} (\phi)  \right] \right| \leq \frac{12 \rho}{K} + \beta_3 .
\end{equation}
Finally, when clipping weights with $\zeta > 0 $ we induce a truncated distribution $\hat{\mu}_{r, \theta}$, i.e. $\hat{r}_0:=\sP(r_0{x_0} > \zeta)$. Using~\cref{lemma:tvd_truncation} this creates another constant factor that contributes $\text{TV}(\mu_{r, \theta}, \hat{\mu}_{r, \theta}) =\beta_4$ to the overall bias:
\begin{equation}
      \sup_{\| \phi \|_{\infty} \leq 1} \left| \sE \left[ \mu_{r, \theta}^K (\phi) -\mu_{\text{target}} (\phi)  \right] \right| \leq \frac{12 \rho}{K} + \beta_3 + \beta_4.
\end{equation}

\section{Datasets}
\label{app:dataset_details}

\looseness=-1
For all datasets besides ALDP we use a training set of 100k contiguous samples (100ps simulation time) from a single MCMC chain, a validation set of the next 20k contiguous samples, and a test set of 100k uniformly sampled samples from the remaining trajectory. Since these are highly multimodal energy functions, this leaves us with \text{biased} training data relative to the Boltzmann distribution. We split trajectories this way to test the model in a challenging and realistic setting --- where biased samples from MD exist and we would like to generate more uncorrelated and unbiased samples. We describe the datasets below and present the simulation parameters in~\cref{tab:datasets}.

\begin{table}[h]
    \centering
    \caption{Overview of simulation parameters}
    \label{tab:datasets}
    \begin{tabular}{l r r r}
        \toprule
        Peptide & Force field & Temperature & Time step \\
        \midrule
        Alanine dipeptide (ALDP) & Amber ff99SBildn & $300\text{K}$ & $1\text{fs}$ \\
        Trialanine (AL3) & Amber 14 & $310\text{K}$ & $1\text{fs}$ \\
        Alanine tetrapeptide (AL4) & Amber ff99SBildn & $300\text{K}$ & $1\text{fs}$ \\
        Hexaalanine (AL6) & Amber 14 & $310\text{K}$ & $1\text{fs}$ \\

        \bottomrule
    \end{tabular}
\end{table}


\looseness=-1
\xhdr{Alanine Dipeptide (AD2)} For this dataset we use the data and data split from~\citet{klein2024transferable}. Here the training set is purposely biased with an overrepresentation of an underrepresented mode, i.e. the positive $\varphi$ state. This  bias makes it easier to reweight to the target Boltzmann distribution.
Alanine Dipeptide consist of one Alanine amino acids, an acetyl group, and an N-methyl group.

\looseness=-1
\xhdr{Trialanine (AL3) and Hexaalanine (AL6)}
For the peptides composed of multiple alanine amino acids, we generate MD trajectories using the \textit{OpenMM} library \citep{eastman2017openmm}. All simulations are conducted in implicit solvent, with the simulation parameters detailed in~\cref{tab:datasets}. These systems do not include any additional capping groups, such as those present in alanine dipeptide (ALDP) and alanine tetrapeptide (AL4), as they are generated in the same manner as described in \citet{klein2023timewarp}. There are two peptide bonds in alanine tripeptide (AL3) and five in alanine hexapeptide (AL6), resulting in two and five distinct Ramachandran plots, respectively.

\looseness=-1
\xhdr{Alanine Tetrapeptide  (AD4)} 
For this dataset we use the same system setup as in \citet{dibak2021temperature}, but treat all bonds as flexible. The original dataset kept all hydrogen bonds fixed, as the Boltzmann Generator was operating in internal coordinates. The MD simulation to generate the dataset is then performed as described above. Alanine Tetrapeptide consist of three Alanine amino acids, an acetyl group, and an N-methyl group. Therefore, there are four distinct Ramachandran plots.


\looseness=-1
\xhdr{Chignolin} 
In addition to the small peptide systems, we also investigate the small protein Chignolin, which consists of ten amino acids. Generating a fully converged all-atom simulation for this system is computationally expensive. Therefore, we use the trajectory provided by \citep{lindorff2011fast}, which was generated using a specialized supercomputer.
In contrast to our other datasets, this simulation was performed in explicit solvent and with a different force field. Since our models do not incorporate additional water molecules, we treat the dataset as if it were in implicit solvent and use the same force field as for the other datasets, namely Amber 14. As a result, the trajectory originates from a slightly different distribution than given by the force-field, likely introducing some bias. Therefore, the task is to generate samples from the equilibrium distribution in implicit solvent while only having access to training data obtained from explicit solvent simulations. As before, we use only the first 100k samples for training. This again highlights the strength of Boltzmann generator based methods, which do not require equilibrium training data. However, it also presents an evaluation challenge, as we lack access to equilibrium samples for the implicit solvent simulation to serve as a reference.



\begin{figure}
    \centering
        \centering
    \begin{subfigure}[b]{0.3\textwidth}
        \centering
           \includegraphics[width=1.0\linewidth]{figures/rama/AD2_test.png}
        \caption{Ramachandran plots for AL2 test dataset}
        \label{fig:al2_test}
    \end{subfigure}%
    \begin{subfigure}[b]{0.3\textwidth}
        \centering
           \includegraphics[width=1.0\linewidth]{figures/rama/AD2_train-2.png}
        \caption{Ramachandran plots for AL2 train dataset}
        \label{fig:al2_train}
    \end{subfigure}%

    \caption{Ramachandran plots for the AL2 dataset with the test (a) and train (b) datasets. We can see that the right mode has been oversampled relative to that of the test set, as in \citet{klein2024transferable}.}
    \label{fig:rama-al23}
\end{figure}


\begin{figure}
    \centering
        \centering
        \begin{subfigure}{0.45\textwidth}
        \includegraphics[width=\linewidth]{figures/rama/AlaAlaAlaRama_test.png}
        \caption{Ramachandran plots for AL3 test dataset}
        \label{fig:al3_test}
        \end{subfigure}
        \centering
        \begin{subfigure}{0.45\textwidth}
        \includegraphics[width=\linewidth]{figures/rama/AlaAlaAlaRama_train.png}
        \caption{Ramachandran plots for AL3 train dataset}
        \label{fig:al3_train}
        \end{subfigure}

    \caption{Ramachandran plots for the AL3 test (a) and train (c) histograms over $\varphi$ and $\psi$ angles. We can see that the training set is completely missing the right mode in $\psi_1$, $\varphi_1$.}
    \label{fig:rama-al3}
\end{figure}

\begin{figure}
    \centering
     \begin{subfigure}[b]{0.8\textwidth}
        \includegraphics[width=1\linewidth]{figures/rama/AceAlaAlaAlaNmeRama_test.png}
        \caption{Ramachandran plots for AL4 test dataset}
    \end{subfigure}
    \begin{subfigure}[b]{0.8\textwidth}
        \includegraphics[width=1\linewidth]{figures/rama/AceAlaAlaAlaNmeRama_train.png}
        \caption{Ramachandran plots for AL4 train dataset}
    \end{subfigure}%
    \caption{Ramachandran plots for the AL4 dataset with test (a) and training (b) histograms over $\varphi$ and $\psi$ angles. We can see that the training set is slightly biased with underrepresentation of the small right mode at $\psi_2 \approx 0$ and $\varphi_2 \approx 1$ in the training set as compared to the test set samples.}
    \label{fig:rama-al4}
\end{figure}


\begin{figure}
    \centering
    \begin{subfigure}[b]{1.0\textwidth}
        \includegraphics[width=1.0\linewidth]{figures/rama/Al6RamaTest.png}
        \caption{Ramachandran plots for AL6 test dataset}
        \label{fig:al6_test}
    \end{subfigure}
    \begin{subfigure}[b]{1.0\textwidth}
        \includegraphics[width=1.0\linewidth]{figures/rama/Al6RamaTrain.png}
    \caption{Ramachandran plots for AL6 train dataset}
    \label{fig:al6_train}
    \end{subfigure}
        \caption{Ramachandran plots for the AL6 dataset with test (a) and training (b) histograms over $\varphi$ and $\psi$ angles. We can see that the training set has broadly comparable coverage and weighting as the test set for this molecule.}
    \label{fig:rama-al6}
\end{figure}

\section{Experimental Details}\label{app:exp_details}

\subsection{Metrics and Timings}

\xhdr{Metrics and sampling setup} For all metrics we first generate samples, then resample to 10k samples, and finally compute metrics to control for the error in distribution metrics from empirical sample size. For all models we draw 10k samples unless otherwise noted. For AL6 ECNF++ due to compute cost we instead draw 1k samples. For the Torus Wasserstein distance we compute the Wasserstein distance on angle space for the dihedral angles between amino acids. For the vector of $\varphi, \psi$ which we denote as $(x, y) \in [-\pi, \pi)^{2s}$ where $s$ is the number of dihedral angles. Specifically we compute:
\begin{equation}
    \sT\text{-}\gW_2 = \left (\inf_{\pi} \int c(x, y)^2 d \pi(x, y) \right)^{1/2}
\end{equation}
where 
\begin{equation}
    c(x, y) = \left ( \sum_i^{2s} ((x_i - y_i) \% \pi)^2 \right )^{1/2}
\end{equation}
and $\pi$ represents a coupling between $x$ and $y$.

\xhdr{Sampling time calculations} For the sampling time, we compute all times on a single A100 80GB, using the maximum power of two batch size possible. %

\xhdr{Training time}
For training times we compute all times on a single A100 80GB GPU except for $\sethree$-EACF which is trained on a single H100. We report the total time in hours until convergence for all methods in the table below.

\begin{table}[h]
    \centering
    \caption{Training time (hours) for all methods.}
    \begin{tabular}{cccccc}
        \hline
        Model & ALDP & AL3 & AL4 & AL6 & Chignolin \\ 
        \hline
        $\sethree$-EACF & 160 & - & -& - & -\\
        ECNF & 4.17 & 5.83 & 8.89 & \\
        ECNF++ & 9.72 & 12.5&  17.17 &  76.94\\
        \nameshort & 16.83 & 24.67 & 41.67 & 57.5 & 427.33 \\ 
        \hline
    \end{tabular}
    \label{tab:training_times}
\end{table}



\subsection{$\sethree$-EACF Implementation details}
\label{app:eacf_implementation_details}
\xhdr{Equivariant augmented coupling flow (EACF)~\citep{midgley2023se}} We adopt the original model configuration from ~\citep{midgley2023se} for our EACF baseline on ALDP. Specifically, we choose the more stable spherical-projection EACF with a 20-layer configuration. Each layer has two ShiftCoM layers and two core-transformation blocks. The EGNN used in the core transformation block consists of 3 message-passing layers with 128 hidden states. Stability enhancement tricks like stable MLP and dynamic weight clipping on each layer's output are fully applied. The model has been trained for 50 epochs with a batch size of 20 using Adam optimizer and peak learning rate of \(1\times10^{-4}\). We use the default 20 samples to estimate likelihoods for importance sampling.

\xhdr{EACF as a Boltzmann generator} EACF is augmented, and therefore to estimate the likelihood of a sample $x$ under an EACF model, we need to use an estimate based on samples from the augmented dimension $a$. Specifically, for a Gaussian distributed augmented variable $a$, we can estimate the marginal density of an observation as
\begin{equation}
    q(x) = \mathbb{E}_{a \sim \pi(\cdot | x)} \left [ \frac{q(x,a)}{\pi(a | x)} \right ],
\end{equation}
however, this is only a consistent estimator of the likelihood and for finite sample sizes has variance. This makes EACF unsuitable for our application of large-scale Boltzmann generators, as in this setting we need to compute exact likelihoods. Variance in likelihood estimation would lead to bias in the final distribution under self-normalized importance sampling or a \nameshort strategy. We therefore do not consider EACF as a viable option for large scale Boltzmann distribution sampling.


\subsection{ECNF Implementation details}
\label{app:ecnf_implementation_details}

\subsubsection{Network and training}
\looseness=-1
\xhdr{Equivariant continuous normalizing flow (ECNF)~\citep{klein2024transferable}} We use the supplied pretrained model from \citet{klein2024transferable} for our ECNF baseline on ALDP. Therefore all training parameters are equivalent to, and specified in, that work. We use the specification for the model ``TBG+Full'' in that work. 

\xhdr{ECNF++} We note five improvements to the ECNF, which together substantially improve performance.
\begin{enumerate}
    \item \textbf{Flow matching loss.} In \citet{klein2024transferable} a flow matching algorithm with smoothing is employed which provides extra stability during training. This is depicted in Alg.~\ref{alg:cfm}, however this smooths out the optimal target distribution~\citep[Proposition 3.3]{tong_conditional_2023}. ECNF uses $\sigma = 0.01$ where we use $\sigma=0$ for ECNF++. We find that $\sigma > 0$ in this case causes poor molecular structures to be generated as the bond lengths are not able to be controlled precisely enough. We note that $\sigma=0$ is used in most recent large scale flow matching models~\cite{liu2024instaflowstephighqualitydiffusionbased,esser2024scalingrectifiedflowtransformers}.
    \item \textbf{Data normalization strategy.} In previous work, data was normalized to \r{A}nstrom (\r{A}) scale. We find this to be too small for stable neural network training and instead employ the standard scheme of standardization based on the training data statistics. Specifically, we subtract the center of mass of each atom, and divide by the standard deviation of the data. Typically this would be done with a per-dimension standard deviation. However, to maintain $\sethree$ equivariance we use a single standard deviation for the whole dataset. On ALDP, the standard deviation of the normalized training data is approximately 0.16, hence we scale up the training data roughly 6-fold on this dataset. We find this greatly improves the training dynamics and final structure precisions.
    \item \textbf{Architecture size.} Empirically, we find the ECNF to be underparameterized. We perform a grid search over layer width and depth, finding a width of 256 and depth of 5 block to be a good balance between performance and speed on ALDP. We used the same parameters for larger molecular systems.
    \item \textbf{Improved optimizer and LR scheduler.} We find using an AdamW \citep{loshchilov2017decoupled} with fairly large weight decay improves performance and stability. Prior work has found weight decay helps to keep the Lipschitz constant of the flow low and avoids stiff dynamics which enables accurate ODE solving during inference. We also use a smoothly varying cosine schedule with warmup (over 5\% of iteration budget) which enables a larger maximum learning rate and faster training than the two step schedule used previously. Both the start and end learning rates are 500 times lower than the defined maximum.
    \item \textbf{Exponential moving average.} We use an exponential moving average (EMA) on the weights with decay $0.999$. This is standard practice in flow models, which improves performance.
\end{enumerate}
These five elements together greatly improve the ECNF training and provide a strong foundation for future Boltzmann generator training on molecular systems using equivariant continuous normalizing flows. Qualitatively, we find ECNFs quite stable to train and robust to training parameters relative to invertible architectures. However, it is very slow to compute the exact likelihood which is necessary for importance sampling.

\xhdr{Other parameters} For both models we use Adam default optimizer parameters for $\beta_1, \beta_2, \epsilon$. For inference we use a Dormand-Prince 45 (dopri5) adaptive step size solver with absolute tolerance $10^{-4}$ and relative tolerance $10^{-4}$. 


\begin{algorithm}[t]
  \caption{ECNF flow matching training}
  \label{alg:cfm}
\begin{algorithmic}
\STATE {\bfseries Input:} Prior $q_0$, Empirical samples from data $q_1$, bandwidth $\sigma$, batchsize $b$, initial network $v_{\theta}$.
\WHILE{Training}
\STATE $\vx_0 \sim q_0(\vx_0); \quad \vx_1 \sim q_1(\vx_1)$ \COMMENT{Sample batches of size $b$ \textit{i.i.d.} from the dataset}
\STATE $t \sim \mathcal{U}(0, 1)$
\STATE $\mu_t \gets t \vx_1 + (1 - t) \vx_0$
\STATE $x \sim \mathcal{N}(\mu_t, \sigma^2 I)$
\STATE $\mathcal{L}(\theta) \gets \| v_\theta(t, x) - (\vx_1 - \vx_0)\|^2$
\STATE $\theta \gets \mathrm{Update}(\theta, \nabla_\theta \mathcal{L}(\theta))$
\ENDWHILE
\STATE \textbf{Return} $v_\theta$
\end{algorithmic}
\end{algorithm}

\xhdr{Likelihood evaluation} Evaluating the likelihood of a CNF model requires calculating the integral of the divergence, as in~\eqref{eqn:instantaneous_change_of_variable}. While there exist fast unbiased approximations of the likelihood using Hutchinson's trace estimator~\cite{Hutchinson01011990,grathwohl_ffjord_2018}, these are unfortunately unsuitable for Boltzmann generator applications where variance in the likelihood estimator leads to biased weights under self-normalized importance sampling. We therefore calculate the Jacobian using automatic differentiation which is both memory and time intensive. For example, on AL6, the maximum batch size that can fit on an 80GB A100 GPU is 8. This batch takes around 2 minutes for 84 integration steps. We also use an improved vectorized Jacobian trace implementation for all CNF which reduces memory by roughly half and time by roughly 3x over previous implementations. We note that these numbers are approximate and depend heavily on both the batch size and the input dimensions.

\xhdr{On using a CNF proposal with \nameshort} In principle it is possible to drop in replace our NF architecture with a CNF in \nameshort. However, there are several drawbacks to such an approach, most notably in efficiency. As previously discussed, CNFs are extremely computationally inefficient to sample a likelihood from. We find on the order of 100 SBG steps are necessary for best performance. This would make CNFs at least two orders of magnitude slower to sample from, when they are aleady at the edge of tractibility for the current importance sampling estimates. We leave it to future work to consider faster CNF likelihoods and note that our \nameshort algorithm could be applied there readily.

\subsection{Sequential Boltzmann generators implementation details}
\label{app:abgen_implementation_details}

\xhdr{Architecture} We scale the TarFlow architecture for increasingly challenging datasets. As advised by \citet{zhai2024normalizing} we scale the layers per block alongside the number of blocks. The layers / blocks / channels and resulting number of parameters are presented in Table \ref{tab:model_configs}. We note the larger number of parameters in the TarFlow relative to the ECNF++ despite the faster inference walltime, due to the lack of simulation and higher computational efficiency of the architecture.

\begin{table}[h]
    \caption{TarFlow configurations across different datasets}
    \label{tab:model_configs}
    \centering
    \begin{tabular}{lcccc}
        \toprule
        Dataset & Layers per Block & Number Blocks & Channels & Number Parameters (M) \\
        \midrule
        ALDP & 4 & 4 & 256 & 12.7 \\
        AL3 & 6 & 6 & 256 & 28.5 \\
        AL4 & 6 & 6 & 384 & 64.0 \\
        AL6 & 6 & 6 & 384 & 64.0 \\
        Chignolin & 8 & 8 & 512 & 202.0 \\
        \bottomrule
    \end{tabular}
\end{table}

\xhdr{Training configuration} The training hyperparameters used closely follow those of \citet{zhai2024normalizing}, although we deviate in using a larger value of weight decay as instability was observed during training. Namely we use a learning rate of \(1 \times 10^{-4}\), weight decay of \(4 \times 10^{-4}\), Adam \(\beta_1,\beta_2\) of \((0.90, 0.95)\) and Adam \(\eps\) of \(1\times10^{-8}\). We additionally employ the same cosine decay learning rate schedule with warmup (start and end learning rate 500 times lower than maximum value) and exponential moving average decay (0.999) used in ECNF++. Training is performed for 1000 epochs. As non-monotonic improvement was observed on validation metrics, we use early stopping based on the $\gE$-$\gW_1$ against the validation dataset, where the proposal \(p_\theta\) was cropped to a target energy range of interest to ignore outlier samples that will be removed by energy cropping during inference.

\begin{table}
    \centering
    \caption{Overview of training configurations}
    \label{tab:training}
    \begin{tabular}{l r r r}
        \toprule
        Training Parameter & ECNF & ECNF++ & TarFlow\\
        \midrule
        Optimizer & Adam & AdamW  & AdamW \\
        Learning Rate & $5 \times 10^{-4}$ & $5 \times 10^{-4}$ & $1 \times 10^{-4}$\\
        Weight Decay & 0.0 &$1 \times 10^{-2} $& $4 \times 10^{-4}$\\
        $\beta_1,\beta_2,\varepsilon$ &h0.9, 0.999, 1e-8 & 0.9, 0.999, 1e-8 & 0.9, 0.95, 1e-8\\
        EMA Decay & 0.000 & 0.999 & 0.999\\
        Width & 64 & 256 & Varies\\
        $N$ blocks & 5 & 5 & Varies\\
        Parameters & 152 K & 2.317 M & Varies  \\
        \bottomrule
    \end{tabular}
\end{table}

\xhdr{Sampling hyperparameters} Whilst the TarFlow is capable of generating low-energy peptide states, it is also prone to generating samples of extremely high target energy. For standard importance sampling this presents no issue as these samples will be assigned importance weights of approximately 0. However, for the \nameshort these high energy samples were prone to numerical instability during Langevin dynamics. To mitigate this issue we truncate the proposal distribution prior \(p_\theta\) based on an energy cutoff, noting that the bias introduced by this operation is bounded in \cref{prop:energybias}.
Similarly to EACF, we additionally remove the samples corresponding to the largest 0.2\% of importance weights, in the case of \nameshort this is performed once, prior to Langevin dynamics. See \S\ref{sec:ablations} for an ablation on the effect of weight clipping in \nameshort. For ALDP, AL3, AL4 and AL6 we use 100 Langevin timesteps for our main results, with \(\eps = 1\times10^{-5}\) and  \(\text{ESS}_\text{threshold} = 0.5\).  For Chignolin we use 100 timesteps, with \(\eps = 1\times10^{-9}\) and \(\text{ESS}_\text{threshold} = 0.99\). For ablations of these hyperparameters see \S\ref{sec:ablations}. Simple multinomial resampling is used for all datasets except Chignolin where stratified resamping is used.

\section{Additional Results}
\label{app:additional_results}

\subsection{Chirality}

The ECNF architecture is E(3) equivariant, hence is equivariant to reflections, and will generate samples of both possible global chiralities. As the energy functions are themselves invariant to reflections this is not resolved by importance sampling. Having the correct global chirality is necessary to match the test dataset dihedral angle distributions where only one global chirality is present in the data. The incorrect chirality can show up as a symmetric mode on Ramachandran plots. To resolve this issue we follow \citet{klein2024transferable} in detecting incorrect chirality conformations, and reflecting them. Points with unresolvable symmetry (e.g mixed chirality conformations) are dropped. The results for $\sT$-$\gW_2$ before and after fixed-chirality samples are presented in Table \ref{tab:chirality_results}. We observe a reduction in metric value (improved performance) on all configurations aside from ECNF AL4, which we attribute to evaluation noise. We further note that non-equivariant methods such as \nameshort do not suffer this effect and hence do not require any symmetry post-processing.



\begin{table*}[ht!]
\caption{\small $\sT$-$\gW_2$ results for unprocessed and fixed-chirality samples from ECNF and ECNF++}
\label{tab:chirality_results}
\begin{tabular}{@{}lcccccc}
    \toprule
    Datasets $\rightarrow$ & \multicolumn2c{Tripeptide (AL3)} & \multicolumn2c{Tetrapeptide (AL4)} & \multicolumn2c{Hexapeptide (AL6)}  \\
    \cmidrule(lr){2-3}\cmidrule(lr){4-5}\cmidrule(lr){6-7}
    Algorithm $\downarrow$ & Unprocessed & Fixed & Unprocessed & Fixed & Unprocessed & Fixed \\
    \midrule
    ECNF
         & 7.010 & 4.241 & 3.853 & 4.902 & - & - \\
    ECNF++ (Ours)   
         & 1.967 $\pm$ 0.062 & 1.177 $\pm$ 0.145 
         & 2.414 $\pm$ 0.000 & 2.082 $\pm$ 0.005
         & 5.405 $\pm$ 0.069 & 4.315 $\pm$ 0.018 \\
    \bottomrule
\end{tabular}
\vspace{-5pt}
\end{table*}

\subsection{Ramachandran Plots}
\label{app:rama_plots}

In this appendix we include the Ramachandran plots for each model on each peptide. Please note that the ground truth training and test Ramachandran plots are located in~\cref{app:dataset_details}.

\xhdr{Alanine dipeptide (ALDP)} In Figure~\ref{fig:ramas_aldp} we present the Ramachandran plot for ALDP. We find that that ECNF++ models the distribution well, but drops the positive \(\varphi\) mode. Which is quite interesting as this mode is oversampled in the training data (see Figure~\ref{fig:rama-al23}).

\xhdr{Trialanine (AL3)} In Figure~\ref{fig:ramas_al3} we can see the Ramachandran plot for resampled points for the Trialanine dataset. Comparing this to the train and test data in Figure~\ref{fig:rama-al3} we see that the AL3 training data is missing the \(\varphi_1\) positive mode which is reflected in all of the models. We find that our \nameshort model is able to best capture the low density modes, although barely captures the lowest density mode in the second peptide bond.

\xhdr{Alanine tetrapeptide (AL4)} In Figure~\ref{fig:ramas_al6} we can see the Ramachandran plot for resampled points for the Trialanine dataset. Comparing this to the train and test distributions in Figure \ref{fig:rama-al4} we observe that both ECNF++ and \nameshort capture the dihedral angle distribution with comparable success, although the ECNF++ has very few points in the positive \(\varphi\) modes. This is to be expected give the relatively small improvement of $\sT$-$\gW_2$ seen in Table \ref{tab:main_results_wide} for this dataset.


\xhdr{Hexaalanine (AL6)} In Figure~\ref{fig:ramas_al6} we present the Ramchandran plots for samples from ECNF++ and \nameshort. Due to computational constraints only 1,000 samples are plotted for ECNF++, whereas 100,000 are present for \nameshort.  With so few samples for ECNF++ it is not possible to compare methods. We find that \nameshort succeeds to capture the low density positive \(\varphi\) modes, albeit with a tight concentration of points as opposed to a broad range of low density. We additionally observe the negative \(\Psi_1\) mode to be well captured by the \nameshort.


\begin{figure*}[h!]
    \captionsetup[subfigure]{aboveskip=-1pt,belowskip=-1pt}
    \centering
    \begin{subfigure}{0.22\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/rama_v2/22_ecnf_rama.png}
        \caption{ECNF on ALDP}
    \end{subfigure}
    \begin{subfigure}{0.22\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/rama_v2/22_ecnf++_rama.png}
        \caption{ECNF++ on ALDP}
    \end{subfigure}
         \begin{subfigure}{0.22\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/rama_v2/22_SBG_rama.png}
        \caption{\nameshort on ALDP}
    \end{subfigure}
    \caption{Alanine dipeptide (ALDP) Ramachandran plots. 10,000 points plotted for ECNF++, 100,000 points plotted for \nameshort.}
    \label{fig:ramas_aldp}
\end{figure*}

\begin{figure*}[h!]
    \captionsetup[subfigure]{aboveskip=-1pt,belowskip=-1pt}
    \centering
    \begin{subfigure}{0.42\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/rama_v2/33_ecnf++_rama.png}
        \caption{ECNF++ on AL3}
    \end{subfigure}
         \begin{subfigure}{0.42\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/rama_v2/33_SBG_rama.png}
        \caption{\nameshort on AL3}
    \end{subfigure}
    \caption{Trialanine (AL3) Ramachandran plots. 10,000 points for ECNF++, 100,000 points for SBG.}
    \label{fig:ramas_al3}
\end{figure*}


\begin{figure*}[h!]
    \captionsetup[subfigure]{aboveskip=-1pt,belowskip=-1pt}
    \centering
    \label{fig:ramas_al4_ecnf}
    \captionsetup[subfigure]{aboveskip=-1pt,belowskip=-1pt}
    \centering
    \begin{subfigure}{0.65\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/rama_v2/42_ecnf++_rama.png}
        \caption{ECNF++ on AL4}
    \end{subfigure}
    \label{fig:ramas_al4_ecnf++}
    \captionsetup[subfigure]{aboveskip=-1pt,belowskip=-1pt}
    \centering
    \begin{subfigure}{0.65\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/rama_v2/42_SBG_rama.png}
        \caption{\nameshort on AL4}
    \end{subfigure}
    \caption{Alanine tetrapeptide (AL4) Ramachanhran plots. 10,000 points plotted for ECNF(++), 100,000 points plotted for \nameshort.}
    \label{fig:ramas_al4}
\end{figure*}

\clearpage
\begin{figure*}[h!]
    \captionsetup[subfigure]{aboveskip=-1pt,belowskip=-1pt}
    \centering
    \begin{subfigure}{0.99\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/rama_v2/63_ecnf++_rama.png}
        \caption{ECNF++ on AL6}
    \end{subfigure}
    \begin{subfigure}{0.99\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/rama_v2/63_SBG_rama.png}
        \caption{\nameshort on AL6}
    \label{fig:ramas_al6_sbg}
    \end{subfigure}
        \caption{Ramachandran plots for various models on AL6 dataset. Due to computational constraints only 1,000 points plotted for ECNF++, 100,000 points plotted for \nameshort.}
        \label{fig:ramas_al6}
\end{figure*}

\subsection{Ablation studies}
\label{sec:ablations}

\xhdr{Ablation of IS and SMC} In \cref{tab:smc_ablation} we compare the quantitative performance of the TarFlow on AL3 without reweighting (Proposal), with standard importance sampling (BG) and with Annealed Langevin SMC (\nameshort). We observe a dramatic reduction in $\gE$-$\gW_1$ for the BG over the proposal, with a strong further decrease for \nameshort on AL3 and AL6. On ALDP $\sT$-$\gW_2$ improves greatly when reweighting, which is expected as the training data and subsequently the proposal distribution was intentionally biased to provide improved coverage \citep{klein2024transferable}. On the other datasets the reweighting moderately increases the $\sT$-$\gW_2$, with little distinction between the reweighting methods.

\begin{table*}[ht!]
\caption{Quantitative results comparing (unweighted) proposal, Boltzmann generator (BG) and sequential Boltzmann generator (SBG).}
\label{tab:smc_ablation}
\resizebox{1\linewidth}{!}{
\begin{tabular}{@{}lccccccccccccccc}
    \toprule
    Datasets $\rightarrow$ & \multicolumn{2}{c}{Alanine dipeptide (ALDP)} & \multicolumn{2}{c}{Tripeptide (AL3)} & \multicolumn{2}{c}{Tetrapeptide (AL4)} & \multicolumn{2}{c}{Hexapeptide (AL6)}  \\
    \cmidrule(lr){2-3}\cmidrule(lr){4-5}\cmidrule(lr){6-7}\cmidrule(lr){8-9}
    Model & $\gE$-$\gW_1$ $\downarrow$ & $\sT$-$\gW_2$ & $\gE$-$\gW_1$ $\downarrow$ & $\sT$-$\gW_2$ & $\gE$-$\gW_1$ $\downarrow$ & $\sT$-$\gW_2$ &  $\gE$-$\gW_1$ $\downarrow$ & $\sT$-$\gW_2$ \\
    \midrule
    Proposal & 9.416 $\pm$ 0.575 & 1.139 $\pm$ 0.013 & 5.873 $\pm$ 0.426 & 0.754 $\pm$ 0.013 & 5.714 $\pm$ 0.361 & 1.716 $\pm$ 0.015 & 45.190 & 2.923 \\
    BG & 0.455 $\pm$ 0.395 & 0.225 $\pm$ 0.014 & 1.862 $\pm$ 0.431 & 0.949 $\pm$ 0.065 & 2.028 $\pm$ 0.317 & 1.861 $\pm$ 0.039 & 0.889 & 3.432 \\
    SBG & 0.466 $\pm$ 0.380 & 0.226 $\pm$ 0.029 & 1.314 $\pm$ 0.409 & 0.948 $\pm$ 0.068 & 1.920 $\pm$ 0.251 & 1.841 $\pm$ 0.047 & 0.305 & 3.326 \\
    \bottomrule
\end{tabular}}
\end{table*}


\xhdr{Center of mass adjusted energy} As discussed in \S\ref{sec:scaling_training}, the TarFlow proposal distribution is not mean-free due to the CoM data augmentation applied to the training data, with a centroid norm distribution \(||C|| \sim \sigma \chi_3\). This can introduce adverse behavior, as the target energy is invariant to \(||C||\), and thus the importance weights \(p(x)/p_\theta(x)\) will depend on \(||C||\) of a sample \(x\). Concretely, a low (target) energy sample generated far from the origin (with large \(||C||\)) will have low likelihood under \(p_\theta\) but high likelihood under \(p\) leading to a very large importance weight.

To provide a visual intuition for this effect, we plot in \cref{fig:com_logit_clipping} the centroid norm distribution of the TarFlow proposal samples before and after (standard) importance sampling (e.g a BG with TarFlow proposal). Here the empirical distribution is generated using \(2\times10^7\) samples, to approximate the asymptotic behavior. In \cref{fig:com_0_0} we observe that, even with this large number of samples, without weight clipping or CoM adjusted energy (\cref{eqn:com_adjust}) the \(||C||\) distribution is greatly influenced by the reweighting. In this case resampling transports most density to high \(||C||\) samples, where there was very little density prior to reweighting and hence little sample diversity, and a large peak manifests resulting from a single sample with very large importance weight. In contrast, in \cref{fig:com_1_0} we see a much smaller change in \(||C||\) distribution after reweighting, with no large peak for any given sample, after applying the CoM adjustment. In this case there is no overweighting of high \(||C||\) regions with limited sample diversity. Adding weight clipping to the standard proposal energy function (\cref{fig:com_0_1}) greatly reduces the change in distribution from reweighting, although the mean remains notably shifted towards higher \(||C||\) samples. Applying weight clipping with the CoM adjusted proposal energy function (\cref{fig:com_1_1}) has little effect on the \(||C||\) distribution beyond smoothing. We emphasize that these plots are presented for \(2 \times 10^7\) samples hence clipping may have a larger still effect for both proposal energy functions for smaller sample sets.

In Figures \ref{fig:com_augmentation_samples} and \ref{fig:com_augmentation_timesteps} we ablate the utility of performing the center of mass energy adjustment to the proposal energy. Specifically, we ablate the CoM adjustment as a function of number of samples used during inference and also as a function of a number of inference timesteps. Each of these ablations is performed on the trialanine tripeptide (AL3). Considering Figure \ref{fig:com_augmentation_samples}, there is little distinction between variants on $\gE$-$\gW_1$ with respect to \(N\) samples, although standard energy without clipping can be identified as the least performant, and all methods improve with increased \(N\). On $\sT$-$\gW_2$ the standard energy without clipping is again evidently the worst performing, with a clear benefit to applying the CoM adjustment where clipping is not used. The best performing variants do employ clipping, with a slight but clear benefit to using the CoM adjustment. Considering Figure \ref{fig:com_augmentation_timesteps}, we observe that, with clipping, the standard energy is more performant on $\gE$-$\gW_1$, however for sufficient timesteps the non-clipped CoM energy is the most performant. On $\sT$-$\gW_2$ the clipped variants are much more performant at low timestep budgets, although the non-clipped CoM asymptotically achieves comparable values. CoM energy with clipping slightly improves with timesteps whereas standard energy with clipping actually diverges. The non-clipped standard energy is least performant on both metrics for almost all timesteps.

\xhdr{Resampling threshold} In Figure \ref{fig:ablate_ess} we ablate the resampling threshold (using the CoM energy witgh clipping as in our main experiments). We observe no clear trend in performance with respect to this hyperparameter with many unstable values, particularly near the boundaries, and a stable range around the value 0.5.


\begin{figure*}[t]
    \centering
    \begin{subfigure}{0.45\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/ablation_plots/use_com_0_clip_logits_0_coms_norm_histogram.png}
        \caption{Weight clipping \xmark, CoM energy \xmark}
        \label{fig:com_0_0}
    \end{subfigure}
    \hfill
         \begin{subfigure}{0.45\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/ablation_plots/use_com_1_clip_logits_0_coms_norm_histogram.png}
        \caption{Weight clipping \xmark, CoM energy \cmark}
        \label{fig:com_1_0}
    \end{subfigure}\\
    \vspace{0.5cm}
         \begin{subfigure}{0.45\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/ablation_plots/use_com_0_clip_logits_1_coms_norm_histogram.png}
        \caption{Weight clipping \cmark, CoM energy \xmark}
        \label{fig:com_0_1}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.45\linewidth}
    \centering
        \includegraphics[width=\linewidth]{figures/ablation_plots/use_com_1_clip_logits_1_coms_norm_histogram.png}
        \caption{Weight clipping \cmark, CoM energy \cmark}
        \label{fig:com_1_1}
    \end{subfigure}
        \caption{Centroid norm $||C||$ histograms for \(2\times10^7\) proposal samples and reweighted proposal samples, with / without both of weight clipping (0.2\%) and center of mass adjusted energy.}
    \label{fig:com_logit_clipping}
\end{figure*}

\begin{figure}[t]
    \captionsetup[subfigure]{aboveskip=-1pt,belowskip=-1pt}
    \centering
      \begin{subfigure}{0.45\linewidth}
         \centering
         \includegraphics[width=\linewidth]{figures/ablation_plots/Energy_vs_samples.png}
         \label{fig:com_energy_vs_samples}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.45\linewidth}
         \includegraphics[width=\linewidth]{figures/ablation_plots/Torus_wasserstein_vs_samples.png}
        \label{fig:com_torus_vs_samples}
    \end{subfigure}
    \caption{Energy Wasserstein-1 and dihedral angle Wasserstein-2 performance of standard and center of mass adjusted energy, with / without weight clipping (0.2\%) at a variety of sampling set sizes.}
       
    \label{fig:com_augmentation_samples}
\end{figure}


\begin{figure}[t]
    \captionsetup[subfigure]{aboveskip=-1pt,belowskip=-1pt}
    \centering
    \begin{subfigure}{0.45\linewidth}
         \centering
         \includegraphics[width=\linewidth]{figures/ablation_plots/Energy_vs_timesteps.png}
         \label{fig:com_energy_vs_timesteps}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.45\linewidth}
         \includegraphics[width=\linewidth]{figures/ablation_plots/Torus_wasserstein_vs_timesteps.png}
        \label{fig:com_torus_vs_timesteps}
    \end{subfigure}
        \caption{Energy Wasserstein-1 and dihedral angle Wasserstein-2 performance of \nameshort on AL3 with respect to Langevin time discretization.}
    \label{fig:com_augmentation_timesteps}
\end{figure}


\begin{figure}[t]
    \captionsetup[subfigure]{aboveskip=-1pt,belowskip=-1pt}
    \centering
      \begin{subfigure}{0.45\linewidth}
         \centering
         \includegraphics[width=\linewidth]{figures/ablation_plots/Energy_vs_ess.png}
         \label{fig:com_energy_vs_ess}
     \end{subfigure}
     \hfill
    \begin{subfigure}{0.45\linewidth}
    \centering
         \includegraphics[width=\linewidth]{figures/ablation_plots/Torus_wasserstein_vs_ess.png}
        \label{fig:com_torus_vs_ess}
    \end{subfigure}
    \caption{ \small Energy Wasserstein-1 and dihedral angle Wasserstein-2 performance of \nameshort on AL3 with respect to ESS threshold.}
    \label{fig:ablate_ess}
\end{figure}


\xhdr{Ablation on EACF Importance Weight Clipping}
We report the additional results on EACF trained on ALDP dataset. We chose to use $0.2\%$ clip threshold on the importance weights for fair comparison. Nevertheless, in the resampling process, we observe a significant degradation in sample diversity, as evidenced by the energy histograms and Rama plots. From the qualitative results in ~\cref{fig:al2_eacf_rama_diff_clip}, we can see that EACF generates highly unreliable importance weights, particularly visible in the energy histograms where there are extreme spikes and poor alignment with the true data distribution. This leads to poor resampling quality, as demonstrated in the corresponding Rama plots where the resampled points fail to capture the true data distribution. While increasing the clipping threshold to $10\%$ shows some improvement, the fundamental issue of inaccurate importance weight estimation by EACF persists across different clipping ratios.


\begin{figure*}[t!]
    \vspace{-4pt}
    \captionsetup[subfigure]{aboveskip=-1pt,belowskip=-1pt}
    \centering   
    \begin{subfigure}{0.31\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/eacf_ad2/clip_0.2/22_eacf_0.002_density.png}
    \end{subfigure}
    \hspace{-4pt}
    \begin{subfigure}{0.31\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/eacf_ad2/clip_2/22_eacf_0.02_density.png}
    \end{subfigure}
    \hspace{-4pt}
    \begin{subfigure}{0.31\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/eacf_ad2/clip_10/22_eacf_0.1_density.png}
    \end{subfigure}
    
    \vspace{4pt}
    
    \begin{subfigure}{0.31\linewidth}
        \centering
        \includegraphics[width=0.9\linewidth]{figures/eacf_ad2/clip_0.2/22_eacf_0.002_rama.png}
        \caption{Clip $0.2\%$}
    \end{subfigure}
    \hspace{-4pt}
    \begin{subfigure}{0.31\linewidth}
        \centering
        \includegraphics[width=0.9\linewidth]{figures/eacf_ad2/clip_2/22_eacf_0.02_rama.png}
        \caption{Clip $2\%$}
    \end{subfigure}
    \hspace{-4pt}
    \begin{subfigure}{0.31\linewidth}
        \centering      \includegraphics[width=0.9\linewidth]{figures/eacf_ad2/clip_10/22_eacf_0.1_rama.png}
        \caption{Clip $10\%$}
    \end{subfigure}
   
    \vspace{-10pt}
    \caption{Energy histogram and Ramachandran Plots of EACF under different clipping ratio $[0.2\%, 2\%, 10\%]$.}
       
    \vspace{-15pt}
    \label{fig:al2_eacf_rama_diff_clip}
\end{figure*}

\end{document}
