[
  {
    "index": 0,
    "papers": [
      {
        "key": "ref25-rt1",
        "author": "Anthony Brohan and Noah Brown and Justice Carbajal...",
        "title": "RT-1: Robotics Transformer for Real-World Control at Scale"
      },
      {
        "key": "ref26-rt2",
        "author": "Anthony Brohan and Noah Brown and Justice Carbajal...",
        "title": "RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "ref27-flamingo",
        "author": "Xinghang Li and Minghuan Liu and Hanbo Zhang and Cunjun Yu and Jie Xu and Hongtao Wu and Chilam Cheang and Ya Jing and Weinan Zhang and Huaping Liu and Hang Li and Tao Kong",
        "title": "Vision-Language Foundation Models as Effective Robot Imitators"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "ref28-clipgrasp",
        "author": "Georgios Tziafas and Yucheng Xu and Arushi Goel and Mohammadreza Kasaei and Zhibin Li and Hamidreza Kasaei",
        "title": "Language-guided Robot Grasping: CLIP-based Referring Grasp Synthesis in Clutter"
      },
      {
        "key": "ref29-clipFO3D",
        "author": "Zhang, Junbo and Dong, Runpei and Ma, Kaisheng",
        "title": "CLIP-FO3D: Learning Free Open-world 3D Scene Representations from 2D Dense CLIP"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "ref30-surveyofModularLLMRL",
        "author": "Cao, Yuji and Zhao, Huan and Cheng, Yuheng and Shu, Ting and Chen, Yue and Liu, Guolong and Liang, Gaoqi and Zhao, Junhua and Yan, Jinyue and Li, Yun",
        "title": "Survey on Large Language Model-Enhanced Reinforcement Learning: Concept, Taxonomy, and Methods"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "ref4-VLM-MSGM",
        "author": "Su, Ke and Zhang, Xingxing and Zhang, Siyang and Zhu, Jun and Zhang, Bo",
        "title": "To Boost Zero-Shot Generalization for Embodied Reasoning With Vision-Language Pre-Training"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "ref2-visnav",
        "author": "Tianyao Zhang and Xiaoguang Hu and Jin Xiao and Guofeng Zhang",
        "title": "A survey of visual navigation: From geometry to embodied AI"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "ref3-autodrive",
        "author": "Luo, Sheng and Chen, Wei and Tian, Wanxin and Liu, Rui and Hou, Luanxuan and Zhang, Xiubao and Shen, Haifeng and Wu, Ruiqi and Geng, Shuyi and Zhou, Yi and Shao, Ling and Yang, Yi and Gao, Bojun and Li, Qun and Wu, Guobin",
        "title": "Delving Into Multi-Modal Multi-Task Foundation Models for Road Scene Understanding: From Learning Paradigm Perspectives"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "ref33-saycan",
        "author": "Michael Ahn and Anthony Brohan and Noah Brown...",
        "title": "Do As I Can, Not As I Say: Grounding Language in Robotic Affordances"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "ref31-Blip2",
        "author": "Junnan Li and Dongxu Li and Silvio Savarese and Steven Hoi",
        "title": "BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models"
      },
      {
        "key": "ref32-LLAVAnext",
        "author": "Feng Li and Renrui Zhang and Hao Zhang and Yuanhan Zhang and Bo Li and Wei Li and Zejun Ma and Chunyuan Li",
        "title": "LLaVA-NeXT-Interleave: Tackling Multi-image, Video, and 3D in Large Multimodal Models"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "ref43-cotRobot",
        "author": "Sun, Lingfeng and Jha, Devesh K. and Hori, Chiori and Jain, Siddarth and Corcodel, Radu and Zhu, Xinghao and Tomizuka, Masayoshi and Romeres, Diego",
        "title": "Interactive Planning Using Large Language Models for Partially Observable Robotic Tasks"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "ref34-xcomposer",
        "author": "Pan Zhang and Xiaoyi Dong and Yuhang Zang...",
        "title": "InternLM-XComposer-2.5: A Versatile Large Vision Language Model Supporting Long-Contextual Input and Output"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "ref35-minicpm",
        "author": "Shengding Hu and Yuge Tu and Xu Han...",
        "title": "MiniCPM: Unveiling the Potential of Small Language Models with Scalable Training Strategies"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "ref40-VLMap",
        "author": "Huang, Chenguang and Mees, Oier and Zeng, Andy and Burgard, Wolfram",
        "title": "Visual Language Maps for Robot Navigation"
      },
      {
        "key": "ref41-ViNG",
        "author": "Shah, Dhruv and Eysenbach, Benjamin and Kahn, Gregory and Rhinehart, Nicholas and Levine, Sergey",
        "title": "ViNG: Learning Open-World Navigation with Visual Goals"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "ref42-Palm-E",
        "author": "Danny Driess and Fei Xia and Mehdi S. M. Sajjadi and Corey Lynch and Aakanksha Chowdhery and Brian Ichter and Ayzaan Wahid and Jonathan Tompson and Quan Vuong and Tianhe Yu and Wenlong Huang and Yevgen Chebotar and Pierre Sermanet and Daniel Duckworth and Sergey Levine and Vincent Vanhoucke and Karol Hausman and Marc Toussaint and Klaus Greff and Andy Zeng and Igor Mordatch and Pete Florence",
        "title": "PaLM-E: An Embodied Multimodal Language Model"
      }
    ]
  }
]