@article{ref2-visnav,
	title = {A survey of visual navigation: From geometry to embodied AI},
	journal = {Engineering Applications of Artificial Intelligence},
	volume = {114},
	pages = {105036},
	year = {2022},
	issn = {0952-1976},
	doi = {https://doi.org/10.1016/j.engappai.2022.105036},
	url = {https://www.sciencedirect.com/science/article/pii/S095219762200207X},
	author = {Tianyao Zhang and Xiaoguang Hu and Jin Xiao and Guofeng Zhang}
}

@misc{ref25-rt1,
	title={RT-1: Robotics Transformer for Real-World Control at Scale}, 
	author={Anthony Brohan and Noah Brown and Justice Carbajal...},
	year={2023},
	eprint={2212.06817},
	archivePrefix={arXiv},
	primaryClass={cs.RO},
	url={https://arxiv.org/abs/2212.06817}, 
}

@misc{ref26-rt2,
	title={RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control}, 
	author={Anthony Brohan and Noah Brown and Justice Carbajal...},
	year={2023},
	eprint={2307.15818},
	archivePrefix={arXiv},
	primaryClass={cs.RO},
	url={https://arxiv.org/abs/2307.15818}, 
}

@misc{ref27-flamingo,
	title={Vision-Language Foundation Models as Effective Robot Imitators}, 
	author={Xinghang Li and Minghuan Liu and Hanbo Zhang and Cunjun Yu and Jie Xu and Hongtao Wu and Chilam Cheang and Ya Jing and Weinan Zhang and Huaping Liu and Hang Li and Tao Kong},
	year={2024},
	eprint={2311.01378},
	archivePrefix={arXiv},
	primaryClass={cs.RO},
	url={https://arxiv.org/abs/2311.01378}, 
}

@misc{ref28-clipgrasp,
	title={Language-guided Robot Grasping: CLIP-based Referring Grasp Synthesis in Clutter}, 
	author={Georgios Tziafas and Yucheng Xu and Arushi Goel and Mohammadreza Kasaei and Zhibin Li and Hamidreza Kasaei},
	year={2023},
	eprint={2311.05779},
	archivePrefix={arXiv},
	primaryClass={cs.RO},
	url={https://arxiv.org/abs/2311.05779}, 
}

@INPROCEEDINGS{ref29-clipFO3D,
	author={Zhang, Junbo and Dong, Runpei and Ma, Kaisheng},
	booktitle={2023 IEEE/CVF International Conference on Computer Vision Workshops (ICCVW)}, 
	title={CLIP-FO3D: Learning Free Open-world 3D Scene Representations from 2D Dense CLIP}, 
	year={2023},
	volume={},
	number={},
	pages={2040-2051},
	keywords={Training;Solid modeling;Adaptation models;Three-dimensional displays;Annotations;Semantic segmentation;Computational modeling},
	doi={10.1109/ICCVW60793.2023.00219}}

@ARTICLE{ref3-autodrive,
	author={Luo, Sheng and Chen, Wei and Tian, Wanxin and Liu, Rui and Hou, Luanxuan and Zhang, Xiubao and Shen, Haifeng and Wu, Ruiqi and Geng, Shuyi and Zhou, Yi and Shao, Ling and Yang, Yi and Gao, Bojun and Li, Qun and Wu, Guobin},
	journal={IEEE Transactions on Intelligent Vehicles}, 
	title={Delving Into Multi-Modal Multi-Task Foundation Models for Road Scene Understanding: From Learning Paradigm Perspectives}, 
	year={2024},
	volume={},
	number={},
	pages={1-25},
	keywords={Task analysis;Roads;Multitasking;Data models;Computer architecture;Surveys;Visualization;Foundation Model;Visual Understanding;Multi-modal Learning;Multi-task Learning;Road Scene},
	doi={10.1109/TIV.2024.3406372}}

@ARTICLE{ref30-surveyofModularLLMRL,
	author={Cao, Yuji and Zhao, Huan and Cheng, Yuheng and Shu, Ting and Chen, Yue and Liu, Guolong and Liang, Gaoqi and Zhao, Junhua and Yan, Jinyue and Li, Yun},
	journal={IEEE Transactions on Neural Networks and Learning Systems}, 
	title={Survey on Large Language Model-Enhanced Reinforcement Learning: Concept, Taxonomy, and Methods}, 
	year={2024},
	volume={},
	number={},
	pages={1-21},
	keywords={Taxonomy;Surveys;Visualization;Reviews;Planning;Reinforcement learning;Games;Transformers;Natural language processing;Large language models;Large language models (LLMs);LLM-enhanced reinforcement learning (RL);multimodal RL;RL;vision-language models (VLMs)},
	doi={10.1109/TNNLS.2024.3497992}}

@misc{ref31-Blip2,
	title={BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models}, 
	author={Junnan Li and Dongxu Li and Silvio Savarese and Steven Hoi},
	year={2023},
	eprint={2301.12597},
	archivePrefix={arXiv},
	primaryClass={cs.CV},
	url={https://arxiv.org/abs/2301.12597}, 
}

@misc{ref32-LLAVAnext,
	title={LLaVA-NeXT-Interleave: Tackling Multi-image, Video, and 3D in Large Multimodal Models}, 
	author={Feng Li and Renrui Zhang and Hao Zhang and Yuanhan Zhang and Bo Li and Wei Li and Zejun Ma and Chunyuan Li},
	year={2024},
	eprint={2407.07895},
	archivePrefix={arXiv},
	primaryClass={cs.CV},
	url={https://arxiv.org/abs/2407.07895}, 
}

@misc{ref33-saycan,
	title={Do As I Can, Not As I Say: Grounding Language in Robotic Affordances}, 
	author={Michael Ahn and Anthony Brohan and Noah Brown...},
	year={2022},
	eprint={2204.01691},
	archivePrefix={arXiv},
	primaryClass={cs.RO},
	url={https://arxiv.org/abs/2204.01691}, 
}

@misc{ref34-xcomposer,
	title={InternLM-XComposer-2.5: A Versatile Large Vision Language Model Supporting Long-Contextual Input and Output}, 
	author={Pan Zhang and Xiaoyi Dong and Yuhang Zang...},
	year={2024},
	eprint={2407.03320},
	archivePrefix={arXiv},
	primaryClass={cs.CV},
	url={https://arxiv.org/abs/2407.03320}, 
}

@misc{ref35-minicpm,
	title={MiniCPM: Unveiling the Potential of Small Language Models with Scalable Training Strategies}, 
	author={Shengding Hu and Yuge Tu and Xu Han...},
	year={2024},
	eprint={2404.06395},
	archivePrefix={arXiv},
	primaryClass={cs.CL},
	url={https://arxiv.org/abs/2404.06395}, 
}

@ARTICLE{ref4-VLM-MSGM,
	author={Su, Ke and Zhang, Xingxing and Zhang, Siyang and Zhu, Jun and Zhang, Bo},
	journal={IEEE Transactions on Image Processing}, 
	title={To Boost Zero-Shot Generalization for Embodied Reasoning With Vision-Language Pre-Training}, 
	year={2024},
	volume={33},
	number={},
	pages={5370-5381},
	keywords={Cognition;Visualization;Artificial intelligence;Training;Three-dimensional displays;Image reconstruction;Navigation;Embodied artificial intelligence;embodied reasoning;zero-shot generalization;vision-language pre-training},
	doi={10.1109/TIP.2024.3459800}}

@INPROCEEDINGS{ref40-VLMap,
	author={Huang, Chenguang and Mees, Oier and Zeng, Andy and Burgard, Wolfram},
	booktitle={2023 IEEE International Conference on Robotics and Automation (ICRA)}, 
	title={Visual Language Maps for Robot Navigation}, 
	year={2023},
	volume={},
	number={},
	pages={10608-10615},
	keywords={Meters;Visualization;Three-dimensional displays;TV;Navigation;Grounding;Natural languages},
	doi={10.1109/ICRA48891.2023.10160969}}

@inproceedings{ref41-ViNG,
	title={ViNG: Learning Open-World Navigation with Visual Goals},
	url={http://dx.doi.org/10.1109/ICRA48506.2021.9561936},
	DOI={10.1109/icra48506.2021.9561936},
	booktitle={2021 IEEE International Conference on Robotics and Automation (ICRA)},
	publisher={IEEE},
	author={Shah, Dhruv and Eysenbach, Benjamin and Kahn, Gregory and Rhinehart, Nicholas and Levine, Sergey},
	year={2021},
	month=may }

@misc{ref42-Palm-E,
	title={PaLM-E: An Embodied Multimodal Language Model}, 
	author={Danny Driess and Fei Xia and Mehdi S. M. Sajjadi and Corey Lynch and Aakanksha Chowdhery and Brian Ichter and Ayzaan Wahid and Jonathan Tompson and Quan Vuong and Tianhe Yu and Wenlong Huang and Yevgen Chebotar and Pierre Sermanet and Daniel Duckworth and Sergey Levine and Vincent Vanhoucke and Karol Hausman and Marc Toussaint and Klaus Greff and Andy Zeng and Igor Mordatch and Pete Florence},
	year={2023},
	eprint={2303.03378},
	archivePrefix={arXiv},
	primaryClass={cs.LG},
	url={https://arxiv.org/abs/2303.03378}, 
}

@INPROCEEDINGS{ref43-cotRobot,
	author={Sun, Lingfeng and Jha, Devesh K. and Hori, Chiori and Jain, Siddarth and Corcodel, Radu and Zhu, Xinghao and Tomizuka, Masayoshi and Romeres, Diego},
	booktitle={2024 IEEE International Conference on Robotics and Automation (ICRA)}, 
	title={Interactive Planning Using Large Language Models for Partially Observable Robotic Tasks}, 
	year={2024},
	volume={},
	number={},
	pages={14054-14061},
	keywords={Vocabulary;Uncertainty;Large language models;Cognition;Planning;Task analysis;Robots},
	doi={10.1109/ICRA57147.2024.10610981}}

