@ARTICLE{ref1-smartmanu,
	author={Ren, Lei and Dong, Jiabao and Liu, Shuai and Zhang, Lin and Wang, Lihui},
	journal={IEEE/ASME Transactions on Mechatronics}, 
	title={Embodied Intelligence Toward Future Smart Manufacturing in the Era of AI Foundation Model}, 
	year={2024},
	volume={},
	number={},
	pages={1-11},
	keywords={Production;Artificial intelligence;Smart manufacturing;Cognition;Manufacturing;Process control;Production facilities;Collaboration;Decision making;Planning;Artificial Intelligence (AI) foundation model;embodied intelligence;multimodal model;robotics;smart manufacturing},
	doi={10.1109/TMECH.2024.3456250}}


@article{ref2-visnav,
	title = {A survey of visual navigation: From geometry to embodied AI},
	journal = {Engineering Applications of Artificial Intelligence},
	volume = {114},
	pages = {105036},
	year = {2022},
	issn = {0952-1976},
	doi = {https://doi.org/10.1016/j.engappai.2022.105036},
	url = {https://www.sciencedirect.com/science/article/pii/S095219762200207X},
	author = {Tianyao Zhang and Xiaoguang Hu and Jin Xiao and Guofeng Zhang}
}

@ARTICLE{ref3-autodrive,
	author={Luo, Sheng and Chen, Wei and Tian, Wanxin and Liu, Rui and Hou, Luanxuan and Zhang, Xiubao and Shen, Haifeng and Wu, Ruiqi and Geng, Shuyi and Zhou, Yi and Shao, Ling and Yang, Yi and Gao, Bojun and Li, Qun and Wu, Guobin},
	journal={IEEE Transactions on Intelligent Vehicles}, 
	title={Delving Into Multi-Modal Multi-Task Foundation Models for Road Scene Understanding: From Learning Paradigm Perspectives}, 
	year={2024},
	volume={},
	number={},
	pages={1-25},
	keywords={Task analysis;Roads;Multitasking;Data models;Computer architecture;Surveys;Visualization;Foundation Model;Visual Understanding;Multi-modal Learning;Multi-task Learning;Road Scene},
	doi={10.1109/TIV.2024.3406372}}

@ARTICLE{ref4-VLM-MSGM,
	author={Su, Ke and Zhang, Xingxing and Zhang, Siyang and Zhu, Jun and Zhang, Bo},
	journal={IEEE Transactions on Image Processing}, 
	title={To Boost Zero-Shot Generalization for Embodied Reasoning With Vision-Language Pre-Training}, 
	year={2024},
	volume={33},
	number={},
	pages={5370-5381},
	keywords={Cognition;Visualization;Artificial intelligence;Training;Three-dimensional displays;Image reconstruction;Navigation;Embodied artificial intelligence;embodied reasoning;zero-shot generalization;vision-language pre-training},
	doi={10.1109/TIP.2024.3459800}}
	
@ARTICLE{ref5-industry,
	author={Xiang, Wei and Yu, Kan and Han, Fengling and Fang, Le and He, Dehua and Han, Qing-Long},
	journal={IEEE Transactions on Industrial Informatics}, 
	title={Advanced Manufacturing in Industry 5.0: A Survey of Key Enabling Technologies and Future Trends}, 
	year={2024},
	volume={20},
	number={2},
	pages={1055-1068},
	keywords={Surveys;Service robots;5G mobile communication;Collaboration;Systems architecture;Fourth Industrial Revolution;Fifth Industrial Revolution;Advanced manufacturing;artificial intelligence of things (AIoT);beyond 5G communications;collaborative robots (CoBots);digital twin;industrial metaverse;Industry 5.0},
	doi={10.1109/TII.2023.3274224}}

@ARTICLE{ref6-cvinmanu, 
	author={Zhou, Longfei and Zhang, Lin and Konz, Nicholas},
	journal={IEEE Transactions on Systems, Man, and Cybernetics: Systems}, 
	title={Computer Vision Techniques in Manufacturing}, 
	year={2023},
	volume={53},
	number={1},
	pages={105-117},
	keywords={Image edge detection;Image segmentation;Task analysis;Robot sensing systems;Sensors;Feature detection;Three-dimensional displays;Assembly;computer vision (CV);deep learning;inspection;machine intelligence;machine learning;manufacturing;production;robotics;survey},
	doi={10.1109/TSMC.2022.3166397}}

@ARTICLE{ref7-3dinmanu,
	author={Cong, Yang and Chen, Ronghan and Ma, Bingtao and Liu, Hongsen and Hou, Dongdong and Yang, Chenguang},
	journal={IEEE Transactions on Cybernetics}, 
	title={A Comprehensive Study of 3-D Vision-Based Robot Manipulation}, 
	year={2023},
	volume={53},
	number={3},
	pages={1682-1698},
	keywords={Robots;Service robots;Grasping;Data acquisition;Pose estimation;Force;Cameras;3-D object recognition;grasping estimation;motion planning;pose estimation;robot manipulation},
	doi={10.1109/TCYB.2021.3108165}}

@ARTICLE{ref8-industry5.0,
	author={Tallat, Raiha and Hawbani, Ammar and Wang, Xingfu and Al-Dubai, Ahmed and Zhao, Liang and Liu, Zhi and Min, Geyong and Zomaya, Albert Y. and Hamood Alsamhi, Saeed},
	journal={IEEE Communications Surveys \& Tutorials}, 
	title={Navigating Industry 5.0: A Survey of Key Enabling Technologies, Trends, Challenges, and Opportunities}, 
	year={2024},
	volume={26},
	number={2},
	pages={1080-1126},
	keywords={Industries;Fourth Industrial Revolution;Service robots;Artificial intelligence;Surveys;Robot sensing systems;Industrial Internet of Things;Industry 5.0;industry 4.0;digital twin;federated learning;Industrial Internet of Things (IIoT);industrial wireless sensor networks;Internet of Robotic Things (IoRT);blockchain;6G;intelligent sensing},
	doi={10.1109/COMST.2023.3329472}}

@ARTICLE{ref9-tampsurvey,
	author={Zhao, Zhigen and Cheng, Shuo and Ding, Yan and Zhou, Ziyi and Zhang, Shiqi and Xu, Danfei and Zhao, Ye},
	journal={IEEE/ASME Transactions on Mechatronics}, 
	title={A Survey of Optimization-Based Task and Motion Planning: From Classical to Learning Approaches}, 
	year={2024},
	volume={},
	number={},
	pages={1-27},
	keywords={Planning;Robots;Surveys;Optimization;Linear programming;Dynamics;Mechatronics;Large language models;Heuristic algorithms;Visualization;AI planning;large language models (LLMs);robot learning;task and motion planning (TAMP);temporal logic;trajectory optimization (TO)},
	doi={10.1109/TMECH.2024.3452509}}
	
@ARTICLE{ref10-3ddetectcar,
	author={Wang, Li and Zhang, Xinyu and Song, Ziying and Bi, Jiangfeng and Zhang, Guoxin and Wei, Haiyue and Tang, Liyao and Yang, Lei and Li, Jun and Jia, Caiyan and Zhao, Lijun},
	journal={IEEE Transactions on Intelligent Vehicles}, 
	title={Multi-Modal 3D Object Detection in Autonomous Driving: A Survey and Taxonomy}, 
	year={2023},
	volume={8},
	number={7},
	pages={3781-3798},
	keywords={Three-dimensional displays;Object detection;Sensors;Laser radar;Point cloud compression;Autonomous vehicles;Cameras;Autonomous driving;3D object detection;multi-modal fusion},
	doi={10.1109/TIV.2023.3264658}}
	
@ARTICLE{ref11-dlfusion,
	author={Wang, Li and Zhang, Xinyu and Song, Ziying and Bi, Jiangfeng and Zhang, Guoxin and Wei, Haiyue and Tang, Liyao and Yang, Lei and Li, Jun and Jia, Caiyan and Zhao, Lijun},
	journal={IEEE Transactions on Intelligent Vehicles}, 
	title={Multi-Modal 3D Object Detection in Autonomous Driving: A Survey and Taxonomy}, 
	year={2023},
	volume={8},
	number={7},
	pages={3781-3798},
	keywords={Three-dimensional displays;Object detection;Sensors;Laser radar;Point cloud compression;Autonomous vehicles;Cameras;Autonomous driving;3D object detection;multi-modal fusion},
	doi={10.1109/TIV.2023.3264658}}
	
@ARTICLE{ref12-mmfx,
	author={Zhang, Jiaming and Liu, Huayao and Yang, Kailun and Hu, Xinxin and Liu, Ruiping and Stiefelhagen, Rainer},
	journal={IEEE Transactions on Intelligent Transportation Systems}, 
	title={CMX: Cross-Modal Fusion for RGB-X Semantic Segmentation With Transformers}, 
	year={2023},
	volume={24},
	number={12},
	pages={14679-14694},
	keywords={Semantic segmentation;Feature extraction;Sensors;Transformers;Task analysis;Benchmark testing;Semantics;Semantic segmentation;scene parsing;cross-modal fusion;vision transformers;scene understanding},
	doi={10.1109/TITS.2023.3300537}}
	
@ARTICLE{ref13-RGBDconstraintMapping,
	author={Shu, Chengfu and Luo, Yutao},
	journal={IEEE Transactions on Intelligent Vehicles}, 
	title={Multi-Modal Feature Constraint Based Tightly Coupled Monocular Visual-LiDAR Odometry and Mapping}, 
	year={2023},
	volume={8},
	number={5},
	pages={3384-3393},
	keywords={Laser radar;Feature extraction;Visualization;Cameras;Point cloud compression;Sensors;Optimization;Mapping;multi-modal feature constraints;sensor fusion;voxel map},
	doi={10.1109/TIV.2022.3215141}}

@INPROCEEDINGS{ref14-LLM-BT,
	author={Zhou, Haotian and Lin, Yunhan and Yan, Longwu and Zhu, Jihong and Min, Huasong},
	booktitle={2024 IEEE International Conference on Robotics and Automation (ICRA)}, 
	title={LLM-BT: Performing Robotic Adaptive Tasks based on Large Language Models and Behavior Trees}, 
	year={2024},
	volume={},
	number={},
	pages={16655-16661},
	keywords={Large language models;Heuristic algorithms;Semantics;Bidirectional control;Chatbots;Transformers;Encoding},
	doi={10.1109/ICRA57147.2024.10610183}}

@INPROCEEDINGS{ref15-LLM3DA,
	author={Chen, Sijin and Chen, Xin and Zhang, Chi and Li, Mingsheng and Yu, Gang and Fei, Hao and Zhu, Hongyuan and Fan, Jiayuan and Chen, Tao},
	booktitle={2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 
	title={LL3DA: Visual Interactive Instruction Tuning for Omni-3D Understanding, Reasoning, and Planning}, 
	year={2024},
	volume={},
	number={},
	pages={26418-26428},
	keywords={Point cloud compression;Training;Visualization;Solid modeling;Three-dimensional displays;Computational modeling;Cognition;Multi-modal learning;large language models;vision and language},
	doi={10.1109/CVPR52733.2024.02496}}
	
@INPROCEEDINGS{ref16-CLIPFO3D,
	author={Zhang, Junbo and Dong, Runpei and Ma, Kaisheng},
	booktitle={2023 IEEE/CVF International Conference on Computer Vision Workshops (ICCVW)}, 
	title={CLIP-FO3D: Learning Free Open-world 3D Scene Representations from 2D Dense CLIP}, 
	year={2023},
	volume={},
	number={},
	pages={2040-2051},
	keywords={Training;Solid modeling;Adaptation models;Three-dimensional displays;Annotations;Semantic segmentation;Computational modeling},
	doi={10.1109/ICCVW60793.2023.00219}}

@ARTICLE{ref17-drivegpt,
	author={Xu, Zhenhua and Zhang, Yujia and Xie, Enze and Zhao, Zhen and Guo, Yong and Wong, Kwan-Yee K. and Li, Zhenguo and Zhao, Hengshuang},
	journal={IEEE Robotics and Automation Letters}, 
	title={DriveGPT4: Interpretable End-to-End Autonomous Driving Via Large Language Model}, 
	year={2024},
	volume={9},
	number={10},
	pages={8186-8193},
	keywords={Autonomous vehicles;Videos;Chatbots;Visualization;Cognition;Turning;Tuning;Autonomous driving;large language model},
	doi={10.1109/LRA.2024.3440097}}

@INPROCEEDINGS{ref18-progprompt,
	author={Singh, Ishika and Blukis, Valts and Mousavian, Arsalan and Goyal, Ankit and Xu, Danfei and Tremblay, Jonathan and Fox, Dieter and Thomason, Jesse and Garg, Animesh},
	booktitle={2023 IEEE International Conference on Robotics and Automation (ICRA)}, 
	title={ProgPrompt: Generating Situated Robot Task Plans using Large Language Models}, 
	year={2023},
	volume={},
	number={},
	pages={11523-11530},
	keywords={Automation;Natural languages;Manipulators;Planning;Task analysis},
	doi={10.1109/ICRA48891.2023.10161317}}

@inproceedings{ref19-3dllm,
	author = {Hong, Yining and Zhen, Haoyu and Chen, Peihao and Zheng, Shuhong and Du, Yilun and Chen, Zhenfang and Gan, Chuang},
	title = {3D-LLM: injecting the 3D world into large language models},
	year = {2024},
	publisher = {Curran Associates Inc.},
	address = {Red Hook, NY, USA},
	abstract = {Large language models (LLMs) and Vision-Language Models (VLMs) have been proven to excel at multiple tasks, such as commonsense reasoning. Powerful as these models can be, they are not grounded in the 3D physical world, which involves richer concepts such as spatial relationships, affordances, physics, layout, and so on. In this work, we propose to inject the 3D world into large language models and introduce a whole new family of 3D-LLMs. Specifically, 3D-LLMs can take 3D point clouds and their features as input and perform a diverse set of 3D-related tasks, including captioning, dense captioning, 3D question answering, task decomposition, 3D grounding, 3D-assisted dialog, navigation, and so on. Using three types of prompting mechanisms that we design, we are able to collect over 1M 3D-language data covering these tasks. To efficiently train 3D-LLMs, we first utilize a 3D feature extractor that obtains 3D features from rendered multi-view images. Then, we use 2D VLMs as our backbones to train our 3D-LLMs. By introducing a 3D localization mechanism, 3D-LLMs can better capture 3D spatial information. Experiments on held-out evaluation dataset, ScanQA, SQA3D and 3DMV-VQA, outperform state-of-the-art baselines. In particular, experiments on ScanQA show that our model outperforms state-of-the-art baselines by a large margin (e.g., the BLEU-1 score surpasses state-of-the-art score by 9\%). Furthermore, experiments on our held-in datasets for 3D captioning, task composition, and 3D-assisted dialogue show that our model outperforms 2D VLMs. Qualitative examples also show that our model could perform more tasks beyond the scope of existing LLMs and VLMs. Project Page: : https://vis-www.cs.umass.edu/3dllm/.},
	booktitle = {Proceedings of the 37th International Conference on Neural Information Processing Systems},
	articleno = {900},
	numpages = {13},
	location = {New Orleans, LA, USA},
	series = {NIPS '23}
}

@misc{ref20-llmi3d,
	title={LLMI3D: Empowering LLM with 3D Perception from a Single 2D Image}, 
	author={Fan Yang and Sicheng Zhao and Yanhao Zhang and Haoxiang Chen and Hui Chen and Wenbo Tang and Haonan Lu and Pengfei Xu and Zhenyu Yang and Jungong Han and Guiguang Ding},
	year={2024},
	eprint={2408.07422},
	archivePrefix={arXiv},
	primaryClass={cs.CV},
	url={https://arxiv.org/abs/2408.07422}, 
}

@misc{ref21-rdt1b,
	title={RDT-1B: a Diffusion Foundation Model for Bimanual Manipulation}, 
	author={Songming Liu and Lingxuan Wu and Bangguo Li and Hengkai Tan and Huayu Chen and Zhengyi Wang and Ke Xu and Hang Su and Jun Zhu},
	year={2024},
	eprint={2410.07864},
	archivePrefix={arXiv},
	primaryClass={cs.RO},
	url={https://arxiv.org/abs/2410.07864}, 
}

@ARTICLE{ref22-databaseprompt,
	author={Xiao, Bin and Kantarci, Burak and Kang, Jiawen and Niyato, Dusit and Guizani, Mohsen},
	journal={IEEE Internet of Things Journal}, 
	title={Efficient Prompting for LLM-Based Generative Internet of Things}, 
	year={2025},
	volume={12},
	number={1},
	pages={778-791},
	keywords={Internet of Things;Cognition;Python;Artificial intelligence;Structured Query Language;Training;Servers;Question answering (information retrieval);Security;Performance evaluation;Generative Internet of Things (GIoT);large language model (LLM);prompt engineering;table question answering (Table-QA)},
	doi={10.1109/JIOT.2024.3470210}}
	
@misc{ref23-semanticAb, 
	author={Huy Ha and Shuran Song},
	year={2022},
	eprint={2207.11514},
	archivePrefix={arXiv},
	primaryClass={cs.CV},
	url={https://arxiv.org/abs/2207.11514}, 
}

@misc{ref24-MoPE,
	title={MoPE: Parameter-Efficient and Scalable Multimodal Fusion via Mixture of Prompt Experts}, 
	author={Ruixiang Jiang and Lingbo Liu and Changwen Chen},
	year={2024},
	eprint={2403.10568},
	archivePrefix={arXiv},
	primaryClass={cs.LG},
	url={https://arxiv.org/abs/2403.10568}, 
}

@misc{ref25-rt1,
	title={RT-1: Robotics Transformer for Real-World Control at Scale}, 
	author={Anthony Brohan and Noah Brown and Justice Carbajal...},
	year={2023},
	eprint={2212.06817},
	archivePrefix={arXiv},
	primaryClass={cs.RO},
	url={https://arxiv.org/abs/2212.06817}, 
}

@misc{ref26-rt2,
	title={RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control}, 
	author={Anthony Brohan and Noah Brown and Justice Carbajal...},
	year={2023},
	eprint={2307.15818},
	archivePrefix={arXiv},
	primaryClass={cs.RO},
	url={https://arxiv.org/abs/2307.15818}, 
}

@misc{ref27-flamingo,
	title={Vision-Language Foundation Models as Effective Robot Imitators}, 
	author={Xinghang Li and Minghuan Liu and Hanbo Zhang and Cunjun Yu and Jie Xu and Hongtao Wu and Chilam Cheang and Ya Jing and Weinan Zhang and Huaping Liu and Hang Li and Tao Kong},
	year={2024},
	eprint={2311.01378},
	archivePrefix={arXiv},
	primaryClass={cs.RO},
	url={https://arxiv.org/abs/2311.01378}, 
}


@misc{ref28-clipgrasp,
	title={Language-guided Robot Grasping: CLIP-based Referring Grasp Synthesis in Clutter}, 
	author={Georgios Tziafas and Yucheng Xu and Arushi Goel and Mohammadreza Kasaei and Zhibin Li and Hamidreza Kasaei},
	year={2023},
	eprint={2311.05779},
	archivePrefix={arXiv},
	primaryClass={cs.RO},
	url={https://arxiv.org/abs/2311.05779}, 
}

@INPROCEEDINGS{ref29-clipFO3D,
	author={Zhang, Junbo and Dong, Runpei and Ma, Kaisheng},
	booktitle={2023 IEEE/CVF International Conference on Computer Vision Workshops (ICCVW)}, 
	title={CLIP-FO3D: Learning Free Open-world 3D Scene Representations from 2D Dense CLIP}, 
	year={2023},
	volume={},
	number={},
	pages={2040-2051},
	keywords={Training;Solid modeling;Adaptation models;Three-dimensional displays;Annotations;Semantic segmentation;Computational modeling},
	doi={10.1109/ICCVW60793.2023.00219}}
	
@ARTICLE{ref30-surveyofModularLLMRL,
	author={Cao, Yuji and Zhao, Huan and Cheng, Yuheng and Shu, Ting and Chen, Yue and Liu, Guolong and Liang, Gaoqi and Zhao, Junhua and Yan, Jinyue and Li, Yun},
	journal={IEEE Transactions on Neural Networks and Learning Systems}, 
	title={Survey on Large Language Model-Enhanced Reinforcement Learning: Concept, Taxonomy, and Methods}, 
	year={2024},
	volume={},
	number={},
	pages={1-21},
	keywords={Taxonomy;Surveys;Visualization;Reviews;Planning;Reinforcement learning;Games;Transformers;Natural language processing;Large language models;Large language models (LLMs);LLM-enhanced reinforcement learning (RL);multimodal RL;RL;vision-language models (VLMs)},
	doi={10.1109/TNNLS.2024.3497992}}

@misc{ref31-Blip2,
	title={BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models}, 
	author={Junnan Li and Dongxu Li and Silvio Savarese and Steven Hoi},
	year={2023},
	eprint={2301.12597},
	archivePrefix={arXiv},
	primaryClass={cs.CV},
	url={https://arxiv.org/abs/2301.12597}, 
}

@misc{ref32-LLAVAnext,
	title={LLaVA-NeXT-Interleave: Tackling Multi-image, Video, and 3D in Large Multimodal Models}, 
	author={Feng Li and Renrui Zhang and Hao Zhang and Yuanhan Zhang and Bo Li and Wei Li and Zejun Ma and Chunyuan Li},
	year={2024},
	eprint={2407.07895},
	archivePrefix={arXiv},
	primaryClass={cs.CV},
	url={https://arxiv.org/abs/2407.07895}, 
}

@misc{ref33-saycan,
	title={Do As I Can, Not As I Say: Grounding Language in Robotic Affordances}, 
	author={Michael Ahn and Anthony Brohan and Noah Brown...},
	year={2022},
	eprint={2204.01691},
	archivePrefix={arXiv},
	primaryClass={cs.RO},
	url={https://arxiv.org/abs/2204.01691}, 
}

@misc{ref34-xcomposer,
	title={InternLM-XComposer-2.5: A Versatile Large Vision Language Model Supporting Long-Contextual Input and Output}, 
	author={Pan Zhang and Xiaoyi Dong and Yuhang Zang...},
	year={2024},
	eprint={2407.03320},
	archivePrefix={arXiv},
	primaryClass={cs.CV},
	url={https://arxiv.org/abs/2407.03320}, 
}

@misc{ref35-minicpm,
	title={MiniCPM: Unveiling the Potential of Small Language Models with Scalable Training Strategies}, 
	author={Shengding Hu and Yuge Tu and Xu Han...},
	year={2024},
	eprint={2404.06395},
	archivePrefix={arXiv},
	primaryClass={cs.CL},
	url={https://arxiv.org/abs/2404.06395}, 
}

@misc{ref36-lora,
	title={LoRA: Low-Rank Adaptation of Large Language Models}, 
	author={Edward J. Hu and Yelong Shen and Phillip Wallis and Zeyuan Allen-Zhu and Yuanzhi Li and Shean Wang and Lu Wang and Weizhu Chen},
	year={2021},
	eprint={2106.09685},
	archivePrefix={arXiv},
	primaryClass={cs.CL},
	url={https://arxiv.org/abs/2106.09685}, 
}

@misc{ref37-dataset,
	title={BridgeData V2: A Dataset for Robot Learning at Scale}, 
	author={Homer Walke and Kevin Black and Abraham Lee and Moo Jin Kim and Max Du and Chongyi Zheng and Tony Zhao and Philippe Hansen-Estruch and Quan Vuong and Andre He and Vivek Myers and Kuan Fang and Chelsea Finn and Sergey Levine},
	year={2024},
	eprint={2308.12952},
	archivePrefix={arXiv},
	primaryClass={cs.RO},
	url={https://arxiv.org/abs/2308.12952}, 
}

@article{ref38-cgLLM,
	title={Large language models for human-robot interaction: A review},
	author={Ceng Zhang and Junxin Chen and Jiatong Li and Yanhong Peng and Ze-bing Mao},
	journal={Biomimetic Intelligence and Robotics},
	year={2023},
	url={https://api.semanticscholar.org/CorpusID:264564300}
}

@article{ref39-cghandoverDirection,
	title = {Human–robot object handover: Recent progress and future direction},
	journal = {Biomimetic Intelligence and Robotics},
	volume = {4},
	number = {1},
	pages = {100145},
	year = {2024},
	issn = {2667-3797},
	doi = {https://doi.org/10.1016/j.birob.2024.100145},
	url = {https://www.sciencedirect.com/science/article/pii/S2667379724000032},
	author = {Haonan Duan and Yifan Yang and Daheng Li and Peng Wang}
}

@INPROCEEDINGS{ref40-VLMap,
	author={Huang, Chenguang and Mees, Oier and Zeng, Andy and Burgard, Wolfram},
	booktitle={2023 IEEE International Conference on Robotics and Automation (ICRA)}, 
	title={Visual Language Maps for Robot Navigation}, 
	year={2023},
	volume={},
	number={},
	pages={10608-10615},
	keywords={Meters;Visualization;Three-dimensional displays;TV;Navigation;Grounding;Natural languages},
	doi={10.1109/ICRA48891.2023.10160969}}

@inproceedings{ref41-ViNG,
	title={ViNG: Learning Open-World Navigation with Visual Goals},
	url={http://dx.doi.org/10.1109/ICRA48506.2021.9561936},
	DOI={10.1109/icra48506.2021.9561936},
	booktitle={2021 IEEE International Conference on Robotics and Automation (ICRA)},
	publisher={IEEE},
	author={Shah, Dhruv and Eysenbach, Benjamin and Kahn, Gregory and Rhinehart, Nicholas and Levine, Sergey},
	year={2021},
	month=may }

@misc{ref42-Palm-E,
	title={PaLM-E: An Embodied Multimodal Language Model}, 
	author={Danny Driess and Fei Xia and Mehdi S. M. Sajjadi and Corey Lynch and Aakanksha Chowdhery and Brian Ichter and Ayzaan Wahid and Jonathan Tompson and Quan Vuong and Tianhe Yu and Wenlong Huang and Yevgen Chebotar and Pierre Sermanet and Daniel Duckworth and Sergey Levine and Vincent Vanhoucke and Karol Hausman and Marc Toussaint and Klaus Greff and Andy Zeng and Igor Mordatch and Pete Florence},
	year={2023},
	eprint={2303.03378},
	archivePrefix={arXiv},
	primaryClass={cs.LG},
	url={https://arxiv.org/abs/2303.03378}, 
}

@INPROCEEDINGS{ref43-cotRobot,
	author={Sun, Lingfeng and Jha, Devesh K. and Hori, Chiori and Jain, Siddarth and Corcodel, Radu and Zhu, Xinghao and Tomizuka, Masayoshi and Romeres, Diego},
	booktitle={2024 IEEE International Conference on Robotics and Automation (ICRA)}, 
	title={Interactive Planning Using Large Language Models for Partially Observable Robotic Tasks}, 
	year={2024},
	volume={},
	number={},
	pages={14054-14061},
	keywords={Vocabulary;Uncertainty;Large language models;Cognition;Planning;Task analysis;Robots},
	doi={10.1109/ICRA57147.2024.10610981}}

@misc{ref44-patchwork++,
      title={Patchwork++: Fast and Robust Ground Segmentation Solving Partial Under-Segmentation Using 3D Point Cloud}, 
      author={Seungjae Lee and Hyungtae Lim and Hyun Myung},
      year={2022},
      eprint={2207.11919},
      archivePrefix={arXiv},
      primaryClass={cs.RO},
      url={https://arxiv.org/abs/2207.11919}, 
}