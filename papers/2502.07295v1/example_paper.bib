@inproceedings{li2016matching,
  title={Matching via Dimensionality Reduction for Estimation of Treatment Effects in Digital Marketing Campaigns.},
  author={Li, Sheng and Vlassis, Nikos and Kawale, Jaya and Fu, Yun},
  booktitle={IJCAI},
  volume={16},
  pages={3768--3774},
  year={2016}
}

@article{glass2013causal,
  title={Causal inference in public health},
  author={Glass, Thomas A and Goodman, Steven N and Hern{\'a}n, Miguel A and Samet, Jonathan M},
  journal={Annual review of public health},
  volume={34},
  number={1},
  pages={61--75},
  year={2013},
  publisher={Annual Reviews}
}

@article{chiang2001smoothing,
  title={Smoothing spline estimation for varying coefficient models with repeatedly measured dependent variables},
  author={Chiang, Chin-Tsang and Rice, John A and Wu, Colin O},
  journal={Journal of the American Statistical Association},
  volume={96},
  number={454},
  pages={605--619},
  year={2001},
  publisher={Taylor \& Francis}
}

@article{fan1999statistical,
  title={Statistical estimation in varying coefficient models},
  author={Fan, Jianqing and Zhang, Wenyang},
  journal={The annals of Statistics},
  volume={27},
  number={5},
  pages={1491--1518},
  year={1999},
  publisher={Institute of Mathematical Statistics}
}

@article{hastie1993varying,
  title={Varying-coefficient models},
  author={Hastie, Trevor and Tibshirani, Robert},
  journal={Journal of the Royal Statistical Society Series B: Statistical Methodology},
  volume={55},
  number={4},
  pages={757--779},
  year={1993},
  publisher={Oxford University Press}
}

@inproceedings{yoon2018ganite,
  title={GANITE: Estimation of individualized treatment effects using generative adversarial nets},
  author={Yoon, Jinsung and Jordon, James and Van Der Schaar, Mihaela},
  booktitle={International conference on learning representations},
  year={2018}
}

@article{sanchez2022diffusion,
  title={Diffusion causal models for counterfactual estimation},
  author={Sanchez, Pedro and Tsaftaris, Sotirios A},
  journal={arXiv preprint arXiv:2202.10166},
  year={2022}
}

@article{louizos2017causal,
  title={Causal effect inference with deep latent-variable models},
  author={Louizos, Christos and Shalit, Uri and Mooij, Joris M and Sontag, David and Zemel, Richard and Welling, Max},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@inproceedings{assaad2021counterfactual,
  title={Counterfactual representation learning with balancing weights},
  author={Assaad, Serge and Zeng, Shuxi and Tao, Chenyang and Datta, Shounak and Mehta, Nikhil and Henao, Ricardo and Li, Fan and Carin, Lawrence},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={1972--1980},
  year={2021},
  organization={PMLR}
}

@inproceedings{hassanpour2019counterfactual,
  title={CounterFactual Regression with Importance Sampling Weights.},
  author={Hassanpour, Negar and Greiner, Russell},
  booktitle={IJCAI},
  pages={5880--5887},
  year={2019},
  organization={Macao}
}

@article{johansson2018learning,
  title={Learning weighted representations for generalization across designs},
  author={Johansson, Fredrik D and Kallus, Nathan and Shalit, Uri and Sontag, David},
  journal={arXiv preprint arXiv:1802.08598},
  year={2018}
}

@inproceedings{shalit2017estimating,
  title={Estimating individual treatment effect: generalization bounds and algorithms},
  author={Shalit, Uri and Johansson, Fredrik D and Sontag, David},
  booktitle={International conference on machine learning},
  pages={3076--3085},
  year={2017},
  organization={PMLR}
}

@inproceedings{johansson2016learning,
  title={Learning representations for counterfactual inference},
  author={Johansson, Fredrik and Shalit, Uri and Sontag, David},
  booktitle={International conference on machine learning},
  pages={3020--3029},
  year={2016},
  organization={PMLR}
}

@inproceedings{zhu2024contrastive,
  title={Contrastive balancing representation learning for heterogeneous dose-response curves estimation},
  author={Zhu, Minqin and Wu, Anpeng and Li, Haoxuan and Xiong, Ruoxuan and Li, Bo and Yang, Xiaoqing and Qin, Xuan and Zhen, Peng and Guo, Jiecheng and Wu, Fei and others},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={38},
  number={15},
  pages={17175--17183},
  year={2024}
}

@article{shi2019adapting,
  title={Adapting neural networks for the estimation of treatment effects},
  author={Shi, Claudia and Blei, David and Veitch, Victor},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@article{wager2018estimation,
  title={Estimation and inference of heterogeneous treatment effects using random forests},
  author={Wager, Stefan and Athey, Susan},
  journal={Journal of the American Statistical Association},
  volume={113},
  number={523},
  pages={1228--1242},
  year={2018},
  publisher={Taylor \& Francis}
}

@inproceedings{schwab2020learning,
  title={Learning counterfactual representations for estimating individual dose-response curves},
  author={Schwab, Patrick and Linhardt, Lorenz and Bauer, Stefan and Buhmann, Joachim M and Karlen, Walter},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={34},
  number={04},
  pages={5612--5619},
  year={2020}
}

@inproceedings{
nie2021vcnet,
title={Varying Coefficient Neural Network with Functional Targeted Regularization for Estimating Continuous Treatment Effects},
author={Lizhen Nie and Mao Ye and qiang liu and Dan Nicolae},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=RmB-88r9dL}
}

@article{bica2020estimating,
  title={Estimating the effects of continuous-valued interventions using generative adversarial networks},
  author={Bica, Ioana and Jordon, James and van der Schaar, Mihaela},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={16434--16445},
  year={2020}
}

@article{wang2022generalization,
  title={Generalization bounds for estimating causal effects of continuous treatments},
  author={Wang, Xin and Lyu, Shengfei and Wu, Xingyu and Wu, Tianhao and Chen, Huanhuan},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={8605--8617},
  year={2022}
}

@inproceedings{kazemi2024adversarially,
  title={Adversarially Balanced Representation for Continuous Treatment Effect Estimation},
  author={Kazemi, Amirreza and Ester, Martin},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={38},
  number={12},
  pages={13085--13093},
  year={2024}
}

@article{weinstein2013cancer,
  title={The cancer genome atlas pan-cancer analysis project},
  author={Weinstein, John N and Collisson, Eric A and Mills, Gordon B and Shaw, Kenna R and Ozenberger, Brad A and Ellrott, Kyle and Shmulevich, Ilya and Sander, Chris and Stuart, Joshua M},
  journal={Nature genetics},
  volume={45},
  number={10},
  pages={1113--1120},
  year={2013},
  publisher={Nature Publishing Group}
}

@misc{kennedy2023semiparametricdoublyrobusttargeted,
      title={Semiparametric doubly robust targeted double machine learning: a review}, 
      author={Edward H. Kennedy},
      year={2023},
      eprint={2203.06469},
      archivePrefix={arXiv},
      primaryClass={stat.ME},
      url={https://arxiv.org/abs/2203.06469}, 
}

@article{KenndySCDE,
    author = {Kennedy, E H and Balakrishnan, S and Wasserman, L A},
    title = {Semiparametric counterfactual density estimation},
    journal = {Biometrika},
    volume = {110},
    number = {4},
    pages = {875-896},
    year = {2023},
    month = {03},
    abstract = {Causal effects are often characterized with averages, which can give an incomplete picture of the underlying counterfactual distributions. Here we consider estimating the entire counterfactual density and generic functionals thereof. We focus on two kinds of target parameters: density approximations and the distance between counterfactual densities. We study nonparametric efficiency bounds, giving results for smooth but otherwise generic models and distances. Importantly, we show how these bounds connect to means of particular nontrivial functions of counterfactuals, linking the problems of density and mean estimation. We propose doubly robust-style estimators, and study their rates of convergence, showing that they can be optimally efficient in large nonparametric models. We also give analogous methods for model selection and aggregation, when many models may be available and of interest. Our results all hold for generic models and distances, but we highlight results for L2 projections on linear models and Kullbach–Leibler projections on exponential families. Finally, we illustrate our method by estimating the density of the CD4 count among patients with HIV, had all been treated with combination therapy versus zidovudine alone, as well as a density effect. Our methods are implemented in the R package npcausal on GitHub.},
    issn = {1464-3510},
    doi = {10.1093/biomet/asad017},
    url = {https://doi.org/10.1093/biomet/asad017},
    eprint = {https://academic.oup.com/biomet/article-pdf/110/4/875/53471755/asad017.pdf},
}

@incollection{vandervaart2002semiparametric,
  title={Semiparametric statistics},
  author={van der Vaart, Aad W.},
  booktitle={Lectures on Probability Theory and Statistics},
  pages={331--457},
  year={2002},
  publisher={Springer},
  series={Lecture Notes in Mathematics},
  volume={1781},
  doi={10.1007/978-3-540-45744-8_4}
}

@article{Chernozhukov2017,
Author = {Chernozhukov, Victor and Chetverikov, Denis and Demirer, Mert and Duflo, Esther and Hansen, Christian and Newey, Whitney},
Title = {Double/Debiased/Neyman Machine Learning of Treatment Effects},
Journal = {American Economic Review},
Volume = {107},
Number = {5},
Year = {2017},
Month = {May},
Pages = {261–65},
DOI = {10.1257/aer.p20171038},
URL = {https://www.aeaweb.org/articles?id=10.1257/aer.p20171038}}

@article{Chernozhukov2018,
    author = {Chernozhukov, Victor and Chetverikov, Denis and Demirer, Mert and Duflo, Esther and Hansen, Christian and Newey, Whitney and Robins, James},
    title = {Double/debiased machine learning for treatment and structural parameters},
    journal = {The Econometrics Journal},
    volume = {21},
    number = {1},
    pages = {C1-C68},
    year = {2018},
    month = {01},
    abstract = {We revisit the classic semi‐parametric problem of inference on a low‐dimensional parameter θ0 in the presence of high‐dimensional nuisance parameters η0. We depart from the classical setting by allowing for η0 to be so high‐dimensional that the traditional assumptions (e.g. Donsker properties) that limit complexity of the parameter space for this object break down. To estimate η0, we consider the use of statistical or machine learning (ML) methods, which are particularly well suited to estimation in modern, very high‐dimensional cases. ML methods perform well by employing regularization to reduce variance and trading off regularization bias with overfitting in practice. However, both regularization bias and overfitting in estimating η0 cause a heavy bias in estimators of θ0 that are obtained by naively plugging ML estimators of η0 into estimating equations for θ0. This bias results in the naive estimator failing to be N−1/2 consistent, where N is the sample size. We show that the impact of regularization bias and overfitting on estimation of the parameter of interest θ0 can be removed by using two simple, yet critical, ingredients: (1) using Neyman‐orthogonal moments/scores that have reduced sensitivity with respect to nuisance parameters to estimate θ0; (2) making use of cross‐fitting, which provides an efficient form of data‐splitting. We call the resulting set of methods double or debiased ML (DML). We verify that DML delivers point estimators that concentrate in an N−1/2‐neighbourhood of the true parameter values and are approximately unbiased and normally distributed, which allows construction of valid confidence statements. The generic statistical theory of DML is elementary and simultaneously relies on only weak theoretical requirements, which will admit the use of a broad array of modern ML methods for estimating the nuisance parameters, such as random forests, lasso, ridge, deep neural nets, boosted trees, and various hybrids and ensembles of these methods. We illustrate the general theory by applying it to provide theoretical properties of the following: DML applied to learn the main regression parameter in a partially linear regression model; DML applied to learn the coefficient on an endogenous variable in a partially linear instrumental variables model; DML applied to learn the average treatment effect and the average treatment effect on the treated under unconfoundedness; DML applied to learn the local average treatment effect in an instrumental variables setting. In addition to these theoretical applications, we also illustrate the use of DML in three empirical examples.},
    issn = {1368-4221},
    doi = {10.1111/ectj.12097},
    url = {https://doi.org/10.1111/ectj.12097},
    eprint = {https://academic.oup.com/ectj/article-pdf/21/1/C1/27684918/ectj00c1.pdf},
}

@book{van2011targeted,
  title={Targeted Learning: Causal Inference for Observational and Experimental Data},
  author={van der Laan, M.J. and Rose, S.},
  isbn={9781441997821},
  lccn={2011930854},
  series={Springer Series in Statistics},
  url={https://books.google.com.hk/books?id=RGnSX5aCAgQC},
  year={2011},
  publisher={Springer New York}
}

@article{NieRLearner,
    author = {Nie, X and Wager, S},
    title = {Quasi-oracle estimation of heterogeneous treatment effects},
    journal = {Biometrika},
    volume = {108},
    number = {2},
    pages = {299-319},
    year = {2020},
    month = {09},
    abstract = {Flexible estimation of heterogeneous treatment effects lies at the heart of many statistical applications, such as personalized medicine and optimal resource allocation. In this article we develop a general class of two-step algorithms for heterogeneous treatment effect estimation in observational studies. First, we estimate marginal effects and treatment propensities to form an objective function that isolates the causal component of the signal. Then, we optimize this data-adaptive objective function. The proposed approach has several advantages over existing methods. From a practical perspective, our method is flexible and easy to use: in both steps, any loss-minimization method can be employed, such as penalized regression, deep neural networks, or boosting; moreover, these methods can be fine-tuned by cross-validation. Meanwhile, in the case of penalized kernel regression, we show that our method has a quasi-oracle property. Even when the pilot estimates for marginal effects and treatment propensities are not particularly accurate, we achieve the same error bounds as an oracle with prior knowledge of these two nuisance components. We implement variants of our approach based on penalized regression, kernel ridge regression, and boosting in a variety of simulation set-ups, and observe promising performance relative to existing baselines.},
    issn = {0006-3444},
    doi = {10.1093/biomet/asaa076},
    url = {https://doi.org/10.1093/biomet/asaa076},
    eprint = {https://academic.oup.com/biomet/article-pdf/108/2/299/37938939/asaa076.pdf},
}

@misc{gao2022estimatingheterogeneoustreatmenteffects,
      title={Estimating Heterogeneous Treatment Effects for General Responses}, 
      author={Zijun Gao and Trevor Hastie},
      year={2022},
      eprint={2103.04277},
      archivePrefix={arXiv},
      primaryClass={stat.ME},
      url={https://arxiv.org/abs/2103.04277}, 
}

@article{Luedtke2024,
author = {Alex Luedtke and Incheoul Chung},
title = {{One-step estimation of differentiable Hilbert-valued parameters}},
volume = {52},
journal = {The Annals of Statistics},
number = {4},
publisher = {Institute of Mathematical Statistics},
pages = {1534 -- 1563},
keywords = {conﬁdence set, Hilbert-valued parameter, nonparametric, one-step estimation},
year = {2024},
doi = {10.1214/24-AOS2403},
URL = {https://doi.org/10.1214/24-AOS2403}
}

@book{Wuthrich2022,
title = {Statistical Foundations of Actuarial Learning and its Applications},
author = {W\"uthrich, Mario V. and Merz, Michael},
year = {2022},
month = jun,
day = {3},
publisher = {Springer Actuarial},
doi = {10.1007/978-3-031-12409-9},
url = {https://link.springer.com/book/10.1007/978-3-031-12409-9},
ssrn = {https://ssrn.com/abstract=3822407},
abstract = {Available at SSRN: https://ssrn.com/abstract=3822407 or http://dx.doi.org/10.2139/ssrn.3822407}
}

@article{Farrell2021,
author = {Max H. Farrell AND Tengyuan Liang AND Sanjog Misra},
title = {Deep Neural Networks for Estimation and Inference},
journal = {Econometrica},
volume = {89},
number = {1},
pages = {181-213},
doi = {https://doi.org/10.3982/ECTA16901},
url = {https://onlinelibrary.wiley.com/doi/abs/10.3982/ECTA16901},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.3982/ECTA16901},
abstract = {<p>We study deep neural networks and their use in semiparametric inference. We establish novel nonasymptotic high probability bounds for deep feedforward neural nets. These deliver rates of convergence that are sufficiently fast (in some cases minimax optimal) to allow us to establish valid second‐step inference after first‐step estimation with deep learning, a result also new to the literature. Our nonasymptotic high probability bounds, and the subsequent semiparametric inference, treat the current standard architecture: fully connected feedforward neural networks (multilayer perceptrons), with the now‐common rectified linear unit activation function, unbounded weights, and a depth explicitly diverging with the sample size. We discuss other architectures as well, including fixed‐width, very deep networks. We establish the nonasymptotic bounds for these deep nets for a general class of nonparametric regression‐type loss functions, which includes as special cases least squares, logistic regression, and other generalized linear models. We then apply our theory to develop semiparametric inference, focusing on causal parameters for concreteness, and demonstrate the effectiveness of deep learning with an empirical application to direct mail marketing.</p>},
year = {2021}
}

@article{KennedyDR2023,
author = {Edward H. Kennedy},
title = {{Towards optimal doubly robust estimation of heterogeneous causal effects}},
volume = {17},
journal = {Electronic Journal of Statistics},
number = {2},
publisher = {Institute of Mathematical Statistics and Bernoulli Society},
pages = {3008 -- 3049},
keywords = {Conditional effects, influence function, Minimax rate, Nonparametric regression},
year = {2023},
doi = {10.1214/23-EJS2157},
URL = {https://doi.org/10.1214/23-EJS2157}
}