@article{Chernozhukov2017,
Author = {Chernozhukov, Victor and Chetverikov, Denis and Demirer, Mert and Duflo, Esther and Hansen, Christian and Newey, Whitney},
Title = {Double/Debiased/Neyman Machine Learning of Treatment Effects},
Journal = {American Economic Review},
Volume = {107},
Number = {5},
Year = {2017},
Month = {May},
Pages = {261–65},
DOI = {10.1257/aer.p20171038},
URL = {https://www.aeaweb.org/articles?id=10.1257/aer.p20171038}}

@article{Chernozhukov2018,
    author = {Chernozhukov, Victor and Chetverikov, Denis and Demirer, Mert and Duflo, Esther and Hansen, Christian and Newey, Whitney and Robins, James},
    title = {Double/debiased machine learning for treatment and structural parameters},
    journal = {The Econometrics Journal},
    volume = {21},
    number = {1},
    pages = {C1-C68},
    year = {2018},
    month = {01},
    abstract = {We revisit the classic semi‐parametric problem of inference on a low‐dimensional parameter θ0 in the presence of high‐dimensional nuisance parameters η0. We depart from the classical setting by allowing for η0 to be so high‐dimensional that the traditional assumptions (e.g. Donsker properties) that limit complexity of the parameter space for this object break down. To estimate η0, we consider the use of statistical or machine learning (ML) methods, which are particularly well suited to estimation in modern, very high‐dimensional cases. ML methods perform well by employing regularization to reduce variance and trading off regularization bias with overfitting in practice. However, both regularization bias and overfitting in estimating η0 cause a heavy bias in estimators of θ0 that are obtained by naively plugging ML estimators of η0 into estimating equations for θ0. This bias results in the naive estimator failing to be N−1/2 consistent, where N is the sample size. We show that the impact of regularization bias and overfitting on estimation of the parameter of interest θ0 can be removed by using two simple, yet critical, ingredients: (1) using Neyman‐orthogonal moments/scores that have reduced sensitivity with respect to nuisance parameters to estimate θ0; (2) making use of cross‐fitting, which provides an efficient form of data‐splitting. We call the resulting set of methods double or debiased ML (DML). We verify that DML delivers point estimators that concentrate in an N−1/2‐neighbourhood of the true parameter values and are approximately unbiased and normally distributed, which allows construction of valid confidence statements. The generic statistical theory of DML is elementary and simultaneously relies on only weak theoretical requirements, which will admit the use of a broad array of modern ML methods for estimating the nuisance parameters, such as random forests, lasso, ridge, deep neural nets, boosted trees, and various hybrids and ensembles of these methods. We illustrate the general theory by applying it to provide theoretical properties of the following: DML applied to learn the main regression parameter in a partially linear regression model; DML applied to learn the coefficient on an endogenous variable in a partially linear instrumental variables model; DML applied to learn the average treatment effect and the average treatment effect on the treated under unconfoundedness; DML applied to learn the local average treatment effect in an instrumental variables setting. In addition to these theoretical applications, we also illustrate the use of DML in three empirical examples.},
    issn = {1368-4221},
    doi = {10.1111/ectj.12097},
    url = {https://doi.org/10.1111/ectj.12097},
    eprint = {https://academic.oup.com/ectj/article-pdf/21/1/C1/27684918/ectj00c1.pdf},
}

@article{NieRLearner,
    author = {Nie, X and Wager, S},
    title = {Quasi-oracle estimation of heterogeneous treatment effects},
    journal = {Biometrika},
    volume = {108},
    number = {2},
    pages = {299-319},
    year = {2020},
    month = {09},
    abstract = {Flexible estimation of heterogeneous treatment effects lies at the heart of many statistical applications, such as personalized medicine and optimal resource allocation. In this article we develop a general class of two-step algorithms for heterogeneous treatment effect estimation in observational studies. First, we estimate marginal effects and treatment propensities to form an objective function that isolates the causal component of the signal. Then, we optimize this data-adaptive objective function. The proposed approach has several advantages over existing methods. From a practical perspective, our method is flexible and easy to use: in both steps, any loss-minimization method can be employed, such as penalized regression, deep neural networks, or boosting; moreover, these methods can be fine-tuned by cross-validation. Meanwhile, in the case of penalized kernel regression, we show that our method has a quasi-oracle property. Even when the pilot estimates for marginal effects and treatment propensities are not particularly accurate, we achieve the same error bounds as an oracle with prior knowledge of these two nuisance components. We implement variants of our approach based on penalized regression, kernel ridge regression, and boosting in a variety of simulation set-ups, and observe promising performance relative to existing baselines.},
    issn = {0006-3444},
    doi = {10.1093/biomet/asaa076},
    url = {https://doi.org/10.1093/biomet/asaa076},
    eprint = {https://academic.oup.com/biomet/article-pdf/108/2/299/37938939/asaa076.pdf},
}

@inproceedings{assaad2021counterfactual,
  title={Counterfactual representation learning with balancing weights},
  author={Assaad, Serge and Zeng, Shuxi and Tao, Chenyang and Datta, Shounak and Mehta, Nikhil and Henao, Ricardo and Li, Fan and Carin, Lawrence},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={1972--1980},
  year={2021},
  organization={PMLR}
}

@article{bica2020estimating,
  title={Estimating the effects of continuous-valued interventions using generative adversarial networks},
  author={Bica, Ioana and Jordon, James and van der Schaar, Mihaela},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={16434--16445},
  year={2020}
}

@misc{gao2022estimatingheterogeneoustreatmenteffects,
      title={Estimating Heterogeneous Treatment Effects for General Responses}, 
      author={Zijun Gao and Trevor Hastie},
      year={2022},
      eprint={2103.04277},
      archivePrefix={arXiv},
      primaryClass={stat.ME},
      url={https://arxiv.org/abs/2103.04277}, 
}

@inproceedings{hassanpour2019counterfactual,
  title={CounterFactual Regression with Importance Sampling Weights.},
  author={Hassanpour, Negar and Greiner, Russell},
  booktitle={IJCAI},
  pages={5880--5887},
  year={2019},
  organization={Macao}
}

@inproceedings{johansson2016learning,
  title={Learning representations for counterfactual inference},
  author={Johansson, Fredrik and Shalit, Uri and Sontag, David},
  booktitle={International conference on machine learning},
  pages={3020--3029},
  year={2016},
  organization={PMLR}
}

@article{johansson2018learning,
  title={Learning weighted representations for generalization across designs},
  author={Johansson, Fredrik D and Kallus, Nathan and Shalit, Uri and Sontag, David},
  journal={arXiv preprint arXiv:1802.08598},
  year={2018}
}

@inproceedings{kazemi2024adversarially,
  title={Adversarially Balanced Representation for Continuous Treatment Effect Estimation},
  author={Kazemi, Amirreza and Ester, Martin},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={38},
  number={12},
  pages={13085--13093},
  year={2024}
}

@misc{kennedy2023semiparametricdoublyrobusttargeted,
      title={Semiparametric doubly robust targeted double machine learning: a review}, 
      author={Edward H. Kennedy},
      year={2023},
      eprint={2203.06469},
      archivePrefix={arXiv},
      primaryClass={stat.ME},
      url={https://arxiv.org/abs/2203.06469}, 
}

@article{louizos2017causal,
  title={Causal effect inference with deep latent-variable models},
  author={Louizos, Christos and Shalit, Uri and Mooij, Joris M and Sontag, David and Zemel, Richard and Welling, Max},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@article{sanchez2022diffusion,
  title={Diffusion causal models for counterfactual estimation},
  author={Sanchez, Pedro and Tsaftaris, Sotirios A},
  journal={arXiv preprint arXiv:2202.10166},
  year={2022}
}

@inproceedings{shalit2017estimating,
  title={Estimating individual treatment effect: generalization bounds and algorithms},
  author={Shalit, Uri and Johansson, Fredrik D and Sontag, David},
  booktitle={International conference on machine learning},
  pages={3076--3085},
  year={2017},
  organization={PMLR}
}

@article{shi2019adapting,
  title={Adapting neural networks for the estimation of treatment effects},
  author={Shi, Claudia and Blei, David and Veitch, Victor},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@book{van2011targeted,
  title={Targeted Learning: Causal Inference for Observational and Experimental Data},
  author={van der Laan, M.J. and Rose, S.},
  isbn={9781441997821},
  lccn={2011930854},
  series={Springer Series in Statistics},
  url={https://books.google.com.hk/books?id=RGnSX5aCAgQC},
  year={2011},
  publisher={Springer New York}
}

@article{wang2022generalization,
  title={Generalization bounds for estimating causal effects of continuous treatments},
  author={Wang, Xin and Lyu, Shengfei and Wu, Xingyu and Wu, Tianhao and Chen, Huanhuan},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={8605--8617},
  year={2022}
}

@inproceedings{yoon2018ganite,
  title={GANITE: Estimation of individualized treatment effects using generative adversarial nets},
  author={Yoon, Jinsung and Jordon, James and Van Der Schaar, Mihaela},
  booktitle={International conference on learning representations},
  year={2018}
}

