\section{Introduction}

Radiological images are fundamental to modern healthcare, with over 8 billion scans performed annually \cite{akhter2023ai}. As diagnostic demand grows, the demand for efficient AI-driven interpretation becomes increasingly acute. Medical Vision-Language Models (VLMs), developed for radiological visual question answering (VQA) in MRI, CT and X-ray images, offer substantial promise in assisting clinicians/patients. Recent advances in general-purpose LLMs/VLMs (e.g., GPT-4o~\cite{hurst2024gpt}, Claude-3.7 Sonnet~\cite{claude-37}) highlight sophisticated reasoning capabilities. However, the medical domain places an especially high premium on explainable decision-making: both clinicians/patients need to understand not just \emph{what} conclusion was reached, but also \emph{why}. Existing medical VLMs often provide only final answers or “quasi-explanations” derived from pre-training pattern matching, which do not necessarily reflect genuine, step-by-step reasoning. Consequently, ensuring interpretability and trustworthiness remains an urgent challenge in real-world clinical settings.

We argue that the limited reasoning capability for existing medical VLM is primarily due to the inherent drawbacks of Supervised Fine-Tuning (SFT) \cite{achiam2023gpt,qwq-32b-preview,chen2022program} which is the most common strategy for adapting large foundation models for specialized medical tasks \cite{hartsock2024vision,lian2024less,chen2024efficiency}. Despite its simplicity, SFT faces two critical challenges: 1) An over-reliance on final-answer supervision often leads to overfitting, shortcut learning, and weaker performance on out-of-distribution (OOD) data -- an issue particularly consequential in high-stake medical scenarios \cite{chu2025sft}.
2) Direct supervision with only final answers provides minimal incentive for cultivating reasoning abilities within VLMs. A possible mitigation is distilling a more capable teacher model's chain-of-thought (CoT) reasoning for SFT \cite{wei2022chain,li2023symbolic}. However, constructing high-quality CoT data is prohibitively expensive to scale in specialized domains like healthcare. As a result, current medical VLMs that rely on SFT often fall short of delivering transparent explanations and robust generalizations when confronted with unfamiliar data.

In contrast, Reinforcement Learning (RL) \cite{schulman2017proximal} offers a compelling alternative for cultivating emergent reasoning by rewarding models for discovering their own logical steps rather than memorizing final answers or copying teacher CoT rationales. Indeed, a recent work \textit{\textbf{SFT Memorizes, RL Generalizes}} \cite{chu2025sft} confirms that RL-trained models often display superior generalization compared to their SFT counterparts. However, conventional RL pipelines typically depend on auxiliary neural reward models, requiring substantial resources to continuously update both policy and reward models \cite{ziegler2019fine,ouyang2022training}.
%—an especially challenging process in data-limited medical domains. 
A promising alternative, group relative policy optimization (GRPO) \cite{shao2024deepseekmath}, eliminates the need for neural reward models by employing a rule-based group-relative advantage strategy (see sec. \ref{sec:methods} for more details). This approach has demonstrated advanced reasoning, fostering capabilities while reducing computational demands in DeepSeek-R1 \cite{guo2025deepseek}. Despite its potential benefits for resource- and data-constrained domains like healthcare, GRPO remains largely unexplored in medical contexts.

In this work, we introduce \textbf{MedVLM-R1}, the first medical VLM capable of generating answers with explicit reasoning by training with GRPO for radiology VQA tasks. Our contributions are as follows:
\begin{enumerate}
    \item \textbf{Medical VLM with Explicit Reasoning}: We introduce MedVLM-R1, the first lightweight medical VLM capable of generating explicit reasoning alongside the final answer, rather than providing only the final answer.

    \item \textbf{Emerging Reasoning Without Explicit Supervision}: Unlike traditional SFT methods that require data with complex reasoning steps, MedVLM-R1 is trained using GRPO with datasets containing only final answers, demonstrating emergent reasoning capabilities without explicit supervision.

    \item \textbf{Superior Generalization and Efficiency}: MedVLM-R1 achieves robust generalization to out-of-distribution data (e.g. MRI → CT/X-ray) and outperforms larger models like Qwen2VL-72B and Huatuo-GPT-Vision-7B, despite being a compact 2B-parameter model trained on just 600 samples.
\end{enumerate}


\section{Related Work}

\paragraph{\textbf{Medical VLMs and Their Limitations.}} The rise of large-scale VLMs has spurred numerous domain-specific adaptations for healthcare, with systems such as LLaVA-Med \cite{li2023llava} and HuatuoGPT-Vision \cite{chen2024huatuogptvisioninjectingmedicalvisual} achieving impressive results in radiology VQA and related diagnostic tasks. Despite these advancements, using SFT on final-answer labels remains the dominant strategy for tailoring large models to medical domains \cite{zhang2023pmc,chen2024efficiency,zhang2024generalist,chaves2024towards}. This approach generally requires substantial amounts of high-quality image-text data (ranging from 660k \cite{li2023llava} to 32M samples \cite{wu2023towards}) which is costly to curate and often hampered by noise/privacy concerns. Moreover, the reliance on final-answer supervision provides limited scope for exposing a model’s intermediate reasoning—an important factor in building clinicians' trust. In addition, SFT-based models often overfit to narrow training distributions, leading to weaker generalization on OOD clinical scenarios.

\paragraph{\textbf{Reinforcement Learning for Enhanced Reasoning.}}
To mitigate SFT's limitations, RL \cite{silver2017mastering,christiano2017deep,ziegler2019fine,ouyang2022training,kumar2024training} has emerged as a compelling alternative for improving model interpretability and robustness. Classic RL methods, such as proximal policy optimization (PPO) \cite{schulman2017proximal}, have been widely adopted in text-based learning (\textit{e.g.,} policy shaping for LLMs) and can reward not only correctness but also the quality of intermediate reasoning steps. Recent studies suggest that while SFT “memorizes,” RL can help models “generalize” \cite{chu2025sft}, offering a more stable trajectory toward domain-transferable representations. Notably, GRPO \cite{shao2024deepseekmath} extends PPO by eliminating its (neural) value function estimator and focusing on a rule-based group-relative advantage for selecting actions, showing promise in resource-constrained settings like DeepSeek-R1 \cite{guo2025deepseek}. Such RL-driven frameworks could be particularly beneficial for medical tasks, where limited data availability, high-stakes decision-making, and the need for explicit reasoning converge. In the following section, we will detail how these insights motivate our approach.% to improving reasoning capabilities in radiology VQA.




