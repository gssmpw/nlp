\section{Experiments}

\paragraph{\textbf{Dataset.}}
We conduct our experiments using the HuatuoGPT-Vision \cite{chen2024huatuogptvisioninjectingmedicalvisual} evaluation dataset, which is a processed and combined dataset from several publicly available medical VQA benchmarks, including VQA-RAD \cite{lau2018dataset}, SLAKE \cite{liu2021slake}, PathVQA \cite{he2020pathvqa}, OmniMedVQA \cite{hu2024omnimedvqa}, and PMC-VQA \cite{zhang2023pmc}. In total, the dataset comprises 17,300 multiple-choice questions linked to images covering various medical imaging modalities, with 2–6 possible choices per question. For this study, we focus on radiology modalities: CT, MRI, and X-ray. Specifically, we use 600 MRI image-question pairs for training and set aside 300 MRI, 300 CT, and 300 X-ray pairs for testing. The MRI test set is used as in-domain test, whereas the CT and X-ray test set serve as OOD test.

\paragraph{\textbf{Implementation details.}}
We adopt Qwen2-VL-2B as our base VLM. This model is originally trained on data from curated web pages, open-source datasets, and synthetic sources. To adapt it to the medical domain, we employ the GRPO reinforcement learning framework outlined in Section~\ref{sec:methods}. Our implementation builds on the public VLM reasoning repositories \cite{open-r1-multimodal,chen2025r1v,shen2025vlmr1}. We perform fine-tuning on two NVIDIA A100 SXM4 80GB for 300 steps, using a batch size of 2, which takes approximately 4 hours. Generation candidate number $G$ is set to 6. The other training optimization hyper-parameters are set as suggested by \cite{chen2025r1v}.

\paragraph{\textbf{Baseline methods and evaluation metric.}}
We compare MedVLM-R1 with the following baselines: 1. Qwen2-VL family \cite{Qwen-VL} including Qwen2-VL-2B (the unmodified base model), Qwen2-VL-7B and -72B which are the large/huge model variants. 2. HuatuoGPT-vision \cite{chen2024huatuogptvisioninjectingmedicalvisual}: A medical VLM built upon Qwen2-VL-7B. 3. SFT: The same Qwen2-VL-2B base model fine-tuned with standard SFT, using the same training setting with 600 MRI question-answer pairs. We apply negative log-likelihood as the loss function to carry out the SFT training. All baselines use a simple prompting format, \textit{e.g.,} \texttt{\{Question\} Your task: provide the correct single-letter choice (A, B, C, D, …).} In contrast, MedVLM-R1 uses the RL-based prompt as described in Section~\ref{sec:methods}, designed to elicit explicit reasoning. For evaluation, each model receives one point for the correct single-letter answer and zero otherwise. In the test of MedVLM-R1, only the correct choice enclosed in the \texttt{<answer>...</answer>} tag is scored as correct; any deviation from this format, even if semantically correct, results in a zero score. 



\section{Results and Discussion}

\paragraph{\textbf{Overall Performance.}}

\input{sec/fig_2}

Table~\ref{tab:prediction} summarizes both in-domain (ID) and out-of-domain (OOD) performance for various VLMs. Note that ID/OOD comparisons specifically refer to models fine-tuned on MRI data. Unsurprisingly, VLMs fine-tuned with both GRPO and SFT significantly outperform zero-shot general-purpose VLMs on in-domain tasks. Our GRPO-trained model shows very strong OOD performance, achieving a \textbf{16\%} improvement on CT and a \textbf{35\%} improvement on X-ray compared to SFT counterparts, underscoring GRPO's superior generalizability. Furthermore, despite being a compact 2B-parameter model trained on just 600 samples, MedVLM-R1 outperforms larger models like Qwen2-VL-72B and HuatuoGPT-Vision-7B, with the latter being specifically trained on large-scale medical data. This highlights the immense potential of RL-based training methods for efficient and scalable medical VLM development.

\paragraph{\textbf{Reasoning Competence and Interpretability.}} Beyond strong generalization, a central strength of MedVLM-R1 is its ability to produce explicit reasoning—a capability absent in all baselines. As illustrated in Figure~\ref{fig:vqa}, MedVLM-R1 presents a logical thought process within the \texttt{<think>} tag, with the final decision enclosed in the \texttt{<answer>} tag. Notably, for relatively simpler questions (problem 1 and 2), the reasoning appears cogent and aligned with medical knowledge. However, more complex queries sometimes reveal heuristic or just partial reasoning. For example, in the third sample, the model arrives at the correct answer via the \textbf{process of elimination} rather than detailed medical analysis, suggesting it leverages cue-based reasoning instead of domain expertise. Likewise, in some instances (e.g., question 4), the causal chain between reasoning and conclusion remains unclear, raising the question of whether the model merely \textbf{retrofits an explanation after predicting the correct answer}. Despite these imperfections, MedVLM-R1 represents a notable step toward interpretability in radiological decision-making.

\begin{table}[!t]
\centering
\setlength{\tabcolsep}{1mm}{}
\caption{\small {Results of VQA-VLMs on MRI (in-domain), and CT and X-Ray (out-of-domain) modalities. "$-2$B" indicates the model has $2$ billion parameters, etc.}}
\label{tab:prediction}
\scalebox{0.85}{
\begin{tabular}{l|c|ccc|c}
\toprule
\multirow{2}{*}{Method} & \multirow{2}{*}{\shortstack{Num. of Seen \\ Medical Sample}} & \multicolumn{3}{c|}{In-Domain / Out-of-Domain} & \multirow{2}{*}{Average} \\ 
\cline{3-5}
         &  & (MRI$\rightarrow$MRI) & (MRI$\rightarrow$CT) & (MRI$\rightarrow$X-ray) & \\ \midrule
Random Guess & / & $25.00$ & $30.25$ & $26.00$ & $27.08$   \\ 
\hline
\multicolumn{6}{c}{\footnotesize\textit{Zero-shot VLM}} \\
\hline
Qwen2-VL-2B & / & $61.67$ & $50.67$ & $53.00$ & $55.11$   \\ 
Qwen2-VL-7B & / & $72.33$ & $68.67$ & $66.63$ & $69.21$   \\ 
Qwen2-VL-72B & / & $68.67$ & $60.67$ & $72.33$ & $67.22$   \\ 
\hline
\multicolumn{6}{c}{\footnotesize\textit{Zero-shot Medical VLM}} \\
\hline
Huatuo-GPT-vision-7B & 1,294,062 & $71.00$ & $63.00$ & $\mathbf{73.66}$ & $69.22$   \\ 
\hline
\multicolumn{6}{c}{\footnotesize\textit{MRI fine-tuned VLM}} \\
\hline
Qwen2-VL-2B (SFT) & 600 & $94.00$ & $54.33$ & $34.00$ & $59.44$   \\ 
\textbf{Ours-2B (GRPO)} & 600 & $\mathbf{95.33}$ & $\mathbf{70.33}$ & $69.00$ & $\mathbf{78.22}$   \\
\bottomrule
\end{tabular}
}
\end{table}

\paragraph{\textbf{Limitations.}} Although MedVLM-R1 demonstrates promising results in MRI, CT, and X-ray datasets, several limitations remain: 1. Modality Gaps: When tested on other medical modalities (e.g., pathology or OCT images), the model fails to converge. We hypothesize this arises from the base model’s insufficient exposure to such modalities during pre-training. 2. Closed-Set Dependence: The current approach is tailored to multiple-choice (closed-set) VQA. In open-ended question settings where no predefined options are provided, the model’s performance degrades substantially. This is also a common challenge for many VLMs. 3. Superficial/hallucinated Reasoning: In some reasoning cases, MedVLM-R1 provides a correct answer without offering a meaningful reasoning process (e.g., \texttt{<think>To determine the correct observation from this spine MRI, let's analyze the image.</think><answer>B</answer>}). Moreover, sometimes the model concludes a correct choice while providing an inference that can lead to another answer. This phenomenon underscores that even models designed for explainability can occasionally revert to superficial/hallucinated justifications, highlighting an ongoing challenge in generating consistently transparent and logically sound rationales. Regarding all these issues, we believe the current 2B-parameter scale of our base model constitutes a potential bottleneck, and we plan to evaluate MedVLM-R1 on larger VLM backbones to address these concerns.


\section{Conclusion}

We present MedVLM-R1, a medical VLM that integrates GRPO-based reinforcement learning to bridge the gap between accuracy, interpretability, and robust performance in radiology VQA. By focusing on explicit reasoning, the model fosters transparency and trustworthiness—qualities essential in high-stakes clinical environments. Our results demonstrate that RL-based approaches generalize better than purely SFT methods, particularly under OOD settings. Although VLM-based medical reasoning is still at a nascent stage and faces considerable challenges, we believe that its potential for delivering safer, more transparent AI-driven healthcare solutions will be appreciated and should be encouraged.
