\section{Methods}
\label{sec:methods}

\begin{figure*}[ht]
    \centering
    \begin{tikzpicture}
    \node[draw=black, 
    line width=1pt,
    rounded corners=10pt,
    text width=\textwidth,
    inner sep=5pt,
    align=left](box){\begin{wrapfigure}{r}{0.24\textwidth} % "r" for right side, and width of the wrap figure
    \vspace{-3em}
        \includegraphics[width=0.22\textwidth]{figs/shoulder015895.png}
        \end{wrapfigure}
        \small\textbf{Question Template}\vspace{0.1cm}
        
        \brown{What can be observed in this image?

        A) Cartilage degeneration  B) Labral pathology

        C) Bone fracture D) Tendonitis
        }

        
        \vspace{-0.25cm}
        \noindent\rule{\dimexpr0.7\textwidth}{0.4pt}
        \textbf{Model Input Prompt}\vspace{0.1cm}
        
        \vspace{-0.05cm}
        % \noindent\hdashrule{\dimexpr0.6\textwidth}{0.4pt}{1mm 0.2mm}
        
        

        
        \brown{\{Question\}} Your task: 

        1. Think through the question step by step, enclose your reasoning process in <think>...</think> tags. 
        
        2. Then provide the correct single-letter choice (A, B, C, D,...) inside <answer>...</answer> tags.
        
        3. No extra information or text outside of these tags.\\

        
    \vspace{-0.25cm}
        \noindent\rule{\dimexpr1\textwidth}{0.4pt}
        \textbf{Model Output Examples}\vspace{0.1cm}

        \blue{<think>
        The image is a grayscale MRI image of an upper arm joint. 
        The bicondylar humeral head of the humerus is visible. There is a well-defined ...
        </think>
        
        <answer>B, there is no clear indication of ... </answer>}

        
        \vspace{-0.25cm}
        
        \noindent\rule{\dimexpr1\textwidth}{0.4pt}
        
        \textbf{Format Reward} = 1 due to the present of all tags and no content outside the tags
        
        \textbf{Accuracy Reward} = 0.5 due to extra explanation appended after the answer 
        \vspace{-0.12cm}
    };
    \end{tikzpicture}
    \caption{\small {The template of our employed prompt, an example of model's response and reward criterion.} }
    \label{fig:main} 
\end{figure*}

\paragraph{\textbf{Overview.}} We leverage RL to incentivize explicit reasoning capabilities in medical VLMs, specifically employing GRPO due to its efficiency and effectiveness. While the seminal work \cite{shao2024deepseekmath} applies GRPO to reasoning in coding and mathematics, we adapt these principles to the medical domain, specifically radiology data (MRI, CT, X-ray). Our approach incorporates medical imaging prompts and custom reward functions designed to encourage explicit reasoning and domain-specific answer formats. This work serves as an initial exploration of using pure reinforcement learning to build a multi-modal medical reasoning model.


\paragraph{\textbf{Base Model and Prompt Template.}} We adopt a state-of-the-art VLM -- Qwen2-VL-2B \cite{wang2024qwen2} as our base model, denoted by $\pi_\theta$, where $\theta$ are the trainable parameters. Given a training dataset $\mathbf{V}$, each sample $\mathbf{v}$ consists of: 1) An image $f$, which is a radiology image and 2) a text prompt $q$, composed of the user’s question alongside a fixed system message, as illustrated in Figure~\ref{fig:main}. The VLM then produces an output $\{o\}$, which includes both a reasoning trace and a final answer in designated XML-like tags (\texttt{<think>}…\texttt{</think>} and \texttt{<answer>}…\texttt{</answer>}). Our RL objective is to optimize $\pi_\theta$ so that answers are accurate, well-formatted, and provide transparent reasoning.

\paragraph{\textbf{Group Relative Policy Optimization (GRPO).}} To encourage robust, interpretable responses, we employ GRPO~\cite{shao2024deepseekmath}, an RL algorithm that extends PPO by focusing on a group-relative advantage instead of a learned value function. Concretely, at each training step:
\begin{enumerate}
\item We sample $G$ candidate outputs $\{o_i\}_{i=1}^G$ from $\pi_{\theta_{old}}$, the model parameters before the current update.
\item We compute a reward $r_i$ for each output using a reward function (see next paragraph). Based on $r_i$ we calculate a group relative advantage $A_i$ which is normalized by the group statistics: $ A_i = \frac{r_i - {\mathrm mean(\{r_1, r_2, \cdots, r_G\})}}{{\mathrm std(\{r_1, r_2, \cdots, r_G\})}}.$ A reward above the group average is advantaged and can further incentivize the model.


\item Our VLM model $\pi_\theta$ is then updated by maximizing $\mathcal{J}_{GRPO}$ which incorporates a clipped regularization on the relative advantage estimation for model preference alignment and training stability:


\begin{equation}
\begin{split}
    & \mathcal{J}_{GRPO}(\theta) = \mathbb{E}_{\mathbf{v} \sim P(\mathbf{V})}\mathbb{E}_{\{o_i\}_{i=1}^G \sim \pi_{\theta_{old}}(\cdot|\mathbf{v})}  \\
    & \frac{1}{G}\sum_{i=1}^G \Bigl[\min \Bigl( r_{i}^{\text{ratio}} A_i, \text{clip} \left( r_{i}^{\text{ratio}}, 1 \pm \epsilon \right)  A_i\Bigl) - \beta \mathbb{D}_{KL}\left(\pi_{\theta} || \pi_{ref}\right)\Bigl]
\end{split}
\label{eq:GRPO}
\end{equation}
with $r_{i}^{\text{ratio}} = \frac{\pi_\theta(o_i|\mathbf{v})}{\pi_{\theta_{\text{old}}}(o_i|\mathbf{v})}$. An additional \textit{Kullback–Leibler} term $\mathbb{D}_{\text{KL}}(\pi_{\theta} \Vert \pi_{ref})$ is applied to penalize divergence from a reference model $\pi_{\text{ref}}$ (the initial checkpoint), helping prevent catastrophic forgetting.  $\epsilon, \beta \in \mathbb{R}\geq0$ control the regularization strengths.
\end{enumerate}

\paragraph{\textbf{Reward function.}} Specifically, for multiple-choice medical VQA tasks, we propose a two-part rule-based reward function, inspired by \cite{guo2025deepseek}:

\noindent 1) \textit{Format Reward.} We incentivize outputs that provide a reasoning trace within the tags \texttt{<think>} ... \texttt{</think>} and a succinct final answer within the tag \texttt{<answer>} ... \texttt{</answer>}. If all four tags are present exactly once and no content is present outside these tags, we assign a format reward of 1. Any missing/duplicated tags or content outside yield 0.

\noindent 2) \textit{Accuracy Reward.} After verifying the correct format, we evaluate the correctness of the final answer. Specifically, If the letter choice \texttt{A, B, C, D,}\ldots inside the \texttt{<answer>} ... \texttt{</answer>} tag and it responds with the ground-truth choice exactly, that is an exact match with a reward of 1 point. Further, if the letter is correct but contains additional explanations or uses the choice content instead of the corresponding letter (e.g., “A: Pulmonary nodule” or "Pulmonary nodule"), that is a partial match with a 0.5 points reward. However, if the letter does not match, is missing, or the answer is not enclosed in the answer tag, that is an incorrect or missing answer and no reward would be granted (0 points).


The total reward $r_i \in [0, 2]$ is the sum of both format and accuracy reward. By structuring the reward function in this hierarchy (format before correctness), we guide the model to first adopt the desired response structure, then refine its answer selection for accurate, interpretable medical reasoning. It is worth noting that both terms are necessary since without \textit{Format Reward}, the final answer cannot be extracted while without \textit{Accuracy Reward}, the model cannot converge.



