% This is a modified version of Springer's LNCS template suitable for anonymized MICCAI 2025 main conference submissions. 
% Original file: samplepaper.tex, a sample chapter demonstrating the LLNCS macro package for Springer Computer Science proceedings; Version 2.21 of 2022/01/12

\documentclass[runningheads]{llncs}
%
\usepackage[T1]{fontenc}
% T1 fonts will be used to generate the final print and online PDFs,
% so please use T1 fonts in your manuscript whenever possible.
% Other font encodings may result in incorrect characters.
%
\usepackage{graphicx,verbatim}
% Used for displaying a sample figure. If possible, figure files should
% be included in EPS format.
%
% If you use the hyperref package, please uncomment the following two lines
% to display URLs in blue roman font according to Springer's eBook style:
%\usepackage{color}
%\renewcommand\UrlFont{\color{blue}\rmfamily}
%\urlstyle{rm}
%
% Basic packages
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{amscd}
\usepackage{graphicx}
\usepackage{bm}
\usepackage{color}
\usepackage{xcolor}
\usepackage{authblk}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{float}
\usepackage{tikz}
\usetikzlibrary{positioning}
% \usepackage{ctable}
\usepackage{url}
\usepackage{pifont}
\usepackage{listings}
\usepackage{breakcites}
\usepackage{orcidlink}
\usepackage{setspace}
\usepackage{latexsym}
\usepackage{caption}
\usepackage{makecell}
\usepackage{array}
\usepackage{wrapfig}
\usepackage{diagbox}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{textcomp}
\usepackage[T1]{fontenc}
\usepackage{upquote}
\usepackage{dashrule}
\usepackage{subfig}
\usepackage{tabularx}
\usepackage[toc, page]{appendix}
\usepackage[noabbrev,capitalise]{cleveref}
\usepackage{seqsplit}
\usepackage{url} 
\usepackage{lipsum}
 \usepackage{hyperref}

% TikZ libraries
\usetikzlibrary{arrows.meta,arrows,fit}

% Color definitions
\definecolor{Gray}{gray}{0.9}

% Custom commands
\newcommand\tstrut{\rule{0pt}{2.4ex}}
\newcommand\bstrut{\rule[-1.0ex]{0pt}{0pt}}
\newcommand{\pt}[1]{\textcolor{blue}{[\textbf{PT:} #1]}}
\newcommand{\jy}[1]{\textcolor{orange}{[\textbf{JY:} #1]}}

\definecolor{citecolor}{HTML}{0071bc}

\usepackage[toc, page]{appendix}
\usepackage{tabularx}
\usepackage{booktabs}

\newlength\savewidth\newcommand\shline{\noalign{\global\savewidth\arrayrulewidth
  \global\arrayrulewidth 1pt}\hline\noalign{\global\arrayrulewidth\savewidth}}
  
\input{macro.tex}
\begin{document}


\title{MedVLM-R1: Incentivizing Medical Reasoning Capability of Vision-Language Models (VLMs) via Reinforcement Learning}

\authorrunning{Pan, Liu et al.}
\titlerunning{MedVLM-R1}
%



\author{Jiazhen Pan\inst{1,2}$^{*}$, Che Liu\inst{3}$^{*}$, Junde Wu\inst{2}, Fenglin Liu\inst{2}, Jiayuan Zhu\inst{2}, Hongwei Bran Li\inst{4}, Chen Chen\inst{5,6}, Cheng Ouyang\inst{2,6}$^{\dagger}$, Daniel Rueckert\inst{1,6}$^{\dagger}$}
\institute{Chair for AI in Healthcare and Medicine, Technical University of Munich (TUM) and TUM University Hospital, Germany 
\and Department of Engineering Science, University of Oxford, UK
\and Data Science Institute, Imperial College London, UK
\and Massachusetts General Hospital, Harvard Medical School, USA
\and School of Computer Science, University of Sheffield, UK
\and Department of Computing, Imperial College London, UK
\\
\email{jiazhen.pan@tum.de, che.liu21@imperial.ac.uk}
}
 
\maketitle              % typeset the header of the contribution
%
%
\let\thefootnote\relax\footnotetext{$^{*}$ Equal contribution}
\let\thefootnote\relax\footnotetext{$^{\dagger}$ Equal advice}

\begin{abstract} 

Reasoning is a critical frontier for advancing medical image analysis, where transparency and trustworthiness play a central role in both clinician trust and regulatory approval. Although Medical Visual Language Models (VLMs) show promise for radiological tasks, most existing VLMs merely produce final answers without revealing the underlying reasoning. To address this gap, we introduce MedVLM-R1, a medical VLM that explicitly generates natural language reasoning to enhance transparency and trustworthiness. Instead of relying on supervised fine-tuning (SFT), which often suffers from overfitting to training distributions and fails to foster genuine reasoning, MedVLM-R1 employs a reinforcement learning framework that incentivizes the model to discover human-interpretable reasoning paths without using any reasoning references. Despite limited training data (600 visual question answering samples) and model parameters (2B), MedVLM-R1 boosts accuracy from 55.11\% to 78.22\% across MRI, CT, and X-ray benchmarks, outperforming larger models trained on over a million samples. It also demonstrates robust domain generalization under out-of-distribution tasks. By unifying medical image analysis with explicit reasoning, MedVLM-R1 marks a pivotal step toward trustworthy and interpretable AI in clinical practice. Inference model is available at: \url{https://huggingface.co/JZPeterPan/MedVLM-R1}.

% \keywords{Medical reasoning  \and Reinforcement learning \and VLMs}

\end{abstract}





\input{sec/1_2_intro_related_work}
\input{sec/3_method}
\input{sec/4_5_6_exp_res_conclusion}


\section{Acknowledgements}
This work is partially funded by the European Research Council (ERC) project Deep4MI (884622). Mr. Wu is supported by the Engineering and Physical Sciences Research Council (EPSRC) under grant EP/S024093/1 and GE HealthCare. Mr. Liu is supported by the Clarendon Fund. Ms. Zhu is supported by the Engineering and Physical Sciences Research Council (EPSRC) under grant EP/S024093/1 and Global Health R\&D of the healthcare business of Merck KGaA, Darmstadt, Germany, Ares Trading S.A. (an affiliate of Merck KGaA, Darmstadt, Germany), Eysins, Switzerland (Crossref Funder ID: 10.13039 / 100009945). Dr. Li is supported by a Postdoc Mobility Grant from SNSF. Dr. Chen is funded by Royal Society (RGS/R2/242355). Dr. Ouyang is supported by UKRI grant EP/X040186/1. 



% ---- Bibliography ----
%
\bibliographystyle{splncs04}
\bibliography{refs}
%

\end{document}
