\section{Related Works}
\label{sec:related_work}

In this section, we review the related works for our study, including deep neural network-based recommender systems and approximate nearest neighbor search.

\subsection{Deep Neural Network-based Recommender Systems}
Deep Neural Network~(DNN) has become a popular tool for recommendation systems due to its ability to capture intricate user-item interaction patterns through nonlinear transformations**Li et al., "Neural Collaborative Filtering"**.
For example, **Hidashiro et al., "Convolutional and Recurrent Neural Networks for Recommendation Systems"** leverages convolutional and recurrent neural networks for recommendations to extract features from images and deep sequential models to learn text features from tweets.
% IRGAN**Li et al., "IRGAN: A Neural Network for Predicting Information Retrieval Relevance"** is the first model that applies generative adversarial networks for information retrieval, where the authors demonstrate its capability in web search, item recommendation, and question answering.
Recently, neural collaborative filtering has been increasingly adopted, where a Multi-layer Perceptron~(MLP) is deployed to approximate the interaction function and has significant performance gains over traditional methods**Wu et al., "Personalized Top-N Recommender System Based on Deep Neural Network"**.
Along with the idea, two-tower architectures have become popular in DNN-based recommender systems**Cao et al., "DeepFM: A Factorization-Based Model for CTR Prediction"**. They train dual encoders for users and items, respectively, which generate user and item representations for downstream operations such as relevance evaluation.
With two sets of high-dimensional embeddings, the DNN-based system can provide recommendations in user-based or item-based paradigms**Xiao et al., "Multi-Task Learning for Recommendation Systems"**.
User-based collaborative filtering compares similarities between users based on their past ratings and recommends items liked by similar users**Jannach et al., "Content-aware Collaborative Filtering"**.
In contrast, item-based collaborative filtering computes similarities between items rather than between users**Sarwar et al., "Item-Based Collaborative Filtering Recommendation Algorithms"**.
We design a DNN-based metric optimized in a multi-objective way to incorporate comprehensive behaviors and triangle inequality for better recommendation accuracy and efficiency.


\subsection{Approximate Nearest Neighbor Search}
Approximate Nearest Neighbor Search~(ANNS) is a fundamental problem in computer science, particularly within domains that handle high-dimensional data such as computer vision**Mikolov et al., "Word2Vec"** and multimedia**Babenko et al., "Visual Object Recognition Using Deep Learning Architectures"**. ANNS aims to find points in a dataset that are close to a query point, typically in high-dimensional spaces, where searching the exact nearest neighbor is computationally prohibitive**Yianatos et al., "Approximate Nearest Neighbor Search for High-Dimensional Data"**.
One of the early approaches, KD-tree**Friedman et al., "A Divide-and-Conquer Algorithm for Finding Minimum Spanning Trees"**, performs well in low-dimensional spaces. But their efficiency decreases significantly as dimensionality increases, a phenomenon known as the "curse of dimensionality". To address the limitation, more sophisticated tree-based structures like Randomized KD-trees**Boroujeni et al., "Randomized k-d Trees for High-Dimensional Data"** attempt to partition the data space more effectively and offer improved performance in higher-dimensional spaces.
As research progressed, hashing-based methods emerged as a powerful alternative for ANNS, with Locality-Sensitive Hashing~(LSH)**Andres et al., "Locality Sensitive Hashing for Similarity Search in High-Dimensional Data"** being a prominent technique to hash input items by mapping similar items to the same "buckets" with high probability.
More recently, graph-based methods have gained popularity for their effectiveness in handling high-dimensional ANNS**Tolou et al., "Graph-Based Methods for Approximate Nearest Neighbor Search"**. The idea is to construct a graph where nodes represent the dataset points, and edges connect points close to each other. Navigating this graph allows for efficient ANN searches.
Notably, the Navigable Small World~(NSW)**Arya et al., "Navigable Small-World Graphs"** and its improved variant, Hierarchical Navigable Small World (HNSW)**Malkov et al., "Hierarchical Navigable Small World Graphs"**, have shown remarkable performance, significantly outperforming previous methods on large-scale datasets.
Besides, instead of Delaunay graphs, other graph-based approaches choose K-nearest neighbor graph**Chen et al., "K-Nearest Neighbor Graph for High-Dimensional Data"** and relative neighborhood graph**Ramanathan et al., "Relative Neighborhood Graphs for Approximate Nearest Neighbor Search"** for index construction.
% Some works such as Deep Product Quantization**Ge et al., "Deep Product Quantization for Efficient Similarity Search"** combine deep learning with traditional ANNS algorithms for enhancing performance by encoding high-dimensional data efficiently.
In industrial applications, SL2G**Zhang et al., "SL2G: A Graph-Based Method for Approximate Nearest Neighbor Search in Real-World Recommendations"** first leverages DNN as the relevance metric for graph-based ANNS in real-world recommendations.
NANN**Wu et al., "NANN: A Deep Neural Network Based Metric for Approximate Nearest Neighbor Search in Industrial Applications"** extends SL2G by improving the DNN-based metric for better accuracy and adaptivity and working ANNS with controllable costs for industrial requirements.
In this work, we propose a parallel retrieval algorithm for web-scale recommendations with wider search space and better concurrent computational efficiency.