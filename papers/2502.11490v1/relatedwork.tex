\section{Related Works}
\label{sec:related_work}

In this section, we review the related works for our study, including deep neural network-based recommender systems and approximate nearest neighbor search.

\subsection{Deep Neural Network-based Recommender Systems}
Deep Neural Network~(DNN) has become a popular tool for recommendation systems due to its ability to capture intricate user-item interaction patterns through nonlinear transformations~\cite{zhang2019deep}.
For example, \cite{zhang2017hashtag} leverages convolutional and recurrent neural networks for recommendations to extract features from images and deep sequential models to learn text features from tweets.
% IRGAN~\cite{wang2017irgan} is the first model that applies generative adversarial networks for information retrieval, where the authors demonstrate its capability in web search, item recommendation, and question answering.
Recently, neural collaborative filtering has been increasingly adopted, where a Multi-layer Perceptron~(MLP) is deployed to approximate the interaction function and has significant performance gains over traditional methods~\cite{he2017neural}.
Along with the idea, two-tower architectures have become popular in DNN-based recommender systems~\cite{yang2020mixed}. They train dual encoders for users and items, respectively, which generate user and item representations for downstream operations such as relevance evaluation.
With two sets of high-dimensional embeddings, the DNN-based system can provide recommendations in user-based or item-based paradigms~\cite{ko2022survey}.
User-based collaborative filtering compares similarities between users based on their past ratings and recommends items liked by similar users~\cite{zhang2020alleviating}.
In contrast, item-based collaborative filtering computes similarities between items rather than between users~\cite{xue2019deep}.
We design a DNN-based metric optimized in a multi-objective way to incorporate comprehensive behaviors and triangle inequality for better recommendation accuracy and efficiency.


\subsection{Approximate Nearest Neighbor Search}
Approximate Nearest Neighbor Search~(ANNS) is a fundamental problem in computer science, particularly within domains that handle high-dimensional data such as computer vision~\cite{ben2015approximate} and multimedia~\cite{ferhatosmanoglu2001approximate}. ANNS aims to find points in a dataset that are close to a query point, typically in high-dimensional spaces, where searching the exact nearest neighbor is computationally prohibitive~\cite{liu2004investigation}.
One of the early approaches, KD-tree~\cite{friedman1977algorithm}, performs well in low-dimensional spaces. But their efficiency decreases significantly as dimensionality increases, a phenomenon known as the "curse of dimensionality". To address the limitation, more sophisticated tree-based structures like Randomized KD-trees~\cite{silpa2008optimised} attempt to partition the data space more effectively and offer improved performance in higher-dimensional spaces.
As research progressed, hashing-based methods emerged as a powerful alternative for ANNS, with Locality-Sensitive Hashing~(LSH) being a prominent technique to hash input items by mapping similar items to the same "buckets" with high probability~\cite{indyk1998approximate}.
More recently, graph-based methods have gained popularity for their effectiveness in handling high-dimensional ANNS~\cite{wang2021comprehensive}. The idea is to construct a graph where nodes represent the dataset points, and edges connect points close to each other. Navigating this graph allows for efficient ANN searches.
Notably, the Navigable Small World~(NSW)~\cite{malkov2014approximate} and its improved variant, Hierarchical Navigable Small World (HNSW)~\cite{malkov2018efficient}, have shown remarkable performance, significantly outperforming previous methods on large-scale datasets.
Besides, instead of Delaunay graphs, other graph-based approaches choose K-nearest neighbor graph~\cite{fu2016efanna} and relative neighborhood graph~\cite{jayaram2019diskann} for index construction.
% Some works such as Deep Product Quantization~\cite{babenko2016efficient} combine deep learning with traditional ANNS algorithms for enhancing performance by encoding high-dimensional data efficiently.
In industrial applications, SL2G~\cite{tan2020fast} first leverages DNN as the relevance metric for graph-based ANNS in real-world recommendations.
NANN~\cite{chen2022approximate} extends SL2G by improving the DNN-based metric for better accuracy and adaptivity and working ANNS with controllable costs for industrial requirements.
In this work, we propose a parallel retrieval algorithm for web-scale recommendations with wider search space and better concurrent computational efficiency.