\subsection{Empirical evidence of representational deficiency in DNA foundation models}

In this section, we investigate the effects of \texttt{[MASK]} token embeddings across both the encoder-only foundation model, BarcodeBERT, and our proposed encoder-decoder model, BarcodeMAE.
To understand how the presence of \texttt{[MASK]} tokens impact taxonomic information during inference, we conducted two experiments using the genus-level 1-NN classification task from \autoref{s:results}. First, we replaced different portions of input sequences with \texttt{[MASK]} tokens, varying the masking ratio from 0.1 to 0.9, and evaluated the performance of the pretrained BarcodeBERT model (for which the encoder saw \texttt{[MASK]} tokens during training). Second, we performed a comparative analysis where instead of substituting the dropped tokens with \texttt{[MASK]}, we instead, removed them entirely. This version was performed for both BarcodeMAE and BarcodeBERT.
This allows us to study the impact of removing portions of the sequence on both models, and the effect of the \texttt{[MASK]} token versus token deletion on model performance.

\begin{figure}[tbh]
    \centering
    \includegraphics[width=0.65\linewidth]{Figures/mask_proportion_plot}
    \caption{\textit{Impact of masking and token deletion on genus-level classification accuracy.} While BarcodeBERT shows stability at higher drop rates, the practical inference scenario occurs at $x\!=\!0$ with no masking, where BarcodeMAE demonstrates superior performance. The robustness to masking or removing tokens shown by BarcodeBERT does not correspond to an improved real-world performance since these conditions are not encountered during inference.}
    \label{fig:emp_mask}
\end{figure}

As shown in \autoref{fig:emp_mask}, we find that BarcodeMAE outperforms BarcodeBERT in the expected downstream use-case where the whole input sequence is shown to the model.
The performance of BarcodeMAE decreases approximately linearly as input tokens are removed from the sequence, and begins to fall as soon as any tokens are removed.
Meanwhile, the BarcodeBERT model, in both masked and removed variants, demonstrates only a shallow decline in performance as tokens are dropped until reaching its training masking ratio of 50\%, after which the accuracy decreases much more rapidly.
These results demonstrate that BarcodeBERT is better able to operate on partially complete information, but can not integrate together all information in the sequence.
Given that the two training tasks are the same, it is surprising that BarcodeMAE does not match the performance of BarcodeBERT for partial sequences, and this suggests there may be potential for further performance gains.

Additionally, we find that BarcodeBERT performs better when dropped tokens are replaced with the \texttt{[MASK]} token instead of being removed completely from the input. The performance gap emerges immediately (+4\% at 10\% dropped) and increases to reach approximately +10\% at 80\% dropped.
% This suggests that \texttt{[MASK]} tokens encapsulate meaningful taxonomic information that the model leverages for predictions. 
Since the \texttt{[MASK]} tokens do not contain any information about the specimen's genus, the fact that the BarcodeBERT model performs better when they are present indicates it learned to use the computation associated with \texttt{[MASK]} tokens to better extract information from rest of the sequence.

These results empirically demonstrate the distribution shift challenge inherent in masked language modelling, as the model develops dependency on \texttt{[MASK]} tokens during training that are absent during inference. The contrasting behaviour of BarcodeMAE, which learns representations solely from observed nucleotides, suggests its architecture may better align with inference-time conditions, where the \texttt{[MASK]} token is not present. 
