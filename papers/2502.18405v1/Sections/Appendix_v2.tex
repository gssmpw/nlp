\section*{Appendices}

\section{Comparison with causal models \label{appendix_1}}

To compare our model with recently developed causal models for DNA sequence analysis, we conducted additional experiments comparing BarcodeMAE with several state-of-the-art models, such as HyenaDNA-tiny and Caduceus-PS-1k which are trained on non-barcode data and BarcodeMamba which is trained on DNA barcode data. For a fair comparison, since BarcodeMamba was trained on the CANADA-1.5M dataset \citep{Hebert2016}, we trained BarcodeMAE on the same dataset and evaluated all models on BIOSCAN-5M.

As shown in \autoref{tab:appendix_results}, While causal models show strong performance in BIN clustering, with HyenaDNA-tiny achieving 85.0\% AMI, they underperform in the genus-level classification of unseen species. BarcodeMamba, specifically trained on DNA barcodes, achieves the highest balanced performance among state space models with 36.3\% genus-level accuracy and 82.7\% BIN clustering AMI, resulting in a harmonic mean of 50.5\%. 

The encoder-only architecture, BarcodeBERT, demonstrates enhanced genus-level classification through 1-NN probing compared to causal models, achieving 40.9\% accuracy. BarcodeMAE surpasses all competing models with 51.2\% genus-level classification accuracy and a harmonic mean of 63.2\%, indicating superior balanced performance across metrics. One interesting finding of these results is that BarcodeMAE surpasses models pre-trained on BIOSCAN-5M by 3\% in the ZSC bin reconstruction task, despite being trained on the smaller CANADA-1.5M dataset.


\input{Tables/appendix_table_v2}

\section{Baseline models}
For evaluation, we utilized the respective Pretrained models from Huggingface ModelHub, specifically:

\begin{itemize}
\item DNABERT-2: \href{https://huggingface.co/zhihan1996/DNABERT-2-117M}{zhihan1996/DNABERT-2-117M}
\item DNABERT-S: \href{https://huggingface.co/zhihan1996/DNABERT-S}{zhihan1996/DNABERT-S}
\item NT: \href{https://huggingface.co/InstaDeepAI/nucleotide-transformer-v2-50m-multi-species}{InstaDeepAI/nucleotide-transformer-v2-50m-multi-species}
\item HyenaDNA: \href{https://huggingface.co/LongSafari/hyenadna-tiny-1k-seqlen}{LongSafari/hyenadna-tiny-1k-seqlen}
\item BarcodeBERT: \href{https://huggingface.co/bioscan-ml/BarcodeBERT}{bioscan-ml/BarcodeBERT}
\item Caduceous: \href{https://huggingface.co/kuleshov-group/caduceus-ps_seqlen-1k_d_model-256_n_layer-4_lr-8e-3}{kuleshov-group/caduceus-ps\_seqlen-1k\_d\_model-256\_n\_layer-4\_lr-8e-3}
\item BarcodeMamba: \href{https://huggingface.co/bioscan-ml/BarcodeMamba}{bioscan-ml/BarcodeMamba-dim384-layer2-char}
\end{itemize}

\subsection{Pretraining}
BarcodeBERT and BarcodeMAE were pretrained for 35 epochs using the AdamW optimizer \cite{AdamW} with a learning rate of $2\times10^{-4}$, a batch size of 128, and a OneCycle learning rate scheduler. The pretraining process utilized four NVIDIA V100 GPUs and required approximately 36 hours to complete for each experiment executed. To examine the impact of pretraining, we also trained a model from scratch on the training subset of the \textit{Seen} partition without any pretraining. 

\subsection{Zero-shot clustering}

We evaluated the models’ ability to group sequences without supervision using a modified version of the framework from \iftoggle{arxiv}{\citet{zsc-Lowe-2024}}{Lowe et. al. \cite{zsc-Lowe-2024}}. Embeddings were extracted from the pretrained encoders and reduced to 50 dimensions using UMAP \citep{umap} to enhance computational efficiency while preserving data structure. These reduced embeddings were clustered with Agglomerative Clustering (cosine distance, Ward’s linkage), using the number of true species as the target number of clusters. Clustering performance was assessed with adjusted mutual information (AMI) to measure alignment with ground-truth labels.

