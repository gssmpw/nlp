\section{Experiments}

In this section, we describe both closed-world and open-world evaluation tasks designed to assess different aspects of the model's performance. Additionally, we present comparative results against current state-of-the-art baselines and conclude with an ablation study examining the impact of $k$-mer length and the number of layers.



\subsection{Evaluation framework}
We evaluate our model through two self-supervised learning (SSL) tasks: a closed-world task assessing generalization to new species within known genera, and an open-world task evaluating the model's ability to handle unseen taxonomic groups.

\textbf{Closed-World Task: 1-NN Probing.} To evaluate model generalization to new species within known genera, we perform genus-level 1-NN classification using cosine similarity. We use the training subset of the \textit{Seen} partition as the reference set and the \textit{Unseen} partition as the query set. This task, while involving unseen species, operates within the closed-world setting as it evaluates performance on known genera from the training taxonomy.

\textbf{Open-World Task: BIN Reconstruction.} To assess the model's ability to identify novel species and capture taxonomic relationships, we implement a Barcode Index Number (BIN) reconstruction task. We merge the test subset from the \textit{Seen} partition with the test subset of \textit{Unseen} partition and employ zero-shot clustering on embeddings generated without fine-tuning \citep{zsc-Lowe-2024}. This evaluation is particularly crucial for understanding the model's capability to group sequences from rare or previously unclassified species based on shared biological features.
% \subsection{Model variants}

% In our experiments, we evaluate five model variants to analyze the individual and combined effects of our proposed components.

% \textbf{BioPretext BERT.} Implements the best-performing biologically-inspired pretext task configuration.

% \textbf{MaskAE BERT.} Implements only the masked auto-encoding strategy.

% \textbf{EncDec BERT.} Utilizes an encoder-decoder architecture maintaining the standard masking. This serves as a controlled baseline to isolate the impact of architectural choices from masking strategies.

% \textbf{BioEncDec BERT.} Integrates the best-performing biologically-inspired pretext task with the encoder-decoder architecture.

% \textbf{BioMaskAE BERT.} Incorporates all three components, best-performing biologically-inspired pretext task, masked auto-encoding, and encoder-decoder architecture.

% This evaluation framework enables assessment of our methodological innovations while maintaining comparability with established approaches in DNA sequence modeling.

% \subsection{Implementation Details}
% \textbf{Model Architecture.} Our implementation builds upon the BERT-based transformer architecture, with an encoder of  6 layers and 6 attention heads and a decoder with 6 layers and 6 heads. The hidden dimension of both the decoder and the encoder is 768 for consistent representation learning capacity. The model generates DNA barcode-level embeddings through global average pooling over the sequence of 768-dimensional output vectors, excluding padding and special tokens.

% \textbf{Tokenization Strategy.} For DNA sequence tokenization, we employ non-overlapping $k$-mer tokenization with a vocabulary size of \( 4^k + 2\), including \texttt{[UNK]} and \texttt{[MASK]} special tokens. To address frame-shift sensitivity, we incorporate a data augmentation strategy during pretraining proposed in BarcodeBERT where sequences are randomly offset before starting the tokenization. Based on previous studies \citep{NucleotideTransformerDalla-Torre2023, Zhou2021DNABERT, arias2023barcodebert} that identified optimal performance with $k$ values of 4 or 6, we evaluated our model using both $k$-mer sizes to find the best-performing tokenization strategy.


% \textbf{Positional Embedding.} The removal of \texttt{[MASK]} tokens from the encoder input necessitates careful consideration of positional embeddings. In BarcodeMAE architecture, we preserve the original sequence positions by skipping the indices of masked positions when assigning positional encodings. This approach ensures that unmasked tokens maintain their relative positions from the original sequence, allowing the model to learn meaningful spatial relationships despite the absence of masked tokens.


% \textbf{Training Details.} We implemented the model using PyTorch and Hugging Face Transformers, training with 50\% token masking and cross-entropy loss. Optimization utilizes AdamW with weight decay of \(1\times10^{-5}\) and a OneCycle schedule with a maximum learning rate of \(1\times10^{-4}\).

\subsection{Results}
\label{s:results}

We compared BarcodeMAE against a comprehensive set of baselines, four encoder-only transformer-based models, DNABERT-2 \citep{zhou2023dnabert2}, DNABERT-S \citep{zhou2024dnaberts}, Nucleotide Transformer \citep{NucleotideTransformerDalla-Torre2023}, all trained on non-barcode data, and BarcodeBERT \citep{arias2023barcodebert}, trained on DNA barcodes. Since BarcodeBERT is a 4-layer encoder-only model and BarcodeMAE uses 6 encoder layers we pretrained a 6-layer model on the BIOSCAN-5M to ensure the fair comparison. We also implemented a baseline that uses an encoder-decoder architecture whilst maintaining the standard masking (BarcodeMAE w/MASK). This serves as a controlled baseline to isolate the impact of architectural choices from masking strategies. Note that, even though BarcodeMAE is conceptually an encoder-decoder model, for both BarcodeMAE and BarcodeMAE w/MASK, we discard the decoder component at inference time, using only the pretrained encoder for downstream tasks. 


As shown in \autoref{tab:main_results}, BarcodeMAE achieves state-of-the-art performance in genus-level classification with 69.0\% accuracy, significantly outperforming the previous best baseline, BarcodeBERT, by over 10\%. This strong performance in the 1-NN probing task suggests that BarcodeMAE develops more effective representations of the taxonomic hierarchy, particularly in closed-world scenarios where the genera are known but the species are unseen. Notably, even our BarcodeMAE w/MASK baseline model outperforms existing approaches, demonstrating that decoupling the encoder and decoder alone contributes to improving representation learning in DNA barcode sequences, independent of masking strategy optimizations.


For BIN reconstruction using ZSC, DNABERT-S achieves the highest AMI score of 87.7\%, potentially due to its diverse pretraining dataset that aligns well with the clustering objective \citep{zhou2024dnaberts}. Notably, BarcodeMAE reaches comparable performance with an AMI of 80.3\%, outperforming models like DNABERT-2 and BarcodeBERT. To assess the performance across closed and open-world tasks, we calculated the harmonic mean between genus-level accuracy and BIN reconstruction AMI. BarcodeMAE achieves the highest harmonic mean of 74.2\%, outperforming all other baselines. This balanced metric highlights BarcodeMAE's robust performance across both genus-level classification and BIN reconstruction tasks.

\input{Tables/main_results_v2}

To further validate the effectiveness of the model on underrepresented taxa, \autoref{fig:emb} visualizes the embeddings for 20 randomly sampled genera with fewer than 50 sequences in the dataset.  The embeddings from BarcodeMAE and the second best-performing model, BarcodeBERT, are projected to two dimensions using t-SNE \citep{vanDerMaaten2008}. The visualization shows that BarcodeMAE produces more cohesive and well-separated clusters compared to BarcodeBERT, indicating its ability to learn more discriminative embeddings even for genera with limited samples. 

% These qualitative results, combined with the superior performance across the evaluation task, demonstrate that our architectural modifications successfully address the representational limitations introduced by masked tokens in DNA language modeling.

\begin{figure}[h]
    \centering \includegraphics[width=\linewidth]{Figures/embeddings_plot_5M}
    \caption{%
    \textit{t-SNE visualization of DNA barcode embeddings} from BarcodeBERT (left) and BarcodeMAE (right) for 20 randomly selected underrepresented genera. Each point represents a DNA barcode sequence, and colours indicate different genera. BarcodeMAE shows more distinct and well-separated clusters, suggesting better discrimination between genera compared to BarcodeBERT.}
    \label{fig:emb}
\end{figure}


\subsection{Ablation study}
We conducted an ablation study to analyze the impact of different architectural configurations on BarcodeMAE model performance, focusing on two key parameters: $k$-mer size and the number of layers and attention heads of the encoder-decoder. As shown in \autoref{tab:maelm-ablation}, we systematically varied the number of layers in both the encoder and decoder. The notation ``enc:L-H dec:M-J" indicates an encoder with L layers and H attention heads and a decoder with M layers and J attention heads. For each architecture, we evaluated both a $k$-mer size of $k=4$ and $k=6$.


\input{Tables/maelm_ablation}

Our experiments demonstrate that the best performance is achieved with balanced encoder and decoder architectures (enc:6-6 dec:6-6), achieving 69.0\% accuracy for $k=6$. This contradicts traditional NLP approaches where shallower decoders are preferred \citep{meng2024maelm}. The improved performance with deeper decoders indicates that DNA sequence modelling requires more complex feature reconstruction capabilities. This finding provides evidence that effective DNA language models need architectures specifically designed for genomic data rather than direct adaptations from NLP.\looseness=-1


\input{Sections/exp-deficiency}
