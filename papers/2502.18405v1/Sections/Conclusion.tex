\section{Conclusion}

We introduced BarcodeMAE, an encoder-decoder architecture that mitigates the fundamental limitations of masked language modelling in DNA barcode sequence analysis. By eliminating \texttt{[MASK]} tokens during encoding, BarcodeMAE reduces the distribution shift between pretraining and inference, significantly enhancing performance over existing DNA foundation models. Notably, it achieves over a 10\% improvement in genus-level classification accuracy on the BIOSCAN-5M dataset compared to the previous state-of-the-art, BarcodeBERT.

While BarcodeMAE does not have the best performance in BIN reconstruction, it achieves the highest harmonic mean across the evaluation tasks, demonstrating a robust performance between closed-world and open-world settings. Our ablation studies reveal that, unlike NLP models that favour shallow decoders, DNA sequence modelling benefits from balanced encoder-decoder architectures, underscoring the need for domain-specific architectural designs.

These findings highlight the critical impact of \texttt{[MASK]} token distribution shifts on foundation model effectiveness, particularly in genomic applications where models are used for feature extraction without fine-tuning. The superior performance of BarcodeMAE across diverse evaluation scenarios validates its architectural approach to addressing masking inefficiencies in genomic foundation models.
