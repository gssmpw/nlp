In this section, we present BarcodeMAE, the encoder-decoder architecture for DNA sequence modelling. We describe the core architectural design and masking strategy that addresses the distribution shift between pretraining and inference, followed by the implementation specifications: tokenization, positional embeddings, and training procedures.

\subsubsection{Encoder-decoder architecture with modified masking}

To address the representational inefficiency in DNA sequence modelling, we adapt the MAE-LM approach \citep{meng2024maelm} for genomic applications.
In training using masked language modelling objectives, part of the encoder's capacity must be allocated to processing \texttt{[MASK]} tokens, which potentially limits the model's overall representational capacity to encode real tokens. The MAE-LM architecture effectively mitigates this limitation by using a bidirectional encoder and shallow bidirectional decoder, where the masked tokens are only presented to the decoder.

% The \texttt{[MASK]} token embeddings typically encode dataset-specific information, resulting in suboptimal utilization of the model's representational capacity. 
% through a two-stage pretraining strategy. The core architecture comprises a transformer-based encoder and a shallow transformer-based decoder, with a novel approach to processing masked tokens. 

The encoder operates on nucleotide sequences with masked-out tokens removed entirely. Given a DNA sequence $\mathbf{x} = [x_1, \ldots, [\texttt{MASK}]_i, \ldots, x_n]$ and the set of masked positions $\mathcal{M}$, the encoder processes only nucleotide tokens. The encoder's input sequence $\mathbf{H}^0$ is composed of token embeddings $\mathbf{e}_{x_i}$ and positional embeddings $\mathbf{p}_i$ for non-masked positions:
\begin{equation}
\mathbf{H}^0 = \{h_i^0\}_{i\notin\mathcal{M}}, \quad h_i^0 = \mathbf{e}_{x_i} + \mathbf{p}_i
\end{equation}
The decoder then processes sequences containing both masked and unmasked positions, explicitly incorporating the \texttt{[MASK]} token in its input. The decoder's input sequence $\hat{\mathbf{H}}^0$ is constructed as:
\begin{equation}
\hat{\mathbf{H}}^0 = \{\hat{h}_i^0\}_{1\leq i\leq n}, \quad
\hat{h}_i^0 = \begin{cases}
\mathbf{e}_{[\text{MASK}]} + \mathbf{p}_i & i \in \mathcal{M} \\
h_L^i + \mathbf{p}_i & i \notin \mathcal{M}
\end{cases}
\end{equation}
where $h_L^i$ represents the final layer output from the encoder for non-masked positions and ${e}_{[\text{MASK}]}$ is the token embedding for the \texttt{[MASK]} token. 


This approach prevents the encoder from learning specific embeddings for the \texttt{[MASK]} token, ensuring the decoder's representational capacity is not devoted to encoding this special token. Consequently, the encoder's representations remain unaffected by the \texttt{[MASK]} token and will use the full representational capacity to learn meaningful patterns from the nucleotide sequences. During downstream tasks, only the encoder is utilized, effectively isolating any potential limitations or inefficiencies related to the \texttt{[MASK]} tokens.

\subsubsection{Model implementation}


BarcodeMAE uses a transformer architecture to implement the MAE-LM framework for DNA barcodes. It is trained using masked language modelling objectives. The architecture consists of a symmetrical design: an encoder and decoder, each comprising 6 transformer layers with 6 attention heads. Both components maintain a consistent hidden dimension of 768 units to ensure uniform representation capacity throughout the network. 
To obtain an embedding of the entire DNA barcode, the model employs global average pooling across the sequence of 768-dimensional output vectors, excluding padding and special tokens. \autoref{fig:BarcodeMAE} illustrates the architectural differences between BarcodeBERT, an encoder-only foundation model, and BarcodeMAE, an encoder-decoder model.

\begin{figure}[!h]
    \centering
    \includegraphics[width=1\linewidth]{Figures/BarcodeMAEvsBarcodeBERT.pdf}
    \caption{\textit{Comparison of pretraining processes for BarcodeBERT (left) and BarcodeMAE (right)}. 
    %The models demonstrate two distinct approaches to processing DNA sequences. 
    BarcodeBERT uses an encoder-only transformer architecture with direct masking. BarcodeMAE processes DNA barcode sequences through a transformer encoder-decoder architecture. The masking strategy differs from other foundation models by excluding the \texttt{[MASK]} token from the encoder input, requiring the decoder to predict masked sequences. After pretraining, the decoder is discarded and only the encoder is used for downstream tasks.}
    \label{fig:BarcodeMAE}
\end{figure}

For DNA sequence processing, we use non-overlapping $k$-mer tokenization with a vocabulary size of $4^k + 2$, including the \texttt{[UNK]} and \texttt{[MASK]} special tokens. To handle frame-shift sensitivity, we incorporate the data augmentation strategy proposed in BarcodeBERT \citep{arias2023barcodebert}, where sequences are randomly offset before tokenization. Based on previous studies \citep{arias2023barcodebert, NucleotideTransformerDalla-Torre2023} showing optimal performance with $k$ values of 4 or 6, we evaluate our model using both of these $k$-mer lengths.

In this model, the encoder processes DNA sequences without \texttt{[MASK]} tokens, requiring a modified positional encoding scheme. Our implementation preserves sequence order by skipping masked position indices during encoding. This design maintains the relative positions of unmasked tokens from the original sequence, enabling spatial relationship modelling in DNA sequences.

We implement our model using PyTorch and the Hugging Face Transformers library. Our model is trained using masked token prediction with a 50\% token masking strategy.  To optimize the cross-entropy loss of masked tokens, we use AdamW \citep{AdamW} with a weight decay coefficient of $1\times10^{-5}$ and a OneCycle scheduler with a maximum learning rate of $1 \times 10^{-4}$.