\begin{abstract}
% The abstract paragraph should be indented 1/2~inch (3~picas) on both left and
% right-hand margins. Use 10~point type, with a vertical spacing of 11~points.
% The word \textsc{Abstract} must be centered, in small caps, and in point size 12. Two
% line spaces precede the abstract. The abstract must be limited to one
% paragraph.
%While transformer-based architectures have been widely adopted in DNA foundation models, their direct adaptation from NLP overlooks the unique characteristics of genomic sequences. We introduce a principled approach to DNA foundation model design through two key innovations: biologically inspired pretext tasks that handle ambiguous nucleotides and a modified encoder-decoder architecture that addresses inefficiencies in masked language modelling. Implementing these within a BERT-based transformer, we evaluate their effectiveness on BIOSCAN-5M, a dataset of over 2 million unique DNA barcodes. Our approach significantly outperforms existing models in both closed-world and open-world tasks. Through ablation studies, we demonstrate that domain-specific architectural adaptations—rather than direct NLP transfer—are crucial for genomic sequence modelling. These findings provide a foundation for designing more effective DNA foundation models that better capture the statistical and structural properties of genomic data.

\noindent
Masked language modelling (MLM) as a pretraining objective has been widely adopted in genomic sequence modelling. While pretrained models can successfully serve as encoders for various downstream tasks, the distribution shift between pretraining and inference detrimentally impacts performance, as the pretraining task is to map \texttt{[MASK]} tokens to predictions, yet the \texttt{[MASK]} is absent during downstream applications. This means the encoder does not prioritize its encodings of non-\texttt{[MASK]} tokens, and expends parameters and compute on work only relevant to the MLM task, despite this being irrelevant at deployment time.
In this work, we propose a modified encoder-decoder architecture based on the masked autoencoder framework, designed to address this inefficiency within a BERT-based transformer. We empirically show that the resulting mismatch is particularly detrimental in genomic pipelines where models are often used for feature extraction without fine-tuning. We evaluate our approach on the BIOSCAN-5M dataset, comprising over 2 million unique DNA barcodes. We achieve substantial performance gains in both closed-world and open-world classification tasks when compared against causal models and bidirectional architectures pretrained with MLM tasks. 
\end{abstract}