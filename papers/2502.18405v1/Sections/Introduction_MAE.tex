\section{Introduction}
% Over the past decades, machine learning applications in computational biology have heavily borrowed from advances in natural language processing (NLP) and computer vision. This cross-domain transfer has been particularly effective in the development of DNA foundation models, which now come in a wide variety of architectures, including transformers 
% \citep{Zhou2021DNABERT,arias2023barcodebert}, state space models (SSMs) \citep{poli2023hyena}, and convolutional neural networks (CNNs) \citep{BadirliNEURIPS2021}. These models leverage different pretraining strategies, from causal to bidirectional learning, enabling strong performance across diverse genomic tasks. While this knowledge transfer from one domain to another has accelerated progress, it also relies on the implicit assumption that genomic data can be treated analogously to data from other domains. This assumption overlooks the unique statistical and structural properties of genomic sequences, which may lead to suboptimal design choices when models are not adapted to the specific characteristics of biological data.


%Over the past few years, 
DNA foundation models have emerged as effective tools for analyzing genomic sequences, utilizing a wide variety of architectures, including transformers \citep{Zhou2021DNABERT,arias2023barcodebert, zhou2024dnaberts}, state space models (SSMs) \citep{poli2023hyena, gao2024barcodemambastatespacemodels}, and convolutional neural networks (CNNs) \citep{Banegas_2023}. These models leverage different pretraining strategies, from causal to bidirectional learning, enabling strong performance across diverse genomic tasks. Among these pretraining strategies, masked language modelling (MLM) has become widely adopted, enabling models to learn effective sequence representations for downstream tasks like specimen identification to taxon and species discovery. However, the effectiveness of MLM is highly dependent on how masking is implemented, as different strategies can affect the model's performance.

In DNA sequence modelling, foundation models typically adopt BERT's three-part masking strategy \citep{Devlin2019BERTPO}, where 80\% of selected tokens are replaced with \texttt{[MASK]}, 10\% remain unchanged, and 10\% are randomly substituted. Models such as the Nucleotide Transformer \citep{NucleotideTransformerDalla-Torre2023} and BarcodeBERT \citep{arias2023barcodebert} followed this approach, while DNABERT \citep{Zhou2021DNABERT} and DNABERT-2 \citep{zhou2023dnabert2} adopted a simpler strategy, replacing 100\% of selected positions with \texttt{[MASK]} tokens. Despite its popularity, MLM introduces a notable limitation: a distribution shift between pretraining and inference due to the absence of \texttt{[MASK]} tokens during downstream tasks. This mismatch leads to representational inefficiencies, as models prioritize the quality of encodings and predictions corresponding to \texttt{[MASK]} tokens but lack a direct target for non-\texttt{[MASK]} token inputs. Consequently, they allocate parameters and compute to tokens never encountered during inference, potentially limiting their ability to capture biologically relevant patterns. While this limitation and its impact on model performance have been studied in natural language processing (NLP) settings \citep{meng2024maelm, Electra}, its effects on DNA sequence foundation models remain unexplored.

In this study, we propose â€ŒBarcodeMAE which uses a modified encoder-decoder architecture based on the masked autoencoder for MLM (MAE-LM; \citealp{meng2024maelm}). BarcodeMAE is designed to address the MLM inefficiency with BERT-style transformer models for biodiversity analysis using DNA barcodes. This approach eliminates \texttt{[MASK]} tokens during encoding, thereby mitigating the distribution shift between pretraining and inference. Computation and parameters needed to predict values for \texttt{[MASK]} tokens is isolated to the decoder block, which is discarded after pretraining and not called at inference time. We empirically show that this mismatch is particularly detrimental in genomic pipelines where models are used for feature extraction without fine-tuning. 
To evaluate our model, we conduct self-supervised pretraining on the BIOSCAN-5M dataset \citep{gharaee2024bioscan5m}, which comprises over 2 million unique DNA barcodes. 
Our model outperforms existing foundation models in genus-level classification, surpassing a comparable encoder-only architecture by over 10 percentage points. Although it does not achieve the highest performance in BIN reconstruction, BarcodeMAE demonstrates superior average performance across evaluation tasks.
% improving the average score by 99\% over the best baseline across both closed-world and open-world tasks.

% Although it does not achieve the highest performance in BIN reconstruction, BarcodeMAE has superior balanced performance across evaluation tasks, by achieving 9\% better average score compared to the best baseline across both closed-world and open-world tasks.


% BarcodeMAE improves upon the encoder-only bidirectional architectures by 7\%, achieving the overall highest performance in both closed-world and open-world tasks. 

%Our results highlight that addressing the \texttt{[MASK]} token mismatch can lead to significant improvements especially, in DNA sequence modelling tasks.



