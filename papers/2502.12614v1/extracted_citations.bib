@inproceedings{crfie,
    title = "Modeling Instance Interactions for Joint Information Extraction with Neural High-Order Conditional Random Field",
    author = "Jia, Zixia  and
      Yan, Zhaohui  and
      Han, Wenjuan  and
      Zheng, Zilong  and
      Tu, Kewei",
    booktitle = "ACL",
    year = "2023",
    pages = "13695--13710",
}

@inproceedings{deepstruct,
  title     = {{D}eep{S}truct: Pretraining of Language Models for Structure Prediction},
  author    = {Wang, Chenguang  and
               Liu, Xiao  and
               Chen, Zui  and
               Hong, Haoyun  and
               Tang, Jie  and
               Song, Dawn},
  booktitle = {ACL},
  year      = {2022},
  pages     = {803--823},
  abstract  = {We introduce a method for improving the structural understanding abilities of language models. Unlike previous approaches that finetune the models with task-specific augmentation, we pretrain language models to generate structures from the text on a collection of task-agnostic corpora. Our structure pretraining enables zero-shot transfer of the learned knowledge that models have about the structure tasks. We study the performance of this approach on 28 datasets, spanning 10 structure prediction tasks including open information extraction, joint entity and relation extraction, named entity recognition, relation classification, semantic role labeling, event extraction, coreference resolution, factual probe, intent detection, and dialogue state tracking. We further enhance the pretraining with the task-specific training sets. We show that a 10B parameter language model transfers non-trivially to most tasks and obtains state-of-the-art performance on 21 of 28 datasets that we evaluate. Our code and datasets will be made publicly available.}
}

@inproceedings{dygiepp,
  title     = {Entity, Relation, and Event Extraction with Contextualized Span Representations},
  author    = {Wadden, David  and
               Wennberg, Ulme  and
               Luan, Yi  and
               Hajishirzi, Hannaneh},
  booktitle = {EMNLP-IJCNLP},
  year      = {2019},
  pages     = {5784--5789},
  abstract  = {We examine the capabilities of a unified, multi-task framework for three information extraction tasks: named entity recognition, relation extraction, and event extraction. Our framework (called DyGIE++) accomplishes all tasks by enumerating, refining, and scoring text spans designed to capture local (within-sentence) and global (cross-sentence) context. Our framework achieves state-of-the-art results across all tasks, on four datasets from a variety of domains. We perform experiments comparing different techniques to construct span representations. Contextualized embeddings like BERT perform well at capturing relationships among entities in the same or adjacent sentences, while dynamic span graph updates model long-range cross-sentence relationships. For instance, propagating span representations via predicted coreference links can enable the model to disambiguate challenging entity mentions. Our code is publicly available at https://github.com/dwadden/dygiepp and can be easily adapted for new tasks or datasets.}
}

@article{flan-t5,
  author     = {Hyung Won Chung and
                Le Hou and
                Shayne Longpre and
                Barret Zoph and
                Yi Tay and
                William Fedus and
                Eric Li and
                Xuezhi Wang and
                Mostafa Dehghani and
                Siddhartha Brahma and
                Albert Webson and
                Shixiang Shane Gu and
                Zhuyun Dai and
                Mirac Suzgun and
                Xinyun Chen and
                Aakanksha Chowdhery and
                Sharan Narang and
                Gaurav Mishra and
                Adams Yu and
                Vincent Y. Zhao and
                Yanping Huang and
                Andrew M. Dai and
                Hongkun Yu and
                Slav Petrov and
                Ed H. Chi and
                Jeff Dean and
                Jacob Devlin and
                Adam Roberts and
                Denny Zhou and
                Quoc V. Le and
                Jason Wei},
  title      = {Scaling Instruction-Finetuned Language Models},
  journal    = {CoRR},
  volume     = {abs/2210.11416},
  year       = {2022},
  url        = {https://doi.org/10.48550/arXiv.2210.11416},
  doi        = {10.48550/arXiv.2210.11416},
  eprinttype = {arXiv},
  eprint     = {2210.11416},
  timestamp  = {Wed, 26 Oct 2022 08:16:51 +0200},
  biburl     = {https://dblp.org/rec/journals/corr/abs-2210-11416.bib},
  bibsource  = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{fsuie,
    title = "{FSUIE}: A Novel Fuzzy Span Mechanism for Universal Information Extraction",
    author = "Peng, Tianshuo  and
      Li, Zuchao  and
      Zhang, Lefei  and
      Du, Bo  and
      Zhao, Hai",
    booktitle = "ACL",
    year = "2023",
    pages = "16318--16333",
}

@inproceedings{genie,
  title     = {{G}en{IE}: Generative Information Extraction},
  author    = {Josifoski, Martin  and
               De Cao, Nicola  and
               Peyrard, Maxime  and
               Petroni, Fabio  and
               West, Robert},
  booktitle = {NAACL},
  year      = {2022},
  pages     = {4626--4643},
  abstract  = {Structured and grounded representation of text is typically formalized by closed information extraction, the problem of extracting an exhaustive set of (subject, relation, object) triplets that are consistent with a predefined set of entities and relations from a knowledge base schema. Most existing works are pipelines prone to error accumulation, and all approaches are only applicable to unrealistically small numbers of entities and relations. We introduce GenIE (generative information extraction), the first end-to-end autoregressive formulation of closed information extraction. GenIE naturally exploits the language knowledge from the pre-trained transformer by autoregressively generating relations and entities in textual form. Thanks to a new bi-level constrained generation strategy, only triplets consistent with the predefined knowledge base schema are produced. Our experiments show that GenIE is state-of-the-art on closed information extraction, generalizes from fewer training data points than baselines, and scales to a previously unmanageable number of entities and relations. With this work, closed information extraction becomes practical in realistic scenarios, providing new opportunities for downstream tasks. Finally, this work paves the way towards a unified end-to-end approach to the core tasks of information extraction.}
}

@misc{gollie,
      title={GoLLIE: Annotation Guidelines improve Zero-Shot Information-Extraction}, 
      author={Oscar Sainz and Iker Garc√≠a-Ferrero and Rodrigo Agerri and Oier Lopez de Lacalle and German Rigau and Eneko Agirre},
      year={2024},
      eprint={2310.03668},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2310.03668}, 
}

@inproceedings{hvpnet,
    title = "Good Visual Guidance Make A Better Extractor: Hierarchical Visual Prefix for Multimodal Entity and Relation Extraction",
    author = "Chen, Xiang  and
      Zhang, Ningyu  and
      Li, Lei  and
      Yao, Yunzhi  and
      Deng, Shumin  and
      Tan, Chuanqi  and
      Huang, Fei  and
      Si, Luo  and
      Chen, Huajun",
    booktitle = "NAACL",
    year = "2022",
    pages = "1607--1618",
    abstract = "Multimodal named entity recognition and relation extraction (MNER and MRE) is a fundamental and crucial branch in information extraction. However, existing approaches for MNER and MRE usually suffer from error sensitivity when irrelevant object images incorporated in texts. To deal with these issues, we propose a novel Hierarchical Visual Prefix fusion NeTwork (HVPNeT) for visual-enhanced entity and relation extraction, aiming to achieve more effective and robust performance. Specifically, we regard visual representation as pluggable visual prefix to guide the textual representation for error insensitive forecasting decision. We further propose a dynamic gated aggregation strategy to achieve hierarchical multi-scaled visual features as visual prefix for fusion. Extensive experiments on three benchmark datasets demonstrate the effectiveness of our method, and achieve state-of-the-art performance.",
}

@inproceedings{hyspa,
    title = "{H}y{SPA}: Hybrid Span Generation for Scalable Text-to-Graph Extraction",
    author = "Ren, Liliang  and
      Sun, Chenkai  and
      Ji, Heng  and
      Hockenmaier, Julia",
    booktitle = "ACL-IJCNLP",
    year = "2021",
    pages = "4066--4078",
}

@article{instructuie,
  author       = {Xiao Wang and
                  Weikang Zhou and
                  Can Zu and
                  Han Xia and
                  Tianze Chen and
                  Yuansen Zhang and
                  Rui Zheng and
                  Junjie Ye and
                  Qi Zhang and
                  Tao Gui and
                  Jihua Kang and
                  Jingsheng Yang and
                  Siyuan Li and
                  Chunsai Du},
  title        = {InstructUIE: Multi-task Instruction Tuning for Unified Information
                  Extraction},
  journal      = {CoRR},
  volume       = {abs/2304.08085},
  year         = {2023},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2304-08085.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{knowcoder,
  title={KnowCoder: Coding Structured Knowledge into LLMs for Universal Information Extraction},
  author={Li, Zixuan and Zeng, Yutao and Zuo, Yuxin and Ren, Weicheng and Liu, Wenxuan and Su, Miao and Guo, Yucan and Liu, Yantao and Li, Xiang and Hu, Zhilei and others},
  journal={arXiv preprint arXiv:2403.07969},
  year={2024}
}

@inproceedings{lasuie,
  author       = {Hao Fei and
                  Shengqiong Wu and
                  Jingye Li and
                  Bobo Li and
                  Fei Li and
                  Libo Qin and
                  Meishan Zhang and
                  Min Zhang and
                  Tat{-}Seng Chua},
  title        = {LasUIE: Unifying Information Extraction with Latent Adaptive Structure-aware
                  Generative Language Model},
  booktitle    = {NeurIPS},
  year         = {2022},
  timestamp    = {Mon, 08 Jan 2024 16:31:25 +0100},
  biburl       = {https://dblp.org/rec/conf/nips/0001WLLLQZZC22.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{lpt,
    title = "Prompts Can Play Lottery Tickets Well: Achieving Lifelong Information Extraction via Lottery Prompt Tuning",
    author = "Liang, Zujie  and
      Wei, Feng  and
      Jie, Yin  and
      Qian, Yuxi  and
      Hao, Zhenghong  and
      Han, Bing",
    booktitle = "ACL",
    year = "2023",
    pages = "277--292",
}

@misc{metaretrieval,
      title={Universal Information Extraction with Meta-Pretrained Self-Retrieval}, 
      author={Xin Cong. Bowen Yu and Mengcheng Fang and Tingwen Liu and Haiyang Yu and Zhongkai Hu and Fei Huang and Yongbin Li and Bin Wang},
      year={2023},
      eprint={2306.10444},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2306.10444}, 
}

@inproceedings{mirror,
  author       = {Tong Zhu and
                  Junfei Ren and
                  Zijian Yu and
                  Mengsong Wu and
                  Guoliang Zhang and
                  Xiaoye Qu and
                  Wenliang Chen and
                  Zhefeng Wang and
                  Baoxing Huai and
                  Min Zhang},
  title        = {Mirror: {A} Universal Framework for Various Information Extraction
                  Tasks},
  booktitle    = {EMNLP},
  pages        = {8861--8876},
  year         = {2023},
  biburl       = {https://dblp.org/rec/conf/emnlp/0002RYWZQCWHZ23.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{oneie,
  title     = {A Joint Neural Model for Information Extraction with Global Features},
  author    = {Lin, Ying  and
               Ji, Heng  and
               Huang, Fei  and
               Wu, Lingfei},
  booktitle = {ACL},
  year      = {2020},
  pages     = {7999--8009},
  abstract  = {Most existing joint neural models for Information Extraction (IE) use local task-specific classifiers to predict labels for individual instances (e.g., trigger, relation) regardless of their interactions. For example, a victim of a die event is likely to be a victim of an attack event in the same sentence. In order to capture such cross-subtask and cross-instance inter-dependencies, we propose a joint neural framework, OneIE, that aims to extract the globally optimal IE result as a graph from an input sentence. OneIE performs end-to-end IE in four stages: (1) Encoding a given sentence as contextualized word representations; (2) Identifying entity mentions and event triggers as nodes; (3) Computing label scores for all nodes and their pairwise links using local classifiers; (4) Searching for the globally optimal graph with a beam decoder. At the decoding stage, we incorporate global features to capture the cross-subtask and cross-instance interactions. Experiments show that adding global features improves the performance of our model and achieves new state of-the-art on all subtasks. In addition, as OneIE does not use any language-specific feature, we prove it can be easily applied to new languages or trained in a multilingual manner.}
}

@inproceedings{tanl,
  author    = {Giovanni Paolini and
               Ben Athiwaratkun and
               Jason Krone and
               Jie Ma and
               Alessandro Achille and
               Rishita Anubhai and
               C{\'{\i}}cero Nogueira dos Santos and
               Bing Xiang and
               Stefano Soatto},
  title     = {Structured Prediction as Translation between Augmented Natural Languages},
  booktitle = {{ICLR}},
  year      = {2021},
  url       = {https://openreview.net/forum?id=US-TP-xnXI},
  timestamp = {Wed, 28 Jul 2021 14:18:31 +0200},
  biburl    = {https://dblp.org/rec/conf/iclr/PaoliniAKMAASXS21.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{tmr,
    title = "Rethinking Multimodal Entity and Relation Extraction from a Translation Point of View",
    author = "Zheng, Changmeng  and
      Feng, Junhao  and
      Cai, Yi  and
      Wei, Xiaoyong  and
      Li, Qing",
    booktitle = "ACL",
    year = "2023",
    pages = "6810--6824",
    abstract = "We revisit the multimodal entity and relation extraction from a translation point of view. Special attention is paid on the misalignment issue in text-image datasets which may mislead the learning. We are motivated by the fact that the cross-modal misalignment is a similar problem of cross-lingual divergence issue in machine translation. The problem can then be transformed and existing solutions can be borrowed by treating a text and its paired image as the translation to each other. We implement a multimodal back-translation using diffusion-based generative models for pseudo-paralleled pairs and a divergence estimator by constructing a high-resource corpora as a bridge for low-resource learners. Fine-grained confidence scores are generated to indicate both types and degrees of alignments with which better representations are obtained. The method has been validated in the experiments by outperforming 14 state-of-the-art methods in both entity and relation extraction tasks. The source code is available at \url{https://github.com/thecharm/TMR}.",
}

@inproceedings{uie,
  title     = {Unified Structure Generation for Universal Information Extraction},
  author    = {Lu, Yaojie  and
               Liu, Qing  and
               Dai, Dai  and
               Xiao, Xinyan  and
               Lin, Hongyu  and
               Han, Xianpei  and
               Sun, Le  and
               Wu, Hua},
  booktitle = {ACL},
  year      = {2022},
  pages     = {5755--5772},
  abstract  = {Information extraction suffers from its varying targets, heterogeneous structures, and demand-specific schemas. In this paper, we propose a unified text-to-structure generation framework, namely UIE, which can universally model different IE tasks, adaptively generate targeted structures, and collaboratively learn general IE abilities from different knowledge sources. Specifically, UIE uniformly encodes different extraction structures via a structured extraction language, adaptively generates target extractions via a schema-based prompt mechanism {--} structural schema instructor, and captures the common IE abilities via a large-scale pretrained text-to-structure model. Experiments show that UIE achieved the state-of-the-art performance on 4 IE tasks, 13 datasets, and on all supervised, low-resource, and few-shot settings for a wide range of entity, relation, event and sentiment extraction tasks and their unification. These results verified the effectiveness, universality, and transferability of UIE.}
}

@inproceedings{umgf,
  author       = {Dong Zhang and
                  Suzhong Wei and
                  Shoushan Li and
                  Hanqian Wu and
                  Qiaoming Zhu and
                  Guodong Zhou},
  title        = {Multi-modal Graph Fusion for Named Entity Recognition with Targeted
                  Visual Guidance},
  booktitle    = {AAAI},
  pages        = {14347--14355},
  year         = {2021},
  biburl       = {https://dblp.org/rec/conf/aaai/ZhangWLWZZ21.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{uniex,
    title = "{U}ni{EX}: An Effective and Efficient Framework for Unified Information Extraction via a Span-extractive Perspective",
    author = "Ping, Yang  and
      Lu, JunYu  and
      Gan, Ruyi  and
      Wang, Junjie  and
      Zhang, Yuxiang  and
      Zhang, Pingjian  and
      Zhang, Jiaxing",
    booktitle = "ACL",
    year = "2023",
    pages = "16424--16440",
}

