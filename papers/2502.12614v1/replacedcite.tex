\section{Related Work}
\label{related-work}

% UIE has been a popular theme since only a single universal model is needed, avoiding the hassle of designing different model structures for multiple tasks. 
% IE tasks can be formulated following two paradigm: autoregressive sequence-to-sequence generation and graph structure decoding.
% UIE can be divided into two categories: generative UIE and extractive UIE. 
\paragraph{Generative UIE} TANL____ sees IE tasks as a sequence-to-sequence problem and utilizes T5 as the generative model. UIE____ also uses T5 as the backbone. In addition, UIE designs Structured Extraction Language (SEL) that can represent diversified IE tasks, thereby enabling it to perform on a wider range of IE tasks.
InstructUIE____ further incorporates the idea of instruction-tuning and utilizes FlanT5-11B____ for IE tasks. DeepStruct____ and GenIE____ both formulate the generated sequence as subject-relation-object triplets, with DeepStruct having a larger model size (10B). LasUIE____ proposes a novel structure-aware generative language model to unleash the power of syntactic knowledge. FSUIE____ introduces fuzzy span loss and fuzzy span attention to reduce over-reliance on span boundaries. 
GOLLIE____ improves zero-shot results on unseen IE tasks by virtue of being fine-tuned to comply with annotation
guidelines.
TMR____ addresses text-image misalignment by introducing a back-translation method using diffusion-based generative models. KnowCoder____  introduces a
code-style schema representation method. While the above generative UIE approaches offer a powerful solution for diversified IE tasks, they do not possess any notable advantages when it comes to efficiency. 
% due to their reliance on large generative models as a foundation. 
% LPT____ sequentially learns compact pruned prompt vectors for each task to achieve lifelong learning for UIE. 
\paragraph{Extractive UIE} DyGIE++____ utilizes a dynamic span graph to model long-range relations, and with graph propagation, the model can disambiguate challenging entity mentions.
UniEX____ converts IE tasks into a token-pair problem, develops a traffine attention mechanism to integrate heterogeneous factors, and obtains the extraction target via a scoring matrix. These single-target extractive UIE approaches can achieve strong performance using a relatively small model; however, they lack the ability to model correlations between relations, thus limiting their capability to extract complex relations. 
\begin{figure*}[t]
    \centering
    % \includegraphics[width=\textwidth]{figs/framework2.pdf}
\includegraphics[width=0.95\textwidth]{figs/multi_modal_LDNet.pdf}
    \caption{
         The overview framework of LDNet. LDNet constructs a unified input format, which combines instruction, schema labels, and text. The representation obtained from the PLM is fused with image representation obtained with the image backbone. The multi-modal representation is fed into the multi-aspect relation modeling component to produce probability matrices for TA, A2A, and AS relations, respectively. These matrices are then subjected to label drop to mask out non-existent relations. Finally, the probability matrices are fed into the decoding process to generate target structures.
    }
    \label{fig:model-framework}
\end{figure*}

OneIE____ also uses a span graph, but unlike DyGIE++, it incorporates global features and adopts a CRF-based tagger to remove the constraint on the length of extracted mentions.
% identifies entity mentions and event triggers as nodes and applies beam search to find the optimal graph considering global features.
% HySPA____ invertibly maps the information graph to a sequence of spans and edge types and leverages a hybrid span decoder to generate such sequences recurrently with linear time and space complexities. 
UMGF____ adopts a unified intra-modal and inter-modal graph fusion method to represent visual and textual features within the same embedding space.
HVPNeT____ designs pyramidal features for images, employing visual representations as insertable visual prefixes to guide error-insensitive predictive decisions of textual representations. 
% CRFIE____ formulates IE as a high-order conditional random field and designs binary factors and ternary factors to directly model interactions between not only a pair of instances but also triplets.
MetaRetriever____ retrieves task-specific knowledge from pre-trained language models to enhance performance.
Mirror____ transforms multiple tasks into a multi-span cyclic graph and predicts relations by verifying whether a cycle exists between slots in a tuple. While these multiple-target extractive UIE approaches take interactions between relations into account, they also include irrelevant relations, which leads to decision complexity and inaccuracy.

It is worth noting that the existing UIE models have basically only conducted experiments on single-modal or multi-modal IE tasks, and have not handled single-modal and multi-modal IE tasks simultaneously like LDNet.