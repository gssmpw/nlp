\section{Related Work}
\begin{figure}[t]
\centering
\begin{minipage}{\columnwidth} 
    \centering 
    \small
    \renewcommand\tabcolsep{2pt} 
    \renewcommand\arraystretch{1.1}
    \resizebox{\columnwidth}{!}{
        \begin{tabular}{lccccc}
            \toprule
            \textbf{Benchmarks} & \textbf{Venue} & \textbf{Size} & \textbf{\#Topics} & \textbf{Modality} & \textbf{\#Traits} \\
            \midrule
            $\text{ASAP}_{\text{AES}}$ ____ & ACL & 17,450 & 8 & T & 0 \\
            ASAP++ ____ & ACL & 10,696 & 6 & T & 8 \\
            CLC-FCE ____ & ACL & 1,244 & 10 & T & 0 \\
            TOEFL11 ____ & EMNLP & 1,100 & 8 & T & 0 \\
            ICLE ____ & COLING & 3,663 & 48 & T & 4\\
            AAE ____ & COLING & 102 & 101 & T & 1 \\
            ICLE++ ____ & NAACL & 1,008 & 10 & T & 10 \\
            CREE ____ & BEA & 566 & 75 & T & 1 \\
            \midrule
            \dataset (Ours) & - & 1054 & \colorbox{red!20}{125} & T,\colorbox{red!20}{I} & \colorbox{red!20}{10} \\
            \bottomrule
        \end{tabular}
    }
    \captionof{table}{Comparison between previous AES benchmarks and our proposed \dataset. The cells highlighted in \colorbox{red!20}{red} indicate the highest number for \textit{\#Topics} and \textit{\#Traits} columns, and the unique modality for \textit{Modality} column.}
    \label{tab:AES datasets}
    \vspace{-2mm}
\end{minipage}
\end{figure}


\subsection{AES Datasets}
Existing AES datasets have advanced the field but remain some limitations (shown in Table \ref{tab:AES datasets}) ____. For example, $\text{ASAP}_{\text{AES}}$ is notable for its size, enabling high-performance prompt-specific systems ____. However, differing score ranges across prompts and heavy preprocessing (\textit{e.g.}, removal of paragraph structures and named entities) reduce its utility. ASAP++ is an extension of ASAP that introduces trait-specific scores ____. However, its traits are coarse-grained, with all content-based traits (\textit{e.g}., coherence, persuasiveness, and thesis clarity) grouped into a single "CONTENT" category. The CLC-FCE dataset includes holistic scores and linguistic error annotations, supporting grammatical error detection alongside scoring tasks, but the small number of essays per prompt hinders the development of prompt-specific systems ____. TOEFL11 dataset focuses on native language identification and provides only coarse-grained proficiency labels (low, medium \& high), which do not fully capture essay quality. ICLE ____ and ICLE++ ____ datasets provide some of the most detailed trait-specific annotations, with ICLE++ scoring essays on 10 dimensions of writing quality. Nevertheless, these datasets are still constrained by limited topic diversity. Similarly, The AAE corpus includes 102 persuasive essays and only focuses on argument structure ____. To address the aforementioned limitations, we propose the \dataset benchmark, which features multimodal context, 125 unique essay topics, and comprehensive scoring across 10 distinct traits.



\subsection{AES Systems}
AES research focuses on three main categories: heuristic approaches, machine learning approaches, and deep learning approaches ____. Heuristic AES approaches focus on holistic scoring by combining trait scores such as Organization, Coherence, and Grammar into a weighted sum. Trait-specific scores are computed using rules, like assessing Organization based on a five-paragraph format ____. Machine learning approaches (\textit{e.g.}, Logistic Regression and Support Vector Machine) rely on handcrafted features, such as lexical ____, length-based ____, and discourse features ____, and perform well in within-prompt scoring but struggle with generalization to new prompts. Deep learning approaches, particularly those using Transformer architectures like BERT ____, have advanced AES by learning essay representations directly from text, enabling multi-trait and cross-prompt scoring. Among these, LLM-based approaches stand out for their ability to leverage commonsense knowledge and understand complex instructions ____. By using prompts, LLMs can perform AES in zero-shot settings with rubrics alone ____ or in few-shot settings with minimal labeled data ____. These methods enhance flexibility, scalability, and performance, especially in low-resource scenarios.

\subsection{Multimodal Large Language Models}
MLLMs have brought significant advancements to diverse tasks and applications ____. Proprietary MLLMs such as GPT-4o ____ and Gemini-1.5 ____ have shown remarkable capabilities in multimodal challenges, excelling in areas such as multimodal reasoning and QA ____. At the same time, Open-source MLLMs have made considerable strides. For instance, LLaVA-NEXT ____ utilizes a pretrained vision encoder to generate visual embeddings, which are then aligned with text embeddings through a lightweight adapter, enabling effective multimodal understanding. Similarly, MLLMs such as Qwen2-VL ____, DeepSeek-VL ____, InternVL ____, MiniCPM ____, Ovis ____, LLaMA3 ____ and Yi-VL ____ implement innovative projection techniques to combine visual and textual features effectively, enabling many multimodal applications. These models showcase the growing potential of MLLMs in advancing both research and practical applications that rely on multimodal data ____. Therefore, we introduce \dataset, a novel benchmark designed to evaluate MLLMsâ€™ capability to score essays with multimodal context, paving the way for AGI systems ____.
\vspace{-2mm}