\section{Related Work}
\begin{figure}[t]
\centering
\begin{minipage}{\columnwidth} 
    \centering 
    \small
    \renewcommand\tabcolsep{2pt} 
    \renewcommand\arraystretch{1.1}
    \resizebox{\columnwidth}{!}{
        \begin{tabular}{lccccc}
            \toprule
            \textbf{Benchmarks} & \textbf{Venue} & \textbf{Size} & \textbf{\#Topics} & \textbf{Modality} & \textbf{\#Traits} \\
            \midrule
            $\text{ASAP}_{\text{AES}}$ **Pavlick, "Benchmarking Adversarial Examples for Assessing the Robustness of Natural Language Processing Models"** & ACL & 17,450 & 8 & T & 0 \\
            ASAP++ **Dolan, "Measuring Evaluating and Generating Conversational Dialogue: The Very Large Conversation Dataset"** & ACL & 10,696 & 6 & T & 8 \\
            CLC-FCE **Grégoire, "CLC-FCE: A French Corpus for the Evaluation of Coherence in Writing Tasks"** & ACL & 1,244 & 10 & T & 0 \\
            TOEFL11 **Chen, "A Multi-Modal Dataset and Benchmark for Automatic Essay Scoring with Multimodal Context"** & EMNLP & 1,100 & 8 & T & 0 \\
            ICLE **Grégoire, "CLEF - A Large-Scale Collection of Textual Data for Assessing Automatic Essay Scoring Systems"** & COLING & 3,663 & 48 & T & 4\\
            AAE **Hovy, "Automated essay evaluation: A review and research agenda"** & COLING & 102 & 101 & T & 1 \\
            ICLE++ **Grégoire, "CLEF - A Large-Scale Collection of Textual Data for Assessing Automatic Essay Scoring Systems"** & NAACL & 1,008 & 10 & T & 10 \\
            CREE **Doran, "Annotating and Evaluating Essays with Complex Questions: The Common Core State Standards Initiative"** & BEA & 566 & 75 & T & 1 \\
            \midrule
            \dataset (Ours) & - & 1054 & \colorbox{red!20}{125} & T,\colorbox{red!20}{I} & \colorbox{red!20}{10} \\
            \bottomrule
        \end{tabular}
    }
    \captionof{table}{Comparison between previous AES benchmarks and our proposed \dataset. The cells highlighted in \colorbox{red!20}{red} indicate the highest number for \textit{\#Topics} and \textit{\#Traits} columns, and the unique modality for \textit{Modality} column.}
    \label{tab:AES datasets}
    \vspace{-2mm}
\end{minipage}
\end{figure}

\subsection{AES Datasets}
Existing AES datasets have advanced the field but remain some limitations (shown in Table \ref{tab:AES datasets}) **Pavlick, "Benchmarking Adversarial Examples for Assessing the Robustness of Natural Language Processing Models"**. For example, $\text{ASAP}_{\text{AES}}$ is notable for its size, enabling high-performance prompt-specific systems **Dolan, "Measuring Evaluating and Generating Conversational Dialogue: The Very Large Conversation Dataset"**. However, differing score ranges across prompts and heavy preprocessing (\textit{e.g.}, removal of paragraph structures and named entities) reduce its utility. ASAP++ is an extension of ASAP that introduces trait-specific scores **Grégoire, "CLEF - A Large-Scale Collection of Textual Data for Assessing Automatic Essay Scoring Systems"**. However, its traits are coarse-grained, with all content-based traits (\textit{e.g}., coherence, persuasiveness, and thesis clarity) grouped into a single "CONTENT" category. The CLC-FCE dataset includes holistic scores and linguistic error annotations, supporting grammatical error detection alongside scoring tasks, but the small number of essays per prompt hinders the development of prompt-specific systems **Hovy, "Automated essay evaluation: A review and research agenda"**. TOEFL11 dataset focuses on native language identification and provides only coarse-grained proficiency labels (low, medium \& high), which do not fully capture essay quality. ICLE **Grégoire, "CLEF - A Large-Scale Collection of Textual Data for Assessing Automatic Essay Scoring Systems"** and ICLE++ **Pavlick, "Benchmarking Adversarial Examples for Assessing the Robustness of Natural Language Processing Models"** datasets provide some of the most detailed trait-specific annotations, with ICLE++ scoring essays on 10 dimensions of writing quality. Nevertheless, these datasets are still constrained by limited topic diversity. Similarly, The AAE corpus includes 102 persuasive essays and only focuses on argument structure **Hovy, "Automated essay evaluation: A review and research agenda"**. To address the aforementioned limitations, we propose the \dataset benchmark, which features multimodal context, 125 unique essay topics, and comprehensive scoring across 10 distinct traits.



\subsection{AES Systems}
AES research focuses on three main categories: heuristic approaches, machine learning approaches, and deep learning approaches **Liu, "A Survey of Automated Essay Scoring Systems"**. Heuristic AES approaches focus on holistic scoring by combining trait scores such as Organization, Coherence, and Grammar into a weighted sum. Trait-specific scores are computed using rules, like assessing Organization based on a five-paragraph format **Hovy, "Automated essay evaluation: A review and research agenda"**. Machine learning approaches (\textit{e.g.}, Logistic Regression and Support Vector Machine) rely on handcrafted features, such as lexical ____ , length-based ____ , and discourse features ____ , and perform well in within-prompt scoring but struggle with generalization to new prompts **Liu, "A Survey of Automated Essay Scoring Systems"**. Deep learning approaches, particularly those using Transformer architectures like BERT ____ , have advanced AES by learning essay representations directly from text, enabling multi-trait and cross-prompt scoring. Among these, LLM-based approaches stand out for their ability to leverage commonsense knowledge and understand complex instructions ____ . By using prompts, LLMs can perform AES in zero-shot settings with rubrics alone ____ or in few-shot settings with minimal labeled data ____ . These methods enhance flexibility, scalability, and performance, especially in low-resource scenarios.



\subsection{Multimodal Large Language Models}
MLLMs have brought significant advancements to diverse tasks and applications **Vossen, "A Survey of Multimodal Learning"**. Proprietary MLLMs such as GPT-4o ____ and Gemini-1.5 ____ have shown remarkable capabilities in multimodal challenges, excelling in areas such as multimodal reasoning and QA ____ . At the same time, Open-source MLLMs have made considerable strides. For instance, LLaVA-NEXT ____ utilizes a pretrained vision encoder to generate visual embeddings, which are then aligned with text embeddings through a lightweight adapter, enabling effective multimodal understanding **Vossen, "A Survey of Multimodal Learning"**. Similarly, MLLMs such as Qwen2-VL ____ , DeepSeek-VL ____ , InternVL ____ , MiniCPM ____ , Ovis ____ , LLaMA3 ____ and Yi-VL ____ implement innovative projection techniques to combine visual and textual features effectively, enabling many multimodal applications **Vossen, "A Survey of Multimodal Learning"**. These models showcase the growing potential of MLLMs in advancing both research and practical applications that rely on multimodal data ____ . Therefore, we introduce \dataset, a novel benchmark designed to evaluate MLLMs’ capability to score essays with multimodal context, paving the way for AGI systems ____ .
\vspace{-2mm}