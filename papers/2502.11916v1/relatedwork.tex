\section{Related Work}
\begin{figure}[t]
\centering
\begin{minipage}{\columnwidth} 
    \centering 
    \small
    \renewcommand\tabcolsep{2pt} 
    \renewcommand\arraystretch{1.1}
    \resizebox{\columnwidth}{!}{
        \begin{tabular}{lccccc}
            \toprule
            \textbf{Benchmarks} & \textbf{Venue} & \textbf{Size} & \textbf{\#Topics} & \textbf{Modality} & \textbf{\#Traits} \\
            \midrule
            $\text{ASAP}_{\text{AES}}$ \cite{cozma2018ASAP} & ACL & 17,450 & 8 & T & 0 \\
            ASAP++ \cite{mathias2018asap++} & ACL & 10,696 & 6 & T & 8 \\
            CLC-FCE \cite{yannakoudakis2011CLCFCE} & ACL & 1,244 & 10 & T & 0 \\
            TOEFL11 \cite{lee2024TOEFL11} & EMNLP & 1,100 & 8 & T & 0 \\
            ICLE \cite{granger2009icle} & COLING & 3,663 & 48 & T & 4\\
            AAE \cite{stab2014AAE} & COLING & 102 & 101 & T & 1 \\
            ICLE++ \cite{li2024icle++} & NAACL & 1,008 & 10 & T & 10 \\
            CREE \cite{bailey2008CREE} & BEA & 566 & 75 & T & 1 \\
            \midrule
            \dataset (Ours) & - & 1054 & \colorbox{red!20}{125} & T,\colorbox{red!20}{I} & \colorbox{red!20}{10} \\
            \bottomrule
        \end{tabular}
    }
    \captionof{table}{Comparison between previous AES benchmarks and our proposed \dataset. The cells highlighted in \colorbox{red!20}{red} indicate the highest number for \textit{\#Topics} and \textit{\#Traits} columns, and the unique modality for \textit{Modality} column.}
    \label{tab:AES datasets}
    \vspace{-2mm}
\end{minipage}
\end{figure}


\subsection{AES Datasets}
Existing AES datasets have advanced the field but remain some limitations (shown in Table \ref{tab:AES datasets}) \cite{Ke2019survey,li2024recent,li2024reflection}. For example, $\text{ASAP}_{\text{AES}}$ is notable for its size, enabling high-performance prompt-specific systems \cite{cozma2018ASAP}. However, differing score ranges across prompts and heavy preprocessing (\textit{e.g.}, removal of paragraph structures and named entities) reduce its utility. ASAP++ is an extension of ASAP that introduces trait-specific scores \cite{mathias2018asap++,li2024reflection}. However, its traits are coarse-grained, with all content-based traits (\textit{e.g}., coherence, persuasiveness, and thesis clarity) grouped into a single "CONTENT" category. The CLC-FCE dataset includes holistic scores and linguistic error annotations, supporting grammatical error detection alongside scoring tasks, but the small number of essays per prompt hinders the development of prompt-specific systems \cite{yannakoudakis2011CLCFCE,li2024recent}. TOEFL11 dataset focuses on native language identification and provides only coarse-grained proficiency labels (low, medium \& high), which do not fully capture essay quality. ICLE \cite{granger2009icle} and ICLE++ \cite{li2024icle++} datasets provide some of the most detailed trait-specific annotations, with ICLE++ scoring essays on 10 dimensions of writing quality. Nevertheless, these datasets are still constrained by limited topic diversity. Similarly, The AAE corpus includes 102 persuasive essays and only focuses on argument structure \cite{stab2014AAE}. To address the aforementioned limitations, we propose the \dataset benchmark, which features multimodal context, 125 unique essay topics, and comprehensive scoring across 10 distinct traits.



\subsection{AES Systems}
AES research focuses on three main categories: heuristic approaches, machine learning approaches, and deep learning approaches \cite{li2024reflection}. Heuristic AES approaches focus on holistic scoring by combining trait scores such as Organization, Coherence, and Grammar into a weighted sum. Trait-specific scores are computed using rules, like assessing Organization based on a five-paragraph format \cite{Attali2006erator}. Machine learning approaches (\textit{e.g.}, Logistic Regression and Support Vector Machine) rely on handcrafted features, such as lexical \cite{chen2013lexical}, length-based \cite{Sowmya2016linguistic,yannakoudakis2012lenthbased}, and discourse features \cite{yannakoudakis2012lenthbased}, and perform well in within-prompt scoring but struggle with generalization to new prompts. Deep learning approaches, particularly those using Transformer architectures like BERT \cite{wang2022bert}, have advanced AES by learning essay representations directly from text, enabling multi-trait and cross-prompt scoring. Among these, LLM-based approaches stand out for their ability to leverage commonsense knowledge and understand complex instructions \cite{MIZUMOTO2023llmbased}. By using prompts, LLMs can perform AES in zero-shot settings with rubrics alone \cite{lee2024TOEFL11} or in few-shot settings with minimal labeled data \cite{mansour2024fewshoting,xiao2024fewshoting}. These methods enhance flexibility, scalability, and performance, especially in low-resource scenarios.

\subsection{Multimodal Large Language Models}
MLLMs have brought significant advancements to diverse tasks and applications \cite{xi2023rise,huo2024mmneuron,yan2024urbanclip,yan2024georeasoner,zou2025deep,dang2024explainable}. Proprietary MLLMs such as GPT-4o \cite{openai2024gpt4ocard} and Gemini-1.5 \cite{gemini} have shown remarkable capabilities in multimodal challenges, excelling in areas such as multimodal reasoning and QA \cite{chang2024survey,yan2024errorradar,yan2024survey,zheng2024reefknot,yan2025position}. At the same time, Open-source MLLMs have made considerable strides. For instance, LLaVA-NEXT \cite{liu2024llavanext} utilizes a pretrained vision encoder to generate visual embeddings, which are then aligned with text embeddings through a lightweight adapter, enabling effective multimodal understanding. Similarly, MLLMs such as Qwen2-VL \cite{qwen2vl}, DeepSeek-VL \cite{lu2024deepseek}, InternVL \cite{internvl2,internvl2.5}, MiniCPM \cite{hu2024minicpmunveilingpotentialsmall}, Ovis \cite{lu2024ovisstructuralembeddingalignment}, LLaMA3 \cite{grattafiori2024llama3herdmodels} and Yi-VL \cite{young2024yi} implement innovative projection techniques to combine visual and textual features effectively, enabling many multimodal applications. These models showcase the growing potential of MLLMs in advancing both research and practical applications that rely on multimodal data \cite{qu2025tool,zou2024look,zhou2024mitigating,huang2024miner}. Therefore, we introduce \dataset, a novel benchmark designed to evaluate MLLMsâ€™ capability to score essays with multimodal context, paving the way for AGI systems \cite{xiao2024automation,tate2024can,yan2024practical}.
\vspace{-2mm}