%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%2345678901234567890123456789012345678901234567890123456789012345678901234567890
%        1         2         3         4         5         6         7         8
%
% Slightly modified by Shaun Canavan for FG2025
%

%\documentclass[letterpaper, 10 pt, conference]{ieeeconf}  % Comment this line out
                                                          % if you need a4paper
\documentclass[a4paper, 10pt, conference]{ieeeconf}      % Use this line for a4
                                                          % paper
\usepackage{FG2025}
\usepackage{marvosym} % 引入 marvosym 宏包

%\FGfinalcopy % *** Uncomment this line for the final submission



\IEEEoverridecommandlockouts                              % This command is only
                                                          % needed if you want to
                                                          % use the \thanks command
\overrideIEEEmargins
% See the \addtolength command later in the file to balance the column lengths
% on the last page of the document

% The following packages can be found on http:\\www.ctan.org
%\usepackage{graphics} % for pdf, bitmapped graphics files
%\usepackage{epsfig} % for postscript graphics files
%\usepackage{mathptmx} % assumes new font selection scheme installed
%\usepackage{times} % assumes new font selection scheme installed
%\usepackage{amsmath} % assumes amsmath package installed
%\usepackage{amssymb}  % assumes amsmath package installed

\def\FGPaperID{260} % *** Enter the FG2025 Paper ID here

\title{\LARGE \bf
AutoMR: A Universal Time Series Motion Recognition Pipeline
}

%use this in case of a single affiliation
%\author{\parbox{16cm}{\centering
%    {\large Huibert Kwakernaak}\\
%    {\normalsize
%    Faculty of Electrical Engineering, Mathematics and Computer Science, University of Twente, Enschede, The Netherlands\\}}
%    \thanks{This work was not supported by any organization.}% <-this % stops a space
%}

%use this in case of several affiliations
\author{\parbox{16cm}{\centering
    {\large Likun Zhang$^{1,2}$ \texttt{<likun.zhang@xintelligencelabs.ac>}, \\Sicheng Yang$^1$ \texttt{<sicheng.yang@xintelligencelabs.ac>}, \\Zhuo Wang$^1$ \texttt{<zhuo.wang@xintelligencelabs.ac>}, \\Haining Liang$^{3}$ \texttt{<hainingliang@hkust-gz.edu.cn>}, \\Junxiao Shen$^{4, \textrm{\Letter}}$\thanks{\footnotesize Corresponding author} \texttt{<junxiao.shen@bristol.ac.uk>}}\\
    \vspace{0.5cm} % 添加垂直间距
    {\normalsize
    $^1$ X-Intelligence Labs 
    $^2$ University of California, Berkeley 
    $^3$ HKUST (Guangzhou) 
    $^4$ University of Bristol \\
    }
}}

\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{booktabs}
\usepackage{hyperref}


\FGfinalcopy % *** Uncomment this line for the final submission

\begin{document}

\ifFGfinal
\thispagestyle{empty}
\pagestyle{empty}
\else
\author{Anonymous FG2025 submission\\ Paper ID \FGPaperID \\}
\pagestyle{plain}
\fi
\maketitle



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{abstract}

In this paper, we present an end-to-end automated motion recognition (AutoMR) pipeline designed for multimodal datasets. The proposed framework seamlessly integrates data preprocessing, model training, hyperparameter tuning, and evaluation, enabling robust performance across diverse scenarios. Our approach addresses two primary challenges: 1) variability in sensor data formats and parameters across datasets, which traditionally requires task-specific machine learning implementations, and 2) the complexity and time consumption of hyperparameter tuning for optimal model performance. Our library features an all-in-one solution incorporating QuartzNet as the core model, automated hyperparameter tuning, and comprehensive metrics tracking. Extensive experiments demonstrate its effectiveness on 10 diverse datasets, and most achieve state-of-the-art performance. This work lays a solid foundation for deploying motion-capture solutions across varied real-world applications.

\end{abstract}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{INTRODUCTION}

Gesture and motion analysis is crucial in fields such as human-computer interaction~\cite{9666999}, healthcare, and robotics. The growing availability of multimodal datasets from wearable sensors \cite{shen2024ringgesture}, cameras, and motion capture systems has driven the need for scalable and adaptable gesture recognition solutions. However, leveraging these datasets effectively presents several challenges, as illustrated in Figure~\ref{fig:challenges}.

One major challenge is the variability in dataset formats, sampling rates, and noise levels. Data collected from different sensors, such as IMUs, skeletal motion capture, or video-based tracking, often require dataset-specific preprocessing pipelines, increasing complexity and limiting scalability. Additionally, gesture recognition models typically need to be tailored to specific datasets or application scenarios. The absence of a unified framework results in repetitive model design and optimization efforts, leading to increased computational costs and resource demands.

Another obstacle is the complexity of hyperparameter tuning. Selecting appropriate learning rates, batch sizes, and network architectures is critical for achieving high performance but often requires domain expertise and substantial computational resources. This challenge is particularly pronounced for non-experts who lack experience in fine-tuning deep learning models. Furthermore, many existing gesture recognition frameworks require extensive customization and expert knowledge \cite{jiang2021emerging}, making them less accessible to non-specialists. Simplifying the deployment process is essential to enable broader adoption across various fields \cite{xu2023xair}.

\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{figures/Challenge.pdf}
    \caption{Key challenges in motion recognition: (1) Dataset variability across different sensor types increases preprocessing complexity, (2) Model adaptation requires dataset-specific optimizations, leading to high computational costs, (3) Hyperparameter tuning demands expertise and computational resources, and (4) Deployment barriers limit accessibility for non-specialists. These challenges highlight the need for a scalable and automated framework.}
    \label{fig:challenges}
\end{figure}

% To address these challenges, this paper presents AutoMR, a unified framework that automates the entire motion recognition pipeline, encompassing data preprocessing, model training, and hyperparameter tuning. AutoMR standardizes diverse datasets, eliminating the need for dataset-specific preprocessing while optimizing performance through automated tuning. By supporting multiple datasets and modalities, AutoMR ensures scalability and efficient deployment.

% The key contributions of this work are as follows:
% \begin{itemize}
%     \item We propose AutoMR, an end-to-end AutoMR framework specifically designed for gesture and motion recognition, integrating preprocessing, model training, and hyperparameter tuning into a unified workflow.
%     \item We demonstrate the adaptability of AutoMR across ten benchmark datasets, covering different sensor modalities, and compare its performance with state-of-the-art models.
%     \item We evaluate AutoMR’s effectiveness in reducing manual intervention and computational costs while achieving competitive accuracy across diverse datasets.
% \end{itemize}

% By automating key components of gesture recognition, AutoMR simplifies the development process and improves accessibility for researchers and practitioners, making it a scalable and practical solution for real-world applications.
To address these challenges, this paper introduces \textit{AutoMR}, a unified framework that automates the motion recognition pipeline, encompassing data preprocessing, model training, and hyperparameter tuning. AutoMR standardizes datasets, eliminates the need for manual preprocessing, and enhances performance through automated tuning. Its compatibility with multiple datasets and sensor modalities ensures scalability and facilitates deployment. Evaluated on ten benchmark datasets, AutoMR demonstrates competitive performance against state-of-the-art models. By reducing manual effort and computational costs, the framework streamlines model development and improves accessibility, making motion recognition more efficient for researchers and practitioners.
To fully assess the effectiveness and reproducibility of our method, We encourage readers to review the provided code: \url{https://github.com/X-Intelligence-Labs/AutoMR}. This code demonstrates our state-of-the-art (SOTA) results achieved on eight datasets and showcases the fully automatic and adaptive nature of our model without any parameter tuning. More details are available on the website: \url{https://x-intelligence-labs.github.io/AutoMR_website/}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{RELATED WORK}

Existing research on gesture and motion data analysis spans task-specific models~\cite{liang2024mask}, AutoMR frameworks~\cite{damdoo2020adaptive}, and hyperparameter tuning tools~\cite{nayak2021hyper}. While each of these areas has made significant contributions, they also present certain limitations when applied to multimodal datasets. 

\subsection{Task-Specific Models}
% Task-specific models have been widely adopted for gesture and motion data recognition tasks. For instance, DeepConvLSTM integrates convolutional layers with LSTMs to capture spatiotemporal patterns, achieving strong results on datasets such as UCI-HAR~\cite{khatun2022deep}. 
Task-specific models are widely used in gesture and motion recognition. For instance, DeepConvLSTM combines convolutional layers and LSTMs to capture spatiotemporal patterns, showing strong performance on UCI-HAR~\cite{khatun2022deep}.
Similarly, Temporal Convolutional Networks (TCNs) use dilated convolutions to model long-range dependencies in time-series data~\cite{airlangga2024performance}, demonstrating success on OPPORTUNITY~\cite{al2024tcn} and other multimodal datasets. However, these methods are typically tailored to specific modalities (e.g., IMU or skeletal motion) and often lack scalability to new data types such as sEMG signals or unsegmented motion capture data.
% Similarly, Temporal Convolutional Networks (TCN) utilize dilated convolutions to model long-range dependencies in time-series data~\cite{airlangga2024performance} and have shown success in OPPORTUNITY~\cite{al2024tcn} and other multimodal datasets. However, these methods are typically tailored to specific datasets or modalities, such as inertial measurement unit (IMU) data or skeletal motion data, and often lack scalability to new datasets or modalities like sEMG signals or unsegmented motion capture data.

\subsection{Auto Training Frameworks for Time-Series Data}
Auto training frameworks, such as Auto-sklearn and H2O AutoGR~\cite{9534091}, offer automation in model selection and hyperparameter tuning. These tools reduce the expertise required to build machine learning pipelines and have demonstrated potential in time-series classification.
However, these frameworks are not tailored for multimodal datasets and often require extensive customization for preprocessing and handling heterogeneous sensor data. For example, TPOT’s limited modality-specific pipelines restrict its applicability to datasets like LMDHG, where skeletal motion data demands complex preprocessing and feature extraction~\cite{gijsbers2018layered}.
% However, most frameworks are not tailored for multimodal datasets, requiring substantial customization for preprocessing and handling heterogeneous sensor data. 
% For example, while TPOT has been used for time-series applications, its limited support for modality-specific pipelines restricts its applicability to datasets like LMDHG, where skeletal motion data requires complex preprocessing and feature extraction~\cite{gijsbers2018layered}.

\subsection{Hyperparameter Tuning Tools}
% Frameworks such as Optuna and Hyperopt~\cite{shekhar2021comparative} leverage Bayesian optimization to efficiently explore hyperparameter spaces and have been applied on datasets like MHEALTH~\cite{talaat2022effective} and OPPORTUNITY~\cite{ozcan2020human} for tuning parameters such as learning rates and network depths. 

Frameworks such as Optuna and Hyperopt are commonly used for optimizing hyperparameters~\cite{shekhar2021comparative}. These tools use algorithms like Bayesian optimization to efficiently explore the search space, and they have been applied to datasets like MHEALTH~\cite{talaat2022effective} and OPPORTUNITY~\cite{ozcan2020human} for tuning hyperparameters such as learning rates and network depths. 
However, they are not integrated into end-to-end workflows and often require significant manual setup for data preprocessing, model training, and evaluation.
% However, these tools lack integration into end-to-end workflows, often requiring significant manual intervention to set up pipelines for data preprocessing, model training, and evaluation.

% Our work addresses these limitations by introducing AutoGR, an end-to-end AutoMR framework designed specifically for gesture and motion data recognition. AutoGR integrates preprocessing, model selection, and hyperparameter optimization, supporting diverse modalities such as sEMG, skeletal motion, and IMU data. Unlike task-specific models, AutoGR is adaptable across datasets without requiring significant manual adjustments. It also incorporates automated hyperparameter tuning, fully integrated into the pipeline, to simplify deployment and reduce the technical barriers for users.

\addtolength{\textheight}{-3cm}   % This command serves to balance the column lengths
                                  % on the last page of the document manually. It shortens
                                  % the textheight of the last page by a suitable amount.
                                  % This command does not take effect until the next page
                                  % so it should come on the page before the last. Make
                                  % sure that you do not shorten the textheight too much.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{METHOD}
% \section{DESCRIPTION AND PROCESSING}
% \subsection{Model Factory}

% The model factory within AutoGR enables dynamic selection, configuration, and instantiation of models tailored for gesture recognition. QuartzNet serves as the primary model due to its strong performance on sequential data\cite{shen2024towards}.

% QuartzNet is a lightweight convolutional model specifically optimized for temporal data, such as gesture sequences. It employs depthwise separable convolutions to reduce parameters and computational costs while retaining the ability to capture hierarchical features. Residual connections enhance gradient flow, enabling stable training for deeper architectures. The modular design of QuartzNet includes blocks with varying kernel sizes and dilation rates, allowing the model to effectively capture multi-scale temporal patterns\cite{kriman2020quartznet}.
% The QuartzNet implementation incorporates the following key components:
% \subsubsection{Initial Convolution Block} A strided convolution layer with a large kernel size down-samples the input sequence, reducing temporal resolution while preserving critical features.
% \subsubsection {QuartzNet blocks}These modular blocks utilize depthwise separable convolutions, residual connections, and varying kernel sizes (33, 39, 51, 63, 75) to capture multi-scale temporal dependencies\cite{nikam2022robust}.
% \subsubsection{Dropout and Global Average Pooling}Dropout layers prevent overfitting, while global average pooling aggregates features across the sequence.
% \subsubsection{Fully Connected Output Layer}Extracted features are mapped to the desired number of output classes via a dense layer.

% For flexibility, the model factory also supports QuartzNetLarge. QuartzNetLarge allows larger configurations with up to 7 blocks and 7 cells per block, input channels ranging from 128 to 512, and head channels ranging from 256 to 1024. This configurability ensures adaptability to datasets with varying complexity, although the standard QuartzNet configuration is typically sufficient for most gesture recognition tasks. Users can choose the dynamic models, ensuring a streamlined process for creating models that fit the task at hand.

% \subsection{Training Module}

% The training module provides an efficient and scalable framework for optimizing QuartzNet and evaluating its performance. Built on TorchTNT, it offers modular workflows for managing metrics, training logic, and visualization, ensuring scalability across diverse datasets. Key features include automated tracking of metrics like accuracy, F1 score, and confusion matrices, along with advanced training techniques such as gradient clipping\cite{Zhang2019WhyGC}, anomaly detection, and dynamic learning rate scheduling. Real-time logging via TensorBoard further enhances performance monitoring and debugging.

% \subsubsection{Core Design of the TrainUnit Class}
% The TrainUnit class is the center of training module, which extends the AutoUnit framework to streamline the training and evaluation pipeline. It supports multi-class classification tasks using loss functions such as cross-entropy and tracks metrics like MulticlassAccuracy and MulticlassConfusionMatrix during training and evaluation. The framework integrates an Adam optimizer with an Exponential Learning Rate Scheduler to improve convergence and generalization. After each evaluation phase, it generates and saves confusion matrix, classification reports, and the best-performing model parameters, ensuring reproducibility and insightful analysis.

% \subsubsection{Robust Training Features}
% To ensure stability and efficiency, the module incorporates advanced features:Gradient Clipping mitigates exploding gradients during backpropagation; Anomaly Detection identifies numerical instabilities such as NaNs or infinities; Checkpointing periodically saves model states and metrics to prevent data loss and ensure consistency.

% \subsubsection{Evaluation and Reporting}
% The module generates detailed evaluation reports, including precision, recall, and F1 scores for each gesture class. Confusion matrices are plotted and normalized to provide a clear visual representation of classification performance. The best model parameters are automatically saved, ensuring robust and reproducible experimentation.

% \subsection{Hyperparameter Tuning}
% The hyperparameter tuning module automates the process of identifying the optimal hyperparameters for a given dataset and model. It utilizes SMAC (Sequential Model-based Algorithm Configuration)\cite{10.1145/3377929.3389999}, a cutting-edge optimization framework, to systematically explore the hyperparameter space and determine configurations that maximize model performance. The module includes four key features:Dynamic Configuration Space, Support for QuartzNet Architectures, Seamless Dataset Integration, and Automated Configuration Management.


% \subsubsection{Configuration Space}
% The hyperparameter search space is dynamically defined to accommodate variations in dataset size and model architecture, enabling an efficient and customized optimization process for each use case\cite{feurer2019hyperparameter}. 
% General Hyperparameters include: Learning Rate defined between 0.0001 and 0.001 for smaller datasets and up to 0.01 for larger datasets; Weight Decay ranges from 0.000001 to 0.001; Dropout Rate Spans values from 0.1 to 0.5; Batch Size ranges between 16 and 64 for smaller datasets and up to 256 for larger datasets.

% QuartzNet-Specific Hyperparameters include: Number of Blocks are configurable between 3 and 7;	Number of Cells per Block ranges from 3 to 7; Input Channels vary between 128 and 512; Head Channels ranges from 256 to 1024; Kernel Sizes includes adjustable sizes for input and head layers, such as 21–45 for input kernels.

% The configuration space is built using the ConfigSpace library, which dynamically adjusts the hyperparameter ranges based on dataset and model characteristics, balancing efficiency and optimization performance

% \subsubsection{Optimization Procedure}
% The hyperparameter tuning process involves several steps to efficiently explore the configuration space and identify optimal parameters for each dataset and model:

% \textbf{Initialization:} The process begins with the `ModelHyperOptimizer` class, which sets up the configuration space by defining hyperparameter ranges, such as learning rate, batch size, dropout rate, and model-specific parameters like kernel sizes or the number of blocks. The datasets are loaded, and the model is initialized with default settings to prepare for tuning.

% \textbf{Training:} For each sampled hyperparameter configuration, the model is trained on the training set. The number of training epochs is predefined (default: 50) to balance computational time and optimization accuracy. During training, standard procedures like backpropagation, gradient descent, and learning rate scheduling are applied to assess how the chosen hyperparameters affect the model's convergence\cite{feurer2019hyperparameter}.

% \textbf{Evaluation:} Once training is complete, the model is tested on the test set to evaluate its performance. The evaluation metric, typically accuracy, is computed to measure the model's ability to generalize to unseen data. Additional metrics, such as F1 score or loss, may also be recorded depending on the specific requirements of the task.

% \textbf{Optimization:} The SMAC framework iteratively adjusts the search process based on previous results. It samples new hyperparameter configurations, focusing on those that show promise for improving accuracy or reducing computational costs. This process involves probabilistic modeling to identify areas of the search space likely to yield better configurations.

% \textbf{Configuration Management:} The best-performing hyperparameter configurations are automatically saved to ensure they can be reused in future experiments. If a previously optimized configuration exists, it is loaded to avoid redundant computations unless re-optimization is explicitly requested. This allows researchers to quickly apply optimal settings without repeating the tuning process.
% \begin{figure}
%     \centering
%     \includegraphics[width=1\linewidth]{figures/process.pdf}
%     \caption{END-TO-END AutoMR Architecture}
%     \label{figure}
% \end{figure}

% \subsection{Architecture Overview}

% The proposed AutoGR framework provides a unified solution for gesture recognition, addressing challenges such as dataset diversity, model scalability, and hyperparameter optimization (Figure 2). It seamlessly integrates data preprocessing, model training, and hyperparameter tuning into a cohesive pipeline. Designed for scalability and adaptability, AutoGR enables the efficient deployment of gesture recognition models across multimodal datasets.

% \subsection{Data Processing}

% The data processing module is designed to standardize and streamline the preparation of gesture datasets with diverse modalities. A base dataset structure defines key properties such as data modality, gesture labels, and sampling parameters. This modular structure supports ten derivative datasets, including Shrec2021, MHEALTH, UCI-HAR, DB4, Berkeley-MHAD, LMDHG, and the four subsets of the OPPORTUNITY Activity Recognition dataset.



% \subsection{Model Factory}
% The AutoGR model factory enables dynamic selection, configuration, and instantiation of models for gesture recognition. QuartzNet is the primary model due to its efficiency in sequential data processing \cite{shen2024towards}. 

% QuartzNet is a lightweight convolutional model optimized for temporal data using depthwise separable convolutions, reducing parameters while preserving feature extraction capabilities. Residual connections improve gradient flow, supporting deeper architectures. The modular structure incorporates blocks with varying kernel sizes and dilation rates, capturing multi-scale temporal patterns \cite{kriman2020quartznet}.

% \textbf{Key Components:}
% \begin{itemize}
%     \item \textbf{Initial Convolution Block:} A strided convolution layer down-samples the input while retaining essential features.
%     \item \textbf{QuartzNet Blocks:} Depthwise separable convolutions, residual connections, and varying kernel sizes (33, 39, 51, 63, 75) capture multi-scale dependencies \cite{nikam2022robust}.
%     \item \textbf{Dropout \& Global Average Pooling:} Dropout prevents overfitting; global pooling aggregates features across sequences.
%     \item \textbf{Fully Connected Layer:} Maps extracted features to output classes.
% \end{itemize}

% For flexibility, QuartzNetLarge extends configurability with up to 7 blocks and 7 cells per block, input channels from 128 to 512, and head channels from 256 to 1024, accommodating datasets of varying complexity.

% \subsection{Training Module}
% Built on TorchTNT, the training module provides a scalable framework for optimizing QuartzNet. It automates metric tracking (accuracy, F1 score, confusion matrix) and integrates advanced techniques like gradient clipping \cite{Zhang2019WhyGC}, anomaly detection, and dynamic learning rate scheduling. Real-time logging via TensorBoard enhances monitoring and debugging.

% \textbf{TrainUnit Core Design:}
% \begin{itemize}
%     \item Extends AutoUnit for streamlined training and evaluation.
%     \item Supports multi-class classification with cross-entropy loss.
%     \item Tracks metrics like MulticlassAccuracy and MulticlassConfusionMatrix.
%     \item Uses Adam optimizer with an Exponential LR Scheduler for convergence.
%     \item Saves best-performing model parameters for reproducibility.
% \end{itemize}

% \textbf{Robust Training Features:}
% \begin{itemize}
%     \item \textbf{Gradient Clipping:} Prevents exploding gradients.
%     \item \textbf{Anomaly Detection:} Identifies numerical instabilities (NaNs, infinities).
%     \item \textbf{Checkpointing:} Periodic model state saving prevents data loss.
% \end{itemize}

% \textbf{Evaluation and Reporting:}  
% Generates detailed reports, including accuracy, precision, recall, and F1 scores per gesture class. Normalized confusion matrices provide a visual assessment of classification performance.

% \subsection{Hyperparameter Tuning}
% The hyperparameter tuning module automates optimal parameter selection using SMAC (Sequential Model-based Algorithm Configuration) \cite{10.1145/3377929.3389999}. It dynamically explores the search space to maximize model performance. For the ablation study, manual hyperparameter tuning is also employed, the batch size is set as 32, 64, 128, or 256.

% \subsubsection{Configuration Space}
% The hyperparameter space adapts to dataset and model variations for efficient optimization \cite{feurer2019hyperparameter}.  
% \textbf{General Hyperparameters:}
% \begin{itemize}
%     \item Learning Rate: 0.0001–0.001 (small datasets), up to 0.01 (large datasets).
%     \item Weight Decay: 0.000001–0.001.
%     \item Dropout Rate: 0.1–0.5.
%     \item Batch Size: 16–64 (small datasets), up to 256 (large datasets).
% \end{itemize}

% \textbf{QuartzNet-Specific Parameters:}
% \begin{itemize}
%     \item Number of Blocks: 3–7.
%     \item Cells per Block: 3–7.
%     \item Input Channels: 128–512.
%     \item Head Channels: 256–1024.
%     \item Kernel Sizes: 21–45 for input layers.
% \end{itemize}

% The ConfigSpace library adjusts hyperparameter ranges dynamically, ensuring balance between efficiency and optimization.

% \subsubsection{Optimization Procedure}
% \textbf{Initialization:} The ModelHyperOptimizer sets hyperparameter ranges and initializes models for tuning.  
% \textbf{Training:} Each sampled configuration is trained for a predefined number of epochs (default: 50), applying standard optimization techniques \cite{feurer2019hyperparameter}.  
% \textbf{Evaluation:} Performance is assessed on the test set using accuracy, F1 score, and loss.  
% \textbf{Optimization:} SMAC iteratively refines the search, prioritizing configurations that improve accuracy or reduce computational cost.  
% \textbf{Configuration Management:} The best configurations are stored for reuse, avoiding redundant computations unless re-optimization is required.

\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{figures/process.pdf}
    \caption{AutoMR end-to-end architecture, illustrating the complete workflow from data preprocessing to model training and hyperparameter tuning. The framework standardizes diverse datasets, selects optimal model configurations, and ensures efficient training and deployment for motion recognition across different sensor modalities.}
    \label{figure}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{figures/dependencies.pdf}
    \caption{AutoMR's modular and hierarchical architecture, illustrating the interaction between core modules and dataset-specific training scripts. The upper layer consists of fundamental components for dataset preprocessing, augmentation, model selection, training, and hyperparameter tuning, ensuring a standardized and optimized workflow. The lower layer contains dataset-specific execution scripts that utilize these core modules for model training on individual datasets, enabling efficient adaptation across diverse sensor modalities.}
    \label{dependencies}
\end{figure}

\subsection{Architecture Overview}

% AutoMR provides a unified solution for motion recognition, addressing challenges such as dataset diversity, model scalability, and hyperparameter optimization, as illustrated in Figure~\ref{figure}. The framework integrates data preprocessing, model training, and hyperparameter tuning into a streamlined pipeline, enabling efficient deployment across multimodal datasets while reducing the need for dataset-specific modifications. 

% As illustrated in Figure~\ref{dependencies}, AutoMR adopts a modular and hierarchical architecture for efficient and scalable motion recognition. The system consists of a core infrastructure that handles fundamental processes and dataset-specific training scripts that leverage these components.

% The upper layer includes modules for dataset preprocessing, augmentation, training management, model selection, and hyperparameter tuning. The dataset module standardizes multimodal input, while the augmentation module enhances data variability. The training module integrates configurations from the model factory module and optimizes performance through the AutoML hyperparameter module.

% The lower layer comprises dataset-specific training scripts such as main MHEALTH, main UCI-HAR, and main Berkeley-MHAD, which interact with the core modules to preprocess data, configure models, and execute training. This structured design minimizes manual adaptation, ensuring AutoMR remains highly scalable and adaptable.

AutoMR provides a unified solution for motion recognition, addressing challenges such as dataset diversity, model scalability, and hyperparameter optimization, as illustrated in Figure~\ref{figure}.
% By integrating data preprocessing, model training, and hyperparameter tuning into a streamlined pipeline, the framework enables efficient deployment across multimodal datasets while reducing the need for dataset-specific modifications.
The core layer includes modules responsible for dataset preprocessing, augmentation, training management, model selection, and hyperparameter tuning (Figure~\ref{dependencies}). The dataset module standardizes multimodal input formats, ensuring consistency across different datasets, while the augmentation module introduces transformations to improve generalization. The training module orchestrates model learning, incorporating configurations from the model factory module and optimizing performance through the AutoML hyperparameter module.

The dataset-specific layer consists of training scripts such as main MHEALTH, main UCI-HAR, and so on, which interact with the core modules to preprocess data, configure models, and execute training. This structured approach minimizes manual adaptation while ensuring scalability, allowing AutoMR to generalize across diverse datasets and facilitate efficient motion recognition.

\subsection{Data Processing and Model Factory}

\begin{table*}[ht]
    \centering
    \caption{description and processing}
    \label{tab:datasets}
    \begin{tabular}{l c c p{9cm}}
        \hline
        \textbf{Dataset} & \textbf{Sequences} & \textbf{Batch Size} & \textbf{Processing and Description} \\
        \hline
        SHREC2021 & 180 & 128 & Contains 3–5 gestures per sequence with 18 gesture classes (static and dynamic). Split into 108 training and 16 test examples per class \cite{10.1016/j.cag.2021.07.007}. \\
        MHEALTH & 10 participants & 256 & Records body motion and vital signs via chest, wrist, and ankle sensors. Segmented into 25-timestamp windows with 50\% overlap \cite{banos2015framework, banos2014mHealthDroid}. \\
        UCI-HAR & 30 participants & 32 & Smartphone-based accelerometer and gyroscope signals. Preprocessed and segmented into 2.56s sliding windows with 50\% overlap, generating 128 readings per window \cite{8975649, pang2021stacked}. \\
        DB4 & 10 participants & 256 & sEMG and kinematic data for 52 hand gestures plus rest. Segmented into 260ms windows with 235ms overlap. Data split in a 4:1 ratio and stored as .txt files \cite{josephs2020semg, 8630679}. \\
        Berkeley-MHAD & 12 participants & 256 & Includes 11 dynamic actions repeated 5 times, totaling 660 sequences. Includes background recordings and T-poses for skeleton extraction \cite{6474999}. \\
        LMDHG & Unsegmented & 64 & Hand gesture sequences annotated with action labels. Skeleton data projected to 2D for visualization. Training/testing split is 4:1 \cite{boulahia2016hif3d, boulahia2017dynamic}. \\
        OPPORTUNITY & Wearable sensors & 256 & Captures human activities using wearable, object, and ambient sensors. Data segmented into 15-frame windows with 7-frame overlap. Augmented to address class imbalance \cite{chavarriaga2013opportunity}. \\
        \hline
    \end{tabular}
\end{table*}

To ensure dataset consistency, we define structured formats that include modality specifications, gesture labels, and sampling parameters. This modular approach supports ten datasets—Shrec2021, MHEALTH, UCI-HAR, DB4, Berkeley-MHAD, LMDHG, and four subsets of the OPPORTUNITY dataset (see TABLE~\ref{tab:datasets})—and aligns diverse data modalities for compatibility with subsequent model training.

% To ensure consistency across datasets, AutoMR defines several structured formats that include modality specifications, gesture labels, and sampling parameters. This modular approach supports ten datasets, including Shrec2021, MHEALTH, UCI-HAR, DB4, Berkeley-MHAD, LMDHG, and four subsets of the OPPORTUNITY dataset (shown in TABLE~\ref{tab:datasets}). The preprocessing pipeline aligns different data modalities, ensuring compatibility with the subsequent model training steps.

% \subsection{Model Factory}

In addition, our model factory dynamically selects, configures, and instantiates models for gesture recognition. We choose QuartzNet as our primary model due to its efficient processing of sequential data~\cite{shen2024towards}. QuartzNet employs depthwise separable convolutions to reduce parameters while preserving feature extraction, and its residual connections improve gradient flow for deeper architectures. Varying kernel sizes and dilation rates enable multi-scale temporal pattern recognition~\cite{kriman2020quartznet}. To accommodate datasets of varying complexity, we also employ QuartzNetLarge, which allows adjustments in block structure, input channels, and head channels~\cite{nikam2022robust}.

% AutoMR dynamically selects, configures, and instantiates models for gesture recognition. QuartzNet is chosen as the primary model due to its efficiency in processing sequential data \cite{shen2024towards}. It employs depthwise separable convolutions to reduce parameters while preserving feature extraction capabilities. Residual connections improve gradient flow, supporting deeper architectures, while varying kernel sizes and dilation rates enable multi-scale temporal pattern recognition \cite{kriman2020quartznet}. To accommodate datasets of varying complexity, we employ QuartzNetLarge, which allows adjustments in block structure, input channels, and head channels~\cite{nikam2022robust}.
% QuartzNetLarge extends configurability, allowing modifications in block structure, input channels, and head channels \cite{nikam2022robust}.

\subsection{Training Module}

The training module, built on TorchTNT, optimizes QuartzNet by automating metric tracking and incorporating techniques such as gradient clipping, anomaly detection, and dynamic learning rate scheduling. It supports multi-class classification using cross-entropy loss and evaluates performance using metrics like accuracy, F1 score, and confusion matrices. 
Our module includes checkpointing mechanisms to prevent data loss and ensure reproducibility by saving the best-performing model parameters, and we use TensorBoard for real-time logging to facilitate monitoring and debugging.
% The module also includes checkpointing mechanisms to prevent data loss and ensures reproducibility by storing the best-performing model parameters. Real-time logging via TensorBoard facilitates monitoring and debugging.

\subsection{Hyperparameter Tuning}

AutoMR automates hyperparameter tuning using SMAC~\cite{10.1145/3377929.3389999} to dynamically optimize performance. The configuration space adapts to each dataset by tuning general parameters (learning rates, weight decay, dropout rates, batch sizes) and QuartzNet-specific settings (number of blocks, cells per block, input channels, kernel sizes), with the ConfigSpace library managing these ranges to balance cost and efficiency.

% AutoMR automates hyperparameter tuning using SMAC (Sequential Model-based Algorithm Configuration) \cite{10.1145/3377929.3389999}, dynamically exploring the search space to optimize performance. The configuration space adapts to dataset characteristics, adjusting learning rates, weight decay, dropout rates, and batch sizes. QuartzNet-specific parameters, such as the number of blocks, cells per block, input channels, and kernel sizes, are also fine-tuned to enhance efficiency. The ConfigSpace library manages hyperparameter ranges, balancing computational cost and performance optimization.

The process begins with the ModelHyperOptimizer defining parameter ranges and initializing models. We then train for a fixed number of epochs using standard techniques, evaluating performance via accuracy, F1 score, and loss. SMAC iteratively refines the search, storing the best configurations for reuse unless re-optimization is required. For ablation studies, we also perform manual tuning with batch sizes of 32, 64, 128, or 256 to assess their impact on performance.

% The optimization process begins with the ModelHyperOptimizer defining parameter ranges and initializing models. Training is conducted for a predefined number of epochs, applying standard optimization techniques. Performance evaluation is based on accuracy, F1 score, and loss, while SMAC iteratively refines the search to prioritize configurations that improve results. To avoid redundant computations, the best configurations are stored and reused unless re-optimization is explicitly required. For ablation studies, manual tuning is performed with batch sizes of 32, 64, 128, or 256 to analyze its impact on performance.

   % \begin{figure}[thpb]
   %    \centering
   %    %\includegraphics[scale=1.0]{figurefile}
   %    \caption{Inductance of oscillation winding on amorphous
   %     magnetic core versus DC bias magnetic field}
   %    \label{figurelabel}
   % \end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


% \subsubsection{SHREC2021}
% This dataset consists of 180 sequences of gestures, where each sequence contains 3 to 5 gestures interspersed with non-gesture movements. The 18 gesture classes are divided into static gestures, defined by hand poses, and dynamic gestures, defined by hand and joint trajectories\cite{10.1016/j.cag.2021.07.007}. The dataset is split into a training set of 108 sequences with 24 examples per gesture and a test set with 16 examples per class. The manual tuning batch size is set as 128.
% \subsubsection{MHEALTH}
% This dataset includes recordings of body motion and vital signs from 10 volunteers performing various physical activities. Sensors placed on the chest, wrist, and ankle record acceleration, angular velocity, and magnetic orientation\cite{banos2015framework}. The data is segmented into windows of 25 timestamps with 50\% overlap, resulting in 25×21 features per window, which are labeled based on the activity performed\cite{banos2014mHealthDroid}. The manual tuning batch size is set as 256.
% \subsubsection{UCI-HAR}
% This dataset is collected from 30 individuals performing daily activities while wearing smartphones on their waists, this dataset includes accelerometer and gyroscope signals\cite{8975649}. The data is preprocessed with noise filtering and segmented into sliding windows of 2.56 seconds with 50\% overlap, yielding 128 readings per window, each labeled according to the activity type\cite{pang2021stacked}. The manual tuning batch size is set as 32.
% \subsubsection{DB4}
% This dataset features sEMG and kinematic data from 10 participants performing 52 hand gestures and a rest position\cite{josephs2020semg}. The data is segmented into 260-millisecond windows\cite{8630679} with a 235-millisecond overlap\cite{smith2011methodology}\cite{PhysRevLett.71.3279}. Labels are assigned using a majority voting mechanism applied to refined gesture labels and mapped to a unified label space for exercises A, B, and C. Rest windows are labeled as 0. The labeled data is stratified and split into training and testing sets in a 4:1 ratio. Preprocessed data, including features and labels, is saved in .txt files for subsequent analysis and training. The manual tuning batch size is set as 256.
% \subsubsection{Berkeley-MHAD}
% This dataset contains 11 actions performed by 12 subjects (7 male, 5 female) aged 23–30, plus one elderly participant. Each subject performs 5 repetitions per action, resulting in 660 sequences. Actions involve dynamic upper and lower body movements, such as jumping, throwing, clapping, and sitting. Subjects were given minimal instructions to allow variations in performance style and speed. The dataset also includes background recordings and T-poses for skeleton extraction\cite{6474999}. The manual tuning batch size is set as 256.
% \subsubsection{LMDHG}
% This dataset includes unsegmented hand gesture sequences performed with one or both hands. Skeleton data is extracted and annotated with action labels specifying frame ranges. Each sequence is segmented into individual gestures, represented as 3D joint coordinates of 46 joints\cite{boulahia2016hif3d}. Labels are translated into English, and each gesture is projected onto the XZ-plane to generate 2D visualizations with temporal coloring\cite{boulahia2017dynamic}. The processed data is split into training and testing sets as 4:1. The manual tuning batch size is set as 64.
% \subsubsection{OPPORTUNITY}
% Designed for human activity recognition, this dataset uses wearable, object, and ambient sensors to record human activities. Data is filtered to retain relevant features and locomotion labels while removing invalid rows. Sliding windows of 15 frames with 7-frame overlap are created, and labels are assigned via voting. To address class imbalance, data augmentation techniques such as translation, scaling, and noise addition are applied to underrepresented classes, while overrepresented classes are down sampled\cite{opportunity_activity_recognition_226}. The processed data is organized into .npy and .txt files for training and testing, ensuring balance and consistency for human activity recognition tasks. The manual tuning batch size is set as 256. We divide this dataset into 4 datasets according to the subjects.

% \subsection{Evaluation metrics}
% To comprehensively evaluate the performance of the AutoMR framework across multiple datasets for gesture recognition, we employ several evaluation metrics, including Accuracy, Precision, Recall, F1 Score, and Confusion Matrix. These metrics provide a detailed assessment of the classification performance for each model on the diverse datasets. Given the imbalance in the class distributions across certain datasets, we calculate the weighted average values of these metrics to ensure a balanced evaluation.

% \subsubsection{F1 score}
% The F1 Score evaluates the balance between precision and recall and is particularly important for imbalanced datasets. It provides a harmonic mean of precision and recall, capturing both false positives and false negatives in the evaluation. The F1 Score is defined as:

% $F1 = 2 \times \frac{\textit{Precision} \times \textit{Recall}}{\textit{Precision} + \textit{Recall}}$

% \subsubsection{precision}
% Precision quantifies the ability of the classifier to correctly identify positive instances while minimizing the misclassification of negative instances as positive. It is defined as:

% $\text{Precision} = \frac{\textit{TP}}{\textit{TP} + \textit{FP}}$

% where TP is the number of true positive predictions, and FP is the number of false positive predictions.
% \subsubsection{Recall}
% Recall measures the ability of the classifier to identify all positive instances. It is particularly crucial in datasets where missing a positive instance has significant implications. Recall is defined as:

% $\textit{Recall} = \frac{\textit{TP}}{\textit{TP} + \textit{FN}}$

% \subsubsection{Accuracy}
% Accuracy measures the overall correctness of the predictions by evaluating the proportion of correctly classified instances out of the total instances. It is defined as:

% $\textit{Accuracy} = \frac{\textit{TP} + \textit{TN}}{\textit{TP} + \textit{TN} + \textit{FP} + \textit{FN}}$

% \subsubsection{Confusion Matrix}
% The confusion matrix offers a comprehensive view of the classification results, detailing the counts of true positives (TP), true negatives (TN), false positives (FP), and false negatives (FN) for each class. It enables the identification of specific classes where the model performs well or struggles, guiding further optimization efforts.

% \subsection{Evaluation Metrics}
% To assess the performance of the AutoMR framework on gesture recognition across multiple datasets, we use Accuracy, Precision, Recall, F1 Score, and the Confusion Matrix. Given class imbalances in some datasets, weighted averages of these metrics are calculated for fair evaluation.

% \subsubsection{F1 Score}
% The F1 Score balances precision and recall, making it crucial for imbalanced datasets. It is the harmonic mean of precision and recall:

% \[
% F1 = 2 \times \frac{\textit{Precision} \times \textit{Recall}}{\textit{Precision} + \textit{Recall}}
% \]

% \subsubsection{Precision}
% Precision measures the proportion of correctly identified positive instances:

% \[
% \text{Precision} = \frac{\textit{TP}}{\textit{TP} + \textit{FP}}
% \]

% \subsubsection{Recall}
% Recall quantifies the classifier’s ability to identify all positive instances:

% \[
% \textit{Recall} = \frac{\textit{TP}}{\textit{TP} + \textit{FN}}
% \]

% \subsubsection{Accuracy}
% Accuracy evaluates the overall classification performance:

% \[
% \textit{Accuracy} = \frac{\textit{TP} + \textit{TN}}{\textit{TP} + \textit{TN} + \textit{FP} + \textit{FN}}
% \]


\section{EXPERIMENTAL RESULTS ANALYSIS}        % RESULTS

\subsection{Comparison with State-of-the-Art Methods}

Table~\ref{table_performance} and Figure~\ref{figure3} compare the performance of AutoMR with existing state-of-the-art (SOTA) methods across ten benchmark datasets. AutoMR achieves the highest accuracy on eight of the ten datasets, demonstrating its generalizability and adaptability across different sensor modalities and gesture recognition tasks. Notably, AutoMR significantly outperforms previous models on datasets such as OPPORTUNITY, where it achieves over 5\% higher accuracy compared to prior SOTA models. However, performance on DB4 and LMDHG is slightly lower than SOTA, indicating potential areas for future improvement, particularly in handling highly noisy or unstructured motion data.

\begin{table}[t]
    \centering
    \caption{Comparison of AutoMR performance with state-of-the-art (SOTA) models on ten datasets. AutoMR outperforms SOTA methods on eight datasets, demonstrating its robustness and adaptability across different sensor modalities.}
    \label{table_performance}
    \begin{tabular}{ccc}
        \toprule
        \textbf{Dataset} & \textbf{AutoMR Accuracy (\%)} & \textbf{SOTA Accuracy (\%)} \\
        \midrule
        Shrec2021~\cite{10.1016/j.cag.2021.07.007}        & 91.48                  & 89.93 \\
        MHEALTH~\cite{praba2023harnet}          & 99.81                  & 99.80 \\
        UCI-HAR~\cite{8975649}           & 97.05                  & 95.25\\
        DB4~\cite{josephs2020semg}              & 66.86                  & 73.00 \\
        Berkeley-MHAD~\cite{10719991}    & 98.98                  & 97.91 \\
        LMDHG~\cite{boulahia2017dynamic}            & 83.38                  & 91.83 \\
        Oppo-s1~\cite{chavarriaga2013opportunity}          & 94.63                  & 85.00 \\
        Oppo-s2~\cite{chavarriaga2013opportunity}          & 92.59                  & 86.00 \\
        Oppo-s3~\cite{chavarriaga2013opportunity}          & 91.16                  & 83.00 \\
        Oppo-s4~\cite{chavarriaga2013opportunity}          & 90.89                  & 77.00 \\
        \bottomrule
    \end{tabular}
\end{table}

\begin{figure}[tp]
    \centering
    \includegraphics[width=1\linewidth]{figures/overall_acc.pdf}
    \caption{Overall accuracy comparison between AutoMR and SOTA models across ten datasets. AutoMR achieves superior performance on eight datasets, highlighting its effectiveness in generalizing across diverse gesture recognition tasks.}
    \label{figure3}
\end{figure}

\subsection{Ablation Study}

\begin{table}[!tp]
    \centering
    \setlength{\tabcolsep}{3pt} % Reduce horizontal spacing
    \renewcommand{\arraystretch}{0.9} % Reduce vertical spacing
    \caption{Ablation study comparing automatic and manual hyperparameter tuning across four key metrics: accuracy, precision, recall, and F1-score. The results show that AutoMR’s automatic tuning achieves performance comparable to manual tuning, demonstrating its reliability for real-world applications.}
    \label{table_tuning_metrics}
    \begin{tabular}{lcccc}
        \toprule
        \textbf{Dataset} & \textbf{Accuracy} & \textbf{Precision} & \textbf{Recall} & \textbf{F1-Score} \\
        \midrule
        Shrec2021-manual & 91.48 & 92.57 & 91.48 & 91.50 \\
        Shrec2021-auto   & 91.48 & 91.64 & 91.48 & 91.38 \\
        MHEALTH-manual   & 99.81 & 99.81 & 99.81 & 99.81 \\
        MHEALTH-auto     & 99.81 & 99.81 & 99.81 & 99.80 \\
        UCI-HAR-manual   & 96.54 & 96.60 & 96.54 & 96.53 \\
        UCI-HAR-auto     & 97.05 & 97.08 & 97.05 & 97.04 \\
        DB4-manual       & 65.52 & 65.88 & 65.52 & 65.53 \\
        DB4-auto         & 66.86 & 67.15 & 66.86 & 66.76 \\
        Berkeley-manual  & 98.98 & 99.00 & 98.98 & 98.97 \\
        Berkeley-auto    & 98.98 & 99.00 & 98.98 & 98.97 \\
        LMDHG-manual     & 83.38 & 85.27 & 83.38 & 82.55 \\
        LMDHG-auto       & 74.25 & 77.13 & 74.25 & 72.92 \\
        \bottomrule
    \end{tabular}
\end{table}

\begin{figure}[tp]
    \centering
    \includegraphics[width=1\linewidth]{figures/22ablation.pdf}
    \caption{Ablation study comparing automatic and manual hyperparameter tuning across ten datasets. Results indicate that automatic tuning performs on par with or better than manual tuning in most cases, validating the efficiency and practicality of AutoMR’s optimization strategy.}
    \label{figure1}
\end{figure}

To evaluate the impact of automated hyperparameter tuning, an ablation study was conducted comparing automatic and manual tuning across multiple datasets. Table~\ref{table_tuning_metrics} and Figure~\ref{figure1} present a detailed comparison of model performance metrics, including accuracy, precision, recall, and F1-score. The results indicate that automated tuning achieves comparable or even slightly superior performance to manually tuned models in most datasets, suggesting that AutoMR can effectively optimize hyperparameters without requiring expert intervention. Notably, datasets such as DB4 and LMDHG show slight discrepancies, where manual tuning provides marginal improvements. This suggests that highly unstructured or sensor-specific datasets may still benefit from fine-tuned adjustments.






% \section{discussion, limitation and conclusion}

% \subsection{Discussion}
% The evaluation of QuartzNet within the AutoGR framework across 10 datasets highlights its strong performance and adaptability. AutoGR automates the entire workflow, including data preprocessing, augmentation, model construction, training, and hyperparameter tuning, enabling seamless application across diverse gesture recognition tasks.

% \textbf{Data Processing:} The data processing module effectively standardizes different dataset formats, sampling rates, and modalities. This ensures consistent and clean data input, reducing noise and allowing the model to focus on meaningful patterns. For example, preprocessing techniques, such as resampling and augmentation, effectively balanced class distributions in datasets like OPPORTUNITY, improving overall model performance.

% \textbf{Data Augmentation:} Augmentation techniques, including time warping and noise addition, enhance generalization by simulating real-world variations like sensor noise and sampling inconsistencies. For datasets like MHEALTH and SHREC2021, augmentation reduced overfitting and improved performance by exposing the model to diverse scenarios during training.

% \textbf{Model Construction and Training:} QuartzNet’s architecture, with depthwise separable convolutions and residual connections, efficiently captures hierarchical and temporal patterns while maintaining computational efficiency. Its adaptability to sequential gesture data ensures strong results across most datasets.

% \textbf{Hyperparameter Tuning:} AutoGR leverages the SMAC optimization framework to automatically determine optimal hyperparameters, such as learning rates and batch sizes. This eliminates the need for manual tuning, making AutoGR accessible to users with varying levels of expertise while ensuring reliable performance.


% \subsection{Limitation and Future Works}
% However, QuartzNet performs less effectively on DB4 and LMDHG datasets. DB4 includes sEMG signals that need detailed temporal and spatial analysis, while LMDHG uses 2D temporal projection images. QuartzNet’s 1D convolutional design does not fully capture the spatial relationships in these data types. To address this, lightweight 2D convolutional models like CNNs can be added to the model factory. These models would better capture the spatial correlations in 2D data, such as those in LMDHG. For DB4, hybrid models that combine temporal and spatial feature extraction could better process sEMG signals. Allowing AutoGR to dynamically select models based on the dataset type would make it more adaptable and effective for different tasks.

% Another improvement is to explore transformer-based architectures. Transformers are good at handling long-term dependencies and could complement QuartzNet for datasets with complex temporal relationships. This addition would increase AutoGR’s ability to handle a wider range of dataset characteristics.

% AutoGR can also include more datasets from fields like healthcare, robotics, and human-computer interaction. Adding data with new modalities, such as radar or camera-based data, would expand its usage. AutoGR will be made open-source to encourage collaboration and innovation . Researchers and developers could contribute by adding features, models, and datasets. This would continuously reduce barriers for users, allowing them to build high-performing models without advanced expertise. Open-source development would also ensure transparency and consistent improvements to the framework.

% \subsection{Conclusion}

% This paper presents AutoGR, an automated framework for gesture recognition across multiple datasets and modalities. AutoGR integrates data preprocessing, augmentation, model construction, training, and hyperparameter tuning into a single workflow. The framework simplifies the deployment of gesture recognition models and reduces the need for manual interventions.

% The evaluation on 10 datasets shows the robustness of AutoGR. QuartzNet, the core model in this framework, achieves strong performance on most datasets due to its ability to capture temporal and hierarchical patterns. Automated hyperparameter tuning with SMAC ensures optimal configurations for different datasets, making the framework accessible to users with varying levels of expertise.

% Although AutoGR has limitations with datasets like DB4 and LMDHG. These highlight areas for improvement, such as integrating 2D models and transformer-based architectures into the model factory. Expanding AutoGR with more datasets and open-source development will further enhance its usability and adaptability. This paper provides a step forward in automated gesture recognition, offering a scalable and flexible solution for diverse real-world applications.


% \section{Discussion, Limitations, and Conclusion}

% \subsection{Discussion}
% The evaluation of QuartzNet in AutoGR across 10 datasets highlights its strong performance and adaptability. AutoGR automates the full workflow, including data preprocessing, augmentation, model construction, training, and hyperparameter tuning, ensuring seamless deployment across diverse gesture recognition tasks.

% \textbf{Data Processing:} AutoGR standardizes dataset formats, sampling rates, and modalities, ensuring consistent, noise-free input. Techniques like resampling and augmentation improve class balance, enhancing model performance on datasets like OPPORTUNITY.

% \textbf{Data Augmentation:} Augmentation methods such as time warping and noise addition improve generalization by simulating real-world variations. This reduces overfitting, as observed in MHEALTH and SHREC2021.

% \textbf{Model Construction and Training:} QuartzNet’s depthwise separable convolutions and residual connections efficiently capture hierarchical temporal patterns while maintaining computational efficiency.

% \textbf{Hyperparameter Tuning:} AutoGR employs SMAC to automate hyperparameter selection, eliminating manual tuning and ensuring optimal performance across different datasets.

% \subsection{Limitations and Future Work}
% QuartzNet underperforms on DB4 and LMDHG, as it struggles with sEMG signals and 2D temporal projections. Future work includes:
% \begin{itemize}
%     \item 2D Convolutional Models: Incorporating CNNs for spatial feature extraction in datasets like LMDHG.
%     \item Hybrid Models: Combining temporal and spatial analysis for sEMG-rich datasets like DB4.
%     \item Transformer-Based Architectures: Exploring transformers to improve handling of long-range dependencies in sequential data.
%     \item Expanding Dataset Scope: Including data from healthcare, robotics, and human-computer interaction with modalities like radar and camera-based inputs.
%     \item Open-Source Development: Encouraging contributions to expand AutoGR’s capabilities, improving accessibility and transparency.
% \end{itemize}

% \subsection{Conclusion}
% We introduce AutoGR, an automated framework integrating data preprocessing, augmentation, model training, and hyperparameter tuning for gesture recognition. Evaluations across 10 datasets demonstrate its robustness, with QuartzNet excelling in capturing temporal and hierarchical patterns. Automated hyperparameter tuning via SMAC enhances accessibility by optimizing configurations for diverse datasets.

% While AutoGR faces challenges with DB4 and LMDHG, integrating 2D models and transformers can further improve adaptability. Expanding dataset support and fostering open-source collaboration will enhance its scalability, making AutoGR a flexible and accessible solution for real-world gesture recognition applications.


% \section{Discussion, Limitations, and Conclusion}

\section{CONCLUSION}

The evaluation of AutoMR across ten datasets demonstrates its effectiveness in automating motion recognition by streamlining data preprocessing, model training, and hyperparameter tuning. AutoMR achieves state-of-the-art performance on eight datasets, highlighting its adaptability across different sensor modalities. However, performance on DB4 and LMDHG remains lower due to the limitations of QuartzNet’s 1D convolutional architecture in capturing spatial dependencies and long-range temporal patterns. Addressing these limitations will require integrating 2D CNNs for enhanced spatial feature extraction and hybrid models that combine sequential and spatial analysis. Additionally, transformer-based architectures may further improve performance by capturing long-range dependencies in unsegmented motion sequences. Beyond model enhancements, expanding dataset coverage with diverse sensor modalities and developing adaptive hyperparameter tuning will improve generalizability. Open-source collaboration will further drive refinements, ensuring continuous advancements. 
% To foster further research and collaboration, we will release the AutoMR codebase publicly, facilitating community-driven improvements and broader adoption. 
% AutoMR provides an automated and scalable solution for motion recognition, and by addressing these challenges, it can become even more robust and applicable to a wider range of real-world tasks.


% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \section*{ETHICAL IMPACT STATEMENT}
% Our work presents an automated motion recognition framework built solely on publicly available datasets and standard data processing techniques. No private or sensitive personal data has been collected or used in this research. While the framework itself is designed for academic and benign applications, we acknowledge that motion recognition technologies could potentially be misused in surveillance or privacy-invasive scenarios. Therefore, we advocate for responsible deployment and usage of such systems. We encourage future researchers to consider and address these ethical concerns, ensuring that technological advances in motion recognition continue to serve societal and scientific progress without compromising individual privacy or ethical standards.
% This work uses only public datasets and standard methods, with no private data involved. % or sensitive personal 
% We advocate for responsible deployment and usage of such systems. We encourage future researchers to consider and address these ethical concerns, ensuring that technological advances in motion recognition continue to serve societal and scientific progress without compromising individual privacy or ethical standards.
% Based on our open-source code, our work is fully reproducible, promoting transparency and ethical research practices.
















% This year, at FG 2025, we will be introducing a new requirement that authors submit an Ethical Impact Statement as part of the submission process. (Note that this requirement applies to all short and long papers submitted to the main track. Each special track may have its own rules.) To support authors in meeting this requirement, as well as reviewers in assessing it, we have prepared this document with general guidelines, a checklist for authors and reviewers to complete, and answers to some frequently asked questions. As this is a new policy, we welcome questions and feedback as we refine it and develop a shared understanding.
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% \clearpage
% \newpage

% \clearpage
% \balance

{\small
\bibliographystyle{ieee}
\bibliography{egbib}
}

\end{document}
