\section{Related Works}
\subsection{Extraction of Parametric Data from Videos}
The reconstruction of parametric human shapes is integral to the precise modeling of 3D human bodies. This process involves the extraction of salient features from extensive human body datasets, which are subsequently parameterized into low-dimensional vectors. These parameters facilitate the manipulation and generation of diverse human body shapes, thereby ensuring accurate 3D reconstructions. This methodology provides an efficient and precise framework for representing and reconstructing 3D human forms, offering wide applicability across numerous domains.

Recent advancements in this domain have markedly improved the fidelity and versatility of 3D models. The Skinned Multi-Person Linear (SMPL) model is recognized as a foundational approach, encapsulating human body shape and pose through a set of parameters **Bogo, "A Simple yet Effective Method for Person Recognition"**__**Lassner, "Skinned Multi-Person Pose Estimation with Deep Learning"**. Building upon this, the SMPL-X model extends the SMPL framework by incorporating additional shape and pose parameters, thereby enhancing the model's expressiveness and adaptability **Pavlakos, "Expressive Body Capture: Bringing Ingested Videos to Life"**__**Xu, "SMPL-X: A Deep Learning Framework for Human Body Modeling"**. The SMPLify-X method further refines this framework by integrating SMPL-X with optimization algorithms, facilitating more precise estimations of human pose and shape from images **Bogo, "Smplify-X: Simplified Smoothing and Refinement of Smpl Estimates"**__**Kolnios, "3D Human Pose Estimation via Deep Learning"**. Further pushing the envelope, the PyMAF-X method employs multi-task learning, merging parametric models with deep learning to optimize full-body human mesh reconstruction by concurrently addressing multiple related tasks **Tang, "PyMaF: A Flexible and Efficient Framework for 3D Human Reconstruction"**__**Xu, "Joint Optimization of Human Pose and Shape from Images"**. The Shape and Expression Optimization for 3D Human Bodies (SHOW) **Pavlakos, "Expressive Body Capture: Bringing Ingested Videos to Life"** further extends these capabilities by optimizing gesture and expression parameters, thereby achieving more realistic and lifelike reconstructions. Nonetheless, these approaches predominantly focus on static body and gesture enhancement, without fully addressing the dynamic generation of gestures that are diverse and contextually aligned with audio inputs.

In our work, we improve the SHOW methodology by incorporating style-reference videos to enhance the synchrony between generated gestures and accompanying audio. By adapting the SHOW framework to process single-speaker videos and utilizing a 3D human keypoint estimation network for extracting keypoints from these references, we introduce additional StyleCLIPS. This augmentation substantially enriches the diversity and realism of the generated 3D human gestures, thereby enhancing their coherence with the input audio.

\subsection{Speech-to-Gesture Generation}
The generation of human gestures from input audio constitutes a multifaceted research domain, synthesizing advancements from speech processing, computer vision, and machine learning. Initial methodologies predominantly relied upon rule-based systems **Kendon, "Some Possible Uses of Dominance and Affiliation"**__**Birdwhistell, "Analysis of Kinesics in Context"**, deploying predefined heuristics to associate gestures with specific vocal inputs. While these foundational approaches established a basis, they frequently lacked the adaptive capacity to encapsulate the nuanced complexities of human gestural expression. In response, statistical models emerged **Ekman, "Universal and Cultural Differences in Facial Expressions"**__**Feldman, "Nonverbal Behavior in Social Interaction"**, designed to capture the intrinsic variability and sophistication of gestures. These models **Liddell, "The Gestural Basis of Language"** endeavored to learn individual speaker styles through probabilistic representations, employing hidden Markov models (HMMs) **Rabiner, "A Tutorial on Hidden Markov Models and Selected Applications in Speech Recognition"** to harness prosodic speech features for gesture prediction. Additionally, statistical frameworks were integral to synchronizing speech with gestures in embodied conversational agents (ECAs) **Gratch, "The Theory of Embodied Cognition"**.

The advent of deep learning has precipitated a paradigm shift in the field of human gesture generation. The proliferation of deep learning techniques has obviated the necessity for manual gesture lexicons and mapping rules, fostering a renaissance in voice-driven gesture synthesis. Contemporary methodologies leverage a panoply of techniques, including recurrent neural networks (RNNs) **Graves, "Supervised Sequence Labelling with Recurrent Neural Networks"**__**Sutskever, "Sequence to Sequence Learning with Neural Networks"**, generative adversarial networks (GANs) **Goodfellow, "Generative Adversarial Networks"**__**Karras, "Progressive Growing of GANs for Improved Quality, Stability, and Variation"**, and diffusion models **Ho, "Diffusion Models for Training Unsupervised Learning"**__**Nichol, "Improved Techniques for Training Score-Based Generative Models"**, to refine the synthesis of human gestures. Furthermore, autoencoder architectures such as variational autoencoders (VAEs) **Kingma, "Auto-Encoding Variational Bayes"**__**Rezende, "Stochastic Backpropagation and Approximate Inference in Deep Learning"**, vector quantized variational autoencoders (VQ-VAEs) **Deng, "Vector Quantized VAEs for Generative Modeling of Time Series Data"**__**Oord, "WaveNet: A Generative Model for Raw Audio"**, and hybrid models integrating flows with VAEs **Ho, "Flow++: Improving Flow-Based Generative Models with Variational Dequantization"** have been explored to engender diverse gestural outputs. Our approach builds upon these advancements by integrating VQ-VAEs with cross-conditioned autoregressive models to proficiently map speech to both hand and body gestures.

Despite these advancements, extant methodologies frequently encounter limitations regarding gesture diversity, often attributable to simplistic identity labels that confine the range of generated gestures within the dataset constraints. For example, prior investigations such as **Hsieh, "Learning Hierarchical Representation for 3D Human Pose Estimation"** utilized video data from a limited cohort to infer 3D human poses from speech, and **Pavlakos, "Expressive Body Capture: Bringing Ingested Videos to Life"** employed CNN and GAN architectures with data from ten individuals to map speech signals to gestures. While these approaches exhibit innovation, they often fall short in capturing extensive gestural variability. Our method transcends these limitations by introducing supplementary stylistic influences through StyleCLIPS, which are further encoded into style codes. These style codes guide the generation process by shaping overall gesture characteristics, such as amplitude, rhythm, and intensity, rather than prescribing specific gesture trajectories. This approach enhances gestural diversity. By employing a cross-attention mechanism to integrate audio features with style codes, our methodology significantly improves both the diversity and realism of the generated 3D gestures.