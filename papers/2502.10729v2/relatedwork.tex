\section{Related Works}
\subsection{Extraction of Parametric Data from Videos}
The reconstruction of parametric human shapes is integral to the precise modeling of 3D human bodies. This process involves the extraction of salient features from extensive human body datasets, which are subsequently parameterized into low-dimensional vectors. These parameters facilitate the manipulation and generation of diverse human body shapes, thereby ensuring accurate 3D reconstructions. This methodology provides an efficient and precise framework for representing and reconstructing 3D human forms, offering wide applicability across numerous domains.

Recent advancements in this domain have markedly improved the fidelity and versatility of 3D models. The Skinned Multi-Person Linear (SMPL) model is recognized as a foundational approach, encapsulating human body shape and pose through a set of parameters \cite{42}. Building upon this, the SMPL-X model extends the SMPL framework by incorporating additional shape and pose parameters, thereby enhancing the model's expressiveness and adaptability \cite{43}. The SMPLify-X method further refines this framework by integrating SMPL-X with optimization algorithms, facilitating more precise estimations of human pose and shape from images \cite{43}. Further pushing the envelope, the PyMAF-X method employs multi-task learning, merging parametric models with deep learning to optimize full-body human mesh reconstruction by concurrently addressing multiple related tasks \cite{44}. The Shape and Expression Optimization for 3D Human Bodies (SHOW) \cite{11} further extends these capabilities by optimizing gesture and expression parameters, thereby achieving more realistic and lifelike reconstructions. Nonetheless, these approaches predominantly focus on static body and gesture enhancement, without fully addressing the dynamic generation of gestures that are diverse and contextually aligned with audio inputs.

In our work, we improve the SHOW methodology by incorporating style-reference videos to enhance the synchrony between generated gestures and accompanying audio. By adapting the SHOW framework to process single-speaker videos and utilizing a 3D human keypoint estimation network for extracting keypoints from these references, we introduce additional StyleCLIPS. This augmentation substantially enriches the diversity and realism of the generated 3D human gestures, thereby enhancing their coherence with the input audio.

\subsection{Speech-to-Gesture Generation}
The generation of human gestures from input audio constitutes a multifaceted research domain, synthesizing advancements from speech processing, computer vision, and machine learning. Initial methodologies predominantly relied upon rule-based systems \cite{22}, deploying predefined heuristics to associate gestures with specific vocal inputs. While these foundational approaches established a basis, they frequently lacked the adaptive capacity to encapsulate the nuanced complexities of human gestural expression. In response, statistical models emerged \cite{23}, designed to capture the intrinsic variability and sophistication of gestures. These models \cite{24} endeavored to learn individual speaker styles through probabilistic representations, employing hidden Markov models (HMMs) \cite{25} to harness prosodic speech features for gesture prediction. Additionally, statistical frameworks were integral to synchronizing speech with gestures in embodied conversational agents (ECAs) \cite{26}.

The advent of deep learning has precipitated a paradigm shift in the field of human gesture generation. The proliferation of deep learning techniques has obviated the necessity for manual gesture lexicons and mapping rules, fostering a renaissance in voice-driven gesture synthesis. Contemporary methodologies leverage a panoply of techniques, including recurrent neural networks (RNNs) \cite{27,28,29}, generative adversarial networks (GANs) \cite{30,31,32}, and diffusion models \cite{12,33,34,35}, to refine the synthesis of human gestures. Furthermore, autoencoder architectures such as variational autoencoders (VAEs) \cite{10,36,37,38}, vector quantized variational autoencoders (VQ-VAEs) \cite{11,39,40}, and hybrid models integrating flows with VAEs \cite{41} have been explored to engender diverse gestural outputs. Our approach builds upon these advancements by integrating VQ-VAEs with cross-conditioned autoregressive models to proficiently map speech to both hand and body gestures.

Despite these advancements, extant methodologies frequently encounter limitations regarding gesture diversity, often attributable to simplistic identity labels that confine the range of generated gestures within the dataset constraints. For example, prior investigations such as \cite{11} utilized video data from a limited cohort to infer 3D human poses from speech, and \cite{13} employed CNN and GAN architectures with data from ten individuals to map speech signals to gestures. While these approaches exhibit innovation, they often fall short in capturing extensive gestural variability. Our method transcends these limitations by introducing supplementary stylistic influences through StyleCLIPS, which are further encoded into style codes. These style codes guide the generation process by shaping overall gesture characteristics, such as amplitude, rhythm, and intensity, rather than prescribing specific gesture trajectories. This approach enhances gestural diversity. By employing a cross-attention mechanism to integrate audio features with style codes, our methodology significantly improves both the diversity and realism of the generated 3D gestures.