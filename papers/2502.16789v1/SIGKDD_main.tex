%%
%% This is file `sample-sigconf.tex',
%% generated with the docstrip utility.
%%
%% The original source files were:
%%
%% samples.dtx  (with options: `all,proceedings,bibtex,sigconf')
%% 
%% IMPORTANT NOTICE:
%% 
%% For the copyright see the source file.
%% 
%% Any modified versions of this file must be renamed
%% with new filenames distinct from sample-sigconf.tex.
%% 
%% For distribution of the original source see the terms
%% for copying and modification in the file samples.dtx.
%% 
%% This generated file may be distributed as long as the
%% original source files, as listed above, are part of the
%% same distribution. (The sources need not necessarily be
%% in the same archive or directory.)
%%
%
%% Commands for TeXCount
%TC:macro \cite [option:text,text]
%TC:macro \citep [option:text,text]
%TC:macro \citet [option:text,text]
%TC:envir table 0 1
%TC:envir table* 0 1
%TC:envir tabular [ignore] word
%TC:envir displaymath 0 word
%TC:envir math 0 word
%TC:envir comment 0 0
%%
%%
%% The first command in your LaTeX source must be the \documentclass
%% command.
%%
%% For submission and review of your manuscript please change the
%% command to \documentclass[manuscript, screen, review]{acmart}.
%%
%% When submitting camera ready or to TAPS, please change the command
%% to \documentclass[sigconf]{acmart} or whichever template is required
%% for your publication.
%%
%%
\documentclass[sigconf]{acmart}

%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    Bib\TeX}}}

%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.
\setcopyright{acmlicensed}
\copyrightyear{2018}
\acmYear{2025}
\acmDOI{XXXXXXX.XXXXXXX}

%% These commands are for a PROCEEDINGS abstract or paper.
% \acmConference[Conference acronym 'XX]{Make sure to enter the correct
%   conference title from your rights confirmation emai}{June 03--05,
%   2018}{Woodstock, NY}
%%
%%  Uncomment \acmBooktitle if the title of the proceedings is different
%%  from ``Proceedings of ...''!
%%
%%\acmBooktitle{Woodstock '18: ACM Symposium on Neural Gaze Detection,
%%  June 03--05, 2018, Woodstock, NY}
\acmISBN{978-1-4503-XXXX-X/18/06}

%%
%% Submission ID.
%% Use this when submitting an article to a sponsored event. You'll
%% receive a unique submission ID from the organizers
%% of the event, and this ID should be used as the parameter to this command.
%%\acmSubmissionID{123-A56-BU3}

%%
%% For managing citations, it is recommended to use bibliography
%% files in BibTeX format.
%%
%% You can then either use BibTeX with the ACM-Reference-Format style,
%% or BibLaTeX with the acmnumeric or acmauthoryear sytles, that include
%% support for advanced citation of software artefact from the
%% biblatex-software package, also separately available on CTAN.
%%
%% Look at the sample-*-biblatex.tex files for templates showcasing
%% the biblatex styles.
%%

%%
%% The majority of ACM publications use numbered citations and
%% references.  The command \citestyle{authoryear} switches to the
%% "author year" style.
%%
%% If you are preparing content for an event
%% sponsored by ACM SIGGRAPH, you must use the "author year" style of
%% citations and references.
%% Uncommenting
%% the next command will enable that style.
%%\citestyle{acmauthoryear}

\usepackage{tabularx}
\usepackage{multirow}


\newcommand{\red}[1]{\textcolor{black}{#1}}
%%
%% end of the preamble, start of the body of the document source.
\begin{document}

%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.
\title{AlphaAgent: LLM-Driven Alpha Mining with Regularized Exploration to Counteract Alpha Decay}

%%
%% The "author" command and its associated commands are used to define
%% the authors and their affiliations.
%% Of note is the shared affiliation of the first two authors, and the
%% "authornote" and "authornotemark" commands
%% used to denote shared contribution to the research.
\author{Ziyi Tang}
\affiliation{%
  \institution{Sun Yat-sen University}
  \city{Guangzhou}
  \country{China}
}

\author{Zechuan Chen}
\affiliation{%
  \institution{Sun Yat-sen University}
  \city{Guangzhou}
  \country{China}
}

\author{Jiarui Yang}
\affiliation{%
  \institution{Sun Yat-sen University}
  \city{Guangzhou}
  \country{China}
}

\author{Jiayao Mai}
\affiliation{%
  \institution{University of New South Wales}
  \city{Sydney}
  \country{Australia}
}

\author{Yongsen Zheng}
\affiliation{%
  \institution{Nanyang Technological University}
  \city{50 Nanyang Avenue }
  \country{Singapore}
}

\author{Keze Wang}
\affiliation{%
  \institution{Sun Yat-sen University}
  \city{Guangzhou}
  \country{China}
}

\author{Jinrui Chen}
\affiliation{%
  \institution{The Chinese University of Hong Kong, Shenzhen}
  \city{Shenzhen}
  \country{China}
}

\author{Liang Lin}
\affiliation{%
  \institution{Sun Yat-sen University}
  \city{Guangzhou}
  \country{China}
}
  
% \email{larst@affiliation.org}
% \authornote{Both authors contributed equally to this research.}
% \email{trovato@corporation.com}
% \orcid{1234-5678-9012}

% \author{G.K.M. Tobin}
% \authornotemark[1]
% \email{webmaster@marysville-ohio.com}
% \affiliation{%
%   \institution{Institute for Clarity in Documentation}
%   \city{Dublin}
%   \state{Ohio}
%   \country{USA}
% }

% \author{Lars Th{\o}rv{\"a}ld}
% \affiliation{%
%   \institution{The Th{\o}rv{\"a}ld Group}
%   \city{Hekla}
%   \country{Iceland}}
% \email{larst@affiliation.org}

% \author{Valerie B\'eranger}
% \affiliation{%
%   \institution{Inria Paris-Rocquencourt}
%   \city{Rocquencourt}
%   \country{France}
% }

% \author{Aparna Patel}
% \affiliation{%
%  \institution{Rajiv Gandhi University}
%  \city{Doimukh}
%  \state{Arunachal Pradesh}
%  \country{India}}

% \author{Huifen Chan}
% \affiliation{%
%   \institution{Tsinghua University}
%   \city{Haidian Qu}
%   \state{Beijing Shi}
%   \country{China}}

% \author{Charles Palmer}
% \affiliation{%
%   \institution{Palmer Research Laboratories}
%   \city{San Antonio}
%   \state{Texas}
%   \country{USA}}
% \email{cpalmer@prl.com}

% \author{John Smith}
% \affiliation{%
%   \institution{The Th{\o}rv{\"a}ld Group}
%   \city{Hekla}
%   \country{Iceland}}
% \email{jsmith@affiliation.org}

% \author{Julius P. Kumquat}
% \affiliation{%
%   \institution{The Kumquat Consortium}
%   \city{New York}
%   \country{USA}}
% \email{jpkumquat@consortium.net}

%%
%% By default, the full list of authors will be used in the page
%% headers. Often, this list is too long, and will overlap
%% other information printed in the page headers. This command allows
%% the author to define a more concise list
%% of authors' names for this purpose.
\renewcommand{\shortauthors}{Trovato et al.}

%%
%% The abstract is a short summary of the work to be presented in the
%% article.
\begin{abstract}
% In quantitative investment research, mining alpha factors presents significant challenges, requiring both the theoretical soundness to ensure factor reliability and the innovation capability to adapt to evolving market dynamics. Recent large language model (LLM) agents demonstrate promising potential for alpha mining through their capacity to synthesize domain knowledge into interpretable factors. However, these approaches have not fully unleashed the innovation potential of LLMs as they primarily rely on existing knowledge (such as established technical indicators and published articles) to construct factors. In highly competitive markets, e.g., the US stock market, factors derived from such well-known sources may have already lost their predictive power, leading to inefficient alpha discovery. 

%
%Traditional methods, such as genetic programming, often suffer from rapidly decaying alphas due to overfitting and overly complex constructions, while LLM-driven approaches, though promising, tend to over-rely on established domain knowledge, generating homogeneous factors that further exacerbate crowding and accelerate decay.

Alpha mining, a critical component in quantitative investment, focuses on discovering predictive signals for future asset returns in increasingly complex financial markets. However, the pervasive issue of alpha decay—where factors lose their predictive power over time—poses a significant challenge for alpha mining.
%
Traditional methods like genetic programming face rapid alpha decay from overfitting and complexity, while approaches driven by Large Language Models (LLMs), despite their promise, often rely too heavily on existing knowledge, creating homogeneous factors that worsen crowding and accelerate decay.
%
To address this challenge, we propose AlphaAgent, an autonomous framework that effectively integrates LLM agents with ad hoc regularizations for mining decay-resistant alpha factors. AlphaAgent employs three key mechanisms: (i) originality enforcement through a similarity measure based on abstract syntax trees (ASTs) against existing alphas, (ii) hypothesis—factor alignment via LLM-evaluated semantic consistency between market hypotheses and generated factors, and (iii) complexity control via AST-based structural constraints, preventing over-engineered constructions that are prone to overfitting. These mechanisms collectively guide the alpha generation process to balance originality, financial rationale, and adaptability to evolving market conditions, mitigating the risk of alpha decay. 
%
%Based on this formalization, AlphaAgent builds an autonomous framework encompassing hypothesis proposal, factor construction, backtesting, and feedback mechanisms, progressively deriving alpha factors. 
%
Extensive evaluations show that AlphaAgent outperforms traditional and LLM-based methods in mitigating alpha decay across bull and bear markets, consistently delivering significant alpha in Chinese CSI 500 and U.S. S\&P 500 markets over the past four years. Notably, AlphaAgent showcases remarkable resistance to alpha decay, elevating the potential for yielding powerful factors.
% To address this issue, we propose AlphaAgent, an autonomous framework based on LLM-agent workflows for quantitative trading alpha mining. AlphaAgent synergizes domain experts' judgment with LLM's efficiency and generative capabilities to discover factors that are both theoretically sound and empirically robust across fast-changing market conditions. Specifically, based on a potential market hypothesis given by domain experts, our agents formulate alpha factors via structured expressions to enhance both factor expressiveness and implementation efficiency, with a penalty mechanism to boost innovation. Subsequently, performance metrics are fed back to guide factor evolution, with this autonomous alpha mining loop continuing until the desired performance criteria are satisfied. 
\end{abstract}

%%
%% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
%% Please copy and paste the code instead of the example below.
%%
\begin{CCSXML}
<ccs2012>
   <concept>
       <concept_id>10010147.10010178.10010199.10010202</concept_id>
       <concept_desc>Computing methodologies~Multi-agent planning</concept_desc>
       <concept_significance>300</concept_significance>
       </concept>
   <concept>
       <concept_id>10010147.10010178.10010179.10010182</concept_id>
       <concept_desc>Computing methodologies~Natural language generation</concept_desc>
       <concept_significance>300</concept_significance>
       </concept>
   <concept>
       <concept_id>10010405.10010481.10010487</concept_id>
       <concept_desc>Applied computing~Forecasting</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
 </ccs2012>
\end{CCSXML}

\ccsdesc[300]{Computing methodologies~Multi-agent planning}
\ccsdesc[300]{Computing methodologies~Natural language generation}
\ccsdesc[500]{Applied computing~Forecasting}

%%
%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.
\keywords{Alpha Mining, Quantitative Investment, Large Language Models, Autonomous Agents}
%% A "teaser" image appears between the author and affiliation
%% information and the body of the document, and typically spans the
%% page.
% \begin{teaserfigure}
%   \includegraphics[width=\textwidth]{sampleteaser}
%   \caption{Seattle Mariners at Spring Training, 2010.}
%   \Description{Enjoying the baseball game from the third-base
%   seats. Ichiro Suzuki preparing to bat.}
%   \label{fig:teaser}
% \end{teaserfigure}

% %\received{20 February 2007}
% \received[revised]{12 March 2009}
% \received[accepted]{5 June 2009}

%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
\maketitle

% \section{Introduction}
% \label{sec:intro}

\section{Introduction}
\label{sec:intro}
%第一段有点长，逻辑还蛮好的，不过阐述背景之后需要进行现有方法的剖析，最好是来一个转折，避免reviewer以为第一段全是背景阐述哈 修改如下：
% Factor investing has emerged as a cornerstone strategy in modern quantitative finance, where investment decisions are driven by systematic identification and exploitation of alpha factors - quantifiable characteristics that can predict future asset returns. The process of alpha factor discovery, commonly known as alpha mining, involves developing and validating mathematical expressions that capture various market inefficiencies, ranging from fundamental metrics and technical indicators to alternative data signals. However, the increasing sophistication of market participants and the rapid dissemination of trading strategies have led to a critical challenge: alpha decay. As market inefficiencies are identified and exploited, their predictive power diminishes over time, a phenomenon particularly pronounced in highly efficient markets such as the U.S. equity market. 

\red{
%Factor investing has emerged as a cornerstone strategy in modern quantitative finance, where investment decisions are driven by systematic identification and exploitation of alpha factors - quantifiable characteristics that can predict future asset returns. However, alpha decay, the deterioration of factor returns over time, is increasingly prevalent due to two critical challenges. First, overfitting through excessive data mining ("p-hacking") represents a significant source of alpha decay. As demonstrated by ~\cite{fama1992cross}, many seemingly promising factors may be mere statistical artifacts lacking financial rationale, resulting from over-optimization on historical data. These spurious factors, while appearing statistically significant in backtests, often experience rapid alpha decay in out-of-sample periods. Second, factor crowding occurs when too many investors adopt similar strategies, further accelerating alpha decay and potentially leading to sudden reversals during market stress periods~\cite{asness2013value,falck2022systematic}. This crowding effect has been repeatedly observed in global markets: the size factor's dramatic underperformance in China's A-share market during early 2024~\cite{wool2024china,CICC2024H2Outlook}, demonstrating how concentrated positioning in popular factors can trigger severe drawdowns when market participants simultaneously attempt to reduce exposure. 
}

Factor investing is a key strategy in modern quantitative finance, focusing on the systematic identification and exploitation of alpha factors (a.k.a., alphas) — quantifiable characteristics that can predict asset returns. However, alpha decay, or the decline of factor returns over time, arises from two main challenges. First, overfitting through excessive data mining ("p-hacking") leads to the emergence of spurious factors that appear significant in backtests but decay rapidly in real-world applications~\cite{fama1992cross}. Second, factor crowding occurs when too many investors adopt similar strategies, which can accelerate alpha decay and trigger sudden reversals during market stress~\cite{asness2013value,falck2022systematic,chen2024alpha}. This was evident in early 2024 with the size factor's underperformance in China's A-share market~\cite{wool2024china,CICC2024H2Outlook}, highlighting the risks of concentrated positioning in popular factors. Thus, it is rather crucial to counteract alpha decay during alpha mining.

%传统方法有哪些，需要加一些参考文献，不然没有说服力哈 
% Recently, some research has begun to focus on the issue of alpha decay. This accelerated factor decay necessitates continuous innovation in factor discovery methodologies. 
%
% {\color{red}Existing methods for alpha mining can be broadly categorized into two groups: traditional approaches  and Large Language Models (LLMs)-based ones ~\cite{AutomateStrategy, QuantAgent, Alpha-GPT, Alpha-GPT_2.0, RD-Agent}.} 
\red{
Traditional methods for alpha mining mainly build on genetic programming (GP)~\cite{lin2019revisiting, 10.1145/1830483.1830584,zhaofan2022genetic,patil2023ai,alphagen,alphaevolve} and reinforcement learning (RL). However, they struggle to effectively address the alpha decay challenge. Traditional GP and RL approaches tend to over-emphasize the optimization of historical performance metrics while neglecting the underlying financial and economic rationale. Without sufficient consideration of financial soundness and economic intuition, these methods often produce alpha factors that show strong historical performance but experience rapid alpha decay when deployed in live markets. 
%
Large Language Models (LLMs) offer promising potential in factor mining due to their extensive understanding of financial knowledge, which could help generate and evaluate factors beyond pure statistical significance. 
%
Despite their versatility, LLMs' direct application to alpha factor mining remains underutilized, revealing significant challenges in addressing alpha decay. The fundamental limitation lies in the lack of effective constraints in current LLM-based factor mining frameworks. Without proper theoretical and empirical constraints to guide factor mining, LLMs tend to overly rely on well-documented financial knowledge and established factors during construction, such as traditional technical indicators (e.g., RSI~\cite{ctuaran2011relative}) and widely studied market anomalies (e.g., momentum, value, and size effects). This constrained-free approach exacerbates factor crowding - LLMs predominantly generate factors that capture the same market inefficiencies as existing ones, which are already heavily exploited by market participants. In rapidly evolving markets like the U.S. stock market, LLM-generated factors thus fail to identify novel sources of alpha, leading to suboptimal investment performance.  This highlights the need for innovative approaches that constrain LLMs' factor generation process by encouraging the exploration of unique factor constructions that balance theoretical soundness with market adaptability.
}

To address these issues, we propose a novel paradigm that effectively constrains LLM-based factor generation \red{to mitigate alpha decay, addressing key limitations of conventional approaches.} At its core, AlphaAgent introduces three critical regularization mechanisms to guide LLM-based factor generation: (1) complexity control through symbolic expression trees and parameter counting, (2) hypothesis alignment via LLM-evaluated semantic consistency between market hypotheses and generated factors, and (3) novelty enforcement through a similarity measure based on abstract syntax trees (ASTs) against existing alpha libraries (e.g., Alpha101). 
%
AlphaAgent formalizes factor construction through an operator library and abstract syntax trees (ASTs), implementing a pairwise subtree isomorphism detection mechanism to quantify factor originality while using LLMs to verify financial intuition alignment through consistency scoring between hypotheses, descriptions, and expressions. These constraints guide LLMs to explore novel market inefficiencies while maintaining theoretical soundness, \red{alleviating the alpha decay challenge.} Based on these constraints, AlphaAgent builds an autonomous workflow encompassing hypothesis proposal, factor construction, factor development, backtesting, and feedback mechanisms. The framework begins with the \textit{idea agent} for the hypothesis proposal, with the first market insight provided by domain experts. Grounded in these hypotheses, the \textit{factor agent} constructs parsimonious and original factors to explore unexploited market inefficiencies, applying regularization mechanisms to balance complexity, novelty, and hypothesis alignment. The \textit{eval agent} then rigorously validates factors' executability and numerical stability while backtesting assesses their predictive effectiveness on historical data. The feedback mechanism evaluates performance metrics and theoretical soundness, guiding iterative refinement. This closed-loop process progressively derives a family of factors that capture emerging rather than overcrowded market inefficiencies, \red{promoting alpha mining by balancing theoretical soundness with factor originality. }


Extensive experiments demonstrate AlphaAgent's effectiveness in generating factors resistant to alpha decay. Through comprehensive evaluations in the Chinese CSI 500 and U.S. S\&P 500 markets from January 2021 to December 2024, our framework demonstrates stable factor performance across different market regimes. Through comprehensive evaluations in the Chinese CSI 500 and U.S. S\&P 500 markets from January 2021 to December 2024, our framework achieves an average annual excess return of 11.0\% (IR=1.5) and 8.74\% (IR=1.05) respectively after accounting for transaction costs, while maintaining robust performance across different market regimes. While traditional factors exhibit substantial decay in predictive power due to market crowding and overfitting, AlphaAgent's alphas maintain stable predictive effectiveness throughout the past 5 years, demonstrating stronger resistance to alpha decay. The framework also displays efficiency in mining alphas, achieving an 81\% higher effective factor ratio (hit ratio) while consuming 30\% fewer tokens. These results substantially outperform traditional alpha mining approaches and existing LLM-driven ones in terms of both performance and performance persistence. 

Our main contributions can be summarized as follows:

\begin{itemize}
    \item We propose a systematic regularization mechanism to counteract alpha decay, combining originality enforcement, hypothesis alignment, and complexity control, facilitating the exploration of original and theoretically grounded alphas. 
    \item We implement a closed-loop multi-agent framework that evolves alphas with three agents that iteratively perform hypothesis generation, factor construction, and evaluation. 
    \item Extensive experiments reveal AlphaAgent's superior performance and resilience against alpha decay, attaining an 81\% improvement in hit ratio.
\end{itemize}

% Despite their versatility, their direct application to alpha factor mining remains surprisingly underutilized, revealing significant gaps that warrant urgent attention. Current LLM-generated factors often falter in adapting to the swift and unpredictable shifts in market conditions. We speculate that this may be due to the over-reliance of LLMs on long-standing financial knowledge during factor construction, such as established technical indicators~\cite{} and classic works~\cite{}, leading to factor homogenization. As a result, in rapidly evolving markets like the U.S. stock market, these factors often fail to capture emerging market inefficiencies and suffer from rapid alpha decay, leading to diminished predictive power and suboptimal investment performance. This highlights the need for approaches that generate factors balancing theoretical soundness with innovation, mitigating factor homogenization, and improving adaptability to dynamic markets.


% Extensive experiments demonstrate the effectiveness of our proposed framework. We evaluate AlphaAgent on both Chinese A-shares and U.S. stock markets over four years from January 2021 to December 2024, encompassing various market conditions including both bull and bear markets. In the Chinese A-shares market, our framework achieves an average annual excess return of 9.5\% after accounting for transaction costs, with an information ratio of 2.1. Similar robust performance is observed in the U.S. market, delivering a 7.12\% annual excess return with an information ratio of 1.8. Notably, our framework maintains consistent performance across different market regimes, with only a modest decline in excess returns during the 2022 bear market period. The framework also demonstrates superior efficiency in factor mining, achieving a 45\% higher effective factor ratio while consuming 30\% fewer tokens compared to existing LLM-based approaches. These results substantially outperform both traditional factor mining methods and existing LLM-based approaches across all key metrics.

% Original Text
%Factor investing has emerged as a cornerstone strategy in modern quantitative finance, where investment decisions are driven by systematic identification and exploitation of alpha factors - quantifiable characteristics that can predict future asset returns. The process of alpha factor discovery, commonly known as factor mining, involves developing and validating mathematical expressions that capture various market inefficiencies, ranging from fundamental metrics and technical indicators to alternative data signals. However, the increasing sophistication of market participants and the rapid dissemination of trading strategies have led to a critical challenge: alpha decay. As market inefficiencies are identified and exploited, their predictive power diminishes over time, a phenomenon particularly pronounced in highly efficient markets such as the U.S. equity market. This accelerated factor decay necessitates continuous innovation in factor discovery methodologies. Traditional approaches, which predominantly rely on historical patterns and established financial theories, often struggle to adapt swiftly to evolving market conditions, especially during periods of regime shifts or unprecedented market events. The need for innovation capability in factor mining is further amplified by the growing complexity of financial markets, where the interactions between various market participants, regulatory changes, and technological advancements create dynamic and sometimes unforeseen trading opportunities. This rapidly changing landscape demands not only theoretical soundness for factor construction but also the agility to quickly identify and capitalize on emerging market inefficiencies prior to their dissipation.


%Large Language Models (LLMs) have shown promising potential in quantitative trading through their ability to process multi-modal financial data and generate interpretable investment signals~\cite{FinVision, LLMFactor, RDAgent}. Existing approaches~\cite{AutomateStrategy, QuantAgent, Alpha-GPT, Alpha-GPT_2.0, RD-Agent} primarily investigate LLM-based agents that effectively integrate various data sources, including market data, news articles, and technical indicators, to capture informed trading signals. Despite these advancements, the direct application of LLMs to alpha factor mining remains underexplored, and such an approach often introduces critical limitations. We observe that LLM-generated factors struggle to adapt to rapidly changing market conditions. We speculate that this may be due to the over-reliance of LLMs on long-standing financial knowledge during factor construction, such as established technical indicators and classic works, leading to factor homogenization. As a result, in rapidly evolving markets like the U.S. stock market, these factors often fail to capture emerging market inefficiencies and suffer from rapid alpha decay, leading to diminished predictive power and suboptimal investment performance. This highlights the need for approaches that generate factors balancing theoretical soundness with innovation, mitigating factor homogenization, and improving adaptability to dynamic markets. 

% In this paper, we propose an autonomous framework based on LLM-agent workflows, named AlphaAgent, towards enduring and effective alpha factor mining. First, 


% This paper proposes an autonomous framework based on the LLM-agent workflow, referred to as AlphaAgent, towards enduring and effective alpha factor mining. At its core, AlphaAgent implements an end-to-end workflow that encompasses hypothesis proposal, factor construction, factor development, backtesting, and feedback mechanisms, forming a closed loop for continuous alpha mining and refinement. To begin with, our framework begins with a hypothesis proposal, where domain experts provide initial market hypotheses based on their professional judgment and market observations. Based on these market hypotheses, our framework introduces structured expression to formalize and simplify factor construction, where an ad-hoc LLM agent composes mathematical expressions using a predefined operator library while incorporating a novel penalty mechanism to encourage innovative combinations beyond conventional patterns. Subsequently, the framework proceeds with rigorous factor development to ensure expressions' executability, semantic correctness, and numerical validity of their computation results, followed by backtesting on recent historical data to validate factor effectiveness. The feedback mechanism then systematically evaluates the iteration results, including factor performance metrics, computational robustness, and theoretical soundness, providing structured guidance for the next iteration of factor refinement in the autonomous alpha mining loop. This iterative process continues to evolve, allowing the framework to progressively derive a family of factors from the initial market hypothesis, each capturing different aspects or variations of the underlying market inefficiency. Additionally, our framework significantly improves the efficiency of alpha mining by requiring fewer tokens while achieving a higher proportion of effective factors.

%This paper proposes a novel framework that effectively integrates Large Language Models (LLMs) with domain-specific constraints for alpha factor mining, addressing key limitations of conventional approaches. At its core, AlphaAgent introduces three critical regularization mechanisms to guide LLM-based factor generation: (1) complexity control through symbolic expression trees and parameter counting, (2) hypothesis alignment via LLM-evaluated semantic consistency between market hypotheses and generated factors, and (3) novelty enforcement through a similarity measure based on abstract syntax trees (ASTs) against existing alpha libraries (e.g., Alpha101). AlphaAgent formalizes factor construction through an operator library and abstract syntax trees (ASTs), implementing a pairwise subtree isomorphism detection mechanism to quantify factor originality while using LLMs to verify financial intuition alignment through consistency scoring between hypotheses, descriptions, and expressions. These constraints harness LLMs' stochastic nature and boost their insight by ensuring generated factors maintain domain plausibility through hypothesis grounding. Based on these constraints, AlphaAgent builds an autonomous workflow encompassing hypothesis proposal, factor construction, factor development, backtesting, and feedback mechanisms. The framework begins with the \textit{idea agent} for the hypothesis proposal, with the first market insight provided by domain experts. Grounded in these hypotheses, the \textit{factor agent} constructs parsimonious and original factors, applying regularization mechanisms to balance complexity, novelty, and hypothesis alignment. The \textit{eval agent} then rigorously validates factors' executability and numerical stability, while backtesting assesses their predictive effectiveness on historical data. The feedback mechanism evaluates performance metrics and theoretical soundness, guiding iterative refinement. This closed-loop process progressively derives a family of factors, capturing variations of market inefficiencies with improved efficiency and a higher proportion of effective factors.


%Extensive experiments demonstrate the effectiveness of our proposed framework. We evaluate AlphaAgent on both Chinese A-shares and U.S. stock markets over four years from January 2021 to December 2024, encompassing various market conditions including both bull and bear markets. In the Chinese A-shares market, our framework achieves an average annual excess return of 9.5\% after accounting for transaction costs, with an information ratio of 2.1. Similar robust performance is observed in the U.S. market, delivering a 7.12\% annual excess return with an information ratio of 1.8. Notably, our framework maintains consistent performance across different market regimes, with only a modest decline in excess returns during the 2022 bear market period. The framework also demonstrates superior efficiency in factor mining, achieving a 45\% higher effective factor ratio while consuming 30\% fewer tokens compared to existing LLM-based approaches. These results substantially outperform both traditional factor mining methods and existing LLM-based approaches across all key metrics.

% The key contributions of our work can be summarized as follows:

% 1) We propose a novel autonomous framework that synergizes human domain expertise with LLM capabilities for alpha factor mining. Unlike existing approaches that primarily rely on known patterns and indicators, our system actively promotes the discovery of innovative factors while maintaining theoretical soundness.

% 2) We develop a structured expression system for factor formulation that significantly enhances both the expressiveness and computational efficiency of generated factors. This system includes a comprehensive grammar for factor construction and an efficient implementation strategy for real-time factor computation.

% 3) We introduce an innovative penalty mechanism that effectively mitigates factor homogenization by discouraging the generation of factors that closely resemble existing technical indicators or well-documented strategies. This mechanism helps maintain factor diversity and promotes the discovery of novel market inefficiencies.

% 4) We conduct extensive empirical evaluations across both Chinese A-shares and US equity markets over a four-year period, demonstrating the robust performance of our approach across different market regimes and conditions. Our results show consistent alpha generation capabilities, with average annual excess returns of 9.5% and 7.12% respectively after accounting for transaction costs.

% The remainder of this paper is organized as follows. Section 2 reviews related work in factor investing and LLM applications in quantitative finance. Section 3 presents the detailed methodology of our AlphaAgent framework. Section 4 describes our experimental setup and empirical results. Section 5 provides comprehensive analyses and discussions of our findings. Finally, Section 6 concludes the paper and suggests directions for future research.

\section{Related Work}
Alpha factors, or mathematical expressions designed to predict future asset returns, have been a central focus in quantitative finance since Fama and French's pioneering work on their three-factor model~\cite{fama1992cross}. Traditional methods for alpha mining primarily rely on genetic programming (GP) for exploring the vast search space of factor formulation~\cite{lin2019revisiting, 10.1145/1830483.1830584,zhaofan2022genetic,patil2023ai}, yet they struggle to generate factors resistant to alpha decay. 
%
AlphaEvolve~\cite{alphaevolve} enriches traditional GP by incorporating parameter learning and matrix operations while maintaining GP's explicit formula structure. ~\cite{shen2023mining} strengthens GP with sparsity constraints during mutation, which guides the search toward alpha factors with lower complexity. 
%
Another branch of traditional alpha factor mining relies on Reinforcement learning (RL) to optimize factor formulation through policy learning~\cite{alphagen,shi2024alphaforgeframeworkdynamicallycombine,finrl}. AlphaGen~\cite{alphagen} mines formulaic alphas with deep reinforcement learning, using combination model performance as a reward signal to guide exploration within the alpha factor search space. Shi et al.~\cite{shi2024alphaforgeframeworkdynamicallycombine} propose an RL-based framework that simultaneously discovers and combines multiple alpha factors with fixed weights to form a unified signal. 
%
Building upon PPO~\cite{ppo}, an RL-based approach~\cite{zhao2024quantfactorreinforceminingsteady} is implemented that discards the critic network and introduces a reward-shaping mechanism aiming to generate more profitable and stable alphas for quantitative investment. 
%
\red{
However, these traditional paradigms face significant challenges in addressing alpha decay. Both GP and RL approaches tend to over-emphasize historical performance optimization, leading to either overly complex, overfitted factors or factors lacking economic rationale~\cite{shen2023mining, shi2024alphaforgeframeworkdynamicallycombine, zhao2024quantfactorreinforceminingsteady}. These limitations result in rapid alpha decay when factors are deployed in live markets.}

Recent advances in Large Language Models (LLMs) offer a promising direction to address the alpha decay challenge, as LLMs excel at capturing evolving patterns and incorporating domain knowledge to generate more resilient factors~\cite{wang2023aligninglargelanguagemodels, haluptzok2023language, zhu2024largelanguagemodelslearn, weng2023agent, sumers2024cognitivearchitectureslanguageagents}. While numerous studies have explored alpha mining using LLMs, the critical issue of alpha decay has received limited attention. AutoAlpha~\cite{zhang2020autoalphaefficienthierarchicalevolutionary} introduces an adaptive factor generation framework that continuously evolves alphas based on recent market conditions. LLMFactor~\cite{wang2024llmfactorextractingprofitablefactors} leverages knowledge-guided prompting to extract economically interpretable factors from financial news and historical data. FAMA~\cite{li-etal-2024-large-language} further advances this direction by dynamic factor combination and cross-sample selection, enabling performance across different market regimes. RD-Agent~\cite{yang2024collaborative} proposes a data-centric feedback loop that allows continuous factor adaptation to changing market conditions. However, existing approaches still suffer from alpha decay as they lack effective regularization mechanisms to prevent factors from overly relying on historical patterns and existing market knowledge.

% Recent advances in Large Language Models (LLMs) offer a promising direction to address these limitations, as LLMs have shown remarkable capabilities across multiple domains~\cite{wang2023aligninglargelanguagemodels, haluptzok2023language, zhu2024largelanguagemodelslearn, weng2023agent, sumers2024cognitivearchitectureslanguageagents}.
% %
% Recent works have explored various approaches to leverage LLMs for alpha mining. AutoAlpha ~\cite{zhang2020autoalphaefficienthierarchicalevolutionary} demonstrates the effectiveness of LLM-based formulaic alphas mining through comprehensive backtests in the Chinese stock market. LLMFactor~\cite{wang2024llmfactorextractingprofitablefactors} introduces a sequential knowledge-guided prompting framework that extracts interpretable factors from financial news and historical data. FAMA~\cite{li-etal-2024-large-language} advances this direction by incorporating cross-sample selection for factor diversification and chain-of-experience for efficient exploration while introducing dynamic factor combination strategies to address factor decay. RD-Agent introduces a data-centric loop to research and develop alpha factors autonomously~\cite{yang2024collaborative}. Despite these advances, challenges remain in adapting LLMs to dynamic market conditions, particularly maintaining factor effectiveness across different market regimes. 

%LLMs have demonstrated remarkable self-improvement capabilities through iterative learning in various domains such as gaming, programming, and mathematical problem solving ~\cite{wang2023aligninglargelanguagemodels, haluptzok2023language, zhu2024largelanguagemodelslearn}. 
%
% To harness this potential for domain-specific tasks, researchers have developed frameworks for autonomous LLM agents~\cite{weng2023agent, sumers2024cognitivearchitectureslanguageagents} that combine adaptive tool utilization~\cite{schick2023toolformer}, sophisticated memory systems and strategic planning mechanisms ~\cite{yao2023react, yao2023tree}. 
%
% Building upon the proven success of existing frameworks in real-world applications~\cite{autogpt}, RD-Agent~\cite{yang2024collaborative} innovatively introduces a Researcher-Developer workflow architecture that emulates real-world R\&D processes, aiming to develop robust and economically sound alpha factors through iterative refinement. 
%


% In this section, we briefly review existing research on multi-agent systems in finance and LLM-based alpha mining. These two areas are closely related to our framework, as multi-agent collaboration can enhance the decision-making processes in financial tasks, while alpha mining techniques leverage LLMs for generating new trading signals and strategies.

% \subsection{Foundation Models in Finance}
% Recent advances in foundation models, including large language models (LLMs) and multi-modal large language models (MLLMs) have spurred the development of multi-agent frameworks designed for financial applications. Such systems often feature different agents specialized in tasks like fundamental analysis, sentiment detection, and risk management, thereby replicating the collaborative nature of real-world trading teams.

% For instance, \textbf{FinRobot}~\cite{FinRobot} provides an open-source, multilayer architecture, which includes a Financial AI Agents layer that breaks down complex financial problems through a Chain-of-Thought (CoT) approach. Its platform-centric design streamlines the integration of different LLM agents to serve various analytical and decision-making functions.

% \textbf{FinAgent}~\cite{FinAgent} extends this idea to multimodal data, harnessing numerical, textual, and graphical information. Its dual-level reflection module enables rapid adaptation to market volatility, while a diversified memory retrieval mechanism and explicit integration of established trading principles enhance both the accuracy and explainability of trading actions.

% Researchers have also explored methods to emulate human-like memory and role structures within multi-agent systems. \textbf{FINMEM}~\cite{Finmem} proposes a layered memory approach, incorporating cognitive components such as profiling and decision-making to handle volatile financial data. This design allows the agent to continuously evolve its professional knowledge in real-time.

% Another representative framework, \textbf{TradingAgents}~\cite{TradingAgents}, brings together multiple LLM-driven agents with distinct specialties, from fundamental and sentiment analysis to technical trading strategies. By simulating dynamic debates and risk checks across these agents, the framework aims to improve trading performance. Similarly, \textbf{TradExpert}~\cite{TradExpert} adopts a Mixture-of-Experts paradigm, training separate LLMs on different data sources (e.g., news, market factors) and consolidating their outputs via a ``general expert'' agent. Beyond single-agent performance, \textbf{Enhancing LLM Trading Performance with Fact-Subjectivity Aware Reasoning}~\cite{FSReasoningAgent} delves into separating factual and subjective reasoning processes within a multi-agent environment, showcasing the benefits of each in bull and bear markets, respectively.

% Finally, the importance of multi-agent collaboration and LLM-based finance research has also been highlighted in survey works such as \textbf{Large Language Model Agent in Financial Trading: A Survey}~\cite{LLMSurvey}, which identifies the core architectural components and performance metrics in LLM-based trading agents, as well as challenges that remain unsolved.


% \subsection{LLM-Based Alpha Mining}
% Alpha factors (often referred to simply as ``alphas'') are essential to quantitative finance, designed to generate excess returns by exploiting specific market patterns, anomalies, or inefficiencies. Formulaic alpha, as a crucial subset of alphas, refers to a rule-based strategy in quantitative investment, relying on predefined mathematical models or algorithms. Formulaic alphas are typically created by search algorithms. Genetic programming~\cite{}, which evolves new alphas through structural and numerical mutations, Reinforcement learning~\cite{} dynamically optimizes trading strategies by learning from market interactions, maximizing cumulative returns through continuous exploration and exploitation of financial data. However, these approaches may lack the flexibility to fully encapsulate the creative insights of human researchers. 

% % , or machine learning models, such as LSTM, XGBoost, LightGBM. 

% %  mining typically involves manually crafted or , but

% % This paper focuses on formulaic alphas that refer to a rule-based strategy in quantitative investment, relying on predefined mathematical models or algorithms. Formulaic alphas are typically created .
% % While these approaches can capture complex market dynamics and improve predictive accuracy, they may also face challenges such as reduced interpretability and increased risk of overfitting. The goal of formulaic alphas is to provide a structured and repeatable framework for identifying and capitalizing on market opportunities.


% \textbf{Alpha-GPT}~\cite{AlphaGPT} addresses this gap by introducing a human-AI interactive alpha mining paradigm. Leveraging prompt engineering, it translates the domain expertise of quantitative researchers into new alpha factors that demonstrate both novelty and empirical effectiveness. Additionally, \textbf{Automate Strategy Finding with LLM in Quant investment}~\cite{AutomateStrategy} adopts a multi-agent architecture integrated with LLM-generated alpha signals from multimodal data. The proposed framework dynamically evaluates market conditions using ensemble learning, which leads to substantial gains in trading performance and stability.

% Overall, these works underscore the potential of LLMs to discover novel and profitable alpha factors, bridging human intuition with the modeling power of large-scale deep learning. They exemplify the ongoing efforts to refine alpha mining for more robust, data-driven decision-making in volatile financial environments.

% IC                                                   0.020820
% ICIR                                                 0.189262
% Rank IC                                              0.023697
% Rank ICIR                                            0.221166
% 1day.excess_return_without_cost.mean                 0.000584
% 1day.excess_return_without_cost.std                  0.005516
% 1day.excess_return_without_cost.annualized_return    0.138911
% 1day.excess_return_without_cost.information_ratio    1.632318
% 1day.excess_return_without_cost.max_drawdown        -0.134526
% 1day.excess_return_with_cost.mean                    0.000379
% 1day.excess_return_with_cost.std                     0.005516
% 1day.excess_return_with_cost.annualized_return       0.090280
% 1day.excess_return_with_cost.information_ratio       1.060993
% 1day.excess_return_with_cost.max_drawdown           -0.140035
% 1day.ffr                                             1.000000
% 1day.pa                                              0.000000
% 1day.pos                                             0.000000




\begin{figure*}[h]
\center
\includegraphics[width=0.8\textwidth]{figures/Overview.pdf}
\vspace{-10pt}
\caption{The autonomous workflow of AlphaAgent, where three agents work collectively to mine alphas that balance financial rationale, originality, and adaptability to evolving market conditions, counteract the risk of alpha decay in alpha mining tasks.}
%\caption{AlphaAgent's autonomous workflow for alpha mining, where three agents work collectively to mine alphas that balance financial rationale, originality, and adaptability to evolving market conditions, counteract the risk of alpha decay. } 
\label{fig:workflow}
\vspace{-10pt}
\end{figure*}


\section{AlphaAgent}
\subsection{Problem Formulation}
\label{subsec:problem_formulation}

The alpha mining task considers a set of stocks $\mathcal{S} = \{s_1, \dots, s_N\}$, a time window $\mathcal{T} = \{t_1, \dots, t_T\}$, and a feature matrix $\mathbf{X} \in \mathbb{R}^{N \times T \times D}$, where $D$ denotes the dimension of the raw features. The objective of alpha mining is to learn an alpha factor (or alpha) $f$ that maps a slice of input features $\mathbf{X}_{t}$ to a predictive signal $r_{t+1}$, namely the subsequent return. Formally, an alpha can be written as $f(\mathbf{X}_{t}) \rightarrow r_{t+1}$, where $r_{t+1}$ is the return on the day $t+1$. The alpha factor mining problem can be formulated as an optimization task:
% \vspace{-5pt}
\begin{equation}
\label{eq:baseline_optimization}
f^* = \arg\max_{f \in \mathcal{F}} \mathcal{L}\bigl(f(\mathbf{X}), \mathbf{y}\bigr) \;-\; \lambda\,\mathcal{R}(f),
\end{equation}
where $\mathcal{F}$ denotes the space of all possible factor expressions, $\mathbf{y}$ represents the ground-truth future returns (e.g., next-day returns), $\mathcal{L}$ measures predictive effectiveness (such as the information ratio or other performance metrics), $\mathcal{R}$ is a regularization term encouraging simplicity or novelty of the factor expression, and $\lambda$ is a balancing parameter that trades off between performance and complexity.

Distinct from conventional pure data-driven methods, we propose to leverage Large Language Models (LLMs) as intelligent agents for alpha factor generation. Traditional methods often struggle to incorporate domain expertise effectively or tend to generate factors that lack economic intuition. LLMs, with their strong natural language understanding and reasoning capabilities~\cite{causalgpt,wang2023aligninglargelanguagemodels,zhu2024largelanguagemodelslearn}, offer a promising solution by being able to comprehend and operationalize human market insights. However, LLMs are inherently intractable due to their stochastic nature and limited ability to extract key information from lengthy contexts, which could lead to inconsistent or irrelevant factor generation. To address these limitations, we introduce two critical aspects in the regularization term to ensure both the practical relevance and long-term effectiveness of the generated factors. Specifically, we introduce market hypotheses $h \in \mathcal{H}$ to guide the LLM-based factor construction process with domain-relevant insights (e.g., candlestick patterns, fundamental analysis results, market microstructure theories), and we ensure the generated factors maintain sufficient novelty. Concretely, we reformulate the above objective as:

\vspace{-5pt}
\begin{equation}
\label{eq:guided_optimization}
f^* = \arg\max_{f \in \mathcal{F}} \;\;\mathcal{L}\bigl(f(\mathbf{X}), \mathbf{y}\bigr) \;-\; \lambda\,\mathcal{R}_{g}(f, h),
\end{equation}

where the regularization term $\mathcal{R}_{g}(f, h)$ encompasses three components: (1) the complexity of the factor expression, (2) the alignment between the factor and a market hypothesis $h \in \mathcal{H}$ grounded in external knowledge (e.g. from domain experts), and (3) the distinctiveness of the generated factor from existing ones. By incorporating these aspects into the regularization term, we ensure that the LLM-generated factors not only maintain theoretical relevance and practical interpretability through market hypotheses but also exhibit sufficient novelty to potentially capture unexploited market inefficiencies. Given the non-convex nature of the optimization objective, we employ an alternating optimization strategy between $\mathcal{L}$ and $\mathcal{R}_{g}$ to ensure convergence to a local minimum. The alternating procedure continues until finding a locally optimal alphas that balances predictive performance with the regularization constraints. This objective provides a flexible mechanism to balance predictive ability, domain soundness, and factor uniqueness, ultimately contributing to the long-term effectiveness of alpha factors. The detailed formulation of $\mathcal{R}_{g}(f, h)$ is delineated in Section \ref{subsec:factor_generation}.





\subsection{Factor Generation Modeling}
\label{subsec:factor_generation}
To operationalize the objective function defined in Eq. \ref{eq:guided_optimization}, a factor implementation mechanism is required that ensures both robustness and alignment with domain hypotheses. However, the inconsistent quality of LLM-generated outputs poses significant challenges in code-based factor construction. Approaches that generate code-based factors may frequently encounter operational barriers such as data format incompatibilities, inconsistencies across package versions, and difficulties in maintaining semantic coherence in extended code implementations. These challenges create a fundamental tension between code executability and semantic consistency, requiring LLMs to constantly balance these competing objectives in factor generation. 




\subsubsection{Factor Parsing with Abstract Syntax Trees}

To address these limitations, we introduce an \emph{operator library} $\mathcal{O}$ that abstracts and standardizes various mathematical and financial operations (e.g., rolling minima/maxima, moving averages, conditional checks). This abstraction layer significantly streamlines the factor construction process by providing LLMs with a consistent and well-defined set of operations, thereby simplifying the semantic alignment between operator compositions and market hypotheses. The Operator Library serves as an intermediate representation that bridges the gap between high-level market insights and low-level implementation details, enabling more robust and maintainable factor generation. We define a \emph{parsing} procedure: 

\vspace{-5pt}
\begin{equation}
\label{eq:hypo_parsing}
\mathcal{G}: \bigl(\mathcal{H}, \mathcal{X}\bigr) \;\rightarrow\; \mathcal{F},
\end{equation}

\noindent where $\mathcal{H}$ represents the space of market hypotheses, with each hypothesis often described in a semi-structured form (refer to Sec. \ref{subsec:overview}). $\mathcal{X}$ denotes the space of raw features. The output space $\mathcal{F}$ contains tree-structured factors through \emph{symbolic assembly} from atomic operators in $\mathcal{O}$, where \emph{symbolic assembly} defines the process of composing and binding operators into factor expressions. By referring to $\mathcal{X}$, each factor can be bound to real data fields such as \verb|$price|, \verb|$volume|, and derivatives thereof. 



We parse the textual hypothesis $h$ to a factor $f \in \mathcal{F}$ as an abstract syntax tree (AST), denoted as $T(f)$, via the following steps: (1) Identify key phrases in $h$ (e.g., \emph{triangle pattern''}, \emph{breakout''}) and map them to relevant operators in $\mathcal{O}$; (2) Assign numeric parameters (e.g., window size, threshold) for each operator based on $h$ or default domain values; (3) Assemble the operators into an abstract syntax tree $T(f)$, that captures the computational dependencies and execution flow of the factor expression. In $T(f)$, leaf nodes correspond to raw feature references (e.g., \verb|$high|, \verb|$low|), internal nodes represent operator instances (e.g., \verb|TS_MIN(.)|, \verb|SMA(.)|), and edges indicate the data flow between operations.



\subsubsection{Interpretability and Complexity Control}

Although objective \eqref{eq:baseline_optimization} focuses on maximizing predictive quality subject to domain alignment, it is equally crucial to control the complexity of any candidate expression. We incorporate a regularization term $\mathcal{R}_{g}(f)$ that penalizes overly complex syntax trees or large numbers of free hyperparameters. This ensures that the final solution not only adheres to the economic rationale encoded in $h$ but also remains interpretable and robust. For instance, we may define



\begin{equation}
\label{eq:regularization}
\begin{aligned}
\mathcal{R}_{g}(f, h) \;=\;\alpha_1 \cdot \mathrm{SL}(f) \;+\;\alpha_2 \cdot \mathrm{PC}(f) \;+\; \alpha_3 \cdot \mathrm{ER}(f, h),
\end{aligned}
\end{equation}


\noindent where $\mathrm{SL}(f)$ measures symbolic length, $\mathrm{PC}(f)$ counts free parameters (e.g., window lengths), and $ER(f,h)$ captures both the factor's novelty compared to existing alpha factors and its alignment with the given market hypothesis $h$. By tuning the weighting coefficients $\{\alpha_1,\alpha_2,\alpha_3\}$, we can obtain parsimonious yet powerful factor specifications.


To quantitatively assess the originality of proposed alpha factors and detect potential duplicates, we introduce a pair-wise factor similarity metric based on AST matching. For any given factor $f_i$, we first parse its expression into an AST representation $T(f_i)$. To compute the similarity between two ASTs $f_i$ and $f_j$, we identify their largest common subtree by recursively comparing their AST structures $T(f_i)$ and $T(f_j)$. The similarity metric $s$ is calculated as:


\begin{equation}
\label{eq:ast_pairwise_sim}
\begin{aligned}
s(f_i, f_j) = \max_{t_i \subseteq T(f_i), t_j \subseteq T(f_j)} \{|t_i| : t_i \cong t_j\},
\end{aligned}
\end{equation}

\noindent where $t_i$ and $t_j$ are subtrees of $T(f_i)$ and $T(f_j)$ respectively, $|t_i|$ denotes the size of the subtree (number of nodes), and $t_i \cong t_j$ indicates structural isomorphism between subtrees. With this similarity metric, a newly proposed factor can be compared with existing alphas that have been widely validated (see Fig. ~\ref{fig:ast}), such as Alpha101~\cite{alpha101}. Formally, we compute its maximum similarity score against an existing alpha zoo $\mathcal{Z} = \{\phi_1, \phi_2, ..., \phi_N\}$, providing a quantitative measure of the factor's originality, written as:

\begin{equation}
\label{eq:ast_ori}
\begin{aligned}
S(f) = \max_{\phi \in \mathcal{Z}} s(f, \phi),
\end{aligned}
\end{equation}



To ensure the semantic consistency between market hypotheses and generated factors, we employ LLMs to evaluate two critical alignments: (1) whether the factor description aligns with the market hypothesis as a valid implementation, and (2) whether the factor expression accurately reflects its description. For a given hypothesis $h$, factor description $d$, and factor expression $f$, we formulate a consistency scoring function:

\begin{equation}
\label{eq:alignment_score}
\begin{aligned}
\mathcal{C}(h, d, f) = c_1(h, d) + c_2(d, f),
\end{aligned}
\end{equation}

\noindent where $c_1(h, d) \in [0,1]$ evaluates whether the factor description $d$ represents a valid implementation of hypothesis $h$, $c_2(d, f) \in [0,1]$ measures the consistency between the factor description and its mathematical expression, and $\alpha$ is a weighting parameter. For example, if a factor claims to capture market liquidity dynamics in its description but its expression contains no liquidity-related components (such as trading volume, bid-ask spread, or market depth), it would receive a low $c_2$ score, indicating potential misalignment between the claimed economic intuition and actual implementation. This scoring mechanism helps filter out factors that either deviate from the original market insight or contain mismatches between their semantic meaning and mathematical formulation, thereby reducing the risk of spurious factor generation.


Based on the above similarity metric and consistency evaluation, we can now formulate $\mathrm{ER}(f, h)$ that quantifies both the originality and hypothesis alignment of a generated factor:

\vspace{-5pt}
\begin{equation}
\label{eq:er_score}
\begin{aligned}
\mathrm{ER}(f, h) = & \beta_1 \cdot S(f) + \beta_2 \cdot \mathcal{C}(h, d, f) + \beta_3 \cdot \log(1 + |\mathcal{F}_f|) \\ 
 = & \max_{\phi \in \mathcal{Z}} S(f, \phi) + c_1(h, d) + c_2(d, f) + \log(1 + |\mathcal{F}_f|),
\end{aligned}
\end{equation}

\noindent where $\beta_1, \beta_2, \beta_3$ are weighting coefficients, $\mathcal{F}_f$ represents the set of raw features used in factor $f$'s expression, and the logarithmic term penalizes excessive feature usage to promote factor parsimony. A lower $\mathrm{ER}$ score indicates better factor quality, with the first term penalizing similarity to existing factors, the second term ensuring hypothesis alignment, and the third term controlling expression complexity. By normalizing $S(f, \phi)$ to $[0,1]$ through appropriate scaling, all terms in the equation become comparable in magnitude. 



\begin{figure}[!t]
\center
\includegraphics[width=0.5\textwidth]{figures/ast.pdf}
\vspace{-10pt}
\caption{This figure shows the factor $f$ and an alpha zoo $\mathcal{Z}$ each represented as an expression or AST whose leaf nodes are depicted as light blue. The factor’s originality score is calculated by the maximum size of common subtrees between its AST and each factor within the alpha zoo. } 
\label{fig:ast}
\vspace{-12pt}
\end{figure}

Through the above factor generation process, each candidate $f$ is guaranteed to satisfy the originality, hypothesis alignment, and complexity constraints, while optional refinement heuristics can iteratively enhance or simplify expressions. The overall framework, described in subsequent sections, integrates \textit{eval agent} for factor evaluation, gathers feedback through reflective analysis, and ultimately establishes an autonomous framework to continuously refine and uncover alpha factors that counteract alpha decay. 


\subsection{Autonomous Multi-Agent Framework}
\label{subsec:overview}
As illustrated in Figure ~\ref{fig:workflow}, AlphaAgent implements a recurrent framework for alpha factor mining through three specialized agents powered by large language models (LLMs): \textit{idea agent}, \textit{factor agent}, and \textit{eval agent}. The \textit{idea agent} synthesizes market hypotheses $h$ by integrating human knowledge, research reports, and market insights. Each hypothesis captures a potential market inefficiency pattern, such as value-momentum dynamics to behavioral biases and market structure anomalies~\cite{asness2013value,alpha101}. Next, the \textit{factor agent} translates these hypotheses into factor expressions that capture their underlying market dynamics. For each hypothesis $h$, the \textit{factor agent} generates multiple candidate implementations to quantify different aspects of the hypothesized inefficiency, including a natural language description of the factor logic and corresponding mathematical expressions using structural operators. The \textit{eval agent} evaluates them through backtesting on historical data, in-depth search for similar existing factors, and analysis of performance metrics. Performance feedback from the \textit{eval agent} guides the next iteration of hypothesis refinement and factor construction, forming a closed loop for continuous alpha mining. 

\paragraph{Idea Agent}
The \textit{idea agent} serves as the foundation of our framework by formalizing market hypotheses through a structured knowledge integration process. Drawing from external knowledge, the \textit{idea agent} employs the chain-of-thought reasoning~\cite{wei2022chain,causalgpt} to generate a market hypothesis with a systematic structure encompassing five interconnected components: (1) \textit{knowledge} synthesizes established financial theories (e.g., market efficiency, behavioral finance), empirical market intuitions (e.g., momentum, mean reversion), and practitioners' conjectures derived from trading experience; (2) \textit{market observations} that provide empirical grounding through analysis of current market conditions and emerging patterns; (3) \textit{justification} that establishes theoretical soundness by linking observed patterns to underlying economic mechanisms; (4) \textit{hypothesis} specifies temporal characteristics of the hypothesized market behavior, encompassing pattern-driven triggers (e.g., ``ascending triangle breakout''), volume-price dynamics (e.g., ``rising volume with price consolidation''), and their anticipated market implications; and (5) \textit{specification} outlines implementation constraints, such as optional numeric or time-window parameters (e.g., ``10-day high/low''). In the initialization phase, the \textit{idea agent} generates the seed hypothesis $h_0$ based on expert-provided research directions and market insights. This initial hypothesis serves as an anchor point for the iterative refinement process, where subsequent hypotheses are generated through feedback from the \textit{eval agent}. Through this structured approach, the \textit{idea agent} constructs market hypotheses that unite theoretical rigor with empirical validity, establishing a foundation for systematic alpha discovery.


\paragraph{Factor Agent}

The \textit{factor agent} serves as the bridge between theoretical market hypotheses and their quantitative manifestations, crafting alpha factor implementations through the regulated process outlined in Section \ref{subsec:factor_generation}. To enhance factor quality, the agent maintains an evolving knowledge base of both successful and failed factor implementations. Failed cases are categorized based on their failure modes, such as hypothesis misalignment and structural complexity violations. These failure cases are then encoded into the agent's knowledge base, allowing it to proactively avoid similar pitfalls in subsequent iterations. The agent employs a multi-stage refinement pipeline: first generating multiple candidate implementations for each hypothesis, then applying filters based on complexity and alignment metrics. During generation, the agent optimizes a group of alpha factors by referencing similar historical cases from its knowledge base, until they satisfy the originality, hypothesis alignment, and complexity constraints. This experiential learning mechanism enables the agent to continuously improve its generation capabilities, producing a variety of original, well-aligned, and parsimonious factor implementations over time. 


\paragraph{Eval Agent}

The \textit{eval agent} first conducts a multi-dimensional evaluation of generated factors through a backtesting system. The evaluation process encompasses three primary aspects: predictive capability metrics that measure the factor's forecasting effectiveness, return performance metrics that assess the factor's profit-generating ability, and risk control metrics that evaluate its stability and robustness under various market conditions. Beyond quantitative assessment, the agent maintains an evaluation history to track factors' performance and identify emerging patterns in both successes and failures. This accumulated evaluation knowledge is systematically fed back to the \textit{idea agent}, enabling its hypothesis refinement. In this sense, the evaluation process not only validates individual factors but also continuously provides insights for the next round, forming a closed-loop mechanism to continuously optimize the overall alpha mining. 


% After $n$ rounds of mining, the output is a factor zoo $\{f_1, f_2,...,f_m\}$ where each factor captures a different aspect of the hypothesized inefficiency. 
% Let $\mathcal{F}_h$ denote all possible factor sets derived from hypothesis $h$, each containing up to $m$ different factors. The workflow can be formalized as an iterative optimization process: 

% \vspace{-10pt}
% \begin{equation}
% \begin{aligned}
% & \{f_1^*, \ldots, f_m^*\} = \underset{f_1, \ldots, f_m}{\arg\max} \sum_{i=1}^m \mathcal{L}(f_i(\mathbf{X}), \mathbf{y}) - \lambda \sum_{i=1}^m \mathcal{R}(f_i) \\
% & \text{s.t.} \quad \begin{cases} 
% f_i \in \mathcal{F}, \quad \mathcal{C}(f_i, h) = 1, \quad \forall i \in \{1,\ldots,m\} \\
% |\rho(f_i(\mathbf{X}), f_j(\mathbf{X}))| \leq \delta, \quad \forall i \neq j
% \end{cases}
% \end{aligned}
% \end{equation}

% where $\mathcal{C}(f_i, h)$ ensures each factor aligns with the hypothesis, $\rho(\cdot,\cdot)$ measures the correlation between factor signals, $\delta$ is a predefined threshold, and the regularization term $\mathcal{R}(f_i)$ promotes factor diversity while penalizing complexity.

% This modular design enables systematic factor discovery while maintaining theoretical soundness through structured expression construction and comprehensive evaluation. Each component will be detailed in the following sections. 

% 实现失败率
% token消耗数
% 迭代增长速度


% \subsection{Datasets}
% Our experiments are conducted in the Chinese A-share market and the U.S. stock market. In both markets, we only use OHLCV data to construct alpha factors. The raw data of the Chinese A-share market is collected from Baostock~\cite{}, while the U.S. stock market data is from Yahoo Finance. 

\section{Experiments}

\subsection{Experiment Settings}
\subsubsection{Metrics}
This study focuses on evaluating how AlphaAgent counteracts the alpha decay challenge against established baseline approaches using a comprehensive set of financial metrics. The Information Coefficient (IC) and Rank Information Coefficient (RankIC) measure forecasting precision through the correlation between predicted scores and actual returns, with IC using raw values and RankIC using ranked values. Risk-adjusted performance indicators include the Information Coefficient Information Ratio (ICIR), which evaluates the consistency of IC performance by comparing its mean to standard deviation, and the Information Ratio (IR), which measures risk-adjusted excess returns relative to a benchmark. For absolute performance assessment, we use the Annualized Return (AR) to quantify the yearly investment outcome normalized from cumulative returns, and the Maximum Drawdown (MDD) to capture the largest peak-to-trough decline in portfolio value. \red{With these multi-faceted evaluation metrics, we can assess both the factor's long-term effectiveness and its resistance to alpha decay, as persistent IC/RankIC values and stable ICIR indicate sustained predictive power, while AR and MDD metrics reveal the practical impact of any deterioration in the factor's effectiveness over time. } 




\subsubsection{Backtest Settings}
The backtesting experiments were conducted using the Qlib framework~\cite{yang2020qlibaiorientedquantitativeinvestment}, on CSI 500 of the Chinese A-share market and S\&P 500 of the U.S. stock market, spanning 2021 to 2024. Raw data employed to construct alpha factors include only OHLCV (i.e., \$open, \$high, \$low, \$close, \$volume). CSI 500 data is collected from Baostock~\cite{baostock}, while S\&P 500 data is from Yahoo Finance~\cite{yfinance}. See Table ~\ref{tab:dataset_splits} for precise dataset splits. 


% \vspace{-5pt}
\begin{table}[h!]
\centering
\footnotesize
\renewcommand{\arraystretch}{1.15}  % 增加行高
\begin{tabular}{lcc}
\toprule
\textbf{Asset} & \textbf{Period} & \textbf{Trading Days} \\
\midrule
        & Training: 2015-01 to 2019-12 & 1258 \\
S\&P 500 & Validation: 2020-01 to 2020-12 & 253 \\
        & Testing: 2021-01 to 2024-12 & 1004 \\
\hline
        & Training: 2015-01 to 2019-12 & 1219 \\
CSI 500  & Validation: 2020-01 to 2020-12 & 243 \\
        & Testing: 2021-01 to 2024-12 & 968 \\
\bottomrule
\end{tabular}
\vspace{3pt}
\caption{Periods of Training, validation, and testing splits, with their trading days for S\&P 500 and CSI 500.}
\label{tab:dataset_splits}
\end{table}
% \vspace{-10pt}


AlphaAgent employs GPT-3.5-turbo as the foundational LLM to support agents. For RD-Agent~\cite{chen2024datacentric}, following its authors, GPT-4-turbo is used. Four fundamental alphas, including \textit{intra-day return}, \textit{daily return}, \textit{20-day relative volume}, and \textit{normalized daily range}, serve as the base alphas and will be concatenated with newly proposed alphas to train a LightGBM model~\cite{NIPS2017_6449f44a}. Before being fed into LightGBM, both features and returns undergo cross-sectional Z-score normalization to ensure comparability across stocks. The LightGBM model, with a maximum depth of 4 layers, is responsible for forecasting the next-day returns. In backtesting, a top-$k$ dropout strategy is employed to select the 50 top-ranked stocks based on the predicted returns and exclude the 5 lowest-ranked stocks. All backtesting results account for transaction fees. For CSI 500, the transaction fees are set at 0.0005 for buying and 0.0015 for selling. For S\&P 500, only selling fees are applied, with a rate of 0.0005. 



\subsubsection{Baselines}

We compare AlphaAgent with several baseline approaches: (1) Traditional time-series forecasting models including \textbf{LSTM}~\cite{graves2012long} for capturing temporal dependencies and \textbf{Transformer}~\cite{10.5555/3295222.3295349} for parallel sequence processing; (2) Tree-based model \textbf{LightGBM}~\cite{NIPS2017_6449f44a} for handling structured financial data; (3) Specialized financial models including \textbf{StockMixer}~\cite{Fan_Shen_2024} and \textbf{TRA}~\cite{HengxuKDD2021}, which focus on integrating multiple trading strategies and handling non-i.i.d. market patterns respectively; (4) Agent-based approaches including \textbf{AlphaForge}~\cite{shi2024alphaforgeframeworkdynamicallycombine} and \textbf{RD-Agent}~\cite{chen2024datacentric}, which leverage deep learning and LLMs for alpha factor generation and optimization; (5) Deep reasoning models, \textbf{OpenAI-\textit{o1}}~\cite{openai2024openaio1card} and \textbf{DeepSeek-\textit{R1}}~\cite{deepseekai2025deepseekr1incentivizingreasoningcapability}.

% We compare AlphaAgent with night baselines. 
% (1) \textbf{LSTM (Long Short-Term Memory)}~\cite{graves2012long} is a recurrent neural network designed to capture long-term dependencies in sequential data, making it ideal for time series forecasting in financial markets. (2) \textbf{Transformer}~\cite{10.5555/3295222.3295349} is a self-attention-based architecture that excels at modeling sequential data by capturing dependencies between all time steps simultaneously, offering superior parallel processing capabilities compared to traditional RNNs. (3) \textbf{LightGBM}~\cite{NIPS2017_6449f44a} is a gradient-boosting framework optimized for speed and performance, particularly effective for structured data and feature-rich datasets. (4) \textbf{StockMixer}~\cite{Fan_Shen_2024}, StockMixer is a simplified multilayer perceptron (MLP) based architecture that combines multiple strategies to balance accuracy and stability, making predictions by integrating indicator mixing, time series mixing, and inter-stock relationship mixing. (5) The \textbf{Temporal Routing Adaptor (TRA)} ~\cite{HengxuKDD2021} breaks free from the conventional i.i.d. (independent and identically distributed) assumption in a stock market by intelligently routing different trading patterns to specialized predictors (6) \textbf{AlphaForge} ~\cite{shi2024alphaforgeframeworkdynamicallycombine} is a deep learning framework that mines alpha factors using a generator and dynamically adjusts their weights a composite model. (7) RD-Agent~\cite{chen2024datacentric} implements a research and development (R\&D) loop that incorporates LLM-based autonomous agents for alpha mining and refinement. 


% \textit{(close - open) / open$, $close / Ref(close, 1) - 1$, $volume / Mean(volume, 20)$, and $(high - low) / Ref(close, 1)$.







\begin{table*}[ht!]
  \footnotesize
  \captionsetup{justification=centering}
  \caption{Performance Comparison of Different Methods on CSI 500 (China) and S\&P 500 (U.S.). \textbf{Bold} and \underline{underlined} numbers represent the best and second-best performance across all compared approaches, respectively. }
  \vspace{-3pt}
  \label{tab:performance}
  \begin{tabular}{lcccccccccc}
    \toprule
    \textbf{Market} & \multicolumn{5}{c}{\textbf{CSI 500} (2021-01-01 to 2024-12-31)} & \multicolumn{5}{c}{\textbf{S\&P 500} (2021-01-01 to 2024-12-31)} \\
    \cmidrule(lr){2-6} \cmidrule(lr){7-11}
    \textbf{Method} & IC & ICIR & AR & IR & MDD & IC & ICIR & AR & IR & MDD \\
    \midrule
    LSTM~\cite{graves2012long} & 0.0175 & 0.1521 & \underline{4.96\%} & \underline{0.6225} & \underline{-9.68\%} & 0.0028 & 0.0181 & -1.51\% & -0.1671 & -26.05\% \\
    Transformer~\cite{10.5555/3295222.3295349} & 0.0131 & 0.1074 & 4.11\% & 0.5074 & -17.45\% & 0.0013 & 0.0129 & -4.55\% &  -0.4964 & -34.96\% \\
    LightGBM~\cite{NIPS2017_6449f44a} & 0.0120 & 0.1209 & -1.18\% & -0.1588 & -18.97\% & 0.0011 & 0.0116 & -2.64\% & -0.4224 &  -21.17\%  \\
    TRA~\cite{HengxuKDD2021} & \underline{0.0198} & \underline{0.1794} & 2.91\% & 0.4261 & -12.73\% & -0.0003 & -0.0027 & -8.51\% & -1.1345 & -49.55\% \\
    Stock-Mixer~\cite{Fan_Shen_2024} & 0.0000 & 0.0003 & -0.35\% & -0.0496 & -16.82\% & 0.0030 & 0.0312 & -2.49\% & -0.3342 & -29.43\% \\
    AlphaForge~\cite{shi2024alphaforgeframeworkdynamicallycombine} & 0.0146 & 0.1299 & 3.45\% & 0.3270 & -17.67\% & 0.0017 & 0.0215 & 2.10\% & \underline{0.2604} & -19.57\% \\
    RD-Agent~\cite{chen2024datacentric} & 0.0113 & 0.0872 & 0.78\% & 0.0744 & -20.85\% & 0.0019 & 0.0123 & 1.69\% & 0.1664 & -23.18\% \\
    DeepSeek-\textit{R1}~\cite{deepseekai2025deepseekr1incentivizingreasoningcapability} \textit{best-of-10} & 0.0132 & 0.1201 & 1.58\% & 0.2086 & -14.95\% & \underline{0.0048} & \underline{0.0369} & \underline{2.75\%} & 0.2400 & \underline{-15.34\%}  \\
    OpenAI-\textit{o1}~\cite{openai2024openaio1card} \textit{best-of-10} & 0.0159 & 0.1502 & 0.46\% & 0.0632 & -21.29\% & 0.0028 & 0.0217 & 2.29\% & 0.2021 & -16.35\% \\
    AlphaAgent & \textbf{0.0212} & \textbf{0.1938} & \textbf{11.00\%} & \textbf{1.488} & \textbf{-9.36\%} & \textbf{0.0056} & \textbf{0.0552} & \textbf{8.74\%} & \textbf{1.0545} & \textbf{-9.10\%} \\
    \bottomrule
  \end{tabular}
\end{table*}







% \begin{enumerate}
%     %%\item Sharpe Ratio(SR): $SR= \frac{E(R_p)-R_f}{\sigma_p}$

%     \item Information Coefficient (IC):
    
%     $IC = corr(F_t, R_{t+1})$ measures the correlation between the forecasted scores $F_t$ and actual returns $R_{t+1}$. Here, $corr(\cdot)$ stands for the correlation coefficient, which measures the strength of the linear relationship between two variables, ranging from -1 to 1. A value closer to either 1 or -1 indicates a stronger correlation, while a value close to 0 indicates a weaker correlation.
    
%     \item Rank Information Coefficient (RankIC):
    
%     $RankIC = corr(rank(F_t), rank(R_{t+1}))$, where $rank(F_t)$ is the rank of forecasted scores at time $t$, and $rank(R_{t+1})$ is the rank of actual returns at time $t+1$. 

%     \item ICIR (Information Coefficient Information Ratio):
    
%     $ICIR = \frac{\overline{IC}}{std(IC)}$, where $\overline{IC}$ is the mean of IC time series, and $std(IC)$ is the standard deviation of IC time series. 
    
%     \item Annualized Return (AR): 
    
%     $AR=(1+CR)^{\frac{1}{n}}-1,$ where $CR$ is the Cumulative Return over the entire investment period, $n$ is the number of years in the investment period.

%     \item Information Ratio (IR): 
    
%     $IR=\frac{E[R_{p}-R_{b}]}{\sigma}=\frac{\alpha}{\omega}=\frac{E[R_{p}-R_{b}]}{\sqrt{\mathrm{var}[R_{p}-R_{b}]}},$ where $R_p$ is the portfolio return, $R_b$ is the benchmark return, $\alpha=E[R_p - R_b]$ is the expected value of the active return, and $\omega=\sigma$ is the standard deviation of the active return, which is an alternate definition of the aforementioned tracking error. 

%     \item Maximum Drawdown (MDD):
    
%     $MDD=max_{t\in(0,T)} (\frac{PV_{peak,t}-PV_t}{PV_{peak,t}}), $ where $PV_t$ is the current portfolio value at time $t$, and $PV_{peak,t}$ is the highest portfolio value observed up to time $t$.

% \end{enumerate}




% \begin{table}[htbp]
%     \centering
%     \caption{Comparison of Different Methods}
%     \begin{tabular}{lcccc}
%         \hline
%         Method & IC & ICIR & RankIC & RankICIR \\
%         \hline
%         GP~\cite{} & 0.0289 & 0.2287 & 0.0623 & 0.4515 \\
%         AFF~\cite{} & 0.0290 & 0.2413 & 0.0525 & 0.4179 \\
%         Method 3 & & & & \\
%         Method 4 & & & & \\
%         Method 5 & & & & \\
%         \hline
%     \end{tabular}
%     \label{tab:comparison}
% \end{table}







% \begin{enumerate}
%     \item \textbf{LSTM (Long Short-Term Memory)}:
%     LSTM is a recurrent neural network designed to capture long-term dependencies in sequential data, making it ideal for time series forecasting in financial markets.
%     \item \textbf{StockMixer}:
%     StockMixer is a simplified multilayer perceptron(MLP) based architecture that combines multiple strategies to balance accuracy and stability, making predictions by integrating indicator mixing, time series mixing, and interstock relationship mixing.
%     \item \textbf{LightGBM}:
%     LightGBM is a gradient boosting framework optimized for speed and performance, particularly effective for structured data and feature-rich datasets.
% \end{enumerate}




\begin{figure*}[ht]
\center
\includegraphics[width=1\textwidth]{figures/ex_return.pdf}
\vspace{-10pt}
\caption{Cumulative Excess Returns of different approaches on CSI 500 and S\&P 500.} 
\label{fig:ex_return}
\vspace{-10pt}
\end{figure*}

\subsection{Overall Performance}
In Table \ref{tab:performance}, we show a comparison of the overall performance of the different methods for four years, from January 1, 2021, to December 31, 2024, in the CSI 500 (China) and S\&P 500 (U.S.) stock markets. Regarding AlphaForge, RD-Agent, Deepseek-R1, OpenAI-o1, and AlphaAgent, we apply their respective output alphas directly in this comparative analysis. For reasoning models Deepseek-R1 and OpenAI-o1, we streamline the task prompt of AlphaAgent to facilitate alpha mining and evolution across five iterative rounds, with each round providing corresponding backtesting results using the identical configuration of AlphaAgent. For RD-Agent and AlphaAgent, we conduct 20 independent trials, with each trial comprising five evolutionary rounds. In each trial, we insert the optimal factors into their respective alpha zoos, yielding the optimal combination as the final result. The performance of each method is evaluated by five key metrics: IC, ICIR, AR, IR, and MDD. Bold numbers indicate the best-performing methodology in each dataset, while underlined numbers represent sub-optimal performance.

Over the four-year period from 2021 to 2024, AlphaAgent consistently outperformed other models across all key metrics in both markets. For predictive power, it achieved the highest IC (0.0212) and ICIR (0.1938) in CSI 500 and similarly led in S\&P 500 with IC of 0.0056 and ICIR of 0.0552. In terms of returns, AlphaAgent generated the best annualized returns of 11.00\% in CSI 500 and 8.74\% in S\&P 500, significantly ahead of the second-best performers (LSTM's 4.96\% and Deepseek-R1's 2.75\% respectively). The model also demonstrated superior risk management, maintaining MDDs below 10\% in both markets (-9.36\% in CSI 500 and -9.10\% in S\&P 500), while achieving the highest risk-adjusted returns with IRs of 1.488 and 1.0545 respectively.


% Figure \ref{fig:ex_return} (a) and (b) illustrate the cumulative excess returns of different models in the CSI 500 and S\&P 500 markets from 2021 to 2024. The analysis reveals distinct patterns of alpha decay across different models and market environments. In the CSI 500 market, the time series models (LSTM and Transformer) demonstrate second-tier overall performance, but show no significant excess returns before 2023 while experiencing notable drawdown for Transformer after 2024-08. AlphaAgent maintains relatively stable performance on a quarterly basis, consistently achieving positive excess returns that reach approximately 45\% by the end of the testing period. \red{In the S\&P 500 market, the alpha decay phenomenon is pronounced across most models, with AlphaAgent being a notable exception. While delivering impressive excess returns exceeding 35\%, AlphaAgent demonstrates remarkable resilience by sustaining its alpha effectiveness throughout the entire testing period. In contrast, other models exhibit severe alpha decay, particularly evident after 2023-02.} The time series forecasting models Transformer and LSTM show severe performance deterioration, with their excess returns remaining consistently negative since 2023-02. "When employing non-sequential models like LightGBM with Alpha158, the cumulative excess returns oscillate around zero, indicating an almost complete loss of alpha generation capability. This stark contrast in alpha persistence between the CSI 500 and S\&P 500 markets suggests that maintaining consistent alpha generation is notably more challenging in the more efficient U.S. market environment, though AlphaAgent successfully overcomes this challenge. DeepSeek-R1, despite its strong reasoning capabilities in factor evolution through multi-round feedback, suffers from a lack of systematic constraints, leading to diminishing factor effectiveness as shown by its declining performance after 2023.

Figure \ref{fig:ex_return} (a) and (b) illustrate the cumulative excess returns of different models in the CSI 500 and S\&P 500 markets from 2021 to 2024, revealing distinct patterns of alpha decay across different models and market environments. The time series models (LSTM and Transformer) exhibit second-tier performance in the CSI 500 market with no significant excess returns before 2023 while suffering severe decay in the S\&P 500 market with consistently negative returns since 2023-02, particularly evident in Transformer's notable drawdown after 2024-08. In contrast, AlphaAgent demonstrates remarkable resilience in both markets, maintaining relatively stable quarterly performance with approximately 45\% cumulative excess returns in CSI 500 and exceeding 37\% in S\&P 500 throughout the testing period. Non-sequential models like LightGBM with Alpha158 show complete alpha decay in the S\&P 500 market as their cumulative excess returns oscillate around zero, while DeepSeek-R1's performance declines after 2023 despite its strong reasoning capabilities, suggesting a lack of systematic constraints. This stark contrast in alpha persistence between models and markets indicates that maintaining consistent alpha generation is notably more challenging in the more efficient U.S. market environment, though AlphaAgent successfully overcomes this challenge.

% Figure \ref{fig:ex_return} (a) and (b) illustrate the cumulative excess returns of different models in the CSI 500 and S\&P 500 markets from 2021 to 2024. The orange line representing AlphaAgent shows consistently superior and stable growth in both markets. In the CSI 500 market, AlphaAgent's curve demonstrates steady upward momentum, reaching approximately 45\% cumulative excess return by the end of 2024, while maintaining a smoother trajectory compared to other models. Similarly, in the S\&P 500 market, AlphaAgent achieves over 35\% cumulative excess return with notably less volatility. In contrast, other approaches show varied performance patterns. Notably, in the CSI 500 market, LSTM achieves the second-best cumulative excess return of about 34\% with a relatively stable upward trend (green line), while Transformer (blue line) demonstrates strong momentum in the latter half of 2023, reaching about 22\% cumulative excess return by the end of 2024 despite higher volatility. However, both models exhibit larger drawdowns compared to AlphaAgent, particularly during market fluctuations from 2021-07 to 2022-07. In the S\&P 500 market, other models, including the promising Deepseek-R1, experience significant drawdowns after 2023-01, with most of them failing to maintain positive excess returns through 2024, highlighting the challenging market conditions during this period. 




\subsection{Alpha Decay Analysis}
\label{subsec:alpha_decay}

Figure \ref{fig:ic_ric_CSI 500} compares the yearly performance between Alpha158~\cite{yang2020qlibaiorientedquantitativeinvestment}, GP~\cite{lin2019revisiting}, a technical indicator RSI, and AlphaAgent's alphas over 5 years on CSI 500. Alpha158, GP, and RSI all exhibit substantial declines in predictive power, with their ICs dropping from 0.022-0.036 to near zero and RankICs decreasing from 0.020-0.042 to around zero, highlighting the widespread challenge of alpha decay in the Chinese stock market. In contrast, AlphaAgent's alphas demonstrate remarkable stability, maintaining predictive effectiveness with IC values consistently around 0.02 and RankIC values around 0.025 throughout the period. This contrast highlights the superior sustainability of AlphaAgent compared to traditional factors, which exhibit stronger signs of alpha decay.

% On S\&P 500, as shown in Fig. \ref{fig:ic_ric_sp500}, Alpha360~\cite{yang2020qlibaiorientedquantitativeinvestment} exhibits a near-complete loss of predictive power over the five-year period, with its IC fluctuating close to zero and even dipping into negative values (around -0.005) in some years. Notably, Alpha360 shows a brief resurgence in 2021, reaching an IC of approximately 0.01, but this anomaly quickly dissipates in subsequent years. Its RankIC follows a similar pattern, hovering around 0 except for a minor uptick in 2021. In stark contrast, AlphaAgent maintains consistent performance, with IC values stabilizing between 0.00 and 0.01 and RankIC remaining near 0.005 across all years except 2021. While the absolute metrics are lower compared to the CSI 500 results in Fig. \ref{fig:ic_ric_CSI 500}, AlphaAgent’s stability in the volatile S\&P 500 environment underscores its adaptive capability. This divergence highlights AlphaAgent’s resilience against alpha decay, even in markets where traditional factors like Alpha360 fail to retain long-term efficacy.

This analysis reveals fundamental differences between how traditional alphas and our approach face modern financial market challenges. GP's rapid performance deterioration suggests potential overfitting to historical patterns, making it particularly vulnerable to changing market conditions. Meanwhile, the significant performance degradation of Alpha158 and RSI Indicator exemplifies the "alpha decay" phenomenon described in Sec. \ref{sec:intro}, to a great extent, stemming from market crowding as these strategies become widely adopted by investors. When multiple market participants simultaneously execute similar trading strategies based on the same factors, their collective actions can diminish the factors' predictive power. In contrast, AlphaAgent's sustained performance suggests that its factor modeling mechanism works to explore sustained and less exploited alphas, effectively mitigating the crowding effect that plagues traditional approaches. 

\begin{figure}[h!]
\center
\includegraphics[width=0.45\textwidth]{figures/yearly_ic_ric_CSI500.pdf}
\vspace{-10pt}
\caption{Yearly IC and RankIC comparison on CSI 500. While other factors' predictive power rapidly decays over time, 15 alphas mined by AlphaAgent maintain stable performance.} 
\label{fig:ic_ric_CSI 500}
\vspace{-10pt}
\end{figure}




\subsection{Alpha Mining Efficiency Analysis}
\label{subsec:vs_rdagent}



This subsection conducts an analysis that focuses on the quality of generated alpha factors and the computational efficiency of the generation process. Figure~\ref{fig:ic_std} illustrates the evolution of IC values across five rounds for AlphaAgent and RD-Agent on CSI 500's test split. First, the results show that RD-Agent exhibits relatively stable and smaller variance (shown by the consistent width of its shaded region), likely due to its lack of exploration incentives for LLM-based agents, indicating more homogeneous factor candidates. Such candidates lead to factor crowding and accelerated alpha decay as similar signals become widely exploited in the market. On the contrary, AlphaAgent consistently maintains higher average IC values compared to RD-Agent throughout all rounds, demonstrating the superior predictive power of its generated factors gained from complexity and hypothesis-alignment constraints against overfitting and financial rationality of the generated factors. A notable observation is an increasing variance (shown by the expanding shaded region) of AlphaAgent's IC values as the rounds progress, suggesting its factor diversity and potentially a wider exploration space brought by its originality penalties, which could lead to a higher probability of discovering effective factors. AlphaAgent's originality penalties drive broad exploration of the factor space, while these complementary mechanisms ultimately contribute to consistently higher average IC values.




\begin{figure}[!h]
\center
\includegraphics[width=0.4\textwidth]{figures/ic_std.pdf}
\vspace{-10pt}
\caption{The evolution of the IC values in the first five rounds for AlphaAgent and RD-Agent. } 
\label{fig:ic_std}
\vspace{-5pt}
\end{figure}


% \begin{figure}[h]
% \center
% \includegraphics[width=0.45\textwidth]{figures/yearly_ic_ric_sp500.pdf}
% \caption{Yearly IC and RankIC comparison between Alpha360 and alphas mined by AlphaAgent during five years on S\&P 500. As shown, Alpha360 nearly loses predictive power except for 2021, AlphaAgent maintains a relatively stable predictive power over five years. } 
% \label{fig:ic_ric_sp500}
% \vspace{-10pt}
% \end{figure}



\subsection{Ablation Study}

In Figure ~\ref{fig:eff}, we ablate AlphaAgent's core components, i.e., factor modeling constraints and symbolic assembly, across three key metrics. This evaluation is based on 100 rounds of evolution, evenly split between the CSI 500 and S\&P 500 markets. The hit ratio measures the proportion of generated alphas achieving exceptional returns per round (>4.0\% annualized for CSI 500 and >1.8\% for S\&P 500, representing the top 5\% of all generated alphas). The dev success rate captures the percentage of factors that can be successfully executed without any code defects or numerical errors. Token efficiency quantifies the number of viable factor candidates generated per token, with the higher efficiency normalized to 1.0 while maintaining relative proportions.

Our ablation studies demonstrate AlphaAgent's effectiveness across these three metrics. In terms of hit ratio, AlphaAgent achieves 0.29 compared to 0.16 without factor modeling constraints, representing an 81\% improvement. This substantial increase indicates that incorporating constraints significantly enhances the quality of generated alpha factors, more likely to mine sustained and predictive alphas. The development success rate comparison (0.83 vs 0.75) evaluates the impact of symbolic assembly - it releases development difficulties by reducing potential code defects and numerical errors. When examining token efficiency, AlphaAgent achieves higher efficiency (1.00) versus 0.81 without symbolic assembly, demonstrating a 23\% improvement in generating viable factor candidates per token. These results collectively validate that our proposed mechanisms effectively improve factor generation across multiple dimensions while relaxing computational overhead.


\vspace{-5pt}
\begin{figure}[!h]
\center
\includegraphics[width=0.48\textwidth]{figures/hit_ratio_comparison.pdf}
\vspace{-15pt}
\caption{A comparison of RD-Agent and AlphaAgent regarding hit ratio, development success rate, and token efficiency.} 
\label{fig:eff}
\vspace{-10pt}
\end{figure}
% 
% In Figure ~\ref{fig:eff}, we evaluate the alpha mining efficiency of RD-Agent and AlphaAgent across three key metrics. This evaluation is based on 100 rounds of evolution, evenly split between the CSI 500 and S\&P 500 markets. The hit ratio measures the proportion of generated alphas achieving exceptional returns per round (>4.0\% annualized for CSI 500 and >1.8\% for S\&P 500, representing the top 5\% of all generated alphas). The dev success rate captures the percentage of factors that can be successfully executed without any code defects or numerical errors. Token efficiency quantifies the number of viable factor candidates generated per token, with the higher efficiency normalized to 1.0 while maintaining relative proportions. 

% Our empirical results demonstrate that AlphaAgent exhibits superior performance across all three key efficiency metrics. Most notably, AlphaAgent achieves a hit ratio of 0.29, representing an 81\% improvement over RD-Agent's 0.16. This substantial increase indicates AlphaAgent's enhanced capability in mining profitable alpha factors. In terms of development robustness, AlphaAgent maintains a higher development success rate of 0.83 compared to RD-Agent's 0.75, suggesting better reliability in integrating symbolic assembly and LLM agents. Furthermore, AlphaAgent's token efficiency is approximately 23\% higher than RD-Agent. This improved efficiency primarily stems from more effective symbolic assembly in the factor construction process, meanwhile indicating that our interpretability and complexity control mechanisms do not introduce additional computational overhead in factor generation. 





\subsection{Implications}
The results shown in Sec. \ref{subsec:alpha_decay} and Sec. \ref{subsec:vs_rdagent} underscore a critical insight into modern quantitative finance: the imperative for alpha mining methods to possess continuous exploration capability beyond fitting historical patterns and relying on established theories. This is particularly evident in today's highly efficient markets where traditional statistical arbitrage strategies face diminishing returns due to increased competition and market adaptation. Our experiments demonstrate that approaches incorporating active exploration mechanisms can better identify and capitalize on emerging market inefficiencies before they dissipate in rapidly evolving markets. With the emergence of LLMs and their growing application in financial analysis, the landscape of alpha mining is undergoing fundamental changes, requiring approaches to be more adaptive and exploratory than ever before. These findings highlight the need for future quantitative trading research to focus on advanced exploration frameworks that balance the exploitation of known patterns with the exploration of novel alpha sources, particularly in an era where AI-driven market analysis is becoming increasingly prevalent. 

% \begin{table*}[h!]
% \centering
% \scriptsize
% \begin{tabularx}{\textwidth}{llX}
% \toprule
% \textbf{Category} & \textbf{Name} & \textbf{Expression} \\
% \midrule

% \multirow{2}{*}{Momentum} 
%                 & Volume-Leading Factor & $0.4 \times \text{RANK}(\text{CORR}(\text{\$volume}, \text{\$close}, 10) + 0.3 \times \text{RANK}(\text{SUM}(\text{\$close}, 5)) + 0.3 \times \text{RANK}(\text{SUM}(\text{\$volume}, 5))$ \\
%                 & Dynamic-Short-Term-Factor & $0.3 \times \text{RANK}(\text{SUM}(\text{\$close}, 5)) + 0.4 \times \text{RANK}(\text{ABS}(\text{\$close} - \text{\$open})) + 0.3 \times \text{RANK}(\text{SUM}(\text{\$volume}, 5))$ \\
%                 & Regret-Aversion Price-Momentum Factor & $\text{ZSCORE}\left(0.4 \times \frac{\text{ABS}(\text{\$close} - \text{\$open})}{\text{\$close}} + 0.3 \times \text{RANK}\left(\frac{\text{DELTA}(\text{\$close}, 1)}{\text{\$close}}\right) + 0.3 \times \left(\frac{\text{SUMIF}(\text{DELAY}(\text{\$close}, 3) - \text{\$close}, 5, \text{\$close} > \text{DELAY}(\text{\$close}, 3))}{\text{COUNT}(\text{\$close} > \text{DELAY}(\text{\$close}, 3), 5)}\right)\right)$ \\
                
% \multirow{2}{*}{Liquidity} 
%                 & Intraday Money Flow Index & $\text{RANK}\left(\left(\frac{1}{4}\left(\text{\$high} - \text{\$low} + 2 \times \text{\$close}\right) \times \text{\$volume}\right) + 1e^{-8}\right)$ \\
%                 & Enhanced-Ensemble-Factor & $\text{ZSCORE}\left(\text{SUM}(\text{\$close}, 10)\right) \times \text{ZSCORE}\left(\text{SUM}(\text{\$volume}, 20)\right)$ \\
                          
% \multirow{3}{*}{Volatility} 
%                 & Volatility-ADX Hybrid Factor & $\text{ZSCORE}\left(\frac{\text{STD}(\text{return}, 20)}{\text{MEAN}(\text{true\_range}, 14) + 1e^{-8}} \times \frac{\text{SMA}\left(\text{MAX}\left(\text{\$high} - \text{DELAY}(\text{\$high}, 1), \text{\$low} - \text{DELAY}(\text{\$low}, 1)\right), 14, 1\right) - \text{SMA}\left(\text{ABS}(\text{\$high} - \text{\$low}), 14, 1\right)}{\text{SMA}\left(\text{MAX}\left(\text{\$high} - \text{DELAY}(\text{\$high}, 1), \text{\$low} - \text{DELAY}(\text{\$low}, 1)\right), 14, 1\right) + 1e^{-8}}\right)$ \\

%                 & Historical-Volatility-Volume-Price-Sentiment Factor & $\frac{1}{5}(\text{RANK}(\text{STD}(\text{\$close}, 10)) + \text{RANK}(\text{STD}(\text{\$volume}, 10)) + \text{RANK}(\text{ABS}(\text{\$close} - \text{\$open})) + \text{RANK}(\text{ABS}(\text{\$high} - \text{\$low})) + \text{RANK}(\text{CORR}(\text{\$close}, \text{\$volume}, 10)))$ \\
%                 & Ensemble-Linear-Nonlinear-Factor & $0.3 \times \text{ZSCORE}(\text{REGBETA}(\text{\$close}, \text{\$open}, 10)) + 0.5 \times \text{ZSCORE}(\text{REGRESI}(\text{\$high}, \text{\$low}, 15)) + 0.2 \times \text{ZSCORE}(\text{EMA}(\text{\$volume}, 20)) + \text{RANK}(\text{\$close})$ \\



% \multirow{2}{*}{Technical Factor} 
%                 & Candlestick-Pattern-Rank Factor & $\text{RANK}\left((\text{\$close} - \text{\$open})                                      \times (\text{\$high} - \text{\$low})\right)$ \\
%                 & Ensemble-Learning-Factor & $\text{RANK}(\text{REGRESI}(\text{\$open}, \text{\$close}, 10)) + \text{ZSCORE}(\text{EMA}(\text{\$high} - \text{\$low}, 15))$ \\
                

% \multirow{3}{*}{Hybrid}  
%                 & Sentiment-Macro-Technical-Factor-Enhanced-V4 & $\text{RANK}(\text{DELTA}(\text{\$close}, 1)) \times \text{ZSCORE}(\text{SUM}(\text{\$volume}, 10)) + \text{STD}(\text{\$close}, 20) \times \text{SUMIF}(\text{\$volume}, 5, \text{\$close} > \text{\$open})$ \\
%                 & Enhanced-Ensemble-Factor & $\text{ZSCORE}(\text{SUM}(\text{\$close}, 10)) \times \text{ZSCORE}(\text{SUM}(\text{\$volume}, 20))$ \\
%                 & Dynamic Price-Momentum-Liquidity Factor & $\text{RANK}(\text{SUM}(\text{EMA}(\text{\$close}, 10), 5)) \times \text{ZSCORE}(\text{SUM}(\text{\$volume}, 20)) + \text{RANK}(\text{SUMIF}(\text{\$close}, 10, \text{\$close} > \text{EMA}(\text{\$close}, 20))) \times \text{ZSCORE}(\text{SUM}(\text{\$volume}, 10))$ \\
%                 & Enhanced Multi-Dimensional Technical Factor & $\frac{1}{7}(\text{RANK}(\text{\$close}) + \text{RANK}(\text{\$volume}) + \text{RANK}(\text{\$high} - \text{\$low}) + \text{RANK}(\text{EMA}(\text{\$close}, 30)) + \text{RANK}(\text{RSI}(\text{\$close}, 14)) + \text{RANK}(\text{MAX}(\text{\$volume}, 10)) + \text{RANK}(\text{STD}(\text{\$close}, 20)))$ \\
%                 & Hybrid-Factor-Price-Volume-Sentiment & $\text{ZSCORE}(\text{\$open}) \times \text{ZSCORE}(\text{\$close}) \times \text{ZSCORE}(\text{\$high}) \times \text{ZSCORE}(\text{\$low}) \times \text{ZSCORE}(\text{\$volume})$ \\

% \midrule

% \midrule

% \bottomrule
% \end{tabularx}
% \caption{An example set of formulaic alphas mined by AlphaAgent across five categories.}
% \label{tab:factors}
% \end{table*}




\section{Conclusion}
This paper introduces AlphaAgent, a novel LLM-driven framework that effectively counteracts the critical challenge of alpha decay with three key regularization mechanisms: originality enforcement, complexity control, and hypothesis alignment. By incorporating these mechanisms into an autonomous framework, AlphaAgent produces decay-resistant and performant alpha factors while maintaining theoretical soundness, achieving substantial excess returns with remarkable consistency and decay resistance through various market conditions. The framework also suggests a promising direction for the next-generation alpha mining framework that swiftly adapts to market evolution while maintaining alpha sustainability. 



% In this paper, we presented AlphaAgent, a novel autonomous framework designed to generate original and performant trading alphas. By uniquely integrating novelty enforcement, hypothesis-factor alignment, and complexity control, AlphaAgent effectively addresses the critical challenge of balancing alpha originality and financial soundness, thus mitigating alpha decay. Rigorous evaluations across both Chinese and U.S. equity markets have unequivocally demonstrated the effectiveness of AlphaAgent in producing factors with both strong predictive capabilities and enduring performance.  





\newpage
\bibliographystyle{ACM-Reference-Format}
\bibliography{reference}


\newpage
\appendix


% \begin{table}[t!]
% \centering
% \begin{tabular}{p{3cm}p{6cm}}
% \toprule
% \textbf{Category} & \textbf{Expression} \\
% \midrule
% \multirow{5}{*}{Volatility} 
%     & $\text{ABS}(\text{\$close} - \text{\$open}) / \text{\$open} \times \text{RANK}(\text{STD}(\text{\$volume}, 5)$ \\
%     & $(\text{BB\_UPPER}(\text{\$close}, 20) - \text{BB\_LOWER}(\text{\$close}, 20)) / (\text{EMA}(\text{\$high} - \text{\$low}, 15) + 1e^{-8})$ \\
%     & $(\text{ABS}(\text{\$high} - \text{\$low})) / (\text{STD}(\text{\$close}, 20) + 1e^{-7})$ \\
%     & $\text{ZSCORE}(\text{STD}(\text{\$high} - \text{\$low}, 20))$ \\
% \addlinespace[0.5em]
% \multirow{3}{*}{Momentum}
%     & $(\text{EMA}(\text{\$close}, 5) - \text{EMA}(\text{\$close}, 20)) \times \text{DELTA}(\text{EMA}(\text{\$volume}, 10), 10)$ \\
%     & $\text{RANK}(\text{SUM}(\text{REGRESI}(\text{SEQUENCE}(5), \text{\$close}, 5), 5))$ \\
%     & $0.3 \times \text{RANK}(\text{SUM}(\text{\$close}, 5)) + 0.4 \times \text{RANK}(\text{ABS}(\text{\$close} - \text{\$open})) + 0.3 \times \text{RANK}(\text{SUM}(\text{\$volume}, 5))$ \\
% \addlinespace[0.5em]
% \multirow{2}{*}{\makecell[l]{Price-Volume\\Correlation}}
%     & $\text{RANK}(\text{CORR}(\text{\$close}, \text{\$volume}, 20)) \times \text{RANK}(\text{SUM}(\text{\$close}, 30))$ \\
%     & $\text{CORR}(\text{DELTA}(\text{\$volume}, 3), \text{STD}(\text{\$volume}, 3), 3)$ \\
% \addlinespace[0.5em]
% \multirow{2}{*}{Technical}
%     & $1 - {\text{\$high}} / (\text{TS\_MAX}(\text{\$high}, 20) + 1e^{-8})$ \\
%     & $(\text{\$close} - \text{TS\_MIN}(\text{\$low}, 14)) / (\text{TS\_MAX}(\text{\$high}, 14) - \text{TS\_MIN}(\text{\$low}, 14) + 1e^{-8}) \times 100$ \\
% \addlinespace[0.5em]
% \multirow{2}{*}{Composite}
%     & $\text{ZSCORE}(\text{\$open}) \times \text{ZSCORE}(\text{\$close}) \times \text{ZSCORE}(\text{\$high}) \times \text{ZSCORE}(\text{\$low}) \times \text{ZSCORE}(\text{\$volume})$ \\
%     & $\text{ZSCORE}(\text{\$close} - \text{TS\_MIN}(\text{\$low}, 20))$ \\
% \bottomrule
% \end{tabular}
% \caption{Example alpha factors generated by AlphaAgent for CSI 500 across five categories}
% \label{tab:factors}
% \end{table}


% \begin{table*}[ht!]
% \centering
% \scriptsize
% \begin{tabularx}{\textwidth}{llX}
% \toprule
% \textbf{Category} & \textbf{Name} & \textbf{Expression} \\
% \midrule

% \multirow{6}{*}{Volatility}
%     % & Volatility-Momentum-ADX Hybrid & $\frac{\text{STD}(\text{\$return}, 20)}{\text{MEAN}(\text{\$true\_range}, 14) + 1e^{-8}} \times \frac{\text{SMA}(\text{MAX}(\text{\$high} - \text{DELAY}(\text{\$high}, 1), \text{\$low} - \text{DELAY}(\text{\$low}, 1)), 14) - \text{SMA}(\text{ABS}(\text{\$high} - \text{\$low}), 14)}{\text{SMA}(\text{MAX}(\text{\$high} - \text{DELAY}(\text{\$high}, 1), \text{\$low} - \text{DELAY}(\text{\$low}, 1)), 14) + 1e^{-8}}$ \\
%     & Price Range-Volume Volatility & $\text{ABS}(\text{\$close} - \text{\$open}) / \text{\$open} \times \text{RANK}(\text{STD}(\text{\$volume}, 5)$ \\
%     & Bollinger Range Volatility & $(\text{BB\_UPPER}(\text{\$close}, 20) - \text{BB\_LOWER}(\text{\$close}, 20)) / (\text{EMA}(\text{\$high} - \text{\$low}, 15) + 1e^{-8})$ \\
%     & Range-Price Volatility Ratio & $(\text{ABS}(\text{\$high} - \text{\$low})) / (\text{STD}(\text{\$close}, 20) + 1e^{-7})$ \\
%     & Range Volatility Z-Score & $\text{ZSCORE}(\text{STD}(\text{\$high} - \text{\$low}, 20))$ \\

% \multirow{3}{*}{Momentum} 
%     & Price-Volume EMA Momentum & $(\text{EMA}(\text{\$close}, 5) - \text{EMA}(\text{\$close}, 20)) \times \text{DELTA}(\text{EMA}(\text{\$volume}, 10), 10)$ \\
%     & Price Regression Momentum & $\text{RANK}(\text{SUM}(\text{REGRESI}(\text{SEQUENCE}(5), \text{\$close}, 5), 5))$ \\
%     & Weighted Price-Volume Momentum & $0.3 \times \text{RANK}(\text{SUM}(\text{\$close}, 5)) + 0.4 \times \text{RANK}(\text{ABS}(\text{\$close} - \text{\$open})) + 0.3 \times \text{RANK}(\text{SUM}(\text{\$volume}, 5))$ \\

% \multirow{2}{*}{Price-Volume Correlation} 
%     & Price-Volume Correlation Momentum & $\text{RANK}(\text{CORR}(\text{\$close}, \text{\$volume}, 20)) \times \text{RANK}(\text{SUM}(\text{\$close}, 30))$ \\
%     & Volume Change Correlation & $\text{CORR}(\text{DELTA}(\text{\$volume}, 3), \text{STD}(\text{\$volume}, 3), 3)$ \\

% \multirow{2}{*}{Technical} 
%     & High Price Drawdown & $1 - {\text{\$high}} / (\text{TS\_MAX}(\text{\$high}, 20) + 1e^{-8})$ \\
%     & Stochastic Range Oscillator & $(\text{\$close} - \text{TS\_MIN}(\text{\$low}, 14)) / (\text{TS\_MAX}(\text{\$high}, 14) - \text{TS\_MIN}(\text{\$low}, 14) + 1e^{-8}) \times 100$ \\


% \multirow{2}{*}{Composite} 
%     & OHLCV Z-Score Product & $\text{ZSCORE}(\text{\$open}) \times \text{ZSCORE}(\text{\$close}) \times \text{ZSCORE}(\text{\$high}) \times \text{ZSCORE}(\text{\$low}) \times \text{ZSCORE}(\text{\$volume})$ \\
%     & Price-Low Range Z-Score & $\text{ZSCORE}(\text{\$close} - \text{TS\_MIN}(\text{\$low}, 20))$ \\
%     % & Composite Price-Volume Volatility Rank & $\frac{1}{5}(\text{RANK}(\text{STD}(\text{\$close}, 10)) + \text{RANK}(\text{STD}(\text{\$volume}, 10)) + \text{RANK}(\text{ABS}(\text{\$close} - \text{\$open})) + \text{RANK}(\text{ABS}(\text{\$high} - \text{\$low})) + \text{RANK}(\text{CORR}(\text{\$close}, \text{\$volume}, 10)))$ \\


% \bottomrule
% \end{tabularx}
% \caption{Example alpha factors generated by AlphaAgent for CSI 500 across five categories}
% \label{tab:factors}
% \end{table*}



% \section{Future Direction}
% Moving forward, we envision extending AlphaAgent's capabilities by integrating advanced techniques such as vision-language models (VLMs) for dynamic adaptation and incorporating diverse data streams encompassing news sentiment and macroeconomic indicators to achieve a more holistic understanding of market dynamics. Looking beyond equities, we aspire to expand AlphaAgent’s reach to financial derivatives, including futures, and the burgeoning cryptocurrency markets. To effectively navigate the complexities of these diverse markets, we aim to explore novel architectures, such as those inspired by Mixture of Experts (MoE), ensuring robust and adaptable performance across a wider spectrum of asset classes. Ultimately, we believe AlphaAgent paves the way for a new era of autonomous and intelligent quantitative investment systems, potentially transforming how financial institutions approach algorithmic trading and alpha research.


% \section{Example Factors}


\end{document}
\endinput
%%
%% End of file `sample-sigconf.tex'.
