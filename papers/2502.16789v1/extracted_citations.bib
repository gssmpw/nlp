@inproceedings{10.1145/1830483.1830584,
author = {Schmidt, Michael D. and Lipson, Hod},
title = {Age-fitness pareto optimization},
year = {2010},
isbn = {9781450300728},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1830483.1830584},
doi = {10.1145/1830483.1830584},
abstract = {We propose a multi-objective method for avoiding premature convergence in evolutionary algorithms, and demonstrate a three-fold performance improvement over comparable methods. Previous research has shown that partitioning an evolving population into age groups can greatly improve the ability to identify global optima and avoid converging to local optima. Here, we propose that treating age as an explicit optimization criterion can increase performance even further, with fewer algorithm implementation parameters. The proposed method evolves a population on the two-dimensional Pareto front comprising (a) how long the genotype has been in the population (age); and (b) its performance (fitness). We compare this approach with previous approaches on the Symbolic Regression problem, sweeping the problem difficulty over a range of solution complexities and number of variables. Our results indicate that the multi-objective approach identifies the exact target solution more often that the age-layered population and standard population methods. The multi-objective method also performs better on higher complexity problems and higher dimensional datasets -- finding global optima with less computational effort.},
booktitle = {Proceedings of the 12th Annual Conference on Genetic and Evolutionary Computation},
pages = {543–544},
numpages = {2},
keywords = {symbolic regression, pareto, evolutionary algorithms, age},
location = {Portland, Oregon, USA},
series = {GECCO '10}
}

@inproceedings{alphaevolve,
author = {Cui, Can and Wang, Wei and Zhang, Meihui and Chen, Gang and Luo, Zhaojing and Ooi, Beng Chin},
title = {AlphaEvolve: A Learning Framework to Discover Novel Alphas in Quantitative Investment},
year = {2021},
isbn = {9781450383431},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3448016.3457324},
doi = {10.1145/3448016.3457324},
abstract = {Alphas are stock prediction models capturing trading signals in a stock market. A set of effective alphas can generate weakly correlated high returns to diversify the risk. Existing alphas can be categorized into two classes: Formulaic alphas are simple algebraic expressions of scalar features, and thus can generalize well and be mined into a weakly correlated set. Machine learning alphas are data-driven models over vector and matrix features. They are more predictive than formulaic alphas, but are too complex to mine into a weakly correlated set. In this paper, we introduce a new class of alphas to model scalar, vector, and matrix features which possess the strengths of these two existing classes. The new alphas predict returns with high accuracy and can be mined into a weakly correlated set. In addition, we propose a novel alpha mining framework based on AutoML, called AlphaEvolve, to generate the new alphas. To this end, we first propose operators for generating the new alphas and selectively injecting relational domain knowledge to model the relations between stocks. We then accelerate the alpha mining by proposing a pruning technique for redundant alphas. Experiments show that AlphaEvolve can evolve initial alphas into the new alphas with high returns and weak correlations.},
booktitle = {Proceedings of the 2021 International Conference on Management of Data},
pages = {2208–2216},
numpages = {9},
keywords = {search algorithm, stock prediction},
location = {Virtual Event, China},
series = {SIGMOD '21}
}

@inproceedings{alphagen,
author = {Yu, Shuo and Xue, Hongyan and Ao, Xiang and Pan, Feiyang and He, Jia and Tu, Dandan and He, Qing},
title = {Generating Synergistic Formulaic Alpha Collections via Reinforcement Learning},
year = {2023},
isbn = {9798400701030},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3580305.3599831},
doi = {10.1145/3580305.3599831},
abstract = {In the field of quantitative trading, it is common practice to transform raw historical stock data into indicative signals for the market trend. Such signals are called alpha factors. Alphas in formula forms are more interpretable and thus favored by practitioners concerned with risk. In practice, a set of formulaic alphas is often used together for better modeling precision, so we need to find synergistic formulaic alpha sets that work well together. However, most traditional alpha generators mine alphas one by one separately, overlooking the fact that the alphas would be combined later. In this paper, we propose a new alpha-mining framework that prioritizes mining a synergistic set of alphas, i.e., it directly uses the performance of the downstream combination model to optimize the alpha generator. Our framework also leverages the strong exploratory capabilities of reinforcement learning (RL) to better explore the vast search space of formulaic alphas. The contribution to the combination models' performance is assigned to be the return used in the RL process. This return drives the alpha generator to find better alphas that improve upon the current set. Experimental evaluations on real-world stock market data demonstrate both the effectiveness and the efficiency of our framework for stock trend forecasting. The investment simulation results show that our framework is able to achieve higher returns compared to previous approaches.},
booktitle = {Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {5476–5486},
numpages = {11},
keywords = {computational finance, reinforcement learning, stock trend forecasting},
location = {Long Beach, CA, USA},
series = {KDD '23}
}

@misc{autogpt,
author = {Significant Gravitas},
title = {AutoGPT},
month = February,
year = 2024,
url = {https://github.com/Significant-Gravitas/AutoGPT},
note = {original-date: 2023-03-16T09:21:07Z}
}

@article{fama1992cross,
  title={The cross-section of expected stock returns},
  author={Fama, Eugene F and French, Kenneth R},
  journal={the Journal of Finance},
  volume={47},
  number={2},
  pages={427--465},
  year={1992},
  publisher={Wiley Online Library}
}

@inproceedings{finrl,
author = {Liu, Xiao-Yang and Yang, Hongyang and Gao, Jiechao and Wang, Christina Dan},
title = {FinRL: deep reinforcement learning framework to automate trading in quantitative finance},
year = {2022},
isbn = {9781450391481},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3490354.3494366},
doi = {10.1145/3490354.3494366},
abstract = {Deep reinforcement learning (DRL) has been envisioned to have a competitive edge in quantitative finance. However, there is a steep development curve for quantitative traders to obtain an agent that automatically positions to win in the market, namely to decide where to trade, at what price and what quantity, due to the error-prone programming and arduous debugging. In this paper, we present the first open-source framework FinRL as a full pipeline to help quantitative traders overcome the steep learning curve. FinRL is featured with simplicity, applicability and extensibility under the key principles, full-stack framework, customization, reproducibility and hands-on tutoring.Embodied as a three-layer architecture with modular structures, FinRL implements fine-tuned state-of-the-art DRL algorithms and common reward functions, while alleviating the debugging workloads. Thus, we help users pipeline the strategy design at a high turnover rate. At multiple levels of time granularity, FinRL simulates various markets as training environments using historical data and live trading APIs. Being highly extensible, FinRL reserves a set of user-import interfaces and incorporates trading constraints such as market friction, market liquidity and investor's risk-aversion. Moreover, serving as practitioners' stepping stones, typical trading tasks are provided as step-by-step tutorials, e.g., stock trading, portfolio allocation, cryptocurrency trading, etc.},
booktitle = {Proceedings of the Second ACM International Conference on AI in Finance},
articleno = {1},
numpages = {9},
keywords = {automated trading, deep reinforcement learning, markov decision process, portfolio allocation, quantitative finance},
location = {Virtual Event},
series = {ICAIF '21}
}

@inproceedings{li-etal-2024-large-language,
    title = "Can Large Language Models Mine Interpretable Financial Factors More Effectively? A Neural-Symbolic Factor Mining Agent Model",
    author = "Li, Zhiwei  and
      Song, Ran  and
      Sun, Caihong  and
      Xu, Wei  and
      Yu, Zhengtao  and
      Wen, Ji-Rong",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2024",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.findings-acl.233/",
    doi = "10.18653/v1/2024.findings-acl.233",
    pages = "3891--3902",
}

@techreport{lin2019revisiting,
 title={Revisiting Stock Alpha Mining Based On Genetic Algorithm},
 author={Lin, X. and Chen, Y. and Li, Z. and He, K.},
 year={2019},
 institution={Huatai Securities Research Center},
 url={https://crm.htsc.com},
 type={Technical Report}
}

@article{patil2023ai,
  title={AI-Infused algorithmic trading: genetic algorithms and machine learning in high-frequency trading},
  author={Patil, RR},
  journal={International Journal for MultidiscMining profitable alpha factors via convolution kernel learningiplinary Research},
  volume={5},
  number={5},
  year={2023}
}

@misc{ppo,
      title={Proximal Policy Optimization Algorithms}, 
      author={John Schulman and Filip Wolski and Prafulla Dhariwal and Alec Radford and Oleg Klimov},
      year={2017},
      eprint={1707.06347},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1707.06347}, 
}

@article{shen2023mining,
  title={Mining profitable alpha factors via convolution kernel learning},
  author={Shen, Zhenyi and Mao, Xiahong and Yang, Xiaohu and Zhao, Dan},
  journal={Applied Intelligence},
  volume={53},
  number={23},
  pages={28460--28478},
  year={2023},
  publisher={Springer}
}

@misc{shi2024alphaforgeframeworkdynamicallycombine,
      title={AlphaForge: A Framework to Mine and Dynamically Combine Formulaic Alpha Factors}, 
      author={Hao Shi and Weili Song and Xinting Zhang and Jiahe Shi and Cuicui Luo and Xiang Ao and Hamid Arian and Luis Seco},
      year={2024},
      eprint={2406.18394},
      archivePrefix={arXiv},
      primaryClass={q-fin.CP},
      url={https://arxiv.org/abs/2406.18394}, 
}

@misc{sumers2024cognitivearchitectureslanguageagents,
      title={Cognitive Architectures for Language Agents}, 
      author={Theodore R. Sumers and Shunyu Yao and Karthik Narasimhan and Thomas L. Griffiths},
      year={2024},
      eprint={2309.02427},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2309.02427}, 
}

@misc{wang2023aligninglargelanguagemodels,
      title={Aligning Large Language Models with Human: A Survey}, 
      author={Yufei Wang and Wanjun Zhong and Liangyou Li and Fei Mi and Xingshan Zeng and Wenyong Huang and Lifeng Shang and Xin Jiang and Qun Liu},
      year={2023},
      eprint={2307.12966},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2307.12966}, 
}

@misc{wang2024llmfactorextractingprofitablefactors,
      title={LLMFactor: Extracting Profitable Factors through Prompts for Explainable Stock Movement Prediction}, 
      author={Meiyun Wang and Kiyoshi Izumi and Hiroki Sakaji},
      year={2024},
      eprint={2406.10811},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2406.10811}, 
}

@article{weng2023agent,
  title   = "LLM-powered Autonomous Agents",
  author  = "Weng, Lilian",
  journal = "lilianweng.github.io",
  year    = "2023",
  month   = "Jun",
  url     = "https://lilianweng.github.io/posts/2023-06-23-agent/"
}

@misc{yang2024collaborative,
    title={Collaborative Evolving Strategy for Automatic Data-Centric Development},
    author={Xu Yang and Haotian Chen and Wenjun Feng and Haoxue Wang and Zeqi Ye and Xinjie Shen and Xiao Yang and Shizhao Sun and Weiqing Liu and Jiang Bian},
    year={2024},
    eprint={2407.18690},
    archivePrefix={arXiv},
    primaryClass={cs.AI}
}

@misc{zhang2020autoalphaefficienthierarchicalevolutionary,
      title={AutoAlpha: an Efficient Hierarchical Evolutionary Algorithm for Mining Alpha Factors in Quantitative Investment}, 
      author={Tianping Zhang and Yuanqi Li and Yifei Jin and Jian Li},
      year={2020},
      eprint={2002.08245},
      archivePrefix={arXiv},
      primaryClass={q-fin.CP},
      url={https://arxiv.org/abs/2002.08245}, 
}

@misc{zhao2024quantfactorreinforceminingsteady,
      title={QuantFactor REINFORCE: Mining Steady Formulaic Alpha Factors with Variance-bounded REINFORCE}, 
      author={Junjie Zhao and Chengxi Zhang and Min Qin and Peng Yang},
      year={2024},
      eprint={2409.05144},
      archivePrefix={arXiv},
      primaryClass={q-fin.CP},
      url={https://arxiv.org/abs/2409.05144}, 
}

@inproceedings{zhaofan2022genetic,
  title={Genetic algorithm based quantitative factors construction},
  author={Zhaofan, Su and Jianwu, Lin and Chengshan, Zhang},
  booktitle={2022 IEEE 20th International Conference on Industrial Informatics (INDIN)},
  pages={650--655},
  year={2022},
  organization={IEEE}
}

@misc{zhu2024largelanguagemodelslearn,
      title={Large Language Models can Learn Rules}, 
      author={Zhaocheng Zhu and Yuan Xue and Xinyun Chen and Denny Zhou and Jian Tang and Dale Schuurmans and Hanjun Dai},
      year={2024},
      eprint={2310.07064},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2310.07064}, 
}

