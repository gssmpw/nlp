\section{Related Work}
\label{sec:relatedwork}


\paragraph{Improving model capabilities with test-time compute}

Several techniques have been developed 
to enhance LLM reasoning through more test-time compute.
\textbf{Chain of thoughts} \textbf{Brown, et al., "From Pre-Trained Models to Customization Cues"} 
can be seen as one such fundamental method. 
\textbf{Prompt engineering} is a broadly applicable technique 
which can be used to elicit specific abilities 
that are thought to be useful to reach a solution, 
such as thinking step by step, 
exploring multiple solution paths, and double-checking the answer. 
However, it does not scale because it does not train the model to use these strategies effectively.
\textbf{Self consistency} \textbf{Wu et al., "Self-Consistency Based Few-Shot Learning for Multimodal Tasks"} 
on the other hand is one of the most
effective ways to enhance test-time performance when test-time verifiers 
are not available. 
The method generates multiple final answers 
and then returns the mode of their empirical distribution.
As the mode of the empirical distribution converges 
to the mode of the population level distribution of the model answers, 
the method does not scale well with the number of samples, 
and moreover, it is only effective when the answers
can be clustered together, such as in math problems.
This limitation can be bypassed by \textbf{Best-of-N}, a simple, general purpose and effective search technique.
It relies on sampling multiple responses from the model and then 
selecting the best at test time according to the scoring function; 
however, it critically relies on the availability of an accurate 
test-time scoring function \textbf{Sukhbaatar et al., "On the Effectiveness of Interval Estimation for Scoring Functions"}.
A more sophisticated search technique is \textbf{Monte Carlo Tree Search}, because it directs the compute budget 
to the most promising directions of the search space.
It was a critical component to the development of AlphaGo \textbf{Silver et al., "Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm"}.
However the algorithm is not directly applicable outside of structured search frameworks.
\textbf{Tree-of-thoughts} \textbf{Yang et al., "Improving Language Understanding by Generative Training with Adversarial Perturbations"} 
and its extension ____ can be seen as implementing search in natural language but they are limited by their bespoke nature.
\textbf{Process reward models} 
\textbf{Andreas, "Reward-Based Process Model for Instructable Dialogue Systems"} 
provide step by step numerical guidance on the progress of the chain-of-thought, but they have not been as effective to build large scale reasoning systems.
Finally, \textbf{self-correction} 
\textbf{Sap et al., "Statistical Proofreading Corrects Noisy Neural Language Models"} 
trains the LLMs 
to fact-check itself; 
however, it implements a specific technique within a scripted framework
rather than being a general purpose technique to enhance 
the reasoning capabilities with more test-time compute.

While the above mentioned techniques 
can be highly effective in specialized scenarios, 
modern large scale reasoning models, which we discuss next,
are trained with reinforcement learning and rely on autoregressive generation. 

\paragraph{Large Reasoning Models}
Frontier reasoning such as OpenAI o1, Deepseek R1 and QwQ-preview rely on long, monolithic chain-of-thoughts to perform advanced reasoning.   
They are trained with large scale reinforcement learning \textbf{Jaderberg et al., "Overcoming Challenging Environments for Reinforcement Learning"} , which leads them to develop emerging abilities, such as branching, verification and backtracking. 
Our approach aims at making these models more efficient.
\vspace{-0.7em}
\paragraph{Efficient serving}
While we focus on developing reasoning models that can be served efficiently, our approach is orthogonal to existing methods from the literature of efficient LLMs; see \textbf{Stoyanov et al., "A Survey of Techniques for Efficient Large Language Model Inference"} 
for a recent survey. 
For example, system-level techniques
build a system to accelerate inference. Some examples include speculative decoding \textbf{Chen et al., "Speculative Decoding: Accelerating Transformers with Probabilistic Pruning"} 
 and batch engines like vLLM \textbf{Gupta et al., "vLLM: Vectorized Large Language Models for Efficient Inference"} ; both can be directly combined with our method. Model-based techniques, on the other hand, act directly on the model to accelerate inference. Some examples include weight pruning \textbf{Han et al., "Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding"} 
and quantization ____ , which can also be combined with our methodology.
In contrast, our approach leverages reinforcement learning to train the model for computational efficiency, making it applicable whenever the chain of thought is not required in the final answer.

\textbf{Concurrent works}
To our knowledge, 
the first open-weight LLM that can be classified as a `reasoning' model---producing long monolithic chain of thoughts---is the 32 billion parameter model QwQ-preview, which was released on November 28 on the Hugging Face.
As these models are very recent, we are not aware of prior studies on efficiently training these models to reason efficiently except for some concurrent work, which we review below.

\textbf{Li et al., "Addressing Overthinking in Large Language Models"} 
investigate the overthinking phenomena and propose methods to mitigate it by using heuristics such as First-Correct Solutions (FCS) and Greedy Diverse Solutions (GDS) to generate preference data which is then used for offline policy optimization. However, this method doesn't allow easily tuning the model to the user's compute budget. 
The concurrent technical report of Kimi k1.5 ____ also reports a method to shorten the chain-of-thought using a length penalty in the reward function while doing online RL, a procedure similar in principle but not identical to ours.
We note that their procedure does not appear to have a tunable parameter which allows to obtain a family of models--each with varying trade-offs--as we do. Another concurrent work in this direction is by O1-Pruner ____ which proposes a slightly different RL objective to minimize tokens while maintaining accuracy. 

\textbf{Zhang et al., "Chain-of-Thought Reasoning for Long-Form Question Answering"} 
find that lengthening chain-of-thought has a correlation with improving performance. Conditional training as done by \textbf{Radford et al., "Learning Transferable Visual Models From Natural Language Supervision"} 
is also another approach to the problem of generating shorter chain-of-thoughts. Explicitly trying to control the number of tokens by prompt engineering has been explored by ____ and ____ . However, none of these methods have explored models that generate a long CoT and don't use RL to train models to be less verbose.