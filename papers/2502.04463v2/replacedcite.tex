\section{Related Work}
\label{sec:relatedwork}


\paragraph{Improving model capabilities with test-time compute}

Several techniques have been developed 
to enhance LLM reasoning through more test-time compute.
\textbf{Chain of thoughts} ____ 
can be seen as one such fundamental method. 
\textbf{Prompt engineering} is a broadly applicable technique ____
 which can be used to elicit specific abilities 
that are thought to be useful to reach a solution, 
such as thinking step by step, 
exploring multiple solution paths, and double-checking the answer. 
However, it does not scale because it does not train the model to use these strategies effectively.
\textbf{Self consistency} ____ on the other hand is one of the most
effective ways to enhance test-time performance when test-time verifiers 
are not available. 
The method generates multiple final answers 
and then returns the mode of their empirical distribution.
As the mode of the empirical distribution converges 
to the mode of the population level distribution of the model answers, 
the method does not scale well with the number of samples, 
and moreover, it is only effective when the answers
can be clustered together, such as in math problems.
This limitation can be bypassed by \textbf{Best-of-N}, a simple, general purpose and effective search technique.
It relies on sampling multiple responses from the model and then 
selecting the best at test time according to the scoring function; 
however, it critically relies on the availability of an accurate 
test-time scoring function ____.
A more sophisticated search technique is \textbf{Monte Carlo Tree Search}, because it directs the compute budget 
to the most promising directions of the search space.
It was a critical component to the development of AlphaGo ____.
However the algorithm is not directly applicable outside of structured search frameworks.
\textbf{Tree-of-thoughts} ____ and its extension ____
can be seen as implementing search in natural language but they are limited by their bespoke nature.
\textbf{Process reward models} 
____ provide step by step numerical guidance on the progress of the chain-of-thought, but they have not been as effective to build large scale reasoning systems.
Finally, \textbf{self-correction} 
____
trains the LLMs 
to fact-check itself; 
however, it implements a specific technique within a scripted framework
rather than being a general purpose technique to enhance 
the reasoning capabilities with more test-time compute.

While the above mentioned techniques 
can be highly effective in specialized scenarios, 
modern large scale reasoning models, which we discuss next,
are trained with reinforcement learning and rely on autoregressive generation. 

\paragraph{Large Reasoning Models}
Frontier reasoning such as OpenAI o1, Deepseek R1 and QwQ-preview rely on long, monolithic chain-of-thoughts to perform advanced reasoning.   
They are trained with large scale reinforcement learning ____, which leads them to develop emerging abilities, such as branching, verification and backtracking. 
Our approach aims at making these models more efficient.
\vspace{-0.7em}
\paragraph{Efficient serving}
While we focus on developing reasoning models that can be served efficiently, our approach is orthogonal to existing methods from the literature of efficient LLMs; see ____ for a recent survey. 
For example, system-level techniques
build a system to accelerate inference. Some examples include speculative decoding ____ and batch engines like vLLM ____; both can be directly combined with our method. Model-based techniques, on the other hand, act directly on the model to accelerate inference. Some examples include weight pruning ____ and quantization ____, which can also be combined with our methodology.
In contrast, our approach leverages reinforcement learning to train the model for computational efficiency, making it applicable whenever the chain of thought is not required in the final answer.

\textbf{Concurrent works}
To our knowledge, 
the first open-weight LLM that can be classified as a `reasoning' model---producing long monolithic chain of thoughts---is the 32 billion parameter model QwQ-preview, which was released on November 28 on the Hugging Face.
As these models are very recent, we are not aware of prior studies on efficiently training these models to reason efficiently except for some concurrent work, which we review below.

____ investigate the overthinking phenomena and propose methods to mitigate it by using heuristics such as First-Correct Solutions (FCS) and Greedy Diverse Solutions (GDS) to generate preference data which is then used for offline policy optimization. However, this method doesn't allow easily tuning the model to the user's compute budget. 
The concurrent technical report of Kimi k1.5 ____ also reports a method to shorten the chain-of-thought using a length penalty in the reward function while doing online RL, a procedure similar in principle but not identical to ours.
We note that their procedure does not appear to have a tunable parameter which allows to obtain a family of models--each with varying trade-offs--as we do. Another concurrent work in this direction is by O1-Pruner ____ which proposes a slightly different RL objective to minimize tokens while maintaining accuracy. 

\textbf{Efficiency of Chain-of-Thought} ____ find that lengthening chain-of-thought has a correlation with improving performance. Conditional training as done by ____ is also another approach to the problem of generating shorter chain-of-thoughts. Explicitly trying to control the number of tokens by prompt engineering has been explored by ____ and ____. However, none of these methods have explored models that generate a long CoT and don't use RL to train models to be less verbose.