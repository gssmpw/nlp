\section{Introduction}
Large language models (LLMs) have made significant advancements by pre-training larger models with extensive datasets \citep{kaplan2020scaling}, but this approach faces diminishing returns due to limited high-quality training data.
An alternative to improve model capabilities, especially in domains involving careful reasoning, involves allowing models to ``think" before answering, as seen in frontier reasoning models like OpenAI's o1, Gemini 2.0 Flash Thinking Experimental, and DeepSeek-R1 \citep{guo2025deepseek}. These models produce intermediate tokens during inference, collectively referred to as \emph{chain-of-thoughts} \citep{wei2022chain}, to perform additional computations before returning an answer. The process of generating a long chain of thought before answering the user query is called \emph{reasoning}.
More precisely, \emph{large reasoning models} with chain-of-thoughts capable of performing advanced reasoning emerge from reinforcement learning (RL) \citep{sutton2018reinforcement, guo2025deepseek} on base models using ground-truth scoring functions (e.g., correctness on math problems). 

\begin{figure}[t]
\centering
\includegraphics[width=0.5\textwidth]{images/master_summary.pdf}
\vspace{-2em}
  \caption{Our procedure allows to derive a family of reasoning models, each with different trade-offs between accuracy and token-efficiency, as a function a scalar parameter $\alpha$. The picture shows accuracy and number of tokens averaged over GSM8K, MATH500 and AIME2024, normalized to that of the original reasoning model. The original reasoning model is  DeepSeek-R1-Distill-Qwen-7B and is labeled  as Full Reasoning. The Instruct model refers to Qwen2.5-Math-7B-Instruct.}
  \label{fig:master_summary}
\end{figure}


\begin{figure*}[h!]
\centering
\includegraphics[width=\textwidth]{images/summary.pdf}
\vspace{-2em}
  \caption{Our procedure trains models to be more token-efficient on easier problems, such as GSM8K, while preserving accuracy on harder problems, such as AIME2024. Full Reasoning refers to the  reasoning model DeepSeek-R1-Distill-Qwen-7B.}
  \label{fig:summary}
\end{figure*}

These reasoning models use test-time compute in the form of very long chain-of-thoughts, an approach that commands a high inference cost due to the quadratic cost of the attention mechanism and linear growth of the KV cache for transformer-based architectures \cite{vaswani2017attention}. However, effective deployment of LLMs demands models that are not only capable but also computationally efficient to serve. 
Even for resource-rich organizations such as large tech companies that have the resources to train reasoning models, excessive inference costs may mean operating at a loss rather than at a profit in order to match the competitor's offering.
Furthermore, reducing inference costs often reduces latency, improves responsiveness, and therefore increases user experience. Finally, lowering the inference computation has a direct impact in reducing carbon emissions, with a positive benefit to both the environment and the society.

We aim to develop a procedure to \emph{train} the model to use the appropriate amount of inference time compute to solve the problem at hand with reasoning.
For straightforward problems, the resulting model would deliver efficient, direct solutions, while for more demanding tasks, it would invest additional computational effort to perform advanced reasoning. Such an adaptable model, that invests the minimum amount of compute to arrive at the correct solution, would be a significant leap forward in terms of operational cost.

We  use reinforcement learning policy gradient methods \citep{sutton2018reinforcement} to train the model to use the least possible amount of tokens to reach the correct solution, thereby minimizing inference costs, ideally without compromising on accuracy. 
We achieve this goal by means of a modified reinforcement learning formulation which \emph{encourages the model to produce correct answers with short chain-of-thoughts}.
To the best of our knowledge, we are among the first to consider training the model to be efficient at inference time, and
we discuss concurrent as well as related  literature in \cref{sec:relatedwork}. 
As a result, the model learns when to stop thinkingâ€”rather than solving an easy math problem, such as simple addition, through multiple approaches, it recognizes when it has found the correct answer and concludes its reasoning efficiently while maintaining accuracy.

In order to achieve this goal, we provide a reinforcement learning implementation of the above principle which involves only a couple of line changes in a standard reinforcement learning pipeline; this allows to directly leverage existing RL codebases.
Our method allows the user to  \emph{control} the reduction in inference-time compute by adjusting a scalar coefficient in an intuitive way. In other words, 
starting from a base reasoning model, our procedure allows to derive a \emph{family of reasoning models}, each with increased generation efficiency (i.e., shorter chain-of-thoughts) compared to the original reasoning model. 

We perform numerical experiments on two recently released open-weight large reasoning models, DeepSeek-R1-Distill-Qwen-1.5B and DeepSeek-R1-Distill-Qwen-7B \cite{guo2025deepseek}
and derive models with a substantial problem-dependent reduction in reasoning cost while approximately maintaining accuracy, see \cref{fig:summary} for a summary of our results.
For the 7B model, 
our method produces a model with a reduction of 16\% tokens on the competition-level benchmark American Invitational Mathematics Examination 2024 while slightly increasing accuracy, and a reduction of 30\% with a small reduction in accuracy of 1\% on the MATH dataset \cite{hendrycks2021measuring}, and a reduction of approximately 50\% tokens on GSM8K \cite{cobbe2021training} with similar accuracy, thereby showing the ability of the model to dynamically reduce its test-time compute budget with minimal loss in accuracy.



Beyond its simplicity, an attractive property of our approach is its \emph{computational efficiency}:
although training reasoning models
with large scale reinforcement learning 
may have a prohibitive cost \cite{guo2025deepseek},
our procedure shows that training them to reason efficiently is highly viable
even with modest academic resources: 
our models are obtained with only 100 reinforcement learning steps (approximately 200 gradient updates).
The fact that we achieve a performance comparable to that of the original reasoning model with a short training is surprising, because in few RL steps the model needs to  optimize for reasoning patterns that are shorter and more token-efficient than the original model. 

Our code and data is available at \url{https://github.com/Zanette-Labs/efficient-reasoning}.

\section{Related Work}
\label{sec:relatedwork}


\paragraph{Improving model capabilities with test-time compute}

Several techniques have been developed 
to enhance LLM reasoning through more test-time compute.
\textbf{Chain of thoughts} \citep{wei2022chain} 
can be seen as one such fundamental method. 
\textbf{Prompt engineering} is a broadly applicable technique \citep{white2023prompt}
 which can be used to elicit specific abilities 
that are thought to be useful to reach a solution, 
such as thinking step by step, 
exploring multiple solution paths, and double-checking the answer. 
However, it does not scale because it does not train the model to use these strategies effectively.
\textbf{Self consistency} \citep{wang2022self} on the other hand is one of the most
effective ways to enhance test-time performance when test-time verifiers 
are not available. 
The method generates multiple final answers 
and then returns the mode of their empirical distribution.
As the mode of the empirical distribution converges 
to the mode of the population level distribution of the model answers, 
the method does not scale well with the number of samples, 
and moreover, it is only effective when the answers
can be clustered together, such as in math problems.
This limitation can be bypassed by \textbf{Best-of-N}, a simple, general purpose and effective search technique.
It relies on sampling multiple responses from the model and then 
selecting the best at test time according to the scoring function; 
however, it critically relies on the availability of an accurate 
test-time scoring function \citep{gao2023scaling}.
A more sophisticated search technique is \textbf{Monte Carlo Tree Search}, because it directs the compute budget 
to the most promising directions of the search space.
It was a critical component to the development of AlphaGo \cite{silver2017mastering}.
However the algorithm is not directly applicable outside of structured search frameworks.
\textbf{Tree-of-thoughts} \citep{yao2024tree} and its extension \citep{gandhi2024stream, besta2024graph}
can be seen as implementing search in natural language but they are limited by their bespoke nature.
\textbf{Process reward models} 
\cite{lightman2024lets} provide step by step numerical guidance on the progress of the chain-of-thought, but they have not been as effective to build large scale reasoning systems.
Finally, \textbf{self-correction} 
\citep{kumar2024training}
trains the LLMs 
to fact-check itself; 
however, it implements a specific technique within a scripted framework
rather than being a general purpose technique to enhance 
the reasoning capabilities with more test-time compute.

While the above mentioned techniques 
can be highly effective in specialized scenarios, 
modern large scale reasoning models, which we discuss next,
are trained with reinforcement learning and rely on autoregressive generation. 

\paragraph{Large Reasoning Models}
Frontier reasoning such as OpenAI o1, Deepseek R1 and QwQ-preview rely on long, monolithic chain-of-thoughts to perform advanced reasoning.   
They are trained with large scale reinforcement learning \cite{guo2025deepseek}, which leads them to develop emerging abilities, such as branching, verification and backtracking. 
Our approach aims at making these models more efficient.
\vspace{-0.7em}
\paragraph{Efficient serving}
While we focus on developing reasoning models that can be served efficiently, our approach is orthogonal to existing methods from the literature of efficient LLMs; see \citet{zhou2024surveyefficientinferencelarge} for a recent survey. 
For example, system-level techniques
build a system to accelerate inference. Some examples include speculative decoding \cite{leviathan2023fast} and batch engines like vLLM \cite{kwon2023efficient}; both can be directly combined with our method. Model-based techniques, on the other hand, act directly on the model to accelerate inference. Some examples include weight pruning \cite{liu2018rethinking} and quantization \cite{lin2024awq}, which can also be combined with our methodology.
In contrast, our approach leverages reinforcement learning to train the model for computational efficiency, making it applicable whenever the chain of thought is not required in the final answer.

\textbf{Concurrent works}
To our knowledge, 
the first open-weight LLM that can be classified as a `reasoning' model---producing long monolithic chain of thoughts---is the 32 billion parameter model QwQ-preview, which was released on November 28 on the Hugging Face.
As these models are very recent, we are not aware of prior studies on efficiently training these models to reason efficiently except for some concurrent work, which we review below.

\citet{chen2024think23overthinkingo1like} investigate the overthinking phenomena and propose methods to mitigate it by using heuristics such as First-Correct Solutions (FCS) and Greedy Diverse Solutions (GDS) to generate preference data which is then used for offline policy optimization. However, this method doesn't allow easily tuning the model to the user's compute budget. 
The concurrent technical report of Kimi k1.5 \cite{kimiteam2025kimik15scalingreinforcement} also reports a method to shorten the chain-of-thought using a length penalty in the reward function while doing online RL, a procedure similar in principle but not identical to ours.
We note that their procedure does not appear to have a tunable parameter which allows to obtain a family of models--each with varying trade-offs--as we do. Another concurrent work in this direction is by O1-Pruner \cite{luo2025o1prunerlengthharmonizingfinetuningo1like} which proposes a slightly different RL objective to minimize tokens while maintaining accuracy. 

\textbf{Efficiency of Chain-of-Thought} \citet{jin-etal-2024-impact} find that lengthening chain-of-thought has a correlation with improving performance. Conditional training as done by \citet{kang2024c3otgeneratingshorterchainofthought} is also another approach to the problem of generating shorter chain-of-thoughts. Explicitly trying to control the number of tokens by prompt engineering has been explored by \citet{nayab2025concisethoughtsimpactoutput} and \citet{han2024tokenbudgetawarellmreasoning}. However, none of these methods have explored models that generate a long CoT and don't use RL to train models to be less verbose. 


\section{Setup}
\label{sec:setup}

\def\p{\ensuremath{p}}
\def\x{\ensuremath{x}}
\def\y{\ensuremath{y}}
\def\cot{\ensuremath{c}}
\def\answer{\ensuremath{y}}
\def\t{\ensuremath{t}}
\def\reward{\ensuremath{r}}
\def\dist{\ensuremath{\rho}}
\def\E{\mathbb E}
\def\f{\ensuremath{f}}
\def\rshape{g}
\def\len{\textsc{len}}
\def\1{{1}}
\def\goldanswer{\answer^\star}
\def\coeff{\alpha}
\def\std{\textsc{std}}
\def\mean{\textsc{mean}}
\def\accuracy{\textsc{Accuracy}}

Let $\p$ be a language model. When provided with a prompt $\x$, the language model produces a response $\y = (\y^1,\y^2,...,\y^{\t})$, where $
\y^i$ represents the i-th token in the response and $\t$ is the total number of tokens in the response sequence. More precisely, the generation is \emph{auto-regressive}, meaning that given the prompt $\x$ and the tokens $\y^{\leq k} = (\y^1,\y^2,...,\y^{k})$ generated so far, the next token $\y^{k+1}$ is generated from the conditional model 
\begin{align}
\label{eqn:auto-regressive}
\y^{k+1} \sim \p(\cdot \mid \x, \y^{\leq k} ).
    % \tag{auto-regressive generation}
\end{align}
The auto-regressive generation stops when the language model $\p$ outputs the end-of-sequence (EOS) token. 
Therefore, if $\y = (\y^1,\y^2,...,\y^{\t})$ is a full response,
$\y^{\t}$ is always the EOS token. 
With a little abuse of notation, we also let $\y \sim \p(\cdot \mid \x)$  denote the process of  sampling the full response $\y = (\y^1,\y^2,...,\y^{\t})$ from the model $\p$ via auto-regressive sampling according to \cref{eqn:auto-regressive}. 

\paragraph{Chain-of-Thoughts}
Chain of thoughts, introduced by \citep{wei2022chain}, 
is a key framework to implement reasoning. 
Given a prompt \x{}, the LLM is said to produce a ``chain of thoughts'' 
when it produces intermediate tokens that are not part of the output
before generating the final answer in an autoregressive way. 
Typically, the final answer 
is not formally separated from the chain-of-thoughts, 
and so we let $\answer$ denote the full output of the model 
$\answer \sim \p(\x)$.

\paragraph{Objective function and reinforcement learning}
We consider problems where the responses generated from an LLM can be
 evaluated by a scoring function $\f(\x,\answer) \mapsto \mathbb R$, 
 often called \emph{reward model} or \emph{verifier}, 
 that measures the suitability of the response. 
For math problems, such as those that we consider in this paper, 
the reward function establishes whether the solution 
to the problem is correct 
\citep{cobbe2021training,hendrycks2021measuring}
\begin{align}
	\f(\x,\answer) 
	=
	\1\{ \answer = \goldanswer(\x) \}
\end{align}
where $\goldanswer(\x)$ is the correct answer to the math problem \x{}.
Since $\answer{}$ is the full output of the model, including the chain of thought, the relation $\answer = \goldanswer(\x)$
tests whether the final answer generated
by the model coincides with the gold answer, 
rather than checking equivalence between strings.

Large reasoning models \cite{guo2025deepseek}
are reportedly trained with reinforcement learning \cite{sutton2018reinforcement}.
When a chain of thoughts is used, 
the objective function to maximize can be written as
\begin{equation}
\label{eqn:RL}
	\accuracy(\p) = \E_{\x \sim \dist} \E_{\answer \sim p(\x{})} \big[ \1\{ \answer = \goldanswer \} \big].
\end{equation}
where  $\dist$ is the prompt distribution.
% 
In the sequel, we simply write $\E$
to denote the expectation.
For math problems, maximizing \cref{eqn:RL} directly  
maximizes the probability that the model correctly solves a random question the prompt distribution.

\section{Method}

\begin{figure*}[h!]
\centering
  \includegraphics[width=\textwidth]{images/pipeline.drawio-3.drawio-2.pdf}
  
  \caption{Pipeline depicting our method. For every prompt, multiple solutions are sampled and rewarded based on correctness and response length. The shortest correct answers are rewarded the highest and the language model is then updated using policy gradients.}
  \label{fig:main_pipeline}
\end{figure*}


We aim to design a method that trains models to use the minimum amount of inference time compute to arrive at the correct answer.
For simpler math problems, such as those in GSM8K \cite{cobbe2021training}, the model should recognize when it has reached the correct solution within a few hundred tokens. In contrast, for competition-level problems like those in the American Invitational Mathematics Examination (AIME), the model should be capable of expending thousands of tokens if that is necessary to find a strategy that solves these exceptionally challenging questions.

One attractive option is to train the model on an objective function derived from \cref{eqn:RL} that \emph{encourages the model to produce correct solutions with the minimum amount of tokens}.
In order to achieve the latter goal, we  penalize the length 
of  the correct responses
\begin{equation}
\label{eqn:lenpen}
    	\E\Big[ 
        \1\{\answer = \goldanswer(\x{}) \}
	( 1 - \coeff f(\len(\answer) ) \Big]
\end{equation}
using a monotonic function $f$ of the input and a tunable parameter $\coeff \in [0, 1)$.
The choice $\coeff = 0$ yields the
reinforcement learning objective \eqref{eqn:RL}; increasing $\coeff$ increases the regularization towards shorter---but correct---responses.

In order to ensure that the length 
regularization is effective,
we first normalize the length of the responses and then use the sigmoid function $\sigma$ to soft-clip it, obtaining
\begin{equation}
f(\len(\x{}))
=
    \sigma \left( \frac{\len(\answer{}) - \mean(\x{})}{\std(\x{})} \right)
\end{equation}
where 
\[
\mean(\x{}) = \mathop{\E}_{\substack{y\sim \p(\x),\\ \text{s.t.} 
\; \1\{\answer = \goldanswer \} = 1 }} \left[\len(\answer)\right]
\]
\[
\std(\x) = \sqrt{\Var_{\substack{\answer\sim \p(\x),\\ \text{s.t.} 
\; \1\{\answer = \goldanswer \} = 1 }}{\left[\len(\answer)\right]}}
\]
are the \emph{per-prompt} mean and standard deviation of the length, 
respectively. 
The per-prompt normalization ensures that longer chains of thought on hard problems are not disproportionately penalized compared to shorter ones on easier problems.
When $\alpha \in [0, 1)$, the sigmoid  ensures that the objective function is always bounded between $[0, 1]$ even for abnormally long or short generations, and that
correct responses, even if long, are always preferred to incorrect ones.
In practice, both the standard deviation and the mean are directly estimated from the rollouts during online training.

\subsection{Optimizing the objective function  with Reinforcement Learning}

Since optimizing \cref{eqn:lenpen} involves sampling from the model auto-regressively,
the objective function is non-differentiable; 
however, it can be optimized with reinforcement learning, 
for instance with policy gradient methods
\citep{sutton2018reinforcement}.

One popular option is proximal policy optimization (PPO) \cite{schulman2017proximalpolicyoptimizationalgorithms} 
which considers the (local) objective function
% , which considers the loss function
\[ 
\min \{ f^t_{\theta}(y, x)\mathcal{A}(y^{<t}, x), \clip_{1-\epsilon}^{1+\epsilon}[ f^t_{\theta}(y, x)]\mathcal{A}(y^{<t}, x) \} \]
defined using the density ratio 
$$
f^t_{\theta}(y, x) = \frac{\pi_\theta(y^{t}|x + y^{<t})}{\pi_{old}(y^{t}|x + y^{<t})}
$$
and for a suitable choice for the advantage estimator $\mathcal{A}(y^{<t}, x)$. Traditionally, in deep reinforcement learning \cite{schulman2017proximalpolicyoptimizationalgorithms} the advantage estimator involves a neural network. 

With language models, maintaining a separate value network to obtain a variance-reduced advantage estimator  \cite{schulman2017proximalpolicyoptimizationalgorithms} may add significant computational and implementation complexity without necessarily increasing performance \cite{kool2019buy, ahmadian2024basicsrevisitingreinforcestyle}.
One simple and effective alternative is to just estimate the advantage using  Monte Carlo (MC) as proposed by \cite{kool2019buy, ahmadian2024basicsrevisitingreinforcestyle}. 
% Tapproach which is possible because we sample multiple rollout responses $\{\answer_i\}$ for the same prompt $\x$.
Such estimator is also called REINFORCE Leave One Out (RLOO) estimator. 
To be precise, the trajectory advantage can be estimated as

\[
\mathcal{A}(\answer_i, x)=\mathcal{R}(\answer_i, x) - \frac{1}{n-1}\sum_{j\neq i}\mathcal{R}(y_j, x)
\]
where $\mathcal{R}$ is the trajectory return and $\answer_i$ is the $i$ generation for prompt $x$.
We then simply use the sequence level advantage as the token level advantage, namely $\mathcal{A}(y^{<t}, x) = \mathcal{A}(y, x)$. In essence, we use PPO with the RLOO advantage estimator.

\subsection{Population-level optimality guarantees}
In this section we analyze the population-level maximizer of
\cref{eqn:lenpen} in a highly simplified setup and show how this can lead to the desired behavior of shortening the chain-of-thoughts without compromising accuracy.

Consider the following simplified setup, where the language model $\p_\theta$ conditioned on a prompt $\x{}$ is a multinomial distribution over $N$  possible responses $\answer_1,\dots, \answer_N$.
More precisely, given $|\mathcal X|$ multinomial distributions $\p( \cdot \mid \x{})$ on the prompt space $\mathcal X$, there exists a value of the parameter $\theta$ that realizes such a choice.
\begin{assumption}[Tabular Representation]
\label{asm:realizability}
For every choice of $p$ such that
\begin{align}
  p(\answer_i \mid \x{}) \in [0,1], \quad \forall \x{} \in \mathcal X, i \in [N] \\
  \sum_i p(\answer_i \mid \x{}) = 1, \quad \forall \x{} \in \mathcal X
\end{align}
there exists a $\theta$ such that
\begin{align}
     p_\theta(\answer_i \mid \x{}) = p(\answer_i \mid \x{}), \quad \forall i \in [N], \forall \x{} \in \mathcal X.
\end{align}
\end{assumption}

This assumption can be justified by the expressive power of the neural network. The following assumption ensures coverage, namely that for every prompt, there exists at least a correct response that the LLM can output for an appropriate value of $\theta$.
It encodes the fact that an LLM 
can learn the correct solution if given enough data.
\begin{assumption}[Coverage]
\label{asm:coverage}
For every prompt $\x{} \in \mathcal X$ there exists a response $\answer \in \{\answer_i \}_{i=1}^{N}$ such that 
$\answer = \goldanswer(\x{})$.
\end{assumption}

Let $\p_{\theta^\star}$ denote the reasoning model that is the population level maximizer of the accuracy:
\begin{equation}
\label{eqn:thetastar}
   \theta^\star = \arg\max_{\theta} \E_{\x \sim \dist} \E_{\answer \sim p_\theta(\x{})} \big[ \1\{ \answer = \goldanswer \} \big]
\end{equation}
where $\dist$ is the distribution over the prompts.
From our simplified setup, in particular from  \cref{asm:realizability} and \cref{asm:coverage} it is easy to see that 
\begin{equation}
    \accuracy(\p_{ \theta^\star}) = 1.
\end{equation}
In other words, if the language model has enough expressive power 
that it can cover the correct solution for each of the prompts, maximization of the population level RL training objective \eqref{eqn:thetastar} leads to a model that can output the correct solution over each prompt in the training dataset.

Let $\theta^\star_{eff}$ denote the population-level parameters of the reasoning model obtained by maximizing \cref{eqn:lenpen}, i.e.,
\begin{equation}
   \theta^\star_{eff} = \arg\max_{\theta} \Big\{ \E_{\x \sim \dist} \E_{\answer \sim p_\theta(\x{})} \big[ ( 1 - \coeff f(\len(\answer) ) \big] \Big\}
\end{equation}
for a certain choice of a monotonically increasing function $f(\cdot) \in [0,1]$ and scalar value $\alpha \in [0, 1)$.


We can prove that the population-level maximizer $\p_{\theta^\star_{eff}}$ 
is as accurate as the population-level maximizer  $\p_{\theta^\star}$.
% while producing considerably shorter responses.
\begin{proposition}[Accuracy is Preserved]
\label{prop:accuracy}
    With the setup just described,  
    \begin{equation}
    \accuracy(\p_{\theta^\star_{eff}}) = 1.
    % \accuracy(\p_{\theta^\star}) = 1.
    \end{equation}
\end{proposition}

Notice that our claim is about the population-level maximizers; finite-sample guarantees can be obtained for both parametric and nonparametric models by using standard techniques from statistics \cite{wainwright2019high}. 

Intuitively, the average length is reduced by virtue of our objective function \eqref{eqn:lenpen}, while accuracy is preserved in the idealized setting that we consider.

\section{Experiments}

\begin{figure*}[h!]
\centering
\includegraphics[width=\textwidth]{images/training_dynamics.pdf}
\vspace{-0.3in}
\caption{The figure shows the dynamics of the training accuracy and the corresponding generation lengths with varying values of $\alpha$ for the 7B model. The training accuracy and response length have been smoothed out using running averages over 25 training iterations.} 
\label{fig:training_dynamics}
\end{figure*}


We seek to evaluate our method through numerical experiments.
In particular, we aim to answer the following questions:
\begin{itemize}
    \item What is the trade-off between  accuracy and inference cost?
    \item What are simple relevant baselines in this setting?
\end{itemize}

We first discuss the setup, then introduce some baselines, present the empirical results and associated trade-offs, and finally discuss some ablations.


\subsection{Setup}
\paragraph{Initial unsuccessful experiments}
In our initial experiments, 
we performed distillation from QwQ-32B-Preview to Qwen2.5-3B-Instruct and Qwen2.5-1.5B-Instruct so as to elicit strong reasoning skills in these two models; these distilled models would have served as a starting point for our method.

However, to our surprise, the distilled models showed a regression in performance on common benchmarks such as MATH and AIME 2024 compared to the instruct model,
despite using much longer chain-of-thoughts with qualitatively more advanced reasoning patterns.
Although our method is still effective in reducing the length of the chain-of-thought of the distilled model, these experiments do not accurately reflect the trade-off between inference-cost and accuracy when the instruct model is also taken into account.

\textbf{Models and Datasets}  We revisited our method following the release of the reasoning models DeepSeek-R1-Distill-Qwen-1.5B and DeepSeek-R1-Distill-Qwen-7B \cite{guo2025deepseek}. These models were distilled from the more powerful DeepSeek-R1 using industry-grade techniques. Along with a LLaMA-variant distilled by the same authors \cite{guo2025deepseek}, they are the only open-weight reasoning models of their size. Notably, they demonstrate impressive performance on challenging benchmarks such as AIME 2024.

For post-training the model using our technique, we choose 3.2k prompts from the MATH, cn\_k12, AIME, AoPS and the Olympiad subsets of the Numina Math dataset \cite{numina_math_datasets}. The dataset includes problems that lack an objective answer, such as proof-based questions. We filter out such problems and ensure that the selected training problems have a numerical answer that can be parsed. We use the same dataset across all baselines to ensure consistency.

\paragraph{Evaluation}
We report the training logs and also evaluate the models on three test datasets, ordered by increasing difficulty:

\begin{itemize}
    \item GSM8K \cite{cobbe2021trainingverifierssolvemath}, which contains grade-school-level math problems,
    \item MATH \cite{hendrycks2021measuring} which is a standard benchmark containing harder problems than GSM8K,
    \item The American Invitational Mathematics Examination (AIME) 2024, a competition-level dataset of challenging mathematical problems.
\end{itemize}

For all models, we set the temperature to 0.6 as suggested in the model's card\footnote{\url{https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B}} and set the token limit to 32K. We use vLLM \cite{kwon2023efficientmemorymanagementlarge} for efficient batch inference. 
We use the parser created by the Qwen Team for the evaluation of their models\footnote{\url{https://github.com/QwenLM/Qwen2.5-Math}} to measure correctness.

We report the \emph{average pass rate@k} for all models. Specifically, for each prompt, we sample \( k \) responses and compute the average accuracy per prompt, which is then averaged across the entire dataset. For GSM8K, we set \( k=1 \) due to its large number of test samples. In contrast, for MATH500, we use \( k=3 \), and for AIME2024, we set \( k=10 \) given its limited set of only 30 questions.

\begin{figure*}[ht!]
\centering
  \includegraphics[width=1.0\textwidth]{images/main_figure_MATH500.pdf}
  \vspace{-1em}
  \caption{This figure describes the results on the MATH500 test set where every prompt is evaluated 3 times to compute the average pass rate. The green triangle in the top-left represents the desirable trend where higher accuracy is achieved with a lower number of tokens. Different colors for \emph{vLLM Cutoff} denote different cutoff values. Similarly for \emph{Ours}, different colors denote different values of $\coeff$.}
  \label{fig:main_results_math}
\end{figure*}


\paragraph{Implementation details}
We build on the OpenRLHF codebase \cite{hu2024openrlhfeasytousescalablehighperformance}. For the 1.5B model, we use 4 GH200 GPUs on one low-density node and for the 7B model, we use 8 GH200 GPUs distributed across two low-density nodes (4 GPUs per node). We set vLLM to the maximum context length (32K) during generation and set the generation temperature to 1. For training the 1.5B, ZeRO Stage 2 \cite{rajbhandari2020zeromemoryoptimizationstraining} is used and for the 7B, ZeRO Stage 3 with activation checkpointing is required to prevent out of memory errors. The training precision is set to bfloat16. We generate 8 responses for each prompt. For every iteration, 32 prompts are selected from the dataset and the global batch size is set to 128 which leads to 2 gradient steps per RL iteration. For the 1.5B, the learning rate is set to $5\cdot10^{-6}$ and for the 7B, it is set to $2\cdot10^{-6}$. For all experiments, Adam \cite{kingma2017adammethodstochasticoptimization} is used as the standard optimizer. We experiment with 4 values of $\alpha$ in the following range: $0.05, 0.1, 0.2$ and $0.4$. For all RL experiments, the value of the KL coefficient is set to $1\cdot 10^{-3}$. The experiments on both model take approximately 20 hours. We use the same prompt template for all models which can be found in Appendix \ref{sec:prompt_appendix}.

\subsection{Baselines}
Apart from the concurrent and related work discussed in \cref{sec:relatedwork}, to our knowledge there are no prior studies in this setting.
Alongside our method, we introduce and implement simple baseline approaches that help balance inference cost and accuracy.
\begin{enumerate}
\item \textbf{Generation Cutoff:} This simple baseline imposes a maximum token limit during the vLLM generation. If a response exceeds the token limit and remains incomplete, it is assigned a score of 0. We evaluate token cutoffs at 8,000, 16,000, 20,000, 24,000, and 32,000.
\item \textbf{Rejection Sampling + SFT:} In this baseline, we generate 8 solutions per prompt using the distilled 1.5B and 7B models. From the generated solutions, we select the shortest correct responses. For a dataset of 3,200 prompts, this process yields approximately 2,200 and 2,500 valid responses for the 1.5B and 7B models, respectively. We experiment with three learning rates: \(1 \times 10^{-5}\), \(5 \times 10^{-6}\), and \(2 \times 10^{-6}\). We find that \(5 \times 10^{-6}\) effectively reduces response length in a meaningful way.

\item \textbf{DPO:} Using the same dataset as above, we select response pairs consisting of the longest and shortest correct solutions and apply Direct Preference Optimization (DPO) \cite{rafailov2023direct} on these preference pairs. While other preference optimization algorithms are applicable in this setting, we choose DPO for its popularity and ease of use. Similar to the SFT baseline, we experiment with three learning rates: \(1 \times 10^{-5}\), \(5 \times 10^{-6}\), and \(2 \times 10^{-6}\). We observe that \(1 \times 10^{-5}\) effectively reduces response length, whereas the other rates do not achieve any meaningful reduction. 
\end{enumerate}


\subsection{Results}

We train DeepSeek-R1-Distill-Qwen-1.5B and DeepSeek-R1-Distill-Qwen-7B models using different values of $\coeff \in [0, 0.05, 0.1, 0.2, 0.4]$ to 
illustrate the trade-offs between models with different lengths for the chain-of-thoughts.
We report the aggregate results in \cref{fig:master_summary} and \cref{fig:summary},
we below we discuss the detailed results.

\subsubsection{Performance on the training set}
We first present the performance on the training dataset in \cref{fig:training_dynamics}. Notably, setting $\coeff = 0$ corresponds to applying RL without any length penalty. Increasing $\coeff$ results in a significant reduction in token usageâ€”up to 50\% compared to the initial modelâ€”while maintaining the same level of accuracy as at the beginning of RL training. Lower values of $\coeff$ improve performance while still reducing the number of tokens.

Furthermore, the trade-off behaves monotonically, as expected: decreasing $\coeff$ leads to less token savings but improves accuracy. This allows users to generate a family of models with varying trade-offs simply by adjusting the coefficient $\coeff$.

\subsubsection{Performance on the test sets}
We report the evaluation results on MATH500 in \cref{fig:main_results_math} and on AIME 2024 in \cref{fig:main_results_aime}. The results for GSM8K can be found in Appendix \ref{sec:gsm_appendix} due to space reason. 


As shown in \cref{fig:main_results_math}, our method enables smooth trade-offs of compute cost and accuracy, allowing models to be tailored to the specific requirements of downstream tasks or users based on different values of \( \alpha \). For instance, with \( \alpha = 0.1 \), the length of the chain-of-thought of the 7B model on the MATH dataset decreases by 30\% (from $\sim 4000$ to $\sim 2800$ tokens) while the accuracy loss is only 1\%. Similarly, in the AIME dataset (Figure \ref{fig:main_results_aime}), setting \( \alpha = 0.2 \) reduces token usage by 30\% (from 14,000 to 9,000) while incurring only a 2\% accuracy drop compared to the DeepSeek-R1-Distill-Qwen-7B.


\begin{figure*}[h]
\centering
  \includegraphics[width=1.0\textwidth]{images/main_figure_AIME.pdf}
  \vspace{-1em}
  \caption{This figure describes the results on the AIME2024 test set where every prompt is evaluated 10 times to compute the average pass rate. The green triangle in the top-left represents the desirable trend where higher accuracy is achieved with a lower number of tokens. Different colors for \emph{vLLM Cutoff} denote different cutoff values. Similarly for \emph{Ours}, different colors denote different values of $\coeff$. We omit the Instruct Models here since their performances are quite poor (approximately 10\% for the 1.5B and 13.3\% for the 7B).}
  \label{fig:main_results_aime}
\end{figure*}

We offer several remarks:

\begin{itemize}
    \item Bigger models seem to more 'token-efficient'. For instance, in the distilled models, the 1.5B model achieves a performance of $\sim84.5\%$ and uses $\sim 5200$ tokens. On the other hand, the 7B model achieves a performance of $\sim 93\%$ using only $\sim 4000$ tokens. 
    \item We prompt the original DeepSeek-R1-Distill-Qwen-7B and one of our models about a simple question ``How much is 1+1?''. While DeepSeek-R1-Distill-Qwen-7B reasoning model expends several tokens (more than a page in Appendix \ref{app:1+1}) to arrive at the correct solution, the model trained with our method quickly reaches the same conclusion within few tokens. 
    \item The models trained with our procedures adapt the length of the chain of thought to the difficulty of the problem. For example, $\alpha = 0.2$ brings a token saving of 22\% on AIME2024 and of 77\% on GSM8K compared to doing RL at $\alpha=0$.
    \item Even without any length penalty (i.e., \( \alpha = 0 \)), we observe a reduction in response length on both the MATH and AIME datasets. We hypothesize that this occurs because these models have not been previously trained with reinforcement learning (RL) and have only undergone a single round of distillation from R1. It could also be because problem in the dataset we use are too easy. The exact effect of the effects of the training dataset difficulty is left for future work. 
    \item The SFT and DPO baselines appear to perform worse than early-stopping the vLLM generation. 
    \item The model with the highest \( \alpha = 0.4 \) experiences a larger performance drop compared to the others. In Figure \ref{fig:ckpts}, we visualize the training dynamics by plotting accuracy every 10 RL iterations. The figure illustrates how \( \alpha = 0.4 \) induces a rapid reduction in response length, likely preventing the model from adapting effectively, ultimately leading to lower performance.

\end{itemize}

\subsection{Ablations}


\begin{figure}[h!]
\centering
\includegraphics[width=0.45\textwidth]{images/main_figure_MATH500_1.5B_ckpts.pdf}

\caption{Evolution of the 1.5B model during training with varying values of $\alpha$. Checkpoints are created after every 10 iterations.} 
\label{fig:ckpts}
\end{figure}


We perform an ablation to study a highly critical design component in the implementation of our method, namely the decision of not normalizing the advantage function in the RL training procedure.

In fact, it is a standard practice (e.g., GRPO 
\cite{shao2024deepseekmathpushinglimitsmathematical}) to
normalize the token-level advantage function and obtain 
\[
\hat{A}_{i,t} = \frac{r_i - r_{mean}}{r_{std}}
\]
where $r_{mean}$ is the mean reward and $r_{std}$ is the standard deviation of the rewards.
While this choice is sensible in a more standard setting, it can have unintended consequences when the objective function contains the length penalty.



\begin{figure}[h!]
  \centering
  % \includegraphics[width=0.45\textwidth]{images/normalize.png}
  \includegraphics[width=0.45\textwidth]{images/normalization_effects.pdf}
  % \includegraphics[width=0.5\textwidth]{images/adv_normalize_acc.png}
  \caption{Advantage normalization rapidly decreases the response length alongside accuracy.}
  \label{fig:adv_normalization}
\end{figure}


Consider the case where for a prompt $x$, all responses are correct. In that case, all rewards will be distributed within $[1-\alpha, 1]$. Assume that the reward distribution is uniformly distributed in $[1-\alpha, 1]$. In that case, the mean reward is $1-\frac{\alpha}{2}$ and the standard deviation is $\frac{\alpha}{\sqrt{12}}$.
The normalized advantage value for a correct response with maximum value $r=1$ (i.e., the shortest correct response) becomes $\frac{1-(1-\alpha/2)}{\frac{\alpha}{\sqrt{12}}} = \sqrt{3}$ which is independent of $\alpha$! 
In other words, the advantage normalization, under certain conditions, can bring a length decrease independent of $\alpha$.
The resulting length decrease is generally too substantial for the model to absorb, and this leads to a sharp  drop in accuracy during training, as can be seen in Figure \ref{fig:adv_normalization}.

\section{Limitations}
Our optimization procedure, while effective, is somewhat more involved than SFT or DPO-derived techniques because of the reinforcement learning setup.
Furthermore, the choice of the penalty coefficient $\coeff$ affects the overall generation cost but does not precisely target a precise generation length, which may be required by some latency-constrained applications. We leave such exact controllability as future work.


\section{Conclusion}

In this work, we introduced a novel methodology that significantly reduces the inference cost for reasoning models while minimally affecting its accuracy. 
Our approach is related in spirit to model distillation; however, rather than reducing deployment cost by reducing the model size, we focus on reducing the deployment cost by reducing the inference cost of the same model. 

A key advantage of our framework is its ability to adapt computational resources based on problem difficulty. This suggests that rather than training separate models targeting various  inference-time compute trade-offs, a single model can adjust its inference budget dynamically. This property holds promise for applications requiring scalable, cost-effective AI solutions that are highly efficient without compromising on accuracy. 