\section{Related Work}
\label{sec:relatedwork}


\paragraph{Improving model capabilities with test-time compute}

Several techniques have been developed 
to enhance LLM reasoning through more test-time compute.
\textbf{Chain of thoughts} \citep{wei2022chain} 
can be seen as one such fundamental method. 
\textbf{Prompt engineering} is a broadly applicable technique \citep{white2023prompt}
 which can be used to elicit specific abilities 
that are thought to be useful to reach a solution, 
such as thinking step by step, 
exploring multiple solution paths, and double-checking the answer. 
However, it does not scale because it does not train the model to use these strategies effectively.
\textbf{Self consistency} \citep{wang2022self} on the other hand is one of the most
effective ways to enhance test-time performance when test-time verifiers 
are not available. 
The method generates multiple final answers 
and then returns the mode of their empirical distribution.
As the mode of the empirical distribution converges 
to the mode of the population level distribution of the model answers, 
the method does not scale well with the number of samples, 
and moreover, it is only effective when the answers
can be clustered together, such as in math problems.
This limitation can be bypassed by \textbf{Best-of-N}, a simple, general purpose and effective search technique.
It relies on sampling multiple responses from the model and then 
selecting the best at test time according to the scoring function; 
however, it critically relies on the availability of an accurate 
test-time scoring function \citep{gao2023scaling}.
A more sophisticated search technique is \textbf{Monte Carlo Tree Search}, because it directs the compute budget 
to the most promising directions of the search space.
It was a critical component to the development of AlphaGo \cite{silver2017mastering}.
However the algorithm is not directly applicable outside of structured search frameworks.
\textbf{Tree-of-thoughts} \citep{yao2024tree} and its extension \citep{gandhi2024stream, besta2024graph}
can be seen as implementing search in natural language but they are limited by their bespoke nature.
\textbf{Process reward models} 
\cite{lightman2024lets} provide step by step numerical guidance on the progress of the chain-of-thought, but they have not been as effective to build large scale reasoning systems.
Finally, \textbf{self-correction} 
\citep{kumar2024training}
trains the LLMs 
to fact-check itself; 
however, it implements a specific technique within a scripted framework
rather than being a general purpose technique to enhance 
the reasoning capabilities with more test-time compute.

While the above mentioned techniques 
can be highly effective in specialized scenarios, 
modern large scale reasoning models, which we discuss next,
are trained with reinforcement learning and rely on autoregressive generation. 

\paragraph{Large Reasoning Models}
Frontier reasoning such as OpenAI o1, Deepseek R1 and QwQ-preview rely on long, monolithic chain-of-thoughts to perform advanced reasoning.   
They are trained with large scale reinforcement learning \cite{guo2025deepseek}, which leads them to develop emerging abilities, such as branching, verification and backtracking. 
Our approach aims at making these models more efficient.
\vspace{-0.7em}
\paragraph{Efficient serving}
While we focus on developing reasoning models that can be served efficiently, our approach is orthogonal to existing methods from the literature of efficient LLMs; see \citet{zhou2024surveyefficientinferencelarge} for a recent survey. 
For example, system-level techniques
build a system to accelerate inference. Some examples include speculative decoding \cite{leviathan2023fast} and batch engines like vLLM \cite{kwon2023efficient}; both can be directly combined with our method. Model-based techniques, on the other hand, act directly on the model to accelerate inference. Some examples include weight pruning \cite{liu2018rethinking} and quantization \cite{lin2024awq}, which can also be combined with our methodology.
In contrast, our approach leverages reinforcement learning to train the model for computational efficiency, making it applicable whenever the chain of thought is not required in the final answer.

\textbf{Concurrent works}
To our knowledge, 
the first open-weight LLM that can be classified as a `reasoning' model---producing long monolithic chain of thoughts---is the 32 billion parameter model QwQ-preview, which was released on November 28 on the Hugging Face.
As these models are very recent, we are not aware of prior studies on efficiently training these models to reason efficiently except for some concurrent work, which we review below.

\citet{chen2024think23overthinkingo1like} investigate the overthinking phenomena and propose methods to mitigate it by using heuristics such as First-Correct Solutions (FCS) and Greedy Diverse Solutions (GDS) to generate preference data which is then used for offline policy optimization. However, this method doesn't allow easily tuning the model to the user's compute budget. 
The concurrent technical report of Kimi k1.5 \cite{kimiteam2025kimik15scalingreinforcement} also reports a method to shorten the chain-of-thought using a length penalty in the reward function while doing online RL, a procedure similar in principle but not identical to ours.
We note that their procedure does not appear to have a tunable parameter which allows to obtain a family of models--each with varying trade-offs--as we do. Another concurrent work in this direction is by O1-Pruner \cite{luo2025o1prunerlengthharmonizingfinetuningo1like} which proposes a slightly different RL objective to minimize tokens while maintaining accuracy. 

\textbf{Efficiency of Chain-of-Thought} \citet{jin-etal-2024-impact} find that lengthening chain-of-thought has a correlation with improving performance. Conditional training as done by \citet{kang2024c3otgeneratingshorterchainofthought} is also another approach to the problem of generating shorter chain-of-thoughts. Explicitly trying to control the number of tokens by prompt engineering has been explored by \citet{nayab2025concisethoughtsimpactoutput} and \citet{han2024tokenbudgetawarellmreasoning}. However, none of these methods have explored models that generate a long CoT and don't use RL to train models to be less verbose.