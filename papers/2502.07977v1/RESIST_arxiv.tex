%***************
\documentclass[draftcls, onecolumn]{IEEEtran}  
%\documentclass{IEEEtran}

% Packages
\usepackage[dvipsnames]{xcolor} % Extended color support
\usepackage[normalem]{ulem} % Underlined and strikethrough text
\usepackage[citecolor=blue]{hyperref} % Adds hyperlinks with citations in blue color
\usepackage{amsmath, amsfonts, amssymb, amsthm, mathabx} % Core math utilities and additional symbols
\usepackage{threeparttable, array, footnote, booktabs, nicematrix} % Table utilities and advanced formatting
\makesavenoteenv{tabular} % Enable footnotes in tabular environments
\makesavenoteenv{table} % Enable footnotes in table environments
\usepackage{textcomp} % Additional text symbols
\usepackage{stfloats} % Floating environments for two-column layout
\usepackage{url} % URL handling
\usepackage{verbatim} % Multiline comments and verbatim text
\usepackage{comment} % Block comments
\usepackage{graphicx, epstopdf} % Graphics support and EPS-to-PDF conversion
\usepackage{caption} % Customization of captions
\usepackage{subfigure} % Subfigure support
\usepackage{algorithmic, algorithm} % Algorithmic and algorithm environments
\usepackage{cite} % Manage citations
\usepackage{balance} % Balance columns in the last page
\usepackage{xr} % Cross-referencing external documents
\usepackage{todonotes} % Allows adding inline and margin notes for tasks or reminders

% Allow breaking equations across pages
\allowdisplaybreaks % Allows splitting long equations

% IEEE-specific command
\IEEEoverridecommandlockouts

% correct bad hyphenation here
\hyphenation{}

% commands, declarations etc.
\input{_defs}
%
% Mathematical utility commands
\newcommand{\norm}[1]{\ensuremath{\left\|#1\right\|}} % Norm notation
\newcommand{\thickhat}[1]{\mathbf{\hat{\text{$#1$}}}} % Bold hat
\newcommand{\thickbar}[1]{\mathbf{\bar{\text{$#1$}}}} % Bold bar
% Mathematical operators
\DeclareMathOperator*{\rint}{\ThisStyle{\rotatebox{-15}{$\SavedStyle\!\int\!$}}} % Rotated integral operator
% Customizations
\renewcommand\qedsymbol{$\blacksquare$} % QED symbol customization

% Theorem environments and styles
\theoremstyle{definition} % Style for definition-like environments
\newtheorem{assumption}{Assumption} % Assumption environment
\newtheorem{claim}{Claim} % Claim environment
\newcounter{dummy} \numberwithin{dummy}{section} % Counter for theorem-like environments
\newtheorem{assum}[dummy]{Assumption} % Alternate assumption environment
\newtheorem{defi}[dummy]{Definition} % Definition environment
\newtheorem{exam}[dummy]{Example} % Example environment
\newtheorem{rema}[dummy]{Remark} % Remark environment
\theoremstyle{plain} % Style for theorem-like environments
\newtheorem{theo}[dummy]{Theorem} % Theorem environment
\newtheorem{prop}[dummy]{Proposition} % Proposition environment
\newtheorem{lemm}[dummy]{Lemma} % Lemma environment
\newtheorem{coro}[dummy]{Corollary} % Corollary environment

% Special environments
\newenvironment{assumptionprime}[1]
  {\renewcommand{\theassumption}{\ref{#1}$'$}%
   \addtocounter{assumption}{-1}%
   \begin{assumption}}
  {\end{assumption}} % Assumption with prime notation

% Algorithm environment customizations
\renewcommand{\algorithmicrequire}{\textbf{Input:}} % Customize "require" to "Input"
\renewcommand{\algorithmicensure}{\textbf{Output:}} % Customize "ensure" to "Output"

% Co-author comments and edits
\newcommand{\edit}[1]{{\color{blue}#1}} % Edits in blue
\newcommand{\delete}[1]{{\color{Bittersweet}\sout{#1}}} % Deletions in red (striked out)
\newcommand{\note}[1]{{\color{magenta}{[\textbf{Note---}#1}]}} % Notes in magenta
\newcommand{\todon}[1]{{\color{magenta}{[\textbf{ToDo---}#1}]}} % To-dos in magenta
\newcommand{\todoc}[1]{{\color{magenta}{[\textbf{Ref---}#1}]}} % Missing references in magenta
\newcommand{\polish}[1]{{\color{magenta}{[\textit{Following blue text needs polishing $\rightarrow$}}]}{\color{blue}#1}} % Reminder for polishing

% Custom colors for co-author comments
\definecolor{ballblue}{rgb}{0.13, 0.67, 0.8}
\definecolor{blush}{rgb}{0.87, 0.36, 0.51}
\definecolor{chamoisee}{rgb}{0.63, 0.47, 0.35}
\definecolor{darkseagreen}{rgb}{0.56, 0.74, 0.56}
\definecolor{purple}{rgb}{0.5, 0.0, 0.5}

% Co-author specific comment commands
\newcommand{\WUB}[1]{\textcolor{purple}{[\textbf{WUB:} #1]}} % Waheed U. Bajwa
\newcommand{\MG}[1]{\textcolor{blush}{[\textbf{MG:} #1]}} % Mert Gürbüzbalaban
\newcommand{\RD}[1]{\textcolor{chamoisee}{[\textbf{RD:} #1]}} % Rishabh Dixit
\newcommand{\CF}[1]{{\color{PineGreen}{[\textbf{CF:} #1}]}} % Cheng Fang
\newcommand{\mgedit}[1]{{\color{blush}#1}} % Edits by Mert Gürbüzbalaban

% Marked revisions for reviewers
\newcommand{\revise}[1]{#1}
%\renewcommand{\revise}[1]{{\color{red}#1}} % Uncomment for marked revisions

% Cross-referencing supplementary material
\makeatletter
\newcommand*{\addFileDependency}[1]{
  \typeout{(#1)}
  \@addtofilelist{#1}
  \IfFileExists{#1}{}{\typeout{No file #1.}}
}
\newcommand*{\myexternaldocument}[1]{
    \externaldocument{#1}
    \addFileDependency{#1.tex}
    \addFileDependency{#1.aux}
}
\makeatother

% Section numbering
\renewcommand\thesection{\arabic{section}}
\renewcommand\thesubsection{\thesection.\arabic{subsection}}
\renewcommand\thesubsubsection{\thesubsection.\arabic{subsubsection}}

\renewcommand\thesectiondis{\arabic{section}}
\renewcommand\thesubsectiondis{\thesectiondis.\arabic{subsection}}
\renewcommand\thesubsubsectiondis{\thesubsectiondis.\arabic{subsubsection}}

%***************
% Graphics path
%***************
\graphicspath{{./images/}} % !!!-change this to the actual project path-!!!
%***************


%***************
\begin{document}

\title{RESIST: Resilient Decentralized Learning Using Consensus Gradient Descent}

\author{
    Cheng Fang\textsuperscript{*}, Rishabh Dixit\textsuperscript{*}, Waheed U.\ Bajwa, and Mert Gürbüzbalaban%
    \thanks{\textsuperscript{*}Cheng Fang and Rishabh Dixit contributed equally to this work. 
    Cheng Fang is with the Department of Electrical and Computer Engineering, Rutgers University, New Brunswick, NJ, USA (e-mail: cf446@soe.rutgers.edu). 
    Rishabh Dixit is with the Department of Mathematics, University of California, San Diego, CA, USA (e-mail: ridixit@ucsd.edu). 
    Waheed U.\ Bajwa is with the Departments of Electrical and Computer Engineering and Statistics, Rutgers University, New Brunswick, NJ, USA (e-mail: waheed.bajwa@rutgers.edu). 
    Mert Gürbüzbalaban is with the Departments of Electrical and Computer Engineering, Management Science and Information Systems, and Statistics, Rutgers University, New Brunswick, NJ, USA (e-mail: mg1366@rutgers.edu).}%
    \thanks{This research was supported in part by the Office of Naval Research under award numbers N00014-21-1-2244 and N00014-24-1-2628; the National Science Foundation (NSF) under awards CCF-1814888, CCF-1907658, DMS-205348, and CNS-2148104; and by funding from industry partners as specified in the Resilient \& Intelligent NextG Systems (RINGS) program.}
}

\maketitle

\begin{abstract}
\noindent \textit{Empirical risk minimization} (ERM) is a cornerstone of modern \textit{machine learning} (ML), supported by advances in optimization theory that ensure efficient solutions with provable algorithmic convergence rates, which measure the speed at which optimization algorithms approach a solution, and statistical learning rates, which characterize how well the solution generalizes to unseen data. Privacy, memory, computational, and communications constraints increasingly necessitate data collection, processing, and storage across network-connected devices. In many applications, these networks operate in decentralized settings where a central server cannot be assumed, requiring decentralized ML algorithms that are both efficient and resilient. Decentralized learning, however, faces significant challenges, including an increased attack surface for adversarial interference during decentralized learning processes. This paper focuses on the \textit{man-in-the-middle} (MITM) attack, wherein adversaries exploit communication vulnerabilities between devices to inject malicious updates during training, potentially causing models to deviate significantly from their intended ERM solutions. To address this challenge, we propose RESIST (\textbf{R}esilient d\textbf{E}centralized learning using con\textbf{S}ensus grad\textbf{I}ent de\textbf{S}cen\textbf{T}), an optimization algorithm designed to be robust against adversarially compromised communication links, where transmitted information may be arbitrarily altered before being received. Unlike existing adversarially robust decentralized learning methods, which often ($i$) guarantee convergence only to a neighborhood of the solution, ($ii$) lack guarantees of linear convergence for strongly convex problems, or ($iii$) fail to ensure statistical consistency as sample sizes grow, RESIST overcomes all three limitations. It achieves algorithmic and statistical convergence for strongly convex, Polyak--{\L}ojasiewicz, and nonconvex ERM problems by employing a multistep consensus gradient descent framework and robust statistics-based screening methods to mitigate the impact of MITM attacks. Experimental results demonstrate the robustness and scalability of RESIST across diverse attack strategies, screening methods, and loss functions, confirming its suitability for real-world decentralized optimization and learning in adversarial environments.
\end{abstract}

\begin{IEEEkeywords}
Adversarial machine learning, decentralized gradient descent, distributed algorithms, empirical risk minimization, man-in-the-middle attack, nonconvex optimization, Polyak--{\L}ojasiewicz functions, robust statistics.
\end{IEEEkeywords}

\section{Introduction}\label{sec:introduction}
Learning a model from training data is foundational to modern \emph{machine learning} (ML) applications. The performance of a learning algorithm is typically evaluated through the \emph{statistical risk}, which measures the expected loss on unseen data. A common approach to minimize statistical risk is \emph{empirical risk minimization} (ERM)~\cite{Vapnik2013nature, sebastiani2002machine, kotsiantis2007supervised, bengio2009learning, Mohri2012foundations}, where a finite number of training samples are used to approximate the true risk. For convex loss functions, the ERM solution typically converges to the \emph{Bayes optimal solution} as the number of samples grows to infinity~\cite{Vapnik2013nature}, highlighting the interplay between data availability and model performance. Beyond statistical convergence, the efficiency of optimization algorithms in solving ERM problems---referred to as \emph{algorithmic convergence}---is critical for practical applications. Strong guarantees, such as linear convergence for strongly convex problems and sublinear rates for nonconvex problems, ensure that optimization methods can efficiently approach the desired solution while scaling to the demands of modern ML systems. Together, statistical learning rates (characterizing generalization) and algorithmic convergence rates (quantifying optimization efficiency) define the practical feasibility of learning algorithms. \looseness=-1

In many modern ML applications, data is inherently distributed across networked devices due to privacy constraints, bandwidth limitations, or sheer scale, as seen in multi-agent systems, Internet-of-Things (IoT) infrastructures, smart grids, and sensor networks. Traditional distributed learning approaches often assume the presence of a central server to coordinate the training process~\cite{YangGangEtAl.ISPM20}, as illustrated in Fig.~\ref{fig:system}(a). However, this assumption introduces potential single points of failure and also may not be practical in environments such as IoT systems and sensor networks. These limitations motivate \emph{decentralized learning}, where learning is performed collaboratively across devices without centralized coordination~\cite{predd2006distributed, boyd2011distributed, AliH.Sayed2014, nedic2018, nokleby2020, sun2021decentralized}, as shown in Fig.~\ref{fig:system}(b). Decentralized learning systems, however, face unique challenges, including potentially non-independent and identically distributed data, changing network topologies, unreliable communication links, and adversarial attacks, which must be addressed to ensure scalability and resilience in practical settings.

Among the challenges faced by decentralized learning systems, adversarial attacks present a particularly critical problem, as they can significantly degrade both algorithmic convergence and generalization performance. While much of the existing literature on robust decentralized learning under adversarial attacks focuses on the Byzantine attack model~\cite{Driscoll2003byzantine,Sousa2012byzantine,vaidya2013byzantine,su2015byzantine,Su2015ByzantineMO,yin2018byzantine,lin2019byzantine,Yang2019ByzantineResilientSG,Kuwaranancharoen2020ByzantineResilientDO,Data2020ByzantineResilientHS,Peng2020ByzantineRobustDS,wu2021Byzantine,Lie2022Byzantine,Fang2022BRIDGE}, which assumes some nodes are compromised by malicious actors and deliberately send arbitrary or corrupted values to their neighbors, this paper focuses on a different and less-explored threat: \textit{man-in-the-middle} (MITM) attacks. Unlike Byzantine attacks, where the adversary operates at the node level (Fig.~\ref{fig:system}(c)), MITM attacks exploit vulnerabilities in communication links, as shown in Fig.~\ref{fig:system}(d). By compromising these communication links, adversaries can inject arbitrary noise or malicious updates into transmitted information. Such adversarially compromised communication links allow transmitted information to be arbitrarily altered before being received, potentially leading to significant errors in the learning process.

To address this threat, we propose and analyze a decentralized learning algorithm specifically designed to resist MITM attacks. Our work highlights the unique challenges posed by adversarially compromised communication links in decentralized learning systems and also demonstrates the theoretical subsumption of the Byzantine attack model within the broader MITM attack model (cf. Sec.~\ref{mapping}). Our analysis encompasses both algorithmic and statistical perspectives, with a focus on strongly convex, Polyak--{\L}ojasiewicz~\cite{lojasiewicz1963propriete}, and nonconvex ERM problems.

\begin{figure}[t]
\centering
\subfigure[Distributed System]{
    \includegraphics[width=.20\textwidth]{images/Distributed_system.png}
}
\hfill
\subfigure[Decentralized System]{
\includegraphics[width=.20\textwidth]{images/Decentralized_system.png}
}
\hfill
\subfigure[Byzantine Attack]{
\includegraphics[width=.20\textwidth]{images/Decentralized_system_under_Byzantine_attack.png}
}
\hfill
\subfigure[Man-in-the-Middle Attack]{
\includegraphics[width=.20\textwidth]{images/Decentralized_system_under_MITM_attack.png}
}
\caption{Illustrations of different system architectures and adversarial attack models: (a) A distributed system with centralized coordination, where a central server manages the training process. (b) A decentralized system, where nodes collaborate without central coordination. (c) A decentralized system under a Byzantine attack, where one of the five nodes is compromised (colored red) and sends arbitrary or corrupted values to its neighbors through red-colored links. (d) A decentralized system under a man-in-the-middle (MITM) attack, where two communication links are under attack (colored red), allowing the attacker to alter the transmitted information before it is received, even though no nodes are compromised. These attacked links can change over time, making the communication vulnerabilities dynamic. A discussion of the mathematical mapping of the Byzantine attack problem to the MITM attack problem is provided in Sec.~\ref{mapping}.}
\label{fig:system}
\end{figure}

\subsection{Relation to prior works}
The advent of large-scale ML tasks and the impracticality of consolidating data into a single location have driven significant interest in collaborative learning approaches~\cite{nokleby2020}. A key category in this field is distributed learning, which includes the parameter--server~\cite{Muli2014} and federated learning~\cite{jakub2016} settings, both relying on a central server to facilitate communication among network nodes. Algorithms for distributed and federated learning can be grouped into three main categories: first-order methods, such as distributed gradient descent and its stochastic variants~\cite{blanchard2017machine,draco2017chen,cao2018robust,damaskinos2018asynchronous,mhamdi2018hidden,xie2018generalized,xie2018phocas,chen2019distributed,rajput2019detox,jin2019distributed,data2019data,el2019sgd,elmhamdi2019fast,He2022distributed}, valued for their low computational complexity; augmented Lagrangian-based methods~\cite{zhange2014DADMM,Chang2016DIADMM,Huang2020DiADMM}, which require solving local optimization subproblems---incurring higher computational complexity than gradient-based approaches---but can address challenging problems while preserving privacy~\cite{Chang2016DIADMM,Huang2020DiADMM}; and second-order methods~\cite{Li2019DiNew,Ghosh2020DiNew,Dinh2022DiNew,liu2023DiNew}, which, despite higher computational and communication costs, achieve second-order optimal convergence guarantees. Reliance on centralized coordination, however, introduces limitations such as single points of failure and system design constraints, prompting the development of decentralized learning systems (cf.~Fig.~\ref{fig:system}(b)). But transitioning algorithmic techniques, along with the derivation of both algorithmic convergence guarantees and statistical learning rates, from distributed to decentralized settings poses unique challenges due to the lack of centralized coordination and fundamental architectural differences.
%
%With large-scale machine learning tasks being introduced to the machine learning literature~\cite{nokleby2020} along with the constraint that it is not feasible for the data to be collected into a single entity, researchers have started to focus on collaborative learning. The first typical category of collaborative learning is distributed learning, which includes the parameter-server setting~\cite{Muli2014} and the federated learning setting~\cite{jakub2016}. In this setting, a central server is assumed to exist in order to communicate with all the other nodes in the network. Most existing work in the distributed/federated setting can be partitioned into three broad classes of iterative algorithms. The first one of these classes of algorithms corresponds to first-order methods such as the distributed gradient descent and its (stochastic) variants~\cite{blanchard2017machine,draco2017chen,cao2018robust,damaskinos2018asynchronous,mhamdi2018hidden,xie2018generalized,xie2018phocas,chen2019distributed,rajput2019detox,jin2019distributed,data2019data,el2019sgd,elmhamdi2019fast, He2022distributed}. The advantage of algorithms is low (local) computational complexity, which makes them favorable, especially for problems that contain high-dimensional data. The second class of algorithms utilized the use of augmented Lagrangian-based methods~\cite{zhange2014DADMM, Chang2016DIADMM, Huang2020DiADMM}, which require each local agent to solve an optimization subproblem. Even though the computational cost of this type of algorithm is relatively high, it can tackle some challenging problems that are hard to solve with the traditional first-order methods and sometimes achieve better learning behavior while preserving privacy \cite{Chang2016DIADMM, Huang2020DiADMM}. The third class of algorithms includes second-order methods\cite{Li2019DiNew, Ghosh2020DiNew, Dinh2022DiNew, liu2023DiNew}, which typically have high computational and/or communications cost but could arrive at the second-order optimum convergence guarantee. Despite the efforts being made in the distributed algorithms literature, distributed algorithms suffered from design constraints of the physical systems, privacy concerns, and potential SPOF. Thus, algorithms suitable for decentralized systems (as shown in the second figure in Figure \ref{fig:system}) arise in the machine learning literature. The naive way of analyzing those decentralized algorithms is to mimic the way that was done in the distributed ones; however, transiting the algorithmic analysis and considering the constraints from the distributed learning setups to the decentralized ones, which lack central coordinating servers, is a nontrivial endeavor. The following paragraph will discuss how decentralized algorithms evolve in the ML literature.

In decentralized learning, the absence of a central server is addressed by restricting communication to direct neighbors. While the grouping of decentralized algorithms into three main categories mirrors that of distributed learning---first-order methods, such as \emph{decentralized gradient descent} (DGD) and its stochastic variants~\cite{nedic2009,ram2010distributed,nedic2015distributed,nedic2020}; augmented Lagrangian-based methods~\cite{forero2010consensus,mota2013admm,shi2014on,ozgadar2017DeADMM}; and second-order methods~\cite{jadbabaie2009DeNEW,wei2013DeNew,Mokhtari2016decentralized,Mokhtari2017network,Turunov2019DeNew}---the methods themselves and their analysis differ significantly due to the lack of centralized coordination. Most existing works focus on achieving algorithmic convergence, often under idealized assumptions of trustworthy communication and faultless operations, while overlooking statistical learning rates that are essential for understanding how well solutions generalize to unseen data. \looseness=-1
%
%In decentralized learning, people gradually overcome the difficulty of the non-existence of the central server by communicating with its direct neighbors only. Most of the existing decentralized algorithms can also be separated into mainly three broad classes of iterative algorithms which includes first-order methods such as the \emph{decentralized gradient descent} (DGD) and its (stochastic) variants~\cite{nedic2009,ram2010distributed, nedic2015distributed,nedic2020}; augmented Lagrangian-based methods~\cite{forero2010consensus,mota2013admm,shi2014on,ozgadar2017DeADMM} and second-order methods\cite{jadbabaie2009DeNEW, wei2013DeNew, Mokhtari2016decentralized, Mokhtari2017network, Turunov2019DeNew}. Some of the works listed above argue that traffic/communication congestion can be relieved by adopting decentralized learning compared with distributed settings. Even though many works have explored the decentralized learning problem, all of these traditional works cited above assume faultless operations within the decentralized network.

Adapting decentralized learning methods to adversarial environments is a relatively recent focus, with most efforts concentrating on the Byzantine attack model. First introduced in its general form in~\cite{Robert1987}, the Byzantine attack refers to compromised nodes that deviate arbitrarily from expected behavior, making detection and defense particularly challenging. The rising prevalence of cybersecurity threats, vulnerabilities in communication channels, and the increasing reliance on ML in mission-critical applications have intensified the demand for robust defenses. Early research focused on detecting Byzantine nodes in distributed settings~\cite{MaranoDetection2008,vempaty2013distributed,HashlamounDetection2018}, followed by approaches leveraging centralized servers for resilient aggregation in the presence of Byzantine attacks~\cite{cao2018robust,su2018securing,yin2018defending,yin2018byzantine,alistarh2018byzantine,li2018rsa,xie2018zeno,xie2019zeno++}.

In decentralized systems, initial efforts focused on Byzantine-resilient consensus averaging~\cite{leblanc2013resilient,vaidya2014iterative}, which were later extended to Byzantine-resilient learning for scalar-valued models~\cite{Su2016fault,sundaram2018distributed}. However, these approaches do not directly apply to the vector-valued ML frameworks considered in this paper. While some works have addressed specific vector-valued problems, such as decentralized support vector machines~\cite{yang2016rdsvm} and decentralized estimation~\cite{xu2018robust,mitra2019resilient,su2018finite,RenEstimation2020,AnEstimation2021}, these solutions are not generalizable to the broader ERM framework.
%
%The Byzantine general problem can be traced back decades ago, and the thread model of the Byzantine attack in its most general form was introduced and analyzed in~\cite{Robert1987}. The Byzantine attack was then defined as some malicious node attack that can deviate arbitrarily far away from its expected behavior. Recently, due to the effectiveness of computer viruses, the vulnerability of communication channels, and the demand for high accuracy in most machine learning applications, safe-guarding distributed/decentralized learning from the Byzantine attacks has been playing a more and more critical role in the cybersecurity/ML literature. The Byzantine attack is one of the most destructive attacks that are hard to detect and defend against due to its nature. Thus, in the beginning, instead of learning, some of the first work can only perform detection tasks with the presence of Byzantine attacks~\cite{MaranoDetection2008, vempaty2013distributed, HashlamounDetection2018} and then start to defend against Byzantine attacks in the distributed setting by utilizing a central server to aggregate the information received from the agents~\cite{cao2018robust, su2018securing, yin2018defending,yin2018byzantine,alistarh2018byzantine,li2018rsa,xie2018zeno,xie2019zeno++}. After a few years of understanding and efforts in the distributed learning with Byzantine failure, some can consider Byzantine failures in decentralized systems, which only solve the problem of Byzantine-resilient averaging consensus~\cite{leblanc2013resilient,vaidya2014iterative}. After continuing efforts, these works were then leveraged to tackle Byzantine-resilient decentralized learning with scalar-valued models~\cite{Su2016fault,sundaram2018distributed}. However, neither of these works is applicable to the general vector-valued ML framework being considered in this paper. In the meanwhile, some researchers have also developed Byzantine-resilient decentralized learning methods for some specific vector-valued problems that include the decentralized support vector machine~\cite{yang2016rdsvm} and decentralized estimation~\cite{xu2018robust,mitra2019resilient,su2018finite, RenEstimation2020, AnEstimation2021}.

Similar to the study of the ERM framework for centralized ML, the algorithmic and statistical guarantees of Byzantine-resilient decentralized learning methods for vector-valued models can be broadly categorized by specific loss function classes, typically divided into convex (strongly convex, strictly convex, and convex) and nonconvex (quasi-convex, semi-convex, and smooth nonconvex). The first work to address the vector-valued Byzantine-resilient learning problem with a general convex loss function was \cite{yang2019byrdie}, which proposed a decentralized coordinate-descent-based learning algorithm termed ByRDiE. This algorithm demonstrated resilience to Byzantine attacks and convergence to the minimizer of a loss function comprising a convex differentiable term and a strictly convex, smooth regularizer. While \cite{yang2019byrdie} characterized both algorithmic convergence and statistical learning rates for ByRDiE, its focus on convex functions limited its scope. More critically, the coordinate-descent nature of ByRDiE leads to slow and inefficient computation for large-scale models, particularly for high-dimensional data in deep neural networks. Let \( d \) denote the number of parameters in the ML model (e.g., the number of weights in a deep neural network). A single iteration of ByRDiE requires \( d \) network-wide collaborative steps, with each step involving the computation of a \( d \)-dimensional gradient at every node, making it computationally expensive. In contrast, BRIDGE, proposed in~\cite{Fang2022BRIDGE}, requires only one round of updates per iteration for vector-valued models, offering a more efficient and scalable computational framework in decentralized settings. However, BRIDGE assumes loss functions are either strongly convex or locally strongly convex, restricting its applicability to a narrower class of problems.

In contrast to the focus on Byzantine attacks in ByRDiE and BRIDGE, this work addresses the MITM attack (cf.~Fig.~\ref{fig:system}(d)), where adversaries exploit communication vulnerabilities to inject malicious updates during training, causing models to deviate significantly from their intended ERM solutions. The MITM attack model introduces unique challenges, as adversaries can dynamically target different communication links over time. To tackle this, we propose RESIST (\textbf{R}esilient d\textbf{E}centralized learning using con\textbf{S}ensus grad\textbf{I}ent de\textbf{S}cen\textbf{T}). While RESIST reduces to BRIDGE when nodes perform a local gradient step after each round of communication with their neighbors (cf.~Sec.~\ref{sec: theoretical analysis} and Algorithm~\ref{gradient descent algorithm}), the broader attack model and the more general class of loss functions analyzed in this work distinguish RESIST from both ByRDiE and BRIDGE. Furthermore, within the framework of RESIST, we demonstrate that the Byzantine attack model can be viewed as a special case of the MITM attack model (cf.~Sec.~\ref{mapping}), highlighting the broader applicability of the MITM framework in this context. These distinctions necessitate a novel theoretical analysis specific to RESIST, making it both a significant generalization and extension of existing approaches.
%
%Compared to most classical ML frameworks, people also algorithmically/statistically analyze Byzantine-resilient decentralized learning algorithms for vector-valued models for certain loss functions, which can usually be categorized into several classes of convex (strongly convex, strictly convex, and convex) and nonconvex (quasi-convex, semi-convex, and smooth nonconvex) loss functions. The first work in the literature that tackled the vector-valued problem with a general convex loss function is \cite{yang2019byrdie}, in which a decentralized coordinate-descent-based learning algorithm termed ByRDiE was proposed and proved its resilience to Byzantine failures and convergence to the minimizer of a loss function that is given by the sum of a convex differentiable function and a strictly convex and smooth regularizer. Even though \cite{yang2019byrdie} provided analysis in both algorithmic convergence as well as statistical convergence (i.e., sample complexity) of ByRDiE, one of the limitations of \cite{yang2019byrdie} is that it only focuses on the class of convex function for simplicity of the analysis. More importantly, the coordinate-descent nature of ByRDiE makes the computation slow and inefficient for learning large-scale models, especially for deep neural networks with high-dimensional data. Suppose we denote $d$ as the number of parameters in the ML model being trained (e.g., the number of weights in a deep neural network). One iteration of ByRDiE then requires updating the $d$ coordinates of the model in $d$ network-wide collaborative steps, each requiring a computation of the local $d$-dimensional gradient at each node in the network. By contrast, BRIDGE ~\cite{Fang2022BRIDGE} requires only one round of updates when dealing with vector-valued models in each iteration. Even though the resulting computational framework of ~\cite{Fang2022BRIDGE} is highly efficient and scalable in a decentralized setting, it assumes the loss functions can only be strongly convex and locally strongly convex loss functions. In this work, we assume a general attack that can move throughout the network by choosing different links at different times, as described later in detail in Section \ref{def of MITM}. Even though RESIST algorithm reduces to BRIDGE in the case when the inner consensus loop index is 1, since the nature of the attack is quite different than the one in~\cite{Fang2022BRIDGE} and the loss function considered in this paper is more general than the one in~\cite{Fang2022BRIDGE}, the proposed algorithm is not a trivial extension from ByRDiE or BRIDGE and therefore necessitates its own theoretical analysis.

Given that the Byzantine attack model can be mapped to the MITM attack model within the framework of this paper (as detailed later in Sec.~\ref{mapping}), we now discuss recent works beyond \cite{yang2019byrdie} and \cite{Fang2022BRIDGE} that focus on Byzantine-resilient vector-valued decentralized learning. These include \cite{Kuwaranancharoen2020ByzantineResilientDO, Peng2020ByzantineRobustDS, Guo2020TowardsBL, Elmhamdijungle2021, WU2023SGD, ghiasvand2024robust, ghavamipour2024privacypreserving, bakshi2024valid}. Among these, \cite{Kuwaranancharoen2020ByzantineResilientDO} addresses only convex loss functions and does not provide algorithmic convergence rates or statistical learning rates. Additionally, the algorithm's robustness diminishes with increasing data dimensions, making it less effective for defending against Byzantine nodes in high-dimensional settings. Similarly, \cite{Peng2020ByzantineRobustDS} focuses on convex loss functions in heterogeneous data settings and time-varying networks but also lacks statistical learning rate guarantees. The MOZI algorithm proposed in \cite{Guo2020TowardsBL} also targets convex loss functions but relies on an aggressive two-step filtering operation that limits the number of Byzantine nodes it can handle. Furthermore, its analysis assumes that faulty nodes send outlier messages relative to regular nodes, a condition often unmet under the Byzantine attack model. For nonconvex loss functions, \cite{Elmhamdijungle2021} introduces three methods, including ICwTM, effectively a variant of BRIDGE from \cite{Fang2022BRIDGE}. ICwTM incurs higher communication overhead as it requires nodes to exchange both local models and gradients, and assumes identical initialization across the network, which may be impractical in certain applications. Additionally, this work does not examine the impact of network topology on learning performance. The work \cite{WU2023SGD} proposes a stochastic gradient descent-based algorithm for nonconvex loss functions with heterogeneous data but does not extend to the MITM attack model and provides only bounds on the average gradient norm rather than guarantees on iterate values. Another approach, \cite{ghiasvand2024robust}, utilizes gradient tracking to manage heterogeneous data and improve communication efficiency but assumes attackers apply uniform perturbations, limiting its applicability to generalized Byzantine or MITM attack scenarios. Finally, \cite{ghavamipour2024privacypreserving} and \cite{bakshi2024valid} develop algorithms for privacy-preserving and validated decentralized learning under Byzantine attacks, respectively, but rely on secure private key or secret-sharing mechanisms among honest nodes, making them unsuitable for scenarios lacking secure communication links.

Next, we focus on the distinction between our work on the MITM attack model and related work in the Byzantine-resilient literature that aligns with our goal of deriving linear (geometric) convergence rates for strongly convex losses. The closest such work is \cite{kuwaranancharoen2023geometric}, which also achieves linear convergence for strongly convex losses while maintaining robustness to Byzantine failures. However, this work has several limitations. First, it is restricted to strongly convex loss functions and cannot be generalized to nonconvex functions such as Polyak--{\L}ojasiewicz (P\L) functions. Second, the algorithms in \cite{kuwaranancharoen2023geometric} do not guarantee exact convergence of local iterates to the global minimum, even when all local loss functions are identical or when the number of local data samples \( N \) approaches infinity. In contrast, our work addresses the more general MITM attack model and provides guarantees for exact convergence to the global minimum asymptotically for strongly convex losses when \( N \) is infinite. Additionally, we establish statistical learning rate guarantees (sample complexity) for finite sample sizes. Lastly, while one of the algorithm variants in \cite{kuwaranancharoen2023geometric} aligns with BRIDGE, the best-performing variant, termed \emph{Simultaneous Distance-MixMax Filtering Dynamics} (SDMMFD), employs three distinct filtering mechanisms per iteration, resulting in three times the redundancy requirements compared to RESIST. Consequently, their algorithm can defend against only one-third of the number of attacks that RESIST can handle in a given network. This redundancy requirement also prevents a direct performance comparison between SDMMFD and RESIST as part of the numerical results reported in Sec.~\ref{numerical section}.
%
%Besides \cite{yang2019byrdie} and ~\cite{Fang2022BRIDGE}, continuing efforts have been made in the robust decentralized literature during the past few years\cite{Kuwaranancharoen2020ByzantineResilientDO, Peng2020ByzantineRobustDS, Guo2020TowardsBL, Elmhamdijungle2021}. First of all, the work in \cite{Kuwaranancharoen2020ByzantineResilientDO} focuses only on convex types of loss functions, and it does not provide either algorithmic convergence rates or statistical convergence rates. Additionally, the number of failure nodes that the proposed algorithm can defend against is reversely proportional to the dimension of the data, and thus, the algorithm can only protect the learning process from a very small number of failure nodes when the date dimension is high. In contrast, the authors in \cite{Peng2020ByzantineRobustDS} focus on Byzantine-resilient decentralized learning with heterogeneous data and time-varying networks. However, it only focuses on the convex loss functions and does not provide a statistical learning rate. Next, an algorithm termed MOZI is proposed in \cite{Guo2020TowardsBL}, focusing only on convex loss functions. The resilience of MOZI, however, requires an aggressive two-step `filtering' operation, which limits the maximum number of Byzantine nodes that can be handled by the algorithm. The analysis in \cite{Guo2020TowardsBL} also assumes that the faulty nodes always send messages that are `outliers' relative to those of the regular nodes, which usually can not be satisfied in the Byzantine literature. Finally, one of the papers in the literature that has investigated Byzantine-resilient decentralized learning for nonconvex loss functions is \cite{Elmhamdijungle2021}. The authors in this work have introduced three methods, among which the so-called ICwTM method is effectively a variant of ~\cite{Fang2022BRIDGE}. The ICwTM algorithm, however, has high communication overhead because it requires the neighbors to exchange both their local models and local gradients in order to guarantee learning performance. In addition, \cite{Elmhamdijungle2021} requires the nodes to have the same initialization, which could be an unrealistic assumption in some real-world applications, and it does not bring out the dependence of the network topology on the learning problem.
%
%With the expanding interest in the development of robust decentralized optimization/machine learning algorithms, several papers additional to those cited above~\cite{kuwaranancharoen2023geometric, WU2023SGD,ghiasvand2024robust,ghavamipour2024privacypreserving,bakshi2024valid} have appeared recently in the literature. The work~\cite{kuwaranancharoen2023geometric} focused on achieving a linear (geometric) convergence rate and, simultaneously, robust to Byzantine failures. The first drawback of this work is that it only focuses on strongly convex loss functions, and the algorithm can not be generalized to some types of nonconvex functions, such as Polyak-{\L}ojasiewicz (P\L) functions. More importantly, the algorithms do not provide an exact convergence guarantee of the local iterates to the global minima asymptotically, even if all the local loss functions are the same or when the number of local data samples $N$ is infinite. In this paper, we provided guarantees for exact convergence to the global minima with infinite samples asymptotically when the loss functions are strongly convex and also provide statistical convergence rate (sample complexity) for finite sample size $N$. Last but not least, their algorithm, which is termed as \emph{Simultaneous Distance-MixMax Filtering Dynamics} (SDMMFD), utilizes three different types of filtering mechanisms with distance-based and performance-based metrics in each iteration, which creates three times redundancy compared to our methods, which makes the algorithm can only defend against up to one-third of failures compared to our work with a given network. That also prevents us from comparing the performance of their algorithm to ours in the numerical experiment section. The work~\cite{WU2023SGD}, designed a SGD-based algorithm that are suitable for nonconvex loss functions with heterogeneous data. However, it assumes the attack can not travel within the network and provides only the upper bounds in the form of the average gradient norm instead of the iterates value, which is determined by the Byzantine attack. The third work~\cite{ghiasvand2024robust} proposed utilizing gradient tracking in the decentralized setting to deal with heterogeneous data and communication efficiency. However, they assume that the attacker can only pose common perturbations to all the samples, which is not generalized to Byzantine/MITM attack settings. The last two works~\cite{ghavamipour2024privacypreserving,bakshi2024valid} developed algorithms for privacy-preserving and validating decentralized learning resilient to Byzantine attacks in the network, respectively. However, their algorithm requires secure private key/secret sharing among the honest nodes in order to defend against the Byzantine attack, which makes the algorithm not generalizable to scenarios where secure links are absent.

A summary of how our work relates to prior works is provided in Table~\ref{table:intro.comparison}. This table compares RESIST with various vector-valued decentralized learning and optimization methods in the literature across key dimensions: the attack model, whether an algorithmic convergence rate is provided, whether a statistical learning rate is provided, and whether the analysis includes nonconvex loss functions.

\begin{table*}[t]
\centering
\begin{tabular}{|c c c c c|}
 \hline
 {\bf Algorithm}  & {\bf Attack Model} & {\bf Algorithmic Convergence Rate} & {\bf Statistical Learning Rate}  & {\bf Nonconvex}\\
 \hline\hline
 DGD~\cite{nedic2015distributed} & None & $\surd$ & $\times$  & $\times$\\
 \hline
 NEXT~\cite{Lorenzo2016NEXTIN} & None & $\times$ & $\times$   & $\surd$\\
 \hline
 Nonconvex DGD~\cite{zeng2018nonconvex} & None & $\surd$ & $\times$  & $\surd$ \\ 
 \hline
 D-GET~\cite{Sun2019ImprovingTS} & None & $\surd$ & $\surd$ &  $\surd$\\
 \hline
 GT-SARAH~\cite{xin2021fast} & None & $\surd$ & $\surd$ &  $\surd$\\
 \hline
 MOZI~\cite{Guo2020TowardsBL} & Non-Byzantine & $\surd$ & $\times$  &$\times$ \\
 \hline
  Dec-FedTrack~\cite{ghiasvand2024robust} & Non-Byzantine & $\surd$ & $\times$  & $\surd$ \\
 \hline
 ByRDiE~\cite{yang2019byrdie} & Byzantine &$\surd$ & $\surd$ &  $\times$\\
 \hline
 Kuwaranancharoen et.\ al \cite{Kuwaranancharoen2020ByzantineResilientDO} & Byzantine & $\times$ & $\times$  & $\times$\\
 \hline
 ICwTM~\cite{Elmhamdijungle2021} &Byzantine & $\surd$ & $\times$  &$\surd$\\
 \hline
 DRSA~\cite{Peng2020ByzantineRobustDS} & Byzantine & $\surd$ & $\times$  & $\times$ \\
 \hline
 BRIDGE~\cite{Fang2022BRIDGE} & Byzantine &$\surd$ & $\surd$  & $\triangle$\\
  \hline
 BASIL~\cite{ElkordyBasil2022} & Byzantine &$\surd$ & $\times$  & $\times$\\
 \hline
 IOS~\cite{WU2023SGD} & Byzantine & $\surd$ & $\times$ &  $\surd$ \\
 \hline
 REDGRAF~\cite{kuwaranancharoen2023geometric}& Byzantine & $\surd$ & $\times$  & $\surd$ \\
 \hline
 SecureDL~\cite{ghavamipour2024privacypreserving} & Byzantine & $\surd$ & $\times$  & $\times$ \\
 \hline
 VALID~\cite{bakshi2024valid}  & Byzantine & $\surd$ & $\times$  & $\times$ \\
 \hline
 {\bf RESIST (This work)} & MITM, Byzantine & $\surd$ & $\surd$ & $\surd$ \\
 \hline
\end{tabular}
\begin{tablenotes}\footnotesize
\item[1] \emph{Non-Byzantine:} Refers to models with assumptions on attack behavior that limit generalizability to Byzantine attacks.
\item[2] $\triangle$: Refers to global nonconvex functions with local strong convexity around stationary points.
\end{tablenotes}
\caption{Comparison of RESIST with various vector-valued decentralized learning and optimization methods in the literature.}
\label{table:intro.comparison}
\end{table*}

\subsection{Our contributions}
The primary contribution of this work is the development and analysis of RESIST, a decentralized first-order method robust to MITM attacks in the network, with a comprehensive analysis addressing both algorithmic convergence and statistical learning rates across different classes of convex and nonconvex loss functions. The MITM attack model has been extensively studied in the communications literature, with \cite{ContiMITMsurvey2016} providing a detailed survey of scenarios where MITM attacks occur in communication networks and potential defenses against them. However, to the best of our knowledge, the MITM attack model has not been studied in decentralized learning settings, though it has been investigated in distributed learning frameworks, as in \cite{ChiangMITM2009, NadendlaMITM2014, ZhangMITM2018}. Notably, \cite{NadendlaMITM2014} considers the MITM attack as a subset of the Byzantine attack, but this is based on the assumption of a \emph{static} attack model where the attacker cannot switch between links. In contrast, the MITM attack model considered in this work, detailed in Sec.~\ref{sec: problem formulation}, assumes a \emph{dynamic} attack model where the adversary can target different links over time, constrained only by the total number of links under attack at any given moment. This dynamic framing makes the MITM attack significantly more potent and challenging to defend against (see also our discussion relating the MITM and Byzantine attack models in Sec.~\ref{mapping}). Our work is the first to study this dynamic MITM attack model in the context of decentralized learning. \looseness=-1

Within this framing, RESIST makes several key contributions to address the challenges posed by (dynamic) MITM attacks in decentralized learning systems. Specifically, RESIST overcomes the slower (sublinear) convergence rate of the BRIDGE algorithm \cite{Fang2022BRIDGE} by achieving geometric convergence rates to the global minimum for strongly convex functions. Algorithmically, RESIST can be viewed as a generalization of BRIDGE, utilizing multiple rounds of consensus steps per gradient iteration. Notably, for a fixed number of algorithmic iterations, RESIST requires fewer gradient computations than BRIDGE, trading off computation for communication and enabling greater computational efficiency in large-scale ML problems. A key similarity between BRIDGE and RESIST is the use of robust-statistics-based screening rules to filter out potentially malicious information. However, while BRIDGE’s analysis relies on results concerning the product of stochastic mixing matrices from \cite{Vaidya2012matrix} over ``filtered'' graphs corresponding to the screening of Byzantine attacks, the dynamic and adaptive nature of the MITM attack model in this work, combined with multiple consensus steps, necessitates the derivation of new variants of the results in \cite{Vaidya2012matrix}. These results, which are crucial for establishing consensus guarantees for RESIST, are provided in Appendix~\ref{section*vaidya_10}.
%
%Man-in-the-middle(MITM) attacks have appeared in communication networks for decades. Detecting/defending MITM attack within the distributed/decentralized network is essential. Recent review paper~\cite{ContiMITMsurvey2016} provides a thorough understanding of the various causes of MITM attacks and potential defense solutions within communication networks. Previous work only focuses on detection in decentralized networks with a lack of the perspective of learning. Also, in only a few papers in the distributed learning literature~\cite{ChiangMITM2009, NadendlaMITM2014, ZhangMITM2018}, one type of their Byzantine attack can be modeled as an MITM attack without the ability to move around the network. However, our MITM attack model defined in Section \ref{mapping} assumes the ability to move around the network, which makes our model potentially more dangerous in the decentralized learning framework.
%
%Our main contribution in this work is the development of a decentralized gradient algorithm RESIST robust to the MITM attack and also its convergence guarantees and sample complexity under different types of loss functions. The proposed algorithm overcomes the BRIDGE algorithm's relatively slower (sub-linear) convergence rate \cite{Fang2022BRIDGE} on strongly convex functions by achieving geometric convergence rates to the global minima. Moreover, for a fixed number of algorithmic iterations, the proposed algorithm RESIST utilizes fewer gradient computation rounds \todon{Mg: gradient computations or consensus steps? total number of gradient computations determines the comp. efficiency}\CF{Since we are doing a multi-step consensus before the local update, we utilized more consensus rounds and fewer gradient computations and, thus the algorithm achieves better computational efficiency} compared to that of BRIDGE, thereby resulting in better computational efficiency. 
%
%From a viewpoint of graph theory, in Appendix \ref{section*vaidya_10}, this work defines a filtered graph topology generated by the algorithm \todon{MG-graph topology is generated by the attacks but not the alg?}\CF{the filtered graph topology is generated by the screening method of the algorithm rather than the attack, with that being said, even there is no attack in the network, the filtered graph will still be generated by the screening method of the algorithm. The most the attack can impact the filtered graph is that it can alter the value transmitted such that the screening method of the algorithm will choose a different filtered graph to perform communication and averaging. }\MG{My understanding is that the topology is not alg. dependent as we remove all the possible comb of links.} and then re-derives some basic results on the product of stochastic mixing matrices over such graphs from \cite{Vaidya2012matrix}, which are crucial for establishing consensus guarantees for the RESIST.

In terms of our results purely from the perspective of convergence rates in decentralized optimization under malicious attacks (dynamic MITM attack model), this work makes three significant contributions. First, in the strongly convex setting, we establish the geometric convergence rate of the iterate and consensus error to a ball around the origin (Theorem~\ref{inexactlmigeo}). The radius of this ball is quantified by factors such as the inexact averaging operation, the algorithm's stepsize, and the coordinate-wise trimmed mean screening method---a filtering approach widely employed in robust distributed~\cite{yin2018byzantine} and decentralized frameworks~\cite{Su2016fault,sundaram2018distributed,yang2019byrdie,Fang2022BRIDGE}. Notably, and in contrast to \cite{kuwaranancharoen2023geometric}, this theorem demonstrates that RESIST achieves \emph{exact} convergence at a geometric rate when the local functions at each node are identical, corresponding to the decentralized risk minimization framework under identical data distributions. \looseness=-1

Second, for loss functions satisfying the Polyak--{\L}ojasiewicz (P{\L}) property~\cite{lojasiewicz1963propriete}, we establish geometric convergence rates of the consensus and function value to a ball around the minimal function value (Theorem~\ref{plrate_theo}). The radius of this ball is similarly influenced by the inexact averaging operation, the algorithm's stepsize, and the screening method. To the best of our knowledge, this is the first work to analyze the P{\L} function class in the context of MITM attacks over decentralized optimization networks.

Finally, for smooth nonconvex functions (Sec.~\ref{sec_gennonconvex_sub1}), using a diminishing stepsize, we derive sublinear convergence rates for the iterate error from a first-order stationary point of the objective and for the consensus error to a ball around the origin (Theorem~\ref{nonconvexrate_theo}). This result matches the best-known convergence rates for centralized stochastic gradient descent methods~\cite{Kento2024SGD} under the same stepsize schedule. Importantly, this error ball vanishes in the decentralized ERM setting as the number of data samples approaches infinity. Additionally, we provide a finite-horizon guarantee for the nonconvex setting with a constant stepsize (Theorem~\ref{nonconvexrate_theo_fixedstep}), extending prior work~\cite{WU2023SGD} to accommodate the dynamic MITM attack model.
%
%From the viewpoint of the optimization literature, the contributions of this work are threefold. Firstly, in the strongly convex setting, using an inexact recursion, this work establishes the geometric convergence rate of the iterate and consensus error to a ball around the origin (Theorem \ref{inexactlmigeo}) where the ball radius is quantified by the inexact averaging operation by the stepsize of the algorithm and the Coordinate-Wise Trimmed Mean screening method which was frequently used in distributed frameworks~\cite{yin2018byzantine} and decentralized frameworks~\cite{Su2016fault,sundaram2018distributed,yang2019byrdie, Fang2022BRIDGE}. In particular, this theorem also shows that the RESIST achieves exact convergence at a geometric rate when the local functions at each node are identical. Secondly, when the loss functions satisfy P{\L} (Polyak {\L}ojasiewicz) property \cite{lojasiewicz1963propriete}, this work establishes geometric convergence rate of the consensus and function value to a ball around the minimal function value (Theorem \ref{plrate_theo}) where the ball radius is again quantified by the inexact averaging operation by the stepsize of the algorithm and the Coordinate-Wise Trimmed Mean screening method. To the best of our knowledge, this is the first work that analyzes the P{\L} function class in the setting of MITM attacks over the network. Lastly, in the smooth nonconvex function class setting (section \ref{sec_gennonconvex_sub1}), for a choice of diminishing step-size, this work establishes sub-linear convergence rates of the iterate error from a first-order stationary point of the objective and the consensus error to a ball around the origin (Theorem \ref{nonconvexrate_theo}), which matches the convergence rate to the first order oracle in centralized stochastic gradient descent method \cite{Kento2024SGD}, which is the best-known results in the literature for the given choice of diminishing step-size. This error ball vanishes in the setting of the ERM problem when the data sample size goes to infinity. This work also derives a finite horizon guarantee in the nonconvex setting with constant step size as in\cite{WU2023SGD} but with MITM attack (Theorem \ref{nonconvexrate_theo_fixedstep}).

In terms of statistical learning rates for decentralized learning systems, our contributions in Sec.~\ref{sec_statisticalrate_1} include the derivation of sample complexity guarantees for the decentralized ERM problem under MITM attacks, covering strongly convex, P{\L}, and general smooth nonconvex loss functions (Theorems~\ref{statisticalconvergencethm}, \ref{statisticalconvergencethm_pl}, and \ref{statisticalconvergencethm_nonconvex}, respectively). These guarantees establish that, even under the dynamic MITM attack model, RESIST solves the ERM problem with a statistical learning rate that matches the rate derived for BRIDGE~\cite{Fang2022BRIDGE}, while extending the results to both the P{\L} and general smooth nonconvex function classes. Notably, as in the BRIDGE framework, our results demonstrate a speed-up in the learning rate due to collaboration, despite the presence of attacks within the network. This speed-up, given \(M\) nodes and \(N\) samples per node, is guaranteed to lie between the local statistical learning rate of \(\mathcal{O}(1/\sqrt{N})\) and the ideal decentralized learning rate without any attacks of \(\mathcal{O}(1/\sqrt{MN})\). To the best of our knowledge, this is the first work to provide such statistical learning rate guarantees for the decentralized ERM problem under adversarial attacks for P{\L} and general smooth nonconvex functions.
%
%From a statistical viewpoint, in Section \ref{sec_statisticalrate_1}, this work provides the sample complexity guarantees for the ERM problem in the strongly convex, P{\L} and the nonconvex function class (Theorems \ref{statisticalconvergencethm}, \ref{statisticalconvergencethm_pl}, \ref{statisticalconvergencethm_nonconvex} respectively). These guarantees establish that even with a more general attack model, the algorithm RESIST, irrespective of the function class, solves the ERM problem with an accuracy of $\cO(1/\sqrt{N})$ and with high probability, provided the size of the data sample is $N$. To the best of our knowledge, this is the first work that provides results of the decentralized ERM problem for P{\L} and smooth nonconvex functions in the setting of network failures with a statistical learning rate.

Last but not least, the numerical experiments in Sec.~\ref{numerical section} validate the theoretical findings using real-world datasets, specifically MNIST~\cite{Lecun1998} and CIFAR-10~\cite{krizhevsky2009learning}. For the MNIST dataset, the experiments demonstrate RESIST’s effectiveness on strongly convex loss functions across various system and algorithm parameters, as shown in Sec.~\ref{sec: numericalconvex}, achieving comparable accuracy to other algorithms under diverse settings. For the CIFAR-10 dataset, the experiments in Sec.~\ref{sec: numericalnonconvex} highlight RESIST’s strong performance on nonconvex objective functions and its robustness across different system parameters, algorithmic design choices, and attack strategies.
%
%Last but not least, in the numerical section, this work corroborates the derived theoretical results on real-life datasets, including MNIST and CIFAR-10 datasets. For the MNIST dataset, the experiments illustrate the usefulness of the algorithm with strongly convex loss functions in case of various choices of algorithm design parameters. It has been shown in Section \ref{sec: numericalconvex} that the algorithm can achieve comparable accuracy in various settings compared with other algorithms. For the CIFAR-10 dataset, in Section \ref{sec: numericalnonconvex}, the experiments demonstrate the algorithm performs well with nonconvex objective functions and also in various choices of algorithm design parameters. 

\subsection{Notation}
The following notation is used throughout the paper. The symbol \(\mathbb{R}_{+}\) denotes the set of non-negative real numbers, \(\emptyset\) represents the empty set, and \(\text{diam}(\cdot)\) and \(|\cdot|\) denote the diameter and cardinality of a set, respectively. The probability measure is written as \(\bbP\), expectation as \(\mathbb{E}\), and \(\text{a.s.}\) signifies ``almost surely.'' The space \(L^{\infty}(\Omega)\) refers to functions on the domain \(\Omega\) with bounded essential supremum, and \(\|\cdot\|_{L^{\infty}(\Omega)}\) denotes the \(L\)-infinity norm over \(\Omega\). Graphs are represented as \(\cG(\cN, \cE)\), where \(\cN\) is the set of nodes and \(\cE\) the set of edges. For two nodes \(u\) and \(v\), the edge \(uv\) is considered an incoming edge to node \(v\) from its neighbor \(u\).

Scalars are denoted by regular-faced letters (e.g., \(a, A\)), vectors by bold-faced lowercase letters (e.g., \(\ba\)), and matrices by bold-faced uppercase letters (e.g., \(\bA\)). All vectors are column vectors. The identity matrix is \(\bI\), the vector of all ones is \(\mathbf{1}\), and \((\cdot)^T\) denotes the transpose. For a vector \(\ba\), \([\ba]_k\) denotes its \(k\)-th element. For a matrix \(\bA\), \([\bA]_i\) refers to the \(i\)-th column, \([\bA]_{ij}\) refers to the element in the \(i\)-th row and \(j\)-th column, and \([\bA]_{[a:b]\times[c:d]}\) represents the sub-block spanning rows \(a\) to \(b\) and columns \(c\) to \(d\). Inner products between vectors \(\ba_1\) and \(\ba_2\) are written as \(\langle \ba_1, \ba_2 \rangle\). The \(\ell_2\)-norm of a vector \(\ba\) is denoted by \(\|\ba\|\), while \(\|\bA\|\), \(\|\bA\|_F\), and \(\|\bA\|_{\infty}\) represent the operator, Frobenius, and infinity norms of a matrix \(\bA\), respectively.

For matrices \(\bA\) and \(\bB\) of identical size, \(\bA \leq \gamma \bB\) (for scalar \(\gamma\)) implies entry-wise inequality: \([\bA]_{ij} \leq \gamma [\bB]_{ij}\) for all \(i, j\). The notation \(\bA \succeq \bB\) indicates that \(\bA - \bB\) is positive semidefinite. Scalar comparisons may also depend on a matrix norm; \(f \lesssim_{\bM} g\) implies \(f \leq C(\bM) g\), where \(C(\bM)\) is a constant related to the matrix norm \(\vvvert \cdot \vvvert_{\bM}\). Similarly, \(\bP(h, J) = \Theta(h)\) means \(\|\bP(h, J)\|_F\) is bounded by a constant times \(h\). The notation \(a_k = \bo(b)\) implies that for any \(\epsilon > 0\), there exists \(k_0\) such that \(|a_k| \leq \epsilon b\) for all \(k \geq k_0\).

Finally, \(\nabla\) denotes the gradient of a function, and \(\nabla_k\) is the partial derivative with respect to the \(k\)-th coordinate. For continuously differentiable functions \(f\), the gradient Lipschitz constant \(\LIP(f)\) is defined as \(\LIP(f) = \sup_{\bx, \by; \hspace{0.1cm} \bx \neq \by} \frac{\|\nabla f(\bx) - \nabla f(\by)\|}{\|\bx - \by\|}\).
%
%The following notation is used throughout the rest of the paper. We denote scalars with regular-faced lowercase and uppercase letters (e.g., $a$ and $A$), vectors with bold-faced lowercase letters (e.g., $\ba$), and matrices with bold-faced uppercase letters (e.g., $\bA$). All vectors are taken to be column vectors. At the same time, $[\ba]_k$ denote the $k$-th element of vector $\ba$, also $[\bA]_{ij}$ denote the element of matrix $\bA$ in $i$-th row and $j$-th column, similarly, we denote $[\bA]_{[a:b]\times[c:d]}$ to be the sub-block of matrix $[\bA]$ from row $a$ to row $b$ and column $c$ to column $d$.  Denote $\| \bA\| $ to be the operator norm of matrix $\bA$, $\| \bA\|_F $ to be the Frobenius norm of matrix $\bA$, $\| \bA\|_{\infty} $ to be the infinity norm of matrix $\bA$, respectively. For matrices $\mathbf{A}$ and $\mathbf{B}$ of identical size, and a scalar $\gamma, \mathbf{A} \leq \gamma \mathbf{B}$ provided that $[\mathbf{A}]_{i j} \leq \gamma [\mathbf{B}]_{i j}$ for all $i, j$, which indicates the entry-wise inequality. we use $\mathbb{R}_{+}$ to denote the set of non-negative real numbers. We use $\| \ba\| $ to denote the $\ell_2$-norm of $\ba$, $\emptyset$ is the empty set, $\mathbf{1}$ to denote the vector of all ones of size $M$, and $\bI$ to denote the identity matrix, while $(\cdot)^T$ denotes the transpose operation. The space $L^{\infty}(\Omega)$ denotes the space of functions on the domain $\Omega$ with bounded essential supremum. Given two matrices $\bA$ and $\bB$, the notation $\bA \succeq \bB$ signifies that $\bA - \bB$ is a positive semidefinite matrix, the inequality $ f\lesssim_{\bM} g$ implies $f \leq C(\bM) g$ where the constant $ C(\bM)$ is a function of some matrix norm $\vvvert \cdot \vvvert_{\bM} $ and we use $\norm{\cdot}_{L^{\infty}(\Omega)}$ to denote the $L$-infinity norm with respect to the set $\Omega$. We also use $\bP(h, J) = \Theta(h)$ to denote that $\| \bP(h, J)\|_F $ is bounded by some constant times $h$. We use $\nabla$ to denote the gradient of a function and $\nabla_k$ to denote the partial derivative with respect to $k$-th coordinate. We use $a_k =\bo(b)$ to denote that for every possible constant $\epsilon$, there exists a $k_0$ such that  $\vert a_k \vert \leq \epsilon b$ for all $k\geq k_0$. We also use $\langle \ba_1, \ba_2 \rangle$ to denote the inner product between two vectors. The symbol $\bbP$ represents a probability measure, $ \mathbb{E}$ is the expectation operator, $\text{diam}(\cdot)$ is the diameter of a set, and ``$\text{a.s.}$" implies almost surely. Finally, given a set, $\vert \cdot\vert$ denotes its cardinality, while we use the notation $\cG(\cJ,\cE)$ to denote a graph with the set of nodes $\cJ$ and edges $\cE$ and for any continuously differentiable function $f$, its gradient Lipschitz constant is denoted by $\LIP(f)$ where $ \LIP(f) = \sup_{\bx, \by; \hspace{0.1cm} \bx \neq \by } \frac{\norm{\nabla f(\bx) -\nabla f(\by)}}{\norm{\bx -\by}}$. We also denote the edge $AC$ as the incoming edge to nodes $C$ with its direct neighbor node $A$.

\subsection{Organization}
The rest of the paper is organized as follows. In Sec.~\ref{sec: problem formulation}, we formalize the risk minimization problem, describe the system model, present the decentralized ERM formulation, and define the MITM attack model. Sec.~\ref{sec: theoretical analysis} introduces the RESIST algorithm, outlines the assumptions on graph connectivity required for its theoretical analysis, and develops preliminary results on consensus guarantees under the MITM attack model with coordinate-wise trimmed mean screening. Sec.~\ref{algorithmic convergence preliminaries} provides consensus guarantees for RESIST and establishes preliminary results necessary for subsequent convergence analysis. In Sec.~\ref{sconvex_section1}, we present algorithmic convergence guarantees for strongly convex loss functions under a two-time-scale analysis, where one scale corresponds to algorithmic iterations (time-scale \(s\)) and the other to the total number of discrete actions---encompassing both inter-neighbor communications and local model updates---performed within a synchronous, slotted framework (time-scale \(t\)). Sec.~\ref{sec:nonconvex convergence rate} extends the algorithmic convergence analysis to P{\L} and smooth nonconvex loss functions. Sec.~\ref{mapping} demonstrates how, within the framework of our paper and analysis, Byzantine attacks can be mapped to MITM attacks in decentralized networks. Sec.~\ref{sec_statisticalrate_1} establishes statistical learning rates for strongly convex, P{\L}, and smooth nonconvex loss functions. Numerical results, using real-world datasets, are presented in Sec.~\ref{numerical section} to demonstrate the effectiveness of RESIST. Finally, we conclude the paper in Sec.~\ref{conclusion}, with all proofs and supplementary discussions provided in Appendices~\ref{section*vaidya_10} through~\ref{appendixE}.
%
%The rest of the paper is organized as follows: In the Section \ref{sec: problem formulation}, we formally define the optimization problem we want to solve with its ERM version including the system model and attack model; in Section \ref{sec: theoretical analysis}, we propose our algorithm RESIST along with our screening method Coordinate-Wise Trimmed Mean (CWTM), choice of the weighting matrix and some assumptions on the graph connectivity; in Section \ref{algorithmic convergence preliminaries}, we provide consensus guarantee of our RESIST algorithm along with some preliminaries that will be used in the next section to establish our convergence guarantees; in Section \ref{sconvex_section1}, we provide the algorithmic convergence guarantees of our RESIST algorithm with strongly-convex loss functions in two time scales; in Section \ref{sec:nonconvex convergence rate}, we provide algorithmic convergence guarantees of our RESIST algorithm with P{\L} and smooth nonconvex loss functions; in Section \ref{mapping}, we illustrate how to map the impact of Byzantine attacks to the ones with MITM attacks in the network; in Section \ref{sec_statisticalrate_1}, statistical learning rates are provided with different types of loss functions including strongly convex, P{\L} and smooth nonconvex; then, numerical results are provided in Section \ref{numerical section} to showcase the usefulness of RESIST algorithm with real-life datasets; we conclude our paper in Section \ref{conclusion}; lastly, all the proofs are given in our appendices from Appendix \ref{section*vaidya_10} to Appendix \ref{appendixE}.

\section{Problem Formulation}\label{sec: problem formulation}
\subsection{Background: Statistical and empirical risk minimization}
Let \(f : (\bw,\bz) \mapsto f(\bw,\bz)\) be a non-negative-valued (and possibly regularized) differentiable \emph{loss function} that maps a \emph{model} \(\bw\) and a \emph{data sample} \(\bz\) to the corresponding loss \(f(\bw,\bz)\). Without loss of much generality, we assume the model \(\bw\) to be parametric, i.e., \(\bw \in \mathbb{R}^d\), where \(d\) denotes the dimensionality of the model \(\bw\), such as the number of parameters in a deep neural network. The data sample \(\bz\), on the other hand, is treated as a random variable defined on a probability space \((\Omega, \mathcal{F}, \mathbb{P})\), i.e., \(\bz\) is \(\mathcal{F}\)-measurable and drawn from the sample space \(\Omega\) according to the probability law \(\mathbb{P}\). The main objective in \emph{machine learning} (ML) is to obtain an optimal model \(\bw^*_{\SR}\) that minimizes the expected loss, known as the \emph{statistical risk} \cite{Mohri2012foundations, MLtextbook}:
\begin{align}
    \label{eqn: SRM.equation}
    \bw^*_{\SR} \in \argmin_{\bw \in \mathbb{R}^d} \mathbb{E}_{\mathbb{P}}[f(\bw,\bz)].
\end{align}

A model \(\bw^*_{\SR}\) satisfying \eqref{eqn: SRM.equation} is termed a \emph{statistical risk minimizer} (also referred to as a \emph{Bayes optimal model}). However, in most ML applications, the full distribution of \(\bz\) is rarely known, making the direct computation of \(\mathbb{E}_{\mathbb{P}}[f(\bw,\bz)]\) infeasible. Instead, a finite collection \(\mathcal{Z} := \{\bz_n\}_{n=1}^N\) of data samples is typically drawn according to \(\mathbb{P}\), and an empirical approximation of \eqref{eqn: SRM.equation} is solved:
\begin{align}
    \label{eqn: central.ERM.equation}
    \bw^*_{\ERM} \in \argmin_{\bw \in \mathbb{R}^d} \frac{1}{N} \sum_{n=1}^N f(\bw,\bz_n).
\end{align}
This formulation, referred to as \emph{empirical risk minimization} (ERM), is widely used to approximate \(\bw^*_{\SR}\) when the data distribution is unavailable. Two primary goals of numerically solving the ERM problem \eqref{eqn: central.ERM.equation} in centralized settings are: ($i$) ensuring that the iterative algorithms used for optimization achieve fast algorithmic convergence to a stationary point (e.g., \(\bw^*_{\ERM}\)) of the average empirical loss \(\frac{1}{N} \sum_{n=1}^N f(\cdot,\bz_n)\), and ($ii$) ensuring that the obtained stationary point \(\bw^*_{\ERM}\) exhibits fast statistical convergence (i.e., lower sample complexity) to the statistical risk minimizer \(\bw^*_{\SR}\). \looseness=-1

In this paper, unlike several prior works (cf.~Table~\ref{table:intro.comparison}), we focus on deriving both the algorithmic convergence rate and the statistical learning rate of the ERM solution in scenarios where data samples are not available in a centralized location, necessitating decentralized collaboration. The results are specific to the decentralized setting under malicious attacks and rely on several assumptions about the loss function \(f(\bw, \bz)\), including its classification into function classes such as convex, P{\L}, and smooth nonconvex, which will be formally characterized in subsequent sections. We now describe our framework for decentralized learning.
%
%A model $\bw^*_{\SR}$ that satisfies \eqref{eqn: SRM.equation} is termed a \emph{statistical risk minimizer} (also known as a \emph{Bayes optimal model}). However, one rarely has the knowledge of the entire distribution of $\bz$ in most machine learning applications, which makes the use of the statistical risk $\E_{\bbP}[f(\bw,\bz)]$ infeasible in those scenarios. Instead, one commonly utilizes a finite collection $\cZ := \{\bz_n\}_{n=1}^N$ of data samples drawn according to the law $\bbP$ and solve an empirical variant of \eqref{eqn: SRM.equation} as follows \cite{Mohri2012foundations,MLtextbook}:
%\begin{align}
%    \label{eqn: central.ERM.equation}
%    \bw^*_{\ERM} \in \argmin_{\bw \in \R^d} \frac{1}{N} \sum_{n=1}^N f(\bw,\bz_n).
%\end{align}
%This formulation, which is termed \emph{empirical risk minimization} (ERM), is typically used to solve for $\bw^*_{\ERM}$ when the data distribution is not available. Two goals of solving the $\bw^*_{\ERM}$ or acquiring the estimator of $\bw^*_{\ERM}$ are: ($i$) the iterative algorithms used for optimizing the ERM problem should have a fast algorithmic convergence to a stationary point (e.g., $\bw^*_{\ERM}$) of \eqref{eqn: central.ERM.equation} in centralized settings; and ($ii$) the above-mentioned stationary point $\bw^*_{\ERM}$ should have fast statistical convergence/lower sample complexity (in terms of the number of samples) to a statistical risk minimizer (e.g., $\bw^*_{\SR}$). In this paper, in contrast to several related prior works listed in (cf.~Table~\ref{table:intro.comparison}), we provide both the algorithmic convergence rate (ACR) and the statistical convergence rate (SCR) of the ERM solution for our method. The final set of results, in this case, relies on a number of assumptions on the loss function $f(\bw,\bz)$, stated in the later sections.

\subsection{System model for decentralized learning}
Consider a network of \(M\) nodes---representing agents, smartphones, computers, etc.---modeled as a directed, static, and connected graph \(\cG(\cN, \cE)\), where \(\cN := \{1, \dots, M\}\) is the set of nodes, and \(\cE\) represents the communication links or edges between them. A directed edge \((i, j) \in \cE\) indicates that node \(j\) can directly receive messages from node \(i\), and vice versa for \((j, i)\). The neighborhood set of node \(j\), denoted \(\cN_j\), includes all nodes with a direct link to \(j\): \(\cN_j := \{i \in \cN : (i, j) \in \cE\}\). Each node \(j\) has access only to its local training dataset, \(\cZ_j := \{\bz_{jn}\}_{n=1}^{|\cZ_j|}\), as the complete dataset \(\cZ = \bigcup_{j=1}^M \cZ_j\) is never available at a single location. Without loss of generality, we assume that all nodes have the same number of data samples, i.e., \(|\cZ_j| = N\) for all \(j \in \cN\), resulting in a total of \(NM\) samples across the network. When deriving the statistical learning rates in Sec.~\ref{sec_statisticalrate_1}, we assume that the local datasets \(\cZ_j\) are drawn independently and identically distributed (i.i.d.) from the overall data distribution defined by the probability law \(\bbP\). Extending our results to scenarios where \(\cZ_j\) are not independent and/or identically distributed remains a direction for future work.

To estimate the statistical risk minimizer \(\bw^*_{\SR}\) (cf.~\eqref{eqn: SRM.equation}) in the decentralized setting, the following ERM problem ideally needs to be solved:
\begin{align}\label{eqn: ERM}
\min\limits_{\bw\in \R^d} \frac{1}{MN}\sum\limits_{j=1}^M\sum\limits_{n=1}^N f(\bw,\bz_{jn}) = \min\limits_{\bw\in \R^d}\frac{1}{M}\sum\limits_{j=1}^M f_j(\bw),
\end{align}
where \(f_j(\bw) := \frac{1}{N} \sum_{n=1}^N f(\bw,\bz_{jn})\) represents the \emph{local} empirical risk associated with the data samples \(\{\bz_{jn}\}_{n=1}^N\) in the local dataset at the \(j\)-th node. In the statistical learning literature, under mild assumptions on the data distribution, it is well established that the minimizer of \eqref{eqn: ERM} converges to \(\bw^*_{\SR}\) with high probability at a rate of \(\mathcal{O}(1/\sqrt{MN})\) for strictly convex loss functions~\cite{Vapnik2013nature}, provided the data is centralized at a single location. However, due to the decentralized nature of the dataset, the results in~\cite{Vapnik2013nature} cannot be directly applied in the decentralized setting. Instead, we assume that each node \(j\) learns and updates a local version of the desired global model, denoted by \(\bw_j\), based on its local dataset \(\cZ_j\), and collaborates with other nodes in the network to solve the following \emph{decentralized} ERM problem: \looseness=-1
\begin{align}\label{eqn: decentralized ERM}
    \min\limits_{\{\bw_1,\dots,\bw_M\}} \frac{1}{M}\sum\limits_{j=1}^M f_j(\bw_j) \quad \text{subject to} \quad \forall i \in \cN, j \in \cN, \ \bw_i = \bw_j.
\end{align}

Traditional first-order decentralized learning algorithms iteratively solve \eqref{eqn: decentralized ERM} to learn the desired global model~\cite{predd2006distributed,forero2010consensus,boyd2011distributed,duchi2012dual,AliH.Sayed2014,nedic2018,sun2021decentralized}. In each iteration, these algorithms typically require each node \(j\) to perform two key tasks: ($i$) refine the local model \(\bw_j\) by performing a consensus update with its neighboring nodes through inter-neighbor communication; and ($ii$) update the local model using a local learning rate and gradient information, followed by broadcasting the updated information to its outgoing neighbors. This iterative process continues until certain convergence criteria are met, depending on the specific objectives of the algorithm. While this paper adopts the same general framework for decentralized learning, our focus is on scenarios where malicious actors may compromise the system. The attack model considered in this work is described next.

\subsection{Man-in-the-middle attack model}\label{def of MITM}
%Although robust distributed/decentralized optimization/machine learning algorithms have been developed for decades, almost all the focus is on the Byzantine type of attack, which is the attack that occupies the nodes and may thus poison the local data completely. Like the formulation in Definition 1 in ~\cite{Fang2022BRIDGE}, the best one could hope is to design an algorithm that will be able to solve the decentralized ERM problem defined later in equation \ref{eqn: decentralized ERM} with only respective to the honest set of nodes. In this work, we focus a different attack model which is the Man-in-the-middle attack. The first recorded Man-in-the-middle (MITM) attack in 1903 in radio communication and cryptography, before the invention of the internet, involved Guglielmo Marconi, the Nobel prize winner and inventor of radio. It was then exported into the cybersecurity literature in \cite{Meyer2004MITM} about a decade later. An MITM attack is a cyber threat where an attacker secretly positions themselves between two communicating parties. By intercepting the traffic, the attacker can eavesdrop on the exchange, collect sensitive data, and even modify the communications without the victims' knowledge. This enables the attacker to gain unauthorized access to information that the two parties intended to share privately. The cause of the attack can be noisy communication links, malfunctioning links, or invasion of third-party entities such as malicious attacks or hackers. A detailed definition of the MITM can be found in section \ref{def of MITM}. As for the difference between Byzantine attacks and Man-in-the-middle attacks, Byzantine attacks can only affect the nodes and their associated outgoing edges in the network. In contrast, Man-in-the-middle attacks can only affect edges (communication links) between nodes but have the ability to jump from one edge to another. However, since most of the updates happen locally and most theoretical guarantees derived from the algorithm are respect to local nodes instead of the whole graph and it is mostly non-distinguishable whether your neighbor nodes are being attacked or the neighboring links are being attacked from the perspective of a local node, we can oftentimes treat the two attacks as the same attack from the perspective of a local node including the analysis. A detailed discussion on how is the decentralized Byzantine resilient problem being mapped into the Man-in-the-middle attack problem can be found in Section \ref{mapping}. Also, in Section \ref{mapping}, this work establishes that the MITM attack framework studied in this paper is more general, and it subsumes the Byzantine attack framework. To the best of our knowledge, our results provide the first theoretical guarantees, both algorithmic and statistical, in the presence of MITM attacks within the decentralized optimization and machine learning literature.

In a decentralized system, malicious actors can compromise the system in two primary ways: by targeting nodes or by attacking the communication links between nodes. Node-level attacks, where an adversary overtakes a node and causes it to deviate arbitrarily from the agreed-upon algorithmic protocol without detection, are commonly referred to as the Byzantine attack model and have been extensively studied in the decentralized learning literature (e.g., see \cite{Fang2022BRIDGE} and references therein). In contrast, significantly less is known about attacks focused on network edges, or communication links. One such attack is the \emph{man-in-the-middle} (MITM) attack. While the MITM attack model has a well-established history (cf.~Sec.~\ref{sec:introduction}), this paper examines a significantly more potent variant within the context of decentralized learning. In this dynamic MITM attack model, the adversary is limited to compromising a fixed number of edges at any given time but can dynamically change the targeted edges over time to inflict maximum disruption on the learning system. For instance, in a directed network spanning a geographic region, an attacker could compromise different subsets of communication links between nodes, varying these subsets over time. The challenge in defending against this scenario lies in the fact that neither the attacker’s strategy nor the specific edges under attack are known to the transmitting nodes at any given time. This dynamic and adaptive nature of the MITM attack model makes it significantly more challenging to defend against than traditional Byzantine-resilient decentralized learning approaches, as it allows the adversary to shift its attacks across edges. Furthermore, as discussed in Sec.~\ref{mapping}, this dynamic MITM attack framework subsumes the Byzantine attack model as a special case, enabling a unified analysis under the framework proposed in this paper.
%
%Under the decentralized system model, attacks can happen both at the edges and the nodes. The node attack model also referred to as the Byzantine attack, has been well studied in the literature (see \cite{Fang2022BRIDGE} and the references therein); however, less is known when the attack is concentrated on the network edges. One type of such attack is termed a Man-in-the-middle attack. In this work, we study a more general Man-in-the-middle attack model where the location of edge attacks can change over time. Such an attack is not hard to visualize; for instance, one could consider the case of a direct network over a geographic region where the agents are trying to communicate with each other to achieve some learning goal. The attacker compromises a subset of links between nodes in a way such that this subset of links changes with time. The added complexity lies in the fact that neither the attacker's strategy nor the location of attacked edges is known to the transmitters at any time instant. Our Man-in-the-middle attack model is more adversarial than the conventional Byzantine attack model because of the attacker's ability to jump attacks between edges.

Mathematically, we assume a synchronous, slotted model for the decentralized system, where each action (e.g., communication or computation) is executed within a predefined time slot, indexed by the iteration \(t\) (referred to as time-scale \(t\)). Let \(\cE_b(t) \subset \cE\) denote the set of edges compromised by malicious actors at a given iteration \(t\), and let \(\cB(t) \subset \cN\) represent the set of source nodes associated with these compromised edges---nodes that transmit information along edges targeted by the attack at time \(t\). For a node \(j\), define \(\cN_j^r(t)\) as the set of neighboring nodes with uncompromised outgoing edges to \(j\). The set of neighbors whose information has been compromised during transmission to node \(j\) can then be defined as \(\cN_j^b(t) := \cN_j \setminus \cN_j^r(t)\), where \(\cN_j\) is the set of all neighboring nodes of \(j\). %Furthermore, let \(\cB_j(t) := \cB(t) \cap \cN_j\) denote the set of neighboring nodes of \(j\) associated with compromised outgoing edges. 
Note that \(\cB(t)\), the set of source nodes corresponding to compromised edges at time \(t\), can be expressed as \(\cB(t) := \bigcup_{j \in \cN} \cN_j^b(t)\). The maximum number of compromised edges incoming to any node in the network at any time instance is defined as \(b := \sup_{0 \leq t < \infty} \sup_j |\cN_j^b(t)|\), representing a parameter that quantifies the adversary’s strength within the system.

\begin{exam}
As an example of the dynamic MITM attack model, consider the case of \(b = 1\). For a representative node \(j\), at time instance \(t_1\), MITM attacks occur on its incoming edges, with the compromised source set being \(\cN_j^b(t_1) = \{u\}\), where node \(u\) is a direct neighbor of \(j\). The transmitted information from node \(u\) to node \(j\) may be altered to an arbitrary value, expressed as \(m_{uj}^\prime(t_1) = m_{uj}(t_1) + \zeta_{uj}(t_1)\), where \(\zeta_{uj}(t_1)\) can be any value, either dependent or independent of \(m_{uj}(t_1)\) (the original data transmitted from node \(u\) to node \(j\)). At another time instance \(t_2\), the attack may shift from edge \(uj\) to edge \(vj\), resulting in the compromised source set \(\cN_j^b(t_2) = \{v\}\). The transmitted information from node \(v\) to node \(j\) can then be altered as \(m_{vj}^\prime(t_2) = m_{vj}(t_2) + \zeta_{vj}(t_2)\), where \(\zeta_{vj}(t_2)\) can again be any value, either dependent or independent of \(m_{vj}(t_2)\) (the original data transmitted from node \(v\) to node \(j\)). This dynamic attack model applies to every node \(j\) in the network, with \(j\) being used here as a representative example. \looseness=-1
\end{exam}
%
%We first lay out certain precise terminology that will be used while analyzing these attacks. Let $\cE_b(t) \subset \cE$ be the set of all edges over which attack can happen at a given iteration $t$ and suppose $\cB(t) \subset \cN $ be the set of source nodes corresponding to these compromised edges. Let $\cN_j^r (t)$ denote the set of neighboring nodes of node $j$ associated with uncompromised edges to $j$. The set of neighbors of node $j$ whose information has been compromised while transmitting to node $j$ can then be defined as $\cN_j^b(t):= \cN_j\setminus\cN_j^r(t)$ where $\cN_j $ is the set of all neighboring nodes of node $j$. Let $\cB_j(t) = \cB(t) \cap \cN_j   $ be the corresponding set of neighbor nodes of $j$ with compromised edges. Then we have the following definition $b$ for the maximum number of compromised edges as follows: $b=\sup_{0 \leq t\leq \infty} \sup_j|{\cB_j(t)}|$. For instance, at some time instant $t_1$, if MITM attacks happen at the incoming edges of node $J$ with set $\cN_j^b(t_1)= \{A\}$ where and node $A$ is the direct neighbors of node $J$, then the transmitted information from node $A$ to node $J$ can be altered to any arbitrary value as $m_{AJ}(t_1)^` = m_{AJ}(t_1) + \zeta(t_1)$ where $\zeta(t_1)$ can be any value that is either dependent or independent of $m_{AJ}(t_1)$ which is defined as the original data transmitted from node $A$ to node $J$. Then for some other time instant $t_2$, attack jumps from edge $AJ$ to edge $CJ$, then the set $\cN_j^b(t_2)= \{C\}$ and the corresponding information transmitted from node $C$ to node $J$ can be altered to any arbitrary value as $m_{CJ}(t_2)^` = m_{CJ}(t_2) + \zeta(t_2)$ where $\zeta(t_2)$ can be any value that is either dependent or independent of $m_{CJ}(t_2)$, which is defined as the original data transmitted from node $C$ to node $J$.

\subsection{Problem statement}
MITM attacks present unique challenges for solving the decentralized ERM problem stated in \eqref{eqn: decentralized ERM}. Such attacks can strategically alter messages transmitted over compromised edges, causing the learned models to deviate significantly from the desired solution. For instance, DGD~\cite{zeng2018nonconvex}, which lacks mechanisms to screen or filter out compromised information, is particularly vulnerable to accumulating falsified data during consensus-based updates. This accumulation ultimately prevents convergence to the solution of \eqref{eqn: decentralized ERM}. To address these challenges, robust statistics-based data aggregation methods, such as trimmed mean or median, are often employed in Byzantine-resilient decentralized learning frameworks to filter out potentially falsified information~\cite{Fang2022BRIDGE}. However, the dynamic nature of MITM attacks introduces additional complexities. Even with robust data aggregation, targeted attacks can significantly delay information mixing within the network. In extreme cases, without adequate assumptions on network connectivity, adversaries could compromise edges in a way that permanently isolates some nodes, preventing effective information exchange.

Similar to challenges faced in Byzantine-resilient decentralized learning~\cite{Fang2022BRIDGE}, achieving an exact solution to the decentralized ERM problem under MITM attacks is fundamentally infeasible. Instead, the best achievable outcome from an optimization perspective is to approximate the solution to \eqref{eqn: decentralized ERM} within a reasonable error margin. This limitation arises because traditional consensus-based methods rely on doubly stochastic mixing matrices, which ensure exact averaging across the network by combining both incoming and outgoing information during the collaboration (i.e., consensus) phase. However, under MITM attacks, compromised edges and the necessary screening mechanisms disrupt proper information exchange, resulting in non-doubly stochastic mixing matrices. This deviation prevents exact averaging and, consequently, hinders convergence to the exact ERM solution, even when employing recent methods like push-pull approaches~\cite{xin2018linear,Pu2021pushpull}.

In this context, our primary goal is to develop an algorithm that can provably address the decentralized ERM problem in the presence of MITM attacks, while providing two key guarantees from an optimization perspective. First, we aim to establish consensus guarantees, quantifying the extent to which the local models \(\bw_j\) agree with one another as a function of the number of algorithmic iterations (time-scale \(s\)). This addresses the consensus constraint \(\forall i \in \cN, j \in \cN, \ \bw_i = \bw_j\) in \eqref{eqn: decentralized ERM}. Second, we seek to derive convergence rates for the approximate solution to \eqref{eqn: decentralized ERM}, ensuring efficient convergence for various classes of local empirical risk functions \(f_j\). These rates are analyzed as a function of both the time-scale \(s\) (algorithmic iterations) and the time-scale \(t\) (total number of discrete actions in the system, including communications and updates), making the results broadly applicable from an optimization perspective. \looseness=-1

Moreover, while achieving the exact solution of \eqref{eqn: decentralized ERM} is infeasible unless the local functions \(f_j\) are identical across nodes, our secondary goal is to demonstrate that the proposed algorithm can still generalize well to unseen data by reliably estimating the statistical risk minimizer. Although our algorithmic solution of \eqref{eqn: decentralized ERM} may not perfectly align with the desired solution, we later show that the proposed algorithm implicitly solves a weighted version of the decentralized ERM problem, formulated as:
\begin{align}\label{eqn: restricted decentralized ERM mitm}
    \min\limits_{\{\bw_1,\dots,\bw_M\}} \sum\limits_{j=1}^M c_j f_j(\bw_j) \quad \text{subject to} \quad \forall i \in \cN, j \in \cN, \ \bw_i = \bw_j, 
\end{align}
where \(c_j \in [0,1]\) and \(\sum_j c_j = 1\). Importantly, the expected value of this weighted decentralized ERM problem aligns with that of the statistical risk minimization problem. Consequently, from a statistical learning theory perspective, we aim to establish the statistical learning rates at which the empirical solution obtained by the proposed algorithm approaches the statistical risk minimizer defined in \eqref{eqn: SRM.equation}.
%
% Under the above attack model in the network, the decentralized ERM problem, as stated in \eqref{eqn: decentralized ERM}, may not be solvable using traditional methods such as DGD~\cite{zeng2018nonconvex} because the attack can easily drift the learning model away from the ERM solution through edge communications. For instance, DGD, without any robust filtering mechanism, when iterated in the presence of edge attacks, could accumulate falsified information in the consensus steps, thereby not converging to the solution of \eqref{eqn: decentralized ERM}. Even if one were to use some data filtering operations such as the trimmed mean, median, etc., the edge attacks can filter nodes sporadically over the network, thereby possibly delaying information mixing in the network. In the worst case, without any assumptions on the network's connectivity, an adversarial attacker can even mount an attack on the edges, and after filtering operation, some nodes are permanently separated from the network. Finally, even if one could defend against such an attack using the existing methods such as BRIDGE \cite{Fang2022BRIDGE}, the best one could hope for is to solve some weighted version of the ERM problem given by 
% \begin{align}\label{eqn: restricted decentralized ERM mitm}
%     \min\limits_{\{\bw_j : j \in \cN\}} \sum\limits_{j \in \cN} c_j f_j(\bw_j) \ \text{subject to} \ \forall i,j \in  \cN, \ \bw_i = \bw_j, 
% \end{align}
% where $c_j \in [0,1]$ and $\sum_j c_j =1$. The reason behind this weighted version of the ERM problem lies in the fact that, in general, in the absence of network attacks, only push-pull consensus-based methods~\cite{xin2018linear, Pu2021pushpull} are known to obtain solutions to the exact version of the ERM problem since these methods mix both incoming and outgoing information resulting in a doubly stochastic mixing matrix. Then performing consensus via a doubly stochastic mixing results in the exact consensus\footnote{By exact consensus we mean that $c_j$ is identical for all $j$ in \eqref{eqn: restricted decentralized ERM mitm}.}. On the other hand, in the presence of network attacks, the information sent out by any nodes is not guaranteed to be received fully by the receivers because of the attack behavior and screening mechanism, thus resulting in a non-doubly stochastic mixing matrix. Thus, it can not provide convergence to the exact minima even when push-pull methods are utilized. 

% % Along similar lines, we can also consider a more adversarial attack scenario where we cannot obtain a global consensus guarantee :
% % \begin{align}\label{eqn: unrestricted decentralized ERM}
% %    \min\limits_{\{\bw_j : j \in \cN\}} \sum\limits_{j \in \cN} c_j f_j(\bw_j) \ \text{subject to} \ \forall i,j \in \cV \subseteq \cN, \ \bw_i = \bw_j, 
% % \end{align}
% % where $c_j \in [0,1]$ and $\sum_j c_j =1$. Observe that in \eqref{eqn: unrestricted decentralized ERM}, the consensus constraint is only required to be satisfied over some subset $\cV$ of the entire set of nodes $\cN$. In simple cases, this set $\cV$ will be considered to be $\cN$. However, there can be sophisticated attack models where one cannot guarantee a global consensus. For instance, it could be the case that the attacker falsifies certain nodes along with their outgoing edges, leading to a mixed Man-in-the-middle Byzantine attack.  

% After introducing the system model and attack model, we are now ready to present our proposed algorithm RESIST (Robust dEcentralized learning with conSensus gradIent deScenT) as listed in Algorithm \ref{gradient descent algorithm} in the next section.

\section{RESIST: Resilient Decentralized Learning Using Consensus Gradient Deecent}\label{sec: theoretical analysis}

\begin{algorithm}{}
\caption{RESIST (\textbf{R}esilient d\textbf{E}centralized learning using con\textbf{S}ensus grad\textbf{I}ent de\textbf{S}cen\textbf{T})}
\label{gradient descent algorithm}
\begin{algorithmic}[1]
\REQUIRE Local empirical loss functions $f_j$ for all $j \in \cN$, maximum number of compromised edges across all iterations and neighborhoods $b$, parameter $J > 1$ controlling the frequency of gradient-based local model updates, step size $h$, and maximum number of iterations $T_{\max}$
\STATE \textbf{Initialize:} Set $s \gets 0$ and initialize $\bw_j(0)$ for all $j \in \cN$
\FOR{$t=0, 1, \dots, T_{\max}-1$}
\IF {$(t+1 )\mod J \neq 0$} 
    \STATE Broadcast $\bw_j(t)$ for all $j \in \cN$
    \STATE Receive $\bw_i(t)$ at each node $j \in \cN$ from all $i \in \cN_j$
    \STATE $\bw_j(t+1) \gets \textsf{CWTM}(\{\bw_i(t)\}_{i \in \cN_j \cup \{j\}}, b), \quad \forall j \in \cN$ \label{RESIST: CWTM} \hfill \emph{// Coordinate-wise trimmed mean subroutine}
\ELSE
    \STATE $\bw_j(t+1) \gets \bw_j(t) - h \nabla f_j(\bw_j(t)), \quad \forall j \in \cN$ \hfill \emph{// Local gradient-based model update step} \label{alg:BRIDGE.update}
    \STATE $s \gets s+1$
\ENDIF
\ENDFOR
\ENSURE Final local models $\bw_j(T_{\max})$ for all $j \in \cN$
\end{algorithmic}
\end{algorithm}

In this section, we formally introduce the proposed algorithm, RESIST (Algorithm~\ref{gradient descent algorithm}), designed to enable efficient decentralized learning while remaining resilient to MITM attacks, which may dynamically shift from one edge to another, as described in the previous section. To facilitate the subsequent analysis of the algorithmic convergence rates and statistical learning rates of RESIST, we also present the main assumptions on the connectivity of the decentralized network in Sec.~\ref{ssec:assumptions_graph}. Additionally, we establish preliminary results in Secs.~\ref{section supporting lemma} and \ref{section:Geometric consensus rate along coordinates}, characterizing the resilience of RESIST in terms of consensus properties under MITM attacks. 

RESIST is a fully decentralized algorithm, meaning it does not require knowledge of the global network topology, and nodes only communicate with their immediate neighbors. Additionally, each node has access only to its own local empirical loss function (i.e., local dataset) and does not access the local data of other nodes. RESIST is a first-order algorithm, as it updates the local models every few iteration indices \( t \) using the local gradient information \( \nabla f_j \) at that time. The primary parameters required for RESIST at each node include the maximum number of edges within the neighborhood of any node expected to be under attack in any slot index \( t \), denoted by \( b \); the step size \( h \); the maximum number of iterations \( T_{\max} \) for which the algorithm should run; and a positive integer parameter \( J > 1 \), which determines how often the local gradient information is used to update the local models---specifically, a gradient step is taken every \( J \)-th iteration index \( t \).
%
%The algorithm is fully decentralized, and each node takes its local dataset as input. The algorithm also takes certain parameters as input, such as the maximum number of compromised edges over all iterations and all neighborhoods $b$, step-size $h$, and the parameter $J$ which quantifies the number of communication rounds between the gradient updates %number of communication rounds before the state updates parameter $J-1$. 

As described in Algorithm~\ref{gradient descent algorithm}, RESIST updates local models through two primary mechanisms. First, in Steps~4--6, each node broadcasts its local model to its outgoing neighbors, receives models from its incoming neighbors, and then updates its own model using the \textit{coordinate-wise trimmed mean} (CWTM) subroutine, described in Algorithm~\ref{CWTM}. This subroutine aggregates information using a coordinate-wise trimmed mean, helping mitigate the impact of MITM attacks on the communication links. This filtered aggregation process occurs over \(J-1\) consecutive iterations \(t\), ensuring robust information exchange before the gradient-based update. Second, in Step~8, nodes update their models using local gradients. Since this gradient-based update is performed independently by each node without relying on information from neighbors, it remains secure against MITM attacks, even if network edges remain compromised.
%
%Each node calls a filtering sub-routine defined in Algorithm \ref{CWTM} that aggregates information from neighbors based on a coordinate-wise trimmed mean (CWTM) algorithm. This aggregation is performed for $J-1$ time steps, after which the nodes update their models using local gradients. Since the gradient update phase happens locally at each node, we are assured that this update is secured from any attack even if the network edges are still compromised.

Since RESIST takes a gradient step only at every \( J \)-th index \( t \), while in the intervening indices nodes engage in local communication and update their local models without taking a gradient step, RESIST operates on two distinct time scales. The first time scale, denoted as \( t \), represents the total number of discrete actions performed within the algorithm, encompassing both inter-neighbor communication-based updates and gradient-based updates of the local models. The second time scale, denoted as \( s \), corresponds to algorithmic iterations---specifically, the number of updates to the local models based on local gradient information. We sometimes refer to \( t \) as the \textit{faster} time scale and \( s \) as the \textit{slower} time scale. Note that updates to the local model occur at both time scales; however, within time scale \( s \), updates are exclusively based on local gradient information, and no inter-neighbor communication takes place at that time.
%
%Also, $t$ and $s$ are the faster iteration and slower iteration index in Algorithm \ref{gradient descent algorithm} correspondingly. More formally, the two indices in the algorithm, $t$ and $s$, correspond to two different time scales at which updates are made to the local models. The time scale associated with the index $t$ is a faster time scale, referred to as the $t$-time scale. The time scale associated with the index $s$ is a slower time scale, referred to as the $s$-time scale.

We now briefly discuss the CWTM filtering subroutine (Algorithm~\ref{CWTM}), which aggregates information from incoming edges along with the node’s own information at a coordinate-wise level. The procedure involves removing the \(b\) largest and \(b\) smallest values in each coordinate before computing the average of the remaining values to update the model at a node. Mathematically, following prior works that use CWTM for filtering~\cite{Vaidya2012matrix, su2015byzantine, yang2019byrdie, Fang2022BRIDGE}, for any iteration \(t\), the \(k\)-th coordinate of the received models \(\bw_i(t)\) at node \(j\), where \(i \in \cN_j\), defines the following sets: \looseness=-1
\begin{align}
\underline{\cN}_j^{k}(t) &:=\argmin\limits_{\cX: \cX\subset \cN_j, \vert \cX\vert =b }\sum\limits_{i\in \cX}[\bw_i(t)]_k,\label{lowerset}\\
\overline{\cN}_j^{k}(t) &:=\argmax\limits_{\cX: \cX\subset \cN_j, \vert \cX\vert =b }\sum\limits_{i\in \cX}[\bw_i(t)]_k, \quad \text{and}\label{upperset}\\
\cC_j^{k}(t) &:=\cN_j\setminus\left\{\underline{\cN}_j^{k}(t)\bigcup\overline{\cN}_j^{k}(t)\right\}.
\end{align}
Here, \(\underline{\cN}_j^{k}(t)\) is the \textit{lower set} (nodes with incoming edges to \(j\) that have the smallest \(b\) values in the \(k\)-th coordinate at time \(t\)), \(\overline{\cN}_j^{k}(t)\) is the \textit{upper set} (nodes with incoming edges to \(j\) that have the largest \(b\) values), and \(\cC_j^{k}(t)\) is the center set (remaining nodes with incoming edges after filtering the extreme values). If multiple sets satisfy the filtering criteria, a random selection is made. After filtering, the information from nodes in the center set is assigned equal weights, and the final average is computed in Step~\ref{weight assignment in center set}. To ensure that the center set is non-empty and the weights remain positive in Step~\ref{weight assignment in center set} of Algorithm~\ref{CWTM}, the filtering parameter must satisfy \(b < \frac{\vert\cN_j\vert +1}{2}\).
%
%We now briefly describe the CWTM algorithm used in the filtering sub-routine defined in Algorithm \ref{CWTM}. First, we define some filtered sets based on the CWTM literature \cite{Vaidya2012matrix, su2015byzantine}. At any iteration $t$, the $k$-th coordinate of the model vector $\bw_i(t)$ at node $i \in \cN$ gives rise to some filtered sets as follows:
%
% \begin{align}
% \overline{\cN}_j^{k}(t) &:=\argmin\limits_{\cX: \cX\subset \cN_j, \vert \cX\vert =b }\sum\limits_{i\in \cX}[\bw_i(t)]_k,\label{lowerset}\\
% \underline{\cN}_j^{k}(t) &:=\argmax\limits_{\cX: \cX\subset \cN_j, \vert \cX\vert =b }\sum\limits_{i\in \cX}[\bw_i(t)]_k, \quad \text{and}\label{upperset}\\
% \cC_j^{k}(t) &:=\cN_j\setminus\left\{\overline{\cN}_j^{k}(t)\bigcup\underline{\cN}_j^{k}(t)\right\}.
% \end{align}
%
% To succinctly elucidate the definition above, we refer to $\overline{\cN}_j^{k}(t) $ as the lower set, $\underline{\cN}_j^{k}(t) $ as the upper set and $ \cC_j(t)$ is the center set for node $j$ generated from the $k$-th coordinates of the receiving information. By lower set, we mean the nodes associated with the lowest $b$ values received from neighbors of node $j$ along $k$-th coordinate at any time $t$ and similarly, the upper set\footnote{In case there are multiple sets of nodes that satisfy the criteria, a random set is selected.}. After filtering out the extreme values in the upper and lower sets, information received corresponding to the nodes in the center set is assigned equal weights. Note that in order for the weights to be positive in step \ref{weight assignment in center set} of Algorithm \ref{CWTM}, we require the filtering parameter $b$ to be less than $\frac{\vert\cN_j\vert +1}{2}$. The CWTM algorithm can now be summarized in Algorithm \ref{CWTM}.

\begin{algorithm}[t]
\caption{Coordinate-wise Trimmed Mean (\textsf{CWTM})} \label{CWTM}
\begin{algorithmic}[1]
\REQUIRE Upper bound \( b \) on the number of potentially compromised incoming edges per node, local models \( \bw_i(t) \) received by node \( j \) from all \( i \in \cN_j \), and local model \( \bw_j(t) \) at node \( j \)
\FOR{$k= 1, \dots, d$}
    \STATE $\underline{\cN}_j^{k}(t) \gets \argmin\limits_{\cX: \cX\subset \cN_j, |\cX| =b }\sum\limits_{i\in \cX}[\bw_i(t)]_k$ \hfill \emph{// Identify nodes with the $b$ smallest values}
    \STATE $\overline{\cN}_j^{k}(t) \gets \argmax\limits_{\cX: \cX\subset \cN_j, |\cX| =b }\sum\limits_{i\in \cX}[\bw_i(t)]_k$ \hfill \emph{// Identify nodes with the $b$ largest values}
    \STATE $\cC_j^{k}(t) \gets \cN_j\setminus\left\{\underline{\cN}_j^{k}(t)\cup\overline{\cN}_j^{k}(t)\right\}$ \hfill \emph{// Filter out nodes with the $b$ smallest and $b$ largest values}
    \STATE $[\bw^{\textsf{\textsc{cwtm}}}_j(t)]_k \gets \frac{1}{\vert\cN_j\vert-2b+1} \sum\limits_{i\in \cC_j^k(t) \cup \{j\}}[\bw_i(t)]_k$ \label{weight assignment in center set} \hfill \emph{// Compute trimmed mean}
\ENDFOR
\ENSURE $\bw^{\textsf{\textsc{cwtm}}}_j(t)$
\end{algorithmic}
\end{algorithm}
% 
% \begin{algorithm}[t]
% \caption{The Coordinate-wise trimmed mean (CWTM)} \label{CWTM}
% \begin{algorithmic}[1]
% \REQUIRE Maximum number of potential compromised edges $b$, $\bw_j(t),\forall j \in \cN$
% \FOR{$j=0, 1, \dots, \cN$}
% \STATE Receive $\bw_i(t)$ at each node $j \in \cN$ from all $i\in \cN_j $
% \FOR{$k= 1, \dots, d$}
% \STATE $\overline{\cN}_j^{k}(t) :=\argmin\limits_{\cX: \cX\subset \cN_j, \vert \cX\vert =b }\sum\limits_{i\in \cX}[\bw_i(t)]_k$,
% \STATE $\underline{\cN}_j^{k}(t) :=\argmax\limits_{\cX: \cX\subset \cN_j, \vert \cX\vert =b }\sum\limits_{i\in \cX}[\bw_i(t)]_k$,
% \STATE $\cC_j^{k}(t) :=\cN_j\setminus\left\{\overline{\cN}_j^{k}(t)\bigcup\underline{\cN}_j^{k}(t)\right\}.$
% \STATE $[\bw^{CWTM}_j(t)]_k= \frac{1}{\vert\cN_j\vert-2b+1} \sum\limits_{i\in \cC_j^k(t)\cup\{j\}}[\bw_i(t)]_k$\label{weight assignment in center set}
% \ENDFOR
% \ENDFOR
% \ENSURE $\bw^{CWTM}_j(t), \quad \forall \quad j \in \cN$
% \end{algorithmic}
% \end{algorithm}

Next, we highlight the parallels and distinctions between the BRIDGE algorithm~\cite{Fang2022BRIDGE} and the proposed RESIST algorithm. When \( J = 2 \), RESIST and BRIDGE are nearly identical in principle, differing primarily in the choice of step size: BRIDGE requires a diminishing step size, whereas RESIST operates with a constant step size \( h \). However, the two algorithms differ significantly in their ability to handle network attacks and their respective defense mechanisms. While BRIDGE is designed to counter Byzantine attacks, which originate at the node level, RESIST is built to defend against MITM attacks, which occur at the edge level and can dynamically shift between different edges over time. At the same time, RESIST can also mitigate Byzantine attacks. Indeed, in Sec.~\ref{mapping}, we formally show that any Byzantine attack can be mapped to an MITM attack, meaning RESIST naturally provides resilience against both. A natural question arises as to whether multi-step consensus---i.e., multiple rounds of communication (quantified by parameter \( J \)) before updating the local models---is necessary. The dynamic nature of MITM attacks necessitates this approach in RESIST to ensure sufficient mixing of information and mitigate the effects of adversarially manipulated edges.
%
%Next, we draw some parallels and differences between the BRIDGE algorithm from \cite{Fang2022BRIDGE} and our current RESIST algorithm. For $J=2$, RESIST and BRIDGE, in principle, are almost identical to one other and only differ in the choice of step-size $h$ where BRIDGE requires a diminishing step-size, and RESIST works with a constant $h$. However, these algorithms differ in functionality regarding the network attacks they can handle, as well as their defense mechanism against network attacks. While BRIDGE is designed to handle only Byzantine attacks, RESIST can handle Man-in-the-middle attacks and even Byzantine attacks\footnote{In a subsequent section \ref{mapping} we show that any Byzantine attack can be mapped to Man-in-the-middle attack and hence can be handled by RESIST.}. The nature of a Man-in-the-middle attack necessitates the use of multi-step consensus rounds (quantified by parameter $J$) in RESIST so as to facilitate sufficient mixing of information. 

Finally, although analytical tools from the Byzantine-resilient literature suffice for analyzing decentralized methods robust to node-level attacks~\cite{vaidya2013byzantine, Fang2022BRIDGE, Lie2022Byzantine}, they do not directly apply to MITM attacks within the RESIST framework. Instead, key techniques from Byzantine-resilient consensus and optimization must be carefully adapted to accommodate the dynamic MITM attack model considered in this paper. Moreover, while standard methods exist for decentralized optimization over time-varying graphs~\cite{nedic2015distributed}, they break down in the presence of network attacks. To analyze the RESIST algorithm, we first extend relevant results from Byzantine-resilient consensus to the MITM attack setting in Secs.~\ref{section supporting lemma} and \ref{section:Geometric consensus rate along coordinates}. Before presenting these results, we state the graph connectivity assumption that enables RESIST’s resilience. This assumption is then used to show that the filtering subroutine CWTM (Algorithm~\ref{CWTM}) effectively protects nodes from falsified incoming information under MITM attacks, focusing exclusively on the consensus phase of the algorithm without considering gradient updates.
% 
% While the tools in Byzantine resilient literature can be used to handle node attacks \cite{vaidya2013byzantine, Fang2022BRIDGE, Lie2022Byzantine}, such tools cannot be directly deployed to tackle MITM attacks. Instead, one needs to carefully reinvent some of these tools from Byzantine resilient literature that can handle our Man-in-the-middle attack model. Further, the distributed optimization literature has standard techniques to optimize over time-varying graphs (see \cite{nedic2015distributed}), but such techniques are rendered useless in the presence of network attacks. It is in this regard that we develop the RESIST algorithm by adapting some of the earlier results in Byzantine resilient literature in the context of our Man-in-the-middle attack model. 
% 
% Having summarized the RESIST defined in Algorithm \ref{gradient descent algorithm}, we now want to show that the filtering sub-routine CWTM in Algorithm \ref{CWTM} indeed protects the nodes from incoming falsified information. But in order to do so, we first need to describe a crucial assumption on network connectivity.

\subsection{Graph connectivity assumption for RESIST}\label{ssec:assumptions_graph}
We begin with a couple of definitions that are essential for stating the graph connectivity assumption. The first definition introduces the concepts of \emph{source node} and \emph{source component} in a directed graph.
\begin{defi}[Source node and source component]
A node in a directed graph \(\cH\), with node set \(\cN(\cH)\) and edge set \(\cE(\cH)\), is termed a \emph{source node} if it has directed paths to all other nodes in the graph. A collection of source nodes forms a \emph{source component} of the graph.
\end{defi}
%
% \begin{defi}
%     A source node is one that has directed paths to all the other nodes in the graph. A collection of source nodes is called a source component.
% \end{defi}

The next definition introduces the notion of \textit{filtered graph topologies} associated with the original graph \(\cG(\cN, \cE)\). This concept is inherently linked to the CWTM operation performed within RESIST (Algorithm~\ref{CWTM}) but applies more broadly to any variant of RESIST that filters out information arriving on \(2b\) incoming edges of a node.  
\begin{defi}[Filtered graph topology]\label{def1a}  
The set of \textit{filtered graph topologies} of the graph \(\cG(\cN, \cE)\) for a given parameter \(b\) is defined as the set \(\cT_{\cF}\) of all filtered graphs of \(\cG\), where each filtered graph \(\cH \in \cT_{\cF}\) is obtained by removing exactly \(2b\) incoming edges at each node in \(\cG\). Formally,
\begin{align*}
    \cT_{\cF} := \bigg\{ \cH \mid \cN(\cH) = \cN(\cG), \ \cE(\cH) \subset \cE(\cG), \ 
    \cH \text{ is obtained by removing exactly } 2b \text{ incoming edges at each node}, \\
    \text{where each } \cH \text{ represents a specific instance of edge removals across all nodes.}
    \bigg\}.
\end{align*} 
Let \(\tau\) denote the cardinality of \(\cT_{\cF}\), i.e., \(\tau := |\cT_{\cF}|\), which we refer to as the number of filtered graphs associated with the underlying graph \(\cG\) for a given parameter \(b\).  
\end{defi}
In each iteration \( t \) of RESIST where the CWTM operation is performed, the algorithm effectively operates on one of the filtered graphs \(\cH \in \cT_{\cF}\). However, the set of filtered graph topologies \(\cT_{\cF}\) (and thus its cardinality \(\tau\)) depends only on the original graph \(\cG\) and the parameter \( b \); it does not depend on \( t \) or on which specific links are actually attacked during each iteration of the RESIST algorithm. Strictly speaking, we should write \(\cT_{\cF}(\cG, b)\) and \(\tau(\cG, b)\) to explicitly indicate their dependence on \(\cG\) and \( b \), but we suppress this notation for simplicity. Additionally, while \(\tau\) may be large depending on the topology of \(\cG\), it remains a finite quantity.
%
% \begin{defi}\label{def1a}
% Let us define the following filtered graph topology as the set ${\mathcal{T}}_{\mathcal{F}}$ of subgraphs of $\cG(\cN, \mathcal{E})$ as follows.
% \begin{align}
% {\mathcal{T}}_{\mathcal{F}}=  \bigg\{\cH \mid \cN(\cH) \subset \cN(\cG); \hspace{0.2cm}  \cE(\cH) \subset \cE(\cG) ;\quad \cH \text { is obtained by removing any possible combination of}\quad \\ \nonumber 2b \quad\text {links}  \text { from } \mathcal{E}  \text { at every node} 
% \bigg\}.
% \end{align}
% \end{defi}
% 
% This topology includes all the potential filtered graphs over all the possible time index $t$, and it is dependent only on the graph property and the choice of filtering parameter $b$. Let $\tau$ denote $\left|{\mathcal{T}}_{\mathcal{F}}\right|$ where $ \tau$ depends on $\cB$, i.e., the set of source nodes corresponding to the compromised edges, depends on the underlying network, and it is finite.

To ensure sufficient mixing of information within RESIST after the CWTM filtering operation---and, in particular, to guarantee that no node becomes isolated after filtering and that the weight assignments in Step~\ref{weight assignment in center set} of Algorithm~\ref{CWTM} remain non-negative---we require the following assumption on network connectivity:
\begin{assum}[Sufficient network connectivity]\label{claim2}
The graph \(\cG(\mathcal{N}, \mathcal{E})\) is assumed to be sufficiently connected, meaning every filtered graph in the set \(\mathcal{T}_{\mathcal{F}}\) contains at least one source component with cardinality greater than one. \looseness=-1
\end{assum}
% 
% In order to generate the upper set and lower set defined in equation \ref{lowerset} and \ref{upperset}, and to prevent any nodes from being isolated after the proposed filtering mechanism along with ensuring non-negativity of weights in step \ref{weight assignment in center set} in Algorithm \ref{CWTM}, we need the assumption as following:
% \begin{assum}\label{claim2}[Sufficient network connectivity]
%     The graph $\cG(\mathcal{N}, \mathcal{E})$ is assumed to be sufficiently connected in the sense that all filtered graphs in the set ${\mathcal{T}}_{\mathcal{F}} $ contain at least one source component with some cardinality greater than $1$. 
% \end{assum}

Note that a network connectivity assumption similar to Assumption~\ref{claim2} also appears in the literature on Byzantine-resilient optimization and learning~\cite{su2015byzantine, Fang2022BRIDGE}. However, since Byzantine attacks target nodes rather than edges, the corresponding assumptions in these works apply to subgraphs obtained by removing nodes along with their edges from the original graph. Specifically, the assumption in those works requires that each \textit{reduced subgraph} contains a source component of cardinality at least \(b+1\), where \(b\) is the maximum number of nodes under attack in the network. In contrast, the nature of MITM attacks necessitates the use of filtered graphs rather than reduced subgraphs. A filtered graph is obtained by removing only incoming edges into each node, whereas a reduced subgraph results from the removal of nodes along with their associated edges. Heuristically, for graphs with sufficiently high edge density (defined as the ratio of existing edges to the maximum possible edges in the graph), filtering edges rather than removing nodes generally results in a sparser structure compared to reduced subgraphs in Byzantine-resilient settings. This is because filtering edges alone leads to a lower edge density than removing both nodes and edges. Consequently, filtered graphs are, in general, less likely to contain a large number of source nodes compared to reduced subgraphs, where paths between nodes are more prevalent.
% \begin{rema}
% Note that in Assumption \ref{claim2}, we require the source component to have cardinality greater than one as opposed to the prior Byzantine resilient optimization/learning literature where the cardinality of the source component is taken to be at least $b+1$ where $b$ is the number of Byzantine nodes (see \cite{su2015byzantine, Fang2022BRIDGE}). Now, in our case, the method of generating filtered graphs is different from their generation of reduced graphs, wherein we only remove compromised incoming edges while the prior works remove the faulty nodes as well as their edges. In principle, for graphs with sufficient connectivity, our method of generating filtered graphs will tend to generate more sparse graphs as opposed to prior works. This is because subgraphs generated by just removing edges tend to have a lower density (ratio between the edges present in a graph and the maximum number of edges that the graph can contain) than subgraphs generated by removing both edges and nodes. Finally, it is not hard to see that a lower-density graph will in general be less likely to have a large number of source nodes compared to the case of high-density graphs, where it is more likely to encounter a path between any two nodes.
% \end{rema}

\subsection{Supporting lemma for the information mixing step in RESIST}\label{section supporting lemma}
We now present a supporting lemma that establishes that the CWTM-based information mixing step (also referred to as the consensus step), Step~\ref{RESIST: CWTM} in Algorithm~\ref{gradient descent algorithm}, ensures that the updated information at every node in the \( k \)-th coordinate is derived solely from information received through uncompromised edges. 

To this end, consider an arbitrary iteration \( t \) such that \( (t+1) \mod J \neq 0 \), and fix an arbitrary coordinate index \( k \in \{1, \dots, d\} \). Define the vector \( \bOmega(t) \in \R^M \), whose elements correspond to the \( k \)-th coordinate of the iterates \( \bw_j(t) \) for all nodes, stacked into the vector \( \bOmega(t) \). Note that most quantities related to the \( d \)-dimensional optimization in this paper, including \( \bOmega(t) \), inherently depend on the coordinate index \( k \). However, since \( k \) is chosen arbitrarily, we often omit this explicit dependence in this and subsequent sections to simplify notation.
% 
% Let us pick an arbitrary index $k \in \{1,\dots,d\}$ and define a vector $\bOmega(t)\in \R^{M}$ whose respective elements correspond to the $k$-th element of the iterate $\bw_j(t)$ of all nodes. Note that most of the variables that correspond to the multi-dimensional model parameter, including $\bOmega(t)$ in our discussion in this section, depend on the index $k$; however, since $k$ is arbitrary, we will often drop this explicit dependence on $k$ in many instances for simplicity of notation. 

In the following lemma, we establish that the consensus step in Algorithm~\ref{gradient descent algorithm} ensures that the update at each node in the \( k \)-th coordinate is computed exclusively using uncompromised information. Specifically, we show that for \( \bOmega(t) \in \R^M \), the update can be expressed as:
\begin{align}\label{eqn: nonfaulty.update}
    \bOmega(t+1) = \bY_k(t) \bOmega(t),
\end{align}
where \( \bY_k(t) \) is a matrix that assigns zero weights to contributions from compromised incoming edges. The explicit structure of \( \bY_k(t) \), referred to as the \textit{mixing matrix}, which depends on both the iteration index \( t \) and the coordinate index \( k \), is detailed in the following lemma.
% 
% In the next lemma, we first show that the consensus step in Algorithm \ref{gradient descent algorithm} update at every node in the $k$-th coordinate can be expressed using only uncompromised information. Especially, for $\bOmega(t) \in \R^M $ we can write
% \begin{align}\label{eqn: nonfaulty.update}
%     \bOmega(t+1)=\bY_k(t)\bOmega(t).
% \end{align}
% where $\bY_k(t)$ matrix has zeros corresponding to compromised incoming edges, and the explicit choice of weights will be provided in the following lemma. 

\begin{lemm}\label{weight assign lemma}
Let \( \bW(t) \in \mathbb{R}^{M \times d} \) be the state matrix whose \( i \)-th row corresponds to the transpose of the state vector \( \bw_i(t) \in \mathbb{R}^d \) at node \( i \), as given in Algorithm~\ref{gradient descent algorithm}. Under Assumption~\ref{claim2}, the mixing step (Step~\ref{RESIST: CWTM}) in Algorithm~\ref{gradient descent algorithm}, for any \( k \in \{1, \dots, d\} \) and any iteration \( t \) such that \( (t+1) \mod J \neq 0 \), can be equivalently expressed as:
\begin{align}
    [\bW(t+1)]_k = \bY_k(t) [\bW(t)]_k,
\end{align}
where the entries of \( \bY_k(t) \), the mixing matrix with zero entries corresponding to compromised incoming edges, are given below (for notational convenience, the iteration index \( t \) is omitted from various quantities in the following expression, though these quantities within the mixing matrix remain implicitly \( t \)-dependent):
\begin{equation}\label{elements in M}
[\bY_k]_{ji}=\begin{cases}
\frac{1}{2(\vert\cN_j\vert-2b+1)},& i\in\cN^r_j\cap\cC_j^k,\\
\frac{1}{\vert\cN_j\vert-2b+1},&i=j,\\
\sum\limits_{i'\in\cN^b_j\cap\cC_j^k}\frac{\theta_{i'}^k}{q_j^k(\vert\cN_j\vert-2b+1)}\\\qquad +\sum\limits_{i'\in\cN^r_j\cap\cC_j^k}\frac{\theta_{i'}^k}{q_j^k(\vert\cN_j\vert-2b+1)},&i\in \overline{\cN}_j^{k}\cap\cN_j^r, \hspace{0.2cm} \theta_{i'}^k \in (0,1),\\
\sum\limits_{i'\in\cN^b_j\cap\cC_j^k}\frac{1-\theta_{i'}^k}{q_j^k(\vert\cN_j\vert-2b+1)}\\\qquad+\sum\limits_{i'\in\cN^r_j\cap\cC_j^k}\frac{1-\theta_{i'}^k}{q_j^k(\vert\cN_j\vert-2b+1)},&i\in \underline{\cN}_j^{k}\cap\cN_j^r, \hspace{0.2cm} \theta_{i'}^k \in (0,1),\\
0,&\text{otherwise},
\end{cases}
\end{equation}
for the case when \( q_j^k := b - b_j^* + b_j^k > 0 \). Here, \( b_j^* := | \cN_j^b | \) denotes the actual (but unknown) number of nodes in the graph that have compromised outgoing edges to node \( j \) in iteration \( t \). The sets \( | \cN_j^b | \) and \( | \cN_j^r | \), both functions of \( t \), are defined in Section~\ref{def of MITM}, while \( b_j^k \) represents the number of nodes with compromised outgoing edges to \( j \) that remain in the filtered set \( \cC_j^k \) in iteration \( t \). The condition \( q_j^k > 0 \) arises in scenarios where at least one node in \( \cC_j^k \) has a compromised link to \( j \), or the actual number of nodes with compromised links to \( j \) is fewer than \( b \), or both. On the other hand, when \( q_j^k := b - b_j^* + b_j^k = 0 \), meaning that all nodes in \( \cC_j^k \) have uncompromised links to node \( j \) in iteration \( t \), the matrix \( \bY_k(t) \) takes the following form:
\begin{equation}\label{elements in MM}
[\bY_k]_{ji}=\begin{cases}
\frac{1}{\vert\cN_j\vert-2b+1},&i\in \{j\}\cup \cC_j^k,\\
0,&\text{otherwise}.
\end{cases}
\end{equation}
\end{lemm}

The proof of this lemma is provided in Appendix~\ref{weightmatrixYlemmaprove}. To further clarify the weight assignments within the mixing matrix, we also present a simple illustrative example in Appendix~\ref{example of weight assignment}.

\begin{rema}
This lemma, along with the discussion in the next section and the analysis in Appendix~\ref{section*vaidya_10}, parallels the corresponding discussion and analysis in~\cite{Vaidya2012matrix} for Byzantine attacks. However, due to the nature of MITM attacks---which result in filtered graphs rather than reduced subgraphs---these results must be explicitly derived under the MITM attack model. Appendix~\ref{section*vaidya_10} provides this necessary derivation. While not the primary contribution of this work, it is included for completeness and self-containment.
\end{rema}

\subsection{Geometric mixing rate for consensus along coordinates}\label{section:Geometric consensus rate along coordinates}
Using the characterization of the mixing matrix in Lemma~\ref{weight assign lemma} for coordinate-wise mixing in RESIST, we now state that the product of mixing matrices, \( \bY_k(t)\bY_k(t-1)\cdots\bY_k(0) \), converges geometrically to a rank-one stationary mixing matrix. This result is critical in deriving the consensus rates of RESIST along each coordinate. In this section, we initially focus on the mixing-based updates to analyze the role of the parameter \( J \) in RESIST. Specifically, we consider the scenario where \( J \) is large enough that the condition \( (t+1) \mod J = 0 \) never applies, thereby isolating the effects of the consensus step. Our primary objective in this section is to outline the implications of Lemma~\ref{weight assign lemma} for geometric mixing along each coordinate, while the full technical analysis is deferred to Appendices~\ref{section*vaidya}--\ref{section*vaidya1}.
% 
% Using the equivalent weight assignment from Lemma \ref{weight assign lemma}, we next show that the product of mixing matrices $\bY(t)\bY(t-1)\cdots\bY(0) $ converges geometrically to a stationary mixing matrix. This will be crucial in developing the consensus rates of RESIST defined in Algorithm \ref{gradient descent algorithm}. 
% 
% For Section \ref{section supporting lemma} and \ref{section:Geometric consensus rate along coordinates}, we assume that the parameter $J$ and thus time index $t$ could extend to infinity in order to see the behaviors of consensus in the case of no gradient updates. Note that the following analysis in Section \ref{section supporting lemma} and \ref{section:Geometric consensus rate along coordinates} seems similar compared to the analysis in \cite{Vaidya2012matrix}, however, due to the different properties of the attack model, we re-derive the results that fit our analysis in Appendix \ref{section*vaidya_10}.

To formally express the geometric mixing behavior, we define a transition matrix \( \bPhi(t,t_0) \) that captures the product of mixing matrices \( \bY_k(t) \) from \eqref{elements in M} and \eqref{elements in MM}, omitting the subscript \( k \) for notational simplicity. This transition matrix propagates information from time index \( t_0 \leq t \) to \( t \) and is given by:
\begin{equation}
    \bPhi(t,t_0) :=\bY(t)\bY(t-1)\cdots\bY(t_0).
\end{equation}  
If Assumption~\ref{claim2} on sufficient network connectivity of \( \cG \) holds, then from the discussion and analysis in Appendices~\ref{section*vaidya}--\ref{section*vaidya1}, it follows that:
\begin{align}\label{eqn: limit}
\lim\limits_{t\rightarrow\infty}\bPhi(t,0)=\bone {\bc}^T,
\end{align}  
where the vector \( {\bc} \in \R^M \) satisfies \( [{\bc}]_j\geq 0 \) and \( \sum_{j=1}^{M}[{\bc}]_j=1 \). The discussion and analysis in Appendix~\ref{section*vaidya_10} further guarantee that this convergence is geometric. Specifically, removing the assumption that \( J \) is very large and considering any \( t_0 \leq t \) with \( t_0 \) and \( t \in [lJ, (l+1)J-2] \) for any \( l = 0,1,2, \dots \), it follows from Appendix~\ref{section*vaidya_10} that:
\begin{equation} \label{13}
\left|[\bPhi(t,t_0)]_{ji}-[{\bc}]_i\right|\leq ( 1-\beta^{\tau M} )^{\left\lfloor\frac{t-t_0}{\tau M}\right\rfloor}, 
\end{equation}
where \( \beta := \frac{\alpha}{4b} \) with \( \alpha :=  \frac{1}{M-2b+1} \), and \( \tau \) denotes the cardinality of the set of filtered graph topologies (see Definition~\ref{def1a}).
% 
% From the definition of $\bY_k(t)$ in Lemma \ref{weight assign lemma} we get that every row of the matrix $\bY_k(t)$ satisfies Corollary \ref{claim1} (deferred to appendix sections \ref{section*vaidya}-\ref{section*vaidya1}.) from \cite{Vaidya2012matrix}. 
%  Next, corresponding to the definition of $\bY_k(t)$ from \eqref{elements in M}, \eqref{elements in MM} and dropping the subscript $k$ for ease of notation, we define a transition matrix $\bPhi(t,t_0)$ from some time index $t_0 \leq t$ to $t$, i.e.,
% \begin{equation}
%     \bPhi(t,t_0) :=\bY(t)\bY(t-1)\cdots\bY(t_0).
% \end{equation} 
% Then, if Assumption~\ref{claim2} on sufficient network connectivity of $\cG$ is satisfied then
% \begin{align}\label{eqn: limit}
% \lim\limits_{t\rightarrow\infty}\bPhi(t,0)=\bone {\bc}^T,
% \end{align}
% where the vector ${\bc} \in \R^M$ satisfies $[{\bc}]_j\geq 0$ and $\sum_{j=1}^{M}[{\bc}]_j=1$. 
% 
% Until \eqref{eqn: limit} in this section, we assume the parameter $J$ and thus time index $t$ could extend to infinity. However, Algorithm \ref{gradient descent algorithm} requires $J$ and $t$ to be finite. In particular, for any $t_0 \leq t$, $t_0$ and $t \in [lJ, (l+1)J-2]$ for any $l= 0,1,2, \cdots$, we have 
% \begin{equation} \label{13}
% \left|[\bPhi(t,t_0)]_{ji}-[{\bc}]_i\right|\leq ( 1-\beta^{\tau M} )^{\left\lfloor\frac{t-t_0}{\tau M}\right\rfloor}, 
% \end{equation}
% where $\beta = \frac{\alpha}{4b} $ with $\alpha =  \frac{1}{M-2b+1} $ and $\tau $ is the cardinality of the filtered graph topology (see Definition \ref{def1a}). For detailed steps on how to derive \eqref{eqn: limit} and \eqref{13}, please refer to Appendix \ref{section*vaidya_10}.

The geometric mixing characterization in \eqref{13} of the mixing steps in RESIST is fundamental in determining the appropriate choice of the parameter \( J \) in the algorithm. By selecting \( J \) appropriately and substituting \( t - t_0 = J-2 \) in \eqref{13}, we ensure that the \( k \)-th coordinate of the local model parameter at each node reaches a state sufficiently close to a weighted agreement (consensus), where the weights are given by the entries of the vector \( \bc \) from \eqref{13}, referred to as the \textit{}consensus vector.
% 
% The above geometric mixing from \eqref{13} plays a crucial role in the selection of parameter $J$ in the RESIST algorithm. By carefully choosing the parameter $J$ and substituting $t-t_0 =J-2$ in \eqref{13}, we can ensure that the $k$-th coordinate of the local model parameter at each node reaches sufficiently close to a weighted consensus where the weights are given by the entries of consensus vector $\bc$ from \eqref{13}.  

\section{Preliminaries for Algorithmic Convergence Guarantees}\label{algorithmic convergence preliminaries}
In this section, we develop certain preliminary results that will be used to derive the convergence guarantees for the RESIST algorithm for the general decentralized optimization problem \eqref{eqn: decentralized ERM} under various classes of loss functions. In the purview of ERM formulation for \eqref{eqn: decentralized ERM}, we start with any fixed set of data points across all nodes given by $\cZ = \bigcup_{j \in \cN}\cZ_j $ where $\cZ_j = \{\bz_{ij}\}_{i = 1}^N$ for any fixed $N > 0$. Next, we suppress the data notation by simply working with local functions of the form $f_j(\cdot):= \frac{1}{N} \sum\limits_{i=1}^N f_j(\cdot; \bz_{ij})$. Note that in RESIST, the datasets $\cZ_j$ are made available locally at each node $j \in \cN$ at the very start and from there onward the algorithm updates deterministically at each node via the local gradients of the form $\nabla f_j(\cdot):= \frac{1}{N} \sum\limits_{i=1}^N \nabla f_j(\cdot; \bz_{ij})$. Hence, in this section, we will omit the data dependency and simply work with local functions $f_j(\cdot): \mathbb{R}^d \to \mathbb{R}$ mapping the $d$-dimensional model space to the reals.  

First, we dive into the property of the product of the $\bY_{k}(t)$ matrices.
Let\footnote{In the product notation $\prod_{i}^j$, the matrix for the top index $j$ will appear on the extreme left of the matrix product sequence. This is referred to as the backward product (see \cite{leizarowitz1992infinite}).} 
\begin{align}
    \bQ_{k}( s) =  \prod\limits_{r= J \lfloor \frac{t}{J} \rfloor }^{J \lfloor \frac{t}{J} \rfloor + J -2} \bY_{k}(r)  \label{cwtm1}
\end{align}
where $s := J\lfloor \frac{t}{J} \rfloor$. Observe that $ \bQ_{k}( s) = \bPhi(J \lfloor \frac{t}{J} \rfloor + J -2,J \lfloor \frac{t}{J} \rfloor)$ where $\bPhi(\cdot,\cdot)$ is the transition matrix defined in Section \ref{section:Geometric consensus rate along coordinates}. Then, the update from the RESIST algorithm can be adapted to the $s$-time scale as follows:
\begin{align}
    \hspace{1.5cm} [{\bW}(s+1)]_k &=  \bQ_{k}( s)[{\bW}(s)]_k - h [{\bT}(s)]_k ;  \hspace{0.2cm}  \label{scr1}\\
[{\bT}(s+1)]_k &= [\nabla F(\bW(s+1))]_k  \label{dst1}
\end{align}
where $\nabla F(\bW(t)) \in \mathbb{R}^{M \times d} $ is defined as the gradient matrix with $i^{th}$ row given by $[\nabla f_i(\bw_i(t))]^{T} $ where $\bw_i(t)$ is the $i^{th}$ row of $ \bW(t)$ and the transition from iteration $s$ to $s+1$ happens in the iteration $sJ + J -1$ of the $t$-time scale. Note that the update \eqref{dst1} above involving the matrix variable $\bT(s)$ may seem redundant at first, but it significantly eases out the notations later. We now provide a corollary similar to Lemma \ref{geometricsupplementlemma} in Section \ref{section*vaidya1010}, which will be used later to obtain rates of consensus and convergence for the RESIST algorithm. 

\begin{coro}\label{coro1}
   Under Assumption \ref{claim2} and for $J>1$, the sequence of matrices $\{\bQ_k(s)\}_{s=0}^{\infty}$ satisfies the following bound for any $i,j \in \{1,\cdots,M\}$:
\begin{align}
      \bigg\lvert \bigg[ \prod_{s=0}^S \bQ_{k}( s)\bigg]_{ji} - [\bc_k]_{i}\bigg \rvert &\leq ( 1-\beta^{\tau M} )^{\left\lfloor\frac{S(J-1)-1}{\tau M}\right\rfloor} \label{coro1_b1*}
\end{align}
For any $S > \frac{\tau M}{J-1} $ where $\bc_k$ is the transpose of row vector of the infinite backward product  $ \prod_{s=0}^{\infty} \bQ_k(s)$, i.e., $$\prod_{s=0}^{\infty} \bQ_k(s) = \prod_{t=0}^{\infty} \bPhi(t,0) =\mathbf{1}\bc_k^T =\bQ_k^{\pi}.$$ where we denote $\bQ_k^{\pi}$ as the stationary mixing matrix with non-uniform weights. Furthermore, for any $J > \tau M +1$ and any $s \geq 0$ we have that:
\begin{align}
    \bigg\lvert \bigg[  \bQ_{k}( s)\bigg]_{ji} - [\bc_k(s)]_{i}\bigg \rvert &\leq ( 1-\beta^{\tau M} )^{\left\lfloor\frac{(J-2)}{\tau M}\right\rfloor} \label{coro1_b2*}
\end{align}
where $\bc_k(s)$ is the transpose of row vector of the infinite backward product $ \prod_{i=s}^{\infty} \bQ_k(i)$, i.e., $$\prod_{i=s}^{\infty} \bQ_k(i) = \mathbf{1}\bc_k(s)^T = \bQ_k^{\pi}(s)$$ and $\bQ_k^{\pi}(s) $ satisfies 
\begin{align}
    \bQ_k^{\pi}(s) & = \bQ_k^{\pi}(s+1)\bQ_k(s) \label{coro1_b3*}
\end{align}
for any $s \geq 0$ with $  \bQ_k^{\pi}(0) :=  \bQ_k^{\pi}$.
\end{coro}

\begin{proof}
    By construction of the mixing matrix $\bY_k(t)$ from \eqref{elements in M}, \eqref{elements in MM} in Lemma \ref{weight assign lemma} we get that $ \bQ_{k}( s)$ from \eqref{cwtm1} for any $s$ is a scrambling matrix (see section \ref{section*vaidya1} for definition)  for $J > \tau M +1$. Then for $S > \frac{\tau M}{J-1} $ we have the bound \eqref{coro1_b1*} from Lemma \ref{geometricsupplementlemma} (section \ref{section*vaidya1010}) and \eqref{13}.
    
For obtaining the second inequality \eqref{coro1_b2*} first observe that multiplying matrices in the tail sequence $\{\bQ_k(i)\}_{i=s}^{\infty}$ for any $s \geq 0$ again results in a scrambling matrix $ \prod_{i=s}^{\infty} \bQ_k(i)$ (Lemmas \ref{geometlemma}, \ref{lemmascramble} in section \ref{section*vaidya1010}) and this matrix will have identical rows, say $\bc_k(s)$. Then, using Lemma \ref{geometricsupplementlemma}, the second inequality follows. Finally, the last equality of Equation \ref{coro1_b3*} follows directly from the definition of the infinite backward product of matrices.  
\end{proof}

Observe that the infinite product $\prod_{i=s}^{\infty} \bQ_k(i) $ in the above Corollary \ref{coro1} is equal to the transition matrix given by $\lim_{t \to \infty}\bPhi(t,sJ) $ along $k$-th coordinate. This infinite product can be viewed as a stationary mixing matrix $ \bQ_k^{\pi}(s) $ with non-uniform weights. Due to the time-varying nature of the row stochastic weight matrices $\bY_k(t)$ in the RESIST algorithm, it is very hard to directly derive a recursion on the exact consensus error due to the uncertainty of the attacker's behavior along with the screening mechanism. By exact consensus error we mean the norm $ \norm{\frac{\mathbf{1}\mathbf{1}^T}{M}[{\bW}(s)]_k  - [{\bW}(s)]_k  }$ where $\mathbf{1} \in \mathbb{R}^{M}$and by recursion we mean the following bound:
$$ \norm{\frac{\mathbf{1}\mathbf{1}^T}{M}[{\bW}(s+1)]_k  - [{\bW}(s+1)]_k  } \leq \rho\norm{\frac{\mathbf{1}\mathbf{1}^T}{M}[{\bW}(s)]_k  - [{\bW}(s)]_k  } + e(s),$$
for some $\rho \geq 0$ and some bounded error $e(s)$.
This is simply because if were to average the update \eqref{scr1} then on the right hand side we cannot get $\frac{\mathbf{1}\mathbf{1}^T}{M}[{\bW}(s)]_k $ as the matrices $\bQ_{k}( s), \frac{\mathbf{1}\mathbf{1}^T}{M}$ may not commute. However, an inexact averaging via $ \bQ_k^{\pi}(s)$ in \eqref{scr1} would alleviate this problem and using \eqref{coro1_b3*} from Corollary \ref{coro1} we then obtain the following recursive bound:
$$ \norm{\bQ_k^{\pi}(s+1)[{\bW}(s+1)]_k  - [{\bW}(s+1)]_k  } \leq \rho\norm{\bQ_k^{\pi}(s)[{\bW}(s)]_k  - [{\bW}(s)]_k  } + e(s),$$
for some $\rho \geq 0$ and some bounded error $e(s)$.

To make the above idea of inexact averaging concrete, we first define certain averaging operators that will be instrumental in the convergence analysis of the RESIST algorithm.
\begin{defi}
   For any $\bA \in \mathbb{R}^{M \times d}$ where $d \geq 1$, the approximate averaging operator $ \widehat{(\cdot)}^{k,s}$ and exact averaging operater $ \overline{(\cdot)}$ are defined as:
    \begin{itemize}
        \item $ \widehat{(\cdot)}^{k,s} : \bA \mapsto {\bQ}^{\pi}_k(s)\bA$\vspace{0.1cm}
        \item $ \overline{(\cdot)} : \bA \mapsto \frac{\mathbf{1}\mathbf{1}^T}{M}\bA$
    \end{itemize}
    and these operators commute\footnote{The operators commute due to the linearity of $\nabla$ operator. By linearity of $\nabla$ operator we mean that $ \nabla (c_1 f_1 + c_2 f_2) = c_1\nabla f_1 +c_2\nabla f_2$ for any scalars $c_1, c_2$ and differentiable functions $f_1, f_2$.} with the $\nabla (\cdot)$ and $[\cdot]_k$ operators.
\end{defi}
We note that any matrix $\bA(s)$ that depends on $s$ when acted on by the operator $ \widehat{(\cdot)}^{k,s}$ or the operator $\overline{(\cdot)} $ results in the matrix $ \widehat{\bA}^{k,s}(s)$ or $\overline{\bA}(s) $ respectively. Similarly, the gradient matrix $ \nabla F(\bW(s))$ when acted on by the operator $ \widehat{(\cdot)}^{k,s}$ or the operator $\overline{(\cdot)} $ results in the gradient matrix $ \nabla\widehat{ F}^{k,s}(\bW(s))$ or $\nabla\overline{ F}(\bW(s))$ respectively.

Next, we define some error sequences that capture the difference between the exact averaging (ideal case without attack) and approximate averaging caused by the uncertainty of the attackers and the screening mechanism of the RESIST algorithm. Those will help us in proving the convergence of the RESIST algorithm.
\begin{defi}\label{deferrorseq}
    Let $ \{\xi^1_k(s)\}_s$, $ \{\xi^2_k(s)\}_s$, $ \{\xi^3_k(s)\}_s$, $ \{\xi^4_k(s)\}_s$, $ \{\xi^5_k(s)\}_s$, $ \{\xi^6(s)\}_s$ be the error sequences that satisfy the relations below for all $k$ and $s$:
    \begin{align}
        \xi^1_k(s) & := {\norm{ [\widehat{\bW}^{k,s}(s)]_k - [{\bW}(s)]_k  } }, \\
        \xi^2_k(s) & := {\norm{ [\widehat{\bT}^{k,s}(s)]_k - { [{\bT}(s)]_k} } }, \\
        \xi^3_k(s) & := {\norm{ [\widehat{\bW}^{k,s}(s)]_k - [\overline{\bW}(s)]_k} }, \\
        \xi^4_k(s) & := {\norm{ [\widehat{\bT}^{k,s}(s)]_k - [\overline{\bT}(s)]_k } }, \\
        \xi^5_k(s) & :=   {\norm{ {[\bW(s)]_k} - [\overline{\bW}(s)]_k} },\\
        \xi^6(s) & := {\norm{ {\bw^*} - \widehat{\bw}^s(s)} },
    \end{align}
    where $\bw^* \in \argmin_{\bw} \frac{1}{M}\sum\limits_{j=1}^M f_j(\bw)$ and\footnote{When local functions are strongly convex, we have $\bw^* = \argmin_{\bw} \frac{1}{M}\sum\limits_{j=1}^M f_j(\bw)$.} for any $s \geq 0$
    \begin{align}
        \widehat{\bw}^s(s) &= \begin{bmatrix}
        \sum\limits_{j=1}^M [\bc_1(s)]_{j} [\bw_j(s)]_1 \\
         \sum\limits_{j=1}^M [\bc_2(s)]_{j}[\bw_j(s)]_2 \\
        \vdots \\
       \sum\limits_{j=1}^M [\bc_k(s)]_{j}[\bw_j(s)]_k \\
        \vdots \\
         \sum\limits_{j=1}^M [\bc_d(s)]_{j} [\bw_j(s)]_d 
    \end{bmatrix},
    \end{align}
    where the weights $ [\bc_k(s)]_{j}$ for any $k, j$ are defined in Corollary \ref{coro1}.
\end{defi}
The sequences in the above definition are termed as error sequences because they either measure the distance of vectors at the $k$-th coordinate from their consensus vectors (both exact and inexact) or they measure the distance of a coordinate-wise inexact averaged vector $\widehat{\bw}^s(s)$ to the optimal $ \bw^*$. In particular, $\xi^1_k(s)$ and $\xi^5_k(s)$ are referred to as the consensus error while $\xi^6(s)$ is referred to as the averaged iterate error.

We are now ready to develop the consensus guarantees for RESIST in Algorithm \ref{gradient descent algorithm}. Before that, we briefly describe the terms in exact and inexact consensus. When all the local vectors $\bw_i(t)$ for all $i\in \cN$ from our algorithm \ref{gradient descent algorithm} converge to the same exact vector, we refer to that as an exact consensus. Algorithms with doubly stochastic averaging, such as the DGD, can achieve exact consensus. However, when only a subset of the local vectors $\bw_i(t)$ for $i \in \cV$, where $\cV \subset \cN$, from our algorithm \ref{gradient descent algorithm}, converge to the same exact vector, we refer to that as an inexact consensus. Algorithms with row stochastic averaging, such as ours, can possibly achieve this type of consensus.

\subsection{Convergence analysis of exact and inexact consensus in $s$-time scale of RESIST algorithm}\label{conv_analysissec1}
Throughout this section, we assume that the local functions $f_i$ for all $i \in \cN$ are continuously differentiable.
 We first present a lemma that establishes the limiting behavior of the tracker update. By tracker, we mean the matrix variable for storing gradients denoted by $\bT(s)$ from \eqref{dst1}.

\begin{lemm}\label{trackerlemma}
    The average tracking vector $[\overline{\bT}(s)]_k $ tracks the average gradient $  [\nabla \overline{F}(\bW(s))]_k$ along any dimension $k$, i.e., $ [\overline{\bT}(s)]_k =  [\nabla \overline{F}(\bW(s))]_k$. Further, suppose the sequence $\{\bW(s)\}_s$ converges to some limit $\bW^*$. Then we have that $[\overline \bT(s)]_k \xrightarrow[]{s \to \infty} \nabla \overline{F} (\bW^*) $ for any dimension $k$.
\end{lemm}

\begin{proof}
    Applying $\overline{(\cdot)}$ operator to $[{\bT}(s)]_k$ yields: 
    \begin{align}
        [\overline{\bT}(s)]_k & = [\nabla \overline{F}(\bW(s))]_k.
    \end{align}
  Then, taking the limit $s \to \infty$ followed by continuity of $\nabla f_i$ yields the result.
\end{proof}

\begin{lemm}\label{wkbarlemma}
    Under Assumption \ref{claim2}, the sequence $\{[\bW(s)]_k\}_s$ for any $k$ satisfies the following bound:
    \begin{align}
        \xi^5_k(s+1) & \leq M^{\frac{3}{2}}( 1-\beta^{\tau M} )^{\left\lfloor\frac{(J-2)}{\tau M}\right\rfloor}\xi^5_k(s) \nonumber  + h \norm{[\overline{\bT}(s)]_k- [{\bT}(s)]_k} ,
    \end{align}
where $\beta = \frac{\alpha}{4b} $ with $\alpha =  \frac{1}{M-2b+1} $.
\end{lemm}
The proof of this lemma is in Appendix \ref{wkbarlemmaproof}. Also, the reason why existing algorithms that tackle Byzantine attack can not be simply adapted into our setting is explained in Remark \ref{remark_timevar1}


\begin{lemm}\label{lemxi1}
    Under Assumption \ref{claim2}, the sequence $ \{\xi^1_k(s) \}_s$ satisfies the following recursion for any $s \geq 0$:
    \begin{align}
      \xi^1_k(s+1) & \leq   M^{\frac{3}{2}}(\sqrt{M}+1)( 1-\beta^{\tau M} )^{\left\lfloor\frac{(J-2)}{\tau M}\right\rfloor} \xi^1_k(s) \nonumber + h(\sqrt{M}+1)\xi^2_k(s).
    \end{align}
\end{lemm}
The proof of this lemma is in Appendix \ref{lemmaxi1proof}.

Observe that by carefully choosing $J$ in the inequalities from Lemmas \ref{wkbarlemma} and \ref{lemxi1}, one can get a geometric decay on the exact and inexact consensus errors with some residual terms. Note that for obtaining geometric decay for the error terms $ \xi^1_k(s)$ and $ \xi^5_k(s)$, we only require that $ M^{\frac{3}{2}}(\sqrt{M}+1)( 1-\beta^{\tau M} )^{\left\lfloor\frac{(J-2)}{\tau M}\right\rfloor} < 1 $ and thus $M^{\frac{3}{2}}( 1-\beta^{\tau M} )^{\left\lfloor\frac{(J-2)}{\tau M}\right\rfloor} < 1 $ in Lemmas \ref{lemxi1} and \ref{wkbarlemma}, respectively. Hence, any $J$ large enough will yield geometric decay rates.


We now state the smoothness assumption for the local functions. We emphasize that unless otherwise stated, we will suppress the data notation by simply working with local functions of the form $f_i(\cdot):= \frac{1}{N} \sum\limits_{j=1}^N f_j(\cdot; \bz_{ij})$ where $f_i(\cdot): \mathbb{R}^d \to \mathbb{R}$ maps the $d$-dimensional model space to the reals. Then, any assumption on $f_i$ will only pertain to its first argument, i.e., the model variable and not the data.  
  
\begin{assum}\label{asumpt1_nonconvex}
    For all $i \in \{1,\dots, M\}$, the function $f_i : \mathbb{R}^d \to \mathbb{R}$ is $L$-gradient Lipschitz continuous and lower bounded, i.e., $ \inf_{\bw}f_i(\bw) > -\infty$.
\end{assum}

Note that functions required in all the assumptions in Section \ref{algorithmic convergence preliminaries} and \ref{sconvex_section1} are only respective to the first argument, which is the model parameter rather than the data samples. Later on in Section \ref{sec_statisticalrate_1}, all the assumptions mentioned will be respective to both arguments, which are the model parameters and the data samples. As a direct consequence of Assumption \ref{asumpt1_nonconvex}, every $f_i$ is coordinate-wise $L$-gradient Lipschitz continuous. The lower boundedness assumption implies $ \argmin f_i \neq \emptyset$ for any $i \in \{1,\cdots,M\}$.



\begin{lemm}\label{tkhatlemma}
 Let $\bw_i^* \in \argmin_{\bw} f_i(\bw) \quad \forall \quad i \in \{1,2,\dots,M\}, \hspace{0.2cm}  \bw^* \in \argmin_{\bw} f(\bw),$ where $ f(\cdot) := \frac{1}{M} \sum\limits_{i=1}^M f_i(\cdot)$. Then under Assumptions \ref{claim2} and \ref{asumpt1_nonconvex}, the sequence $\{[{\bT}(s)]_k\}_s$ for any $k$ satisfies the following bounds:
    \begin{align}
           \xi^2_k(s)  \leq   (\sqrt{M} + 1)L\sqrt{M}{\sum\limits_{k=1}^d \xi^1_k(s)} +   (\sqrt{M} + 1)LM \xi^6(s) + (\sqrt{M} + 1)L\sum\limits_{i=1}^M \norm{ \bw^*- \bw_i^* },
    \end{align}
     \begin{align}
           \norm{[\overline{\bT}(s)]_k- [{\bT}(s)]_k}  \leq  L\sqrt{M}{\sum\limits_{k=1}^d \xi^1_k(s)}  +   LM \xi^6(s)    + L\sum\limits_{i=1}^M \norm{ \bw^*- \bw_i^* }.
    \end{align}   
\end{lemm}
The proof of this lemma is given in Appendix \ref{tkhatlemmaproof}.

As a direct consequence of Lemma \ref{tkhatlemma}, we have the following corollary.
\begin{coro}\label{ballerrorlem}
     Under Assumptions \ref{claim2} and \ref{asumpt1_nonconvex} , the sequence $\{\xi^4_k(s)\}_s$ for any $k$ satisfies the following bound:
    \begin{align}
      \xi^4_k(s) & \leq    (\sqrt{M} + 2)L\sqrt{2}{\sum\limits_{k=1}^d \xi^1_k(s)} +  (\sqrt{M} + 2)LM \xi^6(s) + (\sqrt{M} + 2)L\sum\limits_{i=1}^M \norm{ \bw^*- \bw_i^* }.  
    \end{align}
\end{coro}

\begin{comment}
\begin{lemm}\label{avgdeviationlem}
    The sequence $\{[\widehat{\bW}^{k,s}(s)]_k - [\overline{\bW}(s)]_k\}_s$ for any $k$ satisfies the following bound:
    \begin{align}
         \norm{[\widehat{\bW}^{k,s}(s)]_k - [\overline{\bW}(s)]_k} & \leq \nonumber \\ 
        & \hspace{-3.5cm}\leq   M^{\frac{3}{2}}(\sqrt{M}+1)( 1-\beta^{\tau M} )^{\left\lfloor\frac{(J-2)}{\tau M}\right\rfloor}   \norm{[{\bW}(s)]_k- [\overline{\bW}(s)]_k}.
    \end{align}
\end{lemm}

\begin{proof}
 {Using the facts that $\mathbf{1}\bc_k(s)^T[{\bW}(s)]_k $ lies in the null space of $(\bQ^{\pi}_k - \frac{\mathbf{1}\mathbf{1}^T}{M}) $, that $ [\overline{\bW}(s)]_k$ lies in the null space of $(\bQ^{\pi}_k - \frac{\mathbf{1}\mathbf{1}^T}{M})(\bQ_k(s)- \mathbf{1}\bc_k(s)^T) $ we have
 \begin{align}
     [\widehat{\bW}^{k,s}(s)]_k - [\overline{\bW}(s)]_k & = \nonumber \\ & \hspace{-3cm} (\bQ^{\pi}_k - \frac{\mathbf{1}\mathbf{1}^T}{M})(\bQ_k(s)- \mathbf{1}\bc_k(s)^T)([{\bW}(s)]_k- [\overline{\bW}(s)]_k).
 \end{align}
 Then taking norm in the above equation followed by Corollary~\ref{coro1} and using the properties that $\norm{\bA} \leq \sqrt{M} \norm{\bA}_{\infty}$ for any $\bA \in \bR^{M \times M}$, $ \norm{\bQ^{\pi}_k} \leq \sqrt{M}$ we get:
    \begin{align}
        \norm{[\widehat{\bW}^{k,s}(s)]_k - [\overline{\bW}(s)]_k} & \leq \nonumber \\ & \hspace{-3.5cm}\norm{\bQ^{\pi}_k - \frac{\mathbf{1}\mathbf{1}^T}{M}} \norm{\bQ_k(s)- \mathbf{1}\bc_k(s)^T} \norm{[{\bW}(s)]_k- [\overline{\bW}(s)]_k} \\
        & \hspace{-3.5cm}\leq   M^{\frac{3}{2}}(\sqrt{M}+1)( 1-\beta^{\tau M} )^{\left\lfloor\frac{(J-2)}{\tau M}\right\rfloor}   \norm{[{\bW}(s)]_k- [\overline{\bW}(s)]_k}
    \end{align}
    which completes the proof.}
\end{proof}
\end{comment}

In order to establish convergence guarantees for the RESIST algorithm, we require an update rule on the coordinate-wise inexact averaged vector $ \widehat{\bw}^{s}(s)$. The next lemma provides this update rule.
\begin{lemm}\label{supportlem_inexactrule_007}
    Under Assumptions \ref{claim2} and \ref{asumpt1_nonconvex}, the sequence $\{\widehat{\bw}^{s}(s)\}_s$ satisfies the following inexact gradient descent update\footnote{An inexact gradient descent update refers to the standard gradient descent with some additive error term.} for any $s \geq 0$:
    \begin{align}
    \widehat{\bw}^{s+1}(s+1) &= \widehat{\bw}^{s}(s) - h \nabla f (\widehat{\bw}^{s}(s)) + \be_1(s) + \be_2(s),
\end{align}
where $ f(\cdot) := \frac{1}{M} \sum\limits_{j=1}^M f_j(\cdot)$,
\begin{align}
    \be_1(s) = h \begin{pmatrix}
        \begin{bmatrix}
        \nabla_1 f(\widehat{\bw}^{s}(s)) \\
        \nabla_2 f(\widehat{\bw}^{s}(s)) \\
        \vdots \\
        \vdots \\
        \vdots \\
        \nabla_k f(\widehat{\bw}^{s}(s))  \\
        \vdots \\
        \vdots \\
        \nabla_d f(\widehat{\bw}^{s}(s))
    \end{bmatrix} -   \begin{bmatrix}
        \nabla_1 f^{1,s+1}(\widehat{\bw}^{s}(s)) \\
        \nabla_2 f^{2,s+1}(\widehat{\bw}^{s}(s)) \\
        \vdots \\
        \vdots \\
        \vdots \\
        \nabla_k f^{k,s+1}(\widehat{\bw}^{s}(s))  \\
        \vdots \\
        \vdots \\
        \nabla_d f^{d,s+1}(\widehat{\bw}^{s}(s))
    \end{bmatrix} \end{pmatrix}
\end{align}
and\footnote{Here $ \nabla_{k}$ is the partial derivative with respect to $k$-th coordinate.} 
\begin{align}
    \be_2(s) & = h\begin{pmatrix}
     \begin{bmatrix}
          \sum\limits_{j=1}^M [\bc_1(s+1)]_{j} \nabla_1 f_j(\widehat{\bw}^{s}(s))  \\
           \sum\limits_{j=1}^M [\bc_2(s+1)]_{j} \nabla_2 f_j(\widehat{\bw}^{s}(s))  \\
        \vdots \\
        \vdots \\
           \sum\limits_{j=1}^M [\bc_k(s+1)]_{j} \nabla_k f_j(\widehat{\bw}^{s}(s))  \\
        \vdots \\
        \vdots \\
          \sum\limits_{j=1}^M [\bc_d(s+1)]_{j} \nabla_d f_j(\widehat{\bw}^{s}(s)) 
    \end{bmatrix}  -  \begin{bmatrix}
       \sum\limits_{j=1}^M [\bc_1(s+1)]_{j} \nabla_1 f_j(\bw_j(s)) \\
          \sum\limits_{j=1}^M [\bc_2(s+1)]_{j} \nabla_2 f_j(\bw_j(s)) \\
        \vdots \\
        \vdots \\
          \sum\limits_{j=1}^M [\bc_k(s+1)]_{j} \nabla_k f_j(\bw_j(s))  \\
        \vdots \\
        \vdots \\
           \sum\limits_{j=1}^M [\bc_d(s+1)]_{j} \nabla_d f_j(\bw_j(s))
    \end{bmatrix}         
    \end{pmatrix},
\end{align}
\begin{align}
   \norm{\be_2(s)} &\leq   Lh \sqrt{Md}\sum\limits_{k=1}^d \xi^1_k(s)
\end{align}
with $ f^{k,s+1}(\cdot) := \sum\limits_{j=1}^M [\bc_k(s+1)]_j f_j(\cdot)$ for any $k,s$.
\end{lemm}
The proof of this lemma is given in Appendix \ref{supportlemproof}.
Observe that the inexact gradient descent update from Lemma \ref{supportlem_inexactrule_007} reduces the decentralized problem to a centralized problem since we no longer have to deal with local updates and only need to analyze the algorithm with respect to the average function $f$. The effect of local updates and consensus error is captured by the error term $\be_2(s)$ where $\norm{\be_2(s)}$, up to some constant, is bounded by $\sum\limits_{k=1}^d \xi^1_k(s)$ and therefore can be easily controlled by the geometric decay of $ \xi^1_k(s)$ from Lemma \ref{lemxi1}. Meanwhile, the error term $\be_1(s)$ can be perceived as an adversarial error resulting from the inexact averaging along coordinates in the algorithm due to the malicious behavior and the screening method.  
Then, with some boundedness on the error term $\be_1(s)$, we can easily derive convergence rates of the RESIST algorithm over different classes of the average loss function $f$ using standard convergence analysis of the inexact gradient descent. 

In order to develop convergence rates for RESIST in Algorithm \ref{gradient descent algorithm} under different classes of loss functions, we will need the following assumption on the boundedness of iterates.
\begin{assum}\label{boundedassump}
    The iterate sequence $\{\bw_i(t)\}_t$ at any node $i$ generated by RESIST in Algorithm \ref{gradient descent algorithm} stays uniformly bounded by some sufficiently large compact set $\cK_1$ for any given bounded initialization of RESIST where this compact set depends only on the initialization of RESIST.
\end{assum}
We emphasize that Assumption \ref{boundedassump} has been routinely used in the decentralized optimization literature  \cite{nedic2009, duchi2012dual, jakovetic2014fast, sundhar2010distributed, xin2019frost}. Without this assumption, one can hardly derive and guarantee any convergence behavior with the presence of attack in the case of any of the iterates reaching infinity at any point. So, using it in a general decentralized framework with Man-in-the-middle attacks is very important. We also refer the reader to Section \ref{boundedexistencesec_0} in Appendix \ref{appendixC} with a type of Man-in-the-middle attack model where this Assumption \ref{boundedassump} will hold in some settings. However, proving iterate/gradient boundedness in a more general decentralized setting with Man-in-the-middle attacks is beyond the scope of current work and, therefore, is not pursued here.
We now derive the convergence rates for RESIST under different classes of loss functions.

\section{Algorithmic Analysis Under Convexity}\label{sconvex_section1}
We start this section by formally stating the strong convexity assumption on the local functions.
\begin{assum}\label{asumpt1}
    For all $i \in \{1,\dots, M\}$, the function $f_i : \mathbb{R}^d \to \mathbb{R}$ is $\mu$-strongly convex; i.e. the function $\bw \mapsto f(\bw) - \frac{\mu}{2}\|\bw\|^2$ is convex on $\mathbb{R}^d$.
\end{assum}
Although the Assumption \ref{asumpt1} of strong convexity is stronger than the usual convexity assumption with $\mu= 0$, we would like to emphasize that the loss functions in the ERM problem \eqref{eqn: decentralized ERM} under consideration are often strongly convex due to some form of added regularity (e.g., ridge regression). Also, in practice, while training the model over convex losses, one can easily add an L2 regularization to satisfy the strong convexity assumption. 

We now state an important property of strongly convex smooth functions.

\begin{lemm}[\cite{boyd2004convex} ]\label{lemmconvexcoercive}
    For any function $g$ on a finite dimensional Euclidean space that is $\mu$-strongly convex and $L$-gradient Lipschitz continuous, we have that for any $\bx, \by \in \mathbb{R}^d$:
    \begin{align}
        \langle \nabla g(\bx) - \nabla g(\by), \bx -\by\rangle &\geq \frac{\mu L}{\mu +L }\norm{\bx -\by}^2 + \frac{1}{\mu + L}\norm{\nabla g(\bx) - \nabla g(\by)}^2. \label{propsc}
    \end{align}
\end{lemm}

Using Lemma \ref{lemmconvexcoercive}, we can obtain the following contraction type bound on the error $\xi^6(s)$.
\begin{lemm}\label{convexsclem}
   Under Assumptions \ref{claim2}, \ref{asumpt1_nonconvex} and  \ref{asumpt1}, the sequence $\{\widehat{\bw}^{s}(s) \}_s$ for any $h \in (0, \frac{2}{\mu +L})$ satisfies:
    \begin{align}
    \xi^6(s+1)& \leq  (1-\mu h)\xi^6(s)  + \norm{\be_1(s)}+ L h \sqrt{Md}\sum\limits_{k=1}^d \xi^1_k(s),
\end{align}
where $\be_1(s)$ is defined in Lemma \ref{supportlem_inexactrule_007}.
\end{lemm}
The proof of this lemma is in Appendix \ref{convexsclemproof}.


Observe that using Lemma \ref{convexsclem} recursively for all $s$, we can obtain geometric decay rates for the error $\xi^6(s)$ but up to some residual error terms that depend on $\sup_s\norm{\be_1(s)}$ and also a series sum involving $\xi^1_k(s)$. Also, from Lemmas \ref{wkbarlemma} and \ref{lemxi1}, we will have geometric decay of $\xi^1_k(s)$ and $\xi^5_k(s)$ respectively up to some error terms involving $\xi^2_k(s)$ which again is controlled by Lemma \ref{tkhatlemma}. Now our goal is to derive a geometric decay rate that is uniform across $\xi^1_k(s), \xi^5_k(s), \xi^6(s)$ and for which the residual error terms only involve $\sup_s\norm{\be_1(s)}$. To do so, we make use of tools from linear control systems theory where we construct a vector recursion of the form $$ \bfg(s+1) \leq \bM \hspace{0.1cm}\bfg(s) + \bepsilon(s),$$ where the entries of the vector $\bfg(s)$ would comprise of $ \xi^1_k(s), \xi^5_k(s), \xi^6(s)$ and the residual error vector $\bepsilon(s)$ depends only on $\norm{\be_1(s)}$. The entries of matrix $\bM$ are determined from Lemmas \ref{wkbarlemma}, \ref{lemxi1}, \ref{tkhatlemma} and \ref{convexsclem}. Then, with a spectral radius of the matrix $\bM$ less than $1$, we obtain geometric decay of $\bfg(s)$ with respect to some norm and a residual error that depends on $ \sup_s \norm{\be_1(s)}$. The next lemma describes this recursion:



\begin{lemm}\label{lemmarecursion101}
    Under Assumptions \ref{claim2}, \ref{asumpt1_nonconvex} and \ref{asumpt1}, the vectors $\bfg(s), \bepsilon(s)$ satisfy the following inexact recursion:
    \begin{align}
         \bfg(s+1) \leq  \bM(h,J)\bfg(s) + \bepsilon(s)
    \end{align}
    where $\bM(h,J) = \bM_0+ \bP(h,J)$ for some diagonal matrix $\bM_0$ and a perturbation matrix $ \bP(h,J)$ whose entries depend linearly on $h$ which is given explicitly in Appendix \ref{lemmarecursionproof} and vectors $\bfg(s), \bepsilon(s)$ are defined as:
    \begin{align}
        \bfg(s)^T = \begin{bNiceMatrix}
  \xi^1_1(s) \hspace{0.2cm }\xi^5_1(s) \hspace{0.2cm }
   \xi^1_2(s) \hspace{0.2cm }
  \xi^5_2(s) \hspace{0.2cm }
  %\hline 
   \cdots  \hspace{0.2cm }
    \cdots  \hspace{0.2cm }
   \cdots  \hspace{0.2cm }
   \xi^1_d(s) \hspace{0.2cm }
  \xi^5_d(s) \hspace{0.2cm }
  \xi^6(s)
  \end{bNiceMatrix} , \\
  \bepsilon(s)^T =  \begin{bNiceMatrix}
   a_2 h\Delta  \hspace{0.2cm }
  a_4 h\Delta  \hspace{0.2cm }
   a_2h\Delta  \hspace{0.2cm }
   a_4 h\Delta \hspace{0.2cm }
  %\hline 
   \cdots  \hspace{0.2cm }
    \cdots  \hspace{0.2cm }
    \cdots  \hspace{0.2cm }
    a_2 h\Delta \hspace{0.2cm }
   a_4 h\Delta  \hspace{0.2cm }
 h \gamma(s) 
  \end{bNiceMatrix},
    \end{align}
 where $ a_2=  (\sqrt{M} + 1)^2 L$, $a_4 $\footnotemark$=  L$, $\Delta = \sum\limits_{i=1}^M \norm{ \bw^*- \bw_i^* }$ with $\bw^*, \bw^*_i$ defined from Lemma \ref{tkhatlemma} and $\gamma(s)$ satisfies the bound:
    \begin{align}
        \norm{\be_1(s)} \leq h\sum\limits_{k=1}^d {\lvert \nabla_k f(\widehat{\bw}^{s}(s))  -  \nabla_k f^{k,s+1}(\widehat{\bw}^{s}(s)) \rvert } = h \gamma(s),
    \end{align}
    where the inexact averaged function $f^{k,s+1}(\cdot)$ is defined from Lemma \ref{supportlem_inexactrule_007}.
\end{lemm}
\footnotetext{{We redefine $L$ to be $a_4$ for the consistency of the notations.}}
The proof of Lemma \ref{lemmarecursion101} and the exact expressions for the matrices $\bM_0, \bP(h, J)$ are given in Appendix \ref{lemmarecursionproof}. Note that the matrix $ \bM(h, J)$ is expressed as a sum of a diagonal matrix $ \bM_0$ and a perturbation matrix $\bP(h, J)$ so as to approximate the spectral radius of matrix $\bM(h, J)$ in terms of the spectral radius of $\bM_0$.

\subsection{Convergence analysis of RESIST in $s$-time scale}
 We now present the convergence rates in $s$-time scale for RESIST in Algorithm \ref{gradient descent algorithm} on strongly convex loss functions.
\begin{theo}\label{inexactlmigeo}
   Under Assumptions \ref{claim2}, \ref{asumpt1_nonconvex}, \ref{boundedassump} and \ref{asumpt1} for any sufficiently small $h>0$ and for any $ J > \frac{ \tau M\log( 2M^{\frac{3}{2}}(\sqrt{M}+1) )}{\log( 1-\beta^{\tau M} )^{-1}} + \tau M +2$ :
   \begin{itemize}
       \item  The inexact recursion from Lemma \ref{lemmarecursion101} has the following geometric rate to a $\cO(C_0 + \Delta)$ ball for any $S> 1$ and a positive constant $C_0$:
    \begin{align}
        \norm{ \bfg(S)}_{\bM(h,J)} &  \lesssim_{\bM(h,J)} \bigg(\rho(\bM(h,J))\bigg)^{S} \norm{\bfg(0)} + \frac{( C_0+ \Delta)}{\mu- \epsilon} \label{eqn: slow rate}
    \end{align}
    where $C_0 = \sup_{s \geq 0}\sum\limits_{k=1}^d\lvert \nabla_k f(\widehat{\bw}^{s}(s))  -  \nabla_k f^{k,s+1}(\widehat{\bw}^{s}(s)) \rvert$, $\Delta$ in Lemma \ref{lemmarecursion101} , $0< \epsilon < \mu$, $ \rho(\bM(h,J)) \leq 1- (\mu - \epsilon)h$, $  \norm{\cdot}_{\bM(h,J)}$ is a vector norm compatible to the matrix norm $\vvvert \cdot \vvvert_{\bM(h,J)} $ for matrix $\bM(h,J)$ such that $\vvvert\bM(h,J)\vvvert_{\bM(h,J)} = \rho(\bM(h,J)) <1 $. Note that the constants resulting from the ``$ \lesssim_{\bM(h,J)}$'' symbol are uniformly bounded for any sufficiently small $ h \in [0, \frac{2}{\mu +L}]$. In particular, these constant terms are equal to the product $ {\norm{\mathbf{U}^{-1}}}{\norm{\mathbf{U}}}$ where $\bM = \mathbf{U} \Lambda \mathbf{U}^{-1}$ is the eigendecomposition of $\bM(h, J)$. Since the matrix $\mathbf{U}$ is an $\mathcal{O}(h)$ perturbation of the eigenbasis for $\bM_0$ from matrix perturbation theory, the uniform boundedness of the constants follows.
    \item   Further, recall from Assumption \ref{boundedassump} that the compact set is $\cK_1$. Then for any sufficiently small $h$, for some absolute constant $C_1>0$, the consensus error sequences $ \{\xi_k^1(s)\}_s,  \{\xi_k^5(s)\}_s$ for any $k$ have the following \textbf{improved} geometric rates (smaller geometric constants than $\rho(h,J)$) to a $\cO(h)$ ball for any $S> 1$: 
    \begin{align}
        \xi^1_k(S)  &\leq  (a_1)^S\xi^1_k(0) + \frac{h}{1-a_1}\bigg(a_2 \sqrt{M} (\sqrt{M} + 1) C_1 \text{diam}(\cK_1) + a_2 \Delta\bigg),  \label{eqn: xi1}\\
        \xi^5_k(S)  &\leq  (a_3)^S\xi^5_k(0) + \frac{h}{1-a_3}\bigg(a_4 \sqrt{M} (\sqrt{M} + 1) C_1 \text{diam}(\cK_1) + a_4 \Delta\bigg),   \label{eqn: xi5}
    \end{align}
    where $a_1 = M^{\frac{3}{2}}(\sqrt{M}+1)( 1-\beta^{\tau M} )^{\left\lfloor\frac{(J-2)}{\tau M}\right\rfloor} \hspace{0.1cm} $and $a_3 = M^{\frac{3}{2}}( 1-\beta^{\tau M} )^{\left\lfloor\frac{(J-2)}{\tau M}\right\rfloor} $ with $a_1 <1 $, $a_3  <1$. The averaged iterate error sequence $\{\xi^6(s)\}_s$ has the following geometric rate to a $\cO(C_0 + h)$ ball for any $S > S_0$ where $S_0\geq 1$ :
    \begin{align}
        \hspace{-0.7cm}\xi^6(S) \leq (1-\mu h)^{S-S_0}\xi^6(S_0)  + \frac{C_0}{\mu} + \frac{L \sqrt{Md}}{\mu}\bigg((a_1)^{S_0}\xi^1_k(0)  +\frac{h}{1-a_1}\bigg(a_2 \sqrt{M} (\sqrt{M} + 1) C_1 \text{diam}(\cK_1) + a_2 \Delta\bigg)\bigg) .\label{eqn: xi6}
    \end{align}
   \end{itemize}
\end{theo}
The proof of this theorem is in Appendix \ref{inexactlmigeoproof}.

In Theorem \ref{inexactlmigeo}, for $ \rho(\bM(h, J)) \leq 1- (\mu-\epsilon) h$, one usually doesn't have the control of $\mu$ but only has control of the stepsize $h$. To make this quantity small for faster convergence, one can only choose a large stepsize $h$. However, $h$ has a strict upper bound of 2/L to achieve convergence. On the other hand, in \ref{eqn: xi1} and \ref{eqn: xi5}, when $M$ is large, we can always choose a large enough $J$ such that the quantity $a_1$ can be made small enough for faster convergence. This explains that the second part of Theorem \ref{inexactlmigeo} provides an improved geometric rate. Additionally, \ref{eqn: xi1} and \ref{eqn: xi5} give the guarantee of convergence to a ball of arbitrarily small radius by choosing small enough $h$ while in \ref{eqn: slow rate}, the size of the ball is a constant respect to $h$. The $C_0$ term measures the gradient gaps between exact and inexact averaging of local functions, and the $\Delta$ term captures the sum of the gaps between the minima of local functions and the minima of the averaged functions across the nodes. Both terms will be sufficiently small when the local functions are very close to each other on a compact set (closeness with respect to $L^{\infty}$ norm).

\begin{coro}\label{corro_inexactlmigeo}
    Under the assumptions of Theorem \ref{inexactlmigeo}, for any sufficiently small $h$ and for any $ J > \frac{ \tau M\log( 2M^{\frac{3}{2}}(\sqrt{M}+1) )}{\log( 1-\beta^{\tau M} )^{-1}} + \tau M +2$, the vector $\bfg(S)$ satisfies:
    \begin{align}
        \limsup_{S \to \infty} \norm{\bfg(S)} \lesssim_{\bM(h,J)}  \frac{( C_0+ \Delta)}{\mu -\epsilon}, 
    \end{align}
    for $0 <\epsilon <\mu $. Moreover, the consensus errors $ \xi^1_k(S)$, $\xi^5_k(S)$ for any $k$ satisfy:
    \begin{align}
        \limsup_{S \to \infty} \xi^1_k(S)  &\leq  \frac{h}{1-a_1}\bigg(a_2 \sqrt{M} (\sqrt{M} + 1) C_1 \text{diam}(\cK_1) + a_2 \Delta\bigg), \\
      \limsup_{S \to \infty} \xi^5_k(S)  &\leq  \frac{h}{1-a_3}\bigg(a_4 \sqrt{M} (\sqrt{M} + 1) C_1 \text{diam}(\cK_1) + a_4 \Delta\bigg),
    \end{align}
    and the averaged iterate error $ \xi^6(S) $ satisfies:
    \begin{align}
   \limsup_{S \to \infty}  \xi^6(S) & \leq \frac{C_0}{\mu} + \frac{L \sqrt{Md}}{\mu}\bigg( \frac{h}{1-a_1}\bigg(a_2 \sqrt{M} (\sqrt{M} + 1) C_1 \text{diam}(\cK_1) + a_2 \Delta\bigg)\bigg). 
\end{align}
\end{coro}
 The proof of this corollary is in Appendix \ref{corro_inexactlmigeoproof}. 
 From Theorem \ref{inexactlmigeo} and Corollary \ref{corro_inexactlmigeo}, we get that the consensus errors $ \xi^1_k(s)$ and $ \xi^5_k(s)$ converge to balls of radii $\frac{h}{1-a_1}\bigg(a_2 \sqrt{M} (\sqrt{M} + 1) C_1 \text{diam}(\cK_1) + a_2 \Delta\bigg) $ and $\frac{h}{1-a_3}\bigg(a_4 \sqrt{M} (\sqrt{M} + 1) C_1 \text{diam}(\cK_1) + a_4\Delta\bigg) $, respectively, at a geometric rate. Also, the averaged iterate error $ \xi^6(S)$ converges to a ball of radius $ \frac{C_0}{\mu} + \frac{L \sqrt{Md}}{\mu}\bigg( \frac{h}{1-a_1}\bigg(a_2 \sqrt{M} (\sqrt{M} + 1) C_1 \text{diam}(\cK_1) + a_2 \Delta\bigg)\bigg)$ with a geometric rate. Though the radii of these balls may appear to be large, we note that the radii of the first two balls for the consensus error are controlled by $h$, which can be made sufficiently small by choosing a corresponding small $h$. In the case of averaged iterate error $ \xi^6(S)$, the radius of the ball is controlled by $C_0$ and $h$ where the $h$ dependent term can also be made sufficiently small by choosing a corresponding small $h$. 



If the local functions are identical, i.e., $f_i = f_j$ for all $ i,j \in \cN, i \neq j$, then from the definition of $C_0, \Delta$ in Theorem \ref{inexactlmigeo} we have $C_0 = \Delta  = 0$. Then as a direct consequence of first part of Corollary \ref{corro_inexactlmigeo},  $\lim_{S \to \infty} \norm{\bfg(S)} = 0$ and hence for any $k$, from the definition of state vector $\bfg(s)$ from \eqref{lmistatespace}, the consensus errors vanish asymptotically, i.e.,$\lim_{S \to \infty} \xi^1_k(S) = 0 $ and $\lim_{S \to \infty} \xi^5_k(S) = 0$ and the averaged iterate error also vanishes asymptotically, i.e.,$\lim_{S \to \infty} \xi^6(S)  = 0.$ In the case that the local functions are not identical, we provide an explicit bound for $C_0 + \Delta$ term in the section \ref{discussion of theorems}, which implies the radius of the ball that the RESIST converges to can not be arbitrarily large.

In contrast to \ref{eqn: slow rate}, from Corollary \ref{corro_inexactlmigeo}, \ref{eqn: xi1}, \ref{eqn: xi5} and \ref{eqn: xi6} provide the consensus errors and indicating the averaged iterate error is contained within a $\cO(h)$ ball asymptotically even when $C_0 = \Delta  = 0$. This fact highlights a trade-off between the rate analysis for $\bfg(s)$ and that of $ \xi^1_k(S), \xi^5_k(S), \xi^6(S) $ from Theorem \ref{inexactlmigeo}. The trade-off is that $\norm{\bfg(S)}$ can geometrically converge only up to a $\cO(C_0 + \Delta)$ ball whereas $ \xi^1_k(S), \xi^5_k(S) $ can geometrically converge up to a $\cO(h)$ ball and $ \xi^6(S) $ can geometrically converge up to a $\cO(h + C_0)$ ball. Since $C_0, \Delta$ depend explicitly on the local functions and therefore cannot be controlled for most of the time without additional data, the $\cO(C_0 + \Delta)$ ball for $\norm{\bfg(S)}$ can be bounded away from zero in practice, and thus makes it harder to control the averaged iterate error and the consensus error. On the other hand, since $h$ can be chosen to be arbitrary small, the $\cO(h)$ ball can be controlled and hence the consensus errors $ \xi^1_k(S), \xi^5_k(S)$ from the second parts of Theorem \ref{inexactlmigeo}, Corollary \ref{corro_inexactlmigeo} can be controlled even if the averaged iterate error $\xi^6(S) $ is significant due to the $C_0$ term. 

\subsection{Convergence analysis of RESIST in $t$-time scale}
We now present the $t$-time scale convergence rate for RESIST. To do so, we require the following definition.


\begin{defi}\label{def3_tscale}
    The coordinate-wise inexact averaged vector for the $t$-time scale where $sJ \leq t < sJ+ J -2$ is defined as:
    \begin{align}
        \widehat{\bw}^s(t) &= \begin{bmatrix}
        \sum\limits_{j=1}^M [\bc_1(s)]_{j} [\bw_j(t)]_1 \\
         \sum\limits_{j=1}^M [\bc_2(s)]_{j}[\bw_j(t)]_2 \\
        \vdots \\
       \sum\limits_{j=1}^M [\bc_k(s)]_{j}[\bw_j(t)]_k \\
        \vdots \\
         \sum\limits_{j=1}^M [\bc_d(s)]_{j} [\bw_j(t)]_d 
    \end{bmatrix},
    \end{align}
   where the weights $ [\bc_k(s)]_{j}$ for any $k, j$ follow from Corollary \ref{coro1} and we have that $\widehat{\bW}^s(t) = \mathbf{1}(\widehat{\bw}^s(t))^T$. Also, $ \bW^* = \mathbf{1}({\bw}^*)^T $ where $ \bw^* = \argmin_{\bw} \frac{1}{M}\sum\limits_{i=1}^M f_i(\bw)$.
\end{defi}
\begin{theo}\label{maintheorempaper1}
     Under Assumptions \ref{claim2}, \ref{asumpt1_nonconvex}, \ref{boundedassump}  and \ref{asumpt1}, if $ J > \frac{ \tau M\log( 2M^{\frac{3}{2}}(\sqrt{M}+1) )}{\log( 1-\beta^{\tau M} )^{-1}} + \tau M +2$ then with Definitions \ref{def3_tscale} :
     \begin{itemize}
         \item  Algorithm RESIST for $S = \lfloor \frac{t}{J}\rfloor$ has the following geometric rate (geometric constant of $ \rho(h,J)$) to a $\cO(C_0+\Delta)$ radius ball around $\bW^*$ :
    \begin{align}
       \norm{\bW(t) - \overline{\bW}(t)}_F+  \norm{\bW^* - \widehat{\bW}^S(t)}_F  +  \norm{\bW(t) - \widehat{\bW}^S(t)}_F & \lesssim_{\bM(h,J)} \nonumber \\ & \hspace{-6.5cm} \sqrt{3d}(\sqrt{M}+1) M\bigg( \bigg(\rho(\bM(h,J))\bigg)^{\frac{t}{J}-1} \norm{\bfg(0)} + \frac{h( C_0+ \Delta)}{1- \rho(\bM(h,J))} \bigg) , \label{ballconvergence1}
    \end{align}
    where $\rho(\bM(h,J)) \leq 1-(\mu -  \epsilon)h  < 1 $ for any sufficiently small $h$, $\epsilon= \bo (\mu) >0$ and $C_0 < \infty$. Asymptotically, we have that
    \begin{align}
   \limsup_{t \to \infty} \bigg( \norm{\bW(t) - \overline{\bW}(t)}_F+  \norm{\bW^* - \widehat{\bW}^S(t)}_F  +  \norm{\bW(t) - \widehat{\bW}^S(t)}_F\bigg) & \lesssim_{\bM(h,J)} \nonumber \\ & \hspace{-2cm} \frac{\sqrt{3d} (\sqrt{M}+1) M( C_0+ \Delta)}{\mu- \epsilon}.
\end{align}
\item  Algorithm RESIST, for any $S>S_0$ where $S_0 >0$, has a faster geometric rate (geometric constant better than $\rho(h,J)$) in terms of $\norm{\bW(t) - \bW^*}_F$ to a $\cO(C_0+h)$ radius ball around $\bW^*$ :
     \begin{align}
    \norm{\bW(t) - \overline{\bW}(t)}_F+  \norm{\bW^* - \widehat{\bW}^S(t)}_F  +  \norm{\bW(t) - \widehat{\bW}^S(t)}_F & \leq \nonumber \\   
    & \hspace{-9cm} \sqrt{3d}(\sqrt{M}+1) M \Bigg(d\bigg(   (a_1)^{\frac{t}{J}-1}\xi^1_k(0) + \frac{h}{1-a_1}\bigg(a_2 \sqrt{M} (\sqrt{M} + 1) C_1 \text{diam}(\cK_1) + a_2 \bigg) + \nonumber \\
 & \hspace{-8cm} (a_3)^{\frac{t}{J}-1}\xi^5_k(0) + \frac{h}{1-a_3}\bigg(a_4 \sqrt{M} (\sqrt{M} + 1) C_1 \text{diam}(\cK_1) + a_4 \Delta\bigg) \bigg) +  (1-\mu h)^{{\frac{t}{J}-1}-S_0}\xi^6(S_0)  + \nonumber \\ & \hspace{-7cm} \frac{C_0}{\mu} + \frac{L \sqrt{Md}}{\mu}\bigg((a_1)^{S_0}\xi^1_k(0) + \frac{h}{1-a_1}\bigg(a_2 \sqrt{M} (\sqrt{M} + 1) C_1 \text{diam}(\cK_1) + a_2 \Delta\bigg)\bigg) \Bigg) ,\label{ballconvergence2}
\end{align} 
    where $a_1 <1 $, $a_3 <1$ and $C_1$ is an absolute constant. 
     \end{itemize}
\end{theo}
The proof of this theorem is in Appendix \ref{maintheorempaper1proof}.

Note that, from second bullet point of Theorem \ref{maintheorempaper1}, the exact radius of the $\cO(C_0+h)$ ball is given by :
    \begin{align}
  \limsup_{t \to \infty} \bigg( \norm{\bW(t) - \overline{\bW}(t)}_F+  \norm{\bW^* - \widehat{\bW}^S(t)}_F  +  \norm{\bW(t) - \widehat{\bW}^S(t)}_F \bigg) & \leq \nonumber \\   
    & \hspace{-11cm} \sqrt{3d}(\sqrt{M}+1) M \Bigg(  \frac{hd}{1-a_1}\bigg(a_2 \sqrt{M} (\sqrt{M} + 1) C_1 \text{diam}(\cK_1) + a_2 \bigg) +  \frac{hd}{1-a_3}\bigg(a_4 \sqrt{M} (\sqrt{M} + 1) C_1 \text{diam}(\cK_1) + a_4 \Delta \bigg) + \nonumber \\
 & \hspace{-9cm}  + \frac{C_0}{\mu} + \bigg(\frac{L \sqrt{Md}}{\mu} \frac{h}{1-a_1}\bigg(a_2 \sqrt{M} (\sqrt{M} + 1) C_1 \text{diam}(\cK_1) + a_2 \Delta\bigg)\bigg) \Bigg) .
\end{align} 

\subsection{Discussion of the convergence behavior of Theorem  \ref{inexactlmigeo} and \ref{maintheorempaper1}}\label{discussion of theorems}
\begin{lemm}\label{lemmaimplication}
    For a pair of $\mu$-strongly convex, continuously differentiable functions $f, g : \mathbb{R}^d \rightarrow \mathbb{R}$ with minima at $\by^*_f, \by^*_g $ respectively in some compact set $\Omega \subset \mathbb{R}^d$ which is a closed ball of radius $\theta$ as $\theta$ is sufficiently large, we have that $ \norm{\by^*_f - \by^*_g} \leq \frac{ 1}{\mu} \norm{\nabla (f-g)}_{L^{\infty}(\Omega)}$. 
\end{lemm}
\begin{proof}
    From the fact that $\by^*_f, \by^*_g  \in \Omega$ and $\nabla f(\by^*_f)=\nabla g(\by^*_g) = 0$. Then, by strong convexity, we have:
    \begin{align}
        \mu \norm{\by^*_g - \by^*_f} \leq  \norm{\nabla f(\by^*_g) - \nabla f(\by^*_f) } = \norm{\nabla f(\by^*_g) - \nabla g(\by^*_g) }  \leq  \norm{\nabla (f-g)}_{L^{\infty}(\Omega)},
    \end{align}
    which completes the proof.
\end{proof}

\begin{coro}\label{coroimplication}
    Under Assumptions \ref{claim2}, \ref{asumpt1_nonconvex}, \ref{boundedassump}  and \ref{asumpt1}, suppose for some compact set $\Omega \subset \mathbb{R}^d$ which is a closed ball of radius $\theta$ as $\theta$ is sufficiently large, the set of local functions $\{f_j\}_{j=1}^M$ and the iterate sequence $\{\widehat{\bw}^{s}(s)\}_{s=0}^{\infty}$ satisfy $\{\bw^*_j\}_{j =1}^M \bigcup \bw^*\bigcup \{\widehat{\bw}^{s}(s)\}_{s=0}^{\infty} \subset \Omega$. Then we have that :
    \begin{align}
        C_0 + \Delta \leq  \bigg(2 d (M-1) + \frac{M }{\mu} \bigg)  \max_{\substack{i,j \in \cN;\\ i \neq j}}\norm{\nabla (f_i-f_j)}_{L^{\infty}(\Omega)}
    \end{align}
    and the iterate sequence $\{\bw_j(t)\}_t$ for any $j \in \cN$ from RESIST converges to an $ \cO(   \max_{\substack{i,j \in \cN;\\ i \neq j}}\norm{\nabla (f_i-f_j)}_{L^{\infty}(\Omega)})$ neighborhood of $\bw^*$ with a geometric rate in $t$ according to Theorem \ref{maintheorempaper1}.
\end{coro}


\begin{proof}
    From the definition of $C_0 = \sup_{s \geq 0}\sum\limits_{k=1}^d\lvert \nabla_k f(\widehat{\bw}^{s}(s))  -  \nabla_k f^{k,s+1}(\widehat{\bw}^{s}(s)) \rvert $ and $\Delta = \sum\limits_{i=1}^M \norm{\bw^* -\bw^*_i} $ we can see that:
    \begin{align}
        C_0 &= \sup_{s \geq 0}\sum\limits_{k=1}^d \bigg\lvert  \frac{1}{M}\sum_{i=1}^M \nabla_k f_i(\widehat{\bw}^{s}(s))  -  \frac{1}{M}\sum_{i=1}^M [\bc_k(s+1)]_i \nabla_k  f_i(\widehat{\bw}^{s}(s)) \bigg\rvert \\
        & = \sup_{s \geq 0}\sum\limits_{k=1}^d   \bigg\lvert \sum_{i=1}^M \bigg(\frac{1}{M} - [\bc_k(s+1)]_i\bigg)\bigg(\nabla_k f_i(\widehat{\bw}^{s}(s)) - \nabla_k f(\widehat{\bw}^{s}(s)) \bigg) \bigg\rvert \\
        & = \sup_{s \geq 0}\sum\limits_{k=1}^d   \bigg\lvert \sum_{i=1}^M \bigg(\frac{1}{M} - [\bc_k(s+1)]_i\bigg)\bigg(\frac{1}{M}\sum_{l=1}^M \bigg(\nabla_k f_i(\widehat{\bw}^{s}(s)) - \nabla_k f_l(\widehat{\bw}^{s}(s))\bigg) \bigg) \bigg\rvert \\
        & \leq \frac{2}{M} \sup_{s \geq 0}\sum\limits_{k=1}^d \sum_{i=1}^M \sum_{l=1}^M \bigg\lvert\nabla_k f_i(\widehat{\bw}^{s}(s)) - \nabla_k f_l(\widehat{\bw}^{s}(s)) \bigg\rvert \\
        & \leq \frac{2}{M} \sup_{s \geq 0}\sum\limits_{k=1}^d \sum_{i=1}^M \sum_{l=1}^M \norm{ \nabla f_i(\widehat{\bw}^{s}(s)) - \nabla f_l(\widehat{\bw}^{s}(s)) } \leq 2 d (M-1)  \max_{\substack{i,j \in \cN;\\ i \neq j}}\norm{\nabla (f_i-f_j)}_{L^{\infty}(\Omega)}.
    \end{align}
    Next, we have that:
    \begin{align}
        \norm{\nabla (f_i-f)}_{L^{\infty}(\Omega)} = \norm{\nabla \bigg(f_i-\frac{1}{M}\sum_{l=1}^M f_l\bigg)}_{L^{\infty}(\Omega)} &=  \norm{\frac{1}{M}\sum_{l=1}^M\nabla (f_i- f_l)}_{L^{\infty}(\Omega)} \nonumber \\ & \leq \frac{1}{M}\sum_{l=1}^M\norm{\nabla (f_i- f_l)}_{L^{\infty}(\Omega)},
    \end{align}
    and thus by Lemma \ref{lemmaimplication} we have that $ \norm{\bw^* -\bw^*_i} \leq \frac{1}{\mu} \max_{\substack{i,j \in \cN;\\ i \neq j}}\norm{\nabla (f_i-f_j)}_{L^{\infty}(\Omega)}$ for any $i \in \cN$ and hence we have $\Delta \leq \frac{M }{\mu}  \max_{\substack{i,j \in \cN;\\ i \neq j}}\norm{\nabla (f_i-f_j)}_{L^{\infty}(\Omega)}$. Then by substituting $ C_0 + \Delta \leq \bigg(2 d (M-1) + \frac{M }{\mu}\bigg) \max_{\substack{i,j \in \cN;\\ i \neq j}}\norm{\nabla (f_i-f_j)}_{L^{\infty}(\Omega)}$ in the bound \eqref{ballconvergence1} from Theorem \ref{maintheorempaper1}, the proof is complete.
\end{proof}
From Corollary \ref{coroimplication} we can see that the upper of $ C_0 + \Delta$ is a function of the dissimilarity of local gradients $\norm{\nabla (f_i-f_j)}_{L^{\infty}(\Omega)}$. To give an upper bound of the dissimilarity of local gradients $\norm{\nabla (f_i-f_j)}_{L^{\infty}(\Omega)}$ and implicitly provide upper bound for the term $ C_0 + \Delta$, we now state an assumption of gradient similarity between the local functions which was often used in the decentralized literature. 

\begin{assum}[Bounded gradient similarity \cite{tyou2023localized}]\label{coroimplication*assump}
We have $ \frac{1}{M}\sum\limits_{i=1}^M \norm{\nabla f_i(\bw)}^2 \leq G^2  + D^2  \norm{\nabla f(\bw)}^2  $ for every $\bw \in \mathbb{R}^d$ for some $G, D\geq 0$.
\end{assum}

Assumption \ref{coroimplication*assump} implies that the local gradients cannot be too dissimilar to the averaged gradient. This aligns with our system model, where we assume that the local dataset is sampled i.i.d. from the global dataset. Therefore, this assumption must imply Corollary \ref{coroimplication} for certain values of $D$. The next lemma provides this implication.

\begin{lemm}\label{coroimplication*}
    Under the Assumption  \ref{claim2}, \ref{asumpt1_nonconvex}, \ref{boundedassump}, \ref{asumpt1} and \ref{coroimplication*assump} with $D<1$, Corollary \ref{coroimplication} is implied for some compact set $\Omega \subset \mathbb{R}^d$ which is a closed ball of radius $\theta$ as $\theta$ is sufficiently large.
\end{lemm}
\begin{proof}
   Note that for $B < 1$ from Jensen's inequality, we have the following bound for any $\bw \in \mathbb{R}^d$:
\begin{align}
 \norm{\nabla f(\bw)} \leq \frac{1}{M}\sum\limits_{i=1}^M \norm{\nabla f_i(\bw)}   \leq \sqrt{ \frac{1}{M}\sum\limits_{i=1}^M \norm{\nabla f_i(\bw)}^2 } &\leq \sqrt{G^2  +   B^2\norm{\nabla f(\bw)}^2 } \leq G  +   B\norm{\nabla f(\bw)}\\
   \implies   \norm{\nabla f(\bw)}  &\leq \frac{G}{1-B} \\
     \implies  \norm{\nabla (f_i-f_j)(\bw)} & \leq 2 MG\bigg( 1 + \frac{B}{1-B} \bigg) 
\end{align}
where we also used the first inequality of $ \frac{1}{M}\sum\limits_{i=1}^M \norm{\nabla f_i(\bw)}   \leq  G  +   B\norm{\nabla f(\bw)}$ in the last step. Hence, we have $\nabla (f_i-f_j) \in L^{\infty}(\mathbb{R}^d) $ for any $i,j \in \cN$, $i \neq j$ or equivalently $ \nabla (f_i-f_j) \in L^{\infty}(\Omega) $ for compact set $\Omega$. Then the result from Corollary \ref{coroimplication} follows for the compact set $\Omega$ where we have
\begin{align}
        C_0 + \Delta \leq  \bigg(2 d (M-1) + \frac{M }{\mu} \bigg)  \max_{\substack{i,j \in \cN;\\ i \neq j}}\norm{\nabla (f_i-f_j)}_{L^{\infty}(\Omega)} \leq 2MG \bigg(2 d (M-1) + \frac{M }{\mu} \bigg) \bigg( 1 + \frac{B}{1-B} \bigg). 
    \end{align}
\end{proof}

We now make a crucial discussion pertaining to the geometric convergence rate to a $\cO(C_0 +\Delta)$ ball around $\bw^*$ from Theorem~\ref{maintheorempaper1}:

Observe that the geometric rate from Theorem \ref{maintheorempaper1} does not offer a convergence guarantee to the exact global minima $\bw^*$, and also, it does not guarantee that consensus will be achieved. Moreover, as $t \to \infty$, the iterate matrix $\bW(t)$ can only be within some $\cO(C_0 +\Delta)$ ball around $\bW^*$ where this ball is upper bounded as in Lemma \ref{coroimplication*}. In particular, within the setting of Byzantine attack model which can be easily mapped to the Man-in-the-middle attack model of this work, a recent work \cite{kuwaranancharoen2023geometric} also achieves geometric convergence rate to some neighborhood of the global minima provided the decentralized screening algorithm satisfies a contraction property around some fixed point $\bw_c$ (see Definition 6.4 and 6.5 in \cite{kuwaranancharoen2023geometric}). Then, their main result (Theorem 6.7) achieves geometric rate to a ball of radius $\max_i\norm{\bw^*_i - \bw_c}$ where $\bw^*_i$ is the local minimum at node $i$. Now, the vector $\bw_c$ may not necessarily be equal to $\bw^*$, i.e., the global minimum and \cite{kuwaranancharoen2023geometric} also does not provide any explicit relation between $\bw_c$ and $\bw^*$. In contrast, the $\cO(C_0 +\Delta)$ ball from Theorem \ref{maintheorempaper1} in our work explicitly depends on $\sum_i\norm{\bw^*-\bw^*_i}$ and the norm difference of inexact and exact averaged gradients at the consensus vector. Moreover, from Corollary \ref{coroimplication}, the $\cO(C_0 +\Delta)$ ball's diameter is bounded by the sup norm of the gradient difference of local functions in some compact set and hence can be made arbitrarily small if the local gradients are very close to one another. Therefore, to the best of our knowledge, in the decentralized adversarial setting, our result (Theorem \ref{maintheorempaper1}, Corollary \ref{coroimplication}) is the first one that achieves geometric convergence rate to a ball around the global minimum $\bw^*$ where the radius of the ball is explicitly upper bounded by the $L^{\infty} $ distance between local gradients on a compact set. 

Note that up to this point, all the convergence analysis in this section relies on Assumption \ref{asumpt1}, which requires the local loss functions to be strongly convex. However, this assumption might not be satisfied in modern machine learning applications where deep neural networks are needed for many complicated datasets, such as CIFAR-10, CIFAR-100, and ImageNet. Thus, in the next section, we will provide the convergence guarantee of RESIST without Assumption \ref{asumpt1}, which could be applied to some specific types of nonconvex loss functions.

\section{Algorithmic Analysis Under Nonconvexity}\label{sec:nonconvex convergence rate}

For nonconvex functions, we no longer require Assumption \ref{asumpt1} of strong convexity and only require gradient Lipschitzness (Assumption \ref{asumpt1_nonconvex}). We also note that in this section, unlike the strongly convex case, we only present the $s$-time step convergence rates for the ALGORITHM RESIST and omit the $t$-time step convergence rates for brevity. The $t$-time step convergence rates can be easily recovered using elementary analysis, as done in Theorem \ref{maintheorempaper1}. We now analyze two particular cases of nonconvex functions.
\subsection{Rates for Polyak-{\L}ojasiewicz (P\L) functions}\label{sec_polyaklojas_sub1} 
One common type of nonconvex loss function is Polyak-{\L}ojasiewicz (P\L) functions, which include the two popularly used functions in modern machine learning applications: the least square and logistic regression functions. Functions that satisfy Polyak-{\L}ojasiewicz (P\L) inequality have the property that the gradient of the function grows as a square root function of its sub-optimality as described in the assumption below:

\begin{assum}\label{pl_assumption}
    The averaged function $f := \frac{1}{M}\sum_{i=1}^{M} f_i $ satisfies the Polyak-{\L}ojasiewicz (P\L) inequality \cite{lojasiewicz1963propriete} with parameter $\mu \in (0,L)$, i.e, %where 
    for any $\bw \in  \mathbb{R}^d$ we have:
    \begin{align}
        \frac{1}{2 \mu } \norm{\nabla f(\bw)}^2 \geq f(\bw) - f^*
    \end{align}
    and $f^* := \min_{\bw \in  \mathbb{R}^d} f(\bw)$.
\end{assum}
Note that in Assumption \ref{pl_assumption}, we require the P{\L} inequality to hold for the averaged function $f$ instead of local functions $f_i$. This assumption on the averaged loss function is in line with the Kurdyaka-{\L}ojasiewicz (K{\L}) assumption (a more general form of the P{\L} assumption) on the averaged loss function from \cite{zeng2018nonconvex}, where DGD is adopted to perform decentralized optimization. It can also be observed that having individual P{\L} inequalities for local loss functions $f_i$ is not enough to guarantee a P{\L} inequality for the global averaged function $f$, unlike the case of convexity where the average %sum 
of convex functions is convex (see Appendix \ref{PL example} for one such example). 

To proceed with the rest of our analysis, we make an assumption as the following:
\begin{assum}\label{diamk}
    We assume there exist a sufficiently large compact set $\cK_2$ such that $ \argmin_{\bw}f_i(\bw) \in  \cK_2$ for all $i \in \{1, \cdots,M\}$ and $ \argmin_{\bw}f(\bw) \in  \cK_2 $.
\end{assum}
Note that this assumption is not hard to be satisfied as long as the optimum of local functions and the average of local functions are finite.
\begin{lemm}\label{pl_lem}
    Under Assumptions \ref{claim2}, \ref{asumpt1_nonconvex}, \ref{boundedassump} and Assumption \ref{pl_assumption} with some compact set $\cK = \cK_1 \cup \cK_2$ and its diameter as $\text{diam} (\cK)$ where $\cK_1$ is defined in Assumption \ref{boundedassump} and $\cK_2$ in Assumption \ref{diamk}, the function sequence $\{f(\widehat{\bw}^{s}(s)) \}_s$, for any $h \in (0, \frac{2}{L})$, satisfies:
    \begin{align}
      f(\widehat{\bw}^{s+1}(s+1)) - f^*  & \leq \bigg(1 - {\mu h(2-Lh)}\bigg) (f(\widehat{\bw}^{s}(s)) - f^*) +  \nonumber \\ & \hspace{3cm} L \hspace{0.1cm} \text{diam} (\cK) \bigg(\norm{\be_1(s)} + Lh \sqrt{Md}\sum\limits_{k=1}^d \norm{[\widehat{\bW}^{k,s}(s)]_k - [{\bW}(s)]_k}\bigg) ,
\end{align}
where $\be_1(s)$ is defined in Lemma \ref{supportlem_inexactrule_007}.
\end{lemm}
The proof of this lemma is in Appendix \ref{pl_lemproof}.
Note that for simplicity of notation, for the rest of the paper, any results derived before Lemma \ref{pl_lem} that contain the compact set $\cK_1$ will be replaced by $\cK$ due to the fact that $\text{diam} (\cK) \geq \text{diam} (\cK_1)$ given $\cK = \cK_1 \cup \cK_2$.


\begin{theo}\label{plrate_theo}
   Under Assumptions \ref{claim2}, \ref{asumpt1_nonconvex}, \ref{boundedassump} and Assumption \ref{pl_assumption} for some compact set $\cK$ defined in Lemma \ref{pl_lem} and for any $h \in (0, \frac{2}{L})$, for some absolute constant $C_1$ and for any $ J > \frac{ \tau M\log( 2M^{\frac{3}{2}}(\sqrt{M}+1) )}{\log( 1-\beta^{\tau M} )^{-1}} + \tau M +2$, the consensus error sequences $ \{\xi_k^1(s)\}_s,  \{\xi_k^5(s)\}_s$ ,for any $k$, have the following geometric rates to a $\cO(h)$ ball for any $S> 1$:
    \begin{align}
        \xi^1_k(S)  &\leq  (a_1)^S\xi^1_k(0) + \frac{h}{1-a_1}\bigg(a_2 \sqrt{M} (\sqrt{M} + 1) C_1 \text{diam}(\cK) + a_2 \Delta\bigg), \\
        \xi^5_k(S)  &\leq  (a_3)^S\xi^5_k(0) + \frac{h}{1-a_3}\bigg(a_4 \sqrt{M} (\sqrt{M} + 1) C_1 \text{diam}(\cK) + a_4 \Delta\bigg),
    \end{align}
    where $a_1  <1 $, $a_3 <1$. 

Also, the function error sequence $\{f(\widehat{\bw}^{s}(s)) - f^*\}_s$ has the following geometric rate to a $\cO(C_0 + h)$ ball:
    \begin{align}
      f(\widehat{\bw}^{S}(S)) - f^*    & \leq \bigg(1 - {\mu h(2-Lh)}\bigg)^{S} (f(\widehat{\bw}^{0}(0)) - f^*) +   L \hspace{0.1cm} \text{diam} (\cK) \frac{ C_0}{\mu(2-Lh)} +  \nonumber \\ & \hspace{5cm}  \frac{ L^2h d\sqrt{Md}}{1-a_1} \hspace{0.1cm} (\text{diam} (\cK))^2 \bigg(  \frac{(\sqrt{M}+1)^2}{\mu(2-Lh)}  LM(\sqrt{ d}+2) +  M   \bigg) , \label{pl_ratetheo_bound*}
    \end{align}
    for a positive constant $C_0$.
\end{theo}
The proof of this theorem is in Appendix \ref{plrate_theoproof}.

Note that unlike Theorem \ref{inexactlmigeo} for the strongly convex case where the rates are in terms of iterates, rates provided in Theorem \ref{plrate_theo} are in terms of function values, but they still preserve a geometric decay. To the best of our knowledge, this is the first paper that provides geometric rates of decay to an $\cO(h)$ ball for the P{\L} function class in the decentralized setting with the presence of attacks in the network. 
%A typical example of a P{\L} function in one-dimension is $ f(x) = x^2 + 3 \sin^2(x)$ which is also an invex function (nonconvex) and therefore Theorem \ref{plrate_theo} provides geometric rates for functions such as $ f(x) = x^2 + 3 \sin^2(x)$.    


\subsection{Rates for smooth nonconvex functions}\label{sec_gennonconvex_sub1}
Functions that satisfy the P{\L} property only cover the least square and logistic regression functions used in ML applications. As datasets continue to grow and tasks become increasingly complex, convolutional neural networks (CNNs) and deep neural networks (DNNs) play a crucial role in these applications. However, their involvement leads to smooth yet highly nonconvex loss functions, making optimization more challenging. In those cases, if one would like to apply the RESIST algorithm to those applications, convergence guarantee for smooth nonconvex loss functions is essential. To prove the convergence rates for smooth nonconvex functions, we first need the following lemma.
\begin{lemm}[H\"older inequality for sums\cite{Holder1987}]\label{holderlemma}
     Let $\{a_s\}$ and $\{b_s\}$ be some set of complex numbers, $s \in E$, where $E$ is a finite or an infinite set of indices. Then the following H\"older inequality holds:
     \begin{align}
         \bigg\lvert \sum_{s \in E} a_s b_s \bigg\rvert \leq \bigg( \sum_{s \in E} \lvert a_s \rvert^v\bigg)^{\frac{1}{v}} \bigg( \sum_{s \in E} \lvert b_s \rvert^q\bigg)^{\frac{1}{q}}
     \end{align}
where $v> 1$ and $ \frac{1}{v} + \frac{1}{q} = 1$. 
\end{lemm}

\begin{theo}\label{nonconvexrate_theo}
   Under Assumptions \ref{claim2} , \ref{asumpt1_nonconvex} and \ref{boundedassump} for the compact $\cK$ defined in Lemma \ref{pl_lem}, for $h = h(s) = \frac{p}{(s+1)^{\omega}}$ as decaying stepsizes with $p, \omega >0$ and for any $ J > \frac{ \tau M\log( 2M^{\frac{3}{2}}(\sqrt{M}+1) )}{\log( 1-\beta^{\tau M} )^{-1}} + \tau M +2$, the consensus error sequences $ \{\xi_k^1(s)\}_s,  \{\xi_k^5(s)\}_s$ for any $k$ converge to $0$ with the following rate:
    \begin{align}
        \xi^1_k(S) &= \cO\bigg(\frac{1}{S^{\omega}}\bigg) , \\
        \xi^5_k(S)  &= \cO\bigg(\frac{1}{S^{\omega}}\bigg) .
    \end{align}
   Also, the sequence $\{\nabla f(\widehat{\bw}^{s}(s))\}_s$ has the following rate for any large enough $S $ provided $h(s) = \frac{p}{(s+1)^{\omega}}$ where $\omega = \frac{1}{2} + \epsilon$ with any $0 <\epsilon < 1/2$ and $0 < p \leq \frac{1}{2L}$:
    \begin{align}
         \min_{0 \leq s \leq S-1}\norm{\nabla f(\widehat{\bw}^{s}(s) )}^2  & \leq \frac{\bigg(f(\widehat{\bw}^{0}(0) ) - \inf_{\bw} f(\bw)\bigg)}{p S^{\frac{1}{2} - \epsilon}}  + {  \frac{C_6}{S^{\frac{1}{2} - \epsilon}}} \nonumber \\
&\hspace{4.3cm}+ 2 L \text{diam}(\cK)C_0  +   \frac{ 2 C_4 L^2 d   \sqrt{Md} (\text{diam}(\cK))^2}{S^{\frac{1}{2} - \epsilon}}, 
    \end{align}
    and 
    \begin{align}
     \limsup_{S \to \infty}  \min_{0 \leq s \leq S-1}\norm{\nabla f(\widehat{\bw}^{s}(s) )}^2  & \leq  2 L \text{diam}(\cK)C_0,
    \end{align}
    where $C_0 = \sup_{s \geq 0}\sum\limits_{k=1}^d\lvert \nabla_k f(\widehat{\bw}^{s}(s))  -  \nabla_k f^{k,s+1}(\widehat{\bw}^{s}(s)) \rvert $, $C_4 = \cO\bigg(M^2 (1+p) \bigg(Ld \text{ diam}(\cK)\bigg)^3\bigg) $, \\ and $C_6 = \cO\bigg(p L^3\bigg(Md \text{ diam}(\cK)\bigg)^2\bigg) $.
\end{theo}
The proof of this theorem is in Appendix \ref{nonconvexrate_theoproof}.
Note that the sub-linear rate of $\cO(\frac{1}{S^{0.5 - \epsilon}})$ to $\cO(C_0)$ ball from Theorem \ref{nonconvexrate_theo} matches the convergence rate to the first order oracle in centralized stochastic gradient descent method \cite{Kento2024SGD} with smooth nonconvex loss functions, which is the best-known results in the literature for the given choice of diminishing step-size $h(s) = \frac{p}{(s+1)^{0.5+ \epsilon}}$. However, from the given rate provided with $C_0 = \cO(\delta)$, the best one can do is to infer a $\delta$ first-order optimality for the smooth nonconvex function with attacks. In particular, with the setting of ERM formulation in \eqref{eqn: ERM}, we later show in Theorem \ref{statisticalconvergencethm_nonconvex} that $ C_0 = \cO(\frac{1}{\sqrt{N}})$ with high probability for $N$ local samples at each node. Then, with a sufficiently large number of local samples, we can achieve a near first-order optimality with high probability. Note that proving %some 
second-order optimality guarantees in the nonconvex setting is a much harder problem, as one needs to avoid potential saddle points \cite{Rishabh2022},\cite{Rishabh2023} and, therefore, is left for future work. 
The above asymptotically convergence analysis with diminishing stepsizes is commonly used with smooth nonconvex objective functions. However, recently, the work \cite{WU2023SGD} looked into the convergence behavior with a finite time horizon. Thus, we provide the following theorem to show the non-asymptotic convergence guarantee under smooth nonconvex loss functions with constant step size.

\begin{theo}\label{nonconvexrate_theo_fixedstep}
   Under Assumptions \ref{claim2} and \ref{asumpt1_nonconvex}, suppose the algorithm RESIST is iterated for finitely many gradient steps $S $ with $h = \frac{1}{\sqrt{S}}$ and suppose Assumption \ref{boundedassump} holds for the compact set $\cK$ defined in Lemma \ref{pl_lem} such that $S > L^6(M d\text{ diam}(\cK))^4 $. Then for any $ J > \frac{ \tau M\log( 2M^{\frac{3}{2}}(\sqrt{M}+1) )}{\log( 1-\beta^{\tau M} )^{-1}} + \tau M +2$, the consensus errors $ \xi_k^1(s), \xi_k^5(s)$ for any $k$ and any $s \leq S$ satisfy:
    \begin{align}
        \xi^1_k(s) &= \cO\bigg((a_1)^{s} + \frac{1}{\sqrt{S}}\bigg) , \\
        \xi^5_k(s)  &= \cO\bigg((a_3)^{s} + \frac{1}{\sqrt{S}}\bigg) ,
    \end{align}
    where $a_1  <1 $, $a_3  <1$.
   Also, the finite-length gradient sequence $\{\nabla f(\widehat{\bw}^{s}(s))\}_{s=0}^{S-1}$ satisfies :
    \begin{align}
        \frac{1}{{S}}\sum_{s=0}^{S-1}\norm{\nabla f(\widehat{\bw}^{s}(s) )}^2 & \leq \bigg(1-\frac{L}{\sqrt{S}}\bigg)^{-1}\frac{f(\widehat{\bw}^{0}(0) ) - \inf_{\bw} f(\bw)}{\sqrt{S}} +  \frac{C_9}{\sqrt{S}}  \nonumber  \\ &  + \bigg(1-\frac{L}{\sqrt{S}}\bigg)^{-1} L \text{ diam}(\cK) C_0 
    \end{align}
    where $C_9 = \cO\bigg(L^3\bigg(Md \text{ diam}(\cK)\bigg)^2\bigg) $.
\end{theo}
 The proof of this theorem is in Appendix \ref{nonconvexrate_theo_fixedstepproof}.

Observe that the metric $  \frac{1}{{S}}\sum_{s=0}^{S-1}\norm{\nabla f(\widehat{\bw}^{s}(s) )}^2$ used in Theorem \ref{nonconvexrate_theo_fixedstep} may appear to be a non-standard one but has been used recently in \cite{WU2023SGD} with decentralized SGD algorithm with Byzantine attack. For large enough $S$ and $C_0$ sufficiently small, near first-order optimality can be inferred from Theorem \ref{nonconvexrate_theo_fixedstep}.


We now present the mapping of the decentralized Byzantine attack to the Man-in-the-middle attack. Then, all the prior analysis in our work naturally covers the case of decentralized Byzantine attacks with slight modifications in the definitions of averaging vectors over the graph.


\section{Mapping the Decentralized Byzantine Resilient Problem to the Man-in-the-middle Attack Problem}\label{mapping}

As in \cite{Fang2022BRIDGE}, with Byzantine attacks, the decentralized ERM problem stated in \eqref{eqn: decentralized ERM} is hard to solve. Best one could hope for solving an ERM problem that is restricted to the set of nonfaulty nodes, i.e.,
\begin{align}\label{eqn: restricted decentralized ERM}
    \min\limits_{\{\bw_j : j \in \cR\}} \frac{1}{r}\sum\limits_{j \in \cR} f_j(\bw_j) \ \text{subject to} \ \forall i,j \in \cR, \ \bw_i = \bw_j.
\end{align}
Here, $\cR \subseteq \cN$ and $\cB \subseteq \cN$ are the set of nonfaulty and faulty nodes in the network, respectively. In addition, $r$ denotes the cardinality of the set $\cR$, and the algorithm design parameter $b$ denotes the maximum number of Byzantine nodes that could occur in the network. Thus, $0 \leq |\cB| \leq b$ and $r \geq M - b$. In addition, without loss of generality, the nonfaulty nodes can be labeled from $1$ to $r$, i.e., $\cR:= \{1,\dots,r\}$.

Next, the ERM optimization problem in \eqref{eqn: restricted decentralized ERM} is equivalent to solving the following static Man-in-the-middle attack ERM problem \eqref{eqn: unrestricted decentralized ERM} over the set of all nodes $\cN$:
\begin{align}\label{eqn: unrestricted decentralized ERM}
    \min\limits_{\{\bw_j : j \in \{1, \cdots,M\}} \frac{1}{M}\sum\limits_{j \in \{1, \cdots,M\}} f_j(\bw_j) \ \text{subject to} \ \forall i,j \in  \{1,\cdots,r\}, \ \bw_i = \bw_j; \quad f_j := \text{constant} \hspace{0.2cm} \forall r <j \leq M.
\end{align}
We define the static attack as only the outgoing edges corresponding to the nodes $ \cN \backslash \cR$ being possibly compromised for all time $t$, and the remaining edges remain unaffected.
Then, from the analysis of algorithm RESIST \eqref{scr1}, \eqref{dst1} for the optimization problem \eqref{eqn: unrestricted decentralized ERM}, we get for any coordinate $k$ that :
\begin{align}
    [{\bW}(s+1)]_k &=  \bQ_{k}( s)[{\bW}(s)]_k - h [\nabla F({\bW}(s))]_k \label{scrdstmap}
\end{align}
where $ \bQ_{k}( s) =  \prod\limits_{l= J \lfloor \frac{t}{J} \rfloor }^{J \lfloor \frac{t}{J} \rfloor + J -2} \bY_{k}(l) $ and 
\begin{align}
  \bY_{k}(l)  =   \begin{bmatrix}
        [\bY_{k}(l)]_{[1:r] \times [1:r]}  & \mathbf{0}_{[1:r] \times [r+1:M]} \\
        [\bY_{k}(l)]_{[r+1:M] \times [1:r]}   & [\bY_{k}(l)]_{[r+1:M] \times [r+1:M]} 
    \end{bmatrix}
\end{align}
from Corollary \ref{claim1} in Appendix \ref{section*vaidya_10}. Note that Corollary \ref{claim1} can be applied here since, from the viewpoint of a local neighborhood, a Byzantine attack on $b$ nodes amounts to an MITM attack having at most $b$ compromised incoming links within the neighborhood, provided $ b < \min_{j \in \cN}\frac{\lvert \cN_j\rvert +1}{2}$.
Hence
\begin{align}
    \bQ_{k}(s) =  \begin{bmatrix}
       \prod\limits_{l= J \lfloor \frac{t}{J} \rfloor }^{J \lfloor \frac{t}{J} \rfloor + J -2} [\bY_{k}(l)]_{[1:r] \times [1:r]}  & \mathbf{0}_{[1:r] \times [r+1:M]} \\
        \bA_1(s)   & \bA_2(s)
    \end{bmatrix}  
\end{align}
for some block matrices $  \bA_1(s), \bA_2(s)$. Then the update in \eqref{scrdstmap} happens only across the top $r$ entries, i.e., 
$$    [{\bW}(s+1)]_{k,1:r} =  [\bQ_{k}( s)]_{[1:r] \times [1:r]}[{\bW}(s)]_{k,1:r} - h [\nabla F({\bW}(s))]_{k,1:r}  $$ whereas the bottom $M-r$ entries can behavior arbitrarily under the influence of attacker and they do not affect other entries in any ways. Note that the mapping can happen with respect to analysis; however, due to the nature of two different types of attacks—one targeting the network through links and the other through nodes—a direct mapping of the graph is difficult to establish. Therefore, the definition of ${\mathcal{T}}_{\mathcal{F}}$ in Definition \ref{def1a} must be modified from the definition inherited from Byzantine literature as in~\cite{su2015byzantine, Fang2022BRIDGE} along with some different constant of $\tau$ which is the cardinality of the set ${\mathcal{T}}_{\mathcal{F}}$. Then, the analysis required for consensus and geometric convergence to a ball around the solution of \eqref{eqn: unrestricted decentralized ERM} for the first $r$ nodes naturally extends to the scenario involving Byzantine attacks. Thus, by limiting our focus to the $r$ regular nodes in the graph,  the algorithm RESIST guarantees the same convergence properties for the Byzantine attack in the ERM optimization problem \eqref{eqn: restricted decentralized ERM} as it does for the Man-in-the-middle attack model \eqref{eqn: unrestricted decentralized ERM}.


\section{Statistical Rates}\label{sec_statisticalrate_1}

\subsection{Preliminaries for statistical rates}
Since the functions defined in the previous sections, especially in Section \ref{sconvex_section1} and \ref{sec:nonconvex convergence rate}, do not consider data samples or the dependency on data samples has been omitted for simplicity of notation. In this section (and associated proofs in Appendix \ref{appendixE}), we explicitly define some notations regarding the functions, which may or may not be the same as in previous sections as the following. We use $\ell_j$ to denote the local loss function respective to the model parameter and data samples; $f_j$ is used to denote the local empirical loss function, which is the average loss function among local data samples; $\cR$ is used to denote the expected loss across all the data samples in the network, and lastly, $f$ as the total empirical loss across all the data samples in the network. More explicitly, let $\cZ_j$ be the local dataset at node $j$ with $N $ i.i.d. samples $\bz_{ij}$ for $i \in \{1,\cdots,N\}$ and $j \in \{1,\cdots,M\}$ . Next, for each node $j$ we denote the local empirical loss as the average loss respect to each data sample in local dataset $\cZ_j$ which is $ f_j(\cdot):= \frac{1}{N} \sum\limits_{i=1}^N  \ell_j (\hspace{0.1cm} \cdot \hspace{0.1cm}; \bz_{ij}) $ with 
\begin{align}
    & \bw^*_j \in \argmin_{\bw}\frac{1}{N} \sum\limits_{i=1}^N  \ell_j (\bw; \bz_{ij}) , \quad \bw^* = \bw^*_{\ERM} \in \argmin_{\bw} \frac{1}{MN} \sum\limits_{i=1}^N \sum\limits_{j=1}^M  \ell_j (\bw; \bz_{ij})  ,\nonumber \\
    & \bw^*_{\SR} \in \argmin_{\bw}  \mathbb{E}_{\mathbb{P}} \bigg[\frac{1}{MN} \sum\limits_{i=1}^N \sum\limits_{j=1}^M  \ell_j (\bw; \bz_{ij})\bigg], \quad 
\end{align}
where $f_j$ for all $j \in \{1, \cdots,M\} $. Then from the ERM problem \eqref{eqn: ERM} for any $\bz_{ij} {\sim} \mathbb{P}$ and any deterministic $\bw$, we have that $\hspace{0.1cm} \forall \hspace{0.1cm} j \in \{1,\cdots,M\} $
\begin{align}
    \cR(\bw)&:=\mathbb{E}_{\mathbb{P}} \bigg[\frac{1}{MN} \sum\limits_{i=1}^N \sum\limits_{j=1}^M  \ell_j (\bw; \bz_{ij})\bigg], \\
    \mathbb{E}_{\mathbb{P}} [\nabla f_j(\bw^*_j )] & = \mathbf{0}, \label{ermtemp1} \\
     \nabla\cR(\bw^*_{\SR}) & = \mathbb{E}_{\mathbb{P}} \bigg[\frac{1}{MN} \sum\limits_{i=1}^N \sum\limits_{j=1}^M \nabla \ell_j (\bw^*_{\SR}; \bz_{ij})\bigg] = \mathbb{E}_{\mathbb{P}}  [\nabla f (\bw^*_{\SR})]= \mathbf{0}, \label{ermtemp2}
\end{align}
and 
\begin{align}
     \mathbb{E}_{\mathbb{P}} [\nabla f(\bw^*)] &= \mathbf{0}, \label{ermtemp3} 
\end{align}
\footnote{Note that from here onward we will drop $\mathbb{P}$ subscript for ease of notation.} from the linearity of expectation operator and where $f(\cdot) := \frac{1}{M} \sum_{j=1}^M f_j(\cdot)$.
  Observe that the function $\cR(\cdot)$ is $L$-Lipschitz smooth from Assumption \ref{asumpt1_nonconvex}. We also define that $$ \cR^*_{\SR} :=  \mathbb{E} \bigg[ \frac{1}{MN}\sum\limits_{i=1}^N \sum\limits_{j=1}^M f_j(\bw^*_{\SR} ; \bz_{ij})\bigg], \quad \hat{f}^*_{\ERM} :=   \frac{1}{MN}\sum\limits_{i=1}^N \sum\limits_{j=1}^M f_j(\bw^*_{\ERM} ; \bz_{ij}).$$

We note that in this section, under any given theorem, the convexity (or nonconvexity) of the function $f(\cdot):= \frac{1}{MN} \sum\limits_{i=1}^N \sum\limits_{j=1}^M  \ell_j (\cdot; \bz_{ij}) $ will hold almost surely respective to data distribution $\mathbb{P}$. More formally, we have that the function $f(\cdot)$ will be satisfying either Assumptions \ref{asumpt1_nonconvex}, \ref{asumpt1} (strongly convex class), or Assumptions \ref{asumpt1_nonconvex}, \ref{pl_assumption} (P{\L} function class) or just the Assumption \ref{asumpt1_nonconvex} (smooth nonconvex class) $\mathbb{P}$-almost surely. The next assumption is also required to have a notion of boundedness, almost surely respective to data samples.

\begin{assum}[Statistical uniform boundedness]\label{boundedassumpstat}
    With the setting of ERM problem \eqref{eqn: ERM} with $N$ i.i.d. training samples at each node $j$, the iterate sequence $\{\bw_j(t)\}_t$ for any $j \in \{1, \cdots, M\}$ generated by the RESIST algorithm, stays bounded in some compact set $\cK(N, \{\bz_{ij}\}_{j=1}^N ) \subset \mathbb{R}^d$ $\mathbb{P}$-almost surely as long as the initialization of the algorithm $\{\bw_j(0)\}$ for $j \in \{1, \cdots, M\}$ is bounded. Moreover, for a uniform bounded initialization of the RESIST algorithm with i.i.d. data points $ \{\bz_{ij}\}_{j=1}^N$ and any $N$, we have for any node $j \in \{1, \cdots, M\}$ that
    \begin{align}
         \cK(N, \{\bz_{ij}\}_{j=1}^N) \subset \cK \subset \mathbb{R}^d  \hspace{0.4cm} \mathbb{P}\text{-a.s.}
    \end{align}
     for some compact set $\cK$ defined in Lemma \ref{pl_lem}. 
\end{assum}
Note that the Assumption \ref{boundedassumpstat} is similar compared to Assumption \ref{boundedassump} with only the difference of compact set $\cK(N, \{\bz_{ij}\}_{j=1}^N )$. This compact set depends on the number of samples $N$ and is also random in nature due to the data distribution $\mathbb{P}$. However, to evaluate sample complexity for the RESIST algorithm, we need some form of uniform non-random compactness of iterates. Hence, Assumption \ref{boundedassumpstat} must have uniformly bounded initializations in this section. It must be noted that the Assumption \ref{boundedassumpstat} is not vacuous. When the dataset $\cZ$ is compact, it can be satisfied under some simple example (see section \ref{boundedexistencesec} in Appendix \ref{appendixE} for one such example). 
In the next three subsections, we will provide the statistical learning rate with different types of loss functions corresponding to the ones in Section \ref{sconvex_section1} and \ref{sec:nonconvex convergence rate}.


\subsection{Sample complexity in strongly convex function class}
Recall from Theorem \ref{inexactlmigeo} in Section \ref{sconvex_section1}, which provides a geometric convergence guarantee for the RESIST algorithm with strongly convex loss functions. The two terms $C_0$ and $\Delta$ can be upper bounded by some quantities, which are functions of a number of data samples $N$. In the following theorem, we will show explicitly how the convergence performance is related to the number of data samples, which is often referred to as sample complexity.
\begin{theo}\label{statisticalconvergencethm}
    With ERM formulation in \eqref{eqn: ERM} and $N$ i.i.d. training samples at each node $i$, under Assumptions~\ref{claim2}, \ref{asumpt1_nonconvex}, \ref{asumpt1} and \ref{boundedassumpstat}, the iterate sequence $\{\bw_i(s)\}_s$ generated by algorithm RESIST for any $i \in \cN$ and $ J > \frac{ \tau M\log( 2M^{\frac{3}{2}}(\sqrt{M}+1) )}{\log( 1-\beta^{\tau M} )^{-1}} + \tau M +2$ has a geometric rate in $s$ to an $\cO(h+\frac{h}{\sqrt{N}})$ ball around the minimum of the statistical risk $\bw^*_{\SR}$ with high probability. In particular,
    \begin{itemize}
        \item For any $\epsilon' \in (0,1)$, the consensus errors $ \xi^1_k(s)$, $\xi^5_k(s)$ as defined in Definition \ref{deferrorseq}, for any $k$ satisfy:
   \begin{align}
        \limsup_{s \to \infty} \xi^1_k(s)  &\leq \cO (h M \hspace{0.1cm} \text{diam}(\cK) ) + \cO\bigg(\frac{2M h}{\mu}  \sqrt{\log \bigg( \frac{4d}{\delta} \bigg)} \frac{L^{'}d}{\sqrt{2N}}\bigg), \\
      \limsup_{s \to \infty} \xi^5_k(s)  &\leq  \cO(h M \hspace{0.1cm} \text{diam}(\cK) ) + \cO\bigg(\frac{2M h}{\mu}  \sqrt{\log \bigg( \frac{4d}{\delta} \bigg)} \frac{L^{'}d}{\sqrt{2N}}\bigg),
    \end{align}
with a probability of at least $1-\delta$ where 
\begin{align}
    \delta  = {2d\exp{\bigg(- \frac{2 (\epsilon')^2 M N}{( L^{'}d)^2}\bigg)}}+ {2d\exp{\bigg(- \frac{2 (\epsilon')^2  N}{( L^{'}d)^2}\bigg)}},
\end{align}
for some constant $L^{'}$ satisfing $L^{'} = \max\bigg\{\cO(L d\text{ diam}(\cK)),\cO(L (\text{diam}(\cK))^2) \bigg\}$.
\item    The averaged iterate error $ \norm{\bw^*_{\SR} - \widehat{\bw}^s(s)}  $, for any $\epsilon' \in (0,1)$, for any large enough $N$ and any $h < \min\{\frac{1}{M^2\sqrt{d}}, \frac{2}{\mu +L}\}$, satisfies:
    \begin{align}
   \limsup_{s \to \infty}  \norm{\bw^*_{\SR} - \widehat{\bw}^s(s)} & \leq \cO\bigg(\frac{6}{\mu}\sqrt{\frac{{L^{'}}^2d^2\|{\balpha}\|^2\log\frac{12}{\delta}}{N}}\bigg)  + \cO\bigg(h M \sqrt{Md} \hspace{0.1cm}\text{diam}(\cK) \bigg) 
\end{align}
with a probability of at least $1-\delta$ where 
\begin{align}
     \delta &=  {6d  \exp{\bigg(- \frac{2 (\epsilon')^2 M N}{( L^{'}d)^2}\bigg)}} + {2d\exp{\bigg(- \frac{2 (\epsilon')^2  N}{( L^{'}d)^2}\bigg)}} + \nonumber \\ &
    \hspace{2.5cm}{2\exp\bigg(-\frac{4M N{(\epsilon')}^2}{16(L^{'})^2 M d^2\|{\balpha}\|^2+{(\epsilon')^2}} + M\log\bigg(\frac{12 L^{'}d\sqrt{M}}{{\epsilon'}}\bigg) +d\log\bigg( \frac{12  L^{'}\Gamma_0{d}}{{\epsilon'}}\bigg)\bigg)},
\end{align}
for constant $ \Gamma_0 = \text{diam}(\cK)$ and some vector $\balpha \in \mathbb{R}^M$ such that $\norm{\balpha}^2 \in [\frac{1}{M},1]$. 
\item The averaged iterates converge asymptotically to the exact statistical risk minimizer in probability as data samples approach infinity in the following form:
\begin{align}
   \lim_{N \to \infty} \limsup_{s \to \infty} \bigg(\norm{\bW(s) - \overline{\bW}(s)}_F+ \norm{\bW^*_{\SR} - \widehat{\bW}^s(s)}_F + \norm{\bW(s) -\widehat{\bW}^s(s) }_F\bigg) = 0 \hspace{0.2cm}.
\end{align}
    \end{itemize}
\end{theo}
The proof of this theorem is in Appendix \ref{statisticalconvergencethmproof}.
\begin{rema}
Note that the mixing vector $\balpha$ is not a probability vector. It is actually a vector whose value depends on the screening methods of RESIST and also the behaviors of the failures. It is the actual mixing vector, which is related to the convex combination that drifted away from the exact average because of the impacts of the failures and is the same as defined in \cite{Fang2022BRIDGE}. In the best-case scenario where failure did not drift the converging behavior, and we used 1/M weight assignment in the fully connected network for mixing weights, the value of this vector will be the same and equal to 1/M. However, since we don't know and can not predict the impact of the failures, this quantity is unknown and we can only provide the upper and lower bound of its norm square.
\end{rema}
Theorem \ref{statisticalconvergencethm} consists of three parts. The first part provides asymptotic consensus of local iterates to an order of an $\cO(h+\frac{h}{\sqrt{N}})$ ball with high probability, which the size of the ball can be made arbitrarily small by choosing a small enough $h$ when the sample size is small; the second part provides the asymptotic convergence of the averaged iterates to an order of an $\cO(h+\frac{1}{\sqrt{N}})$ ball with high probability around the statistical minimizer $\bw^*_{SR}$, which the size of the ball can also be made arbitrarily small by choosing a small enough $h$ when the sample size $N$ is large enough; the last part provides the asymptotic exact convergence of the averaged iterates to the statistical minimizer $\bw^*_{SR}$ when sample size $N$ approach infinity.
\subsection{Sample complexity for the P{\L} function class}\label{PL results}
Recall from Theorem \ref{plrate_theo} in Section \ref{sec:nonconvex convergence rate}, which provides a geometric convergence guarantee in function value for the RESIST algorithm with P{\L} functions. The terms $C_0$ and $\Delta$ can be upper bounded by some quantities, which are functions of the number of data samples $N$. In the following theorem, we will show explicitly how the convergence performance in Theorem \ref{plrate_theo} is related to the number of data samples, which is often referred to as sample complexity.
\begin{theo}\label{statisticalconvergencethm_pl}
    With ERM formulation in \eqref{eqn: ERM} and with $N$ i.i.d. training samples at each node $i$, under Assumptions~\ref{claim2}, \ref{asumpt1_nonconvex}, \ref{pl_assumption} and \ref{boundedassumpstat}, the function value sequence $\{f(\widehat{\bw}^s(s))\}_s$ for any $h \in (0, \frac{2}{L})$ and $ J > \frac{ \tau M\log( 2M^{\frac{3}{2}}(\sqrt{M}+1) )}{\log( 1-\beta^{\tau M} )^{-1}} + \tau M +2$ has a geometric rate in $s$ to an $\cO(h+\frac{1}{\sqrt{N}})$ ball around the minimum statistical risk function value $\cR^*_{\SR}$ with high probability. In particular, for any $\epsilon' \in (0,1)$,  for any large enough $N > 1$ and $\sqrt{M} > \mu$ we have that:
    \begin{align}
   \limsup_{s \to \infty}  \lvert{\cR^*_{\SR} - f(\widehat{\bw}^s(s))} \rvert & \leq  \cO\bigg( \frac{ L \hspace{0.1cm} \text{diam} (\cK) }{\mu(2-Lh)}\sqrt{\frac{{L^{'}}^2d^2\|{\balpha}\|^2(\log\frac{12}{\delta})}{N}}\bigg)  +  \cO \bigg(\frac{h L^3 M^{\frac{5}{2}} (d \hspace{0.1cm}\text{diam} (\cK))^2  }{\mu} \bigg) 
\end{align}
with the probability of at least $1-\delta$ where 
\begin{align}
    \delta  &=  {2\exp\bigg(-\frac{4M N{(\epsilon')}^2}{16(L^{'})^2 M d^2\|{\balpha}\|^2+{(\epsilon')^2}} + M\log\bigg(\frac{12 L^{'}d\sqrt{M}}{{\epsilon'}}\bigg) +d\log\bigg( \frac{12  L^{'}\Gamma_0{d}}{{\epsilon'}}\bigg)\bigg)} \nonumber \\ & \hspace{6cm}+ {4d  \exp{\bigg(- \frac{2 (\epsilon')^2 M N}{( L^{'}d)^2}\bigg)}} + 2 \exp{ \bigg( - \frac{2(\epsilon')^2 M N}{(L^{'})^2}\bigg)},
\end{align}
for some constants $L^{'}, \Gamma_0$ (same as in Theorem \ref{statisticalconvergencethm}) and some vector $\balpha \in \mathbb{R}^M$ such that $\norm{\balpha}^2 \in [\frac{1}{M},1]$.
\end{theo}
The proof of this theorem is in Appendix \ref{statisticalconvergencethm_plproof}.

Observe that in Theorem \ref{statisticalconvergencethm_pl} (for P{\L} functions), unlike Theorem \ref{statisticalconvergencethm} (for strongly convex functions), it is hard to provide the statistical rates on the two consensus error terms $ \xi^1_k(s), \xi^5_k(s)$ due to the property of P{\L} functions. The detailed reason is explained in Appendix \ref{appendixE} after the proof of the above theorem.

\subsection{Sample complexity in smooth nonconvex function class}
Recall from Theorem \ref{nonconvexrate_theo} in Section \ref{sec:nonconvex convergence rate}, which provides a sub-linear convergence guarantee for the RESIST algorithm with smooth nonconvex loss functions. The terms $C_0$ can be upper bounded by some quantities, which are functions of the number of data samples $N$. In the following theorem, we will show explicitly how the convergence performance in Theorem \ref{nonconvexrate_theo} is related to the number of data samples, which is often referred to as sample complexity.
\begin{theo}\label{statisticalconvergencethm_nonconvex}
    With the ERM formulation \eqref{eqn: ERM} and with $N$ i.i.d. training samples at each node $i$, under Assumptions~\ref{claim2}, \ref{asumpt1_nonconvex}, \ref{boundedassumpstat}, suppose the algorithm RESIST is iterated with step-size $h:= h(s) = \frac{p}{(s+1)^{\omega}}$ where $\omega = \frac{1}{2} + \epsilon$ with $0< \epsilon <\frac{1}{2}$, $0 < p \leq \frac{1}{2L}$ and let $ J > \frac{ \tau M\log( 2M^{\frac{3}{2}}(\sqrt{M}+1) )}{\log( 1-\beta^{\tau M} )^{-1}} + \tau M +2$. Then the minimum of the norm square of the gradient value over $S$ iterations given by $\min_{0 \leq s \leq S-1}\norm{\nabla f(\widehat{\bw}^{s}(s) )}^2$ has a sub-linear rate of $\cO (\frac{1}{S^{0.5 -\epsilon}})$ to an $\cO(\frac{1}{\sqrt{N}})$ ball around $0$ with high probability. In particular, for any $\epsilon' \in (0,1)$, for any large enough $N > 1$, $d > \epsilon'$ we have that :
     \begin{align}
     \limsup_{S \to \infty}  \min_{0 \leq s \leq S-1}\norm{\nabla f(\widehat{\bw}^{s}(s) )}^2  & \leq   \cO\bigg( L \text{diam}(\cK)\sqrt{\frac{{L^{'}}^2d^2\|{\balpha}\|^2\log\frac{4}{\delta}}{N}}\bigg)   
    \end{align}
with the probability of at least $1-\delta$ where 
\begin{align}
    \delta  &=  {2\exp\bigg(-\frac{4M N{(\epsilon')}^2}{16(L^{'})^2 M d^2\|{\balpha}\|^2+{(\epsilon')^2}} + M\log\bigg(\frac{12 L^{'}d\sqrt{M}}{{\epsilon'}}\bigg) +d\log\bigg( \frac{12  L^{'}\Gamma_0{d}}{{\epsilon'}}\bigg)\bigg)} \nonumber \\ & \hspace{10cm} + {2d  \exp{\bigg(- \frac{2 (\epsilon')^2 M N}{( L^{'}d)^2}\bigg)}}  .
\end{align}
for some constants $L^{'}, \Gamma_0$ (same as in Theorem \ref{statisticalconvergencethm}) and some vector $\balpha \in \mathbb{R}^M$ such that $\norm{\balpha}^2 \in [\frac{1}{M},1]$.

Also,
\begin{align}
   \lim_{N \to \infty} \limsup_{S \to \infty} \min_{0 \leq s \leq S-1}\norm{\nabla f(\widehat{\bw}^{s}(s) )}^2  = 0.
\end{align}
\end{theo}
The proof of Theorem \ref{statisticalconvergencethm_nonconvex} follows directly from Theorem \ref{nonconvexrate_theo} and Lemma \ref{supsampleco_lem}in the infinite sample regime; the result when the number of data sample approach infinity follows directly from Lemma \ref{supsampleco_lem} by taking $N \to \infty$. 
In the following theorem, we will show explicitly how the convergence performance in Theorem \ref{nonconvexrate_theo_fixedstep} is related to the number of data samples, which is often referred to as sample complexity.

\begin{theo}\label{statisticalconvergencethm_nonconvex_fixedstep}
    With the ERM formulation \eqref{eqn: ERM} and with $N$ i.i.d. training samples at each node $i$, under Assumptions~ \ref{claim2}, \ref{asumpt1_nonconvex}, \ref{boundedassumpstat}, suppose the algorithm RESIST is iterated for $S$ gradient steps with a constant step-size $h = \frac{1}{S}$ with $S > L^6(M d\text{ diam}(\cK))^4 $ and $ J > \frac{ \tau M\log( 2M^{\frac{3}{2}}(\sqrt{M}+1) )}{\log( 1-\beta^{\tau M} )^{-1}} + \tau M +2$. Then the following holds for any $\epsilon' \in (0,1)$,  for any large enough $N > 1$ :
     \begin{align}
       \frac{1}{{S}}\sum_{s=0}^{S-1}\norm{\nabla f(\widehat{\bw}^{s}(s) )}^2 & \leq \bigg(1-\frac{L}{\sqrt{S}}\bigg)^{-1}\frac{f(\widehat{\bw}^{0}(0) ) - \inf_{\bw} f(\bw)}{\sqrt{S}} +  \frac{C_9}{\sqrt{S}}  \nonumber  \\ & \hspace{6cm} +  \cO\bigg(L \text{ diam}(\cK)\sqrt{\frac{{L^{'}}^2d^2\|{\balpha}\|^2\log\frac{4}{\delta}}{N}}\bigg)  
    \end{align}
with the probability of at least $1-\delta$ where 
\begin{align}
    \delta  &=  {2\exp\bigg(-\frac{4M N{(\epsilon')}^2}{16(L^{'})^2 M d^2\|{\balpha}\|^2+{(\epsilon')^2}} + M\log\bigg(\frac{12 L^{'}d\sqrt{M}}{{\epsilon'}}\bigg) +d\log\bigg( \frac{12  L^{'}\Gamma_0{d}}{{\epsilon'}}\bigg)\bigg)} \nonumber \\ & \hspace{11cm} +{2d  \exp{\bigg(- \frac{2 (\epsilon')^2 M N}{( L^{'}d)^2}\bigg)}}.
\end{align}
for $C_9 = \cO( L^3(M d\text{ diam}(\cK))^4 )$, some constants $L^{'}, \Gamma_0$ (same as in Theorem \ref{statisticalconvergencethm}) and some vector $\balpha \in \mathbb{R}^M$ such that $\norm{\balpha}^2 \in [\frac{1}{M},1]$. Also, in the infinite sample regime, we have
\begin{align}
    \limsup_{N \to \infty} \frac{1}{{S}}\sum_{s=0}^{S-1}\norm{\nabla f(\widehat{\bw}^{s}(s) )}^2 & \leq \bigg(1-\frac{L}{\sqrt{S}}\bigg)^{-1}\frac{f(\widehat{\bw}^{0}(0) ) - \inf_{\bw} f(\bw)}{\sqrt{S}} +  \frac{C_9}{\sqrt{S}}.
\end{align}
\end{theo}
The proof of Theorem \ref{statisticalconvergencethm_nonconvex_fixedstep} follows directly from Theorem \ref{nonconvexrate_theo_fixedstep} and Lemma \ref{supsampleco_lem}. 
To summarize the asymptotic results from this section, in the strongly convex regime with constant step size (Theorem \ref{statisticalconvergencethm}), we have the $\limsup$ of the iterate error sequence convergence exactly to $0$ as $N \to \infty$; in the P{\L} regime with constant step size $h$ (Theorem \ref{statisticalconvergencethm_pl}), we have the $\limsup$ of the averaged function error sequence converge to an $\mathcal{O}(h)$ ball around $0$ as $N \to \infty$; and finally in the nonconvex regime with diminishing step-size (Theorem \ref{statisticalconvergencethm_nonconvex}) we achieve the $\limsup$ of ``minimum gradient norm" error sequence converge exactly to $0$ as $N \to \infty$.

Up to this point, we have provided the linear algorithmic convergence rate of the RESIST algorithm with smooth and strongly convex objective functions and its statistical convergence rate. Also, linear algorithmic convergence on the function value is also being provided for smooth P{\L} type of objective functions along with its statistical convergence rate. Last but not least, sublinear algorithm convergence rate along with statistical convergence rate are also provided when the objective functions are smooth and nonconvex. The proof of each part is provided in the appendices associated with each section. In the next section, we will showcase how the RESIST algorithm performs when encountering real-life datasets in different settings of experiments.


\section{Numerical Results}\label{numerical section}
The numerical experiments are separated into two main parts. Firstly, we run experiments on the MNIST dataset~\cite{Lecun1998} using a linear classifier with cross-entropy loss plus an %a
$l_2$ regularizer, where the loss is strongly convex, satisfying Assumption \ref{asumpt1}. %, which fully satisfies all our assumptions for the theoretical guarantees in the strongly convex setting (Assumption \ref{asumpt1}). 
In the second part, we run experiments on the CIFAR-10 dataset~\cite{krizhevsky2009learning} using a convolutional neural network, which falls into the class of nonconvex loss functions. Since P{\L} loss functions are special cases of nonconvex loss functions, the performance of RESIST with nonconvex loss functions infers the performance of RESIST with P{\L} loss functions as Assumption \ref{pl_assumption} from Section \ref{PL results}. The network we simulated is the Erdos-Renyi graph with different numbers of nodes $M$ and probability of connection $\rho$.
\subsection{Linear classifier on MNIST}\label{sec: numericalconvex}
The first set of experiments is to showcase the algorithm performs well under a Man-in-the-middle (MITM) attack while the classical Decentralized Gradient Descent (DGD)~\cite{nedic2009} method fails to converge. In the convex setting with independent and identically distributed (i.i.d.) data, we are also going to compare with classical screening methods inherited from distributed/federated learning, and in convex setting with independent and non-identically distributed data, RESIST will be compared with \cite{Peng2020ByzantineRobustDS}, in which the algorithm was termed as ``Byzantine-robust decentralized stochastic optimization'' (DRSA).

The MNIST dataset has 60,000 training images and 10,000 test images of handwritten digits from `0' to `9'. Each image is converted to a 784-dimensional vector, and we distribute 60,000 images equally among $M$ nodes. Then, we connect each pair of nodes with probability $\rho$. During each iteration, up to $b$ number of edges in the network are randomly selected to undergo an MITM attack, which alters the vectors transmitting to the corresponding nodes to a certain value depending on the type of attack. When the network is generated, we check and make sure the network satisfies Assumption \ref{claim2} by ensuring each node has at least $2b+1$ degree with different choices of $b$ ($\rho$ needs to be increased when $b=8$ and $b=16$). Also, even though choosing up to $b$ number of edges in the network to undergo MITM attack makes the actual number of compromised links within any neighborhood $|\cN_j^b(t)|< b$ for most of the iteration, it ensures the Assumption \ref{claim2} will hold for all the iterations during the experiment. We run five sets of experiments as follows : ($i$) RESIST showing linear convergence rate with different choices of parameter $J$; ($ii$) RESIST under Man-in-the-middle attack with different numbers of impacted links compared to classical DGD with multi-step consensus; ($iii$) RESIST with varies sizes of the network when $M=10,20,50$ and $100$; ($iv$) RESIST with different classical screening methods inherited from the distributed/federated setting of learning including Median~\cite{yin2018byzantine}, Krum~\cite{blanchard2017machine} and Bulyan~\cite{mhamdi2018hidden} and ($v$) RESIST and DRSA in extreme non-i.i.d. and moderate non-i.i.d. setting. The performance is evaluated by two metrics: average training loss and average classification accuracy on the 10,000 test images. Note that for all plots, the x-axis represents the total number of training rounds, which includes the iteration of communication and computation, and the actual number of links that undergo MITM attack is equal to the design parameter of the algorithm $b$ except for the experiment being marked as faultless.

\subsubsection{Linear convergence rate with different choices of $J$}
In this experiment setup, we have $M=50, \rho=0.5$, and $b=1$, and we independently and identically distributed all 60,000 training data samples across 50 nodes. We vary the parameter $J$ to be $2,6,11,21,51$. Note that when $J=2$, the algorithm is reduced to BRIDGE \cite{Fang2022BRIDGE} with constant step size. We plot the average training loss vs. total iterations to demonstrate the linear convergence of our algorithm.
\begin{figure}[t]
    \centering
    \includegraphics[width=.4\linewidth]{images/plot_convexJlog.png}
    \caption{Performance comparison of RESIST between different choices of parameter $J$ when the graph and the attack remain the same}
    \label{fig:convexJ}
\end{figure}

As we can see from Figure \ref{fig:convexJ}, when we fix the graph and also the number of compromised links in the network by choosing a larger $J$, the stepsize parameter can be chosen to be larger in order for the algorithm to converge in a faster rate. Also, the straight lines in Figure \ref{fig:convexJ} after about 4000 iterations on a log plot indicate the linear convergence rate of our algorithm. 
\subsubsection{RESIST and DGD with multi-step consensus under Man-in-the-middle attack with different number of impacted links}
In this experiment setup, we have $M=50$, $J=11$, $\rho=0.5$ or $0.75$ or $1$ and the data distribution is i.i.d.. We vary $b$, the design parameter of our algorithm, which is the maximum number of edges that can undergo the MITM attack, to be $0,2,4,8,16$. We also vary $B$, which is the actual number of edges that undergo MITM attack in the network. During each iteration, $B$ number of links are randomly selected to undergo MITM attacks, which alter the information transmitted through this link to some random number. For all the experiments except the one marked "faultless" ($B=0$), we assume that the actual number of edges undergoing MITM attacks $B$ is equal to $b$. For DGD with multi-step consensus, we run experiments only with $B=0$ and $B=1$. Apparently, since DGD with multi-step consensus fails even with only one compromised link, it also can not tolerate more than one compromised link. Noted that in order to have a fair comparison between each run with a different number of compromised edges, especially to ensure the networks satisfy Assumption \ref{claim2}, we increase the probability of connection parameter $\rho$ to 0.75 when $b=8$ and to 1 when $b=16$. 
\begin{figure}[t]
    \centering
    \includegraphics[height=6cm]{images/plot_convexb.png}
    \caption{Comparison of RESIST and DGD with different choices of compromised links in the network}
    \label{fig:convexb}
\end{figure}

From Figure \ref{fig:convexb}, DGD performs well when there is no attack presents with an accuracy of 88.16\%, which matches the state-of-art accuracy for MNIST dataset using linear classifier without data pre-processing and serves as the benchmark of the comparison within this setting. However, the accuracy fails dramatically even with only one compromised link presented in the network, which indicates a single failure can arbitrarily deviate the convergence behavior of DGD with the multi-step consensus. On the other hand, the accuracy of RESIST gradually decreases when the number of compromised links increases in the network. Also, the performance gap between $b=4$ and $b=0$ is about 1.5\%, which is a trade-off that one needs to take into consideration when choosing the robust parameter $b$. Also, when comparing with faulty and faultless settings when $b=4$, the accuracy in the faulty setting is about 0.5\% lower than the one in the faultless setting, which indicates that the impact of the MITM attacks in the network is limited and thus can not arbitrarily deviate the learning behavior of the algorithm, all results above show the trade-off between accuracy and robustness when designing the algorithm.

\subsubsection{RESIST with network of different sizes}
In this experiment setup, we have $b=10\%$ of $M$, $J=11$ or $21$, $\rho=0.5$ and the data distribution is i.i.d.. We vary $M$, the number of nodes in the network, to be $10,20,50,100$. This set of experiments shows how the algorithm behaves when the size of the network changes. The actual number of compromised links under go random MITM attack in this setup is equal to $b$. To simulate the similar impact of the compromised links to the learning process among the network, we keep the number of compromised links to be $10\%$ of $M$ when the network size grows.
\begin{figure}[t]
    \centering
    \includegraphics[height=6cm]{images/plot_convexM.png}
    \caption{Comparison of RESIST with network of different sizes}
    \label{fig:convexM}
\end{figure}

It can be seen from Figure \ref{fig:convexM} that the convergence behavior and accuracy are quite similar when the network size increases until $M$ reaches $100$. When $M=100$ and $J=11$, the oscillations occur after 7000 iterations, impacting the convergence behavior, which also aligns with our theoretical guarantee in Theorem \ref{inexactlmigeo} that a larger $J$ needs to be adapted when the size of network $M$ increases. Note that even though the lower bound in Theorem \ref{inexactlmigeo} is quite loose, and thus we do not need to scale $J$ in a way as in Theorem \ref{inexactlmigeo}, $J$ still needs to be increased according to the growing size of the network. Next, we run an additional experiment with $J=21$. In this case, as shown in Figure \ref{fig:convexM}, the RESIST algorithm could achieve similar convergence behavior and final accuracy compared to the performance of RESIST with smaller-sized networks. 


\subsubsection{RESIST, RESIST-M, K and B with two and four compromised links}
In this experiment setup, we have $M=50$, $b=2$ or $4$, $J=11$, $\rho=0.5$ and the data distribution is i.i.d.. The actual number of compromised links that undergo random MITM attacks in this setup is equal to $b$. We vary the screening methods established in a distributed setting to see how our proposed algorithm can be adapted to other screening methods. We denote RESIST-M, RESIST-K, and RESIST-B as RESIST algorithms by replacing coordinate-wise trimmed mean screening methods with Median, Krum, and Bulyan, respectively.
\begin{figure}[t]
    \centering
    \includegraphics[height=6cm]{images/plot_convexTMKB.png}
    \caption{Comparison of RESIST, RESIST-M, K, and B with two and four compromised links}
    \label{fig:convexTMKB}
\end{figure}

As we can see from Figure \ref{fig:convexTMKB}, RESIST with all four screening methods performs well with some minor differences in average validation accuracy. When the compromised links increase from two to four, the performance of RESIST with each screening method has a slightly degraded performance, which is expected since a larger portion of links in the network is impacted by the Man-in-the-middle attack.
\subsubsection{RESIST and DRSA with two and four compromised links in non-i.i.d. setting}
We provided convergence guarantees for RESIST in previous sections for both strongly convex and nonconvex loss functions. However, the main results are based on the independent and identical distribution (i.i.d.) of the dataset. In the robust decentralized optimization/ML literature,~\cite{Peng2020ByzantineRobustDS}, which is termed as ``Byzantine-robust decentralized stochastic optimization'' (DRSA) and BRIDGE~\cite{Fang2022BRIDGE} are the ones that provided experimental results in the non-i.i.d. setting. Since RESIST with $J=2$ reduces to BRIDGE with constant stepsize, as discussed in the previous sections, we only compare our method to DRSA in this section. Note that for both non-i.i.d. setup, we have $M=50$, $b=2$ or $4$, $J=11$ and $\rho=0.5$. Also, we consider the attack model as random MITM attacks for RESIST, and we adapt the DRSA algorithm from Byzantine attacks to random MITM attacks. We compare RESIST with DRSA~\cite{Peng2020ByzantineRobustDS} in the following non-i.i.d. settings to showcase the performance of RESIST even with the lack of theoretical convergence guarantees:

\begin{figure}
\centering
    \begin{subfigure}
    \centering
    \includegraphics[width=.4\linewidth]{images/plot_exnoniid.png}
    \end{subfigure}
    \begin{subfigure}
    \centering
    \includegraphics[width=.4\linewidth]{images/plot_mnoniid.png}
    \end{subfigure}
\caption{Comparison of RESIST with DRSA with zero, two, and four compromised links in the non-i.i.d. setting}
\label{fig:convexnoniid}
\end{figure}
\textbf{Extreme non-i.i.d. setting:} We partition the dataset corresponding to labels, and for a network with 50 nodes, we distribute all the samples labeled “0” to the first five nodes, then distribute all the samples labeled “1” to the next five nodes, and so on. We can see from the first figure in Figure~\ref{fig:convexnoniid} that in the faultless setting, both algorithms perform well, while in the case when the number of compromised links increases to two, we can see a slight decrease in the accuracy of both algorithms for about 1 percent and when the number of compromised links increase to 4, there is about 3 percent of accuracy drop due to the extreme non-i.i.d. distribution of data for both algorithms. Even though there is a lack of theoretical guarantees of RESIST in the non-i.i.d. setting, the intuition behind the result is that somehow, the attack could utilize the weakness of the data distribution to further harm the algorithm from achieving better performance. Even though the impact is enlarged with the number of attacks increased in the extreme non-i.i.d setting, the impact is not as significant as the one in the non-i.i.d. experiment result of~\cite{Fang2022BRIDGE}. In~\cite{Fang2022BRIDGE}, the gap between faultless extreme non-i.i.d. and faulty extreme non-i.i.d. setting is about 8 percent. The reason behind this is that when considering Byzantine attack as in~\cite{Fang2022BRIDGE}, the attack has the ability to poison the local dataset. Because of the extreme non-i.i.d. nature, the majority of data from one label can not be retrieved, while in the MITM attack setting, all the local datasets are not affected by the attack; thus, the performance gap caused by the attack in extreme non-i.i.d. setting is not as significant as the one in the Byzantine attack setting.

\textbf{Moderate non-i.i.d. setting:} We partition the dataset corresponding to its labels and distribute the samples associated with each label evenly to 10 nodes. Every node receives only two sets of differently labeled data evenly. As we can see from the second figure in Figure~\ref{fig:convexnoniid}, both algorithms perform well in the presence of zero, two, or four compromised links. We conclude from the previous two experimental results from the non-i.i.d. setting that less impact will occur if the data distribution is more toward i.i.d.. Exactly how much the impact on the theoretical convergence guarantee in different non-i.i.d. settings will be one of the future directions of this work.
\subsection{Convolutional Neural Networks on CIFAR-10}\label{sec: numericalnonconvex}
The second set of experiments showcases that the algorithm performs well with MITM attack while the DGD with multi-step consensus fails with nonconvex loss functions. The Convolutional Neural Networks (CNNs) are constructed with four convolutional layers followed by one max pooling layer after each convolutional layer. Two fully connected layers are added after the convolutional and max-pooling layers. The CIFAR-10 dataset has 50,000 training images and 10,000 test images of 10 different classes. Each image is converted to a 3072-dimensional vector, and we distribute 50,000 images equally among 50 nodes. Then, we connect each pair of nodes with probability $\rho$. During each iteration, up to $b$ number of edges are randomly selected to undergo MITM attacks, which alters the vectors transmitting to the corresponding nodes to a certain value depending on the type of attack. We check and make sure the network satisfies Assumption \ref{claim2} by ensuring each node has at least $2b+1$ degree with different choices of $b$ ($\rho$ needs to be increased when $b=8$ and $b=16$). We run five sets of experiments; we vary only one or two variables at a time and fix all the rest to showcase the performance of the model training in various cases: (i) The performance of RESIST with different choices of parameter $J$; (ii) RESIST under Man-in-the-middle attack impacting different number of links in the network compared to DGD with multi-step consensus; (iii) RESIST with different classical screening methods inherited from the distributed/federated setting of learning including Median, and Krum; (iv) MIM-T under different types of MITM attack and (v) RESIST with varies sizes of the network when $M= 10, 20, 50$ and $100$. The performance is evaluated by the average classification accuracy on the 10,000 test images. Note that the x-axis represents the total number of training rounds, which includes the iteration of communication and computation, and the actual number of links that undergo MITM attack is equal to the robust parameter $b$ except for experiments being marked as faultless.


\subsubsection{Performance of RESIST with different number of parameter $J$}
In this experiment setup, we have $M=50, \rho=0.5$, and $b=1$, and the data distribution is i.i.d. We vary the parameter $J$ to be $2,3,6,9$. Note that when $J=2$, the algorithm is reduced to BRIDGE \cite{Fang2022BRIDGE} with constant stepsizes. 
\begin{figure}
\centering
\includegraphics[height=6cm]{images/plot_nonconvexJ.png}
\caption{Comparison between different choices of parameter $J$}
\label{fig:nonconvexJ}
\end{figure}

From Figure \ref{fig:nonconvexJ}, when we fix the graph and also the number of compromised links in the network, increasing $J$ achieves better accuracy until $J$ reaches 6. Compared to BRIDGE with constant stepsizes, both $J=3$ and $J=6$ achieves better accuracy with similar speed of convergence while when $J=9$, the speed of convergence is relatively slow, however, the final accuracy is higher than BRIDGE. Note that although we have provided a lower bound on $J$, due to the looseness of this lower bound and the fact that the iteration budget in the experiments is limited, choosing larger $J$ will not always benefit the convergence behavior as shown in this set of experiments. Thus, most of the time, $J$ will not be required to be lower bounded as in Theorem \ref{inexactlmigeo} and should often be treated as a hyper-parameter for experiments. 

\subsubsection{RESIST under Man-in-the-middle attack impacting different numbers of links in the network compared to DGD with multi-step consensus}\label{vanilla-DGD}
In this experiment setup, we have $M=50$, $J=6$, $\rho=0.5$, and the data distribution is i.i.d. We vary $b$, the design parameter of our algorithm, which is the maximum number of edges that RESIST can defend from the MITM attack, to be $0,1,2,4$. We also vary the number of edges that actually undergo MITM attack in the network. For all the experiments except the one marked "faultless," we assume that the actual number of links that undergo MITM attacks is equal to $b$. Since DGD with multi-step consensus fails even with only one compromised link, it can not tolerate more than one compromised link. 
\begin{figure}[t]
    \centering
    \includegraphics[height=6cm]{images/plot_nonconvexb.png}
    \caption{Comparison of RESIST and Vanilla-DGD with different choices of compromised links in the network}
    \label{fig:nonconvexb}
\end{figure}

From Figure \ref{fig:nonconvexb}, DGD with multi-step consensus performs well when there is no attack present with an accuracy of 59.16\%, which aligns with the accuracy of the centralized setting and also serves as the benchmark of the comparison within this setting. However, the accuracy fails dramatically even with only one compromised link presented in the network, which indicates a single failure can arbitrarily deviate the convergence behavior of DGD. On the other hand, the accuracy of RESIST gradually decreases once the maximum number of compromised links $b$ increase in the network. The performance gap between $b=0$ and $b=4$ is about 1.3\%, which indicates a trade-off between robustness and accuracy; also, when comparing with faulty and faultless settings when $b=1$, the faulty setting is about 0.4\% lower than the faultless setting, which illustrates that the impact of the MITM attacks in the network is limited and thus can not arbitrarily deviate performance of the algorithm.
\subsubsection{RESIST, RESIST-M, and K with one, two, and four compromised links}
In this experiment setup, we have $M=50$, $b=1$ or $2$ or $4$, $J=6$, $\rho=0.5$ and the data distribution is i.i.d.. We vary the screening methods established in distributed settings to see whether another screening method could be applied to our algorithm.
\begin{figure}[t]
    \centering
    \includegraphics[height=6cm]{images/plot_nonconvexTMK.png}
    \caption{Comparison of RESIST, -M, and -K with one, two, and four compromised links}
    \label{fig:nonconvexTMK}
\end{figure}

Observing from Figure \ref{fig:nonconvexTMK}, RESIST with coordinate-wise trimmed mean and coordinate-wise median screening methods performs well with some minor differences in average validation accuracy even when $b$ increases. On the other hand, RESIST with the Krum screening method seems to suffer more from the attack for some reason but is still much better than DGD with multi-step consensus in Section \ref{vanilla-DGD}.
\subsubsection{RESIST with different types of attacks}
In this experiment setup, we have $M=50$, $b=2/4$, $J=6$, $\rho=0.5$ and the data distribution is i.i.d.. We choose different types of MITM attacks as the one imposed into the distributed network, including random attacks~\cite{YoungRandom1997, BellareRandom2014}, sign-flipping attacks~\cite{TaranSign2019, xuSign2023, ParkSign2024}, label flipping/data poisoning attack~\cite{AlfeldLabel2016, TolpeginLabel2020, YerlikayaLabel2022} and constant attack~\cite{TilborgConstant2011, andersonConstant2020} to see how the algorithm performs under different types of MITM attacks.
\begin{figure}[t]
    \centering
    \includegraphics[height=6cm]{images/plot_nonconvexbt.png}
    \caption{Performance of RESIST with different types of MITM attack}
    \label{fig:convexTMKnc}
\end{figure}

As we can see from Figure \ref{fig:convexTMKnc}, RESIST is more robust to random MITM attacks. This is because altering information into random values could be easily captured by the coordinate-wise trimmed mean screening method compared to other types of MITM attacks. The accuracy gap between different numbers of compromised links within the same attack is small ($\sim$0.5\%). In contrast, the accuracy gap with the same number of compromised links across different attack types is relatively large ($\sim$1\%-3\%). 
\subsubsection{RESIST on different sizes of the network}
In this experiment setup, we have $b=10\%$ of $M$, $J=3$ or $6$ or $11$, $\rho=0.5$ and the data distribution is i.i.d.. We vary $M$, the number of nodes in the network, to be $10, 20, 50, 100$. This set of experiments is to show algorithm performance when the network size grows. To simulate the same/similar impact of the compromised links to the learning process among the network, we keep the number of compromised links to be $10\%$ of $M$ when the network size grows.
\begin{figure}[t]
    \centering
    \includegraphics[height=6cm]{images/plot_nonconvexM.png}
    \caption{Performance of RESIST with different size of the network}
    \label{fig:nonconvexM}
\end{figure}

It can be seen from Figure \ref{fig:nonconvexM} that when the parameter $J$ is fixed to be $6$, increasing the size of the network while keeping the same ratio of comprised links tends to achieve better accuracy until $M=50$. This fact complies with the theoretical lower bound in Theorem \ref{inexactlmigeo}, which indicates the need for larger $J$ when the network size increases. To visualize this impact, we also run our algorithms with different $J$ when the size of the network $M$ is fixed. As we can see from Figure \ref{fig:nonconvexM}, when $M=20$, the increased $J$ with the same ratio of compromised links achieves similar performance, indicating that both $J=3$ and $J=6$ are suitable for the size of the network. However, when $M=100$, it is more desired to use a larger $J$ to achieve better performance, which coincides with the results in Figure \ref{fig:nonconvexM}. Tuning the hyperparameter $J$ for the RESIST algorithm is crucial because one could reduce the computational cost of gradient calculation locally. 

\subsubsection{MIM-T with diminishing stepsize} 
Within this set of experiment, we have $b=1$ or $2$ or $4$ , $J=6$, $\rho=0.5$ and the data distribution is i.i.d.. We choose stepsize as constant/diminishing to observe the algorithm's behavior with nonconvex objective functions. For the diminishing-stepsize scenario, the choice of decaying rate is $\frac{1}{t}$. From the theoretical perspective, Theorem \ref{statisticalconvergencethm_nonconvex} provides the asymptotic result of the norm of a minimum of the gradient shrinks to zero in the statistical setting using a proper diminishing stepsize while Theorem \ref{statisticalconvergencethm_nonconvex_fixedstep} indicating an upper bound of the average norm of the gradient within a finite horizon.
\begin{figure}[t]
    \centering
    \includegraphics[height=6cm]{images/plot_nonconvexdim.png}
    \caption{Comparison of RESIST with different sizes of the network}
    \label{fig:nonconvexdim}
\end{figure}
As shown in Figure \ref{fig:nonconvexdim}, the convergence speed of the algorithm is faster when constant stepsizes are employed. The final performance in the diminishing stepsize regime is almost the same as the constant regime, indicating that choosing a proper constant stepsize could be more favorable than diminishing stepsize in this setting. Note that for some scenarios that require near-exact convergence performance, one still needs to choose diminishing stepsizes even though it suffers from a slower convergence rate.

\section{Conclusion}\label{conclusion}
In this work, we introduced a novel algorithm named Robust decentralized learning with
consensus gradient descent (RESIST) and it is designed to solve optimization/machine learning problems with the dataset distributed among the network. We provided its algorithmic convergence rate along with statistical convergence rate in three main types of loss functions, including strongly-convex, P{\L} and smooth nonconvex loss functions. To our best knowledge, it is the first work that formally introduces the Man-in-the-middle attacks in decentralized optimization problems coped with algorithmic convergence guarantee and statistical learning in strongly convex, P{\L}, and smooth nonconvex settings. Numerical experiments are also being provided to emphasize the algorithm's usefulness in real-life datasets such as MNIST and CIFAR-10 with different choices of algorithm design parameters. Other directions include dealing with the non-i.i.d. of the dataset, asynchronous communication protocol, improving the convergence rate/statistical learning rate and the analysis of other popular screening methods in the decentralized literate will remain in our future works.






\begin{appendices}
\section{Supporting Preliminaries on the Connectivity of the Network } \label{section*vaidya_10}
In this Appendix, we will provide some preliminaries regarding the network connectivity and its associated Lemmas, corollaries, and definitions, which will help us derive the consensus and convergence rate of the RESIST algorithm in Section \ref{sconvex_section1} and \ref{sec:nonconvex convergence rate}.
\subsection{Adaptation of Claim 2 from \cite{Vaidya2012matrix} used to prove geometric mixing rate along coordinates in Section \ref{section:Geometric consensus rate along coordinates}} \label{section*vaidya}

Recall from Lemma \ref{weight assign lemma} that the mixing matrix $ \mathbf{Y}_{k}(t)$ depends on the coordinate $k$, and for the sake of simplicity of notation, we omit the $k$-dependency for the rest of this appendix. Furthermore, since the mixing operations from the step \ref{weight assignment in center set} in the sub-routine \ref{CWTM} occur independently across all $k \in \{1,\dots, d\}$, we can, without loss of generality take $d=1$. In that case, the state matrix $\bW(t)$ from Lemma \ref{weight assign lemma} will be an $M$-dimensional vector.

Denote by $\mathbf{v}(0)$ the column vector consisting of initial model parameters of all nodes. Denote by $\mathbf{v}(t)$, where $t \geq 1$, the column vector with size $M$, consisting of the model parameter of all the nodes at the end of the $t$-th iteration, $t \geq 1$. Note that the vector $\mathbf{v}(t)$ is simply the matrix $\bW(t)$ from Lemma \ref{weight assign lemma} for $d=1$. The $i$-th element of vector $\mathbf{v}(t)$ is $v_{i}(t)$. Also, let $\by_i(t)$ be the $i$-th row vector of the matrix $\bY(t)$, where $i \in \cN$.

\begin{coro}\label{claim1}
    We can express the iterative update of the model parameter of any node $ i \in\{1,\cdots, M\} $ performed in the CWTM step of Algorithm \ref{gradient descent algorithm} using the matrix form in the equation below:

\begin{align}\label{update of v_t}
    v_{i}(t)=(\by_i(t)) \mathbf{v}(t-1).
\end{align}
\footnote{$\by_i(t)$ is the vector corresponding to the $i$-th row of the matrix $\mathbf{Y}(t)$. In addition to $t$, vector $\by_i(t)$ may depend on the vector $\mathbf{v}(t-1)$ as well as the behavior of the compromised links to the $i$-th node which are under attack at time $t-1$. For simplicity, the notation $\by_i(t)$ does not explicitly represent this dependence.}
The $i$-th row vector $\by_i(t)$ of the matrix $\mathbf{Y}(t)$  satisfies the following four conditions.
\begin{enumerate}
    \item $\by_i(t)$ is a stochastic row vector of size $M$. Thus, $[\mathbf{Y}(t)]_{ij}\geq 0$, for $1 \leq j \leq M$, and $\sum_{1 \leq j \leq M} [\mathbf{Y}(t)]_{ij}=1$.
\item  $[\mathbf{Y}(t)]_{ii}$ equals $a_{i}$ with $a_i= \frac{1}{\vert\cN_i\vert-2b+1}$ which is the weight that one node assigns to itself.
\item  $[\mathbf{Y}(t)]_{ij}$ is non-zero only if $(j, i) \in \mathcal{E}$ or $j=i$.
\item At least $\left|\cN_{i} \backslash \cN_i^b \right|-b+1$ elements in $[\mathbf{Y}(t)]_{i}$ are lower bounded by some constant $\beta>0$  where $\cN_i^b$ denotes the set of neighboring nodes that have compromised links between them and node $i$ and $b$ is the design parameter of the algorithm as the upper bound on the number of compromised links the algorithm can defend against within each neighborhood. Note that $\beta$ is independent of $i$ and $t$ and the explicit choice of $\beta$ will be provided later in section \ref{grapht_prelim}.
\item For $b<\min_j \frac{|\cN_j|}{2}$, the scalar $ v_{i}(t) $ is a convex combination of the entries of the vector $\bv(t)$.
\end{enumerate}
\end{coro}
The proof of this corollary is similar to Claim 2's proof from \cite{Vaidya2012matrix} (except having compromised nodes, we have compromised links) and hence omitted for brevity.


  
\subsection{Assumption on graph connectivity and its implications used to prove geometric mixing rate along coordinates in Section \ref{section:Geometric consensus rate along coordinates}}\label{grapht_prelim}
From \cite{Vaidya2012matrix}, we derive some basic results to establish the geometric mixing rate along coordinates. Recalling the filtered graph topology ${\mathcal{T}}_{\mathcal{F}}$ from Definition \ref{def1a}, let $\mathbf{H}$ denote the connectivity matrix for graph $\cH \in {\mathcal{T}}_{\mathcal{F}} $ where $ \mathbf{H}$ has entries $1$ corresponding to an incoming edge and $0$ otherwise.

\begin{lemm}[Adaptation of Lemma 1 from \cite{Vaidya2012matrix}]\label{lemmanonzerocol}
    For any $\cH \in {\mathcal{T}}_{\mathcal{F}}$, the matrix power $\mathbf{H}^{M}$ has at least one non-zero column.
\end{lemm}

The proof is provided in \cite{Vaidya2012matrix}.% By Assumption \ref{claim2}, in graph $\cH$, at least one node, say node $k$, has a directed path in $\cH$ to all the remaining nodes in $\cH$. Since the length of the path from $k$ to any other node in $\cH$ can contain at most $M-1$ directed edges (the total nodes in $\cH$ are $M$), the $k$-th column of the matrix $\mathbf{H}^{M}$ will be non-zero. 

 
\begin{defi}\label{def of B}
    An element of a matrix is ``non-trivial" if it is lower bounded by a positive quantity $\beta$.
\end{defi}
Recall that from Corollary \ref{claim1} we have $a_i = \frac{1}{\left|\cN_{i} \right|-2b+1}$ and hence we can set $\alpha =  \frac{1}{M-2b+1} $ where Corollary \ref{claim1} holds for both case $(i), (ii)$ with corresponding formulation of $\by_i(t)$ in \eqref{elements in M} and \eqref{elements in MM} respectively. Then, along similar lines as in \cite{Vaidya2012matrix}, we choose $\beta$ as 
 \begin{align}
     \beta = \min_{k,i} \frac{\alpha}{2 q^k_i} = \frac{\alpha}{4b}.
 \end{align}
\begin{lemm}[Adaptation of Lemma 2 from \cite{Vaidya2012matrix}]\label{betalemma}
    For any $t \geq 1$, there exists a filtered graph $\cH(t)$ such that it is equivalent to one of the filtered graphs $\cH \in{\mathcal{T}}_{\mathcal{F}}$ and $\beta \mathbf{H}(t) \leq \mathbf{Y}(t)$, where $\mathbf{H}(t)$ is the connectivity matrix associated with the filtered graph $\cH(t)$ at time $t$ and $\beta$ is defined above.
\end{lemm}

\begin{proof}
  The proof of this lemma follows along similar lines as in \cite{Vaidya2012matrix}. Observe that the $i$-th row of the weight matrix $\mathbf{Y}(t)$ corresponds to the $\bv(t)$ update performed at node $i$. Recall that $[\mathbf{Y}(t)]_{i j}$ is non-zero only if link $(j, i) \in \mathcal{E}$. Also, by Corollary \ref{claim1}, $ \mathbf{y}_{i}(t)$ (i.e., the $i$-th row of $\mathbf{Y}(t)$) contains at least $\left|\cN_{i} \backslash \cN_i^b\right|-b+1$ non-trivial elements corresponding to uncompromised incoming edges of node $i$ and itself (i.e., the diagonal element).

Now observe that, for any filtered graph $\cH \in {\mathcal{T}}_{\mathcal{F}}$,the $i$-th row of $\mathbf{H}$ contains exactly $\left|\cN_{i} \backslash \cN_i^b\right|-b+1$  non-zero elements, including the diagonal element. Considering the above two observations and the definition of set ${\mathcal{T}}_{\mathcal{F}}$, the lemma follows.
\end{proof}

\subsection{Stochastic matrix properties used to prove geometric mixing rate along coordinates in Section \ref{section:Geometric consensus rate along coordinates}  }\label{section*vaidya1}
We note that this subsection has been presented in \cite{Vaidya2012matrix}, but we give its details here to clarify definitions and properties used in our analysis.
%\begin{defi}
    %A vector is said to be stochastic if all the elements of the vector are nonnegative and the elements add up to 1. A matrix is said to be row stochastic if each row of the matrix is a stochastic vector.
%\end{defi}
For a row stochastic matrix $\mathbf{A}$, coefficients of ergodicity $\delta(\mathbf{A})$ and $\lambda(\mathbf{A})$ are defined as \cite{wolfowitz1963products}:

$$
\begin{aligned}
\delta(\mathbf{A}) & :=\max _{j} \max _{i_{1}, i_{2}}\left|[\mathbf{A}]_{i_{1} j}-[\mathbf{A}]_{i_{2} j}\right| \\
\lambda(\mathbf{A}) & :=1-\min _{i_{1}, i_{2}} \sum_{j} \min \left([\mathbf{A}]_{i_{1} j}, [\mathbf{A}]_{i_{2} j}\right) .
\end{aligned}
$$
It is easy to see that $0 \leq \delta(\mathbf{A}) \leq 1$ and $0 \leq \lambda(\mathbf{A}) \leq 1$, and that the rows are all identical if and only if $\delta(\mathbf{A})=0$. Additionally, $\lambda(\mathbf{A})=0$ if and only if $\delta(\mathbf{A})=0$. %\MG{If the matrix is two-by-two with all entris 0.5; then $\delta(A)=0$ but $\lambda(A)= 0.5$?}

The next result from \cite{hajnal1958weak} establishes a relation between the coefficient of ergodicity $\delta(\cdot)$ of a product of row stochastic matrices and the coefficients of ergodicity $\lambda(\cdot)$ of the individual matrices defining the product.


\begin{prop}[\cite{hajnal1958weak}]\label{claimstochastic}
    Let $\mathbf{Q}(1), \mathbf{Q}(2), \ldots \mathbf{Q}(p)$ be square row-stochastic matrices with the same dimensions and $p\geq 1$. Then,
$\delta(\mathbf{Q}(1) \mathbf{Q}(2) \cdots \mathbf{Q}(p)) \leq \Pi_{i=1}^{p} \lambda(\mathbf{Q}(i)).
$
\end{prop}
Proposition \ref{claimstochastic} implies that if, for all $i, \lambda(\mathbf{Q}(i)) \leq 1-\gamma$ for some $\gamma>0$, then $\delta(\mathbf{Q}(1), \mathbf{Q}(2) \cdots \mathbf{Q}(p))$ will 
go to zero as $p\to\infty$.We next consider the notion of a scrambling matrix, which has also been considered in the literature \cite{hajnal1958weak, wolfowitz1963products}.
\begin{defi}\label{defscramble}
     A row stochastic matrix $\mathbf{H}$ is said to be a scrambling matrix if $\lambda(\mathbf{H})<1$. 
\end{defi}

\begin{rema}\label{scrambling}
In a scrambling matrix $\mathbf{H}$, since $\lambda(\mathbf{H})<1$, for each pair of rows $i_{1}$ and $i_{2}$, there exists a column $j$ (which may depend on $i_{1}$ and $i_{2}$ ) such that $[\mathbf{H}]_{i_{1} j}>0$ and $[\mathbf{H}]_{i_{2} j}>0$, and vice-versa \cite{hajnal1958weak, wolfowitz1963products}. As a special case, if any one column of a row stochastic matrix $\mathbf{H}$ contains only nonzero elements that are lower bounded by some constant $\gamma>0$, then $\mathbf{H}$ must be scrambling, and $\lambda(\mathbf{H}) \leq 1-\gamma$.
\end{rema}

\subsection{Consensus guarantees with geometric convergence}%Geometric consensus guarantees
\label{section*vaidya1010}
To show that a consensus is achieved with geometric rates, 
%establish consensus guarantees,
%a geometric rate of consensus
we again follow the proof techniques from \cite{Vaidya2012matrix}. 

\begin{lemm}[Adaptation of Lemma 3 from \cite{Vaidya2012matrix}]\label{geometlemma}
    In the product below of $\mathbf{H}(t)$ matrices for consecutive $\tau M$ iterations for any $z \geq 0$, at least one column is non-zero,
$$
\prod_{t=z}^{z+\tau M-1} \mathbf{H}(t).
$$
\end{lemm}

\begin{proof}
     Since the product $\prod_{t=z}^{z+\tau M-1} \mathbf{H}(t)$ consists of $\tau M$ matrices in ${\mathcal{T}}_{\mathcal{F}}$, at least one of the $\tau$ distinct connectivity matrices in ${\mathcal{T}}_{\mathcal{F}}$, say matrix $\mathbf{H}_{*}$, will appear in the above product at least $M$ times by pigeonhole principle.

Now observe that: (i) By Lemma \ref{lemmanonzerocol}, $\mathbf{H}_{*}^{M}$ contains a non-zero column; say the $k$-th column is non-zero, and (ii) all the $\mathbf{H}(t)$ matrices in the product has the property that all the elements in the diagonal are non-zero. These two observations together imply that the $k$-th column in the above product is non-zero.
\end{proof}
Let us now define a sequence of matrices $\mathbf{Q}(i)$, which will also be used in Section \ref{sconvex_section1} such that each of these matrices is a product of $\tau M$ of the $\mathbf{Y}(t)$ matrices. Specifically, $\mathbf{Q}(i)=\prod_{t=(i-1) \tau M+1}^{i \tau M} \mathbf{Y}(t)$. Combining the above equality with \eqref{update of v_t} we have: $\mathbf{v}(k \tau M)=\left(\prod_{i=1}^{k} \mathbf{Q}(i)\right) \mathbf{v}(0)$.
\begin{lemm}[Adaptation of Lemma 4 from \cite{Vaidya2012matrix}]\label{lemmascramble}
    For $i \geq 1, \mathbf{Q}(i)$ is a scrambling row stochastic matrix, and $\lambda(\mathbf{Q}(i))$ is bounded from above by $1 - \beta^{\tau M}$.
\end{lemm}
\begin{proof}
    $\mathbf{Q}(i)$ is a product of row stochastic matrices $\{\mathbf{Y}(t)\}$, therefore, $\mathbf{Q}(i)$ is row stochastic. From Lemma \ref{betalemma}, for each $t$, $\beta \mathbf{H}(t) \leq \mathbf{Y}(t)$. Therefore, $\beta^{\tau M} \prod_{t=(i-1) \tau M+1}^{i \tau M} \mathbf{H}(t) \leq \mathbf{Q}(i).$

Using $z=(i-1)M+1$ in Lemma \ref{geometlemma}, we conclude that the matrix product on the left side of the above inequality contains a non-zero column. Therefore, $\mathbf{Q}(i)$ also contains a non-zero column. Therefore, $\mathbf{Q}(i)$ is a scrambling matrix by Remark \ref{scrambling}.

Observe that $\tau M$ is finite, therefore, $\beta^{\tau M}$ is non-zero. Since the non-zero terms in $\mathbf{H}(t)$ matrices are all 1, the non-zero elements in $\prod_{t=(i-1) \tau M+1}^{i \tau M} \mathbf{H}(t)$ must each be greater than or equal to $1$.
Therefore, there exists a non-zero column in $\mathbf{Q}(i)$ with all the elements in the column being greater than or equal to $\beta^{\tau M}$. Therefore $\lambda(\mathbf{Q}(i)) \leq 1-\beta^{\tau M}$.

\end{proof}

\begin{lemm}\label{geometricsupplementlemma}
For the update $\mathbf{v}(t)=\mathbf{Y}(t) \bv(t-1)$, and some time index $t_0$ we have the following geometric rate for $t > t_0$ and every $i$ and $j$:
    \begin{align}
        | [\bPhi(t,t_0)]_{ji} - [\bc]_i | \leq  ( 1-\beta^{\tau M} )^{\left\lfloor\frac{t-t_0}{\tau M}\right\rfloor}
    \end{align}
    for some vector $\bc $ that has identical rows and $ \bPhi(t,t_0) :=\bY(t)\bY(t-1)\cdots\bY(t_0)$. Also, for some positive $\balpha= \alpha\mathbf{1}$ with a  positive scalar $\alpha$ we have that 
    $$
\lim _{t \rightarrow \infty} \mathbf{v}(t)=\balpha.
$$   
\end{lemm}

\begin{proof}
    

By Proposition \ref{claimstochastic},
\begin{align}
\lim _{t \rightarrow \infty} \delta\left(\Pi_{i=t_0}^{t} \mathbf{Y}(i)\right) & \leq \lim _{t \rightarrow \infty} \Pi_{i=t_0}^{t} \lambda(\mathbf{Y}(i)) \\
& \leq \lim _{t \rightarrow \infty} \Pi_{i=t_0}^{\left\lfloor\frac{t}{\tau M}\right\rfloor} \lambda(\mathbf{Q}(i)) \\
& =0.
\end{align}
The above argument makes use of the facts that $\lambda(\mathbf{Y}(t)) \leq 1$ and $\lambda(\mathbf{Q}(i)) \leq\left(1-\beta^{\tau M}\right)<1$ from Lemma \ref{lemmascramble}. Thus, the rows of the matrix $\prod_{i=t_0}^{t} \mathbf{Y}(i)$ become identical as $t \to \infty$. So far, we have only deduced weak ergodicity (which indicates the limit $\prod_{i=t_0}^{\infty} \mathbf{Y}(i)$ is the same regardless of initial time $t_0$) of the infinite product $\prod_{i=t_0}^{\infty} \mathbf{Y}(i)$. However, Theorem A in \cite{leizarowitz1992infinite} stated that weak ergodicity is equivalent to strong ergodicity (which indicates the matrices are uniformly mixing and all trajectories converge to the same stationary distribution) in the case of backward products. Since the product of any arbitrary permutations\footnote{The conclusion of Lemma \ref{geometlemma} still holds for any arbitrary order of multiplication due to strong ergodicity.} of $\{\bY(t)\}_t$ contains a non-zero column, by Lemma \ref{geometlemma} and \ref{lemmascramble}, we get that the infinite product $\prod_{i=t_0}^{\infty} \mathbf{Y}(i)$ is a scrambling matrix and hence converges.

Suppose the rows of this infinite product $\prod_{i=t_0}^{\infty} \mathbf{Y}(i)$ matrix in the limit are given by the vector $\bc$ and thus $\bPhi(t,t_0) \to \bC $ as $t \to \infty$ where the rows of matrix $\bC$ are identical and equal to transpose of $\bc$. This along the fact that $\mathbf{v}(t)=\left(\Pi_{i=1}^{t} \mathbf{Y}(i)\right) \mathbf{v}(t-1)$ together imply that the nodes achieve consensus to some vector $\balpha = \bC \bv(0)$ with $\balpha= \alpha\mathbf{1}$, i.e.,
$$
\lim _{t \rightarrow \infty} \mathbf{v}(t)=\lim _{t \rightarrow \infty}\left(\Pi_{i=1}^{t} \mathbf{Y}(i)\right) \mathbf{v}(0)=\balpha.
$$
 Finally, using the property of ergodicity provided in \cite{leizarowitz1992infinite} we have that $\delta(\bPhi(t,t_0))= \delta(\bPhi(t,t_0) - \bC)$, which gives the following rate:
\begin{align}
     | [\bPhi(t,t_0)]_{ji} - [\bc]_i | \leq \delta(\bPhi(t,t_0) - \bC) \leq ( 1-\beta^{\tau M} )^{\left\lfloor\frac{t-t_0}{\tau M}\right\rfloor}.
\end{align}
This completes the proof.
\end{proof}


\section{Weight Assignment for the Mixing Matrix}\label{weight assign appendix}
In this appendix, we will provide a choice of the weight assignment used in the analysis of the RESIST algorithm along with an associated example to showcase that our screening method will guarantee that the update only involves the information that is not being compromised.
\subsection{Proof of Lemma \ref{weight assign lemma}}\label{weightmatrixYlemmaprove}
\begin{proof}
Let us define the notation $b_j^*(t):= | \cN_j^b(t) |$ as the actual (unknown) number of nodes in the graph that have compromised outgoing edges to node $j$. Then we must have that $ b_j^*(t) \leq b$ for all $t$ and $J$.
To make the rest of the expressions clearer, we drop the iteration index $t$ for the remainder of this discussion wherever necessary, even though the variables are still $t$-dependent. We will, however, occasionally use $k$-dependency where the variables are $k$-th coordinate dependent.
 Next, suppose $b_j^k$ is the number of nodes, with compromised edges to $j$, remaining in the filtered set  $\cC_j^k$, and $q_j^k := b - b_j^* + b_j^k$. Since by definition $b - b_j^* \geq 0$ and $b_j^k \geq 0$, notice that only one of two cases can happen during each iteration for every coordinate $k$: ($i$) $q_j^k > 0$ or ($ii$) $q_j^k = 0$. For case ($i$), we either have $b - b_j^* > 0$ or $b_j^k > 0$ or both. These conditions correspond to the scenario where the node $j$ filters out at least one node from its neighborhood that has uncompromised edges to $j$. Thus, we know that $\overline{\cN}_j^{k}\cap\cN_j^r\neq\emptyset$. Likewise, it follows that $\underline{\cN}_j^{k}\cap\cN_j^r\neq\emptyset$. Then $\exists m_j'\in \overline{\cN}_j^{k}\cap\cN_j^r$ and $m_j''\in \underline{\cN}_j^{k}\cap\cN_j^r$ satisfying $[\bw_{m_j'}]_k\leq[\bw_i]_k\leq[\bw_{m_j''}]_k$ for any $i\in \cC_j^k$. Thus, for every $i\in \cC_j^k\cap\cN_j^b$, $\exists \theta_i^k\in (0,1)$ satisfying $[\bw_i]_k=\theta_i^k [\bw_{m_j'}]_k+(1-\theta_i^k)[\bw_{m_j''}]_k$. Consequently, the elements of the matrix $\bY_k$ can be then written as \eqref{elements in M}.

For case ($ii$), we must have that $b-b_j^* = 0$ and $b_j^k = 0$. Thus, all the filtered nodes in $\cC_j^k$ would be nodes with uncompromised edges to $j$ in this case. Therefore, we can describe $\bY_k$ in this case as \eqref{elements in MM}. 
Combining the expressions of $\bY_k$ in the two cases above allows us to express the update in \eqref{eqn: nonfaulty.update} exclusively in terms of uncompromised information. 
\end{proof}


\subsection{Example illustrating the weight assignment}\label{example of weight assignment}
\begin{figure}[h]
    \centering
    \includegraphics[height=6cm]{images/example.png}
    \caption{Weight assignment example for two-dimensional values for arbitrary iteration $t$.}
    \label{fig:example}
\end{figure}
Assuming the network is as shown with its connectivity in Figure \ref{fig:example} in which nodes are transmitting two-dimensional model parameters. Assume the network can only defend one compromised link within each neighborhood at a given time ($b=1$). The first dimension of the transmitted model are denoted as $[\bw_i(t)]_1$ for $i \in \{A, B, C, D, E\}$ which are the values marked above the transmission links, while the second dimension of the transmitted model are denoted as $[\bw_i(t)]_2$ for $i \in \{A, B, C, D, E\}$ which are the values marked under the transmission links. In this example, the link between node A and B and the link between node C and E denote the compromised links in the network, while all the links between other nodes are not compromised. The value on the left of the comma represents the value transmitted to the left and vice versa. For simplicity, we will omit the notation of the iteration index $t$ in this example. In the first dimension, if we first focus on the weight assignment for node A, it has four incoming links with one compromised link in its neighbors; thus $\vert\cN_A\vert-2b+1=3$ with $\cN_A^r=\{C, D, E\}$ and $b_A^*= 1$. After the screening, $\overline{\cN}_A^{1}=\{D\}$, $\underline{\cN}_A^{1}=\{E\}$ and $\cC_A^1=\{B,C\}$. Values from nodes B and C remain in the center set of node A and thus satisfy the first case with $b-b_A^* = 0$ and $q_A^1 = b_A^1 = 1$. Then, we have $[\bY_1]_{AA}=1/3$, $[\bY_1]_{AC}=1/6$ by \eqref{elements in M}. Even though the value from node B remains in the center set, its value three can be viewed as a convex combination of the values from the value of node D (in the lower set) and the value of node E (in the upper set) as $3=1/3\times5+2/3\times2$. As a consequence, the remaining weight assignment of node A will be $[\bY_1]_{AD}=1/3\times2/3+1/2\times1/3\times2/3=1/3$ and $[\bY_1]_{AE}=1/3\times1/3+1/2\times1/3\times1/3=1/6$ by \eqref{elements in M}. For node B in the first dimension: it has three incoming links with one compromised link in its neighbors; thus $\vert\cN_B\vert-2b+1=2$ with $\cN_B^r=\{C, D\}$ and $b_B^*= 1$. After the screening, $\overline{\cN}_B^{1}=\{A\}$, $\underline{\cN}_B^{1}=\{D\}$ and $\cC_B^1=\{C\}$. Only the value from node C remains in the center set of node B and thus satisfies the second case with $b-b_B^* = 0$ and $q_B^1 = b_B^1 = 0$. Thus the weight assignment of node $B$ is $[\bY_1]_{BB}= [\bY_1]_{BC}= 1/2$ by \eqref{elements in MM}. Other nodes in the network will perform the screening and the weight assignment similarly to node A and node B and thus will be omitted here. In the second dimension, the weight assignments can be done similarly as in the first dimension with different weight assignment based on the values each node receives. Note that even this example only contains two-dimensional information. This screening and weight assignments can be easily generalized to high-dimensional information by treating each dimension separately. 

%if first we still focus on node A, $\vert\cN_A\vert-2b+1=3$ remains the same with $\cN_A^r=\{C, D, E\}$ and $b_A^*= 1$ as they are dimension-independent quantities. After the screening, $\overline{\cN}_A^{2}=\{B/D\}$, $\underline{\cN}_A^{2}=\{E\}$ and $\cC_A^2=\{D/B,C\}$. If node B was selected in the lower set $\overline{\cN}_A^{2}$ then values from node D and C remain in the center set of node A and thus satisfy the second case with $b-b_A^* = 0$ and $q_A^2 = b_A^2 = 0$ and the corresponding weight assignment; If node D was selected in the lower set $\overline{\cN}_A^{2}$ then values from node B and C remains in the center set of node A and thus satisfies the second case with $b-b_B^* = 0$ and $q_B^2 = b_B^2 = 1$. However, node B and node D provide the same value on the second dimension to node A in this iteration, so the contribution of node B can be viewed as the contribution of node D. Thus, the impact of the compromised link AB does not make any impact. For node E in the second dimension, $\vert\cN_E\vert-2b+1=3$ remains the same with $\cN_E^r=\{A, D\}$ and $b_E^*= 1$ as they are dimension-independent quantities. After the screening, $\overline{\cN}_E^{2}=\{D\}$, $\underline{\cN}_E^{2}=\{A\}$ and $\cC_E^2=\{C\}$. Only the value from node C remains in the center set of node B and thus satisfies the first case with $b-b_E^* = 0$ and $q_E^2 = b_E^2 = 1$, and 

\section{Proofs of Supporting Lemmas used to Derive Consensus Guarantee}\label{appendixA}
\subsection{Proof of Lemma \ref{wkbarlemma}} \label{wkbarlemmaproof}

    Applying the $ \overline{(\cdot)} $ operator to both sides of \eqref{scr1} we get the following update:
 \begin{align}
[\overline{\bW}(s+1)]_k &=  \frac{\mathbf{1}\mathbf{1}^T}{M} \bQ_{k}( s)[{\bW}(s)]_k - h [\overline{\bT}(s)]_k.  \label{pf1z}
\end{align}
Next, subtracting \eqref{pf1z} from \eqref{scr1} we obtain:
\begin{align}
       [\overline{\bW}(s+1)]_k - [{\bW}(s+1)]_k &= ( \frac{\mathbf{1}\mathbf{1}^T}{M} - \bI)\bQ_{k}(s)[{\bW}(s)]_k  - h ([\overline{\bT}(s)]_k - [{\bT}(s)]_k) \\
       & = ( \frac{\mathbf{1}\mathbf{1}^T}{M} - \bI)\bQ_{k}( s)([{\bW}(s)]_k-  [\overline{\bW}(s)]_k)  - h ([\overline{\bT}(s)]_k - [{\bT}(s)]_k) \\
        &= ( \frac{\mathbf{1}\mathbf{1}^T}{M} - \bI)(\bQ_{k}( s) - \mathbf{1}\bc_k(s)^T)([{\bW}(s)]_k -  [\overline{\bW}(s)]_k) \nonumber \\ & \hspace{6cm} - h ([\overline{\bT}(s)]_k - [{\bT}(s)]_k), \label{tempfcd}
\end{align}
where in the second step we used the fact that the vector $ [\overline{\bW}(s)]_k$ has identical entries and hence lies in the null space of $(\frac{\mathbf{1}\mathbf{1}^T}{M} - \bI)\bQ_k(s) $ and in the last step we used the fact that the vector $ \mathbf{1}\bc_k(s)^T([{\bW}(s)]_k-  [\overline{\bW}(s)]_k)$ has identical entries and hence lies in the null space of $\frac{\mathbf{1}\mathbf{1}^T}{M} - \bI $. Taking norm on both sides of \eqref{tempfcd}, using the property $\norm{\bA} \leq \sqrt{M} \norm{\bA}_{\infty}$ for any $\bA \in \bR^{M \times M}$ and Corollary~\ref{coro1} then yields:
\begin{align}
    \norm{[\overline{\bW}(s+1)]_k - [{\bW}(s+1)]_k}  & \leq  \norm{\frac{\mathbf{1}\mathbf{1}^T}{M} - \bI} \norm{\bQ_{k}( s) - \mathbf{1}\bc_k(s)^T}\norm{[{\bW}(s)]_k-  [\overline{\bW}(s)]_k}   + h \norm{[\overline{\bT}(s)]_k - [{\bT}(s)]_k} \\ & \leq  M^{\frac{1}{2}}\norm{\bQ_{k}( s) - \mathbf{1}\bc_k(s)^T}_{\infty}\norm{[{\bW}(s)]_k-  [\overline{\bW}(s)]_k}   + h \norm{[\overline{\bT}(s)]_k - [{\bT}(s)]_k} \\
& \leq  M^{\frac{3}{2}}( 1-\beta^{\tau M} )^{\left\lfloor\frac{(J-2)}{\tau M}\right\rfloor}\norm{[{\bW}(s)]_k-  [\overline{\bW}(s)]_k}   + h \norm{[\overline{\bT}(s)]_k - [{\bT}(s)]_k},
\end{align}
which completes the proof.
\qed
 

\subsection{Proof of Lemma \ref{lemxi1}}\label{lemmaxi1proof}


    We first apply the $ \widehat{(\cdot)}^{k,s+1} $ operator to both sides of \eqref{scr1} to get the following update:
\begin{align}
       [\widehat{\bW}^{k,s+1}(s+1)]_k &=  \bQ^{\pi}_k(s+1) \bQ_{k}( s)[{\bW}(s)]_k - h [\widehat{\bT}^{k,s+1}(s)]_k . \label{pf1a}
\end{align}
Subtracting \eqref{scr1} from \eqref{pf1a} yields:
\begin{align}
     [\widehat{\bW}^{k,s+1}(s+1)]_k -   {[{\bW}(s+1)]_k} & =  (\bQ^{\pi}_k(s+1) \bQ_{k}( s) - \bQ_{k}( s))[{\bW}(s)]_k  - h ([\widehat{\bT}^{k,s+1}(s)]_k - { [{\bT}(s)]_k}) \\
      & =   (\bQ^{\pi}_k(s+1)  - \bI) (\bQ_{k}( s) - \mathbf{1}\bc_k(s)^T)[{\bW}(s)]_k   - h ([\widehat{\bT}^{k,s+1}(s)]_k - { [{\bT}(s)]_k}) \\
     & =   (\bQ^{\pi}_k(s+1)  - \bI)(\bQ_{k}( s) - \mathbf{1}\bc_k(s)^T) ([{\bW}(s)]_k - [\widehat{\bW}^{k,s}(s)]_k  )\nonumber \\ &\hspace{5cm}+ h (\bQ^{\pi}_k(s+1)  - \bI)([\widehat{\bT}^{k,s}(s)]_k - { [{\bT}(s)]_k}), \label{pf1c}
\end{align}
where in the second last step, we introduced the vector $\bc_k(s)$ from Corollary \ref{coro1} and used the fact that the matrix $\mathbf{1}\bc_k(s)^T $ lies in the null space of $(\bQ^{\pi}_k(s+1)  - \bI) $. In the last step, we used the facts that the vector $[\widehat{\bW}^{k,s}(s)]_k  = \bQ_k^{\pi}(s) {[{\bW}(s)]_k} $ has all identical entries since $\bQ_k^{\pi}(s)$ has identical rows, $\bQ_{k}( s) $ is row stochastic and thus $ \bQ_{k}( s) [\widehat{\bW}^{k,s}(s)]_k = [\widehat{\bW}^{k,s}(s)]_k $, which has identical entries, and finally the vector $[\widehat{\bW}^{k,s}(s)]_k $ lies in the null space of $ (\bQ^{\pi}_k(s+1)  - \bI)$ and $ (\bQ_{k}( s) - \mathbf{1}\bc_k(s)^T) $. Along similar lines we also have that $([\widehat{\bT}^{k,s+1}(s)]_k - { [{\bT}(s)]_k}) = -(\bQ^{\pi}_k(s+1)  - \bI)([\widehat{\bT}^{k,s}(s)]_k - { [{\bT}(s)]_k}) $.

Finally, taking operator norm on both sides of \eqref{pf1c}, using Cauchy-Schwarz inequality, the bound $ \norm{\bQ^{\pi}_k(s)} = \norm{\mathbf{1}\bc_k(s)^T} \leq \sqrt{M} $ for any $s$, $\norm{\bA} \leq \sqrt{M} \norm{\bA}_{\infty}$ for any $\bA \in \bR^{M \times M}$ and Corollary~\ref{coro1} yields:
\begin{align}
    \norm{[\widehat{\bW}^{k,s+1}(s+1)]_k -{[{\bW}(s+1)]_k}} \leq & \norm{\bQ^{\pi}_k(s+1)  - \bI} \norm{\bQ_{k}( s) - \mathbf{1}\bc_k(s)^T} \norm{ [\widehat{\bW}^{k,s}(s)]_k - [{\bW}(s)]_k  }  \nonumber \\ & \hspace{1cm}+  h \norm{\bQ^{\pi}_k(s+1) - \bI} \norm{[\widehat{\bT}^{k,s}(s)]_k - { [{\bT}(s)]_k}} \\
   \leq  &   \sqrt{M}(\sqrt{M}+1) \norm{\bQ_{k}( s) - \mathbf{1}\bc_k(s)^T}_{\infty} \norm{ [\widehat{\bW}^{k,s}(s)]_k - [{\bW}(s)]_k  }  \nonumber \\ & \hspace{1cm}  + h(\sqrt{M}+1)\norm{[\widehat{\bT}^{k,s}(s)]_k - { [{\bT}(s)]_k}} \\
   \leq  &  M^{\frac{3}{2}}(\sqrt{M}+1)( 1-\beta^{\tau M} )^{\left\lfloor\frac{(J-2)}{\tau M}\right\rfloor} \norm{ [\widehat{\bW}^{k,s}(s)]_k - [{\bW}(s)]_k  } \nonumber \\ & \hspace{1cm} + h(\sqrt{M}+1)\norm{[\widehat{\bT}^{k,s}(s)]_k - { [{\bT}(s)]_k}}.
\end{align}
This completes the proof. 
\qed
\begin{rema}\label{remark_timevar1}
    Note that in the steps leading up to \eqref{pf1c} in the proof of Lemma \ref{lemxi1} we cannot simply use the technique of one step contraction from Lemma 1 in \cite{xin2018linear} because of the fact that our $\bQ_k(s)$ is time varying. Now, even though the spectral radius of the matrix $ \bQ_k(s) - \mathbf{1}(\bc_k(s))^T$ is strictly less than $1$ given when $\bQ_k(s)$ is irreducible, its operator norm may not be less than $1$. Also, no two matrices from the sequence $ \{\bQ_k(s) - \mathbf{1}(\bc_k(s))^T\}_s$ may be simultaneously diagonalizable with the same eigenvectors, and hence we cannot simply apply some $s$-independent matrix norm on both sides of \eqref{pf1c} so as to replace the operator norm with spectral radius. However, the time-invariant mixing matrix in \cite{xin2018linear} makes it possible to apply a compatible matrix norm on both sides of their inequality, something which is not possible in our case.
\end{rema}

\subsection{Proof of Lemma \ref{tkhatlemma}}\label{tkhatlemmaproof}

Let $ \widetilde{\bW}^* \in \mathbb{R}^{M\times d} $ be a matrix whose $i^{th}$ row is $\bw_i^*$. Then, we get $ \nabla {F}(\widetilde{\bW}^*)  = \mathbf{0}$. Further define $ \widehat{\bW}^s(s):= \mathbf{1} (\widehat{\bw}^s(s))^T$. Using the definition of $ \widehat{\bw}^{s}(s)$ we also get:
 \begin{align}
  Lh \sqrt{d} \sum\limits_{j=1}^M \norm{ \widehat{\bw}^{s}(s) - \bw_j(s)}  & = Lh \sqrt{d} \sum\limits_{j=1}^M \sqrt{\sum\limits_{k=1}^d \bigg(  \sum\limits_{l=1}^M [\bc_k(s)]_{l} [\bw_l(s)]_k - [\bw_j(s)]_k\bigg)^2}  \label{e2_ineq1} \\
    & \leq Lh \sqrt{d} \sum\limits_{j=1}^M {\sum\limits_{k=1}^d \bigg\lvert  \sum\limits_{l=1}^M [\bc_k(s)]_{l} [\bw_l(s)]_k - [\bw_j(s)]_k\bigg\rvert} \\
     & = Lh \sqrt{d}\sum\limits_{k=1}^d \sum\limits_{j=1}^M { \bigg\lvert  \sum\limits_{l=1}^M [\bc_k(s)]_{l} [\bw_l(s)]_k - [\bw_j(s)]_k\bigg\rvert} \\
      &\leq Lh \sqrt{Md}\sum\limits_{k=1}^d  \sqrt{ \sum\limits_{j=1}^M\bigg\lvert  \sum\limits_{l=1}^M [\bc_k(s)]_{l} [\bw_l(s)]_k - [\bw_j(s)]_k\bigg\rvert^2} \\
       & = Lh \sqrt{Md}\sum\limits_{k=1}^d \norm{[\widehat{\bW}^{k,s}(s)]_k - [{\bW}(s)]_k}. \label{e2_ineq2}
\end{align}
Then, as a consequence of\eqref{e2_ineq2} we get the following bound:
\begin{align}
    \sum\limits_{j=1}^M \norm{ \widehat{\bw}^{s}(s) - \bw_j(s)} & \leq \sqrt{M}\sum\limits_{k=1}^d \norm{[\widehat{\bW}^{k,s}(s)]_k - [{\bW}(s)]_k} \label{e2_ineq3}.
\end{align}
Taking norm of $ [\widehat{\bT}^{k,s}(s)]_k- [{\bT}(s)]_k$, using the fact that $\norm{\bQ_k^{\pi}}= \norm{\mathbf{1}\bc_k^T} \leq \sqrt{M}$ and simplifying using Assumption~\ref{asumpt1_nonconvex}, Jensen's inequality and \eqref{e2_ineq3} yield:
   \begin{align}
   \norm{[\widehat{\bT}^{k,s}(s)]_k- [{\bT}(s)]_k} & = \norm{[\nabla \widehat{F}^{k,s}(\bW(s))]_k - [\nabla {F}(\bW(s))]_k} \\
   & \leq \norm{\bQ_k^{\pi}(s) -\bI}\norm{ [\nabla {F}(\bW(s))]_k} \\
  & \leq  (\sqrt{M} + 1)\bigg(\norm{ \nabla {F}(\bW(s))- \nabla {F}(\widehat{\bW}^s(s)) }_F +  \norm{ \nabla {F}(\widehat{\bW}^s(s))- \nabla {F}(\widetilde{\bW}^*) }_F \bigg) \\
    &  \leq  (\sqrt{M} + 1)L\bigg(\sqrt{\sum\limits_{i=1}^M \norm{ \bw_i(s)- \widehat{\bw}^s(s) }^2} +  \sqrt{\sum\limits_{i=1}^M \norm{ \bw_i^*- \widehat{\bw}^s(s) }^2} \bigg)  \\
     &\leq  (\sqrt{M} + 1)L\bigg({\sum\limits_{i=1}^M \norm{ \bw_i(s)- \widehat{\bw}^s(s) }} +  {\sum\limits_{i=1}^M \norm{ \bw_i^*- \widehat{\bw}^s(s) }} \bigg)  \\
 &  \leq  (\sqrt{M} + 1)L\sqrt{M}{\sum\limits_{k=1}^d \norm{ [{\bW}(s)]_k- [\widehat{\bW}^{k,s}(s)]_k}} \nonumber \\
 & \hspace{3cm}+  (\sqrt{M} + 1)L\sum\limits_{i=1}^M\bigg(  \norm{ \bw^*- \widehat{\bw}^s(s) } +\norm{ \bw^*- \bw_i^* } \bigg) \\
 & =  (\sqrt{M} + 1)L\sqrt{M}{\sum\limits_{k=1}^d \norm{ [{\bW}(s)]_k- [\widehat{\bW}^{k,s}(s)]_k }} + (\sqrt{M} + 1)LM \norm{ \bw^*-\widehat{\bw}^s(s) }  \nonumber \\
 & \hspace{3cm}+ (\sqrt{M} + 1)L\sum\limits_{i=1}^M \norm{ \bw^*- \bw_i^* } .
    \end{align}
Similarly we get that:
     \begin{align}
   \norm{[\overline{\bT}(s)]_k- [{\bT}(s)]_k} & = \norm{[\nabla \overline{F}(\bW(s))]_k - [\nabla {F}(\bW(s))]_k} \leq \underbrace{\norm{\frac{\mathbf{1}\mathbf{1}^T}{M}-\bI}}_{\leq 1}\norm{ [\nabla {F}(\bW(s))]_k} \\
  &  \leq  L\sqrt{M}{\sum\limits_{k=1}^d \norm{ [{\bW}(s)]_k- [\widehat{\bW}^{k,s}(s)]_k }} + LM \norm{ \bw^*-\widehat{\bw}^s(s) } + L\sum\limits_{i=1}^M \norm{ \bw^*- \bw_i^* } ,
    \end{align}
    which completes the proof.
\qed


\section{The RESIST Algorithm as an Inexact Gradient Descent Update}\label{appendixB}
\subsection{Proof of Lemma \ref{supportlem_inexactrule_007}} \label{supportlemproof}


     For $f^{k,s}(\cdot) := \sum\limits_{i=1}^M [\bc_k(s)]_i f_i(\cdot)$, where $ \bc_k(s)$ is defined in Corollary \ref{coro1} and $ 0 \leq [\bc_k(s)]_i \leq 1$ for all $i$ with $\sum\limits_{i=1}^M [\bc_k(s)]_i  = 1$, we get that $ f^{k,s}$ is $L$-gradient Lipschitz for any $k, s$ by Assumption \ref{asumpt1_nonconvex}.
Then, the local vector update at time $s+1$ defined as $\bw_i(s+1)$ for any node $i$ can be written as:
\begin{align}
    \begin{bmatrix}
        [\bw_i(s+1)]_1 \\
        [\bw_i(s+1)]_2 \\
        \vdots \\
        \vdots\\
        \vdots  \\
        [\bw_i(s+1)]_k \\
        \vdots \\
        \vdots \\
        [\bw_i(s+1)]_d 
    \end{bmatrix} =     \begin{bmatrix}
       \sum\limits_{j=1}^M [\bQ_1(s)]_{ij} [\bw_j(s)]_1 \\
       \sum\limits_{j=1}^M [\bQ_2(s)]_{ij} [\bw_j(s)]_2 \\
        \vdots \\
       \sum\limits_{j=1}^M [\bQ_k(s)]_{ij} [\bw_j(s)]_k \\
        \vdots \\
        \sum\limits_{j=1}^M [\bQ_d(s)]_{ij}[\bw_j(s)]_d 
    \end{bmatrix}  -   h\begin{bmatrix}
        \nabla_1 f_i(\bw_i(s))  \\
        \nabla_2 f_i(\bw_i(s)) \\
        \vdots \\
        \vdots \\
        \vdots  \\
        \nabla_k f_i(\bw_i(s))  \\
        \vdots \\
        \vdots\\
       \nabla_d f_i(\bw_i(s)) 
    \end{bmatrix} .
\end{align}
Applying $ \widehat{(\cdot)}^{k,s+1}$ operator or equivalently multiplying $[\bc_k(s+1)]$ to both sides of the above equality to average the entries in dimension $k$ and at time $s+1$, we get the following expression, which is independent of $i$:
\begin{align}
   \underbrace{ \begin{bmatrix}
        \sum\limits_{j=1}^M [\bc_1(s+1)]_{j}[\bw_j(s+1)]_1 \\
       \sum\limits_{j=1}^M [\bc_2(s+1)]_{j} [\bw_j(s+1)]_2 \\
        \vdots \\
       \sum\limits_{j=1}^M [\bc_k(s+1)]_{j} [\bw_j(s+1)]_k \\
        \vdots \\
       \sum\limits_{j=1}^M [\bc_d(s+1)]_{j} [\bw_j(s+1)]_d 
    \end{bmatrix}}_{\widehat{\bw}^{s+1}(s+1)} &=    \underbrace{ \begin{bmatrix}
        \sum\limits_{j=1}^M [\bc_1(s)]_{j} [\bw_j(s)]_1 \\
         \sum\limits_{j=1}^M [\bc_2(s)]_{j}[\bw_j(s)]_2 \\
        \vdots \\
       \sum\limits_{j=1}^M [\bc_k(s)]_{j}[\bw_j(s)]_k \\
        \vdots \\
         \sum\limits_{j=1}^M [\bc_d(s)]_{j} [\bw_j(s)]_d 
    \end{bmatrix}}_{\widehat{\bw}^{s}(s)}  -   h\begin{bmatrix}
        \sum\limits_{j=1}^M [\bc_1(s+1)]_{j} \nabla_1 f_j(\bw_j(s)) \\
        \sum\limits_{j=1}^M [\bc_2(s+1)]_{j} \nabla_2 f_j(\bw_j(s)) \\
        \vdots \\
       \sum\limits_{j=1}^M [\bc_k(s+1)]_{j} \nabla_k f_j(\bw_j(s))  \\
        \vdots \\
         \sum\limits_{j=1}^M [\bc_d(s+1)]_{j} \nabla_d f_j(\bw_j(s))
    \end{bmatrix} 
    \end{align}
    \begin{align}
    & \hspace{-0cm} =     \begin{bmatrix}
        \sum\limits_{j=1}^M [\bc_1(s)]_{j} [\bw_j(s)]_1 \\
         \sum\limits_{j=1}^M [\bc_2(s)]_{j}[\bw_j(s)]_2 \\
        \vdots \\
       \sum\limits_{j=1}^M [\bc_k(s)]_{j}[\bw_j(s)]_k \\
        \vdots \\
         \sum\limits_{j=1}^M [\bc_d(s)]_{j} [\bw_j(s)]_d 
    \end{bmatrix} -  h \begin{bmatrix}
        \nabla_1 f(\widehat{\bw}^{s}(s)) \\
        \nabla_2 f(\widehat{\bw}^{s}(s))\\
        \vdots \\
        \vdots \\
        \vdots \\
        \nabla_k f(\widehat{\bw}^{s}(s))  \\
        \vdots \\
        \vdots \\
        \nabla_d f(\widehat{\bw}^{s}(s))
    \end{bmatrix} + \underbrace{ h \begin{pmatrix}
        \begin{bmatrix}
        \nabla_1 f(\widehat{\bw}^{s}(s)) \\
        \nabla_2 f(\widehat{\bw}^{s}(s)) \\
        \vdots \\
        \vdots \\
        \vdots \\
        \nabla_k f(\widehat{\bw}^{s}(s))  \\
        \vdots \\
        \vdots \\
        \nabla_d f(\widehat{\bw}^{s}(s))
    \end{bmatrix} -   \begin{bmatrix}
        \nabla_1 f^{1,s+1}(\widehat{\bw}^{s}(s)) \\
        \nabla_2 f^{2,s+1}(\widehat{\bw}^{s}(s)) \\
        \vdots \\
        \vdots \\
        \vdots \\
        \nabla_k f^{k,s+1}(\widehat{\bw}^{s}(s))  \\
        \vdots \\
        \vdots \\
        \nabla_d f^{d,s+1}(\widehat{\bw}^{s}(s))
    \end{bmatrix} \end{pmatrix}}_{=\be_1(s)} \nonumber \\    
    & \underbrace{\hspace{2cm} +h \begin{pmatrix}
     \begin{bmatrix}
          \sum\limits_{j=1}^M [\bc_1(s+1)]_{j} \nabla_1 f_j(\widehat{\bw}^{s}(s))  \\
           \sum\limits_{j=1}^M [\bc_2(s+1)]_{j} \nabla_2 f_j(\widehat{\bw}^{s}(s))  \\
        \vdots \\
        \vdots \\
           \sum\limits_{j=1}^M [\bc_k(s+1)]_{j} \nabla_k f_j(\widehat{\bw}^{s}(s))  \\
        \vdots \\
        \vdots \\
          \sum\limits_{j=1}^M [\bc_d(s+1)]_{j} \nabla_d f_j(\widehat{\bw}^{s}(s)) 
    \end{bmatrix}  -  \begin{bmatrix}
       \sum\limits_{j=1}^M [\bc_1(s+1)]_{j} \nabla_1 f_j(\bw_j(s)) \\
          \sum\limits_{j=1}^M [\bc_2(s+1)]_{j} \nabla_2 f_j(\bw_j(s)) \\
        \vdots \\
        \vdots \\
          \sum\limits_{j=1}^M [\bc_k(s+1)]_{j} \nabla_k f_j(\bw_j(s))  \\
        \vdots \\
        \vdots \\
           \sum\limits_{j=1}^M [\bc_d(s+1)]_{j} \nabla_d f_j(\bw_j(s))
    \end{bmatrix}         
    \end{pmatrix}}_{=\be_2(s)}. \label{stackvec1}
\end{align}   
On the other hand, in order to see how our algorithm update \eqref{scr1} is equivalent to the the inexact gradient descent update with error terms which is in the form of the above equation, we apply $ \widehat{(\cdot)}^{k,s+1}$ operator to \eqref{scr1}, substituting $ [{\bT}(s)]_k = [\nabla {F} (\bW(s))]_k $ and using Corollary \ref{coro1} we get:
\begin{align}
    [\widehat{\bW}^{k,s+1}(s+1)]_k &=  \bQ^{\pi}_{k}( s+1) \bQ_{k}( s)[{\bW}(s)]_k - h [\nabla \widehat{F}^{k,s+1} (\bW(s))]_k \\
    &=  \bQ^{\pi}_{k}( s) [{\bW}(s)]_k - h [\nabla \widehat{F}^{k,s+1} (\bW(s))]_k \\
     &=  {[\widehat{\bW}^{k,s}(s)]_k - h [\nabla \widehat{F}^{k,s+1} (\widehat{\bW}^{k,s}(s))]_k} + h([\nabla \widehat{F}^{k,s+1} (\widehat{\bW}^{k,s}(s))]_k -   [\nabla \widehat{F}^{k,s+1} (\bW(s))]_k) \\
    &=  {[\widehat{\bW}^{k,s}(s)]_k - h [\nabla \overline{F} (\widehat{\bW}^{k,s}(s))]_k} + h([\nabla \overline{F} (\widehat{\bW}^{k,s}(s))]_k -[\nabla \widehat{F}^{k,s+1} (\widehat{\bW}^{k,s}(s))]_k ) \nonumber \\ &\hspace{5cm} + h([\nabla \widehat{F}^{k,s+1} (\widehat{\bW}^{k,s}(s))]_k -   [\nabla \widehat{F}^{k,s+1} (\bW(s))]_k) . \label{convotemp2a}
\end{align}
 Observe that the $k$-th row in the vector equation \eqref{stackvec1} corresponds to the update \eqref{convotemp2a}. Also, notice that the update \eqref{convotemp2a} is in principle a scalar update due to the fact that all the $d$ entries of any given vector on either side of \eqref{convotemp2a} are identical. Then, stacking scalar updates of \eqref{convotemp2a} from $k=1$ to $d$ and representing the stacked vectors $[\widehat{\bW}^{k,s+1}(s+1)]_k$ and $[\widehat{\bW}^{k,s}(s))]_k$ as $\widehat{\bw}^{s+1}(s+1) $ and $\widehat{\bw}^{s}(s)$, respectively, yield the exact vector update as \eqref{stackvec1}. 

Thus, from \eqref{stackvec1} we get the following inexact gradient descent update:
\begin{align}
    \widehat{\bw}^{s+1}(s+1) &= \widehat{\bw}^{s}(s) - h \nabla f (\widehat{\bw}^{s}(s)) + \be_1(s) + \be_2(s) . \label{stackvec2}
\end{align}

Next, using $L$-gradient Lipschitz continuity of $\nabla_k f_j$ for any $k,j$ from Assumption \ref{asumpt1_nonconvex}, the fact that $0 \leq [\bc_k(s)]_j \leq 1$ and a simple application of triangle inequality, we get the following bound on $\be_2(s)$ :
\begin{align}
    \norm{\be_2(s)} &\leq Lh\sqrt{\sum\limits_{k=1}^d \bigg(\sum\limits_{j=1}^M \norm{ \widehat{\bw}^{s}(s) - \bw_j(s)}\bigg)^2} \\
    & = Lh \sqrt{d} \sum\limits_{j=1}^M \norm{ \widehat{\bw}^{s}(s) - \bw_j(s)}. \label{e2boundtemp_*}
    \end{align}
Then using the bounds \eqref{e2_ineq1}-\eqref{e2_ineq2} along with \eqref{e2boundtemp_*}, we get:
\begin{align}
     \norm{\be_2(s)} &\leq Lh \sqrt{Md}\sum\limits_{k=1}^d \norm{[\widehat{\bW}^{k,s}(s)]_k - [{\bW}(s)]_k}. \label{e2boundtemp_*0}
\end{align}
This completes the proof.
\qed








\section{Proof of Geometric Convergence Rate of the RESIST Algorithm Under Strong Convexity}\label{appendixC}
\subsection{On the non-vacuous nature of Assumption \ref{boundedassump}}\label{boundedexistencesec_0}
Suppose the model dimension is $1$, i.e., $f_i :\mathbb{R} \to \mathbb{R}$, Assumptions \ref{claim2}, \ref{asumpt1_nonconvex} hold and that $f_i$ is coercive for all $i$, i.e., $ \lim_{\norm{\bw} \to \infty} f_i(\bw) = \infty$. Further, the graph induced by the network topology is symmetric and strongly connected with no bottlenecks such as a $K$-regular graph with $K= 4b$.
Also, assume the Man-in-the-middle attack is such that the mixing matrix $\bY(t)$ is symmetric, simultaneously diagonalizable for all $t$ and the sequence of those simultaneously diagonalizable matrices $\{\bQ(s)\}_{s=0}^{\infty}$ are
\begin{align}
    \bQ(s) =  \prod\limits_{r= J \lfloor \frac{t}{J} \rfloor }^{J \lfloor \frac{t}{J} \rfloor + J -2} \bY(r),  
\end{align}
where the $\bQ(s)$ matrix is defined from \eqref{cwtm1} after omitting the subscript $k$ and also satisfy\footnote{Here, the inequality $\bA\preccurlyeq \bB$ implies $\bB-\bA$ is positive semi-definite.}
\begin{align}
     \bQ(0) \preccurlyeq \bQ(1) \preccurlyeq \cdots \preccurlyeq  \bQ(s) \preccurlyeq \cdots. \label{liplyapunovc0}
\end{align}
The simultaneous diagonalizable matrices condition will be satisfied by an attack that only changes the graph spectrum (eigenvalues of $\bY(t)$) over time. The condition \eqref{liplyapunovc0} can be satisfied by an attack that progressively decreases the information mixing rate in the network by increasing the eigenvalues of the mixing matrices.  

Next, along similar lines as in (Lemma 3, \cite{zeng2018nonconvex}), for $\bW = [\bw_1, \cdots, \bw_M]^T$ and $F(\bW) = \sum_{i=1}^M f_i(\bw_i)$ we define a Lyapunov function $ \mathcal{L}(\cdot; s): \mathbb{R}^M \to \mathbb{R}$ as follows:
\begin{align}
    \mathcal{L}(\bW; s) := F(\bW) + \frac{1}{2h}\norm{\bW}^2_{\bI - \bQ(s)}
\end{align}
where\footnote{Note that $ \norm{\cdot}_{\bI - \bQ(s)}$ is a semi-norm since $(\bI - \bQ(s)) \frac{\mathbf{1}\mathbf{1}^T}{M}\bW = \mathbf{0}$ for any $ \bW \in \mathbb{R}^M$.} $ \norm{\bW}^2_{\bI - \bQ(s)} = \langle \bW, (\bI - \bQ(s))\bW \rangle$. Note that $\mathcal{L}(\bW; s)$ is a Lyapunov function since $F(\cdot) $ is lower bounded and $\bI - \bQ(s) $ is positive semi-definite due to symmetric mixing matrix $\bQ(s)$. Then, the $s$-time scale update for RESIST can be expressed in terms of the Lyapunov function as follows:
 \begin{align}
     \bW(s+1)= \bW(s) - h \nabla \mathcal{L}(\bW(s); s) \label{liplyapunovc*}
 \end{align}
 \footnote{Here $\nabla$ is with respect to $ \bW(s)$.}due to symmetric $\bQ(s)$. Further, the Lyapuov function $\mathcal{L}(\cdot; s) $ is uniformly gradient Lipschitz continuous over all $s \geq 0$ where
 \begin{align}
     \LIP(\mathcal{L}) \leq L M + \sup_{s \geq 0}\frac{\norm{\bI -\bQ(s)}_2}{h} = L M + \frac{1- \inf_{s \geq 0} \sigma (\bQ(s))}{h}, \label{liplyapunovc1}
 \end{align}
  $\sigma (\bQ(s)) $ is the smallest eigenvalue of $\bQ(s)$ and the eigenvalues of $\bQ(s)$ lie in the interval $(0,1]$.

Next, if $h < \frac{ 1+\inf_{s \geq 0} \sigma (\bQ(s))}{LM}$ then from \eqref{liplyapunovc1} we have:
\begin{align}
    \LIP(\mathcal{L}) h   & \leq L M h + 1-  \inf_{s \geq 0} \sigma (\bQ(s)) < 2. \label{liplyapunovc2}
\end{align}
 Then by gradient Lipschitz continuity of $\mathcal{L}(\cdot; s)$ for $h < \frac{ 1+\inf_{s \geq 0} \sigma (\bQ(s))}{LM}$ and \eqref{liplyapunovc*}, \eqref{liplyapunovc2} we get:
\begin{align}
    \mathcal{L}(\bW(s+1); s) & \leq \mathcal{L}(\bW(s); s) + \langle \nabla \mathcal{L}(\bW(s); s),  \bW(s+1)- \bW(s) \rangle + \frac{ \LIP(\mathcal{L}) }{2}\norm{ \bW(s+1)- \bW(s) }^2 \\
    & = \mathcal{L}(\bW(s); s) - \frac{h}{2}{\bigg(2- \LIP(\mathcal{L}) h \bigg)} \norm{ \nabla \mathcal{L}(\bW(s); s)}^2 \\
    & \leq  \mathcal{L}(\bW(s); s). \label{liplyapunovc3}
\end{align}
From \eqref{liplyapunovc0} we get that $\norm{\bW(s+1)}^2_{\bI - \bQ(s+1)} \leq \norm{\bW(s+1)}^2_{\bI - \bQ(s)} $ and then using \eqref{liplyapunovc3} for $h < \frac{ 1+\inf_{s \geq 0} \sigma (\bQ(s))}{LM}$ we have that:
\begin{align}
    \mathcal{L}(\bW(s+1); s+1) & \leq  \mathcal{L}(\bW(s); s)  \hspace{0.2cm} \forall \hspace{0.1cm} s \geq 0. \label{liplyapunovc4}
\end{align}
Since $f_i$ is coercive, $ \mathcal{L}(\cdot; s)$ is coercive for all $s$ and hence $\mathcal{L}(\cdot; s)$ has bounded sublevel sets for all $s$. For an initialization $\bW(0)$ of RESIST let 
$$ S_{sub}(s) =\bigg\{ \bW \in \mathbb{R}^M: \mathcal{L}(\bW; s) \leq \mathcal{L}(\bW(0); 0)\bigg\}.$$
Then $S_{sub}(s)$ for any $s \geq 0$ is compact.
Also, from \eqref{liplyapunovc0} we get for any $\bW$ that $\norm{\bW}^2_{\bI - \bQ(s+1)} \leq \norm{\bW}^2_{\bI - \bQ(s)} $ for all $s \geq 0$ and thus for any $\bW$
\begin{align}
    \mathcal{L}(\bW; s+1)  \leq  \mathcal{L}(\bW; s)  \hspace{0.2cm} \forall \hspace{0.1cm} s \geq 0. \label{liplyapunovc5}
\end{align}
Using the inequality \eqref{liplyapunovc5} we have 
\begin{align}
S_{sub}(\infty)  \supseteq \cdots \supseteq S_{sub}(s+1)   \supseteq S_{sub}(s) \supseteq \cdots \supseteq S_{sub}(0) \label{liplyapunovc6},
\end{align}
with the convention that 
$$S_{sub}(\infty) =\bigg\{ \bW \in \mathbb{R}^M: \liminf_{s \to \infty}\mathcal{L}(\bW; s) \leq \mathcal{L}(\bW(0); 0)\bigg\}. $$
It is important to note that $ \liminf_{s \to \infty} \norm{\bW}^2_{\bI - \bQ(s)} \geq 0$ for any $\bW$ since $ \norm{\bW}^2_{\bI - \bQ(s)} \geq 0$ for all $s \geq 0$ and any $\bW$. Then $\liminf_{s \to \infty}\mathcal{L}(\bW; s) $ is coercive in $\bW$ with compact sub-level sets and hence $ S_{sub}(\infty)$ is compact.

Then for $h < \frac{ 1+\inf_{s \geq 0} \sigma (\bQ(s))}{LM}$, from \eqref{liplyapunovc4}, \eqref{liplyapunovc6} and compactness of $S_{sub}(\infty)$, we have that the sequence $\{\bW(s)\}_s$ stays bounded in compact $S_{sub}(\infty)$ for all $s$. This completes the example illustrating Assumption \ref{boundedassump}.
 

\subsection{Proof of Lemma \ref{convexsclem}}\label{convexsclemproof}




 Since $f := \frac{1}{M}\sum\limits_{i=1}^M f_i$ is $\mu$-strongly convex and $L$-gradient Lipschitz, we get that $f$ satisfies Lemma \ref{lemmconvexcoercive}. Then expanding $ \norm{\widehat{\bw}^{s}(s) - h \nabla f (\widehat{\bw}^{s}(s)) - \bw^*}^2$ and using \eqref{propsc} we have that:
    \begin{align}
        \norm{ \widehat{\bw}^{s}(s) - h \nabla f (\widehat{\bw}^{s}(s)) - (\bw^*- \nabla f(\bw^*))}^2 &=  \norm{\widehat{\bw}^{s}(s) -\bw^*}^2 + h^2\norm{\nabla f(\widehat{\bw}^{s}(s)) - \nabla f(\bw^*))}^2 \nonumber \\ & -2h \langle\widehat{\bw}^{s}(s) -\bw^* , \nabla f(\widehat{\bw}^{s}(s)) - \nabla f(\bw^*))\rangle \\
        &  \hspace{-4cm} \leq \norm{\widehat{\bw}^{s}(s) -\bw^*}^2  + h^2\norm{\nabla f(\widehat{\bw}^{s}(s)) - \nabla f(\bw^*))}^2 -2h\bigg( \frac{\mu L}{\mu +L }\norm{\widehat{\bw}^{s}(s) -\bw^*}^2 \nonumber \\ &  + \frac{1}{\mu + L}\norm{\nabla f(\widehat{\bw}^{s}(s)) - \nabla f(\bw^*)}^2\bigg) \\
         &  \hspace{-4cm} \leq  \bigg(1- \frac{2hL\mu}{L+\mu}\bigg)\norm{\widehat{\bw}^{s}(s) -\bw^*}^2  + \bigg(h^2-\frac{2h}{\mu + L}\bigg)\norm{\nabla f(\widehat{\bw}^{s}(s)) - \nabla f(\bw^*)}^2 \\
          &  \hspace{-4cm} \leq  \bigg(1- \frac{2hL\mu}{L+\mu}\bigg)\norm{\widehat{\bw}^{s}(s) -\bw^*}^2 + \mu^2\bigg(h^2-\frac{2h}{\mu + L}\bigg)\norm{\widehat{\bw}^{s}(s) -\bw^*}^2 \\
          &  \hspace{-4cm} \leq  ( 1-\mu h)^2\norm{\widehat{\bw}^{s}(s) -\bw^*}^2,
    \end{align}
    where in the second last step we used the fact that $h < \frac{2}{\mu +L}$.
    Then we get that:
    \begin{align}
        \norm{\widehat{\bw}^{s}(s) - h\nabla f(\widehat{\bw}^{s}(s)) - \bw^*}   &  \leq   (1-\mu h)\norm{\widehat{\bw}^{s}(s) - \bw^*} . \label{convexprop}
    \end{align}
    Finally subtracting $\bw^*$ from both sides of \eqref{stackvec2} in the proof of Lemma \ref{supportlem_inexactrule_007}, taking norm, substituting \eqref{convexprop} and \eqref{e2boundtemp_*0} we get: 
    \begin{align}
         \norm{\widehat{\bw}^{s+1}(s+1)- \bw^*} & \leq  (1-\mu h)\norm{\widehat{\bw}^{s}(s) - \bw^*} + \norm{\be_1(s)} + Lh \sqrt{Md}\sum\limits_{k=1}^d \norm{[\widehat{\bW}^{k,s}(s)]_k - [{\bW}(s)]_k},
    \end{align}
   which completes the proof.
\qed


\subsection{Proof of Lemma \ref{lemmarecursion101} } \label{lemmarecursionproof}

    In order to develop rates of convergence for strongly convex functions, using Definition \ref{deferrorseq}, we first express $ \xi^1_k(s+1), \xi^5_k(s+1) $ for all $k \in \{1,\dots,d\}$ and $\xi^6(s+1)$ in terms of  $ \xi^1_k(s), \xi^5_k(s), \xi^6(s) $ and some residual terms corresponding to $\norm{\be_1(s)}$ and $\norm{\bw_i^* -\bw^*} $ for $i \in \cN$. 

Using Lemma \ref{lemxi1} and Lemma \ref{tkhatlemma} we get:
\begin{align}
    \xi^1_k(s+1) &\leq  M^{\frac{3}{2}}(\sqrt{M}+1)( 1-\beta^{\tau M} )^{\left\lfloor\frac{(J-2)}{\tau M}\right\rfloor} \xi^1_k(s) +  h(\sqrt{M}+1)\xi^2_k(s) \\
    &\leq  a_1\xi^1_k(s) +  a_2h\sqrt{M}{\sum\limits_{k=1}^d \xi^1_k(s)}  \nonumber +    a_2 Mh\xi^6(s) + a_2 h \Delta, \label{lmi1}
\end{align}
where $a_1 = M^{\frac{3}{2}}(\sqrt{M}+1)( 1-\beta^{\tau M} )^{\left\lfloor\frac{(J-2)}{\tau M}\right\rfloor}$, $a_2 = (\sqrt{M} + 1)^2 L$ and $\Delta = \sum\limits_{i=1}^M \norm{ \bw^*- \bw_i^* }$.

Similarly, using Lemma \ref{wkbarlemma} and Lemma \ref{tkhatlemma} we get:
\begin{align}
     \xi^5_k(s+1) &\leq  M^{\frac{3}{2}}( 1-\beta^{\tau M} )^{\left\lfloor\frac{(J-2)}{\tau M}\right\rfloor} \xi^5_k(s)  + h\norm{[\overline{\bT}(s)]_k- [{\bT}(s)]_k} \\
     &\leq  a_3 \xi^5_k(s)  +   a_4 h\sqrt{M}{\sum\limits_{k=1}^d \xi^1_k(s)} +   a_4 Mh\xi^6(s) + a_4 h \Delta, \label{lmi2}
\end{align}
where $a_3 = M^{\frac{3}{2}}( 1-\beta^{\tau M} )^{\left\lfloor\frac{(J-2)}{\tau M}\right\rfloor}$ and $a_4 = L$.

From the definition of $\be_1(s)$ in Lemma \ref{convexsclem} and by Jensen's inequality we can write:
\begin{align}
    \norm{\be_1(s)} \leq h\sum\limits_{k=1}^d \underbrace{\lvert \nabla_k f(\widehat{\bw}^{s}(s))  -  \nabla_k f^{k,s+1}(\widehat{\bw}^{s}(s)) \rvert }_{=\gamma_k(s)} = h \gamma(s).\label{e1errorbound}
\end{align}
Then using Lemma \ref{convexsclem} and \eqref{e1errorbound} we get:
\begin{align}
    \xi^6(s+1) &\leq (1-\mu h)\xi^6(s)  +  \norm{\be_1(s)}+    Lh \sqrt{Md}\sum\limits_{k=1}^d \norm{[\widehat{\bW}^{k,s}(s)]_k - [{\bW}(s)]_k} \\
    &\leq (1-\mu h)\xi^6(s)  +   \underbrace{h\sum\limits_{k=1}^d \gamma_k(s)}_{=h \gamma(s)} +    \underbrace{Lh \sqrt{Md}}_{=a_5 h}\sum\limits_{k=1}^d \xi^1_k(s). \label{lmi3}
\end{align}
Let
\begin{align}
    \bA  = \begin{bNiceMatrix}
         a_1 + a_2 h \sqrt{M} &  0  \\
        a_4 h \sqrt{M} & \hspace{-0.3cm} a_3 
    \end{bNiceMatrix},
    \hspace{1cm}\bB &= \begin{bNiceMatrix}
        a_2 h \sqrt{M} & 0 \\
        a_4 h \sqrt{M} & 0
    \end{bNiceMatrix}.
\end{align}
Stacking $ \{\xi^1_k(s)\}_{k=1}^d$, $ \{\xi^5_k(s)\}_{k=1}^d$, $\xi^6(s)$ into a vector for any $s$ and invoking the bounds \eqref{lmi1}, \eqref{lmi2}, \eqref{lmi3} we have the following inexact recursion of the error terms:

\begin{equation}
\underbrace{\begin{bNiceMatrix}
  \xi^1_1(s+1) \\
  \xi^5_1(s+1) \\
   \xi^1_2(s+1) \\
  \xi^5_2(s+1) \\
  %\hline 
   \vdots  \\ \vspace{-0.5cm}
 \vdots  \\
   \vspace{-0.5cm}
    \vdots  \\
   \xi^1_d(s+1) \\
  \xi^5_d(s+1) \\
  \xi^6(s+1) 
   \end{bNiceMatrix}}_{=\bfg(s+1) \hspace{0.1cm} \in \hspace{0.1cm} \mathbb{R}_{+}^{(2d+1)}} \leq \underbrace{\begin{bNiceMatrix}
  \Block{2-2}<\Large>{\bA} & &  \Block{2-2}<\Large>{\bB} && \Block{2-2}<\Large>{\bB} &&  \Block{2-1}<\Large>{\cdots} &   \Block{2-2}<\Large>{\bB} && a_2 M h \\
  &   & & && && &&  a_4 M h \\
 % \hline
  \Block{2-2}<\Large>{\bB} & &   \Block{2-2}<\Large>{\bA} & & \Block{2-2}<\Large>{\bB} & & \Block{2-1}<\Large>{\cdots} & \Block{2-2}<\Large>{\bB} &&   a_2 M h \\
  & &    &   && && &&  a_4 M h \\
  %\hline 
    \hspace{0.5cm}\vdots && && \ddots && &&  \hspace{-1cm}\vdots & \vdots \\ \vspace{-0.5cm}
   \hspace{0.5cm} \vdots && && & \ddots &&&  \hspace{-1cm}\vdots & \vdots \\
   \vspace{-0.5cm}
   \hspace{0.5cm} \vdots && && && \ddots &&  \hspace{-1cm}\vdots & \vdots  \\
    \Block{2-2}<\Large>{\bB} &&  \hspace{-0.5cm}  \Block{2-2}<\Large>{\bB} &&  \Block{2-1}<\Large>{\cdots} & \Block{2-2}<\Large>{\bB} &  &   \Block{2-2}<\Large>{\bA} &  & a_2 M h \\
  & &   && && &   &   & a_4 M h \\
    a_5h &\hspace{-0.3cm} 0 & a_5h & 0 & \cdots & \cdots & \cdots  & a_5h & 0 &  1- \mu h
  \end{bNiceMatrix}}_{=\bM(h,J) \hspace{0.1cm} \in \hspace{0.1cm} \mathbb{R}_{+}^{(2d+1) \times (2d+1)}}\underbrace{\begin{bNiceMatrix}
  \xi^1_1(s) \\
  \xi^5_1(s) \\
   \xi^1_2(s) \\
  \xi^5_2(s) \\
  %\hline 
   \vdots  \\ \vspace{-0.5cm}
 \vdots  \\
   \vspace{-0.5cm}
    \vdots  \\
   \xi^1_d(s) \\
  \xi^5_d(s) \\
  \xi^6(s) 
  \end{bNiceMatrix}}_{=\bfg(s) \in \hspace{0.1cm} \mathbb{R}_{+}^{(2d+1) }} +   \underbrace{\begin{bNiceMatrix}
   a_2 h\Delta  \\
  a_4 h\Delta  \\
   a_2h\Delta  \\
   a_4 h\Delta  \\
  %\hline 
   \vdots  \\ \vspace{-0.5cm}
 \vdots  \\
   \vspace{-0.5cm}
    \vdots  \\
    a_2 h\Delta \\
   a_4 h\Delta  \\
 h \gamma(s) 
  \end{bNiceMatrix} }_{=\bepsilon(s)  \in  \mathbb{R}_{+}^{(2d+1) }} .\label{lmistatespace}
  \end{equation}  
  Let us express $\bM(h,J)= \bM_0 + \bP(h,J) $ where 
  \begin{equation}
      \bM_0 =  \begin{bNiceMatrix}
  a_1 &  0 &  \Block{2-2}<\Large>{\mathbf{0}} && \Block{2-2}<\Large>{\mathbf{0}} &&  \Hdotsfor{1} &   \Block{2-2}<\Large>{\mathbf{0}} && 0 \\
  0 &  a_3  & & && && &&  0 \\
 % \hline
  \Block{2-2}<\Large>{\mathbf{0}} & & a_1 & 0 & \Block{2-2}<\Large>{\mathbf{0}} & & \Hdotsfor{1} & \Block{2-2}<\Large>{\mathbf{0}} &&   0  \\
  & &  0 & a_3  && && &&  0 \\
  %\hline 
    \hspace{0.5cm}\vdots && && \ddots && &&  \hspace{-0.5cm}\vdots & \vdots \\ \vspace{-0.5cm}
   \hspace{0.5cm} \vdots && && & \ddots &&&  \hspace{-0.5cm}\vdots & \vdots \\
   \vspace{-0.5cm}
   \hspace{0.5cm} \vdots && && && \ddots &&  \hspace{-0.5cm}\vdots & \vdots  \\
    \Block{2-2}<\Large>{\mathbf{0}} &&  \hspace{-0.5cm}  \Block{2-2}<\Large>{\mathbf{0}} &&  \Hdotsfor{1} & \Block{2-2}<\Large>{\mathbf{0}} &  &  a_1 & 0 & 0 \\
  & &   && && & 0 & a_3 & 0 \\
  0 & 0 & 0 & 0 & \cdots & \cdots & \cdots  & 0 & 0 & 1
  \end{bNiceMatrix},
  \end{equation}
  \begin{equation}\label{182}
      \bP(h,J) =  \begin{bNiceMatrix}
  a_2 h \sqrt{M} &  0 &  \Block{2-2}<\Large>{\bB} && \Block{2-2}<\Large>{\bB} &&  \Hdotsfor{1} &   \Block{2-2}<\Large>{\bB} && a_2 M h \\
 a_4 h \sqrt{M} &  0 & & && && &&  a_4 M h \\
 % \hline
  \Block{2-2}<\Large>{\bB} & & a_2 h \sqrt{M} & 0 & \Block{2-2}<\Large>{\bB} & & \Hdotsfor{1} & \Block{2-2}<\Large>{\bB} &&   a_2 M h \\
  & &  a_4 h \sqrt{M} & 0 && && &&  a_4 M h \\
  %\hline 
    \hspace{0.5cm}\vdots && && \ddots && &&  \hspace{-1.2cm}\vdots & \vdots \\ \vspace{-0.5cm}
   \hspace{0.5cm} \vdots && && & \ddots &&&  \hspace{-1.2cm}\vdots & \vdots \\
   \vspace{-0.5cm}
   \hspace{0.5cm} \vdots && && && \ddots &&  \hspace{-1.2cm}\vdots & \vdots  \\
    \Block{2-2}<\Large>{\bB} &&  \hspace{-0.5cm}  \Block{2-2}<\Large>{\bB} &&  \Hdotsfor{1} & \Block{2-2}<\Large>{\bB} &  &  a_2 h \sqrt{M} & 0 & a_2 M h \\
  & &   && && & a_4 h \sqrt{M} & 0  & a_4 M h \\
  a_5h &\hspace{-0.3cm} 0 & a_5h & 0 & \cdots & \cdots & \cdots  & a_5h & 0 &  - \mu h
  \end{bNiceMatrix}.
  \end{equation}
  Then, from \eqref{lmistatespace} and the above matrix definitions, we get the following recursion
    \begin{align}
   \bfg(s+1) \leq \bigg(\bM_0+ \bP(h,J)\bigg)\bfg(s) + \bepsilon(s),  \label{matperturbeq1}
\end{align}
where we split the matrix $\bM(h,J)$ into the sum of a constant matrix $ \bM_0$ (constant in $h$) and a perturbation matrix $ \bP(h,J)$. This completes the proof.
\qed
    


\subsection{Proof of Theorem \ref{inexactlmigeo}}\label{inexactlmigeoproof}
This section consists of three parts of the proof. The first part includes the proof of the geometric rates of $\norm{\bfg(S)}$ as in \eqref{eqn: slow rate} of Theorem \ref{inexactlmigeo}; the second part consists of the proof of the geometric convergence rate of two error sequence $\xi^1_k(s)$ and $\xi^5_k(s)$ as in \eqref{eqn: xi1} and \eqref{eqn: xi5} of Theorem \ref{inexactlmigeo}; the last part contains the proof of the geometric convergence rate of the error sequence  $\xi^6_k(s)$ as in \eqref{eqn: xi6} of Theorem \ref{inexactlmigeo}.

\textbf{Rate analysis for $\norm{\bfg(S)}$ convergence to a $\cO(C_0 + \Delta)$ ball as in \eqref{eqn: slow rate}}
\begin{theo}\cite[Theorem 6.3.12]{horn2012matrix}\label{theomatperturb}
     Let $\bX, \bE \in \mathbb{R}^{n \times n}$ and let $q$ be a simple eigenvalue of $\bX$. Let $\mathbf{v}$ and $\mathbf{u}$ be, respectively, the right and left eigenvectors of $\bX$ corresponding to the eigenvalue $q$. Then,
     \begin{enumerate}
         \item  for each $\epsilon>0$, there exists a $\delta>0$ such that, $\forall p \in \mathbb{C}$ with $|p|<\delta$, there is a unique eigenvalue $q(p)$ of $\bX+p \bE$ such that $\left|q(p)-q-p \frac{\mathbf{u}^H \bE \mathbf{v}}{\mathbf{u}^H \mathbf{v}}\right| \leq|p| \epsilon$,
         \item  $q(p)$ is continuous at $p=0$, and $\lim _{p \rightarrow 0} q(p)=q$,
         \item $q(p)$ is differentiable at $p=0,\left.\frac{d q(p)}{d p}\right|_{p=0}=\frac{\mathbf{u}^H \bE \mathbf{v}}{\mathbf{u}^H \mathbf{v}}$.
     \end{enumerate}
        where $(\cdot)^{H}$ is Hermitian operator.
\end{theo}

Observe from Lemma \ref{lemmarecursion101} that $\bP(h, J) = \Theta(h) $ and so we can write $\bP(h, J) = h \bE $ for some constant matrix $\bE$ (constant in terms of $h$). Then for $\bX = \bM_0$ and $\bP(h,J) = h \bE $, Theorem \ref{theomatperturb} can be readily applied. Note that $\mathbf{u} = [0,0,\cdots, 0, 1]^T$ is both the left and right eigenvector for $\bM_0$ corresponding to the simple eigenvalue $1$. Also, we have the following by some simple algebraic manipulation using \eqref{182}:
\begin{align}
    \frac{\mathbf{u}^H \bE \mathbf{u}}{\mathbf{u}^H \mathbf{u}} = -\mu .
\end{align}

Then from Theorem \ref{theomatperturb} for $\mu> \epsilon >0$ and any $h$ sufficiently small, $\bM(h, J)$ has a unique eigenvalue corresponding to the eigenvalue $1$ of $\bM_0$ and its absolute value is upper bounded by $1- (\mu- \epsilon)h$.
Since $a_1 > a_3$ we get that $a_3< a_1<0.5$ for any $ J > \frac{ \tau M\log( 2M^{\frac{3}{2}}(\sqrt{M}+1) )}{\log( 1-\beta^{\tau M} )^{-1}} + \tau M +2$ from the following bound:
\begin{align}
    M^{\frac{3}{2}}(\sqrt{M}+1)( 1-\beta^{\tau M} )^{\left\lfloor\frac{(J-2)}{\tau M}\right\rfloor} & < \frac{1}{2} \\
    \impliedby \frac{(J-2)}{\tau M} & > \frac{ \log( 2M^{\frac{3}{2}}(\sqrt{M}+1) )}{\log( 1-\beta^{\tau M} )^{-1}} +1 \\
    \impliedby J & > \frac{ \tau M\log( 2M^{\frac{3}{2}}(\sqrt{M}+1) )}{\log( 1-\beta^{\tau M} )^{-1}} + \tau M +2.
\end{align}
Also, since $a_3< a_1<0.5$, therefore the spectral radius of $\bM_0 = 1 $.

Since all the other eigenvalues of $\bM_0$ are $a_1, a_3$ with $a_3<a_1<0.5$ and $h$ is sufficiently small, we have that the magnitude of the largest eigenvalue of $\bM(h, J)$ is equal to $1- (\mu- \epsilon)h$, which is strictly smaller than $1$ for $\epsilon < \mu$ and greater than $0.5$ for sufficiently small $h$. Hence we get that the spectral radius of $ \bM(h,J)$ satisfies $\rho(\bM(h,J)) \leq  1- (\mu- \epsilon)h < 1$. Then we have from Lemma 5.6.10 in \cite{horn2012matrix} that there exists a matrix norm, say $\vvvert \cdot \vvvert_{\bM(h,J)}$, such that
$$
\vvvert\bM(h,J)\vvvert_{\bM(h,J)} = \rho(\bM(h,J)) <1 .
$$

Moreover, from Theorem 5.7.13 in \cite{horn2012matrix}, we know that for any matrix norm, $\vvvert \cdot \vvvert_{\bA}$, there exists a compatible vector norm, say $\norm{\cdot}_{\bA}$, such that $\|\bB \mathbf{x}\|_{\bA} \leq\vvvert \bB \vvvert_{\bA}\norm{\mathbf{x}}_{\bA}$ for all matrices $\bB$ and all vectors $\mathbf{x}$. Hence, taking $ \norm{\cdot}_{\bM(h,J)}$ on both sides of \eqref{matperturbeq1}, where $ \norm{\cdot}_{\bM(h,J)}$ is a compatible vector norm to the matrix norm $\vvvert \cdot \vvvert_{\bM(h,J)}$ associated with $\bM(h,J)$, we get that:
\begin{align}
   \norm{ \bfg(s+1)}_{\bM(h,J)} & \leq \norm{\bigg(\bM_0+ \bP(h,J)\bigg)\bfg(s)}_{\bM(h,J)} + \norm{\bepsilon(s)}_{\bM(h,J)} \\
   & \leq  \vvvert \bM_0+ \bP(h,J) \vvvert_{\bM(h,J)}\norm{\bfg(s)}_{\bM(h,J)} + \norm{\bepsilon(s)}_{\bM(h,J)} \\
   & = \rho(\bM(h,J)) \norm{\bfg(s)}_{\bM(h,J)} + \norm{\bepsilon(s)}_{\bM(h,J)} \\
   \implies \norm{ \bfg(S)}_{\bM(h,J)} & \leq \bigg(\rho(\bM(h,J))\bigg)^{S} \norm{\bfg(0)}_{\bM(h,J)} + \sum\limits_{s=0}^{S-1}  \bigg(\rho(\bM(h,J))\bigg)^{(S-s-1)} \norm{\bepsilon(s)}_{\bM(h,J)} \\
   & \lesssim_{\bM(h,J)} \bigg(\rho(\bM(h,J))\bigg)^{S} \norm{\bfg(0)} + \frac{h( C_0+ \Delta)}{1- \rho(\bM(h,J))}, \label{consensuserr_0*}
\end{align}
where in the last step we used the bound\footnote{The exact constants in $ \norm{\bepsilon(s)}_{\bM(h,J)} \lesssim_{\bM(h,J)} h\Delta+ h\gamma(s)$ will depend on $L, M , d$ but these can be directly absorbed in $\lesssim_{\bM(h,J)} $.} $ \norm{\bepsilon(s)}_{\bM(h,J)} \lesssim_{\bM(h,J)} h\Delta+ h\gamma(s)$ followed by the fact that $ \sup_{s \geq 0}\gamma(s) = \sup_{s \geq 0}\sum\limits_{k=1}^d\lvert \nabla_k f(\widehat{\bw}^{s}(s))  -  \nabla_k f^{k,s+1}(\widehat{\bw}^{s}(s)) \rvert = C_0$  where $C_0$ is finite from \eqref{e1errorbound}, Assumption \ref{boundedassump} and continuity of gradients. This completes the first part of the proof.


\textbf{Rate analysis for $\xi^1_k(s)$ and $\xi^5_k(s)$ converging to a $\cO(h)$ ball}\\
From Assumption \ref{boundedassump} we have that $ \{\sup_s \xi^1_k(s)\}_k, \sup_s \xi^6(s) $ are upper bounded by $C_1 \text{diam}(\cK_1)$ for some absolute constant $C_1>0$. Then from \eqref{lmi1} we have for any $S\geq 1$:
\begin{align}
     \xi^1_k(s+1)    &\leq  a_1\xi^1_k(s) +  a_2 \sqrt{M} (\sqrt{M} + 1) C_1 \text{diam}(\cK_1) h + a_2 \Delta h \\
     \implies \xi^1_k(S)  &\leq  (a_1)^S\xi^1_k(0) + \frac{h}{1-a_1}\bigg(a_2 \sqrt{M} (\sqrt{M} + 1) C_1 \text{diam}(\cK_1) + a_2 \Delta\bigg), \label{consensuserr_1*}
\end{align}
where $a_1 = M^{\frac{3}{2}}(\sqrt{M}+1)( 1-\beta^{\tau M} )^{\left\lfloor\frac{(J-2)}{\tau M}\right\rfloor} <1 $. 

Along similar lines, from \eqref{lmi2} we have for any $S\geq 1$:
\begin{align}
     \xi^5_k(s+1)    &\leq  a_3\xi^5_k(s) +  a_4 \sqrt{M} (\sqrt{M} + 1) C_1 \text{diam}(\cK_1) h + a_4 \Delta h \\
     \implies \xi^5_k(S)  &\leq  (a_3)^S\xi^5_k(0) + \frac{h}{1-a_3}\bigg(a_4 \sqrt{M} (\sqrt{M} + 1) C_1 \text{diam}(\cK_1) + a_4 \Delta\bigg), \label{consensuserr_2*}
\end{align}
where $a_3 = M^{\frac{3}{2}}( 1-\beta^{\tau M} )^{\left\lfloor\frac{(J-2)}{\tau M}\right\rfloor} <1 $. 
 

\textbf{Rate analysis for $\xi^6_k(s)$ converging to a $\cO(C_0 + h)$ ball}\\
From \eqref{lmi3}, \eqref{consensuserr_1*} and the definition of $C_0$ we have for any $S_0\geq 1$, $S > S_0$:
\begin{align}
 \xi^6(s+1) &\leq (1-\mu h)\xi^6(s)  + C_0 h + \hspace{0.1cm}a_5  h \sum_{k=1}^d  \xi^1_k(s)   \\
  \implies   \xi^6(S) &\leq (1-\mu h)^{S-S_0}\xi^6(S_0)  + \sum_{s= S_0}^{S-1}\bigg( C_0 h  + a_5  h\sum_{k=1}^d  \xi^1_k(s)  \bigg) (1-\mu h)^{s-S_0}   \\
 \implies   \xi^6(S) &\leq (1-\mu h)^{S-S_0}\xi^6(S_0)  + \frac{h}{1-(1-\mu h)}\bigg( C_0  + a_5 \sup_{s\geq S_0}\sum_{k=1}^d  \xi^1_k(s)  \bigg)    \\
 &\leq (1-\mu h)^{S-S_0}\xi^6(S_0)    \nonumber \\
 & \hspace{1cm}+ \frac{1}{\mu}\bigg( C_0  + a_5 d\bigg((a_1)^{S_0}\xi^1_k(0)+ \frac{h}{1-a_1}\bigg(a_2 \sqrt{M} (\sqrt{M} + 1) C_1 \text{diam}(\cK_1) + a_2 \Delta\bigg)\bigg)  \bigg) \\
 & = (1-\mu h)^{S-S_0}\xi^6(S_0)  + \frac{C_0}{\mu}  \nonumber \\
 &\hspace{1cm}+ \frac{L \sqrt{Md}}{\mu}\bigg((a_1)^{S_0}\xi^1_k(0) + \frac{h}{1-a_1}\bigg(a_2 \sqrt{M} (\sqrt{M} + 1) C_1 \text{diam}(\cK_1) + a_2 \Delta\bigg)\bigg). \label{consensuserr_3*}
\end{align}
where we substituted $a_5 =L \sqrt{Md}$ in the last step. This completes the third part of the proof.
\qed


\subsection{Proof of Corollary \ref{corro_inexactlmigeo}}\label{corro_inexactlmigeoproof}

Taking $S \to \infty$ in \eqref{consensuserr_0*} and substituting $ \rho(\bM(h,J)) = 1- (\mu- \epsilon)h$, we get:
\begin{align}
        \limsup_{S \to \infty} \norm{\bfg(S)} \lesssim_{\bM(h,J)}  \frac{( C_0+ \Delta)}{\mu -\epsilon}.
    \end{align}
Taking $S \to \infty$ in \eqref{consensuserr_1*} and \eqref{consensuserr_2*}, we get:
 \begin{align}
     \limsup_{S \to \infty} \xi^1_k(S)  &\leq  \frac{h}{1-a_1}\bigg(a_2 \sqrt{M} (\sqrt{M} + 1) C_1 \text{diam}(\cK_1) + a_2 \Delta\bigg), \\
      \limsup_{S \to \infty} \xi^5_k(S)  &\leq  \frac{h}{1-a_3}\bigg(a_4 \sqrt{M} (\sqrt{M} + 1) C_1 \text{diam}(\cK_1) + a_4 \Delta\bigg).
 \end{align}
    Finally, taking $S \to \infty$ in \eqref{consensuserr_3*}, we have :
    \begin{align}
        \limsup_{S \to \infty}  \xi^6(S) & \leq \frac{C_0}{\mu} + \frac{L \sqrt{Md}}{\mu}(a_1)^{S_0}\xi^1_k(0) + \frac{L \sqrt{Md}}{\mu}\bigg( \frac{h}{1-a_1}\bigg(a_2 \sqrt{M} (\sqrt{M} + 1) C_1 \text{diam}(\cK_1) + a_2 \Delta\bigg)\bigg).
    \end{align}
    Since the above bound holds for any $S_0$, taking $S_0 \to \infty$ we have:
\begin{align}
  \limsup_{S \to \infty}  \xi^6(S) & \leq \frac{C_0}{\mu} + \frac{L \sqrt{Md}}{\mu}\bigg( \frac{h}{1-a_1}\bigg(a_2 \sqrt{M} (\sqrt{M} + 1) C_1 \text{diam}(\cK_1) + a_2 \Delta\bigg)\bigg). 
\end{align}
This completes the proof.
\qed




\subsection{Proof of Theorem \ref{maintheorempaper1}}\label{maintheorempaper1proof}
This section consists of two parts of the proof. The first part includes the proof of the model parameter of Algorithm RESIST obtaining the geometric rate converging to a $\cO(C_0+\Delta)$ radius ball around $\bW^*$ as in \eqref{ballconvergence1} of Theorem \ref{maintheorempaper1}; the second part consists of the proof of the model parameter of Algorithm RESIST obtaining the geometric rate converging to a $\cO(C_0+h)$ radius ball around $\bW^*$ as in \eqref{ballconvergence2} of Theorem \ref{maintheorempaper1}.
\begin{proof}
\textbf{Model parameter of Algorithm RESIST converging to $\cO(C_0+\Delta)$ ball:}\\
Recall from \eqref{e2_ineq3} that we have the bound : 
\begin{align}
    \sum\limits_{j=1}^M \norm{ \widehat{\bw}^{s}(s) - \bw_j(s)} & \leq \sqrt{M}\sum\limits_{k=1}^d \norm{[\widehat{\bW}^{k,s}(s)]_k - [{\bW}(s)]_k} .
\end{align}
 Then for $\bW^* = \mathbf{1}(\bw^*)^T$ and $\widehat{\bW}^s(s) = \mathbf{1}(\widehat{\bw}^s(s))^T$, using Definition \ref{deferrorseq}, inequality \eqref{e2_ineq3} and Jensen's inequality we get that:
\begin{align}
    \norm{\bW(s) - \overline{\bW}(s)}_F^2 & = {\sum_{k=1}^d (\xi^5_k(s))^2}  \label{finrate1}\\
    \norm{\bW^* - \widehat{\bW}^s(s)}_F^2 & = {\sum_{i=1}^M (\xi^6(s))^2}  = M (\xi^6(s))^2 \label{finrate2} \\
     \norm{\bW(s) -\widehat{\bW}^s(s) }_F^2 & = \sum\limits_{j=1}^M \norm{ \widehat{\bw}^{s}(s) - \bw_j(s)}^2 \nonumber \leq \bigg( \sum\limits_{j=1}^M \norm{ \widehat{\bw}^{s}(s) - \bw_j(s)} \bigg)^2 \nonumber \\
     & \leq M \bigg( \sum\limits_{k=1}^d \norm{[\widehat{\bW}^{k,s}(s)]_k - [{\bW}(s)]_k} \bigg)^2 \leq Md {\sum_{k=1}^d (\xi^1_k(s))^2}.  \label{finrate3}
\end{align}
Then summing up \eqref{finrate1}, \eqref{finrate2} and \eqref{finrate3}, taking square root and using the definition of $\bfg(s)$ from \eqref{lmistatespace} we have the following bound:
\begin{align}
 \sqrt{\norm{\bW(s) - \overline{\bW}(s)}_F^2+  \norm{\bW^* - \widehat{\bW}^s(s)}_F^2 +  \norm{\bW(s) -\widehat{\bW}^s(s) }_F^2} &=  \nonumber \\
 & \hspace{-4cm}\sqrt{\sum_{k=1}^d (\xi^5_k(s))^2 + M (\xi^6(s))^2 +Md {\sum_{k=1}^d (\xi^1_k(s))^2}} \\
 & \hspace{-4cm} \leq \sqrt{Md}\sqrt{\sum_{k=1}^d (\xi^5_k(s))^2 +  (\xi^6(s))^2 +  \sum_{k=1}^d (\xi^1_k(s))^2}   \label{intermb1_0} \\
    & \hspace{-4cm} =   \sqrt{Md} \norm{\bfg(s)}. \label{intermb1}
\end{align}
Next, using Cauchy Schwarz inequality along with \eqref{intermb1}, Theorem \ref{inexactlmigeo} and the fact that $ \norm{\bfg(s)} \lesssim_{\bM(h,J)} \norm{\bfg(s)}_{\bM(h,J)}$ we get that:
\begin{align}
     \norm{\bW(s) - \overline{\bW}(s)}_F+ \norm{\bW^* - \widehat{\bW}^s(s)}_F + \norm{\bW(s) -\widehat{\bW}^s(s) }_F & \lesssim_{\bM(h,J)}\nonumber \\ & \hspace{-4cm}\sqrt{3Md}\bigg(\rho(\bM(h,J))\bigg)^{s} \norm{\bfg(0)} + \frac{\sqrt{3Md} h( C_0+ \Delta)}{1- \rho(\bM(h,J))} . \label{finthm0} 
\end{align}
We now derive the bounds in \eqref{finthm0} in the $t$-time scale. Using the facts that $s =\lfloor \frac{t}{J} \rfloor $, $Js \leq  t < Js+J-1$, $\norm{\bA} \leq \sqrt{M} \norm{\bA}_{\infty} = \sqrt{M}$ for any row stochastic matrix $\bA \in \mathbb{R}^{M \times M}$, that $  [\overline{\bW}(s)]_k  $ lies in the null space of $\bigg(\bI - \frac{\mathbf{1}\mathbf{1}^T}{M}\bigg)  \prod\limits_{r= J \lfloor \frac{t}{J} \rfloor  }^{t} \bY_{k}(r) $ and invoking \eqref{cwtm1} we get:

\begin{align}
   \norm{\bW(t) - \overline{\bW}(t)}_F^2 & = \sum\limits_{k=1}^d \norm{[\bW(t)]_k - [\overline{\bW}(t)]_k}^2 \\ 
   & =\sum\limits_{k=1}^d \norm{ \bigg( \prod\limits_{r= J \lfloor \frac{t}{J} \rfloor  }^{t} \bY_{k}(r)  [\bW(s)]_k - \frac{\mathbf{1}\mathbf{1}^T}{M}\prod\limits_{r= J \lfloor \frac{t}{J} \rfloor  }^{t} \bY_{k}(r) [{\bW}(s)]_k \bigg)}^2 \\ 
    & =\sum\limits_{k=1}^d \norm{\bigg(\bI - \frac{\mathbf{1}\mathbf{1}^T}{M}\bigg)  \prod\limits_{r= J \lfloor \frac{t}{J} \rfloor  }^{t} \bY_{k}(r)  [\bW(s)]_k }^2 \\ 
     & =\sum\limits_{k=1}^d \norm{\bigg(\bI - \frac{\mathbf{1}\mathbf{1}^T}{M}\bigg)  \prod\limits_{r= J \lfloor \frac{t}{J} \rfloor  }^{t} \bY_{k}(r)  \bigg([\bW(s)]_k - [\overline{\bW}(s)]_k \bigg) }^2 \\ 
          & \leq \sum\limits_{k=1}^d \norm{\bigg(\bI - \frac{\mathbf{1}\mathbf{1}^T}{M}\bigg) }^2 \norm{ \prod\limits_{r= J \lfloor \frac{t}{J} \rfloor  }^{t} \bY_{k}(r)  \bigg([\bW(s)]_k - [\overline{\bW}(s)]_k \bigg) }^2 \\ 
   & =\sum\limits_{k=1}^d \norm{ \prod\limits_{r= J \lfloor \frac{t}{J} \rfloor  }^{t} \bY_{k}(r) \bigg( [\bW(s)]_k - [\overline{\bW}(s)]_k \bigg)}^2 
   \end{align}
   \begin{align}
      & \leq \sum\limits_{k=1}^d \norm{\prod\limits_{r= J \lfloor \frac{t}{J} \rfloor  }^{t} \bY_{k}(r)}^2 \norm{  \bigg( [\bW(s)]_k - [\overline{\bW}(s)]_k \bigg)}^2 \\
      & \leq \sum\limits_{k=1}^d  M \norm{  \bigg( [\bW(s)]_k - [\overline{\bW}(s)]_k \bigg)}^2 \\
   &\leq M \sum\limits_{k=1}^d \norm{  \bigg( [\bW(s)]_k - [\overline{\bW}(s)]_k \bigg)}^2 \\
   & =  M \norm{\bW(s) - \overline{\bW}(s)}_F^2. \label{finthm1}
\end{align}
Next, from Definition \ref{def3_tscale} we have $\widehat{\bW}^s(t) = \mathbf{1}(\widehat{\bw}^s(t))^T$. Then using the fact that the vector $  [\widehat{\bW}^s(s)]_k  $ lies in the null space of $\bigg(\bI -\bQ^{\pi}_k(s)\bigg)  \prod\limits_{r= J \lfloor \frac{t}{J} \rfloor  }^{t} \bY_{k}(r) $, that $\norm{\bA} \leq \sqrt{M} \norm{\bA}_{\infty} = \sqrt{M}$ for any row stochastic matrix $\bA \in \mathbb{R}^{M \times M}$ and following the steps leading up to \eqref{finthm1} we have that:
\begin{align}
   \norm{\bW(t) - \widehat{\bW}^s(t)}_F^2 & = \sum\limits_{k=1}^d \norm{[\bW(t)]_k - [\widehat{\bW}^s(t)]_k}^2 \\ 
   & =\sum\limits_{k=1}^d \norm{ \bigg( \prod\limits_{r= J \lfloor \frac{t}{J} \rfloor  }^{t} \bY_{k}(r)  [\bW(s)]_k - \bQ^{\pi}_k(s)\prod\limits_{r= J \lfloor \frac{t}{J} \rfloor  }^{t} \bY_{k}(r) [{\bW}(s)]_k \bigg)}^2 \\ 
    & =\sum\limits_{k=1}^d \norm{\bigg(\bI - \bQ^{\pi}_k(s)\bigg)  \prod\limits_{r= J \lfloor \frac{t}{J} \rfloor  }^{t} \bY_{k}(r)  [\bW(s)]_k }^2 \\ 
     & =\sum\limits_{k=1}^d \norm{\bigg(\bI - \bQ^{\pi}_k(s)\bigg)  \prod\limits_{r= J \lfloor \frac{t}{J} \rfloor  }^{t} \bY_{k}(r)  \bigg([\bW(s)]_k - [\widehat{\bW}^s(s)]_k  \bigg) }^2 \\ 
          & \leq \sum\limits_{k=1}^d \norm{\bigg(\bI - \bQ^{\pi}_k(s)\bigg) }^2 \norm{ \prod\limits_{r= J \lfloor \frac{t}{J} \rfloor  }^{t} \bY_{k}(r)  \bigg([\bW(s)]_k - [\widehat{\bW}^s(s)]_k \bigg) }^2 
          \end{align}
          \begin{align}
   & \leq (\sqrt{M}+1)^2\sum\limits_{k=1}^d \norm{ \prod\limits_{r= J \lfloor \frac{t}{J} \rfloor  }^{t} \bY_{k}(r) \bigg( [\bW(s)]_k - [\widehat{\bW}^s(s)]_k \bigg)}^2 \\ 
      & \leq (\sqrt{M}+1)^2\sum\limits_{k=1}^d \norm{\prod\limits_{r= J \lfloor \frac{t}{J} \rfloor  }^{t} \bY_{k}(r)}^2 \norm{  \bigg( [\bW(s)]_k - [\widehat{\bW}^s(s)]_k  \bigg)}^2 \\
      & \leq (\sqrt{M}+1)^2\sum\limits_{k=1}^d  M \norm{  \bigg( [\bW(s)]_k - [\overline{\bW}(s)]_k \bigg)}^2 \\
   &\leq (\sqrt{M}+1)^2 M \sum\limits_{k=1}^d \norm{  \bigg( [\bW(s)]_k - [\widehat{\bW}^s(s)]_k \bigg)}^2 \\
   & = (\sqrt{M}+1)^2 M  \norm{\bW(s) - \widehat{\bW}^s(s)}_F^2. \label{finthm11}
\end{align}


Similarly, we will also get that 
\begin{align}
    \norm{\bW^* - \widehat{\bW}^s(t)}_F^2  & \leq (\sqrt{M}+1)^2 M\norm{\bW^* - \widehat{\bW}^s(s)}_F^2. \label{finthm2}
\end{align}
Then combining \eqref{finthm0}, \eqref{finthm1}, \eqref{finthm11}, \eqref{finthm2}, substituting $s=S$ and using the facts that $ \frac{t}{J} -1 < S \leq \frac{t}{J} $,  $ \rho(\bM(h,J))< 1$ for $0< \epsilon < \mu$ we get:
\begin{align}
    \norm{\bW(t) - \overline{\bW}(t)}_F+  \norm{\bW^* - \widehat{\bW}^S(t)}_F  +  \norm{\bW(t) - \widehat{\bW}^S(t)}_F & \lesssim_{\bM(h,J)} \nonumber \\ & \hspace{-4.5cm} \sqrt{3d}(\sqrt{M}+1) M\bigg( \bigg(\rho(\bM(h,J))\bigg)^{\frac{t}{J}-1} \norm{\bfg(0)} + \frac{h( C_0+ \Delta)}{1- \rho(\bM(h,J))} \bigg)  .
\end{align}
Last, taking $t \to \infty$ and substituting $ \rho(\bM(h,J)) = 1-(\mu-\epsilon) h$ for any $0< \epsilon < \mu$ from Theorem \ref{inexactlmigeo} we get that:
\begin{align}
   \limsup_{t \to \infty} \bigg( \norm{\bW(t) - \overline{\bW}(t)}_F+  \norm{\bW^* - \widehat{\bW}^S(t)}_F  +  \norm{\bW(t) - \widehat{\bW}^S(t)}_F\bigg) & \lesssim_{\bM(h,J)} \nonumber \\ & \hspace{-8cm}  \limsup_{t \to \infty} \sqrt{3d}(\sqrt{M}+1) M\bigg( \bigg(\rho(\bM(h,J))\bigg)^{\frac{t}{J}-1} \norm{\bfg(0)} + \frac{h( C_0+ \Delta)}{1- \rho(\bM(h,J))} \bigg)\\ 
   & \hspace{-8cm} =  \frac{\sqrt{3d}(\sqrt{M}+1) M ( C_0+ \Delta)}{\mu- \epsilon}.
\end{align}
This completes the first part of the proof.

\textbf{Model parameter of Algorithm RESIST converging to $\cO(C_0+h)$ ball:}\\
Using the bound \eqref{intermb1_0}, Jensen's inequality and the second part of Theorem \ref{inexactlmigeo} for some $S_0 < s$ we can write:
\begin{align}
   \sqrt{\norm{\bW(s) - \overline{\bW}(s)}_F^2+  \norm{\bW^* - \widehat{\bW}^s(s)}_F^2 +  \norm{\bW(s) -\widehat{\bW}^s(s) }_F^2} 
 & \leq  \sqrt{Md} \bigg({\sum_{k=1}^d \xi^5_k(s) +  \xi^6(s)  +  \sum_{k=1}^d \xi^1_k(s)}\bigg) \\
 & \hspace{-7cm}\leq \sqrt{Md} \Bigg(\sum_{k=1}^d \bigg(   (a_1)^s\xi^1_k(0) + \frac{h}{1-a_1}\bigg(a_2 \sqrt{M} (\sqrt{M} + 1) C_1 \text{diam}(\cK_1) + a_2 \bigg) + \nonumber \\
 & \hspace{-3cm} (a_3)^s\xi^5_k(0) + \frac{h}{1-a_3}\bigg(a_4 \sqrt{M} (\sqrt{M} + 1) C_1 \text{diam}(\cK_1) + a_4 \Delta\bigg) \bigg) + \nonumber \\
 & \hspace{-3cm} (1-\mu h)^{s-S_0}\xi^6(S_0)  + \frac{C_0}{\mu} + \frac{L \sqrt{Md}}{\mu}\bigg((a_1)^{S_0}\xi^1_k(0) \nonumber \\ & \hspace{-3cm} + \frac{h}{1-a_1}\bigg(a_2 \sqrt{M} (\sqrt{M} + 1) C_1 \text{diam}(\cK_1) + a_2 \Delta\bigg)\bigg) \Bigg). \label{c0_delta_ballbound}
\end{align}
Then using Cauchy Schwarz inequality, \eqref{finthm1}, \eqref{finthm11}, \eqref{finthm2}, substituting $s=S$ in \eqref{c0_delta_ballbound} and using the facts that $ \frac{t}{J} -1 < S \leq \frac{t}{J} $ we get:
\begin{align}
    \norm{\bW(t) - \overline{\bW}(t)}_F+  \norm{\bW^* - \widehat{\bW}^S(t)}_F  +  \norm{\bW(t) - \widehat{\bW}^S(t)}_F & \leq \nonumber \\   
    & \hspace{-7cm} \sqrt{3d}(\sqrt{M}+1) M \Bigg(d\bigg(   (a_1)^{\frac{t}{J}-1}\xi^1_k(0) + \frac{h}{1-a_1}\bigg(a_2 \sqrt{M} (\sqrt{M} + 1) C_1 \text{diam}(\cK_1) + a_2 \bigg)  \nonumber \\
 & \hspace{-3cm} + (a_3)^{\frac{t}{J}-1}\xi^5_k(0) + \frac{h}{1-a_3}\bigg(a_4 \sqrt{M} (\sqrt{M} + 1) C_1 \text{diam}(\cK_1) + a_4 \Delta\bigg) \bigg) + \nonumber \\
 & \hspace{-3cm} (1-\mu h)^{{\frac{t}{J}-1}-S_0}\xi^6(S_0)  + \frac{C_0}{\mu} + \frac{L \sqrt{Md}}{\mu}\bigg((a_1)^{S_0}\xi^1_k(0) \nonumber \\ & \hspace{-3cm} + \frac{h}{1-a_1}\bigg(a_2 \sqrt{M} (\sqrt{M} + 1) C_1 \text{diam}(\cK_1) + a_2 \Delta\bigg)\bigg) \Bigg) ,
\end{align} 
    where $S > S_0$, $a_1 = M^{\frac{3}{2}}(\sqrt{M}+1)( 1-\beta^{\tau M} )^{\left\lfloor\frac{(J-2)}{\tau M}\right\rfloor} <1 $, $a_3 = M^{\frac{3}{2}}( 1-\beta^{\tau M} )^{\left\lfloor\frac{(J-2)}{\tau M}\right\rfloor} <1$, $a_2 =(\sqrt{M}+1)^2 L$, $a_4 = L$. Last, taking $t \to \infty$ and $S_0 \to \infty$ in the above inequality we get:
\begin{align}
  \limsup_{t \to \infty} \bigg( \norm{\bW(t) - \overline{\bW}(t)}_F+  \norm{\bW^* - \widehat{\bW}^S(t)}_F  +  \norm{\bW(t) - \widehat{\bW}^S(t)}_F \bigg) & \leq \nonumber \\   
    & \hspace{-10cm} \sqrt{3d}(\sqrt{M}+1) M \Bigg(  \frac{hd}{1-a_1}\bigg(a_2 \sqrt{M} (\sqrt{M} + 1) C_1 \text{diam}(\cK_1) + a_2 \bigg) \nonumber \\ & \hspace{-9cm} +  \frac{hd}{1-a_3}\bigg(a_4 \sqrt{M} (\sqrt{M} + 1) C_1 \text{diam}(\cK_1) + a_4 \Delta \bigg)    + \frac{C_0}{\mu} \nonumber \\
 & \hspace{-9cm} + \bigg(\frac{L \sqrt{Md}}{\mu} \frac{h}{1-a_1}\bigg(a_2 \sqrt{M} (\sqrt{M} + 1) C_1 \text{diam}(\cK_1) + a_2 \Delta\bigg)\bigg) \Bigg) ,
\end{align} 
which completes the proof.
\end{proof}

\section{Rates Under Nonconvexity}\label{appendixD}

\subsection{An example in $\mathbb{R}^2$ where sum of P{\L} functions does not satisfy the P{\L} inequality}\label{PL example}
Let $$f(x,y) = \frac{1}{2}(y-\sin(x))^2,$$ $$g(x,y) = \frac{1}{4}(y-3-\sin(x-3))^2.$$ Then $f$ is a P{\L} function from \cite{apidopoulos2022convergence} whose critical set is given by $\{(x,y) : y =\sin(x)\}$ (see Figure 1 in \cite{apidopoulos2022convergence}). Similarly, $g(x,y) = \frac{1}{2}f(x-3,y-3)$ is obtained from translation and scaling of $f(x,y)$ and hence it is also a P{\L} function. However, $f+g$ has saddle points in its landscape (see Figure \ref{fig:enter-label_geogeb}), and therefore, it cannot be a P{\L} function (for a function to satisfy P{\L} inequality, it must not have any saddle points).

\begin{figure}[h]
    \centering
    \includegraphics[width=0.5\linewidth]{geogebra_export.png}
    \caption{Graph of $f(x,y)+g(x,y)$}
    \label{fig:enter-label_geogeb}
\end{figure}


\subsection{Proof of Lemma \ref{pl_lem}}\label{pl_lemproof}
\begin{proof}
    Recall that from the inexact averaged update in Lemma \ref{supportlem_inexactrule_007}, we have 
    \begin{align}
    \widehat{\bw}^{s+1}(s+1) &= \widehat{\bw}^{s}(s) - h \nabla f (\widehat{\bw}^{s}(s)) + \be_1(s) + \be_2(s) , \label{stackvec2*}
\end{align}
where 
\begin{align}
    \norm{\be_2(s)} & \leq  Lh \sqrt{Md}\sum\limits_{k=1}^d \norm{[\widehat{\bW}^{k,s}(s)]_k - [{\bW}(s)]_k}.
\end{align}
Since $f := \frac{1}{M}\sum_{i=1}^{M} f_i$ satisfies the P{\L} inequality from Assumption \ref{pl_assumption} and also Assumption \ref{asumpt1_nonconvex}, we get that:
\begin{align}
    f( \widehat{\bw}^{s}(s) - h \nabla f (\widehat{\bw}^{s}(s)) ) &\leq f(\widehat{\bw}^{s}(s))  + \langle \nabla f(\widehat{\bw}^{s}(s)), -h \nabla f(\widehat{\bw}^{s}(s)) \rangle + \frac{L}{2} \norm{ h \nabla f (\widehat{\bw}^{s}(s)) }^2 \\
    & = f(\widehat{\bw}^{s}(s)) - \frac{h(2-Lh)}{2} \norm{  \nabla f (\widehat{\bw}^{s}(s)) }^2 \\
    & \leq  f(\widehat{\bw}^{s}(s)) - {\mu h(2-Lh)} (f(\widehat{\bw}^{s}(s)) - f^*).
\end{align}
For $0 < h < \frac{2}{L} $, we will have $\mu h(2-Lh) < 1 $ and hence from the last inequality we have
\begin{align}
     f( \widehat{\bw}^{s}(s) - h \nabla f (\widehat{\bw}^{s}(s)) ) - f^* & \leq \bigg(1 - {\mu h(2-Lh)}\bigg) (f(\widehat{\bw}^{s}(s)) - f^*) \\
     \implies f(\widehat{\bw}^{s+1}(s+1)) - f^* & \leq \bigg(1 - {\mu h(2-Lh)}\bigg) (f(\widehat{\bw}^{s}(s)) - f^*) + \nonumber \\ &\hspace{2cm} \bigg(f(\widehat{\bw}^{s+1}(s+1))  -  f( \widehat{\bw}^{s}(s) - h \nabla f (\widehat{\bw}^{s}(s)) ) \bigg) .\label{temp1_pl}
\end{align}
From Lemma \ref{pl_lem}, for some sufficiently large compact set $\cK$ defined and Assumption \ref{asumpt1_nonconvex}, we have that $\sup_{\bw \in \cK} \norm{\nabla f(\bw)} \leq L \hspace{0.1cm} \text{diam} (\cK)$. Then from Mean value theorem, the function $f$ is locally Lipschitz continuous in $\cK$ and for any $\bw_1, \bw_2 \in \cK$ we have:
\begin{align}
    f(\bw_1) - f(\bw_2) &\leq  L \hspace{0.1cm} \text{diam} (\cK) \norm{\bw_1 -\bw_2}. \label{locallipsc}
\end{align}
Then using \eqref{locallipsc} in \eqref{temp1_pl} along with the update \eqref{stackvec2*} and bound on $ \norm{\be_2(s)}$ we have:
\begin{align}
     f(\widehat{\bw}^{s+1}(s+1)) - f^* & \leq \bigg(1 - {\mu h(2-Lh)}\bigg) (f(\widehat{\bw}^{s}(s)) - f^*) + \nonumber \\ &  L \hspace{0.1cm} \text{diam} (\cK) \norm{\widehat{\bw}^{s+1}(s+1) -(\widehat{\bw}^{s}(s) - h \nabla f (\widehat{\bw}^{s}(s)))} \\
     \implies f(\widehat{\bw}^{s+1}(s+1)) - f^* & \leq \bigg(1 - {\mu h(2-Lh)}\bigg) (f(\widehat{\bw}^{s}(s)) - f^*) +  L \hspace{0.1cm} \text{diam} (\cK) \bigg(\norm{\be_1(s)} + \norm{\be_2(s)}\bigg) \\
     & \leq \bigg(1 - {\mu h(2-Lh)}\bigg) (f(\widehat{\bw}^{s}(s)) - f^*) +  \nonumber \\ & \hspace{2cm}  L \hspace{0.1cm} \text{diam} (\cK) \bigg(\norm{\be_1(s)} + Lh \sqrt{Md}\sum\limits_{k=1}^d \norm{[\widehat{\bW}^{k,s}(s)]_k - [{\bW}(s)]_k}\bigg),
\end{align}
which completes the proof.
\end{proof}

\subsection{Proof of Theorem \ref{plrate_theo}}\label{plrate_theoproof}
\begin{proof}
  Under Assumption \ref{pl_assumption} suppose $\bw_i^* \in \argmin_{\bw} f_i(\bw)$ for all $i \in \{1,\cdots, M\}$ and without loss of generality $ \{\bw_i^*\}_{i=1}^M \subset \cK$. Then it can be easily checked that the consensus error bounds for the sequences $ \{\xi_k^1(s)\}_s,  \{\xi_k^5(s)\}_s$ will be exactly the same as in Theorem~\ref{inexactlmigeo} since these bounds were derived without any convexity assumption (see Appendix \ref{inexactlmigeoproof} for proof of Theorem \ref{inexactlmigeo}). Then recalling the consensus error bounds \eqref{consensuserr_1*}, \eqref{consensuserr_2*} from proof of Theorem \ref{inexactlmigeo} we get :
    \begin{align}
     \xi^1_k(S)  &\leq  (a_1)^S\xi^1_k(0) + \frac{h}{1-a_1}\bigg(a_2 \sqrt{M} (\sqrt{M} + 1) C_1 \text{diam}(\cK) + a_2 \Delta\bigg) ,\\
     \xi^5_k(S)  &\leq  (a_3)^S\xi^5_k(0) + \frac{h}{1-a_3}\bigg(a_4 \sqrt{M} (\sqrt{M} + 1) C_1 \text{diam}(\cK) + a_4 \Delta\bigg) ,
\end{align}
where $a_1 = M^{\frac{3}{2}}(\sqrt{M}+1)( 1-\beta^{\tau M} )^{\left\lfloor\frac{(J-2)}{\tau M}\right\rfloor} <1 $, $a_3 = M^{\frac{3}{2}}( 1-\beta^{\tau M} )^{\left\lfloor\frac{(J-2)}{\tau M}\right\rfloor} <1 $ and $\Delta$ is defined in Lemma \ref{lemmarecursion101}. For deriving the function error sequence rates, we use Lemmas \ref{tkhatlemma}, \ref{lemxi1}, and \ref{pl_lem}. Using Lemma \ref{tkhatlemma} followed by Jensen's inequality and Assumption \ref{boundedassump} we have that:
     \begin{align}
           \norm{[\widehat{\bT}^{k,s}(s)]_k- [{\bT}(s)]_k} & \leq    (\sqrt{M} + 1)L\sqrt{M}{\sum\limits_{k=1}^d \norm{ [{\bW}(s)]_k- [\widehat{\bW}^{k,s}(s)]_k }} + \nonumber \\ & \hspace{2cm}   (\sqrt{M} + 1)LM \norm{ \bw^*-\widehat{\bw}^s(s) } + (\sqrt{M} + 1)L\sum\limits_{i=1}^M \norm{ \bw^*- \bw_i^* } \\
            & \leq    (\sqrt{M} + 1)L\sqrt{M d}\underbrace{\norm{ {\bW}(s)-\widehat{\bW}^{k,s}(s) }_F}_{=\sqrt{\sum\limits_{i=1}^M \norm{ {\bw_i}(s)- \widehat{\bw}^{k,s}(s) }}} + \nonumber \\ &   \hspace{2cm}(\sqrt{M} + 1)LM \norm{ \bw^*-\widehat{\bw}^s(s) } + (\sqrt{M} + 1)L\sum\limits_{i=1}^M \norm{ \bw^*- \bw_i^* } \\
              & \leq    (\sqrt{M} + 1)LM(\sqrt{ d}+2) \hspace{0.1cm} \text{diam} (\cK). \label{plbound1}
    \end{align}
      Then from Lemma \ref{lemxi1}, \eqref{plbound1} and Assumption \ref{boundedassump} we have for any $S >0$ :
\begin{align}
      \norm{ [\widehat{\bW}^{k,S}(S)]_k - [{\bW}(S)]_k  }  & \leq   (a_1)^{S} \norm{ [\widehat{\bW}^{k,0}(0)]_k - [{\bW}(0)]_k  }  + \frac{h(\sqrt{M}+1) }{1- a_1} \sup_{s \geq 0}\norm{[\widehat{\bT}^{k,s}(s)]_k- [{\bT}(s)]_k}\\
      & \leq   (a_1)^{S} \norm{ [\widehat{\bW}^{k,0}(0)]_k - [{\bW}(0)]_k  }  + \frac{h(\sqrt{M}+1)^2}{1- a_1}  LM(\sqrt{ d}+2) \hspace{0.1cm} \text{diam} (\cK) \\
       & \leq   (a_1)^{S} \norm{ \widehat{\bW}^{k,0}(0) - {\bW}(0)  }_F  + \frac{h(\sqrt{M}+1)^2}{1- a_1}  LM(\sqrt{ d}+2) \hspace{0.1cm} \text{diam} (\cK) \\
      & \leq   (a_1)^{S} M \text{diam} (\cK)  + \frac{h(\sqrt{M}+1)^2}{1- a_1}  LM(\sqrt{ d}+2) \hspace{0.1cm} \text{diam} (\cK), \label{plbound2}
    \end{align}
    where $a_1 < 1$.
    Substituting the above bound \eqref{plbound2} in Lemma \ref{pl_lem} for $s=S \geq 0$ and using the following bound from \eqref{e1errorbound} given by $$ \norm{\be_1(s)} \leq h\sup_{s \geq 0}\gamma(s) = h\sup_{s \geq 0}\sum\limits_{k=1}^d\lvert \nabla_k f(\widehat{\bw}^{s}(s))  -  \nabla_k f^{k,s+1}(\widehat{\bw}^{s}(s)) \rvert = C_0 h, $$ we have:
      \begin{align}
        f(\widehat{\bw}^{S+1}(S+1)) - f^*  & \leq \bigg(1 - {\mu h(2-Lh)}\bigg) (f(\widehat{\bw}^{S}(S)) - f^*) +  \nonumber \\ & \hspace{-2cm} L \hspace{0.1cm} \text{diam} (\cK) \bigg(h C_0 + Lh d\sqrt{Md}\bigg( (a_1)^{S} M \text{diam} (\cK)  + \frac{h(\sqrt{M}+1)^2}{1- a_1}  LM(\sqrt{ d}+2) \hspace{0.1cm} \text{diam} (\cK)\bigg)  \bigg) \\  
        \implies  f(\widehat{\bw}^{S+1}(S+1)) - f^*  & \leq \bigg(1 - {\mu h(2-Lh)}\bigg)^{S+1} (f(\widehat{\bw}^{0}(0)) - f^*) +   L \hspace{0.1cm} \text{diam} (\cK) \frac{ C_0}{\mu(2-Lh)} +  \nonumber \\ & \hspace{2cm} L \hspace{0.1cm} \text{diam} (\cK) \bigg(  \frac{Lh d\sqrt{Md}(\sqrt{M}+1)^2}{(1- a_1)(\mu(2-Lh))}  LM(\sqrt{ d}+2) \hspace{0.1cm} \text{diam} (\cK) \nonumber \\ & \hspace{2cm}+ Lh d\sqrt{Md}\bigg( \sum\limits_{s=0}^{S} (a_1)^{s} \underbrace{(1- \mu h(2-Lh))^{S-s}}_{\leq 1} M \text{diam} (\cK)  \bigg)  \bigg) \\
        & \leq \bigg(1 - {\mu h(2-Lh)}\bigg)^{S+1} (f(\widehat{\bw}^{0}(0)) - f^*) +   L \hspace{0.1cm} \text{diam} (\cK) \frac{ C_0}{\mu(2-Lh)} +  \nonumber \\ & \hspace{2cm} L \hspace{0.1cm} \text{diam} (\cK) \bigg(  \frac{Lh d\sqrt{Md}(\sqrt{M}+1)^2}{(1- a_1)(\mu(2-Lh))}  LM(\sqrt{ d}+2) \hspace{0.1cm} \text{diam} (\cK) \nonumber \\ & \hspace{2cm}+ \frac{ Lh d\sqrt{Md}}{1-a_1} M \text{diam} (\cK)  \bigg) \\
       \implies  f(\widehat{\bw}^{S}(S)) - f^*    & \leq \bigg(1 - {\mu h(2-Lh)}\bigg)^{S} (f(\widehat{\bw}^{0}(0)) - f^*) +   L \hspace{0.1cm} \text{diam} (\cK) \frac{ C_0}{\mu(2-Lh)} +  \nonumber \\ & \hspace{2cm}  \frac{ L^2h d\sqrt{Md}}{1-a_1} \hspace{0.1cm} (\text{diam} (\cK))^2 \bigg(  \frac{(\sqrt{M}+1)^2}{\mu(2-Lh)}  LM(\sqrt{ d}+2) +  M   \bigg) ,
    \end{align}
    which completes the proof.
\end{proof}


\subsection{Proof of Theorem \ref{nonconvexrate_theo}}\label{nonconvexrate_theoproof}
\begin{proof}
Recalling the bound \eqref{lmi1} from Lemma \ref{lemxi1} and Lemma \ref{tkhatlemma} we have for $h := h(s) = \frac{p}{(s+1)^{\omega}}$, $p>0$ that:
\begin{align}
    \xi^1_k(s+1) &\leq  \underbrace{M^{\frac{3}{2}}(\sqrt{M}+1)( 1-\beta^{\tau M} )^{\left\lfloor\frac{(J-2)}{\tau M}\right\rfloor} }_{=a_1}\xi^1_k(s) +   {h(s)(\sqrt{M} + 1)^2 L\sqrt{M}}{\sum\limits_{k=1}^d \xi^1_k(s)}  \nonumber \\ & \hspace{2cm}+  { h(s)(\sqrt{M} + 1)^2 L M }\xi^6(s)  + { h(s)(\sqrt{M} + 1)^2 L}\underbrace{\sum\limits_{i=1}^M \norm{ \bw^*- \bw_i^* }}_{=\Delta}  .
\end{align}
$\clubsuit$  Using Assumption \ref{boundedassump} in the last step, we can bound $$ \max\bigg\{\Delta,\sup_{s \geq 0}\sum\limits_{k=1}^d\xi^1_k(s), \sup_{s \geq 0}\xi^6(s)  \bigg\} \leq C( M, d) \text{diam}(\cK)$$ for some sufficiently large constant\footnote{Observe that $\Delta = \cO(M \text{diam}(\cK))$, $\xi^6(s) = \cO(\text{diam}(\cK))$ and $ \sum\limits_{k=1}^d\xi^1_k(s) = \cO(\sqrt{Md} \text{ diam}(\cK))$.} $C( M, d) = \cO( M \sqrt{d})$ to get:
\begin{align}
    \xi^1_k(s+1) &\leq a_1 \xi^1_k(s) +   C( M, d) \text{diam}(\cK)  h(s),\\
\implies \xi^1_k(S) &\leq (a_1)^S \xi^1_k(0) +  C( M, d) \text{diam}(\cK)  \sum_{s=0}^{S-1} (a_1)^{S-s-1} h(s) \label{nonconvextemp*1}  \\
\implies \limsup_{S \to \infty}  \xi^1_k(S) &\leq  \limsup_{S \to \infty}  (a_1)^S \xi^1_k(0) + C( M, d) \text{diam}(\cK)  \limsup_{S \to \infty}   \sum_{s=0}^{S-1} (a_1)^{S-s-1} h(s) =0 \\
\implies  \xi^1_k(S) & \xrightarrow{S \to \infty} 0.
\end{align}
Note that in the second last step, we used the fact that $a_1 < 1$ and that the partial sum $ \sum_{s=0}^{S-1} (a_1)^{S-s-1} h(s)  $ is monotonically decreasing in $S$ after any sufficiently large $S$ from the argument below
\begin{align}
    \sum_{s=0}^{S-1} (a_1)^{S-s-1} h(s) &> \sum_{s=0}^{S} (a_1)^{S+ 1-s-1} h(s) \nonumber \\ & = a_1\bigg(\sum_{s=0}^{S-1} (a_1)^{S-s-1} h(s)\bigg) +  (a_1)^{S+ 1-S-1} h(S) \label{partialsumnonconvex} \\
    \impliedby  (1-a_1) \sum_{s=0}^{S-1} (a_1)^{S-s-1} h(s)  & > h(S) = \frac{p}{(S+1)^{\omega}} \\
    \impliedby    \frac{p}{S^{\omega}} (1-(a_1)^S) & > \frac{p}{(S+1)^{\omega}}  \\
    \impliedby   1 + \omega S^{-1} + o(S^{-1})  & > 1 + (a_1)^S + o((a_1)^S)  \text{ for any } \omega > 0 \text{ and } S > 1 .
\end{align}
Then by Monotone Convergence Theorem\footnote{The partial sum $ \sum_{s=0}^{S-1} (a_1)^{S-s-1} h(s)$ is non-negative and decreasing for large $S$.}, taking limit in \eqref{partialsumnonconvex}, we get that the partial sum $ \sum_{s=0}^{S-1} (a_1)^{S-s-1} h(s)$ converges to $0$. 
\\
In particular, we have a decay rate of $\cO(\frac{1}{S^{\omega}})$ from the following bound:
\begin{align}
    \sum_{s=0}^{S-1} (a_1)^{S-s-1} h(s) & =  \sum_{s=0}^{\lfloor\frac{S}{2} \rfloor} (a_1)^{S-s-1} h(s) + \sum_{s=\lfloor\frac{S}{2} \rfloor + 1}^{S-1} (a_1)^{S-s-1} h(s) \\
    & \leq h(0)\sum_{s=0}^{\lfloor\frac{S}{2} \rfloor} (a_1)^{S-s-1}  + h\bigg(\bigg\lfloor\frac{S}{2} \bigg\rfloor + 1\bigg)\sum_{s=\lfloor\frac{S}{2} \rfloor + 1}^{S-1} (a_1)^{S-s-1} \\
   & \leq  (a_1)^{S -\lfloor\frac{S}{2} \rfloor -1} \frac{p}{1-a_1} + \frac{p}{(\lfloor\frac{S}{2} \rfloor + 2)^{\omega}} \frac{1}{1-a_1} \\
   & \underbrace{\leq}_{\text{for any sufficiently large } S} \frac{2p}{(1-a_1)(\lfloor\frac{S}{2} \rfloor + 2)^{\omega}} 
 = \frac{C_5}{S^{\omega}}.\label{partialsumnonconvexbound1*}   
\end{align}
Then by \eqref{nonconvextemp*1} and \eqref{partialsumnonconvexbound1*} we have that:
\begin{align}
     \xi^1_k(S) &= \cO\bigg(\frac{1}{S^{\omega}}\bigg).
\end{align}
$\spadesuit$

Similarly, recalling the bound \eqref{lmi2} from Lemma \ref{wkbarlemma} and Lemma \ref{tkhatlemma} we get for $h := h(s) = \frac{p}{(s+1)^{\omega}}$ that :
\begin{align}
     \xi^5_k(s+1)     &\leq  \underbrace{M^{\frac{3}{2}}( 1-\beta^{\tau M} )^{\left\lfloor\frac{(J-2)}{\tau M}\right\rfloor}}_{=a_3} \xi^5_k(s)  +   {h(s) L\sqrt{M}}{\sum\limits_{k=1}^d \xi^1_k(s)} +    { h(s) L M }\xi^6(s) + { h(s) L}\underbrace{\sum\limits_{i=1}^M \norm{ \bw^*- \bw_i^* }}_{=\Delta}.
\end{align}
    Then, the following similar steps as before from symbol $ \clubsuit$ to symbol $\spadesuit$ and using the fact that $a_3 <1 $, we get that 
    \begin{align}
     \xi^5_k(S) & \xrightarrow{S \to \infty} 0.
\end{align}
Next, recall from the inexact averaged update of Lemma~\ref{supportlem_inexactrule_007} we have for $h := h(s)$ that 
    \begin{align}
    \widehat{\bw}^{s+1}(s+1) &= \widehat{\bw}^{s}(s) - h (s)\nabla f (\widehat{\bw}^{s}(s)) + \be_2(s) + \be_1(s) , \label{stackvec2**}
\end{align}
where\footnote{Since the bound on $ \norm{\be_2(s)}$ from Lemma~\ref{supportlem_inexactrule_007} is derived by using just a single update step for $\widehat{\bw}^{s}(s) $, without loss of generality, we can substitute $h := h(s)$ in the right hand side of the bound on $ \norm{\be_2(s)}$.} 
\begin{align}
    \norm{\be_2(s)} & \leq  Lh(s) \sqrt{Md}\sum\limits_{k=1}^d \norm{[\widehat{\bW}^{k,s}(s)]_k - [{\bW}(s)]_k} \underbrace{=}_{\text{Definition } \ref{deferrorseq}} Lh(s) \sqrt{Md}\sum\limits_{k=1}^d  \xi^1_k(s) ,
\end{align}
 and $$ \norm{\be_1(s)} \leq h(s)\sup_{s \geq 0}\gamma(s) = h(s)\sup_{s \geq 0}\sum\limits_{k=1}^d\lvert \nabla_k f(\widehat{\bw}^{s}(s))  -  \nabla_k f^{k,s+1}(\widehat{\bw}^{s}(s)) \rvert = C_0 h(s), $$
from \eqref{e1errorbound} after substituting $h := h(s)$. Using Assumption \ref{asumpt1_nonconvex} of gradient Lipschitz continuity on $f$ followed by Assumption \ref{boundedassump} on the update \eqref{stackvec2**} for a compact $\cK$ we have that :
\begin{align}
    f(\widehat{\bw}^{s}(s) ) - f(\widehat{\bw}^{s+1}(s+1) ) & \geq \langle \nabla f(\widehat{\bw}^{s}(s) ),\widehat{\bw}^{s}(s)  -\widehat{\bw}^{s+1}(s+1)  \rangle - \frac{L}{2} \norm{\widehat{\bw}^{s}(s)  -\widehat{\bw}^{s+1}(s+1)}^2 \\
& \geq h(s)\norm{\nabla f(\widehat{\bw}^{s}(s) )}^2 - \underbrace{\norm{\nabla f(\widehat{\bw}^{s}(s) )}}_{\leq L \text{ diam}(\cK)} (\norm{\be_2(s)+ \be_1(s)}) \nonumber \\ &  -  \frac{2L(h(s))^2}{2}\norm{\nabla f(\widehat{\bw}^{s}(s) )}^2  - \frac{2L}{2}(\norm{\be_2(s) + \be_1(s)}^2) \\
& \hspace{-1.2cm}\geq h(s)\bigg(1-L h(s)\bigg)\norm{\nabla f(\widehat{\bw}^{s}(s) )}^2 - L \text{diam}(\cK)h(s)\bigg(C_0  +L \sqrt{Md}\sum\limits_{k=1}^d  \xi^1_k(s)\bigg) \nonumber \\ &  - {L(h(s))^2}\bigg(C_0  +L \sqrt{Md}\sum\limits_{k=1}^d  \xi^1_k(s)\bigg)^2. \label{nonconvexineq1}
\end{align}
Next, for some constant $C_2 = C(L,M,d,\text{diam}(\cK))$, using Assumption \ref{boundedassump} we can bound
\begin{align}
    \sup_{s \geq 0}{L}\bigg(C_0  +L \sqrt{Md}\sum\limits_{k=1}^d  \xi^1_k(s)\bigg)^2 \leq C(L,M,d,\text{diam}(\cK)) = C_2 = \cO\bigg(L^3\bigg(Md \text{ diam}(\cK)\bigg)^2\bigg) . \label{C2boundnonconvex}
\end{align}
It must be noted that $C_0 = \cO(LMd \text{ diam}(\cK))$ from a simple application of gradient Lipschitz continuity. Recall that $$C_0 =\sup_{s \geq 0}\sum\limits_{k=1}^d\lvert \nabla_k f(\widehat{\bw}^{s}(s))  -  \nabla_k f^{k,s+1}(\widehat{\bw}^{s}(s)) \rvert,$$ and hence
\begin{align}
   C_0 & \leq  \sup_{s \geq 0} \sum\limits_{k=1}^d \bigg(\lvert \nabla_k f(\widehat{\bw}^{s}(s))  - \nabla_k f(\bw^*) \rvert +  \sum_{j=1}^M\lvert \nabla_k f_j(\bw_j^*)  -  \nabla_k f_j(\widehat{\bw}^{s}(s)) \rvert \bigg) \leq \cO(LMd \text{ diam}(\cK)) \\
   \implies & C_0  +L \sqrt{Md}\sum\limits_{k=1}^d  \xi^1_k(s)  \leq \cO(LMd \text{ diam}(\cK)).
\end{align}
Then using the constant $C_2$ from \eqref{C2boundnonconvex} in the last term on right hand side of inequality \eqref{nonconvexineq1}, followed by rearranging, telescoping and finally using $0 < p \leq \frac{1}{2L}$ we get:
\begin{align}
h(s)(1-L h(s))\norm{\nabla f(\widehat{\bw}^{s}(s) )}^2  & \leq  f(\widehat{\bw}^{s}(s) ) - f(\widehat{\bw}^{s+1}(s+1) ) +  {C_2( h(s))^2} \nonumber \\ &  \hspace{2cm}+ L \text{diam}(\cK)h(s)\bigg(C_0  +L \sqrt{Md}\sum\limits_{k=1}^d  \xi^1_k(s)\bigg) \\
\implies \sum_{s=0}^{S-1} \bigg( h(s)(1-L h(s))\norm{\nabla f(\widehat{\bw}^{s}(s) )}^2 \bigg) & \leq f(\widehat{\bw}^{0}(0) ) -  f(\widehat{\bw}^{S}(S) ) + {C_2 \sum_{s=0}^{S-1} ( h(s))^2} \nonumber \\
& \hspace{2cm}+ L \text{diam}(\cK)C_0  \sum_{s=0}^{S-1}h(s) \nonumber \\ & \hspace{2cm}+  L^2 \text{diam}(\cK)  \sqrt{Md} \bigg(\sum\limits_{k=1}^d  \sum_{s=0}^{S-1}\xi^1_k(s)   h(s) \bigg) \\
\implies \min_{0 \leq s \leq S-1}\norm{\nabla f(\widehat{\bw}^{s}(s) )}^2 \sum_{s=0}^{S-1} \bigg( h(s)\underbrace{(1-L h(s))}_{\geq \frac{1}{2} \text{ for } p \leq \frac{1}{2L}}  \bigg) & \leq f(\widehat{\bw}^{0}(0) ) -  f(\widehat{\bw}^{S}(S) ) + {C_2 \sum_{s=0}^{S-1} ( h(s))^2} \nonumber \\
&\hspace{2cm}+ L \text{diam}(\cK)C_0  \sum_{s=0}^{S-1}h(s) \nonumber \\ & \hspace{2cm}+  L^2 \text{diam}(\cK)  \sqrt{Md} \bigg(\sum\limits_{k=1}^d  \sum_{s=0}^{S-1}\xi^1_k(s)   h(s) \bigg) \\
\implies \frac{1}{2}\min_{0 \leq s \leq S-1}\norm{\nabla f(\widehat{\bw}^{s}(s) )}^2 \sum_{s=0}^{S-1} h(s)   & \leq f(\widehat{\bw}^{0}(0) ) -  f(\widehat{\bw}^{S}(S) ) + {C_2 \sum_{s=0}^{S-1} ( h(s))^2} \nonumber \\
&\hspace{2cm}+ L \text{diam}(\cK)C_0  \sum_{s=0}^{S-1}h(s) \nonumber \\ &\hspace{2cm} +  L^2 \text{diam}(\cK)  \sqrt{Md} \bigg(\sum\limits_{k=1}^d  \sum_{s=0}^{S-1}\xi^1_k(s)   h(s) \bigg)
\end{align}
which, after rearranging yields:
\begin{align}
     \min_{0 \leq s \leq S-1}\norm{\nabla f(\widehat{\bw}^{s}(s) )}^2  & \leq \frac{2\bigg(f(\widehat{\bw}^{0}(0) ) -  f(\widehat{\bw}^{S}(S) )\bigg)}{\sum_{s=0}^{S-1} h(s)}  + {2C_2  \frac{\sum_{s=0}^{S-1} ( h(s))^2}{\sum_{s=0}^{S-1} h(s)}} \nonumber \\
&+ 2 L \text{diam}(\cK)C_0  + 2 L^2 \text{diam}(\cK)  \sqrt{Md} \underbrace{\frac{\bigg(\sum\limits_{k=1}^d  \sum_{s=0}^{S-1}\xi^1_k(s)   h(s) \bigg)}{\sum_{s=0}^{S-1} h(s)} }_{T_1}. \label{nonconvextemp*2}
\end{align}
Using the bound on $ \xi^1_k(s)$ from \eqref{nonconvextemp*1} and from Lemma \ref{pl_lem} that $ \max_{1 \leq k\leq d}\xi^1_k(0) \leq C_3 \text{diam}(\cK) $ for some constant\footnote{Note that $C_3 = \cO(1)$ provided $\cK$ contains some sufficiently large cube in $\mathbb{R}^d$.} $C_3$ from Assumption \ref{boundedassump} followed by H\"older inequality (Lemma \ref{holderlemma}), the term $T_1$ in \eqref{nonconvextemp*2} can be bounded as:
\begin{align}
 T_1 &= \sum\limits_{k=1}^d \frac{\bigg(  \sum_{s=0}^{S-1}\xi^1_k(s)   h(s) \bigg)}{\sum_{s=0}^{S-1} h(s)}  \leq  \frac{d\bigg(  \sum_{s=0}^{S-1} \bigg((a_1)^s C_3 \text{diam}(\cK) +  C_2 \text{diam}(\cK)  \sum_{l=0}^{s-1} (a_1)^{s-l-1} h(l) \bigg)   h(s) \bigg)}{\sum_{s=0}^{S-1} h(s)}  \\
     & =  \frac{d\bigg(  {\sum_{s=0}^{S-1} (a_1)^s   h(s)} C_3 \text{diam}(\cK)   \bigg)}{\sum_{s=0}^{S-1} h(s)} \nonumber   + \frac{d\bigg( C_2 \text{diam}(\cK)   \sum_{s=0}^{S-1} \bigg( \sum_{l=0}^{s-1} (a_1)^{s-l-1} h(l) \bigg)   h(s) \bigg)}{\sum_{s=0}^{S-1} h(s)} \\
      & \hspace{-0.3cm} \underbrace{\leq}_{\text{H\"older inequality}}  \underbrace{\frac{d C_3 \text{diam}(\cK){\sqrt{\bigg(  \sum_{s=0}^{S-1} (a_1)^{2s}      \bigg)}\sqrt{\bigg(  \sum_{s=0}^{S-1}  (h(s))^2   \bigg)}}}{\sum_{s=0}^{S-1} h(s)} }_{T_4}\nonumber  \\ &  + \underbrace{\frac{d C_2 \text{diam}(\cK)\bigg( {\bigg(\sum_{s=0}^{S-1}  \bigg( (h(s))^{1-a} \sum_{l=0}^{s-1} (a_1)^{s-l-1} h(l) \bigg)^{\frac{q}{q-1}} \bigg)^{1-\frac{1}{q}}}{\bigg(  \sum_{s=0}^{S-1}  (h(s))^{aq}   \bigg)^{\frac{1}{q}}} \bigg)}{\sum_{s=0}^{S-1} h(s)}}_{T_5}, \label{partialsumnonconvexbound2*}
\end{align}
where $a \in (0,1)$ and $q>1$. 

For $h(s) = \frac{p}{(s+1)^{\omega}}$ with $p \in (0 , \frac{1}{2L}]$, we now want to optimize $\omega, a, q$ such that the upper bound in \eqref{nonconvextemp*2} is minimized for any given $S$. Observe that in the first two terms on the right-hand side of \eqref{nonconvextemp*2}, we require the partial sum $ \sum_{s=0}^{S-1} h(s)$ to diverge and $ \sum_{s=0}^{S-1} (h(s))^2$ to converge. But that is only possible for $\omega \in (\frac{1}{2},1]$. We also require the numerator of $T_1$ to converge as $S \to \infty$. From the upper bound \eqref{partialsumnonconvexbound2*} on term $T_1$, the numerator of term $T_4$ given by ${\sqrt{\bigg(  \sum_{s=0}^{S-1} (a_1)^{2s}      \bigg)}\sqrt{\bigg(  \sum_{s=0}^{S-1}  (h(s))^2   \bigg)}}$ will converge as $S \to \infty$ for any $\omega \in (\frac{1}{2},1]$. 
Next, we simplify the numerator term in $T_5$. Taking the first numerator term $\sum_{s=0}^{S-1}  \bigg( (h(s))^{1-a} \sum_{l=0}^{s-1} (a_1)^{s-l-1} h(l) \bigg)^{\frac{q}{q-1}} $ in $T_5$, using the bound \eqref{partialsumnonconvexbound1*} for any fixed large enough $S' < (S)$ and any large enough $S$ we get that:
\begin{align}
    \sum_{s=0}^{S-1}  \bigg( (h(s))^{1-a} \sum_{l=0}^{s-1} (a_1)^{s-l-1} h(l) \bigg)^{\frac{q}{q-1}} & \leq  C(S')+ \sum_{s=S'}^{S-1}  \bigg( \frac{p^{(1-a)}}{s^{\omega(1-a)}}\frac{C_5}{s^{\omega}} \bigg)^{\frac{q}{q-1}} \leq C_7\sum_{s=S'}^{S-1}  \bigg( \frac{1}{s^{2\omega - a \omega}} \bigg)^{\frac{q}{q-1}}
\end{align}
and hence the partial sum $\sum_{s=0}^{S-1}  \bigg( (h(s))^{1-a} \sum_{l=0}^{s-1} (a_1)^{s-l-1} h(l) \bigg)^{\frac{q}{q-1}} $ converges if $(2\omega - a \omega)\frac{q}{q-1} > 1 $ or equivalently
\begin{align}
    aq < \frac{1}{\omega}(2q \omega - q +1) .
\end{align}
Also, from \eqref{partialsumnonconvexbound2*} the partial sum $\sum_{s=0}^{S-1}  (h(s))^{aq}  $ of $T_5$ converges if $ {aq}{\omega} > 1$. Hence, we require the following:
\begin{align}
  \frac{1}{\omega}<  aq <    \underbrace{\frac{1}{\omega}(2q \omega - q +1) }_{> \frac{1}{\omega} \text{ for } \omega >\frac{1}{2}},
\end{align}
which can be satisfied for any fixed $ q > 1$ and a fixed $a \in (0,1)$ that depends on $q$ provided $ \omega >\frac{1}{2}$. Hence we get that for any $ \omega \in (\frac{1}{2},1)$ we can always find some $a,q$ such that the numerator terms of $T_4, T_5$ converge and thus can be uniformly bounded for any $S$. Since $ \sum_{s=0}^{S-1}  h(s)$ is maximized as $\omega \downarrow \frac{1}{2}$, from \eqref{partialsumnonconvexbound2*} we get for $ \omega = \frac{1}{2} + \epsilon $ with $0 <\epsilon < 1/2$ that:
\begin{align}
  T_1    &  {\leq}  \frac{d C_4\text{diam}(\cK)}{S^{\frac{1}{2} - \epsilon}} ,
\end{align} 
for some constant\footnote{From \eqref{nonconvextemp*2} and \eqref{C2boundnonconvex} we have $C_4 = \cO\bigg(d C_3 \text{diam}(\cK) + d p C_2 \text{diam}(\cK)\bigg) = \cO\bigg(M^2 (1+p) \bigg(Ld \text{ diam}(\cK)\bigg)^3\bigg) $.} $C_4  = \cO\bigg(M^2 (1+p) \bigg(Ld \text{ diam}(\cK)\bigg)^3\bigg)$ and thus from \eqref{nonconvextemp*2} we get
\begin{align}
     \min_{0 \leq s \leq S-1}\norm{\nabla f(\widehat{\bw}^{s}(s) )}^2  & \leq \frac{\bigg(f(\widehat{\bw}^{0}(0) ) - \inf_{\bw} f(\bw)\bigg)}{p S^{\frac{1}{2} - \epsilon}}  + {  \frac{C_6}{S^{\frac{1}{2} - \epsilon}}} \nonumber \\
&\hspace{2cm}+ 2 L \text{diam}(\cK)C_0  +   \frac{ 2 C_4 L^2 d   \sqrt{Md} (\text{diam}(\cK))^2}{S^{\frac{1}{2} - \epsilon}}, \label{nonconvexfin1*} \\
\implies \limsup_{S \to \infty}  \min_{0 \leq s \leq S-1}\norm{\nabla f(\widehat{\bw}^{s}(s) )}^2  & \leq  2 L \text{diam}(\cK)C_0 
\end{align}
for some constant $C_6 = \cO\bigg(p L^3\bigg(Md \text{ diam}(\cK)\bigg)^2\bigg)$. In the first step of\eqref{nonconvexfin1*}, we used the fact that $f(\widehat{\bw}^{S}(S) ) \geq \inf_{\bw} f(\bw) > -\infty $ by Assumption \ref{asumpt1_nonconvex} and the constant $C_6 = \cO(p C_2)$ from \eqref{nonconvextemp*2}, which completes the proof. 
\end{proof}

\subsection{Proof of Theorem \ref{nonconvexrate_theo_fixedstep}}\label{nonconvexrate_theo_fixedstepproof}
\begin{proof}
    Using \eqref{nonconvextemp*1} from Theorem \ref{nonconvexrate_theo}'s proof for any $0 \leq S' \leq S$, by substituting $ h(s) = \frac{1}{\sqrt{S}}$ for all $0 \leq  s \leq S-1$, we get that:
    \begin{align}
         \xi^1_k(S') &\leq (a_1)^{S'} \xi^1_k(0) +  C( M, d) \text{diam}(\cK)  \sum_{s=0}^{S'-1} (a_1)^{S'-s-1} h(s) \\
         \implies  \xi^1_k(S') &\leq (a_1)^{S'} \xi^1_k(0) +  C( M, d) \text{diam}(\cK) \frac{1}{\sqrt{S}(1-a_1)}, \label{nonconvex_fixedstep_**}
    \end{align}
     where $a_1 = M^{\frac{3}{2}}(\sqrt{M}+1)( 1-\beta^{\tau M} )^{\left\lfloor\frac{(J-2)}{\tau M}\right\rfloor} <1 $ and $ C( M, d) = \cO(M \sqrt{d})$. Similarly, using the bound \eqref{lmi2} from Lemma \ref{wkbarlemma} and Lemma \ref{tkhatlemma} we will get that 
     \begin{align}
        \xi^5_k(S') &\leq (a_3)^{S'} \xi^1_k(0) +  C( M, d) \text{diam}(\cK) \frac{1}{\sqrt{S}(1-a_3)}, 
    \end{align}
   where $a_3 = M^{\frac{3}{2}}( 1-\beta^{\tau M} )^{\left\lfloor\frac{(J-2)}{\tau M}\right\rfloor} <1$. This completes the first part of the proof. 

    For the second part, from \eqref{nonconvexineq1}, for $h(s) = \frac{1}{\sqrt{S}}$, recall that
    \begin{align}
    f(\widehat{\bw}^{s}(s) ) - f(\widehat{\bw}^{s+1}(s+1) ) & \geq \frac{1}{\sqrt{S}}\bigg(1-\frac{L}{\sqrt{S}}\bigg)\norm{\nabla f(\widehat{\bw}^{s}(s) )}^2 - L \text{diam}(\cK)\frac{1}{\sqrt{S}}\bigg(C_0  +L \sqrt{Md}\sum\limits_{k=1}^d  \xi^1_k(s)\bigg) \nonumber \\ &  \hspace{2cm}- {L\bigg(\frac{1}{\sqrt{S}}\bigg)^2}\bigg(C_0  +L \sqrt{Md}\sum\limits_{k=1}^d  \xi^1_k(s)\bigg)^2, \label{nonconvexineq1_fixedstep}
\end{align}
and for some constant $C_2 = C(L,M,d,\text{diam}(\cK))$, using Assumption \ref{boundedassump} and \eqref{C2boundnonconvex} we have the bound
$$ \sup_{s \geq 0}{L}\bigg(C_0  +L \sqrt{Md}\sum\limits_{k=1}^d  \xi^1_k(s)\bigg)^2 \leq C(L,M,d,\text{diam}(\cK)) = C_2 = \cO\bigg(L^3\bigg(Md \text{ diam}(\cK)\bigg)^2\bigg) .$$
Then summing \eqref{nonconvexineq1_fixedstep} from $s=0$ to $S-1$, dividing both sides by $\sqrt{S}$ and using the above bound followed by \eqref{nonconvex_fixedstep_**} we get:
\begin{align}
     f(\widehat{\bw}^{0}(0) ) - f(\widehat{\bw}^{S}(S) ) & \geq  \frac{1}{\sqrt{S}}\bigg(1-\frac{L}{\sqrt{S}}\bigg)\sum_{s=0}^{S-1}\norm{\nabla f(\widehat{\bw}^{s}(s) )}^2 \nonumber \\ & \hspace{2cm}- L \text{diam}(\cK)\frac{1}{\sqrt{S}} \sum_{s=0}^{S-1}\bigg(C_0  +L \sqrt{Md}\sum\limits_{k=1}^d  \xi^1_k(s)\bigg) \nonumber \\ &   \hspace{2cm}- {L\bigg(\frac{1}{\sqrt{S}}\bigg)^2}\sum_{s=0}^{S-1}\bigg(C_0  +L \sqrt{Md}\sum\limits_{k=1}^d  \xi^1_k(s)\bigg)^2 \\
     \implies \frac{f(\widehat{\bw}^{0}(0) ) - f(\widehat{\bw}^{S}(S) )}{\sqrt{S}} & \geq  \frac{1}{{S}}\bigg(1-\frac{L}{\sqrt{S}}\bigg)\sum_{s=0}^{S-1}\norm{\nabla f(\widehat{\bw}^{s}(s) )}^2  \nonumber \\ & \hspace{1cm} - L \text{diam}(\cK)\frac{1}{{S}} \sum_{s=0}^{S-1}\bigg(C_0  +L \sqrt{Md}\sum\limits_{k=1}^d  \xi^1_k(s)\bigg)   - {\frac{1}{\sqrt{S}}\bigg(\frac{1}{\sqrt{S}}\bigg)^2} S C_2 \\
     \implies \frac{1}{{S}}\bigg(1-\frac{L}{\sqrt{S}}\bigg)\sum_{s=0}^{S-1}\norm{\nabla f(\widehat{\bw}^{s}(s) )}^2 & \leq \frac{f(\widehat{\bw}^{0}(0) ) - f(\widehat{\bw}^{S}(S) )}{\sqrt{S}} + L \text{ diam}(\cK) C_0 + {\frac{C_2}{\sqrt{S}}}   \nonumber  \\ & \hspace{0.5cm} +  L^2 \text{ diam}(\cK) \sqrt{Md} \frac{d}{S} \sum\limits_{s=0}^{S-1} \bigg((a_1)^{s} \xi^1_k(0)  +  C( M, d) \text{diam}(\cK) \frac{1}{\sqrt{S}(1-a_1)}\bigg)  \\
     \implies \frac{1}{{S}}\bigg(1-\frac{L}{\sqrt{S}}\bigg)\sum_{s=0}^{S-1}\norm{\nabla f(\widehat{\bw}^{s}(s) )}^2 & \leq \frac{f(\widehat{\bw}^{0}(0) ) - f(\widehat{\bw}^{S}(S) )}{\sqrt{S}} + L \text{ diam}(\cK) C_0 + {\frac{C_2}{\sqrt{S}}}   \nonumber  \\ & \hspace{0.5cm}+  L^2 \text{ diam}(\cK) \sqrt{Md} \frac{d}{S(1-a_1)} \xi^1_k(0)  + (L \text{ diam}(\cK))^2 \sqrt{Md}  \frac{ C( M, d) d}{\sqrt{S}(1-a_1)} 
     \end{align}
     \begin{align}
     \implies \frac{1}{{S}}\sum_{s=0}^{S-1}\norm{\nabla f(\widehat{\bw}^{s}(s) )}^2 & \leq \bigg(1-\frac{L}{\sqrt{S}}\bigg)^{-1}\frac{f(\widehat{\bw}^{0}(0) ) - \inf_{\bw} f(\bw) }{\sqrt{S}} +  \frac{C_9}{\sqrt{S}}   + \bigg(1-\frac{L}{\sqrt{S}}\bigg)^{-1} L \text{ diam}(\cK) C_0,
\end{align}
where $C_9 = \cO(C_2) = \cO\bigg(L^3\bigg(Md \text{ diam}(\cK)\bigg)^2\bigg) $ is a constant that depends on $ L, M, d, \text{diam}(\cK)$ and we used the fact that $ f(\widehat{\bw}^{S}(S) ) \geq \inf_{\bw} f(\bw) > -\infty$ from Assumption \ref{asumpt1_nonconvex}. Finally, $S > L^6(M d\text{ diam}(\cK))^4 $ so that $\frac{C_9}{\sqrt{S}} < 1$ for any large $S$. This completes the proof.
\end{proof}


\section{Statistical Rates and Sample Complexity}\label{appendixE}


Note that from the linearity of expectation and data homogeneity, i.e., the data distribution is $\mathbb{P}$ across all nodes, we have for any $s$:
\begin{align}
{ \mathbb{E}\bigg[\frac{1}{MN} \sum\limits_{i=1}^N \sum\limits_{j=1}^M\nabla_k \ell_j ( \widehat{\bw}^{s}(s); \bz_{ij})\bigg]} =  { \mathbb{E}\bigg[\frac{1}{N} \sum\limits_{i=1}^N \sum\limits_{j=1}^M [\bc_k(s+1)]_j\nabla_k \ell_j ( \widehat{\bw}^{s}(s); \bz_{ij})\bigg]}  . \label{linearexpect1}
\end{align}
The above equality follows from the following definition of conditional expectation:
\begin{align}
 &{ \mathbb{E}\bigg[\frac{1}{N} \sum\limits_{i=1}^N \sum\limits_{j=1}^M [\bc_k(s+1)]_j\nabla_k \ell_j ( \widehat{\bw}^{s}(s); \bz_{ij}) \hspace{0.1cm}\bigg \vert \hspace{0.1cm} \{[\bc_k(s+1)]_j \}_{j \in \{1,\cdots, M\}}\bigg]}    \nonumber \\  & \hspace{3cm}= \frac{1}{N}\sum\limits_{i=1}^N {\sum\limits_{j=1}^M [\bc_k(s+1)]_j} \underbrace{\bigg( { \mathbb{E}\bigg[ \nabla_k \ell_j ( \widehat{\bw}^{s}(s); \bz_{ij}) \hspace{0.1cm} \bigg \vert \hspace{0.1cm} \{[\bc_k(s+1)]_j \}_{j \in \{1,\cdots, M\}}\bigg]} \bigg)}_{= \mathbb{E}\bigg[ \nabla_k f ( \widehat{\bw}^{s}(s); \bz) \hspace{0.1cm} \bigg \vert \hspace{0.1cm} \{[\bc_k(s+1)]_j \}_{j \in \{1,\cdots, M\}}\bigg] \text{ by data homogeneity}} \\
 & \hspace{3cm}= \frac{1}{N}\sum\limits_{i=1}^N {\sum\limits_{j=1}^M [\bc_k(s+1)]_j} \bigg( \mathbb{E}\bigg[ \nabla_k f ( \widehat{\bw}^{s}(s); \bz) \hspace{0.1cm} \bigg \vert \hspace{0.1cm} \{[\bc_k(s+1)]_j \}_{j \in \{1,\cdots, M\}}\bigg] \bigg) \\
  & \hspace{3cm}= \frac{1}{N}\sum\limits_{i=1}^N \bigg( \mathbb{E}\bigg[ \nabla_k f ( \widehat{\bw}^{s}(s); \bz) \hspace{0.1cm} \bigg \vert \hspace{0.1cm} \{[\bc_k(s+1)]_j \}_{j \in \{1,\cdots, M\}}\bigg] \bigg)  \underbrace{\sum\limits_{j=1}^M [\bc_k(s+1)]_j}_{=1} \\
 &  \hspace{3cm}= \frac{1}{N}\sum\limits_{i=1}^N  \mathbb{E}\bigg[ \nabla_k f ( \widehat{\bw}^{s}(s); \bz) \hspace{0.1cm} \bigg \vert \hspace{0.1cm} \{[\bc_k(s+1)]_j \}_{j \in \{1,\cdots, M\}}\bigg] .
\end{align}
Finally, taking total expectation in the last step followed by the data homogeneity across all nodes yields \eqref{linearexpect1} as follows:
\begin{align}
    { \mathbb{E}\bigg[\frac{1}{N} \sum\limits_{i=1}^N \sum\limits_{j=1}^M [\bc_k(s+1)]_j\nabla_k \ell_j ( \widehat{\bw}^{s}(s); \bz_{ij})\bigg]} &=   \mathbb{E}\bigg[ \nabla_k f ( \widehat{\bw}^{s}(s); \bz) \hspace{0.1cm} \bigg] \\
    & = { \mathbb{E}\bigg[\frac{1}{MN} \sum\limits_{i=1}^N \sum\limits_{j=1}^M\nabla_k \ell_j ( \widehat{\bw}^{s}(s); \bz_{ij})\bigg]}.
\end{align}
The rest of the proof in appendix will be divided into three parts: the first part include the proof of sample complexity of the parameter $C_0$ defined in Theorem \ref{inexactlmigeo}; the second part includes the proof of  sample complexity of the parameter $\Delta$ defined in Lemma\ref{lemmarecursion101} along with the proof of Theorem \ref{statisticalconvergencethm}; the last part includes proof of Theorem \ref{statisticalconvergencethm_pl}. 
\subsection{$C_0$ sample complexity:}
\begin{lemm}\label{supsampleco_lem}
    Under Assumptions  \ref{claim2}, \ref{asumpt1_nonconvex}, and \ref{boundedassumpstat} with $N$ i.i.d. samples at each node, for any $\epsilon' \in (0,1)$, for any large enough $N > 1$, $d > \epsilon'$ we have that:
    \begin{align}
      C_0 &<   \cO\bigg(\sqrt{\frac{{L^{'}}^2d^2\|{\balpha}\|^2\log\frac{4}{\delta}}{N}}\bigg) 
\end{align}
with the probability of at least $1-\delta$ where
\begin{align}
    \delta  &=  {2\exp\bigg(-\frac{4M N{(\epsilon')}^2}{16(L^{'})^2 M d^2\|{\balpha}\|^2+{(\epsilon')^2}} + M\log\bigg(\frac{12 L^{'}d\sqrt{M}}{{\epsilon'}}\bigg) +d\log\bigg( \frac{12  L^{'}\Gamma_0{d}}{{\epsilon'}}\bigg)\bigg)} \nonumber \\ &\hspace{2cm} + {2d  \exp{\bigg(- \frac{2 (\epsilon')^2 M N}{( L^{'}d)^2}\bigg)}}  .
\end{align}
\end{lemm}

\begin{proof}
    The gradient samples $ \{\nabla \ell_j ( \bw; \bz_{ij})\}_{i=1}^N$ at each node $j$ for any given $\bw$ are i.i.d. from the fact that $\{\bz_{ij}\}_{i=1}^N $ are i.i.d. and as a result we also get that $ \{[\nabla \ell_j ( \bw; \bz_{ij})]_k\}_{i=1}^N$ are i.i.d. for any coordinate $k$. Since $\widehat{\bw}^{s}(s) \in \cK$ for some compact $\cK$ for all $s$ from Assumption \ref{boundedassumpstat}, it suffices to bound $\sup_{\bw \in \cK} \lvert \nabla_k f(\bw)  - \mathbb{E}[ \nabla_k f(\bw) ]\rvert $. Moreover, from Assumption \ref{boundedassumpstat} we have:
\begin{align}
   \max \bigg\{  \sup_{\bw \in \cK}  \lvert{ \nabla_k \ell_j(\bw; \bz_{ij}) }\rvert ,  \sup_{\bw \in \cK}  \lvert{ \ell_j(\bw; \bz_{ij}) }\rvert \bigg\}  \leq L^{'} ,\hspace{0.5cm}\sup_{\bw \in \cK} \norm{\bw} \leq \Gamma_0 = \text{diam}(\cK)
\end{align}
 for any $k \in  \{1,\cdots,d\}$, any $j \in \{1,\cdots,M\}$ and also for any $ \{\bz_{ij}\}_{i=1}^N \overset{i.i.d.}{\sim} \mathbb{P}$ and any $N \geq 1$. In particular, the constant  $L^{'}$ satisfies $L^{'} = \max\bigg\{\cO(L d\text{ diam}(\cK)),\cO(L (\text{diam}(\cK))^2) \bigg\}$ which can be easily deduced by applying the fundamental theorem of calculus to the function $\ell_j(\cdot)$ in the variable $\bw$.

Next, using union bound over multiple random variable across each dimension followed by Hoeffding's inequality \cite{hoeffding1963probability} for any $ {\epsilon_0} \in (0,1)$:
\begin{align}
    \mathbb{P}\Bigg( \sum_{k=1}^d\sup_{\bw \in \cK}\bigg\lvert {{\frac{1}{MN} \sum\limits_{i=1}^N \sum\limits_{j=1}^M \nabla_k \ell_j ( \bw; \bz_{ij})} - { \mathbb{E}\bigg[\frac{1}{MN} \sum\limits_{i=1}^N \sum\limits_{j=1}^M\nabla_k \ell_j ( \bw; \bz_{ij})\bigg]} }\bigg\rvert \geq \epsilon_0\Bigg) & \leq \nonumber \\
    & \hspace{-10cm}\sum\limits_{k=1}^d  \mathbb{P}\Bigg( \sup_{\bw \in \cK}\bigg\lvert {{\frac{1}{MN} \sum\limits_{i=1}^N \sum\limits_{j=1}^M \nabla_k \ell_j ( \bw; \bz_{ij})} - { \mathbb{E}\bigg[\frac{1}{MN} \sum\limits_{i=1}^N \sum\limits_{j=1}^M\nabla_k \ell_j ( \bw; \bz_{ij})\bigg]} }\bigg\rvert \geq \frac{\epsilon_0}{d}\Bigg) \\
    & \hspace{-6cm} \leq 2\sum\limits_{k=1}^d  \exp{\bigg(- \frac{2 \epsilon_0^2 M N}{( L^{'}d)^2}\bigg)} \\
   & \hspace{-12cm} \implies  \mathbb{P}\Bigg( \sum_{k=1}^d\sup_{\bw \in \cK}\bigg\lvert {{\frac{1}{MN} \sum\limits_{i=1}^N \sum\limits_{j=1}^M \nabla_k \ell_j ( \bw; \bz_{ij})} - { \mathbb{E}\bigg[\frac{1}{MN} \sum\limits_{i=1}^N \sum\limits_{j=1}^M\nabla_k \ell_j ( \bw; \bz_{ij})\bigg]} }\bigg\rvert < \epsilon_0\Bigg)  > \nonumber \\ & \hspace{-3cm}1- 2d  \exp{\bigg(- \frac{2 \epsilon_0^2 M N}{( L^{'}d)^2}\bigg)}.
\end{align}
Then for $\delta_0 =2d  \exp{\bigg(- \frac{2 \epsilon_0^2 M N}{( L^{'}d)^2}\bigg)}$ we get that the following bound holds with the probability of at least $1-\delta_0$:
\begin{align}
  \sup_{s \geq 0}\sum\limits_{k=1}^d\bigg\lvert \nabla_k f(\widehat{\bw}^{s}(s))  -  { \mathbb{E}\bigg[\frac{1}{MN} \sum\limits_{i=1}^N \sum\limits_{j=1}^M\nabla_k \ell_j ( \widehat{\bw}^{s}(s); \bz_{ij})\bigg]} \bigg \rvert &\leq \nonumber \\ & \hspace{-5cm}   \sum_{k=1}^d\sup_{\bw \in \cK}\bigg\lvert {{\frac{1}{MN} \sum\limits_{i=1}^N \sum\limits_{j=1}^M \nabla_k \ell_j ( \bw; \bz_{ij})} - { \mathbb{E}\bigg[\frac{1}{MN} \sum\limits_{i=1}^N \sum\limits_{j=1}^M\nabla_k \ell_j ( \bw; \bz_{ij})\bigg]} }\bigg\rvert \nonumber\\ & < \epsilon_0 =   \sqrt{\log \bigg( \frac{2d}{\delta_0} \bigg)} \frac{L^{'}d}{\sqrt{2MN}} . \label{sampleco_1}
\end{align}


Next, let $\cS_{\bc} = \{\bc_k(s)\}_{s, k=1}^{\infty, d} $ and that\footnote{Though $\balpha$ will depend on the i.i.d. drawn set $\{\bz_{ij}\}_{i=1}^N$ and hence is random, yet it does not impact the end result due to the fact that $\balpha$ is an averaging vector and so $\norm{\balpha}^{-2} \in [1, M]$ by which $ \balpha$ is a uniformly bounded parameter that is independent of $\{\bz_{ij}\}_{i=1}^N$.} $\balpha \in \argmax_{\bq \in \cS_{\bc}} \norm{\bq}$.
Also, let us define 
$$ T_5(s) = \sum_{k=1}^d\bigg\lvert {{\frac{1}{N} \sum\limits_{i=1}^N \sum\limits_{j=1}^M [\bc_k(s+1)]_j\nabla_k \ell_j ( \widehat{\bw}^s(s); \bz_{ij})} - { \mathbb{E}\bigg[\frac{1}{N} \sum\limits_{i=1}^N \sum\limits_{j=1}^M [\bc_k(s+1)]_j\nabla_k \ell_j ( \widehat{\bw}^{s}(s); \bz_{ij})\bigg]} }\bigg\rvert , $$
$$ T_6(s) = \sqrt{\sum_{k=1}^d\bigg\lvert {{\frac{1}{N} \sum\limits_{i=1}^N \sum\limits_{j=1}^M [\bc_k(s+1)]_j\nabla_k \ell_j ( \widehat{\bw}^s(s); \bz_{ij})} - { \mathbb{E}\bigg[\frac{1}{N} \sum\limits_{i=1}^N \sum\limits_{j=1}^M [\bc_k(s+1)]_j\nabla_k \ell_j ( \widehat{\bw}^{s}(s); \bz_{ij})\bigg]} }\bigg\rvert^2}.$$

Then the rest of the proof can be followed from equations (S.17-S.18) in \cite{Fang2022BRIDGE} (supplementary material) for any $\epsilon_1 \in (0,1)$ we get that: 
\begin{align}
   & \mathbb{P}\Bigg( \sup_{s}T_5(s) \geq \epsilon_1\Bigg)  \leq  \mathbb{P}\Bigg( \sup_{s} \sqrt{d} \hspace{0.1cm}T_6(s) \geq \epsilon_1\Bigg)  \leq \nonumber \\
  &    2\exp\bigg(-\frac{4M N{(\frac{\epsilon_1}{\sqrt{d}})}^2}{16(L^{'})^2 M d\|{\balpha}\|^2+{(\frac{\epsilon_1}{\sqrt{d}}})^2} + M\log\bigg(\frac{12L^{'}\sqrt{Md}}{\frac{\epsilon_1}{\sqrt{d}}}\bigg) +d\log\bigg( \frac{12L^{'}\Gamma_0\sqrt{d}}{\frac{\epsilon_1}{\sqrt{d}}}\bigg)\bigg) \\
 &\implies  \mathbb{P}\Bigg( \sup_{s}T_5(s) \geq \epsilon_1\Bigg)  \leq \nonumber \\ 
    &   2\exp\bigg(-\frac{4M N{(\epsilon_1)}^2}{16(L^{'})^2 M d^2\|{\balpha}\|^2+{(\epsilon_1)^2}} + M\log\bigg(\frac{12 L^{'}d\sqrt{M}}{{\epsilon_1}}\bigg) +d\log\bigg( \frac{12  L^{'}\Gamma_0{d}}{{\epsilon_1}}\bigg)\bigg) . 
\end{align}

Equivalently, we have with probability at least $1-\delta_1$ that
\begin{align}
& \sup_{s}\sum_{k=1}^d\bigg\lvert {{\frac{1}{N} \sum\limits_{i=1}^N \sum\limits_{j=1}^M [\bc_k(s+1)]_j\nabla_k \ell_j ( \widehat{\bw}^s(s); \bz_{ij})} - { \mathbb{E}\bigg[\frac{1}{N} \sum\limits_{i=1}^N \sum\limits_{j=1}^M [\bc_k(s+1)]_j\nabla_k \ell_j ( \widehat{\bw}^{s}(s); \bz_{ij})\bigg]} }\bigg\rvert  \nonumber \\  & \hspace{4cm}< \cO\bigg(\sqrt{\frac{{L^{'}}^2d^2\|{\balpha}\|^2\log\frac{2}{\delta_1}}{N}}\bigg), \label{sampleco_2}
\end{align}
where $\delta_1= 2\exp\bigg(-\frac{4M N{(\epsilon_1)}^2}{16(L^{'})^2 M d^2\|{\balpha}\|^2+{(\epsilon_1)^2}} + M\log\bigg(\frac{12 L^{'}d\sqrt{M}}{{\epsilon_1}}\bigg) +d\log\bigg( \frac{12  L^{'}\Gamma_0{d}}{{\epsilon_1}}\bigg)\bigg)$.
Then using union bound on the inequalities \eqref{sampleco_2}, \eqref{sampleco_1}, the following inequality holds:
\begin{align}
 \sup_{s}\sum_{k=1}^d\bigg\lvert {{\frac{1}{N} \sum\limits_{i=1}^N \sum\limits_{j=1}^M [\bc_k(s+1)]_j\nabla_k \ell_j ( \widehat{\bw}^s(s); \bz_{ij})} - { \mathbb{E}\bigg[\frac{1}{N} \sum\limits_{i=1}^N \sum\limits_{j=1}^M [\bc_k(s+1)]_j\nabla_k \ell_j ( \widehat{\bw}^{s}(s); \bz_{ij})\bigg]} }\bigg\rvert \nonumber &\\
& \hspace{-15cm}+ \sup_{s}\sum\limits_{k=1}^d\bigg\lvert \nabla_k f(\widehat{\bw}^{s}(s))  -  { \mathbb{E}\bigg[\frac{1}{MN} \sum\limits_{i=1}^N \sum\limits_{j=1}^M\nabla_k \ell_j ( \widehat{\bw}^{s}(s); \bz_{ij})\bigg]} \bigg \rvert \geq \nonumber \\ & \hspace{-12cm} 2 \max \bigg\{\sqrt{\log \bigg( \frac{2d}{\delta_0} \bigg)} \frac{L^{'}d}{\sqrt{2MN}}, \cO\bigg(\sqrt{\frac{{L^{'}}^2d^2\|{\balpha}\|^2\log\frac{2}{\delta_1}}{N}}\bigg) \bigg \}
\end{align}
with probability of at most $\delta_0 + \delta_1$ which along with \eqref{linearexpect1} and triangle inequality implies
\begin{align}
     \sup_{s}\sum_{k=1}^d\bigg\lvert {{\frac{1}{N} \sum\limits_{i=1}^N \sum\limits_{j=1}^M [\bc_k(s+1)]_j\nabla_k \ell_j ( \widehat{\bw}^s(s); \bz_{ij})} - {\nabla_k f(\widehat{\bw}^{s}(s))\bigg]} }\bigg\rvert &\geq \nonumber \\ & \hspace{-5cm}  2 \max \bigg\{\sqrt{\log \bigg( \frac{2d}{\delta_0} \bigg)} \frac{L^{'}}{\sqrt{2MN}}, \cO\bigg(\sqrt{\frac{{L^{'}}^2d^2\|{\balpha}\|^2\log\frac{2}{\delta_1}}{N}}\bigg) \bigg \}
\end{align}
with probability of at most $\delta_0 + \delta_1$ and so for $$C_0 =   \sup_{s \geq 0}\sum_{k=1}^d\bigg\lvert {{\frac{1}{N} \sum\limits_{i=1}^N \sum\limits_{j=1}^M [\bc_k(s+1)]_j\nabla_k \ell_j ( \widehat{\bw}^s(s); \bz_{ij})} - {\nabla_k f(\widehat{\bw}^{s}(s))\bigg]} }\bigg\rvert $$ we have
\begin{align}
    C_0 <  2 \max \bigg\{\sqrt{\log \bigg( \frac{2d}{\delta_0} \bigg)} \frac{L^{'}}{\sqrt{2MN}}, \cO\bigg(\sqrt{\frac{{L^{'}}^2d^2\|{\balpha}\|^2\log\frac{2}{\delta_1}}{N}}\bigg) \bigg \} \label{sampcompC0*}
\end{align}
with probability of at least $1-(\delta_0 + \delta_1)$.

Finally, setting $\epsilon_0 =\epsilon_1 = \epsilon'$ and 
\begin{align*}
    \delta  & =  \underbrace{2\exp\bigg(-\frac{4M N{(\epsilon')}^2}{16(L^{'})^2 M d^2\|{\balpha}\|^2+{(\epsilon')^2}} + M\log\bigg(\frac{12 L^{'}d\sqrt{M}}{{\epsilon'}}\bigg) +d\log\bigg( \frac{12  L^{'}\Gamma_0{d}}{{\epsilon'}}\bigg)\bigg)}_{=\delta_1} + \nonumber \\ &  \underbrace{2d  \exp{\bigg(- \frac{2 (\epsilon')^2 M N}{( L^{'}d)^2}\bigg)}}_{=\delta_0} ,
\end{align*}
 for any large enough $N > 1$, $d > \epsilon'$ we get that $ \delta_0 < \delta_1 $ since $\|{\balpha}\|^2 \in [\frac{1}{M},1] $ and so $ \delta < 2 \delta_1$. Hence we have
\begin{align}
    C_0 &<  2 \max \bigg\{\sqrt{\log \bigg( \frac{2d}{\delta_0} \bigg)} \frac{L^{'}d}{\sqrt{2MN}}, \cO\bigg(\sqrt{\frac{{L^{'}}^2d^2\|{\balpha}\|^2\log\frac{2}{\delta_1}}{N}}\bigg) \bigg \} \nonumber \\ &= \cO\bigg(\sqrt{\frac{{L^{'}}^2d^2\|{\balpha}\|^2\log\frac{2}{\delta_1}}{N}}\bigg) \nonumber \\
    & < \cO\bigg(\sqrt{\frac{{L^{'}}^2d^2\|{\balpha}\|^2\log\frac{4}{\delta}}{N}}\bigg)   \label{sampcompC0**}
\end{align}
with the probability of at least $1-\delta$. This completes the first part of the proof.
\end{proof}





\subsection{Proof of Theorem \ref{statisticalconvergencethm}}\label{statisticalconvergencethmproof}
\begin{proof}
To find the statistical rates of convergence for RESIST in the strongly convex setting, we need to bound the residual error arising in \eqref{ballconvergence1} from Theorem \ref{maintheorempaper1}. We first split the residual term into $C_0$ and $\Delta$ dependent terms so that their sample complexity bounds can be invoked separately. Recall that from \eqref{ballconvergence1} we have $C_0 = \sup_{s \geq 0}\sum\limits_{k=1}^d\lvert \nabla_k f(\widehat{\bw}^{s}(s))  -  \nabla_k f^{k,s+1}(\widehat{\bw}^{s}(s)) \rvert $, $\Delta = \sum\limits_{i=1}^M \norm{\bw^* -\bw^*_i} $ with $C_0 < \infty $. We already have the sample complexity for $C_0$ from Lemma \ref{supsampleco_lem}, and we only need to establish the sample complexity for $\Delta$.  

\subsection{Sample complexity for $\Delta$:}
Recall that 
\begin{align}
  \Delta = \sum\limits_{i=1}^M \norm{\bw^*- \bw^*_i} \leq  \sum\limits_{i=1}^M \bigg(\norm{\bw^*- \bw^*_{\SR}}+ \norm{\bw^*_{\SR}- \bw^*_i}\bigg).  \label{deltasampledef}
\end{align}
 From $\mu$-strong convexity of $\frac{1}{MN}\sum\limits_{i=1}^N\sum\limits_{i=1}^M \ell_j(\hspace{0.1cm}\cdot \hspace{0.1cm}; \bz_{ij})$ and $\frac{1}{N}\sum\limits_{j=1}^N \ell_j(\hspace{0.1cm}\cdot \hspace{0.1cm}; \bz_{ij})$ for any $i$, using \eqref{ermtemp2} we have that
\begin{align}
    {\mu} \norm{\bw^*- \bw^*_{\SR}} &\leq  \norm{\frac{1}{MN}\sum\limits_{j=1}^M\sum\limits_{i=1}^N \nabla \ell_j(\bw^*; \bz_{ij}) - \frac{1}{MN} \sum\limits_{j=1}^M\sum_{i=1}^N \nabla \ell_j(\bw^*_{\SR}; \bz_{ij}) } \nonumber \\
 & =  \norm{\frac{1}{MN} \sum\limits_{j=1}^M\sum_{i=1}^N  \nabla \ell_j(\bw^*_{\SR}; \bz_{ij}) } \nonumber \\
    & = \norm{\frac{1}{MN} \sum\limits_{j=1}^M\sum_{i=1}^N  \nabla \ell_j(\bw^*_{\SR}; \bz_{ij})  - \mathbb{E}\bigg[\frac{1}{MN} \sum\limits_{j=1}^M\sum_{i=1}^N  \nabla \ell_j(\bw^*_{\SR}; \bz_{ij}) \bigg]}, \label{sampcomp1a} \\
    {\mu} \norm{\bw^*_{\SR}- \bw^*_i} &\leq  \norm{\frac{1}{N}\sum_{i=1}^N \nabla \ell_j(\bw^*_{\SR}; \bz_{ij}) - \frac{1}{N}\sum_{i=1}^N \nabla \ell_j(\bw^*_i; \bz_{ij}) } \nonumber \\ 
    &=  \norm{\frac{1}{N}\sum_{i=1}^N \nabla \ell_j(\bw^*_{\SR}; \bz_{ij}) - \mathbb{E}\bigg[\frac{1}{N}\sum_{i=1}^N \nabla \ell_j(\bw^*_{\SR}; \bz_{ij})\bigg]}. \label{sampcomp1b}
\end{align}
Then using Jensen's inequality on the right-hand sides of \eqref{sampcomp1a}, \eqref{sampcomp1b}, the union bound followed by Hoeffding's inequality for any $ {\epsilon_2} \in (0,1)$, $ {\epsilon_3} \in (0,1)$ and using Assumption \ref{diamk} and Lemma \ref{pl_lem} that $\{\bw^*_{\SR}, \bigcup_{i=1}^M \bw^*_i, \bw^*\} \subset \cK$ we get:
\begin{align}
    \mathbb{P}\Bigg( \norm{\frac{1}{MN} \sum\limits_{j=1}^M\sum_{i=1}^N  \nabla \ell_j(\bw^*_{\SR}; \bz_{ij})  - \mathbb{E}\bigg[\frac{1}{MN} \sum\limits_{j=1}^M\sum_{i=1}^N  \nabla \ell_j(\bw^*_{\SR}; \bz_{ij}) \bigg]} \geq \epsilon_2 \Bigg) & \leq \nonumber \\ 
     \mathbb{P}\Bigg( \sum_{k=1}^d\bigg\lvert{\frac{1}{MN} \sum\limits_{j=1}^M\sum_{i=1}^N  \nabla_k \ell_j(\bw^*_{\SR}; \bz_{ij})  - \mathbb{E}\bigg[\frac{1}{MN} \sum\limits_{j=1}^M\sum_{i=1}^N  \nabla_k \ell_j(\bw^*_{\SR}; \bz_{ij}) \bigg]}\bigg\rvert \geq \epsilon_2 \Bigg) & \leq \nonumber \\
     \sum_{k=1}^d\mathbb{P}\Bigg( \bigg\lvert{\frac{1}{MN} \sum\limits_{j=1}^M\sum_{i=1}^N  \nabla_k \ell_j(\bw^*_{\SR}; \bz_{ij})  - \mathbb{E}\bigg[\frac{1}{MN} \sum\limits_{j=1}^M\sum_{i=1}^N  \nabla_k \ell_j(\bw^*_{\SR}; \bz_{ij}) \bigg]}\bigg\rvert \geq \frac{\epsilon_2}{d} \Bigg) & \leq 2d\exp{\bigg(- \frac{2 \epsilon_2^2 M N}{( L^{'}d)^2}\bigg)} \\
    \implies  \norm{\frac{1}{MN} \sum\limits_{j=1}^M\sum_{i=1}^N  \nabla \ell_j(\bw^*_{\SR}; \bz_{ij})  - \mathbb{E}\bigg[\frac{1}{MN} \sum\limits_{j=1}^M\sum_{i=1}^N  \nabla \ell_j(\bw^*_{\SR}; \bz_{ij}) \bigg]} &< \sqrt{\log \bigg( \frac{2d}{\delta_2} \bigg)} \frac{L^{'}d}{\sqrt{2MN}} \nonumber \\ & \hspace{-10cm} \text{with probability of at least} \hspace{0.1cm} 1-{ \delta_2} \hspace{0.1cm}\text{where} \hspace{0.1cm}{\delta_2 =2d\exp{\bigg(- \frac{2 \epsilon_2^2 M N}{( L^{'}d)^2}\bigg)}}, \label{checkxy1}\hspace{0.1cm}\text{and similarly}\\\mathbb{P}\Bigg( \norm{\frac{1}{N}\sum_{i=1}^N \nabla \ell_j(\bw^*_{\SR}; \bz_{ij}) - \mathbb{E}\bigg[\frac{1}{N}\sum_{i=1}^N \nabla \ell_j(\bw^*_{\SR}; \bz_{ij})\bigg]} \geq \epsilon_3 \Bigg) & \leq  2d\exp{\bigg(- \frac{2 \epsilon_3^2  N}{( L^{'}d)^2}\bigg)} \\
    \implies  \norm{\frac{1}{N}\sum_{i=1}^N \nabla \ell_j(\bw^*_{\SR}; \bz_{ij}) - \mathbb{E}\bigg[\frac{1}{N}\sum_{i=1}^N \nabla \ell_j(\bw^*_{\SR}; \bz_{ij})\bigg]} &<\sqrt{\log \bigg( \frac{2d}{\delta_3} \bigg)} \frac{L^{'}d}{\sqrt{2N}} \nonumber \\ \hspace{0.1cm} \text{with probability of at least} \hspace{0.1cm}   1 - { \delta_3} \hspace{0.1cm} \text{where} \hspace{0.1cm} {\delta_3 =2d\exp{\bigg(- \frac{2 \epsilon_3^2  N}{( L^{'}d)^2}\bigg)}}. \label{checkxy2}
\end{align}
Then using union bound on \eqref{checkxy1}, \eqref{checkxy2} as before followed by \eqref{sampcomp1a}, \eqref{sampcomp1b} and \eqref{deltasampledef} we get:
\begin{align}
    \Delta & < \frac{2M}{\mu} \max \Bigg\{ \sqrt{\log \bigg( \frac{2d}{\delta_2} \bigg)} \frac{L^{'}d}{\sqrt{2MN}}  , \hspace{0.1cm} \sqrt{\log \bigg( \frac{2d}{\delta_3} \bigg)} \frac{L^{'}d}{\sqrt{2N}}\Bigg\} \label{sampcompdelta*}
\end{align}
with probability of at least $1-( \delta_2 + \delta_3)$.

Finally, setting $\epsilon_2 =\epsilon_3 = \epsilon'$ and $$\delta  = \underbrace{2d\exp{\bigg(- \frac{2 (\epsilon')^2 M N}{( L^{'}d)^2}\bigg)}}_{=\delta_2} + \underbrace{2d\exp{\bigg(- \frac{2 (\epsilon')^2  N}{( L^{'}d)^2}\bigg)}}_{=\delta_3} ,$$ we get that $ \delta_2 < \delta_3$ and so $ \delta < 2 \delta_3$. Then, for any large enough $N$, we have
\begin{align}
   \Delta & < \frac{2M}{\mu} \max \Bigg\{ \sqrt{\log \bigg( \frac{2d}{\delta_2} \bigg)} \frac{L^{'}d}{\sqrt{2MN}}  , \hspace{0.1cm} \sqrt{\log \bigg( \frac{2d}{\delta_3} \bigg)} \frac{L^{'}d}{\sqrt{2N}}\Bigg\} \nonumber \\ &=  \frac{2M}{\mu} \sqrt{\log \bigg( \frac{2d}{\delta_3} \bigg)} \frac{L^{'}d}{\sqrt{2N}} \nonumber \\
    & < \frac{2M}{\mu} \sqrt{\log \bigg( \frac{4d}{\delta} \bigg)} \frac{L^{'}d}{\sqrt{2N}} \label{sampcompdelta**}
\end{align}
with the probability of at least $1-\delta$.

Then from Corollary \ref{corro_inexactlmigeo} and \eqref{sampcompdelta**} we get that:
\begin{align}
        \limsup_{s \to \infty} \xi^1_k(s)  &\leq \cO (h M \hspace{0.1cm} \text{diam}(\cK) ) + \cO\bigg(\frac{2M h}{\mu}  \sqrt{\log \bigg( \frac{4d}{\delta} \bigg)} \frac{L^{'}d}{\sqrt{2N}}\bigg), \\
      \limsup_{s \to \infty} \xi^5_k(s)  &\leq  \cO(h M \hspace{0.1cm} \text{diam}(\cK) ) + \cO\bigg(\frac{2M h}{\mu}  \sqrt{\log \bigg( \frac{4d}{\delta} \bigg)}\frac{L^{'}d}{\sqrt{2N}}\bigg),
    \end{align}
with the probability of at least $1-\delta$ where 
\begin{align}
    \delta  = {2d\exp{\bigg(- \frac{2 (\epsilon')^2 M N}{( L^{'}d)^2}\bigg)}}+ {2d\exp{\bigg(- \frac{2 (\epsilon')^2  N}{( L^{'}d)^2}\bigg)}}.
\end{align}


Next, recalling the asymptotics of $\xi^6(s)$ from Corollary \ref{corro_inexactlmigeo}, using the fact that $ \bw^*_{\ERM} = \bw^*$ and invoking triangle inequality we have that the averaged iterate error $ \norm{\bw^*_{\SR} - \widehat{\bw}^s(s)}  $ satisfies:
    \begin{align}
   \limsup_{s \to \infty}  \norm{\bw^*_{\SR} - \widehat{\bw}^s(s)} & \leq \frac{C_0}{\mu} + \frac{L \sqrt{Md}}{\mu}\bigg( \frac{h}{1-a_1}\bigg(a_2 \sqrt{M} (\sqrt{M} + 1) C_1 \text{diam}(\cK) + a_2 \Delta\bigg)\bigg) \nonumber \\ & \hspace{2cm}+  \norm{\bw^*_{\SR} - \bw^*_{\ERM}}. 
\end{align}
with the probability of at least $1-\delta$ where 
\begin{align}
    \delta  = {2d\exp{\bigg(- \frac{2 (\epsilon')^2 M N}{( L^{'}d)^2}\bigg)}}+ {2d\exp{\bigg(- \frac{2 (\epsilon')^2  N}{( L^{'}d)^2}\bigg)}}.
\end{align}
%\subsection*{Sample complexity for $\frac{C_0}{\mu} + \frac{h L \sqrt{Md }a_2}{\mu (1- a_1)}  \Delta$ :}
Suppose we choose a common $ \epsilon'$ across three probability bounds in \eqref{sampcompC0**}, \eqref{checkxy1} and \eqref{sampcompdelta**}, then those probability bounds hold with probability of at least $ 1 - \delta_0$, $ 1 - \delta_1$, $ 1 - \delta_2$, respectively, where 
\begin{align*}
    \delta_0   & =  {2\exp\bigg(-\frac{4M N{(\epsilon')}^2}{16(L^{'})^2 M d^2\|{\balpha}\|^2+{(\epsilon')^2}} + M\log\bigg(\frac{12 L^{'}d\sqrt{M}}{{\epsilon'}}\bigg) +d\log\bigg( \frac{12  L^{'}\Gamma_0{d}}{{\epsilon'}}\bigg)\bigg)}  \nonumber \\ & \hspace{2cm}+ {2d  \exp{\bigg(- \frac{2 (\epsilon')^2 M N}{( L^{'}d)^2}\bigg)}} \\
    \delta_1 &  = {2d\exp{\bigg(- \frac{2 (\epsilon')^2 M N}{( L^{'}d)^2}\bigg)}} + {2d\exp{\bigg(- \frac{2 (\epsilon')^2  N}{( L^{'}d)^2}\bigg)}} \\
    \delta_2 & = 2d\exp{\bigg(- \frac{2 \epsilon_2^2 M N}{( L^{'}d)^2}\bigg)}.
\end{align*}
Thus we have the ordering $ \delta_2 < \delta_1 < \delta_0$ for any large enough $N > 1$, $d > \epsilon'$. Then adding \eqref{sampcompC0**}, \eqref{checkxy1} and \eqref{sampcompdelta**}, followed by the union bound over three probability bounds and using $\delta = \delta_0 + \delta_1 + \delta_2 < 3 \delta_0$ we have:
\begin{align}
    \frac{C_0}{\mu} + \frac{h L a_2\sqrt{Md }}{\mu (1- a_1)}  \Delta   +  \norm{\bw^*_{\SR} - \bw^*_{\ERM}}& \leq \nonumber \\ & \hspace{-6cm}3\max  \Bigg\{ \cO\bigg(\sqrt{\frac{{L^{'}}^2d^2\|{\balpha}\|^2\log\frac{4}{\delta_0}}{N}}\bigg) , \hspace{0.1cm} \frac{2M h L a_2\sqrt{Md }}{\mu^2 (1- a_1)}\sqrt{\log \bigg( \frac{4d}{\delta_1} \bigg)} \frac{L^{'}d}{\sqrt{2N}} ,\hspace{0.1cm} \frac{1}{\mu}\sqrt{\log \bigg( \frac{2d}{\delta_2} \bigg)} \frac{L^{'}d}{\sqrt{2MN}}   \Bigg\} \\
    & \hspace{-6cm} \underbrace{=}_{\text{for } h < \frac{1}{M^2\sqrt{d}}} \cO\bigg(\frac{6}{\mu}\sqrt{\frac{{L^{'}}^2d^2\|{\balpha}\|^2(\log\frac{12}{\delta})}{N}}\bigg) \label{concentrationbound1}
\end{align}
with the probability of at least $1-\delta$ where  
\begin{align}
    \delta &=  {6d  \exp{\bigg(- \frac{2 (\epsilon')^2 M N}{( L^{'}d)^2}\bigg)}} + {2d\exp{\bigg(- \frac{2 (\epsilon')^2  N}{( L^{'}d)^2}\bigg)}} + \nonumber \\ &\hspace{2cm}
    {2\exp\bigg(-\frac{4M N{(\epsilon')}^2}{16(L^{'})^2 M d^2\|{\balpha}\|^2+{(\epsilon')^2}} + M\log\bigg(\frac{12 L^{'}d\sqrt{M}}{{\epsilon'}}\bigg) +d\log\bigg( \frac{12  L^{'}\Gamma_0{d}}{{\epsilon'}}\bigg)\bigg)}.
\end{align}
Hence 
\begin{align}
   \limsup_{s \to \infty}  \norm{\bw^*_{\SR} - \widehat{\bw}^s(s)} & \leq \cO\bigg(\frac{6}{\mu}\sqrt{\frac{{L^{'}}^2d^2\|{\balpha}\|^2(\log\frac{12}{\delta})}{N}}\bigg)  + \cO\bigg(h M \sqrt{Md} \hspace{0.1cm}\text{diam}(\cK) \bigg) 
\end{align}
with the probability of at least $1-\delta$, which completes the first part of the proof.


For the second part, recall that from \eqref{finthm0} after taking $s \to \infty$ we have :
\begin{align}
    \limsup_{s \to \infty} \bigg(\norm{\bW(s) - \overline{\bW}(s)}_F+ \norm{\bW^* - \widehat{\bW}^s(s)}_F + \norm{\bW(s) -\widehat{\bW}^s(s) }_F\bigg) & \lesssim_{\bM(h,J)} \nonumber \\ & \hspace{-6cm} \lim_{s \to \infty}  \sqrt{3Md}\bigg(\rho(\bM(h,J))\bigg)^{s} \norm{\bfg(0)}  + \cO(C_0+ \Delta) \\
     \implies  \limsup_{s \to \infty} \bigg(\norm{\bW(s) - \overline{\bW}(s)}_F+ \norm{\bW^*_{\SR} - \widehat{\bW}^s(s)}_F + \norm{\bW(s) -\widehat{\bW}^s(s) }_F \bigg)& \lesssim_{\bM(h,J)} \nonumber \\ & \hspace{-4cm}\cO(C_0+ \Delta + \norm{\bW^*_{\SR} - \bW^*_{\ERM}}_F). 
\end{align}
with the probability of at least $1-\delta$ where  
\begin{align}
    \delta &=  {6d  \exp{\bigg(- \frac{2 (\epsilon')^2 M N}{( L^{'}d)^2}\bigg)}} + {2d\exp{\bigg(- \frac{2 (\epsilon')^2  N}{( L^{'}d)^2}\bigg)}} + \nonumber \\ &\hspace{2cm}
    {2\exp\bigg(-\frac{4M N{(\epsilon')}^2}{16(L^{'})^2 M d^2\|{\balpha}\|^2+{(\epsilon')^2}} + M\log\bigg(\frac{12 L^{'}d\sqrt{M}}{{\epsilon'}}\bigg) +d\log\bigg( \frac{12  L^{'}\Gamma_0{d}}{{\epsilon'}}\bigg)\bigg)}.
\end{align}
Then using the above bound along with the fact that $C_0+ \Delta + \norm{\bW^*_{\SR} - \bW^*_{\ERM}}_F \overset{N \to \infty}{\rightarrow} 0 $ in probability from \eqref{concentrationbound1}, we get :
\begin{align}
   \lim_{N \to \infty} \limsup_{s \to \infty} \bigg(\norm{\bW(s) - \overline{\bW}(s)}_F+ \norm{\bW^*_{\SR} - \widehat{\bW}^s(s)}_F + \norm{\bW(s) -\widehat{\bW}^s(s) }_F\bigg) = 0 \hspace{0.4cm} \text{with high probability}
\end{align}
which completes the second part of the proof in this appendix. 
\end{proof}


\subsection{Proof of Theorem \ref{statisticalconvergencethm_pl}}\label{statisticalconvergencethm_plproof}
\begin{proof}
From Lemma \ref{supsampleco_lem} we have that:
     \begin{align}
      C_0 &<   \cO\bigg(\sqrt{\frac{{L^{'}}^2d^2\|{\balpha}\|^2\log\frac{4}{\delta}}{N}}\bigg)  
\end{align}
with the probability of at least $1-\delta$ where 
\begin{align*}
     \delta  &=  {2\exp\bigg(-\frac{4M N{(\epsilon')}^2}{16(L^{'})^2 M d^2\|{\balpha}\|^2+{(\epsilon')^2}} + M\log\bigg(\frac{12 L^{'}d\sqrt{M}}{{\epsilon'}}\bigg) +d\log\bigg( \frac{12  L^{'}\Gamma_0{d}}{{\epsilon'}}\bigg)\bigg)} \nonumber \\ & \hspace{2cm}+ {2d  \exp{\bigg(- \frac{2 (\epsilon')^2 M N}{( L^{'}d)^2}\bigg)}}.
\end{align*}
Taking $\limsup_{s \to \infty}$ on both sides of \eqref{pl_ratetheo_bound*} from Theorem \ref{plrate_theo} we get
 \begin{align}
    \limsup_{s \to \infty}  f(\widehat{\bw}^{s}(s)) - f^*    & \leq    L \hspace{0.1cm} \text{diam} (\cK) \frac{ C_0}{\mu(2-Lh)} +  \nonumber \\ & \hspace{0cm}  \frac{ L^2h d\sqrt{Md}}{1-a_1} \hspace{0.1cm} (\text{diam} (\cK))^2 \bigg(  \frac{(\sqrt{M}+1)^2}{\mu(2-Lh)}  LM(\sqrt{ d}+2) +  M   \bigg) \\
    \implies  \limsup_{s \to \infty} \lvert f(\widehat{\bw}^{s}(s)) - \cR^*_{\SR}  \rvert  & \leq     \frac{L \hspace{0.1cm} \text{diam} (\cK) C_0}{\mu(2-Lh)} + \cO \bigg(\frac{h L^3 M^{\frac{5}{2}} (d \hspace{0.1cm}\text{diam} (\cK))^2  }{\mu} \bigg)  + \lvert f^* -\cR^*_{\SR}\rvert. \label{pltemp_fin*}
    \end{align}
    Next, observe that $f^*  =  \hat{f}^*_{\ERM} =  \frac{1}{MN}\sum\limits_{i=1}^N \sum\limits_{j=1}^M \ell_j(\bw^*_{\ERM}; \hspace{0.1cm}\bz_{ij})  $, also $\bw^*_{\SR}$ is a deterministic variable w.r.t. measure $\mathbb{P}$ and $$ \mathbb{E} \bigg[\frac{1}{MN}\sum\limits_{i=1}^N \sum\limits_{j=1}^M \ell_j(\bw^*_{\SR}; \hspace{0.1cm}\bz_{ij})  \bigg] = \cR^*_{\SR}, \hspace{0.2cm} \mathbb{E} \bigg[\frac{1}{MN}\sum\limits_{i=1}^N \sum\limits_{j=1}^M \nabla \ell_j(\bw^*_{\SR}; \hspace{0.1cm}\bz_{ij})  \bigg] = \mathbf{0}.$$ Then by triangle inequality and Assumption \ref{pl_assumption} we have the following bound:
    \begin{align}
        \lvert f^* -\cR^*_{\SR}\rvert &\leq \bigg\lvert \frac{1}{MN}\sum\limits_{i=1}^N \sum\limits_{j=1}^M \ell_j(\bw^*_{\SR}; \hspace{0.1cm}\bz_{ij}) -\cR^*_{\SR}\bigg\rvert +  \bigg\lvert  \underbrace{\frac{1}{MN}\sum\limits_{i=1}^N \sum\limits_{j=1}^M \ell_j(\bw^*_{\ERM}; \hspace{0.1cm}\bz_{ij})}_{= f^*} - \underbrace{\frac{1}{MN}\sum\limits_{i=1}^N \sum\limits_{j=1}^M \ell_j(\bw^*_{\SR}; \hspace{0.1cm}\bz_{ij})}_{= f(\bw^*_{\SR})}\bigg\rvert \\
        & = \bigg\lvert \frac{1}{MN}\sum\limits_{i=1}^N \sum\limits_{j=1}^M \ell_j(\bw^*_{\SR}; \hspace{0.1cm}\bz_{ij}) -\cR^*_{\SR}\bigg\rvert + \underbrace{\bigg\lvert  f^* -  f(\bw^*_{\SR})\bigg\rvert}_{\leq \frac{1}{2\mu} \norm{\nabla f(\bw^*_{\SR})}^2 \text{ by Assumption } \ref{pl_assumption}} \\
        & \leq  \bigg\lvert \frac{1}{MN}\sum\limits_{i=1}^N \sum\limits_{j=1}^M \ell_j(\bw^*_{\SR}; \hspace{0.1cm}\bz_{ij}) -\mathbb{E} \bigg[\frac{1}{MN}\sum\limits_{i=1}^N \sum\limits_{j=1}^M \ell_j(\bw^*_{\SR}; \hspace{0.1cm}\bz_{ij})  \bigg]\bigg\rvert +  \frac{1}{2\mu} \norm{\nabla f(\bw^*_{\SR})}^2 \\
         & = \underbrace{\bigg\lvert \frac{1}{MN}\sum\limits_{i=1}^N \sum\limits_{j=1}^M \ell_j(\bw^*_{\SR}; \hspace{0.1cm}\bz_{ij}) -\mathbb{E} \bigg[\frac{1}{MN}\sum\limits_{i=1}^N \sum\limits_{j=1}^M \ell_j(\bw^*_{\SR}; \hspace{0.1cm}\bz_{ij})  \bigg]\bigg\rvert}_{=T_1} +  \nonumber \\ & \underbrace{\frac{1}{2\mu} \norm{\frac{1}{MN}\sum\limits_{i=1}^N \sum\limits_{j=1}^M \nabla \ell_j(\bw^*_{\SR}; \hspace{0.1cm}\bz_{ij})  - \mathbb{E} \bigg[\frac{1}{MN}\sum\limits_{i=1}^N \sum\limits_{j=1}^M \nabla \ell_j(\bw^*_{\SR}; \hspace{0.1cm}\bz_{ij})  \bigg]}^2}_{=T_2} . \label{plstat_temp1}
    \end{align}
   From Assumption \ref{boundedassumpstat}, we can have the following bounds (same as in the proof of Lemma \ref{supsampleco_lem}): 
\begin{align}
   \max \bigg\{  \sup_{\bw \in \cK}  \lvert{ \nabla_k \ell_j(\bw ;\bz_{ij}) }\rvert ,  \sup_{\bw \in \cK}  \lvert{ \ell_j(\bw ;\bz_{ij}) }\rvert \bigg\}  &\leq L^{'} , \sup_{\bw \in \cK} \norm{\bw} \leq \Gamma_0 = \text{diam}(\cK)
\end{align}
 for any $k \in  \{1,\cdots,d\}$, any $j \in \{1,\cdots,M\}$ and also for any $ \{\bz_{ij}\}_{i=1}^N \overset{i.i.d.}{\sim} \mathbb{P}$ and any $N \geq 1$ where the constant  $L^{'}$ satisfies $L^{'} = \max\bigg\{\cO(L d\text{ diam}(\cK)),\cO(L (\text{diam}(\cK))^2) \bigg\}$.
   Then using Hoeffding's inequality on the term $T_1$ in \eqref{plstat_temp1} we get that for any $\epsilon' \in  (0,1)$:
    \begin{align}
        \bbP(T_1 \geq \epsilon') & \leq 2 \exp{ \bigg( - \frac{2(\epsilon')^2 M N}{(L^{'})^2}\bigg)}, \\ 
        \implies T_1 &< \sqrt{\log \bigg( \frac{2}{\delta_1} \bigg)} \frac{L^{'}}{\sqrt{2MN}} \text{ with probability of at least } 1- \delta_1 \text{ where } \delta_1 = 2 \exp{ \bigg( - \frac{2(\epsilon')^2 M N}{(L^{'})^2}\bigg)}. \label{t1hoeffdingbound}
    \end{align}
Next, using union bound over multiple random variable across each dimension followed by Hoeffding's inequality on the term $T_2$, we get that for any $\epsilon' \in  (0,1)$:
    \begin{align}
    \bbP(\sqrt{ 2\mu T_2} \geq \epsilon') & \leq \bbP\bigg (\sum_{k=1}^d\bigg\lvert{\frac{1}{MN}\sum\limits_{i=1}^N \sum\limits_{j=1}^M \nabla_k \ell_j(\bw^*_{\SR}; \hspace{0.1cm}\bz_{ij})  - \mathbb{E} \bigg[\frac{1}{MN}\sum\limits_{i=1}^N \sum\limits_{j=1}^M \nabla_k \ell_j(\bw^*_{\SR}; \hspace{0.1cm}\bz_{ij})  \bigg]}\bigg\rvert \geq \epsilon' \bigg) \\
    & \leq \sum_{k=1}^d\bbP\bigg (\bigg\lvert{\frac{1}{MN}\sum\limits_{i=1}^N \sum\limits_{j=1}^M \nabla_k \ell_j(\bw^*_{\SR}; \hspace{0.1cm}\bz_{ij})  - \mathbb{E} \bigg[\frac{1}{MN}\sum\limits_{i=1}^N \sum\limits_{j=1}^M \nabla_k \ell_j(\bw^*_{\SR}; \hspace{0.1cm}\bz_{ij})  \bigg]}\bigg\rvert \geq \frac{\epsilon'}{d} \bigg) \\
    & \leq 2d \exp{ \bigg( - \frac{2(\epsilon')^2 M N}{(L^{'}d)^2}\bigg)} \\
    \implies    \bbP(\sqrt{ 2\mu T_2} \geq \epsilon') & \leq 2d \exp{ \bigg( - \frac{2(\epsilon')^2 M N}{(L^{'}d)^2}\bigg)} \\
    \implies \sqrt{2 \mu T_2} &< \sqrt{\log \bigg( \frac{2d}{\delta_2} \bigg)} \frac{L^{'}d}{\sqrt{2MN}} \text{ with probability of at least } 1- \delta_2 \text{ where } \nonumber \\ & \hspace{2.5cm} \delta_2 = 2d \exp{ \bigg( - \frac{2(\epsilon')^2 M N}{(L^{'}d)^2}\bigg)} . \label{t2hoeffdingbound}
    \end{align}
    Suppose a common $\epsilon'$ is chosen for the probability bounds \eqref{t1hoeffdingbound} and \eqref{t2hoeffdingbound}, it can be readily checked that for $N > 1$, $\max\{\delta_1, \delta_2\}<\delta_0 $ where $\delta_0$ comes from Lemma \ref{supsampleco_lem} in the sense that the upper bound on $C_0$ holds with probability of at least $1- \delta_0$. Now adding $C_0$ (Lemma \ref{supsampleco_lem}), terms $T_1$ \eqref{t1hoeffdingbound} and $T_2$ \eqref{t2hoeffdingbound}, invoking \eqref{plstat_temp1} and using union bound, we have with probability $1- \delta$ where $ \delta = \delta_0 + \delta_1 + \delta_2 < 3 \delta_0 $, that 
    \begin{align}
        \frac{L \hspace{0.1cm} \text{diam} (\cK) C_0}{\mu(2-Lh)} +\lvert f^* -\cR^*_{\SR}\rvert & < 3 \max \bigg\{ \cO\bigg( \frac{4 L \hspace{0.1cm} \text{diam} (\cK) }{\mu(2-Lh)}\sqrt{\frac{{L^{'}}^2d^2\|{\balpha}\|^2\log\frac{4}{\delta_0}}{N}}\bigg) , \nonumber \\ & \hspace{2cm} \sqrt{\log \bigg( \frac{2}{\delta_1} \bigg)} \frac{L^{'}}{\sqrt{2MN}} , \hspace{0.1cm} {\log \bigg( \frac{2d}{\delta_2} \bigg)} \frac{(L^{'}d)^2}{{4 MN \mu}}  \bigg\} \\
          & \underbrace{<}_{\text{for } \sqrt{M} > \mu} \cO\bigg( \frac{ L \hspace{0.1cm} \text{diam} (\cK) }{\mu(2-Lh)}\sqrt{\frac{{L^{'}}^2d^2\|{\balpha}\|^2(\log\frac{12}{\delta})}{N}}\bigg) \label{pltemp_fin2*}
    \end{align}
    where 
    \begin{align*}
        \delta  &=  {2\exp\bigg(-\frac{4M N{(\epsilon')}^2}{16(L^{'})^2 M d^2\|{\balpha}\|^2+{(\epsilon')^2}} + M\log\bigg(\frac{12 L^{'}d\sqrt{M}}{{\epsilon'}}\bigg) +d\log\bigg( \frac{12  L^{'}\Gamma_0{d}}{{\epsilon'}}\bigg)\bigg)} \nonumber \\ & \hspace{2cm} + {4d  \exp{\bigg(- \frac{2 (\epsilon')^2 M N}{( L^{'}d)^2}\bigg)}} + 2 \exp{ \bigg( - \frac{2(\epsilon')^2 M N}{(L^{'})^2}\bigg)}.
    \end{align*}
    Then substituting \eqref{pltemp_fin2*} in \eqref{pltemp_fin*} completes the last part of the proof in this appendix.
\end{proof}
Observe that in Theorem \ref{statisticalconvergencethm_pl} (for P{\L} functions), unlike Theorem \ref{statisticalconvergencethm} (for strongly convex functions), we do not provide the statistical rates on the two consensus error terms $ \xi^1_k(s), \xi^5_k(s)$. To understand the reasoning behind, first notice that after any sufficiently large $S$, the consensus errors $ \xi^1_k(S), \xi^5_k(S)$, evaluated in the ERM optimization problem~\eqref{eqn: ERM}, are upper bounded by $\cO(h \Delta)$ term irrespective of the function class (see Theorems \ref{inexactlmigeo}, \ref{plrate_theo}) where $\Delta = \sum_{i=1}^M \norm{\bw_i^* - \bw^*} \leq M \text{diam}(\cK)$. Now, in the strongly convex case, we can upper bound the norm difference $\norm{\bw_i^* - \bw^*}  $ by the corresponding gradient norm difference $ \frac{1}{\mu}\norm{\nabla f(\bw_i^*) - \nabla f(\bw^*)} $ thus giving us the statistics for $\Delta$ in terms of the gradient samples. However, for P{\L} functions, we do not provide statistical convergence rates for the consensus error terms $ \xi^1_k(s), \xi^5_k(s)$ due to the property that P{\L} functions could have multiple minimum for local iterates to converge to.  


\subsection{On the non-vacuous nature of Assumption \ref{boundedassumpstat}}\label{boundedexistencesec}
For the sake of simplicity, we use the same setup as in the previous section \ref{boundedexistencesec_0} with mild modifications so as to incorporate the effect of data samples and their statistics. The model dimension as before is assumed to be $1$, i.e., $\ell_j(\cdot; \bz_{ij}) :\mathbb{R} \to \mathbb{R}$ for all $1 \leq i \leq N $, all $N$, any $\bz_{ij} \overset{i.i.d.}{\sim} \mathbb{P}$ where $\bz_{ij} \in \cZ$, the dataset $\cZ$ is a compact set (a closed ball) in a finite-dimensional Euclidean space, Assumptions \ref{claim2}, \ref{asumpt1_nonconvex} hold for $\ell_j(\cdot; \bz_{ij})$ for any $\bz_{ij} \overset{i.i.d.}{\sim} \mathbb{P}$, $\ell_j(\cdot; \bz_{ij})$ is coercive for all $i, j$, i.e., $ \lim_{\norm{\bw} \to \infty} \ell_j(\bw; \bz_{ij}) = \infty$ and $\ell_j(\cdot; \bz_{ij})$ is uniformly lower bounded for all $\bz_{ij} \overset{i.i.d.}{\sim} \mathbb{P}$ and all $i$ where this lower bound is $0$ without loss of generality. Further, the graph induced by the network topology is symmetric, strongly-connected with no bottlenecks in the sense that there are sufficient number of paths between any two nodes. In addition, we also assume that the probability measure $\mathbb{P}$ is supported on the compact set $\cZ$, $ \ell_j(\bw; \bz)$ is jointly continuous in $\bw, \bz$ for any $\bz \overset{i.i.d.}{\sim} \mathbb{P}$.

Also, suppose the attack for any given realization of data $ \{\bz_{ij}\}_{j=1}^N \subset \cZ$ for any $N$ is such that the mixing matrix $\bY(t; N)$ is symmetric, simultaneously diagonalizable for all $t$ and the sequence of simultaneously diagonalizable matrices $\{\bQ(s;N)\}_{s=0}^{\infty}$, where the $\bQ(s;N)$ matrix is defined from \eqref{cwtm1} as
\begin{align}
    \bQ(s;N) =  \prod\limits_{r= J \lfloor \frac{t}{J} \rfloor }^{J \lfloor \frac{t}{J} \rfloor + J -2} \bY(r;N)  
\end{align}
after omitting the subscript $k$, satisfy
\begin{align}
     \bQ(0;N) \preccurlyeq \bQ(1;N) \preccurlyeq \cdots \preccurlyeq  \bQ(s;N) \preccurlyeq \cdots . \label{liplyapunovc00}
\end{align}
Note that the dependence of $\bQ(s;N)$ is not only on number of samples $N$ but also on the i.i.d. draws of data samples $ \{\bz_{ij}\}_{j=1}^N \subset \cZ$ where $\bz_{ij} \overset{i.i.d.}{\sim} \mathbb{P}$, however for sake of brevity we omit this notation inside the bracket. 

Next, similar to section \ref{boundedexistencesec_0}, for $\bW = [\bw_1, \cdots, \bw_M]^T$ and $F(\bW;N) = \frac{1}{N}\sum_{i=1}^N \sum_{j=1}^M \ell_j(\bw_j;\bz_{ij})$ we define a random Lyapunov function $ \mathcal{L}(\cdot; s,N)  : \mathbb{R}^M \to \mathbb{R}$ as follows:
\begin{align}
    \mathcal{L}(\bW; s, N) := F(\bW;N) + \frac{1}{2h}\norm{\bW}^2_{\bI - \bQ(s;N)}
\end{align}
where $ \norm{\bW}^2_{\bI - \bQ(s;N)} = \langle \bW, (\bI - \bQ(s;N))\bW \rangle \geq 0$. By a simple calculation it can be shown that $\mathcal{L}(\cdot; s, N)$ will be uniformly gradient Lipschitz continuous with $\LIP(\mathcal{L}(\cdot; s, N) ) \leq LM + \frac{1}{h}$. Suppose the initialization $\bW(0)$ of RESIST is non-random and identical across all realizations of samples $\{\bz_{ij}\}_{j=1}^N$ and all $N$ with $\bz_{ij} \overset{i.i.d.}{\sim} \mathbb{P}$. Then by continuity of $f_i(\cdot; \bz)$ in $\bz$ and compactness of $\cZ$, we get that $$  \mathcal{L}(\bW(0); 0, N) \leq C(\text{diam} (\cZ)) < \infty$$
 for any realizations of samples $\{\bz_{ij}\}_{j=1}^N$ for all $N$ and $C$ is some absolute constant which depends on $\text{diam} (\cZ)$. 

 Let 
$$ S_{sub}(s; N) =\bigg\{ \bW \in \mathbb{R}^M: \mathcal{L}(\bW; s,N) \leq C(\text{diam} (\cZ))\bigg\}.$$
Then $S_{sub}(s;N)$ for any $s \geq 0$, for any fixed realizations of samples $\{\bz_{ij}\}_{j=1}^N$, is compact due to coercivity of $ \mathcal{L}(\bW; s,N) $ in $\bW$. Further, as in section \ref{boundedexistencesec_0} we will also have 
\begin{align}
S_{sub}(\infty; N)  \supseteq \cdots \supseteq S_{sub}(s+1; N)   \supseteq S_{sub}(s; N) \supseteq \cdots \supseteq S_{sub}(0; N) 
\end{align}
for any given realization of samples $\{\bz_{ij}\}_{j=1}^N$ drawn i.i.d. from $\mathbb{P}$ and any given $N$.  
Then for $h < \frac{1}{LM}$ and following the steps from \eqref{liplyapunovc6} onward in section \ref{boundedexistencesec_0}, for any fixed realization of samples $\{\bz_{ij}\}_{j=1}^N$ and compactness of $S_{sub}(\infty; N)$, we have that the sequence $\{\bW(s;N)\}_s$ generated by algorithm RESIST stays bounded in compact $S_{sub}(\infty;N)$ for all $s \geq 0$. As a consequence we get that for any sublevel set $S_{sub}(\infty;N)$ corresponding to some samples $\{\bz_{ij}\}_{j=1}^N$ with $\bz_{ij} \overset{i.i.d.}{\sim} \mathbb{P}$, if any $\bW = [\bw_1, \cdots,\bw_j, \cdots, \bw_M]^T  \in S_{sub}(\infty;N)$ then:
\begin{align}
 0 \leq \frac{1}{N}\sum_{j=1}^M \sum_{i=1}^N \ell_j(\bw_j;\bz_{ij}) &\leq C(\text{diam} (\cZ)) . \label{randomlyap1}
\end{align}

We now prove that for any given realization of samples $\{\bz_{ij}\}_{i=1}^N$ drawn i.i.d. from $\mathbb{P}$ and any given $N$, the compact set $S_{sub}(\infty;N)$ is contained within a data independent compact set. Let $\{\bz'_{ij}\}_{i=1}^N$ be a sequence of draws independent from $\{\bz_{ij}\}_{i=1}^N$ with $\bz'_{ij} \overset{i.i.d.}{\sim} \mathbb{P}$. Define 
$$S_{sub} = \bigg\{ \bW  = [\bw_1, \cdots,\bw_j, \cdots, \bw_M]^T  \in \mathbb{R}^M:  \mathbb{E}_{\mathbb{P}} \bigg[\frac{1}{N}\sum_{j=1}^M \sum_{i=1}^N \ell_j(\bw_j;\bz'_{ij})\bigg] \leq 3C(\text{diam} (\cZ))\bigg\}.$$
Note that the set $S_{sub}$ is compact, data independent since $\mathbb{E}_{\mathbb{P}} \bigg[\frac{1}{N}\sum_{j=1}^M \sum_{i=1}^N \ell_j(\bw_j;\bz'_{ij})\bigg] = \mathbb{E}_{\mathbb{P}} [F(\bW; N)] = F'(\bW)$ is coercive\footnote{Weighted average/ expectation of coercive functions is again coercive.} in variable $\bW = [\bw_1, \cdots,\bw_j, \cdots, \bw_M]^T $ and not random due to expectation operator.

Then for any $\bW = [\bw_1, \cdots,\bw_j, \cdots, \bw_M]^T \in S_{sub}(\infty;N)$ with data samples $\{\bz_{ij}\}_{i=1}^N$ in the definition of $S_{sub}(\infty;N)$ we have that:
\begin{align}
     0 \leq  \frac{1}{N}\sum_{i=1}^N\sum_{j=1}^M \ell_j(\bw_j;\bz_{ij}) &\leq C(\text{diam} (\cZ)) \\
     \implies \mathbb{E}_{\mathbb{P}} \bigg[\frac{1}{N}\sum_{j=1}^M \sum_{i=1}^N \ell_j(\bw_j;\bz'_{ij})\bigg] &\leq C(\text{diam} (\cZ)) + \frac{1}{N}\sum_{i=1}^N\sum_{j=1}^M\lvert \underbrace{\mathbb{E}_{\mathbb{P}}[ \ell_j(\bw_j;\bz'_{ij})]}_{= \mathbb{E}_{\mathbb{P}}[ \ell_j(\bw_j;\bz_{ij})] \text{ by data homogeneity}} - \ell_j(\bw_j;\bz_{ij}) \rvert \\
     \implies \mathbb{E}_{\mathbb{P}} \bigg[\frac{1}{N}\sum_{j=1}^M \sum_{i=1}^N \ell_j(\bw_j;\bz'_{ij})\bigg]  &\leq C(\text{diam} (\cZ)) + \sup_{\bw_j \in S_{sub}(\infty;N)}\frac{1}{N}\sum_{i=1}^N\sum_{j=1}^M \bigg(\lvert \mathbb{E}_{\mathbb{P}} [\ell_j(\bw_j;\bz_{ij})]\rvert + \lvert \ell_j(\bw_j;\bz_{ij}) \rvert \bigg) \\
      &\hspace{-1.5cm}\underbrace{=}_{\text{by linearity of expectation, non-negative } \ell_j} C(\text{diam} (\cZ)) +\sup_{\bw_j \in S_{sub}(\infty;N)}\mathbb{E}_{\mathbb{P}} \bigg[ \frac{1}{N}\sum_{i=1}^N\sum_{j=1}^M  \ell_j(\bw_j;\bz_{ij})\bigg] \nonumber \\ & \hspace{1.5cm}+ \sup_{\bw_j \in S_{sub}(\infty;N)} \frac{1}{N}\sum_{i=1}^N\sum_{j=1}^M  \ell_j(\bw_j;\bz_{ij})  \\
     \implies \mathbb{E}_{\mathbb{P}} \bigg[\frac{1}{N}\sum_{j=1}^M \sum_{i=1}^N \ell_j(\bw_j;\bz'_{ij})\bigg]  &\leq 2C(\text{diam} (\cZ)) +  \mathbb{E}_{\mathbb{P}} \bigg[\sup_{\bw_j \in S_{sub}(\infty;N)} \frac{1}{N}\sum_{i=1}^N\sum_{j=1}^M  \ell_j(\bw_j;\bz_{ij})\bigg]  \\
     \implies \mathbb{E}_{\mathbb{P}} \bigg[\frac{1}{N}\sum_{j=1}^M \sum_{i=1}^N \ell_j(\bw_j;\bz'_{ij})\bigg] &\leq 3 C(\text{diam} (\cZ)) ,
\end{align}
where we used inequality \eqref{randomlyap1} in the second last step followed by the fact that $ \sup_{\bw} \mathbb{E} f(\bw; \bz) \leq \mathbb{E} \sup_{\bw} f(\bw; \bz) $ for non-negative $f$ and inequality \eqref{randomlyap1} again in the last step. Hence, $\bW \in S_{sub}$ and since $\{\bz_{ij}\}_{i=1}^N$, $N$ were arbitrary we get that $ S_{sub}(\infty;N) \subseteq S_{sub}$ for all $N$ and all possible realizations of $\{\bz_{ij}\}_{i=1}^N$ drawn i.i.d. from $\mathbb{P}$.
Thus the compact sublevel set $S_{sub} $ satisfies the uniform boundedness condition from Assumption \ref{boundedassumpstat}.

\bibliographystyle{IEEEtran}
\bibliography{MMA}
\end{appendices}
\end{document}