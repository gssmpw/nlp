\section{Heuristics}%
\label{sec:heuristics}

\hyperref[sec:operational-semantics]{The operational semantics} is highly non-deterministic.
A direct, naive implementation would perform duplicate and unnecessary work.
To reduce work, our implementation of reference synthesis uses heuristics to guide the search.
These heuristics have three goals: (i)~to guide the search toward results, (ii)~to avoid duplicate work, and (iii)~to cut search branches early that do not lead to results.
For all these heuristics, we argue that they do not yield solutions that are not derivable from the operational semantics (soundness), and preserve all derivable solutions as well (completeness).

In this section, we discuss these heuristics, using the example in \cref{fig:example-state-for-heuristics}.
Performing reference synthesis on the program on the top-left eventually reaches the state shown in the figure.
The first two constraints are obtained from expanding rule \ref{eq:d-importok} on the initial hole on line 5 ($h_A$), represented by the variable $\UV{A_1}$.
Resolving this reference yields a scope, currently represented by the variable $\UV{s_1}$.
Once this scope is resolved, an incoming edge from $\svar{s}_{\id{B}}$ can be created.
Until then, this edge is not present in the scope graph (hence it is indicated with a dashed line).
The other two constraints correspond to the hole on line 6 ($h_x$), which is expanded to a reference with a single qualifier $\UV{A_2}\mathtt{.}\UV{x}$.
The qualifier should resolve to a scope $\UV{s_2}$, in which afterward, a query resolving to the target scope $\svar{s}_{\id{x}}$ will be performed.


%% Small example program for heuristics
\begin{figure}[t]
  \input{fig/0500-heuristics-example}
  \caption{
    Example program (top-left) and intermediate synthesis state (below and top-right).
  }%
  \label{fig:example-state-for-heuristics}
\end{figure}%


\subsection{Selecting Constraints}%
\label{subsec:selecting-constraints}

As discussed in~\cref{subsec:operational-semantics-discussion}, our system is \emph{confluent}.
%That means that, reordering expansions yields equivalent states.
For that reason, we can choose one single order in which to expand constraints, instead of trying all possible orders.
We choose the constraint to expand according to the following criteria (considered in order):
(i)~prefer queries over predicate constraints;
(ii)~prefer predicate constraints that can lead to queries over those that cannot; and
(iii)~prefer older constraints over newer constraints.
% \begin{enumerate}
%   \item prefer queries over predicates;
%   \item prefer predicate constraints that can lead to queries over those that cannot; and
%   \item prefer older constraints over newer constraints.
% \end{enumerate}
%
The rationale behind this order is that we try to expand queries as soon as possible.
Expanding queries typically blows up the search space less than expanding predicates, and queries often reach terminal states (ground references).
Using age as a tiebreaker, we try to explore the remaining state space in a breath-first manner, to ensure we reach all possible references.
In our example, this implies that we first expand the query constraint~(4), inferring \texttt{x} to be the value of $\UV{x}$, and $s_\id{A}$ to be the target $\UV{s_2}$ of constraint~(3).
As we eventually expand all constraints, we preserve soundness and completeness.


\subsection{Expanding Queries}%
\label{subsec:expanding-queries}

When expanding a query, a source and target scope must be chosen ($s'$ and $s''$ in \textsc{Op-Expand-Query} in \cref{fig:refsyn-operational-semantics}).
Instead of trying the query with all possible combinations of source and target scopes, we can be smart about the source and target scopes we pick.
First, we choose $s''$, based on the premise that it should be equal to the current target scope, $s_t$, or contain it.
Then, we infer a unifier from the query, $\mathcal{G} \vdash E[s'' / y] \rightsquigarrow \theta$.
If no such unifier exists, we stop the search for this branch.
Finally, we traverse the scope graph backwards, starting at $s''$.
We use the inverted regular expression (\eg, $\mathsf{inv}(\reclos{\lblLEX}\, \lblVAR) = \lblVAR\, \reclos{\lblLEX}$) to guide the graph traversal to only find source scopes that have a valid path to the target scope.
This approach only over-approximates when there is another declaration, reachable from $s'$, that shadows $s''$.
It is sound, as eventually we check all the premises of the rule.
It is also complete, as all other choices for $s''$ do not satisfy the premise that relates it to $s_t$, and all other choices for $s'$ do not yield valid query answers.

In our example, when expanding the query constraint (4), the only reasonable choice for $s''$ is $s_{\id{x}}$.
Then, solving~$E$ yields $\set{\UV{x} \mapsto \id{x}}$.
Next, following the inverted regular expression ($\lblVAR$) traversing the edge $s_\id{A}~\cEdge[\lblVAR]~s_\id{x}$ backwards, we choose $s'$ to be $s_{\id{A}}$.
This instantiation is a valid query expansion.


\subsection{Expanding Predicates}%
\label{subsec:expanding-predicates}

When selecting a predicate (\cref{subsec:selecting-constraints}) we fork the computation and try each possible expansion of a predicate to a rule concurrently.
We prioritize expansions that lead to a query, as those might converge to a result quicker.
Additionally, we prioritize rules that have fewer free variables, as those add less freedom to the problem; \ie, tell us more about the final solution.
As we only prioritize certain rules over others, but never discard any, we will eventually try all rules.
\CBP{Assming fairness?}
Therefore, this does not affect completeness.
In our example, this implies that we prioritize expanding the $\mathsf{scopeOfMod}$ constraints using rule \ref{eq:s-mod} before rule \ref{eq:s-qmod}.



\subsection{Isolating Holes}%
\label{subsec:isolating-holes}

The schemes in~\cref{subsec:selecting-constraints} and~\cref{subsec:expanding-predicates} reduce the search space significantly, % by choosing one order in which to expand constraints, rather than trying all permutations.
% However, this might 
but may skew the search to holes with less complicated solutions.
Therefore, when starting the computation, we fork the state for each hole, which we call the \emph{focus hole} of that search branch.
We only expand constraints that are related to this focus hole, ensuring we make progress on this hole specifically.

However, this approach is incomplete, as sometimes the solution of one hole can only be computed after another hole is solved.
For example, for the program in~\cref{fig:example-state-for-heuristics}, \id{x} is a valid solution for $h_x$ (although in a different branch than the state presented there).
However, this can only be computed when the solution for $h_A$ is known, as it requires the edge $s_\id{B} \cEdge[\lblIMP] s_\id{A}$ to be present in the scope graph.
We need a way to ensure such `composite' solutions are computed as well.

% We can detect these cases as follows.
We ensure this as follows.
Suppose query expansion (\cref{subsec:expanding-queries}) traverses an $l$-labeled edge.
When there is a scope $s$ such that future steps \emph{might} create an $l$-labeled edge in $s$ (\ie, $\overline{C} \hookrightarrow (s, l)$; see \citet[\S{}5.3]{RouvoetAPKV20}), we might miss edges.
In this case, we find the constraint that is responsible for the missing edge, and all constraints that (transitively) share a variable with this constraint.
Some of these constraints may correspond to a different hole.
Then, we peek at that hole for solutions and try insert those solutions in our current state.
Solving this state should create the missing edge.
Next, we can resume our backwards traversal.
%
Since we over-approximate the missing edges ($\hookrightarrow$ over-approximates, and the future edges may also have different target scopes), we preserve completeness.
In addition, we do not change the traversal itself, so we also preserve soundness.

In the example, we detect that constraint~(2) is responsible for the missing edge.
As this constraint shares a unification variable with constraint~(1), we detect it is related to $h_A$.
Thus, we insert a solution for $h_A$ (\eg, \id{A}) in the state, after which we can find the solution \id{x} for $h_x$.


\subsection{Recursive Qualifiers}%
\label{subsec:recursive-qualifiers}

Another heuristic concerns \emph{recursive qualifiers}.
We consider a reference recursive if multiple qualifiers in the reference resolve to the same declaration.
For example, consider a Java class \id{A} with field \Java|int x| and a field \Java|A a|.
In this case, a hole referring to \id{x} has an infinite number of possible solutions: \Java|x|, \Java|a.x|, \Java|a.a.x|, etc.
In these cases, each \id{a} refers to the same declaration.

To explain how we optimize synthesizing these references, we consider two of the states the synthesis of \Java|a.a.x| goes through.
After some steps, the synthesis reaches the following state:
%%
\[
  \kappa_1 = \left\langle
    \SG
    \mkern3mu \middle| \mkern3mu \mathsf{resolveQVar}(s_{\id{A}}, \UV{q}_1, \UV{s}_1)
    \mkern3mu \middle| \mkern3mu \set{ \UV{q}_1 \mapsto h, \ldots }
    \mkern3mu \middle| \mkern3mu \set{ h \mapsto \big(s_\id{A} \cdot s_\id{x}, \UV{q}_1.\id{x}\big), \ldots }
  \right\rangle
\]

\noindent
Some forks of this branch will arrive at \Java|a.x| as a solution for this hole.
However, on the path to synthesizing \Java|a.a.x|, the following state will also be reached:
%%
\[
  \kappa_2 = \left\langle
    \SG
  \mkern3mu \middle| \mkern3mu
    \mathsf{resolveQVar}(s_{\id{A}}, \UV{q}_2, \UV{s}_2)
  \mkern3mu \middle| \mkern3mu
    \set{ \UV{q}_2 \mapsto h, \ldots }
  \mkern3mu \middle| \mkern3mu
    \set{ h \mapsto \big(s_\id{A} \cdot s_\id{A} \cdot s_\id{x}, \UV{q}_2.\id{a}.\id{x}\big), \ldots }
  \right\rangle
\]

\noindent
These states are very similar: they share the same scope graph, have $\alpha$-equivalent constraints, and the same target scope in the hole state of $h$.
For that reason, we can conclude that \emph{each solution} for $\UV{q}_1$ is \emph{also a solution} for $\UV{q}_2$, as any derivation starting in $\kappa_1$, is also valid in $\kappa_2$.
Therefore, we can avoid synthesizing the same solution multiple times by reusing solutions for $\UV{q}_1$ when synthesizing $\UV{q}_2$.
As the traces are equivalent, we preserve soundness and completeness.

Algorithmically, we achieve this in the following three steps:
(1)~We detect pairs of solver states where
    (a)~the constraints are equivalent up to $\alpha$-renaming,
    (b)~the hole's term in the recursive solver state is an instantiation of the term in the base state, and
    (c)~the hole has the same target scope.
  For these states, we stop further synthesis on the recursive state.
(2)~For each ground solution for the variable in the base state, we emit a solution derived from the recursive state.
(3)~Repeat from step~2, using this new solution.

In the example above, this detects that $\kappa_2$ is a recursive state with respect to $\kappa_1$.
Thus, solutions from other branches rooted in $\kappa_1$ (such as \Java|a.x|) are reused in the variable $\UV{q}_2$ in $\kappa_2$, yielding \Java|a.a.x|.

This expansion is interleaved with executing other search branches, in order to guarantee liveness.
In the case no base solutions exist, this immediately terminates a search branch that would otherwise run infinitely without returning results.
This ensures that we are terminating on recursive instances that match this pattern.
When we assume the predicates that model references do not generate new scopes, only a finite number of recursive instances can be generated (because there exist a finite number of scopes in the scope graph, and a finite number of rules in a specification; hence only a finite number of states that are not equivalent according to the definition in step~1 above exist.).
That implies that we can only have non-termination when the recursive reference predicate generates fresh scopes, which typical specifications do not do.

The \hyperref[sec:evaluation]{next section} evaluates our implementation, which is based on these heuristics, providing evidence that we indeed preserve soundness and completeness.
