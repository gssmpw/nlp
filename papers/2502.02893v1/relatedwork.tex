\section{Related Work}
\label{Related Work}


\subsection{Sentiment Classification Techniques}

Sentiment classification techniques have evolved significantly over the years, encompassing a range of methods from traditional machine learning to advanced deep learning approaches. Early techniques primarily relied on lexicon-based methods, which use predefined lists of sentiment-laden words to determine the sentiment polarity of a text. Taboada et al. \cite{taboada2011lexicon} describe these methods as simple and interpretable but often limited by contextual nuances and the need for extensive manual maintenance of lexicons.

Machine learning approaches have advanced sentiment classification. Pang et al. \cite{pang2002thumbs} and Sebastiani \cite{sebastiani2002machine} illustrate the use of Support Vector Machines (SVM), Naive Bayes, and logistic regression, which learn from labeled data to classify sentiment. These techniques benefit from handling a wide range of features extracted from text, such as n-grams and part-of-speech tags, but their performance heavily relies on the quality and size of the labeled training data.

In recent years, deep learning techniques have significantly improved sentiment classification. Kim \cite{kim2014convolutional} and Hochreiter and Schmidhuber \cite{hochreiter1997long} demonstrate the efficacy of Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs), including Long Short-Term Memory (LSTM) networks. These models are effective at capturing complex syntactic and semantic patterns crucial for understanding sentiment in context. Transformer-based models like BERT \cite{devlin2018bert} have further advanced the field by enabling fine-tuning on specific sentiment analysis tasks with state-of-the-art performance. 


Despite these advancements, challenges such as domain adaptation and computational resource requirements remain areas of active research. Combining multiple techniques, such as ensemble methods and hybrid approaches, as suggested by Zhang et al. \cite{zhang2018deep} and Yang et al. \cite{yang2019xlnet}, is a promising direction to enhance the robustness and accuracy of sentiment classification systems.


\subsection{Challenges in Manual Annotation and Machine Learning}
The process of manual annotation, essential for training supervised machine learning models, is not only labor-intensive and costly but also demands domain-specific expertise.

Deng et al. \cite{Deng2009} highlighted the substantial human effort required for image annotation in the development of the ImageNet database, which parallels the challenges faced in text annotation tasks. Similarly, Snow et al. \cite{Snow2008} evaluated annotations performed by non-experts for natural language tasks, revealing significant time investments and the necessity for annotators to possess both domain-specific knowledge and understanding of machine learning principles to ensure high-quality annotations.


\subsection{Background on Pre-trained Models}

The rapid advancement of pre-trained models has dramatically shifted the landscape of natural language processing, setting new benchmarks for what is achievable with modern AI technologies. Among the most notable advancements, Brown et al. \cite{Brown2020} introduced GPT-3, a state-of-the-art language model capable of performing various NLP tasks such as text completion, translation, and summarization, with minimal or no task-specific training data. This model is a prime example of zero-shot learning capabilities, where extensive pre-training on diverse datasets allows the model to generalize to new tasks.

While GPT can be applied in various tasks, its use scenarios often involve interactive text generation rather than direct application in structured tasks like large-scale classification, particularly in scenarios involving extensive volumes of user reviews. This usage tends to be computationally expensive and demands substantial resources, which can limit its practicality for continuous, large-scale deployment. Moreover, handling large volumes of text can lead to computational bottlenecks and efficiency issues due to the significant resource demands \cite{strubell-etal-2019-energy, chen2023frugalgptuselargelanguage, Narayanan2021, Rae2021, Brown2020}.




In contrast to the broad generative applications of GPT-3, BERT (Bidirectional Encoder Representations from Transformers), introduced by Devlin et al. \cite{devlin2018bert}, represents another milestone in the evolution of NLP. BERT advanced the understanding of the context in the text by processing words in relation to all other words in a sentence rather than one-directionally. This deeper understanding of context makes BERT especially effective when fine-tuned with an additional output layer, enabling it to perform well in a wide range of tasks, including text classification.

However, the deployment of BERT and its derivatives for direct classification tasks presents substantial challenges. These models typically require fine-tuning on labeled data to perform effectively in specific applications, addressing particular classification challenges \cite{devlin2018bert}. The fine-tuning process itself requires a deep understanding of machine learning workflows and the ability to optimize model parameters. Insights into these strategies are provided by Sun et al. \cite{sun2019finetune}, who discuss the nuances of fine-tuning BERT for text classification tasks. Moreover, Dodge et al. \cite{dodge2020finetune} explore the impact of various factors such as weight initializations and data orders on the fine-tuning of pre-trained language models like BERT. Additionally, Kovaleva et al. \cite{kovaleva2019secrets} examine the inner workings of BERT, underscoring the complexity of its behavior and the critical importance of thoroughly understanding its mechanisms to optimize performance effectively.