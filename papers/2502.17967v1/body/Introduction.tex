\section{Introduction}

% 调整引言与标题之间的间距
% \vspace{-6mm} % 减少标题与引言之间的空白

% \twocolumn[{
% \begin{quote}
% \hsize = \linewidth
% \centering
% \textit{\footnotesize ``$\dots$ in mathematical philosophy, the ancients proceeded in a different manner; and as they proposed to themselves the investigation of quantities by geometry, they considered the magnitude of these quantities directly and without using equations.'' -- Isaac Newton}
% \end{quote}
% }]

Recent advancements in large language models (LLMs) have demonstrated exceptional proficiency across various domains, achieving state-of-the-art performance in natural language processing (NLP) tasks such as translation~\citep{TransLLaMa}, summarization~\citep{exploregptquery}, and reasoning~\citep{LLM-ARC, reasoning}. While LLMs excel in language-based tasks, further progress in numerical and geometric reasoning is essential to tackling complex, interdisciplinary challenges, particularly in fields like finance and scientific research. Benchmarks such as \textsc{GSM8K}~\citep{cobbe2021gsm8k} and \textsc{MATH}~\citep{MATH} have been developed to assess and improve the mathematical problem-solving abilities of LLMs through structured datasets and standardized evaluation protocols. These benchmarks not only advance LLM development but also represent a crucial step in enhancing the ability of artificial intelligence systems to solve mathematical reasoning problems.
 
\begin{figure}
	\centering
	\includegraphics[width = \linewidth]{figure/motivation1.pdf}
	\caption{\textbf{Illustration and Performance Comparison of Textual and Visual (K-Line Chart) Numerical Inputs.} 
	\textbf{Top:} The left figure presents numerical data in K-line chart format, while the right figure shows textual numerical inputs. 
	\textbf{Bottom:} LLMs with visual inputs (orange) significantly outperform those with textual inputs (blue) in 40-day yield trends, demonstrating the advantage of visual data in financial decision-making.}
	\label{motivation}
	\vspace{-3pt}
\end{figure}

In response, recent updates to LLMs have focused on improving their mathematical abilities, with models~\citep{math_servey, math_discover} achieving new records on math-focused benchmarks. However, many of these benchmarks~\citep{Galactica, mathbenchmark} primarily evaluate performance on problems familiar to the models, closely resembling those encountered during training and often following standard formats, solution strategies, or recurring mathematical patterns. This raises concerns about whether LLMs genuinely possess reasoning abilities for novel numerical data or whether their success is largely due to memorization and pattern recognition. These limitations underscore the need for alternative evaluation paradigms that assess LLMs' ability to generalize mathematical principles to unseen scenarios.

% To this end, we introduce the \textit{Agent Trading Arena}, a virtual numerical game designed to generate diverse numerical data for LLMs to analyze and uncover underlying patterns, thereby reducing the influence of human priors and memorization. Inspired by economic games ~\citep{guo2024economics}, the \textit{Agent Trading Arena} is a zero-sum game where agents compete in stock investment, gaining at the expense of others. This design prevents the emergence of universal optimal strategies, ensuring no predefined correct solution exists and encouraging agents to adapt dynamically to the environment. While the game simulates certain aspects of real-world stock trading, such as transaction dynamics, it avoids incorporating predefined rules or domain-specific knowledge. By eliminating human priors and explicit memorization, this environment provides a controlled setting to test LLMs' ability to autonomously deduce and apply underlying numerical laws.

%To address these limitations, we introduce the \textit{Agent Trading Arena}, a virtual numerical game designed to generate diverse numerical data through agent interactions in a zero-sum stock market environment. In this system, LLM-based agents make trading decisions based on a series of virtual historical stock prices; the new stock prices are dynamically determined by agents' bidding activities. Unlike most static benchmarks with predefined optimal strategies, our simulation forces agents to continuously adapt to evolving market conditions. Any temporarily optimal strategy is quickly countered in this dynamic setting, fostering an environment that rewards adaptability. By mimicking competitive trading scenarios where outcomes depend on real-time agent decisions, we create a controlled yet dynamic environment that tests LLMs' ability to infer hidden patterns and generalize numerical laws without relying on memorization or prior knowledge.

To address these limitations, we introduce the \textit{Agent Trading Arena}, a virtual numerical game generating diverse numerical data through agent interactions in a zero-sum stock market environment. In this system, LLM-based agents make trading decisions based on historical stock prices, dynamically determined by agents' bidding activities. Unlike static benchmarks~\citep{mathbenchmark} with predefined strategies, our simulation forces agents to adapt to evolving market conditions. Any temporarily optimal strategy is quickly countered, fostering an environment that rewards adaptability. By mimicking competitive trading scenarios where outcomes depend on real-time agent decisions, we create a controlled yet dynamic environment that tests LLMs' ability to infer hidden patterns and generalize numerical laws without relying on memorization or prior knowledge.

% 为什么要加入zero-sum game
% To mitigate the influence of human priors and memorization on specific problem types, we introduce a virtual numerical game, the Agent Trading Arena, designed to generate diverse numerical data for LLMs to identify and uncover underlying patterns. Inspired by economic games, the Agent Trading Arena is a zero-sum game where LLMs compete against each other. While the game simulates certain aspects of real-world stock trading, such as transaction dynamics, it does not incorporate predefined rules or domain-specific knowledge. By eliminating human priors and avoiding explicit memorization, this environment provides a clean, controlled setting to test LLMs' ability to autonomously deduce and apply numerical laws.

Our experiments reveal that LLMs struggle with textual numerical data, often focusing on absolute values rather than capturing percentage changes and relations between data points, akin to algebraic reasoning. Additionally, LLMs tend to overemphasize recent data while overlooking earlier information, even when explicitly highlighted in the prompt. These limitations suggest that LLMs lack the ability to abstract numerical information into higher-level representations, a crucial skill for generalizing beyond explicit values.

% Our extensive experiments reveal that LLMs struggle to interpret numerical data presented in plain text effectively. These models tend to fixate on specific numerical values, failing to capture broader trends or understand relationships between data points, such as changes in magnitude or overall variations. Moreover, LLMs tend to overemphasize recent data, neglecting earlier information even when explicitly highlighted as important. These limitations suggest that LLMs lack the ability to abstract numerical information into higher-level representations, which is crucial for tasks requiring generalization beyond explicit values.

% 为什么引入visualized

In contrast, LLMs perform better when presented with numerical data in visualized formats, such as scatter plots, line charts, and bar graphs, which involve geometric reasoning. Our experiments show that LLMs processing visual numerical data consistently outperform those processing textual numerical data. As shown in \autoref{motivation}, visual numerical data enables LLMs to achieve significantly higher return rates in the \textit{Agent Trading Arena}, underscoring the advantages of structured visual representations.

%Surprisingly, LLMs exhibit improved performance when presented with numerical data in visual formats, such as scatter plots, line charts, and bar graphs. Our experiments demonstrate that LLMs receiving visualized numerical data consistently outperform those processing textual numerical data. As illustrated in \autoref{motivation}, visualized numerical data enables LLMs to achieve significantly higher return rates in the \textit{Agent Trading Arena}, underscoring the advantages of structured visual representations.


Incorporating the reflection module~\citep{reflect, reflexion} further emphasizes the performance gap between textual and visual data, with the module proving particularly effective for visual representations. By leveraging structured visualizations, the reflection module enhances the model's ability to reason through complex data relations, leading to more accurate and strategic decision-making.


% To further validate our findings, we extend our evaluation to the real-world \textsc{NASDAQ Stock} dataset, which includes historical trading data. The consistent results suggest an intriguing conclusion: LLMs demonstrate stronger reasoning capabilities when interpreting visual geometric data compared to textual numerical data, highlighting a distinction between their strengths in geometry and algebra.

The \textit{Agent Trading Arena} provides a valuable framework for advancing LLM generalization and adaptability, with broad applications in fields like finance, healthcare, and scientific research. To further validate our findings, we extend our evaluation to real-world financial data, such as \textsc{NASDAQ Stock} dataset, which includes historical trading data. The consistent results suggest an important contribution: LLMs exhibit stronger reasoning with visual geometric data compared to textual numerical data, demonstrating the value of visual representations in enhancing LLM performance.

% visualized可以更近一步提高
% These benefits are further enhanced by incorporating a reflection module~\citep{reflect}, a widely used component in LLM-based agent architectures. The reflection module improves the model's ability to reason through complex data relations, leading to more accurate and strategic decision-making. To validate our findings, we extended our evaluation to the real-world \textsc{NASDAQ Stock} dataset, which includes historical trading data. The consistent results suggest an intriguing conclusion: LLMs demonstrate stronger reasoning capabilities when interpreting visualized geometric data compared to processing textual numerical data, highlighting a distinction between their strengths in geometry and algebra.