\section{Related Works}

% 通过train记忆

\subsection{Mathematical Benchmarks for LLMs}

Math word problems (MWPs) have been widely studied, leading to the development of various benchmarks for evaluating models' mathematical reasoning and problem-solving abilities. Early datasets, such as MAWPS~\citep{MAWPS}, standardized existing problems to facilitate consistent evaluation. Math23K~\citep{Math23k} introduced a large-scale collection of Chinese arithmetic problems that require structured equation solving. To increase diversity, benchmarks like ASDiv~\citep{ASDiv} and SVAMP~\citep{SVAMP} provide richer annotations and a broader range of problem types. More recent benchmarks, including GSM8K~\citep{cobbe2021gsm8k} and MATH~\citep{MATH}, focus on multi-step reasoning and advanced mathematical concepts, broadening the scope of evaluation. Additionally, MathQA-Python~\citep{MathQA_py}, a Python variant of MathQA~\citep{MathQA}, emphasizes programmatic reasoning, while MGSM~\citep{MGSM} extends these benchmarks to multilingual contexts. Despite these advancements, current models~\citep{math_reason_survey, lu2023chameleon} primarily rely on memory-based answering strategies learned through extensive training, rather than demonstrating true mathematical reasoning.

% \vspace{-4pt}

\subsection{LLMs for Enhanced Mathematical Reasoning}

Building on these benchmarks, LLMs have advanced mathematical problem-solving by incorporating specialized datasets into their training. Models such as Galactica~\citep{Galactica}, PaLM-2~\citep{palm2}, Minerva~\citep{Minerva}, and LLaMA-2~\citep{llama2} leverage extensive datasets during pre-training, improving their mathematical reasoning abilities and understanding of complex concepts. Fine-tuned models like MetaMath~\citep{MetaMath}, MAmmoTH~\citep{MAmmoTH}, and WizardMath~\citep{WizardMath} are specifically tailored for mathematical tasks. These models undergo domain-specific fine-tuning with carefully curated datasets, enabling them to tackle advanced reasoning challenges and significantly enhance performance on specialized benchmarks.

However, existing methods~\citep{mathbenchmark, Qwen2} often rely on large training datasets, raising concerns about the true reasoning capabilities of LLMs. While these methods primarily assess performance on established problem types, the heavy reliance on extensive training data suggests that models may achieve high performance through memorization and pattern recognition, rather than genuine reasoning. Consequently, alternative evaluation paradigms are needed to more accurately assess LLMs' ability to generalize mathematical principles to novel scenarios.
