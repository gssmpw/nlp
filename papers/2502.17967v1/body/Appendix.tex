\clearpage

\appendix

\section{Agent Trading Arena}
\label{appendix_arena}

\subsection{Agent Trading Arena Details}

As illustrated in \autoref{Agent}, each agent's workflow integrates LLMs for chat pool interactions, stock analysis, decision-making, and reflection. In the stock analysis and decision-making modules, all outputs are validated for consistency with both common sense and operational requirements before execution. 

\begin{figure*}[ht]
	\centering
	\includegraphics[width = \linewidth]{figure/SQIAgent1.pdf}
	\caption{\textbf{Agent Workflow Components:} Share Trading involves stock analysis, decision validation, and trade execution; Environment and Memory manage memory and process trade orders; and Reflection focuses on strategy assessment and refinement based on feedback.}
	\label{Agent}
	%\vspace{-4pt}
\end{figure*}

\subsubsection{Action Decision-Making}

Action generation follows the LLM framework. The agent responsible for generating actions receives corresponding prompts via SQLite. Based on these prompts and specified output formats, the agent decides whether to buy, sell, or hold stocks. The action generation process is outlined in \autoref{Action}, with input prompts shown in \autoref{action} and the corresponding outputs displayed in the adjacent figure.
\begin{equation}\label{Action}
	\begin{cases} 
	A_{\mathrm{date} + 1}^t = \Psi \left(\mathrm{Ins}, Z_{\mathrm{date}}^t, S_{\mathrm{date}} \right), & \text{if~} t \text{~is Iters}, \\
	A_{\mathrm{date}}^{t + 1} = \Psi \left(\mathrm{Ins}, Z_{\mathrm{date}}^t, S_{\mathrm{date}} \right), & \text{otherwise},
	\end{cases}
\end{equation}
where $\mathrm{Ins}$ represents the environment introduction, $Z_{\mathrm{date}}^t$ denotes the memory of the stock transaction on day $\mathrm{date}$ retrieved from the database, and $S_{\mathrm{date}}$ is the strategy for day $\mathrm{date}$ generated via reflection. 

\begin{figure}[ht]
	\centering
	\includegraphics[width = \linewidth]{figure/action.pdf} 
	\caption{\textbf{Inputs and Outputs in Action Decision-Making.}}
 	\label{action}
	%\vspace{-4pt}
\end{figure}

\subsubsection{Environmental Interaction}
\label{apppendix_Enxir_inteusedraction}

To isolate external influences, we created a virtual sandbox environment where each agent is assigned a unique ID, and their actions affect the environment. The function $\phi$ facilitates environmental interactions, as shown in \autoref{Algorithm_env}, where ``OPS'' retrieves agent actions, ``date'' refers to the trading date, and ``Z'' represents the memory used for interaction with the environment. Through $\phi$, each agent's actions, such as buying or selling stocks, determine the current stock price and update the trading platform, including stock prices and available shares. Stock prices are independent of external factors and are influenced solely by the sandbox's internal dynamics. The stock price is updated with each transaction according to the following formula:
\begin{equation}\label{price_update}
 	\begin{aligned}
 	\mathrm{Price}_{\mathrm{curr}} & = \delta \left(Q, F, \mathrm{Price}_{\mathrm{curr}}, \mathrm{Price}_{\mathrm{deal}} \right) \\
 	 & = \frac{\mathrm{Price}_{\mathrm{deal}} \cdot Q \cdot F + \mathrm{Price}_{\mathrm{curr}} \cdot Q_{\mathrm{total}}}{Q \cdot F + Q_{\mathrm{total}}},
 	\end{aligned} 
\end{equation}
where $Q$ is the quantity of stock traded, $F$ is the fluctuation constant, $\mathrm{Price}_{\mathrm{curr}}$ is the current stock price, $Q_{\mathrm{total}}$ is the total number of shares available, and $\mathrm{Price}_{\mathrm{deal}}$ is the price at which the trade occurs. 

\SetAlgorithmName{Algorithm}{Algorithm}{List of Algorithms}
\SetAlFnt{\small}
\SetKwInput{KwIn}{Input}
\SetKwInput{KwOut}{Output}

\begin{algorithm}
\caption{\small Environmental Interaction}
\label{Algorithm_env}
\KwIn{OPS: Function to retrieve agents' actions, 
     date: The current trading date, 
     Z: Memory used for interaction with the environment}
\KwOut{Z}
\For{$P \in \text{Persons}$}{
 	$A \leftarrow \text{OPS}(t, P)$\;
 	$O, N, Q, \text{Price}_{\text{deal}} \leftarrow \text{Extract}(A)$\;
 	$\text{Price}_{\text{curr}}, Q_{\text{total}} \leftarrow \text{Stocks}(N)$\;
 	$\text{Price}_{\text{curr}} \leftarrow \delta(Q, F, \text{Price}_{\text{curr}}, \text{Price}_{\text{deal}})$\;
 	
 	\If{$O = $ ``buy''}{
 	$\text{Cash} \leftarrow \text{Price}_{\text{curr}} \cdot Q$\;
 	\If{$\text{Cash} < P.\text{Cash}$}{
 	$Z \leftarrow \text{SubmitOrder}(O, N, t, \text{Price}_{\text{curr}}, Q)$\;
 	}
 	}
 	
 	\If{$O = $ ``sell''}{
 	$\text{Hold} \leftarrow P(N)$\;
 	\If{$\text{Hold} \neq \text{None}$}{
 	$Q_N \leftarrow \text{Hold}[\text{``Q''}] - Q$\;
 	\If{$Q_N > 0$}{
 	$Z \leftarrow \text{SubmitOrder}(O, N, t, \text{Price}_{\text{curr}}, Q)$\;
 	}
 	}
 	}
 	}

$\text{Market}(\text{date}, \text{Persons})$\;
\Return Z\;
\end{algorithm}

The function $\phi$ executes trading orders and updates the stock price in real-time based on the agent's actions. To prevent excessive volatility and mitigate risk during trading cycles, a daily price fluctuation cap is enforced. Before executing any transaction, each agent evaluates its available funds and refrains from proceeding if insufficient capital is available.

\subsubsection{Memory} 

The superior performance of LLM-based agents arises from the extensive internal knowledge acquired during pre-training. The large number of parameters in LLMs enables the retrieval of diverse information and supports logical and inferential reasoning. To further enhance knowledge retrieval across various tasks, we incorporate a memory module that empowers LLM-based agents with self-improvement capabilities. This memory module facilitates strategy reflection through time-series feedback. Unlike qualitative tasks, quantitative feedback evolves incrementally with subtle differences, presenting a challenge for the generalization of existing LLM-based agents.

To minimize the influence of pre-existing knowledge, we assign specific roles to each agent. The memory module records sensory inputs and accumulates valuable experiences based on immediate feedback following actions. These experiences are stored in a database for future reference. The interaction history between an agent's behavior and the environment constitutes short-term memory, enabling the agent to retain recent events. The historical trajectory is defined as:
\begin{equation}\label{Eq1}
 	M_{\mathrm{date}}^t = \zeta \left(I_{\mathrm{date}}^t, \mathrm{Out}_{\mathrm{date}}^t \right),
\end{equation}
where $\zeta$ processes key information, $I_{\mathrm{date}}^t$ represents the input prompt, and $\mathrm{Out}_{\mathrm{date}}^t$ denotes the resulting output.

The day's trajectory constitutes short-term memory, which is expressed as:
\begin{equation}\label{Eq2}
 	Z_{\mathrm{date}}^t = \left(M_{\mathrm{date}}^0, M_{\mathrm{date}}^1, \dots, M_{\mathrm{date}}^t\right).
\end{equation}

The process for updating short-term memory is given by:
\begin{equation}\label{Eq3}
 	Z_{\mathrm{date}}^{t + 1} = M_{\mathrm{date}}^t \cup Z_{\mathrm{date}}^t, \quad t \in \{0,1,\dots,T\},
\end{equation}
where $Z_{\mathrm{date}}^t$ represents the short-term memory, and $T$ denotes the maximum number of iterations.

The reflection model serves as long-term memory, enabling self-reflection and the consolidation of knowledge. 



\begin{figure*}
	\centering
	\includegraphics[width = \linewidth]{figure/k.pdf}
	\caption{\textbf{Visualization of Stock Inputs and Corresponding Trading Strategy Outputs.}}
	\label{fig:withphoto}
	%\vspace{-4pt}
\end{figure*}

\subsubsection{Reflection}
\label{appendix_reflection}

We propose a strategy distillation method that transforms quantitative results into descriptive text, which is then used as prompts for LLMs. This approach aids in the analysis of results and the generation of actionable, qualitative summaries, enabling LLMs to derive new strategies. These strategies are implemented, monitored, and evaluated over time, while underperforming strategies are archived for future review.

Initially, we evaluate the dayâ€™s trajectory memory and associated strategies. The evaluation function is defined as:
\begin{equation}\label{evaluation}
	E_{\mathrm{date}} = \Psi \left(\mathrm{Ins}, Z_{\mathrm{date}}^T, S_{\mathrm{date}} \right),
\end{equation}
where $\Psi$ represents the evaluation function based on LLMs, $\mathrm{Ins}$ contains the agent role descriptions and output requirements, and $Z_{\mathrm{date}}^T$ denotes the memory for the given day.

Based on the evaluation results and previous strategies, we generate the latest strategy. Past strategies are stored in a library and scored. For the new strategy, we select the top five best-performing and the bottom five worst-performing strategies to provide both positive and negative feedback. The strategy update formula is:
\begin{equation}\label{reflection}
	S_{\mathrm{date} + 1} = \Psi \left(\mathrm{Ins}, Z_{\mathrm{date}}^T, E_{\mathrm{date}}, S_{\mathrm{date}} \right),
\end{equation}
where $E_{\mathrm{date}}$ represents the evaluation results for the day, and $S_{\mathrm{date}}$ is the strategy for that day.

Long-term memory is generated through reflection, as represented by:
\begin{equation}\label{Eq7}
	Z'_{\mathrm{date}} = \left(S_0, S_1, \dots, S_{\mathrm{date}} \right).
\end{equation}

The process for updating long-term memory is defined as follows:
\begin{equation}\label{Eq8}
\begin{aligned}
	& Z'_{\mathrm{date} + 1} = S_{\mathrm{date}} \cup Z'_{\mathrm{date}}, \\
	& \mathrm{date} \in \{0, 1, \dots, \mathrm{DAYS}\}.
\end{aligned} 
\end{equation}
where $Z'_{\mathrm{date}}$ represents the long-term memory, and $\mathrm{DAYS}$ denotes the maximum number of days.

Together, short-term and long-term memory provide essential context for the agents. Success in this environment depends on their ability to understand the game rules and develop strategies that outmaneuver competitors. Agents continuously refine their strategies based on incremental quantitative feedback, adjusting their actions to align with long-term objectives. 

\begin{figure*}
	\centering
	\includegraphics[width = \linewidth]{figure/all.pdf}
	\caption{\textbf{Overview of Complete Workflow.}}
	\label{fig:diagram}
	%\vspace{-4pt}
\end{figure*}

\section{\textsc{NASDAQ Stock}}
\label{appendix_NASDAQ}

This study selects seven stocks from the NASDAQ exchange: AAPL, AMZA, GOOGL, MSFT, NFLX, NVDA, and TSLA. These stocks represent leading companies in the technology, energy, and automotive sectors, providing high market representativeness and significant trading volumes. The NASDAQ stock dataset from Yahoo Finance spans July 3, 2023, to October 29, 2024, excluding weekends and holidays. This dataset reflects current market trends and serves as a timely foundation for our research. It includes daily records of opening price, closing price, highest price, lowest price, and trading volume, as well as relevant technical indicators, offering a comprehensive view of market behavior.

The training and testing periods for MACD, StockFormer~\citep{stockformer}, TimesNet~\citep{timesnet}, 
% StockMixer~\citep{stockmixer}, 
and our system are shown in \autoref{real_stock}. StockFormer and TimesNet require longer training periods, with their training datasets spanning a broader time range compared to the other models. In contrast, our system does not require training and relies solely on historical stock data for making trading decisions. 

\begin{table}[ht]
	\centering
	\footnotesize
	\setlength{\tabcolsep}{2.5pt}
	\begin{tabular}{lrccc}
	\toprule[1.1pt]
	\multirow{2}{*}{Strategy} & \multicolumn{2}{c}{Training} & \multicolumn{2}{c}{Testing} \\
	\cmidrule(lr){2-3} \cmidrule(lr){4-5}
	& \multicolumn{1}{c}{Start} & End & Start & End \\
	\midrule
	MACD & 29/7/2024 & 30/8/2024 & 3/9/2024 & 29/10/2024 \\
	StockFormer & 3/7/2023 & 30/8/2024 & 3/9/2024 & 29/10/2024 \\
	TimesNet & 3/7/2023 & 30/8/2024 & 3/9/2024 & 29/10/2024 \\
	% StockMixer & 3/7/2023 & 30/8/2024 & 3/9/2024 & 29/10/2024 \\
	Ours & 16/8/2024 & 30/8/2024 & 3/9/2024 & 29/10/2024 \\
	\bottomrule[1.1pt]
	\end{tabular}
	\caption{\textbf{Training and Testing Periods for Various Models on the \textsc{NASDAQ Stock} Dataset.}}
	\label{real_stock}
	%\vspace{-4pt}
\end{table}

The testing period is set from September 3, 2024, to October 29, 2024, to prevent potential data leaks that could provide prior knowledge to the GPT-4o system. This timeframe ensures the fairness of the evaluation by mitigating biases from such leaks, thereby enhancing the reliability of the experimental outcomes.

\section{Visualization Input}
\label{appendix_D} 

\autoref{fig:withphoto} illustrates the system's input prompts and corresponding outputs during the strategy update process. The input prompts consist of both textual and visual components, including daily K-line charts, transaction histories, and agent trading volumes, all of which inform the strategy update.

\section{Simulation Process}
\label{appendix_E} 

In the Agent Trading Arena, the simulation process unfolds as follows: First, rumors are generated in the chat pool based on the previous day's stock market analysis. Next, historical stock data is analyzed, followed by decision-making and execution. Short-term memory is formed through interactions with the environment. Finally, the system evaluates this memory, updates the strategy, and consolidates it into long-term memory. This entire process is illustrated in \autoref{fig:diagram}.

\section{Simulation Process}
\label{appendix_F}

The experiments involved several LLMs, including LLaMa-3~\citep{LLaMa3}, GPT-4o~\citep{GPT4o}, DeepSeek~\citep{DeepSeek3v}, Qwen-2.5~\citep{qwen2.5}, and Gemini-1.5~\citep{Gemini1.5}, corresponding to the models Meta-LLaMa-3-70B-Instruct, gpt-4o-2024-08-06, DeepSeek-chat, Qwen2.5-72B-Instruct, and Gemini-1.5-pro.
