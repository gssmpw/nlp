\section{Introduction}
\label{sec:intro}

In recent years, Large Language Models (LLMs) have undergone rapid development, demonstrating impressive capabilities across a wide range of applications, from natural language processing to code generation and complex problem-solving~\cite{liu2023codegeneratedchatgptreally, satpute2024can}.
However, these advances have raised concerns about potential risks associated with the vast knowledge stored in these models, e.g., the inadvertent retention of personally identifiable information (PII)~\cite{jang2022knowledge}, the propagation of unsafe or biased behaviors~\cite{liu2024saferlargelanguagemodels}, and the unauthorized use of copyrighted content~\cite{eldan2023s}.
Furthermore, there is an increasing imperative for LLMs to comply with regulatory standards such as the General Data Protection Regulation (GDPR)~\cite{hoofnagle2019european}, which enforces the ``Right to be Forgotten''~\cite{dang2021right}.
To address these concerns, researchers are investigating various unlearning techniques~\cite{jia2024soul} to selectively remove specific knowledge from pre-trained LLMs while preserving their general language modeling capabilities, thereby avoiding the substantial computational costs associated with building new models from scratch.

\input{tfsrc/fig_musecase.tex}

\input{tfsrc/fig_overalltask.tex}

% \input{tfsrc/tab_briefcomp.tex}

The growing significance of LLM unlearning has heightened the importance of rigorous evaluation or audit of unlearing performance. Recent benchmarks like MUSE~\cite{shi2024muse} and TOFO~\cite{maini2024tofu} assess unlearning efficacy across multiple dimensions, ranging from verbatim text retention to embedded knowledge preservation. 
These pioneering frameworks have advanced the field by establishing standardized datasets, providing pre-trained target models, and introducing multifaceted evaluation metrics. However, their audit suites remain constrained in scopeâ€”for instance, MUSE employs only 100 test questions to evaluate 0.8M corpora. From an auditing perspective, such limited test coverage may inadequately assess the targeted knowledge removal, potentially compromising the comprehensive evaluation of unlearning effectiveness.

Our investigation reveals two fundamental challenges in holistic audit dataset synthesis. The primary concern about \textit{\textbf{audit adequacy}} stems from  simply relying on GPT-4 for automated QA generation from forget corpora. While this approach can generate multiple question-answer pairs for each target text, it introduces significant uncertainty in whether the generated questions comprehensively cover all the critical information contained within the source text.
The second challenge involves \textbf{\textit{knowledge redundancy}} between forget and retain corpora. As illustrated in \autoref{fig:overalltask}, shared knowledge should be preserved during the unlearning process. However, current evaluation methods fail to account for test cases where the information targeted also appears in the retain dataset, as demonstrated in \autoref{fig:musecase}.



In this paper, we propose \sys, a novel automated framework for holistic audit dataset generation that leverages knowledge graphs (KGs) to address the aforementioned limitations.
Benefiting from advances in named entity recognition and information extraction, various tools now enable efficient conversion of unstructured text into structured entity-relation graphs. 
\sys first converts both forget and retain corpora into structural knowledge graphs. By treating each KG edge (i.e., one fact) as a minimal unit, we can explicitly control the coverage of the audit process. 
Subsequently, by identifying and eliminating identical facts within the forget and retain KGs, we remove redundant knowledge from the forget KG, ensuring a well-defined audit scope. 
Finally, \sys utilizes specific facts to guide LLMs in generating high-quality, targeted test questions, guaranteeing comprehensive and accurate auditing.
Through this pipeline, \sys automatically generates large-scale, comprehensive audit datasets for any given forget and retain corpora, thereby providing robust support for LLM unlearning evaluation.

In summary, our contributions are as follows:
\begin{itemize}
    \item We introduce \sys\footnote{\url{https://anonymous.4open.science/r/HANKER-FB86}}, a novel and automated framework for generating holistic audit datasets for LLM knowledge unlearning, which addresses the challenge of \textit{audit adequacy} and \textit{knowledge redundancy}.
    \item We apply \sys to popular benchmark MUSE, significantly expanding the dataset scale and identifying knowledge memorization cases in unlearned LLMs that exceeded previous findings by three orders of magnitude ($10^3\times$).
    \item Our experimental results reveal that knowledge redundancy has a substantial impact on the assessment of unlearning effectiveness.
\end{itemize}
% \jwp{Not sure about exisiting contributions.}

% However, existing works evaluate LLM unlearning effectiveness through a limited number of manually crafted or ad-hoc automatically generated test questions (e.g., using ChatGPT), lacking comprehensive coverage of knowledge information in the forgetting dataset. 
% Moreover, successful LLM unlearning necessitates effective knowledge erasing on the forget dataset while maintaining capabilities on the retain dataset.
% Existing works often evaluate performance on the forget set and retain set independently, overlooking potential knowledge overlap between these sets (such as common sense knowledge) that should not be forgotten according to the standard task specification of unlearning.


% In this paper, we propose a comprehensive framework for automated unlearning verification in Large Language Models (LLMs) through knowledge graph-guided test generation. Our approach leverages knowledge graphs for two critical advantages. First, knowledge graphs enable fine-grained knowledge representation and decomposition, allowing us to systematically capture and analyze the intricate relationships between concepts, entities, and facts within the forget set. This granular representation ensures comprehensive coverage of the knowledge to be unlearned, as it can identify implicit connections and hierarchical relationships that might be overlooked in traditional text-based approaches.
% Second, the structured nature of knowledge graphs facilitates precise conflict detection between the forget set and retain set through graph alignment techniques. 
% By representing both sets as interconnected knowledge graphs, we can efficiently identify potential conflicts.
% This graph-based conflict detection provides a robust foundation for ensuring the integrity of the unlearning operation while preserving essential knowledge.


% Building upon this knowledge graph foundation, we introduce an effective approach that leverages state-of-the-art LLMs (such as GPT-3.5) to automatically generate high-quality test cases. Our method guides the LLM using the structured information from the knowledge graphs to create test questions that precisely target the specific knowledge points. This automated test generation process ensures the coverage completeness of the unlearning requirements while maintaining consistency with the original context and complexity of the knowledge. The resulting framework enables systematic and scalable auditing of LLM unlearning mechanisms.