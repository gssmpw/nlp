\section{Related Work}
\label{sec:rw}

\noindent\textbf{Machine Unlearning for LLMs}. Machine unlearning, a technique first established for classification challenges~\cite{bourtoule2021machine}, has progressively evolved toward applications in large language models. Contemporary research predominantly explores parameter optimization methodologies, achieved through targeted fine-tuning procedures~\cite{yao2023large, jang2022knowledge,wang2024selective,yao2024machine,tian2024forget, liu2024learning,gu2024second, jia2024soul}
The transparent nature of modifying neural architectures engenders enhanced user trust, despite potential compromises to overall model performance. Beyond parameter-based approaches, researchers have pioneered diverse methodologies including advanced contrastive decoding frameworks~\cite{eldan2023s,wang2024rkld,ji2024reversing,huang2024offsetunlearninglargelanguage}, 
task-specific vector implementations~\cite{liu2024saferlargelanguagemodels,dou2025avoidingcopyrightinfringementlarge}, contextual learning strategies~\cite{pawelczyk2024incontextunlearninglanguagemodels,muresanu2024unlearnablealgorithmsincontextlearning}, and sophisticated input processing mechanisms~\cite{gao2024practicalunlearninglargelanguage, liu2024largelanguagemodelunlearning}. 

\noindent\textbf{Evaluation of LLM Unlearning.}
The evaluation unlearning effectiveness of LLM encompasses diverse task scenarios. Early research focused on traditional NLP classification tasks to examine models' prediction~\cite{chen2023unlearnwantforgetefficient}. Subsequently, researchers developed specialized datasets to provide standardized evaluation platforms~\cite{eldan2023s, shi2024muse,maini2024tofu}. 
Besides some work has been devoted to focusing on the robustness of unlearning, i.e., adding perturbations or rewrites to the same problem to activate model memory~\cite{joshi-etal-2024-towards}.
% Beyond dataset creation, the emergence of multidimensional forgetting metrics has significantly advanced the field. However, current approaches lack comprehensive evaluation frameworks for assessing fine-grained unlearning effects at scale.


\noindent\textbf{Knowledge Graphs for Evaluation.}
Knowledge graphs offer distinct advantages beyond the completeness and identifiability properties utilized in this study. They serve as effective tools for evaluating both QA systems~\cite{Wangkgdi2024} and LLM unlearning~\cite{wu2024evaluatingdeepunlearninglarge}. Notably, knowledge graphs enable the assessment of model reasoning capabilities through transitive relationships (if a→b and b→c, then testing whether the model infers a→c).
The framework we propose in this paper conveniently integrates with these techniques.