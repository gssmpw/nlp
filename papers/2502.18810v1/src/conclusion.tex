\section{Conclusion}
\label{sec:conclusion}

In this paper, we introduce \sys, an automated framework for generating holistic audit datasets to evaluate the effectiveness of LLM unlearning. By leveraging knowledge graphs, HANKER addresses two critical challenges in unlearning evaluation: ensuring audit adequacy and eliminating knowledge redundancy between the forget and retain datasets. 
Our empirical analysis on the popular MUSE benchmark demonstrates that \sys can significantly expand the scale of audit datasets, identifying thousands of knowledge memorization cases that previous benchmarks failed to detect, and revealing how
knowledge redundancy significantly skews unlearning effectiveness metrics.