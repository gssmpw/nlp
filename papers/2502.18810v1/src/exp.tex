\section{Experiments}
\label{sec:exp}

\subsection{Experimental Setup}
\label{sec:exp_setup}

% \subsubsection{Unlearning Methods}
% \label{sec:exp_setup_unlearn}


% \textit{\textbf{Datasets and Finetuned Models.}}
Building upon MUSE, a comprehensive benchmark for LLM unlearning that provides extensive datasets and evaluation frameworks ~\cite{shi2024muse}, we integrate \sys to enhance its capabilities. For question generation, we leverage the state-of-the-art DeepSeek-V3 model~\cite{liu2024deepseek}, which has demonstrated superior performance in recent evaluations. The MUSE framework incorporates two primary data domains—NEWS and BOOKS—and includes a specially adapted LLaMA2-7B model that has undergone thorough training on the complete dataset. This fine-tuned model serves as the input for various unlearning techniques.

\textit{\textbf{Unlearning Methods.}} In our evaluation, we investigate three representative unlearning methods, each employing distinct strategies to achieve knowledge removal while preserving model utility.We utilize the default implementations, configurations, and scripts provided in MUSE~\cite{shi2024muse}.
Gradient Ascent(GA) operates by inverting the conventional training objective, maximizing the likelihood loss on forgotten data to discourage the generation of memorized content. 
Negative Preference Optimization (NPO) reframes the unlearning problem through the lens of preference optimization, treating forgotten knowledge as negative examples. 
Task Vectors (TV) implements unlearning through a novel weight arithmetic approach. The method first creates a reinforced model by training on forgotten content, then derives a task vector representing the direction of memorization. Unlearning is achieved by subtracting this vector from the original model weights, effectively steering the model away from the memorized information.
GA and NPO can be further enhanced with two utility preservation strategies: Gradient Descent on Retain set (GDR) and KL Divergence Regularization (KLR). 

\textit{\textbf{Metrics.}}
We evaluate the effectiveness of unlearning through our generated audit suite by quantifying the number of \textit{knowledge memorization cases (KMCs)} in the unlearned model. 
Unlike existing work that assess unlearning based on overall response similarity across the entire dataset, our method applies software testing principles to pinpoint specific failure-revealing test cases—scenarios in which an LLM provider might be liable for disclosing sensitive information. 
The identification process employs two complementary criteria for judgment. The first criteria uses ROUGE Recall to measure surface-level similarity, requiring model outputs to exceed a strict threshold (Recall=1) compared to reference answers. 
The second metric leverages an entailment-based approach ~\cite{yuan2024closer}, utilizing a pre-trained NLI model as described in ~\cite{sileo2024tasksource} to verify semantic equivalence between generated and reference answers without logical inconsistencies.
A higher frequency of detected memorization cases indicates less successful unlearning, while simultaneously demonstrating the comprehensiveness of our testing methodology.


\subsection{Details of Generated Audit Suite}



We applied \sys to two corpora provided by MUSE, namely the NEWS and BOOKS datasets.
The details are summarized in \autoref{tab:exp_datasetdetail}.
For the NEWS dataset, \sys extracted a knowledge graph (KG) from the forget dataset comprising 24,763 facts. After removing redundant knowledge, a final KG containing 16912 facts was obtained, from which 69,609 QA pairs were generated (On average, one fact corresponds to the generation of 4.11 QA pairs).
Similarly, for the BOOKS dataset, \sys extracted a KG with 41,123 facts from the forget dataset. Following the elimination of redundant knowledge, a final KG comprising 27,254 facts was produced, and 111,855 QA pairs were generated from this KG (on average, one fact corresponds to the generation of 4.10 QA pairs).
These results demonstrate the capability of \sys to automatically extract fine-grained knowledge graphs and generate large-scale audit suites.

\input{tfsrc/tab_datasetdetail.tex}
\input{tfsrc/tab_quality.tex}
\input{tfsrc/tab_failurenumber.tex}
\noindent\textbf{Mannual Assessment of the Generated Data.}
To rigorously assess the quality of HANKER's generated audit dataset, we conducted a detailed manual evaluation on randomly sampled 100 text chunks from each of the NEWS and BOOKS datasets. Our assessment focused on both the accuracy of extracted knowledge triples and the quality of generated QA pairs through four key metrics.
Accuracy of Knowledge Fact (AK) measures the precision of knowledge triple extraction from the source text, achieving scores of 0.76 and 0.61 for NEWS and BOOKS respectively. The relatively lower score on BOOKS reflects the inherent challenges in extracting structured knowledge from narrative text compared to more factual NEWS articles.
Question-Fact Relevance (QR) evaluates how well generated questions align with both the context and extracted facts. High scores of 0.91 (NEWS) and 0.84 (BOOKS) indicate that our framework effectively translates extracted knowledge into contextually appropriate questions.
Question Clarity (QC) assesses the linguistic quality and specificity of generated questions. Near-perfect scores of 0.99 across both domains demonstrate our system's exceptional ability to generate clear, unambiguous, and well-formed questions regardless of source material complexity.
Answer-Context Consistency (AC) gauges whether generated reference answers accurately reflect the source context. Strong performance of 0.91 (NEWS) and 0.84 (BOOKS) suggests reliable answer generation that maintains fidelity to the original text.
These results demonstrate \sys's capability in generating high-quality audit datasets, particularly excelling in question generation.



\input{tfsrc/fig_impact_redundancy.tex}

\subsection{Evaluation on Unlearning Methods}




Our result reveals a striking disparity in the ability to detect knowledge memorization cases between \sys's comprehensive audit suite and MUSE's baseline approach. The results paint a concerning picture about the extent of retained knowledge in supposedly unlearned models that were previously undetectable with limited audit sets.
On the NEWS dataset, \sys's detection capability proves remarkably more sensitive: using the ROUGE metric, it identifies over 4,600 memorization cases in the unmodified model, compared to just 33 cases detected by MUSE - a 142-fold increase in detection power. This gap widens even further when examining semantic understanding through the Entailment metric, where \sys detects more than 23,600 cases versus MUSE's 19 cases, representing a dramatic 1,242-fold improvement in identifying retained knowledge.
The BOOKS dataset tells an equally compelling story. \sys's comprehensive evaluation uncovers more than 4,700 memorization cases using ROUGE (compared to MUSE's 25 cases), and a remarkable 38,388 cases using Entailment (versus MUSE's 15 cases). These findings represent average improvements of 188× and 1,125× respectively in detection capability.

Particularly noteworthy is how these results persist across different unlearning methods. Even with state-of-the-art approaches like \(GA_{KLR}\) and \(NPO_{KLR}\), \sys consistently reveals significantly more cases where knowledge removal was incomplete.
This suggests that current unlearning methods may be less effective than previously thought, with their apparent success potentially being an artifact of insufficient testing rather than genuine knowledge removal.

These findings underscore the critical importance of comprehensive testing in evaluating unlearning effectiveness, revealing that the challenge of selective knowledge removal may be substantially more complex than indicated by previous benchmarks.

\subsection{Impact of Knowledge Redundancy on Unlearning Effectiveness Audits}
\label{sec:exp_conflict}



To validate the necessity of knowledge redundancy detection and elimination, we conducted a comprehensive experiment to assess its impact on unlearning evaluation effectiveness. Using the NEWS dataset as our testbed, we compared evaluation outcomes between two scenarios: one using the full dataset (126,224 test cases) and another using our deduplicated dataset (69,609 test cases). Our analysis considered both the number of identified knowledge memorization cases and standard dataset-level metrics (ROUGE and Entailment scores) used in existing evaluations.
The results reveal a striking impact of knowledge redundancy on evaluation outcomes. When using our deduplicated audit set, the number of identified knowledge memorization cases decreased substantially: detection rates dropped by 71.3-73.3\% under the ROUGE criterion and by 58.3-59.2\% under the Entailment criterion. This significant reduction suggests that knowledge redundancy leads to substantial false positives, where retained knowledge is incorrectly flagged as forgetting failures.
Furthermore, our analysis of quantitative metrics demonstrates that knowledge redundancy artificially inflates unlearning effectiveness measures. Without deduplication, ROUGE scores showed artificial inflation ranging from 19.7\% to 26.1\%, while Entailment scores were inflated by 32.4\% to 35.2\%. These inflated metrics indicate that traditional evaluation approaches may significantly overestimate unlearning effectiveness when redundant knowledge is not properly controlled for.

These findings provide compelling evidence for both the effectiveness of our approach and the critical importance of knowledge redundancy elimination in unlearning evaluation. The substantial reductions in false positives and metric inflation demonstrate that rigorous knowledge deduplication is essential for an accurate assessment of unlearning effectiveness.

% \input{tfsrc/fig_showpruningmean.tex}

% Our empirical analysis reveals a critical finding: near-duplicate knowledge instances exhibit disproportionately high memorization rates compared to unique knowledge elements. As demonstrated in Figure 1, the raw dataset contained 6,539 memorized cases (5.18\% memorization rate) among 126,224 total samples. After implementing our knowledge graph-based deduplication protocol, the sanitized dataset retained 69,609 samples with only 301 memorized cases (0.43\% memorization rate).

% This 12.05× reduction in memorization rate underscores two key insights: First, near-duplicate knowledge artifacts create artificial memorization hotspots that distort unlearning effectiveness audits. Second, conventional auditing approaches that fail to account for knowledge redundancy risk Type I errors by misattending model memorization to residual duplicates rather than authentic knowledge retention.

% The statistical significance of this pattern (χ² = 1687.4, df = 1, p < 0.0001) strongly validates our sanitization methodology. By eliminating approximate duplicates through knowledge graph embeddings (cosine similarity threshold ε = 0.93), we effectively surface true memorization patterns while reducing auditing noise by 95.4\%. This methodology establishes a more reliable framework for measuring authentic unlearning effectiveness compared to conventional duplicate-agnostic approaches.

% \subsection{Ablation Study: Diffrent LLMs for Question Generation (cost high and perhaps not entirely necessary?)}

% \subsection{Ablation Study on the Impact of Knowledge Graphs}
% \label{sec:exp_ablation}

% A key innovation of \sys lies in its utilization of knowledge graphs to identify fine-grained facts (the minimal testing units) and guide LLM-based QA generation by incorporating both the original text and specific facts of interest. This approach addresses a critical limitation of existing methods where, even when a text segment is used for question generation, many crucial points may remain uncovered, leading to insufficient testing coverage. To validate this design choice, we conducted ablation studies by removing the fact-guided generation component. \todo{(details to be refined)}

% \subsection{Manual Assessment of the QA Quality}
% \label{sec:exp_manual}