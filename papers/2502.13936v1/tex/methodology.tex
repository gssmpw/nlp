\section{\uppercase{Methodology}}
\label{sec:methodology}
\subsection{Problem definition}
We define a labeled baseline image dataset $ D_{\base} =\ (\vx_i,\vy_i)$, where $\vx_i $ represents the $i^{th}$  image and $\vy_i$ represents the corresponding class label of image $\vx_i $. This dataset comprises images of commercial and military planes. The dataset is partitioned into three splits: the training set split, ${D_{\train}=\ ({\vx}_i,\vy_i})$, the validation set split,  $ {D_{\val}=\ ({\vx}_i,\vy_i})$ and testing set split, ${D_{\test}=\ ({\vx}_i,\vy_i}).$ We use $D_{\train}$ to train a neural network, that consists of a backbone $f_{\theta}$ and a classifier $g_{\phi}$ (last layer of the network). 

The validation set $D_{\val}$ is used in order to save the model with the highest validation accuracy. Finally, we use $D_{\test}\ $ to calculate the test set classification accuracy. 

\subsection{Training Phase}
In each batch, we use training images along with corresponding annotations. Let $D = \{\mathbf{x}_i, \mathbf{y}_i\}_{i=1}^N$ denote the dataset, where $\mathbf{x}_i$ represents the input image, and $\mathbf{y}_i$ represents the corresponding annotations.

During the forward pass, the model predicts $\hat{y}_i$ for each input $\mathbf{x}_i$. The prediction $\hat{y}_i$ includes the bounding box coordinates, class probabilities, and objectness score. 

The total loss, $\mathcal{L}_\text{total}$, is calculated for each batch as:
\begin{equation}
    \mathcal{L}_\text{total} = \mathcal{L}_\text{obj} + \mathcal{L}_\text{cls} + \mathcal{L}_\text{bbox},
\end{equation}
where:
\begin{itemize}
    \item $\mathcal{L}_\text{obj}$ is the objectness loss.
    \item $\mathcal{L}_\text{cls}$ is the classification loss.
    \item $\mathcal{L}_\text{bbox}$ is the bounding box regression loss.
\end{itemize}

The weights of the network, $\mathbf{W}$, are updated using the Adam optimizer:
\begin{equation}
    \mathbf{W}_{t+1} = \mathbf{W}_t - \eta \nabla \mathcal{L}_\text{total},
\end{equation}
where $\eta$ is the learning rate.

Early stopping is employed to halt training if the validation loss does not improve for $p$ consecutive epochs (patience $p = 10$). The training process is summarized in Algorithm \ref{alg:train}.

\begin{algorithm}[!h]
 \caption{YOLOv8 Training Process}
 \label{alg:train}
 \KwData{Dataset $D$, configuration file $data.yaml$, pre-trained model $yolov8s.pt$, number of epochs $E=500$, patience $P=10$}
 \KwResult{Trained model with updated weights}
 
 Initialize model with pre-trained weights $yolov8s.pt$\;
 Set training parameters: $batch\_size = 16$, $epochs = 500$, $learning\_rate = 0.001667$, $optimizer = \text{AdamW}$\;
 Set data configuration file path: $data.yaml$\;
 
 \For{$epoch \gets 1$ \textbf{to} $E$}{
  \For{each batch $B$ in the training set $D$}{
   Perform forward pass on batch $B$\;
   Calculate loss using classification, localization, and confidence components\;
   Perform backward pass and update model weights using AdamW optimizer\;
  }
  
  Calculate validation loss on validation set\;
  \If{validation loss does not improve for $P$ epochs}{
   Save the model with the lowest validation loss\;
   \textbf{Break}\;
  }
 }
 \textbf{Return} the trained model with optimized weights\;
\end{algorithm}


\subsection{Inference Stage}
During the inference stage, the model processes each test image $\mathbf{x}_i$ from the test dataset $D_\text{test} = \{\mathbf{x}_j\}_{j=1}^M$ to predict the class labels and bounding boxes. The predicted class label $\hat{\mathbf{y}}_i$ for each detected object is derived as:
\begin{equation}
    \hat{\mathbf{y}}_i = \arg\max_{k \in [C]} \hat{p}_{ik},
\end{equation}
where $\hat{p}_{ik}$ is the predicted probability for class $k$, and $C$ is the total number of classes. 

The bounding box predictions are represented as $\hat{\mathbf{b}}_i = (\hat{x}_i, \hat{y}_i, \hat{w}_i, \hat{h}_i)$, where $\hat{x}_i$ and $\hat{y}_i$ denote the center coordinates of the bounding box, and $\hat{w}_i$ and $\hat{h}_i$ are its width and height. The model leverages anchor-free mechanisms to predict these bounding boxes directly at specific feature map locations, reducing the reliance on predefined anchor boxes. The bounding boxes are computed through the regression head of the network, which predicts the normalized offsets for each feature map grid cell corresponding to the detected objects.

To refine the predictions, the model applies post-processing techniques such as non-maximum suppression (NMS) to eliminate redundant bounding boxes and retain only the most confident detections. This is mathematically expressed as:
\begin{equation}
    \hat{\mathbf{b}}_i = \text{NMS}(\{\mathbf{b}_{ij}\}_{j=1}^N, \{\hat{p}_{ij}\}_{j=1}^N, \tau),
\end{equation}
where $\{\mathbf{b}_{ij}\}_{j=1}^N$ and $\{\hat{p}_{ij}\}_{j=1}^N$ are the sets of predicted bounding boxes and their associated confidence scores for image $\mathbf{x}_i$, and $\tau$ is the IoU threshold used to filter overlapping boxes.

To evaluate the model's performance, we utilize three key metrics:
\paragraph{Mean Average Precision at IoU 0.50 (mAP@0.50)} This metric evaluates the overall detection performance by calculating the average precision across all classes for a fixed Intersection-over-Union (IoU) threshold of 0.50.
\paragraph{Precision} Defined as the ratio of true positive detections to the sum of true positives and false positives. It measures how many of the predicted detections are relevant.
\begin{comment}{
    \begin{equation}
        \text{Precision} = \frac{\text{True Positives}}{\text{True Positives} + \text{False Positives}}.
    \end{equation}}
\end{comment}
\paragraph{Recall} Defined as the ratio of true positive detections to the total number of ground-truth instances. It measures the model's ability to detect relevant objects.
\begin{comment}
    \begin{equation}
        \text{Recall} = \frac{\text{True Positives}}{\text{True Positives} + \text{False Negatives}}.
    \end{equation}
\end{comment}
    

These metrics collectively provide a comprehensive evaluation of the model's performance, capturing its precision, completeness, and overall detection capability.


\begin{comment}{
\subsection{Gaussian Filtering}
Gaussian filtering is an image processing technique employed for noise reduction and image smoothing. This is accomplished by applying a filter kernel whose weights are defined by a Gaussian function. This function is a bell-shaped curve that assigns higher weights to pixels closer to the center and progressively lower weights to those further away. The Gaussian kernel is defined by the Gaussian function:
\[
G(x, y) = \frac{1}{2\pi\sigma^2} e^{-\frac{x^2 + y^2}{2\sigma^2}}
\]
where \( G(x, y) \) represents the weight at coordinates \( (x, y) \) in the kernel, and \( \sigma \) is the standard deviation controlling the spread of the distribution.

To apply Gaussian filtering to an image, the kernel is centered at each pixel, and the neighbouring pixels are multiplied by the corresponding weights in the kernel. The result is then summed to produce the filtered pixel value.
The convolution \cite{AboutConvolutions} operation (seen in Figure \ref{fig:conv}) can be expressed mathematically as:
\[
I'(x, y) = \sum_{i=-\infty}^{\infty} \sum_{j=-\infty}^{\infty} I(x+i, y+j) \cdot G(i, j)
\]

where \( I(x, y) \) is the intensity of the pixel at coordinates \( (x, y) \) in the original image, \( I'(x, y) \) is the filtered intensity at the same coordinates, and \( G(i, j) \) is the weight at coordinates \( (i, j) \) in the Gaussian kernel.

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.6\linewidth]{imgs/conv.png}
    \caption{Convolution of Image with Kernel \cite{conv}}
    \label{fig:conv}
\end{figure}

The standard deviation \( \sigma \) of the Gaussian distribution determines the amount of smoothing applied to the image. A larger \( \sigma \) value results in a wider kernel and more significant smoothing, while a smaller \( \sigma \) value produces a narrower kernel and less smoothing. Thus, \( \sigma \) controls the trade-off between blurring and preserving image details.

In the case of this project, a $3\times3$ kernel is used with the standard deviation, $\sigma$, calculated automatically by OpenCV based on the kernel size. $\sigma$ is typically chosen such that the majority of the Gaussian distribution falls within the kernel boundaries. This ensures that the blur effect is applied evenly across neighbouring pixels. $\sigma$ is expected to be relatively small for the kernel size used ($3\times3$), typically ranging from around 0.8 to 1.2.}
\end{comment}
