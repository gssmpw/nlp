\section{\uppercase{Background}}
\label{sec:background}

\subsection{Data Collection}
\label{sub:data-collection}
While existing datasets like FGVC-Aircraft provide valuable resources for aircraft recognition, they primarily focus on aircraft images captured from aerial perspectives, which do not align with the specific requirements of ground-based aircraft detection. To address this limitation, we adopted a novel data curation strategy involving a multi-step process.

We meticulously sourced images from various online platforms, including stock photo websites and aviation enthusiast forums such as \href{jetphotos.com}{JetPhotos}. This approach allowed us to gather a diverse collection of images capturing aircraft in various scenarios, with a specific focus on ground-based perspectives.

To efficiently label the large dataset, we employed a semi-automated approach leveraging the Grounding DINO model using Roboflow. This model was trained on a large-scale image-text dataset and can accurately localize objects in images given textual prompts. By providing a prompt such as "plane", the model was able to generate initial bounding box proposals.

However, to ensure high-quality annotations, each image with proposed bounding boxes was then carefully examined. Incorrect or missed detections were corrected, and additional annotations were added as needed. The final dataset split can be seen in \autoref{tab:baseline-split}.

\begin{table}
\small
\centering
\caption{Baseline Dataset Split}

\setlength\tabcolsep{4pt}
\begin{tabular}{lccc}
\toprule
Class & Training&Validation&Test\\ \midrule
Commercial &218 & 62 & 36    \\
Military& 22 & 9 & 6 \\ 

\bottomrule
\end{tabular}
\vspace{0pt}

\label{tab:baseline-split}
\end{table}

The following sections will first explore the baseline augmentation techniques -- classical data augmention Stable Diffusion and its extension, ControlNet. Building upon these foundations, we will then introduce a novel method for data augmentation: Image Compositing.
\subsection{Baselines Methods}
\paragraph {Classical Data Augmentation} These methods used were horizontal flipping, Gaussian blurring and exposure adjustment. Horizontal flipping was applied to introduce spatial variability. This technique mirrors the image along the vertical axis, effectively doubling the dataset size without altering the underlying semantic content. Gaussian blurring introduces a controlled level of noise and blurring, mimicking the effects of atmospheric conditions or sensor noise. Additionally, exposure adjustment was employed to vary the overall intensity of the image, simulating changes in illumination.

\paragraph{Stable Diffusion XL}\label{subsec:sdxl}Stable Diffusion XL is a state-of-the-art text-to-image model capable of generating highly realistic and detailed images from textual descriptions \cite{sdxl}. We provided specific prompts, such as "A photo of a military plane in sky, taken from the ground" or "A photo of a commercial plane in sky, taken from the ground," as well as negative prompts such as "cropped, close-up, low resolution, blurred, partial view, cut-off edges," to ensure they met our specific requirements. The generated images were then labeled using the approach in section \ref{sub:data-collection}.

\paragraph{Stable Diffusion XL with ControlNet}We had provided the Stable Diffusion XL model with a guidance image to influence its output, ensuring that the generated images were consistent with the desired characteristics. This was carried out using the recently published model of ControlNet \cite{controlnet}. The idea of ControlNet is to to use a conditioning input such as a segmentation maps, Canny edges and depth maps that can be used to control the generated image. In our work we utilized Canny edges as the conditioning input for the ControlNet. We used a subset of the training images and obtained their Canny edge images by applying Canny edge detection. Then we feed these Canny edge images as input to the network and in a similar way to \autoref{subsec:sdxl} we provided a prompt that will generate a plane. Our hypothesis is that using the Canny edges and the ControlNet will force the Stable Diffusion XL model to generate plane exactly in the same location as the original input images. In this way we will be able to use the original bounding box information to fine-tune our plane detector. %This technique allowed us to create highly realistic and diverse aircraft images without the need for additional data labeling as the labels associated with the guidance image could be directly applied to the generated image. 


\subsection{Image Compositing}
Image fusion techniques were employed, which involved background removal, sky integration and seam reduction, illustrated in \autoref{fig:6}. Background elements were firstly removed from images containing an aircraft, isolating the foreground object --- the aircraft. The foreground aircraft objects were then integrated onto sky background images captured from a ground perspective. The foreground aircraft was then rotated by an angle between $0^\circ$ to $10^\circ$ and flipped horizontally, increasing robustness of training data. To enhance image realism, Gaussian filtering was applied to blur the boundaries between the foreground aircraft and the background sky, minimising visible seams.

\begin{figure*}[h]
\begin{center}
   \includegraphics[width=\textwidth, height = 0.22\textwidth]{imgs/image_fusion_steps.PNG.jpg}
\end{center}
   \caption{Data Generation Using Image Composition}
\label{fig:6}
\end{figure*}

\paragraph{Gaussian Filtering} An image processing technique employed for noise reduction and image smoothing. This is accomplished by applying a filter kernel whose weights are defined by a Gaussian function. This function is a bell-shaped curve that assigns higher weights to pixels closer to the center and progressively lower weights to those further away. 
\begin{comment}{
The Gaussian function used to generate the filter kernel is defined as:

    \begin{equation}
    G(x, y) = \frac{1}{2\pi\sigma^2} \exp\left(-\frac{x^2 + y^2}{2\sigma^2}\right),
\end{equation}}
\end{comment}

The Gaussian filter is applied to an image by convolving the Gaussian kernel with the image. 
\begin{comment}{
Mathematically, this convolution can be expressed as:
\begin{equation}
    I'(u, v) = \sum_{x=-k}^{k} \sum_{y=-k}^{k} G(x, y) \cdot I(u-x, v-y),
\end{equation}
where:
\begin{itemize}
    \item $I(u, v)$ is the original image intensity at pixel $(u, v)$,
    \item $I'(u, v)$ is the filtered image intensity at pixel $(u, v)$,
    \item $k = \lfloor 3\sigma \rfloor$ determines the size of the kernel.
\end{itemize}}
\end{comment}