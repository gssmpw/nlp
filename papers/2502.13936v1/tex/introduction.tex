Deep learning models, particularly Convolutional Neural Networks (CNNs) have revolutionized the field of computer vision, achieving state-of-the-art performance on a wide range of tasks, including image classification and object detection. However, the performance of these models is heavily reliant on the availability of large, high-quality datasets. In many real-world scenarios, obtaining sufficient training data can be challenging, especially for specific domains or rare classes.

To address this limitation, data augmentation has been shown to produce promising ways to increase the accuracy of classification tasks, to artificially expand training datasets. Previous research has explored various data augmentation techniques such as traditional methods, such as rotation, flipping, and cropping \cite{perez2017effectivenessdataaugmentationimage}, and generative adversarial networks (GANs) to generate synthetic images \cite{8388338}. Some other works have changed images' semantics using an off-the-shelf diffusion model, which generalizes to novel visual concepts from a few labeled examples \cite{trabucco2023effectivedataaugmentationdiffusion}, another study has used Multi-stage Augmented Mixup (MiAMix), which integrates image augmentation into the mixup framework, utilizes multiple diversified mixing methods concurrently, and improves the mixing method by randomly selecting mixing mask augmentation methods \cite{pr11123284}.

One specific area that faces the challenge of scarce labeled data is aircraft detection. Accurate and timely identification of aircraft is crucial in various sectors, including airspace security, airport traffic management, and military applications \cite{Arwin:09}. 

In this paper, we propose a novel data augmentation method for this application that combines elements from multiple images to create a new, synthetic image, which we will refer to as Image Compositing. Impressively, we show that our method outperforms other complex generative model techniques such as multi-modal diffusion models \cite{sd}. 