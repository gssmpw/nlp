\section{\uppercase{experiments}}
\label{sec:experiments}

\subsection{Setup}
\paragraph{Datasets}The final augmented dataset can be seen in \autoref{tab:augmented-split}.

\begin{figure*}[t]
    \centering
        \caption*{Generated Images from Stable Diffusion XL}
    % Row 1: Stable Diffusion XL
    \begin{minipage}{\textwidth}
        \centering
        \begin{minipage}{0.18\textwidth}
            \centering
            \includegraphics[width=\textwidth]{imgs/SDXL1.jpg}
        \end{minipage} \hfill
        \begin{minipage}{0.18\textwidth}
            \centering
            \includegraphics[width=\textwidth]{imgs/SDXL2.jpg}
        \end{minipage} \hfill
        \begin{minipage}{0.18\textwidth}
            \centering
            \includegraphics[width=\textwidth]{imgs/SDXL3.jpg}
        \end{minipage} \hfill
        \begin{minipage}{0.18\textwidth}
            \centering
            \includegraphics[width=\textwidth]{imgs/SDXL4.jpg}
        \end{minipage} \hfill
        \begin{minipage}{0.18\textwidth}
            \centering
            \includegraphics[width=\textwidth]{imgs/SDXL5.jpg}
        \end{minipage}
    \end{minipage}
    
        
    \vspace{1em} % Space between rows
    \caption*{Generated Images from Stable Diffusion + ControlNet}
    % Row 2: Stable Diffusion + ControlNet
    \begin{minipage}{\textwidth}
        \centering
        \begin{minipage}{0.18\textwidth}
            \centering
            \includegraphics[width=\textwidth]{imgs/SDCN1.png}
        \end{minipage} \hfill
        \begin{minipage}{0.18\textwidth}
            \centering
            \includegraphics[width=\textwidth]{imgs/SDCN2.png}
        \end{minipage} \hfill
        \begin{minipage}{0.18\textwidth}
            \centering
            \includegraphics[width=\textwidth]{imgs/SDCN3.png}
        \end{minipage} \hfill
        \begin{minipage}{0.18\textwidth}
            \centering
            \includegraphics[width=\textwidth]{imgs/SDCN4.png}
        \end{minipage} \hfill
        \begin{minipage}{0.18\textwidth}
            \centering
            \includegraphics[width=\textwidth]{imgs/SDCN5.png}
        \end{minipage}
    \end{minipage}
    
    \vspace{1em} % Space between rows

    % Row 3: Image Composition
    \caption*{Generated Images from Image Composition}
    \begin{minipage}{\textwidth}
        \centering
        \begin{minipage}{0.18\textwidth}
            \centering
            \includegraphics[width=\textwidth]{imgs/IC1.jpg}
        \end{minipage} \hfill
        \begin{minipage}{0.18\textwidth}
            \centering
            \includegraphics[width=\textwidth]{imgs/IC2.jpg}
        \end{minipage} \hfill
        \begin{minipage}{0.18\textwidth}
            \centering
            \includegraphics[width=\textwidth]{imgs/IC3.jpg}
        \end{minipage} \hfill
        \begin{minipage}{0.18\textwidth}
            \centering
            \includegraphics[width=\textwidth]{imgs/IC4.jpg}
        \end{minipage} \hfill
        \begin{minipage}{0.18\textwidth}
            \centering
            \includegraphics[width=\textwidth]{imgs/IC5.jpg}
        \end{minipage}
    \end{minipage}
    \vspace{1em}
    \caption{Images Generated using Different Methods}
    \label{fig:generated_images}
\end{figure*}



\begin{table}
\small
\centering
\caption{Augmented Dataset Split}

\setlength\tabcolsep{4pt}
\begin{tabular}{lccc}
\toprule
Class & Training&Validation&Test\\ \midrule
Commercial &307 & 73 & 36    \\
Military& 338 & 43 & 6 \\ 
\bottomrule
\end{tabular}
\vspace{0pt}

\label{tab:augmented-split}
\end{table}

\paragraph{Network} In our work, we had used YOLOv8 and had fine-tuned for our custom datasets. YOLOv8 employs pretrained backbones such as CSPDarknet53. These pretrained weights, $\mathbf{W}_\text{pretrained}$, initialize the model to improve convergence.

\paragraph{Implementation Details}
Our implementation is based on Python. We utilized the Ultralytics YOLOv8 framework for model training and inference, with PyTorch serving as the underlying deep learning library for GPU-accelerated computations. For image augmentation and preprocessing tasks, OpenCV, NumPy, \texttt{diffusers} and \texttt{transformers} libraries.

\paragraph{Hyperparameters}
In the experiment, the batch size is implicitly set by the available GPU memory, but it typically defaults to 16 for optimal performance. The model was trained for 500 epochs, with early stopping enabled by setting the patience to 10. The AdamW optimizer was used, with a learning rate of 0.001667 and momentum of 0.9. The optimizer was configured with parameter groups, where different decay rates were applied to various parts of the model. Specifically, the weight decay for the first group of parameters was set to 0.0 (no weight decay), while for the second group (weights), the decay was set to 0.0005. The third group (bias parameters) had a weight decay of 0.0, ensuring that bias terms did not undergo regularization. The loss function employed is a combination of objectness, classification, and bounding box regression losses, tailored for object detection tasks. We use the validation set to calculate the validation accuracy and save the model with the highest validation accuracy through comparisons at each epoch. The performance of the model is monitored using metrics such as mean Average Precision (mAP), precision, and recall, which are calculated at each epoch to track the modelâ€™s detection accuracy on the test set.

\subsection{Ablation Study}
In this study, we conducted a series of experiments to evaluate the impact of different data augmentation techniques on model performance. For each dataset, we applied a distinct augmentation method and compared it to the baseline, which involved training the model on the original unmodified data without any augmentation. The performance of each approach was assessed using mAP@0.50, precision, and recall. These metrics were calculated for each augmented dataset and presented in a comparative manner in \autoref{tab:augmentation-results}.

\begin{figure*}[ht]
  \centering
\includegraphics[width=0.9\textwidth]{imgs/combined_results.png}  
\caption{Prediction Results for Each Augmented Dataset}
  \label{fig:res}
 \end{figure*}
\subsection{Performance Comparison}
\autoref{tab:augmentation-results} provides a quantitative comparison of precision, recall, and mAP@0.50 metrics across different data augmentation techniques. The baseline model, trained on the original dataset without augmentation, showed moderate performance with an mAP@0.50 of 0.654. In contrast, classical data augmentation techniques such as flipping and blurring significantly improved performance, achieving an mAP@0.821. The proposed Image Compositing method outperformed all other techniques, with the highest mAP@0.911, precision of 0.904, and recall of 0.907. \autoref{fig:generated_images} visually supports these findings by showcasing sample images generated through each augmentation method. The superior performance of Image Compositing when compared to advanced generative models like Stable Diffusion can be attributed to the distribution shift between the source images and the images generated by Stable Diffusion. This shift is evident in \autoref{fig:generated_images}, particularly in the first row, where the airplanes in the images generated by Stable Diffusion noticeably differ from the airplanes in our dataset. %The qualitative comparison highlights the realism and diversity achieved by Image Compositing. 

\autoref{fig:res} visually corroborates the quantitative results presented in \autoref{tab:augmentation-results}. The baseline model exhibits a high number of missed detections and incorrect labeling, resulting in a low precision score as shown in \autoref{tab:augmentation-results}. The classical augmentation method showed an improvement over the baseline, with a notable increase in detection accuracy. However, some aircraft remain undetected, aligning with the higher recall score compared to the original dataset. 

Image compositing gives the best results with accurate and confident bounding box predictions for all aircraft. The model effectively handles cluttered backgrounds and distant objects, which is consistent with the scores in \autoref{tab:augmentation-results}. While showcasing improved performance over the original dataset, the model trained with Stable Diffusion showed some inconsistencies in the bounding box predictions, aligning with its scores which are higher than the baseline but lower than Image Compositing. Stable Diffusion + ControlNet has a balance between precision and recall, but still falls slightly short of the performance achieved by Image Compositing, as evidenced by the scores in \autoref{tab:augmentation-results}.

\begin{table}
\small
\centering
\caption{Performance Comparison of Different Augmentation Methods}


\setlength\tabcolsep{2pt}
\begin{tabular}{lcccc}
\toprule
Method & Precision&Recall&mAP50& Epoch\\ \midrule
Original &0.558 & 0.699 &  0.654 & 2    \\
Classical& 0.856 & 0.794 &0.821& 28 \\ 
Image Compositing &0.904 & 0.907 & 0.911 & 32 \\ 
Stable Diffusion (SD) &0.718 & 0.809 &0.808& 25   \\ 
SD+ControlNet  & 0.874 & 0.703 &0.854 & 37  \\ 

\bottomrule
\end{tabular}
\vspace{0pt}

\label{tab:augmentation-results}
\end{table}



\subsection{Verification of Hypotheses}
The experiments were designed to validate that advanced augmentation methods, including generative models, would improve object detection performance over classical methods and that Image Compositing, as a novel augmentation strategy, would outperform state-of-the-art generative models in both precision and recall.

The results supported both hypotheses. Stable Diffusion XL and Stable Diffusion XL with ControlNet demonstrated significant performance gains (mAP@0.808 and mAP@0.854, respectively) over the baseline model, confirming the effectiveness of advanced augmentation methods. 
Moreover, the superior performance of Image Compositing across all metrics validated its position as the most effective augmentation method tested.

\begin{comment}\subsection{Discussion}
This study demonstrated the effectiveness of diverse data augmentation techniques, including classical methods and advanced generative models like Stable Diffusion XL and ControlNet. By incorporating image composition, we further enhanced the realism and diversity of synthetic data. While these techniques have shown promising results, several avenues for future research remain. These include exploring semi-supervised learning, fine-tuning generative models for domain-specific data, addressing out-of-distribution generation, expanding training dataset diversity, optimizing computational efficiency, developing hybrid augmentation strategies, and incorporating temporal and video data augmentation. By addressing these challenges, we can further advance the state-of-the-art in object detection and unlock new possibilities for real-world applications.

\end{comment}

\begin{comment}
\subsection{Future Work}
In this study, we have demonstrated the effectiveness of various data augmentation techniques, including classical methods, advanced generative models like Stable Diffusion XL and ControlNet and image compositing, for enhancing the performance of object detection models. However, several opportunities remain for further research and refinement of these techniques.

One promising direction is the integration of semi-supervised learning methods. By leveraging unlabeled data, we can potentially further improve model performance. Additionally, fine-tuning generative models like Stable Diffusion XL on domain-specific datasets can enhance the quality and realism of synthetic images, leading to more effective data augmentation.

Another challenge that warrants further investigation is the out-of-distribution generation of images. Current generative models often struggle with generating data that aligns perfectly with the distribution of real-world data, especially for complex scenarios. Future research could explore hybrid approaches that combine generative models with traditional augmentation techniques to improve the diversity and realism of generated images.

Expanding the diversity of the training dataset is also critical for enhancing the robustness of the model. Incorporating a wider variety of aircraft types, environmental conditions, occlusions, and weather effects can lead to more robust models. Additionally, exploring the use of other generative models, such as GANs and VAEs, can provide additional avenues for enriching the training set.

The cross-domain applicability of the proposed augmentation techniques is another possible direction for future research. While our experiments focused on aircraft detection, the same methodologies could be applied to other fields, such as autonomous vehicles, medical imaging, or even drone detection. Investigating the generalizability of our framework across different domains would provide valuable insights into its broader applicability and potential limitations.

From a computational perspective, optimizing the efficiency of generative models is crucial for practical deployment. While techniques such as ControlNet show promising results, the computational cost associated with training and fine-tuning these models remains high. Future work should focus on developing more efficient generative models or hybrid architectures that maintain performance while reducing the computational burden, particularly in real-time applications.

Furthermore, exploring hybrid augmentation strategies that combine classical image transformations with more advanced techniques like diffusion models can offer a more flexible and efficient approach to data augmentation. Novel compositing methods and image blending techniques, powered by machine learning models, could further enhance the realism and diversity of synthetic images.

Lastly, temporal and video data augmentation offers a unique opportunity to extend the current framework into dynamic scenarios. By applying augmentation strategies to video sequences, it will be possible to create training datasets that account for object tracking, motion, and temporal coherence, which are critical for real-world object detection tasks in video feeds.

In conclusion, while our study has made significant strides in improving object detection through advanced data augmentation, the future of this research lies in further refining these techniques, exploring new domains, and improving both the quality and efficiency of the methods for real-world applications.
\end{comment}