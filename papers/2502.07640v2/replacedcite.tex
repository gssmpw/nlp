\section{Related Work}
\paragraph{Automated theorem proving.} 
Automated theorem proving (ATP) is a long-standing problem in symbolic AI____. Traditional approaches represent theorems in first-order logic and prove them using decision procedures____ and search____. The proof search has been enhanced by replacing handcrafted heuristics with machine learning techniques____. However, approaches based on first-order logic struggle to scale to complex theorems and often do not yield human-readable proofs.
% Automated theorem proving (ATP) aims to verify formal mathematical statements fully automatically. Different tools have been developed, including first-order provers (e.g., E ____ and Vampire ____),  Boolean satisfiability (SAT) solvers (e.g., MiniSat ____, CaDiCaL ____) and satisfiability modulo theories (SMT) solvers (e.g., Z3 ____, CVC5 ____) and etc. However these tools have limit capability in dealing with complex theorems. 



In recent years, learning-based theorem proving has undergone a significant transformation. A notable approach, introduced by ____, involves leveraging large language models to assist in theorem proving with proof assistants such as Lean____ and Isabelle____. Follow-up research has explored various avenues, such as retrieving useful lemmas____, utilizing Monte Carlo tree search for proof discovery____, and harnessing the capabilities of large language models (LLMs) for natural language reasoning____. Notably, ____ was the first to apply expert iteration____ to theorem proving. This method alternates between two phases: (1) attempting to prove unsolved theorems and (2) enhancing the prover by incorporating newly discovered proofs into its training data. Expert iteration has yielded significant improvements in several recent provers____, including our {\prover}. 

Most automated theorem provers operate in a stepwise manner, generating individual proof steps that are then assembled into complete proofs using proof search algorithms. Recently, researchers have shown that generating entire proofs is feasible____. This approach avoids the costly search process, resulting in lower latency and potentially offering a more efficient use of computational resources during testing. While {\prover} also generates whole proofs, our data and methodology can, in principle, be adapted to develop stepwise provers as well.


\paragraph{Autoformalization and synthetic data generation.} 
The shortage of high-quality formal mathematical data poses a significant bottleneck in training theorem-proving models. While techniques like reinforcement learning may reduce the reliance on human-written proofs____, there remains a need for a substantial number of formal theorem statements. A promising approach is to synthesize formal statements through autoformalization, where large language models (LLMs) translate informal mathematical statements into formal ones____. 

DeepSeek-Prover____ and InternLM2.5-StepProver____ have successfully implemented this strategy to formalize a large volume of statements into Lean for expert iteration. We adopt a similar approach. The difference is: while ____ focuses on formalizing their internal dataset, we concentrate on formalizing the Numina dataset____ alongside a privately collected dataset. Additionally, we train two formalizers to enhance the diversity of formalization styles, which we demonstrate to be beneficial in Section~\ref{sect:results}. % Additionally, InternLM2.5-Step-Prover____ primarily investigates the search algorithm and online compilation feedback. Integrating these elements with our prover could enhance performance, and we consider this a direction for future work. 
% Besides autoformalization, researchers have also explored generating synthetic datasets of formal proofs without relying on informal data ____, which lies outside the scope of our study.



% \kaiyu{We probably need to explicitly address how our work relates to ____.} Defer to Section 3.