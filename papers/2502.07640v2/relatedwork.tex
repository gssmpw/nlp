\section{Related Work}
\paragraph{Automated theorem proving.} 
Automated theorem proving (ATP) is a long-standing problem in symbolic AI~\citep{robinson2001handbook}. Traditional approaches represent theorems in first-order logic and prove them using decision procedures~\citep{de2008z3,barbosa2022cvc5} and search~\citep{kovacs2013first,schulz2019faster}. The proof search has been enhanced by replacing handcrafted heuristics with machine learning techniques~\citep{urban2011malecop,kaliszyk2018reinforcement}. However, approaches based on first-order logic struggle to scale to complex theorems and often do not yield human-readable proofs.
% Automated theorem proving (ATP) aims to verify formal mathematical statements fully automatically. Different tools have been developed, including first-order provers (e.g., E \citep{schulz2002brainiac} and Vampire \citep{kovacs2013first}),  Boolean satisfiability (SAT) solvers (e.g., MiniSat \citep{een2003extensible}, CaDiCaL \citep{queue2019cadical}) and satisfiability modulo theories (SMT) solvers (e.g., Z3 \citep{de2008z3}, CVC5 \citep{barbosa2022cvc5}) and etc. However these tools have limit capability in dealing with complex theorems. 



In recent years, learning-based theorem proving has undergone a significant transformation. A notable approach, introduced by \citet{polu2020generative}, involves leveraging large language models to assist in theorem proving with proof assistants such as Lean~\citep{de2015lean, moura2021lean} and Isabelle~\citep{paulson1994isabelle}. Follow-up research has explored various avenues, such as retrieving useful lemmas~\citep{irving2016deepmath,mikula2024magnushammer,yang2024leandojo}, utilizing Monte Carlo tree search for proof discovery~\citep{lample2022hypertree}, and harnessing the capabilities of large language models (LLMs) for natural language reasoning~\citep{jiang2022draft,lin2024lean}. Notably, \cite{polu2023formal} was the first to apply expert iteration~\citep{anthony2017thinking} to theorem proving. This method alternates between two phases: (1) attempting to prove unsolved theorems and (2) enhancing the prover by incorporating newly discovered proofs into its training data. Expert iteration has yielded significant improvements in several recent provers~\citep{wu2024internlm2,xin2024deepseekv15}, including our {\prover}. 

Most automated theorem provers operate in a stepwise manner, generating individual proof steps that are then assembled into complete proofs using proof search algorithms. Recently, researchers have shown that generating entire proofs is feasible~\citep{first2023baldur,xin2024deepseek,wang2024theoremllama}. This approach avoids the costly search process, resulting in lower latency and potentially offering a more efficient use of computational resources during testing. While {\prover} also generates whole proofs, our data and methodology can, in principle, be adapted to develop stepwise provers as well.


\paragraph{Autoformalization and synthetic data generation.} 
The shortage of high-quality formal mathematical data poses a significant bottleneck in training theorem-proving models. While techniques like reinforcement learning may reduce the reliance on human-written proofs~\citep{alphaproof}, there remains a need for a substantial number of formal theorem statements. A promising approach is to synthesize formal statements through autoformalization, where large language models (LLMs) translate informal mathematical statements into formal ones~\citep{wu2022autoformalization,wu2024internlm2,xin2024deepseek,xin2024deepseekv15}. 

DeepSeek-Prover~\citep{xin2024deepseek} and InternLM2.5-StepProver~\citep{wu2024internlm2} have successfully implemented this strategy to formalize a large volume of statements into Lean for expert iteration. We adopt a similar approach. The difference is: while \citet{liu2024deepseek} focuses on formalizing their internal dataset, we concentrate on formalizing the Numina dataset~\citep{li2024numinamath} alongside a privately collected dataset. Additionally, we train two formalizers to enhance the diversity of formalization styles, which we demonstrate to be beneficial in Section~\ref{sect:results}. % Additionally, InternLM2.5-Step-Prover~\citep{wu2024internlm2} primarily investigates the search algorithm and online compilation feedback. Integrating these elements with our prover could enhance performance, and we consider this a direction for future work. 
% Besides autoformalization, researchers have also explored generating synthetic datasets of formal proofs without relying on informal data \citep{wu2020int, wang2020learning, poesia2024learning}, which lies outside the scope of our study.



% \kaiyu{We probably need to explicitly address how our work relates to \citet{xin2024deepseekv15,wu2024internlm2}.} Defer to Section 3.