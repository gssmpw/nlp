\section{Introduction}

\begin{figure}[h]
    \centering
\includegraphics[width=0.40\linewidth]{figures/miniF2F_bar_plot.pdf}
\includegraphics[width=0.30\linewidth]{figures/inference_scale_performance.pdf}
\includegraphics[width=0.235\linewidth]{figures/leanworkbook_proofs_bar_plot.pdf}
    \caption{\danqi{the legends of the figures are too small to read.}
    The Pass@$N$ metric indicates that we generate N proofs for a single problem, and it is considered as solved if any of the $N$ proofs successfully solves the problem. (\textbf{Left}) The performance of Pass@32 for full proof generation on miniF2F. We compare with DeepSeek-Prover-v1.5 on the Pass@32 metric (Table 1 of \cite{xin2024deepseekv15}).    (\textbf{Middle}) This sub-figure presents a comparison of \prover-SFT and Deepseek-Prover-v1.5 in terms of miniF2F performance across different inference budgets, ranging from Pass@32, 64, 128, ..., $4 \times 6400$, to $16 \times 6400$. The performance numbers of Deepseek-Prover-v1.5 are copied from Table 1 of \cite{xin2024deepseekv15}. Due to computational resource constraints, we tested \prover-SFT only up to Pass@$4\times6400$. Notably, \prover-SFT's Pass@256 already exceeds the Pass@$16\times6400$ performance of Deepseek-Prover-v1.5-RL (without inference time tree search).  (\textbf{Right}) The number of problems solved in Lean-workbook by \prover-SFT compared to existing works. InternLM2.5-Step-Prover~\citep{wu2024internlm2} and InternLM-Math-Plus~\citep{ying2024internlm} collectively solve and open-source 15.7K samples, while we solve and open-source 29.7K samples.\danqi{please define macros for all models, yours and others.}\danqi{this caption is toooo long.}} %For a more detailed discussion on the comparison between \prover-Prover-SFT, Deepseek-Prover-v1.5-RL, and InternLM2.5-Step-Prover, please refer to \rd{Appendix}
    \label{fig:main_Results}
\end{figure}

\danqi{i realize the draft is still at an early stage. in this case, i will only focus on high-level comments instead of local suggestions.} Recent advancements in large language models (LLMs) have demonstrated remarkable capabilities in reasoning tasks, including solving mathematical problems \citep{wei2022chain, liu2024deepseek, yang2024qwen2}. However, these models often reason through natural language, making their reasoning processes difficult to verify. This challenge in verification can undermine trustworthiness in practical applications. Furthermore, without clear verification, it becomes more difficult to further improve the language model. Proof assistants such as Lean \citep{de2015lean, moura2021lean},  Isabelle \citep{paulson1994isabelle}, and Coq \citep{barras1997coq} provide formal languages that can express reasoning in a way that can be mechanically verified. However, writing proofs for the theorems expressed in these languages is highly non-trivial and needs significant domain expertise. Thus, it is of great interest to train LLMs to write formal proofs.\danqi{the abstract says there are two ways to approach mathematical reasoning: formal and informal mathematics, but you didn't use this terms here. the next paragraph starts with there are two lines of research for formal mathematics. feels a bit inconsistent.}  

There are primarily two lines of research focused on writing proofs in formal language. One approach involves interacting with an online verfier\danqi{what is an online verifier? does it verify proof before the whole proof is generated?} and utilizing search algorithms such as best-first search (BFS) and Monte Carlo tree search (MCTS) to find proofs \citep{jiang2021lisa, han2021proof, wang2020learning, li2024hunyuanprover, wu2024internlm2}. The second approach, whole proof generation,  centers on generating complete proofs solely based on a theorem statement \citep{jiang2022draft, wang2024theoremllama, xin2024deepseek, xin2024deepseekv15}. This method does not require iterative interaction with verifiers during proof generation, making it significantly less computationally demanding. Our work falls into this latter category.

A significant challenge in training LLMs for theorem proving in formal languages is the scarcity of data. Although there are some publicly available datasets in formal language, their size remains limited. For example, the Lean-workbook (including Lean-workbook-plus) \citep{ying2024lean, wu2024internlm2} contains a total of 140K formalized statements; however, only 15.7K of these have been proven\danqi{i don't quite understand what are the 140K-15.7K proofs mean?} and made open-source by InternLM2.5-StepProver and InternLM-Math-Plus \citep{ying2024lean, wu2024internlm2, ying2024internlm}. Additionally, the Open Bootstrapped Theorems dataset \citep{wang2024theoremllama} includes 107K statements with proofs sourced from Mathlib4 \citep{mathlib4}. While Mathlib4 offers valuable formalization, we observe a notable difference in the distribution of statements derived from it compared to general problem-solving benchmarks like the popular miniF2F \citep{zheng2021minif2f}. For instance, statements in miniF2F are largely self-contained and do not rely on definitions introduced in other statements, unlike those in Mathlib4. See appendix for examples of Mathlib4 statements.\danqi{link to a concrete example in appendix; otherwise hard to understand.} Incorporating Mathlib4 data into training does not consistently enhance performance on miniF2F, as discussed in Section~\ref{sect:results}.

Unlike the limited data available in formal languages, there is a vast amount of math problems and solutions written in natural language. For example, Numina~\citep{li2024numinamath} includes 860K high-quality question and answer pairs sourced from MATH~\citep{hendrycks2021measuring}, GSM8K~\citep{cobbe2021training}, AMC~\citep{aops_wiki}, AIME~\citep{maa_aime_2024}, the AoPS Forum~\citep{aops_wiki}, Chinese K-12 Exams~\citep{shao2024deepseekmath}, World Olympiads, and synthetic data~\citep{mitra2024orca}. We start by converting the problems in these datasets into LEAN statements. First, we train language models to formalize a large set of informal data. We then carefully filter for high-quality translations of these formal statements. To broaden the coverage of formalization styles, we train two formalizers. One is trained on natural language and formal language (NL-FL) pairs from Lean-workbook, while the other is trained on NL-FL pairs annotated by Claude-sonnet-3.5~\citep{anthropic2024a}.


Using this extensive problem set, we implement iterative training for the final prover \citep{xin2024deepseek, li2024hunyuanprover, wu2024internlm2}. We begin with Deepseek-Prover-v1.5-RL and generate 16 proof candidates for each formal statement, subsequently verifying the correctness of each candidate. The verified proofs are then collected to train our model, starting from Deepseek-Prover-v1.5-Base.\danqi{remind me again why you use the -RL model to generate data, but train the -base model?} In subsequent rounds, we utilize our own prover during the \(k\)th iteration to gather data for training the \((k+1)\)th prover. Notably, for each \((k+1)\)th prover, we train the model on Deepseek-Prover-v1.5-Base using the data collected by the \(k\)th prover. \danqi{say you observe steady improvements after x iterations? and how many iterations you have done in the end.} Such iterative method is also referred as expert iteration~\citep{polu2022formal}.\danqi{why not just mention expert iteration in the first sentence?}

The contribution of this work is summarized as follows:
 
\begin{itemize}
    \item With our large-scale formalized statements, we demonstrate that straightforward\danqi{don't need to emphasize straightforward/standard... it is okay to not claim novelty here.} iterative training can lead to state-of-the-art (SOTA) performance in the whole proof generation model. Despite training on the same base model, our model outperforms the previous SOTA model, Deepseek-Prover-v1.5-RL, in whole proof generation by 7.6\% on miniF2F. It achieves a Pass@32 score of 57.6\%, while Deepseek-Prover-v1.5-RL scores 50.0\%, as illustrated in Figure~\ref{fig:main_Results} (left). Our model significantly outperforms Deepseek-Prover-v1.5-RL across all sampling budget, including Pass@32, 64, 128, ... up to 3200, which is shown in Figure~\ref{fig:main_Results} (middle).

    \item We conduct extensive evaluations on miniF2F~\citep{zheng2021minif2f}, ProofNet~\citep{azerbayev2023proofnet}, PutnamBench~\citep{tsoukalas2024putnambench}, the entire problem set in the Lean-workbook~\citep{ying2024lean} with 140K statements, and FormalNumina (a held-out subset of our formalized Numina datasets containing 250 statements). We improve upon Deepseek-Prover-v1.5-RL by 5.1\% in average performance across miniF2F, ProofNet,Lean-workbook and FormalNumina (Table~\ref{tab:four_dataset_comparison}). \rd{(can we present average here?)} Our model solves 7 problems on Putnam benchmark by Pass@512, securing the \bt{\textbf{\#1}} position on the PutnamBench leaderboard (Table~\ref{tab:putnambench}).  Additionally, as shown in Figure~\ref{fig:main_Results} (right), we have cumulatively solved 29.7K problems in the Lean-workbook, significantly increasing the existing 15.7K proofs found by InternLM2.5-StepProver \citep{wu2024internlm2} and InternLM-Math-Plus \citep{ying2024internlm}.\danqi{mention these datasets in abstract too. is it really important to claim sota for putnambench = 7 problems? i don't know the significance here.}
    \item We are open-sourcing our code, model, and the new proofs discovered in the Lean-workbook to facilitate future research.
\end{itemize}

\section{Related Work}


\paragraph{Automated theorem proving.} 
Automated theorem proving (ATP) is a long-standing problem in symbolic AI~\citep{robinson2001handbook}. Traditional approaches represent theorems in first-order logic and prove them using decision procedures~\citep{de2008z3,barbosa2022cvc5} and search~\citep{kovacs2013first,schulz2019faster}. The proof search has been improved by replacing handcrafted heuristics with machine learning~\citep{urban2011malecop,kaliszyk2018reinforcement}. However, approaches based on first-order logic do not scale to complex theorems; nor do they produce human-readable proofs.
% Automated theorem proving (ATP) aims to verify formal mathematical statements fully automatically. Different tools have been developed, including first-order provers (e.g., E \citep{schulz2002brainiac} and Vampire \citep{kovacs2013first}),  Boolean satisfiability (SAT) solvers (e.g., MiniSat \citep{een2003extensible}, CaDiCaL \citep{queue2019cadical}) and satisfiability modulo theories (SMT) solvers (e.g., Z3 \citep{de2008z3}, CVC5 \citep{barbosa2022cvc5}) and etc. However these tools have limit capability in dealing with complex theorems. 



In recent years, learning-based theorem proving has undergone a paradigm shift. One approach pioneered by \citet{polu2020generative} and rapidly gaining traction is the use of large language models to prove theorems in proof assistants like Lean~\citep{de2015lean, moura2021lean} and Isabelle~\citep{paulson1994isabelle}. Follow-up research has explored various directions, such as retrieving helpful lemmas~\citep{irving2016deepmath,mikula2024magnushammer,yang2024leandojo}, searching for proofs using Monte Carlo tree search~\citep{lample2022hypertree}, and utilizing the capabilities of LLMs in natural language reasoning~\citep{jiang2022draft,lin2024lean}. \cite{polu2023formal} first applied expert iteration~\citep{anthony2017thinking} to theorem proving, which iterates between two phases: (1) attempting to prove unsolved theorems and (2) improving the prover by adding newly found proofs to its training data. Expert iteration has led to noticeable improvements in multiple recent provers~\citep{wu2024internlm2,xin2024deepseekv15}, including {\prover}. Most neural theorem provers are stepwise: They find proofs by generating individual steps, which are assembled into complete proofs by proof search algorithms. Recently, researchers have demonstrated the feasibility of generating the whole proof~\citep{first2023baldur,xin2024deepseek,wang2024theoremllama}. By avoiding the expensive search process, whole-proof generation achieves lower latency and is potentially a more efficient way of using test-time computing. {\prover} also generates whole proofs, though our data and method can in principle be applied to build stepwise provers.



\paragraph{Autoformalization and synthetic data generation.} 
High-quality formal mathematical data is a bottleneck in training theorem-proving models. Techniques like reinforcement learning can potentially mitigate the need for human-written proofs~\citep{alphaproof}, however, we still need a large number of formal theorem statements. A promising solution is synthesizing statements by using LLMs to perform autoformalization---translating from informal math statements to formal~\citep{wu2022autoformalization,wu2024internlm2,xin2024deepseek, xin2024deepseekv15}. DeepSeek-Prover~\citep{xin2024deepseek} and InternLM2.5-StepProver~\citep{wu2024internlm2} adopted this strategy to curate and autoformalize a large number of statements into Lean for expert iteration. We follow a similar strategy. The main difference is as follows: while \citet{liu2024deepseek} formalizes their internal dataset, we concentrate on formalizing the Numina~\citep{li2024numinamath} dataset in conjunction with a privately collected dataset.  Additionally, we train two formalizers to enhance the diversity of formalization styles, which is shown to be helpful in Section~\ref{sect:results}. % Additionally, InternLM2.5-Step-Prover~\citep{wu2024internlm2} primarily investigates the search algorithm and online compilation feedback. Integrating these elements with our prover could enhance performance, and we consider this a direction for future work. 
% Besides autoformalization, researchers have also explored generating synthetic datasets of formal proofs without relying on informal data \citep{wu2020int, wang2020learning, poesia2024learning}, which lies outside the scope of our study.



% \kaiyu{We probably need to explicitly address how our work relates to \citet{xin2024deepseekv15,wu2024internlm2}.} Defer to Section 3.
