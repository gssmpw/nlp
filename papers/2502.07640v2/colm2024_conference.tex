\documentclass{article} % For LaTeX2e
\usepackage{amssymb}
\usepackage{multirow}

\usepackage{colm2024_conference}
\usepackage{xcolor}
\newcommand{\rd}[1]{\textcolor{red}{#1}}
\newcommand{\bt}[1]{\textcolor{blue}{#1}}
\usepackage{multirow}



\newcommand{\prover}{Goedel-Prover}

\newcommand{\dsprover}{DeepSeek-Prover-V1.5}

\newcommand{\internlmstep}{InternLM2.5-Step-Prover}

\newcommand{\internlmmath}{InternLM-Math}

\newcommand{\qwen}{Qwen2.5-Coder-32B}

\newcommand{\qweninstruct}{Qwen2.5-72B-Instruct}

\newcommand{\sonnet}{Claude-sonnet-3.5}

\newcommand{\lwb}{Lean Workbook}

\newcommand{\putnam}{PutnamBench}

\newcommand{\mathlib}{Mathlib4}

\newcommand{\miniff}{miniF2F}

\newcommand{\proofnet}{ProofNet}

\usepackage{microtype}
\usepackage{hyperref}
\usepackage{url}
\usepackage{booktabs}
\usepackage[most]{tcolorbox}
\usepackage{listings}

\definecolor{darkblue}{rgb}{0, 0, 0.5}
\hypersetup{colorlinks=true, citecolor=darkblue, linkcolor=darkblue, urlcolor=darkblue}


\providecommand{\danqi}[1]{
    {\protect\color{green!60!black}{[danqi: #1]}}
}
\providecommand{\shange}[1]{
    {\protect\color{red!60!black}{[shange: #1]}}
}
\providecommand{\yong}[1]{
    {\protect\color{blue}{[yong: #1]}}
}

\providecommand{\chijin}[1]{
    {\protect\color{magenta!60!black}{[chi: #1]}}
}

\title{{\prover}: A Frontier Model for Open-Source Automated Theorem Proving}

% \title{{\prover}: Pushing the Limits of Open-source Automated Theorem Proving}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \colmfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

% \author{
% Yong Lin\thanks{YL and ST contribute equally to this work. PLI stands for \href{https://pli.princeton.edu}{Princeton Language and Intelligence}.}\\
% PLI, Princeton University \\
% \texttt{yl7690@princeton.edu} \\
% \And
% Shange Tang$^*$\\
% % Princeton Language and Intelligence\\
% Princeton University \\
% \texttt{shangetang@princeton.edu} \\
% \And
% Bohan Lyu\\
% % Department\\
% Tsinghua University \\
% \texttt{lyubh22@gmail.com} \\
% \And
% Jiayun Wu\\
% % Department\\
% Tsinghua University \\
% \texttt{jiayun.wu.work@gmail.com} \\
% \And
% Hongzhou Lin \\
% % Department\\
% Amazon\thanks{This work is independent of and outside of the work at Amazon.} \\
% \texttt{hongzhou.lin89@gmail.com} \\
% \And
% Kaiyu Yang\\
% Meta FAIR\thanks{Meta served only an advisory role. All experiments were conducted on PLI's server.} \\
% \texttt{kaiyuy@meta.com} \\
% \And
% Jia Li\\
% % Department\\
% Numina \\
% \texttt{jia@projectnumina.com} \\
% \And
% Mengzhou Xia\\
% % Princeton Language and Intelligence\\
% PLI, Princeton University \\
% \texttt{mengzhou@princeton.edu} \\
% \And
% Danqi Chen\\
% % Princeton Language and Intelligence\\
% PLI, Princeton University \\
% \texttt{danqic@cs.princeton.edu} \\
% \And
% Sanjeev Arora\\
% % Princeton Language and Intelligence\\
% PLI, Princeton University \\
% \texttt{arora@cs.princeton.edu} \\
% \And
% Chi Jin\\
% % Princeton Language and Intelligence\\
% PLI, Princeton University \\
% \texttt{chij@princeton.edu}
% % Ji Q. Ren \& Yevgeny LeNet \\
% % Department of Computational Neuroscience \\
% % University of the Witwatersrand \\
% % Joburg, South Africa \\
% % \texttt{\{robot,net\}@wits.ac.za} \\
% % \AND
% % Coauthor \\
% % Affiliation \\
% % Address \\
% % \texttt{email}
% }

\author{
Yong Lin\thanks{YL and ST contribute equally to this work. \texttt{\{yong.lin,shangetang\}@princeton.edu}}~\thanks{Princeton Language and Intelligence, Princeton University.} 
 \\
\And
Shange Tang\footnotemark[1]~\footnotemark[2]
\And
Bohan Lyu\thanks{Tsinghua University.}\\
\And
Jiayun Wu\footnotemark[3]\\
% Department\\
\And
Hongzhou Lin\thanks{Amazon. This work is independent of and outside of the work at Amazon.} \\
\And
Kaiyu Yang\thanks{Meta FAIR. KY served an advisory role.
% \danqi{shouldn't Kaiyu served an advisory role instead of Meta lol?} 
All experiments were conducted on PLI servers.} \\
\And
\quad Jia Li\thanks{Numina.} 
\quad
Mengzhou Xia\footnotemark[2]
\quad
Danqi Chen\footnotemark[2]
\quad
Sanjeev Arora\footnotemark[2]
\quad
Chi Jin\footnotemark[2]\\
% \danqi{can you reduce the spaces between the authors in the second row? it will look better.}
}







% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\newcommand\kaiyu[1]{\textcolor{red}{[Kaiyu: #1]}}

% \colmfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}


\maketitle


\begin{abstract}
We introduce Goedel-Prover, an open-source large language model (LLM) that achieves the state-of-the-art (SOTA) performance in automated formal proof generation for mathematical problems. The key challenge in this field is the scarcity of formalized math statements and proofs, which we tackle in the following ways. We train statement formalizers to translate the natural language math problems from Numina into formal language (Lean 4), creating a dataset of 1.64 million formal statements. 
LLMs are used to check that the formal statements accurately preserve the content of the original natural language problems. We then iteratively build a large dataset of formal proofs by training a series of provers. Each prover succeeds in proving many statements that the previous ones could not, and these new proofs are added to the training set for the next prover.
Despite using only supervised fine-tuning, our final prover significantly outperforms the previous best open-source model, \dsprover, which employs reinforcement learning. On the \miniff{} benchmark, our model achieves a success rate of 57.6\% (Pass@32), surpassing \dsprover{} by 7.6\%. On \putnam, \prover{} successfully solves 7 problems (Pass@512), ranking first on the leaderboard. Furthermore, it generates $29.7$K formal proofs for \lwb{} problems, nearly doubling the $15.7$K produced by earlier works.

\end{abstract}

\input{intro}




\section{Method}

We begin by translating informal statements (expressed in natural language) into formal statements (represented in Lean). Using these formal statements, we iteratively train our prover with proofs generated by the prover and verified by the Lean compiler. The details of each step are elaborated in the following parts.


\subsection{Statement Formalization}
\label{sect:statement_formalization}
% A high quality collection of formal statements is very important for training the final prover \citep{xin2024deepseek,li2024hunyuanprover}. However, open-sourced the formal statement dataset is very limited. For example, Leanworkbook and Leanworkbook-plus \cite{ying2024lean, wu2024internlm2} contain 140K formalized statement. However, \cite{ying2024lean, wu2024internlm2} only find the proofs of 16K problems. The Open Bootstrapped Theorems \cite{wang2024theoremllama} dataset contains 107K statements with proofs collected from \mathlib{} \citep{mathlib4}. \rd{However, we observe that the distribution of statements provers extracted from \mathlib{} is significantly different from the general problem sovling such as miniF2F. Adding \mathlib{} data to the training may even hurt miniF2F performance.}

% Unlike the sparsity of data in the formal language, there is a huge amount of math problems and solutions written in natural language. For example, Numina~\citep{li2024numinamath} contains 860K high-quality question and answer pairs, which originate from MATH, GSM8K, AMC and AIME, AoPS Forum, Chinese K-12 Exam, World Olympiads data, and synthetic data. So we first formalize the problems set in Numina these datasets into formal statements.

We first train the statement formalizers to translate informal statements in Numina into formal statements as shown in Figure~\ref{fig:formalizer_training}.  
To enhance the diversity of formalized statements, we train two models to formalize informal statements. 
% To maintain consistency with existing works, we use the terms ``formal statement" and ``formal language statement," as well as ``informal statement" and ``natural language statement," interchangeably.

\begin{itemize}
    \item \textbf{Formalizer A:} We train the Formalizer A model using Formal and Informal (F-I) statement pairs sourced from \lwb{}.
    \item \textbf{Formalizer B:} We employ \sonnet{} to formalize 230K statements from Numina. From this set, we extract 170K statements that successfully passed Lean compilation. These 170K F-I statement pairs are then used to train Formalizer B.
\end{itemize}
Both Formalizer A and B are trained using supervised fine-tuning with \qwen{} \footnote{\url{https://huggingface.co/Qwen/Qwen2.5-Coder-32B}}. 
The training of these two formalizers takes less than 24 hours on 8 H100 GPUs. Table \ref{tab:formalizer_comparison_two_columns} presents two examples in which both Formalizer A and Formalizer B yield reasonable formalizations. However, our final prover exhibits varying performance on these formalized statements, highlighting the influence of formalization style on model effectiveness. 
% Refer to Table \ref{tab:formalizer_comparison_two_columns} for the examples.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.45\linewidth]{figures/Train_formalizer.png}
    \includegraphics[width=0.48\linewidth]{figures/F-I_statement_pair.png}    
    \caption{This figure illustrates the training of the formalizers. The term ``F-I statement pairs" refers to pairs consisting of Formal and Informal (F-I) statements. An example is shown on the right part. We train two formalizers, Formalizer A and B, using F-I statement pairs sourced from various origins.}
    \label{fig:formalizer_training}
\end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.75\linewidth]{figures/Iteration_illustration.png}
    \caption{This figure illustrates the process of expert iteration. Each time, we utilize our iter-\( k \) prover to collect new proofs and add them to the training data. We then conduct supervised fine-tuning starting from \dsprover-Base for another round, resulting in the iter-\( (k+1) \) prover.}
    \label{fig:iterative_procedure}
\end{figure}


\begin{table}[ht]
\centering
\renewcommand{\arraystretch}{1.5} % Adjust row height
\resizebox{\linewidth}{!}{
\begin{tabular}{|p{0.15\textwidth}|p{0.4\textwidth}|p{0.3\textwidth}|}
\hline
 & \textbf{Example 1} & \textbf{Example 2} \\ \hline

\textbf{Informal Statement} & 
The function $f(x) = 2^{|x|} + 3x^2 + ax + 1$ is an even function, then $a$ equals $a=0$. &
If $x$ and $\log_{10} x$ are real numbers and $\log_{10} x<0$, show that $-1<x<0$.
 \\ \hline

\textbf{Formalizer A Output} &
\begin{minipage}[t]{\linewidth}
    \includegraphics[width=\linewidth]{figures/lwb_style_example1.png}
\end{minipage} \newline
\texttt{Pass rate: 14/16} &
\begin{minipage}[t]{\linewidth}
    \includegraphics[width=0.8\linewidth]{figures/lwb_style_example2.png}
\end{minipage} \newline
\texttt{Pass rate: 0/16} \\ \hline

\textbf{Formalizer B Output} &
\begin{minipage}[t]{\linewidth}
    \includegraphics[width=\linewidth]{figures/sonnet_style_example1.png}
\end{minipage} \newline
\texttt{Pass rate: 0/16} &
\begin{minipage}[t]{\linewidth}
    \includegraphics[width=0.8\linewidth]{figures/sonnet_style_example2.png}
\end{minipage} \newline
\texttt{Pass rate: 5/16} \\ \hline

\end{tabular}
}
\caption{Comparison of formalizer outputs for two examples. Comparison of Formalizer Outputs for Two Examples. In Example 1, Formalizer A defines the "even function" directly by stating \( f(-x) = f(x) \). In contrast, Formalizer B first introduces a function called "IsEven" and then defines the even function using "IsEven". Notably, our prover successfully solves the statements provided by Formalizer A but fails with those from Formalizer B.
Example 2 is similar; however, our prover fails to solve the statement provided by Formalizer A but succeeds with the one from Formalizer B.}
\label{tab:formalizer_comparison_two_columns}
\end{table}


% \rd{The examples of translated problems are shown in  Table XXX}. \rd{Some examples of the formalized problems} 

% Example 1:
% \begin{itemize}
%     \item Informal Statement: 
%     \begin{verbatim}'The function $f(x) = 2^{|x|} + 3x^2 + ax + 1$ is an even function, then $a$ equals  $a=0$. 
%     \end{verbatim}
%     \item Formalizer A output: \\
%     \begin{minipage}[t]{0.45\textwidth}
%         \includegraphics[width=\linewidth]{figures/lwb_style_example1.png}
%     \end{minipage}
% % theorem lwb_style_example1  (a : ℝ)
% %   (f : ℝ → ℝ)
% %   (h₀ : ∀ x, f x = 2^abs x + 3 * x^2 + a * x + 1)
% %   (h₁ : ∀ x, f (-x) = f x) :
% %   a = 0   :=  by sorry  
%     \item Formalizer B output: \\
%     \begin{minipage}[t]{0.45\textwidth}
%         \includegraphics[width=\linewidth]{figures/sonnet_style_example1.png}
%     \end{minipage}
% % def IsEven (f : ℝ → ℝ) : Prop := ∀ x, f (-x) = f x

% % theorem sonnet_style_example1 
% %     (f : ℝ → ℝ) 
% %     (h_def : ∀ x, f x = 2^(|x|) + 3*x^2 + a*x + 1) 
% %     (h_even : IsEven f) 
% %     (a : ℝ) : 
% %     a = 0 :=  by sorry
%     \item Pass rate on lwb translated: 14/16, Pass rate on sonnet tranlated 0/16
% \end{itemize}

% Example 2:
% \begin{itemize}
%     \item Informal Statement: 
%     \begin{verbatim}'If \\((x - 3)(4x^2 + 2x - 7) = Ax^3 + Bx^2 + Cx + D\\), what is the value of \\(A + B + C + D\\)?'
% \end{verbatim}
%     % 'If \\((x - 3)(4x^2 + 2x - 7) = Ax^3 + Bx^2 + Cx + D\\), what is the value of \\(A + B + C + D\\)?'
%     \item Formalizer A output: \\
%     \begin{minipage}[t]{0.65\textwidth}
%         \includegraphics[width=\linewidth]{figures/lwb_style_example2.png}
%     \end{minipage}

% % theorem lwb_style_example2  (x : ℝ)
% %   (a b c d : ℝ)
% %   (h₀ : (x - 3) * (4 * x^2 + 2 * x - 7) = a * x^3 + b * x^2 + c * x + d) :
% %   a + b + c + d = 2   :=  by sorry  



%     \item Formalizer B output: \\
%     \begin{minipage}[t]{0.65\textwidth}
%         \includegraphics[width=\linewidth]{figures/sonnet_style_example2.png}
%     \end{minipage}

%   %   theorem sonnet_style_example2 : ∀ (A B C D : ℝ),
%   % (fun x => (x - 3)*(4*x^2 + 2*x - 7)) = (fun x => A*x^3 + B*x^2 + C*x + D) →
%   % A + B + C + D = 2 :=  by sorry
%     \item Pass rate on lwb translated: 0/16, Pass rate on sonnet tranlated 15/16
% \end{itemize}


\paragraph{Quality assessment.}
We employ two tests to assess the quality of the formalized statements. First, the formalized statement must conform to Lean syntax and can successfully compile, with the potential proof replaced by the placeholder ``:= by sorry". This syntax check is known as the Compiling Correctness (CC) Test in the literature \citep{ying2024lean}. Second, the formalized statement must accurately capture the original informal problem, incorporating all assumptions, conditions, and implicit definitions. We refer to this second test as the Faithfulness and Completeness (FC) Test. For the FC test, we use \qweninstruct{}\footnote{\url{https://huggingface.co/Qwen/Qwen2.5-72B-Instruct}} with the prompts shown in Figure~\ref{fig:prompt:assessment}. For each formalized statement, we generate four independent judgments, and the FC score is calculated as \#\{``Appropriate'' in four Judgments\}/4. For example, if the four judgments produced by \qweninstruct{} include three ``Appropriate'' and one ``Inappropriate'', the overall FC score is calculated as 0.75. We filter out formalized statements with an FC score less than 0.5.

% \lstset{
%     numbers=none,
%     keywordstyle= \color{ blue!70},
%     commentstyle= \color{red!50!green!50!blue!50},
%     frame=none,
%     rulesepcolor= \color{ red!20!green!20!blue!20} ,
%     framexleftmargin=2em,
%     columns=fullflexible,
%     breaklines=true,
%     breakindent=0pt,
%     basicstyle=\ttfamily
% }

% \begin{figure}[!ht]
%     \centering
% \begin{tcolorbox}[left=0mm,right=0mm,top=0mm,bottom=0mm,boxsep=1mm,arc=0mm,boxrule=0pt, frame empty, breakable]
%     \small
%     \begin{lstlisting}
% You will receive a math problem consisting of its natural language statement and, in some cases, a natural language proof or solution, along with its formal statement in Lean 4.

% Please evaluate whether the formal Lean statement appropriately translates the natural language statement based on the following criteria:

% 1. Key Elements: The problem's essential components are correctly represented in Lean code.

% 2. Mathematical Accuracy: The translation preserves the accuracy of the mathematical content.

% 3. Structural Fidelity: The translation aligns closely with the original problem, maintaining its structure and purpose.

% 4. Comprehensiveness: All assumptions, conditions, and goals present in the natural language statement are included in the Lean translation.

% Your answer should be in the following format:

% Thought: [Your Answer]

% Judgement: [Your Answer, one of {Appropriate, Inappropriate}]
% \end{lstlisting}
% \end{tcolorbox}
%     \caption{Prompts for Faithfulness and Completeness (FC) Test. }
%     \label{fig:prompt:assessment}
% \end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.9\linewidth]{figures/prompts_FC_test.png}
    \caption{Prompts for Faithfulness and Completeness (FC) Test.}
    \label{fig:prompt:assessment}
\end{figure}


\begin{table}[t]
    \centering
    \begin{tabular}{c|ccc}
        \toprule
        Model & Pass & Formalizer A & Formalizer B\\
        \midrule
        CC Test &Pass@1&  76.74\% & 88.48\% \\
        CC Test &Pass@8& 95.93\% & 98.59\%\\
        \midrule
        FC Test &Pass@1& 48.06\% & 80.42\%\\
        FC Test &Pass@8& 88.01\% & 97.22\%\\
        \midrule
        CC + FC Test &Pass@1& 45.72\% & 76.41\%\\
        CC + FC Test &Pass@8& 82.33\% & 95.78\%\\
         \bottomrule
    \end{tabular}
    \caption{Quality assessment of the formalized statement}
    \label{tab:formalization_statement_assessment}
\end{table}


For each informal statement in Numina, we generate eight formalized statements from each formalizer, resulting in 16 formalized statements per problem. Each statement undergoes the CC and FC Test, and we retain only those valid statements. We then randomly select one valid statement from each formalizer. For example, if five out of eight statements from Formalizer A and three from Formalizer B are valid, we randomly choose one from each. If a formalizer produces no valid statements, we exclude all its statements for that problem. The statistics for each test conducted on both formalizers are summarized in Table~\ref{tab:formalization_statement_assessment}.

In addition to formalizing the 860K open-sourced Numina~\citep{li2024numinamath} datasets, we also formalize a private 68K collection of math problems from Art of Problem Solving (AOPS), which has been collected and processed by the Numina group~\citep{li2024numinamath}. Out of a total of 928K informal statements, 760K have two valid formalized statements generated by Formalizer A and B, while 123K contain only one valid formalized statement.  After formalizing both the Numina and AOPS datasets, we further incorporate 140K statements from \lwb{}, including \lwb{ Plus}. As a result, we have a total of 1.78M formal statements.


\subsection{Expert Iteration}
\label{sect:iterative}
After obtaining a large collection of formalized statements in Section~\ref{sect:statement_formalization}, we employ expert iteration to train the prover~\citep{liu2024deepseek, wu2024internlm2, li2024hunyuanprover}, which is illustrated in Figure~\ref{fig:iterative_procedure}. Specifically, we first utilize \dsprover{-RL}\footnote{\url{https://huggingface.co/deepseek-ai/DeepSeek-Prover-V1.5-RL}} to generate 16 proofs for each statement. We then verify these proofs with the Lean compiler. If at least one proof solves the statement, we retain one proof per statement. In cases where multiple proofs are available, we randomly sample one solution. These collected proofs are used for supervised fine-tuning (SFT) based on \dsprover{-Base}\footnote{\url{https://huggingface.co/deepseek-ai/DeepSeek-Prover-V1.5-Base}}, resulting in the iter-1 prover. We continue this expert iteration process; each time, we use the iter-$k$ prover to generate answers and cumulatively collect correct solutions to train \dsprover{-Base} for the next iteration, the iter-$(k+1)$ prover. Refer to Appendix~\ref{app:training_details} for more details on each iteration.

We experiment with learning rates of \(1 \times 10^{-4}\) and \(5 \times 10^{-5}\), training for either 1 or 2 epochs.  We use the packing trick~\citep{tunstall2022natural} with a small batch size of 8 to speed up the training.  In each iteration, the training time for 1 epoch is approximately 12 hours using 4 H100 GPUs. The inference time for the 1.78M statements set by Pass@16 is 6 hours, utilizing 64 H100 GPUs. Additionally, the verification time for these proofs requires 10 hours with 8,000 CPUs.

\section{Results}
\label{sect:results}
\paragraph{Benchmarks.} Following the works of \citep{wang2024theoremllama, xin2024deepseek, wu2024internlm2, li2024hunyuanprover}, we primarily use miniF2F \citep{zheng2021minif2f} as our main evaluation benchmark. We also track the problems solved by our prover in \lwb{}~\citep{ying2024lean} and investigate the performance on \proofnet~\citep{azerbayev2023proofnet} and \putnam{}~\citep{tsoukalas2024putnambench}. Additionally, we uniformly sample a subset from our formalized dataset to create a held-out evaluation dataset. Below, we provide descriptions of each dataset.
\begin{itemize}
    \item \miniff{} \citep{zheng2021minif2f} is a formal theorem proving benchmark, consisting of 488 problem statements (244 validation and 244 test problems) in Lean. The problems are drawn from high-school exercises, as well as high-school level competitions including the AIME, AMC, and the International Mathematical Olympiad (IMO). The  original benchmark was released in Lean 3, and for our analysis, we use the version of \miniff ~in Lean 4.9.0 provided by \cite{xin2024deepseek}.
    \item \proofnet{} \citep{azerbayev2023proofnet} is a formal theorem proving benchmark of undergraduate-level mathematics, consisting of 371 problem statements in Lean (185 validation and 186 test problems). The problems are primarily drawn from undergraduate pure mathematics textbooks, covering topics such as real and complex analysis, linear algebra, abstract algebra, and topology. The original benchmark was released in Lean 3, and for our analysis, we use the version of \proofnet{} in Lean 4.9.0 provided by \cite{xin2024deepseek}.
    \item \lwb{} \citep{ying2024lean} is a large-scale Lean 4 problem set formalized from natural language math problems (mainly from the forum AOPS), which consists of 140K statements in Lean 4. We also monitor the problems solved by our model during the expert iteration process. Notably, the problem set from \lwb{} is included in this training, which is consistent with \dsprover~\citep{xin2024deepseek} and InternLM2.5-StepProver~\citep{wu2024internlm2}.
    \item \putnam{} \citep{tsoukalas2024putnambench} is a formal theorem proving benchmark on competition mathematics problems sourced from the William Lowell Putnam Mathematical Competition years 1962 - 2023. \putnam comprises 644 Lean 4 statements, covering algebra, analysis, number theory, geometry, combinatorics, probability and set theory.
    \item NuminaTest. We randomly sample 250 statements from our formalized Numina dataset and use it as a held-out testing set. We refer to this subset as NuminaTest.
\end{itemize}




% \begin{table}[]
%     \centering
%     \resizebox{\linewidth}{!}{
%     \begin{tabular}{c|cccc|c}
%         \toprule
%          &  miniF2F & ProofNet & NuminaTest & \lwb{} & Average\\
%          \midrule
%         \dsprover-RL & 50.0\% & \textbf{16.0}\% & 54.0\% & 14.7\% & 33.7\%\\ 
%         \midrule
%         \prover-SFT & \textbf{57.6}\%$_{(+7.6)}$ & {15.2}\%$_{(-0.8)}$ & \textbf{61.2}\%$_{(+7.2)}$ & \textbf{21.2}\%$_{(+6.5)}$ & \textbf{38.8}\%$_{(+5.1)}$\\
%         \bottomrule
%     \end{tabular}
%     }
%     \caption{We compare \prover-SFT with \dsprover-RL \citep{xin2024deepseekv15} across miniF2F, ProofNet, NuminaTest and \lwb{}. We report the Pass@32 performance for miniF2F, ProofNet, and NuminaTest datasets. For the \lwb{}, we evaluate performance using Pass@16 due to the large number of problems (140K) it contains, allowing us to save on computational costs.}
%     \label{tab:four_dataset_comparison}
% \end{table}


\paragraph{Main results.} 
The performance on miniF2F is shown in Table~\ref{tab:full_proof_generation_comparison}. The Pass@32 performance of our \prover-SFT is 57.6\%, surpassing the previous SOTA open source model, \dsprover-RL, by 7.6\%. We observe that our \prover-SFT's Pass@32 is even better than \dsprover-RL's Pass@3200 by 2.7\%.  Furthermore, when both evaluated by Pass@3200, our model achieves 62.7\%, surpassing \dsprover-RL's 54.9\% by 7.8\%. Figure~\ref{fig:main_Results} illustrates the inference time scaling curve for our \prover-SFT, \dsprover-RL and \dsprover-SFT. \prover-SFT demonstrates significant improvements over both \dsprover-RL and \dsprover-SFT across all inference compute budgets. Figure~\ref{fig:iterative_training} illustrates the performance of our model during each iteration. Overall, we observe a relatively consistent improvement in performance across iterations. 

\begin{table}[t]
    \centering
    \resizebox{0.75\linewidth}{!}{
    \begin{tabular}{c|cc}
    \toprule
       Whole-Proof Generation Model & Pass & Performance \\
         \midrule
        TheoremLamma \citep{wang2024theoremllama} & 128 & 33.6\%\\
        Deepseek-Prover-V1 \citep{xin2024deepseek} & 32 & 46.1\% $\pm$ 0.5\% \\
        \dsprover-SFT \citep{xin2024deepseekv15} &32 &  48.2\% $\pm$ 0.6\%\\
        \dsprover-RL \citep{xin2024deepseekv15} &32 & 50.0\% $\pm$ 0.5\%\\
        % InternLM2.5-StepProver-BF+CG &$2 \times 32 \times 600$ & 50.7\%\\
        % {\prover-SFT}&32 & {\textbf{58.2\%}}\\
        {\prover-SFT}&32 & {\textbf{57.6\% $\pm$ 0.7\%}}\\        
        \midrule
        \dsprover-SFT \citep{xin2024deepseekv15} & 3200 & 53.3\%\\ 
        \dsprover-RL \citep{xin2024deepseekv15} &  3200 & 54.9\%\\ 
         {\prover-SFT}& 3200 & {\textbf{62.7\%}}\\
        \midrule
        \dsprover-SFT \citep{xin2024deepseekv15} &  $4 \times 6400$ & 55.8\%\\ 
        \dsprover-RL \citep{xin2024deepseekv15} &  $4 \times 6400$ & 58.5\%\\ 
         {\prover-SFT}&  $4 \times 6400$ & {\textbf{64.7\%}}\\
        \bottomrule
    \end{tabular}
    }
    \caption{Whole-proof generation performance on miniF2F.}
    \label{tab:full_proof_generation_comparison}
\end{table}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.3\linewidth]{figures/minif2f_iteration.pdf}
    \includegraphics[width=0.3\linewidth]{figures/NuminaTest_iteration.pdf}
    
    \includegraphics[width=0.3\linewidth]{figures/ProofNet_iteration.pdf}
    \includegraphics[width=0.3\linewidth]{figures/Lean-workbook_iteration.pdf}
    \caption{The figures show the performance of our model on the four datasets at each iteration. We gradually increase the size of the problem set and add more training data. The details of each iteration are shown in Table~\ref{tab:data_for_training}. }
    \label{fig:iterative_training}
\end{figure}
% \paragraph{Multiple dataset performance.} Table~\ref{tab:four_dataset_comparison} provides a comparison between \prover-SFT and \dsprover-RL across the \miniff, NuminaTest, \lwb{}, and \proofnet. \prover-SFT achieves an average performance of 38.8\%, surpassing \dsprover-RL, which scored 33.7\%, by 5.1\%. 

\paragraph{\putnam{} performance.} \prover-SFT solves 7 out of 644 problems in \putnam{} (Pass@512), achieving the first place on the \putnam{} leaderboard. The previous SOTA method ABEL~\citep{gloeckle24abel}  solves 7 with a slightly higher inference budget (Pass@596) and \internlmstep{}~\citep{wu2024internlm2} solves 6 (Pass@$2\times32\times600$).  The performance is summarized in Table~\ref{tab:putnambench}.


\paragraph{\proofnet{} performance.} We observe that the performance on \proofnet{} exhibits a very low correlation with \miniff{}, \lwb{} and NuminaTest. Specifically, Figure~\ref{fig:proofnet_correlation} presents the correlation of model performance across the four datasets. The model’s performance on NuminaTest, \miniff{}, and \lwb{} exhibits strong positive correlations, whereas its performance on \proofnet{} shows a notably low correlation with the other three datasets.

We also observe that the data distribution in \proofnet{} is significantly different from that of the other three datasets. The problems in \proofnet{} are primarily drawn from undergraduate pure mathematics textbooks, covering topics such as real and complex analysis, linear algebra, abstract algebra, and topology. These topics largely rely on the abstract and general formulations of mathematical definitions in \mathlib{} \citep{mathlib4}. Other datasets, e.g., \miniff{}, largely consist of competition and Olympic-style problems, which require complex reasoning, while only depending on a relatively small set of elementary facts about integers, real numbers, counting, and geometry. We show two examples in Table \ref{tab:proofnet_miniF2F_comparison} to illustrate the style difference between \proofnet{} and \miniff{}.


\begin{table}[h!]
\centering
\resizebox{\linewidth}{!}{
\begin{tabular}{@{}clccc@{}}
\toprule
Ranking & Model & Type & Num-solved & Compute (Pass) \\ \midrule
1 & \textbf{\prover-SFT} \textcolor{green}{$\diamond$} & Whole-Proof Generation & 7 & 512 \\
1 & ABEL~\citep{gloeckle24abel}  & Tree Search Method & 7 & 596 \\
3 & \textbf{\prover-SFT} \textcolor{green}{$\diamond$} & Whole-Proof Generation & 6 & 32 \\
3 & InternLM2.5-StepProver~\citep{wu2024internlm2} \textcolor{green}{$\diamond$} & Tree Search  Method & 6 & 2$\times$32$\times$600 \\
5 & InternLM 7B~\citep{ying2024internlm} \textcolor{green}{$\diamond$} & Whole-Proof Generation & 4 & 4096 \\
6 & GPT-4o  & Whole-Proof Generation & 1 & 10 \\
7 & COPRA (GPT-4o)~\citep{thakur2023language}& Whole-Proof Generation & 1 & 1 \\
8 & ReProver w/ retrieval~\citep{yang2024leandojo} \textcolor{green}{$\diamond$} & Whole-Proof Generation & 0 & 1 \\
9 & ReProver w/o retrieval~\citep{yang2024leandojo} \textcolor{green}{$\diamond$} & Whole-Proof Generation & 0 & 1 \\ \bottomrule
\end{tabular}
}
\caption{Number of problems solved on \putnam{} statements (out of 644). The performance numbers for existing works are taken from the leaderboard. Here \textcolor{green}{$\diamond$} inidicates open-source models.}
\label{tab:putnambench}
\end{table}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.45\linewidth]{figures/ProofNet_corr.pdf}
    \caption{This figure illustrates the correlation of model performance across four datasets. A lower value indicates a weaker correlation between the model's performance on two datasets. Notably, \proofnet{} shows a very low correlation with the other three datasets.}
    \label{fig:proofnet_correlation}
\end{figure}

\begin{table}[t]
\centering
\renewcommand{\arraystretch}{1.5} % Adjust row height
\begin{tabular}{|p{0.15\textwidth}|p{0.35\textwidth}|p{0.35\textwidth}|}
\hline
 & \textbf{Example from \proofnet{}} & \textbf{Example from \miniff{}} \\ \hline

\textbf{Informal Statement} &
Prove that no order can be defined in the complex field that turns it into an ordered field. &
Show that for any natural number $n$, $7$ does not divide $2^n + 1$. \\ \hline

\textbf{Formal Statement} &
\begin{minipage}[t]{\linewidth}
    \includegraphics[width=\linewidth]{figures/ProofNet_example.png}
\end{minipage} &
\begin{minipage}[t]{\linewidth}
    \includegraphics[width=\linewidth]{figures/miniF2F_example.png}
\end{minipage} \\ \hline

\textbf{Comments} &
This problem involves the notion of order, which is undergraduate level. Its formal statement uses the definition \texttt{IsLinearOrder} in \mathlib{}. &
This problem comes from IMO but only involves division. \\ \hline

\end{tabular}
\caption{Comparison of Examples from \proofnet and \miniff. \proofnet{} largely relies on the abstract and general formulations of mathematical results in \mathlib{}. In contrast, \miniff{} largely consists of high-school competition and Olympic style problems, which require complex reasoning. }
\label{tab:proofnet_miniF2F_comparison}
\end{table}


% Example from ProofNet:
% \begin{itemize}
%     \item Informal Statement: 
%     \begin{verbatim}"Prove that no order can be defined in the complex field that turns it into an ordered field." 
%     \end{verbatim}
%     \item Formal Statement: \\
%     \begin{minipage}[t]{0.85\textwidth}
%         \includegraphics[width=\linewidth]{figures/ProofNet_example.png}
%     \end{minipage}
%     \item This problem involves the notion of order which falls in undergraduate level mathematics. Its formal statement uses the definition \texttt{IsLinearOrder} in \mathlib{}.
% \end{itemize}

% Example from miniF2F:
% \begin{itemize}
%     \item Informal Statement: 
%     \begin{verbatim}"Show that for any natural number $n$, $7$ does not divide $2^n + 1$." 
%     \end{verbatim}
%     \item Formal Statement: \\
%     \begin{minipage}[t]{0.85\textwidth}
%         \includegraphics[width=\linewidth]{figures/miniF2F_example.png}
%     \end{minipage}
%     \item This problem comes from IMO, but only involves division.
% \end{itemize}




% \rd{Show example on the distribution shift between ProofNet and miniF2F. The connection between ProofNet and \mathlib{} @Shange} 


\paragraph{Including \mathlib{} in the training data.} Given the close connection between \mathlib{} and \proofnet{}, we investigate the inclusion of \mathlib{} in the training dataset, following the approaches of \cite{xin2024deepseekv15} and \cite{wang2024theoremllama}. Our results in Table~\ref{tab:proofnet_perforemance}  indicate that while adding \mathlib{} enhances the model's performance on \proofnet{}, it also leads to a decrease in \miniff{} performance. Specifically, we analyze the impact of incorporating \mathlib{} into the training set during the sixth iteration, with the proofs generated by the 5th iteration referred to as ``Iter-5 proofs". We observe that including \mathlib{} increases \proofnet{} performance from 13.3\% to 15.6\%, but results in a decline in \miniff{} performance from 56.6\% to 54.1\%, as well as a decrease in NuminaTest performance from 59.2\% to 58.8\%. We continue to include \mathlib{} in the training dataset from the 6th iteration 6 onward, consistent with \dsprover-RL~\citep{xin2024deepseekv15} and TheoremLamma~\citep{wang2024theoremllama}. Additional details can be found in the Appendix~\ref{app:training_details}.


\begin{table}[h]
    \centering
    \resizebox{0.95\linewidth}{!}{
    \begin{tabular}{@{}lccccc@{}}
        \toprule
        Model & Training Dataset &  miniF2F & ProofNet & NuminaTest  & Average \\ 
        \midrule
        Deepseek-RL & -- & 50.0\% & \textbf{16.0\%} & 53.6\% & 39.9\% \\ 
        \midrule
        Iter-6 prover & Iter-5 proofs &  \textbf{56.6\%} & 13.3\% & \textbf{59.2\%} & \textbf{43.0\%} \\ 
        Iter-6 prover  & Iter-5 proofs  + \mathlib{} & 54.1\% & 15.6\% & 58.8\% & 42.8\% \\ 
        \bottomrule
    \end{tabular}
    }
    \caption{Investigation on ProofNet at the 6th iteration. The results indicate that incorporating \mathlib{} into the training data enhances performance on ProofNet but reduces performance on miniF2F and NuminaTest. We continue to include \mathlib{} in the training dataset from iteration 6 onward, in line with \cite{xin2024deepseek}. Details are in Appendix~\ref{app:training_details}.}
    \label{tab:proofnet_perforemance}
\end{table}

\paragraph{Proofs found in \lwb{}.} The \lwb{}, which includes \lwb{}-plus \citep{ying2024lean,wu2024internlm2}, formalizes 140K high-quality problems sourced from AOPS and the Compfiles data. Currently, proofs for only 15.7K statements in \lwb{} have been found and made open-source by InternLM2.5-StepProver \citep{wu2024internlm2} and InternLM-Math-Plus \citep{ying2024internlm}. In contrast, our model has discovered a significantly larger set of proofs within \lwb{}, cumulatively solving 29.7K problems. Figure~\ref{fig:main_Results} (right) shows a comparison of the number of proofs found by our model and prior works. We open-source all the proofs found by our model to benefit the research community.

\paragraph{Ablation on two formalizers.} In Section~\ref{sect:statement_formalization}, we discuss that the style of statement formalization can impact the prover's performance.  Therefore, we propose generating statements using two formalizers trained on data from different sources. In this section, we compare a prover trained on a single statement style (using only one formalizer) with a prover trained on both styles (utilizing statements from both Formalizer A and B). We conducted experiments by training the prover at the 8th iteration on statements with proofs collected by the iter-7 prover, and the results are presented in Table~\ref{tab:formalization_style}. The findings indicate that using both formalizers leads to improved performance.

\begin{table}[h]
    \centering
    \resizebox{0.8\linewidth}{!}{
    \begin{tabular}{c|ccc|c}
    \toprule
        Formalization Model  &  miniF2F &ProofNet & NuminaTest& Average\\
        \midrule
        Formalizer A only & 56.5\% & 13.8\%&59.6\% &43.3\%\\
        Formalizer B only & 56.2\% & \textbf{15.2}\% & 60.0\%&43.8\%\\
        \midrule
        Formalizer A and B & \textbf{57.6}\% &\textbf{15.2}\% & \textbf{61.2}\%&\textbf{44.7}\%\\
        \bottomrule
    \end{tabular}
    }
    \caption{An ablation study on using two formalizers to formalize the statements. The term 'Formalizer A and B' refers to using statements formalized by both A and B. In contrast, 'Formalizer A only' or 'Formalizer B only' indicates that we include statements formalized exclusively by Formalizer A or B, respectively. Notably, in these experiments, we all add the \mathlib{} training data.} %\rd{descriptions, more evaluations}
    \label{tab:formalization_style}
\end{table}




% \begin{table}[h]
%     \centering
%     \begin{tabular}{c|c}
%     \toprule
%          & Proofs found \\
%         \midrule
%          Existing Works & 16K \\
%          \midrule
%         \prover & 29.7K\\
%         \bottomrule
%     \end{tabular}
%     \caption{Proofs found in \lwb{}. }
%     \label{tab:proofs_in_leanworkbook}
% \end{table}



\section{Discussion}

%In this section, we 
We delve into
the characteristics of proofs generated by \prover-SFT and discuss potential directions for improvement, particularly regarding the proof style adopted by the model, the role of search as well as online interaction in proof generation, and the integration of external symbolic computation tools such as SymPy.


\paragraph{The Proof Style.}
We observe that the proofs provided by \prover-SFT often rely on high-level tactics such as \texttt{nlinarith}, \texttt{simp\_all}, and \texttt{norm\_num}, among others. These high-level tactics handle multiple reasoning steps internally, delegating the resolution of intermediate steps to their built-in automation. For example, the \texttt{nlinarith} tactic can automatically solve certain linear and non-linear equalities and inequalities.
% \chijin{I won't use the word ``omit'', but more like something ``leave intermediate steps to the powerful tactics to figure out''}
Figure \ref{fig:proof_style} shows a typical proof generated by our prover. The first several steps involve only trivial transformations of the original problem, whereas the final line uses \texttt{nlinarith} to immediately achieve the goal. Whether this style of proof is sufficient for complex reasoning remains an important area for exploration.


\paragraph{Search and online interaction.}
Currently, \prover-SFT generates the entire proof for the problem at once, without receiving further feedback. While our current approach is appealing in terms of computation, incorporating search and interaction in future work could enhance performance. For example, once a tactic is generated by our prover, it can interact with the Lean compiler to receive feedback on how the goal changes after the tactic is applied. This information can then be utilized in generating the next tactic, potentially improving the overall proof strategy~\citep{wu2024internlm2}. 

\paragraph{SymPy.} 
% % Improvement over 32 pass inference (2/244). No improvement over 3200 pass inference. 
Future work may aim to leverage other software packages to enhance Lean's capabilities. For instance, Lean's \texttt{ring} tactic can handle algebraic simplifications by applying axioms such as distributivity, associativity, and commutativity. However, a combination of tactics is required for non-algebraic transformations of transcendental functions, such as logarithmic and trigonometric functions, and other advanced simplifications beyond commutative rings. We explored using a Python-based computer algebra system, SymPy~\citep{meurer2017sympy}, to simplify complex expressions in theorem statements and feed the simplified form into the prover.
Specifically, we parse equations of the form \( A = B \) within the goals of Lean theorem statements, construct the SymPy expression \( A - B \), and then apply the \texttt{simplify} method in Lean. This procedure directly solves 9.4\% of \miniff{} by simplifying the statements to $0=0$. In addition, it solves 0.8\% of the problems in \miniff{} that were unsolved by \prover-SFT with Pass@32, but did not improve \prover-SFT with Pass@3200. Thus, SymPy simplification is not part of any of our reported results. However, we think such procedures need further exploration. 
%included solely as an exploratory element in the discussion.


\section*{Acknowledgments}
We thank Haoyu Zhao, Hubert Strauss and Suozhi Huang for their helpful discussions.

% \section*{Author Contributions}
% Contributions listed.



\bibliography{colm2024_conference}
\bibliographystyle{colm2024_conference}
\newpage
\appendix
\section*{Appendix}
\section{Expert Iteration Details}
\label{app:training_details}

% \chijin{Fix capitalization of this section title. The content of Appendix B is missing. Add some description in Appendix C.2 }

The main training pipeline is illustrated in Section~\ref{sect:iterative}. When we implement the expert iteration algorithm, we gradually add the data. From iter-0 to iter-3, we gradually add the statements formalized by Claude-sonnet-3.5. At iter-3, we train the Formalizer B and add the formalized statements generated by Formalizer B for iter-4 to iter-6. At iter-7, we begin to add the statements generated by Formalizer A. We also add \mathlib{} data into the training dataset for better ProofNet performance when starting from iter-6.

\begin{table}[h]
    \centering
    \resizebox{\linewidth}{!}{
    \begin{tabular}{c|cc|ccc}
    \toprule
         \multirow{2}{*}{Iteration} & \multicolumn{2}{c|}{Statements} & \multicolumn{3}{c}{Training Data}\\
       & \lwb{} & Formalized & \lwb{} Solved & Formalized Solved & \mathlib{} \\
      \midrule
       Iter-0  & 140K & 0 & 20.6K & 0 & 0\\
       Iter-1  & 140K & 140K & 20.6K & 72.4K & 0\\
       Iter-2  & 140K & 270K & 23.0K & 128.7K & 0\\
       Iter-3  & 140K & 270K & 24.4K & 161.2K & 0 \\
       Iter-4  & 140K & 882K & 25.4K & 425.8K & 0\\
       Iter-5  & 140K & 882K & 27.0K & 436.5K & 0\\
       Iter-6  & 140K & 882K & 27.8K & 443.2K & 104K\\
       Iter-7  & 140K & 1.64M & 28.8K & 887.7K & 104K\\
       Iter-8  & 140K & 1.64M & 29.7K & 915.7K & 104K\\
       Iter-9  & 140K & 1.64M & 30.3K & 928.2K & 104K\\
    \bottomrule
    \end{tabular}
    }
    \caption{Expert iteration details.}
    \label{tab:data_for_training}
\end{table}

% \begin{table}[h]
%     \centering
%     \resizebox{\linewidth}{!}{
%     \begin{tabular}{c|cc|ccc}
%     \toprule
%         & \multicolumn{2}{*}{Statements} & Training Data & Training Data& Training Data\\
%       Iteration & \lwb{} & Formalized & \lwb{} Solved & Formalized Solved & \mathlib{} \\
%       \midrule
%        Iter-0  & 140K&0&20.6K&0&0\\
%        Iter-1  & 140K&140K&20.6K&72.4K&0\\
%        Iter-2  & 140K&270K&23.0K&128.7K&0\\
%        Iter-3  & 140K&270K&24.4K&161.2K&0 \\
%        Iter-4  & 140K&882K&25.4K&425.8K&0\\
%        Iter-5  & 140K&882K&27.0K&436.5K&0\\
%        Iter-6  & 140K&882K&27.8K&443.2K&104K\\
%        Iter-7  & 140K&1.64M&28.8K&887.7K&104K\\
%        Iter-8  & 140K&1.64M&29.7K&915.7K&104K\\
%        Iter-9  & 140K&1.64M&30.3K&928.2K&104K\\
%     \bottomrule
%     \end{tabular}
%     }
%     \caption{Iterative training details.}
%     \label{tab:data_for_training}
% \end{table}

% \begin{table}[h]
%     \centering
%     \resizebox{\linewidth}{!}{
%     \begin{tabular}{c|c|ccc}
%     \toprule
%         & \multirow{2}{*}{Statements} & Training Data & Training Data & Training Data\\
%       Iteration & & \lwb{} Solved & Formalized Solved & \mathlib{} \\
%       \midrule
%        Iter-0  & 140K & 20.6K & 0 & 0\\
%        Iter-1  & 140K & 20.6K & 72.4K & 0\\
%        Iter-2  & 140K & 23.0K & 128.7K & 0\\
%        Iter-3  & 140K & 24.4K & 161.2K & 0 \\
%        Iter-4  & 140K & 25.4K & 425.8K & 0\\
%        Iter-5  & 140K & 27.0K & 436.5K & 0\\
%        Iter-6  & 140K & 27.8K & 443.2K & 104K\\
%        Iter-7  & 140K & 28.8K & 887.7K & 104K\\
%        Iter-8  & 140K & 29.7K & 915.7K & 104K\\
%        Iter-9  & 140K & 30.3K & 928.2K & 104K\\
%     \bottomrule
%     \end{tabular}
%     }
%     \caption{Expert iteration details.}
%     \label{tab:data_for_training}
% \end{table}
% \section{A More Detailed Comparison with Existing Work}


\section{More Examples}
\subsection{\mathlib{} example} Figure \ref{fig:mathlib4_example} and \ref{fig:minif2f_example2} show the statement and proof in mathlib4 and miniF2F respectively. It can be easily seen that both the statement and proof rely on pre-defined objects. Unlike miniF2F statements, the example in Figure \ref{fig:mathlib4_example} can not even pass the lean compilation, given that pre-defined objects are missing.
\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\linewidth]{figures/mathlib4_example.png}
    \caption{A \mathlib{} example which relies on pre-defined objects \texttt{@Acc.ndrec} and \texttt{@Acc.ndrecC}}
    \label{fig:mathlib4_example}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\linewidth]{figures/miniF2F_example2.png}
    \caption{A \miniff{} example which does not rely on pre-defined objects}
    \label{fig:minif2f_example2}
\end{figure}

\subsection{Proof Style}
The proofs from \prover-SFT often rely on high-level tactics like \texttt{nlinarith}, \texttt{simp\_all}, and \texttt{norm\_num}, which handle complex equalities and inequalities while leaving intermediate steps to powerful methods. As shown in Figure \ref{fig:proof_style}, initial steps typically involve trivial transformations, with the final step quickly resolving the goal using \texttt{nlinarith}. The effectiveness of this proof style for complex reasoning is still under exploration.
\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\linewidth]{figures/proof_style_example2.png}
    \caption{Example of proof style, where intermediate steps are absorbed in high-level tactics}
    \label{fig:proof_style}
\end{figure}

\end{document}

