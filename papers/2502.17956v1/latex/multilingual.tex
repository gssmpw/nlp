




\subsubsection{Multilingual Setting}
%
Table \ref{tab:main-multi} shows that PoT continues to outperform CoT in multilingual settings across all languages and model variants.
%
A few exceptions appear in some languages within the Llama 2 family trained on $\mathcal{D}^\texttt{MGSM8KPoT}_\texttt{cross-comment}$, yet overall results confirm that PoT provides consistent advantages over CoT.

As observed in the cross-lingual experiments, CodeLlama2-7B maintains its advantage over Llama2-7B across all languages in the multilingual setting.
%
This performance gap is particularly pronounced in French (+8.0), Chinese (+6.0), and English (+5.9), further suggesting that increasing code data during pretraining yields stronger reasoning capabilities.
%
Scaling to larger models continues to deliver gains, with Llama2-13B showing consistent improvements over both 7B variants.
%
However, the most dramatic improvements come from Llama3-8B, which achieves substantially higher accuracy across all languages, reaching 76.5\% in English while maintaining strong performance even in non-English languages like Thai (57.6\%) and Bengali (55.2\%).
% 
The stronger gains highlights the benefits of explicit multilingual training over multilingual transfer, emphasizing the role of scaling and adaptation in optimizing reasoning across languages.
%
% Interestingly, Llama3â€‘8B deviates from this trend by performing better with CoT than with PoT.
% %
% Upon further investigation of this unexpected behavior, we discovered that this performance gap stems primarily from compilation errors in the generated programs rather than logical reasoning failures.
% %
% For a comprehensive analysis of these findings, please refer to Appendix \ref{ap:Llama3}.











%
Finally, we investigate the most effective way to align Q and R in a multilingual context.
%
As shown in Table \ref{tab:multilingual-ablation}, translating inline comments into the target language consistently yields superior performance across all model variants. 
%
We hypothesize that this improvement comes from the enhanced semantic alignment between code and natural language when comments are presented in the target language during training.
%
In summary, these findings indicate that $\mathcal{D}^\texttt{MGSM8KPoT}_\texttt{parallel}$ provides the optimal Q-R alignment for multilingual settings.
%
\begin{table}[htbp]
\small
  \centering
  \resizebox{\columnwidth}{!}{
  \begin{tabular}{l|c|cc|c}
    \hline
    \textbf{Method} & en & de & bn & ALL \\
    \hline\hline
    \multicolumn{1}{l|}{\underline{Llama2-7B}}& \multicolumn{1}{c|}{} & \multicolumn{2}{c|}{} & \multicolumn{1}{c}{} \\
    PoT Cross Comment & \underline{54.8} & \textbf{47.2} & 18.0 & 36.6  \\
    PoT Cross Question & 46.0 & 37.6 & 28.8 & 37.7  \\ 
    PoT Parallel & \textbf{56.0} & \textbf{47.2} & \textbf{30.8} & \textbf{44.6} \\
    PoT No Comment & 53.6 & 41.6 & \underline{29.2} & \underline{40.6}  \\ 
    \hline
    \multicolumn{1}{l|}{\underline{CodeLlama2-7B}}& \multicolumn{1}{c|}{} & \multicolumn{2}{c|}{} & \multicolumn{1}{c}{} \\
    PoT Cross Comment & \underline{58.0} & 47.2 & 22.4 & 41.1  \\
    PoT Cross Question & 48.0 & 42.8 & 28.8 &  40.5 \\ 
    PoT Parallel & \textbf{61.9} & \textbf{52.8} & \textbf{35.6} & \textbf{49.0}  \\
    PoT No Comment & 56.8 & \underline{47.6} & \underline{35.2} & \underline{45.6}  \\ 
    \hline    
    \multicolumn{1}{l|}{\underline{Llama2-13B}}& \multicolumn{1}{c|}{} & \multicolumn{2}{c|}{} & \multicolumn{1}{c}{} \\
    PoT Cross Comment & \underline{62.0} & \underline{53.6} & 23.2 & 42.2  \\
    PoT Cross Question & 53.0 & 47.6 & \underline{35.9} & 45.1  \\ 
    PoT Parallel & \textbf{63.5} & \textbf{56.4} & \textbf{44.4} & \textbf{54.6}  \\
    PoT No Comment & 58.4 & 51.6 & \underline{35.2} & \underline{46.4}  \\ 
    \hline    
    \multicolumn{1}{l|}{\underline{Llama3-8B}}& \multicolumn{1}{c|}{} & \multicolumn{2}{c|}{} & \multicolumn{1}{c}{} \\
    PoT Cross Comment & \underline{72.8} & \underline{62.4} & \underline{51.2}	& \underline{58.3}  \\
    PoT Cross Question & 37.2 & 30.3 & 30.4 & 31.6  \\ 
    PoT Parallel & \textbf{76.5} & \textbf{64.4} & \textbf{55.2} & \textbf{62.6}  \\
    PoT No Comment &  65.2 & 60.0 & 48.4 & 56.5  \\ 
    \hline    
  \end{tabular}
  }
  \caption{
  The impact of various fine-tuning strategies is examined, where PoT Cross includes either comment-only or question-only translation. In contrast, the Parallel approach involves either the exclusion of comments or the inclusion of translated comments.
  % The complete table is available in Table~\ref{tab:}.
  }
  \label{tab:multilingual-ablation}
\end{table}

% % Intro
% In the multilingual setting, we fine-tune the models using data in both English and the target languages. This enables us to explore alignment strategies for improving reasoning capabilities across languages.
% %
% Similar to the cross-lingual setup, PoT consistently outperforms CoT across all languages and model variants in our multilingual setup, as presented in Table \ref{tab:main-multi}.
% %
% Moreover, the results indicate that when the model is exposed to explicit demonstrations of reasoning in target languages during fine-tuning, the performance gap across languages tends to diminish in most cases.

% Building on the findings of \citet{mathoctopus}, where CoT Cross and CoT Parallel were introduced, we conducted comparative experiments applying PoT with the same strategies. In this setup, the output answer translation serves as the inline comments. Our findings align with those of CoT, demonstrating that the parallel approach is generally more effective, as reflected in its higher accuracy.







% The ablation study aimed at identifying the most effective strategy for PoT alignment in a multilingual setup reveals that PoT Parallel is the most effective approach. This finding confirms that translating inline comments can further enhance performance across all languages, including English, as illustrated in Figure~\ref{tab:multilingual-ablation}.
% %
% The second-best performing approach is PoT Parallel No Comment, where the model is trained on translated questions without inline comments, in contrast to the cross-lingual setup.


% However, the PoT Cross Comment approach, which serves as a comparable alternative to CoT Cross, demonstrates the weakest performance among the four methods.
% %
% Finally, the PoT Cross Question strategy appears to perform slightly better than PoT Cross Comment, as it achieves higher overall accuracy in two out of three cases.




