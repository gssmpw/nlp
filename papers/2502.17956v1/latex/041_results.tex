% \section{GSM8KPoT}



% \begin{table*}[!t]
% \small
%   \centering
%   \resizebox{\textwidth}{!}{
%   % \begin{tabular}{l|l|llllll|lll|l}
%   \begin{tabular}{l|llllllllll|l}
%     \hline
%     % \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} &  \multicolumn{6}{c|}{\textbf{HRL}} & \multicolumn{3}{c|}{\textbf{URL}} & \multicolumn{1}{c}{\textbf{Avg.}} \\
%     \textbf{Method} & en & de & fr & es & ru & zh & ja & th & sw & bn & All \\
%     \hline
%     \multicolumn{1}{l|}{\underline{Llama2-7B}} &     %     % \multicolumn{1}{c|}{} & \multicolumn{6}{c|}{} & \multicolumn{3}{c|}{} & \multicolumn{1}{c}{} \\
%     \multicolumn{1}{c}{} & \multicolumn{6}{c}{} & \multicolumn{3}{c|}{} & \multicolumn{1}{c}{} \\    
%     CoT & 43.6 & 32.4 & 30.4 & 30.4 & 26.4 & 25.2 & 15.2 & 4.8 & 2.0 & 5.6 & 21.6 \\
%     PoT & 58.0 & 40.4 & 40.4 & 43.6 & 37.1 & 38.4 & 32.7 & 7.6 & 5.6 & 12.0 & 31.6 \\
%     \hline
%     \multicolumn{1}{l|}{\underline{CodeLlama 7B}} & 
%     % \multicolumn{1}{c|}{} & \multicolumn{6}{c|}{} & \multicolumn{3}{c|}{} & \multicolumn{1}{c}{} \\
%     \multicolumn{1}{c}{} & \multicolumn{6}{c}{} & \multicolumn{3}{c|}{} & \multicolumn{1}{c}{} \\  
%     CoT & 43.2 & 33.2 & 32.8 & 39.6 & 26.8 & 27.2 & 18.8 & 16.4 & 3.2 & 9.2 & 25.0 \\
%     PoT & 58.8 & 48.4 & 51.6 & 53.6 & 49.8 & 41.6 & 39.6 & 26.8 & 4.4 & 11.2 & 38.6 \\
%     \hline    
%     \multicolumn{1}{l|}{\underline{Llama2-13B}} &     %     % \multicolumn{1}{c|}{} & \multicolumn{6}{c|}{} & \multicolumn{3}{c|}{} & \multicolumn{1}{c}{} \\
%     \multicolumn{1}{c}{} & \multicolumn{6}{c}{} & \multicolumn{3}{c|}{} & \multicolumn{1}{c}{} \\     
%     CoT & 47.4 & 39.2 & 37.6 & 41.2 & 38.0 & 35.2 & 18.8 & 7.2 & 7.4 & 6.8 & 27.9 \\
%     PoT & 64.0 & 52.4 & 54.4 & 55.6 & 51.2 & 44.0 & 40.0 & 13.9 & 7.2 & 13.6 & 39.6 \\
%     \hline        
%     \multicolumn{1}{l|}{\underline{Llama3-8B}} &    % \multicolumn{1}{c|}{} & \multicolumn{6}{c|}{} & \multicolumn{3}{c|}{} & \multicolumn{1}{c}{} \\
%     \multicolumn{1}{c}{} & \multicolumn{6}{c}{} & \multicolumn{3}{c|}{} & \multicolumn{1}{c}{} \\ 
%     CoT & 62.8 & 51.2 & 52.8 & 54.8 & 45.2 & 40.0 & 33.6 & 39.6 & 28.0 & 39.6 & 44.8 \\
%     PoT & 68.4 & 62.2 & 59.2 & 62.4 & 60.4 & 52.4 & 45.4 & 43.6 & 34.8 & 46.0 & 53.5 \\    
%     \hline\hline
%     \multicolumn{12}{c}{\textit{Oracle}} \\
%     \hline
%     \multicolumn{1}{l|}{\underline{Llama 405B}} &    % \multicolumn{1}{c|}{} & \multicolumn{6}{c|}{} & \multicolumn{3}{c|}{} & \multicolumn{1}{c}{} \\
%     \multicolumn{1}{c}{} & \multicolumn{6}{c}{} & \multicolumn{3}{c|}{} & \multicolumn{1}{c}{} \\ 
%     CoT & xx.x & xx.x & xx.x & xx.x & xx.x & xx.x & xx.x & xx.x & xx.x & xx.x & xx.x \\
%     PoT & xx.x & xx.x & xx.x & xx.x & xx.x & xx.x & xx.x & xx.x & xx.x & xx.x & xx.x \\
%     \hline
%     % \multicolumn{1}{l|}{\underline{o1-mini [OpenAI]}}& \multicolumn{1}{c|}{} & \multicolumn{6}{c|}{} & \multicolumn{3}{c|}{} & \multicolumn{3}{c}{} \\
%     % CoT & xx.x & xx.x & xx.x & xx.x & xx.x & xx.x & xx.x & xx.x & xx.x & xx.x & xx.x \\
%     % PoT & xx.x & xx.x & xx.x & xx.x & xx.x & xx.x & xx.x & xx.x & xx.x & xx.x & xx.x \\
%     \hline
%   \end{tabular}
%   }
%   \caption{
%   Accuracy (\%) on MGSM in the cross-lingual setting.
%   }
%   \label{tab:main-cross}
% \end{table*}







\section{Experimental Results}




% \textcolor{blue}{mention QR and RA to section}

% \subsection{Results: Program-of-Thought Fine-tuning }
\subsection{Impact of Q-R Alignment Fine-tuning}
\label{sec:QR-Alignment}
\subsubsection{Cross-lingual Setting}
% intro
% trend in general PoT vs CoT
The experimental results presented in Table \ref{tab:main-cross} indicate that PoT consistently outperforms CoT across all languages and model classes, achieving superior results in 39 out of 40 cases. The only exception is Swahili in the Llama2-13B model, where PoT reached an accuracy of 7.2\%, compared to CoT's 7.4\%, showing only a slight difference.
%

When comparing models of the same size, CodeLlama-7B consistently outperforms Llama2-7B in most languages. 
%
The improvements are notable in non-English languages such as German (+8.0), French (+11.2), and Thai (+19.2), suggesting that the incorporation of code data during pretraining improves structured reasoning even in cross-lingual settings.
%
Scaling up to Llama2-13B leads to further improvements over both Llama2-7B and CodeLlama-7B. 
%
While model size remains an important factor in boosting overall accuracy, the strong performance of CodeLlama-7B relative to Llama2-7B indicates that increased code data during pretraining \cite{codeLlama} can enhance reasoning ability.
%
For models with enhanced multilingual capabilities, such as Llama3-8B, where the performance gap between languages is narrower, the results suggest that PoT remains more effective in cross-lingual settings, achieving superior accuracy across unseen languages.
%
%











In Table \ref{tab:pot-inline-comment}, we compare performance when fine-tuning between \(\mathcal{D}^\texttt{GSM8KPoT}_\texttt{en}\) and \(\mathcal{D}^\texttt{GSM8KPoT}_\texttt{nc}\).
%
Overall, training without comments tends to improve non-English accuracy across Llama2 models for both 7B and 13B variants, where omitting comments reduces English accuracy slightly but yields larger gains in non-English languages, like German and Bengali, boosting the overall score.
%
\begin{table}[htbp]
\tiny
  \centering
  \renewcommand{\arraystretch}{1.2}

  \resizebox{\columnwidth}{!}{
  \begin{tabular}{l|c|cc|c}
    \hline
    \textbf{Method} & en & de & bn & ALL \\
    \hline\hline
    \multicolumn{1}{l|}{\underline{Llama2-7B}}& \multicolumn{1}{c|}{} & \multicolumn{2}{c|}{} & \multicolumn{1}{c}{} \\
    With Comments & \textbf{58.3} & 37.9 & 9.9 & 30.0  \\
    Without Comments & 58.0 & \textbf{40.4} & \textbf{12.0} & \textbf{31.6}  \\   
    \hline     
    \multicolumn{1}{l|}{\underline{CodeLlama-7B}}& \multicolumn{1}{c|}{} & \multicolumn{2}{c|}{} & \multicolumn{1}{c}{} \\
    With Comments & \textbf{61.4}  & 45.2 & \textbf{15.6} & 36.6  \\
    Without Comments & 58.8  & \textbf{48.4} & 11.2 & \textbf{38.6}  \\
    \hline    
    \multicolumn{1}{l|}{\underline{Llama2-13B}}& \multicolumn{1}{c|}{} & \multicolumn{2}{c|}{} & \multicolumn{1}{c}{} \\
    With Comments & \textbf{67.3}  & 48.4 & 13.2 & 37.4  \\
    Without Comments & 64.0  & \textbf{52.4} & \textbf{13.6} & \textbf{39.6}  \\
    \hline
    \multicolumn{1}{l|}{\underline{Llama3-8B}}& \multicolumn{1}{c|}{} & \multicolumn{2}{c|}{} & \multicolumn{1}{c}{} \\
    With Comments & 46.4  & 48.2 & 37.5 & 40.6  \\
    Without Comments & \textbf{68.4}  & \textbf{62.2} & \textbf{46.0} & \textbf{53.5}  \\      
    \hline            
  \end{tabular}
  }
  \caption{The impact of code comments on accuracy across different models in cross-lingual setup. The ALL score is from Appendix~\ref{ap:full-main}.}
  \label{tab:pot-inline-comment}
\end{table}

Interestingly, CodeLlama-7B shows mixed results: including comments helps in English and Bengali, whereas excluding comments improves German and also leads to a higher overall score.
%
This may reflect the specialized training corpus for CodeLlama, which emphasizes code tokens and might interact differently with inline explanations.  

%
Finally, Llama3-8B shows the largest swing: removing comments substantially boosts performance for all languages (including English), suggesting that inline explanations can sometimes distract or misalign the Q–R.
%
Taken together, these findings indicate that, for most models, \(\mathcal{D}^\texttt{GSM8KPoT}_\texttt{nc}\) provides better cross-lingual generalization and more robust Q–R alignment.


\begin{table*}[htbp]
\tiny
\centering
\resizebox{\textwidth}{!}{
\begin{tabular}{l|llllllllll|l}
\hline
\textbf{Method} & en & de & fr & es & ru & zh & ja & th & sw & bn & All \\
\hline
\multicolumn{1}{l|}{\underline{Llama2-7B}} & & & & & & & & & & & \\
CoT Cross & 45.2 & 38.4 & 36.8 & 40.0 & 33.2 & \textbf{33.6} & 23.6 & 16.8 & \textbf{18.8} & 17.2 & 30.4 \\
PoT Cross Comment & \textbf{54.8} & \textbf{47.2} & \textbf{51.2} & \textbf{46.2} & \textbf{42.8} & 33.2 & \textbf{34.8} & \textbf{20.0} & 17.6 & \textbf{18.0} & \textbf{36.6} \\
\hdashline
CoT Parallel & 48.8 & 42.4 & 44.0 & 42.4 & 38.0 & 42.4 & 31.6 & 33.6 & 34.4 & 27.6 & 38.5 \\
PoT Parallel & \textbf{56.0} & \textbf{47.2} & \textbf{46.4} & \textbf{54.0} & \textbf{49.6} & \textbf{44.4} & \textbf{40.0} & \textbf{40.4} & \textbf{37.6} & \textbf{30.8} & \textbf{44.6} \\
\hline
\multicolumn{1}{l|}{\underline{CodeLlama2-7B}} & & & & & & & & & & & \\
CoT Cross & 47.6 & 38.8 & 33.2 & 38.8 & 35.2 & 31.6 & 28.8 & 23.6 & 17.2 & 20.4 & 31.5 \\
PoT Cross Comment & \textbf{58.0} & \textbf{47.2} & \textbf{51.4} & \textbf{52.4} & \textbf{48.0} & \textbf{44.2} & \textbf{38.0} & \textbf{28.8} & \textbf{20.4} & \textbf{22.4} & \textbf{41.1} \\
\hdashline
CoT Parallel & 46.0 & 40.0 & 38.8 & 44.0 & 43.2 & 41.2 & 35.6 & 41.6 & 30.8 & 32.0 & 39.3 \\
PoT Parallel & \textbf{61.9} & \textbf{52.8} & \textbf{54.4} & \textbf{52.4} & \textbf{53.6} & \textbf{50.4} & \textbf{44.8} & \textbf{44.8} & \textbf{39.6} & \textbf{35.6} & \textbf{49.0} \\
\hline
\multicolumn{1}{l|}{\underline{Llama2-13B}} & & & & & & & & & & & \\
CoT Cross & 58.4 & 50.4 & 46.4 & 49.6 & 43.6 & \textbf{43.2} & 33.6 & \textbf{25.6} & \textbf{23.6} & \textbf{24.4} & 39.9 \\
PoT Cross Comment & \textbf{62.0} & \textbf{53.6} & \textbf{52.4} & \textbf{54.8} & \textbf{50.0} & 42.0 & \textbf{39.2} & 21.6 & 23.2 & 23.2 & \textbf{42.2} \\
\hdashline
CoT Parallel & 60.8 & 53.6 & 52.0 & 54.4 & 52.8 & 53.6 & 45.2 & 43.6 & 41.2 & 38.0 & 49.5 \\
PoT Parallel & \textbf{63.5} & \textbf{56.4} & \textbf{59.2} & \textbf{59.2} & \textbf{55.2} & \textbf{54.0} & \textbf{51.6} & \textbf{50.0} & \textbf{52.8} & \textbf{44.4} & \textbf{54.6} \\
\hline
\multicolumn{1}{l|}{\underline{Llama3-8B}} & & & & & & & & & & & \\
CoT Cross & 69.2 & 58.0 & 54.8 & 58.0 & 57.2 & 50.0 & 44.4 & 40.4 & 40.4 & 42.0 & 51.4 \\
PoT Cross Comment & \textbf{72.8} & \textbf{62.4} & \textbf{66.4} & \textbf{67.2} & \textbf{63.6} & \textbf{52.0} & \textbf{49.6} & \textbf{52.0} & \textbf{46.2} & \textbf{51.2} & \textbf{58.3} \\
\hdashline
CoT Parallel & 66.8 & 53.6 & 57.2 & 60.8 & 62.4 & 60.0 & 50.8 & 57.6 & 53.6 & 54.8 & 57.8 \\
PoT Parallel & \textbf{76.5} & \textbf{64.4} & \textbf{63.2} & \textbf{66.4} & \textbf{64.0} & \textbf{63.2} & \textbf{56.4} & \textbf{57.6} & \textbf{59.6} & \textbf{55.2} & \textbf{62.6} \\
\hline
\end{tabular}
}
\caption{
Accuracy (\%) on MGSM in \textbf{multilingual setup}. 
% \textcolor{blue}{idea: if the results are already correct, move Llama3-8b to appendix and discuss in main then raise the issue, could be data leakage.}
%\textcolor{red}{If we fine-tune, we should consider the variance in some models (one or two Llama for 3 random seeds) in the appendix.}
}
\label{tab:main-multi}
\vspace{-3mm}
\end{table*}






\input{latex/multilingual}


