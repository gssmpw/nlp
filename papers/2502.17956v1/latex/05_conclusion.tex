\section{Conclusion}

This study explores the effectiveness of Program-of-Thought (PoT) prompting for reasoning in cross- and multilingual settings by leveraging the reasoning-execution disentanglement concept.
%
We decompose the problem into two key challenges: (i) aligning multilingual questions with structured reasoning steps and (ii) assessing the impact of reasoning quality on final answer correctness.
%
Through systematic experimentation across cross-lingual and multilingual settings, we show that PoT fine-tuning substantially enhances reasoning alignment and generalization,
% outperforming both non-fine-tuned PoT and CoT fine-tuning.
outperforming CoT fine-tuning.

Moreover, we establish a strong correlation between reasoning quality and answer accuracy. 
%
By leveraging \texttt{ICE-Score}-based inference strategies, we enhance performance,
particularly in low-resource languages. 
% across all languages.
%
These findings provide insights into optimizing PoT for multilingual reasoning and open avenues for future research on improving reasoning alignment and execution.

Our findings contribute to a deeper understanding of multilingual PoT reasoning, providing insights into fine-tuning strategies and inference-time optimizations. 
%
Future work can extend this framework to additional reasoning-intensive tasks and explore more advanced alignment techniques to enhance PoTâ€™s multilingual capabilities.



