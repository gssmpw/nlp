\subsection{R-A Relationship: Code Quality Analysis}
\label{section:code-analysis-results}


\textbf{Does Better Strategy Improve Code Quality?}
As discussed in Section~\ref{subsec:code_analysis}, we assess code quality across alignment strategies in cross-lingual and multilingual settings, focusing on Llama2-7B and CodeLlama-7B.
%
Table~\ref{tab:code-quality} shows that higher accuracy correlates with better code quality.
% 
Additionally, code quality in lower resource languages, like Bengali, is much lower than in English and German, which aligns with the accuracy trends.
%
This finding reflects the inherent challenges of generating code in low-resource languages, where model performance is typically more constrained.
%
\begin{table}[htbp]
 \tiny
  \centering
  \resizebox{\columnwidth}{!}{
  \begin{tabular}{l|c|cc|c}
    \hline
    \textbf{Method} & en & de & bn & ALL \\
    \hline\hline
    \multicolumn{5}{c}{\textit{Cross-lingual}} \\
    \hline    
    \multicolumn{1}{l|}{\underline{Llama2-7B}}& \multicolumn{1}{c|}{} & \multicolumn{2}{c|}{} & \multicolumn{1}{c}{} \\
    With Comments & \textbf{2.49} & \textbf{1.87} & 0.45 & 1.39  \\
    Without Comments & \textbf{2.49} & \textbf{1.87} & \textbf{0.49}	& \textbf{1.44}  \\ 
    \hline   
    \multicolumn{1}{l|}{\underline{CodeLlama2-7B}}& \multicolumn{1}{c|}{} & \multicolumn{2}{c|}{} & \multicolumn{1}{c}{} \\
    With Comments & \textbf{2.66} & 2.06 & \textbf{0.61}	&1.97  \\
    Without Comments & 2.55 & \textbf{2.13} & 0.54 & \textbf{2.02}  \\
    \hline\hline
    \multicolumn{5}{c}{\textit{Multilingual}} \\
    \hline
    \multicolumn{1}{l|}{\underline{Llama2-7B}}& \multicolumn{1}{c|}{} & \multicolumn{2}{c|}{} & \multicolumn{1}{c}{} \\
    PoT Cross Comment & \underline{2.56} & 2.41 & 1.26	&1.98  \\
    PoT Cross Question & 2.32 & 2.07 & 1.52	&2.03  \\
    PoT Parallel & \textbf{2.83} & \textbf{2.55} & \textbf{1.96} & \textbf{2.45}  \\
    PoT No Comment & 2.54 & \underline{2.16} & \underline{1.71}	& \underline{2.13}  \\
    
    \hline
    \multicolumn{1}{l|}{\underline{CodeLlama2-7B}}& \multicolumn{1}{c|}{} & \multicolumn{2}{c|}{} & \multicolumn{1}{c}{} \\
    PoT Cross Comment & \underline{2.84} & 2.40 & 1.34	& 2.15  \\
    PoT Cross Question & 2.45 & 2.23 & 1.54	&2.11  \\
    PoT Parallel & \textbf{2.88} & \textbf{2.68} & \textbf{2.04}	& \textbf{2.56}  \\
    PoT No Comment & 2.61 & \underline{2.41} & \underline{1.87} & \underline{2.28} \\
    % \hline\hline
    % \multicolumn{5}{c}{\textit{Oracle}} \\
    % \hline
    % \multicolumn{1}{l|}{\underline{Llama3.1-405B}}& \multicolumn{1}{c|}{} & \multicolumn{2}{c|}{} & \multicolumn{1}{c}{} \\
    % Few-Shot PoT & - & - & -	& -  \\
    \hline
  \end{tabular}
  }
  \caption{
  Code quality assessment with ICE-Score
  }
  \label{tab:code-quality}
\end{table}




% system level
% \begin{figure}[t]
% \centering
% \includegraphics[width=\columnwidth, trim={0 0 0 0.6cm}, clip]{latex/figures/corr-crosslingual-3.pdf}
%     \caption{
%     The relationship between code quality and answer accuracy in a cross-lingual setting exhibited a Spearman correlation of 0.91.
%     Each point corresponds to the accuracy and code quality for a given language, considering a specific system and model variation.
%     \textcolor{blue}{may put them next to each other, left and right with sharing axis + legend}
%     }
%     \label{fig:sys-level-assoc-cross}
% \end{figure}


% \begin{figure}[t]
% \centering
% \includegraphics[width=\columnwidth]{latex/figures/corr-multilingual-3.pdf}
%     \caption{
%     Similarly, in a multilingual setting, the observed trend between code quality and answer accuracy demonstrated a Spearman correlation of 0.76,
%     which is slightly lower than that observed in the cross-lingual scenario.
%     \textcolor{blue}{**make 2 and 3 subfigure and only one caption}
%     }
%     \label{fig:sys-level-assoc-multi}
% \end{figure}


\textbf{System Level Correlation.}  Figure~\ref{fig:sys-level-assoc} illustrates a strong system-level correlation between MGSM accuracy and code quality, as measured by \texttt{ICE-Score}.
%
Across all finetuning strategies, in both cross-lingual and multilingual, we observe a consistent trend where higher code quality positively correlates with improved accuracy.
%
This relationship is quantified by a Spearman rank correlation coefficient of 0.91 and 0.76 for cross-lingual and multilingual, respectively.
%
%
\begin{figure}[htbp]
    \centering
    \includegraphics[width=\columnwidth, trim={0 0 0 0.6cm}, clip]{latex/figures/corr-shared-y-axis-system-legend-5.pdf}
    \caption{The relationship between code quality and answer accuracy in cross-lingual and multilingual settings. Each point represents a given language, considering a specific system and model combination.
    % Both settings present a strong correlation.
    }
    \label{fig:sys-level-assoc}
\end{figure}


Notably, this correlation persists across different model architectures and code generation conditions, reinforcing the importance of alignment strategies in enhancing both code quality and accuracy. 
%
These insights highlight the broader impact of alignment and resource availability on code generation, supporting the necessity of assessing the quality of intermediate outputs.



% \begin{figure}[htbp]
%     \centering
%     \includegraphics[width=\columnwidth, trim={0 0 0 0.6cm}, clip]{latex/figures/sample-level-corr-combine-v0.pdf}
%     % \centering
%     % \begin{subfigure}{0.23\textwidth}
%     %     \centering
%     %     \includegraphics[width=\textwidth]{latex/figures/samplecorr-cross-2.pdf}
%     %     \caption{Cross-lingual setup using LLaMa2 7B.}
%     %     \label{fig:sam-level-assoc-cross}
%     % \end{subfigure}
%     % \hfill
%     % \begin{subfigure}{0.23\textwidth}
%     %     \centering
%     %     \includegraphics[width=\textwidth]{latex/figures/samplecorr-multi-2.pdf}
%     %     \caption{Multilingual setup using LLaMa2 7B.}
%     %     \label{fig:sam-level-assoc-multi}
%     % \end{subfigure}
%     \caption{
%     Distribution of code quality across answer candidates in both setups using LLaMa2 7B.
%     }
%     \label{fig:samplecorr-combined}
% \end{figure}






% \begin{figure}[htbp]
% \centering
% \includegraphics[width=\columnwidth]{latex/figures/samplecorr-cross-2.pdf}
%     \caption{
%     Distribution of code quality across answer candidates in the cross-lingual setup using LLaMa2 7B. \textcolor{blue}{two side histograms}
%     % \textcolor{blue}{remove log-scale in y // normalize red + green // wording - correct & incorrect // remove fill-in}
%     }
%     \label{fig:sam-level-assoc-cross}
% \end{figure}

% \begin{figure}[htbp]
% \centering
% \includegraphics[width=\columnwidth]{latex/figures/samplecorr-multi-2.pdf}
%     \caption{
%     Distribution of code quality across answer candidates in the multilingual setup using LLaMa2 7B.
%     }
%     \label{fig:sam-level-assoc-multi}
% \end{figure}




\textbf{Sample Level Association.}
Beyond system-level trends, we examined whether code quality can determine the correctness of individual solutions.
%
This relationship is demonstrated in Table \ref{tab:ice-score-distribution}, where the percentage distributions of \texttt{ICE-Score} for correct and incorrect answers show substantial differences across score ranges.
%
% This trend is evident in the histograms in Figures~\ref{fig:samplecorr-combined}, where the density distributions of correct and incorrect candidate answers are largely distinct.
% 
To further quantify this discriminative ability, we calculated the AUC for \texttt{ICE-Score} as a predictor of correctness, obtaining strong values of 0.94 and 0.96 for cross-lingual and multilingual settings, respectively.
% 
Additionally, a t-test reveals a statistically significant difference between the correct and incorrect groups.
A detailed language-wise analysis is provided in Appendix~\ref{ap:code-analysis}.
%
\begin{table}[htbp]
  \resizebox{\columnwidth}{!}{
    \centering
    \caption{ICE-Score distribution (\%) for correct and incorrect answers in cross- and multilingual settings.}
    \label{tab:ice-score-distribution}
    \begin{tabular}{l l C{0.7cm} C{0.7cm} C{0.7cm} C{0.7cm} C{0.7cm}}
        \toprule
        \textbf{Setting} & \textbf{Answer Type} & \textbf{0} & \textbf{1} & \textbf{2} & \textbf{3} & \textbf{4} \\
        \midrule
        \multirow{2}{*}{Cross} & Correct   &  3.4 & 1.5 & 3.9 & 3.8 & 87.3  \\
                                       & Incorrect & 75.2  & 14.2  & 8.4 & 0.7 & 1.5 \\
        \midrule
        \multirow{2}{*}{Multi}  & Correct   & 2.0 & 1.3 & 3.6 & 4.1 & 89.0 \\
                                       & Incorrect & 52.8 & 25.8 & 17.5 & 2.3 & 1.6 \\
        \bottomrule
    \end{tabular}
  }
\end{table}

% utilization
\textbf{Application in Test-Time Scaling.}
% 
We now explore the potential of applying the ICE-Score as a heuristic for test-time scaling. 
%
We evaluate three approaches as discussed in Section~\ref{subsec:code_analysis}: (i) baseline model predictions without scaling, (ii) Self-Consistency (\texttt{SC}), and (iii) Soft Self-Consistency (\texttt{Soft-SC}) guided by the \texttt{ICE-Score}.
%
As shown in Table~\ref{tab:test-time-scaling}, our results indicate that test-time scaling substantially improves reasoning accuracy across both cross-lingual and multilingual settings. 
%
Conventional \texttt{SC} provides moderate gains, but \texttt{Soft-SC} with \texttt{ICE-Score} further boosts performance by prioritizing high-quality reasoning steps. 
%
Notably, for Llama2-7B, \texttt{Soft-SC} improves \textbf{cross-lingual performance from 39.2 to 56.6} and \textbf{multilingual performance from 57.2 to 71.2}. 
%
Similarly, CodeLlama-7B shows strong gains in both setups, demonstrating the method's robustness across model architectures.
%
These findings underscore the benefit of intermediate quality assessment as a means to improve cross-lingual and multilingual PoT reasoning and overall performance.

\begin{table}[htbp]
\tiny
  \centering
  \resizebox{\columnwidth}{!}{
  \begin{tabular}{l|c|cc|c}
    \hline
    \textbf{Method} & en & de & bn & ALL \\
    \hline\hline
    \multicolumn{5}{c}{\textit{Cross-lingual}} \\
    \hline    
    \multicolumn{1}{l|}{\underline{Llama2-7B}}& \multicolumn{1}{c|}{} & \multicolumn{2}{c|}{} & \multicolumn{1}{c}{} \\
    Without Comments & 58.0 & 40.4 & 12.0	&31.6  \\
    + \texttt{SC} & 65.2 & 51.6 & 15.2 &39.2  \\
    + \texttt{Soft-SC} (\texttt{ICE-Score}) & \textbf{76.8} & \textbf{69.2} & \textbf{33.6}	& \textbf{56.6}  \\ 
    \hline   
    \multicolumn{1}{l|}{\underline{CodeLlama-7B}}& \multicolumn{1}{c|}{} & \multicolumn{2}{c|}{} & \multicolumn{1}{c}{} \\
    Without Comments & 58.8 & 48.4 & 11.2 & 38.6  \\
    + \texttt{SC} & 69.6 & 57.2 & 17.2 & 46.7  \\
    + \texttt{Soft-SC} (\texttt{ICE-Score}) & \textbf{75.7} & \textbf{71.2} & \textbf{33.6} & \textbf{61.1}  \\
    \hline\hline
    \multicolumn{5}{c}{\textit{Multilingual}} \\
    \hline
    \multicolumn{1}{l|}{\underline{Llama2-7B}}& \multicolumn{1}{c|}{} & \multicolumn{2}{c|}{} & \multicolumn{1}{c}{} \\
    PoT Parallel & 56.0 & 47.2 & 30.8	& 44.6   \\
    + \texttt{SC} & 64.8 & 58.0 & 47.6	& 57.2  \\
    + \texttt{Soft-SC} (\texttt{ICE-Score}) & \textbf{77.6} & \textbf{72.0} & \textbf{65.6}	& \textbf{71.2} \\
    \hline   
    \multicolumn{1}{l|}{\underline{CodeLlama-7B}}& \multicolumn{1}{c|}{} & \multicolumn{2}{c|}{} & \multicolumn{1}{c}{} \\
    PoT Parallel & 61.9 & 52.8 & 35.6 & 49.0  \\
    + \texttt{SC} & 68.8 & 66.4 & 53.6 & 62.8  \\
    + \texttt{Soft-SC} (\texttt{ICE-Score}) & \textbf{79.2} & \textbf{77.6} & \textbf{68.8} & \textbf{75.6}  \\     
    \hline
  \end{tabular}
  }
  \caption{
  A comparative analysis of performance when implementing conventional \texttt{SC} and the proposed \texttt{Soft-SC} with \texttt{ICE-Score} in an optimal framework for cross-lingual and multilingual configurations.
  }
  \label{tab:test-time-scaling}
\end{table}
























% \subsection{What attribute for marginal gains when dropping the comments in cross-lingual?}

% \textbf{Cross-lingual code quality result} \dummy{\lipsum[2]}




% \textbf{Cross-lingual code quality associate with the accuracy} \dummy{\lipsum[2]}




% \textcolor{red}{whatif we have the signal that is associated with the downstream accuracy. We can utilize them to further boost the downstream result -- point out that CodeBERTScore does not work in the appendix table. Scatter plot of CodeBERT and spearman and discuss that we should use ICE-Score}



% \textcolor{blue}{To what extent, does better alignment training strategy attribute to code quality? -- start with the key takeaways}
% \textbf{What does the optimal training strategy attribute to? Code Quality.} \textcolor{red}{How can we justify? Choice: i) Density plot between two disks ii) graph } \dummy{\lipsum[2]}






% \textbf{There is an association in code quality with accuracy where the optimal strategy provides the best of both.} \dummy{\lipsum[2]}

% \textbf{Cross-lingual consideration} \dummy{\lipsum[2]}

% \textcolor{red}{The color in the scatter plot is not meaningful. Language-wise comparison, e.g., t-test looks good. Maybe box-plot with minimax and sd.}


% \subsection{Test-time Improvement for PoT}

% i) system wise
% ii) Sample wise
% iii) language wise?  (can also appear in i and ii)

% \textcolor{blue}{Strong corr between code quality and acc. Then we investigate the potential of test-time improvement}

% \textcolor{red}{what if the test-time boost is poor? What can we learn about it? Or can we refine the program?}

% \textcolor{blue}{From the analysis it's a system level, not a sample level, how can we leverage it? An example level can be demonstrated through distribution. -- AUC and t-test at sample level}


% \textcolor{red}{modify the inference cost by reducing the beam size instead of varying K from fixed beam size}







% \subsection{Does PoT deliver better intermediate reasoning than CoT?}

% \textbf{Adopting the teacher PoT deliver better intermediate reasoning than CoT - Table~\ref{tab:intermediate-reason}} \lipsum[2]

% \textcolor{red}{change the 6.1 to last and put 6.3 to prior. Start from the main impact in the reasoning fields.}















%\subsection{Does PoT robust to the distributional shift?}

%\textbf{[cite GSM-Symbolic] shown that LLM can not generalize to (our best attempt)} \dummy{\lipsum[2]}







% \subsection{What Cause the Notable Advancement in Underrepresented Languages?}

% \textcolor{red}{May drop this one or just empirically demonstrated the alignment in multilingual and cross-lingual -- rethinking to phase this section}

% \textbf{Language generation in URL is more challenging than HRL} \lipsum[2]

%\textbf{PoT can bypass those challenge and conquer the problem} \dummy{\lipsum[2]}









%\subsection{(low priortity - may appear in the appendix) Specialized-Models Comparison}

%\textbf{Comparison between Llemma (math-specific) CoT vs CodeLlama (code-specific) PoT} \dummy{\lipsum[2]}








