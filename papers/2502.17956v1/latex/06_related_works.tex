\section{Related Work}

\textbf{Mathematical Reasoning.}
%
Recent advancements in LLMs' mathematical reasoning capabilities have been driven by Chain-of-Thought (CoT) prompting \cite{og-cot-prompt, scratchpadNye}, which significantly outperforms direct-answer approaches by generating intermediate step-by-step reasoning.
%
Building on CoT, various enhancements have emerged, including self-consistency, which replaces greedy decoding with sampling-based inference to select the most consistent solution \cite{wang2023selfconsistency}.
 %
Meanwhile, PoT and PaL \cite{pot, pal} improve reasoning by delegating computation to a Python interpreter, reducing the task of translating problems into code.
%

Another key advancement is instruction fine-tuning on mathematical datasets.
%
\citet{metamath} introduced MetaMathQA, expanding existing datasets through diverse rephrasings, while \citet{mammoth} leveraged a hybrid MathInstruct dataset combining CoT’s generality with PoT’s computational precision.
%
Additionally, external tool integration has been explored \cite{mario, tora}, with curated tool-use datasets enhancing LLMs’ reasoning capabilities.
%

\textbf{Multilingual Mathematical Reasoning.}
%
Despite LLMs' advancements in English mathematical reasoning, their performance in other languages still lags.
%
Efforts to bridge this gap include sample translation for multilingual alignment \cite{mathoctopus, mcot, qalign} and multilingual preference optimization \cite{mapo}.
%
\citet{mathoctopus} created a multilingual mathematical dataset by translating GSM8K into ten languages, though accurate translations remain a costly and time-consuming endeavor.
%
To mitigate this, \citet{qalign} proposed a two-step approach: translating questions into English before fine-tuning on larger English datasets like MetaMathQA.
%
Alternatively, \citet{mapo} leveraged existing translation models as alignment signals for preference optimization.
%

Beyond dataset translation, prompting techniques offer a cost-effective alternative.
%
\citet{not-all-lang-cross-lingual-cot-prompt} introduced role-playing prompts where the model first translates questions into English before applying CoT reasoning.
%
\citet{tot-multi-prompt} proposed a Tree-of-Thought framework for structured, multi-step reasoning across languages.
%
\citet{crosspal} extended PoT with Cross-PAL, aligning reasoning across multiple languages through code generation.

