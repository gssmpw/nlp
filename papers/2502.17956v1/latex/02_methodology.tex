\section{Proposed Studies}


%\textcolor{blue}{restructure and shorten // dataset generation move to section 3 -- define what we can observe // jab Simplify -> Prof. Sarana revise}


% \section{Fine-tuning Decisions}
% \subsection{Cross-lingual Fine-tuning}
% \subsection{Multilingual Fine-tuning}
% \section{Test-time Scaling}
% \section{Conclusion}




% \textcolor{blue}{What makes others fail and why we do this way. Find a way to demonstrate.}

% \textcolor{red}{research question / hypothesis intro, then sequence section 3 -- To what extent does ICE correlate with performance at the sample level?}

% \subsection{Problem Formulation} 

%A dataset for the mathematical reasoning task can be represented as $\mathcal{D} = \{(\vb*{Q}_i, \vb*{R}_i, \vb*{A}_i)\}_{i=1}^N$, where $\vb*{Q}_i$ represents a math question, $\vb*{R}_i$ is a sequence of intermediate reasoning steps, $\vb*{A}_i$ is a numerical answer, and $N$ is the number of samples in the dataset. 
%
%Similar to \citet{mathoctopus}, we train the models to reason through the language modeling objective:
%\begin{equation}
%    \mathcal{L} = - \frac{1}{B}\sum_{i=1}^B\log p_\theta(\vb*{R}_i|\vb*{Q}_i)
%\label{eq:loss}
%\end{equation}
%where $B$ is the batch size.


%During inference, a trained language model generates multi-step reasoning, denoted as $\hat{\vb*{R}_i}\sim p_\theta(\vb*{Q}_i)$.
%
%In natural language reasoning, the final numerical answer is derived from the predicted chain of thought ($\hat{\vb*{C}_i}$) by applying regular expressions ($\mathsf{Re}$) for pattern matching, represented as  $\hat{\vb*{A}_i}=\mathsf{Re}(\hat{\vb*{C}_i})$.
% 
%In contrast, in programming-based reasoning, particularly within the PoT framework, the answer is obtained by executing ($\mathsf{Ex}$) the generated program-of-thought ($\hat{\vb*{R}_i}$) formulated as $\hat{\vb*{A}_i}=\mathsf{Ex}(\hat{\vb*{R}_i})$.



% \subsection{Fine-Tuning Experimental Design}
%\subsection{GSM8KPoT \& MGSM8KPoT}






%\subsection{Program-of-Thought Data}
\subsection{Fine-tuning for Q-R Alignment (P1)}

% \vspace{2mm}
% \noindent \textbf{Dataset Generation}:
% \label{section:synthesize-pot}
% 
To fairly compare PoT and CoT, we use the \emph{Grade School Math} (\texttt{GSM8K}) dataset \cite{cobbe2021gsm8k} and explore three prompting strategies for generating PoT with an Oracle LLM: (i) zero-shot PoT, (ii) few-shot PoT, and (iii) the proposed few-shot PoT with CoT guidance, 
%xw
as shown in Figure~\ref{fig:pipeline-cross}.
%
% Zero-shot PoT prompting generates Python solutions without examples, while few-shot PoT prompting improves upon this by providing two solved examples. The most effective approach, few-shot PoT prompting with CoT guidance, further enhances program generation by integrating CoT reasoning and PoT solutions within the prompt.
Zero-shot PoT generates Python solutions without examples. Few-shot PoT improves this with two solved examples while adding CoT guidance further enhances program generation.
%
This structured guidance substantially improves accuracy, achieving a 96.1\% correctness rate in PoT-generated samples and leading to the development of the \texttt{GSM8KPoT} dataset (details in Appendix~\ref{ap:pot-syn}).



\begin{figure}[!t]
    \includegraphics[width=\linewidth, trim={0 0 0 0.1cm}, clip]{latex/figures/pipeline-cross-v2.pdf}
    \caption{
    The generation pipeline for \texttt{GSM8KPoT}, in which a PoT answer ($\vb*{R}_i^\text{en}$) is synthesized using the Oracle LLM, with additional natural language reasoning ($\vb*{C}_i^\text{en}$) provided as guidance.
    }
    \label{fig:pipeline-cross}
\end{figure}








Examining PoT in multilingual settings is challenging due to the scarcity of datasets that align questions across multiple languages with structured reasoning steps.
%
To address this, we construct dataset variants (outlined in Table~\ref{tab:compare-metods}) to evaluate cross-lingual and multilingual fine-tuning.
%
% Table~\ref{tab:compare-metods} shows how we control for language effects by varying question and inline comment languages. This enables us to assess the impact of each fine-tuning strategy.
We control for language effects by varying the languages of questions and inline comments, allowing us to assess the impact of each fine-tuning strategy.

In the context of cross-lingual and multilingual PoT, inline comments can potentially play a crucial role. As established by \citet{mgsm}, the language used in multi-step reasoning processes, such as those in CoT reasoning, is a key design consideration.
%
We hypothesize that the design choices for inline comments in PoT function similarly to the language considerations in CoT. 
% Therefore, we conduct a comprehensive analysis to examine the implications of these choices.
Thus, we analyze its implications comprehensively.


%



% \begin{table}[h]
%   \centering
%   \resizebox{\columnwidth}{!}{
%   \begin{tabular}{C{0.8cm}|C{0.8cm}|C{1cm}|C{2.7cm}|C{2.7cm}}
%     \hline
%     \textbf{Setup} & Lang. of $Q$ & Lang. of Comm. in $R$ & Source Dataset & Resultant Dataset \\
%     \hline
%     Cross & En & En & \texttt{GSM8K} & \texttt{GSM8KPoT-en} \\
%     % \hdashline
%     \cdashline{2-5}
%     & En & -  & \texttt{GSM8K} &  \texttt{GSM8KPoT-nc} \\
%     \hline
%     Multi & En & Multi & \texttt{MGSM8kInstruct} + \texttt{GSM8KPoT-en} & \texttt{GSM8KPoT} \texttt{-cross-comment} \\
%      \cdashline{2-5}
%      & Multi & En & \texttt{MGSM8kInstruct} + \texttt{GSM8KPoT-en} & \makecell{\texttt{GSM8KPoT} \\ \texttt{-cross-question}} \\    
%     \cdashline{2-5}
%      & Multi & - & \texttt{MGSM8kInstruct} + \texttt{GSM8KPoT-nc}& \makecell{\texttt{GSM8KPoT} \\ \texttt{-nc}} \\
%     \cdashline{2-5}
%      & Multi & Multi & \texttt{MGSM8kInstruct} + \texttt{GSM8KPoT-en}& \makecell{\texttt{GSM8KPoT} \\ \texttt{-parallel}} \\
%     \hline
%   \end{tabular}
%   }
%   \caption{
%   The number of correct 
%   \textcolor{blue}{(ready to narrate in the paragraph)}
%   }
%   \label{tab:compare-metods}
% \end{table}















\subsubsection{Cross-lingual Setup}
% As shown in Table~\ref{tab:compare-metods},
The cross-lingual setup comprises two datasets: one with inline comments in the reasoning steps and one without, defined as follows.
%
\begin{table}[!t]
  \vspace{-2mm}

  \centering
  \renewcommand{\arraystretch}{1.4}

  \resizebox{\columnwidth}{!}{
  \begin{tabular}{C{1.0cm}|C{1cm}|C{1cm}|C{2.5cm}|C{0.3cm}}
    \hline
    \textbf{Setup} & Lang. of $Q$ & Lang. of Comm. in $R$ & Dataset & Eq. \\
    \hline
    Cross & En & En & $\mathcal{D}^\texttt{GSM8KPoT}_\texttt{en}$  & \ref{eq:gsm8kpot}\\
    % \hdashline
    \cdashline{2-5}
    & En & \emph{nc}  &  $\mathcal{D}^\texttt{GSM8KPoT}_\texttt{nc}$ & \ref{eq:gsm8kpot-nc} \\
    \hline
    Multi & En & Multi & $\mathcal{D}^\texttt{MGSM8KPoT}_\texttt{cross-comment}$ & \ref{eq:mgsm8kpot-cross-comment}\\
     \cdashline{2-5}
     & Multi & En & $\mathcal{D}^\texttt{MGSM8KPoT}_\texttt{cross-question}$ & \ref{eq:mgsm8kpot-cross-question}\\    

    \cdashline{2-5}
     & Multi & Multi & $\mathcal{D}^\texttt{MGSM8KPoT}_\texttt{parallel}$ & \ref{eq:mgsm8kpot-parallel} \\

    \cdashline{2-5}
     & Multi & \emph{nc} & $\mathcal{D}^\texttt{MGSM8KPoT}_\texttt{nc}$ & \ref{eq:mgsm8kpot-nc} \\
    \hline
  \end{tabular}
  }
  \vspace{-2mm}
  \caption{
  Our proposed study employs multiple approaches, leveraging the question-comment characteristics within the dataset to compare different best fine-tuning strategies. \emph{NC} stands for ``no comment''.
  }
  \label{tab:compare-metods}
\end{table}


\vspace{2mm}
\noindent
\underline{\emph{En-En}} --- 
% To construct the first dataset, we adopt 
% the \emph{Grade School Math (GSM)} dataset called, 
% $\texttt{GSM8K}$, as our source dataset.
%
% We convert their reasoning steps from natural language to Python code using the process described in a~\ref{section:synthesize-pot}. 
% 
% This dataset can be denoted as in the following equation.
We employ \texttt{GSM8KPoT} as the foundational dataset, which can be formally represented by the following equation.
\begin{equation}
\mathcal{D}^\texttt{GSM8KPoT}_\texttt{en} = \{(\vb*{Q}_i^\text{en}, \vb*{R}_i^\text{en})\}_{i=1}^N,
\label{eq:gsm8kpot}
\end{equation}
where the questions $\vb*{Q}_i^\text{en}$ are obtained from English GSM8K, and the synthesized intermediate reasoning in the programming language ($\vb*{R}_i^\text{en}$) include inline comments in English. Note that the superscript \text{en} in $\vb*{R}_i^\text{en}$ denotes the language of code comments. 
% 

\vspace{2mm}
\noindent
\underline{\emph{En-nc}} --- We also include a variant with all comments removed.
%
\begin{equation}
\mathcal{D}^\texttt{GSM8KPoT}_\texttt{nc} = \{(\vb*{Q}_i^\text{en}, \vb*{R}_i^\text{nc})\}_{i=1}^N,
\label{eq:gsm8kpot-nc}
\end{equation}
where the superscript ``$\text{nc}$'' in the reasoning steps $\vb*{R}_i^\text{nc}$ stands for ``no comment''. 


%These datasets are independently used to fine-tune language models following the supervised learning objective outlined in Equation~\ref{eq:loss}.
%
%Following the zero-shot cross-lingual approach described in \citet{xtreme}, each model is further evaluated on unseen languages using the test dataset, $\mathcal{D}^\text{test} = \{(\vb*{Q}_i^l, \vb*{A}_i^l) |l\in L_\text{tgt} \}_{i=1}^M$, where $L_\text{tgt}$ denotes the set of unseen target languages and $M$ represents the number of test samples.
% 
% The trained language models ($p_\theta$) generate sequences of reasoning steps $\hat{\vb*{R}}_i^l\sim p_\theta(\vb*{Q}_i^l)$.
% 



% The predicted answers are then obtained through a programmatic execution process ($\mathcal{E}$), expressed as $\hat{\vb*{A}}_i^l=\mathcal{E}(\hat{\vb*{R}}_i^l)$.
% Finally, the predicted answers are compared with the ground-truth answers to evaluate their accuracy.



\subsubsection{Multilingual Setup}

% To examine the effectiveness of PoT in a multilingual context, where explicit reasoning demonstrations are available in target languages, we conduct a comparative analysis with the CoT approach.
% We adopt MGSM8KInstruct~\cite{mathoctopus} as the reference dataset for CoT in multilingual settings.
% This dataset comprises question-reasoning pairs $(\vb*{R}_i$, $\vb*{Q}_i)$ with $\vb*{Q}_i$ expressed in English, along with translations in nine additional languages, enabling the alignment of reasoning capabilities across different languages.
% \citet{mathoctopus} introduced two training strategies:

% As shown in Table~\ref{tab:compare-metods},
The multilingual setup comprises four datasets.
%
Following the concept proposed in MGSM8KInstruct~\cite{mathoctopus}, we consider CoT cross and CoT parallel strategies, varying how the languages of questions and inline comments match or mismatch.
%
% \begin{compactenum}[(i)]
%     % \item \emph{CoT Cross}: Incorporates English questions with answers in the target language, promoting multilingual adaptability. 
%     \item \emph{CoT Cross}: One dataset pairs English questions with reasoning steps containing comments in the target language ($\mathcal{D}^\texttt{MGSM8KPoT}_\texttt{cross-comment}$), while the other pairs multilingual questions with reasoning steps containing English comments ($\mathcal{D}^\texttt{MGSM8KPoT}_\texttt{cross-question}$). 
%    
%     %where English questions have answers in the target language, promoting adaptability, and
%     % \item \emph{CoT Parallel}: Uses question-answer pairs in the same language to enhance the PoT capability within each target language (further details in Appendix~\ref{ap:mathoctopus}).
%     \item \emph{CoT Parallel}: A dataset where both questions and answers are in the same language ($\mathcal{D}^\texttt{MGSM8KPoT}_\texttt{parallel}$).
%     %(details in Appendix~\ref{ap:mathoctopus}).
% \end{compactenum}
%
% In addition, we include a no-comment variant where multilingual questions are paired with reasoning steps that exclude comments. 
We also include a no-comment variant, pairing multilingual questions with reasoning steps that exclude comments.
%
These four datasets are defined as follows. 
%

\vspace{2mm}
\noindent
\underline{\emph{En-Multi}} --- Following the CoT definition in CoT Cross, we translate English inline comments using machine translation (\texttt{MT}), producing program reasoning in target languages:
    \begin{equation}
        \mathcal{D}^\texttt{MGSM8KPoT}_\texttt{cross-comment} = \{(\vb*{Q}_i^\text{en}, \vb*{R}_i^l)|l\in L_\text{all}\}_{i=1}^N,
        \label{eq:mgsm8kpot-cross-comment}
    \end{equation}
    % 
where $L_\text{all}$ denotes the language set. 
%
The superscript $l$ in $\vb*{R}_i^l$ is a variable representing a language. 

\vspace{2mm}
\noindent
\underline{\emph{Multi-En}} --- This variant provides multilingual questions $\vb*{Q}_i^l$ by applying machine translation to $\vb*{Q}_i^\text{en}$, while keeping the inline comments in English.
    \begin{equation}
        \mathcal{D}^\texttt{MGSM8KPoT}_\texttt{cross-question} = \{(\vb*{Q}_i^l, \vb*{R}_i^\text{en})|l\in L_\text{all}\}_{i=1}^N.
        \label{eq:mgsm8kpot-cross-question}
    \end{equation}
    % where the PoT answers also came from $\mathcal{D}^\texttt{GSM8KPoT}_\texttt{en}$.

\vspace{2mm}
\noindent    
\underline{\emph{Multi-Multi}} --- Both questions and inline comments are in the same language $l$:
    \begin{equation}
        \mathcal{D}^\texttt{MGSM8KPoT}_\texttt{parallel} = \{(\vb*{Q}_i^l, \vb*{R}_i^l)|l\in L_\text{all}\}_{i=1}^N.
        \label{eq:mgsm8kpot-parallel}
    \end{equation}    
Note that in this case, the superscript $l$ in $\vb*{Q}_i^l$ and $\vb*{R}_i^l$ denotes the fact that both question and inline comments are in the same language. 

\vspace{2mm}
\noindent
\underline{\emph{Multi-nc}} --- Similar to $\mathcal{D}^\texttt{GSM8KPoT}_\texttt{nc}$, we also include a no-comment variant for this setup. 
    \begin{equation}
        \mathcal{D}^\texttt{MGSM8KPoT}_\texttt{nc} = \{(\vb*{Q}_i^l, \vb*{R}_i^\text{nc})|l\in L_\text{all}\}_{i=1}^N.
        \label{eq:mgsm8kpot-nc}
    \end{equation}

%This structured comparison enables an empirical evaluation of different translation strategies for multilingual PoT adaptation.


% -----To study the potential of PoT in multilingual setup, where the machine translation (MT), denotes as $\mathcal{T}$, is accessible to align the programming capability into target languages.
% We employ several strategies to incorporate the MT to translate the sample from the source language ($\mathcal{L}_\text{src}$), specifically English, into the target languages.

% \begin{compactenum}[(1)]
%     \item \textit{MathOctopus-PoT Cross}: ...
%    \item \textit{MathOctopus-PoT Parallel}: ...
%\end{compactenum}
% Formally, each sample $(\vb*{Q}_i^\text{en}, \vb*{A}_i^\text{en})$ is then translated into a set of samples $\{(\vb*{Q}_i^l, \vb*{A}_i^l)| l\in \mathcal{L}_\text{tgt} \}$.

% \dummy{\lipsum[1]}



\subsection{Code Quality Analysis (P2)} \label{subsec:code_analysis}

% \textcolor{blue}{do we care about other aspects? // justify why we are interested in functional correctness}

% motivate the program generation and execution
% After addressing the numerical problem within a multilingual context using PoT, the task is decomposed into two essential components: multi-step reasoning facilitated through program generation and execution performed via a Python interpreter for numerical computations.
%
% While the interpreter ensures precision in executing complex operations and arithmetic calculations, the primary challenge for models lies in generating programs that are both syntactically and logically sound.
%
% This approach differs from the step-by-step correctness required in CoT, where errors can occur not only in the logical reasoning process but also in arithmetic computations.
After addressing the multilingual problem with PoT, the task is split into two parts: multi-step reasoning via code generation and execution via a Python interpreter for numerical computations.
% While the interpreter ensures arithmetic accuracy, the challenge lies in generating syntactically and logically correct programs, which risks both logical and arithmetic errors -- unlike CoT.
While the interpreter ensures arithmetic accuracy, the challenge lies in generating syntactically and logically correct programs free of errors.


% in-depth investigation in parallel to CoT quality
% To this end, we conceptualize code quality as the intermediate correctness of PoT, evaluated using the \texttt{ICE-Score} \cite{ice-score}. This metric comprises two key components: usefulness, which measures the extent to which the generated code effectively addresses the user query,
% \textcolor{red}{and functional correctness, which is assessed through unit tests and reference code}
% and functional correctness, which is evaluated through intermediate-step validation of code snippets with the support of an Oracle LLM.
% 
% In this study, our primary focus is on the functional correctness aspect to analyze the intermediate validity of program generation.
%
% The \texttt{ICE-Score} is a numerical scale ranging from 0 to 4, where a score of 0 indicates that the code snippet is both incorrect and lacks substantive content, while a score of 4 signifies complete correctness.
We assess code quality using the \texttt{ICE-Score} \cite{ice-score}, which measures usefulness (how well the code addresses the query) and functional correctness (evaluated through intermediate validation with an Oracle LLM). Our focus is functional correctness, rating program validity from 0 (incorrect/incomplete) to 4 (fully correct).

% system wise + sample wise analysis
% Accordingly, we utilize this metric to compare the intermediate validity of candidate code generation with the correctness of the final answer produced by the trained models at two levels:
%
We use \texttt{ICE-Score} to assess whether improved alignment strategies enhance both accuracy and code quality. 
Furthermore, to compare code quality with final answer accuracy, we conduct two analyses:
% Furthermore, to explore the relationship between code quality and the accuracy of the final answer, we conduct two separate analyses.
% \textcolor{blue}{shorten 2 levels}
\begin{inparaenum}[(i)]
    \item \emph{System level}: Spearman correlation assesses whether higher-quality code improves overall model performance.
    \item \emph{Sample level}: AUC and t-test assess whether code validity can determine answer correctness.
\end{inparaenum}


% exploitation using soft self-consistency
% Furthermore, we investigate the potential of leveraging the \texttt{ICE-Score} to enhance model inference in test-time scaling scenarios.
% Based on the strong baseline approach 
% Building upon the strong baseline approach of Self-Consistency (\texttt{SC}) \cite{wang2023selfconsistency}, which generates multiple reasoning candidates and applies majority voting, we can substantially improve the performance of trained models on complex tasks. This method can be regarded as a form of hard voting.
% soft-sc
% We presume that by incorporating the code quality score alongside multiple PoT candidates, we can adopt the Soft Self-Consistency (\texttt{Soft-SC}) method \cite{soft-sc}, where each candidate answer is ranked by the average of interesting metric, for example, log-likelihood, etc.
% This approach extends the hard voting mechanism to promote a soft voting strategy, potentially leading to further performance enhancements.
% Soft Self-Consistency (\texttt{Soft-SC}) \cite{soft-sc} is introduced by integrating the code quality score with multiple PoT candidates. Specifically, each candidate answer is ranked based on the average of relevant metrics, with the \texttt{ICE-Score} serving as the primary criterion. This approach extends the conventional hard voting mechanism to a soft voting strategy, potentially leading to further performance improvements.
% 
\textbf{Test-time Scaling.} We investigate the use of \texttt{ICE-Score} to enhance model inference in test-time scaling. Building on Self-Consistency (\texttt{SC}) \cite{wang2023selfconsistency}, which generates multiple reasoning candidates and applies majority voting (hard voting), we extend this approach with Soft Self-Consistency (\texttt{Soft-SC}) \cite{soft-sc}.
\texttt{Soft-SC} refines this process by averaging the \texttt{ICE-Score} for each final answer candidate, ranking responses by overall code quality. This shift from hard to soft voting may improve performance.

\subsection{Discussions}



The six datasets enable us to examine how language alignment and inline comments impact cross-lingual and multilingual PoT reasoning. 
%
Inline comments act as alignment anchors between questions and reasoning steps expressed in a programming language. 
%
However, they can hinder cross-lingual generalization to unseen languages. 
%
In this respect, we aim to understand (i) how multilingual data availability influences PoT’s ability to generate accurate reasoning steps and (ii) how inline comments affect performance across language setups.
% 

Code quality analysis provides an intermediate observation linking these decisions to the accuracy of the final answer. 
%
By examining both aspects, we establish a structured understanding of how multilingual data and inference-time strategies interact to improve PoT performance, laying the groundwork for our experimental validation in Section~\ref{section:code-analysis-results}.

















\begin{table*}[ht]
\tiny
  \centering
  \resizebox{\textwidth}{!}{
  % \begin{tabular}{l|l|llllll|lll|l}
  \begin{tabular}{l|llllllllll|l}
    \hline
    % \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} &  \multicolumn{6}{c|}{\textbf{HRL}} & \multicolumn{3}{c|}{\textbf{URL}} & \multicolumn{1}{c}{\textbf{Avg.}} \\
    \textbf{Method} & en & de & fr & es & ru & zh & ja & th & sw & bn & All \\
    \hline
    \multicolumn{1}{l|}{\underline{Llama2-7B}} &     %     % \multicolumn{1}{c|}{} & \multicolumn{6}{c|}{} & \multicolumn{3}{c|}{} & \multicolumn{1}{c}{} \\
    \multicolumn{1}{c}{} & \multicolumn{6}{c}{} & \multicolumn{3}{c|}{} & \multicolumn{1}{c}{} \\    
    CoT & 43.6 & 32.4 & 30.4 & 30.4 & 26.4 & 25.2 & 15.2 & 4.8 & 2.0 & 5.6 & 21.6 \\
    PoT & \textbf{58.0} & \textbf{40.4} & \textbf{40.4} & \textbf{43.6} & \textbf{37.1} & \textbf{38.4} & \textbf{32.7} & \textbf{7.6} & \textbf{5.6} & \textbf{12.0} & \textbf{31.6} \\
    \hline
    \multicolumn{1}{l|}{\underline{CodeLlama 7B}} & 
    % \multicolumn{1}{c|}{} & \multicolumn{6}{c|}{} & \multicolumn{3}{c|}{} & \multicolumn{1}{c}{} \\
    \multicolumn{1}{c}{} & \multicolumn{6}{c}{} & \multicolumn{3}{c|}{} & \multicolumn{1}{c}{} \\  
    CoT & 43.2 & 33.2 & 32.8 & 39.6 & 26.8 & 27.2 & 18.8 & 16.4 & 3.2 & 9.2 & 25.0 \\
    PoT & \textbf{58.8} & \textbf{48.4} & \textbf{51.6} & \textbf{53.6} & \textbf{49.8} & \textbf{41.6} & \textbf{39.6} & \textbf{26.8} & \textbf{4.4} & \textbf{11.2} & \textbf{38.6} \\
    \hline    
    \multicolumn{1}{l|}{\underline{Llama2-13B}} &     %     % \multicolumn{1}{c|}{} & \multicolumn{6}{c|}{} & \multicolumn{3}{c|}{} & \multicolumn{1}{c}{} \\
    \multicolumn{1}{c}{} & \multicolumn{6}{c}{} & \multicolumn{3}{c|}{} & \multicolumn{1}{c}{} \\     
    CoT & 47.4 & 39.2 & 37.6 & 41.2 & 38.0 & 35.2 & 18.8 & 7.2 & \textbf{7.4} & 6.8 & 27.9 \\
    PoT & \textbf{64.0} & \textbf{52.4} & \textbf{54.4} & \textbf{55.6} & \textbf{51.2} & \textbf{44.0} & \textbf{40.0} & \textbf{13.9} & 7.2 & \textbf{13.6} & \textbf{39.6} \\
    \hline        
    \multicolumn{1}{l|}{\underline{Llama3-8B}} &    % \multicolumn{1}{c|}{} & \multicolumn{6}{c|}{} & \multicolumn{3}{c|}{} & \multicolumn{1}{c}{} \\
    \multicolumn{1}{c}{} & \multicolumn{6}{c}{} & \multicolumn{3}{c|}{} & \multicolumn{1}{c}{} \\ 
    CoT & 62.8 & 51.2 & 52.8 & 54.8 & 45.2 & 40.0 & 33.6 & 39.6 & 28.0 & 39.6 & 44.8 \\
    PoT & \textbf{68.4} & \textbf{62.2} & \textbf{59.2} & \textbf{62.4} & \textbf{60.4} & \textbf{52.4} & \textbf{45.4} & \textbf{43.6} & \textbf{34.8} & \textbf{46.0} & \textbf{53.5} \\    
    % \hline\hline
    % \multicolumn{12}{c}{\textit{Oracle}} \\
    % \hline
    % \multicolumn{1}{l|}{\underline{Llama3.1-405B}} &    % \multicolumn{1}{c|}{} & \multicolumn{6}{c|}{} & \multicolumn{3}{c|}{} & \multicolumn{1}{c}{} \\
    % \multicolumn{1}{c}{} & \multicolumn{6}{c}{} & \multicolumn{3}{c|}{} & \multicolumn{1}{c}{} \\ 
    % Few-Shot CoT & xx.x & xx.x & xx.x & xx.x & xx.x & xx.x & xx.x & xx.x & xx.x & xx.x & xx.x \\
    % Few-Shot PoT & 92.2	&89.1	&85.9	&92.1	&92.6	&87.9	&86.4	&64.8	&85.9	&59.6 & 83.6 \\
    % \hline
    % \multicolumn{1}{l|}{\underline{o1-mini [OpenAI]}}& \multicolumn{1}{c|}{} & \multicolumn{6}{c|}{} & \multicolumn{3}{c|}{} & \multicolumn{3}{c}{} \\
    % CoT & xx.x & xx.x & xx.x & xx.x & xx.x & xx.x & xx.x & xx.x & xx.x & xx.x & xx.x \\
    % PoT & xx.x & xx.x & xx.x & xx.x & xx.x & xx.x & xx.x & xx.x & xx.x & xx.x & xx.x \\
    \hline
  \end{tabular}
  }
  \caption{
  Accuracy (\%) on MGSM in \textbf{cross-lingual setting}.
  % \textcolor{blue}{separte result table page of cross-lingual and multilingual}
  }
  \label{tab:main-cross}

\end{table*}


