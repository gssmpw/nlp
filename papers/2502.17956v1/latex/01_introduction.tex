% \subsection{Cross-lingual Fine-tuning}
% \subsection{Multilingual Fine-tuning}
% \section{Test-time Scaling}
% \section{Conclusion}



\section{Introduction}

%
Multi-step reasoning is crucial for large language models (LLMs), enabling them to effectively solve complex tasks, including logical, mathematical, and symbolic problems
\cite{og-cot-prompt, tot}.
%
Extending this capability to multilingual settings can greatly expand accessibility, allowing diverse multilingual communities to benefit from these advances. 
% 
However, \citet{mgsm, mathoctopus} showed that LLMs perform worse in non-English languages due to differences in linguistic structure and training data. 
%
This finding highlights the need for approaches that tackle both multi-step reasoning and multilingual challenges.

\subsection{Research Gap}

Traditionally, multi-step reasoning has been handled through chain-of-thought (CoT)~\cite{og-cot-prompt}, which allows LLMs to tackle mathematical problem-solving by breaking down problems into sequential reasoning steps.
%
However, CoT requires models to handle both reasoning and computation, often leading to errors, particularly in multilingual contexts where linguistic disparity exacerbates the challenge. 
% 
Program-of-thought (PoT) prompting~\cite{pot, pal} addresses these limitations by \emph{decoupling reasoning from computation}.
%
By shifting execution to an external interpreter, PoT ensures that the reasoning stage focuses solely on code generation, reducing reliance on the model's linguistic fluency in executing computational steps. 
%
This separation makes PoT advantageous in multilingual settings, where disparity among languages can greatly affect the model's performance.


 
%
Despite its potential, multilingualism in PoT remains underexplored.
% 
Compared to the rich literature on non-English CoT, especially regarding multilingual fine-tuning~\cite{mathoctopus, mcot, mapo, qalign}, PoT is limited to a single cross-lingual prompting study~\cite{crosspal}.
%
% This disparity highlights the need for systematic research on PoT fine-tuning strategies in multilingual settings to fully realize its potential.
This disparity emphasizes the necessity for research into PoT fine-tuning to fully exploit its potential for enhanced generalization to unseen languages and improved performance in multilingual environments.





\subsection{Problem Formulation}
This study examines the feasibility of decoupling natural language reasoning from computation in non-English languages.
%
We formalize multilingual PoT within a two-stage framework as shown in Figure~\ref{fig:exp-framework}: (i) 
$Q \rightarrow R$,  where the model generates reasoning steps $R$ from questions $Q$; (ii) $R \rightarrow A$, where an external interpreter executes $R$ to obtain the final answer $A$.
%
Our research is organized into two problems: P1 and P2, as follows.
%
\begin{figure}[htbp]
\centering
\includegraphics[width=\columnwidth]{latex/figures/exp_framework_v3.pdf}
% \includegraphics[width=\columnwidth]{latex/figures/overview-study.pdf}
    \caption{
    Proposed experimental framework under the PoT workflow $Q \rightarrow R \rightarrow A$. \textbf{P1}: Aligning multilingual questions (Q) with reasoning steps (R) through fine-tuning and inline comments. \textbf{P2}: Assessing the correlation between reasoning steps (R) and final answers (A) through code quality and test-time inference.
    }
    \label{fig:exp-framework}
    \vspace{-2mm}
\end{figure} 

\noindent
\textbf{(P1) Fine-tuning for Q-R Alignment.}
%
This study attempts to answer the question:
%
\emph{
How can we \textbf{align questions $Q$} posed in different languages with effective \textbf{reasoning steps $R$} in PoT, and how do \textbf{fine-tuning decisions} influence cross-lingual and multilingual reasoning performance?
}

We evaluate different fine-tuning decisions under two fine-tuning scenarios.
%
\begin{compactitem}
\item \emph{Cross-lingual:} The model is fine-tuned only in English and evaluated cross-lingual zero-shot.
%in other languages.
%
\item \emph{Multilingual:} Training data includes samples in target languages, allowing direct $Q$-$R$ alignment in target languages.
% 
\end{compactitem}
These examinations analyze the impact of fine-tuning choices on reasoning alignment, providing insights into how language availability influences cross-lingual and multilingual PoT performance.

We explore multiple decisions regarding the use of inline comments in PoT reasoning, evaluating their impact in both cross-lingual and multilingual settings.
%
For the cross-lingual setting, since the model is fine-tuned only in English, we compare keeping English comments versus removing them entirely. 
%
Our results show that removing comments leads to better generalization in unseen languages.
%
For the multilingual setting, with access to target-language training data, we evaluate keeping English comments versus translating them into the target language. 
%
% Our findings indicate that translating comments improves reasoning alignment and overall performance.
Our findings indicate that translating comments improves reasoning alignment, reflecting overall performance.

\noindent
\textbf{(P2) Code Quality vs Performance.}
%
This study attempts to answer the question: 
%
\emph{
To what extent does the \textbf{code quality} of reasoning steps $R$ \textbf{affect the correctness} of final answers $A$, and how can we use this knowledge to \emph{improve PoT performance}?
}

%
We investigate the relationship between PoT performance and code quality measured through \texttt{ICE-Score}~\cite{ice-score}, which quantifies the correctness of intermediate steps within the code.
%
Our analysis reveals a strong correlation between them.
%
Building on this insight, we employ the ICE score as a heuristic for test-time scaling within the Soft Self-Consistency~\cite{soft-sc} method, implementing it as a form of soft voting.
%
Experimental results show that this simple adjustment outperforms the standard Self-Consistency~\cite{wang2023selfconsistency} baseline, where models generate multiple candidates and apply hard voting.
%
In particular, this approach improves the overall accuracy across languages, in cross-lingual settings, increasing the performance from 31.6\% to 56.6\%.



\subsection{Contributions}

The contributions of our work are as follows:
%
\begin{compactitem}
    
\item \textbf{Experimental Framework for Multilingual PoT --- }
%
We exploit the reasoning-execution disentanglement in PoT to break down the problem into two key challenges: $Q$-$R$ alignment (how multilingual questions map to reasoning steps) and $R$-$A$ association (how reasoning quality translates into correct answers).

\item \textbf{Systematic Evaluation of Fine-Tuning for $Q$-$R$ Alignment ---} We investigate how fine-tuning impacts multilingual PoT performance under cross-lingual and multilingual settings, analyzing the role of inline comments.

\item \textbf{Correlation Between Code Quality and Answer Accuracy ---} 
%
We assess how the quality of generated reasoning steps $R$ influences the correctness of the final answers $A$ and leverage this insight to improve test-time inference. 
\end{compactitem}

