\section{MGSM8KInstruct}
\label{ap:mathoctopus}

We adopt MGSM8KInstruct~\cite{mathoctopus} as the reference dataset for CoT in multilingual settings.
% 
This dataset comprises question-reasoning pairs $(\vb*{R}_i$, $\vb*{Q}_i)$ with $\vb*{Q}_i$ expressed in English, along with translations in nine additional languages, enabling the alignment of reasoning capabilities across different languages.
% 
\citet{mathoctopus} introduced two training strategies:
% \textcolor{blue}{drop two dataset notations from CoT since it may get confused. (Probably move to the appendix.)}
\begin{inparaenum}[(i)]
    \item \emph{CoT Cross}: Incorporates English questions with answers in the target language, promoting multilingual adaptability. 
    %
    Formally, the dataset is represented as: \begin{equation*}
        \mathcal{D}^\texttt{MGSM8KInstruct}_\texttt{cross} = \{(\vb*{Q}_i^\text{en}, \vb*{C}_i^l)|l\in L_\text{all}\}_{i=1}^N
    \label{eq:d-mathoctopus-cross}
    \end{equation*}
    where $L_\text{all}$ includes both English and target languages.
    \item \emph{CoT Parallel}: Uses question-answer pairs in the same language to 
    % enhance accuracy within a language
    enhancing the PoT capability within each target language, denoted as: 
    \begin{equation*}
    \mathcal{D}^\texttt{MGSM8KInstruct}_\texttt{parallel} = \{(\vb*{Q}_i^l, \vb*{C}_i^l)|l\in L_\text{all}\}_{i=1}^N.
    \label{eq:d-mathoctopus-parallel}
    \end{equation*}
\end{inparaenum}










\input{latex/appendices/pot_synthesis}


% \section{Variation of Fine-Tuning Process}
% \textcolor{red}{Since there is swahil issue in llama2-13b then: run 3 seeds thingy for llama2-13b in cross and multilingual setup: CoT, PoT}



% \section{Why there are only native CoT and inline comments PoT reported}

% demonstrated that, in cross-lingual setup, it is challenging to enforce the model to generate native multi-step by a matrix of proportion,



% \begin{figure*}
% \caption{pot syn.}
% \centering
% \includegraphics[width=1.0\textwidth]{figures/potsyn_v01.pdf}
% \label{fig:comparesynpot}
% \end{figure*}





















\section{Training Setting}
Our code is primarily based on the MathOctopus codebase, with some minor modifications. The code will be made available.

%
\noindent\textbf{Prompt Template.} During training and testing, we consistently use the same prompt template from MathOctopus \cite{mathoctopus}.

%
\noindent\textbf{Setting.} We fully fintune all our models on a single 4xA100 node for three epochs with a maximum sequence length 1024.
%
For the Llama2 family and CodeLlama, we used a learning rate of 2e-5 and an effective batch size of 512.
%
However, we found that this setting caused the Llama3 8B model not to produce desirable results, which we discuss further in the next section.
%
Thus, we changed the effective batch size to 128 and the learning rate to 5e-6, following \cite{tulu3} for Llama 3 8B.
% 
To generate multiple candidate predictions, we set \( top_k=50 \) and a temperature of 0.7, selecting the top 40 sequences for the voting process.



\section{Computing Resources}

We trained LLaMA family models on 4× NVIDIA A100 (80GB) GPUs, completing the fine-tuning process within approximately one hour for cross-lingual settings and around eight hours for multilingual settings.

During inference, generating predictions in a greedy fashion requires only three minutes. However, when producing multiple answer candidates with K=40, the process takes approximately seven hours to complete.

For Oracle LLM inference, we utilize a separate dedicated setup with 4× NVIDIA A100 (80GB) GPUs to host the LLM service, which is responsible for constructing PoT answers and evaluating code quality. The quality assessment process requires approximately 45 minutes for a single prediction and extends to 32 hours when assessing 40 candidates across all languages for a given model configuration. Additionally, we employ 62 concurrent processes to maximize inference throughput.

In summary, our experiments required a total of 544 A100 GPU hours for fine-tuning, 52 hours for inference, and 146 hours for quality assessment.




\section{Comparison with Non-Fine-Tuned PoT}

We compare our test-time scaling experiments with state-of-the-art (SOTA) non-fine-tuned PoT prompting methods and observe that our product models from PoT parallel with \texttt{SC} outperform SCross-PAL from \citet{crosspal} by 0.9 percentage points.
Furthermore, our proposed \texttt{Soft-SC} with \texttt{ICE-Score} achieves a significant accuracy improvement, increasing from 57.2\% to 71.2\%.


\begin{table}
\tiny
  \centering
  \resizebox{0.7\columnwidth}{!}{
  \begin{tabular}{l|c}
    \hline
    \textbf{Method} & ALL \\
    \hline\hline
    \multicolumn{2}{c}{\textit{Cross-lingual}} \\
    \hline    
    \multicolumn{1}{l|}{\underline{Llama2-7B}} & \multicolumn{1}{c}{} \\
    Without Comments &39.2  \\
    + \texttt{Soft-SC} (\texttt{ICE-Score}) & \textbf{56.6}  \\ 
    \hline   
    \multicolumn{1}{l|}{\underline{CodeLlama-7B}}& \multicolumn{1}{c}{} \\
    Without Comments & 38.6  \\
    + \texttt{SC} & 46.7  \\
    + \texttt{Soft-SC} (\texttt{ICE-Score}) & \textbf{61.1}  \\
    \hline\hline
    \multicolumn{2}{c}{\textit{Multilingual}} \\
    \hline
    \multicolumn{1}{l|}{\underline{Llama2-7B}}& \multicolumn{1}{c}{} \\
    PoT Parallel & 44.6   \\
    + \texttt{SC} & 57.2  \\
    + \texttt{Soft-SC} (\texttt{ICE-Score}) & \textbf{71.2} \\
    \hline   
    \multicolumn{1}{l|}{\underline{CodeLlama-7B}}& \multicolumn{1}{c}{} \\
    PoT Parallel & 49.0  \\
    + \texttt{SC} & 62.8  \\
    + \texttt{Soft-SC} (\texttt{ICE-Score}) & \textbf{75.6}  \\ 
    \hline\hline
    \multicolumn{2}{c}{\textit{Non-Fine-Tuned PoT}} \\
    \hline
    \multicolumn{1}{l|}{\underline{Llama2-7B}}& \multicolumn{1}{c}{} \\
    CLP \cite{clp} & 48.3 \\
    SCLP \cite{clp} & 54.1 \\
    \hdashline
    Cross-PAL \cite{crosspal} & 49.9 \\
    SCross-PAL \cite{crosspal} & \textbf{56.3} \\
    \hline
  \end{tabular}
  }
  \caption{
  The comparison of our adopted test-time scaling approaches with SOTA non-fine-tuned PoT approaches. The results of non-fune-tuned PoT are taken from \citet{crosspal}.
  }
  \label{tab:test-time-scaling-compare-prompt-pot}
\end{table}


% \vspace{2mm}

\section{Sensitivity of Llama3-8B}
During our testing, we observed that Llama3-8B exhibited significant sensitivity to our hyperparameters and chat template configurations.
%
Notably, the model frequently failed to generate the \texttt{def solver():} function header at the beginning of its reasoning chain, which is critical for extracting and compiling the generated code correctly.
%
To mitigate this issue, we inserted a prefix in our prompt, as illustrated in Figure \ref{fig:llama3-prompt}.
%
Additionally, with our initial hyperparameters, Llama3-8B frequently generated code snippets that failed to compile. Specifically, 9.12\% of its outputs were non-compilable, a significantly higher rate compared to Llama2-7B (3.08\%), CodeLlama-7B (2.04\%), and Llama2-13B (1.84\%). 
%
However, after refining our hyperparameters based on the approach outlined by \cite{tulu3} and adjusting the chat template, we observed a substantial reduction in compilation errors, with the failure rate dropping to 1.68\%.
%

\begin{figure}[H]
    \small
    \centering
    \begin{mdframed}
    \textbf{User} \newline
    Below are instructions for a task. \newline
    Write a response that appropriately completes the request in [language]. Please answer in Python with inline comments in [language].\newline
    \#\#\# Instruction: \newline[Question]\newline
    \#\#\# Response:\newline
    \textcolor{red}{def solver():} 
    \end{mdframed}
    \caption{
    Updated prompt with an added prefix (\texttt{def solver():}) for Llama3-8B.
    }
    \label{fig:llama3-prompt}
\end{figure}



\section{Alternative Metric For Code Quality Assessment}
Alternatively, to \texttt{ICE-Score}, we evaluated code quality using \texttt{CodeBERT-Score} \cite{codebertscore}. 
%
However, we noticed that GSM8K \cite{cobbe2021gsm8k} primarily consists of short code snippets where errors often involved small numerical mistakes rather than large structural or semantic differences.
%
Many of the errors stemmed from minor computation mistakes, like using the wrong arithmetic expression or associating wrong counts with the subject.
%
Since \texttt{CodeBERT-Score} is designed to assess broader semantic similarity, it struggled to distinguish the minute differences between correct and incorrect code.
%
As shown in Table \ref{tab:ablation-multi-codebert}, the scores across different systems varied only slightly $(\pm ~1.0\%)$, failing to reflect the accuracy differences observed in Tables \ref{tab:ablation-cross}, \ref{tab:ablation-multi}.
%
This suggests that \texttt{CodeBERT-Score} may not be well-suited for evaluating correctness in GSM8K-style problems.

\input{latex/appendices/full_tables}