\section{Preliminaries}

\subsection{Denoising Diffusion Bridge Models}

Starting with an initial $d$-dimensional data distribution $\mathbf{x}_0 \sim q_{\text{data}}(\mathbf{x})$, diffusion models \cite{song2021scorebasedgenerativemodelingstochastic, ho2020denoisingdiffusionprobabilisticmodels, sohldickstein2015deepunsupervisedlearningusing, song2020generativemodelingestimatinggradients} construct a diffusion process, which can be achieved by defining a forward stochastic process evolving from $\mathbf{x}_0$ through a stochastic differential equation (SDE):
\begin{equation}\label{diffusion_sde}
\mathrm{d} \mathbf{x}_t = \mathbf{f}(\mathbf{x}_t, t) \mathrm{d} t+g_t \mathrm{~d} \mathbf{w}_t,
\end{equation}
where $t$ ranges over the interval $[0, T]$, $\mathbf{f}: \mathbb{R}^d \times [0, T] \rightarrow \mathbb{R}^d$ is the vector-valued drift function, $g:[0, T] \rightarrow \mathbb{R}$ signifies the scalar-valued diffusion coefficient and $\mathbf{w}_t \in \mathbb{R}^d$ is the Wiener process, also known as Brownian motion. To promise the transition probability $p(\mathbf{x}_t \mid \mathbf{x}_s)$ remains Gaussian, almost all the diffusion SDEs take the following linear form \cite{zheng2024diffusionbridgeimplicitmodels} in \eqref{diffusion_sde}:
\begin{equation}\label{linear_form}
\mathbf{f}(\mathbf{x}_t, t) = f(t) \mathbf{x}_t,
\end{equation}
where $f(t)$ is some scalar-valued function. To realize transition between arbitrary distributions, DDBMs introduces Doob’s $h$-transform \cite{särkkä2019applied}, a mathematical technique applied to stochastic processes, which rectifies the drift term of the forward diffusion process to pass through a preset terminal point $\mathbf{x}_T \in \mathbb{R}^d$. Precisely, the forward process of diffusion bridges after Doob's $h$-transform becomes: 
\begin{equation}\label{doob}
\mathrm{d} \mathbf{x}_t = \left[ \mathbf{f}(\mathbf{x}_t, t) + g^2_t \mathbf{h}(\mathbf{x}_t, t, \mathbf{x}_T, T) \right] \mathrm{d} t+g_t \mathrm{~d} \mathbf{w}_t,
\end{equation}
where $\mathbf{h}(\mathbf{x}_t, t, \mathbf{x}_T, T) = \nabla_{\mathbf{x}_t} \log p(\mathbf{x}_T \mid \mathbf{x}_t)$ is the $h$ function. The diffusion bridge can connect the initial $\mathbf{x}_0$ to any given terminal $\mathbf{x}_T$ and thus is promising for various image restoration tasks. Meanwhile, its backward reverse SDE \cite{ANDERSON1982313} is given by
\begin{equation}\label{reverse-bridge-sde}
\begin{aligned}
\mathrm{d} \mathbf{x}_t = & \Big[ \mathbf{f}(\mathbf{x}_t, t) + g^2_t \nabla_{\mathbf{x}_t} \log p(\mathbf{x}_T \mid \mathbf{x}_t) \\
& - g_t^2\nabla_{\mathbf{x}_t} \log p(\mathbf{x}_t \mid \mathbf{x}_T) \Big] \mathrm{d} t+g_t \mathrm{~d} \tilde{\mathbf{w}}_t.
\end{aligned}
\end{equation}
where $\tilde{\mathbf{w}}_t$ is the reverse-time Wiener process and the unknown term $\nabla_{\mathbf{x}_t} \log p(\mathbf{x}_t \mid \mathbf{x}_T)$ can be estimated by a score prediction neural network $s_{\theta}$ \cite{song2021scorebasedgenerativemodelingstochastic}.
% \vspace{-2mm}
\subsection{Generalized Ornstein-Uhlenbeck Bridge}
Generalized Ornstein-Uhlenbeck (GOU) process describes a mean-reverting stochastic process commonly used in finance, physics, and other fields in the following SDE form \cite{ahmad1988introduction, Pavliotis2014, WANG2018921}:
\begin{equation}\label{gou_process}
\mathrm{d} \mathbf{x}_t=\theta_t\left(\boldsymbol{\mu}-\mathbf{x}_t\right) \mathrm{d} t+g_t \mathrm{~d} \mathbf{w}_t,
\end{equation}
where $\boldsymbol{\mu}$ is a given state vector, $\theta_t$ denotes a scalar drift coefficient and $g_t$ represents the diffusion coefficient with $\theta_t$, $g_t$ satisfying the specified relationship $g_{t}^{2} = 2 \lambda^2 \theta_t$ where $\lambda^2$ is a given constant scalar. Based on this, Generalized Ornstein-Uhlenbeck Bridge (GOUB) is a diffusion bridge model \cite{yue2024imagerestorationgeneralizedornsteinuhlenbeck}, which can address image restoration tasks without the need for specific prior knowledge. With the introduction of $\boldsymbol{\mu}$, $\mathbf{x}_t$ tends to $\boldsymbol{\mu}$ as time $t$ progresses. Through Doob's $h$-transform, denote $\bar{\theta}_{s:t} = \int_{s}^{t} \theta_z dz$, $\bar{\theta}_{t} = \int_{0}^{t} \theta_z dz$ for simplification when $s=0$ and $\bar{\sigma}^2_{s:t} = \lambda^2(1-e^{-2\bar{\theta}_{s:t}})$, the forward process of GOUB is formed as:
% \begin{equation}\label{goub_forward_sde}
% \begin{gathered}
% \mathrm{d} \mathbf{x}_t=\left(\theta_t+g_t^2 \frac{e^{-2 \bar{\theta}_{t: T}}}{\bar{\sigma}_{t: T}^2}\right)\left(\mathbf{x}_T-\mathbf{x}_t\right) \mathrm{d} t+g_t \mathrm{~d} \mathbf{w}_t, \\
% \bar{\theta}_{s:t} = \int_{s}^{t} \theta_z dz, \quad \quad \bar{\sigma}^2_{s:t} = \frac{g_t^2}{2\theta_t}(1-e^{-2\bar{\theta}_{s:t}}).
% \end{gathered}
% \end{equation}
\begin{equation}\label{goub_forward_sde}
\mathrm{d} \mathbf{x}_t=\left(\theta_t+g_t^2 \frac{e^{-2 \bar{\theta}_{t: T}}}{\bar{\sigma}_{t: T}^2}\right)\left(\mathbf{x}_T-\mathbf{x}_t\right) \mathrm{d} t+g_t \mathrm{~d} \mathbf{w}_t.
\end{equation}
And the forward transition $p(\mathbf{x}_t\mid\mathbf{x}_0,\mathbf{x}_T)$ is given by
\begin{equation}\label{gou_transition}
\begin{gathered}
p(\mathbf{x}_t\mid\mathbf{x}_0,\mathbf{x}_T)=\mathcal{N}(\bar{\boldsymbol{\mu}}_{t}^{\prime},\bar{\sigma}_t^{\prime2}\mathbf{I}), \\
\bar{\boldsymbol{\mu}}_{t}^{\prime}=e^{-\bar{\theta}_t}\frac{\bar{\sigma}_{t:T}^2}{\bar{\sigma}_T^2}\mathbf{x}_0 + ( 1 - e^{-\bar{\theta}_t}\frac{\bar{\sigma}_{t:T}^2}{\bar{\sigma}_T^2} )\mathbf{x}_T, \ \bar{\sigma}_t^{\prime2}=\frac{\bar{\sigma}_t^2\bar{\sigma}_{t:T}^2}{\bar{\sigma}_T^2}.
\end{gathered}
\end{equation}
Also, GOUB presents a new reverse ODE called Mean-ODE, which directly neglects the Brownian term of \eqref{reverse-bridge-sde}: 
\begin{equation}\label{reverse-mean-ode}
\begin{aligned}
\mathrm{d} \mathbf{x}_t = \Big[ \mathbf{f}&(\mathbf{x}_t, t) + g^2_t \nabla_{\mathbf{x}_t} \log p(\mathbf{x}_T \mid \mathbf{x}_t) \\
& - g_t^2\nabla_{\mathbf{x}_t} \log p(\mathbf{x}_t \mid \mathbf{x}_T) \Big] \mathrm{d} t.
\end{aligned}
\end{equation}

\vspace{-5mm}
\subsection{Stochastic Optimal Control}
% \textbf{Formulation of the Stochastic Optimal Control Problem:}
Stochastic Optimal Control (SOC) is a mathematical discipline that focuses on determining optimal control strategies for dynamic systems under uncertainty. By integrating stochastic processes with optimization theory, SOC seeks to identify the best control strategies in scenarios involving randomness, as commonly encountered in fields like finance \cite{Geering2010} and style transfer \cite{RB}. Considering the dynamics described in \eqref{diffusion_sde}, let us examine the following Linear Quadratic SOC problem \cite{4310229, OConnell2003ConditionedRW, Kappen2008StochasticOC, chen2024generativemodelingphasestochastic}: 
\begin{equation}\label{control_problem_sde}
\begin{gathered}
\min_{\mathbf{u}_{t, \gamma} \in \mathcal{U}} \int_0^T \frac{1}{2} \left\|\mathbf{u}_{t, \gamma}\right\|_2^2 d t+\frac{\gamma}{2}\left\|\mathbf{x}_T^{u}-x_T\right\|_2^2 \\
\text{s.t.} \ \mathrm{d} \mathbf{x}_t = \left( \mathbf{f}(\mathbf{x}_t, t) + g_t \mathbf{u}_{t, \gamma} \right) \mathrm{d} t + g_t \mathrm{~d} \mathbf{w}_t, \ \mathbf{x}_0^u=x_0,
\end{gathered}
\end{equation}
where $\mathbf{x}_t^u$ is the diffusion process under control, $x_0$ and $x_T$ represent for the initial state and the preset terminal respectively, $\left\|\mathbf{u}_{t, \gamma}\right\|_2^2$ is the instantaneous cost, $\frac{\gamma}{2}\left\|\mathbf{x}_T^{u}-x_T\right\|_2^2$ is the terminal cost with its penalty coefficient $\gamma$. The SOC problem aims to design the controller $\mathbf{u}_{t, \gamma}$ to drive the dynamic system from $x_0$ to $x_T$ with minimum cost. 