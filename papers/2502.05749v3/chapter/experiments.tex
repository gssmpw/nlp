\section{Experiments}
In this section, we evaluate our models in image restoration tasks including Image 4$\times$Super-resolution, Image Deraining, and Image Inpainting. We take four evaluation metrics: Peak Signal-to-Noise Ratio (PSNR, higher is better) \cite{fardo2016formalevaluationpsnrquality}, Structural Similarity Index (SSIM, higher is better) \cite{1284395}, Learned Perceptual Image Patch Similarity (LPIPS, lower is better) \cite{zhang2018unreasonableeffectivenessdeepfeatures} and Fr√©chet Inception Distance (FID, lower is better) \cite{heusel2018ganstrainedtimescaleupdate}. For simple expressions in the following sections, UniDB (SDE) and UniDB (ODE) are applied to represent the UniDB-GOU with reverse SDE and reverse Mean-ODE, respectively. Please refer to Appendix \ref{appendix_experimental_details} and \ref{appendix_additional_results} for all related implementation details and more experiment results, respectively. 

\begin{figure}[H] 
    \raggedright
    \includegraphics[width=0.48\textwidth]{picture/r.png}
    \caption{The distances between target and controlled terminal distributions for different datasets (CelebA-HQ, Rain100H, and DIV2K) with different penalty coefficients $\gamma$. The red shaded area and blue dotted line highlight our choice of $\gamma$.}
    \label{experiment_gamma_difference}
    \vspace{-5mm}
\end{figure}


\begin{figure*}[t] 
    \raggedright
    \centering
    \includegraphics[width=0.98\textwidth]{picture/DIV2K_4.pdf}
    \caption{Qualitative comparison of visual results between GOUB (SDE) and UniDB (SDE) on DIV2K with zoomed-in image local regions (UniDB based on GOU process).}
    \label{experiment_DIV2K}
\end{figure*}


\begin{table*}[t]
\setlength{\tabcolsep}{2pt}
  \centering
  \vspace{-3mm}
  \caption{Qualitative comparison with the relevant baselines on DIV2K, Rain100H, and CelebA-HQ 256$\times$256 datasets.}
  \vskip 0.1in
  \label{results}
  \renewcommand{\arraystretch}{1.1}
  \resizebox{\textwidth}{!}{
  \begin{tabular}{|c|cccc|c|cccc|c|cccc|}
    \hline
    \multirow{2}*{\textbf{METHOD}} & \multicolumn{4}{c|}{\textbf{Image Super-Resolution}} & \multirow{2}*{\textbf{METHOD}} & \multicolumn{4}{c|}{\textbf{Image Deraining}} & \multirow{2}*{\textbf{METHOD}} & \multicolumn{4}{c|}{\textbf{Image Inpainting}} \\
    \cline{2-5} \cline{7-10} \cline{12-15}
      & \textbf{PSNR}$\uparrow$ & \textbf{SSIM}$\uparrow$ & \textbf{LPIPS}$\downarrow$ & \textbf{FID}$\downarrow$ & &\textbf{PSNR}$\uparrow$ & \textbf{SSIM}$\uparrow$ & \textbf{LPIPS}$\downarrow$ & \textbf{FID}$\downarrow$ & & \textbf{PSNR}$\uparrow$ & \textbf{SSIM}$\uparrow$ & \textbf{LPIPS}$\downarrow$ & \textbf{FID}$\downarrow$ \\
    \hline
    Bicubic & 26.70 & 0.774 & 0.425 & 36.18 &  MAXIM & 30.81 & 0.902 & 0.133 & 58.72 & PromptIR & 30.22 & 0.918 & 0.068 & 32.69\\
     DDRM & 24.35 & 0.592 & 0.364 & 78.71 &  MHNet &31.08 & 0.899 & 0.126 & 57.93 & DDRM & 27.16 & 0.899 & 0.089 & 37.02 \\

    IR-SDE &  25.90 & 0.657 & 0.231 & 45.36 &  IR-SDE & 31.65 & 0.904 & 0.047 & 18.64 & IR-SDE & 28.37 & 0.916 & 0.046 & 25.13 \\

    GOUB (SDE) & 26.89 & 0.7478 & 0.220 & 20.85 & GOUB (SDE) & 31.96 & 0.9028 & 0.046 & 18.14 & GOUB (SDE) & 28.98 & 0.9067 & 0.037 & 4.30 \\
       
    GOUB (ODE) & 28.50 & 0.8070 & 0.328 & 22.14 & GOUB (ODE) & 34.56 & 0.9414 & 0.077 & 32.83 & GOUB (ODE)  & 31.39 & 0.9392 & 0.052 & 12.24 \\
    \hline  
    UniDB (SDE) & 25.46 & 0.6856 & \textbf{0.179} & \textbf{16.21} & UniDB (SDE) & 32.05 & 0.9036 & \textbf{0.045} & \textbf{17.65} & UniDB (SDE)  & 29.20 & 0.9077 & \textbf{0.036} & \textbf{4.08} \\
    
    UniDB (ODE) & \textbf{28.64} & \textbf{0.8072} & 0.323 & 22.32 & UniDB (ODE)  &\textbf{34.68} & \textbf{0.9426} & 0.074 & 31.16 & UniDB (ODE) & \textbf{31.67} & \textbf{0.9395} & 0.052 & 11.98 \\   
    \hline         
  \end{tabular}
  }
  \vskip -0.1in
\end{table*}


\subsection{Experiments Setup}

According to Proposition \ref{proposition_4.5}, we first quantitatively analyze the $l_2$-norm distances between the two terminal distributions depicted in Figure \ref{experiment_gamma_difference}. We computed the average distances between high-quality and low-quality images in the three datasets (CelebA-HQ, Rain100H, and DIV2K) related to the subsequent experimental section as the distances $\| x_T - x_0 \|^2_2$ in \eqref{terminal_distance}. As can be seen, for all three datasets, these distances remain relatively small, ranging from $10^{-4}$ to $10^{-10}$ when $\gamma$ is within the range of $1\times10^5$ to $1\times10^9$. Therefore, our subsequent experiments will focus on the $\gamma$ of this range to further investigate the performance of UniDB-GOU. 




\begin{figure*}[t] %
    \centering
    \includegraphics[width=\textwidth]{picture/combine_3.pdf}
    \caption{Qualitative comparison of visual results between GOUB (SDE) and UniDB (SDE) on the Rain100H dataset on Image Deraining (Left) and CelebA-HQ dataset on Image Inpainting (Right)  with zoomed-in image local regions (UniDB based on GOU process).}
    \label{experiment_inpainting_celeba}
\end{figure*}  



\begin{table*}[t]
    \centering
    \tabcolsep=0.4cm
    \caption{Quantitative evaluation results for DIV2K, CelebA-HQ and Rain100H of UniDB-GOU with different penalty coefficients $\gamma$.}
    \label{ablation_study_gamma}
    \begin{tabular}{ccccccc}
        \toprule[0.8pt]
        \multirow{2}{*}{\textbf{TASKS}} & \multirow{2}{*}{\textbf{METRICS}} & \multicolumn{5}{c}{\textbf{Different} $\boldsymbol{\gamma}$} \\ \cmidrule(l){3-7}
        & & $5\times10^5$ & $1\times10^6$ & $1\times10^7$ & $1\times10^8$ & $\infty$ \\ \toprule[0.8pt]
        \multirow{4}{*}{\textbf{Image 4$\times$Super-Resolution}} 
        & \textbf{PSNR}$\uparrow$ & 24.94 & 24.72 & 25.46 & 25.06 & \textbf{26.89} \\   
        & \textbf{SSIM}$\uparrow$ & 0.6419 & 0.6587 & 0.6856 & 0.6393 & \textbf{0.7478} \\
        & \textbf{LPIPS}$\downarrow$ & 0.234 & 0.199 & \textbf{0.179} & 0.289 & 0.220 \\
        & \textbf{FID}$\downarrow$ & 20.33 & 18.37 & \textbf{16.21} & 23.76 & 20.85 \\ \toprule[0.8pt]
        \multirow{4}{*}{\textbf{Image Inpainting}} 
        & \textbf{PSNR}$\uparrow$ & 28.73 & 29.15 & \textbf{29.20} & 28.65 & 28.98 \\
        & \textbf{SSIM}$\uparrow$ & 0.9065 & 0.9068 & \textbf{0.9077} & 0.9062 & 0.9067 \\
        & \textbf{LPIPS}$\downarrow$ & 0.038 & 0.036 & \textbf{0.036} & 0.039 & 0.037 \\   
        & \textbf{FID}$\downarrow$ & 4.49 & 4.12 & \textbf{4.08} & 4.64 & 4.30 \\ \toprule[0.8pt]
        \multirow{4}{*}{\textbf{Image Deraining}} 
        & \textbf{PSNR}$\uparrow$ & 29.44 & 31.96 & 32.00 & \textbf{32.05} & 31.96 \\
        & \textbf{SSIM}$\uparrow$ & 0.8715 & 0.9018 & 0.9029 & \textbf{0.9036} & 0.9028 \\   
        & \textbf{LPIPS}$\downarrow$ & 0.058 & 0.045 & 0.046 & \textbf{0.045} & 0.046 \\
        & \textbf{FID}$\downarrow$ & 24.96 & 18.37 & 17.87 & \textbf{17.65} & 18.14 \\ \bottomrule[1pt]
    \end{tabular}
\end{table*}



\subsection{Experimental Details}

\textbf{Image 4$\times$Super-Resolution Tasks.}
In super-resolution, we evaluated our models based on DIV2K dataset \cite{8014884}, which contains 2K-resolution high-quality images. During the experiment, all low-resolution images were 4$\times$ bicubic upscaling to the same image size as the paired high-resolution images. For comparison, we choose Bicubic interpolantion \cite{DDRM}, DDRM \cite{DDRM}, IR-SDE \cite{IRSDE}, GOUB (SDE) \cite{yue2024imagerestorationgeneralizedornsteinuhlenbeck} and GOUB (Mean-ODE) \cite{yue2024imagerestorationgeneralizedornsteinuhlenbeck} following abbreviated as GOUB (ODE) as the baselines. The qualitative and quantitative results are illustrated in Table \ref{results} and Figure \ref{experiment_DIV2K}. Visually, our proposed model demonstrates a significant improvement over the baseline across various metrics. It also excels by delivering superior performance in both visual quality and detail compared to other results.



\textbf{Image Deraining Tasks.}
For image deraining tasks, we conducted the experiments based on Rain100H datasets \cite{8099666}. Particularly, to be consistent with other deraining models \cite{ren2019progressiveimagederainingnetworks, zamir2021multistageprogressiveimagerestoration, IRSDE, yue2024imagerestorationgeneralizedornsteinuhlenbeck}, PSNR and SSIM scores on the Y channel (YCbCr space) are selected instead of the origin PSNR and SSIM. MAXIM \cite{MAXIM}, MHNet \cite{MHNet}, IR-SDE \cite{IRSDE}, GOUB (SDE) \cite{yue2024imagerestorationgeneralizedornsteinuhlenbeck} and GOUB (ODE) \cite{yue2024imagerestorationgeneralizedornsteinuhlenbeck} are chosen as the baselines. The relevant experimental results are shown in the Table \ref{results} and Figure \ref{experiment_inpainting_celeba}. Similarly, our model achieved state-of-the-art results in the deraining task. Visually, it can also be observed that our model excels in capturing details such as the eyebrows, eye bags, and lips.


\textbf{Image Inpainting Tasks.}
In image inpainting tasks, we evaluated our methods on CelebA-HQ 256$\times$256 datasets \cite{karras2018progressivegrowinggansimproved}. For comparison, we choose DDRM \cite{DDRM}, PromptIR \cite{PromptIR}, IR-SDE \cite{IRSDE}, GOUB (SDE) \cite{yue2024imagerestorationgeneralizedornsteinuhlenbeck} and GOUB (ODE) \cite{yue2024imagerestorationgeneralizedornsteinuhlenbeck} as the baselines. As for mask type, we take 100 thin masks consistent with the baselines. The relevant experimental results are shown in Table \ref{results} and Figure \ref{experiment_inpainting_celeba}. It is observed that our model achieved state-of-the-art results in all indicators and also delivered highly competitive outcomes on other metrics. From a visual perspective, our model excels in capturing details such as faces, eyes, chins, and noses.



\subsection{Ablation Study}


\textbf{Penalty Coefficient $\gamma$.} To evaluate the specific impact of different penalty coefficients $\gamma$ on model performance, we conducted the experiments above with several different $\gamma$. The final results are shown in Table \ref{ablation_study_gamma}. The results across all tasks show that the choice of $\gamma$ significantly influences the model's performance on all tasks, different optimal $\gamma$ for different tasks, and our UniDB achieves the best performance in almost all metrics. Particularly in super-resolution tasks, we focus more on the significantly better perceptual scores (LPIPS and FID) \cite{IRSDE}, demonstrating that UniDB ensures to capture and preserve more intricate image details and features as shown in Figure \ref{experiment_DIV2K}. These findings underscore the importance of carefully tuning $\gamma$ to achieve the best performance for specific tasks.