\section{Related Work and Motivation}
\subsection{Mobile AIGC and Its Applications}
As a new concept, Du \textit{et al.} ____ first presented mobile AIGC and analyzed the MASP selection issues.
Then, Zhang \textit{et al.} ____ comprehensively surveyed this topic, including its advantages, architecture, lifecycle, and some open challenges.
From 2023, mobile AIGC has entered a period of rapid development and received widespread attention from academia ____ and industry (e.g., Qualcomm and Meta ____).
%Next, we briefly review the related progress from different perspectives.
%\begin{itemize}
From the model perspective, researchers keep compressing large AIGC models, reducing their costs. For instance, Qualcomm published the world's first on-device Stable Diffusion by knowledge distillation ____. Likewise, Chen \textit{et al.} ____ performed a series of GPU-aware optimizations for diffusion-based AIGC models, reducing the inference latency to three seconds. Similar proposals include LightGrad ____, DiffNAS ____, and SnapFusion ____. 
To improve the efficiency of mobile AIGC networks, Xu \textit{et al.} ____ optimized the model caching strategy of MASPs. Du \textit{et al.} ____ presented distributed mobile AIGC inference. 
By offloading certain inference steps to users, the computation overhead of MASPs can be effectively reduced. Huang \textit{et al.} ____ leveraged federated learning to enable mobile AIGC to generate customized content. 
Wen \textit{et al.} ____ designed an incentive mechanism based on content freshness, thereby encouraging MASPs to reduce latency. 
Cheng \textit{et al.} ____ applied semantic communications to reduce the bandwidth costs of MASPs to transmit AIGC outputs.
Finally, mobile AIGC facilitates various applications. 
For example, Zhang \textit{et al.} ____ presented a terminal-edge-cloud collaborative AIGC architecture to facilitate autonomous driving. Likewise, Zhang \textit{et al.} ____ designed a diffusion-based matting engine for mobile AIGC users sharing and editing content.
%\end{itemize}

Different from existing works, this paper optimizes mobile AIGC from the service perspective.
By interactive prompt engineering and dynamic service provisioning, users' requests for high-quality AIGC outputs can be satisfied rapidly and consume less resources.
Hence, both the user QoE and system efficiency can be improved.

\subsection{Discrete Prompt Engineering}
Prompt engineering refers to the process of strategically refining prompts, thereby effectively guiding AIGC models to produce relevant and high-quality outputs.
According to the data structure, prompts can be split into two types, namely continuous and discrete prompts ____.
The former, typically in the form of texts and images, is user-friendly and widely adopted in various AIGC applications, such as ChatGPT and Stable Diffusion.
Although the efficacy of prompt engineering in promoting generation quality has been well-proven, optimizing discrete prompts is challenging.
This is because most of the current continuous optimization approaches do not fit discrete prompt tokens.
%To this end, researchers traditionally craft prompts manually, such as prompt mining ____ and paraphrasing ____.
%Such methods intrinsically suffer from huge time and labor consumption.
%To automate the prompt engineering process
To this end, an intuitive way is to transfer discrete prompts to continuous forms, e.g., parameterized embeddings.
Afterward, gradient-based optimization approaches can be applied ____.
Although improving efficiency, these methods sacrifice the interpretability of discrete prompts.
The optimized prompts cannot be explained and utilized to help users gain experience in prompting AIGC models.
Another series of proposals ____ abstracted prompt optimization to an evaluation or Markov process.
For instance, Guo \textit{et al.} ____ applied the generic algorithm, which iteratively refines each prompt by \textit{mutating} or \textit{crossing} its elements, with the goal of maximizing the fitness score.
Despite the interpretability, only limited action space and vocabulary are supported, preventing us from fully exploiting the potential of prompt engineering.

With the advancement of LLMs, refining raw prompts from infinite vocabulary becomes possible.
Hence, in this paper, we leverage an LLM to generate task-specific materials for refining raw prompts.
Moreover, to optimize the prompt engineering policy, we adopt IRL ____ to train a proxy reward.
In this way, the efficacy of selected prompt engineering strategies on any given task becomes predictable.
\renewcommand{\arraystretch}{1.2}
\begin{table}
\caption{The summary of main notations.}
\begin{tabular}{l|p{2.5cm}|l|p{2.5cm}}
\Xhline{2.2pt}
\rowcolor[rgb]{0.92,0.92,0.92}
\textbf{Notation}&\multicolumn{1}{c|}{\textbf{Description}}&\textbf{Notation}&\multicolumn{1}{c}{\textbf{Description}}\\
\hline
$Q$ & \# of users & $kc$ &Knowledge chunk\\
\hline
$M$& \# of MASPs & $\mathcal{D}$ & Demonstration dataset\\
\hline
$\pi^{(p)}_\omega$ & Prompt engineering policy& $\pi_E$ & Expert policy\\
\hline
$\pi^{(s)}_\theta$ & Service provisioning policy& $\Omega$ &AIGC model\\
\hline
$p$ & User prompt& $\tau(\cdot)$ & Embedding model\\
\hline
$\mathbf{c}_p$ & Prompt corpus& $\mathcal{D}_{\omega_1}$& Discriminator of IRL\\
\hline
$N_i$& \# of inference trials & $\mathcal{G}_\omega$& Generator of IRL\\
\hline
$P_i$& Transmission power & $\mathbf{s}^{(p)}$ & State of IRL\\
\hline
$\mathbf{p}^{*}$& Optimized prompt & $\mathbf{s}^{(s)}$ & State of D$^3$PG\\
\hline
$\otimes$& Combine operation & $T$ & \# of diffusion steps\\
\Xhline{2.2pt}
\end{tabular}
\vspace{-0.2cm}
\end{table}
\renewcommand{\arraystretch}{1}