\documentclass{article} %
\usepackage[final]{colm2025_conference}

\usepackage{microtype}
\usepackage{hyperref}
\usepackage{url}
\usepackage{booktabs}

\usepackage{lineno}

\usepackage{xspace}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{multirow}
\usepackage{multicol}
\usepackage{makecell}
\usepackage{tabularx}
\usepackage{adjustbox}
\usepackage{pifont}
\usepackage{soul}
\usepackage{enumitem}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{comment}
\usepackage{enumitem}
\usepackage{duckuments}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
\usepackage[most]{tcolorbox}
\usepackage{wrapfig}
\usepackage[T1]{fontenc}


\definecolor{darkblue}{rgb}{0, 0, 0.5}
\definecolor{denim}{RGB}{31, 81, 170}
\hypersetup{colorlinks=true, citecolor=denim, linkcolor=darkblue, urlcolor=darkblue}

\newcommand{\tracing}{\textit{thought-tracing}\xspace}
\newcommand{\Tracing}{\textit{Thought-tracing}\xspace}

\title{Hypothesis-Driven Theory-of-Mind Reasoning\\for Large Language Models}


\author{Hyunwoo Kim \quad Melanie Sclar$^{1}$ \quad Tan Zhi-Xuan$^{2}$ \quad Lance Ying$^{2,3}$ \\
\textbf{Sydney Levine}$^{4}$ \quad \textbf{Yang Liu}$^{5}$ \quad \textbf{Joshua B. Tenenbaum}$^{2}$ \quad \textbf{Yejin Choi}$^{6}$\\
\\
$^{1}$University of Washington \quad $^{2}$MIT \quad $^{3}$Harvard University \\ $^{4}$Allen Institute for AI \quad $^{5}$Amazon \quad $^{6}$Stanford University
}


\begin{document}
\ifcolmsubmission
\linenumbers
\fi

\maketitle
\begin{abstract}
Existing LLM reasoning methods have shown impressive capabilities across various tasks, such as solving math and coding problems.
However, applying these methods to scenarios without ground-truth answers or rule-based verification methods—such as tracking the mental states of an agent—remains challenging.
Inspired by the sequential Monte Carlo algorithm, we introduce \tracing, an inference-time reasoning algorithm designed to trace the mental states of specific agents by generating hypotheses and weighting them based on observations without relying on ground-truth solutions to questions in datasets.
Our algorithm is modeled after the Bayesian theory-of-mind framework, using LLMs to approximate probabilistic inference over agents' evolving mental states based on their perceptions and actions.
We evaluate \textit{thought-tracing} on diverse theory-of-mind benchmarks, demonstrating significant performance improvements compared to baseline LLMs.
Our experiments also reveal interesting behaviors of the recent reasoning models -- e.g., o1 and R1 -- on theory-of-mind, highlighting the difference of social reasoning compared to other domains.
\end{abstract}

\section{Introduction}

New reasoning algorithms and training paradigms for large language models (LLMs) have recently achieved remarkable breakthroughs in domains where well-defined ground-truth answers are readily available, enabling partial-solution evaluation and objective verification -- e.g., mathematics, coding, and puzzles \citep{yao2023tree, besta2024graph, o12024openai, r12025deepseek}.
This stands in stark contrast to social reasoning, where objective answers are not easily obtainable, making partial-solution evaluation less straightforward.
Furthermore, the information-asymmetric nature of theory-of-mind (ToM) -- which requires inferring hidden mental states -- makes social reasoning more challenging due to its increased uncertainty \citep{zhi2024pragmatic,nafar2024probabilistic}.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.98\columnwidth]{figures/figure1.pdf}
    \caption{
        An illustrative overview of our \textit{thought-tracing} algorithm.
        The original context is parsed into a trajectory of the target agent.
        At each time step, the algorithm generates multiple hypotheses about the agent's beliefs and weighs them based on the likelihood of the agent's action.
        These weights are then used to resample the hypotheses, prioritizing more promising ones to be used for the next time step.
        Further details are in Section \ref{sec:tt}.
    }
    \vspace{-1em}
    \label{fig:overview}
\end{figure}


Existing AI research on theory-of-mind has focused on either natural language benchmarks and tailored approaches to solve them \citep{sclar-etal-2023-minding, kim-etal-2023-fantom, wilf-etal-2024-think, jung-etal-2024-perceptions, jin-etal-2024-mmtom}, or on mental state inference in non-linguistic settings \citep{baker2009action,doshi2009monte,zhi2020online,alon2023dis}. 
As LLM-powered AI agents increasingly interact with humans \citep{collins2024building}, the ability  to track and infer others' mental states from open-ended textual input will become crucial for broader applications and data synthesis in the social interaction domain.


To this end, we propose \tracing, an inference-time reasoning algorithm for LLMs that can infer and track the mental states of target agents. 
Our method is modeled after the Bayesian theory-of-mind framework \citep[BToM;][]{btom2017}, viewing the input context as a sequence of states and agent actions, and treating ToM reasoning as probabilistic inference about the agent's mental states based on their perceptions and actions. %
To account for the inherent uncertainty in this task, we follow the high-level structure of sequential Monte Carlo inference \citep[SMC;][]{del2006sequential,lew2023sequential}, tracking multiple weighted hypotheses about the agent's mental states. Importantly, however, these hypotheses are represented in open-ended natural language, and are both generated and weighted by LLMs.
By using LLMs in this way, our method offers a generalizable approach to uncovering the underlying thoughts that drive agent behavior, without requiring the explicit probabilistic models assumed by BToM algorithms, and without relying on question-answer annotations or benchmark-specific assumptions.

Although \tracing is not specifically designed for solving benchmark questions, we evaluate the algorithm using state-of-the-art LLMs on four theory-of-mind benchmarks to assess whether the traced thoughts can enhance downstream question answering task performance.
Experiments show that
(1) \tracing consistently improves performance on all tested models across all benchmarks,
(2) additional inference-time compute (e.g., chain-of-thought) further aids models to process contexts interleaved with mental states,
and (3) models with \tracing outperform reasoning models (e.g., o1 and R1) despite using significantly shorter reasoning traces.

Furthermore, our results reveal interesting behavioral patterns in existing reasoning models on theory-of-mind tasks:
(1) they do not consistently outperform vanilla LLMs using chain-of-thought reasoning,
(2) they fail to generalize to similar scenarios,
(3) they produce significantly longer reasoning traces for theory-of-mind questions than for factual questions,
and (4) reasoning effort (e.g., output length) does not correlate with performance.
These findings highlight that social reasoning differs from mathematical or programming reasoning, areas where reasoning models typically excel.

These results suggest that \tracing represents a promising step towards more robust inference-time ToM reasoning.
We aim to spark new discussions on inference-time reasoning in the social domain, contrasting with the predominant focus on math and coding.









\section{Thought-tracing}
\label{sec:tt}

We introduce \tracing, an inference-time algorithm  that performs ToM reasoning by inferring and propagating weighted hypotheses about agents' thoughts. These hypotheses are represented in natural language, and are generated and weighted using LLMs.
\Tracing follows the conceptual structure of Bayesian Theory of Mind, and takes inspiration from sequential Monte Carlo to perform approximate inference over agents' mental states.

\subsection{Background}
\label{subsec:background}

\textbf{Bayesian Theory of Mind} \citep{btom2017}
frames mental state attribution as probabilistic inference over a generative model of a rational agent.
It focuses on several important roles that beliefs and goals play in a theory-of-mind:
the agent’s perceptions and their prior beliefs jointly effect the current belief, and beliefs and goals are the causes of the agent’s actions.
As such, beliefs and goals can be inferred in various ways: forward simulation of beliefs from the agent's perceptions and prior beliefs, backward inference of from the agent's observed actions, or through a joint integration of all available information.

\Tracing follows the structure of this framework without using explicit models of rational belief updating and action selection. Instead, we use LLMs to simulate how an agent is likely to update their mental states in response to what they perceive, and to evaluate the likelihood of an agent's actions given their mental states. This enables greater generality, and decomposes social inference into the simpler tasks of forward simulation and likelihood evaluation by LLMs.

\textbf{Sequential Monte Carlo (SMC)}
refers to a family of algorithms designed for incremental inference over sequences of posterior distributions \citep{del2006sequential}, such as posterior inference over latent dynamics from time series data.
SMC uses a collection of weighted hypotheses (called particles) to approximate each distribution in the sequence. Given particles for step $t-1$ in the sequence, SMC generates particles for step $t$ via propagation (extending each previous particle with new latent states that exist at step $t$) and reweighting (weighting each particle by the likelihood of the observed data under that particle's latent states). The particles are then resampled according to their weights (focusing samples on regions of high posterior probability) and optionally rejuvenated with Markov chain Monte Carlo (MCMC) to increase particle diversity \citep{chopin2002sequential}. To improve inference quality, SMC can make use of \emph{custom proposals} at for propagation or rejuvenation \citep{lew2023smcp3}, using data-driven cues to generate more plausible hypotheses \citep{perov2015data}.

To infer hidden mental states that change over time, \tracing follows an SMC-like structure, using LLMs as proposals for propagating hypotheses about agents' mental states, and weighting these hypotheses by likelihood scores generated by LLMs. However, for simplicity, \tracing does not compute full importance weights for each particle, since this would require accessing LLM log probabilities not provided by most APIs.

\subsection{Algorithm}
\label{subsec:algo}
\begin{wrapfigure}[17]{r}{0.51\textwidth}
\begin{minipage}{0.5\textwidth}
\vspace{-2.2em}
\begin{algorithm}[H]
\caption{Thought-tracing} \label{alg:tt}
\begin{algorithmic}[1]
\small
\Procedure{Trace}{text $c$, target agent $\mathcal{A}$}
    \State Trajectory $\tau \gets$ \Call{Preprocess}{$c$}
    
    \For{$t = 1$ to $T$}
        \State $(s_t, a_t, p_t) \gets \tau[t]$ %
        
        \If{$t = 1$}
            \State $H_t \gets$ \Call{Initialize}{$s_t, a_t, p_t$}  %
        \Else
            \State $H_t \gets$ \Call{Propagate}{$H_{t-1}, s_t, a_t, p_t$} %
        \EndIf

        \If{$a_t \neq \emptyset$}
            \State $W_t \gets$ \Call{UpdateWeights}{$H_t, a_t$} %
        \EndIf
        
        \If{effective sample size of $H_t$ is low}
            \State $H_t \gets$ \Call{Resample}{$H_t$} %
        \EndIf
        \If{text similarity of $H_t$ is high}
            \State $H_t \gets$ \Call{Rejuvenate}{$H_t$} %
        \EndIf
    \EndFor
     
    \State $\tilde{\tau} \gets$ \Call{HypothesesSummarization}{$\tau$, $H$}
    \State \Return $\tilde{\tau}$ \Comment{$\tau$ with traced thoughts}
\EndProcedure
\end{algorithmic}
\end{algorithm}
\end{minipage}
\end{wrapfigure}
Algorithm \ref{alg:tt} shows the pseudo-code for \tracing.
Specifically, it leverages sequential Monte Carlo principles to maintain a dynamically updated set of natural language hypotheses about the target agent's mental state, weighting them in response to new observations.
These hypotheses are updated based on the agent's perception, prior beliefs, and actions.

\paragraph{Preprocessing}
Given a text input $\mathcal{C}$, we parse it into a trajectory (i.e., sequence of state-action pairs) of the target agent $\mathcal{A}$.
Specifically, we use the LLM for \emph{action labeling} of each sentence of the input.
We classify each sentence whether it includes actions (e.g., physical movements or utterances) from $\mathcal{A}$.
If the sentence does not include any actions from $\mathcal{A}$, it is considered a state sentence, which includes both information about the world state (descriptions of other entities or the environment) and the agent state (the agent's features or characteristics). After we obtain the state-action sequence $\{(s_t, a_t)\}_{t=1}^T$, we use the LLM for \emph{perception inference}, producing percept descriptions $\{p_t\}_{t=1}^T$ that describe whether the agent $\mathcal{A}$ perceived each state $s_t$ and what they perceived during each action $a_t$.
As a result, we obtain target agent $\mathcal{A}$'s trajectory $\tau_{\mathcal{A}} = \{(s_t, a_t, p_t)\}_{t=1}^T$.


\paragraph{Initialization}
\Tracing begins by sampling a collection of weighted hypotheses $H_1 = \{(h_1^{(i)}, w_1^{(i)})\}_{i=1}^N$, where $h_1^{(i)}$ denotes a hypothesis and $w_1^{(i)}$ is the associated weight, from an initial distribution that approximates the step-$1$ posterior $p(h_1 | s_1, a_1, p_1)$. In practice, the LLM is given $(s_1, a_1, p_1)$ as input, and produces a list of $N$ hypotheses, with each hypothesis assigned an initial weight of $w_1^{(i)} = \frac{1}{N}$. We set $N=4$ for our experiments.

\paragraph{Propagation}
At every step $t$, each hypothesis $h_{t-1}^{(i)}$ is propagated forward to produce a new hypothesis $h_{t}^{(i)}$ based on the trajectory so far:  
\begin{equation}
h_{t}^{(i)} \sim p_\theta\bigl(h_{t}\,\big|\;h_{t-1}^{(i)}, \{(s_\tau, a_\tau, p_\tau)\}_{\tau=1}^{t}\bigr).
\end{equation}
Here, $\theta$ denotes the parameters of an LLM and $\{(s_j, a_j, p_j)\}_{j=1}^{t}$ denotes the trajectory (i.e., state, action, perception) up to time $t$. In effect, the LLM is used as a data-driven proposal that generates a plausible mental state $h_t$ given the observations.

\paragraph{Weight Update}
After propagation, each hypothesis $h_t^{(i)}$ receives a weight update based on how well it explains the action or utterance $a_t$.
The LLM is used to output the likelihood that $a_t$ would be produced under each hypothesis:
\begin{equation}
w_t^{(i)} = p_\theta\bigl(a_t \big| h_t^{(i)}, \{(s_j, a_j, p_j)\}_{j=1}^{t-1}, s_t, p_t \bigr)
\end{equation}
While it is possible to compute this likelihood from model's logprobs (as done in \citet{zhi2024pragmatic}), in practice we find that instructing the LLM to choose from six options—from ``very likely (around 90\%)'' to ``very unlikely (below 10\%)''—yields better performance and stability.
We map these options to numerical scores and then normalize the weights so that $\sum_{i}^{N} w_t^{(i)} = 1$.

\paragraph{Resampling \& Rejuvenation}
Over time, some hypotheses may have very low weights, while others have very high weights.
This is referred to as the degeneracy problem in SMC algorithms, and a well-known mitigation is resampling the hypotheses based on their weights \citep{douc2005comparison}.
Therefore, after the weight update, we check the effective sample size (ESS) to decide if resampling is needed.
If the ESS is lower than a specific threshold, we sample $N$ new hypotheses with replacement from the existing ones $H_t$ according to their normalized weights.
This discards low-weight hypotheses and duplicates higher-weight ones to obtain a new set of hypotheses.
Additionally, we check the text similarity of the hypotheses to prevent them from collapsing into a single point.
If similarity is over a certain threshold, we rejuvenate the hypotheses via paraphrasing.%

\paragraph{Hypotheses Summarization}

After iterating through the entire trajectory, we aggregate the hypotheses at each time step leveraging their weights.
In SMC, computing statistics such as means and variances is a common practice to summarize the distribution represented by the particles.
Since our hypotheses are in natural language, we instead use the LLM to generate a weighted summary of the hypotheses set $H_t$ at time step $t$, given the past trajectory and the hypotheses along with their weights.
This process can be viewed as deriving a weighted consensus over the hypotheses at each time step.

The final output of \tracing is a trajectory of the target agent $\mathcal{A}$ with the summarized hypotheses interleaved. 
An example is in Table \ref{tab:example_trace}.
State, action, perception, and beliefs of $\mathcal{A}$ at each time step is concatenated as a reasoning trace formatted with HTML-style tags.

\input{tables/trace_example}




\section{Experiments}
\label{sec:experiments}

Our \tracing algorithm offers a general method for tracing the mental states of a target agent, given context.
While the algorithm is not explicitly designed to solve benchmark questions, we evaluate its validity by testing whether it can improve existing models' performance on theory-of-mind benchmarks.
We apply \tracing to off-the-shelf LLMs and feed the generated thought traces along with the original context and compare their performance against other baselines, such as reasoning models.


\paragraph{Benchmarks \& Metrics}
We test our approach on four theory-of-mind benchmarks that span diverse format and question types:

(1) \textbf{ParaphrasedToMi} \citep{sclar-etal-2023-minding} is a revised version of the classic theory-of-mind benchmark ToMi \citep{le-etal-2019-revisiting} that addresses the limited linguistic diversity of the original ToMi by rewording all ToMi templates using GPT-3 with additional manual filtering. %
The resulting dataset is considerably more complex, as actions are expressed in a less straightforward way.
We use a total of 600 questions and report three metrics: accuracy of (i) all questions, (ii) false belief questions, and (iii) true belief questions.

(2) \textbf{BigToM} \citep{gandhi2023understanding} is a benchmark featuring synthetic narratives centered on a person's desires, actions, and beliefs, along with an event that changes the state of the environment.
It includes questions about the person's beliefs and actions.
We find some of the narratives and ground-truth answers in BigToM include incoherent stories and errors.
To address this issue, we randomly sampled 600 scenarios from the dataset and re-annotated by crowdsourcing 510 online participants (See details in Appendix \ref{app:bigtom}). We only retained samples with an agreement rate higher than 90\%, which resulted in 249 samples.
We report the average accuracy of the questions.

(3) \textbf{FANToM} \citep{kim-etal-2023-fantom} is a multi-party conversation question-answering dataset designed to test coherent theory-of-mind capabilities.
In FANToM, the speakers join and leave the conversation while it continues, making the participants hold both false and true beliefs.
The benchmark includes first-order and second-order theory-of-mind questions about the beliefs of conversation participants in various formats, such as multiple-choice and list type questions.
We use 64 sampled conversations from the short version of FANToM, containing a total of 1,086 questions.
We report three metrics (i) All Qs, (ii) Answerability All Qs, and (iii) Info-Accessibility All Qs, each requiring the model to correctly answer all questions for the corresponding question types for a given conversation snippet.

(4) \textbf{MMToM-QA} \citep{jin-etal-2024-mmtom} is a multi-modal question-answering benchmark that includes questions covering the beliefs and goals of a person searching for an object (e.g., a remote controller).
We use 200 questions in its text-only subset, which describes a person’s activities in a household environment.
Because some objects are associated with unusual locations—such as a remote controller and a fridge—contrary to commonsense, we replace room names and object labels with letters (e.g., room A and object A).
We report three metrics: average accuracies of (i) all questions, (ii) belief questions, and (iii) goal questions.

\input{tables/merged-scores}

\paragraph{Baseline \& Setup}
We apply our method to four state-of-the-art LLMs and compare their performance, including five reasoning models:
GPT-4o \citep{hurst2024gpt}, o1, o1-mini \citep{o12024openai}, o3-mini \citep{o3mini2025openai}, DeepSeek-R1 \citep{deepseekr12025}, Llama 3.3 70B Instruct \citep{dubey2024llama}, Gemini 1.5 Pro \citep{team2024gemini}, Qwen 2.5 72B \citep{yang2024qwen2}, and QwQ 32B preview \citep{qwq-32b-preview}.
We also compare with zero-shot chain-of-thought (CoT) reasoning \citep{kojima2022large}.



\subsection{Overall Results}

\paragraph{Thought-tracing (+TT) improves all baseline models across all benchmarks.}
Table \ref{tab:scores} shows the results on all four benchmarks.
Although \tracing does not take benchmark questions as input, its output significantly improves the model performance on the main metrics -- the average accuracy (Avg.) on Paraphrased ToMi, BigToM, MMToM-QA, and the AllQs score on FANToM.
For example, Llama 3.3 70B + TT, Gemini 1.5 Pro + TT, and Qwen 2.5 72B + TT significantly outperforms GPT-4o + CoT on Paraphrased ToMi.
These results indicate that the traced mental states from \tracing provide accurate intermediate reasoning steps that helps models correctly answer mental state related questions.

\paragraph{Thought-tracing guides better reasoning trajectories.}
The fact that \tracing outperforms CoT indicates the traced thoughts exhibit better reasoning traces.
Moreover, when CoT is applied on top of the traced thoughts, models' performance improves further in many cases.
For example, Llama 3.3 70B and Qwen 2.5 72B achieve the first and second best scores on Paraphrased ToMi, respectively.
All models achieves the best score when \tracing and CoT is applied sequentially on MMToM-QA.
Interestingly, Qwen 2.5 does not show any improvement on FANToM scoring near 0, when \tracing is applied.
However, when CoT is further applied its performance jumps to 32.1, even outperforming o1-medium-effort and o3-mini.
These results indicate that results from \tracing can further guide the model to obtain better reasoning trajectories.



\subsection{Comparison with Reasoning Models}
\label{subsec:reasoning_model_analysis}

\paragraph{Thought-tracing applied models show better performance than reasoning models.}
Except for o1 on MMToM-QA, models with \tracing significantly outperform o1, o1-mini, o3-mini, DeepSeek R1, and QwQ 32B preview on all benchmarks.\footnote{Reasoning models were provided with extra formatting prompts to align with the evaluation setup of these benchmarks. Without those formatting prompts, the scores are significantly lower.}
Moreover, even LLMs with CoT performs better than reasoning models, such as o3-mini, DeepSeek R1 and QwQ 32B.
The o1 model also underperforms than CoT on FANToM.
This suggest that reinforcing mathematical or programming reasoning does not generalize to social reasoning, necessitating separate training scheme in this domain.

\paragraph{Reasoning models show performance trade-off.}
A performance trade-off can be observed between false belief scenarios and true belief scenarios in Paraphrased ToMi.
While o1 and o3-mini achieve near-perfect scores (i.e., 100) on the false belief scenarios, their performance on the easier true belief scenarios drops sharply to below 50 and even below 30 in some cases.
A similar pattern is observed with DeepSeek R1, whose scores on the easier true belief scenarios are lower than those on the false belief scenarios.
In contrast, the QwQ 32B preview follows the pattern seen in other vanilla LLMs, displaying higher scores for true belief scenarios but lower scores for false belief scenarios.
Although these reasoning models do not release their training data, an interesting future direction would be to investigate how reinforcement learning influences this trade-off.
On the other hand, \tracing applied models show a more balanced performance across false belief and true belief scenarios, with improvements observed in both cases, except for GPT-4o.

\begin{figure}[t!]
    \centering
    \includegraphics[width=\linewidth]{figures/token_counts_horizontal_flatter.pdf}
    \vspace{-2em}
    \caption{The average output token counts of reasoning models and \tracing for Paraphrased ToMi and MMToM-QA.}
    \label{fig:token_count}
    \vspace{-1.1em}
\end{figure}

\paragraph{Output token counts and performance}
Figure \ref{fig:token_count} summarizes the average output token counts for o3-mini, o1, DeepSeek R1, and GPT-4o with \tracing on Paraphrased ToMi.

(1) Output token counts in o1 and o3-mini do not correlate with performance.
OpenAI's o1 and o3-mini models include an additional parameter that controls `reasoning effort' using three settings: low, medium, and high.
This parameter guides the model in determining how many reasoning tokens to generate before producing a final response.
Although the o1 model with high reasoning effort outperforms the medium and low settings on ToMi, this pattern does not consistently appear across other benchmarks.
For example, on BigToM, FANToM, and MMToM-QA, the o1 model with low reasoning effort achieves the highest performance.
Similarly, while increased reasoning effort in o3-mini improves performance on MMToM-QA, this is not the case for other benchmarks.
Moreover, performance on true belief scenarios in Paraphrased ToMi gets worse as reasoning effort increases.

(2) Theory-of-mind (ToM) questions elicit more output tokens than factual questions.
In Paraphrased ToMi, all reasoning models use significantly more output tokens when addressing ToM questions compared to factual ones.
This suggests processing social information is more computationally demanding for these models.

(3) Token usage for incorrect and correct responses to ToM questions differs across datasets.
Although a recent analysis reported incorrect responses from DeepSeek R1 and QwQ-32B preview have significantly higher token usage \citep{wang2025thoughts}, this does not hold for ToM benchmarks.
In Paraphrased ToMi, all reasoning models show significantly higher token usage for incorrect ToM responses than correct ones.
Moreover, the pattern is different for MMToM-QA, which the token usage is similar between correct and incorrect responses or is higher for correct ones.
On the other hand, \tracing exhibits balanced token usage regardless of whether the ToM responses are correct or incorrect.
We note that the relative difficulty of the questions in ToMi and MMToM-QA is consistent compared to benchmarks in other domains, and all models achieve near-perfect scores on ToMi's factual questions.

(4) Models with \tracing produce significantly shorter reasoning trajectories while achieving better performance.
As shown in Table \ref{tab:scores}, GPT-4o+TT and GPT-4o+TT+CoT deliver the best and second-best performance among these models on ToMi, with scores of 76 and 74.5, respectively.
Despite the better performance, these models generate significantly fewer output tokens than o3-mini, o1 (with medium and high reasoning effort), and DeepSeek R1.
Furthermore, compared to o1-low and o3-mini-low, GPT-4o+TT and GPT-4o+TT+CoT score almost 10 points higher.
Similarly in MMToM-QA, GPT-4o+TT+CoT outperforms DeepSeek R1 and shows performance comparable with o3-mini-medium while using less than 50\% of the tokens.

\paragraph{Thought switching and performance}
Additionally, we analyze the thought switching patterns for DeepSeek R1, following \citep{wang2025thoughts}, on Paraphrased ToMi and MMToM-QA.
The authors initially reported that frequent thought switching leads to underthinking and ultimately lower performance.
For Paraphrased ToMi, incorrect responses included an average of 9.69 thoughts, while correct ones included an average of 6.21 thoughts, which is consistent with the reported pattern.
However, for MMToM-QA, responses for incorrect responses included an average of 6.83 thoughts similar to correct responses, which contained an average of 6.45 thoughts.


\subsection{Analysis on Thought-tracing}
\label{subsec:tracing-analysis}

\paragraph{Qualitative Error Analysis}
We manually evaluated \tracing's errors on MMToM-QA and find an inherent bias across models.
In many cases, when an agent searches for something and repeatedly encounters an object—despite not interacting with it—the models generate hypotheses suggesting that the object is what the agent is looking for.
However, according to both social common-sense and explicit models of ToM \citep{btom2017,ying2024understanding}, if an agent repeatedly sees an object but continues searching elsewhere, it is likely that they are actually searching for something else.
Since this error may be a side effect of the perception inference in \tracing, we conduct ablation experiments.

\paragraph{Ablation}
\input{tables/variants}
We conducted ablation experiments on three models—GPT-4o, Llama 3.3 70B, and Gemini 1.5 Pro—by removing either the perception inference or the weight update (\S \ref{subsec:algo}).
For the perception inference removal, we directly sample hypotheses only from the state-action pairs in the trajectory at each time step.
For the weight update removal, we apply uniform weights for all hypotheses across the trajectory at every time step.

Table \ref{tab:variants} shows the results.
Both modifications lead to reduced performance, with the removal of perception inference causing a greater drop.
This suggests not only perception inference is a crucial component in LLM's theory-of-mind reasoning \citep{jung-etal-2024-perceptions}, but also the fact that LLMs tend not to incorporate perception inference internally when generating mental state hypotheses.
Additionally, this indicates the weight update based on action likelihood helps the model prioritize more promising hypotheses.

\paragraph{Self-correction during Propagation}
We also tested another variant of \tracing that self-corrects its previous hypothesis and predict a new hypothesis during the propagation step (\S \ref{subsec:algo}).
Table \ref{tab:variants} shows the results.
Although self-correction has shown performance increase in some domains \citep{madaan2023selfrefine, shinn2023reflexion}, it does not show better performance for hypotheses propagation on the three ToM benchmarks.
In most cases, it leads to worse performance, indicating self-correction capability is not effective in our iterative updates, potentially due to the accumulation of correction errors.
We suggest future works to investigate the effects of LLM's self-correcting behavior in the social domain.








\section{Related Work}

\paragraph{Model-based Theory-of-Mind Reasoning}
Cognitive science research has shown that humans reason about other agents' minds by assuming their actions are rational and goal-directed \citep{dennett1981intentional, csibra1999goal, baillargeon2016psychological}. These processes have been formalized as Bayesian Theory of Mind \citep{btom2017,jara2019naive}, which models agents as rational actors that plan and act upon their beliefs and desires to achieve their goals. This probabilistic model can then be inverted via \textit{inverse planning}, inferring agents' mental states from their behavior \citep{baker2009action,ying2023neuro,ying2024understanding}, including via SMC methods \citep{zhi2020online,zhixuan2024infinite}. While \tracing is patterned on this model-based approach, we do not explicitly model rational agents, but instead use LLMs as \emph{implicit models} of agents' behavior, and as \emph{hypothesis generators} for agents' thoughts, enabling our method to be applied to open-ended inputs.

\paragraph{Theory-of-Mind Reasoning in LLMs}
Debates on whether LLMs are capable of ToM reasoning have sparked extensive controversy \citep{ullman2023large, whang2023nytimes, ma-etal-2023-towards-holistic, shapira-etal-2024-clever}.
As a result, many benchmarks for evaluating ToM reasoning have been released \citep[inter alia]{le-etal-2019-revisiting, gandhi2023understanding, wu-etal-2023-hi, shapira-etal-2023-well, jin-etal-2024-mmtom, chen-etal-2024-tombench, xu-etal-2024-opentom} along with task complexity assessments \citep{huang2024notion}.
While LLMs succeed on some tasks, analyses on these benchmarks show that they are not yet at the level of human ToM, with signs of overfitting \citep{sclar-etal-2023-minding} and overall poor capabilities \citep{sap-etal-2022-neural, kim-etal-2023-fantom}.
To mitigate this, several inference-time methods have been introduced \citep{sclar-etal-2023-minding, wilf-etal-2024-think, jung-etal-2024-perceptions}.
However, they rely on specific assumptions or few-shot examples, making them less scalable.
Recently, \citet{sclar2025exploretom} showed search for generating ToM training data and \citet{cross2025hypothetical} introduced a modular design for ToM hypothesis generation in multi-agent game scenarios.
In this work, we aim to minimize assumptions and introduce a flexible algorithm capable of generating traces of a target agent's mental states in open-ended text.


\paragraph{Inference-time Reasoning for LLMs} has emerged as a pivotal area of research, particularly in mathematics, coding, and puzzle solving.
Early work, such as Chain-of-Thought \citep{wei2022chain}, demonstrated the benefits of generating intermediate reasoning steps, and was later augmented by approaches capable of exploring multiple reasoning paths \citep{yao2023tree,besta2024graph}, aggregation across reasoning chains \citep{wang2023selfconsistency}, and problem decomposition \citep{zhou2023leasttomost}.
Recently, reasoning models trained via reinforcement learning---o1 \citep{o12024openai}, R1 \citep{deepseekr12025}, and QwQ \citep{qwq-32b-preview}---have shown remarkable performance.
Most of these approaches leverage objectively verifiable answers to enable reinforcement and accurate selection among multiple reasoning paths.  However, such verification is challenging in social reasoning due to subjectivity and uncertainty. To handle uncertainty, recent methods perform probabilistic inference in LLMs via sequential Monte Carlo \citep{lew2023sequential,zhao2024probabilistic,loula2025syntactic}, but have not been used to infer and track mental states. In this work, we introduce a general SMC-like algorithm capable of operating effectively in the social domain while bypassing the need for objective verification.




\section{Conclusion}
In this paper, we introduced \tracing, a new inference-time algorithm that performs approximate inference over agents' mental states.
Our approach draws inspiration from the sequential Monte Carlo (SMC) method and follows the conceptual structure of Bayesian Theory of Mind \citep[BToM;][]{btom2017}.
\Tracing leverages an SMC-like structure by using LLMs as proposals for natural language hypotheses, which are then weighted with actions from the given text—all without requiring ground-truth answers to questions in datasets.
Our evaluation on four theory-of-mind benchmarks demonstrates its effectiveness, significantly improving upon baseline LLMs and outperforming recent reasoning models such as o3-mini and R1.
Additionally, our findings reveal that reasoning models show different behaviors in the social domain compared to their remarkable performance in areas like math and coding.
Nonetheless, there remains room for improvement due to errors in areas such as how hypotheses about agents' goals are generated (Section \ref{subsec:tracing-analysis}).
This suggests that more work needs to be done to achieve the reliability that explicit BToM models demonstrate in closed-world settings \citep{zhixuan2022solving, zhi2024pragmatic, ying2024understanding}.
We hope these insights pave the way for future research on inference-time reasoning in social settings, where AI agents will play an increasingly important role.




\bibliography{anthology,custom}
\bibliographystyle{colm2025_conference}

\appendix



\section{BigToM Re-annotation}
\label{app:bigtom}

The original BigToM dataset \citep{gandhi2023understanding} includes 6 tasks, each with 200 stimuli. In our re-annotation, we took the first 100 stimuli from each task category.

We recruited 510 participants (Average age = 41.33, 238 males, 265 females, 7 others) via Prolific. 
To minimize potential bias, we did not conduct tutorials for this task.
The experiment took place over a customized interface. Each participant was randomly assigned to complete 30 stimuli randomly sampled from the set of stimuli. Each stimulus received on average 13 ratings.

Table \ref{tab:bigtom_agreement} shows the average agreement for each question type.
We manually went over the re-annotation and found samples with an agreement rate below 90\% often contained incoherent stories or debatable answer options. 
Therefore, we only use samples with an agreement rate above 90\% in our experiments.

\input{tables/bigtom_agreement}

\section{Thought-tracing Setup}

We use greedy decoding through out our \tracing algorithm.
We set the number of hypotheses to 4 across all experiments.
The threshold for the effective sample size is set to 2.
We measure the text similarity across hypotheses using the Jaccard similarity, and the threshold for rejuvenation is set to 0.25.
For action labeling, we use GPT-4o-2024-08-06 for ParaphrasedToMi and BigToM, and a rule-based algorithm for FANToM and MMToM-QA.



\end{document}
