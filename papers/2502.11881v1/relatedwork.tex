\section{Related Work}
\paragraph{Model-based Theory-of-Mind Reasoning}
Cognitive science research has shown that humans reason about other agents' minds by assuming their actions are rational and goal-directed \citep{dennett1981intentional, csibra1999goal, baillargeon2016psychological}. These processes have been formalized as Bayesian Theory of Mind \citep{btom2017,jara2019naive}, which models agents as rational actors that plan and act upon their beliefs and desires to achieve their goals. This probabilistic model can then be inverted via \textit{inverse planning}, inferring agents' mental states from their behavior \citep{baker2009action,ying2023neuro,ying2024understanding}, including via SMC methods \citep{zhi2020online,zhixuan2024infinite}. While \tracing is patterned on this model-based approach, we do not explicitly model rational agents, but instead use LLMs as \emph{implicit models} of agents' behavior, and as \emph{hypothesis generators} for agents' thoughts, enabling our method to be applied to open-ended inputs.

\paragraph{Theory-of-Mind Reasoning in LLMs}
Debates on whether LLMs are capable of ToM reasoning have sparked extensive controversy \citep{ullman2023large, whang2023nytimes, ma-etal-2023-towards-holistic, shapira-etal-2024-clever}.
As a result, many benchmarks for evaluating ToM reasoning have been released \citep[inter alia]{le-etal-2019-revisiting, gandhi2023understanding, wu-etal-2023-hi, shapira-etal-2023-well, jin-etal-2024-mmtom, chen-etal-2024-tombench, xu-etal-2024-opentom} along with task complexity assessments \citep{huang2024notion}.
While LLMs succeed on some tasks, analyses on these benchmarks show that they are not yet at the level of human ToM, with signs of overfitting \citep{sclar-etal-2023-minding} and overall poor capabilities \citep{sap-etal-2022-neural, kim-etal-2023-fantom}.
To mitigate this, several inference-time methods have been introduced \citep{sclar-etal-2023-minding, wilf-etal-2024-think, jung-etal-2024-perceptions}.
However, they rely on specific assumptions or few-shot examples, making them less scalable.
Recently, \citet{sclar2025exploretom} showed search for generating ToM training data and \citet{cross2025hypothetical} introduced a modular design for ToM hypothesis generation in multi-agent game scenarios.
In this work, we aim to minimize assumptions and introduce a flexible algorithm capable of generating traces of a target agent's mental states in open-ended text.


\paragraph{Inference-time Reasoning for LLMs} has emerged as a pivotal area of research, particularly in mathematics, coding, and puzzle solving.
Early work, such as Chain-of-Thought \citep{wei2022chain}, demonstrated the benefits of generating intermediate reasoning steps, and was later augmented by approaches capable of exploring multiple reasoning paths \citep{yao2023tree,besta2024graph}, aggregation across reasoning chains \citep{wang2023selfconsistency}, and problem decomposition \citep{zhou2023leasttomost}.
Recently, reasoning models trained via reinforcement learning---o1 \citep{o12024openai}, R1 \citep{deepseekr12025}, and QwQ \citep{qwq-32b-preview}---have shown remarkable performance.
Most of these approaches leverage objectively verifiable answers to enable reinforcement and accurate selection among multiple reasoning paths.  However, such verification is challenging in social reasoning due to subjectivity and uncertainty. To handle uncertainty, recent methods perform probabilistic inference in LLMs via sequential Monte Carlo \citep{lew2023sequential,zhao2024probabilistic,loula2025syntactic}, but have not been used to infer and track mental states. In this work, we introduce a general SMC-like algorithm capable of operating effectively in the social domain while bypassing the need for objective verification.