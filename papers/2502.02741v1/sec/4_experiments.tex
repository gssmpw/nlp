



\section{Experimental Evaluation}
\label{sec:experiments}

\begin{table*}[!t]\small
    \vspace{-0.6cm}
    \setlength{\tabcolsep}{3pt}
    \centering
    \resizebox{0.9\linewidth}{!}{ %< auto-adjusts font size to fill line
    \begin{tabular}{@{}c|c|l|cccccccccccc|c@{}}
    \toprule
    % Method  & Aotra $\uparrow$ & Gallbladder $\uparrow$  & Kidnery(L) $\uparrow$ & Kidnery(R) $\uparrow$ & Liver $\uparrow$ & Pancreas $\uparrow$ & Spleen $\uparrow$ & Stomach  $\uparrow$  & DSC $\uparrow$ \\ 
    % Method & Aot.$\uparrow$ & Gal.$\uparrow$  & Kid. L$\uparrow$ & Kid. R$\uparrow$ & Liv.$\uparrow$ & Pan.$\uparrow$ & Spl.$\uparrow$ & Sto.$\uparrow$  & DSC $\uparrow$\\ 
    Semantic labels & Prompts & Method & Spl. & R.Kd & L.Kd & GB & Eso. & Liv. & Stom. & Aorta & IVC & Veins & Panc. & AG & DSC \\
    % Semantic labels & Prompts & Method & Aorta & GB  & L.Kd & R.Kd & Liv. & Panc. & Spl. & Stom.  & DSC \\
    \midrule
    % VNet \cite{milletari2016v}   & 75.34 & 51.87 & 77.10 & 80.75 & 87.84 & 40.04 & 80.56 & 56.98 & 68.81 
    % \\
    % DARR \cite{fu2020domain}  & 74.74 & 53.77 & 72.31 & 73.24 & 94.08 & 54.18 & 89.90 & 45.96 & 69.77 \\
    % FastSCNN \cite{poudel2019fast}  & 77.79 & 55.96 & 73.61 & 67.38 & 91.68 & 44.54 & 84.51 & 68.76 & 70.53\\
    % FSSNet \cite{zhang2018fast}  & 82.87 & 64.06 & 78.03 & 69.63 & 92.52 & 53.10 & 85.65 & 70.86 & 74.59\\
    % DABNet \cite{li2019dabnet}   & 85.01 & 56.89 & 77.84 & 72.45 & 93.05 & 54.39 & 88.23 & 71.45 & 74.91\\
    % U-Net \cite{ronneberger2015u}   & 83.17 & 58.74 & 80.40 & 73.36 & 93.13 & 45.43 & 83.90 & 66.59 & 74.99 \\ 
    % AttnUNet \cite{oktay2018attention} & 89.55 & 68.88 & 77.98 & 71.11 & 93.57 & 58.04 & 87.30 & 75.75  & 77.77\\

     & & TransUNet~\cite{chen2021transunet}  & 0.952  & 0.927 & 0.929 & 0.662 & 0.757 & 0.969 & 0.889 & 0.920 & 0.833 & 0.791 & 0.775 & 0.637 & 0.838 \\ 
    & & 3D UX-Net~\cite{lee20223d}  & 0.946 & 0.942 & 0.943 & 0.593 & 0.722 & 0.964 & 0.734 & 0.872 & 0.849 & 0.722 & 0.809 & 0.671 & 0.814 \\
    & & UNETR~\cite{hatamizadeh2022unetr} & 0.968 & 0.924 & 0.941 & 0.750 & 0.766 & 0.971 & 0.913 & 0.890 & 0.847 & 0.788 & 0.767 & 0.741 & 0.856 \\ 
    \CheckmarkBold & -- & Swin-UNETR~\cite{hatamizadeh2021swin} & 0.971 & 0.936 & 0.943 & 0.794 & 0.773 & 0.975 & 0.921 & 0.892 & 0.853 & 0.812 & 0.794 & 0.765 & 0.869 \\
    & & nnUNet~\cite{isensee2019automated} & 0.942 & 0.894 & 0.910 & 0.704 & 0.723 & 0.948 & 0.824 & 0.877 & 0.782 & 0.720 & 0.680 & 0.616 & 0.802 \\
    & & nnFormer~\cite{zhou2021nnformer}  & 0.935 & 0.949 & 0.950 & 0.641 & 0.795 & 0.968 & 0.901 & 0.897 & 0.859 & 0.778 & 0.856 & 0.739  & 0.856 \\ 
    % & & EnsDiff & 0.938 & 0.931 & 0.924 & 0.772 & 0.771 & 0.967 & 0.910 & 0.869 & 0.851 & 0.802 & 0.771 & 0.745 & 0.854 \\
    \hline

    \XSolidBrush & GT & SAM~\cite{kirillov2023segment} & 0.933 & 0.922 & 0.927 & 0.805 & 0.831 & 0.899 & 0.808 & 0.890 & 0.894 & 0.492 & 0.728 & 0.708 & 0.819 \\
    \XSolidBrush & GT & SAM 2~\cite{ravi2024sam} & 0.946 & 0.923 & 0.924 & 0.859 & 0.888 & 0.928 & 0.893 & 0.852 & 0.884 & 0.434 & 0.694 & 0.705 & 0.828 \\
    \XSolidBrush & GT & MedSAM~\cite{ma2024segment} & 0.751 & 0.814 & 0.885 & 0.766 & 0.721 & 0.901 & 0.855 & 0.872 & 0.746 & 0.771 & 0.760 & 0.705 & 0.803 \\
    
    \XSolidBrush & GT & SAM-U~\cite{deng2023sam} & 0.868 & 0.776 & 0.834 & 0.690 & 0.710 & 0.922 & 0.805 & 0.863 & 0.844 & 0.782 & 0.611 & 0.780 & 0.790 \\
    \XSolidBrush & GT & SAM-Med2D~\cite{cheng2023sam} & 0.873 & 0.884 & 0.932 & 0.795 & 0.790 & 0.943 & 0.889 & 0.872 & 0.796 & 0.813 & 0.779 & 0.797 & 0.847 \\

     \XSolidBrush & GT & RFMedSAM 2 & 0.961 & 0.943 & 0.945 & 0.909 & 0.918 & 0.965 & 0.945 & 0.954 & 0.942 & 0.968 & 0.883 & 0.843 & 0.923 \\ \hline
    
    \CheckmarkBold & No Needs &SAMed~\cite{zhang2023customized} & 0.862 & 0.710 & 0.798 & 0.677 & 0.735 & 0.944 & 0.766 & 0.874 & 0.798 & 0.775 & 0.579 & 0.790 & 0.776 \\

    \CheckmarkBold & No Needs & SAM3D~\cite{bui2024sam3d} & 0.933 & 0.901 & 0.909 & 0.601 & 0.733 & 0.944 & 0.882 & 0.856 & 0.778 & 0.722 & 0.759 & 0.590 & 0.801 \\

    \CheckmarkBold & No Needs & RFMedSAM 2 & 0.969 & 0.947 & 0.953 & 0.611 & 0.817 & 0.974 & 0.909 & 0.917 & 0.887 & 0.803 & 0.865 & 0.747 & 0.867 \\ 
    \bottomrule
    \end{tabular}
    }
    \caption{
    Comparison of RFMedSAM 2 with state-of-the-art methods on the BTCV dataset. ``Semantic labels'' indicate the model's ability to infer labels, while ``Prompt'' specifies the source of the prompt.}
    \vspace{-0.2cm}
    \label{tab:sotaSynapse}
\end{table*}

\begin{figure*}[!t]
\centering
  % \vspace{-0.6cm}
\includegraphics[width=1\textwidth]{fig/visualizationBTCV.png}
      \caption{Qualitative comparison on BTCV dataset. RFMedSAM 2 is the most precise for each class and has fewer segmentation outliers.} 
% \textcolor{red}{[include more detailed descriptions.]}}
\vspace{-0.4cm}
\label{fig:visualizationBTCV}
\end{figure*}

\subsection{Datasets and Evaluation Metrics.}

We conduct experiments using two publicly available datasets: the AMOS22 Abdominal CT Organ Segmentation dataset~\cite{ji2022amos} and the Beyond the Cranial Vault (BTCV) challenge dataset~\cite{landman2015miccai}. \textbf{(i)} The AMOS22 dataset contains 200 abdominal CT scans with manual annotations for 16 anatomical structures, which serve as the basis for multi-organ segmentation tasks. The testing set comprises 200 images, and we evaluate our model using the AMOS22 leaderboard. \textbf{(ii)} The BTCV dataset includes 30 cases of abdominal CT scans. Following established split strategies~\cite{hatamizadeh2021swin}, we use 24 cases for training and 4 cases for validation. Performance is assessed using the average Dice Similarity Coefficient (DSC) across 13 abdominal organs.

In Tables~\ref{tab:amos} and~\ref{tab:sotaSynapse}, ``Semantic labels'' refer to the ability of a model to infer and predict labels, while ``Prompt'' specifies the prompt source. Since SAM and MedSAM do not predict semantic labels and require additional prompts, we use GT or predictions inferred by a pre-trained nnUNet to generate prompts, with the corresponding labels used as semantic labels.


\subsection{Comparison with State-of-the-Art Methods}

\subsubsection{Results on the AMOS22 Dataset.} 
Table~\ref{tab:amos} presents the quantitative results on the AMOS22 dataset, comparing our proposed RFMedSAM 2 with widely recognized segmentation methods, including CNN-based methods (nnUNet~\cite{isensee2019automated}), transformer-based methods (UNETR~\cite{hatamizadeh2022unetr}, SwinUNETR~\cite{hatamizadeh2021swin}, nnFormer~\cite{zhou2021nnformer}), and SAM-based methods (SAM~\cite{kirillov2023segment}, SAM 2~\cite{ravi2024sam}, MedSAM~\cite{ma2024segment}, SAMed~\cite{zhang2023customized}, and SAM3D~\cite{bui2024sam3d}). To ensure fairness, all methods are evaluated using 5-fold cross-validation without ensemble techniques.

We observe that our RFMedSAM 2 outperforms all existing methods on most organs, achieving a new state-of-the-art performance in DSC. When utilizing the predictions from nnUNet for bounding box prompts, SAM, SAM 2, and MedSAM exhibit decreases of 34\%, 18\%, and 27\%, respectively, compared to nnUNet's accuracy of 87.8\%. These reductions in accuracy indicate negative implications for the results. SAM 2 achieves the best performance, which demonstrates it presents the strongest zero-shot capabilities. Specifically, RFMedSAM 2 surpasses nnUNet by 2.9\% in DSC, respectively. RFMedSAM 2 surpasses SAMed and SAM3D by 23\% and 25\% in DSC, respectively. The significant improvement demonstrates our proposed prompt-free RFMedSAM 2 is better than other prompt-free SAM models.
In the extremely hard AMOS 2022 dataset, our RFMedSAM 2 achieves state-of-the-art performance, which confirms the efficacy of our method. 

\subsubsection{Results on the BTCV Dataset.} 
Table~\ref{tab:sotaSynapse} shows the quantitative performance on the BTCV dataset, comparing RFMedSAM 2 with leading SAM-based methods with proper prompts(\textit{i.e.},~SAM~\cite{kirillov2023segment}, SAM2~\cite{ravi2024sam}, MedSAM~\cite{ma2024segment}, SAM-U~\cite{deng2023sam}, and SAM-Med2D~\cite{cheng2023sam}), SAM-based methods without prompts (\textit{i.e.},~SAMed~\cite{zhang2023customized} and SAM3D~\cite{bui2023sam3d}), convolution-based methods (VNet~\cite{ronneberger2015u} and nnUNet~\cite{isensee2019automated}), transformer-based methods (TransUNet~\cite{chen2021transunet}, SwinUNet~\cite{cao2021swin}, 
and nnFormer~\cite{zhou2021nnformer}). 
We observe that RFMedSAM 2 outperforms all existing methods, setting a new state-of-the-art benchmark. When provided with proper prompts, RFMedSAM 2 achieves a DSC of 92.3\%, representing a significant 5\% improvement over the previous state-of-the-art method. In comparison, among SAM-based methods with the proper prompts, the best performance, achieved by SAM-Med2D, reaches only 84.7\%. Our proposed RFMedSAM 2 surpasses this by 7.6\%, highlighting its superior effectiveness over SAM-based methods with prompts. When prompts are not provided, our proposed prompt-free RFMedSAM 2 outperforms the other prompt-free SAMed and SAM3D by 9\% and 6\%, respectively. Compared with non-SAM-based methods, our method surpasses nnUNet and nnFormer by 6.4\% and 1\% in DSC for the highly saturated dataset. In Figure~\ref{fig:visualizationBTCV}, we illustrate qualitative results compared to representative methods. These results also demonstrate that our RFMedSAM 2 can predict more accurately the `Stomach', `Spleen', and `Liver' labels. 
% The results demonstrate the effectiveness of our method. 

% SAM-based methods (e.g., SAMed~\cite{zhang2023customized}, SAM3D~\cite{bui2024sam3d}), CNN-based methods (VNet~\cite{ronneberger2015u}, nnUNet~\cite{isensee2019automated}), and transformer-based methods (TransUNet~\cite{chen2021transunet}, SwinUNet~\cite{cao2021swin}, nnFormer~\cite{zhou2021nnformer}). Our RFMedSAM 2 demonstrates superior performance, setting a new state-of-the-art in DSC. Specifically, RFMedSAM 2 outperforms nnFormer by --\%, highlighting its robustness on a highly competitive dataset. Additionally, RFMedSAM 2 outperforms SAM-based methods such as SAM, MedSAM, SAMed, and SAM3D by --\%, --\%, --\%, and --\%, respectively, showcasing its effectiveness in refining medical image segmentation tasks.

% \begin{table*}[!t]\small
%     \setlength{\tabcolsep}{3pt}
%     \centering
%     \resizebox{0.9\linewidth}{!}{ % Adjusts font size to fit the line
%     \begin{tabular}{@{}c|c|l|cccccccccccc|c@{}}
%     \toprule
%     Semantic labels & Prompts & Method & Spl. & R.Kd & L.Kd & GB & Eso. & Liv. & Stom. & Aorta & IVC & Veins & Panc. & AG & DSC \\
%     \midrule
%     & & TransUNet~\cite{chen2021transunet}  & 0.952  & 0.927 & 0.929 & 0.662 & 0.757 & 0.969 & 0.889 & 0.920 & 0.833 & 0.791 & 0.775 & 0.637 & 0.838 \\ 
%     & & 3D UX-Net  & 0.946 & 0.942 & 0.943 & 0.593 & 0.722 & 0.964 & 0.734 & 0.872 & 0.849 & 0.722 & 0.809 & 0.671 & 0.814 \\
%     & & UNETR~\cite{hatamizadeh2022unetr} & 0.968 & 0.924 & 0.941 & 0.750 & 0.766 & 0.971 & 0.913 & 0.890 & 0.847 & 0.788 & 0.767 & 0.741 & 0.856 \\ 
%     -- & -- & Swin-UNETR & 0.971 & 0.936 & 0.943 & 0.794 & 0.773 & 0.975 & 0.921 & 0.892 & 0.853 & 0.812 & 0.794 & 0.765 & 0.869 \\
%     & & nnUNet~\cite{isensee2019automated} & 0.942 & 0.894 & 0.910 & 0.704 & 0.723 & 0.948 & 0.824 & 0.877 & 0.782 & 0.720 & 0.680 & 0.616 & 0.802 \\
%     & & nnFormer  & 0.935 & 0.949 & 0.950 & 0.641 & 0.795 & 0.968 & 0.901 & 0.897 & 0.859 & 0.778 & 0.856 & 0.739  & 0.856 \\ 
%     \hline
%     \XSolidBrush & GT & SAM~\cite{kirillov2023segment} & 0.933 & 0.922 & 0.927 & 0.805 & 0.831 & 0.899 & 0.808 & 0.890 & 0.894 & 0.492 & 0.728 & 0.708 & 0.819 \\
%     \XSolidBrush & GT & SAM 2~\cite{ravi2024sam} & 0.946 & 0.923 & 0.924 & 0.859 & 0.888 & 0.928 & 0.893 & 0.852 & 0.884 & 0.434 & 0.694 & 0.705 & 0.828 \\
%     \XSolidBrush & GT & MedSAM~\cite{ma2024segment} & 0.751 & 0.814 & 0.885 & 0.766 & 0.721 & 0.901 & 0.855 & 0.872 & 0.746 & 0.771 & 0.760 & 0.705 & 0.803 \\
%     \XSolidBrush & GT & RFMedSAM 2 & 0.961 & 0.943 & 0.945 & 0.909 & 0.918 & 0.965 & 0.945 & 0.954 & 0.942 & 0.968 & 0.883 & 0.843 & 0.923 \\
%     \hline
%     \CheckmarkBold & No needs & SAM3D~\cite{bui2024sam3d} & 0.933 & 0.901 & 0.909 & 0.601 & 0.733 & 0.944 & 0.882 & 0.856 & 0.778 & 0.722 & 0.759 & 0.590 & 0.801 \\
%     \CheckmarkBold & No needs & RFMedSAM 2 & 0.967 & 0.951 & 0.952 & 0.641 & 0.800 & 0.974 & 0.879 & 0.916 & 0.887 & 0.797 & 0.857 & 0.724 & 0.862 \\
%     \bottomrule
%     \end{tabular}}
%     \caption{Comparison of RFMedSAM 2 with state-of-the-art methods on the BTCV dataset. ``Semantic labels'' indicate the model's ability to infer labels, while ``Prompt'' specifies the source of the prompt.}
%     \label{tab:sotaSynapse}
% \end{table*}

\begin{table}
    \vspace{-0.6cm}
    \centering
    \resizebox{0.92\linewidth}{!}{ %< auto-adjusts font size to fill line
    \begin{tabular}{@{}c|c|c|c}
    \toprule
     & train with prompts & learnable bboxes  & learnable masks  \\ \midrule
    
    w/ obj\_score & 0.923 & 0.792 & 0.847  \\ \hline
    w/o obj\_score & 0.920 & 0.628 & 0.867 \\
    \bottomrule
    \end{tabular}} 
    \caption{Experiments for different models with and without the prediction of object scores on BTCV dataset.}
    \label{tab:analysis_obj_score}
    % \vspace{-0.6cm}
\end{table}

\begin{table}
    \vspace{-0.2cm}
    \centering
    \resizebox{0.72\linewidth}{!}{ %< auto-adjusts font size to fill line
    \begin{tabular}{@{}c|c|c|c}
    \toprule
    Dataset & Step 0 - UNet & Step 1 - SAM  & Step 2 - SAM  \\ \midrule
    
    BTCV & 0.856 & 0.864 & 0.867  \\ \hline
    AMOS & 0.895 & 0.898 & 0.907 \\
    \bottomrule
    \end{tabular}} 
    \caption{The performance of output predictions for different steps. Two refinements lead to gradual improvement.}
    \label{tab:analysis_refinement}
    \vspace{-0.1cm}
\end{table}

\begin{figure}[!t]
\centering
  \vspace{-0.1cm}
\includegraphics[width=0.99\linewidth]{fig/refinement_comparison.png}
      \caption{Comparisons with different output predictions for Step 0, Step 1, and Step 2.} 
% \textcolor{red}{[include more detailed descriptions.]}}
\vspace{-0.6cm}
\label{fig:refinement_comparison}
\end{figure}



\subsection{Analysis}
\noindent \textbf{Refinements.}
Table~\ref{tab:analysis_refinement} presents experimental results for output predictions at different steps on the BTCV and AMOS datasets. The results show a gradual improvement in performance, starting from the initial prediction at Step 0~(UNet), followed by the second prediction after the first refinement at Step 1~(SAM 2), and finally the third prediction after the second refinement at Step 2~(SAM 2). 
Figure~\ref{fig:refinement_comparison} visualizes these comparisons across the three different steps. The results clearly demonstrate how the hole is progressively filled through the two refinements, highlighting the effectiveness of our model's refinement process.

\noindent \textbf{Object score.} 
% The object scores in SAM 2 play a crucial role in predicting the probability of an object’s presence in a given frame. SAM 2 employs a stricter labeling criterion for point prompts compared to SAM, such as no object~(-1), negative~\slash positive points~(0, 1), and box prompts~(2, 3). In the previous section, precise prompts contain precise labels, which include no object labels to represent there is no object at a certain frame. The object scores are learned to represent whether a prompt at a certain frame of a certain object is no object or point prompt. 
We experiment with three different baseline models: fine-tuning of SAM 2 with prompts, with learnable bounding boxes as the prompt generator, and with learnable masks as the prompt generator, both with and without the prediction of object scores. Table~\ref{tab:analysis_obj_score} shows that i) learning object scores with prompts does not significantly improve performance compared to using prompts without object scores, as the prompt itself indicates whether the object exists in a given frame. ii) The model with learnable bounding boxes benefits from learning object scores, as the accuracy of bounding box predictions is generally not high. iii) The model with learnable masks shows worse performance when learning object scores, as the probability distribution of the output predicted masks provides more accurate predictions. The object scores, which directly determine a single probability plane, can negatively impact this accuracy.

\begin{table}
    \vspace{-0.6cm}
    \centering
    \resizebox{0.68\linewidth}{!}{ %< auto-adjusts font size to fill line
    \begin{tabular}{@{}c|c|c|c}
    \toprule
     & (2, 1024, 1024) & (8, 512, 512)  & (32, 256, 256)  \\ \midrule
    
    DSC & 0.751 & 0.827 & 0.867  \\ 
    \bottomrule
    \end{tabular}} 
    \caption{Experiments for different input patch sizes on BTCV.}
    \label{tab:analysis_patch_size}
    % \vspace{-0.6cm}
\end{table}

\begin{table}
    \vspace{-0.2cm}
    \centering
    \resizebox{0.95\linewidth}{!}{ %< auto-adjusts font size to fill line
    \begin{tabular}{@{}c|c|c|c|c}
    \toprule
     & 3D UNet & 2D UNet  & 2D UNet + Attention & 3D UNet + Attention  \\ \midrule
    
    DSC & 0.825 & 0.807 & 0.805 & 0.815  \\ 
    \bottomrule
    \end{tabular}} 
    \caption{Experiments for different UNet models on BTCV dataset.}
    \label{tab:analysis_UNet}
    \vspace{-0.6cm}
\end{table}

\noindent \textbf{Input Patch Sizes and UNet Choices.} Table~\ref{tab:analysis_patch_size} shows the performance of different input patch sizes with the same number of pixels. Increasing the number of depth dimensions can bring benefits. Table~\ref{tab:analysis_UNet} illustrates different U-Net architectures. It shows 3D UNet is better than 2D UNet since the depth dimension can be learned. Involving attention blocks in the bottleneck can not bring benefits due to a strong inductive bias for medical image segmentation.
% \noindent \textbf{UNet Choices.}  

% \subsection{Analysis}

% %\noindent \textbf{Refinements.}
% Figure~\ref{fig:refinement_comparison} provides a comparison of output predictions from the U-Net (Step 0), Step 1, and Step 2 of SAM 2. The results highlight the progressive refinement achieved through each step.

% \begin{figure}[!hbtp]
% \centering
% %  \vspace{-0.4cm}
% \includegraphics[width=1\linewidth]{fig/refinement_comparison.png}
%       \caption{Comparisons of output predictions for Step 0, Step 1, and Step 2.}
% %\vspace{-0.4cm}
% \label{fig:refinement_comparison}
% \end{figure}









% \subsection{Ablation Study}
% \label{sec:ablation}
% \noindent\textbf{Baseline Models.} The proposed MaskSAM has 9 baselines (\textit{i.e.}, B1, B2, B3, B4, B5, B6, B7, B8, B9) as shown in Table \ref{tab:archAblation}. All baselines contain the entire SAM structure, a prompt generator, and a learnable class token. (i) B1 adopts the prompt generator that only generates learnable binary masks and uses the binary masks as mask prompts to calculate its bounding boxes as box prompts shown in Figure~\ref{fig:prompt_generator}(a). (ii) B2 adopts the prompt generator that only generates learnable boxes as the prompts shown in Figure~\ref{fig:prompt_generator}(b). (iii) B3 adopts the prompt generator that generates learnable binary masks as mask prompts and learnable boxes as box prompts shown in Figure~\ref{fig:prompt_generator}(c). (iv) B4 adopts the prompt generator that generates learnable binary masks as mask prompts and learnable boxes. We average the bounding boxes calculated from binary masks and the learnable boxes as the final box prompts shown in Figure~\ref{fig:prompt_generator}(d). (v) B5 adds depth positional embedding blocks~(DPosEmbed) in the image encoder and mask decoder based on B4. (vi) B6 modifies the vanilla adapter by inserting the invert-bottleneck depth MLPs with a skip connection after the fully-connected layers for upsampling based on B5. (vii) B7 modifies the vanilla adapter by inserting the invert-bottleneck depth MLPs with a skip connection before the fully-connected layers for downsampling based on B5. (viii) B8 replaces the vanilla adapter with our designed DMLPAdapter based on B5. (ix) B9 is our full model, named MaskSAM, illustrated in Figure~\ref{fig:arch}. B9 adopts the DMLPAdapter for prompt embeddings and the DConvAdapter for image embedding based on B8. 
% % (viii) B8 is our full model, named Self-Prompt-SAM, illustrated in Figure~\ref{fig:arch}. 
% % The results of the ablation study are shown in~Table \ref{tab:archAblation}.

% \begin{wraptable}{r}{9.0cm}
%     \vspace{-0.4cm}
%     \centering
%     \resizebox{0.99\linewidth}{!}{ %< auto-adjusts font size to fill line
%     \begin{tabular}{@{}cl|c}
%     \toprule
%     & Method & DSC $\uparrow$   \\ \midrule
%     B1 & SAM + MaskPG + vAdapter & 89.53 \\
%     B2 & SAM + BBoxPG + vAdapter & 88.78   \\ 
%     B3 & SAM + MaskBBoxPG + vAdapter & 90.08   \\
%     B4 & SAM + MaskAvgBBoxPG + vAdapter & 91.45   \\ 
%     B5 & SAM + MaskAvgBBoxPG + vAdapter + DPosEmbed & 91.61   \\ 
%     B6 & SAM + MaskAvgBBoxPG + vAdapter w/ D-MLP after FC-Up + DPosEmbed & 92.88   \\ 
%     B7 & SAM + MaskAvgBBoxPG + vAdapter w/ D-MLP before FC-Down + DPosEmbed & 92.93   \\ 
%     B8 & SAM + MaskAvgBBoxPG + DMLPAdapter + DPosEmbed & 93.10  \\ \hline
%     % B8 & SAM + MaskAverageBBoxPG + DMLPAdapter +  DConvAdapter & 92.20  \\ \hline
%     % B7 & SAM + MSPGenerator + DFusedAdapter + DPosEmbed + MAdapter + MC-Adapter & 93.04 & 90.07 \\ \hline
%     % B6 & LE-Em+hi-dwLMLP+skips & 91.49 & 90.07 & 88.98 & 95.41 \\ \hline
%     B9 & Our Full Model~(B8 + DConvAdapter) & \textbf{93.39}  \\
%     \bottomrule
%     \end{tabular}} 
%     \caption{Ablation studies of proposed methods on ACDC. \{\}PG means a prompt generator with different prompts. vAdapter means vanilla adapter. D-MLP means MLP layers on depth dimension. DPosEmbed means depth positional embedding.}
%     \label{tab:archAblation}
%     \vspace{-0.4cm}
% \end{wraptable}

% \noindent\textbf{Ablation analysis.} The results of the ablation study are shown in~Table \ref{tab:archAblation}. When we use our proposed MaskAvgBBoxPG to first generate auxiliary binary masks and auxiliary boxes, then average the bounding boxes calculated from auxiliary binary masks and learnable auxiliary boxes as the final box prompts, the model achieves the best results and improves by 1.9\%, 2.6\% and 1.3\% compared to B1 with a learnable mask prompt generator, B2 with a learnable box prompt generator, and B3 with a learnable mask and a learnable box prompt generator, respectively. The result confirms the effectiveness of the proposed prompt generator. When inserting depth positional embedding~(DPosEmbed) into the image encoder and mask decoder, the performance of B5 improves by more than 0.15\% compared to B4, demonstrating the effectiveness of DPosEmbed blocks. The DMLPAdapter~(B8) which we insert depth MLPs with a skip connection in the middle of the vanilla adapter achieves the best performance and improves by 0.2\% and 0.1\% compared to B6 in which we insert depth MLPs with a skip connection after fully-connected layers for upsampling and B7 in which depth MLPs are inserted with a skip connection after fully-connected layers for downsampling, respectively. The result confirms the effectiveness of the proposed DMLPAdapter. B9 is our full model, MaskSAM, utilizing the DMLPAdapter for prompt embeddings and the DConvAdapter that we replace the depth MLP with a 3D depth-wise convolution layer from the DMLPAdapter for image embedding based on B8 as shown in~ Figure \ref{fig:arch}. Compared to B8, our model brings about 0.3\% improvements. Therefore, the results demonstrate the effectiveness of our proposed MaskSAM. 
