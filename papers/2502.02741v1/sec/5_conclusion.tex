

\section{Conclusion}
\label{sec:conclusion}

In this paper, we introduced RFMedSAM 2, a novel framework for automatic prompt refinement that extends the SAM 2 pipeline to facilitate multiple refinement stages, enabling its adaptation for volumetric medical image segmentation. We explored two main branches to fully leverage SAM 2's potential. 

The first branch focused on evaluating the upper performance bound of SAM 2 when provided with accurate prompts. To enhance the model, we proposed depth-wise convolutional adapters (DWConvAdapters) for attention blocks involving image embeddings to capture spatial information, as well as CNN-Adapters for convolutional layers to enable efficient fine-tuning. With these adapters and optimized memory attention positioning, our model achieved a Dice Similarity Coefficient (DSC) of 92.30\%, surpassing the nnUNet by 12\% on the BTCV~\cite{landman2015miccai} dataset.

The second branch aimed to overcome the reliance on precise prompts by designing a module capable of generating accurate prompts automatically. Building on the insights gained from determining the upper bound and modifying SAM 2, we proposed an independent U-Net to predict masks and bounding boxes, which serve as input prompts for SAM 2. These prompts underwent two refinement stages within SAM 2, further enhancing performance. Our model achieves a DSC of 90.7\% and 86.7\% on the AMOS2022~\cite{ji2022amos} and BTCV~\cite{landman2015miccai} dataset, respectively.

Overall, our comprehensive approach demonstrates the effectiveness of RFMedSAM 2 in achieving state-of-the-art results in medical image segmentation.
For future work, we plan to explore the extension of RFMedSAM 2 to other types of medical imaging modalities, such as MRI and ultrasound, and investigate its potential for real-time clinical applications. 
%Additionally, further research could focus on optimizing the framework for more efficient deployment in resource-constrained environments.




% without any prompts provided. By designing a prompt generator combined with the image encoder in SAM to generate a set of auxiliary classifier tokens, auxiliary binary masks, and auxiliary bounding boxes. Each pair of auxiliary mask and box prompts, which can solve the requirements of extra prompts, is associated with class label predictions by the sum of the auxiliary classifier token and the learnable global classifier tokens in the mask decoder of SAM to solve the predictions of semantic labels.
% % By inserting learnable global classifier tokens that associates each pair of mask and box prompts with a \textit{single} global class label prediction into the mask decoder, our model achieves the functionality to predict semantic labels for predicted binary masks. 
% By inserting one of our redesigned 3D depth-convolution adapter~(DConvAdapter) for image embeddings and 3D depth-MLP adapter~(DMLPAdapter) for prompt embeddings into each transformer block in the image encoder and mask decoder, our model enables pre-trained 2D SAM models to extract 3D information and adapt to 3D medical images. Our method achieves state-of-the-art performance on AMOS2022~\cite{ji2022amos}, 90.52\% Dice, which improved by 2.7\% compared to nnUNet. Meanwhile, our method surpasses nnUNet by 1.7\% on ACDC~\cite{bernard2018deep} and 1.0\% on Synapse~\cite{landman2015miccai} datasets.
