
\section{Introduction}
\label{sec:intro}

\begin{figure}[!t]
\centering
%  \vspace{-0.2cm}
\includegraphics[width=0.99\linewidth]{fig/wholeArch.png}
      \caption{Overview of our proposed RFMedSAM 2.}
\vspace{-0.4cm}
\label{fig:wholeArch}
\end{figure}

Medical image segmentation is vital for biomedical analysis, aiding in disease diagnosis, anomaly detection, and surgical planning. Over recent years, deep learning-based approaches \cite{ronneberger2015u, isensee2019automated, hatamizadeh2022unetr, zhou2021nnformer} have significantly advanced segmentation tasks, with convolutional neural networks (CNNs) and vision transformers (ViTs) becoming the predominant architectures. However, medical imaging datasets often suffer from limited high-quality annotations, which hampers the training of large-scale models. Consequently, architectures with higher inductive biases, such as CNNs, have been more easily trained from scratch to achieve strong performance in medical segmentation tasks.

Foundation models \cite{devlin2018bert, he2022masked}, trained on vast datasets, have shown remarkable capabilities in zero-shot and few-shot generalization across a range of downstream applications \cite{openai2023gpt, radford2021learning}. These models have shifted the paradigm from training task-specific models to a "pre-training then fine-tuning" approach, significantly impacting the field of computer vision. The introduction of the Segment Anything Model (SAM) \cite{kirillov2023segment}, trained on the SA-1B dataset, marked a milestone in prompt-driven natural image segmentation. SAM's success extended to various applications, including medical image segmentation \cite{ma2024segment, xie2024masksam, zhang2024segment, deng2023sam, zhang2023customized, bui2024sam3d}.

Building on this, SAM 2 has been proposed as an enhancement over SAM, extending its functionality to both image and video domains. SAM 2 allows for real-time segmentation across video sequences using a single prompt. Table~\ref{tab:evaluate} shows that SAM 2 performs better than SAM on the BTCV dataset \cite{landman2015miccai}, achieving a Dice score of 82.77\% compared to SAM’s 81.89\%, motivating further exploration into SAM 2 for medical image segmentation tasks.

However, like SAM, SAM 2 has limitations, including its binary mask outputs, the absence of semantic label inference, and reliance on precise prompts for target object identification. Additionally, the performance of SAM and SAM 2 on medical segmentation tasks without modifications falls short of state-of-the-art models.

%\noindent \textbf{Contributions:} 
To address these challenges and maximize SAM 2’s potential for medical image segmentation, we make the following contributions:
%
\begin{itemize}[leftmargin=*]
\item We introduce RFMedSAM 2, an innovative framework for automatic prompt refinement in medical image segmentation that leverages the multi-stage refinement capabilities of SAM 2.
\item We develop novel adapter modules: a depth-wise convolutional adapter (DWConvAdapter) for attention blocks and a CNN-Adapter for convolutional layers, enhancing spatial information capture and enabling efficient fine-tuning.
\item We establish the upper performance bound of SAM 2 with optimal prompts, achieving a DSC of 92.30\% and surpassing the state-of-the-art nnUNet by 12\% on the BTCV~\cite{landman2015miccai} dataset.
\item We propose an independent UNet for generating masks and bounding boxes as inputs to SAM 2, enabling automatic prompt generation and dual-stage refinement that eliminates the reliance on manual prompts.
\item We perform extensive experiments on challenging medical image datasets (AMOS~\cite{ji2022amos} and BTCV~\cite{landman2015miccai}), demonstrating that RFMedSAM 2 achieves state-of-the-art results, surpassing nnUNet by 2.7\% on the AMOS2022 dataset and 6.4\% on the BTCV dataset.
\end{itemize}


\begin{figure*}[!t]
\centering
  \vspace{-0.6cm}
\includegraphics[width=0.95\textwidth]{fig/originalSAM2_old1.png}
      \caption{Overview of SAM 2. The pipeline includes steps for processing prompted and unprompted frames.}
\vspace{-0.2cm}
\label{fig:originalSAM2}
\end{figure*}


\section{Related Work}
\label{sec:related}

%\noindent \textbf{Medical Image Segmentation:} 
The field of medical image segmentation has evolved substantially, with traditional machine learning methods giving way to deep learning-based approaches. U-Net \cite{ronneberger2015u} set a new benchmark for medical image segmentation with its encoder-decoder structure and skip connections that help retain spatial context. Following this, nnUNet \cite{isensee2019automated} introduced an automated pipeline that adapts U-Net's architecture to different medical datasets, achieving consistent state-of-the-art results. More recently, transformer-based models, such as UNETR \cite{hatamizadeh2022unetr} and nnFormer \cite{zhou2021nnformer}, have been explored to capture global context and improve accuracy. These models leverage self-attention mechanisms, which help in modeling long-range dependencies, but they often require large datasets for effective training, posing a challenge due to the limited availability of annotated medical images.

%\noindent \textbf{Foundation Models and Transfer Learning:} 
Foundation models have transformed the landscape of machine learning by offering a robust starting point for a variety of downstream tasks. The "pre-training then fine-tuning" paradigm has been effective in both natural language processing and computer vision \cite{devlin2018bert, he2022masked}. These models are trained on large, diverse datasets to learn general representations that can be adapted to specific tasks with minimal additional training. This paradigm significantly reduces the reliance on large task-specific datasets and enables zero-shot and few-shot learning. SAM \cite{kirillov2023segment} epitomizes this approach for segmentation tasks by leveraging a pre-trained model that can adapt to new segmentation tasks via prompts. While SAM demonstrated strong zero-shot performance on natural images, its potential in specialized domains like medical imaging sparked interest and subsequent research.

%\noindent \textbf{Adaptations of SAM for Medical Imaging:} 
SAM has been extended and tailored for medical image segmentation in several studies. Works such as MedSAM \cite{ma2024segment}, MaskSAM \cite{xie2024masksam}, Self-Prompt SAM~\cite{xie2025selfpromptsammedicalimage}, and other adaptations \cite{zhang2024segment, deng2023sam} highlight the model's flexibility and the community's effort to harness its strengths for medical applications. These adaptations often involve fine-tuning SAM's prompt-encoding mechanisms or integrating domain-specific training strategies to better suit the complexities of medical images, which can include varied resolutions, noise, and non-standardized structures. Despite these advancements, SAM's original design limitations—such as its binary mask outputs and prompt dependency—persist, which restricts its standalone efficacy in comprehensive medical segmentation tasks.

%\noindent \textbf{Advances in Prompt-Driven Segmentation Models:} 
The concept of prompt-driven segmentation introduced by SAM has inspired the development of models that rely on external cues or prompts for segmentation. The approach aligns well with few-shot and zero-shot learning scenarios where annotations are sparse. SAM 2, an extension of SAM, incorporates improvements like memory attention and a memory encoder to process video sequences with greater consistency \cite{kirillov2023segment}. However, these innovations come with challenges, including prompt dependency and limited semantic understanding, which make them less optimal for fully automated medical segmentation tasks. Studies on prompt generation \cite{shaharabany2023autosam} and refinement have shown that integrating mechanisms for automatic prompt generation can reduce the reliance on manually provided prompts and enhance performance in more complex, real-world settings.

%\noindent \textbf{Challenges and Research Opportunities:} 
The reliance on accurate prompts in SAM 2 and other prompt-driven models presents a clear limitation, particularly in domains where precise annotations are challenging to obtain. Current research is exploring ways to mitigate this dependency, such as designing auxiliary models that can generate reliable prompts or integrating learning mechanisms that adaptively improve prompts during training. Furthermore, memory attention, while effective for maintaining temporal consistency in video segmentation, introduces complexity in terms of training and memory requirements. Addressing these challenges could enable SAM 2 and similar models to reach their full potential in medical image segmentation, bridging the gap between performance and practicality.





