\section{The Proposed Approach}
\label{sec:method}

In this section, we first review SAM and SAM 2. Then, we introduce the overall structure of our proposed automatic prompt refinement SAM 2~(RFMedSAM2). Detailed descriptions of each component in RFMedSAM2 can be found in the Appendix.






\subsection{Overview of SAM and SAM 2}
Segment Anything Model (SAM) has proven to be a robust prompt-based foundation model for image segmentation, showcasing strong zero-shot capabilities across various applications. Building on SAM's success, Segment Anything Model 2 (SAM 2) extends these capabilities to both image and video domains, enabling real-time segmentation of objects across entire video sequences using a single prompt.

%\noindent \textbf{Architectural Details:} 
Both SAM and SAM 2 share a core structure comprising an image encoder, a prompt encoder, and a mask decoder. The image encoder processes input images to generate image embeddings, while the prompt encoder handles input prompts in the form of points, bounding boxes, or masks. The mask decoder then combines image and prompt embeddings to produce binary segmentation masks. SAM employs a Vision Transformer as the backbone of its image encoder, whereas SAM 2 utilizes Hiera~\cite{ryali2023hiera} for enhanced feature representation. SAM 2 also introduces a memory attention module that conditions current frame features on past frames and object pointers, along with a memory encoder that fuses current frame features with output masks to generate memory features.


%\noindent \textbf{Pipeline Procedure:} 
The SAM 2 pipeline consists of two main stages: the Prompted Frame Processing stage and the Unprompted Frame Processing stage, as illustrated in Figure~\ref{fig:originalSAM2}.
%
In the prompted frame processing stage, SAM 2 processes frames that contain explicit prompts. Each frame is handled independently, with the prompt guiding the segmentation process. This stage also expands the batch size to match the number of objects expected, ensuring that the output includes masks for each object in the frame. The results from this stage include predicted masks and object pointers, which are passed to the memory encoder to generate memory features.
%
The unprompted frame processing stage handles frames that do not have explicit prompts. The memory attention module leverages information from previous and prompted frames to build context for segmenting the current frame. In this stage, the prompted frames are assigned a temporal position of 0, while unprompted frames are given temporal positions up to 6, with closer frames having higher temporal positions. This approach helps establish effective context for segmentation, although the original design can struggle with maintaining accurate temporal positioning, potentially leading to errors.

%\noindent \textbf{Performance Summary:} 
Table~\ref{tab:evaluate} summarizes the performance of SAM and SAM 2 under different settings on the BTCV dataset. The results indicate that SAM 2 outperforms SAM when bounding box prompts are used for each frame, achieving a higher Dice score. This demonstrates the advantage of SAM 2’s enhanced architecture and memory attention capabilities for video segmentation tasks.

\begin{table*}[t]
   \vspace{-0.6cm}
    \centering
    \resizebox{0.99\textwidth}{!}{ %< auto-adjusts font size to fill line
    \begin{tabular}{@{}cl|c|c|c|c|c|c|c|c|c|c|c|c|c|c@{}}
    \toprule
    & Prompt & \multicolumn{7}{c|}{Bounding boxes as prompts} & \multicolumn{7}{c}{Central points as prompts} \\ \hline
    & Method & \multicolumn{1}{c|}{SAM} & \multicolumn{6}{c|}{SAM 2} & \multicolumn{1}{c|}{SAM} & \multicolumn{6}{c}{SAM 2}  \\ \hline
    & \# frames \slash ~class & \multicolumn{1}{c|}{All} & \multicolumn{2}{c|}{All} & \multicolumn{2}{c|}{Two} & \multicolumn{2}{c|}{One} & \multicolumn{1}{c|}{All} & \multicolumn{2}{c|}{All} & \multicolumn{2}{c|}{Two} & \multicolumn{2}{c}{One}  \\ \hline
    & Frames for Step 2 & -- & All & Unprompted & All & Unprompted& All & Unprompted & -- & All & Unprompted & All & Unprompted & All & Unprompted    \\ \hline
    & DSC~(\%) & 81.89 & 81.17 & 82.77 & 68.75 & 68.03 & 45.00 & 44.07 & 8.86 & 3.81 & 4.90 & 2.11 & 3.43 & 2.53 & 4.59 \\
    \bottomrule
    \end{tabular}}
    \caption{Performance evaluation of SAM and SAM 2 with different prompt settings on the BTCV dataset.}
    \label{tab:evaluate}
   \vspace{-0.2cm}
\end{table*}






\subsection{Analysis and Insights}
SAM 2 offers notable advantages but also has inherent limitations. This section provides a detailed analysis of these aspects. Table~\ref{tab:evaluate} summarizes experiments with various settings for SAM and SAM 2 on the BTCV dataset. All prompts used in these experiments are derived from ground truth, and the models are evaluated without any structural modifications.

1) Bounding box prompts vs. central points: As shown in Table~\ref{tab:evaluate}, the use of central point prompts results in less than 10\% Dice for both SAM and SAM 2. When bounding box prompts are used, the performance significantly improves. For this reason, bounding boxes are used as prompts in subsequent experiments.

2) Per-frame prompts: The results indicate that SAM 2 performs best (82.77\% Dice) when each frame contains a bounding box for every object, underscoring the importance of per-frame prompts for optimal accuracy.

3) Comparison between SAM and SAM 2: With per-frame prompts, SAM achieves a Dice score of 81.89\%, while SAM 2 reaches 82.77\%, demonstrating SAM 2's improved performance over SAM.

4) Step 2 for refinement: Step 2 in SAM 2, which leverages memory attention for unprompted frames, can be extended to all frames for refinement purposes. Forcing Step 2 on all frames results in a slight drop in the Dice score from 82.77\% to 81.17\%, but it shows potential for refining segmentation results. 
%This is illustrated in Figure~\ref{fig:refinement}(a), where the green area is refined for better accuracy. However, Figure~\ref{fig:refinement}(b) highlights a limitation: assigning all prompted frames a temporal position of 0 leads to incorrect temporal positioning and false positives in unrelated frames. This issue needs to be addressed to avoid such errors.

5) Streaming operation: Most of SAM 2's modules, except for the memory attention, process images individually without involving temporal operations, which reduces memory usage. The memory attention module stacks features from previous and prompted frames to build connections with the current frame. This method is both efficient and effective, so we maintain this streaming operation in our approach, as shown in Figure~\ref{fig:motivate}.

%\begin{figure}[!hbtp]
%\centering
%  \vspace{-0.4cm}
%\includegraphics[width=1\linewidth]{fig/refinement.png}
%      \caption{(a) Benefits of refinement through Step 2. (b) Example of false predictions due to incorrect temporal positioning.}
%\vspace{-0.4cm}
%\label{fig:refinement}
%\end{figure}

\begin{figure*}[!t]
\centering
 \vspace{-0.6cm}
\includegraphics[width=1\textwidth]{fig/motivate.png}
      \caption{(1) Performance comparisons based on proposed methods. (2) Ablation studies for frame selection strategies. (3) Proposed Adapters. (4) Ablation studies for prompt generators.}
\vspace{-0.4cm}
\label{fig:motivate}
\end{figure*}






\subsection{RFMedSAM 2 Architecture}

\subsubsection{Architecture Overview}

Figure~\ref{fig:wholeArch} illustrates the overall architecture of RFMedSAM 2, comprising three primary stages:

\begin{itemize}
\item Initial Prediction Stage: A U-Net model processes the medical images, generating initial multi-class mask predictions. These predictions are converted into bounding boxes to serve as prompts for the next stage.
\item Preliminary Segmentation Stage: The modified image encoder produces image embeddings from the input images, while the prompt encoder converts auxiliary bounding boxes into point embeddings. The mask decoder uses these embeddings to generate initial masks and object pointers. The generated masks are utilized to create new bounding boxes, and the modified memory encoder processes these masks along with current frame features to produce memory features, enabling initial refinement.
\item Refinement Stage: The modified memory attention module takes image features from the encoder and builds relationships with memory features from previous frames. The mask decoder processes these outputs along with new point embeddings from the prompt encoder, producing refined predictions as the final output.
\end{itemize}

\subsubsection{SAM 2 Modifications}

The RFMedSAM 2 design incorporates several key modifications to the SAM 2 architecture:

\begin{itemize}
\item Modified Image Encoder: 
%Figure~\ref{fig:wholeArch}(b) highlights the changes in the image encoder. 
To align various medical imaging modalities with the RGB input format expected by SAM, a sequence of two stacked convolutional layers is added to adapt input modalities. The Hiera~\cite{ryali2023hiera} backbone includes DWConvAdapters in its attention blocks and CNN-Adapters in the FPN module to enhance adaptation.

\item Modified Mask Decoder:
%Figure~\ref{fig:wholeArch}(c) shows the updates in the mask decoder.
The mask decoder includes adapters after self- and cross-attention blocks and in parallel with MLP layers to capture spatial information more effectively. DWConvAdapters facilitate spatial learning, while CNN-Adapters adapt convolutional layers for medical image processing.

\item UNet, Memory Encoder, and Memory Attention:
%Figures~\ref{fig:wholeArch}(d) and (e) illustrate these components. 
The U-Net maintains a symmetric encoder-decoder structure with skip connections for better spatial detail retention. The memory encoder integrates CNN-Adapters to adapt its components for processing medical image features. The memory attention module incorporates DWConvAdapters within its transformer blocks to process spatial information effectively.
\end{itemize}

%\subsection{Impact of Combined Loss Functions}
%
%We explored the interaction between auxiliary losses and final output losses during training. The training process was split into two phases: an initial phase updating parameters based solely on auxiliary losses and a subsequent phase combining auxiliary and final output losses. Detailed results, presented in the Appendix, show that auxiliary losses initially decrease but start increasing once the combined loss phase begins, highlighting the complex interactions between these loss types.



\subsection{Architectural Design}

In this section, we present our architectural advancements for SAM 2 aimed at enhancing its performance in medical image segmentation. Our main focus is on designing an improved memory attention strategy and novel adapters to maximize SAM 2's segmentation capabilities. While ground-truth (GT) prompts are used to explore the upper bound of performance, the key contributions lie in the architectural modifications that support robust fine-tuning and improved adaptability.


\subsubsection{Refined Frame Selection Strategy}
Step 2 in SAM 2, originally used for processing unprompted frames in memory attention, is a crucial refinement step for enhancing segmentation consistency. Our goal was to extend this step to process all frames and refine predictions across the entire sequence. In the original design, SAM 2 assigns a temporal position of 0 to all prompted frames, leading to ambiguous temporal positioning and the potential for false positives when attention is applied across frames.

To optimize the memory attention strategy, we experimented with different frame selection methods and temporal position assignments using a baseline model. Figure~\ref{fig:motivate}(2) illustrates four strategies. The original strategy (Figure~\ref{fig:motivate}(2a)) achieved a Dice Similarity Coefficient (DSC) of 90.74\%, which was outperformed by a simpler approach using only Step 1, indicating limitations due to incorrect temporal positioning.

Our improved strategy, shown in Figure~\ref{fig:motivate}(2b), assigns the current frame index as the temporal position of 0, ensuring that the model prioritizes memory features of the current frame that include Step 1 mask predictions. This strategy significantly enhances performance, achieving a DSC of 91.58\%. Alternative strategies that select both forward and backward frames (Figures~\ref{fig:motivate}(2c) and \ref{fig:motivate}(2d)) resulted in either performance drops or increased memory requirements.

%\textit{
We adopt the frame selection method from Figure~\ref{fig:motivate}(2b), selecting up to 6 previous frames and setting the current frame index as the temporal position of 0. This approach ensures comprehensive memory integration for robust segmentation refinement.
%}


\subsubsection{Design of Novel Adapters for Enhanced Fine-Tuning}
To enable parameter-efficient fine-tuning while retaining SAM 2's zero-shot capabilities, we designed new adaptation mechanisms that enhance spatial and convolutional processing within SAM 2’s architecture:
%
\begin{itemize}
\item Depth-wise Convolutional Adapter (DWConvAdapter): The image encoder, memory attention, and mask decoder contain attention blocks that process image embeddings with rich spatial information. To strengthen this, we introduce the DWConvAdapter (Figure~\ref{fig:motivate}(3b)), which incorporates depth-wise convolutions to capture spatial context effectively. Integrating DWConvAdapters improved the DSC by 0.47\%, demonstrating its utility in enhancing spatial learning.

\item CNN-Adapter for Convolutional Layers: Given the presence of multiple convolutional layers in SAM 2, we also developed a CNN-Adapter to facilitate better adaptation within these layers (Figure~\ref{fig:motivate}(3b)). The addition of CNN-Adapters led to a DSC increase of 0.25\%, further validating the effectiveness of targeted architectural modifications.
\end{itemize}

%\textit{
Our final model incorporates original adapters for point embedding attention blocks, DWConvAdapters for image embedding attention blocks, and CNN-Adapters for convolutional layers. This comprehensive architecture allows SAM 2 to achieve over a 4\% improvement compared to state-of-the-art methods, as shown in Table~\ref{tab:amos}, establishing its capability for advanced medical image segmentation.
%}

\begin{table*}[!t]\small
    \setlength{\tabcolsep}{3pt}
    \centering
    \vspace{-0.6cm}
    \resizebox{1\linewidth}{!}{ %< auto-adjusts font size to fill line
    \begin{tabular}{@{}c|c|l|ccccccccccccccc|c@{}}
    \toprule
    Semantic labels & Prompts & Method & Spl. & R.Kd & L.Kd & GB & Eso. & Liver & Stom. & Aorta & IVC  & Panc. & RAG & LAG & Duo. & Blad. &  Pros. & Average \\
    \midrule
    & & TransBTS \cite{wang2021transbts} & 0.885 & 0.931 & 0.916 & 0.817 & 0.744 & 0.969 & 0.837 & 0.914 & 0.855 & 0.724 & 0.630 & 0.566 & 0.704 & 0.741 & 0.650 & 0.792 \\
    & & UNETR \cite{hatamizadeh2022unetr} & 0.926 & 0.936 & 0.918 & 0.785 & 0.702 & 0.969 & 0.788 & 0.893 & 0.828 & 0.732 & 0.717 & 0.554 & 0.658 & 0.683 & 0.722 & 0.762 \\
    \CheckmarkBold & -- & nnFormer \cite{zhou2021nnformer} & 0.935 & 0.904 & 0.887 & 0.836 & 0.712 & 0.964 & 0.798 & 0.901 & 0.821 & 0.734 & 0.665 & 0.587 & 0.641 & 0.744 & 0.714 & 0.790 \\
    & & SwinUNETR \cite{hatamizadeh2021swin} & 0.959 & 0.960 & 0.949 & \textbf{0.894} & 0.827 & 0.979 & 0.899 & 0.944 & 0.899 & 0.828 & \textbf{0.791} & 0.745 & 0.817 & 0.875 & 0.841 & 0.880 \\
    & & nn-UNet~\cite{isensee2019automated} & 0.965 & 0.959 & 0.951 & 0.889 & 0.820 & \textbf{0.980} & 0.890 & 0.948 & 0.901 & 0.821 & 0.785 & 0.739 & 0.806 & 0.869 & 0.839 & 0.878 \\
    \hline
    \XSolidBrush & nnUNet & SAM~\cite{kirillov2023segment} bbox & 0.679 & 0.741 & 0.640 & 0.168 & 0.443 & 0.773 & 0.671 & 0.651 & 0.554 & 0.434 & 0.232 & 0.324 & 0.444 & 0.698 & 0.602 & 0.538 \\
    \XSolidBrush & nnUNet & SAM 2~\cite{ravi2024sam} bbox & 0.784 & 0.817 & 0.819 & 0.664 & 0.734 & 0.780 & 0.697 & 0.793 & 0.739 & 0.536 & 0.457 & 0.604 & 0.563 & 0.744 & 0.691 & 0.695 \\
    \XSolidBrush & nnUNet & MedSAM~\cite{ma2024segment} bbox & 0.714 & 0.811 & 0.702 & 0.193 & 0.469 & 0.759 & 0.725 & 0.701 & 0.681 & 0.434 & 0.365 & 0.412 & 0.462 & 0.783 & 0.758 & 0.600 \\
    \CheckmarkBold & No needs & SAMed~\cite{zhang2023customized} & 0.849 & 0.857 & 0.830 & 0.573 & 0.733 & 0.894 & 0.816 & 0.855 & 0.784 & 0.727 & 0.622 & 0.683 & 0.701 & 0.844 & 0.819 & 0.772 \\
    \CheckmarkBold & No needs & SAM3D~\cite{bui2024sam3d} & 0.796 & 0.863 & 0.871 & 0.428 & 0.711 & 0.908 & 0.833 & 0.878 & 0.749 & 0.699 & 0.564 & 0.607 & 0.635 & 0.884 & 0.840 & 0.751 \\
    \hline
    \hline
    % \rowcolor{grey!20}{\CheckmarkBold & No needs & RFMedSAM 2   & 0.964 & \textbf{0.964} & \textbf{0.966} & 0.875 & \textbf{0.872} & 0.978 & \textbf{0.928} & \textbf{0.952} & \textbf{0.914} & \textbf{0.878} & 0.763 & \textbf{0.790} & \textbf{0.839} & \textbf{0.896} & \textbf{0.842} & \textbf{0.895}} \\
    \CheckmarkBold & No needs & RFMedSAM 2   & \textbf{0.972} & \textbf{0.971} & \textbf{0.966} & 0.887 & \textbf{0.878} & \textbf{0.980} & \textbf{0.943} & \textbf{0.958} & \textbf{0.925} & \textbf{0.896} & 0.781 & \textbf{0.811} & \textbf{0.853} & \textbf{0.921} & \textbf{0.859} & \textbf{0.907} \\
    \bottomrule
    \end{tabular}
    }
    \caption{Comparison of RFMedSAM 2 with SOTA methods on the AMOS testing dataset, evaluated using Dice Score. All results are based on 5-fold cross-validation without ensemble techniques. ``Semantic labels'' indicate the model's ability to infer semantic labels, while ``Prompts'' specify the prompt source. The best results are shown in \textbf{bold}.}
    \label{tab:amos}
\vspace{-0.4cm}
\end{table*}




\subsection{Advancing Prompt Generation}

After exploring the upper performance limit of SAM 2 with accurate ground truth (GT) prompts, the next step was to develop a practical solution that removes reliance on such precise prompts, which are unrealistic for real-world medical image segmentation. Recognizing that SAM 2 can achieve exceptional performance with accurate prompts, we proposed a prompt generation framework that refines both generated prompts and final predictions during training. Six distinct blocks for automatic prompt generation were designed, as shown in Figure~\ref{fig:motivate}(4a)-(4f), categorized into two main types: learnable point coordinate representations (Figures~\ref{fig:motivate}(4a)-(4c)) and learnable masks (Figures~\ref{fig:motivate}(4d)-(4f)). Performance results for these blocks are shown in the last six bars of Figure~\ref{fig:motivate}(1).

\subsubsection{Learnable Point Coordinate Representations} 
The block depicted in Figure~\ref{fig:motivate}(4a) initializes object queries for each class, which are processed through a series of self-attention and cross-attention blocks that interact with the current image features. Multiple MLP layers are used to adjust the embedding dimensions for generating box coordinates and object scores. SAM 2 employs stricter label criteria for point prompts than its predecessor, using labels such as no object~(-1), negative/positive points~(0, 1), and box prompts~(2, 3). In prior experiments, GT prompts included labels to denote the absence of an object at specific frames. In the current approach, object scores are trained to indicate whether a given frame should contain a prompt or none at all.

Despite these efforts, the block in Figure~\ref{fig:motivate}(4a) only reached a DSC of 77.35\%, indicating a significant performance gap. Integrating object scores from the mask decoder, as seen in Figure~\ref{fig:motivate}(4b), increased performance by 1.9\%, but the results remained below expectations. To bypass the challenges of label representation, we designed a learnable point embedding block, allowing it to learn coordinate and label representations directly (Figure~\ref{fig:motivate}(4c)). However, this approach resulted in an 11\% drop in DSC, highlighting the difficulty of learning precise prompts.% from scratch.

Accurate coordinate prediction, essential for bounding box prompts, proved challenging due to the non-coordinate-encoded nature of image embeddings and the random initialization of embeddings. Additionally, bounding boxes lacked the semantic richness necessary for effective multi-class segmentation. This led us to pivot towards using learnable masks, which offer more robust semantic information.

\subsubsection{Learnable Masks} 
We found that predicting masks first and deriving bounding boxes from them provided a more reliable approach than directly predicting coordinates. The structure illustrated in Figure~\ref{fig:motivate}(4d) incorporates a hierarchical design of convolutional layers combined with multi-level features from the image encoder. Starting with lower-resolution features, the model progressively increases resolution through convolutional layers, combining them with higher-resolution features. Auxiliary loss functions supervise the generated masks by comparing them to the ground truth, achieving a DSC of 84.93\%. Although this was an improvement, it fell short of top-tier performance.

One challenge was that both auxiliary losses from the generated masks and final output losses from SAM 2 impacted updates to the image encoder, leading to conflicts that hindered optimal training. The distinct architectures between the prompt generator and SAM 2 complicated synchronized parameter updates, making it difficult to maintain balance and achieve consistent improvements.

To overcome this, we introduced an independent U-Net architecture alongside SAM 2 to generate masks that do not interfere with SAM 2’s parameter updates (Figure~\ref{fig:motivate}(4e)). This U-Net-generated mask was used to derive bounding boxes as input prompts for SAM 2, raising the performance to 85.38\%. To further enhance the interaction between the U-Net and SAM 2, we routed the masks and bounding boxes directly into the first step of SAM 2, enabling the prediction of a refined set of masks and updated bounding boxes. These were subsequently fed into the second step for further refinement, resulting in an overall performance of 86.48\% DSC.

This multi-stage prompt generation and refinement pipeline significantly reduces reliance on precise GT prompts and emphasizes the model's capacity for self-sufficient prompt generation in realistic medical imaging scenarios.







% i) In the attention modules, DWConvAdapter blocks are inserted after the second cross-attention block, while original adapters are inserted after the remaining attention blocks and parallel to the MLP layers. ii) We design learnable global classifier tokens that sum auxiliary classifier tokens from the prompt generator and concatenate them with sparse prompt embeddings and original mask tokens to equip the model with semantic label prediction capabilities. iii) The mask decoder includes a positional embedding. To enhance depth information learning, a learnable depth positional embedding is incorporated alongside the original image positional embedding. iv) The mask encoder applies self-attention and cross-attention in sequence, with DWConvAdapters inserted after each. This helps the model learn spatial information and depth information for both image and prompt embeddings.

% \clearpage

% Ensure your IEEE copyright release form is signed and submitted with your final paper for publication.
% Contact the IEEE Computer Society Press for any questions: \url{https://www.computer.org/about/contact}.










