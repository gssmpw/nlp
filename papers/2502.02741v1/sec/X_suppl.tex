\clearpage
\setcounter{page}{1}
\maketitlesupplementary

\section{Intrinsic issues of SAM2}
Figure~\ref{fig:originalSAM2} illustrates the whole pipeline of SAM 2, highlighting several intrinsic issues for medical image segmentation. 

i) \textbf{Omission to predict the first few frames:} The first frame in two objects is the second frame, therefore, SAM 2 begins processing from the second frame, disregarding the first frame, even though it contains both objects.

ii) \textbf{Empty prompt affecting object prediction:} When no prompt is provided for an object, but the object is still present, the empty prompt restricts prediction for that object. For instance, at frames $z=1$ and $z=4$, the purple and red objects, respectively, are omitted from the predictions. 

iii) \textbf{Confusion of temporal positions:} All prompted frames are assigned a temporal position of 0. While this approach increases attention to the prompted frames, it loses the relative temporal positioning of all the prompt frames. Moreover, since SAM 2 skips over prompted frames, the relative temporal positions of the unprompted frames are distorted. 
For example, the real relative temporal position of the frame $z=3$ with respect to the current frame $z=6$ should be 3, but due to the prompted frame at $z=4$, the relative temporal position is incorrectly assigned as 2.

\section{Potentials to force Step 2 for all frames.}
 When we provide prompts at each frame for each class, SAM 2 does not process Step 2 and does not leverage the capabilities of Memory Attention, which can build relations with previous frames and prompted frames. To explore this functionality, we force Step 2 for all frames after processing Step 1.  
\begin{figure}[!hbtp]
\centering
  \vspace{-0.2cm}
\includegraphics[width=1\linewidth]{fig/refinement.png}
      \caption{Benefits for refinement by Step 2.} 
% \textcolor{red}{[include more detailed descriptions.]}}
\vspace{-0.2cm}
\label{fig:refinement}
\end{figure}
Although the results decreased slightly from 82.77\% to 81.17\% Dice, we find a potential refinement benefit illustrated in Figure~\ref{fig:refinement}(a). Through Step 2, the green area is refined and becomes more accurate, demonstrating the significant potential of the refinement process. As a result, we plan to incorporate this approach into our method. However, the refinement introduced by Step 2 also has some drawbacks. In Figure~\ref{fig:refinement}(b), we show orthogonal planes in relation to the axial plane~(a sequence of the axial plane images is fed to SAM 2). The top portion of Figure~\ref{fig:refinement}(b) presents unexpected predictions. 
% We applied Step 2 to all frames, and the axial frames in this top area are unprompted. 
Since SAM 2 assigns a temporal position of 0 to the prompted frames, which are always involved in memory attention, the incorrect relative temporal positioning leads to these unexpected and incorrect predictions. We will address this issue in the next section.

\begin{figure*}[!t]
\centering
  % \vspace{-0.2cm}
\includegraphics[width=1\linewidth]{fig/wholeArch_old1.png}
      \caption{Details of the whole architecture of RFMedSAM 2.} 
% \textcolor{red}{[include more detailed descriptions.]}}
\vspace{-0.6cm}
\label{fig:wholeArch_details}
\end{figure*}

\section{Motivation Behind the Designed Adapters.} 
Since the image encoder, the memory attention, and the mask decoder contain attention blocks for image embedding, which includes significant spatial information. Therefore, we design the depth-wise convolutional adaption~(DWConvAdapter) illustrated in Figure~\ref{fig:motivate}(3b) to learn spatial information. After using DWConvAdapters for the attention blocks with image embedding, the performance increases by 0.47\%. The motivation behind the DWConvAdapter design is to extend the original adapter by incorporating a depth-wise convolution layer, followed by layer normalization and a GeLU activation function, to effectively learn spatial information. A parallel skip connection is included to preserve the original structure. In the worst case, where the depth-wise layer learns nothing~(\textit{i.e.,} its output is zero), the skip connection ensures that all original information is retained. 
Building on this concept, we designed the CNN-Adapter for adapting convolutional layers since more convolutional layers are involved at SAM 2 compared to SAM. The CNN-Adapter uses a point-wise convolutional layer to downsample the channel dimension, reducing complexity, followed by a depth-wise convolutional layer to capture spatial dimensions. Finally, a point-wise convolutional layer recovers the channel dimension to its original size. Inspired by ConvNext, we use only layer normalization and a GeLU activation function in this block. The bottleneck structure helps reduce complexity, and a parallel skip connection ensures that the output from the convolutional layers in SAM 2 is preserved. In the worst case, where the depth-wise layer learns nothing (\textit{i.e.,} its output is zero), the skip connection still retains all relevant information.

\section{Architecture of RFMedSAM 2}
Figure~\ref{fig:wholeArch_details}(a) illustrates the overall pipeline and architecture of RFMedSAM 2, which consists of three primary steps. In Step 0, an additional UNet model is employed to take medical images as input, generating initial multi-class mask predictions, which are then used to create auxiliary bounding boxes for the prompt requirements of SAM 2.
In Step 1, the medical images being input are involved into a modified image encoder to produce image embeddings, while the prompt encoder processes the auxiliary bounding boxes to generate point embeddings. These embeddings are passed to the modified mask decoder to generate masks and object pointers. The generated masks are then employed to create second bounding boxes for Step 2. A modified memory encoder processes both the generated masks and current frame features to produce memory features for the next step. Step 2 presents the second prediction by refining the initial predictions and performing the first refinement.
% also takes the medical images as input, which are fed into the modified image encoder to generate image embeddings. The prompt encoder takes the auxiliary bounding boxes as input to generate point embeddings. Finally, the modified mask decoder takes the image embeddings and point embeddings as the input to generate masks and object pointers. 
% We use the generated masks for the second bounding boxes for the next Step 2. Additionally, the modified memory encoder takes the generated masks and the current frame features as the input to generate the memory features for the next Step 2.
% Step 2 presents the second predictions and the first refinement. 
In Step 3, the same image features from the modified image encoder are input into a modified memory attention module, which establishes relationships with memory features from previous frames. The output from this memory attention mechanism is fed into the modified mask decoder, while the memory decoder also processes new point embeddings from the prompt encoder. Step 3 generates the third set of predictions and the second refinement, with the final mask prediction being output by the mask decoder.  
Figure~\ref{fig:wholeArch_details}(b)-(e) illustrates each component of RFMedSAM 2, described as follows. 

\subsection{Modified Image Encoder}
Figure~\ref{fig:wholeArch_details}(b) illustrates the redesigned image encoder. i) SAM works on natural images that have 3 channels for RGB while medical images have varied modalities as channels. There are gaps between the varied modalities of medical images and the RGB channels of natural images. 
Therefore, we design a sequence of two stacked convolutional layers to an invert-bottleneck architecture to learn the adaption from the varied modalities with any size to 3 channels. 
ii) SAM 2 employs Hiera~\cite{ryali2023hiera} that is hierarchical with multiscale output features as its image encoder backbone and a FPN module. Hiera consists of four stages with different feature resolutions and every stage contains various number of attention blocks. We insert our designed DWConvAdapter blocks into each attention block in Hiera. The output of each stage will be connected with one convolution in the FPN module. The latest output feature is up-sampled and summed with the second latest output feature as the image embedding. The third and fourth latest output feature are as skip connections to to incorporate high-resolution embeddings for the mask decoding. To adapt these convolutional layers, we insert our designed CNN-Adapters for the output features from the FPN module. 

\begin{figure*}[!t]
\centering
  \vspace{-0.2cm}
\includegraphics[width=1\linewidth]{fig/moreRefinement.png}
      \caption{More visualization of two refinements.} 
% \textcolor{red}{[include more detailed descriptions.]}}
\vspace{-0.2cm}
\label{fig:morerefinement}
\end{figure*}

\subsection{Modified Mask Encoder}
Figure~\ref{fig:wholeArch_details}(c) illustrates the redesigned mask encoder. The mask encoder contains two subsequent transformers and two following convolutional layers. i) Each transformer first applies self-attention to the prompt embedding. We insert an adapter behind the self-attention. Then, a cross-attention block is adopted for tokens attending to image embedding. We insert an adapter behind the cross-attention. Next, we insert a adapter parallel to an MLP block. Finally, a cross-attention block is utilized for image embedding attending to tokens. We insert a DWConvAdapter behind the cross-attention. In this way, our model can learn the spatial information for the image embedding and adapt information for the prompt embedding. 
ii) We inserted a CNN-Adapter behind the two following convolution layers to adapt convolutional layers from natural images to medical images. 

\subsection{UNet, Modified Memory Encoder and Modified Memory Attention.}
Figure~\ref{fig:wholeArch_details}(d) and (e) illustrate the UNet and the redesigned memory encoder, respectively. 
i) UNet is designed with a symmetrical encoder-decoder structure with skip connections. The encoder consists of several stages, each formed by a sequence of convolutional layers followed by down-sampling layers, progressively increasing the number of channels while reducing the spatial resolution to capture different deep-level features. The decoder upsamples the feature maps using transposed convolutions to restore spatial resolution and refine predictions. Skip connections between corresponding encoder and decoder layers enable the network to retain fine-grained spatial details, enhancing localization accuracy.  
ii) The memory encoder comprises two modules: the mask downsampler, which processes predicted masks, and the fuser, which integrates image features and mask features. To adapt these CNN-based modules to medical images, a CNN-Adapter is inserted after each module.
iii) The memory attention module stacks several transformer blocks, the first one taking the image encoding from the current frame as input. Each block performs self-attention, followed by cross-attention to memory features. Therefore, we inserted our designed DWConvAdapter blocks into each attention block since the transformer blocks process the image embedding with the spatial dimension. 


\section{Impact of Auxiliary Losses on Image Encoder Parameter Updates if Prompt Generator Built with Image Encoder}
Figure~\ref{fig:motivate}(4d) illustrates a hierarchical structure with convolutional layers combined with multi-level features from the image encoder. The features with a lower resolution gradually increase the resolution by convolution layers and then combined with higher resolution features. Auxiliary loss functions are employed to supervise between the predicted masks and the ground truth. Although this approach achieves a DSC of 84.93\%, the result is not competitive. During training, both the auxiliary losses from the generated masks and the final output losses from SAM 2 influence the update of the image encoder parameters, which constitute a significant portion of the model. However, these two types of losses, due to their distinct architectural differences, are challenging to optimize simultaneously and achieve a balanced update for the image encoder parameters. 

\begin{figure}[!hbtp]
\centering
  \vspace{-0.2cm}
\includegraphics[width=1\linewidth]{fig/twolosses_1.png}
      \caption{Oscillated losses if prompt generator built with image encoder.} 
% \textcolor{red}{[include more detailed descriptions.]}}
\vspace{-0.2cm}
\label{fig:twolosses_1}
\end{figure}

% \begin{figure}[!hbtp]
% \centering
%   \vspace{-0.2cm}
% \includegraphics[width=1\linewidth]{fig/twolosses_2.png}
%       \caption{Benefits for refinement by Step 2.} 
% % \textcolor{red}{[include more detailed descriptions.]}}
% \vspace{-0.2cm}
% \label{fig:twolosses_2}
% \end{figure}

We conduct experiments to validate the insights presented in Figure~\ref{fig:twolosses_1}. The training process is divided into two phases: one phase updates the parameters based solely on the auxiliary losses supervised by the auxiliary loss function, while the other phase updates all parameters based on both the auxiliary loss function and the final output loss function. The results indicate that after the second phase begins, the validation loss oscillates and is in an unstable state shown in the red line. The dice of the auxiliary masks present an unstable state since the final output losses affect the update of the image encoder and then affect the accuracy of the auxiliary masks. 

In conclusion, using a prompt generator built with the image encoder creates a challenge in balancing the update of the image encoder's parameters. As a result, we abandon this approach and instead employ an independent U-Net to generate masks and subsequently produce the corresponding bounding boxes. 

% the auxiliary losses stop decreasing and begin to increase, continuing until the end of the training process. This demonstrates that when a prompt generator is built with the image encoder supervised by auxiliary losses, it becomes challenging to balance the updating of the image encoder's parameters, which ultimately affects the optimal performance of the final output masks.

\section{Implementation Details}
We utilize some data augmentations such as rotation, scaling, Gaussian noise, Gaussian blur, brightness, and contrast adjustment, simulation of low resolution, gamma augmentation, and mirroring. We set the initial learning rate to 0.001 and employ a ``poly'' decay strategy in Eq.~\eqref{equa:polydecay}.
\begin{equation}
    lr(e)= init\_lr \times (1 - \frac{e}{\rm MAX\_EPOCH})^{0.9},
\label{equa:polydecay}
\end{equation}
where $e$ means the number of epochs, MAX\_EPOCH means the maximum of epochs, set it to 1000 and each epoch includes 250 iterations. We utilize SGD as our optimizer and set the momentum to 0.99. The weighted decay is set to 3e-5. We utilize both cross-entropy loss and dice loss by simply summing them up as the loss function. We utilize instance normalization as our normalization layer. we employ the deep supervision loss for the supervision of the U-Net. All experiments are conducted using two NVIDIA RTX A6000 GPUs with 48GB memory.

\noindent\textbf{Deep Supervision.} The U-Net network is trained with deep supervision. 
For each deep supervision output, we downsample the ground truth segmentation mask for the loss computation with each deep supervision output. The final training objective is the sum of all resolutions loss:
\begin{equation}
    \begin{aligned}
            \mathcal{L} = w_1 \cdot \mathcal{L}_1 + w_2 \cdot \mathcal{L}_2 + w_3 \cdot \mathcal{L}_3 + \cdot \cdot \cdot w_n \cdot \mathcal{L}_n
    \end{aligned}
    \label{equa:finalloss}
\end{equation}
where the weights halve with each decrease in resolution~(\textit{i.e.,} $w_2 = \frac{1}{2} \cdot w_1; w_3 = \frac{1}{4} \cdot w_1$, etc), and all weight are normalized to sum to 1. Meanwhile, the resolution of $\mathcal{L}_1$ is equal to $2 \cdot \mathcal{L}_2$ and $4 \cdot \mathcal{L}_3$.

\section{More Visualization of Two Refinements}
In Figure~\ref{fig:morerefinement}, we present additional qualitative results showcasing the refinements at different stages. With the two refinements, the results clearly illustrate the progressive improvement in segmentation accuracy, emphasizing the effectiveness of our model's refinement process.

