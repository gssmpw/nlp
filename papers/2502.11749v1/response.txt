\section{Related Works}
\label{sec:related}
% % 在详细展开具体方法的论述之前，我们首先对本文中的符号进行定义，具体如表1所示。
% Before delving into a detailed exposition of the specific methods, we first establish the definitions of symbols used in this paper, as outlined in [1].
% 从上式可知，三个临时变量$\mathcal{X}$, $\mathcal{T}$, $\mathcal{L}$的交互较为复杂，进而使得展开后的网络结构复杂，使得反向传播变得不高效。我们相信，更为直接且简单的迭代求解算法能够得到更加高效的展开网络结构，进而能够取得更好的重建效果。
% Furthermore, both the sparse and low-rank constraints are imposed directly on the image, making them inseparable. Thus, solving the reconstruction model becomes more challenging, which consequently leads to a more complex network structure. Therefore, this motivates us to propose a simple yet efficient structure for the joint low-rank and sparse unrolling network.
% 然而，SLR-Net中的稀疏和低秩约束都是对图像本身施加的，因此不可分离，这也导致其求解重建模型较为复杂。不过，在过去基于优化的模型的研究中已经表明，联合低秩和稀疏模型与低秩plus稀疏的模型对图像的理解不同，而且基于优化的联合方法也取得了非常好的效果。
% Furthermore, the joint low-rank and sparse model can be formulated as,
\begin{equation}
    \label{eq:opt}
    \min_{\mathcal{X}} \frac12 \| \mathsf{A}(\mathcal{X}) - \mathbf{b} \|_2^2 + \lambda_1 \mathsf{R}(\mathcal{X}) + \lambda_2 \mathsf{S}(\mathcal{X}),
\end{equation}
where $\mathsf{R}(\mathcal{X})$ and $\mathsf{S}(\mathcal{X})$ denote the LR and sparse priors, respectively. $\lambda_1$ and $\lambda_2$ are the balancing parameters.
SLR-Net has developed an algorithm that embeds ISTA within ADMM [7]. Specifically, using ADMM and introducing auxiliary variables $\mathcal{T}$, the optimization problem \eqref{eq:opt} can be reformulated as,
\begin{equation}
    \footnotesize
    \label{eq:opt_admm}
    \begin{cases}
        &\min_{\mathcal{X}} \frac12 \| \mathsf{A}(\mathcal{X}) - \mathbf{b} \|_2^2  + \frac\rho2 \|\mathcal{X} - \mathcal{T}+\mathcal{L}\|_F^2  + \lambda_2 \mathsf{S}(\mathcal{X})\\
        &\min_{\mathcal{T}} \frac\rho2 \|\mathcal{X} - \mathcal{T}+\mathcal{L}\|_F^2 + \lambda_1 \mathsf{R}(\mathcal{T})\\
        &\mathcal{L} = \mathcal{L} + \eta(\mathcal{X} - \mathcal{T}),
    \end{cases}
\end{equation}
where $\mathcal{L}$ is the Lagrangian multiplier. ISTA is then embedded to solve the $\mathcal{X}$ subproblem, resulting in, 
\begin{equation}
    \footnotesize
    \label{eq:opt_pgdadmm}
    \begin{cases}
        & \begin{cases}
            & \begin{cases}
                & \bar{\mathcal{X}} = \mathcal{X}^{n} - \mu \nabla_{\mathcal{X}^{n}} \left[ \frac{1}{2} \left\| \mathsf{A} (\mathcal{X}) - \mathbf{b} \right\|_2^2 \right] = \mathcal{X}^{n} - \mu \mathsf{A}^H\left[\mathsf{A}(\mathcal{X}^{n})-\mathbf{b}\right] \\
                & \mathcal{Z} = \mathcal{X} - \mu \nabla_\mathcal{X}\left[\frac12 \| \mathsf{A}(\mathcal{X}) - \mathbf{b} \|_2^2  + \frac\rho2 \|\mathcal{X} - \mathcal{T}+\mathcal{L}\|_F^2\right] \\
                & \min_{\mathcal{X}} \frac1{2\mu} \|  \mathcal{X} - \mathcal{Z} \|_2^2 + \lambda_2 \mathsf{S}(\mathcal{X})
            \end{cases} \\
        &\min_{\mathcal{T}} \frac\rho2 \|\mathcal{X} - \mathcal{T}+\mathcal{L}\|_F^2 + \lambda_1 \mathsf{R}(\mathcal{T})\\
        &\mathcal{L} = \mathcal{L} + \eta(\mathcal{X} - \mathcal{T}).
    \end{cases}
\end{equation}

\subsection{Unrolling Networks using Composite Priors}
Composite priors-driven DUNs in dynamic MRI reconstruction mainly lie in the intersection of the low-rank and sparse priors. LplusS-Net [2] and SLR-Net [3, 5] represent two different types, where the former follows a low-rank plus sparse scheme, while the latter adopts a joint low-rank and sparse formulation which is the focus of this paper. Additionally, SOUL-Net [6] utilizes similar structures to SLR-Net but is applied in CT reconstruction. JSLR-Net [4] also follows the joint low-rank and sparse scheme, but it is unfolded by the HQS algorithm, which may need conjugate gradient (CG) iterations to solve the subproblems. However, CG iterations will increase the complexity of the network and make the training process more difficult [8].