\section{Related Works}
\label{sec:related}
% % 在详细展开具体方法的论述之前，我们首先对本文中的符号进行定义，具体如表1所示。
% Before delving into a detailed exposition of the specific methods, we first establish the definitions of symbols used in this paper, as outlined in Tab.\ref{tab:symbol}.
% % Tab.generated by Excel2LaTeX from sheet 'Sheet1'
% \begin{table}[htbp]
% 	\centering
% 	\caption{The definitions of symbols used in this paper.}
% 	  \begin{tabular}{lll}
% 	  \toprule
% 	  Symbol & Format & Example \\
% 	  \midrule
% 	  Operator & sans serif capital letter & $\mathsf{A}$ \\
% 	  Number set & blackboard bold capital letter & $\mathbb{C}$ \\
% 	  Tensor & Euler script uppercase letter & $\mathcal{X}$\\
% 	  Matrix & bold capital letter & $\mathbf{X}$ \\
% 	  Vector & bold lowercase letter & $\mathbf{b}$ \\
% 	  Scalar & upper/lowercase letter & $H,n_x$ \\
% 	  \bottomrule
% 	  \end{tabular}%
% 	\label{tab:symbol}%
%   \end{table}%
  
\subsection{General Reconstruction Model of Dynamic MRI}
The acquisition of dynamic MRI can be formulated as,
\begin{equation}
	\label{eq:acq}
	\mathbf{b} = \mathsf{A} (\mathcal{X}) + \mathbf{n},
\end{equation} 
where $\mathbf{b} \in \mathbb{C}^{M}$ is the acquired k-space data, $\mathcal{X} \in \mathbb{C}^{H \times W \times T}$ is the distortion-free dynamic MRI image with $H$, $W$, and $T$ representing the spatial dimensions and temporal frames, respectively, $\mathsf{A}: \mathbb{C}^{H \times W \times T} \rightarrow \mathbb{C}^{M}$ is the acquisition operator, and $\mathbf{n} \in \mathbb{C}^{M}$ is the noise. From the above equation, it can be observed that reconstructing the clean image $\mathcal{X}$ from the acquired undersampled k-space data $\mathbf{b}$ is a typical ill-posed linear inverse problem. Specifically, based on the physical principles of MRI, operator $\mathsf{A}$ can be expressed in a more detailed manner,
\begin{equation}
	\mathsf{A} = \mathsf{F_u} \circ \mathsf{S} = \mathsf{M} \circ \mathsf{F} \circ \mathsf{S},
\end{equation}
where $\mathsf{F_u}$ is the undersampling operator, $\mathsf{S}: \mathbb{C}^{H \times W \times T} \rightarrow \mathbb{C}^{C \times H \times W \times T}$ denotes the coil sensitive maps for multi-coil MRI with $C$ coils, and the symbol $\circ$ denotes the composite operation. When the sampling points are in the Cartesian grid, $\mathsf{F_u} = \mathsf{M} \circ \mathsf{F}$ holds with $\mathsf{F}$ being the unitary Fourier transform, and $\mathsf{M}$ denoting the sampling mask. As for single-coil cases, $\mathsf{S}$ can be omitted.

Based on \eqref{eq:acq}, the reconstruction model of dynamic MRI can be formulated as,
\begin{equation}
	\label{eq:rec}
	\hat{\mathcal{X}} = \arg \min_{\mathcal{X}} \frac{1}{2} \left\| \mathsf{A} (\mathcal{X}) - \mathbf{b} \right\|_2^2 + \lambda \mathsf{\Phi}(\mathcal{X}),
\end{equation}
where the first term with $l_2$ norm is the data fidelity term, the second term with $\mathsf{\Phi}: \mathbb{C}^{H \times W \times T} \rightarrow \mathbb{R}_+$ is the regularization term that encodes the prior information of MRI images, and $\lambda$ is the balance parameter.

\subsection{Unrolling Networks using Single Prior}
% 展开网络构建一般包括三个步骤，先验驱动的重建模型，迭代求解算法，展开。实际上，前两个因素即是基于迭代优化算法的重建方法的完备构建过程，展开网络在此基础上对算法在深度学习框架中展开，并赋予其基于数据集学习并端到端训练的能力。展开网络在迭代优化算法的上层包装了一层监督机制，使得以前需要人工经验性调整的超参数（例如，(1)中的lambda）变为可学习参数，另外，还可以利用一些神经网络（FC, CNN, Transformer）等进一步增强网络的特征提取能力，进而能够取得更好的重建效果。
An unrolling network ____ generally involves three factors: a prior-driven reconstruction model, an iterative solving algorithm, and unfolding. Actually, the first two factors can precisely constitute a complete optimization-based reconstruction method. 

%The unrolling network builds upon this foundation by unfolding the algorithm within a deep learning framework and endowing it with the capability for learning based on datasets and end-to-end training. The unrolling network introduces the supervision mechanism on top of the optimization-based method, transforming previously manually adjusted hyperparameters, e.g., $\lambda$ in \eqref{eq:rec}, into trainable parameters. Additionally, it leverages neural networks such as Fully Connected (FC), Convolutional Neural Network (CNN), Transformer, etc., to further enhance the network's feature extraction capabilities, thereby achieving superior reconstruction results.

% 近年来，各种基于单一先验的展开网络层出不穷。
In recent years, various unrolling reconstruction networks based on a single prior have emerged and become mainstream ____. DCCNN ____, MoDL ____, and E2EVarNet ____ are representative examples that using CNNs to learn the implicit image prior. HUMUS-Net ____ took the vision Transformer to substitute CNN. 
CineVN ____ employed a spatiotemporal E2EVarNet combined with conjugate gradient descent for optimized data consistency and improved image quality for dynamic MRI.
However, the implicit nature leads to a lack of interpretability. ISTA-Net ____ and FISTA-Net ____ exploited the learned sparse prior and adopted the ST operator ____ to enforce the sparsity constraint in a CNN-learned multi-channel sparse domain. However, the ST operator applied the same threshold to all channels and the two CNNs that learn the sparse transforms are constrained to be inverse, limiting the flexibility of the sparse constraint. 
Moreover, T2LR-Net ____ utilized the tensor low-rank prior and the tensor nuclear norm ____ to exploit the structural correlations in high-dimensional data, in which CNNs were also used to adaptively learn the low-rank transform domain. % The above is from the view of the intrinsic prior behind DUNs. 

As for the underlying iterative solving algorithms that we termed as the structure of the DUNs, algorithms like ADMM ____, ISTA ____, and others are widely used. However, although ADMM is an effective algorithm for a lot of optimization problems, it usually requires the conjugate gradient (CG) method to solve the subproblems, especially for the MRI reconstruction cases with non-Cartesian sampling and multi-coil data. Inserting CG in DUNs will increase the complexity of the network and make the training process more difficult ____. ISTA does not require CG iterations; it can be iteratively solved through a gradient descent step and a projection step. Another widely used approach is to directly simplify the gradient descent step in ISTA through the use of the Data Consistency (DC) layer ____. % During the iteration, it directly overlays the undersampled true data $\mathbf{b}$ onto the $k$-space of the reconstructed image, while the un-sampled regions still adopt the values obtained through the projection step. Theoretically, only in the absence of noise, this substitution with the DC layer can guarantee the convergence of the iterative algorithm, which in practice is usually unattainable.
The ISTA algorithm for solving \eqref{eq:rec} can be formulated as,
\begin{equation}
	\footnotesize
	\label{eq:ista}
	\begin{cases}
		&\bar{\mathcal{X}} = \mathcal{X}^{n} - \mu \nabla_{\mathcal{X}^{n}} \left[ \frac{1}{2} \left\| \mathsf{A} (\mathcal{X}) - \mathbf{b} \right\|_2^2 \right] = \mathcal{X}^{n} - \mu \mathsf{A}^H\left[\mathsf{A}(\mathcal{X}^{n})-\mathbf{b}\right] \\
		&\mathcal{X}^{(k+1)} = \arg\min_{\mathcal{X}} \frac{1}{2\mu} \left\| \mathcal{X} - \bar{\mathcal{X}} \right\|_2^2 + \lambda \mathsf{\Phi}(\mathcal{X})=\operatorname{prox}_{\mu\lambda \mathsf{\Phi}}(\bar{\mathcal{X}}),
	\end{cases}
\end{equation}
where the operator $\operatorname{prox}_{\mu\lambda \mathsf{\Phi}}(\cdot)$ is the projection onto the set defined by the regularization $\Phi$ with the threshold $\mu\lambda$, with $\mu$ being the step size.

\subsection{Unrolling Networks using Composite Priors}
Composite priors-driven DUNs in dynamic MRI reconstruction mainly lie in the intersection of the low-rank and sparse priors. LplusS-Net ____ and SLR-Net ____ represent two different types, where the former follows a low-rank plus sparse scheme, while the latter adopts a joint low-rank and sparse formulation which is the focus of this paper. Additionally, SOUL-Net ____ utilizes similar structures to SLR-Net but is applied in CT reconstruction. JSLR-Net ____ also follows the joint low-rank and sparse scheme, but it is unfolded by the HQS algorithm, which may need conjugate gradient (CG) iterations to solve the subproblems. However, CG iterations will increase the complexity of the network and make the training process more difficult ____.

In SLR-Net, the low-rank prior is leveraged through the Casorati matrix. The Casorati matrix is obtained by unfolding the dynamic MRI tensor $\mathcal{X}$ along the temporal dimension, resulting in an $HW \times T$ matrix. The nuclear norm of this matrix is then utilized to explore the correlation of temporal evolution curves. However, this approach neglects spatial correlations, and the process of unfolding a high-dimensional tensor into a matrix inevitably loses the high-dimensional structural features of the data. Therefore, we believe a superior approach would be to employ tensor low-rank priors to construct the network. 


% Furthermore, both the sparse and low-rank constraints are imposed directly on the image, making them inseparable. Thus, solving the reconstruction model becomes more challenging, which consequently leads to a more complex network structure. Therefore, this motivates us to propose a simple yet efficient structure for the joint low-rank and sparse unrolling network.
% 然而，SLR-Net中的稀疏和低秩约束都是对图像本身施加的，因此不可分离，这也导致其求解重建模型较为复杂。不过，在过去基于优化的模型的研究中已经表明，联合低秩和稀疏模型与低秩plus稀疏的模型对图像的理解不同，而且基于优化的联合方法也取得了非常好的效果。

Furthermore, the joint low-rank and sparse model can be formulated as,
\begin{equation}
    \label{eq:opt}
    \min_{\mathcal{X}} \frac12 \| \mathsf{A}(\mathcal{X}) - \mathbf{b} \|_2^2 + \lambda_1 \mathsf{R}(\mathcal{X}) + \lambda_2 \mathsf{S}(\mathcal{X}),
\end{equation}
where $\mathsf{R}(\mathcal{X})$ and $\mathsf{S}(\mathcal{X})$ denote the LR and sparse priors, respectively. $\lambda_1$ and $\lambda_2$ are the balancing parameters.
SLR-Net has developed an algorithm that embeds ISTA within ADMM. Specifically, using ADMM and introducing auxiliary variables $\mathcal{T}$, the optimization problem \eqref{eq:opt} can be reformulated as,
\begin{equation}
    \footnotesize
    \label{eq:opt_admm}
    \begin{cases}
        &\min_{\mathcal{X}} \frac12 \| \mathsf{A}(\mathcal{X}) - \mathbf{b} \|_2^2  + \frac\rho2 \|\mathcal{X} - \mathcal{T}+\mathcal{L}\|_F^2  + \lambda_2 \mathsf{S}(\mathcal{X})\\
        &\min_{\mathcal{T}} \frac\rho2 \|\mathcal{X} - \mathcal{T}+\mathcal{L}\|_F^2 + \lambda_1 \mathsf{R}(\mathcal{T})\\
        &\mathcal{L} = \mathcal{L} + \eta(\mathcal{X} - \mathcal{T}),
    \end{cases}
\end{equation}
where $\mathcal{L}$ is the Lagrangian multiplier. ISTA is then embedded to solve the $\mathcal{X}$ subproblem, resulting in, 
\begin{equation}
    \footnotesize
    \label{eq:opt_pgdadmm}
    \begin{cases}
        & \begin{cases}
            & \mathcal{Z} = \mathcal{X} - \mu \nabla_\mathcal{X}\left[\frac12 \| \mathsf{A}(\mathcal{X}) - \mathbf{b} \|_2^2  + \frac\rho2 \|\mathcal{X} - \mathcal{T}+\mathcal{L}\|_F^2\right] \\
            & \min_{\mathcal{X}} \frac1{2\mu} \|  \mathcal{X} - \mathcal{Z} \|_2^2 + \lambda_2 \mathsf{S}(\mathcal{X})
            \end{cases} \\
        &\min_{\mathcal{T}} \frac\rho2 \|\mathcal{X} - \mathcal{T}+\mathcal{L}\|_F^2 + \lambda_1 \mathsf{R}(\mathcal{T})\\
        &\mathcal{L} = \mathcal{L} + \eta(\mathcal{X} - \mathcal{T}).
    \end{cases}
\end{equation}
Note that we have omitted the indexing notation for iterations for the sake of brevity. 
% Theoretically, this algorithm requires subsequent iterations to be computed after the convergence of PGD within the inner loop. However, in practice, deploying this algorithm as a deep unrolling network is impractical. SLR-Net adopts an approximate method by fixing the number of PGD iterations within the inner loop to 1. Actually, the model can be solved using ADMM alone, like what has been proposed in k-t SLR. The reason for embedding PGD lies in bypassing the conjugate gradient step in ADMM, thus reducing computational overhead.
% 从上式可知，三个临时变量$\mathcal{X}$, $\mathcal{T}$, $\mathcal{L}$的交互较为复杂，进而使得展开后的网络结构复杂，使得反向传播变得不高效。我们相信，更为直接且简单的迭代求解算法能够得到更加高效的展开网络结构，进而能够取得更好的重建效果。
From the above equation, it is evident that the interactions among the three temporary variables, $\mathcal{Z}$, $\mathcal{T}$, and $\mathcal{L}$, are relatively complex. This complexity renders the design of composite-prior DUNs difficult and cumbersome, while also hindering efficient backpropagation to some extent, consequently leading to suboptimal reconstruction results. We believe that a more simple iterative solving algorithm can also lead to a more efficient unrolling network structure and, consequently, achieving better reconstruction results and simplifying network design.