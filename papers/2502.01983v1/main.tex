\documentclass[11pt]{amsart}
\usepackage{amssymb,amsmath,amsthm,amsfonts,mathrsfs}
\usepackage[hmargin=3cm,vmargin=3.5cm]{geometry}
\usepackage[dvipsnames,table,xcdraw]{xcolor}
\usepackage[colorlinks = true,
            linkcolor = Fuchsia,
            urlcolor  = ForestGreen,
            citecolor = WildStrawberry,
            anchorcolor = blue]{hyperref}
%\usepackage{graphics} 
\usepackage{graphicx,psfrag}
\usepackage{amscd}
\usepackage[shellescape]{gmp}
%\usepackage{bbm} 
\usepackage{stmaryrd}  %% double square brackets
\usepackage[all,2cell]{xy} \UseAllTwocells \SilentMatrices
\usepackage{tikz-cd,pgfplots}
\usepackage[dvips]{epsfig}
\usepackage{MnSymbol}
\usepackage{comment}
\usepackage{enumitem}
\usepackage{kbordermatrix}
\usepackage[colorinlistoftodos]{todonotes}
\usepackage{cancel}

\usepackage{stackengine,scalerel}
\newcommand\dddag{%
  \sbox0{\ddag}\scalerel*{%
  \stackengine{-.6\ht0}{\ddag}{\ddag}{O}{c}{F}{F}{S}}{\ddag}%
}
\newcommand\DDDag{%
  \sbox0{\ddag}\stretchrel*{%
  \stackengine{-.6\ht0}{\ddag}{\ddag}{O}{c}{F}{F}{S}}{\ddag}%
}

\pgfplotsset{
  compat=1.16,
  myplot/.style={smooth, tension=0.5, mark=none, thick}
}


\newcommand{\caden}[1]{\todo[size=\tiny,color=blue!30]{#1 \\ \hfill --- Caden}}
\newcommand{\Caden}[1]{\todo[size=\tiny,inline,color=blue!30]{#1
      \\ \hfill --- Caden}}
\newcommand{\meeseong}[1]{\todo[size=\tiny,color=yellow!70]{#1 \\ \hfill --- Mee Seong}}
\newcommand{\MeeSeong}[1]{\todo[size=\tiny,inline,color=yellow!70]{#1
      \\ \hfill --- Mee Seong}}
 
\setlength{\marginparwidth}{2cm}
 
 
\renewcommand{\kbldelim}{(}% Left delimiter
\renewcommand{\kbrdelim}{)}% Right delimiter
 

%\usepackage{mathtools}
%\usepackage{auto-pst-pdf}

% Comment out before submission: 
\usepackage[modulo]{lineno}
%\linenumbers

\usetikzlibrary{arrows,automata}
\usetikzlibrary{decorations.markings}
\usetikzlibrary{decorations.pathmorphing}



\makeatletter
\def\@adminfootnotes{%
  \let\@makefnmark\relax  \let\@thefnmark\relax
  \ifx\@empty\@date\else \@footnotetext{\@setdate}\fi%%   <------ added
  \ifx\@empty\@subjclass\else \@footnotetext{\@setsubjclass}\fi
  \ifx\@empty\@keywords\else \@footnotetext{\@setkeywords}\fi
  \ifx\@empty\thankses\else \@footnotetext{%
    \def\par{\let\par\@par}\@setthanks}%
  \fi
}
\makeatother



% \theoremstyle{remark}
%%\newtheorem{theorem}{Theorem}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{prop}[theorem]{Proposition}
\newtheorem{cor}[theorem]{Corollary}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{lemma}[theorem]{Lemma}

\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{conjecture}[theorem]{Conjecture}
\newtheorem{example}[theorem]{Example}

\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{problem}[theorem]{Problem}
\newtheorem{question}[theorem]{Question}

\numberwithin{equation}{section}

%    Absolute value notation
\newcommand{\abs}[1]{\lvert#1\rvert}

%    Blank box placeholder for figures (to avoid requiring any
%    particular graphics capabilities for printing this document).
\newcommand{\blankbox}[2]{%
  \parbox{\columnwidth}{\centering
%    Set fboxsep to 0 so that the actual size of the box will match the
%    given measurements more closely.
    \setlength{\fboxsep}{0pt}%
    \fbox{\raisebox{0pt}[#2]{\hspace{#1}}}%
  }%
}

 
%%%%% Offset in TOC
\let\oldtocsection=\tocsection
\let\oldtocsubsection=\tocsubsection
%\let\oldtocsubsubsection=\tocsubsubsection
\renewcommand{\tocsection}[2]{\hspace{0em}\oldtocsection{#1}{#2}}
\renewcommand{\tocsubsection}[2]{\hspace{1em}\oldtocsubsection{#1}{#2}}
%\renewcommand{\tocsubsubsection}[2]{\hspace{2em}\oldtocsubsubsection{#1}{#2}} 


%\makeatletter
%\renewcommand*{\@biblabel}[1]{[#1]}
%\makeatother

\newcommand*\circled[1]{\tikz[baseline=(char.base)]{
            \node[shape=circle,draw,inner sep=2pt] (char) {#1};}}  % circled letters and numbers 

\usepackage{chngcntr}
\counterwithin{figure}{subsection}

\makeatletter
\@namedef{subjclassname@2020}{%
  \textup{2020} Mathematics Subject Classification}

%\newcommand\mathcircled[1]{%
%  \mathpalette\@mathcircled{#1}%
%}  % put a cirle around math 
%\newcommand\@mathcircled[2]{%
%  \tikz[baseline=(math.base)] \node[draw,circle,inner sep=1pt] (math) {$\m@th#1#2$};%
%}  % put a cirle around math 

\setcounter{tocdepth}{3}  % show subsections in Table of Contents 

\makeatother
 
\title[Diagrammatics of information]{Diagrammatics of information}
  
 
\author{Mee Seong Im}
\address{\parbox{\linewidth}{Department of Mathematics, Johns Hopkins University, Baltimore, MD 21218, USA (current)\\ 
Department of Mathematics, United States Naval Academy, Annapolis, MD 21402, USA}}
\email{\href{mailto:meeseong@jhu.edu}{meeseong@jhu.edu}}
%\thanks{}

\author{Clement Kam} 
\address{U.S. Naval Research Laboratory, Washington, DC 20375, USA}
\email{\href{mailto:khovanov@jhu.edu}{clement.k.kam.civ@us.navy.mil}}
%\thanks{}
 
\author{Caden Pici}
\address{U.S. Naval Research Laboratory, Washington, DC 20375, USA}
\email{\href{mailto:khovanov@math.columbia.edu}{caden.j.pici.civ@us.navy.mil}}
%\thanks{}

  
%    General info
\subjclass[2020]{Primary: 57K16, 18M30, 28D20;
Secondary: 37A35, 68P30, 94A15, 94A40.}
\date{\today}

%\dedicatory{This paper is dedicated to ...}

\providecommand{\keywords}[1]{\textbf{\textit{Key words and phrases.}} #1}

\keywords{Topological quantum field theory, TQFT, diagrammatic algebras, Shannon entropy, information theory, mutual information.} 
 


 

\begin{document}

\def\Aff{\mathsf{Aff}}
\def\AND{\mathsf{AND}}
\def\concatenate{\mathsf{concatenate}}
\def\Br{\mathsf{Br}}
\def\Gal{\mathsf{Gal}}
\def\gen{\mathsf{generators}}
\def\GL{\mathsf{GL}}
\def\SL{\mathsf{SL}}
\def\init{\mathsf{in}}
\def\t{\mathsf{t}}
\def\out{\mathsf{out}}
\def\inner{\mathsf{inner}}
\def\I{\mathsf I}
\def\region{\mathsf{region}}
\def\plane{\mathsf{plane}}
\def\R{\mathbb R}
\def\Q{\mathbb Q}
\def\Z{\mathbb Z}
\def\mc{\mathcal{c}}
\def\finite{\mathsf{finite}}
\def\infinite{\mathsf{infinite}}
\def\N{\mathbb N} 
\def\C{\mathbb C}
\def\sep{\mathsf{sep}}
\def\S{\mathbb S}
\def\SS{\mathbb S} 
\def\CP{\mathbb P}
\def\Ob{\mathsf{Ob}}
\def\op{\mathsf{op}}
\def\new{\mathsf{new}}
\def\old{\mathsf{old}}
\def\OR{\mathsf{OR}}
\def\AND{\mathsf{AND}}
\def\rat{\mathsf{rat}}
\def\rec{\mathsf{rec}}
\def\tail{\mathsf{tail}}
\def\coev{\mathsf{coev}}
\def\eps{\varepsilon}
\def\ev{\mathsf{ev}}
\def\id{\mathsf{id}}
\def\s{\mathsf{s}}
\def\S{\mathsf{S}}
\def\t{\mathsf{t}}
\def\start{\textsf{starting}}
\def\Notation{\textsf{Notation}}
\def\circleft{\raisebox{-.18ex}{\scalebox{1}[2.25]{\rotatebox[origin=c]{180}{$\curvearrowright$}}}}
\renewcommand\SS{\ensuremath{\mathbb{S}}}
\newcommand{\kllS}{\kk\llangle  S \rrangle} %% power ser
\newcommand{\kllSS}[1]{\kk\llangle  #1 \rrangle}
\newcommand{\klS}{\kk\langle S\rangle}  % nc polynomials
\newcommand{\aver}{\mathsf{av}}  % average 
\newcommand{\ophana}{\overline{\phantom{a}}}
\newcommand{\Bool}{\mathbb{B}}
\newcommand{\dmod}{\mathsf{-mod}}
\newcommand{\lang}{\mathsf{lang}}
\newcommand{\pfmod}{\mathsf{-pfmod}}
\newcommand{\primitive}{\mathsf{irr}}
\newcommand{\Bmod}{\Bool\mathsf{-mod}}  % B-module 
\newcommand{\Bmodo}[1]{\Bool_{#1}\mathsf{-mod}}  
\newcommand{\Bfmod}{\Bool\mathsf{-fmod}} % finite B-modules 
\newcommand{\Bfpmod}{\Bool\mathsf{-fpmod}} % finite projective B-modules
\newcommand{\Bfsmod}{\Bool\mathsf{-}\underline{\mathsf{fmod}}}  % stable category 
\newcommand{\undvar}{\underline{\varepsilon}} %sequence of varepsilons, not using anymore
\newcommand{\RLang}{\mathsf{RLang}}
\newcommand{\undotimes}{\underline{\otimes}}
\newcommand{\sigmaacirc}{\Sigma^{\ast}_{\circ}} % equiv classes of words under rotation 
\newcommand{\cl}{\mathsf{cl}}
\newcommand{\PP}{\mathcal{P}} % powerset 
\newcommand{\wedgezero}{\{ \vee ,0\} } % semilattices
\newcommand{\whA}{\widehat{A}}
\newcommand{\whC}{\widehat{C}}
\newcommand{\whM}{\widehat{M}}
%\newcommand{\Sigmalr}{\Sigma^{\leftrightarrow}}
\newcommand{\Sigmalr}{\Sigma^{\Z}}
%\newcommand{\Sigmal}{\Sigma^{\leftarrow}}
\newcommand{\Sigmal}{\Sigma^{-}}
%\newcommand{\Sigmar}{\Sigma^{\rightarrow}}
\newcommand{\Sigmar}{\Sigma^{+}}
\newcommand{\Sigmaa}{\Sigma^{\ast}}
\newcommand{\SigmaZ}{\Sigma^{\Z}}  
\newcommand{\Sigmac}{\Sigma^{\circ}}

%\newcommand{\alphai}{\alpha^{\shortmid}}  
\newcommand{\alphai}{\alpha_I}  % alpha vertical
\newcommand{\alphac}{\alpha_{\circ}}  % alpha circle 
\newcommand{\alphap}{(\alphai,\alphac)} % alpha pair 
\newcommand{\alphalr}{\alpha_{\leftrightarrow}}
\newcommand{\alphaZ}{\alpha_{\Z}}
\newcommand{\mcCinfalpha}{\mcC^{\infty}_{\alpha}}
\newcommand{\mathT}{\mathsf{T}}
\newcommand{\mathF}{\mathsf{F}}
\newcommand{\mcS}{\mathcal{S}}
\newcommand{\mcN}{\mathcal{N}}
\newcommand{\wmcN}{\widetilde{\mcN}}
\newcommand{\Net}{\mathsf{Net}}


% redefine emptyset symbol 
\let\oldemptyset\emptyset
\let\emptyset\varnothing

\newcommand{\undempty}{\underline{\emptyset}}
\newcommand{\undsigma}{\underline{\sigma}}
\newcommand{\undtau}{\underline{\tau}}
\def\basis{\mathsf{basis}}
\def\irr{\mathsf{irr}} % recognizable series 
\def\spanning{\mathsf{spanning}}
\def\elmt{\mathsf{elmt}}

\def\H{\mathsf{H}}
\def\I{\mathsf{I}}
\def\II{\mathsf{II}}
\def\l{\lbrace}
\def\r{\rbrace}
\def\o{\otimes}
\def\lra{\longrightarrow}
\def\Ext{\mathsf{Ext}}
\def\mf{\mathfrak} 
\def\mcC{\mathcal{C}}
\def\mcO{\mathcal{O}}
%\def\mcI{\mathcal{I}}
%\def\uFr{\underline{\mathsf{Fr}}}
\def\Fr{\mathsf{Fr}}

%\def\bbn{\mathbb{B}^n}
\def\ovb{\overline{b}}
\def\tr{{\sf tr}} 
\def\str{{\sf str}} 
\def\det{{\sf det }} 
\def\tral{\tr_{\alpha}}
\def\one{\mathbf{1}}   % unit  object of category 



\def\lra{\longrightarrow}
\def\twoheadlra{\longrightarrow\hspace{-4.6mm}\longrightarrow}
\def\hooklra{\raisebox{.2ex}{$\subset$}\!\!\!\raisebox{-0.21ex}{$\longrightarrow$}}
\def\kk{\mathbf{k}}  %% base field  
\def\gdim{\mathsf{gdim}}  %% graded dimension 
\def\rk{\mathsf{rk}}
\def\undep{\underline{\varepsilon}}
\def\mathM{\mathbf{M}}  % boolean matrix 

% cobordism categories 
\def\CCC{\mathcal{C}} % cat of cobordisms 
\def\wCCC{\widehat{\CCC}}  % completed category

\def\complement{\mathsf{comp}}
\def\Rec{\mathsf{Rec}} % recognizable series  

\def\Cob{\mathsf{Cob}} 
\def\Kar{\mathsf{Kar}}   % Karoubi envelope 

\def\dmod{\mathsf{-mod}}   % modules  
\def\pmod{\mathsf{-pmod}}    % projective modules 

\newcommand{\brak}[1]{\ensuremath{\left\langle #1\right\rangle}}
\newcommand{\brakspace}[1]{\ensuremath{\left\langle \:\: #1\right\rangle}}

\newcommand{\oplusop}[1]{{\mathop{\oplus}\limits_{#1}}}
\newcommand{\ang}[1]{\langle #1 \rangle } 
%\newcommand{\bbn}[1]{\mathbb{B}^{#1}}
\newcommand{\ppartial}[1]{\frac{\partial}{\partial #1}} %partial derivative 
\newcommand{\checkr}{{\bf \color{red} CHECK IT}}
\newcommand{\checkb}{{\bf \color{blue} CHECK IT}}
\newcommand{\checkk}[1]{{\bf \color{red} #1}}

\newcommand{\mcA}{{\mathcal A}}
\newcommand{\cZ}{{\mathcal Z}}
\newcommand{\sq}{$\square$}
\newcommand{\bi}{\bar \imath}
\newcommand{\bj}{\bar \jmath}

%\newcommand{\undn}{\mathbf{n}}
%\newcommand{\undm}{\mathbf{m}}
\newcommand{\undn}{\underline{n}}
\newcommand{\undm}{\underline{m}}
\newcommand{\undzero}{\underline{0}}
\newcommand{\undone}{\underline{1}}
\newcommand{\undtwo}{\underline{2}}

\newcommand{\cob}{\mathsf{cob}} % cobordism 
\newcommand{\comp}{\mathsf{comp}} % complementary

\newcommand{\Aut}{\mathsf{Aut}}
\newcommand{\Hom}{\mathsf{Hom}}
%\newcommand{\Mod}{\mbox{Mod}}
\newcommand{\Idem}{\mathsf{Idem}}
\newcommand{\Ind}{\mbox{Ind}}
\newcommand{\Id}{\textsf{Id}}
\newcommand{\End}{\mathsf{End}}
\newcommand{\iHom}{\underline{\mathsf{Hom}}}
\newcommand{\Bools}{\Bool^{\mathfrak{s}}}
\newcommand{\mfs}{\mathfrak{s}}
\newcommand{\blueline}{line width = 0.45mm, blue}


\newcommand{\drawing}[1]{
\begin{center}{\psfig{figure=fig/#1}}\end{center}}

\def\endomCempt{\End_{\mcC}(\emptyset_{n-1})}

\def\MS#1{{\color{blue}[MS: #1]}}
\def\MK#1{{\color{red}[MK: #1]}}

\allowdisplaybreaks

\begin{abstract}
We introduce a diagrammatic perspective for Shannon entropy created by the first author and Mikhail Khovanov and connect it to information theory and mutual information. We also give two complete proofs that the $5$-term  dilogarithm deforms to the $4$-term infinitesimal dilogarithm.
\end{abstract}

\maketitle
\tableofcontents


%%%%%%%%%%%%%%%%%%%%%%%
%
%  Intro  
%
%%%%%%%%%%%%%%%%%%%%%%%

\section{Introduction}
\label{section:intro}

Mathematics and theoretical physics have emerged in the rapid development of modern technology in recent decades. In particular, information theory~\cite{Khinchin57,Khinchin56,Nielsen20,Brill56,LR97,DH99} plays a pivotal role in communication, quantification, storage, and transfer of knowledge. One of the key measures in information theory is entropy, which quantifies the uncertainty in a random variable or the outcome of a random process. In geometric information theory~\cite{MKW23,Nielsen_geometric_theory_info,Niel_geom_23_1,Niel_geom_23_2,NB_geom_info}, spaces and manifolds of probability distributions are studied from a geometric and analysis point-of-view. 

Applications and interpretations of one-dimensional cobordisms have recently emerged, predominantly explored by M.S. Im and her collaborators, e.g., see~\cite{GIKK24, GIKKL23, GIK22, IK21, IK-top-automata, IK_journey23, IK24_dilogarithms_entropy, IK24_SAF, IK_22_linear, IKV23, IZ21}.
Im and M. Khovanov introduce in \cite{IK24_dilogarithms_entropy} a diagrammatical perspective of entropy and cocycles. In particular, they view entropy as certain cobordisms (see, e.g., \cite{IK_22_linear,Kh20_univ_const_two}) where information about the network only depends on the boundary. This coincides with the fact that in physics, the rich information about black holes also only depends on the geometric and topological structure of their boundaries.

In this paper, we recall diagrammatic interpretation of Shannon entropy in \cite{IK24_dilogarithms_entropy}, and reinterpret them in terms of information theory. We also provide two complete proofs that certain $5$-term dilogarithm deforms into the $4$-term infinitesimal dilogarithm.


\subsection*{Acknowledgments}
The first author would like to thank Mikhail Khovanov and the Research Scientists at the Naval Research Laboratory for extensive discussions. The first author would like to thank the Office of Naval Research (ONR) and United States Naval Research Laboratory (NRL) in Washington, DC for their support. 

\section{Background}
\label{section:background}



Let $\kk$ be a field of characteristic $0$.

\vspace{.25cm}

\subsection{Cathelineau's \texorpdfstring{$\kk$}{k}-vector space}
\label{subsection:Cathelineau_k_vs}

J.-L.~Cathelineau constructed a $\kk$-vector space $J(\kk)$ in \cite{Cath11}, with its spanning set $\langle a,b\rangle$, where $a,b\in\kk$, satisfying the relations: 
\begin{enumerate}
\item\label{item:symm} (symmetry) 
$\langle a,b\rangle  =  \langle b,a \rangle$, 
\item\label{item:scale} (scaling) 
$\langle ca,cb\rangle  =  c \langle a,b\rangle$, $c\in \kk$, 
\item 
\label{item:Cath_cocycle} (2-cocycle relation)
$\langle a,b+c\rangle + \langle b,c \rangle   = \langle a+b,c \rangle + \langle a,b \rangle$.
\end{enumerate} 
For relation~\eqref{item:Cath_cocycle},  the field $(\kk,+)$ under addition is viewed as a group. Furthermore, the symbols $\langle a,b\rangle$ are reminiscent of values of $2$-cocycles. 

First, we state a few preliminary lemmas:
\begin{lemma}
In the vector space $J(\mathbf{k})$, we have 
 $\langle a,0\rangle =\langle 0,a\rangle =0$, and  $\langle a,-a\rangle = 0$.
\end{lemma}

\begin{proof}
We will first prove $\langle a,0\rangle =0$.
Using \eqref{item:scale}, 
$\langle 0, 0 \rangle = \langle 0a, 0b \rangle = 0 \langle a,b\rangle = 0$. 
So we obtain $\langle a , 0+0\rangle + \langle 0,0 \rangle =\langle a+0,0\rangle + \langle a,0\rangle$ by setting $b=c=0$ in \eqref{item:Cath_cocycle}. This tells us that $\langle a,0\rangle =0$. Using symmetry in \eqref{item:symm}, $\langle 0,a\rangle =\langle a,0 \rangle =0$. 
Now, to prove the last equality, 
$\langle a, -a\rangle = a \langle 1,-1\rangle$. 
So $\langle 1,-1 \rangle = \langle -(-1),-1 \rangle = -\langle -1,1 \rangle$. 
We move the symbols to one side and use symmetry to obtain $0 = \langle 1,-1 \rangle + \langle -1,1\rangle = \langle 1,-1\rangle + \langle 1,-1\rangle = 2 \langle 1,-1 \rangle$. Since $\mathbf{k}$ is a field of characteristic $0$, $\langle 1,-1\rangle=0$. Thus $\langle a,-a\rangle=0$.
\end{proof}

\begin{lemma}
\label{lemma_minus_1_a}
We have $\langle 1-a, -1\rangle = \langle a,1-a \rangle$.
\end{lemma}

\begin{proof}
Applying \eqref{item:Cath_cocycle} and letting $a=a$, $b=1-a$, and $c=-1$,  we have 
$\langle a,1-a\rangle + \langle a+1-a,-1\rangle = \langle a,1-a-1\rangle + \langle 1-a,-1\rangle$. So 
$\langle a,1-a\rangle + \cancel{\langle 1,-1\rangle} = \cancel{\langle a,-a\rangle} + \langle 1-a,-1\rangle$ since $\langle a,-a \rangle=0$. We thus obtain the equality. 
\end{proof}
 
If $\mathbf{k}$ is a field of characteristic $p\not= 0$, then 
\begin{equation}
\label{eqn_char_p_Cathelineau}
\sum_{n=1}^p \langle 1, n1\rangle =0. 
\end{equation}



The vector space $J(\kk)$ is known as the space of infinitesimal dilogarithms, which is often  infinite-dimensional over $\kk$.

In the case when $\kk=\mathbb{R}$, $J(\kk)$ is isomorphic to the entropy of finite random variables. 

 

 


\subsection{Vector space isomorphic to Shannon's entropy}
\label{subsection:vs_Shannon_entropy}

The $\kk$-vector space $\beta(\kk)$
has a spanning set $[a]$, $a\in \kk^{*}$ is invertible, of vectors and relations: 
\begin{enumerate}
\item $[1]=0$, 
\item
\label{item_beta_k_4_term}
$[a]-[b]+ a\biggl[ \dfrac{b}{a}\biggr] + (1-a)\biggl[ \dfrac{1-b}{1-a} \biggr]=0, \ \ a\in \kk\setminus\{0,1\}, \ b\in \kk^{*}$.
\end{enumerate}
This vector space also satisfies $[a]=[1-a]$.

\begin{lemma}
The vector space $\beta(\kk)$ satisfies
$\left[\dfrac{1}{a}\right] = -\dfrac{1}{a}[a]$.
\end{lemma} 

\begin{proof}
Let $b=1$ in \eqref{item_beta_k_4_term} to obtain 
\[
[a]- \cancel{[1]} + a\left[ \frac{1}{a}\right] + \cancel{(1-a) \left[ \frac{1-1}{1-a}\right]} = 0. 
\]
This completes the proof.
\end{proof}
If $\mathbf{k}$ is a field of characteristic $p\not=0,2$, then 
\[
\sum_{n=2}^{p-1} [n1] = 0. 
\]
 
The space $\beta(\kk)$ generalizes entropy, which we explain in  Section~\ref{section:entropy_Cath_vs}. This 4-term equation \eqref{item_beta_k_4_term} is a limit of the 5-term equation for the dilogarithm (see [\href{https://people.mpim-bonn.mpg.de/zagier/files/scanned/DilogarithmInGeometryAndNumberTh/fulltext.pdf}{Zagier}]), hence the name infinitesimal dilogarithm. For completeness, we provide a proof of this derivation in Section~\ref{section_deformation}.

 


\subsection{Isomorphism of \texorpdfstring{$J(\mathbf{k})$}{Jk} and \texorpdfstring{$\beta(\mathbf{k})$}{betak}}
Cathelineau proves in ~\cite{Cath88} that the two vector spaces $J(\kk)$ and $\beta(\kk)$ are isomorphic by sending $[a]\mapsto \langle a, 1-a\rangle$. 
%Since $\langle a,b\rangle = a \langle 1, \frac{b}{a}\rangle$, which maps isomorphically to $[\frac{b}{a}]$. 
Conversely, $\langle a,-a \rangle \mapsto [0]=0$, and $\langle a,b\rangle\mapsto (a+b) \left[ \dfrac{a}{a+b} \right]$ whenever $a+b\not= 0$.
Furthermore, $\langle a,b\rangle = \langle b,a\rangle$ if and only if $[a]=[1-a]$. 


Via the isomorphism $J(\kk) \cong \beta(\kk)$, the $2$-cocycle condition in \eqref{item:Cath_cocycle} for $J(\kk)$ is equivalent to 
\[
(a+b+c) \left[ \dfrac{a}{a+b+c} \right] + 
(b+c) \left[ \dfrac{b}{b+c} \right] = 
(a+b+c) \left[ \frac{a+b}{a+b+c}  \right] + (a+b) \left[ \frac{a}{a+b} \right]. 
\]

Cathelineau shows that there is an isomorphism between the second homology and his construction  $H_2(\Aff_1(\kk),\kk_r)\simeq \beta(\kk)\simeq J(\kk)$, where 
\[ 
\Aff_1(\kk) = 
\left\{ 
\begin{pmatrix}
c & a \\ 0 & 1
\end{pmatrix} : c\in \kk^{*}, a\in \kk 
\right\}
\]
is the group of affine symmetries of a $\kk$-line and $\kk_r$ is a suitable right $\Aff_1(\kk)$-module. 
We refer to \cite{IK24_dilogarithms_entropy} for further details. 


 


 

\section{Shannon entropy and and Cathelineau's vector space}
\label{section:entropy_Cath_vs}


\subsection{Shannon entropy}
\label{subsection:Shannon_entropy_inner_product}

Let $X=\{x_1,\ldots, x_n\}$, a finite set. 
Shannon entropy of a finite probability distribution $p_X$ on $X$ that associates probabilities $p_1,\ldots, p_n$, where $\displaystyle{\sum_{i=1}^n} p_i=1$, $0<p_i<1$ to its points $x_i$, respectively, is given by 
\begin{equation}
\label{eq_shannon} 
H(p_X)  =  -\sum_{i=1}^n p_i 
\log p_i. 
\end{equation}
We can also think of $H(p_X)=-E[\log p_i]=E\left[\log \frac{1}{p_i}\right]$, where $E[\log p_i]$ is the expected value of $\log p_i$.
When $n=2$, the function $H(p_X)$ becomes a function of a single probability $p := p_1$. Then the entropy is  
\begin{equation}\label{eq_four_term_relation_entropy_2}
H(p) \ = \ - p\log p - (1-p)\log (1-p). 
\end{equation}
It is natural to extend $H$ from the open interval $(0,1)$ to all real numbers by  defining $H(0) = H(1) = 0$ and 
\begin{equation}
\label{eqn:two_term_entropy}
H(p) \ := \ - p\log|p| - (1-p)\log |1-p|. 
\end{equation}
Then $H$ is defined and continuous on all of $\mathbb{R}$.
The graph of $H$ is given in Figure~\ref{fig8_002}.
\input{fig8_002}

\begin{lemma}
\label{lemma_entropy_basic}
We have 
$H(a) = H(1-a)$ and $H\left(\dfrac{1}{a}\right) = -\dfrac{1}{a}H(a)$.
\end{lemma}

\begin{proof}
For the first equality, we have 
\[
H(a) 
= -a \log |a| -(1-a) \log |1-a| 
= -(1-a) \log |1-a| - (1-(1-a)) \log |1-(1-a)|
= H(1-a).
\]
For the second equality, we have 
\begin{align*}
H\left(\frac{1}{a} \right) 
&= -\frac{1}{a} \log \left| \frac{1}{a}\right|
- \left(1-\frac{1}{a}\right)\log \left|1-\frac{1}{a}\right| \\ 
&= \frac{1}{a} \log |a| - \left( 1-\frac{1}{a}\right) \log \left|  \frac{a-1}{a} \right| \\ 
&= \frac{1}{a} \log |a| - \left( 1-\frac{1}{a}\right) \left(  \log |1-a| - \log |a| \right) \\ 
&= \log |a| - \left( 1-\frac{1}{a}\right)   \log |1-a|  \\ 
&= -\frac{1}{a} \left( -a \log |a| - (1-a) \log |1-a| \right) \\ 
&= -\frac{1}{a} H(a).
\end{align*}
This concludes the proof. 
\end{proof}

The entropy function satisfies a four-term functional equation, see~\cite{Kont02}, 
%\cite{Faddeev56,Faddeev_online,Khinchin56,Tak58},
\begin{equation}
\label{eq_four_term_relation_entropy}
H(p) - H(q) +p\: H\left(\dfrac{q}{p}\right) + (1-p)H\left(\dfrac{1-q}{1-p}\right) = 0.
\end{equation}
Equation~\eqref{eq_four_term_relation_entropy} together with $H(1-p)=H(p)$ and continuity property uniquely determine $H$ as a nonzero function $\R\lra \R$. See~\cite{Faddeev56,Kont02}. Also see \cite[Equation (38)]{Taneja77}. 


 
\begin{proposition}
\label{prop:4_term_relation_entropy}
Let $H:\mathbb{R}\rightarrow \mathbb{R}$, where $H$ is \eqref{eqn:two_term_entropy} satisfying conditions $H(0)=H(1)=0$. Then $H$ satisfies the $4$-term relation~\eqref{eq_four_term_relation_entropy}.
\end{proposition}

Since the proof of Proposition~\ref{prop:4_term_relation_entropy} is not explicitly stated anywhere in the literature, we will provide its proof.

\begin{proof}
We have the following set of equalities: 
\begin{equation}
\label{eqn:proving_4_term}
\begin{split}
 -H(p)&=p\log |p| + (1-p)\log |1-p|, \\ 
 -H(q)&=q\log |q| + (1-q)\log |1-q|, \\ 
 -H\left(\frac{q}{p}\right) &= \frac{q}{p}\log\left|\frac{q}{p}\right| + \left(1-\frac{q}{p}\right) \log \left| 1-\frac{q}{p}\right|  \\ 
    &= \frac{q}{p}\left(\log|q| -\log|p|\right) 
    + \frac{p-q}{p}\left(\log |p-q| -\log|p| \right), \\ 
 -H\left(\frac{1-q}{1-p}\right) 
&= \frac{1-q}{1-p} \log\left| \frac{1-q}{1-p}\right| 
+ \frac{q-p}{1-p}\log \left|\frac{q-p}{1-p}\right|  \\
&= \frac{1-q}{1-p}\left( \log |1-q|-\log|1-p| \right) +\frac{q-p}{1-p}\left( \log |q-p| - \log|1-p| \right).\\
\end{split}
\end{equation}
So 
\begin{align*}
-p H&\left(\dfrac{q}{p}\right) - (1-p)H\left(\dfrac{1-q}{1-p}\right)
=  q\left(\log|q| -\log|p|\right) 
    + (p-q)\left(\log |p-q| -\log|p| \right)\\
&\hspace{4mm}+ 
 (1-q) \left( \log |1-q|-\log|1-p| \right) + (q-p)\left( \log |q-p| - \log|1-p| \right) \\ 
 &= q\log |q| - q\log|p| + \cancel{(p-q)\log|p-q|} - (p-q)\log|p| \\
&\hspace{4mm}+ (1-q)\log|1-q| - (1-q)\log|1-p| + \cancel{(q-p)\log|q-p|} - (q-p)\log|1-p| \\ 
 &= q\log |q| + (1-q)\log|1-q| - \cancel{q\log|p|} + \cancel{q \log |p|} - p\log|p|\\
&\hspace{4mm} - \log|1-p| + \cancel{q \log|1-p|}  - \cancel{q\log|1-p|} + p \log |1-p|\\ 
&= q\log |q| + (1-q)\log|1-q| 
  - (p\log|p| + (1-p) \log|1-p|) \\ 
&= -H(q) + H(p).
\end{align*}
Therefore, 
$H(p) - H(q) +p H\left(\dfrac{q}{p}\right) + (1-p)H\left(\dfrac{1-q}{1-p}\right) = 0$ holds. 
\end{proof}

We end this section by mentioning an important relationship relating Cathelineau's vector space~\cite{Cath88,Cath11} and Kontsevich's observation~\cite{Kont02}, which is
\begin{equation}
\label{eqn_diagrammatic_entropy}
\langle p_1, p_2 \rangle 
= 
\begin{cases}
(p_1 + p_2) H\left( \dfrac{p_1}{p_1 + p_2} \right) \quad  
 &\mbox{ if } p_1 + p_2 \not= 0, \\
\qquad\qquad 0 &\mbox{ if } p_1 + p_2 = 0, 
\end{cases}
\end{equation}
relating Cathelineau's vector space and entropy. 
 


\begin{lemma}
\label{lemma_Cath_properties}
Equation~\eqref{eqn_diagrammatic_entropy} satisfies symmetry, scaling, and the $2$-cocycle relation. 
\end{lemma}

\begin{proof}
We will first prove symmetry: 
\begin{align*}
\langle p_1,p_2 \rangle 
&= (p_1 + p_2) H\left( \frac{p_1}{p_1 + p_2} \right) \\
&= (p_1 + p_2) \left( - \frac{p_1}{p_1 + p_2} \log \left|\frac{p_1}{p_1 + p_2}\right| - \left( 1- \frac{p_1}{p_1 + p_2}\right)\log \left| 1 - \frac{p_1}{p_1 + p_2} \right| \right) \\
&= (p_1 + p_2) \left( - \frac{p_1}{p_1 + p_2} \log \left| \frac{p_1}{p_1 + p_2}\right| - \frac{p_2}{p_1 + p_2} \log \left| \frac{p_2}{p_1 + p_2} \right| \right) \\
&= (p_1 + p_2) \left(- \frac{p_2}{p_1 + p_2} \log \left| \frac{p_2}{p_1 + p_2} \right| - \frac{p_1}{p_1 + p_2} \log \left| \frac{p_1}{p_1 + p_2}\right| \right) \\
&= (p_1 + p_2) \left(- \frac{p_2}{p_1 + p_2} \log \left| \frac{p_2}{p_1 + p_2} \right| - \left( 1- \frac{p_2}{p_1 + p_2}\right)\log \left| 1 - \frac{p_2}{p_1 + p_2} \right| \right) \\
&= (p_1 + p_2) H\left( \frac{p_2}{p_1+p_2} \right) \\ 
&= \langle p_2, p_1\rangle.
\end{align*}
Next, we will prove scaling:
\begin{align*}
\langle c \: p_1, c \: p_2\rangle 
&= (c\: p_1 + c \: p_2) H\left( \frac{c\: p_1}{c\: p_1 + c\: p_2} \right) \\ 
&= (c\: p_1 + c \: p_2)
\left(-\frac{c\: p_1}{c\: p_1 + c\: p_2} \log \left| \frac{c\: p_1}{c\: p_1 + c\: p_2}\right| - \left( 1- \frac{c\: p_1}{c\: p_1 + c\: p_2}\right)\log \left| 1- \frac{c\: p_1}{c\: p_1 + c\: p_2} \right| \right) \\ 
&= c(p_1 + p_2)
\left(-\frac{\cancel{c}\: p_1}{\cancel{c}\: p_1 + \cancel{c}\: p_2} 
\log \left| \frac{ \cancel{c}\: p_1}{\cancel{c}\: p_1 + \cancel{c}\: p_2}\right| 
-  \frac{\cancel{c}\: p_2}{\cancel{c}\: p_1 + \cancel{c}\: p_2} 
\log \left| \frac{\cancel{c}\: p_2}{\cancel{c}\: p_1 + \cancel{c}\: p_2} \right| \right) \\
&= c(p_1 + p_2)
\left(-\frac{p_1}{p_1 + p_2} 
\log \left| \frac{p_1}{p_1 + p_2}\right| 
-  \frac{p_2}{p_1 +p_2} 
\log \left| \frac{p_2}{p_1 + p_2} \right| \right) \\
&= c(p_1 + p_2) H\left( \frac{p_1}{p_1 + p_2} \right) \\ 
&= c \langle p_1, p_2\rangle. 
\end{align*}
Finally, we will prove the $2$-cocycle condition: 
\begin{align*}
\langle p_1, p_2 \rangle &= (p_1 + p_2) H\left( \frac{p_1}{p_1 + p_2} \right)  \\ 
&= \cancel{(p_1 + p_2)} \left( 
-\frac{p_1}{\cancel{p_1 + p_2}} \log 
\left| \frac{p_1}{p_1 + p_2} \right| 
- \frac{p_2}{\cancel{p_1 + p_2}} \log 
\left| \frac{p_2}{p_1 + p_2} \right| \right) \\ 
&= - p_1 \log \left| \frac{p_1}{p_1 + p_2}\right| 
- p_2 \log \left| \frac{p_2}{p_1 + p_2}\right| \\ 
&=  - p_1 \log |p_1| - p_2 \log | p_2| 
+ (p_1 + p_2) \log | p_1 + p_2|
\end{align*}
while 
\begin{align*}
\langle p_1 + p_2, p_3 \rangle 
&= (p_1 + p_2 + p_3) H\left( \frac{p_1 + p_2}{p_1 + p_2 + p_3}\right) \\ 
&= H(p_1 + p_2) \qquad \qquad \qquad \qquad \mbox{ since } p_1 + p_2 + p_3=1 \\ 
&= -(p_1 + p_2) \log |p_1 + p_2| -(1 - (p_1 + p_2))\log |1-(p_1 + p_2)| \\
&= -(p_1 + p_2) \log |p_1 + p_2| - p_3 \log |p_3|. 
\end{align*}
So the sum is: 
\begin{align*}
\langle p_1, p_2 \rangle + \langle p_1 + p_2, p_3 \rangle &= - p_1 \log |p_1| - p_2 \log | p_2| 
+ \cancel{(p_1 + p_2) \log | p_1 + p_2|} \\
&\hspace{4mm} - \cancel{(p_1 + p_2) \log |p_1 + p_2|} - p_3 \log |p_3| \\
&=- p_1 \log |p_1| - p_2 \log | p_2|- p_3 \log |p_3| \\
&=H(p_X).
\end{align*}
On the other hand, 
\begin{align*}
\langle p_2 , p_3 \rangle 
&= (p_2 + p_3) H\left( \frac{p_2}{p_2 + p_3} \right)  \\ 
&= \cancel{(p_2 + p_3)} \left( 
-\frac{p_2}{\cancel{p_2 + p_3}} \log 
\left| \frac{p_2}{p_2 + p_3} \right| 
- \frac{p_3}{\cancel{p_2 + p_3}} \log 
\left| \frac{p_3}{p_2 + p_3} \right| \right) \\ 
&= - p_2 \log \left| \frac{p_2}{p_2 + p_3}\right| 
- p_3 \log \left| \frac{p_3}{p_2 + p_3}\right| \\ 
&=  - p_2 \log |p_2| - p_3 \log | p_3| 
+ (p_2 + p_3) \log | p_2 + p_3| 
\end{align*}
while 
\begin{align*}
\langle p_1, p_2+ p_3 \rangle 
&= (p_1 + p_2 + p_3) H\left( \frac{p_1}{p_1 + p_2 + p_3}\right) \\ 
&= H(p_1)  \qquad \qquad \qquad \qquad \qquad \mbox{ since } p_1 + p_2 + p_3=1\\ 
&= -p_1 \log |p_1| - (1 - p_1)\log |1 - p_1| \\
&= -p_1 \log |p_1| - (p_2 + p_3) \log |p_2 + p_3|.
\end{align*}
So the sum is: 
\begin{align*}
\langle p_2 , p_3 \rangle + \langle p_1, p_2+ p_3 \rangle &= - p_2 \log |p_2| - p_3 \log | p_3| + \cancel{(p_2 + p_3) \log | p_2 + p_3|} \\ 
&\hspace{4mm} -p_1 \log |p_1| - \cancel{(p_2 + p_3) \log |p_2 + p_3|} \\
&=  -p_1 \log |p_1| - p_2 \log |p_2| - p_3 \log | p_3| \\ 
&= H(p_X). 
\end{align*}
This concludes the proof.
\end{proof}





\begin{lemma}
If $p_1 + p_2 = 1$, we have 
\begin{equation}
H\left( p_1 \right)
= H\left( p_2 \right).
\end{equation}
\end{lemma}

\begin{proof}
This follows from Lemma~\ref{lemma_Cath_properties}. 
\end{proof}

Let $\rho: H(\kk) \rightarrow J(\mathbf{k})$, where $\rho(H(a)) = \langle a,1-a \rangle$. 

\begin{theorem}
\label{thm_isom_Cath_entropy}
The map $\rho$ is an isomorphism of vector spaces.
\end{theorem}



\begin{proof}
We have
\[
\rho(H(a)) = \langle a,1-a \rangle 
= \langle 1-a, a \rangle 
= \langle 1-a, 1-(1-a) \rangle 
= \rho(H(1-a))
\]
and 
\begin{align*}
\rho\left(H\left(\frac{1}{a}\right)\right) 
&= \left\langle \frac{1}{a},1-\frac{1}{a} \right\rangle 
= -\frac{1}{a} \langle -1, -a + 1\rangle
= -\frac{1}{a} \langle -1, 1 - a\rangle \\
&= -\frac{1}{a} \langle 1-a, -1 \rangle 
\stackrel{\dagger}{=} -\frac{1}{a} \langle a, 1-a \rangle  
=  -\frac{1}{a}\rho(H(a)),
\end{align*}
where $\dagger$ holds by Lemma~\ref{lemma_minus_1_a}.

Furthermore, equation \eqref{eq_four_term_relation_entropy} under $\rho$ becomes 
\begin{align*}
\rho(H(0)) 
&= \rho(H(a)) -\rho(H(b)) + a \rho\left(H\left(\frac{b}{a}\right)\right) + (1-a)\rho\left(H\left(\frac{1-b}{1-a}\right) \right) \\ 
&= \langle a, 1-a\rangle  - \langle b, 1-b \rangle + a \left\langle \frac{b}{a}, 1-\frac{b}{a} \right\rangle + (1-a) \left\langle \frac{1-b}{1-a}, 1-\frac{1-b}{1-a} \right\rangle \\ 
&= \langle a,1-a \rangle - \langle b,1-b\rangle + \langle b,a-b \rangle 
+ \langle 1-b, 1-a - (1-b)\rangle \\ 
&\stackrel{\ddagger}{=} \langle a,1-a \rangle - \langle b,1-b\rangle - \langle a,b-a \rangle 
+ \langle 1-b, b-a \rangle, 
\end{align*}
where $\ddagger$ holds due to
\[
\langle b,a-b \rangle 
= b \left\langle 1, \frac{a}{b} - 1 \right\rangle
= -b \left\langle 1-\frac{a}{b},-1 \right\rangle 
= -b \left\langle \frac{a}{b},1-\frac{a}{b} \right\rangle 
= -\langle a, b-a \rangle
\] via Lemma~\ref{lemma_minus_1_a}. Now, replacing $a$ with $a$, $b$ with $b-a$, and $c=1-b$ in the $2$-cocycle relation~\eqref{item:Cath_cocycle}, we see that 
\begin{align*}
\langle a, b-a+1-b\rangle 
&+ \langle b-a, 1-b \rangle 
- \langle a+b -a, 1-b \rangle 
- \langle a, b-a \rangle \\
&= 
\langle a, 1-a \rangle + \langle 1-b, b-a \rangle - \langle a, b-a \rangle  
- \langle b,1-b \rangle =0.
\end{align*}
So 
\begin{align*}
\rho(0) 
&= \rho(H(0)) = \rho\left(H(a)-H(b) + a H\left(\frac{b}{a}\right) + (1-a)H\left(\frac{1-b}{1-a}\right)\right) \\
&=  \langle a,1-a \rangle - \langle b,1-b\rangle - \langle a,b-a \rangle 
+ \langle 1-b, b-a \rangle 
= 0. 
\end{align*}
Conversely, let $\lambda: J(\mathbf{k})\rightarrow H(\mathbf{k})$, where 
$\lambda(\langle a,b \rangle) 
= (a+b) H \left( \frac{a}{a+b} \right)$ if $a+b\not=0$, and $\lambda(\langle a,b\rangle)=0$ if $a+b=0$.
So we have 
\begin{align*}
\lambda(\langle a,b \rangle) 
&= (a+b)H\left( \frac{a}{a+b} \right) \\
&= (a+b)H\left( 1- \frac{a}{a+b} \right) \mbox{ by the first equality in Lemma~\ref{lemma_entropy_basic}} \\
&= (a+b)H\left( \frac{\cancel{a} + b - \cancel{a}}{a+b} \right) \\ 
&= (b+a)H\left( \frac{b}{a+b} \right) \\
&= \lambda(\langle b,a \rangle).
\end{align*}
Secondly, $\lambda(\langle ca, cb \rangle)= c\lambda(\langle a,b \rangle)$ holds by scaling in Lemma~\ref{lemma_Cath_properties}. 

Lastly, we have 
\begin{align*}
0 &= \lambda(0) \\ 
&= \lambda(\langle a,b+c\rangle +\langle b,c \rangle - \langle a+b,c \rangle - \langle a,b\rangle) \\ 
&= \lambda(\langle a,b+c\rangle) +\lambda (\langle b,c \rangle) - \lambda (\langle a+b,c \rangle) - \lambda (\langle a,b\rangle) \\ 
&= (a+b+c) H\left( \frac{a}{a+b+c} \right) 
+ (b+c) H\left( \frac{b}{b+c} \right)
- (a+b+c) H\left( \frac{a+b}{a+b+c} \right) 
- (a+b) H\left( \frac{a}{a+b} \right) \\
&= (a+b+c) H\left( \frac{a}{a+b+c} \right) 
+ (b+c) H\left( \frac{b}{b+c} \right)
- (a+b+c) H\left( \frac{a+b}{a+b+c} \right) 
- (a+b) H\left( \frac{b}{a+b} \right)
\end{align*}
since $H\left( \frac{a}{a+b} \right)= H\left( \frac{b}{a+b} \right)$ by the first equality in Lemma~\ref{lemma_entropy_basic}.

Let 
\[
u= \frac{a}{a+b+c} 
\quad 
\mbox{ and }
\quad 
v = \frac{c}{a+b+c}. 
\]
Then 
\begin{align*}
(a+b+c) & H\left( \frac{a}{a+b+c} \right) 
+ (b+c) H\left( \frac{b}{b+c} \right)
- (a+b+c) H\left( \frac{a+b}{a+b+c} \right) 
- (a+b) H\left( \frac{b}{a+b} \right) \\ 
&= (a+b+c) 
\left( H(u) + (1-u)H\left( \frac{1-u-v}{1-u}\right) - H(1-v) - (1-v)H\left( \frac{1-u-v}{1-v}  \right) 
\right) \\ 
&= (a+b+c) 
\left( H(u) + (1-u)H\left( \frac{v}{1-u}\right) - H(1-v) - (1-v)H\left( \frac{u}{1-v}  \right) 
\right)
\end{align*}
by the first equality in Lemma~\ref{lemma_entropy_basic}. 
Now let $p=u$ and $q=1-v$. Then 
\begin{align*}
H(u) &+ (1-u)H\left( \frac{v}{1-u}\right) - H(1-v) - (1-v)H\left( \frac{u}{1-v}  \right) \\
&= H(p) + (1-p)H\left( \frac{1-q}{1-p} \right) - H(q) - q H\left( \frac{p}{q} \right)  \\
&= H(p) + (1-p)H\left( \frac{1-q}{1-p} \right) - H(q) + \cancel{q} \frac{p}{\cancel{q}}  H\left( \frac{q}{p} \right)  \\
&= H(p) + (1-p)H\left( \frac{1-q}{1-p} \right) - H(q) +  p H\left( \frac{q}{p} \right) \\ 
&= 0  
\end{align*}
by the second equality in Lemma~\ref{lemma_entropy_basic} and \eqref{eq_four_term_relation_entropy}.
\end{proof}


In characteristic $p>0$, we have 
\begin{align*}
\rho\left(\sum_{n=1}^p H(n)\right) 
&= \sum_{n=1}^p \rho( H(n)) 
= \sum_{n=1}^p \langle n, 1-n \rangle 
= \sum_{n=1}^p \langle 1-n, -1 \rangle \\ 
&= - \sum_{n=1}^p \langle n-1, 1 \rangle 
= - \sum_{n=1}^p \langle 1, n-1 \rangle 
= - \sum_{n=1}^p \langle 1, n \rangle 
= 0,
\end{align*}
where the last equality holds by \eqref{eqn_char_p_Cathelineau}. 
 

 

Entropy is related to the pair of symbols $\langle \cdot , \cdot \rangle$ via the following: 
\begin{proposition}
\label{thm_entropy_inner_prod}
Let $p_1,\ldots, p_n$ be a finite probability distribution on a finite set $X=\{x_1,\ldots, x_n \}$.
Then for $n\geq 3$, 
\begin{equation}
\label{eqn_relating_entropy_inner}
H(p_X) = H(p_1,\ldots, p_n) = \sum_{j=2}^n 
\left\langle \sum_{i=1}^{j-1} p_i, p_j \right\rangle. 
\end{equation}
\end{proposition}
Expanding out~\eqref{eqn_relating_entropy_inner}, we have 
\[
H(p_1,\ldots, p_n) = \langle p_1, p_2 \rangle + \langle p_1 + p_2, p_3 \rangle 
+ \langle p_1 + p_2 + p_3 , p_4\rangle 
+ \ldots
+ \langle p_1 + p_2 + \ldots + p_{n-1} , p_n \rangle. 
\]


\begin{proof}
Since  
\begin{align*}
\langle p_1, p_2 \rangle 
&+ \langle p_1 + p_2, p_3 \rangle 
+ \langle p_1 + p_2 + p_3 , p_4\rangle 
+ \ldots 
+ \langle p_1 + p_2 + \ldots + p_{n-1} , p_n \rangle \\
&= (p_1+ p_2) H\left( \frac{p_1}{p_1 + p_2} \right) + 
(p_1+ p_2+p_3) H\left( \frac{p_1+p_2}{p_1 + p_2 + p_3} \right) + \ldots + \\  
&\hspace{4mm}+ 
(p_1+ p_2+p_3+ \ldots + p_n) H\left( \frac{p_1+p_2+ \ldots + p_{n-1}}{p_1 + p_2 + p_3 +\ldots + p_n} \right) \\
&= (p_1 + p_2) \left(-\frac{p_1}{p_1 + p_2} \log \left| \frac{p_1}{p_1 + p_2} \right|
- \left(1 - \frac{p_1}{p_1 + p_2}\right) 
\log \left| 1 - \frac{p_1}{p_1 + p_2}\right| \right) \\
&\hspace{4mm}+ (p_1 + p_2 + 
    p_3) \left(-\frac{p_1 + p_2}{p_1 + p_2 + p_3} \log \left| \frac{p_1 + p_2}{
      p_1 + p_2 + p_3}\right| \right. \\
&\hspace{4mm}\left. - \left(1 - \frac{p_1 + p_2}{p_1 + p_2 + p_3}\right) \log \left|
      1 - \frac{p_1 + p_2}{p_1 + p_2 + p_3}\right| 
      \right)  + \ldots + \\
&\hspace{4mm}+ (p_1 + p_2 + p_3 + \ldots + 
    p_n) \left(-\frac{p_1 + p_2 + \ldots + p_{n-1}}{p_1 + p_2 + p_3 + \ldots + p_n} \log \left| \frac{p_1 + p_2 + \ldots + p_{n-1}}{p_1 + p_2 + p_3 + \ldots +  p_n} \right| \right. \\
&\hspace{4mm}\left. - \left(1 - \frac{p_1 + p_2 + \ldots + p_{n-1}}{p_1 + p_2 + p_3 + \ldots + p_n}\right) \log \left|
      1 - \frac{p_1 + p_2 + \ldots + p_{n-1}}{p_1 + p_2 + p_3 + \ldots + p_n}\right|
      \right) \\ 
&= \cancel{(p_1 + p_2)} \left(-\frac{p_1}{\cancel{p_1 + p_2}} \log \left| \frac{p_1}{p_1 + p_2} \right|
-  \frac{p_2}{\cancel{p_1 + p_2}} 
\log \left| \frac{p_2}{p_1 + p_2}\right| \right) \\
&\hspace{4mm}+ \cancel{(p_1 + p_2 + 
    p_3)} \left(-\frac{p_1 + p_2}{\cancel{p_1 + p_2 + p_3}} \log \left| \frac{p_1 + p_2}{
      p_1 + p_2 + p_3}\right| - \frac{p_3}{\cancel{p_1 + p_2 + p_3}} \log \left|
       \frac{p_3}{p_1 + p_2 + p_3}\right|
      \right)  \\
&\hspace{4mm}+ \ldots + \cancel{(p_1 + p_2 + \ldots + p_n)} \left(-\frac{p_1 + p_2 + \ldots + p_{n-1}}{\cancel{p_1 + p_2 + p_3 +\ldots + p_n}} \log \left|\frac{p_1 + p_2 + \ldots + p_{n-1}}{p_1 + p_2 + p_3 + \ldots + p_n} \right| \right. \\
&\hspace{4mm}\left. - \frac{p_n}{\cancel{p_1 + p_2 + p_3 + \ldots + p_n}} \log \left|
      \frac{p_n}{p_1 + p_2 + p_3 + \ldots + p_n}\right|
      \right) \\ 
&=  -p_1\log \left| \frac{p_1}{p_1 + p_2} \right| - p_2  
\log \left| \frac{p_2}{p_1 + p_2}\right| 
- (p_1 + p_2) 
\log \left| \frac{p_1 + p_2}{
 p_1 + p_2 + p_3}\right|  
- p_3 \log \left|
 \frac{p_3}{p_1 + p_2 + p_3}\right| \\
&\hspace{4mm}   - \ldots 
- (p_1 + p_2 + \ldots + p_{n-1}) \log \left|
      \frac{p_1 + p_2 + \ldots + p_{n-1}}{p_1 + p_2 + p_3 + \ldots + p_n}\right| \\ 
&\hspace{4mm} - p_n \log \left|
      \frac{p_n}{p_1 + p_2 + p_3 + \ldots + p_n}\right| \\ 
&= -p_1 \log |p_1| - p_2 \log | p_2| 
+ \cancel{(p_1 + p_2) \log |p_1 + p_2|}
- \cancel{(p_1 + p_2) \log |p_1 + p_2|} 
- p_3 \log |p_3| \\
&\hspace{4mm}
+ \cancel{(p_1 + p_2 + p_3) \log |p_1 + p_2 + p_3|}
- \cancel{(p_1 + p_2 + p_3) \log |p_1 + p_2 + p_3|} + \ldots + \\ 
&\hspace{4mm} + 
(p_1 +p_2 + \ldots + p_{n-1}) \cancel{\log |p_1 + p_2 + \ldots + p_{n-1}|} \\ 
&\hspace{4mm} - 
(p_1 +p_2 + \ldots + p_{n-1}) \cancel{\log |p_1 + p_2 + \ldots + p_{n-1}|} - p_n \log |p_n| \\ 
&\hspace{4mm} + (\underbrace{p_1 + p_2 + p_3 + \ldots + p_n}_{1}) \log |\underbrace{p_1 + p_2 + p_3 + \ldots + p_n}_{1}| \\
&= -p_1 \log |p_1| - p_2 \log |p_2| - p_3 \log |p_3| -\ldots - p_n \log |p_n| = H(p_X). 
\end{align*}
This concludes the proof.
\end{proof}




\section{Joint entropy}

\subsection{Joint entropy of \texorpdfstring{$2$}{2} random variables}
\label{subsection:joint_entropy}


Let $X$ be a discrete random variable associated with probabilities $p_1,\ldots, p_n$. Recall Shannon entropy in~\eqref{eq_shannon}.
Let $Y$ be a discrete random variable associated with probabilities $q_1,\ldots, q_k$. Then the joint entropy of $X$ and $Y$ is defined to be 
\begin{equation}
\label{eqn:joint_entropy_prob}
H(p_X,p_Y) =  -\sum_{i=1}^{n} \sum_{j=1}^{m} p_{ij} \log p_{ij},
\end{equation}
where $p_{ij}= P(X = p_i, Y = q_j)$, the probability that the random variables $X=p_i$ and $Y=q_j$.
Then similar to ~\eqref{eqn_diagrammatic_entropy}, we have 
\begin{equation}
\label{eqn:joint_entropy_inner_product}
\langle p_{ij}, p_{\ell k} \rangle = 
\begin{cases} 
(p_{ij}+p_{\ell k}) H\left( \dfrac{p_{ij}}{p_{ij}+p_{\ell k}} \right) &\mbox{ if } p_{ij}+p_{\ell k} \not=0, \\
\hspace{1.75cm} 0 &\mbox{ if } p_{ij}+p_{\ell k} = 0.
\end{cases}
\end{equation}

\begin{proposition}
\label{proposition:joint_entropy_inner_product}
Equation~\eqref{eqn:joint_entropy_inner_product} satisfies symmetry, scaling, and the $2$-cocycle relation for the vector space $J(\mathbf{k})$. 
\end{proposition}

The proof of Proposition~\ref{proposition:joint_entropy_inner_product} is similar to the proof of Lemma~\ref{lemma_Cath_properties}.
 
Similar in spirit to Proposition~\ref{thm_entropy_inner_prod} and in order to provide a motivation for Theorem~\ref{thm_relating_entropy_inner_2}, we have the following: 
\begin{proposition}
\label{prop_joint_entropy_two}
Let $p_1, p_2$ and $q_1, q_2$ be finite probability distributions on sets $X=\{ x_1, x_2\}$ and $Y=\{ y_1, y_2\}$, respectively. Let $p_{ij}$ be the probability of obtaining $p_i$ and $q_j$. Then a relation between entropy and Cathelineau's vector space is
\begin{equation}
H(p_X,p_Y) = \langle p_{11},p_{21}\rangle + \langle p_{12},p_{22} \rangle +\langle p_{11}+p_{21},p_{12}+p_{22}\rangle. 
\end{equation}
\end{proposition}

\begin{proof}
Using \eqref{eqn:joint_entropy_inner_product}, we have: 
\begin{align*}
\langle p_{11}, p_{21} \rangle &= (p_{11}+p_{21}) H\left(\frac{p_{11}}{p_{11}+p_{21}} \right), \\
\langle p_{12}, p_{22} \rangle &= (p_{12} + p_{22}) H\left(\frac{p_{12}}{p_{12}+p_{22}} \right), \\
\langle p_{11}+p_{21}, p_{12}+p_{22} \rangle &= (p_{11}+p_{21}+p_{12}+p_{22}) H\left(\frac{p_{11}+p_{21}}{p_{11}+p_{21}+p_{12}+p_{22}} \right). \\
\end{align*}
So 
\begin{align*}
\langle p_{11}&,p_{21}\rangle + \langle p_{12},p_{22} \rangle +\langle p_{11}+p_{21},p_{12}+p_{22}\rangle
= (p_{11}+p_{21}) H\left(\frac{p_{11}}{p_{11}+p_{21}} \right) \\
&\hspace{4mm}+ (p_{12} + p_{22}) H\left(\frac{p_{12}}{p_{12}+p_{22}} \right) + (p_{11}+p_{21}+p_{12}+p_{22}) H\left(\frac{p_{11}+p_{21}}{p_{11}+p_{21}+p_{12}+p_{22}} \right) \\
&= (p_{11} + p_{21}) \left(-\frac{p_{11}}{p_{11} + p_{21}} \log \left( \frac{p_{11}}{p_{11} + p_{21}} \right) 
      - \left(1 - \frac{p_{11}}{p_{11} + p_{21}}\right) \log \left( 1 - \frac{p_{11}}{p_{11} + p_{21}}\right) \right) \\
&\hspace{4mm}+
 (p_{12} + p_{22}) \left(-\frac{p_{12}}{p_{12} + p_{22}} \log \left( \frac{p_{12}}{p_{12} + p_{22}} \right) 
      - \left(1 - \frac{p_{12}}{p_{12} + p_{22}}\right) 
      \log \left( 1 - \frac{p_{12}}{p_{12} + p_{22}}\right) \right) \\
&\hspace{4mm}+
 (p_{11} + p_{21} + p_{12} + p_{22}) 
 \left( -\frac{p_{11} + p_{21}}{p_{11} + p_{21} + p_{12} + p_{22}} 
 \log\left( \frac{p_{11} + p_{21}}{p_{11} + p_{21} + p_{12} + p_{22}} \right) \right. \\
&\hspace{4mm} \left. - \left(1 - 
\frac{p_{11} + p_{21}}{p_{11} + p_{21} + p_{12} + p_{22}}\right) 
       \log \left( 
      1 - \frac{p_{11} + p_{21}}{p_{11} + p_{21} + p_{12} + p_{22}}\right) \right) \\ 
&= \cancel{(p_{11} + p_{21})} 
\left(-\frac{p_{11}}{\cancel{p_{11} + p_{21}}} \log \left( \frac{p_{11}}{p_{11} + p_{21}} \right) 
      - \frac{p_{21}}{\cancel{p_{11} + p_{21}}} \log \left(\frac{p_{21}}{p_{11} + p_{21}}\right) \right) \\
&\hspace{4mm}+
 \cancel{(p_{12} + p_{22})} \left(-\frac{p_{12}}{\cancel{p_{12} + p_{22}}} \log \left( \frac{p_{12}}{p_{12} + p_{22}} \right) 
      - \frac{p_{22}}{\cancel{p_{12} + p_{22}}} 
      \log \left(\frac{p_{22}}{p_{12} + p_{22}}\right) \right) \\
&\hspace{4mm}+
 \cancel{(p_{11} + p_{21} + p_{12} + p_{22})} 
 \left( -\frac{p_{11} + p_{21}}{
 \cancel{p_{11} + p_{21} + p_{12} + p_{22}}} 
 \log\left( \frac{p_{11} + p_{21}}{p_{11} + p_{21} + p_{12} + p_{22}} \right) \right. \\
&\hspace{4mm} \left. - 
\frac{p_{12} + p_{22}}{
\cancel{p_{11} + p_{21} + p_{12} + p_{22}}}  
       \log \left( 
     \frac{p_{12} + p_{22}}{p_{11} + p_{21} + p_{12} + p_{22}}\right) \right) \\ 
&=  
- p_{11} \log \left( \frac{p_{11}}{p_{11} + p_{21}} \right) 
      - p_{21} \log \left(\frac{p_{21}}{p_{11} + p_{21}}\right) - p_{12} \log \left( \frac{p_{12}}{p_{12} + p_{22}} \right) - p_{22} 
      \log \left(\frac{p_{22}}{p_{12} + p_{22}}\right) \\
&\hspace{4mm} 
  - (p_{11} + p_{21}) 
 \log\left( \frac{p_{11} + p_{21}}{p_{11} + p_{21} + p_{12} + p_{22}} \right) - (p_{12} + p_{22}) 
\log \left( 
     \frac{p_{12} + p_{22}}{p_{11} + p_{21} + p_{12} + p_{22}}\right) \\ 
&= - p_{11} \log ( p_{11} ) 
      - p_{21} \log (p_{21}) 
      + \cancel{(p_{11} + p_{21}) \log (p_{11} + p_{21})} \\ 
&\hspace{4mm}- p_{12} \log ( p_{12} ) - p_{22} 
      \log (p_{22}) 
      + \cancel{(p_{12} + p_{22}) \log (p_{12} + p_{22})} \\
&\hspace{4mm} 
- \cancel{(p_{11} + p_{21})\log\left( p_{11} + p_{21} \right)} - 
\cancel{(p_{12} + p_{22}) \log (p_{12} + p_{22})} \\
&\hspace{4mm}
+ (\underbrace{p_{11} + p_{21} + p_{12} + p_{22}}_{1}) \log (\underbrace{p_{11} + p_{21} + p_{12} + p_{22}}_{1}) \\
&=- p_{11} \log ( p_{11} ) 
      - p_{21} \log (p_{21}) - p_{12} \log ( p_{12} ) - p_{22} 
      \log (p_{22}) = H(p_X,p_Y).
\end{align*}


\end{proof}

More generally, we have the following: 
\begin{theorem}
\label{thm_relating_entropy_inner_2}
Let $p_1,\ldots, p_n$ and $q_1,\ldots, q_m$ be finite probability distributions on finite sets $X=\{x_1,\ldots, x_n \}$ and $Y=\{y_1, \ldots, y_m \}$, respectively. 
Let $p_{ij}=P(p_i,q_j)$, the probability of obtaining $p_i$ and $q_j$. 
Then 
\begin{equation}
H(p_X, p_Y) = \sum_{i=1}^n \sum_{k=1}^{m-1} \langle  \sum_{j=1}^k p_{ij}, p_{i,j+1} \rangle + \sum_{k=1}^{n-1} \langle \sum_{i=1}^{k} \sum_{j=1}^{m} p_{ij}, \sum_{\ell = 1}^m p_{i\ell} \rangle. 
\end{equation}
\end{theorem}

The proof of Theorem~\ref{thm_relating_entropy_inner_2} is a direct calculation, similar to the proof of Proposition~\ref{thm_entropy_inner_prod} and Proposition~\ref{prop_joint_entropy_two}. We give a diagrammatical calculus perspective of entropy in Section~\ref{section:Shannon_entropy}, which originated in~\cite{IK24_dilogarithms_entropy}. Using this, one can easily prove Theorem~\ref{thm_relating_entropy_inner_2}.




\subsection{Entropy for \texorpdfstring{$u$}{u} random variables}

One may extend \eqref{eqn:joint_entropy_prob} as follows. Consider the finite set $X_i = \{x_{i_1},\ldots, x_{i_{k_i}} \}$, where $1\leq i\leq u$. Let $p_{X_i}$ be a probability distribution on $X_i$ that associates probabilities $p_{i_1},\ldots, p_{i_{k_i}}$ to the points $x_{i_1},\ldots, x_{i_{k_i}}$, respectively, where the sum $\displaystyle{\sum_{j=1}^{k_i}} p_{i_j}=1$, $0< p_{i_j}<1$, $1\leq i\leq u$.


Write 
\[ I_{J} := 1_{j_1}2_{j_2}\cdots u_{j_u}, 
\hspace{4mm} 
J := (j_1,j_2,\ldots, j_u), 
\hspace{4mm} 
\mbox{ and }
\hspace{4mm} 
K := (k_1,\ldots, k_u).
\] 
Let the joint probability for random variables $\mathfrak{X}_1, \ldots, \mathfrak{X}_u$ be 
\[ 
p_{I_{J}} 
= p_{1_{j_1}2_{j_2}\cdots u_{j_u}} 
= 
P(\mathfrak{X}_1 = p_{1_{j_1}}, \mathfrak{X}_2 = p_{2_{j_2}}, \ldots, \mathfrak{X}_u = p_{u_{j_u}}).
\]
We obtain the joint entropy
\begin{equation}
\label{eqn:joint_entropy_generalized}
H(p_{X_1},\ldots, p_{X_u}) 
= 
-\sum_{j_1=1}^{k_1}\sum_{j_2=1}^{k_2} \cdots \sum_{j_u=1}^{k_u} 
p_{1_{j_1}2_{j_2}\cdots u_{j_u}} 
\log (p_{1_{j_1}2_{j_2}\cdots u_{j_u}}),
\end{equation}
or more compactly, we write 
\begin{equation}
\label{eqn_joint_entropy_u_cmpt}
H(p_{X_1},\ldots, p_{X_u}) = 
- \sum_{J = \mathbf{1}}^{K} 
p_{I_{J}} \log(p_{I_{J}}).
\end{equation}


\begin{proposition}
\label{proposition:joint_entropy_general_inner_product}
Under the conditions as above, we have 
\begin{equation}
\label{eqn:joint_entropy_u_rv}
\langle p_{I_{J}}, p_{I_{J'}} \rangle = 
\left( p_{I_{J}} + p_{I_{J'}} \right) 
H\left( \frac{p_{I_{J}}}{p_{I_{J}} + p_{I_{J'}}} \right), 
\end{equation}
which satisfies symmetry, scaling, and the $2$-cocycle relation for $J(\kk)$ in Section~\ref{subsection:Cathelineau_k_vs}.
\end{proposition}

The proof of Proposition~\ref{proposition:joint_entropy_general_inner_product} is similar to the proof of Lemma~\ref{lemma_Cath_properties}.



 
\subsection{Rescaling a finite probability distribution by a scalar}
\label{section:scaling_scalar}
Rescaling the pair of symbols $\langle c p_1, cp_2\rangle = c \langle p_1, p_2 \rangle$ corresponds to rescaling entropy. 
In our diagrammatic story, it will correspond to a red wavy line, which is to the left of our graphical network.
  


\section{Diagrammatics of Shannon entropy}
\label{section:Shannon_entropy}
Let $\kk=\mathbb{R}$ and let $\kk^*$ be the set of units in $\kk$. In this section, we discuss a new perspective of entropy using diagrammatics, as introduced in~\cite{IK24_dilogarithms_entropy}.

We decorate the boundary of a network of lines from values in $\kk$. 
Figure~\ref{fig_1001} gives us the properties of black and red (network) lines. Black lines are additive and take values in $\kk$ while red wavy lines are multiplicative and take values in $\kk^*$. Whenever two additive (black) lines merge (at the intersections of additive lines), we evaluate at the additive vertex using the two symbols $\langle a,b \rangle$, see Figure~\ref{fig_1001} top left. Whenever two additive lines split, we evaluate at the additive vertex the pair of symbols $\langle a,b\rangle$, but with a negative sign:
$-\langle a,b\rangle$, see Figure~\ref{fig_1001} top middle. We can also have a virtual crossing, where two lines cross but their intersection is said to be virtual, see Figure~\ref{fig_1001} top right. In the second row of Figure~\ref{fig_1001}, we see that red wavy (multiplicative) lines have a conormal direction. In particular, whenever an additive line passes through a multiplicative line of weight $c\in \mathbf{k}^*$, we rescale the additive line from $a\mapsto ca$, see Figure~\ref{fig_1001} bottom left. Similar to the additive lines, whenever multiplicative lines merge, we multiply the two weights, Figure~\ref{fig_1001} bottom middle. Finally, we introduce a red dot to represent the switching of normal co-orientations, see Figure~\ref{fig_1001} bottom right.


\input{fig_1001}

\input{fig_1007}

When additive lines are oriented downwards, we keep the vertices to stay the same but rotate the additive vertex clockwise or counterclockwise, resulting in a contribution of $\langle a,b\rangle$ or $-\langle a,b\rangle$. See Figure~\ref{fig_1007}.

\input{fig_1002}

Additive and multiplicative lines satisfy natural isotopy relations, as well as the Cathelineau's $2$-cocycle relation in \eqref{item:Cath_cocycle}. The relation $\brak{a,b+c} + \brak{b,c}   = \brak{a+b,c} + \brak{a,b}$ is satisfied by Figure~\ref{fig_1002} top left by summing over the additive vertices. Figure~\ref{fig_1002} top right satisfies the symmetry relation $\langle a,b \rangle = \langle b,a \rangle$. Figure~\ref{fig_1002} bottom left satisfies the scaling relation $\langle ca, cb \rangle = c\langle a,b\rangle$ since the additive vertex on the left-hand side of the diagram contributes $\langle ca, cb \rangle$ since the additive lines have weights $ca$ and $cb$ while the additive vertex on the right-hand side of the diagram contributes $\langle a,b\rangle$, which we then rescale by the scalar $c$. For Figure~\ref{fig_1002} bottom right, the diagrams show that we can pull apart additive and multiplicative lines that have crossed virtually.

 
% 
% {\color{pink} Certain blue lines can also be used to absorb all the additive vertices.}

\input{fig8_001}

Furthermore, in Figure~\ref{fig_1002} top left, if the values $a,b,c$ sum to $1$, i.e., $a+ b+ c = 1$, then we get Shannon entropy if all the lines in a diagram merge and we sum over all the additive vertex contributions. That is, consider Figure~\ref{fig8_001}.
Although Proposition~\ref{prop:strands_merging_entropy} holds for $n$ strands merging into one, we first prove the case for three additive lines. 

\begin{proposition}
\label{prop:strands_merging_entropy}
Each side in the equality in Figure~\ref{fig8_001} evaluates to Shannon entropy in \eqref{eq_shannon}.     
\end{proposition}

\begin{proof}
Recall from~\cite{IK24_dilogarithms_entropy} that a relation between diagrammatics and scaled entropy is \eqref{eqn_diagrammatic_entropy}.
The additive vertex in the blue circle gives a contribution of $\langle p_1, p_2 \rangle$.
The additive vertex in the dotted orange circle gives a contribution of 
$\langle p_1 + p_2, p_3 \rangle$. 
So the sum of the two additive vertices gives $\langle p_1, p_2 \rangle + \langle p_1 + p_2, p_3 \rangle$. 

On the other hand, the additive vertex in the purple dashed-dotted circle provides $\langle p_2 , p_3 \rangle$. 
The additive vertex in the green dotted circle contributes $\langle p_1, p_2+ p_3 \rangle$. 
Combining the two additive vertices on the right hand side, we have $\langle p_2 , p_3 \rangle + \langle p_1, p_2+ p_3 \rangle$. 
We see that 
\[ 
\langle p_1, p_2 \rangle + \langle p_1 + p_2, p_3 \rangle = \langle p_2 , p_3 \rangle + \langle p_1, p_2+ p_3 \rangle,
\] 
which is the $2$-cocycle condition for Cathelineau's vector space. So by the proof of Lemma~\ref{lemma_Cath_properties}, each side sums to $H(p_X)$. 
\end{proof}

Now consider Figure~\ref{fig_1005}.

\begin{proposition}
\label{proposition:entropy_4_strands}
Figure~\ref{fig_1005} corresponds to entropy $H(p_X)=  -p_1 \log |p_1| - p_2 \log |p_2| - p_3 \log | p_3| -p_4 \log |p_4|$. 
\end{proposition}

\begin{proof}
Here, we will compute only the left hand side in Figure~\ref{fig_1005}, where $-p_1 \log p_1 - p_2 \log p_2 - p_3 \log (p_3) - p_4 \log p_4=H(p_X)$. We will leave the other diagrams as an exercise for the reader. 
From bottom to top, the additive vertices contribute 
\begin{align*}
\langle p_1, p_2 \rangle 
&= (p_1+ p_2) H\left( \frac{p_1}{p_1 + p_2} \right) \\ 
\langle p_1 + p_2, p_3 \rangle &= (p_1+ p_2+p_3) H\left( \frac{p_1+p_2}{p_1 + p_2 + p_3} \right) \\ 
\langle p_1 + p_2 +p_3, p_4 \rangle &= (p_1+ p_2+p_3+ p_4) H\left( \frac{p_1+p_2+ p_3}{p_1 + p_2 + p_3 + p_4} \right),
\end{align*}
respectively. 
Combining the three equations above gives
\begin{align*}
\langle p_1, & \: p_2 \rangle + 
\langle p_1 + p_2, p_3 \rangle  + 
\langle p_1 + p_2 +p_3, p_4 \rangle \\
&= (p_1+ p_2) H\left( \frac{p_1}{p_1 + p_2} \right) + 
(p_1+ p_2+p_3) H\left( \frac{p_1+p_2}{p_1 + p_2 + p_3} \right) \\  
&\hspace{4mm}+ 
(p_1+ p_2+p_3+ p_4) H\left( \frac{p_1+p_2+ p_3}{p_1 + p_2 + p_3 + p_4} \right) \\
&= (p_1 + p_2) \left(-\frac{p_1}{p_1 + p_2} \log \left| \frac{p_1}{p_1 + p_2} \right|
- \left(1 - \frac{p_1}{p_1 + p_2}\right) 
\log \left| 1 - \frac{p_1}{p_1 + p_2}\right| \right) \\
&\hspace{4mm}+ (p_1 + p_2 + 
    p_3) \left(-\frac{p_1 + p_2}{p_1 + p_2 + p_3} \log \left| \frac{p_1 + p_2}{
      p_1 + p_2 + p_3}\right| \right. \\
&\hspace{4mm}\left. - \left(1 - \frac{p_1 + p_2}{p_1 + p_2 + p_3}\right) \log \left|
      1 - \frac{p_1 + p_2}{p_1 + p_2 + p_3}\right|
      \right) \\
&\hspace{4mm}+ (p_1 + p_2 + p_3 + 
    p_4) \left(-\frac{p_1 + p_2 + p_3}{p_1 + p_2 + p_3 + p_4} \log \left| \frac{p_1 + p_2 + p_3}{p_1 + p_2 + p_3 + p_4} \right| \right. \\
&\hspace{4mm}\left. - \left(1 - \frac{p_1 + p_2 + p_3}{p_1 + p_2 + p_3 + p_4}\right) \log \left|
      1 - \frac{p_1 + p_2 + p_3}{p_1 + p_2 + p_3 + p_4}\right|
      \right) \\ 
&= \cancel{(p_1 + p_2)} \left(-\frac{p_1}{\cancel{p_1 + p_2}} \log \left| \frac{p_1}{p_1 + p_2} \right|
-  \frac{p_2}{\cancel{p_1 + p_2}} 
\log \left| \frac{p_2}{p_1 + p_2}\right| \right) \\
&\hspace{4mm}+ \cancel{(p_1 + p_2 + 
    p_3)} \left(-\frac{p_1 + p_2}{\cancel{p_1 + p_2 + p_3}} \log \left| \frac{p_1 + p_2}{
      p_1 + p_2 + p_3}\right| \right. \\
&\hspace{4mm}\left. 
- \frac{p_3}{\cancel{p_1 + p_2 + p_3}} \log \left|
       \frac{p_3}{p_1 + p_2 + p_3}\right|
      \right) \\
&\hspace{4mm}+ \cancel{(p_1 + p_2 + p_3 + 
    p_4)} \left(-\frac{p_1 + p_2 + p_3}{\cancel{p_1 + p_2 + p_3 + p_4}} \log \left| \frac{p_1 + p_2 + p_3}{p_1 + p_2 + p_3 + p_4} \right| \right. \\
&\hspace{4mm}\left. - \frac{p_4}{\cancel{p_1 + p_2 + p_3 + p_4}} \log \left|
      \frac{p_4}{p_1 + p_2 + p_3 + p_4}\right|
      \right) \\ 
&=  -p_1\log \left| \frac{p_1}{p_1 + p_2} \right| - p_2  
\log \left| \frac{p_2}{p_1 + p_2}\right| 
- (p_1 + p_2) 
\log \left| \frac{p_1 + p_2}{
 p_1 + p_2 + p_3}\right|  
- p_3 \log \left|
 \frac{p_3}{p_1 + p_2 + p_3}\right| \\
&\hspace{4mm} - (p_1 + p_2 + p_3) \log \left| \frac{p_1 + p_2 + p_3}{p_1 + p_2 + p_3 + p_4} \right|   - p_4 \log \left|
      \frac{p_4}{p_1 + p_2 + p_3 + p_4}\right| \\ 
&= -p_1 \log |p_1| - p_2 \log |p_2| 
+ \cancel{(p_1 + p_2) \log |p_1 + p_2|}
- \cancel{(p_1 + p_2) \log |p_1 + p_2|} 
- p_3 \log |p_3| \\
&\hspace{4mm}
+ \cancel{(p_1 + p_2 + p_3) \log |p_1 + p_2 + p_3|}
- \cancel{(p_1 + p_2 + p_3) \log |p_1 + p_2 + p_3|} \\ 
&\hspace{4mm}- 
 p_4 \log |p_4| + (\underbrace{p_1 + p_2 + p_3 + p_4}_{1}) \log |\underbrace{p_1 + p_2 + p_3 + p_4}_{1}| \\
&= -p_1 \log |p_1| - p_2 \log |p_2| - p_3 \log |p_3| - p_4 \log |p_4| = H(p_X). 
\end{align*}
This concludes the proof.
\end{proof}

\input{fig_1005}

\input{fig_1006}

\begin{proposition}
\label{prop_entropy_general_case_n}
For each diagram in Figure~\ref{fig_1006}, the additive vertices sum to Shannon entropy. 
\end{proposition}

The proof of Proposition~\ref{prop_entropy_general_case_n} is analogous to Propositions~\ref{prop:strands_merging_entropy} and \ref{proposition:entropy_4_strands}.


 

Now, recall equation \eqref{eqn_diagrammatic_entropy} and Cathelineau's symmetry relation~\eqref{item:symm}. They give us the following correspondence: 
\begin{equation}
\label{entropy_special_case_n2}
\langle a,b\rangle = \langle b,a\rangle  \Leftrightarrow (a+b)H\left(\frac{a}{a+b}\right) = (a+b)H\left(\frac{b}{a+b}\right).
\end{equation}

In particular, in \eqref{entropy_special_case_n2}, take $a=p$ and $b = 1-p$ to obtain 
\[
\langle p,1-p\rangle = \langle 1-p,p\rangle  \Leftrightarrow H(p) = H(1-p).
\]

\input{fig7_003}

\input{fig7_004}

\input{fig8_004}

We may also introduce the $0$-line. See Figures~\ref{fig7_003}, \ref{fig7_004}, and \ref{fig8_004}.

We will now introduce black defects (dots) on additive networks, where whenever we insert a black dot on an additive network, we need to insert another defect on the additive network and reverse orientations on the portion of the additive network connecting these defects.
The contribution from Figure~\ref{fig8_008} left is $\langle p-q,1-p \rangle = (1-q)H\left( \frac{p-q}{1-q} \right)$. The contribution in Figure~\ref{fig8_008} center is 
$\langle q-1,p-q \rangle + \langle p-1,1-p\rangle -\langle q-1,1-q\rangle  = (p-1)H\left( \frac{q-1}{p-1} \right)$.
Finally, the contribution in Figure~\ref{fig8_008} right is $\langle p-q, q-1 \rangle + \langle 1-p, p-1\rangle -\langle 1-q,q-1\rangle  = (p-1)H\left( \frac{p-q}{p-1} \right)$.
They show that entropy also satisfies 
\[
(1-q)H\left( \frac{p-q}{1-q} \right) = (p-1)H\left( \frac{q-1}{p-1} \right) = (p-1)H\left( \frac{p-q}{p-1} \right). 
\]

\input{fig8_008}





\section{Boundary wall}
\label{section_blue_line}

In this section, we introduce a boundary wall, depicted using a blue line at the base of a cobordism. Also see \cite[Section 2]{IK24_dilogarithms_entropy}. The blue horizontal line in a cobordism is called a boundary wall. It absorbs additive vertices and inserts a labeled floating dot to the left of the network. That is, if two lines weighted $a$ and $b$ merge at an additive vertex,  the vertex gets absorbed by the boundary wall and the boundary wall emits a floating defect with the label $(a+b)\left[\dfrac{a}{a+b}\right]$. See Figure~\ref{fig5_017} left. We can also absorb an additive vertex that is connected to the $0$-edge (this will give a contribution of $0$ so we can draw a vertex with weight $0$ or we do not need to draw the $0$-vertex). See Figure~\ref{fig5_017} right. 

\input{fig5_017}

In Figure~\ref{fig5_016}, although the network initially appears complicated, after all additive vertices have been absorbed by the boundary wall, we are left with one line weighted $1$ and one vertex with appropriate weight.

\input{fig5_016}

In Figure~\ref{fig5_102}, we can separate the one additive vertex by using appropriate isotopies, resulting in one floating defect with the weight given by the right-hand side of \eqref{eqn_relating_entropy_inner}, which is Shannon entropy $H(p_X)$. 

\input{fig5_102}

In Figure~\ref{fig1_001}, we have a more global picture of our cobordisms. That is, inside the purple dashed ellipse, we have classical physics, involving entropy and information theory. In the larger blue dotted ellipse, we have the theory of probability. When networks turn around, pointing downwards, and when we can rescale the additive lines, we have a larger depiction in the theory of information theory. We are yet to interpret the additive and multiplicative networks outside the blue ellipse, but we believe these observables may have an important interpretation in (quantum) physics.

\input{fig1_001}

 
\section{Connections from diagrammatics to \texorpdfstring{$J(\kk)$}{J(k)}}
\label{section:diagrammatics_vector_space_Jk}

Now, we may want to ask ourselves how do we go from the diagrammatics in Section~\ref{section:Shannon_entropy} to elements in the vector space $J(\kk)$. 
Given a cobordism (diagram) $\gamma$, 
define $\jmath:\textsf{Diagrams} \rightarrow J(\kk)$, where 
\begin{equation}
\jmath(\gamma) = \sum_{p\in \mathrm{add}(\gamma)} s(p) \omega(p,\gamma)\langle a_p, b_p\rangle \ \in \ J(\mathbf{k}),
\end{equation}
where we sum over all the additive vertices in $\gamma$. The coefficient $s(p)$ equals $1$ if the vertex $p$ is a merge, and $-1$ if the additive vertex $p$ is a split. We denote $a_p$ and $b_p$ the two lines weighted $a$ and $b$, respectively, at point $p$. The coefficient $\omega(p,\gamma)$ is the product of all the multiplicative red lines as we virtually move the additive vertex $p$ far to the left as possible, crossing over any multiplicative lines and taking into account the conormal direction of these lines. 

The value $\jmath(\gamma)$ encodes contribution from additive vertices. It computes $2$-cocycles and it is an invariant under isotopies.
So to a diagrammatic morphism $\gamma$, assign element $\jmath(\gamma)$ of $J(\kk)$. 
Generating objects of this monoidal category are labelled by $a\in \kk$ (endpoints of additive networks) and $c\in \kk^{\ast}$ (endpoints of multiplicative networks). 

From \cite[Section 5]{IK24_dilogarithms_entropy}, we have the following: 
\begin{proposition}[Im--Khovanov]
The invariant $\jmath(\gamma)$ depends only on the source and target objects of the morphism $\gamma$.
\end{proposition}
Because $\jmath(\gamma)$ only depends on the source and the target objects of morphism $\gamma$, and not on any of the interior network, $\jmath$ is an invariant. 
We leave it as future work to explore other invariants of these cobordisms.

\section{Diagrammatics of conditional entropy}



Let $H(Y|X)$ be the conditional entropy of a discrete random variable $Y$ given the random variable $X$.
In terms of probability, conditional entropy is defined to be 
\begin{equation}
\label{eqn:cond_entropy}
H(X|Y) = - \sum_{i=1}^{n} \sum_{j=1}^{m}
P(X_i,Y_j) \log P(X_i|Y_j), 
\end{equation}
where 
\begin{equation}
\label{eqn:joint_entropy}
H(X,Y) = H(Y|X) +  H(X).
\end{equation}
This is reminiscent of an additive version of the ordinary chain rule. 

\input{fig8_003}


\input{fig7_005}

Diagram associated to Proposition~\ref{prop_joint_entropy_two} is Figure~\ref{fig8_003}. 
In order to understand conditional entropy in terms of joint entropy~\eqref{eqn:joint_entropy} using diagrammatics, we provide the diagrammatics for 
\begin{equation}
H(Y|X) =  H(X,Y) - H(X).
\end{equation}
In Figure~\ref{fig7_005}, we give an example of how conditional entropy of two random variables can be interpreted. 
In Figure~\ref{fig7_001}, we provide two different ways to obtain entropy of two random variables: $H(X,Y) = H(X|Y)+H(Y)$ and $H(X,Y) = H(Y|X)+H(X)$. 
In Figure~\ref{fig7_002}, the morphism simplifies to the identity cobordism.


\input{fig7_001}

\input{fig7_002}

We see that one can draw diagrammatics for the information content, i.e., self-information, which is  
$I(X) = \log \frac{1}{p_X}$. In terms of entropy, it can be reinterpreted as $H(p_X)= E[I(X)]$. We leave it as an exercise to draw the diagrams for mutual information: 
\begin{equation}
\begin{split}
    I(X,Y) &= H(X) - H(X|Y)  = H(Y)- H(Y|X) \\ 
    &= H(X) + H(Y)- H(X,Y) \\ 
    &= H(X,Y)-H(X|Y) - H(Y|X). 
\end{split}
\end{equation}
 

\section{Additional properties of \texorpdfstring{$J(\mathbf{k})$}{J(k)}}
\label{section_basic_properties}

From Im--Khovanov in \cite{IK24_dilogarithms_entropy}, we saw that the two symbols $\langle \cdot , \cdot \rangle$ in Cathelineau's $\kk$-vector space is related to $H(p_X)$ via the equation
\[
\langle a,b\rangle = (a+b) H\left( \frac{a}{a+b} \right)
\] 
if $a+b\not=0$; otherwise, $\langle a, b \rangle =0$.
This shows that rescaling the two symbols by $a+b$ gives us a rescaled entropy: 
\begin{equation}
\frac{1}{a+b} \langle a,b \rangle = H\left( \frac{a}{a+b} \right) = H\left(\frac{b}{a+b} \right). 
\end{equation}
In particular, when $a=b$, then 
\begin{equation}
\label{equation:inner_product_a_with_a}
\langle a,a \rangle = 2a H\left(\frac{1}{2}\right),
\end{equation}
or equivalently, 
\[
\frac{1}{2a} \langle a,a \rangle = H\left( \frac{1}{2} \right) 
\quad \mbox{ or } 
\quad  \frac{1}{2} \langle 1,1 \rangle = H\left( \frac{1}{2} \right).
\]

%Let $||a||^2 := \langle a,a \rangle $. 

%\begin{lemma}
%Let $a\geq 0$. Then $||a|| = \sqrt{2a \log 2}$. 
%\end{lemma}

%\begin{proof}
%By \eqref{equation:inner_product_a_with_a}, we have $\langle 1,1 \rangle = 2 H\left(\frac{1}{2}\right) = 2\left( -\frac{1}{2} \log \frac{1}{2} - \frac{1}{2}\log \frac{1}{2} \right) = 2 \log 2$.
%So 
%$||a||^2 = \langle a,a\rangle =a \langle 1,1\rangle =  2a \log 2$. So 
%$||a|| = \sqrt{2a \log 2}$. 
%\end{proof}
%One could also write $||a|| = \sqrt{2a H\left( \frac{1}{2}\right)}$ to be the length of $a$, where $a\geq 0$. 
%Also note that in base $2$, $||a|| = \sqrt{2a}$. 


%{\color{red} Should we extend to a notion of a complex norm? If so, we can remove the condition that $a\geq 0$. }

Although this is an interesting observation, length or norm does not make sense since the formal pair $\langle \cdot ,\cdot \rangle$ of symbols is not linear. Perhaps there may be other interesting properties about Cathelineau's vector space that could easily be observed from the diagrammatics.

%{\color{red} Length/norm doesn't make sense since it's not linear. And there is triangle inequality only under specific conditions.}
%For the symbol $\langle \cdot , \cdot \rangle$, triangle inequality
%\[
%\langle a,b \rangle + \langle b, c\rangle \leq \langle a,c \rangle 
%\] 
%holds if and only if 
%\[
%\frac{a}{b} \log \left| \frac{a+b}{a+c} \right| + 
%\frac{c}{b} \log \left| \frac{c+b}{a+c} \right| \leq 
%\log \left| \frac{b^2}{(a+b)(b+c) } \right|.
%\]


 


 




\section{Deforming \texorpdfstring{$5$}{5}-term dilogarithm into \texorpdfstring{$4$}{4}-term infinitesimal dilogarithm}
\label{section_deformation}

We refer to \cite[Section 1.3]{Cath88} where Cathelineau derives the dilogarithm to infinitesimal dilogarithm. In this section, we give a completely written out derivation of this deformation. 

Let $A$ be a unital commutative ring.
Let $P_{A}$ be an abelian group generated by $\{ z\}$, where $z$ and $1-z$ are invertible in $A$, with relations 
\begin{enumerate}
\item $\{ z\} + \left\{ \frac{1}{z} \right\} = 0$, 
\item $\left\{ \frac{1}{z} \right\} - \{1-z \}=0$,
\item
\label{item_5_term_dilog} $\{z_1 \}-\{ z_2\} 
+ \left\{ \frac{z_2}{z_1} \right\} 
- \left\{ \frac{1-z_2}{1-z_1} \right\} 
+ \left\{ \frac{(1-z_2)z_1}{(1-z_1)z_2} \right\} = 0$, 
\end{enumerate} 
where the last relation~\eqref{item_5_term_dilog} is called the $5$-term dilogarithm equation. 
A homomorphism of unital rings $A\rightarrow A'$ induces a homomorphism of abelian groups $P_A\rightarrow P_{A'}$. 

We now specialize to the case when $A=\mathbf{k}[\eps]$, where $\eps^2=0$, and $A' = \mathbf{k}$. 
Let $\widetilde{T}P(\mathbf{k}) := \ker( P_{\mathbf{k}[\varepsilon]}\rightarrow P_{\mathbf{k}})$, where the homomorphism $\mathbf{k}[\varepsilon]\lra \mathbf{k}$ is identity on $\mathbf{k}$ and $\varepsilon \mapsto 0$. 
This induces the short exact sequence 
\[
\xymatrix{
0 \ar[rr] & & 
\widetilde{T}P(\mathbf{k}) 
\ar@^{(_->}[rr] & & \ar@/_15pt/[ll]_{\sigma}
P_{\mathbf{k}[\varepsilon]} \ar@{->>}[rr] & & P_{\mathbf{k}} \ar[rr] & & 0
}
\]
with the section $\sigma$ given by 
\begin{equation}
\label{eqn_tangent_generators}
\sigma(\{ a+ b\varepsilon \}) = \{a + b\varepsilon \} - \{ a \},  \qquad  a \not=0,1.
\end{equation}
The kernel $\widetilde{T}P(\mathbf{k})$ is also generated by the images of $\{a + b\varepsilon\}$ under $\sigma$ as in 
\eqref{eqn_tangent_generators} over all $a\in \kk^{*}\setminus\{1\}$ and $b\in \kk$.  
We also have $\mathbf{k}^{*}$ acting on $\widetilde{T}P(\mathbf{k})$ via the translation on the coefficient of $\varepsilon$:
\[
c \: \sigma(\{ a+ b\varepsilon\})
= c(\{ a + b \varepsilon\} - \{ a\} ) 
= \{ a + c b \varepsilon\} - \{ a\}
= \sigma(\{a + c b \varepsilon \}). 
\]
Let $\mu:\widetilde{T}P(\mathbf{k}) \rightarrow \mathbf{k}^+ \wedge_{\mathbb{Z}} \mathbf{k}^+$ be a morphism of abelian groups, where 
$\mu(\{ a+b\varepsilon\} - \{ a\} ) = \frac{b}{a}\wedge \frac{b}{1-a}$, or  alternatively, 
$\mu(\{ a+ 2b\varepsilon \} + \{ a\} - 2\{a+b\varepsilon \} )= 2\left( \frac{b}{a}\wedge \frac{b}{1-a} \right)\not=0$. 
Let $N_{\mathbf{k}}$ be the subgroup of $\widetilde{T}P(\mathbf{k})$ generated by expressions of the form 
\begin{equation}
\label{eqn_reln_for_Nk}
\{ a + (b+b')\varepsilon \} + \{ a \} - \{ a+ b\varepsilon\} - \{ a + b'\varepsilon\}.
\end{equation}
In $\widetilde{T}P(\mathbf{k})$, we can view  $\{ a+ b\varepsilon\} - \{ a \}$ as equal to its deformation. That is, let $TP(\mathbf{k}) := \widetilde{T}P(\mathbf{k})/N_{\mathbf{k}}$. Then in $TP(\mathbf{k})$, we have: 
\[\{ a + (b+b')\varepsilon \} - \{ a + b'\varepsilon\} = \{ a+ b\varepsilon\} - \{ a \}.
\]
 

\begin{proposition}
\label{prop_5_term_dilog_to_infinitesimal_dilog}
Let $\mathbf{k}$ be a field of characteristic $0$. Then there exists an isomorphism of vector spaces 
\begin{equation}
\label{eqn_isom_tangent_space_derivations}
\varphi:\beta_{\mathbf{k}}\stackrel{\cong}{\lra} TP(\mathbf{k}), 
\quad 
\mbox{ where }
\varphi([a]) = \{ a + a(1-a)\varepsilon\} - \{ a \}.
\end{equation}
\end{proposition}

This leads us to a commutative diagram with two short exact sequences:
\[
\xymatrix@-1pc{
& & 0 \ar[dd]& & & & & & \\ 
& &  & & & & & & \\
& & N_{\mathbf{k}} \ar@^{(_->}[dd] & & & & & & \\
& &  & & & & & & \\
0 \ar[rr] & & 
\widetilde{T}P(\mathbf{k}) 
\ar@^{(_->}[rr] \ar@{->>}[dd]^{\mbox{quotient by } N_{\mathbf{k}}} & & \ar@/_15pt/[ll]_{\sigma}
P_{\mathbf{k}[\varepsilon]} \ar@{->>}[rr] & & P_{\mathbf{k}} \ar[rr] & & 0 \\
& &  & & & & & & \\
\beta_{\mathbf{k}}\ar[rr]^{\cong\ \varphi} & & TP(\mathbf{k}) \ar[dd] & & & & & & \\
& &  & & & & & & \\
& & 0. & & & & & & \\
}
\]

We will now prove Proposition~\ref{prop_5_term_dilog_to_infinitesimal_dilog}.
\begin{proof}
We will prove that $\varphi$ is well-defined by checking compatibility with the relation \eqref{item_beta_k_4_term} in Section~\ref{subsection:vs_Shannon_entropy}. 
Let 
\[
u = a + a (1-a)\varepsilon, 
\qquad 
v = b + b (1-b)\varepsilon. 
\]
Then 
\begin{align*}
\frac{u(1-v)}{v(1-u)}
&= \frac{(a + a (1-a)\varepsilon)(1-(b + b (1-b)\varepsilon))}{(b + b (1-b)\varepsilon)(1-(a + a (1-a)\varepsilon))} \\ 
&= \frac{a - a b + a(1 - b)(1 - a - b) \varepsilon}{b - a b + b(1 - a)(1 - a - b) \varepsilon} \\ 
&= \frac{a \cancel{b} \cancel{(1-a)}(1-b)}{b^{\cancel{2}}(1-a)^{\cancel{2}}} \\ 
&= \frac{a(1-b)}{b(1-a)}.
\end{align*}

Now, 
\begin{align*}
\varphi&\left( [a] - [b] + (1-a) \left[\frac{1-b}{1-a} \right] + a \left[ 
\frac{b}{a} \right]
\right) \\ 
&= \varphi([a]) - \varphi([b]) + (1-a)\varphi\left( \left[ \frac{1-b}{1-a} \right]\right) + a \varphi\left( \left[ \frac{b}{a} \right] \right) \\ 
&= \{ a + a(1-a)\varepsilon \} - \{ a \} -\left( \{ b + b(1-b)\varepsilon \} - \{ b \}
\right) \\
&\hspace{4mm}+ (1-a) \left( 
\left\{ 
\frac{1-b}{1-a} + \frac{1-b}{1-a} \left( 1- \frac{1-b}{1-a} \right) \varepsilon
\right\} - \left\{  \frac{1-b}{1-a} \right\}
\right) 
+ a \left( \left\{ \frac{b}{a} + \frac{b}{a}\left( 1-\frac{b}{a} \right)\varepsilon \right\} - \left\{ \frac{b}{a} \right\}
\right) \\ 
&= \sigma(\{ a+a(1-a)\varepsilon \}) 
- \sigma(\{b+b(1-b)\varepsilon \}) 
+ (1-a) \sigma \left( 
\left\{ 
\frac{1-b}{1-a} + \frac{1-b}{1-a} 
\left( 1- \frac{1-b}{1-a} \right) \varepsilon 
\right\}
\right) \\ 
&\hspace{4mm}+ a \sigma\left( \left\{
\frac{b}{a} + \frac{b}{a} \left( 1-\frac{b}{a} \right) \varepsilon 
\right\} 
\right) \\ 
&= \sigma(\{ u\}) - \sigma(\{ v \}) +  \sigma \left( 
\left\{ 
\frac{1-b}{1-a} + (1-b) 
\left( 1- \frac{1-b}{1-a} \right) \varepsilon 
\right\}
\right) +  \sigma\left( \left\{
\frac{b}{a} + b \left( 1-\frac{b}{a} \right) \varepsilon 
\right\} 
\right) \\ 
&= \sigma(\{ u\}) - \sigma(\{ v \}) +  \sigma \left( 
\left\{ 
\frac{1-b}{1-a} - \frac{1-b}{1-a}(a-b)  \varepsilon 
\right\}
\right) +  \sigma\left( \left\{
\frac{b}{a} + \frac{b}{a} \left( a-b \right) \varepsilon 
\right\} 
\right) \\ 
&\stackrel{\dddag}{=} \sigma(\{ u\}) - \sigma(\{ v\}) 
+  \sigma\left( \left\{ 
\frac{b}{a} + \frac{b}{a}(a - b) \eps
\right\}\right) 
- \sigma\left( \left\{ 
\frac{1 - b}{1-a} + \frac{1 - b}{1-a}(a - b) \eps 
\right\} \right)  \\ 
&\hspace{4mm}
+  \left\{  \frac{a(1-b)}{b(1-a)} \right\}
-  \left\{  \frac{a(1-b)}{b(1-a)} \right\}
\\
&=  \sigma(\{ u\}) - \sigma(\{ v\}) + \sigma\left( \left\{ 
\frac{b}{a} + \frac{b}{a}(a - b) \eps
\right\}\right) - \sigma\left( \left\{ 
\frac{1 - b}{1-a} + \frac{1 - b}{1-a}(a - b) \eps 
\right\} \right) \\
&\hspace{4mm} + \sigma \left( \left\{  \frac{a(1-b)}{b(1-a)} \right\}\right) \\ 
&=  \sigma\left( \{ u\} - \{ v\}
+  \left\{
\frac{b}{a} (1 + (a - b) \eps)
\right\} -  \left\{ 
\frac{(1 - b) + (1 - b) (a - b) \eps}{1-a}
\right\}  +  \left\{ 
\frac{a(1-b)}{b(1-a)} \right\}\right)
\\ 
&= \sigma \left( 
\{ u \} - \{ v \} + \left\{ \frac{v}{u} \right\} - \left\{  \frac{1-v}{1-u} \right\} 
+ \left\{ \frac{u(1-v)}{v(1-u)} \right\} 
\right) 
\end{align*}
since 
\begin{align*}
\frac{v}{u} 
&= \frac{b + b (1 - b) \eps}{a + a (1 - a) \eps} 
= \frac{b + b (1 - b) \eps}{a + a (1 - a) \eps} 
\frac{a - a (1 - a) \eps}{a - a (1 - a) \eps} \\ 
&= \frac{a b + a b (a - b) \eps}{a^2} 
= \frac{b}{a} + \frac{b}{a}(a-b) \eps, 
\end{align*}
\begin{align*}
\frac{1-v}{1-u} 
&= \frac{1 - (b + b (1 - b) \eps)}{1 - (a + a (1 - a) \eps)} 
= \frac{1 - b - b (1 - b) \eps}{1 - a - a (1 - a) \eps} \\
&= \frac{1 - b - b (1 - b) \eps}{1 - a - a (1 - a) \eps} \frac{1 - a + a (1 - a) \eps}{1 - a + a (1 - a) \eps} \\
&= \frac{(1 - a) (1 - b) + (1 - a) (1 - b) (a - b) \eps}{(1-a)^2} \\ 
&= \frac{(1 - b) + (1 - b) (a - b) \eps}{1-a} \\
&= \frac{1 - b}{1-a} + \frac{1 - b}{1-a}(a - b) \eps, 
\end{align*}
\begin{align*}
\frac{u(1-v)}{v(1-u)} 
&= \frac{(a + a (1 - a) \eps) (1 - (b + b (1 - b) \eps))}{(b + b (1 - b) \eps) (1 - (a + a (1 - a) \eps)} \\ 
&= \frac{a (1 - b) \cancel{(1 + (1 - (a + b)) \eps)}}{b (1 - a) \cancel{(1 + (1 - (a + b)) \eps)}}  \\
&= \frac{a(1-b)}{b(1-a)},
\end{align*}
and for $b'=-b$ in \eqref{eqn_reln_for_Nk}, we have 
\begin{align*}
\{a + (b-b)\eps \} &+ \{ a\}-\{ a+b\eps\} -\{a-b\eps \} = \{a \} + \{ a\}-\{ a+b\eps\} -\{a-b\eps \}. 
\end{align*}
So in $TP(\mathbf{k})$, 
\begin{align*}
0 
&= \sigma(\{a \} + \{ a\}-\{ a+b\eps\} -\{a-b\eps \}) \\ 
&= 2\sigma(\{ a\}) - \sigma(\{ a+ b\eps\})
- \sigma(\{ a-b\eps\}) \\ 
&= 2(\{ a\} -\{ a\}) - \sigma(\{ a+ b\eps\})
- \sigma(\{ a-b\eps\}) \\ 
&= 0 - \sigma(\{ a+ b\eps\})
- \sigma(\{ a-b\eps\}).\\ 
\end{align*}
The equality $\dddag$ holds in $TP(\mathbf{k})$ since 
\[
- \sigma(\{ a+ b\eps\}) =  \sigma(\{ a-b\eps\}).
\]
Now to prove that $\varphi$ is injective, 
let 
\begin{equation}
\rho: TP(\mathbf{k}) \lra \mathbf{k}^{*} \otimes \mathbf{k}^+,
\qquad 
\rho(\{ a + b\eps \} - \{ a \}) 
= a \otimes \frac{b}{1-a} +  (1-a) \otimes \frac{b}{a}, 
\end{equation}
be a map of vector spaces over $\mathbf{k}$. Then we have a commutative diagram
\[
\xymatrix@-1pc{
\beta_{\mathbf{k}} \ar@^{(_->}[rrrr]^D \ar[rrdd]_{\varphi} & & & & \mathbf{k}^{*}\otimes \mathbf{k}^+, \\ 
& & & & \\ 
& & TP(\mathbf{k}) \ar@^{(_->}[rruu]_{\rho}& & \\ 
}
\]
where $D$ and $\rho$ are injective. So $\varphi$ is also injective. 
\end{proof}

\begin{remark}
Note that in general, $\rho$ may not be surjective, but $D$ and $\rho$ are injective imply that $TP(\mathbf{k})$ is not too small. 
\end{remark}




{\bf Second proof.} We give a second argument that the $5$-term dilogarithm deforms to the infinitesimal $4$-term dilogarithm. 
Let $\kk$ be a field, and let $\kk^\flat= \kk^{*}\setminus \{ 1\} = \kk \setminus \{0,1 \}$. 

Let $\beta_2(\kk)$ be the quotient of $\Q[\kk^\flat]$ by 
\[
[a] - [b] +\left[\frac{b}{a} \right] + \left[ \frac{1-b^{-1}}{ 1 - a^{-1}} \right] - \left[\frac{1-b}{1-a}\right] = 0,
\]
with $(1-a)(1-b)\left(1-\frac{b}{a}\right) \in \kk^{*}$. 

Let $\kk_2:= \kk[t]/(t^2)$ be the ring of dual numbers. 
If $a\in \kk^\flat$, define 
\[
\langle  a \rangle := a + a(1-a)t = a(1+(1-a)t) \in \kk_2.
\]
Note that $(1+ct)^{-1}=1-ct$ in $\kk_2$ since $t^2=0$. We see that this element $\langle a \rangle$ is invertible in $\kk_2$, with $\langle a\rangle^{-1} = a^{-1}(1-(1-a)t)$. 


Define the action of $\kk^*$ on the ring $\kk_2$ of dual numbers as $\lambda \ast (b_0 + b_1 t) = b_0 + \lambda b_1 t$, where $\lambda\in\kk^\ast$. 
That is, $\lambda \ast 1 = 1$ and $\lambda \ast t = \lambda t$.
So $\lambda$ only translates the coefficient of $t$.

\begin{lemma}
\label{lemma:quot_dual_numbers}
We have 
$\dfrac{\langle b \rangle}{\langle a\rangle} = a \ast \left\langle \dfrac{b}{a} \right\rangle$. 
\end{lemma}

\begin{proof}
The left hand side shows 
\begin{align*}
\dfrac{\langle b \rangle}{\langle a\rangle} 
= \dfrac{b(1+(1-b)t)}{a(1+(1-a)t)} 
= \dfrac{b}{a} \left(1 + ((1-b)-(1-a))t \right) 
= \dfrac{b}{a} (1+(a-b)t).
\end{align*}
On the other hand, the right hand side shows 
\begin{align*}
a \ast \left\langle \dfrac{b}{a} \right\rangle 
= a\ast \left(\frac{b}{a} \left(1+ \left(1-\frac{b}{a} \right)t \right)  \right)
= \frac{b}{a} \left(1+ \left(1-\frac{b}{a}\right) a t \right) 
= \frac{b}{a} \left( 1+ \frac{a-b}{a} at \right) 
= \frac{b}{a} (1+(a-b)t). 
\end{align*}
\end{proof}

\begin{lemma}
\label{lemma:diff_quot_dual_numbers}
The relation 
\[
\frac{1-\langle b\rangle}{1-\langle a \rangle} 
= (a-1) \ast \left\langle \dfrac{1-b}{1-a} \right\rangle
\]
holds. 
\end{lemma}

\begin{proof}
On the left hand side, we have 
\begin{align*}
\frac{1-\langle b\rangle}{1-\langle a\rangle}   
&= \dfrac{1 - b(1+(1-b)t)}{1-a(1+(1-a)t)}  \\
&=  \dfrac{1 - b - b(1-b)t}{1 - a - a(1-a)t}\cdot \dfrac{1-a + a(1-a)t}{1-a + a(1-a)t} \\
&=\dfrac{(1-a)(1-b)+( a(1-a)(1-b) -(1-a)b(1-b) )t}{(1-a)^2} \\
&=\dfrac{\cancel{(1-a)}(1-b)+ (a-b)\cancel{(1-a)}(1-b) t }{(1-a)^{\cancel{2}}} \\
&=\dfrac{1-b}{1-a}\left(1+(a-b)t \right)
\end{align*}
while on the right hand side, we have 
\begin{align*}
(a - 1) \ast \left\langle \dfrac{1-b}{1-a} \right\rangle
= (a-1) \ast \frac{1-b}{1-a}\left( 1+ \left( 1-\frac{1-b}{1-a}\right)t \right) 
= \frac{1-b}{1-a}\left( 1+ \left( 1-\frac{1-b}{1-a} \right) (a-1) t \right),
\end{align*}
which are indeed equal.
\end{proof}

\begin{lemma}
\label{lemma:add_mult_inv}
    We have $1-\langle a\rangle^{-1} = (1-a^{-1})(1-t)$. 
\end{lemma}

\begin{proof}
Since 
\begin{align*}
1 - \langle a\rangle^{-1} 
&= 1 - a^{-1}(1+(1-a)t)^{-1}
= 1 - a^{-1}(1-(1-a)t) \\ 
&= 1-a^{-1} + a^{-1}(1-a)t 
 = 1-a^{-1} + (a^{-1} - 1)t \\ 
 &= (1-a^{-1})(1-t), 
\end{align*}
the lemma holds. 
\end{proof}

\begin{lemma}
\label{lemma:add_mult_inv_quot}
The equation 
\[
\frac{1-\langle b \rangle^{-1}}{1-\langle a \rangle^{-1}} 
= \frac{1 - b^{-1}}{1-a^{-1}}
\] 
holds.
\end{lemma}

\begin{proof}
The lemma is immediate since 
\begin{align*}
\frac{1-\langle b \rangle^{-1} }{ 1-\langle a \rangle^{-1} }
= \frac{ (1 - b^{-1})\cancel{(1-t)} }{(1 - a^{-1})\cancel{(1-t)}}
= \frac{1 - b^{-1}}{1 - a^{-1}}. 
\end{align*}
\end{proof}


 
\begin{lemma}
\label{lemma_scalar_bracket}
We have $[b\ast \langle c\rangle]=b[\langle c\rangle]$ where $b\in \kk$ and $c\in \kk_2$ and  
$(-1) [\langle 1-a\rangle]=-[\langle a \rangle]$ and 
$a [\langle a^{-1}\rangle] = -[\langle a \rangle ]$. 
\end{lemma}

\begin{proof}
This is clear using the construction of $\beta(\kk)$. 
\end{proof}


\begin{lemma}
\label{lemma:5_term_relation}
The $5$-term dilogarithm 
\begin{equation}
\label{eqn:5_term_relation}
[\langle a \rangle] - [\langle b \rangle] 
+ \left[\frac{\langle b \rangle}{\langle a \rangle}\right]
+ \left[\frac{1-\langle b \rangle^{-1}}{1-\langle a \rangle^{-1}}\right] 
- \left[\frac{1-\langle b \rangle }{1-\langle a \rangle } \right] = 0
\end{equation}
holds in the commutative ring $\beta_2(\kk_2)$. 
\end{lemma}

\begin{proof}
This is clear by the definition of the ring $\beta_2(\kk_2)$.
\end{proof}


\begin{proposition}
Taking the limit as $t\rightarrow 0$, the $5$-term dilogarithm in \eqref{eqn:5_term_relation} deforms to the $4$-term infinitesimal dilogarithm
\begin{equation}
[a]-[b]+ a\biggl[ \dfrac{b}{a}\biggr] + (1-a)\biggl[ \dfrac{1-b}{1-a} \biggr]=0, \ \ a\in \kk\setminus\{0,1\}, \ b\in \kk^{*}. 
\end{equation}
\end{proposition}

\begin{proof}
We have 
$\left[\dfrac{\langle b \rangle}{\langle a \rangle}\right] = \left[ a * \left\langle \dfrac{b}{a} \right\rangle \right]= a \left[ \left\langle \dfrac{b}{a} \right\rangle \right]$ by Lemmas~\ref{lemma:quot_dual_numbers} and \ref{lemma_scalar_bracket}. 
We also have 
\[
\left[\frac{1-\langle b \rangle^{-1}}{1-\langle a \rangle^{-1}}\right] = \left[  \frac{(1-b^{-1})\cancel{(1-t)}}{(1-a^{-1})\cancel{(1-t)}} \right] =  \left[  \frac{1-b^{-1}}{1-a^{-1}} \right]
\] by Lemma~\ref{lemma:add_mult_inv_quot}. 
Finally, we have 
\begin{align*}
\left[\frac{1-\langle b \rangle }{1-\langle a \rangle } \right]
&= \left[(a-1)*\left\langle \frac{1-b}{1-a} \right\rangle \right] = (a-1) \left[\left\langle \frac{1-b}{1-a} \right\rangle \right] 
\end{align*}
by Lemmas~\ref{lemma:diff_quot_dual_numbers} and \ref{lemma_scalar_bracket}. Putting these together, we have 
\[
[\langle a \rangle] 
- [\langle b \rangle] 
+ a \left[\left\langle\frac{ b}{ a }\right\rangle\right]
+ \left[\frac{1- b^{-1}}{1-  a^{-1}}\right] 
+ (1-a)\left[\left\langle \frac{1-b}{1-a} \right\rangle \right] =0.
\]
The element $\left[\frac{1- b^{-1}}{1-  a^{-1}}\right]$ is the zero element in 
\[
\ker(\beta(\kk_2)\longrightarrow \beta(\kk))\big/\left([ a + (b+b') t ] + [a] - [ a+ b t] - [ a + b' t]\right).
\]
We thus obtain 
\[
[\langle a \rangle] 
- [\langle b \rangle] 
+ a \left[\left\langle\frac{ b}{ a }\right\rangle\right]
+ (1-a)\left[\left\langle \frac{1-b}{1-a} \right\rangle \right] =0,
\]
which $t$-deforms to 
\[
[a ] 
- [ b ] 
+ a \left[\frac{ b}{ a }\right] 
+ (1-a)\left[ \frac{1-b}{1-a} \right] =0 \mbox{ for } a\not=b \mbox{ and } a\in\kk^{\flat}.
\]
\end{proof}

 
 

%{\color{red} Define $\kk_2^\flat := \{a + b\: t : a\in\kk^\flat, b\in \kk \}$.}

\section{Future Work}
\label{sec_future}
 It would be interesting to extend the work of Im--Khovanov of diagrammatic interpretation in \cite{IK24_dilogarithms_entropy} to R\'enyi entropy, which is a generalization of Shannon entropy, von Neumann entropy, which is a quantized analogue of Gibbs entropy, and relative entropy, which is also known as Kullback--Leibler divergence. There may be a deeper interpretation if one were to extend the diagrammatics of entropy to their higher-dimensional analogues.


 



\bibliographystyle{amsalpha} 
\bibliography{quantum_entropy}

\end{document}
