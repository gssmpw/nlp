\section{Experiments}

\subsection{Evaluation on ImageNet Classification}
\label{sec:results}

In this section, we assess the effectiveness of MaxSup on ImageNet-1K, comparing its performance against standard Label Smoothing and related variants.

\subsubsection{Experiment Setup}
\label{sec:train1}

\paragraph{Model Training Configurations}  
We conduct extensive experiments with both CNN and Transformer models, including the ResNet family~\citep{he2016deep}, MobileNetV2~\citep{sandler2018mobilenetv2}, and DeiT-Small~\citep{touvron2021training}, all thoroughly evaluated on the large-scale ImageNet dataset~\citep{krizhevsky2012imagenet} for comprehensive performance analysis.

For \textbf{ResNet Series} models, we train for 200 epochs using stochastic gradient descent (SGD) with momentum 0.9, a weight decay of $1\times10^{-4}$, and a batch size of 2048. The initial learning rate is set to 0.85 and scheduled via cosine annealing.\footnote{Additional training hyperparameters follow the FFCV training scripts in \url{https://github.com/libffcv/ffcv}. See \cref{app:training} for further details on training setups.} 
We also evaluate ResNet-based CNNs on CIFAR-100. Here, we use an initial learning rate of 0.1, reducing it by a factor of 5 at the 60th, 120th, and 160th epochs. We train for 200 epochs with a batch size of 128, weight decay of $5\times10^{-4}$, and Nesterov momentum set to 0.9.
For \textbf{DeiT-Small}, we employ the official implementation and train from scratch without knowledge distillation. Although the original DeiT paper emphasizes distillation, we exclude it to provide a clearer, unbiased assessment of MaxSup’s contributions. We also omit CutMix and Mixup to retain the same optimization objective.

\paragraph{Hyperparameters for Compared Methods}  
We compare Max Suppression Regularization against multiple Label Smoothing variants, including Zipf Label Smoothing~\citep{liang2022efficient} and Online Label Smoothing~\citep{zhang2021delving}. When official implementations are available, we use them directly; otherwise, we follow the respective papers’ descriptions closely to ensure fair comparisons. All training hyperparameters are kept identical to those of the baseline models, except for algorithm-specific settings and necessary adjustments. Additionally, we employ a linearly increasing \(\alpha\) scheduler, which generally benefits training and stability; see \cref{app:alpha} for details. This scheduler is applied to both MaxSup and standard Label Smoothing by default to maintain consistency.


\subsubsection{Experiment Results}
\label{sec:classification}

\paragraph{ConvNet Comparison}
Table~\ref{tab:comparison-new} summarizes the performance of MaxSup alongside various smoothing and self-distillation methods on both ImageNet and CIFAR-100. Across all tested convolutional architectures, MaxSup achieves the highest accuracy among label smoothing–based regularizers. By contrast, OLS~\citep{zhang2021delving} and Zipf-LS~\citep{liang2022efficient} yield less consistent gains, suggesting their reported empirical benefits may depend heavily on specific training schedules.
In our reproductions of OLS and Zipf-LS, we follow the authors’ original codebases and method-specific hyperparameters but do not adopt their complete training recipes. For example, the OLS paper uses a step learning-rate scheduler over 250 epochs with an initial rate of 0.1, while Zipf-LS trains for 100 epochs under a separate set of hyperparameters. Our results underscore the robustness of MaxSup across different training setups, in contrast to the more scheme-dependent improvements noted for OLS and Zipf-LS.

\begin{table*}[htbp]
\setlength{\tabcolsep}{4pt}
\centering
\scriptsize
\caption{Comparison of classic convolutional neural networks on ImageNet and CIFAR-100.  
Results are reported as ``mean ± std'' (percentage).  
\textbf{Bold} entries highlight the best performance; \underline{underlined} entries mark the second best.  
(Methods with $^{*}$ denote code adaptations from official repositories; see text for details.)}
\vspace{-2mm}
\label{tab:comparison-new}
\begin{tabular}{@{}lcccccccc@{}}
\toprule
\textbf{Method} 
& \multicolumn{4}{c}{\textbf{ImageNet}} 
& \multicolumn{4}{c}{\textbf{CIFAR-100}} \\
\cmidrule(r){2-5}\cmidrule(l){6-9}
& \textbf{ResNet-18} & \textbf{ResNet-50} & \textbf{ResNet-101} & \textbf{MobileNetV2}
& \textbf{ResNet-18} & \textbf{ResNet-50} & \textbf{ResNet-101} & \textbf{MobileNetV2} \\
\midrule
Baseline 
& 69.09±0.12 & 76.41±0.10 & 75.96±0.18 & 71.40±0.12
& 76.16±0.18 & 78.69±0.16 & 79.11±0.21 & 68.06±0.06 \\

Label Smoothing 
& \underline{69.54±0.15} & 76.91±0.11 & 77.37±0.15 & 71.61±0.09 
& 77.05±0.17 & 78.88±0.13 & 79.19±0.25 & \underline{69.65±0.08} \\

$\text{Zipf-LS}^{*}$ 
& 69.31±0.12 & 76.73±0.17 & 76.91±0.11 & 71.16±0.15 
& 76.21±0.12 & 78.75±0.21 & 79.15±0.18 & 69.39±0.08 \\

$\text{OLS}^{*}$ 
& 69.45±0.15 & \underline{77.23±0.21} & \underline{77.71±0.17} & \underline{71.63±0.11}
& \underline{77.33±0.15} & 78.79±0.12 & \underline{79.25±0.15} & 68.91±0.11 \\

\textbf{MaxSup} 
& \textbf{69.96±0.13} & \textbf{77.69±0.07} & \textbf{78.18±0.12} & \textbf{72.08±0.17}
& \textbf{77.82±0.15} & \textbf{79.15±0.13} & \textbf{79.41±0.19} & \textbf{69.88±0.07} \\

Logit Penalty 
& 68.48±0.10 & 76.73±0.10 & 77.20±0.15 & 71.13±0.10 
& 76.41±0.15 & \underline{78.90±0.16} & 78.89±0.21 & 69.46±0.08 \\

\bottomrule
\end{tabular}
\end{table*}


\paragraph{DeiT Comparison}
Table~\ref{tab:deit-small-comparison} compares various regularization techniques for DeiT-Small on ImageNet. MaxSup achieves an accuracy of $76.49\%$, surpassing Label Smoothing by $0.41$ percentage points. Label Smoothing variants such as Zipf’s and OLS yield only marginally higher or comparable performance relative to standard LS, suggesting that these approaches may be less effective for vision transformer architectures—potentially due to their reliance on extensive data augmentation schemes. In contrast, MaxSup consistently outperforms both standard LS and its variants, indicating its stronger ability to enhance feature representations without additional data manipulations. These findings underscore MaxSup’s robustness across distinct model architectures, especially in settings where other regularization methods show limited effectiveness.

\begin{table}[t]
\centering
\scriptsize
\caption{Accuracy (\%) comparison on DeiT-Small~\citep{touvron2021training} using different Label Smoothing variants. 
Results are reported as ``mean ± std''; parentheses indicate absolute improvement over the baseline.}
\label{tab:deit-small-comparison}
\begin{tabular}{@{}lcc@{}}
\toprule
\textbf{Method} & \textbf{Accuracy (Mean)} & \textbf{Std} \\
\midrule
Baseline & 74.39 & 0.19 \\
Label Smoothing & 76.08 (\(+1.69\)) & 0.16 \\
Zipf-LS & 75.89 (\(+1.50\)) & 0.26 \\
OLS & 76.16 (\(+1.77\)) & 0.18 \\
\textbf{MaxSup} & \textbf{76.49 (\(+2.10\))} & \textbf{0.12} \\
\bottomrule
\end{tabular}
\vspace{-0.4cm}
\end{table}

\begin{table}[ht]
\centering
\scriptsize
\caption{Semantic segmentation results on the ADE20K validation set. Models are pretrained on ImageNet-1K and then fine-tuned with UperNet~\citep{xiao2018unified} for enhanced performance. We report mean Intersection over Union (mIoU) under multi-scale (MS) testing for comprehensive evaluation.}
\label{tab:miou}
\begin{tabular}{@{}lcc@{}}
\toprule
\textbf{Backbone} & \textbf{Method} & \textbf{mIoU (MS)} \\
\midrule
\multirow{3}{*}{DeiT-Small~\citep{touvron2021training}}  
 & Baseline & 42.1 \\
 & Label Smoothing & 42.4 (\(+0.3\)) \\
 & \textbf{MaxSup} & \textbf{42.8 (\(+0.7\))} \\
\bottomrule
\end{tabular}
\vspace{-0.5cm}
\end{table}


\begin{figure*}[t]
\captionsetup{font=footnotesize,labelfont=footnotesize}
\centering
\includegraphics[width=0.914\textwidth]{images/figure3.png}
\vspace{-0.5cm}

\caption{Class activation maps generated by Grad-CAM~\citep{Selvaraju_2019} for DeiT-Small models trained with MaxSup (2nd row), Label Smoothing (3rd row), and a standard Baseline (4th row). The first row shows the original images. Compared with Label Smoothing, MaxSup more effectively suppresses distractions from non-target objects and preserves key features of the target class, thereby reducing instances in which the model partially or completely focuses on irrelevant regions.}
\label{fig:gradcam}
\vspace{-0.5cm}
\end{figure*}



\subsection{Evaluation on Semantic Segmentation }
\label{sec:train2}
To further assess the transferability of MaxSup to downstream tasks, we evaluate its performance on \textbf{semantic segmentation} using the MMSegmentation framework.\footnote{\label{mmsegmentation}\url{https://github.com/open-mmlab/mmsegmentation}} 
Specifically, we employ the UperNet architecture~\citep{xiao2018unified} with a DeiT-Small backbone, trained on ADE20K. 
We compare backbones trained with Label Smoothing and MaxSup (on ImageNet-1K) against a baseline, following the same setup as in \Cref{sec:classification}. 
During fine-tuning, all models use a standard cross-entropy loss.
As shown in \Cref{tab:miou}, MaxSup achieves a mean Intersection over Union (mIoU) of $42.8\%$, surpassing the $42.4\%$ obtained with Label Smoothing. 
These findings further highlight the improved feature representations afforded by MaxSup in downstream tasks such as semantic segmentation.


\subsection{Visualization via Class Activation Maps}
\label{sec:class_activation_map}

To assess how MaxSup influences model decision-making relative to Label Smoothing (LS), we employ Gradient-weighted Class Activation Mapping (Grad-CAM)~\citep{Selvaraju_2019}. Grad-CAM produces class-discriminative localization maps that highlight the regions most relevant to each classification decision.

We conduct comprehensive experiments on the DeiT-Small model under three distinct training setups: MaxSup (second row), Label Smoothing (third row), and standard Cross-Entropy (CE) as a baseline (fourth row). As illustrated in \Cref{fig:gradcam}, \textbf{MaxSup-trained models} exhibit a distinct advantage in effectively mitigating distractions caused by non-target salient objects in the background (e.g., a pole in the `Bird' image, a tube in the `Goldfish' image, and a cap in the `House Finch' image). By contrast, LS-trained models often lose focus or erroneously attend to background objects, further reflecting the detrimental influence of LS’s Error-Enhancement term and its impact on feature learning (Please see Figures~\ref{fig:gradcam}).

Moreover, \textbf{MaxSup} preserves a wider range of relevant object features, as evidenced in the `Shark' and `Monkey' examples, where LS-trained models fail to capture key details (fins and tails). These observations align with the analysis in \cref{sec:vis_feature}, underscoring that MaxSup better retains rich intra-class information. Consequently, MaxSup-trained models yield more accurate and reliable classifications by leveraging these detailed feature representations.

\subsection{Quantitative Analysis of CAM Overlays}
To further quantify the benefits of MaxSup, we measure precision and recall for Grad-CAM overlays on target object regions. Our results indicate that MaxSup consistently attains higher localization precision while maintaining or improving recall, leading to more accurate and comprehensive feature activation. Qualitative assessments similarly reveal that MaxSup-trained models exhibit both a more focused and more extensive attention map, resulting in more reliable and interpretable predictions. These findings demonstrate that MaxSup not only enhances feature representation but also contributes to more trustworthy AI systems. 










