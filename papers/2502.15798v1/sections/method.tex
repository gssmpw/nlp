\section{Max Suppression Regularization (MaxSup)}
\label{sec:method}
We first partition the training objective into two components: the standard Cross-Entropy (CE) loss and a regularization term introduced by Label Smoothing (LS). By expressing LS in terms of logits (\Cref{th:ce_ls}), we isolate two key factors: a \emph{regularization term} that controls overconfidence and an \emph{error-enhancement term} that enlarges the gap between the ground-truth logit \(z_{gt}\) and any higher logits (\Cref{cor:ls}, \Cref{eq:decomp}), ultimately degrading performance. To address these shortcomings, we propose \textbf{Max Suppression Regularization (MaxSup)}, which applies the penalty to the largest logit \(z_{\textit{max}}\) rather than to \(z_{gt}\) (\Cref{eq:maxsup-uni}, \cref{sec:maxsup}). This shift delivers consistent regularization for both correct and incorrect predictions, preserves intra-class variation, and bolsters inter-class separability. Consequently, MaxSup mitigates the representation collapse found in LS, attains superior ImageNet-1K accuracy (\Cref{tab:pre}), and improves transferability (\Cref{tab:feature}, \Cref{tab:validation_performance}). The following sections elaborate on MaxSup’s formulation and its integration into the overall training pipeline.


\subsection{Revisiting Label Smoothing}
\label{sec:revisiting}
Label Smoothing (LS) is a regularization technique designed to reduce overconfidence by softening the target distribution. Rather than assigning probability \(1\) to the ground-truth class and \(0\) to all others, LS redistributes a fraction \(\alpha\) of the probability uniformly across all classes:

\begin{definition}
For a classification task with \(K\) classes, LS converts a one-hot label \(\mathbf{y} \in \mathbb{R}^{K}\) into a soft label \(\mathbf{s} \in \mathbb{R}^{K}\):
\begin{equation}
s_k = (1 - \alpha) y_k + \frac{\alpha}{K},
\end{equation}
where \(y_k = \mathds{1}_{\{k = gt\}}\) denotes the ground-truth class. The smoothing factor \(\alpha \in [0,1]\) reduces the confidence assigned to the ground-truth class and distributes \(\tfrac{\alpha}{K}\) to other classes uniformly, thereby mitigating overfitting, enhancing robustness, and promoting better generalization.
\end{definition}

To clarify the effect of LS on model training, we first decompose the Cross-Entropy (CE) loss into a standard CE term and an additional LS-induced regularization term:
\begin{lemma}
\label{lemma:ls-decomposition}
\textbf{Decomposition of Cross-Entropy Loss with Soft Labels.}
\begin{equation}
H(\mathbf{s}, \mathbf{q})
\;=\;
H(\mathbf{y}, \mathbf{q})
\;+\;
L_{\textit{LS}},
\end{equation}
where
\begin{equation}
L_{\textit{LS}}
\;=\;
\alpha\,\biggl(
    H\Bigl(\tfrac{\mathbf{1}}{K}, \mathbf{q}\Bigr)
    \;-\;
    H(\mathbf{y}, \mathbf{q})
\biggr).
\end{equation}
Here, \(\mathbf{q}\) is the predicted probability vector, \(H(\cdot)\) denotes the Cross-Entropy, and \(\frac{\mathbf{1}}{K}\) is the uniform distribution introduced by LS. This shows that LS adds a regularization term, \(L_{\textit{LS}}\), which smooths the output distribution and helps to reduce overfitting. (See \Cref{sec:proof_lem} for a formal proof.)
\end{lemma}

\noindent
Building on \Cref{lemma:ls-decomposition}, we next explicitly express \(L_{\textit{LS}}\) at the logit level for further analysis.

\begin{theorem}
\label{th:ce_ls}
\textbf{Logit-Level Formulation of Label Smoothing Loss.}
\begin{equation}
L_{\textit{LS}}
\;=\;
\alpha
\Bigl(
    z_{gt}
    \;-\;
    \frac{1}{K} \sum_{k=1}^K z_k
\Bigr),
\end{equation}
where \(z_{gt}\) is the logit corresponding to the ground-truth class, and \(\tfrac{1}{K} \sum_{k=1}^K z_k\) is the average logit. Thus, LS penalizes the gap between \(z_{gt}\) and the average logit, encouraging a more balanced output distribution and reducing overconfidence. (See \Cref{proof:the} for the proof.)
\end{theorem}

\noindent
The behavior of \(L_{\textit{LS}}\) differs depending on whether \(z_{gt}\) is already the maximum logit. Specifically, depending on whether the prediction is correct (\(z_{gt} = z_{\textit{max}}\)) or incorrect (\(z_{gt} \neq z_{\textit{max}}\)), we can decompose \(L_{\textit{LS}}\) into two parts:
\begin{corollary}
\label{cor:ls}
\textbf{Decomposition of Label Smoothing Loss.}
\begin{equation}
\label{eq:decomp}
L_{\textit{LS}}
\;=\;
\underbrace{
    \frac{\alpha}{K} \sum_{z_m < z_{gt}} \Bigl(z_{gt} - z_m\Bigr)
}_{\text{Regularization}}
\;+\;
\underbrace{
    \frac{\alpha}{K} \sum_{z_n > z_{gt}} \Bigl(z_{gt} - z_n\Bigr)
}_{\text{Error-Enhancement}},
\end{equation}
where \(M\) and \(N\) are the numbers of logits below and above \(z_{gt}\), respectively (\(M + N = K - 1\)). Note that the error-enhancement term vanishes when \(z_{gt} = z_{\textit{max}}\).
\begin{enumerate}
    \item \textbf{Regularization}: Penalizes the gap between \(z_{gt}\) and any smaller logits, thereby moderating overconfidence.
    \item \textbf{Error-Enhancement}: Penalizes the gap between \(z_{gt}\) and larger logits, inadvertently increasing overconfidence in incorrect predictions.
\end{enumerate}
\end{corollary}

\noindent
Although LS aims to combat overfitting by reducing prediction confidence, its error-enhancement component can be detrimental for misclassified samples, as it widens the gap between the ground-truth logit \(z_{gt}\) and the incorrect top logit. Concretely:
\begin{enumerate}
    \item \textbf{Correct Predictions} \((z_{gt} = z_{\textit{max}})\):
    The error-enhancement term is zero, and the regularization term effectively reduces overconfidence by shrinking the gap between \(z_{gt}\) and any smaller logits.
    
    \item \textbf{Incorrect Predictions} \((z_{gt} \neq z_{\textit{max}})\):
    LS introduces two potential issues:
    \begin{itemize}
        \item \textbf{Error-Enhancement}: Increases the gap between \( z_{gt} \) and larger logits, reinforcing overconfidence in incorrect predictions.
        \item \textbf{Inconsistent Regularization}: The regularization term lowers \(z_{gt}\) yet does not penalize \(z_{\textit{max}}\), which further impairs learning.
    \end{itemize}
\end{enumerate}
\vspace{-2mm}
These issues with LS on misclassified samples have also been observed in prior work~\citep{xia2024understanding}. By precisely identifying these two components (regularization vs.\ error-enhancement), we can design a more targeted solution.

\paragraph{Ablation Study on LS Components} To gauge the influence of each LS component, we conduct an ablation study on ImageNet-1K using a DeiT-Small model~\citep{touvron2021training} without Mixup or CutMix. As shown in \Cref{tab:pre}, LS’s gains arise solely from the regularization term, whereas the error-enhancement term degrades performance. In contrast, our proposed \textbf{Max Suppression Regularization (MaxSup)} omits the error-enhancement effect and achieves higher accuracy by retaining the beneficial regularization.
\begin{table}[t]
\scriptsize
\centering
\caption{Ablation on LS components using DeiT-Small on ImageNet-1K (without CutMix or Mixup). 
``Regularization'' denotes penalizing logits smaller than \(z_{gt}\); 
``Error-Enhancement'' penalizes logits larger than \(z_{gt}\). 
MaxSup removes error-enhancement while retaining regularization.}
\label{tab:pre}
\definecolor{customgray}{rgb}{0.3,0.3,0.3}
\begin{tabular}{@{}llc@{}}
\toprule
\textbf{Method} & \textbf{Formulation} & \textbf{Accuracy} \\
\midrule
Baseline & -- & 74.21 \\
\hline
\multirow{2}{*}{+ Label Smoothing}
  & \(\tfrac{\alpha}{K}\sum_{z_m<z_{gt}}(z_{gt}-z_m)\)
  & \multirow{2}{*}{75.91} \\
& \(\,+\, \tfrac{\alpha}{K}\sum_{z_n>z_{gt}}(z_{gt}-z_n)\) & \\
+ Regularization 
  & \(\tfrac{\alpha}{M}\sum_{z_m<z_{gt}}(z_{gt}-z_m)\) 
  & 75.98 \\
\textcolor{customgray}{+ Error-Enhancement} 
  & \textcolor{customgray}{\(\tfrac{\alpha}{N}\sum_{z_n>z_{gt}}(z_{gt}-z_n)\)}
  & \textcolor{customgray}{73.63} \\
\textcolor{customgray}{+ Error-Enhancement} 
  & \textcolor{customgray}{\(\alpha\,(z_{gt}-z_{\textit{max}})\)}
  & \textcolor{customgray}{73.69} \\
\midrule
+ MaxSup 
  & \(\alpha\Bigl(z_{\textit{max}} - \tfrac{1}{K}\!\sum_{k=1}^K z_k\Bigr)\)
  & 76.12 \\
\bottomrule
\end{tabular}
\end{table}
From \Cref{tab:pre}, it is evident that LS’s overall accuracy boost is exclusively attributed to the regularization component, whereas error-enhancement consistently degrades performance (73.63\% or 73.69\%). Removing the error-enhancement term while keeping only the regularization improves accuracy slightly (75.98\% vs.\ 75.91\%). Finally, by avoiding error-enhancement entirely and preserving the helpful regularization, \textbf{MaxSup} achieves 76.12\% accuracy—surpassing LS. This result underscores that MaxSup directly addresses LS’s primary shortcoming by consistently applying the intended regularization even when the model’s top-1 prediction is incorrect.


\subsection{Max Suppression Regularization}  
\label{sec:maxsup}  
Label Smoothing (LS) suffers from two main limitations: \emph{inconsistent regularization} and \emph{error amplification}. As discussed in \cref{sec:revisiting} and illustrated in Table~\ref{tab:pre}, LS penalizes the ground-truth logit \(z_{gt}\) even for misclassified examples, thereby unnecessarily widening the gap between \(z_{gt}\) and the incorrect top-1 logit. To address these critical shortcomings, we propose \textbf{Max Suppression Regularization (MaxSup)}, which explicitly penalizes the largest logit \(z_{\textit{max}}\) rather than \(z_{gt}\). This crucial shift ensures uniform regularization across both correct and misclassified samples, effectively eliminating the error-amplification issue seen in LS (Table~\ref{tab:pre}), and preserving the integrity of the ground-truth logit for more stable and robust learning performance.

\begin{definition}\textbf{Max Suppression Regularization}

We define the Cross-Entropy loss with MaxSup as follows:
\begin{equation}
\underbrace{H(\mathbf{s}, \mathbf{q})}_{\text{CE with Soft Labels}}
\;=\;
\underbrace{H(\mathbf{y}, \mathbf{q})}_{\text{CE with Hard Labels}}
\;+\;
\underbrace{L_{\textit{MaxSup}}}_{\text{Max Suppression Loss}},
\end{equation}
where
\begin{equation}
\label{eq:label-MaxSup}
L_{\textit{MaxSup}}
\;=\;
\alpha \Bigl(
    H\bigl(\tfrac{\mathbf{1}}{K}, \mathbf{q}\bigr)
    \;-\;
    H(\mathbf{y}', \mathbf{q})
\Bigr),
\end{equation}
and
\[
y'_k
\;=\;
\mathds{1}_{\bigl\{\,k = \arg\max(\mathbf{q})\bigr\}},
\]
so that \(y'_k = 1\) identifies the model's top-1 prediction and \(y'_k = 0\) otherwise. Here, \(H\bigl(\tfrac{\mathbf{1}}{K}, \mathbf{q}\bigr)\) encourages a uniform output distribution to mitigate overconfidence, while \(H(\mathbf{y}', \mathbf{q})\) penalizes the current top-1 logit. By shifting the penalty from \(z_{gt}\) (the ground-truth logit) to \(z_{\textit{max}}\) (the highest logit), MaxSup avoids unduly suppressing \(z_{gt}\) when the model misclassifies, thus overcoming Label Smoothing’s principal shortcoming.
\end{definition}

\paragraph{Logit-Level Formulation of MaxSup}
Building on the logit-level perspective introduced for LS in \cref{sec:revisiting}, we can express \(L_{\textit{MaxSup}}\) as:
\begin{equation}
\label{eq:maxsup-uni}
L_{\textit{MaxSup}} 
\;=\;
\alpha 
\Bigl(
    z_{\textit{max}} 
    \;-\; 
    \tfrac{1}{K}\sum_{k=1}^{K} z_k
\Bigr),
\end{equation}
where \(z_{\textit{max}} = \max_k\{z_k\}\) is the largest (top-1) logit, and \(\tfrac{1}{K}\sum_{k=1}^{K} z_k\) is the mean logit. Unlike LS, which penalizes the ground-truth logit \(z_{gt}\) and may worsen errors in misclassified samples, MaxSup shifts the highest logit uniformly, thus providing consistent regularization for both correct and incorrect predictions. As shown in Table~\ref{tab:pre}, this approach eliminates LS’s error-amplification issue while preserving the intended overconfidence suppression. 

\paragraph{Comparison with Label Smoothing}
MaxSup fundamentally differs from LS in handling correct and incorrect predictions. When \( z_{gt} = z_{\textit{max}} \), both LS and MaxSup similarly reduce overconfidence. However, when \( z_{gt} \neq z_{\textit{max}} \), LS continues to shrink \( z_{gt} \), widening the gap with the incorrect logit, whereas MaxSup penalizes \( z_{\textit{max}} \), preserving \( z_{gt} \) from undue suppression. As illustrated in \Cref{fig:gradcam}, this allows the model to recover from mistakes more effectively and avoid reinforcing incorrect predictions.

\paragraph{Gradient Analysis}
To understand MaxSup’s optimization dynamics, we compute its gradients with respect to each logit \(z_k\). Specifically,
\begin{equation}
\frac{\partial L_{\textit{MaxSup}}}{\partial z_k}
\;=\;
\begin{cases}
\alpha \Bigl(1 - \tfrac{1}{K}\Bigr), & \text{if } k = \arg\max(\mathbf{q}),\\
-\tfrac{\alpha}{K}, & \text{otherwise}.
\end{cases}
\end{equation}
Thus, the top-1 logit \(z_{\textit{max}}\) is reduced by \(\alpha\bigl(1 - \tfrac{1}{K}\bigr)\), while all other logits increase slightly by \(\tfrac{\alpha}{K}\). In misclassified cases, the ground-truth logit \(z_{gt}\) is therefore spared from penalization, thereby avoiding the error-amplification issue seen in LS. For completeness, Appendix~A provides a full derivation of these gradients, and Figure~\ref{fig:logit-analysis} compares the resulting logit distributions under different regularizers.

\paragraph{Behavior Across Different Samples}  
MaxSup applies a dynamic penalty that depends on the model’s current predictions. For high-confidence, correctly classified examples, it behaves similarly to LS by reducing overconfidence, thus effectively mitigating overfitting. In contrast, for misclassified or uncertain samples, MaxSup specifically suppresses the incorrect top-1 logit, further safeguarding the ground-truth logit \(z_{gt}\). This selective strategy preserves an accurate representation of the true class while actively discouraging the propagation of errors. As shown in Section~\ref{sec:results} and Table~\ref{tab:deit-small-comparison}, this promotes more robust decision boundaries and ultimately leads to stronger generalization performance.

\paragraph{Theoretical Insights and Practical Benefits}
MaxSup provides both theoretical and practical advantages compared to LS. Whereas LS applies a uniform penalty to the ground-truth logit regardless of correctness, MaxSup focuses on penalizing only the most confident logit \(z_{\textit{max}}\). This dynamic adjustment prevents error accumulation in misclassifications, thereby ensuring more stable convergence. As a result, MaxSup achieves stronger generalization, exhibits greater robustness to label noise, and performs well on challenging datasets. Moreover, as shown in Section~\ref{sec:feature}, MaxSup preserves higher intra-class diversity, which substantially improves transfer learning performance (Table~\ref{tab:validation_performance}) and yields more interpretable activation maps (Figure~\ref{fig:gradcam}).


\section{Analysis of MaxSup's Learning Benefits}
\label{sec:feature}
MaxSup simultaneously promotes \textbf{inter-class separability} and \textbf{intra-class variation}, both essential for robust classification and effective feature transfer. In this section, we explore how MaxSup achieves these objectives and contrast its effectiveness with alternative regularization methods.

\subsection{Intra-Class Variation and Transferability}
As noted in Section~\ref{sec:revisiting}, \textbf{Label Smoothing (LS)} primarily restricts overconfidence when the ground-truth class is correctly predicted, inadvertently causing \emph{error enhancement} for misclassified samples. This selective penalty can overly compress intra-class diversity. In contrast, \textbf{MaxSup} uniformly penalizes the top-1 logit in both correct and incorrect cases, eliminating LS’s error-enhancement component and thus preserving more fine-grained distinctions within each class.
Table~\ref{tab:feature} compares \emph{intra-class variation} $\Bar{d}_{\text{within}}$ and \emph{inter-class separability} $R^2$ \citep{kornblith2021better} for a ResNet-50 model trained on ImageNet-1K. Although all regularization strategies reduce $\Bar{d}_{\text{within}}$ relative to the baseline, MaxSup shows the smallest reduction, implying stronger retention of within-class variability—often correlated with improved generalization and transferability.
The benefits of this richer intra-class structure appear clearly in Table~\ref{tab:validation_performance}, where linear transfer performance on CIFAR-10 is reported. Although LS and Logit Penalty improve ImageNet accuracy, they diminish transfer accuracy by over-suppressing informative features. In contrast, MaxSup preserves transfer performance near that of the baseline, suggesting it retains crucial, discriminative features that generalize effectively to downstream tasks.


\begin{table}[t]
\setlength{\tabcolsep}{6pt}
\renewcommand{\arraystretch}{1.2}
\centering
\scriptsize
\caption{Metrics for feature representation quality using ResNet-50 trained on ImageNet-1K. 
We report \emph{intra-class variation} ($\Bar{d}_\text{within}$) and \emph{inter-class separability} ($R^2$), both of which benefit from higher values. 
Although all methods reduce $\Bar{d}_\text{within}$ relative to the baseline, MaxSup preserves the most within-class diversity.}
\label{tab:feature}
\begin{tabular}{@{}l|cc|cc@{}}
\toprule
\multirow{2}{*}{\textbf{Method}} 
& \multicolumn{2}{c|}{$\Bar{d}_\text{within} \uparrow$} 
& \multicolumn{2}{c}{$R^2 \uparrow$} \\
\cmidrule{2-5}
& \textbf{Train} & \textbf{Val} & \textbf{Train} & \textbf{Val} \\
\midrule
Baseline & 0.3114 & 0.3313 & 0.4025 & 0.4451 \\
\hline
LS       & 0.2632 & 0.2543 & 0.4690 & 0.4611 \\
OLS      & 0.2707 & 0.2820 & 0.5943 & 0.5708 \\
Zipf's   & 0.2611 & 0.2932 & 0.5522 & 0.4790 \\
MaxSup   & \textbf{0.2926} & \underline{0.2998} & 0.5188 & 0.4972 \\
Logit Penalty & \underline{0.2840} & \textbf{0.3144} & 0.6448 & 0.6024 \\
\bottomrule
\end{tabular}
\end{table}



\subsection{Impact of Logit Regularization}
Different regularization methods impose distinct constraints on the logit space, thereby shaping the model’s representational capacity~\citep{kornblith2021better}. Among these approaches, \textbf{Logit Penalty} and \textbf{MaxSup} both act directly on logits but differ fundamentally in how they apply regularization.
Logit Penalty operates by minimizing the $\ell_2$-norm of the entire logit vector, causing a global reduction in logit magnitudes that often induces sparsity. This uniform shrinkage can limit intra-class variation, thereby weakening the model’s ability to transfer features to downstream tasks. In contrast, MaxSup targets only the largest (top-1) logit, nudging it closer to the average logit. By selectively penalizing only the most confident prediction, MaxSup avoids universal shrinkage and preserves richer intra-class diversity, a property crucial for effective transferability.
Figure~\ref{fig:logit-analysis} illustrates the distribution of logits under various regularizers. Logit Penalty yields a narrower logit range, reflecting excessive sparsity and aligning with its lower transfer performance (Table~\ref{tab:validation_performance}). By comparison, MaxSup maintains broader logit distributions, thereby retaining the fine-grained feature distinctions needed to excel on downstream tasks.




\begin{table}[t]
\centering
\scriptsize
\caption{Validation performance on CIFAR-10 with a linear probe using $l_2$-regularized multinomial logistic regression. Although Label Smoothing and Logit Penalty improve ImageNet accuracy, they substantially degrade transfer accuracy compared to MaxSup.}
\label{tab:validation_performance}
\begin{tabular}{@{}l|c@{}}
\toprule
\textbf{Method} & \textbf{Linear Transfer Acc.} \\
\midrule
Baseline & 0.8143 \\
\hline
Label Smoothing & 0.7458 \\
Logit Penalty \citep{dauphin2021deconstructing} & 0.7242 \\
MaxSup & \textbf{0.8102} \\
\bottomrule
\end{tabular}
\end{table}



\begin{figure}[t]
    \centering
    \includegraphics[trim={0cm 0cm 0cm 1cm},clip,width=0.45\textwidth]{images/logit.png}
    \vspace{-2mm}
    \caption{Logit density plots under three different regularization strategies: MaxSup, Logit Penalty, and standard Cross Entropy. Logit Penalty induces a narrower logit distribution, reflecting excessive shrinkage that reduces intra-class variation. By contrast, MaxSup preserves a broader range of logits and thus richer representations.}
    \vspace{-1mm}
    \label{fig:logit-analysis}
\end{figure}

