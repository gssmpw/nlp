\section{Proof of Lemma 3.2}
\label{sec:proof_lem}

\begin{proof}
We aim to demonstrate the validity of Lemma 3.2, which states:

\begin{equation}
H(\mathbf{s}, \mathbf{q}) = H(\mathbf{y}, \mathbf{q}) + L_{LS}
\end{equation}

where $L_{LS} = \alpha\left(H\left(\frac{\mathbf{1}}{K}, \mathbf{q}\right) - H(\mathbf{y}, \mathbf{q})\right)$

Let us proceed with the proof:

We begin by expressing the cross-entropy $H(\mathbf{s}, \mathbf{q})$:

\begin{equation}
H(\mathbf{s}, \mathbf{q}) = -\sum_{k=1}^K s_k \log q_k
\end{equation}

In the context of label smoothing, $s_k$ is defined as:

\begin{equation}
s_k = (1-\alpha)y_k + \frac{\alpha}{K}
\end{equation}

where $\alpha$ is the smoothing parameter, $y_k$ is the original label, and $K$ is the number of classes.

Substituting this expression for $s_k$ into the cross-entropy formula:

\begin{equation}
H(\mathbf{s}, \mathbf{q}) = -\sum_{k=1}^K \left((1-\alpha)y_k + \frac{\alpha}{K}\right) \log q_k
\end{equation}

Expanding the sum:

\begin{equation}
H(\mathbf{s}, \mathbf{q}) = -(1-\alpha)\sum_{k=1}^K y_k \log q_k - \frac{\alpha}{K}\sum_{k=1}^K \log q_k
\end{equation}

We recognize that the first term is equivalent to $(1-\alpha)H(\mathbf{y}, \mathbf{q})$, and the second term to $\alpha H(\frac{\mathbf{1}}{K}, \mathbf{q})$. Thus:

\begin{equation}
H(\mathbf{s}, \mathbf{q}) = (1-\alpha)H(\mathbf{y}, \mathbf{q}) + \alpha H\left(\frac{\mathbf{1}}{K}, \mathbf{q}\right)
\end{equation}

Rearranging the terms:

\begin{equation}
H(\mathbf{s}, \mathbf{q}) = H(\mathbf{y}, \mathbf{q}) + \alpha\left(H\left(\frac{\mathbf{1}}{K}, \mathbf{q}\right) - H(\mathbf{y}, \mathbf{q})\right)
\end{equation}

We can now identify $H(\mathbf{y}, \mathbf{q})$ as the original cross-entropy loss and $L_{LS} = \alpha\left(H\left(\frac{\mathbf{1}}{K}, \mathbf{q}\right) - H(\mathbf{y}, \mathbf{q})\right)$ as the Label Smoothing loss.

Therefore, we have demonstrated that:

\begin{equation}
H(\mathbf{s}, \mathbf{q}) = H(\mathbf{y}, \mathbf{q}) + L_{LS}
\end{equation}

with $L_{LS}$ as defined in the lemma. It is noteworthy that the original cross-entropy loss $H(\mathbf{y}, \mathbf{q})$ remains unweighted by $\alpha$ in this decomposition, which is consistent with the statement in Lemma 3.2
\end{proof}

\section{Proof of \Cref{th:ce_ls}}
\label{proof:the}

\begin{proof}

We aim to prove the equation:
\begin{equation}
L_{\textit{LS}}=\alpha (z_{gt} - \frac{1}{K} \sum^{K}_{k = 1} z_k)
\end{equation}

Let $\mathbf{s}$ be the smoothed label vector and $\mathbf{q}$ be the predicted probability vector. We start with the cross-entropy between $\mathbf{s}$ and $\mathbf{q}$:

\begin{equation}
H(\mathbf{s}, \mathbf{q}) = -\sum_{k=1}^{K} s_k \log q_k
\end{equation}

With label smoothing, $s_k = (1 - \alpha) y_k + \frac{\alpha}{K}$, where $\mathbf{y}$ is the one-hot ground truth vector and $\alpha$ is the smoothing parameter. Substituting this:

\begin{equation}
H(\mathbf{s}, \mathbf{q}) = -\sum_{k=1}^{K} [(1 - \alpha) y_k + \frac{\alpha}{K}] \log q_k
\end{equation}

Expanding:

\begin{equation}
H(\mathbf{s}, \mathbf{q}) = -(1 - \alpha)\sum_{k=1}^{K} y_k \log q_k - \frac{\alpha}{K}\sum_{k=1}^{K} \log q_k
\end{equation}

Since $\mathbf{y}$ is a one-hot vector, $\sum_{k=1}^{K} y_k \log q_k = \log q_{gt}$, where $gt$ is the index of the ground truth class:

\begin{equation}
H(\mathbf{s}, \mathbf{q}) = -(1 - \alpha)\log q_{gt} - \frac{\alpha}{K}\sum_{k=1}^{K} \log q_k
\end{equation}

Using the softmax function, $q_k = \frac{e^{z_k}}{\sum_{j=1}^{K} e^{z_j}}$, we can express $\log q_k$ in terms of logits:

\begin{equation}
\log q_k = z_k - \log(\sum_{j=1}^{K} e^{z_j})
\end{equation}

Substituting this into our expression:

\begin{equation}
\begin{split}
H(\mathbf{s}, \mathbf{q}) = &-(1 - \alpha)[z_{gt} - \log(\sum_{j=1}^{K} e^{z_j})] \\
&- \frac{\alpha}{K}\sum_{k=1}^{K} [z_k - \log(\sum_{j=1}^{K} e^{z_j})] \\
= &-(1 - \alpha)z_{gt} + (1 - \alpha)\log(\sum_{j=1}^{K} e^{z_j}) \\
&- \frac{\alpha}{K}\sum_{k=1}^{K} z_k + \alpha\log(\sum_{j=1}^{K} e^{z_j}) \\
= &-(1 - \alpha)z_{gt} - \frac{\alpha}{K}\sum_{k=1}^{K} z_k + \log(\sum_{j=1}^{K} e^{z_j})
\end{split}
\end{equation}


Rearranging:

\begin{equation}
H(\mathbf{s}, \mathbf{q}) = -z_{gt} + \log(\sum_{j=1}^{K} e^{z_j}) + \alpha[z_{gt} - \frac{1}{K}\sum_{k=1}^{K} z_k]
\end{equation}

We can identify:
\begin{itemize}
    \item $H(\mathbf{y}, \mathbf{q}) = -z_{gt} + \log(\sum_{j=1}^{K} e^{z_j})$ (cross-entropy for one-hot vector $\mathbf{y}$)
    \item $L_{\textit{LS}} = \alpha[z_{gt} - \frac{1}{K}\sum_{k=1}^{K} z_k]$
\end{itemize}

Thus, we have proven:
\begin{equation}
H(\mathbf{s}, \mathbf{q}) = H(\mathbf{y}, \mathbf{q}) + L_{\textit{LS}}
\end{equation}

Due to the broad usage of CutMix and Mixup in the training recipe of modern Neural Networks, we additionally take their impact into account together with Label Smoothing. Now we additionally prove the case \textbf{with Cutmix and Mixup}:
\begin{equation}
L^{\prime}_{\textit{LS}} = \alpha (\left(\lambda z_{gt1} + (1 - \lambda) z_{gt2}\right) - \frac{1}{K} \sum_{k=1}^K z_k)
\end{equation}

With Cutmix and Mixup, the smoothed label becomes:
\begin{equation}
s_k = (1 - \alpha) (\lambda y_{k1} + (1-\lambda) y_{k2}) + \frac{\alpha}{K}
\end{equation}
where $y_{k1}$ and $y_{k2}$ are one-hot vectors for the two ground truth classes from mixing, and $\lambda$ is the mixing ratio.

Starting with the cross-entropy:

\begin{align}
H(\mathbf{s}, \mathbf{q}) &= -\sum_{k=1}^{K} s_k \log q_k \\
&= -\sum_{k=1}^{K} [(1 - \alpha) (\lambda y_{k1} + (1-\lambda) y_{k2}) + \frac{\alpha}{K}] \log q_k \\
&= -(1 - \alpha)\sum_{k=1}^{K} (\lambda y_{k1} + (1-\lambda) y_{k2}) \log q_k - \frac{\alpha}{K}\sum_{k=1}^{K} \log q_k
\end{align}

Since $y_{k1}$ and $y_{k2}$ are one-hot vectors:

\begin{equation}
H(\mathbf{s}, \mathbf{q}) = -(1 - \alpha)(\lambda \log q_{gt1} + (1-\lambda) \log q_{gt2}) - \frac{\alpha}{K}\sum_{k=1}^{K} \log q_k
\end{equation}

where $gt1$ and $gt2$ are the indices of the two ground truth classes.

Using $q_k = \frac{e^{z_k}}{\sum_{j=1}^{K} e^{z_j}}$, we express in terms of logits:

\begin{align}
H(\mathbf{s}, \mathbf{q}) &= -(1 - \alpha)[\lambda (z_{gt1} - \log(\sum_{j=1}^{K} e^{z_j})) + (1-\lambda) (z_{gt2} - \log(\sum_{j=1}^{K} e^{z_j}))] \\
&\quad - \frac{\alpha}{K}\sum_{k=1}^{K} [z_k - \log(\sum_{j=1}^{K} e^{z_j})]
\end{align}

Simplifying:

\begin{align}
H(\mathbf{s}, \mathbf{q}) &= -(1 - \alpha)[\lambda z_{gt1} + (1-\lambda) z_{gt2}] + (1 - \alpha)\log(\sum_{j=1}^{K} e^{z_j}) \\
&\quad - \frac{\alpha}{K}\sum_{k=1}^{K} z_k + \alpha\log(\sum_{j=1}^{K} e^{z_j}) \\
&= -(1 - \alpha)[\lambda z_{gt1} + (1-\lambda) z_{gt2}] - \frac{\alpha}{K}\sum_{k=1}^{K} z_k + \log(\sum_{j=1}^{K} e^{z_j})
\end{align}

Rearranging:

\begin{align}
H(\mathbf{s}, \mathbf{q}) &= -[\lambda z_{gt1} + (1-\lambda) z_{gt2}] + \log(\sum_{j=1}^{K} e^{z_j}) \\
&\quad + \alpha[\lambda z_{gt1} + (1-\lambda) z_{gt2} - \frac{1}{K}\sum_{k=1}^{K} z_k]
\end{align}

We can identify:
\begin{itemize}
    \item $H(\mathbf{y}^{\prime}, \mathbf{q}) = -[\lambda z_{gt1} + (1-\lambda) z_{gt2}] + \log(\sum_{j=1}^{K} e^{z_j})$ (cross-entropy for mixed label $\mathbf{y}^{\prime}$)
    \item $L^{\prime}_{\textit{LS}} = \alpha[\lambda z_{gt1} + (1-\lambda) z_{gt2} - \frac{1}{K}\sum_{k=1}^{K} z_k]$
\end{itemize}

Thus, we have proven:
\begin{equation}
H(\mathbf{s}, \mathbf{q}) = H(\mathbf{y}^{\prime}, \mathbf{q}) + L^{\prime}_{\textit{LS}}
\end{equation}
\end{proof}
This completes the proof for both cases of Theorem \ref{th:ce_ls}.


\section{Gradient Analysis}
\label{app:grad}

\subsection{New Objective Function}

The Cross Entropy with Max Suppression is defined as:

\[
L_{\text{MaxSup}, t}(x, y) = H\left( y_k + \frac{\alpha}{K} - \alpha \cdot \mathbf{1}_{k = \operatorname{argmax}(\vq)}, \vq_t^S(x) \right)
\]

where $H(\cdot, \cdot)$ denotes the cross-entropy function.

\subsection{Gradient Analysis}

The gradient of the loss with respect to the logit $z_i$ for each class $i$ is derived as:

\[
\partial_i^{\text{MaxSup}, t} = y_{t,i} - y_i - \frac{\alpha}{K} + \alpha \cdot \mathbf{1}_{i = \operatorname{argmax}(\vq)}
\]

We analyze this gradient under two scenarios:

\noindent\textbf{Scenario 1: Model makes correct prediction}

In this case, Max Suppression is equivalent to Label Smoothing. When the model correctly predicts the target class ($\operatorname{argmax}(\vq) = \operatorname{GT}$), the gradients are:

\begin{itemize}
\item For the target class (GT): 
      $\partial_{\text{GT}}^{\text{MaxSup}, t} = q_{t,\text{GT}} - \left(1 - \alpha \left(1 - \frac{1}{K}\right)\right)$
\item For non-target classes: 
      $\partial_i^{\text{MaxSup}, t} = q_{t,i} - \frac{\alpha}{K}$
\end{itemize}

\noindent\textbf{Scenario 2: Model makes wrong prediction}

When the model incorrectly predicts the most confident class ($\operatorname{argmax}(\vq) \neq \operatorname{GT}$), the gradients are:

\begin{itemize}
\item For the target class (GT): 
      $\partial_{\text{GT}}^{\text{MaxSup}, t} = q_{t,\text{GT}} - \left(1 + \frac{\alpha}{K}\right)$
\item For non-target classes (not most confident): 
      $\partial_i^{\text{MaxSup}, t} = q_{t,i} - \frac{\alpha}{K}$
\item For the most confident non-target class: 
      $\partial_i^{\text{MaxSup}, t} = q_{t,i} + \alpha \left(1 - \frac{1}{K}\right)$
\end{itemize}

The Max Suppression regularization technique implements a sophisticated gradient redistribution strategy, particularly effective when the model misclassifies samples. When the model's prediction ($\operatorname{argmax}(\vq)$) differs from the ground truth (GT), the gradient for the incorrectly predicted class is increased by $\alpha(1 - \frac{1}{K})$, resulting in $\partial_{\operatorname{argmax}(\vq)}^{\text{MaxSup}, t} = q_{t,\operatorname{argmax}(\vq)} + \alpha(1 - \frac{1}{K})$. Simultaneously, the gradient for the true class is decreased by $\frac{\alpha}{K}$, giving $\partial_{\text{GT}}^{\text{MaxSup}, t} = q_{t,\text{GT}} - (1 + \frac{\alpha}{K})$, while for all other classes, the gradient is slightly reduced by $\frac{\alpha}{K}$: $\partial_i^{\text{MaxSup}, t} = q_{t,i} - \frac{\alpha}{K}$. This redistribution adds a substantial positive gradient to the misclassified class while slightly reducing the gradients for other classes. The magnitude of this adjustment, controlled by the hyperparameter $\alpha$, effectively penalizes overconfident errors and encourages the model to focus on challenging examples. By amplifying the learning signal for misclassifications, Max Suppression regularization promotes more robust learning from difficult or ambiguous samples.



\begin{algorithm}[ht!]
\caption{Gradient Descent with Max Suppression (MaxSup)}
\label{alg:maxsup}
\scriptsize
\begin{algorithmic}[1]
\Require Training set $D = \{(\mathbf{x}^{(i)}, \mathbf{y}^{(i)})\}_{i=1}^{N}$; 
         learning rate $\eta$; 
         number of iterations $T$; 
         smoothing parameter $\alpha$; 
         a neural network $f_\theta(\cdot)$; 
         batch size $B$; 
         total classes $K$.
\State Initialize network weights $\theta$ (e.g., randomly).
\For{$t = 1$ to $T$} 
    \Statex \(\quad\) \textit{// Each iteration processes mini-batches of size \(B\).}
    \For{each mini-batch \(\{(\mathbf{x}^{(j)}, \mathbf{y}^{(j)})\}_{j=1}^{B}\) in \(D\)} 
        \State Compute logits: \(\mathbf{z}^{(j)} \gets f_\theta(\mathbf{x}^{(j)})\) for each sample in the batch
        \State Compute predicted probabilities: \(\mathbf{q}^{(j)} \gets \text{softmax}(\mathbf{z}^{(j)})\)

        \State Compute cross-entropy loss: 
        \[
        L_{\mathrm{CE}} \gets \frac{1}{B} \sum_{j=1}^{B} H\bigl(\mathbf{y}^{(j)}, \mathbf{q}^{(j)}\bigr)
        \]

        \State \(\quad\)\textit{// MaxSup component: penalize the top-1 logit}
        \State For each sample \(j\):
        \[
        z_{\textit{max}}^{(j)} \;=\; \max_{k \in \{1,\dots,K\}} z_k^{(j)}, 
        \quad
        \bar{z}^{(j)} \;=\; \frac{1}{K}\sum_{k=1}^{K} z_k^{(j)}
        \]
        \[
        L_{\mathrm{MaxSup}} \gets \frac{1}{B} \sum_{j=1}^{B} 
            \bigl[
                z_{\textit{max}}^{(j)} - \bar{z}^{(j)}
            \bigr]
        \]

        \State Total loss: 
        \[
        L \;\gets\; L_{\mathrm{CE}} \;+\; \alpha \, L_{\mathrm{MaxSup}}
        \]

        \State Update parameters:
        \[
        \theta \;\gets\; \theta - \eta\,\nabla_{\theta}\, L
        \]

    \EndFor
\EndFor
\end{algorithmic}
\end{algorithm}

\section{Pseudo Code}
\label{app:pseudo}
Algorithm~\ref{alg:maxsup} presents pseudo code illustrating gradient descent with Max Suppression (MaxSup). 
The main difference from standard Label Smoothing lies in penalizing the highest logit rather than the ground-truth logit.




\section{Robustness Under Different Training Recipes}
\label{app:training}
We assess MaxSup’s robustness by testing it under a modified training recipe that reduces total training time and alters the learning rate schedule. This setup models scenarios where extensive training is impractical due to limited resources.

Concretely, we adopt the \textbf{TorchVision V1 Weight} strategy, reducing the total number of epochs to 90 and replacing the cosine annealing schedule with a step learning-rate scheduler (step size = 30). We also set the initial learning rate to 0.1 and use a batch size of 512. This streamlined recipe aims to reach reasonable accuracy within a shorter duration.

As reported in Table~\ref{tab:general_training}, MaxSup continues to deliver strong performance across multiple convolutional architectures, generally surpassing Label Smoothing and its variants. Although all methods see a performance decline in this constrained regime, MaxSup remains among the top performers, reinforcing its effectiveness across diverse training conditions.



\begin{table*}[htbp]
\setlength{\tabcolsep}{4pt}
\centering
\scriptsize
\caption{Performance comparison on ImageNet for various convolutional neural network architectures. 
Results are presented as ``mean ± std'' (percentage). 
\textbf{Bold} and \underline{underlined} entries indicate best and second-best, respectively. 
($^{*}$: implementation details adapted from the official repositories.)}
\label{tab:general_training}
\begin{tabular}{@{}lcccc@{}}
\toprule
\textbf{Method} 
& \textbf{ResNet-18} 
& \textbf{ResNet-50} 
& \textbf{ResNet-101} 
& \textbf{MobileNetV2} \\
\midrule
Baseline 
& 69.11±0.12 
& 76.44±0.10  
& 76.00±0.18 
& \underline{71.42}±0.12 \\
Label Smoothing 
& 69.38±0.19 
& 76.65±0.11 
& 77.01±0.15 
& 71.40±0.09 \\
Zipf-LS$^{*}$ 
& 69.43±0.13 
& \underline{76.89}±0.17  
& 76.91±0.14 
& 71.24±0.16 \\
OLS$^{*}$ 
& \underline{69.45}±0.15 
& 76.81±0.21  
& \underline{77.12}±0.17 
& 71.29±0.11 \\
\textbf{MaxSup} 
& \textbf{69.59}±0.13 
& \textbf{77.08}±0.07 
& \textbf{77.33}±0.12 
& \textbf{71.59}±0.17 \\
Logit Penalty 
& 66.97±0.11 
& 74.21±0.16 
& 75.17±0.12 
& 70.39±0.14 \\
\bottomrule
\end{tabular}
\end{table*}


\section{Increasing Smoothing Weight Schedule}
\label{app:alpha}
Building on the intuition that a model’s confidence naturally grows as training progresses, we propose a linearly increasing schedule for the smoothing parameter \(\alpha\). Concretely, \(\alpha\) is gradually raised from an initial value (e.g., 0.1) to a higher value (e.g., 0.2) by the end of training. This schedule aims to counteract the model’s increasing overconfidence, ensuring that regularization remains appropriately scaled throughout.

\paragraph{Experimental Evidence}
As shown in \Cref{tab:alpha}, both Label Smoothing and MaxSup benefit from this \(\alpha\) scheduler. For Label Smoothing, accuracy improves from 75.91\% to 76.16\%, while MaxSup sees a more pronounced gain, from 76.12\% to 76.58\%. This greater improvement for MaxSup (\(+0.46\%\)) compared to Label Smoothing (\(+0.25\%\)) corroborates our claim that MaxSup successfully addresses the inconsistent regularization and error-enhancement issues of Label Smoothing during misclassifications.



\begin{table*}[ht]
\centering
\footnotesize
\caption{Effect of an $\alpha$ scheduler on model performance. 
Here, $t$ and $T$ denote the current and total epochs, respectively. 
The baseline model does not involve any label smoothing parameter (\(\alpha\)).}
\label{tab:alpha}
\begin{tabular}{@{}lcccc@{}}
\toprule
\textbf{Configuration} 
& \textbf{Formulation} 
& \multicolumn{1}{c}{\(\alpha=0.1\)} 
& \multicolumn{1}{c}{\(\alpha = 0.1 + 0.1\, \tfrac{t}{T}\)} 
& \textbf{Remarks}\\
\midrule
Baseline 
& -- 
& \multicolumn{1}{c}{74.21} 
& \multicolumn{1}{c}{74.21} 
& \(\alpha\) not used \\

LS 
& \(\alpha\Bigl(z_{gt} - \tfrac{1}{K}\sum_{k} z_k\Bigr)\)
& 75.91 
& 76.16 
&  \\
MaxSup 
& \(\alpha\Bigl(z_{\textit{max}} - \tfrac{1}{K}\sum_{k} z_k\Bigr)\)
& 76.12 
& \textbf{76.58} 
&  \\
\bottomrule
\end{tabular}
\end{table*}




\section{Visualization of the Learned Feature Space} 
\label{sec:vis_feature}
To illustrate the differences between Max Suppression Regularization and Label Smoothing, we follow the projection technique of \citet{muller2019does}. Specifically, we select three semantically related classes and construct an orthonormal basis for the plane intersecting their class templates in feature space. We then project each sample’s penultimate-layer activation vector onto this plane. To ensure the visual clarity of the resulting plots, we randomly sample 80 images from the training or validation set for each of the three classes.

\paragraph{Selection Criteria}
We choose these classes according to two main considerations:
\begin{enumerate}
    \item \textbf{Semantic Similarity.} We pick three classes that are visually and semantically close.
    \item \textbf{Confusion.} We identify a class that the Label Smoothing (LS)–trained model frequently misclassifies and select two additional classes involved in those misclassifications (\cref{bd_c}, \cref{bd_c1}). Conversely, we also examine a scenario where a class under Max Suppression is confused with others, highlighting key differences (\cref{bd_d}, \cref{bd_d1}).
\end{enumerate}


\begin{figure*}[ht]
\captionsetup{font=footnotesize,labelfont=footnotesize}
    \centering
    \begin{subfigure}[b]{0.245\textwidth}
        \includegraphics[width=\textwidth]{images/db/67_maxsup_val.pdf}
    \end{subfigure}
    % \hfill
    \begin{subfigure}[b]{0.245\textwidth}
        \includegraphics[width=\textwidth]{images/db/282_maxsup_val.pdf}
    \end{subfigure}
    % \hfill
    \begin{subfigure}[b]{0.245\textwidth}
        \includegraphics[width=\textwidth]{images/db/893_maxsup_val.pdf}
    \end{subfigure}
    \begin{subfigure}[b]{0.245\textwidth}
        \includegraphics[width=\textwidth]{images/db/782_maxsup_val.pdf}
    \end{subfigure}
    % \vspace{0.5em}    
    \centering
    \begin{subfigure}[b]{0.245\textwidth}
        \includegraphics[width=\textwidth]{images/db/67_nols_val.pdf}
    \end{subfigure}
    % \hfill
    \begin{subfigure}[b]{0.245\textwidth}
        \includegraphics[width=\textwidth]{images/db/282_nols_val.pdf}
    \end{subfigure}
    % \hfill
    \begin{subfigure}[b]{0.245\textwidth}
        \includegraphics[width=\textwidth]{images/db/893_nols_val.pdf}
    \end{subfigure}
    \begin{subfigure}[b]{0.245\textwidth}
        \includegraphics[width=\textwidth]{images/db/782_nols_val.pdf}
    \end{subfigure}
    \begin{subfigure}[b]{0.245\textwidth}    \includegraphics[width=\textwidth]{images/db/67_ls_val.pdf}
        \caption{Semantically Similar Classes}
        \label{bd_a}
    \end{subfigure}
    % \hfill
    \begin{subfigure}[b]{0.245\textwidth}     \includegraphics[width=\textwidth]{images/db/282_ls_val.pdf}
        \caption{Semantically Similar Classes}
        \label{bd_b}
    \end{subfigure}
    % \hfill
    \begin{subfigure}[b]{0.245\textwidth}     \includegraphics[width=\textwidth]{images/db/893_ls_val.pdf}
        \caption{Confusing Classes (LS)}
        \label{bd_c}
    \end{subfigure}
    \begin{subfigure}[b]{0.245\textwidth}      \includegraphics[width=\textwidth]{images/db/782_ls_val.pdf}
        \caption{Confusing Classes (MaxSup)}
        \label{bd_d}
    \end{subfigure}
    \caption{Visualization of penultimate-layer activations from DeiT-Small (trained with CutMix and Mixup) on the ImageNet \textbf{validation} set. 
The top row shows embeddings for a MaxSup-trained model, and the bottom row shows embeddings for a Label Smoothing (LS)–trained model. 
In each subfigure, classes are either \textit{semantically similar} or \textit{confusingly labeled}. 
Compared to LS, MaxSup yields more pronounced inter-class separability and richer intra-class diversity, suggesting stronger representation and classification performance.}
    \label{fig:comparison}
\end{figure*}

\begin{figure*}[ht]
\captionsetup{font=footnotesize,labelfont=footnotesize}
    \centering
    \begin{subfigure}[b]{0.245\textwidth}
        \includegraphics[width=\textwidth]{images/db/67_maxsup_train.pdf}
    \end{subfigure}
    % \hfill
    \begin{subfigure}[b]{0.245\textwidth}
        \includegraphics[width=\textwidth]{images/db/282_maxsup_train.pdf}
    \end{subfigure}
    % \hfill
    \begin{subfigure}[b]{0.245\textwidth}
        \includegraphics[width=\textwidth]{images/db/893_maxsup_train.pdf}
    \end{subfigure}
    \begin{subfigure}[b]{0.245\textwidth}
        \includegraphics[width=\textwidth]{images/db/782_maxsup_train.pdf}
    \end{subfigure}
    % \vspace{0.5em}    
    \centering
    \begin{subfigure}[b]{0.245\textwidth}
        \includegraphics[width=\textwidth]{images/db/67_nols_train.pdf}
    \end{subfigure}
    % \hfill
    \begin{subfigure}[b]{0.245\textwidth}
        \includegraphics[width=\textwidth]{images/db/282_nols_train.pdf}
    \end{subfigure}
    % \hfill
    \begin{subfigure}[b]{0.245\textwidth}
        \includegraphics[width=\textwidth]{images/db/893_nols_train.pdf}
    \end{subfigure}
    \begin{subfigure}[b]{0.245\textwidth}
        \includegraphics[width=\textwidth]{images/db/782_nols_train.pdf}
    \end{subfigure}
    \begin{subfigure}[b]{0.245\textwidth}    \includegraphics[width=\textwidth]{images/db/67_ls_train.pdf}
        \caption{Semantically Similar Classes}
        \label{bd_a1}
    \end{subfigure}
    % \hfill
    \begin{subfigure}[b]{0.245\textwidth}     \includegraphics[width=\textwidth]{images/db/282_ls_train.pdf}
        \caption{Semantically Similar Classes}
        \label{bd_b1}
    \end{subfigure}
    % \hfill
    \begin{subfigure}[b]{0.245\textwidth}     \includegraphics[width=\textwidth]{images/db/893_ls_train.pdf}
        \caption{Confusing Classes (LS)}
        \label{bd_c1}
    \end{subfigure}
    \begin{subfigure}[b]{0.245\textwidth}      \includegraphics[width=\textwidth]{images/db/782_ls_train.pdf}
        \caption{Confusing Classes (MaxSup)}
        \label{bd_d1}
    \end{subfigure}
    

   \caption{Visualization of the penultimate-layer activations for DeiT-Small (trained with CutMix and Mixup) on selected ImageNet classes. 
The top row shows results for a MaxSup-trained model; 
the bottom row shows Label Smoothing (LS). 
In (a,b), the model must distinguish \textit{semantically similar} classes (e.g., Saluki vs.\ Grey Fox; Tow Truck vs.\ Pickup), 
while (c,d) involve \textit{confusing categories} (e.g., Jean vs.\ Shoe Shop, Stinkhorn vs.\ related objects). Compared to LS, MaxSup yields both improved inter-class separability and richer intra-class variation, indicating more robust representation learning.}
    \label{fig:comparison1}
\end{figure*}

\paragraph{Observations}
As shown in \Cref{fig:comparison1,fig:comparison}, models trained with \textbf{Max Suppression} exhibit:
\begin{itemize}
    \item \textbf{Enhanced inter-class separability.} Distinct classes occupy more clearly separated regions, aligning with improved classification performance.
    \item \textbf{Greater intra-class variation.} Instances within a single class are not overly compressed, indicating a richer representation of subtle differences.
\end{itemize}

For instance, images of \emph{Schipperke} dogs can differ markedly in viewpoint, lighting, background, or partial occlusions. Max Suppression preserves such intra-class nuances in the feature space, enabling the semantic distances to visually related classes (e.g., Saluki, Grey Fox, or Belgian Sheepdog) to dynamically adjust for each image. Consequently, Max Suppression provides a more flexible, fine-grained representation that facilitates better class discrimination.

\begin{table*}[ht]
\setlength{\tabcolsep}{6pt}
\renewcommand{\arraystretch}{1.2}
\centering
\scriptsize
\caption{
Feature representation metrics for a ResNet-50 model trained on ImageNet-1K, reported on both Training and Validation sets. 
We measure intra-class variation ($\bar{d}_\text{within}$) and overall average distance ($\bar{d}_\text{total}$). 
Inter-class separability ($R^2$) is calculated as 
\(R^2 = 1 - \frac{\bar{d}_\text{within}}{\bar{d}_\text{total}}\). 
Higher values (\(\uparrow\)) of \(\bar{d}_\text{within}\) and \(R^2\) are preferred. 
}
\label{tab:feature2}
\begin{tabular}{@{}l|cc|cc|cc@{}}
\toprule
\multirow{2}{*}{\textbf{Method}} 
& \multicolumn{2}{c|}{$\bar{d}_\text{within} \uparrow$} 
& \multicolumn{2}{c|}{$\bar{d}_\text{total}$} 
& \multicolumn{2}{c}{$R^2 \uparrow$} \\
\cmidrule{2-7}
& \textbf{Train} & \textbf{Val} & \textbf{Train} & \textbf{Val} & \textbf{Train} & \textbf{Val} \\
\midrule
Baseline 
& 0.3114 & 0.3313 & 0.5212 & 0.5949 & 0.4025 & 0.4451 \\
\hline
LS 
& 0.2632 & 0.2543 & 0.4862 & 0.4718 & 0.4690 & 0.4611 \\
OLS 
& 0.2707 & 0.2820 & 0.6672 & 0.6570 & 0.5943 & 0.5708 \\
Zipf’s 
& 0.2611 & 0.2932 & 0.5813 & 0.5628 & 0.5522 & 0.4790 \\
\textbf{MaxSup}
& \textbf{0.2926 (+0.03)} & \textbf{0.2998 (+0.05)} 
& 0.6081 (+0.12) & 0.5962 (+0.12) 
& 0.5188 (+0.05) & 0.4972 (+0.04) \\
Logit Penalty 
& 0.2840 & 0.3144 & 0.7996 & 0.7909 & 0.6448 & 0.6024 \\
\bottomrule
\end{tabular}
\end{table*}

