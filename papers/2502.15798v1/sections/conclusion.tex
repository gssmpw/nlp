\section{Conclusion}  
In this work, we carefully examined the root causes of Label Smoothing’s (LS) shortcomings and introduced \textbf{Max Suppression Regularization (MaxSup)} as a targeted remedy. Our analysis shows that LS can unintentionally promote overconfidence in misclassified samples by applying insufficient regularization to erroneous top-1 logits. In contrast, MaxSup effectively addresses this by consistently penalizing the most confident logit, regardless of prediction correctness. Through extensive experiments and in-depth analyses, we demonstrate that MaxSup not only improves accuracy but also preserves richer intra-class variation and significantly enhances inter-class separability. Consequently, models trained with MaxSup capture finer-grained information about individual samples, ultimately leading to stronger transfer learning capabilities. Class activation maps further reveal that MaxSup directs model attention more accurately toward salient parts of target objects, effectively mitigating distractions from background elements.

\section*{Limitations \& Future Work}
Although our findings validate MaxSup’s effectiveness, several directions merit further investigation. Prior work~\citep{muller2019does} shows that teachers trained with LS can degrade performance in Knowledge Distillation~\citep{hinton2015distilling}, and \citet{guo2024cross} suggests LS accelerates convergence via conditioning number analysis. Future research could explore MaxSup’s impact on Knowledge Distillation workflows and its influence on training convergence. Additionally, recent studies~\citep{sukenik2024neural,garrod2024persistence} indicate that \(\ell_2\) regularization biases final layer features and weights toward lower-rank solutions than those typically associated with neural collapse. Investigating how MaxSup interacts with these low-rank biases and whether it leads to similarly optimal or novel solutions is another intriguing avenue for future work.

\section*{Impact Statement}
This paper advances machine learning regularization techniques through an in-depth analysis of Label Smoothing and a proposed alternative, Max Suppression Regularization. By reinforcing generalization and transferability on widely used computer vision tasks, MaxSup has the potential to benefit a broad array of real-world applications, improving robustness without introducing known negative consequences. Thus, we anticipate a net positive impact, offering practitioners a clearer understanding of network regularization and access to more effective training methodologies.




