\section{Related Work}
We survey regularization techniques before focusing on Label Smoothing (LS) and highlighting how \emph{MaxSup} differs.

\subsection{Regularization}
Regularization enhances the generalization of deep neural networks by limiting model complexity through various strategies. Classical approaches such as $\ell_2$~\citep{krogh1991simple} and $\ell_1$~\citep{zou2005regularization} regularization constrain large or sparse weights, respectively, while Dropout~\citep{Srivastava2014} randomly deactivates neurons to prevent feature co-adaptation. Among loss-based methods, Label Smoothing (LS)~\citep{szegedy2016rethinking} redistributes probability mass away from the ground-truth class, improving both accuracy and calibration~\citep{muller2019does}. Variants like Online Label Smoothing (OLS)~\citep{zhang2021delving} and Zipf Label Smoothing (Zipf-LS)~\citep{liang2022efficient} adapt LS by considering the modelâ€™s evolving predictions, yet they still fail to address the fundamental issue that arises when the ground-truth logit is not maximal (\cref{sec:revisiting}, Table~\ref{tab:pre}).
Other loss-based regularizers, such as Confidence Penalty~\citep{pereyra2017regularizing} and Logit Penalty~\citep{dauphin2021deconstructing}, target different aspects of the output distribution. Confidence Penalty discourages overconfident predictions, whereas Logit Penalty minimizes the global $\ell_2$-norm of logits to improve feature separability~\citep{kornblith2021better}. However, Logit Penalty can reduce intra-class variation, impairing transfer learning (\cref{sec:feature}).

\paragraph{Our MaxSup approach}  
MaxSup diverges from these methods by selectively penalizing only the top-1 logit \((z_{\textit{max}})\) rather than the ground-truth logit \((z_{gt})\). Unlike LS-based techniques, which can exacerbate errors by excessively shrinking \(z_{gt}\) for misclassified samples, MaxSup uniformly applies regularization to all predictions, regardless of correctness. Consequently, it effectively sidesteps the \emph{error-enhancement} issue, preserves richer intra-class diversity (\cref{tab:feature}), and sustains robust transfer performance across various datasets and architectures (\cref{tab:validation_performance}).

\subsection{Studies on Label Smoothing}
Label Smoothing (LS) has also been extensively examined in the context of knowledge distillation. \citet{yuan2020revisiting} showed that LS can act as a proxy for distillation, while \citet{shen2021label} explored its role within teacher--student frameworks. \citet{chandrasegaran2022revisiting} further demonstrated that a low-temperature, LS-trained teacher can improve distillation performance. Meanwhile, \citet{kornblith2021better} found that LS tightens class clusters in feature space, reducing transfer performance. From a Neural Collapse (NC) perspective~\citep{zhou2022all, guo2024cross}, LS drives the model toward rigid feature clusters, a phenomenon measured by \citet{xu2023quantifying} via a variability metric.

\paragraph{Comparison with existing LS techniques}  
Our primary objective is to mitigate the error-enhancement effect. Instead of refining a smoothed label, as in OLS or Zipf-LS, \textbf{MaxSup} directly penalizes the highest logit \(z_{\textit{max}}\). This simple yet effective modification ensures uniform regularization even when \(z_{gt}\) is not the top logit, thereby maintaining greater intra-class diversity and avoiding the performance degradation common to LS-based approaches (\cref{sec:maxsup}). Additionally, MaxSup integrates seamlessly with standard training pipelines, requiring no extra computational overhead beyond simply replacing LS.



