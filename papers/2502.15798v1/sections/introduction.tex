\section{Introduction}
\label{sec:intro}
Multi-class classification~\citep{russakovsky2015imagenet, lecun1998mnist} conventionally relies on one-hot labels, implicitly treating each class as if it were completely orthogonal to every other. In reality, however, classes often share low-level attributes~\citep{zeiler2014visualizing, silla2011survey} or exhibit high-level semantic similarities~\citep{chen2021hsva, yi2022exploring, novack2023chils}, which makes the strict orthogonality assumption overly simplistic. This mismatch can lead to \emph{over-confident} classifiers, ultimately reducing generalization~\citep{guo2020online}.

To address overconfidence, \citet{szegedy2016rethinking} introduced \textbf{Label Smoothing} (LS), blending a uniform distribution with the hard label to reduce the model’s certainty in the target class. LS has become a mainstay in both image recognition~\citep{he2016deep, touvron2021training, liu2021swin, zhou2022sp} and neural machine translation~\citep{gao2020towards, alves2023steering}, often improving accuracy and calibration~\citep{muller2019does}. However, studies have also revealed that LS can produce \emph{overly tight clusters} in the feature space~\citep{kornblith2021better, sariyildiz2022no, xu2023quantifying}, thereby lowering intra-class diversity and harming transferability~\citep{feng2021rethinking}. Meanwhile, \citet{zhu2022rethinking} reported that LS inadvertently increases confidence in \emph{incorrect} predictions, though the exact cause remained unclear.

In this paper, we show that LS’s training objective inherently contains an \emph{error-enhancement} term that amplifies misclassified predictions, thus causing overconfident errors and tighter feature clusters (\cref{sec:revisiting}, \cref{tab:pre}). Extending \citet{zhu2022rethinking}, we define “overconfidence” in terms of the network’s top-1 prediction rather than calibration-based criteria. Our analysis further demonstrates that penalizing the ground-truth logit in misclassifications compresses the feature space, reducing intra-class variation (\cref{tab:feature}), as corroborated by Grad-CAM visualizations (\cref{fig:gradcam}).

To overcome these limitations, we propose \textbf{Max Suppression (MaxSup)}, which retains LS’s desirable \emph{regularization} effect while eliminating its \emph{error-enhancement} component. Rather than penalizing the ground-truth logit, MaxSup penalizes the \emph{largest} logit, thus providing consistent regularization regardless of prediction correctness. By preventing ground-truth suppression during misclassification, MaxSup preserves richer intra-class variation and improves inter-class separability. As illustrated in \Cref{fig:maxsup}, this alleviates the compression and attentional shortcomings introduced by LS, leading to more robust feature representations. Extensive experiments on both image classification and semantic segmentation confirm that MaxSup not only alleviates intra-class collapse but also boosts final accuracy, enhances generalization, and strengthens transfer performance.

Our contributions are summarized as follows:
\vspace{-2mm}
\begin{itemize}
    \item We present a \textbf{logit-level analysis of Label Smoothing} that unearths an `error-enhancement'' term, revealing how LS inadvertently reinforces overconfidence in misclassified samples.
    \item We propose \textbf{Max Suppression (MaxSup)}, which preserves LS’s desired regularization while removing its detrimental error-enhancement component, thereby reducing intra-class compression and boosting both classification and downstream task performance.
\end{itemize}
