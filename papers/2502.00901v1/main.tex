\documentclass{article}
\flushbottom

\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs}
\usepackage{enumerate}
\usepackage{bbm}

\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{stmaryrd}
\usepackage{authblk}
\usepackage{natbib}
\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{bbold}
\usepackage{xfrac}
\usepackage{mathtools}
\usepackage{physics}
\usepackage{wrapfig}
\usepackage{nicefrac}

\usepackage{tikz}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations.markings}
\input{tikzlibrarybayesnet.code}
\tikzset{middlearrow/.style={decoration={markings,mark= at position 0.5 with {\arrow{#1}} ,},postaction={decorate}}}
\usetikzlibrary{arrows}
\newcommand{\midarrow}{\tikz \draw[-triangle 90] (0,0) -- +(.1,0);}


\usepackage{amsthm}
\newtheorem{axiom}{Axiom}
\newtheorem{claim}[axiom]{Claim}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}{Corollary}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{assumption}{Assumption}
\newtheorem{definition}{Definition}
\newtheorem{remark}{Remark}
\newtheorem{conjecture}{Conjecture}
\newtheorem{proposition}[theorem]{Proposition}


\usepackage[margin=1in]{geometry}

\setcitestyle{authoryear,round,citesep={;},aysep={,},yysep={;}}

\setlength{\affilsep}{0.5em}

\input{math_commands}

\begin{document}

\title{\Large Fundamental limits of learning in sequence multi-index models and deep attention networks: High-dimensional asymptotics and sharp thresholds }

\author[1]{Emanuele Troiani}
\author[2]{Hugo Cui}
\author[1,3]{Yatin Dandi}
\author[3]{\\Florent Krzakala}
\author[1]{Lenka Zdeborová}


\affil[1]{Statistical Physics Of Computation Laboratory, \'Ecole Polytechnique F\'ed\'erale de Lausanne (EPFL)}
\affil[2]{Center of Mathematical Sciences and Applications, Harvard University {\color{white} \'E}}
\affil[3]{Information Learning and Physics Laboratory, \'Ecole Polytechnique F\'ed\'erale de Lausanne (EPFL)}



\date{}

\renewcommand\Authfont{\fontsize{11}{14.4}\selectfont}
\renewcommand\Affilfont{\fontsize{10}{10.8}\itshape}

\maketitle

\begin{abstract}
In this manuscript, we study the  learning of deep attention neural networks, defined as the composition of multiple self-attention layers, with tied and low-rank weights. We first establish a mapping of such models to sequence multi-index models, a generalization of the widely studied multi-index model to sequential covariates, for which we establish a number of general results.  In the context of Bayesian-optimal learning, in the limit of large dimension $D$ and commensurably large number of samples $N$, we derive a sharp asymptotic characterization of the optimal performance as well as the performance of the best-known polynomial-time algorithm for this setting --namely approximate message-passing--, and characterize sharp thresholds on the minimal sample complexity required for better-than-random prediction performance. 
Our analysis uncovers, in particular, how the different layers are learned sequentially.  Finally, we discuss how this sequential learning can also be observed in a realistic setup.
\end{abstract}

\section{Introduction}
Recent years have witnessed a shift in paradigm in the automated learning from sequential data, such as  language \cite{brown2020languagemodelsfewshotlearners, devlin2019bertpretrainingdeepbidirectional}. The backbone of many of these technological advances arguably lies in the use of \textit{transformer architectures} \cite{vaswani2023attentionneed} --
a parametrization allowing the model to dynamically focus on relevant portions of the input, and extricate increasingly complex correlations between tokens through the successive application of \textit{attention layers}. They thus constitute a function class able to represent intricate dependencies between the tokens of sequential covariates. In spite of their ubiquity, the study of such models from a theoretical viewpoint is still in its infancy.\looseness=-1


This situation stands in contrast with the wealth of results on \emph{multi-index models}, central to many theoretical works e.g.~\citep{Arous2021, abbe22a,Veiga2022, ba2022high, arnaboldi23a, collinswoodfin2023hitting, Damian2023, bietti2023learning, moniri2023theory, berthier2024learning, dandi2023two,dandi2024benefits,simsek2024learning}. Multi-index models define functions based on low-dimensional subspaces of covariates and are popular exemplars among theoreticians.  However, shallow architectures like multi-index models are structurally simpler than attention models. Specifically, they (a) lack the hierarchical structure from multiple attention layers and (b) act on less structured, non-sequential covariates. These distinctions make it unclear whether results from multi-index models apply to multilayer attention architectures.
Here, we show these models share a deep formal connection, enabling a transfer of insights and analytical approaches.

Namely, our first motivation is {\it to extend the theoretical framework of multi-index models to sequence models}. We consider the class of {\it sequence multi-index} (SMI) functions introduced by \cite{cui2024phase,cui2024highdimensionallearningnarrowneural}, and defined over length $M$ sequences of $D$-dimensional tokens ${\x}\in\R^{D\times M}$ of the form
\begin{equation}
y^{\rm SMI}_W 
({\x})=g\left(\frac{\W {\x}}{\sqrt{D}}\right).
\label{eq:SMI}
\end{equation}
where $\W \in {\mathbb R}^{P \times D}$ is a learnable projection matrix, and $g:\R^{P\times M}\to\R^K$ is a (possibly multi-dimensional) link function. As we shall see, SMI functions prove very versatile generalizations of multi-index models ($M=1$) to sequence data of length $M> 1$, that can be studied using related ideas and theoretical tools.\looseness=-1 

 Our second motivation is to deploy the formalism developed for SMI models to study complex neural architectures. 
 Remarkably, the SMI class encompasses a range of models -- with a particularly noteworthy example being \textit{deep attention} architectures. A deep attention model of depth $L$, $y_{\rm DA} : \R^{D\times M}\to  \R^{M\times M}$,  is defined recursively as 
\begin{align}
\label{eq:deep_attention_function}
    & y_{\rm DA}(\x)= \sigma\left(\frac{\x_{L-1}^\top \w_L^{\top} \w_L \x_{L-1} }{D}\right),\end{align}
where $\forall l\in \llbracket 1,L\!-\!1\rrbracket$,
    \begin{align} \x_l= \x_{l-1} \!\left[\!c\mathbb{1}_M\!+\! \sigma\!\!\left(\!\frac{\x_{l-1}^\top \w_l^{\top} \w_l \x_{l-1} }{D}\!\right)\!\right], \label{eq:rec}
\end{align}
with $\x_0\equiv \x \in \R^{D\times M}$ and $\sigma : \R^{M\times M}\to\R^{M\times M} $ a non-linear map, taken in this work to be the softmax function. The deep attention function \eqref{eq:deep_attention_function} corresponds to a composition of $L$ single-head self-attention layers (with tied key and query), specified by the strength $c\in \R$ of its skip connections, and the set of weights $\w_l\in \R^{ P_l \times D}$, for $l=1,\dots,L$. \looseness=-1

Before stating our main contributions, we elucidate the connection between the deep attention model \eqref{eq:deep_attention_function} and the SMI model \eqref{eq:SMI}. While we focus on the single-head tied-weights architecture \eqref{eq:deep_attention_function} for clarity, such a mapping can also be constructed for deep architectures with multiple heads, untied weights, and sequence-to-sequence models with outputs in $\mathbb{R}^{D \times M}$, as detailed in Appendix \ref{appendix:multi-head}. 
The connection between the SMIs and deep attention models constitutes a pathway towards many exciting research avenues in the study of neural architectures beyond the exemplar of multi-layer perceptrons --such as classical multi-index models--, while allowing the transfer of ideas from the study of the latter.\looseness=-1 

\paragraph{Mapping deep attention to the SMI model--}
Starting from a deep attention model \eqref{eq:deep_attention_function} with weights $\{\w_l^\star\}_{l=1}^L$, let us introduce the projections (a.k.a the eponymous indices) $\{Z_l\in \mathbb{R}^{P_l\times M}\}_{l=1}^L $ of the input data $\x\in\R^{D\times M}$ on each weight matrix $\w_l^\star \in\R^{P_l\times D}$:  $Z_l = {\w_l^\star  \x}/{\sqrt{D}}$.
Layer by layer, the post-activation sequences $\x_l$ build on the initial sequence $\x$ by adding increasingly sophisticated correlations between tokens. A key observation, however, is that successive post-activations $\x_l$ retain a simple formal expression, and depend non-linearly on the input $\x$ only through the indices $Z_1, ..., Z_l$. More precisely, at every layer $l$, the post activation $\x_l$ can be written as
\begin{equation}
\label{eq:post_act}
    \x_l = \x B^l_c(Z_1,...,Z_l),
\end{equation}
where we introduced a function $B^l_c:\R^{P_1\times M}\times \dots \times \R^{P_{l}\times M}\to\R^{M\times M}$, subsuming the complex inter-token interactions resulting from the propagation of the input through previous layers. The sequence of mixing functions $\{B^l_c\}_{l=1}^L$ are defined recursively as:
\begin{align}
\label{eq:def_B}
    B^l_c(Z_1,...,Z_l)=
    B^{l-1}_c(Z_1,...,Z_{l-1})^{(1-\delta_{l,L})}\Big[(1-\delta_{l,L})c\mathbb{1}_M + \sigma \left(
    B^{l-1}_c(Z_1,...,Z_{l-1})^\top Z_l^\top Z_lB^{l-1}_c(Z_1,...,Z_{l-1})
    \right)\Big],
\end{align}
with $B^0_c=\mathbb{1}_M$.
The claim \eqref{eq:post_act} then follows from the definitions \eqref{eq:def_B} 
and \eqref{eq:deep_attention_function} by recursion, finally yielding
\begin{align}
\label{eq:equiv_sq_multiindex}
    y_{\rm DA}(\x)=B^L_c(Z_1,...,Z_L).
\end{align}
Eq.~\eqref{eq:equiv_sq_multiindex} can be written more compactly and evocatively as
\begin{align}
    \label{eq:equiv2}
    y_{\rm DA}(x)=g\left(\frac{\W^\star\x}{\sqrt{D}}\right), 
\end{align}
introducing the total weights $\W^\star \in \R^{P\times D}$, defined as the vertical concatenation of the weight matrices $\{\w_l^\star\}_{l=1}^L$ along their first dimension, and denoting $P=P_1+...+P_L$. The link function $g$ in \eqref{eq:equiv2} follows from a redefinition of $B^L_c$, as can be read from \eqref{eq:equiv_sq_multiindex}, and takes values in $\R^{M\times M} \approxeq \R^K$ for $K=M^2$.
One then observes that the equivalent model \eqref{eq:equiv2} is an SMI model \eqref{eq:SMI}, whose width $P$ is related to the depth of the original deep attention model \eqref{eq:deep_attention_function}. The recursive structure of the deep network \eqref{eq:deep_attention_function} is encoded in the way the highly structured function $g$ acts on the different rows of its argument $\W^\star \x\in \R^{P\times M}$. Importantly, the $P$ rows generically do not play symmetric roles, reflecting the structure of the original deep attention function. 


\paragraph{Main contributions --}
Building on the above mapping, we study the statistical and computational limits of learnability from data generated by the models \eqref{eq:SMI} and \eqref{eq:deep_attention_function}. Namely, we consider training data composed of $N$ samples of Gaussian i.i.d. input sequences, and output labels generated by the SMI model \eqref{eq:SMI} with random i.i.d. Gaussian weights (the same realization of weights for every sample). In the limit of large dimension $D$ and number of samples $N$ with $N/D, P, M, K=\Theta(1)$, we  
characterize the minimal prediction error achievable information-theoretically and also by a class of algorithms conjectured optimal among all polynomial algorithms. This is achieved by the following technical contributions: 


\begin{itemize}[leftmargin=*,wide=1pt]
    \item We show how to rigorously generalize results previously established for multi-index models to the SMI models -- including the notions of phase transition for weak recovery, and optimal message-passing algorithms \cite{damian2024computationalstatistical,troiani2024fundamental}.    
    \item We derive sharp asymptotic expressions for optimal estimation errors achievable information theoretically and with the Approximate Message-Passing (AMP) algorithm, in the limit of large covariate dimension $D$, and proportionally large number of samples $N$, but finite sequence length $M$, weight rank $P$ and depth $L$. This characterization builds on the analyses of \cite{Donoho2009, javanmard2013state,Gerbelot} for AMP, which is optimal among first-order methods \cite{celentano20a}. 
    \item We extend the numerical evaluation and analysis of the resulting asymptotic equations from $M=1$ in \cite{Aubin2018,troiani2024fundamental}, and $P=1$ in \cite{cui2024phase,cui2024highdimensionallearningnarrowneural}  to SMI models with both sequence length $M>1$, and number of indices $P>1$.
\end{itemize}


Equipped with the above theoretical results, we extract the \textit{weak recovery thresholds}, namely the sample complexity required for a better-than-random estimation, for the weights of every layer in the case of a two-layer attention model. We show that different layers are learnable at different sample complexities, implying sequential learning of the different weights. We then briefly discuss how such layer-wise learning also arises in more realistic settings.

\subsection*{Further related works}
\paragraph{Analyses of single attention layers -- } Many theoretical studies of attention-based models \cite{vaswani2023attentionneed} consider the case of a \textit{single} attention layer, studying it in isolation \cite{, lopardo2024attention, zhang2023trained, li2023transformers, tian2023scan, jelassi2022vision}. This rich line of works shed light on various aspects of these models, such as their inductive bias \cite{sahiner2022unraveling, ataee2023max, tarzanagh2023transformers}, training dynamics \cite{li2023theoretical}, or expressivity \cite{fu2024can}. \looseness=-1
A fraction of this body of works was devoted to characterizing attention models in the limit of large dimensions. \cite{lu2024asymptotic} and \cite{rende2024mapping} provide tight asymptotic characterization of the error achieved by linear attention layers, respectively for in-context learning and next-token prediction. 
\cite{cui2024phase} uncover a phase transition between semantic and positional learning in a model of attention mechanism with low-rank weights. For a related model,
\cite{marion2024attention} study its population gradient dynamics, and prove its asymptotic Bayes-optimality for a regression task.  Because these works focus on single attention layers however, they cannot capture the compound effect of multiple successive attention layers in building increasingly complex correlations between tokens present in multilayer models such as \eqref{eq:deep_attention_function}.

\paragraph{Analyses of deep attention models -- } In an effort to supersede those limitations, a recent stream of works has striven to tackle the problem of multiple attention layers, under varying simplifications. \cite{geshkovski2024emergence, geshkovski2023mathematical} study the propagation of a signal through multiple attention layers with frozen weights. \cite{ahn2023transformers, von2023transformers} describe how the successive layers can implement gradient descent steps, establishing how an optimization algorithm can be encoded by such an architecture. Deep attention models have also been studied through the lens of their capacity \cite{edelman2022inductive} and expressivity \cite{hahn2020theoretical}, and scaling limits \cite{bordelon2024infinite}. The training dynamics of deep attention models were ascertained close to initialization by \cite{bietti2023learning}, and fully by \cite{abbe2024transformers}, in exchange of the assumption of diagonal weights. Closer to our setting, \cite{tiberi2024dissecting} study the Bayesian learning of a multilayer attention, assuming trainable value matrices but frozen key and query matrices. In this respect, the model is complementary to the one  considered in the present work, where in contrast the learning of key and query weights is addressed, while value matrices are fixed to identity. Another key difference lies in the fact that the attention matrix at every layer is computed using the bare output $\x$ in \cite{tiberi2024dissecting}, as opposed to the output of the previous layer $\x_{l-1}$ as in \eqref{eq:deep_attention_function}.

\paragraph{Learning multi-index models --}
Multi-index models correspond to target function of the form $y(\x)=g(\W\x)$, specified by a link function
$g$ and a finite rank matrix $\W\in\R^{P \times D}$, with $P =\Theta(1)$, and can be viewed as two-layer MLPs with weights $\W$ and activation $g$.
They provide a natural exemplar for functions that operate on high-dimensional covariates, but only depend on a finite number of  directions. Several authors have used multi-index models as testbeds to explore the ability of neural networks to learn low-dimensional subspaces in high dimensions \cite{ba2022high,  moniri2023theory, dandi2023two, dandi2024random, cui2024asymptotics}. A substantial stream of works has furthermore been devoted to studying the behavior of (stochastic) gradient descent on such non-convex objectives \cite{saad1995line, saad_1996, Arous2021, bietti2023learning, simsek2024learning, barak2022hidden, berthier2024learning, abbe23a, damian2022neural, glasgow2023sgd, arnaboldi2024escaping}, with the work of \cite{oko2024learning} providing insights for more generic scalings of the number of hidden units $P$ with the dimension $D$. In closer relation to the present work, \cite{troiani2024fundamental} ascertain the fundamental computational limits of learning multi-index functions, leveraging an analysis of AMP algorithms. In this manuscript, we establish a deep connection between deep attention models \eqref{eq:deep_attention_function} and SMI models \eqref{eq:SMI}, a generalization of multi-index models introduced by \cite{cui2024phase,cui2024highdimensionallearningnarrowneural}, allowing to borrow from the aforementioned wealth of insights and results. The connection to deep attention models was not noticed in \cite{cui2024phase,cui2024highdimensionallearningnarrowneural}, and \cite{cui2024highdimensionallearningnarrowneural} studied the empirical risk minimization for the SMI models using the replica method, and has not established the fundamental statistical and computational limits for the SMI function class. 

\paragraph{Statistical and computational limits of learning --} The optimal prediction errors associated with single-layer neural network functions were first derived in a line of foundational works \cite{gardner1988optimal, sompolinsky1990learning,gyorgyi1990first} for perceptrons, and rigorously in \cite{barbier2019optimal}. The statistical limits of learning in committee machines were explored in \cite{schwarze1992generalization, schwarze1993learning, monasson1995weight}, and its computational limits in \cite{Aubin2018, troiani2024fundamental}. The case of large-width networks was covered in \cite{zavatone2022contrasting} for linear models, and \cite{cui2023bayes, camilli2023fundamental, maillard2024bayes} for non-linear models. Closer to our work, \cite{erba2024bilinear} consider a related model of bilinear sequence regression, characterizing its optimal errors, and proposing an associated AMP algorithm matching these performances.
\cite{abbe2024far} study the weak learnability of a variety of tasks for transformers trained with descent algorithms. 

\section{Fundamentals limits of learning SMI models}
\label{sec:SMI}
The question of determining the statistical and computational limits of learning has traditionally been thoroughly explored for multi-index models \cite{gardner1988optimal, schwarze1992generalization,barbier2019optimal, Aubin2018, troiani2024fundamental}. 
In this first section, we show how these ideas can be extended to SMI models \eqref{eq:SMI}.\looseness=-1 

\subsection{Bayes-optimal learning of SMI models}
\label{subsec:Bayes_SMI}
We are interested in the statistical inference problem of estimating the weights $\W^\star$ of an SMI model $
y^{\rm SMI}_{\W^\star}=g\left(\sfrac{\W^\star {\x}}{\sqrt{D}}\right) $
from $N$ observations $\mathcal{D} \equiv \{(\x^\mu, y^{\rm SMI}_{\W^\star}(\x^\mu))\}_{\mu=1}^N$ of the function on identically and idependently sampled (i.i.d) covariates $\x^\mu\in\R^{D\times M}$ with i.i.d standard Gaussian components. We further consider a random instance of the SMI model, where the weights $\W^\star$ have been randomly sampled from a prior distribution, which we assume for definiteness to be i.i.d Gaussian over all the components. For this problem, we ask the question of the \textit{optimal statistical and computational} prediction error that can be deduced from the estimation of the weights $\W^\star$ from the observations $\mathcal{D}$: what is the information-theoretically lowest reconstruction error? What is the lowest error achievable by polynomial-time algorithms?\looseness=-1

We answer these questions in the \textit{Bayes-optimal setting}, in which the statistician has full knowledge of the parameters of the model except the realization of its weights $\W^*$. The estimator $y(\cdot)$ minimizing the prediction error $\mathbb{E}_{\x}\lVert y(\x)-y^{\rm SMI}_{\W^\star}(\x)\lVert^2$ is then given by the mean $y(\x)=\mathbb{E}_{\W}[y^{\rm SMI}_{\W}(\x)|\mathcal{D}]$ over the Bayesian posterior distribution
\begin{align}
\label{eq: posterior}
    \mathbb{P}(\W|\mathcal{D})&\!\propto\! e^{-\frac{1}{2}\!\Tr[\W\W^\top\!]}\!   \prod\limits_{\mu=1}^N \! \delta\left(y^{\rm SMI}_{\W^\star}(\x^\mu)\! - \! y^{\rm SMI}_{\W}(\x^\mu)\right).
\end{align}


\paragraph{Asymptotic limit -- } In the limit of large covariate dimension $D$ and large number of samples $N$, sampling the posterior distribution 
\eqref{eq: posterior} is generically computationally challenging. It is on the other hand possible, in this limit, to access exact theoretical characterizations of key statistics of the optimal estimators. We detail this analysis in the following sections. More precisely, we consider the asymptotic limit of $D,N\to\infty$ while $\alpha\equiv \sfrac{N}{D}=\Theta(1)$. We further assume that the sequence length $M$, model width $P$ and output dimension $K$ remain finite : $M, P, K=\Theta(1)$.\looseness=-1

\paragraph{Methodology -- }
The following technical results are enabled by the key observation that the SMI model \eqref{eq:SMI} can further be viewed as an ordinary multi-index model, provided the data and weights are properly reshaped. More precisely, to go from an SMI model to an equivalent multi-index model, one can flatten the input $\x\in\R^{D\times M}$, viewed as a length  $M$ sequence of $D$-dimensional tokens, into a vector in $\R^{MD}$. In parallel, the matrix of weights $\W^\star\in \mathbb R^{P\times D}$ should be mapped into a block-diagonal matrix of size $PM \times DM$ with each of the $M$ blocks on the diagonal being equal to $\W^\star$, as detailed in Appendix \ref{appendix:block_AMP}. In words, an SMI model can be viewed as a multi-index model acting on flattened sequences, 
with a non-separable prior on the parameters.\looseness=-1

This mapping allows us to borrow from previous results on multi-index models. On the one hand, the computational limits can be determined along the lines of \cite{troiani2024fundamental}, leveraging an analysis of the generalized-AMP (GAMP) algorithm, which is provably optimal within the class of first-order methods \cite{celentano20a}. This analysis can be carried out using methods developed in \cite{Gerbelot} on the state evolution of GAMP.
The statistical limits of learning may be, on the other hand, obtained by leveraging the rigorous results on the free-energy of the posterior measure \eqref{eq: posterior} for multi-index models established in \cite{Aubin2018}, as sketched in Appendix \ref{app:free-en}, or using the non-rigorous replica method from statistical physics, as detailed in Appendix \ref{app:replica}. 

\subsection{Statistical limits: Bayes-optimal error}
To state our main results, we start by defining a finite-dimensional \textit{output channel }induced by Eq.~\eqref{eq:SMI} that will be referred to throughout the subsequent sections:
\begin{equation} \label{eq:effective}
        Y(\omega, V) = g\left(\omega  + \sqrt{V} Z\right),
\end{equation}
where $Z \in \mathbb{R}^{P \times M}$ and $Z_{ij} \stackrel{\text{i.i.d.}}{\sim}   \mathcal{N}(0,1)$. On an intuitive level, $\omega \in \mathbb{R}^{P\times M}$ and $V \in  \mathbb{S}^+_P$ (a positive semidefinite matrix od size $P$) correspond to estimates of the mean and covariance of the inputs to the nonlinearity $g(\cdot)$ in Eq.~\eqref{eq:SMI}. 

With the above definition and assumptions on $g$ specified in Appendix \ref{app:free-en}, we are in a position to state our first result, characterizing the asymptotic Bayes-optimal error for a general SMI model of the form \eqref{eq:SMI}:

\begin{theorem}\label{thm:free-energy}
Consider the SMI model (\ref{eq:SMI}) with non-linearity $g$. 
Let $\xi \in \mathbb{R}^{P \times M}$ denote a matrix with entries $\xi_{ij} \stackrel{\text{i.i.d.}}{\sim}   \mathcal{N}(0,1)$. Let $H_{Y}(Q)$, denote the conditional entropy of the associated output channel $Y$ defined by Eq.~\eqref{eq:effective} with $\omega=\sqrt{Q}\xi$ and $V=\sqrt{Q}$ for the so-called overlap $Q \in \mathbb{S}^+_P$. Suppose that the following $\sup\inf$ problem:
\begin{equation}
\label{eq:freeenergy}
\begin{split}
    \underset{\hat{Q}\in\mathbb{S}^{+}_{P}}{\sup}\;\underset{Q\in\mathbb{S}^{+}_{P}}{\inf}
    \biggl\{&
    -\tfrac{1}{2}\mathrm{Tr}\bigl(Q\,\hat{Q}\bigr) 
    -\tfrac{1}{2}\log\bigl(\mathbb{1}_{P}+\hat{Q}\bigr)
   + \tfrac{1}{2}\,\hat{Q}
    + \alpha H_{Y}(Q)
   \biggr\}.
\end{split}
\end{equation}
admits a unique global extremizer $\hat{Q}^\star, Q^\star$. Then the asymptotic Bayes-optimal prediction error is given by:
\begin{align}\label{eq:generalisation_error}    &\mathbb{E}_{\x}\left\|y^{\rm SMI}_{\W^\star}(\x)- \mathbb{E}_{\W}\left[{y^{\rm SMI}_{\W}(\x)|\mathcal{D}}\right]\right\|^2\xrightarrow{D \rightarrow \infty}   \mathbb{E}\!\left[\|g(\xi)^2\|^2\!-\!\left\langle g(\xi), g\!\left(\!\sqrt{\mathbb{1}_{P} - Q^{\star}}Z \!+\! \sqrt{Q^{\star}} \xi\right)\right\rangle\right]\!
\end{align}
and the Bayes-optimal estimation error is
$    
\mathbb{E}_{\x}\|\W^*(\W^*)^\top - \mathbb{E}_{\W}\left[{\W\W^\top|\mathcal{D}}\right]\|^2\rightarrow 1 - \|Q^*\|^2_2\,.
$
\end{theorem}


The proof of Theorem \ref{thm:free-energy} is outlined in Appendix \ref{appendix:block_AMP}. A complementary non-rigorous derivation using the replica method is provided in Appendix \ref{app:replica}.

\subsection{Computational Limits: Weak-recovery thresholds} \label{section:weak_recovery}

We next move to presenting a sharp asymptotic characterization of the computational limits of the Bayesian estimation problem \eqref{eq: posterior}. Our approach relies on the study of a GAMP algorithm \cite{Donoho2009}, reported in Algorithm \ref{alg:AMP}, building on two of its remarkable properties. First, the GAMP algorithm associated to a Bayes-optimal estimation problem is provably optimal among first-order methods \cite{celentano20a}. Consequently, the sample complexity required for the learnability for GAMP implies a computational lower bound on the class of all first-order algorithms, which includes in particular widely used gradient descent methods. Secondly, the performance of GAMP algorithms admits a sharp asymptotic description in terms of finite-dimensional variables. This description, known as the \textit{state evolution} equations \cite{Bayati2011, javanmard2013state}, allow the analysis of GAMP in the high-dimensional limit $D\to\infty$.  The state evolution description is formalized through the following Lemma:


\begin{algorithm}[t] 
    \caption{GAMP for SMI model 
    }
    \label{alg:AMP}\begin{algorithmic}
    \STATE \textbf{Inputs} : $\mathcal{D}=\{\x^\mu, y^\mu\}_{\mu=1}^N $
    \STATE \textbf{Initialize} $\hat{\W}^0_{i}=\mathcal{N}(0,\mathbb{1}_P),\, \hat{C}^0=\mathbb{1}_{P},\,g^0_\mu =0_{P\times M}$
    \FOR{$t\le t_{\max}$}
    \STATE $V^t=\hat{C}^t \in \mathbb{R}^{P\times P}$
    \STATE $\omega_{m,\mu}^t=\frac{1}{\sqrt{D}}\sum\limits_{i=1}^D \x^\mu_{m, i} \hat{\W}_{i}^t-V^t g^t_{m\mu} \in \mathbb{R}^{P\times M}$
    \STATE $ 
    g^t_{m,\mu}=\left[g_{\rm out}(y^\mu, \omega_\mu^t, V^t)\right]_m  \in \mathbb{R}^{P}$
    \STATE $ A^t=-\frac{\alpha}{N}\sum\limits_{\mu, m=1}^{N,M}\left[\partial_{\omega}g_{\rm out}(y^\mu, \omega_\mu^t, V^t)\right]_{mm} \in \mathbb{R}^{P\times P}$
     
    \STATE $b^t_{i}=\frac{1}{\sqrt{D}}
    \sum\limits_{\mu,m=1}^{N,M}\x^\mu_{m i} g_{m,\mu}^t+A^t\hat{\W}^t_i \in \mathbb{R}^P$
    \STATE $\hat{\W}^{t+1}_{i}=(\mathbb{1}_P+A^t)^{-1}b_{i}^t$
    \STATE $\hat{C}^{t+1}=(\mathbb{1}_P+A^t)^{-1}$
    \ENDFOR
    \STATE
    \end{algorithmic}
\end{algorithm}



\begin{lemma}[State evolution \citep{Gerbelot}] 
\label{lem:SE}
Define ${g}_{\rm out}:\mathbb{R}^{M\times M} \rightarrow \mathbb{R}^{P\times M}$ to be the following ``denoiser": 
    \begin{equation} \label{eq:g_out}
    \begin{split}
        {g}_{\rm out}(Y, \omega&, V) \coloneqq \mathbb{E} \left[Z \,| \,Y\right]\,,
    \end{split}
    \end{equation}
    where the conditional expectation is w.r.t the output channel in Eq.~\eqref{eq:effective}. Let $\hat{\bW}^{t}$ denote the iterates of Algorithm \ref{alg:AMP} at time $t \in \mathbb{N} $. Assuming ${g}_{\rm out} \in \mathcal{C}^2$, under the high-dimensional limit $N,D\to\infty$ with fixed ratio $\alpha\!=\!\sfrac{N}{D}$, constant $M$ and any finite time $t$, the limiting overlaps converge in probability to the overlap $Q^t$:
\begin{equation} \label{eq:overlap}
     \sfrac{1}{d}\hat{\bW}^{t}{\hat{\bW}^{t\top}} \xrightarrow{P} Q^{t}, \, \sfrac{1}{d}\hat{\bW}^{t}{\bW^{\star\top}} \xrightarrow{P} Q^{t}, 
\end{equation}
with $Q^{t}$ satisfying the \emph{state evolution equations} 
 \begin{equation} \label{eq:SE}
        Q^{t} = F\left(\alpha\, \mathbb{E}_{Y, \xi}\left[ {g}_{\rm out}(Y,\sqrt{Q^{t-1}}\xi,\mathbb{1}_{P} - Q^{t-1})^{\otimes 2} \right]  \right),
    \end{equation}
from an initial condition $Q^{0}$. In \eqref{eq:SE}, we denoted $F(\hat{Q}) = (\mathbb{1}_{P} + \hat{Q})^{-1} \hat{Q}$, and employed the shorthand $Y=Y(\sqrt{Q}\xi, \sqrt{\mathbb{1}_P-Q})$,
with $\xi \in \mathbb{R}^{P \times M}$ denoting a matrix with entries $\xi_{ij} \stackrel{\text{i.i.d.}}{\sim}   \mathcal{N}(0,1)$.
\end{lemma}

We show in Appendix \ref{appendix:derivative_freeenergy} that the fixed points of the state evolution equations \eqref{eq:SE} are the stationary points of the free energy \eqref{eq:freeenergy}. Thus, in cases where there exists a unique local extremizer of \eqref{eq:freeenergy}, GAMP achieves the Bayes-optimal performance.
The proof of Lemma \ref{lem:SE}, which builds upon \cite{berthier2020state}, is outlined in Appendix \ref{appendix:block_AMP}. We also provide a complementary derivation of this result from the Belief Propagation scheme \cite{pearl2014probabilistic} associated with the inference problem \eqref{eq: posterior} in Appendix \ref{appendix:AMP}.
Lemma~\ref{lem:SE}  provides an asymptotic description of the performance of GAMP (Algorithm ~\ref{alg:AMP}), which can be leveraged to ascertain when reconstruction is computationally achievable, what sample complexity is required to do so, and whether the algorithm achieves the Bayes-optimal performance. 
\looseness=-1



Hereafter, let us consider more particularly the case of even SMI models, namely those invariant under the reflection $\W \leftrightarrow -\W$ of their weights. This is the case, for instance, of the tied deep attention model \eqref{eq:deep_attention_function}. As a consequence of this symmetry, $Q=0$ is a trivial fixed point of the state-evolution of GAMP, and the latter fails to reconstruct any part of the row-space of $\W^\star$. 
As is well known for single and multi-index models \cite{mondelli2018fundamental,troiani2024fundamental}, the breaking of such a symmetry necessitates the introduction of \textit{side-information}, defined as an additional observation $\lambda \W^\star + \sqrt{1-\lambda} \boldsymbol{\xi}$, with $\boldsymbol{\xi}\in\R^{P\times D}$ a matrix with Gaussian i.i.d entries, and $\lambda\in\R$. The GAMP algorithm in the presence of side information differs from Algorithm \ref{alg:AMP} only in the updates of $\hat{\W},\hat{C}$, as we discuss in Appendix \ref{appendix:AMP}. 
The introduction of side-information allows us to define the\textit{ weak-recovery threshold} for a subspace $U^\star \subseteq \operatorname{span}(W^\star)$ as the smallest value of $\alpha$ such that the iterates of the GAMP algorithm achieve non-vanishing overlap along all directions in $U$ with arbitrarily small side-information coefficient~$\lambda$. We discuss the formal definitions and existence of such thresholds in Appendix \ref{app:weak-rec}, based on the analysis in \cite{troiani2024fundamental}. These thresholds can further be characterized in closed-form in terms of the stability of the fixed-points of the state-evolution \eqref{eq:SE} of Lemma \ref{lem:SE}. Indeed, the iterates $\hat{W}^t$ can escape the neighborhood of the fixed point $Q=0$ in the presence of vanishing side-information $\lambda$ if and only if the fixed-point $Q=0$ becomes unstable. The sample complexity $\alpha$ corresponding to the onset of this instability is precisely the threshold above which a non-zero subspace of $\operatorname{span}(W^\star)$ can be recovered. Similarly, upon the recovery of certain subspaces in $\operatorname{span}(W^\star)$, the state-evolution dynamics can then reach other fixed points, which can in turn become unstable beyond certain sample complexity thresholds, enabling the learning of further subspaces. This sequential learning is referred to as the \textit{grand-staircase} mechanism in \cite{troiani2024fundamental}. This discussion, which we prolong in Appendix \ref{app:weak-rec}, is formalized in the following theorem.



\begin{theorem}\label{th:weak_recovery}
   Suppose that $g_{\rm out} \in \mathcal{C}^2$ with $G \coloneqq \partial_\omega {g}_{\rm out}(Y, 0, \mathbb{1}_P)\in\mathbb{R}^{P\times P\times M\times M}$ denoting the tensor-Jacobian of $g_{\rm out}$ with respect to $\omega$.
  Define $\mathcal{F}(\mathcal{X})$ to be the following linear operator on $\mathbb{R}^{P\times P}$ 
    \begin{equation}\label{eq:F_def}
    \begin{split}
        &\mathcal{F}(\mathcal{X})_{ij} = \sum_{a,b=1}^M\sum_{kl=1}^{P}\mathbb{E}_Y \left[G_{ikab} \mathcal{X}_{kl} G_{ljab}\right],       
    \end{split}
    \end{equation}
where $Y$ is distributed as in \eqref{eq:effective}. 
    Then the weak recovery threshold for initial recovery -- namely for reconstructing any subspace -- is
    \begin{equation}
        \frac{1}{\alpha} = \sup_{\mathcal{X}\in\mathbb{S}_+^{P_l},\, \|\mathcal{X}\|_F=1} \|\mathcal{F}(\mathcal{X})\|
    \end{equation}
    where $\|\mathcal{X}\|_F$ is the Frobenius norm and $\mathcal{S}_+^{P}$ is the set of positive semi-definite matrices of size $P$.

Furthermore, the subspace weak-recovery threshold for recovering a subspace $U \subseteq \mathbb{R}^P$ is given by:
\begin{equation} \label{eq:limt_si}
    \alpha_1(U) = \inf\{\alpha : \lim_{\lambda \rightarrow 0^+} \lim_{t \rightarrow \infty}P_U Q^t (P_U)^\top \succ 0\},
\end{equation}
where $P_U$ denotes the projection onto $U$ and $Q^t \in \mathbb{R}^{P \times P}$ are recursively defined by:
\begin{equation}\label{eq:se_dynamics}
        Q^{t+1} = F_\lambda\left(\alpha\, \mathbb{E}_{Y, \xi}\left[ {g}_{\rm out}(Y,\sqrt{Q^t}\xi,\mathbb{1}_{P} - Q^t)^{\otimes 2} \right]  \right),
    \end{equation}   
    with $F_\lambda(\hat{Q}) = (\hat{Q}(1-\lambda)+\lambda\mathbb{1}_P)(\mathbb{1}_P+\hat{Q}(1-\lambda))^{-1}$.
\end{theorem}
The existence of the limit defined by Eq.~\eqref{eq:limt_si} is a consequence of the monotonicity of the asymptotic dynamics of GAMP described by Eq.~\eqref{eq:se_dynamics}, see Appendix \ref{appendix:block_AMP}.





\begin{figure*}[t]
\vskip 0.2in
\begin{center}
\includegraphics[scale=1.]{img/Figure_1_twin.pdf}
\caption{
(\textbf{Left}) Diagonal elements of the overlap $Q$ \eqref{eq:overlap} (red and blue) and prediction error (black) achieved by GAMP, as a function of the sample complexity $\alpha=\sfrac{N}{D}$, for a two-layer attention model $P_1=P_2=1, c=1, M=2$. Off-diagonal elements of $Q$ are zero, and thus not plotted. Crosses: numerical implementations of GAMP in dimension $D=1000$, averaged over $16$ runs. Continuous lines: theoretical prediction from Eqs.~\ref{eq:SE} and \eqref{eq:generalisation_error}. Dashed line: prediction error of GAMP when the first layer weights are fixed to zero. 
(\textbf{Right}) Weak recovery thresholds, delineating the different stages of the learning, as a function of the skip connection strength $c$. The red (resp. blue) lines indicate the sample complexities above which the second (resp. first) layers can be learned.  
}\label{fig:toy_model_alpha}\end{center}
\vskip -0.2in

\end{figure*}

\section{Results for deep attention models}


The previous section delineates the fundamental limits of learning in the class of SMI models \eqref{eq:SMI}. We now show how these results seamlessly transfer to deep attention models \eqref{eq:deep_attention_function}, and discuss in particular the consequences of Theorem \ref{th:weak_recovery} for these architectures.

\subsection{Layer-wise learning in depth \texorpdfstring{$L=2$}{L2} attention} \label{section:toy_model}

We examine for definiteness the case of a two-layer attention model \eqref{eq:deep_attention_function} ($L=2, M=2, P_1=P_2=1, c=1$).
Fig.~\ref{fig:toy_model_alpha} (left) illustrates the theoretical predictions of Lemma \ref{lem:SE} for the prediction error \eqref{eq:generalisation_error} and overlap $Q$ \eqref{eq:overlap} (right) achieved by GAMP, as a function of the sample complexity $\alpha$. Before discussing the curves, let us highlight two remarkable features of the behavior of GAMP in this model. First, we observe that the values of these metrics at convergence remain unchanged if GAMP is initialized with strong side information. This finding is a telltale sign of the absence of computational-to-statistical gaps for this particular problem, and strongly suggests that GAMP is in this case Bayes-optimal. Secondly, we notice that the off-diagonal elements $Q_{12}$, $Q_{21}$ of the overlap $Q$ \eqref{eq:overlap} vanish at convergence. 

A striking observation from Fig.\,\ref{fig:toy_model_alpha} is that the learning proceeds \textit{in sequential steps}, as the sample complexity $\alpha$ is increased. For $\alpha < \alpha_1 \approx 0.14$ (grey regime in Fig.\,\ref{fig:toy_model_alpha}), there is insufficient data to learn either set of weights --as signaled by vanishing overlaps $Q_{11}=Q_{22}=0$. As a consequence, the prediction error (black) stays sensibly constant, and does not decrease. In the regime $\alpha_1 < \alpha < \alpha_2 \approx 0.79$ (red regime in Fig.\,\ref{fig:toy_model_alpha}), the sample complexity is greater than the sharp threshold for the learnability of the second layer ($l=2$), and the corresponding overlap becomes non-zero $Q_{22}\neq 0$, reflecting a partial reconstruction of $\w_2$. This translates into a sizable decrease in the prediction error throughout this phase. Finally, for sample complexities $\alpha > \alpha_2$ (blue regime in Fig.\,\ref{fig:toy_model_alpha}), the first layer weights are also learned ($Q_{11}\neq 0$), resulting in yet another decrease in prediction error. The different layers are thus learned sequentially, with a clear separation in sample complexity between the consecutive stages. The weak recovery thresholds $\alpha_1$, $\alpha_2$ delineating the different regimes can be computed using the results described in Section \ref{sec:SMI}. Specifically, the initial recovery threshold $\alpha_1$ is the critical sample complexity above which a first subspace --in this case the span of the second layer weights-- is recovered, as characterized in Theorem~\ref{th:weak_recovery}. $\alpha_2$ is the critical sample complexity required to recover the subspace spanned by the first layer weights, \rm{conditioned on having acquired a non-zero overlap $Q_{22}>0$} with the second layer weights. We describe how to compute $\alpha_2$ in Appendix \ref{appendix:weak_recovery}. 
Finally, we chart the boundaries of the three regimes observed in Fig.\,\ref{fig:toy_model_alpha} (left) in a phase diagram (Fig.\, \ref{fig:toy_model_alpha} (right)), as a function of the skip connection $c$ and the sample complexity, evidencing how the phases subsist across a wide range of skip connection strengths $c$. 



\begin{figure}[t]
\vskip 0.2in
\begin{center}
\includegraphics[scale=1.]{img/Figure_2.pdf}
\caption{
(\textbf{Top}) Evolution of the performance of GAMP with the number of iteration, as measured by the cosine similarity $\mathcal{C}_l =|(\w_l^\star) \hat{\w}_l^\top| / \|\hat{\w}_l\|$ between the GAMP estimate $\hat{\w}_l$ of the $l-$th layer weights and the target weights $\w^\star$, for $L=2,P_1=P_2=1, c=1,M=2$. 
We display a single run of the algorithm in dimension $D=1000$ and sample complexity $\alpha = N/D = 1.2$. 
(\textbf{Bottom}) 
Evolution of the cosine similarity, for the same target function, when training the same model using SGD. We display $8$ runs of the algorithm in dimension $D=500$ and sample complexity $\alpha=15$, with the average indicated in bold. The numerical experiments were performed at $\lambda= 1.4\times 10^{-4},\eta=15$, and batch size $200$, with each batch used for $3$ consecutive iterations.\looseness=-1 
}\label{fig:toy_model_time}
\end{center}
\vskip -0.2in
\end{figure}



\subsection{The role of depth}
It is natural to wonder whether such layer-wise learning is a generic feature of deep attention networks or simply a phenomenon proper to the two-layer case. Fig.\, \ref{fig:real_model} (left) shows the diagonal elements of the overlap $Q$ for a depth $L=3$ attention model with $P_1=P_2=P_3=1, M=2, c=1$. As in the $L=2$ case (see Fig.\,\ref{fig:toy_model_alpha}), the last layer is learned first as the number of samples increases. On the other hand, the shallower layers $l=1,2$ seem to share the same weak recovery threshold, and are learned sensibly at the same rate. We conjecture this to be a general feature of deeper attention models.

Let us make an interesting observation in this direction, which we leave as inspiration for future work. The deep attention model \eqref{eq:deep_attention_function} admits an elegant limiting form in the joint limit of large depth $L\to\infty$ and large skip connection $c = \beta L$, with $\beta$ finite, provided the activation function is simultaneously rescaled as $\sigma(z)=\check{\sigma}(\sfrac{z}{c})$:
$$y_{\rm DA}(\x)= \check{\sigma} \left(
    \left(1+\frac{1}{\beta L}\sum\limits_{l=1}^{L-1}\sigma(Z_l^\top Z_l)\right)^\top Z_L^\top Z_L\left(1+\frac{1}{\beta L}\sum\limits_{l=1}^{L-1}\sigma(Z_l^\top Z_l)\right)
    \right) , 
$$
where we remind that $Z_l=\sfrac{\w_l\x}{\sqrt{D}}$ denotes the projection of the input $\x$ on the $l-$th layer weights $\w_l$. A detailed derivation of this limiting form is provided in Appendix \ref{appendix:deep_limit}.
This simple expansion shows that, in the large depth limit, the deep attention model can be viewed as a combination of a linear attention module $Z_L^\top Z_L$ parametrized by the last layer weights, and a "committee" of attention modules corresponding to the first $L-1$ layers. Since these first layers are interchangeable we expect that also the overlaps should share the same invariance, making it possible to simplify the state evolution equations in this limit, in a similar spirit to \cite{Aubin2018}. 



\begin{figure*}[t] 
\vskip 0.2in
\begin{center}
\includegraphics[scale=1.]{img/Figure_3_new.pdf}
\caption{ ({\bf Left}) Diagonal elements of the overlap $Q$ \eqref{eq:overlap} (red, blue and green) and prediction error (black) achieved by GAMP, as a function of the sample complexity $\alpha=\sfrac{N}{D}$, for a three-layer attention model $P_1=P_2=P_3=1, c=1, M=2$. Off-diagonal elements of $Q$ are zero, and thus not plotted. Dashed line: prediction error when the first and second layer weights are fixed to zero. ({\bf Right}) Similarity $\mathcal{S}_l = \sfrac{\tr((\w_l^{t\top}\w_l^{t })(\w_l^{* \top} \w_l*))}{\lVert \w_l^t\lVert^2\lVert \w_l^*\lVert^2}$ between the weights $\w_l^t$ at training step $t$ and the last-iterate weights $\w^*_l$ as a function of the training time. A transformer with $L=2$ attention layers and a fully connected readout is trained on the TREC classification task \cite{hovy-etal-2001-toward, li-roth-2002-learning}. Different colors indicate different attention layers, with the multiple curves representing distinct runs. 
}\label{fig:real_model}
\end{center}
\vskip -0.2in

\end{figure*}

\subsection{Dynamics of learning}
The precedent subsection evidenced how the different layers warrant different sample complexities to be reconstructed and are learned sequentially by GAMP as more samples become available. A similar sequential learning phenomenon can in fact also be observed at a fixed sample complexity, \textit{as a function of the number of iterations}. Fig.\,\ref{fig:toy_model_time} (left) represents the evolution of the cosine similarity $\mathcal{C}_l = \w_l^\star \hat{\w}_l^\top / \|\hat{\w}_l\|$ between the GAMP estimate $\hat{\w}_l$ of the layer $l$ weights and the corresponding target weights $\w_l^\star$, over the GAMP iterations. The second layer is learned within the first few iterations. In contrast, the first layer is at first not learned, and its reconstruction only commences after approximately $10$ steps, when the second layer is almost perfectly reconstructed.\looseness=-1

This layer-wise learning phenomenon over training time is not restricted to the GAMP algorithm. In Fig.\,\ref{fig:toy_model_time} (right), we plot the performance of Stochastic Gradient Descent (SGD) in reconstructing the weights of the same two-layer architecture, from the same dataset $\mathcal{D}=\{\x^\mu, y_{DA}^{\w_1^\star, \w_2^\star }(\x^\mu)\}_{\mu=1}^N$, by implementing the descent steps
$$
\hat{\w}^{t+1}_l=-\,\eta \sum\limits_{\x\in B_t} \nabla_{\hat{\w}_l} \left\lVert 
    y_{DA}^{\w_1^\star, \w_2^\star }(\x)-
    y_{DA}^{\hat{\w}_1^t, \hat{\w}_2^t }(\x)
    \right\lVert^2-\lambda \hat{\w}_l^t,
$$
with learning rate $\eta$ and weight decay $\lambda$. $B_t\subset \mathcal{D}$ denotes the batch used for the $t-$th step. Notice how we reuse the same batch for multiple gradient updates \cite{arnaboldi2024repetita}.
The simulations reveal that, similarly to GAMP, the learning of SGD proceeds in two sequential steps. While SGD starts learning the second layer weights after one epoch, the first layer is only reconstructed later, when the second layer is almost perfectly learned, with a clear separation in training time between the two stages.
We believe that describing analytically the gradient descent dynamics of SMI models, and in particular of deep attentions, can be an interesting research direction, which can be pursued leveraging the existing literature on the dynamics of multi-index models \cite{gerbelot_dmft, celentano2021highdimensionalasymptoticsordermethods, dandi2024benefits,mignacco2020dynamical}.


\section{Discussion and conclusions}


We have so far focused our investigations on the case of target functions of the form \eqref{eq:deep_attention_function}, evidencing how the weights of different layers of a two-layer attention architecture are learned sequentially over training time. 
In this last section, we briefly illustrate how similar layer-wise learning phenomena can also be observed for more practical transformer architectures, with real datasets.


We consider a classification problem on the Text REtrieval Conference (TREC) dataset \cite{hovy-etal-2001-toward, li-roth-2002-learning}, in which the network is tasked with labeling the object of a question. The data is pre-processed using the uncased base BERT model \cite{devlin2019bertpretrainingdeepbidirectional} to obtain token embeddings that are subsequently fed into a simple transformer architecture with two consecutive attention layers, and a fully-connected readout layer. The network is trained on the cross-entropy loss using the \texttt{Pytorch} \cite{paszke2017automatic} implementation of the AdamW \cite{loshchilov2017decoupled} optimizer, over $40$ epochs. Further details can be found in Appendix \ref{appendix:real_data}.
Fig.~\ref{fig:real_model} shows the evolution of the similarity metric $\mathcal{S}_l = \sfrac{\tr((\w_l^{t\top}\w_l^{t })(\w_l^{* \top} \w_l*))}{\lVert \w_l^t\lVert^2\lVert \w_l^*\lVert^2}$ between the weights $\w_l^t$ at training time $t$ and the last-iterate weights $\w^*_l$, for a given layer $l\in\{1,2\}$, as a function of $t$. The curves reveal a marked sequential learning process.  
They are furthermore quite reminiscent of our findings for two-layer attention target functions, gathered in Fig.\,\ref{fig:toy_model_time}, which we discussed above.
Remarkably, however, compared to Fig.\,\ref{fig:toy_model_time}, the order in which the layers are learned in the TREC task is reversed, echoing the findings of \cite{chen2023layer} that shallower layers tend to converge faster over training. Ascertaining precisely the factors determining the order in which layers are learned in a given setting is a challenging task, which warrants careful mechanistic studies. We leave this question as an exciting future research direction.\looseness=-1

To conclude, in the present manuscript, we consider the problem of learning a deep attention network, defined as the composition of multiple self-attention layers.
We first establish that such architectures pertain to the class of sequence multi-index models, which generalize multi-index models to sequential covariates.
For this larger class of models, in the Bayes-optimal setting, we derive the statistically and computationally optimal estimation errors, and extract from those the weak recovery thresholds. In the case of a depth $2$ attention model, our findings uncover how distinct layers are learned sequentially, with increasing number of samples, or over training time. We briefly discuss how a related layer-wise learning phenomenon can also be observed in practical settings.\looseness=-1

Among the limitations of our work, the SMI functions do not naturally account for fully connected layers usually interlaid with attention layers. Another limitation is that we consider only learning from Gaussian i.i.d. inputs; adding input correlation is possible but left for future work.

\section*{Acknowledgments}
We thank Freya Behrens, Vittorio Erba, Mikhail Terekhov, and Fabrizio Boncoraglio for discussions. We acknowledge
funding from the Swiss National Science Foundation grants SNFS SMArtNet (grant number 212049), and OperaGOST
(grant number 200021\_200390). HC acknowledges support from the Center of Mathematical Sciences and Applications
(CMSA) of Harvard University.

\bibliographystyle{plainnat}
\bibliography{example_paper}



\newpage
\appendix


\section{Generalization to untied and multi-head attention}\label{appendix:multi-head}


The backbone of the analysis of deep attention models \eqref{eq:deep_attention_function} reported in Section \ref{section:toy_model} of the main text is the mapping \eqref{eq:equiv2} to a SMI model \eqref{eq:SMI}. As we extensively discuss in the main text, this formal connection permits the transfer of analytical ideas from the study of multi-index models to that of deep attention architectures. Although we considered in the main text attention architectures with tied query and key weights for clarity and ease of exposition, the class of SMI functions \eqref{eq:SMI} in fact encompasses a much broader array of deep attention architectures, with untied weights, and generic number of heads. In this Appendix, we detail the mapping of these more complex architectures to SMI functions. A direct consequence of this mapping is that the general results on the statistical (Theorem \ref{thm:free-energy}) and computational (Lemma \ref{lem:SE} and Theorem \ref{th:weak_recovery}) limits of learning in SMI models directly apply to such architectures. We discuss sequentially the case of deep attention functions with untied weights, before extending the mapping to multiple heads. Finally, we derive the infinite depth limit $L\to\infty$ discussed in the main text. In the following we will use bold characters for all the quantities that with $\mathcal{O}(D)$ elements, as we did in the main text.

\subsection{Untied weights}
 As such, our main theoretical results consequently hold for untied architectures as well. In this paragraph, we detail the mapping of a deep attention network with generically untied weights to a SMI. Consider the model
\begin{align}
\label{eq:deep_attention_function_untied}
    & y_{\rm UDA}(\x)= \sigma\left(\frac{\x_{L-1}^\top \w_L^{Q\top} \w_L^K \x_{L-1} }{D}\right)\,, \qquad\qquad \forall l\in \llbracket 1,L\!-\!1\rrbracket,\,\, \x_l= \x_{l-1} \!\left[\!c\mathbb{1}_M\!+\! \sigma\!\!\left(\!\frac{\x_{l-1}^\top \w_l^{Q\top} \w_l^K \x_{l-1} }{D}\!\right)\!\right], 
\end{align}
where $\w^Q_l$, $\w_l^K\in \mathbb{R}^{P_l\times D}$ are two sets of independently sampled standard Gaussian weights. As we did in the main text, we can write the untied deep attention model $y_{\rm UDA}(\x)$ as an SMI model by introducing two families of indices $\mathcal{K}_l$, $\mathcal{Q}_l$ for the keys and the queries respectively
\begin{equation}
    \mathcal{K}_l = \frac{\w^K_l \x}{\sqrt{D}}\,,\qquad\qquad \mathcal{Q}_l = \frac{\w^Q_l \x}{\sqrt{D}}\,,
\end{equation}
with which we can define a new mixing functions $\{B_c^l(\mathcal{K}_1,...,\mathcal{K}_l,\mathcal{Q}_1,...,\mathcal{Q}_l)\}_{l=1}^L$, defined recursively as
\begin{equation}
\begin{split}
\label{eq:def_B_untied}
    B^l_c = (B^{l-1}_c)^{(1-\delta_{l,L})} \Big[(1-\delta_{l,L})c\mathbb{1}_M+ \sigma \left(
    B^{l-1\top}_c \mathcal{Q}_l^\top \mathcal{K}_lB^{l-1}_c
    \right)\Big], 
\end{split}
\end{equation}
with $B_c^0 = \mathbb{1}_M$, giving us
\begin{equation}
    y_{\rm UDA}(\x) = B_c^L(\mathcal{K}_1,...,\mathcal{K}_L,\mathcal{Q}_1,...,\mathcal{Q}_L)
\end{equation}
This corresponds to an SMI model \eqref{eq:SMI}. To see this, denote $P=P_1+...+P_L$ and introduce the concatenated weights $\W_{\rm UDA}\in\R^{2P\times D}$, defined as the matrix obtained by vertically stacking $\w^K_1, ..., \w^K_L, \w^Q_1,..., \w^Q_L$, in this order. Then define the link function $g_{\rm UDA}:\R^{2P\times M}\to \R^{M\times M}$ as
\begin{align}
    g_{\rm UDA}(X)=B^L_c(X_{[1:P_1]},X_{[P_1+1: P_1+P_2]}, ...,X_{[P+1: P+P_1]}, ...
    ),
\end{align}
where $X_{[i:j]}$ denotes the $(j-1+1)\times D$ submatrix of $X$ comprising the rows indexed between $i$ and $j$. Then, the deep attention architecture $y_{\rm UDA}$ can be rewritten compactly as
\begin{align}
    y_{\rm UDA}(\x)=g_{\rm UDA}(\W_{\rm UDA}\x).
\end{align}
In other words, the untied deep attention is also a SMI. Consequently, all the technical results reported in the main text directly apply to this architecture.

\subsection{Sequence to Sequence multi-layer attention models}

The SMI class of models defined by Eq.~\eqref{eq:SMI} further subsume sequence-to-sequence models with output in $\mathbb{R}^{D \times M}$ given by:
\begin{align} 
 \mathbf{y} = \x_{l-1} \!\left[\!c\mathbb{1}_M\!+\! \sigma\!\!\left(\!\frac{\x_{l-1}^\top \w_l^{\top} \w_l \x_{l-1} }{D}\!\right)\!\right], \nonumber
\end{align}
instead of the $\mathbb{R}^{M \times M}$ output in Eq.~\eqref{eq:deep_attention_function}. Above, $\x_{l}$ are defined recursively as in Eq.~\eqref{eq:rec} for $l \in [L]$.

The mapping of the above model to SMI follows by writing 
\begin{equation}
    \mathbf{y} = \x \bar{B}^L_c(Z_1,...,Z_l)\,,
\end{equation}
where the functions $\bar{B}^l_c$ are the equivalent for sequence to sequence models of $B^l_c$ in \eqref{eq:def_B}
\begin{align}
    \bar{B}^l_c(Z_1,...,Z_l)=
    \bar{B}^{l-1}_c(Z_1,...,Z_{l-1})\Big[(1-\delta_{l,L})c\mathbb{1}_M+ \sigma \left(
    \bar{B}^{l-1}_c(Z_1,...,Z_{l-1})^\top Z_l^\top Z_l\bar{B}^{l-1}_c(Z_1,...,Z_{l-1})
    \right)\Big]\,,
\end{align}
and $\bar{B}^1_c = \mathbb{1}_M$. 
Intuitively $\bar{B}^L_c(Z_1,...,Z_l) \in \mathbb{R}^{M \times M}$ denotes the components of the tokens (columns) in $\mathbf{y}$ along the basis of the input tokens in $\x$.
Since we assume $D \gg P$, while $\mathbf{y}$ itself is high-dimensional, it can be constructed from $\x$  by specifying the $M \times M$ scalar entries of $\bar{B}^L_c(Z_1,...,Z_l)$. Hence, the estimation of $\mathbf{y}$ given $\x$ is equivalent to the estimation of $ \bar{B}^L_c(Z_1,...,Z_l)$ given $\x$, which reduces to a sequence multi-index (SMI) model with outputs in $\mathbb{R}^{M \times M}$.

Note that for a single layer $L=1$ this sequence-to-sequence variant of the model was analyzed in \cite{cui2024phase}.

\subsection{Multi-head attention}
By the same token, multi-head architectures can also be mapped to a SMI, and thus fall within the scope of our theoretical results. Let us take the input data $\x\in\mathbb{R}^{DH \times M}$ sequence of length $M$ embedded in dimension $DH$. At each layer $l$, the embeddings are split into $H$ vectors of dimension $D$, which are then projected to dimension $P_l$. We can model this by first splitting $\x$ into a list of $H$ elements $\{\x^h\in\mathbb{R}^{D\times M}\}_{h=1}^H$ and then defining $H\times H$ sequence indices for each layer both for the keys and the queries
\begin{equation}
    \mathcal{K}_{l}^{h_1 h_2} = \frac{{}^K\w_l^{h_1 h_2} \x^{h_2}}{\sqrt{D}} \,,\qquad\qquad \mathcal{Q}_{l}^{h_1 h_2} = \frac{{}^Q\w_l^{h_1 h_2} \x^{h_2}}{\sqrt{D}}
\end{equation}
Each attention head is defined as
\begin{equation}
    \sigma \left(\sum_{h_1,h_2}^H \frac{\x_{l-1}^{h_1 \top} {}^Q\w_l^{h h_1\top} {}^K\w^{h h_2}_l \x_{l-1}^{h_2} }{HD}\right) = \sigma \left(\frac{1}{H}\sum_{h_1, h_2}^H (\mathcal{Q}_{l}^{h h_1})^\top \mathcal{K}_{l}^{h h_2}\right)\,.
\end{equation}
The deep multi-head architecture is then defined as 
\begin{align}
    & \forall l\in \llbracket 1,L\rrbracket, ~ \x_l^h= \x_{l-1}^h \left[(1-\delta_{l,L})c\mathbb{1}_M+ \sigma \left(\frac{1}{H}\sum_{h_1, h_2}^H (\mathcal{Q}_{l}^{h h_1})^\top \mathcal{K}_{l}^{h h_2}\right)\right] 
\end{align}
with 
\begin{align}
  y_{\rm MHA}(\x) = \sum\limits_{h}  \boldsymbol{v}_h^\top x_L^h ,
\end{align}
where $\boldsymbol{v}_h\in\R^{P_r\times D}$ are trainable readout weights.
As before, we introduce a set of functions $B^l_c:(\R^{H\times H \times P_1\times M})^{\otimes 2}\times ...\times (\R^{H\times H \times P_{l}\times M})^{\otimes 2}\to\R^{H\times M\times M}$ 
\begin{equation}
\begin{split}
    [B^l_c]_h=&[B^{l-1}_c]_h^{(1-\delta_{l,L})}\left[(1-\delta_{l,L})c\mathbb{1}_M+ \sigma \left(\sum_{h_1,h_2}^H
    [B^{l-1}_c]_{h_1}^\top(\mathcal{Q}_l^{hh_1})^\top \mathcal{K}_l^{hh_2} [B^{l-1}_c]_{h_2}
    \right)\right]\,,
\end{split}
\end{equation}
with $[B^0_c]_h=\mathbb{1}_M$. Then
\begin{align}
  y_{\rm MHA}(\x) = \sum\limits_{h}  \mathcal{V}_h[B_c^L(\{\{\mathcal{K}_l^{h1},..., \mathcal{K}_l^{hH}\}_{h=1}^H\}_{l=1}^L,\{\{\mathcal{Q}_l^{h1},..., \mathcal{Q}_l^{hH}\}_{h=1}^H\}_{l=1}^L)]_h,
\end{align}
defining $\mathcal{V}_h\equiv \boldsymbol{v}_h^\top \x_h$. 

To expound the connection of this deep, multi-head model to SMI models, first denote $P=(P_1+...+P_L)H+P_r$, and define the total weights $\W_{\rm MHA}\in \R^{P\times DH}$, defined as
\begin{align}
   & \W_{\rm MHA}^\top= \left(
      {}^K\boldsymbol{W}_1^1 \dots
      {}^K\boldsymbol{W}_1^H\dots
    {}^K\boldsymbol{W}_L^1\dots
      {}^K\boldsymbol{W}_L^H~~
       {}^Q\boldsymbol{W}_1^1\dots
      {}^Q\boldsymbol{W}_1^H\dots
    {}^Q\boldsymbol{W}_L^1\dots
      {}^Q\boldsymbol{W}_L^H~~
  \boldsymbol{V}    
\right).
\end{align}
with
\begin{align}^K\boldsymbol{W}_l^h=\left(
    \begin{array}{ccc}
 {}^K\w^{h 1}_1  &  &0\\
        & \ddots &\\
       0 &  & {}^K\w^{h H}_1 
    \end{array}
    \right)^\top&&
{}^Q\boldsymbol{W}_l^h=\left(
    \begin{array}{ccc}
 {}^Q\w^{h 1}_1  &  &0\\
        & \ddots &\\
       0 &  & {}^Q\w^{h H}_1 
    \end{array}
    \right)^\top
&&
\boldsymbol{V}=\left(
    \begin{array}{ccc}
 \boldsymbol{v}_1  &  &0\\
        & \ddots &\\
       0 &  & \boldsymbol{v}_H 
    \end{array}\right)^\top
\end{align}
Finally, we construct the link function $g_{\rm MHA}: \R^{P\times M} \to \R^{P_r\times M}$. For any argument $X\in \R^{P\times M}$, we view $X$ as a vertical block vector with $2HL+H$ blocks $\{\{{}^KX^h_l\in\R^{HP_l\times M}\}_{h=1}^H\}_{l=1}^L, \{\{{}^QX^h_l\in\R^{HP_l\times M}\}_{h=1}^H\}_{l=1}^L, \{{}^VX\in\R^{HP_r\times M}\}$, in this order. 
Each of these blocks is further viewed as a block matrix of $H$ vertically stacked blocks, which we index with the subscript $h^\prime$.
Then we define
\begin{align}
   g_{\rm MHA}(X)= \sum\limits_{h}  {}^VX_h[B_c^L(\{\{({}^KX_l^h)_1,..., ({}^KX_l^h)_H\}_{h=1}^H\}_{l=1}^L,\{\{({}^QX_l^h)_1,..., ({}^QX_l^h)_H\}_{h=1}^H\}_{l=1}^L)]_h.
\end{align}
With those definitions, it follows that the multi-head deep attention model can be rewritten as 
\begin{align}
    y_{\rm MHA}(\x)=g_{\rm MHA}(\W_{\rm MHA}\x),
\end{align}
i.e. this model too pertains to the class of SMI models. All the technical results established for SMI models in the main text thus directly transfer to multi-head architectures. 


This Appendix showcases how fairly intricate deep attention architectures, with generically untied weights and multiple heads, fall into the class of SMI models -- which thus proves a truly rich class of models.  The study of the phenomenology of these more complex models and the implication of our theoretical characterization, therefore, are left for future works.


\subsection{Large depth limit of tied, single-head attention} \label{appendix:deep_limit}
In this subsection we study the limit of strong skip connection in the model we are considering, showing how the formalism simplifies in such a limit. We start by redefining the multi-layer attention by rescaling each layer
\begin{align}
    & \forall l\in \llbracket 1,L-1\rrbracket, \qquad \x_l= \x_{l-1} \left[\mathbb{1}_M+ \frac{1}{c}\sigma\left(\frac{\x_{l-1}^\top \w_l \w^{\top}_l \x_{l-1} }{D}\right)\right]\,.
\end{align}
With these new definitions, we are interested in the large depth and skip connection limit $c,L\to\infty$, while keeping the two terms proportional by imposing $c = \beta 
L$. We start by writing the functions $B^l_c$ in this new definition
\begin{equation}\label{appendix:eq:B_recursion}
\begin{split}
    &B^l_c(Z_1,...,Z_l)=B^{l-1}_c(Z_1,...,Z_{l-1})\Big[\mathbb{1}_M + \frac{1}{c}\sigma \left(
    B^{l-1}_c(Z_1,...,Z_{l-1})^\top Z_l^\top Z_lB^{l-1}_c(Z_1,...,Z_{l-1})
    \right)\Big]\,,
\end{split}
\end{equation}
with $B^0_c=\mathbb{1}_M$. We would like to expand the functions above for large $c$, so we define the functions $A^l_c$ such that
\begin{equation}
    B_c^l = \mathbb{1}_M + \frac{A_c^l}{c} + \mathcal{O}\left(\frac{1}{c}\right)\,.
\end{equation}
For $l=0$ we have $A_c^0 = 0$. It is possible to determine the other values of $A_c^l$ by expanding \eqref{appendix:eq:B_recursion}
\begin{equation}
\begin{split}
    B_c^l &= B_c^{l-1}\left[ \mathbb{1}_M + \frac{1}{c}\sigma\left( (B_c^{l-1})^\top Z_l^\top Z_l B_c^{l-1} \right) \right] \\
    &= \left(1 + \frac{1}{c}A_c^{l-1}\right)\left[ \mathbb{1}_M + \frac{1}{c}\sigma\left( \left(1 + \frac{1}{c}A_c^{l-1}\right)^\top Z_l^\top Z_l \left(1 + \frac{1}{c}A_c^{l-1}\right) \right) \right] + \mathcal{O}\left(\frac{1}{c}\right)\\
    &= 1 + \frac{1}{c}A_c^{l-1} + \frac{1}{c}\sigma\left( \left(1 + \frac{1}{c}A_c^{l-1}\right)^\top Z_l^\top Z_l \left(1 + \frac{1}{c}A_c^{l-1}\right) \right)+ \mathcal{O}\left(\frac{1}{c}\right)\\
    &= 1 + \frac{1}{c}A_c^{l-1} + \frac{1}{c}\sigma\left( Z_l^\top Z_l \right)+ \mathcal{O}\left(\frac{1}{c}\right)\,,
\end{split}
\end{equation}
from which we can extract
\begin{equation}
    A_c^l = A_c^{l-1} + \sigma\left( Z_l^\top Z_l \right)\,.
\end{equation}
We can thus provide a closed form expression for $A_c^l$
\begin{equation}
    \forall l\in \llbracket 1,L-1\rrbracket, \qquad A_c^l = \sum_{l'=1}^{l}\sigma\left( Z_{l'}^\top Z_{l'} \right)\,.
\end{equation}
Finally, we have the result
\begin{equation}
    B^L_c(Z_1,...,Z_L) = 
    \sigma\left( \left(1 + \frac{1}{\beta L}\sum_{l=1}^{L-1}\sigma\left( Z_{l}^\top Z_{l} \right)\right)^\top Z_L^\top Z_L \left(1 + \frac{1}{\beta L}\sum_{l=1}^{L-1}\sigma\left( Z_{l}^\top Z_{l} \right)\right) \right)
\end{equation}
This is a very special function. In order to highlight its properties, we will study a generic functions $g:\mathbb{R}^{M\times M}\times\mathbb{R}^{M\times M}\to\mathbb{R}^{M\times M}$
\begin{equation}
    g(J,H) = 
    \sigma\left( \left(1 + \frac{H}{\beta }\right)^\top J \left(1 + \frac{H}{\beta }\right) \right)\,,
\end{equation}
with the identification 
\begin{equation}
    B^L_c(Z_1,...,Z_L) = g\left(Z_L^\top Z_L, \frac{1}{L}\sum_{l=1}^{L-1}\sigma\left( Z_{l}^\top Z_{l} \right)\right)
\end{equation}


\section{Proof of Theorem \ref{thm:free-energy}}\label{appendix:block_AMP}

In this Appendix, we detail the equivalence, discussed in subsection \ref{subsec:Bayes_SMI} of the main text, between a SMI function \eqref{eq:SMI} and a classical multi-index model acting on flattened input and with shared weights. This connection is instrumental in the application of the results of \cite{Gerbelot, Aubin2018, troiani2024fundamental} for multi-index models to SMI models. Leveraging this equivalence, we then provide a proof of Theorem \ref{thm:free-energy}.

\subsection{Mapping SMI models to block-structured multi-index models}
Consider the setup of sequence multi-index models defined by Eq.~\eqref{eq:SMI} and suppose that the link function $g:\mathbb{R}^{P \times M} \rightarrow \mathbb{R}^{K}$ outputs $K$-dimensional vectors $y^{\rm SMI}_{\W^\star}(\x)$.

Given a sequence $\x \in \mathbb{R}^{D \times M}$, we denote by $\x_e \in \mathbb{R}^{DM}$ the corresponding ``flattened" vector defined by:

\begin{equation}\label{eq:flat}
   \x^\top_e \coloneqq [\x_1^\top, \cdots, \x^\top_M],
\end{equation}
where $\x_1, \cdots, \x_M$ denote the columns of $\x$

Under the above flattening, the pre-activations $\W\x \in \mathbb{R}^P$ can be re-expressed through projections of the flattened inputs $x_e$ onto a block-structured weight matrix given by:
\begin{equation}\label{eq:W_flat}
  \W^\star_{e} \coloneqq \begin{bmatrix}
      \W^\star,& 0, &\cdots, & 0 \\
      0, & \W^\star, & \cdots, & 0\\
      \vdots & \vdots & \vdots & \vdots\\
  \end{bmatrix} \in \mathbb{R}^{PM \times DM}.
\end{equation}
Equivalently, $\W^\star_{e}$ can be represented as the Kronecker product $\W^\star \otimes \mathbf{I}_M$

The above representation leads to the following correspondence:
\begin{equation}\label{eq:mat_flat}
    (\W^\star_{e} \x_e) = \begin{bmatrix}
        (\W^\star \x_1)\\
        \cdots\\
        (\W^\star \x_M)
    \end{bmatrix},
\end{equation}
Therefore, the $PM$-dimensional vector $(\W^\star_{e} \x_e)$ is obtained by stacking the projections of different columns of $\x$ onto $\W^\star$. 

With the above definition, $y^{\rm SMI}_{\W^\star}$ can be equivalently expressed as:
\begin{equation}\label{eq:y_map}
y^{\rm SMI}_{\W^\star}=g\left(\frac{\W^\star {\x}}{\sqrt{D}}\right) = g_e\left(\frac{\W^\star_e {\x_e}}{\sqrt{D}}\right),
\end{equation}
where $g_e:\mathbb{R}^{PM} \rightarrow \mathbb{R}^{K}$ is obtained by lifting $g$ through the mapping defined by Eq.~\eqref{eq:mat_flat} to act on inputs in $\mathbb{R}^{PM}$.


\subsection{Proof of Theorem \ref{thm:free-energy}}\label{app:free-en}

We make the following assumption on $g$ derived from \cite{aubin2020exact,barbier2019optimal}:
\begin{assumption}
The non-linearity $g$ in SMI (model \ref{eq:SMI}) lies in $\mathcal{C}^2$ and is entry-wise bounded. 
\end{assumption}

For technical reasons, we further assume that a Gaussian noise matrix with i.i.d $\mathcal{N}(0,\Delta)$ entries with arbitrarily small variance $\Delta$ is added to the SMI output \eqref{eq:SMI} and the corresponding channel \eqref{eq:effective}:
\begin{assumption}
    We consider the limit $\Delta \rightarrow 0$ under the modified activation $\tilde{g}(\cdot) = g(\cdot)+\sqrt{\Delta}\xi$,
\end{assumption}
where $\xi \sim \mathcal{N}(0,\mathbb{I}_K)$ denote noise independent across samples.

The above assumptions are required due to the limitations of the adaptive interpolation framework in \cite{aubin2020exact,barbier2019optimal} and we believe they can be relaxed through a more careful control of the error terms.

We establish Theorem \ref{thm:free-energy} by combining the above mapping defined by Eq.~\eqref{eq:y_map} with the adaptive interpolation scheme in \cite{aubin2020exact,barbier2019optimal}. The central idea behind the adaptive interpolation scheme is to interpolate between the high-dimensional Hamiltonian corresponding to the posterior measure (Eq.  \ref{eq: posterior}) and an effective ``factorized" Hamiltonian constructed through the low-dimensional output channel defined by Eq.~\eqref{eq:effective} and an effective input channel on $\mathbb{R}^{P}$. Analogous to Gaussian-comparison inequalities, one then utilizes Stein's Lemma to relate the time-derivative of the free-energy over the interpolation path to the overlaps corresponding to the Hamiltonian at time $t$. Using the concentration of overlaps (justified through the Nishimori identity and the inclusion of vanishing side-information), one then obtains a family of interpolation paths satisfying systems of ODEs such that the high-dimensional remainder terms during the interpolation vanish. Selecting particular choices for such interpolation paths then yields matching lower and upper bounds for the free-energy.

The mapping of SMI models to block-structured multi-index models on flattened inputs given in Section \ref{appendix:block_AMP} allows us to prove the asymptotic expression for the free-energy through a reduction to the steps utilized in the proof of Theorem 3.1 in \cite{aubin2020exact} and Theorems 1,2 in \cite{barbier2019optimal}. For brevity, we only describe the central differing step, related to the derivation of the time-derivative of the free-entropy described above.

Following \cite{aubin2020exact}, we parameterize the interpolation paths through two functions $r(t):[0,1]\rightarrow \mathcal{S}^+_P$ and $q(t):[0,1]\rightarrow \mathcal{S}^+_P$ and scalar parameters $\epsilon_1, \epsilon_2 \in \mathbb{R}^+$. Let $R_1(t), R_2(t)$ denote the solutions to the following ODEs:
\begin{equation}
    R_1(t) \coloneqq \epsilon_1 + \int_{0}^t r(t) \,{\rm d}s, \qquad  R_2(t) \coloneqq \epsilon_1 + \int_{0}^t q(t)\, {\rm d}s
\end{equation}
 
Define $S_{t,\mu} \in \mathbb{R}^{P \times M}$ for $\mu \in [N]$ as the following ``interpolating" inputs to the effective output channel (Eq.~\eqref{eq:effective}):
\begin{equation}
    S_{t,\mu} \coloneqq \sqrt{1-t}\frac{\W\x}{\sqrt{D}}+\sqrt{R_2(t)} \omega_\mu + \sqrt{t\mathbb{1}-R_2(t)}U^\star_\mu, 
\end{equation}
where $\omega_\mu, U^\star_\mu \in \mathbb{R}^{P \times M}$ have entries $\stackrel{\text{i.i.d.}}{\sim}  \mathcal{N}(0,1)$, with $\omega_\mu$ playing the role of observed side-information while $\W,U^\star_\mu$ denote unkown parameters.

We further define the corresponding interpolating variables at the input channel for $i \in [D]$:
\begin{equation}
    Y'_{t,i}= \sqrt{R_1(t)}\W_i+Z'_i, 
\end{equation}
where $Y'_{t,i}, X^\star_i \in \mathbb{R}^{P}$. We thus obtain two sets of ``interpolating" parameters, observations:
\begin{equation}
    Y_{t,\mu} = g(S_{t,\mu}),
\end{equation}
for $\mu \in [N]$ and $Y'_{t,i}$ for $i \in [D]$.  These define an ``interpolating posterior":
\begin{equation}
   P(\W, U^\star| \{Y'_{t,i}\}_{i \in [D]}, \{Y_{t,\mu},\x_\mu, \omega_\mu\}_{\mu \in [N]}) =  \frac{1}{Z_{n,\epsilon}(t)} e^{-\frac{1}{2}\!\Tr[\W\W^\top\!]}\! \prod\limits_{d=1}^D e^{-\frac{1}{2}\|Y'_{t,i}- \sqrt{R_1(t)}\W_i\|^2}  \prod\limits_{\mu=1}^N P_Y(Y_{t,\mu}|S_{t,\mu}).
\end{equation},
(Eq. 17 in \cite{Aubin2018}) and the associated free-entropy:
\begin{equation}
    f_{n,\epsilon}(t) = \frac{1}{n}\mathbb{E}[\log Z_{n,\epsilon}(t)],
\end{equation}
which reduces to the original model (Equation \ref{eq: posterior}) at $t, \epsilon = 0$.

The only difference from the setup in \cite{aubin2020exact} lies in that $S_{t,\mu}$ are matrices, instead of $P$-dimensional vectors. However, leveraging the flattening defined by Eq.~\eqref{eq:mat_flat}, we may express  $S_{t,\mu}$ as:
\begin{equation}
    S_{t,\mu,e} \coloneqq \sqrt{1-t}\frac{\W^\star_e\x_e}{\sqrt{D}}+\sqrt{R_2(t)}_e \omega_{\mu,e} + \sqrt{t\mathbb{1}-R_2(t)}_e U^\star_{\mu,e}, 
\end{equation}

Under the above flattening, equation 44 in \cite{Aubin2018} (arising from applying Gaussian integration by parts to the interpolating Hamiltonian w.r.t variables $\x_e$ and $U^\star_{\mu,e}$) are modified with the replacement:
\begin{equation}
    \W^\star \W^\top \rightarrow \W^\star_e\W^\top_e.
\end{equation}
The mapping is then completed by noting that the flattening operator commutes under multiplication (as one expects from the properties of Kronecker/tensor products). Therefore:
\begin{equation}\label{eq:com}
    \W^\star_e\W^\top_e = (\W^\star\W^\top)_e.
\end{equation}
Under the above substitution, the limiting free-entropy is obtaining by following the remainder of the proof of Theorem 3.1 in \cite{Aubin2018}, resulting in the variational problem given by Eq.~\eqref{eq:freeenergy}. Subsequently, the resulting prediction error is obtained through steps identical to the proof of Theorem 2 in \cite{barbier2019optimal}.

\section{Proofs of Lemma \ref{lem:SE} and Theorem \ref{th:weak_recovery}}\label{app:weak-rec}

In this section, we prove the results presented in Section \ref{section:weak_recovery}, namely the tight asymptotic characterization of AMP (Algorithm \ref{alg:AMP}) in terms of its finite-dimensional state evolution equations (Lemma \ref{lem:SE}), and the characterization of weak recovery thresholds (Theorem \ref{th:weak_recovery}). We start by establishing Lemma \ref{lem:SE} through a mapping to general results for GAMP algorithms with non-separable non-linearities. We remark that Lemma \ref{lem:SE}
and Theorem \ref{th:weak_recovery} themselves do not establish the optimality of Algorithm \ref{alg:AMP} or even prescribe its particular form -- the latter will be derived in Appendix \ref{appendix:AMP}. However, by comparing the resulting expressions for state-evolution \eqref{eq:SE} to the extremization criteria in Theorem \ref{thm:free-energy}, we observe that GAMP asymptotically achieves the optimal error. 
 
Throughout, we make the following assumptions on the non-linearity $g$:

\begin{assumption}
The denoiser $g_{out}$ defined by \ref{eq:g_out} lies in $\mathcal{C}^2$.
\end{assumption}
It is easy to check that the above assumptions holds for any $g \in \mathcal{C}^2$ in the presence of vanishing noise.

\subsection{Proof of Lemma \ref{lem:SE}}

Analogous to the proof of Theorem \ref{thm:free-energy}, the proof of Lemma \ref{lem:SE} relies on the mapping of the sequence models to multi-index models on flattened inputs, as described in Appendix \ref{appendix:block_AMP}. Under this mapping, Algorithm \ref{alg:AMP} can be expressed in the following form:

\begin{align}
    {\bm \Omega }^{t} &= \x_e f_t({\bf B}^t) - g_{t-1}({\bm \Omega}^{t-1},{\bf y})\bV_t\\
    {\bf B}^{t+1} &= \x^T_e g_t({\bm \Omega}^t,{\bf y}) + f_{t} ({\bf B}^{t})\bA_t,    
\end{align}
where $\bm \Omega^t \in \mathbb{R}^{N \times PM}$ contains rows corresponding to the flattened version of $\omega^t_e$ in Algorithm \ref{alg:AMP} as per Eq.  \ref{eq:mat_flat} and $\bf{B^t} \in  \mathbb{R}^{DP \times PM}$ contains columns given by the columns of $\hat{\W}^t_e$ in Algorithm \ref{alg:AMP}, flattened as per Eq.  \ref{eq:W_flat}. $\bV_t, \bA_t$ then denote the corresponding ``flattened" Onsager terms. Finally, the non-linearities $g_t$ and $f_t$ are defined as:
\begin{equation}
   g_t({\bm \Omega}^t,{\bf y})_\mu = \partial_\omega g_{out}(\omega^t_\mu,y_\mu)_e \in \mathbb{R}^{PM},
\end{equation}
applied row-wise and:
\begin{equation}
    f_t\left(\begin{bmatrix}
        \mathbf{B}^t_1 & \star         & \cdots       & \star \\
        \star         & \mathbf{B}^t_2 & \ddots       & \vdots \\
        \vdots       & \ddots       & \ddots       & \star \\
        \star         & \cdots       & \star        & \mathbf{B}^t_M
    \end{bmatrix}\right)
    =
    \begin{bmatrix}
        \bar{\mathbf{B}}^t & 0                  & \cdots       & 0 \\
        0                  & \bar{\mathbf{B}}^t & \ddots       & \vdots \\
        \vdots           & \ddots            & \ddots       & 0 \\
        0                  & \cdots           & 0            & \bar{\mathbf{B}}^t
    \end{bmatrix} \, (\mathbb{1}_P+A^t)^{-1}_e.
\end{equation}

where $\bf{B}^t_j$ for $j \in [M]$ denotes the $D \times P$ block of parameters in $\bf{B}^t$ associated with the token $j$ and $\bar{\bf{B}^t} = \sum_{m=1}^M  \bf{B}^t_m$. The $PM \times PM$ matrix $(\mathbb{1}_P+A^t)^{-1}_e$ represents the tensorized version of $(\mathbb{1}_P+A^t)^{-1}_e$ as per Equations \ref{eq:flat}, \ref{eq:W_flat}.
We note that the block-structured copying and averaging in $f_t$, which maintains the same estimate $\hat{\W}^t$ for different rows of $\hat{\W}^t_{e}$ is non-separable across the $DM$ rows of $\bf B^t$. Such non-seperable non-linearities are however, permitted within the existing analysis in \cite{Gerbelot,berthier2020state} since it leads to well-defined convergence of empirical averages (Assumptions A5-A7 in Theorem 1 of \cite{Gerbelot}). Furthermore, using the commutativity of the tensorization and matrix-multiplication in \eqref{eq:com}, the resulting state-evolution equation (Eq.~\eqref{eq:SE}) admits the same form as for the regular multi-index models in \cite{troiani2024fundamental}.

Therefore, applying Theorem 1 in \cite{Gerbelot}, we obtain that for any number of finite time step $t \in \mathbb{N}$:
\begin{equation}
   \frac{\hat{\W}^t (\W^*)\top}{D} \xrightarrow[P]{d \rightarrow \infty} Q^t,
\end{equation}
where $Q^t$ satisfies Eq.  \ref{eq:SE}. 


\subsection{Definition of the Weak-Recovery Thresholds and the Proof of Theorem \ref{th:weak_recovery}}

As explained in the discussion preceding Theorem \ref{th:weak_recovery}, Lemma \ref{lem:SE} allows us to characterize weak-recovery under vanishing side-information through a stability analysis of the state-evolution. We begin by providing the precise definitions for weak-recovery thresholds based on \cite{troiani2024fundamental}:

\begin{definition}
    Consider Algorithm \ref{alg:AMP} in the presence of side information of the form $\lambda \W^\star + \sqrt{1-\lambda} \xi$, where $\xi$ contains independent entries $\xi_{ij}\sim \mathcal{N}(0,1)$.
    We say that $\alpha_c > 0$ is a \textit{weak-recovery threshold} for Algorithm \ref{alg:AMP} if the following hold with high probability as $D \rightarrow \infty$:
    \begin{enumerate}
        \item For any $\alpha > \alpha_c$, there exists $ \delta > 0$ such that for any (arbitrarily small) $\lambda > 0$, $\exists t= \mathcal{O}(\log \frac{1}{\lambda})$ such that $\norm{\frac{\hat{\W}^t (\W^\star)^\top}{D}} \geq \delta$.
        \item For $\alpha < \alpha_c$, $\norm{\frac{\hat{\W}^t (\W^\star)^\top}{D}} \leq c\sqrt{\lambda}$ for some constant $c>0$ and any $t  \in \mathbb{N}$.
    \end{enumerate}
\end{definition}

Analogously, we define thresholds characterizing the weak-recovery of given a subpsace $U \subseteq \mathbb{R}^p$:
\begin{definition}\label{def:sub-weak-rec}
    For a subspace $U \subseteq \mathbb{R}^p$, we say that $\alpha_c(U) > 0$ is the  \textit{subspace weak-recovery threshold} for $U$ if:
    \begin{enumerate}
        \item For any $\alpha > \alpha_c(U)$, there exists $ \delta > 0$ such that for any (arbitrarily small) $\lambda > 0$, $\exists t= \mathcal{O}(\log \frac{1}{\lambda})$ such that $\frac{\hat{\W}^t \W^\star}{D} \succ \delta P_U$.
        \item For $\alpha < \alpha_c(U)$, $\exists v \in U$ with $\norm{v}=1$ such that  $\norm{\frac{\hat{\W}^t vv^\top (\W^\star)^\top}{D}} \leq c\sqrt{\lambda}$ for some constant $c>0$ for any $t  \in \mathbb{N}$.
    \end{enumerate}
\end{definition}


Under the above definitions and Lemma \ref{lem:SE}, the proof of Theorem \ref{th:weak_recovery} follows \cite{troiani2024fundamental}. Since the form of state-evolution in Equation \ref{eq:se_dynamics} exactly matches the form in \cite{troiani2024fundamental}, we only outline the main steps.

The inclusion of side-information  $\lambda \W^\star + \sqrt{1-\lambda} \xi$, which amounts to a modification of the prior over $\W$, changes the state evolution dynamics in Lemma \ref{lem:SE} to Equation \ref{eq:se_dynamics}. Under the assumption
$g_{\rm out} \in \mathcal{C}^2$ the following linearization holds for the state-evolution dynamics defined by Equation \ref{eq:se_dynamics}:

\begin{equation}
 Q^{t+1} \approx \alpha\mathcal{F}(\delta Q^t) + \sqrt{\lambda} \mathbf{I}_d + \mathcal{O}(\alpha\norm{\delta Q^t}_F^2) + \mathcal{O}(\lambda),
\end{equation}

The term $\sqrt{\lambda} \mathbf{I}_d$ arising from side-information implies that at $t=1$, the overlaps satisfy for small enough $\lambda$:
\begin{equation}
   \operatorname{tr}(Q^{1},Q^\star) > \lambda > 0,
\end{equation}
with $Q^{1} \in \mathbb{S}_P^+$. The generalized-Perron Frobenius theorem then implies that $\mathcal{F}$ admits a leading eigenvector in $ \mathbb{S}_P^+$. Therefore, whenever $\alpha > \alpha_c$, we obtain a recursion of the form:
\begin{equation}\label{eq:growth}
    \operatorname{tr}(Q^{t+1},Q^\star) \geq (1+\kappa) \operatorname{tr}(Q^{t},Q^\star),
\end{equation}
for some $\kappa >0$, leading to the positive part of Theorem \ref{th:weak_recovery} for $\alpha > \alpha_c$ through Theorem 4.2 in \cite{troiani2024fundamental}. The negative part follows from an analogous reverse inequality as detailed in the proof of Theorem 4.2 in \cite{troiani2024fundamental}. Similarly, the proof of Proposition 5.1 in \cite{troiani2024fundamental} based on the monotonocity of the GAMP dynamics, implies the existence and consistency with Definition \ref{def:sub-weak-rec} of the subspace weak-recovery threshold defined by Equation \ref{eq:limt_si}.






\section{Derivation of GAMP from Belief Propagation}\label{appendix:AMP}

This Appendix details the derivation of the precise form of the GAMP algorithm (Algorithm \ref{alg:AMP}) associated with the Bayesian inference problem \eqref{eq: posterior}, alongside the expressions of the state-evolution equations \eqref{eq:SE} that asymptotically describe its dynamics. A rigorous proof of the state evolution is presented in Appendix \ref{app:weak-rec}.


The departure point of this derivation are the Belief Propagation iterations \cite{pearl2014probabilistic} associated with \eqref{eq: posterior}, then simplifying them to offer a linear time algorithm that can be practically implemented with ease \cite{mezard2009information,wainwright2008graphical,Lauditi_2024}.
Contextually, we will derive the State Evolution iteration \eqref{eq:SE} and show that it describes the performance of GAMP at each iteration. Note that the derivation of GAMP and its state evolution for SMI models was previously reported in \cite{cui2024highdimensionallearningnarrowneural} for empirical risk minimization problems. We complement this analysis with the case of Bayesian inference considered in the present work. While this derivation is heuristic in nature, we feel it provides an insightful view of how to propose algorithms for Bayesian estimation.

We start by defining a factor graph, which is a bipartite graph with two families of nodes: $D$ variable notes, each encoding a weight vector $\W_i\in\mathbb{R}^{P}$ and $D + N$ factor nodes that connect them. $D$ of the factor nodes are each linked to one variable node and encode the Gaussian prior, the remaining $N$ are each connected to all the weights and represent the constraint that properly tuned weights should map the input data $\x^\mu\in\mathbb{R}^{D\times M}$ to the labels $y^\mu\in\mathbb{R}^{M'}$ based on the examples in the dataset through a sequence multi-index model linked by  $g:\mathbb{R}^{P\times M} \to \mathbb{R}^{M'}$.
We give a pictorial representation in Fig.~\ref{fig_factG}. We then introduce two families of messages $\hat{m}_{\mu \to i}$, $m_{i \to \mu}$, which are the marginal probabilities of $\W_i$ if we remove the edges $i \to \mu$ or $\mu \to i$. In the following it will be convenient to call $\W\in\mathbb{R}^{P\times D}$ the stack of vectors $\W_i$
\begin{figure}[htb!]
\centering
	\begin{tikzpicture}[scale=1.2]
    \tikzstyle{annot} = [text width=3cm, text centered]
       \tikzstyle{factor}=[rectangle,minimum size=11pt,draw=black, fill opacity=1.,fill=black]
    \tikzstyle{latent}=[circle,minimum size=16pt,draw=black, fill opacity=1.,fill=white]     	
	\def\Nv{4}
	\def\Nu{3}
	\def\Nf{12}        
         \foreach \name / \y in {1,...,\Nu}
        \node[factor] (F2-\name) at (0,-0.5-\y) {};
        
        \foreach \name / \y in {1,...,\Nv}
        \node[latent] (W-\name) at (3.25,-\y) {};
         \foreach \name / \y in {1,...,\Nv}
        \node[factor] (P-\name) at (5.5,-\y) {};
    
\path[-latex] (F2-1) edge node[above]{$\hat{m}_{\mu \to i}$} (W-1);
\path[-latex] (W-4) edge node[below]{$m_{i \to \mu}$} (F2-3);        	
    \foreach \i in {1,...,\Nu}
    	 \foreach \j in {1,...,\Nv}
        	\edge[-]{F2-\i}{W-\j};  
    	\foreach \j in {1,...,\Nv}
        	\edge[-]{P-\j}{W-\j};     	

\node[annot,above of=F2-1, node distance=1.4cm] {$\delta\left(y^{\mu} - g\left( \frac{\W \x^{\mu}}{\sqrt{D}}\right)\right)$ \\ $\mu =1,\,...\,, N$};
\node[annot,above of=W-1, node distance=1cm] (hl) {$\W_i \in \mathbb{R}^P$ \\ $i=1,\,...\,,D$};
\node[annot,above of=P-1, node distance=1cm] (hl) {$\frac{e^{-\frac{\|\W_i\|^2}{2}}}{(2\pi)^{P/2}}$ \\ $i=1,\,...\,,D$};
\end{tikzpicture}
	 \caption{Factor graph representation of a sequence multi-index model with $4$ variable nodes (circles) and $3 + 4$ factor nodes (squares). Picture inspired by \cite{Aubin2018}}
\label{fig_factG}
\end{figure}




It is possible to define an iterative algorithm on the messages, commonly called the Belief Propagation (BP) equations.
\begin{equation}
	\begin{split}
		&m_{i\to \mu}^{t+1} (\W_i) = \displaystyle \frac{1}{\mathcal{Z}_{i\to \mu}} \frac{e^{-\frac{\|\W_i\|^2}{2}}}{(2\pi)^{P/2}} \prod\limits_{\nu \neq \mu}^N \hat{m}_{\nu \to i}^t (\W_i)\,, \vspace{0.1cm}\\
		&\hat{m}_{\mu \to i}^t (\W_i) =  \displaystyle \frac{1}{\mathcal{Z}_{\mu \to i}} \int \prod\limits_{j\neq i}^D {\rm d}\W_j \delta\left (y^{\mu} - \displaystyle  g\left( \sum_{j=1}^D  \frac{\W_{j}\x^\mu_{j}}{\sqrt{D}} \right)\right)  m_{j \to \mu}^t (\W_j ) \,.
	\end{split}
	\label{supp:BPEquations}
\end{equation}
It is impossible to iterate the BP equations for this problem in practice, but that's not an issue: to estimate the weights $w$ we only need the first few moments of the messages. This observation will allow us to obtain an efficient algorithm called Relaxed BP.
It is convenient to introduce the sequence indices $Z^\mu \in \mathbb{R}^{P\times M}$ using a Fourier transform.
\begin{equation}
\begin{split}
&\delta\left (y^{\mu} - g\left( \sum_{j=1}^D  \frac{\W_{j}\x^\mu_{j}}{\sqrt{D}} \right)\right) = \\
&\qquad\qquad\frac{1}{(2\pi)^{PM}}
\int_{\mathbb{R}^{P\times M}} {\rm d} Z^\mu d \hat{Z}^\mu \exp\left\{ -i \sum_{m=1}^M{({\hat{Z}_m}^\mu)}^\top \left( \sum_{j=1}^D  \frac{\W_{j}\x^\mu_{jm}}{\sqrt{D}}\right) + i \sum_{m=1}^M{Z_m^\mu}^\top {\hat{Z}_m}^\mu\right\}\delta\left (y^{\mu} - g(Z^\mu) \right) \,.	
\end{split}
\end{equation}
This representation can be put in the equation \eqref{supp:BPEquations} for $\hat{m}_{\mu \to i}^t (\W_i )$ , taking care of separating the contribution of $\W_i$
\begin{equation}
\begin{split}
\hat{m}_{\mu \to i}^t (\W_i ) = 
\frac{1}{(2\pi)^{PL}\mathcal{Z}_{\mu\to i} }
\int_{\mathbb{R}^{P\times M}} {\rm d} Z^\mu {\rm d} \hat{Z}^\mu \delta\left (y^{\mu} - g(Z^\mu) \right)
\exp\left\{ -i \sum_{m=1}^M{(\hat{Z}_m^\mu)}^\top \left( \displaystyle   \frac{\W_{i}\x^\mu_{im}}{\sqrt{D}}\right) + i \sum_{m=1}^M{Z_m^\mu}^\top {\hat{Z}_m}^\mu\right\}&\\
 \times\prod\limits_{j\neq i}^D  \underbrace{\int_{\mathbb{R}^P} {\rm d} \W_j 
		 m_{j \to \mu}^t (\W_j ) \exp\left\{ -i \sum_{m=1}^M{({\hat{Z}_m}^\mu)}^\top \left( \displaystyle   \frac{\W_{j}\x^\mu_{jm}}{\sqrt{D}}\right) \right\}}_{\equiv I_j}
		 \label{mtilde}& \,,
\end{split}
\end{equation}
It is time to define the mean and variance of the messages $m_{j\to \mu}^t$
\begin{equation} \label{appendix:eq:moment_messages}
		\hat{\W}_{j\to \mu}^t \equiv  \displaystyle \int_{\mathbb{R}^P} {\rm d} \W_j
		 m_{j\to \mu}^t (\W_j ) \W_j  \,, \qquad\qquad
		 \hat{C}_{j\to \mu}^t \equiv \displaystyle  \int_{\mathbb{R}^P} {\rm d} \W_j
		 m_{j\to \mu}^t (\W_j ) \W_j \W_j^\top - \hat{\W}_{j\to \mu}^t(\hat{\W}_{j\to \mu}^t)^\top \,.
\end{equation}
In the limit $D\to \infty$ the term $I_j$ can be easily expanded. Then, we can recognize that the lowest terms in the expansion are nothing but simple functions of the messages \eqref{appendix:eq:moment_messages}, and obtain a resummed expression at leading order in $D$.
\begin{small}
\begin{align*}
I_j &= \int_{{\mathbb{R}}^P} {\rm d} \W_j
		 m_{j \to \mu}^t (\W_j ) e^{ -i \sum_{m=1}^M{(\hat{Z}_m^\mu)}^\top \left( \displaystyle \frac{  \W_{j}\x^\mu_{jm}}{\sqrt{D}}\right)} \notag\\
         &\approx  \exp\left\{ -i \sum_{m=1}^M \frac{\x^\mu_{jm}}{\sqrt{D}}{(\hat{Z}_m^\mu)}^\top  \hat{\W}_{j\to \mu}^t + \sum_{m,m'=1}^M \frac{1}{2}\frac{\x^\mu_{jm}\x^\mu_{jm'}}{D} {(\hat{Z}_m^\mu)}^\top  \hat{C}_{j\to \mu}^t  \, \hat{Z}_{m'}^\mu\right\} \,,\\&
         \approx  \exp\left\{ -i \sum_{m=1}^M \frac{\x^\mu_{jm}}{\sqrt{D}}{(\hat{Z}_m^\mu)}^\top  \hat{\W}_{j\to \mu}^t + \sum_{m}^M \frac{1}{2}\frac{(\x^\mu_{jm})^2}{D} {(\hat{Z}_m^\mu)}^\top  \hat{C}_{j\to \mu}^t  \, \hat{Z}_{m}^\mu\right\}
\end{align*} 
\end{small}
and finally using the inverse Fourier transform, we obtain
\begin{small}
\begin{align*}
&\hat{m}_{\mu \to i}^t (\W_i ) \simeq 
\frac{1}{\mathcal{Z}_{\mu \to i}} \frac{1}{(2\pi)^{PM}}
\int_{\mathbb{R}^{P\times M}} {\rm d} Z\, \delta(y^\mu - g(Z^\mu))  
\int_{\mathbb{R}^{P\times M}} {\rm d} \hat{Z}\, \exp\left\{ -i \sum_{m=1}^M{(\hat{Z}_m^\mu)}^\top \left( \displaystyle \frac{\W_{i}\x^\mu_{im}}{\sqrt{D}}\right) + i \sum_{m=1}^M{(Z_m^\mu})^\top {\hat{Z}_m}^\mu\right\} \\
&\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\times\prod\limits_{j\neq i}^D  \exp\left\{ -i \sum_{m=1}^M \frac{\x^\mu_{jm}}{\sqrt{D}}{(\hat{Z}_m^\mu)}^\top  \hat{\W}_{j\to \mu}^t + \sum_{m}^M \frac{1}{2}\frac{(\x^\mu_{jm})^2}{D} {(\hat{Z}_m^\mu)}^\top  \hat{C}_{j\to \mu}^t  \, \hat{Z}_{m}^\mu\right\} \\
&= \frac{1}{\mathcal{Z}_{\mu \to i}} \frac{1}{(2\pi)^{PM}}
\int_{\mathbb{R}^{P\times M}} {\rm d} Z\, \delta(y^\mu - g(Z^\mu))  
\int_{\mathbb{R}^{P\times M}}\!\!\!\! \!\!\! \!\!\! {\rm d} \hat{Z}\, 
\prod_{m=1}^M\left[
e^{i{(\hat{Z}_m^\mu)}^\top \left(Z_m^\mu - \frac{\x_{im}^\mu}{\sqrt{D}}\W_i -\sum_{j\neq i}^D\frac{\x_{jm}^\mu}{\sqrt{D}}\hat{\W}_{j\to \mu}^t \right) + \frac{(\x^\mu_{jm})^2}{2D} {(\hat{Z}_m^\mu)}^\top  \hat{C}_{j\to \mu}^t  \, \hat{Z}_{m}^\mu }\right] \\
&= \frac{1}{\mathcal{Z}_{\mu \to i}} \frac{1}{\sqrt{(2\pi)^{PM}\prod_{m=1}^M{\rm det}{(V_{im}^{\mu,t})}}}
\int_{\mathbb{R}^{P\times M}}\!\!\! \!\!\! \!\!\!  {\rm d} Z\, \delta(y^\mu - g(Z^\mu))\prod_{m=1}^M \left[ 
\underbrace{e^{-\frac{1}{2} \left( Z_m -\frac{\x^{\mu}_{im}}{\sqrt{D}} \W_i - \omega_{im}^{\mu,t} \right)^\top (V_{im}^{\mu,t})^{-1} \left( Z_m -\frac{\x^\mu_{im}}{\sqrt{D}} \W_i - \omega_{im}^{\mu,t} \right)}}_{\equiv H_{im}^\mu}\right] \,,
\end{align*}
\end{small}
where we introduced the mean and variance of  $H_{im}^\mu$
\begin{equation}
	\omega_{im}^{\mu,t} \equiv \displaystyle  \frac{1}{\sqrt{D}} \sum\limits_{j\neq i}^D \x_{jm}^\mu  \hat{\W}_{j\to \mu}^t \,, \hspace{0.5cm} 
		V_{im}^{\mu,t} \equiv \displaystyle  \frac{1}{D} \sum\limits_{j\neq i}^D (\x_{jm}^\mu)^2  \hat{C}_{j \to \mu}^t \,.
		\label{appendix:amp:omega_V}
\end{equation}
As we did, before we will expand the term $H_{im}^\mu$ in the $D\to \infty$ limit keeping the leading order terms
\begin{align*}
	H_{im}^\mu &\simeq   e^{-\frac{1}{2} \left( Z_m -\omega_{im}^{\mu,t} \right)^\top (V_{im}^{\mu,t})^{-1} \left( Z_m - \omega_{im}^{\mu,t} \right) } 
	\left( 1 + \frac{\x_{im}^\mu}{\sqrt{D}} \W_i^\top (V_{im}^{\mu,t})^{-1} (Z_m - \omega_{im}^{\mu,t}) -\frac{1}{2}\frac{(\x_{im}^\mu)^2}{D} \W_i^\top (V_{im}^{\mu,t})^{-1} \W_i \right.\\
& \left. + \frac{1}{2} \frac{(\x_{im}^\mu)^2}{D} \W_i^\top (V_{im}^{\mu,t})^{-1} (Z_m - \omega_{im}^{\mu,t})(Z_m - \omega_{im}^{\mu,t})^\top  (V_{im}^{\mu,t})^{-1} \W_i \right).
\end{align*}
As we resum the expression above, we can express it using $g_{\rm out}$ and $\partial_\omega g_{\rm out}$, as defined in \eqref{appendix:eq:g_out}. For convenience, we can define $g^{\mu,t}\in \mathbb{R}^{P}$, $G^{\mu,t}_m\in \mathbb{R}^{P\times P}$
\begin{equation}
    g^{\mu,t}_m \equiv \left[g_{\rm out} (y^\mu,\omega_i^{\mu,t}, V_i^{\mu,t})\right]_m\,,\qquad\qquad G^{\mu,t}_m \equiv \left[\partial_\omega g_{\rm out}(y^\mu, \omega_i^{\mu,t}, V_i^{\mu,t})\right]_{mm}\,.
\end{equation}
We arrive to the expression
\begin{equation}
\begin{split}
\hat{m}_{\mu  \to i}^t (\W_i ) &\approx \frac{1}{\mathcal{Z}_{\mu \to i}} \left \{1 +  \sum_{m=1}^M\frac{\x_{im}^\mu}{\sqrt{D}} \W_{i}^\top  g^{\mu,t}_m +
\sum_{m=1}^M\frac{(\x_{im}^\mu)^2}{2D} \W_{i}^\top g^{\mu,t}_m  {(g^{\mu,t}_m)}^\top  \W_{i} + \sum_{m=1}^M\frac{(\x_{im}^\mu)^2}{2D} \W_{i}^\top  G^{\mu,t}_i  \W_{i}
\right\}\\
&= \frac{1}{\mathcal{Z}_{\mu \to i}} \left\{ 1 + \W_{i}^\top  B_{\mu \to i}^t +\frac{1}{2}  \W_{i}^\top  B_{\mu \to i}^t (B_{\mu \to i}^t)^\top  \W_{i} -\frac{1}{2} \W_{i}^\top  A_{\mu \to i}^t \W_{i} \right\} \\
&\approx\sqrt{\frac{\det(A_{\mu \to i}^t)}{(2\pi)^P}} \exp\left\{-\frac{1}{2}\left(\W_{i}^\top  - (A_{\mu \to i}^t)^{-1}B_{\mu \to i}^t \right)^\top  A_{\mu \to i}^t\left(\W_{i}^\top  - (A_{\mu \to i}^t)^{-1}B_{\mu \to i}^t \right) \right\} \,,
\label{supp:mtilde}
\end{split}
\end{equation}
where we implicitly defined $A_{\mu \to i}$ and $B_{\mu \to i}$:
\begin{equation}
\label{appendix:amp:A_B}
		B_{\mu \to i}^t \equiv  \sum_{m=1}^M\frac{\x_{im}^\mu}{\sqrt{D}} g_m^{\mu,t}, \qquad\qquad
		A_{\mu \to i}^t \equiv - \sum_{m=1}^M\frac{(\x_{im}^\mu)^2}{D}  G_m^{\mu,t}\,.
\end{equation}
The approximation in equation \eqref{supp:mtilde} can be plugged in the first equation of \eqref{supp:BPEquations}
\begin{equation}
\begin{split}
    &m_{i\to \mu}^{t+1} (\W_i) \notag\\&= \displaystyle \frac{1}{\mathcal{Z}_{i\to \mu}} \frac{e^{-\frac{\|\W_i\|^2}{2}}}{(2\pi)^{P/2}} \prod\limits_{\nu \neq \mu}^N \sqrt{\frac{\det(A_{\nu \to i}^t)}{(2\pi)^P}} \exp\left\{-\frac{1}{2}\left(\W_{i}^\top  - (A_{\nu \to i}^t)^{-1}B_{\nu \to i}^t \right)^\top  A_{\nu \to i}^t\left(\W_{i}^\top  - (A_{\nu \to i}^t)^{-1}B_{\nu \to i}^t \right) \right\} \\
    &= \displaystyle \frac{1}{\mathcal{Z}_{i\to \mu}} \sqrt{\frac{ \prod\limits_{\nu\neq\mu}^N\det(A_{\nu \to i}^t)}{(2\pi)^{P}}}  e^{-\frac{1}{2}\W_{i}\left(\mathbb{1}_P + \sum\limits_{\nu\neq\mu}^N A_{\nu \to i}^t\right) \W_{i}^\top + \W_i \sum\limits_{\nu\neq\mu}^N B_{\nu \to i}^t - \frac{1}{2} \sum\limits_{\nu\neq\mu}^N(B_{\nu \to i}^t)^\top (A_{\nu \to i}^t)^{-1} B_{\nu \to i}^t }\,.
\label{mil}
\end{split}
\end{equation}
We have come full circle, as we can now explicitly write the moments of $m_{i\to \mu}^{t}$ we first introduced in \eqref{appendix:eq:moment_messages}.
\begin{equation}
		\hat{\W}_{i\to \mu}^{t+1} = \left(\mathbb{1}_P + \sum\limits_{\nu\neq\mu}^N A_{\nu \to i}^t\right)^{-1}\left(\sum\limits_{\nu \ne \mu}^N  B_{\nu \to i}^t\right)\,,   \qquad\qquad
		\hat{C}_{i \to \mu}^{t+1} = \left(\mathbb{1}_P + \sum\limits_{\nu\neq\mu}^N A_{\nu \to i}^t\right)^{-1} \,.
\end{equation}
What we have obtained is thus a consistent set of equations that iterates on the moments of the BP messages. We give a compact description of this algorithm here
\begin{equation} \label{appendix:eq:relaxed_BP}
	\begin{split}
		\omega_{im}^{\mu,t} &= \displaystyle  \frac{1}{\sqrt{D}} \sum\limits_{j\neq i}^D \x_{jm}^\mu  \hat{\W}_{j\to \mu}^t\vspace{0.3cm} \\
		V_{im}^{\mu,t} &= \displaystyle  \frac{1}{D} \sum\limits_{j\neq i}^D (\x_{jm}^\mu)^2  \hat{C}_{j \to \mu}^t \vspace{0.3cm} \\
        g^{\mu,t}_m &= \left[g_{\rm out} (y^\mu,\omega_i^{\mu,t}, V_i^{\mu,t})\right]_m \vspace{0.3cm}\\
        G^{\mu,t}_m &= \left[\partial_\omega g_{\rm out}(y^\mu, \omega_i^{\mu,t}, V_i^{\mu,t})\right]_{mm} \vspace{0.3cm}\\
		B_{\mu \to i}^t &= \sum_{m=1}^M\frac{\x_{im}^\mu}{\sqrt{D}} g_m^{\mu,t}\vspace{0.3cm}  \\
		A_{\mu \to i}^t &= - \sum_{m=1}^M\frac{(\x_{im}^\mu)^2}{D}  G_m^{\mu,t}\vspace{0.3cm} \\
        \hat{\W}_{i\to \mu}^{t+1} &= \left(\mathbb{1}_P + \sum\limits_{\nu\neq\mu}^N A_{\nu \to i}^t\right)^{-1}\left(\sum\limits_{\nu \ne \mu}^N  B_{\nu \to i}^t\right)\,,   \\
		\hat{C}_{i \to \mu}^{t+1} &= \left(\mathbb{1}_P + \sum\limits_{\nu\neq\mu}^N A_{\nu \to i}^t\right)^{-1} \\
	\end{split}
\end{equation}




\subsection{Relaxed BP implies the State Equation}
A first result we can get from Relaxed BP is the state equation \eqref{eq:SE}. The key ingredient to obtain that is noticing how at each step the quantities in \eqref{appendix:eq:relaxed_BP} either concentrate or converge in distribution to Gaussian variables in the limit of large $D$.

The first step is to specify how $y^\mu$ is generated. We can do so by defining the index of the generative model $Z^\mu \in \mathbb{R}^{P\times M}$ and it is equivalent $Z_{i}^\mu$ with the weight $\W_j$ removed
\begin{equation}
    Z_{m}^{\mu} = \displaystyle  \frac{1}{\sqrt{D}} \sum\limits_{j=1}^D \x_{jm}^\mu  \W^*_j\,,\qquad\qquad Z_{im}^{\mu} = \displaystyle  \frac{1}{\sqrt{D}} \sum\limits_{j\neq i}^D \x_{jm}^\mu  \W^*_j\,.
\end{equation}
With this definition the labels are $y^{\mu} = g(Z^\mu)$. Now, in the limit of large $D$, $Z^\mu$ and $\omega^\mu$ are going to be jointly Gaussian at each step with the following covariance structure
\begin{equation}\label{appendix:eq:z_w_dist}
\begin{bmatrix}
Z^\mu_{m} \\
\omega^{\mu,t}_{im}
\end{bmatrix}
\sim \mathcal{N}\left(
\begin{bmatrix}
0 \\
0
\end{bmatrix},
\begin{bmatrix}
\mathbb{1}_P & Q^t \\
Q^t & Q^t
\end{bmatrix}
\right)\,,
\end{equation}
where $Q^t$ is the same overlap of the weights estimated at time $t$ and the target 
\begin{equation}
    Q^t \equiv \mathbb{E}\left[\lim_{D\to\infty}\frac{\hat{\W}^t_i{({\W}_i^* )}^\top}{D}\right] = \mathbb{E}\left[\lim_{D\to\infty}\frac{\hat{\W}^t_i{(\hat{\W}^t_i )}^\top}{D}\right]\,,
\end{equation}
where the second equality is because of the Nishimori identity.
The expectation is taken over the target weights $\W^*$ and the input data $\x$.
We can show this by looking at the first few moments of $Z^\mu_{m}$ and $\omega^{\mu,t}_{im}$
\begin{equation}
\begin{split}
    &\mathbb{E}\left[ Z_{im}^{\mu} \right] = 0\,,\qquad\mathbb{E}\left[ Z_{im}^{\mu} (Z_{im}^{\mu})^\top \right] = \mathbb{1}_P\,, \\
    &\mathbb{E}\left[ \omega_{im}^{\mu,t} \right] = 0\,,\qquad\mathbb{E}\left[ \omega_{im}^{\mu,t} (
    \omega_{im}^{\mu,t})^\top \right] = \mathbb{E}\left[\frac{1}{D}\sum_{j\neq i}^D\hat{\W}_{j\to \mu}^t(\hat{\W}_{j\to \mu}^t)^\top\right] \approx \mathbb{E}\left[\frac{1}{D}\sum_{j}^D\hat{\W}_{j}^t(\hat{\W}_{j}^t)^\top \right] = Q^t\,, \\
    &\mathbb{E}\left[ z_{im}^{\mu} (
    \omega_{im}^{\mu,t})^\top \right] = \mathbb{E}\left[\frac{1}{D}\sum_{j\neq i}^D\W^*_j(\hat{\W}_{j\to \mu}^t)^\top \right]\approx \mathbb{E}\left[ \frac{1}{D}\sum_{j}^D\W^*_j(\hat{\W}_{j}^t)^\top \right] = Q^t\,.
\end{split}
\end{equation}
Next, we derive the leading order behavior of $g_{\rm out}$ and $\partial_\omega g_{\rm out}$. \begin{equation}
\begin{split}
&\begin{split}
    g_{\rm out}(y^\mu, \omega_i^{\mu,t}, , V_i^{\mu,t}) &= 
    g_{\rm out}(g(Z^{\mu,t}), \omega_i^{\mu,t}, , V_i^{\mu,t})    \\
    &\approx g_{\rm out}(g(Z^{\mu,t}_i), \omega_i^{\mu,t}, V_i^{\mu,t}) + \frac{1}{\sqrt{D}} \sum\limits_{m=1}^M \x_{im}^\mu  \partial_z \left[g_{\rm out}(g(Z^{\mu,t}_i), \omega_i^{\mu,t}, V_i^{\mu,t})\right]_m \W^*_i\\
\end{split}\,,\\
&\partial_\omega g_{\rm out}(y^\mu, \omega_i^{\mu,t}, V_i^{\mu,t}) \approx 
    \partial_\omega g_{\rm out}(g(Z^{\mu,t}_i), \omega_i^{\mu,t}, V_i^{\mu,t})\,.\\
\end{split}
\end{equation}
The covariances $V^{\mu,t}_{im}$ will concentrate
\begin{equation}
\begin{split}    
    \mathbb{E}\left[V_{im}^{\mu,t} \right] &\approx \frac{1}{D}\sum_{j=1}^D\mathbb{E}\left[\hat{C}_{j\to \mu}^t \right] \\&=\frac{1}{D}\sum_{j=1}^D\mathbb{E}\left[\int_{\mathbb{R}^P} {\rm d} \W_j m_{j\to \mu}^{t-1} (\W_j ) \W_j \W_j^\top\right] - \frac{1}{D}\sum_{j=1}^D\mathbb{E}\left[\hat{\W}_{j\to \mu}^{t-1}(\hat{\W}_{j\to \mu}^{t-1})^\top\right] \\
    &= \mathbb{1}_P - Q^{t-1}\,,
\end{split}
\end{equation}
where in the first expectation on the second line we used Nishimori. We can then decompose the sum of the $B_{\nu\to i}^t$ as follows
\begin{equation}
    \sum\limits_{\nu \ne \mu}^N  B_{\nu \to i}^t \approx \underbrace{\sum_{\mu,m}^{N,M}\frac{\x_{im}^\mu}{\sqrt{D}} g_{\rm out}\left[g(Z_i^{\mu,t}), \omega_i^{\mu,t}, V_i^{\mu,t}\right]_m}_{\xi_i^t} + \underbrace{\frac{1}{D} \sum\limits_{\mu,m}^{N,M} (x_{im}^\mu)^2\left[ \partial_z g_{\rm out}(g(Z^{\mu,t}_i), \omega_i^{\mu,t}, V_i^{\mu,t}\right]_m \W^*_i}_{S^t \W^*_i}\,.
\end{equation}
Here, $\xi_i^t \in \mathbb{R}^P$ is at each step a Gaussian variable with covariance $\hat{Q}^t$
\begin{equation}
    \mathbb{E}[\xi_i^t] = 0\,,\qquad\qquad \mathbb{E}[\xi_i^t (\xi_i^t)^\top] = \hat{Q}^t\,,
\end{equation}
where
\begin{equation}
    \hat{Q}^t \equiv \alpha \mathbb{E}_{Z, \omega}\left[g_{\rm out}(g(Z), \omega^t, \mathbb{1}_P - Q^t)\,g_{\rm out}(g(Z), \omega^t, \mathbb{1}_P - Q^t)^\top\right]\,,
\end{equation}
and the expectation is taken over $Z$, $\omega$ distributed as in \eqref{appendix:eq:z_w_dist}.
It is possible to show with an extremely tedious computation that $S^t (S^t)^\top = \hat{Q}^t$.
Finally, we have the State Equation 
\begin{equation}\label{appendix:eq:state_evolution_iter}
    Q^{t+1} = \mathbb{E}\left[\frac{1}{D}\sum_{i=1}^D \hat{\W}_{i\to \mu}^{t+1}(\hat{\W}_{i\to \mu}^{t+1})^\top \right] = (1+\hat{Q}^t)^{-1}\hat{Q}^t\,,
\end{equation}
which is exactly the same expression as \eqref{eq:SE}. 


\subsection{The fixed points of the State Equation are extremisers of the the variational problem \texorpdfstring{\eqref{eq:freeenergy}}{ref freeenergy}} \label{appendix:derivative_freeenergy}
It is not immediate by just looking at it, but the State Evolution \eqref{eq:SE} at its fixed point describes the extremisers of the functional \eqref{eq:freeenergy}. To show this we first rewrite \eqref{eq:freeenergy} as 
\begin{equation}
\sup_{\hat{Q}\in\mathbb{S}^+_P} \inf_{Q\in\mathbb{S}^+_P} S(Q,\hat{Q})\,,
\end{equation}
where
\begin{equation}
\small
    S(Q, \hat{Q}) =\!\! -\frac{1}{2}{\rm Tr}(Q\hat{Q}) - \frac{1}{2}\log(\mathbb{1}_P + \hat{Q}) + \frac{1}{2}\hat{Q} + \alpha \mathbb{E}_{\xi\sim \mathcal{N}(0,\mathbb{1}_P)} \mathbb{E}_{z^0\sim \mathcal{N}(0,\mathbb{1}_P)} \log\mathcal{Z}_{\rm out}\left(\scriptscriptstyle g(\sqrt{\mathbb{1}_P - Q}Z^0 + \sqrt{Q}\xi), \sqrt{Q}\xi, \mathbb{1}_P - Q\right)\,.
\end{equation}
where we already introduced for convenience $\mathcal{Z}_{\rm out}$ and $g_{\rm out}$, defined as
\begin{equation}
    \mathcal{Z}_{\rm out}(y, \omega, V) = \mathbb{E}_{\{z\sim \mathcal{N}(\omega, V)\}} \left[\delta(y - g(Z))\right]\,,\qquad g_{\rm out}(y, \omega, V) = \frac{\mathbb{E}_{\{z\sim \mathcal{N}(\omega, V)\}}\left[ (Z - \omega)V^{-1}\delta(y - g(Z)) \right]}{\mathbb{E}_{\{z\sim \mathcal{N}(\omega, V)\}}\left[\delta(y - g(Z)) \right]} \,.
\end{equation}
In the following we will use the identities
\begin{equation}
    \partial_\omega \mathcal{Z}_{\rm out} = \mathcal{Z}_{\rm out} g_{\rm out}\,,\qquad \partial_V \mathcal{Z}_{\rm out} = \frac{1}{2}\mathcal{Z}_{\rm out} (\partial_\omega g_{\rm out} + g_{\rm out}\otimes g_{\rm out})\,.
\end{equation}
The extremisers are going to satisfy the following system.
\begin{equation}
\begin{cases}
    \partial_Q S(Q,\hat{Q}) = 0\\
    \partial_{\hat{Q}} S(Q,\hat{Q}) = 0\\
\end{cases}
\end{equation}
or equivalently
\begin{equation}
\begin{cases}
    \hat{Q} = 2\alpha \partial_Q \mathbb{E}_{\xi\sim \mathcal{N}(0,\mathbb{1}_P)} \mathbb{E}_{z^0\sim \mathcal{N}(0,\mathbb{1}_P)} \log\mathcal{Z}_{\rm out}\left(g(\sqrt{\mathbb{1}_P - Q}Z^0 + \sqrt{Q}\xi), \sqrt{Q}\xi, \mathbb{1}_P - Q\right) \\
    Q = (\mathbb{1}_P - \hat{Q})^{-1} - \mathbb{1}_P = (\mathbb{1}_P - \hat{Q})^{-1}\hat{Q} \\
\end{cases}
\end{equation}
The hard part is computing the derivative with respect to $Q$ of the first equation above. We start by adding a dummy variable $y$, which allows us to have the identity
\begin{equation}
\begin{split}
    &\partial_Q \mathbb{E}_{\xi} \mathbb{E}_{z^0} \log\mathcal{Z}_{\rm out}\left(g(\sqrt{\mathbb{1}_P - Q}Z^0 + \sqrt{Q}\xi), \sqrt{Q}\xi, \mathbb{1}_P - Q\right) = \\&\qquad\qquad\qquad\qquad\qquad
    \partial_Q \int {\rm d}y\, \mathbb{E}_{\xi} \mathcal{Z}_{\rm out}\left(y, \sqrt{Q}\xi, \mathbb{1}_P - Q\right)\log\mathcal{Z}_{\rm out}\left(y, \sqrt{Q}\xi, \mathbb{1}_P - Q\right)\,.
\end{split}
\end{equation}
We can now compute the derivative
\begin{equation}
\small
\begin{split}
    &\partial_Q \int {\rm d}y\, \mathbb{E}_{\xi} \mathcal{Z}_{\rm out}\log\mathcal{Z}_{\rm out} \\
    &\qquad=\int {\rm d}y\, \mathbb{E}_{\xi}\left[\partial_Q\mathcal{Z}_{\rm out}\log\mathcal{Z}_{\rm out} + \partial_Q \mathcal{Z}_{\rm out}\right]\\
    &\qquad=\int {\rm d}y\, \mathbb{E}_{\xi}\left[\left(\frac{1}{2}Q^{-1/2}\xi\partial_\omega \mathcal{Z}_{\rm out} - \partial_V \mathcal{Z}_{\rm out}\right)\log\mathcal{Z}_{\rm out} + \frac{1}{2}Q^{-1/2}\xi\partial_\omega \mathcal{Z}_{\rm out} - \partial_V \mathcal{Z}_{\rm out} \right]\\
    &\qquad=\int {\rm d}y\, \mathbb{E}_{\xi}\Big[\left(\frac{1}{2}Q^{-1/2}\xi \mathcal{Z}_{\rm out}g_{\rm out} - \frac{1}{2}\mathcal{Z}_{\rm out} (\partial_\omega g_{\rm out} + g_{\rm out}\otimes g_{\rm out})\right)\log\mathcal{Z}_{\rm out}\notag\\
    &\qquad \qquad \qquad \qquad + \frac{1}{2}Q^{-1/2}\xi\mathcal{Z}_{\rm out}g_{\rm out} - \frac{1}{2}\mathcal{Z}_{\rm out} (\partial_\omega g_{\rm out} + g_{\rm out}\otimes g_{\rm out}) \Big]\\
    &\qquad=\frac{1}{2}\int {\rm d}y\, \mathbb{E}_{\xi}\Big[ \partial_\omega(\mathcal{Z}_{\rm out}g_{\rm out}\log\mathcal{Z}_{\rm out}) - \mathcal{Z}_{\rm out} (\partial_\omega g_{\rm out} + g_{\rm out}\otimes g_{\rm out})\log\mathcal{Z}_{\rm out}\notag\\
    &\qquad \qquad \qquad \qquad + \partial_\omega(\mathcal{Z}_{\rm out}g_{\rm out}) - \mathcal{Z}_{\rm out} (\partial_\omega g_{\rm out} + g_{\rm out}\otimes g_{\rm out}) \Big]\\
    &\qquad=\frac{1}{2}\int {\rm d}y\, \mathbb{E}_{\xi}\left[ \partial_\omega(\mathcal{Z}_{\rm out}g_{\rm out}\log\mathcal{Z}_{\rm out}) - \mathcal{Z}_{\rm out} (\partial_\omega g_{\rm out} + g_{\rm out}\otimes g_{\rm out})\log\mathcal{Z}_{\rm out}  \right]\\
    &\qquad=\frac{1}{2}\int {\rm d}y\, \mathbb{E}_{\xi}\left[ \mathcal{Z}_{\rm out}g_{\rm out}^{\otimes 2} \right]\,,
\end{split}
\end{equation}
where in the $5$-th line we used Stein's lemma. Removing the dummy variable we get 
\begin{equation}
\begin{cases}
    \hat{Q} = \alpha \mathbb{E}_{\xi\sim \mathcal{N}(0,\mathbb{1}_P)} \mathbb{E}_{z^0\sim \mathcal{N}(0,\mathbb{1}_P)} g_{\rm out}\left(g(\sqrt{\mathbb{1}_P - Q}Z^0 + \sqrt{Q}\xi), \sqrt{Q}\xi, \mathbb{1}_P - Q\right)^{\otimes 2} \\
    Q = (\mathbb{1}_P - \hat{Q})^{-1}\hat{Q} \\
\end{cases}
\end{equation}
which is equivalent to the state equation without time indices.

\subsection{From relaxed BP to GAMP}
The relaxed BP equations \eqref{appendix:eq:relaxed_BP} describe a perfectly usable algorithm with $\mathcal{O}(D^2)$ time complexity. We can do better and obtain a linear time iterative scheme.
Let us define the following quantities
\begin{equation}
	\begin{split}
        \omega_{m}^{\mu,t} &= \displaystyle  \frac{1}{\sqrt{D}} \sum\limits_{j=1}^D \x_{jm}^\mu  \hat{\W}_{j\to \mu}^t\vspace{0.3cm} \,,\\
		V_{m}^{\mu,t} &= \displaystyle  \frac{1}{D} \sum\limits_{j=1}^D (\x_{jm}^\mu)^2  \hat{C}_{j \to \mu}^t \vspace{0.3cm}\,, \\
        \hat{\W}_{i}^{t+1} &= \left(\mathbb{1}_P + \sum\limits_{\nu=1}^N A_{\nu \to i}^t\right)^{-1}\left(\sum\limits_{\nu=1}^N  B_{\nu \to i}^t\right)\,,   \vspace{0.3cm}\\
		\hat{C}_{i}^{t+1} &= \left(\mathbb{1}_P + \sum\limits_{\nu=1}^N A_{\nu \to i}^t\right)^{-1} \,,\\
    \end{split}
\end{equation}
which are very similar to their equivalent in relaxed BP. The idea is to have a set of consistent equations on these new quantities. Since here the moments $\omega$, $V$, $\hat{\W}$, $\hat{C}$ have one less index that in relaxed BP, the algorithm will be $\mathcal{O}(D)$ instead of $\mathcal{O}(D^2)$. 

For large $D$ we have that at leading order $\hat{C}_{i \to \mu}^{t+1} \approx \hat{C}_{i}^{t+1}$. 
Similarly, $V_{m}^{\mu}$ is changed minimally.
\begin{equation}
    V_{m}^{\mu,t} \approx \frac{1}{D} \sum\limits_{j=1}^D (\x_{jm}^\mu)^2  \hat{C}_{j}^t\,.
\end{equation}
The other quantities are a bit more delicate. First, notice how $\hat{w}_i$ has a non-negligible correction of order $\mathcal{O}(1/\sqrt{D})$
\begin{equation}
\begin{split}    
    \hat{\W}_{i\to \mu}^{t+1} &\approx \left(\mathbb{1}_P + \sum\limits_{\nu=1}^N A_{\nu \to i}^t\right)^{-1}\left(\sum\limits_{\nu =1}^N  B_{\nu \to i}^t - \sum_{m=1}^M\frac{\x_{im}^\mu}{\sqrt{D}} \left[g_{\rm out}(y^\mu, \omega_i^{\mu,t}, V_i^{\mu,t})\right]_m\right)\\
    &\approx \hat{\W}^{t+1}_i - \hat{C}^{t+1}_i \sum_{m=1}^M\frac{\x_{im}^\mu}{\sqrt{D}} \left[g_{\rm out}(y^\mu, \omega_i^{\mu,t}, V_i^{\mu,t})\right]_m\,. \\
\end{split}
\end{equation}
Similarly, removing one index to $\omega$ and $V$ inside $g_{\rm out}$ produces an extra correction in $\mathcal{O}(1/\sqrt{D})$
\begin{equation}
    g_{\rm out}(y^\mu, \omega_i^{\mu,t},  V_i^{\mu,t}) \approx 
    g_{\rm out}(y^\mu, \omega^{\mu,t}, V^{\mu,t}) - \sum\limits_{m=1}^M \frac{\x_{im}^\mu}{\sqrt{D}} \left[\partial_\omega g_{\rm out}(y^\mu, \omega^{\mu,t}, V^{\mu,t})\right]_{m} \hat{\W}^t_i \,.
\end{equation}
All this results in a correction in the equation for $\omega$, typically called Onsager reaction term
\begin{equation}
    \omega_{m}^{\mu,t} \approx \frac{1}{\sqrt{D}} \sum\limits_{j=1}^D \x_{jm}^\mu  \hat{\W}_{j}^t - \frac{1}{D}\sum_{j = 1}^D (\x_{jm}^\mu)^2 \hat{C}_i^{t-1} \left[g_{\rm out}(y^\mu, 
 \omega^{\mu,t-1}, V^{\mu,t-1})\right]_m\,.
\end{equation}
The moment $B_{\mu \to i}$ similarly receives a correction
\begin{equation}
    B_{\mu \to i}^t \approx \sum_{m=1}^M\frac{\x_{im}^\mu}{\sqrt{D}} \left[g_{\rm out}(y^\mu, \omega^{\mu,t}, V^{\mu,t})\right]_m - \frac{1}{D}\sum\limits_{m=1}^M (\x_{im}^\mu)^2 \left[\partial_\omega g_{\rm out}(y^\mu, \omega^{\mu,t}, V^{\mu,t})\right]_{mm} \hat{\W}^t_i\,.
\end{equation}
It is convenient to define two new quantities $A^t$ and $b^t_i$
\begin{equation}
\begin{split}
    &A^t_i \equiv \sum_{\nu = 1}^N A_{\mu \to i}^t \approx -\frac{1}{D}\sum_{\mu, m}^{N,M} (\x_{im}^\mu)^2 \left[\partial_\omega  g_{\rm out}(y^\mu, \omega^{\mu,t}, V^{\mu,t})\right]_{mm}\,, \\
    &b^t_i \equiv \sum_{\nu = 1}^N B_{\mu \to i}^t \approx
    \sum_{\mu,m}^{N,M}\frac{\x_{im}^\mu}{\sqrt{D}} \left[g_{\rm out}(y^\mu, \omega^{\mu,t}, V^{\mu,t})\right]_m + A^t_i \hat{\W}^t_i\,,\\
\end{split}
\end{equation}
from which we finally obtain
\begin{equation}
\begin{split}
    \hat{\W}_{i}^{t+1} &= \left(\mathbb{1}_P + A^t_i\right)^{-1}b_i^t\,,   \vspace{0.3cm}\\
    \hat{C}_{i}^{t+1} &= \left(\mathbb{1}_P + A^t_i\right)^{-1} \,,\\
\end{split}
\end{equation}
This concludes the derivation. At very large $D$, 
it is possible to introduce $\hat{C}^t$, $V^t$, $A^t$ as the version without indices of $\hat{C}^t_i$, $V^{\mu,t}_{m}$, $A_i^t$
\begin{equation}
\begin{split}
    &\hat{C}^t = \left(\mathbb{1}_P + A^t\right)^{-1} \\
    &A^t = -\frac{\alpha}{N}\sum_{\mu, m}^{N,M} \left[\partial_\omega  g_{\rm out}(y^\mu, \omega^{\mu,t}, V^{\mu,t})\right]_{mm}\\
    &V^t = \hat{C}^t\,.
\end{split}
\end{equation}
This simplified algorithm is described in pseudo-code in Algorithm \ref{alg:AMP} in the main text.


\subsection{AMP with side information on the target}
In this section we generalize GAMP to include some side information on the weights $\W^*$ used to generate the data. Suppose we are provided not only with the dataset $\mathcal{D} = \{\x^\mu,y^\mu\}_{\mu=1}^N$, but also with a side information matrix $S \in \mathbb{R}^{P\times D}$
\begin{equation}
    S = \sqrt{\lambda} \W^* + \sqrt{1-\lambda} \zeta\,,
\end{equation}
where $\zeta\sim \mathcal{N}(0,\mathbb{1})$ is distributed as a standard Gaussian variable and $\lambda>0$ is a small constant. This additional information can be encoded in the prior on the weights $w$
\begin{equation}
    \W_i \sim \frac{1}{(2\pi(1-\lambda))^{P/2}} {\rm exp}\left\{-\frac{\|\W_i - \sqrt{\lambda} S_i\|^2}{2(1-\lambda)}\right\}\,.
\end{equation}
We can now go back and go through the derivation again with this new prior. Eq.  \eqref{mil} becomes
\begin{equation}
\small
\begin{split}
    m_{i\to \mu}^{t+1} (\W_i)
    &= \displaystyle \frac{1}{\mathcal{Z}_{i\to \mu}} \sqrt{\frac{ \prod\limits_{\nu\neq\mu}^N\det(A_{\nu \to i}^t)}{(2\pi(1-\lambda))^{P}}}\,  e^{-\frac{1}{2}\W_{i}\left(\frac{1}{1-\lambda}\mathbb{1}_P + \sum\limits_{\nu\neq\mu}^N A_{\nu \to i}^t\right) \W_{i}^\top + \W_i \left(\frac{\sqrt{\lambda}}{1-\lambda}S_i + \sum\limits_{\nu\neq\mu}^N B_{\nu \to i}^t\right) - \frac{1}{2} \sum\limits_{\nu\neq\mu}^N(B_{\nu \to i}^t)^\top (A_{\nu \to i}^t)^{-1} B_{\nu \to i}^t }\,,
\end{split}
\end{equation}
which gives us the new relaxed BP equations at leading order in small $\lambda$
\begin{equation}
\small
		\hat{\W}_{i\to \mu}^{t+1} = \left((1-\lambda)\mathbb{1}_P + \sum\limits_{\nu\neq\mu}^N A_{\nu \to i}^t\right)^{-1}\left((1-\lambda)\sum\limits_{\nu \ne \mu}^N  B_{\nu \to i}^t + \sqrt{\lambda}  S_i\sum\limits_{\nu\neq\mu}^N A_{\nu \to i}^t\right)\,,   \,
		\hat{C}_{i \to \mu}^{t+1} = \left((1-\lambda)\mathbb{1}_P + \sum\limits_{\nu\neq\mu}^N A_{\nu \to i}^t\right)^{-1} \,,
\end{equation}
and their equivalent in GAMP
\begin{equation}
    \hat{\W}_{i}^{t+1} = \left((1-\lambda)\mathbb{1}_P + A_i^t\right)^{-1}\left((1-\lambda)b_i^t + \sqrt{\lambda}S_iA_i^t \right)\,, \qquad\qquad
    \hat{C}_{i}^{t+1} = \left((1-\lambda)\mathbb{1}_P + A_i^t\right)^{-1} \,.
\end{equation}
The state equation \eqref{appendix:eq:state_evolution_iter} is then slightly modified
\begin{equation}
    Q^{t+1} = (1+(1-\lambda)\hat{Q}^t)^{-1}((1-\lambda)\hat{Q}^t+\lambda)\,.
\end{equation}



\subsection{Sketch of the derivation of the State Evolution using the replica method}
\label{app:replica}
In this section, we provide a sketch of how to characterize the performance of a Bayes Optimal estimation using the replica method, a heuristic procedure inspired by the Statistical Physics of Disordered Systems \cite{gardner1988optimal, mezard1987spin, mezard2009information}. Despite being non-rigorous, the method is very flexible and often provides a result that can be later proven rigorously \cite{barbier2019optimal, Aubin2018}. We invite the interested reader to consult the many reviews available for a more detailed explanation \cite{zdeborova2016statistical, Lauditi_2024}. A closely related characterization of the learning of SMI models using the replica method was first presented in \cite{cui2024highdimensionallearningnarrowneural} for empirical risk minimization problems.


Let us consider a generic sequence multi-index model. We are interested in studying a model with $P$ indices $Z_p$, each describing sequences of length $M$. 
By this we mean that we have a dataset $\mathcal{D} = \{\x^\mu, y^\mu\}_{\mu=1}^N$ of $N$ labeled samples. Each sample $\x^\mu$ is a sequence of $M$ tokens of dimension $D$ taken from a standard Gaussian distribution: 
\begin{equation}
\x^\mu \in \mathbb{R}^{D \times M}\,,\qquad\qquad \x^\mu_{i, m} \sim \mathcal{N}(0,1)\,.
\end{equation}
The indices are obtained by projecting the input data onto a set of $P$ weight vectors $\w_p$ with standard Gaussian entries
\begin{equation}
\w_p \in \mathbb{R}^{D}\,, \qquad\qquad
\w^*_{p,i} \sim \mathcal{N}(0,1)\,, \qquad\qquad Z^{\mu}_p = \frac{\w_p^* x^\mu}{\sqrt{D}}\,.
\end{equation}
In the following, we will call $\W\in\mathbb{R}^{P\times D}$ the weight matrix obtained by stacking the vectors $\w_p$. 
The labels are generated by linking together the indices with thorough a function $g$ such that
\begin{equation}g:\mathbb{R}^{P\times M}\to \mathbb{R}^{M'}\,,\qquad\qquad y_\mu = g(Z^{\mu}_1, ..., Z^{\mu}_P)\,.
\end{equation}
Our results will hold in the high dimensional, proportional limit, that is when $N,D \to \infty$ with a finite ratio $\alpha = N/D$. All the other quantities $M$, $M'$, and $P$ are assumed to be much smaller than $D$.
We are interested in characterizing the performance of a Bayes Optimal (BO) estimator on average over the dataset. 
The derivation starts with Bayes theorem:
\begin{equation}\label{appendix:eq:distribution_posterior}
    {\mathbb{P}}(\W|\mathcal{D}) = \frac{{\mathbb{P}}(\W)\prod_{\mu=1}^N{\mathbb{P}}(y^\mu|\W, \x^\mu)}{{\mathbb{P}}(\mathcal{D})} = \frac{\mathbb{P}(\W)}{\mathcal{Z}} \prod_{\mu=1}^{N}\delta\left(y^\mu - g\left(\frac{\w_1 \x^\mu}{\sqrt{D}},...,\frac{\w_P \x^\mu}{\sqrt{D}}\right)\right) \,.
\end{equation}
where $\mathcal{Z}$ is a normalization constant
\begin{equation}
    \mathcal{Z} = \mathbb{E}_{\W}\left[\prod_{\mu=1}^{N}\delta\left(y^\mu - g\left(\frac{\w_1 \x^\mu}{\sqrt{D}},...,\frac{\w_P \x^\mu}{\sqrt{D}}\right)\right)\right] \,.
\end{equation}
On the other hand, what we are interested in is the BO estimator {\it on average over the randomness of the dataset}, 
To this purpose,e we are resolved to compute the averaged cumulant generating function (or free entropy):
\begin{equation}
    \phi = \lim_{D\to\infty}\frac{1}{D}\mathbb{E}_{\mathcal{D}}\left[\log\mathcal{Z}\right] = \lim_{n\to 0}\frac{\partial}{\partial n} \lim_{D\to\infty}\frac{1}{D}\log \mathbb{E}_{\mathcal{D}}\left[\mathcal{Z}^n\right] \,,
\end{equation} 
where the last equality is called the replica trick: we consider a system with $n\in\mathbb{N}^+$ copies (or replicas) of the original model, extract the leading behaviour at large $D$, then analytically prolong the result to real valued $n$ and take the limit $n\to 0$.
We are thus allowed to simply compute the moments of $\mathcal{Z}$
\begin{align}
    \mathbb{E}_{\mathcal{D}}\left[\mathcal{Z}^n\right] =& \mathbb{E}_{\mathcal{D}, \W^a}\left[ \prod_{\mu,a=1}^{N,n}\delta\left(y_\mu - g\left(\frac{\w_1^a \x^\mu}{\sqrt{D}},...,\frac{\w_P^a \x^\mu}{\sqrt{D}}\right)\right)\right] \,.
\end{align}
To make the expression more symmetric we think of the labels $y^\mu$ as generated by a zeroth replica
\begin{equation}
    y^\mu = g\left(\frac{\w_1^0 \x^\mu}{\sqrt{D}},...,\frac{\w_P^0 \x^\mu}{\sqrt{D}}\right)\,,
\end{equation}
giving us
\begin{align}
    \mathbb{E}_{\mathcal{D}}\mathcal{Z}^n =& \mathbb{E}_{\x^\mu, \W^a} \left[\prod_{\mu,a=1}^{N,n}\delta\left(g\left(\frac{\w_1^0 \x^\mu}{\sqrt{D}},...,\frac{\w_P^0 \x^\mu}{\sqrt{D}}\right) - g\left(\frac{\w_1^a \x^\mu}{\sqrt{D}},...,\frac{\w_P^a \x^\mu}{\sqrt{D}}\right)\right)\right] \,.
\end{align}
Now notice that all the indices are jointly correlated Gaussian variables:
\begin{equation}
    Z^{a,\mu}_{p} = \frac{\w_p^a \x^\mu}{\sqrt{D}}\in\mathbb{R}^M\,,
\qquad\qquad
    \mathbb{E}Z^{a,\mu}_{p} = 0\,, \quad \mathbb{E}Z^{a,\mu}_{p,m}Z^{b,\nu}_{q,l} = Q^{ab}_{pq}\delta_{\mu\nu}\delta_{ml}\,,
\end{equation}
where we introduced the overlaps $Q^{ab}_{pq}$:
\begin{equation}\label{appendix:eq:Q_definition}
    Q^{ab}_{pq} = \frac{1}{D}\sum_{i=1}^D \w^a_{i,p} \w^b_{i,q}\,.    
\end{equation}
Since $Q^{ab}_{pq}$ contains also the covariance of the target weights $\W^*$ we have $Q^{00}_{pq} = \delta_{pq}$.
Our goal will be to show that the overlaps $Q^{ab}_{pq}$ are the only quantity that characterizes the model, and that the partition function can be written a supremum over them.
With these new definitions, we can rewrite the partition function as
\begin{align}
    \mathbb{E}_{\mathcal{D}}\mathcal{Z}^n =& \int {\rm d} Q^{ab}_{pq}\, I_{\rm prior}(\{Q_{pq}^{ab}\}) I_{\rm channel}(\{Q_{pq}^{ab}\})^N\,,
\end{align}
where the quantities $I_{\rm prior}$ and $I_{\rm channel}$ are pieces that depend only on the prior or the channel respectively:
\begin{align}
    &I_{\rm prior} = \mathbb{E}_{\W^a}\left[\prod_{p,q=1,a,b=0}^{P,n}\delta\left(DQ^{ab}_{pq} - \w^a_p \w^b_q\right)\right] \,,\\
    &I_{\rm channel} = \mathbb{E}_{z_p^a \sim \mathcal{N}(0, Q)}\left[\prod_{a=1}^{n}\delta\left(g\left(Z^0_{1},...,Z^0_{P}\right) - g\left(Z^{a}_{1},...,Z^{a}_{p}\right)\right)\right]\,.
\end{align}
In the second expression we introduced a new variable $z_p^a \sim \mathcal{N}(0, Q)$ as the distribution of each of the tokens of $Z_p^a$, that is $z_p^a \sim Z_{pm}^a$.
We can massage these two quantities into compact expressions. We start with $I_{\rm prior}$:
\begin{align}
    I_{\rm prior} =& \mathbb{E}_{\W^a}\left[\prod_{p,q=1,a,b=0}^{P,n}\delta\left(D Q^{ab}_{pq} - \w^a_p (\w^b_q)^\top\right)\right] \\
    =&\mathbb{E}_{\W^a}\left[
    \exp\left\{\sum_{p,q=1,a,b=0}^{P,n}D\hat{Q}^{ab}_{pq}Q^{ab}_{pq} - \sum_{p,q=1,a,b=0}^{P,n}\hat{Q}^{ab}_{pq} \w^a_p (\w^b_q)^\top\right\}\right] \\
    =&\mathbb{E}_{\W^a}\left[
    \exp\left\{\sum_{p,q=1,a,b=0}^{P,n}D\hat{Q}^{ab}_{pq}Q^{ab}_{pq} - \sum_{p,q=1,a,b=0}^{P,n}\hat{Q}^{ab}_{pq} \sum_{i=1}^D \w^a_{i,p} \w^b_{i,q}\right\}\right] \\
    =&\exp\left\{\sum_{p,q=1,a,b=0}^{P,n}D\hat{Q}^{ab}_{pq}Q^{ab}_{pq}\right\}\mathbb{E}_{\W^a}\left[\exp \left\{- \sum_{p,q=1,a,b=0}^{P,n}\hat{Q}^{ab}_{pq} \w^a_{p} \w^b_{q}\right\}\right]^D \\
    =&\exp\left\{\sum_{p,q=1,a,b=0}^{P,n}D\hat{Q}^{ab}_{pq}Q^{ab}_{pq} - \frac{D}{2}\log\det(2\hat{Q}^{ab}_{pq})\right\}\,. 
\end{align}
Since we are only interested in the large $D$ behaviour, we can use the saddle point approximation
\begin{equation}
    \left(Q^{ab}_{pq}\right)^{-1} = 2 \hat{Q}^{ab}_{pq} \,,
\end{equation}
which gives us the simple expression
\begin{equation}
    I_{\rm prior} = \exp\left\{\frac{D}{2} \log \det (Q^{ab}_{pq})\right\}\,.
\end{equation}
This is a good moment to state some properties of the overlap $Q$. First, as we consider $n+1$ identical replicas in the model, $Q^{ab}_{pq}$ needs to be symmetric under exchange of replicas $a \leftrightarrow b$, commonly called the Nishimori condition \cite{Nishimori_1980}. We then make a "replica symmetric" ansatz for the overlap
\begin{equation}
\begin{split}
    &Q^{ab}_{pq} = \delta_{ab}\delta_{pq} + (1-\delta_{ab})Q_{pq}\,. \\
\end{split}        
\end{equation}
which simplifies the equations and allows us to analytically continue the theory. It is similarly reasonable to assume that $Q_{pq}$ are symmetric, positive definite matrices: $Q_{pq} \in \mathcal{S}^+_P$.
After a tedious computation, it is possible to show the following identities
\begin{align}\label{eq:Qidentities}
    &det(Q^{ab}_{pq}) = \det(\mathbb{1}_P - Q)^n\det(\mathbb{1}_P + n Q)\,,    \\
    &(Q^{-1})^{aa} = (\mathbb{1}_P - Q)^{-1}(\mathbb{1}_P + (n-1) Q)(\mathbb{1}_P + n Q)^{-1}\,,\\
    &(Q^{-1})^{ab} = -(\mathbb{1}_P - Q)^{-1}Q(\mathbb{1}_P + n Q)^{-1}\,.
\end{align}
The quantity $I_{\rm prior}$ can now be simplified even further
\begin{align}
    I_{\rm prior} =& \exp\left\{\frac{D}{2}\left[n\log\det(\mathbb{1}_P - Q) + \log\det(\mathbb{1}_P + n Q)\right]\right\} \approx  \exp\left\{\frac{nD}{2}\left[\log\det(\mathbb{1}_P - Q) + {\rm Tr\,} Q\right]\right\} \,,
\end{align}
where the approximation is valid for small enough $n$.

We can now focus on the channel part 
$I_{\rm channel}$ 
of the partition function. We will show how the distribution over the variables $z^{a}_p$ assumes a nice expression in the $n\to 0$ limit: 
\begin{align}
&\frac{{\rm d} z}{\det Q} exp\left\{ -\frac{1}{2}\sum_{p,q=1,a,b=0}^{P,n}z^a_p (Q^{-1})^{ab}_{pq} z^b_q  \right\} \\
&= \frac{{\rm d} z}{\det Q} exp\left\{ -\frac{1}{2}\sum_{p,q=1,a=0}^{P,n}z^a_p (Q^{-1})^{00}_{pq} z^a_q-\frac{1}{2}\sum_{p,q=1,a\neq b}^{P,n}z^a_p (Q^{-1})^{01}_{pq} z^b_q  \right\} \\
&= \frac{{\rm d} z}{\det Q} exp\left\{ -\frac{1}{2}\sum_{p,q=1,a=0}^{P,n}z^a_p [(Q^{-1})^{00}_{pq}-(Q^{-1})^{01}_{pq}] z^a_q-\frac{1}{2}\sum_{p,q=1,a,b=0}^{P,n}z^a_p (Q^{-1})^{01}_{pq} z^b_q  \right\} \\
&= \frac{{\rm d} z {\rm d} \xi}{\det Q} exp\left\{ -\frac{1}{2}\sum_{p,q=1,a=0}^{P,n}z^a_p [(Q^{-1})^{00}_{pq}-(Q^{-1})^{01}_{pq}] z^a_q-i\sum_{p,q=1,a=0}^{P,n}\xi_p \sqrt{(Q^{-1})^{01}_{pq}} z^a_q - \frac{1}{2}\sum_{p=1}^P \xi_p^2 \right\} \label{eq:z_to_zeta}
\end{align}
In the limit $n\to 0$ the identities \eqref{eq:Qidentities} become
\begin{align}
    &det(Q^{ab}_{pq}) \to 1\,,\\
    &(Q^{-1})^{aa} \to \mathbb{1}_P \,,\\
    &(Q^{-1})^{ab} = -(\mathbb{1}_P - Q)^{-1}Q \,.
\end{align}
As we can see in \eqref{eq:z_to_zeta}, it is possible to write a factorized distribution on each replica of $z_p$, while all of them are coupled with an external random variable $\xi$. We will drop the replica index $a$ to have:
\begin{small}
\begin{align}
&\frac{{\rm d} z {\rm d} \xi}{\det Q} exp\left\{ -\frac{1}{2}\sum_{p,q=1,a,b=0}^{P,n}z^a_p (Q^{-1})^{ab}_{pq} z^b_q  \right\}  = {\rm d} \xi e^{- \frac{1}{2}\|\xi\|^2 }\, \left[ {\rm d} z\, exp\left\{ -\frac{1}{2}z^\top (\mathbb{1}_P - Q)^{-1} z + \xi^\top \sqrt{(\mathbb{1}_P - Q)^{-1}Q} z \right\} \right]^{n+1}\,.
\end{align}
\end{small}
We can now do a change of variable $\zeta \to \sqrt{\mathbb{1}_P-Q}\zeta$ and massage the result to obtain
\begin{small}
\begin{align}
    &\frac{{\rm d} z {\rm d} \xi}{\det Q} exp\left\{ -\frac{1}{2}\!\!\!\sum_{p,q=1,a,b=0}^{P,n}z^a_p (Q^{-1})^{ab}_{pq} z^b_q  \right\}  = {\rm d} \xi e^{- \frac{1}{2}\|\xi\|^2 }\!\! \left[ \frac{{\rm d} z}{\sqrt{\det (\mathbb{1}_P - Q)}}\, e^{ -\frac{(z-\sqrt{Q}\xi)^\top (\mathbb{1}_P - Q)^{-1} (z-\sqrt{Q}\xi)}{2}  } \right]^{n+1}\,.
\end{align}
\end{small}
The computation is almost done: define $\mathcal{Z}_{\rm out}(y, \omega, V)$ as
\begin{equation} \label{appendix:eq:Z_out}
    \mathcal{Z}_{\rm out}(y, \omega, V) = \mathbb{E}_{\{z\sim \mathcal{N}(\omega, V)\}} \left[\delta(y - g(Z))\right]
\end{equation}
from which we get for small $n$
\begin{align}
   I_{\rm channel} &= \mathbb{E}_{\xi\sim \mathcal{N}(0,\mathbb{1}_P)} \mathbb{E}_{z^0\sim \mathcal{N}(\sqrt{Q}\xi,\mathbb{1}_P-Q)} \left[\mathbb{E}_{z\sim \mathcal{N}(\sqrt{Q}\xi,\mathbb{1}_P-Q)}[ \delta\left(g(Z^0)-g(Z)\right) ] \right]^n \\
   &= \mathbb{E}_{\xi\sim \mathcal{N}(0,\mathbb{1}_P)} \mathbb{E}_{z^0\sim \mathcal{N}(\sqrt{Q}\xi,\mathbb{1}_P-Q)} \mathcal{Z}_{\rm out}\left(g(Z^0), \sqrt{Q}\xi, \mathbb{1}_P - Q\right)^n \\
   &\approx 1 + n \mathbb{E}_{\xi\sim \mathcal{N}(0,\mathbb{1}_P)} \mathbb{E}_{Z^0\sim \mathcal{N}(\sqrt{Q}\xi,\mathbb{1}_P-Q)} \log\mathcal{Z}_{\rm out}\left(g(Z^0), \sqrt{Q}\xi, \mathbb{1}_P - Q\right) \\
   &\approx \exp{\left\{ n \mathbb{E}_{\xi\sim \mathcal{N}(0,\mathbb{1}_P)} \mathbb{E}_{z^0\sim \mathcal{N}(\sqrt{Q}\xi,\mathbb{1}_P-Q)} \log\mathcal{Z}_{\rm out}\left(g(Z^0), \sqrt{Q}\xi, \mathbb{1}_P - Q\right) \right\}} \,.
\end{align}
Bringing everything together we have
\begin{equation}
\small
    \frac{1}{n D}\log \mathbb{E}_{\mathcal{D}}\mathcal{Z}^n = S(Q) = \frac{1}{2}\left[\log\det(\mathbb{1}_P - Q) + {\rm Tr\,} Q\right] + \alpha \mathbb{E}_{\xi\sim \mathcal{N}(0,\mathbb{1}_P)} \mathbb{E}_{z^0\sim \mathcal{N}(\sqrt{Q}\xi,\mathbb{1}_P-Q)} \log\mathcal{Z}_{\rm out}\left(g(Z^0), \sqrt{Q}\xi, \mathbb{1}_P - Q\right)
\end{equation}
Since we are interested in the large $D$ limit, we can use the saddle point approximation to obtain the free entropy:

\begin{equation}
    \phi = \sup_{Q \in \mathcal{S}_P^+} S(Q) \,.  
\end{equation}
The supremum $\tilde Q$ is achieved for $\partial_Q S(\tilde Q) = 0$.
After a tedious computation it is possible to show that the supremum is achieved at the solution of the following fixed point equation, typically called the state equation:
\begin{equation} \label{appendix:eq:state_equation}
    \tilde Q = F\left[\alpha \mathbb{E}_{Z^0, \xi}\left[g_{\rm out}\left(g(Z^0), \sqrt{\tilde Q}\xi, \mathbb{1}_P - Q\right)\, g_{\rm out}\left(g(Z^0), \sqrt{\tilde Q}\xi, \mathbb{1}_P - \tilde Q\right)^\top\right]\right]
\end{equation}
with $F[X] = (\mathbb{1}_P - X)^{-1}X$, $Z^0 \sim \sqrt{\mathbb{1}_P - \tilde{Q}}Z + \sqrt{\tilde{Q}}\xi$, $Z\sim \mathcal{N}(0,\mathbb{1}_P)$. Lastly, $g_{\rm out}$ is:
\begin{equation} \label{appendix:eq:g_out}
    g_{\rm out}(y, \omega, V) = \frac{\mathbb{E}_{\{z\sim \mathcal{N}(\omega, V)\}}\left[ (Z - \omega)V^{-1}\delta(y - g(Z)) \right]}{\mathbb{E}_{\{z\sim \mathcal{N}(\omega, V)\}}\left[\delta(y - g(Z)) \right]} 
\end{equation}



\section{Specialization to deep attention models} 

The main theoretical results of subsection \ref{subsec:Bayes_SMI} are stated for the broad class of SMI functions \eqref{eq:SMI}. These results are formulated, in particular, in terms of the denoiser function $g_{\rm out}$ \eqref{eq:g_out}, whose form is tributary to the specific architecture considered. In this Appendix, we derive the expression of $g_{\rm out}$ for the special case of rank $P_l=1$ deep attention functions and length $M=2$ data -- thereby specializing the characterizations of Theorem \ref{th:weak_recovery} and Lemma \ref{lem:SE} to this specific architecture. This specialization allows on the one hand to plug the specialized $g_{\rm out}$ in Algorithm \ref{alg:AMP} and thus implement AMP for deep attention networks, and on the other and to reach the tight theoretical predictions for its performance plotted in Fig.\,\ref{fig:toy_model_alpha}.

\subsection{Specializing \texorpdfstring{$g_{\rm out}$}{gout} to specific activations for multi-layer attention in the \texorpdfstring{$M=2$}{M2}, \texorpdfstring{$P_l = 1$}{Pl1} case}

We start by rewriting $g_{\rm out}$ as an integral
\begin{equation}
    [g_{\rm out}(y, \omega, V)]_{lm} = \frac{\int {\rm d}Z\,e^{-\sum\limits_{l,l',m=1}^{L,2}\frac{(Z_{lm}-\omega_{lm})(Z_{l'm}-\omega_{l'm})}{2V_{l l'}}} \sum_{l'=1}^L(Z_{l'm} - \omega_{l'm})V^{-1}_{l'l}\delta\left(y - B_c^L(Z)\right) }{\int {\rm d}Z\,e^{-\sum\limits_{l,l',m=1}^{L,2}\frac{(Z_{lm}-\omega_{lm})(Z_{l'm}-\omega_{l'm})}{2V_{l l'}}}\delta\left(y - B_c^L(Z)\right) } 
\end{equation}

We can now massage the integrals in the numerator and denominator. Let us take the denominator as an example. We can isolate the integral over $Z_{m,L}$
\begin{align}
    &\int {\rm d}Z\,e^{-\sum\limits_{l,l',m=1}^{L,2}\frac{(Z_{lm}-\omega_{lm})(Z_{l'm}-\omega_{l'm})}{2V_{l l'}}}\delta\left(y - B_c^L(Z)\right) \\
    &=\int \left[\prod_{l=1}^{L-1}{\rm d}Z_l\right]\,e^{-\sum\limits_{l,l',m=1}^{L-1,2}\frac{(Z_{lm}-\omega_{lm})(Z_{l'm}-\omega_{l'm})}{2V_{l l'}}}\int {\rm d} Z_L e^{-\sum\limits_{l,m=1}^{L,2}\frac{(Z_{lm}-\omega_{lm})(Z_{Lm}-\omega_{Lm})}{2V_{l L}}} 
    \\
    &\qquad\qquad\qquad\qquad\qquad \times \delta\left(y - \sigma\left(Z_L^\top (B_c^{L-1}(Z_1,...,Z_{L-1})^\top B_c^{L-1}(Z_1,...,Z_{L-1}) Z_L\right)\right) \\
    &=\int \left[\prod_{l=1}^{L-1}{\rm d}Z_l\right]\,\frac{e^{-\sum\limits_{l,l',m=1}^{L-1,2}\frac{(Z_{lm}-\omega_{lm})(Z_{l'm}-\omega_{l'm})}{2V_{l l'}}}}{\left|\det B_c^{L-1}(Z_1,...,Z_{L-1})\right|}\int {\rm d} Z_L e^{-\sum\limits_{l,m=1}^{L,2}\frac{(Z_{lm}-\omega_{lm})(Z_{Lm}-\omega_{Lm})}{2V_{l L}}} \delta\left(y - \sigma\left(Z_L^\top Z_L\right)\right)\\
    &=\int \left[\prod_{l=1}^{L-1}{\rm d}Z_l\right]\,\frac{e^{-\sum\limits_{l,l',m=1}^{L-1,2}\frac{(Z_{lm}-\omega_{lm})(Z_{l'm}-\omega_{l'm})}{2V_{l l'}}}}{\left|\det B_c^{L-1}(Z_1,...,Z_{L-1})\right|}I_\sigma
\end{align}
Intuitively, we mapped an $L\times M$ dimensional integral with a Dirac delta as an argument into a $(L-1)\times M$ dimensional one, which is much more easily integrated numerically. $I_\sigma$ has to be computed analytically. We give two examples with linear and softmax activation in the following two subsections. A similar manipulation can be realized for the numerator. Putting the two procedures together yields, for the case $L=2$ discussed in the main text:
\begin{corollary}
    In the case $M=2$, $P_1=P_2=1$ and softmax activation, the output filter \eqref{eq:g_out} 
    becomes
    \begin{equation} \label{eq:g_out_toy}
        \begin{split}
            &[g_{\rm out}(Y, \omega, V)]_{lm} = \frac{
                \sum\limits_{Z_2}\int {\rm d}Z_1 \,\frac{e^{-\!\!\!\sum\limits_{l,l',m=1}^{2}\!\!\!\!\frac{(Z_{lm}-\omega_{lm})(Z_{l'm}-\omega_{l'm})}{2V_{l l'}}} \sum\limits_{l'=1}^2(Z_{l'm}-\omega_{l'm}) V^{-1}_{l' l}
                }{
                \left|\det B_c^{1}(Z_1)\right|}
            }{
                \sum\limits_{Z_2}\int {\rm d}Z_1 \,\frac{e^{-\sum\limits_{l,l',m=1}^{2}\!\!\!\!\!\!\frac{(Z_{lm}-\omega_{lm})(Z_{l'm}-\omega_{l'm})}{2V_{l l'}}}}{\left|\det B_c^{1}(Z_1)\right|} 
            }        
        \end{split}
    \end{equation}
    where the sum over $Z_2$ is intended over the solutions of the two equations $B^{1}_{c}(Z_1)Z_2 = \pm\gamma(Y_{1,2},Y_{2,1})$, with $\gamma(x,y)$ defined as
    \begin{equation}
        \gamma(x,y) = \frac{1}{\sqrt{\log \left(\left(x^{-1}-1\right) \left(y^{-1}-1\right)\right)}} \begin{bmatrix}
            -\log \left(x^{-1}-1\right) \\
            \log \left(y^{-1}-1\right)
        \end{bmatrix}
    \end{equation}
\end{corollary}

\subsubsection{Computing \texorpdfstring{$I_\sigma$}{Isigma} for linear activation and \texorpdfstring{$M=2$}{M2}}
We state again $I_\sigma$ in a compact form
\begin{equation}
    I_{\sigma} = \int {\rm d} z\, \delta(y - \sigma(z z^\top)) F[z]
\end{equation}
where $F$ is a generic function of $z$. It is convenient to write all the matrices explicitly
\begin{equation}
    I_{\rm linear} = \int {\rm d} z_1\, {\rm d} z_2\, \delta\left( \begin{bmatrix}
        y_{11} & y_{12} \\
        y_{21} & y_{22} \\
    \end{bmatrix} - \begin{bmatrix}
        z_1^2 & z_1 z_2 \\
        z_1 z_2 & z_2^2 \\
    \end{bmatrix}\right) F[z_1, z_2]
\end{equation}
We can see that $y_{12} = y_{12}$, and also that $y_{11}, y_{22}>0$. On the other hand, the information on $\{y_{11}, y_{12}, y_{22}\}$ is redundant, as one can use either just $\{y_{11}, y_{12}\}$ or $\{y_{22}, y_{12}\}$. We will choose the former, and write the integral as
\begin{equation}
    I_{\rm linear} = \int {\rm d} z_1\, {\rm d} z_2\, \delta(y_{11} - z_1^2)\delta(y_{12} - z_1 z_2)  F[z_1, z_2]\,
\end{equation}
which we can easily solve obtaining
\begin{equation}
    I_{\rm linear} = \frac{F\left[-\sqrt{y_{11}}, - \frac{y_{12}}{\sqrt{y_{11}}}\right] + F\left[\sqrt{y_{11}}, \frac{y_{12}}{\sqrt{y_{11}}}\right]}{2 |y_{11}|}
\end{equation}


\subsubsection{Computing \texorpdfstring{$I_\sigma$}{Isigma} for softmax activation and \texorpdfstring{$M=2$}{M2}}
Let us state again the problem
\begin{equation}
    I_{\rm softmax} = \int {\rm d} z_1\, {\rm d} z_2\, \delta\left( \begin{bmatrix}
        y_{11} & y_{12} \\
        y_{21} & y_{22} \\
    \end{bmatrix} - {\rm softmax}\begin{bmatrix}
        z_1^2 & z_1 z_2 \\
        z_1 z_2 & z_2^2 \\
    \end{bmatrix}\right) F[z_1, z_2]
\end{equation}
This case is more involved. Let us write out explicitly the softmax
\begin{equation}
    {\rm softmax}\begin{bmatrix}
        z_1^2 & z_1 z_2 \\
        z_1 z_2 & z_2^2 \\
    \end{bmatrix} = \begin{bmatrix}
        \frac{e^{z_1^2}}{e^{z_1^2}+e^{z_1 z_2}} & \frac{e^{z_1 z_2}}{e^{z_1^2}+e^{z_1 z_2}} \\
        \frac{e^{z_1 z_2}}{e^{z_2^2}+e^{z_1 z_2}} & \frac{e^{z_2^2}}{e^{z_2^2}+e^{z_1 z_2}} \\
    \end{bmatrix}
\end{equation}
This tell us that $y_{11} + y_{12} = y_{22} + y_{21} = 1$. A less immediate identity is that $y_{12} + y_{21} \leq 1$, as we can see from these simple manipulations
\begin{equation}
\begin{split}
    &y_{12} + y_{21} \leq 1\\
    &\frac{e^{z_1 z_2}}{e^{z_1^2}+e^{z_1 z_2}} + \frac{e^{z_1 z_2}}{e^{z_2^2}+e^{z_1 z_2}} \leq 1 \\
    &e^{2z_1 z_2} \leq e^{z_1^2+z_2^2}\\
    &2 z_1 z_2 \leq z_1^2 + z_2^2
\end{split}
\end{equation}
The last inequality is always true because of the arithmetic and geometric mean inequality. 
We choose to keep only the information in $\{y_{12}, y_{21}\}$. We will thus get

\begin{equation}
    I_{\rm softmax} = \int {\rm d} z_1\, {\rm d} z_2\, \delta\left(y_{12} - \frac{e^{z_1 z_2}}{e^{z_1^2}+e^{z_1 z_2}}\right)\delta\left(y_{21} - \frac{e^{z_2 z_2}}{e^{z_1^2}+e^{z_1 z_2}}\right)  F[z_1, z_2]\,
\end{equation}
Let us try to solve the following system of equations:
\begin{equation}
\begin{cases}
y_{12} &= \frac{e^{z_1 z_2}}{e^{z_1^2}+e^{z_1 z_2}} \\
y_{21} &= \frac{e^{z_1 z_2}}{e^{z_2^2}+e^{z_1 z_2}} \\
\end{cases}
\end{equation}
which can be rewritten as
\begin{equation}
\begin{cases}
\log\left(\frac{1}{y_{12}} - 1\right) &= z_1^2 - z_1 z_2 \\
\log\left(\frac{1}{y_{21}} - 1\right) &= z_2^2 - z_1 z_2 \\
\end{cases}
\end{equation}
This is a simple system of two second-degree equations in $z_1$, $z_2$. The solutions are
\begin{equation}
\begin{cases}
z_1 & = \pm \frac{\log\left(\frac{1}{y_{12}} - 1\right)}{\sqrt{\log\left(\frac{1}{y_{12}} - 1\right)\log\left(\frac{1}{y_{21}} - 1\right)}} \\
z_2 &= \mp \frac{\log\left(\frac{1}{y_{21}} - 1\right)}{\sqrt{\log\left(\frac{1}{y_{12}} - 1\right)\log\left(\frac{1}{y_{21}} - 1\right)}} \\
\end{cases}
\end{equation}
This defines a mapping $(y_{12}, y_{21}) \mapsto (z_1, z_2) = \pm\gamma(y_{12}, y_{21})$. The Jacobian of the transformation is
\begin{equation}
    |\det J_\gamma| = \frac{1}{2\left|2y_{12}y_{21}(y_{12}-1)(y_{21}-1)\log\left(\frac{y_{12}y_{21}}{(y_{12}-1)(y_{21}-1)}\right)\right|}
\end{equation}
Notice that all the quantities above are real-valued because $y_{12} + y_{21} \leq 1$.

Finally, we obtain
\begin{equation}
    I_{\rm softmax} = \frac{F\left[-\gamma(y_{12},y_{21})\right] + F\left[\gamma(y_{12},y_{21})\right]}{2\left|y_{12}y_{21}(y_{12}-1)(y_{21}-1)\log\left(\frac{y_{12}y_{21}}{(y_{12}-1)(y_{21}-1)}\right)\right|}
\end{equation}


\subsection{Weak recovery thresholds for two layers of rank one attention}\label{appendix:weak_recovery}


\begin{figure}[h!] 
\vskip 0.2in
\begin{center}
\includegraphics[scale=1.]{img/Figure_5.pdf}
\caption{ {\bf Left}: Overlap of the first layer $Q_{11}$ in two layers of rank one attention with skip connection $c=1$ and sample complexity $\alpha=1$ as a function of the overlap of the second layer $Q_{22}$, kept fixed during the iteration. We can see that unless $Q_{22}$ is almost one, $Q_{11}$ is not learned.
}\label{fig:staircase}
\end{center}
\vskip -0.2in

\end{figure}

In this section we describe in detail how to obtain the weak recovery thresholds for the model in Section \ref{section:toy_model}. We start by computing $G \coloneqq \partial_\omega {g}_{\rm out}(Y, 0, \mathbb{1}_P)$. We can write it compactly as

\begin{equation}
    G_{ll'mm'} = \frac{\int {\rm d}Z\,e^{-\frac{1}{2}\sum\limits_{l,l',m=1}^{2,2}Z_{lm}Z_{l'm}} (Z_{lm}Z_{l'm'} - \delta_{ll'}) \delta(y - g(Z))}
    {\int {\rm d}Z\,e^{-\frac{1}{2}\sum\limits_{l,l',m=1}^{2,2}Z_{lm}Z_{l'm}} \delta(y - g(Z))}
\end{equation}
Since $g$ is an even function, $G_{ll'mm'}$ is zero if $l\neq l'$.
This implies that the operator $\mathcal{F}$ in \eqref{eq:F_def} is diagonal, hence $\alpha_1$ is simply
\begin{equation}
        \frac{1}{\alpha_1} = \sup_{\mathcal{X}\in\mathbb{S}_+^{P_l},\, \|\mathcal{X}\|=1} \|\mathcal{F}(\mathcal{X})\| = \max_{l}
        \sum_{m,m'=1}^2\mathbb{E}_Y [(G_{llmm'})^2]
\end{equation}
Implementing the formula above reveals that for these models the maximum is achieved for $l=2$.
Computing $\alpha_2$ is harder, but we can make the simplifying assumption that for $\alpha>\alpha_2$ the second layer is perfectly recovered, so \begin{equation}
    Q = \left(
    \begin{array}{cc}
        0  & 0\\
       0 &  1
    \end{array}\right)
\end{equation}
On the other hand, if the second layer is perfectly recovered, one has to simply compute a linear perturbation on $Q$ along $Q_{11}$, giving us
\begin{equation}
    \frac{1}{\alpha_2} = \sum_{m,m'=1}^2\mathbb{E}_Y [(G_{11mm'})^2]
\end{equation}
We would like to stress that having learned the second layer is crucial for the learning of the first, and it's central to the grand staircase phenomenon. We can see this also in the model of attention. Suppose we run AMP and choose not to keep the weights in the second layer at a fixed overlap $Q_22^*$ with the target: with all other parameters fixed, the value of the overlap of the second layer at convergence depends strongly on $Q_22^*$, as shown in Figure \ref{fig:staircase} for $\alpha = 1$. In this particular case AMP would learn the second layer almost perfectly and achieve a good overlap in the first one, as we can see in Figure \ref{fig:toy_model_alpha} in the main. On the other hand, if we prevent the algorithm to learn the second layer, then also the first one is not learned. 
\subsection{Language and hardware specific details}\label{appendix:numerics}

Here we discuss the coding details of the implementation of GAMP and the state evolution that we use for the figures. The interested reader can look at the code repository at \url{https://github.com/SPOC-group/SequenceIndexModels}. A common ingredient is the denoising function $g_{\rm out}(Y,\omega,V)$ in \eqref{eq:g_out_toy}, which is just a two-dimensional integral over a smooth function. We perform the integral by quadrature using the function \texttt{dblquad} in Scipy on the set $[-3,3]\times[-3,3]$. Additionally, we regularize the integrand by adding $10^{-3}$ to the diagonal of $V$ and $10^{-6}$ to the square root argument in \eqref{eq:g_out_toy}.
For GAMP we need also to compute $\partial_\omega g_{\rm out}$, which we do by centered finite differences computed on two points at distance $10^{-5}$. 
For state evolution we have to perform and expectation over $Y$, $\xi$ as in \eqref{eq:SE}. We choose to do so using a Monte Carlo method, that is by taking $1440$ samples of $Y$ and $\xi$ and computing $g_{\rm out}$ for each of them. At each iteration we symmetrise the overlap $Q$.

For each run of GAMP or the state evolution we use $2$ Intel Xeon Platinum 8360Y processors and approximately $290$ GB of RAM.
In Fig.~\ref{fig:toy_model_alpha} (left) we smoothen the prediction error and the overlap theory curves by interpolating with splines above the critical threshold.

\section{Details on the experiments on real datasets}\label{appendix:real_data}

In this Appendix, we provide further details on the numerical experiment presented in Fig.\,\ref{fig:real_model}, namely the training of a simple transformer model on the TREC \cite{hovy-etal-2001-toward, li-roth-2002-learning} classification task. 

\paragraph{Data -- } The TREC dataset \cite{hovy-etal-2001-toward, li-roth-2002-learning} contains $5500$ labeled questions of average length $\approx 10$, divided in $6$ classes, classifying the type of the question, e.g. whether it bears on a concept, human, or numerical value. The vocabulary size is 8700. The data is pre-processed using the uncased base BERT model \cite{devlin2019bertpretrainingdeepbidirectional} and padded into sequences of length $M=32$.

\paragraph{Architecture -- } We consider a small transformer model with two consecutive self-attention layers, with tied key and query matrices, and a fully-connected readout layer. More precisely, the architecture comprises
\begin{itemize}
    \item \textbf{Embedding layer}: embeds the input sequence in dimension $D=128$
    \item \textbf{Attention layers}: two consecutive self-attention layers 
    $$
    \x_l=\x_{l-1}\,\mathrm{softmax}\left[
    \frac{\x_{l-1}^\top \w_l\w_l^\top \x_{l-1}}{D}
    \right]
    $$
    with tied weights $\w_l\in \R^{D\times P_l}$, and hidden dimension $P_1=P_2=64$.
    \item \textbf{Pooling}: the sequence after the attention layers $\x_2 \in\R^{D\times M}$ is averaged over its second dimension.
    \item \textbf{Fully-connected readout:} linear projection from dimension $D$ to $6$ (number of classes).
\end{itemize}
The architecture is then trained over the cross-entropy loss, using the \texttt{Pytorch} \cite{paszke2017automatic} implementation of the \texttt{AdamW} \cite{loshchilov2017decoupled} optimizer, using learning rate $\eta=5\times 10^{-5}$ and batch size $16$. The weights $\w_l$ at initialization are sampled from a Gaussian distribution with zero mean and standard deviation $10^{-4}$.


\end{document}
