We outline in \cref{tab:notations} a scheme of the notation used throughout the paper.

\subsection{Proof of \Cref{thm:eps-opt-preserve}}
\label{proof:thm-eps-opt-preserve}

\begin{proof}
Let \(m := F(\theta^*;D)\) and \(\hat{m} := F(\hat{\theta};\bar{D})\).  
We must show that \(\lvert\,m - \hat{m}\rvert \le \epsilon\).

\begin{enumerate}
\item 
By \(\epsilon\)-stability, for \emph{all} \(\theta\in\Theta\):
\[
  \bigl\lvert F(\theta;D) \;-\; F(\theta;\bar{D})\bigr\rvert
  \;\le\;
  \epsilon.
\]
In particular, for \(\theta = \theta^*\),
\[
  \bigl\lvert F(\theta^*;D) \;-\; F(\theta^*;\bar{D})\bigr\rvert
  \;\le\;
  \epsilon.
\]
Hence
\[
  F(\theta^*;\bar{D}) 
  \;\ge\;
  F(\theta^*;D)\;-\;\epsilon
\]
and
\[
  F(\theta^*;\bar{D}) 
  \;\le\;
  F(\theta^*;D)\;+\;\epsilon.
\]

\item 
Since \(\hat{\theta}\) is the minimizer of \(F(\cdot;\bar{D})\), we have
\[
  F(\hat{\theta};\bar{D}) 
  \;\le\; 
  F(\theta^*;\bar{D}).
\]
Because \(\theta^*\) is the minimizer of \(F(\cdot;D)\), 
\[
  F(\hat{\theta};D) 
  \;\ge\; 
  F(\theta^*;D).
\]

\item 
To bound \(\hat{m}-m\), we can add and subtract $F(\theta^*;\bar{D})$ to have
\begin{align*}
  \hat{m}-m
  &\;=\;
  \Bigl(F(\hat{\theta};\bar{D}) \;-\; F(\theta^*;\bar{D})\Bigr) \\
  &\;+\;
  \Bigl(F(\theta^*;\bar{D}) \;-\; F(\theta^*;D)\Bigr).
\end{align*}
The first term is \(\le 0\) (since \(\hat{\theta}\) is a minimizer on \(\bar{D}\)), 
and the second term is \(\le \epsilon\).  
Hence
\[
  \hat{m}-m
  \;\le\;
  0 + \epsilon
  \;=\;
  \epsilon.
\]

\item 
Analogously, to bound \(m-\hat{m}\), we can rewrite
\begin{align*}
  m-\hat{m}
  &\;=\;
  \Bigl(F(\theta^*;D) \;-\; F(\hat{\theta};D)\Bigr) \\
  &\;+\;
  \Bigl(F(\hat{\theta};D) \;-\; F(\hat{\theta};\bar{D})\Bigr).
\end{align*}
The first term is \(\le 0\) (since \(\theta^*\) is a minimizer on \(D\)), 
and the second term is \(\le \epsilon\).  
Thus,
\[
  m-\hat{m}
  \;\le\;
  0 + \epsilon
  \;=\;
  \epsilon.
\]

\item 
Combining these inequalities:
\[
  -\epsilon 
  \;\le\; 
  \hat{m}-m 
  \;\le\; 
  \epsilon
  \quad\Longrightarrow\quad
  \lvert
    m - \hat{m}
  \rvert
  \;\le\;
  \epsilon.
\]
Hence 
\(\bigl\lvert F(\theta^*;D) - F(\hat{\theta};\bar{D})\bigr\rvert\le\epsilon\), 
completing the proof.
\end{enumerate}
\end{proof}

\subsection{Proof of \Cref{thm:expected-eps-stability}}
\label{proof:thm-expected-eps-stability}
\begin{proof}
By hypothesis, for every \(\theta\in\Theta\),
\[
  \mathbb{E}_{\bar{D}}\!\Bigl[
    \bigl\lvert F(\theta;D) \;-\; F(\theta;\bar{D})\bigr\rvert
  \Bigr]
  \;\le\;\epsilon.
\]
Using Jensen's inequality for the absolute value,
\begin{align*}
  &\bigl\lvert
    \mathbb{E}_{\bar{D}}\!\bigl[
      F(\theta;D) \;-\; F(\theta;\bar{D})
    \bigr]
  \bigr\rvert \\
  &\;\le\;
  \mathbb{E}_{\bar{D}}\!\Bigl[
    \bigl\lvert
      F(\theta;D) \;-\; F(\theta;\bar{D})
    \bigr\rvert
  \Bigr] 
  \;\le\;\epsilon.
\end{align*}
Hence,
\[
  -\epsilon 
  \;\le\;
  \mathbb{E}_{\bar{D}}\!\bigl[
    F(\theta;\bar{D}) - F(\theta;D)
  \bigr]
  \;\le\;
  \epsilon
\]
for each fixed $\theta$. It thus follows that
\[
  \mathbb{E}_{\bar{D}}\!\bigl[F(\theta;\bar{D})\bigr]
  \;\le\;
  F(\theta;D) + \epsilon
\]
and
\[
  \mathbb{E}_{\bar{D}}\!\bigl[F(\theta;\bar{D})\bigr]
  \;\ge\;
  F(\theta;D) - \epsilon.
\]
Consequently,
\[
  \min_{\theta\in\Theta}\,
  \mathbb{E}_{\bar{D}}\!\bigl[F(\theta;\bar{D})\bigr]
  \;\le\;
  \min_{\theta\in\Theta}
  \bigl[
    F(\theta;D) + \epsilon
  \bigr]
  \;=\;
  m^* + \epsilon,
\]
where \(m^* := \min_{\theta\in\Theta} F(\theta;D)\).  
Meanwhile, by a min-versus-expectation (Jensen-type) inequality,
\[
  \mathbb{E}_{\bar{D}}\!\bigl[\min_{\theta\in\Theta}\,
  F(\theta;\bar{D})\bigr]
  \;\ge\;
  \min_{\theta\in\Theta}
  \mathbb{E}_{\bar{D}}\!\bigl[
    F(\theta;\bar{D})
  \bigr].
\]
Hence,
\begin{align*}
  \mathbb{E}_{\bar{D}}\!\bigl[\widehat{m}(\bar{D})\bigr]
  &\;=\;
  \mathbb{E}_{\bar{D}}\!\Bigl[\min_{\theta\in\Theta}
    F(\theta;\bar{D})
  \Bigr]
  \\ 
  &\;\ge\; 
  \min_{\theta\in\Theta}
  \mathbb{E}_{\bar{D}}\!\bigl[
    F(\theta;\bar{D})
  \bigr]
  \;\ge\;
  m^* - \epsilon.
\end{align*}
Combining these two bounds results in
\[
  m^* - \epsilon
  \;\le\;
  \mathbb{E}_{\bar{D}}\!\bigl[\widehat{m}(\bar{D})\bigr]
  \;\le\;
  m^* + \epsilon
\]
and, therefore,
\[
\Bigl|
    m^*
    \;-\;
    \mathbb{E}_{\bar{D}}\!\bigl[\widehat{m}(\bar{D})\bigr]
  \Bigr|
  \;\le\;\epsilon.
\]
\end{proof}

\subsection{Proof of \Cref{prop:estimator-unbiased}} \label{proof:prop-estimator-unbiased}
%
\begin{proof}
We must show that 
\[
\bigl\lvert
  \mathbb{E}\!\bigl[\hat{Z}_{jl} \,\bigm|\,
    Y_{i_0l},\dots,Y_{i_kl}
  \bigr]
  \;-\;
  \mathbb{E}\!\bigl[Z_{jl} \,\bigm|\,
    Y_{i_0l},\dots,Y_{i_kl}
  \bigr]
\bigr\rvert
\;\to\;0
\]
in probability as
$|\hat{I}|\to\infty$.
Under the assumptions of the proposition (including linear inheritance of abilities, 
\(\hat{\lambda}\to\lambda\) in probability, and bounded \(\|\alpha_i\|\)), 
we may bound this difference as follows:
\begin{align*}
&\bigl\lvert
  \mathbb{E}\!\bigl[\hat{Z}_{jl} \, \bigm|\,
    Y_{i_0l}, \ldots, Y_{i_kl}
  \bigr]
  \;-\;
  \mathbb{E}\!\bigl[Z_{jl} \,\bigm|\,
    Y_{i_0l}, \ldots, Y_{i_kl}
  \bigr]
\bigr\rvert\\
&\;\le\;
\frac{1 - \hat{\lambda}}{|I_j \setminus \hat{I}_j|}
\sum_{i \in I_j \setminus \hat{I}_j}
\Bigl|
  \sigma\!\bigl((\hat{\lambda}_1\theta_{l_1}
    +\hat{\lambda}_2\theta_{l_2})^\top \alpha_i 
    \;-\;\beta_i\bigr)\\
  &\;-\;
  \sigma\!\bigl(\theta_{l_m}^\top \alpha_i 
    \;-\;\beta_i\bigr)
\Bigr|.
\end{align*}
Since \(\sigma\) is \(1/4\)-Lipschitz on \(\mathbb{R}\), we have
\begin{align*}
    &= \;\le\;
    \frac{1}{|I_j|}
    \sum_{i \in \hat{I}_j}
    \Bigl|
      \bigl(
        (\hat{\lambda}_1\theta_{l_1}
         +\hat{\lambda}_2\theta_{l_2})
        \;-\;
        \theta_{l_m}
      \bigr)^\top \alpha_i
    \Bigr| \\ 
    &\;\le\;
    \frac{1}{|I_j|}
    \sum_{i \in \hat{I}_j}
    \|\alpha_i\|_2
    \,
    \|(\hat{\lambda}_1\theta_{l_1}
        +\hat{\lambda}_2\theta_{l_2})
      \;-\;\theta_{l_m}\|_2.
\end{align*}
Since \(\sup_{i\in I_j}\|\alpha_i\|_2 \le c\), it follows that
\[
\;\le\;
c\,
\bigl\|
  (\hat{\lambda}_1-\lambda_1)\,\theta_{l_1}
  \;+\;
  (\hat{\lambda}_2-\lambda_2)\,\theta_{l_2}
\bigr\|_2
\;\to\;0
\]
in probability as $|\hat{I}|\to\infty$.
(The last step uses \(\hat{\lambda}\to\lambda\) in probability, with 
\(\theta_{l_1}, \theta_{l_2}\) fixed in \(\mathbb{R}^d\).)  
Hence \(\hat{Z}_{jl}\) converges in probability to \(Z_{jl}\), completing the proof.
\end{proof}

\subsection{Proof of \Cref{thm:eps-opt-preserve}}
\label{proof:eps-opt-preserve}
\begin{proof}
By \Cref{prop:estimator-unbiased}, \(\hat{Z}^{\mathrm{mp\text{-}IRT}}\) 
becomes arbitrarily close (in probability) to \(Z\) as 
\(\lvert \bar{D}\rvert\to\infty\).  Under standard regularity conditions, this 
implies 
\[
  \bigl|
    Z(\theta;D)
    \;-\;
    \hat{Z}^{\mathrm{mp\text{-}IRT}}(\theta;\bar{D})
  \bigr|
  \;\le\;\epsilon
\]
in expectation, for all sufficiently large $\lvert \bar{D}\rvert$, hence \(\hat{Z}^{\mathrm{mp\text{-}IRT}}\) is \(\epsilon\)-stable in expectation.  
Applying \Cref{thm:expected-eps-stability} completes the argument.
\end{proof}
