Our method \approach speeds up evolutionary model merging by reducing the computational cost of fitness evaluation. It achieves this by shrinking the fitness evaluation dataset and using IRT-based performance estimators to maintain full-dataset accuracy from subset evaluations. \Cref{fig:teaser} shows an overview of our method (\cref{alg:merge3}).

\subsection{Extract \& Estimate} \label{sec: extract_and_estimate}
%
Evaluating the fitness function involves generating and assessing answers for each data sample, repeated across all models in the population at every evolutionary step. Given the computational demands of evolutionary algorithms and LLMs, this process is highly intensive. To mitigate this, we reduce the dataset $D$ to a smaller subset $\bar{D} \subset D$ with $|\bar{D}| \ll |D|$. After exploring various subsampling strategies, we found uniform random sampling as effective as more complex methods (see \cref{app:add-exp:extract-ste}) and adopted it for simplicity. Since dataset reduction is not our main focus, we leave further optimizations for future work.

Reducing the dataset speeds up evaluation but does not guarantee identical results -- particularly when the subset is significantly smaller, as in our case. To bridge this gap, we build an IRT-based estimator that adjusts for this discrepancy, effectively estimating performance to reflect full-dataset results \cite{lord1968statistical, tinybenchmarks}.

\label{sec:estimate}

\paragraph{IRT model.}\label{sec:irt}
%
We first define an {estimator} to assess each endpoint model's inherent abilities, derived from the latents of a Bayesian network. This ensures that merging preserves individual model strengths. In the Evolve step (\S \ref{sec:evolve}), the estimated latent abilities are fed to a {\em performance} estimator to compute the final fitness.

To estimate LLM abilities, we build on \citet{tinybenchmarks}, who applied IRT to evaluate LLM performance; however, while they used IRT for benchmarking, we extend it to estimate inherent abilities relevant for model merging, and explicitly use them to guide merging in the Evolve step.

In IRT, latent variables ($\gamma$) represent a model's underlying abilities, while manifest variables ($Y$) indicate response correctness. The framework models the probability of a correct response based on model abilities and item characteristics (e.g., difficulty).

IRT defines this probability as:
\begin{equation}
     \mathbb{P}(Y_{im} = 1 \, | \, \gamma_m, \alpha_i, \beta_i) = \frac{1}{1 + \exp(-\alpha_i^\top \gamma_m + \beta_i)} \,
    \label{eq:irt}
\end{equation}
%
Here, ${\gamma_m \in \mathbb{R}^d}$ represents model $m$'s latent abilities, $\alpha_i \in \mathbb{R}^d$ defines the ability dimensions needed to answer example $i$, and $\beta_i$ denotes its difficulty. 
%
A model is more likely to answer correctly when its abilities ($\gamma_m$) align with the example’s required traits ($\alpha_i$) and less likely when the difficulty ($\beta_i$) is higher.
%
$Y_{im}$ is a binary variable indicating whether model $m$ correctly predicts example $i$ (1 if correct, 0 otherwise).

Crucially, this approach estimates a model’s likelihood of answering correctly {\em without directly analyzing the example's content}, relying solely on the estimated IRT parameters ($\gamma_m, \alpha_i, \beta_i$).

\paragraph{Fitting.}\label{sec:fit_irt}
We use variational inference to efficiently estimate both example-specific $(\alpha_i, \beta_i)$ and model-specific ($\gamma_m$) parameters within a hierarchical Bayesian model \citep{lalor2023py}, initialized as detailed in appendix \ref{app:fitting_details}.
Following \citet{tinybenchmarks}, we estimate $\alpha_i$ and $\beta_i$ using correctness data ($Y_{im}$) from publicly available model evaluations, namely the Open LLM leaderboard. 
%
To estimate $\gamma_m$, each endpoint model generates answers for the full evaluation dataset, which are then used to assess correctness ($Y_i$) (see Figure~\ref{fig:teaser}). This procedure is repeated for each model $m$, producing the corresponding $\gamma_m$ ($\gamma_1$ and $\gamma_2$ in the Figure).

To summarize, unlike previous work, where IRT latent abilities remain hidden variables, we explicitly derive $\gamma_m$ as an {\em ability estimator} to quantify each model's strengths. Additionally, rather than estimating $\gamma_m$ from a subset, we compute it using the {\em full} evaluation dataset, providing a more comprehensive measure of model ability, which we now leverage to enhance the merging process.

\begin{figure*}
    \centering
    \includegraphics[width=\textwidth]{figures/estimation_comparison_main.pdf}
    \caption{\textit{Performance Estimators:} Absolute error of various estimators as a function of sample size (lower is better). Our \mpirt{} and \gmpirt{} estimators consistently achieve lower error across various sample sizes and datasets.
     Additional results available in \Cref{fig:estimation_comparison_winogrande_hellaswag}.}
    \label{fig:estimation_comparison}
\end{figure*}

\subsection{Evolve: Performance Estimator}
\label{sec:evolve}
The \textit{performance estimator}, a key part of the Evolve step, efficiently approximates the fitness function, which measures the merged model's accuracy. Since fitness evaluation runs repeatedly during evolution (once per model per iteration), reducing its computational cost is crucial. Instead of evaluating the full dataset, the estimator predicts performance using only the endpoint models' abilities and the reduced dataset from previous steps, significantly accelerating the process.

We introduce two novel performance estimators for merging: \mpirt{} and \gmpirt{}. Since model merging linearly combines weights, we assume the merged model's ability is also a linear combination of the endpoint abilities. This makes our approach far more efficient, estimating only the interpolation coefficients ($\lambda_i$) instead of recomputing the full ability vector $\gamma$ of the merged model from scratch (as done in \pirt{} and \gpirt{} \cite{tinybenchmarks}).
%
\begin{assumption}[Linear Combination of Latent Abilities]\label{conj:latent-abilities-linear}
Let \(\{m_0, m_1, \dots, m_n\}\) be endpoint models with latent ability vectors \(\gamma_{i}\). If a new model \(\tilde{m}\) is formed as a linear combination of their parameters, its ability vector \(\gamma_{\tilde{m}}\) can be expressed as:
\begin{align}
    \gamma_{\tilde{m}}
    \;=\;
    \sum_{i=1}^n \lambda_i \,\gamma_{i}
    \;=\;
    [\gamma_{1}, \ldots, \gamma_{n}]\,\lambda
\end{align}
%
where \(\lambda = (\lambda_1, \ldots, \lambda_n)\) are the interpolation coefficients.
\end{assumption}
%
This assumption allows us to compute the multidimensional IRT model (Eq. \ref{eq:irt}) for model merging as a linear combination of the individual models' abilities:
\begin{align}
    p_{i\tilde{m}} 
    &= \mathbb{P}\!\left( Y_{i\tilde{m}} = 1 \;\middle|\; 
    \lambda_1 \gamma_{1} + \lambda_2 \gamma_{2}, \, 
    \alpha_i, \, \beta_i \right) \nonumber\\
    &= \frac{1}{1 + \exp\!\left( 
    -\alpha_i^\top \bigl( \lambda_1 \gamma_{1} 
    + \lambda_2 \gamma_{2} \bigr) 
    + \beta_i \right)}
     \label{eq:prob_model}
\end{align}
%
Since the endpoint models' latent abilities \(\gamma_i\) were pre-estimated over the full dataset \(D\) in the Estimate step, we only need the subset \(\bar{D}\) to estimate the interpolation coefficients \(\lambda_i\) via MLE.

\paragraph{Performance Estimators.}
To estimate the accuracy of the merged model $\tilde{m}$ using only the reduced dataset $\bar{D}$ and $p_{i\tilde{m}}$, we define the \emph{merged performance-IRT} (\textsc{mp-IRT}) estimator as:
\begin{align}
    \hat{Z}_{\tilde{m}}^{\mathrm{mp\text{-}IRT}}=
    \frac{\hat{\tau}}{\lvert \bar{D} \rvert} 
    \sum_{i \in \bar{D}} 
    Y_{i\tilde{m}}
    +
    \frac{1 - \hat{\tau}}{\lvert D \setminus \bar{D} \rvert} 
    \sum_{i \in D \setminus \bar{D}} 
    \hat{p}_{i\tilde{m}}
\end{align}
where ${\hat{\tau} = \frac{|\bar{D}|}{|D|}}$ downweights smaller subsets that may be noisier. In practice, we are considering the observed correctness for the data points we have access to, while $\hat{p}_{i\tilde{m}}$ predictions are used for the rest, enabling accurate performance estimation across all examples despite evaluating only a subset, where $\hat{p}_{i\tilde{m}} = \mathbb{P}\!\left( Y_{i\tilde{m}} = 1 \;\middle|\; \hat{\lambda}_1 \hat{\gamma}_{1} + \hat{\lambda}_2 \hat{\gamma}_{2}, \, \hat{\alpha}_i, \, \hat{\beta}_i \right)$ is the distribution defined by plugging into \cref{eq:prob_model} the parameter found via MLE. 

Although designed for model merging, \(\hat{Z}_{\tilde{m}}^{\mathrm{mp\text{-}IRT}}\) inherits certain limitations of \(\textsc{p-IRT}\)~\citep{tinybenchmarks}, such as non-uniform weighting and imperfect IRT fits. To mitigate these, we define a \emph{generalized} estimator that interpolates between \(\hat{Z}_{\tilde{m}}^{\mathrm{mp\text{-}IRT}}\) and the observed correctness on \(\bar{D}\):
%
\begin{align}\label{gp-irt}
    \hat{Z}_{\tilde{m}}^{\mathrm{gmp\text{-}IRT}}
    \;=\;
    c \sum_{i \,\in\, \bar{D}} w_i\,\hat{Y}_{i\tilde{m}}
    \;+\;
    (1 - c)\;\hat{Z}_{\tilde{m}}^{\mathrm{mp\text{-}IRT}} 
\end{align}
%
where \(c\) is a heuristic scalar chosen as in~\citet{tinybenchmarks} and \(w_i\) are uniform per-sample weights.

Although model merging can sometimes degrade performance due to weight interference—suggesting non-linear ability interactions— our assumption is empirically supported as we are interested only in evolved models that show a positive performance gain. As validated in our experiments (\S \ref{sec:exp_validation}), our custom estimators, designed around this assumption, outperform standard IRT estimators.


\subsection{Evolve: Evolutionary Search}
\label{sec:evolve_evo}  
%
The final step of our algorithm frames model merging as a multi-objective optimization problem.  
Each merging objective \( F(\tilde{m}, D_i) \) represents the performance of the merged model \(\tilde{m}\) on task~\(i\). In practice, we select a multi-objective evolutionary algorithm (e.g., NSGA-II~\citep{nsga-ii}) and a merging strategy (e.g., TIES~\citep{ties}), aiming to optimize the corresponding {Pareto front}, formally defined as:  
\[
P_{\overline{F}_D}(\Theta)
\;=\;
\bigl\{
\theta_i \,\in\, \Theta 
:\; \nexists\,\theta_j \,\in\, \Theta
\;\text{s.t.}\;\theta_j \succ \theta_i
\bigr\}
\]  
where \(\succ\) denotes \emph{Pareto-dominance}. A model \(m\) Pareto-dominates \(m'\) if:  
\begin{align*}
\forall \, F \,\in\, \overline{F}_D:\; 
F(m; D) &\leq F(m'; D)\\
&\quad\text{and}\\
\exists \, F \,\in\, \overline{F}_D:\; 
F(m; D) &< F(m'; D)
\end{align*}  
This means \(m\) is strictly better in at least one metric and no worse in all others. Models on the Pareto front are thus not dominated by any other model.

In our setting, to reduce computational costs, we approximate optimization using \(\overline{F}_{\bar{D}}\) instead of \(\overline{F}_D\), where \(\bar{D} \subset D\) is obtained by the \textit{extraction} step. Performance on \(\bar{D}\) is then estimated using the {performance estimator}.  


