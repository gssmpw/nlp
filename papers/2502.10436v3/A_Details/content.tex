\label{app:add-details}
This section provides additional implementation and experimental details that were not included in the main paper.

\subsection{IRT Fitting Details} \label{app:fitting_details}
%
As previously stated, we used the implementation from \citet{tinybenchmarks} and adopted their configuration settings. Specifically, we used $\gamma_m \sim N(\mu_{\gamma}\ones_d, 1/u_{\gamma}I_d)$, $\alpha_{i} \sim N(\mu_{\alpha}\ones_d, 1/u_{\alpha}I_d)$, and $\beta_{i} \sim N(\mu_\beta, 1/u_\beta)$.
Following \citet{tinybenchmarks}, we also applied (hyper)priors to the prior parameters using the software for fitting hierarchical Bayesian models \citep{lalor2023py}: $\mu_{\gamma} \sim N(0, 10)$, $u_{\gamma} \sim \Gamma(1, 1)$, $\mu_{\alpha} \sim N(0, 10)$, $u_{\alpha} \sim \Gamma(1, 1)$, $\mu_\beta \sim N(0, 10)$, and $u_\beta \sim \Gamma(1, 1)$. 
For both the model and example-specific parameters $\gamma_m$, $\alpha_{i}$, and $\beta_{i}$, we take their point estimates as the means of their respective variational distributions. 
The $\gamma$ model dimensionality is set to $15$ following the parameter choice suggested by \citet{tinybenchmarks}.


\begin{table}[ht]
\centering
\caption{Mistral-based models with shortened column headers and names. Role can be either E, M or B, referring to endpoint, merge or base model respectively. Spec refers instead to specialization, with \texttt{mth}, \texttt{ger}, \texttt{ita}, \texttt{jpn}, \texttt{dut} and \texttt{gen} referring to Math, German, Italian, Japanese, Dutch and General respectively. We finally have the author and model ID as per the Huggingface.}
\label{tab:short-mistral-models}
\vspace{10pt}
\resizebox{\columnwidth}{!}{% 
\begin{tabular}{ccll}
\toprule
\textbf{Role} & \textbf{Spec} & \textbf{Author} & \textbf{Model} \\
\midrule
E & \texttt{mth} & \emph{upaya07 }      & \model{Arithmo2-Mistral-7B}     \\
E & \texttt{mth,jpn} & \emph{SakanaAI }      & \model{EvoLLM-JP-v1-7B}     \\
E & \texttt{mth} & \emph{GAIR}          & \model{Abel-7B-002}      \\
E & \texttt{mth} & \emph{meta-math}     & \model{MetaMath-Mistral-7B}     \\
B & \texttt{gen}  &\emph{ mistralai}     & \model{Mistral-7B-v0.1}      \\
E & \texttt{ger} &\emph{ jphme  }          & \model{em\_german\_mistral\_v01}   \\
E & \texttt{ger} & \emph{LeoLM  }       & \model{leo-mistral-hessianai-7b}       \\
E & \texttt{ita} & \emph{DeepMount00}   & \model{Mistral-Ita-7b}       \\
E & \texttt{jpn} & \emph{augmxnt  }     & \model{shisa-gamma-7b-v1}       \\
E & \texttt{dut} & \emph{BramVanroy}    & \model{GEITje-7B-ultra}     \\
E & \texttt{ro} & \emph{OpenLLM-Ro}    & \model{RoMistral-7b-Instruct}     \\
M & \texttt{gen} &\emph{ chlee10 }      & \model{T3Q-Merge-Mistral7B}    \\
E & \texttt{gen} &\emph{ liminerity}    & \model{M7-7b}        \\
E & \texttt{gen} & \emph{yam-peleg}     & \model{Experiment26-7B}     \\
M & \texttt{gen} & \emph{PracticeLLM}   & \model{SOLAR-tail-10.7B-Merge-v1.0}    \\
E & \texttt{gen} &\emph{ upstage}       & \model{SOLAR-10.7B-v1.0}      \\
E & \texttt{gen} & \emph{Yhyu13}        & \model{LMCocktail-10.7B-v1}   \\
M & \texttt{gen} & \emph{FuseAI}        & \model{FuseChat-7B-Slerp}  \\
M & \texttt{gen} & \emph{FuseAI}        & \model{FuseChat-7B-TA}   \\
E & \texttt{gen} & \emph{FuseAI}        & \model{OpenChat-3.5-7B-Mixtral}  \\
E & \texttt{gen} & \emph{FuseAI}        & \model{OpenChat-3.5-7B-Solar}  \\
M & \texttt{gen} & \emph{jan-hq}        & \model{supermario-slerp-v3}\\
E & \texttt{gen} & \emph{jan-hq}        & \model{supermario-slerp-v2}\\
E & \texttt{gen} & \emph{jan-hq}        & \model{supermario-v2}\\
M & \texttt{gen} & \emph{superlazycoder}& \model{NeuralPipe-7B-slerp
}   \\
E & \texttt{gen} & \emph{OpenPipe}      & \model{mistral-ft-optimized-1218}     \\
E & \texttt{gen} & \emph{mlabonne}      & \model{NeuralHermes-2.5-Mistral-7B} \\
\bottomrule
\end{tabular}%
}
\end{table}

\subsection{\approachbf{} Algorithm}
% 
Below we outline the pseudo-code for the end-to-end \approach{} algorithm:
%
\begin{algorithm}[ht]
    \caption{MERGE3 Algorithm} \label{alg:merge3}
    \begin{algorithmic}[1]
    \REQUIRE Dataset $D$, models $\lbrace M_1, M_2, \ldots, M_n \rbrace$, iterations $T$
    \ENSURE Pareto-optimal merged models
    % \PROCEDURE{MERGE3}{$D, \lbrace M_1, \ldots, M_n \rbrace, T$}
        \STATE $\bar{D} \gets \Call{RandomSample}{D, k}$
            \Comment{Sample $k$ items from $D$}
        \STATE $\lbrace \gamma_1,\gamma_2,\ldots,\gamma_n \rbrace \gets \Call{EstimateAbilities}{\{M_1, \ldots, M_n\}, D}$
        \STATE $P \gets \text{GenerateInitialPopulation}{\{M_1, \ldots, M_n\}}$
            \Comment{Initialize population}
        \FOR{$t \gets 1$ to $T$}
            \FORALL{$M \in P$}
                \STATE $\lambda \gets \Call{FitLambda}{M,\,\lbrace \gamma_1,\dots,\gamma_n \rbrace,\,\bar{D}}$
                    \Comment{Fit interpolation weights}
                \STATE $\text{preds} \gets \Call{GetPredictions}{M,\,\bar{D},\,\lambda}$
                    \Comment{Compute predictions}
                \STATE $\text{corr} \gets \Call{GetCorrectness}{\text{preds},\,\bar{D}}$
                    \Comment{Evaluate correctness}
                \STATE $F(M) \gets \Call{EstimateFitness}{\text{corr},\,\lambda}$
                    \Comment{Estimate fitness score}
            \ENDFOR
            \STATE $P \gets \Call{SelectParents}{P, f}$ 
                \Comment{Select based on fitness}
            \STATE $P \gets \Call{ApplyMutation}{P}$
                \Comment{Apply mutation}
            \STATE $P \gets \Call{ApplyCrossover}{P}$
                \Comment{Generate offspring}
        \ENDFOR
        \STATE \textbf{return} $\Call{ParetoFront}{P}$
    % \ENDPROCEDURE
    \end{algorithmic}
\end{algorithm}

\subsection{Experimental Details}
\label{app:add-details-evo}

\paragraph{Models.}
One key assumption of model merging is that the endpoint models lie within the same basin \cite{task-vectors}. This means that merging arbitrary models is not feasible; rather, \textbf{all models involved must be fine-tuned versions of the same base model}. To satisfy this requirement, we selected several fine-tuned models from the Hugging Face Hub that originated from the same base model. Specifically, we focused on models fine-tuned starting from \model{Mistral-7B} \cite{mistral}, following common best practices in the community \cite{sakana}.
\Cref{tab:short-mistral-models} lists all the models used in our experiments, along with their corresponding names on the Hugging Face Hub. A total of $27$ models were considered for our experiments.

In the rest of the section, we provide further details for reproducing the experiments in \cref{sec: cross-lingual} and \cref{sec: multi-lingual} of the main paper.

\subsubsection{Cross-Lingual Transfer}

In the cross-lingual transfer evolutionary merging, we evolved four merged models with mathematical capabilities in different languages: Japanese, Romanian, German, and Dutch. In each of these experiments, we deployed an ad-hoc genetic algorithm for single-objective optimization. We employed the Simulated Binary Crossover~\cite{10.1145/1276958.1277190} operator to generate offspring solutions by combining parent solutions. To maintain diversity and explore the search space, we applied Polynomial Mutation~\cite{10.1145/1276958.1277190}, which introduces small perturbations to offspring solutions and enhances the algorithm's ability to escape local optima. This combination of SBX and PM effectively balances exploration and exploitation, facilitating efficient convergence toward optimal solutions.

Furthermore, guided by empirical tests, we decided to deploy \method{SLERP} to evolve solutions for the Romanian and Dutch problems, while we used a combination of \method{TIES} and \method{DARE} for the Japanese and the German ones. We deployed four different sizes of the fitness datasets for Japanese, namely 20, 30, 50, and 100, in order to obtain a more detailed analysis of the method for comparison with the work of~\cite{sakana}. On the other hand, we kept the fitness dataset size fixed to 20 for all other aforementioned experiments. The fitness dataset was extracted from the test set of \dataset{GSM8K}, and we used the remaining, non-overlapping samples as test set for evaluating the model. To get the language-specific versions of \dataset{GSM8K}, we used \model{Unbabel/TowerInstruct-7B-v0.2} \cite{alves2024tower} to translate the datasets. In each experiment, the population size was fixed to 25 and the number of iterations to 7.

To check the correctness of the solution, following \citet{sakana}, we used a regex to extract the last numerical value returned in the model's answer and compare it with the ground truth. The solution is also checked to be in the correct language with the language identifier from fastText~\cite{joulin2017bag}.

The mathematical models used in combination with \method{TIES} and \method{DARE} were \model{Abel-7B-002} and \model{Arithmo2-Mistral-7B}, whereas we used \model{MetaMath-Mistral-7B} in combination with \method{SLERP}. Moreover, we employed the following language-specialized models: \model{shisa-gamma-7b-v1}, \model{em\_german\_mistral\_v01}, \model{GEITje-7B-ultra}, and \model{RoMistral-7b-Instruct}. More information about these models can be found in \cref{tab:short-mistral-models}.

Lastly, we evaluated \model{EvoLLM-JP-v1-7B} \cite{sakana} under the same conditions as \approach{} to assess its accuracy, following the prompting structure outlined by \citet{sakana}.

\subsubsection{Multi-Lingual Transfer}

In this experiment, we tackle the ARC dataset in multiple languages (Italian, Dutch, German, and English)\footnote{We used the dataset on the Hugging Face Hub from openGPT-X/arcx} \cite{thellmann2024crosslingual} using a multi-objective evolutionary merging procedure based this time on NSGA-II \cite{nsga-ii}. We configure the population size to 25 and the number of evolutionary iterations to 7. We deployed a combination of \method{TIES} and \method{DARE} as merging strategy. As in previous settings, both the fitness function and the test metrics operate by extracting the final model-generated choice via a regex, but this time they look for an instance from the set \{A, B, C, D\} rather than a number. On top of this, we employed a dataset composed by 20 datapoints for each language from the relative translation of \dataset{ARC} to compute the fitness, and we extracted the test set as for the previous experiments. Furthermore, unlike the single-objective approach described earlier, here we explicitly optimize multiple objectives simultaneously. This time, the employed models are \model{Mistral-Ita-7b}, \model{GEITje-7B-ultra}, \model{leo-mistral-hessianai-7b}, and the base model \model{Mistral-7B-v0.1}. 

\subsubsection{Ability and Performance Estimator} In these experiments (reported in \cref{sec:exp_validation} and \cref{par: pe}) we used the test set of the standard version of \dataset{GSM8K}, \dataset{HellaSwag}, \dataset{ARC}, \dataset{Winogrande}, and \dataset{TruthfulQA}. Furthermore, we used 6 different models to test the different performance of the ability and performance estimator: \model{SOLAR-tail-10.7B-Merge-v1.0}, \model{FuseChat-7B-Slerp}, \model{NeuralPipe-7B-slerp}, \model{T3Q-Merge-Mistral7B}, \model{FuseChat-7B-TA}, and \model{supermario-slerp-v3}. These models were chosen as already available on the Open LLM Leaderboard.


%\subsection{Notation}

%In \cref{tab:notations} we outline a scheme of the notation used along the paper.

\begin{table}
\centering
\caption{Notation used in the paper.}
\label{tab:notations}
\vspace{10pt}
\resizebox{\columnwidth}{!}{%
\begin{tabular}{cl}
    \toprule
    \textbf{Notation} & \textbf{Description} \\ 
    \midrule
    $D$ & Full dataset. \\
    $\bar{D}$ & Reduced subset of the dataset. \\
    $D_{i}$ & Subdataset for task $i$. \\
    $\gamma_m$ & Latent abilities of model $m$. \\
    $\Gamma_m$ & True latent abilities of model $m$. \\
    $\gamma_m^{\{\mathrm{p,gp}\}-\mathrm{IRT}} $ &  Latent abilities of model $m$ via \pirt{} ability estimator.\\
    $\gamma_m^{\{\mathrm{mp,gmp}\}-\mathrm{IRT}} $ &  Latent abilities of model $m$ via \mpirt{} ability estimator.\\
    $\alpha_i, \beta_i$ & IRT parameters related to dataset item $i$. \\
    $\lambda$ & Interpolation coefficients for latent abilities. \\
    $\hat{\lambda},\hat{\gamma},\hat{\alpha},\hat{\beta}$ & MLE of the aforementioned parameters. \\
    $p_{i,m}$ & IRT model for datapoint $i$ and model $m$. \\
    \multirow{2}{*}{$\hat{p}_{i,m}$} & IRT model for datapoint $i$ and model $m$ \\
                    & parametrized by MLE estimators of $\alpha, \beta,\gamma, \lambda$. \\
    $\tilde{m}$ & Merged language model. \\
    $Y_{i,m}$ & Sample-level correctness of model $m$ for example $i$. \\
    $\hat{Z}^{\mpirt{}}$ & Merged performance estimator \mpirt{}. \\
    $\hat{Z}^{\gmpirt{}}$ & Generalized merged performance estimator \gmpirt{}. \\
    $F(m)$ & Fitness value of a model $m$. \\
    $\theta$ & Parameters being optimized in evolutionary search. \\
    $P_{\bar{F}_D}$ & Pareto front defined by function's set $\bar{F}$ and data $D$ \\
    $\theta^{\star}$ & Global optimum on $D$. \\
    $\hat{\theta}$ & Global optimum on $\bar{D}$. \\
    $N$ & Number of samples in the dataset. \\
    \bottomrule
\end{tabular}
}
\end{table}