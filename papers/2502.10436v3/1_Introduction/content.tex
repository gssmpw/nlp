%!Tex root=../main.tex

Model merging has become a powerful and accessible approach for developing new state-of-the-art models without the need for cluster-grade computing typically required for large model training \cite{yang2024model}.
Its key advantage lies in performing the merging process post-hoc directly in the parameters of pre-existing models (endpoint models), eliminating the need for training and significantly reducing the demand for expensive computational resources.

This approach has significantly broadened access to the field, with ML practitioners producing competitive models out of existing ones on standard consumer GPUs\footnote{At the time of writing, around 30\% of models on the Hugging Face Open LLM leaderboard are merged models.}\citep{task-vectors}.
%
\begin{figure}[t]
    \centering
    \includegraphics[width=.9\linewidth]{figures/flops_vs_accuracy.pdf}
    \caption{\textbf{Accuracy on Japanese \dataset{GSM8K} over fitness evaluation FLOPs.} \approach{} is competitive with a model evolved on the full dataset by only using a consumer-grade GPU and $2\%$ of the data (point size reflects data amount).}
    \label{fig:speed_accuracy}
\end{figure}
%
However, although computationally inexpensive, most of the existing approaches are quite rudimentary, require ad-hoc choices, and are usually based on ungrounded trial-and-error strategies for selecting the merge coefficients, which ultimately limits their downstream performance \citep{ties, yu2024language}.
On the other hand, recent work has shown that evolutionary merging can produce models of unprecedented quality by automating the hyperparameter search for merging coefficients \citep{sakana}.
While this technique can incorporate any standard merging method, such models are absent from public leaderboards likely due to a mismatch between the high computational demands of evolutionary merging and single-GPU setups typical of merging practitioners.
Indeed this computational cost is significantly high: computing the fitness function requires generating and evaluating answers for each dataset element, for each candidate in every evolutionary step. As shown in \Cref{fig:speed_accuracy}, the fitness computation alone in the 1,000-trial evolutionary merge from \citet{sakana} requires approximately $4\times10^6$ TFLOPs, with the full algorithm demanding largely over a month of continuous computation if run on a single NVIDIA 4090 (\S \ref{app:add-exp-4090}).
This renders evolutionary merging effectively out of reach for consumer hardware, risking to exclude the very user base it was meant to empower.


% APPROACH
In this paper, we address this challenge by introducing \approachns{}, an evolutionary merging framework that runs on a single consumer GPU with competitive results (see \cref{fig:speed_accuracy}). Unlike the competing approach, \approachns{} operates with just $0.077 \times 10^6$ TFLOPs, namely a {\bf 50-fold reduction}. This drastic decrease in computational cost makes it feasible on consumer hardware, freeing up FLOPs for further optimization or additional tasks.

Our approach starts by \textbf{E}xtracting a reduced subset of the fitness evaluation dataset, significantly alleviating the computational bottleneck of fitness computation (\cref{fig:teaser}). However, this reduction risks losing accuracy if the subset lacks diversity. To address this, we apply Item Response Theory (IRT) \cite{lord1968statistical}—a well-established statistical framework—to bridge the gap between reduced-dataset evaluations and full-dataset performance.
%
Specifically, we first \textbf{E}stimate the latent abilities of the endpoint models using IRT, ensuring the merged models accurately reflect their components' strengths. Then, we \textbf{E}volve the endpoint models with IRT-based performance estimators designed for model merging, assuming the merged model’s ability is a combination of those of the endpoint models. This approach significantly improves the efficiency and accuracy of fitness estimation, integrating merging-specific insights into performance estimation theory while maintaining high accuracy with reduced datasets.

Experimental results show that \approachns{} effectively transfers mathematical skills by merging a strong math model with three language-specific models, achieving 10–20\% higher accuracy than standard merging baselines in each language. Building on this, we evolve a single multilingual model by merging Italian, English, German, and Dutch models, outperforming individually fine-tuned models by up to 19\% on \dataset{ARC} \cite{ARC}, a widely used benchmark for reasoning.
Furthermore, \approachns{} achieves competitive accuracy on Japanese \dataset{GSM8K} \cite{gsm8k}, matching models evolved on full datasets while maintaining high efficiency, demonstrating that our evolutionary strategy preserves performance while drastically reducing computational costs.

To summarize, our contributions are fourfold:
\begin{itemize} 
    \item We introduce a novel, efficient evolutionary model merging framework leveraging Item Response Theory, making merging feasible on consumer hardware.
    \item We demonstrate its effectiveness in transferring skills across languages and synthesizing state-of-the-art multilingual models without standard training.
    \item We advance the theoretical foundations of performance estimation in model merging and provide formal guarantees for our proposed estimators.
    \item We release a modular library for evolutionary merging on consumer GPUs, alongside a suite of state-of-the-art models for several low-resource languages.
\end{itemize}

\begin{figure}
    \centering
    \includegraphics[width=.99\linewidth]{figures/teaserTN2.png}
    \caption{\textbf{\approachbf~for math + Japanese merging (\dataset{GSM8K})}.
    The method \textbf{Extracts} a reduced evolutionary dataset, \textbf{Estimates} ability parameters ($\gamma$) via Item Response Theory (IRT) based on their response correctness, and \textbf{Evolves} the endpoint models through iterative merging. Leveraging an IRT-based performance estimator, it approximates full-dataset fitness with reduced data, cutting fitness estimation costs while preserving full-dataset accuracy -- making evolutionary merging feasible on consumer GPUs.
    }
    \label{fig:teaser}
\end{figure}


