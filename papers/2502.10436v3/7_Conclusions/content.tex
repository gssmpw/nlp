We introduced \approachns{}, an evolutionary merging framework that makes high-quality model merging feasible on a single consumer GPU. By combining a subset-based approach with IRT-driven performance estimation, \approachns{} reduces merging costs by up to fifty-fold compared to prior methods -- without sacrificing the quality of the merged model. Our experiments demonstrate successful cross-lingual transfer in mathematics (e.g., from English to Japanese), as well as the synthesis of new multilingual models that outperform each of their language-specific endpoints. Overall, \approachns{} expands the practical reach of evolutionary merging, allowing everyday practitioners to benefit from advanced multi-task and multilingual model compositions at a fraction of the usual computational cost.

\section*{Impact Statement}
The introduction of \approach provides an efficient and accessible method for evolutionary model merging on consumer-grade GPUs. By combining dataset reduction techniques and Item Response Theory (IRT)-based performance estimations, \approachns{} significantly lowers computational requirements while maintaining competitive performance. This enables researchers and developers to synthesize high-quality multilingual and cross-lingual models without requiring cluster-scale hardware.

The open-source release of \approach aims to make evolutionary model merging widely accessible, fostering further innovation in resource-constrained environments. With applications in multilingual NLP and low-resource language modeling, \approach addresses practical challenges in the field, offering an efficient solution for creating state-of-the-art models on standard hardware.