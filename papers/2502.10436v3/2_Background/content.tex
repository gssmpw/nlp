% MODEL MERGING

\paragraph{Model Merging} has emerged as an efficient alternative to ensembling by integrating existing models without any additional training. One set of methods identifies neuron permutations that align the models into a shared optimization basin, allowing them to be merged through straightforward averaging \citep{git-rebasin, repair, zip-it, rebasin-implicit-sinkhorn, cycle-consistent}. Closer to our work, multi-task model merging focuses on the case where a single pre-trained model is fine-tuned for different tasks \citep{task-vectors, ties, yu2024language, matenamerging, wortsman2022model, davari2023model, wang2024localizing, zhou2024atm, gargiulo2025tasksingularvectorsreducing}. 
In this direction, several works address task interference by pruning or selectively combining parameters---e.g., TIES-merging~\citep{ties}, Model Breadcrumbs~\citep{davari2023model}, and DARE Merging~\citep{yu2024language}â€”or by optimizing merge coefficients \citep{yang2023adamerging}, introducing task-specific modules~\citep{yang2024representation}, and disentangling weights~\citep{ortiz2024task}. 

\paragraph{Evolutionary Algorithms.}
Evolutionary Algorithms are black-box optimization algorithms operating on a population of potential solutions by evolving them through generations with operators such as selection, mutation, recombination, and crossover \citep{6791438, petrowski2017evolutionary, dasgupta1997evolutionary}. 
Recent applications include neural architecture search~\citep{real2019regularized} and hyperparameter tuning~\citep{vincent2023improved}, where evolutionary methods efficiently navigate large design spaces without manual intervention. 
The fitness function is crucial, as it evaluates the quality of each solution, guiding the selection process by favoring higher-scoring (fitter) solutions for reproduction~\citep{ea}. Closest to our work, \citet{sakana} propose to apply evolutionary algorithms to optimize model merging recipes, eliminating the need for trial-and-error in combining parameters. In this context, the most obvious candidate for a fitness function is simply the performance of the resulting model over a held-out validation set. 

\paragraph{Item Response Theory.}
Item Response Theory (IRT) \citep{cai2016item, van2018handbook, brzezinska2020item, lord1968statistical} is a paradigm to design, analyze, and score responses to tests such as SAT or GRE \citep{an2014item, kingston1982feasibility, petersen1982using}. Based on the relationship between individuals' performances on a test item and the test takers' levels of performance on the corresponding required ability, IRT has recently spread from psychometrics to natural language processing. In this direction, \citet{lalor2016building} leverage IRT's latent dimensions to evaluate language models, while \citet{vania2021comparing} use it to analyze benchmark saturation in NLP evaluations. 
More relevant to our work, \citet{zhuang2023efficiently} and \citet{tinybenchmarks} employ IRT-driven adaptive testing to alleviate the computational burden of large-scale evaluations for large language models (LLMs). Although their focus is on LLM evaluation, which shares similarities with the efficient evaluation of fitness functions in model merging, our work builds on these approaches to design IRT-based estimators specifically tailored for model merging. Unlike prior applications of IRT, which are limited to LLM evaluations, our approach adapts the framework to address the unique challenges of evolutionary model merging, enabling efficient and accurate fitness estimation.
