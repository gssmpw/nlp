\begin{figure}
    \includegraphics[width=\linewidth]{figures/latent_abilities_tasks_main.pdf}
    \caption{\textit{Ability Estimator:} Cosine similarity between estimated and true abilities for different tasks (higher is better). Our estimated abilities $\gamma^{\{\mathrm{mp},\mathrm{gmp}\}-{\mathrm{IRT}}}$ better approximate true abilities.}
    \label{fig:ability-estimator-comparison}
\end{figure}

\begin{figure*}
    \centering
    \begin{subfigure}{0.33\linewidth}
    \includegraphics[width=\linewidth]{figures/cross-Romanian.pdf}
    \caption{To Romanian.}
    \label{fig:cross-ro}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.33\linewidth}
    \includegraphics[width=\linewidth]{figures/cross-German.pdf}
    \caption{To German.}
    \label{fig:cross-de}
    \end{subfigure}
    \begin{subfigure}{0.33\linewidth}
    \includegraphics[width=\linewidth]{figures/cross-Dutch.pdf}
    \caption{To Dutch.}
    \label{fig:cross-nl}
    \end{subfigure}
    \caption{\textit{Cross-lingual skill transfer}: merging math models (dark blue) with language-specific models (red) effectively transfers mathematical skills across languages (green - our method) compared to baselines (white). Accuracy on \dataset{GSM8K} for each target language.
    %, with white indicating baseline merging approaches.
    }
    \label{fig:cross-transfer}
\end{figure*}

\begin{figure}
    \centering
    \includegraphics[width=.9\linewidth]{figures/comparison-Sakana.pdf}
    \caption{Accuracy of merged models for Japanese \dataset{GSM8K}.}
    \label{fig:evolve-comparison-sakana}
\end{figure}
%
In this section, we evaluate \approachns, demonstrating its effectiveness in evolutionary model merging on consumer-grade GPUs. We first validate the proposed ability and performance estimators, assessing their accuracy in approximating full-dataset evaluations. Next, we examine cross-lingual transfer, where \approach enables efficient merging of multilingual models, improving mathematical reasoning across languages. Finally, we evaluate its ability to synthesize multilingual models, surpassing individual fine-tuned baselines while remaining computationally efficient.
All the merging experiments were performed with our custom-made library \textit{Mergenetic} (see Appendix \ref{app:mergenetic}).

\subsection{Validating Estimators}
In this section, we empirically validate our merged-performance estimators by comparing them against standard \pirt{} and \gpirt{} estimators \cite{tinybenchmarks} across five benchmark datasets: \dataset{GSM8K} \cite{gsm8k}, \dataset{Winogrande} \cite{sakaguchi2021winogrande}, \dataset{TruthfulQA} \cite{lin2021truthfulqa}, \dataset{Hellaswag} \cite{zellers2019hellaswag}, and \dataset{ARC} \cite{ARC}. Due to space limitations, additional results are provided in Appendix \ref{app:add-exp}.


\paragraph{Ability Estimators.} 
\label{sec:exp_validation}
%
To validate our ability estimators we compare their inferred latent ability vectors to the reference ``ground-truth'' vectors \(\Gamma\). Specifically, we measure the cosine similarity and the Euclidean distance from the ground-truth \(\Gamma\) both for \(\gamma^{{\{\mathrm{mp,gmp}\}-\mathrm{IRT}}}\), estimated with our merged-performance IRT approaches, and \(\gamma^{\{\mathrm{p,gp}\}-\mathrm{IRT}}\), estimated with the \pirt{} and \gpirt{} estimators \citep{tinybenchmarks}.
Here, \(\Gamma_{m}\) is computed by fitting the IRT model (as in \cref{sec:irt}) to each merged model \(m\) using its entire set of responses on the full dataset \(D\). Incorporating all available data, \(\Gamma_{m}\) serves as our best proxy for the model’s true ability. Conversely, both \(\gamma_m^{{\{\mathrm{mp,gmp}\}-\mathrm{IRT}}}\) and \(\gamma_m^{\{\mathrm{p,gp}\}-\mathrm{IRT}}\) are estimated using only a smaller subset \(\bar{D} \subset D\) of size \(n\).
\Cref{fig:ability-estimator-comparison} shows the results of this comparison for $n=10$ and $n=20$, while the results for $n=15, 30, 50, 100$ are reported in \Cref{app:estimation-step} along with the same experiment over different languages. 
Across all five benchmark tasks our proposed ability estimator \(\gamma_m^{{\{\mathrm{mp,gmp}\}-\mathrm{IRT}}}\) consistently yields ability vectors with higher cosine similarity to \(\Gamma\) than \(\gamma_m^{\{\mathrm{p,gp}\}-\mathrm{IRT}}\). This trend is evident across both subset sizes, highlighting the robustness of our approach even with limited data. The superior performance of \(\gamma_m^{{\{\mathrm{mp,gmp}\}-\mathrm{IRT}}}\) empirically validates \Cref{conj:latent-abilities-linear}, confirming that an IRT-based ability estimator designed around this assumption provides more accurate ability estimates than a general-purpose alternative.

\paragraph{Performance Estimators.} \label{par: pe}
%
To assess the accuracy of our proposed performance estimators, we measure their absolute estimation error across different sample sizes. Specifically, we evaluate the performance estimates of six merged models using random sampling, \pirt{}, \gpirt{} \cite{tinybenchmarks}, \mpirt{}, and \gmpirt{} across various subset sizes. The resulting absolute errors shown in Figure~\ref{fig:estimation_comparison} are reported for \dataset{ARC}, \dataset{GSM8K}, \dataset{TruthfulQA}, and an aggregate average across all five benchmarks.


As shown in the figure, our proposed estimators, \mpirt{} and \gmpirt{}, consistently achieve lower absolute error compared to \gpirt{} and \pirt{}. While all IRT-based methods outperform random sampling, the incorporation of merged-performance IRT significantly enhances estimation accuracy. Notably, both \mpirt{} and \gmpirt{} maintain low empirical error and reduced variance even when operating with very small subsets (\(|\bar{D}| \approx 1.5\%\) of the full dataset). This highlights the robustness of our approach in low-data regimes. 

Since lower empirical error often correlates with reduced \emph{expected} error (as formalized in Section~\ref{sec:theory}), we adopt \mpirt{} and \gmpirt{} as our primary estimators for evolving merged language models in subsequent experiments.

% Math transfer to DE, RO, NL, Jap
\subsection{Cross-Lingual Transfer of Mathematical Skills}
\label{sec: cross-lingual}
To assess the transfer of mathematical reasoning from English to other languages, we merge an English math-specialized model with a \model{Mistral-7B} \cite{mistral} fine-tuned on each target language, then evaluate on the corresponding \dataset{GSM8K} translations \citep{gsm8k}. Appendix \ref{app:add-details-evo} provides details on the specific models used for merging. Following \citet{sakana}, we label an answer correct only if it is both accurate and written in the target language. We benchmark our approach against three commonly used merging baselines -- \method{Task Arithmetic} \citep{task-vectors}, \method{TIES} \citep{ties} and \method{DARE} \citep{yu2024language}. Following standard practice in the merging community, we apply either \method{TIES} and \method{DARE} jointly or \method{SLERP} \cite{10.1145/325165.325242}.

As shown in figure \ref{fig:cross-transfer}, merging a language-specific fine-tuning with a math-specialized model consistently surpasses both endpoint models by 10–20\% in accuracy on the translated \dataset{GSM8K}. In contrast, standard baselines often yield sub-optimal merges, performing worse than the endpoints themselves. This highlights the importance of optimized merging coefficients and motivates our evolutionary framework. To rule out the possibility that gains arise merely from exploiting a small, in-distribution dataset—rather than true cross-lingual transfer—we also merge the same Italian fine-tuning with itself; the results, available in the appendix \ref{app:add-exp-self-merging}, shows no improvement, reinforcing that our observed gains stem from genuine knowledge transfer.

Next, we evaluate our method for transferring math skills from English to Japanese and compare it to \method{EvoMerge} \citep{sakana}, which serves as an upper bound by computing fitness on the full dataset. As illustrated in figure \ref{fig:evolve-comparison-sakana}, our approach confirms the significant gains seen for the other languages, greatly surpassing both the performance of the endpoint models and that of the merging baselines.
While the accuracy is lower than that of the model obtained by computing the fitness on the full dataset as done by \citet{sakana}, figure \ref{fig:speed_accuracy} shows that our approximation yields a method that is $50\times$ more efficient, effectively making evolutionary merging feasible on a single consumer GPU.

\subsection{Evolving a Multilingual model}
\label{sec: multi-lingual}

\begin{table}
      \caption{Evolving a multilingual model. For each language, we report the accuracy on the corresponding translated \dataset{ARC} of both the language-specific model and the evolved multilingual model.} 
      \label{tab:multilingual}
  \resizebox{\linewidth}{!}{%
    \begin{tabular}{ccccccccc}
    \\
    \toprule
    \multirow{2}{*}{Model} & \multicolumn{8}{c}{\textbf{Accuracy} ($\uparrow$)} \\ 
    \cmidrule{2-9}
    & \multicolumn{2}{c}{Italian} & \multicolumn{2}{c}{English} & \multicolumn{2}{c}{German} & \multicolumn{2}{c}{Dutch} \\
    \midrule
    Finetuned & 0.61 & -- & 0.75 & -- & 0.61 & -- & 0.50 & -- \\ 
    \approach & \textbf{0.69} & \improvUP{8\%} & \textbf{0.79} & \improvUP{4\%} & \textbf{0.72} & \improvUP{11\%} & \textbf{0.69} & \improvUP{19\%} \\ 
    \bottomrule
    \end{tabular}
    }
\end{table}
We next combine individually fine-tuned models for \{\texttt{IT, EN, DE, NL}\} into a single multilingual model. Appendix \ref{app:add-details-evo} provides details on the specific models used for each language. As shown in \cref{tab:multilingual}, the resulting merged model surpasses each language-specific variant by up to 19\% in accuracy on the \dataset{ARC-Challenge} dataset \cite{ARC}. Even more notably, it outperforms all its constituent endpoints, demonstrating a clear positive transfer of knowledge across languages.
Beyond the clear accuracy boosts in each language, a few key insights stand out. First, the largest improvement occurs for Dutch (from 50\% to 69\%), suggesting that merging particularly benefits languages where the baseline performance is lower. Second, even English, which starts from the highest baseline, still gains by 4\%, indicating that positive transfer is not limited to low-resource or weaker endpoints. Finally, the fact that the merged model outperforms all individual fine-tunings (rather than landing between them) points to a genuine cross-lingual synergy, wherein knowledge from each language-specific model collectively strengthens the multilingual result.
