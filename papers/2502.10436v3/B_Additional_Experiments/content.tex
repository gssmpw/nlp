\label{app:add-exp}
We report here additional experiments and analyses.

\subsection{Extract Step} \label{app:add-exp:extract-ste} 
In the extract step outlined in \cref{sec:estimate}, random sampling has been proposed as the main method to subsample the dataset $\bar{D} \subset D$. 
While we explored various dataset subsampling strategies, we ultimately opted for uniform random sampling, as our experiments showed that more complex approaches offered no significant advantage over this simpler method. In this section we report some of the experiments behind this decision and the two alternative methods tried in the extraction step: IRT Clustering (IRT), introduced by \citet{tinybenchmarks}, and a custom Representation Clustering (RC) method.

\subsubsection{IRT Clustering} Given a dataset $D$ and the parameter of a fitted IRT model $\alpha$ and $\beta$, one can define a low-dimensional embedding of each datapoint $i \in D$ by $E_i = [\alpha_i \| \beta_i]$. Therefore, IRT-clustering obtains a representative subset by first obtaining a clustering over this embedding space through $K$-Means, and then choosing the points closest to the centroids as representative samples.

\subsubsection{Representation Clustering} 

Let \(\{m_j\}_{j=1}^M\) be the set of endpoint models, and let \(D = \{x_i\}_{i=1}^N\) be our full dataset. For each sample \(x_i\), we first encode it into a high-dimensional vector by concatenating model-specific embeddings. Concretely, we compute:
\[
    E_{i,j} = \frac{1}{T_i}\sum_{t=1}^{T_i} E_{i,j,t} \in \mathbb{R}^d,
\]
where \(E_{i,j,t}\) is the embedding of the \(t\)-th token of sample \(x_i\) under model \(m_j\), and \(T_i\) is the number of tokens in \(x_i\). We form the concatenated representation:
\[
    E_i = [E_{i,1}\|E_{i,2}\|\cdots\|E_{i,M}] \in \mathbb{R}^{M\cdot d}.
\]
%
Since \(E_i\) can be very high-dimensional, we apply Principal Component Analysis (PCA) to project \(E_i\) onto a lower-dimensional space:
\[
    \tilde{E}_i = \text{PCA}_k(E_i) \in \mathbb{R}^k, \quad k \ll M\cdot d.
\]
%
Next, we apply \(k\)-means clustering to the reduced embeddings \(\{\tilde{E}_i\}_{i=1}^N\):
\[
    \min_{\{\mathbf{c}_k\}_{k=1}^K}\sum_{i=1}^N \min_{1\leq k \leq K} \|\tilde{E}_i - \mathbf{c}_k\|^2,
\]
where \(\mathbf{c}_k\) is the centroid of the \(k\)-th cluster. This partitions the dataset into \(K\) clusters, each capturing a distinct region of the representation space.
From each cluster \(k\), we select the representative sample \(x_{i_k^\star}\) whose embedding \(\tilde{E}_{i_k^\star}\) is closest to the centroid \(\mathbf{c}_k\):
\[
    i_k^\star = \arg\min_{x_i \in C_k} \|\tilde{E}_i - \mathbf{c}_k\|,
\]
where \(C_k\) is the set of samples assigned to cluster \(k\).
To approximate the full-dataset metrics from the selected subset \(\bar{D}=\{x_{i_k^\star}\}_{k=1}^K\), we assign a weight to each representative sample. Since the size of the cluster \(C_k\) indicates how prevalent that region of representation space is, we define $w_{i_k^\star} = \frac{|C_k|}{|D|}$.
These weights ensure that the contribution of each representative sample to the overall metric reflects the true proportion of samples that it represents in the original dataset. By evaluating a new model \(m\) only on \(\bar{D}\) and using \(\{w_{i_k^\star}\}\) to calculate a weighted average, we approximate \(m\)’s performance on the full dataset \(D\) at a fraction of the computational cost.

A schematic overview of the full process is outlined in \cref{alg:repr-clust}. 

\begin{algorithm}
\caption{Representation Clustering Extractor}\label{alg:repr-clust}
\begin{algorithmic}[1]
    \REQUIRE Dataset $D$, Endpoint Models $m_1,...,m_n$, Desired subset size $K$
    \ENSURE Subset of size $K$ with weights $w_i$

    \FOR{$i$ in $D$}
        \STATE $E_i \gets []$ 
        \FOR{$m$ in $\{m_1, \dots, m_n\}$}
        \STATE $E_{im} \gets$ embed $i$ with model $m$
        \STATE  $E_i \gets E_i | E_{im}$ 
        \ENDFOR
    \ENDFOR
    \STATE $\{\mathbf{E}_i\}_{i \in D} \gets \operatorname{PCA}(\{\mathbf{E}_i\}_{i \in D})$
    \STATE Apply k-means clustering to $\{\mathbf{E}_i\}_{i \in D}$, obtaining $K$ centroids $\{\mathbf{c}_k\}_{k=1}^K$
    \STATE For each cluster $k$, select the closest example $i_k^{\star} = \arg\min_{i \in D} \|\mathbf{E}_i - \mathbf{c}_k\|_2$
    \STATE Let $C_k = \{i \in D \ | \ \arg\min_{c \in \{\mathbf{c}_k\}_{k=1}^K} \|\mathbf{E}_i - c\|_2 = \mathbf{c}_k\}$ be the set of examples in cluster $k$
    \STATE Assign weights $w_{i_k^{\star}} = \frac{|C_k|}{|D|}$ for $k=1,...,K$
    \STATE \textbf{return}  $\left\{ i_k^{\star}, w_{i_k^{\star}} \right\}_{k=1}^{K}$
\end{algorithmic}
\end{algorithm}


\subsubsection{Experiments}
To compare the performance of the Sample Extractors, we followed a procedure similar to that described in \cref{par: pe}, computing the absolute estimation error for each extractor. For random sampling, the accuracy estimator was obtained via uniform averaging, whereas for IRT and RC it was obtained via weighted averaging. We evaluated the estimator in two different settings: (1) merging a math model with a language-tuned model (similar to the cross-lingual setting of \cref{sec: cross-lingual}) for several languages (Italian, German, Romanian, Dutch) and testing the extractor on the corresponding translations of \dataset{GSM8K} (see \cref{fig:extractor_language}), and (2) merging several math models and testing the extractor on the English version of \dataset{GSM8K} (see \cref{fig:extractor_merges}).

Focusing on \cref{fig:extractor_language}, we see that performance variability is somewhat higher (larger error bars) due to different language-specific datasets. Even so, Random sampling never falls behind IRT or RC, especially for small sample sizes. By the time the subset size reaches 50 or more examples, all three methods converge to comparable accuracy-error levels, underscoring the robustness of Random sampling. Instead, in \cref{fig:extractor_merges}, the trends are broadly similar for RC and Random sampling, while slightly worse for IRT.  Again, as the dataset sample size grows, overall error drops and the gap among methods narrows. 

To sum up, the Random sampler can sometimes lag slightly behind the more sophisticated IRT and RC. Nevertheless, neither of these methods has a clear advantage over the others. Given its simplicity and negligible overhead, the Random strategy stands out as a highly practical choice for dataset subsampling—especially when the marginal improvements of more complex methods do not clearly justify their added complexity.

\begin{figure}
    \includegraphics[width=\linewidth]{figures/accuracy_error_extract_languages.pdf}
    \caption{\textit{Extractors across Languages:} Absolute error of the estimated accuracy of Sample Extractors, averaged across merges of language-specific and English Math finetunings of \model{Mistral-7B-v0.1}, evaluated on translations of \dataset{GSM8K} and presented as a function of the number of dataset samples.}
    \label{fig:extractor_language}
\end{figure}

\begin{figure}
    \includegraphics[width=\linewidth]{figures/accuracy_error_extract_merges.pdf}
    \caption{\textit{Extractors across Merges:} Absolute error of the estimated accuracy of Sample Extractors, averaged across merges of English Math models based on \model{Mistral-7B-v0.1}, evaluated on \dataset{GSM8K} and presented as a function of the dataset sample size.}
    \label{fig:extractor_merges}
\end{figure}


\subsection{Estimation step} \label{app:estimation-step}

\subsubsection{Additional Experiment for Ability Estimator}
We report in \cref{fig:ability-estimator-comparison-all} the Euclidean distance between the estimated and ground-truth ability vectors across different sample sizes. The results are consistent with the case $n=10, 20$ seen in \cref{fig:ability-estimator-comparison}, with our estimated ability vector being significantly closer to the ground-truth one compared to the ability vector estimated by pIRT and gp-IRT. Similarly, we report the corresponding cosine similarity in \cref{fig:ability-estimator-comparison-cosine}, confirming much higher similarity in our case.  
%
\begin{figure}
    \includegraphics[width=\linewidth]{figures/latent_abilities_languages_appendix_eucl.pdf}
    \caption{\textit{Ability Estimator over languages:} Euclidean distance (lower is better) between estimated and true abilities for different languages.}
    \label{fig:ability-estimator-comparison-all}
\end{figure}
%
\begin{figure}
    \includegraphics[width=\linewidth]{figures/latent_abilities_languages_appendix_cosine.pdf}
    \caption{\textit{Ability Estimator over languages:} Cosine similarity (higher is better) between estimated and true abilities for different languages.}
    \label{fig:ability-estimator-comparison-cosine}
\end{figure}
%
\begin{figure}
    \includegraphics[width=\linewidth]{figures/latent_abilities_tasks_appendix.pdf}
    \caption{\textit{Ability Estimator over tasks:} Cosine similarity (higher is better) between estimated and true abilities for different tasks.}
    \label{fig:ability-estimator-comparison-cosine-tasks}
\end{figure}
%
\subsubsection{Additional Experiment for Performance Estimator} 
We report in \cref{fig:estimation_comparison_winogrande_hellaswag} the evaluation of performance estimators across \dataset{Winogrande} and \dataset{Hellaswag}, extending the results in \cref{fig:estimation_comparison}.

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/estimation_comparison_appendix.pdf}
    \caption{\textit{Performance Estimators over Winogrande and Hellaswag.} Absolute error of various estimators as a function of sample size (lower is better). gmp-IRT consistently achieves lower error.}
\label{fig:estimation_comparison_winogrande_hellaswag}
\end{figure}

\subsection{Evolve Step} 

\subsubsection{Additional Experiment for Multilingual Evolution: Self-Merging}
\label{app:add-exp-self-merging}

In this section, we present an ablation study to rigorously test whether the observed improvements in the merged models arise from genuine cross-lingual knowledge transfer or merely from fitting to the prompt template. To structure this analysis, we formalize our inquiry through two statistical hypotheses:

\paragraph{Null Hypothesis ($H_0$).} The improvements seen in the merged models are due to the model fitting itself on the prompt template, rather than any cross-lingual knowledge exchange.

\paragraph{Alternative Hypothesis ($H_1$).} The improvements arise from actual cross-lingual knowledge transfer---specifically, from the mathematical model to the linguistic model---and are not merely the result of fitting the prompt template.

To evaluate these hypotheses, we propose a \emph{self-merging} procedure. Concretely, we take the linguistic model and merge it with \emph{itself} using the standard \approach methodology outlined in \cref{alg:merge3}. Under $H_0$, if the improvements are solely due to the prompt template, merging the model with itself should lead to performance gains (i.e., the merged model would still ``fit'' the template). Conversely, under $H_1$, if cross-lingual knowledge transfer is responsible for the enhanced performance, self-merging should \emph{not} yield improvements. In fact, additional noise could even degrade performance relative to the baseline.

We conducted this self-merging experiment on the Italian model using the \dataset{GSM8K} dataset. The results, shown in \cref{fig:self-merge}, reveal that performance actually \emph{decreases} when the model is merged with itself. This observation strongly supports the alternative hypothesis ($H_1$): the performance gains in cross-lingual merges indeed stem from genuine knowledge transfer, rather than mere adaptation to a prompt template.

\begin{figure}
    \centering
    \includegraphics[width=.8\linewidth]{figures/self-merge.pdf}
    \caption{Accuracy of the base model (Mistral-7B), the Italian Endpoint (\texttt{IT}), Self-Merge and \approach models on the Italian-translated version of \dataset{GSM8k}.}
    \label{fig:self-merge}
\end{figure}

\subsubsection{Additional Experiment: Time \& FLOPs Requirements Evolutionary Merging}
\label{app:add-exp-4090}
\paragraph{Hardware Setting.} To compare the efficiency of different model evaluation strategies, we measured the time required to evolve merged LLM models using a single NVIDIA 4090 with 24GB of VRAM, and report the Throughput $R$ in \cref{tab:throughput_comparison}. 

\paragraph{Results Discussion.} Over a 12-hour period, we were able to evaluate 8 models on 1000 samples of \dataset{GSM8K}, allowing us to estimate that evaluating 1000 models would take approximately 62 days under similar conditions. In contrast, \approach enabled the evaluation of a larger number of merged models in significantly less time by using a reduced dataset. These results suggest that researchers and practitioners could leverage consumer-grade GPUs for efficient LLM merging and evaluation, making rapid experimentation of model merging methods more accessible. We report in \cref{tab:evolution_comparison} the estimated total time of the Evolve runs, which we calculated using the following formula:
%
\[
T(N_{\text{models}}) \;=\; \frac{N_{\text{models}}}{R_{\text{Dataset Size}}}
\]
%
\begin{table}[h]
    \centering
    \caption{Throughput (\( R \)) in models per hour for different sample sizes per fitness evaluation on \dataset{GSM8K}. These estimates are based on 12-hour Evolve runs on a single NVIDIA 4090 with 24GB of VRAM.}
    \label{tab:throughput_comparison}
    \vspace{10pt}
    \resizebox{\linewidth}{!}{
    \begin{tabular}{lccccc}
        \toprule
        \textbf{Sample size} & 1000 & 100 & 50 & 30 & 20 \\
        \midrule
        \textbf{Throughput (Models/Hour)} & 0.67 & 8.33 & 14.17 & 16.67 & 17.08 \\
        \bottomrule
    \end{tabular}
    }
\end{table}

\begin{table}[h]
    \centering
    \caption{Comparison of Evolve methods by number of trials, estimated total time on a single NVIDIA 4090, sample size used for Fitness computation, and final accuracy on \dataset{GSM8K}. The number of trials is the result of $\text{population size} \times \text{iterations}$, parameters of the Genetic Algorithms of each method, and represents the total number of merged models evaluated during the entire Evolve run.}
    \label{tab:evolution_comparison}
    \vspace{10pt}
    \resizebox{\linewidth}{!}{
    \begin{tabular}{lcccc}
        \toprule
        \textbf{Method} & \textbf{$N_{\text{models}}$} & \textbf{Estimated total time} & \textbf{Sample size} & \textbf{Accuracy} \\
        \midrule
        EvoLLM-JP-7B & 1000 & 62 days & 1000 & 0.49 \\
        MERGE$^3_{100}$ & 175 & 21h & 100 & 0.42 \\
        MERGE$^3_{50}$ & 175 & 12h 20m & 50 & 0.38 \\
        MERGE$^3_{30}$ & 175 & 10h 30m & 30 & 0.38 \\
        MERGE$^3_{20}$ & 175 & 10h 15m & 20 & 0.34 \\
        \bottomrule
    \end{tabular}
    }
\end{table}


\paragraph{FLOPs Calculation.} We provide a Jupyter Notebook that describes the FLOPs calculations for our experiments in the supplementary material, based on the \textit{calc-flops} library\footnote{ \url{https://github.com/MrYxJ/calculate-flops.pytorch}.}. This script has been used to estimate the FLOPs for the experiment in \Cref{fig:speed_accuracy}.