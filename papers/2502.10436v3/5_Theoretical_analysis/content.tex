\label{sec:theory}

In this section, we provide theoretical guarantees for our \textit{performance estimator}, demonstrating that its estimated accuracy is a reliable approximation of full-dataset accuracy. We provide formal guarantees for its performance, analyze its stability under dataset reduction, and explain why it remains a robust proxy for the true fitness of the merged models. 
This analysis not only solidifies the estimator's theoretical foundation but also offers practical insights into its behavior in finite-data and asymptotic regimes.

The section is structured as follows: first (\S \ref{subsec:eps-stability}), we derive a correlation between the accuracy of the performance estimator and the quality of the minimum found by solving an optimization problem using that performance estimator as objective function; second (\S \ref{subsec:mp-irt-theory}), we study the asymptotic properties of the performance estimator as the dataset size approaches infinity, formalizing it as an unbiased estimator; and finally (\S \ref{subsec:mp-irt-eps-stability-analysis}), we demonstrate that our performance estimator behaves in expectation within a \(\,\epsilon\)-bound of the accuracy on the true optimum dataset. The proofs for all the theorems and propositions presented below are outlined in \cref{app:proofs}.


\subsection{Part~I: \texorpdfstring{\(\epsilon\)}{ε}-Stable Estimators and 
\texorpdfstring{\(\epsilon\)}{ε}-Optimality Preservation}
\label{subsec:eps-stability}

We first consider a performance metric \(F(\theta;D)\) for 
\(\theta \in \Theta \subset \mathbb{R}^n\), where \(D\) is a dataset.  
If we choose a smaller subset \(\bar{D} \subset D\) to approximate this metric, 
denoted \(F(\theta;\bar{D})\), we wish to control the loss in optimality incurred 
by replacing \(F(\theta;D)\) with \(F(\theta;\bar{D})\).  

\begin{definition}[\(\epsilon\)-Stability.]
    Given two datasets \(D\) and \(\bar{D}\), we say \(F(\cdot;\bar{D})\) is 
    \emph{\(\epsilon\)-stable with respect to} \(F(\cdot;D)\) if, for all 
    \(\theta \in \Theta\),
    \[
      \bigl|F(\theta;D)\;-\;F(\theta;\bar{D})\bigr|\;\le\;\epsilon
    \]
\end{definition}

Under this condition, minimizing \(F(\cdot;\bar{D})\) yields an objective value 
within \(\epsilon\) of minimizing \(F(\cdot;D)\).  Formally:

\begin{theorem}[\(\epsilon\)-Optimality Preservation]
\label{thm:eps-opt-preserve}
Let \(D\) be a dataset, let \(\bar{D}\subset D\) be a subset, and let 
\(F(\cdot;\bar{D})\) be \(\epsilon\)-stable with respect to \(F(\cdot;D)\), 
with a fixed \(\epsilon>0\).  Define
\[
  \theta^\star
  \;=\;
  \arg\!\min_{\theta\in \Theta}\;F(\theta;D)
  \quad\text{and}\quad
  \hat{\theta}
  \;=\;
  \arg\!\min_{\theta\in \Theta}\;F(\theta;\bar{D})
\]
Then
\[
    \bigl|F(\theta^\star;D)\;-\;F(\hat{\theta};\bar{D})\bigr|
    \;\le\;
    \epsilon
\]
\end{theorem}
%
Thus, \(\epsilon\)-stability ensures that any global minimizer on \(\bar{D}\) 
achieves an objective value on \(D\) no worse than \(\epsilon\) from 
the true global optimum.
%
% \paragraph{\texorpdfstring{\(\epsilon\)}{ε}-Stability in Expectation.}
Nevertheless, uniformly bounding \(\bigl|F(\theta;D)-F(\theta;\bar{D})\bigr|\) for all \(\theta\) may be too strong in practice. For this reason, we introduce:
\begin{definition}[\(\epsilon\)-Stability in expectation]\label{subsec:eps-stability-expectation}
    Given two datasets \(D\) and \(\bar{D}\), we say \(F(\cdot;\bar{D})\) is 
    \emph{\(\epsilon\)-stable in expectation with respect to} \(F(\cdot;D)\) if
    \[    \mathbb{E}_{\bar{D}}\bigl[\bigl|F(\theta;D)\;-\;F(\theta;\bar{D}) \bigr|\bigr]
      \;\le\;\epsilon
    \]
    where the expectation is over the (random) choice of \(\bar{D}\)
\end{definition}
%
Under this  relaxed notion, we still obtain a similar control on the \emph{expected} suboptimality gap:
%
\begin{theorem}[Expected \(\epsilon\)-Stability of the Minimum]
\label{thm:expected-eps-stability}
Suppose \(F(\cdot;\bar{D})\) is \(\epsilon\)-stable in expectation with respect 
to \(F(\cdot;D)\).  Let 
\[
  m^\star \;:=\;\min_{\theta\in\Theta}\,F(\theta;D)
  \quad\text{and}\quad
  \widehat{m}(\bar{D}) \;:=\;\min_{\theta\in\Theta}\,F(\theta;\bar{D})
\]
Then
\[
  \bigl|
    m^\star
    \;-\;
    \mathbb{E}_{\bar{D}}\bigl[\widehat{m}(\bar{D})\bigr]
  \bigr|
  \;\le\;
  \epsilon
\]
\end{theorem}

Hence, even if stability only holds \emph{on average}, the expected gap between 
the global optimum on \(D\) and the optimum on \(\bar{D}\) remains at most \(\epsilon\).

\subsection{Part~II: Theoretical Guarantees for \texorpdfstring{\mpirt{}}{mp-IRT}}
\label{subsec:mp-irt-theory}

We now apply these ideas to our proposed \mpirt{} estimator  (cf.\ \S\ref{sec:estimate}).  We first show that \mpirt{} is asymptotically  unbiased, and then combine this fact with \Cref{thm:expected-eps-stability}  to argue that \mpirt{}-based minimizers remain close to those that minimize  the full-dataset performance measure.

\paragraph{Asymptotic unbiasedness.}
\label{par:asymptotic-consistency}
The following proposition establishes that, as \(\bar{D}\) grows,  \(\hat{Z}^{\mathrm{mp\text{-}IRT}}\) converges in probability to the true performance \(Z\). Its proof relies on classical limit arguments for unbiased estimators.

\begin{proposition}[Asymptotic unbiasedness of \mpirt{}]
\label{prop:estimator-unbiased}
Assume:
\begin{enumerate*}[label=(\roman*)]
    \item \(\hat{\lambda} \to \lambda\) in probability as 
    \(\lvert \hat{I}\rvert \to \infty\),
    \item for each \(i\in I\), the true values \(\alpha_i,\beta_i,\theta_1,\theta_2\) 
          are known, with \(\sup_{i\in I}\|\alpha_i\|_2 \le c\) for a fixed \(c\),
    \item linear inheritance of abilities 
          (cf.\ \Cref{conj:latent-abilities-linear}) holds.
\end{enumerate*}
Then, for all \(j,l\),
\begin{align*}
    &\Bigl|\mathbb{E}\bigl[\hat{Z}_{jl}\,\bigm|\,
      Y_{i_0l},\dots,Y_{i_kl}
    \bigr]
    \;-\;
    \mathbb{E}\bigl[Z_{jl}\,\bigm|\,
      Y_{i_0l},\dots,Y_{i_kl}
    \bigr]
  \Bigr| \;\to\; 0
\end{align*}
\end{proposition}
in probability as $\lvert \hat{I}\rvert \to \infty$.
Thus, for sufficiently large subsets \(\bar{D}\), the discrepancy between 
\(\hat{Z}_{\tilde{m}}\) and \(Z_{\tilde{m}}\) can be made arbitrarily small 
with high probability.

\subsection{Part~III: performance preservation via \mpirt{}}
\label{subsec:mp-irt-eps-stability-analysis}
%
We now conclude that \mpirt{} preserves near-optimality when we train on 
a suitably large \(\bar{D}\subset D\).  Since 
Proposition~\ref{prop:estimator-unbiased} asserts that \(\hat{Z}\) approximates 
\(Z\) well for large \(\lvert \bar{D}\rvert\), it follows (under mild conditions) 
that \(\mpirt{}\) remains \(\epsilon\)-stable in expectation.  Hence, 
\Cref{thm:expected-eps-stability} shows that minimizing \(\hat{Z}\) on 
\(\bar{D}\) yields, on average, a solution within \(\epsilon\) of the 
full-dataset optimum.

\begin{theorem}[Asymptotic performance preservation of \mpirt{}]
\label{thm:mpirt-asymptotic-eps-preserve}
Let \(\bar{D}\subset D\) be a random subset used to compute 
\(\hat{Z}^{\mathrm{mp\text{-}IRT}}\).  Suppose that, as \(\lvert \bar{D}\rvert\to\infty\), 
\(\hat{Z}^{\mathrm{mp\text{-}IRT}}\) converges in probability to \(Z\) (the true 
performance on \(D\)), and that \(\hat{Z}^{\mathrm{mp\text{-}IRT}}\) is 
\(\epsilon\)-stable in expectation for sufficiently large \(\lvert \bar{D}\rvert\).  
Then the expected global optimum of \(\hat{Z}^{\mathrm{mp\text{-}IRT}}\) on \(\bar{D}\) 
differs from that of \(Z\) on \(D\) by at most~\(\epsilon\).  As 
\(\lvert \bar{D}\rvert\to \infty\), \(\epsilon\to 0\).
\end{theorem}

\paragraph{Finite-sample analysis via the Law of Large Numbers.}
In practice, we rarely have \(\lvert \bar{D}\rvert\to\infty\).  Instead, one can 
appeal to \emph{expected} \(\epsilon\)-stability 
(\Cref{thm:expected-eps-stability}) and then \emph{estimate} the corresponding 
expectation empirically.  For instance, one may draw multiple subsets 
\(\bar{D}_1,\ldots,\bar{D}_S\) at random from \(D\) and compute 
\[
  \frac{1}{S}\,\sum_{s=1}^S 
  \Bigl\lvert F(\theta;D) \;-\; F(\theta;\bar{D}_s)\Bigr\rvert
\]
as an empirical approximation to 
\(\mathbb{E}_{\bar{D}}\bigl[\lvert F(\theta;D)-F(\theta;\bar{D})\rvert\bigr]\).
By the Law of Large Numbers, if this empirical average remains small (say, 
\(\approx \tilde{\epsilon}\)), then the true expectation is also small.  
Consequently, \Cref{thm:expected-eps-stability} implies that the optimal 
solution on each \(\bar{D}_s\) is within \(\tilde{\epsilon}\) of the global 
optimum on \(D\), on average.

\paragraph{Conclusion.}
In summary, \mpirt{} inherits asymptotic consistency from \(\mathrm{p\text{-}IRT}\) 
while requiring only a subset \(\bar{D}\subset D\).  By showing it is 
\(\epsilon\)-stable (in expectation) for large \(\lvert \bar{D}\rvert\), we conclude 
that \emph{optimizing on \(\bar{D}\) yields (on average) a solution close to the 
true optimum on \(D\).}  In finite-sample regimes, multiple random draws of 
\(\bar{D}\) can be used to empirically verify that the discrepancy remains small, 
thereby justifying the practical use of \mpirt{} on moderately sized subsets.