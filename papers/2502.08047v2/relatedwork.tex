\section{Related Work}
\begin{figure*}[t]

\begin{center}
\centerline{\includegraphics[width=0.9\linewidth]{figures/benchoverview.pdf}}
\vskip -0.1in
\caption{\textbf{\bench{}}: An illustration of our proposed real-world GUI benchmark. The left shows that for each task, \bench{} provides a user query, instructional video, and pre-actions. The pre-actions lead to different initial states. The key characteristic of our \bench{} is the various initial states of the same task to stimulate the real-world testing process. The right shows the software included in our benchmark and the interactions about testing the agents in our GUI environment.}
\label{fig:benchoverview}
\end{center}
\vskip -0.3in
\end{figure*}

\subsection{GUI Benchmarks}

GUI benchmarks are essential for evaluating the performance and robustness of GUI agents. For web applications, WebShop~\cite{webshop}, and WebArena~\cite{webarena} are two text-based GUI benchmarks, the GUI information is formatted in the text style which is limited to reflect the dynamic GUI state changes. In OS environments, OSWorld~\cite{OSWorld} is a comprehensive benchmark including various operating systems with real applications. Mobile benchmarks MobileAgent~\cite{wang2024mobile} and AppAgent~\cite{zhang2023appagent} propose two GUI benchmarks of mobile applications. Windows-related benchmarks like AssistGUI~\cite{assistgui} and WindowAgentArena~\cite{windowsagentarena} propose a list of real tasks in the Windows platform. 
However, these benchmarks primarily rely on a static testing process and do not adequately capture the complexity and dynamic nature of GUI environments. As a result, they are insufficient for comprehensively evaluating GUI agents. We summarize the differences between \bench{} and other related benchmarks in Tab. \ref{tab:benchmarkcompare}.

\subsection{GUI Agents}

Building upon MLLMs, GUI agents have the potential to solve various computer tasks to avoid repetitive work or as an AI assistant to help the user. CogAgent~\cite{hong2024cogagent} is a vision language model focused on GUI understanding to facilitate GUI navigation, while SeeClick~\cite{cheng2024seeclick} and SeeAct \cite{zheng2024seeact} focus on the GUI grounding for enhancing the task performance. MobileAgent~\cite{wang2024mobile} and AppAgent~\cite{zhang2023appagent} are proposed to design the agent on mobile device. Ferret-UI~\cite{you2025ferret} is another representative work focusing on enhancing the grounding ability in the IOS platform. AutoGLM~\cite{liu2024autoglmautonomousfoundationagents} propose the GUI modes capable of learning through autonomous environmental interactions by reinforcing existing GUI models. These agents have shown their ability in GUI understanding (e.g., GUI elements grounding) or action prediction but still face limitations in handling dynamic and complicated full GUI tasks. 
Therefore, to enhance GUI automation in dynamic environments, we propose \agent{} that improves adaptability in complex GUI settings and enables agents to effectively handle unpredictable interface changes.


\subsection{Critical Thinking in Agents}

Recent advancements in foundation models and agents, particularly in LLMs such as OpenAI-o1~\cite{openaio1} and Deepseek-R1~\cite{deepseekr1}, have increasingly incorporated thinking processes before providing answers to effectively handle challenging reasoning tasks. The LLM-based agents utilize \textit{verify-then-correct} process to evaluate and refine intermediate reasoning steps or outputs, ensuring logical coherence and consistency. One notable LLM-based agent framework, Reflexion~\cite{shinn2024reflexion}, demonstrates the effectiveness of self-reflection in solving complex tasks. Furthermore, Critic~\cite{gou2023critic} integrates external tools into the critique process, leveraging them to improve performance. Noticing the GUI task is lengthy and complicated, the \textit{verify-then-correct} process is highly suitable for the GUI scenario. Which is not only aims to enhance the reasoning performance but also indispensable to designing the key module Actor-Critic \cite{konda1999actorcritic} to ensure task completion.  A closely related work, AssistGUI~\cite{assistgui}, integrates a critical module only after the Actor module to evaluate action completion. Building upon it, we introduce two additional critical modules: Planner-Critic, applied after the Planner, and Step-Check, applied before the Actor. These two modules lead to a comprehensive and fundamental GUI agent framework \textbf{\agent{}} which will provide insights for future GUI agent design.