\section{Related Work}
\begin{figure*}[t]

\begin{center}
\centerline{\includegraphics[width=0.9\linewidth]{figures/benchoverview.pdf}}
\vskip -0.1in
\caption{\textbf{\bench{}:} An illustration of our proposed real-world GUI benchmark. The left shows that for each task, \bench{} provides a user query, instructional video, and pre-actions. The pre-actions lead to different initial states. The key characteristic of our \bench{} is the various initial states of the same task to stimulate the real-world testing process. The right shows the software included in our benchmark and the interactions about testing the agents in our GUI environment.}
\label{fig:benchoverview}
\end{center}
\vskip -0.3in
\end{figure*}

\subsection{GUI Benchmarks}

GUI benchmarks are essential for evaluating the performance and robustness of GUI agents. For web applications, WebShop Kulesza, "WebShop"__ WebArena Wong, "Web Arena"__ are two text-based GUI benchmarks, the GUI information is formatted in the text style which is limited to reflect the dynamic GUI state changes. In OS environments, OSWorld Li, Zhang, and Chen, "OS World Benchmark"__ is a comprehensive benchmark including various operating systems with real applications. Mobile benchmarks MobileAgent Kim, Lee, and Kim, "Mobile Agent Benchmark"__ and AppAgent Kim et al., "App Agent Benchmark"__ propose two GUI benchmarks of mobile applications. Windows-related benchmarks like AssistGUI Zhang et al., "Assist GUI Benchmark"__ and WindowAgentArena Liu, Wang, and Li, "Window Agent Arena Benchmark"__ propose a list of real tasks in the Windows platform. 
However, these benchmarks primarily rely on a static testing process and do not adequately capture the complexity and dynamic nature of GUI environments. As a result, they are insufficient for comprehensively evaluating GUI agents. We summarize the differences between \bench{} and other related benchmarks in Tab. \ref{tab:benchmarkcompare}.

\subsection{GUI Agents}

Building upon MLLMs, GUI agents have the potential to solve various computer tasks to avoid repetitive work or as an AI assistant to help the user. CogAgent Chen et al., "Cog Agent"__ is a vision language model focused on GUI understanding to facilitate GUI navigation, while SeeClick Zhang and Liu, "See Click Benchmark"__ and SeeAct Wang et al., "See Act Benchmark"__ focus on the GUI grounding for enhancing the task performance. MobileAgent Kim, Lee, and Kim, "Mobile Agent Benchmark"__ and AppAgent Kim et al., "App Agent Benchmark"__ are proposed to design the agent on mobile device. Ferret-UI Liu et al., "Ferret UI Benchmark"__ is another representative work focusing on enhancing the grounding ability in the IOS platform. AutoGLM Zhang, Chen, and Li, "Auto GLM Benchmark"__ propose the GUI modes capable of learning through autonomous environmental interactions by reinforcing existing GUI models. These agents have shown their ability in GUI understanding (e.g., GUI elements grounding) or action prediction but still face limitations in handling dynamic and complicated full GUI tasks. 
Therefore, to enhance GUI automation in dynamic environments, we propose \agent{} that improves adaptability in complex GUI settings and enables agents to effectively handle unpredictable interface changes.


\subsection{Critical Thinking in Agents}

Recent advancements in foundation models and agents, particularly in LLMs such as OpenAI-o1 Brown et al., "Open AI Model"__ and Deepseek-R1 Zhang et al., "Deep Seek Model"__, have increasingly incorporated thinking processes before providing answers to effectively handle challenging reasoning tasks. The LLM-based agents utilize \textit{verify-then-correct} process to evaluate and refine intermediate reasoning steps or outputs, ensuring logical coherence and consistency. One notable LLM-based agent framework, Reflexion Wang et al., "Reflexion Framework"__ demonstrates the effectiveness of self-reflection in solving complex tasks. Furthermore, Critic Liu et al., "Critic Model"__ integrates external tools into the critique process, leveraging them to improve performance. Noticing the GUI task is lengthy and complicated, the \textit{verify-then-correct} process is highly suitable for the GUI scenario. Which is not only aims to enhance the reasoning performance but also indispensable to designing the key module Actor-Critic Zhang et al., "Actor Critic Model"__ to ensure task completion.  A closely related work, AssistGUI Zhang et al., "Assist GUI Benchmark"__, integrates a critical module only after the Actor module to evaluate action completion. Building upon it, we introduce two additional critical modules: Planner-Critic, applied after the Planner, and Step-Check, applied before the Actor. These two modules lead to a comprehensive and fundamental GUI agent framework \textbf{\agent{}} which will provide insights for future GUI agent design.