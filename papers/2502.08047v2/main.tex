%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}
\usepackage{multirow}
\usepackage[table]{xcolor}
% \usepackage{xcolor}
\usepackage{soul}

\usepackage{tcolorbox}

% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}


% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

\usepackage{pifont}% http://ctan.org/pkg/pifont
\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%

\definecolor{graybg}{gray}{.92}
\definecolor{bgcolor}{RGB}{246,248,250}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

\newcommand\bench{WorldGUI} % DynoGUI, VersaGUI, RealWorldGUI
\newcommand\agent{GUI-Thinker}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}

\newcommand{\difei}[1]{\textcolor{blue}{#1 @difei}}

\definecolor{ForestGreen}{HTML}{228B22}
\newcommand{\revise}[1]{\textcolor{ForestGreen}{#1}}

% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{\bench{}: Dynamic Testing for Comprehensive Desktop GUI Automation}

\begin{document}

\twocolumn[
\icmltitle{\bench{}: Dynamic Testing for Comprehensive Desktop GUI Automation}
% Enhancing Desktop GUI Automation with Dynamic Testing Processes
% Robust Desktop GUI Automation in Dynamic Environments
% Dynamic Testing for Comprehensive Desktop GUI Automation
% Desktop Graphical User Interface Automation in Dynamic Testing Processes


% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
% \icmlsetsymbol{corr}{*}

\begin{icmlauthorlist}
\icmlauthor{Henry Hengyuan Zhao}{nus}
\icmlauthor{Difei Gao}{nus}
\icmlauthor{Mike Zheng Shou}{nus}

\end{icmlauthorlist}

\icmlaffiliation{nus}{Show Lab, National University of Singapore}
% \icmlaffiliation{comp}{Company Name, Location, Country}
% \icmlaffiliation{sch}{School of ZZZ, Institute of WWW, Location, Country}

\icmlcorrespondingauthor{Mike Zheng Shou}{mike.shou@nus.edu}
% \icmlcorrespondingauthor{Firstname2 Lastname2}{first2.last2@www.uk}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
% \printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}

Current Graphical User Interface (GUI) agents have achieved outstanding performance in GUI element grounding. However, planning remains highly challenging, especially due to sensitivity to the initial state of the environment. Specifically, slight differences in the initial state—such as the target software not being open or the interface not being in its default state—often lead to planning errors. This issue is widespread in real application scenarios, but existing benchmarks fail to evaluate it. In this paper, we present \bench{}, a novel GUI benchmark that designs GUI tasks with various initial states to simulate real computer-user interactions. The benchmark spans a wide range of tasks across 10 popular software applications, including PowerPoint, VSCode, and Adobe Acrobat. In addition, to address the challenges of dynamic GUI automation tasks, we propose \agent{}, a holistic framework, leveraging a critique mechanism, that effectively manages the unpredictability and complexity of GUI interactions. Experimental results demonstrate that \agent{} significantly outperforms Claude-3.5 (Computer Use) by 14.9\% in success rate on \bench{} tasks. This improvement underscores the effectiveness of our critical-thinking–based framework in enhancing GUI automation. The code is available at \href{https://github.com/showlab/WorldGUI}{https://github.com/showlab/WorldGUI}

\end{abstract}

\section{Introduction}

Graphical User Interface (GUI) automation has become a prominent research area, driven by the need to enhance user productivity. This domain encompasses software usage, file management, office design, coding, and web browsing. 
Building upon Multimodal Large Language Models (MLLMs) such as GPT-4o~\cite{gpt4o} and Claude-3.5 \cite{claude3.5}, GUI agents have the potential to solve various computer tasks to avoid repetitive work or as an AI assistant to help the user. 

\begin{figure}[t]
% \vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=0.86\linewidth]{figures/testprocess.pdf}}
\caption{The comparison of static and dynamic testing processes. Our \bench{} takes the first step to facilitate comprehensive GUI evaluation with various initial states. The red node represents an incorrect state.}
\label{fig:testprocess}
\end{center}
\vskip -0.4in
\end{figure}


GUI automation operates in a dynamic environment, which goes beyond the traditional computer vision tasks like image recognition \cite{he2016deep} and visual question answering \cite{antol2015vqa}. However, current GUI benchmarks such as OSWorld \cite{OSWorld} and WebArena \cite{webarena} do not capture this dynamism. As shown on the left side of Fig. \ref{fig:testprocess}, most GUI benchmarks focus on initial and final states, measuring success rates but overlooking the changing initial conditions present in real GUI scenarios. These benchmarks often ignore situations where: (1) The software interface is not in its default state. (2) The agent might get user queries at any time. (3) Differences in agent robustness, where agents with the same low success rate (e.g. 20\%) may vary in their ability to self-verify or self-correct, but these abilities are not measured in a static setting. As a result, these benchmarks fail to fully assess the capabilities of GUI agents.


In this paper, we take the first step toward comprehensive GUI agent testing by designing GUI tasks with various initial states. As illustrated on the right side of Fig. \ref{fig:testprocess}, the testing process of \bench{} can be featured: \textbf{(1) Intermediate Starting States}: Real user interactions with GUI assistants do not always begin from default conditions, allowing tasks to start from intermediate states where users may seek assistance at any point (see Fig. \ref{fig:testprocess} (b)). \textbf{(2) Contextual Variability}: In some cases, tasks may originate from entirely different contexts or interfaces, requiring the agent to adapt by modifying existing steps (see Fig. \ref{fig:testprocess} (c)) or introducing new steps (see Fig. \ref{fig:testprocess} (d)) to ensure task progression. By incorporating these elements, \bench{} better mirrors real-world GUI environments, enabling a more accurate and thorough assessment of GUI agent capabilities. 



Specifically, we present \bench{}, a new benchmark featuring 10 desktop software applications and 315 tasks, including PowerPoint, Word, Excel, VSCode, etc. For each task, we provide a user query, an instructional video, and the corresponding project file. To ensure the task quality, we engaged four trained annotators skilled in using these applications and proficient in constructing data with scripts and agents for annotation. To stimulate the dynamic testing scenarios, we demonstrate each task to obtain ground-truth (GT) plans and then conduct the augmentations for each task using pre-actions. (See details in Sec. \ref{sec:benchmark}.)


In addition, we introduce a novel GUI agent framework, \agent{}, which builds upon critical thinking philosophy, an aspect less emphasized in previous GUI agents \cite{hong2024cogagent, cheng2024seeclick, autowebglm, agents, osatlas}. In dynamic GUI environments, application settings may not be in default configurations. This unpredictability requires agents to have the essential ability to detect and adapt to such changes to ensure task accuracy. Through our analysis of real-world GUI scenarios, we identify three critical designs for a comprehensive agent: \textbf{(1) Post-Planning Critique}, \textbf{(2) Pre-Execution Validation}, and \textbf{(3) Post-Action Evaluation}. Specifically, \agent{} comprises five core components: Planner, Planner-Critic, Step-Check, Actor, and Actor-Critic. We argue that these components are fundamental for effective GUI agents. 

To summarize, our key contributions are the following: \textbf{(1)} We are the first to stress the dynamic testing processes in GUI automation and propose a new benchmark \bench{} which designs the GUI tasks with various initial states to simulate the real interactions; \textbf{(2)} We introduce \agent{}, a comprehensive GUI framework. \agent{} incorporate the thinking into the overall agent design, which provides valuable insights and guidance for future development; 
\textbf{(3)} We explore the essential property of critical thinking in GUI agents and empirically show that critical thinking is extremely useful for handling complex tasks.

\section{Related Work}

\begin{figure*}[t]

\begin{center}
\centerline{\includegraphics[width=0.9\linewidth]{figures/benchoverview.pdf}}
\vskip -0.1in
\caption{\textbf{\bench{}}: An illustration of our proposed real-world GUI benchmark. The left shows that for each task, \bench{} provides a user query, instructional video, and pre-actions. The pre-actions lead to different initial states. The key characteristic of our \bench{} is the various initial states of the same task to stimulate the real-world testing process. The right shows the software included in our benchmark and the interactions about testing the agents in our GUI environment.}
\label{fig:benchoverview}
\end{center}
\vskip -0.3in
\end{figure*}

\subsection{GUI Benchmarks}

GUI benchmarks are essential for evaluating the performance and robustness of GUI agents. For web applications, WebShop~\cite{webshop}, and WebArena~\cite{webarena} are two text-based GUI benchmarks, the GUI information is formatted in the text style which is limited to reflect the dynamic GUI state changes. In OS environments, OSWorld~\cite{OSWorld} is a comprehensive benchmark including various operating systems with real applications. Mobile benchmarks MobileAgent~\cite{wang2024mobile} and AppAgent~\cite{zhang2023appagent} propose two GUI benchmarks of mobile applications. Windows-related benchmarks like AssistGUI~\cite{assistgui} and WindowAgentArena~\cite{windowsagentarena} propose a list of real tasks in the Windows platform. 
However, these benchmarks primarily rely on a static testing process and do not adequately capture the complexity and dynamic nature of GUI environments. As a result, they are insufficient for comprehensively evaluating GUI agents. We summarize the differences between \bench{} and other related benchmarks in Tab. \ref{tab:benchmarkcompare}.

\subsection{GUI Agents}

Building upon MLLMs, GUI agents have the potential to solve various computer tasks to avoid repetitive work or as an AI assistant to help the user. CogAgent~\cite{hong2024cogagent} is a vision language model focused on GUI understanding to facilitate GUI navigation, while SeeClick~\cite{cheng2024seeclick} and SeeAct \cite{zheng2024seeact} focus on the GUI grounding for enhancing the task performance. MobileAgent~\cite{wang2024mobile} and AppAgent~\cite{zhang2023appagent} are proposed to design the agent on mobile device. Ferret-UI~\cite{you2025ferret} is another representative work focusing on enhancing the grounding ability in the IOS platform. AutoGLM~\cite{liu2024autoglmautonomousfoundationagents} propose the GUI modes capable of learning through autonomous environmental interactions by reinforcing existing GUI models. These agents have shown their ability in GUI understanding (e.g., GUI elements grounding) or action prediction but still face limitations in handling dynamic and complicated full GUI tasks. 
Therefore, to enhance GUI automation in dynamic environments, we propose \agent{} that improves adaptability in complex GUI settings and enables agents to effectively handle unpredictable interface changes.


\subsection{Critical Thinking in Agents}

Recent advancements in foundation models and agents, particularly in LLMs such as OpenAI-o1~\cite{openaio1} and Deepseek-R1~\cite{deepseekr1}, have increasingly incorporated thinking processes before providing answers to effectively handle challenging reasoning tasks. The LLM-based agents utilize \textit{verify-then-correct} process to evaluate and refine intermediate reasoning steps or outputs, ensuring logical coherence and consistency. One notable LLM-based agent framework, Reflexion~\cite{shinn2024reflexion}, demonstrates the effectiveness of self-reflection in solving complex tasks. Furthermore, Critic~\cite{gou2023critic} integrates external tools into the critique process, leveraging them to improve performance. Noticing the GUI task is lengthy and complicated, the \textit{verify-then-correct} process is highly suitable for the GUI scenario. Which is not only aims to enhance the reasoning performance but also indispensable to designing the key module Actor-Critic \cite{konda1999actorcritic} to ensure task completion.  A closely related work, AssistGUI~\cite{assistgui}, integrates a critical module only after the Actor module to evaluate action completion. Building upon it, we introduce two additional critical modules: Planner-Critic, applied after the Planner, and Step-Check, applied before the Actor. These two modules lead to a comprehensive and fundamental GUI agent framework \textbf{\agent{}} which will provide insights for future GUI agent design.





\section{\bench{} Benchmark}
\label{sec:benchmark}

\subsection{Task Formulation} 
\textbf{GUI Automation Definition.} The GUI automation task can be considered a partially observable Markov decision process (POMDP) $(\mathcal{S}, \mathcal{O}, \mathcal{A}, \mathcal{T}, \mathcal{R})$ with state space $\mathcal{S}$, observation $\mathcal{O}$, action space $\mathcal{A}$, transition function $\mathcal{T}$: $\mathcal{S} \times \mathcal{A} \rightarrow \mathcal{S}$, and reward function $\mathcal{R}$: $\mathcal{S} \times \mathcal{A} \rightarrow \mathbb{R}$. In our setting, given a natural language query $q$, eg., \textit{Format the slide background with gradient fill} that describes a specific task in high-level, along with an instructional video $v$ as a supplement that more detailed illustrates how to complete it, the agent first get the observation $o_{t} \in \mathcal{O}$ from the state $s_t \in \mathcal{S}$ in the execution environment and then generate the executable action $a_t \in \mathcal{A}$, resulting in a new state $s_{t+1} \in \mathcal{S}$ and a new observation $o_{t+1} \in \mathcal{O}$. The process repeats until the task is finished or failed. The reward function $\mathcal{R}$: $\mathcal{S} \times \mathcal{A} \rightarrow [0, 1]$ here returns a binary integer at the final step indicating the task completion status. 

\textbf{\bench{} Task Definition.} 
As illustrated in Fig.~\ref{fig:benchoverview}, each GUI task is paired with a user query and an instructional video. To achieve state diversity within each task, we generate various initial states that converge to the same final state, resulting in distinct ground truth (GT) plans for each case. This is accomplished through the use of \textbf{pre-actions}, which consist of a sequence of executable code to initialize tasks from different initial states. With the augmentation of initial states, \bench{} is capable of mimicking the different testing scenarios as shown in Fig. \ref{fig:testprocess}.


\textbf{Observation Space.} The observation space $\mathcal{O}$ indicates the information of the operating system (OS) available to the agent in each state $s_t$. In our \bench{} settings, we follow the previous work AssistGUI~\cite{assistgui}, encompassing two types of information: metadata $m_t$ from the application and screenshot $V_t$ of current state $s_t$. The metadata mainly includes the layout of panels and pop-up windows. The screenshot $V_t$ offers holistic visual information of the current state used for planning and action generation.


\textbf{Action Space.} Our action space includes all raw mouse and keyboard actions, such as left-click, right-click, double-click, drag, keystrokes, and key combinations for shortcuts, among others. Mouse-related actions also specify the target position in the pixel space of the observed screenshot. To ensure a universal and comprehensive representation of actions, we adopted the widely used Python library, PyAutoGUI, for controlling mouse and keyboard inputs. Each action is represented using the syntax \texttt{action\_type(arguments)} as in Tab.~\ref{tab:actionspace}.


\subsection{Data Collection}

\subsubsection{Data Source}

\bench{} consists of a broad spectrum of desktop applications, which can be categorized into five main groups: \textit{(i)} Office work, includes PowerPoint, Word, Excel, and Adobe Acrobat; \textit{(ii)} Windows Usage, includes System Settings and File Management; \textit{(iii)} Web Browsing, includes the configuration of Youtube and website operations; \textit{(iv)} Coding, focus on the customization, configuration and editing of Visual Studio Code (VSCode); \textit{(v)} Media Creation, uses the AI tool like Stable Diffusion to create or edit the image or videos by following user instruction.


\begin{figure*}[t]
\begin{center}
\centerline{\includegraphics[width=0.88\linewidth]{figures/agentoverview.pdf}}
\vskip -0.1in
\caption{\textbf{\agent{}}. An overview of \agent{}, includes five proposed components: Planner, Planner-Critic, Step-Check, Actor, and Actor-Critic. The Planner module receives the user query and an instructional video as input and generates an initial plan for the Planner-Critic process. This plan is then refined and executed step by step. Before each step is passed to the Actor module, it undergoes a Step-Check. After the Actor produces an action, the Actor-Critic module iteratively verifies the completion of the action and makes corrections if needed.}
\label{fig:agentoverview}
\end{center}
\vskip -0.3in
\end{figure*}

\subsubsection{Pipeline of Data Construction} 

For each task 1) we manually collect the raw videos from the website, 2) then manually cut the raw lengthy videos into sub-clips, 3) ask annotators to manually write the user queries, 4) prepare the project files for each task for reproducibility, 5) generate the ground-truth plans by executing the scripts and agent, 6) conduct the data augmentation by varying the initial state for each task. As shown in Fig.~\ref{fig:datapipeline}, to achieve the above six steps, we invite four annotators and write the necessary scripts to help structure and format the data. Additionally, for the GT plan generation and pre-action generation, we build simple agents to obtain the data.

\textbf{Raw Video Collection.} We collect raw videos from the YouTube website as there are a lot of high-quality tutorials for desktop applications. For each software, we ask the annotators to watch the videos and then download them using different software.


\textbf{Instruction Video Preparation.} After obtaining the raw videos, we write the script codes to cut the lengthy and noisy videos into the sub-clips (30 seconds to 3 minutes) that serve as the instructional video.

\textbf{User Query Generation.} After obtaining the instructional videos, annotators are asked to manually write user queries corresponding to each video. For example, a user query for a task involving File Explorer might be: \textit{``Rename all the files by deleting 'class' from their names.''}

\textbf{Project File Preparation.} Following the AssistGUI~\cite{assistgui}, we create the project file for each task to ensure reproducibility without relying on resource-intensive virtual machines~\cite{OSWorld} or Docker environments~\cite{windowsagentarena}. This approach guarantees that the testing process begins from a consistent state. When combined with pre-actions, it enables augmentation of the same task with various initial states.

\textbf{GT Plan Generation.} We write the script to accept user query $q$ and instructional video $v$ as input and generate the raw plans by agent (powered by GPT-4o). Since the raw plans are not flawless, annotators are asked to watch the videos and manually execute the tasks following the raw plans. During this process, annotators edit the plans to correct any inaccurate steps or descriptions, ultimately producing the finalized GT plans.

\textbf{Pre-Actions Generation.} To vary the task, we propose introducing pre-actions before the task begins. These pre-actions are created by annotators and involve corresponding scripts and agents. They are written in Python code, for example:

\texttt{from pyautogui import click, rightClick\textbackslash n rightClick(800,400)}. 
The pre-actions primarily serve two purposes: \textbf{1) Simulating Intermediate Task States}: Pre-actions can complete specific steps of a task, creating a starting point from an intermediate state. This approach addresses scenarios where users may seek AI assistance because they are unable to complete a task. For example, if the task involves opening a dropdown menu, the pre-action may pre-open the menu. If the agent fails to recognize this precondition and follows its plan to click the menu again, it might inadvertently close the menu, causing task failure.
\textbf{2) Introducing Diverse Initial Context States}: Pre-actions can also introduce variations in the initial state, such as opening random tabs or settings. This ensures that the starting state is unconventional, challenging the agent to adapt by modifying its plan or adding new steps. See details in Fig.~\ref{fig:dataaugment}.

\subsection{Evaluation} \bench{} employs an execution-oriented evaluation approach by utilizing post-processing scripts to assess task completion. Specifically, for tasks like Office work and Web Browsing, we adopt exact matching to compare the differences between the ground-truth (GT) screenshots and the final screenshots. For tasks like File Management which would produce new folders or change the locations of files, etc. We create the shell script to check the status of files.

\subsection{Data Statistics}

\bench{} compiles GUI tasks from 10 widely used applications on the Windows platform, including productivity software such as PowerPoint, Excel, and VSCode. A total of 107 meta tasks were collected from these applications, with each task being augmented 0 to 3 times based on its specific content, resulting in 208 augmented tasks. In total, \bench{} comprises 315 tasks. See the details in Sec. \ref{sec:dataappendix}.



\section{GUI-Thinker: Thinking before Doing}

In this section, we introduce a new comprehensive GUI framework \textbf{\agent{}} with a core and essential mechanism: \textit{critical thinking}, which is vital for designing GUI agents capable of handling dynamic environments that have been overlooked in prior GUI agents ~\cite{hong2024cogagent, cheng2024seeclick, lin2024showui, zhang2023appagent, agents}. The \agent{} includes the \textbf{five fundamental but essential components} as in Fig. \ref{fig:agentoverview} and an \textbf{Interaction reasoning loop} detailed in Algorithm \ref{alg:reasoningloop}. 
We summarize our design principles in the following:

\textbf{$\bullet$ Post-Planning Critique:} After the planning phase, a critique module verifies and, if necessary, self-corrects the generated plans to ensure their accuracy.

\textbf{$\bullet$ Pre-Execution Validation:} Before executing each subtask, a validation module determines whether the subtask should be executed. This step is crucial, as the current GUI environment may indicate that the subtask is unnecessary or requires modification to align with the current state conditions.

\textbf{$\bullet$ Post-Action Evaluation:} After each action execution, a mechanism evaluates whether the action was successfully completed before proceeding to the next subtask.

These critique mechanisms ensure the reliability and adaptability of \agent{} in complex GUI environments.


\subsection{State-Aware Planner} 
The State-Aware Planner processes the instructional video $v$ and user query $q$ generates an initial plan as shown in the left of Fig.~\ref{fig:plannercritic}. We use the speech recognition model Whisper \cite{radford2023robust} to translate the video $v$ into the subtitle and then send it to the MLLM for task planning. The task plan is hierarchically structured as $p=[p_1, p_2, ..., p_N]$ where $p_i$ is a text string describing the $i$-th milestone of the task. Under each $p_i$, there is a list of subtasks $[S^i_1, S^i_2, S^i_N]$, where $S^i_j$ is the $j$-th subtask in the $i$-th milestone. To ensure the produced plans fit the GUI environment, we propose incorporating an initial screenshot $V_0$ to represent the current state. This additional context allows the agent to output plans that align with the actual state. For example, if the instructional video suggests clicking on the ``Layout'' tab in the Word application, but the current state (as indicated by the screenshot) shows that the ``Layout'' tab is already selected, there is no need to perform this action again. By utilizing the visual information from the screenshot, the State-Aware Planner can modify the plans accordingly, rather than strictly following the guidance in the instructional video or the existing knowledge from backbone MLLMs.


\begin{figure}[t]
\begin{center}
\centerline{\includegraphics[width=0.86\linewidth]{figures/plannercritic.pdf}}
% \vskip -0.1in
\caption{State-Aware Planner and Planner-Critic. The Planner generates an initial plan. Then, the Planner-Critic will assess the correctness of the plan and provide necessary corrections.}
\label{fig:plannercritic}
\end{center}
\vskip -0.3in
\end{figure}


\subsection{Planner-Critic}
\textbf{Post-Planning Critique.} The goal of the Planner-Critic is to assess the correctness of the initial plans generated by the State-Aware Planner and provide corrections if needed. This module is designed to ensure the accuracy of the plans while leveraging the self-critique capabilities of MLLMs. As illustrated in Fig.~\ref{fig:plannercritic}, for each Initial Plan, the output consists of four components: 

(1) \verb|<Flag>|: Indicates whether the Initial Plan is correct.

(2) \verb|<Feedback>|: Identifies the error type, categorized into one of three groups: ``Wrong Steps,'' ``Missing Steps,'' or ``Redundant Steps.''

(3) \verb|<Correction>|: Provide the corrected plans if the Flag indicates that the Initial Plan is incorrect.

(4) \verb|<Reason>|: In addition to giving the corrected plans, we force the model to give the reasons. As related works, CoT~\cite{wei2022chain}, GPT-o1~\cite{openaio1}, and Deepseek-R1~\cite{deepseekr1} demonstrate that generating reasoning steps along with the answer would enhance the performance.

\begin{figure}[t]
\begin{center}
\centerline{\includegraphics[width=0.86\linewidth]{figures/stepcheck.pdf}}
\caption{Step-Check. This module first checks the step completion status via an MLLM, and then based on the output navigates current task processing.}
\label{fig:stepcheck}
\end{center}
\vskip -0.3in
\end{figure}

\begin{figure}[t]
\begin{center}
\centerline{\includegraphics[width=0.86\linewidth]{figures/actorcritic.pdf}}
\caption{Actor-Critic. This module includes two parts: task verification and task correction. The design follows the \textit{verify-then-correct} mechanism. We set an internal loop in this module to iteratively correct the actions.} 
\label{fig:actorcritic}
\end{center}
\vskip -0.3in
\end{figure}


\subsection{Step-Check} 

\textbf{Pre-Execution Validation.} After the plan assessment, a navigation mechanism is crucial before sending each subtask $S_t=S_j^i$ at the time step $t$ to the Actor module. To address this, we designed a new module called Step-Check. Through extensive investigation, we discovered that during GUI task testing, perfect execution plans are rarely feasible due to the unpredictable nature of real application environments. Most software retains user preferences (e.g., remember the last configuration of user), meaning that when executing a specific task, the plan $p$ generated by the Planner might not align with the actual state of the software. Therefore, the model must determine whether to proceed with a subtask $S_t$ based on the current state (screenshot: $V_t$, metadata: $M_t$). 

As illustrated in Fig.~\ref{fig:stepcheck}, we employ an MLLM to determine whether the current task has been completed or requires modification. We systematically categorize the possible outcomes into four types: 

(1) \verb|<Modify>|: Indicates that the subtask should be modified or additional subtasks should be added. 

(2) \verb|<Pass>|: Indicates that the current subtask is unnecessary and can be skipped. 

(3) \verb|<Continue>|: Indicates that the subtask is valid and should be executed as planned. 

(4) \verb|<Finished>|: Indicates that the subtask has already been completed and requires no further action.

In cases where the screenshot does not provide sufficient visual information for the MLLM to determine the output, the model outputs \textbf{``$\#$Cannot confirm''}. When this occurs, we design a \textbf{Region Search} module implemented by an LLM. This module takes the corresponding GUI information extracted by the GUI parser and the task description of the current subtask to identify the relevant region. It then crops the region using the generated bounding box as the center coordinate, with the maximum width and height set to half of the original screenshot dimensions (ensure the region is smaller than the origin screenshot). The cropped screenshot is subsequently sent to the Step-Check module to regenerate the decision. 


\begin{table*}[!t]
    \centering
    \scalebox{0.8}{
    \begin{tabular}{ll|cc|cc|cc|cc|cc|c}
    \toprule
    \multirow{2}{*}{\textbf{Method}} & \multirow{2}{*}{\textbf{MLLM}} & \multicolumn{2}{c}{\textbf{Office}} & \multicolumn{2}{c}{\textbf{Win. Usage}} & \multicolumn{2}{c}{\textbf{Web}} & \multicolumn{2}{c}{\textbf{Coding}} & \multicolumn{2}{c|}{\textbf{Media}} & \multirow{2}{*}{\textbf{Overall}}\\
    & & Meta & Aug. & Meta & Aug. & Meta & Aug. & Meta & Aug. & Meta & Aug. & \\
    \midrule
    % \textbf{Plan-Act} & GPT-4o \\
    \textbf{AssistGUI} & GPT-4o & 26.7 & 26.4 & 34.8 & 21.4 & 33.3 & 20.5 & 36.4 & 27.3 & 42.9 & 21.4 & 26.7\\
    \textbf{Computer Use} & Claude-3.5 & 28.9 & 22.0 & 30.4 & 23.8 & 71.4 & 46.2 & 54.5 & 31.8 & 42.9 & 21.4 & 32.4\\
    \midrule
    \rowcolor{graybg} \textbf{GUI-Thinker} & GPT-4o & 44.4 & 31.9 & 34.8 & 21.4 & 47.6 & 38.5 & 45.5 & 27.3 & 57.1 & 50.0 & 36.2 \\
    \rowcolor{graybg} \textbf{GUI-Thinker} & Claude-3.5 & 57.8 & 41.8 & 43.5 & 23.8 & 76.2 & 48.7 & 63.6 & 40.9 & 57.1 & 57.1 & 47.3 \\
    \midrule
    \textbf{Human$^*$} & -- & 88.9 & 85.7 & 100.0 & 97.6 & 95.2 & 82.1 & 81.8 & 81.8 & 71.4 & 71.4 & 87.6 \\
    \bottomrule
    \end{tabular}
    }
    \vskip -0.1in
    \caption{Success rate (\%) of different agents on \bench{}. Human$^*$ denotes the average performance of four expert participants who have watched the instructional video only once, similar to the model. Meta represents the meta task, while Aug. represents the augmented task.}
    \label{tab:mainresult}
\end{table*}

\begin{table*}[ht]
    \centering
    \scalebox{0.82}{
    \begin{tabular}{l|cc|cc|cc|cc|cc|c}
    \toprule
    \multirow{2}{*}{\textbf{Method}} & \multicolumn{2}{c}{\textbf{Office}} & \multicolumn{2}{c}{\textbf{Win. Usage}} & \multicolumn{2}{c}{\textbf{Web}} & \multicolumn{2}{c}{\textbf{Coding}} & \multicolumn{2}{c|}{\textbf{Media}} & \multirow{2}{*}{\textbf{Overall}}\\
     & Meta & Aug. & Meta & Aug. & Meta & Aug. & Meta & Aug. & Meta & Aug. & \\
    \midrule
    \rowcolor{graybg} Full Model & 44.4 & 31.9 & 34.8 & 21.4 & 47.6 & 38.5 & 45.5 & 27.3 & 57.1 & 50.0 & 36.2 \\
    -- w/o Planner-Critic & 31.1 & 26.4 & 21.7 & 19.0 & 38.1 & 30.8 & 36.4 & 22.7 & 42.9 & 35.7 & 27.9\\
    -- w/o Step-Check & 31.1 & 30.8 & 21.7 & 19.0 & 33.3 & 35.9 & 45.5 & 22.7 & 28.6 & 28.6 & 29.2 \\
    -- w/o Actor-Critic & 15.6 & 11.0 & 4.3 & 4.8 & 28.6 & 23.1 & 0.0 & 4.5 & 28.6 & 28.6 & 13.3 \\
    \bottomrule
    \end{tabular}
    }
    \vskip -0.1in
    \caption{Success rate (\%) of our \agent{} (Full Model) with the ablation of different critical modules.}
    \vskip -0.1in
    \label{tab:ablationmodule}
\end{table*}


\subsection{Actor}

The goal of the Actor is to translate natural language subtask $S_{t}$ into executable code $C_t$. Using an MLLM as the backbone model, the Actor processes metadata $m_t$ and screenshot $V_t$ as GUI context to generate precise executable actions, such as \texttt{click(100, 200)}. Additionally, it leverages the history of previous actions as memory to aid in generating subsequent actions. The generated actions will executed in the environment, and then the new screenshot $V_{t+1}$ and metadata $m_{t+1}$ will be captured for the next processing.



\subsection{Actor-Critic}

\textbf{Post-Action Evaluation.} After generating an action, the Actor-Critic module evaluates subtask $S_{t-1}$ completion and makes corrections if necessary. As illustrated in Fig. \ref{fig:actorcritic}, in the first step, the module implemented by an MLLM compares screenshots \(V_{t-1}\) (before action execution) and \(V_t\) (after execution) while processing each subtask $S_t$ to determine the action correctness. The model outputs a \verb|<Success>| flag to indicate task completion. If the \verb|<Success>| flag is true, the current state $s_t=\verb|<Next>|$. If the \verb|<Success>| flag is false (set $s_t=\verb|<Critic>|$) and the number of trial steps is below the maximum limit, the Actor-Critic module activates the \textbf{Locate GUI Elements} and \textbf{Actor Correction} processes. We introduce the module \textbf{Locate GUI Elements} to identify the relevant GUI elements and regenerate actions using the \textbf{Actor Correction} module. The corrected actions are then executed in the environment, generating updated observations (\(\mathcal{O}_{t}\)) that include new screenshots and metadata for the continued Actor-Critic iteration. The process repeats until the \verb|<Success>| flag is true or the maximum number of trials is reached.



\begin{table*}[t]
    \centering
    \scalebox{0.86}{
    \begin{tabular}{l|cc|cc|cc|cc|cc|c}
    \toprule
    \multirow{2}{*}{\textbf{MLLM}} & \multicolumn{2}{c}{\textbf{PPT}} & \multicolumn{2}{c}{\textbf{Word}} & \multicolumn{2}{c}{\textbf{Excel}} & \multicolumn{2}{c}{\textbf{VSCode}} & \multicolumn{2}{c|}{\textbf{Acrobat}} & \multirow{2}{*}{\textbf{Overall}}\\
     & Meta & Aug. & Meta & Aug. & Meta & Aug. & Meta & Aug. & Meta & Aug. & \\
    \midrule
    \rowcolor{graybg} Claude-3.5 & 54.5 & 47.8 & 54.5 & 45.0 & 58.3 & 30.4 & 63.6 & 40.9 & 63.6 & 44.0 & 47.3\\
    GPT-4o & 45.5 & 39.1 & 36.4 & 25.0 & 50.0 & 34.8 & 54.5 & 27.2 & 45.5 & 32.0 & 36.1 \\
    Gemini-2.0-Flash & 36.4 & 34.8 & 27.3 & 20.0 & 41.7 & 34.8 & 54.5 & 36.3 & 18.2 & 24.2 & 32.0 \\
    \bottomrule
    \end{tabular}
    }
    \vskip -0.1in
    \caption{Success rate (\%) of our \agent{} with the ablation on MLLM.}
    \label{tab:ablationmllm}
    \vskip -0.1in
\end{table*}


\begin{table}[t]
    \centering
    \vskip 0.1in
    \scalebox{0.86}{
    \begin{tabular}{l|ccc|c}
    \toprule
     \textbf{Method} & \textbf{PPT}& \textbf{Word} & \textbf{Excel} & \textbf{Overall}\\
    \midrule
    \rowcolor{graybg} Full Model & 41.2 & 29.0 & 40.0 & 37.0\\
    w/o Inst. Video & 35.3 & 22.6 & 22.2 & 29.0 \\
    \bottomrule
    \end{tabular}
    }
    \vskip -0.1in
    \caption{Success rate (\%) of our \agent{} with the ablation of Instructional Video (Inst. Video).}
    \vskip -0.1in
    \label{tab:noinstvideo}
\end{table}

\section{Experiments}

\textbf{Implementation Details.} We implement the MLLM in our \agent{} by using GPT-4o~\cite{gpt4o} (gpt-4o-2024-08-06) by default. For the computer mouse and keyboard control, we use the Python library PyAutoGUI. Following the AssistGUI~\cite{assistgui}, we use the GUI parser to obtain the position information of elements, e.g., buttons, icons, and text. We use some vision foundation models such as Google OCR to extract the text. By default, we use the center coordinates to represent the location of each element. All the testing is under the same screenshot resolution (1920 $\times$ 1080). In all experiments, we set the max trials of the Actor-Critic to 3 for light interaction costs. For the total trials of each task, we set it to $2 \times N+1$, where $N$ is the length of subtasks $S_i$ in plan $p$. In most cases, we set the trials to 21.

\textbf{Evaluation.} Given that our \bench{} includes 315 GUI tasks, we engaged four participants with strong coding and software backgrounds to test all tasks and document their evaluation results. 

\textbf{Metric.} Following the previous works of OSworld and AssistGUI, we use Success Rate (SR) as the metric.

\textbf{Baselines.} We compare our \agent{} with two strong approaches: AssistGUI \cite{assistgui} and Cluade 3.5 (Computer Use) \cite{claude3.5}. \textbf{AssistGUI} is a prominent agent framework designed for Desktop GUI Automation, which can plan the task and then execute the task step by step by following the query. We implement it by increasing the MLLM to GPT-4o for better performance. \textbf{Cluade 3.5 (Computer Use)} is the strongest MLLM model specially designed for autonomous computer use. We use the open-source implementation OOTB \cite{hu2024dawnguiagentpreliminary} as the codebase and then add the subtitle of instructional videos into the input prompt for a fair comparison.


\subsection{Main Results on \bench{}}
As shown in Tab. \ref{tab:mainresult}, our \agent{} (Claude-3.5) outperforms previous agents, achieving the highest success rate. Compared to AssistGUI (GPT-4o), \agent{} (GPT-4o) demonstrates superior performance in Office, Web, and Coding GUI tasks. This demonstrates that the critical modules are essential for effectively handling complex tasks. However, the overall success rate (SR) remains low, indicating that GUI tasks in \bench{} are still challenging, as reflected in the Human$^*$ SR of 87.6\%. When comparing the Meta and augmented (Aug.) tasks, we observe that all agents perform better on the Meta tasks but struggle with the Aug. tasks, suggesting that current agents are not yet effective in handling tasks with varying states. Analyzing the results of Computer Use (Claude-3.5), we find that it excels in Web and Coding GUI tasks, achieving a 71.4\% SR on Web tasks and 54.5\% SR on Coding tasks, likely due to its design being optimized for computer usage. These results significantly surpass those of \agent{} (GPT-4o), which lacks the corresponding GUI automation capabilities. Additionally, by equipping \agent{} with Claude-3.5 as the base model, our agent achieves comparable performance in Web and Coding tasks. For the Office task, Computer Use (Claude-3.5) struggles, likely due to the dense domain-specific icons and buttons in these applications, which pose challenges for perception. By incorporating Claude-3.5 as the MLLM in our \agent{}, \agent{} (Claude-3.5) achieves twice the SR (57.8\% vs. 28.9\%) on the Office task (Meta), demonstrating the effectiveness of our agent framework.

\subsection{Ablation Study of \agent{}}

\textbf{Impact of Critical Modules.} In Tab. \ref{tab:ablationmodule}, we investigate the effectiveness of our proposed critical modules: Planner-Critic, Step-Check, and Actor-Critic. We use \agent{} (GPT-4o) as the Full Model to reduce financial costs. The results show that removing any of these modules leads to a decline in performance across all task groups, highlighting their importance. Planner-Critic plays a crucial role in plan correction, as its removal leads to a relative performance drop across most tasks, particularly in Office (44.4 $\rightarrow$ 31.1), Web (47.6 $\rightarrow$ 38.1), and Coding (45.5 $\rightarrow$ 36.4). This indicates that effective plan refinement is essential for successfully completing GUI tasks. Step-Check is particularly important for Web and Media tasks, where removing it results in a notable decline (Web: 47.6 $\rightarrow$ 33.3, Media: 57.1 $\rightarrow$ 28.6). This suggests that some web-based tasks require additional step modifications to improve execution accuracy. Actor-Critic is the most critical module, as its removal results in the sharpest performance drop across all task groups (Overall SR: 36.2 $\rightarrow$ 13.3). We observe that many subtasks are difficult to execute correctly on the first attempt and heavily rely on the Actor-Critic for additional action correction. The most drastic declines are seen in Coding (45.5 $\rightarrow$ 0.0), Windows Usage (34.8 $\rightarrow$ 4.3), and Office (44.4 $\rightarrow$ 15.6), indicating that these tasks are more rely on the Actor-Critic module. Overall, these results demonstrate that all three modules contribute significantly to the performance, with Actor-Critic being the most impactful.

\textbf{Impact of Different MLLM.} We also investigate the impact of using different MLLMs in our framework. As shown in Tab. \ref{tab:ablationmllm}, we compare three MLLMs across five popular applications. Claude-3.5 consistently achieves the highest performance on all five applications, particularly excelling in VSCode and Acrobat tasks, where it attains an SR of over 60\%. GPT-4o demonstrates stable performance across these applications, while Gemini-2.0 underperforms compared to the other two MLLMs.


\textbf{Impact of Instructional Video.} In Tab. \ref{tab:noinstvideo}, we study the impact of removing the instructional video by modifying the prompt to include only the user query for generating the initial plan. On the three Office applications, we observe a significant performance decline, as these tasks rely more heavily on additional domain knowledge for successful planning. In contrast, the MLLM performs relatively well on Win. Usage tasks, such as Settings and File Management, where it has more inherent familiarity.


\section*{Conclusion} In this paper, we take the first step toward comprehensive GUI agent evaluation by introducing a new benchmark, \bench{}. In addition to the standard static testing processes, we incorporate dynamic testing procedures to ensure that \bench{} effectively captures the complexity and dynamism of real-world GUI environments. Furthermore, to enhance GUI automation, we propose a novel agent framework, \agent{}, built upon a critical thinking philosophy and comprising five core components. This framework enables the agent to dynamically identify uncommon states and adjust its plans or actions accordingly. Finally, we evaluate the latest computer-using agent, Claude-3.5, using our \bench{} benchmark, demonstrating the effectiveness of \agent{} across a variety of GUI tasks.

\section*{Impact Statement} 

This paper introduces \bench{} and \agent{}, a benchmark and agent framework designed to advance the field of GUI automation. Our work has several important societal and research implications. Firstly, \bench{} represents a significant step toward comprehensive GUI agent evaluation by incorporating diverse initial states for each GUI task. This approach better reflects real-world conditions, inspiring future GUI-related benchmarks to emphasize realistic GUI testing scenarios in their designs. Secondly, we propose a holistic GUI agent framework, \agent{}, which integrates five essential components and an iterative reasoning loop. By embedding critical thinking into its module design, \agent{} enhances adaptability in complex and dynamic GUI environments, allowing agents to handle unpredictable interface changes more effectively. Our work lays a strong foundation for future research in adaptive, reasoning-driven GUI automation, with potential applications in software testing, accessibility tools, and intelligent user assistance systems.

\appendix

% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocite
\nocite{langley00}

\bibliography{main}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn

\section{Comparison with other benchmarks}

In this section, we summarize the data statistics of other benchmarks in Tab. \ref{tab:benchmarkcompare} as the comparison. We can observe that the main difference of our \bench{} is we incorporate various initial states to stimulate the real GUI automation while other benchmarks are not considered.

\begin{table*}[h]
    \centering
    \scalebox{0.8}{
    \renewcommand\arraystretch{0.9}
    \tabcolsep=1mm
    \begin{tabular}{lccccccc}
    \toprule
    Benchmark & Softwares & Tasks & Platform & Task Focus & Inst. Video & GT Plan & Various States \\
    \midrule
    % Mind2Web \cite{mind2web}&  \\
    % AndriodEnv  \\
    % AndroidWorld \cite{}\\
    WebArena \cite{webarena} & 6 & 812 & Web & Website Usage & \xmark{} & \xmark{} & \xmark{}\\
    VisualWebArena \cite{visualwebarena} & 3 & 910 & Web & Website Usage & \xmark{} & \xmark{} & \xmark{}\\
    Mobile-Eval \cite{wang2024mobile} & 10 & 30 & Android OS & APP Usage & \xmark{} & \xmark{} & \xmark{} \\
    APPAgent \cite{zhang2023appagent} & 10 & 50 & Android OS & APP Usage & \xmark{} & \xmark{} & \xmark{} \\
    OSWorld \cite{OSWorld} & 10 & 369 & Ubuntu+Win. & Software Usage & \xmark{} & \xmark{} & \xmark{}\\
    \midrule
    AssistGUI \cite{assistgui} & 9 & 100 & Windows & Productivity Software Usage & \cmark{} & \xmark{} & \xmark{}\\
    VideoGUI \cite{lin2024videogui} & 10 & 86 & Win. + Web & Productivity Software Usage & \cmark & \xmark & \xmark{} \\
    WindowAgentArena \cite{windowsagentarena} & 11 & 154 & Windows & SoftWare Usage & \xmark{} & \xmark{} & \xmark{}\\
    \textbf{\bench{}} & 10 & 315 & Win. + Web &  Productivity Software Usage & \cmark{} & \cmark{} & \cmark{}\\
    \bottomrule
    \end{tabular}
    }
    \caption{\textbf{Comparison with related benmarks.} \textbf{\bench{}} is a unique benchmark that has the various states for each task to stimulate the real-world agent-computer interactions.}
    \label{tab:benchmarkcompare}
\end{table*}

\textbf{Why Instructional Video?} The rationale for using instructional videos is that, in some cases, completing a task requires extensive configuration, making it impractical to provide all necessary information within the input query. As illustrated in Fig. \ref{fig:whyinstvideo}, our user query is \textit{``Generate a photo of a girl with short brown hair with EasyNegative to improve the quality''}, which does not specify how to improve the quality. The instructional video will serve as contextual information to guide the agent to complete that.


\begin{figure} [h]
    \centering
    \includegraphics[width=0.51\linewidth]{figures/sdinterface.png}
    \includegraphics[width=0.4\linewidth]{figures/whyinstvideofig.pdf}
    \vskip -0.1in
    \caption{An example of using Stable Diffusion for media creation in \bench{}. The left shows the interface of using stable diffusion to create the photo. The right shows the user query and GT plan. If no instructional video, it is hard to include all the settings }
    \label{fig:whyinstvideo}
\end{figure}


\section{Details of Actor Space}

In this section, we detail the action space used in our \bench{}. Our action space includes all raw mouse and keyboard actions, such as left-click, right-click, double-click, drag, keystrokes, and key combinations for shortcuts, among others. Mouse-related actions also specify the target position in the pixel space of the observed screenshot. To ensure a universal and comprehensive representation of actions, we adopted the widely used Python library, PyAutoGUI, for controlling mouse and keyboard inputs. Each action is represented using the syntax \texttt{action\_type(arguments)} as in Tab.~\ref{tab:actionspace}.

\begin{table}[!h]
    \centering
    \scalebox{0.88}{
    \begin{tabular}{l|l}
    \toprule
    Action Type & Example \\
    \midrule
    Mouse Movement  & \texttt{moveTo(120, 200)} \\
    Mouse Clicks & \texttt{click(200, 300)} \\
    Keyboard Type & \texttt{write('classes')} \\
    Hotkey & \texttt{hotkey('ctrl', 'a')} \\
    Scrolling & \texttt{scroll(-100)} \\
    Drag & \texttt{dragTo(120, 220, 2)} \\
    Mouse Down and Up & \texttt{mouseDown(); mouseUp()} \\
    Press Keys & \texttt{press('delete')} \\
    Key Down and Up & \texttt{keyDown('shift')} \\
    \bottomrule
    \end{tabular}
    }
    \caption{The action types and its example in \bench{}.}

    \label{tab:actionspace}
\end{table}


\section{Data}
\label{sec:dataappendix}

% \textbf{Data Sources.} 

\textbf{(1) Annotators.} In this work, we have four annotators: A, B, C, and D. The team comprises one PhD student, one Master's student, and two undergraduate students. Prior to annotation, all annotators receive training on using the applications in \bench{} to ensure high-quality annotations. For the 10 desktop applications, we divide the software into four parts, assigning each part to a different annotator. For the human tests presented in Tab. \ref{tab:mainresult}, the annotators demonstrate tasks on software that they did not annotate. As shown in Tab. \ref{tab:benchmarkcompare}, each annotator is responsible for different software during both the annotation and human testing phases to make the soundness of the Human Test results.

\begin{table*}[h]
    \centering
    \scalebox{0.9}{
    \begin{tabular}{c|cc}
    \toprule
       Annotators  & Annotation Phase & Human Test Phase \\
    \midrule
        A & PowerPoint, Word, Excel & VSCode, Stable Diffusion, Web\\
        B & Adobe Acrobat, Stable Diffusion & Excel, Settings\\
        C & Settings, Web & PowerPoint, File Explorer, Youtube\\
        D & VSCode, File Explorer & Word, Adobe Acrobat\\
    \bottomrule
    \end{tabular}
    }
    \caption{This table shows the software used during the annotation and human testing phases by different annotators.}
    \label{tab:annotators}
\end{table*}

\textbf{(2) Data Construction Pipeline.} For each task 1) we manually collect the raw videos from the website, 2) then manually cut the raw lengthy videos into sub-clips, 3) ask annotators to manually write the user queries, 4) prepare the project files for each task for reproducibility, 5) generate the ground-truth plans by executing the scripts and agent, 6) conduct the data augmentation by varying the initial state for each task. As shown in Fig.~\ref{fig:datapipeline}, to achieve the above six steps, we invite four annotators and write the necessary scripts to help structure and format the data. Additionally, for the GT plan generation and pre-action generation, we build simple agents to obtain the data.


\textbf{(3) Project File and Enviroment Configuration.} In this paper, we utilize project files to ensure the reproducibility of task executions. This approach is simpler and more cost-effective compared to using virtual machines \cite{OSWorld} or Docker environments \cite{windowsagentarena}. The types of project files for each application are illustrated in Tab. \ref{tab:dataoverview}. To enable cross-platform usage, such as controlling the computer via a phone or other systems, we implement a frontend and backend system. The frontend receives user queries, captures screenshots and metadata, and then sends this information to the backend for API calls and the invocation of local vision foundation models. Cross-platform support allows us to deploy autonomous agents in a simple and cost-effective manner.

\textbf{(4) Creating Augmented Tasks.} In our study, to stimulate dynamic testing processing in real GUI interactions, we propose to design GUI tasks with various initial tasks. Specifically, we propose pre-actions before executing the task. The pre-actions primarily serve two purposes: \textbf{1) Simulating Intermediate Task States}: Pre-actions can complete specific steps of a task, creating a starting point from an intermediate state. This approach addresses scenarios where users may seek AI assistance because they are unable to complete a task. For example, if the task involves opening a dropdown menu, the pre-action may pre-open the menu. If the agent fails to recognize this precondition and follows its plan to click the menu again, it might inadvertently close the menu, causing task failure.
\textbf{2) Introducing Diverse Initial Context States}: Pre-actions can also introduce variations in the initial state, such as opening random tabs or settings. This ensures that the starting state is unconventional, challenging the agent to adapt by modifying its plan or adding new steps. We illustrate one example in Fig.~\ref{fig:dataaugment}. Here, the meta task and augmented task, have the same user query and instructional video and it will ideally have the same final state. We additionally provide more examples about augmenting the meta task in Fig. \ref{fig:augexample}.

\begin{figure}[t]
\begin{center}
\centerline{\includegraphics[width=0.6\linewidth]{figures/dataaugment.pdf}}
\caption{An example of augmenting one GUI task with manually aug the initial state and then using the execution scripts and corresponding agents to obtain the pre-action for each augmented case.}
\label{fig:dataaugment}
\end{center}
% \vskip -0.2in
\end{figure}

\begin{figure}[h]
\begin{center}
\centerline{\includegraphics[width=\linewidth]{figures/augmentedexample.pdf}}
\caption{We present the examples of conducting the augmentations from the meta task.}
\label{fig:augexample}
\end{center}
% \vskip -0.2in
\end{figure}

\textbf{(5) Data Statistic.} \bench{} comprises 315 GUI tasks sourced from 10 widely used Windows applications, including PowerPoint, Excel, Word, Adobe Acrobat, VSCode, File Explorer, Settings, Web, Youtube, and Stable Diffusion. Here we provide the details about the task activities and project file for each application in Tab. \ref{tab:dataoverview}. We collected 107 meta tasks from these applications, each of which was augmented 0 to 3 times based on its specific content, resulting in 208 augmented tasks. Fig. \ref{fig:dspiechart} presents the task distribution, provides query examples, and illustrates the distribution of meta and augmented tasks across different applications.

\begin{table*}[h]
    \centering
    \scalebox{0.9}{
    \begin{tabular}{llll}
    \toprule
       Applications  & All Task & Task Activities & Project File Type \\
    \midrule
        PowerPoint & 34 & Change the content style and layout; Design new effects. & project.pptx\\
        Word & 31 & Formatting the content style and layout & project.docx \\
        Excel & 35 & Table formatting; Data management and processing & project.xlsx\\
        Adobe Acrobat & 36 & Automatic add electric signature; Document management & project.pdf\\
        VSCode & 33 & Code editing; Software configuration & vscode.exe\\
        Settings & 39 & Advanced personalized and safety settings; & ms-Settings\\
        File Explorer & 26 & File management: Add, delete, rename, and move files. & explorer.exe\\
        Web & 32 & Web operation & web browser + URL\\
        Youtube & 28 & Video and account configurations & web browser + UR\\
        Stable Diffusion  & 21 & Media creation with complicated effects & Local Run Platform\\
        
    \bottomrule
    \end{tabular}
    }
    \caption{This table shows all tasks, task activities, and project file of the desktop applications used in \bench{}.}
    \label{tab:dataoverview}
\end{table*}





\begin{figure*}[t]
\begin{center}
% \includegraphics[width=0.45\linewidth]{figures/data_piechart.pdf}
% \hfill
% \includegraphics[width=0.52\linewidth]{figures/query_list.pdf}
\includegraphics[width=0.32\linewidth]{figures/data_piechart.pdf}
\hfill
\includegraphics[width=0.3\linewidth]{figures/query_list.pdf}
\hfill
\includegraphics[width=0.35\linewidth]{figures/data_barchart.pdf}
\vskip -0.1in
\caption{\textbf{Distribution of collect tasks, selected queries, and task amount of \bench{}}. We have gathered tasks across 10 desktop applications, focusing on the use of productivity software as well as fundamental computer operations and settings.}
\label{fig:dspiechart}
\end{center}
% \vskip -0.2in
\end{figure*}

\begin{figure*}[t]
\begin{center}
\centerline{\includegraphics[width=0.9\linewidth]{figures/datapipeline.pdf}}
\caption{Pipeline of Data Construction. Human: Represents the annotators. Code: Refers to the scripts (e.g., Python Code) utilized to achieve the goal. Agent: We design an agent built upon the MLLMs to achieve the goal.}
\label{fig:datapipeline}
\end{center}
\vskip -0.2in
\end{figure*}



\section{GUI-Thinker Reasoning Loop Algorithm}
In this section, we provide the details of our reasoning loop algorithm in Algorithm \ref{alg:reasoningloop}.

\begin{algorithm}[h]
   \caption{GUI-Thinker Reasoning Loop Algorithm}
   \label{alg:reasoningloop}
\begin{algorithmic}
   \STATE {\bfseries Input:} State $s$, Action Code $C$, Screenshot $V$, Metadata $m$, Current subtask $S$, Critic\_count $z$
   \STATE Generate task plan $p$ with \textbf{Planner} and \textbf{Planner-Critic}
   \STATE Initial current subtask $S_{t=0} = S_1^1$, where $S_1^0$ is the $1$-th subtask in the $1$-th milestone of $p$. 
   \STATE Initial $s_0 = \textless Continue\textgreater$  
   \WHILE{$S_t$ is not end and $t < $ max trials}
   \STATE Observe metadata $m_t$ and Screenshot $V_t$ from Env.
    \STATE Obtain state $s_t$ by running \textbf{Step-Check}.
    \IF{$s_t = \textless Next\textgreater$}
    \STATE Go to the next task $S_{t+1}=next(S_{t})$
    \ENDIF
    \STATE Check potential modification of subtask $S_t$ 
    \STATE Obtain action code $C_t$ by running \textbf{Actor};
     Execute the action code $C_t$ in the Env.; Observe metadata $m_t$ and Screenshot $V_t$ from Env.
    \STATE Set $C_t=$ \text{None}; $t = t+1$; Set state $s_t = \textless Critic\textgreater$ (For each subtask, the first step is finished, then execute the actor-critic process)
    \STATE Observe metadata $m_t$ and Screenshot $V_t$ from Env.
    \STATE Running \textbf{Actor-Critic} and obtain the state $s_t$
    \IF{$s_t = \textless Next\textgreater$}
    \STATE Go to the next task $S_{t+1}=next(S_{t})$.
    \ENDIF
    \WHILE{$s_t = \textless Critic\textgreater$ and $z <$ max critique trials}
    \STATE Running \textbf{Actor-Critic} and obtain the state $s_t$ and corrected action code $C_t$
    \IF{$s_t = \textless Next\textgreater$}
    \STATE Go to the next task $S_{t+1}=next(S_{t})$.
    \ENDIF
    \STATE Execute the action code $C_t$ in the Env.; Observe metadata $m_t$ and Screenshot $V_t$ from Env.
    \STATE Set $C_t=$ \text{None}; $z=z+1$
    \ENDWHILE
   \STATE Go to the next task $S_{t+1}=next(S_{t})$
   \STATE $t = t+1$
   
   \ENDWHILE
\end{algorithmic}
\end{algorithm}


\section{Qualitative Results}

(1) In Fig. \ref{fig:ansuccessexample}, we present a successful prediction example, demonstrating that the \bench{} can effectively plan each step for a task, accurately perceive specific elements in the GUI, and convert them into the correct action code. Additionally, we display the parsed GUI elements, which can accurately identify most content, including small icons and dense text elements.

\begin{figure} [h]
\centering
\includegraphics[width=\linewidth]{figures/ansucessexample.pdf}
\vskip -0.1in
\caption{We show one successful prediction of our \agent{}.}
\label{fig:ansuccessexample}
\end{figure}

\begin{figure} [h]
    \centering
    \includegraphics[width=0.48\linewidth]{figures/excel_parser.png}
    \includegraphics[width=0.48\linewidth]{figures/yt_parser.png}
    \vskip -0.1in
    \caption{We show two examples of using GUI Parser to obtain the element position information.}
    \label{fig:parservisuall}
\end{figure}


(2) We provide the visualization results of using Planner-Critic, Step-Check, and Actor-Critic in Fig. \ref{fig:plannercriticexample}, Fig. \ref{fig:stepcheckexample}, and Fig. \ref{fig:actorcriticexample}. These qualitative results demonstrate the effectiveness of these critical modules in GUI automation.

\begin{figure} [h]
\centering
\includegraphics[width=\linewidth]{figures/plannercritic_example.pdf}
\vskip -0.1in
\caption{An example of using Planner-Critic to correct the plan.}
\label{fig:plannercriticexample}
\end{figure}

\begin{figure} [h]
\centering
\includegraphics[width=\linewidth]{figures/stepcheck_example.pdf}
\vskip -0.1in
\caption{Two examples of using Step-Check to check the subtask status.}
\label{fig:stepcheckexample}
\end{figure}

\begin{figure} [h]
\centering
\includegraphics[width=\linewidth]{figures/actorcritic_example.pdf}
\vskip -0.1in
\caption{An example of using Actor-Critic to correct the actions.}
\label{fig:actorcriticexample}
\end{figure}

(3) We also highlight some common errors encountered. 1)
The model has the difficulty of obtaining the desired information when we augment the task by invoking the dropdown menu of the Settings application. As shown in the left of Fig. \ref{fig:failurecases1}, when we click on the 'System' button on the left, it is challenging for our model to extract the button's position as it is hidden. Such cases require the model to have a higher level of ability to delete the content in the input box or click on the blank area. 2) As shown in the right of Fig. \ref{fig:failurecases1}, the model has difficulty dragging a bar to achieve the desired value. 3) The model struggles with the visual choice when there is no text information in the screenshot, as shown on the left of Fig. \ref{fig:failurecases2}. The subtask aims to select the center button, but the current model makes it hard to detect the center choice only from the screenshot. 4) The model cannot successfully locate the position of the input box, as the GUI parser will easily locate the text location 'Replace with', it always outputs the action like clicking on the 'Replace with', which will destroy the whole task success.

\begin{figure} [h]
\centering
\includegraphics[width=\linewidth]{figures/failure1.pdf}
\caption{We display some common errors.}
\label{fig:failurecases1}
\end{figure}

\begin{figure} [htp]
\centering
\includegraphics[width=\linewidth]{figures/failure2.pdf}
\caption{We display some common errors}
\label{fig:failurecases2}
\end{figure}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}

