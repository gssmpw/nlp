% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[preprint]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}
\usepackage{booktabs}
\usepackage{subcaption}
% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}
\usepackage{hyperref}
% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}
\usepackage{listings}
\usepackage{xcolor}

\definecolor{codegray}{gray}{0.9}
\definecolor{keywords}{RGB}{0,0,180}
\definecolor{comments}{RGB}{0,128,0}
\definecolor{strings}{RGB}{180,0,0}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{codegray},   
    commentstyle=\color{comments},
    keywordstyle=\color{keywords}\bfseries,
    stringstyle=\color{strings},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    numberstyle=\tiny\color{gray},
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=4
}

\lstset{style=mystyle}

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{Mind the Confidence Gap: Overconfidence, Calibration, and Distractor Effects in Large Language Models}

% Author information can be set in various styles:
% For several authors from the same institution:
\author{Prateek Chhikara\\
        University of Southern California, USA\\
        \texttt{\href{mailto:pchhikar@usc.edu}{pchhikar@usc.edu}}
        }
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a separate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

% \author{Prateek Chhikara \\
%   Affiliation / Address line 1 \\
%   Affiliation / Address line 2 \\
%   Affiliation / Address line 3 \\
%   \texttt{email@domain} \\\And
%   Second Author \\
%   Affiliation / Address line 1 \\
%   Affiliation / Address line 2 \\
%   Affiliation / Address line 3 \\
%   \texttt{email@domain} \\}


\begin{document}
\maketitle
\begin{abstract}
Large Language Models (LLMs) demonstrate impressive performance across diverse tasks, yet confidence calibration remains a challenge. Miscalibration—where models are overconfident or underconfident—poses risks, particularly in high-stakes applications. This paper presents an empirical study on LLM calibration, examining how model size, distractors, and question types affect confidence alignment. We introduce an evaluation framework to measure overconfidence and investigate whether multiple-choice formats mitigate or worsen miscalibration. Our findings show that while larger models (e.g., \texttt{GPT-4o}) are better calibrated overall, they are more prone to distraction, whereas smaller models benefit more from answer choices but struggle with uncertainty estimation. Unlike prior work, which primarily reports miscalibration trends, we provide actionable insights into failure modes and conditions that worsen overconfidence. These findings highlight the need for calibration-aware interventions and improved uncertainty estimation methods. To ensure reproducibility, we provide our code and dataset\footnote{\url{https://github.com/prateekchhikara/llms-calibration}}.
\end{abstract}

\section{Introduction}
Large Language Models (LLMs) have transformed natural language understanding, achieving state-of-the-art performance across diverse applications, from conversational AI \cite{skjuve2024people, zhang-2024-guided} to scientific discovery \cite{kumar2024large}. Their versatility has also facilitated advancements in multimodal learning, where language models integrate with vision-based systems to enhance tasks such as automated content generation and decision support \cite{zhang2023visual, chhikara2024fire}. However, as LLMs are increasingly deployed in high-stakes domains such as healthcare, finance, and law, critical concerns arise regarding \textit{calibration}—the alignment between model confidence and actual correctness \cite{dhuliawala2023diachronic}. When LLMs express unwarranted confidence in incorrect predictions, they risk misleading users, spreading misinformation, and reducing trust and reliability in AI-driven systems \cite{chen2023close, zhang2024study}.

\begin{figure}[!tbp]
    \centering
    \includegraphics[width=\linewidth]{figures/calibration_histograms.pdf}
    \caption{LLM generating an incorrect answer with high confidence.}
    \label{main_figure}
\end{figure}

Figure \ref{main_figure} illustrates this issue: when asked \textit{“Who received the IEEE Frank Rosenblatt Award in 2010?”}, an LLM incorrectly responds with \textit{“Geoffrey Hinton”}, assigning a 93\% confidence score, despite correct answer being \textit{“Michio Sugeno”}. This overconfidence is especially problematic because users tend to equate high-confidence outputs with reliability \cite{liu2024litcab}. The right-hand side of Figure \ref{main_figure} further demonstrates the broader miscalibration problem: while a well-calibrated model should align confidence with accuracy, real-world LLMs frequently overestimate their correctness.

Although calibration errors in neural networks are well-documented \cite{guo2017calibration}, the calibration behavior of LLMs remains poorly understood, particularly in response to fine-tuning techniques such as Reinforcement Learning from Human Feedback (RLHF) \cite{leng2024taming, li2024can}. Prior work \cite{zhu-etal-2023-calibration, geng2024survey} has noted that RLHF can \textit{amplify} overconfidence, but the extent and specific conditions under which this occurs remain largely unexplored. Key open questions include: \texttt{(i)} \textit{Do bigger models exhibit better calibration, or does scale introduce new sources of miscalibration?} and \texttt{(ii)} \textit{Can structured input formats, such as multiple-choice answer choices, improve confidence alignment, or do they introduce additional biases?} Addressing these gaps is crucial for ensuring the reliable deployment of LLMs in decision-critical applications.

This paper presents an empirical study of LLM calibration, focusing on three key objectives: 
\texttt{(i)} \textbf{\textit{Quantifying Overconfidence:}} We systematically measure miscalibration across multiple LLMs and analyze the extent of overconfidence.
\texttt{(ii)} \textbf{\textit{Evaluating the Role of Answer Choices:}} We assess whether providing structured answer options (e.g., multiple-choice format) improves or worsens accuracy and calibration.
\texttt{(iii)} \textbf{\textit{Identifying Failure Modes:}} We categorize question types where LLMs exhibit the highest miscalibration, revealing systematic weaknesses in uncertainty estimation.

Unlike prior work \cite{ulmer2024calibrating, kapoor2024calibration, jiang2021can} that broadly characterizes miscalibration, our study pinpoints specific conditions that worsen overconfidence, offering an actionable framework for improving LLM reliability. Our findings emphasize the need for task-specific confidence adjustments and structured uncertainty modeling to enhance trust in real-world deployments. Our study reveals that miscalibration is not a uniform problem; its severity varies based on model scale and input structure, with larger models gaining significant calibration improvements when provided with distractors, whereas smaller models struggle with meaningful uncertainty estimation. 



\section{Experimental Setup}

\paragraph{Evaluation Dataset:}
We use the SimpleQA dataset \cite{wei2024measuring}, which provides a reliable benchmark for evaluating LLM factual accuracy and calibration. Comprising short, fact-seeking queries with clearly defined correct answers, SimpleQA enables precise measurement of model confidence and alignment with factual correctness. Its high-quality annotations, verified by multiple independent AI trainers, ensure accuracy and unambiguity, making it well-suited for calibration assessment.
The dataset contains 4326 question-answer pairs. For each pair, we used \texttt{GPT-4o-mini} to generate three distractors—factually incorrect yet contextually plausible answers that matched the answer type (e.g., dates for date-based questions), remained distinct, and maintained similar specificity. 
Additional details on the prompt are provided in Appendix \ref{appendix_prompts}. 


\paragraph{Selected LLMs:} 
We select models from OpenAI\footnote{\url{https://openai.com}} and GroqCloud\footnote{\url{https://groq.com}}. From OpenAI, we include \texttt{GPT-4o}, \texttt{GPT-4o-mini}, and \texttt{GPT-4-turbo} \cite{hurst2024gpt}, all large-scale models with billions of parameters. From GroqCloud, we select smaller, efficiency-optimized models: \texttt{LLaMA-3-8B-8192} \cite{dubey2024llama}, \texttt{LLaMA3.1-8B-instant}, and \texttt{Gemma2-9B-it} \cite{team2024gemma}. More details about the selected models are in Appendix \ref{selected_models}.

\begin{table*}[!t]
    \centering
    \caption{Performance metrics of LLMs in the Normal (\textbf{$\mathcal{N}$}) and Distractor (\textbf{$\mathcal{D}$}) settings, including accuracy (\texttt{\tiny correct}), $\texttt{NOT\_ATTEMPTED}$ (\texttt{\tiny na}), ECE, and the number of helped (\textbf{$\mathcal{D}_{helped}$}) and harmed (\textbf{$\mathcal{D}_{harmed}$}) instances.}
\resizebox{\textwidth}{!}{ \small
    \begin{tabular}{l|ccc|ccc|cc}
        \toprule
        \textbf{LLMs} & \textbf{$\mathcal{N}_{correct}$} & \textbf{$\mathcal{N}_{na}$} & \textbf{$\mathcal{N}_{ECE}$} & \textbf{$\mathcal{D}_{correct}$} & \textbf{$\mathcal{D}_{na}$} & \textbf{$\mathcal{D}_{ECE}$} & \textbf{$\mathcal{D}_{helped}$} & \textbf{$\mathcal{D}_{harmed}$} \\
        \midrule
        \texttt{GPT-4o-mini} \raisebox{-1pt}{\includegraphics[height=0.8em]{figures/openai.png}} & 8.46\% & 6.80\% & 0.750 & 47.43\%  & 0.02\% & 0.320 & 1644 (93.78\%) & 109 (6.22\%) \\
        \texttt{GPT-4-turbo} \raisebox{-1pt}{\includegraphics[height=0.8em]{figures/openai.png}} & 20.37\% & 6.17\% & 0.612 & 65.40\%  & 0\% & 0.165 & 1877 (95.86\%) & 81 (4.14\%) \\
        \texttt{GPT-4o} \raisebox{-1pt}{\includegraphics[height=0.8em]{figures/openai.png}} & \textbf{35.14\%} & 7.88\% & 0.450 & \textbf{73.42\%} & 0.02\%  & 0.037 & 1569 (91.97\%) & 137 (8.03\%) \\  \midrule
        \texttt{LLaMA3.1-8b-instant} \raisebox{-1pt}{\includegraphics[height=0.8em]{figures/meta.png}} & 5.58\% & 18.78\% & 0.799  & 44.64\% & 0.12\% & 0.367 & 1355 (95.29\%) & 67 (4.71\%) \\
        \texttt{LLaMA 3-8B-8192} \raisebox{-1pt}{\includegraphics[height=0.8em]{figures/meta.png}} & 4.79\% & 21.20\% & 0.810 & 44.01\%  & 0.55\% & 0.361 & 1382 (95.57\%) & 62 (4.43\%) \\ 
        \texttt{Gemma2-9B-it} \raisebox{-1pt}{\includegraphics[height=0.8em]{figures/google.png}} & 5.48\% & 33.29\% & 0.799 & 45.58\%  & 1.78\% & 0.367  & 1143 (94.38\%) & 68 (5.62\%) \\
        \bottomrule
    \end{tabular}
    }
    \label{main_table}
\end{table*}



\begin{figure*}[!t]
    \centering
    \begin{minipage}{0.329\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/gpt-4o-mini_same_judge.pdf}
        % \caption{Subfig 1}
    \end{minipage}
    \begin{minipage}{0.329\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/gpt-4-turbo_same_judge.pdf}
        % \caption{Subfig 2}
    \end{minipage}
    \begin{minipage}{0.329\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/gpt-4o_same_judge.pdf}
        % \caption{Subfig 3}
    \end{minipage}
    
    \vspace{-0.22cm} % Space between rows

    \begin{minipage}{0.329\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/llama-3.1-8b-instant_same_judge.pdf}
        % \caption{Subfig 1}
    \end{minipage}
    \begin{minipage}{0.329\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/llama3-8b-8192_same_judge.pdf}
        % \caption{Subfig 2}
    \end{minipage}
    \begin{minipage}{0.329\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/gemma2-9b-it_same_judge.pdf}
        % \caption{Subfig 3}
    \end{minipage}
    \caption{Reliability diagrams (RDs) showing calibration performance in \textbf{$\mathcal{N}$} (\textcolor[HTML]{A11B9B}{$\bullet$}) and \textbf{$\mathcal{D}$} (\textcolor[HTML]{fcce25}{$\bullet$}) settings. The numbers on top of bars represent the number of correctly predicted instances. \small (y-axis: actual accuracy, x-axis: predicted confidence)}
    \label{reliability_diagram}
\end{figure*}



\paragraph{Evaluation Criteria} Following prior work, we use \texttt{GPT-4o-mini} as an LLM-based judge to classify responses as \texttt{CORRECT}, \texttt{INCORRECT}, or $\texttt{NOT\_ATTEMPTED}$ \cite{packer2023memgpt, wei2024measuring}. A response is \texttt{CORRECT} if it fully captures the gold target’s key information without contradiction, allowing minor variations in wording, order, or hedging. It is \texttt{INCORRECT} if it contains factual errors, contradictions, or misleading speculation, even if hedged. $\texttt{NOT\_ATTEMPTED}$ applies when a response lacks essential information without introducing errors, including vague or evasive answers. We experiment with using the same LLM for both prediction and judgment, finding that smaller LLM judges often misclassify responses or hesitate to assign $\texttt{NOT\_ATTEMPTED}$ when no valid answer is generated. Manual inspection confirms these issues, and further details are provided in the Appendix \ref{appendix_different_llm_judge}.

\paragraph{Evaluation Methods:} The standard LLM approach (\textbf{$\mathcal{N}$}) generates answers based only on the given question and a predefined prompt, without external knowledge. We introduce an alternative distractor method (\textbf{$\mathcal{D}$}), where the model receives the question along with the correct answer and three incorrect options (shuffled). This setup enables us to evaluate whether additional context enhances calibration, reinforces overconfidence, or simply encourages random guessing from the provided options. Details on prompts are in the Appendix \ref{appendix_prompts}.


\paragraph{Evaluation Metrics:} To evaluate performance, we measure correctly answered questions for both variations (\textbf{$\mathcal{N}$} and \textbf{$\mathcal{D}$}). For calibration assessment, we use Expected Calibration Error (ECE) to quantify the misalignment between a model’s predicted confidence and actual accuracy. A well-calibrated model produces confidence estimates that closely match its true correctness, with an ECE of zero indicating perfect calibration. Following \cite{naeini2015obtaining}, we compute ECE using empirical binning (bin size 0.1) to ensure a robust measurement of miscalibration. Additionally, we introduce two metrics, \textbf{$\mathcal{D}_{helped}$} and \textbf{$\mathcal{D}_{harmed}$}, which measure the number of instances where adding options either improved or worsened the model’s performance.









\section{Results Analysis and Discussion}

\paragraph{Performance across LLMs}
Results indicate a clear performance gap between bigger and smaller LLMs, with bigger models (e.g., \texttt{GPT-4o}) significantly outperforming smaller ones in open-ended question answering (Table \ref{main_table}). However, when provided with structured answer choices, smaller models exhibit disproportionately higher gains, while bigger models experience relative increase in distractor-induced errors (\textbf{$\mathcal{D}_{harmed}$}). This suggests that bigger models rely more heavily on associative recall and self-generated confidence heuristics, which can be misleading when plausible but incorrect distractors are introduced. 

For instance, \texttt{GPT-4o}’s accuracy jumps from 35.14\% (\textbf{$\mathcal{N}$}) to 73.42\% (\textbf{$\mathcal{D}$}), while \texttt{LLaMA3-8B} exhibits an even more dramatic improvement from 4.79\% to 44.01\%. The fact that smaller models show a greater relative benefit from answer choices suggests that their decision-making relies more on structured elimination strategies rather than independent fact retrieval. This finding highlights a key consideration for practical deployment: while bigger models can be more self-reliant, their confidence in unaided settings does not necessarily translate into robustness when faced with misleading choices.


\begin{figure*}[!tbp]
    \centering
    \begin{minipage}{0.26\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/Date.pdf}
        % \caption{Subfig 1}
    \end{minipage}\hspace{-10pt}
    \begin{minipage}{0.26\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/Number.pdf}
        % \caption{Subfig 2}
    \end{minipage}\hspace{-10pt}
    \begin{minipage}{0.26\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/Person.pdf}
        % \caption{Subfig 3}
    \end{minipage}\hspace{-10pt}
    \begin{minipage}{0.26\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/Place.pdf}
        % \caption{Subfig 1}
    \end{minipage}

    \caption{Performance (\texttt{correct}) of LLMs across different question types in both \textbf{$\mathcal{N}$} (\textcolor[HTML]{A11B9B}{$\bullet$}) and \textbf{$\mathcal{D}$} (\textcolor[HTML]{fcce25}{$\bullet$}) settings.}
    \label{radial_charts}
\end{figure*}


\paragraph{Calibration and Confidence Patterns}
A well-calibrated model should align its confidence scores with actual accuracy—meaning that if a model expresses 80\% confidence in an answer, it should be correct roughly 80\% of the time. Our study reveals that bigger models, while better calibrated overall, still exhibit systematic overconfidence in open-ended tasks. The ECE for \texttt{GPT-4o} decreases significantly in the \textbf{$\mathcal{D}$} setting, demonstrating that answer choices improve confidence alignment by providing external constraints. However, smaller models, while benefiting from options, still fail to estimate their uncertainty meaningfully, as evidenced by consistently high ECE values.

Figure \ref{reliability_diagram} shows that calibration behavior varies significantly across models. While \texttt{GPT-4o} is relatively well-calibrated at higher confidence levels (70–100\%), minor overconfidence remains. In contrast, smaller models frequently overestimate their accuracy, assigning high confidence to incorrect answers, particularly in the 80–100\% range. Additionally, all LLMs at lower confidence levels (20–40\%) show inconsistencies in uncertainty estimation, sometimes underestimating correctness but not necessarily engaging in random guessing. This distinction has significant implications in real-world applications: in AI-driven tutoring or medical diagnosis, overconfidence in incorrect answers can lead to misleading recommendations. Calibration-aware interventions—such as confidence regularization or explicit uncertainty weighting—are necessary to mitigate these risks.



\paragraph{Performance Across Question Types}
To analyze calibration weaknesses, we evaluate performance across different question types (Figure \ref{radial_charts}). \textbf{Person}-based queries are most challenging, likely due to name ambiguities and inherent variability in names, overlapping roles, and contextual dependencies that require deeper reasoning beyond surface-level pattern matching. LLMs frequently confuse historical figures with similar names, but providing structured answer choices significantly improves accuracy, suggesting that explicit disambiguation helps mitigate uncertainty in this category. In contrast, \textbf{place}-based queries exhibit relatively strong performance across both settings, indicating that geographic knowledge is well-represented in pretraining. However, calibration improvements vary (Table \ref{tab:before_after_transposed}): the \textbf{person} category sees highest relative ECE drop (69\%), while \textbf{place} category shows the lowest (50\%). This suggests that structured choices help correct overconfidence in ambiguous queries but offer limited calibration gains when models already retrieve knowledge with high confidence. These results demonstrate that miscalibration depends on both task framing and knowledge representation, not just model scale. While structured reasoning improves confidence alignment in \textbf{person}-based queries, factual retrieval tasks like \textbf{place}-based questions may require alternative calibration strategies to prevent persistent overconfidence.

\begin{table}[!t]
    \centering
    \caption{ECE comparison across question types.}
    \resizebox{\linewidth}{!}{ 
    \begin{tabular}{lcccccccc}
        \toprule
        & \multicolumn{2}{c}{\textbf{Date}} & \multicolumn{2}{c}{\textbf{Number}} & \multicolumn{2}{c}{\textbf{Person}} & \multicolumn{2}{c}{\textbf{Place}} \\
        \cmidrule(lr){2-3} \cmidrule(lr){4-5} \cmidrule(lr){6-7} \cmidrule(lr){8-9}
        & $\mathcal{N}$ & $\mathcal{D}$ & $\mathcal{N}$ & $\mathcal{D}$ & $\mathcal{N}$ & $\mathcal{D}$ & $\mathcal{N}$ & $\mathcal{D}$ \\
        \midrule
        \texttt{4o-mini} & 0.77 & 0.32 & 0.73 & 0.35 & 0.76 & 0.25 & 0.73 & 0.42  \\
        \texttt{4-turbo} & 0.62 & 0.23 & 0.65 & 0.26 & 0.62 & 0.03 & 0.52 & 0.19  \\
        \texttt{4o} & 0.41 & 0.05 & 0.53 & 0.13 & 0.51 & 0.07 & 0.35 & 0.04 \\
        \texttt{LLaMA3.1} & 0.84 & 0.37 & 0.82 & 0.38 & 0.80 & 0.31 & 0.73 & 0.40  \\
        \texttt{LLaMA3} & 0.85 & 0.35 & 0.82 & 0.36 & 0.81 & 0.33 & 0.73 & 0.45  \\
        \texttt{Gemma2} & 0.80 & 0.33 & 0.78 & 0.42 & 0.84 & 0.33 & 0.76 & 0.41  \\ \midrule
        mean & 0.72 & 0.28 & 0.72 & 0.32 & 0.72 & 0.22 & 0.64 & 0.32  \\ 
        \bottomrule
    \end{tabular}
    }
    \label{tab:before_after_transposed}
\end{table}




\paragraph{Implications for Model Deployment}
Our findings indicate that calibration errors are not uniform and vary based on both model size and input structure. The susceptibility of bigger models to distractors suggests that prompt engineering strategies should include explicit calibration constraints—for example, by encouraging models to justify their confidence levels rather than relying on raw probability scores. For smaller models, answer ranking or re-scoring techniques may provide more robust confidence adjustments, ensuring that they do not simply default to high-confidence selections when uncertain. Future research should explore task-specific calibration techniques rather than applying one-size-fits-all solutions, particularly for applications where overconfidence poses significant risks (e.g., legal and medical AI systems).

\section{Conclusion}
This study provides an investigation of calibration in LLMs, revealing key trade-offs between model size, answer format, and confidence alignment. While bigger models exhibit better calibration, they remain susceptible to distraction, whereas smaller models show dramatic performance improvements with structured answer choices but fail to meaningfully estimate uncertainty. Unlike prior work that broadly characterizes LLM miscalibration, our study pinpoints conditions under which calibration errors arise, offering a framework for mitigating overconfidence in real-world applications. Future work should explore calibration-aware fine-tuning strategies and uncertainty-aware response generation to enhance LLM reliability, particularly in high-stakes domains.

\section{Limitations}
Our study is limited by its focus on a small set of LLMs, excluding domain-specific or fine-tuned variants, which may affect generalizability. Additionally, the use of LLMs to pick from multiple-choice options introduces potential selection bias, as highlighted by recent work \cite{zheng2023large}. While we mitigate this by randomizing option order, it may not fully eliminate bias, impacting generation reliability.

Furthermore, our reliance on \texttt{GPT-4o-mini} as the primary evaluation judge could be questioned, as automated LLM-based scoring may introduce biases. Although we conducted a small-scale human evaluation to validate its reliability, finding substantial agreement with human annotators (Cohen’s kappa = 0.82), minor inconsistencies were observed in ambiguous cases (see Appendix \ref{appendix_different_llm_judge} for details).

% \section*{Acknowledgments}


\bibliography{custom}

\clearpage

\onecolumn

\appendix

\section{Custom Prompts}
\label{appendix_prompts}

We generate the answers for \textbf{$\mathcal{N}$} setting using the following prompt. The prompt outputs the answer and confidence for the answer in a json format.

\begin{lstlisting}[language=Python]
LLM_RESPONSE_PROMPT = """
You are an intelligent assistant who is given a question. Your role is to provide accurate, helpful, and well-reasoned responses based on your knowledge and capabilities.

Along with the question, you need to provide a confidence score for your answer. The confidence score should be a number between 0 and 100, where:
- 0-25 indicates low confidence
- 26-75 indicates moderate confidence 
- 76-100 indicates high confidence

Guidelines for providing answers:
1. Be direct and concise in your answer while ensuring completeness. Avoid unnecessary words or tangents.
2. If you are uncertain, provide a lower confidence score.
3. Base your confidence score on:
   - The reliability and recency of available information
   - Your knowledge of the specific domain

Here are some examples:

Example 1:
Question: What is the capital of France?
Answer: Paris
Confidence score: 91
(High confidence as this is a well-established fact)

Example 2:
Question: Which country has the best healthcare system?
Answer: It depends on the criteria used. Some rankings favor Switzerland, while others favor Sweden or Singapore.
Confidence score: 25
(There is no definitive answer, and the confidence is low due to the lack of a clear consensus.)

Example 3:
Question: Which state is between Washington and California?
Answer: Oregon
Confidence score: 87
(Maximum confidence as this is a clear geographic fact)

Example 4:
Question: What was Albert Einstein's favorite food?
Answer: There is no definitive record of his favorite food, but he reportedly liked pasta.
Confidence score: 25
(There are anecdotal mentions, but no verified records.)

Example 5:
Question: Is Irvine a city in California?
Answer: Yes
Confidence score: 81
(High confidence as this is a verifiable fact)

Example 6:
Question: What is the most popular programming language for AI development?
Answer: Python
Confidence score: 66
(Moderate-high confidence based on current trends, but this can change over time)

Here is a new example. Simply reply with your answer and confidence score.

Question: {{question}}

Provide your response in the following JSON format:
{
    "answer": "Your answer here",
    "confidence_score": number between 0-100
}
"""
\end{lstlisting}

We generate the answers for \textbf{$\mathcal{D}$} setting using the following prompt. The prompt outputs the answer and confidence for the answer in a json format.

\begin{lstlisting}[language=Python]
LLM_RESPONSE_PROMPT_DISTRACTORS = """
You are an intelligent assistant who is given a question and a list of options. Your role is to provide accurate, helpful, and well-reasoned answer based on your knowledge and capabilities and the options provided.

Along with the answer, you need to provide a confidence score for your answer. The confidence score should be a number between 0 and 100, where:
- 0-25 indicates low confidence
- 26-75 indicates moderate confidence 
- 76-100 indicates high confidence

Guidelines for providing answers:
1. Return the answer from the list of options provided only. It is guaranteed that the answer will be one of the options provided.
2. If you are uncertain, provide a lower confidence score.
3. Base your confidence score on:
   - The reliability and recency of available information
   - Your knowledge of the specific domain

Here are some examples:

Example 1:
Question: What is the capital of France?
Options:
- Paris
- London
- Rome
- Madrid
Answer: Paris
Confidence score: 91
(High confidence as this is a well-established fact)

Example 2:
Question: Which country has the best healthcare system?
Options:
- Switzerland
- Sweden
- Singapore
- United States
Answer: It depends on the criteria used. Some rankings favor Switzerland, while others favor Sweden or Singapore.
Confidence score: 25
(There is no definitive answer, and the confidence is low due to the lack of a clear consensus.)

Example 3:
Question: Which state is between Washington and California?
Options:
- Oregon
- Washington
- California
- Idaho
Answer: Oregon
Confidence score: 87
(Maximum confidence as this is a clear geographic fact)

Example 4:
Question: What was Albert Einstein's favorite food?
Options:
- Pizza
- Pasta
- Sushi
- Tacos
Answer: There is no definitive record of his favorite food, but he reportedly liked pasta.
Confidence score: 25
(There are anecdotal mentions, but no verified records.)

Example 5:
Question: Is Irvine a city in California?
Options:
- Yes
- No
Answer: Yes
Confidence score: 81
(High confidence as this is a verifiable fact)

Example 6:
Question: What is the most popular programming language for AI development?
Options:
- Python
- Java
- C++
- JavaScript
Answer: Python
Confidence score: 66
(Moderate-high confidence based on current trends, but this can change over time)

Here is a new example. Simply reply with your answer and confidence score.

Question: {{question}}
Options: {{options}}

Provide your response in the following JSON format:
{
    "answer": "Your answer here",
    "confidence_score": number between 0-100
}
"""
\end{lstlisting}

To generate the 3 distractors for each question-answer pair, we use the following prompt with few shot examples.

\begin{lstlisting}[language=Python]
DISTRACTORS_GENERATION_PROMPT = """You are an expert synthetic data generator. Your task is to generate three plausible but incorrect answers to a given question.

Guidelines for generating wrong answers:
1. Each answer should be factually incorrect but plausible within the context
2. Match the answer type (e.g. if asking for a date, provide wrong dates)
3. The wrong answers should be clearly distinct from the correct answer and from each other
4. Maintain a similar level of specificity as the original answer
5. The answers should be realistic and not obviously wrong

Example 1:
Question: What is the capital of France?
Answer: Paris
Wrong Answers: 
- Lyon
- Marseille 
- Bordeaux
Reason: All are major French cities, but incorrect as capital

Example 2:
Question: Who was the first president of the United States?
Answer: George Washington
Wrong Answers:
- John Adams
- Thomas Jefferson
- Benjamin Franklin
Reason: All are founding fathers but not the first president

Example 3:
Question: In what year did World War II end?
Answer: 1945
Wrong Answers:
- 1943
- 1944
- 1946
Reason: All are plausible years during or near WWII but not when it ended

Example 4:
Question: Who wrote Romeo and Juliet?
Answer: William Shakespeare
Wrong Answers:
- Christopher Marlowe
- Ben Jonson
- John Webster
Reason: All are prominent Elizabethan playwrights

Example 5:
Question: What is the largest planet in our solar system?
Answer: Jupiter
Wrong Answers:
- Saturn
- Neptune
- Uranus
Reason: All are gas giant planets, but smaller than Jupiter

Please generate three wrong answers that follow these guidelines for the given question.
The answers should be:
- Factually incorrect but plausible
- Match the same answer type (e.g. date, person, number)
- Clearly distinct from the correct answer and each other
- Similar in specificity/detail level
- Realistic and not obviously wrong

Return only three wrong answers as a list in JSON format with the following requirements:
- Each wrong answer should be a string
- The output should be a single JSON object with key "wrong_answers" 
- The value should be an array of exactly 3 wrong answers
- No explanations or additional text should be included
- The answers should maintain consistent formatting with the correct answer

Example format:
{{
    "wrong_answers": ["opt1", "opt2", "opt3"]
}}

Question: {question}
Correct Answer: {answer}
Generate three wrong answers:
"""
\end{lstlisting}


\begin{table*}[!bp]
    \centering
    \footnotesize
    \caption{Performance metrics of LLMs in the Normal (\textbf{$\mathcal{N}$}) and Distractor (\textbf{$\mathcal{D}$}) settings, including accuracy (\texttt{\tiny correct}), non-attempt (\texttt{\tiny na}), ECE, and the number of helped (\textbf{$\mathcal{D}_{helped}$}) and harmed (\textbf{$\mathcal{D}_{harmed}$}) instances.}
    \begin{tabular}{l|ccc|ccc|cc}
        \toprule
        \textbf{LLMs} & \textbf{$\mathcal{N}_{correct}$} & \textbf{$\mathcal{N}_{none}$} & \textbf{$\mathcal{N}_{ECE}$} & \textbf{$\mathcal{D}_{correct}$} & \textbf{$\mathcal{D}_{none}$} & \textbf{$\mathcal{D}_{ECE}$} & \textbf{$\mathcal{D}_{helped}$} & \textbf{$\mathcal{D}_{harmed}$} \\
        \midrule
        \texttt{GPT-4o-mini} \raisebox{-1pt}{\includegraphics[height=0.8em]{figures/openai.png}} & 8.46\% & 6.80\% & 0.750 & 47.43\%  & 0.02\% & 0.320 & 1644 (93.78\%) & 109 \\
        \texttt{GPT-4-turbo} \raisebox{-1pt}{\includegraphics[height=0.8em]{figures/openai.png}} & 20.99\% & 7.14\% & 0.616 & 65.33\%  & 0.02\% & 0.165 & 1821 (95.44\%) & 87 \\
        \texttt{GPT-4o} \raisebox{-1pt}{\includegraphics[height=0.8em]{figures/openai.png}} & 36.75\% & 8.16\% & 0.437 & 73.48\% & 0\%  & 0.037 & 1507 (91.22\%) & 145 \\  \midrule
        \texttt{LLaMA3.1-8b-instant} \raisebox{-1pt}{\includegraphics[height=0.8em]{figures/meta.png}} & 8.24\% & 19.58\% & 0.780  & 44.94\% & 0.21\% & 0.367 & 1294 (91.45\%) & 121 \\
        \texttt{LLaMA 3-8B-8192} \raisebox{-1pt}{\includegraphics[height=0.8em]{figures/meta.png}} & 9.27\% & 24.99\% & 0.790 & 45.56\%  & 2.45\% & 0.355 & 1251 (90.46\%) & 132 \\ 
        \texttt{Gemma2-9B-it} \raisebox{-1pt}{\includegraphics[height=0.8em]{figures/google.png}} & 9.52\% & 34.49\% & 0.771 & 46.58\%  & 1.87\% & 0.359  & 1060 (88.70\%) & 135 \\
        \bottomrule
    \end{tabular}
    \label{acl_table}
\end{table*}

\section{Same LLM judge as Prediction LLM}
\label{appendix_different_llm_judge}
We employ the same LLM as both the judge and the base model responsible for predicting answers to the questions. The performance metrics are presented in Table \ref{acl_table}. Upon manually inspecting instances from smaller LLMs, we observe that the LLM judge occasionally misclassifies responses and refrains from assigning $\texttt{NOT\_ATTEMPTED}$ to certain data points. This can be seen by comparing \textbf{$\mathcal{N}_{none}$} in Table\ref{main_table} and \ref{acl_table}. To address this issue and ensure consistency, we use \texttt{gpt-4o-mini} as the LLM judge across all models. We also create reliability diagrams of pipeline where LLM-judge was different and the graphs are shown in Figure \ref{fig_appendix}.

To validate the reliability of \texttt{GPT-4o-mini} as an LLM judge, we conducted a small-scale human evaluation. We sampled 100 responses and had three human annotators independently classify them as $\texttt{CORRECT}$, $\texttt{INCORRECT}$, or $\texttt{NOT\_ATTEMPTED}$. The inter-annotator agreement, measured using Cohen’s kappa, was 0.82 for \texttt{GPT-4o-mini}, indicating substantial agreement. This comparison allowed us to measure the extent of biases introduced by automated evaluation and confirm that LLM judges generally aligned with human judgments, though minor inconsistencies were observed in ambiguous cases.




\begin{figure*}[!tbp]
    \centering
    \begin{minipage}{0.329\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/gpt-4o-mini.pdf}
        % \caption{Subfig 1}
    \end{minipage}
    \begin{minipage}{0.329\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/gpt-4-turbo.pdf}
        % \caption{Subfig 2}
    \end{minipage}
    \begin{minipage}{0.329\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/gpt-4o.pdf}
        % \caption{Subfig 3}
    \end{minipage}
    
    \vspace{0.05cm} % Space between rows

    \begin{minipage}{0.329\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/llama-3.1-8b-instant.pdf}
        % \caption{Subfig 1}
    \end{minipage}
    \begin{minipage}{0.329\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/llama3-8b-8192.pdf}
        % \caption{Subfig 2}
    \end{minipage}
    \begin{minipage}{0.329\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/gemma2-9b-it.pdf}
        % \caption{Subfig 3}
    \end{minipage}
    \caption{Reliability diagrams (RDs) showing calibration performance in \textbf{$\mathcal{N}$} (\textcolor[HTML]{A11B9B}{$\bullet$}) and \textbf{$\mathcal{D}$} (\textcolor[HTML]{fcce25}{$\bullet$}) settings. The numbers on top of bars represent the number of correctly predicted instances. \small (y-axis: actual accuracy, x-axis: predicted confidence)}
    \label{fig_appendix}
\end{figure*}



\section{LLMs details}
\label{selected_models}
In our study, we utilize six distinct LLMs representing a diverse range of capabilities and architectures. The GPT models (\texttt{GPT-4o-mini}, \texttt{GPT-4-turbo}, and \texttt{GPT-4o}) are state-of-the-art LLMs provided via OpenAI's API. These models are known for their advanced reasoning, contextual understanding, and robust performance across a variety of tasks. The larger models, such as \texttt{GPT-4o}, excel in handling complex queries and demonstrate superior accuracy, while the smaller variant, \texttt{GPT-4o-mini}, serves as a lightweight alternative with reduced computational overhead. We use GroqCloud for the \texttt{LLaMA3.1-8B-instant} and \texttt{LLaMA3-8B-8192} models. These models are smaller in scale but are effective in tasks requiring efficient knowledge retrieval and basic reasoning. Finally, \texttt{Gemma2-9B-it}, also from GroqCloud, offers a balance between size and capability, though its performance is limited by its relatively smaller model size and less extensive pretraining. Together, these models provide a comprehensive landscape for evaluating calibration, accuracy, and decision-making across diverse architectures and computational setups.


\end{document}
