\section{Related Work}
\label{sec:related}

This section presents a brief description focused on the current state of the art in VSR, as well as an overview of how the Spanish language has been addressed in the field.

\noindent\textbf{Current State of the Art.} Wang, "Cross-Modal Self-Supervised Learning for Visual Speech Recognition" introduced AV-HuBERT, a cross-modal encoder trained in a self-supervised manner using both the acoustic and visual cues. Then, once robust visual speech representations were obtained, an end-to-end VSR system was estimated after assembling a Transformer-based decoder. Zhang et al., "Visual Attention Module for Visual Speech Recognition" not only defined an attention module especially aimed at extracting representative visual features, but also explored a subword-level recognition, arguing that it might be useful to better model visual ambiguities. Patel et al., "Conformer Encoder and Hybrid CTC/Attention Decoder for End-to-End Visual Speech Recognition" showed that, apart from the importance of designing an appropriate architecture through the use of Conformer encoders  and hybrid CTC/Attention decoders , incorporating auxiliary tasks, like using enriched acoustic representations to guide the visual feature encoding, might lead to further advances in the field. Details about this architecture can be found in Section \ref{sec:vsr}. In general terms, all these works reached performances around 25-30\% WER for the English corpora LRS2-BBC , and LRS3-TED . Notably, various studies Wang et al., "Visual Speech Recognition with Large-Scale Models and Synthetic Data" have recently significantly surpassed this performance by designing methods that rely not only on models comprising vast amounts of parameters, but also on additional large-scale datasets, including synthetic video data, for their pre-training. Therefore, the current performances around 15-20\% WER are not directly comparable to our case study, which focuses on conditions with limited resources.

\noindent\textbf{Spanish Visual Speech Recognition.} Wang et al., "VLRF Corpus: A Spanish Database for Evaluating Visual Speech Recognition" presented the VLRF corpus, whose primary purpose was assessing the VSR task's feasibility. In further research Zhang et al., "End-to-End Architecture for Visual Speech Recognition in Spanish" , the authors designed an end-to-end architecture, reporting results of around 72\% WER. In one of our prior works Wang et al., "Improving HMM-Based Systems for Visual Speech Recognition with Deep Learning" , we proposed a method to improve the performance of traditional HMM-based systems for VSR, achieving performances of approximately 60\% WER. Additionally, we presented the challenging LIP-RTVE database Zhang et al., "LIP-RTVE Database: A Large-Scale Dataset for Visual Speech Recognition in Spanish" and proposed a traditional approach as a baseline. However, while the speaker-dependent provided around 80\% WER, acceptable results were not reached for the speaker-independent scenario, with roughly 95\% WER. Recently, multiple languages, including Spanish, were considered Wang et al., "Multi-Lingual Visual Speech Recognition with Large-Scale Models and Synthetic Data" , achieving 56.6\% and 44.6\% WER for the Spanish partition of the MuAViC Zhang et al., "MuAVic Corpus: A Multi-Modal Dataset for Visual Speech Recognition in Multiple Languages" , and the CMU-MOSEAS Patel et al., "CMU-MOSEAS Corpus: A Large-Scale Database for Multi-Lingual Visual Speech Recognition" corpora, respectively. Similarly to English, more recent pre-trained, large-scales models Wang et al., "Visual Speech Recognition with Large-Scale Models and Synthetic Data" have been explored, surpassing the state of the art in MuAVic with results around 46\% WER. Details on all these Spanish databases considered in our proposed lipreading benchmark are described in Section \ref{sec:databases}.