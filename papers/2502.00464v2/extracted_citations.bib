@article{adriana2022alr,
  author={Fernandez-Lopez, A. and Sukno, F. M.},
  journal={IEEE/ACM TASLP}, 
  title={End-to-{E}nd {L}ip-{R}eading {W}ithout {L}arge-{S}cale {D}ata}, 
  year={2022},
  volume={30},
  number={},
  pages={2076-2090},
  doi={10.1109/TASLP.2022.3182274}}

@article{afouras2018lrs3,
  title={L{RS3}-{TED}: a large-scale dataset for visual speech recognition},
  author={Afouras, T. and Chung, J.-S. and Zisserman, A.},
  journal={arXiv preprint arXiv:1809.00496},
  year={2018},
  url={\url{https://doi.org/10.48550/arXiv.1809.00496}}
}

@inproceedings{anwar23muavic,
  author={M. Anwar and B. Shi and V. Goswami and W. Hsu and J. Pino and C. Wang},
  title={{MuAViC: A Multilingual Audio-Visual Corpus for Robust Speech Recognition and Robust Speech-to-Text Translation}},
  year=2023,
  booktitle={Interspeech},
  pages={4064--4068},
  doi={10.21437/Interspeech.2023-2279}
}

@inproceedings{fernandez2017towards,
  Title={Towards estimating the upper bound of visual-speech recognition: The visual lip-reading feasibility database},
  Author={Fernandez-Lopez, A. and Martinez, O. and Sukno, F. M.},
  Booktitle={12th FG},
  Pages={208--215},
  Year={2017},
  url={\url{https://doi.org/10.1109/FG.2017.34}},
  Organization={IEEE}
}

@article{gimeno2024continuous,
  title={{Continuous lipreading based on acoustic temporal alignments}},
  author={Gimeno-G{\'o}mez, D. and Mart{\'\i}nez-Hinarejos, C.-D.},
  journal={EURASIP Journal on Audio, Speech, and Music Processing},
  volume={2024},
  number={1},
  pages={25},
  year={2024},
  publisher={Springer}
}

@inproceedings{gulati20_interspeech,
  author={A. Gulati and J. Qin and C. C. Chiu and N. Parmar and Y. Zhang and J. Yu and W. Han and S. Wang and Z. Zhang and Y. Wu and R. Pang},
  title={{Conformer: Convolution-augmented Transformer for Speech Recognition}},
  year=2020,
  booktitle={{Proc. Interspeech}},
  pages={5036--5040},
  url={\url{https://doi.org/10.21437/Interspeech.2020-3015}}
}

@inproceedings{liu2023synthvsr,
  title={{SynthVSR: Scaling Up Visual Speech Recognition With Synthetic Supervision}},
  author={Liu, X. and Lakomkin, E. and Vougioukas, K. and Ma, P. and Chen, H. and Xie, R. and Doulaty, M. and Moritz, N. and Kolar, J. and Petridis, S. and Pantic, M. and Fuegen, C.},
  booktitle={CVPR},
  pages={18806--18815},
  year={2023}
}

@inproceedings{lrec2022liprtve,
  author={D. Gimeno-Gómez and C.-D. Martínez-Hinarejos},
  title={L{IP-RTVE}: {A}n {A}udiovisual {D}atabase for {C}ontinuous {S}panish in the {W}ild},
  booktitle={{Proc. LREC}},
  year={2022},
  publisher={ELRA},
  pages={2750--2758},
  url={\url{https://aclanthology.org/2022.lrec-1.294}},
}

@article{ma2022visual,
  title={Visual Speech Recognition for Multiple Languages in the Wild},
  author={Ma, P. and Petridis, S. and Pantic, M.},
  journal={Nature Machine Intelligence},
  pages={930--939},
  volume={4},
  number={11},
  year={2022},
  doi={10.1038/s42256-022-00550-z},
}

@inproceedings{ma2023auto,
  author={Ma, P. and Haliassos, A. and Fernandez-Lopez, A. and Chen, H. and Petridis, S. and Pantic, M.},
  booktitle={ICASSP}, 
  title={Auto-AVSR: Audio-Visual Speech Recognition with Automatic Labels}, 
  year={2023},
  volume={},
  number={},
  pages={1-5},
  doi={10.1109/ICASSP49357.2023.10096889}
}

@inproceedings{prajwal2021sub,
    author={Prajwal, K. R. and Afouras, T. and Zisserman, A.},
    title={Sub-Word Level Lip Reading With Visual Attention},
    booktitle={{CVPR}},
    year={2022},
    pages={5162-5172},
    url={\url{https://openaccess.thecvf.com/content/CVPR2022/html/Prajwal_Sub-Word_Level_Lip_Reading_With_Visual_Attention_CVPR_2022_paper.html}}
}

@inproceedings{salesky21_interspeech,
  author={E. Salesky and M. Wiesner and J. Bremerman and R. Cattoni and M. Negri and M. Turchi and D. W. Oard and M. Post},
  title={{The Multilingual TEDx Corpus for Speech Recognition and Translation}},
  year=2021,
  booktitle={Proc. Interspeech},
  pages={3655--3659},
  url={\url{https//doi.org/10.21437/Interspeech.2021-11}}
}

@article{shi2022learning,
  title={Learning audio-visual speech representation by masked multimodal cluster prediction},
  author={Shi, B. and Hsu, W. N. and Lakhotia, K. and Mohamed, A.},
  journal={arXiv preprint arXiv:2201.02184},
  year={2022},
  url={\url{https://doi.org/10.48550/arXiv.2201.02184}}
}

@inproceedings{son2017lrs2,
  title={Lip reading sentences in the wild},
  author={Son Chung, J. and Senior, A. and Vinyals, O. and Zisserman, A.},
  booktitle={{CVPR}},
  pages={6447--6456},
  year={2017},
  url={\url{https://openaccess.thecvf.com/content_cvpr_2017/html/Chung_Lip_Reading_Sentences_CVPR_2017_paper.html}}
}

@article{watanabe2017ctcattention,
  author={Watanabe, S. and Hori, T. and Kim, S. and Hershey, J. R. and Hayashi, T.},
  journal={IEEE JSTSP},
  title={Hybrid CTC/Attention Architecture for End-to-End Speech Recognition}, 
  year={2017},
  volume={11},
  number={8},
  pages={1240-1253},
  doi={10.1109/JSTSP.2017.2763455}}

@inproceedings{yeo2024limited,
  author={Yeo, J. H. and Kim, M. and Watanabe, S. and Ro, Y. M.},
  booktitle={ICASSP}, 
  title={Visual Speech Recognition for Languages with Limited Labeled Data Using Automatic Labels from Whisper}, 
  year={2024},
  volume={},
  number={},
  pages={10471-10475},
  doi={10.1109/ICASSP48485.2024.10446720},
}

@inproceedings{zadeh2020moseas,
  title={C{MU-MOSEAS}: A Multimodal Language Dataset for Spanish, Portuguese, German and French},
  author={Zadeh, A. B. and Cao, Y. and Hessner, S. and Liang, P. P. and Poria, S. and Morency, L.-P.},
  booktitle={{EMNLP}},
  pages={1801--1812},
  url={\url{https://doi.org/10.18653/v1/2020.emnlp-main.141}},
  year={2020}
}

