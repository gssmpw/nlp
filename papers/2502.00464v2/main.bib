%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Human-based Background %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%
@article{mcgurk1976hearing,
	Author={McGurk, H. and MacDonald, J.},
	Journal={Nature},
	Number={5588},
	Pages={746--748},
	Title={Hearing lips and seeing voices},
	Volume={264},
	Year={1976},
	doi={10.1038/264746a0},
	Publisher={Nature Publishing Group}
}

 @article{besle2004bimodal,
  title={Bimodal speech: early suppressive visual effects in human auditory cortex},
  author={Besle, J. and Fort, A. and Delpuech, C. and Giard, Marie-H.},
  journal={European journal of Neuroscience},
  volume={20},
  number={8},
  pages={2225--2234},
  year={2004},
  doi={10.1111\%2Fj.1460-9568.2004.03670.x},
  publisher={Wiley Online Library},
}

@article{campbell2008processing,
  title={{The Processing of Audio-Visual Speech: Empirical and Neural Bases}},
  author={Campbell, R.},
  journal={Philosophical Transactions of the Royal Society B: Biological Sciences},
  volume={363},
  number={1493},
  pages={1001--1010},
  year={2008},
  publisher={The Royal Society London}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Auditory Approaches %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%
@book{gales2008application,
  Author = {Gales, M. and Young, S.},
  Title = {The application of hidden Markov models in speech recognition},
  Year = {2008},
  Publisher = {Now Publishers Inc},
  url={\url{http://doi.org/10.1561/2000000004}}
}

@article{baevski2020wav2vec,
  title={wav2vec 2.0: A framework for self-supervised learning of speech representations},
  author={Baevski, A. and Zhou, Y. and Mohamed, A. and Auli, M.},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={12449--12460},
  year={2020},
  doi={10.5555/3495724.3496768}
}

%%%%%%%%%%%%%%%%%%
%% Applications %%
%%%%%%%%%%%%%%%%%%

@article{silent2020passwd,
  author={Ezz, M. and Mostafa, A. M. and Nasr, A. A.},
  journal={IEEE Access}, 
  title={{A Silent Password Recognition Framework Based on Lip Analysis}}, 
  year={2020},
  volume={8},
  number={},
  pages={55354-55371}
}

@inproceedings{stafylakis2018zero,
  title={{Zero-Shot Keyword Spotting for Visual Speech Recognition In-the-Wild}},
  author={Stafylakis, T. and Tzimiropoulos, G.},
  booktitle={Proc. of ECCV},
  pages={513--529},
  year={2018},
}

@article{jha2019spotting,
  title={{Spotting Words in Silent Speech Videos: A Retrieval-Based Approach}},
  author={Jha, A. and Namboodiri, V. P. and Jawahar, C. V.},
  journal={Machine Vision and Applications},
  volume={30},
  pages={217--229},
  year={2019},
  publisher={Springer}
}

@inproceedings{theobald2006law,
  title={{Lip-Reading Enhancement for Law Enforcement}},
  author={Theobald, B. J. and Harvey, R. and Cox, S. J. and Lewis, C. and Owen, G. P.},
  booktitle={Optics and Photonics for Counterterrorism and Crime Fighting II},
  volume={6402},
  pages={24--32},
  year={2006},
  organization={SPIE}
}

@article{bowden2013recent,
  title={Recent Developments in Automated Lip-Reading},
  author={Bowden, R. and Cox, S. and Harvey, R. and Lan, Y. and Ong, E.-J. and Owen, G. and Theobald, B.-J.},
  journal={Optics and Photonics for Counterterrorism, Crime Fighting and Defence IX; and Optical Materials and Biomaterials in Security and Defence Systems Technology X},
  volume={8901},
  pages={179--191},
  year={2013},
  publisher={SPIE}
}

@article{laux2023care,
  title={{Two-Stage Visual Speech Recognition for Intensive Care Patients}},
  author={Laux, H. and Hallawa, A. and Assis, J. C. S. and Schmeink, A. and Martin, L. and Peine, A.},
  journal={Scientific Reports},
  volume={13},
  number={1},
  pages={928},
  year={2023},
  publisher={Nature Publishing Group UK London}
}

@article{musalia2023liopa,
  title={A User Evaluation of Speech/Phrase Recognition Software in Critically Ill Patients: A DECIDE-AI Feasibility Study},
  author={Musalia, M. and Laha, S. and Cazalilla-Chica, J. and Allan, J. and Roach, L. and Twamley, J. and Nanda, S. and Verlander, M. and Williams, A. and Kempe, I. and Patel, I. I. and Campbel-West, F. and Blackwood, B. and McAuley, D. F.},
  journal={{Critical Care}},
  volume={27},
  number={1},
  pages={277},
  year={2023},
  publisher={Springer}
}

@article{koller2015continuous,
  title={{Continuous Sign Language Recognition: Towards Large Vocabulary Statistical Recognition Systems Handling Multiple Signers}},
  author={Koller, O. and Forster, J. and Ney, H.},
  journal={{Computer Vision and Image Understanding}},
  volume={141},
  pages={108--125},
  year={2015},
  publisher={Elsevier},
  doi={10.1016/j.cviu.2015.09.013},
}

@article{ivanko2019signlip,
author={Ivanko, D. and Ryumin, D. and Karpov, A.},
title={{Automatic Lip-Reading of Hearing Impaired People}},
journal={The International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences},
volume={XLII-2/W12},
year={2019},
pages={97--101},
doi={10.5194/isprs-archives-XLII-2-W12-97-2019},
}

@inproceedings{liao2023lightasd,
  author={Liao, J. and Duan, H. and Feng, K. and Zhao, W. and Yang, Y. and Chen, L.},
  title={{A Light Weight Model for Active Speaker Detection}},
  booktitle={Proc. of the IEEE/CVF CVPR},
  year={2023},
  pages={22932-22941}
}

@inproceedings{tao2021talknetasd,
author = {Tao, R. and Pan, Z. and Das, R. and Qian, X. and Shou, M. and Li, h.},
title = {{Is Someone Speaking? Exploring Long-Term Temporal Features for Audio-Visual Active Speaker Detection}},
year = {2021},
isbn = {9781450386517},
publisher = {Association for Computing Machinery},
doi = {10.1145/3474085.3475587},
booktitle = {Proc. of the 29th ACM International Conference on Multimedia},
pages = {3927–3935},
numpages = {9},
}

@inproceedings{prajwal2020lip,
  title={A Lip Sync Expert Is All You Need for Speech to Lip Generation in the Wild},
  author={Prajwal, K.R. and Mukhopadhyay, R. and Namboodiri, V. and Jawahar, C. V.},
  booktitle={Proceedings of the 28th ACM international conference on multimedia},
  pages={484--492},
  year={2020}
}

@inproceedings{park2024facetoface,
  author={S. J. Park and C. W. Kim and H. Rha and M. Kim and J. Hong and J. Yeo and Y. M. Ro},
  title={{Let's Go Real Talk: Spoken Dialogue Model for Face-to-Face Conversation}},
  year={2024},
  booktitle={Proc. of the 62nd ACL},
  pages={16334--16348},
}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Visual Speech Recognition Challenges %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

@inproceedings{fernandez2017optimizing,
  Title = {Optimizing Phoneme-to-Viseme Mapping for Continuous Lip-Reading in Spanish},
  Author = {Fernández-López, A. and Sukno, F.},
  Booktitle = {International Joint Conference on Computer Vision, Imaging and Computer Graphics},
  Pages = {305--328},
  Year = {2017},
  Organization = {Springer},
  url={\url{https://doi.org/10.1007/978-3-030-12209-6_15}},
}

@inproceedings{bear2014phoneme,
  title={Which phoneme-to-viseme maps best improve visual-only computer lip-reading?},
  author={Bear, H. and Harvey, R. and Theobald, B. and Lan, Y.},
  booktitle={International Symposium on Visual Computing},
  pages={230--239},
  year={2014},
  organization={Springer},
  url={\url{https://doi.org/10.1007/978-3-319-14364-4_22}},
}

@inproceedings{bear2016decoding,
  title={Decoding visemes: Improving machine lip-reading},
  author={Bear, H. and Harvey, R.},
  booktitle={I{CASSP}},
  pages={2009--2013},
  year={2016},
  organization={IEEE},
  url={\url{https://doi.org/10.1109/ICASSP.2016.7472029}},
}

@inproceedings{bear2014resolution,
  title={Resolution limits on visual speech recognition},
  author={Bear, H. and Harvey, R. and Theobald, B. and Lan, Y.},
  booktitle={I{CIP}},
  pages={1371--1375},
  year={2014},
  organization={IEEE},
  url={\url{https://doi.org/10.1109/ICIP.2014.7025274}},
}

@phdthesis{thangthai2018computer,
  title={Computer lipreading via hybrid deep neural network hidden Markov models},
  author={Thangthai, K.},
  year={2018},
  school={University of East Anglia},
  url={\url{https://ueaeprints.uea.ac.uk/id/eprint/69215}}
}

@inproceedings{dungan2018impact,
  author={Dungan, L. and Karaali, A. and Harte, N.},
  booktitle={I{CIP}}, 
  title={The Impact of Reduced Video Quality on Visual Speech Recognition}, 
  year={2018},
  volume={},
  number={},
  pages={2560-2564},
  url={\url{http://doi.org/10.1109/ICIP.2018.8451754}}
}

@inproceedings{cox2008challenge,
  title={The challenge of multispeaker lip-reading.},
  author={Cox, Stephen J and Harvey, Richard W and Lan, Yuxuan and Newman, Jacob L and Theobald, Barry-John},
  booktitle={A{VSP}},
  pages={179--184},
  year={2008},
  url={\url{https://www.isca-speech.org/archive_open/avsp08/av08_179.html}}
}

@article{duchnowski2000development,
  title={Development of speechreading supplements based on automatic speech recognition},
  author={Duchnowski, P. and Lum, D. and Krause, J.C and Sexton, M. and Bratakos, M. and Braida, L.},
  journal={IEEE trans. on biomedical engineering},
  volume={47},
  number={4},
  pages={487--496},
  year={2000},
  doi={10.1109/10.828148}
}

@inproceedings{leung04interspeech,
  author={K.-Y. Leung and Man-Wai Mak and Sun-Yuan Kung},
  title={{Articulatory feature-based conditional pronunciation modeling for speaker verification}},
  year=2004,
  booktitle={Interspeech},
  pages={2597--2600},
  doi={10.21437/Interspeech.2004-545},
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Audiovisual Approaches %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%
@article{potamianos2003recent,
  Title = {Recent advances in the automatic recognition of audiovisual speech},
  Author = {Potamianos, G. and Neti, C. and Gravier, G. and Garg, A. and Senior, A.},
  Journal = {Proc. of the IEEE},
  Volume = {91},
  Number = {9},
  Pages = {1306--1326},
  Year = {2003},
  doi={10.1109/JPROC.2003.817150},
  Publisher = {IEEE}
}

@inproceedings{maja2021conformers,
    author={Ma, P. and Petridis, S. and Pantic, M.},
    booktitle={I{CASSP}}, 
    title={End-To-End Audio-Visual Speech Recognition with Conformers}, 
    year={2021},
    volume={},
    number={},
    pages={7613-7617},
    url={\url{https://doi.org/10.1109/ICASSP39728.2021.9414567}}
}

%%%%%%%%%%%%%%%%%%%%%%%
%% Surveys & Reviews %%
%%%%%%%%%%%%%%%%%%%%%%%
@article{fernandez2018survey,
    Title={Survey on automatic lip-reading in the era of deep learning},
    Author={Fernandez-Lopez, A. and Sukno, F. M.},
    Journal={Image and Vision Computing},
    Volume={78},
    Pages={53--72},
    Year={2018},
    doi={https://doi.org/10.1016/j.imavis.2018.07.002},
    Publisher={Elsevier}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% CTC/Attention Architecture %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
@inproceedings{graves2006ctc,
  author={Graves, A. and Fern\'{a}ndez, S. and Gomez, F. and Schmidhuber, J.},
  title={Connectionist Temporal Classification: Labelling Unsegmented Sequence Data with Recurrent Neural Networks},
  year={2006},
  isbn={1595933832},
  publisher={ACM},
  url={\url{https://doi.org/10.1145/1143844.1143891}},
  booktitle={23rd ICML},
  pages={369–376},
  numpages={8},
}

@article{vaswani2017attention,
    title={Attention is all you need},
    author={Vaswani, A. and Shazeer, N. and Parmar, N. and Uszkoreit, J. and Jones, L. and Gomez, A. N and Kaiser, L. and Polosukhin, I.},
    journal={NeurIPS},
    volume={30},
    pages={6000--6010},
    year={2017},
    url={\url{https://dl.acm.org/doi/10.5555/3295222.3295349}}
}

@article{watanabe2017ctcattention,
  author={Watanabe, S. and Hori, T. and Kim, S. and Hershey, J. R. and Hayashi, T.},
  journal={IEEE JSTSP},
  title={Hybrid CTC/Attention Architecture for End-to-End Speech Recognition}, 
  year={2017},
  volume={11},
  number={8},
  pages={1240-1253},
  doi={10.1109/JSTSP.2017.2763455}}
  
@inproceedings{watanabe2019mhaasr,
    author={Karita, S. and Chen, N. and Hayashi, T. and Hori, T. and Inaguma, H. and Jiang, Z. and Someki, M. and Soplin, N. E. Y. and Yamamoto, R. and Wang, X. and Watanabe, S. and Yoshimura, T. and Zhang, W.},
    booktitle={ASRU}, 
    title={A Comparative Study on Transformer vs RNN in Speech Applications}, 
    year={2019},
    volume={},
    number={},
    pages={449-456},
    doi={10.1109/ASRU46091.2019.9003750}
}

@inproceedings{lee2021interctc,
  author={Lee, J. and Watanabe, S.},
  booktitle={{ICASSP}}, 
  title={{Intermediate Loss Regularization for CTC-Based Speech Recognition}}, 
  year={2021},
  volume={},
  number={},
  pages={6224-6228},
  doi={10.1109/ICASSP39728.2021.9414594}
}

@inproceedings{higuchi2021maskctc,
  author={Higuchi, Y. and Inaguma, H. and Watanabe, S. and Ogawa, T. and Kobayashi, T.},
  booktitle={{ICASSP}}, 
  title={{Improved Mask-CTC for Non-Autoregressive End-to-End ASR}}, 
  year={2021},
  volume={},
  number={},
  pages={8363-8367},
  doi={10.1109/ICASSP39728.2021.9414198}}

@article{wei2023sim,
  title={{Sim-T: Simplify the Transformer Network by Multiplexing Technique for Speech Recognition}},
  author={Wei, G. and Duan, Z. and Li, S. and Yang, G. and Yu, X. and Li, J.},
  journal={{arXiv preprint arXiv:2304.04991}},
  year={2023}
}

@article{fernandez2023sparsevsr,
  title={{Sparsevsr: Lightweight and noise robust visual speech recognition}},
  author={Fernandez-Lopez, A. and Chen, H. and Ma, P. and Haliassos, A. and Petridis, S. and Pantic, M.},
  journal={{arXiv preprint arXiv:2307.04552}},
  year={2023}
}

%%%%%%%%%%%%%%%%%%%%%%
%% VSR Architecture %%
%%%%%%%%%%%%%%%%%%%%%%
@inproceedings{he2016resnet,
  author={He, K. and Zhang, X. and Ren, S. and Sun, J.},
  booktitle={{CVPR}}, 
  title={Deep Residual Learning for Image Recognition}, 
  year={2016},
  volume={},
  number={},
  pages={770-778},
  url={\url{https://doi.org/10.1109/CVPR.2016.90}}
  }

%% Swish Activation Function %%
@article{swish2017prajit,
  author={P. Ramachandran and B. Zoph and Q. V. Le},
  title={Searching for Activation Functions},
  journal={arXiv preprint arXiv:1710.05941},
  volume={},
  pages={},
  year={2017},
  url={\url{https://arxiv.org/abs/1710.05941}}
}

%% Relative Postional Embedding %%
@inproceedings{dai2019transformerxl,
    title={Transformer-{XL}: Attentive Language Models beyond a Fixed-Length Context},
    author={Dai, Z. and Yang, Z. and Yang, Y. and Carbonell, J. and Le, Q. and Salakhutdinov, R.},
    booktitle={Proc. of the 57th ACL},
    year={2019},
    publisher ={ACL},
    doi={10.18653/v1/P19-1285},
    pages={2978--2988},
}

@inproceedings{gulati20_interspeech,
  author={A. Gulati and J. Qin and C. C. Chiu and N. Parmar and Y. Zhang and J. Yu and W. Han and S. Wang and Z. Zhang and Y. Wu and R. Pang},
  title={{Conformer: Convolution-augmented Transformer for Speech Recognition}},
  year=2020,
  booktitle={{Proc. Interspeech}},
  pages={5036--5040},
  url={\url{https://doi.org/10.21437/Interspeech.2020-3015}}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Current State of the Art in VSR %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
@article{shi2022learning,
  title={Learning audio-visual speech representation by masked multimodal cluster prediction},
  author={Shi, B. and Hsu, W. N. and Lakhotia, K. and Mohamed, A.},
  journal={arXiv preprint arXiv:2201.02184},
  year={2022},
  url={\url{https://doi.org/10.48550/arXiv.2201.02184}}
}

@inproceedings{haliassos2024braven,
  author={Haliassos, A. and Zinonos, A. and Mira, R. and Petridis, S. and Pantic, M.},
  booktitle={ICASSP}, 
  title={{BRAVEn: Improving Self-supervised pre-training for Visual and Auditory Speech Recognition}}, 
  year={2024},
  volume={},
  number={},
  pages={11431-11435},
  doi={10.1109/ICASSP48485.2024.10448473},
}

@article{ma2022visual,
  title={Visual Speech Recognition for Multiple Languages in the Wild},
  author={Ma, P. and Petridis, S. and Pantic, M.},
  journal={Nature Machine Intelligence},
  pages={930--939},
  volume={4},
  number={11},
  year={2022},
  doi={10.1038/s42256-022-00550-z},
}

@inproceedings{ma2023auto,
  author={Ma, P. and Haliassos, A. and Fernandez-Lopez, A. and Chen, H. and Petridis, S. and Pantic, M.},
  booktitle={ICASSP}, 
  title={Auto-AVSR: Audio-Visual Speech Recognition with Automatic Labels}, 
  year={2023},
  volume={},
  number={},
  pages={1-5},
  doi={10.1109/ICASSP49357.2023.10096889}
}

@inproceedings{liu2023synthvsr,
  title={{SynthVSR: Scaling Up Visual Speech Recognition With Synthetic Supervision}},
  author={Liu, X. and Lakomkin, E. and Vougioukas, K. and Ma, P. and Chen, H. and Xie, R. and Doulaty, M. and Moritz, N. and Kolar, J. and Petridis, S. and Pantic, M. and Fuegen, C.},
  booktitle={CVPR},
  pages={18806--18815},
  year={2023}
}

@inproceedings{prajwal2021sub,
    author={Prajwal, K. R. and Afouras, T. and Zisserman, A.},
    title={Sub-Word Level Lip Reading With Visual Attention},
    booktitle={{CVPR}},
    year={2022},
    pages={5162-5172},
    url={\url{https://openaccess.thecvf.com/content/CVPR2022/html/Prajwal_Sub-Word_Level_Lip_Reading_With_Visual_Attention_CVPR_2022_paper.html}}
}

@INPROCEEDINGS{chang2024conformervsr,
  author={Chang, O. and Liao, H. and Serdyuk, D. and Shah, A. and Siohan, O.},
  booktitle={ICASSP}, 
  title={{Conformer is All You Need for Visual Speech Recognition}}, 
  year={2024},
  volume={},
  number={},
  pages={10136-10140},
  doi={10.1109/ICASSP48485.2024.10446532},
}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Spanish Visual Speech Recognition %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
@article{adriana2022alr,
  author={Fernandez-Lopez, A. and Sukno, F. M.},
  journal={IEEE/ACM TASLP}, 
  title={End-to-{E}nd {L}ip-{R}eading {W}ithout {L}arge-{S}cale {D}ata}, 
  year={2022},
  volume={30},
  number={},
  pages={2076-2090},
  doi={10.1109/TASLP.2022.3182274}}
 
% Ma et al. (2022)

@inproceedings{gimenogomez21iberspeech,
  author={D. Gimeno-Gómez and C.-D. Martínez-Hinarejos},
  title={{Analysis of Visual Features for Continuous Lipreading in Spanish}},
  year=2021,
  booktitle={{Proc. IberSPEECH}},
  pages={220--224},
  url={\url{https://doi.org/10.21437/IberSPEECH.2021-47}}
}

@article{speakeradapted2023vsr,
author = {Gimeno-Gómez, D. and Martínez-Hinarejos, C.-D.},
title = {Comparing Speaker Adaptation Methods for Visual Speech Recognition for Continuous Spanish},
journal = {Applied Sciences},
volume = {13},
year = {2023},
number = {11},
article-number = {6521},
issn = {2076-3417},
DOI = {10.3390/app13116521}
}

@article{gimeno2024continuous,
  title={{Continuous lipreading based on acoustic temporal alignments}},
  author={Gimeno-G{\'o}mez, D. and Mart{\'\i}nez-Hinarejos, C.-D.},
  journal={EURASIP Journal on Audio, Speech, and Music Processing},
  volume={2024},
  number={1},
  pages={25},
  year={2024},
  publisher={Springer}
}

@inproceedings{kim2023lip,
  title={{Lip reading for low-resource languages by learning and combining general speech knowledge and language-specific knowledge}},
  author={Kim, M. and Yeo, J. Hun and Choi, J. and Ro, Y. M.},
  booktitle={ICCV},
  pages={15359--15371},
  year={2023}
}

@inproceedings{yeo2024limited,
  author={Yeo, J. H. and Kim, M. and Watanabe, S. and Ro, Y. M.},
  booktitle={ICASSP}, 
  title={Visual Speech Recognition for Languages with Limited Labeled Data Using Automatic Labels from Whisper}, 
  year={2024},
  volume={},
  number={},
  pages={10471-10475},
  doi={10.1109/ICASSP48485.2024.10446720},
}

@inproceedings{anwar23muavic,
  author={M. Anwar and B. Shi and V. Goswami and W. Hsu and J. Pino and C. Wang},
  title={{MuAViC: A Multilingual Audio-Visual Corpus for Robust Speech Recognition and Robust Speech-to-Text Translation}},
  year=2023,
  booktitle={Interspeech},
  pages={4064--4068},
  doi={10.21437/Interspeech.2023-2279}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Audiovisual Databases %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%
@inproceedings{fernandez2017towards,
  Title={Towards estimating the upper bound of visual-speech recognition: The visual lip-reading feasibility database},
  Author={Fernandez-Lopez, A. and Martinez, O. and Sukno, F. M.},
  Booktitle={12th FG},
  Pages={208--215},
  Year={2017},
  url={\url{https://doi.org/10.1109/FG.2017.34}},
  Organization={IEEE}
}

@inproceedings{son2017lrs2,
  title={Lip reading sentences in the wild},
  author={Son Chung, J. and Senior, A. and Vinyals, O. and Zisserman, A.},
  booktitle={{CVPR}},
  pages={6447--6456},
  year={2017},
  url={\url{https://openaccess.thecvf.com/content_cvpr_2017/html/Chung_Lip_Reading_Sentences_CVPR_2017_paper.html}}
}

@article{afouras2018lrs3,
  title={L{RS3}-{TED}: a large-scale dataset for visual speech recognition},
  author={Afouras, T. and Chung, J.-S. and Zisserman, A.},
  journal={arXiv preprint arXiv:1809.00496},
  year={2018},
  url={\url{https://doi.org/10.48550/arXiv.1809.00496}}
}

@inproceedings{zadeh2020moseas,
  title={C{MU-MOSEAS}: A Multimodal Language Dataset for Spanish, Portuguese, German and French},
  author={Zadeh, A. B. and Cao, Y. and Hessner, S. and Liang, P. P. and Poria, S. and Morency, L.-P.},
  booktitle={{EMNLP}},
  pages={1801--1812},
  url={\url{https://doi.org/10.18653/v1/2020.emnlp-main.141}},
  year={2020}
}

@inproceedings{salesky21_interspeech,
  author={E. Salesky and M. Wiesner and J. Bremerman and R. Cattoni and M. Negri and M. Turchi and D. W. Oard and M. Post},
  title={{The Multilingual TEDx Corpus for Speech Recognition and Translation}},
  year=2021,
  booktitle={Proc. Interspeech},
  pages={3655--3659},
  url={\url{https//doi.org/10.21437/Interspeech.2021-11}}
}

@inproceedings{lrec2022liprtve,
  author={D. Gimeno-Gómez and C.-D. Martínez-Hinarejos},
  title={L{IP-RTVE}: {A}n {A}udiovisual {D}atabase for {C}ontinuous {S}panish in the {W}ild},
  booktitle={{Proc. LREC}},
  year={2022},
  publisher={ELRA},
  pages={2750--2758},
  url={\url{https://aclanthology.org/2022.lrec-1.294}},
}

@inproceedings{ardila2020common,
    title={Common Voice: A Massively-Multilingual Speech Corpus},
    author={Ardila, R. and Branson, M. and Davis, K. and Kohler, M. and Meyer, J. and Henretty, M. and Morais, R. and Saunders, L. and Tyers, F.  and Weber, G.},
    booktitle={{Proc. LREC}},
    year={2020},
    pages={4218--4222},
    url={\url{https://aclanthology.org/2020.lrec-1.520}}
}

@inproceedings{pratap20interspeech,
  author={V. Pratap and Q. Xu and A. Sriram and G. Synnaeve and R. Collobert},
  title={{MLS: A Large-Scale Multilingual Dataset for Speech Research}},
  year={2020},
  booktitle={Proc. Interspeech},
  pages={2757--2761},
  url={\url{https://doi.org/10.21437/Interspeech.2020-2826}}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Implementation Details %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%
@inproceedings{watanabe18_interspeech,
  author={S. Watanabe and T. Hori and S. Karita and T. Hayashi and J. Nishitoba and Y. Unno and N. {E. Y. Soplin} and J. Heymann and M. Wiesner and N. Chen and A. Renduchintala and T. Ochiai},
  title={{ESPnet: End-to-End Speech Processing Toolkit}},
  year=2018,
  booktitle={Proc. Interspeech},
  pages={2207--2211},
  url={\url{https://doi.org/10.21437/Interspeech.2018-1456}}
}


%%%%%%%%%%%%%%%%%%%%%
%% Data Processing %%
%%%%%%%%%%%%%%%%%%%%%
@inproceedings{deng2020retina,
  author={Deng, J. and Guo, J. and Ververas, E. and Kotsia, I. and Zafeiriou, S.},
  booktitle={{CVPR}}, 
  title={RetinaFace: Single-Shot Multi-Level Face Localisation in the Wild}, 
  year={2020},
  volume={},
  number={},
  pages={5202-5211},
  url={\url{10.1109/CVPR42600.2020.00525}}
}
  
@inproceedings{bulat2017facealign,
  author={Bulat, A. and Tzimiropoulos, G.},
  booktitle={ICCV}, 
  title={How Far are We from Solving the 2D \& 3D Face Alignment Problem? (and a Dataset of 230,000 3D Facial Landmarks)}, 
  year={2017},
  volume={},
  number={},
  pages={1021-1030},
  url={\url{https://doi.org/10.1109/ICCV.2017.116}}
}

%%%%%%%%%%%%%%%%%%%%
%% Training Setup %%
%%%%%%%%%%%%%%%%%%%%
@inproceedings{loshchilov2017decoupled,
  title={Decoupled Weight Decay Regularization},
  author={I. Loshchilov and F. Hutter},
  booktitle={ICLR},
  year={2019},
  url={\url{https://openreview.net/pdf?id=Bkg6RiCqY7}},
}

@inproceedings{leslie2019onecycle,
  author = {L. N. Smith and N. Topin},
  title = {{Super-convergence: very fast training of neural networks using large learning rates}},
  volume = {11006},
  booktitle = {AI and ML for Multi-Domain Operations Applications},
  organization = {SPIE},
  pages = {369--386},
  year = {2019},
  url = {\url{https://doi.org/10.1117/12.2520589}},
}


%%%%%%%%%%%%%%%%%%%%%%%
%% Evaluation Metric %%
%%%%%%%%%%%%%%%%%%%%%%%

@inproceedings{bisani2004bootstrap,
  Author = {Bisani, M. and Ney, H.},
  Title = {Bootstrap estimates for confidence intervals in ASR performance evaluation},
  Booktitle = {ICASSP},
  Volume = {1},
  Pages = {409--412},
  Year = {2004},
  url={\url{https://doi.org/10.1109/ICASSP.2004.1326009}}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Interesting Works derived from VSR %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
@inproceedings{pingchuan2021lira,
  author={Pingchuan Ma and Rodrigo Mira and Stavros Petridis and Björn W. Schuller and Maja Pantic},
  title={{Li{RA}: Learning Visual Speech Representations from Audio Through Self-Supervision}},
  year={2021},
  booktitle={Proc. Interspeech},
  pages={3011--3015},
  doi={10.21437/Interspeech.2021-1360}
}

@inproceedings{afouras2020distilling,
  author={Afouras, T. and Chung, Joon S. and Zisserman, A.},
  booktitle={{ICASSP}}, 
  title={ASR is All You Need: Cross-Modal Distillation for Lip Reading}, 
  year={2020},
  volume={},
  number={},
  pages={2143-2147},
  url={\url{https://doi.org/10.1109/ICASSP40776.2020.9054253}}
}
  
@inproceedings{akbari2018lip2aud,
  author={Akbari, H. and Arora, H. and Cao, L. and Mesgarani, N.},
  booktitle={{ICASSP}}, 
  title={Lip2Audspec: Speech Reconstruction from Silent Lip Movements Video}, 
  year={2018},
  volume={},
  number={},
  pages={2516-2520},
  doi={10.1109/ICASSP.2018.8461856}
}

@article{qu2021lipsound2,
  author={Qu, L. and Weber, C. and Wermter, S.},
  journal={IEEE Transactions on NNLS}, 
  title={LipSound2: Self-Supervised Pre-Training for Lip-to-Speech Reconstruction and Lip Reading}, 
  year={2022},
  volume={},
  number={},
  pages={1-11},
  doi={10.1109/TNNLS.2022.3191677}}
  
@inproceedings{mira2022svts,
  author={R. Mira and A. Haliassos and S. Petridis and B. W. Schuller and M. Pantic},
  title={{SVTS: Scalable Video-to-Speech Synthesis}},
  year=2022,
  booktitle={Proc. Interspeech},
  pages={1836--1840},
  doi={10.21437/Interspeech.2022-10770}
}
  
%%%%%%%%%%%%%%%%%%%
%% Study on ROIs %%
%%%%%%%%%%%%%%%%%%%
@inproceedings{zhang2020rois,
  author={Zhang, Y. and Yang, S. and Xiao, J. and Shan, S. and Chen, X.},
  booktitle={15th IEEE FG}, 
  title={Can We Read Speech Beyond the Lips? Rethinking RoI Selection for Deep Visual Speech Recognition}, 
  year={2020},
  volume={},
  number={},
  pages={356-363},
  doi={10.1109/FG47880.2020.00134}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Gradient Accumulation %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%

@inproceedings{ott2018accum,
    title={Scaling Neural Machine Translation},
    author={Ott, M. and Edunov, S. and Grangier, D. and Auli, M.},
    booktitle ={Proc. of the 3rd Conference on Machine Translation},
    year={2018},
    publisher={ACL},
    url={\url{https://doi.org/10.18653/v1/W18-6301}},
    pages={1--9},
}

%%%%%%%%%%%%%%%%%
%% Future Work %%
%%%%%%%%%%%%%%%%%


@InProceedings{adapters2019nlp,
  title={Parameter-Efficient Transfer Learning for {NLP}},
  author={Houlsby, N. and Giurgiu, A. and Jastrzebski, S. and Morrone, B. and De Laroussilhe, Q. and Gesmundo, A. and Attariyan, M. and Gelly, S.},
  booktitle={Proc. of ICML},
  pages={2790--2799},
  year={2019},
  volume={97},
  url={\url{https://proceedings.mlr.press/v97/houlsby19a.html}}
}

@inproceedings{bapna2019adapters,
    title={Simple, Scalable Adaptation for Neural Machine Translation},
    author={Bapna, A. and Firat, O.},
    booktitle={Proc. of EMNLP-IJCNLP},
    year={2019},
    url={\url{https://doi.org/10.18653/v1/D19-1165}},
    pages={1538--1548},
}

@article{crosslingual2022asradapters,
  author={Hou, W. and Zhu, H. and Wang, Y. and Wang, J. and Qin, T. and Xu, R. and Shinozaki, T.},
  journal={IEEE/ACM TASLP}, 
  title={Exploiting Adapters for Cross-Lingual Low-Resource Speech Recognition}, 
  year={2022},
  volume={30},
  number={},
  pages={317-329},
  doi={10.1109/TASLP.2021.3138674}}

@inproceedings{asr2022adapters,
  author={Thomas, B. and Kessler, S. and Karout, S.},
  booktitle={{ICASSP}}, 
  title={Efficient Adapter Transfer of Self-Supervised Speech Models for Automatic Speech Recognition}, 
  year={2022},
  volume={},
  number={},
  pages={7102-7106},
  doi={10.1109/ICASSP43922.2022.9746223}}

@article{ji2021distill,
  author={Yoon, Ji Won and Lee, Hyeonseung and Kim, Hyung Yong and Cho, Won Ik and Kim, Nam Soo},
  journal={IEEE/ACM Transactions on Audio, Speech, and Language Processing}, 
  title={TutorNet: Towards Flexible Knowledge Distillation for End-to-End Speech Recognition}, 
  year={2021},
  volume={29},
  number={},
  pages={1626-1638},
  doi={10.1109/TASLP.2021.3071662}}


%%%%%%%%%%%%%%%%
%% Zipf's Law %%
%%%%%%%%%%%%%%%%
@misc{zipf1936law,
  title={The Psychobiology of Language},
  author={Zipf, G. K.},
  year={1936},
  publisher={Houghton, Mifflin},
  url={\url{https://psycnet.apa.org/record/1935-04756-000}}
}

@misc{zipf1949human,
  title={Human Behavior and the Principle of Least Effort},
  author={Zipf, G. K.},
  year={1949},
  publisher={Addison-Wesley Press},
  url={\url{https://psycnet.apa.org/record/1950-00412-000}}
}

@article{piantadosi2014zipf,
  title={Zipf’s word frequency law in natural language: A critical review and future directions},
  author={Piantadosi, S. T.},
  journal={Psychonomic bulletin \& review},
  volume={21},
  pages={1112--1130},
  year={2014},
  publisher={Springer},
  doi={10.3758/s13423-014-0585-6},
}

@inproceedings{manaris2006esperanto,
author = {Manaris, B. and Pellicoro, L. and Pothering, G. and Hodges, H.},
title = {Investigating Esperanto's Statistical Proportions Relative to Other Languages Using Neural Networks and Zipf's Law},
year = {2006},
isbn = {0889865566},
publisher = {ACTA Press},
address = {USA},
booktitle = {Proceedings of the 24th IASTED International Conference on Artificial Intelligence and Applications},
pages = {102–108},
numpages = {7},
location = {Innsbruck, Austria},
series = {AIA'06},
url={\url{https://doi.org/10.5555/1166890.1166908}},
}

@book{feng2023formal,
  title={Formal Analysis for Natural Language Processing: A Handbook},
  author={Feng, Zhiwei},
  year={2023},
  publisher={Springer Nature},
  url={\url{https://doi.org/10.1007/978-981-16-5172-4}},
}

@inproceedings{acosta2024annotheia,
  author={Acosta-Triana, J.-M. and Gimeno-Gómez, D. and Martínez-Hinarejos, C.-D},
  title={{AnnoTheia: A Semi-Automatic Annotation Toolkit for Audio-Visual Speech Technologies}},
  booktitle={{Proc. of LREC-COLING}},
  pages={1260--1269},
  year={2024},
}

@inproceedings{chung2017lip,
  title={Lip reading in the wild},
  author={Chung, J. and Zisserman, A.},
  booktitle={13th Asian Conference on Computer Vision},
  pages={87--103},
  year={2017},
  organization={Springer}
}

@inproceedings{lrw2019chinese,
  author={Yang, S. and Zhang, Y. and Feng, D. and Yang, M. and Wang, C. and Xiao, J. and Long, K. and Shan, S. and Chen, X.},
  booktitle={14th IEEE International Conference on Automatic Face \& Gesture Recognition}, 
  title={{LRW-1000: A Naturally-Distributed Large-Scale Benchmark for Lip Reading in the Wild}}, 
  year={2019},
  volume={},
  number={},
  pages={1-8},
  doi={10.1109/FG.2019.8756582}}

@article{egorov2021lrwr,
  title={{LRWR: large-scale benchmark for lip reading in Russian language}},
  author={Egorov, E. and Kostyumov, V. and Konyk, M. and Kolesnikov, S.},
  journal={arXiv preprint arXiv:2109.06692},
  year={2021}
}

@article{harte2015tcd,
  title={{TCD-TIMIT: An audio-visual corpus of continuous speech}},
  author={Harte, N. and Gillen, E.},
  journal={IEEE Transactions on Multimedia},
  volume={17},
  number={5},
  pages={603--615},
  year={2015},
  publisher={IEEE},
  doi={10.1109/TMM.2015.2407694},
}