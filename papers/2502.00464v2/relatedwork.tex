\section{Related Work}
\label{sec:related}

This section presents a brief description focused on the current state of the art in VSR, as well as an overview of how the Spanish language has been addressed in the field.

\noindent\textbf{Current State of the Art.} \cite{shi2022learning} introduced AV-HuBERT, a cross-modal encoder trained in a self-supervised manner using both the acoustic and visual cues. Then, once robust visual speech representations were obtained, an end-to-end VSR system was estimated after assembling a Transformer-based decoder. \cite{prajwal2021sub} not only defined an attention module especially aimed at extracting representative visual features, but also explored a subword-level recognition, arguing that it might be useful to better model visual ambiguities. \cite{ma2022visual} showed that, apart from the importance of designing an appropriate architecture through the use of Conformer encoders \citep{gulati20_interspeech} and hybrid CTC/Attention decoders \citep{watanabe2017ctcattention}, incorporating auxiliary tasks, like using enriched acoustic representations to guide the visual feature encoding, might lead to further advances in the field. Details about this architecture can be found in Section \ref{sec:vsr}. In general terms, all these works reached performances around 25-30\% WER for the English corpora LRS2-BBC \citep{son2017lrs2}, and LRS3-TED \citep{afouras2018lrs3}. Notably, various studies \citep{ma2023auto,liu2023synthvsr} have recently significantly surpassed this performance by designing methods that rely not only on models comprising vast amounts of parameters, but also on additional large-scale datasets, including synthetic video data, for their pre-training. Therefore, the current performances around 15-20\% WER are not directly comparable to our case study, which focuses on conditions with limited resources.

\noindent\textbf{Spanish Visual Speech Recognition.} \cite{fernandez2017towards} presented the VLRF corpus, whose primary purpose was assessing the VSR task's feasibility. In further research \citep{adriana2022alr}, the authors designed an end-to-end architecture, reporting results of around 72\% WER. In one of our prior works \citep{gimeno2024continuous}, we proposed a method to improve the performance of traditional HMM-based systems for VSR, achieving performances of approximately 60\% WER. Additionally, we presented the challenging LIP-RTVE database \citep{lrec2022liprtve} and proposed a traditional approach as a baseline. However, while the speaker-dependent provided around 80\% WER, acceptable results were not reached for the speaker-independent scenario, with roughly 95\% WER. Recently, multiple languages, including Spanish, were considered in \citep{ma2022visual}, achieving 56.6\% and 44.6\% WER for the Spanish partition of the MuAViC \citep{salesky21_interspeech,anwar23muavic}, and the CMU-MOSEAS \citep{zadeh2020moseas} corpora, respectively. Similarly to English, more recent pre-trained, large-scales models \citep{yeo2024limited} have been explored, surpassing the state of the art in MuAVic with results around 46\% WER. Details on all these Spanish databases considered in our proposed lipreading benchmark are described in Section \ref{sec:databases}.