\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
% \usepackage{xcolor}
% \usepackage[table]{xcolor}
\usepackage{booktabs} 
\usepackage[a4paper, total={184mm,239mm}]{geometry}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}


% \title{Architectural exploration of standard interconnects for novel accelerators in Transformer applications
% % \thanks{Identify applicable funding agency here. If none, delete this.}
% }
\title{\fontsize{23.96pt}{\baselineskip}\selectfont Gem5-AcceSys: Enabling System-Level Exploration of Standard Interconnects for Novel Accelerators\\}
%[-0.8em]}
% \thanks{ This work was supported in part by the Swiss State Secretariat for Education, Research, and Innovation (SERI) through the SwissChips research project. This work is partially supported by Intel as part of the Intel Center forHeterogeneous Integrated Platforms (HIP)}}
% Architectural exploration of standard interconnects for novel accelerators in Transformer applications
% Gem5-AcceSys: Enabling System-Level Exploration of Standard Interconnects for Novel Accelerators

% \author{\IEEEauthorblockN{Qunyou Liu}
% \IEEEauthorblockA{\textit{EPFL} \\
% % \textit{Embedded Systems Laboratory (ESL)}\\
% Lausanne, Switzerland \\
% qunyou.liu@epfl.ch}
% \and
% \IEEEauthorblockN{Marina Zapater}
% \IEEEauthorblockA{\textit{HES-SO} \\
% % \textit{Institute of Reconfigurable \& Embedded Digital Systems (REDS)}\\
% Yverdon-les-Bains, Switzerland \\
% marina.zapater@heig-vd.ch}
% \and
% \IEEEauthorblockN{David Atienza}
% \IEEEauthorblockA{\textit{EPFL} \\
% % \textit{Embedded Systems Laboratory (ESL)}\\
% Lausanne, Switzerland \\
% david.atienza@epfl.ch}
% }
%\vspace{-0.1cm}
%\author{\IEEEauthorblockN{Qunyou Liu}
%\IEEEauthorblockA{\textit{École Polytechnique Fédérale de Lausanne (EPFL)} \\
%\textit{Embedded Systems Laboratory (ESL)}\\
%Lausanne, Switzerland \\
%qunyou.liu@epfl.ch}
%\and
%\IEEEauthorblockN{Marina Zapater}
%\IEEEauthorblockA{\textit{HES-SO University of Applied Sciences and Arts Western Switzerland} \\
%\textit{Institute of Reconfigurable \& Embedded Digital Systems (REDS)}\\
%Yverdon-les-Bains, Switzerland \\
%marina.zapater@heig-vd.ch}
%\and
%\IEEEauthorblockN{David Atienza}
%\IEEEauthorblockA{\textit{École Polytechnique Fédérale de Lausanne (EPFL)} \\
%\textit{Embedded Systems Laboratory (ESL)}\\
%Lausanne, Switzerland \\
%david.atienza@epfl.ch}
%}

\author{
	\IEEEauthorblockN{
	Qunyou Liu\IEEEauthorrefmark{1},
    Marina Zapater\IEEEauthorrefmark{2},
    David Atienza\IEEEauthorrefmark{1},  \\
	}			  
    \IEEEauthorblockA{\IEEEauthorrefmark{1}\textit{Embedded Systems Laboratory (ESL), EPFL, Switzerland, }\textit{qunyou.liu@epfl.ch, david.atienza@epfl.ch}}    
    \IEEEauthorblockA{\IEEEauthorrefmark{2}\textit{University of Applied Sciences and Arts Western Switzerland (HES-SO), Switzerland, }\textit{marina.zapater@heig-vd.ch}}
} 
\IEEEaftertitletext{\vspace{-1.0\baselineskip}} 

% \IEEEaftertitletext{\vspace{-3\baselineskip}} 
% \vspace{-0.2cm}
\maketitle
% \vspace{-0.25cm}
\begin{abstract}
% [Verison1]As demand for efficient and high-performance processing grows due to emerging workloads in Machine Learning (ML) and image processing, new hardware accelerators such as GPUs, ISPs, and Data Streaming Accelerators (DSAs) are increasingly vital. These accelerators improve ML and image processing tasks by offloading the computation kernel from the CPU to specific hardware. However, with the proliferation of these heterogeneous hardware solutions, the need for effective simulation tools becomes critical. Current open-source simulators often fall short in their ability to adequately support the design and performance evaluation of these accelerators.
% To address this gap, we introduce Gem5-ESL, a system-accelerator co-design framework that integrates practical interfaces such as PCIe, DMA, RTL, SMMU, and device-side memory and cache. This framework is designed to enhance full-system simulation capabilities and facilitate precise performance evaluation of hardware accelerators. To validate our framework, we developed a transformer-based accelerator optimized for General Matrix Multiply (GEMM) operations and implemented a kernel driver to support full-system simulation. We then assessed the performance of our accelerator in both heterogeneous and homogeneous systems across various memory hierarchies, showcasing the versatility and efficiency of Gem5-ESL.

% As the demand for efficient, high-performance processing grows due to emerging workloads in the machine learning (ML) and image processing domain, hardware accelerators such as GPUs, or Data Streaming Accelerators (DSAs), are becoming increasingly essential. These accelerators enhance ML and image processing tasks by offloading computation from the CPU to dedicated hardware. Interconnects play a crucial role in data transfer between accelerators and memory, and are therefore becoming increasingly important in system-level architectural design.
% To attain the highest system-level performance, there is a need to assess the impact of standard interconnects, such as PCIe, alongside various interconnects and memory hierarchy configurations. In this paper, we tackle this challenge by assessing the performance of a state-of-the-art matrix multiplication accelerator under different interconnects, cache setups, and memory configurations. Using transformers as our main workload, we examine the performance of PCIe under several memory types (including DDR4, DDR5, GDDR6 and HBM2). We also compare the performance of systems using interconnects to host-side memory with those employing accelerator-specific device-side memory. Our results demonstrate the critical role that interconnects and memory hierarchy play in overall system performance and show how systems with optimized interconnects can achieve up to 80\% of the performance of systems with device-side memory, and even outperform them in certain scenarios.
The growing demand for efficient, high-performance processing in machine learning (ML) and image processing has made hardware accelerators, such as GPUs and Data Streaming Accelerators (DSAs), increasingly essential. These accelerators enhance ML and image processing tasks by offloading computation from the CPU to dedicated hardware. These accelerators rely on interconnects for efficient data transfer, making interconnect design crucial for system-level performance. This paper introduces Gem5-AcceSys, an innovative framework for system-level exploration of standard interconnects and configurable memory hierarchies. Using a matrix multiplication accelerator tailored for transformer workloads as a case study, we evaluate PCIe performance across diverse memory types (DDR4, DDR5, GDDR6, HBM2) and configurations, including host-side and device-side memory. Our findings demonstrate that optimized interconnects can achieve up to 80\% of device-side memory performance and, in some scenarios, even surpass it. These results offer actionable insights for system architects, enabling a balanced approach to performance and cost in next-generation accelerator design.




\end{abstract}

\begin{IEEEkeywords}
Memory Hierarchy, PCIe, Interconnects, Hardware Accelerators, System-Level Simulation
\end{IEEEkeywords}
\vspace{-0.25cm}
\section{Introduction}
\vspace{-0.15cm}
Transformer models, such as Vision Transformer (ViT)~\cite{Dosovitskiy2021}, have revolutionized machine learning (ML) and natural language processing (NLP), setting new benchmarks in various tasks~\cite{Devlin2019}. Their growing adoption in real-time applications demands efficient and scalable hardware architectures to meet rising computational needs. Systolic arrays have shown promise in accelerating these tasks, offering parallel data flow and high computational throughput ideal for matrix multiplication, central to both traditional ML and transformer-based models, such as Google's Tensor Processing Unit~\cite{Jouppi2017} and TiC-SAT~\cite{Amirshahi2023}. However, most studies on systolic-array-based architectures primarily focus on hardware-level evaluations, often neglecting the broader system-level context. This leaves a gap in understanding how the interconnects and memory hierarchy, which are vital components in moving data between processors and memory, impact the overall system performance. While some system-level evaluations exist, they often lack the use of standard interconnects such as PCIe, and heterogeneous memory hierarchy such as non-uniform memory access (NUMA), reducing their practicality for real-world applications.

% To address this, we aim to study the performance of a systolic-array-based accelerator designed to accelerate the ViT transformer from a system-level perspective, with a focus on the impact of interconnects and memory hierarchy. In our research, we evaluate the performance using a standard interconnect, PCIe, and investigate how different memory types, such as DDR3/4/5, GDDR6, and HBM2, influence the system’s efficiency. By focusing on system-level architecture, we aim to provide a more practical and comprehensive understanding of how interconnects and memory hierarchy affect the performance of accelerators in real-world scenarios.
To bridge this gap, we introduce Gem5-AcceSys, a novel framework enabling system-level exploration of interconnects and memory hierarchies in accelerators. Using a matrix multiplication accelerator tailored for transformer workloads as a case study, our design framework incorporates essential components such as PCIe~\cite{Ajanovic2009},~\cite{Vasa2020}, NUMA architectures, and configurable memory hierarchies. This integration provides a practical platform for evaluating real-world performance under various configurations. Specifically, we assess the impact of standard interconnects (PCIe) and diverse memory types, including DDR3/4/5, GDDR6, and HBM2, on system efficiency. By emphasizing system-level architecture, our work provides a comprehensive understanding of how interconnects and memory hierarchies influence transformer's performance and how general matrix multiplication (GEMM) and Non-GEMM impact the system performance with the overhead introduced by standard interconnects. This research offers actionable insights for designing scalable and cost-effective accelerator systems tailored to modern ML workloads. More specifically, our contributions are as follows:
\begin{enumerate}
    \item \textbf{Framework for Interconnect Exploration}: We present the framework, Gem5-AcceSys, to support PCIe interconnects, NUMA architecture, and processing near memory based on the Gem5 simulator, enabling system-level co-design with features like device-side memory, local buffer, SMMU, DMA, and RTL-based accelerators.
    
    \item \textbf{PCIe Bandwidth impact on Performance Study}: We investigate the impact of varying PCIe bandwidth by adjusting the number of lanes and lane speeds for the target GEMM workload. Additionally, we vary the packet request size from the perspective of the accelerator to identify the optimal packet size for the design.
    
    \item \textbf{Memory Access and Address Translation Study}: We analyze the impact of memory type and location on system performance from a system-level perspective. Additionally, we investigate address translation overhead for Transformer workloads, demonstrating the effects of virtual address spaces on accelerator efficiency.

    \item \textbf{Impact of Non-GEMM and GEMM Workload Analysis}: We conduct a detailed runtime analysis of Transformer workloads, dividing them into two components: GEMM and Non-GEMM. To optimize performance, we profiled these workloads separately and identified thresholds to determine when device-side memory should be utilized.
\end{enumerate}


% The rapid evolution of computational workloads, driven by advancements in fields like machine learning and image processing, necessitates the development of specialized hardware accelerators. These accelerators, ranging from commercially available GPUs \cite{gpu} and Image Signal Processors (ISPs) \cite{isp} to academic innovations like TiC-SAT \cite{tic-sat} and Eyeriss \cite{eyeriss}, are designed to handle specific computational tasks with greater efficiency than general-purpose processors. As the complexity and diversity of these tasks grow, the challenge of designing, integrating, and optimizing these accelerators intensifies.

% %To address this challenge, several open-source simulators have been developed to enhance accelerator design. Among these, \textbf{Gem5-Aladdin} \cite{gem5-aladdin} provides a co-design framework for simultaneous development of accelerators and system architectures. \textbf{Gem5-Salam} \cite{gem5-salam} incorporates LLVM methodologies for concurrent simulation and performance estimation. \textbf{Gem5-RTL} \cite{gem5-rtl} integrates Register Transfer Level (RTL) designs with Gem5 for precise assessments of power and area. \textbf{Gem5-X} \cite{gem5-x} supports simulations of many-core heterogeneous systems, accommodating more than 256 cores and incorporating novel memory technologies.

% Despite these advancements, significant limitations remain, particularly in supporting practical, real-world system integration. Existing simulators often rely on simplistic interconnection models and face issues with compilation and simulation times, which hinder rapid prototyping and fail to capture full-system dynamics.

% To address these deficiencies, we introduce \textbf{Gem5-ESL}, a new framework designed to advance system-accelerator co-design simulation. Our contributions are fourfold:
% \begin{itemize}
%     \item \textbf{Practical Interface for Accelerator Simulation via PCIe}: Enhancing realism and applicability for external accelerators.
%     \item \textbf{Support for Complex Memory Hierarchies}: Integrating both device-side memory and cache hierarchies to better manage data-intensive demands.
%     \item \textbf{Innovative Accelerator Wrapper Design}: Facilitating seamless integration of RTL code, Direct Memory Access (DMA), and local memory buffers using multi-threading.
%     \item \textbf{Enhanced Full-System Simulation Capabilities}: Supporting virtual memory managed specifically for accelerators through a sophisticated driver design framework and the System Memory Management Unit (SMMU).
% \end{itemize}

% We validate our framework through the development and testing of a \textbf{Transformer-Based Accelerator}, designed to optimize General Matrix Multiply (GEMM) operations. The effectiveness and benefits of Gem5-ESL are demonstrate, and the entire framework is made available to the community via [githubLink], fostering further research and development in hardware accelerators.

%The remainder of this paper is organized as follows: Section 2 discusses related work and the limitations of existing simulators. Section 3 details the architecture and implementation of Gem5-ESL. Section 4 presents our validation results, and Section 5 concludes with future directions and potential applications.


% \section{Related Work}

% % \subsection{State-of-the-art Simulators}
% % Several open-source simulators have been introduced to enhance the design and integration of specialized hardware accelerators:

% % \textbf{Gem5-Aladdin} integrates the Aladdin accelerator modeling framework with the Gem5 system simulator. This combination provides a comprehensive tool for co-designing and simulating both accelerators and system architectures, making it particularly powerful for early-stage design space exploration. The simulator supports pre-RTL simulations, enabling designers to evaluate accelerator performance and its interaction with the system before committing to detailed RTL designs \cite{gem5-aladdin}.

% % \textbf{Gem5-Salam} leverages the LLVM compiler infrastructure to offer a system architecture for modeling and simulating custom hardware accelerators. By incorporating LLVM-based methodologies, Gem5-Salam can simultaneously simulate and estimate the performance and area of accelerator designs. This approach facilitates a deeper understanding of how various accelerator architectures impact overall system performance \cite{gem5-salam}.

% % \textbf{Gem5-RTL} extends the capabilities of the Gem5 simulator by directly integrating RTL designs, allowing for detailed performance and area assessments. This framework supports the evaluation of accelerators' impact on system resources through precise power and area measurements, which are critical for making informed design decisions. However, translating RTL to C++ and executing it within the main simulation thread can lead to significant increases in both compilation and simulation times, potentially hindering rapid prototyping \cite{gem5-rtl}.

% % \textbf{Gem5-X} addresses scalability challenges by providing a framework capable of simulating many-core heterogeneous systems with more than 256 cores. It incorporates interfaces for advanced memory technologies like scratchpad memory and High Bandwidth Memory (HBM), making it well-suited for exploring the performance implications of integrating various accelerators in large-scale systems \cite{gem5-x}.


% %\subsection{Limitations of Existing Simulators}
% Despite the advancements offered by these simulators, several limitations hinder their effectiveness in practical, real-world system integration:

% \textbf{Simplistic Interconnection Models}: Most existing simulators rely on basic interconnection models such as buses or shared memory. These models do not accurately capture the physical constraints and performance implications of integrating external accelerators, such as GPUs and Application-Specific Integrated Circuits (ASICs). This simplification can lead to unrealistic simulation results, limiting the applicability of these tools for designing complex, high-performance systems \cite{gem5-aladdin}.

% %\textbf{Compilation and Simulation Overhead}: For instance, Gem5-RTL’s method of translating RTL to C++ and executing it within the main simulation thread significantly increases both compilation and simulation times. This overhead hampers rapid prototyping and iterative design processes, which are critical in early-stage development. The inability to capture full-system dynamics, particularly in terms of complex memory hierarchies, further limits its usefulness under heavy computational workloads \cite{gem5-rtl}.

% \textbf{Limited Real-World System Integration}: Current simulators often lack support for practical interfaces and integration methodologies required for real-world system design. For example, the absence of realistic interfaces like PCIe for accelerator simulation limits the ability to accurately model and evaluate the performance and integration of external accelerators. This gap between simulation capabilities and real-world requirements can lead to suboptimal design decisions \cite{gem5-salam}.

% \textbf{Scalability Issues}: While Gem5-X addresses some scalability concerns by supporting many-core systems, the complexity of integrating and managing diverse accelerators in large-scale heterogeneous systems remains a significant challenge. The simulator’s ability to accurately represent the performance and interaction of various accelerators within such systems is crucial but not fully realized in existing tools \cite{gem5-x}.

\vspace{-0.5cm}
\section{State-of-the-art}
\vspace{-0.15cm}
% The goal of our design framework is to explore standard interconnects and memory hierarchies  within the context of accelerators running AI workloads and connected to a CPU system running full-system Linux support. 

% Our framework significantly extends the capabilities of the traditional Gem5 simulator by integrating a comprehensive suite of enhancements designed to facilitate modern system design and simulation, such as PCIe, DMA, SMMU, configurable memory, and submemory system.

% While several open-source simulators like Gem5-Aladdin~\cite{gem5-aladin}, Gem5-Salam~\cite{Rogers2020}, Gem5-RTL~\cite{Lopez2021}, and Gem5-X~\cite{Qureshi2021} have been developed to aid in the design and integration of specialized hardware accelerators, they lack support for crucial functionalities such as standard PCIe interconnects, NUMA architectures, and processing near memory (PNM) capabilities. These limitations motivate our work to extend the Gem5 simulator with these features, enabling comprehensive system-level co-design.
% Our work addresses these limitations by extending the Gem5 simulator to support standard interconnects and memory hierarchies, enabling comprehensive exploration of system-level co-design for AI accelerators. The enhanced framework integrates key features such as PCIe interconnects, Direct Memory Access (DMA), System Management Unit (SMMU), and configurable memory systems, providing robust support for accelerators connected to CPU systems running full-system Linux. These additions allow detailed evaluations of interconnect and memory designs, fostering a deeper understanding of their impact on accelerator performance in real-world scenarios.
Several open-source simulators have been developed to aid in the design and integration of specialized hardware accelerators, notably Gem5-Aladdin~\cite{Shao2016}, Gem5-Salam~\cite{Rogers2020}, Gem5-RTL~\cite{Lopez2021}, and Gem5-X~\cite{Qureshi2021}. These frameworks provide valuable tools for early-stage design exploration and detailed evaluations of accelerators within system simulations. Gem5-Aladdin integrates accelerator modeling with system simulation, enabling pre-RTL design analysis. Gem5-Salam employs LLVM-based methodologies to model and estimate the performance and area of custom accelerators. Gem5-RTL incorporates RTL designs for detailed performance assessments, while Gem5-X supports simulations of many-core heterogeneous systems with advanced memory technologies.
However, these simulators have limitations that hinder their applicability for comprehensive system-level co-design. They rely on simplistic interconnects (e.g., basic buses, shared memory), lacking support for standard interfaces like PCIe. Memory hierarchy support is limited, with features like Non-Uniform Memory Access (NUMA) and processing near memory (PNM) absent~\cite{Khan2024}. High simulation overhead due to RTL integration, as seen in frameworks like Gem5-RTL, impedes rapid prototyping. Scalability issues arise when accurately simulating diverse accelerators within large-scale systems. Additionally, inadequate system integration is evident from the lack of features like Direct Memory Access (DMA) and System Memory Management Unit (SMMU) for efficient data handling~\cite{Whitham2010},~\cite{Paraskevas2020}. These limitations highlight the need for a comprehensive framework that supports standard interconnects, complex memory hierarchies, and realistic full-system interactions. To address this gap, we propose \textbf{Gem5-AcceSys} to enable detailed system-level co-design of accelerators.
\vspace{-0.25cm}
\section{Design framework}
\vspace{-0.25cm}
% \subsection{Overall Architecture}

% Fig.~\ref{fig:designFrame} illustrates the comprehensive architecture of our enhanced design framework. This framework significantly expands the capabilities of the Gem5 simulator by incorporating crucial functionalities that are essential for contemporary system design. Building upon the Gem5 base, this new system introduces integration with the PCIe interface, DMA, a SMMU, a configurable sub-memory architecture, and RTL integration. The system is organized into two primary sections: the CPU cluster with its cache, and the memory system interfaced with the accelerator through PCIe components. On the left side, we see the CPU Cluster, which comprises one or more CPUs and their associated caches. This is connected to the system's main memory (DRAM) through a memory controller. The connection to the memory controller is facilitated by a MemBus, which stands for Memory Bus, indicating the pathway for data transfers between the CPU and the memory.

% The integration of PCIe offers a more realistic interface for peripheral device simulations, moving beyond the conventional virtual latency associated with memory buses. DMA simplifies the functional design for accelerators by enabling direct memory transfers, thereby alleviating the data movement burden from the accelerator. This allows designers to focus more on the core design of the accelerator itself. Moreover, the SMMU facilitates virtual to physical address translation from the accelerator’s perspective, transitioning the working address space from virtual to physical, thereby streamlining driver design.

% The addition of a configurable sub-memory system enables the definition of an optimized memory configuration, which is critical in the co-design of accelerator systems. Additionally, our framework supports detailed hardware simulation, allowing for the assessment of power, performance, and area metrics. We have successfully integrated RTL into the accelerator wrapper to enhance simulation fidelity and analytical depth.

% \begin{figure}[h]
%   \centering
%   \includegraphics[width=\linewidth]{figures/Design_Framework.png}
%   \caption{Design Framework Architecture}
%   \label{fig:designFrame}
% \end{figure}


% \subsection{Interface Integration and SMMU}
% The architectural enhancements begin with a sophisticated integration of the PCIe (Peripheral Component Interconnect Express) interface, a high-speed serial computer expansion bus standard~\cite{Ajanovic2009}~\cite{Vasa2020}. This standard is pivotal in replacing the simpler bus-based data communication model previously utilized in Gem5. Our enhancements include the integration of the root complex, switch, and link components essential for PCIe functionality:

% \begin{itemize}
%     \item \textbf{PCIe RC (Root Complex)}: This component serves as the endpoint on the CPU side, interfacing with PCIe devices. It acts as the primary conduit between the CPU and the PCIe bus, orchestrating the flow of data and commands.
%     \item \textbf{PCIe PHY (Physical Layer)}: This layer encompasses the physical circuitry required to connect the PCIe Root Complex with the devices on the PCIe bus. Within our system, the PCIe PHY links to the PCIe Switch, which is tasked with routing data among various PCIe devices. It also connects to the PCIe Link, the physical pathway that facilitates data transfer between the PCIe Switch and the hardware accelerator.
%     \item \textbf{PCIe Switch}: This component is responsible for managing and routing the data traffic between multiple PCIe devices, ensuring efficient and orderly data transfers. It plays a pivotal role in the scalability of the system by enabling multiple connections and communication pathways within the PCIe network.
% \end{itemize}

% Positioned strategically between the MemBus and the PCIe components, the SMMU plays a critical role in memory protection and address translation, responsible for translating device-visible virtual addresses into physical addresses in system memory~\cite{Whitham2010}~\cite{Paraskevas2020}. It ensures seamless communication between the hardware accelerator (the device) and the CPU's memory space. The inclusion of the SMMU greatly enhances the framework’s ability to manage virtual memory mappings and perform efficient address translations, both of which are crucial for supporting high-performance, data-intensive applications. Furthermore, the integration of the SMMU simplifies the software support required for the accelerator, streamlining development processes and enhancing system reliability.

\subsection{Overall Architecture and Interface Integration}
\vspace{-0.15cm}
Fig.~\ref{fig:designFrame} illustrates our enhanced design framework, with newly added components highlighted in light blue, significantly expanding the Gem5 simulator's capabilities for contemporary system design. This framework integrates essential functionalities, including PCIe interconnects, DMA, SMMU, configurable sub-memory architecture, and RTL-based accelerator support. The system is organized into two primary sections: the CPU cluster with its cache and the accelerator system interfaced with the memory bus (MemBus) through PCIe components.

\begin{figure}[h]
\vspace{-0.2cm}
  \centering
  \includegraphics[width=0.8\linewidth]{figures/Design_Framework.png}
  \vspace{-0.25cm}
  \caption{Design Framework Architecture}
  \label{fig:designFrame}
  \vspace{-0.4cm}
\end{figure}

The CPU cluster, comprising one or more CPUs and associated caches, connects to the main memory (DRAM) via a memory controller. Data transfers between the CPU and memory are facilitated by a MemBus. PCIe integration provides a realistic interface for peripheral device simulations, surpassing the limitations of conventional memory bus latency models. The DMA feature further enhances system efficiency by enabling direct memory transfers, reducing the data movement burden on the accelerator and simplifying its design. The SMMU enables virtual-to-physical address translation, streamlining driver development and improving memory mapping efficiency for accelerators~\cite{Whitham2010},~\cite{Paraskevas2020}. A configurable sub-memory system supports the optimization of memory configurations crucial for co-designing accelerator systems. This includes device-side memory with local buffers and host-side memory configurations to evaluate different system trade-offs. Key PCIe components include:

\begin{itemize}
    \item \textbf{PCIe RC (Root Complex)}: The CPU endpoint that manages data and commands on the PCIe bus.
    \item \textbf{PCIe PHY (Physical Layer)}: Connects the RC to devices, facilitating communication via the PCIe Switch and Link.
    \item \textbf{PCIe Switch}: Routes traffic among PCIe devices, supporting multiple connections and enhancing scalability.
\end{itemize}

By positioning the SMMU between the MemBus and PCIe components, the framework enhances memory protection and address translation capabilities, enabling efficient use of virtual address spaces.
\vspace{-0.25cm}
\subsection{Accelerator Wrapper}
% The Accelerator Wrapper encapsulates the specific logic and interfaces of the hardware accelerator. On the right side, the PCIe interface is depicted, which connects to the controller of the hardware accelerator. This controller is tasked with managing data flow between the accelerator and the rest of the system.

% Adjacent to the controller, the accelerator itself is positioned, which may be implemented as either an RTL-based or a C++-based module. To integrate the RTL code into the framework, we utilize Verilator~\cite{Chi2022} to convert the RTL code into C++ and then compile it as an executable binary file. This executable is run as a child process that interacts with the main process through system calls within a shared memory space.

% The presence of a DMA block within the controller indicates that the accelerator has the capability to directly read from and write to memory without CPU intervention, a feature crucial for high-performance operations.

% Additionally, the Local Mem Buffer likely serves as buffer memory, situated close to the accelerator to provide quick and temporary storage for computations. The Device Memory Controller facilitates the transfer of data between the accelerator domain and device-side memory, further enhancing system efficiency. Furthermore, the Accelerator has interface to configurable memory hierarchy.

The Accelerator Wrapper contains logic and interfaces for the hardware accelerator, including a PCIe interface that connects to the accelerator's controller. This controller manages data flows between the accelerator and system. Adjacent to this, the single accelerator or accelerator cluster, either RTL-based or C++-based, integrates with the system using Verilator~\cite{Chi2022} to convert RTL code to C++, compiling it into an executable that runs as a child process with shared memory system calls. A DMA block within the controller enables direct memory access, bypassing the CPU for higher performance. A Local Mem Buffer provides quick, temporary computation storage, and a Device Memory (DevMem) Controller oversees data transfers between the accelerator and device memory, boosting efficiency. The accelerator also interfaces with a configurable memory hierarchy.

\vspace{-0.25cm}
\subsection{Memory Hierarchy and Subsystem Performance Analysis}
% Our design framework incorporates a configurable memory hierarchy that includes host-side memory, device-side memory, a last-level cache, and a device-side cache. This flexibility supports comprehensive design space exploration from the perspective of the memory system. Our design framework also provides interface to other DRAM models, like DRAMsim3~\cite{Li2020}, Ramulator2~\cite{Luo2023} and Dramsys5~\cite{Steiner2022}. Incorporating other open-source DRAM models into our design, we can get more accurate timing of DRAM access and the power statistics information. The framework provides two distinct memory access methods: direct cache (DC) access and direct memory (DM) access.

% Fig.~\ref{fig:designFrame} illustrates the processes of DC and DM modes. In the DC mode, data requests from the accelerator are initially directed to the cache hierarchy, as indicated by arrow 1. If the requested data is present in the cache (a cache hit), the request is fulfilled at that level, reducing latency. In cases where there is a miss in the last-level cache, the request is escalated to the host-side memory. The data traverses through the MemBus (arrow 2), subsequently reaching the memory controller via arrows 4 and 5, which introduces additional latency. Our framework allows for in-depth analysis of the cache hierarchy's effect on system performance, including factors like cache size and latency. Our design supports modifications to the cache hierarchy, facilitating adjustments to the size and latency of the IOCache and the last-level cache.

% Conversely, the DM method circumvents the cache hierarchy completely. Data requests are sent directly to the main memory, following the path denoted by arrows 3 and 5. This approach minimizes latency by avoiding intermediate cache checks, providing a streamlined access route to the memory. This approach necessitates software-based management of data coherency, given that the cache system is not involved in this memory access path.
Our design framework incorporates a configurable memory hierarchy, including host-side and device-side memories, along with a last-level cache and a device-side cache, supporting comprehensive design space exploration. It interfaces with DRAM models like DRAMsim3~\cite{Li2020}, Ramulator2~\cite{Luo2023}, and Dramsys5~\cite{Steiner2022} for accurate DRAM timing and power statistics. The framework offers three memory access methods: direct cache (DC) access, direct memory (DM) access and DevMem access. Fig.~\ref{fig:designFrame} illustrates DC and DM modes. In DC mode, data requests from the accelerator are directed to the cache hierarchy. Cache hits retrieve data immediately, reducing latency, while cache misses access host-side memory via the MemBus (arrow 2) and memory controller (arrows 4 and 5), adding latency. This mode enables detailed analysis of cache effects on performance and allows adjustments to cache size and latency. To support this, we modified the simulator to implement a cache coherency model between the accelerator's cache and the CPU cache. In DM mode, requests bypass the cache and go straight to main memory via arrows 3 and 5, minimizing latency by eliminating cache checks. This method requires software management of data coherency since it bypasses the cache system. Arrows 6 bypasses the whole PCIe system to access device-side memory avoid the data movement overhead introduce by PCIe system. By leveraging configurable memory hierarchies and flexible access methods, our design framework enables the selection of optimal memory configurations to enhance accelerator performance. Additional insights into the impact of memory hierarchy on performance are discussed in Section~\ref{sec:expeandres}.

% \subsection{Simulation Methodology}
% To integrate the RTL code into the framework, we utilize Verilator~\cite{Chi2022} to compile the RTL code into an executable binary file. Fig.~\ref{fig:Gem5Arch} illustrates the workflow and component interactions involved in integrating RTL (Register Transfer Level) code within the context of hardware accelerator simulation and design. The process is divided into distinct stages, each encapsulating specific tasks within the design and simulation flow.

% \begin{figure}[h]
%   \centering
%   \includegraphics[width=\linewidth]{figures/Gem5Arch.png}
%   \caption{Design Framework Architecture}
%   \label{fig:Gem5Arch}
% \end{figure}

% Starting from the left, the RTL code is compiled by Verilator into a C++ module, and then into an executable file. This executable (referred to as the Child Process) interacts with the main process through system calls, operating within a shared memory space for efficient process communication.

% The core of the integration is the Accelerator Design Framework, which provides a structured approach to incorporate the accelerator model into the system. The PCIe object within this framework represents the PCIe hardware interface, essential for the accelerator's communication with other system components. Positioned beneath the PCIe object is the Kernel Driver, which acts as an interface between the hardware and user-space applications, facilitating the management and execution of hardware operations from the software side. The User Space Program, a software component, interacts with the kernel driver to perform operations on the hardware accelerator, likely through system calls or a designated API provided by the kernel driver.

% Starting with the RTL, which is synthesized by an EDA tool using the desired process technology, we obtain area information. Verilator enables us to generate a waveform from the executable file during the simulation process. Additionally, by using the timing mode in gem5, we can ascertain the execution time of the overall process, a critical performance metric. The combination of area, waveform, and performance metrics facilitates a comprehensive PPA (Power, Performance, and Area) analysis, essential for evaluating the efficiency of the hardware design.

% Accelerator modeling within Gem5-XXX leverages a multi-threading model to distribute tasks across multiple cores. This approach, embodied in the child process method, allows the main Gem5 framework to invoke accelerators as separate child processes, saving significant simulation time and enhancing process communication efficiency through shared memory.

% Furthermore, the framework emphasizes Power, Performance, and Area (PPA) design space exploration. It utilizes Verilator to generate waveforms during simulation, crucial for conducting dynamic power analysis. Gem5’s timing mode is employed to measure execution times, providing a detailed performance analysis that aids in identifying and optimizing potential bottlenecks.
\vspace{-0.25cm}
\subsection{Comparison with Existing Simulators}
To highlight the unique features of Gem5-AcceSys and how it extends beyond existing Gem5-based simulators, we provide a concise comparison in Table~\ref{tab:simulator_comparison}. This table summarizes key features such as accelerator (Acce) design level support, interconnect capabilities, address translation, memory simulation, kernel driver support, DMA capabilities, device-side memory support, simulation scope, and accelerator process models.


\begin{table*}[htbp]
\vspace{-0.5cm}
\centering
\caption{Comparison of Gem5-based frameworks for hardware accelerator simulation}
\label{tab:simulator_comparison}
% \begin{tabular}{lcccc>{\columncolor{red!20}}c}
\begin{tabular}{lccccc}
\toprule
\textbf{Feature} & \textbf{Gem5-Aladdin} & \textbf{Gem5-SALAM} & \textbf{Gem5-RTL} & \textbf{Gem5-X} & \textbf{Gem5-AcceSys} \\
\midrule
\textbf{Acce Design Level} & C++ & LLVM IR & RTL & C++ & C++, RTL \\
\textbf{Interconnect} & Basic buses & Basic buses & Basic buses & Basic buses & Basic buses, PCIe \\
\textbf{Acce Address Translation} & Yes & No & No & No & Yes (SMMU) \\
\textbf{External Memory simulator} & No & No & No & No & Ramulator/DRAMsys \\
\textbf{Kernel Driver Support} & No & No & No & Limited & Yes \\
\textbf{Multi-Channel DMA} & Yes & No & No & No & Yes \\
\textbf{Device-Side Memory} & No & No & No & Yes & Yes \\
\textbf{Full-System Simulation} & Yes & Bare-metal & Yes & Yes & Yes \\
\textbf{Acce Process Model} & Integrated & Integrated & Integrated & Integrated & Child process (Multi-threaded) \\
\bottomrule
\end{tabular}
\end{table*}
As shown in Table~\ref{tab:simulator_comparison}, Gem5-AcceSys provides comprehensive support for features essential in modern accelerator design, addressing limitations present in existing simulators. This includes support for standard interconnects like PCIe, accelerator address translation via SMMU, integration with external memory simulators, kernel driver support, multi-channel DMA, and device-side memory.
\vspace{-0.25cm}
\section{Experimental setup}
\vspace{-0.25cm}
To demonstrate the capabilities of our design framework and analyze the impact of standard interconnects and configurable memory architectures on transformer workloads, we conduct experiments using the DC access method, with configurations and parameters detailed in Table~\ref{tab:system_configuration}.
\vspace{-0.1cm}
\begin{table}[h]
\vspace{-0.5cm}
\caption{System Configuration}
\centering
\begin{tabular}{lc}
\hline
\textbf{Component} & \textbf{Specification}\\
\hline
CPU & ARM, 1 GHz \\
Data Cache & 64 kB\\
Instruction Cache & 32 kB\\
Last Level Cache & 2 MB \\
IOCache & 32 kB \\
Memory & DDR3\_1600\_8x8, 4 GB \\
PCIe Link & Version 2.0, 4 Gb/s, 4 Lanes \\
PCIe RootComplex & 150ns Latency \\
PCIe Switch & 50ns Latency \\
\hline
\end{tabular}
\label{tab:system_configuration}
\vspace{-0.5cm}
\end{table}
\vspace{-0.25cm}
\subsection{Hardware Accelerator Design}

We use a Systolic Array (SA) accelerator named MatrixFlow~\cite{Liu2025}. MatrixFlow is a loosely-coupled SA specifically optimized for transformer models. It uses a new matrix multiplication technique alongside an optimized data structure, which significantly enhances data streaming efficiency and reduces memory overhead. MatrixFlow contains 16x16 multiply–accumulate units, and uses data in integer format as input and output. Through a combination of hardware-software co-design, including the integration of PCIe and DMA for efficient data handling, MatrixFlow achieves remarkable speed-ups of up to 400x compared to a single-threaded CPU system, and demonstrates substantial performance improvements over both loosely- and tightly-coupled accelerators in the state-of-the-art.
\vspace{-0.5cm}
\subsection{Workload}
First, we select GEMM as a workload because it represents a general and widely used computational pattern, showcasing the versatility of our design framework in evaluating performance and analyzing bottlenecks. Next, to further illustrate our framework's capabilities in co-optimizing complex systems, we use the Vision Transformer (ViT)~\cite{Dosovitskiy2021} as a case study. We evaluate workloads from the ViT\_base, ViT\_large, and ViT\_huge models, which have hidden dimensions of 768, 1024, and 1280, respectively, and utilize 12 or 16 attention heads.
% For BERT Large, a model with 1024 hidden dimensions and 16 attention heads, we set the input length to 256 and use int32 data type. These workloads highlight how our framework assists designers in system-level optimization for AI accelerators.
\vspace{-0.25cm}
\section{Performance Evaluation}
\vspace{-0.25cm}
\label{sec:expeandres}

To evaluate the performance of our design framework, we focus first on GEMMs and analyze  its performance under various configurations. Based on the insights gained from this analysis, we narrow-down the set of interesting configurations and focus on the entire transformer.
\vspace{-0.25cm}
\subsection{Performance Bounds and Roofline Model}
We begin by analyzing the roofline model of our accelerator system, focusing on GEMMs with dimensions 1024. To isolate the effects of computation on performance, we fix the PCIe bandwidth at 8 GB/s and vary the computation time of the systolic array within the Gem5 framework.

\begin{figure}[h]
\vspace{-0.5cm}
  \centering
  \includegraphics[width=\linewidth]{figures/PCIe_MemBound_study_normalized_with_vertical_line.png}
  \vspace{-0.8cm}
  \caption{Roofline Model of the Accelerator System}
  \label{fig:roofline}
  \vspace{-0.35cm}
\end{figure}

Fig.~\ref{fig:roofline} illustrates the roofline model of the proposed accelerator. The x-axis represents computation time (ns), while the y-axis shows the normalized execution time. For computation times exceeding 1500 ns, the system operates in the \textbf{memory-bound} region, where performance improves linearly as computation time decreases. This indicates that performance is limited by memory bandwidth or PCIe data transfer speed.
As computation time drops below 1500 ns, the system transitions to the \textbf{computation-bound} region, as shown by the plateau in performance. Here, further improvements in memory bandwidth have no significant effect because the computation speed of the systolic array becomes the bottleneck. This transition, marked by the red line, highlights the shift in limiting factors. To better understand these performance bounds, we proceed with a detailed analysis of GEMM workloads.
\vspace{-0.25cm}
\subsection{GEMMs}
\label{sec:GEMMres}
We start by analyzing the impact of the following factors:
\begin{itemize}
    \item \textbf{PCIe Link:} Analyzing how PCIe bandwidth, latency , and packet sizes affect the data transfer efficiency and overall computation speed.
    \item \textbf{Memory Type, location, latency and bandwidth:} Assessing performance variations between device-side and host-side memory, under different memory technologies including  DDR4, LPDDR, GDDR, and HBM.
    \item \textbf{Address Translation:} Analysing the overhead of Address Translation wrt matrix size and their effects on computational delays and resource utilization.
\end{itemize}


\subsubsection{PCIe Link Performance Analysis}

\paragraph{Bandwidth Sweeping}
To evaluate the impact of PCIe bandwidth on GEMM performance, we vary the number of lanes (2, 4, 8, 16) and their speeds (2 Gbps to 64 Gbps), using 2048x2048 matrices on the systolic array accelerator.  As depicted in Fig.~\ref{fig:GEMM_bandwidth}, increasing bandwidth consistently reduces execution time, with performance scaling until the system transitions from memory-bound to compute-bound at 16 lanes. Notably, bandwidth impacts performance significantly, with the highest bandwidth outperforming the lowest by up to 1109.9\%. This highlights the effectiveness of our framework in identifying optimal bandwidth configurations for balanced performance and resource efficiency.

%Each increase in bandwidth contributes to a reduction in execution time, suggesting that system performance can be substantially enhanced by optimizing bandwidth. 

%These findings underscore the pivotal role of bandwidth in system design, particularly for tasks involving large-scale matrix computations. The study provides a quantitative basis for selecting PCIe configurations that optimize both performance and cost-effectiveness, making it a critical consideration for hardware system planners aiming to maximize computational throughput.
\begin{figure}[h]
\vspace{-0.25cm}
  \centering
  \includegraphics[width=\linewidth]{figures/GEMM_bandwidth_normalized_inverted_style.png}
  \vspace{-0.8cm}
  \caption{Performance (Execution time) for Matrix Size 2048 under varying per-lane bandwidth and number of lanes}
  \vspace{-0.25cm}
  \label{fig:GEMM_bandwidth}
\end{figure}

\textbf{\textit{Key Takeaway \#1:} PCIe bandwidth significantly impacts accelerator performance, particularly in memory-bound regions. However, as systems become compute-bound, the benefits of additional bandwidth diminish.}


\paragraph{Packet Size Sweeping}
We configure the PCIe Link at 4GB/s, 8GB/s, 16GB/s, 32GB/s, and 64GB/s, analyzing the impact of request packet sizes from accelerator ranging from 64 bytes to 4096 bytes. As shown in Fig.~\ref{fig:GEMM_packet}, varying packet sizes result in differing processing times, even reach 36\%. Initially, execution time decreases as packet size increases from 64 bytes, reaching a minimum around 256 bytes, reflecting efficiency gains at moderate sizes. Beyond this point, execution time increases with larger packet sizes up to 4096 bytes, forming a convex curve that highlights a non-linear relationship: both very small and very large packet sizes are less efficient. This behavior stems from the PCIe hierarchy, where packets must pass through the RootComplex and Switch. Larger packets disrupt the pipeline, causing stalls at each component before data reaches the accelerator.

% \vspace{-0.5cm}
\begin{figure}[h]
  \centering
  \includegraphics[width=\linewidth]{figures/Performance_at_4GBps2.png}
  \vspace{-0.8cm}
  \caption{Execution Time under different packet sizes for different PCIe bandwidth.}
  \vspace{-0.25cm}
  \label{fig:GEMM_packet}
  \vspace{-0.25cm}
\end{figure}

\textbf{\textit{Key Takeaway \#2:} Packet size significantly affects execution time with a convex trend: 64-byte packets incur 12\% overhead, and 4096-byte packets 36\%, relative to the optimal 256-byte size.}

% \subsubsection{Cache/IOCache performance analysis}
% %Cache as an intermediate transparent buffer between memory and computation core plays an important role in modern memorg system. But should we implement the cache in the side of accelerator, or should we let the access the cache before accessing the memory is a crucial problem. 
% %To study the impact of direct cache access and the impact of IOCache. We conduct experiment at different scenario, for example, with different PCIe bandwidth configuration.

% \paragraph{PCIe bandwidth Sweeping}
% To study the impact of DC access and the impact of IOCache we conducted experiments with varying PCIe bandwidths. Initially, we set  PCIe bandwidth to 2 GBps (4 4Gbps lanes), 8 GBps (4 16Gbps lanes), and 64 GBps (16 64Gbps lanes). At each bandwidth setting, we run GEMMs using direct cache access with IOCache support, configured with a latency of 16 cycles, as well as DM access.
% Figures~\ref{fig:DC_DM_PCIe_multiple} (a), (b), and (c) illustrates the normalized time spent by DC access relative to DM access across different matrix sizes under PCIe bandwidth configurations.

% \begin{figure}[h]
%   \centering
%   \includegraphics[width=\linewidth]{figures/DC_DM_PCIe_normalized_style.png}
%   \caption{Normalized Time Spent by DM Access Relative to DC Access under Different PCIe Bandwidths}
%   \label{fig:DC_DM_PCIe_multiple}
% \end{figure}

% In Figures~\ref{fig:DC_DM_PCIe_multiple} (a), corresponding to a PCIe bandwidth of 2 GBps, the normalized time spent by DC relative to DM remains close to 1 across various matrix sizes, indicating comparable performance between DC and DM for smaller bandwidths. Notably, the normalized DC time slightly increases as the matrix size grows, surpassing the DM time for larger matrices.

% Figures~\ref{fig:DC_DM_PCIe_multiple} (b) depicts the scenario for a PCIe bandwidth of 8 GBps. Similar to the previous configuration, the normalized DM time stays relatively stable near 1, while the normalized DC time shows a slight increase with matrix size. The increase in DC time is less pronounced compared to the 2 GBps bandwidth, suggesting that higher bandwidth reduces the discrepancy between DM and DC performance.

% In Figures~\ref{fig:DC_DM_PCIe_multiple} (c), with a PCIe bandwidth of 64 GBps, a different trend emerges. The normalized DC time consistently outperforms the DM time, particularly as the matrix size decrease. This indicates that at higher bandwidths, DC access becomes significantly more efficient than DM access, especially for smaller matrices.

% Overall, we see how PCIe bandwidth affects the relative performance of DM and DC access. At lower bandwidths, the performance of DM and DC is comparable, but as bandwidth increases, DM access becomes increasingly advantageous, particularly for larger workloads.

% \textbf{Key takeaway \#3: DC Access has a limited impact on system performance, varying with PCIe bandwidth and matrix size. At low PCIe bandwidths, its effect is minimal, with a slight performance gain (0.8\%) for small matrices and a decline (0.4\%) as matrix size increases. At high PCIe bandwidths, DC Access outperforms DM Access by 20\%, but this advantage diminishes as the matrix size reaches 2048, aligning with DM Access performance.}

% \paragraph{IOCache Size and Latency Sweeping}
% To examine the impact of the IOCache, we  focus on two main factors: size and latency. We vary IOCache size while keeping latency fixed at 16 cycles, and conversely, we vary latency for a fixed IOCache size of 32 kB.
% \begin{figure}[h]
%   \centering
%   \includegraphics[width=\linewidth]{figures/IOCacheSizeLatency_Study.png}
%   \caption{Impact of IOCache Latency(a) and Size(b) on Execution time}
%   \label{fig:IOCacheSizeLatency_Study}
% \end{figure}

% Fig.~\ref{fig:IOCacheSizeLatency_Study} (a) and (b) illustrate the results of these experiments.  Figures~\ref{fig:IOCacheSizeLatency_Study} (a) shows a positive linear trend between IOCache latency and execution time. Higher latencies in the IOCache can significantly degrades system performance up to -5\%.

% Fig.~\ref{fig:IOCacheSizeLatency_Study} (b) shows that as the IOCache size increases from 0 to approximately 2 MB, there is a notable decrease in execution time. Beyond 2 MB, further increases in IOCache size result in minimal performance gains due to the limitation of the next level cache. However, overall, the performance improvement attributable to IOCache is limited to only 0.5\%.

% %These findings highlight the constrained impact of both IOCache latency and size on overall system performance. The performance gains derived from adjustments to IOCache size and latency are ultimately very limited.

% \textbf{Key takeaway \#4: IOCache has a minimal impact on DC Access performance. Due to IOCache latency, performance degrades slightly, with only a 0.8\% improvement observed even as the IOCache size increases.}

\subsubsection{Memory performance analysis}
To examine the performance impact of different memory types and locations, we use ramulator2 as our backend DRAM model, and test DDR3, DDR4, HBM, and GDDR5 with the bandwidth and data rate configurations detailed in Table~\ref{tab:mem_config}.

\begin{table}[h]
\vspace{-0.5cm}
\caption{Memory Configuration}
\centering
\begin{tabular}{lcccc}
\hline
\textbf{Component} & \textbf{Channel} & \textbf{Data width} & \textbf{Bandwidth} & \textbf{Data Rate} \\
\hline
DDR3 &  1 &  64 & 12.8 GB/s & 1600 MT/s \\
DDR4 &  1 &  64 & 19.2 GB/s & 2400 MT/s \\
DDR5 &  2 &  32 & 25.6 GB/s & 3200 MT/s \\
HBM2 & 2 &  128 & 64 GB/s & 2000 MT/s \\
GDDR6 & 2 &  64 & 32 GB/s & 2000 MT/s \\
\hline
\end{tabular}
\label{tab:mem_config}
\end{table}
\vspace{-0.5cm}

\paragraph{Device-side vs Host-side Memory and Memory Type}

\begin{figure}[h]
  \centering
  \includegraphics[width=0.9\linewidth]{figures/Performance_MemType_Speedup2.png}
  \vspace{-0.5cm}
  \caption{Impact of DRAM type and location}
  \vspace{-0.25cm}
  \label{fig:device_vs_host_memory}
\end{figure}

Fig.~\ref{fig:device_vs_host_memory} presents the normalized speedup (wrt DDR4 device-side data) comparison between device-side memory and two host-side memory configurations (one with a 2GB/s and one with a 64GB/s PCIe link) for DDR4, HBM, GDDR5, and LPDDR5. 

The results demonstrate that device-side memory consistently outperforms host-side memory across all tested memory types, regardless of the speed of the PCIe bus. Host-side memory shows lower speedups, with a clear dependence on PCIe speed. When using the 64GB/s PCIe configuration, host-side memory can achieve around 78\% of the performance relative to device-side memory, which is a substantial improvement over the 2 GB/s PCIe. Performance degradation depends on the memory technology, and is aggravated for GDDR5 and HBM, as the gap between host-side and device-side memory grows larger.

%This indicates that PCIe bandwidth has a significant impact on the performance of host-side memory, with higher bandwidth reducing the performance gap between host-side and device-side memory. However, despite this improvement, device-side memory still maintains a notable advantage across all memory types tested.

\textbf{\textit{Key Takeaway \#3:} Implementing device-side memory significantly boosts performance, improving accelerator efficiency by up to 2 times compared to all other configurations.}

%\textbf{Key takeaway 6: Memory type impacts performance differently depending on the scenario. Host-side memory shows minimal performance variation. However, device-side memory significantly influences overall performance. With a high-performance PCIe link, host-side memory performance improves and can reach up to 80\% of that of device-side memory.}

\paragraph{Memory bandwidth and latency sweeping}
We investigate the impact of memory bandwidth and latency by varying one while keeping the other one constant, using HBM as a case study with gem5's default DRAM model.

\vspace{-0.25cm}
\begin{figure}[h]
\vspace{-0.25cm}
  \centering
  \includegraphics[width=\linewidth]{figures/Performance_Lat_bandwidth.png}
  \vspace{-0.6cm}
  \caption{Impact of Memory Bandwidth(a) and Memory Latency(b)}
  \label{fig:Performance_Bandwidth}
\end{figure}
\vspace{-0.25cm}

Fig.~\ref{fig:Performance_Bandwidth}(a) shows that normalized execution time decreases significantly as bandwidth increases up to approximately 50GB/s, yielding up to a 60\% performance improvement. This underscores the critical role of bandwidth in alleviating performance bottlenecks at lower levels. Beyond 100GB/s, the curve plateaus, with only a 1.7\% improvement from 50GB/s to 256GB/s, thus other system components become the bottleneck.

%For systems where rapid processing is paramount, optimizing HBM configurations around the 100 GB/s bandwidth mark may offer the most cost-effective balance between performance and expenditure. Settings beyond this point, while incrementally improving performance, may not justify the additional cost associated with higher bandwidth levels.

Fig.~\ref{fig:Performance_Bandwidth}(b)  illustrates that execution time increases with latency, especially sharply from 1ns to 12ns, then plateaus between 12ns and 36ns. However, the overall time overhead due to increased latency is only about 4.9\%. This analysis indicates that while both bandwidth and latency affect performance, the system is significantly more sensitive to changes in bandwidth. Thus, bandwidth is the more crucial factor in GEMM-dominated workloads.
% This trend highlights the critical impact of latency on performance in memory-intensive operations. The findings suggest that lower latency significantly enhances performance, with diminishing returns as latency increases beyond 12 ns. 
%This behavior is pivotal for optimizing HBM setups in scenarios where both time efficiency and memory performance are crucial, guiding system architects in balancing latency and computational throughput to achieve optimal system performance.

%Comparing the impact of latency and bandwidth on HBM performance, it is evident that both parameters play crucial roles, but their importance varies depending on the specific performance metrics and system requirements. Latency directly affects the speed at which data is accessed, with lower latency significantly enhancing performance, as shown by the steep initial rise in processing time with increased latency. However, once a certain threshold is reached, the additional latency has a diminishing impact. On the other hand, bandwidth influences the volume of data that can be processed per unit time, with higher bandwidth generally resulting in better performance. 

% From our analysis, while increased bandwidth consistently improves throughput, optimal performance is more acutely sensitive to changes in latency at lower levels. Therefore, in environments where rapid access to data is more critical than the amount of data processed, latency emerges as the more influential factor. Conversely, for applications requiring high data throughput, bandwidth assumes greater importance. 
%This nuanced understanding is essential for system designers to tailor HBM configurations that align with specific computational demands.

\textbf{\textit{Key Takeaway \#4:} Bandwidth has a greater impact than latency in GEMM performance: bandwidth improves performance by 60\% and latency only adds 5\% overhead.}

\subsubsection{Address Translation performance analysis}

% In this section, we analyze the performance impact of virtual address translation on system performance, particularly focusing on scenarios with larger matrix sizes. The results of our experiments are summarized in Table~\ref{tab:addrtrans}, which presents key metrics such as memory footprint, translation times, page table walk (PTW) times, and various translation lookup and miss rates.
In this section, we analyze the impact of virtual address translation on system performance, focusing on larger matrix sizes. Table~\ref{tab:addrtrans} summarizes key metrics such as memory footprint, translation times, and page table walk (PTW) times.

As matrix size increases, translation overhead initially decreases, reaching 1.00\% at 1024, but rises to 6.49\% at 2048. This reflects how larger matrices reduce per-translation costs through amortization but face increasing complexity with larger address spaces. Similarly, the mean translation time drops to its lowest value at 1024 but spikes to 54.38 cycles at 2048, indicating inefficiencies. PTW times also increase with matrix size, reaching 368.14 cycles at 2048, highlighting the growing complexity of address translation for very large datasets. 
%In summary, while larger matrices amortize per-translation costs, the increasing translation overhead and page table walk (PTW) times associated with managing larger memory footprints can create significant bottlenecks. 
This highlights the critical need for careful optimization of address translation mechanisms to maintain system performance, particularly for large-scale workloads.

% As matrix sizes increase, the memory footprint grows significantly, leading to a corresponding rise in the number of pages that must be translated. For instance, when the matrix size increases from 64 to 2048, the memory footprint expands from 12 pages to 12,288 pages, causing a substantial increase in translation times—from 3,130 cycles to 68,430,699 cycles.
% Translation overhead decreases with matrix size, hitting a low of 1.00\% at 1024, but rises again to 6.49\% at 2048, suggesting the complexity of managing large matrices offsets initial gains. This analysis shows that while larger matrices benefit from lower per-translation costs, the overall increase in translation time and overhead can become a bottleneck for very large datasets. The mean translation time, representing the average time for a single address translation, decreases with matrix size, reaching its lowest at 1024 due to amortization of overheads, but spikes to 54.38 cycles at 2048, indicating inefficiencies with larger address spaces. Similarly, Page Table Walk (PTW) times increase with matrix size, reaching 368.14 cycles at 2048, reflecting the growing complexity of address translation. Translation overhead starts at 6.02\% for smaller matrices, drops to 1.00\% at 1024, but rises again to 6.49\% at 2048, showing that the benefits of larger matrices diminish with increasing complexity.
% The mean translation time, which represents the average time required to complete a single address translation, initially decreases as the matrix size grows, reaching its lowest point at a matrix size of 1024. This decrease is due to the amortization of fixed overheads across a larger number of translations. However, at the maximum matrix size of 2048, the mean translation time spikes to 54.38 cycles, indicating potential inefficiencies or increased complexity in managing very large address spaces.
% Page Table Walk (PTW) times also escalate with larger matrix sizes. The PTW mean time, which represents the average time taken for a page table walk, increases steadily with matrix size, reflecting the growing complexity of address translation as more pages are involved. Notably, at a matrix size of 2048, the PTW mean time reaches 368.14 cycles, significantly higher than the times observed for smaller matrices.
% Translation overhead, defined as the percentage of total execution time attributed to address translation, starts at 6.02\% for the smallest matrix size and decreases as the matrix size increases, reaching a low of 1.00\% for a matrix size of 1024. However, for the largest matrix size of 2048, the translation overhead rises again to 6.49\%, suggesting that the benefits of larger matrices are offset by the increasing complexity and time required for address translation.
% Overall, the analysis reveals that while larger matrix sizes benefit from lower per-translation costs due to amortization, the absolute translation times and overheads increase dramatically. This suggests that for applications dealing with very large datasets, address translation can become a significant bottleneck, potentially limiting overall system performance.
% In summary, while larger matrices reduce per-translation costs, increased translation time and overhead for very large datasets can become a bottleneck, limiting system performance.

\begin{table*}[htbp]
\vspace{-0.25cm}
\centering
\caption{Results with Larger Matrix Sizes}
\label{tab:addrtrans}
\begin{tabular}{lcccccc}
\toprule
\textbf{Metric} & \textbf{64} & \textbf{128} & \textbf{256} & \textbf{512} & \textbf{1024} & \textbf{2048} \\
\midrule
Memory Footprint (Pages) & 12.0 & 48.0 & 192.0 & 768.0 & 3072.0 & 12288.0 \\
Translation Times & 3130 & 18470 & 142738 & 1082780 & 8593259 & 68430699 \\
Trans Mean Time & 23.42683 & 20.37948 & 13.87159 & 9.91735 & 10.478634 & 54.38005 \\
PTW Times & 15 & 54 & 227 & 1034 & 7675 & 479244 \\
PTW Mean Time & 176.6666 & 281.90740 & 265.255507 & 252.465184 & 294.609381 & 368.141137 \\
% Maintlb Lookup times & 375 & 1582 & 11524 & 77328 & 814928 & 17345813 \\
% Maintlb Misses times & 195 & 774 & 3107 & 12554 & 91090 & 7408778 \\
uTLB Lookup times & 2350 & 17690 & 137290 & 1081610 & 8586250 & 68423690 \\
uTLB Misses times & 195 & 862 & 8644 & 65808 & 731513 & 10416279 \\
Trans Overhead & 6.02\% & 1.87\% & 1.59\% & 1.30\% & 1.00\% & 6.49\% \\
\bottomrule
\end{tabular}
\vspace{-0.5cm}
\end{table*}

% \textbf{\textit{Key Takeaway \#5:} From the accelerator's perspective, memory access patterns are typically regular, resulting in minimal overhead from address translation.}
\textbf{\textit{Key Takeaway \#5:} Address translation overhead significantly impacts performance for large memory footprints. Larger matrices increase translation complexity and page table walk times, requiring optimization to avoid bottlenecks.}
\vspace{-0.25cm}
\subsection{Transformer Performance Evaluation}
We apply the GEMM analysis insights to Transformer inference using four system configurations:
\begin{enumerate}
    \item A system using host memory and small PCIe bandwidth of 2 GB/s with 4 4Gbps lanes (PCIe-2GB).
    \item A system using host memory and moderate PCIe bandwidth of 8 GB/s with 8 8Gbps lanes (PCIe-8GB).
    \item A system using host memory and PCIe bandwidth of 64 GB/s with 16 64Gbps lanes (PCIe-64GB).
    \item A system without host memory, utilizing device-side memory instead (DevMem).
\end{enumerate}

We select ViT as the target workload and configure each system based on the GEMM analysis:
\begin{itemize}
    \item For PCIe-2GB and PCIe-8GB: packet size of 256 and DDR4 memory.
    \item For PCIe-64GB: packet size of 256 and HBM2 memory.
    \item For DevMem: packet size of 64 and HBM2 memory.
\end{itemize}
\vspace{-0.25cm}
\begin{figure}[h]
\vspace{-0.25cm}
    \centering
    \includegraphics[width=\linewidth]{figures/Comparison_of_Normalized_Speedup_Values.png}
    \vspace{-0.8cm}
    \caption{Performance comparison of memory locations and interconnects}
    \label{fig:config_comparison}
    \vspace{-0.25cm}
\end{figure}

Fig.~\ref{fig:config_comparison} shows that PCIe-64GB achieves significant performance improvements over the baseline PCIe-2GB, with speedups ranging from approximately 2.5x to 3.4x, highlighting the critical impact of PCIe bandwidth when using host memory. In contrast, the DevMem configuration, despite leveraging device-side memory (HBM2 with burst size of 64) and reduced data transfer times due to proximity to computational units, performs slightly worse than PCIe-64GB.
\vspace{-0.1cm}
\subsection{Transformer Workload Analysis}
\subsubsection{GEMM and Non-GEMM Performance Evaluation}
To understand the performance discrepancies observed, we conduct a detailed analysis of the Transformer workload, profiling both GEMM and Non-GEMM operations across different ViT models and system configurations, as highlighted in prior research~\cite{Ivanov2020},~\cite{Karami2024}. Fig.~\ref{fig:NonGEMM_comparison} shows that while DevMem offers the best performance for GEMM workloads due to its high data bandwidth, it exhibits the poorest performance in Non-GEMM operations, incurring up to a 500\% overhead compared to systems using the PCIe interface. This degradation stems from the high latency introduced by the NUMA architecture and longer access times to device-side memory. Consequently, despite DevMem's advantages in GEMM tasks, these overheads hinder system performance in Non-GEMM computations.

%\vspace{-0.1cm}
\begin{figure}[h]
%\vspace{-0.1cm}
    \centering
    \includegraphics[width=\linewidth]{figures/Comparison_of_Normalized_GEMM_and_Non_GEMM.png}
    \vspace{-0.4cm}
    \caption{Performance Comparison of GEMM and Non-GEMM workload}
    \label{fig:NonGEMM_comparison}
    \vspace{-0.2cm}
\end{figure}

\textbf{\textit{Key Takeaway \#6:} Device-side memory overhead from NUMA architecture can degrade performance, accounting for ~40\% in Transformer experiments like ViT\_large.}

\subsubsection{Performance Analysis of GEMM and Non-GEMM Workloads}
To quantify the performance trade-offs between GEMM and Non-GEMM workloads, we propose a model that expresses the total execution time of Transformers as a combination of these two components. By understanding their respective contributions and computational requirements, we can determine the optimal balance between device-side and host-side memory utilization.
The total execution time \( \text{Time}_{\text{overall}} \) of a Transformer workload can be expressed as:
\vspace{-0.25cm}
\[
\text{Time}_{\text{overall}} = T_{\text{other}} + \frac{W_{\text{GEMM}}}{P_{\text{GEMM}}} + \frac{W_{\text{NonGEMM}}}{P_{\text{NonGEMM}}}
\]

Where \( T_{\text{other}} \) is the fixed time for other operations.
\( W_{\text{GEMM}} \) is the fraction of GEMM workload (\( 0 \leq W_{\text{GEMM}} \leq 1 \)).
\( P_{\text{GEMM}} \) and \( P_{\text{NonGEMM}} \) are the performance metrics for GEMM and Non-GEMM workloads, respectively. In our case, the Non-GEMM percentage represents the proportion of overall time spent on Non-GEMM workloads when executed on a PCIe system configuration.

\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{figures/Performance_vs_NonGEMM_Workload_Normalized_Line_with_Intersections.png}
    \vspace{-0.8cm}
    \caption{Overall Transformer Performance as a Function of Non-GEMM Workload Fraction for various PCIe Bandwidths vs DevMem}
    \label{fig:NonGEMMPer_comparison}
    \vspace{-0.5cm}
\end{figure}

We vary the \( W_{\text{NonGEMM}} \) from 0\% to 100\% to analyze the performance variance. Fig.~\ref{fig:NonGEMMPer_comparison} shows the performance variation with increasing Non-GEMM percentage. Our analysis reveals that DevMem outperforms the PCIe system when the GEMM workload fraction (\( W_{\text{GEMM}} \)) exceeds certain thresholds, which decrease as PCIe bandwidth increases. Specifically, DevMem is preferable when \( W_{\text{GEMM}} \) exceeds 34.31\% for a PCIe bandwidth of 2\,GB/s, 10.16\% for 8\,GB/s, and 4.27\% for 64\,GB/s. This indicates that as PCIe bandwidth increases, the advantage of using DevMem diminishes unless the workload is overwhelmingly dominated by GEMM operations.

\textbf{\textit{Key Takeaway \#7:} The choice between PCIe and DevMem depends on workload composition (GEMM vs. Non-GEMM) and PCIe bandwidth; DevMem is preferred when Non-GEMM fractions are below a threshold.}

\vspace{-0.15cm}
\section{Conclusion}
\vspace{-0.15cm}
In this paper, we have introduced Gem5-AcceSys, a comprehensive framework that extends the Gem5 simulator to support standard interconnects like PCIe, NUMA architectures, and configurable memory hierarchies. This advancement addresses critical limitations of existing simulators, enabling detailed system-level co-design and realistic performance evaluation of hardware accelerators. Using Gem5-AcceSys, we conduct an in-depth analysis of a matrix multiplication accelerator tailored for transformer workloads. Our experiments demonstrate that optimized PCIe interconnects can achieve up to 80\% of the performance of systems using device-side memory, and in some cases, even surpass them. We also uncover trade-offs between GEMM and Non-GEMM workloads, highlighting how their balance influences the optimal choice of memory configurations and interconnect strategies. By facilitating the exploration of standard interconnects and memory hierarchies, Gem5-AcceSys provides actionable insights for system architects, empowering them to balance performance and cost in designing efficient and scalable next-generation accelerator systems.

% This paper presents a comprehensive framework for exploring the performance impact of interconnects and memory hierarchies in transformer workloads, emphasizing the critical role of PCIe and memory configurations. By extending the Gem5 simulator to include standard interconnects, NUMA architectures, and advanced memory subsystems, we provide a robust platform for system-level co-design. Our findings highlight that while device-side memory significantly boosts GEMM performance, PCIe interconnects with optimized bandwidth and packet sizes can achieve comparable efficiency, offering a cost-effective alternative. Additionally, the analysis reveals the nuanced trade-offs between GEMM and Non-GEMM workloads, guiding system architects in selecting memory and interconnect strategies tailored to workload characteristics.

% In this paper, we introduce Gem5-ESL, a comprehensive system-accelerator co-design framework designed to address the growing complexity and performance demands of modern heterogeneous systems. Through the integration of practical interfaces like PCIe, DMA, and SMMU, as well as support for complex memory hierarchies and device-side memory, Gem5-ESL enhances the fidelity and applicability of full-system simulations for architectural exploration and optimization.

% Our experimental evaluations highlighted several key insights:

% \begin{itemize}
%     \item \textbf{PCIe Link Bandwidth}: We observed that increasing PCIe bandwidth significantly improves accelerator performance up to a certain threshold, beyond which the gains plateau. This underlines the importance of carefully balancing bandwidth with other system components to achieve optimal performance.
    
%     \item \textbf{Packet Size Impact}: The size of data packets transmitted from the accelerator also plays a critical role in performance. Smaller packet sizes generally result in lower overhead, while larger packets can introduce significant inefficiencies, particularly when the packet size exceeds certain thresholds.
    
%     \item \textbf{Direct Cache Access vs. Direct Memory Access}: The choice between direct cache and memory access is influenced by factors such as PCIe bandwidth and matrix size. While direct cache access can outperform direct memory access at higher bandwidths, its advantage diminishes as the matrix size increases, emphasizing the need for tailored cache strategies.
    
%     \item \textbf{IOCache}: Our findings indicate that IOCache has a minimal impact on overall system performance, with slight degradations observed as latency increases. This suggests that while IOCache can provide some benefits, its role is secondary compared to other factors like PCIe bandwidth and memory hierarchy.
    
%     \item \textbf{Device-Side Memory}: Implementing device-side memory offers substantial performance gains, particularly in scenarios where high data throughput is essential. However, this configuration can also introduce overheads, particularly in NUMA architectures, where CPU access to accelerator data is required.
    
%     \item \textbf{Memory Type and Bandwidth}: The type and location of memory significantly influence system performance. Our results demonstrate that device-side memory outperforms host-side memory, particularly when paired with high-bandwidth PCIe links. Furthermore, bandwidth is shown to have a more substantial impact on performance than latency, particularly in memory-intensive operations.
    
%     \item \textbf{Address Translation}: While memory access patterns from the accelerator tend to be regular, minimizing the overhead from address translation, larger matrix sizes can introduce significant delays due to increased complexity in managing address spaces.
    
%     \item \textbf{System-Level Impact}: In our transformer-based case study, we found that while high PCIe bandwidth and optimized memory configurations can significantly boost performance, device-side memory in a NUMA architecture may degrade performance due to overheads associated with CPU data access.
% \end{itemize}

% Overall, Gem5-ESL provides a powerful platform for exploring and optimizing the design of heterogeneous systems. By enabling detailed simulation of both accelerators and system architectures, it offers valuable insights that can guide the development of more efficient and effective hardware solutions for emerging workloads in machine learning and image processing.

\vspace{-0.15cm}
\section*{Acknowledgments}
\vspace{-0.15cm}
We thank the anonymous reviewers for their feedback, Pengbo Yu for hardware and manuscript guidance, and Gabriel Catel Torres and Clément Dieperink for software support. 

This work was supported in part by the Swiss State Secretariat for Education, Research, and Innovation (SERI) through the SwissChips research project, and also by Intel as part of the Intel Center for Heterogeneous Integrated Platforms (HIP).

% \begin{thebibliography}{00}
% \bibitem{gpu} Q. Huang, Z. Huang, P. Werstein and M. Purvis, "GPU as a General Purpose Computing Resource," 2008 Ninth International Conference on Parallel and Distributed Computing, Applications and Technologies, Dunedin, New Zealand, 2008, pp. 151-158, doi: 10.1109/PDCAT.2008.38.
% \bibitem{isp} Marques da Silva, M. H., “ISP meets Deep Learning: A Survey on Deep Learning Methods for Image Signal Processing”, <i>arXiv e-prints</i>, 2023. doi:10.48550/arXiv.2305.11994.
% \bibitem{tic-sat} A. Amirshahi, J. A. Harrison Klein, G. Ansaloni and D. Atienza, "TiC-SAT: Tightly-coupled Systolic Accelerator for Transformers," 2023 28th Asia and South Pacific Design Automation Conference (ASP-DAC), Tokyo, Japan, 2023, pp. 657-663.
% \bibitem{eyeriss} Y. -H. Chen, T. Krishna, J. S. Emer and V. Sze, "Eyeriss: An Energy-Efficient Reconfigurable Accelerator for Deep Convolutional Neural Networks," in IEEE Journal of Solid-State Circuits, vol. 52, no. 1, pp. 127-138, Jan. 2017, doi: 10.1109/JSSC.2016.2616357.
% \bibitem{gem5-aladdin} Y. S. Shao, S. L. Xi, V. Srinivasan, G. -Y. Wei and D. Brooks, "Co-designing accelerators and SoC interfaces using gem5-Aladdin," 2016 49th Annual IEEE/ACM International Symposium on Microarchitecture (MICRO), Taipei, Taiwan, 2016, pp. 1-12, doi: 10.1109/MICRO.2016.7783751.
% \bibitem{gem5-salam} S. Rogers, J. Slycord, M. Baharani and H. Tabkhi, "gem5-SALAM: A System Architecture for LLVM-based Accelerator Modeling," 2020 53rd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO), Athens, Greece, 2020, pp. 471-482, doi: 10.1109/MICRO50266.2020.00047.
% \bibitem{gem5-rtl} Guillem López-Paradís, Adrià Armejach, and Miquel Moretó. 2021. Gem5 + rtl: A Framework to Enable RTL Models Inside a Full-System Simulator. In Proceedings of the 50th International Conference on Parallel Processing (ICPP '21). Association for Computing Machinery, New York, NY, USA, Article 29, 1–11. https://doi.org/10.1145/3472456.3472461
% \bibitem{gem5-x} Yasir Mahmood Qureshi, William Andrew Simon, Marina Zapater, Katzalin Olcoz, and David Atienza. 2021. Gem5-X: A Many-core Heterogeneous Simulation Platform for Architectural Exploration and Optimization. ACM Trans. Archit. Code Optim. 18, 4, Article 44 (December 2021), 27 pages. https://doi.org/10.1145/3461662
% \bibitem{pcie1} J. Ajanovic, "PCI express 3.0 overview," 2009 IEEE Hot Chips 21 Symposium (HCS), Stanford, CA, USA, 2009, pp. 1-61, doi: 10.1109/HOTCHIPS.2009.7478337.
% \bibitem{pcie2} M. Vasa, C. -L. Liao, S. Kumar, C. -H. Chen and B. Mutnury, "PCIe Gen-5 Design Challenges of High-Speed Servers," 2020 IEEE 29th Conference on Electrical Performance of Electronic Packaging and Systems (EPEPS), San Jose, CA, USA, 2020, pp. 1-3, doi: 10.1109/EPEPS48591.2020.9231458.
% \bibitem{smmu1} J. Whitham and N. Audsley, "Studying the Applicability of the Scratchpad Memory Management Unit," 2010 16th IEEE Real-Time and Embedded Technology and Applications Symposium, Stockholm, Sweden, 2010, pp. 205-214, doi: 10.1109/RTAS.2010.21.
% \bibitem{smmu2} Paraskevas, K., Iordanou, K., Luján, M., \& Goodacre, J. (2020). Analysis of the Usage Models of System Memory Management Unit in Accelerator-attached Translation Units. Paper presented at 6th International Symposium on Memory Systems, Washington,DC, United States. https://doi.org/10.1145/3422575.3422781
% \bibitem{verilator} Yuan Chi, Xian Lin, and Xin Zheng. 2023. Design of High-performance SoC Simulation Model Based on Verilator. In Proceedings of the 2022 5th International Conference on Algorithms, Computing and Artificial Intelligence (ACAI '22). Association for Computing Machinery, New York, NY, USA, Article 92, 1–6. https://doi.org/10.1145/3579654.3579751
% \bibitem{dramsim3} S. Li, Z. Yang, D. Reddy, A. Srivastava and B. Jacob, "DRAMsim3: A Cycle-Accurate, Thermal-Capable DRAM Simulator," in IEEE Computer Architecture Letters, vol. 19, no. 2, pp. 106-109, 1 July-Dec. 2020, doi: 10.1109/LCA.2020.2973991.
% \bibitem{ramulator} Haocong Luo, Yahya Can Tugrul, F. Nisa Bostancı, Ataberk Olgun, A. Giray Yaglıkcı, and Onur Mutlu, "Ramulator 2.0: A Modern, Modular, and Extensible DRAM Simulator," arXiv, 2023.
% \bibitem{dramsys} Steiner, L., Jung, M., Prado, F.S. et al. DRAMSys4.0: An Open-Source Simulation Framework for In-depth DRAM Analyses. Int J Parallel Prog 50, 217–242 (2022).
% \bibitem{MatrixFlow} Qunyou Liu....
% \bibitem{bert} Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova:
% BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. NAACL-HLT (1) 2019: 4171-4186
% \bibitem{bertana} Ivanov, A., Dryden, N., Ben-Nun, T., Li, S., \& Hoeﬂer, T. (2020). Data Movement Is All You Need: A Case Study of Transformer Networks.
% \bibitem{ddr4} JEDEC. (2012). DDR4 SDRAM Standard JESD79-4.
% \bibitem{hbm} JEDEC. (2013). High Bandwidth Memory (HBM) DRAM JESD235.
% \bibitem{gddr5} JEDEC. (2008). Graphics Double Data Rate (GDDR5) SGRAM Standard JESD212.
% \bibitem{lpddr5} JEDEC. (2019). Low Power Double Data Rate 5 (LPDDR5) JESD209-5.

% \bibitem{nearmem} Asif Ali Khan, João Paulo C. De Lima, Hamid Farzaneh, Jeronimo Castrillon, "The Landscape of Compute-near-memory and Compute-in-memory: A Research and Commercial Overview", Jan 2024.

% \bibitem{gpu} Q. Huang, Z. Huang, P. Werstein and M. Purvis, "GPU as a General Purpose Computing Resource," 2008 Ninth International Conference on Parallel and Distributed Computing, Applications and Technologies, Dunedin, New Zealand, 2008, pp. 151-158, doi: 10.1109/PDCAT.2008.38.
% \bibitem{isp} Claudio Filipi Goncalves dos Santos, Rodrigo Reis Arrais, Jhessica Victoria Santos da Silva, Matheus Henrique Marques da Silva, Wladimir Barroso Guedes de Araujo Neto, Leonardo Tadeu Lopes, Guilherme Augusto Bileki, Iago Oliveira Lima, Lucas Borges Rondon, Bruno Melo de Souza, Mayara Costa Regazio, Rodolfo Coelho Dalapicola, and Arthur Alves Tasca. 2025. ISP Meets Deep Learning: A Survey on Deep Learning Methods for Image Signal Processing. ACM Comput. Surv. 57, 5, Article 127 (May 2025), 44 pages. https://doi.org/10.1145/3708516

\begin{thebibliography}{99}
\bibitem{Dosovitskiy2021} A. Dosovitskiy \textit{et al.}, “An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale,” in \textit{Proc. ICLR}, 2021.
\bibitem{Devlin2019} J. Devlin, M. Chang, K. Lee, and K. Toutanova, “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,” in \textit{Proc. NAACL-HLT}, 2019, pp. 4171–4186.
\bibitem{Jouppi2017} N. P. Jouppi \textit{et al.}, “In-Datacenter Performance Analysis of a Tensor Processing Unit,” in \textit{Proc. 44th Annu. Int. Symp. on Computer Architecture (ISCA)}, Toronto, ON, Canada, Jun. 2017, pp. 1–12.
\bibitem{Amirshahi2023} AA. Amirshahi, J. A. H. Klein, G. Ansaloni, and D. Atienza, “TiC-SAT: Tightly-Coupled Systolic Accelerator for Transformers,” in \textit{Proc. 28th Asia and South Pacific Design Automation Conf. (ASP-DAC)}, Tokyo, Japan, Jan. 2023, pp. 657–663.
\bibitem{Ajanovic2009} J. Ajanovic, “PCI Express 3.0 Overview,” in \textit{Proc. 2009 IEEE Hot Chips 21 Symposium}, 2009, pp. 1–61.
\bibitem{Vasa2020} M. Vasa, C.-L. Liao, S. Kumar, C.-H. Chen, and B. Mutnury, “PCIe Gen-5 Design Challenges of High-Speed Servers,” in \textit{Proc. IEEE Conf. on Electrical Performance of Electronic Packaging and Systems (EPEPS)}, 2020, pp. 1–3. DOI: 10.1109/EPEPS48591.2020.9231458.
\bibitem{Shao2016} Y. S. Shao, S. L. Xi, V. Srinivasan, G.-Y. Wei, and D. Brooks, “Co-designing accelerators and SoC interfaces using gem5-Aladdin,” in \textit{Proc. 49th Annu. IEEE/ACM Int. Symp. on Microarchitecture (MICRO)}, Taipei, Taiwan, 2016, pp. 1–12. DOI: 10.1109/MICRO.2016.7783751.
\bibitem{Rogers2020} S. Rogers, J. Slycord, M. Baharani, and H. Tabkhi, “gem5-SALAM: A System Architecture for LLVM-based Accelerator Modeling,” in \textit{Proc. 53rd Annu. IEEE/ACM Int. Symp. on Microarchitecture (MICRO)}, 2020, pp. 471–482. DOI: 10.1109/MICRO50266.2020.00047.
\bibitem{Lopez2021} A. Armejach, G. López-Paradís, and M. Moretó, “gem5 + RTL: A Framework to Enable RTL Models Inside a Full-System Simulator,” in \textit{Proc. 50th Int. Conf. on Parallel Processing (ICPP)}, 2021, pp. 1–11. DOI: 10.1145/3472456.3472461.
\bibitem{Qureshi2021} Y. M. Qureshi, W. A. Simon, M. Zapater, K. Olcoz, and D. Atienza, “Gem5-X: A Many-Core Heterogeneous Simulation Platform for Architectural Exploration and Optimization,” \textit{ACM Trans. Archit. Code Optim.}, vol. 18, no. 4, Art. 44, Dec. 2021, 27 pages. DOI: 10.1145/3461662.
\bibitem{Khan2024} A. A. Khan, J. P. C. De Lima, H. Farzaneh, and J. Castrillon, “The Landscape of Compute-near-memory and Compute-in-memory: A Research and Commercial Overview,” \textit{arXiv:2401.14428}, 2024.
\bibitem{Whitham2010} J. Whitham and N. Audsley, “Studying the Applicability of the Scratchpad Memory Management Unit,” in \textit{Proc. IEEE Real-Time and Embedded Technology and Applications Symp. (RTAS)}, 2010, pp. 205–214. DOI: 10.1109/RTAS.2010.21.
\bibitem{Paraskevas2020} K. Paraskevas, K. Iordanou, M. Luján, and J. Goodacre, “Analysis of the Usage Models of System Memory Management Unit in Accelerator-Attached Translation Units,” in \textit{Proc. Int. Symp. on Memory Systems (MEMSYS)}, 2020, pp. 86–96. DOI: 10.1145/3422575.3422781.
\bibitem{Chi2022} Y. Chi, X. Lin, and X. Zheng, “Design of a High-Performance SoC Simulation Model Based on Verilator,” in \textit{Proc. 5th Int. Conf. on Algorithms, Computing and Artificial Intelligence (ACAI)}, 2022, pp. 1–6. DOI: 10.1145/3579654.3579751.
\bibitem{Li2020} S. Li, Z. Yang, D. Reddy, A. Srivastava, and B. Jacob, “DRAMsim3: A Cycle-Accurate, Thermal-Capable DRAM Simulator,” \textit{IEEE Comput. Archit. Lett.}, vol. 19, no. 2, pp. 106–109, Jul.–Dec. 2020. DOI: 10.1109/LCA.2020.2973991.
\bibitem{Luo2023} H. Luo, Y. C. Tuğrul, F. N. Bostancı, A. Olgun, A. G. Yağlıkçı, and O. Mutlu, “Ramulator 2.0: A Modern, Modular, and Extensible DRAM Simulator,” \textit{arXiv:2308.11030v2}, 2023.
\bibitem{Steiner2022} L. Steiner, M. Jung, F. S. Prado, K. Bykov, and N. Wehn, “DRAMSys4.0: An Open-Source Simulation Framework for In-Depth DRAM Analyses,” \textit{Int. J. Parallel Program.}, vol. 50, no. 2, pp. 217–242, Apr. 2022. DOI: 10.1007/s10766-022-00727-4.
\bibitem{Liu2025} Q. Liu, M. Zapater, and D. Atienza, “MatrixFlow: System-Accelerator Co-Design for High-Performance Transformer Applications,” \textit{arXiv preprint arXiv:2503.05290}, 2025.
\bibitem{Ivanov2020} A. Ivanov, N. Dryden, T. Ben-Nun, S. Li, and T. Hoefler, “Data Movement Is All You Need: A Case Study on Optimizing Transformers,” \textit{arXiv:2007.00072}, 2020.
\bibitem{Karami2024} R. Karami, S.-C. Kao, and H. Kwon, “NonGEMM Bench: Understanding the Performance Horizon of the Latest ML Workloads with NonGEMM Operations,” \textit{arXiv:2404.11788v3}, 2024.
\end{thebibliography}

% \end{thebibliography}
\vspace{12pt}

\end{document}
