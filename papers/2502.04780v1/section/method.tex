\section{Method}

\begin{table*}[t]
\centering
\caption{
    Different settings and tasks.
    In the rows corresponding to Communication Structure, nodes denote agents ($\mathcal{V}$), arrows represent edges ($E$), and color indicates the role of agents.
    }
\resizebox{\textwidth}{!}{
\begin{tabular}{l||c|c||c||c}
\toprule
\textbf{Settings} & \multicolumn{2}{c||}{\textbf{Problem-Solving}} & {\textbf{Actor-Critic}} &{\textbf{Competitive}} \\ 
\midrule
\textbf{Structure $(\mathcal{V},E,\mathcal{P})$} & {\begin{minipage}[b]{0.45\columnwidth}
		\centering
		\raisebox{-.5\height}{\includegraphics[height=2.2cm]{figure/setting1.pdf}}
	  \end{minipage} }&\begin{minipage}[b]{0.4\columnwidth}
		\centering
		\raisebox{-.5\height}{\includegraphics[height=2.2cm]{figure/setting2.pdf}}
	  \end{minipage}& \begin{minipage}[b]{0.48\columnwidth}
		\centering
		\raisebox{-.5\height}{\includegraphics[height=2.2cm]{figure/setting3.pdf}}
	  \end{minipage}&{\begin{minipage}[b]{0.4\columnwidth}
		\centering
		\raisebox{-.5\height}{\includegraphics[height=2.2cm]{figure/setting4.pdf}}
	  \end{minipage}} \\
\midrule
\textbf{Tasks} & {\makecell{College-Physics\\ College-Chemistry}}& PubMedQA & PubMedQA  &{\makecell{Resource Exchange\\ Seller-Buyer\\ Ultimatum}}  \\
\midrule
\textbf{Reward for each agent $R_i$} & \multicolumn{2}{c||}{Final Output Correctness} & \makecell{Final Output Correctness} &{Utility Function Value}\\ 
\bottomrule
\end{tabular}
}
    \label{tab:setting} 
\end{table*}


\subsection{Multi-agent systems with LLMs}  
%\shirwu{can we simplify this formulation a bit? For example, is $\gamma$ a necessary component?}
We define a multi-agent system by a tuple $\langle \mathcal{S}, \mathcal{A}, \mathcal{T}, \mathcal{R}, \mathcal{N}, \mathcal{G} \rangle$. Here, $\mathcal{N} \triangleq \{A^{(1)}, A^{(2)}, \ldots, A^{(N)}\}$ is the set of $N$ agents, each agent $A^{(i)}$ uses a policy $\pi_i$ parameterized by $\theta^{(i)}$. $s \in \mathcal{S}$ is the state of the environment, $\mathbf{a} \in \mathcal{A}$ is the joint actions, and $\mathcal{A}$ is the joint action space. $\mathcal{T}: \mathcal{S} \times \mathcal{A} \to \mathcal{S}$ is the transition function where $\mathcal{T}(s, \mathbf{a})$ yields the next state of the environment given the current state and joint actions $\mathbf{a}$. The environment feedback is modeled via a payoff function $\mathcal{R}_i: \mathcal{S} \times \mathcal{A} \to \mathbb{R}^N$, which provides rewards for each agent $k$ based on the state-action pairs. 

The communication structure between agents is modeled as a directed graph $\mathcal{G} = (\mathcal{V}, E, \mathcal{P})$, where $\mathcal{V}$ represents agents, and $E$ defines interaction order. 

For each edge $(i, j) \in E$, agent $A^{(j)}$ receives an input derived from the state-action pair $(s, \mathbf{a})$ and the output of agent $A^{(i)}$. This input determines agent $A^{(j)}$'s subsequent action. For each agent $A^{(i)}$ in a topological graph $\mathcal{G}$, its predecessors are the set of agents that  influence its output:
$\mathrm{Pre}(A^{(i)}) = \{A^{(j)} \mid (A^{(j)}, A^{(i)}) \in \mathcal{G}\}.$ Here, $(A^{(j)}, A^{(i)})$ denotes a directed edge in the graph, indicating that the output of agent $A^{(j)}$ directly influences the input of agent $A^{(i)}$.



Throughout this paper, the collection of our agents will be based on language models and the primary environment that we use will be natural language. In particular:

\begin{equation}
   \begin{aligned}
    & a_i \sim \pi_i(\cdot | s_{t}, \{a_j\}_{A^{(j)} \in \mathrm{Pre}(A^{(i)})}) \quad \forall A^{(i)}\in \mathcal{N} \\
    & \mathbf{a}_t = (a_1, ..., a_N) \\
    & s_{t+1} = \mathcal{T}(s_t, \mathbf{a}_t) = \text{Concat}(s_t, \mathbf{a}_t)
   \end{aligned}
\end{equation}

where $\pi_i$ denotes the probability distribution of the $i$-th language model, $\text{Concat}$ is the concatenation of the previous state and the responses, and we will use $\mathbf{\pi} = \{\pi_1, \ldots, \pi_N\}$ to denote the joint policy. Generally, each agent aims to maximize its own reward:
\begin{equation}
    \max_{\pi_i} \mathbb{E}_{\mathbf{\pi}}\left[\sum_{t=0}^{\infty}  R_i(s_t, \mathbf{a}_t)\right],
\end{equation}
where $R_i$ denotes the $i$-th component of the reward vector $\mathcal{R}$ and the expectation is taken under the joint policy $\mathbf{\pi}$. 

\subsection{\model}
The training pipeline of the proposed framework, denoted as \model, is illustrated in Figure~\ref{fig:pipeline}. \model{} adopts a fine-tuning strategy to iteratively improve the policy parameters $\theta^{(n)}$ of each agent $A^{(n)}$ over $T$ iterations. The process is initialized with a dataset $\mathcal{D} = \{(x_i, y_i)\}_{i=1}^D$, where each pair $(x_i, y_i)$ represents a problem and its solution.
The core training procedure is outlined in Algorithm~\ref{alg:method}. 
\begin{algorithm}[htbp]
\caption{\model}
\label{alg:method}
\begin{algorithmic}[1]
\STATE \textbf{Input:} A group of agents $A^{(1)},\cdots,A^{(N)}$.

An initial dataset of problems $x$ with answer $y:\mathcal{D} = \{(x_i, y_i)\}_{i=1}^D$, total number of fine-tuning Iterations $T$.
\STATE \textbf{Initialize:} Initialize policy parameters $\theta^{(n)}$ for each agent $A^{(n)}$, $k = 1, 2, \dots, N$.

\FOR{Fine-tuning Iteration $\text{t} =1,\cdots,T$}    
   
     \STATE \fcolorbox[HTML]{F8CECC}{F8CECC} {$a_i^{(n)}=\mathcal{P}_{\theta^{(n)}_{\text{t}}}(\cdot|x_i,\textbf{a}_i^{\mathrm{Pre}(A^{(n)})})$, $k = 1, 2, \dots, K$.}
        
        \FOR{each agent $k = 1, 2, \dots, K$}
            
        
            \STATE  \setlength{\fboxrule}{1.2pt}\fcolorbox[HTML]{82B366}{FFFFFF} {$\mathcal{C}_{\text{t}}^{(n)}\leftarrow \{(x_i, a_i^{(n)}| i \in [1,D] \land R_i(s,a)>\epsilon)\}$} Good Trajectory Set of Each Agent.
            \STATE \fcolorbox[HTML]{B85450}{FFFFFF}{ \textbf{Augmentation$(\{(x_i, a_i^{(n)} \land R_i(s,a)<\epsilon)\})$}} 
        \ENDFOR
        

    \STATE  \fcolorbox[HTML]{DAE8FC}{DAE8FC} {$\theta^{(n)}_{\text{t}} \leftarrow \textbf{Standard SFT on }\mathcal{C}_{\text{t}}^{(n)}$, $n=1,\cdots,N$}
\ENDFOR
\end{algorithmic}
\end{algorithm}

At each fine-tuning iteration $t$:
\begin{itemize}
    \item \textbf{Action Sampling:} For each agent \( A^{(n)} \), an action \( a_i^{(n)} \) is sampled from its policy, 
    \[
    a_i^{(n)} = \mathcal{P}_{\theta^{(n)}}(\cdot | x_i, \mathbf{a}_i^{\mathrm{Pre}(A^{(n)})}),
    \]
    conditioned on the input problem $x_i$ and the action set \( \mathbf{a}_i^{\mathrm{Pre}(A^{(n)})} \) generated by previous agents. In scenarios involving multiple interaction rounds, such as the Competitive Setting, \( \mathbf{a}_i^{\mathrm{Pre}(A^{(n)})} \) includes outputs from all agents in all preceding rounds.
    \item \textbf{Trajectory Evaluation and Augmentation:} The trajectories generated by each agent are evaluated using the payoff function $R(s, \mathbf{a})$. Based on a reward threshold $\epsilon$,  
    high-reward trajectories ($R(s, \mathbf{a}) > \epsilon$) are added to the good trajectory set $\mathcal{C}_{t}^{(n)}$. Since the tasks are challenging, the good trajectory set tends to be small. To leverage more data for fine-tuning, we propose trajectory augmentation pipeline for each task, detailed in the Appendix \ref{ap:pipeline}. Specifically, we first generate feedback to refine the agent's original response.
    The feedback and original response are then combined to prompt the agent to regenerate a new solution, which is then rephrased into a direct problem-solving step. Afterward, we return to the action sampling process to produce the final answer and evaluate it. 

    \item \textbf{Fine-Tuning:} The policy parameters $\theta^{(n)}$ are updated via supervised fine-tuning (SFT) on $\mathcal{C}_{t}^{(n)}$.
\end{itemize}

This iterative process ensures that each agent's policy is progressively refined to maximize performance based on the joint system dynamics and reward. 

