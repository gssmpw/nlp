\section{Introduction} 
\begin{figure*}[htbp]
    \centering
    \includegraphics[width=1\linewidth]{figure/general_pipeline.pdf}
    \caption{General training pipeline of \model.Agents solve problems sequentially, storing correct responses for fine-tuning and augmenting incorrect ones through feedback, regeneration, and rephrasing. This iterative process improves performance via reward-based evaluation and supervised fine-tuning. The module colors in the figure correspond to those in Algorithm \ref{alg:method}.
}
    \label{fig:pipeline}
\end{figure*}

Multi-agent AI systems powered by large language models~(LLMs), where specialized agents collaborate to solve complex tasks, are becoming increasingly successful in real-world applications. Recent work has demonstrated their effectiveness in complex reasoning~\citep{wang2024mixture, smitshould}, coding~\citep{wu2023autogen}, drug discovery~\citep{swanson2024virtual} and ensuring safety via debate~\citep{chern2024combating, irving2018ai}. 
These successes arise from specialized agents integrating their distinct capabilities through structured interactions, enabling more effective problem-solving than single agents. Moreover, multi-agent scrutiny acts as a built-in self-correction mechanism, where agents refine and verify each other’s outputs. This often outperforms single agent setting, particularly on tasks demanding rigorous reasoning or factual validation.

Despite these successes, optimizing multi-agent systems remains a fundamental challenge due to (1) the difficulty of acquiring appropriate training signals for each agent and (2) the sensitivity to multiple moving parts that influence overall performance~\citep{smitshould}. While task-level reward feedback is available, credit assignment across agents remains ambiguous—it is unclear how to attribute success or failure to specific intermediate decisions and reasoning steps made by each LLM agent. This challenge parallels the multi-agent credit assignment problem in reinforcement learning~\citep{foerster2018counterfactual}. However, in language-based systems, reasoning unfolds through complex and unstructured interactions, making attribution far more difficult than in traditional RL settings with well-defined action spaces.

We present \model{}, a framework for learning effective multi-agent behaviors from outcome rewards. Our key insight is that when multiple agents successfully solve a task together, their entire interaction trajectory likely contains useful patterns - even if we cannot pinpoint exactly which steps or decisions were crucial for success. Drawing inspiration from recent advances in bootstrapping reasoning capabilities~\citep{zelikman2022star}, we collect and learn from successful agent interactions across many tasks, allowing the system to iteratively discover effective collaboration strategies from self-generated data. This approach sidesteps the need for direct supervision of intermediate steps, instead letting agents learn which interaction patterns tend to lead to successful outcomes. For trajectories that result in failed attempts, we perform trajectory augmentation by resampling original attempts with feedback from an additional agent grounded in the ground truth.

Our experiments demonstrate that \model{} significantly enhances multi-agent performance across multiple domains. It improves reasoning and biomedical QA accuracy by 2.86\% to 21.88\%, while also strengthening agent negotiation in competitive scenarios. Beyond these gains, our approach offers a scalable mechanism for self-improvement, enabling agents to iteratively refine their reasoning and collaboration strategies. More broadly, \model{} provides a general framework for optimizing multi-agent systems via self-generated synthetic data, offering a principled way to enhance performance without requiring fine-grained human supervision.
