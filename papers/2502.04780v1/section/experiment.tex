\section{Multi-agent Settings}
In this section, we explore several settings where agents with distinct expertise interact to solve challenging tasks. As shown in Table \ref{tab:setting}, we systematically analyze different agent configurations.

\begin{table*}[t]
\label{tab:competitive}
\caption{ Tasks and setups in the competitive setting. Each task involves two agents with distinct roles, initial resources, and objectives. \emph{Resource Exchange} focuses on maximizing total resources through trade. Ultimatum requires negotiating a split of $\$100$. \emph{Sell\&Buy} involves price negotiation for an item. Each task follows a turn-based structure with a fixed maximum number of rounds and ends when an agreement is reached.}
\centering
\small
\begin{tabular}{c|c|c|c|c|c|c}
\toprule
\textbf{Task} & \multicolumn{2}{c|}{\textbf{Resource Exchange}} & \multicolumn{2}{c|}{\textbf{Ultimatum}} & \multicolumn{2}{c}{\textbf{Sell\&Buy}} \\
\midrule
\textbf{Roles} & \textbf{Player 1} & \textbf{Player 2} & \textbf{Player 1} & \textbf{Player 2} & \textbf{Seller} & \textbf{Buyer} \\
\midrule
\textbf{Initial resources} & 25Xs, 5Ys & 5Xs, 25Ys & \$ 100 & 0 & 1X & 100 ZUPs \\
\textbf{Goal} & \multicolumn{2}{c|}{Maximize total resources} & \multicolumn{2}{c|}{Negotiate a split} & Maximize price & Minimize price \\
\textbf{Utility} & Xs + Ys & Xs + Ys  & Split amount-50  & Split amount-50 & Selling price - 50 & 50-Selling price \\
\textbf{Ending condition} & \multicolumn{2}{c|}{When either player accepts} & \multicolumn{2}{c|}{When either player accepts} & \multicolumn{2}{c}{When either player accepts} \\
\textbf{Max. \# of turns} & \multicolumn{2}{c|}{8 rounds of interaction} & \multicolumn{2}{c|}{8 rounds of interaction} & \multicolumn{2}{c}{10 rounds of interaction} \\
\bottomrule
\end{tabular}
\end{table*}

\subsection{Problem Solving Settings}  
 
\textbf{Agents with Specific Expertise.}  
In this setting, each agent is assigned a domain-specific role to facilitate a structured and efficient problem-solving process. For instance, in the physics and chemistry domains, the problem-solving pipeline begins with a domain expert (e.g., a physicist or chemist) who analyzes the domain-specific problem, followed by a mathematician who formalizes the reasoning with quantitative models, and finally, a summarizer who consolidates the insights into a clear and comprehensive answer. This sequential collaboration ensures that the expertise of each agent is leveraged effectively while maintaining clarity in the solution process.

The sequential dependency between the agents can be described as follows:
\begin{align}
    a_{\text{Phy}} &\sim \pi_{\text{Phy}}(\cdot |q), \\
    a_{\text{Math}} &\sim \pi_{\text{Math}}(\cdot |q, a_{\text{Phy}}), \\
    a_{\text{Sum}} &\sim \pi_{\text{Sum}}(\cdot |q, a_{\text{Phy}}, a_{\text{Math}}),
\end{align}
where $q $ is the input question, $a_{\text{Phy}} $ is the response generated by the Physicist, $a_{\text{Math}} $ is the response generated by the Mathematician based on both the question and the  Physicist's response,$a_{\text{Sum}} $ is the final answer synthesized by the Summarizer using the question, the  Physicist's response, and the Mathematician's response.

\textbf{Analyze Long Context and Answer Question.}  
In scenarios involving lengthy and complex contexts, we consider a common two-agent setup: the Context Analyst and the Problem Solver. The Context Analyst's responsibility is to thoroughly examine the context, extract essential information, and provide a concise and accurate summary. The Problem Solver then uses this summary to analyze the question and formulate the final answer. This division of labor not only improves interpretability, but also reduces the cognitive load on each agent.  

\subsection{Actor-Critic Setting}  
\label{sec:Actor-Critic Setting}

The popular Actor-Critic framework facilitates iterative agent improvement through a feedback loop: the Actor Agent generates solutions while the critic evaluates and refines them, enhancing both the Actor Agent's reasoning and the Critic Agent's error correction capabilities.
In practice, we separate judgment and feedback tasks by introducing a Judgment Agent alongside the Critic Agent, where the Judgment Agent classifies the Actor Agent's solutions as correct or incorrect, and for incorrect solutions, the critic provides feedback to guide the Actor Agent in regenerating improved solutions.
Reward mechanisms are designed as: the Actor Agent receives rewards for correct solutions, the Judgment Agent for accurate classifications, and the critic for providing actionable feedback that leads to correct regenerations.


\begin{table*}[h]
\caption{Evaluation results of the proposed method and baselines on accuracy(\%). Best results are in \textbf{bold} numbers and second-best results are in \underline{underline} numbers.} 
\centering
\small
\label{tab:problem-solving-main}
\begin{tabular}{l|l|ccc}
\toprule
\textbf{Model}                 & \textbf{Method}         & College Physics  & College Chemistry  &PubMedQA~\citep{jin2019pubmedqa}\\ \midrule
\multirow{5}{*}{GPT-3.5-turbo} & \textbf{Single-Agent}   & 24.30            & 38.46              & 56.40                \\ 
                               & \textbf{STaR}           & 29.91            & 47.69              & 63.80                \\ 
                               & \textbf{COMM}           & 30.84            & \underline{50.77}  &\underline{71.80}     \\     
                               &\textbf{TextGrad}        &\underline{32.71} & 41.54              &  NA    \\
                               & \textbf{\model}         &\textbf{33.64}    &\textbf{ 56.92}     &\textbf{74.20} 
                               \\ \midrule
\multirow{5}{*}{GPT-4o-mini}   & \textbf{Single-Agent}   & 39.25            & 41.54              & 67.40                \\ 
                               & \textbf{STaR}           & 42.06            & 47.69              & 69.20                \\ 
                               & \textbf{COMM}           &{42.06}           &\underline{49.23}   &\underline{70.60}\\     
                               &\textbf{TextGrad}        &\underline{42.99} & 44.62              & 68.20         \\
                               & \textbf{\model}         &\textbf{46.73}    &\textbf{60.00}      &\textbf{73.40} \\                          
\bottomrule
\end{tabular}
\end{table*}

\subsection{Competitive Settings}  


Competitive scenarios~\citep{bianchi2024well} examine multi-agent interactions under opposing objectives, where agents must balance cooperation and competition to achieve their goals. In this framework, two agent roles are defined: \textbf{Player 1} and \textbf{Player 2}. Each player is initialized with a specific amount of resources, which evolve over the course of the game based on their interactions.  The game progresses as a sequence of moves, resulting in a trajectory of states:  
\begin{equation}
\begin{aligned}
\text{Player 1 Trajectory: } x_0^{\text{player}1},x_1^{\text{player}1},\cdots,x_T^{\text{player}1}\\
\text{Player 2 Trajectory: }x_0^{\text{player}2},x_1^{\text{player}2},\cdots,x_T^{\text{player}2}
\end{aligned}
\end{equation}
The sequence captures the evolution of game states as players compete at each timestep $t = 0, 1, \dots, T $, ultimately determining a winner and a loser. Our goal is to optimize each player's policy to maximize its own expected reward based on trajectory data and role-specific context. This can be formulated as:
\begin{equation}
\begin{aligned}
\max \sum_{i=1}^T P_\theta(x_i^{\text{player1}} | x_{0:i-1}^{\text{player1}}, x_{0:i-1}^{\text{player2}})
\end{aligned}
\end{equation}
where Player 1 optimizes its policy based on the historical trajectory of both itself and Player 2, and similarly for Player 2.

We explore three distinct competitive settings, all of which unfold over multiple rounds:

\textbf{Resource Exchange Scenario.} 
In this scenario, agents engage in a simulated environment where they exchange resources to maximize their individual utility. 

\textbf{Seller and Buyer Scenario.}  
This setting models economic interactions where one agent assumes the role of a seller and another the role of a buyer. The agents negotiate prices and terms to complete transactions, testing their ability to strategize under asymmetric setting. 

\textbf{Multi-Turn Ultimatum Game.}  
The Multi-Turn Ultimatum Game explores scenarios of fairness, cooperation, and negotiation over multiple rounds. One agent proposes a division of a resource, and the other agent decides whether to accept or reject it.  


\section{Experiments}
\subsection{Baseline}

We compare our \model{} against the following baselines:

\textbf{Single-Agent} utilizes a single language model to process input and generate responses.

\textbf{STaR}~\citep{zelikman2022star}, the Self-Taught Reasoner, focuses on enhancing the reasoning capabilities of a single agent by iteratively training it to improve its step-by-step reasoning through self-supervised fine-tuning. 

\textbf{Prompt Multi-Agent System (CoMM)}~\citep{chen2024comm} introduces a training-free, multi-agent collaborative framework where agents interact and share information to solve tasks collectively. 

\textbf{TextGrad}~\citep{yuksekgonul2024textgrad} optimizes prompts for each agent in a multi-agent system by backpropagating natural language feedback through each interaction.



\subsection{Setup and Datasets}
\textbf{Backbone Model.}
For a fair comparison, we use gpt-3.5-turbo-0125 and gpt-4o-mini-2024-07-18 as the backbone model, and set the temperature to 0 in all our experiments. We use OpenAI's Fine-tuning API for supervised fine-tuning.

\textbf{College Physics/Chemistry.}
These two datasets are constructed by combining questions from Massive Multitask Language Understanding (MMLU)~\citep{hendrycks2020measuring}, Graduate-Level Google-Proof Q\&A (GPQA) ~\citep{rein2023gpqa}, and Theorem-Driven Question Answering (TheoremQA)~\citep{chen2023theoremqa}. It focuses on college-level physics problems, which remain difficult and demonstrate room for improvement in performance with large language models.
We split the dataset into training and test sets, with the detailed data distribution provided in Appendix \ref{ap:dataset}.

\textbf{PubMedQA.}
This is a biomedical question-answering dataset comprising 1000 open-domain questions ~\citep{jin2019pubmedqa}, each paired with context from PubMed abstracts and corresponding answers. It focuses on research-driven queries, requiring domain-specific understanding and reasoning over scientific texts. We follow the original split of the dataset for training (500) and testing (500) sets.

\subsection{Experimental Result of Problem Solving Setting }
\subsubsection{Main Result} 
Table~\ref{tab:problem-solving-main} presents a performance comparison of various models and methods under the Problem Solving Setting. We observe that the prompted Multi-Agent System (COMM) generally improves performance, as agent collaboration enhances the ability to solve complex problems. STaR outperforms the base Single-Agent, indicating that fine-tuning contributes to improved performance. For smaller and weaker models, and in scenarios with long context lengths such as PubMedQA, TextGrad faces significant challenges in instruction-following during optimization. TextGrad (GPT-3.5-turbo) could not be applied to PubMedQA as its optimizer failed to parse instructions due to the model's limited capability and the excessive context length of the problem. Similarly, TextGrad (GPT-4o-mini) struggles to generate answers in the required format, requiring manual extraction of answers. Our proposed method, \model, consistently outperforms across all tasks. By decomposing tasks into manageable sub-tasks assigned to agents and, crucially, fine-tuning each agent to specialize in its designated task, \model{} maximizes the effectiveness of collaboration, ensuring a more coordinated and efficient overall performance.


\subsubsection{Ablation Experiments}
\begin{table}[t]
\caption{Ablation results on PubMedQA.}
\centering
\small
\label{tab:problem-solving-ablation}
\begin{tabular}{l|l|c}
\toprule
Model                          & method                & PubMed  \\
\midrule
\multirow{6}{*}{GPT-3.5-turbo} & \model{}                 & 74.20 \\
                               & \model{} + Base          & 72.00 \\
                               & Base + \model{}          & 73.20 \\
                               & FT on One Base LLM       & 70.40 \\
                               & \model{} w/o Aug.        & 73.40 \\
                               & Additional FT Itr        & 75.00      \\
\midrule
\multirow{6}{*}{GPT-4o-mini}   & \model{}                 &73.40      \\
                               & \model{} + Base          & 72.80   \\
                               & Base + \model{}          & 71.60   \\
                               & FT on One Base LLM       & 72.00   \\
                               & \model{} w/o Aug.        & 72.20    \\
                               & Additional FT Itr        & 73.60        \\
\bottomrule                       
\end{tabular}
\end{table}
To evaluate the contributions of various components in \model{}, we conducted a series of ablation experiments.  Each experiment was designed to answer a key question about the effectiveness of the multi-agent system. All ablations were performed on representative tasks within the Problem Solving Setting (PubMedQA) to ensure consistency in evaluation as shown in Table~\ref{tab:problem-solving-ablation}.
% \james{I think the ablations can be organized as a set of questions. For example, is it better to fine-tune different LLM for different roles or finetune one LLM for all the roles? Then introduce our experiment and result. }

\textbf{Does mixing \model{} with a base agent degrade performance?} To understand the benefits of a jointly optimizing a collaborative multi-agent system, we first train all the agents together using \model{}. Then we replaced one \model{} agent with the original base agent---either \model{} Analyst $+$ base Solver or base Analyst $+$ \model{} Solver. This substitution hurts performance, demonstrating benefits from joint multi-agent optimization compared to optimizing a single agent. 

\textbf{Should we fine-tune different LLMs for different roles, or optimize one LLM for all roles?}  
We explored whether a single LLM fine-tuned on the combined training data of multiple roles could match the performance of separate role-specific models. 
The results showed a notable performance decline, highlighting that different roles require specialized adaptation and that a shared model struggles to effectively generalize across distinct agent functions.

\textbf{How useful is experience augmentation?}  
To assess the impact of experience augmentation, we removed the augmentation module while keeping the rest of the pipeline unchanged. Data augmentation introduces more diverse and challenging experiences as training data, enhancing the model's capability; therefore, omitting the augmentation module could negatively impact performance.

\textbf{Does additional fine-tuning improve performance?
}  

We investigated whether increasing the number of fine-tuning iterations leads to further performance gains. Each iteration follows the full optimization pipeline illustrated in Figure~\ref{fig:pipeline}, the previously fine-tuned \model{} is used to generate a new experience library, which is then used to further fine-tune the base model.
As expected, an additional iteration yielded marginal performance gains, suggesting that the model can benefit from extended training.


\begin{table*}[t]
\caption{Evaluation results of the proposed method and baselines on accuracy(\%).}
\centering
\small
\label{tab:Actor-critic}
\begin{tabular}{l|cc|cc}
\toprule
\textbf{Model }         &\multicolumn{2}{c|}{GPT-3.5-Turbo}  &\multicolumn{2}{c}{GPT-4o-mini}   \\
\midrule
\textbf{Method  }       & TP Accuracy& Overall Accuracy& TP Accuracy& Overall Accuracy  \\
\midrule
Self-Correct   & 11.80          &     16.40       & 24.60         & 28.80          \\
Prompt         & 18.40          &     47.60       & 51.60         & 58.20      \\
\model{}       & \textbf{35.00} &\textbf{ 50.60}  &\textbf{59.80 }&\textbf{66.80 }     \\
\midrule
\multicolumn{5}{c}{------------------------\qquad \textbf{Ablation Study}\qquad------------------------} \\
\model{} + BASE Actor Agent&   34.20  &  49.00   & 49.60         & 54.40  \\
\model{} + BASE Judgment Agent&  20.20 &  40.20    & 53.00         &  59.40   \\
\model{} + BASE Critic Agent&   35.00   &   50.40  &   59.80       &  64.20  \\
FT on One Base LLM        &    33.80 &    43.60  &    56.00      & 59.60    \\

\bottomrule
\end{tabular}
\end{table*}

\subsection{Experimental Result of Actor-Critic Setting}

Table~\ref{tab:Actor-critic} presents a performance comparison of various models, methods, and ablations under the Actor-Critic Setting on PubMedQA. 
As mentioned in Section \ref{sec:Actor-Critic Setting}, the Actor Agent first generates a solution, which is then evaluated by the Judgment Agent to determine its correctness. For solutions deemed incorrect by the Judgment Agent, the Critic Agent analyzes the original solution and provides feedback without access to the correct answer. The Actor Agent then regenerates the solution based on this feedback.

A key challenge in this setting is the Judgment Agent's limited ability to  differentiate between correct and incorrect solutions leading to two potential issues: (1) correct solutions may be mistakenly judged as incorrect and potentially modified into incorrect ones during the feedback and regeneration stages; (2) incorrect solutions may be judged as correct, failing to receive the necessary corrections.
We report TP (True Positive) Accuracy as the ratio of solutions both correctly generated by the Actor and accurately validated by the Judgment Agent, while Overall Accuracy  measures the total correct solutions after regeneration, accounting for the combined contributions of all agents.

We evaluate our method against two representative baselines: (1) Self-Correct, where Actor-generated solutions are refined through direct feedback-guided regeneration, and (2) Prompt,  which exclusively employs prompting strategies to coordinate Actor-Judgment-Critic interactions without optimization mechanisms.
A critical limitation observed in the Self-Correct framework is its significantly lower TP accuracy. This issue arises from its feedback mechanism, which modifies all generated responses with high probability, potentially leading to erroneous modifications of the initially correct solution. This is a common issue with using out-of-the-box LLMs for self-correction with no specialized training~\citep{kumar2024training}.

Comparing GPT-3.5-Turbo and GPT-4o-mini, we also find that GPT-3.5-Turbo struggles more with misjudging correct answers as incorrect, leading to a severe drop in TP Accuracy. Our method, \model, achieves a notable improvement in TP Accuracy, highlighting the Judgment Agent's enhanced ability to assess whether a response requires modification. The overall higher accuracy underscores the effectiveness of \model's framework, where fine-tuning enhances each agent's task-specific capabilities, and the collaboration of Judgment, Critic, and Actor Agents ensures appropriate revision of incorrect responses while minimizing unnecessary changes to correct answers.

The ablation study further underscores the contribution of each agent in \model. Fine-tuning only a single base LLM leads to a performance drop, highlighting the necessity of specialized agent roles and joint optimization. Notably, replacing the Judgment Agent with a baseline version significantly reduces TP Accuracy, reinforcing its essential role in filtering correct responses before feedback is applied.


\subsection{Experimental Result of Competitive Settings}
% \wanjia{TODO: more explanation}
To analyze the effect of training in the competitive setting, we study the performance of agents in scenarios where one player initially had a higher probability of winning, referred to as the "winning player," while the other player was at a disadvantage, called the "losing player." In general, when \model{} took on the role of the winning player competing against a base agent, it demonstrated an increased win rate and payoff. Additionally, when \model{} played the role of the losing player, it experienced fewer losses. Similarly, for both GPT-3.5 and GPT-4o-mini when they compete with each other, \model-GPT-3.5 and \model-GPT-4o-mini both demonstrate improved performance.


\begin{figure}[htbp]
    \centering
    \includegraphics[width=\linewidth]{figure/resource_25_5.pdf}
    \caption{Resource Exchange Game: Player 1 (25Xs + 5Ys), Player 2 (5Xs + 25Ys). Win Rate in decisive games and Payoff in all games. We show Player 2 Win rate/payoff in all cells.}
    \label{fig:resource_25_5}
\end{figure}
\subsubsection{Resource Exchange} 
The win rates and average payoffs for the Resource Exchange game are presented in Figure \ref{fig:resource_25_5}. 
Overall, the agent going second tends to beat the first agent.  Furthermore, the fine-tuned \model{} demonstrates a significant improvement in both the win rate and payoff for the current player. To evaluate the generalization capability of our approach, we conducted additional experiments with models fine-tuned on games featuring Initial Resource configurations of 25Xs + 5Ys and 5Xs + 25Ys, and then tested them on games with different Initial Resource configurations (35Xs + 15Ys and 15Xs + 35Ys). As demonstrated in Figure \ref{fig:resource_35_15}, \model{} maintains notable improvements in the new Initial Resource configurations, effectively validating the generalizability of our proposed pipeline.



\subsubsection{Multi-Turn Ultimatum}

In this setting, Player 1 consistently dominates the game. Therefore, Figure \ref{fig:ultimate_100} presents the game outcomes from Player 1's perspective. As shown in the Figure \ref{fig:ultimate_100} , \model{} fine-tuned Player 1 effectively secure a higher share of the split.  Generalization experiments show that \model{} Player 1 trained in the Resource = 100 setting maintains utility gains in the new Resource = 1000 setting (Figure \ref{fig:ultimate_1000}).
\begin{figure}[htbp]
    \centering
    \includegraphics[width=\linewidth]{figure/ultimate_bar_100.pdf}
    \caption{Player 1's payoff in the Ultimatum game with Initial Resource settings of 100. \model{} as Player 1 can effectively secure a higher share of the split.}
    \label{fig:ultimate_100}
\end{figure}







\subsubsection{Buyer-Seller}
% Figure~\ref{fig:buysell} summarizes the outcome for the game configuration where the Seller values the object at 40/30 (cost of production), and the Buyer values the object at 60/70(willingness to pay). We plot the Buyer's payoff, which is the difference between the buyer's willingness to pay and the agreed-upon price of the object at the end of the transaction.  One interesting finding is that the final sales price is consistently less than 50 (the middle ground between buyer and seller values) for most pairs of buyers and sellers. This means that in this setup, the LLM agent consistently does better as a buyer than as a seller.
In this setting, sellers are willing to sell when the price exceeds 40, while buyers are willing to buy when the price is below 60.  We plot the final selling price as shown in Figure~\ref{fig:buysell_40_60}.
Notably, it is consistently below 50 for most buyer-seller pairs, indicating that the LLM agent performs better as a buyer than as a seller. After fine-tuning, SIRIUS as a seller shows significant improvement, consistently selling at 50, resulting in a tie with the buyer. 
To test the generalization capability and ensure the seller is not overfitting to a price of 50, we adjusted the initial configuration to 30 and 70. Figure~\ref{fig:buysell_30_70} shows that the SIRIUS seller trained in the previous setup still demonstrates significant improvement.


\begin{figure}[h!]
    \centering
    \includegraphics[width=\linewidth]{figure/buysell_40_60.pdf}
    \caption{Final Selling Price for a Seller\&Buyer with object valuations of 40 and 60. A higher number means the seller gets a greater payoff.}
    \label{fig:buysell_40_60}
\end{figure}


