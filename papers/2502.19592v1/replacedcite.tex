\section{Related Works}
\label{section:related works}
\noindent \textbf{Real-time Neural Implicit Mapping}.
Neural implicit mapping methods based on NeRF utilize RGB or RGBD images to train neural networks for various scene prediction tasks, including color, volume density, occupancy, and signed distance.  
Examples include iSDF ____ for real-time signed distance field reconstruction, H2-Mapping ____ for efficient color and signed distance prediction on edge devices, and GO-Surf ____ for fast surface reconstruction using multi-resolution features.  
Some approaches, like iMAP ____ and NICE-SLAM ____, integrate camera pose tracking with scene representation, while Co-SLAM ____ further enhances reconstruction speed.  
However, these methods primarily focus on centralized training with a single agent.
In contrast, this paper builds upon Co-SLAM and introduces an asynchronous distributed training framework specifically designed for multi-robot systems.

\noindent\textbf{Multi-agent Mapping}. 
Existing multi-agent mapping methods involve transmitting either explicit local maps (e.g., 3D point clouds with image features ____) or raw sensor data to a central server for map alignment and fusion ____.  
Alternatively, methods like MAANS ____  employ a central server to fuse bird's-eye view images predicted by each agent.  
Recent approaches leverage neural implicit mapping to reduce communication costs by sharing compact neural networks instead of raw data ____.
However, existing neural implicit mapping methods require synchronous communication, hindering their applicability in real-world robotic scenarios where communication is often asynchronous.  
Bandwidth limitations, best-effort protocols (e.g., UDP), and obstacles can lead to delayed or lost updates.
Our proposed RAMEN method addresses this gap by enabling robust multi-agent neural implicit mapping under asynchronous communication.

\noindent \textbf{Distributed Neural Network Optimization}.
Distributed optimization of neural networks allows multiple agents to collaboratively train a model without relying on a central server ____.
Common approaches include distributed stochastic gradient descent (DSGD) ____, which combines local gradients with neighbors' parameters, and distributed stochastic gradient tracking (DSGT) ____, which improves convergence by estimating the global gradient.
Other methods, like the decentralized linearized alternating direction method (DLM) ____ and consensus alternating direction method of multipliers (C-ADMM) ____,  optimize local objectives while enforcing agreement among agents. 
However, these methods typically require synchronous communication, limiting their practicality. 
While techniques like partial barriers and bounded delay ____ allow for some degree of tolerance to communication disruptions by introducing waiting periods, they are primarily designed for distributed training with data servers and are not ideal for robotic tasks requiring continuous real-time updates.
RAMEN tackles communication challenges without relying on such barriers, enabling efficient and robust multi-robot collaborative learning.

\noindent \textbf{Uncertainty in Neural Implicit Maps.}
Quantifying uncertainty in neural implicit maps is crucial for tasks like next-best view selection, model refinement, and artifact removal.
Existing methods for estimating neural network parameter uncertainty often rely on computationally expensive techniques like deep ensembles ____ or variational Bayesian neural networks ____, which require significant modifications to the training pipeline.
While alternative approaches estimate spatial uncertainty by perturbing input coordinates ____ or network parameters ____, they do not directly address the uncertainty of each learnable parameter.
In contrast, RAMEN efficiently quantifies parameter uncertainty by combining a frequency-based heuristic ____ with a multi-resolution hash grid ____. 
This requires minimal extra computation and allows us to leverage uncertainty information for robust distributed mapping.


%=====================================