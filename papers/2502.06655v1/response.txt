\section{Related Works}
\subsection{LLMs Evaluation}
The rapid growth of large language models (LLMs) underscores the need for increasingly robust and fair evaluation methods.
Benchmarks offer an effective alternative for model evaluation. The research community has made significant strides in expanding the comprehensiveness of benchmarks **Radford et al., "Language Models Play DOTA"**, while also introducing more complex and challenging tasks to push the boundaries of model capabilities ____.

Complementing these evaluation benchmarks, our proposed Unbiased Evaluator introduces an evaluation protocol grounded in a causal perspective, offering a more comprehensive and unbiased assessment.



\subsection{Benchmark Contamination}
Recent research has attached great importance to contamination in LLMs. In particular, **Henderson et al., "Measuring Massive Multitask Learning"** knowledged that contamination poses significant challenges to the reliability and validity of LLM evaluations. Several research studies ____ developed various methods to detect data contamination.

Several works **Kaplan et al., "Scaling Laws for Neural Language Models"**____ have been proposed to address the contamination issue. Among these, protocols like ____ are specifically designed for mathematical tasks, while ____ focuses on long-context evaluation.
Among the research of Agents-as-an-Evaluator****Choi et al., "Pile: An 800GB Diverse Multitask Language Model"**, MPA **Raffel et al., "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"** proposes to involve paraphrasing and judging agents to automatically transform existing problems in benchmarks into new ones. CogMath____ decouple questions into several evaluation dimensions via an multi-agent system for evaluating LLM's mathematical
abilities.

Our proposed Unbiased Evaluator stands in contrast to these, as it is designed to be generalized for a wide range of tasks and ensures an unbiased evaluation.

\subsection{Evaluation Bias}
Recent studies have highlighted that LLM-as-a-Judge exhibit various types of biases across various tasks____, such as position bias, length bias, self-enhancement bias etc. These internal biases of LLMs may also affect LLM-as-a-judge, leading to unfair evaluation outcomes and subsequently impacting the development of LLMs.

Unlike prior research that mainly focuses on biases in LLM-as-a-Judge, this paper addresses the biases inherent in the generation of Agents-as-an-Evaluator, an area largely unexplored but critical to understanding the fairness and impact of LLMs in evaluative roles.