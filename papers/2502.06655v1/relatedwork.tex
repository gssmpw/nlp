\section{Related Works}
\subsection{LLMs Evaluation}
The rapid growth of large language models (LLMs) underscores the need for increasingly robust and fair evaluation methods.
Benchmarks offer an effective alternative for model evaluation. The research community has made significant strides in expanding the comprehensiveness of benchmarks \cite{wang2018glue, wang2019superglue, srivastava2022beyond, hendryckstest2021, liang2022holistic, white2024livebench}, while also introducing more complex and challenging tasks to push the boundaries of model capabilities \cite{wei2024measuring, he2024chinese, lightman2023let, aime2024a, amc2023b, he2024olympiadbench}. 

Complementing these evaluation benchmarks, our proposed Unbiased Evaluator introduces an evaluation protocol grounded in a causal perspective, offering a more comprehensive and unbiased assessment.



\subsection{Benchmark Contamination}
Recent research has attached great importance to contamination in LLMs. In particular, \cite{lee2021deduplicating,sainz2023nlp,mcintosh2024inadequacies,riddell2024quantifying,jiang2024does} knowledged that contamination poses significant challenges to the reliability and validity of LLM evaluations. Several research studies \cite{ni2024training} developed various methods to detect data contamination.

Several works \cite{fan2023nphardeval, lei2023s3eval, zhu2023dyval, zhu2024dynamic, liucogmath} have been proposed to address the contamination issue. Among these, protocols like \cite{fan2023nphardeval, zhu2023dyval, liucogmath} are specifically designed for mathematical tasks, while \cite{lei2023s3eval} focuses on long-context evaluation.
Among the research of Agents-as-an-Evaluator\cite{zhu2024dynamic,liucogmath}, MPA \cite{zhu2024dynamic} proposes to involve paraphrasing and judging agents to automatically transform existing problems in benchmarks into new ones. CogMath\cite{liucogmath} decouple questions into several evaluation dimensions via an multi-agent system for evaluating LLM's mathematical
abilities. 

Our proposed Unbiased Evaluator stands in contrast to these, as it is designed to be generalized for a wide range of tasks and ensures an unbiased evaluation.

\subsection{Evaluation Bias}
Recent studies have highlighted that LLM-as-a-Judge exhibit various types of biases across various tasks\cite{dai2024unifying,gallegos2024bias,chen2402humans,ye2024justice}, such as position bias, length bias, self-enhancement bias etc. These internal biases of LLMs may also affect LLM-as-a-judge, leading to unfair evaluation outcomes and subsequently impacting the development of LLMs. 

Unlike prior research that mainly focuses on biases in LLM-as-a-Judge, this paper addresses the biases inherent in the generation of Agents-as-an-Evaluator, an area largely unexplored but critical to understanding the fairness and impact of LLMs in evaluative roles.