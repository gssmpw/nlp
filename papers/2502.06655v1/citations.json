[
  {
    "index": 0,
    "papers": [
      {
        "key": "wang2018glue",
        "author": "Wang, Alex",
        "title": "Glue: A multi-task benchmark and analysis platform for natural language understanding"
      },
      {
        "key": "wang2019superglue",
        "author": "Wang, Alex and Pruksachatkun, Yada and Nangia, Nikita and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel",
        "title": "Superglue: A stickier benchmark for general-purpose language understanding systems"
      },
      {
        "key": "srivastava2022beyond",
        "author": "Srivastava, Aarohi and Rastogi, Abhinav and Rao, Abhishek and Shoeb, Abu Awal Md and Abid, Abubakar and Fisch, Adam and Brown, Adam R and Santoro, Adam and Gupta, Aditya and Garriga-Alonso, Adri{\\`a} and others",
        "title": "Beyond the imitation game: Quantifying and extrapolating the capabilities of language models"
      },
      {
        "key": "hendryckstest2021",
        "author": "Dan Hendrycks and Collin Burns and Steven Basart and Andy Zou and Mantas Mazeika and Dawn Song and Jacob Steinhardt",
        "title": "Measuring Massive Multitask Language Understanding"
      },
      {
        "key": "liang2022holistic",
        "author": "Liang, Percy and Bommasani, Rishi and Lee, Tony and Tsipras, Dimitris and Soylu, Dilara and Yasunaga, Michihiro and Zhang, Yian and Narayanan, Deepak and Wu, Yuhuai and Kumar, Ananya and others",
        "title": "Holistic evaluation of language models"
      },
      {
        "key": "white2024livebench",
        "author": "White, Colin and Dooley, Samuel and Roberts, Manley and Pal, Arka and Feuer, Ben and Jain, Siddhartha and Shwartz-Ziv, Ravid and Jain, Neel and Saifullah, Khalid and Naidu, Siddartha and others",
        "title": "Livebench: A challenging, contamination-free llm benchmark"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "wei2024measuring",
        "author": "Wei, Jason and Karina, Nguyen and Chung, Hyung Won and Jiao, Yunxin Joy and Papay, Spencer and Glaese, Amelia and Schulman, John and Fedus, William",
        "title": "Measuring short-form factuality in large language models"
      },
      {
        "key": "he2024chinese",
        "author": "He, Yancheng and Li, Shilong and Liu, Jiaheng and Tan, Yingshui and Wang, Weixun and Huang, Hui and Bu, Xingyuan and Guo, Hangyu and Hu, Chengwei and Zheng, Boren and others",
        "title": "Chinese simpleqa: A chinese factuality evaluation for large language models"
      },
      {
        "key": "lightman2023let",
        "author": "Lightman, Hunter and Kosaraju, Vineet and Burda, Yura and Edwards, Harri and Baker, Bowen and Lee, Teddy and Leike, Jan and Schulman, John and Sutskever, Ilya and Cobbe, Karl",
        "title": "Let's verify step by step"
      },
      {
        "key": "aime2024a",
        "author": "{AI-MO}",
        "title": "{Aime 2024}"
      },
      {
        "key": "amc2023b",
        "author": "{AI-MO}",
        "title": "{amc 2023}"
      },
      {
        "key": "he2024olympiadbench",
        "author": "He, Chaoqun and Luo, Renjie and Bai, Yuzhuo and Hu, Shengding and Thai, Zhen Leng and Shen, Junhao and Hu, Jinyi and Han, Xu and Huang, Yujie and Zhang, Yuxiang and others",
        "title": "Olympiadbench: A challenging benchmark for promoting agi with olympiad-level bilingual multimodal scientific problems"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "lee2021deduplicating",
        "author": "Lee, Katherine and Ippolito, Daphne and Nystrom, Andrew and Zhang, Chiyuan and Eck, Douglas and Callison-Burch, Chris and Carlini, Nicholas",
        "title": "Deduplicating training data makes language models better"
      },
      {
        "key": "sainz2023nlp",
        "author": "Sainz, Oscar and Campos, Jon Ander and Garc{\\'\\i}a-Ferrero, Iker and Etxaniz, Julen and de Lacalle, Oier Lopez and Agirre, Eneko",
        "title": "Nlp evaluation in trouble: On the need to measure llm data contamination for each benchmark"
      },
      {
        "key": "mcintosh2024inadequacies",
        "author": "McIntosh, Timothy R and Susnjak, Teo and Arachchilage, Nalin and Liu, Tong and Watters, Paul and Halgamuge, Malka N",
        "title": "Inadequacies of large language model benchmarks in the era of generative artificial intelligence"
      },
      {
        "key": "riddell2024quantifying",
        "author": "Riddell, Martin and Ni, Ansong and Cohan, Arman",
        "title": "Quantifying contamination in evaluating code generation capabilities of language models"
      },
      {
        "key": "jiang2024does",
        "author": "Jiang, Minhao and Liu, Ken and Zhong, Ming and Schaeffer, Rylan and Ouyang, Siru and Han, Jiawei and Koyejo, Sanmi",
        "title": "Does Data Contamination Make a Difference? Insights from Intentionally Contaminating Pre-training Data For Language Models"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "ni2024training",
        "author": "Ni, Shiwen and Kong, Xiangtao and Li, Chengming and Hu, Xiping and Xu, Ruifeng and Zhu, Jia and Yang, Min",
        "title": "Training on the Benchmark Is Not All You Need"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "fan2023nphardeval",
        "author": "Fan, Lizhou and Hua, Wenyue and Li, Lingyao and Ling, Haoyang and Zhang, Yongfeng",
        "title": "Nphardeval: Dynamic benchmark on reasoning ability of large language models via complexity classes"
      },
      {
        "key": "lei2023s3eval",
        "author": "Lei, Fangyu and Liu, Qian and Huang, Yiming and He, Shizhu and Zhao, Jun and Liu, Kang",
        "title": "S3eval: A synthetic, scalable, systematic evaluation suite for large language models"
      },
      {
        "key": "zhu2023dyval",
        "author": "Zhu, Kaijie and Chen, Jiaao and Wang, Jindong and Gong, Neil Zhenqiang and Yang, Diyi and Xie, Xing",
        "title": "Dyval: Graph-informed dynamic evaluation of large language models"
      },
      {
        "key": "zhu2024dynamic",
        "author": "Zhu, Kaijie and Wang, Jindong and Zhao, Qinlin and Xu, Ruochen and Xie, Xing",
        "title": "Dynamic Evaluation of Large Language Models by Meta Probing Agents"
      },
      {
        "key": "liucogmath",
        "author": "Liu, Jiayu and Huang, Zhenya and Dai, Wei and Cheng, Cheng and Wu, Jinze and Sha, Jing and Liu, Qi and Wang, Shijin and Chen, Enhong",
        "title": "CogMath: Evaluating LLMs' Authentic Mathematical Ability from a Cognitive Perspective"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "fan2023nphardeval",
        "author": "Fan, Lizhou and Hua, Wenyue and Li, Lingyao and Ling, Haoyang and Zhang, Yongfeng",
        "title": "Nphardeval: Dynamic benchmark on reasoning ability of large language models via complexity classes"
      },
      {
        "key": "zhu2023dyval",
        "author": "Zhu, Kaijie and Chen, Jiaao and Wang, Jindong and Gong, Neil Zhenqiang and Yang, Diyi and Xie, Xing",
        "title": "Dyval: Graph-informed dynamic evaluation of large language models"
      },
      {
        "key": "liucogmath",
        "author": "Liu, Jiayu and Huang, Zhenya and Dai, Wei and Cheng, Cheng and Wu, Jinze and Sha, Jing and Liu, Qi and Wang, Shijin and Chen, Enhong",
        "title": "CogMath: Evaluating LLMs' Authentic Mathematical Ability from a Cognitive Perspective"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "lei2023s3eval",
        "author": "Lei, Fangyu and Liu, Qian and Huang, Yiming and He, Shizhu and Zhao, Jun and Liu, Kang",
        "title": "S3eval: A synthetic, scalable, systematic evaluation suite for large language models"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "zhu2024dynamic",
        "author": "Zhu, Kaijie and Wang, Jindong and Zhao, Qinlin and Xu, Ruochen and Xie, Xing",
        "title": "Dynamic Evaluation of Large Language Models by Meta Probing Agents"
      },
      {
        "key": "liucogmath",
        "author": "Liu, Jiayu and Huang, Zhenya and Dai, Wei and Cheng, Cheng and Wu, Jinze and Sha, Jing and Liu, Qi and Wang, Shijin and Chen, Enhong",
        "title": "CogMath: Evaluating LLMs' Authentic Mathematical Ability from a Cognitive Perspective"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "zhu2024dynamic",
        "author": "Zhu, Kaijie and Wang, Jindong and Zhao, Qinlin and Xu, Ruochen and Xie, Xing",
        "title": "Dynamic Evaluation of Large Language Models by Meta Probing Agents"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "liucogmath",
        "author": "Liu, Jiayu and Huang, Zhenya and Dai, Wei and Cheng, Cheng and Wu, Jinze and Sha, Jing and Liu, Qi and Wang, Shijin and Chen, Enhong",
        "title": "CogMath: Evaluating LLMs' Authentic Mathematical Ability from a Cognitive Perspective"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "dai2024unifying",
        "author": "Dai, Sunhao and Xu, Chen and Xu, Shicheng and Pang, Liang and Dong, Zhenhua and Xu, Jun",
        "title": "Unifying Bias and Unfairness in Information Retrieval: A Survey of Challenges and Opportunities with Large Language Models"
      },
      {
        "key": "gallegos2024bias",
        "author": "Gallegos, Isabel O and Rossi, Ryan A and Barrow, Joe and Tanjim, Md Mehrab and Kim, Sungchul and Dernoncourt, Franck and Yu, Tong and Zhang, Ruiyi and Ahmed, Nesreen K",
        "title": "Bias and fairness in large language models: A survey"
      },
      {
        "key": "chen2402humans",
        "author": "Chen, Guiming Hardy and Chen, Shunian and Liu, Ziche and Jiang, Feng and Wang, Benyou",
        "title": "Humans or llms as the judge? a study on judgement biases, 2024"
      },
      {
        "key": "ye2024justice",
        "author": "Ye, Jiayi and Wang, Yanbo and Huang, Yue and Chen, Dongping and Zhang, Qihui and Moniz, Nuno and Gao, Tian and Geyer, Werner and Huang, Chao and Chen, Pin-Yu and others",
        "title": "Justice or prejudice? quantifying biases in llm-as-a-judge"
      }
    ]
  }
]