%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}
\usepackage[dvipsnames]{xcolor}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}

\usepackage{listings}

\renewcommand{\lstlistingname}{}
\lstset{
    basicstyle=\ttfamily\small, 
    breaklines=true,            
    frame=none,                 
    numbers=none,               
    xleftmargin=0em,           
    xrightmargin=0em,           
    captionpos=b,          
    numberbychapter=false, 
}
\usepackage{multirow}
\usepackage{multicol}
\usepackage{tabularx}

% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Unbiased Evaluation of Large Language Models from a Causal Perspective}

\begin{document}

\twocolumn[
\icmltitle{Unbiased Evaluation of Large Language Models from a Causal Perspective
}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Meilin Chen}{equal}
\icmlauthor{Jian Tian}{equal}
\icmlauthor{Liang Ma}{}
\icmlauthor{Di Xie}{}
\icmlauthor{Weijie Chen}{}
\icmlauthor{Jiang Zhu}{}
\end{icmlauthorlist}

\centering
\vspace{0.6em}
Hikvision Research Institute

% \icmlaffiliation{yyy}{Department of XXX, University of YYY, Location, Country}
% \icmlaffiliation{comp}{Company Name, Location, Country}
% \icmlaffiliation{hik}{Hikvision Research Institute}

\icmlcorrespondingauthor{Meilin Chen}{merlinarer@gmail.com}
\icmlcorrespondingauthor{Liang Ma}{maliang6@hikvision.com}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the ``keywords'' metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
% \printAffiliationsAndNotice{\icmlEqualContribution}
% otherwise use the standard text.
\begingroup
\renewcommand{\thefootnote}{}
\footnotetext{\icmlEqualContribution. Correspondence to: Meilin Chen (merlinarer@gmail.com), Liang Ma (maliang6@hikvision.com).}
\endgroup

\begin{abstract}
Benchmark contamination has become a significant concern in the LLM evaluation community. Previous Agents-as-an-Evaluator address this issue by involving agents in the generation of questions. Despite their success, the biases in Agents-as-an-Evaluator methods remain largely unexplored. 
In this paper, we present a theoretical formulation of evaluation bias, providing valuable insights into designing unbiased evaluation protocols. Furthermore, we identify two type of bias in Agents-as-an-Evaluator through carefully designed probing tasks on a minimal Agents-as-an-Evaluator setup. To address these issues, we propose the Unbiased Evaluator, an evaluation protocol that delivers a more comprehensive, unbiased, and interpretable assessment of LLMs.
Extensive experiments reveal significant room for improvement in current LLMs. Additionally, we demonstrate that the Unbiased Evaluator not only offers strong evidence of benchmark contamination but also provides interpretable evaluation results.
\end{abstract}

\vspace{-2.2em}

\section{Introduction}
\label{Introduction}

Recently, proprietary models such as GPT-4 \cite{achiam2023gpt}, Claude\cite{TheC3}, Gemini\cite{team2023gemini}, and open-source ones, such as Llama\cite{touvron2023llama}, Mistral\cite{jiang2023mistral}, Qwen\cite{bai2023qwen}, Yi\cite{ai2024yi} have demonstrated remarkable capabilities in natural language processing tasks and beyond. As the capabilities of community models continue to grow, the importance of robust and fair model evaluation becomes increasingly critical. 
The community has made great efforts to evaluate model performance by expanding the comprehensiveness of benchmarks\cite{wang2018glue,wang2019superglue,srivastava2022beyond,hendryckstest2021, liang2022holistic,white2024livebench}, or by introducing more complex and challenging tasks to push the boundaries of model capabilities\cite{wei2024measuring,he2024chinese,lightman2023let,aime2024a,amc2023b,he2024olympiadbench}.

Despite their success, public benchmarks, most widely used to assess and compare model performance, are particularly vulnerable to contamination issues\cite{lovin2023gpt,bender2021dangers,kocon2023chatgpt,li2023open,zhou2023don,ni2024training}, which are increasingly inevitable due to the scale of training data used in modern models.
Recently, agent-based evaluation have been proposed to address contamination issue\cite{zhu2024dynamic,liucogmath}. Among them, MPA \cite{zhu2024dynamic} proposes to involve probing and judging agents to automatically transform existing problems in benchmarks into new ones. CogMath\cite{liucogmath} decouple questions into several evaluation dimensions via an multi-agent system for evaluating LLM's mathematical abilities.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.50\textwidth]{figs/fig_comp.pdf}
    \vspace{-2em}
    \caption{(a) Agents-as-an-Evaluator suffers from data and model bias. (b) Our proposed Unbiased Evaluator dynamically evaluate the LLMs with designed \emph{\textbf{B}}ags \emph{\textbf{O}}f \emph{\textbf{A}}tomic In\emph{\textbf{T}}erventions (\textbf{BOAT}).}
    \label{fig_comp}
    \vspace{-1em}
\end{figure}



\begin{figure*}[t]
    \centering
    \includegraphics[width=0.49\textwidth]{figs/fig_model_bias_1.pdf}
    \hfill
    \includegraphics[width=0.49\textwidth]{figs/fig_model_bias_2.pdf}
    \vspace{-0.8em}
    \caption{Model bias visualizations. Left: $\mathcal{R}_{OC}$ vs. Strength on ARC-C and MMLU datasets. Right: $\mathcal{R}_{UC}$ vs. Strength on ARC-C and MMLU datasets. The point where strength equal zero represents the original datasets. Mistral, Yi, Llama, Qwen represents Mistral-Large-2411, Yi1.5-34B-Chat, Llama3.1-70B-Instruct and Qwen2.5-72B-Instruct, respectively. For Agents-as-an-Evaluator, we observe a significant increase in $\mathcal{R}_{OC}$ with growing strength, while $\mathcal{R}_{UC}$ remains relatively stable, indicating the existence of model bias. Compared with Agents-as-an-Evaluator, our Unbiased Evaluator remains relatively stable on both $\mathcal{R}_{OC}$ and $\mathcal{R}_{UC}$.}
    \label{fig_model_bias}
     \vspace{-1em}
\end{figure*}


We termed this paradigm as \emph{\textbf{Agents-as-an-Evaluator}}. 
Different from previous LLM-as-a-Judge \cite{zheng2023judging}, which operates on the evaluation side by solely determining whether something falls within the scope of a given rule, Agents-as-an-Evaluator extends beyond assessment to the generation side, where LLMs actively contribute to the design of the very questions involved in the task.
Considering that prior works have revealed that LLM-as-a-Judge posses certain biases\cite{blodgett2020language,ahn2022knowledge,ferrara2023should,gallegos2024bias}, 
compared to LLM-as-a-Judge, the critical step of question generation in Agents-as-an-Evaluator may introduce a greater potential for bias. 
Therefore, an important question rises:

\textbf{\emph{To what extent are Agents-as-an-Evaluator biased and is there a simple, unbiased alternative for benchmark contamination? }}

In this paper, we begin with a theoretical formulation of evaluation bias, followed by an in-depth analysis of the potential bias in designing evaluation protocols. Our results demonstrate that evaluation bias can be decomposed into three components: original, independent and related terms. 
Therefore, the key takeaway for future research in designing evaluation protocols is that any newly introduced biases should ideally mitigate and, more importantly, counteract existing biases in the original benchmark.

To conduct bias analysis, we design a minimum Agents-as-an-Evaluator, i.e. an LLM is tasked to rephrase the questions without changing their meaning. Build upon this, together with our designed bias probing task, we empirically reveal that Agents-as-an-Evaluator exhibit two biases: data bias and model bias. As shown in Fig.~\ref{fig_comp} (a), data bias emerges from accuracy imbalance across different domains during generation, while model bias stems from inherent unfairness during evaluation. 



To mitigate the bias in existing methods, we propose the \emph{\textbf{Unbiased Evaluator}}, as shown in Fig.~\ref{fig_comp} (b), an evaluation protocol grounded in a causal perspective. Drawing inspiration from interventions in causal inference, where AI systems are tasked with responding to manipulated pairs to understand how altering one factor ( ``intervention'') impacts other variables in a complex system. Specifically, the evaluation process is formulated as a causal diagram, where LLMs reason over input variables to arrive at the final answer. Taking advantage of this, we design \emph{\textbf{B}}ags \emph{\textbf{O}}f \emph{\textbf{A}}tomic In\emph{\textbf{T}}erventions (\textbf{BOAT}) to dynamically evaluate the LLMs. By combining these interventions, the Unbiased Evaluator provides a more comprehensive, unbiased and interpretable assessment.
 
In summary, our contributions are three-fold: 
(1) A theoretical formulation of evaluation bias, offering valuable findings for the importance of minimizing the relative term when designing evaluation protocols.
(2) The first comprehensive bias analysis for Agents-as-an-Evaluator, revealing data and model bias which undermine the reliability and trustworthiness of Agents-as-an-Evaluator.
(3) An unbiased evaluation protocol, Unbiased Evaluator,  provides a more comprehensive, unbiased and interpretable assessment for benchmark contamination.


\begin{table*}[t]
\centering
\vspace{-0.8em}
\caption{Data bias. The Pearson and Kendall correlation  between domain acc. of MMLU and $\mathcal{R}_{CE}$ across four LLMs.}
\begin{tabularx}{0.9\textwidth}{X|c|c|c|c|c|c|c|c}
\toprule
\multirow{3}{*}{Models} & \multicolumn{4}{c|}{Agents-as-an-Evaluator 
} & \multicolumn{4}{c}{Ours} \\ 
\cmidrule{2-9}
 & \multicolumn{2}{c|}{Kendall} & \multicolumn{2}{c}{Pearson} &\multicolumn{2}{|c|}{Kendall} & \multicolumn{2}{c}{Pearson} \\ 
\cmidrule{2-9}
 & $\tau$ & $p$-value & $c$ & $p$-value  & $\tau$ & $p$-value & $c$ & $p$-value \\ 
\cmidrule{1-9}
Mistral-Large-2411 & -0.38 & 2.73$\times 10^{-5}$ & -0.47 & 2.18$\times 10^{-4}$ & 0.02 & 0.76 & 0.07 & 0.59 \\ 
\cmidrule{1-9}
Yi1.5-34B-Chat & -0.17 & 5.86$\times 10^{-3}$ & -0.38 & 3.52 $\times 10^{-3}$ & 0.10 & 0.24 & 0.16 & 0.20 \\ 
\cmidrule{1-9}
Llama3.1-70B-Instruct & -0.39 & 1.81$\times 10^{-5}$ & -0.45 & 4.86$\times 10^{-4}$ & 0.14 & 0.10 & 0.15 & 0.05 \\ 
\cmidrule{1-9}
Qwen2.5-72B-Instruct & -0.25 & 5.51$\times10^{-3}$  & -0.41 & 1.44$\times 10^{-3}$ & 0.07 & 0.41 & 0.10 & 0.12 \\ 
\bottomrule
\end{tabularx}
\vspace{-1.5em}
\label{table_data_bias}
\end{table*}




\section{Related Works}

\subsection{LLMs Evaluation}
The rapid growth of large language models (LLMs) underscores the need for increasingly robust and fair evaluation methods.
Benchmarks offer an effective alternative for model evaluation. The research community has made significant strides in expanding the comprehensiveness of benchmarks \cite{wang2018glue, wang2019superglue, srivastava2022beyond, hendryckstest2021, liang2022holistic, white2024livebench}, while also introducing more complex and challenging tasks to push the boundaries of model capabilities \cite{wei2024measuring, he2024chinese, lightman2023let, aime2024a, amc2023b, he2024olympiadbench}. 

Complementing these evaluation benchmarks, our proposed Unbiased Evaluator introduces an evaluation protocol grounded in a causal perspective, offering a more comprehensive and unbiased assessment.



\subsection{Benchmark Contamination}
Recent research has attached great importance to contamination in LLMs. In particular, \cite{lee2021deduplicating,sainz2023nlp,mcintosh2024inadequacies,riddell2024quantifying,jiang2024does} knowledged that contamination poses significant challenges to the reliability and validity of LLM evaluations. Several research studies \cite{ni2024training} developed various methods to detect data contamination.

Several works \cite{fan2023nphardeval, lei2023s3eval, zhu2023dyval, zhu2024dynamic, liucogmath} have been proposed to address the contamination issue. Among these, protocols like \cite{fan2023nphardeval, zhu2023dyval, liucogmath} are specifically designed for mathematical tasks, while \cite{lei2023s3eval} focuses on long-context evaluation.
Among the research of Agents-as-an-Evaluator\cite{zhu2024dynamic,liucogmath}, MPA \cite{zhu2024dynamic} proposes to involve paraphrasing and judging agents to automatically transform existing problems in benchmarks into new ones. CogMath\cite{liucogmath} decouple questions into several evaluation dimensions via an multi-agent system for evaluating LLM's mathematical
abilities. 

Our proposed Unbiased Evaluator stands in contrast to these, as it is designed to be generalized for a wide range of tasks and ensures an unbiased evaluation.

\subsection{Evaluation Bias}
Recent studies have highlighted that LLM-as-a-Judge exhibit various types of biases across various tasks\cite{dai2024unifying,gallegos2024bias,chen2402humans,ye2024justice}, such as position bias, length bias, self-enhancement bias etc. These internal biases of LLMs may also affect LLM-as-a-judge, leading to unfair evaluation outcomes and subsequently impacting the development of LLMs. 

Unlike prior research that mainly focuses on biases in LLM-as-a-Judge, this paper addresses the biases inherent in the generation of Agents-as-an-Evaluator, an area largely unexplored but critical to understanding the fairness and impact of LLMs in evaluative roles.

\section{Bias Analysis}
\subsection{Theoretical Analysis}


\textbf{Formulation of Evaluation Bias.} 
\label{definition_3_1}
Consider in an LLM evaluation, where the true capability of a model is parameterized by \( \phi \), which is a fixed but unknown quantity we aim to estimate. 
 Given evaluation data \( \mathcal{X} = \{x_1, x_2, \dots, x_n\} \), the estimator \( \hat{\phi} = \mathcal{E}(\mathcal{X}) \) is a function of the data and provides an approximation of \( \phi \). 
 Then, the difference between expectation of \( \hat{\phi} \) and \( \phi \) can be defined as evaluation bias:
\begin{align}
\vspace{-2em}
\epsilon(\hat{\phi}) = \mathbb{E}[\hat{\phi}] - \phi
\vspace{-1em}
\end{align}
where \( \mathbb{E}[\hat{\phi}] \) is the expected value of the estimator over \( \mathcal{X} \). 
A bias greater than 0 indicates that the estimator overestimates \( \phi \), often due to contaminated data. A bias of 0 means the estimator is unbiased, providing a perfect estimate. A bias less than 0 suggests the estimator underestimates \( \phi \), which can occur when a model is evaluated on a limited dataset that fails to capture the task's complexity.

\label{proposition_3_1}
 \textbf{Proposition 3.1.} (\emph{Proof in Appendix~\ref{proof_3_1}}) Given a new designed evaluation protocol, which transforms original benchmark $D$ into $D'$. Then, the strength of bias with the new evaluation protocol can be decomposed into three terms: original, related and independent term.
\begin{align}
\mathbb{E}[\epsilon(\hat{\phi}_{D'})^2] &= \underbrace{\mathbb{E}[\epsilon(\hat{\phi}_D)^2]}_{\texttt{original}} + \underbrace{2\text{Cov} (\epsilon(\hat{\phi}_D), \Delta)}_{\texttt{related}} \nonumber \\
&+ \underbrace{2\mathbb{E}[\epsilon(\hat{\phi}_D)] \mathbb{E}[\Delta] + \mathbb{E}[\Delta^2]}_{\texttt{independent}}
\vspace{-3em}
\end{align}
where, $ \Delta $ is the delta bias which arises from the introduction of new  evaluation protocol.
\vspace{-1em}
\begin{itemize}
\item $\mathbb{E}[\epsilon(\hat{\phi}_D)^2]$ is an original term that is associated with the original bias existing in the original benchmark $D$. 
\item $ 2\text{Cov}(\epsilon(\hat{\phi}_D), \Delta) $ is a related term which pertains to biases that newly introduced biases are correlated with the pre-existing biases in the original benchmark.  
\item $ 2\mathbb{E}[\epsilon(\hat{\phi}_D)] \mathbb{E}[\Delta] + \mathbb{E}[\Delta^2] $  is an independent term, stemming from biases inherent to the methodology itself. 
\end{itemize}

Proposition 3.1 offers a pivotal perspective for future research in designing evaluation protocols that any newly introduced biases should ideally mitigate and, more importantly, counteract existing biases in the original benchmark.

\begin{figure*}[ht]
    \centering
    \includegraphics[width=0.98 \textwidth]{figs/fig_main.pdf}
    \vspace{-1em}
    \caption{(a) Traditional evaluation methods rely on static and fixed variables, suffering from contamination issues. (b) Unbiased Evaluator enhance the evaluation process by augmenting these variables through carefully designed \emph{\textbf{B}}ags \emph{\textbf{O}}f \emph{\textbf{A}}tomic In\emph{\textbf{T}}erventions (\textbf{BOAT}). (c) An example of BOAT. \underline{Underlined contents} are derived from multiple interventions.}
    \label{fig_pipline}
    \vspace{-1em}
\end{figure*}

\subsection{Bias in Agents-as-an-Evaluator}
In addition to our theoretical analysis, we conduct extensive experiments in this section to demonstrate that Agents-as-an-Evaluator exhibit two types of bias: data bias and model bias. 
\vspace{-1em}
\begin{itemize}
\item Data bias arises from an imbalance in accuracy across different domains during generation. For example, in tasks involving diverse domains, such as various subjects in MMLU, LLMs tend to excel in domains where they already perform well while struggling significantly in domains where their performance is weaker.
\vspace{-1.5em}
\item Model bias originates from inherent unfairness during evaluation. an LLM tends to generate content that aligns more with its implicit strengths, giving it an unfair advantage.
\end{itemize}
\vspace{-1em}

\textbf{Discussion.} Previous works\cite{chen2402humans,ye2024justice} have revealed the self-enhanced bias in LLM-as-a-Judge, i.e. LLM judges may favor the answers generated by themselves. Model bias, however, focus more on the bias of generation side in Agents-as-an-Evaluator, which still remain unexplored.


\subsection{Probing Task}
\label{sec_probing_task}
In this section, we design a probing task to detect the potential bias in Agents-as-an-Evaluator.

\textbf{Minimum Agents-as-an-Evaluator.} To perform bias analysis, following \cite{zhu2024dynamic}, we design a minimal Agents-as-an-Evaluator framework. Specifically, given an original benchmark  $\mathcal{D}$, an LLM is tasked with rephrasing the questions in $\mathcal{D}$ without altering their meaning. Formally, this process generates a new benchmark, $\mathcal{D'} = LLM(\mathcal{D})$.

\textbf{Task Design.} Given an original benchmark $\mathcal{D} $ on with $m$ evaluation problems $ \{x_1, x_2, \dots, x_m\}$, and $n$ cutting edged LLMs \( \mathcal{M} = \{\theta_1, \theta_2, \dots, \theta_n\} \), $\mathcal{D'}_i $ denotes the rephrased benchmark using $i$-th LLM $\theta_i$, rephrased $j$-th evaluation problem $x'_{ij}$ in $\mathcal{D'}_i $ is rephrased from $d_j$ with probability $p$, i.e.
\begin{align}
    \mathcal{D'}_i &= \{x'_{i1}, x'_{i2}, \dots, x'_{im}\} \nonumber \\
    x'_{ij} &= 
    \begin{cases}
    \theta_i(x_j) & \texttt{random} \leq p \\
    x_j & \texttt{random} > p
    \end{cases}   &
    \forall x'_{ij} \in \mathcal{D'}_i 
\end{align}

Note that during probing task, the ground truth label of each problem remains unchanged. With $\mathcal{D'}_i $, we perform LLM-as-a-Judge using all models in $\mathcal{M}$. Specifically, we ask each model to assess its confidence that it thinks the ground truth label is still the right answer for the rephrased question, i.e.

\begin{align}
    \mathcal{S}_{ijk} &= \theta_k(x'_{ij})  \nonumber \\
    \forall i \in [1,n]; \forall &j \in [1,m];  \forall k \in [1,n] 
\end{align}
where $\mathcal{S}_{ijk}$ is the confidence score assessed by $\theta_k$ for question $x'_{ij}$. The score $\mathcal{S}_{ijk}$ lies within the range \([1,10]\), where a higher score indicates that the model has greater confidence in the ground truth label being the correct answer, while a lower score suggests stronger confidence that the ground truth label is not the correct answer. Detailed prompt is presented in Appendix ~\ref{appendix_prompt}.

\textbf{Metric design.} 
To quantify bias during LLMs evaluation, we draw inspiration from group consensus in human society, where collective opinions are often regarded as more reliable and fair compared to individual perspectives. Building on this idea, we propose three metrics: $\mathcal{R}_{CE}$(Consensus-Error Rate), $\mathcal{R}_{OC}$(Over-Confidence Rate) and $\mathcal{R}_{UC}$(Under-Confidence Rate), incorporating both self and collective judgment.

\vspace{-2em}

\begin{align}
&\mathcal{R}_{CE}=\frac{1}{m} \sum_{j=1}^{m} [\underbrace{(10-\mathcal{S}_{iji}) * (10-\hat{\mathcal{S}}_{ij})}_{\text{\shortstack{\texttt{consensus}\\\texttt{intensity}}}}]
*\underbrace{\prod_{k=1}^n \textit{sgn}({\mathcal{S}_{ijk}<tu})}_{\texttt{all consensus}}
\end{align}

\vspace{-1em}

where $\textit{sgn}$ is a sign function. $tu$ is a upper threshold of ``NO''. In this paper, following prompt in probing task, $tu$ is set to 5. $\hat{\mathcal{S}}_{ij} $ is the averaged score among the other models:  $\hat{\mathcal{S}}_{ij} = \frac{1}{n-1}\sum_{k=1 ~ k!=i}^n (\mathcal{S}_{ijk})$.

\vspace{-2em}

\begin{align}
&\mathcal{R}_{OC}=\frac{1}{m} \sum_{j=1}^{m}[\underbrace{\mathcal{S}_{iji} * (10 - \hat{\mathcal{S}}_{ij})}_{\texttt{conflict intensity}}] \nonumber \\
&*\underbrace{\textit{sgn}\left[\mathcal{S}_{iji}>tl\right]}_{\texttt{self confidence}} * \underbrace{\prod_{k=1 ~ k!=i}^n \textit{sgn}({\mathcal{S}_{ijk}<tu})}_{\texttt{collective consensus}}
\end{align}

\vspace{-2em}

\begin{align}
&\mathcal{R}_{UC}=\frac{1}{m} \sum_{j=1}^{m}[\underbrace{ (10 - \mathcal{S}_{iji}) * \hat{\mathcal{S}}_{ij}}_{\texttt{conflict intensity}}] \nonumber \\
&*\underbrace{\textit{sgn}\left[\mathcal{S}_{iji}<tu\right]}_{\texttt{self confidence}} * \underbrace{\prod_{k=1 ~ k!=i}^n \textit{sgn}({\mathcal{S}_{ijk}>tl})}_{\texttt{collective consensus}}
\end{align}

\vspace{-1em}

where $tl$ is lower threshold of ``Yes'', and $tl$ is set to 6.


The Consensus-Error Rate \(\mathcal{R}_{CE}\) quantifies the overall consensus in predicting ``NO'', with a weight that increases as the confidence values \(\mathcal{S}_{iji}\) and \(\hat{\mathcal{S}}_{ij}\) decrease, reflecting higher confidence in predicting ``NO''. 
On the other hand, the Over-Confidence Rate \(\mathcal{R}_{OC}\) measures the scenario where a LLM exhibits high self-confidence in predicting ``Yes'', while the collective consensus predicts ``NO''. 
In contrast, the Under-Confidence Rate \(\mathcal{R}_{UC}\) quantifies the scenario where a LLM exhibits high confidence in predicting ``NO'', while the collective consensus predicts ``Yes''.


\textbf{Data Bias.} We treat each subject in MMLU as a  domain and calculate the Pearson and Kendall correlation  between domain accuracy and $\mathcal{R}_{CE}$ across four LLMs. The results, presented in Table~\ref{table_data_bias}, reveal a relatively strong negative correlation with small $p$-values. This indicates that LLMs tend to perform better in domains with lower $\mathcal{R}_{CE}$ values while facing greater challenges in domains where their performance is weaker.


\begingroup
\renewcommand{\arraystretch}{0.3}
\begin{table*}[t]
\caption{Bags of Atomic Interventions (BOAT). This is a demo case for a clear demonstration. For further details, please refer to Sec.~\ref{sec_implementation}.}
\centering
\resizebox{0.9\textwidth}{!}{
\begin{tabularx}{\textwidth}{c|c|c|c|X|c}
\toprule
\multirow{2}{*}{$\mathcal{T}$} & \multirow{2}{*}{ positions} & \multirow{2}{*}{interventions} & \multicolumn{3}{c}{examples} \\
\cmidrule{4-6}
 & & & original & \centering intervened & label \\
\midrule
\multirow{6}{*}{MCQ} & instruction & Distractor Hint &  \multirow{6}{*}{\parbox{3cm}{\textcolor{RedOrange}{\texttt{\small{Here is a multiple choice question, answer A or B.}}} \newline \textcolor{Cyan}{\texttt{\small{Is 9.8 bigger than 9.11?}}} \newline \textcolor{ForestGreen}{\texttt{\small{A: True  B: False}}}}} & \textcolor{RedOrange}{\texttt{\small{Here is a multiple choice question, answer A or B, if there is no answer, reply N.}}} & A \\
\cmidrule{2-3}
\cmidrule{5-6}
& question & Distractor Question &  &  \textcolor{Cyan}{\texttt{\small{Is 9.8 bigger than 9.11? 1+1=?}}}  & A \\
\cmidrule{2-3}
\cmidrule{5-6}
& \multirow{3}{*}{answer} & Answer Removal & & \textcolor{ForestGreen}{\texttt{\small{A: 2  B: False}}} & N\\
& & Option Shuffling &  & \textcolor{ForestGreen}{\texttt{\small{A: False B: True }}} & B \\
& & Label Replacement &  & \textcolor{ForestGreen}{\texttt{\small{\uppercase\expandafter{\romannumeral1}: True \uppercase\expandafter{\romannumeral2}: False }}} & \uppercase\expandafter{\romannumeral1} \\
\cmidrule{2-3}
\cmidrule{5-6}
&  overall & Binary Transformation &  & \textcolor{RedOrange}{\texttt{\small{Here is a multiple choice question, judge True (T) or False (F) for each choice.}}} & TF \\
\midrule
\multirow{2}{*}{Math} & overall & Question Jitter & \multirow{2}{*}{\parbox{3cm}{\textcolor{RedOrange}{\texttt{\small{Answer this question.}}} \newline \textcolor{Cyan}{\texttt{\small{1+1=?}}} }} & \texttt{\small{\textcolor{RedOrange}{Judge True (T) or False (F) for given possible answer.} \textcolor{Cyan}{1+2=2?}}} & F \\
\cmidrule{2-3}
\cmidrule{5-6}
 & overall & Answer Jitter & & \texttt{\small{\textcolor{RedOrange}{Here is a multiple choice question.} \textcolor{Cyan}{1+1=?} \textcolor{ForestGreen}{A: 0 B: 1 C: 2 D: 3}}} & C\\
\toprule
\end{tabularx}
}
\label{table_interventions}
\vspace{-1em}
\end{table*}
\endgroup


\textbf{Model Bias.}
In Fig.~\ref{fig_model_bias}, we visualize how $\mathcal{R}_{OC}$ and $\mathcal{R}_{UC}$ evolve as the strength parameter $p$ in Eq.~(4) increases. We observe a significant increase in $\mathcal{R}_{OC}$ with growing strength, particularly for models like Llama3.1-70b-Instruct and Yi-34B-Chat. In contrast, $\mathcal{R}_{UC}$ remains relatively stable. This suggests that transitioning from the original evaluation protocol ($p=0$) to the Agents-as-Evaluator ($p=1$) causes LLMs to generate content that aligns closer with their implicit strengths ($\mathcal{R}_{OC}$), rather than diverging from them ($\mathcal{R}_{UC}$), ultimately providing themselves with an unfair advantage.

\vspace{-1em}

\section{Unbiased Evaluator}


\subsection{General formulation}

Consider evaluating an LLM \(\theta\), where the model is tasked with generating answers for a set of evaluation problems \(\mathcal{X} = \{x_1, x_2, \dots, x_n\}\). The ground truth labels \(\mathcal{Y} = \{y_1, y_2, \dots, y_n\}\) are then used to calculate performance metrics. For each problem \(x_i \in \mathcal{X}\) and its corresponding label \(y_i \in \mathcal{Y}\), the evaluation process can be framed as a causal analysis, where the input \(\mathcal{X}\) and the output \(\mathcal{Y}\) represent the cause and effect, respectively.

The evaluation process for a question \(x\) can generally be represented as a causal diagram, specifically a causal directed acyclic graph (DAG) \(\mathcal{G} = (E, V)\). In this representation, the nodes or vertices \(V = \{V_{\text{input}}, V_{\text{latent}}, R\}\) correspond to the observed input variables \(V_{\text{input}}\), unobserved latent variables \(V_{\text{latent}}\), and the ground truth label \(R\). The edges \(E\) capture the direct causal relationships between these variables. 

Specifically, \(V_{\text{input}} = \{A, B, C, D\}\) represents the input variables, which are composed of several atomic elements derived from the input samples. In widely used multiple-choice questions, atomic variables may include elements such as the instruction, answer labels, and other context-specific components. For instance, consider the question: ``Is 9.8 bigger than 9.11? A: True, B: False''. Here, the atomic variables include ``9.8'', ``bigger'', ``9.11'', ``A'', ``B'', ``True'', and ``False''. A large language model (LLM) reasons over these atomic variables to arrive at the final answer. 


Traditional evaluation methods, as depicted in Figure~\ref{fig_pipline} (a), rely on static and fixed inputs, i.e., input variables, to investigate causal effects. However, this rigid framework may inadvertently introduce contamination issues, limiting the validity and robustness of the causal analysis. In contrast, our proposed approach, termed the Unbiased Evaluator, adopts a more dynamic and adaptive strategy, as shown in Figure~\ref{fig_pipline} (b). 
Specifically, given input variables and ground truth label pairs \(\{\{A, B, C, D\}, R\}\) for a task \(\mathcal{T}\), we enhance the evaluation by augmenting  variables through carefully designed \emph{\textbf{B}}ags \emph{\textbf{O}}f \emph{\textbf{A}}tomic In\emph{\textbf{T}}erventions (\textbf{BOAT}), i.e. new input variable configurations, such as \(\{\{\hat{A}, B, \hat{C}, D\}, \hat{R}\}\) and \(\{\{\hat{B}, \hat{C}, E\}, \hat{R}\}\), are intervened with $\mathcal{F}(\cdot|\mathcal{T})$. More intervened examples are presented in Appendix~\ref{intervened_examples}.

Our proposed approach not only mitigates potential contamination by incorporating diverse interventions but also offers an interpretable framework for evaluating LLMs. By examining counterfactual scenarios, such as comparing the effects of \(\hat{A}\) versus \(A\), our method fosters a deeper and more transparent understanding of model performance. We demonstrate this in the following experiments procedures.


\begin{table*}[t]
    \centering
    \caption{The performance of different LLMs on vanilla benchmarks and Unbiased Evaluator.}
    \label{tab:main_result}
    \resizebox{0.95\textwidth}{!}{
    \begin{tabular}{c|c|ccc|ccc|ccc}
        \toprule
        & \multirow{2}{*}{Model} & \multicolumn{3}{c}{ARC-C} & \multicolumn{3}{c}{MMLU} & \multicolumn{3}{c}{GSM8K} \\
        \cmidrule{3-5}
        \cmidrule{6-8}
        \cmidrule{9-11}
        & & Vanilla & Ours & $\Delta$ & Vanilla & Ours & $\Delta$ & Vanilla & Ours & $\Delta$ \\
        \midrule
        \multirow{3}{*}{proprietary} & GPT-4o & 94.51 & 89.36 $\pm$ 0.26 & 5.15 & 83.51 & 68.82 $\pm$ 0.42 & 14.69 & 96.21 & 86.01 $\pm$ 0.64 & 10.20 \\
& GPT-4-Turbo & 96.48 & 89.74 $\pm$ 0.13 & 6.74 & 84.10 & 71.81 $\pm$ 0.28 & 12.29 & 97.73 & 90.18 $\pm$ 0.34 & 7.55 \\
& Gemini-2.0 & 95.71 & 88.15 $\pm$ 0.26 & 7.56 & 86.20 & 73.57 $\pm$ 0.09 & 12.63 & 97.88 & 89.35 $\pm$ 0.04 & 8.53 \\
\midrule
% \multirow{5}{*}{open-source} & Qwen2-72B-Instruct & 95.11 & 87.38 $\pm$ 0.92 & 7.73 & 82.69 & 67.30 $\pm$ 0.42 & 15.39 & 96.44 & 81.17 $\pm$ 1.13 & 15.27 \\
\multirow{4}{*}{open-source}  & Qwen2.5-72B-Instruct & 94.33 & 85.69 $\pm$ 0.86 & 8.64 & 84.07 & 69.56 $\pm$ 0.39 & 14.51 & 98.41 & 88.86 $\pm$ 0.89 & 9.55 \\
& Llama3.1-70B-Instruct & 93.99 & 81.23 $\pm$ 0.74 & 12.76 & 80.70 & 61.93 $\pm$ 0.12 & 18.77 & 95.98 & 82.97 $\pm$ 0.66 & 13.01 \\
& Yi1.5-34B-Chat & 93.91 & 71.79 $\pm$ 0.45 & 22.12 & 77.72 & 56.70 $\pm$ 0.18 & 21.02 & 91.96 & 69.60 $\pm$ 0.92 & 22.36 \\
& Mistral-Large-2411 & 94.85 & 81.89 $\pm$ 0.31 & 12.96 & 82.37 & 61.63 $\pm$ 0.09 & 20.74 & 97.73 & 90.04 $\pm$ 0.68 & 7.69 \\
    \bottomrule
    \end{tabular}}
\vspace{-2em}
\end{table*}


\subsection{Bags of Atomic Interventions}


Based on the general formulation described above, we provide the details of our designed Bags of Atomic Interventions in Table~\ref{table_interventions}. Specifically, we focus on two widely used tasks: Multiple Choice Questions (MCQ) and Mathematics.
For Multiple Choice Questions, we have designed six atomic interventions targeting different intervention positions within the task.
\begin{itemize}
\vspace{-1em}
\item \textbf{Distractor Hint.}
A hint is introduced in the form of an additional option indicating that no correct answer exists for the question. 
\vspace{-0.4em}
\item \textbf{Distractor Question.}
To ensure the model thoroughly understands the question, a distractor question is introduced, randomly selected from the same dataset. 
\vspace{-1.8em}
\item \textbf{Answer Removal.}
Building upon the first strategy, some answers (including correct or not) is removed and replaced with an unrelated option from other questions. 
\vspace{-1.8em}
\item \textbf{Option Shuffling.}
The order of the options is randomly shuffled to investigate whether the model's performance is influenced by their positional arrangement.
\vspace{-1.8em}
\item \textbf{Label Replacement.}
The conventional option labels (\textit{A, B, C, D}) are replaced with numerical labels (\textit{1, 2, 3, 4} or \textit{\uppercase\expandafter{\romannumeral1}, \uppercase\expandafter{\romannumeral2}, \uppercase\expandafter{\romannumeral3}, \uppercase\expandafter{\romannumeral4}}) to explore whether the labeling format affects the model’s output tendencies.
\vspace{-0.4em}
\item \textbf{Binary Transformation.}
The stem of a multiple-choice question is combined with each of the four options to create four true/false questions. 
\end{itemize}
\vspace{-1em}

For mathematics, we design two interventions to transform questions into True/False question and MCQ, and then combined with aforementioned MCQ interventions.

\vspace{-1em}

\begin{itemize} 
\item \textbf{Question Jitter.} Slightly altering the numbers in a question allows an open-ended math problem to be reframed as a True/False question.
\vspace{-0.7em}
\item \textbf{Answer Jitter.} To transform an open-ended math problem into a multiple-choice question, the numerical answer can be adjusted with minor variations.
\end{itemize}
\vspace{-1em}


\begin{figure}[t]
    \centering
    \subfigure[Qwen2.5-72B-Instruct]{
        \includegraphics[width=0.21\textwidth]{figs/confusion_matrix_Qwen2.5-72B-Instruct.pdf}
    }
    \quad
    \subfigure[Llama3.1-70B-Instruct]{
        \includegraphics[width=0.21\textwidth]{figs/confusion_matrix_Llama3.1-70b-Instruct.pdf}
    }
    \quad
    \subfigure[Yi1.5-34B-Chat]{
        \includegraphics[width=0.21\textwidth]{figs/confusion_matrix_Yi1.5-34B-Chat.pdf}
    }
    \quad
    \subfigure[Mistral-Large-2411]{
        \includegraphics[width=0.21\textwidth]{figs/confusion_matrix_Mistral-Large-2411.pdf}
    }
    \vspace{-1em}
    \caption{The confusion matrix of original benchmarks and Unbiased Evaluator on ARC-C.}
    \vspace{-2em}
    \label{fig:confusion_matrix}
\end{figure}


\begin{figure}[t]
    \centering
    \subfigure[ARC-C(Qwen1.5)]{
        \includegraphics[width=0.45\linewidth]{figs/bar_qwen15_arc_c_scaling.pdf}
    }
    \quad
    \subfigure[MMLU(Qwen1.5)]{
        \centering
        \includegraphics[width=0.45\linewidth]{figs/bar_qwen15_mmlu_scaling.pdf}
    }
    \quad
    \subfigure[ARC-C(Llama2)]{
        \includegraphics[width=0.21\textwidth]{figs/bar_llama2_arc_c_scaling.pdf}
    }
    \quad
    \subfigure[MMLU(Llama2)]{
        \includegraphics[width=0.21\textwidth]{figs/bar_llama2_mmlu_scaling.pdf}
    }
    \caption{The performance of Qwen1.5 series and Llama2 series models on ARC-C and MMLU, considering both original benchmarks and Unbiased Evaluator.}
    \label{fig:scaling_law}
    \vspace{-2em}
\end{figure}


 Note that these atomic interventions can be organically combined to form complex interventions, providing a more comprehensive evaluation of large language models. For instance, Distractor Hint can be seamlessly integrated with Answer Removal, Option Shuffling, and Label Replacement to create a more sophisticated and targeted intervention.

\textbf{Discussion.} The design of BOAT is guided by the findings from Proposition \ref{proposition_3_1}. Specifically, the mechanisms of Answer Removal and Binary Transformation are introduced to address biases inherent in the original benchmark, thereby mitigating related term. For instance, public benchmarks like MMLU are known to exhibit ambiguities, which will be alleviated with these interventions. Additionally, we have made effort to minimize independent term introduced by our method (refer to the implementation details).
 
\section{Experiments}

\subsection{Experimental Setup}

\textbf{Evaluated Datasets and LLMs.}  
Following \cite{zhu2024dynamic}, we evaluate on two widely used benchmarks for multiple-choice questions: ARC-Challenge (ARC-C) \cite{clark2018think} and MMLU \cite{hendryckstest2021}. For mathematical problem-solving, we utilize the GSM8K dataset \cite{cobbe2021training}.  
The evaluation includes three proprietary large language models (LLMs): GPT-4o \cite{openai_gpt4o}, GPT-4-Turbo \cite{achiam2023gpt}, and Gemini 2.0 \cite{team2023gemini}. Additionally, we assess several open-source models, including Llama \cite{touvron2023llama}, Mistral \cite{jiang2023mistral}, Qwen \cite{bai2023qwen}, and Yi \cite{ai2024yi}.  



\textbf{Implementation Details.}
\label{sec_implementation}
We build our method on the widely-used OpenCompass \cite{2023opencompass} evaluation framework. Following the approach in \cite{zhu2024dynamic}, we set the generation temperature to 0 for all models and cap the output length at a maximum of 1000 tokens. All evaluations are conducted in a 5-shot setting, with results averaged over 5 runs. Moreover, the interventions in BOAT are not randomly combined but follow specific constraints. We regulate the probability of each intervention to ensure balance. 
When applying a binary transformation to questions, modifications involving phrases such as ``which'' or ``following'' were excluded. Furthermore, during the Answer Removal process, we ensured that the answers extracted from different questions were not identical.
For additional details, please refer to Appendix~\ref{impl_details}.


\subsection{Main Results}
Table~\ref{tab:main_result} presents the evaluation results of different LLMs on the original protocol and our Unbiased Evaluator, showing all LLMs experienced performance degradation on the Unbiased Evaluator. Specifically, GPT-4-Turbo and Gemini 2.0 demonstrated the smallest relative performance drop, maintaining their position as the strongest models. In contrast, GPT-4o performed similarly to GPT-4-Turbo on the original protocol but exhibited a larger average performance decline, particularly on MMLU and GSM8K. We hypothesize that the omni design of GPT-4o may hinder its performance on NLP tasks.
Compared to proprietary models, open-source ones showed a more significant average drop, with Yi1.5-34B-Chat standing out as particularly affected, suggesting a substantial potential for improvement.

\vspace{-0.8em}


\subsection{Bias Analysis}
Following Sec.~\ref{sec_probing_task}, we conduct an analysis of data and model bias for our proposed method, with the results presented in Table~\ref{table_data_bias} and Fig.~\ref{fig_model_bias}. Regarding data bias, our method exhibits a smaller coefficient and a large $p$-value, indicating reduced data bias. In terms of model bias, compared to Agents-as-Evaluator, our Unbiased Evaluator remains relatively stable in both $\mathcal{R}_{OC}$ and $\mathcal{R}_{UC}$ as the strength increases, suggesting lower model bias.


\subsection{Confusion Matrix Analysis}
We present a comprehensive confusion matrix in Fig.~\ref{fig:confusion_matrix}, which evaluates performance across four dimensions: ``True/False'' labels from the original protocol and our Unbiased Evaluator. For instance, the category labeled ``Original True'' and ``Unbiased Evaluator False'' highlights scenarios where the model performs correctly under the original protocol but fails when assessed with the Unbiased Evaluator. A striking observation across all models is the high frequency of ``Original True'' and ``Unbiased Evaluator False'' instances. This trend suggests issues with data contamination and inherent capability limitations within the models. 
Notably, the ``Original True'' and ``Unbiased Evaluator False'' of Yi1.5-34B-Chat are significantly higher compared to other larger models. This indicates that it may be more susceptible to data contamination and capability constraints.

\vspace{-0.8em}

\subsection{Effect of Interventions \& Evaluation Interpretability}

Figure~\ref{fig:diff_strategies} illustrates the accuracy of Qwen2.5-7B-Instruct when each single atomic interventions are applied. The results show that the model's performance varies significantly depending on the type of intervention. Notably, compared to the original protocol, the Binary Transformation intervention poses the greatest challenge, leading to a substantial decline in accuracy. This suggests that the model may not fully comprehend every option in a MCQ but instead might rely on certain heuristic shortcuts to arrive at an answer.
Moreover, even with relatively simple interventions, such as Option Shuffling and Label Replacement, the model's performance exhibits a slight degradation. This provides strong evidence of potential data contamination.


\begin{figure}[t]
    \centering
    \includegraphics[width=0.4\textwidth]{figs/bar_diff_strategies.pdf}
    \vspace{-1.5em}
    \caption{Accuracy of Qwen2.5-7B-Instruct on ARC-C when each single atomic interventions are applied. Orig denotes the original benchmark. DQ, DH, AE, OS, LR and BT represents Distractor Hint, Distractor Question, Answer Removal, Option Shuffling, Label Replacement and Binary Transformation, respectively.}
    \label{fig:diff_strategies}
    \vspace{-2em}
\end{figure}


\subsection{Effect of the Scaling}

As shown in Figure~\ref{fig:scaling_law}, we evaluate the Qwen1.5 and Llama2 series models with Unbiased Evaluator on MMLU and ARC-C. The results indicate that as the model parameters increase, performance gradually improves. Notably, under the original evaluation protocol, the larger Qwen1.5 models achieved over 90\% performance, nearing saturation and limiting the ability to evaluate larger models, such as 32/72/110B. In contrast, Unbiased Evaluator demonstrate a consistent performance improvement from 32B to 110B.

\vspace{-1em}

\subsection{Human Verification}
To ensure the correctness of proposed method, we randomly selected 300 samples from the MMLU and 100 samples from the ARC-C and GSM8K, 500 questions in total. 9 human experts (with bachelor or higher degree) are divided into 3 groups, each with 3 person. They were asked to judge the following question: Whether the answers to the intervened questions are correct. As shown in Table~\ref{human_verification}, the human verification demonstrates an overall accuracy rate of 99.3\%, 99.9\% and 99.7\% for ARC-C, MMLU and GSM8K, respectively, indicating the effectiveness of our methodology. 

\vspace{-1em}

\section{Conclusions}
This paper present a theoretical analysis of evaluation bias, offering valuable findings for designing protocols. Moreover, two types of bias in Agents-as-an-Evaluator are identified with probing task. To mitigate the bias, guided with previous findings, an new evaluation protocol, Unbiased Evaluator, is proposed to offer unbiased and interpretable assessment for benchmark contamination. We look forward that our method may bring inspirations for future design for LLMs evaluation.

\section{Impact Statement}
Bias in the evaluation of LLMs is a crucial issue for ensuring responsible AI development in society. This work conduct a detailed bias analysis on previous protocol, followed by a novel evaluation protocol designed to fairly measure the true capabilities of LLMs. By providing a more precise assessment framework, this protocol aims to enhance our understanding of these models, ultimately contributing to more transparent, fair, and reliable AI systems.



% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocite
% \nocite{langley00}

\bibliography{example_paper}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn

\section{Prompt in Probing Task}
\label{appendix_prompt}
\begin{lstlisting}[]
I have a question with a possible answer.

### Question:
{question}

### Possible Answer:
{answer}

You need to rate the given possible answer on a scale of 1 to 10 based on the confidence of its correctness for the question, using the following rating rules:

Score 1 to 2: You are very confident that the given possible answer is completely incorrect for the question.
Score 3 to 4: You are fairly confident that the given possible answer is incorrect, but there is a small chance it could be partially correct.
Score 5 to 6: You are uncertain about the correctness of the given possible answer; it could be right or wrong.
Score 7 to 8: You are fairly confident that the given possible answer is correct, but there is a small chance it could be partially incorrect.
Score 9 to 10: You are very confident that the given possible answer is completely correct for the question.

Finally, you must rate the answer strictly on a scale of 1 to 10 with in the format of ``<<< >>>'', for example, ``<<<5>>>.''
\end{lstlisting}


\section{Implementation Details}
\label{impl_details}
\subsection{Hyperparameter}
To ensure a balance of interventions in BOAT, the probability for each intervention was set to 0.5, except for Binary Transformation, which was assigned a probability of 0.1. Under the 5-shot evaluation setting, we introduced the same combinations of atomic interventions to the few-shot samples as those in the final question. An exception was made for questions affected by Answer Removal: in such cases, we randomly selected half of the few-shot samples for intervention to prevent the model from repeating the output corresponding to option N.


\subsection{BOAT constraints}

As shown in~\ref{tab:dependencies}, there are constraints among different interventions, and not all interventions can be combined arbitrarily. For example, the Answer Removal can only be introduced when the Distractor Hint intervention exists. Moreover, the binary transformation intervention cannot be combined with other interventions.

Please note that the instructions will change depending on the interventions. The detailed instructions are presented in~\ref{tab:instuctions}.

\begin{table*}[t]
    \centering
    \caption{The BOAT constraints are represented in a matrix where each cell indicates whether the second intervention (column) can be applied when the first intervention (row) is introduced. A check mark ($ $\checkmark$ $) denotes that the second intervention can be combined with the first one, while a black triangle ($\blacktriangle$) indicates that the second intervention is a required addition.}
    \vspace{1em}
    \label{tab:dependencies}
    \resizebox{\textwidth}{!}{
    \begin{tabular}{ccccccc}
        \hline
        First/Second Intervention & Distractor Hint & Distractor Question & Answer Removal & Option Shuffling & Label Replacement & Binary Transformation \\
        \hline
        Distractor Hint &  &  $\checkmark$  &  $\checkmark$  &  $\checkmark$  &  $\checkmark$  &  \\
        Distractor Question &  $\checkmark$  & &  &  $\checkmark$  &  $\checkmark$  &  \\
        Answer Removal & $\blacktriangle$ &  &  &  $\checkmark$   &  $\checkmark$  &  \\
        Option Shuffling &  $\checkmark$  &  $\checkmark$  &  $\checkmark$  & &  $\checkmark$  &  \\
        Label Replacement &  $\checkmark$  &  $\checkmark$  &  $\checkmark$  &  $\checkmark$  & &  \\
        Binary Transformation &  &  &  &  &  &  \\
        \hline
    \end{tabular}}
\end{table*}


\begin{table*}[t]
    \centering
    \caption{The detailed instructions under different interventions.}
    \label{tab:instuctions}
    \resizebox{\textwidth}{!}{
    \begin{tabular}{c|c}
        \hline
        Intervention & Instruction \\
        \hline
        Distractor Hint & If there is no correct answer in the options, please reply with N. \\
        Distractor Question & Here are two questions and only one of them corresponds to the options. Please select the correct answer. \\
        Answer Removal & Here is a multiple choice question. \\
        Option Shuffling & Here is a multiple choice question. \\
        Label Replacement & Here is a multiple choice question. \\
        Binary Transformation & The following are true/false questions. If the answer is correct, please reply with T, otherwise reply with F. \\
        \hline
    \end{tabular}
    }
\end{table*}


\section{Proof of Proposition}

\subsection{Proof of Proposition 3.1}
\label{proof_3_1}
 Given the definition in~\ref{definition_3_1}, the evaluation bias on benchmark $D$ is given by:
\begin{align}
\epsilon(\hat{\phi}_D) = \mathbb{E}[\hat{\phi}_D] - \phi
\end{align}
Similarly, the evaluation bias on rephased one $D'$ from $D$ is defined as $ \epsilon_{D'} $, then
\begin{align}
\epsilon(\hat{\phi}_{D'}) &= \mathbb{E}[\hat{\phi}_{D'}] - \phi \nonumber \\
&= \mathbb{E}[\hat{\phi}_{D'}] - (\mathbb{E}[\hat{\phi}_D] - \epsilon(\hat{\phi}_D)) \nonumber \\
&= (\mathbb{E}[\hat{\phi}_{D'}] - \mathbb{E}[\hat{\phi}_D]) + \epsilon(\hat{\phi}_D) \nonumber \\
&=\Delta + \epsilon(\hat{\phi}_D)
\end{align}
where $ \Delta $ is the delta bias which arises from the introduction of new  evaluation protocol.

Consider that $\epsilon(\hat{\phi}_{D'})$ could be positive, negative, and ideally, zero. Therefore, we seek to analyze the its mean squared term:
\begin{align}
\mathbb{E}[\epsilon(\hat{\phi}_{D'})^2] &
= \mathbb{E}[(\Delta + \epsilon(\hat{\phi}_D))^2] \nonumber \\
&= \mathbb{E}[\Delta^2] + \mathbb{E}[\epsilon(\hat{\phi}_D)^2] + 2\mathbb{E}[\Delta\epsilon(\hat{\phi}_D)] \label{substite}
\end{align}
Because,
\begin{align}
\text{Cov}(\epsilon(\hat{\phi}_D), \Delta) &= \mathbb{E}[(\epsilon(\hat{\phi}_D) - \mathbb{E}[\epsilon(\hat{\phi}_D)])(\Delta - \mathbb{E}[\Delta])] \nonumber \\
&= \mathbb{E}[\epsilon(\hat{\phi}_D)\Delta -  \epsilon(\hat{\phi}_D)\mathbb{E}[\Delta] - \Delta\mathbb{E}[\epsilon(\hat{\phi}_D)] + \mathbb{E}[\epsilon(\hat{\phi}_D)]\mathbb{E}[\Delta]] \nonumber \\
&= \mathbb{E}[\epsilon(\hat{\phi}_D)\Delta] -  \mathbb{E}[\epsilon(\hat{\phi}_D)]\mathbb{E}[\Delta] - \mathbb{E}[\Delta]\mathbb{E}[\epsilon(\hat{\phi}_D)] + \mathbb{E}[\epsilon(\hat{\phi}_D)]\mathbb{E}[\Delta] \nonumber \\
&= \mathbb{E}[\epsilon(\hat{\phi}_D)\Delta] -  \mathbb{E}[\epsilon(\hat{\phi}_D)]\mathbb{E}[\Delta]
\end{align}
Therefore,
\begin{align}
\mathbb{E}[\epsilon(\hat{\phi}_D) \Delta] = \text{Cov}(\epsilon(\hat{\phi}_D), \Delta) + \mathbb{E}[\epsilon(\hat{\phi}_D)] \mathbb{E}[\Delta]
\end{align}
Substitute the expression into Equation (\ref{substite}),
\begin{align}
\mathbb{E}[\epsilon(\hat{\phi}_{D'})^2] = \mathbb{E}[\epsilon(\hat{\phi}_D)^2] + 2\text{Cov}(\epsilon(\hat{\phi}_D), \Delta) + 2\mathbb{E}[\epsilon(\hat{\phi}_D)] \mathbb{E}[\Delta] + \mathbb{E}[\Delta^2]
\end{align}
Here, 
\begin{itemize}
\item $\mathbb{E}[\epsilon(\hat{\phi}_D)^2]$ is an original term that is associated with the original bias existing in the original benchmark $D$. 
\item $ 2\text{Cov}(\epsilon(\hat{\phi}_D), \Delta) $ is a related term which pertains to biases that newly introduced biases are correlated with the pre-existing biases in the original benchmark.  
\item $ 2\mathbb{E}[\epsilon(\hat{\phi}_D)] \mathbb{E}[\Delta] + \mathbb{E}[\Delta^2] $  is an independent term, stemming from biases inherent to the methodology itself. 
\end{itemize}

\section{Intervened Examples}
\label{intervened_examples}

%ARC-C, [hint,label]%
\begin{lstlisting}[title={Example \#1}]
Here is a multiple choice question. If there is no correct answer in the options, please reply with N.
Question:
The end result in the process of photosynthesis is the production of sugar and oxygen. Which step signals the beginning of photosynthesis?
I. Chemical energy is absorbed through the roots.
II. Light energy is converted to chemical energy.
III. Chlorophyll in the leaf captures light energy.
IV. Sunlight is converted into chlorophyll.
Answer:
III
\end{lstlisting}


%ARC-C, [question]%
\begin{lstlisting}[title={Example \#2}]
Here are two questions and only one of them corresponds to the options. Please select the correct answer.
Question:
A group of engineers wanted to know how different building designs would respond during an earthquake. They made several models of buildings and tested each for its ability to withstand earthquake conditions. Which will most likely result from testing different building designs?
The voltage is held constant in an electric circuit. What will happen to the current in this circuit if the resistance is doubled?
A. buildings will be built faster
B. buildings will be made safer
C. building designs will look nicer
D. building materials will be cheaper
Answer:
B
\end{lstlisting}

%ARC-C, [question,hint,label,remove]%
\begin{lstlisting}[title={Example \#3}]
Here are two questions and only one of them corresponds to the options. Please select the correct answer. If there is no correct answer in the options, please reply with N.
Question:
Which statement best explains why a tree branch floats on water?
What happens to a wooden log when it is burned?
I. Wood is light.
II. parasitism
III. Wood is magnetic.
IV. Wood is porous.
Answer:
N
\end{lstlisting}

%ARC-C, [binary]%
\begin{lstlisting}[title={Example \#4}]
The following are true/false questions. If the answer is correct, please reply with T, otherwise reply with F.
Question:
Statement 1: A polished metal ball looks very shiny and bright on a sunny day. What makes the ball look shiny? The ball makes light.
Statement 2: A polished metal ball looks very shiny and bright on a sunny day. What makes the ball look shiny? The ball reflects light.
Statement 3: A polished metal ball looks very shiny and bright on a sunny day. What makes the ball look shiny? The ball absorbs light and then releases it.
Statement 4: A polished metal ball looks very shiny and bright on a sunny day. What makes the ball look shiny? The ball absorbs light and keeps it inside.
Answer:
F T F F
\end{lstlisting}

\section{Human Verification}
\label{human_verification}
\begin{table}[h]
\centering
\caption{Results of human verification on ARC-C, MMLU and GSM8K datasets.}
\vspace{1em}
\begin{tabular}{ccccc}
\toprule
& ARC-C & MMLU & GSM8K & Average \\
\midrule
Group 1 & 1.000 & 0.997 & 0.990 & 0.996 \\
Group 2 & 0.990 & 1.000 & 1.000 & 0.997 \\
Group 3 & 0.990 & 1.000 & 1.000 & 0.997 \\
Average & 0.993 & 0.999 & 0.997 &  / \\
\bottomrule
\end{tabular}
\end{table}


\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
