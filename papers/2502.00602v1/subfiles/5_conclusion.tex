\section{Conclusion and Limitations}
\label{sec:conclusion}

% In this work we study HTO, a token-dependent overfiting of KE. 
% We show how HTO leads to an edited LLM's suboptimal reasoning ability and analyze its direct cause from standard training paradigm,
% wherefore {\NAME} is proposed. 
% Our solution adaptively assigns each token a unique smoothed distribution to allow more granular control on the LLM fine-tuning process. 
% Experiments with diverse KE methods validate the effectiveness of our methods. 
% Motivated by promising results here, we plan to generalize {\NAME} to broader KE methods that rely on more specialized loss. 


We study HTO, a token-dependent overfitting in KE, and show how it degrades an edited LLMâ€™s reasoning ability. 
Inspired by an in-depth analysis on its cause, we propose {\NAME}, which adaptively assigns each token a unique smoothed distribution for better control to mitigate HTO. 
Our solution enjoys several theoretical advantages, and achieves superior performance on diverse tasks. 
Encouraged by promising results, we plan to generalize our method on broader KE methods that involves more specialized losses.