\section{Introduction}
\label{sec:intro}


Language models (LMs) parameterized by deep neural networks~\citep{vaswani2017attention,lewis2019bart,radford2019language,brown2020language}
demonstrate strong generalizability across various natural language generation and classification tasks~\citep{see2019massively,raffel2020exploring, ji2023survey}.
These successes underscore their versatility, establishing them as new foundations for natural language processing applications~\citep{bommasani2021opportunities,zhou2023comprehensive}. 
Furthermore, with model sizes continually increasing, 
large language models (LLMs) exhibit emerging abilities to follow natural language instructions~\citep{dong2022survey,ouyang2022training}, 
which empowers their zero-shot adaptations to unseen tasks~\citep{kojima2022large}, paving the way towards artificial general intelligence~\citep{bubeck2023sparks}. 


Despite this remarkable potential, the real-world LLM deployment remains largely unresolved: 
LLMs are capable of comprehending a wide range of human instructions and queries, 
but they can only provide feedback based on their \textit{static} knowledge from the data they were trained on. 
In a fast-changing world, most knowledge quickly becomes outdated. 
For example, the updated knowledge about \textit{the president of United States} would refer to \textit{Donald Trump} rather than \textit{Joe Biden}. 
Failing to maintain update-to-date knowledge could amplify critical issues such as making factual fallacy~\citep{de2021editing} or producing harmful generations~\citep{hartvigsen2022toxigen}.
However, the significant computational cost of retraining makes it impractical to frequently incorporate new knowledge.

% but the notable training cost renders an retraining to incorporate new knolwedge prohibitive. 


% Background on KE 

As a remedy, 
\textit{knowledge editing} (KE), whose goal is to update an LLM with some \textit{specific} knowledge without hurting irrelevant others and general ability, is proposed~\citep{wang2023knowledge, zhang2024comprehensive}.
% To this end, 
Full fine-tuning of LLMs proved ineffective as it severely disrupted irrelevant knowledge~\citep{wang2023knowledge}, leading to an \textit{editing-locality} trade-off. 
Here \textit{locality} refers to the ability to maintain knowledge unrelated to the update, such as \textit{the prime minister of Canada} for the previous case.
To achieve a good locality, model updates need to be \textit{selective} and should rely on \textit{a small fraction} of parameters~\citep{wang2023knowledge}.
Following this principle, parameter-efficient fine-tuning (PEFT) methods such as LoRA~\citep{hu2021lora} have achieved good performance~\citep{wu2023eva}.
On the other hand, \citet{huang2023transformer, dong2022calibrating} restricted the updates to some pre-specified feed-forward network (FFN) layer that serves as knowledge storage~\citep{dai2021knowledge}.
\citet{meng2022locating, meng2022mass} refined the process by introducing a \textit{locating} stage to {identify} which layer the target knowledge is stored. These fine-grained manners have demonstrated impressive success in maintaining high locality~\citep{zhang2024comprehensive}. 




% Despite the remarkable ability to achieve good editing-locality trade-off, 
Nevertheless, 
existing methods still suffered from losing LLM generalizability, 
especially when dealing with tasks that involve the edited knowledge, 
due to the so-called \textit{overfitting} of KE~\citep{zhang2024uncovering}. 
Specifically, 
KE often involves one piece of new knowledge to edit at a time, which entails updating (selected) parameters with single training instance. 
Consequently, edited LLMs tend to pay excessive attention to the edited subject, but fail to reason about the new knowledge~\citep{zhong2023mquake,zhang2024uncovering}. 
Previous works highlighted this challenge, and quantified this ability with a new metric known as \textit{portability}~\citep{zhong2023mquake,wang2024deepedit}. 
However, the underlying causes of overfitting and their relationship to the KE process remain under-explored, 
leaving \textit{if KE overfitting can be solved in a principled manner} an open question. 



In this work,
we take the first step toward a deeper understanding of this overfitting, 
and pave the way for a principled solution to mitigate it. 
We first provide strong evidence that \textit{KE overfitting leads to catastrophic degradation of an LLM's reasoning ability}.
In particular, we showed that as the LLM is edited with new knowledge, 
the probability of correct reasoning consistently decreases.
To quantify this, we investigated the \textit{portability loss} at each fine-tuning step (lower indicates better reasoning ability).
We observed that while portability loss initially decreased, it grew up quickly thereafter. 
In addition, the final loss was significantly higher than the initial value. 
This finding confirms that {overfitting is a direct cause of suboptimal portability}. 

% Hetereogenous problem, mention its root in standard training "loss". 

To understand this overfitting, 
we checked how new knowledge is fitted during the KE process. 
Based on our findings,
\textit{KE may only require learning a few pivotal tokens (words)},
as many tokens already exhibit small \textit{initial} loss values. 
Intuitively, an LLM's pre-trained knowledge may enable it to \textit{infer} remaining parts base on pivotal tokens. 
However, existing methods overlook this token-level difference in KE.
Even when selectively updating parameters, 
these methods aim to maximize the likelihoods of the {entire} sentence describing the new knowledge, 
which boils down to maximizing the probability of \textit{all} tokens indiscriminately~\citep{bengio2000neural,radford2019language,brown2020language}. 
As a result, this coarse-grained training paradigm leads to varying degrees of overfitting across tokens. 
We term this phenomenon \textit{heterogeneous token overfitting} (HTO) in KE.
Sec \ref{sec:problem} details our new insight on KE overfitting and its influence on portability. \textit{This is our first main contribution.}


In light of how HTO roots at a token level,
we propose {\NAME}, a new KE training paradigm to tackle it.
{\NAME} assigns each token an adaptive training target according to its (over)fitting state.
An efficient solution is proposed to construct these training objectives in a dynamic way that allows to maintain much pre-trained knowledge if possible. 
The theoretical advantage of our method lies in three folds. 
First, our solution induces negligible computation cost compared to standard training (much cheaper than a LLM forward).
{Second, our solution provides a better parameter update through the lens of importance function~\citep{koh2017understanding}.} 
Finally, {\NAME} has a close connection to direct preference optimization (DPO), a widely-used framework for LLM post-training~\citep{rafailov2024direct,zhang2024negative}, but does not require additional preference data pairs. 
Sec \ref{sec:method} covers these aspects in details.
\textit{The proposed {\NAME} and our theoretical analysis is another main technical contribution of this work.}





Our paper is organized as follows. 
Sec \ref{sec:problem} and Sec \ref{sec:method} details the new overfitting phenomenon in KE 
and our proposed {\NAME} for mitigation respectively. 
Extensive experimental results in Sec \ref{sec:experiment} demonstrate the superiority of our solution. 
In the remaining part of this paper, we review related works in Sec \ref{sec:background}, and conclude the paper in Sec \ref{sec:conclusion}. 
