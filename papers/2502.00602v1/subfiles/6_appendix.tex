\clearpage
\onecolumn





\section{Omitted Theorems and Proofs}
\label{app:proof}

In this section we present the full theoretical analysis.
All theorems are (re)stated in a formal manner for the convenience of reading. 

\subsection{Notations}
For completeness we highlight important notations that will be used. 
Throughout this paper, we use $\ce[\cdot \| \cdot]$ and $\kl[\cdot \| \cdot]$ to compute cross-entropy and Kullback–Leibler divergence between two distributions respectively. 
Specifically, given two discrete distributions $p, q$, $\ce [p \| q] 
= \sum_{i} - p_i \log q_i$, and $\kl [p \| q] = \sum_{i} - p_i \log \frac{q_i}{p_i}$.
In addition, $\ones(\cdot)$ is the indicator function such that $\ones(a) = 1$ if event $a$ holds and 0 otherwise. 
For $a \in \R^{p}$, define the $l_2$ norm as $\|a\|_2 = \sqrt{\sum_{i =1}^p a_i^2}$.
For $a, b \in \R^{p}$, define the inner product as $\langle a, b \rangle = a ^\top b$.
Define the cosine similarity $\cos(a, b) = \frac{\langle a, b \rangle}{\|a\|_2 \|b\|_2}$.





\subsection{{\NAME} is universal and efficient}
%\linjun{let's present the main theorem first, and then state proofs and lemmas}

The first merit of {\NAME}, as stated in the main body, lies in its universality and efficiency.

\begin{proposition}
{\NAME} loss generalizes CE loss and reduces to the latter when $\epsilon = 0, \lambda = 1$.
\end{proposition}
% 
% In addition, our choice of $\pit$ makes {\NAME} computationally efficient, as shown in the proposition below.
% Notably, the additional complexity negligible compared to a forward operation.
\begin{proposition}
Using Alg \ref{alg:ours}, the additional computation complexity induced by {\NAME} is $\mathcal O(|\mathcal V|)$ when fitting a token, where $|\mathcal V|$ is the vocabulary size. 
\end{proposition}


Our proofs rely on the following lemma, which plays a key role in connecting {\NAME} to a regularized loss. 

\begin{lemma}
\label{lem:to-ce}
Given $y_i$, for an arbitrary token $y$ and context $\cv$, and $\pit = \lambda \de{y_i}{y} + \pif (y)$, we have 
\begin{align}
\label{eq:ce-reg}
\ce [\pit (y \mid \cv) \| \pil(y \mid \cv)]
&= 
\lambda \ce [\de{y_i}{y} \| \pil (y \mid \cv)] + (1 - \lambda) \ce [\pif(y \mid \cv) \mid (y \mid \cv)]. 
\end{align}
\end{lemma}


\begin{proof}
The proof is based on the definition of cross entropy \citep{cover1999elements}. 
\begin{align}
\notag 
& \ce [\pit (y \mid \cv) \| \pil(y \mid \cv)] \\
&= 
\notag 
- \sum_{i=1}^{|\mathcal V|} \pit(y \mid \cv) \log \pil (y \mid \cv) \\
&= 
\notag 
- \sum_{i=1}^{|\mathcal V|}  \left( \lambda \de{y_i}{y} + (1 - \lambda) \pif (y \mid \cv) \right)  \log \pil (y \mid \cv) \\ 
&= 
\notag 
- \left( \lambda \sum_{i=1}^{|\mathcal V|} \de{y_i}{y} \log \pil (y \mid \cv) + (1 - \lambda) \sum_{i=1}^{|\mathcal V|} \pif (y \mid \cv)  \log \pil (y \mid \cv)  \right) \\ 
&= 
\label{eq:to-ce} 
\lambda \ce [\de{y_i}{y} \| \pil (y \mid \cv)] + (1 - \lambda) \ce [\pif(y \mid \cv) \| \pil (y \mid \cv)].
\end{align}
This completes our proof. 
\end{proof}
% \begin{proposition}
% {\NAME} loss generalizes CE loss and reduces to the latter when $\epsilon = 0, \lambda = 1$.
% \end{proposition}
We are ready to prove Prop \ref{prop:gen-ce}. 

\begin{proof}
The proof is based on the fact that {\NAME} objective minimizes a \textit{forward} KL-divergence, which is equivalent to minimizing cross-entropy \citep{cover1999elements,bishop2006pattern}. 
Namely,
\begin{align}
\notag
\ell_{\NAME}(\theta)
&\triangleq 
\notag
\sum_{j=1}^m \max(\kl [\pit (y \mid \cv_i) \| \pil(y \mid \cv_i)], \epsilon) \\
&=
\notag
\sum_{j=1}^m \kl [\pit (y \mid \cv_i) \| \pil(y \mid \cv_i)] \ones\left (\kl [\pit (y \mid \cv_i) \| \pil(y \mid \cv_i)] > \epsilon \right) \\
&\overset{(a)}{=} 
\notag
\sum_{j=1}^m \left(\ce [\pit^{(j)} \| \pil^{(j)}] + H (\pit^{(j)}) \right) \ones\left (\kl [\pit^{(j)}] \| \pil^{(j)}] > \epsilon \right) \\ 
&= 
\label{eq: overtone final}
\sum_{j=1}^m \ce [\pit^{(j)} \| \pil^{(j)}] \ones\left (\kl [\pit^{(j)}] \| \pil^{(j)}] > \epsilon \right) + C. 
\end{align}
% 
Starting from step $(a)$, we denote $\pit^{(j)} = \pit (y \mid \cv_i)$ and $\pil^{(j)}$ similarly for brevity, $C$ denotes terms that are constant to learnable parameter $\theta$. 
Therefore, setting $\epsilon = 0$ gets us rid of the indicator term. Further plug in Eq \eqref{eq:to-ce}, we see that setting $\lambda = 1$ reduces to the standard CE loss. This completes the proof. 

\end{proof}


% \begin{proposition}
% Using Alg \ref{alg:ours}, the extra computation complexity induced by {\NAME} is $\mathcal O(|\mathcal V|)$ when fitting a token, where $|\mathcal V|$ is the vocabulary size. 
% \end{proposition}


In terms of Prop \ref{prop:efficiency}, the computation overhead can be seen by checking Algo \ref{alg:ours}. 

\begin{proof} 
The additional computation complexity of {\NAME} is due to line 8-10 in Algo \ref{alg:ours}. These steps involve finding the maximal logits, pruning small logits, and compute the probability with softmax function from the pruned logits. All of them have linear time complexity $|\mathcal V|$.
%This computation needs to be conducted for each token. Therefore, the total complexity is $\mathcal O(m |\mathcal{V}|)$
This completes our proof. 

% To see that {\NAME} does not impose additional NFE on the LM. Note that in order to construct $\pit$, we only need $\pif$, which can be obtained from the LM's current \textit{logits}, which is required by CE loss training as well. In addition, since $\pit$ serves the role of a \textit{smoothed} label and does not involves optimizing $\theta$, there is no need for additional back-propagation. In conclusion, {\NAME}, same as CE loss, can be computed in a single forward, and can be optimized in a single back-propagation. 

\end{proof}








\subsection{{\NAME} provides better updates}
\label{app:proof:better}

We present the formal analysis of how {\NAME} provides better parameters update as outlined in Thm \ref{thm:better}. 
Our analysis is established in the same spirit of influence function \citep{koh2017understanding}. 


We first restate Thm \ref{thm:better}, which outlines the two aspects where {\NAME} is better than training standard CE loss. 

\begin{theorem}[Informal]
Under regularity conditions, compared to optimizing the vanilla CE loss, {\NAME} provides a more favorable update direction for the parameters and has less influence on unrelated knowledge.
\end{theorem}

The formal statement is as follows. 
% The formal statement about (1) and (2) are given by Thm \ref{thm:opt-direct}, and Thm \ref{thm:better} respectively, which are proved in the following. 

\begin{theorem}[Formal]
\label{thm: formal main thm}
    Let $G$ be the ideal gradient of retraining the LLM using $\hat{\theta}^\old$ as the initial value, as defined in Eq~\eqref{eq: G definition}.
    Considering the simplified case where $\epsilon = 0$ in Eq \eqref{eq: overtone final}, under Assumptions~\ref{asmp:stationary} and~\ref{asmp:gradnorm}, 
    there exists some $\lambda \in [0,1]$ such that
    \begin{equation*}
        \cos \qty( \nabla_\theta \ell_\ce (z^\new; \hat{\theta}^\old), G ) < \cos \qty( \nabla_\theta \ell_\NAME (z^\new; \hat{\theta}^\old), G ).
    \end{equation*}
    In other words, using the {\NAME} loss provides a better approximation of the direction of $G$ compared to the standard CE loss, meaning the gradient direction is closer to $G$.

    Now, denote the new estimator obtained through either $\ell_\ce$ or $\ell_\NAME$ by $\hat{\theta}^\new_\ce$ or $\hat{\theta}^\new_\NAME$, respectively. Let $ Z^\un = (X^\un, Y^\un) $ be a random vector representing unrelated data. Under Assumptions~\ref{asm: un-dist} and~\ref{asm: norm ratio of a and c}, we have
    \begin{equation*}
        \E_{Z^\un} \qty[\qty| \pi_{\hat{\theta}^\new_\NAME} (Z^\un) 
        - \pi_{\hat{\theta}^\old} (Z^\un) | ] < \E_{Z^\un} \qty[\qty| \pi_{\hat{\theta}^\new_\ce} (Z^\un) 
        - \pi_{\hat{\theta}^\old} (Z^\un)| ].
        \end{equation*}
    This result indicates that updates based on the {\NAME} loss induce smaller deviations in the predicted distribution for unrelated data compared to updates based on the standard CE loss, thereby better preserving \textit{locality}.
\end{theorem}

Theorem \ref{thm: formal main thm} consists of two parts: Theorem \ref{thm:opt-direct} and Theorem \ref{thm: unrelated data}. Theorem \ref{thm:opt-direct} states that our method provides a more effective direction for parameter updates, while Theorem \ref{thm: unrelated data} asserts that our method results in a smaller perturbation on unrelated knowledge. The assumptions and proofs will be presented in Sections \ref{sec: related data} and \ref{sec: unrelated data}, respectively.

\subsubsection{Our method gives a better direction of parameter updates}
\label{sec: related data}


    
Without loss of generality, suppose that a LLM is pretrained on some large textual corpus $\{z_n \}_{n=1}^N$, each training sample $z_n = (\xv_n, \yv_n)$ where $\yv_n = (y_1, \cdots, y_{m_n})$. 
KE involves updating some knowledge carried by $z^\old = (\xv, \yv^\old)$ to new $z^\new = (\xv, \yv^\new)$.
Let $\hat{\theta}^\old$ denote the pre-trained LLM parameters.
Given this piece of new knowledge, 
the ideal LLM should have parameters $\hat{\theta}^\new$ from a full retraining by solving  
\begin{align}
\label{eq:opt-param}
\min\nolimits_\theta \frac{1}{N} \sum_{n=1}^N \ell_{\ce} (z_n; \theta) - \frac{1}{N} \ell_{\ce} (z^\old; \theta) + \frac{1}{N}\ell_{\ce} (z^\new; \theta),
\end{align}
where $\ell_\ce$ denotes the standard CE loss.
In general, we define $\ell_\delta(\theta)$ as
\begin{equation*}
    \ell_\delta(\theta) = \sum_{i=1}^n \ell_{\ce} (z_i; \theta) + \delta \qty(\ell_{\ce} (z^\new; \theta) - \ell_{\ce} (z^\old; \theta)).
\end{equation*}
Moreover define
\begin{equation*}
    \hat{\theta}_{\delta} = \arg\min_{\theta} \ell_\delta(\theta).
\end{equation*}
So we find that $\hat \theta_0 = \hat \theta^\old$ and $\hat \theta_{\frac{1}{N}} = \hat \theta^\new$.
Starting from $\hat{\theta}^\old$, when we perform gradient descent by using loss $\ell_{\frac{1}{N}}(\theta)$ to retrain the model, the gradient will be
\begin{equation}
\label{eq: G definition}
    G \triangleq \nabla_\theta \ell_{\frac{1}{N}}(\hat{\theta}^\old).
\end{equation}
So we just take $G$ as the \textit{optimal} direction to represent that if we retrained the LLM, i.e., the direction of the gradient descent at $\hat \theta^\old$.

We make following assumption on $\hat \theta^\old$ such that it is a local the minimizer of $\ell_0(\theta)$.
\begin{assumption}
\label{asmp:stationary}
    The pretrained LLM is converged, namely, $\nabla_\theta \ell_0 (\hat \theta^\old) = 0$. 
\end{assumption}
For brevity, denote
\begin{equation}
\begin{aligned}
\label{eq: abc definition}
a 
&=  
\nabla_\theta \ell_\ce (z^\new; \hat{\theta}^\old), \\
b
&= 
- \nabla_\theta \ell_{\ce} (z^\old; \hat{\theta}^\old), \\
c 
&=
\sum_{i =1}^m \nabla_\theta \ce \qty[ \pif(y \mid \cv_i^\new) \|  \pi_\theta(y \mid \cv_i^\new)]\eval_{\theta = \hat{\theta}^\old}.
\end{aligned}
\end{equation}

\begin{assumption}
\label{asmp:gradnorm}
$\cos(b, c)$ satisfies
\begin{equation}
\label{eq: cosbc bound}
\cos(b, c) > 1 - \frac{\norm{b}_2^2}{8 \norm{a + b}_2^2} 
\qty(1 - \cos(a, a + b))^2.
\end{equation}
\end{assumption}

\begin{remark}
[Interpretation of the Assumption \ref{asmp:gradnorm}]
    The Assumption \ref{asmp:gradnorm} ensure direction $b$ and $c$ will not be far away. Roughly speaking, when we take $\frac{\norm{b}_2^2}{8 \norm{a + b}_2^2}$ as some constant. It says that $1 - \cos(b, c) < (1 - \cos(a, a+b))^2$, which means the directions of $b$ and $c$ are closer compared with $a$ and $a+b$.
    When we look it more carefully, 
    Note that $a$ represents $\nabla_\theta \ell_\ce (z^\new; \hat{\theta}^\old)$ and $a + b$ represents the ideal direction $G$.
    Since the old knowledge gradient \( b \) is present, directly fine-tuning \( \ell_{\text{CE}} \) (i.e., the baseline method) results in a deviation compared with the ideal direction $G$. This directional deviation is measured by \( \cos(a, a + b) \). Let $S^{(i)}$ denote the collection of \textit{unfiltered} tokens in $\pif(y \mid \cv_i^\new)$,
   \begin{align}
    b & = - \nabla_\theta \ell_{\ce} (z^\old; \hat{\theta}^\old) = \sum_{i=1}^m \nabla_\theta \log \pil (y_i^\old \mid \cv_i^\old) \eval_{\theta = \hat{\theta}^\old},\\
    c & = \sum_{i =1}^m \nabla_\theta \ce \qty[ \pif(y \mid \cv_j^\new) \| \pi_\theta(y \mid \cv_j^\new) ] \eval_{\theta = \hat{\theta}^\old}
    = - \sum_{i =1}^m \sum_{y \in S^{(i)}} \pif(y \mid \cv_i^\new) \nabla_\theta \log \pi_\theta(y \mid \cv_i^\new) \eval_{\theta = \hat{\theta}^\old}. \label{eq: c detail}
    \end{align}
    Given the new knowledge $\cv_j^\new$, when $y \in S^{(i)}$, it implies that $y$ is likely close to $y_i^\old$ with some probability. Compared to the scenario where the old knowledge $\cv_j^\old$ is given, the gradients $\nabla_\theta \log \pil (y_i^\old \mid \cv_i^\old) \eval_{\theta = \hat{\theta}^\old}$ and $\nabla_\theta \log \pi_\theta(y \mid \cv_i^\new) \eval_{\theta = \hat{\theta}^\old}$ tend to point in opposite directions. This is because both gradients are evaluated at $y^\old$ or a point close to $y^\old$, but the first is conditioned on $\cv_j^\old$ while the second is conditioned on $\cv_j^\new$. Equivalently, this implies that $b$ and $c$ are aligned in the same direction.
    To ensure that we can find a closer direction, we require $ b $ and $ c $ to be approximately as close as $ a $ and $ a + b $. Our goal is to align with the negative gradient direction of the old knowledge. This ensures that when leveraging the information from $ c $ to weight our method, we can identify a direction that closely approximates the ideal optimization direction.
    %Our gradient can be seen as a simplified \textit{correction} for direction of Baseline method Eq \eqref{eq: baseline direct}.
\end{remark}

\begin{remark}
\label{rem: logistic model}
    To elaborate further, we take logistic regression as an example for illustration.

     When considering only the $k$-th token, for a training point \( z_k = (c_k, y_k) \), let \( p(y_k \mid c_k) = \sigma(y_k\theta^\top c_k) \), where \( y_k \in \{-1,1\} \) and \( \sigma(t) = \frac{1}{1 + \exp(-t)} \) is the sigmoid function. 
     the gradient of the log-probability with respect to \( \theta \) is given by:
    \begin{equation*}
        \nabla_\theta \log p(z_k, \theta) = \sigma(-y_k\theta^\top c_k)y_k c_k.
    \end{equation*}
    Then, we find that:
    \begin{equation*}
        b = \sigma(-y_k^\old \theta^\top c_k^\old )y_k^\old c_k^\old,
    \end{equation*}
    \begin{equation*}
        c = -\sum_{y_k \in S^{(i)}} p_{y_k} \sigma(-y_k\theta^\top c_k^\new) y_k c_k^\new 
        = - p_\old \sigma(-y_k^\old \theta^\top c_k^\new)y_k^\old c_k^\new - p_\new \sigma(-y_k^\new \theta^\top c_k^\new)y_k^\new c_k^\new.
    \end{equation*}
    This follows from the fact that \( y_k \in \{-1,1\} \). Note that \( c_k^\new \) and \( c_k^\old \) may be far apart, and \( p_\old \) is likely to be large since \( \pif \) is a denoised version of \( \pil \), meaning it contains less noise \citep{tang2024top}. As a result, the directions of \( b \) and \( c \) will be close.
\end{remark}

\begin{theorem}
\label{thm:opt-direct}
    Let $G$ be the ideal gradient of retraining the LLM using $\hat{\theta}^\old$ as the initial value, as defined in Eq \eqref{eq: G definition}.
    Considering the simplified case where $\epsilon = 0$ in Eq \eqref{eq: overtone final}, under Assumptions~\ref{asmp:stationary} and~\ref{asmp:gradnorm}, 
    there exists some $\lambda \in [0,1]$ such that
    \begin{equation*}
        \cos \qty( \nabla_\theta \ell_\ce (z^\new; \hat{\theta}^\old), G ) < \cos \qty( \nabla_\theta \ell_\NAME (z^\new; \hat{\theta}^\old), G ).
    \end{equation*}
    In other words, using the {\NAME} loss provides a better approximation of the direction of $G$ compared to the standard CE loss, in the sense that {\NAME} gradient direction is closer to $G$. %\linjun{need to be more specific what do you mean by better}
\end{theorem}

\begin{proof}

First, by definition, 
the optimal gradient direction $G$ when using $\theta^\old$ as the initial value
is given by 
\begin{align*}
G 
&= 
\nabla_\theta \ell_{\frac{1}{N}}(\hat{\theta}^\old) \\
&= 
\nabla_\theta \ell_0(\hat{\theta}^\old) + \frac{1}{N}\qty(\nabla_\theta \ell_{\ce} (z^\new; \hat{\theta}^\old) - \nabla_\theta \ell_{\ce} (z^\old; \hat{\theta}^\old))\\
&\overset{(a)}{=} 
\frac{1}{N}\qty(\nabla_\theta \ell_{\ce} (z^\new; \hat{\theta}^\old) - \nabla_\theta \ell_{\ce} (z^\old; \hat{\theta}^\old)),
\end{align*}
where $(a)$ holds from the stationary condition of $\hat{\theta}^\old$ as per Assumption \ref{asmp:stationary}. 
Note that this optimal direction is inaccessible since it is infeasible to find the ground truth $z^\old$ wherefrom the LLM's old knowledge is learned. 
In practice, only $z^\new$ is available, which is provided by the user. 

To see that {\NAME} can provide a better direction, 
we check the gradient of CE loss $\ell_\ce$ and our loss $\ell_{\NAME}$. 
Recall the definition of $a, b, c$ given by Eq \eqref{eq: abc definition}, for CE loss, we have 


% However, it is impossible for us to retrain the whole language model and the old knowledge is unknown or difficult to say by ourselves. Then we can only use the new knowledge to fine-tune the language model.

% When retraining the model, gradient descent is inherently performed. Therefore, it is sufficient to focus on the optimal gradient direction. Since $\nabla_\theta \ell_0(\hat{\theta}^\old) = 0$. If we retrain our model from $\theta^\old$ by using the new loss function $\ell_{\frac{1}{n}}(\theta)$, then the gradient willl be


% Firstly, we will try to establish why our method is more effective when we use it to fine-tune an LLM.
% When we consider training the language model. Suppose we have $n$ samples $z_i$, for each $z_i = (\xv_i, \yv_i)$ and $\yv_i = (y_1, \cdots, y_{m_i})$. from now on, given $x$, we want to replace $z^\old = (\xv, \yv^\old)$ by $z^\new = (\xv, \yv^\new)$. Then the ideal parameter $\hat \theta^\new$ is given by the following (auto-regressive) retraining, 
% \begin{align*}
% \min\nolimits_\theta \frac{1}{n} \sum_{i=1}^n \ell_{\ce} (z_i; \theta) - \frac{1}{n} \ell_{\ce} (z^\old; \theta) + \frac{1}{n}\ell_{\ce} (z^\new; \theta).
% \end{align*}
% We denote the previous parameter as $\hat{\theta}^\old$. Then we use the influence function to represent the difference between $\hat \theta^\old$ and $\hat{\theta}^\new$. To be specific,
% Define the perturbed parameters $\hat{\theta}_{\delta}$ can be written as
% \begin{equation*}
% \hat{\theta}_{\delta} = \arg\min_{\theta \in \Theta} \qty{ \ell_\delta(\theta) \triangleq\sum_{i=1}^n \ell_{\ce} (z_i; \theta) + \delta \qty(\ell_{\ce} (z^\new; \theta) - \ell_{\ce} (z^\old; \theta) ) }.
% \end{equation*}
% To make sure we can analyze, we have the following assumption:
% \begin{assumption}
% \label{asm: loss}
% Assume that loss $\ell_\ce(z; \theta)$ is differentiable. 
% %Additionally assume that the condition number $c_\Lambda = \frac{\lambda_1^2}{\lambda_d^2} \leq c_1$.
% \end{assumption}

% Then we find that $\hat{\theta}^\old = \hat{\theta}_0$ and $\hat{\theta}^\new = \hat{\theta}_{\frac{1}{n}}$. What we are measuring is the direct change between $\hat{\theta}^\old$ and $\hat{\theta}^\new$, which represents the transition between two local minima. When retraining the model, gradient descent is inherently performed. Therefore, it is sufficient to focus on the optimal gradient direction. Since $\nabla_\theta \ell_0(\hat{\theta}^\old) = 0$. If we retrain our model from $\theta^\old$ by using the new loss function $\ell_{\frac{1}{n}}(\theta)$, then the gradient willl be
% \begin{align*}
%     \nabla_\theta \ell_{\frac{1}{n}}(\hat{\theta}^\old) & = \nabla_\theta \ell_0(\hat{\theta}^\old) + \frac{1}{n}\qty(\nabla_\theta \ell_{\ce} (z^\new; \hat{\theta}^\old) - \nabla_\theta \ell_{\ce} (z^\old; \hat{\theta}^\old))\\
%     & = \frac{1}{n}\qty(\nabla_\theta \ell_{\ce} (z^\new; \hat{\theta}^\old) - \nabla_\theta \ell_{\ce} (z^\old; \hat{\theta}^\old)) \triangleq G
% \end{align*}
% \begin{align*}
% \frac{\dif \hat{\theta}_{\delta}}{\dif \delta}\bigg |_{\delta = 0} =
% - H_{\hat{\theta}^{\old}}^{-1} \qty(\nabla_\theta \ell_{\ce} (z^\new; \hat{\theta}^\old) - \nabla_\theta \ell_{\ce} (z^\old; \hat{\theta}^\old)) 
% \triangleq I_{KE}.
% \end{align*}

% However, it is impossible for us to retrain the whole language model and the old knowledge is unknown or difficult to say by ourselves. Then we can only use the new knowledge to fine-tune the language model.

% Next, we try to calculate the gradient of $\ell_\ce$ and $\ell_\NAME$ and compare the cosine distance of them and $G$. First, for the baseline method, we have
\begin{equation}
\label{eq: baseline direct}
    \nabla_\theta \ell_\ce(z^\new; \theta) =
    - \sum_{i=1}^m \nabla_\theta \log \pil (y_i^\new \mid \cv_i^\new) = a,
\end{equation}
where $\cv_i^\new = \xv \oplus y_{<i}^\new$, as derived in Sec \ref{sec:method} in the main body. 

For {\NAME} loss,
according to Eq \eqref{eq:to-ce} and Eq \eqref{eq: overtone final}, we have
\begin{align*}
\nabla_\theta \ell_\NAME(z^\new; \theta) 
&= 
\sum_{i=1}^m \nabla_\theta \ce \qty[ \pit(y \mid \cv_i^\new) \| \pi_\theta(y \mid \cv_i^\new) ]\\
&= 
\lambda \sum_{i=1}^m \nabla_\theta \ce \qty[ \de{y_i^\new}{y} \| \pi_\theta(y \mid \cv_i^\new) ] 
+ (1 - \lambda) \sum_{i=1}^m \nabla_\theta \ce \qty[ \pif(y \mid \cv_i^\new) \| \pi_\theta(y \mid \cv_i^\new) ]\\
&= 
- \qty( \lambda \sum_{i=1}^m \nabla_\theta \log \pil (y_i \mid \cv_i^\new) + (1 - \lambda) \sum_{i=1}^m  - \nabla_\theta \ce \qty[ \pif(y \mid \cv_i^\new) \| \pil (y \mid \cv_i^\new) ])\\
&=
\lambda a + (1 - \lambda) c.
\end{align*}

%$H_{\hat{\theta}^{\old}}^{-1}$ is positive definite, then  
%$H_{\hat{\theta}^{\old}}^{-1} = P \Lambda P^\top$  
%where
%$\Lambda = \diag(\lambda_1^2, \cdots, \lambda_d^2)$, $\lambda_1^2 > \dots > \lambda_d^2 > 0$.
%WLOG, we only need to consider $H_{\hat{\theta}^{\old}}^{-1} = \Lambda$.

% If the cosine is larger, that means when we perform gradient descent, the direction is closer to the retain model direction $G$, which means it is more effective.
% \tcco{The previous remark seems to give an example of $c_1$, so I transformed it into a exact inequality instead of an assumption. This assumption should hold so long as the gradients (a, b, c) are well-performed right? }


Next, we check cosine similarity $\cos \qty( \nabla_\theta \ell_\ce (z^\new; \hat{\theta}^\old), G )$ and $\cos \qty( \nabla_\theta \ell_\NAME (z^\new; \hat{\theta}^\old), G )$.
A larger cosine similarity indicates an update direction that aligns with the ideal $G$ better and is more effective. 

% \begin{assumption}
% \label{asm: difference between b and c}
%     Define $\delta_{bc} = \frac{c}{\|c\|_2} - \frac{b}{\|b\|_2}$. Additionally assume that $\|\delta_{bc}\|_2 < c_1$, where $c_1$ is some constant depends on $a, b$.
% \end{assumption}
% \begin{remark}
%     $\|\delta_{bc}\|_2 < \frac{\|a + b\|_2 }{\|a + b\|_2 + \|b\|_2} \qty(\frac{1}{\cos(a, a + b)} - 1)$.
% \end{remark}

% \begin{theorem}
% \label{thm:opt-direct}
% Compared with standard CE loss, using {\NAME} loss gives a better approximation of the optimal direction to update parameter given by the influence function. More formally, under Assumptions \ref{asm: loss} and \ref{asm: difference between b and c}, $\exists \lambda \in [0,1]$, s.t. $\cos \qty( \nabla_\theta \ell_\ce (z^\new; \hat{\theta}^\old), G ) < \cos \qty( \nabla_\theta \ell_\NAME (z^\new; \hat{\theta}^\old), G )$.
% \end{theorem}

% \begin{proof}
Note that
\begin{equation*}
    \cos \qty( \nabla_\theta \ell_\ce (z^\new; \hat{\theta}^\old), G ) = \frac{\langle a,  a + b \rangle}{\norm{a}_2 \norm{ (a + b)}_2},
\end{equation*}
\begin{equation*}
    \cos \qty( \nabla_\theta \ell_\NAME (z^\new; \hat{\theta}^\old), G ) = \frac{\langle \lambda a + (1 - \lambda) c,  a + b \rangle}{\norm{\lambda a + (1 - \lambda) c}_2 \norm{ (a + b)}_2}.
\end{equation*}
We will show that, there $\exists \lambda \in [0,1]$, s.t.
\begin{equation*}
    \frac{\langle a,  a + b \rangle}{\norm{a}_2} < \frac{\langle \lambda a + (1 - \lambda) c,  a + b \rangle}{\norm{\lambda a + (1 - \lambda) c}_2}.
\end{equation*}
We further denote $\delta_{bc} = \frac{c}{\|c\|_2} - \frac{b}{\|b\|_2}$ which quantifies the directional difference between $ b $ and $ c $. We then have:
\begin{equation}
\label{eq: norm b and c}
c = \qty( \frac{b}{\|b\|_2} + \delta_{bc} ) \|c\|_2.
\end{equation}
Take $\lambda = \frac{\|c\|_2}{\|b\|_2 + \|c\|_2}$, by substituting $c$ by Eq \eqref{eq: norm b and c} and applying the triangle inequality, we obtain
\begin{align*}
    \frac{\langle \lambda a + (1 - \lambda) c,  a + b \rangle}{\norm{\lambda a + (1 - \lambda) c}_2} & = \frac{\Bigl\langle 
    \qty(\frac{\|c\|_2}{\|b\|_2 + \|c\|_2}) a 
    + \qty(\frac{\|b\|_2 \|c\|_2}{\|b\|_2 + \|c\|_2}) 
    \qty( \frac{b}{\|b\|_2} + \delta_{bc} ), a + b 
    \Bigr\rangle}
    {\norm{
    \qty(\frac{\|c\|_2}{\|b\|_2 + \|c\|_2}) (a + b) 
    + \qty(\frac{\|b\|_2 \|c\|_2}{\|b\|_2 + \|c\|_2}) \delta_{bc} 
    }_2}\\
    & \geq  \frac{\norm{a + b}_2^2 - \norm{a + b}_2 \qty( \norm{\delta_{bc}}_2 \norm{b}_2 )}
    {\norm{a + b}_2 + \norm{b}_2 \norm{\delta_{bc}}_2}\\
    & \geq \frac{\norm{a + b}_2 - \norm{b}_2 \norm{\delta_{bc}}_2}
{\norm{a + b}_2 + \norm{b}_2 \norm{\delta_{bc}}_2} \norm{a + b}_2.
\end{align*}

Therefore, to show {\NAME} provides a larger cosine similarity, 
it suffices to show that 
\begin{equation*}
    \frac{\norm{a + b}_2 - \norm{b}_2 \norm{\delta_{bc}}_2}
{\norm{a + b}_2 + \norm{b}_2 \norm{\delta_{bc}}_2} > \cos(a, a + b),
\end{equation*}
% Equivalently, the $\norm{\delta_{bc}}_2$ needs to satisfies
which is equivalent to show
\begin{equation*}
    \|\delta_{bc}\|_2 < \frac{\|b\|_2 }{\|a + b\|_2 } \qty(\frac{1 - \cos(a, a + b)}{1 + \cos(a, a + b)}). 
\end{equation*}
Note that $\|\delta_{bc}\|_2^2 = 2 - 2\cos(b, c)$, it suffices to show
\begin{equation*}
    \cos(b, c) > 1 - \frac{\norm{b}_2^2}{2 \norm{a + b}_2^2} 
    \qty( \frac{1 - \cos(a, a + b)}{1 + \cos(a, a + b)} )^2.
\end{equation*}
Since $\cos(a, a + b) \leq 1$, this condition holds from Assumption \ref{asmp:gradnorm}. 
This completes our proof. 
\end{proof}



\subsubsection{Our method leads to a smaller perturbation on unrelated knowledge.}
\label{sec: unrelated data}

Now denote our new estimator obatined through either $\ell_\ce$ or $\ell_\NAME$ by $\hat{\theta}^\new_\ce$ or $\hat{\theta}^\new_\NAME$. After updating the model parameters to incorporate new knowledge, it is crucial to assess whether this update introduces significant changes to unrelated data.

Without loss of generality, let $ \zv^\un = (\xv^\un, \yv^\un) $ represent a query-answer pair, where $ \xv^\un $ is an unrelated query and $ \yv^\un $ is its corresponding predicted answer. To ensure good \textit{locality}, the predicted distribution on $ \zv^\un $ should remain unchanged against modifications introduced by the update, ensuring that the model’s behavior on unaffected regions of the data distribution is preserved. That means we want to compare $\qty| \pi_{\hat{\theta}^\new_\ce} (\zv^\un) 
- \pi_{\hat{\theta}^\old} (\zv^\un) |$ with $\qty| \pi_{\hat{\theta}^\new_\NAME} (\zv^\un) 
- \pi_{\hat{\theta}^\old} (\zv^\un) |$.

Now, treating $ Z^\un = (X^\un, Y^\un) $ as a random vector following a certain distribution, we define
\begin{equation*}
W \triangleq \nabla_\theta \pi_\theta (Z^\un) \Big|_{\theta = \hat{\theta}^\old}.
\end{equation*}
Since $ W $ is a function of $ Z^\un $, it is also a random vector. In particular, we introduce the following assumption.

\begin{assumption}
\label{asm: un-dist}
Assume that $ \frac{W}{\|W\|_2} $ and $ \|W\|_2 $ are independent. Furthermore, assume that
\begin{equation*}
\frac{W}{\|W\|_2}  \sim \mathcal{U}(\mathbb{S}^{d-1}),
\end{equation*}
where $ \mathcal{U}(\mathbb{S}^{d-1}) $ denotes the uniform distribution on the unit sphere in $ \mathbb{R}^d $ with $d$ denoting the dimensionality of the parameter space.
\end{assumption}

\begin{remark}
Since it represents the gradient of the loss evaluated on unrelated data, we lack any prior information about $ W $. Given that, we assume that $ \frac{W}{\|W\|_2} $ is isotropically distributed.
\end{remark}

Recall the definition of $a, b, c$ given by Eq \eqref{eq: abc definition},
we define $\kappa_R = \frac{\norm{c}_2}{\norm{a}_2}$. 

\begin{assumption}
\label{asm: norm ratio of a and c}
We assume that $\kappa_R < 1$.
\end{assumption}
\begin{remark}
[Interpretation of the Assumption \ref{asm: norm ratio of a and c}]
    As shown in Eq. \eqref{eq: c detail} and Eq. \eqref{eq: baseline direct}:
    \begin{align*}
        a &= - \sum_{i=1}^m \nabla_\theta \log \pil (y_i^\new \mid \cv_i^\new),\\
        c &= - \sum_{i=1}^m \sum_{y \in S^{(i)}} \pif(y \mid \cv_i^\new) \nabla_\theta \log \pi_\theta(y \mid \cv_i^\new) \Big|_{\theta = \hat{\theta}^\old}.
    \end{align*}
    
    This implies that $c$ is a weighted combination of $a$ and contributions from other values of $y \in S^{(i)}$. Note that at $\hat{\theta}^\old$, given $\cv_i^\new$, when $y \neq y_i^\new$, the other points are closer to $y_i^\old$. Since the loss has already reached its minimum, these other points tend to have smaller gradient norms compared to $y_i^\new$. 
%Recall the logistic model in Remark \ref{rem: logistic model}, 
\end{remark}

%\linjun{need to add interpretation and justifications for these assumptions}

\begin{theorem}
\label{thm: unrelated data}
Let $ Z^\un = (X^\un, Y^\un) $ be a random vector representing unrelated data. Under Assumptions~\ref{asm: un-dist} and~\ref{asm: norm ratio of a and c}, we have
\begin{equation*}
    \E_{Z^\un} \qty[\qty| \pi_{\hat{\theta}^\new_\NAME} (Z^\un) 
- \pi_{\hat{\theta}^\old} (Z^\un) | ] < \E_{Z^\un} \qty[\qty| \pi_{\hat{\theta}^\new_\ce} (Z^\un) 
- \pi_{\hat{\theta}^\old} (Z^\un)| ].
\end{equation*}
This result indicates that updates based on the {\NAME} loss induce smaller deviations in the predicted distribution for unrelated data compared to updates based on the standard CE loss, thereby better preserving \textit{locality}.
\end{theorem}


\begin{proof}
Again let $\hat{\theta}^\old$ denote the pretrained parameters. 
For any new parameters $\tilde \theta^\new$, 
the change of $\pil (\zv^\un)$ when $\theta$ moves from $\hat{\theta}^\old$ to $\tilde \theta^\new$ can be approximated by the first-order Taylor expansion with 
% To approximate the change in $\pil (\zv^\un)$ when $\theta$ changes from the pretrained $\hat{\theta}^\old$ to $\hat{\theta}^\new$ \tcco{Is this the optimal one from above?},
% the first-order Taylor expansion gives us 
% By using Taylor expansion, we have
\begin{equation*}
\pi_{\tilde{\theta}^\new} (\zv^\un) 
- \pi_{\hat{\theta}^\old} (\zv^\un) 
=
\nabla_\theta \pi_\theta (\zv^\un) 
\eval_{\theta = \hat{\theta}^\old}^\top 
\qty( \tilde{\theta}^\new - \hat{\theta}^\old ) 
+ o\qty(\norm{\hat{\theta}^\new - \hat{\theta}^\old}_2).
\end{equation*}
Note that when we perform one step gradient descent, the parameter change can further be expressed by 
\begin{align*}
    \tilde{\theta}^\new - \hat{\theta}^\old = - \alpha \nabla_\theta \ell (z^{\new} ; \hat{\theta}^{\old}),
\end{align*}
where $\ell(z^\new; \theta)$ can be either CE loss or {\NAME} loss, and $\alpha$ denotes the learning rate. 

% % Note that when training with CE loss or {\NAME}, 
% Note that $\hat{\theta}^\new - \hat{\theta}^\old
% = - \alpha \nabla_\theta \ell_k (z^{\new} ; \hat{\theta}^{\old})$, where $k$ could be $\ce$ or $\NAME$.

Then to show {\NAME} leads to smaller perturbation in expectation, it suffices to show that there exists $\lambda \in [0, 1]$ such that 
% we will only need to show there $\exists \lambda \in [0,1]$, s.t. holds with expectation.
\begin{equation*}
    \E \qty[ \qty| a^\top W | ] > \E \qty[ \qty| \lambda a^\top W + (1 - \lambda) c^\top W | ].
\end{equation*}
By triangle inequality, we only need to show
\begin{equation*}
    \E \qty[ \qty| a^\top W | ] > \E \qty[ \qty| c^\top W | ].
\end{equation*}
Finally, by Assumption \ref{asm: un-dist}, $\frac{W}{\|W\|_2} \sim \mathcal{U}(\mathbb{S}^{d-1})$ and $ \frac{W}{\|W\|_2} $ and $ \|W\|_2 $ are independent, we have
\begin{equation*}
    \frac{\E \qty[ \qty| c^\top \frac{W}{\|W\|_2} | \|W\|_2]}{\E \qty[ \qty| a^\top \frac{W}{\|W\|_2} | \|W\|_2 ]} = \frac{\E \qty[ \qty| c^\top \frac{W}{\|W\|_2} | ] \E\qty[\|W\|_2]}{\E \qty[ \qty| a^\top \frac{W}{\|W\|_2} | ] \E\qty[\|W\|_2]} = \kappa_R <1.
\end{equation*}
This completes our proof. 
\end{proof}


\if0
Next, let's study the effect of {\NAME} loss. 
We can show that {\NAME} loss provides a better direction for knowledge editing from the lens of \textit{influence function} \citep{koh2017understanding}. 
Given some old knowledge $z_\old = (\xv, \yv_\old)$, someone wants to update it to $z_\new = (\xv, \yv_\new)$. 
The ideal parameter is $\theta_n$ is given by the following (auto-regressive) retraining, 
\begin{align*}
\min\nolimits_\theta \frac{1}{n} \sum_{i=1}^n \ell_{\ce} (z_i; \theta) - \frac{1}{n} \ell_{\ce} (z_0; \theta) + \frac{1}{n}\ell_{\ce} (z_n; \theta).
\end{align*}
Here we follow \citet{} and incorporate the training data $z$ as another input of the loss function as well.  
We make the following assumptions on the loss function, as in \citep{koh2017understanding}.
% 
\begin{assumption}
Loss $\ell(z, \theta)$ is twice-differentiable, and its Hessian is invertible. 
\end{assumption}
% 

As proved by \citet{},
the optimal direction to update $\theta_0$ towards $\theta_n$ can be approximated by the influence function for
\begin{align*}
\theta_{n, \epsilon} \triangleq \argmin\nolimits_\theta \frac{1}{n} \sum_{i=1}^n \ell_{\ce} (z_i; \theta) - \epsilon (\ell_{\ce} (z_0; \theta) - \ell_{\ce} (z_n; \theta)),
\end{align*}
which is given by 
\begin{align*}
\left. \frac{\dif \theta_{n, \epsilon}}{ \dif \epsilon} \right|_{\epsilon = 0} 
&\approx 
- H^{-1}_\ell (\nabla_\theta \ell_{\ce} (z_n; \theta_0) - \nabla_\theta \ell_{\ce} (z_0; \theta_0))
\triangleq I_{KE}(z).
\end{align*}
% 
Then based on these definitions, 
we have the following theorem.
\begin{theorem}
\label{thm:opt-direct}
Compared with standard CE loss, using {\NAME} loss gives a better approximation of the optimal direction to update parameter given by the influence function. 
% Using {\NAME} loss gives an approximation of the optimal direction to update parameter given by the influence function. 
\end{theorem}

\begin{proof}
% In order to show that {\NAME} gradients aligns better with the optimal direction, note that the Hessian is a constant pre-condition. 

For simplicity, we drop the $\max(\cdot, \epsilon)$ clipping mechanism (by setting $\epsilon = 0$), and assume that $\pit$ mixes $\de{y_i}{}$ and $\pif$. 

First, we note that the old knowledge for training, $z_0 = (\xv, \yv)$, can be unknown. 
To estimate the optimal direction, one common approximation \citep{} is to use the model's generations by drawing $\tilde \yv \sim \pil (\xv)$ and use $\Tilde{z}_0 = (\xv, \tilde \yv)$, i.e., 
\begin{align}
I_{KE}(z) 
\notag &\approx
-H_\ell^{-1} 
\left(
\nabla_\theta \ell_{\ce} (z_n; \theta_0) - 
\E_{\yv \sim \pil(\tilde \yv \mid \xv)} [\nabla_\theta \ell_{\ce} (z_n; \theta_0)]
\right) \\
&\overset{(a)}{\approx} 
-H_\ell^{-1}
\left( 
\nabla_\theta \ell_{\ce} (z_n; \theta_0) - 
\sum_{i=1}^m \E_{\tilde y_i \sim \pil(y \mid \cv_i)}
\left[\nabla_\theta \ce [\de{\tilde y_i}{y} \| \pil(y \mid \cv_i)]\right]
\right),
\label{eq:opt-direct}
\end{align}
where $(a)$ plugs in the auto-regressive loss from teacher-forcing mechanism \citep{}. 

Next, let's check the gradient of {\NAME}. 
Let $S$ denote the collection of \textit{unfiltered} tokens in $\pif$.
According to Eq \eqref{eq:to-ce} and Eq \eqref{eq:final}, we have 
\begin{align*}
\nabla_\theta \ell_{\NAME} (z; \theta) 
&=  
\sum_{i=1}^m \nabla_\theta \ce [\pit^{(i)} \| \pil^{(i)}] \\
&= 
\lambda \sum_{i=1}^m \nabla_\theta \ce [\de{y_i}{} \mid \pil^{(i)}] + (1 - \lambda) \sum_{i=1}^m \nabla_\theta \ce [\pif^{(i)} \mid \pil^{(i)}] \\
% &= 
% \lambda \ell_{\ce} (z; \theta) + ( 1 - \lambda) \sum_{i=1}^m \nabla_\theta \ce [\pif^{(i)} \mid \pil^{(i)}].
% &= 
% \lambda \nabla\theta \ell_{\ce}(z; \theta) + 
% (1 - \lambda) \nabla_\theta \ell_{\ce}(z_f; \theta) \\
&= 
\lambda \nabla\theta \ell_{\ce}(z; \theta) + 
(1-\lambda) \sum_{i=1}^m \left( - \sum_{s\in S} \pif(y_s \mid \cv_i) \nabla_\theta \log \pil (y_s \mid \cv_i) \right) \\
&= 
\lambda \nabla_\theta \ell_{\ce}(z; \theta)
+ 
(1 - \lambda) \sum_{i=1}^m 
\E_{y_s \sim \pif (y \mid \cv_i)} 
\left[ 
- \nabla_\theta \log \pil (y_s \mid \cv_i)
\right] \\
&= 
\lambda \nabla_\theta \ell_{\ce}(z; \theta)
+ 
(1 - \lambda) \sum_{i=1}^m 
\E_{y_s \sim \pif (y \mid \cv_i)} 
\left[ 
\nabla_\theta \ce [ \de{y_s}{y} \| \pil (y \mid \cv_i)]
\right] \\
&= 
\lambda 
\underbrace{\left(
\nabla_\theta \ell_{\ce}(z; \theta)
- \sum_{i=1}^m 
\E_{y_s \sim \pif (y \mid \cv_i)} 
\left[ 
\nabla_\theta \ce [ \de{y_s}{y} \| \pil (y \mid \cv_i)]
\right]
\right)}_{\text{Eq \eqref{eq:opt-direct} Approximation}}
+ \\
& \underbrace{\quad \sum_{i=1}^m 
\E_{y_s \sim \pif (y \mid \cv_i)} 
\left[ 
\nabla_\theta \ce [ \de{y_s}{y} \| \pil (y \mid \cv_i)]
\right]}_{\text{Regularization}}.
% \lambda \nabla_\theta \ell_{\ce}(z; \theta) + 
% (1 - \lambda) \sum_{i=1}^m 
% \E_{y_i \sim \pif (y \mid \cv_i)},
\end{align*}
Our gradient can be seen as a simplified \textit{correction} for direction given by Eq \eqref{eq:opt-direct}. 
First, our loss replaces the full distribution $\pil$ with a better $\pif$, which is less noisy \citep{}.
Second, our loss ignores the second order information by \textit{approximating} the Hessian with the identity matrix. 
Third, since now $z_0$ is approximated, 
to avoid incorrectly extract unrelated knowledge, we also add a \textit{regularization} to maintain the extracted knowledge.
The two losses are weighted by hyper-parameter $\lambda$. 
In comparison, a standard CE loss directly ignores the influence of the old knowledge. 



Conceptually, when $\lambda \rightarrow \infty$, the regularization term will be dropped. Note that in this setting, $\pit$ is no longer a valid distribution, because all $s \in S \backslash \{y_i\}$ will have negative probabilities. However, the ``cross entropy'' gradient still holds and sheds light on how {\NAME} updates the model parameters. While in execution, we fix $\lambda \in [0, 1]$, having $\lambda > 1$ has also been explored in the literature \citep{}.


This analysis can be extended to multi-step updates. To this end, we can treat the updated parameters as the new initial state ``$\theta_0$'', then connect the new directions from Eq \eqref{eq:opt-direct} and {\NAME}.
This completes our proof. 
\end{proof}



To this end, 
consider a scenario where someone wants to finetune an LM parameterized by $\theta$ in order to edit a new knowledge $z = (\xv, \yv)$. 
The finetuning involves some loss $\ell (z, \theta)$ and some regularization $R(\theta)$. 
We show that fine-tuning {\NAME} may lead to \textit{smaller} perturbation on unrelated questions. 

Specifically, 
given some text unrelated to the editing $\xv'$, 
we want to quantify the influence of fine-tuning LM using loss $\ell (z, \theta)$ on the model's answer to $\xv'$, $\pi_\theta(\xv')$.
Let $\theta_0$ denote the parameters before editing. 
% To measure how the fine-tuning will affect $\pi_\theta(\xv')$, 
We compute the parameters change if $\ell(z, \theta)$ is upweighted by $\epsilon$, i.e., 
we are interested in $\theta_\epsilon - \theta_0$, where 
% 
\begin{align*}
\theta_\epsilon = \argmin_\theta \epsilon \ell(z, \theta) + R(\theta).
\end{align*}
% 
Note that when $\epsilon = 0$, the optimal $\theta_{\epsilon = 0}$ will remain unchanged, i.e., $\theta_0$. 
Again, we make the following assumptions on the loss function and regularization. 
% 
\begin{assumption}
\label{asmp:loss-2}
Loss $\ell(z, \theta)$ is twice-differentiable. 
In addition, gradient norm $\| \nabla_\theta \ell(z, \theta) \|$ is positively proportional to the loss value $\ell(z, \theta)$. 
\end{assumption}

\begin{assumption}
\label{asmp:reg}
Regularization $R(\theta)$ has invertible Hessian. 
\end{assumption}

Here we make a few remarks on the two assumptions. 
The (twice) differentiability of $\ell$ is valid based on the parametrization of the LM, and the regularization such as $L_2$ penalty has in fact constant Hessian. 
In terms of the relationship between gradient norm and loss, our insight is based on the finding that underfitted data (with high loss) often affects the training more, so there is a common positive correlation, here we further assume that they are positively proportional. 

Then, 
according to \citet{}, the parameter change is characterized by the \textit{influence function} 
% 
\begin{align*}
 \left. \frac{\dif \theta_\epsilon}{\dif \epsilon} \right|_{\epsilon = 0} 
 \approx - H^{-1}_R(\theta_0) \nabla_\theta \ell(z; \theta_0) \triangleq I_\text{FT}(z), 
\end{align*}
% 
where the existence of inverse Hessian of $R$, $H^{-1}_R$, holds from Asmp \ref{}. 
Then by chain rule, we have 
% 
\begin{align*}
& \left. \frac{\dif \pi_{\theta_\epsilon} (\xv')}{ \dif \epsilon} \right|_{\epsilon = 0} \\
&= 
\left. - \nabla_\theta \pi_{\theta}(\xv'; \theta_{\epsilon}) \frac{\dif \theta_\epsilon}{\dif \epsilon} \right|_{\epsilon = 0} \\
&\approx  
- \nabla_\theta \pi_{\theta}(\xv'; \theta_0) H^{-1}_R(\theta_0) \nabla_\theta \ell(z; \theta_0).
\end{align*}
% 
Therefore,
for any given $\xv'$ and $\theta_0$, the first two terms are deterministic, and 
the speed of $\pi_\theta(\xv')$ deviating from $\pi_{\theta_0}(\xv')$ is determined by the gradient of $\ell(z; \theta_0)$. 

While it is in general infeasible to compute the influence function, 
below theorem reveals that compared to standard CE loss, {\NAME} loss has a smaller gradient norm.


\begin{theorem}
Under Asmp \ref{asmp:loss-2} and Asmp \ref{asmp:reg}, 
training with Eq \eqref{eq:dks-loss} would have smaller gradient norm. 
\end{theorem}


\begin{proof}
Again, for simplicity we drop the $\max(\cdot, \epsilon)$ clipping, and assume that $\pit$ mixes $\de{y_i}{}$ and $\pif$. 

As proved in Thm \ref{thm:opt-direct}, 
the \textit{negative} gradient of {\NAME} loss is given by 
\begin{align*}
- \nabla_\theta \ell_{\NAME} (z; \theta) 
&=  
- \sum_{i=1}^m \nabla_\theta \ce [\pit^{(i)} \| \pil^{(i)}] \\
&= 
- \lambda \sum_{i=1}^m \nabla_\theta \ce [\de{y_i}{} \mid \pil^{(i)}] + (1 - \lambda) \sum_{i=1}^m - \nabla_\theta \ce [\pif^{(i)} \mid \pil^{(i)}] \\
% &= 
% \lambda \ell_{\ce} (z; \theta) + ( 1 - \lambda) \sum_{i=1}^m \nabla_\theta \ce [\pif^{(i)} \mid \pil^{(i)}].
% &= 
% \lambda \nabla\theta \ell_{\ce}(z; \theta) + 
% (1 - \lambda) \nabla_\theta \ell_{\ce}(z_f; \theta) \\
&= 
\lambda \sum_{i=1}^m \nabla_\theta \log \pil(y_i \mid \cv_i) + 
(1-\lambda) \sum_{i=1}^m \left(\sum_{s\in S} \pif(y_s \mid \cv_i) \nabla_\theta \log \pil (y_s \mid \cv_i) \right) \\
&\overset{(a)}{=}
\lambda \sum_{i=1}^m \nabla_\theta \log \pil(y_i \mid \cv_i) + 
(1-\lambda) \sum_{i=1}^m \left(\sum_{s\in S} 
\frac{\pil(y_s \mid \cv_i)}{\sum_{s\in S} \pil(y_s \mid \cv_i)} \frac{\nabla_\theta \pil (y_s \mid \cv_i)}{\pil (y_s \mid \cv_i)} \right) \\
&= 
\lambda \sum_{i=1}^m \nabla_\theta \log \pil(y_i \mid \cv_i) + 
(1-\lambda) \sum_{i=1}^m \left(\sum_{s\in S} 
\frac{\nabla_\theta \pil(y_s \mid \cv_i)}{\sum_{s\in S} \pil(y_s \mid \cv_i)}  \right) \\
&= 
\lambda \sum_{i=1}^m \nabla_\theta \log \pil(y_i \mid \cv_i) + 
(1-\lambda) \sum_{i=1}^m \left(
\frac{\nabla_\theta \sum_{s\in S} \pil(y_s \mid \cv_i)}{\sum_{s\in S} \pil(y_s \mid \cv_i)}  \right) \\
&=
\lambda \sum_{i=1}^m \nabla_\theta \log \pil(y_i \mid \cv_i) + 
(1-\lambda) \sum_{i=1}^m \nabla_\theta \log \pil(y \in Y_S \mid \cv_i).
% 
% \frac{\nabla_\theta \sum_{s\in S} \pil(y_s \mid \cv_i)}{\sum_{s\in S} \pil(y_s \mid \cv_i)}  \right) \\
% \lambda \sum_{i=1}^m \frac{\nabla_\theta  \pil(y_i \mid \cv_i)}{\pil(y_i \mid \cv_i)} + 
% (1-\lambda) \sum_{i=1}^m \left(\sum_{s\in S} 
% \frac{\pil(y_s \mid \cv_i)}{Z} \frac{\nabla_\theta \pil (y_s \mid \cv_i)}{\pil (y_s \mid \cv_i)} \right) \\
% &= 
% \lambda \sum_{i=1}^m  \frac{\nabla_\theta \pil(y_i \mid \cv_i)}{\pil(y_i \mid \cv_i)} + 
% (1-\lambda) \sum_{i=1}^m \left(\sum_{s\in S} 
% \frac{\nabla_\theta \pil(y_s \mid \cv_i)}{Z}  \right).
\end{align*}
%
Here
$S$ denotes the collection of \textit{unfiltered} tokens index in $\pif$, and $Y_S$ denotes the collection of these tokens. For simplicity we further denote $\pil(y \in Y_s \mid \cv_i) = \pil (Y_s \mid \cv_i)$. 
Step $(a)$ holds from the fact that $\pif$ is constructed by filtering out noisy tokens and then renormalizing the unfiltered probabilities from $\pil$ \citep{}.
% Finally, $(b)$ holds from the definition of 

Next, for any $i \in [m]$, 
we note that 
\begin{align*}
&
\left \| \lambda \nabla_\theta \log \pil(y_i \mid \cv_i) + 
(1-\lambda) \nabla_\theta \log \pil (Y_s \mid \cv_i) \right \|  \\
&{\leq} 
\lambda \| \nabla_\theta \log \pil(y_i \mid \cv_i) \| + 
(1 - \lambda) \| \nabla_\theta \log \pil (Y_s \mid \cv_i) \|  \\
&\overset{(a)}{\propto} 
\lambda -\log \pil(y_i \mid \cv_i) + 
(1 - \lambda) - \log \pil (Y_s \mid \cv_i) \\
&\overset{(b)}{\leq} 
\lambda -\log \pil(y_i \mid \cv_i) + ( 1 - \lambda) - \log \pil(y_i \mid \cv_i) \\
&= 
-\log \pil(y_i \mid \cv_i) \\
&\propto 
\| \nabla_\theta \ce[\de{y_i}{} \| \pil^{(i)} ]\|. 
% \lambda  \frac{-\log \pil(y_i \mid \cv_i) }{\pil(y_i \mid \cv_i)}+ 
% (1-\lambda) \frac{ - \log \sum_{s \in S} \pil(y_s \mid \cv_i)}{\sum_{s \in S } \pil(y_s \mid \cv_i)} 
\end{align*}
% 
Here step (a) holds from Asmp \ref{asmp:loss-2},
and step (b) holds from the fact that since $Y_S$ is the \textit{subset} of most-likely tokens, its loss (i.e., $y$ being sampling outside $Y_S$) is expected much smaller than $y_i$, which comes from the new knowledge.
Taking sum over $i \in [m]$ completes our proof. 
\end{proof}

The analysis above shows that the gradient norm of {\NAME} loss in general will be no larger than CE loss, which is expected to have less influence on unrelated knowledge. 
In fact, by keeping a large portion of top tokens in $Y_S$ (i.e., in $\pif$), $\pit(Y_S \mid \cv_i) \approx 1$, the bound can be tighter. 
\fi


\subsection{Connection between {\NAME} and DPO}

We end up this section by the following analysis on the connection between {\NAME} and direct preference optimization \citep{rafailov2024direct}.

\begin{theorem}
Let $\epsilon = 0$, 
then optimizing {\NAME} directly can be seen as optimizing an unbiased estimate of a DPO objective plus some additional KL penalty. 
\end{theorem}


\begin{proof}
From Prop \ref{prop:gen-ce} and Lem \ref{lem:to-ce}, at step $i$, we have the negative loss (objective) to maximize
\begin{align}
- \ell_{\NAME, i}(\theta)
\notag &= 
- \kl [\pit (y \mid \cv_i) \| \pil(y \mid \cv_i)] \\
\notag &= 
- \left(\lambda \ce [\de{y_i}{y} \| \pil (y \mid \cv_i)] + (1 - \lambda) \ce [\pif(y \mid \cv_i) \| \pil (y \mid \cv_i)] \right) \\
&= 
- \lambda \left( \ce [\de{y_i}{y} \| \pil (y \mid \cv_i)] - \ce [\pif(y \mid \cv) \| \pil (y \mid \cv_i)]\right) - \ce [\pif(y \mid \cv_i) \| \pil (y \mid \cv_i)] \\
&=
\lambda \left ( \log \pil (y_i \mid \cv_i) + \ce [\pif(y \mid \cv) \| \pil (y \mid \cv_i)]\right) - \ce [\pif(y \mid \cv_i) \| \pil (y \mid \cv_i)]
\label{eq:dpo-sample}
\end{align}
From the lens of DPO, 
note that the editing knowledge $(\xv, \yv)$ can be seen as a \textit{preferred sample} drawn from unknown $\pi^+$ (e.g., retraining the LM from scratch).
Consequently, 
Eq \eqref{eq:dpo-sample} is in fact an \textit{unbiased estimator} of 
\begin{align*}
& \lambda \left(\underbrace{\E_{y^+ \sim \pi^+(y \mid \cv_i) } [\log \pil (y^+ \mid \cv_i)]}_{\text{Preferred distriibution}} - \E_{y^- \sim \pif(y \mid \cv_i)} [\log \pil(y^- \mid \cv_i)] \right) + \ce [\pif(y \mid \cv_i) \| \pil (y \mid \cv_i)] \\
&= 
\lambda \E_{y^+, y^-}\left [ \frac{\log \pil (y^+ \mid \cv_i)}{\log \pil(y^- \mid \cv_i)} \right ] + \kl [\pif(y \mid \cv_i) \| \pil (y \mid \cv_i)] + C \\
&\overset{(a)}{=} 
\lambda 
\left(\E_{y^+, y^-}\left [ \frac{\log \pil (y^+ \mid \cv_i)}{\log \pil(y^- \mid \cv_i)} - \frac{\log \pif (y^+ \mid \cv_i)}{\log \pif(y^- \mid \cv_i)} \right ] + 
\E_{y^+} [\log \pif (y^+ \mid \cv_i) - \E_{y^-} [\log \pif (y^- \mid \cv_i)]
\right) \\
&\quad
+ \kl [\pif(y \mid \cv_i) \| \pil (y \mid \cv_i)] + C \\
&= 
\lambda \left(\E_{y^+, y^-}\left [ \frac{\log \pil (y^+ \mid \cv_i)}{\log \pil(y^- \mid \cv_i)} - \frac{\log \pif (y^+ \mid \cv_i)}{\log \pif(y^- \mid \cv_i)} \right ] + 
\underbrace{\E_{y^+} [\log \pif (y^+ \mid \cv_i)] - \E_{y^-} [\log \pif (y^- \mid \cv_i)]}_{\text{constant wrt $\theta$}}
\right) \\
&\quad
+ \kl [\pif(y \mid \cv_i) \| \pil (y \mid \cv_i)] + C \\
&= 
{\underbrace{\E_{y^+, y^-}\left [ \lambda \frac{\log \pil (y^+ \mid \cv_i)}{\log \pif (y^+ \mid \cv_i)} - \lambda \frac{\log \pil(y^- \mid \cv_i) }{\log \pif(y^- \mid \cv_i)} \right ]}_{\text{DPO with Clipped ReLU Activation}}} 
+ 
\underbrace{\kl [\pif(y \mid \cv_i) \| \pil (y \mid \cv_i)]}_{\text{Additional Penalty}} + C,
\end{align*}
where the first term incorporates a \textit{preferred} distribution, of which the user-provided new knowledge $y_i$ serves an unbiased estimate.
Step (a) plugs in the log-likelihood ratio between the $(y^+, y^-)$ pair from $\pif$, which is constant with respect to $\theta$ and doesn't affect the objective thereof. 
In the final step, we treat the first term as a token-level DPO objective using current $\pif$ as the \textit{reference} model, and the preference model is given by
\begin{align*}
\pr (y^+ \succ y^- \mid \cv_i) 
&= \text{ClippedReLU}(r(\cv_i, y^+) - r(\cv_i, y^-)),
\end{align*}
where
\begin{align*}    
\text{ClippedReLU}(z) &= \min(\max(z, 0), 1),
\end{align*}
when 
\begin{align*}
0 \leq \lambda \frac{\log \pil (y^+ \mid \cv_i)}{\log \pif (y^+ \mid \cv_i)} - \lambda \frac{\log \pil(y^- \mid \cv_i) }{\log \pif(y^- \mid \cv_i)} \leq 1
\end{align*}
Notably, since our base distribution, $\pif$, is the clipped version of $\pil$, and $\lambda \in [0, 1]$, the difference in probability of $y^+$($y^-$) given $\cv_i$ is expected small, so that $\text{ClippedReLU}(z) = z$ holds. 
Finally, the additional penalty is another term to push $\pil$ stay close to $\pif$ but using a forward form, which has also been explored in preference learning \citep{wang2024beyond}.  

In conclusion, 
{\NAME} can be seen as an unbiased estimator of a special DPO problem. This completes our proof. 
\end{proof}






\section{Implementation Details}
\label{app:implementation}

\subsection{Hyperparameters used in KE}
We present the implementation details of our algorithms.
All of our experiments are run on EasyEdit \citep{wang2024easyedit}. 
In general, we tuned hyperparameters for each KE method basis \textit{using to the base version}, if the default setting from EasyEdit showed noticable inferior performance.
See below for more details.


\textbf{FT-M} used the following hyperparameters:
\begin{itemize}
    \item On ZsRE, Wiki\sub{recent}, Wiki\sub{counterfact}, and WikiBio: default training parameters from EasyEdit for both LLaMA 2 and LLaMA 3. 
    \item On MQuAKE: Layers to tune: {(20,21,22,23,24)}. Learning rate: {1e-3}. Others unchanged. 
\end{itemize}

\textbf{LoRA} used the following hyperparameters:
\begin{itemize}
    \item On ZsRE, Wiki\sub{recent}, Wiki\sub{counterfact}, and WikiBio: default training parameters from EasyEdit for both LLaMA 2 and LLaMA 3. 
    \item On MQuAKE: LoRA rank: {12}. Iteration numbers: {50}. Others unchanged. 
\end{itemize}

\textbf{MELO} used the following hyperparameters:
\begin{itemize}
    \item We set initial radius for each code in the code-book to 60 for LLaMA 2, and 30 for LLaMA 3. 
    Due to the fact that the default choice 0.1 was too small to retrieve any edited parameters for rephrased queries or reasoning. 
    
    \item 
    Others unchanged. 
    
\end{itemize}


\textbf{WISE} used the following hyperparameters:
\begin{itemize}
    \item 
    On {\NAME}, we shrunk activation thresholds by 0.6 in consideration of the milder overfitting from our method. 
    We didn't tune this shrinkage factor so it can be suboptimal. All other parameters used default values from EasyEdit. 

    \item
    We removed data augmentation for better measure HTO influence. This led to significantly faster editing speed (around 5 times speedup). 
\end{itemize}


\textbf{ROME} and \textbf{MEMIT} used default choices from EasyEdit. 


Finally, {\NAME} is tuned on a KE model base and applied to both LLMs. 
We didn't tune hyper-parameters extensively, so below $\epsilon$ and $n$ can be suboptimal. 
\begin{itemize}
    \item FT-M: $\epsilon = 0.01$, $n = 0.5$ for $n \sigma$-filtering, $\lambda = 0.1$ for mixing. 
    \item LoRA: $\epsilon = 0.05$, $n = 0.5$ for $n \sigma$-filtering, $\lambda = 0.1$ for mixing. 
    \item MELO: $\epsilon = 0.05$, $n = 1$ for $n \sigma$-filtering, $\lambda = 0.1$ for mixing. 
    \item WISE: $\epsilon = 0.05$, $n = 1$ for $n \sigma$-filtering, $\lambda = 0.1$ for mixing. 
\end{itemize}


\subsection{MQuAKE Experiment Details}


MQuAKE benchmark follows a different evaluation pipeline for 1-Hop and 2-Hop reasoning questions \citep{zhong2023mquake,wang2024deepedit} that checks the existence of ground truth answer in LLM's generation.
Our evaluation rubric followed \citet{zhong2023mquake}. 
We noted that the reliability of evaluation results heavily relies on the use of a good prompt, our prompts are given below.

\begin{itemize}
\item \textbf{1-Hop questions}:  we used 1-shot prompting to guide the model provide answers directly, the complete prompt is 

\noindent\fbox{%
    \parbox{0.8\textwidth}{%
    You are a helpful AI assistant. Answer questions directly.

    Always format your response as:
    
    Final answer: [concise and direct final answer]

    Question: Who is the spouse of the head of state in United States of America?

    Answer: Jill Biden

    Question: \textit{ \# 1-Hop question related to the new knowledge \#}

    Answer: 
    }%
}

\item \textbf{2-Hop questions}:  Again we used 1-shot prompting to guide the model provide answers based on chain-of-thought~\citep{wei2022chain}, the complete prompt is 


\noindent\fbox{%
    \parbox{0.8\textwidth}{%
    You are a helpful AI assistant. For each question:
    
    1. Break it down into simpler subquestions
    
    2. Answer each subquestion step by step. 
    
    3. Use your answers to provide a final answer after "Final answer: "
    
    Always format your response as:
        
    Subquestion: [your subquestion]
    
    Generated answer: [your answer]
    
    Final answer: [concise and direct final answer]
    
    Question: Who is the spouse of the head of state in United States of America?
    
    Subquestion: Who is the head of state in United States of America?
    
    Answer: The head of state in United States of America is Joe Biden.
    
    Subquestion: Who is the spouse of Joe Biden?
    
    Answer: The spouse of Joe Biden is Jill Biden.
    
    Final answer: Jill Biden
    
    Question: \textit{ \# 2-Hop question related to the new knowledge \#}

    }%
}
\end{itemize}

In generation, we set temperature to 0.1. The maximum length was 30 for 1-Hop questions, and 200 for 2-Hop questions. 
\textbf{Chat templates} are applied. 

\section{More Experiment Results}
\label{app:results}


We present the complete Continual Editing results here. 
Note that sequence $T=1$ reduces to Single Edit results, but we present them again for completeness. 




\begin{table*}[htb!]
\definecolor{verylightgray}{gray}{0.9}

\centering
% \small
\caption{
Continual Editing performance (LLaMA 2). 
WISE requires additional irrelevant data for training, which is only available in ZsRE benchmark. 
}
\label{tab:con-llama2}
\resizebox{0.95\linewidth}{!}{%{}
\renewcommand{\tabcolsep}{4pt}
\begin{tabular}{
>{\bfseries}r 
ccccc c 
cccc c 
cccc c 
ccc c 
}
\toprule[0.4ex]

& \multicolumn{5}{c}{\bf ZsRE} && \multicolumn{4}{c}{\bf Wiki\sub{recent}} && \multicolumn{4}{c}{\bf Wiki\sub{counterfact}} && \multicolumn{3}{c}{\bf WikiBio} \\




\midrule[0.2ex]
& \multicolumn{19}{c}{\bf $T=1$} \\
\cmidrule[0.2ex]{2-20}
% \midrule
&\bf Rel. &\bf Gen. &\bf Por. &\bf Loc. &\bf Avg. &&\bf Rel. &\bf Por. &\bf Loc. &\bf Avg. &&\bf Rel. &\bf Por. &\bf Loc. & \bf Avg. &&\bf Rel. &\bf Loc. &\bf Avg. \\
\cmidrule{2-6} \cmidrule{8-11} \cmidrule{13-16} \cmidrule{18-20}
ROME  & 96.61 & 83.91 & 55.7 & 96.96 & 83.3  && 99.02 & 54.21 & 55.91 & 69.71  && 97.2 & 56.85 & 50.4 & 68.15  && 96.41 & 59.14 & 77.78 \\
MEMIT & 94.22 & 88.2 & 57.91 & 98.28 & 84.65  && 97.71 & 52.93 & 55.05 & 68.56  && 96.38 & 59.34 & 45.7 & 67.14  && 93.78  & 56.74 & 75.26 \\
\noalign{\vskip 0.2ex}\cdashline{2-20}\noalign{\vskip 0.2ex}


FT-M & 99.75 & 99.33 & 54.32 & 93.01 & 86.60 && 100.0 & 62.93 & 45.92 & 69.62 && 100.0 & 74.7 & 54.86 & 76.52 && 100.0 & 90.04 & 95.02 \\
\rowcolor{gray!15}
+ Ours & 99.75 & 96.8 & 57.08 & 96.54 & 87.54 &&  100.0 & 63.91 & 60.4 & 74.77 &&   100.0 & 73.62 & 75.34 & 82.99 &&  100.0 & 93.46 & 96.73 \\
\noalign{\vskip 0.2ex}\cdashline{2-20}\noalign{\vskip 0.2ex}


LoRA & 100.0 & 100.0 & 23.34 & 30.44 & 63.45 &&  100.0 & 55.41 & 28.29 & 61.23 &&  100.0 & 71.92 & 9.99 & 60.64 &&  100.0 & 48.84 & 74.42 \\
\rowcolor{gray!15}
+ Ours & 100.0 & 94.31 & 61.16 & 87.2 & 85.67 &&  100.0 & 63.67 & 58.72 & 74.13 &&  100.0 & 73.96 & 57.85 & 77.27 &&  97.68 & 68.45 & 83.06 \\
\noalign{\vskip 0.2ex}\cdashline{2-20}\noalign{\vskip 0.2ex}


MELO & 100.0 & 96.77 & 27.11 & 92.35 & 79.06 &&  99.13 & 54.04 & 40.96 & 64.71 &&  99.0 & 71.78 & 55.83 & 75.54 &&  99.97 & 80.77 & 90.37 \\
\rowcolor{gray!15}
+ Ours  & 100.0 & 93.31 & 50.36  & 97.2 & 85.22 &&  100.0 & 60.25 & 66.48 & 75.58 &&  99.91 & 71.81 & 78.09 & 83.27 &&  99.68 & 82.58 & 91.13 \\
\noalign{\vskip 0.2ex}\cdashline{2-20}\noalign{\vskip 0.2ex}


WISE & 92.42 & 70.86 & 54.57 & 100.0 & 79.46 && - & - & - & -  && - & - & - & - && - & - & - \\
\rowcolor{gray!15}
+ Ours & 97.55 & 76.09 & 54.17 & 100.0 & 81.95 && - & - & - & -  && - & - & - & - && - & - & - \\


\midrule[0.2ex]
& \multicolumn{19}{c}{\bf $T=10$} \\
\cmidrule[0.2ex]{2-20}
% \midrule
&\bf Rel. &\bf Gen. &\bf Por. &\bf Loc. &\bf Avg. &&\bf Rel. &\bf Por. &\bf Loc. &\bf Avg. &&\bf Rel. &\bf Por. &\bf Loc. & \bf Avg. &&\bf Rel. &\bf Loc. &\bf Avg. \\
\cmidrule{2-6} \cmidrule{8-11} \cmidrule{13-16} \cmidrule{18-20}
ROME  & 74.94  &  69.67  &  51.12  &  71.72  &  66.86  && 98.14  &  55.16  &  54.73  &  69.34  && 86.17  &  47.36  &  38.99  &  57.51  && 40.55  & 25.98  &  33.27 \\
MEMIT & 68.39  &  66.26  &  46.66  &  84.22  &  66.38  && 96.51  &  54.2  &  52.56  &  67.76  && 89.64  &  54.71  &  38.2  &  60.85  && 52.2  &  38.54  &  45.37   \\
\noalign{\vskip 0.2ex}\cdashline{2-20}\noalign{\vskip 0.2ex}


FT-M & 89.14 & 87.43 & 47.13 & 84.26 & 76.99 &&  97.4 & 56.47 & 41.4 & 65.09 &&  96.41 & 70.32 & 42.44 & 69.72 &&  92.96 & 77.69
 & 85.32 \\
 \rowcolor{gray!15}
 + Ours & 92.8 & 88.21 & 55.74 & 91.06 & 81.95 &&  96.42 & 61.65 & 53.13 & 70.40 &&  98.72 & 72.47 & 65.46 & 78.88 &&  95.26 & 84.43 
 & 89.84 \\
\noalign{\vskip 0.2ex}\cdashline{2-20}\noalign{\vskip 0.2ex}


LoRA & 29.25 & 30.41 & 19.83 & 24.81 & 26.07 &&  35.17 & 23.8 & 24.98 & 27.98 &&  22.64 & 13.87 & 10.24 & 15.58 &&  70.45 & 46.82
 & 58.64 \\
\rowcolor{gray!15}
+ Ours & 85.4  & 81.5 & 61.03 & 74.41 & 75.59 &&  94.55 & 59.16 & 49.09 & 67.60 &&  71.61 & 51.91 & 32.65 & 52.06 &&  74.74 & 48.35 
 & 61.55 \\
\noalign{\vskip 0.2ex}\cdashline{2-20}\noalign{\vskip 0.2ex}


MELO & 94.13 & 83.06 & 50.48 & 96.5 & 81.04 &&   91.73 & 53.02 & 81.09 & 75.28 &&  92.52 & 64.55 & 99.98 & 85.68 &&  95.44 & 97.94
 & 96.69 \\
 \rowcolor{gray!15}
+ Ours & 94.38 & 81.89 & 54.92 & 98.41 & 82.40 &&  91.69 & 54.95 & 93.22 & 79.95 &&  93.49 & 63.36 & 99.98 & 85.61 &&  95.24 & 97.77
 & 96.50 \\
 \noalign{\vskip 0.2ex}\cdashline{2-20}\noalign{\vskip 0.2ex}


WISE & 84.5 & 73.81 & 53.19 & 100.0 & 77.88 && - & - & - & -  && - & - & - & - && - & - & - \\
\rowcolor{gray!15}
+ Ours & 86.68 & 77.24 & 54.0 & 100.0 & 79.48 && - & - & - & -  && - & - & - & - && - & - & - \\


\midrule[0.2ex]
& \multicolumn{19}{c}{\bf $T=100$} \\
\cmidrule[0.2ex]{2-20}
% \midrule
&\bf Rel. &\bf Gen. &\bf Por. &\bf Loc. &\bf Avg. &&\bf Rel. &\bf Por. &\bf Loc. &\bf Avg. &&\bf Rel. &\bf Por. &\bf Loc. & \bf Avg. &&\bf Rel. &\bf Loc. &\bf Avg. \\
\cmidrule{2-6} \cmidrule{8-11} \cmidrule{13-16} \cmidrule{18-20}
ROME  & 25.37  &  22.68  &  4.73  &  5.1  &  14.47  && 24.99  &  13.12  &  8.55  &  15.56  && 0.0  &  0.0  &  0.0  &  0.0  && 2.63  & 15.74  &  9.18 \\
MEMIT & 2.58  &  2.88  &  0.24  &  2.5  &  2.05  && 70.22  &  41.12  &  38.43  &  49.92  && 0.82  &  0.97  &  0.26  &  0.69  && 0.0  & 15.74  &  7.87  \\
\noalign{\vskip 0.2ex}\cdashline{2-20}\noalign{\vskip 0.2ex}



FT-M & 88.36 & 84.51 & 41.76 & 54.11 & 67.19 &&  97.51 & 53.73 & 33.88 & 61.71 &&  95.69 & 66.23 & 26.69 & 62.87 &&  93.56 & 67.51  & 80.53 \\
\rowcolor{gray!15}
+ Ours & 89.38 & 82.13 & 52.69 & 72.39 & 74.15 &&  96.32 & 58.28 & 47.04 & 67.21 &&  95.93 & 68.16 & 44.28 & 69.46 &&  95.35 & 74.91 & 85.13 \\
\noalign{\vskip 0.2ex}\cdashline{2-20}\noalign{\vskip 0.2ex}



LoRA & 0.67 & 0.78 & 1.00 & 0.03 & 0.62 &&  0.5 & 0.5 & 0.12 & 0.37 &&  0.67 & 0.0 & 0.0 & 0.22 &&  47.02 & 27.06 & 37.04 \\
\rowcolor{gray!15}
+ Ours & 62.23 & 58.06 & 56.62 & 59.57 & 59.12 &&  70.49 & 47.05 & 49.87 & 55.80 &&  32.17 & 28.99 & 29.19 & 30.12 &&  52.96 & 25.73 & 39.34 \\

MELO & 38.13 & 36.12 & 53.88 & 98.08 & 56.55 &&  26.33 & 24.98 & 53.73 & 35.01 &&  24.87 & 24.21 & 78.71 & 42.60 &&  48.88 & 97.61 & 48.88 \\
\rowcolor{gray!15}
+ Ours & 39.13 & 37.28 & 54.75 & 98.58 & 57.44 &&  47.95 & 39.65 & 86.77 & 58.12 &&  24.92 & 25.39 & 97.12 & 49.14 &&  52.17 & 97.44 & 74.81 \\
\noalign{\vskip 0.2ex}\cdashline{2-20}\noalign{\vskip 0.2ex}


WISE & 84.59 & 71.59 & 54.45 & 100.0 & 77.66 && - & - & - & -  && - & - & - & - && - & - & - \\
\rowcolor{gray!15}
+ Ours & 92.42 & 84.22 & 56.71 & 100.0 & 83.34  && - & - & - & -  && - & - & - & - && - & - & - \\


\bottomrule[0.4ex]
\end{tabular}
}
\vspace{-0.2cm}
\end{table*}








\begin{table*}[htb!]
\definecolor{verylightgray}{gray}{0.9}

\centering
% \small
\caption{
Continual Editing performance (LLaMA 3). 
WISE requires additional irrelevant data for training, which is only available in ZsRE benchmark. 
}
\label{tab:con-llama3}
\resizebox{0.95\linewidth}{!}{%{}
\renewcommand{\tabcolsep}{4pt}
\begin{tabular}{
>{\bfseries}r 
ccccc c 
cccc c 
cccc c 
ccc c 
}
\toprule[0.4ex]

& \multicolumn{5}{c}{\bf ZsRE} && \multicolumn{4}{c}{\bf Wiki\sub{recent}} && \multicolumn{4}{c}{\bf Wiki\sub{counterfact}} && \multicolumn{3}{c}{\bf WikiBio} \\


\midrule[0.2ex]
& \multicolumn{19}{c}{\bf $T=1$} \\
\cmidrule[0.2ex]{2-20}
&\bf Rel. &\bf Gen. &\bf Por. &\bf Loc. &\bf Avg. &&\bf Rel. &\bf Por. &\bf Loc. &\bf Avg. &&\bf Rel. &\bf Por. &\bf Loc. & \bf Avg. &&\bf Rel. &\bf Loc. &\bf Avg. \\
% & Rel. & Gen. & Por. & Loc. & Avg. && Rel. & Por. & Loc. & Avg. && Rel. & Por. & Loc. & Avg. && Rel. & Loc. & Avg. \\
\cmidrule{2-6} \cmidrule{8-11} \cmidrule{13-16} \cmidrule{18-20}

ROME  & 99.17 & 97.91 & 58.12 & 95.9 & 87.78  && 98.84 & 54.76 & 49.74 & 67.78  && 99.94 & 58.0 & 42.94 & 66.96  && 92.43  & 72.63 & 82.53 \\
MEMIT & 96.67 & 92.46 & 58.78 & 98.23 & 86.53  && 98.51 & 53.65 & 48.45 & 66.87  && 99.44 & 57.81 & 42.73 & 66.66  && 96.26  & 71.23 & 83.75  \\
\noalign{\vskip 0.2ex}\cdashline{2-20}\noalign{\vskip 0.2ex}

FT-M & 100.0 & 99.75 & 40.43 & 79.43 & 79.90 &&  100.0 & 57.13 & 30.01 & 62.38 &&  100.0 & 72.62 & 31.47 & 68.03 &&  100.0  & 92.96 & 96.48 \\
\rowcolor{gray!15}
+  Ours & 100.0 & 99.75 & 48.63 & 94.78 & 85.79 &&  100.0 & 60.88 & 44.67 & 68.52 &&  100.0 & 73.5 & 58.29 & 77.26 &&  99.99  & 94.87 & 97.43 \\
\noalign{\vskip 0.2ex}\cdashline{2-20}\noalign{\vskip 0.2ex}

LoRA & 100.0 & 100.0 & 26.55 & 38.85 & 66.35 &&  100.0 & 52.99 & 26.46 & 59.82 &&  100.0 & 71.1 & 9.02 & 60.04 && 100.0 & 59.77 & 79.88 \\
\rowcolor{gray!15}
+ Ours & 100.0 & 98.5 & 51.57 & 93.13 & 85.80 &&  100.0 & 61.46 & 56.1 & 72.52 &&  100.0 & 72.8 & 57.54 & 76.78 && 98.16 & 77.24 & 87.7 \\
\noalign{\vskip 0.2ex}\cdashline{2-20}\noalign{\vskip 0.2ex}

MELO & 100.0 & 96.84 & 39.63 & 98.8 & 83.82 &&  100.0 & 59.07 & 65.78 & 74.95 &&  100.0 & 71.55 & 87.77 & 86.44 && 100.0  & 98.56 & 99.28 \\
\rowcolor{gray!15}
+ Ours & 100.0 & 95.77 & 43.08 & 98.8 & 84.41 &&  100.0 & 58.72 & 69.1 & 75.94 &&  100.0 & 70.26 & 89.81 & 86.69 && 99.98  & 98.56 & 99.27 \\
\noalign{\vskip 0.2ex}\cdashline{2-20}\noalign{\vskip 0.2ex}

WISE    & 71.67 & 51.29 & 49.27 & 100.0 & 68.06  && - & - & - & -  && - & - & - & - && - & - & - \\
\rowcolor{gray!15}
+ Ours  & 82.67 & 62.34 & 47.54 & 100.0 & 73.14  && - & - & - & -  && - & - & - & - && - & - & - \\





\midrule[0.2ex]
& \multicolumn{19}{c}{\bf $T=10$} \\
\cmidrule[0.2ex]{2-20}
&\bf Rel. &\bf Gen. &\bf Por. &\bf Loc. &\bf Avg. &&\bf Rel. &\bf Por. &\bf Loc. &\bf Avg. &&\bf Rel. &\bf Por. &\bf Loc. & \bf Avg. &&\bf Rel. &\bf Loc. &\bf Avg. \\
% & Rel. & Gen. & Por. & Loc. & Avg. && Rel. & Por. & Loc. & Avg. && Rel. & Por. & Loc. & Avg. && Rel. & Loc. & Avg. \\
\cmidrule{2-6} \cmidrule{8-11} \cmidrule{13-16} \cmidrule{18-20}

ROME  & 43.91  &  40.14  &  25.11  &  31.7  &  35.22  && 91.17  &  51.25  &  43.67  &  62.03  && 86.52  &  45.37  &  32.9  &  54.93  && 4.01  & 7.58  &  5.79 
 \\
MEMIT & 59.74  &  58.36  &  37.34  &  71.06  &  56.62  && 98.38  &  54.42  &  47.08  &  66.63  && 98.61  &  58.48  &  36.28  &  64.46  && 5.4  & 1.61  &  3.5 
\\
\noalign{\vskip 0.2ex}\cdashline{2-20}\noalign{\vskip 0.2ex}

FT-M & 79.54 & 78.44 & 25.03 & 43.97 & 56.75 &&  87.22 & 48.12 & 25.8 & 53.71 &&  90.13 & 62.37 & 13.83 & 55.44 &&  95.59  & 87.45
 & 91.52 \\
 \rowcolor{gray!15}
+  Ours & 84.74 & 81.41 & 44.2 & 75.67 & 71.50 &&  92.77 & 52.65 & 38.99 & 61.47 &&  93.04 & 66.5 & 39.99 & 66.51 &&  96.81  &  91.17
 & 93.99 \\
 \noalign{\vskip 0.2ex}\cdashline{2-20}\noalign{\vskip 0.2ex}

LoRA & 18.54 & 17.55 & 6.63 & 6.56 & 12.32 &&  21.7 & 13.66 & 11.97 & 15.78 &&  12.59 & 5.92 & 0.69 & 6.40 && 51.09  & 44.45  &  47.77 \\
\rowcolor{gray!15}
+  Ours & 73.28 & 72.39 & 53.13 & 69.36 & 67.04 &&  93.68 & 56.97 & 49.34 & 66.66 &&  71.99 & 49.52 & 32.24 & 51.25 && 64.26 & 55.11  &  59.69 \\
\noalign{\vskip 0.2ex}\cdashline{2-20}\noalign{\vskip 0.2ex}

MELO & 94.08 & 80.47 & 47.97 & 98.8 & 80.33 &&  92.56 & 54.51 & 86.58 & 77.88 &&  92.97 & 63.74 & 98.3 & 85.00 && 94.77  & 98.56  &  96.67\\
\rowcolor{gray!15}
+ Ours & 94.08 & 80.94 & 49.77 & 98.8 & 80.90 &&  91.56 & 54.24 & 89.16 & 78.32 &&  92.97 & 62.69 & 98.32 & 84.66 && 94.91  &  98.56  &  96.74 \\
\noalign{\vskip 0.2ex}\cdashline{2-20}\noalign{\vskip 0.2ex}

WISE  & 51.14 & 43.36 & 51.0 & 100.0 & 61.38  && - & - & - & -  && - & - & - & - && - & - & - \\
\rowcolor{gray!15}
+ Ours & 58.21 & 53.22 & 49.21 & 100.0 & 65.16 && - & - & - & -  && - & - & - & - && - & - & - \\


\midrule[0.2ex]
& \multicolumn{19}{c}{\bf $T=100$} \\
\cmidrule[0.2ex]{2-20}
&\bf Rel. &\bf Gen. &\bf Por. &\bf Loc. &\bf Avg. &&\bf Rel. &\bf Por. &\bf Loc. &\bf Avg. &&\bf Rel. &\bf Por. &\bf Loc. & \bf Avg. &&\bf Rel. &\bf Loc. &\bf Avg. \\
% & Rel. & Gen. & Por. & Loc. & Avg. && Rel. & Por. & Loc. & Avg. && Rel. & Por. & Loc. & Avg. && Rel. & Loc. & Avg. \\
\cmidrule{2-6} \cmidrule{8-11} \cmidrule{13-16} \cmidrule{18-20}

ROME  & 7.18  &  6.02  &  1.04  &  2.24  &  4.12  && 8.89  &  1.36  &  0.31  &  3.52  && 3.92  &  0.99  &  0.0  &  1.64  && 0.88  &  7.47  &  4.18 \\
MEMIT & 0.0  &  0.0  &  0.0  &  0.0  &  0.0  && 0.57  &  0.92  &  0.4  &  0.63  && 0.81  &  0.86  &  0.0  &  0.56  && 0.01  & 23.44  &  11.73 \\
\noalign{\vskip 0.2ex}\cdashline{2-20}\noalign{\vskip 0.2ex}

FT-M & 78.79 & 78.29 & 13.7 & 15.42 & 46.55 &&  94.27 & 44.09 & 22.99 & 53.78 &&  87.47 & 55.62 & 2.78 & 48.62 &&  93.65  & 85.83
 & 89.74 \\
\rowcolor{gray!15}
+  Ours & 81.2 & 77.87 & 32.65 & 44.66 & 59.09 &&  96.19 & 53.73 & 32.42 & 60.78 &&  92.97 & 62.02 & 20.71 & 58.57 &&  94.23  &  85.83 
 & 94.23 \\
\noalign{\vskip 0.2ex}\cdashline{2-20}\noalign{\vskip 0.2ex}

LoRA & 1.75 & 1.81 & 1.29 & 2.13 & 1.74 &&  1.33 & 1.58 & 0.93 & 1.28 &&  1.00 & 0.00 & 0.00 & 0.33 && 15.88  & 17.61  &  16.74 \\
\rowcolor{gray!15}
+  Ours & 51.38 & 50.3 & 49.72 & 35.83 & 46.81 &&  64.82 & 42.92 & 44.27 & 50.67 &&  25.31 & 20.18 & 17.49 & 20.99 && 19.03  & 10.9  &  14.96  \\
 \noalign{\vskip 0.2ex}\cdashline{2-20}\noalign{\vskip 0.2ex}

MELO & 29.79 & 28.83 & 50.01 & 98.8 & 51.86 &&  36.71 & 29.02 & 83.23 & 49.65 &&  22.2 & 22.9 & 97.85 & 22.55 && 52.19  & 98.56  &  75.37  \\
\rowcolor{gray!15}
+ Ours & 29.79 & 28.73 & 50.01 & 98.8 & 51.83 &&  40.42 & 34.85 & 92.67 & 55.98 &&  22.45 & 22.9 & 97.85  & 47.73  && 52.15  &  98.56  &  75.36 \\
 \noalign{\vskip 0.2ex}\cdashline{2-20}\noalign{\vskip 0.2ex}

WISE & 84.87 & 74.87 & 39.24 & 100.0 & 74.75  && - & - & - & -  && - & - & - & - && - & - & - \\
\rowcolor{gray!15}
+ Ours  & 86.83 & 77.54 & 34.99 & 100.0 & 74.84  && - & - & - & -  && - & - & - & - && - & - & - \\






\bottomrule[0.4ex]
\end{tabular}
}
\vspace{-0.2cm}
\end{table*}

