\section{Overfitting Issue in Knowledge Editing}
\label{sec:problem}

This section presents a new token-dependent overfitting phenomenon in knowledge editing (KE) that has been overlooked in the literature.
Background of KE is also provided. 

\subsection{Preliminaries}

% Language models

% Knowledge editing

% Forgetting problem and over optimziation in knowledge editing 


Given a text
$\xv = (x_1, \dots, x_n)$, where each $x_i \in \mathcal V$ is a token from vocabulary $\mathcal V$, 
a large language model (LLM) parameterized by $\theta$ computes probability $\pi_\theta(\xv)$ based on chain rule~\citep{bengio2000neural}:
\begin{align}
    \pi_\theta(\xv) 
    &= \prod_{i=1}^n \pi_\theta (x_i \mid {x_1, \dots, x_{i-1}}) 
    \triangleq \prod_{i=1}^n \pi_\theta (x_i \mid \xv_{<i}), 
    % \label{eq:lm}
    \notag
\end{align}
where $\pi_\theta(x_i \mid \xv_{<i} )$ is the predicted distribution of token $x_i$ given previous $\xv_{<i}$. 
The LLM is usually trained with maximum likelihood estimation~\citep{hochreiter1997long,sutskever2014sequence,cho2014learning}. 
%$ In specific, for an $L$-layer LM, let $\hv_i^{(l)}$ denote the intermediate \textit{representation} of the $i$-th token at the $l$-th layer. The predicted distribution is given by softmax regression parameterized by $\Wmat$ at layer $L$:
% \begin{align*}
%     \pi_\theta(x_i \mid \xv_{<i}) = \text{softmax}(\textbf W \boldsymbol h^{(L)}_i).
% \end{align*}
To generate a sentence $\xv$, the LLM computes $\pi_\theta(x_i \mid \xv_{<i})$ and draws $x_i$ from it; then $x_i$ is combined with $\xv_{<i}$ as new inputs for future steps. 
This process completes if a special token that marks the end of the sentence is returned, or if the maximum length is reached.



\textbf{Knowledge Editing (KE)} aims to update specific knowledge in a pre-trained LLM while preserving unrelated others. 
A knowledge can be represented by natural language $(\xv, \yv)$, $\xv$ describes the \textit{subject} and \textit{relation}, and $\yv$ entails corresponding \textit{object}. 
For instance, suppose $\xv$ is \textit{The president of United States is}, $\yv$ can be \textit{Donald Trump}. 
KE asks the LLM to respond given $\xv$ with new $\yv$, 
while satisfying the following criteria meanwhile~\citep{zhang2024comprehensive}:
(1) \textbf{Generality:} the edited model should generalize to all equivalent inquires about the \textit{US president}.
(2) \textbf{Portability:} questions reasoned from the new knowledge such as \textit{the first lady of United States} should be answered correctly.
(3) \textbf{Locality:} {unrelated} knowledge such as \textit{the prime minister of Canada} should be unchanged. 
These requirements of precisely updating specific knowledge proves non-trivial~\citep{wang2023knowledge,zhang2024comprehensive}. 




\subsection{Overfitting in Knowledge Editing}
In response to {precise} KE requirements, 
existing attempts restrict the updates to only a minimal amount of parameters.
This design establishes remarkable progress in maintaining good locality~\citep{zhang2024comprehensive,wang2024wise}.
However, it proves insufficient to maintain good generalizability (generalilty and {portability}) due to the so-called \textit{overfitting} issue~\citep{zhong2023mquake,zhang2024uncovering}. 


Namely,
many KE tasks involve one piece of new knowledge at a time,
requiring to fine-tune an LLM on single training instance. 
In such challenging scenarios, 
the LLM often encounters severe overfitting even only a few parameters are updated. 
This greatly restricts its ability to {generalize} the edited knowledge. 
As shown in \citet{zhong2023mquake,zhang2024uncovering}, 
edited LLMs usually pay excessive attention to the edited subject, 
but fail to address multi-hop reasoning questions involving the new knowledge.
As a result, this limitation results in suboptimal {portability}.


\begin{figure}[htb!]
\captionsetup[subfigure]{font=footnotesize,labelformat=parens,labelfont=footnotesize}
% \captionsetup{font=footnotesize}
\def\subfigwidth{0.4\linewidth}
\centering
\begin{subfigure}[t]{\subfigwidth}
    \includegraphics[width=\linewidth]{figures/lora_gen.pdf}
\end{subfigure}%
% \hfill%
\begin{subfigure}[t]{\subfigwidth}
    \includegraphics[width=\linewidth]{figures/lora_por.pdf}
\end{subfigure}%

\caption{
Loss (average) change of ground truth answers to \textit{generality} (rephrased, left)
and \textit{portability} (reasoning, right) questions. 
}
\label{fig:overfit}
\vspace{-0.2cm}
\end{figure}

As a direct evidence, Fig \ref{fig:overfit} shows the change of generality and portability loss\footnote{The perplexity loss of the ground truth answer to a question.} at different iterations from fine-tuning LLaMA2 7B~\citep{touvron2023llama} with LoRA, a representative KE baseline method~\citep{zhang2024comprehensive}. 
As the training goes on, the generality loss decreases. However, the portability loss decreases at the beginning of training, but starts to increase later.
This confirms the existence of overfitting.
More importantly, 
the ultimate portability loss is significantly larger than before editing, indicating that 
\textit{the reasoning ability is in fact undermined by the KE process}, 


\subsection{Heterogeneous Token Overfitting}



\begin{figure}[htb!]
\captionsetup[subfigure]{font=footnotesize,labelformat=parens,labelfont=footnotesize}
% \captionsetup{font=footnotesize}
\def\subfigwidth{0.4\linewidth}
\centering
\begin{subfigure}[t]{\subfigwidth}
    \includegraphics[width=\linewidth]{figures/lora_initloss.pdf}
    \caption{Initial loss.}
    \label{fig:init-loss}
\end{subfigure}%
% \hfill%
\begin{subfigure}[t]{\subfigwidth}
    \includegraphics[width=\linewidth]{figures/lora_overfit.pdf}
    \caption{Underfitting degree (UD).}
    \label{fig:hto}
\end{subfigure}%

\caption{
Token-level initial loss and UD (negative indicates overfitted).
Dashed lines mark the mean values.
}
% \label{fig:hto}
\vspace{-0.2cm}
\end{figure}


% 
Towards a deeper understanding of this overfitting phenomenon, 
we check the loss of each token,
and find that \textit{different tokens tend to have distinct initial loss values}.
As depicted in Fig \ref{fig:init-loss}, 
{before} editing LLaMA2, only {certain} tokens (e.g., the beginning) have significant loss values. 
On the other hand, some tokens take small loss value and are \textit{initially-fitted} by nature.
As an intuitive explanation, consider the previous \textit{US president} example.
No matter a user wants to edit the answer to \textit{Donald Trump} or \textit{Joe Biden}, after seeing the first word \textit{Donald} or  \textit{Joe} as a \textit{hint}, 
the LLM is expected to be capable of infer the remaining part based on its pretrained knowledge. 

Nonetheless, 
existing KE methods overlook this token-level difference.
Consequently, they tend to overfit {tokens} that have varied losses at different speeds.
For verification, we compute the \textit{pre-edited} log-likelihood of tokens generated by the model with greedy decoding, and that of the editing instance during the KE process.
We define \textit{underfitting degree} (UD) as the difference between the pre-edited and running log-likelihood, negative UD indicates an overfitting.
Fig \ref{fig:hto} shows UD of different tokens when half of them are overfitted. 
Strong pattern of UD varies across different tokens confirms our concern. 
We dub this issue as \textit{heterogeneous token overfitting} (HTO) of KE.

\setlength{\intextsep}{12pt} % Default space above and below figures
\setlength{\columnsep}{20pt} % Default space between columns


% The training paradigm is a direct cause of HTO. 
HTO's direct cause lies in the training paradigm.
Formally, given editing instance $(\xv, \yv = [y_1, \dots, y_m])$ where $\yv$ contains $m$ tokens, 
many KE methods resort to a conventional LLM training objective\footnote{We restrict our study to the widely-used \textit{teacher-forcing} mechanism~\citep{lamb2016professor}.}.
In particular, they seek to maximize likelihood of $\pil (\yv \mid \xv)$ by minimizing an \textit{averaged} cross-entropy (CE) loss with gradient descent on
\begin{align}
\label{eq:ce-loss}
\ell_{\ce}(\theta) 
&\triangleq
\sum_{i=1}^m \ce[\de{y_i}{y} \| \pi_\theta(y \mid \xv \oplus \yv_{<i} )] \\
&= 
-\sum_{i=1}^m  \log \pi_\theta (y_i \mid \cv_i) \notag \\ 
\notag 
\nabla_\theta \ell_\ce(\theta) 
&=
- \sum_{i=1}^m \nabla_\theta \log \pil (y_i \mid \cv_i). 
\end{align}
% 
Here $\cv_i = \xv \oplus \yv_{<i}$ denotes the context for token $y_i$, $\de{y_i}{y}$ is the Kronecker delta function%
\footnote{$\delta_{y_i}(y) = 1$ if $y=y_i$ else 0.},
and $\ce[\cdot \| \cdot]$ computes CE between two distributions. 

During training, gradient $\nabla_\theta \ell_{\ce} (\theta)$ maximizes the probability of $y_i$ whiling minimizing the probabilities of all other candidates.
When the model is repeatedly updated using gradient(s) from the \textit{single} datapoint, as in KE, 
the probabilities of \textit{initially-fitted} tokens become disproportionately large, 
while tokens with high initial loss values are gradually fitted.
% If the model further needs to be updated with the gradient(s) from this \textit{single} data repeatedly, as in KE, 
% the probabilities of \textit{initially-fitted} tokens will be made disproportionately large during the course of fitting those have large initial loss values.
That is to say, HTO lies in \textit{indiscriminately} optimizing CE loss of \textit{all} tokens, without considering their difference.
Existing attempts for mitigating overfitting such as early stopping~\citep{yao2007early} and label smoothing~\citep{szegedy2016rethinking,muller2019does} also ignore this token-level difference, making them conceptually less suitable for HTO. 


\section{Propose Method}
\label{sec:method}


Given the importance of token-level difference in HTO, 
we propose {\NAME} to offer a granular control that applies to various KE methods, theoretical analysis is also provided.  

\subsection{Counteract HTO with {\NAME}}

We present {\NAME}, a token-level strategy for HTO mitigation. 
Our method \textit{smooths} $\yv$'s distribution for fitting in an adaptive way. 
Specifically,
we replace
each delta distribution $\de{y_i}{y}$ with a unique smoothed \textit{target} distribution $\pit(y \mid \cv_i)$, 
and refine the cross entropy by a clipped \textit{forward} KL divergence.
Our complete loss is given by 
\begin{align}
\label{eq:dks-loss}
\ell_{\NAME}(\theta)
&\triangleq 
\sum_{i=1}^m \max(\kl [\pit (y \mid \cv_i) \| \pi_\theta(y \mid \cv_i)], \epsilon),
\end{align}
% 
where clipped $\max(\cdot, \epsilon)$ imposes a \textit{token-level} early stopping when predicted $\pil$ is close enough to $\pit$. 
% Here $\pit$ is constructed from $\pil$ and is not trainable, this makes our solution efficient.

\textbf{Principles of $\pit$ design.}
We note that two principles should be met in order to make $\pit$ a good distribution to target on. 
First,  $\pit$ should convey that ground truth token $y_i$ is most probable, otherwise, the objective may lead to incorrect knowledge. 
Second, compared to uniform prior that smooths all tokens equally, 
the model's own pre-trained knowledge is a better prior to help mitigate forgetting problem~\citep{zhang2020self,lee2022adaptive}.  


In light of the two principles, we use
$\de{y_i}{}$ and the LLM's \textit{current} knowledge from its predicted distribution $\pil$ to construct target $\pit$.
However, as will be verified later, directly use $\pil$ can be suboptimal due to the non-negligible noise it carries~\citep{hewitt2022truncation,tang2024top}. 
Specifically, \citet{tang2024top} argued that $\pil$ mixes a distinct subset of \textit{informative} tokens, and a subset of \textit{noisy} tokens associating with small \textit{logits} that fall outside $n\sigma$-distant away from the maximal value. 
By filtering out noisy tokens in $\pil$, the LLM performance can be boosted at inference time. 
We bring this insight to the training (editing) phase and mix the \textit{filtered} distribution\footnote{For brevity $\pif^{(i)} = \pif( y \mid \cv_i)$, $\pit^{(i)}$ is defined similarly. Plain $\pif$ and $\pit$ will be used when discussing the general idea.} $\pif^{(i)}$ with $\de{y_i}{}$ by
\begin{align}
\label{eq:mix}
% \footnotesize
\pit^{(i)} \triangleq
\begin{cases}
\pit^\text{can} \triangleq \lambda \de{y_i}{} + (1 - \lambda) \pif^{(i)} & \text{if $y_i = \argmax_y \pit^\text{can}$}, \\[1ex]
\de{y_i}{} & \text{otherwise},
\end{cases}
\end{align}
% 
% 
where $\lambda$ is a hyper-parameter. 
Namely, we adopt the candidate mixture $\pit^\text{can}$ if it correctly assigns the maximal probability to $y_i$, otherwise, we \textit{skip} the mixing and use $\de{y_i}{}$. 
This \textit{skip} mechanism helps reduce potential knowledge conflicts by discarding $\pif^{(i)}$ (from $\pil$) when it heavily relies on outdated knowledge, which often happens in the first few training steps, empirical benefit is shown in Sec \ref{sec:exp:ablation}. 
Algo \ref{alg:ours} outlines the process of our solution. 


\begin{algorithm}[!t]
\caption{OVERTONE Training Paradigm}
\label{alg:ours}
\begin{algorithmic}[1]
\STATE \textbf{Input:}
Editing data $(\xv, \yv = [y_1, \dots, y_m])$, 
LM parameters $\theta_0$, 
mixing hyper-parameter $\lambda$,
early-stopping threshold $\epsilon$, 
filtering threshold $n$,
total training steps $T$. 

\STATE \textbf{Initialize: }
$\theta = \theta_0$.

\FOR{$t=1, \dots, T$} 

\STATE
\textit{{\# Inner loop is parallelized in practice, unroll for better readability.}}
\FOR{$i=1, \dots, m$}

\STATE
Set context $\cv_i = \xv \oplus \yv_{<i}$. 


\STATE 
Compute logits from the LM as $\sv^{(i)} = f_\theta(\cv_i) \in \R^{|\mathcal V|}$.
Take softmax and get $\pil^{(i)}$. 

\STATE
Top $n\sigma$-filter~\citep{tang2024top}:
Compute $s^{(i)}_{\max} = \max_k \sv^{(i)}$, $\sigma = \text{std}(\sv^{(i)})$. 
Define filtered logit $\tilde s^{(i)}_k = -\infty$ if $s^{(i)}_k \leq s^{(i)}_{\max} - n \sigma$ else $\tilde s^{(i)}_k = s^{(i)}_k$. 

\STATE
Take softmax on filtered $\tilde \sv$ and get filtered $\pif^{(i)}$.

\STATE
Compute target $\pit^{(i)}$ based on Eq \eqref{eq:mix}. 


\STATE
Compute loss 
\begin{align*}
    \ell_{\NAME}^{(i)} = 
    \max(\kl[\pit^{(i)} \| \pil^{(i)}], \epsilon).
\end{align*}

\ENDFOR

\STATE
Compute sample loss 
\begin{align*}
    \ell_{\NAME}(\theta) = \sum_{i=1}^m \ell_{\NAME}^{(i)}. 
\end{align*}

\STATE
Update with learning rate $\alpha$
\begin{align*}
    \theta \leftarrow \theta - \alpha \nabla_\theta \ell_{\NAME}(\theta)
\end{align*}

\ENDFOR 
\OUTPUT Edited parameter $\theta$. 
\end{algorithmic}
\end{algorithm}











\subsection{Theoretical Advantages of {\NAME}}

This section provides theoretical analysis on key factors that merit {\NAME} for KE.
All proofs and more in-depth technical background are deferred to App \ref{app:proof}. 

\textbf{Merit 1. {\NAME} is universal and efficient.}

% We start with the connection between our Eq \eqref{eq:dks-loss} and CE loss.
% As shown in the propositions below, 
While seemingly distinct, {\NAME} is in fact a generalization of CE loss.
Moreover, our choice of $\pit$ makes it computationally efficient, with computation overhead negligible compared to LLM forward operation.
\begin{proposition}
\label{prop:gen-ce}
{\NAME} loss generalizes CE loss and reduces to the latter when $\epsilon = 0, \lambda = 1$.
\end{proposition}
% 
% In addition, our choice of $\pit$ makes {\NAME} computationally efficient, as shown in the proposition below.
% Notably, the additional complexity negligible compared to a forward operation.
\begin{proposition}
\label{prop:efficiency}
Using Alg \ref{alg:ours}, the additional computation complexity induced by {\NAME} is $\mathcal O(|\mathcal V|)$ when fitting a token, where $|\mathcal V|$ is the vocabulary size. 
\end{proposition}

\textbf{Merit 2. {\NAME} provides better updates.}

{\NAME} leads to more effective parameter updates, as demonstrated through the lens of the influence function~\citep{koh2017understanding}, outlined in the following informal theorem. Due to page limitations, the formal version and corresponding assumptions are deferred to Appendix \ref{app:proof:better}.

\begin{theorem}[Informal] \label{thm:better} Under regularity conditions, compared to optimizing the vanilla CE loss, {\NAME} provides a more favorable update direction for the parameters and has less influence on unrelated knowledge. \end{theorem}








% \begin{theorem}[Informal]
% \label{thm:better}
% \tc{Under regularity conditions,
% compared with optimizing CE loss, 
% {\NAME} gives a better update direction of parameters, and has less influence on unrelated knowledge
% compared with optimizing CE loss.
% }
% \end{theorem}
% \begin{theorem}[Informal]
% \label{thm:better}
% % \tcco{Feel free to update accordingly} 
% \tc{Under regular conditions on standard CE loss and fine-tuning regularizer acting on the LM. 
% Compared with optimizing CE loss, 
% {\NAME} gives a better approximation of the optimal update direction of parameters towards the retraining one, and has less influence on unrelated knowledge, 
% as per influence function.}
% \end{theorem}



\textbf{Merit 3. {\NAME} has close connection to DPO and other constrained optimizations.}

One might question whether {\NAME} is conceptually superior to constrained optimization approaches, such as fine-tuning only a small set of specific parameters~\citep{dong2022calibrating,dai2021knowledge}, limiting update magnitudes~\citep{zhu2020modifying}, or employing low-rank updates~\citep{hu2021lora}.
We emphasize that {\NAME} introduces a new objective that can be solved with any optimization methods, regardless of whether constraints are imposed. 
In other words, {\NAME} can be seamlessly combined with existing constrained optimization-based solutions for KE.


Below theorem  draws a connection between {\NAME} and direct preference optimization (DPO), which has shown superior performance of maintaining pretrained knowledge in LLM post-training~\citep{wang2023making}.
% 
\begin{theorem}
Let $\epsilon = 0$, 
optimizing {\NAME} can be seen as optimizing an unbiased estimate of a DPO objective plus some additional KL penalty. 
\end{theorem}
% 
Compared with conducting explicit DPO, {\NAME} does not require collecting preference data, and is more efficient thereof. 
Furthermore, as highlighted in \citet{rozner2024knowledge},
another challenge of applying DPO to KE is that determining \textit{win-loss} data pairs can be unstraightforward in KE. 
In contrast, {\NAME}  walks around this challenge by refraining from treating any token as \textit{unpreferred}, and instead acts on a distribution level. 


