
\section{Experiments}
\label{sec:experiment}



We evaluate the proposed {\NAME} paradigm on four performant KE methods applying to two representative large language models (LMs) over five benchmarking datasets. 
Ablation studies are also conducted to help understand its effectiveness. 
Results show that {\NAME} helps improve editing performance by a large margin on all methods. 


\subsection{Experiment Setup}


\textbf{Base Models.}
We conduct experiments on {two} representative LMs, \textbf{LLaMA 2-7b-Chat}~\citep{touvron2023llama} and \textbf{LLaMA 3-8b-Instruct}~\citep{dubey2024llama3}, which have been widely studied in the literature~\citep{zhang2024comprehensive, wang2024wise}. 
From now on, we refer to the two LMs as \textbf{LLaMA 2} and \textbf{LLaMA 3} for brevity. 




\textbf{Tasks.}
Following \citet{wang2023knowledge, zhang2024comprehensive}, 
we edit different kinds of knowledge: WikiData\sub{recent}, WikiData\sub{counterfact}~\citep{cohen2024evaluating}, WikiBio~\citep{hartvigsen2024aging}, and ZsRE~\citep{yao2023editing}. 
Besides the four popular benchmarks, 
we also explore more complex MQuAKE~\citep{zhong2023mquake,wang2024deepedit}.
Due to page limitation, we refer readers to \citet{zhang2024comprehensive} for more benchmark details.
When editing an LLM, we consider two scenarios: 
(1) \textbf{Single Editing}: one piece of knowledge is edited at a time. 
(2) \textbf{Continual Editing}: multiple pieces of knowledge are edited in a sequential way. This is more challenging due to forgetting and knowledge conflicting~\citep{hartvigsen2024aging, wang2024wise}. 



\textbf{Editing Methods.}
We apply {\NAME} to four representative KE methods from different families that have achieved state-of-the-art performance~\citep{zhang2024comprehensive, wang2024easyedit}.
\textbf{FT-M}~\citep{zhang2024comprehensive}
fine-tunes a special layer identified by causal-tracing analysis wherein the knowledge is stored.
\textbf{LoRA}~\citep{hu2021lora}
learns additive low-rank updates for model parameters on the new knowledge. 
\textbf{MELO}~\citep{yu2024melo} and \textbf{WISE}~\citep{wang2024wise} 
incorporates additional parameter copies to learn new knowledge, along with some gating mechanism to determine whether original or new knowledge should be used at inference time.
Despite incorporating certain explicit or implicit constraints on the learnable parameters, these methods are all trained to minimize the CE loss.  
For better benchmarking, we also report results from two widely-studied methods \textbf{ROME}~\citep{meng2022locating} and \textbf{MEMIT}~\citep{meng2022mass}.
ROME applies a causal-tracing analysis to identify the layer wherein the knowledge is stored and then solves an analytic rank-one update,
and 
MEMIT extends ROME by identifying a series of layers to edit and finding the updates as least-squares solutions. 
\textit{To reflect the challenging nature of KE under data scarcity regime, 
we focus on KE methods that {do not require a larges-scale hard-to-access training data, or training additional models}. 
No data augmentation were applied during the editing.}



\textbf{Evaluation Criteria.}
We evaluate the performance from four aspects as discussed in Sec \ref{sec:problem}: \textbf{reliability (Rel.)}, \textbf{generality (Gen.)}, \textbf{portability (Por.)}, and \textbf{locality (Loc.)}.
Due to page limits we refer readers to \citet{zhang2024comprehensive, wang2024wise} for their formulations. 
We report the average of different metrics for more complete comparisons. 



\textbf{Implementation Details.}
All of our experiments are implemented in EasyEdit~\citep{wang2024easyedit}.
More details and hyper-parameters can be found in App \ref{app:implementation}.




\subsection{Single Editing Performance}
\label{sec:exp:single}

We evaluate the effectiveness of {\NAME} in conducting Single Editing 
on ZsRE, WikiData\sub{recent}, WikiData\sub{counterfact}, and WikiBio with different KE methods. 
WISE was tested on ZsRE, the only benchmark that contains additional irrelevant data during the editing time that is required by WISE. 

Single Editing results are reported in Tab \ref{tab:single}. 
From the table, 
all KE methods gained significant improvement from the proposed {\NAME} paradigm.
Specifically, 
The four methods hardly performed comparable to baselines ROME and MEMIT from normal training,
but were capable of exceeding them when trained with {\NAME}. 
For instance, 
without {\NAME}, 
ROME achieved the highest and the second-highest average performance for editing LLaMA 2 and LLaMA 3 respectively on Wiki\sub{recent}. 
However, 
when equipped with {\NAME}, FT-M, LoRA, and MELO outperformed ROME on both tasks. 

We next check where the improvement was made.
from the table, 
the first gain was from improved portability. 
% significant gains were obtained in portability and locality without hurting the editing success, {echoing well with our motivation and theoretical analysis}. 
To see this, 
note that when editing LLaMA 2 on ZsRE, 
LoRA reached a portability that was nearly {three times} of the base version. 
Similarly, MELO also reached an almost doubled portability. 
More evidence can be found from editing LLaMA 3 as well. 
In addition, 
all methods, especially those initially fall short in maintaining good locality, achieved excellent performance in this regard.
As an evidence, 
LoRA's reached a nearly five times locality improvements when editing both LLaMA 2 and LLaMA 3 on Wiki\sub{counterfact}. 
We want to highlight that, all these improvements were made \textit{without compromising editing reliability}.
That is to say, 
all the four methods achieved better trade-offs between reliability and reasoning (and locality) from the proposed {\NAME}.
More importantly, this success was established in a \textit{model-agnostic} manner, 
in the sense that {\NAME} is not specialized for any particular KE method studied here. 
Instead, it offers a highly flexible and generic paradigm that can be combined with existing solutions in a plug-and-play manner. 


\textbf{More Complex Editing task}. 
To further evaluate how {\NAME} performs on complex benchmark in the filed of KE, 
we test FT-M and LoRA with editing the two LLMs on MQuAKE-2002~\citep{wang2024deepedit}\footnote{This is a cleaned version of MQuAKE by fixing knowledge conflicts~\citep{wang2024deepedit}.}, following \citet{zhong2023mquake}. 
This task requires the edited LLM to answer one- and two-hops reasoning questions about the edited knowledge. 
% Our prompts and setup details are provided in App \ref{app:implementation}.
Experiment results are reported in Table \ref{tab:mquake}. 
As before, {\NAME} was capable of achieving better portability without hurting the editing performance. 



These empirical results echo well with our theoretical analysis, and confirm the superiority of {\NAME}.




\begin{table*}[htb!]
\definecolor{verylightgray}{gray}{0.9}

\centering
% \small
\caption{
Single Editing performance. 
Four KE methods gained improvement from {\NAME} training paradigm.
WISE requires additional irrelevant data for training, which is only available in ZsRE benchmark. 
}
\label{tab:single}
\resizebox{0.95\linewidth}{!}{%{}
\renewcommand{\tabcolsep}{4pt}
\begin{tabular}{
>{\bfseries}r 
ccccc c 
cccc c 
cccc c 
ccc c 
% cccc>{\columncolor{verylightgray}}c c 
% ccc>{\columncolor{verylightgray}}c c 
% ccc>{\columncolor{verylightgray}}c c 
% cc>{\columncolor{verylightgray}}c c 
% ccc>{\columncolor{verylightgray}}c 
}
\toprule[0.4ex]

& \multicolumn{5}{c}{\bf ZsRE} && \multicolumn{4}{c}{\bf Wiki\sub{recent}} && \multicolumn{4}{c}{\bf Wiki\sub{counterfact}} && \multicolumn{3}{c}{\bf WikiBio} \\




\midrule[0.2ex]
& \multicolumn{19}{c}{\bf LLaMA 2-7b-chat} \\
\cmidrule[0.2ex]{2-20}
% \midrule
&\bf Rel. &\bf Gen. &\bf Por. &\bf Loc. &\bf Avg. &&\bf Rel. &\bf Por. &\bf Loc. &\bf Avg. &&\bf Rel. &\bf Por. &\bf Loc. & \bf Avg. &&\bf Rel. &\bf Loc. &\bf Avg. \\
\cmidrule{2-6} \cmidrule{8-11} \cmidrule{13-16} \cmidrule{18-20}
ROME  & 96.61 & 83.91 & 55.7 & 96.96 & 83.3  && 99.02 & 54.21 & 55.91 & 69.71  && 97.2 & 56.85 & 50.4 & 68.15  && 96.41 & 59.14 & 77.78 \\
MEMIT & 94.22 & 88.2 & 57.91 & 98.28 & 84.65  && 97.71 & 52.93 & 55.05 & 68.56  && 96.38 & 59.34 & 45.7 & 67.14  && 93.78  & 56.74 & 75.26 \\
\noalign{\vskip 0.2ex}\cdashline{2-20}\noalign{\vskip 0.2ex}


FT-M & 99.75 & 99.33 & 54.32 & 93.01 & 86.60 && 100.0 & 62.93 & 45.92 & 69.62 && 100.0 & 74.7 & 54.86 & 76.52 && 100.0 & 90.04 & 95.02 \\
\rowcolor{gray!15}
+ Ours & 99.75 & 96.8 & 57.08 & 96.54 & 87.54 &&  100.0 & 63.91 & 60.4 & 74.77 &&   100.0 & 73.62 & 75.34 & 82.99 &&  100.0 & 93.46 & 96.73 \\
\noalign{\vskip 0.2ex}\cdashline{2-20}\noalign{\vskip 0.2ex}


LoRA & 100.0 & 100.0 & 23.34 & 30.44 & 63.45 &&  100.0 & 55.41 & 28.29 & 61.23 &&  100.0 & 71.92 & 9.99 & 60.64 &&  100.0 & 48.84 & 74.42 \\
\rowcolor{gray!15}
+ Ours & 100.0 & 94.31 & 61.16 & 87.2 & 85.67 &&  100.0 & 63.67 & 58.72 & 74.13 &&  100.0 & 73.96 & 57.85 & 77.27 &&  97.68 & 68.45 & 83.06 \\
\noalign{\vskip 0.2ex}\cdashline{2-20}\noalign{\vskip 0.2ex}


MELO & 100.0 & 96.77 & 27.11 & 92.35 & 79.06 &&  99.13 & 54.04 & 40.96 & 64.71 &&  99.0 & 71.78 & 55.83 & 75.54 &&  99.97 & 80.77 & 90.37 \\
\rowcolor{gray!15}
+ Ours  & 100.0 & 93.31 & 50.36  & 97.2 & 85.22 &&  100.0 & 60.25 & 66.48 & 75.58 &&  99.91 & 71.81 & 78.09 & 83.27 &&  99.68 & 82.58 & 91.13 \\
\noalign{\vskip 0.2ex}\cdashline{2-20}\noalign{\vskip 0.2ex}


WISE & 92.42 & 70.86 & 54.57 & 100.0 & 79.46 && - & - & - & -  && - & - & - & - && - & - & - \\
\rowcolor{gray!15}
+ Ours & 97.55 & 76.09 & 54.17 & 100.0 & 81.95 && - & - & - & -  && - & - & - & - && - & - & - \\




\midrule[0.2ex]
& \multicolumn{19}{c}{\bf LLaMA 3-8b-Instruct} \\
\cmidrule[0.2ex]{2-20}
&\bf Rel. &\bf Gen. &\bf Por. &\bf Loc. &\bf Avg. &&\bf Rel. &\bf Por. &\bf Loc. &\bf Avg. &&\bf Rel. &\bf Por. &\bf Loc. & \bf Avg. &&\bf Rel. &\bf Loc. &\bf Avg. \\
% & Rel. & Gen. & Por. & Loc. & Avg. && Rel. & Por. & Loc. & Avg. && Rel. & Por. & Loc. & Avg. && Rel. & Loc. & Avg. \\
\cmidrule{2-6} \cmidrule{8-11} \cmidrule{13-16} \cmidrule{18-20}

ROME  & 99.17 & 97.91 & 58.12 & 95.9 & 87.78  && 98.84 & 54.76 & 49.74 & 67.78  && 99.94 & 58.0 & 42.94 & 66.96  && 92.43  & 72.63 & 82.53 \\
MEMIT & 96.67 & 92.46 & 58.78 & 98.23 & 86.53  && 98.51 & 53.65 & 48.45 & 66.87  && 99.44 & 57.81 & 42.73 & 66.66  && 96.26  & 71.23 & 83.75  \\
\noalign{\vskip 0.2ex}\cdashline{2-20}\noalign{\vskip 0.2ex}

FT-M & 100.0 & 99.75 & 40.43 & 79.43 & 79.90 &&  100.0 & 57.13 & 30.01 & 62.38 &&  100.0 & 72.62 & 31.47 & 68.03 &&  100.0  & 92.96 & 96.48 \\
\rowcolor{gray!15}
+  Ours & 100.0 & 99.75 & 48.63 & 94.78 & 85.79 &&  100.0 & 60.88 & 44.67 & 68.52 &&  100.0 & 73.5 & 58.29 & 77.26 &&  99.99  & 94.87 & 97.43 \\
\noalign{\vskip 0.2ex}\cdashline{2-20}\noalign{\vskip 0.2ex}

LoRA & 100.0 & 100.0 & 26.55 & 38.85 & 66.35 &&  100.0 & 52.99 & 26.46 & 59.82 &&  100.0 & 71.1 & 9.02 & 60.04 && 100.0 & 59.77 & 79.88 \\
\rowcolor{gray!15}
+ Ours & 100.0 & 98.5 & 51.57 & 93.13 & 85.80 &&  100.0 & 61.46 & 56.1 & 72.52 &&  100.0 & 72.8 & 57.54 & 76.78 && 98.16 & 77.24 & 87.7 \\
\noalign{\vskip 0.2ex}\cdashline{2-20}\noalign{\vskip 0.2ex}

MELO & 100.0 & 96.84 & 39.63 & 98.8 & 83.82 &&  100.0 & 59.07 & 65.78 & 74.95 &&  100.0 & 71.55 & 87.77 & 86.44 && 100.0  & 98.56 & 99.28 \\
\rowcolor{gray!15}
+ Ours & 100.0 & 95.77 & 43.08 & 98.8 & 84.41 &&  100.0 & 58.72 & 69.1 & 75.94 &&  100.0 & 70.26 & 89.81 & 86.69 && 99.98  & 98.56 & 99.27 \\
\noalign{\vskip 0.2ex}\cdashline{2-20}\noalign{\vskip 0.2ex}

WISE    & 71.67 & 51.29 & 49.27 & 100.0 & 68.06  && - & - & - & -  && - & - & - & - && - & - & - \\
\rowcolor{gray!15}
+ Ours  & 82.67 & 62.34 & 47.54 & 100.0 & 73.14  && - & - & - & -  && - & - & - & - && - & - & - \\

\bottomrule[0.4ex]
\end{tabular}
}
\vspace{-0.2cm}
\end{table*}








\begin{table}[htb!]
\definecolor{verylightgray}{gray}{0.9}

\centering
% \small
\caption{
Editing performance on MQuAKE.
}
\label{tab:mquake}
\resizebox{0.6\linewidth}{!}{%{}
\renewcommand{\tabcolsep}{3pt}
\begin{tabular}{
>{\bfseries}r 
cccc c 
cccc
}
\toprule[0.4ex]

& \multicolumn{4}{c}{\bf LLaMA 2-7b-chat} && \multicolumn{4}{c}{\bf LLaMA 3-8b-Instruct} \\

\midrule[0.2ex]
\bf & \bf Rel. &\bf 1-Hop. & \bf 2-Hop. & \bf Avg. &&\bf Rel. &\bf 1-Hop. &\bf 2-Hop. &\bf Avg. \\
\cmidrule{2-5} \cmidrule{7-10}

FT-M & 100.0 & 83.0 & 30.0 & 71.0 && 100.0 & 82.0 & 24.0 & 68.67\\
\rowcolor{gray!15}
+ Ours & 99.86 & 89.0 & 37.0 & 75.29 && 100.0 & 85.0 & 30.0 & 71.67 \\
\noalign{\vskip 0.2ex}\cdashline{2-10}\noalign{\vskip 0.2ex}

LoRA & 100.0 & 95.0 & 39.0 & 78.0 && 100.0 & 98.0 & 35.0 & 77.67 \\
\rowcolor{gray!15}
+ Ours & 99.75 & 93.0 & 48.0 & 80.25 && 100.0 & 95.0 & 40.0 & 78.33 \\


\bottomrule[0.4ex]
\end{tabular}
}
\vspace{-0.2cm}
\end{table}


\subsection{Continual Editing Performance}
\label{sec:exp:continual}


We next study the more challenging scenarios, where massive edits are conducted in a continual (sequential) way.
Experiments were again run on the four benchmarks. 

Due to page limit, We defer the complete results to App \ref{app:results}, and visualize the average of reliability, generality, portability, and locality in Fig \ref{fig:continual}. 
Specifically, we evaluate the performance after new $T$ pieces of knowledge length are edited sequentially. 
Different KE methods are represented in separate colors.
Solid boxes indicate normal training performance, and transparent boxes show results from training with {\NAME}. 
The unfilled area within the boxes quantifies the improvements form {\NAME}.


As in Single Editing scenarios,
{\NAME} again improved the performance of four KE methods, 
enabling them to surpass ROME and MEMIT by a large margin across diverse settings.
Furthermore, on three out of the four benchmarks (ZsRE, Wiki\sub{recent}, and Wiki\sub{counterfact}), 
the improvements were even more pronounced when the editing sequence is longer ($T=10, 100$).
Notably, according to our results on ZsRE, 
LoRA (and FT-M) achieved highly competitive continual editing performance when enhanced with {\NAME}, 
on par with specialized continual editing methods like MELO and WISE. 
In contrast, in previous works~\citep{zhang2024comprehensive,wang2024wise}, vanilla LoRA is generally considered unsuitable for continual editing unless significant adaptations are implemented.

To conclude, these results clear demonstrated the flexibility and power of {\NAME} in diverse KE scenarios. 



\begin{figure*}[htb!]
    \captionsetup[subfigure]{font=footnotesize,labelformat=parens,labelfont=footnotesize}
    % \captionsetup{font=footnotesize}
    \def\subfigwidth{0.24\linewidth}
    \centering
    \begin{subfigure}[t]{\subfigwidth}
        \includegraphics[width=\linewidth]{figures/zsre_llama2.pdf}
        \caption{\footnotesize  \bf  LLaMA 2 (ZsRE)}
        \label{fig:subfig-a}
    \end{subfigure}%
    \hfill%
    \begin{subfigure}[t]{\subfigwidth}
        \includegraphics[width=\linewidth]{figures/recent_llama2.pdf}
        \caption{\footnotesize  \bf  LLaMA 2 (Wiki\sub{recent})}
        \label{fig:subfig-b}
    \end{subfigure}%
    \hfill%
    \begin{subfigure}[t]{\subfigwidth}
        \includegraphics[width=\linewidth]{figures/counterfact_llama2.pdf}
        \caption{\footnotesize  \bf LLaMA 2 ( Wiki\sub{counterfact})}
        \label{fig:subfig-c}
    \end{subfigure}
    \hfill%
    \begin{subfigure}[t]{\subfigwidth}
        \includegraphics[width=\linewidth]{figures/wikibio_llama2.pdf}
        \caption{\footnotesize  \bf LLaMA 3 (WikiBio)}
        \label{fig:subfig-d}
    \end{subfigure}

    \vspace{1em}

    \begin{subfigure}[t]{\subfigwidth}
        \includegraphics[width=\linewidth]{figures/zsre_llama3.pdf}
        \caption{\footnotesize  \bf LLaMA 3 (ZsRE)}
        \label{fig:subfig-a2}
    \end{subfigure}%
    \hfill%
    \begin{subfigure}[t]{\subfigwidth}
        \includegraphics[width=\linewidth]{figures/recent_llama3.pdf}
        \caption{\footnotesize \bf LLaMA 3 (Wiki\sub{recent})}
        \label{fig:subfig-b2}
    \end{subfigure}%
    \hfill%
    \begin{subfigure}[t]{\subfigwidth}
        \includegraphics[width=\linewidth]{figures/counterfact_llama3.pdf}
        \caption{\footnotesize \bf  LLaMA 3 (Wiki\sub{counterfact})}
        \label{fig:subfig-c2}
    \end{subfigure}
    \hfill%
    \begin{subfigure}[t]{\subfigwidth}
        \includegraphics[width=\linewidth]{figures/wikibio_llama3.pdf}
        \caption{\footnotesize \bf LLaMA 3 (WikiBio)}
        \label{fig:subfig-d2}
    \end{subfigure}

    \caption{
    Continual Editing performance under different sequence length $T$. 
    Solid and transparent bars show performance with and without {\NAME}. 
    Unfilled area marks the performance gap. 
    ROME and MEMIT didn't use {\NAME}. 
    }
    \label{fig:continual}
\vspace{-0.2cm}
\end{figure*}





\subsection{Ablation Studies}
\label{sec:exp:ablation}

We end this section with an ablation study on {\NAME} to showcase how each component contributes to its final performance.
Results from editing LLaMA 2 on ZsRE with LoRA are presented in Tab \ref{tab:ablation}. 
According to the table, we note the following findings. 
First, pure token-level smoothing (``w/o clip'') increases both portability and locality, confirming that overfiting due to CE loss indeed hurts editing performance. 
Additionally, the way to smooth target distribution plays a critical role: 
using the unedited predicted distributions (``w/o dyn-$\pif$'') leads to significant drop, due to the conflicts raise from the outdated internal knowledge. 
Extra evidence can be seen from (``w/o chk-$\pif$''), where the mixture (Eq \eqref{eq:mix}) is always applied without checking if the probability of label $y_i$ is the largest.
Finally, the noise in predicted distribution $\pil$ also hinders the editing process: 
without filtering them out (``w/o flt-$\pif$''), both generality and portability decreased. 
All empirical results aligns well with our analysis in Sec \ref{sec:method}. 

\begin{table}[htb!]
\definecolor{verylightgray}{gray}{0.9}

\centering
% \small
\caption{
Ablation studies on {\NAME}, 
``{w/o clip}'' sets $\epsilon = 0$,
``{w/o dyn-$\pif$}'' uses unedited prediction,
``{w/o chk-$\pif$}'' always adopt the mixture in Eq \eqref{eq:mix},
``{w/o flt-$\pif$}'' uses full $\pil$ without filtering out tail (noisy) regions.
}
\label{tab:ablation}
\resizebox{0.5\linewidth}{!}{%{}
% \renewcommand{\tabcolsep}{3pt}
\begin{tabular}{
>{\bfseries}r 
cccc c 
cccc
}
\toprule[0.4ex]

& \multicolumn{4}{c}{\bf LLaMA 2-7b-chat} \\

\midrule[0.2ex]
&\bf  Rel. &\bf  Gen. &\bf  Por. &\bf  Loc. &\bf  Avg. \\
\cmidrule{2-6}


LoRA & 100.0 & 100.0 & 23.34 & 30.44 & 63.45 \\
\noalign{\vskip 0.2ex}\cdashline{2-6}\noalign{\vskip 0.2ex}
w/o clip & 100.0  &  99.75  &  26.6  &  41.08  &  66.86 \\
% w/o prior\sub{m} & 0.0  &  0.0  &  6.83  &  23.21  &  7.51 \\ 
% w/ unif. mix\sub{s} & 0.0  &  0.0  &  6.45  &  23.04  &  7.37 \\ 
w/o dyn-$\pif$ & 99.18  &  97.67  &  36.32  &  51.57  &  71.18 \\
w/o chk-$\pif$ & 95.35  &  86.51  &  57.92  &  90.08  &  82.47 \\
w/o flt-$\pif$ & 100.0  &  83.93  &  58.2  &  90.36  &  83.12 \\
\noalign{\vskip 0.2ex}\cdashline{2-6}\noalign{\vskip 0.2ex}
\rowcolor{gray!15}
+ Ours & 100.0 & 94.31 & 61.16 & 87.2 & 85.67 \\


% \noalign{\vskip 0.2ex}\cdashline{2-6}\noalign{\vskip 0.2ex}
% w/ early stop & 100.0  &  98.58  &  52.53  &  81.37  &  83.12 \\
% w/ label smooth & 0.0  &  0.0  &  6.58  &  22.72  &  7.32 \\ 
% 100.0  &  92.31  &  60.48  &  89.68  &  85.61  &&

\bottomrule[0.4ex]
\end{tabular}
}
\vspace{-0.2cm}
\end{table}
