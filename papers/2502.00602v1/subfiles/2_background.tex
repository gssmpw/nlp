\section{Related Works}
\label{sec:background}

% \tcco{Copied from BaFT paper, to be updated. }

Existing KE methods mainly fall into two classes. 

\textbf{Internal Storage} updates model parameters for the adaptation. 
Early studies fine-tuned a LLM directly but suffered from severe forgetting problem~\citep{wang2023knowledge}.
For more precise editing, \citet{zhu2020modifying} imposed a relaxed $\ell_2$ norm constraint on parameter updates, 
and \citet{dong2022calibrating,huang2023transformer} limited the updates to some specific feed-forward network (FFN) layer(s),
based on findings that knowledge is often stored therein~\citep{dai2021knowledge}. 
For further refinement, the \textit{locate-and-edit} paradigm~\citep{meng2022locating, meng2022mass} first identifies the layer storing the knowledge, then modifies its parameters in an analytic form or through least squared solution. 
On the other hand, PEFT methods such as LoRA~\citep{hu2021lora} also provided performance on par with locating-based solutions~\citep{wu2023eva,zhang2024comprehensive}. 
In general, these works primarily focus on identifying a small set of parameters most relevant to the new knowledge, 
However, these approaches are typically trained with instance-level loss, overlooking the token-level differences.
Therefore, they remain susceptible to HTO in a similar manner. 
This work addresses HTO, an orthogonal aspect of the KE process, and complements existing studies in a model-agnostic manner.
% an orthogonal aspect that is complementary to existing studies, in a model-agnostic regard. 
Our {\NAME} is established without assumptions about which parameters are updated,
allowing it to be seamlessly integrated with existing methods without compromising their selective nature.
% This allows one to easily combine {\NAME} with existing methods without hurting their selective nature. 
We validate our approach by showing that {\NAME} enhances the performance of two representative internal stage methods across diverse scenarios.  




\textbf{External Storage} resorts to external memories without updating original parameters. 
This category includes meta-learning-based MEND~\citep{mitchell2021fast} and its multi-task varient InstructEdit~\citep{zhang2024instructedit}, 
in-context learning-based IKE~\citep{zheng2023can}, retrieval-based LTE~\citep{jiang2024learning}, augmentation-based StableKE~\citep{wei2024stable}, and proxy model-based SERAC~\citep{mitchell2022memory}.
Notwithstanding, these methods often require large-scale, \textit{hard-to-access} dataset for retrieval (e.g., IKE, LTE),
or for training auxiliary models (e.g., MEND, InstructEdit, SERAC). 
As a result, their practicality is limited, and they struggle with Continual Editing that needs frequent updates~\citep{wang2024wise}.
Recently, specialized methods for Continual Editing have been proposed.
These approaches introduce adapters (GRACE~\citep{hartvigsen2024aging}), LoRAs (MELO~\citep{yu2024melo}), or weight copies (WISE~\citep{wang2024wise}) to memorize new knowledge, and learn gating mechanism to determine whether to use original or new knowledge.
The gating mechanisms are often learned through additional representation-distance-based codebooks~\citep{yu2024melo} or distinct margin losses~\citep{wang2024wise}, making external storage methods more complex. 
However, like internal storage methods, they optimize editing parameters using instance-level loss functions, ignoring token-level differences.
Consequently, they may also suffer from HTO and can benefit from our {\NAME} framework.
Experiments with two external storage methods demonstrate that our solution can be straightforwardly incorporated to more complex KE methods, highlighting 
the flexibility and versatility of {\NAME}.