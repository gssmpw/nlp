\section{Experiments}

\begin{table*}[tb!]
\centering
\caption{Experiment results of circuit generation accuracy and DAS steps. Impr. is the percentage decrease in DAS steps.}
\label{table:circuitgen_tb}
\vskip 0.05in
\footnotesize
\resizebox{0.888\linewidth}{!}{
\begin{tabular}{c|c|cc|cc|ccc|ccc}
\toprule
\multicolumn{4}{c|}{\textbf{Benchmark}} & \multicolumn{2}{c|}{\textbf{CircuitNN}} & \multicolumn{3}{c|}{\textbf{T-Net}} & \multicolumn{3}{c}{\textbf{CircuitAR-2B}} \\ 
\midrule
\textbf{Category} & \textbf{IWLS} & \textbf{\# PI} & \textbf{\# PO} & \textbf{Acc.(\%)$\uparrow$} & \textbf{Steps$\downarrow$} & \textbf{Acc.(\%)$\uparrow$} & \textbf{Steps$\downarrow$} & \textbf{Impr.(\%)$\uparrow$} & \textbf{Acc.(\%)$\uparrow$} & \textbf{Steps$\downarrow$} & \textbf{Impr.(\%)$\uparrow$} \\ 
\midrule
\multirow{2}{*}{\makecell{Random}} 
& ex00 & 6 & 1 & 100 & 88715 & 100 & 85814 & 3.27 & 100 & 52023 & 41.36 \\
& ex01 & 6 & 1 & 100 & 64617 & 100 & 68686 & -6.30 & 100 & 29636 & 54.14 \\
\midrule
\multirow{3}{*}{\makecell{Basic \\ Functions}} 
& ex11 & 7 & 1 & 100 & 104529 & 100 & 49354 & 52.78 & 100 & 47231 & 54.81 \\
& ex16 & 5 & 5 & 100 & 115150 & 100 & 121108 & -5.17 & 100 & 45434 & 60.54 \\
& ex17 & 6 & 6 & 100 & 90584 & 100 & 57875 & 36.11 & 100 & 58548 & 35.66 \\
\midrule
\multirow{2}{*}{Expresso} 
& ex38 & 8 & 7 & 100 & 86727 & 100 & 86105 & 0.71 & 100 & 74847 & 13.70 \\
& ex46 & 5 & 8 & 100 & 75726 & 100 & 75603 & 0.16 & 100 & 26854 & 64.54 \\
\midrule
\multirow{2}{*}{\makecell{Arithmetic \\ Function}} 
& ex50 & 8 & 2 & 100 & 87954 & 100 & 65689 & 25.31 & 100 & 42729 & 51.42 \\
& ex53 & 8 & 2 & 100 & 92365 & 100 & 75140 & 18.65 & 100 & 68246 & 38.26 \\
\midrule
\multirow{1}{*}{LogicNet} 
& ex92 & 10 & 3 & 100 & 220936 & 100 & 206941 & 6.33 & 100 & 134192 & 39.26 \\
\midrule
\multicolumn{4}{c|}{\textbf{Average}} & \textbf{100} & 102730 & \textbf{100} & 88831 & 13.19 & \textbf{100} & \textbf{57974} & \textbf{45.37} \\
\bottomrule
\end{tabular}}
\end{table*}

\begin{table*}[tb!]
\centering
\setlength\tabcolsep{3.6pt}
\caption{Experiment results of circuit generation size. Impr. represents the percentage decrease in search space and used NAND gates.}
\label{table:circuitgen_nand}
\vskip 0.05in
\footnotesize
\resizebox{0.888\linewidth}{!}{
\begin{tabular}{c|c|cc|cc|cc|cc|cc}
\toprule
\multicolumn{2}{c|}{\textbf{Benchmark}} & \multicolumn{2}{c|}{\textbf{CircuitNN}} & \multicolumn{4}{c|}{\textbf{T-Net}} & \multicolumn{4}{c}{\textbf{CircuitAR-2B}} \\ 
\midrule
\multirow{2}{*}{\textbf{Category}} & \multirow{2}{*}{\textbf{IWLS}} & \multicolumn{2}{c|}{\textbf{\# NAND$\downarrow$}} & \multicolumn{2}{c|}{\textbf{\# NAND$\downarrow$}} & \multicolumn{2}{c|}{\textbf{Impr.(\%)$\uparrow$}} & \multicolumn{2}{c|}{\textbf{\# NAND$\downarrow$}} & \multicolumn{2}{c}{\textbf{Impr.(\%)$\uparrow$}} \\
& & Search Space & Used & Search Space & Used & Search Space & Used & Search Space & Used & Search Space & Used \\ 
\midrule
\multirow{2}{*}{\makecell{Random}}
& ex00 & 700 & 58 & 400 & 68 & 42.86 & -17.24 & 126 & 61 & 82.00 & -5.17 \\
& ex01 & 700 & 66 & 400 & 62 & 42.86 & 6.06 & 138 & 66 & 80.29 & 0.00 \\
\midrule
\multirow{3}{*}{\makecell{Basic \\ Functions}} 
& ex11 & 300 & 52 & 180 & 52 & 40.00 & 0.00 & 98 & 45 & 67.33 & 13.46 \\
& ex16 & 700 & 78 & 400 & 59 & 42.86 & 24.36 & 113 & 57 & 83.86 & 26.92 \\
& ex17 & 800 & 109 & 500 & 98 & 37.50 & 10.09 & 196 & 95 & 75.50 & 12.84 \\
\midrule
\multirow{2}{*}{Expresso} 
& ex38 & 800 & 98 & 500 & 94 & 37.50 & 4.08 & 178 & 86 & 77.75 & 12.24 \\
& ex46 & 800 & 77 & 500 & 78 & 37.50 & -1.30 & 161 & 79 & 79.88 & -2.60 \\
\midrule
\multirow{2}{*}{\makecell{Arithmetic \\ Functions}} 
& ex50 & 300 & 59 & 180 & 56 & 40.00 & 5.09 & 77 & 48 & 74.33 & 18.64 \\
& ex53 & 1000 & 118 & 600 & 116 & 40.00 & 1.70 & 185 & 111 & 81.50 & 5.93 \\
\midrule
\multirow{1}{*}{LogicNet} 
& ex92 & 1000 & 99 & 600 & 90 & 40.00 & 9.09 & 168 & 86 & 83.20 & 13.13 \\
\midrule
\multicolumn{2}{c|}{\textbf{Average}} & 710 & 81.40 & 426 & 77.30 & 40.11 & 4.19 & \textbf{144} & \textbf{73.40} & \textbf{78.56} & \textbf{9.54} \\
\bottomrule
\end{tabular}}
\end{table*}

\subsection{Experiment Settings}

\minisection{Training Datasets}
We generate training datasets with around 400k circuits (average 200 gates per circuit) from the open-source datasets~\cite{bryan1985iscas, albrecht2005iwls, amaru2015epfl}.
The training dataset construction details will be illustrated in \Cref{appendix:dataset}.

\minisection{Data Augmentation}
We provide more details about data augmentation in \Cref{appendix:dataaug} and investigate the impact of idle of NAND gates in \Cref{appendix:idlenand}.

\minisection{Training Process}
We provide more details about the training processes of CircuitVQ and CircuitAR in \Cref{appendix:circuitvq} and \Cref{appendix:circuitar}. 

\minisection{Baseline Selection}
For baseline selection, we choose CircuitNN~\cite{deepmind2024ai4sys} and T-Net~\cite{wang2024tnet} due to their state-of-the-art (SOTA) performance in circuit generation guided by truth tables. 
Additionally, several other studies~\cite{tsaras2024shortcircuit, li2024circuittrans, zhou2024seadag} have explored circuit generation using different paradigms. 
We discuss these approaches in \Cref{sec:related}, as they diverge from the DAS paradigm employed in this work.

\minisection{Evaluation}
To validate the effectiveness of our CircuitAR models, we conduct evaluations using circuits from the IWLS competition~\cite{2022iwls}, which include five distinct function categories: random, basic functions, Espresso~\cite{rudell1985espresso}, arithmetic functions, and LogicNets~\cite{umuroglu2020logicnets}. 
Random circuits consist of random and decomposable Boolean functions, basic functions include majority functions and binary sorters, and arithmetic functions involve arithmetic circuits with permuted inputs and dropped outputs. 
Furthermore, we evaluate the BitsD for CircuitAR models with different sizes to assess their conditional circuit generation capability. 
This evaluation is performed on our circuit generation benchmark with 1000 circuits separate from the training dataset.

\subsection{Scalability and Emergent Capability}
\label{sec:scale}
To analyze CircuitAR's scaling behavior, we perform experiments along two primary dimensions: parameter scaling (\Cref{fig:scale_size}) and data scaling (\Cref{fig:scale_tokens}). 
Our results reveal distinct performance patterns quantified through BitsD, demonstrating how these scaling axes influence performance.
Additionally, we observe emergent capability in generating complex circuit structures of CircuitAR.


\minisection{Parameter Scaling}
As shown in \Cref{fig:scale_size}, increasing model capacity exhibits robust scaling laws. 
The 300M parameter model achieves 0.5317 BitsD, while scaling to 2B parameters yields 0.4362 BitsD. 
This progression follows a power-law relationship~\cite{kaplan2020scaling}, where performance scales predictably with model size. 
Notably, marginal returns diminish at larger scales: the 0.3B$\rightarrow$0.6B transition provides a 7.94\% improvement versus 6.07\% for 1B$\rightarrow$2B, highlighting practical trade-offs between capacity and computational costs. 
These findings corroborate theoretical expectations~\cite{thomas2006informationtheory}, confirming that larger models compress logical information more efficiently.

\minisection{Data Scaling}
\Cref{fig:scale_tokens} illustrates consistent performance gains with increased training tokens across all sizes.
For the 2B model, BitsD improves by 8.13\% (0.4748$\rightarrow$0.4362) when scaling from 1B to 4B tokens.  
Moreover, larger models exhibit superior data efficiency.
Specifically, the 2B model achieves better performance (0.4362 BitsD) with 4B tokens than the 1B model (0.4644 BitsD), emphasizing the interplay between model capacity and training scale.

\minisection{Emergent Capability}
\Cref{fig:emergent} highlights CircuitAR's emergent capability in generating complex circuit structures. 
A clear phase transition is observed at the 2B parameter threshold, where circuit depth increases significantly compared to the 1B model, indicating an emergent capacity for handling structural complexity. Moreover, an inverse correlation between model scale and NAND gate count reveals an efficiency paradigm. 
While models ranging from 300M to 1B parameters maintain similar component counts, the 2B model achieves a reduction in NAND gates despite its increased depth, suggesting enhanced topological optimization capabilities at scale. 
This emergent behavior demonstrates that increasing model parameters can enhance structural efficiency in circuit generation.

\subsection{SOTA Circuit Generation}
Given the superior performance of CircuitAR-2B, as demonstrated in \Cref{sec:scale}, we employ it to generate preliminary circuit structures conditioned on truth tables, which are subsequently refined using DAS. 
Detailed experimental results are presented in \Cref{table:circuitgen_tb} and \Cref{table:circuitgen_nand}.

\minisection{Efficiency}
As illustrated in \Cref{table:circuitgen_tb}, CircuitAR-2B achieves a 45.37\% average improvement in optimization steps compared to CircuitNN, while maintaining 100\% accuracy according to the provided truth tables. 
This performance significantly surpasses T-Net's 13.19\% improvement. 
The substantial reduction in optimization steps indicates that the preliminary circuit structures generated by CircuitAR-2B effectively prune the search space without compromising the quality of DAS. 

\minisection{Effectiveness}
\Cref{table:circuitgen_nand} demonstrates that CircuitAR-2B reduces NAND gate usage by an average of 9.54\% compared to CircuitNN, while simultaneously reducing the search space by 78.56\%. 
Notably, for both basic functions (e.g., ex16, with a 26.92\% reduction) and complex benchmarks (e.g., ex92, with a 13.13\% reduction), our method exhibits superior hardware resource utilization compared to the baseline approaches. 
This dual improvement in search space compression and circuit compactness underscores the effectiveness of the preliminary circuit structures generated by our CircuitAR-2B under the condition of the truth tables.

Critically, the 100\% accuracy across all benchmarks confirms that our method maintains functional correctness while achieving these efficiency gains. 
These results validate our hypothesis that integrating learned structural priors with CircuitAR enables more sample-efficient circuit generation compared to basic CircuitNN~\cite{deepmind2024ai4sys} and template-driven~\cite{wang2024tnet} DAS approaches.    

\subsection{Ablation Study}
\begin{table}[tb!]
\centering
\caption{Ablation study on the initialization process with edge probability generated by CircuitAR-2B.}
\label{table:abinit}
\vskip 0.05in
\footnotesize
\resizebox{0.988\linewidth}{!}{
\begin{tabular}{c|c|cc|cc}
\toprule
\multicolumn{2}{c|}{\textbf{Benchmark}} & \multicolumn{2}{c|}{\textbf{CircuitAR-2B w/o init}} & \multicolumn{2}{c}{\textbf{CircuitAR-2B}} \\ 
\midrule
\textbf{Category} & \textbf{IWLS} & \textbf{Acc.(\%)$\uparrow$} & \textbf{Steps$\downarrow$} & \textbf{Acc.(\%)$\uparrow$} & \textbf{Steps$\downarrow$} \\ 
\midrule
\multirow{2}{*}{\makecell{Random}} 
& ex00 & 100 & 72364 & 100 & 52023 \\
& ex01 & 100 & 40528 & 100 & 29636 \\
\midrule
\multirow{3}{*}{\makecell{Basic \\ Functions}} 
& ex11 & 100 & 64517 & 100 & 47231 \\
& ex16 & 100 & 76066 & 100 & 45434 \\
& ex17 & 100 & 88609 & 100 & 58548 \\
\midrule
\multirow{2}{*}{Expresso} 
& ex38 & 100 & 99594 & 100 & 74847 \\
& ex46 & 100 & 40892 & 100 & 26854 \\
\midrule
\multirow{2}{*}{\makecell{Arithmetic \\ Function}} 
& ex50 & 100 & 69958 & 100 & 42729 \\
& ex53 & 100 & 89627 & 100 & 68246 \\
\midrule
\multirow{1}{*}{LogicNet} 
& ex92 & 100 & 162651 & 100 & 134192 \\
\midrule
\multicolumn{2}{c|}{\textbf{Average}} & \textbf{100} & 80481 & \textbf{100} & \textbf{57974} \\
\bottomrule
\end{tabular}}
\end{table}

We conducted an ablation study to evaluate the effectiveness of the probability matrix $\hat{\mathcal{A}}$ generated by CircuitAR-2B. 
As summarized in \Cref{table:abinit}, the experiment results reveal that both variants achieve perfect accuracy (100\%) across all benchmarks, suggesting that the initialization process does not impair the ability to generate functionally equivalent circuits. 
The primary distinction lies in the efficiency of the search process, quantified by the number of search steps. 
\Cref{table:abinit} underscores the significance of the initialization process, demonstrating that our CircuitAR models can produce high-quality preliminary circuit structures, which can guide the subsequent DAS process effectively.



