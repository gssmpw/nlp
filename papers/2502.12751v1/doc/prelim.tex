\section{Preliminaries}

\subsection{Modeling Circuit as DAG}
In this work, we model the circuit as a directed acyclic graph (DAG)~\cite{brummayer2006circuitdag}, which facilitates graph autoregressive modeling. 
Specifically, each node in the DAG corresponds to a logic gate, such as AND, OR, NAND, or other gate types, while the directed edges represent the connections between these components.

\subsection{Differentiable CircuitNN}
As depicted in \Cref{fig:circuitnn}, CircuitNN~\cite{deepmind2024ai4sys} replaces traditional neural network layers with logic gates (e.g., NAND) as basic computational units, learning to synthesize circuits by optimizing logic correctness based on truth tables.
During training, input connections of each gate are determined through learnable probability distributions, enabling adaptive circuit architecture modification.
To enable gradient-based learning, CircuitNN transforms discrete logic operations into continuous, differentiable functions using NAND gates for simplicity. 
The NAND gate is logically complete, allowing the construction of any complex logic circuit.
Its continuous relaxation can be defined as:
\begin{equation}
\text{NAND}(x,y) = 1 - x \cdot y, \text{ where } x,y \in [0, 1].
\end{equation}
Additionally, CircuitNN employs Gumbel-Softmax~\cite{jang2016gumbel} for stochastic sampling of gate inputs. 
Through stochastic relaxation, gate and network outputs are no longer binary but take continuous values ranging from 0 to 1 instead. 
This end-to-end differentiability allows the model to learn gate input distributions using gradient descent.
After training, the continuous, probabilistic circuit is converted back into a discrete logic circuit by selecting the most probable connections based on the learned probability distributions as shown in \Cref{fig:circuitnn}.