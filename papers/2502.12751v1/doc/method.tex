\begin{figure*}[tb]
    \centering
    \includegraphics[width=0.868\linewidth]{figs/circuitvq.pdf} 
    \caption{The training process of CircuitVQ, designed based on the circuit autoencoder and a vector quantizer.}
    \label{fig:circuitvq}
\end{figure*}
\section{Methodology}
In this section, we first introduce CircuitVQ (\Cref{sec:circuitvq}), a model built upon the Circuit AutoEncoder framework (\Cref{sec:circuit_ae}) and trained with the task of circuit reconstruction. 
Utilizing CircuitVQ as a tokenizer, we subsequently train CircuitAR (\Cref{sec:circuitar}) with graph autoregressive modeling paradigm, which can generate preliminary circuit structures conditioned on a provided truth table. 
Finally, the initial circuit structure generated by CircuitAR serves as a guide for DAS (\Cref{sec:das}) to refine and generate circuits functionally equivalent to the given truth table.

\subsection{Circuit AutoEncoder}
\label{sec:circuit_ae}
Let $\mathcal{G} = (\mathcal{V}, \mathcal{A})$ represent a circuit, where $\mathcal{V}$ denotes the set of $N$ nodes, with each node $v_i \in \mathcal{V}$.
Following the architecture of CircuitNN~\cite{deepmind2024ai4sys, wang2024tnet}, each node $v_i$ can be classified into one of three types: primary inputs (PIs), primary outputs (POs), and NAND gates, each labeled by $u_{i} \in \mathcal{U}, i\in\{1,2,3\}$ respectively. 
The adjacency matrix $\mathcal{A} \in \{0, 1\}^{N \times N}$ captures the connectivity between nodes, where $\mathcal{A}_{i, j} = 1$ indicates the presence of a directed edge from $v_i$ to $v_j$.

In the circuit autoencoder framework, an encoder, denoted as $g_E$, encodes the circuit $\mathcal{G}$ into a latent representation $\vec{Z} \in \mathbb{R}^{N \times d}$ with dimensionality $d$.
The encoding process for a circuit can be formulated as:
\begin{equation}
    \vec{Z} = g_{E}(\mathcal{V}, \mathcal{A}).
    \label{eq:encoder}
\end{equation}
Simultaneously, a decoder $g_D$ aims to reconstruct the original circuit $\mathcal{G}$ from the latent representation $\vec{Z}$. 
Since node types can be directly derived from the truth table, the decoder is designed to focus on reconstructing the adjacency matrix $\mathcal{A}$, which can be formalized as follows:
\begin{align}
	\tilde{\mathcal{G}} = (\mathcal{V}, \tilde{\mathcal{A}}) = (\mathcal{V}, f(g_{D}(\vec{Z}, \mathcal{V}))),
	\label{eq:decoder}
\end{align}
where $\tilde{\mathcal{A}} \in \mathbb{R}^{N \times N}$ denotes the reconstructed adjacency matrix, obtained by decoding the latent representation $\vec{Z}$ through $g_D$ and applying a mapping function $f: \mathbb{R}^{N \times d} \rightarrow \mathbb{R}^{N \times N}$. 
Meanwhile, $\tilde{\mathcal{G}}$ represents the reconstructed graph.
A robust encoder $g_E$ capable of capturing fine-grained structural information is essential to facilitate the circuit reconstruction task. 
We incorporate the Graphormer~\cite{ying2021graphormer} architecture into $g_E$. 
For the decoder $g_D$, we adopt a simple Transformer-based~\cite{dubey2024llama3} architecture, as an overly powerful decoder could negatively impact the performance of the circuit tokenizer.

\subsection{CircuitVQ}
\label{sec:circuitvq}
As mentioned in \Cref{sec:circuit_ae}, we propose a circuit autoencoder architecture for the circuit reconstruction task.
The outputs of $g_E$ and the inputs of $g_D$ are continuous.
The circuit tokenizer is required to map the circuit to a sequence of discrete circuit tokens for masked autoregressive modeling illustrated in \Cref{sec:circuitar}.
Specifically, a circuit $\mathcal{G}$ can be tokenized to $\vec{Y} = [y_1, y_2, \cdots, y_{N}] \in \mathbb{R}^{N}$ using the circuit quantizer $\mathcal{C}$ which contains $K$ discrete codebook embeddings.
Here, each token $y_i$ belongs to the vocabulary set $\{1, 2, \dots, K\}$ of $\mathcal{C}$.
Consequently, we develop a circuit tokenizer, CircuitVQ, based on the circuit autoencoder by integrating a circuit quantizer $\mathcal{C}$.
As shown in \Cref{fig:circuitvq}, the tokenizer comprises three components: a circuit encoder $g_E$, a circuit quantizer $\mathcal{C}$, and a circuit decoder $g_D$. 

Firstly, $g_E$ encodes the circuit into vector representations $\vec{Z}$. 
Subsequently, $\mathcal{C}$ identifies the nearest neighbor in the codebook for $\vec{z}_{i} \in \vec{Z}$. 
Let $\{\vec{e}_1, \vec{e}_2, \dots, \vec{e}_{K}\}$ represent the codebook embeddings and $\vec{e}_{K} \in \mathbb{R}^{d}$. 
For the $i$-th node, the quantized code $y_i$ is determined by:
\begin{align}
	y_i = \arg \min_{j} || \ell_2(\vec{z}_i) - \ell_2(\vec{e}_j)||_2,
\label{eq:vq_dis}
\end{align}
where $j \in \{1, 2, \dots, K\}$ and $\ell_2$ normalization is applied during the codebook lookup~\cite{van2017vqvae, yu2021vqgan_i}. 
This distance metric is equivalent to selecting codes based on cosine similarity.
Consequently, the output of $\mathcal{C}$ for each node representation $\vec{z}_i$ the can be calculated based on the given \Cref{eq:vq_dis}:
\begin{align}
	\tilde{\vec{z}}_i &= \mathcal{C}(\vec{z}_{i}) = \ell_2(\vec{e}_{y_i}), \text{ where } \tilde{\vec{z}}_i \in \tilde{\vec{Z}}.
\label{eq:vq_emb}
\end{align}
\begin{figure}[t]
    \centering
    \includegraphics[width=0.968\linewidth]{figs/circuitar.pdf} 
    \caption{The training process of CircuitAR under the condition of the truth table, leveraging CircuitVQ as the tokenizer.}
    \label{fig:circuitar}
\end{figure}
After quantizing the circuit into discrete tokens, the $\ell_2$-normalized codebook embeddings $\tilde{\vec{Z}} = \{\tilde{\vec{z}}_i\}_{i=1}^{N}$ are fed to $g_D$.
The output vectors $\tilde{\vec{X}} = \{\tilde{\vec{x}}_i \}_{i=1}^{N} = g_D(\tilde{\vec{Z}}, \mathcal{V})$ are used to reconstruct the original adjacency matrix $\mathcal{A}$ of the circuit $\mathcal{G}$. 
Specifically, the reconstructed adjacency matrix $\tilde{\mathcal{A}}$ is derived from the output vectors $\tilde{\vec{X}}$ as follows:
\begin{align}
    \tilde{\mathcal{A}} = f(\tilde{\vec{X}}) = \sigma\left(f_1(\tilde{\vec{X}}) \cdot f_2(\tilde{\vec{X}})^T\right),
\end{align}
where both $f_1: \mathbb{R}^{d} \rightarrow \mathbb{R}^{d}$ and $f_2: \mathbb{R}^{d} \rightarrow \mathbb{R}^{d}$ are learnable projection functions, and $\sigma(x)$ denotes the sigmoid function.
The training objective of the circuit reconstruction task is to minimize the binary cross-entropy loss between the reconstructed adjacency matrix $\tilde{\mathcal{A}}$ and the original adjacency matrix $\mathcal{A}$, which can be calculated as follows:
\begin{align}
	\mathcal{L}_\text{rec} = -\frac{1}{N^2} \sum_{i=1}^{N} \sum_{j=1}^{N} \bigl[ 
		& \mathcal{A}_{ij} \log(\tilde{\mathcal{A}}_{ij}) \nonumber \\
		& + (1 - \mathcal{A}_{ij}) \log(1 - \tilde{\mathcal{A}}_{ij}) \bigr].
\end{align}

Given that the quantization process in \Cref{eq:vq_dis} is non-differentiable, gradients are directly copied from the decoder input to the encoder output during backpropagation, which enables the encoder to receive gradient updates.
Intuitively, while the quantizer selects the nearest codebook embedding for each encoder output, the gradients of the codebook embeddings provide meaningful optimization directions for the encoder. 
Consequently, the overall training loss for CircuitVQ is defined as:
\begin{align}
	\mathcal{L}_\text{vq} = \mathcal{L}_\text{rec} + \| z - \text{sg}[e_{y_{i}}] \|_2^2 + \beta \cdot \| \text{sg}[z] - e_{y_{i}} \|_2^2
\end{align}
where $\mathrm{sg}[\cdot]$ stands for the stop-gradient operator which is an identity at the forward pass while having zero gradients during the backward pass and $\beta$ denotes the hyperparameter for commitment loss~\cite{van2017vqvae}.

\minisection{Codebook utilization}
A common issue in vector quantization training is codebook collapse, where only a small proportion of codes are actively utilized.
To mitigate this problem, empirical strategies~\cite{yu2021vqgan_i,jang2016gumbel} are employed. 
Specifically, we compute the $\ell_2$-normalized distance to identify the nearest code while reducing the dimensionality of the codebook embedding space~\cite{yu2021vqgan_i}. 
These low-dimensional codebook embeddings are subsequently mapped back to a higher-dimensional space before being passed to the decoder.
Furthermore, we leverage the Gumbel-Softmax trick~\cite{jang2016gumbel} to enable smoother token selection during training, ensuring that a broader range of tokens in the codebook are actively trained, thereby improving the overall utilization of the codebook.

\begin{algorithm}[tb]
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}
\caption{Autoregressive Decoding of CircuitAR}
\label{algo:decode}
\begin{algorithmic}[1] % Add line numbers for better reference
\REQUIRE Masked tokens $\bar{\vec{Y}} = [\bar{y}_i]_{i=1}^N, \forall \bar{y}_i = \mathit{m}$, token length $N$, total iterations $T$.
\ENSURE Predicted tokens $\tilde{\vec{Y}} = [\tilde{y}_i]_{i=1}^N \forall \tilde{y}_i \neq \mathit{m}$.

\FOR{$t \gets 0$ \textbf{to} $T - 1$}
    \STATE Initialize the number of masked tokens $n$;

    \STATE Compute probabilities $p(\bar{y}_i) \in \mathbb{R}^{K}$ for each $\bar{y}_i \in \bar{\vec{Y}}$;

    % \STATE \textbf{Step 3:} Confidence Score Prediction;
    \STATE Initialize $\vec{S} \gets [s_i]_{i=1}^N$, where $s_i = 0$, and $\tilde{\vec{Y}} \gets \bar{\vec{Y}}$;
    \FOR{$i \gets 1$ \textbf{to} $N$}
        \IF{$\bar{y}_i = \mathit{m}$}
            \STATE Sample a token $o_i \in \{1, \dots, K\}$ from $p(\bar{y}_i)$;
            \STATE $s_i \gets p(\bar{y}_i)[o_i]$ and $\tilde{y}_i \gets o_i$;
        \ELSE
            \STATE $s_i \gets 1$;
        \ENDIF
    \ENDFOR

    % \STATE \textbf{Step 4:} Predicted Tokens Selection;
    \FOR{$i \gets 1$ \textbf{to} $N$ \textbf{and} $\bar{y}_i \neq \mathit{m}$}
    	\STATE $r \gets \text{sorted}(S)[n]$; // Select the $n$-th highest score from the sorted $\vec{S}$ in decending order
        \STATE $\bar{y}_i \gets 
        \begin{cases}
            \tilde{y}_i, & \text{if } s_i < r, \\
            \bar{y}_i,    & \text{otherwise;}
        \end{cases}$ 
    \ENDFOR
    \STATE $\tilde{\vec{Y}} \gets \bar{\vec{Y}}$;
\ENDFOR
\end{algorithmic}
\end{algorithm}

\subsection{CircuitAR}
\label{sec:circuitar}

After completing the CircuitVQ training, we train CircuitAR using a graph autoregressive modeling paradigm as shown in \Cref{fig:circuitar}, where CircuitVQ functions as the tokenizer. 
Let $\vec{Y} = [y_i]_{i=1}^N$ represent the discrete latent tokens of the input circuit $\mathcal{G}$, tokenized by CircuitVQ. 
During the masked autoregressive training process, we sample a subset of nodes $\mathcal{V}_{s} \subset \mathcal{V}$ and replace them with a special mask token $\mathit{m}$. 
For the masked $\vec{Y}$, the latent token $\bar{y}_i$ is defined as:
\begin{equation}
\bar{y}_i = \begin{cases} 
    y_i, & \text{if } v_{i} \notin \mathcal{V}_{s}; \\
    \mathit{m}, & \text{if } v_{i} \in \mathcal{V}_{s}.
      \end{cases}
\end{equation}
Following \citet{chang2022maskgit} and \citet{li2024mar}, we employ a cosine mask scheduling function $\gamma(r) = \cos(0.5\pi r)$ in the sampling process. 
This involves uniformly sampling a ratio $r$ from the interval $[0, 1]$ and then selecting $\lceil \gamma(r) \cdot N \rceil$ tokens from $\vec{Y}$ to mask uniformly. 
Let $\bar{\vec{Y}} = [\bar{y}_i]_{i=1}^N$ denote the output after applying the masking operation to $\vec{Y}$. 
The masked sequence $\bar{\vec{Y}}$ is then fed into a multi-layer transformer with bidirectional attention to predict the probabilities $p(y_i | \bar{\vec{Y}}, \mathcal{T})$ for each $v_{i} \in \mathcal{V}_{s}$ under the condition of the truth table. 
The transformer is designed based on Llama models, each CircuitAR layer consists of a self-attention block, a cross-attention block and an FFN block.
Specifically, the info of the truth table is conditioned by serving $\mathcal{T}$ as the input key and value of the cross-attention block.
The training loss for CircuitAR is defined as:
\begin{equation}
	\mathcal{L}_\text{ar} = -\sum\limits_{\mathcal{D}} \sum\limits_{v_{i} \in \mathcal{V}_{s}} 
	\log p(y_i | \bar{\vec{Y}}, \mathcal{T}),
\end{equation}
where $\mathcal{D}$ represents the set of training circuits.

\minisection{Autoregressive decoding}
We introduce a parallel decoding method, where tokens are generated in parallel. 
This approach is feasible due to the bidirectional self-attention mechanism of CircuitAR. 
At inference time, we begin with a blank canvas $\bar{\vec{Y}} = [\mathit{m}]^N$ and the decoding process of CircuitAR follows \Cref{algo:decode}. 
Specifically, the decoding algorithm generate a circuit in $T$ steps. 
At each iteration, the model predicts all tokens simultaneously but retains only the most confident predictions following the cosine schedule~\cite{chang2022maskgit, li2024mar}. 
The remaining tokens are masked and re-predicted in the next iteration. 
%The mask ratio decreases progressively until all tokens are generated within $T$ iterations.

\subsection{Differentiable Architecture Search}
\label{sec:das}

After completing the training process of CircuitAR, autoregressive decoding is performed based on the input truth table $\mathcal{T}$ to generate preliminary circuit structures represented by the reconstructed adjacency matrix $\tilde{\mathcal{A}}$. 
This matrix $\tilde{\mathcal{A}}$ can serve as prior knowledge for DAS, enabling the generation of a precise circuit that is logically equivalent to $\mathcal{T}$.

\minisection{DAG Search}
The reconstructed adjacency matrix $\tilde{\mathcal{A}}$ is a probability matrix that denotes the probabilities of connections between gates. 
However, $\tilde{\mathcal{A}}$ may contain cycles. 
Identifying the optimal DAG with the highest edge probabilities is an NP-hard problem, we employ a greedy algorithm to obtain a suboptimal DAG. 
As illustrated in \Cref{algo:dag_search}, the algorithm initializes $\bar{\mathcal{A}} \in \mathbb{R}^{N \times N}$ with edge probabilities and enforces basic structural rules: PIs have no indegree, POs have no outdegree, and self-loops are prohibited in circuit designs. 
Following this initialization, a depth-first search (DFS) is conducted to detect cycles in $\bar{\mathcal{A}}$. 
If no cycles are found, $\bar{\mathcal{A}}$ is a valid DAG, and the algorithm terminates. 
If a cycle is detected, the edge with the lowest probability within the cycle is identified and removed by setting the corresponding edge in $\bar{\mathcal{A}}$ to 0. 
This process repeats iteratively until no cycles remain.
This greedy approach ensures the derivation of a valid DAG $\bar{\mathcal{A}}$ that approximates the optimal structure while preserving the acyclic property necessary for circuit design. 
The resulting DAG serves as a foundation for further refinement in the DAS process, ultimately generating a precise circuit that is logically equivalent to $\mathcal{T}$.

%\begin{figure}[tb!]
%    \centering
%    \includegraphics[width=0.888\linewidth]{figs/scale_tokens.pdf} 
%    \caption{Training with more tokens improves BitsD for all sizes of CircuitAR. We show BitsD over training tokens for our CircitAR.}
%    \label{fig:scale_tokens}
%\end{figure}
%
%
%\begin{figure}[tb!]
%    \centering
%    \includegraphics[width=0.888\linewidth]{figs/scale_size.pdf} 
%    \caption{Scaling behavior of CircuitAR with different model parameters on circuit generation benchmark.}
%    \label{fig:scale_tokens}
%\end{figure}

\begin{algorithm}[tb]
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}
\caption{DAG Search}
\label{algo:dag_search}
\begin{algorithmic}[1]
\REQUIRE Adjacency matrix $\tilde{\mathcal{A}} \in \mathbb{R}^{N \times N}$, PI node list $Q_i$, PO node list $Q_o$.
\ENSURE Adjacency matrix $\bar{\mathcal{A}}$ of a valid DAG.

\STATE Initialize $\bar{\mathcal{A}} \gets \tilde{\mathcal{A}}$.
\FOR{each edge $(i, j)$ in $\tilde{\mathcal{A}}$ $(i \neq j)$}
	% \IF{$i = j$}
	% 	\STATE \textbf{continue};
	% \ENDIF
	\STATE $\bar{\mathcal{A}}[i][j] \gets 0$.
    \IF{$i \notin Q_o$ \textbf{and} $j \notin Q_i$  \textbf{and} $\tilde{\mathcal{A}}[i][j] > 0.5$}
        \STATE $\bar{\mathcal{A}}[i][j] \gets 1$;
    \ENDIF
\ENDFOR

\WHILE{True}
    \STATE $c \gets \text{cycleDetect}(\bar{\mathcal{A}})$; // Detect a cycle using DFS and return the list of nodes forming the cycle.
    \IF{$\text{len}(c) = 0$}
        \STATE \textbf{break}; // No cycles detected; $\bar{\mathcal{A}}$ is a valid DAG.
    \ENDIF
    \STATE Initialize $s \gets \infty$.
    \FOR{$i \gets 0$ \textbf{to} $\text{len}(c) - 1$}
        \STATE $j \gets c[i]$ and $k \gets c[(i + 1) \bmod \text{len}(c)]$;
        \IF{$\tilde{\mathcal{A}}[j][k] < s$}
            \STATE $s \gets \tilde{\mathcal{A}}[j][k]$ and $r \gets (j, k)$;
        \ENDIF
    \ENDFOR
    \STATE $\bar{\mathcal{A}}[r[0]][r[1]] \gets 0$;
\ENDWHILE
\end{algorithmic}
\end{algorithm}


\begin{figure*}[]
\centering
\begin{minipage}[t]{0.31\linewidth}
\centering
\includegraphics[width=0.98\linewidth]{figs/scale_size.pdf}
\caption{Scaling behavior of CircuitAR with different model parameters on circuit generation benchmark.}
\label{fig:scale_size}
\end{minipage}
\hspace{6pt}
\begin{minipage}[t]{0.31\linewidth}
\centering
\includegraphics[width=0.98\linewidth]{figs/scale_tokens.pdf}
\caption{Training with more tokens improves BitsD for CircuitAR with different model parameters.}
\label{fig:scale_tokens}
\end{minipage}
\hspace{6pt}
\begin{minipage}[t]{0.31\linewidth}
\centering
\includegraphics[width=0.98\linewidth]{figs/emergence.pdf}
\caption{Emergent capability in generating complex circuit structures of our CircuitAR.}
\label{fig:emergent}
\end{minipage}
\end{figure*}

\minisection{Initialization}
After executing \Cref{algo:dag_search}, the adjacency matrix of a valid DAG $\bar{\mathcal{A}} \in \mathbb{R}^{N \times N}$ and its corresponding probability matrix $\hat{\mathcal{A}} = \bar{\mathcal{A}} \cdot \tilde{\mathcal{A}}$, where $\hat{\mathcal{A}} \in \mathbb{R}^{N \times N}$, are obtained. 
Using $\bar{\mathcal{A}}$, we derive the hierarchical structure $H = \{h_1, h_2, \dots, h_l\}$, where $h_l$ represents the node list of the $l$-th layer. 
The set $H$ encapsulates the layer count $l$ and the width information of each layer, which is used to initialize CircuitNN illustrated in \Cref{fig:circuitnn}.
For connection probabilities, since each node can only connect to nodes from preceding layers, we normalize the connection probabilities such that their summation equals 1. 
This yields the weights $w \in \mathbb{R}^{N_{p}}$ for possible connections, where $N_{p}$ denotes the number of nodes in the previous layer. 
To ensure compatibility with the Softmax function applied in CircuitNN, we initialize the logits $\hat{w} \in \mathbb{R}^{N_{p}}$ such that the Softmax output matches the normalized connection probabilities. 
The logits are initialized as follows:
\begin{equation}
	\hat{w} = \log(w + \epsilon) - \frac{1}{N_{p}} \sum_{i=1}^{N_{p}} \log(w_i + \epsilon), 
\label{eq:init}
\end{equation} 
where $\epsilon$ is a small constant for numerical stability.
After initialization, the precise circuit structure is obtained through DAS, guided by the input truth table.
Notably, if DAS converges to a local optimum, the weights of the least initialized nodes can be randomly selected and reinitialized using \Cref{eq:init} to facilitate further optimization.

\subsection{Bits Distance}

DAS introduces inherent randomness, complicating the evaluation of CircuitAR's circuit generation capability using post-DAS metrics. 
To overcome this, we introduce Bits Distance (BitsD), a metric offering a more reliable assessment of CircuitAR's conditional generation ability.
BitsD quantifies the discrepancy between the outputs of an untrained CircuitNN, initialized via CircuitAR, and the labels from the truth table. 
It measures how well CircuitAR generates circuits conditioned on the truth table. 
Specifically, after initializing CircuitNN, we feed it with the truth table inputs and compute the mean absolute error (MAE) between the untrained CircuitNN outputs and the truth table labels. 
This MAE is defined as Bits Distance.
A smaller BitsD indicates that the untrained CircuitNN is closer to the target circuit described by the truth table. 
%BitsD provides a stable and interpretable evaluation of CircuitAR's performance, independent of DAS randomness. 
%It also aids in comparing initialization strategies or optimizing CircuitAR's architecture for complex circuit generation tasks. A large BitsD may indicate refining the initialization method or adjusting the model structure.
