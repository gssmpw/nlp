\section{Implementation Details}

\subsection{CircuitVQ}
\label{appendix:circuitvq}
The training process of CircuitVQ employs a linear learning rate schedule with the AdamW optimizer set at a learning rate of $2 \times 10^{-4}$, a weight decay of 0.1, and a batch size of 2048.
The model is fine-tuned for 20 epochs on 8$\times$A100 GPUs with 80G memory each.
Moreover, we use the Graphormer~\cite{ying2021graphormer} as our CircuitVQ architecture as mentioned before. 
Specifically, CircuitVQ comprises 6 encoder layers and 1 decoder layer. 
The hidden dimension and FFN intermediate dimension are 1152 and 3072, respectively. 
Additionally, the multi-head attention mechanism employs 32 attention heads. 
For the vector quantizer component, the codebook dimensionality is set to 4 to improve the codebook utilization, and the codebook size is configured to 8192.

\subsection{CircuitAR}
\label{appendix:circuitar}
The training process of CircuitAR employs a linear learning rate schedule with the AdamW optimizer set at a learning rate of $2 \times 10^{-4}$, a weight decay of 0.1, and a batch size of 4096.
The model is fine-tuned for 20 epochs on 16$\times$A100 GPUs with 80G memory each.
Moreover, we use the Transformer variant of Llama~\cite{dubey2024llama3} as our CircuitAR architecture as mentioned before. 
To form different model sizes, we vary the hidden dimension, FFN intermediate dimension, number of heads and number of layers. 
We present the details of our CirucitAR architecture configurations in \Cref{tab:arch_config}. 
For the rest of the hyperparameters, we keep them the same as the standard Llama model.
\begin{table}[!h]
\centering
\caption{Model architecture configurations of CircuitAR.}
\vskip 0.05in
\footnotesize
\begin{tabular}{l|c|c|c|c}
\toprule
& \textbf{hidden dim} & \textbf{FFN dim} & \textbf{heads} & \textbf{layers} \\
\midrule
CircuitAR-300M & 1280      & 3072    &  16      & 16 \\
CircuitAR-600M & 1536      & 4096    &  16      & 20 \\
CircuitAR-1B   & 1800      & 6000    &  24      & 24 \\
CircuitAR-2B   & 2048      & 8448    &  32      & 30 \\
\bottomrule
\end{tabular}
\label{tab:arch_config}
\end{table}


\section{Training Datasets}
\label{appendix:dataset}
This section presents a multi-output subcircuit extraction algorithm designed for generating training datasets.
The algorithm processes circuits represented in the And Inverter Graph (AIG) format by iterating over each non-PI node as a pivot node. 
The extraction process consists of three key stages:
\begin{enumerate}
    \item \textbf{Single-Output Subcircuit Extraction.} 
The algorithm extracts single-output subcircuits by analyzing the transitive fan-in of the pivot node. 
The transitive fan-in includes all nodes that influence the output of the pivot node, encompassing both direct predecessors and nodes that propagate signals to it. 
The extraction process employs a breath-first search (BFS) algorithm, constrained by a maximum input size, to ensure comprehensive coverage of relevant nodes associated with the pivot node.
    \item \textbf{Multi-Output Subcircuit Expansion.} 
Single-output subcircuits are expanded into multi-output subcircuits through transitive fan-out exploration. 
The transitive fan-out comprises all nodes influenced by the pivot node, including immediate successors and downstream nodes reachable through signal propagation. 
This expansion captures the broader network of nodes that either influence or are influenced by the subcircuits of the pivot node.
    \item \textbf{Truth Table Generation.} 
The algorithm computes truth tables for the extracted subcircuits to serve as training labels. 
Additionally, these truth tables help identify functionally equivalent subcircuits.
Recognizing these equivalences is essential, as it can lead to data imbalance in the training set.
\end{enumerate}

To mitigate data imbalance, a constraint is imposed, limiting each truth table to at most $M$ distinct graph topologies. 
For truth tables with fewer than $M$ representations, logic synthesis techniques (specifically rewriting algorithms) are applied to generate functionally equivalent subcircuits with distinct topologies. 
This approach ensures topological diversity while maintaining functional equivalence.
Finally, the training datasets with around 400000 circuits (average 200 gates per circuit) are generated using circuits from the ISCAS'85~\cite{bryan1985iscas}, IWLS'05~\cite{albrecht2005iwls}, and EPFL~\cite{amaru2015epfl} benchmarks. 
The maximum input size is set to 16, and $M$ is set to 5 during the generation process. 

\section{Data Augmentation}
\label{appendix:dataaug}

Following dataset generation, we identified that the data volume was still insufficient. 
To address this limitation, we implemented data augmentation techniques. 
Leveraging the topological invariance of graphs, we randomly shuffled the order of graph nodes, as this operation does not alter the underlying structure of the circuit. 
Furthermore, since inserting idle nodes preserves the circuit structure, we randomly introduced idle nodes into the graphs. 
The proportion of idle nodes is randomly selected ranging from 0\% to 80\%.
Moreover, incorporating idle nodes enables CircuitAR to identify which nodes can remain inactive for a fixed number of nodes. 
This allows CircuitAR to generate circuits logically equivalent to the truth table while utilizing fewer graph nodes. 
This strategy can improve CircuitAR's efficiency and enhance its generalizability.

\begin{figure}[]
    \centering
    \includegraphics[width=0.4\linewidth]{figs/idle_ratio.pdf} 
    \caption{The impact of idle NAND gates on BitsD for CircuitAR with different ratios of isolated NAND gates.}
    \label{fig:idle_gates}
\end{figure}

\section{Idle NAND Gates}
\label{appendix:idlenand}

As illustrated in \Cref{fig:idle_gates}, all model variants exhibit a gradual decline in BitsD as the proportion of isolated gates increases from 0\% to 45\%. 
Larger models (2B parameters), demonstrate significantly greater robustness, maintaining BitsD values within a narrow range (0.4362 to 0.4413) across varying isolation ratios. 
In contrast, smaller models (300M parameters) show a more pronounced degradation, with BitsD increasing from 0.5317 to 0.5484 under the same conditions. 
This disparity highlights the enhanced ability of larger models to efficiently utilize NAND gates for implementing the same truth table. 
The consistently low BitsD observed in the 2B model (0.4412 at 45\% idle ratios) underscores its practical utility in predefining search spaces for DAS, offering a notable advantage over smaller models.