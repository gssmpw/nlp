\section{Related Works}
\label{sec:related}

\subsection{Autoregressive Modeling}
The autoregressive modeling paradigm~\cite{openai2023gpt4, tian2024var} has been widely adopted for generation tasks in language and vision domains. 
Built on the transformer architecture, autoregressive models are commonly implemented using causal attention mechanisms in language domains~\cite{openai2023gpt4}, which process data sequentially. 
However, information does not inherently require sequential processing in vision and graph generation tasks. 
To address this, researchers have employed bidirectional attention mechanisms for autoregressive modeling~\cite{li2024mar, tian2024var,chang2022maskgit,li2023mage}. 
This approach predicts the next token based on previously predicted tokens while allowing unrestricted communication between tokens, thereby relaxing the sequential constraints of traditional autoregressive methods. 
In this paper, we adopt masked autoregressive modeling for circuit generation, leveraging its ability to provide a global perspective and enhance the modeling of complex dependencies.

\subsection{Circuit Generation}  
In addition to the DAS-based approaches, researchers have also explored next-gate prediction techniques inspired by LLMs for circuit generation. 
Circuit Transformer~\cite{li2024circuittrans} predicts the next logic gate using a depth-first traversal and equivalence-preserving decoding. 
SeaDAG~\cite{zhou2024seadag} employs a semi-autoregressive diffusion model for DAG generation, maintaining graph structure for precise control. 
ShortCircuit~\cite{tsaras2024shortcircuit} uses a transformer to generate Boolean expressions from truth tables via next-token prediction.
However, these methods are limited by their global view, restricting circuit size and failing to reduce the search space. 
In contrast, our approach uses global-view masked autoregressive decoding to generate circuits while ensuring logical equivalence and significantly reducing the search space during the DAS process.
