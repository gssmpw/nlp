\appendix

\section{Notations}\label{s:notations}
Table~\ref{notations} is a summary of the main notations used in our paper. In particular, the differences between $\omega$, $\mathcal{W}$ and $W$ are noted. It is complemented by Figure~\ref{fig:schema}, which illustrates our method.

 \begin{table}[h!]
 \begin{center}
 \scalebox{1}{
 \begin{tabular}{ | c | c|}
  \hline
  $G$ & An arbitrary undirected and unweighted graph\\
   & with $N$ vertices $V$, and edge set $E$\\
  $G_W$ & The watermarked graph\\
  $G^*$ & Another arbitrary graph to test for the watermark presence\\
  $A$, $A_W$ & Adjacency matrices of graph $G$ and $G_W$ respectively\\
  $A'$ & Matrix obtained before the binarization of $A_W$\\
  $\theta$ &
  Similarity threshold for the watermark extraction success\\
  $\omega$ & Watermark key, of length $m$\\
  $\sigma$ & Standard deviation for generating the key\\
  $\chi$ & List of indices to insert the key\\
  $\mathcal{W}$ & Intermediary (not yet binarized) watermark\\
  $W$ & Final watermark\\
  \hline
 \end{tabular}
 }
 \end{center}
 \caption{Main notations in this paper.}
 \label{notations}
 \end{table}

\begin{figure*}[h!]
\centerline{\includegraphics[width=0.85\textwidth]{pics/fig1_2025.pdf}}
\caption{The \scheme framework for watermarking unweighted graphs using an image domain scheme (hereby Cox \textit{et al.}~\cite{TIP}).
} 
\label{fig:schema}
\end{figure*}

\section{Watermarking non-graph objects}
\label{sec:related}

This section is an overview of methods for watermarking graph-related objects that are not graphs themselves.

\paragraph*{Watermarking various digital objects}
Initially introduced to protect multimedia objects~\cite{560423,TIP}, watermarking techniques are now being applied to a wide range of other areas, such as the model protection of deep neural networks \cite{le2020adversarial}, graph neural networks \cite{zhao2021watermarking} or large language models~\cite{kirchenbauer2023watermark} to just cite a few recent works. Watermarking approaches sometimes use graph abstractions to hide information in the object, as it is the case with software watermarking. (See e.g.~\cite{dey2019software} for a review.)
\par In software watermarking approaches, graph abstractions capture the interactions between variables of a source code to be watermarked. The purpose is thus to watermark the software; graph representations are simply leveraged to build watermarking schemes, and are not the target of the watermarks. Some approaches focus on databases, to watermark datasheets or RDBMS tuples, for example \cite{agrawal2002watermarking,cudre2011graph,kumar2020recent}; directly watermarking graph objects remains much more general, as targeting graph structures themselves \cite{COSN,isc}, and because no assumption is made on the presence or absence of tuples and rich data to in which embed information into.
\par In graph signal watermarking~\cite{icassp}, since signals are data defined on the vertices of graphs, it is natural to watermark them by using graph eigenvectors. However, this technique thus consists of watermarking signals defined over graphs, and it never watermarks graphs themselves.

\paragraph*{Watermarking graphs}

This paper is dedicated to watermark unweighted graphs.
Recently, two graph watermarking approaches have been introduced that use purely graph techniques~\cite{isc,COSN} to watermark graphs. Since they are direct competitors to \scheme, both schemes are described in detail in the main part of the paper (Section~\ref{sec:benchmark}), and then benchmarked.

\paragraph*{Digital image watermarking}
Efficient image watermarking has motivated decades of research. Images can be watermarked by taking into account features such as the perception of the human visual system (HVS)~\cite{HVS}, or by applying structural operations to them such as JPEG compression~\cite{JPEG}.
Ingemar J. Cox \textit{et al.}~\cite{TIP} proposed the general method of Fourier transform and in-spectrum watermarking. This work is still used today for other types of transforms such as wavelets~\cite{wvl} or fractional Fourier transforms~\cite{FFT}.

\section{Theoretical Analysis of Uniqueness}
\label{s:theory}
This section aims to complete the experimental study of \scheme. In particular, with respect to the uniqueness and success of a watermark.

We have defined watermarking schemes and how Cox's watermarking scheme can be adapted to graphs. We now study the effect of binarization, which is related to design goal 2.

\subsection{Uniqueness of Watermarks}

Let $G_{W_1}$ be the watermarked graph resulting from a pair $(G, \omega_1)$. $G$ is the graph to be watermarked and $W_1$ the resulting watermark from $\omega_1$. The uniqueness problem is to determine whether $G_{W_1}$ is unique. 

The only source of randomness in the scheme is the generation of the key; thus if $G$ is watermarked with another key $\omega_2$ resulting in $G_{W_2}$, then there is uniqueness if $G_{W_1} \neq G_{W_2}$. This property is not trivial because of the binarization which spreads keys over all the Fourier coefficients of the adjacency matrices.

We then study the probability of uniqueness:
\begin{equation}\label{result}
    P(G_{W_1} \neq G_{W_2}) = 1 - P(G_{W_1} = G_{W_2})
\end{equation}

\paragraph*{Proof}
    Let $A_{W_1}$, respectively $A_{W_2}$, be the adjacency matrices of $G_{W_1}$, respectively $G_{W_2}$. By the hypothesis of the deterministic mapping of vertices into adjacency matrices:
$P(G_{W_1} = G_{W_2}) = P(A_{W_1} = A_{W_2}).$
Similarly, let $A'_1$, respectively $A'_2$ be the matrices corresponding to $A_{W_1}$, respectively $A_{W_2}$ \textbf{before} the binarization.

By definition, binarization is done using the average of the adjacency matrix $A$ of $G$:

\begin{center}
    \begin{flalign} \label{eq:P}
    &P(G_{W_1} = G_{W_2}) 
        = P\left(\bigcap_{i,j} (A_{W_1}[i,j] = A_{W_2}[i,j])\right)\nonumber\\
    & = P\left ( \bigcap _{i,j}\begin{matrix}[
    A_{W_1}[i,j]=A_{W_2}[i,j]=A[i,j]=0\\ 
    \cup A_{W_1}[i,j]=A_{W_2}[i,j]=A[i,j]=1\\ 
    \cup A_{W_1}[i,j]=A_{W_2}[i,j]=1-A[i,j]=0\\ 
    \cup A_{W_1}[i,j]=A_{W_2}[i,j]=1-A[i,j]=1
    ] \end{matrix} \right )&\nonumber\\
    & = P\left ( \bigcap _{i,j}\begin{matrix}
    | A'_{1,2}[i,j] | \leq av(A) \wedge A[i,j]=0\\ 
    \cup | A'_{1,2}[i,j]| > av(A) \wedge A[i,j]=1\\ 
    \cup |A'_{1,2}[i,j]| \leq av(A) \wedge A[i,j]=1\\ 
    \cup |A'_{1,2}[i,j]| > av(A) \wedge A[i,j]=0
     \end{matrix} \right ).
    \end{flalign}
\end{center}


Here the notation $A'_{1,2}$ means that the result must be true for both $A'_{1}$ and $A'_{2}$ respectively and $av(A)$ is the average of all elements of the matrix $A$. \par
By the linearity of the inverse Fourier transform,\\
for $n \in \{1,2\}$:
\begin{equation}
\label{linearize}
A'_n = A + FT^{-1}(\mathcal{W}_n).
\end{equation}
with $$FT(A)[i,j] = \sum_{k=1}^N \sum_{l=1}^N A[k,l]e^{-2\imath\pi\left ( \frac{ik}{N}+\frac{jl}{N} \right )}$$
and  $$FT^{-1}(A)[i,j] = \frac{1}{N^2}\sum_{k=1}^N \sum_{l=1}^N A[k,l]e^{2\imath\pi\left ( \frac{ik}{N}+\frac{jl}{N} \right )}.$$

By construction, all the elements of each key $\omega$ (and its matrix version $\mathcal{W}$) are independent. The probability $P(G_{W_1} = G_{W_2})$ in Equation~\eqref{result} can be computed with the cumulative distribution of $FT^{-1}(\mathcal{W}_n)$ ($n \in \{1,2\}$) in  Equation~\eqref{linearize}.

\subsection{Computing the Cumulative Distribution}
\label{sec:distortion}
First, note that:
$$FT^{-1}(\mathcal{W}_{n}) = Z \mathcal{W}_{n} Z,$$
where $Z[k,l] = \frac{1}{N}e^{2\imath\pi kl/N}$. This shows that $FT^{-1}(\mathcal{W}_n)$ is a matrix version of a Gaussian vector. With this, we can get an exact formula for the probability distribution function of this inverse Fourier transform flattened to one dimension~\cite{zdvi}.
In fact, let $\tilde{\mathcal{W}_{n}}$ be the flattened vector of $\mathcal{W}_{n}$, where:
$$
\forall k \in \{1,...,N^2\}, \tilde{\mathcal{W}_{n}}[k] = \mathcal{W}_{n}[\lceil k/N \rceil ,(k-1) \mod N+1],
$$
where $\lceil k/N \rceil$ being the upper integer part of $k/N$ and $(k-1) \mod N$ being the rest of the Euclidean division of $k-1$ by $N$. Using these notations, we easily show that:
$$FT^{-1}(\tilde{\mathcal{W}_{n}}) = \tilde{Z} \mathcal{W}_{n},$$
where $\tilde{Z} = Z \bigotimes Z$ is the $N^2 \times N^2$ Kronecker product of $Z$ with itself. Let $(z_{k,l})$ be the coefficients of $Z$, then for $(k',l') \in \{1,...,N^2\}$:
$$
(\tilde{Z})_{k', l'} = z_{\lceil k' /N\rceil , \lceil l' /N\rceil} z_{(k'-1) \mod N+1, (l'-1) \mod N+1}.
$$
Since $\tilde{\mathcal{W}_{n}}$ is a Gaussian vector, $FT^{-1}(\tilde{\mathcal{W}_{n}})$ is also a Gaussian vector. Its average and standard deviation are computed using the matrix $\tilde{Z}$. Therefore, we can use standard approximation techniques to approximate Equation~\eqref{eq:P}. 

This section has successfully derived a precise analytical form for evaluating the probability of a watermark's uniqueness. However, considering the size of the vectors involved, large graphs will require a significant approximation time. In practice, direct experiments such as those in section~\ref{exp:unique} will be preferred. Despite this computational challenge, the analytical insights shed light on the theoretical components at stake in discretizing a watermark in an unweighted graph.

\paragraph{Application on large graphs.} The connection between graph densities and collisions is given by the theoretical analysis in Equation~\eqref{eq:P}. 

Indeed, as $av(A) = \frac{|E|}{N^2}$, the threshold for binarization is equal to the density divided by $N$. For the $N=1M$ graphs (density less than $10$) in Figure~\ref{fig:uniqueness} and the given keys:
$$av(A) \simeq 0$$
$$avg(|FT^{-1}(W)|) \simeq 0$$
$$avg(|A+FT^{-1}(W)[i,j]|) \simeq av(A)$$

Then, it is clear that the denser a graph, the more $A[i,j]$ coefficients are equal to $1$, and decreases the probability of collision (as seen in Equation~\eqref{eq:P}).

\subsection{Successful Embeddings: a Corollary of Uniqueness}
In this section, we show that studying a successful embedding is a corollary of the analytical form of the uniqueness. The embedding of a key $\omega$ in a graph $G$ is successful if the watermarked graph $G_W$ is different from the original graph $G$. In other words, $G_W$ must be related to its key $\omega$.
In fact, the key is binarized after spreading over the entire adjacency matrix. It can happen that the binarization discards a key built with insufficiently large parameters, so that no watermark is inserted into the graph. The study of successful embedding is therefore the estimation of $P(G_{W} = G)$.

\paragraph*{Proof}
Since $\texttt{Keygen}(0,0) =  []$ (an empty key) and  $\texttt{Embed}(G, []) = G$, studying $P(G_{W} = G)$ is a special case of Equation~\eqref{result} where $G_{W_1} = G_{W}$ (the graph watermarked with the key $\omega$) and $G_{W_2} = G$ (the graph watermarked with the empty key $[]$).

Note that in practice, the owner of the graph knows when the embedding has succeeded or failed: if the watermarked graph is equal to the original graph after an embedding attempt (which is linear to check in $O(|E|)$ with the \textit{edit distance} for example), it is sufficient for the owner to retry an embedding operation with larger key parameters until the two graphs are truly different. (This will be studied empirically in Section \ref{ss:distorsion}.)

\section{Scalability reduction}\label{a:reduction}
In Section~\ref{ss:scalability}, we explain that a scability reduction is applied to \scheme as it is the case in ~\cite{isc, COSN}. This section provides the details and explanation of such an operation. 

When considering graphs, the most significant information is contained in the vertices of the highest degree vertices; thus, analogous to low frequencies, in \scheme, the watermark is inserted into the lowest amplitude coefficients in the spectrum among the higher-degree vertices.
While the size of images on which the Cox \textit{et al.} scheme is typically applied is in the order of tens of thousands of pixels,
we also choose to take the top $N_0 = 10^4$ higher-degree vertices of $G$ to apply \scheme onto.

In this way, \texttt{Embed} and \texttt{Extract} are applied only on $N_0$ vertices and not on the $N$ vertices of the original graph. Their worst-case time complexities, computed in Section~\ref{ss:complexity}, are now $\mathcal{O}\left( N_0^2 \log N_0\right)$. However, to determine the subgraph on which to apply \scheme, it is necessary to select the $N_0$ vertices of highest degrees (which is calculated in $\mathcal{O}\left( N \log N_0\right)$). This cost is added naturally to the \texttt{Embed} function. 

The worst-time complexities of \scheme, with the dimensionality reduction, are summarized in Table~\ref{complexities}.

\begin{table}[]
\begin{center}
\begin{tabular}{ccc}
\scheme & raw complexities & adapted complexities\\
 \hline
\texttt{Keygen} & $\mathcal{O}(m)$ & $\mathcal{O}(m)$\\
\texttt{Embed} & $\mathcal{O}\left( N^2\log N\right)$ & $\mathcal{O}\left( (N + N_0^2) \log N_0\right)$ \\
\texttt{Extract} & $\mathcal{O}\left( N^2 \log N\right)$ & $\mathcal{O}\left( N_0^2 \log N_0\right)$
\end{tabular}
\end{center}
 \caption{Worst-time complexities of \scheme to watermark a graph $G$ with $N$ vertices with a key of length $m$ with and without the dimensionality reduction to $N_0$ vertices.
 }
\label{complexities}
\end{table}

\subsection{Guideline's complexities}
In Section~\ref{ss:distorsion}, we give practical guidelines for low distortion. We now studied this operation in detail, and its effect on the scalability of \scheme.

As mentioned before, we set the key length $m$ of \scheme as the key length used in~\cite{isc}. In fact, $m$ can be set in worst-case complexity $\mathcal{O}(N_0)$. Once the key length $m$ is set, it is always possible to find a value $\sigma$ to watermark the graph while achieving the distortion goal. Experiments have shown that for any graph $G$ and for any fixed key length $m \in [\![1, N_0*(N_0-1)/2]\!] $, there exists a value $\sigma_{max}$ to watermark the graph with a strictly positive edit distance. $\sigma_{max}$ can be estimated with at most $\mathcal{O}(\log N_0)$ operations.  An automated way to set $\sigma$ resulting in a small ED guarantee, is a dichotomous search in $[\![1, \sigma_{max}]\!]$. The worst-case time complexity of such setting is testing $\mathcal{O}(\sigma_{max} \log \sigma_{max})$ values for $\sigma$ when each test is of complexity $\mathcal{O}\left( N_0^2 \log N_0\right)$. To summarize, the worst-case time complexity for setting $\sigma$ knowing $m$ is 
$\mathcal{O}\left( N_0^2 \log N_0\right)$. This is the same as the worst-case complexity of the embedding function, so the proposed guidelines do not affect the scalability of \scheme.

\section{Additional experiments}\label{a:add-exp}
\subsection{(Goal 5) Testing undetectability w. Graph Neural Networks}
In addition to the properties stated in the paper and which follow the properties stated in the related papers, we add a Goal 5. We want to measure whether a graph neural network is able to distinguish a watermarked graph from an original graph. Indeed, if such a network existes, it could be maliciously used by an adversary in addition to the classical edge-flip attack.

We present a novel attack strategy. So far, the approach studied has been to modify the graph in an attempt to remove the watermark. While the two related approaches assume that their scheme is undetectable by construction, we decide to go a step further and propose a new attack that focuses on identifying the watermarked graphs instead. If successful, this implies that the watermark significantly degrades the graph despite a small ED. To achieve this, we use a conventional Graph Neural Network (GNN) that solves a graph classification task. To train a predictor for identifying watermarked graphs, we first need a labeled dataset. 

We generated $2,000$ independent Barabási-Albert graphs of $1,000$ vertices with an attachment parameter of $3$. Subsequently, we watermarked half of these graphs by generating $1,000$ independent keys with parameters $m = 200$ and $\sigma = 100$ (chosen as outlined in Section~\ref{p:fixed_params} to achieve an ED between $1$ and $100$ edges of the graph, i.e. between $0.034$ and $3.4$\% edges). 
Our dataset includes the $1,000$ non-watermarked graphs (of label 0) and the $1,000$ watermarked graphs (of label 1). Notably, we underline that the seed graphs for generating the watermarked graphs were \textbf{excluded} from the dataset, to avoid bias in the classification task.

We adapted the graph classification tutorial from the Python library called PyTorch Geometric~\cite{GNNtuto} with our dataset. The task of the GNN is to distinguish graphs into two classes: those who have been watermarked and those who were not. The degrees of the vertices have been added as their features in this dataset. In other words, the degree sequence of each graph will serve as a basis for attempts in detecting a deviation from the benign sequences of non watermarked graphs.
The training is set to $80$\% of the dataset, leaving $20$\% of graphs as a test set.

Our experiments show that with its default parameters or some classic variants of these (number of hidden channels, learning rate...), the GNN is not able to succeed in the task of discriminating between watermarked graphs and unwatermarked graphs. Indeed the accuracy on train and test datasets oscillates between 0.4775 and 0.5225 depending on the hyperparameters.

We conclude that this attack is ineffective against \scheme. This implies that the watermarks inserted in \scheme are small enough (ED between $0.034$ and $3.4$\% edges) to be undetectable by such a GNN classification attack.

\subsection{Testing Laplacian eigenvalue's stability}
In our examples, we focus on Edit Distance (ED) due to its generality and widespread use in the state-of-the-art. We also consider degree centrality, as shown in Figure~\ref{fig:spearman}. The next section discusses the impact of \scheme on the Laplacian eigenvalues of a graph, a metric that is sometimes used in graph studies. 

There is no theoretical guarantee of the invariance of the Laplacian eigenvalues after applying Cox-G. However, we performed additional experiments with Barabasi-Albert graphs of different sizes ($1k$ or $10k$ nodes) and densities (attachment parameter: $3$, $5$, or $7$). Experimentally, the maximum variation of an eigenvalue is at most $\pm-4\%$ of the initial value. However, there is no conservation of the smallest non-zero eigenvalue, which can vary by up to $2\%$ of its value (e.g., from $\sim 1.268$ to $\sim 1.297$ for a BA graph with $10k$ vertices and $70k$ edges). Depending on the intended use of the graph, this variation of up to $4\%$ over the entire spectrum (calculated as an infinite norm) may be acceptable.

In conclusion, we suggest that future readers test their own metrics of interest before sharing graphs watermarked with Cox-G. Even if there is no theoretical guarantee on the preserving of this metric, the variation induced for the watermark is sometimes negligible for the desired use.