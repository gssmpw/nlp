
\section{Background and Related Work}
\label{sec:Related Work}
\iffalse
\subsection{AI training needs FP operation}
In current Computing-in-Memory (CIM) architectures, integer MAC (Multiply-Accumulate) and fixed-point MAC operations are the most popular computation methods. This is primarily because integer and fixed-point arithmetic are simpler to implement and require fewer memory resources compared to floating-point operations. For edge devices, which are characterized by limited power budgets and memory constraints, these methods offer significant advantages in terms of energy efficiency and performance. Integer and fixed-point operations are ideal for many AI workloads, particularly in inference tasks, where precision can be traded off for efficiency. By contrast, floating-point (FP) MAC operations involve more complex calculations and larger memory requirements, making them less efficient for resource-constrained devices like those used at the edge. However, for tasks that require high precision, such as training large neural networks, floating-point operations become indispensable despite their cost.


\subsection{FP operations flow}
Floating-point computation is inherently more complex than integer or fixed-point computation because it involves managing both the mantissa and exponent of the numbers. To handle this complexity, floating-point MAC operations are generally divided into two key steps:

\subsubsection{Exponent Computation} 

\subsubsection{Mantissa Computation} 

 %All-Digital Structures for Floating-Point Computation
\subsection{Digital structure are prevalent}
Recent research in CIM architectures tends to favor all-digital structures for performing the two-step floating-point computation process. Digital approaches are advantageous because they are more scalable and easier to integrate with existing digital CMOS technologies. Digital circuits provide higher accuracy and flexibility compared to analog solutions, which often suffer from noise and variability issues in real-world applications. Specifically, in floating-point CIM architectures, digital circuits handle both the exponent and mantissa computations efficiently. However, one of the main challenges in these systems is the limited parallelism within the memory banks, which constrains the throughput of the CIM architecture when performing floating-point operations.
\fi







\iffalse




\subsection{FP calculation}
We first briefly introduce the essential FP arithmetic operations.
\subsubsection{FP format}
A general FP number in scientific notation is expressed as $FP=(-1)^S\cdot2^E\cdot1.M$. Here, S is sign(0 or 1); E is Exponent; M is mantissa. Taking the IEEE half-precision FP format(FP16) as an example, S is 1-bit, E is 5-bit, M is 10-bit, as shown below using two positive numbers for simplicity.

\subsubsection{FP Multiplication}
The multiplication of FP numbers can be represented by shown example below using two positive numbers for simplicity. 
\begin{equation}
    (2^{E_0} \cdot 1.M_0)\cdot(2^{E_1}\cdot1.M_1 = (2^{E_0+E_1})\cdot(1.M_0\cdot1.M_1)
\end{equation}


\subsubsection{FP addition}
The addition of FP numbers is non-trival, it involves following steps: first, find maximum exponent $E_{max} = max{(E_0,E_1)}$ among all exponents. Second, calculating the exponent difference, $E_i - E_{max}$, between each exponent and the maximum. Subsequently, the mantissa part would be shifted towards the right according to the exponent difference and then summed together. The sum will then undergo extra processes like truncation to comply with the standard FP format.  

\subsubsection{FP vector-matrix multiplication}

\fi





\subsection{Floating-Point Arithmetic Primitives}

FP arithmetic operations are crucial to high-precision DNNs. 
% models.

\noindent{\textbf{FP format:}} A general FP number in scientific notation is expressed as $f=(-1)^{S}\cdot 2^{E}\cdot 1.M$,
where $S$ ($S=0$ or $S=1$), $E$, and $M$ ($M\in(0,1)$) represent the sign, exponent, and mantissa (fraction) of the number, respectively.
The `1' before $M$ is a hidden bit that is not explicitly shown in the binary format of an FP number. 
$E$ is the actual exponent, with an offset applied to the exponent encoded in the standard format.
For example, in the IEEE 8-bit FP format (FP8, E4M3 or E5M2), $S$ is 1-bit, $E$ is 4(5)-bit, $M$ is 3(2)-bit (Fig.~\ref{fig: FP_intro}(b)).



\noindent{\textbf{FP multiplication:}} The multiplication of FP numbers is a straightforward process, involving exponent addition (\circled{1}) and mantissa multiplication (\circled{2}). 
For simplicity, the example below demonstrates this using two positive FP numbers.
% \vskip -6pt
\begin{equation}
             (2^{E_0}\cdot1.M_0) \cdot (2^{E_1}\cdot1.M_1) = \underset{\text{\circled{1}}}{\underbrace{(2^{E_0 +E_1})}} \cdot \underset{\text{\circled{2}}}{\underbrace{(1.M_0\cdot 1.M_1)}}. 
    \label{eq: fp_mul}
\end{equation}
    \vskip -3pt
\noindent{\textbf{FP addition:}} However, the addition of floating-point (FP) numbers is more complex and can be expressed as follows:
% Eq.~\eqref{eq: fp_add} shows an example using two positive numbers for simplicity.
\begin{equation}
             2^{E_0}\cdot1.M_0+2^{E_1}\cdot1.M_1 = \underset{\text{\circled{1}}}{\underbrace{ 2^{{E_{\max}}}}} \cdot \underset{\text{\circled{3}}}{\underbrace{
\left\{ 
             \begin{array}{lr}
              \underset{\text{\circled{2}}}{\underbrace{2^{{E_{0}}-{E_{\max}}}}} \cdot 1.M_{0} &  \\
                +&  \\
              \underset{\text{\circled{2}}}{\underbrace{2^{{E_{1}}-{E_{\max}}}}} \cdot 1.M_{1}
             \end{array}
\right.}}.
    \label{eq: fp_add}
\end{equation}
The process begins with alignment, where the maximum exponent, $E_{\max}=\max\{E_0, E_1\}$,  is identified among all exponents (\circled{1}), followed by calculating the exponent difference, $E_{0(1)}-{E_{\max}}$, for each exponent (\circled{2}).
Next, the mantissa is shifted to the right based on this exponent difference and then summed (\circled{3}).
Finally, the result undergoes additional steps, such as truncation, to conform to the standard FP format.

\noindent{\textbf{FP vector-matrix multiplication:}}  By generalizing Eq.~\eqref{eq: fp_add} to the accumulation of $n$ FP numbers, where each number, $2^{E_i}\cdot 1.M_{i}$ is assumed to be a product of a weight-activation pair, i.e., $(-1)^{W_{S,i}}\cdot2^{W_{E,i}}\cdot 1.W_{M,i}$ and $(-1)^{X_{S,i}}\cdot 2^{X_{E,i}}\cdot 1.X_{M,i}$ based on Eq.~\eqref{eq: fp_mul} (i.e., $E_i=W_{E,i}+X_{E,i}$ and $1.M_{i}=1.W_{M,i}\cdot1.X_{M,i}$), the vector-matrix multiplication (VMM) of FP numbers is achieved. 
This operation is fundamental to FP DNN models.
FP enables high-precision computations in DNN models, supporting the highest accuracy and best training quality~\cite{micikevicius2022fp8}.
Thus, efficient hardware acceleration for FP VMM is highly desirable.



\begin{figure}[!t]
    \centering
    \includegraphics[width=0.3\textwidth]{Figures/FP_intro2.png} % 
     \vskip -12pt
    \caption{(a) Illustration of conventional FP SRAM CIM architecture. $X_{E,i}$ and $W_{E,i}$ are the exponent parts of activation and weight. $X_{M,i}$ and $W_{M,i}$ are the mantissa parts of activation and weight. \textcircled{1}, \textcircled{2}, and \textcircled{3} are circuit-level representations of the steps in Eq.~\eqref{eq: fp_add}, (b) FP8 format (E4M3 and E5M2).}
    \label{fig: FP_intro}
     \vskip -15pt
\end{figure}











\iffalse


\subsection{Computing-in-Memory for FP Operation}
Figure 1 illustrated how digital CIM accelerate the FP VMM. 

Recent work are interested in using digtial CIM structure to accelerate the FP DNN models.  Tu et al. 's CIM macro operates all FP computation, but they transform the Float-point to Fixed-point first.  Lee et al. also implement a CIM macro that it only computes part of FP and the rest is operated in digital circuits. Digital computing provides exceptional accuracy for FP operation. However, digital FP CIM macros have been limited to a few edge applications due to the power consumption. 
Circuit-level techniques, 
Although there are many different optimization research, they primarily focus on the accuracy of computation and accelerate every exponent sum and mantissa multiplication in the digital domain, resulting in remarkable, yet necessary energy waste.

Our work based on the weight theory, the maximum error for mantissa multiplication can not exceed 1/4, so we take a more energy friendly, but precision loss way -- analog circuit to accelerate the multiplication. As for mantissa addition, which contribute 3/4 to the result, we keep operating it in digital domain. 





\fi










\subsection{Compute-in-Memory for FP DNNs}







Previous work has explored CIM acceleration for FP DNN models using emerging non-volatile memory (NVM) devices~\cite{luo2020accelerating, PF_CIM_RRAM_0}.
More recently, static random-access memory (SRAM) has become a cornerstone in the design of CIM systems for practical applications due to its exceptional accuracy in digital computing, along with superior performance, power efficiency, and area benefits~\cite{reCIM, Sparse_intense, SRAM_CIM_INT_1, SRAM_CIM_FP_1}.
For instance, a previous study~\cite{reCIM} introduced a reconfigurable FP/INT CIM processor capable of supporting both BFloat16 (BF16)/FP32 and INT8/16 in the same digital CIM macro.
Another study~\cite{Sparse_intense} proposed dividing FP operations into high-efficiency intensive-CIM and flexible sparse-digital parts, based on the observation that most FP exponents cluster within a narrow range. 
Circuit-level techniques such as time-domain exponent summation mechanisms have also been employed to enhance the energy efficiency of CIM macros~\cite{SRAM_CIM_FP_1}.

Nonetheless, these prior works share the common simplified architecture to accelerate FP VMM, as illustrated in Fig.~\ref{fig: FP_intro}(a), where all operations are performed in the digital domain.
The process begins with the alignment of exponents by summing the exponent parts of the weight-activation pairs (${W_{E,i}}$ and ${X_{E,i}}$, Step \circled{1}) and calculating the exponent difference between each sum, $E_i$,  and the maximum exponent, $E_{\max}$ (Step \circled{2}).
These exponent differences ($E_i-E_{\max}$) are then used to shift the mantissa parts of the activations, which are subsequently multiplied and accumulated (Step \circled{3}).
As shown in Fig.~\ref{fig: FP_intro}(a), these steps align with the operations described in Eq.~\eqref{eq: fp_add}.
Yet, our analysis reveals that by leveraging the inherent properties of FP arithmetic and the resilience of DNN models to minor computation errors, there is substantial untapped potential to use approximate computing (e.g., analog computing) to accelerate FP DNNs without sacrificing accuracy (Section~\ref{sec: addition}).

% This insight presents valuable opportunities to further enhance the performance of FP SRAM CIM macros.



\begin{figure*}
    \centering
    \includegraphics[width=0.8\textwidth]{Figures/Macro_Architecture-1.png} % 
     \vskip -6pt
    \caption{(a) Architecture overview of the proposed hybrid-domain FP CIM macro, (b) Hybrid-domain FP SRAM CIM mantissa unit, (c) Switched-capacitor array with the Flash ADC, (d) 6T SRAM, (e) pseudo XOR gate, (f) pseudo AND gate, LAC for sub-add (blue line) and MUL for ACIM (orange line).}
    \label{fig: Architecture}
     \vskip -15pt
\end{figure*}