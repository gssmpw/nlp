
\section{Hybrid-Domain FP CIM Architecture Design}

\subsection{Insight: Addition is More Important}
\label{sec: addition}


Through a careful examination of FP arithmetic, we have identified a lightweight approach for implementing SRAM CIM macros in a hardware-friendly manner, enhancing energy efficiency while preserving the accuracy of FP DNNs. To illustrate this key concept, we decompose the conventional FP mantissa multiplication in Eq.~\eqref{eq: fp_mul} into two parts as follows:
\begin{equation}
\begin{split}
&(1.M_0\cdot 1.M_1)=(1+M_0)\cdot (1+M_1)\\
 &=(\underset{\text{sub-ADD}}{\underbrace{1+M_0+M_1}}+ \underset{\text{sub-MUL}}{\underbrace{M_0 \cdot M_1}}).
\end{split}
    \label{eq: fp_mul_1}
\end{equation}
Here, $(1+M_0+M_1)$ and $(M_0\cdot M_1)$ are defined as mantissa sub-addition (sub-ADD) and mantissa sub-multiplication (sub-MUL), respectively.
Based on this decomposition and given that $M_{0(1)}\in(0,1)$, we analyze the significance of sub-MUL in mantissa multiplication\cite{cao2024addition}, i.e.,
\begin{equation}
\dfrac{M_0 \cdot M_1}{(1+M_0)\cdot(1+M_1)}=\dfrac{1}{(1+1/M_0)\cdot(1+1/M_1)}\leq \dfrac{1}{4}.
\label{eq: fp_mul_sig}
\end{equation}
Here, the `$=$' holds true if and only if $M_0=M_1	\rightarrow 1$.
Eq.~\eqref{eq: fp_mul_sig} shows that for a single weight-activation pair, if the accuracy of sub-ADD is maintained, the \textit{total computation error of mantissa multiplication (and thus the FP product) will not exceed $1/4$ of the true value, even with the aggressive removal of sub-MUL operations}. 
Our analysis reveals that while mantissa sub-MUL operations are computationally intensive, they often affect only a minority of FP products. 
In contrast, mantissa sub-ADD, though computationally lighter, constitutes the majority of FP products. Addition is also far more energy-efficient than multiplication; for instance, INT8 addition consumes only about 10\% of the energy required for INT8 multiplication, as shown in previous work~\cite{TPU}.
This key insight--`Addition is More Important'--guides our design strategy for FP SRAM CIM macro: \textbf{dedicating digital resources to ensure the precision of compute-light mantissa sub-ADD operations while leveraging energy-efficient analog computing for the more compute-intensive mantissa sub-MUL operations}.




\label{sec:method}









\subsection{Overall Macro Architecture}




With the above key insight, we integrate analog CIM into existing well-established digital FP CIM architectures~\cite{SRAM_CIM_FP_1,SRAM_CIM_INT_1} based on SRAM, achieving significant improvements in energy efficiency with minimal area overhead.
Fig.~\ref{fig: Architecture} illustrates the proposed architecture, designed for FP8 DNN models (E4M3) as an example.
Note that FP8 demonstrates state-of-the-art energy efficiency in DNN applications while preserving accuracy\cite{micikevicius2022fp8}. 
The exponent unit comprises core components such as the exponent summation array, $E_{\text{max}}$ identifier, exponent difference extractor, and mantissa alignment, in line with previous designs~\cite{time_domain,SRAM_CIM_FP_1}. 
The details are thus omitted here.
We focus on the mantissa part.
We introduce a novel hybrid-domain MAC unit to precisely manage mantissa sub-ADD in the digital realm while enhancing the energy efficiency of mantissa sub-MUL through analog computing.


Specifically, we demonstrate a hybrid-domain CIM mantissa MAC array with a $4 \times 4$ size as a proof of concept (Fig.~\ref{fig: Architecture}(b)). 
This $4 \times 4$ array is ideally suited for a 4-bit weight mantissa in E4M3 FP8 formate, including a 1-bit sign indicator.
Each bit of a weight mantissa $W_{M,i}$ is stored within the same row but across different columns of the 6T-SRAM cell array, ordered from the most significant bit (MSB) to the least significant bit (LSB).
There are two key design innovations in this MAC array: (1) an area-efficient hybrid-domain local computing cell (LCC) is embedded into each memory cell to facilitate both sub-ADD and sub-MUL;
(2) the analog partial sums of sub-MUL are accumulated into the analog domain to minimize analog-to-digital conversions.
In the following subsections, we will introduce how sub-ADD and sub-MUL are enabled with these circuit-level design innovations.
%with this MAC unit.

\iffalse
The MAC array uses aligned mantissa parts to achieve VMM with analog computing, sub-MUL, and digital computing, sub-ADD. 
The 

The sum of the multiplication between  $W_{M,i}$ and $X_{m,i}[j]$ are accumulated in cap computation unit, and converse from a analog signal to a 3b digital signal.



As is shown in Fig. \textcolor{blue}{~\ref{fig: Architecture}} (a), since Exponent Unit, Shift and ADD/Subtract, LDAT are digital circuits, they are high precision and easy to construct, we focused on the CIM mantissa MAC and capacitance computation and ADC transform. 

Fig. \textcolor{blue}{~\ref{fig: Architecture}} (b) shows the overall structure of MAC unit and CCA unit, the bit lines are omitted. 

Each bit of a weight mantissa $W_{M,i}$ is stored within the same row but across different columns of the 6T-SRAM cell array, ordered from the most significant bit (MSB) to the least significant bit (LSB). Mantissa MAC uses the aligned mantissa parts to achieve VMM with analog computing, sub-MUL, and digita computing, sub-ADD. The sum of the multiplication between  $W_{M,i}$ and $X_{m,i}[j]$ are accumulated in cap computation unit, and converse from a analog signal to a 3b digital signal.
\fi




\iffalse


Fig.xx illustrates the microarchitecture of the proposed FP SRAM CIM macro (targeting FP8 DNN models (E4M3) as an example), which contains the exponent unit and mantissa MAC unit.
We built this macro based on well-establish FP CIM architectures
which contains the exponent unit and mantissa MAC unit.


It consists of a time-domain CIM exponent summation array (ESA, to add the exponent parts of weight-activation pairs), time-domain MAX identifier (to find the maximum exponent sum), exponent difference extractor (EDE, to extract the exponent difference between each exponent sum and the maximum one), time-to-digital processing unit (to convert the exponent difference in the time domain into digits), EDE-based input alignment unit (to shift mantissa parts of activations), hybrid-domain CIM mantissa VMM array (HD-MVA, for FP VMM), analog-to-digital converters (ADCs), shift-and-add (S\&A) module, local digital adder tree, partial-product management (PM) unit, and other auxiliary modules such as the IO module and pipeline and timing control unit.
For exponent summation and alignment, we implement them in the time domain, since previous work~\cite{time_domain,SRAM_CIM_FP_1} has shown that the time-domain implementation is more energy efficient as compared to other digital implementation manners.
For mantissa multiplication, we develop a hybrid-domain SRAM CIM macro to precisely manage mantissa sub-ADD in the digital realm while enhancing the energy efficiency of mantissa sub-MUL through analog computing.

\fi


% We develop a hybrid

\iffalse

We developed a 4x4 hybrid mantissa computation CIM macro array as a demonstration. As is shown in Fig. \textcolor{blue}{~\ref{fig: Architecture}} (a), since Exponent Unit, Shift and ADD/Subtract, LDAT are digital circuits, they are high precision and easy to construct, we focused on the CIM mantissa MAC and capacitance computation and ADC transform.  Fig. \textcolor{blue}{~\ref{fig: Architecture}} (b) shows the overall structure of MAC unit and CCA unit, the bit lines are omitted. Each bit of a weight mantissa $W_{M,i}$ is stored within the same row but across different columns of the 6T-SRAM cell array, ordered from the most significant bit (MSB) to the least significant bit (LSB). Mantissa MAC uses the aligned mantissa parts to achieve VMM with analog computing, sub-MUL, and digita computing, sub-ADD. The sum of the multiplication between  $W_{M,i}$ and $X_{m,i}[j]$ are accumulated in cap computation unit, and converse from a analog signal to a 3b digital signal.

\fi





\iffalse

Fig.\textcolor{blue}{~\ref{fig: Architecture}} (c) i depicts the SRAM cell structure. Unlike most other CIM designs, the weight data are directly output to the local logic gate, bypassing control by the WL signal. This scheme enables weight read operations without requiring synchronization with the WL signal, effectively eliminating potential phase alignment issues between WL and the input signal.  As is known, SRAM cells are not always stable and may experience errors such as signal inversion during hold, read, even write operations. Additionally, if two SRAM cells inadvertently activate simultaneously, they may interfere with one another, leading to erroneous read or write operations.


\fi



\iffalse
In mantissa addition mode, with a bit-serial input manner,  a 2-b partial sum of the activation mantissa($X_{m,i}[j]$) and weight mantissa($W_M[j]$) can be generated along each row in the jth cycle. From Fig.\textcolor{blue}{~\ref{fig: Architecture}} (c), the detailed circuit of MAC is combined with LAC and LAS, when $X_{m,i}[j] = 1$, the $LAC_{i,j} = W_{M,i}[j]-V_{th}$. When $X_{M,i}[j]=0$, the $LAC_{i,j} = V_{ss}$ .  

Fig. \textcolor{blue}{~\ref{fig:Element Unit}} depicts the proposed mantissa computing unit. The basic unit consists of a 6T-SRAM for weight storage, local computing logic with an AND gate for addition, and an XOR gate for multiplication. Each column is equipped with two pass transistors (P0/P1) at the top, which control the precharging of all SRAM cells in that column via the global line.

\subsection{6T-SRAM Cell}
Fig. \textcolor{blue}{~\ref{fig: Architecture}} (d) illustrates the detailed structure of the 6T-SRAM. The N-transistors N0/N1 are used to decouple the storage region from the bit lines LBL and LBLB. The select line WL enables both read and write operations for the 1-bit weight mantissa (e.g., $W_M[j]$) in memory mode.

Each bit of a weight mantissa $W_{M,i}$ is stored within the same row but across different columns of the 6T-SRAM cell array, ordered from the most significant bit (MSB) to the least significant bit (LSB).
\fi

\subsection{Digital CIM -- Sub-ADD}

The sub-ADD operation is achieved through two pseudo-digital logic gates (a pseudo AND gate and a pseudo XOR gate) in the LCL. 
These gates, each composed of only two transistors to minimize area overhead, form a half-adder. 
Additionally, the pseudo AND gate is reused in a time-multiplexed manner for the sub-MUL operation, as shown in the next section. 
Using a bit-serial input, a 2-bit partial sum of the activation and weight mantissas is generated along each row in the $j^{\text{th}}$ cycle. When $X_{m,i}[j] = 1$, $\text{LAC}_{i,j} = W_{M,i}[j] - V_{\text{th}}$; when $X_{M,i}[j] = 0$, $\text{LAC}_{i,j} = V_{\text{SS}}$. 
Here, LAC represents the carry bit in the summation of the activation and weight mantissa, specifically for the $j^{\text{th}}$ bit of the mantissa part.
Similiarly, pseudo XOR is equivalent to normal XOR gate in half adder.
The 2-bit partial sum will be accumulated with the local adder to obtain the full sum of sub-ADD.
%Additionally, pseudo AND is a time-multiplexing logic gate. 




\subsection{Analog CIM -- Sub-MUL}


Fig.{~\ref{fig: Architecture}}(b) illustrates the sub-MUL operation, which is divided into two key steps: first, the pseudo AND gate in each cell performs the multiplication between the activation mantissa and weight mantissa; second, the resulting multiplication values from each cell are summed along each bit line.

% \textcolor{red}{The logic here be: (1) briefly introduce how the pseudo AND gate in each cell performs the multiplication; (2) how analog partial sums of each BL can be accumulated with capacitor; (3) How to compensate for the mismatch of the capacitor.}

\subsubsection{Analog multiplication by reusing the AND gate}
The pseudo AND logic gate obtains and sends the mantissa multiplication result to the capacitor computation unit through orange line on Fig.~\ref{fig: Architecture}(f). 
The analog calculation relies on Kirchhoff’s law: voltage on each GBLB decreases due to the discharge current from cells and increases with the charging current. The final voltage on each bit line reflects the net current of charged and discharged cells on each GBLB.


\subsubsection{Analog accumulation of partial sums}

We use the switched-capacitor array to efficiently accumulate analog partial sums of sub-ADD across bitlines.
%The capacitor charge-sharing computation integrates the analog multiplication signals, enabling efficient accumulation. 
This mechanism reduces the required analog-to-digital conversions from four to one per input cycle, significantly enhancing energy efficiency.
Assume the voltage of each $\text{GBLB}[i]$ is $V_i$ and the capacitance is $C_i$, then we have $Q_i= V_i \cdot C_i$.
% \begin{equation}
% Q_i= V_i \cdot C_i
% \end{equation}
All charges are collected first in the computation capacitance region as $Q_{total} =\sum_{i=0}^{3} V_i \cdot C_i$.
% \begin{equation}
% Q_{total} =\sum_{i=0}^{3} V_i \cdot C_i.
% \end{equation}
When switch S1 is activated, charge sharing occurs at this stage, resulting in $V_{o} =({\sum_{i=0}^{3} V_i \cdot C_i})/({\sum_{i=0}^{3} \cdot C_i})$.
% \begin{equation}
% V_{o} =\frac{\sum_{i=0}^{3} V_i \cdot C_i}{\sum_{i=0}^{3} \cdot C_i}.
% \end{equation}
Hence, $V_o$  represents the 1-bit multiplication of the activation mantissa and the 4-bit weight mantissa.
\iffalse
The final result is given by the formula:
\begin{equation}
\sum_{n=0}^{3}V_{o,n}=\sum_{n=0}^{3} \sum_{x=0}^{3} 2^m \cdot X_n[m] \cdot M_n.
\end{equation}
\fi


Fig.~\ref{fig: Architecture}(c) illustrates the multi-bit weight realization using charge sharing among computational capacitors. 
From the LSB to the MSB, the GBLB (orange line) connects to computation capacitors $C_0, C_1, C_2, C_3$ in a 1:2:4:8 capacitance ratio. 
Initially, all capacitors are pre-charged to ${V}_{\text{DD}}$.
To maintain a total of 8 units of capacitance on each column, the compensation capacitors $C_4, C_5, C_6, C_7$ are configured with ratios of 7:6:4:0, respectively.
After processing by the computation capacitors, the final voltage represents the multiplication of the 4-bit input with 1-bit weights. 
This voltage, representing a $4\times4$-bit multiply-accumulate (MAC) result, is isolated from the GBLB to prevent signal interference and converted to a 3-bit digital output by the Flash ADC.





The Flash ADC utilizes area-efficient sense amplifiers (SAs) instead of traditional analog comparators to reduce both area and energy consumption. Each 3-bit Flash ADC is composed of 7 SAs.
In a $4\times4$ CIM array, the multiplication result voltage on each global bit line buffer (GBLB) may yield 4 possible values, resulting in $4\times16$ potential outputs, which would typically require a 6-bit ADC for full encoding.
To optimize efficiency, we encode only the 3 most significant bits (MSBs). 
This selective encoding not only minimizes encoding area overlap due to nonlinearity but also reduces the impact on accuracy, ensuring effective performance with lower resource demands.
{Our simulation results in Section~\ref{sec:Results} show that 3-bit ADC is sufficient to achieve loss-less accuracy for inference/training.











\iffalse

Fig.  \textcolor{blue}{~\ref{fig: Architecture}} (b) illustrates the sub-multi, Mantissa multiplication is divided into two steps: multiplication between mantissa activation and mantissa weight on each cell's pseudo AND gate,  then summation of cells' multiplication result on each bit line. 

\subsubsection{Computation Capacitors}
The capacitor charge sharing computation is used to integrate the analog multiplication signal. It can significantly reduce the number of times of shift and add after ADC quantification. Not only can it save the area but also decrease the power consumption.  Assume the voltage of each GBLB[i] is $V_i$, the capacitance is $C_i$. That we have,
\begin{equation}
Q_i= V_i \cdot C_i
\end{equation}
All charges are collected first in the computation capacitance region, which is
\begin{equation}
Q_{total} =\sum_{i=0}^{3} V_i \cdot C_i
\end{equation}
When switch $S_1$ turns on, the charge sharing will happen in this stage, then we have,
\begin{equation}
V_{o} =\frac{\sum_{i=0}^{3} V_i \cdot C_i}{\sum_{i=0}^{3} \cdot C_i}
\end{equation}
Hence, $V_o$  is the 1-b multiplication of the activation mantissa and 4-b weight mantissa. 



The final result can be represent by the formula:
\begin{equation}
\sum_{n=0}^{3}V_{o,n}=\sum_{n=0}^{3} \sum_{x=0}^{3} 2^m \cdot X_n[m] \cdot M_n
\end{equation}



Fig. \textcolor{blue}{~\ref{fig: Architecture}  }(c)  illustrates the realization of multi-bit weight by using charge sharing among computation caps. From LSB to MSB, the corresponding GBLB connects to computation caps with 1:2:4:8 capacitance ratios. Before GBLB activation, all the caps are pre-charged to VDD first. The sum of the compensation caps and the computation caps is 8 units of capacitance, so the ratio of the compensation caps is 7:6:4:0. As mentioned above, the calculation in the analog domain relies on Kirchoff’s law. The voltage on each GBLB will decrease with the discharge current of the cells and increase with the charging current of the cells. The final voltage of each bit is determined by the number of cells charged and discharged on each GBLB. The final voltage after the computation caps calculation is obtained by multiplying the 4b input and the one-bit weights. After sampling the GBLB, the computation caps are isolated from the GBLB, so that the calculation of the computation caps will not be disturbed by the signal changes on the GBLB. The voltage represents the MAC result of 4x4b, which will be converted by the Flash ADC into 3b digital output.

The Flash ADC uses area-efficient sense-Amplifier instead of analog comparator to save area and reduce the energy consumption.  Each 3-b Flash ADC consists of 7 SAs. In a 4x4 CIM array, the voltage of the multiplication result on each GBLB may have 4 possible results, so the 4-bit GBLB can form a total of 4x16 results, which requires a 6-bit ADC to encode all of them. Therefore, in the ADC encoding, we only take 3 bits from the MSB position. This can not only reduce the overlap of the encoding area caused by nonlinearity, but also minimize the impact of reduced accuracy.


\fi