@incollection{Bengio+chapter2007,
author = {Bengio, Yoshua and LeCun, Yann},
booktitle = {Large Scale Kernel Machines},
publisher = {MIT Press},
title = {Scaling Learning Algorithms Towards {AI}},
year = {2007}
}

@article{Hinton06,
author = {Hinton, Geoffrey E. and Osindero, Simon and Teh, Yee Whye},
journal = {Neural Computation},
pages = {1527--1554},
title = {A Fast Learning Algorithm for Deep Belief Nets},
volume = {18},
year = {2006}
}
@

@book{goodfellow2016deep,
title={Deep learning},
author={Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron and Bengio, Yoshua},
volume={1},
year={2016},
publisher={MIT Press}
}
@incollection{atkinson1968human,
  title={Human memory: A proposed system and its control processes},
  author={Atkinson, Richard C and Shiffrin, Richard M},
  booktitle={Psychology of learning and motivation},
  volume={2},
  pages={89--195},
  year={1968},
  publisher={Elsevier}
}

@misc{deepseekv3,
      title={DeepSeek-V3 Technical Report}, 
      author={DeepSeek-AI and Aixin Liu and Bei Feng and Bing Xue and Bingxuan Wang and Bochao Wu and Chengda Lu and Chenggang Zhao and Chengqi Deng and Chenyu Zhang and Chong Ruan and Damai Dai and Daya Guo and Dejian Yang and Deli Chen and Dongjie Ji and Erhang Li and Fangyun Lin and Fucong Dai and Fuli Luo and Guangbo Hao and Guanting Chen and Guowei Li and H. Zhang and Han Bao and Hanwei Xu and Haocheng Wang and Haowei Zhang and Honghui Ding and Huajian Xin and Huazuo Gao and Hui Li and Hui Qu and J. L. Cai and Jian Liang and Jianzhong Guo and Jiaqi Ni and Jiashi Li and Jiawei Wang and Jin Chen and Jingchang Chen and Jingyang Yuan and Junjie Qiu and Junlong Li and Junxiao Song and Kai Dong and Kai Hu and Kaige Gao and Kang Guan and Kexin Huang and Kuai Yu and Lean Wang and Lecong Zhang and Lei Xu and Leyi Xia and Liang Zhao and Litong Wang and Liyue Zhang and Meng Li and Miaojun Wang and Mingchuan Zhang and Minghua Zhang and Minghui Tang and Mingming Li and Ning Tian and Panpan Huang and Peiyi Wang and Peng Zhang and Qiancheng Wang and Qihao Zhu and Qinyu Chen and Qiushi Du and R. J. Chen and R. L. Jin and Ruiqi Ge and Ruisong Zhang and Ruizhe Pan and Runji Wang and Runxin Xu and Ruoyu Zhang and Ruyi Chen and S. S. Li and Shanghao Lu and Shangyan Zhou and Shanhuang Chen and Shaoqing Wu and Shengfeng Ye and Shengfeng Ye and Shirong Ma and Shiyu Wang and Shuang Zhou and Shuiping Yu and Shunfeng Zhou and Shuting Pan and T. Wang and Tao Yun and Tian Pei and Tianyu Sun and W. L. Xiao and Wangding Zeng and Wanjia Zhao and Wei An and Wen Liu and Wenfeng Liang and Wenjun Gao and Wenqin Yu and Wentao Zhang and X. Q. Li and Xiangyue Jin and Xianzu Wang and Xiao Bi and Xiaodong Liu and Xiaohan Wang and Xiaojin Shen and Xiaokang Chen and Xiaokang Zhang and Xiaosha Chen and Xiaotao Nie and Xiaowen Sun and Xiaoxiang Wang and Xin Cheng and Xin Liu and Xin Xie and Xingchao Liu and Xingkai Yu and Xinnan Song and Xinxia Shan and Xinyi Zhou and Xinyu Yang and Xinyuan Li and Xuecheng Su and Xuheng Lin and Y. K. Li and Y. Q. Wang and Y. X. Wei and Y. X. Zhu and Yang Zhang and Yanhong Xu and Yanhong Xu and Yanping Huang and Yao Li and Yao Zhao and Yaofeng Sun and Yaohui Li and Yaohui Wang and Yi Yu and Yi Zheng and Yichao Zhang and Yifan Shi and Yiliang Xiong and Ying He and Ying Tang and Yishi Piao and Yisong Wang and Yixuan Tan and Yiyang Ma and Yiyuan Liu and Yongqiang Guo and Yu Wu and Yuan Ou and Yuchen Zhu and Yuduan Wang and Yue Gong and Yuheng Zou and Yujia He and Yukun Zha and Yunfan Xiong and Yunxian Ma and Yuting Yan and Yuxiang Luo and Yuxiang You and Yuxuan Liu and Yuyang Zhou and Z. F. Wu and Z. Z. Ren and Zehui Ren and Zhangli Sha and Zhe Fu and Zhean Xu and Zhen Huang and Zhen Zhang and Zhenda Xie and Zhengyan Zhang and Zhewen Hao and Zhibin Gou and Zhicheng Ma and Zhigang Yan and Zhihong Shao and Zhipeng Xu and Zhiyu Wu and Zhongyu Zhang and Zhuoshu Li and Zihui Gu and Zijia Zhu and Zijun Liu and Zilin Li and Ziwei Xie and Ziyang Song and Ziyi Gao and Zizheng Pan},
      year={2024},
      eprint={2412.19437},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2412.19437}, 
}

@misc{glm2024chatglm,
      title={ChatGLM: A Family of Large Language Models from GLM-130B to GLM-4 All Tools}, 
      author={Team GLM and Aohan Zeng and Bin Xu and Bowen Wang and Chenhui Zhang and Da Yin and Diego Rojas and Guanyu Feng and Hanlin Zhao and Hanyu Lai and Hao Yu and Hongning Wang and Jiadai Sun and Jiajie Zhang and Jiale Cheng and Jiayi Gui and Jie Tang and Jing Zhang and Juanzi Li and Lei Zhao and Lindong Wu and Lucen Zhong and Mingdao Liu and Minlie Huang and Peng Zhang and Qinkai Zheng and Rui Lu and Shuaiqi Duan and Shudan Zhang and Shulin Cao and Shuxun Yang and Weng Lam Tam and Wenyi Zhao and Xiao Liu and Xiao Xia and Xiaohan Zhang and Xiaotao Gu and Xin Lv and Xinghan Liu and Xinyi Liu and Xinyue Yang and Xixuan Song and Xunkai Zhang and Yifan An and Yifan Xu and Yilin Niu and Yuantao Yang and Yueyan Li and Yushi Bai and Yuxiao Dong and Zehan Qi and Zhaoyu Wang and Zhen Yang and Zhengxiao Du and Zhenyu Hou and Zihan Wang},
      year={2024},
      eprint={2406.12793},
      archivePrefix={arXiv},

}
@article{gpt3,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  journal={OpenAI blog},
  year={2019}
}
@book{mohri2018foundations,
  title={Foundations of machine learning},
  author={Mohri, Mehryar and Rostamizadeh, Afshin and Talwalkar, Ameet},
  year={2018},
  publisher={MIT press}
}
@book{cover1999elements,
  title={Elements of information theory},
  author={Cover, Thomas M},
  year={1999},
  publisher={John Wiley \& Sons}
}
@misc{tishby2015deep,
      title={Deep Learning and the Information Bottleneck Principle}, 
      author={Naftali Tishby and Noga Zaslavsky},
      year={2015},
      eprint={1503.02406},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@inproceedings{komeili-etal-2022-internet,
    title = "{I}nternet-Augmented Dialogue Generation",
    author = "Komeili, Mojtaba  and
      Shuster, Kurt  and
      Weston, Jason",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-long.579",
    doi = "10.18653/v1/2022.acl-long.579",
    pages = "8460--8478",
}
@misc{bge_m3,
  title={BGE M3-Embedding: Multi-Lingual, Multi-Functionality, Multi-Granularity Text Embeddings Through Self-Knowledge Distillation},
  author={Chen, Jianlv and Xiao, Shitao and Zhang, Peitian and Luo, Kun and Lian, Defu and Liu, Zheng},
  year={2023},
  eprint={2309.07597},
  archivePrefix={arXiv},
  primaryClass={cs.CL}
}
@inproceedings{shuster-etal-2021-retrieval-augmentation,
    title = "Retrieval Augmentation Reduces Hallucination in Conversation",
    author = "Shuster, Kurt  and
      Poff, Spencer  and
      Chen, Moya  and
      Kiela, Douwe  and
      Weston, Jason",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2021",
    month = nov,
    year = "2021",
    address = "Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.findings-emnlp.320",
    doi = "10.18653/v1/2021.findings-emnlp.320",
    pages = "3784--3803",
}


@inproceedings{qian2024grounding,
  author       = {Hongjin Qian and
                  Zheng Liu and
                  Kelong Mao and
                  Yujia Zhou and
                  Zhicheng Dou},
  editor       = {Lun{-}Wei Ku and
                  Andre Martins and
                  Vivek Srikumar},
  title        = {Grounding Language Model with Chunking-Free In-Context Retrieval},
  booktitle    = {Proceedings of the 62nd Annual Meeting of the Association for Computational
                  Linguistics (Volume 1: Long Papers), {ACL} 2024, Bangkok, Thailand,
                  August 11-16, 2024},
  pages        = {1298--1311},
  publisher    = {Association for Computational Linguistics},
  year         = {2024},
  url          = {https://doi.org/10.18653/v1/2024.acl-long.71},
  doi          = {10.18653/V1/2024.ACL-LONG.71},
  timestamp    = {Tue, 24 Sep 2024 10:55:39 +0200},
  biburl       = {https://dblp.org/rec/conf/acl/Qian0M0D24.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{longlora,
  title={LongLoRA: Efficient Fine-tuning of Long-Context Large Language Models},
  author={Yukang Chen and Shengju Qian and Haotian Tang and Xin Lai and Zhijian Liu and Song Han and Jiaya Jia},
  journal={arXiv:2309.12307},
  year={2023}
}


}

@article{abdin2024phi3,
  author       = {Marah I Abdin and
                  Sam Ade Jacobs and
                  Ammar Ahmad Awan and
                  Jyoti Aneja and
                  Ahmed Awadallah and
                  Hany Awadalla and
                  Nguyen Bach and
                  Amit Bahree and
                  Arash Bakhtiari and
                  Harkirat S. Behl and
                  Alon Benhaim and
                  Misha Bilenko and
                  Johan Bjorck and
                  S{\'{e}}bastien Bubeck and
                  Martin Cai and
                  Caio C{\'{e}}sar Teodoro Mendes and
                  Weizhu Chen and
                  Vishrav Chaudhary and
                  Parul Chopra and
                  Allie Del Giorno and
                  Gustavo de Rosa and
                  Matthew Dixon and
                  Ronen Eldan and
                  Dan Iter and
                  Amit Garg and
                  Abhishek Goswami and
                  Suriya Gunasekar and
                  Emman Haider and
                  Junheng Hao and
                  Russell J. Hewett and
                  Jamie Huynh and
                  Mojan Javaheripi and
                  Xin Jin and
                  Piero Kauffmann and
                  Nikos Karampatziakis and
                  Dongwoo Kim and
                  Mahoud Khademi and
                  Lev Kurilenko and
                  James R. Lee and
                  Yin Tat Lee and
                  Yuanzhi Li and
                  Chen Liang and
                  Weishung Liu and
                  Eric Lin and
                  Zeqi Lin and
                  Piyush Madan and
                  Arindam Mitra and
                  Hardik Modi and
                  Anh Nguyen and
                  Brandon Norick and
                  Barun Patra and
                  Daniel Perez{-}Becker and
                  Thomas Portet and
                  Reid Pryzant and
                  Heyang Qin and
                  Marko Radmilac and
                  Corby Rosset and
                  Sambudha Roy and
                  Olatunji Ruwase and
                  Olli Saarikivi and
                  Amin Saied and
                  Adil Salim and
                  Michael Santacroce and
                  Shital Shah and
                  Ning Shang and
                  Hiteshi Sharma and
                  Xia Song and
                  Masahiro Tanaka and
                  Xin Wang and
                  Rachel Ward and
                  Guanhua Wang and
                  Philipp Witte and
                  Michael Wyatt and
                  Can Xu and
                  Jiahang Xu and
                  Sonali Yadav and
                  Fan Yang and
                  Ziyi Yang and
                  Donghan Yu and
                  Chengruidong Zhang and
                  Cyril Zhang and
                  Jianwen Zhang and
                  Li Lyna Zhang and
                  Yi Zhang and
                  Yue Zhang and
                  Yunan Zhang and
                  Xiren Zhou},
  title        = {Phi-3 Technical Report: {A} Highly Capable Language Model Locally
                  on Your Phone},
  journal      = {CoRR},
  volume       = {abs/2404.14219},
  year         = {2024},
  url          = {https://doi.org/10.48550/arXiv.2404.14219},
  doi          = {10.48550/ARXIV.2404.14219},
  eprinttype    = {arXiv},
  eprint       = {2404.14219},
  timestamp    = {Mon, 16 Sep 2024 12:58:56 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2404-14219.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{
peng2024yarn,
title={Ya{RN}: Efficient Context Window Extension of Large Language Models},
author={Bowen Peng and Jeffrey Quesnelle and Honglu Fan and Enrico Shippole},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=wHBfxhZu1u}
}

@inproceedings{liukivi,
  title={KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache},
  author={Liu, Zirui and Yuan, Jiayi and Jin, Hongye and Zhong, Shaochen and Xu, Zhaozhuo and Braverman, Vladimir and Chen, Beidi and Hu, Xia},
  booktitle={Forty-first International Conference on Machine Learning},
  year={2024}
}

@misc{luo2024bge,
      title={BGE Landmark Embedding: A Chunking-Free Embedding Method For Retrieval Augmented Long-Context Large Language Models}, 
      author={Kun Luo and Zheng Liu and Shitao Xiao and Kang Liu},
      year={2024},
      eprint={2402.11573},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@article{liu2005statistical,
  title={Statistical language modeling for information retrieval.},
  author={Liu, Xiaoyong and Croft, W Bruce},
  journal={Annu. Rev. Inf. Sci. Technol.},
  volume={39},
  number={1},
  pages={1--31},
  year={2005}
}
@misc{wang2023learning,
      title={Learning to Filter Context for Retrieval-Augmented Generation}, 
      author={Zhiruo Wang and Jun Araki and Zhengbao Jiang and Md Rizwan Parvez and Graham Neubig},
      year={2023},
      eprint={2311.08377},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{edge2024localglobalgraphrag,
      title={From Local to Global: A Graph RAG Approach to Query-Focused Summarization}, 
      author={Darren Edge and Ha Trinh and Newman Cheng and Joshua Bradley and Alex Chao and Apurva Mody and Steven Truitt and Jonathan Larson},
      year={2024},
      eprint={2404.16130},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2404.16130}, 
}
@article{rafailov2024direct,
  title={Direct preference optimization: Your language model is secretly a reward model},
  author={Rafailov, Rafael and Sharma, Archit and Mitchell, Eric and Manning, Christopher D and Ermon, Stefano and Finn, Chelsea},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}
@misc{asai2023selfraglearningretrievegenerate,
      title={Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection}, 
      author={Akari Asai and Zeqiu Wu and Yizhong Wang and Avirup Sil and Hannaneh Hajishirzi},
      year={2023},
      eprint={2310.11511},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2310.11511}, 
}
@article{qwen2,
  title={Qwen2 technical report},
  author={Yang, An and Yang, Baosong and Hui, Binyuan and Zheng, Bo and Yu, Bowen and Zhou, Chang and Li, Chengpeng and Li, Chengyuan and Liu, Dayiheng and Huang, Fei and others},
  journal={arXiv preprint arXiv:2407.10671},
  year={2024}
}
@misc{qian2024memoragmovingnextgenrag,
      title={MemoRAG: Moving towards Next-Gen RAG Via Memory-Inspired Knowledge Discovery}, 
      author={Hongjin Qian and Peitian Zhang and Zheng Liu and Kelong Mao and Zhicheng Dou},
      year={2024},
      eprint={2409.05591},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2409.05591}, 
}
@inproceedings{lee2024humaninspiredreadingagentgist,
  author       = {Kuang{-}Huei Lee and
                  Xinyun Chen and
                  Hiroki Furuta and
                  John F. Canny and
                  Ian Fischer},
  title        = {A Human-Inspired Reading Agent with Gist Memory of Very Long Contexts},
  booktitle    = {Forty-first International Conference on Machine Learning, {ICML} 2024,
                  Vienna, Austria, July 21-27, 2024},
  publisher    = {OpenReview.net},
  year         = {2024},
  url          = {https://openreview.net/forum?id=OTmcsyEO5G},
  timestamp    = {Mon, 02 Sep 2024 16:55:26 +0200},
  biburl       = {https://dblp.org/rec/conf/icml/LeeCFCF24.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@inproceedings{wang2024knowledge,
  author       = {Yu Wang and
                  Nedim Lipka and
                  Ryan A. Rossi and
                  Alexa F. Siu and
                  Ruiyi Zhang and
                  Tyler Derr},
  editor       = {Michael J. Wooldridge and
                  Jennifer G. Dy and
                  Sriraam Natarajan},
  title        = {Knowledge Graph Prompting for Multi-Document Question Answering},
  booktitle    = {Thirty-Eighth {AAAI} Conference on Artificial Intelligence, {AAAI}
                  2024, Thirty-Sixth Conference on Innovative Applications of Artificial
                  Intelligence, {IAAI} 2024, Fourteenth Symposium on Educational Advances
                  in Artificial Intelligence, {EAAI} 2014, February 20-27, 2024, Vancouver,
                  Canada},
  pages        = {19206--19214},
  publisher    = {{AAAI} Press},
  year         = {2024},
  doi          = {10.1609/AAAI.V38I17.29889},
  timestamp    = {Tue, 02 Apr 2024 16:32:09 +0200},
  biburl       = {https://dblp.org/rec/conf/aaai/0160LRSZD24.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}



@article{chan2024rqraglearningrefinequeries,
  author       = {Chi{-}Min Chan and
                  Chunpu Xu and
                  Ruibin Yuan and
                  Hongyin Luo and
                  Wei Xue and
                  Yike Guo and
                  Jie Fu},
  title        = {{RQ-RAG:} Learning to Refine Queries for Retrieval Augmented Generation},
  journal      = {CoRR},
  volume       = {abs/2404.00610},
  year         = {2024},
  url          = {https://doi.org/10.48550/arXiv.2404.00610},
  doi          = {10.48550/ARXIV.2404.00610},
  eprinttype    = {arXiv},
  eprint       = {2404.00610},
  timestamp    = {Sun, 06 Oct 2024 21:24:58 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2404-00610.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@misc{qian2024longllmsnecessitylongcontexttasks,
      title={Are Long-LLMs A Necessity For Long-Context Tasks?}, 
      author={Hongjin Qian and Zheng Liu and Peitian Zhang and Kelong Mao and Yujia Zhou and Xu Chen and Zhicheng Dou},
      year={2024},
      eprint={2405.15318},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2405.15318}, 
}

@misc{pan2024llmlingua2datadistillationefficient,
      title={LLMLingua-2: Data Distillation for Efficient and Faithful Task-Agnostic Prompt Compression}, 
      author={Zhuoshi Pan and Qianhui Wu and Huiqiang Jiang and Menglin Xia and Xufang Luo and Jue Zhang and Qingwei Lin and Victor Rühle and Yuqing Yang and Chin-Yew Lin and H. Vicky Zhao and Lili Qiu and Dongmei Zhang},
      year={2024},
      eprint={2403.12968},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2403.12968}, 
}
@misc{qwen2025qwen25technicalreport,
      title={Qwen2.5 Technical Report}, 
      author={Qwen and : and An Yang and Baosong Yang and Beichen Zhang and Binyuan Hui and Bo Zheng and Bowen Yu and Chengyuan Li and Dayiheng Liu and Fei Huang and Haoran Wei and Huan Lin and Jian Yang and Jianhong Tu and Jianwei Zhang and Jianxin Yang and Jiaxi Yang and Jingren Zhou and Junyang Lin and Kai Dang and Keming Lu and Keqin Bao and Kexin Yang and Le Yu and Mei Li and Mingfeng Xue and Pei Zhang and Qin Zhu and Rui Men and Runji Lin and Tianhao Li and Tianyi Tang and Tingyu Xia and Xingzhang Ren and Xuancheng Ren and Yang Fan and Yang Su and Yichang Zhang and Yu Wan and Yuqiong Liu and Zeyu Cui and Zhenru Zhang and Zihan Qiu},
      year={2025},
      eprint={2412.15115},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2412.15115}, 
}
@inproceedings{gao2022precisezeroshotdenseretrieval,
  author       = {Luyu Gao and
                  Xueguang Ma and
                  Jimmy Lin and
                  Jamie Callan},
  editor       = {Anna Rogers and
                  Jordan L. Boyd{-}Graber and
                  Naoaki Okazaki},
  title        = {Precise Zero-Shot Dense Retrieval without Relevance Labels},
  booktitle    = {Proceedings of the 61st Annual Meeting of the Association for Computational
                  Linguistics (Volume 1: Long Papers), {ACL} 2023, Toronto, Canada,
                  July 9-14, 2023},
  pages        = {1762--1777},
  publisher    = {Association for Computational Linguistics},
  year         = {2023},
  url          = {https://doi.org/10.18653/v1/2023.acl-long.99},
  doi          = {10.18653/V1/2023.ACL-LONG.99},
  timestamp    = {Thu, 10 Aug 2023 12:36:02 +0200},
  biburl       = {https://dblp.org/rec/conf/acl/GaoMLC23.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@misc{together2023redpajama,
  author = {Soboleva, Daria and Al-Khateeb, Faisal and Myers, Robert and Steeves, Jacob R and Hestness, Joel and Dey, Nolan},
  title = {{SlimPajama: A 627B token cleaned and deduplicated version of RedPajama}},
  month = {June},
  year = 2023,
  url = {https://huggingface.co/datasets/cerebras/SlimPajama-627B},
}
@misc{chen2024longloraefficientfinetuninglongcontext,
      title={LongLoRA: Efficient Fine-tuning of Long-Context Large Language Models}, 
      author={Yukang Chen and Shengju Qian and Haotian Tang and Xin Lai and Zhijian Liu and Song Han and Jiaya Jia},
      year={2024},
      eprint={2309.12307},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2309.12307}, 
}
@inproceedings{gqa,
  author       = {Joshua Ainslie and
                  James Lee{-}Thorp and
                  Michiel de Jong and
                  Yury Zemlyanskiy and
                  Federico Lebr{\'{o}}n and
                  Sumit Sanghai},
  editor       = {Houda Bouamor and
                  Juan Pino and
                  Kalika Bali},
  title        = {{GQA:} Training Generalized Multi-Query Transformer Models from Multi-Head
                  Checkpoints},
  booktitle    = {Proceedings of the 2023 Conference on Empirical Methods in Natural
                  Language Processing, {EMNLP} 2023, Singapore, December 6-10, 2023},
  pages        = {4895--4901},
  publisher    = {Association for Computational Linguistics},
  year         = {2023},
  url          = {https://doi.org/10.18653/v1/2023.emnlp-main.298},
  doi          = {10.18653/V1/2023.EMNLP-MAIN.298},
  timestamp    = {Fri, 12 Apr 2024 13:11:50 +0200},
  biburl       = {https://dblp.org/rec/conf/emnlp/AinslieLJZLS23.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@misc{vaswani2023attentionneed,
      title={Attention Is All You Need}, 
      author={Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
      year={2023},
      eprint={1706.03762},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1706.03762}, 
}

@article{dao2022flashattention,
  title={Flashattention: Fast and memory-efficient exact attention with io-awareness},
  author={Dao, Tri and Fu, Dan and Ermon, Stefano and Rudra, Atri and R{\'e}, Christopher},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={16344--16359},
  year={2022}
}

@misc{gutiérrez2024hipporagneurobiologicallyinspiredlongterm,
      title={HippoRAG: Neurobiologically Inspired Long-Term Memory for Large Language Models}, 
      author={Bernal Jiménez Gutiérrez and Yiheng Shu and Yu Gu and Michihiro Yasunaga and Yu Su},
      year={2024},
      eprint={2405.14831},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2405.14831}, 
}
@article{RAGBench,
  author       = {Robert Friel and
                  Masha Belyi and
                  Atindriyo Sanyal},
  title        = {RAGBench: Explainable Benchmark for Retrieval-Augmented Generation
                  Systems},
  journal      = {CoRR},
  volume       = {abs/2407.11005},
  year         = {2024},
  url          = {https://doi.org/10.48550/arXiv.2407.11005},
  doi          = {10.48550/ARXIV.2407.11005},
  eprinttype    = {arXiv},
  eprint       = {2407.11005},
  timestamp    = {Fri, 23 Aug 2024 14:08:56 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2407-11005.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{CRAG,
  author       = {Xiao Yang and
                  Kai Sun and
                  Hao Xin and
                  Yushi Sun and
                  Nikita Bhalla and
                  Xiangsen Chen and
                  Sajal Choudhary and
                  Rongze Daniel Gui and
                  Ziran Will Jiang and
                  Ziyu Jiang and
                  Lingkun Kong and
                  Brian Moran and
                  Jiaqi Wang and
                  Yifan Xu and
                  An Yan and
                  Chenyu Yang and
                  Eting Yuan and
                  Hanwen Zha and
                  Nan Tang and
                  Lei Chen and
                  Nicolas Scheffer and
                  Yue Liu and
                  Nirav Shah and
                  Rakesh Wanga and
                  Anuj Kumar and
                  Scott Yih and
                  Xin Dong},
  editor       = {Amir Globersons and
                  Lester Mackey and
                  Danielle Belgrave and
                  Angela Fan and
                  Ulrich Paquet and
                  Jakub M. Tomczak and
                  Cheng Zhang},
  title        = {{CRAG} - Comprehensive {RAG} Benchmark},
  booktitle    = {Advances in Neural Information Processing Systems 38: Annual Conference
                  on Neural Information Processing Systems 2024, NeurIPS 2024, Vancouver,
                  BC, Canada, December 10 - 15, 2024},
  year         = {2024},
  url          = {http://papers.nips.cc/paper\_files/paper/2024/hash/1435d2d0fca85a84d83ddcb754f58c29-Abstract-Datasets\_and\_Benchmarks\_Track.html},
  timestamp    = {Wed, 05 Feb 2025 17:21:59 +0100},
  biburl       = {https://dblp.org/rec/conf/nips/YangSXSBCCGJJKM24.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@article{LegalBench,
  author       = {Nicholas Pipitone and
                  Ghita Houir Alami},
  title        = {LegalBench-RAG: {A} Benchmark for Retrieval-Augmented Generation in
                  the Legal Domain},
  journal      = {CoRR},
  volume       = {abs/2408.10343},
  year         = {2024},
  url          = {https://doi.org/10.48550/arXiv.2408.10343},
  doi          = {10.48550/ARXIV.2408.10343},
  eprinttype    = {arXiv},
  eprint       = {2408.10343},
  timestamp    = {Tue, 24 Sep 2024 17:36:31 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2408-10343.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{multihopbench,
  author       = {Yixuan Tang and
                  Yi Yang},
  title        = {MultiHop-RAG: Benchmarking Retrieval-Augmented Generation for Multi-Hop
                  Queries},
  journal      = {CoRR},
  volume       = {abs/2401.15391},
  year         = {2024},
  url          = {https://doi.org/10.48550/arXiv.2401.15391},
  doi          = {10.48550/ARXIV.2401.15391},
  eprinttype    = {arXiv},
  eprint       = {2401.15391},
  timestamp    = {Tue, 06 Feb 2024 14:15:49 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2401-15391.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
% Use this file for citations not found in the ACL Anthology (contained in "anthology.bib").
@String(ICLR = {International Conference on Learning Representations})
@String(NeurIPS = {Advances in Neural Information Processing Systems})
@String(ICML = {International Conference on Machine Learning})
@String(AAAI = {Proceedings of the AAAI Conference on Artificial Intelligence})
@misc{gao2024retrievalaugmented,
      title={Retrieval-Augmented Generation for Large Language Models: A Survey}, 
      author={Yunfan Gao and Yun Xiong and Xinyu Gao and Kangxiang Jia and Jinliu Pan and Yuxi Bi and Yi Dai and Jiawei Sun and Qianyu Guo and Meng Wang and Haofen Wang},
      year={2024},
      eprint={2312.10997},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{qian2023optimizing,
      title={Optimizing Factual Accuracy in Text Generation through Dynamic Knowledge Selection}, 
      author={Hongjin Qian and Zhicheng Dou and Jiejun Tan and Haonan Chen and Haoqi Gu and Ruofei Lai and Xinyu Zhang and Zhao Cao and Ji-Rong Wen},
      year={2023},
      eprint={2308.15711},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{grattafiori2024llama3herdmodels,
      title={The Llama 3 Herd of Models}, 
      author={MetaAI },
      year={2024},
      eprint={2407.21783},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2407.21783}, 
}

@misc{qian2023webbrain,
      title={WebBrain: Learning to Generate Factually Correct Articles for Queries by Grounding on Large Web Corpus}, 
      author={Hongjing Qian and Yutao Zhu and Zhicheng Dou and Haoqi Gu and Xinyu Zhang and Zheng Liu and Ruofei Lai and Zhao Cao and Jian-Yun Nie and Ji-Rong Wen},
      year={2023},
      eprint={2304.04358},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{zhao2024surveylargelanguagemodels,
      title={A Survey of Large Language Models}, 
      author={Wayne Xin Zhao and Kun Zhou and Junyi Li and Tianyi Tang and Xiaolei Wang and Yupeng Hou and Yingqian Min and Beichen Zhang and Junjie Zhang and Zican Dong and Yifan Du and Chen Yang and Yushuo Chen and Zhipeng Chen and Jinhao Jiang and Ruiyang Ren and Yifan Li and Xinyu Tang and Zikang Liu and Peiyu Liu and Jian-Yun Nie and Ji-Rong Wen},
      year={2024},
      eprint={2303.18223},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2303.18223}, 
}
@misc{mao2023large,
      title={Large Language Models Know Your Contextual Search Intent: A Prompting Framework for Conversational Search}, 
      author={Kelong Mao and Zhicheng Dou and Fengran Mo and Jiewen Hou and Haonan Chen and Hongjin Qian},
      year={2023},
      eprint={2303.06573},
      archivePrefix={arXiv},
      primaryClass={cs.IR}
}
@misc{xiao2024efficientstreaminglanguagemodels,
      title={Efficient Streaming Language Models with Attention Sinks}, 
      author={Guangxuan Xiao and Yuandong Tian and Beidi Chen and Song Han and Mike Lewis},
      year={2024},
      eprint={2309.17453},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2309.17453}, 
}

@misc{yoon2024compactcompressingretrieveddocuments,
      title={CompAct: Compressing Retrieved Documents Actively for Question Answering}, 
      author={Chanwoong Yoon and Taewhoo Lee and Hyeon Hwang and Minbyul Jeong and Jaewoo Kang},
      year={2024},
      eprint={2407.09014},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2407.09014}, 
}
@inproceedings{guo22webformer,
author = {Guo, Yu and Ma, Zhengyi and Mao, Jiaxin and Qian, Hongjin and Zhang, Xinyu and Jiang, Hao and Cao, Zhao and Dou, Zhicheng},
title = {Webformer: Pre-training with Web Pages for Information Retrieval},
year = {2022},
isbn = {9781450387323},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3477495.3532086},
doi = {10.1145/3477495.3532086},
abstract = {Pre-trained language models (PLMs) have achieved great success in the area of Information Retrieval. Studies show that applying these models to ad-hoc document ranking can achieve better retrieval effectiveness. However, on the Web, most information is organized in the form of HTML web pages. In addition to the pure text content, the structure of the content organized by HTML tags is also an important part of the information delivered on a web page. Currently, such structured information is totally ignored by pre-trained models which are trained solely based on text content. In this paper, we propose to leverage large-scale web pages and their DOM (Document Object Model) tree structures to pre-train models for information retrieval. We argue that using the hierarchical structure contained in web pages, we can get richer contextual information for training better language models. To exploit this kind of information, we devise four pre-training objectives based on the structure of web pages, then pre-train a Transformer model towards these tasks jointly with traditional masked language model objective. Experimental results on two authoritative ad-hoc retrieval datasets prove that our model can significantly improve ranking performance compared to existing pre-trained models.},
booktitle = {Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {1502–1512},
numpages = {11},
keywords = {web page, pre-training, dom tree, ad-hoc retrieval},
location = {<conf-loc>, <city>Madrid</city>, <country>Spain</country>, </conf-loc>},
series = {SIGIR '22}
}
@inproceedings{lewis2020retrieval,
  title="{R}etrieval-{A}ugmented {G}eneration for Knowledge-Intensive {NLP} Tasks",
  author={Lewis, Patrick and Perez, Ethan and Piktus, Aleksandra and Petroni, Fabio and Karpukhin, Vladimir and Goyal, Naman and K\"{u}ttler, Heinrich and Lewis, Mike and Yih, Wen-tau and Rockt\"{a}schel, Tim and Riedel, Sebastian and Kiela, Douwe},
  booktitle=NeurIPS,
  pages={9459--9474},
  url={https://proceedings.neurips.cc/paper_files/paper/2020/file/6b493230205f780e1bc26945df7481e5-Paper.pdf},
  volume={33},
  year={2020}
}
@article{xu2024think,
  title={Think: Thinner key cache by query-driven pruning},
  author={Xu, Yuhui and Jie, Zhanming and Dong, Hanze and Wang, Lei and Lu, Xudong and Zhou, Aojun and Saha, Amrita and Xiong, Caiming and Sahoo, Doyen},
  journal={arXiv preprint arXiv:2407.21018},
  year={2024}
}
@article{li2024snapkv,
  title={Snapkv: Llm knows what you are looking for before generation},
  author={Li, Yuhong and Huang, Yingbing and Yang, Bowen and Venkitesh, Bharat and Locatelli, Acyr and Ye, Hanchen and Cai, Tianle and Lewis, Patrick and Chen, Deming},
  journal={arXiv preprint arXiv:2404.14469},
  year={2024}
}

@inproceedings{kilt,
  author       = {Fabio Petroni and
                  Aleksandra Piktus and
                  Angela Fan and
                  Patrick S. H. Lewis and
                  Majid Yazdani and
                  Nicola De Cao and
                  James Thorne and
                  Yacine Jernite and
                  Vladimir Karpukhin and
                  Jean Maillard and
                  Vassilis Plachouras and
                  Tim Rockt{\"{a}}schel and
                  Sebastian Riedel},
  editor       = {Kristina Toutanova and
                  Anna Rumshisky and
                  Luke Zettlemoyer and
                  Dilek Hakkani{-}T{\"{u}}r and
                  Iz Beltagy and
                  Steven Bethard and
                  Ryan Cotterell and
                  Tanmoy Chakraborty and
                  Yichao Zhou},
  title        = {{KILT:} a Benchmark for Knowledge Intensive Language Tasks},
  booktitle    = {Proceedings of the 2021 Conference of the North American Chapter of
                  the Association for Computational Linguistics: Human Language Technologies,
                  {NAACL-HLT} 2021, Online, June 6-11, 2021},
  pages        = {2523--2544},
  publisher    = {Association for Computational Linguistics},
  year         = {2021},
  url          = {https://doi.org/10.18653/v1/2021.naacl-main.200},
  doi          = {10.18653/V1/2021.NAACL-MAIN.200},
  timestamp    = {Sun, 02 Oct 2022 16:12:49 +0200},
  biburl       = {https://dblp.org/rec/conf/naacl/PetroniPFLYCTJK21.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@misc{zhang2023h2oheavyhitteroracleefficient,
      title={H$_2$O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models}, 
      author={Zhenyu Zhang and Ying Sheng and Tianyi Zhou and Tianlong Chen and Lianmin Zheng and Ruisi Cai and Zhao Song and Yuandong Tian and Christopher Ré and Clark Barrett and Zhangyang Wang and Beidi Chen},
      year={2023},
      eprint={2306.14048},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2306.14048}, 
}

@article{dong2023survey,
  title={A survey on long text modeling with transformers},
  author={Dong, Zican and Tang, Tianyi and Li, Lunyi and Zhao, Wayne Xin},
  journal={arXiv preprint arXiv:2302.14502},
  year={2023}
}


@article{ji2023survey,
  title={Survey of hallucination in natural language generation},
  author={Ji, Ziwei and Lee, Nayeon and Frieske, Rita and Yu, Tiezheng and Su, Dan and Xu, Yan and Ishii, Etsuko and Bang, Ye Jin and Madotto, Andrea and Fung, Pascale},
  journal={ACM Computing Surveys},
  volume={55},
  number={12},
  pages={1--38},
  year={2023},
  publisher={ACM New York, NY}
}

@article{li2022faithfulness,
  title={Faithfulness in natural language generation: A systematic survey of analysis, evaluation and optimization methods},
  author={Li, Wei and Wu, Wenhao and Chen, Moye and Liu, Jiachen and Xiao, Xinyan and Wu, Hua},
  journal={arXiv preprint arXiv:2203.05227},
  year={2022}
}
@article{kaddour2023challenges,
  title={Challenges and applications of large language models},
  author={Kaddour, Jean and Harris, Joshua and Mozes, Maximilian and Bradley, Herbie and Raileanu, Roberta and McHardy, Robert},
  journal={arXiv preprint arXiv:2307.10169},
  year={2023}
}
@inproceedings{izacard2021distilling,
  author       = {Gautier Izacard and
                  Edouard Grave},
  title        = {Distilling Knowledge from Reader to Retriever for Question Answering},
  booktitle    = {9th International Conference on Learning Representations, {ICLR} 2021,
                  Virtual Event, Austria, May 3-7, 2021},
  publisher    = {OpenReview.net},
  year         = {2021},
  url          = {https://openreview.net/forum?id=NTEz-6wysdb},
  timestamp    = {Wed, 23 Jun 2021 17:36:39 +0200},
  biburl       = {https://dblp.org/rec/conf/iclr/IzacardG21.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}



@article{shi2023replug,
  title={Replug: Retrieval-augmented black-box language models}, 
  author={Weijia Shi and Sewon Min and Michihiro Yasunaga and Minjoon Seo and Rich James and Mike Lewis and Luke Zettlemoyer and Wen-tau Yih},
  year={2023},
  journal={arXiv preprint arXiv:2301.12652},
  url={https://arxiv.org/pdf/2301.12652}
}

@inproceedings{dinan2018wizard,
  title="{W}izard of {W}ikipedia: Knowledge-Powered Conversational Agents",
  author={Emily Dinan and Stephen Roller and Kurt Shuster and Angela Fan and Michael Auli and Jason Weston},
  booktitle=ICLR,
  year={2019},
  url={https://openreview.net/forum?id=r1l73iRqKm},
}

@inproceedings{fid,
  title={Distilling Knowledge from Reader to Retriever for Question Answering},
  author={Izacard, Gautier and Grave, Edouard},
  booktitle={International Conference on Learning Representations},
year={2021},
}

@inproceedings{guu2020realm,
  title="{REALM}: Retrieval-Augmented Language Model Pre-Training",
  author={Guu, Kelvin and Lee, Kenton and Tung, Zora and Pasupat, Panupong and Chang, Ming-Wei},
  year={2020},
  publisher={JMLR.org},
  booktitle=ICML,
  articleno={368},
  numpages={10},
  url={https://dl.acm.org/doi/abs/10.5555/3524938.3525306}
}

@inproceedings{wang2018r,
  title={R3: Reinforced ranker-reader for open-domain question answering},
  author={Wang, Shuohang and Yu, Mo and Guo, Xiaoxiao and Wang, Zhiguo and Klinger, Tim and Zhang, Wei and Chang, Shiyu and Tesauro, Gerry and Zhou, Bowen and Jiang, Jing},
  booktitle=AAAI,
  volume={32},
  year={2018},
  url={https://dl.acm.org/doi/10.5555/3504035.3504769}
}

@article{nogueira2020passage,
  title={Passage Re-ranking with BERT}, 
  author={Rodrigo Nogueira and Kyunghyun Cho},
  year={2020},
  journal={arXiv preprint arXiv:1901.04085},
  url={https://arxiv.org/pdf/1901.04085}
}

@article{shi2023large,
  title={Large Language Models Can Be Easily Distracted by Irrelevant Context}, 
  author={Freda Shi and Xinyun Chen and Kanishka Misra and Nathan Scales and David Dohan and Ed Chi and Nathanael Schärli and Denny Zhou},
  year={2023},
  journal={arXiv preprint arXiv:2302.00093},
  url={https://arxiv.org/pdf/2302.00093}
}
@article{kNN-LM,
  title={Generalization through memorization: Nearest neighbor language models},
  author={Khandelwal, Urvashi and Levy, Omer and Jurafsky, Dan and Zettlemoyer, Luke and Lewis, Mike},
  journal={arXiv preprint arXiv:1911.00172},
  year={2019}
}

@article{ease,
  title={EASE: Entity-aware contrastive learning of sentence embedding},
  author={Nishikawa, Sosuke and Ri, Ryokan and Yamada, Ikuya and Tsuruoka, Yoshimasa and Echizen, Isao},
  journal={arXiv preprint arXiv:2205.04260},
  year={2022}
}

@misc{li2022large,
      title={Large Language Models with Controllable Working Memory}, 
      author={Daliang Li and Ankit Singh Rawat and Manzil Zaheer and Xin Wang and Michal Lukasik and Andreas Veit and Felix Yu and Sanjiv Kumar},
      year={2022},
      eprint={2211.05110},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@article{sugre,
  title={Knowledge Graph-Augmented Language Models for Knowledge-Grounded Dialogue Generation},
  author={Kang, Minki and Kwak, Jin Myung and Baek, Jinheon and Hwang, Sung Ju},
  journal={arXiv preprint arXiv:2305.18846},
  year={2023}
}

@article{ICRALM,
  author       = {Ori Ram and
                  Yoav Levine and
                  Itay Dalmedigos and
                  Dor Muhlgay and
                  Amnon Shashua and
                  Kevin Leyton{-}Brown and
                  Yoav Shoham},
  title        = {In-Context Retrieval-Augmented Language Models},
  journal      = {Trans. Assoc. Comput. Linguistics},
  volume       = {11},
  pages        = {1316--1331},
  year         = {2023},
  url          = {https://doi.org/10.1162/tacl\_a\_00605},
  doi          = {10.1162/TACL\_A\_00605},
  timestamp    = {Wed, 19 Jun 2024 17:28:03 +0200},
  biburl       = {https://dblp.org/rec/journals/tacl/RamLDMSLS23.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@article{AutoGPT,
  title={Auto-GPT for Online Decision Making: Benchmarks and Additional Opinions},
  author={Yang, Hui and Yue, Sifu and He, Yunzhong},
  journal={arXiv preprint arXiv:2306.02224},
  year={2023}
}

@article{gpt-3,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}
@misc{gpt-4,
  title       = "GPT-4 Technical Report",
  author      = "OpenAI",
  howpublished = "\url{https://cdn.openai.com/papers/gpt-4.pdf}",
  year        = {2023},
}


@misc{gemini,
  title       = {Gemini: A Family of Highly Capable Multimodal Models},
  author      = {Google},
  howpublished = "\url{https://goo.gle/GeminiPaper}",
  year        = {2023},
}
@article{llama,
  title={Llama 2: Open foundation and fine-tuned chat models},
  author={Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
  journal={arXiv preprint arXiv:2307.09288},
  year={2023}
}


@online{stella,
  author = {dunzhang},
  title = {dunzhang/stella\_en\_1.5B\_v5},
  year = 2024,
  url = {https://huggingface.co/dunzhang/stella\_en\_1.5B\_v5},
  urldate = {2024-09-30}
}


@inproceedings{li2021guided,
  title={Guided generation of cause and effect},
  author={Li, Zhongyang and Ding, Xiao and Liu, Ting and Hu, J Edward and Van Durme, Benjamin},
  booktitle={Proceedings of the Twenty-Ninth International Conference on International Joint Conferences on Artificial Intelligence},
  pages={3629--3636},
  year={2021}
}
@article{Metzler_2021,
   title={Rethinking search: making domain experts out of dilettantes},
   volume={55},
   ISSN={0163-5840},
   url={http://dx.doi.org/10.1145/3476415.3476428},
   DOI={10.1145/3476415.3476428},
   number={1},
   journal={ACM SIGIR Forum},
   publisher={Association for Computing Machinery (ACM)},
   author={Metzler, Donald and Tay, Yi and Bahri, Dara and Najork, Marc},
   year={2021},
   month=jun, pages={1–27} }

@article{longctx, 
year = {2023}, 
title = {{Retrieval meets Long Context Large Language Models}}, 
author = {Xu, Peng and Ping, Wei and Wu, Xianchao and McAfee, Lawrence and Zhu, Chen and Liu, Zihan and Subramanian, Sandeep and Bakhturina, Evelina and Shoeybi, Mohammad and Catanzaro, Bryan}, 
journal = {arXiv}, 
doi = {10.48550/arxiv.2310.03025}, 
eprint = {2310.03025}, 
abstract = {{Extending the context window of large language models (LLMs) is getting popular recently, while the solution of augmenting LLMs with retrieval has existed for years. The natural questions are: i) Retrieval-augmentation versus long context window, which one is better for downstream tasks? ii) Can both methods be combined to get the best of both worlds? In this work, we answer these questions by studying both solutions using two state-of-the-art pretrained LLMs, i.e., a proprietary 43B GPT and LLaMA2-70B. Perhaps surprisingly, we find that LLM with 4K context window using simple retrieval-augmentation at generation can achieve comparable performance to finetuned LLM with 16K context window via positional interpolation on long context tasks, while taking much less computation. More importantly, we demonstrate that retrieval can significantly improve the performance of LLMs regardless of their extended context window sizes. Our best model, retrieval-augmented LLaMA2-70B with 32K context window, outperforms GPT-3.5-turbo-16k and Davinci003 in terms of average score on seven long context tasks including question answering and query-based summarization. It also outperforms its non-retrieval LLaMA2-70B-32k baseline by a margin, while being much faster at generation. Our study provides general insights on the choice of retrieval-augmentation versus long context extension of LLM for practitioners.}}, 
note = {Experimental}
}

@article{jiang2024minference,
    title={MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention},
    author={Jiang, Huiqiang and Li, Yucheng and Zhang, Chengruidong and Wu, Qianhui and Luo, Xufang and Ahn, Surin and Han, Zhenhua and Abdi, Amir H and Li, Dongsheng and Lin, Chin-Yew and Yang, Yuqing and Qiu, Lili},
    journal={arXiv preprint arXiv:2407.02490},
    year={2024}
}
@misc{sturua2024jinaembeddingsv3multilingualembeddingstask,
      title={jina-embeddings-v3: Multilingual Embeddings With Task LoRA}, 
      author={Saba Sturua and Isabelle Mohr and Mohammad Kalim Akram and Michael Günther and Bo Wang and Markus Krimmel and Feng Wang and Georgios Mastrapas and Andreas Koukounas and Nan Wang and Han Xiao},
      year={2024},
      eprint={2409.10173},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2409.10173}, 
}
@misc{jin2024llmmaybelonglmselfextend,
      title={LLM Maybe LongLM: Self-Extend LLM Context Window Without Tuning}, 
      author={Hongye Jin and Xiaotian Han and Jingfeng Yang and Zhimeng Jiang and Zirui Liu and Chia-Yuan Chang and Huiyuan Chen and Xia Hu},
      year={2024},
      eprint={2401.01325},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2401.01325}, 
}

@article{shinn2023reflexion,
  title={Reflexion: an autonomous agent with dynamic memory and self-reflection},
  author={Shinn, Noah and Labash, Beck and Gopinath, Ashwin},
  journal={arXiv preprint arXiv:2303.11366},
  year={2023}
}

@article{pcw, 
year = {2022}, 
title = {{Parallel Context Windows Improve In-Context Learning of Large Language Models}}, 
author = {Ratner, Nir and Levine, Yoav and Belinkov, Yonatan and Ram, Ori and Magar, Inbal and Abend, Omri and Karpas, Ehud and Shashua, Amnon and Leyton-Brown, Kevin and Shoham, Yoav}, 
journal = {arXiv}, 
doi = {10.48550/arxiv.2212.10947}, 
eprint = {2212.10947}, 
abstract = {{For applications that require processing large amounts of text at inference time, Large Language Models (LLMs) are handicapped by their limited context windows, which are typically 2048 tokens. In-context learning, an emergent phenomenon in LLMs in sizes above a certain parameter threshold, constitutes one significant example because it can only leverage training examples that fit into the context window. Existing efforts to address the context window limitation involve training specialized architectures, which tend to be smaller than the sizes in which in-context learning manifests due to the memory footprint of processing long texts. We present Parallel Context Windows (PCW), a method that alleviates the context window restriction for any off-the-shelf LLM without further training. The key to the approach is to carve a long context into chunks (``windows'') that fit within the architecture, restrict the attention mechanism to apply only within each window, and re-use the positional embeddings among the windows. We test the PCW approach on in-context learning with models that range in size between 750 million and 178 billion parameters, and show substantial improvements for tasks with diverse input and output spaces. Our results motivate further investigation of Parallel Context Windows as a method for applying off-the-shelf LLMs in other settings that require long text sequences.}}, 
note = {Window}
}
@article{dong2022survey,
  title={A survey on in-context learning},
  author={Dong, Qingxiu and Li, Lei and Dai, Damai and Zheng, Ce and Wu, Zhiyong and Chang, Baobao and Sun, Xu and Xu, Jingjing and Sui, Zhifang},
  journal={arXiv preprint arXiv:2301.00234},
  year={2022}
}
@article{driess2023palm,
  title={Palm-e: An embodied multimodal language model},
  author={Driess, Danny and Xia, Fei and Sajjadi, Mehdi SM and Lynch, Corey and Chowdhery, Aakanksha and Ichter, Brian and Wahid, Ayzaan and Tompson, Jonathan and Vuong, Quan and Yu, Tianhe and others},
  journal={arXiv preprint arXiv:2303.03378},
  year={2023}
}

@article{yang2023auto,
  title={Auto-gpt for online decision making: Benchmarks and additional opinions},
  author={Yang, Hui and Yue, Sifu and He, Yunzhong},
  journal={arXiv preprint arXiv:2306.02224},
  year={2023}
}
@article{kočiský2017narrativeqa,
  author       = {Tom{\'{a}}s Kocisk{\'{y}} and
                  Jonathan Schwarz and
                  Phil Blunsom and
                  Chris Dyer and
                  Karl Moritz Hermann and
                  G{\'{a}}bor Melis and
                  Edward Grefenstette},
  title        = {The NarrativeQA Reading Comprehension Challenge},
  journal      = {Trans. Assoc. Comput. Linguistics},
  volume       = {6},
  pages        = {317--328},
  year         = {2018},
  url          = {https://doi.org/10.1162/tacl\_a\_00023},
  doi          = {10.1162/TACL\_A\_00023},
  timestamp    = {Wed, 19 Jun 2024 17:28:03 +0200},
  biburl       = {https://dblp.org/rec/journals/tacl/KociskySBDHMG18.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@article{raffel2020exploring,
  title={Exploring the limits of transfer learning with a unified text-to-text transformer},
  author={Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J},
  journal={Journal of machine learning research},
  volume={21},
  number={140},
  pages={1--67},
  year={2020}
}

@misc{gpt35turbo,
      title={GPT-3.5 Turbo fine-tuning and API updates}, 
      author={OpenAI},
      year={2023},
      url={https://openai.com/blog/gpt-3-5-turbo-fine-tuning-and-\\
           api-updates}
}

@misc{openai2023gpt4,
      title={GPT-4 Technical Report}, 
      author={OpenAI},
      year={2023},
      eprint={2303.08774},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

% claude-2

@misc{claude2,
      title={Model card and evaluations for Claude models}, 
      author={Anthropic},
      year={2023},
      url={https://www-files.anthropic.com/production/images/Model-Card-Claude-2.pdf}
}

@inproceedings{izacard2021leveraging,
  title={Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering},
  author={Izacard, Gautier and Grave, {\'E}douard},
  booktitle={Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume},
  pages={874--880},
  year={2021}
}

@misc{OpenOrca,
  title = {OpenOrca: An Open Dataset of GPT Augmented FLAN Reasoning Traces},
  author = {Wing Lian and Bleys Goodson and Eugene Pentland and Austin Cook and Chanvichet Vong and "Teknium"},
  year = {2023},
  publisher = {HuggingFace},
  journal = {HuggingFace repository},
  howpublished = {\url{https://https://huggingface.co/Open-Orca/OpenOrca}},
}

@article{jiang2023mistral,
  title={Mistral 7B},
  author={Jiang, Albert Q and Sablayrolles, Alexandre and Mensch, Arthur and Bamford, Chris and Chaplot, Devendra Singh and Casas, Diego de las and Bressand, Florian and Lengyel, Gianna and Lample, Guillaume and Saulnier, Lucile and others},
  journal={arXiv preprint arXiv:2310.06825},
  year={2023}
}

@article{zhao2023pytorch,
  title={Pytorch fsdp: experiences on scaling fully sharded data parallel},
  author={Zhao, Yanli and Gu, Andrew and Varma, Rohan and Luo, Liang and Huang, Chien-Chin and Xu, Min and Wright, Less and Shojanazeri, Hamid and Ott, Myle and Shleifer, Sam and others},
  journal={arXiv preprint arXiv:2304.11277},
  year={2023}
}


@article{adolphs1999social,
  title={Social cognition and the human brain},
  author={Adolphs, Ralph},
  journal={Trends in cognitive sciences},
  volume={3},
  number={12},
  pages={469--479},
  year={1999},
  publisher={Elsevier}
}
@book{bryant2011computer,
  title={Computer systems: a programmer’s perspective},
  author={Bryant, Randal E and O’Hallaron, David Richard},
  year={2011},
  publisher={Prentice Hall}
}


# long context llm

# data view: shi2023context, bai2024longalign, lv2024longwanjuan, fu2024data

@inproceedings{du2022glm,
  title={GLM: General Language Model Pretraining with Autoregressive Blank Infilling},
  author={Du, Zhengxiao and Qian, Yujie and Liu, Xiao and Ding, Ming and Qiu, Jiezhong and Yang, Zhilin and Tang, Jie},
  booktitle={Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={320--335},
  year={2022}
}

@inproceedings{li2023long,
  title={How Long Can Context Length of Open-Source LLMs truly Promise?},
  author={Li, Dacheng and Shao, Rulin and Xie, Anze and Sheng, Ying and Zheng, Lianmin and Gonzalez, Joseph and Stoica, Ion and Ma, Xuezhe and Zhang, Hao},
  booktitle={NeurIPS 2023 Workshop on Instruction Tuning and Instruction Following},
  year={2023}
}

@inproceedings{shi2023context,
  title={In-Context Pretraining: Language Modeling Beyond Document Boundaries},
  author={Shi, Weijia and Min, Sewon and Lomeli, Maria and Zhou, Chunting and Li, Margaret and Lin, Xi Victoria and Smith, Noah A and Zettlemoyer, Luke and Yih, Wen-tau and Lewis, Mike},
  booktitle={The Twelfth International Conference on Learning Representations},
  year={2023}
}

@misc{mosaicml2023introducing,
  title={Introducing mpt-30b: Raising the bar for open-source foundation models},
  author={MosaicML NLP Team and others},
  year={2023},
  publisher={Accessed}
}

@misc{together2023,
  title={Together 32K},
  url={https://huggingface.co/togethercomputer/LLaMA-2-7B-32K},
  author={Together Team},
  year={2023}
}

@inproceedings{xiong2023effective,
  author       = {Wenhan Xiong and
                  Jingyu Liu and
                  Igor Molybog and
                  Hejia Zhang and
                  Prajjwal Bhargava and
                  Rui Hou and
                  Louis Martin and
                  Rashi Rungta and
                  Karthik Abinav Sankararaman and
                  Barlas Oguz and
                  Madian Khabsa and
                  Han Fang and
                  Yashar Mehdad and
                  Sharan Narang and
                  Kshitiz Malik and
                  Angela Fan and
                  Shruti Bhosale and
                  Sergey Edunov and
                  Mike Lewis and
                  Sinong Wang and
                  Hao Ma},
  editor       = {Kevin Duh and
                  Helena G{\'{o}}mez{-}Adorno and
                  Steven Bethard},
  title        = {Effective Long-Context Scaling of Foundation Models},
  booktitle    = {Proceedings of the 2024 Conference of the North American Chapter of
                  the Association for Computational Linguistics: Human Language Technologies
                  (Volume 1: Long Papers), {NAACL} 2024, Mexico City, Mexico, June 16-21,
                  2024},
  pages        = {4643--4663},
  publisher    = {Association for Computational Linguistics},
  year         = {2024},
  url          = {https://doi.org/10.18653/v1/2024.naacl-long.260},
  doi          = {10.18653/V1/2024.NAACL-LONG.260},
  timestamp    = {Thu, 29 Aug 2024 17:13:57 +0200},
  biburl       = {https://dblp.org/rec/conf/naacl/XiongLMZBHMRSOK24.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@inproceedings{peng2023rwkv,
  title={RWKV: Reinventing RNNs for the Transformer Era},
  author={Peng, Bo and Alcaide, Eric and Anthony, Quentin and Albalak, Alon and Arcadinho, Samuel and Biderman, Stella and Cao, Huanqi and Cheng, Xin and Chung, Michael and Derczynski, Leon and others},
  booktitle={Findings of the Association for Computational Linguistics: EMNLP 2023},
  pages={14048--14077},
  year={2023}
}

@misc{gu2023mamba,
      title={Mamba: Linear-Time Sequence Modeling with Selective State Spaces}, 
      author={Albert Gu and Tri Dao},
      year={2023},
      eprint={2312.00752},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{mohtashami2024random,
  title={Random-access infinite context length for transformers},
  author={Mohtashami, Amirkeivan and Jaggi, Martin},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}
@misc{zhu2024largelanguagemodelsinformation,
      title={Large Language Models for Information Retrieval: A Survey}, 
      author={Yutao Zhu and Huaying Yuan and Shuting Wang and Jiongnan Liu and Wenhan Liu and Chenlong Deng and Haonan Chen and Zhicheng Dou and Ji-Rong Wen},
      year={2024},
      eprint={2308.07107},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2308.07107}, 
}

@misc{fu2024data,
      title={Data Engineering for Scaling Language Models to 128K Context}, 
      author={Yao Fu and Rameswar Panda and Xinyao Niu and Xiang Yue and Hannaneh Hajishirzi and Yoon Kim and Hao Peng},
      year={2024},
      eprint={2402.10171},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}



@article{cai2024internlm2,
  author       = {Zheng Cai and
                  Maosong Cao and
                  Haojiong Chen and
                  Kai Chen and
                  Keyu Chen and
                  Xin Chen and
                  Xun Chen and
                  Zehui Chen and
                  Zhi Chen and
                  Pei Chu and
                  Xiaoyi Dong and
                  Haodong Duan and
                  Qi Fan and
                  Zhaoye Fei and
                  Yang Gao and
                  Jiaye Ge and
                  Chenya Gu and
                  Yuzhe Gu and
                  Tao Gui and
                  Aijia Guo and
                  Qipeng Guo and
                  Conghui He and
                  Yingfan Hu and
                  Ting Huang and
                  Tao Jiang and
                  Penglong Jiao and
                  Zhenjiang Jin and
                  Zhikai Lei and
                  Jiaxing Li and
                  Jingwen Li and
                  Linyang Li and
                  Shuaibin Li and
                  Wei Li and
                  Yining Li and
                  Hongwei Liu and
                  Jiangning Liu and
                  Jiawei Hong and
                  Kaiwen Liu and
                  Kuikun Liu and
                  Xiaoran Liu and
                  Chengqi Lv and
                  Haijun Lv and
                  Kai Lv and
                  Li Ma and
                  Runyuan Ma and
                  Zerun Ma and
                  Wenchang Ning and
                  Linke Ouyang and
                  Jiantao Qiu and
                  Yuan Qu and
                  Fukai Shang and
                  Yunfan Shao and
                  Demin Song and
                  Zifan Song and
                  Zhihao Sui and
                  Peng Sun and
                  Yu Sun and
                  Huanze Tang and
                  Bin Wang and
                  Guoteng Wang and
                  Jiaqi Wang and
                  Jiayu Wang and
                  Rui Wang and
                  Yudong Wang and
                  Ziyi Wang and
                  Xingjian Wei and
                  Qizhen Weng and
                  Fan Wu and
                  Yingtong Xiong and
                  et al.},
  title        = {InternLM2 Technical Report},
  journal      = {CoRR},
  volume       = {abs/2403.17297},
  year         = {2024},
  url          = {https://doi.org/10.48550/arXiv.2403.17297},
  doi          = {10.48550/ARXIV.2403.17297},
  eprinttype    = {arXiv},
  eprint       = {2403.17297},
  timestamp    = {Tue, 08 Oct 2024 07:48:00 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2403-17297.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{bai2024longalign,
  title={Longalign: A recipe for long context alignment of large language models},
  author={Bai, Yushi and Lv, Xin and Zhang, Jiajie and He, Yuze and Qi, Ji and Hou, Lei and Tang, Jie and Dong, Yuxiao and Li, Juanzi},
  journal={arXiv preprint arXiv:2401.18058},
  year={2024}
}

@article{lv2024longwanjuan,
  title={LongWanjuan: Towards Systematic Measurement for Long Text Quality},
  author={Lv, Kai and Liu, Xiaoran and Guo, Qipeng and Yan, Hang and He, Conghui and Qiu, Xipeng and Lin, Dahua},
  journal={arXiv preprint arXiv:2402.13583},
  year={2024}
}

@misc{ai2024yi,
      title={Yi: Open Foundation Models by 01.AI}, 
      author={01. AI and : and Alex Young and Bei Chen and Chao Li and Chengen Huang and Ge Zhang and Guanwei Zhang and Heng Li and Jiangcheng Zhu and Jianqun Chen and Jing Chang and Kaidong Yu and Peng Liu and Qiang Liu and Shawn Yue and Senbin Yang and Shiming Yang and Tao Yu and Wen Xie and Wenhao Huang and Xiaohui Hu and Xiaoyi Ren and Xinyao Niu and Pengcheng Nie and Yuchi Xu and Yudong Liu and Yue Wang and Yuxuan Cai and Zhenyu Gu and Zhiyuan Liu and Zonghong Dai},
      year={2024},
      eprint={2403.04652},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}



# position traing-free

@article{chen2023extending,
  title={Extending context window of large language models via positional interpolation},
  author={Chen, Shouyuan and Wong, Sherman and Chen, Liangjian and Tian, Yuandong},
  journal={arXiv preprint arXiv:2306.15595},
  year={2023}
}

@inproceedings{song2023hierarchical,
  title={Hierarchical Context Merging: Better Long Context Understanding for Pre-trained LLMs},
  author={Song, Woomin and Oh, Seunghyuk and Mo, Sangwoo and Kim, Jaehyung and Yun, Sukmin and Ha, Jung-Woo and Shin, Jinwoo},
  booktitle={The Twelfth International Conference on Learning Representations},
  year={2023}
}

@inproceedings{liu2023scaling,
  title={Scaling Laws of RoPE-based Extrapolation},
  author={Liu, Xiaoran and Yan, Hang and An, Chenxin and Qiu, Xipeng and Lin, Dahua},
  booktitle={The Twelfth International Conference on Learning Representations},
  year={2023}
}



# efficient traing to extend

@inproceedings{peng2023yarn,
  title={YaRN: Efficient Context Window Extension of Large Language Models},
  author={Peng, Bowen and Quesnelle, Jeffrey and Fan, Honglu and Shippole, Enrico},
  booktitle={The Twelfth International Conference on Learning Representations},
  year={2023}
}

@inproceedings{chen2023longlora,
  title={LongLoRA: Efficient Fine-tuning of Long-Context Large Language Models},
  author={Chen, Yukang and Qian, Shengju and Tang, Haotian and Lai, Xin and Liu, Zhijian and Han, Song and Jia, Jiaya},
  booktitle={The Twelfth International Conference on Learning Representations},
  year={2023}
}

@article{ding2024longrope,
  title={LongRoPE: Extending LLM Context Window Beyond 2 Million Tokens},
  author={Ding, Yiran and Zhang, Li Lyna and Zhang, Chengruidong and Xu, Yuanyuan and Shang, Ning and Xu, Jiahang and Yang, Fan and Yang, Mao},
  journal={arXiv preprint arXiv:2402.13753},
  year={2024}
}

@article{tworkowski2024focused,
  title={Focused transformer: Contrastive training for context scaling},
  author={Tworkowski, Szymon and Staniszewski, Konrad and Pacek, Miko{\l}aj and Wu, Yuhuai and Michalewski, Henryk and Mi{\l}o{\'s}, Piotr},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}




# long-context QA

# NarrativeQA
@article{kovcisky2018narrativeqa,
  title={The narrativeqa reading comprehension challenge},
  author={Ko{\v{c}}isk{\`y}, Tom{\'a}{\v{s}} and Schwarz, Jonathan and Blunsom, Phil and Dyer, Chris and Hermann, Karl Moritz and Melis, G{\'a}bor and Grefenstette, Edward},
  journal={Transactions of the Association for Computational Linguistics},
  volume={6},
  pages={317--328},
  year={2018},
  publisher={MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~…}
}

# Qasper
@inproceedings{dasigi2021dataset,
  title={A Dataset of Information-Seeking Questions and Answers Anchored in Research Papers},
  author={Dasigi, Pradeep and Lo, Kyle and Beltagy, Iz and Cohan, Arman and Smith, Noah A and Gardner, Matt},
  booktitle={Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
  pages={4599--4610},
  year={2021}
}

# LongBench


@inproceedings{bai2023longbench,
  author       = {Yushi Bai and
                  Xin Lv and
                  Jiajie Zhang and
                  Hongchang Lyu and
                  Jiankai Tang and
                  Zhidian Huang and
                  Zhengxiao Du and
                  Xiao Liu and
                  Aohan Zeng and
                  Lei Hou and
                  Yuxiao Dong and
                  Jie Tang and
                  Juanzi Li},
  editor       = {Lun{-}Wei Ku and
                  Andre Martins and
                  Vivek Srikumar},
  title        = {LongBench: {A} Bilingual, Multitask Benchmark for Long Context Understanding},
  booktitle    = {Proceedings of the 62nd Annual Meeting of the Association for Computational
                  Linguistics (Volume 1: Long Papers), {ACL} 2024, Bangkok, Thailand,
                  August 11-16, 2024},
  pages        = {3119--3137},
  publisher    = {Association for Computational Linguistics},
  year         = {2024},
  url          = {https://doi.org/10.18653/v1/2024.acl-long.172},
  doi          = {10.18653/V1/2024.ACL-LONG.172},
  timestamp    = {Tue, 24 Sep 2024 10:55:50 +0200},
  biburl       = {https://dblp.org/rec/conf/acl/BaiLZL0HDLZHDTL24.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

# multi-hop


@inproceedings{ho2020constructing,
  title={Constructing A Multi-hop QA Dataset for Comprehensive Evaluation of Reasoning Steps},
  author={Ho, Xanh and Nguyen, Anh-Khoa Duong and Sugawara, Saku and Aizawa, Akiko},
  booktitle={Proceedings of the 28th International Conference on Computational Linguistics},
  pages={6609--6625},
  year={2020}
}

@article{trivedi2022musique,
  title={MuSiQue: Multihop Questions via Single-hop Question Composition},
  author={Trivedi, Harsh and Balasubramanian, Niranjan and Khot, Tushar and Sabharwal, Ashish},
  journal={Transactions of the Association for Computational Linguistics},
  volume={10},
  pages={539--554},
  year={2022},
  publisher={MIT Press One Broadway, 12th Floor, Cambridge, Massachusetts 02142, USA~…}
}



# long-context summarization

# GovReport
@inproceedings{huang2021efficient,
  title={Efficient Attentions for Long Document Summarization},
  author={Huang, Luyang and Cao, Shuyang and Parulian, Nikolaus and Ji, Heng and Wang, Lu},
  booktitle={2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2021},
  pages={1419--1436},
  year={2021},
  organization={Association for Computational Linguistics (ACL)}
}

# QMSum
@inproceedings{zhong2021qmsum,
  title={QMSum: A New Benchmark for Query-based Multi-domain Meeting Summarization},
  author={Zhong, Ming and Yin, Da and Yu, Tao and Zaidi, Ahmad and Mutuma, Mutethia and Jha, Rahul and Hassan, Ahmed and Celikyilmaz, Asli and Liu, Yang and Qiu, Xipeng and others},
  booktitle={Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
  pages={5905--5921},
  year={2021}
}

# Multi-News
@inproceedings{fabbri2019multi,
  title={Multi-News: A Large-Scale Multi-Document Summarization Dataset and Abstractive Hierarchical Model},
  author={Fabbri, Alexander and Li, Irene and She, Tianwei and Li, Suyi and Radev, Dragomir},
  booktitle={Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
  pages={1074},
  year={2019},
  organization={Association for Computational Linguistics}
}

@article{johnson2019billion,
  title={Billion-scale similarity search with {GPUs}},
  author={Johnson, Jeff and Douze, Matthijs and J{\'e}gou, Herv{\'e}},
  journal={IEEE Transactions on Big Data},
  volume={7},
  number={3},
  pages={535--547},
  year={2019},
  publisher={IEEE}
}


# lost-in-the-middle
@article{liu2024lost,
  title={Lost in the middle: How language models use long contexts},
  author={Liu, Nelson F and Lin, Kevin and Hewitt, John and Paranjape, Ashwin and Bevilacqua, Michele and Petroni, Fabio and Liang, Percy},
  journal={Transactions of the Association for Computational Linguistics},
  volume={12},
  pages={157--173},
  year={2024},
  publisher={MIT Press One Broadway, 12th Floor, Cambridge, Massachusetts 02142, USA~…}
}


@inproceedings{xu2023retrieval,
  author       = {Peng Xu and
                  Wei Ping and
                  Xianchao Wu and
                  Lawrence McAfee and
                  Chen Zhu and
                  Zihan Liu and
                  Sandeep Subramanian and
                  Evelina Bakhturina and
                  Mohammad Shoeybi and
                  Bryan Catanzaro},
  title        = {Retrieval meets Long Context Large Language Models},
  booktitle    = {The Twelfth International Conference on Learning Representations,
                  {ICLR} 2024, Vienna, Austria, May 7-11, 2024},
  publisher    = {OpenReview.net},
  year         = {2024},
  url          = {https://openreview.net/forum?id=xw5nxFWMlo},
  timestamp    = {Thu, 08 Aug 2024 08:11:03 +0200},
  biburl       = {https://dblp.org/rec/conf/iclr/0008PWM0LSBSC24.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

# long context loss

@inproceedings{sharan2018prediction,
  title={Prediction with a short memory},
  author={Sharan, Vatsal and Kakade, Sham and Liang, Percy and Valiant, Gregory},
  booktitle={Proceedings of the 50th Annual ACM SIGACT Symposium on Theory of Computing},
  pages={1074--1087},
  year={2018}
}

@inproceedings{sun2021long,
  title={Do Long-Range Language Models Actually Use Long-Range Context?},
  author={Sun, Simeng and Krishna, Kalpesh and Mattarella-Micke, Andrew and Iyyer, Mohit},
  booktitle={Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing},
  pages={807--822},
  year={2021}
}



# llama2
@article{touvron2023llama,
  title={Llama 2: Open foundation and fine-tuned chat models},
  author={Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
  journal={arXiv preprint arXiv:2307.09288},
  year={2023}
}



# short tasks

# MMLU
@inproceedings{hendrycks2020measuring,
  title={Measuring Massive Multitask Language Understanding},
  author={Hendrycks, Dan and Burns, Collin and Basart, Steven and Zou, Andy and Mazeika, Mantas and Song, Dawn and Steinhardt, Jacob},
  booktitle={International Conference on Learning Representations},
  year={2020}
}

# BoolQ
@inproceedings{clark2019boolq,
  title={BoolQ: Exploring the Surprising Difficulty of Natural Yes/No Questions},
  author={Clark, Christopher and Lee, Kenton and Chang, Ming-Wei and Kwiatkowski, Tom and Collins, Michael and Toutanova, Kristina},
  booktitle={Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)},
  pages={2924--2936},
  year={2019}
}

# RACE
@inproceedings{lai2017race,
  title={RACE: Large-scale ReAding Comprehension Dataset From Examinations},
  author={Lai, Guokun and Xie, Qizhe and Liu, Hanxiao and Yang, Yiming and Hovy, Eduard},
  booktitle={Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing},
  pages={785--794},
  year={2017}
}

# CSQA
@inproceedings{commonsenseqa2019,
    title = "{C}ommonsense{QA}: A Question Answering Challenge Targeting Commonsense Knowledge",
    author = "Talmor, Alon  and
      Herzig, Jonathan  and
      Lourie, Nicholas  and
      Berant, Jonathan",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N19-1421",
    doi = "10.18653/v1/N19-1421",
    pages = "4149--4158",
    abstract = "When answering a question, people often draw upon their rich world knowledge in addition to the particular context. Recent work has focused primarily on answering questions given some relevant document or context, and required very little general background. To investigate question answering with prior knowledge, we present CommonsenseQA: a challenging new dataset for commonsense question answering. To capture common sense beyond associations, we extract from ConceptNet (Speer et al., 2017) multiple target concepts that have the same semantic relation to a single source concept. Crowd-workers are asked to author multiple-choice questions that mention the source concept and discriminate in turn between each of the target concepts. This encourages workers to create questions with complex semantics that often require prior knowledge. We create 12,247 questions through this procedure and demonstrate the difficulty of our task with a large number of strong baselines. Our best baseline is based on BERT-large (Devlin et al., 2018) and obtains 56{\%} accuracy, well below human performance, which is 89{\%}.",
}

# ARC
@article{clark2018think,
  title={Think you have solved question answering? try arc, the ai2 reasoning challenge},
  author={Clark, Peter and Cowhey, Isaac and Etzioni, Oren and Khot, Tushar and Sabharwal, Ashish and Schoenick, Carissa and Tafjord, Oyvind},
  journal={arXiv preprint arXiv:1803.05457},
  year={2018}
}

# HellaSwag
@inproceedings{zellers2019hellaswag,
  title={HellaSwag: Can a Machine Really Finish Your Sentence?},
  author={Zellers, Rowan and Holtzman, Ari and Bisk, Yonatan and Farhadi, Ali and Choi, Yejin},
  booktitle={Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
  pages={4791--4800},
  year={2019}
}

% GSM8K
@article{cobbe2021training,
  title={Training verifiers to solve math word problems},
  author={Cobbe, Karl and Kosaraju, Vineet and Bavarian, Mohammad and Chen, Mark and Jun, Heewoo and Kaiser, Lukasz and Plappert, Matthias and Tworek, Jerry and Hilton, Jacob and Nakano, Reiichiro and others},
  journal={arXiv preprint arXiv:2110.14168},
  year={2021}
}

% MATH
@inproceedings{hendrycks2021measuring,
  title={Measuring Mathematical Problem Solving With the MATH Dataset},
  author={Hendrycks, Dan and Burns, Collin and Kadavath, Saurav and Arora, Akul and Basart, Steven and Tang, Eric and Song, Dawn and Steinhardt, Jacob},
  booktitle={Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2)},
  year={2021}
}



# ZeroSCROLLS
@inproceedings{shaham2023zeroscrolls,
  title={ZeroSCROLLS: A Zero-Shot Benchmark for Long Text Understanding},
  author={Shaham, Uri and Ivgi, Maor and Efrat, Avia and Berant, Jonathan and Levy, Omer},
  booktitle={Findings of the Association for Computational Linguistics: EMNLP 2023},
  pages={7977--7989},
  year={2023}
}

# L-eval
@article{an2023eval,
  title={L-eval: Instituting standardized evaluation for long context language models},
  author={An, Chenxin and Gong, Shansan and Zhong, Ming and Li, Mukai and Zhang, Jun and Kong, Lingpeng and Qiu, Xipeng},
  journal={arXiv preprint arXiv:2307.11088},
  year={2023}
}

# Loogle
@article{li2023loogle,
  title={LooGLE: Can Long-Context Language Models Understand Long Contexts?},
  author={Li, Jiaqi and Wang, Mengmeng and Zheng, Zilong and Zhang, Muhan},
  journal={arXiv preprint arXiv:2311.04939},
  year={2023}
}
@inproceedings{zhang2024inftybench,
  author       = {Xinrong Zhang and
                  Yingfa Chen and
                  Shengding Hu and
                  Zihang Xu and
                  Junhao Chen and
                  Moo Khai Hao and
                  Xu Han and
                  Zhen Leng Thai and
                  Shuo Wang and
                  Zhiyuan Liu and
                  Maosong Sun},
  editor       = {Lun{-}Wei Ku and
                  Andre Martins and
                  Vivek Srikumar},
  title        = {{\i}nftyBench: Extending Long Context Evaluation Beyond 100K Tokens},
  booktitle    = {Proceedings of the 62nd Annual Meeting of the Association for Computational
                  Linguistics (Volume 1: Long Papers), {ACL} 2024, Bangkok, Thailand,
                  August 11-16, 2024},
  pages        = {15262--15277},
  publisher    = {Association for Computational Linguistics},
  year         = {2024},
  url          = {https://doi.org/10.18653/v1/2024.acl-long.814},
  doi          = {10.18653/V1/2024.ACL-LONG.814},
  timestamp    = {Tue, 24 Sep 2024 10:55:41 +0200},
  biburl       = {https://dblp.org/rec/conf/acl/ZhangCHXCH0TW0024.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


# ppl
@article{beltagy2020longformer,
  title={Longformer: The long-document transformer},
  author={Beltagy, Iz and Peters, Matthew E and Cohan, Arman},
  journal={arXiv preprint arXiv:2004.05150},
  year={2020}
}

@article{roy2021efficient,
  title={Efficient content-based sparse attention with routing transformers},
  author={Roy, Aurko and Saffar, Mohammad and Vaswani, Ashish and Grangier, David},
  journal={Transactions of the Association for Computational Linguistics},
  volume={9},
  pages={53--68},
  year={2021},
  publisher={MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~…}
}

@inproceedings{press2021train,
  title={Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation},
  author={Press, Ofir and Smith, Noah and Lewis, Mike},
  booktitle={International Conference on Learning Representations},
  year={2021}
}


# S3Eval
@misc{lei2024s3eval,
      title={S3Eval: A Synthetic, Scalable, Systematic Evaluation Suite for Large Language Models}, 
      author={Fangyu Lei and Qian Liu and Yiming Huang and Shizhu He and Jun Zhao and Kang Liu},
      year={2024},
      eprint={2310.15147},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

# Discovery
@misc{li2024longcontext,
      title={Long-context LLMs Struggle with Long In-context Learning}, 
      author={Tianle Li and Ge Zhang and Quy Duc Do and Xiang Yue and Wenhu Chen},
      year={2024},
      eprint={2404.02060},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

# Ruler
@misc{hsieh2024ruler,
      title={RULER: What's the Real Context Size of Your Long-Context Language Models?}, 
      author={Cheng-Ping Hsieh and Simeng Sun and Samuel Kriman and Shantanu Acharya and Dima Rekesh and Fei Jia and Boris Ginsburg},
      year={2024},
      eprint={2404.06654},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}





# induction head

@misc{olsson2022incontext,
      title={In-context Learning and Induction Heads}, 
      author={Catherine Olsson and Nelson Elhage and Neel Nanda and Nicholas Joseph and Nova DasSarma and Tom Henighan and Ben Mann and Amanda Askell and Yuntao Bai and Anna Chen and Tom Conerly and Dawn Drain and Deep Ganguli and Zac Hatfield-Dodds and Danny Hernandez and Scott Johnston and Andy Jones and Jackson Kernion and Liane Lovitt and Kamal Ndousse and Dario Amodei and Tom Brown and Jack Clark and Jared Kaplan and Sam McCandlish and Chris Olah},
      year={2022},
      eprint={2209.11895},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}





# starcoder

@article{li2023starcoder,
  title={StarCoder: may the source be with you!},
  author={Li, Raymond and Zi, Yangtian and Muennighoff, Niklas and Kocetkov, Denis and Mou, Chenghao and Marone, Marc and Akiki, Christopher and Jia, LI and Chim, Jenny and Liu, Qian and others},
  journal={Transactions on Machine Learning Research},
  year={2023}
}




@misc{an2024make,
      title={Make Your LLM Fully Utilize the Context}, 
      author={Shengnan An and Zexiong Ma and Zeqi Lin and Nanning Zheng and Jian-Guang Lou},
      year={2024},
      eprint={2404.16811},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}





@misc{an2024learning,
      title={Learning From Mistakes Makes LLM Better Reasoner}, 
      author={Shengnan An and Zexiong Ma and Zeqi Lin and Nanning Zheng and Jian-Guang Lou and Weizhu Chen},
      year={2024},
      eprint={2310.20689},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{lin2004rouge,
  title={Rouge: A package for automatic evaluation of summaries},
  author={Lin, Chin-Yew},
  booktitle={Text summarization branches out},
  pages={74--81},
  year={2004}
}



@article{roziere2023code,
  title={Code llama: Open foundation models for code},
  author={Roziere, Baptiste and Gehring, Jonas and Gloeckle, Fabian and Sootla, Sten and Gat, Itai and Tan, Xiaoqing Ellen and Adi, Yossi and Liu, Jingyu and Remez, Tal and Rapin, J{\'e}r{\'e}my and others},
  journal={arXiv preprint arXiv:2308.12950},
  year={2023}
}

@inproceedings{yang2018hotpotqa,
  author       = {Zhilin Yang and
                  Peng Qi and
                  Saizheng Zhang and
                  Yoshua Bengio and
                  William W. Cohen and
                  Ruslan Salakhutdinov and
                  Christopher D. Manning},
  editor       = {Ellen Riloff and
                  David Chiang and
                  Julia Hockenmaier and
                  Jun'ichi Tsujii},
  title        = {HotpotQA: {A} Dataset for Diverse, Explainable Multi-hop Question
                  Answering},
  booktitle    = {Proceedings of the 2018 Conference on Empirical Methods in Natural
                  Language Processing, Brussels, Belgium, October 31 - November 4, 2018},
  pages        = {2369--2380},
  publisher    = {Association for Computational Linguistics},
  year         = {2018},
  url          = {https://doi.org/10.18653/v1/d18-1259},
  doi          = {10.18653/V1/D18-1259},
  timestamp    = {Fri, 06 Aug 2021 00:40:21 +0200},
  biburl       = {https://dblp.org/rec/conf/emnlp/Yang0ZBCSM18.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@misc{zhang2024extending,
      title={Extending Llama-3's Context Ten-Fold Overnight}, 
      author={Peitian Zhang and Ninglu Shao and Zheng Liu and Shitao Xiao and Hongjin Qian and Qiwei Ye and Zhicheng Dou},
      year={2024},
      eprint={2404.19553},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{deepseekai2024deepseekv2,
      title={DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model}, 
      author={DeepSeek-AI},
      year={2024},
      eprint={2405.04434},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{vicuna2023,
    title = {Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90\%* ChatGPT Quality},
    url = {https://lmsys.org/blog/2023-03-30-vicuna/},
    author = {Chiang, Wei-Lin and Li, Zhuohan and Lin, Zi and Sheng, Ying and Wu, Zhanghao and Zhang, Hao and Zheng, Lianmin and Zhuang, Siyuan and Zhuang, Yonghao and Gonzalez, Joseph E. and Stoica, Ion and Xing, Eric P.},
    month = {March},
    year = {2023}
}
@misc{guo2023longcoder,
      title={LongCoder: A Long-Range Pre-trained Language Model for Code Completion}, 
      author={Daya Guo and Canwen Xu and Nan Duan and Jian Yin and Julian McAuley},
      year={2023},
      eprint={2306.14893},
      archivePrefix={arXiv},
      primaryClass={cs.SE}
}


@inproceedings{gliwa-etal-2019-samsum,
    title = "{SAMS}um Corpus: A Human-annotated Dialogue Dataset for Abstractive Summarization",
    author = "Gliwa, Bogdan  and
      Mochol, Iwona  and
      Biesek, Maciej  and
      Wawer, Aleksander",
    editor = "Wang, Lu  and
      Cheung, Jackie Chi Kit  and
      Carenini, Giuseppe  and
      Liu, Fei",
    booktitle = "Proceedings of the 2nd Workshop on New Frontiers in Summarization",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-5409",
    doi = "10.18653/v1/D19-5409",
    pages = "70--79",
    abstract = "This paper introduces the SAMSum Corpus, a new dataset with abstractive dialogue summaries. We investigate the challenges it poses for automated summarization by testing several models and comparing their results with those obtained on a corpus of news articles. We show that model-generated summaries of dialogues achieve higher ROUGE scores than the model-generated summaries of news {--} in contrast with human evaluators{'} judgement. This suggests that a challenging task of abstractive dialogue summarization requires dedicated models and non-standard quality measures. To our knowledge, our study is the first attempt to introduce a high-quality chat-dialogues corpus, manually annotated with abstractive summarizations, which can be used by the research community for further studies.",
}

@misc{longchat2023,
    title = {How Long Can Open-Source LLMs Truly Promise on Context Length?},
    url = {https://lmsys.org/blog/2023-06-29-longchat},
    author = {Dacheng Li and Rulin Shao and Anze Xie and Ying Sheng and Lianmin Zheng and Joseph E. Gonzalez and Ion Stoica and Xuezhe Ma and Hao Zhang},
    month = {June},
    year = {2023}
}

@inproceedings{fabbri2019multinews,
  author       = {Alexander R. Fabbri and
                  Irene Li and
                  Tianwei She and
                  Suyi Li and
                  Dragomir R. Radev},
  editor       = {Anna Korhonen and
                  David R. Traum and
                  Llu{\'{\i}}s M{\`{a}}rquez},
  title        = {Multi-News: {A} Large-Scale Multi-Document Summarization Dataset and
                  Abstractive Hierarchical Model},
  booktitle    = {Proceedings of the 57th Conference of the Association for Computational
                  Linguistics, {ACL} 2019, Florence, Italy, July 28- August 2, 2019,
                  Volume 1: Long Papers},
  pages        = {1074--1084},
  publisher    = {Association for Computational Linguistics},
  year         = {2019},
  url          = {https://doi.org/10.18653/v1/p19-1102},
  doi          = {10.18653/V1/P19-1102},
  timestamp    = {Wed, 31 Jul 2024 07:40:34 +0200},
  biburl       = {https://dblp.org/rec/conf/acl/FabbriLSLR19.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{huang-etal-2021-efficient,
    title = "Efficient Attentions for Long Document Summarization",
    author = "Huang, Luyang  and
      Cao, Shuyang  and
      Parulian, Nikolaus  and
      Ji, Heng  and
      Wang, Lu",
    editor = "Toutanova, Kristina  and
      Rumshisky, Anna  and
      Zettlemoyer, Luke  and
      Hakkani-Tur, Dilek  and
      Beltagy, Iz  and
      Bethard, Steven  and
      Cotterell, Ryan  and
      Chakraborty, Tanmoy  and
      Zhou, Yichao",
    booktitle = "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.naacl-main.112",
    doi = "10.18653/v1/2021.naacl-main.112",
    pages = "1419--1436",
    abstract = "The quadratic computational and memory complexities of large Transformers have limited their scalability for long document summarization. In this paper, we propose Hepos, a novel efficient encoder-decoder attention with head-wise positional strides to effectively pinpoint salient information from the source. We further conduct a systematic study of existing efficient self-attentions. Combined with Hepos, we are able to process ten times more tokens than existing models that use full attentions. For evaluation, we present a new dataset, GovReport, with significantly longer documents and summaries. Results show that our models produce significantly higher ROUGE scores than competitive comparisons, including new state-of-the-art results on PubMed. Human evaluation also shows that our models generate more informative summaries with fewer unfaithful errors.",
}
@inproceedings{ho-etal-2020-constructing,
    title = "Constructing A Multi-hop {QA} Dataset for Comprehensive Evaluation of Reasoning Steps",
    author = "Ho, Xanh  and
      Duong Nguyen, Anh-Khoa  and
      Sugawara, Saku  and
      Aizawa, Akiko",
    editor = "Scott, Donia  and
      Bel, Nuria  and
      Zong, Chengqing",
    booktitle = "Proceedings of the 28th International Conference on Computational Linguistics",
    month = dec,
    year = "2020",
    address = "Barcelona, Spain (Online)",
    publisher = "International Committee on Computational Linguistics",
    url = "https://aclanthology.org/2020.coling-main.580",
    doi = "10.18653/v1/2020.coling-main.580",
    pages = "6609--6625",
    abstract = "A multi-hop question answering (QA) dataset aims to test reasoning and inference skills by requiring a model to read multiple paragraphs to answer a given question. However, current datasets do not provide a complete explanation for the reasoning process from the question to the answer. Further, previous studies revealed that many examples in existing multi-hop datasets do not require multi-hop reasoning to answer a question. In this study, we present a new multi-hop QA dataset, called 2WikiMultiHopQA, which uses structured and unstructured data. In our dataset, we introduce the evidence information containing a reasoning path for multi-hop questions. The evidence information has two benefits: (i) providing a comprehensive explanation for predictions and (ii) evaluating the reasoning skills of a model. We carefully design a pipeline and a set of templates when generating a question-answer pair that guarantees the multi-hop steps and the quality of the questions. We also exploit the structured format in Wikidata and use logical rules to create questions that are natural but still require multi-hop reasoning. Through experiments, we demonstrate that our dataset is challenging for multi-hop models and it ensures that multi-hop reasoning is required.",
}
@article{zhang2024soaring,
  title={Soaring from 4K to 400K: Extending LLM's Context with Activation Beacon},
  author={Zhang, Peitian and Liu, Zheng and Xiao, Shitao and Shao, Ninglu and Ye, Qiwei and Dou, Zhicheng},
  journal={arXiv preprint arXiv:2401.03462},
  year={2024}
}
@misc{zheng2023judging,
      title={Judging LLM-as-a-judge with MT-Bench and Chatbot Arena},
      author={Lianmin Zheng and Wei-Lin Chiang and Ying Sheng and Siyuan Zhuang and Zhanghao Wu and Yonghao Zhuang and Zi Lin and Zhuohan Li and Dacheng Li and Eric. P Xing and Hao Zhang and Joseph E. Gonzalez and Ion Stoica},
      year={2023},
      eprint={2306.05685},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{bge_embedding,
      title={C-Pack: Packaged Resources To Advance General Chinese Embedding}, 
      author={Shitao Xiao and Zheng Liu and Peitian Zhang and Niklas Muennighoff},
      year={2023},
      eprint={2309.07597},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{llm_embedder,
      title={Retrieve Anything To Augment Large Language Models}, 
      author={Peitian Zhang and Shitao Xiao and Zheng Liu and Zhicheng Dou and Jian-Yun Nie},
      year={2023},
      eprint={2310.07554},
      archivePrefix={arXiv},
      primaryClass={cs.IR}
}

@misc{kaplan2020scaling,
      title={Scaling Laws for Neural Language Models}, 
      author={Jared Kaplan and Sam McCandlish and Tom Henighan and Tom B. Brown and Benjamin Chess and Rewon Child and Scott Gray and Alec Radford and Jeffrey Wu and Dario Amodei},
      year={2020},
      eprint={2001.08361},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}



@inproceedings{bajaj2016ms,
  author       = {Tri Nguyen and
                  Mir Rosenberg and
                  Xia Song and
                  Jianfeng Gao and
                  Saurabh Tiwary and
                  Rangan Majumder and
                  Li Deng},
  editor       = {Tarek Richard Besold and
                  Antoine Bordes and
                  Artur S. d'Avila Garcez and
                  Greg Wayne},
  title        = {{MS} {MARCO:} {A} Human Generated MAchine Reading COmprehension Dataset},
  booktitle    = {Proceedings of the Workshop on Cognitive Computation: Integrating
                  neural and symbolic approaches 2016 co-located with the 30th Annual
                  Conference on Neural Information Processing Systems {(NIPS} 2016),
                  Barcelona, Spain, December 9, 2016},
  series       = {{CEUR} Workshop Proceedings},
  volume       = {1773},
  publisher    = {CEUR-WS.org},
  year         = {2016},
  url          = {https://ceur-ws.org/Vol-1773/CoCoNIPS\_2016\_paper9.pdf},
  timestamp    = {Thu, 11 Apr 2024 13:33:56 +0200},
  biburl       = {https://dblp.org/rec/conf/nips/NguyenRSGTMD16.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{yang2020retriever,
  title={Is Retriever Merely an Approximator of Reader?}, 
  author={Sohee Yang and Minjoon Seo},
  year={2020},
  journal={arXiv preprint arXiv:2010.10999},
  url={https://arxiv.org/pdf/2010.10999}
}

@inproceedings{khandelwal2021nearest,
  title={Nearest Neighbor Machine Translation},
  author={Urvashi Khandelwal and Angela Fan and Dan Jurafsky and Luke Zettlemoyer and Mike Lewis},
  booktitle=ICLR,
  year={2021},
  url={https://openreview.net/forum?id=7wCBOfJ8hJM}
}

@inproceedings{khandelwal2020generalization,
  title={Generalization through Memorization: Nearest Neighbor Language Models},
  author={Urvashi Khandelwal and Omer Levy and Dan Jurafsky and Luke Zettlemoyer and Mike Lewis},
  booktitle=ICLR,
  year={2020},
  url={https://openreview.net/forum?id=HklBjCEKvH}
}

@article{mallen2023trust,
  title={When Not to Trust Language Models: Investigating Effectiveness of Parametric and Non-Parametric Memories}, 
  author={Alex Mallen and Akari Asai and Victor Zhong and Rajarshi Das and Daniel Khashabi and Hannaneh Hajishirzi},
  year={2023},
  journal={arXiv preprint arXiv:2212.10511},
  url={https://arxiv.org/pdf/2212.10511}
}

@article{chung2022scaling,
  title={Scaling Instruction-Finetuned Language Models}, 
  author={Hyung Won Chung and Le Hou and Shayne Longpre and Barret Zoph and Yi Tay and William Fedus and Yunxuan Li and Xuezhi Wang and Mostafa Dehghani and Siddhartha Brahma and Albert Webson and Shixiang Shane Gu and Zhuyun Dai and Mirac Suzgun and Xinyun Chen and Aakanksha Chowdhery and Alex Castro-Ros and Marie Pellat and Kevin Robinson and Dasha Valter and Sharan Narang and Gaurav Mishra and Adams Yu and Vincent Zhao and Yanping Huang and Andrew Dai and Hongkun Yu and Slav Petrov and Ed H. Chi and Jeff Dean and Jacob Devlin and Adam Roberts and Denny Zhou and Quoc V. Le and Jason Wei},
  year={2022},
  journal={arXiv preprint arXiv:2210.11416},
  url={https://arxiv.org/pdf/2210.11416}
}

@conference{nentidis2023bioasq,
  title={BioASQ at~CLEF2023: The Eleventh Edition of~the~Large-Scale Biomedical Semantic Indexing and~Question Answering Challenge},
  author={Nentidis, Anastasios and Krithara, Anastasia and Paliouras, Georgios and Farr{\'e}-Maduell, Eul{\`a}lia and Lima-L{\'o}pez, Salvador and Krallinger, Martin},
  booktitle={Advances in Information Retrieval},
  year={2023},
  url={https://link.springer.com/chapter/10.1007/978-3-031-28241-6_66},
}

@article{kamalloo2023evaluating,
  title={Evaluating Open-Domain Question Answering in the Era of Large Language Models},
  author={Kamalloo, Ehsan and Dziri, Nouha and Clarke, Charles LA and Rafiei, Davood},
  year={2023},
  journal={arXiv preprint arXiv:2305.06984},
  url={https://arxiv.org/pdf/2305.06984}
}

@inproceedings{wei2022chain,
  title={Chain of Thought Prompting Elicits Reasoning in Large Language Models},
  author={Jason Wei and Xuezhi Wang and Dale Schuurmans and Maarten Bosma and brian ichter and Fei Xia and Ed H. Chi and Quoc V Le and Denny Zhou},
  booktitle={Advances in Neural Information Processing Systems},
  editor={Alice H. Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho},
  year={2022},
  url={https://openreview.net/forum?id=_VjQlMeSB_J}
}

@article{mialon2023augmented,
  title={Augmented Language Models: a Survey}, 
  author={Grégoire Mialon and Roberto Dessì and Maria Lomeli and Christoforos Nalmpantis and Ram Pasunuru and Roberta Raileanu and Baptiste Rozière and Timo Schick and Jane Dwivedi-Yu and Asli Celikyilmaz and Edouard Grave and Yann LeCun and Thomas Scialom},
  year={2023},
  journal={arXiv preprint arXiv:2302.07842},
  url={https://arxiv.org/pdf/2302.07842}
}

@inproceedings{alon2022neurosymbolic,
  title={Neuro-Symbolic Language Modeling with Automaton-augmented Retrieval},
  author={Uri Alon and Frank F. Xu and Junxian He and Sudipta Sengupta and Dan Roth and Graham Neubig},
  booktitle={ICML 2022 Workshop on Knowledge Retrieval and Language Models},
  year={2022},
  url={https://openreview.net/forum?id=ZJZmKGM6UB}
}

@article{multitaskhallu,
  title={A multitask, multilingual, multimodal evaluation of chatgpt on reasoning, hallucination, and interactivity},
  author={Bang, Yejin and Cahyawijaya, Samuel and Lee, Nayeon and Dai, Wenliang and Su, Dan and Wilie, Bryan and Lovenia, Holy and Ji, Ziwei and Yu, Tiezheng and Chung, Willy and others},
  journal={arXiv preprint arXiv:2302.04023},
  year={2023}
}

@inproceedings{raghallucination,
  author       = {Kurt Shuster and
                  Spencer Poff and
                  Moya Chen and
                  Douwe Kiela and
                  Jason Weston},
  editor       = {Marie{-}Francine Moens and
                  Xuanjing Huang and
                  Lucia Specia and
                  Scott Wen{-}tau Yih},
  title        = {Retrieval Augmentation Reduces Hallucination in Conversation},
  booktitle    = {Findings of the Association for Computational Linguistics: {EMNLP}
                  2021, Virtual Event / Punta Cana, Dominican Republic, 16-20 November,
                  2021},
  pages        = {3784--3803},
  publisher    = {Association for Computational Linguistics},
  year         = {2021},
  url          = {https://doi.org/10.18653/v1/2021.findings-emnlp.320},
  doi          = {10.18653/V1/2021.FINDINGS-EMNLP.320},
  timestamp    = {Fri, 16 Feb 2024 08:27:36 +0100},
  biburl       = {https://dblp.org/rec/conf/emnlp/0001PCKW21.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@article{llmlie,
  title={Llm lies: Hallucinations are not bugs, but features as adversarial examples},
  author={Yao, Jia-Yu and Ning, Kun-Peng and Liu, Zhen-Hui and Ning, Mu-Nan and Yuan, Li},
  journal={arXiv preprint arXiv:2310.01469},
  year={2023}
}
@misc{xu2023recomp,
      title={RECOMP: Improving Retrieval-Augmented LMs with Compression and Selective Augmentation}, 
      author={Fangyuan Xu and Weijia Shi and Eunsol Choi},
      year={2023},
      eprint={2310.04408},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@article{hallucination,
  title={Siren's Song in the AI Ocean: A Survey on Hallucination in Large Language Models},
  author={Zhang, Yue and Li, Yafu and Cui, Leyang and Cai, Deng and Liu, Lemao and Fu, Tingchen and Huang, Xinting and Zhao, Enbo and Zhang, Yu and Chen, Yulong and others},
  journal={arXiv preprint arXiv:2309.01219},
  year={2023}
}

@misc{liu2023tcrallm,
      title={TCRA-LLM: Token Compression Retrieval Augmented Large Language Model for Inference Cost Reduction}, 
      author={Junyi Liu and Liangzhi Li and Tong Xiang and Bowen Wang and Yiming Qian},
      year={2023},
      eprint={2310.15556},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{li2023unlocking,
      title={Unlocking Context Constraints of LLMs: Enhancing Context Efficiency of LLMs with Self-Information-Based Content Filtering}, 
      author={Yucheng Li},
      year={2023},
      eprint={2304.12102},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{jiang2023longllmlingua,
  author       = {Huiqiang Jiang and
                  Qianhui Wu and
                  Xufang Luo and
                  Dongsheng Li and
                  Chin{-}Yew Lin and
                  Yuqing Yang and
                  Lili Qiu},
  editor       = {Lun{-}Wei Ku and
                  Andre Martins and
                  Vivek Srikumar},
  title        = {LongLLMLingua: Accelerating and Enhancing LLMs in Long Context Scenarios
                  via Prompt Compression},
  booktitle    = {Proceedings of the 62nd Annual Meeting of the Association for Computational
                  Linguistics (Volume 1: Long Papers), {ACL} 2024, Bangkok, Thailand,
                  August 11-16, 2024},
  pages        = {1658--1677},
  publisher    = {Association for Computational Linguistics},
  year         = {2024},
  url          = {https://doi.org/10.18653/v1/2024.acl-long.91},
  doi          = {10.18653/V1/2024.ACL-LONG.91},
  timestamp    = {Tue, 24 Sep 2024 10:55:41 +0200},
  biburl       = {https://dblp.org/rec/conf/acl/JiangWL0L0Q24.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@misc{nakano2022webgpt,
      title={WebGPT: Browser-assisted question-answering with human feedback}, 
      author={Reiichiro Nakano and Jacob Hilton and Suchir Balaji and Jeff Wu and Long Ouyang and Christina Kim and Christopher Hesse and Shantanu Jain and Vineet Kosaraju and William Saunders and Xu Jiang and Karl Cobbe and Tyna Eloundou and Gretchen Krueger and Kevin Button and Matthew Knight and Benjamin Chess and John Schulman},
      year={2022},
      eprint={2112.09332},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@article{WebGLM,
  title={WebGLM: Towards An Efficient Web-Enhanced Question Answering System with Human Preferences},
  author={Liu, Xiao and Lai, Hanyu and Yu, Hao and Xu, Yifan and Zeng, Aohan and Du, Zhengxiao and Zhang, Peng and Dong, Yuxiao and Tang, Jie},
  journal={arXiv preprint arXiv:2306.07906},
  year={2023}
}
@inproceedings{longrag,
  author       = {Qingfei Zhao and
                  Ruobing Wang and
                  Yukuo Cen and
                  Daren Zha and
                  Shicheng Tan and
                  Yuxiao Dong and
                  Jie Tang},
  editor       = {Yaser Al{-}Onaizan and
                  Mohit Bansal and
                  Yun{-}Nung Chen},
  title        = {LongRAG: {A} Dual-Perspective Retrieval-Augmented Generation Paradigm
                  for Long-Context Question Answering},
  booktitle    = {Proceedings of the 2024 Conference on Empirical Methods in Natural
                  Language Processing, {EMNLP} 2024, Miami, FL, USA, November 12-16,
                  2024},
  pages        = {22600--22632},
  publisher    = {Association for Computational Linguistics},
  year         = {2024},
  url          = {https://aclanthology.org/2024.emnlp-main.1259},
  timestamp    = {Thu, 14 Nov 2024 17:20:55 +0100},
  biburl       = {https://dblp.org/rec/conf/emnlp/ZhaoWCZTD024.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@article{activerag,
  author       = {Zhipeng Xu and
                  Zhenghao Liu and
                  Yibin Liu and
                  Chenyan Xiong and
                  Yukun Yan and
                  Shuo Wang and
                  Shi Yu and
                  Zhiyuan Liu and
                  Ge Yu},
  title        = {ActiveRAG: Revealing the Treasures of Knowledge via Active Learning},
  journal      = {CoRR},
  volume       = {abs/2402.13547},
  year         = {2024},
  url          = {https://doi.org/10.48550/arXiv.2402.13547},
  doi          = {10.48550/ARXIV.2402.13547},
  eprinttype    = {arXiv},
  eprint       = {2402.13547},
  timestamp    = {Sat, 20 Jul 2024 15:05:01 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2402-13547.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@misc{zhao2024retrievalaugmentedgenerationrag,
      title={Retrieval Augmented Generation (RAG) and Beyond: A Comprehensive Survey on How to Make your LLMs use External Data More Wisely}, 
      author={Siyun Zhao and Yuqing Yang and Zilong Wang and Zhiyuan He and Luna K. Qiu and Lili Qiu},
      year={2024},
      eprint={2409.14924},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2409.14924}, 
}
@inproceedings{hu2022lora,
  title={Lo{RA}: Low-Rank Adaptation of Large Language Models},
  author={Edward J Hu and yelong shen and Phillip Wallis and Zeyuan Allen-Zhu and Yuanzhi Li and Shean Wang and Lu Wang and Weizhu Chen},
  booktitle=ICLR,
  year={2022},
  url={https://openreview.net/forum?id=nZeVKeeFYf9}
}


@inproceedings{jiang2023active,
  author       = {Zhengbao Jiang and
                  Frank F. Xu and
                  Luyu Gao and
                  Zhiqing Sun and
                  Qian Liu and
                  Jane Dwivedi{-}Yu and
                  Yiming Yang and
                  Jamie Callan and
                  Graham Neubig},
  editor       = {Houda Bouamor and
                  Juan Pino and
                  Kalika Bali},
  title        = {Active Retrieval Augmented Generation},
  booktitle    = {Proceedings of the 2023 Conference on Empirical Methods in Natural
                  Language Processing, {EMNLP} 2023, Singapore, December 6-10, 2023},
  pages        = {7969--7992},
  publisher    = {Association for Computational Linguistics},
  year         = {2023},
  url          = {https://doi.org/10.18653/v1/2023.emnlp-main.495},
  doi          = {10.18653/V1/2023.EMNLP-MAIN.495},
  timestamp    = {Fri, 12 Apr 2024 13:11:50 +0200},
  biburl       = {https://dblp.org/rec/conf/emnlp/JiangXGSLDYCN23.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{yu2014deep,
  title={Deep Learning for Answer Sentence Selection},
  author={Lei Yu and Karl Moritz Hermann and Phil Blunsom and Stephen Pulman},
  journal={arXiv preprint arXiv:1412.1632},
  year={2014},
  url={https://arxiv.org/pdf/1412.1632}
}