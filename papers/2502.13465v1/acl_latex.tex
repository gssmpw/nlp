% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[final]{acl}

% Standard package includes
\usepackage{times}
\usepackage{booktabs}
\usepackage{amsmath}
\usepackage{latexsym}
\usepackage{dsfont}
\usepackage{wrapfig}
\usepackage{multirow}
\usepackage{svg}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}
\include{math_commands}
% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{\textsc{HawkBench}: Investigating Resilience of RAG Methods on Stratified Information-Seeking Tasks}

\author{Hongjin Qian$^{1}$, Zheng Liu$^1$\thanks{Corresponding author.}, Chao Gao $^4$, Yankai Wang$^3$ \\  \textbf{Defu Lian}$^3$, \textbf{Zhicheng Dou}$^2$\\
        $^1$ Beijing Academy of Artificial Intelligence \\ 
        $^2$  Gaoling School of Artificial Intelligence, Renmin University of China\\
        $^3$ University of Science and Technology of China \\
        $^4$  The Hong Kong University of Science and Technology \\
        %Engineering Research Center of Next-Generation Intelligent Search and Recommendation, MOE \\
        \texttt{\{chienqhj,zhengliu1026\}@gmail.com} \\
}


\begin{document}
\maketitle

\begin{abstract}
In real-world information-seeking scenarios, users have dynamic and diverse needs, requiring RAG systems to demonstrate adaptable resilience. To comprehensively evaluate the resilience of current RAG methods, we introduce HawkBench, a human-labeled, multi-domain benchmark designed to rigorously assess RAG performance across categorized task types. By stratifying tasks based on information-seeking behaviors, HawkBench provides a systematic evaluation of how well RAG systems adapt to diverse user needs.

Unlike existing benchmarks, which focus primarily on specific task types (mostly factoid queries) and rely on varying knowledge bases, HawkBench offers:  (1) systematic task stratification to cover a broad range of query types, including both factoid and rationale queries, (2) integration of multi-domain corpora across all task types to mitigate corpus bias, and (3) rigorous annotation for high-quality evaluation.

HawkBench includes 1,600 high-quality test samples, evenly distributed across domains and task types. Using this benchmark, we evaluate representative RAG methods, analyzing their performance in terms of answer quality and response latency. Our findings highlight the need for dynamic task strategies that integrate decision-making, query interpretation, and global knowledge understanding to improve RAG generalizability. We believe HawkBench serves as a pivotal benchmark for advancing the resilience of RAG methods and their ability to achieve general-purpose information seeking.
We will release our codes and data in \href{https://github.com/qhjqhj00/HawkBench}{\textit{this repository}}.
\end{abstract}

\section{Introduction}
Large Language Models (LLMs) excel in general reasoning and knowledge-based tasks but often struggle with timeliness and knowledge coverage gaps, particularly in specialized domains and user-specific data~\cite{gpt-4, deepseekv3}. To address these limitations, incorporating external knowledge has become a common approach, with Retrieval-Augmented Generation (RAG) emerging as an effective solution to enhance factual accuracy and adaptability~\cite{zhu2024largelanguagemodelsinformation}.

During the information-seeking process using RAG, users may have a wide range of information needs, from simple factoid retrieval to more complex rationale-based queries~\cite{qian2024memoragmovingnextgenrag,zhao2024retrievalaugmentedgenerationrag}. This versatility requires RAG systems to possess diverse capabilities, including accurate referencing and advanced reasoning skills.

Recent advancements in RAG methods have enhanced vanilla RAG systems by targeting specific advanced capabilities. For instance, some methods focus on improving multi-hop reasoning to handle tasks with implicit information intents~\cite{longrag, activerag}, while others address information aggregation tasks by constructing intermediate structures, such as graphs or memory modules, to better integrate relevant information~\cite{qian2024memoragmovingnextgenrag, edge2024localglobalgraphrag}. 

While these advancements enable RAG systems to effectively leverage external knowledge for specific tasks, their ability to generalize across diverse scenarios remains uncertain.
A recent survey categorizes external knowledge-based tasks into distinct levels, emphasizing that no single method can effectively address all query types~\cite{zhao2024retrievalaugmentedgenerationrag}. This suggests that current RAG methods lack the resilience required for general-purpose information-seeking tasks, highlighting the need for a systematic evaluation of RAG methods across a broad range of information-seeking tasks, examining the resilience of these methods when faced with information-seeking tasks in any form.

Existing public benchmarks for RAG evaluation focus narrowly on isolated dimensions of information-seeking tasks. For instance, LegalBench-RAG evaluates information-seeking tasks in the legal domain~\cite{LegalBench}, MutiHop-RAG tests multi-hop reasoning~\cite{multihopbench}, and CRAG emphasizes comprehensive evaluation on factual QA tasks~\cite{CRAG}. While these benchmarks excel in their targeted domains, they collectively fail to assess the resilience of RAG methods across stratified task types due to three critical limitations:

\textbf{First, fragmented evaluation protocols}. Current benchmarks are siloed by design, each prioritizing distinct query types. This specialization creates inconsistent evaluation criteria, hindering fair comparisons of RAG performance across diverse task categories.
\textbf{Second, domain bias and knowledge leakage}. Many benchmarks rely on heterogeneous knowledge bases (e.g., Wikipedia and web snippets), leading to corpus-dependent performance gaps that obscure true method capabilities. Worse, LLMs are often pretrained on these same sources (e.g., Wikipedia), inflating benchmark scores through memorization rather than genuine retrieval-augmented reasoning. 
\textbf{Third, limited query diversity.} Most benchmarks disproportionately emphasize factoid questions (e.g., "When was Einstein born?"), neglecting rationale-based queries (e.g., "Explain how relativity revolutionized physics") that require synthesis and contextual analysis. This narrow focus misaligns with real-world user needs, where information-seeking behaviors span both factual lookup and complex exploratory reasoning.




In this paper, we introduce HawkBench, a human-labeled, multi-domain benchmark designed to systematically evaluate the resilience of RAG methods across stratified information-seeking tasks. Unlike existing benchmarks, HawkBench provides a structured evaluation framework that facilitates fair and comprehensive comparisons of diverse RAG approaches.
HawkBench is characterized by the following key features:

\textbf{Domain Thoroughness} – We curate raw texts from a diverse range of sources—including professional textbooks, academic papers, financial reports, legal contracts, and novels—to ensure that the benchmark reflects real-world information needs. This broad selection captures both general and specialized knowledge, offering a robust foundation for evaluation.

\textbf{Systematic Task Stratification} – We systematically define four query types: (1)~explicit factoid queries, (2)~implicit factoid queries, (3)~explicit rationale queries, and (4)~implicit rationale queries. This stratification, inspired by \citet{zhao2024retrievalaugmentedgenerationrag} with refined modifications, ensures comprehensive task coverage. Importantly, all query types share the same underlying knowledge distribution, allowing for direct and fair performance comparisons across different tasks.

\textbf{Rigorous Annotation Quality} – HawkBench employs a hybrid annotation process that leverages both advanced LLMs—specifically GPT-4 and DeepSeek-V3—and human oversight. Initially, LLMs generate query-answer pairs from the curated texts. Expert annotators then evaluate these pairs against predefined stratification levels, refine the answers by correcting inaccuracies, and enhance clarity. This process results in a high-quality dataset of 1,600 annotated test samples, evenly distributed across all task types.

We further validate HawkBench by applying representative RAG methods and performing a comprehensive analysis of their performance in terms of both answer quality and response latency. Our empirical results reveal that while current RAG methods excel in specific tasks, they generally lack overall resilience. Enhancing their adaptability will require dynamic task strategies that integrate decision-making, query interpretation, and a holistic understanding of global knowledge.

Our contributions are as follows:
(1)~We introduce HawkBench, a high-quality benchmark with stratified tasks designed to assess the resilience of RAG methods for general-purpose information-seeking.
(2)~We conduct a comprehensive empirical evaluation of recent RAG methods on HawkBench, enabling a side-by-side comparison of their capabilities.
(3)~We propose insights and strategies to improve the generalizability and adaptability of current RAG methods.

\begin{figure*}[t]
    \centering
    \includegraphics[width=\linewidth]{main.pdf}
    \caption{Query Stratification of \textsc{HawkBench}. To account for referencing difficulty, we categorize tasks into queries with explicit intent and implicit intent. Regarding reasoning, tasks are categorized into factoid queries and rationale queries. By combining these two categorizations, we stratify information-seeking tasks into four levels.}
    \label{fig:type}
    % \vspace{-10pt}
\end{figure*}

\section{HawkBench}

\subsection{Preliminary}
Recent advancements in large language models (LLMs) have popularized the Retrieval-Augmented Generation (RAG) approach, which leverages external knowledge to perform specific tasks. In RAG, a generation model $\theta(\cdot)$ and a retrieval model $\gamma(\cdot)$ collaborate to produce a final response $\gY$. Formally, the process is expressed as:
\begin{align}
    \gY &= \theta(q, \gZ), \quad \gZ = \gamma(q, \gX),
\end{align}
where $q$ denotes the input query, $\gX$ represents the external knowledge base, $\gZ$ is the retrieved relevant information, and $\gY$ is the generated answer.

This RAG framework can be viewed as an information-refinement process following the \textit{Markov chain}:
$
\gX \rightarrow \gZ \rightarrow \gY.
$
As information passes through each stage, it is progressively distilled, leading to the inequality
$
I(\gX, \gZ) \geq I(\gY, \gZ),
$
where $I(\cdot)$ denotes mutual information. Ideally, the retrieval step should extract a $\gZ$ that is both \emph{sufficient}---containing all the information necessary to generate $\gY$---and \emph{minimal}---excluding irrelevant details from $\gX$. In fact, the condition 
$
I(\gX, \gZ) = I(\gY, \gZ)
$
would hold if and only if an optimal retrieval output $\gZ^*$ exists that perfectly balances these two criteria.
Achieving such an optimal $\gZ^*$ is challenging due to estimation biases in both the retrieval and generation processes. To better understand these challenges, it is essential to consider two interrelated dimensions:
% \vspace{-7pt}
\paragraph{Referencing} 
The retrieval process must determine not only which pieces of information in $\gX$ are relevant to the query $q$ but also how much information is required. The \emph{referencing} is straightforward when $q$ explicitly states its intent, as the semantic connections between $q$ and the relevant content in $\gX$ are easier to measure. However, for implicit queries---where the intent is not clearly stated---identifying the necessary evidence becomes more complex. Thus, the referencing dimension measures \textit{how to access} the relevant knowledge, capturing both the volume of information needed and its accessibility within the knowledge base.

\paragraph{Reasoning}
Once the retrieval model produces $\gZ$, the generation model must process and integrate this information to formulate the final answer $\gY$. For factoid queries, the retrieved information typically aligns closely with the required answer, meaning that the reasoning effort is relatively minimal. In contrast, when the query demands a rationale---requiring the synthesis and integration of multiple pieces of information---the generation process must engage in more complex in-context reasoning. Therefore, the reasoning dimension measures \textit{how to utilize} the relevant knowledge, reflecting the cognitive effort needed to bridge the gap between the retrieved data and the final, coherent response.

To systematically analyze the difficulty of information-seeking tasks within the RAG framework, we decompose queries along these two dimensions. As shown in Figure~\ref{fig:type} (left), we categorize tasks based on:
\textbf{Referencing:} Whether the query explicitly or implicitly conveys its intent, thereby affecting the ease with which relevant information can be identified.
\textbf{Reasoning:} Whether the task involves straightforward fact extraction or requires integrating information to form a reasoned response.
By combining these dimensions, we define four levels of information-seeking tasks, each posing unique challenges to the RAG pipeline, as outlined in the next section.




\subsection{Query Stratification}
In Figure~\ref{fig:type} (middle), we illustrate our query stratification, presenting the four query types below.

\paragraph{Level 1: Explicit Factoid Query}
Level 1 queries exhibit an explicitly stated information-seeking intent and typically require minimal reasoning. The answer is directly available in the retrieved text. For instance, the query 
\begin{quote}
\small
\textit{``What is OpenAI’s most recent AI model?''}
\end{quote}
clearly specifies its intent, allowing the retrieval system to easily locate the pertinent information. The generator can then extract the final answer with little or no additional reasoning.

\paragraph{Level 2: Implicit Factoid Query}
Level 2 queries present an implicit information-seeking intent, which necessitates an extra step to resolve the reference before the answer can be extracted. Consider the query 
\begin{quote}
\small
\textit{``Has the company that proposed MLA made any recent advancements?''}
\end{quote}
The query does not directly name the company. The system must first infer that ``the company that proposed MLA'' refers to, for example, DeepSeek. Once this implicit reference is established, the relevant knowledge can be retrieved, and the answer can be extracted with minimal reasoning. Thus, Level 2 queries require additional referencing effort compared to Level 1, while the reasoning for answer extraction remains straightforward.

\paragraph{Level 3: Explicit Rationale Query}

In Level 3 queries, the intent is explicitly stated, but there exists a semantic gap between the query and the relevant information. Although the query clearly indicates what is being asked, the final answer is not directly extractable from a single text fragment and requires synthesizing information from multiple sources. For example, the query 
\begin{quote}
\small
\textit{``How do recent techniques enhance the long-context processing capabilities of LLMs?''}
\end{quote}
explicitly requests an explanation. However, the necessary rationale is dispersed across several texts. This scenario demands a more complex retrieval process, possibly aided by structured representations (e.g., graphs), and a generator capable of synthesizing the information into a coherent answer.

\paragraph{Level 4: Implicit Rationale Query}
Level 4 queries pose the highest challenge as they involve both an implicit intent and the need to generate a global explanation. For example, the query 
\begin{quote}
\small
\textit{``How have recent LLM techniques impacted the NLP community?''}
\end{quote}
requires the system to first infer the underlying intent and then integrate diverse pieces of information across the entire knowledge base to form a comprehensive explanation. This task demands extensive referencing to identify loosely connected yet relevant content and significant reasoning to synthesize a unified, high-level response.


\subsection{Comparison of the Four Query Levels}

In Figure~\ref{fig:type} (right), we compare the four query levels across two aspects: \textit{Reference} and \textit{Reasoning}.

First, in terms of \textit{Reference}, the amount of relevant knowledge required increases from Level 1 to Level 4 queries, reflected in the mutual information between the knowledge base and retrieved knowledge, \(I(\gX, \gZ)\). Level 1 queries require minimal knowledge, as answers are directly extractable from a few text chunks. In contrast, higher-level queries, such as Level 3 and Level 4, require synthesizing information from a broader range of texts.

Second, in terms of \textit{Reasoning}, complexity increases across levels due to the growing semantic gap between retrieved knowledge and the final answer. For Level 1 queries, reasoning is minimal, but for Level 3 and Level 4 queries, more reasoning is needed to connect multiple, loosely connected pieces of information. This is reflected in the decreasing mutual information \(I(\gZ, \gY)\) as redundant information is filtered out during refinement.

These varying requirements for referencing and reasoning present significant challenges for current RAG systems, which struggle to adapt to the diversity of information-seeking tasks. There is no one-size-fits-all solution, as each task demands distinct capabilities. This underscores the necessity of benchmarking current RAG methods across a broad range of tasks to better assess their resilience.





\subsection{Construction}

\paragraph{Corpus Collection} While most current LLMs are proficient in general world knowledge due to their training on large-scale corpora, they often lack coverage in specialized, domain-specific areas. To address this gap, HawkBench incorporates 229 domain-specific texts into its knowledge base. These texts cover a wide range of domains, including professional textbooks (manually labeled into categories such as technology, humanities, art, and science), as well as financial reports, legal contracts, novels, and academic papers. This diverse and comprehensive collection ensures that HawkBench can thoroughly evaluate the domain resilience of RAG methods by covering a broad range of user information needs.


\paragraph{Annotation Process} 
The annotation process for constructing HawkBench follows a systematic approach, as illustrated in Figure~\ref{fig:annotation}. The process consists of three key steps:

(1) \textbf{Configuration:} The annotator selects the target query level and domain, with assistance from a strong LLM (GPT-4o and DeepSeek-v3).

(2) \textbf{Question-Answer Pair Generation:} The system prompts the LLM agent using built-in QA generation prompts to produce initial question-answer pairs. During this step, the system first samples from the knowledge base, selecting a random text span of varying lengths based on the task type. For Level-1 tasks, approximately 1K tokens are used as the context. For Level-2 tasks, we use a retrieval system retrieves the top-10 passages based on the generated L1 query, selecting five passages to prompt the agent to transform explicit factoid queries into implicit intent queries. For Level-3 and Level-4 tasks, up to 120K tokens are sampled as the knowledge context to guide the agent in generating information aggregation queries, with different prompts controlling the process. The codes for annotation system and all built-in prompts are released in \href{https://github.com/qhjqhj00/HawkBench}{\textit{this repository}}.

(3) \textbf{Quality Control:} The annotator reviews the generated question to ensure it aligns with the target task type’s definition. If the question is unsuitable, it is discarded. If the question is valid, the annotator evaluates the generated answer for clarity, conciseness, and semantic richness. The answer is then manually edited to ensure high quality.

%TODO annotator payment
We employed three PhD students proficient in English as annotators. As shown in Table~\ref{tab:ann}, the difficulty of annotating different task types varies significantly. For Level-1 tasks, most generated QA pairs are valid with only minor edits needed, making this task relatively quick. In contrast, for Levels 2–4, the generated QA pairs are often invalid and discarded, and the quality of the answers generally requires more extensive manual editing. This process results in longer annotation times for higher-level tasks. The total annotation time includes both system latency (primarily due to QA pair generation) and manual annotation work. The three annotators dedicated approximately one week of full-time work to constructing HawkBench, each receiving a salary of around \$1,000. Additionally, constructing HawkBench incurred around \$597 in GPT-4o usage and \$278 in DeepSeek-v3 usage. 

\begin{table}[t]
\small
    \centering
    \begin{tabular}{c|cccc}
         Level& Discard \% & Edit \% & Ave. Time & Total Time \\
         \midrule
        1 & 6.7\% & 3.5\%  & 26s & 4.5h\\
        2 &  28.1\% & 41.4\%  & 71s &  23.1h\\
        3& 25.2\%  & 47.9\%  & 183s &  41.5h\\
        4&  29.1\%  & 40.6\%  & 201s & 45.2h\\
    \end{tabular}
    \caption{Statistical Details of HawkBench Construction.}
    \label{tab:ann}
    % \vspace{-10pt}
\end{table}

\paragraph{Dataset Distribution}
Table~\ref{tab:outdomain} presents the statistical details of HawkBench. The dataset contains 1,600 test samples, derived from 229 context knowledge bases. The compressed file size of HawkBench is only 26MB, making it highly portable for distribution. We have thoroughly reviewed the licenses of all source texts to ensure that they permit redistribution. HawkBench is distributed under the Apache License 2.0.

\begin{table*}[t]
\small
\centering
\begin{tabular}{lc|cccccccc}
\toprule
\multirow{2}{*}{Method}& \multirow{2}{*}{Type} & \multicolumn{2}{c}{\textsc{Level-1}} & \multicolumn{2}{c}{\textsc{Level-2}} & \multicolumn{2}{c}{\textsc{Level-3}} & \multicolumn{2}{c}{\textsc{Level-4}} \\

&  & Rouge-L & F1& Rouge-L & F1& Rouge-L & $\gS$-F1 & Rouge-L & $\gS$-F1 \\

\midrule
LLM & Long LLM &13.0 & 12.9 & 12.9 & 11.5 & \underline{26.2} & 24.0 & 16.9 & 33.2 \\
Lingua-2 & Compression & 11.4 & 11.4 & 12.2 & 11.4 & 23.7 & 23.9 & 15.4 & 25.2\\
MInference  & Accelerating & 11.5 & 11.1 & 12.6 & 11.2 & 25.6 & 24.2 & 17.1 & \underline{33.3} \\
\midrule
RAG & Standard RAG &50.9 & 57.5 & 34.0 & 38.6 & 17.9 & 27.3 & 15.3 & 18.3\\
HyDE  & Enhanced RAG & \textbf{64.4} & \underline{73.5} & \underline{40.0} & \underline{44.5} & 19.4 & 28.0 & 15.6 & 18.4\\
RQRAG & Enhanced RAG & \underline{64.2} &\textbf{73.6} & \textbf{41.1} & \textbf{46.8} & 19.7 & 28.6 & 15.4 & 17.4\\
MemoRAG & Global RAG& 44.8 & 50.2 & 33.7 & 37.3 & \textbf{27.3} & \textbf{34.1} & \underline{19.0} & \textbf{35.0}\\
GraphRAG  &Global RAG& 49.3 & 57.4 & 34.0 & 37.0 & 25.3 & \underline{32.5} & \textbf{20.6} & 28.7 \\
\bottomrule




\end{tabular}

\caption{Evaluation performance across four levels, averaged over all domains. The best scores are highlighted in bold, and the second-best scores are underlined. }\label{tab:level}
\vspace{-5pt}
\end{table*}

\begin{figure*}
    \centering
    \includegraphics[width=0.9\linewidth]{domain.pdf}
    \caption{Evaluation performance across four levels and eight domains for selected methods.}
    \label{fig:circle}
\vspace{-10pt}
\end{figure*}

\section{Experiment}

\subsection{Baselines and Metrics}
To investigate the resilience of RAG methods on HawkBench, we select the following representative baseline methods:
\textbf{Vanilla RAG:} This method retrieves the top passages as context.
\textbf{Enhanced RAG Methods:}  \textit{HyDE}~\cite{gao2022precisezeroshotdenseretrieval} generates a hypothetical document to enhance query retrieval.
\textit{RQRAG}~\cite{chan2024rqraglearningrefinequeries} rewrites the input query into sub-queries to refine retrieval.
\textbf{Global RAG:} These methods index the knowledge base into an intermediate form to enhance global awareness. This includes memory-based methods such as \textit{MemoRAG}~\cite{qian2024memoragmovingnextgenrag} and graph-based methods like \textit{GraphRAG}~\cite{edge2024localglobalgraphrag}.

Additionally, we explore the application of long LLMs in HawkBench, including vanilla LLMs, the prompt compression method \textit{Lingua-2}~\cite{pan2024llmlingua2datadistillationefficient}, and long-context acceleration methods such as \textit{MInference}~\cite{jiang2024minference}. All baselines in the main experiments use \textit{Qwen2.5-7B-instruct} as the generator~\cite{qwen2025qwen25technicalreport}, with \textit{BGE-M3} as the retriever~\cite{bge_m3} and the top-$k$ set to 5 for all RAG methods.

For Level-1 and Level-2 tasks, which focus on factoid queries, we use \textit{Rouge-L} and lexical F1-score as evaluation metrics. For Level-3 and Level-4 tasks, which involve rationale queries, we introduce a new evaluation metric, $\gS$-F1, defined as:
\begin{align}
    \gS\text{-F1}(A,A^*) &= \frac{1}{2n} \sum_{i=1}^{n} \mathds{1}_{\{\text{LLM}(s_i, A^*) = \text{True}\}}  \\ 
    &+ \frac{1}{2n} \sum_{i=1}^{n} \mathds{1}_{\{\text{LLM}(s_i^*, A) = \text{True}\}},
\end{align}
where $A^*$ represents the ground-truth answer, and $A$ denotes the predicted answer. This metric evaluates:
1) The proportion of sentences in the ground-truth answer $s^*_i \in A^*$ that can be supported by the predicted answer.
2) The proportion of sentences in the predicted answer $s_i \in A$ that correctly reflect the meaning of the ground-truth answer.

The indicator function $\mathds{1}_{\text{condition}}$ returns 1 if the condition holds, and 0 otherwise.

Compared to lexical F1-score, $\gS$-F1 evaluates sentence-level semantic equivalence between the ground-truth and predicted answers, making it a more robust metric for rationale-based tasks as lexical overlapping cannot infer the rationale equality. In addition to $\gS$-F1, we also employ \textit{Rouge-L} to evaluate Level-3 and Level-4 tasks.

\subsection{Main Results}
We conduct comprehensive experiments across all baselines, with the full results presented in Table~\ref{tab:qa}. To provide a more detailed analysis, we examine the results from multiple perspectives, offering a deeper understanding of performance across different dimensions.

\paragraph{Resilience across Levels} 
Table~\ref{tab:level} presents the performance of all baselines across the four task levels, averaged by domain. From these results, we draw several key insights:

(1) Standard RAG and Enhanced RAG methods perform well on factoid queries (Level-1 and Level-2), suggesting that these queries often rely on specific text spans that can be easily located with minimal reasoning or simple enhancements.

(2) Global RAG methods underperform on Level-1 and Level-2 tasks but excel on Level-3 and Level-4 tasks. This indicates that global reasoning is not beneficial for factoid queries and may even hinder performance. However, for rationale queries, which require synthesizing information from a broad range of text, global awareness helps gather more comprehensive evidence, leading to improved performance.

(3) Directly applying long LLMs to process the entire knowledge base is feasible but underperforms on factoid queries due to over-referencing and redundant noise. However, for rationale queries, long LLMs outperform vanilla RAG methods due to their strong reasoning ability over long contexts. Efficient long-context methods, such as accelerated pre-filling or prompt compression, yield performance comparable to vanilla LLMs.

\paragraph{Resilience over Domains}

Figure~\ref{fig:circle} presents the experimental results across different levels and domains for selected methods. The results highlight how different methods perform across domain-specific knowledge:

(1) For structured knowledge sources, such as financial reports and legal documents, most methods perform well on factoid queries. The inherent clarity and precision of these texts reduce semantic ambiguity, improving retrieval accuracy.

(2) For explanatory texts, such as academic papers that focus on providing rationales, global RAG methods excel. Their global awareness enables them to effectively organize and integrate explicit reasoning from the knowledge base.

(3) For unstructured knowledge in domains like literature, art, and humanities—where texts contain higher semantic ambiguity—global RAG methods perform better on Level-4 tasks. This suggests that aggregating high-level implicit information is more effective for narrative-based content than for structured knowledge domains.

\begin{figure}
    \centering
    \includegraphics[width=0.9\linewidth]{topk.pdf}
    \caption{Evaluation performance across four levels for vanilla RAG with varying Top-$k$ selections.}
    \label{fig:topk}
    \vspace{-10pt}
\end{figure}

\paragraph{Impact of Top-k}
Figure~\ref{fig:topk} analyzes the impact of Top-k selection. The results show that while increasing Top-k introduces more knowledge into the generation process, it also increases redundancy. The trade-off between knowledge recall and precision varies across query levels. Factoid queries rely on precise evidence, and excessive redundancy significantly degrades performance. In contrast, rationale queries benefit from higher recall, as effective information aggregation requires a more comprehensive set of evidence from the knowledge~base.

% \vspace{-10pt}
\paragraph{Efficiency Analysis}
\begin{table}[h]
\small
    \centering
    \begin{tabular}{c|c@{\hspace{5pt}}c@{\hspace{5pt}}c@{\hspace{5pt}}c@{\hspace{5pt}}c@{\hspace{5pt}}}
         Level& RAG  & HyDE &LLM & MemoRAG& GraphRAG \\
         \midrule
        1 & 0.6&  1.0 & 29.1 & 20.9 & 1.7 ($+\infty$)\\
        2 & 0.7&  2.0 & 32.7 & 21.5 & 2.0 ($+\infty$)\\
        3&  1.6&  2.1 & 48.3 & 33.4 & 3.0($+\infty$)\\
        4&  1.7&  2.2 & 52.1 & 35.9 & 3.5($+\infty$)\\
    \end{tabular}
    \caption{Task latency (queries per second) comparison across methods and levels. Experiments were conducted on an Nvidia A800-80G GPU using the \textsc{Art} dataset. GraphRAG employs GPT-4o for graph construction, which can take up to half an hour, denoted by $+\infty$.}
    \label{tab:effic}
\end{table}
Table~\ref{tab:effic} presents a comparison of task latency across methods and task levels. The following insights can be drawn from the results:
(1) Standard RAG methods are highly efficient, as the retrieval process is not sensitive to the size of the knowledge base. In contrast, long LLMs and global RAG methods experience a significant increase in latency across all tasks, while only improving performance on rationale tasks.
(2) Long LLMs incur the highest latency for all task types but fail to deliver a clear performance advantage. This suggests that directly using the full knowledge base may not be a proper approach.
(3) The graph construction process for GraphRAG relies heavily on robust model APIs, leading to substantial construction latency. However, once the graph is constructed, performance becomes efficient. This indicates that optimizing the process of perceiving the global knowledge base—such as accelerating the graph construction in GraphRAG or memory formation in MemoRAG—could be beneficial for improving performance on rationale queries.

\subsection{Key Insights}

\paragraph{Current RAG methods Lack Resilience}
Current RAG methods tend to be optimized for specific types of information-seeking tasks (e.g., fact retrieval or rationale generation). However, this specialization leads to a lack of overall resilience across a broader range of tasks. While empirical analyses provide heuristics to guide method selection for particular tasks, we still lack a systematic, adaptable solution that can handle diverse tasks with varying requirements. This gap emphasizes the need for developing RAG systems that can dynamically adjust to different information-seeking challenges, moving beyond task-specific optimizations toward a more generalized framework.

\paragraph{Global Awareness: Construction and Utilization Challenges}
Global awareness is essential for tasks that require the integration of information from multiple sources. However, current global RAG methods struggle with efficiently building and fully leveraging this awareness. While methods such as GraphRAG (which uses graph construction) and memory-based approaches show promise, their reliance on inefficient global intermediate construction processes (e.g., building graphs or memory stores) remains a major bottleneck. For example, graph construction can take tens of minutes, making it impractical for real-time use. Optimizing these construction processes could make these systems more viable. Additionally, there is a need for research into how to best utilize global intermediates (e.g., graphs, memory caches) to improve retrieval and reasoning. Exploring efficient ways to construct and use these intermediates is an important direction for future work.

\paragraph{Dynamic Task Understanding and Adaptive Query Interpretation}
As information-seeking tasks become more complex, the need for dynamic task understanding and adaptive query interpretation becomes increasingly important. A one-size-fits-all solution is not feasible; instead, RAG systems must integrate decision-making mechanisms that allow them to dynamically adjust how they access (referencing) and utilize (reasoning) knowledge. By understanding the task context and adapting the retrieval strategy accordingly, RAG systems can more effectively address a wider range of queries. This adaptability would significantly enhance the robustness and efficiency of RAG methods, enabling them to handle varying complexities and task types more effectively.

\paragraph{The Potential of Agentic Information-Seeking Systems}
Looking ahead, agentic information-seeking systems—capable of autonomously navigating knowledge acquisition—represent a promising frontier. These systems could perform complex tasks, such as writing papers or conducting literature surveys, by integrating retrieval, reasoning, and synthesis. Recent advancements, such as OpenAI’s \href{https://openai.com/index/introducing-deep-research/}{Deep Research}, highlight the potential for these agentic systems to become next-generation solutions for a wide range of tasks. With the ability to autonomously manage complex information-seeking tasks, these systems could transform how we approach knowledge-intensive tasks, making them an exciting area for future exploration.
\section{Related Work}

\paragraph{RAG Methods}  
RAG was introduced by \citet{lewis2020retrieval} to enhance language models' ability to handle knowledge-intensive tasks by providing relevant context through retrieval. Research in RAG has focused on two main areas: (1) improving retrieval quality to set an upper bound for generation accuracy \cite{qian2024grounding, gao2024retrievalaugmented}, and (2) optimizing the use of retrieved passages for relevance and accessibility during generation \cite{jiang2023active, longrag}.  

The integration of RAG with LLMs has gained momentum, especially in knowledge-intensive applications \cite{raghallucination}. As a result, there is increasing demand for more generalized RAG systems capable of handling a wider range of tasks, including those beyond factoid queries \cite{zhao2024retrievalaugmentedgenerationrag}. However, traditional RAG pipelines face challenges in addressing complex tasks with implicit information needs, often failing to provide sufficient context for accurate generation \cite{gao2024retrievalaugmented, zhao2024retrievalaugmentedgenerationrag}.  
Recent advances have aimed to expand RAG’s applicability. For example, \textit{GraphRAG} \cite{edge2024localglobalgraphrag} and \textit{HippoRAG} \cite{gutiérrez2024hipporagneurobiologicallyinspiredlongterm} introduce knowledge graphs to facilitate retrieval and enhance global awareness. Agent-based approaches, such as \textit{ActiveRAG} \cite{activerag, yoon2024compactcompressingretrieveddocuments}, plan information access and utilization via agents. 


\paragraph{RAG Benchmarking}  
As RAG systems are increasingly adopted, the need for comprehensive evaluation benchmarks has become evident. Early benchmarks, such as KILT \cite{kilt}, primarily focused on task-specific aspects like single-hop and multi-hop reasoning, as well as factoid queries. Recently, new benchmarks have been developed to address specialized tasks and domains. For example, MultiHop-RAG evaluates multi-hop tasks \cite{multihopbench}, LegalBench-RAG focuses on the legal domain \cite{LegalBench}, CRAG offers a comprehensive evaluation framework for factoid question answering tasks, and RAGBench is designed to assess the explainability of RAG systems \cite{RAGBench}. While these benchmarks provide valuable insights into various facets of RAG performance, they lack a comprehensive framework to evaluate the resilience of RAG systems when faced with diverse information-seeking needs, particularly for stratified queries~\cite{zhao2024retrievalaugmentedgenerationrag}.


\section{Conclusion}

In this paper, we introduce HawkBench, a comprehensive framework designed to evaluate the resilience of RAG systems across diverse information-seeking tasks. HawkBench is distinguished by its systematic task stratification, multi-domain corpora, and high-quality annotations, making it an robust tool for assessing the resilience of RAG methods. Our evaluation of representative RAG methods reveals that while current RAG systems are often optimized for specific tasks, they lack resilience across general tasks. This highlights the need for dynamic task strategies that integrate decision-making, query interpretation, and global knowledge utilization to enhance the generalizability of RAG systems. HawkBench thus serves as a critical resource for advancing the development of resilient, versatile RAG systems capable of addressing a wide range of real-world user needs.
\section*{Limitations}

This paper focuses on constructing a benchmark, HawkBench, to evaluate the resilience of RAG methods across stratified tasks. While the benchmark provides a comprehensive framework, there are several limitations to consider. First, dataset bias may arise during the curation process, as the raw data are collected from multiple domains. This diversity, while beneficial, may inadvertently introduce biases that could affect the generalizability of the results. Additionally, during the annotation process, both the assisting LLMs and human annotators may introduce errors, which could impact the overall evaluation quality. Although we strive for thoroughness in evaluating task and domain diversity, HawkBench’s size, while reasonable, may not cover all professional knowledge-intensive domains or task types.

Furthermore, while we conduct comprehensive experiments using HawkBench, it is not feasible to test all available RAG methods, alternative retrievers, or LLMs on this benchmark. We selected representative methods and models that are expected to provide generalizable findings, but this selection does not encompass the full range of possible approaches. Additionally, we did not evaluate commercial RAG solutions in this study, as these systems are typically closed-sourced and subject to changes over time, making them challenging to incorporate into a static benchmark evaluation.

\bibliography{custom}

\appendix
\section{Implementation Details}

In our evaluation of baseline methods on HawkRAG, we use BGE-M3~\cite{bge_m3} as the retriever for vanilla RAG, RQ-RAG, HyDE, and MemoRAG, setting the hit number to 5. For methods that segment long contexts into chunks, we utilize the \href{https://pypi.org/project/semantic-text-splitter/}{semantic-text-splitter} tool, limiting chunks to a maximum of 512 tokens. MemoRAG employs the officially released \href{https://huggingface.co/TommyChien/memorag-qwen2-7b-inst}{memorag-qwen2-7b-inst} as its memory model. For GraphRAG, we leverage GPT-4o for graph construction and use the retrieved context for generation. All baseline methods adopt \href{https://huggingface.co/Qwen/Qwen2.5-7B-Instruct}{Qwen-2.5-7B-instruct-128K} as the generator.

HawkRAG's raw texts are sourced from \href{https://huggingface.co/datasets/P1ayer-1/books-3-textbooks}{books-3-textbooks}, \href{https://huggingface.co/datasets/albertvillanova/legal_contracts}{legal contracts}, \href{https://huggingface.co/datasets/ThanhT04/arvix-processed-dataset}{arXiv papers}, and \href{https://huggingface.co/datasets/khaihernlow/financial-reports-sec}{financial reports}. During annotation, the annotator would select either GPT-4o or DeepSeek-v3 as the assisting agent. Our annotation system, illustrated in Figure~\ref{fig:annotation}, is implemented using \href{https://streamlit.io/}{Streamlit}. The statistic details of HawkBench are presented in Table~\ref{tab:outdomain}. In Table~\ref{tab:qa}, we present the full results of the main experiments.

All experiments were conducted on a server equipped with 8 NVIDIA A800-80G GPUs.


\begin{table*}[h]
\small
    \centering
\begin{tabular}{lc@{\hspace{4pt}}c@{\hspace{4pt}}|c@{\hspace{4pt}}c@{\hspace{4pt}}c@{\hspace{4pt}}|c@{\hspace{4pt}}c@{\hspace{4pt}}c@{\hspace{4pt}}|c@{\hspace{4pt}}c@{\hspace{4pt}}c@{\hspace{4pt}}|c@{\hspace{4pt}}c@{\hspace{4pt}}c@{\hspace{4pt}}}
\toprule
Dataset & Num & $\langle |\gC| \rangle$ & Num &  $\langle |\gQ| \rangle$ &$ \langle |\gA| \rangle$ & Num &  $\langle |\gQ| \rangle$ &$ \langle |\gA| \rangle$ & Num &  $\langle |\gQ| \rangle$ &$ \langle |\gA| \rangle$ & Num &  $\langle |\gQ| \rangle$ &$ \langle |\gA| \rangle$   \\
 &  &  & \multicolumn{3}{c}{\textsc{Level-1}} & \multicolumn{3}{c}{\textsc{Level-2}} & \multicolumn{3}{c}{\textsc{Level-3}} & \multicolumn{3}{c}{\textsc{Level-4}} \\
\midrule
\textsc{Technology} & 200 &144803.0 & 50 & 15.8 & 5.1 & 50 & 57.7 & 14.1 & 50 & 25.3 & 96.4 & 50 & 26.3 & 42.0\\
\textsc{Novel} & 200 &  166960.2 & 50 & 14.2 & 6.8 & 50 & 51.6 & 19.0 & 50 & 28.2 & 121.5 & 50 & 31.1 & 63.5\\
\textsc{Art} & 200 &  115591.8 & 50 & 17.0 & 6.9 & 50 & 53.6 & 14.8 & 50 & 27.0 & 125.2 & 50 & 34.4 & 87.7\\
\textsc{Humanities} & 200 &  152600.3 & 50 & 16.8 & 6.9 & 50 & 56.1 & 26.6 & 50 & 29.1 & 134.1 & 50 & 33.6 & 72.3\\
\textsc{Paper} & 200 & 41702.0 & 50 & 18.2 & 9.5 & 50 & 75.7 & 17.1 & 50 & 34.0 & 101.0 & 50 & 28.6 & 40.3 \\
\textsc{Science} & 200 &  143517.0 & 50 & 16.3 & 7.6 & 50 & 54.3 & 15.3 & 50 & 26.8 & 109.2 & 50 & 29.0 & 47.9\\
\textsc{Finance} & 200 &  37364.6 & 50 & 17.2 & 10.5 & 50 & 62.6 & 12.5 & 50 & 27.0 & 105.6 & 50 & 28.0 & 65.0\\
\textsc{Legal} & 200 & 49331.1 & 50 & 19.3 & 11.9 & 50 & 53.0 & 21.0 & 50 & 27.2 & 113.0 & 50 & 27.0 & 46.7\\
\midrule
Total &1600 & 106483.7 & 400 & 16.8 & 8.2 & 400 & 58.1 & 17.5 & 400 & 28.1 & 113.3 & 400 & 29.7 & 58.2 \\
\bottomrule



\end{tabular}
\caption{Statistical Information of HawkBench. The symbols $\langle |\gC| \rangle$, $\langle |\gQ| \rangle$, and $\langle |\gA| \rangle$ represent the average lengths of the context, query, and answer, respectively.}
\label{tab:outdomain}
\end{table*}


\begin{figure*}[h]
    \centering
    \includegraphics[width=0.9\linewidth]{demo.pdf}
    \caption{Annotation Interface of HawkBench.}
    \label{fig:annotation}
\end{figure*}



\include{results}



\end{document}
