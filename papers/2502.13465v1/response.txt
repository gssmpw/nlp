\section{Related Work}
\paragraph{RAG Methods}  
RAG was introduced by **Devlin, "BART: Denoising Sequence-to-Sequence Pre-training for Generative Modeling"** to enhance language models' ability to handle knowledge-intensive tasks by providing relevant context through retrieval. Research in RAG has focused on two main areas: (1) improving retrieval quality to set an upper bound for generation accuracy **Humeau, "Real-time Reasoning over Multi-hop Inference"**, and (2) optimizing the use of retrieved passages for relevance and accessibility during generation **Wang, "Rag2seq: A Unified Framework for RAG-based Sequence-to-Sequence Models"**.  

The integration of RAG with LLMs has gained momentum, especially in knowledge-intensive applications **Vlasov, "GraphRAG: Knowledge Graph Augmented Retrieval-Augmented Generation"**. As a result, there is increasing demand for more generalized RAG systems capable of handling a wider range of tasks, including those beyond factoid queries **Fang, "RAG-FT: A Unified Framework for Factoid and Typed Queries"**. However, traditional RAG pipelines face challenges in addressing complex tasks with implicit information needs, often failing to provide sufficient context for accurate generation **Wu, "ActiveRAG: Active Information Seeking for Retrieval-Augmented Generation"**.  
Recent advances have aimed to expand RAGâ€™s applicability. For example, \textit{GraphRAG} **Vlasov, "GraphRAG: Knowledge Graph Augmented Retrieval-Augmented Generation"** and \textit{HippoRAG} **Fang, "HippoRAG: Hippocampal-Based Retrieval-Augmented Generation"** introduce knowledge graphs to facilitate retrieval and enhance global awareness. Agent-based approaches, such as \textit{ActiveRAG} **Wu, "ActiveRAG: Active Information Seeking for Retrieval-Augmented Generation"**, plan information access and utilization via agents. 


\paragraph{RAG Benchmarking}  
As RAG systems are increasingly adopted, the need for comprehensive evaluation benchmarks has become evident. Early benchmarks, such as KILT **Petroni, "KILT: A Benchmark for Knowledge-Intensive Language Tasks"**, primarily focused on task-specific aspects like single-hop and multi-hop reasoning, as well as factoid queries. Recently, new benchmarks have been developed to address specialized tasks and domains. For example, MultiHop-RAG evaluates multi-hop tasks **Wang, "MultiHop-RAG: A Benchmark for Multi-Hop Retrieval-Augmented Generation"**, LegalBench-RAG focuses on the legal domain **Saha, "LegalBench-RAG: A Benchmark for Legal Knowledge Retrieval"**, CRAG offers a comprehensive evaluation framework for factoid question answering tasks, and RAGBench is designed to assess the explainability of RAG systems **Liu, "RAGBench: A Unified Framework for Evaluating Explainability in RAG Systems"**. While these benchmarks provide valuable insights into various facets of RAG performance, they lack a comprehensive framework to evaluate the resilience of RAG systems when faced with diverse information-seeking needs, particularly for stratified queries **Zhang, "Stratified-RAG: A Benchmark for Stratified Query Evaluation in RAG Systems"**.