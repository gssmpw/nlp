\section{Related Work}
\paragraph{RAG Methods}  
RAG was introduced by \citet{lewis2020retrieval} to enhance language models' ability to handle knowledge-intensive tasks by providing relevant context through retrieval. Research in RAG has focused on two main areas: (1) improving retrieval quality to set an upper bound for generation accuracy \cite{qian2024grounding, gao2024retrievalaugmented}, and (2) optimizing the use of retrieved passages for relevance and accessibility during generation \cite{jiang2023active, longrag}.  

The integration of RAG with LLMs has gained momentum, especially in knowledge-intensive applications \cite{raghallucination}. As a result, there is increasing demand for more generalized RAG systems capable of handling a wider range of tasks, including those beyond factoid queries \cite{zhao2024retrievalaugmentedgenerationrag}. However, traditional RAG pipelines face challenges in addressing complex tasks with implicit information needs, often failing to provide sufficient context for accurate generation \cite{gao2024retrievalaugmented, zhao2024retrievalaugmentedgenerationrag}.  
Recent advances have aimed to expand RAG’s applicability. For example, \textit{GraphRAG} \cite{edge2024localglobalgraphrag} and \textit{HippoRAG} \cite{gutiérrez2024hipporagneurobiologicallyinspiredlongterm} introduce knowledge graphs to facilitate retrieval and enhance global awareness. Agent-based approaches, such as \textit{ActiveRAG} \cite{activerag, yoon2024compactcompressingretrieveddocuments}, plan information access and utilization via agents. 


\paragraph{RAG Benchmarking}  
As RAG systems are increasingly adopted, the need for comprehensive evaluation benchmarks has become evident. Early benchmarks, such as KILT \cite{kilt}, primarily focused on task-specific aspects like single-hop and multi-hop reasoning, as well as factoid queries. Recently, new benchmarks have been developed to address specialized tasks and domains. For example, MultiHop-RAG evaluates multi-hop tasks \cite{multihopbench}, LegalBench-RAG focuses on the legal domain \cite{LegalBench}, CRAG offers a comprehensive evaluation framework for factoid question answering tasks, and RAGBench is designed to assess the explainability of RAG systems \cite{RAGBench}. While these benchmarks provide valuable insights into various facets of RAG performance, they lack a comprehensive framework to evaluate the resilience of RAG systems when faced with diverse information-seeking needs, particularly for stratified queries~\cite{zhao2024retrievalaugmentedgenerationrag}.