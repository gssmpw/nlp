\section{Method} \label{sec:methodology}
\subsection{Gaussian Building Mesh Extraction}
We use the mesh extraction procedure we introduced in December 2024 \cite{gbm}. For conciseness, the process is briefly described here, and is not benchmarked. We refer the readers to \cite{gbm} for the original implementation details and benchmark comparisons. We also refer the readers to \cite{2023gaussian_splatting} for background and theory on Gaussian Splatting.

Gaussian Building Mesh (GBM) \cite{gbm} is an end-to-end 3D building mesh extraction pipeline we recently proposed, leveraging Google Earth Studio \cite{google_earth_studio}, Segment Anything Model-2 (SAM2)\cite{SAM2} and GroundingDINO \cite{groundingdino}, and a modified \cite{2dgsp} Gaussian Splatting \cite{2023gaussian_splatting} model. The pipeline enables the extraction of a 3D building mesh from inputs such as the building's name, address, postal code, or geographical coordinates. Since the GBM uses Gaussian Splatting (GS) as its 3D representation, it also allows for the synthesis of new photorealistic 2D images of the building under different viewpoints. 
%The GBM pipeline is as follows.
%\begin{itemize}
%    \item Google Earth Studio is used to retrieve multi-view remote sensing images from an address, postal code, place name, or geographic coordinates.
%    \item SAM2 and GroundingDINO are used for consistent building mask extraction using text or click-based inputs. This module extracts the building from its background in 2D images.
%    \item A mask refinement method was developed, incorporating morphological operations and the Ramer-Douglas-Peucker algorithm to improve 2D mask accuracy.
%    \item An improved version of the 2DGS Gaussian Splatting \cite{2dgsp} model is utilized to learn a 3D representation from 2D multi-view masked remote sensing images.
%    \item 3D colored building meshes are produced through masked Truncated Signed Distance Function (TSDF) fusion. 
%\end{itemize}

\subsection{Google Maps Platform Integration}
We use the Python client binding for Google Maps Platform Services APIs to create an integration tool to automatically retrieve the GIS and mapping information of a building. For these image analysis experiments, the data is retrieved with four API calls. The first is a Google Maps Platform Geocoding/Reverse Geocoding API call which retrieves the complete address information including geographic coordinates, entrance(s) coordinates, and building polygon mask vertex coordinates. Then, a Google Maps Platform Elevation API call is used to retrieve the ground elevation using the building's coordinates as input. Additional API calls to other Cloud Services can also be performed at this step. Finally, two API calls are made using the Google Maps Platform Static Maps API to retrieve map(s) and satellite/aerial image(s) at the desired zoom level. This process is illustrated in Figure \ref{fig:googlepipeline}. The aerial/satellite image(s) are then used as one of the inputs to our Multi-Agent LLM Module.

Our Google Map Platform Integration can easily be modified to retrieve additional data from the cloud-based mapping service by adding parallel API calls below the Geocoding/Reverse Geocoding API call. For example, if we wish to analyze real-time traffic data, we can simply perform API calls to the Traffic API.
\input{diagram_googlepipeline}
\subsection{Multi-Agent LLM Analysis of Multi-View/Scale Images}
The motivation of this module is to create a multi-agent LLM system to analyze the data retrieved from Google Cloud Platform Services integration. In this paper, we restrict the scope of this paper to the multi-agent content analysis of multi-view/scale images.

For this analysis, the primary goal is to retrieve and store keywords describing the architectural style, function, landscape, and architectural elements of the building using keywords. With the idea that an accurate set of keywords will allow an agent to reconstruct a text-based description of the image without actually seeing the image, we also retrieve the caption using keywords as a secondary objective. This process is illustrated in Fig. \ref{fig:LLMpipeline}.


From a building's address, place name, postal code, or geographic coordinates, we retrieve multi-view off-nadir images of the building of interest using Google Earth Studio (or use the ones previously retrieved in the GBM module). We also retrieve top-down view aerial/satellite image(s) at different scales using the building using Google Map Platform Integration. For each image, we initiate a GPT4o/GPT4o-mini agent and prompt it to analyze the image and retrieve a set of keywords for each image. We then initiate two agents, one to aggregate the keywords from all the images of the building, and one to turn the aggregated keywords into a human-readable caption description. 


\input{Diagram_LLMPipeline}













\subsection{Metrics}
Although metrics such as BLEU and CIDEr are commonly used to evaluate captioning performance, they require supervised datasets with ground truth captions. However, our images lack ground truth captions. Therefore, we use the CLIP (Contrastive Languageâ€“Image Pretraining) score \cite{clip}. A CLIP-trained Transformer is used to embed both the caption and image into a shared image-language latent space. Given the text embedding of the caption, $\mathbf{t}$ and the image embedding of the corresponding image, $\mathbf{i}$, the CLIP score is given by 

\begin{equation}
\text{CLIP Score (\%)} = 100 \frac{\mathbf{t} \cdot \mathbf{i}}{\|\mathbf{t}\| \|\mathbf{i}\|}.
\end{equation}
We also use perplexity to roughly assess LLM image-to-keyword extraction confidence. Perplexity is a measure of the model's confidence in its response, and is given by
\begin{equation}
\text{PPL} = \exp \left( - \frac{1}{N} \sum_{i=1}^{N} \log P(x_i)\right)
\end{equation}
where $\log P(x_i)$ is the log-probability of the token $x_i$ as assessed by the model during text-token autoregression.