\newpage
\appendix % Start Appendix

\renewcommand{\thefigure}{A\arabic{figure}} % Figure numbering as A1, A2, etc.
\renewcommand{\thetable}{A\arabic{table}}   % Table numbering as A1, A2, etc.
\setcounter{figure}{0}  % Reset figure counter
\setcounter{table}{0}   % Reset table counter

\section*{Additional Results}
The per-scene-per-model average CLIP scores are shown in Fig. \ref{fig:4ModelsCaptionCLIp_perbuilding}. Deepseek-V3 achieves higher caption CLIP scores than other LLMs, except in two cases. The first is the ICON scene, which features twin high-rise buildings with retail spaces at the ground level. The second is the Parliament Hill of Canada scene overlooking the Ottawa River, characterized by its gothic-style architecture. Notably, the Parliament Hill scene also received the lowest overall CLIP scores.


\begin{figure*}[htpb]
\centering
\includegraphics[width = 0.9\textwidth]{Figures/PerBuilding_Captuioning_CLIP.png}
\caption{Average CLIP score of each building image captioning model for each different scene.}\label{fig:4ModelsCaptionCLIp_perbuilding}
\end{figure*}

\section*{Input images}
Input images for the Parliament Hill scene (Fig. \ref{fig:parliament_hill}) and the ICON scene (Fig. \ref{fig:icon}) are provided. These are the two scenes with the lowest multi-agent captioning CLIP scores. 
\input{Figure_input_images_parl}
\input{Figure_input_images_icon}
