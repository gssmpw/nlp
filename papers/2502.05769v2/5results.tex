\section{Experiments and Discussions}
\subsection{Experiments}
We chose seven different buildings to test our framework. These include well-known landmarks, commercial, residential, and institutional buildings. We extract 31 multi-view images in a 360$\degree$ view pose around the building of interest, which we then use in conjunction with our GBM module to create the 3D colored mesh of the building. Then we subsample six images, one every 70$\degree$, as inputs to the Multi-Agent LLM module. We also use the Google Map Platform integration to retrieve two aerial/satellite image(s), one at Google Maps zoom level 18, and one at Google Maps zoom level 19 as inputs to the Multi-Agent LLM module.

In preliminary experiments, we noticed a relatively large variation in final CLIP-score across different attempts even while using the same model and prompt, since the LLMs' outputs are not deterministic (even when using \textit{LLM Temperature} = 0). As such, we perform two experiments. We want to understand the performance of the module when using different models as LLM agents, and we want to understand the distribution of scores across different attempts for both the keyword extraction step, and the captioning step. 

\begin{figure}[htpb]
\centering
\includegraphics[width = 0.4\textwidth]{Figures/keyword_Perplexity.png}
\caption{Box plot of image-to-keyword perplexity distribution per model and level of image detail (2240 samples total).}\label{fig:perplexity}
\end{figure}
\subsubsection{Keyword Extraction}
For multi-agent image-to-keyword extraction, both \textit{chatgpt-4o-latest, and gpt-4o-mini} are suitable. Additionally, both high-resolution and low-resolution image analysis are available. We test all 4 combinations for all 7 scenes, for 10 iterations, with 8 images and LLM agents, resulting in a total of 2240 API calls. We record the LLM responses and the perplexity scores. The results are shown in Fig.\ref{fig:perplexity}.

%%%%%%%%%%%%%%%%%%%%%%

\subsubsection{Captioning}
For the keyword-to-caption step, we fix the per-image keywords for each scene using the results from one of the \textit{gpt-4o high image resolution} API calls. For each of the 7 buildings, we test 5 iterations of keyword-aggregation-caption for each of the four models: \textit{gpt-4o-mini, chatgpt-4o-latest, deepseek-chat, deepseek-reasoner}. Each test requires two API calls, totaling 448 API calls. We additionally calculate CLIP scores for every single one of the input images. Resulting in $7\cdot5\cdot4\cdot8 = 1120$ CLIP scores. The per-model clip score distributions are visualized in Fig \ref{fig:4ModelsCaptionCLIp}.

\subsubsection{Visualization}
We present a visualization of the extracted 3D model, caption, keywords, and Google Maps Platform-based information for the Perimeter Institute (PI) building scene in Fig. \ref{fig:visualizationFig}. The Perimeter Institute for Theoretical Physics is an independent research centre located at 31 Caroline St. N, Waterloo, Ontario, Canada. We show the 3D mesh and depth maps extracted from the scene, the 2D map, and the aerial image with the building's polygon at Google Maps zoom level 18, retrieved via the Google Maps Platform Static Maps API. We also plot the keywords extracted from a single view, as well as the caption generated by the Multi-Agent LLM module.
\begin{figure}[htpb]
\centering
\includegraphics[width = 0.4\textwidth]{Figures/Captioning_CLIP.png}
\caption{Box plot of CLIP score distribution of building captions generated from aggregating multiple iterations of multi-view keywords and captioning (1280 samples total).}\label{fig:4ModelsCaptionCLIp}
\end{figure}

\input{Figure_Visualization}


\subsection{Discussion}
Fig. \ref{fig:perplexity} shows the level of detail does not significantly affect the LLM agents' confidence in their own predictions. Perhaps surprisingly, the much smaller \textit{gpt-4o-mini} is more confident in its own responses on average. We note this does not necessarily denote keyword accuracy since it is possible the larger model considers many equivalent alternate visual descriptions, lowering its confidence in any individual description. Visual inspection of captions and images shows that across all four configurations, caption-to-image agreement is high. Although it is possible that the smaller model is enough for the visual captioning task, we nonetheless used the larger model with high visual quality for multi-view/multi-scale keyword extraction to test the keyword-to-caption step.

\textit{Deepseek-reasoner}, the Deepseek-R1 model has the poorest captioning score. Additionally, this model sometimes failed at the task, shown as outliers in Fig. \ref{fig:4ModelsCaptionCLIp}. This is perhaps because image captioning is an autoregressive text generation task and not a reasoning task. Consequently, we decided not to test OpenAI's reasoning model, the GPT-o1. Its performance on reasoning tasks is comparable to Deepseek-R1 according to \cite{deepseekr1}, yet it is more expensive by a factor of 30-100. \textit{Deepseek-chat}. The Deepseek-V3 model has on average the best captioning performance, offering a very good best price-performance ratio with performance comparable to GPT-4o, but prices similar to GPT-4o Mini (see Table \ref{Tab:models}).



Our future research aims to leverage the multi-agent LLM tool for geospatial data analysis, integrating various data sources from Google Cloud Platform services, including Google Maps Platform APIs and Google Earth Engine. Additionally, benchmarking the reasoning capabilities of large language models, such as GPT-o1/o3 and Deepseek-R1, for remote sensing and GIS tasks could yield valuable insights.







