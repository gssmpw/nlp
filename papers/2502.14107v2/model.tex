\section{Model}
\label{sec:model}

The two radios integrated into our sensor nodes -- the CC1200 \cite{instruments2013cc1200} and the CC2538 System-on-Chip \cite{instruments2013cc2538} -- support dynamic transmission power adaptation and define thresholds for successfully receiving packets. The CC1200 has a sensitivity of -123 dBm and its transmission power can be adjusted up to a maximum level of 16 dBm. This radio operates in sub-Giga Hertz frequency bands. The CC2538 radio chip operates in the 2.4 GHz and has a sensitivity of -97 dBm. Its transmission power can be adjusted up to a maximum level of 7 dBm. In addition, the Contiki operating system  \cite{oikonomou2022contiki} -- which we used to set up the sensing network -- enables the definition of soft thresholds to improve the reliability of packet reception. 

Once a receiving node defines the threshold power of incoming packets, the purpose of an adaptive transmission power model is to estimate the transmission power, so that packets reaching the receiving node are received with a minimum power that equals or exceeds the threshold power. This task requires a feedback system and can be accomplished in two ways. The decision leading to dynamic power adaptation can be made either at the transmitter or at the receiver. In the first case, the transmitter evaluates the power with which ACK packets are received and estimates how far the receiver is from it and adjusts the transmission power of the next packet accordingly. In the second, the decision is made at the receiver; by evaluating the power of received packets, it adjusts its own power to transmit ACK packets. The power it uses implicitly reminds the transmitter to adjust its transmission power for the next packet. In either case, a lightweight model is required to predict the received power.

The relationship between the transmitted power, received power, and the distance of separation for a line-of-sight (LOS) wireless link is classically expressed as follows: 
\begin{equation}
\label{eq:model_1}
P_{rx} \propto G_{tx} G_{rx} A_{tx} A_{rx} \frac{P_{tx}}{d^2},
\end{equation}
where $G$ and $A$ refer to the gains and the cross-sectional areas of the transmitter and the receiver antennas. The terms can be cleanly separated if the received power is expressed in decibel:
\begin{equation}
\label{eq:model_2}
10 \; log \left (P_{rx} \right ) =  K + 10 \; log \left ( P_{tx} \right ) - 20 \; log \left (d \right ),
\end{equation}
where $K = 10 \; \log \left (G_{tx} G_{rx} A_{tx} A_{rx} \right) $ is a constant once the antennas are selected and can be experimentally determined by fixing $P_{tx}$ and $d$ and observing the received power. For example, for the CC2538 radio, at $10m$ distance and 0 dBm transmission power, the received power is on average -50 dBm (consequently, $K = 10^{-3}$). Once $K$ is established, the received power can be expressed in terms of the transmission power and the distance. The advantage of this model is that its computational cost is modest. For our case, both radios provide the power of received packets in dBm and their transmission powers are configured in dBm. Nevertheless, the usefulness of this model is limited on account of the constant and complex motion the communicating nodes undergo. In addition, the LOS assumption may not hold, or holds only marginally, since  water scatters, refracts, and reflects the electromagnetic signal \cite{van2013propagation}.

Can the received power be predicted from its immediate past history? This can be answered in the affirmative if the received power exhibits a strong autocorrelation. Once we have sufficient statistics of the received power, it is possible to test this. The advantage of this approach is that the statistics can be established and updated from received packets alone. One of its limitations is that due to the complexity of the motion  the nodes undergo, the correlation between neighbor samples could be too weak. 

Alternatively, a direct correlation between the underlying motion and the link quality fluctuation can be established. This can be achieved in two ways. One of them is to estimate the relative distance of the receiver (i.e., relative to its past position) as a function of the perceived change in the acceleration, $\Delta d =  f( \Delta \textbf{a})$. Once $\Delta d$ is estimated, it can be summed to the previously known displacement of the node in order to determine the overall displacement, $d$. Then $d$ can be inserted in Equation \ref{eq:model_1} to estimate the transmission power which satisfies the threshold condition. This approach, too, is error-prone due to the difficulty of estimating displacement from the 3D acceleration. Alternatively, the change in the received power can be directly correlated with the change in the perceived acceleration, so that $\Delta P_{rx}$ can be expressed in terms of $ f(\Delta \textbf{a})$.  

\subsection{Linear Relationship}
Given the relationship between two random variables $\textbf{x}$ and $\textbf{y}$, it is possible to establish the statistics of one of the random variables in terms of the statistics of the other random variable. For example, if $\textbf{y} =  a$\textbf{x}$ + b$, where $a$ and $b$ are constants and $a > 0$, then the distribution of $\textbf{y}$ can be established in terms of the distribution of $\textbf{x}$:
\begin{align}
    \label{eq:rel_1}
    F_y(y) & = P\left \{ \mathbf{y} \leq y \right \} = P\left \{ \left (a\mathbf{x} + b  \right ) \leq y \right \} \\ \nonumber
           & = P\left \{ \mathbf{x} \leq \frac{y - b}{a}  \right\}
           =  F_x \left (  \frac{y - b}{a}  \right ).
\end{align}

\begin{figure}
	\centering
	\includegraphics[width=0.45\textwidth]{random_variables.pdf}
	\caption{Relationship between two random variables.}
	\label{fig:random_variables}
\end{figure}

Similarly, if two random variables have a one-to-one relationship, their relationship can be estimated from their distribution functions, assuming that these are available. Consider Figure ~\ref{fig:random_variables}; given that the random variable $\mathbf{y}$ is the dependent variable (and $\mathbf{x}$, the independent variable), then it is true that for every $y_i$, there is a corresponding $x_j$. Furthermore,
\begin{equation}
    \label{eq:rel_2}
     P\{\mathbf{y} \leq y_i \} = P\{\mathbf{x} \leq x_j \}.
\end{equation}
In other words,
\begin{equation}
    \label{eq:rel_3}
    F_y(y_i) = F_x(x_j).
\end{equation}
From which we can derive (ref. to \cite{dargie2014stochastic}):
\begin{equation}
    \label{eq:rel_4}
    y_i = F^{-1}_y \left (F_x(x_j) \right ).
\end{equation}

An interesting aspect of Equation~\ref{eq:rel_3} is that if the two distributions have similar characteristics, we can suspect the existence of a linear relationship between $\mathbf{x}$ and $\mathbf{y}$. We shall demonstrate this for two distribution functions, uniform and exponential. Suppose both $\mathbf{x}$ and $\mathbf{y}$ are uniformly distributed, the former between $[0, a]$, and the latter between $[0,b]$. Hence, $F_y(y) = y/a$ and $F_x(x) = x/a$, from which we conclude (applying Equation~\ref{eq:rel_4}): $y = (b/a) x$. Now suppose that the two random variables are exponentially distributed, with $F_y(y) = ( 1 -  e^{-\lambda_y y})$ and   $F_x(x) = (1 - e^{-\lambda_x x}), \lambda_x, \lambda_y > 0$.  Applying Equation~\ref{eq:rel_4} to these distribution functions leads to:
\[
\mathbf{y} = \left [ \frac{\lambda_x}{\lambda_y} \right ]  \mathbf{x}.
\]
 Since $\lambda_x$ and  $\lambda_y$ are constants, so is their ratio, so that: 
\[
\mathbf{y} = A \mathbf{x}.
\]
where $A = \lambda_x/\lambda_y$. The assertion of a linear relationship is valid for any type of distributions.

\begin{figure}
	\centering
	\includegraphics[width=0.48\textwidth]{histogram_cc1200.pdf}
	\caption{Histograms depicting the change in the RSSI of received packets and the change in the linear acceleration of the transmitting node. The nodes were using the CC1200 radio and the channel had a center frequency of 869.3 MHz and a bandwidth of 2 MHz.}
	\label{fig:histogram_cc1200}
\end{figure}

\begin{figure}
	\centering
	\includegraphics[width=0.48\textwidth]{histogram_cc2538.pdf}
	\caption{Histograms depicting the change in the RSSI of received packets and the change in the linear acceleration of the transmitting node. The nodes were using the CC2538 radio (2.4 GHz).}
	\label{fig:histogram_cc2538}
\end{figure}

Figures \ref{fig:histogram_cc1200} and~\ref{fig:histogram_cc2538} display the histograms of the changes in the RSSI and linear acceleration for the CC2538 and CC1200 radios and the deployments we carried out at Crandon Beach, Miami. The statistics are established based on 5 independent experiments carried out on different days. Each experiment lasted approximately 30 minutes. The pattern is similar for the deployments carried out at South Beach and North Biscayne Bay. This suggests that the change in the received power can be regarded as linearly correlated with the changes in the linear accelerations. If this observation is valid, then it is possible to employ linear models to establish a relationship between $\Delta P_{tx}$ and $f\Delta \mathbf{a}$.

\subsection{Minimum Mean Square Estimation}
\label{sec:mmse}

\begin{figure}
	\centering
	\includegraphics[width=0.48\textwidth]{rssi_cc1200_crandon.pdf}
	\caption{Link quality fluctuation between two communicating nodes as a result of the constant movement of the surface of water at one of our deployment settings. }
	\label{fig:crssi_cc1200}
\end{figure}

Figure \ref{fig:crssi_cc1200} displays the change in the RSSI of received packets. For this  deployment, the transmitter was deployed on the surface of water and the receiver, on dry land. A 25 mph North-East wind was blowing, thus moving the transmitter appreciably in a three-dimensional space. The RSSI consists of two components, a long-term variation, which is distance dependent (a result of the translational motion of the node), and a local variation, which is due to the local oscillation of the node. We can exploit these characteristics to estimate the RSSI at time $t$ in terms of its past history as well as the perceived change in the acceleration at time $t$: 
\begin{equation}
\label{eq:model_3}
    \hat{ \mathbf{r}}(t) =  \rho (t) \mathbf{r} (t-1) + \alpha (t) \mathbf{a} (t),
\end{equation}
where $\mathbf{a}(t)$ is the 3D acceleration the node experienced at time $t$. Both $\mathbf{r}(t)$ and $\mathbf{a}(t)$ are random variables. The objective of the minimum mean square error estimation is to minimize the  mean square error:% -- the difference between $\mathbf{r} (t)$ and $\hat{\mathbf{r}(t)}$:
\begin{equation}
\label{eq:model_4}
E \left [ \mathbf{e}^2(t) \right ] = E \left [ \left (\mathbf{r} (t) - \hat{\mathbf{r}}(t) \right )^2 \right ].
\end{equation}
The mean square error is minimized by determining the optimal $\rho(t)$ and $\alpha(t)$, which are, in turn, determined by inserting Equation~\ref{eq:model_3} in Equation~\ref{eq:model_4} and differentiating Equation~\ref{eq:model_4} with respect to $\rho(t)$ and $\alpha(t)$ and setting the results to 0:
\begin{align}
\label{eq:model_4_orthogonal}
\frac{\partial E \left [ \mathbf{e}^2(t)\right]}{\partial \rho(t)} & =   E \left [ \left ( \mathbf{r}(t) - \hat{\mathbf{r}}(t) \right ) \mathbf{r}(t-1) \right]  = 0 \\ \nonumber 
\frac{\partial E \left [ \mathbf{e}^2(t)\right]}{ \partial \alpha(t)} & =   E \left [ \left ( \mathbf{r}(t) - \hat{\mathbf{r}}(t) \right ) \mathbf{a}(t) \right]  = 0.  \nonumber 
\end{align}
Note that, due to the three-dimensional nature of the acceleration, $\alpha(t)$ is a compound term consisting of $\alpha_x(t)$,  $\alpha_y(t)$, and  $\alpha_z(t)$. Hence, we have\footnote{We shall drop the time indices whenever the context is clear.}:

\begin{footnotesize}
\begin{align}
\label{eq:model_5}
E \left [ \mathbf{r} (t) \mathbf{r} (t-1) \right ]  & = \rho E \left [ \mathbf{r}^2(t-1) \right ] + \dots + \alpha_z E \left [ \mathbf{r} (t -1) \mathbf{a}_z (t) \right] \\ \nonumber
E \left [ \mathbf{r} (t) \mathbf{a}_x (t) \right ]  & = \rho E \left [ \mathbf{r}(t-1) \mathbf{a}_x (t) \right ] + \dots + \alpha_z E \left [ \mathbf{a}_x (t) \mathbf{a}_z (t) \right] \\ \nonumber
E \left [ \mathbf{r} (t) \mathbf{a}_y (t) \right ]  & = \rho E \left [ \mathbf{r}(t-1) \mathbf{a}_y (t) \right ] + \dots + \alpha_z E \left [ \mathbf{a}^2_y (t) \mathbf{a}_z (t) \right] \\ \nonumber
E \left [ \mathbf{r} (t) \mathbf{a}_z (t) \right ]  & = \rho E \left [ \mathbf{r}(t-1) \mathbf{a}_z (t) \right ] + \dots + \alpha_z E \left [ \mathbf{a}^2_z (t) \right]. 
\end{align}
\end{footnotesize}
Or in matrix form we have:
\begin{equation}
    \label{eq:model_6b}
    \begin{bmatrix}
    R_{r} \\
    R_{x} \\
    R_{y} \\
    R_{z} 
    \end{bmatrix}
     = 
     \begin{bmatrix}
         R_{rr} & R_{rx} & R_{ry}  & R_{rz} \\ 
         R_{rx} & R_{xx} & R_{xy}  & R_{xz} \\ 
         R_{ry} & R_{xy} & R_{yy}  & R_{yz} \\ 
         R_{rz} & R_{xz} & R_{yz}  & R_{zz} \\ 
     \end{bmatrix}
     \begin{bmatrix}
         \rho \\
         \alpha_x \\
         \alpha_y \\
         \alpha_z \\
     \end{bmatrix},
\end{equation}
where $R_{r}$ refers to the correlation between $\mathbf{r}(t)$ and $\mathbf{r}(t-1)$; $R_{x}$ refers to the correlation between $\mathbf{r}(t)$ and $\mathbf{a}_x(t)$, etc.; $R_{rr}$ refers to the correlation of $\mathbf{r} (t-1) $ with itself, $R_{rx}$ refers to the correlation between $\mathbf{r}(t-1)$ and $\mathbf{a}_x$; and $R_{xy}$ refers to the correlation between $\mathbf{a}_x(t)$ and $\mathbf{a}_y(t)$, etc. The left term in Equation~\ref{eq:model_6b} encodes the correlation of the measurement with the parameter to be estimated, i.e., $\mathbf{r}(t)$; the middle term encodes the correlation of the measurement with itself; and the last term contains the coefficients which optimize the estimation. The strength of the correlations depends on many factors, including the synchronization between the samples representing the RSSI and the 3D acceleration; the magnitude and direction of the waves; the rates at which the RSSI and the inertial sensors are sampled; as well as the observation period. More compactly, we can express Equation~\ref{eq:model_6b} as:
\begin{equation}
    \label{eq:model_6}
     R =   E   A.
\end{equation}
%where $R$ refers to the correlation between the observation and the parameter to be estimated (i.e., the received power); $E$ refers to the correlation between the observation (evidence); and $A$ refers to the mean square coefficients to be estimated. Referring to the three terms in Equation~\ref{eq:model_6} as $R$, $E$, $A$, respectively, 
Hence, the optimal coefficients minimizing the mean square error in Equation~\ref{eq:model_4} can be expressed as follows:
\begin{equation}
\label{eq:model_7}
 A = E^{-1} R.
 \end{equation}

\subsection{Gradient Descent}
However, Equation~\ref{eq:model_7} relies on the inverse of $E$. To solve for $A$, we can use standard linear systems solvers from numerical analysis e.g., Cholesky decomposition \cite{trefethen2022numerical}. To solve the system $R=EA$, where $R$ is $m\times m$ positive definite matrix, costs $O(m^3)$ operations. An alternative light-weight approach is to employ gradient descent, which is the approach we adopt in this paper. Thus, instead of solving the system $R=EA$ directly, we consider the following optimization problem:

\begin{equation}
\label{eq:model_7a}
\min_{A}\,\,\, \frac{1}{2} \|E A - R\|_2^2,
\end{equation}
where $||\cdot||_2$ denotes the $\ell_2$ norm. Equivalently, we can solve
%Expanding the objective function, we have:
%\[
%\frac{1}{2} \|E A - R\|_2^2 = \frac{1}{2} A^T E^T E A - A^T E^T R + \frac{1}{2} R^T R.
%\]
%Since the last term does not depend on $A$, the problem is equivalent to minimizing:
%\[
%\min_{A}\,\,\, \frac{1}{2} A^T E^T E A - A^T E^T R.
%\]
\begin{equation}
\label{eq:model_7b}
\min_{A}\,\,\, \frac{1}{2} A^T E^T E A - A^T E^T R.
\end{equation}
To solve this minimization problem, we employ the gradient descent algorithm. Let $f(A) = \frac{1}{2} A^T E^T E A - A^T E^T R$. The negative gradient of this function is given by:
\begin{equation}
\label{eq:model_7c}
-\nabla f(A) = E^T R - E^T E A.
\end{equation}
Gradient descent starts with an initial estimate $A^{(0)}$ and iteratively moves in the direction of the negative gradient to minimize the function. The update rule is:
\begin{equation}
\label{eq:model_7d}
 A^{(k+1)} = A^{(k)} + \alpha_k r_k, \quad k = 0, 1, 2, \dots,
\end{equation}
where $\alpha_k$ denotes the step size at the $k$-th iteration, and $r_k = E^T R - E^T E A^{(k)}$. From gradient descent theory, the optimal step size $\alpha_k$ can be determined as:
\begin{equation}
\label{eq:model_7e}
\alpha_k = \frac{||r_k^T||_2^2}{r_k^T E^T E r_k}.
\end{equation}
We repeat the iterative updates until either a maximum number of iterations $T$ is reached or the norm of the gradient falls below a pre-defined threshold. Compared to solving the linear system directly, gradient descent is computationally efficient, relying only on matrix-vector products. Specifically, when $R$ is $m\times m$ positive definite  matrix, each iteration of gradient descent costs $O(m^2)$. A critical part of the algorithm is the initialization $A^{(0)}$. Running the algorithm for a few iterations provides reasonable solutions with ease of implementation, minimal computational and memory overhead.

\subsection{Model Statistics}

The inverse in Equation~\ref{eq:model_7} implies that $\rho$ is directly proportional to $E[\mathbf{r}(t) \mathbf{r}(t-1)]$ and inversely proportional to $E[\mathbf{r}^2(t-1)]$; similarly, $\alpha_x$ is directly proportional to $E[\mathbf{r}(t)\mathbf{a}_x(t)]$, but inversely proportional to $E[\mathbf{a}^2_x(t)]$, etc. This is logical, as our confidence in the estimation increases if the correlation between the parameter to be estimated and the measurement increases, but decreases if the variance of the measurement increases. Once the optimal coefficients are determined, it is possible to determine the mean and the variance of the error:
\begin{align}
\label{eq:model_8}
    E \left [\mathbf{e} (t) \right ]    & = E \left [ \mathbf{r}(t) - \hat{\mathbf{r}}(t)\right ] \\ \nonumber
                                    & =  E \left [ \mathbf{r}(t) - \left ( \rho(t) \mathbf{r}(t - 1) + \alpha (t)\mathbf{a}(t) \right ) \right ] \\ \nonumber
                                    & = E \left [ \mathbf{r}(t) \right] - \rho (t) E \left [ \mathbf{r}(t-1) \right ] - \alpha (t) E \left [\mathbf{a} (t) \right ]. 
\end{align}
In Equation~\ref{eq:model_8}, the acceleration is once again expressed in a compact form. In the absence of any acceleration, the difference between $\mathbf{r}(t)$ and $\mathbf{r}(t-1)$ is due to a measurement error only and, therefore, the estimation error is 0. From Equation~\ref{eq:model_4_orthogonal}, it is clear that the error is orthogonal to the measurement (data) (ref. also to \cite{Liu2023, guo2011estimation}, and \cite{papoulis2002probability}, pp. 261-269); hence, the optimal mean square error is:

\begin{small}
\begin{align} 
\label{eq:model_9}
 E \left [ \mathbf{e}^2 (t)\right ] & =  E \left [ \left ( \mathbf{r}(t) - \hat{\mathbf{r}}(t) \right ) \mathbf{r}(t) \right ] \\ \nonumber 
     & = E \left [ \mathbf{r}^2(t) \right ] - \rho (t) E \left [ \mathbf{r}(t) \mathbf{r} (t-1) \right]  + 
 \alpha (t) E \left [ \mathbf{r}(t)\mathbf{a}(t) \right ].
\end{align}
\end{small}
One of the most significant consequences of Equations~\ref{eq:model_8} and \ref{eq:model_9} is that the conditional probability density function of the change in the received power can be described in terms of them:

\begin{small}
\begin{align}
    \label{eq:model_10}
    f \left ( r(t)|r(t-1),a(t) \right )  =   \nonumber \\
    \frac{1}{\sqrt{2 \pi P}}  \exp \left \{- \frac{\left ( r(t) - \rho(t) r(t-1) - \alpha(t) a(t) \right)^2}{P} \right \},
\end{align}
\end{small}
where $P = E \left [ \mathbf{e}^2(t) \right ]$ as in Equation~\ref{eq:model_9}. Equation~\ref{eq:model_10} presupposes that both $\mathbf{r}(t-1)$ and $\mathbf{a}(t)$ are normally distributed, which they are, as we already demonstrated in Figs.~and \ref{fig:histogram_cc1200} and \ref{fig:histogram_cc2538}.