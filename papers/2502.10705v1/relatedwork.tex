\section{Related Work}
\subsection{Collaborative Perception}
% Collaborative perception overcomes inherent limitations in single-agent perception by sharing complementary information among agents \cite{Han2023CollaborativePI}. Some early works can be categorized into early collaboration that shares raw observations \cite{Luo2023EdgeCooperNC} and late collaboration that transmits perception results \cite{Allig2019AlignmentOP}. However, these approaches often fail to strike a balance between communication efficiency and performance \cite{li2021learning}, hindering their practical applications. Recently, the intermediate collaboration paradigm, which operates in a compact feature space, has gained popularity as it offers a better performance-bandwidth trade-off \cite{Han2023CollaborativePI}. 
Collaborative perception overcomes inherent limitations in single-agent perception by sharing complementary information among agents \cite{Han2023CollaborativePI}. Some early works can be categorized into early collaboration that shares raw observations \cite{Zhang2021EMPEM, Luo2023EdgeCooperNC} and late collaboration that transmits perception results \cite{ Miller2020CooperativePA}. However, these approaches often fail to strike a balance between communication efficiency and performance \cite{li2021learning}, hindering their practical applications. Recently, the intermediate collaboration paradigm, which operates in a compact feature space, has gained popularity as it offers a better performance-bandwidth trade-off \cite{Han2023CollaborativePI}.

V2VNet \cite{Wang2020V2VNetVC} represents a milestone in this field, employing a graph neural network to model the dynamic interactions among agents. After that, AttFuse \cite{Xu2021OPV2VAO} introduces the self-attention to aggregate intermediate features from different agents and release a high-quality OPV2V dataset. To alleviate the adverse impacts of pose errors, CoAlign \cite{Lu2022RobustC3} proposes an agent pose correction method and a multi-scale fusion method. To retain the advantages of early collaboration while reducing bandwidth, DiscoNet \cite{li2021learning} and MKD-Cooper \cite{li2023mkd} introduce knowledge distillation to guide the learning of the intermediate collaboration model. Most existing efforts assume that the training data for the collaborative perception model is comparable to the data encountered during deployment. However, this assumption is often deemed impractical in real-world deployment situations.

% V2VNet \cite{Wang2020V2VNetVC} represents a milestone in this field, employing a graph neural network to model the dynamic interactions among agents. After that, AttFuse \cite{Xu2021OPV2VAO} introduces the self-attention to aggregate intermediate features from different agents and release a high-quality OPV2V dataset. To alleviate the adverse impacts of pose errors on collaborative perception, CoAlign \cite{Lu2022RobustC3} proposes an agent pose correction method and a multi-scale fusion method. To retain the advantages of early collaboration while reducing communication bandwidth requirements, DiscoNet \cite{li2021learning} and MKD-Cooper \cite{li2023mkd} introduce knowledge distillation to guide the learning of the intermediate collaboration model. Most existing efforts assume that the training data for the collaborative perception model is comparable to the data encountered during deployment. However, this assumption is often deemed impractical in real-world deployment situations.

So far, only a few works, S2R-ViT \cite{li2023s2r} and DUSA \cite{akong2023dusa}, recognize the potential implications of this assumption not holding. These studies employ a technique called unsupervised domain adaptation, where labeled training data is combined with unlabeled deployment data during the training stage to uncover the distribution of the deployment data. They have the following limitations: (1) It is difficult to determine the deployment data during the training stage; (2) When the deployment data changes significantly, it requires costly re-training of the model from scratch; (3) This discriminative-based method may  distort the learned features \cite{tang2020unsupervised}, thereby affecting the final perceptual performance. In contrast to previous studies that focus solely on simulation-to-reality domain adaptation settings, our research not only addresses the aforementioned limitations but also broadens the applicability to a wider range of scenarios.


\subsection{Parameter-Efficient Fine-Tuning}
In natural language processing and computer vision, Parameter-Efficient Fine-Tuning (PEFT in short) offers an efficient alternative to full-parameter fine-tuning for specific tasks \cite{xin2024parameter}. The core idea behind PEFT is to achieve comparable performance to full-parameter fine-tuning by updating only a portion of the existing model's or newly added parameters. Inspired by the manually defined prompt \cite{petroni2019language}, the learnable prompt adjusts the model by adding a few parameterized input blocks into the input layer of the trained Transformer model \cite{jia2022visual, dong2022lpt, nie2023pro}. Some subsequent works have explored adjusting other elements of the Transformer architecture, such as attention block \cite{li2021prefix}. Another mainstream research is adapter, which inserts subnetworks containing bottlenecks within the backbone network to fine-tune the output of each layer \cite{houlsby2019parameter, chen2022adaptformer, xin2024vmt}. However, these works are all targeted at image or language models, which are not compatible with collaborative perception. In contrast, we introduce PEFT as a lightweight plugin that enables fast adaptation by encoding the inconsistency between the deployment and training data in collaborative perception.

In the context of collaborative perception, there is also a relevant method known as MACP \cite{ma2024macp}, which introduces the concept of PEFT to transfer a single-agent perception model to multi-agent perception. Different  from MACP, our goal is to achieve fast adaptation to new deployed scenario of collaborative perception with low cost by leveraging the complementary interaction of the proposed Collaboration Adapter and Agent Prompt

\begin{figure*}[t]
\centering
\includegraphics[width=1.9\columnwidth]{fig2_overall.pdf} % Reduce the figure size so that it is slightly narrower than the column.
\caption{The overall architecture of CoPEFT. It involves standard components in intermediate collaboration augmented with two lightweight elements: a Collaboration Adapter and an Agent Prompt. (a) The Collaboration Adapter, guided by several collaborative perception priors, adapts the feature maps from a macro-level perspective for new data. (b) The Agent Prompt offers fine-grained environmental information from a micro-level perspective, which can be conceptualized as the insertion of a virtual agent to further assist in adapting feature maps. By updating only the parameters of the Collaboration Adapter, Agent Prompt, and Decoder Network, CoPEFT effectively realizes the dynamic combination of general, specific, and environmental knowledge for fast adaptation.}
\label{fig:overall_framework}
\end{figure*}