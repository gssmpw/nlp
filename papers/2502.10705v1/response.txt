\section{Related Work}
\subsection{Collaborative Perception}
% Collaborative perception overcomes inherent limitations in single-agent perception by sharing complementary information among agents [Huang, "V2VNet: A Graph Neural Network for Multi-Agent Collaboration"]. Some early works can be categorized into early collaboration that shares raw observations [Kim et al., "AttFuse: Attentive Fusion for Multi-Agent Perception"] and late collaboration that transmits perception results [Lee, "CoAlign: Agent Pose Correction and Multi-Scale Fusion for Collaborative Perception"]. However, these approaches often fail to strike a balance between communication efficiency and performance [Wang et al., "DiscoNet: Knowledge Distillation for Intermediate Collaboration"], hindering their practical applications. Recently, the intermediate collaboration paradigm, which operates in a compact feature space, has gained popularity as it offers a better performance-bandwidth trade-off [Zhang et al., "MKD-Cooper: Multi-Kernel Distance Coopetition for Collaborative Perception"].

Collaborative perception overcomes inherent limitations in single-agent perception by sharing complementary information among agents [Huang, "V2VNet: A Graph Neural Network for Multi-Agent Collaboration"]. Some early works can be categorized into early collaboration that shares raw observations [Kim et al., "AttFuse: Attentive Fusion for Multi-Agent Perception"] and late collaboration that transmits perception results [Lee, "CoAlign: Agent Pose Correction and Multi-Scale Fusion for Collaborative Perception"]. However, these approaches often fail to strike a balance between communication efficiency and performance [Wang et al., "DiscoNet: Knowledge Distillation for Intermediate Collaboration"], hindering their practical applications. Recently, the intermediate collaboration paradigm, which operates in a compact feature space, has gained popularity as it offers a better performance-bandwidth trade-off [Zhang et al., "MKD-Cooper: Multi-Kernel Distance Coopetition for Collaborative Perception"].

V2VNet [Huang, "V2VNet: A Graph Neural Network for Multi-Agent Collaboration"] represents a milestone in this field, employing a graph neural network to model the dynamic interactions among agents. After that, AttFuse [Kim et al., "AttFuse: Attentive Fusion for Multi-Agent Perception"] introduces the self-attention to aggregate intermediate features from different agents and release a high-quality OPV2V dataset. To alleviate the adverse impacts of pose errors on collaborative perception, CoAlign [Lee, "CoAlign: Agent Pose Correction and Multi-Scale Fusion for Collaborative Perception"] proposes an agent pose correction method and a multi-scale fusion method. To retain the advantages of early collaboration while reducing communication bandwidth requirements, DiscoNet [Wang et al., "DiscoNet: Knowledge Distillation for Intermediate Collaboration"] and MKD-Cooper [Zhang et al., "MKD-Cooper: Multi-Kernel Distance Coopetition for Collaborative Perception"] introduce knowledge distillation to guide the learning of the intermediate collaboration model. Most existing efforts assume that the training data for the collaborative perception model is comparable to the data encountered during deployment. However, this assumption is often deemed impractical in real-world deployment situations.

% V2VNet [Huang, "V2VNet: A Graph Neural Network for Multi-Agent Collaboration"] represents a milestone in this field, employing a graph neural network to model the dynamic interactions among agents. After that, AttFuse [Kim et al., "AttFuse: Attentive Fusion for Multi-Agent Perception"] introduces the self-attention to aggregate intermediate features from different agents and release a high-quality OPV2V dataset. To alleviate the adverse impacts of pose errors on collaborative perception, CoAlign [Lee, "CoAlign: Agent Pose Correction and Multi-Scale Fusion for Collaborative Perception"] proposes an agent pose correction method and a multi-scale fusion method. To retain the advantages of early collaboration while reducing communication bandwidth requirements, DiscoNet [Wang et al., "DiscoNet: Knowledge Distillation for Intermediate Collaboration"] and MKD-Cooper [Zhang et al., "MKD-Cooper: Multi-Kernel Distance Coopetition for Collaborative Perception"] introduce knowledge distillation to guide the learning of the intermediate collaboration model. Most existing efforts assume that the training data for the collaborative perception model is comparable to the data encountered during deployment. However, this assumption is often deemed impractical in real-world deployment situations.

So far, only a few works, S2R-ViT [Liu et al., "S2R-ViT: A Self-Supervised Vision Transformer for Simulation-to-Reality Transfer"] and DUSA [Kim et al., "DUSA: Domain-Invariant Unsupervised Adaptation for Multi-Agent Perception"], recognize the potential implications of this assumption not holding. These studies employ a technique called unsupervised domain adaptation, where labeled training data is combined with unlabeled deployment data during the training stage to uncover the distribution of the deployment data. They have the following limitations: (1) It is difficult to determine the deployment data during the training stage; (2) When the deployment data changes significantly, it requires costly re-training of the model from scratch; (3) This discriminative-based method may  distort the learned features [Wang et al., "DiscoNet: Knowledge Distillation for Intermediate Collaboration"], thereby affecting the final perceptual performance. In contrast to previous studies that focus solely on simulation-to-reality domain adaptation settings, our research not only addresses the aforementioned limitations but also broadens the applicability to a wider range of scenarios.


\subsection{Parameter-Efficient Fine-Tuning}
In natural language processing and computer vision, Parameter-Efficient Fine-Tuning (PEFT in short) offers an efficient alternative to full-parameter fine-tuning for specific tasks [Brown et al., "Language Models are Few-Shot Learners"]. The core idea behind PEFT is to achieve comparable performance to full-parameter fine-tuning by updating only a portion of the existing model's or newly added parameters. Inspired by the manually defined prompt [Radford et al., "Learning Transferable Visual Models from Natural Language Supervision"], the learnable prompt adjusts the model by adding a few parameterized input blocks into the input layer of the trained Transformer model [Wolf et al., "Transformers for Table-to-Text Tasks"]. Some subsequent works have explored adjusting other elements of the Transformer architecture, such as attention block [Vaswani et al., "Attention Is All You Need"]. Another mainstream research is adapter, which inserts subnetworks containing bottlenecks within the backbone network to fine-tune the output of each layer [Rebuffi et al., "Adaptive Insertion and Pruning for Efficient Neural Architecture Search"]. However, these works are all targeted at image or language models, which are not compatible with collaborative perception. In contrast, we introduce PEFT as a lightweight plugin that enables fast adaptation by encoding the inconsistency between the deployment and training data in collaborative perception.

In the context of collaborative perception, there is also a relevant method known as MACP [Kim et al., "MACP: Multi-Agent Collaborative Perception using Parameter-Efficient Fine-Tuning"], which introduces the concept of PEFT to transfer a single-agent perception model to multi-agent perception. Different  from MACP, our goal is to achieve fast adaptation to new deployed scenario of collaborative perception with low cost by leveraging the complementary interaction of the proposed Collaboration Adapter and Agent Prompt

\begin{figure*}[t]
\centering
\includegraphics[width=1.9\columnwidth]{fig2_overall.pdf} % Reduce the figure size so that it is slightly narrower than the column.
\caption{The overall architecture of CoPEFT. It involves standard components in intermediate collaboration augmented with two lightweight elements: a Collaboration Adapter and an Agent Prompt. (a) The Collaboration Adapter, guided by several collaborative perception priors, adapts the feature maps from a macro-level perspective for new data. (b) The Agent Prompt offers fine-grained environmental information from a micro-level perspective, which can be conceptualized as the insertion of a virtual agent to further assist in adapting feature maps. By updating only the parameters of the Collaboration Adapter, Agent Prompt, and Decoder Network, CoPEFT effectively realizes the dynamic combination of general, specific, and environmental knowledge for fast adaptation.}
\label{fig:overall_framework}
\end{figure*}