\section{Introduction}

Open-source pre-trained large language models such as Llama~\citep{touvron2023llama} and Qwen~\citep{bai2023qwen} have demonstrated exceptional capabilities. Through supervised fine-tuning, these models can be adapted to various downstream tasks such as code generation~\citep{roziere2023code}, mathematical problem solving~\citep{yang2024qwen2-math}, and agents~\citep{hong2024cogagent}. When the model with parameter size $\phi$ and uses FP16/BF16 mixed-precision training~\citep{micikevicius2017mixed, kalamkar2019bf16} with the Adam optimizer~\citep{kingma2014adam}, the parameters and gradients require $4\phi$ bytes of memory, while the optimizer states require $12\phi$ bytes. Thus, the minimum memory usage, excluding activations, already reaches $16\phi$ bytes. Such high memory demands limit the training of large language models under constrained resources. To reduce memory usage, Low-Rank Adaptation (LoRA)~\citep{hu2021lora} decomposes the weight matrix \(W \in \mathbb
{R}^{m \times n}\) into \(W = W_0 + \Delta W = W_0 + sAB\), where \(s\) is a scaling factor, and \(A \in \mathbb
{R}^{m\times r}, B \in \mathbb
{R}^{r \times n }, r \ll \text{min}(m,n)\), as shown in Figure~\ref{fig:GoRA}(a). During training, LoRA only updates the low-rank weights \(A\) and \(B\), keeping \(W_0\) unchanged, thereby significantly reducing the memory footprint of optimizer states. Although LoRA performs well on simpler tasks when applied to pre-trained large language models, its performance on more challenging tasks such as mathematics and code still lags behind full fine-tuning~\citep{biderman2024lora_learn_less, ghosh2024closer}.

A critical factor in LoRA is its rank. \citet{kalajdzievski2023rslora} demonstrate that increasing the rank of LoRA can significantly improve performance when paired with an appropriate scaling factor. However, a direct increase in rank leads to a substantial rise in the number of trainable parameters and memory consumption and imposes practical constraints on rank selection.
To address this limitation, several studies~\citep{lialin2023relora, ren2024melora} propose to ensemble multiple low-rank subspaces, allowing for rank increases without proportionally increasing the number of trainable parameters. Nevertheless, these approaches often come at the expense of usability due to their intrusion to architecture or training process. Another promising line of research explores assigning different ranks to weights based on their importance. For example, AdaLoRA~\citep{zhang2023adalora} dynamically allocates ranks by quantifying the importance of weight vectors during training and masking less significant ones, as illustrated in Figure~\ref{fig:GoRA}(b). However, this masking mechanism necessitates a larger parameter space during initialization, increasing the number of trainable parameters and limiting the upper bound of rank allocation.
Consequently, dynamically allocating ranks without significantly increasing the number of trainable parameters remains an open challenge.

Another critical factor in LoRA is its initialization strategy. Vanilla LoRA initializes the matrix \(A_0\) using a normal distribution (In PEFT library, \(A_0\) is initialized with Kaiming distribution~\citep{he2015delving}) and the matrix \(B_0\) with zeros. This initialization method ensures that the initial weights \(W + A_0B_0 = W\), maintaining training stability. Besides, zero initialization is not the only option: When \(A_0B_0\) is nonzero, subtracting \(A_0B_0\) from \(W\) also ensures stability, a process called full weight reset. Existing nonzero initialization methods can be categorized into experience-driven and data-driven initialization. In experience-driven initialization, ~\citet{meng2024pissa, wang2024milora} employ decomposition techniques such as Singular Value Decomposition (SVD) to capture specific features of pre-trained weights. However, these methods are inherently task-agnostic, which limits their generalizability across diverse tasks. In contrast, data-driven initialization incorporates task-specific information. For example, LoRA-GA~\citep{wang2024lora-ga} computes the singular values of gradients to initialize LoRA matrices, minimizing the difference between LoRA and full fine-tuning.
% in the first step. 
However, as shown in Figure~\ref{fig:GoRA}(c), existing nonzero initialization methods require resetting the full weights, creating a gap between training and inference. Thus, designing a better nonzero initialization method without resetting full weights remains an open problem.

Given the challenges of dynamic rank allocation and nonzero initialization , we turn to full gradients, which are crucial for assessing weight importance and closely related to LoRA's optimization process~\citep{hao2024flora, zhao2024galore}. As shown in Figure~\ref{fig:GoRA}(d), we propose a method called GoRA. Specifically, before training, we compute and save the full gradients \(G\) of weights \(W\) on a subset of training samples, using these gradients to assess the importance of \(W\). Given an initial rank \(r_0\), we calculate the total number of trainable parameters and normalize the importance of each weight. Based on the normalized importance, we allocate new trainable parameters and corresponding ranks of low-rank adapters to each weight, achieving dynamic rank allocation without significantly increasing the number of trainable parameters and allowing for a higher rank allocation upper bound as shown in Table~\ref{tab: ablation1}. In GoRA's initialization, we maintain LoRA's normal distribution initialization for matrix \(A\), while matrix \(B\) is initialized using \(-(A^TA)^{-1}A^TG\). This initialization ensures that the LoRA computation \( -A(A^TA)^{-1}A^TG \approx -G \) approximates a gradient descent step, enabling data-driven initialization while maintaining training stability without resetting full weights. Our key contributions are summarized as follows:
\begin{itemize}
    \item We conduct an in-depth investigation into LoRA's rank allocation and initialization strategy. Our study uncovers the limitations of existing methods. We propose GoRA, which, for the first time, achieves adaptive rank allocation and initialization without compromising the usability and efficiency.
    
    \item We use the gradients of weights to assess their importance and allocate ranks. We then initialize low-rank weights using the pseudo-inverse compressed gradients, enhancing 
    performance while ensuring training stability.
    
    \item GoRA consistently outperforms LoRA on multiple tasks and surpasses full fine-tuning in certain scenarios. It slightly outperforms full fine-tuning on GLUE and, in high-rank settings, exceeds it on GSM8k and HumanEval by margins of 2.05 and 0.40, respectively.
\end{itemize}


    % \item Extensive experiments demonstrate the effectiveness of our method. Compared to the original LoRA, GoRA improves the performance of T5-Base~\citep{raffel2020t5} on the GLUE~\citep{wang2018glue} benchmark by an average of 5.89\%. For Llama-3.1-8B-Base~\citep{dubey2024llama3}, GoRA achieves performance improvements of 0.36, 5.9, and 6.3 on MTBench~\citep{zheng2023mtbench}, GSM8K~\citep{cobbe2021gsm}, and HumanEval~\citep{chen2021human-eval}, respectively.