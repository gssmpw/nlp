\section{Proofs}

\subsection{Proof of optimal approximation of \(G\) given \(A\).}
\label{inverse}

Let \( G \) be an \( m \times n \) matrix, and \( A \) be an \( m \times r \) matrix where \( r \ll \text{min}(m,n) \). We aim to derive the projection formula that minimizes the Frobenius norm of the error \( \| G - \hat{G} \|_F \), where \( \hat{G} \) is the optimal approximation of \( G \) in the column space of \( A \), denoted as \( \text{Col}(A) \).

The best approximation \( \hat{G} \) lies in \( \text{Col}(A) \), so we can express \( \hat{G} \) as:
\[
\hat{G} = A B,
\]
where \( B \) is an \( r \times n \) matrix of coefficients to be determined. Our goal is to find \( B \) such that the error \( \| G - \hat{G} \|_F \) is minimized.

The error matrix is given by:
\[
E = G - \hat{G} = G - A B.
\]
To minimize \( \| E \|_F^2 \), we take the derivative of \( \| E \|_F^2 \) with respect to \( B \) and set it to zero. Expanding \( \| E \|_F^2 \), we have:
\[
\| E \|_F^2 = \text{Tr}\left((G - A B)^T (G - A B)\right),
\]
where \( \text{Tr} \) represents the trace of a matrix.

Expanding this expression:
\[
\| E \|_F^2 = \text{Tr}(G^T G) - 2 \text{Tr}(B^T A^T G) + \text{Tr}(B^T A^T A B).
\]

Taking the derivative with respect to \( B \) and setting it to zero:
\[
-2 A^T G + 2 A^T A B = 0.
\]

Simplifying:
\[
A^T A B = A^T G.
\]

Assuming \( A^T A \) is invertible, we solve for \( B \):
\[
B = (A^T A)^{-1} A^T G.
\]

Substituting \( B \) into \( \hat{G} = A B \), we get:
\[
\hat{G} = A (A^T A)^{-1} A^T G.
\]

Thus, the best approximation \( \hat{G} \) is:
\[
\boxed{\hat{G} = A(A^T A)^{-1} A^T G.}
\]

The matrix \( \hat{G} = A(A^T A)^{-1} A^T G \) is the projection of \( G \) onto the column space of \( A \), and it minimizes the Frobenius norm of the error \( \| G - \hat{G} \|_F \).


\subsection{Proof of Expectation of Frobenius Norm of AB.}
\label{scale}

Let \( A \) be a random Gaussian matrix of size \( m \times r \), where each element of \( A \) is sampled independently from \( \mathcal{N}(0, 1) \). Let \( G \) be a random Gaussian matrix of size \( m \times n \), where each element of \( G \) is also sampled independently from \( \mathcal{N}(0, 1) \). Define:
\[
B = (A^\top A)^{-1} A^\top G,
\]
and consider the product:
\[
AB = A (A^\top A)^{-1} A^\top G.
\]
The goal is to compute the expected Frobenius norm \( \mathbb{E}[\|AB\|_F] \), where the Frobenius norm is defined as:
\[
\|AB\|_F = \sqrt{\sum_{i,j} (AB)_{ij}^2}.
\]

First, observe that \( AB \) can be rewritten as:
\[
AB = A (A^\top A)^{-1} A^\top G.
\]
Let \( P = A (A^\top A)^{-1} A^\top \). Note that \( P \) is a projection matrix onto the column space of \( A \), and thus \( P \) satisfies:
\[
P^2 = P, \quad P^\top = P, \quad \text{and} \quad \text{rank}(P) = r.
\]
Substituting \( P \) into the expression for \( AB \), we have:
\[
AB = P G.
\]

The Frobenius norm of \( AB \) is given by:
\[
\|AB\|_F^2 = \|PG\|_F^2 = \text{Tr}((PG)(PG)^\top).
\]
Since \( (PG)^\top = G^\top P \), this becomes:
\[
\|AB\|_F^2 = \text{Tr}(P G G^\top P).
\]

The matrix \( G G^\top \) is a \( m \times m \) random Wishart matrix. When \( G \) is a standard Gaussian matrix of size \( m \times n \), the expected value of \( G G^\top \) is:
\[
\mathbb{E}[G G^\top] = n \cdot I_m,
\]
where \( I_m \) is the \( m \times m \) identity matrix. Substituting this result into the expression for \( \|AB\|_F^2 \), we get:
\[
\mathbb{E}[\|AB\|_F^2] = \mathbb{E}[\text{Tr}(P G G^\top P)] = \text{Tr}(P \mathbb{E}[G G^\top] P).
\]
Using \( \mathbb{E}[G G^\top] = n \cdot I_m \), this simplifies to:
\[
\mathbb{E}[\|AB\|_F^2] = \text{Tr}(P (n \cdot I_m) P) = n \cdot \text{Tr}(P^2).
\]
Since \( P^2 = P \), we have:
\[
\mathbb{E}[\|AB\|_F^2] = n \cdot \text{Tr}(P).
\]

The trace of \( P \) is equal to its rank, which is the dimension of the column space of \( A \). Since \( A \) is a \( m \times r \) matrix, we have:
\[
\text{Tr}(P) = r.
\]
Thus:
\[
\mathbb{E}[\|AB\|_F^2] = n \cdot r.
\]

Taking the square root, the expected Frobenius norm of \( AB \) is:
\[
\boxed{\mathbb{E}[\|AB\|_F] = \sqrt{n \cdot r}.}
\]

\section{Implementation Details}

\subsection{Implementation Details for Baseline Methods}

Several baseline methods introduce tunable hyperparameters compared with vanilla LoRA~\citep{hu2021lora}. To ensure a fair comparison, we adopt the optimal settings reported in the original papers whenever possible. Specifically, for LoRA+~\cite{hayou2024lora+}, we set the learning rate ratio of matrices A and B to 16. For LoRA-GA~\citep{wang2024lora-ga}, we use the "stable output" scaling method and reinitialize the full weights during initialization. For AdaLoRA~\citep{zhang2023adalora}, the initial rank is set to 12, the final rank to 8, with \(t_i = 150\) and \(t_f=900\). For PiSSA~\citep{meng2024pissa}, the number of iterations for SVD is set to 64.

\subsection{Implementation Details for GoRA}
\label{tdlora implementation}
For all experiments, except for the model trained on MetaMathQA~\citep{yu2023metamath}, we set the scaling factor \(\gamma\) to \(5e-2\). For the model trained on MetaMathQA, \(\gamma\) is set to 
\(8e-2\).  To address the imbalance in GoRA's matrices 
\(A\) and \(B\), we set the learning rate of matrix 
\(B\) to be 16 times that of matrix \(A\) Throughout the experiments, the maximum rank was defined as four times the average rank, and the minimum rank was set to half the average rank. We employed a 64-step accumulated gradient approach for GoRA. In the ablation studies, we adhered to the same hyperparameter settings as in the main experiments, unless otherwise specified.

\subsection{Training Environments}
For natural language understanding tasks reported in section~\ref{t5 exp}, we conduct our experiments using the Huggingface Transformers framework for model and trainer implementation on a single RTX 4090 GPU. In contrast, for natural language generation tasks reported in section~\ref{llama exp} and section~\ref{ablations}, we utilize the DeepSpeed ZeRO2~\citep{rajbhandari2020zero} data parallel framework and FlashAttention-2~\citep{dao2023flashattention} mechanism, leveraging the power of 8 RTX 4090 GPUs in a Slurm cluster. All codes of GoRA and baseline methods are implemented in PyTorch.

% \section{Additional Experiment Results}


\section{Time and Memory Cost}

\begin{table}[ht!]
\centering
\caption{Time and memory cost comparison between GoRA and baseline methods. We report the number of trainable parameters(\#Params), Memory cost recorded by DeepSpeed(Memory), training time(Time@train) and initialization time(Time@init).}
\label{tab: cost}
\small
\resizebox{0.7\columnwidth}{!}{
\begin{tabular}{l|cccc}
\hline
Method & \textbf{\#Params} & \textbf{Memory} & \textbf{Time@train} & \textbf{Time@init}\\ 
\hline
LoRA  &    6.82M & 19.75GB & 5h50min & -     \\
AdaLoRA &  10.24M & 19.93GB & 8h49min & -     \\
GoRA  &    7.00M & 19.75GB & 5h48min & 4min  \\
\hline
\end{tabular}}
\end{table}

To evaluate the additional computational overhead introduced by our method, we conducted comprehensive benchmarks to measure both time and memory consumption. Specifically, we trained the Llama-3.1-8B-Base model on a 100K subset of MetaMathQA using a single RTX 4090 GPU, with a maximum sequence length of 512. As detailed in Table~\ref{tab: cost}, GoRA introduces only a 2.6\% increase in trainable parameters compared to vanilla LoRA, whereas AdaLoRA~\citep{zhang2023adalora} results in a 50\% increase in the number of trainable parameters. Notably, the number of trainable parameters in GoRA does not grow linearly with the increase of rank allocation upper bound, demonstrating its parameter efficiency. Consequently, GoRA exhibits nearly identical memory usage to LoRA and incurs no additional training time. The initialization time (4min) of GoRA is negligible compared with training time (5h48min), and it is worth emphasizing that this initialization time is solely determined by the number of gradient computation steps (64 steps in this case) before training, remaining constant regardless of the total training duration.



\section{Limitations And Future Works}
In this study, we have demonstrated that GoRA outperforms baseline low-rank adaptation methods and achieves performance comparable to full fine-tuning. However, our evaluation has not yet extended to larger models and more extensive datasets. We hypothesize that for larger models, such as Llama-3.1-70B~\citep{dubey2024llama3}, GoRA could more effectively leverage the pre-trained knowledge inherent in these models. Additionally, while this research primarily focuses on language models and natural language processing tasks, there is potential to generalize GoRA to a broader range of model types and tasks such as visual language models and visual question answering.

Another limitation of this study is that the initialization of matrix \(A\) is not restricted to random initialization. Employing alternative methods, such as extracting distinguishing features from pre-trained weights to initialize matrix 
\(A\), could potentially enhance performance, as it would combine the benefits of both experience-driven and data-driven initialization approaches. Furthermore, it is worth noting that GoRA demonstrates theoretical compatibility with other LoRA variants, such as DoRA~\citep{liu2024dora}. These promising avenues remain to be explored in future research endeavors.