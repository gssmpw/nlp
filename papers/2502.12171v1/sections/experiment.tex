\section{Experiments}
We conducted extensive experiments on GoRA and baseline methods for both natural language understanding and natural language generation tasks in section~\ref{t5 exp} and section~\ref{llama exp} respectively. For natural language understanding tasks, we trained T5-Base~\citep{raffel2020t5} on five sub-datasets of GLUE~\citep{wang2018glue}: MNLI, SST-2, CoLA, QNLI, and MRPC, and reported accuracy on the corresponding validation sets. For natural language generation tasks, we trained the Llama-3.1-8B-Base~\citep{dubey2024llama3} model on three vital capabilities: chat, mathematics, and coding. Performance was evaluated using the test sets of MTBench~\citep{zheng2023mtbench}, GSM8K~\citep{cobbe2021gsm}, and HumanEval~\citep{chen2021human-eval}. All experiments were conducted over a single epoch training and replicated using three distinct random seeds, with the final results presented as mean values accompanied by their standard deviations.

\textbf{Baseline Methods}: We compared GoRA with several baseline methods to demonstrate the effectiveness of our approach:
\begin{enumerate}
    \renewcommand{\labelenumi}{\alph{enumi}.}
    \item \textbf{Full}: Trains all parameters in the layers that need to be trained. Requiring maximum memory occupation.
    
    \item \textbf{LoRA}~\citep{hu2021lora}: Inserts low-rank adapters into the layers that need to be trained. 
    
    \item \textbf{Convergence Optimization Methods for LoRA}
    \begin{itemize}
        \item \textbf{RSLoRA}~\citep{kalajdzievski2023rslora}: Modifies the scaling factor of LoRA from \(\frac{\alpha}{r}\) to \(\frac{\alpha}{\sqrt{r}}\), allowing LoRA's performance to benefit from higher rank.
        \item \textbf{DoRA}~\citep{liu2024dora}: Restricts LoRA to update weights only in the direction, without changing the magnitude of the weights.
        \item \textbf{LoRA+}~\citep{hayou2024lora+}: Based on the imbalance between matrices A and B in LoRA, it uses a relative larger learning rate for matrix B compared to matrix A.
    \end{itemize}

    \item \textbf{Initialization Optimization Methods for LoRA}
    \begin{itemize}
        \item \textbf{OLoRA}~\citep{buyukakyuz2024olora}: Initializes LoRA weights using the QR decomposition of the full weights.
        \item \textbf{PiSSA}~\citep{meng2024pissa}: Initializes LoRA weights using the significant singular vectors from the SVD decomposition of the full weights.
        \item \textbf{LoRA-GA}~\citep{wang2024lora-ga}: Initializes LoRA weights using the significant singular vectors from the SVD decomposition of gradients.
    \end{itemize}
    
    \item \textbf{Adaptive Methods for LoRA}
    \begin{itemize}
        \item \textbf{AdaLoRA}~\citep{zhang2023adalora}: Approximates the form of the low-rank adapter to SVD decomposition, achieving dynamic rank allocation by masking singular values. It also adds an orthogonal penalty term to the loss to ensure orthogonality in the low-rank adapter's features.
    \end{itemize}
\end{enumerate}

\subsection{Experiment Results on Natural Language Understanding Tasks}
\label{t5 exp}

\begin{table*}[ht!]
\centering
\caption{Performance of fine-tuning T5-Base on 5 sub-tasks of the GLUE benchmark.}
\label{tab:t5}
\begin{tabular}{l|ccccc|c}
\hline
Method & \textbf{MNLI}       & \textbf{SST-2}     & \textbf{CoLA}      & \textbf{QNLI}      & \textbf{MRPC}      & \textbf{Average} \\ 
\hline
Full   & 86.33$\pm$0.00      & 94.75$\pm$0.21     & 80.70$\pm$0.24     & 93.19$\pm$0.22     & 84.56$\pm$0.73     & 87.91            \\ 
LoRA  & 85.30$\pm$0.04      & 94.04$\pm$0.11     & 69.35$\pm$0.05     & 92.96$\pm$0.09     & 68.38$\pm$0.01     & 82.08            \\ 
\hline
\rowcolor{gray!20}\multicolumn{7}{c}{\textit{Convergence Optimization Methods for LoRA}}    \\
RSLoRA & 85.73$\pm$0.10      & 94.19$\pm$0.23     & 72.32$\pm$1.12     & 93.12$\pm$0.09     & 52.86$\pm$2.27     & 79.64            \\ 
DoRA   & 85.67$\pm$0.09      & 94.04$\pm$0.53     & 72.04$\pm$0.94     & 93.04$\pm$0.06     & 68.08$\pm$0.51     & 82.57            \\ 
LoRA+  & 85.81$\pm$0.09      & 93.85$\pm$0.24     & 77.53$\pm$0.20     & 93.14$\pm$0.03     & 74.43$\pm$1.39     & 84.95            \\ 
\hline
\rowcolor{gray!20}\multicolumn{7}{c}{\textit{Initialization Optimization Methods for LoRA}}    \\
PiSSA  & 85.75$\pm$0.07      & 94.07$\pm$0.06     & 74.27$\pm$0.39     & 93.15$\pm$0.14     & 76.31$\pm$0.51     & 84.71            \\ 
LoRA-GA & 85.70$\pm$0.09      & 94.11$\pm$0.18     & \textbf{80.57$\pm$0.20}     & 93.18$\pm$0.06     & 85.29$\pm$0.24     & 87.77            \\ 
\hline
\rowcolor{gray!20}\multicolumn{7}{c}{\textit{Adaptive Methods for LoRA}}    \\
AdaLoRA & 85.45$\pm$0.11      & 93.69$\pm$0.20     & 69.16$\pm$0.24     & 91.66$\pm$0.05     & 68.14$\pm$0.28     & 81.62            \\ 
\rowcolor{green!20}
GoRA & \textbf{85.91$\pm$0.02}      & \textbf{94.68$\pm$0.43}     & 79.86$\pm$0.35     & \textbf{93.27$\pm$0.08}     & \textbf{86.10$\pm$0.20}     & \textbf{87.96}            \\ 
\hline
\end{tabular}
\end{table*}

\textbf{Settings}: We adopted the baseline performances reported in LoRA-GA\citep{wang2024lora-ga}. To ensure fairness in comparison, our experimental setup was consistent with theirs: we used the Adam\citep{kingma2014adam} optimizer with Beta1 set to 0.9, Beta2 to 0.999, weight decay to 0, batch size to 32, and a cosine decay learning rate scheduler with a warmup ratio of 0.03. Additionally, we trained all linear layers in the model except the language head, with a peak learning rate of 1e-4, a maximum sequence length of 128, and FP32 precision. 

\textbf{Results}: Table \ref{tab:t5} provides a detailed comparison of GoRA's performance against multiple baseline methods on five sub-tasks of the GLUE benchmark. It is evident that GoRA achieved the best performance on four datasets: MNLI, SST-2, QNLI, and MRPC, demonstrating its strong adaptability and generalization capabilities. Although GoRA slightly underperformed LoRA-GA on the CoLA task, the gap was minimal at only 0.71 percentage points, still maintaining a high level of performance. More importantly, GoRA's average score reached 87.96, surpassing all baseline methods and even slightly exceeding the average score of full fine-tuning (Full) at 87.91. This result fully demonstrates that GoRA can maximize model potential while maintaining parameter efficiency, even outperforming full fine-tuning. Additionally, GoRA's performance on the MRPC and QNLI datasets was particularly outstanding, further validating its strong capabilities in small-sample learning and sentence-pair tasks.

\subsection{Experiment Results on Natural Language Generation Tasks}
\label{llama exp}


\textbf{Settings}: We trained the mathematical capability using a 100K subset of the MetamathQA~\citep{yu2023metamath} training set, the coding capability using a 100K subset of the Code-FeedBack~\citep{zheng2024opencodeinterpreter} training set, and the dialogue capability using a 52K subset of the WizardLM~\citep{xu2024wizardlm} training set. For evaluation purposes, we removed all textual descriptions from the Code-FeedBack training labels, retaining only executable code. We used the AdamW optimizer~\citep{loshchilov2017decoupled} with beta1 set to 0.9, beta2 to 0.999, weight decay to 5e-4, batch size to 64, and a cosine decay learning rate scheduler with a warmup ratio of 0.03 and a learning rate decay ratio of 0.1. For all baseline methods and GoRA, we trained every linear layer in the model's attention modules (i.e., wq, wk, wv and wo) with a peak learning rate of 5e-5 and BF16 mixed precision strategy. To ensure measurable results, the learning rate for AdaLoRA was set to 5e-4. For evaluation, mathematical capability was assessed by calculating the accuracy of results extracted using regular expressions; coding capability was evaluated using the PASS@1 score; and chat capability was assessed by averaging scores from 0-10 given by GPT-4o, Gemini-1.5-Pro, and Llama-3.1-70B-Instruct. The prompts used for scoring were consistent with those reported in~\citep{zheng2023mtbench} to ensure result reliability. For more setup information, please refer to Appendix~\ref{tdlora implementation}.

\setlength{\textfloatsep}{5pt}
\begin{table}[ht!]
\centering
\caption{Performance of fine-tuning Llama-3.1-8B-Base on MTBench, GSM8k, and HumanEval. GoRA demonstrates substantial improvements over baseline LoRA variants. In high average rank settings (Rank32 and Rank128), GoRA even surpasses full fine-tuning on GSM8k and HumanEval in a large margin. Unless stated otherwise, the LoRA rank or average rank of GoRA is set to 8.}
\label{tab:results_large}
\resizebox{\columnwidth}{!}{%
\begin{tabular}{l|ccc}
\hline
Method & \textbf{MTBench} &\textbf{GSM8k} & \textbf{HumanEval} \\ 
\hline Full & 5.88$\pm0.23$ &73.69$\pm0.28$ & 51.63$\pm1.27$ \\
LoRA & 6.15$\pm0.02$ & 67.78$\pm 1.25$ & 43.09$\pm0.35$ \\
\hline
RSLoRA & 6.18$\pm0.09$    & 68.36$\pm0.74$ & 45.78$\pm2.80$ \\
DoRA & 6.24$\pm0.12$ & 69.17$\pm1.00$ & 43.70$\pm1.54$ \\
LoRA+ & \textbf{6.35$\pm$0.10} & 71.29$\pm0.93$ & 44.51$\pm2.11$ \\
\hline
OLoRA & 6.13$\pm0.04$ & 68.54$\pm0.42$ & 43.29$\pm2.44$ \\
PiSSA & 6.08$\pm0.09$ & 68.56$\pm1.03$  & 44.10$\pm1.54$ \\
LoRA-GA & 5.99$\pm0.06$ & 71.39$\pm0.90$ & 43.29$\pm0.61$ \\
\hline
AdaLoRA & 6.19$\pm0.16$ & 70.63$\pm0.77$ & 41.46$\pm3.66$ \\
\rowcolor{green!20}
GoRA & 6.34$\pm$0.04 & \textbf{72.91$\pm$0.76} & \textbf{48.98$\pm$2.14} \\
\rowcolor{green!20}
GoRA (Rank32) & 6.21$\pm$0.10 & 75.59$\pm1.04$ & 51.22$\pm1.83$ \\
\rowcolor{green!20}
GoRA (Rank128) & 5.82$\pm$0.31 & 75.74$\pm0.40$ & 52.03$\pm1.41$ \\ 
\hline
\end{tabular}
\vspace{-10pt}
}
\end{table}

\textbf{Results}: Table \ref{tab:results_large} shows the performance of GoRA and baseline methods on the three natural language generation benchmarks. Specifically, GoRA demonstrated exceptional performance on the more challenging HumanEval and GSM8K benchmarks, substantially surpassing all baseline methods. On the GSM8K dataset, GoRA scored 72.91, outperforming LoRA-GA's 71.39 by 1.52 points; on the HumanEval dataset, GoRA achieved 48.98, surpassing RSLoRA's 45.78 by 3.20 points. On MTBench, GoRA slightly underperforms in terms of overall effectiveness, scoring 6.34â€”just 0.01 points lower than LoRA+'s 6.35. Notably, GoRA performed well across different rank allocation settings. For example, GoRA (Rank128) achieved 75.74 and 52.03 on the GSM8K and HumanEval, respectively, surpassing full fine-tuning's 73.69 and 51.63. Even the Rank32 configuration of GoRA, while slightly underperforming Rank128, still outperformed full fine-tuning on GSM8k. The training loss curves of GoRA are depicted in Figure~\ref{fig:loss}.


\begin{figure}[ht!] 
    \centering \includegraphics[width=0.5\textwidth]{images/loss.png}
    \caption{The training loss curved of full fine-tuning, LoRA, GoRA and GoRA (rank128) on MetaMathQA-100K and CodeFeedback-100K datasets. GoRA shows lower start loss and faster convergence speed compared to vanilla LoRA.}
    \label{fig:loss}
\end{figure}