\section{Related Works}

\subsection{Rank of LoRA}
The choice of rank is crucial in LoRA training, with higher ranks yielding better outcomes~\citep{kalajdzievski2023rslora}. However, increasing the rank raises memory usage, making it challenging to train with sufficient ranks on limited hardware. Previous works~\citep{meng2024periodiclora, lialin2023relora} attempt to continuously merge and reinitialize low-rank weights during training to increase the overall rank. However, these methods often require resetting the states of the optimizer and learning rate scheduler during reinitialization to ensure that weight updates take place in distinct low-rank subspaces, significantly increasing training complexity and making the training process unstable. ~\citet{ren2024melora} proposed aggregating multiple mini low-rank adapters diagonally to increase the overall rank. Nevertheless, this approach requires modifying the structure of LoRA, limiting its usability.

At the same time, the importance of different weights during training is not uniform, and a natural idea is to allocate larger ranks to relatively more important weights. ~\citet{zhang2023adalora, hu2023salora} attempted to dynamically mask less important ranks during training to achieve dynamic rank training. However, these methods require allocating larger matrices for low-rank adapters to reserve space for masked ranks, leading to an increase in the number of trainable parameters, which affects their usability and limits the upper bound of rank allocation. IncreLoRA~\citep{zhang2023increlora} introduces an approach that begins with a single rank and incrementally increases the rank during training. This method effectively addresses the challenge of large initial matrices. However, it is not well compatible with distributed training frameworks such as FSDP~\citep{zhao2023pytorch} and ZeRO~\citep{rajbhandari2020zero}, which are critical for large model training.

\subsection{Initialization of LoRA}
Parameter initialization is a crucial concept in deep learning. Initialization methods such as ~\citep{glorot2010understanding} enable stable training of deep neural networks. A good initialization strategy is also essential for LoRA. Beyond zero initialization used by vanilla LoRA, some studies have explored different initialization strategies: PiSSA~\citep{meng2024pissa} performs SVD decomposition on weights and uses the most important singular features to initialize low-rank weights; MiLoRA~\citep{wang2024milora}, in contrast to PiSSA, uses the least important singular features to initialize low-rank weights; similarly, OLoRA~\citep{buyukakyuz2024olora} uses QR decomposition of weights to initialize low-rank weights; EVA~\citep{paischer2024eva} uses SVD decomposition of activations to initialize low-rank weights; and LoRA-GA~\citep{wang2024lora-ga} uses SVD decomposition of gradients to initialize low-rank weights. These methods can improve LoRA's performance to some extent. 

However, due to their non-zero initialization nature, they require subtracting the LoRA initialization results from the full weights to ensure correct forward propagation at the very beginning of training, creating a gap between training and inference. Recomputing the initialization result of LoRA during inference is not feasible in cases involving randomness~\citep{meng2024pissa} or requiring original training data~\citep{wang2024lora-ga, paischer2024eva}. The most straightforward solution is to save not only the LoRA weights but also the reinitialized full weights, but this sacrifices one of LoRA's significant advantages, namely minimal checkpoint storage~\citep{fomenko2024note}. Another approach is to save the initialized LoRA weights and use block matrix multiplication to eliminate the training-inference gap, but this reduces the usability of these methods.

% ~\citep{hu2021lora} pointed out that LoRA can achieve good training results on downstream tasks with very small ranks, and increasing the rank does not effectively enhance the model's performance. However, ~\cite{kalajdzievski2023rslora} argued that this phenomenon is caused by the inappropriate scaling factor used in the original LoRA. Adjusting the scaling factor can significantly improve performance and training stability with larger ranks. Therefore, 


% Due to the stability of QR decomposition, OLoRA can recompose weights during inference to eliminate this gap, while PiSSA's fast SVD has some randomness, making it unable to recompose weights during inference. Moreover, the initialization of LoRA-GA and EVA is driven by training data, making it difficult to eliminate the training-inference gap.