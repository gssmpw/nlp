\section{Method}
In this section, we will reinterpret LoRA from the perspective of a gradient compressor and introduce GoRA's gradient-based dynamic rank allocation method and initialization strategy.

\subsection{View LoRA as a Gradient Compressor}
\label{ss: gradient compressor}
The core idea of LoRA is to fine-tune a model by leveraging the intrinsic low-rank property of a weight matrix \(W \in \mathbb
{R}^{m \times n}\). Specifically, a pair of low-rank matrices \(A \in \mathbb
{R}^{m \times r}\) and \(B \in \mathbb
{R}^{r \times n}\) are initialized alongside \(W\). During training, \(W\) remains frozen, while the model is updated by training the low-rank matrices \(A\) and \(B\), thereby reducing memory usage during training. For any training step \(t\), the update to \(W\) is given by~\eqref{eq: LoRA}, where \(\alpha\) is a tunable hyperparameter that ensures the scale of the LoRA computation depends only on \(\alpha\) and is independent of the rank \(r\):

\begin{eqnarray}
    \label{eq: LoRA}
    W_t = W_0 + \Delta{W} = W_0 + \frac{\alpha}{r}A_tB_t.
\end{eqnarray}

Specifically, given the training loss \(L\), the gradient of the weight matrix \(W\) can be computed as \(\frac{\partial{L}}{\partial{W}}\). Using the chain rule, the gradients of \(A\) and \(B\) are \(\frac{\partial{L}}{\partial{W}}B_t^T\) and \(A_t^T\frac{\partial{L}}{\partial{W}}\), respectively. Given a learning rate \(\eta\), the updates to the weight are as shown in~\eqref{eq: LoRA update}-\eqref{eq: LoRA update end}:

\begin{align}
\label{eq: LoRA update}
A_{t} &= A_{t-1} - \eta \frac{\alpha}{r} \frac{\partial{L_t}}{\partial{W_t}}B_{t-1}^T \\
B_{t} &= B_{t-1} - \eta \frac{\alpha}{r} A_{t-1}^T\frac{\partial{L_t}}{\partial{W_t}} \\
\Delta B &= - \eta \frac{\alpha}{r} \sum_{t=1}^T A_{t-1}^T\frac{\partial{L_t}}{\partial{W_t}} \\
\Delta A &= - \eta \frac{\alpha}{r} \sum_{t=1}^T\frac{\partial{L_t}}{\partial{W_t}}B_{t-1}^T \\
\Delta W &= \frac{\alpha}{r} A_{t}B_{t} - \frac{\alpha}{r} A_0B_0 \nonumber \\
&= \frac{\alpha}{r} ((A_0+\Delta A)(\Delta B) - A_0B_0) \label{eq: LoRA update end} \\
&= \frac{\alpha}{r}(A_0\Delta B + \Delta A \Delta B - A_0B_0) \nonumber \\
&= \frac{\alpha}{r}(\Delta A \Delta B + A_0\Delta B) \nonumber.
\end{align}

Experiments from LoRA-FA~\citep{zhang2023lora-fa} have shown that freezing the randomly initialized matrix \(A\) and only training matrix \(B\) can achieve performance close to that of LoRA. When matrix \(A\) is frozen, the weight update is given by~\eqref{eq: LoRA-fa update}. Matrix \(B\) accumulates the gradients compressed by \(A^T\) during training, and when multiplied by \(A\), the compressed gradients are up-projected. Thus, LoRA-FA can be viewed as a process of gradient accumulation and compression, with the compression matrix being the randomly initialized \(A\).

\begin{equation}
    \begin{aligned}
        \Delta W &= \frac{\alpha}{r}A_0\Delta B \\
        &= -\eta \frac{\alpha}{r}  \Sigma_{t=0}^{T}A_0 A_0^T\frac{\partial{L_t}}{\partial{W_t}}.
        \label{eq: LoRA-fa update}
    \end{aligned}
\end{equation}

The update form of LoRA-FA provides significant inspiration. We hypothesize that vanilla LoRA has similar properties, i.e., LoRA acts as a gradient compressor. Based on this hypothesis, we can allocate larger ranks to parameters whose gradients and parameters themselves contain more low-rank information and initialize LoRA parameters using compressed gradients.

\subsection{GoRA Rank Allocation}
Given the above hypothesis of LoRA as a gradient compressor, our dynamic rank allocation strategy has four main objectives: (1) to measure the importance of weight based on respective \(n\)-step accumulated gradient \(G=\frac{1}{n}\Sigma_{i=1}^{n}\frac{\partial{L_i}}{W}\) and allocate rank accordingly; (2) to complete rank allocation before formal training begins, thus avoiding the complexity of dynamically changing parameter shapes during training; (3) to keep the number of trainable parameters roughly consistent with LoRA (difference less than 10\%); and (4) to maintain a form as consistent as possible with LoRA to ensure compatibility.

Consider the method for assessing the importance of weight. Since gradients exhibit significant low-rank properties~\citep{wang2024lora-ga}, and the nuclear norm comprehensively considers all singular values \(\sigma\) of a matrix, using the nuclear norm of the gradient to calculate weight importance is a viable option. However, as shown in Table~\ref{tab: ablation3}, this simple metric does not adequately reflect the importance of weight as we expected. Therefore, we use the sensitivity of parameters to loss, widely used in model pruning~\citep{zhang2022platon}, as the importance metric. We provide the following importance calculation formula:
\begin{eqnarray}
    \label{eq: importance}
    \text{I}(W) = \text{avg}(|W*G|).
\end{eqnarray}

We calculate the importance of weights in the model that are to be adapted using low-rank matrices and obtain the importance set \({\{\text{I}(W_i)}\}_{i=1}^N\). To facilitate dynamic rank allocation, we normalize the importance set. The normalization formula is given by:
\begin{eqnarray}
    \label{eq: normalized importance}
    \text{I}(W_i) = \frac{\text{I}(W_i)}{\Sigma_{i=1}^N \text{I}({W_i})}.
\end{eqnarray}

After obtaining the normalized importance set, we calculate the initial trainable parameter budget \(B_0\) for the entire model: given an initial rank (average rank) \(r_{0}\), the budget for a single weight \(W_i \in \mathbb
{R}^{m \times n}\) is \(B_{0}^{W_i}=(\sqrt{m+n}) \times r_0\). The total budget for the model is \(B_{0} = \Sigma_{i=1}^N B_{0}^{W_i}\). Therefore, the allocated rank for \(W_i\) is determined by the following formula, where \(B_f = \Sigma_{i=1}^N B_{f}^{W_i}\) is the total number of trainable parameters in the final model, and \(r_{\text{max}}\) and \(r_{\text{min}}\) are the tunable upper and lower bounds of the rank, respectively:
\begin{equation}
    \begin{aligned}
        \label{eq: allocated rank}
        r_{W_i} &= [\frac{B_{f}^{W_i}}{\sqrt{m+n}}]=[\frac{B_{0}*\text{I}(W_i)}{\sqrt{m+n}}] \\ &\text{s.t.} r_{\text{min}} \leq r_{W_i} \leq r_{\text{max}},
    \end{aligned}
\end{equation}
where the square brackets [] denote rounding to the nearest integer.

In summary, before formal training begins, we compute the \(n\)-step accumulated gradients for the matrices \(W\) that need to be adapted. Using \(G\), we calculate the importance of \(W\) and allocate ranks for GoRA, achieving the four objectives of our dynamic rank allocation strategy.


\subsection{GoRA Initialization Strategy}
Once ranks are allocated for each layer, it is crucial to properly initialize the low-rank matrices. The compression form in ~\eqref{eq: LoRA-fa update} is not optimal as 
\(A\) is randomly initialized and fixed; to achieve the optimal results, we need to properly initialize the \(B\) matrix so that the computation results of low-rank adapters at the beginning of training can compress the \(n\)-step accumulated gradients as closely as possible. The optimal initialization of the \(B\) matrix can be obtained using the Moore-Penrose inverse of the \(A\) matrix:

\begin{eqnarray}
\label{eq: compress init}
B &=& -(A^TA)^{-1}A^TG \\
AB &=& -A(A^TA)^{-1}A^TG
\label{eq: compress init end}.
\end{eqnarray}

According to \eqref{eq: compress init}-\eqref{eq: compress init end}, when \(B\) is initialized with \(-(A^TA)^{-1}A^TG\), the result of \(AB\) is the optimal low-rank approximation of \(G\) given \(A\), detailed proof provided in Appendix~\ref{inverse}. 
Note that due to the properties of pseudo-inverse computation, the scale of \(AB\) does not exactly match that of \(G\). Assuming \(G \in \mathbb
{R}^{m\times n}\) and \(A \in \mathbb
{R}^{m\times r}\) both follow a distribution with mean 0 and variance 1, the expected Frobenius norm of \(G\), \(\mathbf{E}[||G||_{F}]\), is \(\sqrt{mn}\), and the expected Frobenius norm of \(AB\), \(\mathbf{E}[||AB||_{F}]\), is \(\sqrt{rn}\), detailed proof provided in Appendix~\ref{scale}. To ensure that the computation results of low-rank adapters at the beginning of training approximate a single step of stochastic gradient descent with a tunable step size \(\gamma\), we introduce the scaling factor \(\xi\) for \(B\):

\begin{equation}
    \begin{aligned}
         \frac{\alpha}{r} A(\xi B) &\approx \xi \frac{\alpha}{r}\sqrt{\frac{r}{m}}G \\
        &\approx \frac{\xi \alpha}{\sqrt{rm}}G\\
        &\approx - \gamma G.
        \label{eq: compress scaling}
    \end{aligned}
\end{equation}

Thus, to make the initialization effect of GoRA equivalent to a single step of gradient descent, the scaling factor \(\xi\) should be set to \(\frac{\gamma \cdot \sqrt{rm}}{\alpha}\). Inspired by RSLoRA, to better utilize the larger ranks obtained through dynamic allocation, we modify the forward computation formula to \(W_t = W_0 + \Delta{W} = W_0 + \frac{\alpha}{\sqrt{r}}A_tB_t\). Accordingly, the value of \(\xi\) should be adjusted to \(\frac{\gamma \cdot \sqrt{m}}{\alpha}\). Additionally, setting the step size \(\gamma\) to a relatively large value can further enhance performance and yield optimal results. The full processes of our algorithm are shown in Algorithm~\ref{alg:tdlora}.

\begin{algorithm}[ht]
\caption{GoRA Rank Allocation and Initialization}
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}
\renewcommand{\algorithmiccomment}[1]{\hfill$\triangleright$ #1}
\begin{algorithmic}[1]
\label{alg:tdlora}
\REQUIRE Model $f(\cdot)$ with $L$ layers, parameters $W$, gradient accumulation steps $N$, loss function $\mathcal{L}$, scale factor $\gamma$, Initial trainable parameter budget $B_0$
\ENSURE Initialized low-rank matrices $A$, $B$

\FOR{$l = 1$ to $L$}
    \STATE $G_{l}^{avg} \gets 0$
\ENDFOR

\FOR{$i = 1$ to $N$}
    \STATE Sampled mini-batch $B_i = \{x,y\}$
    \STATE $\hat{y} \gets f(x, W)$
    \STATE $\ell \gets \mathcal{L}(y, \hat{y})$ 
    \FOR{$l = 1$ to $L$}
        \STATE $G_{l}^{avg} \gets G_{l}^{avg} + \frac{1}{n} \frac{\partial\ell}{\partial W_l}$ 
    \ENDFOR
\ENDFOR

\FOR{$l = 1$ to $L$}
    \STATE Compute importance $I(W_l) \gets \text{avg}(|W_l*G_l^{avg}|)$ 
\ENDFOR

\FOR{$l = 1$ to $L$}
    \STATE $I(W_l) \gets \frac{I(W_l)}{\sum_{l=1}^L I(W_l)}$
\ENDFOR

\FOR{$l = 1$ to $L$}
    \STATE $m, n \gets \text{size}(W_l)$
    \STATE $r_{l} \gets \text{clip}(\text{round}(\frac{B_0 \cdot I(W_l)}{\sqrt{m + n}}), r_{\text{min}}, r_{\text{max}})$
    \STATE $A_{l} \gets \text{kaiming\_uniform}(m, r)$ 
    \STATE $B_{l} \gets -(A_{l}^T A_{l})^{-1} A_{l}^T G_l^{avg}$ 
    \STATE $\xi = \frac{\gamma \cdot \sqrt{m}}{\alpha}$ 
    \STATE $B_l = \xi \cdot B_l$ 
\ENDFOR

\STATE \textbf{Return} $A$, $B$
\end{algorithmic}
\end{algorithm}