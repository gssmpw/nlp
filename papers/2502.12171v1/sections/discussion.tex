\section{Discussion}
\label{ablations}
In this section, we present a comprehensive set of ablation studies to evaluate the effectiveness of GoRA's dynamic rank allocation mechanism and its initialization strategy. Additionally, we discuss the impact of key hyperparameter choices introduced by GoRA, providing insights into their roles in shaping the model's performance.

\begin{figure*}[htp!] 
    \centering 
    \includegraphics[width=\textwidth]{images/rank_distribution.png}
    \caption{(a) Result rank distribution of fine-tuning Llama-3.1-8B-Base on the MetaMathQA-100K dataset using GoRA;(b) Difference values between GoRA and LoRA in directional updates of full-rank weights after merging;(c) Difference values between GoRA and LoRA in magnitude updates of full-rank weights after merging. Data points presented for every two layers.}
    \label{fig:rank}
\end{figure*}


\subsection{The Effect of Rank Allocation Strategy.}

The rank allocation strategy is a critical component influencing the performance of GoRA. As highlighted in Table~\ref{tab: ablation1}, we conducted ablation studies to evaluate different rank allocation ranges. The results demonstrate that a broader rank allocation range consistently leads to superior performance. For instance, (4-32) achieved a score of 48.98 on HumanEval, significantly outperforming both the fixed rank allocation strategy (8-8) and the more conservative allocation strategy (6-15). 

Figure~\ref{fig:rank} illustrates the rank distribution of (4-32). Notably, most ranks are allocated to the wv layers, while the wq layers receive the fewest rank allocations. This observation aligns with findings reported in prior work~\citep{hu2021lora}. Moreover, weights with higher ranks receive larger updates after merging the low-rank matrices. These observations underscore the effectiveness of our rank allocation strategy.

\begin{table}[h!]
\centering
\small
\caption{Ablation studies on Rank Allocation strategy. The notation "GoRA(\(x\)-\(y\))" indicates the minimum and maximum rank allocation range, where \(x\) is the lower bound and \(y\) is the upper bound. We also report number of trainable parameters(\#Params) for model fine-tuned on MetaMathQA-100K using each setting.}
\label{tab: ablation1}
\resizebox{\columnwidth}{!}{%
\begin{tabular}{l|cc|c}
\hline
Method                & \textbf{GSM8k} & \textbf{HumanEval} & \textbf{\#Params}\\ 
\hline
GoRA(4-32)                  & \textbf{72.88$\pm$0.99} & \textbf{48.98$\pm$2.14} & 7.00M    \\
GoRA(6-15)                  & 72.25$\pm$0.27 & 46.34$\pm2.44$ & 7.09M       \\
GoRA(8-8)                  & 72.10$\pm1.12$ & 46.55$\pm5.09$ & 6.82M       \\
AdaLoRA(0-12) & 70.63$\pm$0.77 & 41.46$\pm$3.66 & 10.24M \\
\hline
\end{tabular}%
}
\end{table}


\subsection{The Effect of Initialization Strategy.}

Table~\ref{tab: ablation2} summarizes the results of ablation studies conducted with various initialization scaling factors. Our experiments revealed that the choice of scaling factor \(\gamma\) has a substantial impact on the model's effectiveness. Notably, GoRA achieved its highest performance on the HumanEval dataset with \(\gamma = 5e-2\), attaining a score of 48.98. Meanwhile, GoRA with 
\(\gamma = 8e-2\) slightly outperformed other configurations on the GSM8k dataset, achieving a score of 72.91. Conversely, when \(\gamma = 0\), GoRA exhibited the weakest performance on GSM8k, scoring 72.45.A carefully selected scaling factor ensures that the initial LoRA computation closely approximates a gradient descent step, establishing a robust foundation for subsequent optimization. This is critical for ensuring training stability and achieving superior performance. However, improper selection of \(\gamma\) whether too high or too lowâ€”can disrupt training stability or lead to suboptimal results.

\begin{table}[h!]
\centering
\small
\caption{Ablation Studies on Initialize Scaling \((\gamma)\). The notation "GoRA(\(\gamma = x\))" indicates the value of the scaling factor used in the initialization. We also report the loss at the first training step (\#Loss@1) for models fine-tuned on the MetaMathQA-100K dataset under each setting.}
\label{tab: ablation2}
\resizebox{\columnwidth}{!}{%
\begin{tabular}{l|cc|c}
\hline
Method                & \textbf{GSM8k} & \textbf{HumanEval} & \textbf{\#Loss@1}\\ 
\hline
GoRA(\(\gamma=8e-2\))                  & \textbf{72.91$\pm$0.76} & 46.54$\pm1.54$ & 0.59        \\
GoRA(\(\gamma=5e-2\))         &    72.88$\pm0.99$ & \textbf{48.98$\pm$2.14} & 0.60 \\
GoRA(\(\gamma=3e-2\))                  & 72.71$\pm1.22$  & 45.93$\pm1.27$ & 0.61\\
GoRA(\(\gamma=0\)) & 72.45$\pm1.14$ & 46.34$\pm0.61$ & 0.67        \\
\hline
\end{tabular}%
}
\end{table}


\subsection{The Effect of Different Importance Metrics.}

\begin{table}[h!]
\centering
\small
\caption{Ablation Studies on Different Importance metrics, where \(||\cdot||_*\) represents the nuclear norm.}
\label{tab: ablation3}
\resizebox{\columnwidth}{!}{%
\begin{tabular}{l|cc}
\hline
Method               & \textbf{GSM8k} & \textbf{HumanEval} \\ 
\hline
GoRA(\(\text{avg}(|W* G|)\))  &    \textbf{72.88$\pm$0.99} & \textbf{48.98$\pm$2.14}       \\
GoRA(\(||W* G||_*\))     &    72.65$\pm0.78$ & 45.12$\pm3.17$       \\
GoRA(\(||G||_*\))  &    72.70$\pm0.68$ & 43.09$\pm0.93$       \\
\hline
\end{tabular}%
}
\end{table}

Table~\ref{tab: ablation3} compares different importance metrics, including sensitivity of parameter to loss, gradient nuclear norm, and parameter-gradient product nuclear norm. The results show that parameter sensitivity consistently outperforms the other methods, particularly on the HumanEval dataset, where Parameter Sensitivity achieved a score of 48.98, compared to 43.09 for gradient nuclear norm and 45.12 for parameter-gradient product nuclear norm. 