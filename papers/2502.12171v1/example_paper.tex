%%%%%%%% arxiv 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%
\pdfoutput=1
\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables
\usepackage{hyperref}

\PassOptionsToPackage{dvipsnames}{xcolor}
\usepackage[accepted]{example_sty}
\usepackage{colortbl}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

\usepackage{hyperref}  
\hypersetup{
    colorlinks=true, 
    linkcolor=red,   
    urlcolor=blue}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}


\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

\usepackage[textsize=tiny]{todonotes}
\newcommand{\yc}[1]{\textcolor{red}{#1}}

\arxivtitlerunning{Gradient-driven Adaptive Low Rank Adaptation}

\begin{document}

\twocolumn[
\arxivtitle{GoRA: Gradient-driven Adaptive Low Rank Adaptation}
\arxivsetsymbol{equal}{*}

\begin{arxivauthorlist}
\arxivauthor{haonan he}{equal,ustc,iim}
\arxivauthor{peng ye}{equal,cuhk,shailab,fdu}
\arxivauthor{yuchen ren}{shailab,syu}
\arxivauthor{yuan yuan}{iim}
\arxivauthor{lei chen}{iim}
\end{arxivauthorlist}

\arxivaffiliation{ustc}{University of Science and Technology of China}
\arxivaffiliation{shailab}{Shanghai Artificial Intelligence Laboratory}
\arxivaffiliation{syu}{University of Sydney}
\arxivaffiliation{fdu}{Fudan University}
\arxivaffiliation{iim}{Institute of Intelligent Machines, HFIPS, Chinese 
Academy of Sciences}
\arxivaffiliation{cuhk}{The Chinese University of Hong Kong}
\arxivcorrespondingauthor{lei chen}{chenlei@iim.ac.cn}

\arxivkeywords{LLMs, LoRA}

\vskip 0.3in
]

\printAffiliationsAndNotice{\arxivEqualContribution} 

\input{sections/abs}
\vspace{-5mm}
\input{sections/intro}
\input{sections/related_works}
\input{sections/method}
\input{sections/experiment}
\input{sections/discussion}

\section{Conclusions}
In conclusion, GoRA significantly improves LoRA by adaptive rank allocation and initialization based on gradient information. This approach enhances performance while maintaining efficiency, outperforming vanilla LoRA and even full fine-tuning in some cases. Extensive experiments demonstrate GoRA's effectiveness. Looking ahead, future research directions may include scaling GoRA to larger model architectures, exploring its application across diverse task domains, investigating alternative initialization strategies for matrix \(A\) and integrating GoRA with other LoRA variants such as DoRA.  Overall, GoRA offers an efficient solution for fine-tuning large language models.

\clearpage

\section*{Impact Statement}
This paper presents work whose goal is to advance the field of parameter-efficient fine-tuning. Specifically, to improve low-rank adaptation performance without sacrificing its efficiency and usability. There are many potential societal consequences of our work, none of which we feel must be specifically highlighted here.

\bibliography{example_paper}
\bibliographystyle{example_paper}

\newpage
\appendix
\onecolumn
\input{sections/appendix}

\end{document}