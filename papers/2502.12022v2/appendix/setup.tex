\section{Experimental Setup}\label{app:exp_setup}

\subsection{{\method} Implementation Details}\label{app:method_details}
In this appendix, we give the implementation details of our {\method} framework.

\paragraph{Data Construction}
For the original training set, denoted as {\dorige}, we utilize the training sets of GSM8K \citep{gsm8k2021cobbe} and MATH \citep{MATH2021hendrycks}. 
The GSM8K training set comprises 7,473 examples, while the MATH training set includes 7,500 examples. 
For simplicity, we directly adopt the DART-MATH-Hard dataset \citep{dartmath2024tong} as our {\daug}. 
DART-MATH-Hard, which is an augmented dataset derived from the GSM8K and MATH training sets through rejection sampling, contains approximately 0.6M examples in total. 
Notably, the number of responses varies across different training queries.
To convert CoT solutions into TIR format, we use \texttt{GPT-4o-2024-08-06} with a carefully designed prompt, as described in Table~\ref{tabapp:rewrite_prompt}. 
While most CoT solutions are successfully transformed into TIR format, we observe some anomalies. 
For instance, some rewritten TIRs fail to conclude with a final answer, while some TIRs produce code with syntax errors. 
To address these issues, we filter out ill-formed TIRs using rule-based matching. 
After filtering, we obtain a candidate dataset containing approximately 483K examples.


\begin{table*}[htbp]
    \centering
    \footnotesize
    \begin{tabularx}{\textwidth}{|X|}
        \hline
        \textbf{Rewriting Prompt Template} \\ 
        \hline
        You are a helpful mathematical assistant. A problem will be presented after ``Problem:'', followed by a reference solution after ``Original Solution:''. Your task is to rewrite the original solution. During rewriting, you tend to leverage Python (sympy is preferred) to facilitate solving the problem with step-by-step reasoning, especially for calculation and simplification. The specific requirements are as follows: \\ 
        \\
        1. Analyze the problem and write functions to solve it, ensuring that the functions do not require any arguments. \\
        2. Present the final result in \LaTeX{} using a $\boxed{\text{ANS}}$ without any units. \\
        3. Utilize the `pi' symbol and `Rational' from Sympy for $\pi$ and fractions, and simplify all fractions and square roots without converting them to decimal values. \\
        4. Avoid using sentences like ``Reasoning step in natural language:'', ``Reasoning in Python codes:'', and other similar phrases. \\
        5. Combine multiple calculation steps with Python code blocks where appropriate, avoiding unnecessary separate blocks. Limit the number of Python code blocks to fewer than 5 and use them wisely. \\
        6. The new solution format should be as follows: \\
        \\
        ``Reasoning step 1 in natural language without specific calculations \\
        \verb|```python| \\
        Python code block 1 for calculation and simplification, please print out the final output using \verb|print| \\
        \verb|```| \\
        \verb|```output| \\
        The output for code block 1 \\
        \verb|```| \\
        ...... \\
        Reasoning step N in natural language without specific calculations \\
        \verb|```python| \\
        Python code block N for calculation and simplification, please print out the final output using \verb|print| \\
        \verb|```| \\
        \verb|```output| \\
        The output for code block N \\
        \verb|```| \\
        Conclude the final answer.'' \\
        \\
        Problem: \{problem\} \\
        \\
        Original Solution: \{raw\_answer\} \\
        \\
        New Solution: \\[5pt]
        \hline
    \end{tabularx}
    \caption{The prompt for transforming CoT to TIR.}
    \label{tabapp:rewrite_prompt}
\end{table*}



\paragraph{Anchor Construction}
For the embedding, we use \texttt{text-embedding-ada-002} to encode all queries in our candidate set {\dd} into 1,536-dimensional vectors.
We then cluster these representations by K-means algorithm.
We set the number of clusters to be 100 for both GSM8K and MATH (cluster separately).
That is to say, the size of the anchor set is $A=100$.


\paragraph{Contribution Quantification}
To compute the CoT and TIR scores, we use a new candidate set, denoted as {\dcandidatee}. This new candidate set is constructed by randomly selecting one pair of CoT and TIR solutions for each training query from the original candidate set, thereby reducing computational costs.
The CoT score is then simplified to:
{\small  
\begin{equation*}  
S_{\text{CoT}}^k = \frac{1}{A} \sum_{i=1}^A  \mathbb{I} \big(a_i, \mathcal{G}(\cdot \,|\, \underbrace{x_k, y^*}_{\text{1-shot prompt}}, q_i)\big),  
\end{equation*}  
}  
A similar formulation is used for the TIR score.


\paragraph{Data Selection}
The distributions of ($S_{\text{CoT}}^k - S_{\text{TIR}}^k$) on GSM8K and MATH reveal distinct patterns (see Section~\ref{sec:ana_scores} and Appendix~\ref{app:scores}): all base LLMs demonstrate a tendency to rely more on CoT for GSM8K queries, while preferring TIR for MATH queries. 
As a result, it is reasonable to select different decision functions, $\mathcal{H}$, for GSM8K and MATH.
Specifically, for GSM8K, the dataset for supervised fine-tuning ($D_{\text{SFT}}$) is defined as:  
$$
D_{\text{SFT}} = \bigcup_{k=1}^N \{(x_k, y_k^j)\}_{j=1}^{M_k} \cup \bigcup_{k \in A} \{(x_k, z_k^j)\}_{j=1}^{M_k},
$$  
where the index set $A = \{k: S_{\text{CoT}}^k - S_{\text{TIR}}^k < \text{quantile}_1\}$.  

For MATH, $D_{\text{SFT}}$ is defined as:  
$$
D_{\text{SFT}} = \bigcup_{k=1}^N \{(x_k, z_k^j)\}_{j=1}^{M_k} \cup \bigcup_{k \in B} \{(x_k, y_k^j)\}_{j=1}^{M_k},
$$  
where the index set $B = \{k: S_{\text{CoT}}^k - S_{\text{TIR}}^k > \text{quantile}_2\}$.  

The thresholds quantile$_1$ and quantile$_2$ are determined through grid search. 
Notably, the performance of {\method} is not sensitive to these quantiles (see Section~\ref{sec:ablation} and Table~\ref{tabapp:ablation}). 
Additionally, we explored alternative decision functions $\mathcal{H}$ in our ablation study, with further details provided in Section~\ref{sec:ablation} and Appendix~\ref{app:ablation}.





\begin{table*}[htbp!]
  \resizebox{\textwidth}{!}{
  \footnotesize
  \centering
    \begin{tabular}{@{}lllccccccccc@{}}
\toprule
\multicolumn{1}{c}{\multirow{2}{*}{Model}} & \multirow{2}{*}{Quantiles} & \multirow{2}{*}{Metric} & \multicolumn{3}{c}{In-Domain} & \multicolumn{5}{c}{Out-of-Domain} & \multirow{2}{*}{AVG} \\ \cmidrule(lr){4-11}
\multicolumn{1}{c}{} &  &  & GSM8K & MATH & \multicolumn{1}{c|}{ID AVG} & MAWPS & SVAMP & College & Olympiad & \multicolumn{1}{c|}{OOD AVG} &  \\ \midrule
\multirow{15}{*}{Qwen2.5-0.5B} 
 & \multirow{3}{*}{50, 60} &  Acc & 52.2 & 37.2 & \multicolumn{1}{c|}{44.7} & 86.4 & 55.7 & 27.5 & 9.9 & \multicolumn{1}{c|}{44.9} & 44.8\\
 & & Token & 313.5 & 503.1 & \multicolumn{1}{c|}{408.3} & 224.3 & 304.7 & 496.1 & 748.2 & \multicolumn{1}{c|}{443.3} & 431.7\\
 & & \# Code & 0.2 & 2.62 & \multicolumn{1}{c|}{1.41} & 0.63 & 0.32 & 2.85 & 3.03 & \multicolumn{1}{c|}{1.71} & 1.61\\ \cmidrule(l){2-12} 
  & \multirow{3}{*}{40, 60} & Acc & 53.5 & 36.4 & \multicolumn{1}{c|}{\textbf{45.0}} & 85.9 & 57.9 & 26.4 & 8.4 & \multicolumn{1}{c|}{44.7} & 44.8\\
 & & Token & 307.2 & 504.2 & \multicolumn{1}{c|}{405.7} & 217.7 & 290.6 & 486.8 & 715.2 & \multicolumn{1}{c|}{427.6} & 420.3\\
 & & \# Code & 0.24 & 2.5 & \multicolumn{1}{c|}{1.37} & 0.56 & 0.3 & 2.7 & 2.84 & \multicolumn{1}{c|}{1.6} & 1.52\\ \cmidrule(l){2-12} 
& \multirow{3}{*}{30, 60} & Acc & 53.1 & 37.0 & \multicolumn{1}{c|}{\textbf{45.0}} & 86.2 & 56.3 & 26.7 & 10.2 & \multicolumn{1}{c|}{44.8} & 44.9\\
& & Token & 312.7 & 507.5 & \multicolumn{1}{c|}{410.1} & 218.6 & 298.1 & 482.4 & 720.6 & \multicolumn{1}{c|}{429.9} & 423.3\\
& & \# Code & 0.21 & 2.49 & \multicolumn{1}{c|}{1.35} & 0.49 & 0.29 & 2.73 & 2.81 & \multicolumn{1}{c|}{1.58} & 1.50 \\ \cmidrule(l){2-12} 

 & \multirow{3}{*}{30, 65$^*$} & Acc & 52.8 & 36.6 & \multicolumn{1}{c|}{44.7} & 85.9 & 59.4 & 26.9 & 8.6 & \multicolumn{1}{c|}{\textbf{45.2}} & \textbf{45.0} \\
 &  & Token & 309.7 & 508.7 & \multicolumn{1}{c|}{409.2} & 217.3 & 292.9 & 500.9 & 743.0 & \multicolumn{1}{c|}{438.5} & 428.8 \\
 &  & \# Code & 0.19 & 2.63 & \multicolumn{1}{c|}{1.41} & 0.52 & 0.33 & 2.82 & 3.06 & \multicolumn{1}{c|}{1.68} & 1.59 \\ \cmidrule(l){2-12} 
 & \multirow{3}{*}{30, 70} & Acc & 52.2 & 37.1 & \multicolumn{1}{c|}{44.7} & 86.4 & 55.7 & 27.6 & 9.9 & \multicolumn{1}{c|}{44.9} & 44.8\\
 & & Token & 313.5 & 503.1 & \multicolumn{1}{c|}{408.3} & 224.3 & 304.7 & 496.1 & 748.2 & \multicolumn{1}{c|}{443.3} & 431.7\\
 & & \# Code & 0.2 & 2.62 & \multicolumn{1}{c|}{1.41} & 0.63 & 0.32 & 2.85 & 3.03 & \multicolumn{1}{c|}{1.71} & 1.61\\
\bottomrule
\end{tabular}
  }
  \caption{Performance across different quantiles using Qwen2.5-0.5B. The best accuracies within each group are shown in \textbf{bold}.
  The three metrics, ``Acc'', ``Token'', and ``\# Code'' represent the average accuracy, total tokens per generation, and number of code executions. 
  ``Acc'' is reported in \%. ``ID AVG'', ``OOD AVG'', and ``AVG'' denote the averages of these metrics across in-domain, out-of-domain, and all six benchmarks. The two numbers in the ``Quantiles'' are the quantile of GSM8K and MATH, respectively. * denote our chosen quantiles.}
  \label{tabapp:threshold}
\end{table*}




\subsection{Evaluation Benchmarks}\label{app:benchmarks}



We give a brief introduction of evaluated benchmarks mentioned in Section~\ref{sec:exp_setup}.
\begin{itemize}
    \item GSM8K \citep{gsm8k2021cobbe} is a grade-school math benchmark, consisting of 7,473 training examples and 1,319 test examples. It is available at \href{https://huggingface.co/datasets/openai/gsm8k}{this link}, and under \href{https://lbesson.mit-license.org/}{MIT License}.
    \item MATH \citep{MATH2021hendrycks} is a competition-level math dataset, including 5,000 test examples and 7,500 training examples. It is available at \href{https://huggingface.co/datasets/hendrycks/competition_math}{this link}, and under \href{https://lbesson.mit-license.org/}{MIT License}.
    \item MAWPS \citep{mawps2016koncel} is a benchmark of math word problems (MWPs), incorporating 238 test examples. 
    It is under \href{https://lbesson.mit-license.org/}{MIT License} and can be found at \href{https://github.com/LYH-YF/MWPToolkit}{https://github.com/LYH-YF/MWPToolkit}. 
    \item SVAMP \citep{SVAMP2021patel} includes 1,000 simple MWPs, which is available at \href{https://github.com/LYH-YF/MWPToolkit}{https://github.com/LYH-YF/MWPToolkit}. It is under \href{https://lbesson.mit-license.org/}{MIT License}.
    \item CollegeMath \citep{CollegeMath2024Tang}: This dataset comprises 2818 college-grade mathematical questions sourced from 9 different textbooks, covering 7 fields including linear algebra and differential equations. It is designed to evaluate generalization in intricate mathematical reasoning across various domains.
    It is available at \href{https://github.com/microsoft/unilm/tree/master/mathscale}{this link}.
    \item OlympiadBench-Math \citep{OlympiadBench2024He}: This collection comprises 675 high-level Olympiad mathematical problems selected from various competitions and represents a text-only English fraction of OlympiadBench.
    It is available at \href{https://github.com/OpenBMB/OlympiadBench}{this link}.
\end{itemize}

\subsection{Evaluation Metrics}\label{app:metric}

In addition to evaluating accuracy across the six benchmarks mentioned in Section~\ref{sec:exp_setup}, we also assess the computational costs associated with interacting with external Python interpreters. 
As described in Algorithm~\ref{alg:interleave}, TIR involves multiple interactions with Python interpreters. 
The associated time costs can be divided into two categories: the time required to execute Python code blocks and the increased generation costs caused by progressively longer input sequences.
The first type of time cost is reflected in the number of interactions with Python interpreters, i.e., the number of code executions. 
The second type can be approximated by the number of generated tokens, which includes both input and output tokens. 
Since the number of generations is equivalent to the number of code executions, we use the average total tokens per generation to evaluate this cost. 
Naturally, TIR incurs a higher number of generated tokens due to multiple generations with progressively longer contexts.




\subsection{SFT and Evaluation Setup}\label{app:sft_setup}

\paragraph{SFT Setup}
In our experiments, we utilize various base LLMs, including general-purpose models (e.g., LLaMA-3-8B \citep{llama3modelcard}) and math-specialized models (e.g., Qwen2.5-Math \citep{Qwen25Math2024Yang}). 
The details of these base LLMs are outlined below:
\begin{itemize}
    \item \textbf{Llama-3} \citep{llama3modelcard}:  \href{https://www.llama.com/llama3/license/}{LLaMA 3 Community License}. We use Llama-3-8B as the base LLM in our experiments.
    \item \textbf{Qwen2.5} \citep{qwen252024Yang}: Qwen2.5 series are developed with dedication to math and coding. 
    We used 0.5B, 1.5B, 3B, and, 7B models. They are licensed under \href{https://www.apache.org/licenses/LICENSE-2.0}{Apache 2.0}.
    \item \textbf{Qwen2.5-Math} \citep{Qwen25Math2024Yang}: Qwen2.5-Math is a series of specialized math language models built upon the Qwen2.5 LLMs. 
    We use 3B and 7B variants. 
    They are under the same license as the Qwen2.5 series.
\end{itemize}
We set the maximum input length for all base models to be 4,096.
During SFT, we employ the Adam optimizer with a learning rate of $2 \times 10^{-5}$ and set batch size to 64, conducting training over three epochs.
Unlike \citet{numinamath7b, Qwen25Math2024Yang}, we use the same training prompt for both CoT and TIR. The prompt is provided in Table~\ref{tabapp:train_prompt}.
We utilize 8 A100 GPUs to do SFT, and training a 7B model for 3 epochs with exclusively CoT data spends approximately 12 hours.


\begin{table*}[htbp]
    \centering
    \footnotesize
    \begin{tabularx}{\textwidth}{|X|}
        \hline
        \textbf{Training and Inference Prompt Template} \\ 
        \hline
        Below is an instruction that describes a task. Write a response that appropriately completes the request. \\[5pt]
        \textbf{\#\#\# Instruction:} \\ 
        \{instruction\} \\[5pt]
        \textbf{\#\#\# Response:} \\[5pt]
        \hline
    \end{tabularx}
    \caption{Training prompt for base LLMs.}
    \label{tabapp:train_prompt}
\end{table*}


\paragraph{Evaluation Setup}
For evaluation, we adopt the same prompt used during SFT, as recommended by \citet{dartmath2024tong}. 
For TIR inference, please refer to Algorithm~\ref{alg:interleave}, where the maximum number of interactions is set to $n = 6$.
CoT inference can be viewed as a special case of Algorithm~\ref{alg:interleave} with $n = 1$.