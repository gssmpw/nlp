\section{Preliminaries}\label{app:background}

\subsection{Rejection Fine-Tuning}\label{app:rft}
For training LLMs, the original training datasets are often insufficient. 
To mitigate this issue, many studies adopt Rejection Fine-Tuning (RFT) \citep{RFT2023Yuan, metamath2023yu, dartmath2024tong} to augment the original dataset, thereby increasing the training data size and improving model performance. RFT is a fine-tuning approach that uses synthesized data generated via rejection sampling \citep{RFT2023Yuan}.

Suppose the original training set is $\mathcal{D}_{orig} = \{x_i, y_i \}_{i=1}^N$, consisting of $N$ data pairs $(x_i, y_i)$. 
The rejection sampling process works as follows: for each query $x_i$, a teacher model (e.g., GPT-4) generates $M$ responses, resulting in $\{x_i, y_i^j \}_{j=1}^M$, where $M$ is a predefined number (e.g., $M=10$ in \citet{metamath2023yu}). 
This yields $N \cdot M$ response examples in total.
A filtering process is then applied: if a response $y_i^j \neq y_i$, it is discarded. T
he result is the augmented training set $\mathcal{D}_{aug} = {\{x_i, y_i \}_{i=1}^N}_{j=1}^{M_i}$, where $M_i \leq M$ represents the number of correct responses for query $x_i$.
Notably, $M_i$ is often larger for simpler queries $x_i$, as these are more likely to produce correct responses.

RFT is widely employed for improving mathematical reasoning in LLMs \citep{metamath2023yu, dartmath2024tong, E-GSM2024Xu}. 
Typically, the queries remain unchanged \citep{dartmath2024tong} or are altered in a controlled way \citep{metamath2023yu}. 
This is because the filtering stage of the rejection sampling process relies on the availability of ground-truth outputs.



\subsection{TIR Inference Pipeline}\label{app:tir}

Tool-Integrated Reasoning (TIR) addresses mathematical problems by intertwining natural language reasoning with the execution of Python code. 
The process is initiated with gernerating a natural language reasoning step, denoted as $r_1$. 
When it is more advantageous to utilize programmatic tools, such as complex calculations, a Python code block, $a_1$, is created as guided by $r_1$. 
This code block is then run, and its result, $o_1$, is fed back into the model for further generation. 
This cycle is repeated until the maximal number of code blocks is reached or until the model concludes its answer within ``\lstinline|\boxed{}|.'' 
The entire reasoning path unfolds as $\tau = r_1 a_1 o_1 \dots r_{n-1} a_{n-1} o_{n-1} r_n$, where $r_i$ is the $i$-th natural language reasoning step, $a_i$ denotes the corresponding Python code block, and $o_i$ represents the output from executing the code. 
The complete inference workflow is detailed in Algorithm \ref{alg:interleave} (from \citet{tora2023Gou}).
From Algorithm \ref{alg:interleave}, TIR usually requires multiple generations based on previous reasoning paths and outputs returned by Python interpreter, which is more computationally expensive than CoT.
However, TIR can provide more precise calculation results than CoT.

\begin{figure}[thbp]
\begin{algorithm}[H]
\small
\begin{algorithmic}[1]
\Require problem $q$, model $\mathcal{G}$, prompt ${p}$, external tools $\mathcal{E}$, stop condition \textit{Stop($\cdot$)}, maximum iteration rounds $n$
\State $\tau_{0} \leftarrow \text{""}$ \algorithmiccomment{Trajectory Initialization}
\For{$i \leftarrow 1$ to $n$}
\State $r_{i}\sim \mathbb{P}_{\mathcal{G}}(\cdot|p\oplus q\oplus \tau_{i-1})$ \algorithmiccomment{Rationale Generation}

\If{\textit{Stop}($r_i$)} \algorithmiccomment{Stopping Criteria}
\State \Return $\tau_{i-1}\oplus r_i$
\EndIf

\State $a_i \sim \mathbb{P}_{\mathcal{G}}(\cdot|p\oplus q\oplus \tau_{i-1}\oplus r_i)$ \algorithmiccomment{Program Generation}
\State $o_{i} \leftarrow \mathcal{E}(a_i)$ \algorithmiccomment{Tool Execution}

\State $\tau_{i} \leftarrow \tau_{i-1}\oplus r_{i}\oplus a_i\oplus o_{i}$ \algorithmiccomment{Trajectory Update}
\EndFor
\State \Return $\tau_{n}$
\end{algorithmic}
\caption{Inference of TIR}
\label{alg:interleave}
\end{algorithm}
\end{figure}



%The whole inference pipeline is given in Algorithm \ref{alg:interleave} (from \citet{tora2023Gou}).
%The process involves inputting an LLM $\mathcal{G}$ with an inference prompt $p$ to construct a tool-use pathway $\tau$ for each question $q$. 
%Initially, the pathway $\tau_0$ is initialized as an empty string. At each iteration $i$, the model first generates a rationale as follows:

%\[
%r_{i} \sim \mathbb{P}_{\mathcal{G}}(\cdot|p\oplus q\oplus \tau_{i-1}),
%\]
%where $\oplus$ represents concatenation. 
%If $r_i$ contains an answer enclosed in ``\lstinline|\boxed{}|'', thus meeting the stopping criterion \textit{Stop}($r_i$), the generation process terminates. Otherwise, the model proceeds to draft a program for tool use:

%\[
%a_i \sim \mathbb{P}_{\mathcal{G}}(\cdot|p\oplus q\oplus \tau_{i-1}\oplus r_i)
%\]

%Following \citet{PAL2023gao}, if the generated code block $a_i$ includes code execution stop words such as \lstinline|```output|, we execute the program using a tool and retrieve the output $o_i$ as $o_{i} \leftarrow \mathcal{E}(a_i)$. 
%The output $o_i$ is subsequently provided to the model to facilitate further reasoning.
%The trajectory $\tau$ is then updated by appending the newly generated rationale $r_{i}$, program $a_i$, and the corresponding output $o_i$:

%\[
%\tau_{i} \leftarrow \tau_{i-1}\oplus r_{i}\oplus a_i\oplus o_{i}
%\]
%This iterative process is repeated until the reasoning sequence either meets the stopping criterion (e.g., producing an answer) or reaches a predefined maximum number of iterations $n$.




\subsection{Implicit Instruction Tuning}\label{app:iit}


In-Context Learning (ICL) can be interpreted as a form of implicit instruction tuning, where the model is effectively "fine-tuned" using the given demonstrations in an implicit manner \citep{dai2022iit1, yang2023iit2, irie2022iit3, 1-shot2023li}.  
Let $\mathbf{X_{\text{ins}}}, \mathbf{X_{\text{test}}} \in \mathbb{R}^{\din}$ represent the few-shot demonstration inputs and the test input, respectively. 
We define the attention query vector as $\bv{Q} = \mathbf{W}_Q \mathbf{X_{\text{test}}^\top}$, while the attention key and value vectors are given by $\bv{K} = \mathbf{W}_K [\mathbf{X}_{\text{ins}} \Vert \mathbf{X}_{\text{test}} ]$ and $\bv{V} = \mathbf{W}_V [\mathbf{X}_{\text{ins}} \Vert \mathbf{X}_{\text{test}} ]$, where $\Vert$ denotes concatenation. 
The projection matrices $\mathbf{W}_K, \mathbf{W}_V, \mathbf{W}_Q \in \mathbb{R}^{\dout \times \din}$ are used to compute the attention queries, keys, and values. 
The self-attention mechanism for a single attention head in any given layer is formulated as follows:
{\small
\begin{align*}
    &\textsf{Attention}(\bv{K}, \bv{V}, \bv{Q}) 
=  \\ &\mathbf{W}_V [\mathbf{X}_{\text{ins}} \Vert \mathbf{X}_{\text{test}} ]  \textsf{Softmax} \left(\frac{\mathbf{W}_K [\mathbf{X}_{\text{ins}} \Vert \mathbf{X}_{\text{test}} ] ^\top \bv{Q}} {\sqrt{ \din }}\right).
\end{align*}
}
Applying an approximation, this can be rewritten as:
{\small
\[
\mathbf{W}_V [\mathbf{X}_{\text{ins}} \Vert \mathbf{X}_{\text{test}} ] \left(\mathbf{W}_K [\mathbf{X}_{\text{ins}} \Vert \mathbf{X}_{\text{test}}]\right) ^\top \bv{Q}.
\]
}
By expanding this expression, we obtain:
{\small
\[
\underbrace{\mathbf{W}_V\mathbf{X}_{\text{test}}(\mathbf{W}_K\mathbf{X}_{\text{test}})^\top}_{\textit{Only test input.}} \bv{Q} + \underbrace{\mathbf{W}_V\mathbf{X}_{\text{ins}}(\mathbf{W}_K\mathbf{X}_{\text{ins}})^\top}_{\textit{Only demonstration samples.}} \bv{Q}.
\]
}
The whole approximation process can be given as follows:
{\small
\begin{align*}
    & \textsf{Attention}(\bv{K}, \bv{V}, \bv{Q}) \\
    &= \mathbf{W}_V [\mathbf{X}_{\text{ins}} \Vert \mathbf{X}_{\text{test}} ]  \textsf{Softmax} \left(\frac{\mathbf{W}_K [\mathbf{X}_{\text{ins}} \Vert \mathbf{X}_{\text{test}} ] ^\top \bv{Q}} {\sqrt{ \din }}\right)\\
    &\approx \mathbf{W}_V [\mathbf{X}_{\text{ins}} \Vert \mathbf{X}_{\text{test}} ] \left(\mathbf{W}_K [\mathbf{X}_{\text{ins}} \Vert \mathbf{X}_{\text{test}}]\right) ^\top \bv{Q} \\
    &= \underbrace{\mathbf{W}_V\mathbf{X}_{\text{test}}(\mathbf{W}_K\mathbf{X}_{\text{test}})^\top}_{\textit{ Only test input.}} \bv{Q} + \underbrace{\mathbf{W}_V\mathbf{X}_{\text{ins}}(\mathbf{W}_K\mathbf{X}_{\text{ins}})^\top}_{\textit{Only instruction sample.}} \bv{Q} \\
    &= [\underbrace{\mathbf{W}_V\mathbf{X}_{\text{test}}(\mathbf{W}_K\mathbf{X}_{\text{test}})^\top}_{\textit{ Only test input.}}  + \underbrace{\mathbf{W}_V\mathbf{X}_{\text{ins}}(\mathbf{W}_K\mathbf{X}_{\text{ins}})^\top}_{\textit{Only instruction sample.}}] \bv{Q},
\end{align*}
}
where the constant $\sqrt{d_{\text{in}}}$ acts as a scaling factor. 
The first term, $\mathbf{W}_V\mathbf{X}_{\text{test}}(\mathbf{W}_K\mathbf{X}_{\text{test}})^\top$, corresponds to a zero-shot learning scenario where no demonstration samples are involved, and only the test input is considered. 
Meanwhile, the second term, $\mathbf{W}_V\mathbf{X}_{\text{ins}}(\mathbf{W}_K\mathbf{X}_{\text{ins}})^\top$, can be interpreted as an implicit adjustment to the model parameters. 
This adjustment is achieved through the meta-gradient mechanism \citep{dai2022iit1, yang2023iit2, irie2022iit3}, meaning the few-shot examples influence the model as if performing implicit instruction tuning.
