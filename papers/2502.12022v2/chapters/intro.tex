\section{Introduction}\label{sec:intro}

Mathematical reasoning is a cornerstone of human intelligence, and enabling machines to solve mathematical problems is crucial for many applications \citep{llm4math2024Ahn, OlympiadBench2024He}. 
Recent years have seen rapid progress in leveraging LLMs for mathematical reasoning, with advancements in continuing pretraining \citep{minerva202lewkowycz, llemma2023azerbayev, deepseekmath2024shao}, supervised fine-tuning (SFT) \citep{metamath2023yu, dartmath2024tong, yan2024s, openmathinstruct2024toshniwal}, reinforcement learning (RL) \citep{deepseekmath2024shao, deepseekr12025deepseekai}, prompting \citep{CoT2022Wei, PHP2023Zheng, Plan-Solve2023Wang}, and evaluation \citep{CollegeMath2024Tang, ugmathbench2025xu, omnimath2024gao, ugphysics2025xu}. 
Among these, SFT has attracted significant attention, even in recent test-time scaling efforts \citep{s12025muennighoff}.

\begin{figure}[t]
    \centering
    \includegraphics[width=1\linewidth]{figures/illustration.pdf}
    \caption{Illustration of our research question. (a) \citet{automatictoolselect2023zhao} post-select between CoT and TIR by another LLM. (b) \citet{mammoth2023yue} choose CoT if TIR fails due to syntax error or execution timeout. (c) \citet{qwen252024Yang} controls the selection between CoT and TIR by predefined inference prompts. (d) We aim to teach LLMs to choose the appropriate one spontaneously according to their aptitude. }
    \label{fig:illustration}
\end{figure}


% LLM for math can follow two reasoning patterns: Chain-of-Thought (CoT) \citep{CoT2022Wei} and Tool-Integrated Reasoning (TIR) \citep{PoT2022Chen, PAL2023gao}. 
% CoT expresses the reasoning process as intermediate steps in natural language, making it flexible, interpretable, and easy to follow, though it does not guarantee computational accuracy.
% In contrast, TIR employs executable code to ensure rigorous and precise computations using Python interpreters. 
% TIR, unlike CoT, demands greater computational resources as it necessitates interactive execution, whereas CoT generally involves just a single forward pass of the LLM.
LLM math reasoning typically follows two paradigms: Chain-of-Thought (CoT) \citep{CoT2022Wei}, which uses natural language intermediate steps for flexibility and interpretability but lacks computational guarantees, and Tool-Integrated Reasoning (TIR) \citep{PoT2022Chen, PAL2023gao}, which employs executable code for precise computations at the cost of higher resource demands due to interactive execution. In contrast, CoT requires only a single forward pass, making it more computationally efficient.
Previous work often trains LLMs exclusively on CoT/TIR data \citep{dartmath2024tong, deepseekmath2024shao, yan2024s, tora2023Gou, mathcoder2023wang, mathcoder22024lu}, or relies on external signals or predefined prompts to select between the two \citep{automatictoolselect2023zhao, mammoth2023yue, Qwen25Math2024Yang} (see Figure~\ref{fig:illustration}).
Recent advancements in slow-thinking models, such as DeepSeek R1~\citep{deepseekr12025deepseekai}, have demonstrated the potential of long CoT reasoning.
Then a promising direction lies in enabling LLMs to adaptively incorporate tools during slow-thinking processes, combining the strengths of both CoT and TIR (Figure~\ref{fig:illustration} (d)).
A critical first step toward this goal is to teach base LLMs to adaptively select between CoT and TIR according to their inherent aptitude through SFT.
By doing so, we can better unlock the model's potential to use tools effectively while maintaining its generalization ability across diverse problem-solving scenarios (Figure~\ref{fig:illustration} (d))
% \citet{automatictoolselect2023zhao} find that CoT and TIR each excel on different types of math problems, and they propose using an additional LLM to select between these two via prompting (Figure~\ref{fig:illustration} (a)).
% Similarly, MAmmoTH \citep{mammoth2023yue} combines both approaches, using CoT when TIR fails due to errors or timeouts (Figure~\ref{fig:illustration} (b)).
% Qwen-2.5-Math \citep{Qwen25Math2024Yang} uses different inference prompts to elicit CoT and TIR (Figure~\ref{fig:illustration} (c)).
% However, 
% These methods rely primarily on external signals or manually predefined prompts to choose appropriate reasoning patterns, not by LLMs themselves.

%iterative, deliberate reasoning using pure text-based approaches.

%A critical first step toward this goal is to investigate how LLMs can adaptively leverage TIR during SFT, aligning the training process with the base model's inherent capabilities. 
%By doing so, we can better unlock the model's potential to use tools effectively while maintaining its generalization ability across diverse problem-solving scenarios (Figure~\ref{fig:illustration} (d)).
% Whether LLMs can spontaneously select a better reasoning pattern based on their aptitude remains an open question and has yet to be explored (Figure~\ref{fig:illustration} (d)).


In this work, we propose \textbf{T}eaching LLMs \textbf{A}ccording to \textbf{T}heir Aptitude (\textbf{\method}), an adaptive framework that enables LLMs to spontaneously select between CoT and TIR for math problem solving by using SFT data selection that aligns with LLM's inherent aptitude.
%This framework facilitates aptitude-driven reasoning, ensuring that the model selects the most suitable reasoning strategy (CoT or TIR) for different queries. 
% Instead of adopting a fixed strategy for all training queries, {\method} adaptively tailors the training data selection process by considering both the query characteristics and the base LLM's aptitude. 
% This ensures that the resulting model is equipped to select a suitable reasoning strategy (CoT or TIR) for different queries at test time, facilitating aptitude-driven reasoning. 
Unlike previous methods, {\method} leverages base-LLM-aware data selection during SFT, ensuring that the training process not only suits the model's potential but also mitigates the risk of catastrophic forgetting~\citep{chu2025sftrl}. 
By adaptively selecting between CoT and TIR based on the base model's aptitude, {\method} preserves and enhances the generalizability of the model, particularly for out-of-domain tasks. 
% This approach enables the model to autonomously choose the most effective reasoning strategy—CoT or TIR—during inference, ensuring robust and adaptable performance across diverse mathematical problem-solving scenarios.
%The {\method} framework facilitates aptitude-driven reasoning, ensuring that the model selects a suitable reasoning strategy (CoT or TIR) for different queries at test time.
%As a result, LLMs trained with the resulting SFT data can learn to spontaneously select between CoT and TIR for different problems at inference time.
%The core intuition behind TATA is that 
% Intuitively, if an LLM exhibits improved performance when trained on CoT solutions compared to TIR solutions for a specific query, this suggests a preference for CoT reasoning in such cases. 
% This preference can generalize to inference, where the model is expected to favor CoT for similar problems during test time.
% The same principle applies to TIR.

Concretely, we begin with a dataset {\dd}, which consists of $N$ triplets, each containing a query, a CoT solution, and a TIR solution.
We then construct an anchor set, {\danchor}, to evaluate the model's performance. For each training query in {\dd}, we assess the LLM's accuracy on {\danchor} by providing either the CoT or TIR solution of the query as a one-shot example. 
Based on the model's performance in each setting, we select the most effective reasoning paradigm for training queries and use it to construct the SFT data, {\dsft}.
%The model's performance in each setting is quantified into two separate scores, one for CoT and one for TIR. 
%Based on these scores, we select the most effective reasoning paradigm for training queries and use it to construct the SFT data, {\dsft}.

To assess the effectiveness of {\method}, we conduct extensive evaluations across six mathematical reasoning benchmarks, utilizing both general-purpose LLMs (e.g. Llama-3-8B \citep{llama3modelcard}) and math-specialized LLMs (e.g. Qwen2.5-Math-7B \citep{Qwen25Math2024Yang}) as base models. 
Experimental results demonstrate that {\method} successfully personalizes reasoning strategies according to each LLM’s aptitude, leading to better performance across different models and benchmarks.

To summarize, our contributions are as follows:

1. We propose {\method}, an adaptive framework that enables LLMs to spontaneously select between CoT and TIR for adaptive mathematical reasoning based on their inherent aptitudes.  
2. Extensive experiments demonstrate that {\method} effectively combines the strengths of both CoT and TIR, achieving comparable or even superior performance while offering higher inference efficiency compared to TIR. 
3. Comprehensive analyses highlight the critical role of base-LLM-aware data selection for CoT and TIR, which is the core of our {\method} framework. 