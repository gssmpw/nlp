\section{The {\method} Framework}\label{sec:method}
\subsection{Problem Setting}\label{sec:problem_setting}

Many advanced capabilities can be distilled through SFT~\citep{s12025muennighoff, lima2024zhou, deepseekr12025deepseekai}. 
Building on this, we aim to teach LLMs to spontaneously select between CoT and TIR using SFT by carefully selecting reasoning patterns tailored to different training queries.
In this section, we formally formulate our research question as an SFT data selection problem.

\paragraph{Data Structure}
Suppose we have a candidate dataset {\dde} consisting of triplets in the form $(x_i, y_i^j, z_i^j)$ for the $i$-th training example, where $1 \leq j \leq M_i$. 
Here, $x_i$ represents the $i$-th training problem, while $y_i^j$ and $z_i^j$ denote the $j$-th CoT solution and TIR solution to this problem, respectively.
Notably, the TIR solution $z_i^j$ is adapted from $y_i^j$, meaning both solutions follow the same steps to solve the mathematical problem $x_i$, but differ in their reasoning formats: $y_i^j$ relies exclusively on natural language reasoning, whereas $z_i^j$ incorporates Python code blocks to perform calculations for certain reasoning steps.

\paragraph{Objective}
Our objective is to construct an SFT dataset from the candidate dataset {\dde} by incorporating suitable reasoning patterns for different training queries.
Specifically, for each problem $x_i$ in {\dde}, we need to decide whether to include its CoT solutions or TIR solutions in the SFT dataset. 
Formally, this involves determining whether $\{(x_i, y_i^j)\}_{j=1}^{M_i} \subseteq D_{\text{SFT}}$ or $\{(x_i, z_i^j)\}_{j=1}^{M_i} \subseteq D_{\text{SFT}}$.\footnote{We also consider scenarios where both CoT and TIR solutions for a query are included in the SFT dataset.}
For example, CoT-only SFT~\citep{E-GSM2024Xu} constructs the dataset such that $\{(x_i, y_i^j)\}_{j=1}^{M_i} \subseteq D_{\text{SFT}}, \forall i$. 
In contrast, TIR-only SFT~\citep{tora2023Gou} selects $\{(x_i, z_i^j)\}_{j=1}^{M_i} \subseteq D_{\text{SFT}}, \forall i$. 
Our work aims to go beyond static selection approaches by dynamically tailoring the reasoning paradigm to suit the specific requirements of each training query, while also accounting for the base LLM's aptitude.

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.95\linewidth]{figures/overview.pdf}
    \caption{Overview of our \textbf{T}eaching LLMs \textbf{A}ccording to \textbf{T}heir Aptitude (\textbf{\method}) framework. 
    Here, {\dorig} denotes the original training set, {\daug} represents the augmented training set obtained through rejection sampling with CoT only, and {\dd} refers to the candidate set consisting of (query, CoT, TIR) triplets. {\danchor} is the anchor set of size $A$. {\scote} and {\stire} are scores calculated based on the LLMs' aptitude on the anchor set, elicited using 1-shot prompts. Finally, $\mathcal{H}$ represents the SFT data selection process.
    Fine-tuning on the resulting SFT data enables LLMs to spontaneously select between CoT and TIR at test time according to their aptitude.}
    \label{fig:overview}
\end{figure*}


\subsection{{\method} Overview}\label{sec:method_overview}

\begin{quote}
    \textit{``Teach according to students' aptitude.''} \\
    \vspace{-2.5em} % Adjust the spacing as needed
    \begin{flushright}
        --- Confucius
    \end{flushright}
\end{quote}

\paragraph{Motivation}
%Previous work on math-related SFT typically relies on a single reasoning pattern, either using CoT data alone~\citep{metamath2023yu, deepseekmath2024shao, openmathinstruct2024toshniwal} or TIR data alone~\citep{tora2023Gou, mathcoder2023wang, mathcoder22024lu}.
%CoT leverages natural language to reason, making it efficient in generation but prone to imprecise calculations. 
%In contrast, TIR ensures reliable outcomes but is computationally expensive (see Section~\ref{sec:tir}).
%As illustrated in Figure~\ref{fig:illustration}, several recent studies~\citep{automatictoolselect2023zhao, mammoth2023yue, numinamath7b, Qwen25Math2024Yang} have begun exploring ways to combine these two reasoning formats.
%However, these methods often depend on external planners~\citep{automatictoolselect2023zhao}, naive trial-and-error approaches~\citep{mammoth2023yue}, or human intervention~\citep{Qwen25Math2024Yang}.
Following the success of distillation through SFT~\citep{s12025muennighoff, deepseekr12025deepseekai}, we aim to enable LLMs to adaptively select appropriate reasoning strategies based on their own aptitude, guided by the control of SFT data selection (see Section~\ref{sec:problem_setting}). 
Intuitively, if an LLM demonstrates improved performance on certain queries when trained with CoT solutions instead of TIR solutions, it suggests its inclination toward CoT reasoning in those cases.
This preference can be extrapolated to new cases, where the model is expected to favor CoT for similar problems during testing. 
The same principle applies to TIR-based reasoning.
Inspired by IIT theory (see Section~\ref{sec:iit}), LLMs can be indirectly ``fine-tuned'' with CoT or TIR examples through one-shot learning, thereby replacing the need for actual SFT.



\paragraph{Overview}
As depicted in Figure~\ref{fig:overview}, our proposed framework, {\method}, comprises four main steps: data construction, anchor construction, contribution quantification, and data selection. 
In the data construction stage, we adapt an original training set, {\dorig}, containing CoT solutions, to form the candidate set {\dde}. 
This candidate set includes triplets of queries, a CoT solution, and corresponding TIR solution.
Next, during the anchor construction stage, a representative anchor set of size $A$ is generated from the original training set by clustering.
In the contribution quantification stage, we compute two scores, {\scote} and {\stire}, for each query $q_k$ in the candidate set {\dde}. 
These scores indicate the impact of CoT and TIR solutions on the performance of LLMs using IIT (see Setion~\ref{sec:iit}).
Finally, the data selection step formulates a decision based on {\scote} and {\stire}, determining whether to include CoT or TIR solutions for queries in {\dd}.

\subsection{{\method} Details}\label{sec:method_details}

\paragraph{Data Construction}
We start with an original math training set (e.g., MATH~\citep{MATH2021hendrycks} training set), denoted as {\dorige}, which consists of $N$ training examples, where the $i$-th problem is represented as $x_i$ with its corresponding golden answer $y_i$. 
To further enhance the training set, we apply RFT (see Section~\ref{sec:RFT}), resulting in an augmented dataset, {\dauge}, where $y_i^j$ denotes the $j$-th augmented CoT solution for the $i$-th training problem $x_i$. 
Next, we convert each CoT solution $y_i^j$ into the TIR format $z_i^j$ by prompting a strong LLM (e.g., \texttt{GPT-4o}). 
During this process, the original logic in $y_i^j$ is preserved, while Python blocks are introduced to handle necessary computations. 
This transformation produces a candidate dataset {\dde}, which is required for our problem setting (see Section~\ref{sec:problem_setting}).


\paragraph{Anchor Construction}
To evaluate the impact of specific CoT or TIR solutions on the performance of LLMs, we construct an anchor set, denoted by {\danchore}, where $A$ is the size of the anchor set, $q_i, a_i$ is the $i$-th question and corresponding ground-truth answer in {\danchor}.
We expect {\danchor} to be diverse, ensuring that accuracy on this set fairly reflects the LLMs' overall performance.
To achieve this, we first encode all queries from {\dorig} into vector representations using an embedding model (e.g., \texttt{text-embedding-ada-002}) and then cluster them into $A$ distinct groups.
%These embeddings are then clustered into $A$ distinct groups, and the center of each cluster is selected as a representative question to include in {\danchor}.
The center of each cluster is selected to {\danchor}.
This approach takes the semantic diversity of questions into account, making {\danchor} a reliable indicator of LLMs' performance.



\paragraph{Contribution Quantification}
To quantify the contribution of CoT and TIR for each triplet $(x_k, y_k^j, z_k^j)$ in {\dd} to the LLMs' math reasoning abilities, we implicitly "fine-tune" the LLMs using CoT and TIR formats separately through one-shot learning (see Section~\ref{sec:iit}). 
For the $k$-th query $x_k$ and its corresponding CoT solutions $y_k^j$ ($1 \leq j \leq M_k$), we compute a CoT score, denoted as {\scote}, as follows:
{\small
\begin{equation*}
S_{\text{CoT}}^k = \frac{1}{M_k} \sum_{j=1}^{M_k} \frac{1}{A} \sum_{i=1}^A  \mathbb{I} \big(a_i, \mathcal{G}(\cdot \,|\, \underbrace{x_k, y_k^j}_{\text{1-shot prompt}}, q_i)\big),
\end{equation*}
}
where $x_k$ and $y_k^j$ serve as the one-shot prompt for the LLM $\mathcal{G}$ to generate a response for the question $q_i$ in the anchor set, and $\mathbb{I}$ is an indicator function that returns 1 if the modelâ€™s generated answer matches the ground-truth answer $a_i$ of question $q_i$, and 0 otherwise. 
{\scote} represents the average accuracy on the anchor set {\danchor} when using CoT format as the one-shot prompt, averaged over all CoT solutions $y_k^j$ ($1 \leq j \leq M_k$) for query $x_k$. 
Similarly, the TIR score, {\stire}, is defined as:
{\small
\begin{equation*}
S_{\text{TIR}}^k = \frac{1}{M_k} \sum_{j=1}^{M_k} \frac{1}{A} \sum_{i=1}^A  \mathbb{I} \big(a_i, \mathcal{G}(\cdot \,|\, \underbrace{x_k, z_k^j}_{\text{1-shot prompt}}, q_i)\big).
\end{equation*}
}
The only difference is that the TIR format $z_k^j$ is used as the one-shot example instead of CoT.

\paragraph{Data Selection}
Currently, two scores, {\scote} and {\stire}, are associated with the $k$-th query $q_k$ in the candidate set {\dd}.
The next step is to determine whether to include the CoT or the TIR solutions for this specific query $q_k$ in {\dd}. 
Specifically, the goal is to decide between $\{(x_k, y_k^j)\}_{j=1}^{M_k} \subseteq D_{\text{SFT}}$ or $\{(x_k, z_k^j)\}_{j=1}^{M_k} \subseteq D_{\text{SFT}}$. 
We formalize this decision process with a decision function $\mathcal{H}_k = (S_{\text{CoT}}^k, S_{\text{TIR}}^k)$, where the final decision is represented as a series of decisions $\mathcal{H} = \{\mathcal{H}_k\}_{k=1}^N$, where $N$ is the number of queries in candidate set {\dd}. 
For instance, a simple decision function $\mathcal{H}_k$ could involve consistently choosing CoT solutions, i.e., $\{(x_k, y_k^j)\}_{j=1}^{M_k} \subseteq D_{\text{SFT}}$ for all $k$. 
This corresponds to performing SFT exclusively on CoT data.
