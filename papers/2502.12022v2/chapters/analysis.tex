\section{Analysis and Discussion}\label{sec:analysis}



\subsection{Analysis of CoT scores and TIR scores}\label{sec:ana_scores}
% Different LLMs analysis, pot cot score distribution plot
% maybe analyze why qwen math LLMs such weird behavior
% highlight the differences in the distribution for different LLMs (i.e. teach according to LLMs' aptitude, different LLMs select totally different examples)

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.32\linewidth]{figures/qwen0_5b-cot-tir.pdf}
    \includegraphics[width=0.32\linewidth]{figures/qwen7b-cot-tir.pdf}
    \includegraphics[width=0.32\linewidth]{figures/llama3-cot-tir.pdf}
    \caption{The distribution of ($S_{\text{CoT}}^k - S_{\text{TIR}}^k$): Qwen2.5-0.5B (left), Qwen2.5-7B (middle), LLaMA-3-8B (right).}
    \label{fig:cot-tir-scores}
\end{figure*}

To further investigate how different LLMs exhibit varying reasoning patterns, we analyze the distribution of {\scote} and {\stire}. 
As illustrated in Figure~\ref{fig:cot-tir-scores} (see also Appendix~\ref{app:scores}), different base LLMs display distinct distributions of ($S_{\text{CoT}}^k - S_{\text{TIR}}^k$), indicating varying inclinations towards CoT and TIR reasoning for queries in the candidate set {\dcandidatee}. 
Interestingly, even base LLMs from the same family can demonstrate different tendencies towards CoT and TIR (e.g., Qwen2.5-0.5B vs. Qwen2.5-7B).
Notably, Qwen2.5-7B exhibits a stronger preference for CoT on GSM8K and for TIR on MATH, compared to Qwen2.5-0.5B.
%More results and figures are provided in Appendix~\ref{app:scores}.






%\subsection{CoT and TIR behavior at test time}\label{sec:ana_cot_tir}
% CoT Acc and Tir Acc analysis, how would our pot+part_cot befinits results
% MATH level ratio
% pot fail, cot succeed, why? cot succeed, pot fails, why?





\subsection{Transferability of Data Selection between Different LLMs}\label{sec:ana_l0-aware}
% L0 aware: LLM A sft on LLM B selected data (ood performance
% 3. qwen 0.5 use llama8b 4. qwen0.5 use 7b

To evaluate whether data selected by one LLM can benefit another LLM, we conducted additional experiments using Qwen2.5-0.5B to assess this type of transferability. 
Specifically, we fine-tuned Qwen2.5-0.5B on data selected by Qwen2.5-7B and LLaMA-3-8B, with the results in Table~\ref{tab:transfer}. 
As expected, compared to fine-tuning Qwen2.5-0.5B on its own selected data, fine-tuning on data selected by another LLM leads to a decline in {\method} performance.
This finding suggests that our {\method} approach is base model-aware, emphasizing the principle of "teaching LLMs according to their aptitude." 
Interestingly, using data selected by LLMs within the same family (e.g., Qwen2.5-7B) yields more consistent performance compared to data selected by LLMs from a different family (LLaMA-3-8B). 
Complete results are in Appendix~\ref{app:transfer}.

\begin{table}[htbp!]
\footnotesize
\centering
\begin{tabular}{lccc}
\toprule
Selected by & ID AVG & OOD AVG & AVG \\
\midrule
\method & \textbf{44.7} & \textbf{45.2} & \textbf{45.0} \\
LLaMA-3-8B & 43.8 & 44.2 & 44.1 \\
Qwen2.5-7B & \underline{44.5} & \underline{44.6} & \underline{44.6} \\
\bottomrule
\end{tabular}
\caption{The best results (in \%) are highlighted in \textbf{bold}, while the second-best results are \underline{underlined}.}
\label{tab:transfer}
\end{table}


\subsection{Exploring Reinforcement Learning}\label{sec:rl}
% RL related
Recent advancements in reinforcement learning (RL) \citep{o1, deepseekr12025deepseekai} have demonstrated promising results in enhancing long CoT reasoning. 
To explore the role of RL in the spontaneous selection between CoT and TIR, we employ Direct Preference Optimization (DPO) to LLMs fine-tuned with our {\method} framework \citep{dpo2023rafailov} by constructing preference pairs based on the CoT and TIR scores of queries in the new candidate set {\dcandidatee}. 
Detailed experimental setup and methodologies are provided in Appendix~\ref{app:rl}.
As shown in Table~\ref{tab:dpo}, DPO achieves results comparable to those of {\method}. 
The complete results are provided in Table~\ref{app:rl}.
This suggests that the original data has already been effectively learned by the base LLM during the SFT stage, and applying additional DPO on the same dataset yields minor improvement. 
This observation aligns with LIMO \citep{limo2025ye}, which argue that the capabilities of pretrained LLMs are latent, with both SFT and RL serving as different methods to elicit these inherent abilities. 






\begin{table}[htbp]
\centering
\footnotesize
\begin{tabular}{llccc}
\toprule
Model & Method & Acc & Token & \# Code \\
\midrule
\multirow{2}{*}{LLaMA-3-8B} & \method & 61.5 & 371.7 & \textbf{1.32} \\
& +DPO & \textbf{61.6} & \textbf{365.4} & 1.34 \\
\midrule
\multirow{2}{*}{Qwen2.5Math-7B} & \method & \textbf{71.7} & \textbf{393.8} & \textbf{1.26} \\
& +DPO & \textbf{71.7} & 395.2 & 1.32 \\
\bottomrule
\end{tabular}
\caption{DPO Results. The best results are in \textbf{bold}.}
\label{tab:dpo}
\end{table}

