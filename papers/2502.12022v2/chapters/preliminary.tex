\section{Background}\label{sec:preliminary}

\subsection{Rejection Fine-Tuning}\label{sec:RFT}
%Rejection fine-tuning (RFT) is the fine-tuning process using synthesized data generated by rejection sampling \citep{RFT2023Yuan}.
Rejection fine-tuning (RFT) is a widely-adopted approach to enhance math reasoning abilities by augmenting the original training set using rejection sampling \citep{RFT2023Yuan}.
Suppose that the original training set {\dorige} consists of N pairs of data points $(x_i, y_i)$.
For each query $x_i$, $M$ responses are generated by a teacher model (e.g., GPT-4): $\{x_i, y_i^j \}_{j=1}^M$.
If $y_i^j \neq y_i$, then the response $y_i^j$ is discarded, leading to the augmented training set {\dauge}, where $M_i \leq M$ is the number of correct responses for query $x_i$.
More details are given in Appendix~\ref{app:rft}.

\subsection{TIR Inference Pipeline}\label{sec:tir}
Tool-Integrated Reasoning (TIR) \citep{tora2023Gou} combines natural language reasoning with Python code execution in an interleaved manner.
When a Python code block is encountered, it is executed using a Python interpreter, and the resulting output, along with the previous context, is fed back into the LLM to facilitate further reasoning (see Algorithm~\ref{alg:interleave}). 
Solving math problems with TIR often requires multiple iterations of these interactions, which typically results in higher computational costs compared to CoT. 
However, TIR offers more reliable results by leveraging external tools for computation. 
The whole inference pipeline of TIR is provided in Appendix~\ref{app:tir}.




\subsection{Implicit Instruction Tuning}\label{sec:iit}

In-Context Learning (ICL) can be viewed as implicit instruction tuning (IIT), i.e., “fine-tune” the demonstration implicitly \citep{1-shot2023li}.
Let $\mathbf{X_{\text{ins}}}, \mathbf{X_{\text{test}}} \in \mathbb{R}^{\din}$ be the few-shot demonstration inputs and the test input, respectively. 
Suppose $\mathbf{W}_K, \mathbf{W}_V, \mathbf{W}_Q \in \mathbb{R}^{\dout \times \din}$ are projection matrices to compute the attention queries, keys, and values. 
The self-attention is formulated as follows:
{\small
\begin{align*}
    & \mathbf{W}_V [\mathbf{X}_{\text{ins}} \Vert \mathbf{X}_{\text{test}} ]  \textsf{Softmax} \left(\frac{\mathbf{W}_K [\mathbf{X}_{\text{ins}} \Vert \mathbf{X}_{\text{test}} ] ^\top \bv{Q}} {\sqrt{ \din }}\right)\\
    &\approx [\underbrace{\mathbf{W}_V\mathbf{X}_{\text{test}}(\mathbf{W}_K\mathbf{X}_{\text{test}})^\top}_{\textit{ Only test input.}}  + \underbrace{\mathbf{W}_V\mathbf{X}_{\text{ins}}(\mathbf{W}_K\mathbf{X}_{\text{ins}})^\top}_{\textit{Only instruction sample.}}] \bv{Q},
\end{align*}
}where $\Vert$ denotes concatenation.
The first term only involves the test input $\mathbf{X_{\text{test}}}$, and the second term is related to few-shot exemplars, which can be interpreted as an IIT to the model parameters \citep{dai2022iit1, yang2023iit2} (see Appendix~\ref{app:iit}).
%The detailed derivation is given in Appendix~\ref{app:iit}.