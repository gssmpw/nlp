@misc{raffel2021call,
    author = {Raffel, Colin},
    title = {A Call to Build Models Like We Build Open-Source Software},
    year = {2021},
    note = {Accessed: 2025-01-15}
}

@incollection{mccloskey1989catastrophic,
  title={Catastrophic interference in connectionist networks: The sequential learning problem},
  author={McCloskey, Michael and Cohen, Neal J},
  booktitle={Psychology of learning and motivation},
  volume={24},
  pages={109--165},
  year={1989},
  publisher={Elsevier}
}

@inproceedings{guu2020retrieval,
  title={Retrieval augmented language model pre-training},
  author={Guu, Kelvin and Lee, Kenton and Tung, Zora and Pasupat, Panupong and Chang, Mingwei},
  booktitle={International conference on machine learning},
  pages={3929--3938},
  year={2020},
  organization={PMLR}
}

@article{kirkpatrick2017overcoming,
  title={Overcoming catastrophic forgetting in neural networks},
  author={Kirkpatrick, James and Pascanu, Razvan and Rabinowitz, Neil and Veness, Joel and Desjardins, Guillaume and Rusu, Andrei A and Milan, Kieran and Quan, John and Ramalho, Tiago and Grabska-Barwinska, Agnieszka and others},
  journal={Proceedings of the national academy of sciences},
  volume={114},
  number={13},
  pages={3521--3526},
  year={2017},
  publisher={National Acad Sciences}
}

@article{hu2021lora,
  title={Lora: Low-rank adaptation of large language models},
  author={Hu, Edward J and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
  journal={arXiv preprint arXiv:2106.09685},
  year={2021}
}

@inproceedings{du2024mogu,
    title={Mo{GU}: A Framework for Enhancing Safety of {LLM}s While Preserving Their Usability},
    author={Yanrui Du and Sendong Zhao and Danyang Zhao and Ming Ma and Yuhan Chen and Liangyu Huo and Qing Yang and Dongliang Xu and Bing Qin},
    booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},
    year={2024},
}

@article{chiang2024chatbot,
  title={Chatbot arena: An open platform for evaluating llms by human preference},
  author={Chiang, Wei-Lin and Zheng, Lianmin and Sheng, Ying and Angelopoulos, Anastasios Nikolas and Li, Tianle and Li, Dacheng and Zhang, Hao and Zhu, Banghua and Jordan, Michael and Gonzalez, Joseph E and others},
  journal={arXiv preprint arXiv:2403.04132},
  year={2024}
}

@article{lu2023routing,
  title={Routing to the expert: Efficient reward-guided ensemble of large language models},
  author={Lu, Keming and Yuan, Hongyi and Lin, Runji and Lin, Junyang and Yuan, Zheng and Zhou, Chang and Zhou, Jingren},
  journal={arXiv preprint arXiv:2311.08692},
  year={2023}
}

@article{xu2023wizardlm,
  title={Wizardlm: Empowering large language models to follow complex instructions},
  author={Xu, Can and Sun, Qingfeng and Zheng, Kai and Geng, Xiubo and Zhao, Pu and Feng, Jiazhan and Tao, Chongyang and Jiang, Daxin},
  journal={arXiv preprint arXiv:2304.12244},
  year={2023}
}


@article{diao2023mixture,
  title={Mixture-of-domain-adapters: Decoupling and injecting domain knowledge to pre-trained language models memories},
  author={Diao, Shizhe and Xu, Tianyang and Xu, Ruijia and Wang, Jiawei and Zhang, Tong},
  journal={arXiv preprint arXiv:2306.05406},
  year={2023}
}

@inproceedings{belofsky2023token,
  title={Token-Level Adaptation of LoRA Adapters for Downstream Task Generalization},
  author={Belofsky, Joshua},
  booktitle={Proceedings of the 2023 6th Artificial Intelligence and Cloud Computing Conference},
  pages={168--172},
  year={2023}
}


@misc{wang2022adamixmixtureofadaptationsparameterefficientmodel,
      title={AdaMix: Mixture-of-Adaptations for Parameter-efficient Model Tuning}, 
      author={Yaqing Wang and Sahaj Agarwal and Subhabrata Mukherjee and Xiaodong Liu and Jing Gao and Ahmed Hassan Awadallah and Jianfeng Gao},
      year={2022},
      eprint={2205.12410},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
}


@article{wu2024mixture,
  title={Mixture of lora experts},
  author={Wu, Xun and Huang, Shaohan and Wei, Furu},
  journal={arXiv preprint arXiv:2404.13628},
  year={2024}
}

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, A},
  journal={Advances in Neural Information Processing Systems},
  year={2017}
}

@inproceedings{houlsby2019parameter,
  title={Parameter-efficient transfer learning for NLP},
  author={Houlsby, Neil and Giurgiu, Andrei and Jastrzebski, Stanislaw and Morrone, Bruna and De Laroussilhe, Quentin and Gesmundo, Andrea and Attariyan, Mona and Gelly, Sylvain},
  booktitle={International conference on machine learning},
  pages={2790--2799},
  year={2019},
  organization={PMLR}
}

@article{pfeiffer2020adapterfusion,
  title={Adapterfusion: Non-destructive task composition for transfer learning},
  author={Pfeiffer, Jonas and Kamath, Aishwarya and R{\"u}ckl{\'e}, Andreas and Cho, Kyunghyun and Gurevych, Iryna},
  journal={arXiv preprint arXiv:2005.00247},
  year={2020}
}

@article{sukhbaatar2024branch,
  title={Branch-Train-MiX: Mixing Expert LLMs into a Mixture-of-Experts LLM},
  author={Sukhbaatar, Sainbayar and Golovneva, Olga and Sharma, Vasu and Xu, Hu and Lin, Xi Victoria and Rozi{\`e}re, Baptiste and Kahn, Jacob and Li, Daniel and Yih, Wen-tau and Weston, Jason and others},
  journal={arXiv preprint arXiv:2403.07816},
  year={2024}
}


@inproceedings{fan2024on,
    title={On Giant's Shoulders: Effortless Weak to Strong by Dynamic Logits Fusion},
    author={Chenghao Fan and Zhenyi Lu and Wei Wei and Jie Tian and Xiaoye Qu and Dangyang Chen and Yu Cheng},
    booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},
    year={2024},
}

@inproceedings{mavromatis2024pack,
title={Pack of {LLM}s: Model Fusion at Test-Time via Perplexity Optimization},
author={Costas Mavromatis and Petros Karypis and George Karypis},
booktitle={First Conference on Language Modeling},
year={2024},
}

@inproceedings{mitchell2024an,
    title={An Emulator for Fine-tuning Large Language Models using Small Language Models},
    author={Eric Mitchell and Rafael Rafailov and Archit Sharma and Chelsea Finn and Christopher D Manning},
    booktitle={The Twelfth International Conference on Learning Representations},
    year={2024},
}

@InProceedings{Leng_2024_CVPR,
    author    = {Leng, Sicong and Zhang, Hang and Chen, Guanzheng and Li, Xin and Lu, Shijian and Miao, Chunyan and Bing, Lidong},
    title     = {Mitigating Object Hallucinations in Large Vision-Language Models through Visual Contrastive Decoding},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2024},
    pages     = {13872-13882}
}

@inproceedings{sennrich-etal-2024-mitigating,
    title = "Mitigating Hallucinations and Off-target Machine Translation with Source-Contrastive and Language-Contrastive Decoding",
    author = "Sennrich, Rico  and
      Vamvas, Jannis  and
      Mohammadshahi, Alireza",
    booktitle = "Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics (Volume 2: Short Papers)",
    year = "2024",
}

@inproceedings{pei-etal-2023-preadd,
    title = "{PREADD}: Prefix-Adaptive Decoding for Controlled Text Generation",
    author = "Pei, Jonathan  and
      Yang, Kevin  and
      Klein, Dan",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2023",
    year = "2023",
}

@inproceedings{gera-etal-2023-benefits,
    title = "The Benefits of Bad Advice: Autocontrastive Decoding across Model Layers",
    author = "Gera, Ariel  and
      Friedman, Roni  and
      Arviv, Ofir  and
      Gunasekara, Chulaka  and
      Sznajder, Benjamin  and
      Slonim, Noam  and
      Shnarch, Eyal",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    year = "2023",
}


@inproceedings{chuang2024dola,
    title={DoLa: Decoding by Contrasting Layers Improves Factuality in Large Language Models},
    author={Yung-Sung Chuang and Yujia Xie and Hongyin Luo and Yoon Kim and James R. Glass and Pengcheng He},
    booktitle={The Twelfth International Conference on Learning Representations},
    year={2024},
}

@inproceedings{shi-etal-2024-trusting,
    title = "Trusting Your Evidence: Hallucinate Less with Context-aware Decoding",
    author = "Shi, Weijia  and
      Han, Xiaochuang  and
      Lewis, Mike  and
      Tsvetkov, Yulia  and
      Zettlemoyer, Luke  and
      Yih, Wen-tau",
    booktitle = "Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 2: Short Papers)",
    year = "2024",
}

@inproceedings{shi-etal-2024-decoding,
    title={Decoding-Time Language Model Alignment with Multiple Objectives},
    author={Ruizhe Shi and Yifang Chen and Yushi Hu and Alisa Liu and Hannaneh Hajishirzi and Noah A. Smith and Simon Shaolei Du},
    booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},
    year={2024},
}

@inproceedings{liu-etal-2024-tuning,
  title={Tuning Language Models by Proxy},
  author={Alisa Liu and Xiaochuang Han and Yizhong Wang and Yulia Tsvetkov and Yejin Choi and Noah A. Smith},
  booktitle={First Conference on Language Modeling},
  year={2024},
}

@inproceedings{shen-etal-2024-learning,
    title = "Learning to Decode Collaboratively with Multiple Language Models",
    author = "Shen, Zejiang  and
      Lang, Hunter  and
      Wang, Bailin  and
      Kim, Yoon  and
      Sontag, David",
    booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    year = "2024",
}

@inproceedings{liu-etal-2021-dexperts,
    title = "{DE}xperts: Decoding-Time Controlled Text Generation with Experts and Anti-Experts",
    author = "Liu, Alisa  and
      Sap, Maarten  and
      Lu, Ximing  and
      Swayamdipta, Swabha  and
      Bhagavatula, Chandra  and
      Smith, Noah A.  and
      Choi, Yejin",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    year = "2021",
}


@inproceedings{li-etal-2023-contrastive,
    title = "Contrastive Decoding: Open-ended Text Generation as Optimization",
    author = "Li, Xiang Lisa  and
      Holtzman, Ari  and
      Fried, Daniel  and
      Liang, Percy  and
      Eisner, Jason  and
      Hashimoto, Tatsunori  and
      Zettlemoyer, Luke  and
      Lewis, Mike",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    year = "2023",
}


@inproceedings{hoffmann2022training,
  title={Training compute-optimal large language models},
  author={Hoffmann, Jordan and Borgeaud, Sebastian and Mensch, Arthur and Buchatskaya, Elena and Cai, Trevor and Rutherford, Eliza and de Las Casas, Diego and Hendricks, Lisa Anne and Welbl, Johannes and Clark, Aidan and others},
  booktitle={Proceedings of the 36th International Conference on Neural Information Processing Systems},
  pages={30016--30030},
  year={2022}
}

@article{kaplan2020scaling,
  title={Scaling laws for neural language models},
  author={Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown, Tom B and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario},
  journal={arXiv preprint arXiv:2001.08361},
  year={2020}
}

@article{liangholistic,
  title={Holistic Evaluation of Language Models},
  author={Liang, Percy and Bommasani, Rishi and Lee, Tony and Tsipras, Dimitris and Soylu, Dilara and Yasunaga, Michihiro and Zhang, Yian and Narayanan, Deepak and Wu, Yuhuai and Kumar, Ananya and others},
  journal={Transactions on Machine Learning Research},
  year={2023}
}

@inproceedings{chiangchatbot,
  title={Chatbot Arena: An Open Platform for Evaluating LLMs by Human Preference},
  author={Chiang, Wei-Lin and Zheng, Lianmin and Sheng, Ying and Angelopoulos, Anastasios Nikolas and Li, Tianle and Li, Dacheng and Zhu, Banghua and Zhang, Hao and Jordan, Michael and Gonzalez, Joseph E and others},
  booktitle={Forty-first International Conference on Machine Learning},
  year={2024}
}

@misc{bigtech,
  author = {Will Henshall},
  title = {Big Tech Companies Were Investors in Smaller AI Labs. Now They’re Rivals},
  year = {2024},
  publisher = {Time Magazine},
  howpublished = {https://time.com/6977424/ai-competition-openai-anthropic-microsoft-amazon/},
}

@article{Brown2020LanguageMA,
  title={Language Models are Few-Shot Learners},
  author={Tom B. Brown and Benjamin Mann and Nick Ryder and Melanie Subbiah and Jared Kaplan and Prafulla Dhariwal and Arvind Neelakantan and Pranav Shyam and Girish Sastry and Amanda Askell and Sandhini Agarwal and Ariel Herbert-Voss and Gretchen Krueger and Tom Henighan and Rewon Child and Aditya Ramesh and Daniel M. Ziegler and Jeff Wu and Clemens Winter and Christopher Hesse and Mark Chen and Eric Sigler and Ma-teusz Litwin and Scott Gray and Benjamin Chess and Jack Clark and Christopher Berner and Sam McCandlish and Alec Radford and Ilya Sutskever and Dario Amodei},
  journal={ArXiv},
  year={2020},
  volume={abs/2005.14165},
}

@article{lazaridou2021pitfalls,
  title={Pitfalls of static language modelling},
  author={Lazaridou, Angeliki and Kuncoro, Adhiguna and Gribovskaya, Elena and Agrawal, Devang and Liska, Adam and Terzi, Tayfun and Gimenez, Mai and de Masson d’Autume, Cyprien and Ruder, Sebastian and Yogatama, Dani and others},
  journal={arXiv preprint arXiv:2102.01951},
  year={2021}
}

@article{kasai2024realtime,
  title={REALTIME QA: what's the answer right now?},
  author={Kasai, Jungo and Sakaguchi, Keisuke and Le Bras, Ronan and Asai, Akari and Yu, Xinyan and Radev, Dragomir and Smith, Noah A and Choi, Yejin and Inui, Kentaro and others},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{dhingra2022time,
  title={Time-aware language models as temporal knowledge bases},
  author={Dhingra, Bhuwan and Cole, Jeremy R and Eisenschlos, Julian Martin and Gillick, Daniel and Eisenstein, Jacob and Cohen, William W},
  journal={Transactions of the Association for Computational Linguistics},
  volume={10},
  pages={257--273},
  year={2022},
  publisher={MIT Press One Broadway, 12th Floor, Cambridge, Massachusetts 02142, USA~…}
}

@inproceedings{weievaluating,
  title={Evaluating Copyright Takedown Methods for Language Models},
  author={Wei, Boyi and Shi, Weijia and Huang, Yangsibo and Smith, Noah A and Zhang, Chiyuan and Zettlemoyer, Luke and Li, Kai and Henderson, Peter},
  booktitle={The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track},
  year={2024}
}

@inproceedings{chen2024copybench,
  title={CopyBench: Measuring Literal and Non-Literal Reproduction of Copyright-Protected Text in Language Model Generation},
  author={Chen, Tong and Asai, Akari and Mireshghallah, Niloofar and Min, Sewon and Grimmelmann, James and Choi, Yejin and Hajishirzi, Hannaneh and Zettlemoyer, Luke and Koh, Pang Wei},
  booktitle={Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing},
  pages={15134--15158},
  year={2024}
}

@inproceedings{song2023globalbench,
  title={GlobalBench: A Benchmark for Global Progress in Natural Language Processing},
  author={Song, Yueqi and Khanuja, Simran and Liu, Pengfei and Faisal, Fahim and Ostapenko, Alissa and Winata, Genta and Aji, Alham and Cahyawijaya, Samuel and Tsvetkov, Yulia and Anastasopoulos, Antonios and others},
  booktitle={Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing},
  pages={14157--14171},
  year={2023}
}

@inproceedings{faisal-etal-2024-dialectbench,
    title = "{DIALECTBENCH}: An {NLP} Benchmark for Dialects, Varieties, and Closely-Related Languages",
    author = "Faisal, Fahim  and
      Ahia, Orevaoghene  and
      Srivastava, Aarohi  and
      Ahuja, Kabir  and
      Chiang, David  and
      Tsvetkov, Yulia  and
      Anastasopoulos, Antonios",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    year = "2024",
}

@article{rao2024normad,
  title={Normad: A benchmark for measuring the cultural adaptability of large language models},
  author={Rao, Abhinav and Yerukola, Akhila and Shah, Vishwa and Reinecke, Katharina and Sap, Maarten},
  journal={arXiv preprint arXiv:2404.12464},
  year={2024}
}

@article{shi2024culturebank,
  title={Culturebank: An online community-driven knowledge base towards culturally aware language technologies},
  author={Shi, Weiyan and Li, Ryan and Zhang, Yutong and Ziems, Caleb and Horesh, Raya and de Paula, Rog{\'e}rio Abreu and Yang, Diyi and others},
  journal={arXiv preprint arXiv:2404.15238},
  year={2024}
}

@article{sun2022paradigm,
  title={Paradigm shift in natural language processing},
  author={Sun, Tian-Xiang and Liu, Xiang-Yang and Qiu, Xi-Peng and Huang, Xuan-Jing},
  journal={Machine Intelligence Research},
  volume={19},
  number={3},
  pages={169--183},
  year={2022},
  publisher={Springer}
}

@article{team2023gemini,
  title={Gemini: a family of highly capable multimodal models},
  author={Team, Gemini and Anil, Rohan and Borgeaud, Sebastian and Alayrac, Jean-Baptiste and Yu, Jiahui and Soricut, Radu and Schalkwyk, Johan and Dai, Andrew M and Hauth, Anja and Millican, Katie and others},
  journal={arXiv preprint arXiv:2312.11805},
  year={2023}
}

@article{achiam2023gpt,
  title={Gpt-4 technical report},
  author={Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and others},
  journal={arXiv preprint arXiv:2303.08774},
  year={2023}
}

@misc{team2023internlm,
  title={Internlm: A multilingual language model with progressively enhanced capabilities},
  author={Team, InternLM},
  journal={2023-01-06)[2023-09-27]. https://github. com/InternLM/InternLM},
  year={2023}
}

@article{leibo2024theory,
  title={A theory of appropriateness with applications to generative artificial intelligence},
  author={Leibo, Joel Z and Vezhnevets, Alexander Sasha and Diaz, Manfred and Agapiou, John P and Cunningham, William A and Sunehag, Peter and Haas, Julia and Koster, Raphael and Du{\'e}{\~n}ez-Guzm{\'a}n, Edgar A and Isaac, William S and others},
  journal={arXiv preprint arXiv:2412.19010},
  year={2024}
}

@inproceedings{suzgun2023challenging,
  title={Challenging BIG-Bench Tasks and Whether Chain-of-Thought Can Solve Them},
  author={Suzgun, Mirac and Scales, Nathan and Sch{\"a}rli, Nathanael and Gehrmann, Sebastian and Tay, Yi and Chung, Hyung Won and Chowdhery, Aakanksha and Le, Quoc and Chi, Ed and Zhou, Denny and others},
  booktitle={Findings of the Association for Computational Linguistics: ACL 2023},
  pages={13003--13051},
  year={2023}
}

@misc{open-llm-leaderboard-v2,
  author = {Clémentine Fourrier and Nathan Habib and Alina Lozovskaya and Konrad Szafer and Thomas Wolf},
  title = {Open LLM Leaderboard v2},
  year = {2024},
  publisher = {Hugging Face},
}
  %howpublished = "\url{https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard}",

@article{sprague2024cot,
  title={To cot or not to cot? chain-of-thought helps mainly on math and symbolic reasoning},
  author={Sprague, Zayne and Yin, Fangcong and Rodriguez, Juan Diego and Jiang, Dongwei and Wadhwa, Manya and Singhal, Prasann and Zhao, Xinyu and Ye, Xi and Mahowald, Kyle and Durrett, Greg},
  journal={arXiv preprint arXiv:2409.12183},
  year={2024}
}

@inproceedings{xieadaptive,
  title={Adaptive Chameleon or Stubborn Sloth: Revealing the Behavior of Large Language Models in Knowledge Conflicts},
  author={Xie, Jian and Zhang, Kai and Chen, Jiangjie and Lou, Renze and Su, Yu},
  booktitle={The Twelfth International Conference on Learning Representations},
  year={2024}
}

@article{wang2023resolving,
  title={Resolving knowledge conflicts in large language models},
  author={Wang, Yike and Feng, Shangbin and Wang, Heng and Shi, Weijia and Balachandran, Vidhisha and He, Tianxing and Tsvetkov, Yulia},
  journal={First Conference on Language Modeling},
  year={2024}
}

@inproceedings{sclarquantifying,
  title={Quantifying Language Models' Sensitivity to Spurious Features in Prompt Design or: How I learned to start worrying about prompt formatting},
  author={Sclar, Melanie and Choi, Yejin and Tsvetkov, Yulia and Suhr, Alane},
  booktitle={The Twelfth International Conference on Learning Representations},
  year={2024}
}

@article{liu2024lost,
  title={Lost in the middle: How language models use long contexts},
  author={Liu, Nelson F and Lin, Kevin and Hewitt, John and Paranjape, Ashwin and Bevilacqua, Michele and Petroni, Fabio and Liang, Percy},
  journal={Transactions of the Association for Computational Linguistics},
  volume={12},
  pages={157--173},
  year={2024},
  publisher={MIT Press One Broadway, 12th Floor, Cambridge, Massachusetts 02142, USA~…}
}

@inproceedings{sorensenposition,
  title={Position: A Roadmap to Pluralistic Alignment},
  author={Sorensen, Taylor and Moore, Jared and Fisher, Jillian and Gordon, Mitchell L and Mireshghallah, Niloofar and Rytting, Christopher Michael and Ye, Andre and Jiang, Liwei and Lu, Ximing and Dziri, Nouha and others},
  booktitle={Forty-first International Conference on Machine Learning},
  year={2024}
}

@inproceedings{feng2024modular,
  title={Modular Pluralism: Pluralistic Alignment via Multi-LLM Collaboration},
  author={Feng, Shangbin and Sorensen, Taylor and Liu, Yuhan and Fisher, Jillian and Park, Chan Young and Choi, Yejin and Tsvetkov, Yulia},
  booktitle={Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing},
  pages={4151--4171},
  year={2024}
}

@inproceedings{santurkar2023whose,
  title={Whose opinions do language models reflect?},
  author={Santurkar, Shibani and Durmus, Esin and Ladhak, Faisal and Lee, Cinoo and Liang, Percy and Hashimoto, Tatsunori},
  booktitle={International Conference on Machine Learning},
  pages={29971--30004},
  year={2023},
  organization={PMLR}
}

@inproceedings{feng2023pretraining,
  title={From Pretraining Data to Language Models to Downstream Tasks: Tracking the Trails of Political Biases Leading to Unfair NLP Models},
  author={Feng, Shangbin and Park, Chan Young and Liu, Yuhan and Tsvetkov, Yulia},
  booktitle={Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={11737--11762},
  year={2023}
}

@inproceedings{sorensen2024value,
  title={Value kaleidoscope: Engaging ai with pluralistic human values, rights, and duties},
  author={Sorensen, Taylor and Jiang, Liwei and Hwang, Jena D and Levine, Sydney and Pyatkin, Valentina and West, Peter and Dziri, Nouha and Lu, Ximing and Rao, Kavel and Bhagavatula, Chandra and others},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={38},
  pages={19937--19947},
  year={2024}
}

@misc{workforce,
  author = {EEOC},
  title = {High Tech, Low Inclusion: Diversity in the High Tech Workforce and Sector 2014 - 2022},
  year = {2024},
  publisher = {U.S. Equal Employment Opportunity Commission},
}

@article{johnson2006collaboration,
  title={Collaboration, peer review and open source software},
  author={Johnson, Justin P},
  journal={Information Economics and Policy},
  volume={18},
  number={4},
  pages={477--497},
  year={2006},
  publisher={Elsevier}
}

@article{zhang2023instruction,
  title={Instruction tuning for large language models: A survey},
  author={Zhang, Shengyu and Dong, Linfeng and Li, Xiaoya and Zhang, Sen and Sun, Xiaofei and Wang, Shuhe and Li, Jiwei and Hu, Runyi and Zhang, Tianwei and Wu, Fei and others},
  journal={arXiv preprint arXiv:2308.10792},
  year={2023}
}

@article{luo2023empirical,
  title={An empirical study of catastrophic forgetting in large language models during continual fine-tuning},
  author={Luo, Yun and Yang, Zhen and Meng, Fandong and Li, Yafu and Zhou, Jie and Zhang, Yue},
  journal={arXiv preprint arXiv:2308.08747},
  year={2023}
}

@inproceedings{lin2024mitigating,
  title={Mitigating the alignment tax of rlhf},
  author={Lin, Yong and Lin, Hangyu and Xiong, Wei and Diao, Shizhe and Liu, Jianmeng and Zhang, Jipeng and Pan, Rui and Wang, Haoxiang and Hu, Wenbin and Zhang, Hanning and others},
  booktitle={Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing},
  pages={580--606},
  year={2024}
}

@article{shi2023replug,
  title={Replug: Retrieval-augmented black-box language models},
  author={Shi, Weijia and Min, Sewon and Yasunaga, Michihiro and Seo, Minjoon and James, Rich and Lewis, Mike and Zettlemoyer, Luke and Yih, Wen-tau},
  journal={arXiv preprint arXiv:2301.12652},
  year={2023}
}

@article{Press2022MeasuringAN,
  title={Measuring and Narrowing the Compositionality Gap in Language Models},
  author={Ofir Press and Muru Zhang and Sewon Min and Ludwig Schmidt and Noah A. Smith and Mike Lewis},
  journal={ArXiv},
  year={2022},
  volume={abs/2210.03350},
}

@article{feng2023cook,
  title={CooK: Empowering General-Purpose Language Models with Modular and Collaborative Knowledge},
  author={Feng, Shangbin and Shi, Weijia and Bai, Yuyang and Balachandran, Vidhisha and He, Tianxing and Tsvetkov, Yulia},
  journal={arXiv preprint arXiv:2305.09955},
  year={2023}
}

@article{wang2022self,
  title={Self-consistency improves chain of thought reasoning in language models},
  author={Wang, Xuezhi and Wei, Jason and Schuurmans, Dale and Le, Quoc and Chi, Ed and Narang, Sharan and Chowdhery, Aakanksha and Zhou, Denny},
  journal={arXiv preprint arXiv:2203.11171},
  year={2022}
}

@article{shinn2024reflexion,
  title={Reflexion: Language agents with verbal reinforcement learning},
  author={Shinn, Noah and Cassano, Federico and Gopinath, Ashwin and Narasimhan, Karthik and Yao, Shunyu},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{xu2024sayself,
  title={SaySelf: Teaching LLMs to Express Confidence with Self-Reflective Rationales},
  author={Xu, Tianyang and Wu, Shujin and Diao, Shizhe and Liu, Xiaoze and Wang, Xingyao and Chen, Yangyi and Gao, Jing},
  journal={arXiv preprint arXiv:2405.20974},
  year={2024}
}

@article{madaan2024self,
  title={Self-refine: Iterative refinement with self-feedback},
  author={Madaan, Aman and Tandon, Niket and Gupta, Prakhar and Hallinan, Skyler and Gao, Luyu and Wiegreffe, Sarah and Alon, Uri and Dziri, Nouha and Prabhumoye, Shrimai and Yang, Yiming and others},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{ji2023survey,
  title={Survey of hallucination in natural language generation},
  author={Ji, Ziwei and Lee, Nayeon and Frieske, Rita and Yu, Tiezheng and Su, Dan and Xu, Yan and Ishii, Etsuko and Bang, Ye Jin and Madotto, Andrea and Fung, Pascale},
  journal={ACM Computing Surveys},
  volume={55},
  number={12},
  pages={1--38},
  year={2023},
  publisher={ACM New York, NY}
}

@article{liang2023encouraging,
  title={Encouraging divergent thinking in large language models through multi-agent debate},
  author={Liang, Tian and He, Zhiwei and Jiao, Wenxiang and Wang, Xing and Wang, Yan and Wang, Rui and Yang, Yujiu and Shi, Shuming and Tu, Zhaopeng},
  journal={arXiv preprint arXiv:2305.19118},
  year={2023}
}

@article{feng2024don,
  title={Don't Hallucinate, Abstain: Identifying LLM Knowledge Gaps via Multi-LLM Collaboration},
  author={Feng, Shangbin and Shi, Weijia and Wang, Yike and Ding, Wenxuan and Balachandran, Vidhisha and Tsvetkov, Yulia},
  journal={arXiv preprint arXiv:2402.00367},
  year={2024}
}

@article{cohen2023lm,
  title={Lm vs lm: Detecting factual errors via cross examination},
  author={Cohen, Roi and Hamri, May and Geva, Mor and Globerson, Amir},
  journal={arXiv preprint arXiv:2305.13281},
  year={2023}
}

@article{xiong2023examining,
  title={Examining inter-consistency of large language models collaboration: An in-depth analysis via debate},
  author={Xiong, Kai and Ding, Xiao and Cao, Yixin and Liu, Ting and Qin, Bing},
  journal={arXiv preprint arXiv:2305.11595},
  year={2023}
}


% routing papers
% routing
@article{hu2024routerbench,
  title={ROUTERBENCH: A Benchmark for Multi-LLM Routing System},
  author={Hu, Qitian Jason and Bieker, Jacob and Li, Xiuyu and Jiang, Nan and Keigwin, Benjamin and Ranganath, Gaurav and Keutzer, Kurt and Upadhyay, Shriyash Kaustubh},
  journal={arXiv preprint arXiv:2403.12031},
  year={2024}
}

@article{shnitzer2023large,
  title={Large language model routing with benchmark datasets},
  author={Shnitzer, Tal and Ou, Anthony and Silva, M{\'\i}rian and Soule, Kate and Sun, Yuekai and Solomon, Justin and Thompson, Neil and Yurochkin, Mikhail},
  journal={arXiv preprint arXiv:2309.15789},
  year={2023}
}

@article{ong2024routellm,
  title={Routellm: Learning to route llms with preference data},
  author={Ong, Isaac and Almahairi, Amjad and Wu, Vincent and Chiang, Wei-Lin and Wu, Tianhao and Gonzalez, Joseph E and Kadous, M Waleed and Stoica, Ion},
  journal={arXiv preprint arXiv:2406.18665},
  year={2024}
}

@article{ding2024hybrid,
  title={Hybrid LLM: Cost-efficient and quality-aware query routing},
  author={Ding, Dujian and Mallick, Ankur and Wang, Chi and Sim, Robert and Mukherjee, Subhabrata and Ruhle, Victor and Lakshmanan, Laks VS and Awadallah, Ahmed Hassan},
  journal={arXiv preprint arXiv:2404.14618},
  year={2024}
}

@article{stripelis2024tensoropera,
  title={TensorOpera Router: A Multi-Model Router for Efficient LLM Inference},
  author={Stripelis, Dimitris and Hu, Zijian and Zhang, Jipeng and Xu, Zhaozhuo and Shah, Alay Dilipbhai and Jin, Han and Yao, Yuhang and Avestimehr, Salman and He, Chaoyang},
  journal={arXiv preprint arXiv:2408.12320},
  year={2024}
}

@article{feng2024graphrouter,
  title={Graphrouter: A graph-based router for llm selections},
  author={Feng, Tao and Shen, Yanzhen and You, Jiaxuan},
  journal={arXiv preprint arXiv:2410.03834},
  year={2024}
}

@article{hutchins2000distributed,
  title={Distributed cognition},
  author={Hutchins, Edwin},
  journal={International Encyclopedia of the Social and Behavioral Sciences. Elsevier Science},
  volume={138},
  pages={1--10},
  year={2000}
}

@inproceedings{muqeethlearning,
  title={Learning to Route Among Specialized Experts for Zero-Shot Generalization},
  author={Muqeeth, Mohammed and Liu, Haokun and Liu, Yufan and Raffel, Colin},
  booktitle={Forty-first International Conference on Machine Learning},
  year={2024}
}

% cascading
@article{chen2023frugalgpt,
  title={Frugalgpt: How to use large language models while reducing cost and improving performance},
  author={Chen, Lingjiao and Zaharia, Matei and Zou, James},
  journal={arXiv preprint arXiv:2305.05176},
  year={2023}
}

@article{yue2023large,
  title={Large language model cascades with mixture of thoughts representations for cost-efficient reasoning},
  author={Yue, Murong and Zhao, Jie and Zhang, Min and Du, Liang and Yao, Ziyu},
  journal={arXiv preprint arXiv:2310.03094},
  year={2023}
}

@article{gupta2024language,
  title={Language Model Cascades: Token-level uncertainty and beyond},
  author={Gupta, Neha and Narasimhan, Harikrishna and Jitkrittum, Wittawat and Rawat, Ankit Singh and Menon, Aditya Krishna and Kumar, Sanjiv},
  journal={arXiv preprint arXiv:2404.10136},
  year={2024}
}

@inproceedings{duimproving,
  title={Improving Factuality and Reasoning in Language Models through Multiagent Debate},
  author={Du, Yilun and Li, Shuang and Torralba, Antonio and Tenenbaum, Joshua B and Mordatch, Igor},
  booktitle={Forty-first International Conference on Machine Learning},
  year={2024}
}

@inproceedings{feng-etal-2024-dont,
    title = "Don't Hallucinate, Abstain: Identifying {LLM} Knowledge Gaps via Multi-{LLM} Collaboration",
    author = "Feng, Shangbin  and
      Shi, Weijia  and
      Wang, Yike  and
      Ding, Wenxuan  and
      Balachandran, Vidhisha  and
      Tsvetkov, Yulia",
    booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2024",
}

@inproceedings{fengknowledge,
  title={Knowledge Card: Filling LLMs' Knowledge Gaps with Plug-in Specialized Language Models},
  author={Feng, Shangbin and Shi, Weijia and Bai, Yuyang and Balachandran, Vidhisha and He, Tianxing and Tsvetkov, Yulia},
  booktitle={The Twelfth International Conference on Learning Representations},
  year={2024}
}

@article{peng2023check,
  title={Check your facts and try again: Improving large language models with external knowledge and automated feedback},
  author={Peng, Baolin and Galley, Michel and He, Pengcheng and Cheng, Hao and Xie, Yujia and Hu, Yu and Huang, Qiuyuan and Liden, Lars and Yu, Zhou and Chen, Weizhu and others},
  journal={arXiv preprint arXiv:2302.12813},
  year={2023}
}

@inproceedings{burnsweak,
  title={Weak-to-Strong Generalization: Eliciting Strong Capabilities With Weak Supervision},
  author={Burns, Collin and Izmailov, Pavel and Kirchner, Jan Hendrik and Baker, Bowen and Gao, Leo and Aschenbrenner, Leopold and Chen, Yining and Ecoffet, Adrien and Joglekar, Manas and Leike, Jan and others},
  booktitle={Forty-first International Conference on Machine Learning},
  year={2024}
}

@inproceedings{wu2024autogen,
  title={AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation},
  author={Wu, Qingyun and Bansal, Gagan and Zhang, Jieyu and Wu, Yiran and Li, Beibin and Zhu, Erkang and Jiang, Li and Zhang, Xiaoyun and Zhang, Shaokun and Liu, Jiale and others},
  booktitle={ICLR 2024 Workshop on Large Language Model (LLM) Agents},
  year={2024}
}

@article{guo2024large,
  title={Large language model based multi-agents: A survey of progress and challenges},
  author={Guo, Taicheng and Chen, Xiuying and Wang, Yaqi and Chang, Ruidi and Pei, Shichao and Chawla, Nitesh V and Wiest, Olaf and Zhang, Xiangliang},
  journal={arXiv preprint arXiv:2402.01680},
  year={2024}
}

@article{zhao2024language,
  title={Language Model Council: Benchmarking Foundation Models on Highly Subjective Tasks by Consensus},
  author={Zhao, Justin and Plaza-del-Arco, Flor Miriam and Curry, Amanda Cercas},
  journal={arXiv preprint arXiv:2406.08598},
  year={2024}
}

@inproceedings{zhaocompeteai,
  title={CompeteAI: Understanding the Competition Dynamics of Large Language Model-based Agents},
  author={Zhao, Qinlin and Wang, Jindong and Zhang, Yixuan and Jin, Yiqiao and Zhu, Kaijie and Chen, Hao and Xie, Xing},
  booktitle={Forty-first International Conference on Machine Learning},
  year={2024}
}

@inproceedings{ilharcoediting,
  title={Editing models with task arithmetic},
  author={Ilharco, Gabriel and Ribeiro, Marco Tulio and Wortsman, Mitchell and Schmidt, Ludwig and Hajishirzi, Hannaneh and Farhadi, Ali},
  booktitle={The Eleventh International Conference on Learning Representations},
  year={2023}
}

@inproceedings{yu2024language,
  title={Language models are super mario: Absorbing abilities from homologous models as a free lunch},
  author={Yu, Le and Yu, Bowen and Yu, Haiyang and Huang, Fei and Li, Yongbin},
  booktitle={Forty-first International Conference on Machine Learning},
  year={2024}
}

@article{yadav2024ties,
  title={Ties-merging: Resolving interference when merging models},
  author={Yadav, Prateek and Tam, Derek and Choshen, Leshem and Raffel, Colin A and Bansal, Mohit},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{huang2023lorahub,
  title={Lorahub: Efficient cross-task generalization via dynamic lora composition},
  author={Huang, Chengsong and Liu, Qian and Lin, Bill Yuchen and Pang, Tianyu and Du, Chao and Lin, Min},
  journal={arXiv preprint arXiv:2307.13269},
  year={2023}
}

@article{feng2024model,
  title={Model Swarms: Collaborative Search to Adapt LLM Experts via Swarm Intelligence},
  author={Feng, Shangbin and Wang, Zifeng and Wang, Yike and Ebrahimi, Sayna and Palangi, Hamid and Miculicich, Lesly and Kulshrestha, Achin and Rauschmayr, Nathalie and Choi, Yejin and Tsvetkov, Yulia and others},
  journal={arXiv preprint arXiv:2410.11163},
  year={2024}
}

@inproceedings{bansalllm,
  title={LLM Augmented LLMs: Expanding Capabilities through Composition},
  author={Bansal, Rachit and Samanta, Bidisha and Dalmia, Siddharth and Gupta, Nitish and Ganapathy, Sriram and Bapna, Abhishek and Jain, Prateek and Talukdar, Partha},
  booktitle={The Twelfth International Conference on Learning Representations},
  year={2024}
}

@inproceedings{wangfusing,
  title={Fusing Models with Complementary Expertise},
  author={Wang, Hongyi and Polo, Felipe Maia and Sun, Yuekai and Kundu, Souvik and Xing, Eric and Yurochkin, Mikhail},
  booktitle={The Twelfth International Conference on Learning Representations},
  year={2024}
}

@article{guo2025deepseek,
  title={DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning},
  author={Guo, Daya and Yang, Dejian and Zhang, Haowei and Song, Junxiao and Zhang, Ruoyu and Xu, Runxin and Zhu, Qihao and Ma, Shirong and Wang, Peiyi and Bi, Xiao and others},
  journal={arXiv preprint arXiv:2501.12948},
  year={2025}
}

@article{tambon2024assessing,
  title={Assessing programming task difficulty for efficient evaluation of large language models},
  author={Tambon, Florian and Nikanjam, Amin and Khomh, Foutse and Antoniol, Giuliano},
  journal={arXiv preprint arXiv:2407.21227},
  year={2024}
}

@inproceedings{hulora,
  title={LoRA: Low-Rank Adaptation of Large Language Models},
  author={Hu, Edward J and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu and others},
  booktitle={International Conference on Learning Representations},
  year={2021}
}

@inproceedings{asaiself,
  title={Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection},
  author={Asai, Akari and Wu, Zeqiu and Wang, Yizhong and Sil, Avirup and Hajishirzi, Hannaneh},
  booktitle={The Twelfth International Conference on Learning Representations},
  year={2024}
}

@inproceedings{jiang2023active,
  title={Active Retrieval Augmented Generation},
  author={Jiang, Zhengbao and Xu, Frank F and Gao, Luyu and Sun, Zhiqing and Liu, Qian and Dwivedi-Yu, Jane and Yang, Yiming and Callan, Jamie and Neubig, Graham},
  booktitle={Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing},
  pages={7969--7992},
  year={2023}
}

@inproceedings{jiang2023llm,
  title={LLM-Blender: Ensembling Large Language Models with Pairwise Ranking and Generative Fusion},
  author={Jiang, Dongfu and Ren, Xiang and Lin, Bill Yuchen},
  booktitle={Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={14165--14178},
  year={2023}
}

@article{yadav2024survey,
  title={A survey on model moerging: Recycling and routing among specialized experts for collaborative learning},
  author={Yadav, Prateek and Raffel, Colin and Muqeeth, Mohammed and Caccia, Lucas and Liu, Haokun and Chen, Tianlong and Bansal, Mohit and Choshen, Leshem and Sordoni, Alessandro},
  journal={arXiv preprint arXiv:2408.07057},
  year={2024}
}

@article{subramaniam2025multiagent,
  title={Multiagent Finetuning: Self Improvement with Diverse Reasoning Chains},
  author={Subramaniam, Vighnesh and Du, Yilun and Tenenbaum, Joshua B and Torralba, Antonio and Li, Shuang and Mordatch, Igor},
  journal={arXiv preprint arXiv:2501.05707},
  year={2025}
}

@inproceedings{duposition,
  title={Position: Compositional Generative Modeling: A Single Model is Not All You Need},
  author={Du, Yilun and Kaelbling, Leslie Pack},
  booktitle={Forty-first International Conference on Machine Learning},
  year={2024}
}

@inproceedings{minsilo,
  title={SILO Language Models: Isolating Legal Risk In a Nonparametric Datastore},
  author={Min, Sewon and Gururangan, Suchin and Wallace, Eric and Shi, Weijia and Hajishirzi, Hannaneh and Smith, Noah A and Zettlemoyer, Luke},
  booktitle={The Twelfth International Conference on Learning Representations},
  year={2024}
}

@article{li2022branch,
  title={Branch-train-merge: Embarrassingly parallel training of expert language models},
  author={Li, Margaret and Gururangan, Suchin and Dettmers, Tim and Lewis, Mike and Althoff, Tim and Smith, Noah A and Zettlemoyer, Luke},
  journal={arXiv preprint arXiv:2208.03306},
  year={2022}
}

@article{gururangan2023scaling,
  title={Scaling expert language models with unsupervised domain discovery},
  author={Gururangan, Suchin and Li, Margaret and Lewis, Mike and Shi, Weijia and Althoff, Tim and Smith, Noah A and Zettlemoyer, Luke},
  journal={arXiv preprint arXiv:2303.14177},
  year={2023}
}

@inproceedings{kudugunta2023matformer,
  title={Matformer: Nested transformer for elastic inference},
  author={Devvrit, Fnu and Kudugunta, Sneha and Kusupati, Aditya and Dettmers, Tim and Chen, Kaifeng and Dhillon, Inderjit S and Tsvetkov, Yulia and Hajishirzi, Hannaneh and Kakade, Sham M and Farhadi, Ali and and Jain, Prateek},
  booktitle={NeurIPS},
  year={2024}
}

@article{jang2023personalized,
  title={Personalized soups: Personalized large language model alignment via post-hoc parameter merging},
  author={Jang, Joel and Kim, Seungone and Lin, Bill Yuchen and Wang, Yizhong and Hessel, Jack and Zettlemoyer, Luke and Hajishirzi, Hannaneh and Choi, Yejin and Ammanabrolu, Prithviraj},
  journal={arXiv preprint arXiv:2310.11564},
  year={2023}
}

@article{subramaniam2024debategpt,
  title={DebateGPT: Fine-tuning Large Language Models with Multi-agent Debate Supervision},
  author={Subramaniam, Vighnesh and Torralba, Antonio and Li, Shuang},
  year={2024}
}

@article{naous2023having,
  title={Having beer after prayer? measuring cultural bias in large language models},
  author={Naous, Tarek and Ryan, Michael J and Ritter, Alan and Xu, Wei},
  journal={arXiv preprint arXiv:2305.14456},
  year={2023}
}

@inproceedings{kumar2023language,
  title={Language Generation Models Can Cause Harm: So What Can We Do About It? An Actionable Survey},
  author={Kumar, Sachin and Balachandran, Vidhisha and Njoo, Lucille and Anastasopoulos, Antonios and Tsvetkov, Yulia},
  booktitle={Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics},
  pages={3299--3321},
  year={2023}
}

@article{gallegos2024bias,
  title={Bias and fairness in large language models: A survey},
  author={Gallegos, Isabel O and Rossi, Ryan A and Barrow, Joe and Tanjim, Md Mehrab and Kim, Sungchul and Dernoncourt, Franck and Yu, Tong and Zhang, Ruiyi and Ahmed, Nesreen K},
  journal={Computational Linguistics},
  pages={1--79},
  year={2024},
  publisher={MIT Press 255 Main Street, 9th Floor, Cambridge, Massachusetts 02142, USA~…}
}

@article{kumar2024compo,
  title={ComPO: Community Preferences for Language Model Personalization},
  author={Kumar, Sachin and Park, Chan Young and Tsvetkov, Yulia and Smith, Noah A and Hajishirzi, Hannaneh},
  journal={arXiv preprint arXiv:2410.16027},
  year={2024}
}

@article{kirk2024prism,
  title={The PRISM Alignment Project: What Participatory, Representative and Individualised Human Feedback Reveals About the Subjective and Multicultural Alignment of Large Language Models},
  author={Kirk, Hannah Rose and Whitefield, Alexander and R{\"o}ttger, Paul and Bean, Andrew and Margatina, Katerina and Ciro, Juan and Mosquera, Rafael and Bartolo, Max and Williams, Adina and He, He and others},
  journal={arXiv preprint arXiv:2404.16019},
  year={2024}
}

@article{jung2024trust,
  title={Trust or Escalate: LLM Judges with Provable Guarantees for Human Agreement},
  author={Jung, Jaehun and Brahman, Faeze and Choi, Yejin},
  journal={arXiv preprint arXiv:2407.18370},
  year={2024}
}

@article{yao2024survey,
  title={A survey on large language model (llm) security and privacy: The good, the bad, and the ugly},
  author={Yao, Yifan and Duan, Jinhao and Xu, Kaidi and Cai, Yuanfang and Sun, Zhibo and Zhang, Yue},
  journal={High-Confidence Computing},
  pages={100211},
  year={2024},
  publisher={Elsevier}
}

@inproceedings{karamolegkou2023copyright,
  title={Copyright Violations and Large Language Models},
  author={Karamolegkou, Antonia and Li, Jiaang and Zhou, Li and S{\o}gaard, Anders},
  booktitle={The 2023 Conference on Empirical Methods in Natural Language Processing},
  year={2023}
}

@inproceedings{mireshghallahcan,
  title={Can LLMs Keep a Secret? Testing Privacy Implications of Language Models via Contextual Integrity Theory},
  author={Mireshghallah, Niloofar and Kim, Hyunwoo and Zhou, Xuhui and Tsvetkov, Yulia and Sap, Maarten and Shokri, Reza and Choi, Yejin},
  booktitle={The Twelfth International Conference on Learning Representations},
  year={2024}
}

@inproceedings{carlini2021extracting,
  title={Extracting training data from large language models},
  author={Carlini, Nicholas and Tramer, Florian and Wallace, Eric and Jagielski, Matthew and Herbert-Voss, Ariel and Lee, Katherine and Roberts, Adam and Brown, Tom and Song, Dawn and Erlingsson, Ulfar and others},
  booktitle={30th USENIX Security Symposium (USENIX Security 21)},
  pages={2633--2650},
  year={2021}
}

@article{zhang2024cogenesis,
  title={Cogenesis: A framework collaborating large and small language models for secure context-aware instruction following},
  author={Zhang, Kaiyan and Wang, Jianyu and Hua, Ermo and Qi, Biqing and Ding, Ning and Zhou, Bowen},
  journal={arXiv preprint arXiv:2403.03129},
  year={2024}
}
@inproceedings{yang-etal-2024-large-language-models,
    title = "Do Large Language Models Latently Perform Multi-Hop Reasoning?",
    author = "Yang, Sohee  and
      Gribovskaya, Elena  and
      Kassner, Nora  and
      Geva, Mor  and
      Riedel, Sebastian",
    booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    year = "2024",
}
@article{yin2024lofit,
  title={LoFiT: Localized Fine-tuning on LLM Representations},
  author={Yin, Fangcong and Ye, Xi and Durrett, Greg},
  journal={arXiv preprint arXiv:2406.01563},
  year={2024}
}
@article{meng2022locating,
  title={Locating and Editing Factual Associations in {GPT}},
  author={Kevin Meng and David Bau and Alex Andonian and Yonatan Belinkov},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2022},
  note={arXiv:2202.05262}
}
@inproceedings{stolfo-etal-2023-mechanistic,
    title = "A Mechanistic Interpretation of Arithmetic Reasoning in Language Models using Causal Mediation Analysis",
    author = "Stolfo, Alessandro  and
      Belinkov, Yonatan  and
      Sachan, Mrinmaya",
    booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
    year = "2023",
}
@inproceedings{nandaprogress,
  title={Progress measures for grokking via mechanistic interpretability},
  author={Nanda, Neel and Chan, Lawrence and Lieberum, Tom and Smith, Jess and Steinhardt, Jacob},
  booktitle={The Eleventh International Conference on Learning Representations},
year="2023",
}

@article{han2023ssd,
  title={Ssd-2: Scaling and inference-time fusion of diffusion language models},
  author={Han, Xiaochuang and Kumar, Sachin and Tsvetkov, Yulia and Ghazvininejad, Marjan},
  journal={arXiv preprint arXiv:2305.14771},
  year={2023}
}