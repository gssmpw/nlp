\section{Related Work}
Two recent works discuss related topics. 

\citet{yadav2024survey} present a taxonomy of model merging and mixture-of-experts (MoE) approaches, arguing for reusing and routing of existing expert models. They focus primarily on weight-level collaboration approaches, while we aggregate a broader family of methods with a broader definition of \emph{multi-LLM collaboration} where models could collaborate through four different levels of information exchange.

\citet{duposition} present a position paper arguing for compositional generative modeling, discussing the benefits of combining multiple modules across computer vision, reinforcement learning, robotics, and a brief mention of language. We specifically focus on language models and dive deep into LLM-specific arguments, methods, and future research.
%Colin paper, Yilun paper