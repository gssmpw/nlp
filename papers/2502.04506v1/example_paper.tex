\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2024} with \usepackage[nohyperref]{icml2024} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}
\newcommand{\Sref}[1]{\S\ref{#1}}
\newcommand{\draftcomment}[1]{#1}
\definecolor{purp}{HTML}{791f87}
\newcommand\alisa[1]{\draftcomment{\textcolor{purp}{\textbf{[Alisa: #1]}}}}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Multi-LLM Collaboration}

\begin{document}

\twocolumn[
\icmltitle{When One LLM Drools, Multi-LLM Collaboration Rules}
%\icmltitle{Position: Multi-LLM Collaboration: One LLM is Not Enough}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2024
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Shangbin Feng}{a1}
\icmlauthor{Wenxuan Ding}{a2}
\icmlauthor{Alisa Liu}{a1}
\icmlauthor{Zifeng Wang}{a3}
\icmlauthor{Weijia Shi}{a1}
\icmlauthor{Yike Wang}{a1}
\icmlauthor{Shannon Zejiang Shen}{a4}
\icmlauthor{Xiaochuang Han}{a1}
\icmlauthor{Hunter Lang}{a4}
\icmlauthor{Chen-Yu Lee}{a3}
\icmlauthor{Tomas Pfister}{a3}
\icmlauthor{Yejin Choi}{a5}
\icmlauthor{Yulia Tsvetkov}{a1}
% \icmlauthor{Firstname1 Lastname1}{equal,yyy}
% \icmlauthor{Firstname2 Lastname2}{equal,yyy,comp}
% \icmlauthor{Firstname3 Lastname3}{comp}
% \icmlauthor{Firstname4 Lastname4}{sch}
% \icmlauthor{Firstname5 Lastname5}{yyy}
% \icmlauthor{Firstname6 Lastname6}{sch,yyy,comp}
% \icmlauthor{Firstname7 Lastname7}{comp}
% %\icmlauthor{}{sch}
% \icmlauthor{Firstname8 Lastname8}{sch}
% \icmlauthor{Firstname8 Lastname8}{yyy,comp}
% %\icmlauthor{}{sch}
% %\icmlauthor{}{sch}
\end{icmlauthorlist}

\icmlaffiliation{a1}{University of Washington}
\icmlaffiliation{a2}{The University of Texas at Austin}
\icmlaffiliation{a3}{Google}
\icmlaffiliation{a4}{Massachusetts Institute of Technology}
\icmlaffiliation{a5}{Stanford University}

% \icmlaffiliation{yyy}{Department of XXX, University of YYY, Location, Country}
% \icmlaffiliation{comp}{Company Name, Location, Country}
% \icmlaffiliation{sch}{School of ZZZ, Institute of WWW, Location, Country}

\icmlcorrespondingauthor{Shangbin Feng}{shangbin@cs.washington.edu}
%\icmlcorrespondingauthor{Yulia Tsvetkov}{yuliats@cs.washington.edu}
% \icmlcorrespondingauthor{Firstname2 Lastname2}{first2.last2@www.uk}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
%\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
%\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
    This position paper argues that in many realistic (i.e., complex, contextualized, subjective) scenarios, one LLM is not enough to produce a reliable output. We challenge the status quo of relying solely on a single general-purpose LLM and argue for \emph{multi-LLM collaboration} to better represent the extensive diversity of data, skills, and people. We first posit that a single LLM underrepresents real-world data distributions, heterogeneous skills, and pluralistic populations, and that such representation gaps cannot be trivially patched by further training a single LLM. We then organize existing multi-LLM collaboration methods into a hierarchy, based on the level of access and information exchange, ranging from API-level, text-level, logit-level, to weight-level collaboration. Based on these methods, we highlight how multi-LLM collaboration addresses challenges that a single LLM struggles with, such as reliability, democratization, and pluralism. Finally, we identify the limitations of existing multi-LLM methods and motivate future work. We envision multi-LLM collaboration as an essential path toward compositional intelligence and collaborative AI development.
\end{abstract}

\section{Introduction}
\vspace{5pt}

%The successes of scaling models \citep{kaplan2020scaling} and data \citep{hoffmann2022training} have provided a rosy blueprint for general intelligence by developing a single gargantuan language model. 
The successes of scaling models \citep{kaplan2020scaling} and data \citep{hoffmann2022training} have fueled the overly optimistic hope that simply building an ever-larger language model is a path to achieving human-like intelligent AI models.
From research artifacts to user-facing products, the commercialization of LLM and AI technologies further reinforces this status quo: major players train a single general-purpose in-house LLM and compete by attempting to outrank other models \citep{bigtech}. This quest for the ``best'' single LLM---measured by leaderboard scores, user adoption, and profitability---has brought about the bloom of LLM research and development where new models emerge daily and the state-of-the-art is constantly reshaped \citep{liangholistic, chiangchatbot, guo2025deepseek}.

\begin{figure}
    \centering
    \includegraphics[width=0.95\linewidth]{icml2025/underrepresent.pdf}
    \vspace*{-20pt}
    \caption{Despite the quest for general-purpose models, a single LLM suffers from underrepresentation of data (language varieties, domains, styles), skills (reasoning abilities, linguistic and communication skills, creative capacities, and technical competencies), and people (opinions, values, cultural norms).
    %\alisa{I'm thinking that this figure doesn't add much value beyond what's already written in the text of this caption, and the content of Figure 2 would be a better fit for a Figure 1.}
    }
    \label{fig:teaser}
\end{figure}

In this position paper, we challenge the status quo by arguing that \textbf{one LLM is not enough} and advocate for \textbf{multi-LLM collaboration}, where multiple language models collaborate for compositional generative modeling. We first argue \emph{why} one LLM is not enough (\Sref{sec:why}): despite being general-purpose, a single monolithic model struggles to reflect the intricate diversity of the real world and \emph{underrepresents} the long tail of data types, model skills, and people. We then propose a taxonomy of multi-LLM collaboration protocols (\Sref{sec:how}) in which LLMs collaborate, interact, and exchange information at the API-level, text-level, logit-level, and weight-level, offering diverse modes of collaboration compatible with all stages of the LLM lifecycle and usage types. We then argue that multi-LLM systems empowered by these protocols bring out benefits that a single LLM struggles to reflect (\Sref{sec:what}): pluralism, democratization, efficiency, adaptability, and more. Together, these arguments demonstrate that multi-LLM collaboration is an important yet overlooked family of methods, and a promising approach to advance language technologies.

We also identify some limitations of existing multi-LLM collaboration protocols and applications (\Sref{sec:discussion}), which motivate us to lay out an actionable roadmap for future work beyond monolithic models and towards advancing modular multi-LLM systems. We hope that this position will be a call-to-action for the research community to propose, evaluate, and promote collaboration strategies and communication protocols for multi-LLM collaboration.

\section{One LLM Is Not Enough}
\label{sec:why}
\vspace{5pt}

From the early successes of scaling up model size \citep{kaplan2020scaling} and training data \citep{hoffmann2022training}, language technologies have transitioned from task-specific systems to ``general-purpose''  language models \citep{Brown2020LanguageMA}: by pretraining on gigantic corpora and aligning with extensive instruction tuning and preference optimization, one LLM can be prompted to solve a diverse range of tasks and problems, leading some to believe that the future of language technologies is in figuring out the recipe for scaling and developing a single omnipotent LLM. Despite its promise, we argue that a single LLM, as designed today, is not enough to achieve a truly reliable system: even with the best effort to curate data, design architectures, and improve model inference, a single LLM suffers from \emph{underrepresentation} on three fronts: data, skills, and people.

\paragraph{Underrepresentation of data.} Despite extensive data curation efforts, a single LLM is ultimately trained on a static snapshot of what is readily available, and there are always elements in the real-world language distributions that are missing or down-weighted in this static snapshot \citep{lazaridou2021pitfalls}. For example, constant changes in the state of the world after the time of data collection quickly make the parametric information of LLMs outdated \citep{dhingra2022time, kasai2024realtime}. Private and copyrighted texts would require careful consideration in LLM training, but are otherwise essential for personalization and context \citep{weievaluating, chen2024copybench}.  Languages, dialects, language varieties in the long tail of data distributions are easily outnumbered and overshadowed by the majority languages/dialects \citep{song2023globalbench, faisal-etal-2024-dialectbench}. Evolving trends, unspoken cultural and social norms essential for socially-aware LLM applications, commonsense and implicit knowledge are hard to pin down with static data snapshots \citep{rao2024normad, shi2024culturebank}. The list goes on, and much of the real-world variation expressed through language will inevitably be lost when we solely rely on a single LLM with a static hodgepodge of training corpora.

\paragraph{Underrepresentation of skills.} Earlier language technologies were defined by task-specific progress with specialized methods, models, and subcommunities of experts for tasks like machine translation, summarization, question answering, and natural language inference \citep{sun2022paradigm}. LLMs broke from this trend by being seemingly ``general-purpose'' and it appears plausible that all we will need in the near future is a single omnipotent LLM that works best in any task and context.

However, no single LLM is Pareto-optimal \emph{empirically} and it is prohibitively expensive (if not impossible) to optimize for a single model that outperforms all other models on \emph{all} skills. For example, Gemini \citep{team2023gemini} currently ranks best on Chatbot Arena \citep{chiangchatbot} focusing on instruction following, GPT-4o \citep{achiam2023gpt} is best on the HELM leaderboard \citep{liangholistic} with an emphasis on QA and math reasoning, while a fine-tuned version of InternLM \citep{team2023internlm} is best on textual and algorithmic tasks in Big-Bench Hard \citep{suzgun2023challenging} on Open LLM Leaderboard \citep{open-llm-leaderboard-v2}.\footnote{Leaderboards accessed on Nov 24, 2024.} These models would all rankly poorly on GlobalBench \citep{song2023globalbench} and DialectBench \citep{faisal-etal-2024-dialectbench} compared to multilingual LLMs, where tasks include languages and language varieties not captured in the most popular leaderboards. This demonstrates that even the most advanced LLMs have major limitations in skills and task coverage, and that additional specialization of models is critical.

% \alisa{how this paragraph supports the argument is unclear $\rightarrow$}
% In addition, the diverse skills of state-of-the-art LLMs are often demonstrated through prompting that  works disproportionately for different tasks and contexts. For example, the benefit of chain-of-thought prompting is great for math and symbolic reasoning but poor and even counterproductive for classification and linguistics tasks \citep{sprague2024cot}. These prompting efforts are further encumbered by challenges such as knowledge conflicts \citep{xieadaptive, wang2023resolving}, prompt sensitivity \citep{sclarquantifying}, and long context \citep{liu2024lost}. As a result, a single LLM made ``general-purpose'' with prompting could still be unreliable with tradeoffs and underreprensentations of skills and capabilities.

\paragraph{Underrepresentation of people.} All LLMs are ultimately used by people with diverse needs, pluralistic values, and varying socio-cultural backgrounds. Despite the ever-increasing model size and benchmark scores, we witness a constant lack of representation of actual LLM users.

On one hand, a single LLM struggles to reflect pluralistic human values, cultures, and social contexts \citep{sorensenposition, feng2024modular, leibo2024theory}, in any language. LLM users are not homogeneous, bringing a wealth of perspectives and diversity that reflects and shapes our world: despite the potential diversity in data sources, even state-of-the-art LLMs cannot equitably serve the entire spectrum of users by reflecting such heterogeneity. For example, LLMs often feature a West-centric cultural persona \citep{naous2023having} and struggle to adapt to cultural variation \citep{rao2024normad}; a single LLM would most likely reinforce the majority class in training data and exhibit biases in opinions and perspectives \citep{santurkar2023whose, feng2023pretraining}; user agency often remains overlooked since monolithic LLMs lack steerability and controllability in value-laden instructions and contexts \citep{sorensen2024value}. Since LLMs are already trained on diverse texts from the web, representing populations would require solutions beyond scaling data for a general-purpose LLM.

Moreover, by solely relying on one single model we are also solely relying on only one team of model developers. With the increasing cost and opaqueness of independently developing an LLM, these teams are becoming highly homogeneous: big tech companies, researchers with advanced degrees, overrepresentation of certain demographic groups are common sketches of the teams behind state-of-the-art LLMs \citep{workforce}. However, this leaves the vast majority of actual and underprivileged LLM users without a say in the decision making of model training and development, while they can only access these LLMs which might not have been developed with their needs and priorities in mind. An open and collaborative development approach that is the cornerstone of open-source communities \citep{johnson2006collaboration} is thus neglected in the over-focus on chasing the best single model, underrepresenting the voices and needs of actual LLM users that go beyond synthetic benchmark numbers.

\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{icml2025/overview.pdf}
    \vspace*{-20pt}
    \caption{We propose a typology of multi-LLM collaboration approaches, focusing on different levels of access to LLMs, and survey existing methods that fall into each type.}
    \label{fig:overview}
\end{figure*}

\paragraph{Challenges to Improve One Model's Coverage} %Representation} 
A tempting solution to these problems of underrepresentation is to further train the current best LLM to improve the representation of data, skills, and users. We argue that this band-aid approach is challenging at best:

When \emph{data} is underrepresented, model developers can scrape from previously unseen domains and perform further fine-tuning. However, it is costly to frequently re-train and update model versions with gigantic LLMs, while private and copyrighted data simply should not be included in LLM training data. Retrieval-augmented geneartion \citep{guu2020retrieval, shi2023replug} could provide unseen data as context, but it is unclear whether LLMs would fully leverage the context \citep{shi-etal-2024-trusting} and to what extent is this steerability reliable \citep{sprague2024cot}.

When \emph{skills} are underrepresented, model developers can derive targeted supervised fine-tuning data for continual learning \citep{zhang2023instruction}. However, tuning to patch a weakness in skills may lead to tradeoffs in other tasks and sometimes even catastrophic forgetting \citep{luo2023empirical, lin2024mitigating}, as any specialization on the trained model might harm its general-purpose utility.

When \emph{humans} are underrepresented, model developers can survey the needs of diverse populations and communities for LLMs and invite collaborative contributions \citep{fengknowledge}. However, there is little to no incentive for teams behind commercial state-of-the-art LLMs to take great strides towards equitable language technologies without obvious profitable gains.

It is important to note that these underrepresentation issues of a single LLM, especially with respect to data and skills, are grounded in \emph{empirical evidence}, i.e., current state-of-the-art LLMs are suffering from these challenges. There might emerge future ``perfect'' algorithms/architectures/etc.~that fully address these issues, but given that multi-LLM collaboration research is \emph{already demonstrating empirical benefits} in addressing these issues, we advocate for multi-LLM collaboration as a promising and effective research avenue. %  it more and seek to advance language technologies beyond a single model.

% As such, we posit that a single LLM will inevitably underrepresent the diversity and variation of data, skills, and people. This should motivate the research community to look for alternatives beyond scaling a single LLM. In the following, we argue that \emph{multi-LLM collaboration} is a promising direction for patching and resolving these problems of underrepresentation in language technologies.

\section{Types of Multi-LLM Collaboration}
\label{sec:how}
\vspace{5pt}

We categorize existing (often unrelated) method into a conceptual family of multi-LLM collaboration strategies, organizing the methods by (1) collaboration at different levels of access to an LLM, as illustrated in Figure~\ref{fig:overview}, and (2) collaboration at different stages of LLM's lifecycle: (pre)training, post-training, and inference.

\subsection{Collaboration at different levels of model access}
\paragraph{API-Level} As the name suggests, access to an LLM's  API is sufficient to enable API-level collaboration between models. Such strategies focus on the dynamic selection of the most cost-efficient and high-performing model among a diverse pool of LLMs for different inputs. Intuitively, we should assign simpler requests to smaller \citep{tambon2024assessing}, more efficient LLMs for \emph{reduced cost and latency}, and domain-specific requests to expert LLMs for \emph{improved performance}. There are two mainstream lines of research on API-level LLM collaboration: \emph{Routing}~\citep{hu2024routerbench} and \emph{Cascading}~\citep{chen2023frugalgpt}. 

\emph{Routing} selects the most suitable model only based on the input, without performing inference on any LLM. A typical routing paradigm involves designing a separate router model that learns from human preferences. As a key step, preference labels~\citep{ong2024routellm} that represent the relative response quality of different LLMs are collected for each input.
%Researchers have 
Prior work developed various router models to learn from input--preference pairs, including non-parametric routers like KNN-based router~\citep{shnitzer2023large}, and parametric routers like MLP-~\citep{hu2024routerbench}, encoder-~\citep{ding2024hybrid}, and decoder-based~\citep{ong2024routellm} routers. Beyond preference data, additional information can be leveraged to assist in routing decision-making. To select the most suitable expert LLM, domain-specific routing strategies~\citet{lu2023routing, stripelis2024tensoropera} extract key information about the task and domain directly from the input. \citet{feng2024graphrouter} further introduce a heterogeneous graph framework to leverage contextual interactions among tasks, queries, and LLMs.

\emph{Cascading} defers the input to larger/more capable LLMs when the response from the smaller LLM is not satisfactory enough. The crux of cascading is the \emph{deferral rule} to determine whether to terminate and return the prediction or to invoke the next LLM. The pioneering work Frugal GPT~\cite{chen2023frugalgpt} trains a regression model that predicts a response quality score and establishes the deferral rule by thresholding the predicted score. \citet{yue2023large} presents a consistency-based approach that estimates the response confidence score, such that inputs with low response confidence are deferred to the next LLM. \citet{gupta2024language} further incorporate token-level uncertainty into deferral rules. Cascading strategies, while potentially improving overall quality by leveraging additional signal from smaller LLMs, often come with increased cost and latency due to the overhead of decoding intermediate responses.


\paragraph{Text-Level} Text-level approaches enable multi-LLM collaboration through exchanges of generated texts, where one LLM's output becomes another LLM's input. They usually follow a conversational setting where LLMs can ``cooperate'' or ``compete'' with each other.

For cooperation, models can \emph{divide and conquer} complex problems through multi-agent systems where each agent is seeded by different models/prompts \citep{wu2024autogen, guo2024large}; specialized models can \emph{augment} general-purpose LLMs to patch their gaps \citep{fengknowledge, shen-etal-2024-learning}; one LLM can generate \emph{feedback} or perform verification for another LLM's outputs and consequently refine the generation \citep{burnsweak, feng-etal-2024-dont}.

For \emph{competition}, multiple LLMs can ``debate'' with each other to advance factuality and reasoning \citep{liang2023encouraging, duimproving}. Recent research also explored employing a pool of diverse specialized LLMs to model social \citep{zhao2024language} and economic \citep{zhaocompeteai} behavior to simulate the real-world environment.

Much effort of multi-LLM collaboration research currently operates at the text-level, probably because such interaction allows for the use of APIs in closed models, the ease of engineering to redirect model outputs, and transparency through intermediate model outputs. However, text-level multi-LLM collaboration also faces challenges such as error propagation from outputs of individual models, the lack of generalization across tasks, and the costs of model inference for multiple LLMs.

\paragraph{Logit-Level}

LLMs may also collaborate by jointly contributing to each next-token prediction. 
In this case, the logit-level predictions of multiple LLMs are combined via arithmetics to create a single next-token logit distribution, which is then normalized into a probability distribution.
This approach uses other LLMs as ``experts'' and/or ``anti-experts'', whose predictions are additively or negatively combined in the prediction, respectively.

Using an anti-expert achieves the effect of steering \textit{away} from the preferences of that model, and is also known as \textit{contrastive decoding} \citep{li-etal-2023-contrastive}.
For instance, the anti-expert may be an LLM tuned explicitly to be toxic \citep{liu-etal-2021-dexperts}, to achieve safer generations, or a smaller LLM \cite{li-etal-2023-contrastive}, to avoid the pitfalls of weaker LMs for better open-ended generation.
In fact, the anti-expert does not need to be a distinct LLM, and can instead be the result of ablating some part of the current LLM, e.g., by withholding necessary context \citep{pei-etal-2023-preadd, sennrich-etal-2024-mitigating, Leng_2024_CVPR, shi-etal-2024-trusting} or early-exiting from an earlier layer of the transformer model \citep{gera-etal-2023-benefits, chuang2024dola}.

On the other hand, using multiple expert LLMs combines their predictions in a product-of-experts fashion.
Intuitively, this leads to next-token predictions that are high-probability under all LLMs.
This has been used to achieve decoding-time adaptation of LLMs using small tuned experts with a large pretrained LLM \cite{liu-etal-2024-tuning, mitchell2024an}, allowing for on-the-fly customization of the weights of multiple finetuning objectives \cite{shi-etal-2024-decoding}.

The weights assigned to experts and anti-experts may also be automatically determined at each time step \cite{mavromatis2024pack, fan2024on, du2024mogu}.
At the extreme, this takes the form of token-level routing among models \citep{shen-etal-2024-learning}.

\paragraph{Weight-Level} 
The collaboration of multiple LLMs through parameter-level collaboration has been explored using paradigms such as mixtures of feed-forward layers \citep{sukhbaatar2024branch}, adapters \citep{wang2022adamixmixtureofadaptationsparameterefficientmodel, pfeiffer2020adapterfusion}, and low-rank adaptation (LoRA) experts \citep{wu2024mixture}. In this paradigm, components like feed-forward layers and adapters are first trained independently on domain-specific or task-specific data to achieve specialization. Subsequently, in a combination stage, these independently trained modules are jointly optimized to collaborate effectively, creating a unified system that benefits from the specialized expertise of each component.
\vspace{5pt}

This framework supports collaboration across varying levels of input granularity the way experts are selected and aggregated. For example, some approaches dynamically select modules for individual \emph{tokens} \citep{vaswani2017attention, houlsby2019parameter, pfeiffer2020adapterfusion, belofsky2023token, wu2024mixture, sukhbaatar2024branch}, enabling fine-grained expertise sharing. 
Others perform collaboration at the \emph{sentence} level \citep{diao2023mixture, xu2023wizardlm}, where different input sentences activate different modules. At the \emph{task} level, methods such as \citet{chiang2024chatbot} assign a single expert model to all examples from a particular task.
Weight-level collaboration typically allows for deeper integration of experts by enabling routing decisions at each layer where modules are inserted, offering greater flexibility and adaptability to diverse tasks and data.
\vspace{5pt}

Another line of weight-level collaboration research is the merging/composition of model weights across multiple LLMs. These approaches mainly vary by \emph{data dependency}, i.e., how much task-specific data is required to compose and adapt models. \emph{Zero-shot} model composition approaches rely on heuristics about model weights \citep{yu2024language, yadav2024ties} or task arithmetic \citep{ilharcoediting} to produce composed models and advance generalization without access to task data. Given a small set of task data, \emph{dynamic composition} approaches optimize the model composition based on performance and metrics on the task data \citep{huang2023lorahub} with perplexity heuristics \citep{mavromatis2024pack} and evolutionary algorithms \citep{feng2024model}. If supervised data is abundant, \emph{learn-to-fuse} approaches design trainable modules \citep{bansalllm}, adapters \citep{wangfusing}, or even LLMs \citep{jiang2023llm} to ``glue'' multiple LLMs together: the component LLMs are often kept frozen while the trainable parts go through supervised fine-tuning from scratch. Weight-level approaches offer a spectrum of solutions based on data availability, and the \emph{many-to-one} setup offers reduced inference costs. However, weight-level approaches are less interpretable in how model expertise is composed and do not tap into the power of collaborative generation like text- or logit-level approaches.

\subsection{Collaboration at different stages of LLM development}
We can also categorize multi-LLM collaboration approaches by the three stages of the LLM lifecycle: \emph{(pre)training, post-training, and inference}. Pretraining-time approaches focus on partitioning LLM training data \citep{gururangan2023scaling} and training multiple specialized LLMs separately \citep{li2022branch} or at the same time \citep{kudugunta2023matformer}. Post-training approaches explore collaborative alignment with modular reward models \citep{jang2023personalized}, multi-LLM self-alignment \citep{feng2024modular}, or constructing synthetic supervised fine-tuning data through multi-LLM debate \citep{subramaniam2024debategpt, subramaniam2025multiagent}. The vast majority of multi-LLM collaboration approaches currently operate at inference time, offering diverse ways of reusing existing models spanning all four collaboration levels \citep{hu2024routerbench, duimproving, liu-etal-2024-tuning}. In general, weight-level methods often require more (pre)training and post-training efforts, while API-level/logit-level collaborations focus more on inference-time solutions.

By conceptually structuring and organizing these (originally unrelated) methods into a family of approaches, we argue that multi-LLM collaboration research offers flexible methodologies for any level of model access across all stages in the LLM lifecycle, providing an alternative and promising school of thought to advance language technologies.

\section{The Promise of Multi-LLM Collaboration}
\label{sec:what}
\vspace{5pt}

Multi-LLM collaboration systems offer unique advantages over single general-purpose models: we summarize the methodological and empirical benefits of existing multi-LLM proposals in this section.

\paragraph{Factuality and reliability} Despite prior efforts \citep{shi2023replug, Press2022MeasuringAN, feng2023cook} to expand the knowledge of LLMs, knowledge gaps---missing or outdated information in LLMs---may persist due to the ever-evolving nature of knowledge, presenting challenges to the reliability of LLM responses. Self-reflection \citep{wang2022self, xu2024sayself, shinn2024reflexion, madaan2024self}, where a single LLM critically evaluates its own generations, is used in decoding, confidence calibration, and inference to improve factual accuracy and mitigate hallucinations. However, this method suffers from confirmation biases \citep{ji2023survey} and relies on the assumption that LLMs can generate novel reflections from their initial outputs \citep{liang2023encouraging}. Recent studies address these issues by promoting collaboration between multiple LLMs. With distinct knowledge gaps, LLMs evaluate and reflect on each other's outputs, collaboratively probing and identifying the knowledge gaps of each other. Specifically, \citet{feng2024don} enable robust LLM abstention through multi-LLM collaboration to reflect on generated text in cooperative or competitive settings. \citet{cohen2023lm} employ cross-examination to detect errors in LLM generations. Other studies \citep{xiong2023examining, liang2023encouraging, duimproving} suggest that multiple LLMs could propose and debate their individual responses and reasoning processes over multiple rounds to arrive at a common final answer, and LLMs with comparable abilities have been shown to demonstrate such collaborative spirit \citep{xiong2023examining}. Given its superior performance in various experiment settings, we believe that multi-LLM collaboration offers a promising way to further improve the factual validity of generated context and reduce fallacious answers and hallucinations that contemporary models are prone to.

\paragraph{Alignment and pluralism} State-of-the-art LLMs are documented with all kinds of cultural \citep{naous2023having}, political \citep{santurkar2023whose}, and broadly social biases \citep{kumar2023language}. This comes with the fact that these models have already seen ``diverse'' web data that should serve as a decentralized representation of real-world diversity. Much research attributes this to LLMs learning disproportionately from and hence reinforcing the majority in training data \citep{feng2023pretraining, gallegos2024bias}, thus scaling data diversity used in training a single LLM is not an effective solution. We see an increasing line of work focused on \emph{modular multi-LLM systems} to alleviate these biases, including modular plug-ins \citep{feng2024modular}, multi-LLM as a judge \citep{zhao2024language}, and employing multiple and compositional reward models \citep{jang2023personalized}. Together with data-side modularity spanning diverse communities \citep{kumar2024compo, kirk2024prism} we believe multi-LLM collaboration offers a modular and flexible solution to addressing the fairness and pluralism challenges of LLMs.

\paragraph{Efficiency} The most capable LLMs at the moment often feature gargantuan sizes and prohibitively high inference costs. However, not all queries require such computation overhead: by employing multi-LLM collaboration across sizes/expertise the largest model doesn't need to be called every single time. MatFormer \citep{kudugunta2023matformer} simultaneously trains modules of varying sizes in a nested architecture and could be selectively activated to result in LLMs of varying sizes given the compute budget. Instead of training an LLM on \emph{all} the data, approaches such as Branch-Train-Merge \citep{li2022branch} leverage the modularity of data provenance to train a pool of LLM experts and dynamically aggregated for inference. A growing line of research also investigates \emph{defer} and \emph{backoff} behavior between models of varying sizes and/or specialization \citep{shen-etal-2024-learning, jung2024trust}. These approaches highlight multi-LLM collaboration as a promising direction to balance utility and training/inference efficiency.
%Matformer, trust or escalate, btm

\paragraph{Adaptation} Training a gigantic LLM and re-purposing it with prompt engineering is the most popular status quo of LLM research and applications. However, one gigantic model is prohibitively expensive to re-train and update, while the effectiveness of prompt-based adaptation is limited and brittle \citep{sprague2024cot}. Multi-LLM collaboration offers strategies for adapting language models that are lightweight and flexible: Token-level methods pair a general-purpose LLM with specialized peers for collaborative generation \citep{shen-etal-2024-learning}; logit-level approaches mixes the logit distributions of multiple LLMs for collaborative decoding \citep{liu-etal-2024-tuning}; weight-level approaches flexibly reuse and adapt existing models/adapters through weight arithmetic \citep{ilharcoediting, han2023ssd, yadav2024ties, feng2024model}. Multi-LLM collaboration offers diverse and flexible solutions for adaptation spanning varying levels of model access.
%One model is costly to re-train and update, but lorahub, model swarms, co-llm, etc.

\paragraph{Privacy} Despite the extensive effort to curate (pre)training data, private and copyrighted data will need to be left out for privacy, compliance, and ethics concerns \citep{karamolegkou2023copyright, yao2024survey}. These data sets are nonetheless helpful in highly specialized or personalized contexts. Multi-LLM offers preliminary solutions where private/copyrighted data could be employed in a local model at the data provenance, then interact with a larger general-purpose model \citep{zhang2024cogenesis}. Though it might be possible to extract private data from the model \citep{carlini2021extracting}, we envision future work on augmenting the ``private'' LLM with contextual integrity guardrails \citep{mireshghallahcan} for controllable and context-aware information sharing.
%Hard to include private/copyrighted data for training general-purpose model, but they are nevertheless useful. Having a small model plug-in etc.

\paragraph{Democratization and collaborative development} A single LLM is often trained by only a team of researchers and engineers, struggling to reflect the diversity of real-world LLM users. The priorities of long-tail and underprivileged users are often not incorporated when making decisions about model training and alignment. On the contrary, multi-LLM collaboration uniquely enables decentralized and collaborative development: all stakeholders in LLM development and applications could contribute models based on their needs, priorities, and compute budgets, then composed through various levels of multi-LLM collaboration protocols (\Sref{sec:how}). In this way, we democratize language technologies through participatory and collaborative development where everyone is welcome.

% ...

% [a table on the empirical gains/benefits of multi-llm collaboration as reported in these references?]

\section{Future Directions for Multi-LLM Collaboration Research}
\label{sec:discussion}

We identify various limitations of existing multi-LLM collaboration systems and motivate future work.

\paragraph{Theories of human communication} While the current approach focuses on developing a single general-purpose LLM, there is no ``general-purpose'' human, but specialized individuals collaborating through various communication protocols for collective intelligence \citep{hutchins2000distributed}. We thus argue that future multi-LLM collaboration research could benefit from cognitive science and communications theories, designing social science-inspired protocols for multiple LLMs to compose and collaborate.

\paragraph{Encapsulation and handoff} Another interesting challenge in multi-LLM collaboration is the absence of clear handoff boundaries. In software engineering, \emph{encapsulation} serves as a cornerstone of collaborative development by establishing well-defined interfaces between components: modifications to one part of the codebase do not propagate unexpected changes to others. However, especially in weight-level LLM collaboration, cleanly separating and containing the expertise of different models remains an open challenge. While recent work has demonstrated progress in developing modularized model components~\citep{pfeiffer2020adapterfusion, hu2021lora,yadav2024survey}, modifications to base model weights can still introduce unpredictable behavioral changes beyond the intended training objectives (for example, catastrophic forgetting~\citep{mccloskey1989catastrophic, kirkpatrick2017overcoming}). Developing reliable encapsulation mechanisms can ensure robust and predictable model composition, and could be a critical step to achieve the vision for ``building LMs like open-source software''~\citep{raffel2021call}.

\paragraph{Compatibility with the status quo} Despite the active research in multi-LLM collaboration, there is limited uptake in large-scale and industry settings beyond academic papers. One reason could be that many existing multi-LLM approaches require the training/development of extra modules such as gates and routers \citep{jiang2023llm, muqeethlearning}, while most open-source activities only feature the sharing of model weights. We thus argue that future multi-LLM protocols should be compatible with the status quo of model sharing by employing limited to no extra step beyond employing existing model checkpoints.

\paragraph{Interpretability insights} 
Interpretability techniques unveil the mechanisms underlying language models for reasoning \citep{stolfo-etal-2023-mechanistic}, factual association \citep{meng2022locating}, and more \citep{nandaprogress}. The interpretability insights enable localized manipulation of sub-modules for efficient enhancement and editing \citep{yin2024lofit}, thereby facilitating the potential for lightweight multi-model collaboration.
Moreover, while diverse language models may exhibit similar or distinct mechanisms for comparable tasks, the reliability of their capability beyond mere memorization varies \citep{yang-etal-2024-large-language-models}. Interpretability tools offer insights into determining the fitting weight/contribution of each component model in multi-LLM collaboration and could lead to improved collaboration outcomes.

\paragraph{Evaluating multi-LLM collaboration} Research on modular and multi-LLM systems has not yet devised an agreed-upon and detailed evaluation methodology. Most of the existing work resorts to evaluation with tasks and datasets typical for a single LLM. Future work could explore specifically evaluating multi-LLM collaboration, designing tasks and datasets where multiple models are evaluated separately and in collaboration, e.g., ablating by withholding copyright data \citep{minsilo}, or evaluating multi-agent collaboration where multiple models divide and conquer complex problems \citep{guo2024large}.

\paragraph{Democratizing ways of contribution} While we hope that collaborative and participatory contributions to multi-LLM systems could alleviate the underrepresentation of people, \emph{not everyone knows how to train an LLM and contribute}. This is especially true for the already underrepresented and underprivileged \citep{kirk2024prism}, thus the benefits of multi-LLM collaboration will not reach them if we expect users to train and contribute models on their own. Thus, we argue that we should lower the barrier of contribution: for example, by designing an agent framework that automatically solicits user feedback in natural language, fetches data, trains models, generates synthetic data to evaluate, and finally pushes the model and contribute. In this way, users only need to provide a few sentences of feedback about the gaps in existing LLMs, and a new component LLM could be developed and contributed on their behalf.

%\vspace*{-10pt}
\section{Alternative Views}

We identify two alternative views to our position.

\emph{We could patch the underrepresentations of data, skills, and people by further augmenting a single model.} While existing band-aid approaches such as LoRA fine-tuning \citep{hu2021lora} or retrieval augmented generation (RAG) \citep{shi2023replug, jiang2023active, asaiself} patch the gaps in data and skills to some extent, we present empirical evidence of their limitations in Section \ref{sec:why}, suffering from challenges such as privacy and copyright, catastrophic forgetting, lack of participation, and more. Further fine-tuning with LoRA could patch the gap of skills, but it risks jeopardizing the general-purposeness and leads to tradeoffs of existing skills \citep{kirkpatrick2017overcoming}; retrieval could provide new information and data to improve reliability, but there is no guarantee that LLMs would fully leverage the retrieved context \citep{shi-etal-2024-trusting}. While it is not impossible that with future progress a single LLM could offer perfect representations, we argue that multi-LLM collaboration offers a more concrete and actionable roadmap to advance language technologies, and a more efficient one, as it reuses developments made so far.

\emph{We could enable collaboration through a single model.} It is theoretically possible to collaborate in the development lifecycle of a single model. Different communities could contribute heterogeneous data to be combined for training a single model; different engineering teams could train part of the model architecture for later merging; different users could annotate diverse alignment preferences to jointly align an LLM. We argue that while they are all possible, it is more natural to collaborate on the level of models since 1) model sharing is the default open-source activity, 2) there are already 1,261,059 LLMs\footnote{Huggingface accessed on Jan 6, 2025.} openly available for collaboration, and 3) the companies that have the resource to carry out these protocols are incentivized to not go open about development of their LLM for competitive advantage. We envision multi-LLM collaboration as a promising path to reuse existing models, promote collaborative development, and advance compositional intelligence.

%collaboration through a single model: pretraining data, alignment data, prompting

%\vspace*{-10pt}
\section{Related Work}

Two recent works discuss related topics. 

\citet{yadav2024survey} present a taxonomy of model merging and mixture-of-experts (MoE) approaches, arguing for reusing and routing of existing expert models. They focus primarily on weight-level collaboration approaches, while we aggregate a broader family of methods with a broader definition of \emph{multi-LLM collaboration} where models could collaborate through four different levels of information exchange.

\citet{duposition} present a position paper arguing for compositional generative modeling, discussing the benefits of combining multiple modules across computer vision, reinforcement learning, robotics, and a brief mention of language. We specifically focus on language models and dive deep into LLM-specific arguments, methods, and future research.
%Colin paper, Yilun paper


\section{Conclusion}

We argue that one LLM is not enough and advocate for multi-LLM collaboration to better represent diverse data distributions, heterogeneous skills, and pluralistic populations. We propose a hierarchy of existing multi-LLM collaboration approaches based on information exchange levels, spanning API-level, text-level, logit-level, and weight-level collaboration. We then summarize the benefits of existing multi-LLM systems over a single model and discuss the limitations of existing methods to motivate future work. We envision multi-LLM collaboration as a viable path to compositional intelligence and an important initiative toward collaborative AI development.

\bibliography{example_paper}
\bibliographystyle{icml2025}

\end{document}
