\section{Related Work}
\vspace{-.2cm}
\textbf{Continual Learning}. 
% As a topic of extensive research, continual learning has witnessed the development of diverse methodologiesRusu, "Pseudo-Rehearsal: Adversarial Learning for Continual Task Adaptation"__Parisi, "Continual Lifelong Learning with Neural Networks: A Statistical Framework"__, which can be roughly divided into three families:
Continual learning has witnessed the development of diverse methodologiesKirkpatrick, "Overcoming Catastrophic Forgetting in Neural Networks with Expert Memories"__Hwang, "Rehearsal-Based Online Continual Learning"__, which can be roughly divided into three families:
% The methods can be roughly divided into three families: regularization-based methods, parameter isolation methods and replay-based methods____.
% Regularization-based methods circumvent the need to store raw inputs by incorporating an additional regularization term into the loss function.
%This regularization term helps consolidate previous knowledge while learning from new data.
(\rom{1}) Regularization-based methods: LwF employs the knowledge distillation loss, where the previous model's output is utilized as soft labels for the current tasks when working with new data____.
%IMM____ utilize Gaussian mixture model framework to approximate the posterior distribution across multiple tasks.
Stable SGD____ demonstrated performance enhancements by calibrating pivotal hyperparameters and systematically reducing the learning rate upon the arrival of each task.
% Through the strategic allocation of model parameters to individual tasks, parameter isolation methods aim to mitigate potential catastrophic forgetting.
(\rom{2}) Parameter isolation methods: ____ suggested augmentation of the model with new branches tailored to incoming tasks.
% concurrently maintaining the previously established task parameters.
%Replay-based methods either preserve samples of previous tasks or synthesize pseudo-samples utilizing generative models.
% As a replay-based method, Continual Prototype Evolution (CoPE) combines the principles of the nearest-mean classifier with a reservoir-based sampling strategy____.
(\rom{3}) Replay-based methods: 
% Continual Prototype Evolution (CoPE) combines the principles of the nearest-mean classifier with a reservoir-based sampling strategy____.
%To avoid overfitting to a subset of retained samples, GEM applies constrained updates for new tasks such that they do not adversely affect prior tasks____.
%Furthermore, while generative replay-based techniques fall under the category of replay-based methods, they eliminate the necessity for a replay buffer to store data. Rather, an auxiliary generator model the data distribution of acquired knowledge, producing synthetic data for replay in instances____.
Generative replay-based methods use an auxiliary generator to model the data distribution of acquired knowledge, producing synthetic data for replay in instancesRebuffi, "ICARL: Incremental Classifier-and-Refiner Labyrinth"__Chen, "Efficient Lifelong Learning with A-GEM"__.
%\CS{too long, too much useless sentences}
While existing research predominantly focused on the efficient memorization of past knowledge, we turn our attention to a more foundational question: \emph{is prior knowledge perpetually beneficial?}

%Regularization-based methods avoids storing raw inputs. Instead, they introduce an extra regularization term in the loss function, consolidating previous knowledge when learning on new data.
%LwF utilizes knowledge distillation loss, using the previous model output as soft labels for current tasks on the new data____.
%IMM____ proposes to learn the posterior distribution for multiple tasks as a mixture of Gaussians.
%Stable SGD____ shows impressive performance gain through controlling essential hyperparameters and gradually decreasing learning rate each time a new task arrives.
%Parameter isolation methods dedicates different model parameters to each task, to prevent any possible forgetting.
%____ propose to grow new branches for new tasks, while freezing previous task parameters.
%Replay-based methods stores samples in raw format or generates pseudo-samples with a generative model.
%Continual Prototype Evolution (CoPE) combines the nearest-mean classifier approach with an efficient reservoir-based sampling scheme____.
%GEM propose to avoid overfitting the subset of stored samples by only constraining new task updates to not interfere with previous tasks____.
%In addition, the generative replay based methods belong to the replay-based methods, but they do not need a replay buffer for storing previous data. Instead, an additional generator is trained to capture the data distribution of data learnt so far, and it generates synthetic data for replay when the real data becomes unavailable____.
%Existing continual learning methods rarely apply normalizing flow model for generative replay, the likelihood density estimation of generative models for feature learning also remains to explore.

\textbf{Federated Learning}. 
% \WE{delete here}
Federated learning represents a distributed learning paradigm among multiple clients and a central server.
% Federated learning represents a distributed learning paradigm with the objective of establishing a global model on a central server by aggregating the parameters trained at client nodes based on their local datasets.
%In recent years, federated learning has attracted increasing attention.
%Notable challenges associated with this domain encompass communication efficiencyKonevcny, "Federated Optimization in Heterogeneous Networks"__Konecny, " Federated Learning: Strategies for Improving Communication Efficiency"__, privacy constraintsJaggi, "Privacy-preserving Distributed Deep Learning on Untrusted Servers"__Bonawitz, "Practical Secure Aggregation for Privacy-Preserving Collaborative Machine Learning"__, and statistical heterogeneityHard, "Federated Optimization of Convolutional Networks via Primal-Dual Block Coordinate Descent"__Konecny, " Communication-Efficient Federated Neural Network Learning from Non-IID Data at the Edge"__.
Researchers have been endeavoring to address the  statistical heterogeneity by developing a comprehensive global modelMcMahan, "Communication Efficient Learning of Deep Networks from Partially Labeled and Unlabeled Data with Adaptive Regularization"__Konecny, "Federated Optimization: Distributed Model Fusion via Local Solver Differentiation"__.
____ aimed to achieve a fair distribution of model performance by optimizing its efficacy across any given target distribution.
%____ propose novel aggregation strategies based on Bayesian non-parametric approaches.
% ____ incorporated a regularization term during local training to promote uncorrelated representations, thereby mitigating the risk of dimensional collapse.
% ____ introduced a proximal term, aiming to ensure that local updates remain in proximity to the global model.
____ suggested the utilization of a generator to aggregate user information. This, in turn, guides the local training by employing the acquired knowledge as an inductive bias.
%\CS{too long, too much useless sentences}
In this work, we consider a more challenging learning problem associated with statistical heterogeneity in federated scenarios: how to facilitate collaboration when all clients are tackling different tasks?
% \cs{Federated learning methodologies operate within defined privacy constraints.
% Our approach formulates a generator within the feature space and exclusively transmits model parameters, adhering to these privacy restrictions.}

%Federated learning is a distributed learning framework, which aims to learn a global model on a server while aggregating the parameters learned at the clients on their private data.
%Recent years have witnessed growing attention to federated learning____, of which several challenges have been concerning topics including communication efficiencyKonevcny, "Federated Optimization in Heterogeneous Networks"__Konecny, " Federated Learning: Strategies for Improving Communication Efficiency"__, privacyJaggi, "Privacy-preserving Distributed Deep Learning on Untrusted Servers"__Bonawitz, "Practical Secure Aggregation for Privacy-Preserving Collaborative Machine Learning"__, and data heterogeneityHard, "Federated Optimization of Convolutional Networks via Primal-Dual Block Coordinate Descent"__Konecny, " Communication-Efficient Federated Neural Network Learning from Non-IID Data at the Edge"__.
%There are researchers aiming to tackle the heterogeneity by learning a global model. ____ seek a fair model performance distribution by maximizing the model performance on any arbitrary target distribution.
%____ introduce well-designed aggregation policies by leveraging Bayesian non-parametric methods.
%____ applies a regularization term during local training that encourages uncorrelated representations to mitigate dimensional collapse.
%____ propose a proximal term to restrict the local updates to be closer to the initial model.
%____ proposes to learn a generator to ensemble user information to regulate local training using the learned knowledge as an inductive bias.
%Federated learning methods are subject to privacy constraints.
%Our method constructs generator in the feature space and only transmits model parameters, strictly following the privacy constraints.

\textbf{Federated Continual Learning}.
To date, there are a few studies in the domain of federated continual learning.
____ studied the scenario of data distributions changing over time in federated learning.
Federated reconnaissance presented a scenario with incrementally new classes during training and proposed to utilize prototype networksRebuffi, "ICARL: Incremental Classifier-and-Refiner Labyrinth"__.
____ proposed a regularization-based algorithm and a new theoretical framework for itKhadiv, "Regularization Methods for Continual Learning in Federated Settings"__.
____ presented a distillation-based method to deal with catastrophic forgetting, using previous model and global model as teachers for the training of local modelsJin, "Distilling Knowledge from Teachers for Deep Transfer Learning"__.
____ proposed a novel parameter isolation method for the federated diagram, where the network weights are decomposed into global parameters and task-specific parametersFang, "Parameter Isolation with Adaptive Neural Architectures for Federated Continual Learning"__.
____ considered a federated class-incremental setting and developed a distillation-based method to alleviate catastrophic forgetting from both local and global perspectivesLiu, "Class-Incremental Learning in Federated Settings: A Distillation-Based Approach"__.
____ customized the generative replay based method ACGAN with model consolidation and consistency enforcementTang, "Generative Replay Based Continual Learning for Incremental Class Recognition"__.
% Our method exhibits notable differences in several aspects compared to the aforementioned methods. 
% Firstly, we consider the issue of memorizing biased features due to the lack of explicit regularization in federated continual learning settings.
% Secondly, our approach learns a generative model for user data in the feature space, which allows for effective knowledge sharing and adaptation across different tasks and clients.
% Lastly, our method enables efficient computation of local updates by leveraging the learned generator, leading to significant reductions in communication overhead.

%\CS{too long, too much useless sentences}