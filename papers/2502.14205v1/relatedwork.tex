\section{Related Work}
\vspace{-.2cm}
\textbf{Continual Learning}. 
% As a topic of extensive research, continual learning has witnessed the development of diverse methodologies~\citep{DBLP:journals/pami/LangeAMPJLST22}, which can be roughly divided into three families:
Continual learning has witnessed the development of diverse methodologies~\citep{DBLP:journals/pami/LangeAMPJLST22}, which can be roughly divided into three families:
% The methods can be roughly divided into three families: regularization-based methods, parameter isolation methods and replay-based methods~\citep{DBLP:journals/pami/LangeAMPJLST22}.
% Regularization-based methods circumvent the need to store raw inputs by incorporating an additional regularization term into the loss function.
%This regularization term helps consolidate previous knowledge while learning from new data.
(\rom{1}) Regularization-based methods: LwF employs the knowledge distillation loss, where the previous model's output is utilized as soft labels for the current tasks when working with new data~\citep{DBLP:conf/eccv/LiH16}.
%IMM~\citep{DBLP:conf/nips/LeeKJHZ17} utilize Gaussian mixture model framework to approximate the posterior distribution across multiple tasks.
Stable SGD~\citep{DBLP:conf/nips/MirzadehFPG20} demonstrated performance enhancements by calibrating pivotal hyperparameters and systematically reducing the learning rate upon the arrival of each task.
% Through the strategic allocation of model parameters to individual tasks, parameter isolation methods aim to mitigate potential catastrophic forgetting.
(\rom{2}) Parameter isolation methods: \cite{DBLP:journals/corr/RusuRDSKKPH16} suggested augmentation of the model with new branches tailored to incoming tasks.
% concurrently maintaining the previously established task parameters.
%Replay-based methods either preserve samples of previous tasks or synthesize pseudo-samples utilizing generative models.
% As a replay-based method, Continual Prototype Evolution (CoPE) combines the principles of the nearest-mean classifier with a reservoir-based sampling strategy~\citep{DBLP:conf/iccv/LangeT21}.
(\rom{3}) Replay-based methods: 
% Continual Prototype Evolution (CoPE) combines the principles of the nearest-mean classifier with a reservoir-based sampling strategy~\citep{DBLP:conf/iccv/LangeT21}.
%To avoid overfitting to a subset of retained samples, GEM applies constrained updates for new tasks such that they do not adversely affect prior tasks~\citep{DBLP:conf/nips/Lopez-PazR17}.
%Furthermore, while generative replay-based techniques fall under the category of replay-based methods, they eliminate the necessity for a replay buffer to store data. Rather, an auxiliary generator model the data distribution of acquired knowledge, producing synthetic data for replay in instances~\citep{DBLP:conf/icml/OdenaOS17, DBLP:conf/nips/WuHLWWR18}.
Generative replay-based methods use an auxiliary generator to model the data distribution of acquired knowledge, producing synthetic data for replay in instances~\citep{DBLP:conf/icml/OdenaOS17, DBLP:conf/nips/WuHLWWR18}.
%\CS{too long, too much useless sentences}
While existing research predominantly focused on the efficient memorization of past knowledge, we turn our attention to a more foundational question: \emph{is prior knowledge perpetually beneficial?}

%Regularization-based methods avoids storing raw inputs. Instead, they introduce an extra regularization term in the loss function, consolidating previous knowledge when learning on new data.
%LwF utilizes knowledge distillation loss, using the previous model output as soft labels for current tasks on the new data~\citep{DBLP:conf/eccv/LiH16}.
%IMM~\citep{DBLP:conf/nips/LeeKJHZ17} proposes to learn the posterior distribution for multiple tasks as a mixture of Gaussians.
%Stable SGD~\citep{DBLP:conf/nips/MirzadehFPG20} shows impressive performance gain through controlling essential hyperparameters and gradually decreasing learning rate each time a new task arrives.
%Parameter isolation methods dedicates different model parameters to each task, to prevent any possible forgetting.
%\cite{DBLP:journals/corr/RusuRDSKKPH16} propose to grow new branches for new tasks, while freezing previous task parameters.
%Replay-based methods stores samples in raw format or generates pseudo-samples with a generative model.
%Continual Prototype Evolution (CoPE) combines the nearest-mean classifier approach with an efficient reservoir-based sampling scheme~\citep{DBLP:conf/iccv/LangeT21}.
%GEM propose to avoid overfitting the subset of stored samples by only constraining new task updates to not interfere with previous tasks~\citep{DBLP:conf/nips/Lopez-PazR17}.
%In addition, the generative replay based methods belong to the replay-based methods, but they do not need a replay buffer for storing previous data. Instead, an additional generator is trained to capture the data distribution of data learnt so far, and it generates synthetic data for replay when the real data becomes unavailable~\citep{DBLP:conf/icml/OdenaOS17, DBLP:conf/nips/WuHLWWR18}.
%Existing continual learning methods rarely apply normalizing flow model for generative replay, the likelihood density estimation of generative models for feature learning also remains to explore.

\textbf{Federated Learning}. 
% \WE{delete here}
Federated learning represents a distributed learning paradigm among multiple clients and a central server.
% Federated learning represents a distributed learning paradigm with the objective of establishing a global model on a central server by aggregating the parameters trained at client nodes based on their local datasets.
%In recent years, federated learning has attracted increasing attention.
%Notable challenges associated with this domain encompass communication efficiency~\citep{konevcny2016federated}, privacy constraints~\citep{agarwal2018cpsgd}, and statistical heterogeneity~\citep{karimireddy2020scaffold}.
Researchers have been endeavoring to address the  statistical heterogeneity by developing a comprehensive global model~\citep{DBLP:conf/iclr/WangYSPK20}.
\cite{mohri2019agnostic} aimed to achieve a fair distribution of model performance by optimizing its efficacy across any given target distribution.
%\cite{DBLP:conf/iclr/WangYSPK20} propose novel aggregation strategies based on Bayesian non-parametric approaches.
% \cite{DBLP:conf/iclr/ShiLZTB23} incorporated a regularization term during local training to promote uncorrelated representations, thereby mitigating the risk of dimensional collapse.
% \cite{li2020federated} introduced a proximal term, aiming to ensure that local updates remain in proximity to the global model.
\cite{DBLP:conf/icml/ZhuHZ21} suggested the utilization of a generator to aggregate user information. This, in turn, guides the local training by employing the acquired knowledge as an inductive bias.
%\CS{too long, too much useless sentences}
In this work, we consider a more challenging learning problem associated with statistical heterogeneity in federated scenarios: how to facilitate collaboration when all clients are tackling different tasks?
% \cs{Federated learning methodologies operate within defined privacy constraints.
% Our approach formulates a generator within the feature space and exclusively transmits model parameters, adhering to these privacy restrictions.}

%Federated learning is a distributed learning framework, which aims to learn a global model on a server while aggregating the parameters learned at the clients on their private data.
%Recent years have witnessed growing attention to federated learning~\citep{mcmahan2017communication}, of which several challenges have been concerning topics including communication efficiency~\citep{konevcny2016federated}, privacy~\citep{agarwal2018cpsgd} and data heterogeneity~\citep{karimireddy2020scaffold}.
%There are researchers aiming to tackle the heterogeneity by learning a global model. \cite{mohri2019agnostic} seek a fair model performance distribution by maximizing the model performance on any arbitrary target distribution.
%\cite{DBLP:conf/iclr/WangYSPK20} introduce well-designed aggregation policies by leveraging Bayesian non-parametric methods.
%\cite{DBLP:conf/iclr/ShiLZTB23} applies a regularization term during local training that encourages uncorrelated representations to mitigate dimensional collapse.
%\cite{li2020federated} propose a proximal term to restrict the local updates to be closer to the initial model.
%\cite{DBLP:conf/icml/ZhuHZ21} proposes to learn a generator to ensemble user information to regulate local training using the learned knowledge as an inductive bias.
%Federated learning methods are subject to privacy constraints.
%Our method constructs generator in the feature space and only transmits model parameters, strictly following the privacy constraints.

\textbf{Federated Continual Learning}.
To date, there are a few studies in the domain of federated continual learning.
\cite{casado2020federated} studied the scenario of data distributions changing over time in federated learning.
Federated reconnaissance presented a scenario with incrementally new classes during training and proposed to utilize prototype networks~\citep{hendryx2021federated}.
\cite{guo2021new} proposed a regularization-based algorithm and a new theoretical framework for it.
\cite{DBLP:journals/corr/abs-2109-04197} presented a distillation-based method to deal with catastrophic forgetting, using previous model and global model as teachers for the training of local models.
\cite{yoon2021federated} proposed a novel parameter isolation method for the federated diagram, where the network weights are decomposed into global parameters and task-specific parameters.
\cite{DBLP:conf/cvpr/DongWFSXW022} considered a federated class-incremental setting and developed a distillation-based method to alleviate catastrophic forgetting from both local and global perspectives.
\cite{DBLP:conf/iclr/QiZ023} customized the generative replay based method ACGAN with model consolidation and consistency enforcement.
% Our method exhibits notable differences in several aspects compared to the aforementioned methods. 
% Firstly, we consider the issue of memorizing biased features due to statistical heterogeneity, a concern that potentially compromises the performance across all clients and tasks.
% Then, we propose to train an NF model in the facilitation of generative replay in feature space, aimed at realizing accurate forgetting.
% Furthermore, the likelihood of generated features pertaining to the local distribution estimated through the NF model is used to accurately pinpoint unbiased knowledge.
Our method considers the issue of memorizing biased feature due to statistical heterogeneity, exhibiting notable differences compared to the aforementioned methods.

% \cs{Firstly, We consider the issue of memorizing biased feature, which may be aggravated due to statistical heterogeneity in federated learning scenario.
% We propose to train normalizing flow model in the feature space for generative replay.
% And the probability density of feature estimated by normalizing flow model is used to adaptively select unbiased knowledge.}


%