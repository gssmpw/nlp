\section{Discussion}


\textbf{Limitations.} 
Although we have a diverse set of users and use cases, it is unclear to what extent our results encapsulate all real-world use cases.
We run extensive pilot tests to ensure platform usability, but we recognize that certain aspects---specifically our pairwise completions and slower latency---do not perfectly mirror real-world platforms such as Github Copilot.
Further, while we rank models based on user preferences, this should not be treated as the sole defining metric of model quality, but instead an informative one.
In this work, we evaluate multiple LLMs with strong coding capabilities; however, we are unable to include Github Copilot because the model powering Github Copilot is not available via API.
Finally, due to privacy considerations, we choose not to release all code contexts collected in the study without careful post-processing.
We strive to make more data open through periodic releases.

\textbf{Future work.} Our analyses of \systemName data stress the need to create a diverse set of questions including multiple written and programming languages, downstream applications, and code structures.
\systemName findings highlight the importance of conducting evaluations with real users, tasks, and environments.
To extend this platform, future evaluations may also consider building on the \systemName system in multiple ways: more nuanced forms of feedback in the programming setting, including measuring trajectories and code persistence metrics, and more forms of interaction, including inline prompt editing and chat dialogue within an IDE.
We open-source \systemName to facilitate these future extensions.

\section{Conclusion}

We introduce a platform, \systemName, to evaluate LLMs in the wild using live human feedback for the use case of coding assistants.
\systemName is deployed and has collected over \sampleCount~votes across 10 models; we will release a curated dataset to showcase the diversity of user preferences.
We show that evaluating the coding capabilities of LLMs in \systemName leads to rankings that differ from existing approaches which rely on static benchmarks or chat-based interactions, demonstrating how these differences could be attributed to the shift in distribution between \systemName and prior evaluations. 
These different contexts also facilitate further understanding of how user preferences vary, highlighting the importance of evaluating new models with real users, tasks, and environments.



