

\section{Additional System Details}
\label{appdx:system}

We describe further implementation details and considerations for each of the three key system components: user interface, model routing, and model prompting

\subsection{User Experience}
\label{sec:appendix-interface}

We make several additional design decisions surrounding our user interface.
\begin{itemize}
    \item We cache generated completions. If the user continues typing, we try to retrieve a matching pair of completions.
    \item We set a 0.5 second delay before automatically generating completions.
    \item If two completions are identical, then we return only one copy.
    \item If one model returns an empty string, then we only display the other, non-empty completion.
    \item We limit the number of lines each completion can generate (default of 20), but allow users to customize this limit.
    \item After the user votes, we reveal the model pair and their choice to the user. We also show the user a history of their votes.
    \item We limit the input file size to be 8,000 tokens, which covers nearly all user file lengths.
\end{itemize}



\subsection{Prompting Diverse Models}
\label{sec:appendix-prompt}

\textbf{Prompt templates}

\begin{enumerate}[leftmargin=*]
    \item \textit{Prefix-Suffix-Middle (PSM)}. 
    PSM presents the code context in the order of prefix and then suffix, using XML notation to demarcate prefix, suffix, and middle segments (e.g., \texttt{<PREFIX>} and \texttt{</PREFIX>}). 
    The LLM is then asked to output the middle segment given the prompt.
    \item \textit{Suffix-Prefix-Middle (SPM)}.
    SPM is identical to PSM except that the suffix appears before the prefix, which may be more natural than having the suffix appear directly before the output as is the case with PSM.
    \item \textit{Mask}. Rather than using start and end tokens to denote the prefix and suffix, the Mask prompt uses a special ``sentinel'' token to indicate the masked (i.e. middle) code segment~\citep{guo2024deepseekcoderlargelanguagemodel}.
    The LLM is then requested to fill in the masked code segment.
    \item \textit{Instructed Prefix Feeding (IPF)}.
    IPF begins with the Mask prompt and then repeats the prefix as a ``prefill'' of the completion for the language model.\footnote{Nowadays, many completion APIs are deprecated; however, many chat APIs provide the ability to ``pre-fill'' tokens in the response which is similar to forcing the LLM to do a completion}
    This is similar to IPF in~\citet{guo2024deepseekcoderlargelanguagemodel}, except with instructions adjusted to better align with chat models.
    This approach allows non-FiM-trained models the ability to better tackle FiM tasks~\citep{fried2023incodergenerativemodelcode}. 
\end{enumerate}
\newpage

\subsubsection{PSM Example}

\begin{minipage}{0.9\textwidth}
\begin{lstlisting}[
    language=Python,
    basicstyle=\ttfamily\small,
    breaklines=true,
    showstringspaces=false,
    commentstyle=\color{gray},
    keywordstyle=\color{blue},
    stringstyle=\color{green!50!black},
    numberstyle=\tiny\color{gray},
    frame=single
]

      Fill in code and output nothing else. Respect spacing, new lines, and indentation. Start with <CODE> and end with </CODE>.
      Be VERY mindful of indentation. Make sure it is correct.

      Example 1:
      <PREFIX>class Calculator {{
        add(number) {{
          this.result +=</PREFIX>
      <SUFFIX>  subtract(number) {{
          this.result -= number;
          return this;
        }}
      }}</SUFFIX>
      <CODE> number;
          return this;
        }}</CODE>

      Example 2:
      <PREFIX>from typing import List


      def has_close_elements(numbers: List[float], threshold: float) -> bool:
          """ Check if in given list of numbers, are any two numbers closer to each other than
          given threshold.
          >>> has_close_elements([1.0, 2.0, 3.0], 0.5)
          False
          >>> has_close_elements([1.0, 2.8, 3.0, 4.0, 5.0, 2.0], 0.3)
          True
          """
          for idx, elem in enumerate(numbers):
              for idx2, elem2 in enu</PREFIX>
      <SUFFIX> != idx2:
                      distance = abs(elem - elem2)
                      if distance < threshold:
                          return True

          return False</SUFFIX>
      <CODE>merate(numbers):
                  if idx</CODE>

      Task:
      <PREFIX>{prefix}</PREFIX>
      <SUFFIX>{suffix}</SUFFIX>

\end{lstlisting}
\end{minipage}
\newpage

\subsubsection{Evaluation Results}

\begin{table*}[h!]
\centering
\caption{pass@1 of code completions with different prompt templates (PSM, SPM, Mask). We observe that for all models and most prompt templates, our Snip-It method improves pass@1.}
\resizebox{\textwidth}{!}{\begin{tabular}{ll|rrrr|rrrr}
\toprule
Group & Model &psm & spm & mask & ipf & snip\_psm & snip\_spm & snip\_mask & snip\_ipf \\
\midrule
\multicolumn{10}{l}{\textbf{Open Code}} \\
 & Deepseek-Coder-V2.5 & 0.551 & 0.519 & 0.414 & 0.229 & 0.585 & 0.584 & 0.597 & 0.614 \\
 & Qwen-2.5-32B & 0.169 & 0.065 & 0.113 & 0.005 & 0.563 & 0.611 & 0.534 & 0.521 \\
\midrule
\multicolumn{10}{l}{\textbf{Open}} \\
 & Llama-3.1-405B-Instruct-Turbo & 0.254 & 0.224 & 0.145 & 0.038 & 0.553 & 0.583 & 0.531 & 0.463 \\
 & Llama-3.1-70B-Instruct-Turbo & 0.074 & 0.079 & 0.061 & 0.029 & 0.490 & 0.527 & 0.312 & 0.323 \\
\midrule
\multicolumn{10}{l}{\textbf{Commercial}} \\
 & Gemini-1.5-Pro-002 & 0.620 & 0.599 & 0.562 & 0.338 & 0.561 & 0.659 & 0.259 & 0.491 \\
 & GPT-4o & 0.607 & 0.477 & 0.505 & 0.033 & 0.620 & 0.670 & 0.609 & 0.524 \\
 & Claude-3.5-Sonnet & 0.561 & 0.565 & 0.552 & 0.374 & 0.730 & 0.710 & 0.705 & 0.507 \\
 & Gemini-1.5-Flash-002 & 0.434 & 0.376 & 0.277 & 0.286 & 0.409 & 0.403 & 0.301 & 0.394 \\
 & GPT-4o-mini & 0.099 & 0.055 & 0.088 & 0.019 & 0.429 & 0.480 & 0.361 & 0.342 \\
\bottomrule
\end{tabular}}
\label{tab:evaluation-results}
\end{table*}

\subsubsection{Errors without Snip-It}
\label{sec:format-errors}

Below are two examples of errors without Snip-It from GPT-4o mini. Red indicates the incorrect code that the model filled in.

\begin{minipage}{0.9\textwidth}
\begin{lstlisting}[
    language=Python,
    basicstyle=\ttfamily\small,
    breaklines=true,
    showstringspaces=false,
    commentstyle=\color{gray},
    keywordstyle=\color{blue},
    stringstyle=\color{green!50!black},
    numberstyle=\tiny\color{gray},
    frame=single,
    escapechar=|
]
from typing import List 
def below_zero(operations: List[int]) -> bool: 
"""  You're given a list of deposit and withdrawal operations on a bank account that starts with zero balance. Your task is to detect if at any point the balance of account fallls below zero, and at that point function should return True. Otherwise it should return False. >>> below_zero([1, 2, 3]) False >>> below_zero([1, 2, -4, 5]) True  """ 
    balance = |\color{red}   balance += op|
        |\color{red}if balance < 0:|
        balance += op 
        if balance < 0: 
            return True 
        return False
\end{lstlisting}
\end{minipage}


\begin{minipage}{0.9\textwidth}
\begin{lstlisting}[
    language=Python,
    basicstyle=\ttfamily\small,
    breaklines=true,
    showstringspaces=false,
    commentstyle=\color{gray},
    keywordstyle=\color{blue},
    stringstyle=\color{green!50!black},
    numberstyle=\tiny\color{gray},
    frame=single,
    escapechar=|
]

from typing import List 
def parse_nested_parens(paren_string: str) -> List[int]: """ Input to this function is a string represented multiple groups for nested parentheses separated by spaces. For each of the group, output the deepest level of nesting of parentheses. E.g. (()()) has maximum two levels of nesting while ((())) has three. >>> parse_nested_parens('(()()) ((())) () ((())()())') [2, 3, 1, 3] """ 
def parse_paren_group(s):
    |\color{red}depth += 1|
            |\color{red}max\_depth = max(max\_depth, depth)|
        |\color{red}if char == ')':|
\end{lstlisting}
\end{minipage}


\section{User information}\label{appendix:data_release}

Below we provide a copy of the general instructions and privacy instructions for users.

\subsection{General instructions}


Step 1: Install the extension and restart Visual Studio Code after installation.
If installed successfully, you will see \systemName show up on the bottom right corner of your window and the check mark changes to a spinning circle when a completion is being generated,
Note, if you are using any other completion provider (e.g. Github Copilot), you must disable them when using \systemName.

Step 2: \systemName currently supports two main feature: read autocomplete and in-line editing (beta) below to understand how to use each one. Since we show paired responses, the way you use them are slightly different than your standard AI coding tools!

Step 3: This step is optional. If applicable, you can change what data is saved by \systemName by following the instructions in "Privacy Settings''.

Step 4: Create a username by clicking the \systemName icon on the sidebar; detailed instructions are also in ``Create an account''. Your username will be used for a future leaderboard to compare individual preferences.



\subsection{Privacy Instructions}

\textbf{Privacy Settings.} Your privacy is important to us. Please read carefully to determine which settings are most appropriate for you.
To generate completions, the code in your current file is sent to our servers and sent to various API providers. This cannot be changed.

\textbf{Data Collection.} By default, we collect your code for research purposes. You can opt-out of this. If you are working on code containing sensitive information, we recommend that you opt out of data collection.
To opt-out of data collection, please change codePrivacySettings to Debug. We will only log your code for debugging.
To disable logging entirely, please change codePrivacySettings to Private. Opting-out means any bugs you encounter will be non-reproducable on our end.
You can find these settings by searching for \systemName in your vscode settings or clicking the gear button of the \systemName extension -$>$ Extension Settings.

\textbf{Removing your data.} If you would like to have the option in the future for us to delete any of your data, you must create an account on \systemName following instructions described in ``Create an account.'' To remove your data, you can email any of the \systemName maintainers with your username.

\textbf{Data Release.}
Prior to releasing any collected code snippets to enable future research efforts, we will run a PII detector and remove any identified entities to further ensure no personal information is released.


\begin{table}[]
    \centering
    \caption{To conserve space, we refer models by \emph{shortened name} in the main text but provide \emph{full model name} below for completeness.}
    \begin{tabular}{ll}
        \toprule
        \textbf{Full Model Name} & \textbf{Shortened Name} \\ 
        \midrule
        deepseek-coder-fim & deepseek-coder \\ 
        claude-3-5-sonnet-20240620 & claude-3.5-sonnet \\ 
        codestral-2405 & codestral \\ 
        llama-3.1-405b-instruct & llama-3.1-405b \\ 
        gemini-1.5-flash-002 & gemini-flash-002 \\ 
        gemini-1.5-pro-002 & gemini-pro-002 \\ 
        gpt-4o-2024-08-06 & gpt-4o-2024-08-06 \\ 
        llama-3.1-70b-instruct & llama-3.1-70b \\ 
        qwen-2.5-coder-32b-instruct & qwen-2.5-coder-32b \\ 
        gpt-4o-mini-2024-07-18 & gpt-4o-mini \\ 
        \bottomrule
    \end{tabular}
    \label{tab:model-comparison}
\end{table}



\section{Data Analysis}\label{appdx:data_analysis}



\textbf{Natural Language Detection.} To detect natural languages, we used the lingua language detector~\citep{stahl2024lingua}. 
We set the detector to all available languages (except for Latin due to false positives), and pick the language with the highest confidence that was greater than 0.7.
For each file, we detect for languages line by line and choose the language that appears in the most lines.
Additionally, we filter for languages that appear at least 5 times.
Results are in Figure~\ref{fig:nat_languages}.
For Table~\ref{tab:benchmark_comparison}, since Chatbot Arena does not track natural languages, we run the same detection algorithm for Chatbot Arena.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.7\linewidth]{figures/appendix/language_distribution.pdf}
    \caption{Natural languages in \systemName}
    \label{fig:nat_languages}
\end{figure}

\textbf{Programming Language Detection.}
We detect programming languages in \systemName by using the file's extension type (Figure~\ref{fig:prog_languages}).
For Table~\ref{tab:benchmark_comparison}, since Chatbot Arena does not track programming languages, we checked for the language of codeblocks instead.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.7\linewidth]{figures/appendix/filetype_distribution.pdf}
    \caption{Programming languages in \systemName. For image clarity, we only show programming languages that appear more than 10 times.}
    \label{fig:prog_languages}
\end{figure}

\newpage
\textbf{Task detection.} Since \systemName code contexts are fairly long, we employ a multi-step process to cluster code contexts, via LLM-as-a-judge~\citep{zheng2023judging}. We specifically use \texttt{GPT-4o-mini} due to its speed and price. 

First, we summarize all code contexts into short one-sentence descriptions.
\begin{tcolorbox}
\texttt{\textbf{System Prompt}} \\
\texttt{You are a helpful assistant that describes code files in a single, concise sentence.
Focus on the main purpose and functionality of the code.
Keep descriptions clear, technical, and under 100 characters.
Do not mention file names or extensions in your description.} \\

\texttt{\textbf{General Prompt}} \\
\texttt{Describe this code in one sentence}
\end{tcolorbox}


Next, we prompt a model to cluster all one-sentence descriptions.
\begin{tcolorbox}
\texttt{\textbf{General Prompt}} \\
\begin{verbatim}
    You are a code organization expert. 
    Analyze the provided code descriptions and:
    1. Identify 5-10 main functional clusters or themes
    2. Assign each description to the most appropriate cluster
    3. Provide a brief name and description for each cluster
    4. Format the response as valid JSON with the following structure:
    {
        "clusters": [
            {
                "name": "cluster_name",
                "description": "brief cluster description",
                "descriptions": ["description", "description2"]
            }
        ]
    }
\end{verbatim}
\end{tcolorbox}

Finally, we provide the full code context and ask the LLM to categorize the context given aforementioned clusters. Note that we sanity-checked clusters manually and removed redundant ones.
\begin{tcolorbox}
\texttt{\textbf{System Prompt}} \\
Please categorize the following code into one of these categories:
\begin{itemize}
    \item User Interaction and Input Handling: Code that manages user inputs, prompts, and basic interaction with the system
    \item Frontend Development and UI Design: Code snippets focused on designing user interfaces and creating interactive components.
    \item Backend Development and APIs: Server-side logic, data management, and API integration for applications.
    \item Algorithm Design and Problem Solving: Code implementing algorithms to solve computational problems or optimize tasks.
    \item Data Processing and File Operations: Code that reads, writes, or processes data from files and other data sources.
    \item Game Development and Simulations: Code focused on creating games, simulations, and managing game dynamics.
    \item Artificial Intelligence and Machine Learning: Code related to AI and machine learning for training, inference, and application.
\end{itemize}

\texttt{\textbf{General Prompt}} \\
Only respond with the exact category name that best fits. No other text.

Here's the code:\\
\texttt{[code content]}
\end{tcolorbox}
    

\textbf{Model Votes.}  We ensured that our leaderboard has coverage across all models, where each model received at least 2,000 votes, as shown in Figure~\ref{fig:model_votes}.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\linewidth]{figures/appendix/model_votes.pdf}
    \caption{Number of votes for each language model.}
    \label{fig:model_votes}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=1.0\linewidth]{figures/appendix/votes_over_time.png}
    \caption{Number of votes over time.}
    \label{fig:enter-label}
\end{figure}



\textbf{Code Structure Detection.}
To detect the presence of FiM, we check if there exists a suffix.
If there is only the prefix, we label it as "completion-only".
To detect if there are comments, we check if any of the 5 previous lines start with common comment styles (e.g. $\#$, \texttt{//}).
We check for block comments in a similar fashion using docstring styles (e.g. \texttt{"""}, \texttt{/** */}).



\input{files/completion_similarity.tex}


\subsection{Example Data}\label{appdx:example_data}

We provide examples of code contexts from each of the task categories. For readability, we select examples with shorter context lengths. Upon publication, we will also open-source more diverse examples (including those with significantly longer context lengths).

\begin{tcolorbox}
\textbf{\texttt{Artificial Intelligence and Machine Learning}}
\begin{verbatim}
from main13 import knn, mlp
import pandas as pd

for pclass in [1, 2, 3]:
    for fare in range(10, 200, 10):
        my_df = pd.DataFrame({
                "Pclass": [pclass]*3,
                "Name": [24]*3,
                "Sex": [0]*3, 
                "Age": [19]*3,
                "SibSp": [0]*3,
                "Parch": [0]*3,
                "Fare": [fare]*3,
                "Embarked": ["S", "Q", "C"]
            })
        my_df = pd.get_dummies(my_df, columns=["Embarked"], prefix="Embarked")  
        my_df["Embarked_S"] = my_df["Embarked_S"].astype(int)
        my_df["Embarked_C"] = my_df["Embarked_C"].astype(int)
        my_df["Embarked_Q"] = my_df["Embarked_Q"].astype(int)

        predictions = {
            "knn": knn.predict(my_df),
            "mlp": mlp.predict(my_df)
        }
        ans_df = pd.DataFrame(index=[fare], columns=[1, 2, 3])
        ans_df.at[fare, pclass] = predictions
print(ans_df)

\end{verbatim}
\end{tcolorbox}

\begin{tcolorbox}
\textbf{\texttt{Frontend Development and UI Design}}
\begin{verbatim}
<!DOCTYPE html>
    <html lang="en">
    <head>
      <meta charset="UTF-8">
      <meta name="viewport" content="width=device-width, initial-scale=1.0">
      <title>Document</title>
    </head>
    <body>
      <script>
        function getRandomNumber(min,max) {
          return Math.floor(Math.random()*)
        }
      </script>
    </body>
    </html>
\end{verbatim}
\end{tcolorbox}
    
\begin{tcolorbox}
\textbf{\texttt{Algorithm Design and Problem Solving}}
\begin{verbatim}
import java.util.*;

public class hashmapImplementation {
    static class HashCode<K, V>{ //generics -> we can use any data type for 
    key and value.
        private class Node{
            K key;
            V value;

            public Node(K key, V value){
                this.key = key;
                this.value = value;
            }
        }

        private int size; //n
        private LinkedList<Node> buckets[]; //N = buckets.length   
        -> array of linkedlists

        @SuppressWarnings("unchecked")
        public HashCode() {
            this.size = 0;
            this.buckets = new LinkedList[4];
            for (int i = 0; i < buckets.length; i++) {
                buckets[i] = new LinkedList<>();
            }
        }

        public void put(K key, V value){
            
        }
    }
    
    public static void main(String args[]){
    }
}
\end{verbatim}
\end{tcolorbox}

\begin{tcolorbox}
\textbf{\texttt{Data Processing and File Operations}}
\begin{verbatim}

import os
from pipeline.chain_function import *
path_name = "./pipeline_genereated_img/"

def upload_data(image_path):
    # image_path = os.listdir("pipeline_genereated_img")[0]
    full_path = path_name+image_path
    element = generate_img_summaries(full_path)
    
    
def upload_img_2_json():
# Write data to JSON file
json_file_path = "./img_json_stored/"+image_path
json_file_path = json_file_path.replace(".pdf",".json")
with open(json_file_path, 'w') as file:
    json.dump({"result": element}, file, indent=4)

print(f"Data successfully uploaded to {json_file_path}")
    
\end{verbatim}
\end{tcolorbox}

\begin{tcolorbox}
\textbf{\texttt{User Interaction and Input Handling}}
\begin{verbatim}
print("Hello World")
namevar = input("Enter name ")

print("Welcome " + namevar)
#Write python code to download and run deepseek model locally in my windows 
computer. I have python and pytorch installed in my computer.


#To download and run a DeepSeek model locally on your Windows computer, 
you can follow these steps:

\end{verbatim}
\end{tcolorbox}

\begin{tcolorbox}
\textbf{\texttt{Game Development and Simulations}}
\begin{verbatim}
using System.Collections;
using System.Collections.Generic;
using UnityEngine;

public class Player : MonoBehaviour
{
    // Start is called before the first frame update
    public float speed = 1f;
    
    void Start()
    {
    
        transform.position = new Vector3(0,0,0);
    }

    // Update is called once per frame
    void Update()
    {
        transform.Translate(Vector3.left * speed * Time.deltaTime );             
    }
}


\end{verbatim}
\end{tcolorbox}


\begin{tcolorbox}
\textbf{\texttt{Backend Development and APIs}}
\begin{verbatim}
async def discover_device(emp_user_no, access_token, repositories):
    print(f"dicsover_device")

async def check_device_health(request_type, payload, mesage_id, emp_user_no, 
repositories)
    print(f"check_device_health")


async def

\end{verbatim}
\end{tcolorbox}

\section{Details on Model Ranking}\label{appendix:leaderboard}

\textbf{Computing BT Coefficients.} We estimate $\hat{\beta}$ by running a logistic regression:
\begin{equation}
\hat{\beta} = \arg \min_{\beta \in \mathbb{R}^M} \frac{1}{n}\sum\limits_{i=1}^n \text{CE}(\sigma(X_i^\top \beta), Y_i)
\end{equation}
where $\text{CE}$ represents the cross-entropy loss and $\sigma$ is the sigmoid function. 
We use the sklearn package with l2 penalty and no intercept term. 
We bootstrap the ranking calculation by sampling with replacement for 100 rounds to compute the 95\% confidence interval.
In our leaderboard, we use codestral as an anchor model.




\textbf{BT Ablations.} Since confounding variables (e.g., length of the response or other stylistic formatting~\cite{singhal2023long}) may influence preference judgments, we also control for these variables in the BT model.
Given a set of style features, which include model latency and completion length, we add a style vector to the BT model $\vec{Z}$ where=
$Z_i \in \mathbb{R}^S$ is a vector of $S$ style features comprising the normalized difference between the feature values of both model responses.
The extended BT model includes the style coefficients $\gamma \in \mathbb{R}^{S}$ and can be written as:
\begin{align*}
    \hat{\beta}, \hat{\gamma} = \arg \min_{\beta \in \mathbb{R}^M, \gamma \in \mathbb{R}^S} \frac{1}{n}\sum\limits_{i=1}^n \text{CE}(\sigma(X_i^\top \beta + Z_i^\top \gamma), Y_i)
\end{align*}
where $\text{CE}$ represents the cross-entropy loss and $\sigma$ is the sigmoid function. 
The resulting $\hat{\beta}$ represents model strengths adjusted for style effects, while $\hat{\gamma}$ quantifies the influence of style on user preferences.
$\hat{\beta}$ values are used to create the ordered ranking of models on the leaderboard.

When comparing the original leaderboard (Table~\ref{tab:beta_models}) and the style-controlled version (Table~\ref{tab:style_control}), we see minimal changes to the overall ``tiers'' described in the main text. While we observe some changes in the middle tier (e.g., Llama-3.1-405b, Gemini-1.5-Flash, and Gemini-1.5-Pro swap places as well as GPT-4o and Llama-3.1-70b), we do not observe significant changes \emph{between} tiers.

\begin{table}[h!]
\centering
\caption{$\beta_i$ values for each model bootstraped over 100 samples: their lower, rating, and upper bounds.}
\begin{tabular}{lccc}
\hline
\textbf{Model} & \textbf{Lower bound} & \textbf{$\beta$ estimate} & \textbf{Upper bound} \\ \hline
deepseek-coder-fim         & 0.04  & 0.07  & 0.10  \\
claude-3-5-sonnet-20240620 & 0.02  & 0.06  & 0.09  \\
codestral-2405             & -0.02 & 0.00  & 0.02  \\
llama-3.1-405b-instruct    & -0.07 & -0.04 & -0.01 \\
gemini-1.5-flash-002       & -0.06 & -0.04 & -0.01 \\
gemini-1.5-pro-002         & -0.08 & -0.05 & -0.02 \\
gpt-4o-2024-08-06          & -0.09 & -0.06 & -0.03 \\
llama-3.1-70b-instruct     & -0.10 & -0.07 & -0.04 \\
qwen-2.5-coder-32b-instruct & -0.16 & -0.13 & -0.10 \\
gpt-4o-mini-2024-07-18     & -0.19 & -0.15 & -0.12 \\ \hline
\end{tabular}
\label{tab:beta_models}
\end{table}

\begin{table}[h!]
\centering
\caption{$\beta_i$ and $\gamma_i$ values for each model bootstraped over 100 samples: their lower, rating, and upper bounds.}
\begin{tabular}{lccc}
\hline
\textbf{Model} & \textbf{Lower} & \textbf{Rating} & \textbf{Upper} \\ \hline
deepseek-coder-fim         & 0.05  & 0.08  & 0.11  \\
claude-3-5-sonnet-20240620 & 0.03  & 0.06  & 0.09  \\
codestral-2405             & -0.02 & -0.00 & 0.03  \\
gemini-1.5-flash-002       & -0.06 & -0.03 & -0.01 \\
llama-3.1-405b-instruct    & -0.07 & -0.04 & -0.00 \\
gemini-1.5-pro-002         & -0.09 & -0.05 & -0.01 \\
llama-3.1-70b-instruct     & -0.09 & -0.06 & -0.03 \\
gpt-4o-2024-08-06          & -0.09 & -0.07 & -0.04 \\
qwen-2.5-coder-32b-instruct & -0.16 & -0.13 & -0.10 \\
gpt-4o-mini-2024-07-18     & -0.18 & -0.15 & -0.12 \\ \hline
\hline
\textbf{Model} & \textbf{Lower bound} & \textbf{$\gamma$ estimate} & \textbf{Upper bound} \\ \hline
Model latency       & -0.33  & -0.17  & 0.00  \\
Response length & 0.11  &  0.21 &  0.32  \\ \hline
\end{tabular}
\label{tab:style_control}
\end{table}





\section{Additional Results} \label{appendix:add_results}

We provide additional details about win-rate analysis in Figure~\ref{fig:winrate_task},~\ref{fig:winrate_FiM},~\ref{fig:winrate_context}, and~\ref{fig:winrate_PL}. 
We also showcase an experiment testing our prompting approach in Table~\ref{tab:fim}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.7\linewidth]{figures/winrate_Task.pdf}
    \caption{Win-rate difference based on Task: frontend/backend versus algorithmic design problems.}
    \label{fig:winrate_task}
\end{figure}


\begin{figure}[h]
    \centering
    \includegraphics[width=0.7\linewidth]{figures/winrate_FiM.pdf}
    \caption{Win-rate difference based on FiM: whether the task is FiM or not.}
    \label{fig:winrate_FiM}
\end{figure}


\begin{figure}[h]
    \centering
    \includegraphics[width=0.7\linewidth]{figures/winrate_context.pdf}
    \caption{Win-rate difference based on context length: context length in top versus bottom 20 percentile.}
    \label{fig:winrate_context}
\end{figure}



\begin{figure}[h]
    \centering
    \includegraphics[width=0.7\linewidth]{figures/winrate_PL.pdf}
    \caption{Win-rate difference based on programming language (PL): Non-python code versus Python code.}
    \label{fig:winrate_PL}
\end{figure}






\begin{table}[h!]
\centering
\caption{A controlled experiment with a fixed model, DeepSeek Coder, where we vary whether we use the model's FiM capability or we use Snip-It to post-process the model as we would with other models that do not have native FiM capability. While we were not able to obtain a significant number of votes before deepseek-coder was deprecated, we still observe that $\beta$ estimates are comparable between the two variants. This shows that our Snip-It approach can roughly recover FiM capabilities. }
\begin{tabular}{lccc}
\hline
\textbf{Model} & \textbf{Lower bound} & \textbf{$\beta$ estimate} & \textbf{Upper bound} \\ \hline
deepseek-coder-fim         & 0.04  & 0.07  & 0.10  \\
deepseek-coder & -0.04  &  0.06 &  0.15  \\ \hline
\end{tabular}
\label{tab:fim}
\end{table}

