

\section{Related Work}\label{sec:related}


\textbf{Human Preferences for Evaluations.} 
A diverse set of human preferences---including binary preferences~\citep{bai2022training}, fine-grain feedback~\citep{wu2023fine, kirk2024the}, and natural language~\citep{scheurer2022training}---are increasingly used for training and fine-tuning LLMs~\citep{ouyang2022training}.
Preferences are also important for human-centric evaluation, especially as LLMs are deployed in contexts that involve human interaction.
Platforms like Chatbot Arena~\citep{chiang2024chatbot} and Vision Arenas~\citep{chou2024visionarena,lu2024wildvision} provide a way for users to interact with LLMs and provide paired preference judgments.
However, existing arenas lack integration into actual user environments to reflect the diverse data that may appear in a user's workflow.
We study the use case of LLMs as coding assistants and introduce \systemName to ground preference evaluations in a developer's working environment.



\textbf{Evaluations of LLM Coding Capabilities.} Static benchmarks, e.g., HumanEval~\citep{chen2021evaluating} and MBPP~\citep{austin2021program}, largely focusing on interview-style programming problems have been the most commonly used to evaluate coding capabilities~\citep{lu2021codexglue, nijkamp2022codegen,zhu2022xlcost, wang2022recode, liu2023your, jimenez2023swe, khan2023xcodeeval,yan2023codescope, cassano2023multipl, muennighoff2023octopack, dinh2023large,yang2023intercode}, measured using \texttt{pass@k}. 
Recent benchmarks aim to create more realistic problems, which include multi-turn program evaluations~\citep{nijkamp2022codegen} and repository-level challenges~\citep{jimenez2023swe,jain2024r2e}, and create live benchmarks that reduce contamination risks~\citep{jain2024livecodebench,white2024livebench}.
Our evaluation platform complements the existing suite of benchmarks by contextualizing model evaluations in an actual user's workflow as coding assistants, measuring a model's quality based on user preferences.
Preference data retains signal when models output slightly incorrect, but still useful answers as opposed to a strict or all or nothing when evaluating using test cases.



A growing set of user studies aim to study human interactions with LLMs~\citep{lee2023evaluating}, particularly how programmers use LLM assistance for software development~\citep{barke2022grounded, vaithilingam2022expectation,ross2023programmer,peng2023impact,mozannar2022reading,murali2024ai,chen2024need}.
A notable work by~\citet{cui2024productivity} conducted a field study on GitHub Copilot with many users. 
However, these studies generally face challenges of scale in terms of the number of users and the models considered, primarily relying on commercial tools like GitHub Copilot or ChatGPT.
~\citet{mozannar2024realhumaneval} conducted a study to evaluate six different LLMs of varying performance and ~\citet{izadi2024languagemodelscodecompletion} similarly conducted a study with three different LLMs, but the models evaluated in both studies are no longer considered state-of-the-art.
Our platform aims to address these challenges by building and deploying an actual coding assistant that allows for scalable and adaptable evaluation as new models emerge.












