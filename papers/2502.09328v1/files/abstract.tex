Evaluating in-the-wild coding capabilities of large language models (LLMs) is a challenging endeavor with no clear solution.
We introduce \systemName, a platform to collect user preferences for code generation through native integration into a developer's working environment.
\systemName comprises a novel interface for comparing pairs of model outputs, a sampling strategy optimized to reduce 
latency, and a prompting scheme to enable code completion functionality.
\systemName has served over~\completions suggestions from 10 models and collected over~\sampleRounded pairwise judgements. 
Our results highlight the importance of model evaluations in integrated settings. 
We find that model rankings from \systemName differ from those of existing evaluations, which we attribute to the more realistic distribution of data and tasks contained in \systemName. 
We also identify novel insights into human preferences on code such as an observed consistency in user preference across programming languages yet significant variation in preference due to task category.
We open-source \systemName and release data to enable human-centric evaluations and improve understanding of coding assistants.



\begin{center}
\begin{tabular}{rll}
    \vscode & \textbf{\small{VSCode Download}} & \url{https://lmarena.ai/copilot}\\
    \github & \textbf{\small{Github Repo}} & \url{https://github.com/lm-sys/copilot-arena} 
\end{tabular}
\end{center}