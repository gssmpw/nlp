


\begin{figure}[t]
\centering
\includegraphics[width=0.7\columnwidth]{figures/rankings_comparison1.pdf}
\vspace{-0.5cm}
\caption{We compare model rankings in \systemName (1st column) to existing evaluations, both static benchmarks (2nd-4th column) and live preference evaluations (last two columns).
For existing evaluations, we show the \emph{change} in rank relative to \systemName rank, with positive values in green denoting models performing better on existing evaluations, negative values in red denoting models performing worse, and a dash indicating that the model is not present in the live leaderboard. 
We also report the Spearman rank correlation coefficients between \systemName and other leaderboards.
}
\label{fig:leaderboard-correlations}
\vspace{-0.1cm}
\end{figure}



\section{Model Rankings}\label{sec:leaderboard_calculation}


\subsection{\systemName Leaderboard}

We construct a leaderboard using our user preference judgements.
Let $n$ denote the number of judgments and $M$ the number of models. 
For each battle $i \in [n]$, we define: $X_i \in \{-1, 0, 1\}$: $X_{i,m} = 1$ if model $m$ is presented in the top position, $X_{i,m} = -1$ if presented in the bottom position, and 0 otherwise. The outcome $Y_i \in \{0, 1\}$, where 1 indicates the top model won.
As is standard in other work on pairwise preference evaluation~\citep{chiang2024chatbot,lu2024wildvision}, we apply a Bradley-Terry (BT) model~\cite{bradley1952rank} to estimate the relative strengths of models $\beta \in \mathbb{R}^M$, where the probability $p_{ij}$ that model $i$ beats model $j$ can be modeled as:
\begin{align*}
p_{ij} = \frac{e^{\beta_i}}{e^{\beta_i} + e^{\beta_j}}.
\end{align*}
We bootstrap the battles in the BT calculation to construct a 95\% confidence interval for the rankings, which are used to create a leaderboard that ranks all models, where each model's rank is determined by which other models' lower bounds fall below its upper bound.



Constructing our leaderboard (Figure~\ref{fig:leaderboard-correlations}, 1st column), we find that our leaderboard is segmented into multiple tiers based on the estimated $\beta_i$ values (Table~\ref{tab:beta_models}). 
In the first tier, DeepSeek Coder and Claude Sonnet-3.5 are at the top, with Codestral following closely behind. 
In general, we observe that code-specific models (e.g., DeepSeek Coder and Codestral) are competitive with general-purpose state-of-the-art models (e.g. Claude Sonnet-3.5),
especially if they are trained to infill.
In the second tier, there are 5 models of varying sizes and from different model providers that have relatively similar strengths.
In the final tier, users preferred two models the least. 
In particular, Qwen-2.5-coder is an exception, performing notably worse than other code-specific models.
Implementation of leaderboard computation and additional ablations on provided in Appendix~\ref{appendix:leaderboard}.




\subsection{Comparison against prior evaluations}\label{subsec:comparison}

We compare our leaderboard to existing evaluations which encompass both live preference leaderboards with human feedback and static benchmarks (Figure~\ref{fig:leaderboard-correlations}, 2nd-5th column). 
For human preferences, we compare against Chatbot Arena~\citep{chiang2024chatbot} across both the general leaderboard and the coding subset.
For static coding benchmarks, we select three that are recent and continue to be maintained (of which we have at least 8 out of 10 overlapping models): LiveBench~\cite{white2024livebench}, LiveCodeBench~\cite{jain2024livecodebench}, and BigCodeBench~\citep{zhuo2024bigcodebenchbenchmarkingcodegeneration}.
We do not compare to rankings from any user studies because they are difficult to keep updated in comparison to both static benchmarks and live comparative systems. 


We find the highest correlation (Spearman's rank correlation ($r_s$) of $0.62$) with Chatbot Arena (coding)~\citep{chiang2024chatbot} and similarly high correlation ($r_s=0.48$) with Chatbot Arena (general).
However, we find a low correlation ($r_s\leq0.1$) with most static benchmarks.
The stronger correlation with human preference evaluations compared to static benchmarks likely indicates that human feedback captures distinct aspects of model performance that static benchmarks fail to measure.
We notice that smaller models tend to overperform (e.g., GPT-4o mini and Qwen-2.5-Coder 32B), particularly in static benchmarks.
We attribute these differences to the unique distribution of data and tasks that \systemName evaluates over, which we explore in more detail next.






\section{Data Analysis} \label{sec:comparison}

\input{files/copilot_stats}


\begin{table}[t]
\caption{We compare \systemName with prior evaluations in terms of scale, context length, task type, and code structure. \systemName provides broad coverage across programming languages (\textbf{PL}), natural languages (\textbf{NL}), \textbf{context length} in characters, \textbf{multiple task types}, and structural dimensions---whether the context contains \textbf{code} and fill-in-middle (\textbf{FiM}) tasks are present.  Chatbot Arena (code), which is a subset of Chatbot Arena (general), only contains code in 40\% and infilling in 2.6\% of its input and is denoted by \halfcheckmark. In Figure~\ref{fig:leaderboard-correlations}, we compare against benchmarks that are updated with the latest models (denoted by *). 
}
\label{tab:benchmark_comparison}
\begin{center}
\begin{tabular}{l|cc|cc|c|cc}
\toprule
& \multicolumn{2}{c|}{\textbf{Scale}} & \multicolumn{2}{c|}{\textbf{Context Len}} & {\textbf{Task}} & \multicolumn{2}{c}{\textbf{Structure}} \\
\cmidrule{2-8}
\textbf{Benchmark} & \# PL & \# NL & p50 & p95 & Multi & Code & FiM \\
\midrule
\systemName & 103 & 24 & 1.6k & 18k & \cmark & \cmark & \cmark \\
\hline
HumanEval & 1 & 1 & 0.4k & 0.9k & \xmark & \cmark & \xmark\\
HumanEval-XL & 12 & 23 & 0.4k & 0.9k & \xmark & \cmark & \xmark\\
SAFIM & 4 & 1 & 3k & 5.9k & \cmark & \cmark & \cmark \\
LiveCodeBench* & 1 & 1 & 1.4k & 2.5k & \xmark & \cmark & \xmark \\
LiveBench* & 1 & 1 & 2.3k & 3.9k & \cmark & \cmark & \xmark \\
BigCodeBench* & 1 & 1 & 1.1k & 1.9k & \cmark & \cmark & \xmark \\
\hline
Chatbot Arena (general)* & $\geq17$ & $\geq49$ & 0.7k & 2.9k & \cmark & \halfcheckmark & \halfcheckmark \\
Chatbot Arena (code)* & $\geq17$ & $\geq39$ & 1.4k & 7.8k & \cmark & \halfcheckmark  & \halfcheckmark \\
\bottomrule
\end{tabular}
\end{center}
\end{table}



\subsection{Exploring \systemName Data}\label{subsec:comparison}

Evaluating models in real user workflows leads to a diverse data distribution in terms of programming and natural languages, tasks, and code structures---e.g., context lengths, last-line contexts, and completion structures (Figure~\ref{fig:user_stats}).
We discuss how our data distribution compares against those considered in prior evaluations (Table~\ref{tab:benchmark_comparison}).


\textbf{Programming and natural language:}
Previous benchmarks such as HumanEval~\cite{chen2021evaluating} cover a limited number of languages, primarily focusing on Python and English~\cite{bavarian2022efficient, jain2024livecodebench, white2024livebench,  zhuo2024bigcodebenchbenchmarkingcodegeneration}.
While recent work such as HumanEval-XL~\cite{peng2024humaneval} and SAFIM~\cite{gong2024evaluation} has expanded coverage to up to a dozen programming languages,
\systemName covers 103 programming languages  which is an order of magnitude more than most other benchmarks.
Similarly, while the majority of \systemName users (36\%) write in English, we also identify 24 different natural languages which is comparable to Chatbot Arena (general)~\citep{chiang2024chatbot} and benchmarks focused on multilingual generation 
\citep{peng2024humaneval}. 


\textbf{Downstream tasks:} 
Existing benchmarks tend to source problems from coding competitions~\cite{jain2024livecodebench, white2024livebench}, handwritten programming challenges~\cite{chen2021evaluating}, or from a curated set of GitHub repositories~\cite{gong2024evaluation}.
In contrast, \systemName users are working on a diverse set of realistic tasks, including but not limited to frontend components, backend logic, and ML pipelines (we provide representative examples of the different task clusters in Appendix~\ref{appdx:example_data}).
Coding style problems (i.e., algorithm design) comprise a much smaller portion---18\%---of \systemName's data. 
Further, the distribution of downstream tasks for our in-editor suggestions differs from questions raised by chat conversations, e.g., in Chatbot Arena~\citep{chiang2024chatbot}, where coding questions also focus on code explanation or suggesting commands.


\textbf{Code structures and context lengths:}
Most coding benchmarks follow specific structures, e.g., taking structured docstrings as input~\cite{chen2021evaluating, zhuo2024bigcodebenchbenchmarkingcodegeneration,jain2024livecodebench, white2024livebench} or infilling tasks~\cite{bavarian2022efficient, gong2024evaluation}.
This means that most benchmarks have relatively short context lengths (e.g., all HumanEval~\citep{chen2021evaluating} problems are less than 2k characters).
Similarly,~\citet{chiang2024chatbot} focuses on natural language input collected from chat conversations, with many prompts not including any code context (e.g., 40\% of Chatbot Arena's coding tasks contain code context and only 2.6\% focus on infilling).
As such, input prompts are also relatively short, with 95\% of prompts falling between 1-3k characters.
Unlike any existing evaluation, \systemName is structurally diverse, comprising a mixture of infilling versus code completion and forms of docstring tasks.
Since users are working in actual IDEs, they work on significantly longer inputs: the median context length is around 1.6k characters and 95\% of inputs fall within 18k characters.








\subsection{Understanding User Preferences of Code} \label{sec:analysis}

Given our diversity of input features,  we evaluate how each impacts user preference.
We partition each feature into contrasting subsets (e.g. FiM vs non-FiM), which we refer to as $X$ and $\Tilde{X}$.
For each subset, we compute the win-rate\footnote{Inspecting win-rates helps circumvent potential issues that may arise from applying BT regression to slices with fewer votes.} matrix $W \in \mathbb{R}^{M \times M}$ where $W(X)$ represents the win-rate matrix of subset $X$.
For each feature, we compute a win-rate difference matrix $\Delta \in \mathbb{R}^{M \times M}$, which represents the number of substantial differences in the win-rate between $W(X)$ and $W(\Tilde{X})$. 
\begin{equation*}
    \Delta_{i,j} = \mathbbm{1}[(W_{i,j}(X) - W_{i,j}(\Tilde{X})) > \epsilon]
\end{equation*}
In our analysis, substantial changes are those in the top 90th percentile of win-rate changes ($\epsilon = 0.166$).
Since $M=10$, the maximum amount of significant changes is 90 ($|\Delta| \leq 90$).

We compute $\Delta$ for four input features---task type, context length, FiM, and programming language---where contrasting strata are present in sufficient quantity ($\geq10\%$) within our dataset.
We stratify the data as follows:  
For tasks, we compare frontend/backend against algorithm design.
For context length, we compare the top 20\% against the bottom 20\%.
For FiM, we compare FiM against completion only.
For programming languages, we compare all other programming languages against Python.
We stratify these input features to highlight differences between the data distribution in \systemName compared to static benchmarks (Table~\ref{tab:benchmark_comparison}), where a positive win-rate indicates increased model performance on data that may be considered out of the distribution of typical static benchmarks.
See Appendix~\ref{appendix:add_results} for full data on win-rates.




\begin{figure}[t]
    \centering
    \includegraphics[width=0.7\columnwidth]{figures/heatmap_features1.pdf}
    \caption{
    Significant win-rate changes ($\Delta$) as a result of different data partitions: frontend/backend versus algorithmic problems, long versus short contexts, FiM vs non-FiM, non-Python vs Python.
    We report the number of positive and negative changes (e.g., +1/-2 means that a model improved over 1 model and worsened against 2 models).
    In general, we observe the largest percentage of total changes as a result of differences in task (e.g., frontend/backend versus algorithmic problems), while the smallest effects as a result of differences in programming language.}
    %\cd{might be helpful to display proportions of $X$ and $\tilde{X}$ in this figure. like Non-Python ($51$\%)}}  
    %\wc{Note: too much clutter}
    \label{fig:winrate}
\end{figure}


\textbf{Downstream task significantly affects win-rate, while programming languages have little effect.} 
Changing task type significantly affects relative model performance, with 28 significant win-rate changes (31.1\% of all possible changes).
This gap may indicate that certain models are overexposed to competition-style algorithmic coding problems.
On the other hand, the effect of programming language on win-rates was remarkably small, resulting in only 6 (6.6\%) significant changes, meaning that models that perform well on Python will likely perform well on another language.
We hypothesize that this is because of the inherent similarities between programming languages, and learning one improves performance in another, aligning with trends reported in prior work~\cite{peng2024humaneval}.
Context length and FiM have moderate effects to win-rate, which lead to 16 (17.8\%) and 14 (15.6\%) significant changes respectively.



\textbf{Smaller models tend to perform better on data similar to static benchmarks, while the performance of larger models is mixed.}
For example, Qwen-2.5 Coder performs noticeably worse on frontend/backend tasks (-2), longer contexts (-3), and non-Python settings (-2).
We observe similar trends for the two other small models (Gemini Flash and GPT-4o mini) across multiple features.
We hypothesize that overexposure may be particularly problematic for smaller models.
On the other hand, performance amongst larger models is mixed.
For example, Gemini-1.5 Pro performs noticeably better (+3) on long context which aligns with its goal of long context understanding~\cite{geminiteam2024}.
However, Llama-3.1 405B underperforms on frontend/backend tasks (-4).


\textbf{Surprisingly, models explicitly trained for infilling do not experience large changes to win-rate.} 
Neither DeepSeek Coder, Codestral, nor Qwen-2.5 Coder sees any noticeable performance gains due to FiM. 
We run an experiment using DeepSeek Coder's Chat API with our prompting scheme (Section~\ref{sec:prompting}) rather than FiM, and observe that relative model performance remains consistent (Table~\ref{tab:fim}).
These results suggest that \systemName captures signals about code quality or usefulness rather than just formatting.





















