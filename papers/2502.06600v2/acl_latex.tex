% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.
\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[final]{acl}
\newcommand{\STAB}[1]{\begin{tabular}{@{}c@{}}#1\end{tabular}}
% Standard package includes
\usepackage{times}
\usepackage{latexsym}
\usepackage{amsmath}
\usepackage{xcolor}
\usepackage{anyfontsize}
\usepackage{multirow}
% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
\usepackage{enumitem}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets
\usepackage{booktabs}
% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}
\usepackage{hyperref}
% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}
%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}
\newcommand{\chryssa}[1]{\textcolor{cyan!60!blue}{[CZ: #1]}}
\usepackage{tikz}
\usepackage{tikz-dependency}
\newcommand*\circled[1]{\tikz[baseline=(char.base)]{
            \node[shape=circle,draw,inner sep=.6pt] (char) {#1};}}
% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{Evaluation of Multilingual Image Captioning: \\How far can we get with CLIP models?}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a separate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}



\author{
  \textbf{Gonçalo Gomes\textsuperscript{1,2}},
  \textbf{Chrysoula Zerva\textsuperscript{1,3}},
  \textbf{Bruno Martins\textsuperscript{1,2}}
%  \textbf{Fourth Author\textsuperscript{1}},
%\\
%  \textbf{Fifth Author\textsuperscript{1,2}},
%  \textbf{Sixth Author\textsuperscript{1}},
%  \textbf{Seventh Author\textsuperscript{1}},
%  \textbf{Eighth Author \textsuperscript{1,2,3,4}},
%\\
%  \textbf{Ninth Author\textsuperscript{1}},
%  \textbf{Tenth Author\textsuperscript{1}},
%  \textbf{Eleventh E. Author\textsuperscript{1,2,3,4,5}},
%  \textbf{Twelfth Author\textsuperscript{1}},
%\\
%  \textbf{Thirteenth Author\textsuperscript{3}},
%  \textbf{Fourteenth F. Author\textsuperscript{2,4}},
%  \textbf{Fifteenth Author\textsuperscript{1}},
%  \textbf{Sixteenth Author\textsuperscript{1}},
%\\
%  \textbf{Seventeenth S. Author\textsuperscript{4,5}},
%  \textbf{Eighteenth Author\textsuperscript{3,4}},
%  \textbf{Nineteenth N. Author\textsuperscript{2,5}},
%  \textbf{Twentieth Author\textsuperscript{1}}
\\
  \textsuperscript{1}Instituto Superior Técnico, University of Lisbon
\\
  \textsuperscript{2}INESC-ID
\\
  \textsuperscript{3}Instituto de Telecomunicações
\\
  \small{
    {\{goncaloecgomes, chrysoula.zerva, bruno.g.martins\}@tecnico.ulisboa.pt}
  }
}

\begin{document}
\maketitle
\begin{abstract}
The evaluation of image captions, looking at both linguistic fluency and semantic correspondence to visual contents, has witnessed a significant effort. Still, despite advancements such as the CLIPScore metric, multilingual captioning evaluation has remained relatively unexplored. This work presents several strategies, and extensive experiments, related to evaluating CLIPScore variants in multilingual settings. To address the lack of multilingual test data, we consider two different strategies: (1) using quality aware machine-translated datasets with human judgements, and (2) re-purposing multilingual datasets that target semantic inference and reasoning. Our results highlight the potential of finetuned multilingual models to generalize across languages and to handle complex linguistic challenges. Tests with machine-translated data show that multilingual CLIPScore models can maintain a high correlation with human judgements across different languages, and additional tests with natively multilingual and multicultural data further attest to the high-quality assessments. 
\end{abstract}

\section{Introduction}

Computer-generated image captions are nowadays commonly used as descriptive annotations. The Image Captioning (IC) task has been extensively studied, including in multilingual settings, with many recent approaches combining established vision encoders with large language model decoders~\cite{ramos2023smallcap, ramos2023retrieval, ramos2023lmcap,yang2023multicapclip,geigle2023mblip,ramos2024mpaella}. The automatic evaluation of captions, accounting for linguistic fluency and alignment with visual contents, has also witnessed a significant effort. Approaches like CLIPScore~\cite{hessel2021clipscore} have been proposed to evaluate captions through cosine similarity between image and text embeddings, leveraging large-scale pre-trained vision-and-language models and achieving high correlations with human judgments. Still, despite the many recent advancements, most approaches are English-centric, while multilingual image captioning evaluation has remained relatively unexplored, and lacking in resources. 

This work explores the use of CLIPScore in multilingual captioning evaluation. Given the lack of available benchmarks for the evaluation of multilingual captioning metrics, we propose two different evaluation strategies: (1) using a combination of Machine Translation (MT) and Quality Estimation (QE) models to obtain high-quality multilingual data from English-centric benchmarks, and (2) re-purposing multilingual benchmarks originally designed for evaluation of the semantic inference and reasoning capabilities of vision-language models. 

Through extensive experiments, we show that multilingual CLIP models achieve comparable or even better performance on English benchmarks, while allowing for multilingual assessments. We also propose a multilingual finetuning strategy for CLIPScore, that allows to account for linguistic and cultural diversity while learning from human judgements, resulting in further performance improvements.  Performance generally increases according to model size, and larger models, trained on more data, attained similar or even better performance to methods that extended the original CLIPScore~\cite{sarto2023positive,kim2022mutual,hu2023infometic,kim2023pr,narins2024validated,wada2024polos}. Tests with machine-translated data show that multilingual CLIPScore can also maintain a high correlation with human judgements across different languages, and additional tests with natively multilingual and multicultural data further attest to high-quality assessments across languages.

Our study highlights the importance of multilingual and multicultural research in the evaluation of image captions, aiming to inspire the development of multilingual frameworks in this domain. We show that multilingual models, trained on equally sized datasets, perform just as well as English-only models on English assessments, while also excelling across various evaluation tasks that involve multilingual and multicultural data, making them far more versatile and valuable assets than the corresponding English-centric models.

In sum, our main contributions include (i) a finetuning strategy that accounts for linguistic and cultural diversity, as well as alignment with human judgements (Section \ref{sec: multilingual_clipscore}); (ii) an MT-based extension of English-centric benchmarks to multiple languages, incorporating human evaluations and diverse linguistic phenomena, while preserving the original benchmarks' quality (Sections \ref{sec: multilingual_correlation} and \ref{sec: multilingual_classification}) ; and (iii) an adaptation of existing multilingual and multicultural datasets for captioning metric evaluation (Section \ref{sec: multilingual_classification}). The code and adapted datasets supporting our evaluation experiments are available in a public GitHub repository\footnote{\url{https://github.com/gecgomes/Multilingual_IC_Eval}}.

\section{Related Work}

Conventional image captioning evaluation has relied on reference-based assessments, where machine-generated captions are compared against human-generated ones (i.e., the references). 
Frequently used metrics such as BLEU or CIDEr~\cite{vedantam2015cider}  rely on lexical matches, and hence may fail to capture fine nuances and semantic overlaps in rich captions. A recent shift in the evaluation paradigm involves the use of learned vision-and-language models to enable evaluation through reference-free metrics.

The CLIPScore metric~\cite{hessel2021clipscore} was one of the first proposals for evaluating image captions that departed from traditional metrics. Grounded in a vision-and-language encoder, specifically the original Contrastive Language-Image Pretraining (CLIP) model~\cite{radford2021learning}, this strategy employs a modified cosine similarity between representations for the input image and the caption under evaluation. CLIPScore exhibits a high correlation with human judgments across various datasets, and despite being a reference-free metric, it even surpasses established reference-based metrics like BLEU and CIDEr. Today, CLIPScore-based metrics are widely used for evaluating image captions, and have inspired the development of numerous new evaluation metrics that build on CLIP~\cite{sarto2023positive,hu2023infometic,kim2022mutual}, including reference-based variants.

Despite the advancements, recent studies have noted that CLIPScore can lack rating granularity, emphasising the need for better benchmark datasets to evaluate image captioning metrics~\cite{ahmadi2024examination}. Datasets like VICR~\cite{narins2024validated} or Polaris~\cite{wada2024polos} have employed more rigorous methods for collecting human ratings, but remain limited to English. Alongside these datasets, researchers have shown that models specifically trained for image captioning evaluation can slightly outperform CLIPScore. We corroborate these findings by proposing a finetuned version of CLIPScore that surpasses previous variants.

Recent studies emphasize the importance of evaluating visio-linguistic grounding capabilities with more complex linguistic constructs \cite{parcalabescu2022valse}. Additionally, the absence of multilingual and multicultural benchmarks has been a key topic of discussion. For instance, \citet{kim2023pr} proposed a multilingual image captioning evaluation method. They finetuned the CLIP text encoder using a language-agnostic approach to distinguish between original and perturbed text. Moreover, the authors introduced a new dataset aimed at evaluating multilingual captioning metrics, although it has not yet been made publicly available.

The lack of linguistic and cultural diversity in vision-language datasets has been further criticized by~\citet{liu2021visually}, who argued that relying on English-based lexical databases and image queries introduces a North American and Western European bias, since existing vision-linguistic datasets often fail to capture concepts relevant to non-English languages and cultures outside of Europe and North America. To mitigate this limitation, the authors built MaRVL, which proposes a new protocol for constructing an ImageNet-style class hierarchy that better reflects diverse languages and cultures, together with textual descriptions reflecting these classes. They benchmarked several multilingual multimodal models, but these often performed just above chance, highlighting the challenges of handling out-of-distribution concepts and images. The results call for a reassessment of model robustness and accuracy, and for the development of more inclusive systems.

Similarly, \citet{bugliarello2022iglue} highlighted the importance of multilingual benchmarks, noting that vision-and-language research has primarily focused on English tasks. To address this, the authors proposed a multilingual dataset for evaluating vision-language inference, where models predict entailment relationships between a textual hypothesis and an image premise, expanding the scope of evaluation to include diverse languages.

\section{Multilingual CLIPScore} \label{sec: multilingual_clipscore}
The CLIPScore metric uses an adjusted cosine similarity to compare representations for the input image and the caption being assessed, as originally described by \citet{hessel2021clipscore}. In our work, we adopted the original formulation. A more detailed explanation of the CLIPScore and RefCLIPScore metrics can be found in Appendix~\ref{app:clipscore}.

To boost performance across languages, we propose a strategy to finetune multilingual CLIP models in a setting that considers both linguistic and cultural diversity, while accounting for human preference alignment. Two distinct datasets were used for finetuning. The first, CrossModal-3600, focuses on multilingual captions and multicultural imagery \cite{thapliyal2022crossmodal}, whereas the second, VICR, comprises English image-caption pairs that are evaluated by humans \cite{narins2024validated}, and which we machine translated to different languages following a strict quality-aware translation scheme to help maintain high quality. We also propose the combination of different training objectives tailored to the specific characteristics of each of the two datasets.

In more detail, to enhance the model's ability to process multilingual and multicultural instances, we finetuned it on both datasets using the original CLIP contrastive loss, which can be formally described as follows:
\begin{equation}
\resizebox{0.95\columnwidth}{!}{%
    $\mathcal{L}_C = -\frac{1}{2N}\sum_{i=1}^N\left[\text{log}\frac{e^{s_{i,i}/\tau}}{\sum^N_{j=1} e^{s_{i,j}/\tau}} + \text{log}\frac{e^{s_{i,i}/\tau}}{\sum^N_{j=1} e^{s_{j,i}/\tau}}\right].$
}
\end{equation}
In the equation, $N$ is the number of image-text pairs in a batch, $s_{i,j}$ is the similarity score between the \textit{i}-th image and the \textit{j}-th text description, and $\tau$ is a temperature parameter that scales the similarity scores and helps in controlling the concentration level for the distribution of scores.

For the second dataset, to improve the alignment of CLIPScore with human ratings, we also considered a Pearson correlation loss:
\begin{equation}
\resizebox{0.65\columnwidth}{!}{%
    $\mathcal{L}_P = 1 - \frac{(x-\overline{x})^T(y-\overline{y})}{||(x-\overline{x})||\cdot ||(y-\overline{y})||},$
    }
\end{equation}
where $x$ is the vector of CLIPScore values, $y$ is a vector with the human rating scores, and $\overline{x}$ and $\overline{y}$ are the respective average values. 

Considering that both loss functions can benefit from larger batch sizes, we sample instances for training by alternating between each task, without mixing instances with respect to different losses in the same batch. We accumulate gradients for two steps before updating the network, effectively combining both loss effects while leveraging the benefits of larger batches, i.e., $\mathcal{L} \sim \mathcal{L}_C + \mathcal{L}_P$.

\section{Experimental Evaluation}

This section presents the datasets, the experimental setup, and the results for different CLIP models, considering English, multilingual, and multicultural scenarios for image captioning evaluation.

\subsection{Datasets}

To ensure a fair and comprehensive evaluation of our multilingual models, we extended several well-established English-centric datasets to a multilingual scenario. These datasets, originally developed for assessing human judgments in image-captioning tasks, contain one or more human quality assessments for each image-caption pair.

\begin{itemize}\setlength\itemsep{-0.3em} 
    \item \textbf{Expert} (Flickr8K-Expert), consisting of $5,664$ pairs~\cite{hodosh2013framing}.
    \item \textbf{Crowdflower} (Flickr8K-CF), with a total of $47,830$ pairs~\cite{hodosh2013framing}. 
    \item \textbf{Composite}, containing with a total of $13,146$ pairs~\cite{aditya2015images}.
    \item \textbf{VICR}, with $10,175$ training, $2,310$ validation, and $3,161$ test pairs~\cite{narins2024validated}. 
\end{itemize}

In addition to using the aforementioned datasets featuring human ratings, we also evaluated the robustness of our models to various linguistic phenomena using the \textbf{VALSE} dataset~\cite{parcalabescu2022valse}, which is used to perform evaluation through a binary classification task, and which comprises $6,704$ correct image-caption pairs alongside their foil caption versions.

While the comparison between multilingual and monolingual models on English data offers some insights, it provides a limited perspective on multilingual performance. Unfortunately, high-quality multilingual resources featuring curated human assessments of caption quality are scarce or non-existent, restricting the scope of multilingual evaluation. To overcome this, we developed a translation scheme that leverages large machine translation models~\cite{m2m100,alves2024tower,liu2020multilingual}, paired with language and translation quality estimation models~\cite{rei2022cometkiwi,rei2023scaling}, to translate English captions with pre-existing human assessments into multiple languages. This approach targets high translation quality, {filtering out low-quality translations and thus ensuring the validity of human judgments across target languages}. Further details about our translation scheme can be found in Appendix \ref{app:translation}.

Our language selection is in line with recent machine translation studies~\cite{alves2024tower}, covering high-resource languages (i.e., English, French, German, Spanish, and Chinese) and also mid- (i.e., Portuguese, Italian, and Russian) to low-resource languages (i.e., Dutch and Korean). We translated both the \textbf{VICR} and \textbf{VALSE} datasets into the nine aforementioned languages using our MT scheme. 

In addition, to further expand our multilingual evaluation, we used {natively multilingual and multicultural datasets}, i.e., \textbf{XVNLI}~\cite{bugliarello2022iglue} and \textbf{MaRVL}~\cite{liu2021visually}, {re-purposing them into classification tasks for the evaluation of image captioning metrics}. 
We also expanded both native multilingual datasets by translating them into English, allowing us to compare a multilingual model with its English-only counterpart.

%To finetune our models, we used both VICR and its translation variants, together with the natively multilingual and multicultural dataset named CrossModal-3600~\cite{thapliyal2022crossmodal}.

\subsection{Evaluation Metrics}

We evaluate the different models using correlation with human judgements, and also through classification tasks. Regarding the correlation experiments, we measure performance using three different correlation coefficients, namely Spearman $\rho$ and Kendall $\tau$ with variations $b$ and $c$. The correlation metrics are formally defined in Appendix~\ref{app:metrics}. 

For the classification experiments, we measure accuracy under the assumption that a caption entailed by an image should reflect a higher CLIPScore than a contradiction/foil caption. 

\subsection{Experiments and Results}

This section presents experimental results for the different models and evaluation datasets, establishing a comparison with previously reported results and contributing to the multi-linguistic exploration of existing models and datasets. We also performed a qualitative study focusing on image-caption pairs that feature concepts that could be associated with cultural bias, which is reported in Appendix~\ref{app:qualitative}. 

\subsubsection{Model Selection} \label{sec:model_selection}

\input{tables/english-only-corr}

To select the specific CLIP model to be used in our multilingual experiments, following the CLIPScore methodology described by \citet{hessel2021clipscore}, we first analysed correlation results between CLIPScore values and human ratings across four English datasets and considering publicly available CLIP models. Results are reported in Table \ref{tab:english-only-corr}, indicating that CLIPScore estimates improve significantly with larger CLIP models trained on more data, even if the training data is multilingual and the model is tested on English-centric data.

Apple's public ViT-H/14 model, with a $378^2$ pixel resolution and trained exclusively on English data, achieves the highest correlation results among the different English models. However, a multilingual model of the same size, trained on a mixture of English and multilingual data from LAION and with significantly less English-specific focus (i.e., \href{https://huggingface.co/laion/CLIP-ViT-H-14-frozen-xlm-roberta-large-laion5B-s13B-b90k}{laion/CLIP-ViT-H-14-frozen-xlm-roberta-large-laion5B-s13B-b90k}), emerges as the second-best alternative. 

Both models are comparable in size and were trained on 5 billion instances. While Apple's model relied solely on English data, the multilingual LAION model incorporated a diverse mixture of languages. Despite its reduced exposure to English-specific data, the multilingual model delivers competitive performance on English benchmarks, even surpassing Apple's model in several correlation metrics. This indicates that the multilingual training data enabled effective generalization, enhancing the model's capabilities even for tasks involving only English data.

We selected the \href{https://huggingface.co/laion/CLIP-ViT-H-14-frozen-xlm-roberta-large-laion5B-s13B-b90k}{laion/CLIP-ViT-H-14-frozen-xlm-roberta-large-laion5B-s13B-b90k} variant for further experiments, as it demonstrated the best performance. From this point forward, we will refer to this multilingual model as MLAION (ML), and to our finetuned version as MLAION-F (MLF).

Additionally, experiments detailed in Appendix~\ref{app:english_experiments} demonstrate that simply scaling up model size and training with more diverse data can yield performance on par with more complex captioning evaluation methods. Our study shows that using CLIPScore with better CLIP models can compete and even outperform specialized IC evaluation approaches. Our finetuned multilingual CLIP model outperformed the original multilingual LAION ViT-H and Apple's best model across all CLIPScore variants on well-established human judgment datasets. With references, it managed to surpass specialized architectures like VICR~\cite{narins2024validated}, Polos~\cite{wada2024polos}, InfoMetIC~\cite{hu2023infometic}, or RefPACScore~\cite{sarto2023positive}, achieving state-of-the-art performance comparable to CAMScore~\cite{cui2025evaluating}, G-VEval~\cite{tong2024g}, and CLAIR~\cite{chan2023clair}.

\subsubsection{Correlation on Multilingual Data} \label{sec: multilingual_correlation}

\input{tables/multilingual-results}

Table \ref{tab:multilingual-results} displays the correlation between multilingual CLIPScore values and human ratings across the different languages. Our finetuned version achieves significantly better correlations with human judgments, both in reference-based and reference-free settings, across all evaluated languages and correlation metrics. 
The finetuned CLIPScore model strongly correlates with human preferences in high-resource languages (i.e., English, French, German, Spanish, and Chinese), and it also exhibits equally strong performance in medium- and low-resource languages. Importantly, our finetuned CLIPScore model outperformed the pre-finetuned version even when the latter was using references, achieving a higher average correlation across all metrics without using any references. This observation is particularly useful as it encourages the application of the model to new instances, without requiring additional human input.

Appendix \ref{app:results} provides additional results regarding different model sizes and loss variants for model finetuning. The findings in the appendix support the idea that smaller CLIP models can obtain higher gains in correlation with human judgements when using our finetuning strategy, compared to the original models.

Delving deeper into the impact of MT quality, we note that in an ideal scenario, i.e. assuming perfect machine translation results and balanced CLIP performance across languages, the correlations between CLIPScore values across the different languages would equal one, signifying a perfect alignment. To explore deviations from this ideal behaviour, we use heatmaps to visually represent the interrelationships between CLIPScore values across the different languages.

\begin{figure*}[!t]
  \centering
  \includegraphics[width=2\columnwidth]{images/low_high_heatmap_pearson_2.pdf}
  \vspace{-0.25cm}
  \caption{Pearson correlation scores between different languages, for the original multilingual CLIPScore model (squared cells) and our finetuned version (circular cells). The first heatmap considers the complete set of instances from the VICR dataset, reporting results for both the original and finetuned model versions (lower/upper diagonal values). The second and third heatmaps consider the subset of instances with COMETKiwi scores below/above the 25th/75th percentile value for each language (lower/upper diagonal values), for the original multilingual CLIPScore model and our finetuned model version, respectively.}
  \vspace{-0.25cm}
  \label{fig:heatmap}
\end{figure*}

Figure~\ref{fig:heatmap} presents Pearson correlation scores between languages, for the best multilingual CLIPScore model (presented in squared cells), and for our finetuned version (presented in circular cells). 
The image contains three heatmaps:
\begin{itemize}[]\setlength\itemsep{-0.3em} 
    \item The first, on the left, shows CLIPScore correlations for the full VICR test dataset, comparing the finetuned model (upper triangle with circular cells) and the pre-finetuned model (lower triangle with squared cells).
    \item The second and third heatmaps display correlations based on translation quality. They focus on translations with COMETKiwi scores in the bottom 25\% (lower triangle) and top 25\% percentile (upper triangle). The second heatmap with squared cells represents the pre-finetuned model, and the third heatmap with circular cells shows our finetuned version.
\end{itemize}

In the left heatmap of Figure~\ref{fig:heatmap}, which includes the entire VICR test set, we observe consistently high correlation values across all languages. The results indicate that CLIPScore correlations are influenced by the quality of the translated captions.

The second and third heatmaps show slightly higher correlations for high-quality translations (upper triangles) compared to the poorest translations (lower triangles). However, even the poorest translations still demonstrate relatively high correlations, suggesting that the overall quality of the translations remains strong across different languages.

It is worth noting that, as expected, the most impacted languages are those that use a different script, particularly in the non-finetuned case. We also see a significant improvement in correlations between languages when using our finetuned CLIPScore model. Although this improvement is expected, given that the finetuned model was trained on in-distribution data, it also reflects the high quality of our multilingual training data, further validating our translation strategy.

\subsubsection{Multilingual Classification} \label{sec: multilingual_classification}

This section explores the robustness of the multilingual CLIPScore assessments through different types of classification tasks. Inspired by previous work~\cite{hessel2021clipscore,sarto2023positive} which assessed accuracy in English-only benchmarks, our goal is to delve deeper into the nuanced realm of multilingual and multicultural understanding.

\paragraph{Robustness to Linguistic Phenomena:}
\input{tables/valse_main}
Some of the experiments used our machine translated versions of the VALSE dataset, designed to evaluate the robustness to phenomena such as inconsistencies in numeric quantities or spatial relations~\cite{parcalabescu2022valse}. VALSE comprises seven tests that encompass a range of linguistic structures. In each test, a model is presented with a visual input and is tasked with distinguishing true captions from altered versions (i.e., foils), modified to exhibit a specific feature. The evaluation is based on the assumption that the CLIPScore values for true captions should be higher than those for the corresponding foils. Additional information about the dataset is provided in Appendix~\ref{app:datasets}. 

Table~\ref{tab:valse-results} displays the average performance across different language variants. The results show that our finetuned model delivers the highest average performance across nearly all languages. Compared to the models reported in the original VALSE dataset paper, our MultiLingual Finetuned (MLF) model was only surpassed by the multi-task ViLBERT 12-in-1 model~\cite{lu202012}.

Table~\ref{tab:valse-extra} in Appendix \ref{app:results} contains a more detailed breakdown of the performance across the different tests within VALSE. We observe a significant improvement of 6\% to 10\%, on average, in the existence quantifier, plurality, and counting adversarial tests with the finetuned model. For the remaining tests, we saw a more modest performance increase, ranging from 1\% to 5\% compared to the non-finetuned model, with the exception of the action replacement and co-reference tests where performance did not improve. The lower performance for the co-reference tests likely stems from the nature of the task and limitations of our evaluation method. CLIPScore, which was designed to evaluate declarative captions, struggles to effectively score a caption that combines both a question and a yes/no answer. This mismatch may explain the lower results for coreference handling, and can be a key factor for the higher performance of ViLBERT 12-in-1 in the overall average score shown in Table~\ref{tab:valse-results}, when compared to CLIPScore strategies.

In the VALSE experiments, we also observed that different loss functions contributed distinct benefits to various test scenarios. For example, the Pearson loss proved particularly effective in the existence quantifier and action actant swap tests, compared to the contrastive loss. In contrast, the contrastive loss delivered superior performance in the foil-it and the counting adversarial tests. This demonstrates the advantages of our proposed training strategy, which combines both losses to maximize performance.

\input{tables/marvl_xvnli}

\paragraph{Classification of Multicultural Instances:}

We also used natively multilingual datasets (i.e., XVNLI and MaRVL) to assess the multilingual and multicultural capabilities of CLIPScore models. 

These datasets originally aimed at assessing semantic inference between the contents of images and texts, featuring positive and negative instances whose challenges for interpretation are more likely to lie on fine-grained semantic understanding and compositional reasoning instead of object detection, and with captions featuring slight variations in how the contents of an image are expressed.

Each instance in the XVNLI dataset contains an image-caption pair and a categorical label associated with the relationship between the pair. This label can be either (a) contradiction, (b) neutral, or (c) entailment. Based on these labels, we defined three multilingual classification tasks under this scenario, leveraging concordant/discordant instances as illustrated in Figure~\ref{fig:xvnli-acc}:

\begin{itemize}[align=left]\setlength\itemsep{-0.3em}
    \item [\textbf{Task 1:}] This setting only considers contradiction and entailment instances, under the assumption that the order of the CLIPScore values should match the order of the labels.
    \item [\textbf{Task 2:}] We consider a larger set of duplets and the ordering between the three possible labels. 
    \item [\textbf{Task 3:}]  This setting considers the three possible labels, but we now assess triples of instances $A$, $B$, and $C$, sharing the same image. We only assume a correct classification when we achieve a perfect match between the order of the labels and the CLIPScore values.
\end{itemize}
    
% \textbf{Task 1:} This setting only considers contradiction and entailment instances, under the assumption that the order of the CLIPScore values should match the order of the labels.

% \textbf{Task 2:} In a more challenging scenario, we can consider a larger set of duplets and the ordering between the three possible labels. 

% \textbf{Task 3:} In this case, we also consider the three possible labels, but we now assess triples of instances $A$, $B$, and $C$ from the dataset, sharing the same image. We only assume a correct classification when we achieve a perfect match between the order of the labels and the CLIPScore values.

In the case of the MaRVL dataset, each instance contains a caption, two images, and a boolean label with the value "true" when the caption accurately matches both images, and "false" when the caption matches at maximum only one of the images. The data can be analysed considering two or four instances simultaneously, sharing the same caption but featuring distinct pairs of images. We devise two evaluation tasks based on MaRVL.

\begin{figure}[h!]
  \centering
  \includegraphics[width=\columnwidth]{images/xvnli_marvl.png}
  \caption{The three different XVNLI multilingual classification tasks, where accuracy is defined with basis on comparisons between CLIPScore values.}
  \label{fig:xvnli-acc}
\end{figure} 

\begin{itemize}[align=left]\setlength\itemsep{-0.3em} 
    \item [\textbf{Task 1:}] We consider only two instances at a time. We check whether the highest CLIPScore among the images in a true-labeled instance is greater than the lowest CLIPScore among the images in a false-labeled instance. This accounts for the fact that a false instance may contain one image that aligns with the caption.
    \item [\textbf{Task 2:}] We take a more challenging approach by considering four instances sharing the same caption. The objective in this case is for the highest CLIPScores (both of them) from the true-labeled instances to exceed the lowest CLIPScores from the false-labeled instances
\end{itemize}
% \textbf{Task 1:} We consider the scenario where the CLIPScore for the image that best aligns with the caption should be higher than the CLIPScore for the image that least aligns with the caption.

% \textbf{Task 2:} In a more challenging scenario, we consider sets of four instances sharing the same caption and decide on a correct classification only if all best-aligning captions have a CLIPScore higher than all the least aligning captions. 

Our objective in rearranging both datasets into the aforementioned classification tasks (several tests with increasing difficulty) was to allow for the evaluation of the ability of CLIPScore models to reason with fine-grained semantic distinctions in natively multilingual and multicultural contexts.

In this set of multilingual experiments, we assessed three distinct models: the original multilingual LAION/ViT-H-14 (ML), our finetuned variant (MLF), and the English-only LAION/ViT-H-14 (EN). Apart from the multilingual setup, we employed our translation scheme to convert the native non-English data into English, in order to evaluate English models within these native multilingual scenarios. This approach allows us to compare the benefits of leveraging machine translation during training versus inference, and an indirect assessment of the quality of our MT process.

Table~\ref{tab:xvnli} summarizes the results for the classification tasks from XVNLI. As expected, accuracy declined from Task 1 to 3 for all model variants, indicating that model performance tends to decrease as task complexity increases. Additionally, the English-only model performs worse than our MultiLingual Finetuned (MLF) model across all languages and tasks, with particularly noticeable gaps in Arabic and French (languages for which MLF is also outperformed by the ML variant). This suggests that while machine translation can make non-English data accessible to English-only models, it leads to performance degradation, potentially forgoing some cultural nuances. 

Table ~\ref{tab:marvl}, shows a similar trend for MaRVL, i.e., our MLF model performs on par or even better than other variants for most languages, but displays different patterns for low-resource languages like Tamil and Swahili. The English-only model also performs better in the more difficult accuracy task. This behaviour might reflect the challenges multilingual models face in extremely low-resource languages, where the occurrence is scarce to non-existent in the pretraining data. Additionally, it may relate to the imbalanced quality of out-of-English versus into-English MT, with the latter being better for low-resource languages.

Overall, our findings indicate that the MLF model achieved superior results compared to the original ML model, emphasizing the effectiveness of our training strategy. 

In addition, our analysis using natively multilingual datasets (XVNLI and MaRVL) confirms that a translation pipeline with an English-centric model performs worse than directly using a multilingual model. While back-translating captions into English for evaluation with an English-only model has been explored in prior studies ~\cite{wada2023jaspice}, our findings reveal that this approach significantly increases computational costs and response times without offering any practical advantages during inference, even when translation quality is high. Instead, our results highlight the greater value of machine translation for cost-effective dataset expansion during finetuning, which consistently enhances multilingual CLIP model performance.

A more detailed explanation for both the XVNLI and MaRVL experiments is given in Appendix \ref{app:multilingual_experiments}.

While the experiments with the XVNLI and MaRVL datasets provide interesting insights into the effectiveness of multilingual CLIPScore models, they also involve several important limitations. Considering the XVNLI experiments, previous studies have reported good results in multimodal inference leveraging CLIP~\cite{song2022clip}. However, the authors of SNLI-VE~\cite{xie2019visual}, from which XVNLI is derived, noted that good performance (i.e., an accuracy up to 67\%) can be achieved when looking only at the information in the textual hypothesis, without the visual premise, pointing to significant biases in the XVNLI data. In the case of the MaRVL experiments, given that the captions refer to a pair instead of individual images, the CLIPScore values can be unreliable when attempting to match images to textual sentences. Previous studies have noted that CLIP models can treat inputs as a bag-of-words and suffer from a concept association bias~\cite{yamada2022lemons}, e.g. ignoring the missing information when two concepts are present in one of the inputs while the other only contains a single concept. Hence, there is room for future work that pushes even further the analysis of the robustness of caption evaluation metrics. We note that the proposed process to design classification tasks can eventually transfer to newer datasets that address these limitations.

\section{Conclusions}

This study highlights the importance of expanding image captioning evaluation to include multilingual and multicultural research, encouraging more inclusive frameworks in this field. Using a machine translation scheme with quality filtering, we can cost-effectively extend well-established English-centric benchmarks to multiple languages, without compromising benchmark quality and validity, which can be very beneficial for the finetuning and evaluation of new, multilingual evaluation models.

We also propose a finetuning strategy to better leverage and learn from both multi-cultural data and human preferences, and test our models on a set of different datasets and tasks. Our findings show that multilingual models trained with the same amount but with less English-specific data perform equally well on English tasks, while excelling in multilingual and multicultural ones. This reveals the potential of multilingual models to generalize across languages, making them more versatile assets. Additionally, our finetuning approach significantly boosted the model's ability to handle complex linguistic challenges, such as quantifiers, plurality, and numeric inconsistencies, highlighting its adaptability to more intricate language patterns.

Further to machine translation of English data, we also propose a strategy to adapt multilingual datasets from other tasks to support captioning evaluation. The integration of natively multilingual and multicultural datasets into both training and evaluation processes mitigates cultural information loss, reinforcing the reliability of our proposed pipeline for training and evaluating multilingual CLIP models, and making them effective tools for real-world multilingual and multicultural evaluation.

Overall, our work contributes to multilingual captioning evaluation, both in terms of modelling and benchmarking. We hope it will inspire and support further work in this under-researched field.

\newpage
\section*{Limitations and Ethical Considerations}

Although our work does not raise new ethical issues within the domain of vision-language models (e.g., we conducted our experiments on public datasets carefully designed for academic research and extensively used in previous studies), there are still some concerns which we describe below. 

Models like CLIP are, for instance, notorious for their internal biases, e.g. inherited from the training data itself. We, therefore, recommend caution in the use of the approach proposed in this paper and anticipate further research into the specific issue of model biases before relying on our work beyond research environments. Another important limitation in the work reported in this paper concerns the use of machine translated data in some of the evaluation experiments, which, despite our best efforts to avoid translation errors, can still lead to different types of biases and to the reliance on artificially impoverished language. The development of manually curated benchmarks, specifically designed for the assessment of multilingual metrics for image captioning evaluation, is left as an important challenge for future work.

We also note that we used GitHub Copilot\footnote{\url{https://github.com/features/copilot}} during the development of our research work, and we used ChatGPT\footnote{\url{https://openai.com/chatgpt/}} for minor verifications during the preparation of this manuscript.

\section*{Acknowledgements}

We thank the anonymous reviewers for their valuable comments and suggestions. This research was supported by the Portuguese Recovery and Resilience Plan through project C645008882-00000055 (i.e., the Center For Responsible AI), by Fundação para a Ciência e Tecnologia (FCT) through the projects with references 2024.07385.IACDC and UIDB/50021/2020 (DOI:10.54499/UIDB/50021/2020), by EU's Horizon Europe Research and Innovation Actions (UTTER, contract 101070631), and also by FCT/MECI through national funds, and when applicable co-funded EU initiatives, under contract UID/50008 for Instituto de Telecomunicações.

\bibliography{custom}

\appendix
\clearpage
\input{Appendix}


\end{document}
