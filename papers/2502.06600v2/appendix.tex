\section{The CLIPScore Metric} \label{app:clipscore}

We now formally describe the CLIPScore and RefCLIPScore metrics~\cite{hessel2021clipscore}, which in our study are assessed in multilingual image captioning scenarios. In brief, CLIPScore is based on a modified cosine similarity between representations for the input image and the caption under evaluation. The image and the caption are both passed through the respective feature extractors from a given CLIP model. Then, we compute the cosine similarity of the resultant embeddings, adjusting the resulting value through a re-scaling operation. For an image with visual CLIP embedding $\textbf{v}$ and a candidate caption with textual CLIP embedding $\textbf{c}$, a re-scaling parameter is set as $w = 2.5$ and we compute the corresponding CLIPScore as follows:
\begin{equation}
\text{CLIPScore}({\textbf{c}}, {\textbf{v}}) =  w \times \max(\cos({\textbf{c}}, {\textbf{v}}), 0).
\end{equation}

To compute a corpus-level CLIPScore, e.g. for evaluating the overall quality of a captioning method over a given dataset of images, we can simply average over all the image-candidate pairs. 

Note that CLIPScore does not depend on the availability of underlying references for each of the images in an evaluation dataset. However, an extension named RefCLIPScore was also proposed, which additionally extracts the vector representations $\textbf{R}$ of each available reference with the CLIP text encoder, and computes the harmonic mean of the CLIPScore value from Equation 3, and the maximal reference cosine similarity:
\begin{align}
\text{RefCLIPScore}&({\textbf{c}}, {\textbf{R}}, {\textbf{v}}) = \notag \\
\text{H-Mean}&( \text{CLIPScore}({\textbf{c}}, {\textbf{v}}), \\
            &\max(\max_{{\textbf{r}} \in {\textbf{R}}} \cos({\textbf{c}}, {\textbf{r}}), 0)).\notag
\end{align}

\section{The Correlation Metrics} \label{app:metrics}
This appendix presents a formal definition of the metrics used in the correlation experiments.

Seeing each of our evaluation datasets as sets of $n$ observations with the form $(\hat{y}_1,y_1), \ldots, (\hat{y}_n,y_n)$, for CLIPScore values $\hat{y}_i$ and reference ratings $y_i$, the Spearman correlation coefficient $\rho$ is defined as the Pearson correlation between the results of converting the scores $\hat{y}_i$ and $y_i$ to ranks. 

Instead of using ranks, we can also define any pair of observations $(\hat{y}_{i},y_{i})$ and $(\hat{y}_{j},y_{j})$, where $i < j$, as concordant (or otherwise discordant) if the sort order of the instances agrees (i.e. if either both $\hat{y}_{i} > \hat{y}_{j}$ and $y_{i} > y_{j}$ holds, or both $\hat{y}_{i} < \hat{y}_{j}$ and $y_{i} < y_{j}$). Based on pairs, the Kendall $\tau$ correlation coefficient assesses the strength of association between the CLIPScore values and the reference ratings, with the $\tau_b$ variant accounting for ties and being defined as:
\begin{equation}
\tau _{B}={\frac {n_{c}-n_{d}}{\sqrt {(n_{0}-n_{1})(n_{0}-n_{2})}}},
\end{equation}
where $n_{c}$ is the number of concordant pairs, $n_{d}$ the number of discordant pairs, $n_{0} = n(n-1)/2$, $n_{1} = \sum _{i}t_{i}(t_{i}-1)/2$, $n_{2} = \sum _{j}u_{j}(u_{j}-1)/2$, $t_{i}$ is the number of tied values in the $i^\text{th}$ group of ties for the CLIPScore, and  $u_{j}$ is the number of tied values in the $j^\text{th}$ group of ties for the reference ratings.

In turn, $\tau_c$ accounts with the fact that the underlying scales of the scores are different for CLIPScore and the reference ratings, being defined as:
\begin{equation}
\tau_{c}={\frac {n_{c}-n_{d}}{n_{0}}} \times {\frac {n-1}{n}} \times {\frac {m}{m-1}},
\end{equation}
where $m$ is the number of values in the ranking scale for the reference ratings.

\section{Description of the Datasets}\label{app:datasets}

The following datasets were used in the tests that assessed correlation with human judgment.
\begin{itemize}\setlength\itemsep{0em}
\item \textbf{Flickr8K-Expert \cite{hodosh2013framing}:}
This dataset comprises $16,992$ expert human judgments for $5,664$ image-caption pairs from the Flickr8K dataset. Human assessors graded captions on a scale of 1 to 4, where 4 indicates a caption that accurately describes the image without errors, and 1 signifies a caption unrelated to the image.

\item \textbf{Flickr8K-CF \cite{hodosh2013framing}:} This dataset consists of $145,000$ binary quality judgments collected with CrowdFlower, involving  $47,830$ image-caption pairs with $1,000$ unique Flickr8K images. Each pair received at least three binary judgments, and we use the proportion of {\it yes} annotations as the score for each pair.

\item \textbf{Composite \cite{aditya2015images}:} This dataset contains $13,146$ image-caption pairs taken from MSCOCO (2007 images), Flickr8K (997 images), and Flickr30K (991 images). Each image originally had five reference captions. One of these references was chosen for human rating and subsequently removed from the reference set that is to be used when assessing evaluation metrics.

\item \textbf{VICR \cite{narins2024validated}:} The Validated Image Caption Rating (VICR) dataset features 68,217 ratings, collected through a gamified approach, for 15,646 image-caption pairs involving 9,990 distinct images. The authors of the dataset demonstrated that it exhibits a superior inter-rater agreement compared to other alternatives (e.g., an improvement of 19\% in Fleissâ€™ $\kappa$ when compared to the agreement for the Flickr8K-Expert dataset), and it features a more balanced distribution across various levels of caption quality. In our tests, we used the test split of the VICR dataset, which includes 3,161 image-caption pairs, with 2,000 images from the MSCOCO 2014 validation dataset and 1,161 images from the Flickr8K dataset. When using VICR to finetune CLIP models with a contrastive loss, we used the original image captions from MSCOCO or Flickr8K.
\end{itemize}

All the previous datasets are originally available only for English, but we translated them to nine other different languages using the approach described in Appendix~\ref{app:translation}.

For the experiments that assessed accuracy in terms of distinguishing correct vs incorrect captions, we used the following datasets.
\begin{itemize}\setlength\itemsep{0em}
\item \textbf{VALSE \cite{parcalabescu2022valse}}: VALSE is designed to test visio-linguistic grounding capabilities on specific linguistic phenomena. It is composed by seven tasks, each with the same structure: given a visual input, a model is asked to distinguish real captions from foils, where a foil is constructed from a caption by altering a word or phrase that realizes a specific linguistic phenomenon. The tests include: (a) existential quantifiers, where models need to differentiate between examples (i) where there is no entity of a certain type or (ii) where one or more of these entities are visible in an image; (b) plurality, where models need to distinguish between noun phrases denoting a single entity in an image ({\it exactly one flower}), versus multiple entities ({\it some flowers}); (c) counting, where models needs to differentiate between examples where the specific number of entities in the associated image is correct or incorrect, given the statement; (d) spatial relations, where models need to distinguish between different spatial relations, with foils differing from the original caption only by the replacement of a spatial preposition; (e) actions, particularly (i) action replacement and (ii) actant swaping, where models need to (i) identify whether an action mentioned in the text matches the action seen in the image (e.g., {\it a man shouts} versus {\it smiles at a woman}), and (ii) correctly identify the participants of an action and the roles they play (e.g., is it the man who is shouting or is it the woman); (f) coreference, where models need to perform pronominal coreference resolution, encompassing cases where (i) the pronoun has a noun (phrase) antecedent and pronoun and (noun) phrase are both grounded in the visual modality (e.g., in {\it a woman is driving a motorcycle}, is she wearing a helmet?), and cases where (ii) the pronoun refers to a region in the image or even to the entire image (e.g., {\it is this outside?}); (g) foil-it cases, in which the foil minimally differs from the original caption, only by swapping a important noun.

\item \textbf{XVNLI \cite{bugliarello2022iglue}: } XVNLI is a multilingual dataset for evaluating vision-language inference, challenging models to predict entailment relationships between a textual hypothesis and an image premise. XVNLI includes high/mid-resource languages like Arabic, French, Spanish, Russian, and English. This dataset includes 1,164 instances per language, each featuring an image and two captions in different languages. There are 357 unique images in total.

\item \textbf{MaRVL \cite{liu2021visually}: } MaRVL follows a format similar to the English NLVR2 dataset~\cite{suhr2019corpus} and is designed as a multicultural vision-language reasoning dataset, where the goal is to determine the correctness of a sentence about a pair of images. MaRVL predominantly comprises very low-resource languages: Indonesian, Chinese, Swahili, Tamil, and Turkish. This dataset includes around one thousand instances per language, each featuring two image and one caption. There are 1,411 unique captions in total. The English content is composed by collecting the reverse translations from the low-resource languages into English, as provided by the authors on the original GitHub repository\footnote{\url{https://github.com/marvl-challenge/marvl-code/tree/master/data}}. 
\end{itemize}

We also used an additional English dataset in experiments reported in Appendix \ref{app:english_experiments}.
\begin{itemize}
    \item {\textbf{Pascal-50S} \cite{vedantam2015cider}}: The dataset features preference judgments between pairs of sentences associated to images. There are a total of 4K sentence pairs, evenly split across four categories, such as two human captions, two machine captions, and so on. For each pair, 48 human pairwise judgments were collected\footnote{Instead of being presented with the image, the human annotators were presented only with a reference caption (and the two caption candidates to rank).}. Following prior work, instead of computing correlation coefficients, accuracy is computed. Specifically, the caption preferred by a majority of annotators to be correct is considered, and we measure how often the evaluation metric assigns a higher score to the preferred caption in the pair. Ties are broken randomly.
\end{itemize}

For model training, besides instances in the training split from the aforementioned VICR dataset, we also used data from the natively multilingual CrossModal-3600 dataset (i.e., XM3600, in short). 

\begin{itemize}
\item \textbf{XM3600~\cite{thapliyal2022crossmodal}:} This is a geographically-diverse set of 3600 images annotated with human-generated reference captions in 36 languages. The images were selected from all across the world, covering regions where the 36 languages are spoken, and consistently annotating captions in terms of style across all languages, while avoiding annotation artifacts due to direct translation.
\end{itemize}

\section{The Machine Translation Scheme} \label{app:translation}
This appendix describes the translation scheme that was used to machine translate the datasets used in our experiments. This scheme is designed to mitigate low-quality translations, or hallucinations generated by the machine translation model, thus providing reliable datasets at the end. We specifically used a large (i.e., 1.2 billion parameters) open-access multilingual machine translation model named M2M100~\cite{m2m100}, available on the HuggingFace\footnote{ \url{https://huggingface.co/facebook/m2m100_1.2B}}  model hub. M2M100 was trained on a range of high and low-resource languages from different families and using different scripts, achieving state-of-the-art performance across a diverse set of 100 languages. 

While machine translated data allows us to assess multilingual captioning metrics, the results will depend not only on the performance of the metrics but also on the quality of the translations. Low-quality translations, or hallucinations generated by the translation model, will impact the caption and break our assumption that human ratings for the English data can be transferred across languages. To address this issue, we propose to use the COMETKiwi~\cite{rei2022cometkiwi,rei2023scaling} machine translation quality estimation metric to control for translation quality, assessing the impact of low quality translations on the observed results. 

We specifically began by translating the VICR dataset, followed by the other English datasets mentioned in Appendix D. VICR features English captions with human ratings and also reference captions originally from the MSCOCO and Flickr8K datasets. For each caption, whether a candidate or a reference, we return 25 translations using a beam search technique with 100 beams. Subsequently, we filtered the candidates with a language checker, to ensure proper translation into the intended language. After the language check, we selected for each instance the translation that scored higher based on a large COMETKiwi model\footnote{\url{https://huggingface.co/Unbabel/wmt23-cometkiwi-da-xxl}}.


\section{Assessments on English Data} \label{app:english_experiments}

\input{tables/old-publish}

Table~\ref{tab:old_publish} compares recent studies, proposing other metrics, against the best performing English-only and multilingual CLIPScore models\footnote{Note that we computed all the correlation scores for the CLIPScore variants and traditional image caption metrics based on lexical matches, whereas the results for other more recent metrics are taken from the corresponding publications.}. We considered different evaluation settings, assessing results (a) without references, (b) using references, and (c) combining CIDER with RefCLIPScore when using references~\cite{qiu2023gender}. 

Results confirm that the original CLIPScore outperforms standard captioning metrics in all evaluated datasets, such as BLEU or CIDEr. The correlations consistently improve with RefCLIPScore, but the combination of RefCLIPScore with CIDEr only improved the smaller CLIP model (by previously published results from \citet{qiu2023gender}), instead decreasing the performance in the cases involving the other CLIPScore variants.  

Our study demonstrates that more powerful CLIP models can compete with, and even surpass, complex and specialized systems in image captioning evaluation. The multilingual model performs competitively with the best English-only models across nearly all metrics, whether human references are used or not. This strong performance in both reference-free and reference-aided settings highlights the potential of multilingual CLIPScore. Notably, our finetuned model outperformed the original multilingual LAION ViT-H/14 model and Apple's best model across all CLIPScore variants on the VICR and Expert datasets, even without references. When references were incorporated, our model achieved the best performance on these datasets, surpassing specialized architectures such as VICR, Polos, InfoMetIC, or RefPACScore. By comparing results against current state-of-the-art systems, we found that our finetuned model delivers competitive results with more complex and specialized image captioning evaluation methods like CAMScore, G-VEval, or CLAIR. 

\input{tables/clip-pascal}
\input{tables/reclip-pascal}

Besides the English datasets with numeric ratings discussed in Appendix \ref{app:datasets}, through which we assessed correlations, we used the Pascal-50S dataset \cite{vedantam2015cider} to evaluate model performance on English data. In this dataset, raters made pairwise judgments between pairs of sentences associated with an image. There are 4K sentence pairs in total, split evenly across four categories:
\begin{itemize}\setlength\itemsep{-0.3em}
    \item HC: Two correct captions written by a human; 
    \item HI: Two captions written by a human, but one of the captions is wrong; 
    \item HM: Two correct captions, with one written by a human and one by an algorithm; 
    \item MM: Two correct captions by an algorithm.
\end{itemize}

For each pair, 48 human pairwise judgments of preferred captions were gathered. Following prior work, instead of computing correlation coefficients, we compute accuracy.

Tables ~\ref{tab:clip-pascal} and ~\ref{tab:refclip-pascal} compare the top-performing English and multilingual models, including our finetuned version of the best multilingual model, on the Pascal-50S benchmark using CLIPScore and RefCLIPScore, respectfully. The results show significant improvements after applying our finetuning strategy, particularly in the most challenging tasks (i.e., HC and MM, where both captions are correct and written by a similar entity). Ablation experiments using model checkpoints, trained solely with the contrastive loss or the Pearson loss (i.e., finetuned model with the contrastive loss (C) and finetuned model with the Pearson correlation loss (P), respectively), demonstrated improvements over the original model, suggesting that both loss functions contribute to enhanced performance.

\section{Multicultural Experiments} \label{app:multilingual_experiments}

This appendix details the datasets and experimental settings that were considered for the tests including natively multilingual and multicultural data.
\subsection{Settings for the XVNLI Experiments}
Each instance in the XVNLI dataset contains an image-caption pair and a categorical label associated with the relationship between the pair. This label can be either (a) contradiction, (b) neutral, or (c) entailment. With basis on the labels, we defined three multilingual classification experiments under this scenario, leveraging concordant/discordant instances as illustrated in Figure~\ref{fig:xvnli-acc}:

\textbf{Experiment 1:} This setting only considers instances with the extreme label classes (i.e., contradiction and entailment), noting that some previous studies have pointed to the fact that SNLI-VE, from which XVNLI is derived, has some problems in the annotations for the neutral class~\cite{kayser2021vil}. We compare pairs of instances $A$ and $B$ with the same image, in which the label associated with $A$ differs from the label associated with $B$. When computing CLIPScore values individually for the instances $A$ and $B$, the order of the CLIPScore values should match the order of the labels (i.e., contradiction $<$ entailment).

\textbf{Experiment 2:} In a more challenging scenario, we can consider a larger set of instances and the ordering between the three possible labels (i.e. entailment $>$ neutral $>$  contradiction), i.e. including also the neutral class. Similarly to the previous case, by fixing an image and comparing pairs of captions associated with that image with different labels, we assess the matching of the order between the labels against the CLIPScore values.

\textbf{Experiment 3:} In this case, we also consider the three possible labels, but we now assess triples of instances $A$, $B$, and $C$ from the dataset, sharing the same image. We only assume a correct classification when we achieve a perfect match between the order of the labels and the CLIPScore values.

\begin{figure*}[t!]
  \centering
  \includegraphics[width=2\columnwidth]{images/qualitative.png}
  \caption{Multilingual CLIPScore values for image-caption pairs featuring concepts biased to particular languages.}
  \label{fig:multicultural}
\end{figure*}

\subsection{Settings for the MaRVL Experiments}
For the MaRVL dataset, each instance consists of a caption, two images, and a boolean label with the value "true" when the caption accurately matches both images, and "false" when the caption is incorrect (i.e., because its contents only describe at maximum one of the images instead of both). The data can be analysed considering two or four instances at a time, sharing the same caption but featuring distinct pairs of images. The MaRVL test was designed in such a way that, within the four instances associated to the same captions, two of them are labeled as "true" while the remaining two are labeled as "false". We consider two multilingual classification experiments under this scenario, defined as follows:

\textbf{Experiment 1:} We draw comparisons between pairs of instances with distinct labels but featuring the same caption. For the instances labeled as "true", we compute the CLIPScore values for the images associated with the caption and select the maximum, obtaining the score for the image that best aligns with the caption. Conversely, we perform a similar computation for the instances labeled as "false", this time choosing the minimum CLIPScore value, which results in the score for the image that least aligns with the caption, presumably the incorrect image. The maximum CLIPScore value in an instance labeled as "true" should be higher than the minimum CLIPScore value of an instance labeled as "false".

\textbf{Experiment 2:} In a more challenging scenario, we consider sets of four instances sharing the same caption (i.e., two instances labeled "false" and two instances labeled "true") and decide on a correct classification if both maximum CLIPScore values of the "true" instances are higher than both the minimum CLIPScore values of the "false" instances. 

\section{A Qualitative Study with Captions Featuring Culturally Related Concepts}
\label{app:qualitative}

We performed a small qualitative study on image-caption pairs that feature concepts where some languages should exhibit a particular bias (e.g., {\it codfish} in the case of Portugal, {\it paella} for Spain, {\it beer} for Germany, {\it croissant} for France, {\it ushanka} for Russia, and {\it cheongsam} for China). We attempted to see if the multilingual CLIPScore could distinguish between two plausible captions, where one mentions a specific concept that should better match the image. Figure \ref{fig:multicultural} shows that the multilingual CLIPScore is indeed capable of distinguishing nuanced multicultural concepts, favouring culturally specific captions over generic ones.

\section{Additional Classification Results} \label{app:results}

Table~\ref{tab:valse-extra} presents classification results on the different tasks from the VALSE dataset, separately for each of the considered languages and comparing different finetuning strategies (i.e., without model finetuning, considering only the contrastive loss, only the Pearson correlation loss, or the combined loss function). Results are, in general, better when considering the combined loss function.

In turn, Table~\ref{tab:multilingual-results-2} presents correlation results on the VICR dataset separately for each language and comparing the same two CLIP models under the different finetuning strategies. Results again show that the smaller CLIP model approaches the performance of the larger model, with better results consistently obtained when considering the combined loss function.

\input{tables/table_valse_complete}

\begin{table*}[t!]
\centering
\resizebox{2\columnwidth}{!}{%
\begin{tabular}{l c c c   c   c c c   c   c c c   c   c c c   c   c c c    c   c c c }
 & \multicolumn{11}{c}{\scriptsize Multilingual CLIP ViT-B/32} & ~ & \multicolumn{11}{c}{\scriptsize Multilingual CLIP ViT-H/14} \\ \cline{2-12} \cline{14-24}
 & \multicolumn{3}{c}{\scriptsize Contrastive} & ~ & \multicolumn{3}{c}{\scriptsize Pearson} & ~ & \multicolumn{3}{c}{\scriptsize Combined} & ~ & \multicolumn{3}{c}{\scriptsize Contrastive} & ~ & \multicolumn{3}{c}{\scriptsize Pearson} & ~ & \multicolumn{3}{c}{\scriptsize Combined} \\ 
 \cline{2-4} \cline{6-8} \cline{10-12} \cline{14-16} \cline{18-20} \cline{22-24}  
 & ~~$\tau_b$~~ & ~~$\tau_c$~~ & ~~$\rho$~~ &    & ~~$\tau_b$~~ & ~~$\tau_c$~~ & ~~$\rho$~~ &    & ~~$\tau_b$~~ & ~~$\tau_c$~~ & ~~$\rho$~~ &    & ~~$\tau_b$~~ & ~~$\tau_c$~~ & ~~$\rho$~~ &   & ~~$\tau_b$~~ & ~~$\tau_c$~~ & ~~$\rho$~~ &   & ~~$\tau_b$~~ & ~~$\tau_c$~~ & ~~$\rho$~~ \\  \hline 
ENG      & 65.8 & 72.2 & 81.5 &  & 66.7 & 73.2 & 82.3 &  & 67.2 & 73.5 & 82.6 &  & 68.6 & 75.0 & 83.8 &  & 64.5 & 70.6 & 80.6 &  & \textbf{68.7} & \textbf{75.4} & \textbf{84.1}      \\ \hline
GER      & 65.1 & 71.4 & 80.8 &  & 65.6 & 72.1 & 81.4 &  & 66.5 & 72.6 & 81.9 &  & \textbf{68.0} & 74.3 & 83.2 &  & 62.9 & 69.1 & 79.3 &  & \textbf{68.0} & \textbf{74.8} & \textbf{83.6}      \\
FRE      & 65.4 & 71.7 & 81.1 &  & 66.0 & 72.4 & 81.7 &  & 66.7 & 72.9 & 82.1 &  & \textbf{68.0} & 74.4 & 83.3 &  & 62.8 & 69.0 & 79.3 &  & \textbf{68.0} & \textbf{74.7} & \textbf{83.6}     \\
SPA      & 65.1 & 71.3 & 80.8 &  & 65.9 & 72.6 & 81.7 &  & 66.4 & 72.7 & 81.9 &  & \textbf{67.8} & 74.3 & 83.2 &  & 62.4 & 68.6 & 78.9 &  & \textbf{67.8} & \textbf{74.5} & \textbf{83.5}      \\
CHI      & 64.4 & 70.6 & 80.2 &  & 64.7 & 71.3 & 80.7 &  & 65.7 & 71.9 & 81.3 &  & \textbf{67.4} & 73.8 & 82.8 &  & 62.1 & 68.3 & 78.6 &  & \textbf{67.4} & \textbf{74.0} & \textbf{83.1}     \\ 
POR      & 65.2 & 71.5 & 80.9 &  & 65.7 & 72.1 & 81.4 &  & 66.5 & 72.7 & 81.9 &  & \textbf{67.9} & 74.3 & 83.2 &  & 62.7 & 68.9 & 79.2 &  & \textbf{67.9 }& \textbf{74.6} & \textbf{83.5}      \\
ITA      & 65.0 & 71.3 & 80.7 &  & 65.6 & 72.2 & 81.4 &  & 66.3 & 72.6 & 81.8 &  & \textbf{67.9} & 74.3 & 83.2 &  & 62.5 & 68.7 & 79.0 &  & \textbf{67.9} & \textbf{74.6} & \textbf{83.5}      \\
RUS      & 64.6 & 70.8 & 80.3 &  & 65.1 & 71.6 & 80.9 &  & 65.9 & 71.9 & 81.4 &  & 67.4 & 73.8 & 82.8 &  & 62.7 & 68.9 & 79.2 &  & \textbf{67.6} & \textbf{74.2} & \textbf{83.2}     \\
KOR      & 63.8 & 70.0 & 79.6 &  & 64.3 & 70.8 & 80.2 &  & 65.1 & 71.2 & 80.7 &  & 67.3 & 73.7 & 82.7 &  & 62.1 & 68.3 & 78.6 &  & \textbf{67.4} & \textbf{74.1} & \textbf{83.1}     \\
DUT      & 65.2 & 71.4 & 80.8 &  & 65.8 & 72.4 & 81.6 &  & 66.6 & 72.8 & 82.0 &  & 68.1 & 74.5 & 83.4 &  & 62.9 & 69.1 & 79.3 &  & \textbf{68.2} & \textbf{74.9} & \textbf{83.8}       \\ \hline 
AVG~~    & 65.0 & 71.2 & 80.7 &  & 65.5 & 72.1 & 81.3 &  & 66.2 & 72.5 & 81.8 &  & 67.8 & 74.2 & 83.2 &  & 62.8 & 68.9 & 79.2 &  & \textbf{67.9} & \textbf{74.6} & \textbf{83.5}      \\

\hline 
\end{tabular}
}
\caption{Correlation between multilingual CLIPScore values and human rankings, considering machine-translated versions of the VICR dataset into 9 languages besides the original English. The last row presents macro-averaged correlation results across all the languages (including English). \textbf{Bold} values signify the best score per language.}
\label{tab:multilingual-results-2}
\end{table*}