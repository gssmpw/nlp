\subsubsection{Overview}

\paper{\vspace*{-.3cm}}

\begin{algorithm}[t]
  \encode(initial states)\;
  \While{$\top$}{
    \lIf{the current encoding contains an error state}{\Return{$\unknown$}} \label{alg1:fail}
    \encode(one more unrolling of $\to_\tau$ and of all learned relations)\; \label{alg1:unroll}
    \encode(learned relations must not be used twice in a row)\; \label{alg1:trans}
    \lIf{$\neg\checksat()$}{\Return{$\safe$} \label{alg1:safe}}
    $\sigma \gets \getmodel(); \quad \text{ let } \vec{v}_1 \to_{\tau_1} \ldots \to_{\tau_{k-1}} \vec{v}_{k}$ be the run that corresponds to $\sigma$\; \label{alg1:sigma}
    \If{$\vec{v}_s \to_{\tau_s} \ldots \to_{\tau_{s+\ell-1}} \vec{v}_{s+\ell}$ is a loop \label{alg1:loop}}{
      \If{$\vec{v}_s \not\to_{\pi} \vec{v}_{s+\ell}$ for all learned relations $\to_\pi$}{
        learn a transitive relation from the loop and $\sigma$\;\label{alg1:learn}
      }
      pick learned relation $\to_\pi$ with $\vec{v}_s \to_{\pi} \vec{v}_{s+\ell}$\;
      \encode($\to_{\pi}$ has preference over loops from
 $\vec{v}_s$ to $\vec{v}_{s+\ell}$)\;\label{alg1:block}
%      the $s^{th}$ to the $(s+\ell)^{th}$ state
      backtrack to the point where
$\to_\tau$ was only unrolled $s-1$ times\;
%      step before the loop\;
    }
  }
  \caption{TRL (high level); Input: initial \& error states, relation~$\to_\tau$}
  \label{alg:overview}
  \end{algorithm} 
%
We start with an informal explanation of our approach.
%
Given a relational formula $\tau$, one can prove safety with BMC by unrolling its transition relation $\to_\tau$
$D$ times, where $D$ is the \emph{diameter} \cite{bmc2}.
%
So $D$ is the ``longest shortest path'' from an initial to some other state, or more formally:
\[
  D \Def \sup_{\vec{v}' \text{ is reachable from an initial state}} \left( \min \{i \in \NN \mid \vec{v} \text{ is an initial state}, \vec{v} \to_\tau^i \vec{v}' \}\right)
\]
%
So every reachable state can be reached in $\leq D$ steps.
%
Hence, if BMC finds no counterexample in $D$ steps, the system is safe.
%
\Cref{alg:overview} is a high level version of our new algorithm \emph{Transitive Relation Learning} (TRL),
where this observation is exploited in \Cref{alg1:safe}, as unsatisfiability of the underlying SMT problem implies that the diameter has been reached (see the end of this section for more details).
%
For infinite state systems, $D$ is rarely finite:
%
Consider the relational formula~\eqref{eq:count}



\smallskip
\noindent
\begin{minipage}{0.49\textwidth}
\begin{equation}
  \label{eq:count}
  x' \doteq x + 1
\end{equation}
\end{minipage}
\begin{minipage}{0.49\textwidth}
\begin{equation}
  \label{eq:learned}
  n > 0 \land x' \doteq x + n.
\end{equation}
\end{minipage}

\medskip
\noindent
with the initial state $x \doteq 0$.
%
Then $k \in \NN$ steps are needed to reach a state with $x \doteq k$.
%
So $D$ is infinite, i.e., the diameter cannot be used to prove safety directly.

The core idea of TRL is to ``enlarge'' $\tau$ to decrease its diameter.
%
To this end, our approach ``adds'' transitive relations, which will be called \emph{learned relations} in the sequel, to $\tau$ (\Cref{alg1:learn}).
%
For \eqref{eq:count}, TRL would learn the relational formula \eqref{eq:learned}.
%
Then the diameter of $\eqref{eq:count} \lor \eqref{eq:learned}$ is $1$, as a state with $x \doteq k$ can be reached in $1$ step by setting $n$ to $k$.
%
Transitive relations are particularly suitable for decreasing the diameter, as they allow us to ignore runs where the same learned relation is used twice in a row.
%
This is exploited in \Cref{alg1:trans} of \Cref{alg:overview}, where \encode($P$) means that we add an encoding of the property $P$ to the underlying SMT solver.

This raises the questions \emph{when} and \emph{how} new relations should be learned.
%
Regarding the ``when'', our approach unrolls the  transition relation, just like BMC (\Cref{alg1:unroll}).
%
Thereby, it looks for \emph{loops} (\Cref{alg1:loop}) and learns a relation when a new loop is encountered (\Cref{alg1:learn}).
%
However, as we analyze unstructured systems, the definition of a ``loop'' is not obvious.
%
Details will be discussed in \Cref{sec:loops}.

Regarding the ``how'', TRL ensures that we have a \emph{model} for the loop, i.e., a valuation $\sigma_\Loop$ that corresponds to an evaluation of the loop body (which can be extracted from $\sigma$ in \Cref{alg:overview}).
%
Then given a loop $\tau_\Loop$ and a model $\sigma_\Loop$, apart from transitivity we only require two more properties for a learned relation $\to_\pi$.
%
First, the evaluation that corresponds to $\sigma_\Loop$ must also be possible with $\to_\pi$.
%
This ensures that \Cref{alg:overview} makes ``progress'', i.e., that we do not unroll the same loop with the same model again.
%
Second, the following set must be finite:
\[
  \{\pi \mid \sigma_\Loop \text{ is a model of } \tau_\Loop, {\to_\pi} \text{ is learned from $\tau_\Loop$ and $\sigma_\Loop$}\}
\]
So TRL only learns finitely many relations from a given loop $\tau_\Loop$.
%
While TRL may diverge (\Cref{remark:termination}), this ``usually'' ensures termination in practice.

Apart from these restrictions, we have lots of freedom when computing learned relations, as ``enlarging'' $\tau$ (i.e., adding disjuncts to $\tau$) is \emph{always} sound for proving safety.
%
The \emph{transitive projection} that we use to learn relations (see \Cref{sec:rec}) heavily exploits this freedom.
%
It modifies the recurrence analysis from \cite{kincaid15} by replacing expensive operations -- convex hulls and polyhedral projections -- by a cheap variation of \emph{model based projection} \cite{spacer}.
%
While convex hulls and polyhedral projections are over-approximations (for integer arithmetic), model based projections under-approximate.
%
This is surprising at first, but the justification for using under-approximations is that, as mentioned above, ``enlarging'' $\tau$ is always sound.

Without our modifications, recurrence analysis over-approximates, so we ``mix'' over- and under-approximations.
%
Thus, learned relations are \emph{not} under-approximations, so \Cref{alg:overview} cannot prove unsafety and returns $\unknown$ in \Cref{alg1:fail}.

Learned relations may reduce the diameter, but computing the diameter\paper{\pagebreak[3]} is difficult.
%
Instead, TRL adds \emph{blocking clauses} to the SMT encoding that force the SMT solver to prefer learned relations over loops (\Cref{alg1:block}).
%
Then unsatisfiability implies that the diameter has been reached, so that $\safe$ can be returned (\Cref{alg1:safe}).

For our example \eqref{eq:count}, once \eqref{eq:learned} has been learned, it is preferred over \eqref{eq:count}.
%
As \eqref{eq:learned} must not be used twice in a row (\Cref{alg1:trans}), the SMT problem in \Cref{alg1:safe} becomes unsatisfiable after adding a blocking clause for $s = \ell = 1$ (that blocks \eqref{eq:count} for the $1^{st}$ step), and one for $s = 2$ and $\ell = 1$ (that blocks \eqref{eq:count} for the $2^{nd}$ step).
%
Since we check for reachability of error states after every step (\Cref{alg1:fail}), this implies safety.


%JG I think that this overview on the contents and structure is really needed. Otherwise,
%the reader of the intro does not even know that 
%we also handle unsafety, that there is an implementation and evaluation, and where to
%find the proofs.
After introducing preliminaries in \Cref{sec:preliminaries}, we present our new algorithm
TRL in \Cref{sec:til}.
%
As TRL builds upon \emph{transitive projections}, we show how to implement such a
projection for linear integer arithmetic in \Cref{sec:rec}.
In \Cref{sec:Unsafety}, we adapt TRL to prove unsafety.
%
\report{\Cref{sec:related} discusses related work and we evaluate our approach empirically
in \Cref{sec:experiments}. All proofs can be found in \Cref{sec:proofs}.}
\paper{Finally, in \Cref{sec:related} we discuss related work and evaluate our approach empirically. All proofs can be found in \cite{arxiv}.}
