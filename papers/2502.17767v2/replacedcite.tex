\section{Related work}
We highlight connections of our work to three areas of related work: restarting Krylov methods, Krylov methods for iterative refinement, and backward stable randomized least-squares algorithms.

\paragraph{Restarting Krylov methods}
Our proposal is equivalent, in exact arithmetic, to restarting the PLSQR method, i.e.,
%
\begin{equation} \label{eq:LSQR-restarting}
    \begin{split}
    \vec{x}_1 &\gets \Call{PLSQR}{\mat{A},\mat{P},\vec{b},\texttt{initial}=\vec{0}}, \\
    \vec{x}_2 &\gets \Call{PLSQR}{\mat{A},\mat{P},\vec{b},\texttt{initial}=\vec{x}_1}.
    \end{split}
\end{equation}
%
(Compare \cref{eq:PLSQR-IR}.)
As best we can tell, the proposal to use restarting or iterative refinement \emph{to improve the attainable accuracy of preconditioned Krylov methods} is somewhat novel.
Some references (e.g., \cite[p.~49]{She94a}) advocate for periodically recomputing the residual in CG using the formula $\vec{b} - \mat{A}\vec{x}_i$ to improve its resilience to finite precision effects, which has a similar effect to restarting; however, this technique is an empirical fix and no stability analysis is available.
Residual recomputing does not appear to be a widely adopted practice, and MATLAB's \texttt{lsqr} and \texttt{pcg} methods terminate with errors many orders of magnitude larger than direct methods for the examples considered in this paper.
It is not obvious whether residual recomputing is applicable at all with LSQR.

In contrast to CG and LSQR, restarting the GMRES method is common.
We emphasize that the motivations for restarting/iterative refinement for PLSQR in our work and the typical motivations for restarting GMRES are entirely separate: Restarting for GMRES is done periodically to control computational costs associated with storing and orthogonalizing the full Krylov subspace; restarting GMRES can \emph{hurt} the accuracy and convergence of the method.
By contrast, our work restarts PLSQR only at judicious times, which leads to significant \emph{improvements} in the numerical accuracy. 

\paragraph{Krylov methods as an alternative for iterative refinement}
An alternative line of work (see, e.g., ____) has used preconditioned Krylov methods like GMRES or CG as \emph{replacements} for iterative refinement for solving linear systems of equations using multiple precisions.
The basic idea is this:
Given access to a low-precision \LU factorization $\mat{A} \approx \mat{\hat{L}}\mat{\hat{U}} \eqqcolon \mat{P}$ computed in low precision $u' \gg u$, we may solve $\mat{A}\vec{x} = \vec{b}$ using GMRES with preconditioner $\mat{P}$, applied via triangular substitution.
While this work shares some similarity to ours, their aims are fundamentally different: They use preconditioned Krylov methods to perform the role of iterative refinement in at least two precisions $u', u$, whereas we use iterative refinement to improve the attainable accuracy of preconditioned Krylov methods using iterative refinement in a uniform precision $u$.

\paragraph{Backward stable randomized least squares}
Our paper builds on recent work ____ by a subset of the present authors and Maike Meier, which demonstrated that, provided access to both a high-quality preconditioner \emph{and a high-quality initialization}, a class of preconditioned iterative methods for least squares achieve backward stability with a single step of iterative refinement, resolving the open question ____ of whether \emph{any} fast randomized least-squares solver was backward stable.
The present paper differs from ____ in that we consider square systems, we do not assume access to a high-quality initialization, and PLSQR-IR generally requires more than one step of iterative refinement to attain backward stability.
We emphasize that our results \emph{are not} a consequence of the main theorem in ____.