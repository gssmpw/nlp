\section{Conclusion and Future Work}
\label{sec:conclusion}

We present \method, a novel \ipi defense based on the key observation that successful attacks reduce the dependence between agent tool calls and user inputs. 
Through extensive experiments, we demonstrate that \method significantly outperforms existing defenses while maintaining high utility. Our work establishes that identifying and leveraging fundamental behavioral patterns of \ipi attacks, such as the tool call and user input independence property, provides an effective methodology for defense design.

Our work opens several future directions. 
First, \method can be extended to detect broader attack goals beyond direct task manipulation~\cite{wu2024agentattack}. 
Second, the computational efficiency of masked re-execution can be improved through techniques like KV cache and selective state masking. 
Third, \method's behavioral pattern detection can be combined with other defense approaches like prompt augmentation to create more robust protection mechanisms.



\section*{Impact Statement}
This work advances the security of LLM-based agent systems against indirect prompt injection attacks. While our method introduces additional computational costs, we believe this overhead is justified by the critical importance of protecting agent systems from malicious manipulation. Our defense mechanism helps prevent unauthorized actions while preserving legitimate functionality, contributing to the safe deployment of LLM agents in real-world applications. However, we acknowledge that no security measure is perfect, and continued research is necessary to address evolving attack methods.