\section{Related Work}
\label{sec:rw}

\textbf{Indirect Prompt Injection Attacks.}
At a high level, indirect prompt injection attacks against agents can be categorized as general attacks and agent-specific attacks.
General attacks focus on developing universal attack prompt patterns that force the target agent to conduct the attacker tasks rather than the user tasks.
Notably, the escape character attacks~\cite{pi_against_gpt3} utilize special characters like ``\prompt{\symbol{92}n}'' to manipulate context interpretation.
Context-ignoring attacks~\cite{ignore_previous_prompt, schulhoff2023ignore} explicitly instruct the \llm to disregard the previous context.
Fake completion attacks~\cite{delimiters_url} attempt to deceive the \llm by simulating task completion. 
% Recent work~\cite{chen2024struq} also utilized optimization-based approaches using techniques like GCG~\cite{zou2023universal} to optimize the attack pattern.
These methods are often tested on \ipi benchmarks~\cite{debenedetti2024agentdojo,xu2024advweb} with pre-specified injection points and attack tasks.
There are also some early explorations of \llm attacks against a specific type of agent.
For example, attacks against web agents inject the attack content into the web pages to ``fool'' the agent into the attack tasks~\cite{wu2024agentattack,liao2024eia,xu2024advweb}. 
Attacks against computer agents manipulate the computer interface~\cite{zhang2024attacking}. 
Note that there are also some direct prompt injection attacks against LLMs~\cite{yu2023assessing, wu2024agentattack, wu2024new, toyer2024tensor}.
These methods directly append the attack prompts after the user inputs, which may not be practical in real-world applications.

\textbf{Defenses against \ipi.}
% Unlike jailbreaking attacks that explicitly prompt \llm to generate harmful content, defending against \ipi poses unique challenges due to its stealthy nature: both injected malicious prompts and their consequences can appear as legitimate tasks. 
Existing defenses can be categorized based on resource requirements.
Defenses that require additional training resources either conduct adversarial training of the LLM(s) in the target agent~\cite{wallace2024instruction, chen2024struq, chen2024aligning} or add additional models to detect whether the inputs contain injected prompts~\cite{deberta-v3-base-prompt-injection-v2, inan2023llama}. 
However, these methods face practical limitations due to their substantial computational and data requirements.
In addition, adversarial training may jeopardize the model's normal utility in broader application domains.
As we will show later, adding additional detection models naturally harms the agent's utility under attack and suffers from high false negative rates.

Training-free defenses either design additional prompts for the user inputs or constrain the allowed tool calls of the agent. 
First, most training-free defenses explore additional prompts that either help the model ignore or detect potential attack instructions in the retrieved data.
Specifically, ignorance strategies include adding a delimiter between the user prompt and retrieved data~\cite{hines2024defending, alex2023ultimate, delimiters_url}, repeating the user prompt~\cite{learning_prompt_sandwich_url}
Such defenses, while lightweight, have limited efficacy against stronger attacks (as shown in Sec~\ref{sec:eval}).
Known-answer detection~\cite{liu2024formalizing} adds additional questions with known answers to the user prompt and detects if the model finally outputs the answer. 
However, this method can only identify injections post-execution, when attacks may have already succeeded.
Second, tool filtering~\cite{debenedetti2024agentdojo} allows LLMs to select a set of permitted tools for the given user task and block all calls to unauthorized tools.
This approach harms utility as the LLMs sometimes filter out necessary tools. 
More importantly, it is easy to bypass as the attackers can design their attack tasks with only the tools related to the user attack.  
In comparison, our method is a lightweight and highly effective training-free defense that well maintains the agent's normal utility. 

Note that other defenses require human intervention~\cite{wu2025isolategpt}, white-box model access~\cite{wu2024system}, or reverting agent actions~\cite{patil2024goex}.
Due to these strong assumptions and lack of full automation, we exclude these approaches from our analysis.











%%%%%%%%%%%%%%%%%%%%%%% Old text %%%%%%%%%%%%%%%%%%%%%%%%%%%



% agents directly inject the 

% zhang2024attacking,

% Prompt injection attacks can be classified into direct and indirect injection. 
% Indirect prompt injection (\ipi), which is our focus, targets agent systems by manipulating information retrieved. Based on their objectives, \ipi attacks can be categorized into two types. 
% First, illusion attacks, which preserve the user's original task while subtly manipulating retrieved information, for example, such as manipulating prices in shopping agents~\cite{wu2024agentattack} or spoofing login interfaces~\cite{liao2024eia}. 
% Second, goal misdirection attacks, which aim to redirect the \llm to perform malicious tasks~\cite{debenedetti2024agentdojo, zhang2024attacking, xu2024advweb}. Existing \ipi approaches primarily develop universal adversarial patterns applicable across models and tasks. For example, the escape character attacks~\cite{pi_against_gpt3} that utilize special characters like ``\prompt{\symbol{92}n}'' to manipulate context interpretation, context-ignoring attacks~\cite{ignore_previous_prompt, schulhoff2023ignore} that explicitly instruct the \llm to disregard previous context, and fake completion attacks~\cite{delimiters_url} that attempt to deceive the \llm by simulating task completion. 

% Direct prompt injection~\cite{yu2023assessing, wu2024new, toyer2024tensor} involves adversarial attempts to extract sensitive information, such as system prompts, through manually crafted or model-generated inputs. 


% \paragraph{Prompt Injection Attacks}
% Prompt injection attacks can be classified into two primary categories: direct and indirect injection. Direct prompt injection~\cite{yu2023assessing, toyer2024tensor} involves adversarial attempts to extract sensitive information, such as system prompts, through manually crafted or model-generated inputs. Indirect prompt injection (\ipi), which is the focus of this work, targets LM systems by manipulating information retrieved through external tools such as RAG~\cite{zhong-etal-2023-poisoning, zou2024poisonedrag, chen2024agentpoison, lee2024prompt}, tool use~\cite{fu2023misusing, debenedetti2024agentdojo, wu2024agentattack, liao2024eia, xu2024advweb}, and LM recommendation~\cite{kumar2024manipulating, nestaas2024adversarial}. \ipi attacks can be categorized into two types based on their objectives~\cite{wu2024agentattack}: (1) illusion attacks, which preserve the user's original task while subtly manipulating retrieved information (e.g., manipulating retrieved car prices), and (2) goal misdirection attacks, which aim to redirect the \llm to perform malicious tasks. The latter category typically pose more severe security risks. In this paper, we focus on the goal misdirection attacks. 

% % Unlike traditional adversarial images or jailbreak prompts that optimize attack patterns for specific models and tasks, 
% Most existing \ipi approaches focus on generating universal adversarial patterns applicable across models and tasks. The escape character attacks~\cite{pi_against_gpt3}, which utilize special characters like ``\prompt{\symbol{92}n}'' or ``\prompt{\symbol{92}r}'' to manipulate context interpretation. Context-ignoring attacks~\cite{ignore_previous_prompt, schulhoff2023ignore} explicitly instruct the \llm to disregard previous instructions and context. And fake completion attacks~\cite{delimiters_url} attempt to deceive the \llm by simulating task completion. \citet{liu2024formalizing} introduces combined attacks by combining the above three attacks. \citet{debenedetti2024agentdojo} proposed important instruction attack by emphasize the malicious instructions are more important, they further enhanced these attacks by incorporating additional context to convince \llm the embedded malicious instructions are trusted. \citet{chen2024struq} further applied GCG~\cite{zou2023universal} to optimize the attack patterns to achieve higher attack success rate.


% many attacks in the following doc is missing https://docs.google.com/document/d/1i5B1tp1srUbPj7nC4mWXRz7OkPbLW3c09r8Qyo6MShU/edit?tab=t.0#heading=h.lzqyp6glpp9j
% like the attack against web agent and code agent and pop-up

% Direct and indirection
% general 
% design attack prompts 
%1
%2 
%3
%4 
% design benchmarks 
% benchmarks in different types of agents
% agentdojo for personal assistant
% webarnea for web agent
% XXXX which is similar as agentdojo

% there are some early exploration of  agent-specific attacks

% RAG attack 
% we apply our method to RAG
% AgentPoison: Red-teaming {LLM} Agents via Poisoning Memory or Knowledge Bases
% --> this work is about poisoning not prompt injection

% WebAgent:
% Dissecting Adversarial Attacks on Multimodal LM Agents
% Maybe not the following works given their weird threat model:
% EIA: Environmental Injection Attack on Generalist Web Agents for Privacy Leakage
% AdvWeb: Controllable Black-box Attacks on VLM-powered Web Agents
% they either achieves low asr or impractical; why they are not practical

% Coding agent:
% A New Era in LLM Security: Exploring Security Concerns in Real-World LLM-based Systems (2024)
% specific to chatgpt pluge-in, which is already out of date (actively maintained and used)
% --> added it into related work, more like direct prompt injection

% jailbreaking attacks against agent which we do not consider
% RedCodeAgent: Automatic Red-teaming Agent against Code Agents
% Take a look 
% --> it is about jailbreak

% Personal assistant:
% Attacking Vision-Language Computer Agents via Pop-ups
% Take a look 
% --> added it into related work, it is similar to visualwebarena-adv.


% move the jailbreak attacks and defenses to expeirment part

% training-time and test-time approaches. At training time, models are trained on prompt injection datasets to maintain proper priority hierarchies between system and user prompts~\cite{wallace2024instruction, chen2024struq, chen2024aligning}. Test-time defenses encompass two complementary strategies: augmentation and detection. On the augmentation front, researchers have developed several techniques, including delimiter-based prompt enhancement~\cite{hines2024defending, alex2023ultimate, delimiters_url}, prompt repetition~\cite{learning_prompt_sandwich_url}, tool filtering~\citet{debenedetti2024agentdojo}, and isolated environments~\cite{wu2025isolategpt}. In parallel, detection-based approaches operate at two distinct stages of model interaction: pre- and post-response. Pre-response methods leverage classifiers to identify suspicious instructions in retrieved data before it enters the agent system~\cite{deberta-v3-base-prompt-injection-v2, inan2023llama}, whereas post-response techniques analyze the agent's output for injection signatures. Among post-response approaches, known-answer detection~\cite{liu2024formalizing} appends verification instructions and monitors specific output markers, though its effectiveness is limited to simple injection cases. Similarly, attention tracking~\cite{hung2024attention} offers another detection avenue by measuring attention allocation to original prompts, but requires white-box model access and risks false positives.