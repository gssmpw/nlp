\section{Introduction}
\label{sec:intro}

% background and problem
Together with the recent success of LLM agents~\citep{openai_tool_use, anthropic_tool_use, llama_tool_use, deepseek2025} comes the serious security concern of indirect prompt injection attacks~(\ipi)~\cite{naihin2023testing, ruan2024identifying, yuan2024rjudge, liu2024formalizing, zhan24injecagent, debenedetti2024agentdojo, zhang2024agent}.
Attackers exploit the agent's interaction with external resources by embedding malicious tasks in tool-retrieved information such as database~\cite{zhong-etal-2023-poisoning, zou2024poisonedrag} and websites~\cite{liao2024eia, xu2024advweb, wu2024agentattack}.
These malicious tasks will force the agent to take unauthorized actions, leading to severe consequences. % (e.g., privacy leakage and financial losses). 

\begin{figure}[t!]
    \centering
    \includegraphics[width=0.5\textwidth]{figs/average_scatter.pdf}
    \vspace{-0.3in}
    \caption{Comparison of averaged Utility under Attack (UA, higher is better) performance and Attack Success Rate (ASR, lower is better) on \gpt, \othree, and \llama across different defense methods. %The x-axis show the UA percentage, indicating how well each defense method maintains model functionality under attacks. The y-axis show the ASR percentage. 
    Our proposed methods (\method and \method-Aug) achieve superior performance with extremely low ASR while maintaining high UA, outperforming all the baseline defense methods. Detailed comparisons among these defenses are in \cref{subsec:sec4-results}.}
    \label{fig: average_scatter}
    \vspace{-.1in}
\end{figure}

% existing works and limitations
Defending against \ipi attacks is significantly challenging.
First, unlike jailbreaking \llm, the injected malicious prompts and their resultant behaviors can be legitimate tasks.
Second, implementing effective defenses requires a careful balance between security guarantees and utility maintenance. 
Existing \ipi defenses either require essential model training resources, are only applicable to simple attacks, or harm normal utilities under attack scenarios.
Specifically, resource-expensive defenses retrain the LLM in the agent~\cite{chen2024struq, wallace2024instruction} or train an additional model to detect injected prompts in the retrieved data~\cite{deberta-v3-base-prompt-injection-v2}. 
Such methods are less practical due to the greedy resource requirements. 
Furthermore, adversarial training may jeopardize the model's normal utility, while model-based detection naturally harms the agent's utility under attack scenarios and suffers from high false negative rates (\cref{subsec:sec4-results}).
Existing training-free defenses either augment the user inputs with additional prompts~\cite{alex2023ultimate, hines2024defending, learning_prompt_sandwich_url} or filter out malicious tool calls~\cite{debenedetti2024agentdojo}.
As shown in~\cref{subsec:sec4-results}, prompt augmentation methods maintain high utility but fail to prevent sophisticated attacks, while tool filter achieves low ASR at the cost of severely degrading utility. 

% Our method and insight
% \kaijie{do you think we need to make this part more clear?}
In this paper, we proposed a novel \ipi defense, \method, based on the key insight that the agent's tool calls are less dependent on the user inputs when subjected to attacks.
\method re-executes the agent's action trajectory with masked states, where only retrieved outputs are preserved and the user inputs are masked by a masking function. 
Then, \method detects attacks by comparing tool calls between the original execution and a masked re-execution.
When similar tool calls are found at a certain step, it indicates an attack since the tool calls are unrelated to the user's input.
We introduce three key designs to further strengthen \method: a customized masking function to prevent arbitrary tool calls during the masked execution; a tool call cache for the masked execution to better identify attacks in the original execution; and a focused tool call comparison mechanism to knock off noisy information.
%\kaijie{we already said that we compare tool calls before, here we emphasize it again, but deleting this will cause inconsistency.}
These designs resolve key technical challenges discussed in~\cref{subsec:overview}, significantly reducing false positives and false negatives.   

% characteristics
Through extensive experimentation on the \dojo benchmark using three LLMs: \gpt, \othree, \llama, we demonstrate that \method and \method-Aug (combining \method with prompt augmentation) significantly outperforms five SOTA defenses against four SOTA attacks.
As shown in~\cref{fig: average_scatter}, \method and \method-Aug archive the lowest attack success rate while maintaining the normal utility for both benign and attack scenarios.
Specifically, \method-Aug creates synergistic effects, further reducing ASR to $0.32$\% while maintaining $68.72$\% utility on \gpt. 
In addition, we also conduct an ablation study to validate our three key designs and show \method's insensitivity to key hyper-parameters.
To our knowledge, \method is the first \ipi detection that leverages the independence between malicious tool calls and user input and achieves so far the best balance between security and utility maintenance.





% \kaijie{a summarization of our contributions here?}

% Our results reveal several key findings:

% \begin{itemize}
%     \item Existing defense approaches face fundamental limitations: prompt augmentation methods (delimiting and repeat prompts) maintain high utility but fail to prevent sophisticated attacks; tool filter achieves low ASR at the cost of severely degrading utility; and model-based detections suffer from high false negatives while still failing to maintain utility under attacks.
    
%     \item \method, overcomes these limitations by masked re-execution and tool comparison, achieving both superior attack prevention and high utility. 
    
%     \item Integration with prompt augmentation (\method-Aug) creates synergistic effects, further reducing ASR to 0.08\% while improving utility to 70.79\% on the \gpt model. 
    
%  %    \item Our approach effectively handles edge cases in which users explicitly request execution of external instructions, addressing a critical security vulnerability that other methods overlook.
% \end{itemize}

% % contributions 
% To summarize, our key contributions are:
% \begin{itemize}
%     \item A novel post-response detection framework that fundamentally reimagines \ipi defense through behavioral divergence analysis. 
%     Unlike existing approaches that attempt to prevent attacks through pre-response filtering or prompt modification, our method identifies attacks by analyzing the convergence of parallel execution paths, achieving both high task completion utility and near-zero ASR.

%     % \item A theoretical foundation for detecting \ipi attacks that introduces the concept of behavioral divergence in parallel execution paths. This provides a principled approach to distinguishing between legitimate tool outputs and malicious instructions, establishing a new paradigm for securing AI agent systems.

%     \item A hybrid defense strategy that demonstrates how post-response detection can be effectively combined with prompt augmentation techniques. This integration (\method-Aug) achieves synergistic improvements in both security and utility, particularly against sophisticated attacks.

%     \item Comprehensive empirical evaluation across multiple benchmarks and attack types, demonstrating that our approach significantly outperforms existing methods in AI agent system deployment. Our analysis provides insights into the trade-offs between different defense strategies and their implications for practical applications.
% \end{itemize}