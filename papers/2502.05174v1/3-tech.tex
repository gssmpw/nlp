

% \begin{figure*}[t!]
% \centering
% \includegraphics[width=0.7\textwidth]{figs/melon.pdf}
% \caption{The pipeline of proposed \method detection method. Green and red boxes indicate benign and potentially compromised states respectively. The method compares tool calls between the original execution path (top) and an instruction-seeking path (bottom) to detect indirect prompt injection attacks. The red arrow in tool execution indicates potential malicious instruction injection.}
% \label{fig: overall}
% \end{figure*}

\begin{figure*}[t!]
\centering
\includegraphics[width=\textwidth]{figs/melon_horizon.pdf}
\vspace{-.3in}
\caption{\method detection pipeline comparing two parallel execution paths: the original run (top) and masking run (bottom). The original run processes the user task $\mathcal{T}_u$ while the masking run uses a task-neutral prompt $\mathcal{T}_f$. Green boxes show benign execution where no malicious task is detected (left), as the masking run generates no matching tool calls. Red boxes indicate prompt injection attack (right), where the agent deviates to execute an injected malicious task $\mathcal{T}_m$, resulting in matching tool calls between the two paths. Tool calls from the masking run are stored in the tool call cache $\mathcal{H}_t$ for comparison.}
\label{fig: overall}
\vspace{-.15in}
\end{figure*}


\section{Metholody of \method}
\label{sec:tech}
\subsection{Preliminaries}
\label{subsec:prelim}
\textbf{Formalization and Definition of LLM Agent.}
% There is no standardized definition of LLM agents. 
In this work, we define an LLM agent $\pi$ as an integrated system comprising LLM(s) and a set of tools $\mathcal{F} = \{f_1, ..., f_n\}$ for environment interaction.
The agent receives a user prompt specifying a task $\mathcal{T}_u$ (e.g., ``\prompt{Summarize my agenda and tell me the time of the next event.}'') and executes it through a structured multi-step procedure.
% ~(Appendix~\cref{fig: basic agent pipeline}).

At each step $t$, we define the \textit{state} as $\mathcal{S}_t = (\mathcal{T}_u, \mathcal{A}_{1:t}, \mathcal{O}_{1:t})$, where $\mathcal{T}_u$ is the user task, $\mathcal{A}_{1:t} = \{ (\mathcal{R}_1, \mathcal{C}_1), ..., (\mathcal{R}_t, \mathcal{C}_t) \}$ is the sequence of LLM-generated actions with each action pair consisting of an LLM response $\mathcal{R}_i$ and a set of tool calls $\mathcal{C}_i = \{c_i^1, ..., c_i^{m_i}\}$. Each tool call $c_i^j$ specifies a tool $f_j \in \mathcal{F}$ and its parameters (e.g., ``\prompt{retrieve\_event(date=20250131)}''). $\mathcal{O}_{1:t} = \{\mathcal{O}_1, ..., \mathcal{O}_t\}$ denotes the sequence of observations, where each $\mathcal{O}_i$ contains the tool execution outputs corresponding to $\mathcal{C}_i$. In step $t+1$, The agent system first generates action $\mathcal{A}_{t+1} = \pi(\mathcal{S}_t)$ based on previous state, then obtaining observation $\mathcal{O}_{t+1} = \text{Exec}(\mathcal{C}_{t+1})$ by executing the tool calls. This process continues iteratively until the user task $\mathcal{T}_u$ is completed or errors occur.



\textbf{Threat Model.}
We follow the assumption of \ipi, where attackers \textit{cannot access} the LLMs' input and output inside the target agent.
Their access is limited to manipulating the external information retrieved by the agent via tool calls, such as websites, emails, or files. 
The attackers aim to redirect the agent from executing the original user task to performing a malicious task $T_m$.
For example, the attacker task could be ``\prompt{Send your bank account and password to hacker@gmail.com}''.
We denote $\mathcal{O}_t'$ to be the tool execution outputs injected with $T_m$ and $\mathcal{O}_{1:t}'=\{\mathcal{O}_1, ..., \mathcal{O}_{t}'\}$ as the sequences of previous tool execution outputs.
% while the user task is ``\prompt{Summarize my agenda and tell me the time of the next event.}''. 
We assume all user tasks to be legitimate and the defender has complete access to the entire agent system, including the states $\mathcal{S}_t = (\mathcal{T}_u, \mathcal{A}_{1:t}, \mathcal{O}_{1:t})$. 
However, we do not assume the defender has the resources to train LLMs or can access LLM internal representations. 
% We also do not assume the defender has the privilege to alter tools. 
% We believe this is a practical and reasonable assumption in that most agent users do not have the rich computational power, and the tools are typically developed by third parties where agent users only have permission to use them rather than change them. 


\subsection{Technical Overview}
\label{subsec:overview}
\textbf{Insights and Technical Challenges.}
Our design is based on the key observation that whenever a malicious attacker task $T_m$ is present in the retrieved data, it attempts to redirect the agent from executing the user task $T_u$ toward executing $T_m$ instead.
Given a state $\mathcal{S}_t = (\mathcal{T}_u, \mathcal{A}_{1:t}, \mathcal{O}_{1:t}')$, if $\mathcal{O}_{t}'$ that injected with $T_m$ successfully hijacks the agent's behavior to focus on executing $T_m$, it induces a \textit{state collapse} where the agent's next action $\mathcal{A}_{t+1}$ becomes conditionally independent of $\mathcal{T}_u$ and $\mathcal{A}_{1:t}$, depending primarily on $\mathcal{O}_{1:t}'$. For benign cases where $\mathcal{O}_{t}$ does not contain malicious instructions or the attack does not succeed, the agent maintains functional dependencies on all state components $(\mathcal{T}_u, \mathcal{A}_{1:t}, \mathcal{O}_{1:t})$. 
Formally, for a successful attack at step $t$, we can observe:
$
    \mathbb{P}_\pi( \mathcal{A}_{t+1} | (\mathcal{T}_u, \mathcal{A}_{1:t}, \mathcal{O}_{1:t}')) \approx \mathbb{P}_\pi( \mathcal{A}_{t+1} | \mathcal{O}_{1:t}'),
$
where $\mathbb{P}$ is the probability. For benign executions, the agent's actions maintain their dependency on the user inputs:
$
    \mathbb{P}_\pi( \mathcal{A}_{t+1} | (\mathcal{T}_u, \mathcal{A}_{1:t}, \mathcal{O}_{1:t})) \gg \mathbb{P}_\pi( \mathcal{A}_{t+1} | \mathcal{O}_{1:t}).
$
% where either $\mathcal{O}_{1:t}$ is benign, or the LLM maintains its adherence to $\mathcal{T}_u$ despite the presence of malicious task $\mathcal{T}_m$ in $\mathcal{O}_{1:t}$. 


This statistical discrepancy naturally leads to a masking-based approach to detect the injected prompts.
Let $\mathcal{M}: \mathcal{S} \rightarrow \mathcal{S}$ be a masking operator that preserves only original tool execution outputs. We define two execution paths:
\begin{align*}
    \mathcal{A}_{t+1}^o = (\mathcal{R}_{t+1}^o, \mathcal{C}_{t+1}^o) &= \pi(\mathcal{S}_t) \text{ (the original run)}, \\
    \mathcal{A}_{t+1}^m = (\mathcal{R}_{t+1}^m, \mathcal{C}_{t+1}^m) &= \pi(\mathcal{M}(\mathcal{S}_t)) \text{ (the masking run)},
\end{align*}
where $\mathcal{*}_{t+1}^o$ and $\mathcal{*}_{t+1}^m$ represent the action generated by the original run and masked run at step $t+1$, respectively.
By comparing the agent's next action generated by the original run and the masking run, we can identify potential attacks:
\textit{
if the action deviates after masking, that indicates the agent's action is related to the user task and the original execution is benign, otherwise, the execution is independent from the user prompt, indicating there are attacker-injected prompts during the execution.
}

% Following this idea, a straightforward approach can be that~\textit{at step $t+1$, when the agent generates outputs $(R_{t+1}, C_{t+1}) = \pi\bigl(T_u, (R_{1:t}, C_{1:t}, E_{1:t})\bigr)$}, we feed the tool execution outputs to the target agent while masking the rest contents, and record if the LLM output deviates from the original one without masking. 

To compare the actions of~\textit{the original run} and~\textit{the masking run}, we can feed $\mathcal{A}_{t+1}^o$ and $\mathcal{A}_{t+1}^m$ into a text embedding model and compute their distance in the embedding space.
If at any step, we observe that the distance is~\textit{higher} than a certain threshold, it means the LLM output depends only on $\mathcal{O}_{1:t}$ and signals an attacker-injected prompt. 




However, although straightforward, this masking function and the comparison method face three~\textit{technical challenges} during our exploration. 
\ding{182}, when presented with only $\mathcal{O}_{1:t}$ in the masking run, the LLM may treat these tool execution outputs as few-shot examples and generate contextually plausible but potentially arbitrary tool calls. Thus, for injected cases, the generated tool calls may deviate from the malicious task $\mathcal{T}_m$ embedded in $\mathcal{O}_{t}'$, leading to false negatives, while for benign cases, these arbitrary calls might coincidentally align with the user task $\mathcal{T}_u$, causing false positives.
% original pass delay 
\ding{183}, we observe that, in some cases, the execution timing of $\mathcal{T}_m$ differs between the original run and the masking run. In the original run, the agent might execute the user task $\mathcal{T}_u$ first before proceeding to $\mathcal{T}_m$. However, in the masking run, since there is no user task to complete, the agent directly executes $\mathcal{T}_m$. As a result, at step $t$, while the masking run has begun executing $\mathcal{T}_m$, the original run may still be processing $\mathcal{T}_u$. This timing mismatch leads to false negatives when comparing tool calls at corresponding steps.
\ding{184}, typically, the action generated by LLM contains both text responses $\mathcal{R}$ and the tool calls $\mathcal{C}$ with specific arguments. In successful attack cases, the masking run and original run may differ in their text responses $\mathcal{R}$ while maintaining identical tool calls $\mathcal{C}$ that execute the malicious task $\mathcal{T}_m$, which again introduces false negatives.
% Furthermore, when the user's task is about summarizing the previous state, masking $T_u$ will not change the output in benign cases that much, as the LLM tends to summarize the input when it does not contain a clear instruction.
% Here, directly comparing the output will also introduce false positives.



\begin{algorithm}[t]
\caption{\method Algorithm at Step $t$}
\label{alg:detection}
\begin{algorithmic}[1]
\REQUIRE Agent $\pi$, user task $\mathcal{T}_u$, masking function $\mathcal{M}$, similarity threshold $\theta$, state $\mathcal{S}_t$, tool call cache $\mathcal{H}_{t}$.
\ENSURE Detection result.

\STATE $\mathcal{O}_1^t \leftarrow \text{concat}(\mathcal{O}_{1:t})$ \COMMENT{Consolidate tool outputs}

\STATE $\mathcal{S}_m \leftarrow \mathcal{M}(\mathcal{S}_t) = (\mathcal{T}_f, \emptyset, \mathcal{O}_1^t)$ \COMMENT{Construct masked state}
\STATE $\mathcal{A}_{t+1}^m \leftarrow \pi(\mathcal{S}_m)$ \COMMENT{Generate masked action}
\STATE $\mathcal{C}_{t+1}^m \leftarrow \text{tools}(\mathcal{A}_{t+1}^m)$ \COMMENT{Extract tool calls}
\STATE $\mathcal{H}_{t+1} \leftarrow \mathcal{H}_{t+1} \cup \mathcal{C}_{t+1}^m$ \COMMENT{Update tool call cache}

\STATE $\mathcal{S}_o \leftarrow (\mathcal{T}_u, \mathcal{A}_{1:t}, \mathcal{O}_{1:t})$ \COMMENT{Original state}
\STATE $\mathcal{A}_{t+1}^o \leftarrow \pi(\mathcal{S}_o)$ \COMMENT{Generate original action}
\STATE $\mathcal{C}_{t+1}^o \leftarrow \text{tools}(\mathcal{A}_{t+1}^o)$ \COMMENT{Extract tool calls}

\FOR{each tool call $c \in \mathcal{C}_{t+1}^o$}
    \FOR{each cached call $h \in \mathcal{H}_{t+1}$}
        \STATE $\text{sim} \leftarrow \frac{\phi(c) \cdot \phi(h)}{|\phi(c)| \cdot |\phi(h)|}$ \COMMENT{Compute similarity}
        \IF{$\text{sim} > \theta$}
            \STATE \textbf{return} True \COMMENT{Attack detected}
        \ENDIF
    \ENDFOR
\ENDFOR
\STATE \textbf{return} False \COMMENT{No attack detected}
\end{algorithmic}

\end{algorithm}




\textbf{Overview of \method.}
We design \method by optimizing the straightforward approach above with novel strategies to address the key technical challenges (demonstrated in~\cref{fig: overall}).
Specifically, to address challenge \ding{182}, we modify the masking function $\mathcal{M}$ in two ways: (1) instead of simply removing $\mathcal{T}_u$, we replace it with a task-neutral prompt $\mathcal{T}_f$ (see \cref{appendix-method-tf}) that typically requires no tool interactions, and (2) we consolidate the sequence of tool outputs $\mathcal{O}_{1:t}$ into a single context $\mathcal{O}_1^t$. 
Formally, our modified masking function is:
$\mathcal{M}(\mathcal{T}_u, \mathcal{A}_{1:t}, \mathcal{O}_{1:t}) = (\mathcal{T}_f, \emptyset, \mathcal{O}_1^t)$. 
To further make the LLM better follow the $\mathcal{T}_f$ in the masking run, we create two few-shot chat samples (see~\cref{appendix-method-fewshot}) that include user, LLM response, and tool execution outputs.
These designs together reduce false negatives by guiding the LLM to focus on detecting embedded instructions rather than mimicking the tool call patterns in $\mathcal{O}_{1:t}$.
For challenge \ding{183}, we implement a tool call cache $\mathcal{H}$ that records the sequence of tool calls from the masking run: $\mathcal{H}_{t+1} = \{\mathcal{C}_1^m, ..., \mathcal{C}_{t+1}^m\}$, where $\mathcal{C}_i^m$ denotes the tool calls generated by the masking run in step $i$. We identify an attack when any tool call in the original run is similar to a cached call: $\exists c \in \mathcal{C}_{t+1}^o, \exists h \in \mathcal{H}: \text{sim} (c, h) > \theta$, where $\theta$ is the similarity threshold. 
This cache-based approach reduces false negatives by capturing all potential attack-related tool calls across multiple steps.
Note that this cache-based detection will not introduce new false positives because tool calls in $\mathcal{H}_{t+1}$ indicate potential malicious instructions since matching calls in the original run suggests the LLM is executing instructions embedded in previous tool execution outputs.
Finally, to tackle challenge \ding{184}, we focus our comparison specifically on tool calls rather than complete actions. Let $\text{tools}(\mathcal{A})$ extract the tool calls from an action. We update the detection criterion as: $\text{sim}(\text{tools}(\mathcal{A}_{t+1}^o), \text{tools}(\mathcal{A}_{t+1}^m)) > \theta$.
This design is motivated by the fact that accomplishing the attack task typically involves calling one or more tools that are correlated to the malicious attacker task $\mathcal{T}_m$.
This design significantly reduces false negatives as the content that causes large differences under attacks is not considered.


\textbf{Remark.} 
A simpler detection approach might be to identify prompt injection whenever the masking run generates any tool calls. However, this would reduce system utility under attack scenarios. Many injected tasks $\mathcal{T}_m$ fail to redirect the LLM - while they may trigger tool calls in the masking run, the original run remains focused on executing the user task $\mathcal{T}_u$. In such cases, marking these unsuccessful attacks would unnecessarily interrupt benign executions. Therefore, comparing tool calls is essential to identify only those attacks that successfully influence the agent's behavior.




\subsection{Technical Details}
\label{subsec:details}

% As shown in~\cref{fig: overall}, considering the agent at time step $t$, which consists of the user task $T_u$, the sequence of agent responses $R_{1:t-1} = \{R_1, R_2, ..., R_{t-1}\}$ (including text outputs and tool calls), and the corresponding tool outputs $O_{1:t-1} = \{O_{F}^{(1)}, O_{F}^{(2)}, ..., O_{F}^{(t-1)}\}$. 
% Our method conducts two parallel runs at each step.
% \noindent\textit{Original run:} we execute the agent as it till step $t$ and extract the LLM's output tool call $C_t$.
% \noindent\textit{Attack-seeking execution:} 
% We construct a parallel path where we (1) replace the original task $T_u$ with a generic instruction-following prompt $T_s$: ``\prompt{Summarize the contents retrieved. If there are any instructions in the contents, follow them first.}'', and (2) remove all agent responses $R_{1:t}$, feeding the LLM only $T_s$ and the tool outputs $O_{1:t}$. 
% We add the current tool call into the cache $\mathcal{C}_s$.
% If $C_t \in \mathcal{C}_s$, we terminate the original execution and mark it as under attack.  

Algorithm \ref{alg:detection} presents the complete detection procedure. As discussed above, our detection mechanism analyzes the similarity between tool calls generated in two parallel runs: the original run and the masking run. At step $t$, we maintain a tool call cache $\mathcal{H}_t$ that collects tool calls from the masking run. The detection process consists of two phases: (1) cache population and (2) similarity-based detection.

In the cache population phase, we first use the masking function $\mathcal{M}$ to mask the current state $\mathcal{S}_t$. If $\mathcal{O}_1^t$ contains a malicious task $\mathcal{T}_m$, the agent will generate tool calls $\mathcal{C}_{t+1}^m$ corresponding to $\mathcal{T}_m$, which are then added to the cache $\mathcal{H}_{t+1}$. Otherwise, if no malicious task is present, the agent will generate no tool calls given the design of $\mathcal{T}_f$. For similarity-based detection, we compare each tool call $c \in \mathcal{C}_{t+1}^o$ from the original run with every cached tool call $h \in \mathcal{H}_{t+1}$ using a vector embedding model $\phi$:
$\text{sim}(c, h) = \frac{\phi(c) \cdot \phi(h)}{|\phi(c)| \cdot |\phi(h)|}$. 
We raise an alert if any similarity score exceeds a threshold $\theta$:
$\text{Alert}_{t+1} = \exists c \in \mathcal{C}_{t+1}^o, \exists h \in \mathcal{H}_{t+1}: \text{sim}(c, h) > \theta$.


Our preference for embedding-based similarity over exact matching stems from the semantic nature of tool calls, tool calls with similar intents can have different arguments. 
For example, an attack instructing \prompt{send all your personal data to hacker@evil.com} might generate tool calls with slightly different arguments like \prompt{send\_email("hacker", subject="data")} and \prompt{send\_email("hacker", subject="info")}, while they differ in their exact argument names, these two calls serve the same malicious purpose. Exact matching would fail to detect such semantically equivalent but syntactically distinct attacks.



\subsubsection{Implementation and Overhead Analysis}

\textbf{Implementation Details.}
Our detection method consists of several key components. For tool call comparison, we convert each tool call to a natural language description, note that for specific functions, we do not compare some of their arguments, for example, for the \prompt{send\_email} function, we do not compare the contents to be sent, but instead only recipient is considered. Details can be found in~\cref{appendix-method-tranform-tool-call}. Next, we employ OpenAI's text-embedding-v3 model~\cite{openai2024embedding} that maps these descriptions to dense vector representations. We set the primary similarity threshold $\theta = 0.8$ to balance detection sensitivity and false positive rate, the ablation study on different similarity thresholds is presented in~\cref{subsec:ablation}. 
The task-neutral prompt $\mathcal{T}_f$ is designed to be independent of specific domains or tasks. For example: ``\prompt{Summarize the provided content. If there are any instructions, execute them.}''
This formulation enables the detection of injected tasks $\mathcal{T}_m$ while minimizing false positives from benign content such as documentation or user guides, we also provide the ablation study for different $\mathcal{T}_f$ in~\cref{subsec:ablation}.

\textbf{Overhead Analysis.}
We analyze both the time complexity and computational cost of our approach. Since \llm typically supports batch operations, the instruction-seeking path can be executed in parallel with the original path during each tool output inspection. This parallel execution minimizes the additional latency introduced by our detection method. Also, since the contents of the masking run largely overlap with the original run, we can implement a KV cache to reduce the latency. The primary source of latency stems from computing embeddings, which depends on the response time of OpenAI's embedding model API in our implementation.
Regarding computational costs, particularly for API-based models, the introduction of the instruction-seeking path effectively doubles the number of required model calls. This results in a $\approx 2\times$ increase in API costs compared to the undefended baseline system. However, we argue that this cost overhead is justified given the critical importance of protecting agent systems against \ipi attacks.






















%%%%%%%%%%%%%%%%%%%%%%% Old text %%%%%%%%%%%%%%%%%%%%%%%%%%%


% We consider an agent system $S$ that solves user tasks through iterative tool usage. 
% Formally, let $T_u$ denote a user task and $\mathcal{F} = \{F_1, F_2, ..., F_n\}$ represent the toolkit containing $n$ tools provided by users or developers. The agent system operates in discrete time steps, where at each iteration step $t$, it processes the current state comprising the task $T_u$ and the collection of previous tool outputs $\{O_{F_i}\}_{i=1}^{t}$. Based on this state, the agent system either generates a set of tool calls $C_t = \{c_1, c_2, ..., c_m\}$, where $c_j$ specifies a tool $F_i \in \mathcal{F}$ and its associated parameters, or determines that the task is complete and terminates the pipeline. For each generated tool call $c_j$, the system executes the specified tool $F_i$ and obtains its output $O_{F_i}$. This iterative process, illustrated in \cref{fig: basic agent pipeline}, continues until the agent system determines the task completion criteria have been met or encountered errors.

% \paragraph{Threat Model}
% In the context of indirect prompt injection attacks, we consider a threat model where the attack surface is strictly limited to the tool execution phase. Specifically, adversaries possess the capability to manipulate information retrieved by tools, such as website content, email content, or file contents. However, the attack surface explicitly excludes modification of user prompts, system messages, direct LLM responses, or the agent system's core functionality.
% The security model assumes that both the user and the agent system operate with benign intentions, meaning they do not actively initiate malicious operations. The adversarial objective in this context is goal misdirection, where attackers attempt to force the agent system to deviate from the user's intended task and turn to executing malicious tasks.

% \paragraph{Defender's Capabilities and Assumptions}
% The defender has complete access to modify and monitor all components of the agent system pipeline: (i) input prompts, including user instructions $T_u$ and system prompts, (ii) agent system responses and intermediate states, and (iii) tool outputs $O_{F_i}$.

% \subsection{\method}

% \begin{figure*}[t!]
% \centering
% \includegraphics[width=\textwidth]{figs/coco.pdf}
% \caption{The pipeline of proposed \method detection method. Green and red boxes indicate benign and potentially compromised states respectively. The method compares tool calls between the original execution path (top) and an instruction-seeking path (bottom) to detect indirect prompt injection attacks. The red arrow in tool execution indicates potential malicious instruction injection.}
% \label{fig: overall}
% \end{figure*}

% Our method introduces a novel defense mechanism against indirect prompt injection attacks through parallel execution path analysis. The fundamental insight stems from the adversarial nature of embedded malicious instructions in tool outputs: when present, these instructions attempt to redirect the agent system's behavior from the user's intended task $T_u$ toward alternative, potentially harmful actions. We detect such misdirection by analyzing behavioral divergence between parallel execution scenarios, as illustrated in \cref{fig: overall}.

% \subsubsection{Detection Framework}
% Consider the agent system state at time step $t$, which consists of the user task $T_u$, the sequence of agent responses $R_{1:t} = \{R_1, R_2, ..., R_t\}$ (including both text outputs and tool calls), and the corresponding tool outputs $O_{1:t} = \{O_{F_1}, O_{F_2}, ..., O_{F_t}\}$. Our method constructs two parallel execution paths:
% \begin{enumerate}
%     \item \textit{Original Execution Path:} We maintain the agent system's original behavior, where it processes inputs based on the complete execution history, including the user's original instruction $T_u$, previous agent responses $R_{1:t}$, and tool outputs $O_{1:t}$. Let $C_t^o$ denote the set of tool calls generated in this path at step $t$.
%     \item \textit{Instruction-Seeking Path:} We construct a parallel path where we (1) replace the original task $T_u$ with a generic instruction-following prompt $T_s$: ``\prompt{Summarize the contents retrieved. If there are any instructions in the contents, follow them first.}'', and (2) remove all agent responses $R_{1:t}$, preserving only $T_s$ and the tool outputs $O_{1:t}$. Let $C_t^s$ denote the set of tool calls generated in this path.
% \end{enumerate}

% The theoretical foundation for our detection approach lies in the behavioral divergence between these paths. For benign tool outputs containing no explicit instructions, the two paths should exhibit distinctly different behaviors: the original path generates tool calls $C_t^o$ aligned with the user's intent, while the instruction-seeking path, finding no explicit instructions to follow, should primarily perform summarization without generating significant tool calls ($C_t^s \approx \emptyset$). However, when malicious instructions are embedded in the tool outputs, both paths tend to generate similar tool calls ($C_t^o \approx C_t^s$) as they both respond to and execute the embedded instructions.


% \subsubsection{Detection Algorithm}
% Our detection mechanism operates by analyzing the similarity between tool calls generated in the parallel execution paths. At each time step $t$, we maintain a suspicious tool calls bank $B_t$ that collects tool calls from the instruction-seeking path. The detection process consists of two phases: (1) suspicious tool call collection and (2) similarity-based detection.

% For the suspicious tool call collection phase, we first combine all tool outputs $O_{1:t}$ into a single context $O_{combined}$. This combined context, along with the instruction-seeking prompt $T_s$, is then processed by the agent system to generate a set of tool calls $C_t^s$, which are added to the suspicious tool calls bank $B_t$.
% In the similarity-based detection phase, for each tool call $c \in C_t^o$ from the original execution path, we compute its semantic similarity with every suspicious tool call $b \in B_t$ using a dense vector embedding model:
% \begin{equation}
% sim(c, b) = \frac{Embed(c) \cdot Embed(b)}{|Embed(c)| \cdot |Embed(b)|}, \forall b \in B_t
% \end{equation}
% An alert is raised if any similarity score exceeds a predefined threshold $\theta$:
% \begin{equation}
% \text{Alert}_t =  \exists c \in C_t^o, \exists b \in B_t: \text{sim}(c, b) > \theta
% \end{equation}

% We use embedding similarity here primarily because directly comparing tool calls by exact match will produce more false negatives, as for some tools, the args can be slightly different but take the same effects. We discussed it in detail in~\cref{sec4.2-ablation}.

% Algorithm \ref{alg:detection} presents the complete detection procedure.
% \begin{algorithm}[t]
% \caption{Pseudo code for \method}
% \label{alg:detection}
% \begin{algorithmic}[1]
% \STATE Initialize suspicious tool calls bank $B \gets \emptyset$
% \STATE Combine all tool outputs $O_{combined} \gets \mbox{Combine}(O)$
% \STATE Set instruction-seeking prompt $T_s$ 
% \STATE $C_s \gets \mbox{ExtractToolCalls}(\mbox{Agent}(Ts, O{combined}))$
% \STATE Add to suspicious tool calling bank $B \gets B \cup C_s$
% \STATE $C_o \gets \mbox{ExtractToolCalls}(\mbox{Agent}(T_u, O))$
% \FOR{each tool call $c \in C_o$}
%     \FOR{each suspicious call $b \in B$}
%         \IF{$\mbox{CosineSim}(\mbox{Emb}(c), \mbox{Emb}(b)) > \theta$}
%             \STATE \textbf{return} True \COMMENT{Potential misdirection detected}
%         \ENDIF
%     \ENDFOR
% \ENDFOR
% \STATE \textbf{return} False
% \end{algorithmic}
% \end{algorithm}



% \subsubsection{Implementation Details}

% The practical implementation of our detection algorithm relies on several key components. For tool calling comparison, we first transform it into natural language descriptions, and then employ OpenAI's text-embedding-v3 model~\cite{openai2024embedding}, which maps the natural language descriptions to dense vector representations in $\mathbb{R}^d$. The similarity threshold $\theta$ is empirically determined and set to $0.9$ based on a small validation experiments on our test set, balancing between detection sensitivity and false positive rate.
% The instruction-seeking prompt $T_s$ is carefully designed to be domain-agnostic while maintaining sufficient specificity to trigger embedded instructions. This design choice ensures robust detection across various types of indirect prompt injection attacks while minimizing false positives from benign tool outputs that contain instruction-like content (e.g., technical documentation or user guides). In order to make the agent system follow the summarization instruction strictly, we also provide an example to it.  Details can be found in~\cref{appendix-method}.


% \subsection{System Overhead and Costs}
% We analyze both the time complexity and computational cost of our approach. Since \llm typically support batch operations, the instruction-seeking path can be executed in parallel with the original path during each tool output inspection. This parallel execution minimizes the additional latency introduced by our detection method. The primary source of latency stems from computing embeddings, which depends on the response time of OpenAI's embedding model API in our implementation.
% Regarding computational costs, particularly for API-based models, the introduction of the instruction-seeking path effectively doubles the number of required model calls. This results in a $\approx 2\times$ increase in API costs compared to the undefended baseline system. However, we argue that this cost overhead is justified given the critical importance of protecting agent systems against indirect prompt injection attacks.


