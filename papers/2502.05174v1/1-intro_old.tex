\section{Introduction}
\label{sec:intro}

Recent advances in large language models (LLMs)~\cite{achiam2023gpt, anthropic_tool_use, team2023gemini, llama_tool_use} have marked a paradigm shift in artificial intelligence: from developing models with sophisticated reasoning capabilities to enabling effective tool utilization~\cite{yao2022webshop, anthropic_tool_use, openai_tool_use, llama_tool_use}. 
Drawing parallels with human evolution, where tool use catalyzed cognitive advancement, the integration of tool-use capabilities has empowered AI systems to address complex real-world tasks through specialized external resources. 
This advancement has given rise to increasingly capable AI agents that can interact with external tools, databases, and APIs to accomplish sophisticated tasks that would be impossible through language processing alone.


\begin{figure}[t!]
    \centering
    \includegraphics[width=0.5\textwidth]{figs/average_scatter.pdf}
    \vspace{-.2in}
    \caption{Comparison of averaged Utility under Attack (UA, higher is better) performance and Attack Success Rate (ASR, lower is better) on \gpt, \othree, and \llama across different defense methods. %The x-axis show the UA percentage, indicating how well each defense method maintains model functionality under attacks. The y-axis show the ASR percentage. 
    Our proposed methods (\method and \method-Aug) achieve superior performance with extremely low ASR while maintaining high UA, outperforming all the baseline defense methods. Detailed comparison among these defenses are in \cref{subsec:sec4-results}.}
    \vspace{-.2in}
    \label{fig: defense_performance}
\end{figure}


However, this enhanced functionality introduces critical security vulnerabilities~\cite{naihin2023testing, ruan2024identifying, yuan2024rjudge, liu2024formalizing, zhan24injecagent, debenedetti2024agentdojo, zhang2024agent}, particularly indirect prompt injection~(\ipi) attacks, which exploit the agent's interaction with external resources by embedding malicious instructions in tool-retrieved information such as database~\cite{zhong-etal-2023-poisoning, zou2024poisonedrag, chen2024agentpoison, lee2024prompt} and websites~\cite{liao2024eia, xu2024advweb, wu2024agentattack}, potentially leading to unauthorized actions and data breaches. 


Defending against \ipi attacks presents a significant challenge, requiring a delicate balance between security and maintaining the agent's ability to process legitimate tool outputs effectively. Current defense approaches primarily fall into three categories: prompt augmentation, pre-response detection, and post-response detection methods. Prompt augmentation techniques~\cite{hines2024defending, learning_prompt_sandwich_url,} attempt to reinforce the agent's adherence to original instructions through various prompting strategies, but often fail to prevent sophisticated attacks while introducing substantial overhead. Pre-response detection methods~\cite{deberta-v3-base-prompt-injection-v2, inan2023llama} aim to identify malicious content before it reaches the agent but demonstrate limited detection efficacy (see \cref{sec4-results}). Post-response detection methods monitor agent behavior after execution to identify potential attacks, but existing approaches either can only detect attacks after they have succeeded in compromising the system~\cite{liu2024formalizing} or require white-box access~\cite{hung2024attention} to the \llm.


In this paper, we proposed \textit{C}ontrasive \textit{O}output \textit{CO}mparison (\method), a novel post-response detection method that significantly secure AI agent systems against \ipi attacks while maintaining their basic utilities. \method operates by analyzing behavioral divergence between two parallel execution paths: one following the original user task and another specifically seeking embedded instructions. This approach leverages the key insight that malicious instructions cause behavioral convergence between these typically divergent paths. Moreover, we show that \method can be integrated with prompt augmentation methods to create \method-Aug, further enhancing both security and utility. Through extensive experimentation on the AgentDojo benchmarks, we demonstrate that \method and \method-Aug achieves state-of-the-art performance with $64.11$\% and $70.79$\% utility while maintaining an extremely low attack success rate of $0.76$\% and $0.08$\% respectively for \gpt. These results substantially outperform existing defense methods, establishing a new standard for securing AI agent systems without compromising their core capabilities. Our results reveal several key findings:
\begin{itemize}
    \item Existing defense approaches face fundamental limitations: prompt augmentation methods (delimiting and repeat prompts) maintain high utility but fail to prevent sophisticated attacks; tool filter achieves low ASR at the cost of severely degrading utility; and pre-response detection methods suffer from high false negatives while still failing to maintain utility under attacks.
    \item \method, a post-response detection approach. overcomes these limitations by analyzing behavioral divergence, achieving both superior attack prevention and high utility. This demonstrates that monitoring agent behavior after execution is more effective than attempting to identify attacks beforehand or restricting tool usage.
    \item Integration with prompt augmentation (\method-Aug) creates synergistic effects, further reducing ASR to 0.08\% while improving utility to 70.79\%. 
    \item Our approach effectively handles edge cases in which users explicitly request execution of external instructions, addressing a critical security vulnerability that other methods overlook.
\end{itemize}


To summarize, our key contributions are:
\begin{itemize}
    \item A novel post-response detection framework that fundamentally reimagines \ipi defense through behavioral divergence analysis. Unlike existing approaches that attempt to prevent attacks through pre-response filtering or prompt modification, our method identifies attacks by analyzing the convergence of parallel execution paths, achieving both high task completion utility and near-zero ASR.

    % \item A theoretical foundation for detecting \ipi attacks that introduces the concept of behavioral divergence in parallel execution paths. This provides a principled approach to distinguishing between legitimate tool outputs and malicious instructions, establishing a new paradigm for securing AI agent systems.

    \item A hybrid defense strategy that demonstrates how post-response detection can be effectively combined with prompt augmentation techniques. This integration (\method-Aug) achieves synergistic improvements in both security and utility, particularly against sophisticated attacks.

    \item Comprehensive empirical evaluation across multiple benchmarks and attack types, demonstrating that our approach significantly outperforms existing methods in AI agent system deployment. Our analysis provides insights into the trade-offs between different defense strategies and their implications for practical applications.
\end{itemize}