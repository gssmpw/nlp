@misc{alex2023ultimate,
  title = {{Ultimate ChatGPT prompt engineering guide for general users and developers}},
  howpublished = "\url{https://www.imaginarycloud.com/blog/chatgpt-prompt-engineering}",
  author = {Alexandra Mendes},
  year={2023}
}

@article{chen2024aligning,
  title={Aligning llms to be robust against prompt injection},
  author={Chen, Sizhe and Zharmagambetov, Arman and Mahloujifar, Saeed and Chaudhuri, Kamalika and Guo, Chuan},
  journal={arXiv preprint arXiv:2410.05451},
  year={2024}
}

@article{chen2024struq,
  title={StruQ: Defending against prompt injection with structured queries},
  author={Chen, Sizhe and Piet, Julien and Sitawarin, Chawin and Wagner, David},
  journal={arXiv preprint arXiv:2402.06363},
  year={2024}
}

@inproceedings{debenedetti2024agentdojo,
  title={AgentDojo: A Dynamic Environment to Evaluate Prompt Injection Attacks and Defenses for LLM Agents},
  author={Debenedetti, Edoardo and Zhang, Jie and Balunovic, Mislav and Beurer-Kellner, Luca and Fischer, Marc and Tram{\`e}r, Florian},
  booktitle={The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track},
  year={2024}
}

@misc{deberta-v3-base-prompt-injection-v2,
  author = {ProtectAI},
  title = {Fine-Tuned DeBERTa-v3-base for Prompt Injection Detection},
  year = {2024},
  publisher = {HuggingFace},
  url = {https://huggingface.co/ProtectAI/deberta-v3-base-prompt-injection-v2},
}

@misc{delimiters_url,
  title = {{Delimiters won’t save you from prompt injection}},
  howpublished = "\url{https://simonwillison.net/2023/May/11/delimiters-wont-save-you}",
  author = {Simon Willison},
  year={2023}
}

@article{fu2023misusing,
  title={Misusing tools in large language models with visual adversarial examples},
  author={Fu, Xiaohan and Wang, Zihan and Li, Shuheng and Gupta, Rajesh K and Mireshghallah, Niloofar and Berg-Kirkpatrick, Taylor and Fernandes, Earlence},
  journal={arXiv preprint arXiv:2310.03185},
  year={2023}
}

@article{hines2024defending,
  title={Defending Against Indirect Prompt Injection Attacks With Spotlighting},
  author={Hines, Keegan and Lopez, Gary and Hall, Matthew and Zarfati, Federico and Zunger, Yonatan and Kiciman, Emre},
  journal={arXiv preprint arXiv:2403.14720},
  year={2024}
}

@article{hung2024attention,
  title={Attention Tracker: Detecting Prompt Injection Attacks in LLMs},
  author={Hung, Kuo-Han and Ko, Ching-Yun and Rawat, Ambrish and Chung, I and Hsu, Winston H and Chen, Pin-Yu and others},
  journal={arXiv preprint arXiv:2411.00348},
  year={2024}
}

@inproceedings{ignore_previous_prompt,
    author = {Perez, Fábio and Ribeiro, Ian},
    title = {Ignore Previous Prompt: Attack Techniques For Language Models},
    booktitle = {NeurIPS ML Safety Workshop},
    year = {2022}
}

@article{inan2023llama,
  title={Llama guard: Llm-based input-output safeguard for human-ai conversations},
  author={Inan, Hakan and Upasani, Kartikeya and Chi, Jianfeng and Rungta, Rashi and Iyer, Krithika and Mao, Yuning and Tontchev, Michael and Hu, Qing and Fuller, Brian and Testuggine, Davide and others},
  journal={arXiv preprint arXiv:2312.06674},
  year={2023}
}

@article{kumar2024manipulating,
  title={Manipulating large language models to increase product visibility},
  author={Kumar, Aounon and Lakkaraju, Himabindu},
  journal={arXiv preprint arXiv:2404.07981},
  year={2024}
}

@misc{learning_prompt_sandwich_url,
  title = {{Sandwitch defense}},
  howpublished = "\url{https://learnprompting.org/docs/prompt_hacking/defensive_measures/sandwich_defense}",
  year={2023}
}

@article{lee2024prompt,
  title={Prompt Infection: LLM-to-LLM Prompt Injection within Multi-Agent Systems},
  author={Lee, Donghyun and Tiwari, Mo},
  journal={arXiv preprint arXiv:2410.07283},
  year={2024}
}

@article{liao2024eia,
  title={Eia: Environmental injection attack on generalist web agents for privacy leakage},
  author={Liao, Zeyi and Mo, Lingbo and Xu, Chejian and Kang, Mintong and Zhang, Jiawei and Xiao, Chaowei and Tian, Yuan and Li, Bo and Sun, Huan},
  journal={arXiv preprint arXiv:2409.11295},
  year={2024}
}

@inproceedings{liu2024formalizing,
  title={Formalizing and benchmarking prompt injection attacks and defenses},
  author={Liu, Yupei and Jia, Yuqi and Geng, Runpeng and Jia, Jinyuan and Gong, Neil Zhenqiang},
  booktitle={33rd USENIX Security Symposium (USENIX Security 24)},
  pages={1831--1847},
  year={2024}
}

@article{nestaas2024adversarial,
  title={Adversarial search engine optimization for large language models},
  author={Nestaas, Fredrik and Debenedetti, Edoardo and Tram{\`e}r, Florian},
  journal={arXiv preprint arXiv:2406.18382},
  year={2024}
}

@article{patil2024goex,
  title={GoEX: Perspectives and Designs Towards a Runtime for Autonomous LLM Applications},
  author={Patil, Shishir G and Zhang, Tianjun and Fang, Vivian and Huang, Roy and Hao, Aaron and Casado, Martin and Gonzalez, Joseph E and Popa, Raluca Ada and Stoica, Ion and others},
  journal={arXiv preprint arXiv:2404.06921},
  year={2024}
}

@misc{pi_against_gpt3,
  title = {{Prompt injection attacks against GPT-3}},
  howpublished = "\url{https://simonwillison.net/2022/Sep/12/prompt-injection/}",
  author = {Simon Willison},
  year={2022}
}

@inproceedings{schulhoff2023ignore,
    title = "Ignore This Title and {H}ack{AP}rompt: Exposing Systemic Vulnerabilities of {LLM}s Through a Global Prompt Hacking Competition",
    author = "Schulhoff, Sander  and
      Pinto, Jeremy  and
      Khan, Anaum  and
      Bouchard, Louis-Fran{\c{c}}ois  and
      Si, Chenglei  and
      Anati, Svetlina  and
      Tagliabue, Valen  and
      Kost, Anson  and
      Carnahan, Christopher  and
      Boyd-Graber, Jordan",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.emnlp-main.302/",
    doi = "10.18653/v1/2023.emnlp-main.302",
    pages = "4945--4977",
}

@article{wallace2024instruction,
  title={The instruction hierarchy: Training llms to prioritize privileged instructions},
  author={Wallace, Eric and Xiao, Kai and Leike, Reimar and Weng, Lilian and Heidecke, Johannes and Beutel, Alex},
  journal={arXiv preprint arXiv:2404.13208},
  year={2024}
}

@article{wu2024agentattack,
  title={Adversarial Attacks on Multimodal Agents},
  author={Wu, Chen Henry and Koh, Jing Yu and Salakhutdinov, Ruslan and Fried, Daniel and Raghunathan, Aditi},
  journal={arXiv preprint arXiv:2406.12814},
  year={2024}
}

@article{wu2024new,
  title={A new era in llm security: Exploring security concerns in real-world llm-based systems},
  author={Wu, Fangzhou and Zhang, Ning and Jha, Somesh and McDaniel, Patrick and Xiao, Chaowei},
  journal={arXiv preprint arXiv:2402.18649},
  year={2024}
}

@article{wu2024system,
  title={System-Level Defense against Indirect Prompt Injection Attacks: An Information Flow Control Perspective},
  author={Wu, Fangzhou and Cecchetti, Ethan and Xiao, Chaowei},
  journal={arXiv preprint arXiv:2409.19091},
  year={2024}
}

@inproceedings{wu2025isolategpt,
  title={{IsolateGPT: An Execution Isolation Architecture for LLM-Based Systems}}, 
  author={Wu, Yuhao and Roesner, Franziska and Kohno, Tadayoshi and Zhang, Ning and Iqbal, Umar},
  booktitle={Network and Distributed System Security Symposium (NDSS)},
  year={2025},
}

@article{xu2024advweb,
  title={Advweb: Controllable black-box attacks on vlm-powered web agents},
  author={Xu, Chejian and Kang, Mintong and Zhang, Jiawei and Liao, Zeyi and Mo, Lingbo and Yuan, Mengqi and Sun, Huan and Li, Bo},
  journal={arXiv preprint arXiv:2410.17401},
  year={2024}
}

@article{yu2023assessing,
  title={Assessing prompt injection risks in 200+ custom gpts},
  author={Yu, Jiahao and Wu, Yuhang and Shu, Dong and Jin, Mingyu and Xing, Xinyu},
  journal={arXiv preprint arXiv:2311.11538},
  year={2023}
}

@misc{zhang2024attacking,
    title={Attacking Vision-Language Computer Agents via Pop-ups},
    author={Yanzhe Zhang and Tao Yu and Diyi Yang},
    year={2024},
    eprint={2411.02391},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

@inproceedings{zhong-etal-2023-poisoning,
    title = "Poisoning Retrieval Corpora by Injecting Adversarial Passages",
    author = "Zhong, Zexuan  and
      Huang, Ziqing  and
      Wettig, Alexander  and
      Chen, Danqi",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.emnlp-main.849/",
    doi = "10.18653/v1/2023.emnlp-main.849",
    pages = "13764--13775",
}

@article{zou2023universal,
  title={Universal and transferable adversarial attacks on aligned language models},
  author={Zou, Andy and Wang, Zifan and Carlini, Nicholas and Nasr, Milad and Kolter, J Zico and Fredrikson, Matt},
  journal={arXiv preprint arXiv:2307.15043},
  year={2023}
}

@article{zou2024poisonedrag,
  title={Poisonedrag: Knowledge poisoning attacks to retrieval-augmented generation of large language models},
  author={Zou, Wei and Geng, Runpeng and Wang, Binghui and Jia, Jinyuan},
  journal={arXiv preprint arXiv:2402.07867},
  year={2024}
}

