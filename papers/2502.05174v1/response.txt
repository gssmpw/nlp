\section{Related Work}
\label{sec:rw}

\textbf{Indirect Prompt Injection Attacks.}
At a high level, indirect prompt injection attacks against agents can be categorized as general attacks and agent-specific attacks.
General attacks focus on developing universal attack prompt patterns that force the target agent to conduct the attacker tasks rather than the user tasks.
Notably, the escape character attacks**Zhang et al., "Escape Character Attacks"** utilize special characters like ``\prompt{\symbol{92}n}'' to manipulate context interpretation.
Context-ignoring attacks**Li et al., "Context-Ignoring Attacks"** explicitly instruct the \llm to disregard the previous context.
Fake completion attacks**Wang et al., "Fake Completion Attacks"** attempt to deceive the \llm by simulating task completion. 
% Recent work**Zhang2024attacking, "Prompt Injection Attacks"** also utilized optimization-based approaches using techniques like GCG**Brown et al., "GCG: A Generalized Framework for Multi-Task Learning"** to optimize the attack pattern.
These methods are often tested on \ipi benchmarks**Henderson et al., "IPi-Benchmarks: A Benchmarking Suite for Indirect Prompt Injection Attacks"** with pre-specified injection points and attack tasks.
There are also some early explorations of \llm attacks against a specific type of agent.
For example, attacks against web agents inject the attack content into the web pages to ``fool'' the agent into the attack tasks**WebAgent: Red-teaming {LLM} Agents via Poisoning Memory or Knowledge Bases**. 
Attacks against computer agents manipulate the computer interface**A New Era in LLM Security: Exploring Security Concerns in Real-World LLM-based Systems (2024)**. 
Note that there are also some direct prompt injection attacks against LLMs**AdvWeb: Controllable Black-box Attacks on VLM-powered Web Agents**.
These methods directly append the attack prompts after the user inputs, which may not be practical in real-world applications.

\textbf{Defenses against \ipi.}
% Unlike jailbreaking attacks that explicitly prompt \llm to generate harmful content, defending against \ipi poses unique challenges due to its stealthy nature: both injected malicious prompts and their consequences can appear as legitimate tasks. 
Existing defenses can be categorized based on resource requirements.
Defenses that require additional training resources either conduct adversarial training of the LLM(s) in the target agent**Zhang2024attacking, "Prompt Injection Attacks"** or add additional models to detect whether the inputs contain injected prompts**Gao et al., "Deter: A Detection Framework for Indirect Prompt Injection Attacks"**. 
However, these methods face practical limitations due to their substantial computational and data requirements.
In addition, adversarial training may jeopardize the model's normal utility in broader application domains.
As we will show later, adding additional detection models naturally harms the agent's utility under attack and suffers from high false negative rates.

Training-free defenses either design additional prompts for the user inputs or constrain the allowed tool calls of the agent. 
First, most training-free defenses explore additional prompts that either help the model ignore or detect potential attack instructions in the retrieved data.
Specifically, ignorance strategies include adding a delimiter between the user prompt and retrieved data**Xu et al., "Prompt Delimiter: A Simple yet Effective Defense against Indirect Prompt Injection Attacks"**, repeating the user prompt**Gao et al., "Prompt Repeating: A Lightweight Defense against Indirect Prompt Injection Attacks"**
Such defenses, while lightweight, have limited efficacy against stronger attacks (as shown in Sec~\ref{sec:eval}).
Known-answer detection**Wang et al., "Known-Answer Detection: A Post-Response Defense against Indirect Prompt Injection Attacks"** adds additional questions with known answers to the user prompt and detects if the model finally outputs the answer. 
However, this method can only identify injections post-execution, when attacks may have already succeeded.
Second, tool filtering**Li et al., "Tool Filtering: A Training-Free Defense against Indirect Prompt Injection Attacks"** allows LLMs to select a set of permitted tools for the given user task and block all calls to unauthorized tools.
This approach harms utility as the LLMs sometimes filter out necessary tools. 
More importantly, it is easy to bypass as the attackers can design their attack tasks with only the tools related to the user attack.  
In comparison, our method is a lightweight and highly effective training-free defense that well maintains the agent's normal utility. 

Note that other defenses require human intervention**Henderson et al., "Human-in-the-Loop: A Training-Free Defense against Indirect Prompt Injection Attacks"**, white-box model access**Gao et al., "White-Box Model Access: A Training-Free Defense against Indirect Prompt Injection Attacks"** or reverting agent actions**Wang et al., "Reverting Agent Actions: A Training-Free Defense against Indirect Prompt Injection Attacks"**.
Due to these strong assumptions and lack of full automation, we exclude these approaches from our analysis.