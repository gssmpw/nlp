\begin{abstract}
% Recent research has demonstrated that LLM agents are vulnerable to indirect prompt injection (\ipi) attacks, where malicious tasks embedded in tool-retrieved information can redirect the agent from executing the user-specified task to performing unauthorized operations. 
Recent research has explored that LLM agents are vulnerable to indirect prompt injection (\ipi) attacks, where malicious tasks embedded in tool-retrieved information can redirect the agent to take unauthorized actions. 
Existing defenses against \ipi have significant limitations: either require essential model training resources, lack effectiveness against sophisticated attacks, or harm the normal utilities. 
% We present \method (\textit{C}ontrastive \textit{O}utput \textit{CO}mparison), 
We present \method (\textit{M}asked re-\textit{E}xecution and Too\textit{L} comparis\textit{ON}), a novel \ipi defense.
Our approach builds on the observation that under a successful attack, the agent's next action becomes less dependent on user tasks and more on malicious tasks.
Following this, we design \method to detect attacks by re-executing the agent's trajectory with a masked user prompt modified through a masking function.
We identify an attack if the actions generated in the original and masked executions are similar.
We also include three key designs to reduce the potential false positives and false negatives.
Extensive evaluation on the \ipi benchmark \dojo demonstrates that \method outperforms SOTA defenses in both attack prevention and utility preservation.
Moreover, we show that combining \method with a SOTA prompt augmentation defense (denoted as \method-Aug) further improves its performance.
We also conduct a detailed ablation study to validate our key designs. % and show \method is robust against changes in key hyper-parameters. 
\end{abstract}
