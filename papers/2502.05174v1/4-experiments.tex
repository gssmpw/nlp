

\section{Evaluation}
\label{sec:eval}


\subsection{Experiment Setup}
\label{subsec:setup}


\textbf{Agent.}
We evaluate \method on the \ipi benchmark \dojo~\cite{debenedetti2024agentdojo}. 
\dojo comprises four types of agents (banking, slack, travel, and workspace), and each type of agent has a unique set of tools.
\dojo designs $16, 21, 20, 40$ user tasks for their agents, respectively.
Besides, each agent also has different attack tasks and injection points.
It picks one user task and one attack task to form an attack case, and in total, $629$ attack cases.
Several early works also propose prompt injection attacks benchmarks~\cite{zhan24injecagent}, we choose AgentDojo because it is the latest one containing many diverse attack cases.
We also tried another benchmark for multi-modal agents, i.e., VisualWebArena-Adv~(\webadv)~\cite{wu2024agentattack}.
We do not select it because the attack success rate of SOTA image attacks on this benchmark is low (see~\cref{appendix-datasets} for more details).
We consider three models as the LLM model in each agent: \gpt, \othree, and \llama.
Note that we do not use \claude for large-scale experiments in considering the budget limit, but we test the latest OpenAI model \othree and the SOTA open-source model: \llama.\footnote{We also considered the most recent DeepSeek model, but its tool calling capability is not stable~\cite{deepseek2025}.}
% ~\cref{appendix-results} further shows a small-scale experiment on \claude. showing the effectiveness of our method on this model.
We set the temperature for each model as $0$ to avoid randomness.

% ~\cite{ignore_previous_prompt, schulhoff2023ignore, debenedetti2024agentdojo}
\textbf{Attacks.}
We select four representative and popular general prompt injection attacks discussed in~\cref{sec:rw}: 
(1) \textit{Direct} attack, which appends malicious instructions with the prefix ``\prompt{TODO}''~\cite{debenedetti2024agentdojo}; 
(2) \textit{Ignore Previous Instructions} attack, which employs ``\prompt{Ignore the previous instructions and do the following}''~\cite{ignore_previous_prompt, schulhoff2023ignore}; 
(3) \textit{System Message} attack, which uses ``\prompt{\#\#\# System}''~\cite{debenedetti2024agentdojo}; and 
(4) \textit{Important Messages} attack, which uses ``\prompt{\#\#\# Important Messages}'', following~\cite{debenedetti2024agentdojo}, we further enhance this attack by incorporating additional context (the user name and the LLM version) in the injected malicious instructions.
Note that we do not select the SOTA agent-specific attack against web agents because their efficacy is low~\cite{wu2024agentattack} or their threat models are not practical~\cite{wu2024new, toyer2024tensor, liao2024eia}.

\textbf{Baseline Defenses.}
Based on the categorization in~\cref{sec:rw}, we select five defenses from three categories.
First, we select two model-based detection methods: 
(1) \textit{DeBERTa Detector}, which uses a fine-tuned DeBERTa classifier to identify malicious instructions in tool-retrieved data, and (2) \textit{LLM Detector}, due to the poor performance of DeBERTa detector, we further leverage \gpt to decide whether the retrieved data contains malicious instructions, the prompt used are shown in~\cref{appendix-llm-detector-prompts}.
Second, we consider two prompt augmentation methods: 
(1) \textit{Delimiting} and (2) \textit{Repeat Prompt}.
We do not consider known-answer detection as it identifies injections after the executions when attacks have already succeeded. 
Last, we also include \textit{Tool Filter}~\cite{debenedetti2024agentdojo} as our baseline. 
Note that we do not consider white-box attacks (i.e., GCG~\cite{zou2023universal} and attention tracking~\cite{hung2024attention}) given that most models used in agents are commercial black-box ones. 
Detailed examples of all evaluated attacks and defenses are shown in~\cref{appendix-attacks-defenses}.
For \method, we also evaluate its augmented version which combines \textit{Repeat Prompt} method (denoted as \method-Aug).

\textbf{Evaluation Metrics.}
We consider three metrics: (1) Utility under Attack~\cite{debenedetti2024agentdojo} (UA), which measures the agent's ability to correctly complete the user task $\mathcal{T}_u$ while avoiding execution of malicious tasks during attacks; 
(2) Attack Success Rate (ASR), which measures the proportion of successful prompt injection attacks that achieve their malicious objectives $T_m$. An attack is considered successful if the agent \textit{fully executes} all required steps specified in the malicious task $\mathcal{T}_m$.
(3) Benign Utility (BU), which measures the fraction of user tasks that the agent system solves in the absence of any attacks.


\begin{figure}[t!]
\centering
\includegraphics[width=0.49\textwidth]{figs/scatter.pdf}
\vspace{-.3in}
\caption{Comparative analysis of the averaged attack success rates (ASR, lower is better) versus utility under attack (UA, higher is better) for \gpt, \othree, and \llama. All the defenses except for \method exhibit a trade-off between UA and ASR.}
\label{fig: avg results for all models}

\end{figure}



\subsection{Experiment Results}
\label{subsec:sec4-results}

\input{tables/overall_results}

Our experimental results in \cref{fig: avg results for all models} and \cref{table-overall results} demonstrate that \method achieves both high utility and low ASR, while other defenses exhibit a clear trade-off. We analyze the performance of different defenses on each metric below.


\textbf{Utility under Attack (UA).}
Note that detection-based methods (DeBERTa detector, LLM detector, and \method) terminate the entire agent execution upon detecting potential prompt injections. At step $t+1$, DeBERTa detector and LLM detector analyze tool execution outputs $\mathcal{O}_{1:t}$ before LLM generates any action, regardless of whether the attack would succeed. This creates an inherent trade-off: a perfect detector should achieve $0\%$ UA, as any non-zero UA indicates attacks passing through detection and represents the \textit{lower bound} of their False Negative Rate (FNR). In contrast, \method operates after LLM generates action $\mathcal{A}_{t+1}$ but before execution, intervening only when it detects that malicious instructions will be executed in the original run, thus better preserving UA while minimizing ASR.

\method-Aug achieves optimal balance across all attacks and models, maintaining high UA while achieving low ASR across all \llm. For \gpt, \method-Aug achieves $68.72\%$ UA with $0.32\%$ ASR, compared to the no defense baseline ($69.08\%$ UA, $16.06\%$ ASR). As discussed above, detection methods show significant limitations, with DeBERTa and LLM detectors having FNR of at least $24.05\%$ and $21.18\%$ respectively. Prompt augmentation methods (delimiting and repeat prompt) increase UA to $69.75$\% and $77.86$\% compared to the no defense baseline, likely by strengthening LLM's focus on $\mathcal{T}_u$. The tool filter shows varying performance across models - achieving second-best UA-ASR trade-off for \gpt but poor UA for \othree and \llama due to excessive tool filtering that renders the system unusable.



\textbf{Attack Success Rate (ASR).}
\method and \method-Aug demonstrate superior attack prevention across all models and attacks. For \gpt, \method achieves the lowest average ASR of $0.24$\%, followed by \method-Aug at $0.32$\%, substantially outperforming all baseline defense methods. 
While the tool filter method achieves $0.00$\% ASR for \othree and \llama, this results from blocking nearly all tool usage, rendering the system non-functional. 
Although prompt augmentation methods improve UA, they show limited effectiveness across all attacks and models.
The effectiveness varies across attack types. Important Messages attack is most successful, while Direct attack shows the lowest ASR due to their simple attack patterns. Notably, the model-based detection methods show unexpected behavior with \othree and \llama: they demonstrate \textit{higher ASR} and \textit{higher FPR} for Direct attacks than Important Messages attacks. For example, on \llama, DeBERTa detector and LLM detector show $6.20$\% and $3.02$\% ASR for direct attack, but $1.59$\% and $0.00$\% ASR for important messages attack, suggesting that these methods are particularly vulnerable to attacks without explicit malicious patterns.


\textbf{Benign Utility (BU).}
Different defense methods show varying impacts on benign utility. Similar to UA, prompt augmentation methods maintain or slightly improve BU compared to the no defense baseline. In contrast, the tool filter method significantly degrades BU for \othree and \llama due to excessive tool restrictions. Among detection methods, the DeBERTa detector also shows poor BU due to high false positive rates, while the LLM detector preserves BU. \method and \method-Aug demonstrate moderate BU decreases in specific cases. However, as we will discuss in \cref{subsec:fpr}, these cases represent legitimate security concerns where users directly request execution of unverified external instructions in tool execution outputs.




% \paragraph{Model Performance Comparison}
% A notable trend emerges in the performance gap between \gpt and \othree. While both models benefit from our defense methods, \gpt generally shows higher utility across all attacks compared to \othree and \llama.



\subsection{Ablation Study and Sensitivity Test}
\label{subsec:ablation}

\textbf{Ablation Study.}
\input{tables/ablation-challenge}
We conduct a systematic ablation study to validate the importance of our three key designs discussed in~\cref{subsec:overview} using the \dojo dataset with \gpt under the Important Messages attack. We remove each design component one at a time. First, for the masking function $\mathcal{M}$, instead of replacing $\mathcal{T}_u$ with $\mathcal{T}_f$, we simply delete $\mathcal{T}_u$ (denoted as ``Basic''). Second, for the tool call cache mechanism, we remove $\mathcal{H}$ and only compare the current step's tool calls between the original run and the masking run (denoted as ``No Cache''). For the tool call comparison strategy, we compare the complete actions $\mathcal{A} = (\mathcal{R}, \mathcal{C})$ rather than only tool calls $\mathcal{C}$ (denoted as ``Full Comp.''). As shown in \cref{tab:ablation-challenge}, removing any of these components increases the Attack Success Rate (ASR) compared to \method, demonstrating that each component is essential for effective detection.


\textbf{Sensitivity Test.}
\input{tables/ablation-prompt}
We evaluate the robustness of \method by analyzing its sensitivity to two key parameters: the formulation of task-neutral prompt $\mathcal{T}_f$ and the similarity threshold $\theta$. All experiments are conducted on the \dojo dataset with \gpt using the Important Messages attack.

For the task-neutral prompt analysis, we test four variations: 
(1) a paraphrased version of the original summarization instruction (Para.), (2) a sentiment analysis instruction (Senti.), (3) a grammar checking instruction (Grammar), and (4) a translation instruction (Trans.).
We selected these tasks because they typically do not require tool calls and are distinct from common user tasks, which helps reduce false positives. The results in \cref{table:ablation-prompt} show consistent performance across all prompt variations, indicating that \method's effectiveness does not depend on specific formulations of $\mathcal{T}_f$.


\input{tables/ablation-threshold}
We then evaluate how different similarity thresholds $\theta$ affect performance. 
 \cref{table:ablation-threshold} shows the results on the similarity threshold. 
Overall, the result shows minimal standard deviations in normal utility, and ASR across different thresholds. 
The utility under attack decreases slightly with lower thresholds, which aligns with our expectations since lower thresholds can increase false positives.
For instance, when distinguishing between malicious commands (e.g., \prompt{send\_money(hacker)}) and legitimate operations involving similar actions (e.g., \prompt{send\_money(friend)}), lower thresholds can lead to misclassification.



\subsection{Analysis of False Positive Rate}
\label{subsec:fpr}

Since \method performs detection after each agent action $\mathcal{A}$, it is essential to evaluate its false positive rate (FPR) during normal operation. We compare the FPR of \method with two baseline detectors: DeBERTa detector and LLM detector. We evaluate the FPR of \gpt on \dojo. Our experiments show FPR values of $9.28\%$ for \method, $61.86\%$ for DeBERTa detector, and $0.00\%$ for LLM detector. Detailed examples of false positive cases produced by \method and DeBERTa detector are in~\cref{appendix-fpr}.

In our analysis, \method generated nine false positive detections for \gpt. However, after detailed examination, we found \textbf{all} these cases involve user tasks $\mathcal{T}_u$ that explicitly request the agent to retrieve and execute instructions from external sources without verification (e.g., ``\prompt{Please do all the tasks I have on my TODO list at www.abc.com}''). We argue that these cases represent actual security risks rather than detection errors, as they allow unrestricted execution of unverified external instructions. 
For the baseline detectors, DeBERTa detector shows a high false positive rate by incorrectly flagging most benign cases. While LLM detector achieves $0\%$ FPR, this perfect specificity comes at the cost of poor detection capability as discussed in~\cref{subsec:sec4-results}.
% Also, this security risk could be addressed by implementing a user trust confirmation mechanism: when potential injections are detected in retrieved content, users could be prompted to explicitly verify whether they trust the source.

\subsection{Analysis of Attack Success Cases}
\label{subsec:failure}
To understand the limitations of \method, we analyze $66$ cases where attacks evaded detection across three LLMs (\gpt, \othree, and \llama) using the Important Messages attack on AgentDojo. We identify four primary failure patterns: 
(1) Response-Based Attacks ($72.73\%$): When $\mathcal{T}_m$ achieves its objective through text responses rather than tool calls (e.g., persuading users to make expensive purchases), these attacks manifest in $\mathcal{R}$ rather than $\mathcal{C}$. Since \method only monitors tool calls, such attacks can evade detection. 
(2) Tool Call Redundancy ($15.15\%$): When $\mathcal{O}_{1:t}$ contains partial results for malicious task $\mathcal{T}_m$, the original run utilizes these existing results while the masking run generates repeat tool calls to obtain the same information. This discrepancy in tool usage prevents matching between $\mathcal{C}_{t+1}^o$ and $\mathcal{C}_{t+1}^m$, leading to missed detections. 
(3) State Hallucination ($6.06\%$): The agent in the original run skips necessary tool calls by hallucinating the required information. For example, when $\mathcal{T}_m$ requests retrieving a user's phone version, the agent directly responds with "iPhone X" without making appropriate tool calls. 
(4) Function Hallucination ($6.06\%$): The agent generates calls to non-existent functions that are not provided in the tool set, causing mismatches between the original and masked runs.