\section{Related Work}
\label{sec:related}



\noindent\textbf{Real-time Object Detectors.} 
Real-time object detectors have consistently attracted the community's attention due to their significant practical value.
%
The YOLO series~\cite{redmon2016yolov1, redmon2017yolo9000, redmon2018yolov3, bochkovskiy2020yolov4, jocher2020yolov5, li2023yolov6, wang2023yolov7, wang2024gold, chen2023yolo, yolov8, wang2024yolov9, wang2024yolov10, jocher2024yolov11} has emerged as a leading framework for real-time object detection.
%
The early YOLO systems~\cite{redmon2016yolov1, redmon2017yolo9000, redmon2018yolov3} establish the framework for the YOLO series, primarily from a model design perspective. 
%
YOLOv4~\cite{bochkovskiy2020yolov4} and YOLOv5~\cite{jocher2020yolov5} add CSPNet~\cite{wang2020cspnet}, data augmentation, and multiple feature scales to the framework. YOLOv6~\cite{li2023yolov6} further advance these with BiC and SimCSPSPPF modules for the backbone and neck, alongside anchor-aided training. 
%
YOLOv7~\cite{wang2023yolov7} introduce E-ELAN~\cite{wang2022designing_elan} (efficient layer aggregation networks) for improved gradient flow and various bag-of-freebies, while YOLOv8~\cite{yolov8} integrate a efficient C2f block for enhanced feature extraction.
%
In recent iterations, YOLOv9~\cite{wang2024yolov9} introduce GELAN for architecture optimization and PGI for training improvement, while YOLOv10~\cite{wang2024yolov10} apply NMS-free training with dual assignments for efficiency gains. YOLOv11~\cite{jocher2024yolov11} further reduces latency and increases accuracy by adopting the C3K2 module (a specification of GELAN~\cite{wang2024yolov9}) and lightweight depthwise separable convolution in the detection head. 
%
Recently, an end-to-end object detection method, namely RT-DETR~\cite{zhao2024rtdetrs}, improved traditional end-to-end detectors~\cite{carion2020detr, liu2022dab_detr, zhu2020deformable_detr, meng2021conditional_detr, li2022dn_detr} to meet real-time requirements by designing an efficient encoder and an uncertainty-minimal query selection mechanism. RT-DETRv2~\cite{lv2024rt_detrv2} further enhances it with bag-of-freebies.
%
Unlike previous YOLO series, this study aims to build a YOLO framework centered around attention to leverage the superiority of the attention mechanism.



\begin{figure*}[t]
\centering
\includegraphics[width=0.99\linewidth]{figs_v12/area_attention.pdf}
\caption{\textbf{Comparison of the representative local attention mechanisms with our area attention.}  Area Attention adopts the most straightforward equal partitioning way to divide the feature map into $l$ areas vertically or horizontally. (default is 4). This avoids complex operations while ensuring a large receptive field, resulting in high efficiency.}
\label{fig:split}
\end{figure*}



\noindent\textbf{Efficient Vision Transformers.} 
Reducing computational costs from global self-attention is crucial to effectively applying vision transformers in downstream tasks. PVT~\cite{pvt} addresses this using multi-resolution stages and downsampling features. 
%
Swin Transformer~\cite{liu2021swin} limits self-attention to local windows and adjusts the window partitioning style to connect non-overlapping windows, balancing communication needs with memory and computation demands.
%
Other methods, such as axial self-attention~\cite{ho2019axial} and criss-cross attention~\cite{huang2019ccnet}, calculate attention within horizontal and vertical windows. CSWin transformer~\cite{dong2022cswin} builds on this by introducing cross-shaped window self-attention, computing attention along horizontal and vertical stripes in parallel.
%
In addition, local-global relations are established in works such as~\cite{chu2021twins, yu2021glance}, improving efficiency by reducing reliance on global self-attention.
%
Fast-iTPN~\cite{tian2024fast} improves downstream task inference speed with token migration and token gathering mechanisms.
%
Some approaches~\cite{shen2021linear_attn, wang2020linformer_attn, katharopoulos2020transformers_attn, xie2025sana} use linear attention to decrease the complexity of the attention. Although Mamba-based vision models~\cite{zhu2024vision_mamba, liu2024vmamba} aim for linear complexity, they still fall short of real-time speeds~\cite{liu2024vmamba}.
%
FlashAttention~\cite{dao2022flashattention, dao2023flashattentionv2} identifies high bandwidth memory bottlenecks that lead to inefficient attention computation and addresses them through I/O optimization, reducing memory access to enhance computational efficiency.
%
In this study, we discard complex designs and propose a simple area attention mechanism to reduce the complexity of attention. Additionally, we employ FlashAttention to overcome inherent memory accessing problems of the attention mechanism~\cite{dao2022flashattention, dao2023flashattentionv2}.



\begin{figure*}[!htb]
\centering
\includegraphics[width=0.99\linewidth]{figs_v12/archi_comparison.pdf}
\caption{\textbf{The architecture comparison with popular modules} including (a): CSPNet~\cite{wang2020cspnet}, (b) ELAN~\cite{wang2022designing_elan}, (c) C3K2 (a case of GELAN)~\cite{wang2024yolov9, jocher2024yolov11}, and (d) the proposed R-ELAN (residual efficient layer aggregation networks). }
\label{fig:archi_comparison}
\end{figure*}