\section{Related Work}
\label{sec:related}



\noindent\textbf{Real-time Object Detectors.} 
Real-time object detectors have consistently attracted the community's attention due to their significant practical value.
%
The YOLO series____ has emerged as a leading framework for real-time object detection.
%
The early YOLO systems____ establish the framework for the YOLO series, primarily from a model design perspective. 
%
YOLOv4____ and YOLOv5____ add CSPNet____, data augmentation, and multiple feature scales to the framework. YOLOv6____ further advance these with BiC and SimCSPSPPF modules for the backbone and neck, alongside anchor-aided training. 
%
YOLOv7____ introduce E-ELAN____ (efficient layer aggregation networks) for improved gradient flow and various bag-of-freebies, while YOLOv8____ integrate a efficient C2f block for enhanced feature extraction.
%
In recent iterations, YOLOv9____ introduce GELAN for architecture optimization and PGI for training improvement, while YOLOv10____ apply NMS-free training with dual assignments for efficiency gains. YOLOv11____ further reduces latency and increases accuracy by adopting the C3K2 module (a specification of GELAN____) and lightweight depthwise separable convolution in the detection head. 
%
Recently, an end-to-end object detection method, namely RT-DETR____, improved traditional end-to-end detectors____ to meet real-time requirements by designing an efficient encoder and an uncertainty-minimal query selection mechanism. RT-DETRv2____ further enhances it with bag-of-freebies.
%
Unlike previous YOLO series, this study aims to build a YOLO framework centered around attention to leverage the superiority of the attention mechanism.



\begin{figure*}[t]
\centering
\includegraphics[width=0.99\linewidth]{figs_v12/area_attention.pdf}
\caption{\textbf{Comparison of the representative local attention mechanisms with our area attention.}  Area Attention adopts the most straightforward equal partitioning way to divide the feature map into $l$ areas vertically or horizontally. (default is 4). This avoids complex operations while ensuring a large receptive field, resulting in high efficiency.}
\label{fig:split}
\end{figure*}



\noindent\textbf{Efficient Vision Transformers.} 
Reducing computational costs from global self-attention is crucial to effectively applying vision transformers in downstream tasks. PVT____ addresses this using multi-resolution stages and downsampling features. 
%
Swin Transformer____ limits self-attention to local windows and adjusts the window partitioning style to connect non-overlapping windows, balancing communication needs with memory and computation demands.
%
Other methods, such as axial self-attention____ and criss-cross attention____, calculate attention within horizontal and vertical windows. CSWin transformer____ builds on this by introducing cross-shaped window self-attention, computing attention along horizontal and vertical stripes in parallel.
%
In addition, local-global relations are established in works such as____, improving efficiency by reducing reliance on global self-attention.
%
Fast-iTPN____ improves downstream task inference speed with token migration and token gathering mechanisms.
%
Some approaches____ use linear attention to decrease the complexity of the attention. Although Mamba-based vision models____ aim for linear complexity, they still fall short of real-time speeds____.
%
FlashAttention____ identifies high bandwidth memory bottlenecks that lead to inefficient attention computation and addresses them through I/O optimization, reducing memory access to enhance computational efficiency.
%
In this study, we discard complex designs and propose a simple area attention mechanism to reduce the complexity of attention. Additionally, we employ FlashAttention to overcome inherent memory accessing problems of the attention mechanism____.



\begin{figure*}[!htb]
\centering
\includegraphics[width=0.99\linewidth]{figs_v12/archi_comparison.pdf}
\caption{\textbf{The architecture comparison with popular modules} including (a): CSPNet____, (b) ELAN____, (c) C3K2 (a case of GELAN)____, and (d) the proposed R-ELAN (residual efficient layer aggregation networks). }
\label{fig:archi_comparison}
\end{figure*}