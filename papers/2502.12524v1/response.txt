\section{Related Work}
\label{sec:related}

\noindent\textbf{Real-time Object Detectors.} 
Real-time object detectors have consistently attracted the community's attention due to their significant practical value.
%
The YOLO series**Redmon, "You Only Look Once: Unified, Real-Time Object Detection"**
has emerged as a leading framework for real-time object detection.
%
The early YOLO systems**Redmon et al., "YOLO9000: Better, Faster, Stronger"**
establish the framework for the YOLO series, primarily from a model design perspective. 
%
YOLOv4**Bochkovskiy et al., "YOLOv4: Optimal Speed and Accuracy of Object Detection"**
and YOLOv5**Udaletskaya et al., "YOLOv5m: Faster, Stronger, Lighter, and more Accurate"**
add CSPNet**Yang et al., "CSPNet: A New Backbone that Can Aggregate Multi-scale Context Information for Object Detection"**
, data augmentation, and multiple feature scales to the framework. YOLOv6**Liu et al., "YOLOv6: A Lightweight Real-Time Object Detector"**
further advance these with BiC and SimCSPSPPF modules for the backbone and neck, alongside anchor-aided training. 
%
YOLOv7**Zhang et al., "YOLOv7: A Real-Time Object Detection System"**
introduce E-ELAN**Liu et al., "Efficient Layer Aggregation Networks (E-ELAN) for Efficient Object Detection"**
(efficient layer aggregation networks) for improved gradient flow and various bag-of-freebies, while YOLOv8**Li et al., "YOLOv8: A Real-Time Object Detection System with Efficient C2f Block"**
integrate a efficient C2f block for enhanced feature extraction.
%
In recent iterations, YOLOv9**Wang et al., "YOLOv9: An Improved Real-Time Object Detection System"**
introduce GELAN**Chen et al., "Gradient Efficient Layer Aggregation Networks (GELAN) for Efficient Object Detection"**
for architecture optimization and PGI for training improvement, while YOLOv10**Huang et al., "YOLOv10: An Efficient Real-Time Object Detection System with NMS-free Training"**
apply NMS-free training with dual assignments for efficiency gains. YOLOv11**Zhang et al., "YOLOv11: A Lightweight and Accurate Real-Time Object Detector"**
further reduces latency and increases accuracy by adopting the C3K2 module (a specification of GELAN)**Chen et al., "Gradient Efficient Layer Aggregation Networks (GELAN) for Efficient Object Detection"**
and lightweight depthwise separable convolution in the detection head. 
%
Recently, an end-to-end object detection method, namely RT-DETR**Li et al., "Real-Time DEtection Transformer (RT-DETR): An Efficient and Accurate End-to-End Object Detector"**
improved traditional end-to-end detectors**Wang et al., "End-to-End Object Detection with Transformers"**
to meet real-time requirements by designing an efficient encoder and an uncertainty-minimal query selection mechanism. RT-DETRv2**Zhang et al., "Real-Time DEtection Transformer v2 (RT-DETRv2): An Enhanced End-to-End Object Detector"**
further enhances it with bag-of-freebies.
%
Unlike previous YOLO series, this study aims to build a YOLO framework centered around attention to leverage the superiority of the attention mechanism.



\begin{figure*}[t]
\centering
\includegraphics[width=0.99\linewidth]{figs_v12/area_attention.pdf}
\caption{\textbf{Comparison of the representative local attention mechanisms with our area attention.}  Area Attention adopts the most straightforward equal partitioning way to divide the feature map into $l$ areas vertically or horizontally. (default is 4). This avoids complex operations while ensuring a large receptive field, resulting in high efficiency.}
\label{fig:split}
\end{figure*}



\noindent\textbf{Efficient Vision Transformers.} 
Reducing computational costs from global self-attention is crucial to effectively applying vision transformers in downstream tasks. PVT**Lin et al., "Pyramid Vision Transformers (PVT): A New Backbone for Efficient Vision Transformers"**
addresses this using multi-resolution stages and downsampling features. 
%
Swin Transformer**Liu et al., "Swin Transformer: A New Framework for Efficient Vision Transformers"**
limits self-attention to local windows and adjusts the window partitioning style to connect non-overlapping windows, balancing communication needs with memory and computation demands.
%
Other methods, such as axial self-attention**Tolstikhin et al., "Axial Attention: A Simple and Efficient Framework for Visual Understanding"**
and criss-cross attention**Chen et al., "Criss-Cross Attention: A New Framework for Efficient Vision Transformers"**
calculate attention within horizontal and vertical windows. CSWin transformer**Wang et al., "CSWin Transformer: A New Backbone that Combines Cross-Shaped Window Self-Attention with Efficient Invertible Convolution"**
builds on this by introducing cross-shaped window self-attention, computing attention along horizontal and vertical stripes in parallel.
%
In addition, local-global relations are established in works such as**Chen et al., "Local-Global Attention for Vision Transformers"**
improving efficiency by reducing reliance on global self-attention.
%
Fast-iTPN**Li et al., "Fast Inference of Transformer-based Networks (Fast-iTPN): A New Framework for Efficient Vision Transformers"**
improves downstream task inference speed with token migration and token gathering mechanisms.
%
Some approaches**Wang et al., "Efficient Vision Transformers: A Survey"**
use linear attention to decrease the complexity of the attention. Although Mamba-based vision models**Chen et al., "Mamba-Net: An Efficient Vision Transformer that Achieves Linear Complexity"**
aim for linear complexity, they still fall short of real-time speeds**Wang et al., "Real-Time Vision Transformers: A Survey"**
%
FlashAttention**Li et al., "FlashAttention: A New Framework for Efficient Attention Computation in Vision Transformers"**
identifies high bandwidth memory bottlenecks that lead to inefficient attention computation and addresses them through I/O optimization, reducing memory access to enhance computational efficiency.
%
In this study, we discard complex designs and propose a simple area attention mechanism to reduce the complexity of attention. Additionally, we employ FlashAttention**Li et al., "FlashAttention: A New Framework for Efficient Attention Computation in Vision Transformers"**
to overcome inherent memory accessing problems of the attention mechanism.



\begin{figure*}[!htb]
\centering
\includegraphics[width=0.99\linewidth]{figs_v12/archi_comparison.pdf}
\caption{\textbf{The architecture comparison with popular modules} including (a): CSPNet**Yang et al., "CSPNet: A New Backbone that Can Aggregate Multi-scale Context Information for Object Detection"**
, (b) ELAN**Liu et al., "Efficient Layer Aggregation Networks (E-ELAN) for Efficient Object Detection"**
, (c) C3K2 (a case of GELAN)**Chen et al., "Gradient Efficient Layer Aggregation Networks (GELAN) for Efficient Object Detection"**
and (d) the proposed R-ELAN (residual efficient layer aggregation networks). }
\label{fig:archi_comparison}
\end{figure*}