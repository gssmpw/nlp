\section{Introduction}
\label{sec:intro}








Federated Learning (FL) is a widely used paradigm for training models across distributed data sources~\cite{fedavg}. 
In FL, clients compute model updates locally on private data and send them to a central server. 
FL was originally designed with privacy in mind, aiming to share model updates instead of raw data, under the assumption that these updates would not leak sensitive information. However, recent attacks have shown that exposing client updates in the clear makes FL vulnerable to full-scale reconstruction attacks~\citep{zhu2019deep,geiping2020inverting}.


In response to these privacy risks, FL has been combined with secure aggregation techniques such as multiparty computation (MPC)~\cite{bonawitz2017practical, bell2020secure, fereidooni2021safelearn, so2022lightsecagg} or zero-knowledge proofs (ZKPs) to ensure that the server only learns the aggregated update and not individual updates (Figure~\ref{fig:a_secure}).
However, this privacy guarantee comes at the cost of robustness --- since the server no longer sees individual updates, it lacks a mechanism to verify their validity before aggregation. 
There is active research on Byzantine-robust defense mechanisms to filter out malformed updates in plaintext FL~\cite{sun2019normbound, steinhardt2017normball, yin2018trimmedmean, blanchard2017krum, cao2020fltrust}, but integrating these checks into secure aggregation is challenging.
Since these checks are typically built in plaintext, it is generally the case that the server has access to data, or metadata about the updates that it would not have when using secure aggregation. This makes it difficult to translate these predicates to secure aggregation.


\input{contents/figures/fig_intro}

There are recent efforts to incorporate robustness checks into secure FL~\cite{franzese2023robust}, but they remain limited in terms of the flexibility of checks that are supported or the reliance on public validation data (Figure~\ref{fig:b_robust_with_publicval}).
Specifically, ZKP-based approaches~\cite{rofl, elsa} only support simple predicates, such as $l_2$ and $l_{\infty}$ norm bounds, while Eiffel~\cite{CCS:CGJv22} extends support to more predicates but relies on a public data to determine the predicate hyperparameters.
In practice, public validation data may not always be available, and even when it exists, its utility depends on how well it represents the overall client distribution. As client data naturally evolves or new clients joining over time, maintaining an effective public dataset becomes non-trivial, particularly in secure FL setups. 

To this end, we attempt to resolve the dilemma of privacy vs. robustness of FL from a fresh angle by asking:
\begin{tcolorbox}
    \begin{itemize}[leftmargin=*]
        \item Can we leverage clients' private data while preserving the privacy guarantee of secure aggregation?
        \item Can leveraging private data enable more advanced checks that enhance robustness while overcome the reliance on public validation data? 
    \end{itemize}
\end{tcolorbox}
We present~\ours, a general framework for \textbf{S}ecurely \textbf{L}everaging clients' private data to \textbf{V}alidate updates for \textbf{R}obust FL.
Aided by MPC, \ours\ can compute statistics over multiple clients' private data that is otherwise impossible in previous secure FL work (Figure~\ref{fig:c_ours}). These statistics, such as cross-client validation accuracy, can play a vital role in enhancing model robustness.
The modular design of \ours\ makes it compatible with existing MPC implementations, and thus allows flexible deployment in various threat models.
Furthermore, \ours\ naturally adapts to distribution shifts as it can securely refresh its validation data up-to-date.  

In summary, our contributions are as follows,
\begin{enumerate}
    \item We propose the first framework that enables cross-client checks using private inputs from different clients, eliminating reliance on public validation data. Our approach is compatible with existing MPC protocols, allowing flexible customization based on the threat model and computational constraints.
    \item  We evaluate \ours~against model poisoning attacks, including an adaptive attack---the strongest possible under the threat model. \ours~demonstrates competitive robustness and outperforms prior work under adaptive attacks (\eg by up to 50\% on CIFAR-10).
    \item We study various scenarios for client data distribution shift scenarios and empirically demonstrate the adaptability of \ours. In contrast, prior works struggle, experiencing severe accuracy degradation (\eg 30\% drop from MNIST to SVHN) or failing to progress.
\end{enumerate}









