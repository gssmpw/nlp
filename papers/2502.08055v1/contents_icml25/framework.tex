\section{\ours: an Overview}
\label{sec:framework}





At a high level, our framework securely leverages client's private data to verify the integrity of model updates. It comprises two main components: 1) a secure and private cross-client check procedure that computes useful statistics for robustness, \eg the accuracy of a client's model on another client's data, to determine the weight of each model update, and 2) a secure aggregation protocol that calls the aforementioned cross-client check procedure and computes the global model for the next round.

We describe an MPC-computation friendly filtering mechanism based on a simple check score in~\secref{slvr-check}, and proceed to the MPC protocol that facilitate the secure computation in~\secref{slvr-protocol}.







\subsection{Check Procedure for Robustness}
\label{sec:slvr-check}
Our check procedure determines the weight of each client's model update in aggregation. As shown in~\figref{pcheck}, it consists of 1) a check committee selection for each client, 2) a check function/score for each client's update, and 3) a weight assignment to each update.  
At the start of training, the server $\server$ specifies a subroutine that determines the weights of client updates to the MPC nodes $\mpcnodes$. A client $\calC_i$ secret-share a random subset of their training data to $\mpcnodes$, which we refer to as \textit{validation data}, denoted as $\bfD^{\msf{val}}_i$. 







\mypara{Check Committee.} Let $\cor$ denote the number of malicious clients. 
$\mpcnodes$ sample a set of $2 \cor + 1$ clients for each client $\calC_i$. This set of clients, denoted by $\calC_{check, i}$, is called the check committee for $\calC_i$. Validation data submitted by these clients will be used to check $\calC_i$'s model. The size $2\cor+1$ guarantees an honest-majority in $\calC_{check, i}$.


\mypara{Check Score.} Intuitively, a check score should reflect how much positive contribution a client's update make to the global objective. Let $\score{i}{j}$ be the score assigned to $\calC_i$.
A choice of check score can be defined as,
\begin{align}
\label{eqn:check_acc}
    &\score{i}{j} = \msf{Chk}_{acc}(\bfw_i, \bfD^{\msf{val}}_j) - \msf{Chk}_{acc}(\bfw^{(t - 1)}, \bfD^{\msf{val}}_j), \\ \nonumber
    &\msf{Chk}_{acc}(\bfw, \bfD) = \frac{1}{|\bfD|} \sum_{(\bfx_k, y_k) \in \bfD} \indicator{\hat{y}(\bfx_k; \bfw) = y_k}
\end{align}
where $\bfw^{(t - 1)}$ is the global model at the end of the previous round.
$\bfw^{(t)}_i = \bfw^{(t - 1)} + \bfu^{(t)}_i$ is $i$-th client's local model at round $t$ (we omit the superscript $t$ for the client model for brevity).
Let $\bff(\cdot; \bfw)$ be a neural network classifier parameterized by $\bfw$, and 
$\hat{y}(\bfx; \bfw) = \argmax_l \bff_l (\bfx; \bfw)$ is the predicted label given $\bfx$.

Essentially, $\score{i}{j}$ measures the increase of accuracy of $\calC_i$'s new model on $\bfD^{\msf{val}}_j$ over the current global model~\footnote{We note that $\score{}{}$ is not limited to the accuracy difference in Equation~\ref{eqn:check_acc}. See Appendix~\ref{app:softscore} for a `soft' alternative using confidence score, which corresponds to \ours(prob) in Sec~\ref{sec:experiment}.}.
Last, we condense the set of scores $\{\score{i}{j} \}_{j \in \calC_{check, i}}$ into a scalar $\score{i}{}$ as the check score for $\calC_i$'s model. We define $\score{i}{}$ to be the trimmed mean of $\{\score{i}{j} \}_{j \in \calC_{check, i}}$ to enhance numerical stability. 






\mypara{Weights Assignment.}
We keep the top $k\%$ of the model updates in the final aggregation, where $k = 1-\cor/m$ is a public threshold.

\mypara{Synergy with Norm Bound.} Our check score and the norm bound are naturally complementary. During the early epochs, validation loss alone may not be the most stable indicator of malicious update; the standard norm bound constraint can effectively limit adversarial impact. As the model converges, the cross-client check becomes increasingly effective since the statistics used in the check are closely associated with the learning objective. 






\subsection{Secure Aggregation with Check Results}
\label{sec:slvr-protocol}

\figref{pisecagg} shows the secure computation backbone of \ours. The most critical components are 1) a setup phase to establish the necessary security keys, 2) a secret-sharing ($\PShare$) and a reconstruction protocol ($\PRec$) protocol pair, and 3) MPC subroutines for operations in the check algorithms such as inference ($\FSecInf$), random selection ($\FRand$) and sorting ($\FSort$).
We elaborate our instantiation in Appendix~\ref{app:mpcdetails}.


\begin{protofig}{Protocol $\PSecAgg$}{Protocol for Robust Secure Aggregation}{pisecagg}
\label{proto:ours-acc}

\textbf{Parameters:} MPC nodes $\mpcnodes$, the clients $\calC_1, \ldots, \calC_m$. Number of corrupted clients, $\cor$. The threshold $k$ to select the top-$k$ updates, number of validation datapoints, $d$. \\

\textbf{Setup:} $\mpcnodes$ call $\FRand$ to receive shared pairwise keys, $\kappa_{ij}$, for $i, j \in |\mpcnodes|$ and a key, $\kappa$, shared between all the parties. \\

\textbf{Validation committees:} For each client $\calC_i$, $\mpcnodes$ use their common key, $\kappa$, to sample a sets of clients, $\calC_{check, i}$, where $|\calC_{check, i}| = 2 \cor + 1$. \\

\textbf{Model Updates:} At the start of every round, each client and $\mpcnodes$ run $\PShare$ on its local model, $\vec u_i$, and the validation data $\vec D_i$,  where $|\vec D^{\msf{val}}_i| = d$, to generate $\arith{\vec u_i}, \arith{\vec D^{\msf{val}}_i}$ with $\mpcnodes$. \\

\textbf{Robustness Check and Secure Aggregation:} 
\begin{enumerate}
    \item $\mpcnodes$ run $\PCheck$ with input $\vec w_i$, the global model from the previous round, $\vec w$, and $\vec D^{\msf{val}}_i$, for $i \in [m]$. 
    \item They receive $\arith{b_i}$, for $i \in [m]$, where $b_i = 1$ if $\calC_i$'s model has passed the check, and 0 otherwise.
    \item Compute $\arith{\vec w'_i} = \arith{\vec w_i} \cdot \arith{b_i}$, by running $\PMult$.
    \item Locally compute $\arith{\vec w_{\msf{aggr}}} = \Sigma_{i = 1}^m \arith{\vec w'_i}$.
    \item Reconstruct $\vec w_{\msf{aggr}}$ by running $\PRec$.
\end{enumerate}

\end{protofig}




\mypara{Security and Privacy Properties.} Security and privacy are straightforward to argue for our protocol, as the protocol is only composed of calls to the ideal functionalities. The parties do not communicate messages with each other outside of these calls, and only perform local computation. Our choice of the check function, $\msf{Chk}$, does not impact privacy as the result of the check is never revealed to the parties. The results are only used to obliviously compute the weight multiplier for each client's model update, and the final aggregated model is revealed to the server.
We formally state the following theorem, and give a proof sketch in \appref{proofs}.


\begin{protofig}{Protocol $\PCheck$}{Protocol for Robustness Check}{pcheck}
\label{proto:pcheck}

The protocol receives a set of client updates to check, $\vec u_i$, and validation datasets $\vec D^{\msf{val}}_i$, for $i \in [m]$, to check against. $\lambda$ is a public multiplier for the bound. \\

\textbf{Norm Check:} To compute the norm bound, $\mpcnodes$ do the following,
\begin{enumerate}
    \item Compute $\arith{\msf{norm}_i} = \sqrt{\arith{\vec u_i^2}}$ by running $\PMult, \PSqrt$. Call $\FSort$ on the vector $\arith{\msf{norm}} = \{ \arith{\msf{norm}_i} \}$ for $i \in [m]$. 
    \item Set $\msf{bound}$ = $\lambda * \msf{Median}(\{ \msf{norm}_i \})$, for $i \in [m]$. 
    \item Compute the vector $\{ \arith{b^{\msf{norm}}_i} \}$ via $\FComp$, for $i \in [m]$, where $b^{\msf{norm}}_i = 1$ if $\msf{norm}_i < \msf{bound}$, and 0 otherwise.
\end{enumerate}

\textbf{Accuracy Check:} For each client, $\calC_i$,
\begin{enumerate}
    \item Call $\FSecInf$ with $(\vec w_i, \vec D^{val}_j)$, and $(\vec w, \vec D^{val}_j)$, to receive $\arith{\Acc{i}{j}}$ and $\arith{\Acc{i}{\msf{t - 1}, j}}$ respectively, where $\vec w$ is the global model from the previous round, for $j \in \calC_{check, i}$.
    \item $\mpcnodes$ locally compute $\arith{\diff{i}{j}} = \arith{\Acc{i}{j}} - \arith{\Acc{i}{t - 1, j}}$.
    \item Call $\FSort$ on $\arith{\diff{i}{j}}$, for $j \in \calC_{check, i}$, to receive a sharing of the sorted scores.
    \item Let $\msf{trim \mhyphen diff}^j_i$, for $j \in \cor + 1$, denote the list each $P_k \in \mpcnodes$ obtains by locally trimming the first and last $\cor/2$ elements from the sorted scores. 
    \item Locally compute $\arith{\msf{scr}_i} = \Sigma_{j = 1}^{\cor + 1} \arith{\msf{trim \mhyphen diff}^j_i} / (\cor + 1)$. 
\end{enumerate}

\textbf{Select top-$k$:}
\begin{enumerate}
    \item Call $\FSort$ on the list $(i, \arith{\score{i}{}})$ to sort on the scores. Call $\FZeroOne$ with $k$ to receive $k$ shares of one and $m - k$ shares of zero. Set the top $k$ shares to be shares of one and the rest to be shares of zero, received from $\FZeroOne$.
    \item Call $\FSort$ on $(i, \arith{\score{i}{}})$, to sort on the indices, $i \in [m]$. 
    \item Compute $\arith{b_i} = \arith{\score{i}{}} \cdot \arith{b^{\msf{norm}}_i}$, for $i \in [m]$ via $\PMult$, and output the vector.
\end{enumerate}


    

\end{protofig}


\begin{theorem}\label{thm:secagg}
    Protocol $\PSecAgg$ securely realises the functionality $\FSecAgg$ in the presence of a malicious adversary that can statically corrupt up to $\cor < m/2$ parties in the protocol, in the $(\FSecInf, \FSort, \FRand)$-hybrid model.
\end{theorem}

\textbf{Output privacy:} We consider output privacy to be out of scope of this work. MPC formally only guarantees privacy of the inputs, and that the intermediate values do not leak any additional data than what is already allowed by the algorithm. Since all the parties in the system learn the aggregated global model at the end of every round of training, it is possible for an adversary to try to reverse engineer the model to infer data about the honest clients. A popular approach to mitigating this is using differential privacy, which involves sampling and adding noise to the model updates. We note that it is an interesting future direction to combine differential privacy with our framework.



















