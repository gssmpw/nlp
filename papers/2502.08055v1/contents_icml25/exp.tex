\section{Experiments}
\label{sec:experiment}



In this section, we employ experiments and show that~\ours~has (1) better robustness in the presence of malicious clients (Section~\ref{sec:empirical-robustness}; Table~\ref{tab:robustness}), (2) better adaptability to the change in the underlying distribution of client data (Section~\ref{sec: empirical-adaptability}; Figure~\ref{fig:adaptability}), and (3) reasonable computation and communication overhead that can be significantly improved via parallelism (Section~\ref{sec: mpc-benchmark}; Table~\ref{tab:mpc-benchmark}).



\subsection{Setup}
\label{sec:exp-setup}

We briefly describe the experimental setup following the conventions in the FL literature~\cite{fang2020local,CCS:CGJv22}.
See Appendix~\ref{app:setup} for full descriptions.

\mypara{Models and Datasets.} We consider the following model architectures and datasets: LeNet-5 for MNIST and Fashion-MNIST (FMNIST), ResNet-18 for SVHN and CIFAR-10.
We follow the given train vs. test set split and further split each train set, balanced across classes. We follow the setup in~\cite{CCS:CGJv22} and reserve random 10k training samples as the public validation set ($\calD_{pubval}$) for aggregation baselines that leverage public validation datasets. The remaining training samples are partitioned across clients by a Dirichlet distribution $\text{Dir}_K(\alpha)$ with $\alpha = 0.5$ to emulate data heterogeneity in realistic FL scenarios~\cite{cao2020fltrust}.
We assume there are 100 clients where $10\%$ or $20\%$ of them are malicious clients.



\mypara{Defense baselines.} We compare the performance of \ours with the following aggregation methods that are most commonly considered in Byzantine-robust secure FL literature: 
\begin{enumerate}
    \item \textit{Norm Bound (adaptive)}~\cite{rofl, elsa} accepts a client update if the update is bounded by a $\tau$: $\msf{Chk}(\bfu_i) = \indicator{||\bfu_i^{(t)}||_2 < \tau}$.
    At the start of the $t$th communication round, the server computes the threshold as $\tau = \lambda \times \text{median}(\bfu_1^{(r-1)}, \cdots, \bfu_m^{(t-1)})$ adaptively, and then broadcast it to all clients. Hence, each client can submit the norm-bound check result based on the threshold value.

    \item \textit{Norm Bound ($\calD_{pubval}$ or public data)}~\cite{CCS:CGJv22} is identical to the above Norm Bound (adaptive) except that the threshold is computed by referring to the public validation dataset: $\tau = ||\bfu_{pubval}||_2$ where $\bfu_{pubval}$ is the previous round's global model update computed with the public validation data $\calD_{pubval}$. $\tau$ can be computed by anyone participating in the communication.

    \item \textit{Norm Ball}~\cite{steinhardt2017normball,CCS:CGJv22} accepts a client update if the update is within a spherical radius from the reference %
    update computed using the public validation dataset: $\msf{Chk}(\bfu_i, \calD_{pubval}) = \indicator{||\bfu_i^{(t)} - \bfu_{pubval}||_2 < \tau}$ where $\tau = \lambda \times ||\bfu_{pubval}||_2$. $\bfu_{pubval}$ and the value for $\tau$ is available to anyone participating in the communication.

    \item \textit{Cosine Similarity}~\cite{cao2020fltrust,CCS:CGJv22} checks the cosine similarity between each client update and the global model update from the previous round $\bfu^{(t-1)}$: $\msf{Chk}(\bfu_i, \calD_{pubval}) = \indicator{\cos(\bfu_i^{(t)}, \bfu^{(t-1)}) < \tau}$ where $\cos(u, v) = \frac{<u, v>}{||u||_2 ||v||_2}$. The threshold is computed as $\tau = \lambda \times \cos(\bfu_{pubval}, \bfu^{(t-1)})$, and again, can be computed by anyone in the communication.
\end{enumerate}

$\lambda$ is a constant where a larger $\lambda$ allows for more gradient updates, leading to faster convergence but reduced robustness.


\mypara{Attack baselines.}
We evaluate each aggregation method against the following attacks:
\begin{enumerate}
    \item \textit{Additive Noise}~\cite{li2019additive} adds Gaussian noise to a malicious client update.
    \item \textit{Sign Flipping}~\cite{damaskinos2018signflipping} flips the sign of a client update: $\bfu'_i = - \bfu_i$, for a client $\calC_i$.
    \item \textit{Label Flipping}~\cite{fang2020local} is a data poisoning attack that flips the label of each training instance. Specifically, it flips a label $l \in \{0, 1, \dots, L-1\}$ into $L-l-1$ where $L$ is the number of classes.
    \item \textit{Adaptive Attack} is considered the strongest attack adaptive to each aggregation as described in Section~\ref{sec:adaptiveattack}.
\end{enumerate}

\input{contents/tables/robustness}



\subsection{Robustness against Poisoning Attacks}
\label{sec:empirical-robustness}

\mypara{\ours~is more Byzantine-robust.}
Table~\ref{tab:robustness} summarizes the robust accuracy of each aggregation method under attacks.
According to our goal in Equation~\ref{obj:defense},
All aggregation methods demonstrate comparable robust accuracies against most cases of non-adaptive, common attack baselines (\ie Additive Noise, Labelflip, and Signflip), varying by only a small margin of $2 \mhyphen 3\%$ accuracy in each column.
However, all other defense baselines suffer from significant accuracy drops under adaptive attacks, and the robustness gap compared to \ours~becomes more pronounced with more complex tasks. For example, while the accuracy gap on MNIST is relatively modest, it widens to approximately 50\% on CIFAR-10.
In other words, unlike other aggregation baselines that are more susceptible to certain attacks (\eg Non-adaptive vs. Adaptive) or fail on certain data sets (\eg Norm Ball on CIFAR-10), \ours~shows consistent robustness against all attacks on all data sets.
Such consistency makes it a more desirable choice in real-world uncertainty. An adversary may employ the strongest possible attack, which is the adaptive attack in most cases, it is crucial to provide robustness even under those attacks; specifically, as in Figure~\ref{fig:adaptiveattack}, in the worst case, the adversary can only pull down the performance of FL with \ours~to ~97\% on MNIST, ~87\% on FMNIST, ~83\% on SVHN, and ~72\% on CIFAR-10 after 1000 rounds.



\input{contents/figures/fig_adaptive}
\input{contents/figures/fig_shift}


\subsection{Adaptability to Distribution Shifts}
\label{sec: empirical-adaptability}

In this section, we examine the adaptability of \ours~along with other baseline aggregation protocols in the face of distribution shifts.
We take into account two practical scenarios that may frequently occur in real-world FL settings:


\begin{enumerate}
    \item \textbf{Scenario 1: evolving clients} (Figure~\ref{subfig:scenario-evolving}).
    Some clients involved in the communication send different data over time to the central server. Consider a cross-silo FL system for predicting patient outcomes in hospitals. Over time, a hospital's patient demographic may shift due to various factors such as changes in the local population, disease outbreaks, or the introduction of new healthcare programs. This could lead to an evolution in the type of patient data the hospital, or a subset of hospitals send to the central server, reflecting these changes.

    \item \textbf{Scenario 2: new clients} (Figure~\ref{subfig:scenario-new}). Along with the existing clients, new clients join the communication, but their data differs slightly from the existing clients. For instance, in the above FL example for a healthcare application, the existing clients could be hospitals from a different geographic location, or specialized hospitals and clinics that want to join the system to solve the same task. The new clients' data coming from such sources may differ due to their specialized nature of care, different patient demographics, or unique healthcare practices.
\end{enumerate}

We simulate these scenarios using MNIST for existing clients and SVHN for evolving or new clients, with LeNet-5 as the model. The server initially communicates with 100 MNIST clients and uses a static MNIST validation dataset. Distribution shifts occur every 100 or 250 rounds, where clients transition to SVHN (Scenario 1) or new SVHN clients join (Scenario 2). We measure global model accuracy on MNIST initially and on the combined MNIST-SVHN test set thereafter. Further details are provided in Appendix~\ref{app:setup}.
We aim to evaluate how cross-client checks contribute to adaptability under changing data distributions. Therefore, we disable the norm-bound check in \ours~and rely exclusively on cross-client checks to address the occurrence of distribution shifts.


\mypara{\ours~can adapt to changing client data distributions.}
Figure~\ref{fig:adaptability} shows the convergence of the global model against the number of communication rounds.
We observe that baseline aggregation protocols relying on a static public validation dataset suffer significant performance degradation under distribution shifts.
Specifically, Norm Bound ($\calD_{pubval}$) and Norm Ball reject every update from SVHN clients because the magnitude of updates computed on SVHN far exceeds the threshold determined on the static MNIST validation set.
The same observation applies to Norm Bound (adaptive) that uses the median of gradient updates to adaptively set the threshold since the gradient updates computed with new SVHN data are likely to fall above the median. One could relax the threshold by multiplying larger constant values to the computed median of gradients, but again, this highlights that appropriate threshold parameter setting is crucial to ensure robustness and adaptability under changing distributions, which requires non-trivial efforts, especially in secure FL settings.
On the contrary, \ours~is capable of dynamically incorporating the new shifted local data into validation. Consequently, its choice of updates balances the performance on both MNIST and SVHN, which leads to much stronger adaptability without any parameter tuning.



\subsection{MPC Benchmarks}
\label{sec: mpc-benchmark}
We benchmark the computation and communication overhead of our MPC implementation. The framework is instantiated using building blocks from MP-SPDZ~\cite{CCS:Keller20}. Benchmarks were conducted on a MacBook Pro with an Apple Silicon M4 Pro processor and 48 GB RAM. We simulated the network setup locally, and ran all the parties on the same machine, with 4 threads. For the LAN case, we simulated a network with 1 Gbps bandwidth, and 1 ms latency, and for WAN, we used 200 Mbps and 20 ms respectively.

We benchmark the three components of our MPC protocol -- the norm bound check, the cross-client check, and selecting the top-$k$ model updates separately. In reality, the normbound and cross-client check can be run in parallel, with top-$k$ run on the outputs of these checks. All the components are instantiated using a semi-honest 3PC protocol from MP-SPDZ, specifically, replicated-ring-party.x which implements the replicated secret-sharing based protocol from \cite{CCS:AFLNO16}. The protocols are run with a 128-bit ring, although the larger ring size compared to the typical 64-bit ring is only to the size of the norm. It is possible to operate over a 64-bit ring, and switch to a 128-bit ring only for the norm, although we do not implement this optimization. 

\input{contents/tables/mpc-analysis}


We report the MPC benchmarks in Table~\ref{tab:mpc-benchmark}. Compared to previous FL schemes, \ours\ unsurprisingly incurs increasing overhead as it supports private collection and computation of richer information via cross-client check. Nevertheless, the checks can be completed in reasonable time. Moreover, a large portion of the computations, \eg $\score{i}{j}$ for different $(i,j)$ pairs, are highly parallelizable, suggesting substantial speed-ups with more powerful infrastructure than our lab computer.



