\subsection{Detailed Experimental Setup}
\label{app:setup}


\mypara{Datasets.}
We consider the following four datasets:
\begin{enumerate}
    \item MNIST~\cite{lecun2010mnist} is $28\times28$ grayscale image dataset for 0-9 digit classification. It consists of 60k training and 10k test images balanced over 10 classes.
    \item Fashion-MNIST (FMNIST)~\cite{fmnist} is identical to MNIST in terms of the image size and format, the number of classes, and the number of training and test images.
    \item SVHN~\cite{svhn} is a dataset for a more complicated digit classification benchmark than MNIST. It contains 73,257 training and 26,032 testing 32Ã—32 RGB images of printed digits (from 0 to 9) cropped from real-world pictures of house number plates.

    \item CIFAR-10~\cite{cifar10} is $32\times32$ colored images balanced over ten object classes. It has 50k training and 10k test images.
\end{enumerate}
We follow the given train vs. test set split and further split each train set, balanced across classes. We follow the setup in~\cite{CCS:CGJv22} and reserve random 10k training samples as the public validation set ($\calD_{pubval}$) for aggregation baselines that leverage public validation datasets. For fair comparison, the remaining training samples are partitioned across clients by a Dirichlet distribution $\text{Dir}_K(\alpha)$ and used for training in all settings. We set $\alpha = 0.5$ to emulate data heterogeneity in realistic FL scenarios~\cite{cao2020fltrust}.
We assume there are 100 clients where $10\%$ or $20\%$ of them are malicious clients.

\mypara{Models.} Each client has a local model with one of the two following model architectures:
\begin{enumerate}
    \item LeNet-5~\cite{lecun1998gradient} with 5 convolutional layers and 60k parameters are used for MNIST and FMNIST.
    \item ResNet-18~\cite{resnet} with 18 layers and 11M parameters is used for SVHN and CIFAR-10.
\end{enumerate}

\mypara{Metrics.}
We compute the global model's accuracy on the entire test set at the end of each communication round. We repeat 10 trials using a random seed from 40 to 49 for each setting and report the average accuracy.


\mypara{Setup for distribution shifts in Section~\ref{sec: empirical-adaptability}.} We emulate the aforementioned scenarios using the MNIST data for existing clients and SVHN data for evolving or new clients, each with LeNet-5 as the model.
Note that both the original and shifted data share the same label set (\ie digits 0-9), simulating the covariate shifts~\cite{ood_bench, bai2023feed}. This ensures that a single global model will be capable of accommodating both distributions.
In the initial 250 rounds, the server communicates with 100 MNIST clients and establishes a public validation dataset using MNIST data, which remains static throughout the communication.
Following this, for Scenario 1, every 250 rounds, a random selection of 20 out of 100 clients begin to employ SVHN data and share local updates based on this data.
For Scenario 2, every 250 rounds, along with the 100 existing MNIST clients, a new set of 20 clients using randomly selected SVHN data join the communication. Consequently, the total number of participating clients increases to 120.
We transform the SVHN data into greyscale images, resized to 28x28 to match the format of the MNIST data. It is also split using the same non-IID split factor ($\alpha = 0.5$), ensuring consistency in the data setting for both MNIST and SVHN.
We report the global model's accuracy on the MNIST test set for the first 250 rounds and on the union of MNIST and SVHN tests for the remaining.
We also vary the periods of the distribution shifts by experimenting with intervals of 250 and 100 rounds.


