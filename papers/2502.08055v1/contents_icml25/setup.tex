
\section{Problem Overview}
\label{sec:prelim}

In this section, we introduce the FL setting (Section~\ref{sec:setup}), followed by its threat analysis (Section~\ref{sec:sec-model}) and an overview of our goal (Section~\ref{sec:goals}).
A comprehensive list of notataions used throughout the paper can be found in Table~\ref{tab:notations}.

\subsection{Federated Learning with Secure Aggregation}
\label{sec:setup}


Federated learning with secure aggregation is a recent advancement in FL that attempts to enhance the security and privacy of FL. The clients no longer send plain-text model updates to the server for aggregation. Instead, the updates are aggregated via a secure computation protocol. We assume the existence of an \emph{aggregation committee} to facilitate the secure aggregation. In some scenarios, such as peer-to-peer (P2P) learning, the aggregation committee could be sampled from the set of clients \cite{SP:MWAPR23,CCS:BBGLR20}, whereas in the multi-server federated learning setting, the aggregation committee could just be the servers. We abstract these differences away by assuming that there exists a set of MPC nodes, denoted by $\mpcnodes$, that receive the updates from the clients and perform secure aggregation.



Formally, we consider an FL setting in which $m$ clients collaboratively train a global model $\bff_{\bfw}(\cdot)$ maintained on a single cloud server, $\calS$, parameterized by $\bfw$. Each client $\calC_i$ has its own local training dataset $\bfD_i = \{(\bfx_j, y_j)\}_{j=1}^{N_i}$ where $N_i = |\bfD_i|$, for $i\in [m]$. 
At the start of $t$-th communication round, the server broadcasts the current global model parameters $\bfw^{(t)}$ to all the clients and the aggregation committee (of which the server may be a part of). Each client $\calC_i$ uses $\bfw^{(t)}$ as the initial model, locally computes an update $\bfu^{(t)}_{i}$ on its local data $\bfD_i$ and then sends $\bfu^{(t)}_i$ to $\mpcnodes$ in secret-shared form. 
$\mpcnodes$ run the validation protocol, aggregates using an aggregation function $\calA$ (\eg weighted sum over the updates~\cite{fedavg}), and reconstructs the aggregated update $\bfu_{\msf{aggr}}^{(t)} = \calA(\bfu^{(t)}_1, \bfu^{(t)}_2, \dots, \bfu^{(t)}_{m})$ to $\calS$. 
$\calS$ updates the global model $\bfw^{(t+1)}$ based on $\bfu_{\msf{aggr}}^{(t)}$~\footnote{We refer to client model $\bfw^{(t)}_i$ and update $\bfu^{(t)}_i$ interchangeably because $\bfu^{(t)}_i = \bfw^{(t)}_i-\bfw^{(t-1)}$ and the global model $\bfw^{(t-1)}$ is public knowledge.}.
This process is repeated for $T$ communication rounds.

\mypara{MPC.} An MPC protocol enables a set of $r$ parties, denoted by $\PartySet{} = \{ P_1, \ldots, P_r \}$, to jointly compute a function $f$ that they all agree upon, on their private inputs $x_1, \ldots, x_r$. MPC guarantees that the parties only ever learn the final result of the computation, $f(x_1, \ldots, x_r)$, and nothing else about the inputs is leaked by any intermediate value they may observe during the course of the computation.

\mypara{Secret-Sharing.} When we say a value is secret-shared in this work, we assume it is shared via a linear secret-sharing scheme (LSSS). In an LSSS, a value $x \in \F_p$ is said to be secret-shared among $r$ parties if each $P_i$, for $i \in [r]$, holds $\arith{x}_i$ such that $x = \sum_{i = 1}^r \arith{x}_i \mod p$. Given two secret-shared values $x$, $y$, we can compute a linear combination $z = a \cdot x + b \cdot y$, where $a, b$ are public constants, by carrying out the same operations on the respective shares; $\arith{z} = a \cdot \arith{x} + b \cdot \arith{y}$. 
Computing the product of two secret values, $\arith{z} = \arith{x} \cdot \arith{y}$, can also be done albeit more challenging. 
The guarantee secret-sharing schemes give is that, an adversary in possession of shares will not learn anything about the underlying secret, as long as the number of shares it possesses is below the reconstruction threshold. In our instantiation of the framework, we use the replicated secret-sharing scheme from~\citet{CCS:AFLNO16}.







\subsection{Security Model}
\label{sec:sec-model}

\mypara{Corruption.} There are three entities in our system: (1) clients, submitting model updates and validation data, (2) $\mpcnodes$, that work to verify the validity of the updates, and (3) the server that receives the aggregated update. 
We assume that up to half, or $< r/2$, of $\mpcnodes$ are corrupted (\ie honest majority). 
We assume $\cor$ out of $m$ clients are corrupt, and they could collude with $\mpcnodes$.
Since we work in a setting with a large number of clients, we assume that the fraction of corrupt clients is small (\eg $10\%$), as is standard in the literature.
Our framework supports two models of corruption. In the first one, we assume the clients are maliciously corrupt, meaning that both the updates and the validation data could be maliciously crafted, while the $\mpcnodes$ are passively corrupt (semi-honest). The second model is the stronger one to defend against, where we assume $\mpcnodes$ are also maliciously corrupt.


\mypara{Adversary's knowledge.}
We consider a gray-box scenario where the malicious parties know everything about the protocols and parameters in the framework except 1) the benign clients' model update and private data, and 2) the randomness in the local computation of benign clients. 
In addition, each malicious client $\calC_i$ has access to a clean dataset $\bfD_i$ drawn from the underlying distribution---the dataset stored at the client before being compromised. 


\mypara{Adversary's goal.} 
First, the adversary attempts to reduce the utility of the honest parties in FL. In this paper, we mainly consider \emph{model poisoning}, in which the malicious clients attempt to maximize the loss of the global model at time $T$ by sending malformed model updates.
Let the first $m_c$ clients $\calC_1, \cdots, \calC_{m_c}$ be malicious without loss of generality. The adversarial goal is
\begin{equation}
\label{obj:adv}
    \max_{\bfu^{(t)}_i, i\in[m_c], t\in[T]}\ \ \  \calL(\bfw^{(T)}, \calD_{test}),
\end{equation}
where $\bfu^{(t)}_i$ is the malformed update submitted by $\calC_i, i\in [m_c]$ at time step $t$.
$\calD_{test}$ is the test distribution and $\calL(\bfw^{(T)}, \calD_{test})$ is the loss of the global model at the final round $T$ on the test distribution. 
Second, the malicious parties also want to undermine the privacy of honest parties by inferring as much information about the private data and/or the model updates of the honest clients.





\subsection{Our Goals}
\label{sec:goals}


\mypara{Robustness of the global model against adaptive attacks.}
In response to the adversarial goal in Equation~\ref{obj:adv}, we seek to preserve the integrity of the global model as specified in the following min-max objective,
\begin{align}
\label{obj:defense}
\min_{\calA}\max_{\bfu^{(t)}_i, i\in[m_c], t\in[T]}\ \ \  \calL(\bfw^{(T)}, \calD_{test}),
\end{align}
That is, we want to find an aggregation mechanism $\calA$ to minimize the test loss of the final global model $\bfw^{(T)}$ under the \textit{strongest} possible adversarial attack.
Specifically, assuming an adversary with knowledge of $\calA$ and control over local training data and local model updates on the malicious clients, one should consider the robustness against the strongest realizable attack devised considering the assumption. 
Accordingly, we introduce an aggregation protocol and report its evaluation result against its adaptive attack.


\mypara{Adaptability to Distribution Shift.} 
The underlying data
distribution of an FL task can change in common scenarios. New clients joining an FL protocol may have slightly different data distribution from existing clients, \eg in health care, new hospitals from a different geographic location with different patient demographics may want to join an existing federation. Clients already in the protocol may also see different data distribution over time, \eg a hospital's patient demographic may shift due to changes in the local population, disease outbreaks, or the introduction of new healthcare programs. 
Consider a global model converged with respect to an original client distribution $\calD_{old}$. 
Let $\calD_{new}$ represent the update distribution, incorporating both original and shifted client data.
Depending on how different $\calD_{new}$ is from $\calD_{old}$, the accuracy of the global model is bound to suffer for a period of time, or may not even be able to adapt to $\calD_{new}$. 
We address this aspect as \emph{adaptability} and report the global model accuracy on the test set sampled from the changing client data distribution over time. 
None of the prior works on robust secure aggregation baselines were designed or evaluated in this context, even though distribution shifts are ubiquitous in real-world FL deployments.
Particularly, the primary limitation of prior approaches stems from their reliance on public datasets for determining aggregation parameters~\cite{CCS:CGJv22}; in PPFL, maintaining a public dataset representative of the evolving data distribution is not straightforward. 
In contrast, our framework provides a way to take a reference of evolving client local data securely, hence naturally leading to adaptive aggregation results.


\mypara{Privacy of the client updates.} Our framework guarantees that even when up to half the aggregation committee and the clients are maliciously corrupted, nothing can be learned based on the \emph{intermediate values} observed during the computation, about the honest clients' updates beyond what is permissible by the algorithm by any other party in the protocol. 

\mypara{Correctness of secure aggregation.} Assuming the MPC protocol instantiated is secure with abort, we guarantee that if the protocol terminates without the adversary aborting, the final aggregate has been computed correctly.

