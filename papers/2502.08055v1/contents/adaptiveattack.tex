\section{Adaptive Attacks}
\label{sec:adaptiveattack}

In order to capture the constantly evolving real-world threats, we evaluate our framework against an \emph{adaptive} adversary which knows the robust aggregation and the check algorithms. Its objective can be characterized as,
\begin{align}
\label{obj:adaptiveadv}
    & \max_{\substack{\bfw^{(t)}_i, \ \bfD^{\msf{val}}_i, \\ \forall i\in[\cor], t\in[T]}}\ \ \  \calL(\bfw^{(T)}, \calD_{test}),
\end{align}
\ie the adversary can manipulate both the \emph{client updates} and the \emph{validation dataset} at the corrupted clients to maximize the loss of the global model on clean test data. 

\mypara{Malicious Model Updates.} The generation of malicious model updates can be formulated as a constrained optimization problem: the attacker searches for the updates that diverge maximally from the learner's goal subject to the constraints imposed by the defense mechanism. We adopt the attack framework in~\citet{fang2020local} and instantiate attacks for all defense baselines (see Appendix~\ref{app:adaptive-attack}).

\mypara{Malicious Validation Data.}
A new attack surface for \ours\ is that the adversary can also manipulate malicious clients' validation data (\ie $\bfD^{\msf{val}}_i, \forall i \in [\cor]$). 
We evaluate the robustness of \ours\ against this new threat by considering the most potent attacker---each malicious client $\calC_{adv}$ can \emph{arbitrarily} manipulate the check score if the validation data is provided by $\calC_{adv}$. 
We propose a strong manipulation strategy called the extreme manipulation and prove its optimal attack strength as the following. 

\begin{definition}[Extreme Check Score Manipulation]
\label{def:optimalattack}
When $\calC_{j}$ is malicious, a check score manipulation is \emph{extreme} if
\begin{equation}
\label{eqn:extreme}
\score{i}{j} =
\begin{cases}
1 &\mbox{ if $\calC_i$ is malicious,}\\
-1 &\mbox{ otherwise.}
\end{cases}
\end{equation}
\end{definition}

\begin{theorem}(Optimality of the Extreme Manipulation.)
\label{thm:optimal}
    Let $\bfw^{(T)}_{adv}$ be a model that maximizes the objective in Formula~\ref{obj:adaptiveadv} and is obtainable by some combination of malicious model updates and validation data set manipulation. Then there must be a set of malicious model updates that lead to $\bfw^{(T)}_{adv}$ under the extreme check score manipulation in Definition~\ref{def:optimalattack}.
\end{theorem}

\begin{proof}
    First, we observe that the range of $\scr(i,j)$ in our protocol is from $-1$ to $1$.
    Consider a malicious $\calC_i$. When $C_j$ is malicious, $\scr(i, j)$ equals 1, which is the largest possible. When $C_j$ is benign, $\scr(i, j)$ is independent to malicious data manipulation. As a result, $\scr_i$ for a malicious client $\calC_i$ under the extreme manipulation is no less than under other manipulation. Similarly, $\scr_i$ for a benign client under the extreme manipulation will be no more than that under other manipulation. Therefore, the ranking of $\scr_i$ for a malicious client $\calC_i$ will never drop when switching from a non-extreme manipulation to extreme manipulation.

    Let $\bfw_{adv}$ be a malicious model update from $\calC_i$ in the sequence that leads to $\bfw^{(T)}_{adv}$ under some non-extreme validation data manipulation.
    \begin{enumerate}
        \item
        If $\bfw_{adv}$ is included in aggregation, then $\scr_i$ must already be in the top $k\%$ among all clients, $k$ being the threshold defined in the protocol. Since switching to the extreme manipulation will never reduce the ranking of a malicious $\scr_i$, $\bfw_{adv}$ will remain in aggregation. 
        \item
        If $\bfw_{adv}$ is not included in aggregation, then the attacker can send any $\bfw'_{adv}$ that will be excluded from aggregation as well.~\footnote{If such a $\bfw'_{adv}$ does not exist, i.e., all updates from the malicious client will be accepted, then we allow the attacker to refrain from submitting the updates. The extreme manipulation serves to check the robustness lower bound of our work. Therefore, we allow it more flexibility, i.e. more attacker strength.}
    \end{enumerate}
    Therefore, an adversary using extreme manipulation can always construct malicious model updates that have the same impact in each aggregation step, and thus obtain $\bfw^{(T)}_{adv}$.
\end{proof}

\mypara{Robustness Lower Bound.} We acknowledge that in practice, the adversary may not find an actual validation set that achieves the conditions in Equation~\ref{eqn:extreme}. Nevertheless, we equip the adaptive attack against our framework with the extreme check score manipulation in our empirical study in Section~\ref{sec:experiment}.
We implement the extreme manipulation by allowing the adversary to directly set the value of $\scr(i, j)$ when $\calC_j$ is a malicious client. By giving this additional power to the adversary, we can find an empirical \emph{lower} bound to the robustness of our work. 
We also note that it is still possible for an attacker to craft validation sets to boost scores for malicious clients while lowering those of benign ones in reality. 
They may flip labels to disrupt benign updates or embed backdoors that trigger high scores for malicious updates.
Thus, our empirical lower bound serves as a practical robustness indicator.







