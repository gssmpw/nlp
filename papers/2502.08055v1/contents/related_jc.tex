\section{Related Work}
\label{sec:related}


\subsection{Robust Federated Learning}

\mypara{Robustness to Model Poisoning.} Model poisoning is one of the most fundamental threats to FL. As formulated by~\citet{blanchard2017byzantine} and~\citet{yin2018trimmedmean}, a set of Byzantine malicious clients can send manipulated model updates to the server to degrade the global model's performance.
To mitigate this, various defenses have been proposed, primarily through robust aggregation methods that aim to filter out malicious updates from the final aggregation~\cite{sun2019normbound, steinhardt2017normball, yin2018trimmedmean, blanchard2017krum, cao2020fltrust}.
However, these approaches often fall short against adaptive attacks~\cite{fang2020local,SP:LBVKH23} or require access to a public dataset for validation~\cite{cao2020fltrust}, which may not always be available.


\mypara{Robustness to Distribution Shift.}
Another key challenge in FL is handling evolving client data distribution over communication rounds. 
Many approaches address this from the perspective of client heterogeneity, incorporating techniques such as transfer learning, multi-task learning, and meta-learning to mitigate distribution shifts across different domains and tasks~\cite{smith2017federated, khodak2019adaptive}.
More recent works study explicitly tackles evolving client distributions over time~\cite{yoon2021federated} or periodic client distribution shifts~\cite{periodicshift}.


While these advances improve the robustness of FL, they are designed for standard (plaintext) FL, where model updates are exchanged in the clear. 
However, privacy-preserving federated learning (PPFL) introduces additional constraints, making it non-trivial to directly apply these techniques. 
Many existing robust FL methods rely on inspecting individual model updates, a process that is inherently difficult in PPFL due to encryption or secure aggregation mechanisms. 
Our work aims to bridge this gap by providing a unified framework for PPFL that enhances robustness against both model poisoning attacks and distribution shifts.




\subsection{Privacy-preserving Federated Learning}
Various techniques have been proposed for PPFL, with key differences in their underlying security assumptions.
One of the primary differentiators is whether a protocol relies on a single-server or a multi-server aggregator. 
Single-server protocols for secure aggregation include works like \cite{CCS:BBGLR20,SP:MWAPR23,rofl,CCS:CGJv22}, while \cite{SP:RSWP23,scionfl} represents some of the state-of-the-art approaches in the multi-server setting. These techniques typically involve one or more of the following methods: secret-sharing \cite{SP:MWAPR23}, homomorphic encryption (HE) \cite{CCS:BIKMMP17}, and zero-knowledge (ZK) \cite{rofl,CCS:CGJv22}. 


However, these methods generally focus on ensuring privacy and do not protect against adversarial behaviors such as model poisoning, as they do not validate client-submitted updates before aggregation.
To address this, recent research has explored incorporating zero-knowledge proofs (ZKPs) to enable privacy-preserving validation of model updates. 
While initial approaches were computationally expensive, advancements in ZK SNARKs, such as Bulletproofs~\cite{SP:BBBPWM18}, have significantly improved efficiency. 
ZK SNARKs offers small proof sizes and fast verification, making them particularly well-suited for FL, where clients often have bandwidth and computational constraints.


\input{contents/tables/alt-comp-table.tex}

\subsection{Robust and Privacy-preserving Federated Learning}

Among existing approaches that integrate robustness and privacy in FL, RoFL~\cite{rofl}, EIFFeL~\cite{CCS:CGJv22}, and ELSA~\cite{elsa} are the most relevant to our work.
They aim to enhance the robustness of PPFL under various threat models while integrating secure aggregation. 
A common strategy in these works is to ensure that only valid model updates contribute to the global model by using predicate-based validation, where clients prove the validity of their updates through ZKPs.

In contrast, our framework takes a different approach by leveraging secure multiparty computation (MPC) to compute useful robustness-related statistics on secret-shared data. 
This design allows for greater flexibility in defining validation checks, as opposed to relying solely on predefined ZKP predicates. 
In addition, \ours~allows the predicate to take multiple clients' private data as inputs, which enables cross-client validation that is neither feasible in plaintext FL nor in previous works. 
We compare with the features provided by some of the state-of-the-art frameworks in \tabref{new_comparison}.

\mypara{Secure Multiparty Computation (MPC).} 
Secure MPC is a well-established cryptographic technique that enables multiple parties to jointly compute a function over their private inputs while ensuring that no individual party learns anything beyond the output.
There are a range of applications for which MPC could be applied, such as secure auctions~\cite{FC:BCDGJK09}, privacy-preserving data analytics~\cite{prio,FC:BogTalWil12}, privacy-preserving secure training and inference~\cite{C:EGKRS20,CCS:MohRin18}, to name a few. 
In this work, the two building blocks we will use are protocols for secure inference and randomness generation.

