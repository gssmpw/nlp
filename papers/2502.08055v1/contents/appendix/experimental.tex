

\section{Experimental Details}
We repeat each setup for 10 trials (using random seeds 40-49) and report the average.
All the experiments are run on a server with thirty-two AMD EPYC 7313P 16-core processors, 528 GB of memory, and four Nvidia A100 GPUs. Each GPU has 80 GB of memory.

\input{contents_icml25/app_setup}


\subsection{Soft $\score{}{}$ with Confidence Score}
\label{app:softscore}
We highlight that the computation of $\score{}{}$ is not limited to the accuracy difference in Equation~\ref{eqn:check_acc}. Rather,  one could compute other functions such as the maximum softmax value:
\begin{align}
\label{eqn:check_prob}
    &\score{i}{j} = \msf{Chk}_{prob}(\bfw_i, \bfD^{\msf{val}}_j) , \\ \nonumber
    &\msf{Chk}_{prob}(\bfw, \bfD) = \frac{1}{|\bfD|} \sum_{(\bfx_k, y_k) \in \bfD} \max_l ~ \bff(\bfx_k; \bfw)
\end{align}
where $\bff(\bfx_k; \bfw) \in \R^L$ is the predicted softmax probability over $L$ classes.
The above scoring function gauges the confidence of the model's predictions on the given validation data. Intuitively, given an uncorrupted set of data, which is the majority case, a benign model would have high confidence while a corrupted model would output low confidence on their predictions~\cite{hendrycks2022msp, hendrycks2022scaling}.

\subsection{Elaboration on Adaptive Attacks}
\label{app:adaptive-attack}
Assume the first $\cor$ out of $m$ clients are malicious without loss of generality. Then the adaptive local model poisoning attack aims to optimize 
\begin{align}    
\label{eqn:adaptive-attack}
&\max_{\bfw'_1,\cdots,\bfw'_{\cor}} \bfs^{T}(\bfw - \bfw')\\ \nonumber
\text{subject to} ~&\bfw = \calA(\bfw_1, \cdots, \bfw_{\cor}, \bfw_{\cor+1} \cdots, \bfw_{m})\\ \nonumber
&\bfw' = \calA(\bfw'_1, \cdots, \bfw'_{\cor}, \bfw_{\cor+1} \cdots, \bfw_{m})
\end{align}
where $\bfw_i$ is the clean local model under no attack from client $i$, $\bfw'_{i}$ is the compromised model from client $i$, and $\bfs$ represents the sign of change of clean global model in the current iteration under no attack.
According to~\cite{fang2020local} the solution can be found heuristically by solving
\begin{align}
\label{eqn:adaptive-attack-approx}
&\max_{\lambda} \ \ \  \lambda \\ \nonumber
\text{subject to} ~&\bfw'_1 = \bfw - \lambda\bfs, \\ \nonumber
&\bfw'_i = \bfw'_1 \mbox{ and } \pmb{\alpha}_i=1, \forall i\in [\cor]\\ \nonumber
\end{align}

In this heuristics, the malicious clients may collude to send the same poisoning model or support each other to be included in the final aggregation, which pull the aggregate model update in the opposite direction under no attack. The adversary wants to find the largest magnitude $\lambda$ of moving in the opposite direction such that the poisoning models $\bfw'_i$s are still included in the final aggregation.

\mypara{Adversary's Knowledge.}
In order to solve the optimization problem in Eqn.~\ref{eqn:adaptive-attack}, the adversary can make the malicious clients collude with each other and have access to their local training data, local model updates, loss function, and learning rate on the malicious clients, but not on the benign clients.
It also knows the entire defense mechanism and any thresholds used by the mechanism; \ie in our proposed aggregation protocol, we accept the top 90\% client models ranked by their validation scores (in the case of 10\% corruption). 
In RoFL, EIFFeL, or Elsa, for example, threshold parameters used in their validation predicate are also known as they are broadcast to everyone at the start of every round. On the contrary, the threshold values cannot be revealed to anyone in ours as they are hidden inside the MPC node.
Accordingly, they do not know $\bfs$, $\bfw$, and $\bfwâ€™$ directly, but can only estimate them by colluding with each other. 
This corresponds to \textit{partial knowledge} in Section 3 of \citet{fang2020local}. The adversary approximates $\bfs$ using the before-attack local updates on the malicious clients at each communication round.

We also note that the adversary does \emph{not} know 1) the exact data used for validation, and 2) the exact integrity check result list $\calV$ because 1) is our privacy goal and 2) $\calV$ is only computed after the malicious clients stage their model update.


\mypara{Adaptive Attack to Norm bound and Norm ball.}
We can derive closed-form solutions from Eqn.~\ref{eqn:adaptive-attack} for norm bound and norm ball in the simplest case.
The objective in Eqn.~\ref{eqn:adaptive-attack} can be rewritten as follows: assuming a basic mean aggregation for $\calA$, and a robust validation check $\msf{Chk}(\bfw)$,
\begin{align*}
\bfs^T (\bfw - \bfw') 
  &= \bfs^T \Biggl\{ \left( \sum_{i=1}^{\cor} \bfw_i\, \indicator{\msf{Chk}(\bfw_i) = 1} 
           + \sum_{i=\cor+1}^{m} \bfw_i\, \indicator{\msf{Chk}(\bfw_i) = 1} \right) \\
  &\quad\; - \left( \sum_{i=1}^{\cor} \bfw'_i\, \indicator{\msf{Chk}(\bfw'_i) = 1} 
           + \sum_{i=\cor+1}^{m} \bfw_i\, \indicator{\msf{Chk}(\bfw_i) = 1} \right) \Biggr\} \\
  &= \bfs^T \left( \sum_{i=1}^{\cor} \bfw_i\, \indicator{\msf{Chk}(\bfw_i) = 1} 
           - \sum_{i=1}^{\cor} \bfw'_i\, \indicator{\msf{Chk}(\bfw'_i) = 1} \right)
\end{align*}
Since the first term is constant independent to $\bfw'_1, \dots, \bfw'_{\cor}$, now the objective becomes,
\begin{align*}
&\min_{\bfw'_1,\cdots,\bfw'_{\cor}} \bfs^{T} \sum_{i=1}^{\cor} \bfw'_i \indicator{\msf{Chk}(\bfw'_i) = 1} \\ \nonumber
\text{subject to} ~&\bfw = \calA(\bfw_1, \cdots, \bfw_{\cor}, \bfw_{\cor+1} \cdots, \bfw_{m})\\ \nonumber
&\bfw' = \calA(\bfw'_1, \cdots, \bfw'_{\cor}, \bfw_{\cor+1} \cdots, \bfw_{m})
\end{align*}

For norm bound, we further constrain that $||\bfw'_i||_2 < \tau, \forall i \in [\cor]$ so that malicious updates always pass the validity check and are included in the aggregation.
Then we obtain $\bfw'_i = - \bfs \cdot \frac{\tau - \epsilon}{\sqrt{d}}$ where $||\bfs||_2 = \sqrt{d}$ and $\epsilon$ is some positive small value (\eg $10^{-6}$) to ensure the inequality constraint.

For norm ball, we further constrain that $||\bfw'_i - \bfw_{pubval}|| < \tau, \forall i \in [\cor]$. 
We obtain $\bfw'_i = \bfw_{pubval} + \Delta \bfw'_i$ where $\Delta \bfw'_i = -\bfs \cdot \min (\tau - \epsilon, \frac{\tau - \epsilon}{\sqrt{d}})$.

\mypara{Adaptive Attack to Cosine similarity.}
We follow the instantiation of adaptive attack in \citet{fltrust} under partial knowledge setup.

\mypara{Adaptive Attack to \ours.}
To check if the poisoning models $\bfw'_{i}$s are still included in the aggregate, the adversary will use its own dataset to estimate the accuracy of its poisoning models as well as the benign models. Let $\bfD'$ denote the dataset stored at the malicious clients. The adversary computes the accuracy of all benign models $\bfw_{\cor+1}, \cdots, \bfw_{m}$ on $\bfD'$. Then for each $\lambda$ and its associated poisoning model $\bfw_i' = \bfw - \lambda\bfs$, the adversary evaluate its accuracy on $\bfD'$ and only accept $\lambda$ if the accuracy enters the top 50\% among the accuracy of benign models. We sample $\lambda\in\{1e^{-5}, 1e^{-1}\}$ and choose the largest accepted $\lambda$ to construct poisoning models.

\subsection{Proof: Optimality of Extreme Manipulation}
\label{app:extrememanipultationproof}

\begin{definition}[Extreme Check Score Manipulation]
\label{def:optimalattack}
When $\calC_{j}$ is malicious, a check score manipulation is \emph{extreme} if
\begin{equation}
\label{eqn:extreme}
\score{i}{j} =
\begin{cases}
1 &\mbox{ if $\calC_i$ is malicious,}\\
-1 &\mbox{ otherwise.}
\end{cases}
\end{equation}
\end{definition}



We show in Theorem~\ref{thm:optimal} the optimality of the check score manipulation in Definition~\ref{def:optimalattack} in the sense that such manipulation will also allow some malicious model updates to maximize the adversarial objective. 
\begin{theorem}(Optimality of the Extreme Manipulation.)
\label{thm:optimal}
    Let $\bfw^{(T)}_{adv}$ be a model that maximizes the objective in Formula~\ref{obj:adaptiveadv} and is obtainable by some combination of malicious model updates and validation data set manipulation. Then there must be a set of malicious model updates that lead to $\bfw^{(T)}_{adv}$ under the extreme check score manipulation in Definition~\ref{def:optimalattack}.
\end{theorem}



\begin{proof}
    First, we observe that the range of $\scr(i,j)$ in our protocol is from $-1$ to $1$.
    Consider a malicious $\calC_i$. When $C_j$ is malicious, $\scr(i, j)$ equals 1, which is the largest possible. When $C_j$ is benign, $\scr(i, j)$ is independent to malicious data manipulation. As a result, $\scr_i$ for a malicious client $\calC_i$ under the extreme manipulation is no less than under other manipulation. Similarly, $\scr_i$ for a benign client under the extreme manipulation will be no more than that under other manipulation. Therefore, the ranking of $\scr_i$ for a malicious client $\calC_i$ will never drop when switching from a non-extreme manipulation to extreme manipulation.

    Let $\bfw_{adv}$ be a malicious model update from $\calC_i$ in the sequence that leads to $\bfw^{(T)}_{adv}$ under some non-extreme validation data manipulation.
    \begin{enumerate}
        \item
        If $\bfw_{adv}$ is included in aggregation, then $\scr_i$ must already be in the top $k\%$ among all clients, $k$ being the threshold defined in the protocol. Since switching to the extreme manipulation will never reduce the ranking of a malicious $\scr_i$, $\bfw_{adv}$ will remain in aggregation. 
        \item
        If $\bfw_{adv}$ is not included in aggregation, then the attacker can send any $\bfw'_{adv}$ that will be excluded from aggregation as well.~\footnote{If such a $\bfw'_{adv}$ does not exist, i.e., all updates from the malicious client will be accepted, then we allow the attacker to refrain from submitting the updates. The extreme manipulation serves to check the robustness lower bound of our work. Therefore, we allow it more flexibility, i.e. more attacker strength.}
    \end{enumerate}
    Therefore, an adversary using extreme manipulation can always construct malicious model updates that have the same impact in each aggregation step, and thus obtain $\bfw^{(T)}_{adv}$.
\end{proof}

\mypara{Robustness Lower Bound.} We acknowledge that in practice, the adversary may not find an actual validation set that achieves the conditions in Equation~\ref{eqn:extreme}. Nevertheless, we equip the adaptive attack against our framework with the extreme check score manipulation in our empirical study in Section~\ref{sec:experiment}.
We implement the extreme manipulation by allowing the adversary to directly set the value of $\scr(i, j)$ when $\calC_j$ is a malicious client. By giving this additional power to the adversary, we can find an empirical \emph{lower} bound to the robustness of our work. 
We also note that it is still possible for an attacker to craft validation sets to boost scores for malicious clients while lowering those of benign ones in reality. 
They may flip labels to disrupt benign updates or embed backdoors that trigger high scores for malicious updates.
Thus, our empirical lower bound serves as a practical robustness indicator.
