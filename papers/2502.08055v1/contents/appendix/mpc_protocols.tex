\section{Details of $\PSecAgg$ and $\PCheck$}
\label{app:mpcdetails}
We found the best performance of our framework when we used both a norm bound and the accuracy function as ways to achieve robustness, as shown in \secref{experiment}. 
Both the checks can be instantiated from off-the-shelf MPC protocols, owing to their use of standard building blocks. We combine both the checks by carefully using the subprotocols required. We use subprotocols for secure inference, multiplication, oblivious sorting, while assuming a shared key setup. There has been a lot of interest in proposing efficient instantiations of sorting and secure inference in the recent years~\cite{CCS:AFOPRT21,ruffle,tetrad,CCS:MohRin18,USENIX:DalEscKel21,EPRINT:KelSun22}, offering protocols at various levels of security. The only restriction we have in terms of using these different frameworks is that, we require the secret-sharing to have an access structure such that the honest parties can reconstruct the secret. The variants of replicated secret-sharing \cite{EPRINT:FLNW16} that have been used in the recent PPML works, including the ones mentioned above, satisfy these criteria. We refer the readers to these, along with MP-SPDZ~\cite{EPRINT:Keller20} for detailed descriptions of the input sharing, $\PShare$, and reconstruction protocols, $\PRec$. For completeness, we provide the functionality for using the maximum softmax value in \appref{proofs}, but we do not report numbers from experiments with this function.

As mentioned earlier, our secure aggregation protocol assumes there exists a set of parties that can run MPC, denoted by $\mpcnodes$. For ease of description, one could assume that $\mpcnodes$ constitutes the servers that want to train the FL model together, as is the case in multi-server FL. However, it is possible to form an aggregation committee by sampling from the clients, as shown in works such as \cite{SP:MWAPR23,CCS:BBGLR20}. We leave it as a potential future direction to integrate our check into a protocol with a changing set of parties constituting $\mpcnodes$. 
We assume that the nodes run a Setup phase, which establishes shared PRF keys between every pair of parties, as well as a common PRF key between all of them. These keys can be used to generate common random values by keeping track of a counter, and incrementing it every time the function is called. In every round, $\mpcnodes$ use the keys to generate a set of $2 \cor + 1$ random IDs for each client, $\calC_i$, $i \in \{1, m\}$. Each client's model will be checked against validation data from the set of random IDs assigned to it.

In addition to the features we strive to achieve, such as not relying on public datasets, we also placed an emphasis on making the MPC implementation-friendly. Since secure inference is often part of the system when training an FL model, reusing that building block to also perform a robustness check makes the system easier to deploy. We describe high level details of the protocol below.

Each round $t$, begins with each client $\calC_i$, for $i \in [1, m]$, secret-sharing its model update to $\mpcnodes$ using $\PShare$. $\mpcnodes$ call $\FSecInf$ (\figref{fsecinf}) with the client's model, the global model from the previous round, and each of the validation datasets in $\calC_{check, i}$. As mentioned before, there are $2 \cor + 1$ client datasets against which the model is checked, where $\cor$ is the total number of corrupted clients. 

$\mpcnodes$ receive $\arith{\Acc{i}{\msf{new}, j}}$, for $j \in \calC_{check, i}$ and $\arith{\Acc{i}{\msf{prev}, j}}$, which denote the accuracy of the client $\calC_i$'s model and the global model of the previous round, against the validation client's data respectively. The difference between the two accuracies is denoted by $\diff{i}{j}$. These values are then sorted by calling $\FSort$ (\figref{fsort}), which sorts the array obliviously. $\mpcnodes$ then compute the trimmed mean of this vector. Since it is known to all parties that they need to trim the top and bottom $m_c/2$ elements from this array, they can do the computation locally and compute the mean of the remaining ones. The result of the trimmed mean is denoted by $\score{i}{}$ for each client $\calC_i$.

$\mpcnodes$ compute the score for each client, to get a secret-sharing of the list of scores, $\{ \score{i}{} \}_{i \in m}$. In order to eliminate the lowest-performing updates, the parties sort the scores and select the top $k \%$ of them, where $k$ is a parameter picked by the server at the start of the protocol, that is known to all the parties.

In parallel, $\mpcnodes$ also compute the norm of the model updates received, and set the bound to be $\lambda$ times the median of all norm values of the updates. Here $\lambda$ is a non-zero constant to control the tightness of the bound. The final updates to be aggregated are the ones that pass both the accuracy check, and the norm bound check.

$\mpcnodes$ then use $\PRec$ to reconstruct the aggregated update, $\bfu_{\msf{aggr}}$. The formal description of the protocol appears in \figref{pisecagg}. In order to use $\FMaxSoft$, we only need to replace the calls to $\FSecInf$ with $\FMaxSoft$, in \figref{pisecagg}.
