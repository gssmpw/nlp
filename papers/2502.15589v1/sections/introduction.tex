
\section{Introduction}

\begin{figure}[!t] % !t, !htbp
    \centering
    \scalebox{0.8}{
    \includegraphics[width=1\linewidth]{figures/intro-v1.pdf} 
    }
    % \vspace{-1mm}
    \caption{
    (a) A CoT case. Tokens highlighted in yellow represent critical reasoning tokens, while the remaining tokens primarily ensure fluency. 
    Humans typically only write the yellow parts when solving this problem.
    (b) Comparison of reasoning between Vanilla and \ours. 
    ``C Ti'' denotes the i-th compressed thought representation, and we illustrate the semantic information potentially expressed after compression.
    }
    \label{fig:intro}
    \vspace{-1mm}
\end{figure} 

% [Original] Recently, Large Language Models (LLMs) have demonstrated remarkable capabilities in complex reasoning tasks.
Recent advancements in Large Language Models (LLMs) have demonstrated their remarkable capabilities in complex reasoning tasks. 
As research in this domain progresses, the reasoning patterns of these models have gradually evolved from ``fast thinking'' to ``slow thinking''. 
% [Original] Chain-of-Thought (CoT)~\citep{nips22_cot} prompting, which decomposes complex problems into multiple sub-steps and solves them sequentially, has significantly enhanced the reasoning abilities of models. 
This transition is exemplified by methods such as Chain-of-Thought (CoT)~\citep{nips22_cot} prompting, which enhances reasoning by breaking down complex problems into sequential sub-steps.  
% [Original] Furthermore
Building on this, the \textit{o1-like thinking mode}~\citep{arixv24_o1,arxiv24_qwq,arxiv25_deepseek_r1} introduces multiple reasoning abilities such as trial-and-error, backtracking, correction, and iteration, further improving the success rate of models in solving complex problems. 
However, this performance improvement comes at the cost of generating a large number of tokens~\citep{arxiv24_o1_study}. 
Given that current LLMs are predominantly based on the Transformer architecture~\citep{nips17_transformer}, the computational complexity of the attention mechanism grows quadratically with the context length, while the storage overhead of the KV Cache increases linearly with the context length.
% [Original] (e.g., for Qwen32B~\citep{arxiv24_qwen2_5}, when the context length reaches \(10^4\), the KV Cache occupies a space comparable to the model itself). 
For example, in the case of Qwen32B~\citep{arxiv24_qwen2_5}, when the context length reaches \(10^4\), the KV Cache occupies a space comparable to the model itself. 
Consequently, the increase in token generation leads to a sharp rise in memory overhead and computational costs, severely limiting the practical efficiency of LLMs in long-text generation and complex reasoning tasks.

% To mitigate this issue, two main approaches have been proposed: 
% The first approach involves using prompt engineering~\citep{arxiv24_tale,arxiv24_break_the_chain,arxiv24_concise_thoughts} or training~\citep{nips24_skip_steps,arxiv24_c3ot,arxiv25_related_work_rl1,arxiv25_o1_pruner,arxiv24_ccot,arxiv24_coconut} to guide LLMs to generate fewer even zero~\citep{arxiv23_kd_cot,arxiv24_icot} tokens during reasoning, which does not require additional intervention during inference. 
% The second approach~\citep{nips23_h2o,arxiv24_sepllm}, which does not require additional training, reduces memory usage by retaining important parts of the KV Cache and discarding less important parts during inference, necessitating real-time intervention token-by-token. 
% note that this does not include works that manage the KV Cache only during the Prefill phase, such as SnapKV
% However, the first approach often requires iterative refinement and relies on carefully constructed data, while the second approach significantly increases inference latency due to the need for importance assessment at each token generation step.
% \zyq{Another way to describe for reference(Below)}
To mitigate this issue, two main approaches have been proposed, primarily differentiated by their intervention requirements during inference.
The first category requires no additional intervention during inference, achieving efficiency through prompt engineering~\citep{arxiv24_tale,arxiv24_break_the_chain,arxiv24_concise_thoughts} or specialized training~\citep{nips24_skip_steps,arxiv24_c3ot,arxiv25_related_work_rl1,arxiv25_o1_pruner,arxiv24_ccot,arxiv24_coconut} to guide LLMs in generating fewer or even zero~\citep{arxiv23_kd_cot,arxiv24_icot} intermediate tokens during reasoning. % \zyq{If appropriate} 
The second category operates through real-time token-by-token intervention during inference~\citep{nips23_h2o,arxiv24_sepllm}, reducing memory usage by selectively retaining important parts of the KV Cache while discarding less critical ones.
However, both approaches face distinct challenges: the first typically requires careful data construction and iterative refinement, while the second introduces substantial inference latency due to the computational overhead of token-wise importance assessment.

In this work, we propose a new approach by training LLMs to dynamically compress historical content during reasoning. 
Our motivation stems from two observations: 
1) Tokens generated by the LLM serve dual purposes: 
ensuring linguistic fluency and facilitating actual reasoning (as highlighted in yellow in Figure~\ref{fig:intro}(a)), making compression feasible. 
2) When humans solve problems similar to the one in Figure~\ref{fig:intro}(a), they typically write only key steps (highlighted in yellow), while storing the rest of the thought process mentally.
% [Origin] while the rest of the thought process is stored in the brain.

Based on these insights, we introduce \ours, a method that dynamically compresses intermediate thoughts during generation. 
As illustrated in Figure~\ref{fig:intro}(b), after generating a lengthy thought step (e.g., \texttt{Thought i}), it is compressed into a compact representation (e.g., \texttt{C Ti}), and the original thought chain is discarded, with reasoning continuing based solely on the compressed content. 
This approach significantly reduces the number of tokens stored in the context window, thereby lowering memory overhead and computational costs.

In practice, we train the LLM to learn when and how to compress. 
Specifically, we construct data to teach the model when to compress; the hidden states of the thoughts to be compressed are reduced to a set of hidden states corresponding to a small number of special tokens (i.e., gist tokens~\citep{nips23_gist}).
% [Original], and through carefully designed attention masks, the LLM learns how to compress and how to continue generating based on the compressed content. 
Through carefully designed attention masks, the LLM then learns how to compress and how to continue generating based on the compressed content.
{To quantify the amount of information used during reasoning, we further propose the Dependency (Dep) metric, defined as the total number of historical tokens each generated token depends on (see Figure~\ref{fig:exp:metric}). 
This metric effectively measures the degree of compression, with a lower Dep value indicating reduced reliance on the original long context and more significant compression.}

We conduct extensive experiments across four datasets using two different models. 
The results indicate that, with the Qwen model, \ours~reduces the peak token usage by 70\% and decreases inference time by 26\% compared to the Vanilla model, while maintaining comparable accuracy (with only a 1\% drop).
Our contributions are as follows:
1) We propose \ours, a method that dynamically compresses thought chains during reasoning, significantly reducing memory overhead and inference time.
2) We introduce the Dependency (Dep) metric to measure the compression ratio and the amount of information used during reasoning.
3) We demonstrate that \ours~achieves a good balance between reasoning efficiency and accuracy, offering new insights for future LLM inference acceleration.