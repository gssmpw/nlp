% \vspace{-1mm}
\section{Conclusion}
In this paper, we present \ours, a new approach to enhance the efficiency of LLMs in complex reasoning tasks by dynamically compressing intermediate thoughts during generation. 
By training the LLM to learn when and how to compress verbose thought steps into compact representations, \ours~significantly reduces memory overhead and computational costs while maintaining competitive accuracy. 
We introduce the \textit{Dependency} (abbr., Dep) metric to quantify the degree of compression across different accelerating methods.
% providing a principled way to evaluate the reliance on historical tokens during reasoning. 
Extensive experiments demonstrate that \ours~is an effective approach to balancing efficiency and performance. 

% Better Split Function

% Dynamic Compression Size

% Instruction Compression if need

% MoE
