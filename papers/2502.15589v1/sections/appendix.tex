
\clearpage
\appendix

\section*{Appendix}
\section{Metric: \texttt{Dependency}}
\label{sec:app:metric}
\begin{figure}[!htbp] % !t, !htbp
    \centering
    \scalebox{1}{
    \includegraphics[width=1.0\linewidth]{figures/metric-cal-v1.pdf} 
    }
    % \vspace{-1mm}
    \caption{Illustration of the metric Dependency.}
    \label{fig:app:metric}
    % \vspace{-3mm}
\end{figure} 
\subsection{Motivation}
\ours~and AnLLM~\citep{acl24_anllm} are dynamic compression methods, meaning the number of compressions and the compression ratio are determined by the LLM itself rather than being predefined hyperparameters. 
In contrast, H2O~\citep{nips23_h2o} and SepLLM~\citep{arxiv24_sepllm} allow users to set hyperparameters to control the maximum number of tokens retained during inference. 
This fundamental difference makes it challenging to directly and fairly compare dynamic compression methods like \ours~and AnLLM with KV cache compression approaches like H2O and SepLLM.  

Traditionally, KV cache compression methods are compared by setting the same maximum peak token count, but this metric becomes inadequate in our context. 
As illustrated in Figure~\ref{fig:app:metric}, which shows the relationship between generated tokens and context length for Vanilla, H2O, and \ours, 
\ours~occasionally exceeds H2O in peak token count. 
However, this metric is misleading because \ours's peak memory usage occurs only momentarily, while H2O maintains a consistently high token count over time.  

Moreover, previous KV cache compression methods often compress prompt parts only and assume a fixed prompt length, allowing compression ratios to be predefined. 
In our setting, however, the output is also needed to be compressed.
The output token count is unknown, making it impossible to preset a global compression ratio. 
Consequently, relying solely on maximum peak token count as a comparison metric is insufficient.  

To address these challenges, we propose a new metric called \textit{Dependency}, which quantifies the total amount of information dependencies during the generation process. 
This metric enables fair comparisons between dynamic compression methods and traditional KV cache compression approaches by ensuring evaluations are conducted under similar effective compression ratios.  
\subsection{Definition}
We introduce the \textbf{Dependency} (abbr., Dep) metric, defined as the sum of dependencies of each generated token on previous tokens during the generation of an output. 
Geometrically, it represents the area under the curve in Figure~\ref{fig:app:metric}. 
Dependency can be calculated either from its definition or through its geometric interpretation. 
Here, we focus on the geometric approach. 
Let the initial prompt length be \( L_P \), the model's output length be \( L_O \), and the maximum context length set by KV cache compression methods be \( L_C \).  

\textbf{Dependency for Vanilla.}
The area under Vanilla's curve forms a right trapezoid, calculated as:  
\[
\begin{aligned}
\texttt{Dependency} &= \frac{(L_P + L_P + L_O) \times L_O}{2} \\
&= \frac{{L_O}^2}{2} + L_P \times L_O
\end{aligned}
\]

\textbf{Dependency for H2O.}
The area under H2O's curve consists of a trapezoid (left part in Figure~\ref{fig:app:metric}(b)) and a rectangle (right part in Figure~\ref{fig:app:metric}(b)):  
\[
\begin{aligned}
S_\texttt{Trapezoid} &= \frac{(L_P + L_C) \times (L_C - L_P)}{2} \\
S_\texttt{rectangle} &= L_C \times (L_O - L_C + L_P) \\
\texttt{Dependency} &= S_\texttt{Trapezoid} + S_\texttt{rectangle} \\
&= \frac{2L_PL_C + 2L_OL_C - {L_P}^2 - {L_C}^2}{2}
\end{aligned}
\]

\textbf{Dependency for \ours~and AnLLM.}
For \ours~and AnLLM, Dependency does not have a closed-form solution and must be computed iteratively based on its definition. 

\subsection{Application}
\textbf{Value of Dependency.}
A higher Dependency value indicates that more tokens need to be considered during generation, reflecting greater information usage.
Conversely, a lower Dependency value suggests a higher effective compression ratio.  

\textbf{Dependency Ratio.}
By dividing the Dependency of an accelerated method by that of Vanilla, we obtain the compression ratio relative to Vanilla. For example, in Table~\ref{table:exp_main}'s ``Avg.'' column, Vanilla's Dependency is 16.6M, H2O's is 4.4M, and \ours's is 3.7M. 
Thus, H2O achieves a compression ratio of \( \frac{16.6}{4.4} \approx 3.8 \), while \ours~achieves \( \frac{16.6}{3.7} \approx 4.5 \).  

This metric provides a unified framework for evaluating both dynamic and static compression methods, ensuring fair and meaningful comparisons.

% \section{Inference Pseudocode}


\begin{figure}[!htbp] % !t, !htbp
    \centering
    \scalebox{1}{
    \includegraphics[width=1.0\linewidth]{figures/attention_mode-v1.pdf} 
    }
    % \vspace{-1mm}
    \caption{Illustration of Attention Mask in Table~\ref{table:exp:ablation:attention}.}
    \label{fig:app:attention_mode}
    % \vspace{-3mm}
\end{figure} 

\section{Experiment}
The code and data will be released at \url{https://github.com/zjunlp/LightThinker}.
\label{sec:app:exp}
\subsection{Training Data}
\label{sec:app:exp:train_data_case}
Examples of training samples are shown in Figure~\ref{prompt:case:train}.

\subsection{Baseline Details}
\label{sec:app:exp:baseline_details}
\textbf{H2O}~\citep{nips23_h2o} is a training-free acceleration method that greedily retains tokens with the highest cumulative attention values from historical tokens. 
It includes two hyper-parameters: the maximum number of tokens and the current window size (i.e., \texttt{local\_size}). 
The maximum number of tokens for each task is listed in the ``Peak'' column of Table~\ref{table:exp_main}, and the \texttt{local\_size} is set to half of the maximum number of tokens. 
The experimental code is implemented based on \url{https://github.com/meta-llama/llama-cookbook}.

\textbf{SepLLM}~\citep{arxiv24_sepllm} is another training-free acceleration method that considers tokens at punctuation positions as more important. 
It includes four parameters: the maximum number of tokens is set to 1024, \texttt{local\_size} is set to 256, \texttt{sep\_cache\_size} is set to 64, and \texttt{init\_cache\_size} is set to 384. 
We also tried another set of parameters (\texttt{init\_cache\_size}=4, \texttt{sep\_cache\_size}=64, \texttt{local\_size}=720, maximum number of tokens=1024), but found that the first set of parameters performed slightly better.

\textbf{AnLLM}~\citep{acl24_anllm} is a training-based method that shares a similar overall approach with \ours~but accelerates by saving historical content in anchor tokens. 
The specific differences between the two are detailed in Section~\ref{sec:method:discussion}.

\subsection{Training Details}
\label{sec:app:exp:training_details}
Both \textbf{Vanilla} and \textbf{AnLLM} are trained on the B17K~\citep{bespoke_stratos_train_dataset} dataset using the R1-Distill~\citep{arxiv25_deepseek_r1} model for 5 epochs, while \ours~is trained for 6 epochs. 
The maximum length is set to 4096, and a cosine warmup strategy is adopted with a \texttt{warmup\_ratio} of 0.05. 
Experiments are conducted on 4 A800 GPUs with DeepSpeed ZeRo3 offload enabled. 
The batch size per GPU is set to 5, and the gradient accumulation step is set to 4, resulting in a global batch size of 80. 
The learning rate for Vanilla is set to 1e-5, while for AnLLM and \ours, it is set to 2e-5.

\subsection{Evaluation Details}
\label{sec:app:exp:evaluation_details}
For the CoT in Table~\ref{table:exp_main}, the prompts used are shown in Figure~\ref{prompt:system:base} and Figure~\ref{prompt:task:base}. 
For the R1-Distill model, no system prompt is used, and the task-specific prompts are shown in Figure~\ref{prompt:task:r1}. 
Vanilla, H2O, SepLLM, AnLLM, and \ours~share the same set of prompts, with the system prompt shown in Figure~\ref{prompt:system:vanilla} and downstream task prompts shown in Figure~\ref{prompt:task:r1}. 
The options for MMLU~\citep{iclr21_mmlu} and GPQA~\citep{colm24_gpqa} multiple-choice questions are randomized.

\subsection{Additional Results}
\label{sec:app:exp:additional_results}
Figure~\ref{fig:app:token} compares the number of tokens generated by two models across different datasets. 
Figure~\ref{fig:app:frequency} shows the distribution of compressed lengths for LightThinker on two models and four datasets. Figure~\ref{fig:app:attention_mode} illustrates the attention masks for the baselines in Table~\ref{table:exp:ablation:attention}.
Figure~\ref{prompt:case} shows a complete case in Figure~\ref{fig:exp:case}.

\begin{figure*}[!htbp] % !t, !htbp
    \centering
    \scalebox{0.8}{
    \includegraphics[width=1.0\linewidth]{figures/token_split-v2.pdf} 
    }
    % \vspace{-1mm}
    \caption{Average number of generated tokens.}
    \label{fig:app:token}
    % \vspace{-3mm}
\end{figure*} 

\input{sections/related_work}

\section{Discussions}
\label{sec:app:discussion}
\textbf{Viewing \ours~from Other Perspectives.}
In previous sections, we design \ours~from a compression perspective. 
Here, we further discuss it from the perspectives of \textit{Memory} and \textit{KV Cache Compression}, where KV Cache can be viewed as a form of LLM work memory.
In Memory perspective, \ours's workflow can be summarized as follows: it first performs autoregressive reasoning, then stores key information from the reasoning process as memory (memory), and continues reasoning based on the memorized content. 
Thus, the information in the cache tokens acts as a compact memory, though it is only effective for the current LLM and lacks transferability.
In KV Cache Compression perspective,
Unlike methods such as H2O~\citep{nips23_h2o}, which rely on manually designed eviction policy to select important tokens, \ours~merges previous tokens in a continuous space, \textit{ceating} new representations. 
The content and manner of merging are autonomously determined by the LLM, rather than being a discrete selection process.

% \citet{emnlp24_onegen}

% \citeauthor{emnlp24_onegen}

\begin{figure*}[!htbp] % !t, !htbp
    \centering
    \scalebox{1}{
    \includegraphics[width=0.8\linewidth]{figures/frequency_split.pdf} 
    }
    % \vspace{-1mm}
    \caption{Token compression frequency distribution for \ours.}
    \label{fig:app:frequency}
    % \vspace{-3mm}
\end{figure*} 

\input{prompts/system_prompt_base_model}
\input{prompts/system_prompt_b17k}
\input{prompts/prompt_r1_model}
\input{prompts/prompt_base_model}
\input{prompts/training_case}
\input{prompts/case}


% \section{Future Work}
