

\section{Methodology}
We propose \textbf{\ours}, a method designed to accelerate the reasoning process of LLMs, as illustrated in Figure~\ref{fig:method}.
The core idea is to \textit{train LLMs to dynamically compress the current thought during reasoning}, enabling subsequent generation to be based on the compact compressed content rather than the original long thought.

\subsection{Overview}

\textbf{Notation.}
For clarity, we define the following notations.
Lowercase letters, such as $x_i$, denote a single token.
Uppercase letters, such as $X$, denote sequences of tokens.
The notation `$\texttt{[·]}$' denotes a special token, such as `$\texttt{[c]}$', while `$\texttt{<·>}$' denotes an optional special token, such as `$\texttt{<w>}$'.
The \textit{o1-like thinking mode} dataset $\mathcal{D}=\{(X,Y)_i\}_{i=1}^{|\mathcal{D}|}$ consists of $|\mathcal{D}|$ samples, where $X=\{x_i\}_{i=1}^{|X|}$ represents a question, and $Y=\{y_i\}_{i=1}^{|Y|}$ represents the corresponding thought and final answer. 
Recent works~\citep{arxiv25_openthoughts,arxiv25_deepseek_r1} show that SFT on $\mathcal{D}$ significantly enhances LLM reasoning capabilities.

\textbf{Design.}
% [Originanl] To achieve this idea, we address the following two key questions:
% \underline{\textit{i) When to compress}} significantly impacts reasoning efficiency and compression quality. 
To achieve the core idea, we focus on addressing two key questions:
\underline{\textit{i) When to compress?}} 
The timing of compression significantly impacts reasoning efficiency and compression quality. 
We explore two different strategies.
The first is \textit{token-level}~\citep{iclr25_activation_beacon}, where compression is performed after a fixed number of tokens.
This strategy is straightforward to implement but may ignore semantic boundaries.
The second is \textit{thought-level}~\citep{acl24_anllm}, where compression is performed after a complete ``thought'', defined as a sentence or paragraph.
This strategy better preserves semantic information but requires a more complex segmentation function.
\underline{\textit{ii) How to compress?}}
The goal of compression is to encode the current lengthy thought into a more compact representation.
We investigate two different approaches.
The first is \textit{text compression}, where the current thought is encoded into a shorter text~\citep{emnlp23_llmlingua} or a chunk of continuous vectors~\citep{emnlp23_autocompressors,iclr24_icae}.
This method requires an additional encoding model and increases computational overhead.
The second is \textit{hidden state compression}, where the hidden state of the current thought is compressed into the hidden states of a few special tokens (i.e., \textit{gist tokens}~\citep{nips23_gist}).
We choose this method as it does not require additional models.
% \zyq{Merge}
Specifically, in our work, we address the first question by \textit{reconstructing data} and the second by \textit{constructing thought-based attention mask}.



\subsection{\ours}

\textbf{Data Reconstruction.}
To enable LLMs to dynamically compress during generation, we reconstruct the original dataset $\mathcal{D}$ as follows.
First, we segment the output.
Given the input $X$ and output $Y$, we use a segmentation function $\texttt{Seg()}$ to divide $Y$ into $k$ subsequences $S$, i.e., $Y=\{S_i\}_{i=1}^{k}$.
The function can be based on \textit{token-level} or \textit{thought-level}.
Then, we insert the special tokens.
Specifically, we insert a set of special tokens $\{\texttt{<w>}, C, \texttt{[o]}\}$ between adjacent subsequences $S_i$, where $\texttt{<w>}$ is an \textit{optional} compression trigger, indicating the need to compress the preceding thought.
It can be omitted if the $\texttt{Seg()}$ is token-level or if $\texttt{<w>}\in S_i$.
The token $C=\{\texttt{[c}_\texttt{i}\texttt{]}\}_{i=1}^{|C|}$ consists of $|C|$ special tokens, serving as gist tokens to store compressed content. 
Here we refer to $C$ as \textit{cache tokens} and denote $|C|$ as the \textit{cache size}.
The token $\texttt{[o]}$ is a mandatory output token, enabling continual generation based on compressed content, inspired by~\citeauthor{emnlp24_onegen}.
Finally, the enhanced data is 
% $$\hat{Y}=\{S_1, \underline{\texttt{<w>}, C, \texttt{[o]}}, S_2, \underline{\texttt{<w>}, C, \texttt{[o]}}, \dots, S_k\},$$
\begin{align*}
\hat{Y}=\{S_1, \underline{\texttt{<w>}, C, \texttt{[o]}}, S_2, \underline{\texttt{<w>}, C, \texttt{[o]}}, \dots, S_k\},
\end{align*}
and the enhanced dataset is defined as $\hat{\mathcal{D}}=\{(X,\hat{Y})_i\}_{i=1}^{|\hat{\mathcal{D}}|}$.
For simplicity, we assume $\texttt{<w>}\in S_i$, so we omit it. 
Additionally, we use superscripts to distinguish different special tokens at different positions, such as $C^{(1)}$ and $\texttt{[o]}^{(1)}$ for tokens following $S_1$, though they are the same across different positions.

\textbf{Thought-based Attention Mask Construction.}
To enable LLMs to learn how to compress and how to generate based on the compressed content (i.e., how to understand the compressed content), we manipulate \textit{Thought-based Mask Construction} as shown in Figure~\ref{fig:method}(b).
Specifically, let $S_{<i}=\{S_1,\dots,S_{i-1}\}$ denotes the sequence before the $i$-th thought $S_i$.

During compression, $C^{(i)}$ tokens can only attend to the question $X$, previous compressed content $\{{C,\texttt{[o]}\}}^{(<i)}$, and the current thought $S_i$, that is, 
\begin{align*}
C^{(i)} &\leftarrow \texttt{Cmp}(X, \\
&\quad \{C^{(1)}, \texttt{[o]}^{(1)}, \dots, C^{(i-1)}, \texttt{[o]}^{(i-1)}\}, S_i),
\end{align*}
% $$C^{(i)}\leftarrow \texttt{Cmp}(X,\{{C^{(1)},\texttt{[o]}^{(1)},\dots,C^{(i-1)},\texttt{[o]}^{(i-1)}\}},S_i)$$, 
where $\texttt{Cmp()}$ is compression operation.
This allows the LLM to compress the key content of $S_i$ into $C^{(i)}$.

During generation, token $\texttt{[o]}^{(i)}$ can only attend to the question $X$ and the previous compressed content $\{C, \texttt{[o]}\}^{(\le i)}$, that is, 
% $$S_{i+1}\leftarrow \texttt{Gen}(X,\{C^{(1)}, \texttt{[o]}^{(1)},\dots,C^{(i)},\texttt{[o]}^{(i)}\}),$$
\begin{align*}
S_{i+1}\leftarrow \texttt{Gen}(X,\{C^{(1)}, \texttt{[o]}^{(1)},\dots,C^{(i)},\texttt{[o]}^{(i)}\}),
\end{align*}
where $\texttt{Gen()}$ is generation operation.
This enables the LLM to continue reasoning based on the question and previous compressed content.

\textbf{Training and Inference.}
Our training objective is to maximize the following probability distribution:
% \begin{align*}
% &P_\theta(S_1|X) \cdot P_\theta (S_2|X,C^{(1)},\texttt{[o]}^{(1)})\cdot \dots \\
% &\quad\cdot P_\theta\left ( S_k|X, \left \{ C^{(i)},\texttt{[o]}^{(i)} \right \}_{i=1}^{k}  \right ),
% \end{align*}
\begin{align*}
&P_\theta(S_1|X) \cdot P_\theta (S_2|X,C^{(1)},\texttt{[o]}^{(1)})\cdot \dots \\
&\quad\cdot P_\theta ( S_k|X, \{ C^{(i)},\texttt{[o]}^{(i)} \}_{i=1}^{k-1}  ),
\end{align*}
where $\theta$ represents the LLM parameters.
Notably, during training, LLM is not allowed to predict the input $X$ and the special tokens $C$ and $\texttt{[o]}$.
The training samples are drawn from the $\mathcal{\hat{D}}$, and we employ an attention mask to encourage the LLM to learn to compress and comprehend the compressed content. 
The entire training process remains based on next token prediction. 
The detailed inference procedure is illustrated in Fig.~\ref{fig:intro}(b) and Fig.~\ref{fig:method}(c). 
% with the corresponding pseudocode provided in Appendix~\ref{}.

\subsection{Discussions}
\label{sec:method:discussion}
% \begin{figure}[!t] % !t, !htbp
%     \centering
%     \scalebox{1}{
%     \includegraphics[width=1.0\linewidth]{figures/discouple-v1.pdf} 
%     }
%     % \vspace{-1mm}
%     \caption{\ours}
%     \label{fig:discouple}
%     % \vspace{-3mm}
% \end{figure} 

% Compression rate
% different between ours and anllm
% compress is cache operation
More discussion of \ours~is detailed in the Appendix~\ref{sec:app:discussion} due to page constraints.

\textbf{What content has been compressed?}
Our cache tokens do not aim for lossless compression, but rather focus on preserving predictive information crucial for subsequent inference. 
As illustrated in the dashed box of Figure~\ref{fig:intro}(b), the cache tokens selectively retain only the information that significantly contributes to future reasoning.

% 这边可以挪到附录
% \textbf{Viewing \ours~from Other Perspectives.}
% In previous sections, we design \ours~from a compression perspective. 
% Here, we further discuss it from the perspectives of \textit{Memory} and \textit{KV Cache Compression}, where KV Cache can be viewed as a form of LLM work memory.
% In Memory perspective, \ours's workflow can be summarized as follows: it first performs autoregressive reasoning, then stores key information from the reasoning process as memory (memory), and continues reasoning based on the memorized content. 
% Thus, the information in the cache tokens acts as a compact memory, though it is only effective for the current LLM and lacks transferability.
% In KV Cache Compression perspective,
% Unlike methods such as H2O~\citep{nips23_h2o}, which rely on manually designed eviction policy to select important tokens, \ours~merges previous tokens in a continuous space, \textit{ceating} new representations. 
% The content and manner of merging are autonomously determined by the LLM, rather than being a discrete selection process.

% \textbf{Compression Rate.}

\textbf{Difference between \ours~and AnLLM.}
AnLLM~\citep{acl24_anllm} is a method similar to ours, capable of compressing the thought during inference.
In Fig.~\ref{fig:comp}, we compare the differences in Attention Mask between \ours~and AnLLM:
1) \textit{Decoupling Generation and Compression.}
In AnLLM, the $\texttt{[c}_\texttt{i}\texttt{]}$ token is tasked with both compressing historical information and generating subsequent content, as shown by the blue and pink arrows in Fig.~\ref{fig:comp}. 
This design tightly couples generation and compression. 
In contrast, \ours~decouples these tasks: the $\texttt{[c}_\texttt{i}\texttt{]}$ token solely compresses historical information, while the $\texttt{[o]}$ token performs reasoning based on the compressed content.
2) \textit{Context Visibility during Compression.}
AnLLM can only access the current thought during compression.
% For instance, when compressing $S_2$, only $S_2$ is visible, while the question $X$ and historical compressed content remain inaccessible.
\ours, however, allows access to $X$, historical compressed content, and the current thought during compression, thereby enhancing contextual understanding.
Ablation experiments in Section~\ref{ablation} demonstrate that these designs significantly improve performance.


