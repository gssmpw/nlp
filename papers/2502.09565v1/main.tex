\documentclass[10pt]{article}
\usepackage{PRIMEarxiv}
\usepackage{adjustbox}
\usepackage{longtable}
\usepackage{url}
\usepackage{needspace}
\usepackage[numbers, sort&compress]{natbib}
\usepackage{graphicx, xcolor, todonotes, appendix, booktabs, caption, array, authblk, svg}
\usepackage{listings}

\lstset{frame=tb,
  aboveskip=3mm,
  belowskip=3mm,
  showstringspaces=false,
  columns=flexible,
  basicstyle={\small\ttfamily},
  breaklines=true,
  breakatwhitespace=true,
  tabsize=3,
  breakindent=0pt
}


\title{MDCrow: Automating Molecular Dynamics Workflows with Large Language Models}


\author[1]{Quintina Campbell$^\dagger$}
\author[1,3]{Sam Cox$^\dagger$}
\author[1]{Jorge Medina$^\dagger$}
\author[2]{Brittany Watterson}
\author[1,3]{Andrew D. White$^*$}

\affil[1]{Department of Chemical Engineering, University of Rochester, Rochester, New York, USA}
\affil[2]{Department of Biomedical Engineering,
University of Rochester, Rochester, New York, USA}
\affil[3]{FutureHouse Inc., San Francisco, CA}

\renewcommand\Authands{, }  
\renewcommand\Authsep{, }   

\begin{document}
\date{\today}

\maketitle
\def\thefootnote{$\dagger$}\footnotetext{These authors contributed equally to this work}
\def\thefootnote{*}\footnotetext{Corresponding author: \texttt{andrew.white@rochester.edu}}
\def\thefootnote{\arabic{footnote}}

\begin{abstract}
Molecular dynamics (MD) simulations are essential for understanding biomolecular systems but remain challenging to automate. Recent advances in large language models (LLM) have demonstrated success in automating complex scientific tasks using LLM-based agents. In this paper, we introduce MDCrow, an agentic LLM assistant capable of automating MD workflows. MDCrow uses chain-of-thought over 40 expert-designed tools for handling and processing files, setting up simulations, analyzing the simulation outputs, and retrieving relevant information from literature and databases. We assess MDCrow's performance across 25 tasks of varying required subtasks and difficulty, and we evaluate the agent's robustness to both difficulty and prompt style. \texttt{gpt-4o} is able to complete complex tasks with low variance, followed closely by \texttt{llama3-405b}, a compelling open-source model. While prompt style does not influence the best models' performance, it has significant effects on smaller models.
\end{abstract}
\section{Introduction} 

Molecular dynamics (MD) simulations is a common method to understand dynamic and complex systems in chemistry and biology. While MD is now routine, its integration into and impact on scientific workflows has increased dramatically over the past few decades \cite{MD_Applications_in_proteins, MD_review_2002, hollingsworth2018molecular}. There are two main reasons for this: First, MD provides valuable insights. Through simulations, scientists can study structural and dynamic phenomena, perturbations, and dynamic processes in their chemical systems. Second, innovations in hardware and expert-designed software packages have made MD much more accessible to both experienced and novice users \cite{hollingsworth2018molecular}. 

For a given protein simulation, parameter selection is nontrivial: the user must provide the input structure (such as a PDB \cite{velankar2021protein} file), select a force field (e.g., CHARMM \cite{brooks2009charmm}, AMBER \cite{ponder2003force}), and specify parameters such as temperature, integrator, simulation length, and equilibration protocols. Simulations also generally require pre- and post-processing steps, along with various analyses. For instance, a user may need to clean or trim a PDB file, add a solvent, or analyze the protein's structure. After simulation, they might examine the protein's shape throughout the simulation or assess its stability under different conditions. The choices for pre-processing, analysis, and simulation parameters are highly specific to any given use case and often require expert intuition. Thus, automating this process is difficult but beneficial. 

Several efforts have been made to automate MD workflows \cite{Admiral, RadonPy,Gmx_qk,Foundry,Chaperong,MolAr,ProFESSA,Sim_stack,Fab_sim,PyAuto_FEP,Proto_Caller}, focusing largely on specific domains, such as RadonPy for polymer's simulations \cite{RadonPy}, or PyAutoFEP for proteins and small molecules for drug-screening \cite{PyAuto_FEP}. Other approaches are constrained to a particular combination of simulation software and simulation (e.g. GROMACS and Free Energy Perturbation). Certainly, there has been significant community-driven improvement in automating and creating MD toolkits \cite{suplatov2020easyamber, martinez2009packmol, michaud2011mdanalysis, mdtraj, eastman2017openmm, abraham2015gromacs, lammps,Sim_stack} and user-friendly interfaces and visualizations \cite{goret2017mdanse, ribeiro2018qwikmd, rusu2014mdwiz, hildebrand2019bringing, biarnes2012metagui, humphrey1996vmd, sellis2009gromita, martinez2017playmolecule}.
While these advances improve the capabilities and ease of use in many cases, the inherent variability of MD workflows still poses a great challenge for full automation. 

Large-Language Model (LLM) agents \cite{schick2023toolformer, mrkl, react, narayanan2024aviary} have gained popularity for their ability to automate technical tasks through reasoning and tool usage, even surpassing domain-specialized LLMs (e.g., BioGPT \cite{luo2022biogpt}, Med-PaLM \cite{singhal2023large}) when programmed for specialized roles \cite{gao2024empoweringbiomedicaldiscovery}. These agents have demonstrated promising results in scientific tasks within a predefined toolspace, with tools like ChemCrow and Coscientist successfully automating complex workflows and novel design in chemical synthesis \cite{bran2024augmenting, boiko2023autonomous, cactus}. Likewise, LLM-driven automation has been explored in materials research \cite{jablonka202314, su2024automation, chiang2024llamp, kim2024large}, literature and data aggregation \cite{lee2024harnessing, skarlinski2024language}, and more sophisticated tasks \cite{calms, BioPlanner, crisprgpt, LLM-RDF, chiang2024llamp, ChatMOF, TAIS, ProtAgent}. Most similar to this work, ProtAgents \cite{ProtAgent} is a multi-agent modeling framework tackling protein-related design and analysis, and LLaMP \cite{chiang2024llamp} applies a retrieval-augmented generation (RAG)-based ReAct agent to simulate inorganic materials by interfacing with literature databases, Wikipedia, and atomistic simulation tools. Although preliminary work has applied agentic LLMs to MD via a RAG-based agent \cite{chiang2024llamp}, no fully adaptive and autonomous system exists for biochemical MD or protein simulations. See Ramos et al.\cite{ramos2024review} for a recent review on the design, assessment, and applications of scientific agents.


\begin{figure}[ht!]
    \centering
    \includegraphics[width=0.9\linewidth]{figs/TOC.png}
    \caption{\textbf{A.} MDCrow workflow. Starting with a user prompt and initialized with a set of MD tools, MDCrow follows a chain-of-thought process until it completes all tasks in the prompt. The final output includes a response, along with all resulting analyses and files. \textbf{B}. The tool distribution categorized into 4 types: information retrieval, PDB and protein handling, simulation, and analysis. A few examples from each category are shown. \textbf{C.} Two example prompts that MDCrow is tested on. The first is the simplest prompt, containing only 1 subtask. The most complex task requires 10 subtasks. \textbf{D}. Average subtask completion across all 25 prompts as task complexity (the number of subtasks per prompt) increases. The top three performing base-LLMs are shown. Among them, \texttt{gpt-4o} and \texttt{llama3-405b} consistently maintain high stability, staying close to 100\% completion even as task complexity increases.
    }
    \label{fig:toc}
\end{figure}

Here we present MDCrow, an LLM-agent capable of autonomously completing MD workflows. Our main contributions to the field are (1) we assess MDCrow's performance across 25 tasks with varying difficulty and compare performance of different LLM models; (2) we measure robustness how agents are prompted and task complexity based on required number of subtasks we compare with simply equipping an LLM with a python interpreter with the required packages installed, rather than using a custom built environment. Our main conclusions is that MDCrow with \texttt{gpt-4o} or \texttt{llama3-405b} is able to perform nearly all of our assessed tasks and is relatively insensitive to how precise the instructions are given to it. See Figure \textbf{\ref{fig:toc}D} for an overview of the main results.

\section{Methods} \label{methods}

\subsection{MDCrow Toolset}
MDCrow is an LLM agent, which consists of an environment of tools that emit observations and an LLM that selects actions (tools + inpnut arguments). MDCrow is built with Langchain \cite{langchain} and a ReAct style prompt.\cite{react}. The tools mostly consist of analysis and simulation methods; we use OpenMM \cite{eastman2017openmm} and MDTraj \cite{mdtraj} packages, but in principle our findings generalize to any such packages. 

MDCrow's tools can be categorized in four groups: Information Retrieval, PDB \& Protein, Simulation, and Analysis (see Figure \textbf{\ref{fig:toc}B}). 

\paragraph{Information Retrieval Tools} These tools enable MDCrow to build context and answer simple questions posed by the user. Most of the tools serve as wrappers for UniProt API functionalities \cite{uniprot}, allowing access to data such as 3D structures, binding sites, and kinetic properties of proteins. Additionally, we include a LiteratureSearch tool, which uses PaperQA \cite{skarlinski2024language} to answer questions and retrieve information from literature. PaperQA accesses a local database of relevant PDFs, selected specifically for the test prompts, which can be found in SI section \textbf{\ref{references_table}}. This real-time information helps the system provide direct answers to user questions and can also assist the agent in selecting parameters or guiding simulation processes. 

\paragraph{PDB \& Protein Tools} MDCrow uses these tools to interact directly with PDB files, performing tasks such as cleaning structures with PDBFixer \cite{eastman2017openmm}, retrieving PDBs for small molecules and proteins, and visualizing PDBs through Molrender \cite{molrender} or NGLview \cite{nguyen2018nglview}.


\paragraph{Simulation Tools} All included simulation tools use OpenMM \cite{eastman2017openmm} for simulation and PackMol \cite{martinez2009packmol} for solvent addition. These tools are built to manage dynamic simulation parameters, handle errors related to inadequate parameters or incomplete preprocessing, and address missing forcefield templates efficiently. The agent responds to simulation setup errors through informative error messages, improving overall robustness. Finally, the simulation tools outputs Python scripts that can be modified directly by MDCrow whenever the simulation requires additional steps or parameters.


\paragraph{Analysis Tools} This group of tools is the largest in the toolset, designed to cover common MD workflow analysis methods, many of which are built on MDTraj \citep{mdtraj} functionalities. Examples include computing the root mean squared distance (RMSD) with respect to a reference structure, the radius of gyration, analyzing the secondary structure, and various plotting functions.

\subsection{Chatting with Simulations}
A key challenge in developing an automated MD assistant is ensuring it can manage a large number of files, analyses, and long simulations and runtimes. Although MDCrow has been primarily tested with shorter simulations, it is designed to handle larger workflows as well. Its ability to retrieve and resume previous runs allows users to start a simulation, step away during the long process, and later continue interactions and analyses without needing to stay engaged the entire time. An example of this chatting feature is shown in Figure \textbf{\ref{fig:chat}}.

MDCrow creates an LLM-generated summary of the user prompt and agent trace, which is assigned to a unique run identifier provided at the end of the run (but accessible at any time during the session). Each run's files, figures, and path registry are saved in a unique checkpoint folder linked to the run identifier.

When resuming a chat, the LLM loads the summarized context of previous steps and maintains access to the same file corpus, as long as the created files remain intact. To resume a run, the user simply provides the checkpoint directory and run identifier. MDCrow then loads the corresponding memory summaries and retrieves all associated files, enabling seamless continuation of analyses.

\begin{figure}[ht!]
    \centering
    \includegraphics[width=0.8\linewidth]{figs/chat_demo.png}
    \caption{\textbf{Example Chat} Example of chat with MDCrow. The user first asks to download PDB files for two systems. Then, once MDCrow has completed this task, the user asks for analysis of the files. Next, the user asks for a quick 10 ps simulation of both files, and MDCrow saves all files for later handling. Lastly, the user asks for plots of RMSD for each simulation over time, and MDCrow responds with each plot.}
    \label{fig:chat}
\end{figure}

\section{Results}
\subsection{MDCrow Performance on Various Tasks}
To assess MDCrow's ability to complete tasks of varying difficulty, we designed 25 prompts with different levels of complexity and documented the number of subtasks (minimum required steps) needed to complete each task. MDCrow was not penalized for taking additional steps, but was penalized for omitting necessary ones. For example, the first prompt in Figure \textbf{\ref{fig:toc}C} contains a single subtask, whereas the complex task requires 10 subtasks: downloading the PDB file, performing three simulations, and performing two analyses per simulation. If the agent failed to complete an earlier step, it was penalized for every subsequent step it could not perform due to that failure.

The 25 prompts require between 1 and 10 subtasks, with their distribution shown in Figure \textbf{\ref{fig:results_1}B}. Each prompt was tested across three GPT models (\texttt{gpt-3.5-turbo-0125, gpt-4-turbo-2024-04-09, gpt-4o-2024-08-06}) \cite{gpt3, achiam2023gpt}, two Llama models (\texttt{llama-v3p1-405b-instruct, llama-v3p1-70b-instruct}) \cite{dubey2024llama} (accessed via the Fireworks AI API with 8-bit floating point (8FP) quantization \cite{fireworksAI2024}), and two Claude models (\texttt{claude-3-opus-20240229, claude-3-5-sonnet-20240620}) \cite{sonnet_model_card, opus_model_card}. A newer Claude Sonnet model, \texttt{claude-3-5-sonnet-20241022} was tested in later experiments but was not found to give superior results, so it was not tested on these 25 prompts. All other parameters were held constant across tests, and each version of MDCrow executed a single run per prompt.

Each run was evaluated by experts recording the number of required subtasks the agent completed and using Boolean indicators to indicate accuracy, whether the agent triggered a runtime error, and whether the trajectory contained any hallucinations. Since the agent trajectories for each run are inherently variable, accuracy is defined as the result's consistency with the expected trajectory rather than comparing against a fixed reference.

The percentage of tasks that were deemed to have valid solutions for MDCrow with each base-LLM is shown in Figure \textbf{\ref{fig:results_1}A}. The lowest performing model was \texttt{gpt-3.5}. This is not surprising, as this model had some of the highest hallucination rates (32\% of prompt completions contained hallucinations), compared to the absence of documented hallucinations in the higher performing models, \texttt{gpt-4o} and \texttt{llama3-405b}. However, the discrepancy in accuracy rates between models cannot solely be attributed to hallucinations, as \texttt{gpt-3.5} attempted fewer than half of the required subtasks, whereas the higher-performing models, \texttt{gpt-4o} and \texttt{llama3-405b}, attempted 80-90\% of the required subtask, earning accuracy in answering for 72\% and 68\% of tasks, respectively (Figures \textbf{\ref{fig:results_1}C, D}).

These results indicate that MDCrow can handle complex MD tasks but is limited by the capabilities of the base model. For \texttt{gpt-4-turbo}, \texttt{gpt-3.5}, and \texttt{llama3-70b}, the number of trajectories with verified results decreases significantly as task complexity increases (Figure \textbf{\ref{fig:results_1}C}). In contrast, \texttt{gpt-4o} and \texttt{llama3-405b} show only a slight decline, demonstrating that MDCrow performs well even for complex tasks when paired with more robust base models.


\begin{figure}[ht!]
    \centering
    \includegraphics[width=1.0\linewidth]{figs/e4_figure_feb132025.png}
    \caption{MDCrow Performance across Large Language Models.
    \textbf{A.} Summary of MDCrow performance dependent on LLM. Percentage of accuracy is determined by whether it gave acceptable final answer or not. While statistically indistinguishable from Claude and Llama models, \texttt{gpt-4o} significantly outperforms the rest of GPT models on giving accurate solutions (t-test, $0.004 \le$ p-value $\le 0.046$).  % p values for gpt4o beating gpt-3.5 and gpt-4-turbo: 0.00396 and 0.0459 respectively
    % NOT significant -- p values for gpt4o beating opus 3, sonnett 3.5, llama3-70b, llama3-405b: 0.96, 0.69, 0.08, 0.76, respectively
    \textbf{B.} The distribution of number of subtasks in each task across 25 prompts. The prompts range from 1-10 steps, with each step count belonging to at least 2 prompts. 
    \textbf{C.} Percentages of prompts with accurate solutions with respect to LLM used and number of subtasks per task. The correlation between accuracy and complexity is statistically significant for all LLMs (Spearman correlation,  $3.9\times10^{-7} \le$ p-value $\le 1.1\times10^{-2}$)
    % gpt-3.5 (0.0000049), gpt-4-turbo (0.00000039), gpt-4o (0.00011), Claude 3 Opus (0.011), Claude 3.5 Sonnet (0.000051), llama3-70b (0.000074), llama3-405b (0.00027)
    \textbf{D.} Percentage of the subtasks that the agent completed for each base LLM per task. 
    }
    \label{fig:results_1}
\end{figure}

\subsection{MDCrow Robustness}
We evaluated the robustness of MDCrow on complex prompts and different prompt styles. We hypothesized that some models would excel at completing complex tasks, while others would struggle—either forgetting steps or hallucinating—as the number of required subtasks increased. To test this, we created a sequence of 10 prompts that increased in complexity. The first prompt required a single subtask, and each subsequent prompt added an additional subtask (see Figure \textbf{\ref{fig:results_robustness}A}). Each prompt was tested twice: once in a natural, conversational style and once with explicitly ordered steps. Example prompts can be seen in Figure \textbf{\ref{fig:results_robustness}B}.

To quantify robustness, we calculated the coefficient of variation (CV) for the percentage of completed subtasks across tasks. A lower CV indicates greater consistency in task completion and, therefore, higher robustness. The analysis revealed clear differences in robustness across models and prompt types. Overall, \texttt{gpt-4o} and \texttt{llama3-405b} demonstrated moderate to high robustness, while the Claude models showed significantly lower robustness. The performance comparison is shown in Figure \textbf{\ref{fig:results_robustness}C}.

We expected that the percentage of subtasks completed by each model would decrease as task complexity increased. However, with \texttt{gpt-4o} and \texttt{llama3-405b} as base models, MDCrow demonstrated a strong relationship between the number of required and completed subtasks (Figure \textbf{\ref{fig:results_robustness}D}) for both prompt types, indicating consistent performance regardless of task complexity or prompt style. The three included Claude models demonstrated less impressive performance. \texttt{claude-3-opus} followed the linear trend very loosely, becoming more erratic as task complexity increased. As the tasks required more subtasks, the model consistently misses nuances in the instructions and make logical errors. Both \texttt{claude-3.5-sonnet} models gave poor performance on these tasks, often producing the same error (see SI section \textbf{\ref{Claude-SI}}).

\begin{figure}
    \centering
    \includegraphics[width=1.0\linewidth]{figs/robustness_fig.png}
    \caption{\textbf{A}. The number of subtasks in each task, categorized by type. Task 1 begins with a single pre-simulation subtask (\textit{Download a PDB file}) and each subsequent task adds a single subtask, adding to a total of 10 tasks with a maximum of 10 subtasks. \textbf{B}. Example of "Natural" and "Ordered" prompt style on a three-step prompt. \textbf{C}. The robustness of MDCrow built on each model with both prompt types, measured by coefficient of variation (CV). Lower CV is interpreted as greater consistency. \texttt{gpt-4o} and \texttt{llama3-405b} are the more robust models, as the Claude models have higher CVs. \textbf{D}. Comparison of subtask completion across models and prompt types. In the 9-subtask prompt, \texttt{gpt-4o} encountered an error after only one step and gave up without trying to fix it. In general, \texttt{gpt-4o} and {llama3-405b} have relatively robust performance with increasing complexity for both prompt types. \texttt{claude-3-opus} struggles with more complex tasks, making more logical errors for complex tasks. The two \texttt{claude-3.5-sonnet} models showed fairly poor performance across this experiment.}
    \label{fig:results_robustness}
\end{figure} 

\subsection{MDCrow Comparison}
We also compared MDCrow to two baselines: a ReAct \cite{react} agent with only a Python REPL tool and a single-query LLM. MDCrow and the baselines were tested on the same 25 prompts as previously mentioned, all using \texttt{gpt-4o}. We use different system prompts to accommodate each framework, guiding the LLM to utilize common packages with MDCrow, and these prompts can be found in SI section \textbf{\ref{prompts-SI}}. 

\begin{figure}[ht!] 
    \centering
    \includegraphics[width=1.0\linewidth]{figs/compare_frmwk_figure_2025jan14.png}
    \caption{Performance across LLM Frameworks using the same 25-prompt set: MDCrow, direct LLM with no tools (single-query), and ReAct agent with only Python REPL tool. All use \texttt{gpt-4o}. 
    \textbf{A.} Performance among LLM frameworks measured by whether accuracy and average percentage of subtasks they complete for each of 25 task prompts. 
    MDCrow is significantly better at giving accurate solutions than direct LLM (t-test, $p=1\times10^{-3}$) and ReAct (t-test, $p=4\times10^{-4}$).
    MDCrow completes significantly more subtasks on average compared to direct LLM (t-test, $p=1\times10^{-6}$) and ReAct (t-test, $p=6\times10^{-6}$).
    \textbf{B.} Percentage of tasks completed with the respect to LLM framework used and the number of subtasks required for each task. The correlation between accuracy and number of subtasks required is statistically significant, %$p<0.05$ 
    $p=1\times10^{-3}$ for direct LLM and $p=1\times10^{-4}$ MDCrow. The p value for ReAct is $p=7\times10^{-2}$. %$p<0.10$
    }
    \label{fig:llm_fw_exps}
\end{figure}

The single-query LLM is asked to complete the prompt by writing the code for all subtasks, not unlike what standalone ChatGPT would be asked to do. We then execute the code ourselves and evaluate the outcomes accordingly. ReAct with Python REPL can write and execute codes using a chain-of-thought framework.
We find that MDCrow outperforms the two baselines significantly, as shown in Figure \textbf{\ref{fig:llm_fw_exps}A}, on attempting all subtasks and achieving an accurate solution. Not surprisingly, the two baseline methods struggled with code syntax errors and incorrect handling of PDB files. There is not a significant difference between the two baselines, indicating that the ReAct framework did not significantly boost the model's robustness. 

In Figure \textbf{\ref{fig:llm_fw_exps}B}, we observe that the performance of all three methods generally declines as task complexity increases. However, both baseline methods drop to zero after just three steps, with performance then fluctuating erratically at higher complexities. This is not surprising, as proper file processing and simulation setup are crucial for optimal LLM performance in MD tasks. In contrast, MDCrow demonstrates greater robustness and reliability in handling complex tasks, thanks to its well-designed system for accurate file processing and simulation setup, as well as its ability to dynamically adjust to errors.

\subsection{MDCrow Extrapolation through Chatting}
We further show MDCrow's ability to harness its chatting feature and extrapolate outside of its toolset to complete new tasks. This task requires MDCrow to perform an annealing simulation, which is not part of the current toolset. The agent achieves this by first setting up a simulation to find appropriate system parameters and handle possible early errors. Then, the agent modifies the script according to the user's request. Once the simulation is complete, the user later asks for simulation analyses, shown in Figures \textbf{\ref{fig:annealing}A, B}.

This shows that MDCrow has the ability to generalize outside of its toolset and is capable of completing more complicated and/or user-specific simulations. By utilizing the chatting feature, users can walk MDCrow through new analyses, reducing the risk of catastrophic mistakes.

\begin{figure}[h!]
    \centering
    \includegraphics[width=\linewidth]{figs/annealing_fig.png}
    \caption{
    \textbf{A.} MDCrow simulating annealing. The user directly instructs MDCrow to simulate an annealing simulation of protein 1L2Y. Once the simulation is complete, the user utilizes the chatting feature to ask for further analyses.
    \textbf{B.} RMSD, RGy, and temperature throughout the simulation, as requested by the user in A.}
    \label{fig:annealing}
\end{figure}

\section{Discussion}
Although LLMs' scientific abilities are growing \citep{jaech2024openai, hurst2024gpt, labbench}, they cannot yet independently complete MD workflows, even with a ReAct framework and Python interpreter. However, with frontier LLMs, chain-of-thought, and an expert-curated toolset, MDCrow successfully handles a broad range of tasks. It performs 80\% better than \texttt{gpt-4o} in ReAct workflows at completing subtasks, which is expected due to MD workflows' need for file handling, error management, and real-time data retrieval. 

In some cases, particularly for complex tasks beyond its explicit toolset, MDCrow’s performance may improve with human guidance. The system’s chatting feature allows users to continue previous conversations, clarify misunderstandings, and guide MDCrow step-by-step through difficult tasks. This adaptability helps MDCrow recover from failures, refine its approach based on user intent, and handle more complex workflows. This suggests that, with more advanced LLM models, targeted feedback, and the addition of specialized tools, MDCrow could tackle an even broader range of tasks. We did not do a full evaluation of MDCrow's capabilities through this chatting feature in this work.

For all LLMs, task accuracy and subtask completion are affected by task complexity. Interestingly, while \texttt{gpt-4o} can handle multiple steps with low variance, \texttt{llama3-405b} is a compelling second best, as an open-source model. Other models, such as \texttt{gpt-3.5} and \texttt{claude-3.5-sonnet}, struggle with hallucinations or inability to follow multistep instructions. Performance on these models, however, is improved with explicit prompting or model-specific optimization (especially for \texttt{claude-3.5-sonnet}).

These tasks were focused on routine applications of MD with short simulation runtimes, limited to proteins, common solvents, and force fields included in the OpenMM package. We did not explore small-molecule force fields, especially related to ligand binding. Future work could explore multi-modal approaches \citep{mllmtool, assistgpt} for tasks like convergence analysis or plot interpretations. The current framework relies on human-created tools, but as LLM-agent systems become more autonomous \citep{wang2023voyager}, careful evaluation and benchmarking will be essential.


\section{Conclusion}
Running and analyzing MD simulations is non-trivial and typically hard to automate. Here, we explored using LLM agents to accomplish this. We built MDCrow, an LLM and environment consisting of over 40 tools purpose built for MD simulation and analysis. We found MDCrow could complete 72\% of the tasks with the optimal settings (\texttt{gpt-4o}). \texttt{llama-405B} was able to complete 68\%, providing a compelling open-source model. The best models were relatively robust to how the instructions are given, although weaker models struggle with unstructured instructions. Simply using an LLM with a python interpreter and required packages installed had a 28\% accuracy. The performance of MDCrow was relatively stable as well, though dependent on the model. Correct assessment of these complex scientific workflows is challenging, and had to be done by hand. Chatting with the simulations, via extended conversations, is even more compelling, but is harder to assess.

This work demonstrates the steps to automate and assess computational scientific workflows. As LLMs continue improving in performance, and better training methods arise for complex tasks like this, we expect LLM agents to be increasingly important for accelerating science. MDCrow, for example, can now automatically assess hypotheses with 72\% accuracy with simulation and can scale-out to thousands of simultaneous tasks. The code and tasks are open source and available at \texttt{https://github.com/ur-whitelab/MDCrow}.


\section{Acknowledgments}
Research reported in this work was supported by the National Institute of General Medic al Sciences of the National Institutes of Health under award number R35GM137966, % quinny
National Science Foundation under grant number of 1751471, % qlc, sc, jm
Robert L. and Mary L. Sproull Fellowship gift and U.S. Department of Energy, Grant No. DE-SC0023354. % jorge 
Work at FutureHouse is supported by the generosity of Eric and Wendy Schmidt. 
We thank the Center for Integrated Research Computing (CIRC) at University of Rochester for providing computational resources and technical support. 

\bibliographystyle{unsrt}
\newpage
\bibliography{MDCrow}
\appendix

\newpage

\input{SI/SupMat}

\end{document}
