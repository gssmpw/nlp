%! TEX Root = ../main.tex

\noindent
\textbf{Training datasets.}
We use the video encoder from Wang~\etal~\cite{wang2023videomaev2}, which is pre-trained on 1.35M video clips for masked auto-encoding.
To fine-tune our model, we use a limited number of synthetic datasets covering a varying range of types of annotations, and rely on the priors from the pre-trained video encoder for generalization.
We use Kubric~\cite{greff2022kubric}, a synthetic dataset in which multiple objects interact, annotated with depth, flow, and 2D and 3D point tracking.
To include videos with long 3D trajectories, we add PointOdyssey~\cite{zheng2023pointodyssey} and DynamicReplica~\cite{karaev2023dynamicstereo}.
Both are synthetic datasets with animated characters in mostly indoor scenes.
Both datasets have depth annotations and, in addition, DynamicReplica offers optical flow annotations.
To further increase scene diversity, we also include TartanAir~\cite{tartanair2020iros}, which provides annotations for flow and depth.

\noindent\textbf{Architectures.}
Our video encoder~\cite{wang2023videomaev2} processes video-clips of size $16 \times 224\times224$.
It uses a patch size of $2\times14\times14$, which results in $P=2048$ video tokens, and an embedding dimension of $C=1408$.
It has 40 encoder blocks, and we use the output from blocks 14, 21, 28, 36 for DPT heads for dense tasks, while the sparse heads use features only from the last block.
Feeding the sparse and dense heads with tokens from different blocks allows us to maintain the performance on dense tasks while we fine-tune our model to train the memory mechanism for the tracking tasks, as we discuss below.
For a $16\times224\times224$ video clip, our method generates the outputs for all our tasks in $\sim$300ms on an NVIDIA A6000 GPU.
This corresponds to $\sim$19ms for a single frame, which is competitive with single-task approaches (see Supplementary for detailed comparisons).
However, our method's latency may prevent its use for applications that require strict real-time performance.

\noindent\textbf{Training.}
We initialize our video encoder using a pretrained VideoMAE~\cite{wang2023videomaev2} and fine-tune our model in two stages.
In both stages, we construct a batch of many tracks per video for the tracking task.
In the first stage, we train end-to-end for depth, flow, 2D and 3D point tracking tasks on a single window of $T=16$ frames.
In the second stage, we further fine-tune our model for the tracking tasks using unrolled-window training and the memory mechanism for online tracking.
We train on videos of length $S=40$ by using 4 overlapping windows of size 16 frames and a stride of 8.
Due to memory constraints, in the second stage, we freeze all the parameters, except the last three layers (37-39) of the video encoder and the sparse task head.
This allows us to maintain the performance on depth and flow, while training the memory mechanism to improve the tracking tasks.
Both stages use a batch size of 8 and are trained on a single 8-GPU (NVIDIA A100) node for 100k iterations. Training takes 1 day and 2 days respectively.

\noindent\textbf{Losses.}
We use the SILog~\cite{eigen2014depth} loss for depth and L1 loss for optical flow. 
For tracking, we use L1 loss for 2D track positions, scale-invariant loss for track depth (similar to dense depth), and binary cross entropy loss for track visibility.
Like with the choices of tasks heads, we pick the most widely used losses for each of our tasks.
However, since we train for multiple tasks at once, weighting the losses appropriately is critical.
We find the loss weights empirically by first bringing the losses in the same order of magnitude and then doing a small hyperparameter search around those weights.

Please refer to the Supplementary for additional implementation details.
