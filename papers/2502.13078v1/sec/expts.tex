%! TEX Root = ../main.tex

First, we provide details about our architecture and training. 
Then, for each task, we discuss the baselines, the evaluation metrics and datasets, and show quantitative and qualitative results.
We also present an ablation study that evaluates the contribution of different components of our approach.
Finally, we show a way to extend our approach to new tasks by using motion-based segmentation as an example.

\subsection{Implementation}\label{sec:impl}
\input{sec/implementation}


\input{tables/depth.tex}


\begin{figure}
    \centering
    \includegraphics[width=0.99\columnwidth]{figures/depth_results/depth_figure.pdf}
    \caption{
        \textbf{Qualitative results for depth estimation.}
        We include one example each from Bonn, KITTI, and ScanNet. 
        Inference is conducted on 16-frame clips, but only 1 frame is shown. 
        }\label{fig:depth}
\end{figure}

\subsection{Video Depth Estimation}


We follow DepthCrafter~\cite{hu2024depthcrafter} and evaluate video depth estimation on a collection of five datasets.
We do not use any of the datasets for training our models or the baselines to better understand their generalization abilities.
There is an inherent scale-ambiguity in the estimated depthmaps. 
We follow the common practice of aligning linearly the estimation with the GT before calculating evaluation metrics.
The alignment is done for all the frames at once, and is carried out in \emph{disparity} space via least-square fitting.
For comparison on single image datasets, we repeat the single frame 16 times to compute our estimations.
We report two metrics: AbsRel ($\text{mean}(|\hat{\mathbf{d}}-\mathbf{d}| / \mathbf{d}))$ and $\delta_1$ (ratio of pixels satisfying $\max(\mathbf{d}/\hat{\mathbf{d}}, \hat{\mathbf{d}}/\mathbf{d})<1.25$), where $\mathbf{d}$ represents GT, and $\hat{\mathbf{d}}$ is depth estimation after alignment. 
We upsample our estimations from $224\times224$ to each dataset's original resolution for evaluation.

We consider video approaches including NVDS~\cite{wang2023nvds}, ChronoDepth~\cite{shao2024chronodepth}, DepthCrafter~\cite{hu2024depthcrafter}, 
as well as single-image ones, including Marigold~\cite{ke2024repurposing} and DepthAnything~\cite{yang2024depthanything,yang2024depthanythingv2}. 
Among them, DepthCrafter~\cite{hu2024depthcrafter} and DepthAnything~\cite{yang2024depthanything,yang2024depthanythingv2} each represent the SOTA respectively. 
Marigold and DepthCrafter are diffusion models, which afford impressive levels of details, but require an expensive iterative denoising process. 

Our results show consistent advantages over both SOTA single-image and video depth approaches on the four video datasets (Table~\ref{tab:depth}). 
Since \methodName is a video approach, applying it on single images from NYUv2 does not provide the necessary temporal context for it to perform well. 
DepthCrafter also similarly suffers on NYUv2. 
Figure~\ref{fig:depth} shows qualitative samples and comparison with select SOTA approaches. 
\methodName produces a level of details comparable to that of diffusion models such as DepthCrafter, while generally capturing more accurate relative scales.

\noindent
\textbf{Discussion.} Our final model performs on par with a specialized depth model (Table~\ref{tab:depth}), 
despite optimized jointly for all of our tasks. 
Scale alignment between windows for online inference makes a significant impact on ScanNet. 
This is due to the fast-paced view change in ScanNet samples making scale inconsistency between windows more prominent. 
It is also worth noting that \methodName performs competitively on KITTI, despite not fine-tuned on synthetic datasets that include driving scenarios.

\input{tables/flow.tex}

\begin{figure}
    \centering
    \includegraphics[width=0.99\columnwidth]{figures/flow_results/flow_figure.pdf}
    \caption{
        \textbf{Qualitative results for optical flow estimation on Spring.}
        Our results compare favorably to baselines in terms of both details and accuracy. 
        Inference is conducted on 16-frame clips, but only 1 frame is shown. 
        }\label{fig:flow}
\end{figure}

\subsection{Multi-Frame Optical Flow Estimation}

We use the Spring dataset~\cite{mehl2023spring} for evaluation.
We sample 289 16-frame clips from the \emph{train} split.
Spring is not used to train ours or other approaches we compare against, allowing us to evaluate generalization ability.
The input frames are resized to $224\times224$ for all evaluation.
We use the Endpoint Error (EPE), as well as a more robust metric, ratio of EPE $<1$, for the evaluation. 

We consider two baselines for comparison. 
RAFT~\cite{teed2020raft}, a competitive and widely used two-frame approach, creates dense pairwise pixel features and uses recurrent updates to estimate optical flow. 
MemFlow~\cite{dong2024memflow}, a recently published work, ranks among the top methods on the Spring benchmark.
It is a multi-frame approach that relies on a memory mechanism to leverage temporal context. 
Quantitatively, \methodName compares favorably to both RAFT and MemFlow on Spring (Table~\ref{tab:flow}).
Our model can capture well both small and large motions and presents more precise motion boundaries (Figure~\ref{fig:flow}). 
In addition, multi-frame approaches like MemFlow and ours generally have an edge in temporal stability (see Supplementary). 
Unlike many specialized approaches, our model currently only operates on low-resolution videos and further work is needed to enable efficient high-res estimation.

\input{tables/track_2d.tex}

\input{tables/track_3d.tex}

\begin{figure*}
    \centering
    \includegraphics[width=0.99\textwidth]{figures/track_results/track_figure.pdf}
    \caption{\textbf{Qualitative results of Sparse 2D/3D tracking on the TAPVid-3D benchmark.}
    Comparison with SpaTracker, a SOTA 3D tracking approach, demonstrates the superior quality of our 2D and 3D tracks.
    For joint visualization of depth and 3D tracks, we align them using median scaling.
    We use our depth maps for visualization of GT and for SpaTracker we use the ones used by their approach. 
    }\label{fig:track}
\end{figure*}

\subsection{Sparse 2D/3D Track Estimation}
We evaluate on TAPVid-3D~\cite{koppula2024tapvid3d}, a benchmark containing around 2.1M long-range 3D point trajectories from over 4000 real-world videos, covering a variety of objects, camera and object motion patterns, and indoor and outdoor environments.
It consists of three datasets: Aria~\cite{pan2023aria}, DriveTrack~\cite{sun2020waymodataset}, and PStudio~\cite{joo2015pstudio}.
It introduced several baselines by combining SOTA 2D point tracking approaches, such as TAPIR~\cite{doersch2023tapir}, BootsTAPIR~\cite{doersch2024bootstap}, and CoTracker~\cite{karaev2023cotracker}, with depth solutions like ZoeDepth~\cite{bhat2023zoedepth}, a monocular depth estimation approach, and COLMAP~\cite{schoenberger2016sfm,schoenberger2016mvs}, a structure-from-motion pipeline.
The top performing approach on the benchmark is SpaTracker~\cite{spatracker}.

The benchmark evaluates both 3D and 2D tracking approaches, and uses metrics that measure the ability to predict point visibility using an occlusion accuracy metric (OA), the accuracy of predicted trajectories in the visible regions (APD), and joint occlusion and geometric estimation (AJ).
To resolve the scale ambiguity in depth estimation, the benchmark uses global median scaling by computing the median of the depth ratios between the estimated and ground-truth 3D tracks over all the points and frames in a video.
We use the \textit{full\_eval} split evaluation numbers provided in the TAPVid-3D benchmark for comparing approaches.

On 3D tracking, we outperform previous approaches on average across all the metrics (Table~\ref{table:tapvid3d3dtrack}).
Among feedforward approaches, we perform better on all the datasets.
Approaches that combine 2D track estimation with COLMAP perform better on the DriveTrack~\cite{sun2020waymodataset} dataset.
This could be due to a relatively large bias of tracking mostly static vehicles, where COLMAP gives much more accurate depth.
Such COLMAP-based baselines, however, perform poorly on Aria~\cite{pan2023aria} and PStudio~\cite{joo2015pstudio}, which are mostly dynamic.
We show qualitative evaluation against the SOTA SpaTracker approach in Figure~\ref{fig:track}.

On 2D tracking, we are slightly behind the SOTA 2D tracking approaches (Table~\ref{table:tapvid3d2dtrack}).
Our approach becomes more competitive and performs better than TAPIR on average when we fine-tune our model only for the 2D tracking task.
We believe our reduced performance on 2D tracking comes from working at lower image resolution, $224\times224$ for us as compared to $384\times512$ for CoTracker and $256\times256$ for others, and a lack of task-specific tricks, like tracking multiple points together (CoTracker) or assuming access to all frames in the video and performing a global track-refinement (TAPIR and BootsTAPIR), both of which could also benefit our tracking head.
We also ablate our online tracking approach on both 2D and 3D tracking benchmarks, and show improved performance due to the use of memory mechanism when tracking points from one window to next.
Overall, we attribute our strong performance to our unified approach and carefully designed sparse head.



\subsection{Ablations}\label{sec:ablation}
To understand the contribution of different components of our approach, we perform an ablation study for depth, flow, 2D and 3D point tracking, as shown in Table~\ref{tab:ablation}.
For each of these tasks, we report average over the datasets not used in our training: for depth we use datasets in Table~\ref{tab:depth}, for optical flow we use the Spring dataset, and for tracking we use the \textit{minival} split from the TAPVid-3D~\cite{koppula2024tapvid3d} benchmark.
Our main contribution is to show how to leverage the priors of a pretrained VideoMAE for multiple dense and sparse low-level 4D perception tasks at once.
To show the usefulness of our end-to-end fine-tuning strategy, we compare against a pretrained and frozen VideoMAE, where we only fine-tune the task-specific heads. 
Table~\ref{tab:ablation} shows that our fine-tuned VideoMAE (row 3) produces better results than the pretrained and frozen VideoMAE across all tasks (row 2). 
A version trained end-to-end from scratch results in worse performance (row 1), which shows that our system leverages the pretraining of the VideoMAE.
Finally, by adding the proposed memory mechanism for the tracking head and using our two-stage training process, we obtain improvements in both 2D and 3D tracking tasks, while maintaining the performance on other tasks.

\begin{table}
    \centering
    \resizebox{.95\columnwidth}{!}{
      \begin{tabular}{l|cccc}
        \multicolumn{1}{c|}{} & \begin{tabular}[c]{@{}c@{}}Depth\\ AbsRel$\downarrow$ / $\delta_1\uparrow$\end{tabular} & \begin{tabular}[c]{@{}c@{}}Optical flow\\ EPE$\downarrow$ / EPE$<1\uparrow$\end{tabular} & \begin{tabular}[c]{@{}c@{}}2D Track\\ 2D-AJ$\uparrow$\end{tabular} & \multicolumn{1}{c}{\begin{tabular}[c]{@{}c@{}}3D Track\\ 3D-AJ$\uparrow$\end{tabular}} \\ \cline{1-5}
        \multicolumn{1}{l|}{From scratch} & 0.259 / 0.594 & 0.246 / 96.2 & 16.6 & \multicolumn{1}{c}{1.3} \\
        \multicolumn{1}{l|}{VideoMAE frozen} & 0.137 / 0.841 & 0.120 / 98.2 & 29.3 & \multicolumn{1}{c}{3.3} \\
        \multicolumn{1}{l|}{Ours (w/o Mem)} & \textbf{0.120 / 0.876} & \textbf{0.100 / 98.5} & 41.1 & \multicolumn{1}{c}{8.7} \\
        \multicolumn{1}{l|}{Ours} & \textbf{0.120 / 0.876} & \textbf{0.100 / 98.5} & \textbf{50.2} & \multicolumn{1}{c}{\textbf{10.8}} \\
      \end{tabular}
    }
    \caption{\textbf{Ablation study.} Training using pre-trained VideoMAE performs better than training from scratch (row 3 vs. 1), which shows our approach leverages VideoMAE priors. Our approach performs better than using a frozen VideoMAE and only fine-tuning the heads (row 3 vs. 2), which shows end-to-end fine-tuning helps. Adding memory mechanism and two-stage training strategy improves tracking performance while maintaining performance on other tasks (row 4 vs. 3).}
    \label{tab:ablation}
\end{table} 



\subsection{Additional Task: Motion-based Segmentation}
We use the motion-based segmentation task to show one way to add a new task to our network.
We do this simply by freezing our trained video encoder and fine-tuning our proposed dense head for this task.
We generate the ground-truth annotations for training and evaluation by using \emph{video} datasets that provide camera, depth and 3D-motion information.
For training, we use the Kubric~\cite{greff2022kubric} dataset and fine-tune using binary cross entropy loss.
For evaluation, we use the Virtual KITTI (VKITTI)~\cite{cabon2020vkitti2} and Spring~\cite{mehl2023spring} datasets.
We compare against RigidMask (RM)~\cite{yang2021rigidmotion}, a SOTA two-frame rigid-motion segmentation approach that combines dynamic motion signals from flow, optical-expansion~\cite{yang2020upgrading} and depth.
It is trained on the SceneFlowDatasets~\cite{mayer2016scenflow}; however, they also train a version for driving scenarios (RM-Drive).
To evaluate, we report foreground IoU (higher is better) on VKITTI and Spring.
\setlength{\columnsep}{2pt}
\setlength{\intextsep}{0pt}
\begin{wrapfigure}[3]{r}[5pt]{.41\columnwidth}
  \resizebox{0.4\columnwidth}{!}{
    \begin{tabular}{c|c|c}
      & VKITTI & Spring \\
    \hline
    RM & 32.6 & 16.5\\ 
    RM-Drive & 35.4 & 8.5\\ 
    Ours & {\bf 46.7} & {\bf 23.7}
    \end{tabular}
    }
\end{wrapfigure}
On both datasets, our video-based approach achieves better performance.
Note that while fine-tuning on driving scenes allows RigidMask (RM-Drive) to reduce the gap slightly on VKITTI, it significantly hurts performance on Spring, highlighting the benefit of our model's generalization ability.
As shown in Figure~\ref{fig:dyn}, for both the indoor scenarios with human-object interactions and the outdoor driving scenarios, our approach performs better and can detect small motions (see more comparisons in Supplementary).
Freezing the video encoder and fine-tuning a task-specific head is the simplest way to add a new task that does not affect the performance of other tasks we train for.
Better strategies may exist that allow for some fine-tuning of the video encoder without affecting the performance of other tasks, though the investigation is outside the scope of this paper.

\begin{figure}
  \centering
  \includegraphics[width=0.99\columnwidth]{figures/dynamic/dyn_seg.pdf}
  \caption{\textbf{Qualitative results of motion-based segmentation.} 
  Samples are chosen from the TAPVid-3D benchmark.
  Across various scenarios, ours show advantages in small motions, boundary accuracy, as well as temporal consistency (see Supp.).
  The GT masks overlaid on images are only provided to identify qualitatively which objects are moving, but are not pixel-accurate.
  }\label{fig:dyn}
\end{figure}