%! TEX Root = ../main.tex

Large collections of videos are our most complete and compact source of priors about the world.
Much like text did for large-language models, the corpus of videos we amassed over the years allowed video-language models (VLMs) to produce remarkable zero-shot results on high-level vision tasks such as video captioning, video question answering, and others.
However, zero-shot, low-level 3D and 4D vision perception tasks, such as depth from video, tracking, optical flow, and others remain a challenge.
Pretrained video diffusion models fine-tuned on target-domain data showed potential on dense vision perception tasks (\eg, depth~\cite{ke2024repurposing,hu2024depthcrafter}, flow~\cite{saxena2024surprising}, etc.), but the fine-tuning makes them task-specific, and therefore limits their ability to leverage priors across multiple tasks at once.
Sparse vision perception tasks, such as tracking, are even more challenging to tackle with a general foundation model, because their representation does not fit naturally into data structures like dense pixel planes.
As a result, they have been historically addressed with carefully designed approaches, which are often optimization-based~\cite{wang2023omnimotion}, or with specialized architectures~\cite{karaev2023cotracker,doersch2023tapir}.

Can we leverage the priors learned from a large body of video data and solve \emph{multiple} low-level 4D vision perception tasks, both \emph{dense} and \emph{sparse}, with a \emph{unified} architecture, in a \emph{zero-shot} way?

This goal presents multiple challenges.
To effectively learn and share priors across tasks---including those beyond 4D perception---we need an architecture with a strong backbone shared across tasks.
This architecture should also be general to allow for pretraining on auxiliary tasks.
Not least, dense and sparse tasks require fundamentally different representations, \eg, dense 2D planes vs.~3D tracks.
\begin{figure}
    \captionsetup{skip=5pt}
    \centering
    \includegraphics[width=\columnwidth]{figures/teaser/teaser_v3.pdf}
    \caption{We propose \methodName, a single, general-purpose architecture that solves several low-level 4D perception tasks with zero-shot generalization capabilities. We show that a pre-trained video encoder can be combined with lightweight heads and surpass or at least match the performance of SOTA methods, which use specialized architectures and are trained to solve individual tasks.}
    \label{fig:teaser}
\end{figure}
We fulfill these requirements by combining a pretrained video masked auto-encoder (VideoMAE)~\cite{tong2022videomae,wang2023videomaev2} with per-task, lightweight heads (Figure~\ref{fig:method}).
VideoMAEs have been successfully employed for a variety of mid- and high-level vision tasks~\cite{wang2023videomaev2}, but their ability to capture the spatio-temporal relationship between pixels is underexplored in the context of low-level 4D perception.
We choose a VideoMAE as the backbone for our system because of the powerful priors it learned in its pretraining.
Moreover, VideoMAEs offer a feedforward, online mechanism to tokenize videos within a relatively small computational envelope.
For dense tasks, we couple the VideoMAE with heads based on the dense prediction transformer (DPT), which has been shown to perform well on depth estimation, image segmentation, and others~\cite{ranftl2021dpt}.
For sparse tasks we focus on tracking, and specifically, on the track-any-point (TAP) formulation~\cite{doersch2022tap}.
We posit that tracking is important for perception because understanding fine-grained, complex motions and the physical interaction between objects is critical to downstream applications, including 3D reconstruction~\cite{lei2024mosca, wang2023vggsfm} and robotics manipulation~\cite{xu2024flow,wen2022you,wen2024anypointtrajectory, bharadhwaj2024track2act}.
We formulate the problem of estimating tracks as that of predicting, for queried pixels, \emph{2D} heatmaps with associated depth and visibility tokens.
This is the mechanism that allows us to tackle sparse and dense tasks within a \emph{unified} framework.

Our formulation presents several desirable properties and advantages.
First, the pretrained VideoMAE model allows us to tap into priors learned from large datasets---potentially different and more varied than those typically used for low-level 4D perception.
It also affords us efficient computation: our system solves all tasks in roughly 300ms for a 16-frame video chunk ($\sim$19ms/frame), which is comparable to, or faster than, methods specialized for each task (see Table~\ref{tab:inferencetime} in the Supplementary).
Moreover, combining it with per-task heads allows us to train a relatively small number of parameters for new tasks, which we show by freezing the system and adding a head for motion-based segmentation.
Lastly, but perhaps most importantly, breaking the architecture into a general VideoMAE and per-task heads offers a mechanism to solve both dense and sparse tasks with a \emph{unified} model (Figure~\ref{fig:teaser}).
Despite being general, our architecture performs on par with, or better than, state-of-the-art methods.
This is remarkable because the baselines we compare against are task-specific, carefully designed for, and specialized to excel at their respective tasks.
Finally, VideoMAEs are already used as encoders for VLMs~\cite{wang2024internvideo2,2023videochat}, and we speculate that training them to reason about low-level 4D perception may impart those capabilities to the downstream VLMs they may be used with, though validating this statement is outside the scope of our paper.

\begin{figure}
    \captionsetup{skip=3pt}
    \centering
    \includegraphics[width=\columnwidth]{figures/method/method.pdf}
    \caption{We split our architecture into a powerful encoder, which extracts tokens from the input video, and per-task lightweight heads. For sparse tasks, such as 3D tracking, we define additional query tokens, namely the point we wish to track, $\mathcal{P}$, with the corresponding feature token, $\mathcal{F}$, and output tokens (heatmap $\mathcal{H}$, depth $\mathcal{D}$, and visibility $\mathcal{V}$). Figure~\ref{fig:sparse_head} further details the processing performed by the sparse task head.}\label{fig:method}
\end{figure}


