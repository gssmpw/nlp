%! TEX Root = ../main.tex
\begin{abstract}
The spatio-temporal relationship between the pixels of a video carries critical information for low-level 4D perception.
A \emph{single} model that reasons about it should be able to solve \emph{several} such tasks well.
Yet, most state-of-the-art methods rely on architectures specialized for the task at hand.
We present \emph{L4P} (pronounced ``LAP"), a feedforward, general-purpose architecture that solves low-level 4D perception tasks in a unified framework.
L4P combines a ViT-based backbone with per-task heads that are lightweight and therefore do not require extensive training.
Despite its general and feedforward formulation, our method matches or surpasses the performance of existing specialized methods on both dense tasks, such as depth or optical flow estimation, and sparse tasks, such as 2D/3D tracking.
Moreover, it solves all those tasks at once in a time comparable to that of individual single-task methods.
\end{abstract}
