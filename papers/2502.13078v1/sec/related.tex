%! TEX Root = ../main.tex
Our method unites the strong generalization capabilities of pre-trained foundation models with lightweight task-specific heads. 
In this section, we review relevant literature for both the foundation models and the individual tasks. 

\subsection{Foundation models for vision perception}
Self-supervised pre-training of large models on huge unlabeled data has shown great success on vision tasks.
Among them, Vision Transformers (ViT)~\cite{vit} pretrained with masked autoencoding (MAE)~\cite{he2022masked} have become a major choice to fine-tune for many vision tasks, SegmentAnything (SAM)~\cite{kirillov2023SAM} being a notable example.
VideoMAE~\cite{tong2022videomae} and MAE-ST~\cite{feichtenhofer2022maest} adopt MAE on videos for spatio-temporal representation learning.
VideoMAEv2~\cite{wang2023videomaev2} introduces a dual masking strategy that allows them to efficiently scale up the model to a billion parameters and to effectively leverage priors from large data.
Their representation shows strong performance on action recognition, and more recently is combined with large language models to enable multimodal video understanding~\cite{wang2024internvideo2,2023videochat}.
However, their uses remain unexplored for the low-level 4D perception tasks, which we address in this work.

\subsection{Dense Prediction Tasks}
We review related works for the dense prediction tasks of depth, flow and motion-based segmentation.

\noindent
\textbf{Depth Estimation.}
Depth perception is critical to many applications, and depending on the use cases many solutions exists for this task. 
Traditional stereo approaches~\cite{furukawa2009accurate, galliani2015massively,schonberger2016pixelwise}, as well as their more recent deep learning counterparts~\cite{yao2018mvsnet,huang2018deepmvs, sayed2022simplerecon}, take two or more posed input views and leverage the underlying 3D correspondences, which limit their uses to only static scenes.
Depth estimation from a single image~\cite{eigen2014depth,ranftl2020midas,ranftl2021dpt, yang2024depthanything,yang2024depthanythingv2} 
has made impressive progress, but still suffer from 3D and temporal inconsistency, limiting their utility for video settings.
In this work, we focus on video depth estimation, which, unlike multi-view stereo (MVS), focuses on dynamic scenes and assumes no provided camera poses.
Some early works rely on test-time optimization~\cite{luo2020consistent,kopf2021robust} to enforce consistency.
More recent works introduce feedforward~\cite{teed2018deepv2d, wang2023nvds} and diffusion-based~\cite{shao2024chronodepth,hu2024depthcrafter,zhao2024stereocrafter} solutions. 

Since depth annotations are limited, many approaches exploiting learned priors from foundation models.
DINOv2~\cite{oquab2023dinov2} pretraining allows them to achieve competitive single-image depth estimation quality with only a lightweight head and minimal fine-tuning.
DUSt3R~\cite{wang2024dust3r} and MASt3R~\cite{leroy2024mast3r} show that cross-view completion pre-training~\cite{weinzaepfel2023croco} can provide strong prior for cross-view reasoning. 
Diffusion models are another powerful source of priors, where Marigold~\cite{ke2024repurposing} and DepthCrafter~\cite{hu2024depthcrafter} demonstrate impressive detailed outputs in single-image and video depth estimation respectively.
Like in previous works, we adopt large-scale pretraining by using a video encoder pre-trained with MAE; however, we solve this task along with other 4D perception tasks in a unified framework.

\noindent
\textbf{Optical flow estimation.}
Though straightforward when posed as a dense prediction task, optical flow traditionally requires specialized architectures leveraging domain knowledge to be competitive~\cite{teed2020raft,sun2018pwc,wang2024gflow,xu2023unifying}.
Similar to depth estimation, recent works adopt powerful priors from large-scale pretraining, \eg via diffusion priors~\cite{saxena2024surprising} or large-scale cross-view completion pretraining~\cite{weinzaepfel2023croco}.
Most related to ours are approaches that take multi-frame inputs~\cite{ren2019fusion,janai2018unsupervised,shi2023videoflow,dong2024memflow}.
They generally showcase improved accuracy and stability in predictions by leveraging temporal coherence; 
however they all rely on heavily customized networks, in contrast to ours using only generic architectures.

\noindent
\textbf{Motion-based segmentation.}
Early learning-based approaches rely on combining appearance features and optical flow to solve this task~\cite{bharadhwaj2024track2act,fragkiadaki2015motionseg,jain2017fusionseg,Tokmakov2017motionv2,Tokmakov2017motionv1}.
Recent works extract geometric properties from optical flow before using it to train a binary classification network. 
For example, MoA-Net~\cite{bideau2019moanet} compensates for the camera rotation in the optical flow, and RigidMask~\cite{yang2021rigidmotion} goes further by extracting optical expansion~\cite{yang2020upgrading} signal from flow and using single-image depth priors.
However, they are prone to noises in the estimated flow and are limited by their two-frame formulation.
We show that using priors from our video-based approach, we can achieve good performance by only training a task-specific head for this task.

\subsection{Sparse Prediction Tasks}
Tracking Any Point (TAP) in a video for long durations is fundamental for understanding complex dynamics of the world and has many applications~\cite{wen2024anypointtrajectory, bharadhwaj2024track2act,lei2024mosca, wang2023vggsfm,cheng2023segment}.
Particle Video~\cite{harley2022particle}, PIPs~\cite{bian2023contextpips} and TAP-Net~\cite{doersch2022tap} lay the initial foundation by adopting several ideas from optical flow approaches like building cost-maps between query points and image features, iterative estimation of tracks, etc.
On the other hand, OmniMotion~\cite{wang2023omnimotion} optimizes a volumetric representation for each video to solve this task, but its time-consuming nature limits its applicability.
TAPIR~\cite{doersch2023tapir} introduces the idea of coarse-to-fine track estimation with a global temporal refinement strategy and BootsTAPIR~\cite{doersch2024bootstap} further improves it by adopting a self-supervised learning approach.
CoTracker~\cite{karaev2023cotracker} proposes to jointly track multiple points to leverage spatial correlations and achieve strong 2D tracking performance while still being an online approach.
SpaTracker~\cite{spatracker} introduces one of the first feedforward 3D point tracking approaches by using depthmaps from a depth estimator to uplift pixels to 3D and leveraging CoTracker's tracking formulation to track points in 3D.

Unlike previous approaches that rely on carefully designed, specialized architectures, our solution solves both the 2D and 3D point tracking tasks in a single unified framework shared with other tasks. 
We share some motivations with Tracking at Any Granularity~\cite{harley2024tag}, a concurrent work. 
While they focus on training a large model from scratch on a large collection of datasets for tracking various types of queries, we focus on leveraging large-scale pre-trained models to solve the point tracking task as one of the many capabilities afforded by our unified framework.