%! TEX Root = ../main.tex
\begin{figure*}[htbp]
    \captionsetup{skip=4pt}
    \centering
    \includegraphics[width=\textwidth]{figures/method/sparse_head.pdf}
    \caption{Detailed structure for the sparse head. The tokens extracted from the video and the I/O tokens (query $\mathcal{P}$oint token, query point $\mathcal{F}$eature token, and query point $\mathcal{H}$eat map, $\mathcal{D}$epth, and $\mathcal{V}$isibility) tokens are processed by a SAM-style two-way attention layer. The outputs are then reshaped and resized. The resulting per-frame featuremaps and $\mathcal{H}$eat map, $\mathcal{D}$epth, and $\mathcal{V}$isibility tokens are combined via a dot-product. We also implement a memory mechanism that combines the video tokens and the $\mathcal{F}$eature token from overlapping windows.%time $t$ and time $t+1$.
    }
    \label{fig:sparse_head}
\end{figure*}


We provide an overview of our approach in Figure~\ref{fig:method}.
Our model uses a pre-trained ViT-based video encoder~\cite{wang2023videomaev2} (Section~\ref{sec:method_mae}) to capture spatio-temporal features in an RGB video clip of length $T$.
We use lightweight task-specific heads that decode the video features for low-level 3D/4D perception tasks.
For pixel-wise dense tasks like depth, flow, and motion-based segmentation, we design a DPT-based head~\cite{ranftl2021dpt}, originally only designed for images, to work with videos (Section~\ref{sec:method_dense}).
For the sparse task of tracking any pixel in a video, we take inspiration from the head architecture proposed in SAM~\cite{kirillov2023SAM}~(Section~\ref{sec:method_sparse}).
Given a pixel queried in any frame of the input video, we extend the head, also originally designed to work for images, to decode video tokens into a 2D trajectory, its depth with respect to the camera, and the track visibility in each frame~(Section~\ref{sec:windowTrack}).
The VideoMAE architecture extracts video tokens from windows of fixed length $T$, and cannot process longer sequences.
To allow for an online approach, we propose a memory mechanism to track points in arbitrarily long videos~(Section~\ref{sec:memory}).

\subsection{Video Masked Auto-Encoders}\label{sec:method_mae}

Motivated by scalable and powerful pre-training methods and architectures, we use the ViT-based video encoder from VideoMAEv2~\cite{wang2023videomaev2}, which was pre-trained using the masked auto-encoding task.
The encoder works with videos of size $T \times H \times W$, uses a spatio-temporal patch-size of $t \times h \times w$ and cube embedding~\cite{tong2022videomae} to transform an input video into a sequence of tokens.
These are then processed by the ViT architecture with spatio-temporal attention to generate the video tokens $\mathcal{S} \in \mathbb{R}^{P \times C}$, where $P$ is the number of tokens and $C$ is the embedding dimension.
We run the video encoder only once per video clip.
Once they are encoded, we can apply the lightweight heads to decode the tokens to the desired output.
For the point tracking task, we can independently prompt these tokens to track many points in parallel. 

\subsection{Dense Prediction Heads}\label{sec:method_dense}
Dense prediction tasks produce outputs with spatial dimensions aligned with their inputs, typically at the same resolution $H \times W$.  
A wide array of common computer vision problems can be formulated as dense prediction tasks. 
In this work, we explore depth estimation, optical flow estimation, and motion-based segmentation as examples.

It is often critical to capture both local and global spatial structures for dense prediction tasks to succeed. 
We opt to adapt DPT~\cite{ranftl2021dpt} as our dense prediction head due to its proven performance and efficiency on single-image depth estimation. 
DPT progressively assembles and combines tokens from various layers inside transformers to produce full-resolution predictions. 
To leverage the 3D tokens from VideoMAE and to enable temporal reasoning, we replace all 2D convolutions inside the DPT head with 3D convolutions. 
We find that this modification is enough to bring in temporal consistency with minimal computation overhead. 
The DPT heads for each of the dense tasks differ only in the final layer, which outputs one channel for depth and motion-based segmentation, and two channels for optical flow.

For videos longer than $T$ frames, we run inference with stride $T/2$.
To enforce consistency, we use an affine transformation to align the depth predictions for frames at the overlap of consecutive windows. 
This strategy has no effect on the individual windows for relative depth, but it greatly improves long-term temporal consistency.
For optical flow and motion-based segmentation, we simply overwrite the overlapping predictions. 


\begin{figure*}
    \captionsetup{skip=0pt}
    \begin{center}
    \includegraphics[width=0.9\textwidth]{figures/teaser/teaser.pdf}
    \end{center}
    \caption{
        We show results for all perception tasks supported by \methodName. 
        Both examples show dynamic scenes with camera and object motions.}\label{fig:all_in_one}
\end{figure*}

\subsection{Sparse Prediction Heads}\label{sec:method_sparse}

Given a pixel prompt, $(t_i,x_i,y_i)$, in a video, we want to estimate the corresponding 3D trajectory, $\mathcal{T}_i=\{\hat{x}_i(t),\hat{y}_i(t),\hat{d}_i(t),\hat{v}_i(t)\}_{t=0}^{S-1}$, where at time $t$, $(\hat{x}_i(t),\hat{y}_i(t))$ denotes the 2D track location, $\hat{d}_i(t)$ is the track depth with respect to camera, and $\hat{v}_i(t)$ is the track visibility indicating if a track is visible or occluded.
This is a challenging task since it requires tracking the pixel in 2D when visible, tracking it through occlusions, and reasoning about the depth of the track.
Moreover, our video encoder has limited temporal context, since it can only process videos with fixed temporal window of $T$ frames, and we want to enable tracking for arbitrarily long videos with $S>T$ frames.
This makes adapting a general-purpose head particularly challenging.
To tackle this, we adapt an online approach.
We propose a head that allows us to estimate the 3D track for an input pixel prompt within the temporal context ($T$ frames) of the video encoder.
For online estimation beyond $T$ frames, we propose a memory mechanism for our head and a recipe to train it efficiently (see Figure~\ref{fig:sparse_head} for an overview).

\subsubsection{Tracking within the Temporal Context}\label{sec:windowTrack}
Posing the sparse tracking task within our unified framework requires special care. 
Instead of directly estimating point-track positions, we propose to represent tracks using dense probability heatmaps.
Casting tracking as a problem of estimating pixel-aligned 2D maps affords us a shared representation between sparse and dense tasks, which is critical for using a shared backbone.
To achieve this, we adapt the prompt-encoding and mask-decoding mechanisms from SAM~\cite{kirillov2023SAM}.
The input pixel prompt is encoded using 3D positional encoding and a learnable embedding to generate input point token $\mathcal{P}$ with embedding dimension of $C$.
Similarly, we define output tokens with learnable embeddings to estimate different components of a 3D track: a heatmap token ($\mathcal{H}$) to estimate the 2D pixel position of the track across the video, a depth ($\mathcal{D}$) and a visibility ($\mathcal{V}$) token. 
Input and output tokens interact with the video tokens, $\mathcal{S}$, also encoded using 3D positional encoding, using a two-way attention mechanism to decode the video features.
These video features are then reshaped and up-sampled, and a final inner product with the processed output tokens gives us output masks of size $T \times H \times W$.
For the 2D track estimation, we interpret this output mask as a probability heatmap that encodes the 2D track position and apply a 2D soft-argmax operation to estimate the 2D track position $(\hat{x}_i(t),\hat{y}_i(t))$ at each frame $t$.
For depth and visibility, we simply apply a 2D average pooling operation, followed by exponential and sigmoid operations respectively to estimate the track depth $\hat{d}_i(t)$ and the visibility $\hat{v}_i(t)$ at each frame $t$.
This simple design also allows us to query points anywhere in the video and track them in parallel.
We adapt two-way attention from SAM~\cite{kirillov2023SAM} and keep this head lightweight by using only two two-way attention layers.
We also replace the 2D convolutions in the original mask-decoder of SAM with 3D convolutions to enable temporal reasoning.

\subsubsection{A Memory Mechanism for Long Videos}\label{sec:memory} 
To track beyond a single window of length $T$ frames, we adopt an online approach.

A na\"{i}ve approach is to chain tracks across windows.
Given two consecutive and overlapping windows, and a 2D track estimated in the first one, we can use a point on the track in the temporal overlap between the two windows as the query for tracking in the second one.
To pick a good point to chain the tracks, we can select the one with highest visibility score.
However, this solution is brittle.
First, a tracked point may not be visible in the overlap between the windows.
To tackle this problem, inspired by Karaev~\etal~\cite{karaev2023cotracker}, we introduce a track-feature token $\mathcal{F}$ that is passed to subsequent windows as an additional prompt (see Figure~\ref{fig:sparse_head}).
However, unlike Karaev~\etal, we do not initialize it explicitly with the local appearance around the query point, so the two-way attention head is free to capture the most useful information to track through occlusions.
Second, the na\"{i}ve solution described above does not allow the system to reason across temporal windows, which makes it prone to drifting or to losing tracks.
The track-feature tokens help, but to provide even more cross-window information, we pass the video tokens decoded by the two-way attention stage of the current window to the next, as shown in Figure~\ref{fig:sparse_head}.
We achieve this by projecting the decoded video tokens in the overlapping region via a linear layer, and by adding them to the corresponding incoming video tokens to the two-way attention stage in the next window.
Our memory strategy based on these two mechanisms is critical to allow proper reasoning across temporal windows as shown by the comparison in ablation study in Section~\ref{sec:ablation}.


\noindent\textbf{Online training.} Training the memory mechanism requires unrolled-window training~\cite{karaev2023cotracker}, in which we compute the video features for all the overlapping windows in a video of length $S$, and then compute the tracks for the entire video in an online fashion.
However, training such an approach end-to-end is prohibitive due to memory constraints.
To alleviate this, we adapt a two-stage training strategy.
First, we train only for a single window but train all the parameters of our network, and in a second stage we freeze all but the last few layers of our video encoder, and fine-tune it along with the tracking head for unrolled window training.
In Tables~\ref{table:tapvid3d2dtrack} and~\ref{table:tapvid3d3dtrack} we show this approach improves the performance over the na\"{i}ve chaining approach.
