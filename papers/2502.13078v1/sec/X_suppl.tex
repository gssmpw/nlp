%! TEX Root = ../main.tex
\clearpage
\section*{\huge Supplementary Material}
\setcounter{page}{1}
\setcounter{section}{0}
\setcounter{table}{0}

In this supplementary document, we provide comparisons for the inference time, and discuss additional implementation details regarding architecture, dataset and training.
Please refer to our project webpage for a high-level overview and video results and comparisons.

\section{Inference time}
We compare inference time for several approaches we compared against in our paper in Table~\ref{tab:inferencetime}.
For a video clip of size $16\times224\times224$, our inference runs in around 300ms on NVIDIA A6000 to generate output for all the tasks we address in our paper.
While our approach does not provide latency low-enough for real-time applications, its inference time of 19ms per-frame makes it at least comparable speed-wise to methods specialized for each task.
Our ability to solve multiple perception tasks under a reasonable computational envelope could be potentially useful for many applications like robotics, autonomous vehicles, etc.

\section{Training datasets}
Our video encoder~\cite{wang2023videomaev2} has been pre-trained on 1.35M video clips across various data sources using masked auto-encoding.
To fine-tune our model, we use a limited number of synthetic datasets covering varying a range of annotations, and rely on the priors from the video encoder for generalization.

\noindent
\textbf{Kubric~\cite{greff2022kubric}.}
This synthetic dataset has multi-object interactions with many random objects.
We use it to generate annotations for depth, flow, motion-based segmentation, and 2D/3D tracking.
Each video is 24 frames long, and we use a total of 15k videos from the movi-e and movi-f subsets of the data.
Kubric provides meta-data for object and camera trajectories, which could be used to generate 3D tracks.
We follow official guidelines and generate the annotations for 3D tracking by sampling around 8-12k tracks in each video.
For motion-based segmentation, we use the camera-pose and the 3D track information to detect which 3D tracks come from dynamic vs. static objects, which we then combine with provided rigid-object segmentations to generate annotations for motion-based segmentation.

\noindent
\textbf{PointOdyssey~\cite{zheng2023pointodyssey}.}
We use this synthetic dataset for depth and 2D/3D tracking annotations.
The dataset consists of 159 videos, averaging 2k frames long, with human and object interactions under different lighting and atmospheric conditions.
We sample smaller video clips from the long videos to form our dataset.

\noindent
\textbf{DynamicReplica~\cite{karaev2023dynamicstereo}.}
We use this synthetic dataset for depth, flow and 2D/3D tracking annotations.
The dataset consists of 524 videos with humans and animals in motion, and we sample smaller video clips to form our dataset.
Since this dataset has higher fps videos, we sample videos with strides of 1, 2 and 4.

\noindent
\textbf{TartanAir~\cite{tartanair2020iros}.}
Finally, to increase the scene-level data distribution we use TartanAir to generate annotations for flow and depth.
The data is collected in photo-realistic simulation environments, with both indoor and outdoor scenes, in the presence of various light and weather conditions.
We sample smaller video clips from this dataset.


\begin{table}
    \centering
    \resizebox{.8\columnwidth}{!}{
      \begin{tabular}{l|ccc}
        \multicolumn{1}{c|}{} & Task & GPU & Per-frame time (ms) \\
        \hline
        \multicolumn{1}{l|}{RAFT~\cite{teed2020raft}} & Optical flow & A100 & 29 \\
        \multicolumn{1}{l|}{MemFlow~\cite{dong2024memflow}} & Optical flow & A100 & 48 \\
        \multicolumn{1}{l|}{DepthAnything~\cite{yang2024depthanything}} & Depth & A100 & 10 \\
        \multicolumn{1}{l|}{DepthCrafter~\cite{hu2024depthcrafter}} & Depth & A100 & 436 \\
        \multicolumn{1}{l|}{RigidMask~\cite{yang2021rigidmotion}} & Motion Seg. & V100 & 260 \\
        \multicolumn{1}{l|}{SpaTracker (w/o depth)~\cite{spatracker}} & 3D Track & A6000 & 17 \\
        \multicolumn{1}{l|}{Ours} & All & A6000 & 19 \\
      \end{tabular}
    }
    \caption{\textbf{Inference time.} 
    We compare our per-frame inference time with several task-specific approaches and show at least comparable speed-wise to methods specialized for each task. 
    SpaTracker inference time is measured without the depth estimation.}
    \label{tab:inferencetime}
\end{table} 


\section{Architecture}
Our video encoder~\cite{wang2023videomaev2} processes video-clips of size $16 \times 224\times224$.
It uses a patch-size of $2\times14\times14$, which results in $P=2048$ video tokens, and an embedding dimension of $C=1408$.
It has 40 encoder blocks, and we use output from blocks $[14, 21, 28, 36]$ for DPT heads for dense tasks, while our sparse-head uses features from the last block.
We adapt DPT head~\cite{ranftl2021dpt} without any modifications, and only replace 2D-convolutions with 3D.
For the sparse head, we adapt two-way attention from SAM~\cite{kirillov2023SAM} without any modifications and following SAM we keep this head lightweight by using only two two-way attention layers.
We replace the 2D convolutions in the original mask-decoder of SAM with 3D convolutions.
All our input/output tokens are of dimension $C=1408$ and use learnable embeddings.
We use 3D positional encoding for the point query token and the video tokens for the two-way attention stage.


\section{Training}
We train in two-stages. 
In the first stage we train all the parameters of our model for depth, flow and 3D tracking on a single window of $T=16$ frames.
During this stage, for each video we estimate all $T$ frames for dense tasks and for 3D tracking we construct a batch of 80 tracks and select point queries randomly across the visible parts of tracks.
In the second stage, we further fine-tune our model using unrolled-window training.
To train this stage efficiently, we only train for the 2D and 3D tracking tasks and freeze all the parameters, except the last three layers (37-39) of the video encoder and the sparse head.
We train on videos of length $S=40$, windows of size 16 frames and a stride of 8, which results in 4 overlapping windows, and we construct a batch of 48 tracks during this stage, but generate point queries only in the first 20 frames to force the network to learn long-range tracking.
We use AdamW optimizer with a maximum learning rate of $5\times10^{-5}$ and a cosine annealing strategy.
Both stages use a batch size of 8, are trained on a single 8-GPU NVIDIA A100 node for 100k iterations and take 1 day and 2 days respectively to train.

For depth, we use SILog~\cite{eigen2014depth} loss, for flow, we use L1-loss on the estimated uv-offsets, and for tracking, we use L1-loss for the 2D track positions, scale-invariant loss for the track depth (similar to dense depth), and binary-cross entropy loss for the track visibility.
We found the loss weights empirically by first weighting the losses to be in the same order of magnitude and then doing a small hyperparameter search around those weights.
We use the loss weights of $20$ for the flow and depth losses. For tracking, we use loss weights of $1.0$, $20.0$, and $15.0$ for 2D track position loss, track depth loss and track visibility loss, respectively.
