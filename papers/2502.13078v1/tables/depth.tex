%! TEX Root = ../main.tex
\begin{table}
\begin{center}
    \resizebox{\columnwidth}{!}{
    \begin{tabular}{rccccccccccc}
        \toprule
        & \multicolumn{2}{c}{Sintel ($\sim$50 frames)} & \multicolumn{2}{c}{ScanNet} (90 frames) & \multicolumn{2}{c}{KITTI} ($\sim$110 frames) & \multicolumn{2}{c}{Bonn} (110 frames) & \multicolumn{2}{c}{NYUv2} (1 frame)\\
        \cmidrule(lr{0.1em}){2-3}\cmidrule(lr{0.1em}){4-5}\cmidrule(lr{0.1em}){6-7}\cmidrule(lr{0.1em}){8-9}\cmidrule(lr{0.1em}){10-11}
        & AbsRel $\downarrow$ & $\delta_1$ $\uparrow$ & AbsRel $\downarrow$ & $\delta_1$ $\uparrow$ & AbsRel $\downarrow$ & $\delta_1$ $\uparrow$ & AbsRel $\downarrow$ & $\delta_1$ $\uparrow$ & AbsRel $\downarrow$ & $\delta_1$ $\uparrow$  \\
        \midrule
        Marigold~\cite{ke2024repurposing} & $0.532$ & $0.515$ & $0.166$ & $0.769$ & $0.149$ & $0.796$ & $0.091$ & $0.931$ & $0.070$ & $0.946$  \\
        DA~\cite{yang2024depthanything} & $0.325$ & $0.564$ & $0.130$ & $0.838$ & $0.142$ & $0.803$ & $0.078$ & $0.939$ & $\mathbf{0.042}$ & $\mathbf{0.981}$  \\
        DA-V2~\cite{yang2024depthanythingv2} & $0.367$ & $0.554$ & $0.135$ & $0.822$ & $0.140$ & $0.804$ & $0.106$ & $0.921$ & $\underline{0.043}$ & $\underline{0.978}$  \\
        \midrule
        NVDS~\cite{wang2023nvds} & $0.408$ & $0.483$ & $0.187$ & $0.677$ & $0.253$ & $0.588$ & $0.167$ & $0.766$ & $0.151$ & $0.780$  \\
        ChronoDepth~\cite{shao2024chronodepth}  & $0.587$ & $0.486$ & $0.159$ & $0.783$ & $0.167$ & $0.759$ & $0.100$ & $0.911$ & $0.073$ & $0.941$  \\
        DepthCrafter~\cite{hu2024depthcrafter} & $0.270$ & $\mathbf{0.697}$ & $0.123$ & $0.856$ & $0.104$ & $0.896$ & $0.071$ & $\underline{0.972}$ & $0.072$ & $0.948$  \\
        \midrule
        \methodName-depth (Ours) & $\underline{0.251}$ & $0.659$ & $0.102$ & $0.895$ & $0.099$ & $0.916$ & $0.061$ & $\underline{0.972}$ & $0.078$ & $0.932$  \\
        \methodName-depth* (Ours) & $0.267$ & $\underline{0.693}$ & $\mathbf{0.070}$ & $\mathbf{0.954}$ & $0.097$ & $0.903$ & $\mathbf{0.057}$ & $\underline{0.972}$ & $0.078$ & $0.932$  \\
        \methodName (Ours) & $0.263$ & $0.662$ & $0.103$ & $0.898$ & $\underline{0.093}$ & $\underline{0.924}$ & $0.060$ & $\mathbf{0.973}$ & $0.081$ & $0.925$  \\
        \methodName* (Ours) & $\mathbf{0.247}$ & $0.691$ & $\underline{0.072}$ & $\underline{0.951}$ & $\mathbf{0.090}$ & $\mathbf{0.928}$ & $\underline{0.058}$ & $\mathbf{0.973}$ & $0.081$ & $0.925$  \\
        \bottomrule
    \end{tabular}
    }
\end{center}
\caption{\textbf{Zero-shot depth estimation results.} 
We compare our methods against both single-image baselines (Row 1-3) and SOTA video depth estimation approaches (Row 4-6). 
\methodName-depth refers to our model trained specifically for depth estimation and \methodName refers to the version trained jointly for all tasks. 
Models marked * have predictions in overlapping windows aligned using the strategy described in Section~\ref{sec:method_dense}.
On video datasets (all except NYUv2), our model consistently performs better than DepthCrafter, the closest competition, and by a large margin on ScanNet and KITTI. 
\textbf{Best} and \underline{second best} results are highlighted.
}\label{tab:depth}
\end{table}
