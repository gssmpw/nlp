% Use this file for citations not found in the ACL Anthology (contained in "anthology.bib").

@book{Aho:72,
    author  = {Alfred V. Aho and Jeffrey D. Ullman},
    title   = {The Theory of Parsing, Translation and Compiling},
    year    = "1972",
    volume  = "1",
    publisher = {Prentice-Hall},
    address = {Englewood Cliffs, NJ}
}

@book{APA:83,
    author  = {{American Psychological Association}},
    title   = {Publications Manual},
    year    = "1983",
    publisher = {American Psychological Association},
    address = {Washington, DC}
}

@article{Chandra:81,
	author = {Ashok K. Chandra and Dexter C. Kozen and Larry J. Stockmeyer},
	year = "1981",
	title = {Alternation},
	journal = {Journal of the Association for Computing Machinery},
	volume = "28",
	number = "1",
	pages = "114--133",
	doi = "10.1145/322234.322243",
}

@inproceedings{andrew2007scalable,
  title={Scalable training of {$L_1$}-regularized log-linear models},
  author={Andrew, Galen and Gao, Jianfeng},
  booktitle={Proceedings of the 24th International Conference on Machine Learning},
  pages={33--40},
  year={2007},
  url={https://dl.acm.org/doi/abs/10.1145/1273496.1273501}
}

@book{Gusfield:97,
    author  = {Dan Gusfield},
    title   = {Algorithms on Strings, Trees and Sequences},
    year    = "1997",
    publisher = {Cambridge University Press},
    address = {Cambridge, UK},
    url={https://www.cambridge.org/core/books/algorithms-on-strings-trees-and-sequences/F0B095049C7E6EF5356F0A26686C20D3}
}

@article{rasooli-tetrault-2015,
    author    = {Mohammad Sadegh Rasooli and Joel R. Tetreault},
    title     = {Yara Parser: {A} Fast and Accurate Dependency Parser},
    journal   = {Computing Research Repository},
    volume    = {arXiv:1503.06733},
    year      = {2015},
    url       = {http://arxiv.org/abs/1503.06733},
    note    = {version 2}
}

@article{Ando2005,
	Acmid = {1194905},
	Author = {Ando, Rie Kubota and Zhang, Tong},
	Issn = {1532-4435},
	Issue_Date = {12/1/2005},
	Journal = {Journal of Machine Learning Research},
	Month = dec,
	Numpages = {37},
	Pages = {1817--1853},
	Publisher = {JMLR.org},
	Title = {A Framework for Learning Predictive Structures from Multiple Tasks and Unlabeled Data},
	Volume = {6},
	Year = {2005},
	url={https://www.jmlr.org/papers/volume6/ando05a/ando05a.pdf}
}

@article{ct1965,
  title={An algorithm for the machine calculation of complex {F}ourier series},
  author={Cooley, James W. and Tukey, John W.},
  journal={Mathematics of Computation},
  volume={19},
  number={90},
  pages={297--301},
  year={1965},
  url={https://www.ams.org/journals/mcom/1965-19-090/S0025-5718-1965-0178586-1/S0025-5718-1965-0178586-1.pdf}
}

@article{shinn2023reflexion,
  title={Reflexion: Language Agents with Verbal Reinforcement Learning},
  author={Shinn, Noah and Cassano, Federico and Berman, Edward and Gopinath, Ashwin and Narasimhan, Karthik and Yao, Shunyu},
  journal={arXiv preprint arXiv:2303.11366},
  year={2023},
  url={https://doi.org/10.48550/arXiv.2303.11366}
}

@article{yuksekgonul2024textgrad,
  title={TextGrad: Automatic "Differentiation" via Text},
  author={Yuksekgonul, Mert and Bianchi, Federico and Boen, Joseph and Liu, Sheng and Huang, Zhi and Guestrin, Carlos and Zou, James},
  journal={arXiv preprint arXiv:2406.07496},
  year={2024},
  url={https://doi.org/10.48550/arXiv.2406.07496}
}

@article{thirunavukarasu2023large,
  title={Large language models in medicine},
  author={Thirunavukarasu, Anuj J. and Ting, Daniel S. J. and Elangovan, Karthik and Gutierrez, Luis and Tan, Tien F. and Ting, Daniel S. W.},
  journal={Nature Medicine},
  volume={29},
  number={8},
  pages={1930--1940},
  year={2023},
  doi={10.1038/s41591-023-02448-8}
}

@misc{openai2023gpt4,
  title={GPT-4 Technical Report},
  author={OpenAI},
  year={2023},
  url={https://cdn.openai.com/papers/gpt-4.pdf}
}

@misc{singhal2022medpalm,
  title={Med-PaLM: Large Language Models Encode Clinical Knowledge},
  author={Singhal, Karan and Azizi, Shekoofeh and Tu, Tong N. and Mahdavi, Shravya S. and Wei, Jason and Chung, Hyung Won and Scales, Nathan and Tanwani, Aneesh and Cole-Lewis, Heather and Pfohl, Stephen and others},
  year={2022},
  url={https://sites.research.google/med-palm/}
}

@misc{touvron2024llama3,
  title={The Llama 3 Herd of Models},
  author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timothée and Rozière, Baptiste and Goyal, Naman and Batra, Shagun and Haziza, Daniel and others},
  year={2024},
  url={https://ai.meta.com/research/publications/the-llama-3-herd-of-models/}
}



@misc{poon2020domain,
  title={Domain-Specific Language Model Pretraining for Biomedical Natural Language Processing},
  author={Poon, Hoifung and Gao, Jianfeng},
  year={2020},
  url={https://www.microsoft.com/en-us/research/blog/domain-specific-language-model-pretraining-for-biomedical-natural-language-processing/}
}

@article{jin2020what,
  title={What Disease Does This Patient Have? A Large-Scale Open Domain Question Answering Dataset from Medical Exams},
  author={Jin, Di and Pan, Eileen and Oufattole, Nassim and Weng, Wei-Hung and Fang, Hanyi and Szolovits, Peter},
  journal={arXiv preprint arXiv:2009.13081},
  year={2020},
  url={https://doi.org/10.48550/arXiv.2009.13081}
}

@article{jin2019pubmedqa,
  title={PubMedQA: A Dataset for Biomedical Research Question Answering},
  author={Jin, Qiao and Dhingra, Bhuwan and Liu, Zhengping and Cohen, William W. and Lu, Xinghua},
  journal={arXiv preprint arXiv:1909.06146},
  year={2019},
  url={https://doi.org/10.48550/arXiv.1909.06146}
}

@article{maharjan2024openmedlm,
  title={OpenMedLM: Prompt Engineering Can Out-Perform Fine-Tuning in Medical Question-Answering with Open-Source Large Language Models},
  author={Maharjan, Jaya and Garikipati, Anusha and Singh, N. P. and others},
  journal={Scientific Reports},
  volume={14},
  number={1},
  pages={14156},
  year={2024},
  doi={10.1038/s41598-024-64827-6}
}

@article{nori2023generalist,
  author    = {Harsha Nori and Yin Tat Lee and Sheng Zhang and Dean Carignan and Richard Edgar and Nicolo Fusi and Nicholas King and Jonathan Larson and Yuanzhi Li and Weishung Liu and Renqian Luo and Scott Mayer McKinney and Robert Osazuwa Ness and Hoifung Poon and Tao Qin and Naoto Usuyama and Chris White and Eric Horvitz},
  title     = {Can Generalist Foundation Models Outcompete Special-Purpose Tuning? Case Study in Medicine},
  journal   = {Microsoft Research},
  year      = {2023},
  url       = {https://www.microsoft.com/en-us/research/publication/can-generalist-foundation-models-outcompete-special-purpose-tuning-case-study-in-medicine/}
}

@article{bashir2010information,
  title={An Information-Theoretic Perspective on Overfitting and Underfitting.” arXiv, Nov. 06, 2020. doi: 10.48550},
  author={Bashir, D and Montanez, GD and Sehra, S and Segura, P Sandoval and Lauw, J},
  journal={arXiv},
  year={2010}

}
@article{durall2020combating,
  title={Combating mode collapse in gan training: An empirical analysis using hessian eigenvalues},
  author={Durall, Ricard and Chatzimichailidis, Avraam and Labus, Peter and Keuper, Janis},
  journal={arXiv preprint arXiv:2012.09673},
  year={2020}
}
@article{khattab2023dspy,
  title={DSPy: Compiling Declarative Language Model Calls into Self-Improving Pipelines},
  author={Khattab, Omar and Singhvi, Arnav and Maheshwari, Paridhi and Zhang, Zhiyuan and Santhanam, Keshav and Vardhamanan, Sri and Haq, Saiful and Sharma, Ashutosh and Joshi, Thomas T. and Moazam, Hanna and others},
  journal={arXiv preprint arXiv:2310.03714},
  year={2023},
  doi={10.48550/arXiv.2310.03714}
}

@article{guo2023connecting,
  title={Connecting Large Language Models with Evolutionary Algorithms Yields Powerful Prompt Optimizers},
  author={Guo, Qingyan and Wang, Rui and Guo, Junliang and Li, Bei and Song, Kaitao and Tan, Xu and Liu, Guoqing and Bian, Jiang and Yang, Yujiu},
  journal={arXiv preprint arXiv:2309.08532},
  year={2023},
  doi={10.48550/arXiv.2309.08532}
}

@article{01ai2024yi,
  author = {{01.AI}},
  title = {Yi: Open Foundation Models by 01.AI},
  journal = {arXiv preprint arXiv:2403.04652},
  year = {2024},
  month = {mar},
  eprint = {2403.04652},
  archivePrefix = {arXiv},
  primaryClass = {cs.LG}
}

@article{pal2022medmcqa,
  title={MedMCQA : A Large-scale Multi-Subject Multi-Choice Dataset for Medical domain Question Answering},
  author={Pal, Ankit and et al},
  journal={arXiv preprint arXiv:2203.14371},
  year={2022},
  eprint={2203.14371},
  archivePrefix={arXiv},
  primaryClass={cs.CL}
}

@article{hu2021lora,
  title={Lora: Low-rank adaptation of large language models},
  author={Hu, Edward J and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
  journal={arXiv preprint arXiv:2106.09685},
  year={2021}
}
@article{dong2022survey,
  title={A survey on in-context learning},
  author={Dong, Qingxiu and Li, Lei and Dai, Damai and Zheng, Ce and Ma, Jingyuan and Li, Rui and Xia, Heming and Xu, Jingjing and Wu, Zhiyong and Liu, Tianyu and others},
  journal={arXiv preprint arXiv:2301.00234},
  year={2022}
}
@article{wu2024benchmarking,
  title={Benchmarking open-source large language models, GPT-4 and Claude 2 on multiple-choice questions in nephrology},
  author={Wu, Sean and Koo, Michael and Blum, Lesley and Black, Andy and Kao, Liyo and Fei, Zhe and Scalzo, Fabien and Kurtz, Ira},
  journal={NEJM AI},
  volume={1},
  number={2},
  pages={AIdbp2300092},
  year={2024},
  publisher={Massachusetts Medical Society}
}@article{wu2024adversarial,
  title={Adversarial Databases Improve Success in Retrieval-based Large Language Models},
  author={Wu, Sean and Koo, Michael and Kao, Li Yo and Black, Andy and Blum, Lesley and Scalzo, Fabien and Kurtz, Ira},
  journal={arXiv preprint arXiv:2407.14609},
  year={2024}
}
@article{dubey2024llama,
  title={The llama 3 herd of models},
  author={Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and Kadian, Abhishek and Al-Dahle, Ahmad and Letman, Aiesha and Mathur, Akhil and Schelten, Alan and Yang, Amy and Fan, Angela and others},
  journal={arXiv preprint arXiv:2407.21783},
  year={2024}
}
@article{chen2023meditron,
  title={Meditron-70b: Scaling medical pretraining for large language models},
  author={Chen, Zeming and Cano, Alejandro Hern{\'a}ndez and Romanou, Angelika and Bonnet, Antoine and Matoba, Kyle and Salvi, Francesco and Pagliardini, Matteo and Fan, Simin and K{\"o}pf, Andreas and Mohtashami, Amirkeivan and others},
  journal={arXiv preprint arXiv:2311.16079},
  year={2023}
}
@article{chung2024scaling,
  title={Scaling instruction-finetuned language models},
  author={Chung, Hyung Won and Hou, Le and Longpre, Shayne and Zoph, Barret and Tay, Yi and Fedus, William and Li, Yunxuan and Wang, Xuezhi and Dehghani, Mostafa and Brahma, Siddhartha and others},
  journal={Journal of Machine Learning Research},
  volume={25},
  number={70},
  pages={1--53},
  year={2024}
}
@article{liu2024deepseek,
  title={Deepseek-v3 technical report},
  author={Liu, Aixin and Feng, Bei and Xue, Bing and Wang, Bingxuan and Wu, Bochao and Lu, Chengda and Zhao, Chenggang and Deng, Chengqi and Zhang, Chenyu and Ruan, Chong and others},
  journal={arXiv preprint arXiv:2412.19437},
  year={2024}
}
@article{almazrouei2023falcon,
  title={The falcon series of open language models},
  author={Almazrouei, Ebtesam and Alobeidli, Hamza and Alshamsi, Abdulaziz and Cappelli, Alessandro and Cojocaru, Ruxandra and Debbah, M{\'e}rouane and Goffinet, {\'E}tienne and Hesslow, Daniel and Launay, Julien and Malartic, Quentin and others},
  journal={arXiv preprint arXiv:2311.16867},
  year={2023}
}
@article{jiang2024mixtral,
  title={Mixtral of experts},
  author={Jiang, Albert Q and Sablayrolles, Alexandre and Roux, Antoine and Mensch, Arthur and Savary, Blanche and Bamford, Chris and Chaplot, Devendra Singh and Casas, Diego de las and Hanna, Emma Bou and Bressand, Florian and others},
  journal={arXiv preprint arXiv:2401.04088},
  year={2024}
}
@article{chu2024qwen2,
  title={Qwen2-audio technical report},
  author={Chu, Yunfei and Xu, Jin and Yang, Qian and Wei, Haojie and Wei, Xipin and Guo, Zhifang and Leng, Yichong and Lv, Yuanjun and He, Jinzheng and Lin, Junyang and others},
  journal={arXiv preprint arXiv:2407.10759},
  year={2024}
}
@article{guluzade2025elmtex,
  title={ELMTEX: Fine-Tuning Large Language Models for Structured Clinical Information Extraction. A Case Study on Clinical Reports},
  author={Guluzade, Aynur and Heiba, Naguib and Boukhers, Zeyd and Hamiti, Florim and Polash, Jahid Hasan and Mohamad, Yehya and Velasco, Carlos A},
  journal={arXiv preprint arXiv:2502.05638},
  year={2025}
}
@article{opsahl2024optimizing,
  title={Optimizing Instructions and Demonstrations for Multi-Stage Language Model Programs},
  author={Opsahl-Ong, Krista and Ryan, Michael J and Purtell, Josh and Broman, David and Potts, Christopher and Zaharia, Matei and Khattab, Omar},
  journal={arXiv preprint arXiv:2406.11695},
  year={2024}
}
@article{ernst2024introduction,
  title={Introduction to reinforcement learning},
  author={Ernst, Damien and Louette, Arthur},
  journal={Feuerriegel, S., Hartmann, J., Janiesch, C., and Zschech, P},
  pages={111--126},
  year={2024}
}
@article{pryzant2023automatic,
  title={Automatic prompt optimization with" gradient descent" and beam search},
  author={Pryzant, Reid and Iter, Dan and Li, Jerry and Lee, Yin Tat and Zhu, Chenguang and Zeng, Michael},
  journal={arXiv preprint arXiv:2305.03495},
  year={2023}
}
@inproceedings{li2024promptist,
  title={PROMPTIST: Automated Prompt Optimization for Text-to-Image Synthesis},
  author={Li, WeiJie and Wang, Jin and Zhang, Xuejie},
  booktitle={CCF International Conference on Natural Language Processing and Chinese Computing},
  pages={295--306},
  year={2024},
  organization={Springer}
}
@article{kepel2024autonomous,
  title={Autonomous prompt engineering in large language models},
  author={Kepel, Daan and Valogianni, Konstantina},
  journal={arXiv preprint arXiv:2407.11000},
  year={2024}
}
@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}
@article{wei2022chain,
  title={Chain-of-thought prompting elicits reasoning in large language models},
  author={Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Xia, Fei and Chi, Ed and Le, Quoc V and Zhou, Denny and others},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={24824--24837},
  year={2022}
}
@article{ge2023few,
  title={Few-shot learning for medical text: A review of advances, trends, and opportunities},
  author={Ge, Yao and Guo, Yuting and Das, Sudeshna and Al-Garadi, Mohammed Ali and Sarker, Abeed},
  journal={Journal of Biomedical Informatics},
  pages={104458},
  year={2023},
  publisher={Elsevier}
}

@article{singhal2025toward,
  title={Toward expert-level medical question answering with large language models},
  author={Singhal, Karan and Tu, Tao and Gottweis, Juraj and Sayres, Rory and Wulczyn, Ellery and Amin, Mohamed and Hou, Le and Clark, Kevin and Pfohl, Stephen R and Cole-Lewis, Heather and others},
  journal={Nature Medicine},
  pages={1--8},
  year={2025},
  publisher={Nature Publishing Group US New York}
}