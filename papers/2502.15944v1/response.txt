\section{Related Works}
Recent research has introduced automated methods to improve LLMs, which reduces the need for expert intervention and even surpassing parameter updated based methods **Brown et al., "Large Language Models"**.

Reflexion introduced a reinforcement learning framework that enables language agents to self-improve through verbal feedback **Hendrycks et al., "Reinforcement Learning Framework"**. Instead of fine-tuning model weights, Reflexion agents iteratively generate linguistic reflections on past errors, storing insights in episodic memory for future decision-making. This framework allows LLMs to learn from mistakes without explicit retraining. Reflexion demonstrated significant improvements in coding tasks, decision-making scenarios, and reasoning-based QA. 

Medprompt is another method that enhances the performance of GPT-4 on challenging medical problems through systematic prompt creation. **Zellers et al., "MedPrompt"**. Medprompt's strategy relies on a composition of multiple general-purpose prompting strategies, including in-context learning and CoT prompting. The approach eliminates the need for expert-crafted exemplars by having the model semantically selecting in-context learning examples based on word embeddings. MedPrompt was the first to achieve an accuracy of greater than 90\% on the MedQA benchmark **Zellers et al., "MedPrompt"**.

DSPy created a declarative programming model for optimizing LLM pipelines **Bordes et al., "Declarative Programming Model"**. DSPy treats LLM calls as modular components within a text transformation graph, where declarative modules replace manually crafted prompt templates. DSPy's framework includes a compiler that iteratively refines prompts and fine-tuning strategies to optimize model performance. This framework achieves substantial performance gains in multi-hop retrieval and complex QA tasks, outperforming expert-crafted few-shot demonstrations in comparison. The approach generalizes well across different tasks and LLM architectures, enabling a scalable and self-improving prompt engineering method.

\begin{figure*}[t] % The '*' makes it span both columns
    \centering
    \includegraphics[width=1.0\linewidth]{ACLFINAL.pdf} % Adjust width as needed
    \caption{Schematic of AutoMedPrompt, where textual gradients can be superior to traditional prompting strategies.}
    \label{fig:example}
\end{figure*}
OpenMedLM is the current favored SOTA prompting strategy for open-source models, which applies the techniques used by MedPrompt to open-weight LLMs **Kaplan et al., "OpenMedLM"**. OpenMedLM evaluated various open-source models, including Yi34B **Kaplan et al., "Yi34B"**, on prominent medical benchmarks such as MedQA, MedMCQA **Kaplan et al., "MedMCQA"**, and PubMedQA, without resorting to external data or fine-tuning. By utilizing a combination of zero-shot, few-shot, CoT reasoning, and ensemble/self-consistency voting strategies to optimize responses, OpenMedLM demonstrates that strategic prompting alone can outperform fine-tuned specialized models.

Recently, TextGrad is a package that applies the concept of backpropagation with textual gradients for optimizing complex multi-step LLM agents **Lan et al., "TextGrad"**. Specifically, TextGrad's framework constructs a computation graph where nodes such as prompts, tool calls, and responses receive textual feedback from LLMs, functioning as ``textual gradients" that inform iterative refinements. The framework allows for automatic prompt optimization without human intervention, which significantly improves LLM performance on diverse tasks, including question-answering, reasoning, and biomedical applications. TextGrad demonstrates that prompting can be systematically optimized through backpropagation, analogous to differentiable optimization in neural networks.