@inproceedings{Banerjee,
	author = {Banerjee, Pradeep Kr. and Karhadkar, Kedar and Wang, Yu Guang and Alon, Uri and Mont\'{u}far, Guido},
	title = {Oversquashing in GNNs through the Lens of Information Contraction and Graph Expansion},
	year = {2022},
	publisher = {IEEE Press},
	url = {https://doi.org/10.1109/Allerton49937.2022.9929363},
	doi = {10.1109/Allerton49937.2022.9929363},
	abstract = {The quality of signal propagation in message-passing graph neural networks (GNNs) strongly influences their expressivity as has been observed in recent works. In particular, for prediction tasks relying on long-range interactions, recursive aggregation of node features can lead to an undesired phenomenon called “oversquashing”. We present a framework for analyzing oversquashing based on information contraction. Our analysis is guided by a model of reliable computation due to von Neumann that lends a new insight into oversquashing as signal quenching in noisy computation graphs. Building on this, we propose a graph rewiring algorithm aimed at alleviating oversquashing. Our algorithm employs a random local edge flip primitive motivated by an expander graph construction. We compare the spectral expansion properties of our algorithm with that of an existing curvature-based non-local rewiring strategy. Synthetic experiments show that while our algorithm in general has a slower rate of expansion, it is overall computationally cheaper, preserves the node degrees exactly and never disconnects the graph.},
	booktitle = {2022 58th Annual Allerton Conference on Communication, Control, and Computing (Allerton)},
	pages = {1–8},
	numpages = {8},
	location = {Monticello, IL, USA}
}

@inproceedings{Fosr,
	title={Fo{SR}: First-order spectral rewiring for addressing oversquashing in {GNN}s},
	author={Kedar Karhadkar and Pradeep Kr. Banerjee and Guido Montufar},
	booktitle={The Eleventh International Conference on Learning Representations },
	year={2023},
	url={https://openreview.net/forum?id=3YjQfCLdrzz}
}

@article{NT2019RevisitingGN,
	title={Revisiting Graph Neural Networks: All We Have is Low-Pass Filters},
	author={Hoang NT and Takanori Maehara},
	journal={ArXiv},
	year={2019},
	volume={abs/1905.09550}
}

@misc{borf,
	title={Revisiting Over-smoothing and Over-squashing Using Ollivier-Ricci Curvature}, 
	author={Khang Nguyen and Hieu Nong and Vinh Nguyen and Nhat Ho and Stanley Osher and Tan Nguyen},
	year={2023},
	eprint={2211.15779},
	archivePrefix={arXiv},
	primaryClass={cs.LG}
}

@article{chandraeffective,
	author = {Chandra, Ashok K. and Raghavan, Prabhakar and Ruzzo, Walter L. and Smolensky, Roman and Tiwari, Prasoon},
	date = {1996/12/01},
	date-added = {2024-01-21 13:42:29 +0100},
	date-modified = {2024-01-21 13:43:03 +0100},
	doi = {10.1007/BF01270385},
	id = {Chandra1996},
	isbn = {1420-8954},
	journal = {computational complexity},
	number = {4},
	pages = {312--340},
	title = {The electrical resistance of a graph captures its commute and cover times},
	url = {https://doi.org/10.1007/BF01270385},
	volume = {6},
	year = {1996},
	bdsk-url-1 = {https://doi.org/10.1007/BF01270385}}

@inproceedings{dichotomoy,
title={Characterizing Graph Datasets for Node Classification: Homophily-Heterophily Dichotomy and Beyond},
author={Oleg Platonov and Denis Kuznedelev and Artem Babenko and Liudmila Prokhorenkova},
booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
year={2023},
url={https://openreview.net/forum?id=m7PIJWOdlY}
}

@misc{diffwire,
	doi = {10.48550/ARXIV.2206.07369},
	
	url = {https://arxiv.org/abs/2206.07369},
	
	author = {Arnaiz-Rodríguez, Adrián and Begga, Ahmed and Escolano, Francisco and Oliver, Nuria},
	
	keywords = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), FOS: Computer and information sciences, FOS: Computer and information sciences},
	
	title = {DiffWire: Inductive Graph Rewiring via the Lovász Bound},
	
	publisher = {arXiv},
	
	year = {2022},
}

@misc{digiovanni2023oversquashing,
	title={On Over-Squashing in Message Passing Neural Networks: The Impact of Width, Depth, and Topology}, 
	author={Francesco Di Giovanni and Lorenzo Giusti and Federico Barbero and Giulia Luise and Pietro Lio' and Michael Bronstein},
	year={2023},
	eprint={2302.02941},
	archivePrefix={arXiv},
	primaryClass={cs.LG}
}

@inproceedings{effectiveresistance,
author = {Black, Mitchell and Wan, Zhengchao and Nayyeri, Amir and Wang, Yusu},
title = {Understanding oversquashing in GNNs through the lens of effective resistance},
year = {2023},
publisher = {JMLR.org},
abstract = {Message passing graph neural networks (GNNs) are a popular learning architectures for graph-structured data. However, one problem GNNs experience is oversquashing, where a GNN has difficulty sending information between distant nodes. Understanding and mitigating oversquashing has recently received significant attention from the research community. In this paper, we continue this line of work by analyzing oversquashing through the lens of the effective resistance between nodes in the input graph. Effective resistance intuitively captures the "strength" of connection between two nodes by paths in the graph, and has a rich literature spanning many areas of graph theory. We propose to use total effective resistance as a bound of the total amount of oversquashing in a graph and provide theoretical justification for its use. We further develop an algorithm to identify edges to be added to an input graph to minimize the total effective resistance, thereby alleviating oversquashing. We provide empirical evidence of the effectiveness of our total effective resistance based rewiring strategies for improving the performance of GNNs.},
booktitle = {Proceedings of the 40th International Conference on Machine Learning},
articleno = {107},
numpages = {20},
location = {Honolulu, Hawaii, USA},
series = {ICML'23}
}

@inproceedings{hamilton,
	title={The ricci flow on surfaces},
	author={Richard Hamilton},
	booktitle={ Mathematics and general relativity, Proceedings of the AMS-IMS-SIAM Joint Summer Research Conference in the Mathematical Sciences on Mathematics in General Relativity},
	year={1988}
}

@inproceedings{homonecessity,
title={Is Homophily a Necessity for Graph Neural Networks?},
author={Yao Ma and Xiaorui Liu and Neil Shah and Jiliang Tang},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=ucASPPD9GKN}
}

@article{interplaycommunity,
	abstract = {Graph Neural Networks (GNNs) are effective in many applications. Still, there is a limited understanding of the effect of common graph structures on the learning process of GNNs. To fill this gap, we study the impact of community structure and homophily on the performance of GNNs in semi-supervised node classification on graphs. Our methodology consists of systematically manipulating the structure of eight datasets, and measuring the performance of GNNs on the original graphs and the change in performance in the presence and the absence of community structure and/or homophily. Our results show the major impact of both homophily and communities on the classification accuracy of GNNs, and provide insights on their interplay. In particular, by analyzing community structure and its correlation with node labels, we are able to make informed predictions on the suitability of GNNs for classification on a given graph. Using an information-theoretic metric for community-label correlation, we devise a guideline for model selection based on graph structure. With our work, we provide insights on the abilities of GNNs and the impact of common network phenomena on their performance. Our work improves model selection for node classification in semi-supervised settings.},
	author = {Hussain, Hussain and Duricic, Tomislav and Lex, Elisabeth and Helic, Denis and Kern, Roman},
	date = {2021/10/26},
	date-added = {2024-05-05 09:34:06 +0200},
	date-modified = {2024-05-05 09:34:33 +0200},
	doi = {10.1007/s41109-021-00423-1},
	id = {Hussain2021},
	isbn = {2364-8228},
	journal = {Applied Network Science},
	number = {1},
	pages = {80},
	title = {The interplay between communities and homophily in semi-supervised classification using graph neural networks},
	url = {https://doi.org/10.1007/s41109-021-00423-1},
	volume = {6},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1007/s41109-021-00423-1}}

@inproceedings{jamadandi2024spectral,
title={Spectral Graph Pruning Against Over-Squashing and Over-Smoothing},
author={Adarsh Jamadandi and Celia Rubio-Madrigal and Rebekka Burkholz},
booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},
year={2024}
}

@inproceedings{keriven2022not,
  author={Nicolas Keriven},
  title={Not too little, not too much: a theoretical analysis of graph (over)smoothing},
  year={2022},
  cdate={1640995200000},
  url={http://papers.nips.cc/paper_files/paper/2022/hash/0f956ca6f667c62e0f71511773c86a59-Abstract-Conference.html},
  booktitle={NeurIPS},
}

@inproceedings{labelawaregcn,
author = {Chen, Hao and Xu, Yue and Huang, Feiran and Deng, Zengde and Huang, Wenbing and Wang, Senzhang and He, Peng and Li, Zhoujun},
title = {Label-Aware Graph Convolutional Networks},
year = {2020},
isbn = {9781450368599},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3340531.3412139},
doi = {10.1145/3340531.3412139},
abstract = {Recent advances in Graph Convolutional Networks (GCNs) have led to state-of-the-art performance on various graph-related tasks. However, most existing GCN models do not explicitly identify whether all the aggregated neighbors are valuable to the learning tasks, which may harm the learning performance. In this paper, we consider the problem of node classification and propose the Label-Aware Graph Convolutional Network (LAGCN) framework which can directly identify valuable neighbors to enhance the performance of existing GCN models. Our contribution is three-fold. First, we propose a label-aware edge classifier that can filter distracting neighbors and add valuable neighbors for each node to refine the original graph into a label-aware (LA) graph. Existing GCN models can directly learn from the LA graph to improve the performance without changing their model architectures. Second, we introduce the concept of positive ratio to evaluate the density of valuable neighbors in the LA graph. Theoretical analysis reveals that using the edge classifier to increase the positive ratio can improve the learning performance of existing GCN models. Third, we conduct extensive node classification experiments on benchmark datasets. The results verify that LAGCN can improve the performance of existing GCN models considerably, in terms of node classification.},
booktitle = {Proceedings of the 29th ACM International Conference on Information \& Knowledge Management},
pages = {1977–1980},
numpages = {4},
keywords = {node classification, neural networks, graph convolutional networks},
location = {Virtual Event, Ireland},
series = {CIKM '20}
}

@InProceedings{li2019deepgcns,
	title={DeepGCNs: Can GCNs Go as Deep as CNNs?},
	author={Guohao Li and Matthias Müller and Ali Thabet and Bernard Ghanem},
	booktitle={The IEEE International Conference on Computer Vision (ICCV)},
	year={2019}
}

@book{mixingtimes,
  added-at = {2010-01-19T17:51:27.000+0100},
  author = {Levin, David A. and Peres, Yuval and Wilmer, Elizabeth L.},
  biburl = {https://www.bibsonomy.org/bibtex/2097dc4d1d0e412b2444f540b04110797/tmalsburg},
  interhash = {61354795a6accb6407bfdbf04753a683},
  intrahash = {097dc4d1d0e412b2444f540b04110797},
  keywords = {markovchains probabilitytheory textbook},
  publisher = {American Mathematical Society},
  timestamp = {2010-01-19T17:51:27.000+0100},
  title = {{Markov chains and mixing times}},
  year = 2006
}

@inproceedings{mustafa2023are,
	title={Are {GAT}s Out of Balance?},
	author={Nimrah Mustafa and Aleksandar Bojchevski and Rebekka Burkholz},
	booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
	year={2023},
	url={https://openreview.net/forum?id=qY7UqLoora}
}

@inproceedings{mustafa2024gate,
  title = {{GATE}: How to Keep Out Intrusive Neighbors},
  author = {Mustafa, Nimrah and Burkholz, Rebekka},
  booktitle = {Forty-first International Conference on Machine Learning},
  year = {2024},
  url = {https://openreview.net/forum?id=Sjv5RcqfuH},
}

@inproceedings{ono,
	title={Graph Neural Networks Exponentially Lose Expressive Power for Node Classification},
	author={Kenta Oono and Taiji Suzuki},
	booktitle={International Conference on Learning Representations},
	year={2020}
}

@inproceedings{quachem,
	author = {Gilmer, Justin and Schoenholz, Samuel S. and Riley, Patrick F. and Vinyals, Oriol and Dahl, George E.},
	title = {Neural Message Passing for Quantum Chemistry},
	year = {2017},
	publisher = {JMLR.org},
	abstract = {Supervised learning on molecules has incredible potential to be useful in chemistry, drug discovery, and materials science. Luckily, several promising and closely related neural network models invariant to molecular symmetries have already been described in the literature. These models learn a message passing algorithm and aggregation procedure to compute a function of their entire input graph. At this point, the next step is to find a particularly effective variant of this general approach and apply it to chemical prediction benchmarks until we either solve them or reach the limits of the approach. In this paper, we reformulate existing models into a single common framework we call Message Passing Neural Networks (MPNNs) and explore additional novel variations within this framework. Using MPNNs we demonstrate state of the art results on an important molecular property prediction benchmark; these results are strong enough that we believe future work should focus on datasets with larger molecules or more accurate ground truth labels.},
	booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
	pages = {1263–1272},
	numpages = {10},
	location = {Sydney, NSW, Australia},
	series = {ICML'17}
}

@misc{salez2021sparse,
	title={Sparse expanders have negative curvature}, 
	author={Justin Salez},
	year={2021},
	eprint={2101.08242},
	archivePrefix={arXiv},
	primaryClass={math.PR}
}

@inproceedings{sjlr,
	author = {Giraldo, Jhony H. and Skianis, Konstantinos and Bouwmans, Thierry and Malliaros, Fragkiskos D.},
	title = {On the Trade-off between Over-Smoothing and Over-Squashing in Deep Graph Neural Networks},
	year = {2023},
	isbn = {9798400701245},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3583780.3614997},
	doi = {10.1145/3583780.3614997},
	abstract = {Graph Neural Networks (GNNs) have succeeded in various computer science applications, yet deep GNNs underperform their shallow counterparts despite deep learning's success in other domains. Over-smoothing and over-squashing are key challenges when stacking graph convolutional layers, hindering deep representation learning and information propagation from distant nodes. Our work reveals that over-smoothing and over-squashing are intrinsically related to the spectral gap of the graph Laplacian, resulting in an inevitable trade-off between these two issues, as they cannot be alleviated simultaneously. To achieve a suitable compromise, we propose adding and removing edges as a viable approach. We introduce the Stochastic Jost and Liu Curvature Rewiring (SJLR) algorithm, which is computationally efficient and preserves fundamental properties compared to previous curvature-based methods. Unlike existing approaches, SJLR performs edge addition and removal during GNN training while maintaining the graph unchanged during testing. Comprehensive comparisons demonstrate SJLR's competitive performance in addressing over-smoothing and over-squashing.},
	booktitle = {Proceedings of the 32nd ACM International Conference on Information and Knowledge Management},
	pages = {566–576},
	numpages = {11},
	keywords = {curvature, graph neural networks, over-smoothing, over-squashing},
	location = {Birmingham, United Kingdom},
	series = {CIKM '23}
}

@article{zhou2021dirichlet,
	title={Dirichlet energy constrained learning for deep graph neural networks},
	author={Zhou, Kaixiong and Huang, Xiao and Zha, Daochen and Chen, Rui and Li, Li and Choi, Soo-Hyun and Hu, Xia},
	journal={Advances in neural information processing systems},
	year={2021}
}

