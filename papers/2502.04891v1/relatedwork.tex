\section{Related work}
\textbf{Graph rewiring.}
A key component for GNNs is the input graph, since it not only acts as the data for model training but is also the computational structure on which \emph{message passing} \citep{quachem} is performed. Real-world graphs, however, can be noisy and sub-optimal for downstream tasks. For example, recent studies have pointed out issues like over-squashing \citep{alon2021on,topping2022understanding,digiovanni2023oversquashing}, caused by topological bottlenecks, which affect how information is diffused. This highlights the importance of the graph topology and begs the question: how can we obtain an optimal computational structure that aligns with the downstream task? Graph rewiring has emerged as a popular technique to effect changes to the edge structure. This can be done based on various criteria. For instance, \citet{topping2022understanding,sjlr,borf} propose to use different variants of Ricci curvature \citep{hamilton} to rewire the graph, while \citet{effectiveresistance} propose the effective resistance \citep{chandraeffective}, and \citet{Banerjee,deac2022expander} transform the input graph into an expander graph \citep{salez2021sparse} for efficient message passing. 
Edges can be added or deleted and even though GNNs should be able to learn to drop task-irrelevant neighbors, trainability and expressiveness issues can limit this ability \citep{mustafa2023are,mustafa2024gate}, which explains why edge deletions can also help fight over-smoothing in addition to over-squashing \citep{jamadandi2024spectral}.

\textbf{Spectral gap maximization.}
Contemporaneously, spectral-based methods such as \citet{Fosr} aim to \textit{maximize} the spectral gap by edge additions, as a larger spectral gap is inherently linked to faster mixing time \citep{mixingtimes} and thus better information flow. However, this can be detrimental in the case of heterophilic graphs \citep{homonecessity,dichotomoy} as we might add edges between nodes of different labels resulting in over-smoothing \citep{li2019deepgcns,NT2019RevisitingGN,ono,zhou2021dirichlet,keriven2022not}. The spectral gap can also be maximized by deleting edges \citep{jamadandi2024spectral} and this has shown to be beneficial in slowing down detrimental over-smoothing while simultaneously mitigating over-squashing, especially in heterophilic settings. Contrarily, \citet{diffwire} advocate for spectral gap \textit{minimization}, but do not explain when this could be advantageous. 

\textbf{Graph and task alignment.}
Our findings reveal that the underlying mechanism enhancing GNN performance by rewiring actually depends on whether we modify edges connecting nodes with similar or dissimilar features, that are usually associated with similar or dissimilar labels. 
In fact, \citet{interplaycommunity} take a first step in this direction by analysing the interplay between community and node-labels. 
They propose an information-theoretic metric, and demonstrate its impact on performance by artificially creating and destroying communities in real-world graphs. This also highlights the importance of the positive influence of same-label neighbours and how different-label neighbours can impair node classification performance \citep{labelawaregcn}.
We take this analysis several steps further and analyze why spectral rewiring cannot induce this alignment (\autoref{th:sbmsgproof}).  

The desirability of alignment between the graph structure and the task in GNNs has been explored in the context of their training dynamics by \citet{yang2024how}. This study theoretically analyzes how GNN models tend to align their Neural Tangent Kernel (NTK) matrix $\mathbf{\Theta}_t$ with the adjacency matrix $A$ of the input graph. 
They further derive a generalization bound for the NTK regime without considering node features, specifically in cases where the adjacency matrix $A$ is well-aligned with the optimal kernel matrix $\mathbf{\Theta}^*$. 
This matrix $\mathbf{\Theta}^*$ precisely indicates whether a pair of nodes share the same label, making this concept of alignment similar to ours \textemdash though not explicitly referring to the graph's communities\textemdash~and to the concept of homophily. 
Our theory on SBMs supports this result on GNN performance, while additionally relating it to the denoising effect of node features by their neighborhoods (\autoref{th:sbmperfproof}) and considering different levels of alignment (\autoref{th:sbmnoiseproof}).