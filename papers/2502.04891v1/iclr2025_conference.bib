@article{JMLR:v18:16-480,
  author  = {Emmanuel Abbe},
  title   = {Community Detection and Stochastic Block Models: Recent Developments},
  journal = {Journal of Machine Learning Research},
  year    = {2018},
  volume  = {18},
  number  = {177},
  pages   = {1--86},
  url     = {http://jmlr.org/papers/v18/16-480.html}
}

@inproceedings{Fey/Lenssen/2019,
  title={Fast Graph Representation Learning with {PyTorch Geometric}},
  author={Fey, Matthias and Lenssen, Jan E.},
  booktitle={ICLR Workshop on Representation Learning on Graphs and Manifolds},
  year={2019},
}
@inproceedings{mmgnn,
author = {Bi, Wendong and Du, Lun and Fu, Qiang and Wang, Yanlin and Han, Shi and Zhang, Dongmei},
title = {MM-GNN: Mix-Moment Graph Neural Network towards Modeling Neighborhood Feature Distribution},
year = {2023},
isbn = {9781450394079},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3539597.3570457},
doi = {10.1145/3539597.3570457},
abstract = {Graph Neural Networks (GNNs) have shown expressive performance on graph representation learning by aggregating information from neighbors. Recently, some studies have discussed the importance of modeling neighborhood distribution on the graph. However, most existing GNNs aggregate neighbors' features through single statistic (e.g., mean, max, sum), which loses the information related to neighbor's feature distribution and therefore degrades the model performance. In this paper, inspired by the method of moment in statistical theory, we propose to model neighbor's feature distribution with multi-order moments. We design a novel GNN model, namely Mix-Moment Graph Neural Network (MM-GNN), which includes a Multi-order Moment Embedding (MME) module and an Element-wise Attention-based Moment Adaptor module. MM-GNN first calculates the multi-order moments of the neighbors for each node as signatures, and then use an Element-wise Attention-based Moment Adaptor to assign larger weights to important moments for each node and update node representations. We conduct extensive experiments on 15 real-world graphs (including social networks, citation networks and web-page networks etc.) to evaluate our model, and the results demonstrate the superiority of MM-GNN over existing state-of-the-art models.},
booktitle = {Proceedings of the Sixteenth ACM International Conference on Web Search and Data Mining},
pages = {132–140},
numpages = {9},
keywords = {feature distribution, graph neural networks, graph representation learning, moment, social networks},
location = {Singapore, Singapore},
series = {WSDM '23}
}


@inproceedings{grapsage,
author = {Hamilton, William L. and Ying, Rex and Leskovec, Jure},
title = {Inductive representation learning on large graphs},
year = {2017},
isbn = {9781510860964},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Low-dimensional embeddings of nodes in large graphs have proved extremely useful in a variety of prediction tasks, from content recommendation to identifying protein functions. However, most existing approaches require that all nodes in the graph are present during training of the embeddings; these previous approaches are inherently transductive and do not naturally generalize to unseen nodes. Here we present GraphSAGE, a general inductive framework that leverages node feature information (e.g., text attributes) to efficiently generate node embeddings for previously unseen data. Instead of training individual embeddings for each node, we learn a function that generates embeddings by sampling and aggregating features from a node's local neighborhood. Our algorithm outperforms strong baselines on three inductive node-classification benchmarks: we classify the category of unseen nodes in evolving information graphs based on citation and Reddit post data, and we show that our algorithm generalizes to completely unseen graphs using a multi-graph dataset of protein-protein interactions.},
booktitle = {Proceedings of the 31st International Conference on Neural Information Processing Systems},
pages = {1025–1035},
numpages = {11},
location = {Long Beach, California, USA},
series = {NIPS'17}
}

@inproceedings{mustafa2024gate,
  title = {{GATE}: How to Keep Out Intrusive Neighbors},
  author = {Mustafa, Nimrah and Burkholz, Rebekka},
  booktitle = {Forty-first International Conference on Machine Learning},
  year = {2024},
  url = {https://openreview.net/forum?id=Sjv5RcqfuH},
}

@inproceedings{lsgnn,
author = {Chen, Yuhan and Luo, Yihong and Tang, Jing and Yang, Liang and Qiu, Siya and Wang, Chuan and Cao, Xiaochun},
title = {LSGNN: towards general graph neural network in node classification by local similarity},
year = {2023},
isbn = {978-1-956792-03-4},
url = {https://doi.org/10.24963/ijcai.2023/395},
doi = {10.24963/ijcai.2023/395},
abstract = {Heterophily has been considered as an issue that hurts the performance of Graph Neural Networks (GNNs). To address this issue, some existing work uses a graph-level weighted fusion of the information of multi-hop neighbors to include more nodes with homophily. However, the heterophily might differ among nodes, which requires to consider the local topology. Motivated by it, we propose to use the local similarity (LocalSim) to learn node-level weighted fusion, which can also serve as a plug-and-play module. For better fusion, we propose a novel and efficient Initial Residual Difference Connection (IRDC) to extract more informative multi-hop information. Moreover, we provide theoretical analysis on the effectiveness of LocalSim representing node homophily on synthetic graphs. Extensive evaluations over real benchmark datasets show that our proposed method, namely Local Similarity Graph Neural Network (LSGNN), can offer comparable or superior state-of-the-art performance on both homophilic and heterophilic graphs. Meanwhile, the plug-and-play model can significantly boost the performance of existing GNNs.},
booktitle = {Proceedings of the Thirty-Second International Joint Conference on Artificial Intelligence},
articleno = {395},
numpages = {9},
location = {Macao, P.R.China},
series = {IJCAI '23}
}

@article{wang2019dgl,
    title={Deep Graph Library: A Graph-Centric, Highly-Performant Package for Graph Neural Networks},
    author={Minjie Wang and Da Zheng and Zihao Ye and Quan Gan and Mufei Li and Xiang Song and Jinjing Zhou and Chao Ma and Lingfan Yu and Yu Gai and Tianjun Xiao and Tong He and George Karypis and Jinyang Li and Zheng Zhang},
    year={2019},
    journal={arXiv preprint arXiv:1909.01315}
}

@article{Blondel_2008,
doi = {10.1088/1742-5468/2008/10/P10008},
url = {https://dx.doi.org/10.1088/1742-5468/2008/10/P10008},
year = {2008},
month = {oct},
publisher = {},
volume = {2008},
number = {10},
pages = {P10008},
author = {Vincent D Blondel and Jean-Loup Guillaume and Renaud Lambiotte and Etienne Lefebvre},
title = {Fast unfolding of communities in large networks},
journal = {Journal of Statistical Mechanics: Theory and Experiment},
abstract = {We propose a simple method to extract the community structure of large networks. Our method is a heuristic method that is based on modularity optimization. It is shown to outperform all other known community detection methods in terms of computation time. Moreover, the quality of the communities detected is very good, as measured by the so-called modularity. This is shown first by identifying language communities in a Belgian mobile phone network of 2 million customers and by analysing a web graph of 118 million nodes and more than one billion links. The accuracy of our algorithm is also verified on ad hoc modular networks.}
}




@inproceedings{chen2020idgl,
author = {Chen, Yu and Wu, Lingfei and Zaki, Mohammed J.},
title = {Iterative deep graph learning for graph neural networks: better and robust node embeddings},
year = {2020},
isbn = {9781713829546},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {In this paper, we propose an end-to-end graph learning framework, namely Iterative Deep Graph Learning (IDGL), for jointly and iteratively learning graph structure and graph embedding. The key rationale of IDGL is to learn a better graph structure based on better node embeddings, and vice versa (i.e., better node embeddings based on a better graph structure). Our iterative method dynamically stops when the learned graph structure approaches close enough to the graph optimized for the downstream prediction task. In addition, we cast the graph learning problem as a similarity metric learning problem and leverage adaptive graph regularization for controlling the quality of the learned graph. Finally, combining the anchor-based approximation technique, we further propose a scalable version of IDGL, namely IDGL-ANCH, which significantly reduces the time and space complexity of IDGL without compromising the performance. Our extensive experiments on nine benchmarks show that our proposed IDGL models can consistently outperform or match the state-of-the-art baselines. Furthermore, IDGL can be more robust to adversarial graphs and cope with both transductive and inductive learning.},
booktitle = {Proceedings of the 34th International Conference on Neural Information Processing Systems},
articleno = {1620},
numpages = {13},
location = {Vancouver, BC, Canada},
series = {NIPS '20}
}


@inproceedings{
yang2024how,
title={How Graph Neural Networks Learn: Lessons from Training Dynamics},
author={Chenxiao Yang and Qitian Wu and David Wipf and Ruoyu Sun and Junchi Yan},
booktitle={Forty-first International Conference on Machine Learning},
year={2024},
url={https://openreview.net/forum?id=Dn4B53IcCW}
}


@inproceedings{Steck_2024, series={WWW ’24},
   title={Is Cosine-Similarity of Embeddings Really About Similarity?},
   volume={201},
   url={http://dx.doi.org/10.1145/3589335.3651526},
   DOI={10.1145/3589335.3651526},
   booktitle={Companion Proceedings of the ACM on Web Conference 2024},
   publisher={ACM},
   author={Steck, Harald and Ekanadham, Chaitanya and Kallus, Nathan},
   year={2024},
   month=may, pages={887–890},
   collection={WWW ’24} }




@article{Silvester_2000, title={Determinants of block matrices}, volume={84}, DOI={10.2307/3620776}, number={501}, journal={Mathematical Gazette}, author={Silvester, John R.}, year={2000}, month=nov, pages={460–467} }


@article{PhysRevE.70.066111,
  title = {Finding community structure in very large networks},
  author = {Clauset, Aaron and Newman, M. E. J. and Moore, Cristopher},
  journal = {Phys. Rev. E},
  volume = {70},
  issue = {6},
  pages = {066111},
  numpages = {6},
  year = {2004},
  month = {Dec},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevE.70.066111},
  url = {https://link.aps.org/doi/10.1103/PhysRevE.70.066111}
}


@misc{ioffe2015batch,
      title={Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift}, 
      author={Sergey Ioffe and Christian Szegedy},
      year={2015},
      eprint={1502.03167},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@misc{wang2020unifying,
      title={Unifying Graph Convolutional Neural Networks and Label Propagation}, 
      author={Hongwei Wang and Jure Leskovec},
      year={2020},
      eprint={2002.06755},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{LPGNN,
  author       = {Qian Huang and
                  Horace He and
                  Abhay Singh and
                  Ser{-}Nam Lim and
                  Austin R. Benson},
  title        = {Combining Label Propagation and Simple Models Out-performs Graph Neural
                  Networks},
  journal      = {CoRR},
  volume       = {abs/2010.13993},
  year         = {2020}
}

@misc{bi2022make,
      title={Make Heterophily Graphs Better Fit GNN: A Graph Rewiring Approach}, 
      author={Wendong Bi and Lun Du and Qiang Fu and Yanlin Wang and Shi Han and Dongmei Zhang},
      year={2022},
      eprint={2209.08264},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{dhgr,
author = {Guo, Jiayan and Du, Lun and Bi, Wendong and Fu, Qiang and Ma, Xiaojun and Chen, Xu and Han, Shi and Zhang, Dongmei and Zhang, Yan},
title = {Homophily-oriented Heterogeneous Graph Rewiring},
year = {2023},
isbn = {9781450394161},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3543507.3583454},
doi = {10.1145/3543507.3583454},
abstract = {With the rapid development of the World Wide Web&nbsp;(WWW), heterogeneous graphs&nbsp;(HG) have explosive growth. Recently, heterogeneous graph neural network&nbsp;(HGNN) has shown great potential in learning on HG. Current studies of HGNN mainly focus on some HGs with strong homophily properties&nbsp;(nodes connected by meta-path tend to have the same labels), while few discussions are made in those that are less homophilous. Recently, there have been many works on homogeneous graphs with heterophily. However, due to heterogeneity, it is non-trivial to extend their approach to deal with HGs with heterophily. In this work, based on empirical observations, we propose a meta-path-induced metric to measure the homophily degree of a HG. We also find that current HGNNs may have degenerated performance when handling HGs with less homophilous properties. Thus it is essential to increase the generalization ability of HGNNs on non-homophilous HGs. To this end, we propose HDHGR, a homophily-oriented deep heterogeneous graph rewiring approach that modifies the HG structure to increase the performance of HGNN. We theoretically verify HDHGR. In addition, experiments on real-world HGs demonstrate the effectiveness of HDHGR, which brings at most more than 10\% relative gain.},
booktitle = {Proceedings of the ACM Web Conference 2023},
pages = {511–522},
numpages = {12},
keywords = {Heterogeneous Graph Neural Network},
location = {<conf-loc>, <city>Austin</city>, <state>TX</state>, <country>USA</country>, </conf-loc>},
series = {WWW '23}
}

@ARTICLE{clusterhypothesis,
  author={Chapelle, O. and Scholkopf, B. and Zien, Eds., A.},
  journal={IEEE Transactions on Neural Networks}, 
  title={Semi-Supervised Learning (Chapelle, O. et al., Eds.; 2006) [Book reviews]}, 
  year={2009},
  volume={20},
  number={3},
  pages={542-542},
  keywords={Semisupervised learning;Books;Unsupervised learning;Hidden Markov models;Benchmark testing;Image processing;Information retrieval;Image retrieval;Bioinformatics;Machine learning},
  doi={10.1109/TNN.2009.2015974}}



@article{spectralclusteringsparse,
author = {Dall'Amico, Lorenzo and Couillet, Romain and Tremblay, Nicolas},
title = {A unified framework for spectral clustering in sparse graphs},
year = {2021},
issue_date = {January 2021},
publisher = {JMLR.org},
volume = {22},
number = {1},
issn = {1532-4435},
abstract = {This article considers spectral community detection in the regime of sparse networks with heterogeneous degree distributions, for which we devise an algorithm to efficiently retrieve communities. Specifically, we demonstrate that a well parametrized form of regularized Laplacian matrices can be used to perform spectral clustering in sparse networks without suffering from its degree heterogeneity. Besides, we exhibit important connections between this proposed matrix and the now popular non-backtracking matrix, the Bethe-Hessian matrix, as well as the standard Laplacian matrix. Interestingly, as opposed to competitive methods, our proposed improved parametrization inherently accounts for the hardness of the classification problem. These findings are summarized under the form of an algorithm capable of both estimating the number of communities and achieving high-quality community reconstruction.},
journal = {J. Mach. Learn. Res.},
month = {jan},
articleno = {217},
numpages = {56},
keywords = {community detection, sparsity, heterogeneous degree distribution, spectral clustering, unsupervised learning}
}

@article{limitations,
  title = {Limitations in the spectral method for graph partitioning: Detectability threshold and localization of eigenvectors},
  author = {Kawamoto, Tatsuro and Kabashima, Yoshiyuki},
  journal = {Phys. Rev. E},
  volume = {91},
  issue = {6},
  pages = {062803},
  numpages = {21},
  year = {2015},
  month = {Jun},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevE.91.062803},
  url = {https://link.aps.org/doi/10.1103/PhysRevE.91.062803}
}



@inproceedings{borde2024neural,
title={Neural Snowflakes: Universal Latent Graph Inference via Trainable Latent Geometries},
author={Haitz S{\'a}ez de Oc{\'a}riz Borde and Anastasis Kratsios},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=djM3WzpOmK}
}

@inproceedings{structuraldisparity,
 author = {Mao, Haitao and Chen, Zhikai and Jin, Wei and Han, Haoyu and Ma, Yao and Zhao, Tong and Shah, Neil and Tang, Jiliang},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Oh and T. Naumann and A. Globerson and K. Saenko and M. Hardt and S. Levine},
 pages = {37013--37067},
 publisher = {Curran Associates, Inc.},
 title = {Demystifying Structural Disparity in Graph Neural Networks: Can One Size Fit All?},
 url = {https://proceedings.neurips.cc/paper_files/paper/2023/file/74f1edadbdf495e7258ee8db7b1d3acd-Paper-Conference.pdf},
 volume = {36},
 year = {2023}
}


@InProceedings{collectivelearning,
  title = 	 {A Collective Learning Framework to Boost GNN Expressiveness for Node Classification},
  author =       {Hang, Mengyue and Neville, Jennifer and Ribeiro, Bruno},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {4040--4050},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  url = 	 {https://proceedings.mlr.press/v139/hang21a.html}
}

@misc{sato2024trainingfree,
      title={Training-free Graph Neural Networks and the Power of Labels as Features}, 
      author={Ryoma Sato},
      year={2024},
      eprint={2404.19288},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{xia2023learning,
title={Learning Invariant Representations of Graph Neural Networks via Cluster Generalization},
author={Donglin Xia and Xiao Wang and Nian Liu and Chuan Shi},
booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
year={2023},
url={https://openreview.net/forum?id=zrCmeqV3Sz}
}

@inproceedings{dichotomoy,
title={Characterizing Graph Datasets for Node Classification: Homophily-Heterophily Dichotomy and Beyond},
author={Oleg Platonov and Denis Kuznedelev and Artem Babenko and Liudmila Prokhorenkova},
booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
year={2023},
url={https://openreview.net/forum?id=m7PIJWOdlY}
}


@inproceedings{supervisedcommunity,
title={Supervised Community Detection with Line Graph Neural Networks},
author={Zhengdao Chen and Lisha Li and Joan Bruna},
booktitle={International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=H1g0Z3A9Fm},
}


@inproceedings{homonecessity,
title={Is Homophily a Necessity for Graph Neural Networks?},
author={Yao Ma and Xiaorui Liu and Neil Shah and Jiliang Tang},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=ucASPPD9GKN}
}
@inproceedings{
han2023mlpinit,
title={{MLPI}nit: Embarrassingly Simple {GNN} Training Acceleration with {MLP} Initialization},
author={Xiaotian Han and Tong Zhao and Yozen Liu and Xia Hu and Neil Shah},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=P8YIphWNEGO}
}

@inproceedings{mlpgood,
title={Graph Neural Networks are Inherently Good Generalizers: Insights by Bridging {GNN}s and {MLP}s},
author={Chenxiao Yang and Qitian Wu and Jiahua Wang and Junchi Yan},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=dqnNW2omZL6}
}

@inproceedings{
huang2024training,
title={Training Graph Transformers via Curriculum-Enhanced Attention Distillation},
author={Yisong Huang and Jin Li and Xinlong Chen and Yang-Geng Fu},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=j4VMrwgn1M}
}


@inproceedings{jamadandi2024spectral,
title={Spectral Graph Pruning Against Over-Squashing and Over-Smoothing},
author={Adarsh Jamadandi and Celia Rubio-Madrigal and Rebekka Burkholz},
booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},
year={2024}
}

@INPROCEEDINGS{distillinggcn,
  author={Yang, Yiding and Qiu, Jiayan and Song, Mingli and Tao, Dacheng and Wang, Xinchao},
  booktitle={2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={Distilling Knowledge From Graph Convolutional Networks}, 
  year={2020},
  volume={},
  number={},
  pages={7072-7081},
  keywords={Knowledge engineering;Three-dimensional displays;Task analysis;Computational modeling;Computer science;Training;Neural networks},
  doi={10.1109/CVPR42600.2020.00710}}



@inproceedings{learningtodistil,
author = {Yang, Cheng and Guo, Yuxin and Xu, Yao and Shi, Chuan and Liu, Jiawei and Wang, Chunchen and Li, Xin and Guo, Ning and Yin, Hongzhi},
title = {Learning to Distill Graph Neural Networks},
year = {2023},
isbn = {9781450394079},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3539597.3570480},
doi = {10.1145/3539597.3570480},
abstract = {Graph Neural Networks (GNNs) can effectively capture both the topology and attribute information of a graph, and have been extensively studied in many domains. Recently, there is an emerging trend that equips GNNs with knowledge distillation for better efficiency or effectiveness. However, to the best of our knowledge, existing knowledge distillation methods applied on GNNs all employed predefined distillation processes, which are controlled by several hyper-parameters without any supervision from the performance of distilled models. Such isolation between distillation and evaluation would lead to suboptimal results. In this work, we aim to propose a general knowledge distillation framework that can be applied on any pretrained GNN models to further improve their performance. To address the isolation problem, we propose to parameterize and learn distillation processes suitable for distilling GNNs. Specifically, instead of introducing a unified temperature hyper-parameter as most previous work did, we will learn node-specific distillation temperatures towards better performance of distilled models. We first parameterize each node's temperature by a function of its neighborhood's encodings and predictions, and then design a novel iterative learning process for model distilling and temperature learning. We also introduce a scalable variant of our method to accelerate model training. Experimental results on five benchmark datasets show that our proposed framework can be applied on five popular GNN models and consistently improve their prediction accuracies with 3.12\% relative enhancement on average. Besides, the scalable variant enables 8 times faster training speed at the cost of 1\% prediction accuracy.},
booktitle = {Proceedings of the Sixteenth ACM International Conference on Web Search and Data Mining},
pages = {123–131},
numpages = {9},
keywords = {graph neural networks, knowledge distillation},
location = {<conf-loc>, <city>Singapore</city>, <country>Singapore</country>, </conf-loc>},
series = {WSDM '23}
}


@misc{tian2023knowledge,
      title={Knowledge Distillation on Graphs: A Survey}, 
      author={Yijun Tian and Shichao Pei and Xiangliang Zhang and Chuxu Zhang and Nitesh V. Chawla},
      year={2023},
      eprint={2302.00219},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}


@inproceedings{labelawaregcn,
author = {Chen, Hao and Xu, Yue and Huang, Feiran and Deng, Zengde and Huang, Wenbing and Wang, Senzhang and He, Peng and Li, Zhoujun},
title = {Label-Aware Graph Convolutional Networks},
year = {2020},
isbn = {9781450368599},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3340531.3412139},
doi = {10.1145/3340531.3412139},
abstract = {Recent advances in Graph Convolutional Networks (GCNs) have led to state-of-the-art performance on various graph-related tasks. However, most existing GCN models do not explicitly identify whether all the aggregated neighbors are valuable to the learning tasks, which may harm the learning performance. In this paper, we consider the problem of node classification and propose the Label-Aware Graph Convolutional Network (LAGCN) framework which can directly identify valuable neighbors to enhance the performance of existing GCN models. Our contribution is three-fold. First, we propose a label-aware edge classifier that can filter distracting neighbors and add valuable neighbors for each node to refine the original graph into a label-aware (LA) graph. Existing GCN models can directly learn from the LA graph to improve the performance without changing their model architectures. Second, we introduce the concept of positive ratio to evaluate the density of valuable neighbors in the LA graph. Theoretical analysis reveals that using the edge classifier to increase the positive ratio can improve the learning performance of existing GCN models. Third, we conduct extensive node classification experiments on benchmark datasets. The results verify that LAGCN can improve the performance of existing GCN models considerably, in terms of node classification.},
booktitle = {Proceedings of the 29th ACM International Conference on Information \& Knowledge Management},
pages = {1977–1980},
numpages = {4},
keywords = {node classification, neural networks, graph convolutional networks},
location = {Virtual Event, Ireland},
series = {CIKM '20}
}

@inproceedings{StrategiesPretraining,
title={Strategies for Pre-training Graph Neural Networks},
author={Weihua Hu and Bowen Liu and Joseph Gomes and Marinka Zitnik and Percy Liang and Vijay Pande and Jure Leskovec},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=HJlWWJSFDH}
}


@InProceedings{SGC,
  title = 	 {Simplifying Graph Convolutional Networks},
  author =       {Wu, Felix and Souza, Amauri and Zhang, Tianyi and Fifty, Christopher and Yu, Tao and Weinberger, Kilian},
  booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
  pages = 	 {6861--6871},
  year = 	 {2019},
  editor = 	 {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume = 	 {97},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {09--15 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v97/wu19e/wu19e.pdf},
  url = 	 {https://proceedings.mlr.press/v97/wu19e.html},
  abstract = 	 {Graph Convolutional Networks (GCNs) and their variants have experienced significant attention and have become the de facto methods for learning graph representations. GCNs derive inspiration primarily from recent deep learning approaches, and as a result, may inherit unnecessary complexity and redundant computation. In this paper, we reduce this excess complexity through successively removing nonlinearities and collapsing weight matrices between consecutive layers. We theoretically analyze the resulting linear model and show that it corresponds to a fixed low-pass filter followed by a linear classifier. Notably, our experimental evaluation demonstrates that these simplifications do not negatively impact accuracy in many downstream applications. Moreover, the resulting model scales to larger datasets, is naturally interpretable, and yields up to two orders of magnitude speedup over FastGCN.}
}


@misc{shchur2019pitfalls,
      title={Pitfalls of Graph Neural Network Evaluation}, 
      author={Oleksandr Shchur and Maximilian Mumme and Aleksandar Bojchevski and Stephan Günnemann},
      year={2019},
      eprint={1811.05868},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{interplaycommunity,
	abstract = {Graph Neural Networks (GNNs) are effective in many applications. Still, there is a limited understanding of the effect of common graph structures on the learning process of GNNs. To fill this gap, we study the impact of community structure and homophily on the performance of GNNs in semi-supervised node classification on graphs. Our methodology consists of systematically manipulating the structure of eight datasets, and measuring the performance of GNNs on the original graphs and the change in performance in the presence and the absence of community structure and/or homophily. Our results show the major impact of both homophily and communities on the classification accuracy of GNNs, and provide insights on their interplay. In particular, by analyzing community structure and its correlation with node labels, we are able to make informed predictions on the suitability of GNNs for classification on a given graph. Using an information-theoretic metric for community-label correlation, we devise a guideline for model selection based on graph structure. With our work, we provide insights on the abilities of GNNs and the impact of common network phenomena on their performance. Our work improves model selection for node classification in semi-supervised settings.},
	author = {Hussain, Hussain and Duricic, Tomislav and Lex, Elisabeth and Helic, Denis and Kern, Roman},
	date = {2021/10/26},
	date-added = {2024-05-05 09:34:06 +0200},
	date-modified = {2024-05-05 09:34:33 +0200},
	doi = {10.1007/s41109-021-00423-1},
	id = {Hussain2021},
	isbn = {2364-8228},
	journal = {Applied Network Science},
	number = {1},
	pages = {80},
	title = {The interplay between communities and homophily in semi-supervised classification using graph neural networks},
	url = {https://doi.org/10.1007/s41109-021-00423-1},
	volume = {6},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1007/s41109-021-00423-1}}

@inproceedings{
Errica2020A,
title={A Fair Comparison of Graph Neural Networks for Graph Classification},
author={Federico Errica and Marco Podda and Davide Bacciu and Alessio Micheli},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=HygDF6NFPB}
}
@inproceedings{
papp2021dropgnn,
title={Drop{GNN}: Random Dropouts Increase the Expressiveness of Graph Neural Networks},
author={P{\'a}l Andr{\'a}s Papp and Karolis Martinkus and Lukas Faber and Roger Wattenhofer},
booktitle={Advances in Neural Information Processing Systems},
editor={A. Beygelzimer and Y. Dauphin and P. Liang and J. Wortman Vaughan},
year={2021},
url={https://openreview.net/forum?id=fpQojkIV5q8}
}


@InProceedings{complexphysics,
  title = 	 {Learning to Simulate Complex Physics with Graph Networks},
  author =       {Sanchez-Gonzalez, Alvaro and Godwin, Jonathan and Pfaff, Tobias and Ying, Rex and Leskovec, Jure and Battaglia, Peter},
  booktitle = 	 {Proceedings of the 37th International Conference on Machine Learning},
  pages = 	 {8459--8468},
  year = 	 {2020},
  editor = 	 {III, Hal Daumé and Singh, Aarti},
  volume = 	 {119},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {13--18 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v119/sanchez-gonzalez20a/sanchez-gonzalez20a.pdf},
  url = 	 {https://proceedings.mlr.press/v119/sanchez-gonzalez20a.html},
  abstract = 	 {Here we present a machine learning framework and model implementation that can learn to simulate a wide variety of challenging physical domains, involving fluids, rigid solids, and deformable materials interacting with one another. Our framework—which we term "Graph Network-based Simulators" (GNS)—represents the state of a physical system with particles, expressed as nodes in a graph, and computes dynamics via learned message-passing. Our results show that our model can generalize from single-timestep predictions with thousands of particles during training, to different initial conditions, thousands of timesteps, and at least an order of magnitude more particles at test time. Our model was robust to hyperparameter choices across various evaluation metrics: the main determinants of long-term performance were the number of message-passing steps, and mitigating the accumulation of error by corrupting the training data with noise. Our GNS framework advances the state-of-the-art in learned physical simulation, and holds promise for solving a wide range of complex forward and inverse problems.}
}




@InProceedings{esnrgraph,
  title = 	 {Towards Understanding and Reducing Graph Structural Noise for {GNN}s},
  author =       {Dong, Mingze and Kluger, Yuval},
  booktitle = 	 {Proceedings of the 40th International Conference on Machine Learning},
  pages = 	 {8202--8226},
  year = 	 {2023},
  editor = 	 {Krause, Andreas and Brunskill, Emma and Cho, Kyunghyun and Engelhardt, Barbara and Sabato, Sivan and Scarlett, Jonathan},
  volume = 	 {202},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {23--29 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v202/dong23a/dong23a.pdf},
  url = 	 {https://proceedings.mlr.press/v202/dong23a.html},
  abstract = 	 {Graph neural networks (GNNs) have emerged as a powerful paradigm to learn from relational data mostly through applying the message passing mechanism. However, this approach may exhibit suboptimal performance when applied to graphs possessing various structural issues. In this work, we focus on understanding and alleviating the effect of graph structural noise on GNN performance. To evaluate the graph structural noise in real data, we propose edge signal-to-noise ratio (ESNR), a novel metric evaluating overall edge noise level with respect to data features or labels based on random matrix theory. We have found striking concordance between the proposed ESNR metric and the GNN performance in various simulated and real data. To reduce the effect of the noise, we propose GPS (Graph Propensity Score) graph rewiring, which estimates the edge likelihood for rewiring data graphs based on self-supervised link prediction. We provide a theoretical guarantee for GPS graph rewiring and demonstrate its efficacy by comprehensive benchmarks.}
}

@article{
    shchur2019overlapping,
    title={Overlapping Community Detection with Graph Neural Networks},
    author={Oleksandr Shchur and Stephan G\"{u}nnemann},
    journal={Deep Learning on Graphs Workshop, KDD},
    year={2019},
}
@article{bruna2017community,
  title={Community detection with graph neural networks},
  author={Bruna, Joan and Li, X},
  journal={stat},
  volume={1050},
  pages={27},
  year={2017}
}

@inproceedings{
  brody2022how,
  title={How Attentive are Graph Attention Networks? },
  author={Shaked Brody and Uri Alon and Eran Yahav},
  booktitle={International Conference on Learning Representations},
  year={2022},
  url={https://openreview.net/forum?id=F72ximsx7C1}
}

@inproceedings{signs,
title={Masks, Signs, And Learning Rate Rewinding},
author={Gadhikar, Advait Harshal and Burkholz, Rebekka},
booktitle={International Conference on Learning Representations},
year={2024}
}
@InProceedings{er-paper,
  title = 	 {Why Random Pruning Is All We Need to Start Sparse},
  author =       {Gadhikar, Advait Harshal and Mukherjee, Sohom and Burkholz, Rebekka},
  booktitle = 	 {International Conference on Machine Learning},
  year = 	 {2023},
}
@inproceedings{uniExist,
title={On the Existence of Universal Lottery Tickets},
author={Rebekka Burkholz and Nilanjana Laha and Rajarshi Mukherjee and Alkis Gotovos},
booktitle={International Conference on Learning Representations},
year={2022}
}
@misc{plant,
      title={Plant 'n' Seek: Can You Find the Winning Ticket?}, 
      author={Jonas Fischer and Rebekka Burkholz},
      year={2021},
      eprint={2111.11153},
      archivePrefix={arXiv}
}
@inproceedings{depthexist,
title={Most Activation Functions Can Win the Lottery Without Excessive Depth},
author={Rebekka Burkholz},
booktitle={Advances in Neural Information Processing Systems},
year={2022}
}
@inproceedings{convexist,
title={Convolutional and Residual Networks Provably Contain Lottery Tickets},
author={Rebekka Burkholz},
booktitle={International Conference on Machine Learning},
year={2022}
}
@inproceedings{cnnexist,
title={Proving the Lottery Ticket Hypothesis for Convolutional Neural Networks},
author={Arthur da Cunha and Emanuele Natale and Laurent Viennot},
booktitle={International Conference on Learning Representations },
year={2022}
}
@misc{equivariantLT,
  author = {Ferbach, Damien and Tsirigotis, Christos and Gidel, Gauthier and Avishek, Bose},
 title = {A General Framework For Proving The Equivariant Strong Lottery Ticket Hypothesis},
  publisher = {arXiv},
  year = {2022}
}
@inproceedings{frankle2021review,
title={Pruning Neural Networks at Initialization: Why Are We Missing the Mark?},
author={Jonathan Frankle and Gintare Karolina Dziugaite and Daniel Roy and Michael Carbin},
booktitle={International Conference on Learning Representations},
year={2021}
}
@book{arpack,
author = {Lehoucq, R. B. and Sorensen, D. C. and Yang, C.},
title = {ARPACK Users' Guide: Solution of Large Scale Eigenvalue Problems by Implicitly Restarted Arnoldi Methods},
publisher = {SIAM},
year = {1998},
}

@INPROCEEDINGS{resnets,
	author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	booktitle={2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 
	title={Deep Residual Learning for Image Recognition}, 
	year={2016},
	volume={},
	number={},
	pages={770-778},
	doi={10.1109/CVPR.2016.90}}




@misc{borf,
	title={Revisiting Over-smoothing and Over-squashing Using Ollivier-Ricci Curvature}, 
	author={Khang Nguyen and Hieu Nong and Vinh Nguyen and Nhat Ho and Stanley Osher and Tan Nguyen},
	year={2023},
	eprint={2211.15779},
	archivePrefix={arXiv},
	primaryClass={cs.LG}
}

@inproceedings{sjlr,
	author = {Giraldo, Jhony H. and Skianis, Konstantinos and Bouwmans, Thierry and Malliaros, Fragkiskos D.},
	title = {On the Trade-off between Over-Smoothing and Over-Squashing in Deep Graph Neural Networks},
	year = {2023},
	isbn = {9798400701245},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3583780.3614997},
	doi = {10.1145/3583780.3614997},
	abstract = {Graph Neural Networks (GNNs) have succeeded in various computer science applications, yet deep GNNs underperform their shallow counterparts despite deep learning's success in other domains. Over-smoothing and over-squashing are key challenges when stacking graph convolutional layers, hindering deep representation learning and information propagation from distant nodes. Our work reveals that over-smoothing and over-squashing are intrinsically related to the spectral gap of the graph Laplacian, resulting in an inevitable trade-off between these two issues, as they cannot be alleviated simultaneously. To achieve a suitable compromise, we propose adding and removing edges as a viable approach. We introduce the Stochastic Jost and Liu Curvature Rewiring (SJLR) algorithm, which is computationally efficient and preserves fundamental properties compared to previous curvature-based methods. Unlike existing approaches, SJLR performs edge addition and removal during GNN training while maintaining the graph unchanged during testing. Comprehensive comparisons demonstrate SJLR's competitive performance in addressing over-smoothing and over-squashing.},
	booktitle = {Proceedings of the 32nd ACM International Conference on Information and Knowledge Management},
	pages = {566–576},
	numpages = {11},
	keywords = {curvature, graph neural networks, over-smoothing, over-squashing},
	location = {Birmingham, United Kingdom},
	series = {CIKM '23}
}


@inproceedings{mustafa2023are,
	title={Are {GAT}s Out of Balance?},
	author={Nimrah Mustafa and Aleksandar Bojchevski and Rebekka Burkholz},
	booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
	year={2023},
	url={https://openreview.net/forum?id=qY7UqLoora}
}
@InProceedings{ramanujan,
	title = 	 {A Study on the Ramanujan Graph Property of Winning Lottery Tickets},
	author =       {Pal, Bithika and Biswas, Arindam and Kolay, Sudeshna and Mitra, Pabitra and Basu, Biswajit},
	booktitle = 	 {Proceedings of the 39th International Conference on Machine Learning},
	pages = 	 {17186--17201},
	year = 	 {2022},
	editor = 	 {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
	volume = 	 {162},
	series = 	 {Proceedings of Machine Learning Research},
	month = 	 {17--23 Jul},
	publisher =    {PMLR},
	pdf = 	 {https://proceedings.mlr.press/v162/pal22a/pal22a.pdf},
	url = 	 {https://proceedings.mlr.press/v162/pal22a.html},
	abstract = 	 {Winning lottery tickets refer to sparse subgraphs of deep neural networks which have classification accuracy close to the original dense networks. Resilient connectivity properties of such sparse networks play an important role in their performance. The attempt is to identify a sparse and yet well-connected network to guarantee unhindered information flow. Connectivity in a graph is best characterized by its spectral expansion property. Ramanujan graphs are robust expanders which lead to sparse but highly-connected networks, and thus aid in studying the winning tickets. A feedforward neural network consists of a sequence of bipartite graphs representing its layers. We analyze the Ramanujan graph property of such bipartite layers in terms of their spectral characteristics using the Cheeger’s inequality for irregular graphs. It is empirically observed that the winning ticket networks preserve the Ramanujan graph property and achieve a high accuracy even when the layers are sparse. Accuracy and robustness to noise start declining as many of the layers lose the property. Next we find a robust winning lottery ticket by pruning individual layers while retaining their respective Ramanujan graph property. This strategy is observed to improve the performance of existing network pruning algorithms.}
}

@incollection{universallanguage,
	title={The Lottery Ticket Hypothesis for Pre-trained BERT Networks},
	author={Tianlong Chen and Jonathan Frankle and Shiyu Chang and Sijia Liu and Yang Zhang and Zhangyang Wang and Michael Carbin},
	booktitle={Advances in Neural Information Processing Systems},
	volume={33},
	pages={15834--15846},
	year={2020}
}

@incollection{universal,
	title = {One ticket to win them all: generalizing lottery ticket initializations across datasets and optimizers},
	author = {Morcos, Ari and Yu, Haonan and Paganini, Michela and Tian, Yuandong},
	booktitle = {Advances in Neural Information Processing Systems},
	pages = {4932--4942},
	year = {2019}
}

@inproceedings{Kipf:2017tc,
	author = {Kipf, Thomas N. and Welling, Max},
	title = {{Semi-Supervised Classification with Graph Convolutional Networks}},
	booktitle = {ICLR},
	year = {2017}
}

@inproceedings{Velickovic:2018we,
	author = {Veli{\v c}kovi{\'c}, Petar and Cucurull, Guillem and Casanova, Arantxa and Romero, Adriana and Li{\`o}, Pietro and Bengio, Yoshua},
	title = {{Graph Attention Networks}},
	booktitle = {ICLR},
	year = {2018}
}

@inproceedings{Hamilton:2017tp,
	author = {Hamilton, William L. and Ying, Zhitao and Leskovec, Jure},
	title = {{Inductive Representation Learning on Large Graphs}},
	booktitle = {NIPS},
	year = {2017},
	pages = {1024--1034},
}

@inproceedings{quachem,
	author = {Gilmer, Justin and Schoenholz, Samuel S. and Riley, Patrick F. and Vinyals, Oriol and Dahl, George E.},
	title = {Neural Message Passing for Quantum Chemistry},
	year = {2017},
	publisher = {JMLR.org},
	abstract = {Supervised learning on molecules has incredible potential to be useful in chemistry, drug discovery, and materials science. Luckily, several promising and closely related neural network models invariant to molecular symmetries have already been described in the literature. These models learn a message passing algorithm and aggregation procedure to compute a function of their entire input graph. At this point, the next step is to find a particularly effective variant of this general approach and apply it to chemical prediction benchmarks until we either solve them or reach the limits of the approach. In this paper, we reformulate existing models into a single common framework we call Message Passing Neural Networks (MPNNs) and explore additional novel variations within this framework. Using MPNNs we demonstrate state of the art results on an important molecular property prediction benchmark; these results are strong enough that we believe future work should focus on datasets with larger molecules or more accurate ground truth labels.},
	booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
	pages = {1263–1272},
	numpages = {10},
	location = {Sydney, NSW, Australia},
	series = {ICML'17}
}

@inproceedings{
	alon2021on,
	title={On the Bottleneck of Graph Neural Networks and its Practical Implications},
	author={Uri Alon and Eran Yahav},
	booktitle={International Conference on Learning Representations},
	year={2021},
	url={https://openreview.net/forum?id=i80OPhOCVH2}
}

@book{Chung:1997,
	added-at = {2009-02-05T15:34:40.000+0100},
	author = {Chung, F. R. K.},
	biburl = {https://www.bibsonomy.org/bibtex/295ef10b5a69a03d8507240b6cf410f8a/folke},
	description = {This monograph is an intertwined tale of eigenvalues and their use in unlocking a thousand secrets about graphs. The stories will be told --- how the spectrum reveals fundamental properties of a graph, how spectral graph theory links the discrete universe to the continuous one through geometric, analytic and algebraic techniques, and how, through eigenvalues, theory and applications in communications and computer science come together in symbiotic harmony....},
	interhash = {0f0fd754924d4dd54bc185bd1c71d00b},
	intrahash = {95ef10b5a69a03d8507240b6cf410f8a},
	keywords = {graph spectral theory},
	publisher = {American Mathematical Society},
	timestamp = {2009-02-05T15:34:40.000+0100},
	title = {Spectral Graph Theory},
	year = 1997
}

@article{Eldan2017BraesssPF,
	title={Braess's paradox for the spectral gap in random graphs and delocalization of eigenvectors},
	author={Ronen Eldan and Mikl{\'o}s Z. R{\'a}cz and Tselil Schramm},
	journal={Random Structures \& Algorithms},
	year={2017},
	volume={50}
}

@ARTICLE{scars,  author={Scarselli, Franco and Gori, Marco and Tsoi, Ah Chung and Hagenbuchner, Markus and Monfardini, Gabriele},  journal={IEEE Transactions on Neural Networks},   title={The Graph Neural Network Model},   year={2009},  volume={20},  number={1},  pages={61-80},  doi={10.1109/TNN.2008.2005605}}


@article{Shlomi_2021,
	doi = {10.1088/2632-2153/abbf9a},
	url = {https://doi.org/10.1088/2632-2153/abbf9a},
	year = 2021,
	month = {jan},
	publisher = {{IOP} Publishing},
	volume = {2},
	number = {2},
	pages = {021001},
	author = {Jonathan Shlomi and Peter Battaglia and Jean-Roch Vlimant},
	title = {Graph neural networks in particle physics},
	journal = {Machine Learning: Science and Technology},
	abstract = {Particle physics is a branch of science aiming at discovering the fundamental laws of matter and forces. Graph neural networks are trainable functions which operate on graphs—sets of elements and their pairwise relations—and are a central method within the broader field of geometric deep learning. They are very expressive and have demonstrated superior performance to other classical deep learning approaches in a variety of domains. The data in particle physics are often represented by sets and graphs and as such, graph neural networks offer key advantages. Here we review various applications of graph neural networks in particle physics, including different graph constructions, model architectures and learning objectives, as well as key open problems in particle physics for which graph neural networks are promising.}
}

@article{gdlbook,
	author    = {Michael M. Bronstein and
	Joan Bruna and
	Taco Cohen and
	Petar Velickovic},
	title     = {Geometric Deep Learning: Grids, Groups, Graphs, Geodesics, and Gauges},
	journal   = {CoRR},
	volume    = {abs/2104.13478},
	year      = {2021},
	url       = {https://arxiv.org/abs/2104.13478},
	eprinttype = {arXiv},
	eprint    = {2104.13478},
	timestamp = {Tue, 04 May 2021 15:12:43 +0200},
	biburl    = {https://dblp.org/rec/journals/corr/abs-2104-13478.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

@INPROCEEDINGS{gori,
	author={Gori, M. and Monfardini, G. and Scarselli, F.},
	booktitle={Proceedings. 2005 IEEE International Joint Conference on Neural Networks, 2005.}, 
	title={A new model for learning in graph domains}, 
	year={2005},
	volume={2},
	number={},
	pages={729-734 vol. 2},
	doi={10.1109/IJCNN.2005.1555942}}

@inproceedings{xu2018how,
	title={How Powerful are Graph Neural Networks?},
	author={Keyulu Xu and Weihua Hu and Jure Leskovec and Stefanie Jegelka},
	booktitle={International Conference on Learning Representations},
	year={2019},
	url={https://openreview.net/forum?id=ryGs6iA5Km},
}

@inproceedings{Leman,
	title={THE REDUCTION OF A GRAPH TO CANONICAL FORM AND THE ALGEBRA WHICH APPEARS THEREIN},
	author={Adrien Leman},
	year={1968}
}

@article{WLkernels, author = {Shervashidze, Nino and Schweitzer, Pascal and van Leeuwen, Erik Jan and Mehlhorn, Kurt and Borgwardt, Karsten M.}, title = {Weisfeiler-Lehman Graph Kernels}, year = {2011}, issue_date = {2/1/2011}, publisher = {JMLR.org}, volume = {12}, number = {null}, issn = {1532-4435}, abstract = {In this article, we propose a family of efficient kernels for large graphs with discrete node labels. Key to our method is a rapid feature extraction scheme based on the Weisfeiler-Lehman test of isomorphism on graphs. It maps the original graph to a sequence of graphs, whose node attributes capture topological and label information. A family of kernels can be defined based on this Weisfeiler-Lehman sequence of graphs, including a highly efficient kernel comparing subtree-like patterns. Its runtime scales only linearly in the number of edges of the graphs and the length of the Weisfeiler-Lehman graph sequence. In our experimental evaluation, our kernels outperform state-of-the-art graph kernels on several graph classification benchmark data sets in terms of accuracy and runtime. Our kernels open the door to large-scale applications of graph kernels in various disciplines such as computational biology and social network analysis.}, journal = {J. Mach. Learn. Res.}, month = {nov}, pages = {2539–2561}, numpages = {23} }

@inproceedings{ono,
	title={Graph Neural Networks Exponentially Lose Expressive Power for Node Classification},
	author={Kenta Oono and Taiji Suzuki},
	booktitle={International Conference on Learning Representations},
	year={2020}
}

@article{NT2019RevisitingGN,
	title={Revisiting Graph Neural Networks: All We Have is Low-Pass Filters},
	author={Hoang NT and Takanori Maehara},
	journal={ArXiv},
	year={2019},
	volume={abs/1905.09550}
}

@article{Morris_Ritzert_Fey_Hamilton_Lenssen_Rattan_Grohe_2019, title={Weisfeiler and Leman Go Neural: Higher-Order Graph Neural Networks}, volume={33}, url={https://ojs.aaai.org/index.php/AAAI/article/view/4384}, DOI={10.1609/aaai.v33i01.33014602}, abstractNote={&lt;p&gt;In recent years, graph neural networks (GNNs) have emerged as a powerful neural architecture to learn vector representations of nodes and graphs in a supervised, end-to-end fashion. Up to now, GNNs have only been evaluated empirically—showing promising results. The following work investigates GNNs from a theoretical point of view and relates them to the 1-dimensional Weisfeiler-Leman graph isomorphism heuristic (1-WL). We show that GNNs have the same expressiveness as the 1-WL in terms of distinguishing non-isomorphic (sub-)graphs. Hence, both algorithms also have the same shortcomings. Based on this, we propose a generalization of GNNs, so-called &lt;em&gt;k&lt;/em&gt;-dimensional GNNs (&lt;em&gt;k&lt;/em&gt;-GNNs), which can take higher-order graph structures at multiple scales into account. These higher-order structures play an essential role in the characterization of social networks and molecule graphs. Our experimental evaluation confirms our theoretical findings as well as confirms that higher-order information is useful in the task of graph classification and regression.&lt;/p&gt;}, number={01}, journal={Proceedings of the AAAI Conference on Artificial Intelligence}, author={Morris, Christopher and Ritzert, Martin and Fey, Matthias and Hamilton, William L. and Lenssen, Jan Eric and Rattan, Gaurav and Grohe, Martin}, year={2019}, month={Jul.}, pages={4602-4609} }


@inproceedings{bodnar2021weisfeiler,
	title={Weisfeiler and Lehman Go Topological: Message Passing Simplicial Networks},
	author={Cristian Bodnar and Fabrizio Frasca and Yu Guang Wang and Nina Otter and Guido Montufar and Pietro Li{\`o} and Michael M. Bronstein},
	booktitle={ICLR 2021 Workshop on Geometrical and Topological Representation Learning},
	year={2021},
	url={https://openreview.net/forum?id=RZgbB-O3w6Z}
}

@inproceedings{cw,
	title={Weisfeiler and Lehman Go Cellular: {CW} Networks},
	author={Cristian Bodnar and Fabrizio Frasca and Nina Otter and Yu Guang Wang and Pietro Li{\`o} and Guido Montufar and Michael M. Bronstein},
	booktitle={Advances in Neural Information Processing Systems},
	editor={A. Beygelzimer and Y. Dauphin and P. Liang and J. Wortman Vaughan},
	year={2021},
	url={https://openreview.net/forum?id=uVPZCMVtsSG}
}

@inproceedings{
	bevilacqua2022equivariant,
	title={Equivariant Subgraph Aggregation Networks},
	author={Beatrice Bevilacqua and Fabrizio Frasca and Derek Lim and Balasubramaniam Srinivasan and Chen Cai and Gopinath Balamurugan and Michael M. Bronstein and Haggai Maron},
	booktitle={International Conference on Learning Representations},
	year={2022},
	url={https://openreview.net/forum?id=dFbKQaRk15w}
}

@inproceedings{
	topping2022understanding,
	title={Understanding over-squashing and bottlenecks on graphs via curvature},
	author={Jake Topping and Francesco Di Giovanni and Benjamin Paul Chamberlain and Xiaowen Dong and Michael M. Bronstein},
	booktitle={International Conference on Learning Representations},
	year={2022},
	url={https://openreview.net/forum?id=7UmjRGzp-A}
}

@inproceedings{Cheeger1969ALB,
	title={A lower bound for the smallest eigenvalue of the Laplacian},
	author={Jeff Cheeger},
	year={1969}
}

@misc{diffwire,
	doi = {10.48550/ARXIV.2206.07369},
	
	url = {https://arxiv.org/abs/2206.07369},
	
	author = {Arnaiz-Rodríguez, Adrián and Begga, Ahmed and Escolano, Francisco and Oliver, Nuria},
	
	keywords = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), FOS: Computer and information sciences, FOS: Computer and information sciences},
	
	title = {DiffWire: Inductive Graph Rewiring via the Lovász Bound},
	
	publisher = {arXiv},
	
	year = {2022},
}


@InProceedings{chungbraess,
	author={Chung, Fan
	and Young, Stephen J.},
	title={Braess's Paradox in Large Sparse Graphs},
	booktitle={Internet and Network Economics},
	year={2010},
	publisher={Springer Berlin Heidelberg},
}

@inproceedings{veli2022message,
	title={Message passing all the way up},
	author={Petar Veli{\v{c}}kovi{\'c}},
	booktitle={ICLR 2022 Workshop on Geometrical and Topological Representation Learning},
	year={2022},
	url={https://openreview.net/forum?id=Bc8GiEZkTe5}
}



@inproceedings{hamilton,
	title={The ricci flow on surfaces},
	author={Richard Hamilton},
	booktitle={ Mathematics and general relativity, Proceedings of the AMS-IMS-SIAM Joint Summer Research Conference in the Mathematical Sciences on Mathematics in General Relativity},
	year={1988}
}

@article{braess,
	author = {Braess, Dietrich and Nagurney, Anna and Wakolbinger, Tina},
	title = {On a Paradox of Traffic Planning},
	journal = {Transportation Science},
	volume = {39},
	number = {4},
	pages = {446-450},
	year = {2005},
	doi = {10.1287/trsc.1050.0127},
	
	URL = { 
	https://doi.org/10.1287/trsc.1050.0127
	
	},
	eprint = { 
	https://doi.org/10.1287/trsc.1050.0127
	
	}
	,
	abstract = { For each point of a road network, let there be given the number of cars starting from it, and the destination of the cars. Under these conditions one wishes to estimate the distribution of traffic flow. Whether one street is preferable to another depends not only on the quality of the road, but also on the density of the flow. If every driver takes the path that looks most favorable to him, the resultant running times need not be minimal. Furthermore, it is indicated by an example that an extension of the road network may cause a redistribution of the traffic that results in longer individual running times. }
}

@article{ValiantR10,
	author    = {Gregory Valiant and
	Tim Roughgarden},
	title     = {Braess's Paradox in large random graphs},
	journal   = {Random Struct. Algorithms},
	volume    = {37},
	number    = {4},
	pages     = {495--515},
	year      = {2010},
	url       = {https://doi.org/10.1002/rsa.20325},
	doi       = {10.1002/rsa.20325},
	timestamp = {Fri, 26 May 2017 22:50:39 +0200},
	biburl    = {https://dblp.org/rec/journals/rsa/ValiantR10.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{Braess1968berEP,
	title={{\"U}ber ein Paradoxon aus der Verkehrsplanung},
	author={Dietrich Braess},
	journal={Unternehmensforschung},
	year={1968},
	volume={12},
	pages={258-268}
}

@inproceedings{gasteiger_diffusion_2019,
	title = {Diffusion Improves Graph Learning},
	author = {Gasteiger, Johannes and Wei{\ss}enberger, Stefan and G{\"u}nnemann, Stephan},
	booktitle={Conference on Neural Information Processing Systems (NeurIPS)},
	year = {2019} }


@article{chungconjecture,
	author = {Chung, Fan and Young, Stephen J. and Zhao, Wenbo},
	title = {Braess's paradox in expanders},
	journal = {Random Structures \& Algorithms},
	volume = {41},
	number = {4},
	pages = {451-468},
	keywords = {Braess's paradox, selfish routing, expanders},
	doi = {https://doi.org/10.1002/rsa.20457},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/rsa.20457},
	eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/rsa.20457},
	abstract = {Abstract Expander graphs are known to facilitate effective routing and most real-world networks have expansion properties. At the other extreme, it has been shown that in some special graphs, removing certain edges can lead to moreefficient routing. This phenomenon is known as Braess's paradox and is usually regarded as a rare event. In contrast to what one might expect, we show that Braess's paradox is ubiquitous in expander graphs. Specifically, we prove that Braess's paradox occurs in a large class of expander graphs with continuous convex latency functions. Our results extend previous work which held only when the graph was both denser and random and for random linear latency functions. We identify deterministic sufficient conditions for a graph with as few as a linear number of edges, such that Braess's Paradox almost always occurs, with respect to a general family of random latency functions. © 2012 Wiley Periodicals, Inc. Random Struct. Alg., 2012},
	year = {2012}
}

@article{Rudelson_2015,
	year = {2015},
	month = {oct},	
	publisher = {Duke University Press},
	volume = {164},
	number = {13},
	author = {Mark Rudelson and Roman Vershynin},
	title = {Delocalization of eigenvectors of random matrices with independent entries},
	journal = {Duke Mathematical Journal}
}

@article{nature,
	abstract = {Machine learning plays an increasingly important role in many areas of chemistry and materials science, being used to predict materials properties, accelerate simulations, design new structures, and predict synthesis routes of new materials. Graph neural networks (GNNs) are one of the fastest growing classes of machine learning models. They are of particular relevance for chemistry and materials science, as they directly work on a graph or structural representation of molecules and materials and therefore have full access to all relevant information required to characterize materials. In this Review, we provide an overview of the basic principles of GNNs, widely used datasets, and state-of-the-art architectures, followed by a discussion of a wide range of recent applications of GNNs in chemistry and materials science, and concluding with a road-map for the further development and application of GNNs.},
	author = {Reiser, Patrick and Neubert, Marlen and Eberhard, Andr{\'e} and Torresi, Luca and Zhou, Chen and Shao, Chen and Metni, Houssam and van Hoesel, Clint and Schopmans, Henrik and Sommer, Timo and Friederich, Pascal},
	date = {2022/11/26},
	date-added = {2023-01-12 17:43:27 +0100},
	date-modified = {2023-01-12 17:44:25 +0100},
	doi = {10.1038/s43246-022-00315-6},
	id = {Reiser2022},
	isbn = {2662-4443},
	journal = {Communications Materials},
	number = {1},
	pages = {93},
	read = {0},
	title = {Graph neural networks for materials science and chemistry},
	url = {https://doi.org/10.1038/s43246-022-00315-6},
	volume = {3},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.1038/s43246-022-00315-6}}

@incollection{bongini2023biognn,
	title={BioGNN: How Graph Neural Networks Can Solve Biological Problems},
	author={Bongini, Pietro and Pancino, Niccol{\`o} and Scarselli, Franco and Bianchini, Monica},
	booktitle={Artificial Intelligence and Machine Learning for Healthcare},
	pages={211--231},
	year={2023},
	publisher={Springer}
}

@article{zhou2021dirichlet,
	title={Dirichlet energy constrained learning for deep graph neural networks},
	author={Zhou, Kaixiong and Huang, Xiao and Zha, Daochen and Chen, Rui and Li, Li and Choi, Soo-Hyun and Hu, Xia},
	journal={Advances in neural information processing systems},
	year={2021}
}

@inproceedings{bojchevski2020,
	title={Scaling Graph Neural Networks with Approximate PageRank},
	author={Bojchevski, Aleksandar and Klicpera, Johannes and Perozzi, Bryan and Kapoor, Amol and Blais, Martin and R{\'o}zemberczki, Benedek and Lukasik, Michal and Gunnemann, Stephan},
	booktitle = {Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
	year={2020},
	publisher = {ACM},
	address = {New York, NY, USA}
}
@inproceedings{scalinggnns,
	author = {Huang, Zengfeng and Zhang, Shengzhong and Xi, Chong and Liu, Tang and Zhou, Min},
	title = {Scaling Up Graph Neural Networks Via Graph Coarsening},
	year = {2021},
	isbn = {9781450383325},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3447548.3467256},
	doi = {10.1145/3447548.3467256},
	abstract = {Scalability of graph neural networks remains one of the major challenges in graph machine learning. Since the representation of a node is computed by recursively aggregating and transforming representation vectors of its neighboring nodes from previous layers, the receptive fields grow exponentially, which makes standard stochastic optimization techniques ineffective. Various approaches have been proposed to alleviate this issue, e.g., sampling-based methods and techniques based on pre-computation of graph filters.In this paper, we take a different approach and propose to use graph coarsening for scalable training of GNNs, which is generic, extremely simple and has sublinear memory and time costs during training. We present extensive theoretical analysis on the effect of using coarsening operations and provides useful guidance on the choice of coarsening methods. Interestingly, our theoretical analysis shows that coarsening can also be considered as a type of regularization and may improve the generalization. Finally, empirical results on real world datasets show that, simply applying off-the-shelf coarsening methods, we can reduce the number of nodes by up to a factor of ten without causing a noticeable downgrade in classification accuracy.},
	booktitle = {Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
	pages = {675–684},
	numpages = {10},
	keywords = {graph neural networks, graph coarsening, scalable training},
	location = {Virtual Event, Singapore},
	series = {KDD '21}
}

@inproceedings{Chen2021AUL,
	title={A Unified Lottery Ticket Hypothesis for Graph Neural Networks},
	author={Tianlong Chen and Yongduo Sui and Xuxi Chen and Aston Zhang and Zhangyang Wang},
	booktitle={International Conference on Machine Learning},
	year={2021}
}

@InProceedings{li2019deepgcns,
	title={DeepGCNs: Can GCNs Go as Deep as CNNs?},
	author={Guohao Li and Matthias Müller and Ali Thabet and Bernard Ghanem},
	booktitle={The IEEE International Conference on Computer Vision (ICCV)},
	year={2019}
}

@article{Beygelzimer2005ImprovingNR,
	title={Improving network robustness by edge modification},
	author={Alina Beygelzimer and Geoffrey Grinstein and Ralph Linsker and Irina Rish},
	journal={Physica A-statistical Mechanics and Its Applications},
	year={2005},
	volume={357},
	pages={593-612}
}

@article{10.1007/s10618-015-0447-5,
	author = {Chan, Hau and Akoglu, Leman},
	title = {Optimizing Network Robustness by Edge Rewiring: A General Framework},
	year = {2016},
	issue_date = {September 2016},
	publisher = {Kluwer Academic Publishers},
	address = {USA},
	volume = {30},
	number = {5},
	issn = {1384-5810},
	url = {https://doi.org/10.1007/s10618-015-0447-5},
	doi = {10.1007/s10618-015-0447-5},
	abstract = {Spectral measures have long been used to quantify the robustness of real-world graphs. For example, spectral radius (or the principal eigenvalue) is related to the effective spreading rates of dynamic processes (e.g., rumor, disease, information propagation) on graphs. Algebraic connectivity (or the Fiedler value), which is a lower bound on the node and edge connectivity of a graph, captures the "partitionability" of a graph into disjoint components. In this work we address the problem of modifying a given graph's structure under a given budget so as to maximally improve its robustness, as quantified by spectral measures. We focus on modifications based on degree-preserving edge rewiring, such that the expected load (e.g., airport flight capacity) or physical/hardware requirement (e.g., count of ISP router traffic switches) of nodes remain unchanged. Different from a vast literature of measure-independent heuristic approaches, we propose an algorithm, called EdgeRewire, which optimizes a specific measure of interest directly. Notably, EdgeRewire is general to accommodate six different spectral measures. Experiments on real-world datasets from three different domains (Internet AS-level, P2P, and airport flights graphs) show the effectiveness of our approach, where EdgeRewire produces graphs with both (i) higher robustness, and (ii) higher attack-tolerance over several state-of-the-art methods.},
	journal = {Data Min. Knowl. Discov.},
	month = {sep},
	pages = {1395–1425},
	numpages = {31},
	keywords = {Attack tolerance, Robustnesss measures, Optimization algorithms, Graph spectrum, Graph robustness, Edge rewiring}
}

@article{10.1002/rsa.20457,
	author = {Chung, Fan and Young, Stephen J. and Zhao, Wenbo},
	title = {Braess's Paradox in Expanders},
	year = {2012},
	issue_date = {December 2012},
	publisher = {John Wiley &amp; Sons, Inc.},
	address = {USA},
	volume = {41},
	number = {4},
	issn = {1042-9832},
	url = {https://doi.org/10.1002/rsa.20457},
	doi = {10.1002/rsa.20457},
	abstract = {Expander graphs are known to facilitate effective routing and most real-world networks have expansion properties. At the other extreme, it has been shown that in some special graphs, removing certain edges can lead to moreefficient routing. This phenomenon is known as Braess's paradox and is usually regarded as a rare event. In contrast to what one might expect, we show that Braess's paradox is ubiquitous in expander graphs. Specifically, we prove that Braess's paradox occurs in a large class of expander graphs with continuous convex latency functions. Our results extend previous work which held only when the graph was both denser and random and for random linear latency functions. We identify deterministic sufficient conditions for a graph with as few as a linear number of edges, such that Braess's Paradox almost always occurs, with respect to a general family of random latency functions. © 2012 Wiley Periodicals, Inc. Random Struct. Alg., 2012 © 2012 Wiley Periodicals, Inc.},
	journal = {Random Struct. Algorithms},
	month = {dec},
	pages = {451–468},
	numpages = {18},
	keywords = {expanders, selfish routing, Braess's paradox}
}

@article{luxburg,
	abstract = {In recent years, spectral clustering has become one of the most popular modern clustering algorithms. It is simple to implement, can be solved efficiently by standard linear algebra software, and very often outperforms traditional clustering algorithms such as the k-means algorithm. On the first glance spectral clustering appears slightly mysterious, and it is not obvious to see why it works at all and what it really does. The goal of this tutorial is to give some intuition on those questions. We describe different graph Laplacians and their basic properties, present the most common spectral clustering algorithms, and derive those algorithms from scratch by several different approaches. Advantages and disadvantages of the different spectral clustering algorithms are discussed.},
	author = {von Luxburg, Ulrike},
	date = {2007/12/01},
	date-added = {2023-01-15 09:01:59 +0100},
	date-modified = {2023-01-15 09:02:13 +0100},
	doi = {10.1007/s11222-007-9033-z},
	id = {von Luxburg2007},
	isbn = {1573-1375},
	journal = {Statistics and Computing},
	number = {4},
	pages = {395--416},
	title = {A tutorial on spectral clustering},
	url = {https://doi.org/10.1007/s11222-007-9033-z},
	volume = {17},
	year = {2007},
	bdsk-url-1 = {https://doi.org/10.1007/s11222-007-9033-z}}


@book{stewart1990matrix,
	title={Matrix Perturbation Theory},
	author={Stewart, G.W. and Sun, J.},
	isbn={9780126702309},
	lccn={lc90033378},
	series={Computer Science and Scientific Computing},
	url={https://books.google.de/books?id=l78PAQAAMAAJ},
	year={1990},
	publisher={Elsevier Science}
}
@inproceedings{bojchevski2019adversarial,
	title =      {Adversarial Attacks on Node Embeddings via Graph Poisoning},
	author =      {Aleksandar Bojchevski and Stephan G{\"{u}}nnemann},
	booktitle ={Proceedings of the 36th International Conference on Machine Learning, {ICML}},
	year =      {2019},
	series =      {Proceedings of Machine Learning Research},
	publisher =      {PMLR},
}

@inproceedings{
	frankle2018the,
	title={The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks},
	author={Jonathan Frankle and Michael Carbin},
	booktitle={International Conference on Learning Representations},
	year={2019},
	url={https://openreview.net/forum?id=rJl-b3RcF7},
}


@article{Cora,
	abstract = {Domain-specific internet portals are growing in popularity because they gather content from the Web and organize it for easy access, retrieval and search. For example, www.campsearch.com allows complex queries by age, location, cost and specialty over summer camps. This functionality is not possible with general, Web-wide search engines. Unfortunately these portals are difficult and time-consuming to maintain. This paper advocates the use of machine learning techniques to greatly automate the creation and maintenance of domain-specific Internet portals. We describe new research in reinforcement learning, information extraction and text classification that enables efficient spidering, the identification of informative text segments, and the population of topic hierarchies. Using these techniques, we have built a demonstration system: a portal for computer science research papers. It already contains over 50,000 papers and is publicly available at www.cora.justresearch.com. These techniques are widely applicable to portal creation in other domains.},
	author = {McCallum, Andrew Kachites and Nigam, Kamal and Rennie, Jason and Seymore, Kristie},
	date = {2000/07/01},
	date-added = {2023-01-16 12:06:15 +0100},
	date-modified = {2023-01-16 12:06:28 +0100},
	doi = {10.1023/A:1009953814988},
	id = {McCallum2000},
	isbn = {1573-7659},
	journal = {Information Retrieval},
	number = {2},
	pages = {127--163},
	title = {Automating the Construction of Internet Portals with Machine Learning},
	url = {https://doi.org/10.1023/A:1009953814988},
	volume = {3},
	year = {2000},
	bdsk-url-1 = {https://doi.org/10.1023/A:1009953814988}}

@article{Citeseer, title={Collective Classification in Network Data}, volume={29}, url={https://ojs.aaai.org/index.php/aimagazine/article/view/2157}, DOI={10.1609/aimag.v29i3.2157}, abstractNote={Many real-world applications produce networked data such as the world-wide web (hypertext documents connected via hyperlinks), social networks (for example, people connected by friendship links), communication networks (computers connected via communication links) and biological networks (for example, protein interaction networks). A recent focus in machine learning research has been to extend traditional machine learning classification techniques to classify nodes in such networks. In this article, we provide a brief introduction to this area of research and how it has progressed during the past decade. We introduce four of the most widely used inference algorithms for classifying networked data and empirically compare them on both synthetic and real-world data.}, number={3}, journal={AI Magazine}, author={Sen, Prithviraj and Namata, Galileo and Bilgic, Mustafa and Getoor, Lise and Galligher, Brian and Eliassi-Rad, Tina}, year={2008}, month={Sep.}, pages={93} }


@inproceedings{Pubmed,
	title={Query-driven Active Surveying for Collective Classification},
	author={Galileo Namata and Ben London and Lise Getoor and Bert Huang},
	year={2012}
}

@inproceedings{Actor,
	author = {Tang, Jie and Sun, Jimeng and Wang, Chi and Yang, Zi},
	title = {Social Influence Analysis in Large-Scale Networks},
	year = {2009},
	isbn = {9781605584959},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/1557019.1557108},
	doi = {10.1145/1557019.1557108},
	abstract = {In large social networks, nodes (users, entities) are influenced by others for various reasons. For example, the colleagues have strong influence on one's work, while the friends have strong influence on one's daily life. How to differentiate the social influences from different angles(topics)? How to quantify the strength of those social influences? How to estimate the model on real large networks?To address these fundamental questions, we propose Topical Affinity Propagation (TAP) to model the topic-level social influence on large networks. In particular, TAP can take results of any topic modeling and the existing network structure to perform topic-level influence propagation. With the help of the influence analysis, we present several important applications on real data sets such as 1) what are the representative nodes on a given topic? 2) how to identify the social influences of neighboring nodes on a particular node?To scale to real large networks, TAP is designed with efficient distributed learning algorithms that is implemented and tested under the Map-Reduce framework. We further present the common characteristics of distributed learning algorithms for Map-Reduce. Finally, we demonstrate the effectiveness and efficiency of TAP on real large data sets.},
	booktitle = {Proceedings of the 15th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
	pages = {807–816},
	numpages = {10},
	keywords = {social networks, topical analysis propagation, large-scale network, social influence analysis},
	location = {Paris, France},
	series = {KDD '09}
}

@article{squirrel,
	author = {Rozemberczki, Benedek and Allen, Carl and Sarkar, Rik},
	title = "{Multi-Scale attributed node embedding}",
	journal = {Journal of Complex Networks},
	volume = {9},
	number = {2},
	year = {2021},
	month = {05},
	abstract = "{We present network embedding algorithms that capture information about a node from the local distribution over node attributes around it, as observed over random walks following an approach similar to Skip-gram. Observations from neighbourhoods of different sizes are either pooled (AE) or encoded distinctly in a multi-scale approach (MUSAE). Capturing attribute-neighbourhood relationships over multiple scales is useful for a range of applications, including latent feature identification across disconnected networks with similar features. We prove theoretically that matrices of node-feature pointwise mutual information are implicitly factorized by the embeddings. Experiments show that our algorithms are computationally efficient and outperform comparable models on social networks and web graphs.}",
	issn = {2051-1329},
	doi = {10.1093/comnet/cnab014},
	url = {https://doi.org/10.1093/comnet/cnab014},
	note = {cnab014},
	eprint = {https://academic.oup.com/comnet/article-pdf/9/2/cnab014/40435146/cnab014.pdf},
}

@inproceedings{Fosr,
	title={Fo{SR}: First-order spectral rewiring for addressing oversquashing in {GNN}s},
	author={Kedar Karhadkar and Pradeep Kr. Banerjee and Guido Montufar},
	booktitle={The Eleventh International Conference on Learning Representations },
	year={2023},
	url={https://openreview.net/forum?id=3YjQfCLdrzz}
}

@inproceedings{Banerjee,
	author = {Banerjee, Pradeep Kr. and Karhadkar, Kedar and Wang, Yu Guang and Alon, Uri and Mont\'{u}far, Guido},
	title = {Oversquashing in GNNs through the Lens of Information Contraction and Graph Expansion},
	year = {2022},
	publisher = {IEEE Press},
	url = {https://doi.org/10.1109/Allerton49937.2022.9929363},
	doi = {10.1109/Allerton49937.2022.9929363},
	abstract = {The quality of signal propagation in message-passing graph neural networks (GNNs) strongly influences their expressivity as has been observed in recent works. In particular, for prediction tasks relying on long-range interactions, recursive aggregation of node features can lead to an undesired phenomenon called “oversquashing”. We present a framework for analyzing oversquashing based on information contraction. Our analysis is guided by a model of reliable computation due to von Neumann that lends a new insight into oversquashing as signal quenching in noisy computation graphs. Building on this, we propose a graph rewiring algorithm aimed at alleviating oversquashing. Our algorithm employs a random local edge flip primitive motivated by an expander graph construction. We compare the spectral expansion properties of our algorithm with that of an existing curvature-based non-local rewiring strategy. Synthetic experiments show that while our algorithm in general has a slower rate of expansion, it is overall computationally cheaper, preserves the node degrees exactly and never disconnects the graph.},
	booktitle = {2022 58th Annual Allerton Conference on Communication, Control, and Computing (Allerton)},
	pages = {1–8},
	numpages = {8},
	location = {Monticello, IL, USA}
}


@inproceedings{
	deac2022expander,
	title={Expander Graph Propagation},
	author={Andreea Deac and Marc Lackenby and Petar Veli{\v{c}}kovi{\'c}},
	booktitle={The First Learning on Graphs Conference},
	year={2022},
	url={https://openreview.net/forum?id=IKevTLt3rT}
}

@inbook{adhikari,
	author = {Bijaya Adhikari and Yao Zhang and Aditya Bharadwaj and B. Aditya Prakash},
	title = {Condensing Temporal Networks using Propagation},
	booktitle = {Proceedings of the 2017 SIAM International Conference on Data Mining (SDM)},
	year = {2017},
	pages = {417-425},
	doi = {10.1137/1.9781611974973.47},
	URL = {https://epubs.siam.org/doi/abs/10.1137/1.9781611974973.47},
	eprint = {https://epubs.siam.org/doi/pdf/10.1137/1.9781611974973.47},
	abstract = { Abstract Modern networks are very large in size and also evolve with time. As their size grows, the complexity of performing network analysis grows as well. Getting a smaller representation of a temporal network with similar properties will help in various data mining tasks. In this paper, we study the novel problem of getting a smaller diffusion-equivalent representation of a set of time-evolving networks. We first formulate a well-founded and general temporal-network condensation problem based on the so-called system-matrix of the network. We then propose NetCon-dense, a scalable and effective algorithm which solves this problem using careful transformations in sub-quadratic running time, and linear space complexities. Our extensive experiments show that we can reduce the size of large real temporal networks (from multiple domains such as social, co-authorship and email) significantly without much loss of information. We also show the wide-applicability of Net-Condense by leveraging it for several tasks: for example, we use it to understand, explore and visualize the original datasets and to also speed-up algorithms for the influence-maximization problem on temporal networks. }
}

@inproceedings{eden,
	author = {Eden, Talya and Jain, Shweta and Pinar, Ali and Ron, Dana and Seshadhri, C.},
	title = {Provable and Practical Approximations for the Degree Distribution Using Sublinear Graph Samples},
	year = {2018},
	isbn = {9781450356398},
	publisher = {International World Wide Web Conferences Steering Committee},
	address = {Republic and Canton of Geneva, CHE},
	url = {https://doi.org/10.1145/3178876.3186111},
	doi = {10.1145/3178876.3186111},
	abstract = {The degree distribution is one of the most fundamental properties used in the analysis of massive graphs. There is a large literature on graph sampling, where the goal is to estimate properties (especially the degree distribution) of a large graph through a small, random sample. Estimating the degree distribution of real-world graphs poses a significant challenge, due to their heavy-tailed nature and the large variance in degrees. We design a new algorithm, SADDLES, for this problem, using recent mathematical techniques from the field of sublinear algorithms. The SADDLES algorithm gives provably accurate outputs for all values of the degree distribution. For the analysis, we define two fatness measures of the degree distribution, called the h-index and the z-index. We prove that SADDLES is sublinear in the graph size when these indices are large. A corollary of this result is a provably sublinear algorithm for any degree distribution bounded below by a power law. We deploy our new algorithm on a variety of real datasets and demonstrate its excellent empirical behavior. In all instances, we get extremely accurate approximations for all values in the degree distribution by observing at most $1\%$ of the vertices. This is a major improvement over the state-of-the-art sampling algorithms, which typically sample more than $10\%$ of the vertices to give comparable results. We also observe that the h and z-indices of real graphs are large, validating our theoretical analysis.},
	booktitle = {Proceedings of the 2018 World Wide Web Conference},
	pages = {449–458},
	numpages = {10},
	keywords = {sublinear, graphs, sampling, degree distribution},
	location = {Lyon, France},
	series = {WWW '18}
}

@inproceedings{admm2,
	author = {Li, Jiayu and Zhang, Tianyun and Tian, Hao and Jin, Shengmin and Fardad, Makan and Zafarani, Reza},
	title = {SGCN: A Graph Sparsifier Based on Graph Convolutional Networks},
	year = {2020},
	isbn = {978-3-030-47425-6},
	publisher = {Springer-Verlag},
	address = {Berlin, Heidelberg},
	url = {https://doi.org/10.1007/978-3-030-47426-3_22},
	doi = {10.1007/978-3-030-47426-3_22},
	abstract = {Graphs are ubiquitous across the globe and within science and engineering. With graphs growing in size, node classification on large graphs can be space and time consuming, even with powerful classifiers such as Graph Convolutional Networks (GCNs). Hence, some questions are raised, particularly, whether one can keep only some of the edges of a graph while maintaining prediction performance for node classification, or train classifiers on specific subgraphs instead of a whole graph with limited performance loss in node classification. To address these questions, we propose Sparsified Graph Convolutional Network (SGCN), a neural network graph sparsifier that sparsifies a graph by pruning some edges. We formulate sparsification as an optimization problem, which we solve by an Alternating Direction Method of Multipliers (ADMM)-based solution. We show that sparsified graphs provided by SGCN can be used as inputs to GCN, leading to better or comparable node classification performance with that of original graphs in GCN, DeepWalk, and GraphSAGE.},
	booktitle = {Advances in Knowledge Discovery and Data Mining: 24th Pacific-Asia Conference, PAKDD 2020, Singapore, May 11–14, 2020, Proceedings, Part I},
	pages = {275–287},
	numpages = {13},
	keywords = {Graph sparsification, Graph convolutional network, Node classification},
	location = {Singapore, Singapore}
}

@InProceedings{zheng20d,
	title = 	 {Robust Graph Representation Learning via Neural Sparsification},
	author =       {Zheng, Cheng and Zong, Bo and Cheng, Wei and Song, Dongjin and Ni, Jingchao and Yu, Wenchao and Chen, Haifeng and Wang, Wei},
	booktitle = 	 {Proceedings of the 37th International Conference on Machine Learning},
	pages = 	 {11458--11468},
	year = 	 {2020},
	editor = 	 {III, Hal Daumé and Singh, Aarti},
	volume = 	 {119},
	series = 	 {Proceedings of Machine Learning Research},
	month = 	 {13--18 Jul},
	publisher =    {PMLR},
	pdf = 	 {http://proceedings.mlr.press/v119/zheng20d/zheng20d.pdf},
	url = 	 {https://proceedings.mlr.press/v119/zheng20d.html},
	abstract = 	 {Graph representation learning serves as the core of important prediction tasks, ranging from product recommendation to fraud detection. Real-life graphs usually have complex information in the local neighborhood, where each node is described by a rich set of features and connects to dozens or even hundreds of neighbors. Despite the success of neighborhood aggregation in graph neural networks, task-irrelevant information is mixed into nodes’ neighborhood, making learned models suffer from sub-optimal generalization performance. In this paper, we present NeuralSparse, a supervised graph sparsification technique that improves generalization power by learning to remove potentially task-irrelevant edges from input graphs. Our method takes both structural and non-structural information as input, utilizes deep neural networks to parameterize sparsification processes, and optimizes the parameters by feedback signals from downstream tasks. Under the NeuralSparse framework, supervised graph sparsification could seamlessly connect with existing graph neural networks for more robust performance. Experimental results on both benchmark and private datasets show that NeuralSparse can yield up to 7.2% improvement in testing accuracy when working with existing graph neural networks on node classification tasks.}
}

@inproceedings{
	Pei2020Geom-GCN,
	title={Geom-GCN: Geometric Graph Convolutional Networks},
	author={Hongbin Pei and Bingzhe Wei and Kevin Chen-Chuan Chang and Yu Lei and Bo Yang},
	booktitle={International Conference on Learning Representations},
	year={2020},
	url={https://openreview.net/forum?id=S1e2agrFvS}
}
@article{battaglia2018relational,
	title={Relational inductive biases, deep learning, and graph networks},
	author={Battaglia, Peter W and Hamrick, Jesse B and Bapst, Vincent and Sanchez-Gonzalez, Alvaro and Zambaldi, Vinicius and Malinowski, Mateusz and Tacchetti, Andrea and Raposo, David and Santoro, Adam and Faulkner, Ryan and others},
	journal={arXiv preprint arXiv:1806.01261},
	year={2018}
}

@misc{zachary1977information,
	title={An information flow model for conflict and fission in small groups},
	author={Zachary, Wayne W},
	journal={Journal of anthropological research},
	volume={33},
	number={4},
	pages={452--473},
	year={1977}
}

@misc{digiovanni2023oversquashing,
	title={On Over-Squashing in Message Passing Neural Networks: The Impact of Width, Depth, and Topology}, 
	author={Francesco Di Giovanni and Lorenzo Giusti and Federico Barbero and Giulia Luise and Pietro Lio' and Michael Bronstein},
	year={2023},
	eprint={2302.02941},
	archivePrefix={arXiv},
	primaryClass={cs.LG}
}

@inproceedings{platonov2023critical,
	title={A critical look at evaluation of GNNs under heterophily: Are we really making progress?},
	author={Platonov, Oleg and Kuznedelev, Denis and Diskin, Michael and Babenko, Artem and Prokhorenkova, Liudmila},
	booktitle={The Eleventh International Conference on Learning Representations},
	year={2023}
}

@inproceedings{
	hui2023rethinking,
	title={Rethinking Graph Lottery Tickets: Graph Sparsity Matters},
	author={Bo Hui and Da Yan and Xiaolong Ma and Wei-Shinn Ku},
	booktitle={The Eleventh International Conference on Learning Representations },
	year={2023},
	url={https://openreview.net/forum?id=fjh7UGQgOB}
}

@inproceedings{
	hoang2023revisiting,
	title={{REVISITING} {PRUNING} {AT} {INITIALIZATION} {THROUGH} {THE} {LENS} {OF} {RAMANUJAN} {GRAPH}},
	author={Duc N.M Hoang and Shiwei Liu and Radu Marculescu and Zhangyang Wang},
	booktitle={The Eleventh International Conference on Learning Representations },
	year={2023},
	url={https://openreview.net/forum?id=uVcDssQff_}
}

@inproceedings{
	lim2021large,
	title={Large Scale Learning on Non-Homophilous Graphs: New Benchmarks and Strong Simple Methods},
	author={Derek Lim and Felix Matthew Hohne and Xiuyu Li and Sijia Linda Huang and Vaishnavi Gupta and Omkar Prasad Bhalerao and Ser-Nam Lim},
	booktitle={Advances in Neural Information Processing Systems},
	editor={A. Beygelzimer and Y. Dauphin and P. Liang and J. Wortman Vaughan},
	year={2021},
	url={https://openreview.net/forum?id=DfGu8WwT0d}
}

@inproceedings{
	frankle2021pruning,
	title={Pruning Neural Networks at Initialization: Why Are We Missing the Mark?},
	author={Jonathan Frankle and Gintare Karolina Dziugaite and Daniel Roy and Michael Carbin},
	booktitle={International Conference on Learning Representations},
	year={2021},
	url={https://openreview.net/forum?id=Ig-VyQc-MLK}
}


@article{noteonos,
	author       = {Chen Cai and
	Yusu Wang},
	title        = {A Note on Over-Smoothing for Graph Neural Networks},
	journal      = {CoRR},
	volume       = {abs/2006.13318},
	year         = {2020},
	url          = {https://arxiv.org/abs/2006.13318},
	eprinttype    = {arXiv},
	eprint       = {2006.13318},
	timestamp    = {Mon, 02 Jan 2023 09:02:11 +0100},
	biburl       = {https://dblp.org/rec/journals/corr/abs-2006-13318.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@misc{rusch2023survey,
	title={A Survey on Oversmoothing in Graph Neural Networks}, 
	author={T. Konstantin Rusch and Michael M. Bronstein and Siddhartha Mishra},
	year={2023},
	eprint={2303.10993},
	archivePrefix={arXiv},
	primaryClass={cs.LG}
}

@inproceedings{keriven2022not,
  author={Nicolas Keriven},
  title={Not too little, not too much: a theoretical analysis of graph (over)smoothing},
  year={2022},
  cdate={1640995200000},
  url={http://papers.nips.cc/paper_files/paper/2022/hash/0f956ca6f667c62e0f71511773c86a59-Abstract-Conference.html},
  booktitle={NeurIPS},
}

@unknown{halfhop,
	author = {Azabou, Mehdi and Ganesh, Venkataramana and Thakoor, Shantanu and Lin, Chi-Heng and Sathidevi, Lakshmi and Liu, Ran and Valko, Michal and Veličković, Petar and Dyer, Eva},
	year = {2023},
	booktitle = {International Conference on Machine Learning},
	month = {08},
	pages = {},
	title = {Half-Hop: A graph upsampling approach for slowing down message passing}
}

@misc{salez2021sparse,
	title={Sparse expanders have negative curvature}, 
	author={Justin Salez},
	year={2021},
	eprint={2101.08242},
	archivePrefix={arXiv},
	primaryClass={math.PR}
}

@inproceedings{platonov2023characterizing,
	title={Characterizing Graph Datasets for Node Classification: Homophily-Heterophily Dichotomy and Beyond},
	author={Oleg Platonov and Denis Kuznedelev and Artem Babenko and Liudmila Prokhorenkova},
	booktitle={The Second Learning on Graphs Conference},
	year={2023},
	url={https://openreview.net/forum?id=D4GLZkTphJ}
}

@inproceedings{ma2022is,
	title={Is Homophily a Necessity for Graph Neural Networks?},
	author={Yao Ma and Xiaorui Liu and Neil Shah and Jiliang Tang},
	booktitle={International Conference on Learning Representations},
	year={2022},
	url={https://openreview.net/forum?id=ucASPPD9GKN}
}

@inproceedings{effectiveresistance,
author = {Black, Mitchell and Wan, Zhengchao and Nayyeri, Amir and Wang, Yusu},
title = {Understanding oversquashing in GNNs through the lens of effective resistance},
year = {2023},
publisher = {JMLR.org},
abstract = {Message passing graph neural networks (GNNs) are a popular learning architectures for graph-structured data. However, one problem GNNs experience is oversquashing, where a GNN has difficulty sending information between distant nodes. Understanding and mitigating oversquashing has recently received significant attention from the research community. In this paper, we continue this line of work by analyzing oversquashing through the lens of the effective resistance between nodes in the input graph. Effective resistance intuitively captures the "strength" of connection between two nodes by paths in the graph, and has a rich literature spanning many areas of graph theory. We propose to use total effective resistance as a bound of the total amount of oversquashing in a graph and provide theoretical justification for its use. We further develop an algorithm to identify edges to be added to an input graph to minimize the total effective resistance, thereby alleviating oversquashing. We provide empirical evidence of the effectiveness of our total effective resistance based rewiring strategies for improving the performance of GNNs.},
booktitle = {Proceedings of the 40th International Conference on Machine Learning},
articleno = {107},
numpages = {20},
location = {Honolulu, Hawaii, USA},
series = {ICML'23}
}

@article{chandraeffective,
	author = {Chandra, Ashok K. and Raghavan, Prabhakar and Ruzzo, Walter L. and Smolensky, Roman and Tiwari, Prasoon},
	date = {1996/12/01},
	date-added = {2024-01-21 13:42:29 +0100},
	date-modified = {2024-01-21 13:43:03 +0100},
	doi = {10.1007/BF01270385},
	id = {Chandra1996},
	isbn = {1420-8954},
	journal = {computational complexity},
	number = {4},
	pages = {312--340},
	title = {The electrical resistance of a graph captures its commute and cover times},
	url = {https://doi.org/10.1007/BF01270385},
	volume = {6},
	year = {1996},
	bdsk-url-1 = {https://doi.org/10.1007/BF01270385}}

@article{gray_toeplitz_2005,
	title = {Toeplitz and {Circulant} {Matrices}: {A} {Review}},
	volume = {2},
	shorttitle = {Toeplitz and {Circulant} {Matrices}},
	doi = {10.1561/0100000006},
	language = {en},
	number = {3},
	urldate = {2024-02-02},
	journal = {Foundations and Trends in Communications and Information Theory},
	author = {Gray, Robert M.},
	year = {2005},
	pages = {155--239},
}

@misc{dwivedi2023long,
      title={Long Range Graph Benchmark}, 
      author={Vijay Prakash Dwivedi and Ladislav Rampášek and Mikhail Galkin and Ali Parviz and Guy Wolf and Anh Tuan Luu and Dominique Beaini},
      year={2023},
      eprint={2206.08164},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{tönshoff2023did,
      title={Where Did the Gap Go? Reassessing the Long-Range Graph Benchmark}, 
      author={Jan Tönshoff and Martin Ritzert and Eran Rosenbluth and Martin Grohe},
      year={2023},
      eprint={2309.00367},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{gutteridge2023drew,
  title={{DRew}: Dynamically Rewired Message Passing with Delay},
  author={Gutteridge, Benjamin and Dong, Xiaowen and Bronstein, Michael M and Di Giovanni, Francesco},
  booktitle={International Conference on Machine Learning},
  pages={12252--12267},
  year={2023},
  organization={PMLR}
}

@misc{ba2016layer,
      title={Layer Normalization}, 
      author={Jimmy Lei Ba and Jamie Ryan Kiros and Geoffrey E. Hinton},
      year={2016},
      eprint={1607.06450},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@book{mixingtimes,
  added-at = {2010-01-19T17:51:27.000+0100},
  author = {Levin, David A. and Peres, Yuval and Wilmer, Elizabeth L.},
  biburl = {https://www.bibsonomy.org/bibtex/2097dc4d1d0e412b2444f540b04110797/tmalsburg},
  interhash = {61354795a6accb6407bfdbf04753a683},
  intrahash = {097dc4d1d0e412b2444f540b04110797},
  keywords = {markovchains probabilitytheory textbook},
  publisher = {American Mathematical Society},
  timestamp = {2010-01-19T17:51:27.000+0100},
  title = {{Markov chains and mixing times}},
  year = 2006
}

@article{modularity, title={Modularity and community structure in networks}, volume={103}, DOI={10.1073/pnas.0601602103}, number={23}, journal={Proceedings of the National Academy of Sciences}, author={Newman, M. E.}, year={2006}, month={Jun}, pages={8577–8582}} 