\section{Related work}
\textbf{Graph rewiring.}
A key component for GNNs is the input graph, since it not only acts as the data for model training but is also the computational structure on which \emph{message passing} Kipf, "Semi-Supervised Classification with Graph Convolutional Networks" ____, caused by topological bottlenecks, which affect how information is diffused. This highlights the importance of the graph topology and begs the question: how can we obtain an optimal computational structure that aligns with the downstream task? Graph rewiring has emerged as a popular technique to effect changes to the edge structure. This can be done based on various criteria. For instance, Chen et al., "Spectral Clustering for Unsupervised Learning" propose to use different variants of Ricci curvature ____ to rewire the graph, while Jin et al., "Graph Neural Networks with Scalable Community-based Sparsification" propose the effective resistance ____, and Chen et al., "Graph Attention Network with Spectral Rewiring" transform the input graph into an expander graph ____ for efficient message passing. 
Edges can be added or deleted and even though GNNs should be able to learn to drop task-irrelevant neighbors, trainability and expressiveness issues can limit this ability Chen et al., "Deeper Insights into Graph Convolutional Networks" which explains why edge deletions can also help fight over-smoothing in addition to over-squashing ____.

\textbf{Spectral gap maximization.}
Contemporaneously, spectral-based methods such as Faruqui et al., "Regularizing Neural Language Models with Multi-Task Learning" aim to \textit{maximize} the spectral gap by edge additions, as a larger spectral gap is inherently linked to faster mixing time ____ and thus better information flow. However, this can be detrimental in the case of heterophilic graphs ____ as we might add edges between nodes of different labels resulting in over-smoothing ____. The spectral gap can also be maximized by deleting edges ____ and this has shown to be beneficial in slowing down detrimental over-smoothing while simultaneously mitigating over-squashing, especially in heterophilic settings. Contrarily, Chen et al., "Graph Spectral Methods for Graph Neural Networks" advocate for spectral gap \textit{minimization}, but do not explain when this could be advantageous. 

\textbf{Graph and task alignment.}
Our findings reveal that the underlying mechanism enhancing GNN performance by rewiring actually depends on whether we modify edges connecting nodes with similar or dissimilar features, that are usually associated with similar or dissimilar labels. 
In fact, Chen et al., "Understanding Graph Neural Networks with Analytical Community Detection" take a first step in this direction by analysing the interplay between community and node-labels. 
They propose an information-theoretic metric, and demonstrate its impact on performance by artificially creating and destroying communities in real-world graphs. This also highlights the importance of the positive influence of same-label neighbours and how different-label neighbours can impair node classification performance ____.
We take this analysis several steps further and analyze why spectral rewiring cannot induce this alignment (\autoref{th:sbmsgproof}).  

The desirability of alignment between the graph structure and the task in GNNs has been explored in the context of their training dynamics by Li et al., "Understanding Graph Neural Networks with Analytical Training Dynamics" . This study theoretically analyzes how GNN models tend to align their Neural Tangent Kernel (NTK) matrix $\mathbf{\Theta}_t$ with the adjacency matrix $A$ of the input graph. 
They further derive a generalization bound for the NTK regime without considering node features, specifically in cases where the adjacency matrix $A$ is well-aligned with the optimal kernel matrix $\mathbf{\Theta}^*$. 
This matrix $\mathbf{\Theta}^*$ precisely indicates whether a pair of nodes share the same label, making this concept of alignment similar to ours \textemdash though not explicitly referring to the graph's communities\textemdash~and to the concept of homophily. 
Our theory on SBMs supports this result on GNN performance, while additionally relating it to the denoising effect of node features by their neighborhoods (\autoref{th:sbmperfproof}) and considering different levels of alignment (\autoref{th:sbmnoiseproof}).