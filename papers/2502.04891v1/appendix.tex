\section{Proofs} \label{app:proofs}
\subsection{Proof of \autoref{th:sbmsgproof}}

\begin{proof} 
We consider an SBM with 2 classes and $\frac{N}{2}$ nodes in each class, with intra-class edge probability $p$ and interclass edge probability $q$. Its adjacency matrix $A$ is a random matrix where $A_{ij}=\text{Bernoulli}(p)$ if nodes $i,j$ are in the same cluster, and $A_{ij}=\text{Bernoulli}(q)$ otherwise.
For a large $N$, the adjacency matrix $A$ can be approximated by its expected value, which is a block matrix:
$\tilde{A}=\begin{pmatrix}
    P&Q\\Q&P
\end{pmatrix}$, where $P=p\cdot\mathbbm{1}_{\frac{N}{2}} + (1-p)I_{\frac{N}{2}}$, where all values are $p$ except the diagonal which consists of ones, and $Q=q\cdot\mathbbm{1}_{\frac{N}{2}}$, where all values are $q$.
By summing up each row we find that the expected degree matrix $\tilde{D}$ is the diagonal matrix with entries $\tilde{D}_{ii} = 1-p + \frac{N}{2}(p+q)$.

To find the second largest eigenvalue $\lambda_2$, we need to spectrally analyze the (expected) normalized Laplacian of $\tilde{A}$; that is, $\mathcal{L}=I-\tilde{D}^{-1/2}\tilde{A}\tilde{D}^{-1/2}$. We have that $\tilde{D}^{-1/2} = \left(\frac{1}{1-p + \frac{N}{2}(p+q)}\right)^{1/2}I_N$, so $$\tilde{D}^{-1/2}\tilde{A}\tilde{D}^{-1/2} = \left(\frac{1}{1-p + \frac{N}{2}(p+q)}\right)\tilde{A}\coloneqq \tilde{d}\tilde{A}\text{, where we define $\tilde{d}$ for convenience.}$$

Then $\mathcal{L}=I-\tilde{d}\tilde{A}$. We need to find $\lambda$ such that $\det\left(\mathcal{L}-\lambda I\right)  = 0$.  
$$\mathcal{L}-\lambda I 
= I-\tilde{d}\tilde{A}-\lambda I
= -\tilde{d}\tilde{A}-(\lambda-1) I
=\begin{pmatrix}
    -\tilde{d}P-(\lambda-1) I&-\tilde{d}Q\\-\tilde{d}Q&-\tilde{d}P-(\lambda-1) I
\end{pmatrix}$$ 

$(-\tilde{d}P-(\lambda-1) I)$ and $-\tilde{d}Q$ commute, so by \citep{Silvester_2000}, the determinant of that matrix is $$ \det\left(\left(-\tilde{d}P-(\lambda-1) I\right)^2 - \left(-\tilde{d}Q\right)^2\right) 
= \det\left(\tilde{d}(Q-P)-(\lambda-1) I\right)\det\left(-\tilde{d}(Q+P)-(\lambda-1) I\right)
$$

We have that $Q - P 
= (q - p)\cdot\mathbbm{1}_{\frac{N}{2}} + (1-p) I_{\frac{N}{2}}$, which has eigenvalues $(1-p)$ and $((q - p)\frac{N}{2}+(1-p))$, so we finally get the required eigenvalue $$\lambda_2 = \tilde{d}((q - p)\frac{N}{2}+(1-p)) + 1
= \frac{(q - p)\frac{N}{2} + (1-p)}{(q + p)\frac{N}{2} + (1-p)} + 1$$

For $N > 2$ and $p,q\in(0,1)$: $\frac{\partial}{\partial p}\left(\frac{(q - p)\frac{N}{2} + (1-p)}{(q + p)\frac{N}{2} + (1-p)} + 1\right) = 
-\frac{2N(Nq + 2)}{\left((N - 2)p + Nq + 2\right)^2} < 0$, while 
$\frac{\partial}{\partial q}\left(\frac{\left(  (p - q)\frac{N}{2} + (1-p)\right)}{\left( (p + q)\frac{ N}{2} + (1-p)\right)} + 1\right) 
= 
\frac{2 N^2 p}{\left((N - 2) p + N q + 2\right)^2} > 0
$. This proves that $\lambda_2$ increases when $p$ decreases, and when $q$ increases. So a higher spectral gap is related to a lower community structure.
\end{proof} 

\subsubsection{Extensions of the theorem}

The argument still follows for a higher amount of blocks. Let $\tilde{A}=\begin{pmatrix}
    P&Q&Q\\Q&\ddots&Q\\Q&Q&P
\end{pmatrix}$ with $k$ diagonal $P$ blocks of sizes $\frac{N}{k}$ each. The degree of every node is now $\tilde{D}_{ii}=1-p+\frac{N}{k}p+\frac{N(k-1)}{k}q$. Because of the block structure of our matrix, we still get the second eigenvalue from the difference between on and off diagonal blocks $Q-P$, which now has eigenvalues $(1-p)$ and $((q-p)\frac{N}{k}+(1-p))$. Therefore
 $$\lambda_2 = \tilde{d}((q - p)\frac{N}{k}+(1-p)) + 1
= \frac{(q - p)\frac{N}{k} + (1-p)}{\frac{N}{k}p+\frac{N(k-1)}{k}q + (1-p)} + 1
=
\frac{-k (p - 1) - N (p - q)}{k (N q - p + 1) + N (p - q)} + 1$$
If $k$ is constant with respect to $N$, this quantity grows like $1-\frac{p-q}{(k-1) q + p}$. If $k=aN$, then it goes to 1 as $N$ increases.

The argument also still holds for different-sized communities. Let $\tilde{A}=\begin{pmatrix}
    P1&Q1\\Q2&P2
\end{pmatrix}$, where $P1=p\cdot\mathbbm{1}_{M} + (1-p)I_{M}$ and $P2=p\cdot\mathbbm{1}_{N-M} + (1-p)I_{N-M}$, where all values are $p$ except the diagonal which consists of ones, and $Q1=q\cdot\mathbbm{1}_{M}$, $Q2=q\cdot\mathbbm{1}_{N-M}$, where all values are $q$. We assume that $M > N-M$.
Then $\tilde{D}$ is the diagonal matrix with entries $\tilde{D}_{ii} = (1+(M-1)p + (N-M)q)$ if $i<M$, and $\tilde{D}_{ii} = (1+(N-M-1)p + Mq)$ otherwise. We define $\tilde{d}_1=\frac{1}{1+(M-1)p + (N-M)q}$ and $\tilde{d}_2=\frac{1}{1+(N-M-1)p + Mq}$ for convenience. Because $\tilde{d}_1 < \tilde{d}_2$, the second largest eigenvalue will come from the interactions of the first block. So the eigenvalues are $(1-p)$ and $((q-p)M+(1-p))$. Therefore
 $$\lambda_2 = \tilde{d}_1((q-p)M+(1-p)) + 1
= \frac{(q-p)M+(1-p)}{(1+(M-1)p + (N-M)q)} + 1
$$
If $M= a N$, this quantity grows like $1-\frac{a(p-q)}{ap + (1-a)q}$. If $M$ is constant, then the second block gets bigger than the first and we get the second eigenvalue from it instead.

\subsection{Proof of \autoref{th:sbmperfproof}}


\begin{proof} 
	
	We consider an SBM with 2 classes and $\frac{N}{2}$ nodes in each class, with intra-class edge probability $p$ and inter-class edge probability $q$.
	Each node $i\in\{0,\ldots,N-1\}$ has one feature, $x_i$, and a label $\ell_i$ which corresponds to the block it belongs to: $\ell_i = 1 \Leftrightarrow i \geq \frac{N}{2}$. The task is, therefore, to predict each node's community association. In this case, the alignment of communities and labels is perfect.
	
	Each feature $x_i$ is aligned with its label following a normal distribution: class-$0$ node features follow $\mathcal{N}(-\mu_0,\sigma_0^2)$, while class-$1$ node features follow $\mathcal{N}(\mu_0,\sigma_0^2)$, as shown in Figure \ref{fig:features}. 
	A perfect classifier $f$ without any knowledge of the graph structure builds a decision boundary at $x=0$. The expected number of misclassified nodes is $\frac{N}{2}$ times the intersection area of both distributions \textemdash because they are normalized from a population of $\frac{N}{2}$ each. Such area is $2 \cdot \Phi(\frac{-\mu_0}{\sigma_0})$, where $\Phi$ is the cumulative distribution function of the standard normal distribution (see Figure \ref{fig:cumulativefunct}). Therefore, the proportion of misclassifications is $e(f) = \frac{N}{2}\cdot 2\cdot \Phi(\frac{-\mu_0}{\sigma_0}) \cdot \frac{1}{N} = \Phi(\frac{-\mu_0}{\sigma_0})$. As $\Phi$ is a cumulative function, it is monotonically increasing with respect to its argument.
	
	The classification error of $f$ can be reduced by performing a step of message passing on the graph, which utilizes the community information to further separate the two classes. We shall consider a single round of sum aggregation as an example.
	
	\begin{figure}[ht]
		\centering
		\subfigure[The distribution of features from both clusters before training. The area in purple corresponds to nodes wrongly classified by the decision boundary $x=0$.]{
			\begin{tikzpicture}
				\begin{axis}[
					xlabel = $x_i$,
					ylabel = {Density},
					ymin = 0,
					ymax = 0.6,
					legend style={
						at={(0.5,1)},
						anchor= north,
						legend columns=2,
					},
					extra x ticks       = {-1, 1},
					extra x tick labels = {$-\mu_0$, $\mu_0$},
					xtick = {0},
					xtick style = {draw = black},
					ytick = \empty,
					height=4.5cm,
					width=7.5cm,
					]
					\addplot [
					name path=A,
					domain=-4:4, 
					samples=100, 
					color=red,
					]
					{gauss(-1,1)};
					\addlegendentry{$\mathcal{N}(-\mu_0,\sigma_0^2)$}
					\addplot [
					name path=B,
					domain=-4:4, 
					samples=100, 
					color=blue,
					]
					{gauss(1,1)};
					\addlegendentry{$\mathcal{N}(\mu_0,\sigma_0^2)$}
					\path [name path=level] (axis cs:-4,0) -- (axis cs:4,0);
					\addplot fill between[of=A and level, soft clip={domain=0:4},
					every even segment/.style  = {blue!60!red, opacity=0.3},
					];
					\addplot fill between[of=B and level, soft clip={domain=-4:0},  
					every even segment/.style  = {blue!60!red, opacity=0.3},
					];
				\end{axis}
			\end{tikzpicture}
			\label{fig:features}}\quad
		\subfigure[The cumulative distribution function at $x=-1$ of $\mathcal{N}(\mu_0,\sigma_0^2)$ is equal to the cumulative distribution function at $x=-\frac{\mu_0}{\sigma_0}$ of the standard normal distribution $\mathcal{N}(0,1)$, which is $\Phi(\frac{-\mu_0}{\sigma_0})$. The purple area of Figure \ref{fig:features} is two times this quantity.]{
			\begin{tikzpicture}
				\begin{axis}[
					xlabel = $x_i$,
					ylabel = {Density},
					ymin = 0,
					ymax = 0.6,
					legend pos = north east,
					extra x ticks       = {-1},
					extra x tick labels = {$-\frac{\mu_0}{\sigma_0}$},
					xtick = {0},
					xtick style = {draw = black},
					ytick = \empty,
					height=4.5cm,
					width=7.5cm,
					]
					\addplot [
					name path=A,
					domain=-4:4, 
					samples=100, 
					color=black,
					]
					{gauss(0,1)};
					\addlegendentry{$\mathcal{N}(0,1)$}
					\draw[dashed,thick, name path=line] (-1, 0) -- (-1, {1/(1*sqrt(2*pi))*exp(-((-1-0)^2)/(2*1^2))});
					
					\begin{scope}
						\addplot [
						domain=-4:-1, 
						samples=100, 
						fill=black,
						pattern=north east lines,
						]
						{gauss(0,1)}  \closedcycle;
					\end{scope}
				\end{axis}
			\end{tikzpicture}
			\label{fig:cumulativefunct}}
            \caption{Illustration of the setup for the feature distributions for \autoref{th:sbmperfproof}.}
	\end{figure}
	
	Any node has an expected $\mathcal{E}_p=p\cdot\left(\frac{N}{2}-1\right)$ intra-class neighbors, plus itself, and an expected $\mathcal{E}_q=q\cdot\frac{N}{2}$ inter-class neighbors. In the next proof \ref{app:sbmnoiseproof} we compute the same quantity with neighbour distributions instead, for a more fine-grained approximation. The hidden state of a class-1 node $i$ after a step of sum aggregation is therefore the sum of $\mathcal{E}_p + 1$ random variables $\sim\mathcal{N}(\mu_0,\sigma_0^2)$ and $\mathcal{E}_q$ random variables $\sim\mathcal{N}(-\mu_0,\sigma_0^2)$. This follows another normal distribution with mean $\mu_1\coloneqq \mu_0\cdot(1+\mathcal{E}_p-\mathcal{E}_q)$ and variance $\sigma_1^2\coloneqq\sigma_0^2\cdot(1+\mathcal{E}_p+\mathcal{E}_q)$. Conversely, the hidden state of a class-0 node $i$ follows a normal distribution of mean $-\mu_1$ and the same variance $\sigma_1^2$.
	The decision boundary of a perfect classifier is still at $x=0$, but the average proportion of misclassified nodes is now $\Phi(\frac{-\mu_1}{\sigma_1})$, which depends on $p$ and $q$. Specifically, it tends to be monotonically decreasing with respect to $p$; this means that the higher the community structure, the more accurate the classifier can be, because there is more information to utilize.
	
	Let us take $\mu_0=1$ and $\sigma_0=1$ to simplify the calculations. We need to check that $\frac{\partial}{\partial p}\left(\frac{-\mu_1}{\sigma_1} \right) < 0$. For $N > 2$ and $p,q\in(0,1)$:
	
	\begin{align*}
		\mu_1 &= 1\cdot(1+p\left(\frac{N}{2}-1\right)-q\cdot\frac{N}{2}) = 1-p+\frac{N}{2}\cdot(p-q) \\
		\sigma_1^2 &= 1\cdot(1+p\left(\frac{N}{2}-1\right)+q\cdot\frac{N}{2}) = 1-p+\frac{N}{2}\cdot(p+q) \\
		-\frac{\mu_1}{\sigma_1} &= -\frac{1-p+\frac{N}{2}\cdot(p-q)}{\sqrt{1-p+\frac{N}{2}\cdot(p+q)}}
		\\
		\frac{\partial}{\partial p}\left(\frac{-\mu_1}{\sigma_1} \right) &=
		-\frac{(N - 2)((N - 2)p + 3Nq + 2)}{2\sqrt{2}\left((N - 2)p + Nq + 2\right)^{\frac{3}{2}}}
		< 0
		\\
		&\Longleftrightarrow\ (N - 2)((N - 2)p + 3Nq + 2) > 0
	\end{align*}
	
	On the other side, $\frac{\partial}{\partial q}\left(\frac{-\mu_1}{\sigma_1} \right) > 0$. 
	\begin{align*}
		\frac{\partial}{\partial q}\left(\frac{-\mu_1}{\sigma_1} \right) &=
		\frac{N(6 + 3(N-2)p + Nq)}{2\sqrt{2}(2 + (N-2)p + Nq)^{\frac{3}{2}}} > 0
		\Longleftrightarrow\ N(6 + 3(N-2)p + Nq) > 0 
	\end{align*}
	
	This proves that, by reducing the community structure (either by decreasing $p$ or increasing $q$), then the quantity $\frac{-\mu_1}{\sigma_1}$ increases, so the expected proportion of misclassified nodes $e(f) = \Phi\left(\frac{-\mu_1}{\sigma_1}\right)$ also increases. In consequence, it harms the performance of classifier $f$.

The graph's information provides a better separation between the two classes if the intra-class edge probability is high enough. From this we can conclude that reducing the intra-class edge probability is not a good strategy to improve the classification performance for any model on the graph.
\end{proof}


\subsection{Proof of \autoref{th:sbmnoiseproof}} \label{app:sbmnoiseproof}
\begin{proof}
We consider another SBM with 2 classes and $\frac{N}{2}$ nodes in each class, with intra-class edge probability $p$ and inter-class edge probability $q$.
Each node $i\in\{0,\ldots,N-1\}$ has again one feature, $x_i$, aligned with its class label $\ell_i$ following a normal distribution: $\mathcal{N}(-\mu_0,\sigma_0^2)$ for class 0 and $\mathcal{N}(\mu_0,\sigma_0^2)$ for class 1. However, now $\ell_i$ corresponds to its community with a fixed probability $\psi$ \textemdash recovering \autoref{th:sbmperfproof} when $\psi=1$.

What is the probability of any node $i$ such that, after a round of sum aggregation, its modified representation $x_i'$ is now misclassified ($M$)? As the two classes are symmetric:
\begin{align*}
	P(M)
	&= P(M , L_0) + P(M , L_1) 
	 = P(L_0)P(M|L_0) + P(L_1)P(M|L_1) \\
	&= \frac{1}{2}P(M|L_0) + \frac{1}{2}P(M|L_1)
	= P(M|L_0)
\end{align*} 

Then the question becomes the following: what is the probability of a node with label $L_0$ being misclassified? It depends whether it belongs to community $C_0$ or $C_1$. 
\begin{align*}
	P(M|L_0)
	&= P(M, C_0|L_0) + P(M, C_1|L_0) 
	= P(C_0|L_0)P(M|L_0, C_0) + P(C_1|L_0)P(M|L_0, C_1)  \\
	&= \psi P(M|L_0, C_0) + (1-\psi)P(M|L_0, C_1)
\end{align*}

$P(M|L_0, C_0) = P(X_{(L_0,C_0)}' > 0)$. We now need to calculate what is the predicted label of a $(L_0,C_0)$ node after a sum aggregation round. For this we need the distribution of its neighbours.
We consider the node to have a self loop, as it uses its own feature too.

\begin{itemize}
	\item The number of nodes $(L_0,C_0)$ (that are not node $i$) follows a binomial distribution $N_{0} \sim B(\frac{N}{2}-1,\psi)$. 
 However, for easiness of proof we will approximate it by a normal distribution, which is appropriate for $N$ large enough: $N_{0}\sim \mathcal {N}((\frac{N}{2}-1)\psi, (\frac{N}{2}-1)\psi(1-\psi))$.
 The amount of them connected to node $i$ follows a conditional binomial distribution $H_{00} \sim B(n_{0},p)\ |\ N_{0} = n_{0}$, which we again approximate by $H_{00} \sim  \mathcal {N}(n_{0}p, n_{0}p(1-p))\ |\ N_{0} = n_{0}$. 
	\item The number of nodes $(L_0,C_1)$ that are connected to node $i$ follows $H_{01} \sim B(\frac{N}{2}-1 - n_{0},q)\ |\ N_{0} = n_{0}$, approximated by $H_{01} \sim \mathcal {N}((\frac{N}{2}-1 - n_{0})q, (\frac{N}{2}-1 - n_{0})q(1-q))\ |\ N_{0} = n_{0}$. 
\item Since $H_{00}$ and $H_{01}$ are conditionally independent given $N_{0} = n_{0}$, their sum $H_0 = H_{00} + H_{01}$ also follows a normal distribution with parameters given by the sum of their means and variances. Thus, the number of total $L_0$ nodes connected to node $i$ (except itself) follows $H_0\sim \mathcal {N}(n_{0}p + (\frac{N}{2}-1 - n_{0})q, n_{0}p(1-p) + (\frac{N}{2}-1 - n_{0})q(1-q))\ |\ N_{0} = n_{0}$. We are going to get rid of the dependency of $N_0$ by estimating it by a normal distribution with the mean and variance of the marginal distribution of $H_0$:
 \begin{align*}
 \mathbb{E}[H_0] &= \mathbb{E}[\mathbb{E}[H_0|N_0]] = \mathbb{E}[N_0]p + \left(\frac{N}{2}-1 - \mathbb{E}[N_0]\right)q \\
 &= \left(\frac{N}{2}-1\right)\psi p + \left(\frac{N}{2}-1 - \left(\frac{N}{2}-1\right)\psi\right)q \\
 &= \left(\frac{N}{2}-1\right) (p \psi + q (1-\psi))
 \end{align*}
 \begin{align*}
 \text{Var}[H_0] &= \mathbb{E}[\text{Var}(H_0|N_0)]+\text{Var}(\mathbb{E}[H_0|N_0]) \\
 &= \mathbb{E}[N_{0}]p(1-p) + \left(\frac{N}{2}-1 - \mathbb{E}[N_{0}]\right)q(1-q) \\
 &+  \text{Var}\left(N_0(p-q) + \left(\frac{N}{2}-1\right)q\right) \\ 
 %
 &= \left(\frac{N}{2}-1\right)\psi p(1-p) + \left(\frac{N}{2}-1 - \left(\frac{N}{2}-1\right)\psi\right)q(1-q) \\
 &+  (p-q)^2\left(\frac{N}{2}-1\right)\psi(1-\psi) \\ 
 %
 &= \left(\frac{N}{2}-1\right) (\psi p (1-p) + (1-\psi) q (1-q) + (p-q)^2\psi(1-\psi)) 
 %
 \end{align*}
	\item The number of nodes $(L_1,C_1)$ follows $N_{1} \sim B(\frac{N}{2},\psi)$, approximated by $N_{1}\sim \mathcal {N}(\frac{N}{2}\psi, \frac{N}{2}\psi(1-\psi))$.
 The amount of them connected to node $i$ follows $H_{11} \sim B(n_{1},q)\ |\ N_{1} = n_{1}$,
 %
 approximated by $H_{11} \sim  \mathcal {N}(n_{1}q, n_{1}q(1-q))\ |\ N_{1} = n_{1}$. 
	\item The number of nodes $(L_1,C_0)$ that are connected to node $i$ follows $H_{10} \sim B(\frac{N}{2} - n_{1},p)\ |\ N_{1} = n_{1}$, approximated by $H_{10} \sim \mathcal {N}((\frac{N}{2} - n_{1})p, (\frac{N}{2} - n_{1})p(1-p))\ |\ N_{1} = n_{1}$. 
 \item Similarly to $L_0$, the number of total $L_1$ nodes connected to node $i$ follows $H_1 \sim \mathcal {N}(n_{1}q + (\frac{N}{2} - n_{1})p, n_{1}q(1-q) + (\frac{N}{2} - n_{1})p(1-p))\ |\ N_{1} = n_{1}$. We will estimate it by a normal distribution with its mean and variance:
 \begin{align*}
 \mathbb{E}[H_1] &= \mathbb{E}[\mathbb{E}[H_1|N_1]] = \mathbb{E}[N_1]q + (\frac{N}{2} - \mathbb{E}[N_1])p \\
 &= \frac{N}{2}\psi q + \left(\frac{N}{2} - \frac{N}{2}\psi\right)p \\
 &= \frac{N}{2} (p (1-\psi) + q \psi) 
 \end{align*}
 \begin{align*}
 \text{Var}[H_1] &= \mathbb{E}[\text{Var}(H_1|N_1)]+\text{Var}(\mathbb{E}[H_1|N_1]) \\
 &= \mathbb{E}[N_1]q(1-q) + \left(\frac{N}{2} - \mathbb{E}[N_1]\right)p(1-p) +  \text{Var}\left(N_1 (q-p) + \frac{N}{2}p\right) \\ 
 %
 &= \frac{N}{2}\psi q(1-q) + \left(\frac{N}{2} - \frac{N}{2}\psi\right)p(1-p) +  (p-q)^2\frac{N}{2}\psi(1-\psi) \\ 
 %
 &= \frac{N}{2}(\psi q(1-q) + (1 - \psi)p(1-p)  +  (p-q)^2\psi(1-\psi)) 
 %
 \end{align*}
 
\end{itemize}

The representation of node $i$ after one step of sum aggregation is the summation of $H_0+1$ (independent) normal distributions $\sim\mathcal{N}(-\mu_0,\sigma_0^2)$ and $H_1$ (independent) normal distributions $\sim\mathcal{N}(\mu_0,\sigma_0^2)$. Therefore:
\begin{align*}
	X_{(L_0,C_0)}' \sim
	\mathcal{N}(-\mu_0(1+h_{0}-h_{1}),\ \sigma^2_0(1+h_{0}+h_{1})) \ |\ H_{0}=h_{0},\ H_{1}=h_{1} 
\end{align*}
Again calculating its mean and variance:
 \begin{align*}
 \mathbb{E}[X_{(L_0,C_0)}'] &= \mathbb{E}[\mathbb{E}[X_{(L_0,C_0)}'|H_0,H_1]] = -\mu_0(1+\mathbb{E}[H_{0}]-\mathbb{E}[H_{1}]) \\
 &= -\mu_0\left(1+\left(\frac{N}{2}-1\right) (p \psi + q (1-\psi))-\frac{N}{2} (p (1-\psi) + q \psi) \right)
 %%%%%%%%%%%%%
 \end{align*}
 \begin{align*}
 \text{Var}[X_{(L_0,C_0)}'] &= \mathbb{E}[\text{Var}(X_{(L_0,C_0)}'|H_0,H_1)]+\text{Var}(\mathbb{E}[X_{(L_0,C_0)}'|H_0,H_1]) \\
 &= \sigma^2_0(1+\mathbb{E}[H_{0}]+\mathbb{E}[H_{1}]) +\mu_0^2(\text{Var}(H_0)+\text{Var}(H_{1})) \\
 &= \sigma^2_0 \left(1+\left(\frac{N}{2}-1\right) (p \psi + q (1-\psi))+\frac{N}{2} (p (1-\psi) + q \psi)\right) \\ 
 &+\mu_0^2\left(\left(\frac{N}{2}-1\right) (\psi p (1-p) + (1-\psi) q (1-q) + (p-q)^2\psi(1-\psi))\right. \\
 &+ \left.\frac{N}{2}(\psi q(1-q) + (1 - \psi)p(1-p)  +  (p-q)^2\psi(1-\psi))\right)
 \end{align*}

For a more clear analysis of this formula, we take $\mu_0=1,\sigma_0=1$ and $N$ large enough:
 \begin{align*}
 \mathbb{E}[X_{(L_0,C_0)}'] &\approx -\frac{N}{2} (p \psi + q (1-\psi)- p (1-\psi) - q \psi) = -\frac{N}{2} (2 \psi - 1) (p - q)
 %%%%%%%%%%%%%
 \end{align*}
 \begin{align*}
 \text{Var}[X_{(L_0,C_0)}'] &\approx 
 \frac{N}{2} \bigg( p \psi + q (1-\psi) + p (1-\psi) + q \psi +  2(p-q)^2\psi(1-\psi)  \\ 
 &+\psi p (1-p) + (1-\psi) q (1-q) + \psi q(1-q) + (1 - \psi)p(1-p)  \bigg) \\
 &= 
 \frac{N}{2} ( p+q + p(1-p) + q(1- q)  +  2(p-q)^2\psi(1-\psi) )
 \end{align*}

Finally, we have $P(X_{(L_0,C_0)}' > 0) \approx 1 - \Phi\left(\frac{-\mathbb{E}[X_{(L_0,C_0)}']}{\sqrt{ \text{Var}[X_{(L_0,C_0)}']}} \right) =$ $$\Phi\left(
\frac{\frac{N}{2} (2 \psi - 1) (p - q)}{\sqrt{\frac{N}{2} ( p+q + p(1-p) + q(1- q)  +  2(p-q)^2\psi(1-\psi) )}}
\right)$$

For $P(M|L_0, C_1) = P(X_{(L_0, C_1)}' > 0)$, the calculation of the predicted label of a $(L_0,C_1)$ node follows exactly the same steps, but exchanging $p$ and $q$, as the probabilities for nodes to be connected to node $i$ are now exactly of the opposite community. So we have $P(X_{(L_0, C_1)}' > 0) \approx $ 
 \begin{align*}
&\Phi\left(
\frac{\frac{N}{2} (2 \psi - 1) (q - p)}{\sqrt{\frac{N}{2} ( p+q + p(1-p) + q(1- q)  +  2(p-q)^2\psi(1-\psi) )}}
\right) =\  1-P(X_{(L_0,C_0)}' > 0)
 \end{align*}

And $
P(M) \approx \psi P(X_{(L_0,C_0)}' > 0) + (1-\psi)(1-P(X_{(L_0,C_0)}' > 0)) = (1-\psi)+(2\psi-1)P(X_{(L_0,C_0)}' > 0)$. 
\end{proof}

\section{Algorithms}\label{app:algs}






\begin{algorithm} [t]
   \caption{Proxy Spectral Gap based Greedy Graph Addition (\textsc{ProxyAddMin})}
   \label{alg:proxyaddmin}
   \begin{algorithmic}
       \REQUIRE \!\!Graph ${\mathcal{G}} = (\mathcal{V},{\mathcal{E}})$, num. edges to add $N$, spectral gap $ \lambda_1(\mathcal{L}_{{\mathcal{G}}})$, second eigenvector ${f}$ of ${\mathcal{G}}$.
       \REPEAT
       \FOR {$(u,v) \in \bar{\mathcal{E}}$}
       \STATE {Consider $\hat{\mathcal{G}} = \mathcal{G}\cup (u,v)$.}
           \STATE{Calculate proxy value for the spectral gap of $\hat{\mathcal{G}}$ used in \citet{jamadandi2024spectral}:}
           \STATE{$\lambda_1(\mathcal{L}_{\hat{\mathcal{G}}}) \approx \lambda_1(\mathcal{L}_{{\mathcal{G}}}) + ((f_u - f_v)^2 - \lambda_1(\mathcal{L}_{{\mathcal{G}}})\cdot (f^2_u + f^2_v))$}
       \ENDFOR \\
           \STATE{Find the edge that maximizes the proxy: $(u^{+},v^{+}) = \argmin\limits_{(u,v) \in \bar{\mathcal{E}}} \lambda_1(\mathcal{L}_{\hat{\mathcal{G}}})$.}\vspace{-\medskipamount}
           \STATE{Update graph edges: ${\mathcal{E}} = {\mathcal{E}} \cup \{(u^{+},v^{+})\}$.}
           \STATE{Update degrees: $d_{u^{+}} = d_{u^{+}}+1, d_{v^{+}}= d_{v^{+}}+1$}
           \STATE{Update eigenvectors and eigenvalues of ${\mathcal{G}}$ accordingly.}
       \UNTIL{$N$ edges added.}
       
           \textbf{Output :} Denser graph $ \hat{\mathcal{G}} = (\mathcal{V},\hat{\mathcal{E}})$.
       \end{algorithmic}
\end{algorithm}


\begin{algorithm} [t]
   \caption{Proxy Spectral Gap based Greedy Graph Sparsification (\textsc{ProxyDelMin})}
   \label{alg:proxydelmin}
   \begin{algorithmic}
       \REQUIRE \!Graph ${\mathcal{G}} = (\mathcal{V},{\mathcal{E}})$, num. edges to prune $N$, spectral gap $ \lambda_1(\mathcal{L}_{{\mathcal{G}}})$, second eigenvector ${f}$.
       \REPEAT
       \FOR {$(u,v) \in {\mathcal{E}}$}
       \STATE {Consider $\hat{\mathcal{G}} = \mathcal{G}\setminus (u,v)$.}
           \STATE{Calculate proxy value for the spectral gap of $\hat{\mathcal{G}}$ used in \citet{jamadandi2024spectral}:}
           \STATE{$\lambda_1(\mathcal{L}_{\hat{\mathcal{G}}}) \approx \lambda_1(\mathcal{L}_{{\mathcal{G}}}) - ((f_u - f_v)^2 - \lambda_1(\mathcal{L}_{{\mathcal{G}}})\cdot (f^2_u + f^2_v))$}
       \ENDFOR \\
           \STATE{Find the edge that minimizes the proxy: $(u^{-},v^{-}) = \argmin\limits_{(u,v) \in {\mathcal{E}}} \lambda_1(\mathcal{L}_{\hat{\mathcal{G}}})$.}\vspace{-\medskipamount}
           \STATE{Update graph edges: ${\mathcal{E}} = {\mathcal{E}} \setminus \{(u^{-},v^{-})\}$.}
           \STATE{Update degrees: $d_{u^{-}} = d_{u^{-}}-1, d_{v^{-}}= d_{v^{-}}-1$}
           \STATE{Update eigenvectors and eigenvalues of ${\mathcal{G}}$ accordingly.}
       \UNTIL{$N$ edges deleted.}

           \textbf{Output :} Sparse graph  $ \hat{\mathcal{G}} = (\mathcal{V},\hat{\mathcal{E}})$.
       \end{algorithmic}
   \end{algorithm}

   
\subsection{Spectral gap minimization}
We use the algorithms presented in \citet{jamadandi2024spectral} for adding (\texttt{ProxyAddMax}) and deleting edges (\texttt{ProxyDelMax}) based on a proxy of the spectral gap. We modify these algorithms to minimize the gap and call them 
\texttt{ProxyAddMin} (Alg. \ref{alg:proxyaddmin}) and \texttt{ProxyDelMin} (Alg. \ref{alg:proxydelmin}).
\subsection{Community and Feature similarity algorithms}
We present the three algorithm families proposed in this paper for graph rewiring: \texttt{ComMa} (Algs. \ref{alg:HigherComMa}, \ref{alg:LowerComMa}), \texttt{FeaSt} (Alg. \ref{alg:FeaSt}), and \texttt{ComFy} (Alg. \ref{alg:ComFy}). Each algorithm has an addition and a deletion variant. Furthermore, \texttt{ComMa} has two extra variants for increasing (\texttt{HigherComMa}, Alg. \ref{alg:HigherComMa}) or decreasing (\texttt{LowerComMa}, Alg. \ref{alg:LowerComMa}) the community structure. 

\begin{algorithm} [H]
   \caption{\texttt{HigherComMa}: Increasing community structure. Variants: \texttt{ADD}, \texttt{DEL}}
   \label{alg:HigherComMa}
   \begin{algorithmic}
       \REQUIRE \!\!Graph ${\mathcal{G}} = (\mathcal{V},{\mathcal{E}})$, num. edges to add (\texttt{ADD}) / delete (\texttt{DEL}) $N$. \\
       \STATE{Calculate communities (here used Louvain): $(C_0,\ldots,C_m) = \text{CommunityDetection}(\mathcal{G})$}
       \IF{\texttt{ADD}}
       \STATE{Consider set of edges to add: $E = \{(u,v)  \in \bar{\mathcal{E}}\ |\ \text{Comm}[u]=\text{Comm}[v]\}$}
       \ELSIF{\texttt{DEL}}
       \STATE{Consider set of edges to delete: $E = \{(u,v) \in \mathcal{E}\ |\ \text{Comm}[u]\neq\text{Comm}[v]\}$}
       \ENDIF
       \REPEAT
       \IF{\texttt{ADD}}
       \STATE{Randomly and uniformly pick an edge $e$ from $E$}
       \STATE{Update graph edges: $\mathcal{E} = \mathcal{E} \cup \{e\}$ (\texttt{ADD}) or $\mathcal{E} = \mathcal{E} \setminus \{e\}$ (\texttt{DEL})}
       \ENDIF
       \UNTIL{$N$ edges modified.}
       
           \textbf{Output :} Modified graph $ \hat{\mathcal{G}} = (\mathcal{V},\hat{\mathcal{E}})$.
       \end{algorithmic}
\end{algorithm}

\begin{algorithm} [H]
   \caption{\texttt{LowerComMa}: Decreasing community structure. Variants: \texttt{ADD}, \texttt{DEL}}
   \label{alg:LowerComMa}
   \begin{algorithmic}
       \REQUIRE \!\!Graph ${\mathcal{G}} = (\mathcal{V},{\mathcal{E}})$, num. edges to add (\texttt{ADD}) / delete (\texttt{DEL}) $N$. \\
       \STATE{Calculate communities (here used Louvain): $(C_0,\ldots,C_m) = \text{CommunityDetection}(\mathcal{G})$}
       \IF{\texttt{ADD}}
       \STATE{Consider set of edges to add: $E = \{(u,v) \in \bar{\mathcal{E}}\ |\ \text{Comm}[u]\neq\text{Comm}[v]\}$}
       \ELSIF{\texttt{DEL}}
       \STATE{Consider set of edges to delete: $E = \{(u,v) \in \mathcal{E}\ |\ \text{Comm}[u]=\text{Comm}[v]\}$}
       \ENDIF
       \REPEAT
       \IF{\texttt{ADD}}
       \STATE{Randomly and uniformly pick an edge $e$ from $E$}
       \STATE{Update graph edges: $\mathcal{E} = \mathcal{E} \cup \{e\}$ (\texttt{ADD}) or $\mathcal{E} = \mathcal{E} \setminus \{e\}$ (\texttt{DEL})}
       \ENDIF
       \UNTIL{$N$ edges modified.}
       
           \textbf{Output :} Modified graph $ \hat{\mathcal{G}} = (\mathcal{V},\hat{\mathcal{E}})$.
       \end{algorithmic}
\end{algorithm}


\begin{algorithm} [H]
   \caption{\texttt{FeaSt}: Maximizing feature similarity. Variants: \texttt{ADD}, \texttt{DEL}}
   \label{alg:FeaSt}
   \begin{algorithmic}
       \REQUIRE \!\!Graph ${\mathcal{G}} = (\mathcal{V},{\mathcal{E}})$, node features $X$, num. edges to add (\texttt{ADD}) / delete (\texttt{DEL}) $N$. \\
       \STATE{Calculate the pairwise cosine similarity of $X$: $\text{sim}(u,v)$.}
       \STATE{Calculate the mean of the current graph's similarity values: $\overline{\text{sim}} = \frac{1}{|\mathcal{E}|}\sum_{(u,v)\in\mathcal{E}}\text{sim}(u,v)$}
       \IF{\texttt{ADD}}
       \FOR {$(u,v) \in \bar{\mathcal{E}}$}
           \STATE{Calculate the graph's mean similarity in the presence of $(u,v)$: $\text{rank}(u,v) = \frac{\overline{\text{sim}}|\mathcal{E}| + \text{sim}(u,v)}{|\mathcal{E}|+1}$}
       \ENDFOR
       \ELSIF{\texttt{DEL}}
       \FOR {$(u,v) \in {\mathcal{E}}$}
           \STATE{Calculate the graph's mean similarity in the absence of $(u,v)$: $\text{rank}(u,v) = \frac{\overline{\text{sim}}|\mathcal{E}| - \text{sim}(u,v)}{|\mathcal{E}|-1}$}
       \ENDFOR
       \ENDIF
        \STATE{Find top $N$ edges in the ranking: $E_N = \text{top}_N(\text{rank}(u,v))$}
       \STATE{Update graph edges: $\mathcal{E} = \mathcal{E} \cup E_N$ (\texttt{ADD}) or $\mathcal{E} = \mathcal{E} \setminus E_N$ (\texttt{DEL})}
       
           \textbf{Output :} Modified graph $ \hat{\mathcal{G}} = (\mathcal{V},\hat{\mathcal{E}})$.
       \end{algorithmic}
\end{algorithm}

\begin{algorithm} [H]
   \caption{\texttt{ComFy}: Maximizing feature similarity across communities. Variants: \texttt{ADD}, \texttt{DEL}}
   \label{alg:ComFy}
   \begin{algorithmic}
       \REQUIRE \!\!Graph ${\mathcal{G}} = (\mathcal{V},{\mathcal{E}})$, node features $X$, (approx.) num. edges to add (\texttt{ADD}) / delete (\texttt{DEL}) $N$. \\
       \STATE{Calculate the pairwise cosine similarity of $X$: $\text{sim}(u,v)$.}
       \STATE{Calculate communities (here used Louvain): $(C_0,\ldots,C_m) = \text{CommunityDetection}(\mathcal{G})$}
       \STATE{Consider pairs of communities: $\mathcal{C} = \{(C_i,C_j) \in C\times C\ |\ i\leq j\}$}
       \FOR {$(C_i,C_j) \in \mathcal{C}$}
       \STATE{Existing edges between $C_i$ and $C_j$: ${E_{ij}}=\{ (u,v) \in {\mathcal{E}}\ |\ \text{Comm}[u] = C_i \wedge \text{Comm}[v] = C_j \}$}
       \STATE{Calculate the mean of the existing edges' similarities: $\overline{\text{sim}_{ij}} = \frac{1}{|E_{ij}|}\sum_{(u,v)\in E_{ij}}\text{sim}(u,v)$}
       \IF{\texttt{ADD}}
       \STATE{Non-existing edges between $C_i,C_j$: $\overline{E_{ij}}=\{ (u,v) \in \bar{\mathcal{E}}\ |\ \text{Comm}[u] = C_i \wedge \text{Comm}[v] = C_j \}$}
       \FOR {$(u,v) \in \overline{E_{ij}}$}
           \STATE{Calculate the mean similarity in the presence of $(u,v)$: $\text{rank}_{ij}(u,v) = \frac{\overline{\text{sim}_{ij}}|{E_{ij}}| + \text{sim}(u,v)}{|{E_{ij}}|+1}$}
       \ENDFOR
       \ELSIF{\texttt{DEL}}
       \FOR {$(u,v) \in {E_{ij}}$}
           \STATE{Calculate the mean similarity in the absence of $(u,v)$: $\text{rank}_{ij}(u,v) = \frac{\overline{\text{sim}_{ij}}|{E_{ij}}| - \text{sim}(u,v)}{|{E_{ij}}|-1}$}
       \ENDFOR
       \ENDIF
       \STATE{Calculate edge budget: $B(i,j)=\text{round}(N\cdot\frac{|C_i|\cdot|C_j|}{\sum_{(C_x,C_y) \in \mathcal{C}} |C_x|\cdot|C_y|}$)}
        \STATE{Find top $B(i,j)$ edges in the ranking: $E^{B(i,j)}_{ij} = \text{top}_{B(i,j)}(\text{rank}_{ij}(u,v))$}
       \STATE{Update graph edges: $\mathcal{E} = \mathcal{E} \cup E^{B(i,j)}_{ij}$ (\texttt{ADD}) or $\mathcal{E} = \mathcal{E} \setminus E^{B(i,j)}_{ij}$ (\texttt{DEL})}
       \ENDFOR
       
           \textbf{Output :} Modified graph $ \hat{\mathcal{G}} = (\mathcal{V},\hat{\mathcal{E}})$. Number of modifications is approximately $N$.
       \end{algorithmic}
\end{algorithm}





\section{Datasets and hyperparameters} \label{app:hyperparams}
\subsection{Dataset statistics}\label{app:datasetstats}
In \autoref{tab:datasetstats} we provide a summary of the datasets used for the experiments (\S\ref{s:experiments}). We also provide various metrics such as Edge Label Informativeness (ELI) and Adjusted Homophily score proposed in \citep{platonov2023characterizing}. The Normalized Mutual Information (NMI), Adjusted Mutual Information (AMI) and Modularity score after performing community detection using the Louvain method to understand how informative the graph structure is.  The adjustment in AMI is only necessary when comparing between sets of different size, but within a dataset the number of classes does not change. Therefore we can compare the effect of our algorithm by means of the NMI’s value. However, the AMI is useful to compare the alignment across different datasets.



\begin{table}[ht]
\centering
\caption{Dataset statistics.}
\label{tab:datasetstats}
\resizebox{\textwidth}{!}{%
\begin{tabular}{cccccccc}
\hline
Dataset        & \#Nodes & \#Edges & NMI    & AMI     & ELI     & Homophily & Modularity \\ \hline
Cora           & 2708    & 10138   & 0.4556 & 0.4489  & 0.5802  & 0.7637    & 0.8023     \\
Citeseer       & 3327    & 7358    & 0.3270 & 0.3151  & 0.4437  & 0.6620    & 0.8519     \\
Pubmed         & 19717   & 88648   & 0.1973 & 0.1966  & 0.4092  & 0.6860    & 0.7671     \\
Cornell        & 183     & 277     & 0.1250 & 0.0202  & 0.1556  & -0.2201   & 0.6227     \\
Texas          & 183     & 279     & 0.0673 & 0.0016  & 0.19234 & -0.2936   & 0.5548     \\
Wisconsin      & 251     & 450     & 0.0867 & 0.0351  & 0.1310  & -0.1732   & 0.6293     \\
Chameleon      & 890     & 8854    & 0.1035 & 0.0823  & 0.0138  & 0.0295    & 0.6680     \\
Squirrel       & 2223    & 57850   & 0.0176 & 0.0153  & 0.0013  & 0.0086    & 0.4451     \\
Actor          & 7600    & 26659   & 0.0044 & -0.0002 & 0.00017 & 0.00277   & 0.5113     \\
CS             & 18333   & 163788  & 0.5528 & 0.5501  & 0.6467  & 0.7845    & 0.7321     \\
Photo          & 7650    & 238162  & 0.6845 & 0.6835  & 0.6662  & 0.7850    & 0.7363     \\
Physics        & 34493   & 495924  & 0.4376 & 0.4372  & 0.7222  & 0.8724    & 0.6627     \\
Roman-Empire   & 22662   & 32927   & 0.0214 & 0.0030  & 0.1101  & -0.0468   & 0.9887     \\
Amazon-Ratings & 24492   & 93050   & 0.0426 & 0.0381  & 0.0398  & 0.1402    & 0.9645     \\
Minesweeper    & 10000   & 39402   & 0.0011 & 0.0004  & 0.0001  & 0.0094    & 0.8860     \\ \hline
\end{tabular}%
}
\end{table}









\subsection{Details of the experiments}
We use PyTorch Geometric \citep{Fey/Lenssen/2019} and Deep Graph Library (DGL) \citep{wang2019dgl} for all our experiments. For datasets Cora, Citeseer, Pubmed, Cornell, Texas, Wisconsin, Chameleon, Squirrel and Actor we use a 60/20/20 split for train/test/validation respectively. The hyperparameters are tuned on the validation set. Our backbone model is a 2-layered GCN \citep{Kipf:2017tc}. We report the final test accuracy averaged over 100 splits of the data. For datasets Roman-empire, Amazon-ratings and Minesweeper we use the code base of the authors \cite{platonov2023critical}, where the datasets are split 50/25/25 for train/test/validation respectively. Our backbone model here is a 5-layered GCN and the final test accuracy is reported averaged over 10 splits. We report the hyperparameters such as the Normalized Mutual Information (NMI) between the cluster labels and the ground truth labels after community detection \citep{Blondel_2008} before and after rewiring the graph to understand how it affects the community structure-node label alignment. We also report the number of edges added and deleted to effect the required change in test accuracy. The empirical runtimes, in seconds, are presented in seconds in Tables \ref{tab:hyperparamsonlysim},\ref{tab:hyperparamsonlysimboth},\ref{tab:hyperparamscomm},\ref{tab:hyperparamsinvcomm},\ref{tab:hyperparamscommboth},\ref{tab:hyperparamsinvcommboth}. Our code is available here:
\url{https://github.com/RelationalML/ComFy}.

\begin{table}[H]
\centering
\caption{Empirical runtimes for FeaSt based rewiring.}
\label{tab:hyperparamsonlysim}
\resizebox{12cm}{!}{%
\begin{tabular}{lrrrrrrr}
\toprule
Dataset & AvgTestAcc & NMIBefore & NMIAfter & EdgesAdded & EdgesDeleted & Rewire Time (s) \\
\midrule
Cora & 87.730±0.390 & 0.456 & 0.432 & 1000 & 0 & 9.333 \\
Cora & 90.740±0.390 & 0.456 & 0.432 & 0 & 500 & 9.637 \\
Citeseer & 78.540±0.340 & 0.327 & 0.338 & 1000 & 0 & 8.549 \\
Citeseer & 81.600±0.390 & 0.327 & 0.330 & 0 & 10 & 8.422 \\
Pubmed & 86.430±0.090 & 0.197 & 0.206 & 1000 & 0 & 89.224 \\
Pubmed & 86.760±0.100 & 0.197 & 0.196 & 0 & 50 & 89.745 \\
Cornell & 59.460±1.490 & 0.125 & 0.099 & 20 & 0 & 7.402 \\
Cornell & 51.350±1.400 & 0.125 & 0.114 & 0 & 5 & 7.525 \\
Texas & 54.050±1.510 & 0.067 & 0.063 & 5 & 0 & 7.630 \\
Texas & 64.860±1.430 & 0.067 & 0.190 & 0 & 100 & 7.568 \\
Wisconsin & 60.000±1.090 & 0.087 & 0.077 & 10 & 0 & 7.533 \\
Wisconsin & 60.000±1.270 & 0.087 & 0.134 & 0 & 50 & 7.538 \\
Chameleon & 43.260±0.620 & 0.103 & 0.103 & 20 & 0 & 9.513 \\
Chameleon & 42.700±0.690 & 0.103 & 0.103 & 0 & 20 & 9.093 \\
Squirrel & 35.510±0.440 & 0.018 & 0.018 & 50 & 0 & 14.566 \\
Squirrel & 36.400±0.360 & 0.018 & 0.018 & 0 & 100 & 13.219 \\
Actor & 31.250±0.220 & 0.004 & 0.005 & 100 & 0 & 79.587 \\
Actor & 31.970±0.210 & 0.004 & 0.006 & 0 & 100 & 78.594 \\
\bottomrule
\end{tabular}
}

\end{table}

\begin{table}[H]
\centering
\caption{Empirical runtimes for FeaSt+Add+Delete.}
\label{tab:hyperparamsonlysimboth}
\resizebox{12cm}{!}{%
\begin{tabular}{lrrrrrrr}
\toprule
Dataset & AvgTestAcc & NMIBefore & NMIAfter & EdgesAdded & EdgesDeleted & Rewire Time (s) \\
\midrule
Cora & 85.710±0.360 & 0.456 & 0.464 & 10 & 10 & 11.450 \\
Cora & 85.710±0.360 & 0.456 & 0.464 & 10 & 10 & 11.450 \\
Citeseer & 80.190±0.340 & 0.327 & 0.322 & 50 & 50 & 10.777 \\
Citeseer & 80.190±0.340 & 0.327 & 0.322 & 50 & 50 & 10.777 \\
Pubmed & 87.010±0.120 & 0.197 & 0.198 & 1000 & 1000 & 97.618 \\
Pubmed & 87.010±0.120 & 0.197 & 0.198 & 1000 & 1000 & 97.618 \\
Cornell & 54.050±1.620 & 0.125 & 0.115 & 5 & 5 & 8.774 \\
Cornell & 54.050±1.620 & 0.125 & 0.115 & 5 & 5 & 8.774 \\
Texas & 56.760±1.650 & 0.067 & 0.165 & 100 & 100 & 8.747 \\
Texas & 56.760±1.650 & 0.067 & 0.165 & 100 & 100 & 8.747 \\
Wisconsin & 58.000±1.260 & 0.087 & 0.120 & 50 & 50 & 8.079 \\
Wisconsin & 58.000±1.260 & 0.087 & 0.120 & 50 & 50 & 8.079 \\
Chameleon & 44.940±0.700 & 0.103 & 0.158 & 100 & 100 & 8.780 \\
Chameleon & 44.940±0.700 & 0.103 & 0.158 & 100 & 100 & 8.780 \\
Squirrel & 35.730±0.480 & 0.018 & 0.019 & 500 & 500 & 13.645 \\
Squirrel & 35.730±0.480 & 0.018 & 0.019 & 500 & 500 & 13.645 \\
Actor & 32.630±0.210 & 0.004 & 0.008 & 50 & 50 & 82.668 \\
Actor & 32.630±0.210 & 0.004 & 0.008 & 50 & 50 & 82.668 \\
\bottomrule
\end{tabular}
}
\end{table}



\begin{table}[H]
\centering
\caption{Empirical runtimes for ComFy}
\label{tab:runtimecomfy}
\resizebox{12cm}{!}{%
\begin{tabular}{@{}ccccccc@{}}
\toprule
Dataset   & AvgTestAcc & NMIBefore & NMIAfter & EdgesAdded & EdgesDeleted & Rewire Time (s) \\ \midrule
Cora      & 87.73±0.26 & 0.4556    & 0.4580   & 100        & 0            & 14.99      \\
Cora      & 88.13±0.27 & 0.4556    & 0.44876  & 0          & 2000         & 18.39      \\
Citeseer  & 77.36±0.38 & 0.32701   & 0.32492  & 100        & 0            & 11.75      \\
Citeseer  & 78.07±0.35 & 0.32701   & 0.35509  & 0          & 1000         & 11.86      \\
Pubmed    & 86.74±0.10 & 0.19726   & 0.19572  & 50         & 0            & 415.23     \\
Pubmed    & 86.23±0.11 & 0.19726   & 0.20603  & 0          & 2000         & 415.59     \\
Cornell   & 67.57±1.68 & 0.1249    & 0.0955   & 10         & 0            & 9.91       \\
Cornell   & 70.27±1.50 & 0.1249    & 0.1269   & 0          & 10           & 9.50       \\
Texas     & 62.16±1.52 & 0.0672    & 0.0678   & 10         & 0            & 9.74       \\
Texas     & 64.86±1.51 & 0.0672    & 0.0915   & 0          & 0            & 9.74       \\
Wisconsin & 62.00±1.12 & 0.0866    & 0.1526   & 50         & 0            & 9.79       \\
Wisconsin & 66.00±1.34 & 0.0866    & 0.1180   & 0          & 50           & 10.36      \\
Chameleon & 41.57±0.83 & 0.10349   & 0.14758  & 100        & 0            & 13.98      \\
Chameleon & 45.51±0.76 & 0.10349   & 0.10340  & 0          & 1500         & 10.94      \\
Squirrel  & 36.85±0.38 & 0.01762   & 0.01762  & 500        & 0            & 17.51      \\
Squirrel  & 39.10±0.43 & 0.01762   & 0.01762  & 0          & 1500         & 20.98      \\
Actor     & 32.30±0.25 & 0.00436   & 0.00491  & 500        & 0            & 143.81     \\
Actor     & 31.12±0.19 & 0.00436   & 0.01364  & 0          & 2000         & 141.79     \\ \bottomrule
\end{tabular}%
}
\end{table}








\begin{table}[H]
\centering
\caption{Empirical runtimes for HigherComMa based rewiring.}
\label{tab:hyperparamscomm}
\resizebox{12cm}{!}{%
\begin{tabular}{lrrrrrrr}
\toprule
Dataset & AvgTestAcc & NMI & EdgesAdded & EdgesDeleted & FinalGap & Rewire Time (s) \\
\midrule
Cora & 83.64±0.38 & 0.4531 & 100 & 0 & 0.004825 & 0.06 \\
Cora & 83.82±0.31 & 0.4565 & 0 & 127 & 0.003925 & 0.05 \\
Citeseer & 77.31±0.40 & 0.3252 & 10 & 0 & 0.001555 & 0.03 \\
Citeseer & 77.31±0.41 & 0.3273 & 0 & 10 & 0.001551 & 0.03 \\
Pubmed & 85.83±0.11 & 0.1933 & 50 & 0 & 0.014013 &  7.47\\
Pubmed & 85.90±0.11 & 0.1975 & 0 & 50 & 0.013990 & 8.67 \\
Cornell & 49.03±1.26 & 0.1283 & 5 & 0 & 0.079053 & 0.01 \\
Cornell & 49.93±1.34 & 0.1038 & 0 & 10 & 0.000001 & 0..01 \\
Texas & 52.66±1.47 & 0.0633 & 100 & 0 & 0.072213 & 0.01 \\
Texas & 48.57±1.53 & 0.0695 & 0 & 10 & 0.062387 & 0.01 \\
Wisconsin & 50.55±1.24 & 0.0886 & 5 & 0 & 0.074916 & 0.01 \\
Wisconsin & 50.32±1.38 & 0.0866 & 0 & 10 & 0.068169 & 0.01 \\
Chameleon & 41.23±0.72 & 0.1536 & 100 & 0 & 0.006417 & 0.04 \\
Chameleon & 40.44±0.69 & 0.0875 & 0 & 20 & 0.005920 & 0.04 \\
Squirrel & 34.51±0.40 & 0.0176 & 20 & 0 & 0.051575 & 0.63 \\
Squirrel & 34.66±0.39 & 0.0150 & 0 & 1000 & 0.050114 & 0.68 \\
Actor & 30.92±0.21 & 0.0080 & 20 & 0 & 0.032282 & 6.57 \\
Actor & 30.71±0.24 & 0.0222 & 0 & 50 & 0.032679 & 6.42 \\
\bottomrule
\end{tabular}
}
\end{table}

\begin{table}[H]
\centering
\caption{Empirical runtimes for HigherComMa+add+delete based rewiring.}
\label{tab:hyperparamscommboth}
\resizebox{12cm}{!}{%
\begin{tabular}{lrrrrrrr}
\toprule
Dataset & AvgTestAcc & NMIAfter & EdgesAdded & EdgesDeleted & FinalGap & Rewire Time (s) \\
\midrule
Cora & 83.820±0.340 & 0.463 & 50 & 100 & 0.004 & 0.06 \\
Citeseer & 77.320±0.380 & 0.332 & 10 & 50 & 0.000 & 0.03 \\

Pubmed & 85.830±0.110 & 0.195 & 100 & 50 & 0.014 & 7.14 \\

Cornell & 48.920±1.480 & 0.113 & 10 & 17 & 0.000 & 0.01 \\


Texas & 52.440±1.640 & 0.067 & 100 & 10 & 0.068 & 0.01 \\

Wisconsin & 51.350±1.400 & 0.091 & 20 & 10 & 0.069 & 0.01 \\
Chameleon & 41.220±0.750 & 0.094 & 500 & 100 & 0.004 & 0.04 \\

Squirrel & 34.700±0.400 & 0.016 & 20 & 500 & 0.051 & 0.82 \\

Actor & 30.810±0.190 & 0.025 & 100 & 50 & 0.032 & 6.88 \\
\bottomrule
\end{tabular}
}
\end{table}





\begin{table}[H]
\centering
\caption{Empirical runtimes for LowerComMa based rewiring.}
\label{tab:hyperparamsinvcomm}
\resizebox{12cm}{!}{%
\begin{tabular}{lrrrrrrr}
\toprule
Dataset & AvgTestAcc & NMIAfter & EdgesAdded & EdgesDeleted & FinalGap & Rewire Time (s) \\
\midrule
Cora & 83.41±0.37 & 0.4604 & 10 & 0 & 0.008448 & 0.693458 \\
Cora & 83.61±0.35 & 0.4583 & 0 & 2 & 0.004784 & 0.663406 \\
Citeseer & 77.15±0.36 & 0.3195 & 10 & 0 & 0.001556 & 0.686848 \\
Citeseer & 77.39±0.37 & 0.3240 & 0 & 4 & 0.001555 & 0.697997 \\
Pubmed & 85.85±0.090 & 0.1899 & 50 & 0 & 0.014267 & 13.045066 \\
Pubmed & 85.90±0.10 & 0.1844 & 0 & 7 & 0.014069 & 11.933355 \\
Cornell & 51.08±1.67 & 0.0695 & 100 & 0 & 0.131261 & 7.787908 \\
Cornell & 49.69±1.43 & 0.1249 & 0 & 1 & 0.080970 & 7.696815 \\
Texas & 50.29±1.71 & 0.0516 & 100 & 0 & 0.201174 & 8.309750 \\
Wisconsin & 50.95±1.29 & 0.1094 & 20 & 0 & 0.089029 & 8.400457 \\
Wisconsin & 50.61±1.35 & 0.0886 & 0 & 4 & 0.076910 & 8.699127 \\
Chameleon & 40.61±0.64 & 0.0791 & 50 & 0 & 0.007699 & 8.595781 \\
Chameleon & 40.43±0.71 & 0.1034 & 0 & 17 & 0.006315 & 8.385635 \\
Squirrel & 34.48±0.39 & 0.0207 & 100 & 0 & 0.056416 & 10.908470 \\
Squirrel & 34.76±0.40 & 0.0166 & 0 & 12 & 0.051370 & 9.688496 \\
Actor & 30.79±0.23 & 0.0055 & 500 & 0 & 0.070495 & 17.127213 \\
Actor & 30.79±0.22 & 0.0077 & 0 & 2 & 0.032679 & 15.543629 \\
\bottomrule
\end{tabular}
}
\end{table}



\begin{table}[H]
\centering
\caption{Empirical runtimes for LowerComMa+add+delete based rewiring.}
\label{tab:hyperparamsinvcommboth}
\resizebox{12cm}{!}{%
\begin{tabular}{lrrrrrrr}
\toprule
Dataset & AvgTestAcc & NMIAfter & EdgesAdded & EdgesDeleted & FinalGap & Rewire Time (s) \\
\midrule
Cora & 83.73±0.32 & 0.4466 & 10 & 2 & 0.007464 & 0.660313 \\

Citeseer & 77.42±0.38 & 0.3197 & 10 & 4 & 0.001556 & 0.427487 \\

Pubmed & 85.87±0.10 & 0.2036 & 50 & 7 & 0.014268 & 13.815228 \\

Cornell & 52.36±1.52 & 0.0690 & 100 & 1 & 0.132826 & 8.077217 \\

Texas & 51.6±1.53 & 0.0516 & 100 & 0 & 0.201174 & 7.848707 \\

Wisconsin & 51.45±1.33 & 0.1096 & 5 & 4 & 0.078300 & 7.622503 \\

Chameleon & 40.9±0.66 & 0.0995 & 20 & 17 & 0.007556 & 7.997294 \\

Squirrel & 34.75±0.41 & 0.0187 & 100 & 12 & 0.056383 & 10.509906 \\

Actor & 30.85±0.23 & 0.0034 & 20 & 2 & 0.036560 & 15.892848 \\
\bottomrule
\end{tabular}
}
\end{table}



\begin{table}[ht]
\centering
\caption{Different community detection metrics for various datasets after applying FeaSt.}
\label{tab:commdetectmetricsfeastacc}
\resizebox{\textwidth}{!}{%
\begin{tabular}{cccccccccccc}
\hline
\multicolumn{12}{c}{FeaSt-Add}                                                                                        \\ \hline
Dataset &
  NMIBefore &
  NMIAfter &
  AMIBefore &
  AMIAfter &
  ModularityBefore &
  ModularityAfter &
  ELIBefore &
  ELIAfter &
  HomBefore &
  HomAfter &
  \begin{tabular}[c]{@{}c@{}}Test\\ Acc\end{tabular} \\ \hline
Cora      & 0.4556 & 0.4726 & 0.4489 & 0.4656 & 0.8023 & 0.8021 & 0.5802   & 0.5822  & 0.7637  & 0.7659  & 89.74±0.26 \\
Citeseer  & 0.3270 & 0.3384 & 0.3151 & 0.3249 & 0.8519 & 0.8401 & 0.4437   & 0.4633  & 0.6620  & 0.67998 & 79.48±0.40 \\
Chameleon & 0.1035 & 0.1035 & 0.0823 & 0.0823 & 0.6680 & 0.6680 & 0.0138   & 0.0138  & 0.0295  & 0.0295  & 44.94±0.78 \\
Squirrel  & 0.0176 & 0.0167 & 0.0153 & 0.0143 & 0.4451 & 0.4451 & 0.001325 & 0.00133 & 0.00861 & 0.00869 & 35.73±0.43 \\ \hline
\multicolumn{12}{c}{FeaSt-Del}                                                                                        \\ \hline
Cora      & 0.4556 & 0.4497 & 0.4489 & 0.4379 & 0.8023 & 0.8039 & 0.5802   & 0.5816  & 0.7637  & 0.7645  & 87.32±0.30 \\
Citeseer  & 0.3270 & 0.3400 & 0.3151 & 0.3212 & 0.8519 & 0.8558 & 0.4437   & 0.4523  & 0.6620  & 0.6694  & 78.38±1.46 \\
Chameleon & 0.1035 & 0.1047 & 0.0823 & 0.0809 & 0.6680 & 0.6655 & 0.0138   & 0.0137  & 0.0295  & 0.0297  & 47.19±0.62 \\
Squirrel  & 0.0176 & 0.0184 & 0.0153 & 0.0151 & 0.4451 & 0.4453 & 0.001325 & 0.00133 & 0.00861 & 0.00865 & 37.75±0.39 \\ \hline
\end{tabular}%
}
\end{table}



\begin{table}[ht]
\centering
\caption{Different community detection metrics for various datasets after applying ComFy.}
\label{tab:commdetectmetricscomfyacc}
\resizebox{\textwidth}{!}{%
\begin{tabular}{cccccccccccc}
\hline
\multicolumn{12}{c}{ComFy-Add}                                                                                    \\ \hline
Dataset &
  NMIBefore &
  NMIAfter &
  AMIBefore &
  AMIAfter &
  ModularityBefore &
  ModularityAfter &
  ELIBefore &
  ELIAfter &
  HomBefore &
  HomAfter &
  \begin{tabular}[c]{@{}c@{}}Test\\ Acc\end{tabular} \\ \hline
Cora      & 0.4556 & 0.4556 & 0.4489 & 0.4489 & 0.8023 & 0.8032 & 0.5802 & 0.5776  & 0.7637 & 0.7620 & 89.13±0.26 \\
Citeseer  & 0.3270 & 0.3297 & 0.3151 & 0.3175 & 0.8519 & 0.8501 & 0.4437 & 0.44033 & 0.6620 & 0.6602 & 80.42±0.39 \\
Chameleon & 0.1035 & 0.0842 & 0.0823 & 0.0706 & 0.6680 & 0.6687 & 0.0138 & 0.0140  & 0.0295 & 0.0307 & 47.19±0.62 \\
Squirrel  & 0.0176 & 0.0176 & 0.0153 & 0.0153 & 0.4451 & 0.4451 & 0.0013 & 0.0013  & 0.0086 & 0.0086 & 37.75±0.39 \\ \hline
\multicolumn{12}{c}{ComFy-Del}                                                                                    \\ \hline
Cora      & 0.4556 & 0.4499 & 0.4489 & 0.4382 & 0.8023 & 0.8021 & 0.5802 & 0.5806  & 0.7637 & 0.7634 & 88.33±0.31 \\
Citeseer  & 0.3270 & 0.3263 & 0.3151 & 0.3133 & 0.8519 & 0.8678 & 0.4437 & 0.4461  & 0.6620 & 0.6650 & 81.37±0.36 \\
Chameleon & 0.1035 & 0.1044 & 0.0823 & 0.0705 & 0.6680 & 0.6649 & 0.0138 & 0.0139  & 0.0295 & 0.0298 & 45.51±0.64 \\
Squirrel  & 0.0176 & 0.0176 & 0.0153 & 0.0153 & 0.4451 & 0.4451 & 0.0013 & 0.0013  & 0.0086 & 0.0086 & 37.75±0.42 \\ \hline
\end{tabular}%
}
\end{table}




\subsection{Sensitivity to hyperparameters}\label{app:hyperparameters}
The hyperparameters used in the experiments are given in \autoref{tab:gcnhyperparams}. For the large heterophilic datasets Roman-empire, Amazon-ratings and Minesweeper we use the model hyperparameters recommended by the authors \citep{platonov2023characterizing}. However, we found setting the learning rate to $3e^{-3}$ instead of $3e^{-5}$ yields better results. The GCN hyperparameters on the other datasets are tuned based on the validation set through a grid search. As is common, we found that the performance of not only our method but GNNs in general is sensitive to the learning rate but otherwise robust across datasets and architectures. Our rewiring techniques do not require any change of the basic GNN hyperparameters. In fact, we use the same ones as the baselines. Our rewiring methods come with an additional hyperparameter, i.e., the rewiring budget (and thus how many edges are added or deleted). This is true for all the rewiring methods that have been proposed \citep{topping2022understanding,Fosr,sjlr,borf,effectiveresistance,jamadandi2024spectral} in the literature. In \autoref{tab:edgebudgets} and \autoref{tab:edgebudgetscomfy}  we present results for 4 datasets Cora, Citeseer, Chameleon and Squirrel and how their performance varies with respect to edge modification budgets. While the performance is clearly sensitive to this choice, the rewiring budget seems to be task specific but not very architecture specific, as we could use the same budgets for different GNN variants, such as GIN and GraphSAGE.




\begin{table}[ht]
\centering
\caption{GCN hyperparameters used in the experiments.}
\label{tab:gcnhyperparams}
\resizebox{7cm}{!}{%
\begin{tabular}{@{}cccc@{}}
\toprule
Dataset        & LR            & Dropout       & HiddenDimension \\ \midrule
Cora           & 0.01          & 0.41          & 128             \\
Citeseer       & 0.01          & 0.31          & 32              \\
Pubmed         & {0.01} & {0.41} & {32}     \\
Cornell        & 0.001         & 0.51          & {128}    \\
Texas          & 0.001         & {0.51} & 128             \\
Wisconsin      & 0.001         & 0.51          & 128             \\
Chameleon      & 0.001         & 0.21          & 128             \\
Squirrel       & 0.001         & 0.51          & 128             \\
Actor          & 0.001         & 0.51          & 128             \\
CS             & 0.001         & 0.51          & 512             \\
Photo          & 0.01          & 0.51          & 512             \\
Physics        & 0.01          & 0.51          & 512             \\
Roman-empire   & 0.003         & 0.31          & 512             \\
Amazon-ratings & 0.003         & 0.31          & 512             \\
Minesweeper    & 0.003         & 0.31          & 512             \\ \bottomrule
\end{tabular}%
}
\end{table}



\begin{table}[ht]
\centering
\caption{GCN test accuracy variability for different edge budgets for FeaSt.}
\label{tab:edgebudgets}
\resizebox{7cm}{!}{%
\begin{tabular}{@{}ccccc@{}}
\toprule
\multicolumn{5}{c}{GCN+FeaSt} \\ \midrule
{ Dataset} &   { EdgesAdded} &   { Accuracy} &   { EdgesDeleted} &   { Accuracy} \\ \midrule
{ } &   {10} &   { 85.11±0.37} &   {10} &   {82.49±0.39} \\
{} &   { 50} &   { {79.88±0.41}} &   { {50}} &   { 83.70±0.34} \\
{} &   {100} &   { 86.72±0.36} &   {{100}} &   {86.92±0.34} \\
{} &   { 500} &   { {83.90±0.35}} &   { {500}} &   {\textbf{90.74±0.39}} \\
\multirow{-5}{*}{{ Cora}} &   { 1000} &   { \textbf{87.73±0.39}} &   { 1000} &   { 85.51±0.31} \\ \midrule
{ } &   { 10} &   {77.36±0.35} &   { 10} &   {\textbf{81.60±0.39}} \\
{ } &   { 50} &   { 77.59±0.37} &   { {50}} &   { 74.06±0.36} \\
{} &   { 100} &   {75.24±0.41} &   { 100} &   {78.30±0.33} \\
{} &   {500} &   {75.94±0.35} &   {500} &   {75.71±0.39} \\
\multirow{-5}{*}{{\color[HTML]{000000} Citeseer}} &   { {1000}} &   {\textbf{78.54±0.34}} &   { {1000}} &   {75.00±0.33} \\ \midrule
{} &   {{20}} &   {\textbf{43.26±0.62}} &   {{20}} &   {\textbf{42.70±0.69}} \\
 &   { 50} &   {38.20±0.71} &   { 50} &   {41.01±0.68} \\
 &   100 &   41.01±0.64 &   100 &   35.96±0.68 \\
{ } &   500 &   37.08±0.64 &   500 &   40.45±0.63 \\
\multirow{-5}{*}{{\color[HTML]{000000} Chameleon}} &   1000 &   40.45±0.62 &   1000 &   39.33±0.73 \\ \midrule
 &   20 &   33.26±0.38 &   20 &   34.38±0.40 \\
 &   {50} &   \textbf{35.51±0.44} &   50 &   35.28±0.38 \\
 &   100 &   33.48±0.44 &   {100} &   \textbf{36.40±0.36} \\
 &   500 &   33.26±0.37 &   500 &   33.71±0.39 \\
\multirow{-5}{*}{Squirrel} &   1000 &   33.26±0.38 &   1000 &   32.36±0.38 \\ \bottomrule
\end{tabular}%
}
\end{table}




\begin{table}[ht]
\centering
\caption{GCN test accuracy variability for different edge budgets for ComFy}
\label{tab:edgebudgetscomfy}
\resizebox{7cm}{!}{%
\begin{tabular}{@{}ccccc@{}}
\toprule
\multicolumn{5}{c}{GCN+ComFy} \\ \midrule
{\color[HTML]{000000} Dataset} &
  {\color[HTML]{000000} EdgesAdded} &
  {\color[HTML]{000000} Accuracy} &
  {\color[HTML]{000000} EdgesDeleted} &
  {\color[HTML]{000000} Accuracy} \\ \midrule
{\color[HTML]{000000} } &
  {\color[HTML]{000000} 50} &
  {\color[HTML]{000000} 86.72±0.27} &
  {\color[HTML]{000000} 500} &
  {\color[HTML]{000000} 86.52±0.27} \\
{\color[HTML]{000000} } &
  {\color[HTML]{000000} {100}} &
  {\color[HTML]{000000} {87.73±0.26}} &
  {\color[HTML]{000000} {1000}} &
  {\color[HTML]{000000} 84.31±0.27} \\
{\color[HTML]{000000} } &
  {\color[HTML]{000000} 500} &
  {\color[HTML]{000000} 86.12±0.32} &
  {\color[HTML]{000000} {1500}} &
  {\color[HTML]{000000} 85.71±0.31} \\
\multirow{-4}{*}{{\color[HTML]{000000} Cora}} &
  {\color[HTML]{000000} 1000} &
  {\color[HTML]{000000} \textbf{85.11±0.27}} &
  {\color[HTML]{000000} {2000}} &
  {\color[HTML]{000000} \textbf{88.13±0.27}} \\ \midrule
{\color[HTML]{000000} } &
  {\color[HTML]{000000} 50} &
  {\color[HTML]{000000} 77.36±0.38} &
  {\color[HTML]{000000} {500}} &
  {\color[HTML]{000000} 75.24±0.38} \\
{ } &
  { {100}} &
  { \textbf{77.36±0.38}} &
  { {1000}} &
  { \textbf{78.07±0.35}} \\
{ } &
  { 500} &
  { 75.47±0.33} &
  { 1500} &
  { 76.42±0.36} \\
\multirow{-4}{*}{{ Citeseer}} &
  { 1000} &
  { {75.71±0.39}} &
  { {2000}} &
  { 74.76±0.39} \\ \midrule
{ } &
  { {5}} &
  { {35.39±0.72}} &
  { {100}} &
  { {41.57±0.73}} \\
{ } &
  { 10} &
  { 38.20±0.73} &
  { 500} &
  { 37.08±0.69} \\
{ } &
  50 &
  41.01±0.64 &
  1000 &
  44.38±0.69 \\
{ } &
  {100} &
  \textbf{41.57±0.83} &
  {1500} &
  \textbf{45.51±0.76} \\
\multirow{-5}{*}{{\color[HTML]{000000} Chameleon}} &
  500 &
  39.33±0.60 &
  2000 &
  42.13±0.74 \\ \midrule
 &
  {5} &
  \textbf{36.85±0.38} &
  100 &
  35.51±0.41 \\
 &
  {10} &
  {30.34±0.44} &
  500 &
  33.71±0.40 \\
 &
  50 &
  34.16±0.41 &
  {1000} &
  {37.08±0.41} \\
 &
  100 &
  32.81±0.37 &
  {1500} &
  \textbf{39.10±0.43} \\
\multirow{-5}{*}{Squirrel} &
  500 &
  34.61±0.42 &
  2000 &
  36.85±0.39 \\ \bottomrule
\end{tabular}%
}
\end{table}

\section{Additional Experiments}

\subsection{Comparison with various baselines}\label{app:diversebaselines}
We compare our proposed algorithms with other diverse methods \citep{mmgnn,diffwire,esnrgraph} in \autoref{tab:diversebaselines}. In \citep{mmgnn} the authors suggest to use multi-order moments to model a neighbor’s feature distribution and propose MM-GNN to use a multi-order moment embedding and an attention mechanism to weight importance of certain nodes going beyond single statistic aggregation mechanisms such as mean, max and sum. In \citep{diffwire}, the authors propose DiffWire, an inductive way to rewire the graph based on the Lov\'{a}sz bound by formulating two new layers that are interspersed between regular GNN layers. In \citep{esnrgraph} the authors propose a way to de-noise the graph by proposing graph propensity score (GPS) and GPS-PE (with positional encoding) methods to rewire the graph. Although the authors call their method ``graph rewiring", the proposal involves separating edges in the graph as training edges and message-passing edges and use a self-supervised link prediction task to impute edges between nodes. Note that these methods go beyond `rewiring-as-a-pre-processing' paradigm, which is the case for all our proposed algorithms. We report the results reported in their respective papers, and hence NA for some datasets. Not all code is made available for reproducing the results. We also report our proposed algorithms with different variant of GNNs such as GIN \citep{xu2018how} and GraphSAGE \citep{Hamilton:2017tp} to emphasize on the fact that our rewiring algorithms can be combined with any GNN model. The top performance is highlighted in bold. From the table we can clearly see that our proposed algorithms outperform the chosen diverse baselines on 6 out of 9 datasets.




\begin{table}[ht]
\caption{Additional baselines with diverse methods.}
\label{tab:diversebaselines}
\resizebox{\textwidth}{!}{%
\begin{tabular}{@{}cccccccccc@{}}
\toprule
Method       & Cora       & Citeseer   & Pubmed     & Cornell    & Texas      & Wisconsin          & Chameleon             & Squirrel              & Actor      \\ \midrule
MM-GNN       & 84.21±0.56 & 73.03±0.58 & 80.26±0.69 & NA         & NA         & NA                 & \textbf{63.32 ± 1.31} & \textbf{51.38 ± 1.73} & NA         \\
GCN+DiffWire & 83.66±0.60 & 72.26±0.50 & 86.07±0.10 & 69.04±2.2  & NA         & {79.05±2.1} & NA                    & NA                    & 31.98±0.30 \\
GPS &  79.5±0.80 &   71.5±0.60 &   77.7±0.30 &   {74.6±3.00} &   {80.0±1.80} &   77.3±4.40 &   41.5±3.60 &   43.0±0.90 &   \textbf{38.3±0.70} \\
GPS-PE       & 80.5±0.80  & 71.5±0.40  & 77.7±0.50  & 68.6±4.70  & 75.1±4.30  & 78.8±1.50          & 37.6±1.60             & 34.9±1.30             & 36.3±0.80  \\
GCN+FeaStAdd & 87.73±0.39 & 78.54±0.34 & 86.43±0.09 & 59.46±1.49 & 54.05±1.51 & 60.00±1.09         & 43.26±0.62            & 39.33±0.73            & 31.25±0.22 \\
GCN+FeaStDel &   \textbf{90.74±0.39} &   \textbf{81.60±0.39} &   {86.76±0.10} &   51.35±1.63 &   64.86±1.43 &   60.00±1.27 &   42.70±0.69 &   36.40±0.36 &   31.97±0.21 \\
GCN+ComFyAdd & 87.73±0.26 & 77.36±0.38 & 86.74±0.10 & 67.57±1.68 & 62.16±1.52 & 62.00±1.12         & 41.57±0.83            & 36.85±0.38            & 32.30±0.25 \\
GCN+ComFyDel & 88.13±0.27 & 78.07±0.35 & 86.23±0.11 & 70.27±1.50 & 64.86±1.51 & 66.00±1.34         & 45.51±0.76            & 39.10±0.43            & 31.12±0.19 \\
GIN+FeaStAdd       & 87.12±0.34 & 75.71±0.41 & 88.36±0.11 & 51.35±1.62 & 70.27±1.48 & 62.00±1.40 & 42.70±0.64 & 38.20±0.48 & 28.62±0.23 \\
GIN+FeaStDel       & 85.31±0.34 & 73.35±0.48 & \textbf{89.83±0.12} & 59.46±1.73 & 72.97±1.34 & 70.00±1.31 & 45.51±0.60 & 40.67±0.43 & 29.21±0.23 \\
GIN+ComFyAdd       & 84.10±0.28 & 75.00±0.46 & 89.75±0.14 & 62.16±1.99 & 67.57±1.48 & 68.00±1.32 & 46.07±0.72 & 38.43±0.47 & 29.74±0.21 \\
GIN+ComFyDel       & 85.71±0.37 & 74.29±0.39 & 88.46±0.11 & 56.76±1.60 & 67.57±1.50 & 66.00±1.42 & 51.12±0.73 & 40.67±0.54 & 30.33±0.22 \\  
GraphSAGE+FeaStAdd & 89.74±0.26 & 79.48±0.40 & 86.84±0.11 & 81.08±1.46 & 75.68±1.52 & 80.00±1.04 & 44.94±0.78 & 35.73±0.43 & 37.37±0.22 \\
GraphSAGE+FeaStDel & 87.32±0.30 & 80.42±0.39 & 87.62±0.10 & 78.38±1.46 & 81.08±1.43 & 86.00±1.07 & 47.19±0.62 & 37.75±0.39 & 37.76±0.21 \\
GraphSAGE+ComFyAdd & 89.13±0.26 & 81.37±0.36 & 88.33±0.09 & \textbf{89.19±1.37} & 81.08±1.52 & \textbf{86.00±1.06} & 43.82±0.72 & 37.30±0.41 & 35.86±0.22 \\
GraphSAGE+ComFyDel & 88.33±0.31 & 81.60±0.37 & 88.03±0.11 & 78.38±1.41 & \textbf{83.78±1.47} & 78.00±1.13 & 45.51±0.64 & 37.75±0.42 & 36.45±0.22 \\ 
\bottomrule
\end{tabular}%
}
\end{table}


\subsection{Scalability}\label{app:scale}
We present additional results for large homophilic graphs to understand how our proposed algorithms scale with increasing graph size. The statistics for the datasets used is presented in \autoref{tab:datasetstats}. We present results on CS, Physics, and Photo \citep{shchur2019pitfalls} available as PyTorch geometric datasets. We train a two-layered GCN with the following hyperparameters, the learning rate = $\{0.001, 0.01\}$ and hidden dimension size = 512. The results are presented in \autoref{tab:largehomo}. Further, we pick the largest dataset among these which is Physics with $34,493$ nodes and $495,924$ edges and run a version of our algorithms which samples the nodes randomly and calculates the feature similarity and rewires only on the subset of those nodes. We use sampling ratio 0.2 to represent 20\% of the nodes. The results are presented in \autoref{tab:physsampling}. Clearly from the table, we can see that our proposed algorithms are robust, in that, we can bring down the runtime significantly and still obtain comparable accuracy to the full graph.

\begin{table}[ht]
\centering
\caption{Node classification results on large homophilic graphs.}
\label{tab:largehomo}
\resizebox{10cm}{!}{%
\begin{tabular}{@{}ccccc@{}}
\toprule
Dataset                  & Method      & Edges Modified  & Rewire Time (s)      & Accuracy            \\ \midrule
\multirow{5}{*}{CS}      & GCNBaseline & NA             & NA              & 91.76±0.08          \\
                         & FeaStAdd    & 500            & 52.20           & 92.10±0.08          \\
                         & FeaStDel    & {10000} & {53.52}  & \textbf{92.71±0.06} \\
                         & ComFyAdd    & 100            & 318.58          & {91.98±0.06} \\
                         & ComFyDel    & 500            & {331.71} & 92.30±0.08          \\ \midrule
\multirow{5}{*}{Physics} & GCNBaseline & NA             & NA              & 94.55±0.04          \\
                         & FeaStAdd    & 100            & 190.24          & 94.85±0.05          \\
                         & FeaStDel    & 500            & 192.07          & 95.01±0.05          \\
                         & ComFyAdd    & 100            & 1282.06         & \textbf{95.04±0.05} \\
                         & ComFyDel    & 500            & 1300.39         & 94.69±0.05          \\ \midrule
\multirow{5}{*}{Photo}   & GCNBaseline & NA             & NA              & 78.70±0.41          \\
                         & FeaStAdd    & 100            & 42.63           & 79.10±0.47          \\
                         & FeaStDel    & 10000          & 40.34           & 81.10±0.51          \\
                         & ComFyAdd    & 100            & 82.49           & 77.30±0.60          \\
                         & ComFyDel    & 1000           & 80.94           & \textbf{81.60±0.49}          \\ \bottomrule
\end{tabular}%
}
\end{table}





\begin{table}[ht]
\centering
\caption{Node classification results on Physics dataset with node sampling.}
\label{tab:physsampling}
\resizebox{7cm}{!}{%
\begin{tabular}{@{}cccc@{}}
\toprule
Sampling ratio       & Method   & Rewire Time (s)       & Accuracy            \\ \midrule
\multirow{4}{*}{100} & FeaStAdd & 190.24           & 94.85±0.05          \\
                     & FeaStDel & {192.07}  & {95.01±0.05} \\
                     & ComFyAdd & 1282.06          & \textbf{95.04±0.05} \\
                     & ComFyDel & {1300.39} & 94.69±0.05          \\ \midrule
\multirow{4}{*}{20}  & FeaStAdd & 32.60            & 94.60±0.05          \\
                     & FeaStDel & 33.03            & 94.86±0.04          \\
                     & ComFyAdd &  718.40                &  94.62±0.05                   \\
                     & ComFyDel & 713.65                 &  94.53±0.05                   \\ \bottomrule
\end{tabular}%
}
\end{table}




\subsection{Results with different GNN variants}\label{app:ginsage}
In \autoref{tab:ginsage} we present results for GIN \citep{xu2018how} and GraphSAGE \citep{grapsage} variants, demonstrating that our rewiring schemes are architecture agnostic and can be used as a pre-processing step to make the input graphs amenable to message-passing. We also add MLP as a baseline.


\begin{table}[ht]
\centering
\caption{Accuracy on node classification with GIN and GraphSAGE.}
\label{tab:ginsage}
\resizebox{\textwidth}{!}{%
\begin{tabular}{@{}cccccccccc@{}}
\toprule
Method             & Cora       & Citeseer   & Pubmed     & Cornell    & Texas      & Wisconsin  & Chameleon  & Squirrel   & Actor      \\ \midrule
MLP                & 73.02±0.39 & 70.84±0.51 & 87.68±0.10 &  73.54±1.45          & 76.22±1.45 & 81.68±1.06 & 35.70±0.69 & 31.84±0.40 &36.05±0.23 \\\midrule
GIN                & 85.51±0.29 & 74.53±0.41 & 88.33±0.12 &  37.84±1.62          & 54.05±1.61 & 56.00±1.21 & 41.57±0.64 & 37.08±0.39 & 24.21±0.22 \\
GIN+FeaStAdd       & 87.12±0.34 & 75.71±0.41 & 88.36±0.11 & 51.35±1.62 & 70.27±1.48 & 62.00±1.40 & 42.70±0.64 & 38.20±0.48 & 28.62±0.23 \\
GIN+FeaStDel       & 85.31±0.34 & 73.35±0.48 & \textbf{89.83±0.12} & 59.46±1.73 & 72.97±1.34 & 70.00±1.31 & 45.51±0.60 & 40.67±0.43 & 29.21±0.23 \\
GIN+ComFyAdd       & 84.10±0.28 & 75.00±0.46 & 89.75±0.14 & 62.16±1.99 & 67.57±1.48 & 68.00±1.32 & 46.07±0.72 & 38.43±0.47 & 29.74±0.21 \\
GIN+ComFyDel       & 85.71±0.37 & 74.29±0.39 & 88.46±0.11 & 56.76±1.60 & 67.57±1.50 & 66.00±1.42 & \textbf{51.12±0.73} & \textbf{40.67±0.54} & 30.33±0.22 \\ \midrule
GraphSAGE          & 87.73±0.26 & 77.12±0.31 & 86.56±0.10 & 67.57±1.36 & 78.38±1.37 & 76.00±1.18 & 38.76±0.61 & 35.96±0.38 & 35.99±0.21 \\
GraphSAGE+FeaStAdd & \textbf{89.74±0.26} & 79.48±0.40 & 86.84±0.11 & 81.08±1.46 & 75.68±1.52 & 80.00±1.04 & 44.94±0.78 & 35.73±0.43 & 37.37±0.22 \\
GraphSAGE+FeaStDel & 87.32±0.30 & 80.42±0.39 & 87.62±0.10 & 78.38±1.46 & \textbf{81.08±1.43} & \textbf{86.00±1.07} & 47.19±0.62 & 37.75±0.39 & \textbf{37.76±0.21} \\
GraphSAGE+ComFyAdd & 89.13±0.26 & 81.37±0.36 & 88.33±0.09 & \textbf{89.19±1.37} & \textbf{81.08±1.52} & \textbf{86.00±1.06} & 43.82±0.72 & 37.30±0.41 & 35.86±0.22 \\
GraphSAGE+ComFyDel & 88.33±0.31 & \textbf{81.60±0.37} & 88.03±0.11 & 78.38±1.41 & 83.78±1.47 & 78.00±1.13 & 45.51±0.64 & 37.75±0.42 & 36.45±0.22 \\ \bottomrule
\end{tabular}%
}
\end{table}

\subsection{Our algorithms against Feature Noise}\label{app:featnoise}
To understand how our proposed algorithm perform in presence of feature noise, we artificially add Gaussian noise with $0$ mean and standard deviation  $\{0.01,0.03, 0.05, 0.08, 0.1,0.2\}$ controlling the level of noise. We compare our proposed algorithms FeaSt and ComFy against the baseline GCN for increasing feature noise. We add/delete 10 edges. This is shown in \autoref{fig:featnoise} for datasets Chameleon and Squirrel. Evidently, our proposed algorithms are robust to noise perturbations and consistently outperform the baseline by a large margin.


\begin{figure}[ht]
   \centering
   \hspace*{0pt}\hfill
       \subfigure[FeaSt+Chameleon+FeatureNoise.]%
       {\includegraphics[width=0.3\linewidth]{img/chameleonfeatnoisefeast.pdf}\label{fig:chameleonfeastfeatnoise}} 
   \hfill
   \subfigure[FeaSt+Squirrel+FeatureNoise.]%
       {\includegraphics[width=0.3\linewidth]{img/squirrelfeatnoisefeast.pdf}\label{fig:squirrelfeatnoisefeast}}
   \hfill\hspace*{0pt}
   \\
   \hspace*{0pt}\hfill
       \subfigure[ComFy+Chameleon+FeatureNoise.]%
       {\includegraphics[width=0.3\linewidth]{img/chameleon-comfynoise.pdf}\label{fig:chameleoncomfyfeatnoise}}
   \hfill
       \subfigure[ComFy+Squirrel+FeatureNoise.]%
       {\includegraphics[width=0.3\linewidth]{img/squirrelcomfy-noise.pdf}\label{fig:squirrelcomfyfeatnoise}} 
   \hfill\hspace*{0pt}\\ 
   \caption{We analyse the behaviour of GCNs and our rewiring methods FeaSt and ComFy in presence of feature noise.}
   \label{fig:featnoise}
\end{figure}

\subsection{Our algorithms against Label Noise} \label{app:labelnoise}
To understand how our algorithms perform in presence of label noise, we randomly flip a certain percentage of labels in the training node before rewiring the graph. We flip $\{0,3,5,10,20,50\}$ percent of the labels and compare the baseline GCN and our methods FeaSt and ComFy with 10 edge additions/deletions. We plot the results in \autoref{fig:labelnoise}, for increasing label noise, we can see that our methods are as robust as the baseline, because they lose performance at the same rate.


\begin{figure}[h]
   \centering
   \hspace*{0pt}\hfill
       \subfigure[FeaSt+Chameleon+LabelNoise.]%
       {\includegraphics[width=0.3\linewidth]{img/chameleonfeastlabelnoise.pdf}\label{fig:chameleonfeastlabelnoise}} 
   \hfill
   \subfigure[FeaSt+Squirrel+LabelNoise.]%
       {\includegraphics[width=0.3\linewidth]{img/squirrelfeastlabelnoise.pdf}\label{fig:squirrelfeastlabelnoise}}
   \hfill\hspace*{0pt}
   \\
   \hspace*{0pt}\hfill
       \subfigure[ComFy+Chameleon+LabelNoise.]%
       {\includegraphics[width=0.3\linewidth]{img/comfychameleonlabelnoise.pdf}\label{fig:chameleoncomfylabelnoise}}
   \hfill
       \subfigure[ComFy+Squirrel+LabelNoise.]%
       {\includegraphics[width=0.3\linewidth]{img/squirrelcomfylabelnoise.pdf}\label{fig:squirrelcomfylabelnoise}} 
   \hfill\hspace*{0pt}\\ 
   \caption{We analyse the behaviour of GCNs and our rewiring methods FeaSt and ComFy in presence of label noise.}
   \label{fig:labelnoise}
\end{figure}




\subsection{Our algorithms on SBMs with lower community structure} \label{app:comfyonsbm}
Using the same setups as in the proofs for Theorems \ref{th:sbmperfproof} and \ref{th:sbmnoiseproof}, we evaluate the performance of ComMa (\autoref{fig:accsedgescomma}) and ComFy (\autoref{fig:accsedges}) on SBM graphs with varying levels of community strength, under edge additions or deletions of ${0,50,200,500}$. Classification accuracy is measured via a simple mean aggregation step across four tasks, defined by different levels of alignment between labels and communities: $\psi \in {0.7,0.8,0.9,1.0}$. Results are averaged over 8 seeds.


\textbf{Levels of community strength.} For a 2-class, $n$-node SBM with $(p,q) = \left(\frac{a\ln(n)}{n}, \frac{b\ln(n)}{n}\right)$, it is known \citep[Thm. 13]{JMLR:v18:16-480} that the community structure is recoverable when $|\sqrt{a} - \sqrt{b}| > \sqrt{2}$. For $n=100$ and $q=0.2$, this implies a community detection threshold of $p > \left(\sqrt{\frac{0.2n}{\ln(n)}} + \sqrt{2}\right)^2 \cdot \frac{\ln(n)}{n} \approx 0.56$. Values below this threshold can be seen as having low community structure. We analyze four settings: three below and one above the threshold, with $p \in {0.3, 0.4, 0.5, 0.6}$.


\textbf{Behaviour on SBMs for ComMa.} This particular SBM setup obtains performance gains as community strength increases \textemdash although this is not guaranteed for all types of graphs and tasks. In this case, HigherComMa (\autoref{fig:accsedgescomma}) can show advantages, but will suffer when the graph structure cannot be recovered. This is the case for $p=0.3$, especially with edge additions. However, for $p=0.4$ and $p=0.5$ (both still below the threshold), edge additions provide consistent benefits. For $p=0.6$, where the community structure becomes clearer, deletions cease to be useful, and performance plateaus as the number of deletions grows.

\textbf{Behaviour on SBMs for ComFy.} ComFy (\autoref{fig:accsedges}) is effective when the community structure is not clear, as it enhances the communities' signal via feature denoising (e.g., for $p=0.3$). As $p$ increases, tasks with high alignment ($\psi=1.0$) gain little from ComFy, while those with noisier label alignments ($\psi=0.8$, $\psi=0.7$) continue to benefit. 

\textbf{Behaviour on SBMs for FeaSt.} In this simple setup with only two communities, ComFy's distribution of communities is not required for good performance. In fact, its trends match those of FeaSt, as is shown in \autoref{fig:accsedgesfeast}. Yet, Feast shows higher improvements in absolute terms (especially in high Alignment $\psi=1$) due to the homophilic setup considered.

These trends align with our theoretical predictions (\S\ref{s:sbmpq}) and are also consistent with the results observed on real-world GNN benchmarks (\S\ref{s:experiments}). 



\begin{figure}[H]
    \centering
    \subfigure[For $p=0.3$.]{\includegraphics[width=0.49\linewidth]{img/MeanAccuracy-ComMa-0.3-0.2.pdf}\label{fig:pmc-55}}
    \subfigure[For $p=0.4$.]{\includegraphics[width=0.49\linewidth]{img/MeanAccuracy-ComMa-0.4-0.2.pdf}\label{fig:pmc-6}}\\
    \subfigure[For $p=0.5$.]{\includegraphics[width=0.49\linewidth]{img/MeanAccuracy-ComMa-0.5-0.2.pdf}\label{fig:pmc-7}}
    \subfigure[For $p=0.6$.]{\includegraphics[width=0.49\linewidth]{img/MeanAccuracy-ComMa-0.6-0.2.pdf}\label{fig:pmc-8}}
    \caption{The effect of ComMa on mean aggregation in SBMs for low levels of community strength. Each figure is a different SBM-$(p,0.2)$. Their rows are different levels of alignment.}
    \label{fig:accsedgescomma}
\end{figure}



\begin{figure}[H]
    \centering
    \subfigure[For $p=0.3$.]{\includegraphics[width=0.49\linewidth]{img/MeanAccuracy-ComFy-0.3-0.2.pdf}\label{fig:pm-55}}
    \subfigure[For $p=0.4$.]{\includegraphics[width=0.49\linewidth]{img/MeanAccuracy-ComFy-0.4-0.2.pdf}\label{fig:pm-6}}\\
    \subfigure[For $p=0.5$.]{\includegraphics[width=0.49\linewidth]{img/MeanAccuracy-ComFy-0.5-0.2.pdf}\label{fig:pm-7}}
    \subfigure[For $p=0.6$.]{\includegraphics[width=0.49\linewidth]{img/MeanAccuracy-ComFy-0.6-0.2.pdf}\label{fig:pm-8}}
    \caption{The effect of ComFy on mean aggregation in SBMs for low levels of community strength. Each figure is a different SBM-$(p,0.2)$. Their rows are different levels of alignment.}
    \label{fig:accsedges}
\end{figure}

\begin{figure}[H]
    \centering
    \subfigure[For $p=0.3$.]{\includegraphics[width=0.49\linewidth]{img/MeanAccuracy-FeaSt-0.3-0.2.pdf}\label{fig:pm-55-f}}
    \subfigure[For $p=0.4$.]{\includegraphics[width=0.49\linewidth]{img/MeanAccuracy-FeaSt-0.4-0.2.pdf}\label{fig:pm-6-f}}\\
    \subfigure[For $p=0.5$.]{\includegraphics[width=0.49\linewidth]{img/MeanAccuracy-FeaSt-0.5-0.2.pdf}\label{fig:pm-7-f}}
    \subfigure[For $p=0.6$.]{\includegraphics[width=0.49\linewidth]{img/MeanAccuracy-FeaSt-0.6-0.2.pdf}\label{fig:pm-8-f}}
    \caption{The effect of FeaSt on mean aggregation in SBMs for low levels of community strength. Each figure is a different SBM-$(p,0.2)$. Their rows are different levels of alignment.}
    \label{fig:accsedgesfeast}
\end{figure}

\subsection{Illustration of \autoref{th:sbmnoiseproof}}\label{app:3dplot}
In \autoref{fig:3dplot} we show 3D plots illustrating the expected proportion of misclassified nodes $P(M)$ in a Stochastic Block Model (SBM) as described in the setting in \autoref{th:sbmnoiseproof}. In the plots, when $P(M)$ is low (yellow), performance is better, and when it is high (purple), performance is worse. The plot shows $P(M)$ as a function of alignment $\psi$, and different configurations of $(p,q)$: \begin{itemize}
    \item \ref{fig:pm-a} for $\psi$ and the theoretical spectral gap formula in the limit $-\frac{p-q}{p+q}$ from \autoref{th:sbmsgproof}.
    \item \ref{fig:pm-b} and \ref{fig:pm-c} for $p$ and $q$ given fixed $\psi\in\{0.7,1.0\}$. As $\psi$ increases, the minimum (purple) and maximum (yellow) possible performance extend their range.
    \item \ref{fig:pm-d}, \ref{fig:pm-e}, \ref{fig:pm-f} for $\psi$ and $p$ given fixed values of $q\in\{0.2,0.5,0.7\}$. As the value of $q$ increases, the community structure is less pronounced. Therefore, the range of values of $p$ for which performance can achieve 100\% (in yellow) becomes more and more narrow.
\end{itemize}

\begin{figure}[ht]
    \centering
    \subfigure[Against $\psi$ and $\frac{q-p}{p+q}$. ]{\includegraphics[width=0.32\linewidth]{img/3d_plot_thm3.pdf}\label{fig:pm-a}}
    \subfigure[Fixed $\psi=0.7$. ]{\includegraphics[width=0.32\linewidth]{img/3d_plot_thm3_alignment.pdf}\label{fig:pm-b}}
    \subfigure[Fixed $\psi=1.0$. ]{\includegraphics[width=0.32\linewidth]{img/3d_plot_thm3_alignment1.pdf}\label{fig:pm-c}}\\
    \subfigure[Fixed $q=0.2$. ]{\includegraphics[width=0.32\linewidth]{img/3d_plot_thm3_q02.pdf}\label{fig:pm-d}}
    \subfigure[Fixed $q=0.5$. ]{\includegraphics[width=0.32\linewidth]{img/3d_plot_thm3_q05.pdf}\label{fig:pm-e}}
    \subfigure[Fixed $q=0.7$. ]{\includegraphics[width=0.32\linewidth]{img/3d_plot_thm3_q07.pdf}\label{fig:pm-f}}
    \caption{Plots illustrating the expected proportion of misclassified nodes $P(M)$ from \autoref{th:sbmnoiseproof}.}
    \label{fig:3dplot}
\end{figure}
