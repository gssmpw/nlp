\section{Related work}
% LLMs have been evaluated using benchmarks that test their performance on various language tasks**Devlin, "BERT"**. While these benchmarks have advanced the field, the tasks they evaluate 

Benchmarks like **Glue, "GLUE Benchmark and Evaluation"**,**SuperGLUE, "SuperGLUE: A Hierarchical Benchmark for Natural Language Understanding"**, Winogrande, "Winogrande: A Large-Scale Adversarial Dialogue Dataset"**, HellaSwag, "HellaSwag: Can a Machine Really Finish Your Sentence?"**, MMLU, "MMLU: A Multi-Task Benchmark for Evaluating Language Models"** and others, that contain objective tasks such as natural language inference and semantic similarity have been widely used within the literature to evaluate LLMs**. However, these tasks do not fully represent the realistic usage of LLMs in practical scenarios, for example as conversational agents or decision-making systems.

Other benchmarks such as **Miao et al., "MT-Bench: A Benchmark Suite for Machine Translation"**,**AlpacaEval, "AlpacaEval: An Evaluation Framework for Language Models on Instruction-Following Tasks"**, Arena-Hard, "Arena-Hard: A Multi-Task Evaluation Framework for Conversational AI Systems" and InFoBench, "InFoBench: A Benchmark Suite for Evaluating Instruction Following in Multilingual Dialogue Systems"** provide a framework for evaluating LLMs in a more practical conversational or instruction-based setting. However, their reliance on subjective AI scoring raises self-enhancement bias concerns**, making them less suitable for model evaluation.

Chatbot Arena, "Chatbot Arena: A Platform for Evaluating Conversational AI Systems" involves evaluating LLMs using human users, thus removing the potential for self-enhancement bias. However, Chatbot Arena requires large-scale deployment, making it difficult to replicate this evaluation for local models.

IFEval, "IFEval: An Evaluation Framework for Instruction Following in Dialogue Systems" was designed to evaluate the ability of LLMs to follow instructions in more practical scenarios, making it an effective measure of models intended for real-world usage. Unlike MT-Bench, IFEval uses objective criteria for evaluation, removing the subjectivity inherent in AI-judged systems. However, its primary limitation is that it is currently available only in English, which restricts its applicability for evaluating multilingual models.

% IFEval, "IFEval: An Evaluation Framework for Instruction Following in Dialogue Systems" is an objective tool assessing how well LLMs follow instructions in practical scenarios. Unlike MT-Bench, it avoids subjective judgments. However, its exclusive availability in English limits its application to multilingual models.

The model evaluation of the instruction-tuned Qwen 2.5, "Qwen 2.5: A Multilingual Evaluation Framework for Instruction Following" extended IFEval to support multilingual settings by translating 100 examples per language and removing instructions that were not applicable to a given language. While this approach allows for deterministic evaluation of LLMs on multilingual data, it may neglect language-specific aspects of instruction-following.

To overcome the limitations of existing benchmarks, we propose a new, multilingual version of IFEval that is not simply a translation of previous datasets but contains language-specific instructions for novel evaluation.