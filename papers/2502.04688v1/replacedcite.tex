\section{Related work}
% LLMs have been evaluated using benchmarks that test their performance on various language tasks________________. While these benchmarks have advanced the field, the tasks they evaluate 

Benchmarks like GLUE____, ARC____, SuperGLUE____, Winogrande____, HellaSwag____, MMLU____, and others, that contain objective tasks such as natural language inference and semantic similarity have been widely used within the literature to evaluate LLMs____. However, these tasks do not fully represent the realistic usage of LLMs in practical scenarios, for example as conversational agents or decision-making systems.

Other benchmarks such as MT-Bench____, AlpacaEval____, Arena-Hard____ and InFoBench____ provide a framework for evaluating LLMs in a more practical conversational or instruction-based setting. However, their reliance on subjective AI scoring raises self-enhancement bias concerns____, making them less suitable for model evaluation.

Chatbot Arena____ involves evaluating LLMs using human users, thus removing the potential for self-enhancement bias. However, Chatbot Arena requires large-scale deployment, making it difficult to replicate this evaluation for local models.

IFEval____ was designed to evaluate the ability of LLMs to follow instructions in more practical scenarios, making it an effective measure of models intended for real-world usage. Unlike MT-Bench, IFEval uses objective criteria for evaluation, removing the subjectivity inherent in AI-judged systems. However, its primary limitation is that it is currently available only in English, which restricts its applicability for evaluating multilingual models.

% IFEval____ is an objective tool assessing how well LLMs follow instructions in practical scenarios. Unlike MT-Bench, it avoids subjective judgments. However, its exclusive availability in English limits its application to multilingual models.

The model evaluation of the instruction-tuned Qwen 2.5____ extended IFEval to support multilingual settings by translating 100 examples per language and removing instructions that were not applicable to a given language. While this approach allows for deterministic evaluation of LLMs on multilingual data, it may neglect language-specific aspects of instruction-following.

To overcome the limitations of existing benchmarks, we propose a new, multilingual version of IFEval that is not simply a translation of previous datasets but contains language-specific instructions for novel evaluation.

% * LLMs have been evaluated in many ways.
% * GLUE, SuperGLUE etc., but they may not be relevant to practical uses of LLMs as they evaluate classification, natural language inference etc
% * MT-Bench is also good, more practical than GLUE etc but is subjectively assessed by AI and thus is prone to bias
% * Chatbot Arena is a great way of evaluating LLMs, but it cannot be replicated for an arbitrary model easily
% * IFEval is useful because it it is objectively assessed and shows the practical instruction following of the model. But it is only in English
% * While some work has been done by Qwen 2.5 to make a multilingual version, they didnt release their full methodology and they did not include new language specific tasks.
% * Therefore, we make a language specific version of IFEval for evaluating the instruction following capabilities of multilingual LLMs.