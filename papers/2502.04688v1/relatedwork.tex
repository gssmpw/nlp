\section{Related work}
% LLMs have been evaluated using benchmarks that test their performance on various language tasks~\cite{rajpurkar-etal-2016-squad}~\cite{wang-etal-2018-glue}~\cite{wang2019superglue}~\cite{hendrycks2021mmlu}. While these benchmarks have advanced the field, the tasks they evaluate 

Benchmarks like GLUE~\cite{wang-etal-2018-glue}, ARC~\cite{allenai:arc}, SuperGLUE~\cite{wang2019superglue}, Winogrande~\cite{sakaguchi2019winogrande}, HellaSwag~\cite{zellers2019hellaswag}, MMLU~\cite{hendryckstest2021}, and others, that contain objective tasks such as natural language inference and semantic similarity have been widely used within the literature to evaluate LLMs~\cite{anil2023palm,le2023bloom,dettmers2023qlora}. However, these tasks do not fully represent the realistic usage of LLMs in practical scenarios, for example as conversational agents or decision-making systems.

Other benchmarks such as MT-Bench~\cite{zheng2023judging}, AlpacaEval~\cite{dubois2024length}, Arena-Hard~\cite{li2024crowdsourced} and InFoBench~\cite{qin-etal-2024-infobench} provide a framework for evaluating LLMs in a more practical conversational or instruction-based setting. However, their reliance on subjective AI scoring raises self-enhancement bias concerns~\cite{xu-etal-2024-pride}, making them less suitable for model evaluation.

Chatbot Arena~\cite{zheng2023judging} involves evaluating LLMs using human users, thus removing the potential for self-enhancement bias. However, Chatbot Arena requires large-scale deployment, making it difficult to replicate this evaluation for local models.

IFEval~\cite{zhou2023instruction} was designed to evaluate the ability of LLMs to follow instructions in more practical scenarios, making it an effective measure of models intended for real-world usage. Unlike MT-Bench, IFEval uses objective criteria for evaluation, removing the subjectivity inherent in AI-judged systems. However, its primary limitation is that it is currently available only in English, which restricts its applicability for evaluating multilingual models.

% IFEval~\cite{Instruction-Following Evaluation for Large Language Models} is an objective tool assessing how well LLMs follow instructions in practical scenarios. Unlike MT-Bench, it avoids subjective judgments. However, its exclusive availability in English limits its application to multilingual models.

The model evaluation of the instruction-tuned Qwen 2.5~\cite{qwen2.5} extended IFEval to support multilingual settings by translating 100 examples per language and removing instructions that were not applicable to a given language. While this approach allows for deterministic evaluation of LLMs on multilingual data, it may neglect language-specific aspects of instruction-following.

To overcome the limitations of existing benchmarks, we propose a new, multilingual version of IFEval that is not simply a translation of previous datasets but contains language-specific instructions for novel evaluation.

% * LLMs have been evaluated in many ways.
% * GLUE, SuperGLUE etc., but they may not be relevant to practical uses of LLMs as they evaluate classification, natural language inference etc
% * MT-Bench is also good, more practical than GLUE etc but is subjectively assessed by AI and thus is prone to bias
% * Chatbot Arena is a great way of evaluating LLMs, but it cannot be replicated for an arbitrary model easily
% * IFEval is useful because it it is objectively assessed and shows the practical instruction following of the model. But it is only in English
% * While some work has been done by Qwen 2.5 to make a multilingual version, they didnt release their full methodology and they did not include new language specific tasks.
% * Therefore, we make a language specific version of IFEval for evaluating the instruction following capabilities of multilingual LLMs.