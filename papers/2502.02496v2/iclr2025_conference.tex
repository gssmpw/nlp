
\documentclass{article} % For LaTeX2e
\usepackage{iclr2025_conference,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage{hyperref}
\usepackage{url}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{comment}
\usepackage[capitalise]{cleveref}
\usepackage{bm}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{pifont}
\usepackage[font=small,skip=3pt]{caption}
\usepackage{dsfont}
\usepackage{xcolor}
\usepackage[draft]{fixme}
%\usepackage{natbib} 

\usepackage{wrapfig}
\usepackage{float}
\usepackage{booktabs}
\usepackage{multirow}

\usepackage{algorithmic}
\usepackage{algorithm}

\usepackage[page,header]{appendix}
\usepackage{titletoc}

\usepackage{tikz}
\usetikzlibrary{positioning, arrows.meta}

% Theorem stuff
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}
\newtheorem{proposition}{Proposition}
\newtheorem{lemma}{Lemma}
\newtheorem{corollary}{Corollary}
\theoremstyle{definition}
\newtheorem{definition}{Definition}
\newtheorem{example}{Example}
\newtheorem{assumption}{Assumption}
%\theoremstyle{remark}
\newtheorem{remark}{Remark}

% new commands
\newcommand*\circled[1]{\tikz[baseline=(char.base)]{
            \node[shape=circle,draw,inner sep=1pt](char){#1};}}

\newcommand{\twoline}[2]{\begin{tabular}[t]{@{}c@{}}#1\\#2\end{tabular}}

% Define colors
\definecolor{ckblue}{rgb}{0.53,0.81,0.92} % Skyblue color for CK
\definecolor{drred}{rgb}{1,0,0} % Red color for DR

% Register authors with colored and bold prefixes
\FXRegisterAuthor{ck}{ackck}{\textcolor{ckblue}{\textbf{CK:}}}
\FXRegisterAuthor{dr}{ackdr}{\textcolor{drred}{\textbf{DR:}}}

% Define custom fixme commands
\newcommand{\ckfixme}[1]{\fxnote[author=CK, layout=inline]{\textcolor{ckblue}{#1}}}
\newcommand{\drfixme}[1]{\fxnote[author=DR, layout=inline]{\textcolor{drred}{#1}}}

%%%%%%%%%% Definitions
\extrafloats{100}


\title{Deep Weight Factorization: Sparse Learning Through the Lens of Artificial Symmetries}


% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

\author{Chris Kolb, Tobias Weber, Bernd Bischl, \& David R\"ugamer\\ %\thanks{ Use footnote for providing further information
%about author (webpage, alternative address)---\emph{not} for acknowledging
%funding agencies.  Funding acknowledgements go at the end of the paper.} \\
Department of Statistics, LMU Munich, Munich\\
Munich Center for Machine Learning (MCML), Munich
\\
\texttt{\{chris.kolb,tobias.weber,bernd.bischl,david\}@stat.uni-muenchen.de} \\
%\And
%Ji Q. Ren \& Yevgeny LeNet \\
%Department of Computational Neuroscience \\
%University of the Witwatersrand \\
%Joburg, South Africa \\
%\texttt{\{robot,net\}@wits.ac.za} %\\
%\AND
%Coauthor \\
%Affiliation \\
%Address \\
%\texttt{email}
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}
\newcommand{\w}{\mathbf{w}}

\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}


\maketitle

\begin{abstract}
Sparse regularization techniques are well-established in machine learning, yet their application in neural networks remains challenging due to the non-differentiability of penalties like the $L_1$ norm, which is incompatible with stochastic gradient descent. A promising alternative is shallow weight factorization, where weights are decomposed into two factors, allowing for smooth optimization of $L_1$-penalized neural networks by adding differentiable $L_2$ regularization to the factors. 
In this work, we introduce deep weight factorization, extending previous shallow approaches to more than two factors. We theoretically establish equivalence of our deep factorization with non-convex sparse regularization and analyze its impact on training dynamics and optimization.
Due to the limitations posed by standard training practices, we propose a tailored initialization scheme and identify important learning rate requirements necessary for training factorized networks.
We demonstrate the effectiveness of our deep weight factorization through experiments on various architectures and datasets, consistently outperforming its shallow counterpart and widely used pruning methods.
\end{abstract}



\section{Introduction}
% less high-level intro but maybe also less vulnerable to criticism if framed correctly?
Making models sparse is a contemporary challenge in deep learning, currently attracting a lot of attention. Among the more prominent methods to achieve sparsity are model pruning methods \citep{gale2019state,blalock2020state} and regularization approaches sparsifying the model during training \citep{hoefler2021sparsity}. While in statistics and machine learning, sparse regularization approaches are well-established \citep[see, e.g.,][]{tian2022comprehensive}, the non-smoothness of sparsity penalties such as the $L_1$ norm impedes the optimization of neural networks when using classical stochastic gradient descent (SGD) optimization. A possible solution that allows SGD-based optimization while inducing $L_1$ regularization is \emph{weight factorization}. Originally proposed in statistics for linear models \citep{hoff2017lasso}, the idea of factorizing the weights $\w = \bomega_1 \odot \bomega_2$ to obtain a differentiable $L_1$ penalty on the product $\bomega_1\odot\bomega_2$ has recently been adopted also in deep learning \citep[see, e.g.,][]{ziyin2023spred}.  This simple trick allows the integration of 
\begin{wrapfigure}[12]{r}[0pt]{0.583\textwidth}
    \includegraphics[width=\linewidth]
    %figures/new/lenet300100_fmnist_mean_combined_plots.pdf
    {figures/new/lenet300100_fmnist_combined_plots_mean_revised_100_inset.pdf}
        \caption{Sparsity-accuracy tradeoff using a vanilla $L_1$ penalization with SGD (blue) compared to (deep) weight factorization. Means and std.~deviations over 3 random seeds are shown.
        }
        \label{fig:lenet300100-fmnist-seeds-pruning}
\end{wrapfigure}
convex $L_1$-based sparsity into neural network training while promising direct applicability of familiar SGD. 
As shown in \cref{fig:lenet300100-fmnist-seeds-pruning}, the obtained sparsity of differentiable $L_1$ is superior to vanilla $L_1$ regularization. This holds even after applying additional post-hoc pruning, demonstra\-ting that the inferior sparsity performance of vanilla $L_1$ is not just due to a suboptimal thres\-hold but also the incompatibility of SGD and non-smooth penalties.

Given the success of (shallow) weight factorization, we study deep weight factorization in this work, i.e., factorizing $\w = \bomega_1 \odot \cdots \odot \bomega_D, D\geq 2$ (cf.~\cref{fig:intro-plot}). 
%While factorizing weights into two parts has been shown to successfully enable $L_1$-type sparsity in neural networks, 
We investigate whether theoretical guarantees support the use of a \emph{depth}-$D$ factorization, whether it is beneficial for sparsity, what implications its usage has on training dynamics, and analyze other practices such as initialization. %, and if there are theoretical guarantees supporting the use of a \emph{depth}-$D$ factorization.

\paragraph{Our contributions}


In this work, we address the aforementioned challenges and close an important gap in the current literature. We first theoretically show the equivalence of factorized neural networks with sparse regularized optimization problems for depth $D \geq 2$, allowing for differentiable non-convex sparsity regularization in any neural network. We then discuss optimization strategies for these factorized networks including their initialization and appropriate learning rate schedules. We also analyze the training dynamics of such networks, showing a particularly interesting connection between the evolution of weight norms, compression, accuracy, and generalization. Conducting experiments on a range of architectures and datasets, we further substantiate our theoretical findings and demonstrate that our proposed factorized networks usually outperform the recently proposed shallow factorization and yield competitive results to commonly used pruning methods.


\begin{figure}
    \centering
    \resizebox{0.8\textwidth}{!}{
        \input{figures/overview/overview-figure.tex}
    }
    \caption{Overview of the proposed method (cf.~\cref{alg:train}). Our approach proceeds by factorizing the neural network weights and running SGD on the factors $\bomega_d$ with weight decay. Post-training, the factors are collapsed again, with the resulting sparse solutions being minimizers of the non-smooth $L_{2/D}$-regularized objective.}
    \label{fig:intro-plot}
\end{figure}


\section{Background and related literature}

\subsection{Notation}\label{sec:notation}
Let $\left\{(\bm{x}_i, y_i)\right\}_{i=1}^n$ be the training data of independent samples $(\bm{x}_i, y_i) \in \mathcal{X} \times \mathbb{R}^c$, and $n,c\in\mathbb{N}$. Let %$\mathcal{H}_f \subset \{f: \mathbb{R}^p \times \mathcal{X} \rightarrow \mathbb{R}^c\}$ be a hypothesis class of neural networks parameterized by $\w \in \mathbb{R}^p$ and let 
$f(\w, \bm{x}): \mathcal{X} \rightarrow \mathbb{R}^c$ denote a network realization for any $\w \in \mathbb{R}^p$.
In general, we are interested in minimizing $\ell(\cdot, \cdot): \mathbb{R}^c \times \mathbb{R}^c \rightarrow \mathbb{R}_0^{+}$ denoting a continuous per-sample loss. %, e.g., cross-entropy or squared loss.
The \textit{$L_q$ norm} of a vector $\mathbf{w} \in \mathbb{R}^p$ is defined as $\|\w\|_q = \left( \sum_{i=1}^p |\mathrm{w}_i|^q \right)^{1/q}$ for $q>0$. Note that $L_q$ \textit{regularizers} are defined differently as $\Vert \w \Vert_q^q$ and that for $q<1$, only a non-convex quasi-norm is defined. For two vectors $\bomega_1, \bomega_2 \in \mathbb{R}^p$, we use $\odot$ to denote their element-wise multiplication. For an optimization problem $\min_{\w} \mathcal{L}(\w)$, we denote $\hat{\w} := \argmin_{\w} \mathcal{L}(\w)$. Finally, the \textit{compression ratio} (CR) is defined as the ratio of original to sparse model parameters.%by $CR=\#\text{original params}/\#\text{sparse params}$, %to parameters in the sparse model $\w_{\text{sparse}}$: $\text{CR} = p/{\|\w_{\text{sparse}}\|_0}$, where $\|\cdot\|_0$ counts the number of non-zero entries. 
% and the \textit{remaining ratio} (RR) is the fraction of remaining parameters in the sparse model: $\text{RR} = 1 - \text{sparsity} % = 1 - (1-\|\w_{\text{sparse}\|_0}/p)
% =1/\text{CR}$.

\subsection{Differentiable \texorpdfstring{$L_1$}{L1} regularization}

%optimization literature

%\citet{hoff2017lasso} \citet{tibs2021} %\citet{grandvalet1998least}

%\citet{woodworth2020kernel} \citet{gunasekar2018implicit,nacson2022implicit}

Weight factorizations were previously mostly studied for regularized linear models or as toy models for deep learning theory. We briefly illustrate this using the idea of a differentiable lasso. %, as popularized by \citet{hoff2017lasso}, using $L_2$ regularization.  

\paragraph{Differentiable lasso} %We apply a shallow factorization with depth $D=2$ to the weights of 
The original lasso objective is defined as
\begin{equation} \label{eq:lasso_objective}
\min_{\w \in \mathbb{R}^p} \mathcal{L}_{\w,\lambda}(\w) := \sum_{i=1}^n \left( y_i - \bm{x}_i^\top \w \right)^2 + \lambda \| \w \|_1,
\end{equation}
where $\lambda > 0$ promotes sparsity via the $L_1$ norm \citep{tibshirani1996regression}. By factorizing $\w$ into $\bomega_1$ and $\bomega_2$ such that $\w = \bomega_1 \odot \bomega_2$, and replacing the non-differentiable $L_1$ penalty with an $L_2$ penalty on $\bomega=(\bomega_1,\bomega_2)$, we can obtain a differentiable formulation of the lasso \citep{hoff2017lasso}:
\begin{equation} \label{eq:factorized_objective}
\min_{\bomega_1, \bomega_2 \in \mathbb{R}^p} \mathcal{L}_{\bomega,\lambda}(\bomega) := \sum_{i=1}^n \left( y_i - \bm{x}_i^\top ( \bomega_1 \odot \bomega_2 ) \right)^2 + \frac{\lambda}{2} \left( \| \bomega_1 \|_2^2 + \| \bomega_2 \|_2^2 \right),
\end{equation}
%\citet{grandvalet1998least} first connected the product of two parameters and $L_2$ regularization to a variant of the lasso. 
The formulation \cref{eq:factorized_objective} is equivalent to \cref{eq:lasso_objective} in the sense that all minima of the non-convex objective in \cref{eq:factorized_objective} are global and related to the unique lasso solution of \cref{eq:lasso_objective} as $\hat{\bomega}_1 \odot \hat{\bomega_2}=\hat{\w}$. \citet{hoff2017lasso} proposes solving \cref{eq:factorized_objective} via alternating ridge regression. %, fixing one factor while analytically solving for the other. 
However, this relies on the biconvexity of the problem and cannot be easily extended beyond linear models.

\paragraph{Differentiable $L_1$ regularization in general neural networks}

% \citet{ziyin2023symmetry}

Recently, \citet{ziyin2023spred} proposed applying a shallow factorization to arbitrary weights of a neural network. Coupled with weight decay, this allows obtaining a differentiable formulation of the sparsity-inducing $L_1$ penalty that can be optimized with simple SGD. Specifically, by factorizing the weights $\w$ of any neural network $f_{\w}(\w,\bx)$ as $\w = \bomega_1 \odot \bomega_2$, and applying $L_2$ regularization to the factors, the resulting optimization problem %becomes %:
%\begin{equation} \label{eq:general_factorized_objective}
%\min_{\bomega_1, \bomega_2 \in \mathbb{R}^p} \mathcal{L}_{\bomega,\lambda}(\bomega) := \sum_{i=1}^n \ell\left(y_i,  f_{\w}(\bomega_1 \odot \bomega_2, \bm{x}_i) \right) + \frac{\lambda}{2} \left( \| \bomega_1 \|_2^2 + \| \bomega_2 \|_2^2 \right).
%\end{equation}
has the same minima as the $L_1$ regularized vanilla network. The key insight for the equivalence with $L_1$ regularization is that the factorization $\w = \bomega_1 \odot \bomega_2$ introduces a rescaling symmetry in the (unregularized) loss $\mathcal{L}_{\bomega,0}$. 

\begin{definition}[Rescaling Symmetry]\label{def:artificial-rescaling}
Let the parameters of a loss function $\mathcal{L}_{\bm{\theta}}(\bm{\theta})$ be partitioned as $\bm{\theta}=(\bomega_1, \bomega_2, \bm{\theta}_0)$, with $\bm{\theta}_0$ denoting the remaining parameters. Then $\mathcal{L}_{\bm{\theta}}(\bm{\theta})$ possesses a rescaling symmetry w.r.t.\ arbitrary parameters $\bomega_1, \bomega_2$ belonging to $\bm{\theta}=(\bomega_1, \bomega_2, \bm{\theta}_0)$ if for any $c \neq 0$:
\begin{equation*}\label{eq:rescaling-symmetry}
  \mathcal{L}_{\bm{\theta}}(\bomega_1, \bomega_2, \bm{\theta}_0)= \mathcal{L}_{\bm{\theta}}(c \cdot \bomega_1, c^{-1} \cdot \bomega_2, \bm{\theta}_0) \,\,\, \forall \, \bm{\theta}.  
\end{equation*}
\end{definition}


% cite \citep{ziyin2023symmetry} and work in relevant content 

\begin{wrapfigure}[12]{r}[0pt]{0.23\textwidth}
\vspace{-0.35cm}
\centering
\includegraphics[width=\linewidth]{figures/new/factorization_fibers.pdf}
\captionsetup{skip=3pt} % 
\caption[]{Scalar rescaling symmetry and min-norm factorizations.}
\label{fig:rescaling-symmetry}
\end{wrapfigure}
While previous works mainly studied rescaling symmetries naturally arising in, e.g., homogeneous activation functions \citep{neyshabur2015search,parhi2023deep}, weight factorization constitutes an \textit{artificial} symmetry that is independent of $\mathcal{L}$, and by extension also of $\ell(\cdot,\cdot)$ and $f_{\w}(\cdot,\bx)$. This applicability to any parametric problem designates artificial symmetries as a powerful tool for constrained learning \citep{ziyin2023symmetry,chen2024stochastic}. Intuitively, the additional $L_2$ regularization enforces preference for min-norm factorizations $(\bomega_1^{\ast},\bomega_2^{\ast})$ among all feasible factorizations of a given $\w$ (\cref{fig:rescaling-symmetry}). At such a min-norm factorization of $\w$, the $L_2$ penalty in \cref{eq:factorized_objective} reduces to $%\frac{1}{2} (\|\bomega_1\|_2^2+\|\bomega_2\|_2^2) \geq
\Vert \bomega_1^{\ast} \odot \bomega_2^{\ast} \Vert_1=\Vert \w \Vert_1$, effectively inducing $L_1$ regularization on the collapsed parameter. This approach allows for implementing $L_1$ regularization in general networks using GD without requiring specialized algorithms to handle non-differentiable regularization.

We refer to \cref{app:literature} for additional related methods and discussion. \cref{app:intuition} provides some intuition why weight factorization with $L_2$ regularization promotes sparse solutions based on \cref{fig:rescaling-symmetry}.
%
%\paragraph{Factorization without regularization} The implicit bias of SGD induced by weight factorization in linear models without explicit regularization has been studied in deep learning as a proxy to neural network optimization dynamics~\citep{gunasekar2018implicit,woodworth2020kernel}. 
% However, these implicit regularization approaches are impractical for real applications, as they require vanishing initializations or specific loss functions. 

\section{Theoretical results} \label{sec:theory}
% specific for equivalence of optimization problems for depths>2
% If this section gets too long, separate content into background and theory sections

Based on a given network specification of $f(\w,\bm{x})$, we study its \emph{depth-$D$} factorization with $D\geq2$, which we call Deep Weight Factorization (DWF) and is defined as follows:
\begin{definition}[Deep Weight Factorization]
    A depth-$D$ factorization with $D\in\mathbb{N}_{\geq2}$ of an arbitrary neural network $f_{\w}(\w,\bm{x}),\, \w \in \mathbb{R}^p$, is given by $f_{\w}(\bomega_1 \odot \ldots \odot \bomega_D, \bx)$ with $\bomega= (\bm{\omega}_1,\ldots,\bm{\omega}_D)$ and factors $\bm{\omega}_d = (\omega_{1,d},\ldots,\omega_{p,d}) \in \mathbb{R}^{p}, d\in[D]$. The original and factorized parameters are related through $\w = \bm{\omega}_1 \odot \ldots \odot \bm{\omega}_D=: \bomegae$, where $\bomegae$ denotes the collapsed parameter. %:\mathbb{R}^{Dp} \to \mathbb{R}^p$ defining the factorization, omitting its argument $\bomega$. . %, i.e., $f_\w(\w,\bm{x})=f_\w(\bomegae,\bm{x})=f_{\bomega}(\bomega,\bm{x})$. 
    Further, a factorization depth is called \emph{shallow} for $D=2$ and otherwise \emph{deep}.
\end{definition}

%By default, all weights and biases in $f_{\w}$ are factorized using DWF. The factorization, however, can also be applied only to an arbitrary selection of elements of $\w$. Clearly, DWF does not change the expressivity of the underlying network $\fw(\w,\bx)$. Our focus lies on the effect of $L_2$ regularization and the behavior of SGD optimization in factorized networks.

In this work, we focus on unstructured sparsity. This means all weights and biases in $f_{\w}$ are factorized using DWF. In principle, however, the factorization can also be selectively applied to arbitrary subsets of the parameters $\w$. Importantly, while DWF does not alter the expressive capacity of the underlying network $f_{\w}$, it drastically alters the optimization dynamics and enables sparse learning in conjunction with $L_2$ regularization or weight decay. Therefore, our focus lies on examining the effects of $L_2$ regularization and the behavior of SGD optimization in factorized networks.


The regularized training loss %given $L_q$ regularization $\mathcal{J}(\cdot)=\Vert\cdot\Vert_q^q,\,0<q\leq1$, without and 
with DWF and regularization strength $\lambda>0$ is defined to be
%
{
\begin{eqnarray}\label{eq:losses}
%\mathcal{L}_{\w,\lambda}(\mathbf{w})=\frac{1}{n} \sum_{i=1}^n \ell\left(y_i, f_\w\left(\w, \bm{x}_i\right)\right)+\lambda \mathcal{J}_{\w}(\w), \,\,\,\,
\mathcal{L}_{\bomega,\lambda}(\bomega)=
%\frac{1}{n} \sum_{i=1}^n \ell\left(y_i, f_{\bomega}\left(\bomega, \bm{x}_i\right)\right)+\frac{\lambda}{D} \Vert{\bomega}\Vert_2^2 =
\frac{1}{n} \sum_{i=1}^n \ell\left(y_i, f_{\w}\left(\bomega_1 \odot \ldots \odot \bomega_D, \bm{x}_i\right)\right)+\frac{\lambda}{D} \sum_{d=1}^{D}\Vert{\bomega_d}\Vert_2^2. 
\end{eqnarray}
}
% 


For a given $\w$, applying DWF to the training objective introduces an infinite set of feasible factorizations $\{(\bomega_1,\ldots,\bomega_D): \bomegae = \w\}$ that leave the network output $f_\w(\w, \bx)$ and loss invariant. Those factorizations, however, differ in their respective norms. While the norm of individual factors can grow arbitrarily large, there exist factorizations that minimize the Euclidean norm, or equivalently, the factor $L_2$ penalty. $L_2$ regularization thus biases the optimization toward min-norm factorizations. This regularization ensures that the parameter representation strives to be evenly distributed across factors. The following result formalizes the necessary optimality conditions for the factorized objective, identifying solution candidates as those that achieve minimal norm configuration.

\begin{lemma}[Necessary condition for solution and minimum $L_2$ penalty] \label{lemma:min-l2-penalty}
Let $\bomega = (\bomega_1, \ldots, \bomega_D) \in \mathbb{R}^{Dp}$ be a local minimizer of $\mathcal{L}_{\bomega,\lambda}(\bomega)$. Then i) $|\omega_{j,1}| = \ldots = |\omega_{j,D}|\,$ for all $j \in [p]$, and ii) the factor $L_2$ penalty reduces to $D^{-1} \sum_{d=1}^D \|\bomega_d\|_2^2 = \|\bomegae\|_{2/D}^{2/D}$.
\end{lemma}

Using the result of~\cref{lemma:min-l2-penalty}, we introduce the concept of \textbf{factor misalignment} to quantify the distance from balanced factorizations required for solutions. %For a depth-$D$ factorized network with parameters $\bomega = (\bomega_1, \ldots, \bomega_D)$ and collapsed parameter $\w = \bigodot_{d=1}^D \bomega_d$, 
Specifically, the factor misalignment is defined as $M(\bomega) = D^{-1} \sum_{d=1}^D \Vert\bomega_d\Vert_2^2 - \Vert \bomegae \Vert_{2/D}^{2/D}$ and captures the difference between the factor $L_2$ penalty and that of a balanced minimum-norm factorization of the same collapsed $\bomegae$. The misalignment satisfies $M(\bomega) \geq 0$, with equality if and only if the factorization is balanced. %$|\omega_{j,1}| = \ldots = |\omega_{j,D}| = |\omegae_j|^{1/D}$ for all $j \in [p]$. 
%
This allows us to restrict the search for potential solutions to balanced factorizations $M(\bomega)=0$, as required by \cref{lemma:min-l2-penalty}. \cref{lemma:balancedness} in \cref{app:conserved-balancedness} describes the remarkable implications of reaching zero misalignment for SGD dynamics, collapsing the dynamics to a constrained symmetry-induced subspace in which the parameters remain for all future iterations (cf.~\cref{fig:vgg19-misalign} for dynamics of $M(\bomega)$).

%Using results from \cref{lemma:min-l2-penalty}, we know that under balancedness that $L_{2,D}$ and $L_2$ must be equivalent ..., motivates the following result:

The results from \cref{lemma:min-l2-penalty,lemma:balancedness} highlight the significance of factor misalignment for both the landscape of loss functions under DWF and the trajectories of SGD optimization. For balanced factorizations, the usual smooth $L_2$ penalty remarkably takes the equivalent form of a sparsity-inducing regularizer, with SGD dynamics being restricted to simpler symmetry-induced subspaces. Notably, both $L_2$ regularization and SGD noise naturally drive the dynamics towards balance \citep{chen2024stochastic}. These observations motivate the following key result:


\begin{theorem}[Equivalence of optimization problems] \label{thm:equi}
    The optimization problems %in \cref{eq:org} and \cref{eq:orgover} 
\begin{eqnarray} \label{eq:org}    
&\min_{\w\in \mathbb{R}^p} \Lw(\w):= \frac{1}{n} \sum_{i=1}^n \ell\left(y_i, f_{\w}\left(\w,\bm{x}_i\right)\right) + \lambda \| \w \|_{2/D}^{2/D} \phantom{of_the_opera_of_the_op}\\ \label{eq:orgover}
&\min_{\bm{\omega} \in \mathbb{R}^{Dp}} \Lomega(\bomega) := \frac{1}{n} \sum_{i=1}^n \ell\left(y_i, f_{\w}\left(\bomega_1 \odot \ldots \odot \bomega_D,\bm{x}_i\right)\right)+\frac{\lambda}{D}\sum_{d=1}^{D}\Vert{\bomega_d}\Vert_2^2
\end{eqnarray}
    have the same global and local minima with the respective minimizers related as $\hat{\w}=\hat{\bomega}_1 \odot \ldots \odot \hat{\bomega}_D$. 
\end{theorem}
Practically speaking, instead of attempting to optimize the non-smooth problem in Eq.~(\ref{eq:org}), we can alternatively optimize the smooth problem in Eq.~(\ref{eq:orgover}) as every local or global solution of the DWF model will yield a corresponding local or global solution in the original model space. Hence, this allows inducing sparsity in typical deep learning applications with SGD-optimization by a simple $L_2$ regularization using Eq.~(\ref{eq:orgover}). In contrast, the non-differentiability in~(\ref{eq:org}) will cause the optimization to oscillate and not provide the desired sparsity (see \cref{sec:failure-direct-l1} and \cref{fig:lenet300100-fmnist-seeds-pruning}). The $L_{2/D}$ penalty in Eq.~(\ref{eq:org}) becomes non-convex and increasingly closer to the $L_0$ penalty for $D>2$, permitting a more aggressive penalization of small weights than $L_1$ regularization \citep{frank1993statistical}.

%Due to the generality of \cref{thm:equi}, it is further worth noting the following:

%\begin{remark}
%    \cref{thm:equi} does not assume any particular structure of $\mathcal{L}$. This means that the result is \textbf{not} restricted in its application to a specific network architecture. 
%\end{remark}

%\begin{remark}
%    \cref{thm:equi} also recovers the case where $D=2$, thereby also providing an alternative proof for the results discussed in \citet{ziyin2023spred}.
%\end{remark}

While the theoretical equivalence derived in \cref{thm:equi} establishes correspondence of all minimizers and suggests a simple way to induce sparsity in arbitrary neural networks, the optimization of a DWF model is not straightforward and little is known about the learning dynamics of such a model. We will hence study these two aspects in the following section. 

\section{Optimization and dynamics of deep factorized networks} \label{sec:opt}

Two crucial aspects of successfully training DWF models are their initialization and the learning rate when optimizing with SGD. 

\begin{figure}
    \centering
    \includegraphics[width=1.0\linewidth]{figures/new/combined_equivar_plots.pdf}
    \caption{DWF initialization strategies. \textbf{Left}: factor densities with variance matching and truncation. \textbf{Middle}: product densities for $D=4$ illustrating kurtosis explosion without truncation. \textbf{Right}: sparsity-accuracy curves for different initializations and $D$, showing the failure of standard initialization.}
    \label{fig:init-combined-plot}
\end{figure}

\subsection{Initialization} \label{sec:init}

Applying DWF to a neural network factorizes each parameter into a product of $D$ factors. However, initializations for factorized neural networks are not straightforward since the product distribution of random variables often drastically differs from the factor distribution, leading to pathological behavior, especially for deep factorizations. To retain the properties of standard initializations, defined in \cref{app:proofs-definitions}, adjustments to the factor initializations need to be implemented.\\
Our exposition focuses on the simplest case $\mathrm{w}_j^{(l)} \sim \mathcal{N}(0,1/n_{\text{in}}^{(l)})$, where $n_{\text{in}}^{(l)}$ is the number of input units to the $l$-th layer \citep{lecun2002efficient}, but similar arguments can be made for other approaches. While \citet{ziyin2023spred} use standard initialization for shallow factorizations with good results, this consistently fails for $D>2$ in our experiments and only works in few cases for $D=2$ (cf.~\cref{fig:init-combined-plot,fig:initgrid-lenet5-fmnist-compar}). The following result shows that initializing a factorized neural network using a standard scheme %, the collapsed parameter $\bomegae = \textstyle{\prod_{d=1}^{D}} \bomega_d$ has vanishing variance rather than the desired distribution (\cref{fig:init-combined-plot}). 
leads to deteriorating initialization quality and vanishing activation variance:

\begin{lemma}[Standard initializations in factorized networks]\label{lemma:init-factorized-networks} Consider a factorized neural network with $L$ layers and factorization depth $D \geq 2$, where $\w^{(l)}=\bomega_1^{(l)} \odot \ldots \odot \bomega_D^{(l)}$ and the scalar factors $\omega_{j,d}^{(l)}$ are initialized using a standard scheme. Then \textbf{i)} the collapsed weights $\omegae_j^{(l)}=\prod_{d=1}^D \omega_{j,d}^{(l)} \overset{p}{\longrightarrow} 0$ as $D$ grows, and \textbf{ii)} for any $D\geq 2$, the variance of the activations vanishes in both $n_{\text{in}}$ and $L$.  
\end{lemma}


\paragraph{Rectifying the failure of standard initializations in DWF} Given a standard initialization $\mathrm{w} \sim \mathcal{P}(\mathrm{w})$ with variance $\sigma_{\mathrm{w}}^2$, the first step is to correct the variance of the product $\omegae$ %and layer activations %effects of weight factorization on the quality of standard initialization 
by initializing the factors $\omega_d$ so that the variance of their product matches that of $\mathcal{P}(\mathrm{w})$. %Since standard schemes use independent zero-mean distributions, and the variance of their product factorizes, 
This variance matching of $\omegae$ and $\mathrm{w}$ is achieved by setting 
$\text{Var}(\omega_d) = \text{Var}(\mathrm{w})^{1/D}$ and named \textit{VarMatch} initialization here. %Another approach to rectify the product variance initializes $\omega_1 \sim \mathcal{P}(\mathrm{w})$ and $\omega_d \sim \mathcal{N}(0,1)$ for the remaining factors. Both, 
However, only considering the variance overlooks the importance of higher-order moments for initialization in deep learning. For example, given a factor initialization $\omega_d \sim \mathcal{N}(0, \sigma^2)$, we have $\mathbb{E}\big[(\omegae)^2\big] = \sigma^{2D}, \quad \mathbb{E}\big[(\omegae)^4\big] = 3^D \sigma^{4D}$,
implying the kurtosis of $\omegae$ grows exponentially as $\kappa_{\omegae} = 3^D$ regardless of variance matching (cf.~\cref{fig:init-combined-plot}). In DWF with plain variance matching, we observe a performance decline and the undesirable emergence of inactive weights (cf.~\cref{fig:resnet18-truncation-compar}).\footnote{Inactive or dead weights are collapsed weights $\omegae$ consisting of factors $\omega_d$ with vanishingly small initialization values, resulting in $\omegae$ not changing during training.%The multiplicative dependence of gradient on current weights in DWF further removes any escape probability and the weights remain inactive at $0$.
} %whose values do not change over training
%even for a variance-corrected approach initialization.

%Since standard schemes use independent zero mean distributions, and because the variance of the product of these random variables factorizes into the product of their variances, we achieve mean-variance matching by simply setting $\text{Var}(\omega_d) = \text{Var}(w)^{1/D}$. A similar approach, proposed in \citet{ziyin2023spred}, includes initializing $\omega_1 \sim \mathcal{P(w)}$ and $\omega_d$ to be of unit variance $\mathcal{N}(0,1)$. However both approaches, as often overlooked in the initialization literature, ignore the importance of higher-order moments for initialization quality. In factorized neural networks, this leads to a product distribution with high kurtosis and probability mass concentrated near zero (\cref{fig:init-combined-plot}). %Considering higher moments has been largely overlooked, as it poses less of an issue in other applications.

Since variance matching alone does not yield satisfactory results for $D>2$, %given the exponential increase in kurtosis, %To address this, 
we additionally propose a tailored interval truncation of the factor initialization outside of a certain absolute value range and name this approach $\texttt{DWF}$ initialization (see also~\cref{alg:init}). This redistributes the accumulating probability mass away from $0$ and prevents catastrophic initialization of dead weights. The truncation thresholds control the smallest and largest possible absolute values $\omegae_{\min}$ and $\omegae_{\max}$ of $\omegae$, defining the support of the product distribution. Setting the upper truncation threshold to $(2 \sigma_{\w})^{1/D}$ to address large outliers and the lower threshold to  $\varepsilon^{1/D}$, for some $\varepsilon>0$, successfully removes pathological product initializations in our experiments. 
%The details of our initialization are described in \cref{alg:init}.
%\footnote{In our experiments, we set the lower absolute threshold to $\omegae_{\min}^{1/D}$ with $\omegae_{\min} = 3 \times 10^{-3}$.} (\cref{fig:init-combined-plot})
%.

Together, the crucial ingredients for \texttt{DWF} initialization are corrections for both the vanishing variance of the product distribution and its concentration around zero. 

\begin{remark}
The factorized bias parameters should not be initialized to all zeros, as this corresponds to a saddle point from which gradient descent cannot escape by symmetry (see \cref{lemma:balancedness}).
\end{remark}



\subsection{Learning rate}

\begin{figure}[t]
    \centering
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=0.9\linewidth]{figures/new/trunc_init_compar.pdf}
        \caption{Acc. degradation and dead weights w/o truncation despite $\lambda=0$ for ResNet-18 and $D=10$.}
        \label{fig:resnet18-truncation-compar}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=0.9\linewidth]{figures/new/lr_comparison.pdf}
        \caption{Sparsity only emerges with sufficiently large LR for LeNet-300-100 and $D=4$.}
        \label{fig:lenet300100-lrfail}
    \end{subfigure}%
    %\hspace{1cm}
    \caption[]{Failure modes when optimizing factorized neural networks.}
    \label{fig:optim-failure-modes}
\end{figure}

Another challenge in optimizing a depth-factorized model is the choice of learning rate (LR). As shown in \cref{fig:lenet300100-lrfail}, if the LR is chosen too small, the model cannot learn a sparse representation despite achieving the same generalization as a $99.4\%$ sparse model trained with large LR. This closely follows previous analyses of large LRs in neural network training dynamics: \citet{nacson2022implicit} show that large LRs help transition to a sparsity-inducing regime in diagonal linear networks. In more realistic scenarios, \citet{andriushchenko2023sgd} observe that a piece-wise constant (step decay) LR schedule with large initial LR induces phased learning dynamics including a short initial learning phase, followed by a period of sparse feature learning and loss stabilization, and sudden generalization upon reduction of the LR. Particular to symmetries and SGD, \citet{chen2024stochastic} demonstrate how large LRs help generalization by causing SGD to be attracted to symmetry-induced structures through stochastic collapse. %\\
We conjecture that in DWF, the introduction of $D$-fold artificial symmetries (cf.~\cref{def:artificial-rescaling}) accelerates this phenomenon and thus additionally aids sparse learning. The first row of \cref{fig:lrdecay-sparsities-resnet18} shows the training dynamics of deep factorized ResNets and demonstrates the requirement of large and small LR phases for DWF training.
%The left plot displays the training dynamics using our recommended cosine LR schedule, achieving high sparsity and good generalization. %If the large LR is kept constant (middle plot) only sparsification without generalization occurs. The plot on the right illustrates the requirement for both LR levels using a step decay schedule, where in the large LR phase the model becomes sparse and only suddenly generalizes after reducing the LR. 
These training dynamics are further discussed in the following section. Additionally, \cref{app:details-init-lrs} includes an ablation study on different LRs and factorization depths $D$, suggesting optimal sparsity-accuracy tradeoffs for initial LRs slightly below a critical threshold where training becomes unstable (\cref{fig:lr-grid-mnist}).



\begin{figure}[htp]
\centering
\includegraphics[width=1\textwidth]{figures/new/triple_trajectory_combined_revised.pdf}
\caption[]{Factorized ResNet-18 on CIFAR10 with $D=4$. Dashed lines indicate phase transitions. \textbf{Top}: Different LR schedules with same initial LR and $\lambda$. \textbf{Left}: cosine LR learns sparse and generalizing solutions. \textbf{Mid}: a const.\ large LR causes sparsification but no generalization. \textbf{Right}: step-decay LR displays sharply distinct sparsification and generalization phases in large and small LR phases. \textbf{Bottom}: For cosine LR, the three distinct learning phases occur at all sparsity levels, with sharper contracted dynamics for high sparsity.}
%\caption[]{Training dynamics of factorized ResNet-18 on CIFAR10 with $D=4$. Dashed lines indicate phase transitions. \textbf{Top}: need for both large and small LR phases. \textbf{Bottom}: contracting dynamics for increasing $\lambda$.}
\label{fig:lrdecay-sparsities-resnet18}
%\vspace{-0.3cm}
\end{figure}

\subsection{Learning dynamics and delayed generalization}\label{sec:learning-dynamics-1}

The learning dynamics of DWF with cosine annealing exhibit three distinct phases, characterized by changes in accuracy, sparsity, and $L_2$ norm of the collapsed weights (\cref{fig:lrdecay-sparsities-resnet18}, second row):
In an \textbf{initial phase}, SGD learns easy-to-fit patterns without overfitting while the $L_2$ norm decreases. The \textbf{reorganization phase} is characterized by temporary drops in accuracy and an increase in weight norm, hinting at a period of representational restructuring to accommodate sparsity constraints. Sparsity emerges during or at the end of this phase. The final \textbf{mixed sparsification and generalization phase} shows improvements in training and validation accuracy as sparsification continues at a decreasing rate. The mixed nature of the final phase, contrasting sharply separated sparsification and generalization with step decay, is owed to the gradual reduction in cosine annealing. Notably, with increasing regularization $\lambda$, the dynamics contract, and the phases occur in closer succession. This phased behavior shows that the more contracted the reorganization phase is, the higher compression and the more severe delayed generalization will be. This is reminiscent of the ``grokking'' phenomenon \citep{power2022grokking} shown to be tightly linked to $L_2$ regularization  \citep{liu2023omnigrok}.



\subsection{Impact of regularization and evolution of layer-wise metrics}\label{sec:learning-dynamics-sparsity}

To investigate dynamics in more detail, we analyze the effect of $D$ and $\lambda$ on the sparsity and training trajectories (\cref{fig:resnet18-cifar10-cr-acc-combined}). Similar results for different architectures/datasets are included in \cref{app:additional-architectures}. 

\begin{figure}[ht]
\centering
\includegraphics[width=1\textwidth]{figures/new/combined_cr_acc_dynamics/resnet18_cifar10/resnet18_cifar10_combined_cr_acc_dynamics_revised.pdf}
\caption[]{Impact of regularization $\lambda$ on compression (\textbf{top}), training, and validation accuracy (\textbf{bottom}) for factorized ResNet-18 and $D \in \{2,3,4\}$. For large $\lambda$ severely delayed generalization and extreme compression emerges simultaneously. Colors indicate the same $\lambda$ in both rows.}
\label{fig:resnet18-cifar10-cr-acc-combined}
%\vspace{-0.3cm}
\end{figure}

% illustrates the impact of $L_2$ regularization strength and factorization depth $D$ on the training dynamics of ResNet-18 on CIFAR10. 
As expected, increasing $\lambda$ leads to higher compression ratios across all depths. Moreover, greater $D$ enables higher compression ratios for the same $\lambda$. During the initial phase, the regularized training curves coincide with the unregularized trajectory until their departure at the onset of the reorganization phase. This departure occurs earlier the stronger the regularization. For greater factorization depths, the same $\lambda$ values induce higher sparsity at the cost of reduced generalization performance, indicating a stronger regularizing effect\footnote{Note that this does not imply worse performance in general, but a different optimal $\lambda$ for different $D$.}. The relationship between sparsity, $\lambda$, and the collapsed weight norm is further discussed in \cref{app:cr-norm-lambda}. \cref{app:layerwise-cr-norms} presents the layer-wise evolution of sparsity and weight norms, providing more detailed insights into the effects of DWF across the network topology for different architectures (e.g.,~\cref{fig:layerwise-l2-cr-dep3-resnet18}). Two key observations can be made: the first and last layers exhibit less sparsity, owed to their increased importance for the prediction. For the intermediate layers, there is a general trend toward higher compression for later layers. Secondly, the non-monotonic dynamics of the collapsed weight norm seem to be almost entirely driven by the first few and the last layers, while the intermediate layers behave homogeneously. Finally, the evolution of factor misalignment and its relation to the onset of sparsity is discussed in \cref{app:misalignment}.



\section{Performance evaluation} \label{sec:exp}

%Focus exposition of training dynamics in main text mainly to ResNet-18 trained on CIFAR10, results for other model combinations can be found in the appendix.

%In this section, we present experimental results of applying DWF to a wide range of neural network architectures and datasets. 
In this section, we evaluate the performance of DWF. %, including LeNet-300-100, LeNet-5, VGG-16, VGG-19, and ResNet-18. 
In \cref{app:additional-architectures,app:experimental-details}, we provide further results %on %WRN-16-8 and ResNet-34 
%can be found in 
%(\ref{) %Our experiments span datasets of varying complexity, from simpler classification tasks like MNIST, Fashion-MNIST, Kuzushiji-MNIST, and CIFAR10, to more challenging benchmarks such as CIFAR100, and Tiny ImageNet. 
and details on the experimental setup, including hyperparameters and training protocols.
%In \cref{sec:failure-direct-l1}, we demonstrate the limitations of direct $L_1$ regularization using standard SGD optimization.\



\subsection{Failure of vanilla \texorpdfstring{$L_1$}{L1} optimization with SGD} \label{sec:failure-direct-l1} % necessary? -- maybe yes? maybe just in the appendix?

% Adding $L_1$ regularization is effective for inducing sparse solutions in a training objective, but the resulting optimization is challenging due to the non-differentiability of the regularizer at the sparse solutions. Despite this, it is customary to add $L_1$ regularization to the training of DNNs to encourage sparsity \citet{han2015learning, wen2016structuredsparsity, scardapane2017group}, necessitating an additional post-hoc pruning step to obtain sparsity. 
The failure of SGD with vanilla $L_1$ regularization to achieve inherent sparsity has been previously observed %for linear models 
by \citet{ziyin2023spred,kolb2023smoothing}. It is natural to ask whether this limitation is merely a benign optimization artifact or if it degrades the prunability of the regularized models. %\\
We, therefore, train a LeNet-300-100 on Fashion-MNIST with vanilla $L_1$ regularization as well as with DWF and $D={2,3}$, inducing differentiable $L_1$ and non-convex $L_{2/3}$ regularization. %Each model is trained with varying regularization $\lambda$ using a logarithmically spaced grid between $10^{-6}$ and $10^{-2}$. 

\textbf{Results}\,\, The left plot in \cref{fig:lenet300100-fmnist-seeds-pruning} (page 1) shows the tradeoff between performance and inherent sparsity (before pruning) for 100 logarithmically spaced $\lambda$ values, confirming prior findings on the limitations of vanilla $L_1$ optimization. In contrast, differentiable $L_1$ regularization using DWF achieves a compression ratio of about 350 at $80\%$ test accuracy. In addition, our DWF network with $D=3$ is up to four times sparser than $D=2$ at the same accuracy, underscoring the advantages of deeper factorizations. In the right plot of \cref{fig:lenet300100-fmnist-seeds-pruning}, we subsequently apply post-hoc magnitude pruning to each of the models at increasing compression ratios (without fine-tuning) until reaching random chance performance and use the best-performing pruned model at each fixed compression ratio to obtain the pruning curves. %The procedure is repeated for three random initializations, and the mean and standard deviations are shown for each curve. 
Results indicate that differentiable sparse training with factorized networks provides better tradeoffs than vanilla $L_1$, even after accounting for the issues producing sparsity when using SGD with vanilla $L_1$. At $80\%$ test accuracy, vanilla $L_1$ plus pruning requires twice as many parameters as its DWF counterpart, and three times as many as DWF ($D=3)$. This suggests SGD with $L_1$ struggles to find similarly well-prunable structures, while DWF yields much sparser models. %, particularly for higher compression ratios.




% lenet300100_fmnist_combined_plots_modified

\subsection{Run times}\label{sec:runtimes}

The perceptive reader might be concerned about the computational overhead induced by training deep factorized networks. Our experiments show this concern to be unwarranted, as the effect of the factorization depth is rather unimportant compared to batch size for both time per sample and memory cost. %While both slightly increase with factorization depth, the differences become indiscernible for common batch sizes like 256.
\cref{app:runtimes} illustrates this for WRN-16-8 \citep{zagoruyko2016wide} and VGG-19 \citep{simonyan2014very}. For both models, the impact of factorization depth on computation time and memory usage %is most noticeable at smaller batch sizes, but 
becomes negligible as batch size increases. These findings suggest that practitioners can leverage deeper factorized networks without incurring substantial additional computational costs, particularly at typical batch sizes used in modern deep learning.


\subsection{Compression benchmark}\label{sec:benchmark}

We now evaluate DWF for factorization depths $D \in \{2,3,4\}$ against various pruning methods concerning test accuracy vs.\ compression, as well as the layer-wise allocation of the remaining weights.\\ %We compare against several methods in the pruning literature ranging from simple heuristics to more sophisticated approaches. 
\underline{Architectures and datasets}: Our experiments cover commonly used computer vision benchmarks: LeNet-300-100 and LeNet-5 \citep{lecun1998gradient} %as two simple but effective dense and convolutional models 
on MNIST, Fashion-MNIST, and Kuzushiji-MNIST, VGG-16 and VGG-19 \citep{simonyan2014very} on CIFAR10 and CIFAR100, and ResNet-18 \citep{he2016deep} on CIFAR10 and Tiny ImageNet.\\
\underline{Methods}: We compare our method against Global magnitude pruning (\textbf{GMP}) after training \citep{han2015learning}, a simple pruning method that removes the smallest weights across all layers and is surprisingly competitive, especially at low sparsities \citep{gale2019state, frankle2020pruning}; Single-shot Network Pruning (\textbf{SNIP}) \citep{lee2019snip}, a pruning-at-initialization technique, % identifying important connections based on their impact on the loss function and 
showing competitive performance %thoroughly benchmarked 
against other recent pruning methods \citep{wang2020picking}; \textbf{SynFlow} \citep{tanaka2020pruning}, considered a state-of-the-art method for high sparsity regimes; %and uses a data-free iterative procedure with exceptional performance specifically designed for maximal compression. f
\textbf{Random pruning}, serving as a naive baseline that removes weights uniformly at random; a shallow factorized network ($D=2$) which is our variant of the \textbf{spred} algorithm \citep{ziyin2023spred} with our tailored initialization.\\
\underline{Tuning}: For comparison methods, we use the established training configurations in \citet{lee2019snip, wang2020picking, frankle2020pruning} when available, and otherwise ensure comparability by using the same configuration for all methods. All models are trained with SGD and cosine learning rate annealing \citep{loshchilov2022sgdr}. 
For our method, \textbf{no} post-hoc pruning or fine-tuning is required and all layers are regularized equally. Further details are given in \cref{app:experimental-details}.

\begin{figure}[ht]
\centering
\includegraphics[width=0.95\textwidth]{figures/new/combined_plot_lenets_median-minmed.pdf}
\caption[]{Accuracy vs.\ sparsity tradeoffs for LeNet architectures on MNIST and replacements of varying difficulty. Lines depict median test accuracies and shaded areas the minimum over three random initializations. %The dotted line shows $\varepsilon=5\%$ tolerance.
}
\label{fig:lenet-performance-grid-errorbars}
%\vspace{-0.3cm}
\end{figure}

\paragraph{Results} \cref{fig:lenet-performance-grid-errorbars} shows results for the fully-connected and convolutional LeNet-300-100 and LeNet-5 architectures on MNIST, F-MNIST, and K-MNIST.
Across all datasets and models, our proposed DWF consistently outperforms existing pruning techniques, particularly at higher compression ratios. For LeNet-300-100 on MNIST, DWF with $D=3,4$ remains within $5\%$ of the dense performance even above a compression ratio of $500$, significantly surpassing other methods. On F-MNIST and K-MNIST, a similar performance gain is observed. LeNet-5 exhibits similar trends, with DWF maintaining high accuracy at compression ratios where other techniques, especially random pruning and SNIP, have collapsed. Notably, DWF sustains performance up to a compression ratio of $100$ on K-MNIST, while competitors rapidly decline. SynFlow and GMP generally outperform random pruning and SNIP, but still fall short of DWF. When comparing shallow and deep factorizations, we see that $D>2$ retains performance and delays model collapse much longer than $D=2$. The results demonstrate that DWF offers substantial gains in compression capability. Further, the clear separation between DWF and other methods in the high-sparsity regime indicates that our approach captures aspects of the model's representational power that are missed by the other techniques. These findings underscore the potential of DWF, particularly under severe parameter constraints.

For larger architectures and more complex datasets (\cref{fig:cnn-performance-grid}), DWF continues to demonstrate superior performance, % over existing pruning methods, 
albeit less pronounced. %For ResNet-18 and VGG-16 on CIFAR10, as well as VGG-19 on CIFAR100, DWF consistently maintains the highest accuracy. %However, the performance gap narrows, particularly in the medium sparsity regime. 
% For ResNet-18 on Tiny ImageNet, magnitude pruning initially outperforms DWF, though DWF regains the lead as sparsity increases. 
%Across these larger models, 
We observe that the $D=2$ factorization excels in the medium sparsity regime below a compression ratio of $100$, while $D>2$ shows enhanced resilience to performance degradation and delayed model collapse at more extreme sparsity levels.

\begin{figure}[ht]
\centering
\includegraphics[width=0.635\textwidth]{figures/new/combined_plot_cnn.pdf}
\caption[]{Accuracy vs.\ sparsity for larger ResNet and VGG architectures on CIFAR and Tiny ImageNet.}
\label{fig:cnn-performance-grid}
%\vspace{-0.3cm}
\end{figure}

\cref{tab:interpolated_cr_by_method} %provides a summary of the compression capabilities demonstrated in the previous plots, 
showcases the sparsest models achieved by each method while maintaining performance within $5\%$ or $10\%$ of the dense model accuracy. These tolerance levels are suitable for testing the medium to high sparsity regimes we focus on. Other levels can be read from \cref{fig:cnn-performance-grid}. Of the $10$ presented scenarios, DWF with $D=3,4$ achieves the highest compression in $9$ cases, demonstrating the robustness of DWF in preserving model performance under extreme sparsity requirements. The best-ranked DWF model achieves $2$ to $5$ times the compression ratio of the best pruning method, and surpasses shallow factorization in all but one setting, albeit with smaller improvements ranging from $8\%$ to $298\%$. For example, DWF with $D=3$ reaches a compression ratio of $1014$ ($1456$) for VGG-19 on CIFAR100 (ResNet-18 on CIFAR10) at $10\%$ tolerance, improving by $8\%$ ($25\%$) over $D=2$ and by $381\%$ ($102\%$) over the best pruning method SynFlow.

%compared to $1169$ ($939$) for $D=2$ and $721$ ($211$) for the best pruning method SynFlow. %For VGG-19 on CIFAR100 at $10\%$ tolerance, DWF ($D=3$) achieves a compression ratio of $1014$, surpassing the runner-up pruning method (SynFlow at $218$) by a factor of almost $5$.


%%%% include table of sparsest models performing approximately at dense accuracy

% Depth=2 (spred) was shown in their paper to outperform state-of-the-art sparse training approaches like Soft-Threshold Reparameterization \citep{kusupati2020}


\input{tables/sparsest_model_table.tex}

\paragraph{Allocation of layer-wise sparsity} Finally, we investigate the reasons for model collapse in SNIP and GMP in the high sparsity regime by plotting the layer-wise remaining ratio (1/CR) for ResNet-18 and VGG-16 on CIFAR10 in the medium and extreme compression regimes, as shown in \cref{fig:layerwise-remaining-grid}. At high compression, for both ResNet-18 and VGG-16, we observe that GMP and SNIP catastrophically prune entire layers. In contrast, SynFlow and DWF automatically learn adaptive layer-wise sparsity budgets which helps in avoiding such issues. While SynFlow does prune some layers entirely in ResNet-18, these correspond to skip connections that can be removed without interrupting the synaptic flow. Comparing SynFlow and DWF, we observe that DWF produces higher sparsity in the first and last layers across all settings. This is a particularly desirable property, as it leads to greater computational savings for a given overall sparsity level. Moreover, as opposed to removing the skip connections, DWF allocates less sparsity to these structures, suggesting a qualitatively distinct underlying structure optimization mechanism.

\begin{figure}[ht]
\centering
\includegraphics[width=0.635\textwidth]{figures/new/layerwise_remaining_ratio_combined.pdf}
\caption{Allocation of layer-wise sparsity for different methods. SNIP and GMP show catastrophic pruning of whole layers (collapse) for high sparsities, whereas DWF, like SynFlow, finds adaptive sparsity allocations.}
\label{fig:layerwise-remaining-grid}
%\vspace{-0.3cm}
\end{figure}


%\include{tables/lenets_mnists_results}

\section{Conclusion}\label{sec:discussion}

This paper introduces deep weight factorization (DWF), an extension of a previously proposed differentiable $L_1$ regularization to induce sparsity in general neural networks. By factorizing weights not only into two, but $D\geq 3$ parts, our method provably induces differentiable $L_{2/D}$ regularization that can be incorporated in any neural network. We identify practical training obstacles and propose tailored optimization strategies such as a depth-specific initialization and a sparsity-promoting learning rate scheme. We also characterize three distinct phases that describe the learning dynamics and (delayed) generalization behavior of DWF. Experiments demonstrate that DWF is usually superior to shallow factorization and outperforms dominant pruning techniques.

\paragraph{Limitations and future work} In this work, we primarily focused on $D\in\{2,3,4\}$. While not incurring significant computational overhead (cf.~\cref{fig:runtime-persample-2}), we found that increasing the factorization depth beyond four did not yield further sparsity improvements and introduced optimization challenges (cf.~\cref{fig:moredepth-ablation-depth-mnist-lenet300100}). Additionally, a limitation of our work is that we exclusively focused on DWF approaches resulting in unstructured sparsity regularization. Hence, an interesting potential direction for future research is to extend our factorization to structured sparsity problems. %methods, promising greater computational benefits in practical deployment scenarios.

\subsubsection*{Acknowledgments}
DR’s research is funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) – 548823575.

\bibliography{iclr2025_conference}
\bibliographystyle{iclr2025_conference}

\clearpage

\appendix
% \section{Appendix}
\appendixpage

\startcontents[sections]
\printcontents[sections]{l}{1}{\setcounter{tocdepth}{2}}

\section{Further related literature}\label{app:literature}

\paragraph{Convex and Non-convex sparse regularization}
Convex and non-convex regularization for sparsity-inducing effects has a long history as it is well-understood with strong theoretical underpinnings. Notable works in this direction include
\citet{tibshirani1996regression,fan2001variable, meinshausen2006high, zhang2008sparsity} for convex norm-based sparse regularization and 
\citet{friedman2010regularization,fan2001variable, zhang2010nearly, xu2010l1} studying its non-convex extensions. While mathematically rigorous, they see limited application in deep learning due to their non-differentiability exactly where sparsity is achieved, requiring the implementation of non-smooth optimization routines that are often inflexible and do not scale well. While some works exist on proximal-type optimization routines in deep learning \citep{yang2020proxsgd, deleu2021structured}, their popularity remains far behind pruning and other approaches, with these methods being rarely applied, used as comparisons, or given much attention in surveys \citep{hoefler2021sparsity,gale2019state}. Nevertheless, the use of $L_1$ or structured $L_{2,1}$ regularization is ubiquitous in sparse deep learning \citep{han2015learning, he2017channel, liu2017learning, li2022pruning}. These sparsity regularization pruning methods \citep{cheng2024survey} use sparse regularization to shrink weights before applying a subsequent pruning step for actual sparsity. However, the conceptual framework underlying these heuristic-based methods remains poorly understood. %While an SGD-based solution, as the proposed DWF method, also requires analyzing the learning behavior of factorized networks, this is often much less cumbersome and---as shown in our experiments---can yield on-par performance compared to standard optimization routines while not being accompanied by a notable increase in run time.

\paragraph{Weight factorization without and with explicit regularization}

Weight factorizations, also studied under the names diagonal linear networks \citep{woodworth2020kernel}, redundant parameterization \citep{ziyin2023spred}, or Hadamard product parameterization \citep{hoff2017lasso,tibs2021,kolb2023smoothing} in various contexts, can be traced back to \citet{grandvalet1998least} and was rediscovered both in statistics \citep{hoff2017lasso} and machine learning \cite{neyshabur2015search}. Later,  \citet{tibs2021,kolb2023smoothing} and \citet{ziyin2023spred} used this approach to induce sparsity via $L_2$ regularization. Further works using weight factorization from the field of optimization include \citet{poon2021smooth,poon2023smooth}. \citet{ouyang2025kurdyka} show that the Kurdyka-\L ojasiewicz exponent at a second-order stationary point of a factorized and $L_2$ regularized objective can be inferred from its corresponding $L_1$ penalized counterpart. These works are closest to ours in spirit, namely, factorizing the weights of existing problems to achieve sparsity in the original weight space under $L_2$ regularization. None of these works, however, studied deeper factorizations of neural network parameters. A closely related approach to incorporate the induced non-convex $L_q$ regularization ($q<1$) into DNN training was recently proposed by \citet{outmezguine2024decoupled}, who base their method on the $\bm{\eta}$-trick \citep{bach2012optimization} instead of the $L_2$ regularized weight factorization we study. This method re-parametrizes the regularization function instead of factorizing the network parameters, utilizing a different variational formulation of the $L_q$ quasi-norm. Despite having the same effective $L_q$ regularization, DWF incorporates additional symmetry-induced sparsity-promoting effects through weight factorization and the stochastic collapse phenomenon \citep{ziyin2023symmetry, chen2024stochastic}. Another branch of literature studies the \textit{representation cost} of DNN architectures $f_{\w}$ for a specific function $f$, defined as $R(f)=\min _{\w: f_{\w}=f}\|\w\|_2^2$, showing that the $L_2$ cost of representing a linear function using a diagonal linear network yields the $L_{2/D}$ quasi-norm \citep{dai2021representation, jacot2023implicit}.

Apart from the previous works using explicit regularization, the implicit regularization effect of weight factorization was studied by various researchers, both in statistics and deep learning, including 
\citet{neyshabur2015search,gunasekar2018implicit,gissin2019implicit,vaskevicius2019implicit,woodworth2020kernel,pesme2021implicit,zhao2022high,li2021implicit}. However, such approaches are usually impractical for real applications, requiring vanishing initializations or specific loss functions. For example, the implicit bias of deep weight factorizations does not extend to non-convex regularizers under the squared loss~\citep{nacson2022implicit}.
Powerpropagation \citep{schwarz2021powerpropagation} is a different sparsity-inducing weight transformation with implicit regularization effects, proposed as a modular tool designed for use in practical applications.
% 
% 
%Some works transferring these toy model principles to DL architectures factorize layers with or without $L_2$ regularization, either for acceleration or implicit/explicit regularization effects.
% 
%\citet{tibs2021} also noted that certain sparse linear regression problems correspond to optimizing neural networks with special structures, such as diagonal linear networks, under weight decay.
% 
% \paragraph{Implicit acceleration by adding factors}

Various papers have also studied the factorization of weights without explicit regularization in the context of implicit acceleration caused by the factors acting as adaptive learning rates. Notable examples include \citet{arora2019implicit,wang2022random,li2024improving}.

\paragraph{Pruning in neural networks} 
The landscape of pruning and sparse training methods is confusing due to the plethora of complex sparsification pipelines, incorporating numerous techniques at the same time, and an enormous amount of hyperparameters like pruning schedules or learning rates at different stages. A fair comparison is further complicated by the lack of established, streamlined evaluation processes and opposing objectives in some methods. Pruning, arguably the most popular and widespread method, can be used in innumerable ways to sparsify neural networks, which makes comparisons among these particularly difficult \citep{wang2023state}. Existing pruning techniques use pruning at initialization \citep{lee2019snip,wang2020picking,tanaka2020pruning}, pruning after training by, e.g., magnitude pruning \citep{han2015learning,gale2019state}, pruning during training, iterative pruning \citep{frankle2020pruning}, or pruning and re-growth \citep{evci2020rigging}. Recent surveys can be found in \citet{blalock2020state,hoefler2021sparsity, cheng2024survey}. Prominent examples include the lottery ticket hypothesis \citep{frankle2018lottery}, proposing that many networks contain equally performant, but much smaller subnetworks that can be found at initialization. A Bayesian pruning version is suggested in \citet{dhahrishaving}. Another approach related to pruning is soft thresholding reparameterization \citep{kusupati2020soft}, a sparse training method incorporating a soft-thresholding step into its network. Despite its success, however, it was shown to be outperformed by the differentiable $L_1$ approach, i.e., DWF with $D=2$ \citep{ziyin2023spred}. Recently, \citet{zhang2024how} established theoretical bounds on network prunability using convex geometry, showing that the fundamental one-shot pruning limit without sacrificing performance is determined by weight magnitudes and the sharpness of the loss landscape, providing a unified framework to help explain the effectiveness of magnitude-based pruning. They empirically show that $L_1$ regularized post-hoc magnitude pruning approximately matches the derived pruning limit before performance degrades significantly. 

\paragraph{Sparsity-inducing regularization in neural networks}
Apart from pruning, the application of norm-based regularizers is also common in deep learning \citep{scardapane2017group, wen2016structuredsparsity,han2015learning,bui2021improving}. This includes $L_0$-type reguarlization methods \citep{louizos2018learning, zhou2021effective,savarese2020winning}. However, some of these, such as \citet{louizos2018learning}, were found to not work well due to the stochastic sampling in their training procedure. Other approaches include adaptive regularization \citep{glandorf2023hypersparse} or dynamic masking \citep{liu2020dynamic}.

\paragraph{Sparsity based on structural constraints}

While we focus on unstructured sparsity in this paper, various approaches for structured pruning were proposed, including \citet{wen2016structuredsparsity,li2022pruning,bui2021improving,liu2017learning}. For a recent survey, see
\citet{he2023structured}.
A link also made in our paper is the connection between sparsity and symmetries. Using structures of symmetries can guide the sparsification of neural networks. Papers studying this link include the works of \citet{kunin2020neural,simsek2021geometry,le2022training}, but also some various recent work such 
\citet{ziyin2023symmetry, ziyin2023probabilistic,chen2024stochastic}.

\paragraph{Matrix factorization related induced regularization}
In \citet{srebro2004mmmf, mazumder2010spectral, shang2020unified, hastie2015matrix} different matrix factorization regularization schemes are proposed to, e.g., learn incomplete matrices \citep{mazumder2010spectral,hastie2015matrix} or for better generalization \citep{srebro2004mmmf}.
%
%
%\paragraph{Other factorizations in neural network architectures}
There are also neural architectures implementing flavors of matrix factorization to achieve better performance or acceleration
\citep{guo2020expandnets,jing2020implicit, bhardwaj2022collapsible}. Although not directly related to our factorization, we will briefly explain their idea to contrast it with our approach. For example, \citet{guo2020expandnets} note a beneficial effect of applying $L_2$ regularization on the (matrix) factors in the form of $\Vert \bm{W}_1 \Vert_2^2 + \Vert \bm{W}_2 \Vert_2^2$ as opposed to the $L_2$ regularizer $\Vert \bm{W}_1 \bm{W}_2 \Vert_2^2$ proposed in \citet{arora2019implicit}. This observation can be explained by the low-rank bias induced on the product matrix, whereas the second approach is simple $L_2$ regularization on the product.

\section{Intuition for sparsity via $L_2$ regularized weight factorization}\label{app:intuition}

Deep Weight Factorization introduces overparameterization by decomposing each original weight $\mathrm{w}$ multiplicatively into $D \geq 2$ factors $\omega_1,\ldots,\omega_D$. Without additional $L_2$ regularization, this induces artificial rescaling symmetries (\cref{def:artificial-rescaling}), resulting in infinitely many possible factorizations for each weight, all producing the same collapsed network and thus leaving the loss function unchanged. However, when $L_2$ regularization is applied, it influences the choice among these factorizations by preferring those with minimal Euclidean norm. With $L_2$ regularization, only minimum-norm (balanced) factorizations can be optimal, as otherwise, we could always decrease the $L_2$ penalty by picking a more balanced factorization while leaving the unregularized loss unchanged (cf.~\cref{lemma:min-l2-penalty}).

To provide some geometric intuition using a more concrete example, consider the simplest case of factorizing a scalar weight $\mathrm{w} \in \mathbb{R}$ into two factors, $\mathrm{w} = \omega_1 \cdot \omega_2$, as illustrated in \cref{fig:rescaling-symmetry} for $\mathrm{w} \in \{0, 0.25\}$. The set of all possible factorizations $\{(\omega_1, \omega_2) \in \mathbb{R}^2: \omega_1 \omega_2 = \mathrm{w}\}$ is given by the points on the coordinate axes for $\mathrm{w} = 0$ and forms a rectangular hyperbola in the $(\omega_1, \omega_2)$ plane for non-zero $\mathrm{w}$ (cf. \cref{fig:rescaling-symmetry}). Among these, the factorizations with minimal $L_2$ norm (i.e., minimal distance to the origin) are located at the vertices of the hyperbola. These minimum-norm factorizations are balanced, meaning the factors are equal in magnitude, as the vertices of a rectangular hyperbola always lie either on the diagonal $\omega_2=\omega_1$ or $\omega_2=-\omega_1$. Specifically, the two vertices of the resulting hyperbola are given by $(\sqrt{|\mathrm{w}|}, \sqrt{|\mathrm{w}}|)$ and $(-\sqrt{|\mathrm{w}|}, -\sqrt{|\mathrm{w}|})$ for positive $\mathrm{w}$, and $(\sqrt{|\mathrm{w}|}, -\sqrt{|\mathrm{w}|})$ and $(-\sqrt{|\mathrm{w}|}, \sqrt{|\mathrm{w}|})$ for negative $\mathrm{w}$. Combined with the case $\mathrm{w}=0$, the minimum-norm factorizations $(\omega_1^{\ast},\omega_2^{\ast})$ for any $\mathrm{w}$ are obtained as

\begin{equation}
(\omega_1^{\ast},\omega_2^{\ast})= \begin{cases}\left(\sqrt{\left|\mathrm{w}\right|}, \sqrt{\left|\mathrm{w}\right|}\right) \text { or }\left(-\sqrt{\left|\mathrm{w}\right|},-\sqrt{\left|\mathrm{w}\right|}\right) & \text {, } \mathrm{w}>0 \\ (0,0) & \text {, } \mathrm{w}=0 \\ \left(\sqrt{\left|\mathrm{w}\right|},-\sqrt{\left|\mathrm{w}\right|}\right) \text { or }\left(-\sqrt{\left|\mathrm{w}\right|}, \sqrt{\left|\mathrm{w}\right|}\right) & \text {, } \mathrm{w}<0 .\end{cases}
\end{equation}

At these points, the $L_2$ penalty evaluates to $2|\mathrm{w}|$, effectively turning into an $L_1$ penalty on the collapsed weight $\mathrm{w}$ scaled by a factor of 2.

For deeper factorizations involving more than two factors the same line of reasoning applies, but visualizing the set of possible factorizations as in \cref{fig:rescaling-symmetry} for $D=2$ becomes challenging. The minimum $L_2$ penalty at balanced factorizations reduces to a non-convex sparsity-inducing $L_{2/D}$ penalty on the collapsed weight. This serves as a lower bound of the $L_2$ penalty for every fixed value of $\mathrm{w}$. Once the factors reach this balanced state, which is an "absorbing state" under (S)GD, the optimization process locks in this configuration for all future iterations by symmetry (cf. \cref{lemma:balancedness}). Thus, the combination of DWF and $L_2$ regularization induces sparsity in the collapsed weights by promoting balanced factorizations, at which the $L_2$ penalty reduces to a lower-degree quasi-norm penalty on $\mathrm{w}$.

\clearpage

\section{Further results and missing proofs}\label{app:proofs-definitions}


\subsection{Proof of \cref{lemma:min-l2-penalty}}

\begin{proof}
Let $\bomega = (\bomega_1, \ldots, \bomega_D) \in \mathbb{R}^{Dp}$ be a local minimizer of $\mathcal{L}_{\bomega,\lambda}(\bomega)$.
As the factorization is applied independently to each parameter, it suffices to treat the scalar case: We will prove that $|\omega_{j,1}| = \ldots = |\omega_{j,D}|$ for all $j \in [p]$.

The rescaling symmetries of DWF ensures that $\mathcal{L}_{\bomega,0}$ (the factorized loss without regularization) is constant over all possible factorizations of a collapsed parameter $\bomegae$. However, the $L_2$ regularization term $\lambda D^{-1} \sum_{d=1}^D \|\bomega_d\|_2^2$ enforces a preference for min-norm factorization. For each scalar weight indexed by $j \in [p]$, consider its factors $\omega_{j,1}, \ldots, \omega_{j,D}$. Applying the AM-GM inequality to the $L_2$ penalty of the DWF loss yields

\begin{equation}
D^{-1} \textstyle\sum_{d=1}^D \omega_{j,d}^2 \geq \left(\textstyle\prod_{d=1}^D (\omega_{j,d})^2\right)^{1/D} = |\omega_{j,1} \cdots \omega_{j,D}|^{2/D} = |\omegae_j|^{2/D} \quad \forall \, j \in [p]
\end{equation}

This shows the balancedness requirement for the minimizers of $\Lomega(\bomega)$, as the AM-GM inequality holds tight if and only if all terms are equal, i.e., $|\omega_{j,1}| = \ldots = |\omega_{j,D}|$.

Summing over the factorizations of all weights yields the non-convex $L_{2/D}$ regularizer $\Vert \bomegae \Vert_{2/D}^{2/D}$ as the minimum $L_2$ penalty for a given collapsed weight $\bomegae \in \mathbb{R}^p$.
\end{proof}

\subsection{Proof of \cref{lemma:init-factorized-networks}}

\begin{definition}[Standard Weight Initialization]
\label{def:std_init}
A standard weight initialization scheme for a neural network layer with $\text{n}_{\text{in}}$ input units and $\text{n}_{\text{out}}$ output units is a probability distribution with mean 0 and variance $\sigma^2$, where $\sigma^2 = \frac{c g^2}{\text{n}_{\text{mode}}}$. Here, $g$ is a gain factor depending on the activation function, $c$ is a constant, and $\text{n}_{\text{mode}}$ is either $\text{n}_{\text{in}}$, $\text{n}_{\text{out}}$ or their sum. Common examples include the Kaiming ($\sigma^2 = \frac{2}{\text{n}_{\text{in}}}$) \citep{he2015delving}, Glorot ($\sigma^2 = \frac{2}{\text{n}_{\text{in}} + \text{n}_{\text{out}}}$) \citep{glorot2010understanding}, or LeCun initialization ($\sigma^2 = \frac{1}{\text{n}_{\text{in}}}$) \citep{lecun2002efficient}.
\end{definition}

\begin{proof} \label{proof:init-factorized-networks}
Recall that using a standard initialization (cf.~\cref{def:std_init}), each factor is initialized as $\omega_{j,d}^{(l)} \sim \mathcal{N}(0,\sigma_l^2)$, where $\sigma_l^2 = 1/n_{in}^{(l)}<1$ in the case of LeCun initialization \citep{lecun2002efficient}. For clarity, we assume the width $n_{in}^{(l)}$ to be constant across layers $l \in [L]$.\\
To prove the first statement, we note that $\mathbb{E}[\omegae_j^{(l)}] = 0$ and $\text{Var}\left(\omegae_j^{(l)}\right) = \prod_{d=1}^D \text{Var}\left(\omega_{j,d}^{(l)}\right) = \sigma^{2D}$. Applying Chebyshev's inequality, we get for any $\varepsilon>0$

\begin{equation}
\mathbb{P}\big(\big|\omegae_j^{(l)} - \mathbb{E}\big[\omegae_j^{(l)}\big]\big| \geq \varepsilon\big) = \mathbb{P}(|\omegae_j^{(l)}| \geq \varepsilon)  \leq \frac{\text{Var}\big(\omegae_j^{(l)}\big)}{\varepsilon^2} = \frac{\sigma^{2D}}{\varepsilon^2}\,.
\end{equation}

Finally, we have $0 \leq \lim_{D \to \infty} \mathbb{P}(|\omegae_j^{(l)}| \geq \varepsilon) \leq \lim_{D \to \infty} \frac{\sigma^{2D}}{\varepsilon^2} = 0$, and thus, by the squeeze theorem:
$\lim_{D \to \infty} \mathbb{P}(|\omegae_j^{(l)}| \geq \varepsilon) = 0$. This shows that $\omegae_j^{(l)} \overset{p}{\longrightarrow} 0$ as $D \to \infty$.\\

%%%%%%%%%%%%%

For the second point, we denote the pre-activation of neuron $k$ in layer $l$ as

\begin{equation}
  y_k^{(l)} = \sum_{i=1}^{n_{\text{in}}} \mathrm{w}_{k_i}^{(l)} \phi\left(y_i^{(l-1)}\right) = \sum_{i=1}^{n_{\text{in}}} \left( \prod_{d=1}^{D} \omega_{k_i,d}^{(l)} \right) \phi\left(y_i^{(l-1)}\right),
\end{equation}

where $\omega_{k_i,d}^{(l)}$ is the $d$-th scalar factor of the weight $\mathrm{w}_{k_i}^{(l)}$ associated with input $i$ of neuron $k$ in layer $l$. The activation $\phi\left(y_i^{(l-1)}\right)$ is the activation function $\phi$ applied to the pre-activations from layer $l-1$. To simplify calculations, we assume that the activation function is approximately linear around the origin, implying $\text{Var}\left(\phi\left(y_i^{(l-1)}\right)\right) \approx \text{Var}\left(y_i^{(l-1)}\right)$ and allowing us to ignore the gain factor, as valid for, e.g., $\text{tanh}$ activation. Using that the factors $\omega_{k_i,d}^{(l)}$ and activations $\phi\left(y_i^{(l-1)}\right)$ are independent and identically distributed, respectively, the variance of $y_k^{(l)}$ is given by:

\begin{equation}
  \text{Var}\left(y_k^{(l)}\right) = \sum_{i=1}^{n_{\text{in}}} \text{Var}\left( \prod_{d=1}^{D} \omega_{k_i,d}^{(l)} \cdot \phi\left(y_i^{(l-1)}\right) \right)   = \sum_{i=1}^{n_{\text{in}}} \text{Var}\left(\phi\left(y_i^{(l-1)}\right)\right) \cdot \prod_{d=1}^{D} \text{Var}\big(\omega_{k_i,d}^{(l)}\big).
\end{equation}


Since the factors are initialized with $\text{Var}\left(\omega_{k_i,d}^{(l)}\right) = \sigma_l^2 = \frac{1}{n_{\text{in}}}$, the variance of $y_k^{(l)}$ is:

\begin{equation}\label{eq:layer-variance-recursive}
\text{Var}\left(y_k^{(l)}\right) = \sum_{i=1}^{n_{\text{in}}} \text{Var}\left(y_i^{(l-1)}\right)
\left( \frac{1}{n_{\text{in}}} \right)^D
%\sigma_l^{2D}
 =  n_{\text{in}} \cdot \frac{\text{Var}\left(y^{(l-1)}\right)}{n_{\text{in}}^D} = \frac{\text{Var}\left(y^{(l-1)}\right)}{n_{\text{in}}^{D-1}}
\end{equation}

In non-factorized layers, the $n_{\text{in}}$ in \cref{eq:layer-variance-recursive} cancel out, resulting in equal activation variances across layers. In contrast, standard initializations in factorized layers do not account for the exponent $D$ appearing in the variance of the collapsed weight $\omegae_{k_i}^{(l)}$, and thus result in a variance reduction in each subsequent layer as a function of input units and factorization depth $D$. Applying the above relationship recursively, we see that the variance at layer $L$ is

\begin{equation}
\text{Var}(y_k^{(L)}) %= \text{Var}(y^{(1)}) \prod_{l=2}^{L} 
= \text{Var}(y^{(1)}) \cdot \left( 
\frac{1}{n_{\text{in}}} \right)^{(D-1)(L-1)}
\end{equation}
To avoid reducing or amplifying the magnitudes of input signals exponentially, a proper initialization requires $\text{Var}(y_k^{(L)})$ to equal some constant, typically set to unity \citep{he2015delving}. In factorized networks with $D \geq 2$, however, standard initialization causes strong dependence on $n_{\text{in}}$, $D$, and $L$. %Proper initialization must account for the cumulative decay in variance across layers to avoid vanishing activation variances.

\end{proof}



\subsection{Proof of \cref{thm:equi}}

Before proving the theorem, we introduce some required notation. We define the inverse factorization function as $\mathcal{K}: \mathbb{R}^{Dp} \to \mathbb{R}^p,\, \bomega \mapsto \bomega_1 \odot \ldots \odot \bomega_D = \bomegae$, and remark that it is a smooth surjection. Using $\mathcal{K}$, we can relate both objectives using the factor misalignment $M(\bomega)=D^{-1} \sum_{d=1}^D \Vert \bomega_d \Vert_2^2 - \Vert \bomegae \Vert_{2/D}^{2/D}$. Using \cref{lemma:min-l2-penalty}, the DWF objective $\Lomega(\bomega)$ can be expressed as $\Lomega(\bomega)=\Lw(\mathcal{K}(\bomega)) + \lambda M(\bomega)$, where the misalignment $M(\bomega)\geq0$ attains zero if and only if $\bomega$ represents a balanced factorization. Further, let $B(\w,\varepsilon)$ denote an open ball with radius $\varepsilon$ around $\w \in \mathbb{R}^p$ and recall that $\mathcal{K}$ is continuous at $\bomega$ if $\forall\,\varepsilon>0\,\exists\,\delta>0:\,\mathcal{K}(B(\bomega, \delta)) \subseteq B(\mathcal{K}(\bomega), \varepsilon)$.

\begin{proof}

First, we show that if $\hat{\w} \in \argmin_{\w \in \mathbb{R}^p} \Lw(\w)$, then $\exists\, \hat{\bomega} \in \argmin_{\bomega \in \mathbb{R}^{Dp}} \Lomega(\bomega)$ such that $\mathcal{K}(\hat{\bomega})=\hat{\w}$ and $\Lw(\hat{\w})=\Lomega(\hat{\bomega})$.\\
Since $\hat{\w}$ is a local minimizer of $\Lw(\w)$, $\exists \, \varepsilon_0>0:\, \forall \, \w' \in B(\hat{\w},\varepsilon_0):\,\Lw(\hat{\w}) \leq \Lw(\w')$. By surjectivity and the multiplicative structure of $\mathcal{K}$, we can pick a balanced factorization $\hat{\bomega}$ of $\hat{\w}$ so that $\mathcal{K}(\hat{\bomega})=\hat{\w}$ and $M(\hat{\bomega})=0$. Balanced factorizations are unique up to sign flip permutations that leave the product sign invariant. Therefore $\Lw(\hat{\w})=\Lw(\mathcal{K}(\hat{\bomega}))=\Lomega(\hat{\bomega})$. By continuity of $\mathcal{K}$, $\exists \, \delta_0:\,\mathcal{K}(B(\hat{\bomega},\delta_0)) \subseteq B(\mathcal{K}(\hat{\bomega}),\varepsilon_0) = B(\hat{\w},\varepsilon_0)$, i.e., all $\bomega' \in B(\hat{\bomega},\delta_0)$ map to some $\w' \in B(\hat{\w},\varepsilon_0)$. Then we obtain the following chain of inequalities

\begin{equation}\nonumber
    \forall \bomega' \in B(\hat{\bomega},\delta_0): \Lomega(\hat{\bomega}) = \Lw(\hat{\w}) \leq \Lw(\underbrace{\mathcal{K}(\bomega')}_{\w'}) \leq \Lw(\mathcal{K}(\bomega'))+\underbrace{M(\bomega')}_{\geq 0} = \Lomega(\bomega'),
\end{equation}

where the first equality holds because $M(\hat{\bomega})=0$ and the subsequent inequality because $\hat{\w}$ is a local minimizer of $\Lw(\w)$. This shows that $\hat{\bomega} \in \argmin_{\bomega \in \mathbb{R}^{Dp}} \Lomega(\bomega)$ with $\Lw(\hat{\w})=\Lomega(\hat{\bomega})$.\\

For the other direction, assume that $\hat{\bomega} \in \argmin_{\bomega \in \mathbb{R}^{Dp}} \Lomega(\bomega)$, i.e., $\exists \, \varepsilon_0>0:\,\forall\,\bomega' \in B(\hat{\bomega},\varepsilon_0):\, \Lomega(\hat{\bomega}) \leq \Lomega(\bomega')$. By \cref{lemma:min-l2-penalty}, $M(\hat{\bomega})=0$ and therefore $\Lomega(\hat{\bomega})=\Lw(\mathcal{K}(\hat{\bomega}))+M(\hat{\bomega})=\Lw(\hat{\w})$. We prove that $\hat{\w}=\mathcal{K}(\hat{\bomega})$ is a local minimizer of $\Lw(\w)$ by contradiction. Assume $\hat{\w}$ is not a local minimizer of $\Lw(\w)$, then $\forall\,\delta>0:\exists\,\w' \in B(\hat{\w},\delta): \Lw(\w') < \Lw(\hat{\w})$. However, if $\hat{\bomega}$ is a balanced factorization of $\hat{\w}$, the following auxiliary result shows that for a perturbed $\w'$ around $\hat{\w}$, there must also be a balanced factorization $\bomega'$ of $\w'$ close to $\hat{\bomega}$:
\begin{lemma}\label{lemma:aux-lemma-thm}
Let $\hat{\bomega} \in \mathbb{R}^{Dp}$ so that $M(\hat{\bomega})=0$ and let $\mathcal{K}(\hat{\bomega})=\hat{\w}$. Then $\forall\, \varepsilon>0\,\exists \delta>0:\, \w' \in B(\hat{\w},\delta) \implies \exists \bomega' \in B(\hat{\bomega},\varepsilon):\, \mathcal{K}(\bomega')=\w'$ and $M(\bomega')=0$.
\end{lemma}
\begin{proof}
    %We consider a factorization $\hat{\bomega}_j=(\hat{\omega}_{j,1},\ldots,\hat{\omega}_{j,D})$ of the scalar $\hat{w}_j$, from which the result directly follows for the vector-valued case, as all $w_j,\,j \in [p]$, are factorized independently. Since $M(\hat{\bomega}_j)=0$, \cref{lemma:min-l2-penalty} requires $|\hat{\omega}_{j,1}|=\ldots=|\hat{\omega}_{j,D}|=|\hat{w}_j|^{1/D}$. As $w_j \mapsto |\omega_{j,d}|=|w_j|^{1/D}$ is continuous,  $w_j \mapsto \big(|\omega_{j,1}|,\ldots,|\omega_{j,D}|\big)=\big(|w_j|^{1/D},\ldots,|w_j|^{1/D}\big)$ is also continuous, which means: $\forall \varepsilon>0 \exists \delta>0$: $w_j' \in B(\hat{w}_j,\delta) \implies (|\omega_{j,1}'|,\ldots,|\omega_{j,D}'|)=(|w_j'|^{1/D},\ldots,|w_j'|^{1/D}) \in B\big((|\hat{w}_j|^{1/D},\ldots,|\hat{w}_j|^{1/D}),\varepsilon\big)$. 
    
    %Since changing the signs of the factors $\omega_{j,d}'$ does not affect their absolute values or the Euclidean norm of $\bomega_j'$, and we can ensure $\mathcal{K}_j(\bomega_j')=\prod_{d=1}^D \omega_{j,d}'=w_j'$ by choosing appropriate sign patterns for $\{\omega_{j,d}'\}_{d=1}^D$, we can always select $\bomega_j'$ in $B(\hat{\bomega}_j,\varepsilon)$ mapping to $w_j' \in B(\hat{w}_j,\delta)$ with $M(\bomega_j)=0$. It follows $\forall \varepsilon>0 \exists \delta>0: w_j' \in B(\hat{w}_j,\delta) \Longrightarrow \exists \bomega_j' \in B(\hat{\bomega}_j,\varepsilon): \mathcal{K}_j(\bomega_j')=w_j'$ and $M(\bomega_j')=0$.
We first consider the scalar case of a balanced factorization $\hat{\bomega}_j = (\hat{\omega}_{j,1}, \ldots, \hat{\omega}_{j,D})$ mapping to $\hat{\mathrm{w}}_j \in \mathbb{R}$, i.e., $M(\hat{\bomega}_j) = 0$ and $\mathcal{K}_j(\hat{\bomega}_j) = \hat{\mathrm{w}}_j$. By \cref{lemma:min-l2-penalty}, the magnitudes of the factors are equal: $|\hat{\omega}_{j,d}| = |\hat{\mathrm{w}}_j|^{1/D}$ for all $d \in [D]$. We now construct factors $\bomega_j'$ for any $\mathrm{w}_j'$ close to $\hat{\mathrm{w}}_j$ such that $\mathcal{K}_j(\bomega_j') = \mathrm{w}_j'$, $M(\bomega_j') = 0$, and $\| \bomega_j' - \hat{\bomega}_j \|_2 < \varepsilon / \sqrt{p}$. \\
Let $\varepsilon > 0$ be arbitrary. Since $\mathrm{w}_j \mapsto |\mathrm{w}_j|^{1/D}$ is continuous at $\hat{\mathrm{w}}_j$, there exists $\delta_j > 0$ such that $|\mathrm{w}_j' - \hat{\mathrm{w}}_j| < \delta_j$ implies $\left| |\mathrm{w}_j'|^{1/D} - |\hat{\mathrm{w}}_j|^{1/D} \right| < \varepsilon / (\sqrt{D p})$. The factors $\bomega_j'$ are defined as follows:
\[
\omega_{j,d}' =
\begin{cases}
\displaystyle \text{sign}(\hat{\omega}_{j,d})\cdot |\mathrm{w}_j'|^{1/D}, & \text{if } \hat{\mathrm{w}}_j \neq 0,\ \forall d \in [D], \\[2ex]
\displaystyle \text{sign}(\mathrm{w}_j') \cdot |\mathrm{w}_j'|^{1/D}, & \text{if } \hat{\mathrm{w}}_j = 0,\ d = 1, \\[2ex]
\displaystyle |\mathrm{w}_j'|^{1/D}, & \text{if } \hat{\mathrm{w}}_j = 0,\ d = 2, \ldots, D.
\end{cases}
\]
This ensures the magnitudes are equal, so $M(\bomega_j') = 0$, and that the product of the factors satisfies $\mathcal{K}_j(\bomega_j') = \mathrm{w}_j'$. For $\hat{\mathrm{w}}_j\neq0$, we can apply the sign pattern of $\hat{\bomega}_j$ to the $\omega_{j,d}'$ by choosing $\delta_j$ small enough. The resulting distance between $\bomega_j'$ and $\hat{\bomega}_j$ is then:
{\footnotesize
\[
\| \bomega_j' - \hat{\bomega}_j \|_2 = \sqrt{D} \cdot \left| |\mathrm{w}_j'|^{1/D} - |\hat{\mathrm{w}}_j|^{1/D} \right| < \frac{\varepsilon}{\sqrt{p}},
\]
}

since $\left| |\mathrm{w}_j'|^{1/D} - |\hat{\mathrm{w}}_j|^{1/D} \right| < \varepsilon / (\sqrt{D p})$. Extending to the vector case, let $\delta = \min_j \{\delta_j\}$. For any $\w' \in B(\hat{\w},\delta)$, each component $\mathrm{w}_j'$ satisfies $|\mathrm{w}_j' - \hat{\mathrm{w}}_j| < \delta_j$. Applying the scalar construction to each $\mathrm{w}_j'$, we obtain $\bomega'$ such that $\mathcal{K}(\bomega') = \w'$ and $M(\bomega') = 0$. Together, we get
\[
\| \bomega' - \hat{\bomega} \|_2 = \Big( \sum_{j=1}^p \| \bomega_j' - \hat{\bomega}_j \|_2^2 \Big)^{1/2} < \varepsilon.
\]
Therefore, for any $\varepsilon > 0$, there exists $\delta > 0$ such that $\w' \in B(\hat{\w}, \delta)$ implies the existence of $\bomega' \in B(\hat{\bomega}, \varepsilon)$ with $\mathcal{K}(\bomega') = \w'$ and $M(\bomega') = 0$.
\end{proof}

Choosing $\varepsilon=\varepsilon_0$, then $\exists \delta_0>0:\forall\, \tilde{\w} \in B(\hat{\w},\delta_0)\, \exists \tilde{\bomega} \in B(\hat{\bomega},\varepsilon_0):\, \mathcal{K}(\tilde{\bomega})=\tilde{\w}$ and $M(\tilde{\bomega})=0$. By assumption $\hat{\w}$ is not a minimizer of $\Lw(\w)$, hence $\exists\,\w' \in B(\hat{\w},\delta_0):\, \Lw(\w')<\Lw(\hat{\w})$. Let $\bomega'\in B(\hat{\bomega},\varepsilon_0)$ be the corresponding balanced factorization of $\w'$ constructed using \cref{lemma:aux-lemma-thm}, with the properties $\mathcal{K(\bomega')}=\w'$,  $M(\bomega')=0$ and thus $\Lomega(\bomega')=\Lw(\w')$. But then

\begin{equation}
    \exists \bomega' \in B(\hat{\bomega},\varepsilon_0):\,\Lomega(\bomega')=\Lw(\w') < \Lw(\hat{\w}) = \Lomega(\hat{\bomega}),
\end{equation}

contradicting $\hat{\bomega} \in \argmin_{\bomega \in \mathbb{R}^{Dp}} \Lomega(\bomega)$. Therefore, if $\hat{\bomega}$ is a local minimizer of $\Lomega(\bomega)$, then $\hat{\w}=\mathcal{K}(\hat{\bomega})$ is a local minimizer of $\Lw(\w)$ with $\Lomega(\hat{\bomega})=\Lw(\hat{\w})$. This finishes the proof.
\end{proof}



\begin{comment}
Before proving the theorem, we introduce some required notations and auxiliary results. Our strategy is to first show equivalence between the the $L_{2/D}$-regularized objective $\Lw(\w)$ and an intermediate objective $\tilde{\mathcal{L}}_{\w,\lambda}(\bomega):= \mathcal{L}_{\w,\lambda}(\bomega_1 \odot \ldots \odot \bomega_D)$ that still contains the regularizer $\Vert \bomega_1 \odot \ldots \odot  \bomega_D \Vert_{2/D}^{2/D}$ instead of the $L_2$ regularization in the DWF loss. In a second step, we show the equivalence of $\tilde{\mathcal{L}}_{\bomega,\lambda}(\bomega)$ and the DWF objective $\Lomega(\bomega)$. We define the inverse factorization function as $\mathcal{K}: \mathbb{R}^{Dp} \to \mathbb{R}^p,\, \bomega \mapsto \bomega_1 \odot \ldots \odot \bomega_D = \bomegae$, and remark that it is a smooth surjection. To show the correspondence of minimizers between both objectives, we require $\mathcal{K}$ to be locally open at each minimizer \citep{levin2024effect}.

\begin{definition}[Local openness]\label{def:local-openness}
A mapping $\mathcal{K}: \mathbb{R}^{Dp} \to \mathbb{R}^p, \bomega \mapsto \mathcal{K}(\bomega)$ is locally open at $\bomega$ if for every $\varepsilon > 0$ we can find $\delta > 0$ such that $B(\mathcal{K}(\bomega), \delta) \subseteq \mathcal{K}(B(\bomega, \varepsilon))$, where $B(\bomega, \varepsilon)$ denotes an $\varepsilon$-ball around $\bomega$. Further $\mathcal{K}$ is called globally open if it is locally open at all $\bomega \in \mathbb{R}^{Dp}$.
\end{definition}

Using the same notation, continuity can be defined as $\forall\,\varepsilon>0\,\exists\,\delta>0:\,\mathcal{K}(B(\bomega, \delta)) \subseteq B(\mathcal{K}(\bomega), \varepsilon)$. For DWF, global openness can be established using the following result extending a result for scalar-valued factorizations in \citet{balcerzak2016certain}: 

\begin{proposition}[\cite{kolb2023smoothing}, Lemma 5.3]\label{prop:global-openness}
The inverse weight factorization $\mathcal{K}: \mathbb{R}^{Dp} \rightarrow \mathbb{R}^p,\left(\bomega_1, \ldots, \bomega_D\right) \mapsto \bomega_1 \odot \ldots \odot \bomega_D$ is globally open.
\end{proposition}

With this context we can now state the proof of \cref{thm:equi}:

\begin{proof}
%\begin{definition}[Deep Weight Factorization]
%A depth-$D$ factorization with $D\in\mathbb{N}_{\geq2}$ of an arbitrary neural network $f_{\w}(\w,\bm{x}),\, \w \in \mathbb{R}^p$, is given by $f_{\w}(\bomega_1 \odot \ldots \odot \bomega_D, \bx)=:f_{\bm{\omega}}(\bm{\omega},\bm{x}),\,$ with $\bomega= (\bm{\omega}_1,\ldots,\bm{\omega}_D)$ and factors $\bm{\omega}_d = (\omega_{1,d},\ldots,\omega_{p,d}) \in \mathbb{R}^{p}, d\in[D]$. The original and factorized parameters are related through $\w = \bm{\omega}_1 \odot \ldots \odot \bm{\omega}_D= \bomegae$, with $\bomegae$ denoting the collapsed parameter.
%\end{definition}


Let $\mathcal{L}_{\w,\lambda}(\w)$ be the penalized objective function using $\w \in \mathbb{R}^p$, and $\tilde{\mathcal{L}}_{\w,\lambda}(\bomega) = \mathcal{L}_{\w,\lambda}(\mathcal{K}(\bomega))$ be the intermediary objective defined on $\bomega \in \mathbb{R}^{Dp}$. For the first step, we establish the following results:

\begin{lemma}\label{lemma:base-to-intermedia}
If $\hat{\w}$ is a local minimizer of $\mathcal{L}_{\w,\lambda}(\w)$, % and $\mathcal{K}(\bomega)=\bomega_1 \odot \ldots \odot \bomega_D := \bomegae$, 
then any $\hat{\bomega}$ such that $\hat{\bomega} \in \mathcal{K}^{-1}(\hat{\w})= \{\bomega : \mathcal{K}(\bomega) = \hat{\w}\}$ is a local minimizer of $\tilde{\mathcal{L}}_{\w,\lambda}(\bomega)$ with $\mathcal{L}_{\w,\lambda}(\hat{\w}) = \tilde{\mathcal{L}}_{\w,\lambda}(\hat{\bomega})$.
\end{lemma}

\begin{proof}
Assume $\hat{\w}$ is a local minimizer of $\mathcal{L}_{\w,\lambda}(\w)$, then $\exists \varepsilon > 0 : \forall\w' \in B(\hat{\w}, \varepsilon) : \mathcal{L}_{\w,\lambda}(\hat{\w}) \leq \mathcal{L}_{\w,\lambda}(\w')$. Since $\mathcal{K}(\bomega)$ is surjective, choose any $\hat{\bomega} \in \mathcal{K}^{-1}(\hat{\w}) = \{\bomega : \mathcal{K}(\bomega) = \hat{\w}\}$. By continuity of $\mathcal{K}$, there $\exists \delta > 0 : \mathcal{K}(B(\hat{\bomega}, \delta)) \subseteq B(\mathcal{K}(\hat{\bomega}), \varepsilon) = B(\hat{\w}, \varepsilon)$. This means $\forall \bomega' \in B(\hat{\bomega}, \delta) : \mathcal{K}(\bomega') = \w' \in B(\hat{\w}, \varepsilon)$. Since by assumption, $\mathcal{L}_{\w,\lambda}(\hat{\w}) \leq \mathcal{L}_{\w,\lambda}(\w')$ for all $\w' \in B(\hat{\w}, \varepsilon)$, and by continuity all $\bomega' \in B(\hat{\bomega}, \delta)$ map to some $\w'$ in $B(\hat{\w}, \varepsilon)$ under $\mathcal{K}$, we conclude that

\[\forall \bomega' \in B(\hat{\bomega}, \delta) :  \tilde{\mathcal{L}}_{\w,\lambda}(\hat{\bomega})=\mathcal{L}_{\w,\lambda}(\mathcal{K}(\hat{\bomega})) = \mathcal{L}_{\w,\lambda}(\hat{\w}) \leq \mathcal{L}_{\w,\lambda}(\w') = \mathcal{L}_{\w,\lambda}(\mathcal{K}(\bomega')) = \tilde{\mathcal{L}}_{\w,\lambda}(\bomega').\]

Therefore, if $\hat{\w}$ is a local minimizer of $\mathcal{L}_{\w,\lambda}(\w)$, then any $\hat{\bomega} \in \mathcal{K}^{-1}(\hat{\w})$ is a local minimizer of $\tilde{\mathcal{L}}_{\w,\lambda}(\bomega) = \Lw(\mathcal{K}(\bomega))$ with equivalent local minima $\mathcal{L}_{\w,\lambda}(\hat{\w}) = \mathcal{L}_{\w,\lambda}(\mathcal{K}(\hat{\bomega}))$.
\end{proof}

\begin{lemma}\label{lemma:intermedia-to-base}
If $\hat{\bomega}$ is a local minimizer of $\tilde{\mathcal{L}}_{\w,\lambda}(\bomega)=\Lw(\mathcal{K}(\bomega))$, %and the continuous surjection $\mathcal{K}(\bomega)=\bomega_1 \odot \ldots \odot \bomega_D := \bomegae$ is locally open at $\hat{\bomega}$, 
then $\mathcal{K}(\hat{\bomega}) = \hat{\w}$ is a local minimizer of $\mathcal{L}_{\w,\lambda}(\w)$ with $\mathcal{L}_{\w,\lambda}(\hat{\w}) = \tilde{\mathcal{L}}_{\w,\lambda}(\hat{\bomega})$.
\end{lemma}

\begin{proof}
Assume $\hat{\bomega}$ is a local minimizer of $\tilde{\mathcal{L}}_{\w,\lambda}(\bomega)$, then $\exists\, \varepsilon > 0 : \forall\bomega' \in B(\hat{\bomega}, \varepsilon) : \mathcal{L}_{\w,\lambda}(\mathcal{K}(\hat{\bomega})) \leq \mathcal{L}_{\w,\lambda}(\mathcal{K}(\bomega'))$. Since $\mathcal{K}(\bomega)$ is open at $\hat{\bomega}$ (\cref{prop:global-openness}), there is $\delta > 0$ such that $B(\mathcal{K}(\hat{\bomega}), \delta) \subseteq \mathcal{K}(B(\hat{\bomega}, \varepsilon))$. Thus, $\forall\,\w' \in B(\mathcal{K}(\hat{\bomega}), \delta):\, \exists\, \bomega' \in B(\hat{\bomega}, \varepsilon)$ such that $\mathcal{K}(\bomega') = \w'$. But since we have by assumption that $\forall\,\bomega' \in B(\hat{\bomega}, \varepsilon) : \mathcal{L}_{\w,\lambda}(\mathcal{K}(\hat{\bomega})) \leq \mathcal{L}_{\w,\lambda}(\mathcal{K}(\bomega'))$, and we established $\forall\,\w' \in B(\hat{\w}, \delta) \, \exists\, \bomega' \in B(\hat{\bomega}, \varepsilon) : \w' = \mathcal{K}(\bomega')$, we obtain

\[\forall\, \w' \in B(\hat{\w}, \delta) : \mathcal{L}_{\w,\lambda}(\hat{\w}) = \mathcal{L}_{\w,\lambda}(\mathcal{K}(\hat{\bomega})) \leq \mathcal{L}_{\w,\lambda}(\mathcal{K}(\bomega')) = \mathcal{L}_{\w,\lambda}(\w').\]

Thus, $\hat{\w} = \mathcal{K}(\hat{\bomega})$ is a local minimizer of $\mathcal{L}_{\w,\lambda}(\w)$ with corresponding local minimum $\mathcal{L}_{\w,\lambda}(\hat{\w}) = \mathcal{L}_{\w,\lambda}(\mathcal{K}(\hat{\bomega}))$.
\end{proof}

Together, \cref{lemma:base-to-intermedia} and \cref{lemma:intermedia-to-base} establish the equivalence of all minima between $\Lw(\w)$ and the factorized intermediate objective $\tilde{\mathcal{L}}_{\w,\lambda}(\bomega)=\Lw(\mathcal{K}(\bomega))$, both of which are non-differentiable. The following result extends the equivalence to the differentiable factorized objective with $L_2$ regularization.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%\begin{lemma}\label{lemma:intermedia-diffable-equiv-new}
%Let $\Lomega(\bomega)$ be the factorized objective \cref{eq:factorized_objective} with $L_2$ regularization and $\tilde{\mathcal{L}}_{\w,\lambda}(\bomega) = \Lw (\mathcal{K}(\bomega))$ be the intermediate objective defined above. Then \textbf{i)} if $\hat{\bomega}$ is a local minimizer of $\Lomega(\bomega)$, then $\hat{\bomega}$ is also a local minimizer of $\tilde{\mathcal{L}}_{\w,\lambda}(\bomega)$ with $\Lomega(\hat{\bomega}) = \tilde{\mathcal{L}}_{\w,\lambda}(\hat{\bomega})$. Further, 
%\textbf{ii)} if $\tilde{\bomega}$ is a local minimizer of $\tilde{\mathcal{L}}_{\w,\lambda}(\bomega)$, then there exists $\hat{\bomega}$ such that $\mathcal{K}(\hat{\bomega}) = \mathcal{K}(\tilde{\bomega})$, and $\hat{\bomega}$ is a local minimizer of $\Lomega(\bomega)$ with $\Lomega(\hat{\bomega}) = \tilde{\mathcal{L}}_{\w,\lambda}(\tilde{\bomega})$.
%\end{lemma}


%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{lemma}\label{lemma:intermedia-diffable-equiv}
Let $\Lomega(\bomega)$ be the factorized objective \cref{eq:factorized_objective} with $L_2$ regularization and $\tilde{\mathcal{L}}_{\w,\lambda}(\bomega) = \Lw (\mathcal{K}(\bomega))$ be the intermediate objective defined above. Then \textbf{i)} if $\hat{\bomega}$ is a local minimizer of $\Lomega(\bomega)$, then $\hat{\bomega}$ is also a local minimizer of $\tilde{\mathcal{L}}_{\w,\lambda}(\bomega)$ with $\Lomega(\hat{\bomega}) = \tilde{\mathcal{L}}_{\w,\lambda}(\hat{\bomega})$. Further, 
\textbf{ii)} if $\tilde{\bomega}$ is a local minimizer of $\tilde{\mathcal{L}}_{\w,\lambda}(\bomega)$, then there exists $\hat{\bomega}$ such that $\mathcal{K}(\hat{\bomega}) = \mathcal{K}(\tilde{\bomega})$, and $\hat{\bomega}$ is a local minimizer of $\Lomega(\bomega)$ with $\Lomega(\hat{\bomega}) = \tilde{\mathcal{L}}_{\w,\lambda}(\tilde{\bomega})$.
\end{lemma}

\begin{proof}
We start by relating both objectives using the factor misalignment $M(\bomega)=D^{-1} \sum_{d=1}^D \Vert \bomega_d \Vert_2^2 - \Vert \bomegae \Vert_{2/D}^{2/D}$. The DWF objective is then $\Lomega(\bomega)=\tilde{\mathcal{L}}_{\w,\lambda}(\bomega) + \lambda M(\bomega)$, where $M(\bomega)\geq0$ attains zero if an only if $\bomega$ represents a balanced factorization (\cref{lemma:min-l2-penalty}).

%%%%%%%%%%%%%%%%%%

To prove the first point, we assume that $\hat{\bomega}$ is a local minimizer of  $\Lomega(\bomega)$, so that $\exists\, \varepsilon>0:\forall\, \bomega' \in B(\hat{\bomega}, \varepsilon): \Lomega(\hat{\bomega})<\Lomega(\bomega')$. By \cref{lemma:min-l2-penalty}, only balanced factors can be solutions to $\Lomega(\bomega)$, implying $M(\hat{\bomega})=0$ and $\Lomega(\hat{\bomega})=\tilde{\mathcal{L}}_{\w,\lambda}(\hat{\bomega})$. To evoke a contradiction, assume that $\hat{\bomega}$ is not a local minimizer of $\tilde{\mathcal{L}}_{\w,\lambda}(\bomega)$, so that $\forall \, \varepsilon>0:\,\exists\,\bomega' \in B(\hat{\bomega},\varepsilon)$ with $\tilde{\mathcal{L}}_{\w,\lambda}(\bomega')<\tilde{\mathcal{L}}_{\w,\lambda}(\hat{\bomega})$. 

First, note that $\bomega'$ can not result in the same collapsed weight $\mathcal{K}(\bomega')$ as $\hat{\bomega}$, since $\hat{\bomega}$ already is a balanced min-norm factorization. %otherwise $\tilde{\mathcal{L}}_{\w,\lambda}(\bomega')<\tilde{\mathcal{L}}_{\w,\lambda}(\hat{\bomega})$ would not be possible as $M(\hat{\bomega})=0$. 
Hence, $\mathcal{K}(\bomega') \neq \mathcal{K}(\hat{\bomega})$.\\
To reduce notational overload, we abbreviate $\mathcal{K}(\bomega)=\bomegae$ again. Observing that points in the neighborhood of the product $\hat{\bomegae}$ of a balanced factorization $\hat{\bomega}$ can be attained by simple rescaling of $\hat{\omega}_{j,d}$ to conserve balancedness, each balanced factor is a continuous function of the resulting $\bomegae$. Selecting an arbitrary index from the set of feasible balanced factorizations lets us define a continuous function $\mathcal{F}:\mathbb{R}^{p} \to \mathbb{R}^{Dp}, \bomegae \mapsto (\bomega_1,\ldots,\bomega_D)$, that maps products to a specific balanced factorization. By continuity,   $\forall\,\varepsilon>0\,\exists\,\delta>0:\,\mathcal{F}(B(\bomegae, \delta)) \subseteq B(\mathcal{F}(\bomegae), \varepsilon)$. This means for every $\bomegae' \in B(\hat{\bomegae},\delta)$ there is is a corresponding balanced factorization $\tilde{\bomega} \in B(\hat{\bomega},\varepsilon)$ that also maps to $\bomegae'$ and thereby implies $\tilde{\mathcal{L}}_{\w,\lambda}(\tilde{\bomega}) = \tilde{\mathcal{L}}_{\w,\lambda}(\bomega')$ since $M(\tilde{\bomega})=0$. Together, we get the following chain of inequalities: 

$$\Lomega(\tilde{\bomega})= \tilde{\mathcal{L}}_{\w,\lambda}(\tilde{\bomega}) = \tilde{\mathcal{L}}_{\w,\lambda}(\bomega') < \tilde{\mathcal{L}}_{\w,\lambda}({\hat{\bomega}})  = \Lomega(\hat{\bomega}).$$

The first and last equalities hold because $M(\tilde{\bomega})=M(\hat{\bomega})=0$, and the second because $\tilde{\bomega}$ and $\bomega'$ are different factorizations of $\bomegae'$ and $\tilde{\mathcal{L}}_{\w,\lambda}(\bomega)$ is constant over factorizations the product $\bomegae'$. Then we have found $\tilde{\bomega} \in B(\hat{\bomega},\varepsilon)$ so that $\Lomega(\tilde{\bomega})<\Lomega(\hat{\bomega})$ leading to a contradiction. Therefore, if $\hat{\bomega}$ is a local minimizer of $\Lomega(\bomega)$, then $\hat{\bomega}$ is also a local minimizer of $\tilde{\mathcal{L}}_{\w,\lambda}(\bomega)$ with equivalent minima. 

For the second step, %to show that if $\tilde{\bomega}$ is a local minimizer of $\tilde{\mathcal{L}}_{\w,\lambda}(\bomega)$, then there exists $\hat{\bomega}$ such that $\mathcal{K}(\hat{\bomega}) = \mathcal{K}(\tilde{\bomega})$, and $\hat{\bomega}$ is a local minimizer of $\Lomega(\bomega)$ with $\Lomega(\hat{\bomega}) = \tilde{\mathcal{L}}_{\w,\lambda}(\tilde{\bomega})$,
we proceed by assuming $\tilde{\bomega}$ is a local minimizer of $\tilde{\mathcal{L}}_{\w,\lambda}(\bomega)$, so $\exists\, \varepsilon>0:\forall\, \bomega' \in B(\tilde{\bomega}, \varepsilon): \tilde{\mathcal{L}}_{\w,\lambda}(\tilde{\bomega})\leq\tilde{\mathcal{L}}_{\w,\lambda}(\bomega')$. Let $\hat{\bomega}$ be a balanced factorization of $\mathcal{K}(\tilde{\bomega})$. Then $M(\hat{\bomega})=0$ and $\mathcal{K}(\hat{\bomega})=\mathcal{K}(\tilde{\bomega})$, implying $\tilde{\mathcal{L}}_{\w,\lambda}(\tilde{\bomega})=\tilde{\mathcal{L}}_{\w,\lambda}(\hat{\bomega})=\Lomega(\hat{\bomega})$.\\
By continuity of $\mathcal{K}$, $\exists\, \delta>0:\forall\, \bomega' \in B(\hat{\bomega}, \delta): \mathcal{K}(\bomega') \in B(\mathcal{K}(\hat{\bomega}), \varepsilon)$. For any $\bomega' \in B(\hat{\bomega}, \delta)$, we have $\tilde{\mathcal{L}}_{\w,\lambda}(\bomega') \geq \tilde{\mathcal{L}}_{\w,\lambda}(\tilde{\bomega})$, as $\mathcal{K}(\bomega') \in B(\mathcal{K}(\tilde{\bomega}), \varepsilon)$ and $\tilde{\mathcal{L}}_{\w,\lambda}(\bomega)$ depends only on $\mathcal{K}(\bomega)$.\\
Therefore, $\forall\, \bomega' \in B(\hat{\bomega}, \delta): \Lomega(\bomega') = \tilde{\mathcal{L}}_{\w,\lambda}(\bomega') + \lambda M(\bomega') \geq \tilde{\mathcal{L}}_{\w,\lambda}(\tilde{\bomega}) = \Lomega(\hat{\bomega})$, where the inequality holds as $M(\bomega') \geq 0$. Thus, $\hat{\bomega}$ is a local minimizer of $\Lomega(\bomega)$ with $\Lomega(\hat{\bomega}) = \tilde{\mathcal{L}}_{\w,\lambda}(\tilde{\bomega})$.

\end{proof}

%\begin{comment}
%\begin{proof}

%We start by relating both objectives to each other using the factor misalignment $M(\bomega)=D^{-1} \sum_{d=1}^D \Vert \bomega_d \Vert_2^2 - \Vert \bomegae \Vert_{2/D}^{2/D}$. The DWF objective can then be written as $\Lomega(\bomega)=\tilde{\mathcal{L}}_{\w,\lambda}(\bomega) + \lambda M(\bomega)$, where $M(\bomega)\geq0$ attains zero if an only if $\bomega$ represents a balanced factorization and all solutions must satisfy $M(\bomega)=0$ (\cref{lemma:min-l2-penalty}).\\
%For the first point, let $\hat{\bomega}$ be a local minimizer of $\Lomega(\bomega)$. Then $\exists\, \varepsilon > 0$ such that $\forall\, \bomega \in B(\hat{\bomega}, \varepsilon)$:

%\begin{equation}
%\tilde{\mathcal{L}}_{\w,\lambda}(\hat{\bomega}) = \Lomega(\hat{\bomega}) \leq \Lomega(\bomega) = \tilde{\mathcal{L}}_{\w,\lambda}(\bomega) + \lambda M(\bomega) \leq \tilde{\mathcal{L}}_{\w,\lambda}(\bomega)
%\end{equation}

%The first equality holds because $\hat{\bomega}$ is balanced as a minimizer of $\Lomega(\bomega)$, so $M(\hat{\bomega}) = 0$. This shows that $\hat{\bomega}$ is also a local minimizer of $\tilde{\mathcal{L}}_{\w,\lambda}(\bomega)$.\\
%For the second point, let $\tilde{\bomega}$ be a local minimizer of  $\tilde{\mathcal{L}}_{\w,\lambda}(\bomega)$, so that $\exists\, \varepsilon>0:\forall\, \bomega \in B(\tilde{\bomega}, \varepsilon): \tilde{\mathcal{L}}_{\w,\lambda}(\tilde{\bomega})<\tilde{\mathcal{L}}_{\w,\lambda}(\bomega)$. We then construct $\hat{\bomega}$ as the balanced factorization of the product $\mathcal{K}(\tilde{\bomega})$. By definition, $M(\hat{\bomega})=0$ and $\mathcal{K}(\hat{\bomega})=\mathcal{K}(\tilde{\bomega})$, so that $\tilde{\mathcal{L}}_{\w,\lambda}(\tilde{\bomega})=\tilde{\mathcal{L}}_{\w,\lambda}(\hat{\bomega})=\Lomega(\hat{\bomega})$. Then $\forall\, \bomega \in B(\hat{\bomega}, \varepsilon)$:

%To show the second point, let $\tilde{\bomega}$ be a local minimizer of $\tilde{\mathcal{L}}_{\w,\lambda}(\bomega)$. Let $\hat{\bomega}$ be the balanced factorization with $\mathcal{K}(\hat{\bomega}) = \mathcal{K}(\tilde{\bomega})$. Then $\exists\, \varepsilon > 0$ such that $\forall\, \bomega \in B(\hat{\bomega}, \varepsilon)$:

%\begin{equation}
%\Lomega(\hat{\bomega}) = \tilde{\mathcal{L}}_{\w,\lambda}(\hat{\bomega}) = \tilde{\mathcal{L}}_{\w,\lambda}(\tilde{\bomega}) \leq \tilde{\mathcal{L}}_{\w,\lambda}(\bomega) \leq \tilde{\mathcal{L}}_{\w,\lambda}(\bomega) + \lambda M(\bomega) = \Lomega(\bomega)
%\end{equation}

%The first equality holds because $\hat{\bomega}$ is balanced, so $M(\hat{\bomega}) = 0$. The second equality holds because $\mathcal{K}(\hat{\bomega}) = \mathcal{K}(\tilde{\bomega})$. This shows that $\hat{\bomega}$ is a local minimizer of $\Lomega(\bomega)$.
%\end{proof}

The previous results transitively establish that if $\hat{\bomega}$ is a local minimizer of $\Lomega(\bomega)$, then $\hat{\bomegae}=\hat{\bomega}_1 \odot \ldots \odot \hat{\bomega}_D$ is a local minimizer of $\Lw(\w)$. Conversely, if $\hat{\w}$ is a minimizer of $\Lw(\w)$, then there exists a local minimizer $\hat{\bomega}$ of $\Lomega(\bomega)$ such that $\hat{\bomegae}=\hat{\w}$. This finishes the proof.

\end{proof}
\end{comment}

\subsection{Balanced factors and absorbing states in SGD optimization (\cref{lemma:balancedness})}\label{app:conserved-balancedness}

\begin{lemma}[Balanced factors are absorbing states in SGD]\label{lemma:balancedness}
Consider the SGD iterates of a depth-$D$ factorized network with parameters $\bomega^{(t)} = (\bm{\omega}_1^{(t)},\ldots,\bm{\omega}_D^{(t)})$ at iteration $t \in \mathbb{N}$, where the $j$-th entry of the collapsed weight vector $\bomegae^{(t)}$ is $\varpi_j^{(t)} = \omega_{j,1}^{(t)} \cdot \ldots \cdot \omega_{j,D}^{(t)}$. Then, \textbf{i)} if $\omegae_j^{(t)}=0$ and $M\big(\bomega_j^{(t)}\big)=0$, then  
%if $\omega_{j,d}^{(t)} = 0$ for all $d \in [D]$, then 
$\omega_{j,d}^{(t')} = 0$ for all $d \in [D]$ and $t' > t$.  Further, \textbf{ii)} $M\big(\bomega_j^{(t)}\big)=0$ implies $M\big(\bomega_j^{(t')}\big)=0$ for all $t'>t$. 
%balancedness of a factorization of $\omegae_j^{(t)}$, i.e., $|\omega_{j,1}^{(t)}| = \ldots = |\omega_{j,D}^{(t)}|$ implies preservation of balance for all $t' > t$. %That is, $|\omega_{j,1}^{(t')}| = \ldots = |\omega_{j,D}^{(t')}|$ for all $j \in [p]$ and $t' > t$. 
\end{lemma}

In other words, a balanced factorization at $0$ causes the SGD dynamics to ``collapse'' and the factors remain zero for all subsequent iterations, effectively reducing the expressiveness of the model.

\begin{proof}
Consider the SGD updates for the factors $\bomega_d \in \mathbb{R}^p, \, d \in [D]$, in a factorized network with $L_2$ regularization. Let $\mathcal{L}_{\bomega,0}(\bomega)$ denote the part of the loss function without regularization and assume a batch size of $n$ without loss of generality:

\begin{equation}
\bomega_d^{(t+1)} = \bomega_d^{(t)} - \eta^{(t)} \big( \nabla_{\bomega_d} \mathcal{L}_{\bomega,0}(\bomega^{(t)}) + 2 D^{-1} \lambda \bomega_d^{(t)} \big)
\end{equation}

Using the chain rule, the SGD updates are given by:

\begin{equation}
\bomega_d^{(t+1)} = \bomega_d^{(t)} - \eta^{(t)} \big( \nabla_{\bomegae} \mathcal{L}_{\bomega,0}(\bomega^{(t)}) \odot \big(\textstyle\bigodot_{k \neq d} \bomega_k^{(t)}\big) + 2 D^{-1} \lambda \bomega_d^{(t)} \big)
\end{equation}

To show the collapse in the dynamics for a balanced zero factorization, consider the scalar case $\varpi_j^{(t)}=0$ with factorization $\bomega_j^{(t)}=\{\omega_{j,d}^{(t)}\}_{d=1}^{D}$ such that $M\big(\bomega_j^{(t)}\big)=0$. Then $\omega_{j,d}^{(t)} = 0$ for all $d \in [D]$, and the update becomes:

\begin{equation}
\omega_{j,d}^{(t+1)} = 0 - \eta^{(t)} \big( [\nabla_{\bomegae} \mathcal{L}_{\bomega,0}(\bomega^{(t)})]_j \cdot 0 + 2D^{-1} \lambda \cdot 0 \big) = 0
\end{equation}

This holds for all subsequent iterations, proving $\omega_{j,d}^{(t')} = 0$ for all $d \in [D]$ and $t' > t$. Next we show the more general case of SGD dynamics conserving balancedness, i.e., $M\big(\bomega_j^{(t)}\big)=0$, or equivalently, $|\omega_{j,1}^{(t)}| = \cdots = |\omega_{j,D}^{(t)}| := m_j^{(t)}$. Let $\omega_{j,d}:=s_{j,d}^{(t)} m_j^{(t)}$, where $s_{j,d}^{(t)}=\text{sign}\big(\omega_{j,d}^{(t)}\big)$ and $s_{\varpi_j}^{(t)}=\text{sign}(\prod_{d=1}^D s_{j,d}^{(t)})$. We investigate the scalar updates: 

\begin{equation}
\omega_{j,d}^{(t+1)} = s_{j,d}^{(t)} m_j^{(t)} - \eta^{(t)} \big( [\nabla_{\bomegae} \mathcal{L}_{\bomega,0}(\bomega^{(t)})]_j \cdot (m_j^{(t)})^{D-1} \cdot \frac{s_{\varpi_j}^{(t)}}{s_{j,d}^{(t)}} + 2D^{-1} \lambda s_{j,d}^{(t)} m_j^{(t)} \big)
\end{equation}

Because $1/s_{j,d}^{(t)}=s_{j,d}^{(t)}$, we can factor out $s_{j,d}^{(t)}$ from all terms in the update. Hence, the resulting magnitude at iteration $t+1$ is:

\begin{equation}
|\omega_{j,d}^{(t+1)}| = \big|m_j^{(t)} - \eta^{(t)} \big( [\nabla_{\bomegae} \mathcal{L}_{\bomega,0}(\bomega^{(t)})]_j \cdot (m_j^{(t)})^{D-1} \cdot s_{\varpi_j}^{(t)} + 2D^{-1} \lambda m_j^{(t)} \big) \big|
\end{equation}

Since the magnitude is constant over $d$, it is shown that $M\big(\bomega_j^{(t')}\big)=0$ for all $t'>t$.
\end{proof}

%\begin{lemma}[Balanced factors are an absorbing state/stationary condition in SGD optimization + balancedness at $\bm{0}$ enforces that the parameter stays at $\bm{0}$ for all iterations]\label{lemma:balancedness}
%lorem ipsum
%\end{lemma}

This ``stochastic collapse" \citep{chen2024stochastic} in the gradient dynamics is a recently investigated phenomenon where the noise in SGD dynamics drives iterates toward simpler \textit{invariant sets} of the weight space that remain unchanged under SGD. The previous result (\cref{lemma:balancedness}) about zero misalignment being an absorbing state in DWF with SGD optimization exemplifies this collapse. However, the dynamics that govern the collapse are poorly understood, including how it is determined when and to which simpler structure the model collapses, with unclear implications for generalization in broad settings. The attractivity of these simpler structures is associated with symmetries and high gradient noise levels and closely related to the recently studied Type-II saddle points \citep{ziyin2023probabilistic}, potentially helping to explain the benefits of large initial LRs, adaptively regularizing overly expressive networks to constrained substructures via stochastic collapse. While potentially positive effects on generalization were shown, the research community is not yet certain about the broader consequences of this phenomenon.





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\clearpage

\section{Algorithms} \label{app:algos}

In the following, we provide the algorithms for the proposed initialization (\cref{sec:init}) of DWF networks in \cref{app:alg_init} and how to train these networks in \cref{app:alg_train}.

\subsection{DWF initialization} \label{app:alg_init}


\input{algs/alg-init}

\subsection{DWF training} \label{app:alg_train}


\input{algs/alg-dwf-training}


\clearpage

\section{Details on optimization} \label{app:details-init-lrs}

\subsection{Learning rates in factorized networks}

\paragraph{Ablation study on overall learning rate}

Our goal is to determine suitable LR ranges for achieving high sparsity with good generalization in factorized networks. Additionally, we investigate how deeper factorization affects LR requirements. We train factorized LeNet-300-100 with $D \in \{2,3,4\}$ and our $\texttt{DWF}$ initialization on MNIST, using initial LRs ranging from $10^{-3}$ to $2$. All models are trained using SGD with a cosine LR schedule. The results, displayed in \cref{fig:lr-grid-mnist}, show excessively high LRs lead to unstable results, especially at higher compression ratios. Similarly, too small LRs result in poor or even no sparsification. Notably, models with greater $D$ exhibit more robustness to LR variations, maintaining performance over a wider range of compression ratios compared to shallower factorizations. Across all depths, selecting a large initial LR slightly below the edge where training becomes unstable yields the best overall results, balancing both effective training with high compression ratios. and providing evidence for the importance of a large LR phase in DWF training.

\begin{figure}[h]
\centering
\includegraphics[width=0.98\linewidth]{figures/new/lenet300100_mnist_depths234_lrgrid.pdf}
\caption{Sparsity-accuracy tradeoffs for a grid of learning rates, demonstrating the importance of appropriately large LRs for DWF. Left to right shows factorization depths $D \in \{2,3,4\}$.}
\label{fig:lr-grid-mnist}
\end{figure}
\vspace{-0.3cm}

\paragraph{Ablation on the stability of optimal LRs across the sparsity range}

In another ablation study, we investigate the impact of different sparsity requirements on the optimal initial LR. To do this, we train a LeNet-300-100 on MNIST for $D \in \{2,3,4\}$ on a large number of LR and $\lambda$ combinations. For each $D$, we train all combinations of the learning rate $\eta$ and the regularization $\lambda$, comprising $8$ different LRs between $10^{-3}$ and $1$, and a grid of $20$ $\lambda$ values logarithmically spaced between $10^{-6}$ and $10^{-1}$. We obtain the Pareto frontier for each $D$ by removing all runs that are dominated by other runs in either test accuracy or compression ratio. \cref{fig:moredepth-ablation-lr-lambda} shows the corresponding tradeoffs, with the color of the points indicating the optimal learning rate for the corresponding $\lambda$. Confirming the importance of large LRs, the result further demonstrates that the range of optimal LRs remains at a high level across sparsity requirements, except for a slight trend toward distinctly larger LRs for models with little regularization. This can be explained by the intricate relationship between LR and $\lambda$, together forming the \textit{intrinsic} LR. When $\lambda$ is reduced, this is compensated using a larger LR to recover optimal performance \citep{li2020reconciling}.

%using LeNet-300-100 on MNIST. Note
%that none of the smaller LRs are selected as optimal.

%\begin{figure}[ht]
%\centering
%\includegraphics[width=0.45\textwidth]{figures/new/moredepth_lenet300100_mnist_pareto_frontier_colored.pdf}
%\caption[]{Ablation on optimal LRs at different amounts of sparsity using LeNet-300-100 on MNIST. Note that none of the smaller LRs are selected as optimal.}
%\label{fig:moredepth-ablation-lr-lambda}
%\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

\begin{figure}[ht]
\centering

\begin{subfigure}[t]{0.48\textwidth} % [t] forces top alignment
    \centering
    \includegraphics[width=\textwidth]{figures/new/moredepth_lenet300100_mnist_pareto_frontier_colored.pdf}
    \caption{Ablation on optimal LRs at different amounts of sparsity using LeNet-300-100 on MNIST. Note that none of the smaller LRs are selected as optimal.}
    \label{fig:moredepth-ablation-lr-lambda}
\end{subfigure}
\hfill
\begin{subfigure}[t]{0.48\textwidth} % [t] forces top alignment
    \centering
    \includegraphics[width=\textwidth]{figures/new/plot_lenet5_fmnist_inits.pdf}
    \caption{Factor initializations and depths $D$. For $D=2$, standard initialization performs worse and becomes untrainable for $D>2$.}
    \label{fig:initgrid-lenet5-fmnist-compar}
\end{subfigure}

\caption{Experiments on optimal LRs at different amounts of sparsity and different initialization approaches.}
\label{fig:combined-lrgrid-init-lenet5}
\end{figure}


\subsection{Ablation study on initializations}

In \cref{fig:initgrid-lenet5-fmnist-compar}, we extend the experimental analysis of standard and corrected initialization schemes on performance and sparsity in factorized networks, complementing the experiment on a fully-connected architecture (right plot of \cref{fig:init-combined-plot}) by a convolutional LeNet-5 architecture. Similar to the results in \cref{fig:init-combined-plot}, we observe a failure of standard initialization for $D>2$. For $D=2$, contrasting the results for LeNet-300-100 in \cref{fig:init-combined-plot}, standard initialization indeed achieves some sparsity for LeNet-5 on F-MNIST. The attainable tradeoff, however, is vastly outperformed by using the two corrections in the \texttt{DWF} initialization (\cref{alg:init}).




\subsection{Relationship between sparsity, regularization and weight norms}\label{app:cr-norm-lambda}

In \cref{fig:combined-cr-l2-dynamics-lambdagrid}, we present results on the relationship between sparsity (measured via the CR), regularization induced by different $\lambda$ values, and the implicit $L_2$ weight norms of the collapsed parameter $\bomegae$. From the first row, we see that increases in compression ratio for increasing $\lambda$ values have a similar trend for all depths, starting to induce sparsity at approximately the same regularization strengths. For all datasets and regularization strengths, except for extremely large $\lambda$ values on ResNet-18, the $D=4$ model always yields a higher compression than $D=3$, which in turn is sparser than the $D=2$ model for given $\lambda$. In the second row and for the smaller models, we see a short increase in the $L_2$ norm with increasing $\lambda$, followed by a drop in $L_2$ norm that finally goes to zero at the point where the highest compression is achieved. Remarkably, the collapsed model $L_2$ norm increases with $\lambda$ exactly up to the point where sparsity emerges. A slightly different behavior can be seen for ResNet, where the collapsed norm seems to monotonically decrease for increasing $\lambda$ values (i.e., norms do not increase first and then decrease). Finally, the third row indicates smaller $L_2$ norms the more compressed models become, again with deeper factorizations achieving higher compression ratios at the same $L_2$ norm just before model collapse.

\begin{figure}
    \centering
    \includegraphics[width=1.0\linewidth]{figures/new/combined_trajectory_plot.pdf}
    \caption{Relationship between different regularization strengths and compression ratio (first row), regularization strength and $L_2$ norm (second row), as well as compression ratio and $L_2$ norm (third row) for different datasets (columns) and different factorization depths $D$ (colors).}
    \label{fig:combined-cr-l2-dynamics-lambdagrid}
\end{figure}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\clearpage

\section{Additional results and ablation studies}\label{app:additional-architectures}

\subsection{Ablation study on the factorization depth \texorpdfstring{$D$}{D}}

In our experiments, we considered deeper factorizations up to a depth of $D=4$. This cut-off is not chosen arbitrarily but follows empirical observations that non-convex $L_q$ regularization achieves an optimal tradeoff between superior sparsity performance and difficulty of numerical optimization roughly at $q=0.5$ \citep{hu2017group}. In an ablation study, we investigate if this also holds for the DWF approach. \cref{fig:moredepth-ablation-depth-mnist-lenet300100} displays the sparsity-accuracy curves attained by factorizations depths up to $D=8$ and three different LRs in the range that performed well for $D=4$. We use the same hyperparameter configuration as described in \cref{app:experimental-details}. Results show that in all settings, deeper factorizations beyond $D=4$ offer no improvements in generalization or sparsity, while their training becomes increasingly unstable.

\begin{figure}[ht]
\centering
\includegraphics[width=1\textwidth]{figures/new/moredepth_lenet300100_mnist_fixed_lr_comparison.pdf}
\caption[]{Factorization depths $D>4$ empirically do not improve performance but become unstable to train. Sparsity-accuracy curves for LeNet-300-100 on MNIST with increasing LRs shown from left to right. %The results demonstrate the benefits of large LRs, particularly for little additional regularization.
}
\label{fig:moredepth-ablation-depth-mnist-lenet300100}
%\vspace{-0.3cm}
\end{figure}

%%%%%%


\subsection{Combined training and validation accuracy}

\cref{fig:combined-trainval-cr} contains the deferred training and compression trajectories over a range of $\lambda$ values, as shown exemplarily for ResNet-18 on CIFAR10 in the main text (\cref{fig:resnet18-cifar10-cr-acc-combined}). For improved clarity, we display the running mean of the validation accuracy over three iterations. In addition, \cref{fig:trainval-comb-vgg19warm-cifar100} illustrates the learning dynamics for a much finer grid of $\lambda$ values in the top row to provide a clearer picture of how the training trajectories are affected by different $\lambda$ values. Validation accuracies without moving average smoothing are displayed in the bottom row. 







\begin{figure}[h]
\centering
\includegraphics[width=0.96\textwidth]{figures/new/vgg19warm_cifar100_train_val_acc_dynamics_combined.pdf}
\caption[]{Impact of regularization $\lambda$ on training (\textbf{top}) and validation accuracy (\textbf{bottom}) for VGG-19 on CIFAR100 and $D \in \{2,3,4\}$. The top row shows the training curves for the whole grid of $\lambda$ values. Bottom row shows validation accuracies without running mean for selected $\lambda$.}
\label{fig:trainval-comb-vgg19warm-cifar100}
%\vspace{-0.3cm}
\end{figure}

\begin{figure}[h]
\centering

\begin{subfigure}[b]{0.9\textwidth}
    \centering
    \includegraphics[width=0.9\textwidth]{figures/new/combined_cr_acc_dynamics/lenet5bn_kmnist/lenet5bn_kmnist_combined_cr_acc_dynamics.pdf}
    \caption{Convolutional LeNet-5 on K-MNIST}
    \label{fig:trainval-cr-lenet5bn-kmnist}
\end{subfigure}

\vspace{0.2cm}

\begin{subfigure}[b]{0.9\textwidth}
    \centering
    \includegraphics[width=0.9\textwidth]{figures/new/combined_cr_acc_dynamics/vgg16warm_cifar10/vgg16warm_cifar10_combined_cr_acc_dynamics.pdf}
    \caption{VGG-16 on CIFAR10}
    \label{fig:trainval-cr-vgg16-cifar10}
\end{subfigure}

\vspace{0.2cm}

\begin{subfigure}[b]{0.9\textwidth}
    \centering
    \includegraphics[width=0.9\textwidth]{figures/new/combined_cr_acc_dynamics/wrn168_cifar100/wrn168_cifar100_combined_cr_acc_dynamics.pdf}
    \caption{WRN-16-8 on CIFAR100}
    \label{fig:trainval-cr-wrn168-cifar100}
\end{subfigure}

\vspace{0.2cm}

\begin{subfigure}[b]{0.9\textwidth}
    \centering
    \includegraphics[width=0.9\textwidth]{figures/new/combined_cr_acc_dynamics/resnet34_tinyimagenet/resnet34_tinyimagenet_combined_cr_acc_dynamics.pdf}
    \caption{ResNet-34 on Tiny ImageNet}
    \label{fig:trainval-cr-resnet34-tiny}
\end{subfigure}

\caption{Impact of regularization $\lambda$ on compression (\textbf{top}), training, and validation accuracy (\textbf{bottom}) for various architectures and datasets, using $D \in \{2,3,4\}$.}
\label{fig:combined-trainval-cr}
\end{figure}






%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%
\subsection{Evolution of layer-wise compression and weight norms}\label{app:layerwise-cr-norms}
%%%%%%%%%%%%%%%%%%%

%This section contains the remaining plots for \cref{sec:learning-dynamics-sparsity}, containing fine-grained descriptions of layer-wise dynamics regarding the evolution of sparsity in \cref{fig:combined-trainval-cr}. Similarly, we plot the layer-wise dynamics of each layer across networks, showing interesting and qualitatively similar patterns overall.

This section provides a detailed examination of the layer-wise dynamics regarding the evolution of sparsity, complementing the analysis in \cref{sec:learning-dynamics-sparsity}. Figure \ref{fig:combined-layerwise-cr-dep3} illustrates the layer-wise evolution of sparsity (top) and collapsed weight norm (bottom) for different architectures and datasets, using a factorization depth $D=3$ and increasing regularization strength $\lambda$. The plots reveal broadly consistent patterns across different architectures. For stronger regularization, we observe a more rapid and pronounced onset of sparsity across all layers. Different layers exhibit varying rates of sparsification, with deeper layers generally achieving higher compression ratios more quickly than earlier layers.
The layer-wise norm trajectories show a characteristic pattern of initial increase, for the first layer, followed by a peak and gradual decrease. The deeper levels exhibit a simpler dynamic, showing an initial short decline followed by a low plateau. Stronger regularization leads to earlier peaking and faster decay of weight norms, corresponding to faster sparsification.
Notably, the first layer exhibits distinct behavior (cf.~\cref{fig:combined-sparsity-misalignment}), often showing the lowest compression ratio and the highest peak in weight norm. These more complex dynamics indicate stronger feature learning in earlier layers closer to the input.
Combined, this analysis provides insights into how DWF affects different parts of the network during training and how this process is mediated by regularization. %The consistent patterns across architectures suggest that these dynamics are a fundamental aspect of the DWF method rather than being architecture-specific.

\begin{figure}[h]
\centering

\begin{subfigure}[b]{1\textwidth}
    \centering
    \includegraphics[width=1\textwidth]{figures/new/resnet18_cifar10_3_combined_cr_l2_trajectories.pdf}
    \caption{ResNet-18 on CIFAR10}
    \label{fig:layerwise-l2-cr-dep3-resnet18}
\end{subfigure}

\vspace{0.2cm} % Adjust this value to control vertical spacing between subfigures

\begin{subfigure}[b]{1\textwidth}
    \centering
    \includegraphics[width=1\textwidth]{figures/new/vgg16warm_cifar10_3_combined_cr_l2_trajectories.pdf}
    \caption{VGG-16 on CIFAR10}
    \label{fig:layerwise-cr-dep3-vgg16}
\end{subfigure}

\vspace{0.2cm} % Adjust this value to control vertical spacing between subfigures

\begin{subfigure}[b]{1\textwidth}
    \centering
    \includegraphics[width=1\textwidth]{figures/new/vgg19warm_cifar100_3_combined_cr_l2_trajectories.pdf}
    \caption{VGG-19 on CIFAR100}
    \label{fig:layerwise-cr-dep3-vgg19}
\end{subfigure}

\caption{Layer-wise evolution of sparsity (\textbf{top}) and collapsed weight norm (\textbf{bottom}) using $D=3$ and increasing regularization $\lambda$ (left to right) for different architectures and datasets.}
\label{fig:combined-layerwise-cr-dep3}
\end{figure}
\vspace{-0.4cm}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\subsection{Evolution of misalignment and onset of sparsity}\label{app:misalignment} 

%Here we showcase how the factor misalignment $M(\bomega)$ (\cref{sec:theory}) evolves over training as a function of the regularization strength $\lambda$. We further relate the onset of sparsity to the vanishing of misalignment, which we observe to hold even at the level of individual layers.

We investigate the empirical dynamics of the factor misalignment $M(\bomega)$ and demonstrate that DWF ensures balanced factorizations for sufficiently large $\lambda$. Our analysis reveals an interesting connection between the reduction of misalignment and the onset of sparsity in the learning dynamics, both at the layer-wise and overall model levels.


\begin{figure}[h]
\centering
\begin{subfigure}[b]{1\textwidth}
    \centering
    \includegraphics[width=0.9\textwidth]{figures/new/resnet18_cifar10_2_sparsity_misalignment_per_weight.pdf}
    \caption{ResNet-18 on CIFAR10 ($D=2$)}
    \label{fig:sparsity-misalignment-layerwise-resnet18-cifar10-dep2}
\end{subfigure}

\vspace{0.1cm}

\begin{subfigure}[b]{1\textwidth}
    \centering
    \includegraphics[width=0.9\textwidth]{figures/new/vgg16warm_cifar10_2_sparsity_misalignment_per_weight.pdf}
    \caption{VGG-16 on CIFAR10 ($D=2$)}
    \label{fig:sparsity-misalignment-layerwise-vgg16warm-cifar10-dep2}
\end{subfigure}

\caption{Evolution of the average layer-wise factor misalignment (dashed) together with layer-wise sparsity (solid) for ResNet-18 and VGG-16 on CIFAR10 and $D=2$. Increasing values of $\lambda$ shown from left to right.}
\label{fig:combined-sparsity-misalignment}
\end{figure}

Figures \ref{fig:sparsity-misalignment-layerwise-resnet18-cifar10-dep2} and \ref{fig:sparsity-misalignment-layerwise-vgg16warm-cifar10-dep2} illustrate the layer-wise evolution of sparsity and the average misalignment per layer for depth-$2$ factorized ResNet-18 and VGG-16 trained on CIFAR10. The factor misalignment $M(\bomega)$ is calculated at the layer level and normalized by the number of weights in each layer, providing a granular view of misalignment evolution across the network.


\begin{figure}[ht]
\centering
\includegraphics[width=1\textwidth]{figures/new/vgg19warm_cifar100_misalignment_dynamics_combined.pdf}
\caption[]{Evolution of factor misalignment $M(\bomega)$ for VGG-19 on CIFAR100 with increasing $\lambda$ and factorization depths $D \in \{2,3,4\}$ (left to right).}
\label{fig:vgg19-misalign}
%\vspace{-0.3cm}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The results reveal a clear relationship between the elimination of misalignment and sparsity emergence. The onset of sparsity coincides almost exactly with the elimination of average misalignment per layer, providing empirical evidence for the theoretical connection discussed in \cref{sec:theory}. Larger values of $\lambda$ lead to faster reduction in misalignment and earlier onset of sparsity, demonstrating stronger regularization favors more balanced factorizations.


Two important observations emerge from these results. First, earlier layers broadly exhibit higher initial layer-wise misalignment but decrease at a higher rate than later layers. Surprisingly, a larger initial misalignment coincides with the most rapid and pronounced onset of sparsity as the average misalignment approaches zero. Second, the final layer (yellow) displays distinctly decoupled dynamics, with sparsity emerging within the first few epochs, as opposed to the approximately simultaneous onset for the remaining layers.


We also explore if the onset of sparsity relates to the dynamics of different components of the regularized loss. Figure \ref{fig:onset-sparsity-equivar} shows the overall training loss $\Lomega(\bomega^{(t)})$, the data fit part $\mathcal{L}_{\bomega,0}(\bomega^{(t)})$, and the (non-collapsed) factor $L_2$ penalty $D^{-1} \lambda \Vert \bomega^{(t)} \Vert_2^2$. The $L_2$ penalty is further decomposed into its minimal penalty and the excess penalty or misalignment $\lambda \cdot M(\bomega^{(t)})$, as described in \cref{lemma:min-l2-penalty}.
\begin{figure}[h]
\centering
\includegraphics[width=0.65\textwidth]{figures/new/loss_decomp_resnet18_cifar10_9e-05_3.pdf}
\caption{Evolution of loss components and sparsity for ResNet-18 with depth $D=3$ and $\lambda=9\times10^{-5}$.}
\label{fig:onset-sparsity-equivar}
\end{figure}
Early in training, the $L_2$ component strongly exceeds the data fit component. Since the data fit levels out much earlier than the $L_2$ penalty, they intersect at some point during training that both LR and $\lambda$ influence. Notably, this point where the loss components are balanced coincides precisely with the onset of sparsity and the overall misalignment approaching zero.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Post-hoc pruning and fine-tuning}\label{sec:post-hoc-finetuning}

Since DWF operates distinctly from most sparsification methods, this offers potential for integration with other pruning techniques. To demonstrate this, we combined DWF with post-hoc pruning on a ResNet-18 with $D=2$ factorization trained on CIFAR10. The setup used an initial learning rate of $0.27$ and a batch size of 256. Each model was trained across a range of $\lambda$ values to obtain a raw sparsity-accuracy tradeoff curve. These models were then further pruned along a sequence of compression ratios and fine-tuned for $50$ epochs using SGD with an LR of $0.11$.
\cref{fig:posthoc-pruning-finetune-resnet18} presents the results of this experiment. Combining DWF with post-hoc pruning led to increased sparsity at certain accuracy levels up to three times while maintaining comparable accuracy. This demonstrates the potential for integrating DWF with existing pruning techniques.
\begin{figure}[ht]
\centering
\includegraphics[width=0.6\textwidth]{figures/new/resnet18_cifar10_posthocpruning_dep2.pdf}
\caption[]{Additional post-hoc pruning and fine-tuning. ResNet-18 ist first trained with DWF and $D=2$. The models are post-hoc magnitude pruned and re-trained for another 50 epochs.}
\label{fig:posthoc-pruning-finetune-resnet18}
\end{figure}
\vspace{-0.5cm}

\subsection{Additional sparsity-accuracy tradeoffs}

%\cref{fig:other-combinations-depths} provides additional experiments on sparsity-accuracy tradeoff for DWF applied to the WRN-16-8 and ResNet-18 architectures on CIFAR100 and Tiny Imagenet.

Figure \ref{fig:other-combinations-depths} presents sparsity-accuracy tradeoffs for WRN-16-8, ResNet-18, and ResNet-34 on CIFAR100 and Tiny ImageNet datasets, using factorization depths $D \in \{2, 3, 4\}$. Contrasting our training protocol for section \cref{sec:benchmark}, we do not tune the LRs here and set them to fixed values across datasets and architectures.
The results show that DWF consistently produces a range of sparsity-accuracy tradeoffs across different architectures and datasets without incurring model collapse. Deeper factorizations generally achieve higher accuracies at extreme sparsity levels.

\begin{figure}[ht]
\centering
\includegraphics[width=1\textwidth]{figures/new/combined_plot_others.pdf}
\caption[]{Additional experiments applying DWF to WRN-16-8 and ResNet-18. For these experiments, the LRs were not tuned for each setting but set to $\{0.2,0.5,0.7\}$ for $D \in \{2,3,4\}$ across models and datasets.}
\label{fig:other-combinations-depths}
%\vspace{-0.3cm}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\clearpage

\subsection{Additional benchmark results}\label{app:table-mnist-benchmark}

The following \cref{tab:accuracy_comparison} shows test accuracies for different compression ratios on different LeNet model specifications and different MNIST datasets. While GMP or SNIP sometimes perform best for 90\% or 95\% sparsity, DWF models show the highest sparsity in all medium- and high-sparsity cases. In total, Synflow and SNIP each work best in 1 case, GMP in 6 cases, $D=2$ yields the highest sparsity in 5 cases, $D=3$ in 14 cases, and $D=4$ in 21 cases.

\input{tables/lenets_mnists_results.tex}

%%%%%%%%%%%%%%%%%%%%%%%%



\clearpage

\section{Experimental details}\label{app:experimental-details}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Description of comparison methods}\label{app:other-methods}


In the following, we briefly describe the comparison methods used in our study, covering different approaches of network sparsification before or post-training.

\textbf{SNIP} (Single-shot Network Pruning): This method introduces the concept of connection sensitivity to quantify the impact of individual weights on the network's loss function, given by \( \mathbf{z}^{(l)} = \left| \mathbf{g}^{(l)} \odot \mathbf{w}^{(l)} \right| \) for layer $l \in [L]$, where \( \mathbf{g}^{(l)} \) is the loss gradient with respect to \( \mathbf{w}^{(l)} \). By computing this score for each weight at initialization, SNIP identifies and preserves the most crucial connections, enabling effective one-shot pruning before training. This approach has shown remarkable efficacy in maintaining network performance even at high sparsity levels \citep{lee2019snip}.\\
\textbf{SynFlow}: As a data-independent pruning approach, SynFlow addresses the important issue of layer collapse in neural network pruning. It utilizes a layerwise conservation principle to ensure conservation of synaptic flow across the network, thereby maintaining high model capacity even under extreme compression ratios. SynFlow has demonstrated state-of-the-art performance at very high sparsity levels, outperforming many data-driven approaches in scenarios where over 99\% of parameters are pruned \citep{tanaka2020pruning}.\\
\textbf{Global Magnitude Pruning} (GMP): This method is based on the assumption that the weight magnitudes are a good proxy for their importance in the network. Despite its heuristic nature, GMP has proven remarkably effective, especially at low sparsity levels. Its success has led to numerous refinements and adaptations of pruning schedules and criteria, with its lasting popularity in both research and practice highlighting its robustness and efficacy \citep{han2015learning,blalock2020state,frankle2020pruning}.\\
\textbf{Random Pruning}: Serving as a baseline method, random pruning uniformly removes weights or structures without considering their importance, thereby helping to evaluate the effectiveness of more sophisticated pruning strategies.

\subsection{Details on architectures, datasets, and training hyperparameters}

\paragraph{Neural network architectures}

In the following, we briefly describe the neural architectures used in our experiments.


\begin{itemize}
    \item \textit{LeNet-300-100}: This fully-connected network, designed for MNIST classification, consists of an input layer (784 units), two hidden layers (300 and 100 units respectively), and an output layer (10 units). All layers utilize ReLU activation functions. The architecture closely follows the original version proposed by \citep{lecun1989optimal}, adapted to incorporate modern activations for improved performance.
    \item \textit{LeNet-5} \citep{lecun1998gradient} is a small but effective convolutional network with two convolutional layers (6 and 16 filters, both 5x5), and three fully connected layers (120, 84, and 10 units). We use ReLU activations and add batch normalization \citep{ioffe2015batch} and average pooling after each convolutional layer.
    \item \textit{VGG-16} for CIFAR10/100 consists of 13 convolutional layers and 3 fully connected layers \citep{simonyan2014very}. The convolutional part is described by 2x(64 filters), 2x(128 filters), 3x(256 filters), 3x(512 filters), 3x(512 filters), with max pooling inserted after each group. All filter sizes are 3x3. Batch normalization is applied before each ReLU activation as described by \citep{lee2019snip}. \textit{VGG-19} extends VGG-16 by adding one more convolutional layer to each of the last three convolutional blocks, resulting in 19 layers in total. Following \citep{zagoruyko2015cifar}, the two fully-connected layers before the output are reduced to a single layer layer with 512 units compared to the ImageNet version.
    \item \textit{ResNet-18} is a popular residual network with 18 layers \citep{he2016deep}. In our implementation, the architecture is adapted following common practice for smaller image datasets \citep{tanaka2020pruning}. We modify the first convolutional layer to use 3x3 filters and remove the initial max pooling layer. The network consists of an initial convolutional layer, followed by 4 stages of basic blocks (2 blocks each), with filter sizes [64, 128, 256, 512]. Global average pooling is used before the fully connected output layer. Likewise, our \textit{ResNet-34} implementation is also adapted for smaller datasets. The architecture follows a similar pattern to ResNet-18 with more layers in each stage. As with ResNet-18, we use 3x3 filters in the first layer and omit the initial max pooling, appropriate for the image size of our experiments.
    \item \textit{WideResNet} is a ResNet variant whose increased width compared to plain ResNets allows for better feature representations. In our experiments, we choose WRN-16-8, which is specifically suited for CIFAR-like tasks \citep{zagoruyko2016wide}.
\end{itemize}

\paragraph{Datasets}

\input{tables/datasets.tex}

In our experimental evaluation, we use several standard image classification datasets of varying size and complexity, summarized in \cref{tab:datasets}.\\
MNIST, Fashion-MNIST (F-MNIST), and Kuzushiji-MNIST (K-MNIST) are grayscale image datasets, each containing 10 classes with images of 28x28 pixels. The original MNIST comprises handwritten digits, while F-MNIST contains images of clothing items, and K-MNIST has handwritten Japanese characters. These datasets combine a range of classification tasks with similar input dimensions but varying levels of difficulty.\\
CIFAR10 and CIFAR100 contain 32x32x3 (color) images with 10 and 100 classes respectively. These datasets present more challenging classification tasks due to their higher resolution, color information, and larger number of classes for CIFAR100.\\
Finally, Tiny ImageNet is a subset of the ImageNet dataset featuring 200 classes with 64x64x3 color images. This dataset is markedly more challenging and computationally intensive due to the relatively complex task with more and higher resolution images, as well as a larger number of classes.
All datasets are split into training (50,000 or 60,000 samples) and test (10,000 samples) sets. We further apply standard data pre-processing and augmentation techniques: For the three MNIST variants, we use pixel rescaling to $[0,1]$. The CIFAR and Tiny ImageNet images are normalized. For larger networks, we additionally employ data augmentation, including horizontal flips, width and height shifts (up to 12.5\%), and rotations (up to $15 ^{\circ}$). \cref{tab:training_hyperparams} contains the combinations of architectures and datasets we conducted experiments on.


%\input{tables/models-datasets.tex}

\paragraph{Training hyperparameters}

In our experiments, we use training hyperparameter configurations following broadly established standard settings \citep{simonyan2014very,he2015delving,zagoruyko2016wide}, as displayed in \cref{tab:training_hyperparams}. For both LeNet-300-100 and LeNet-5, we set the initial LR to $0.15$ and found it to perform well across datasets, with the exception of LeNet-300-100 on K-MNIST. 
Because established LRs were found to be suboptimal for DWF, we additionally select the best-performing LR (using small $\lambda=10^{-6}$) from a discrete grid between $0.05$ and $1$ for each factorization depth, architecture, and dataset. For DWF, the sparsity level is controlled using a logarithmically spaced sequence of $\lambda$ parameters between $10^{-6}$ and $10^{-1}$ on which we train each model to obtain the sparsity-accuracy tradeoff curves. 
For the comparison methods in \cref{sec:benchmark}, we follow the implementation details provided in \citet{frankle2020pruning,lee2019snip} if available. To make for a fair comparison, we also train the two LeNet architectures using the same LR of $0.15$ and cosine decay. For the larger networks, we only adjust the LR schedule from step to cosine decay but use the prescribed initial LR. To obtain tradeoff curves for the respective pruning methods, we train each method on a sequence of $15$ compression ratios between $10^1$ and $10^5$.


\input{tables/hyperparameters}


\paragraph{Further details for DWF}

Although our method requires no post-hoc pruning, it is sensible to apply a sufficiently small threshold to the final collapsed weights to account for numerical inaccuracies which have no impact on performance. We set this threshold to $\texttt{float32.mach.eps} \approx 1.19 \times 10^{-7}$. Additionally, the $\texttt{DWF}$ initialization (\cref{alg:init}) requires specification of the lower truncation threshold for the factor initializations, which we set to $\omegae_{\min} =3 \times 10^{-3}$ for all our experiments (cf. left plot of \cref{fig:init-combined-plot}).


%\clearpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Other approaches to factor initialization}\label{app:other-inits}

\subsection{Root initialization and results}

An alternative option to obtain an initialization of factors $\bomega_d$ that recovers the distribution of the original weight $\w$ is given in the following.

\begin{definition}[Root initialization]
    A root initialization of a depth-$D$ factorized weight $\w = \bomega_1 \odot \ldots \odot \bomega_D$ is given by first drawing a single standard weight initialization (\cref{def:std_init}) for each entry of $\w$ and assigning $\bomega_1 \leftarrow \operatorname{sign}(\w)\cdot|\w|^{1/D}$ and $\bomega_2,\ldots,\bomega_D \leftarrow |\w|^{1/D}$ element-wise.
\end{definition}

\cref{fig:lenet300100-initcompar-all} compares the root initialization against the vanilla initialization and the proposed \texttt{DWF} initialization with and without truncation. While the root initialization yields satisfactory results improving upon vanilla initialization for $D=2$, we observe that it behaves similarly to the VarMatch initialization for $D=3$, both outperformed compared to our \texttt{DWF} initialization and yields the worst results for $D=4$.

% here root init and exact intractable init
\begin{figure}[h]
\centering
\includegraphics[width=1\textwidth]{figures/new/lenet300100_fmnist_initcompar_lenet300100_fmnist_all.pdf}
\caption[]{Sparsity-accuracy tradeoffs for different depths $D$ (columns) and initializations (colors).}
\label{fig:lenet300100-initcompar-all}
%\vspace{-0.3cm}
\end{figure}

In \cref{fig:lrdecay-resnet18-root}, we further analyze the learning dynamics of a DWF model with root initialization. The results demonstrate qualitatively similar learning dynamics to our proposed \texttt{DWF} initialization, suggesting them to be a general feature of DWF and SGD optimization.

\begin{figure}[h]
\centering
\includegraphics[width=1\textwidth]{figures/new/lrdecay_trajectory_root.pdf}
\caption[]{Learning dynamics for the root initialization for different learning rate schedules (columns).}
\label{fig:lrdecay-resnet18-root}
%\vspace{-0.3cm}
\end{figure}

%We also explored a theoretical solution for obtaining an exact Gaussian distribution of the product, as detailed in the following. However, this approach involves an inverse Mellin transform, which is challenging to compute accurately in practice. Given these difficulties and the unclear benefits in typical scenarios, we opted for a more stable approach using Gaussian initializations with variance scaling and interval truncation.

\begin{comment}
\subsection{Derivation of exact Gaussian factor distribution using Mellin Transform}


We now present a theoretical approach to derive a distribution of the factors $\bm{\omega}_d$ in Deep Weight Factorization, such that the product of $D$ i.i.d. random variables from this distribution follows a normal distribution $\mathcal{N}(0, \sigma_{\mathrm{w}}^2)$ with pre-specified variance $\sigma_{\mathrm{w}}^2$. This analysis provides insight into the statistical properties of factorized weights and the challenges arising from of such a construction. Consider a neural network $f_{\w}(\w,\bx)$ parameterized by weights $\w \in \mathbb{R}^p$. In DWF, these weights are factorized into $D$ factors $ \w = \bomegae = \bm{\omega}_1 \odot \bm{\omega}_2 \odot \ldots \odot \bm{\omega}_D$,
where $\bm{\omega}_d \in \mathbb{R}^p$ for $d \in [D]$. Our goal is to find a distribution $\mathcal{P}$ for the elements $\omega_{j,d}$ of the factor weights $\bm{\omega}_d$, such that the product of $D$ i.i.d. random variables from $\mathcal{P}$ follows a distribution $\mathcal{N}(0, \sigma_{\mathrm{w}}^2)$. Specifically, for each $j \in [p]$, we require (with layer-wise varying $\sigma_{\mathrm{w}}^2$):
\begin{equation}
\mathrm{w}_j = \prod_{d=1}^D \omega_{j,d} \sim \mathcal{N}(0, \sigma_{\mathrm{w}}^2).
\end{equation}

To find such a distribution $\mathcal{P}$, we utilize the Mellin transform, which is used in statistics for analyzing products of independent non-negative random variables, with extensions to cases with both positive and negative values~\citep{springer1966distribution}. An important property of the Mellin transform is that it converts the convolution of the densities of products into multiplication in the Mellin domain, but only for non-negative random variables. Since the random variables $\omega_{j,d}$ can take both positive and negative values, we partition their density into positive and negative parts $\rho_{\omega}(\omega) = \rho_{\omega}^+(\omega) + \rho_{\omega}^-(\omega)$ \citep{springer1979algebra},
where:
\begin{align}
    \rho_{\omega}^+(\omega) &=
    \begin{cases}
        \rho_{\omega}(\omega), & \omega \geq 0, \\
        0, & \text{elsewhere},
    \end{cases} \quad
    \rho_{\omega}^-(\omega) =
    \begin{cases}
        \rho_{\omega}(\omega), & \omega \leq 0, \\
        0, & \text{elsewhere}.
    \end{cases}
\end{align}

Similarly, we partition the density of the product $\mathrm{w}_j$ into $\rho_{\mathrm{w}}(\mathrm{w}) = \rho_{\mathrm{w}}^+(\mathrm{w}) + \rho_{\mathrm{w}}^-(\mathrm{w})$. The Mellin transform $\mathit{M}_\rho(s)$ of a function $\rho(\omega)$ defined on $\mathbb{R}_0^+$ is defined as:
\begin{equation}
    \mathit{M}_\rho(s) = \int_0^{\infty} \omega^{s-1} \rho(\omega) \, d\omega.
\end{equation}

The Mellin transform of the positive part of the product of $D$ i.i.d.\ and even (symmetric around $0$) random variables is given by~\citep{springer1966distribution}:
\begin{equation}
    \mathit{M}_{\mathrm{w}}^+(s) = 2^{D-1} \left[ \mathit{M}_{\omega}^+(s) \right]^D.
\end{equation}

Since we want the product distribution to be $\mathcal{N}(0,\sigma_{\mathrm{w}}^2)$, we require the Mellin transform of the positive part of a zero-mean Gaussian density \citep{springer1979algebra}:
\begin{equation}
\mathit{M}_{\mathcal{N}}^+(s) = \int_0^{\infty} \mathrm{w}^{s-1} \rho_{\mathrm{w}}^+(\mathrm{w}) \, d\mathrm{w} = \frac{2^{(s-3)/2}}{\sqrt{\pi}} \, (\sigma_{\mathrm{w}})^{s-1} \, \Gamma\left( \frac{s}{2} \right),\quad \operatorname{Re}(s)>0,
\end{equation}
where $\Gamma(\cdot)$ denotes the Gamma function. 
Setting $\mathit{M}_{\mathrm{w}}^+(s) = \mathit{M}_{\mathcal{N}}^+(s)$, we have:
\begin{equation}
    2^{D-1} \left[ \mathit{M}_{\omega}^+(s) \right]^D = \frac{2^{(s-3)/2}}{\sqrt{\pi}} \, (\sigma_{\mathrm{w}})^{s-1} \, \Gamma\left( \frac{s}{2} \right).
\end{equation}

Dividing both sides by $2^{D-1}$, taking the $D$-th root, and simplifying, we obtain the Mellin transform of the positive part of the factor distribution $\mathcal{P}$:
%\begin{equation}
%    \left[ \mathit{M}_{\omega}^+(s) \right]^D = \frac{2^{(s - 2D - 1)/2}}{\sqrt{\pi}} \, (\sigma_{\mathrm{w}})^{s-1} \, \Gamma\left( \frac{s}{2} \right).
%\end{equation}

%Taking the $D$-th root of both sides:
%\begin{equation}
%    \mathit{M}_{\omega}^+(s) = \left( \frac{2^{(s - 2D - 1)/2}}{\sqrt{\pi}} \right)^{1/D} \, (\sigma_{\mathrm{w}})^{(s - 1)/D} \left[ \Gamma\left( \frac{s}{2} \right) \right]^{1/D}.
%\end{equation}

%Therefore, the Mellin transform of the positive part of the factor distribution $\mathcal{P}$ is:
\begin{equation}
    \mathit{M}_{\omega}^+(s) = \left( \frac{2^{s - 2D -1}}{\pi} \right)^{1/(2D)} \, (\sigma_{\mathrm{w}})^{(s - 1)/D} \left[ \Gamma\left( \frac{s}{2} \right) \right]^{1/D}.
\end{equation}

To obtain $\rho_{\omega}^+(\omega)$, computation of the inverse Mellin transform is required, with $c$ a real constant such that the integration path is in the region of convergence: \begin{equation} \rho_{\omega}^+(\omega) = \mathds{1}_{\{\omega>0\}}\frac{1}{2\pi i} \int_{c - i\infty}^{c + i\infty} \omega^{-s} \mathit{M}_{\omega}^+(s) \, ds\,. \end{equation}

Finally, the full factor density is obtained by reflecting the positive part about the $y$-axis to get $\rho_{\omega}^-(\omega)=\rho_{\omega}^+(-\omega)$ and combined $\rho_{\omega}(\omega)=\rho_{\omega}^+(\omega)+\rho_{\omega}^-(\omega)$, defined on the whole real line.

\paragraph{Practical challenges}

While the theoretical approach provides a valid method to derive the factor distribution, significant practical challenges arise because the factor density is obtained as the inverse Mellin transform that involves integrating over an infinite contour in the complex plane. Numerical methods for inverse Mellin transforms can further be unstable and sensitive to several parameter choices, although high computational precision is required, especially for large $D$ or varying $\sigma_{\mathrm{w}}^2$. Lastly, the Mellin transform $\mathit{M}_{\omega}^+(s)$ depends on both the $D$ and $\sigma_{\mathrm{w}}^2$, altering the factor distribution non-parametrically and complicating the practical applicability. Due to these challenges, deriving the factor distribution via inverse Mellin transforms remains challenging and with unclear utility in deep learning applications, given that preliminary experiments suggest it is unlikely that precise approximations have a notable effect on trainability and performance in non-pathological cases. Alternative methods, such as our proposed initialization using simple Gaussian distributions with variance scaling and interval truncation, are sufficient to reach baseline performances and thus are more suitable from a practical point of view. 

%Dividing by $2^{D-1}$ and taking the $D$-th root, we obtain:
%\begin{equation}
%    \mathit{M}_{\omega}^+(s) = 2^{\frac{s}{2D}-1} (\sigma_{\mathrm{w}})^{s/D} \left[ \Gamma\left( \frac{s}{2} \right) \right]^{1/D}.
%\end{equation}

%Since $\mathit{M}_{\omega}^-(-s) = \mathit{M}_{\omega}^+(s)$, % the Mellin transform of the entire factor distribution is:
%\begin{equation}
%    \mathit{M}_{\omega}(s) = 2 \cdot 2^{\frac{s}{2D} - 1} (\sigma_{\mathrm{w}})^{s/D} \left[ \Gamma\left( \frac{s}{2} \right) \right]^{1/D} = 2^{\frac{s}{2D}} (\sigma_{\mathrm{w}})^{s/D} \left[ \Gamma\left( \frac{s}{2} \right) \right]^{1/D}.
%\end{equation}

%The presence of the Gamma function and fractional exponents makes inversion and precise approximation difficult. 
\end{comment}

\subsection{Exact Gaussian Factor Representation}

Although the density of random variables whose product is Gaussian is non-trivial, an exact expression in terms of compositions of transformations of Gamma-distributed random variables can be derived. The expression is found by writing the (symmetric) standard Gaussian as $\mathrm{w} = R \cdot e^U$, where $R$ is a Rademacher variable taking values $\pm 1$ with equal probability and $U := \ln |\mathrm{w}|$. \citet{pinelis2018exp} establish the infinite divisibility of the $\exp$-normally distributed $U$ by inspection of its characteristic function. This result can be readily exploited to obtain factor distributions such that their $D$-times product is a zero-mean Gaussian with arbitrary variance $\sigma_{\mathrm{w}}^2>0$:

\begin{lemma}[Gaussian Product Factor (GPF) Distribution]\label{lemma:init-exact-gaussian}
Let $D \in \mathbb{N}^+$. Consider independent and identically (i.i.d.) distributed random variables $\{\omega_d\}_{d=1}^D$ constructed as:
\begin{equation}
\omega_d \stackrel{\mathcal{D}}{\sim} R_d \cdot \exp\left\{ \frac{\ln(2 \cdot \sigma_{\mathrm{w}}^2)}{2D}- \mathcal{G}_{0,d} - \sum_{k=1}^{\infty} \left[ \frac{\mathcal{G}_{k,d}}{2k+1} - \frac{1}{2D} \ln\left(1 + \frac{1}{k}\right) \right] \right\}\,,
\end{equation}
where $R_d$ are i.i.d. Rademacher random variables (taking values $\pm1$ with probability $\frac{1}{2}$) and $\mathcal{G}_{k,d}$ are i.i.d. $\text{Gamma}(\frac{1}{D}, 1)$ variables for all $k \geq 0$ and $d \in \{1,\ldots,D\}$. Then their product follows a zero-mean normal distribution:
\begin{equation}
\prod_{d=1}^D \omega_d \stackrel{\mathcal{D}}{\sim} \mathcal{N}(0,\sigma_{\mathrm{w}}^2)
\end{equation}
\end{lemma}

\begin{proof}
We first establish the result for $\sigma_{\mathrm{w}}^2 = 1$ and then extend to arbitrary positive variances. Let $\mathrm{w} \sim \mathcal{N}(0,1)$. Given the symmetry of the normal distribution, we can represent $\mathrm{w}$ as $\mathrm{w} = R \cdot e^U$, where $R$ is a Rademacher random variable and $U = \ln|\mathrm{w}|$ is the logarithm of its absolute value. The characteristic function of $U$ is given by $\phi_U(t) = \mathbb{E}[e^{itU}] = \mathbb{E}[e^{it\ln|w|}]$. Since $\mathrm{w}$ has an even density, we can simplify the expectation as:
\begin{equation}
\phi_U(t) = 2\int_0^\infty e^{it\ln x} \cdot f_\mathrm{w}(x) dx
\end{equation}
Substituting the standard normal density $f_\mathrm{w}(x) = \frac{1}{\sqrt{2\pi}}e^{-x^2/2}$:
\begin{equation}
\phi_U(t) = \frac{2}{\sqrt{2\pi}} \int_0^\infty x^{it}e^{-x^2/2}dx
\end{equation}
Substituting $u = x^2/2$ implies $x = \sqrt{2u}$ and $dx = \frac{du}{\sqrt{2u}}$ and therefore  
\begin{equation}
    \phi_U(t) = \frac{2}{\sqrt{2\pi}} \int_0^\infty (\sqrt{2u})^{it} e^{-u} \frac{du}{\sqrt{2u}},
\end{equation} 
The term $(\sqrt{2u})^{it}$ can be written as $2^{it/2}u^{it/2}$, giving using further simplification:
\begin{equation}
\phi_U(t) = \frac{2^{it/2}}{\sqrt{\pi}} \int_0^\infty u^{(it-1)/2} e^{-u} du
\end{equation}
The integral is the Gamma function with argument $s = \frac{1+it}{2}$:
\begin{equation}
\phi_U(t) = \frac{2^{it/2}}{\sqrt{\pi}} \cdot \Gamma\left(\frac{1+it}{2}\right) = 2^{it/2} \cdot \frac{\Gamma(\frac{1+it}{2})}{\Gamma(\frac{1}{2})}
\end{equation}
using the identity $\Gamma(\frac{1}{2}) = \sqrt{\pi}$.
%\begin{equation}
%\phi_U(t) = 2^{it/2} \cdot \frac{\Gamma(\frac{1+it}{2})}{\Gamma(\frac{1}{2})}
%\end{equation}
We now apply Euler's product formula to both gamma functions:
\begin{equation}
\Gamma(z) = \frac{1}{z}\prod_{k=1}^\infty \left(\left(1+\frac{1}{k}\right)^z \cdot \frac{1}{1+\frac{z}{k}}\right)
\end{equation}
Let $z = \frac{1+it}{2}$ and $z_0 = \frac{1}{2}$. Then:
{\scriptsize
\begin{equation}
\Gamma\left(\frac{1+it}{2}\right) = \frac{2}{1+it}\prod_{k=1}^\infty \left(\left(1+\frac{1}{k}\right)^{\frac{1+it}{2}} \cdot \frac{1}{1+\frac{1+it}{2k}}\right)\,\,\text{and}\,\,\Gamma\left(\frac{1}{2}\right) = 2\prod_{k=1}^\infty \left(\left(1+\frac{1}{k}\right)^{\frac{1}{2}} \cdot \frac{1}{1+\frac{1}{2k}}\right)
\end{equation}
}
Taking their ratio and substituting into our expression for $\phi_U(t)$:
\begin{equation}
\phi_U(t) = 2^{it/2} \cdot \frac{2}{1+it} \cdot \frac{1}{2} \prod_{k=1}^\infty \left(\left(1+\frac{1}{k}\right)^{\frac{1+it}{2}-\frac{1}{2}} \cdot \frac{1+\frac{1}{2k}}{1+\frac{1+it}{2k}}\right)
\end{equation}
The exponent simplifies as $\frac{1+it}{2}-\frac{1}{2} = \frac{it}{2}$, giving:
\begin{equation}
\phi_U(t) = 2^{it/2} \cdot \frac{1}{1+it} \prod_{k=1}^\infty \left(\left(1+\frac{1}{k}\right)^{it/2} \cdot \frac{1+\frac{1}{2k}}{1+\frac{1+it}{2k}}\right)
\end{equation}
For the fraction in the product, multiply numerator and denominator by $2k$:
\begin{equation}
\frac{1+\frac{1}{2k}}{1+\frac{1+it}{2k}} = \frac{2k+1}{2k+1+it} = \frac{1}{1+\frac{it}{2k+1}}
\end{equation}
Writing the expression in exponential form:
\begin{equation}
\phi_U(t) = e^{\frac{it}{2}\ln 2} \cdot \frac{1}{1+it} \prod_{k=1}^\infty \left(e^{\frac{it}{2}\ln(1+\frac{1}{k})} \cdot \frac{1}{1+\frac{it}{2k+1}}\right)
\end{equation}
This characteristic function reveals that $U$ can be expressed as:
\begin{equation}
U \stackrel{\mathcal{D}}{\sim} \frac{\ln(2)}{2} - \mathcal{E}_0 - \sum_{k=1}^\infty \left[\frac{\mathcal{E}_k}{2k+1} - \frac{1}{2}\ln(1+\frac{1}{k})\right]
\end{equation}
where $\mathcal{E}_k \stackrel{iid}{\sim} \text{Exp}(1)$ for all $k \geq 0$, since $e^{\frac{it}{2}\ln 2}$ is the characteristic function of the constant $\frac{\ln 2}{2}$, $\frac{1}{1+it}$ is the characteristic function of $-\mathcal{E}_0$, and for each $k$, $e^{\frac{it}{2}\ln(1+\frac{1}{k})}$ is the characteristic function of $\frac{1}{2}\ln(1+\frac{1}{k})$. Lastly, $\frac{1}{1+\frac{it}{2k+1}}$ is the characteristic function of $-\frac{\mathcal{E}_k}{2k+1}$. For the factors $\omega_d$, we distribute the components of $U$ across $D$ dimensions using the property that the sum of $D$ independent $\text{Gamma}(\frac{1}{D},1)$ random variables follows $\text{Exp}(1)$:
\begin{equation}
\ln|\omega_d| \stackrel{\mathcal{D}}{\sim} \frac{\ln(2)}{2D} - \mathcal{G}_{0,d} - \sum_{k=1}^\infty \left[\frac{\mathcal{G}_{k,d}}{2k+1} - \frac{1}{2D}\ln(1+\frac{1}{k})\right]
\end{equation}
where $\mathcal{G}_{k,d} \sim \text{Gamma}(\frac{1}{D},1)$. Taking the product:
\begin{equation}
\prod_{d=1}^D \omega_d = \left(\prod_{d=1}^D R_d\right) \cdot \exp\left(\sum_{d=1}^D \ln|\omega_d|\right)
\end{equation}
Since $\sum_{d=1}^D \mathcal{G}_{k,d} \sim \text{Gamma}(1,1) = \mathcal{E}_k$, the sum of logarithms recovers the distribution of $U$:
\begin{equation}
\sum_{d=1}^D \ln|\omega_d| \stackrel{\mathcal{D}}{\sim} U
\end{equation}
Therefore, $\prod_{d=1}^D \omega_d \stackrel{\mathcal{D}}{\sim} \mathcal{N}(0,1)$. For arbitrary $\sigma_{\mathrm{w}}^2 > 0$, we modify the constant term in the factor distribution to $\frac{\ln(\sigma_{\mathrm{w}}^2)}{2D}$ to produce the desired product variance, yielding the result:
{\footnotesize
\begin{equation}\label{eq:factor-construction-impl}
\omega_d \stackrel{iid}{\sim} R_d  \cdot \exp\left\{ \frac{\ln(2 \cdot \sigma_{\mathrm{w}}^2)}{2D}- \mathcal{G}_{0,d} - \sum_{k=1}^{\infty} \left[ \frac{\mathcal{G}_{k,d}}{2k+1} - \frac{1}{2D} \ln\left(1 + \frac{1}{k}\right) \right] \right\} \Rightarrow \prod_{d=1}^D \omega_d \stackrel{\mathcal{D}}{\sim} \mathcal{N}(0,\sigma_{\mathrm{w}}^2).
\end{equation}.
}
\end{proof}

\paragraph{Practical challenges} Unfortunately, it is hard to efficiently implement sampling from the factor distribution due to the infinite sum and the series of additional operations required to compute $\omega_d$. Since the summands tend to $0$ as $k \to \infty$, it seems practical to truncate the infinite sum at a reasonable value to approximate the true factor distribution. Fig.~\ref{fig:init_exact_approx} shows the kernel density estimates of the factor distribution for $D = \{2, 3, 4, 10, 20\}$ as well as the densities of the resulting product alongside an overlay of the ground truth Gaussian. To obtain the estimates, $n = 1000$ products were sampled using the target standard deviation $\sigma_{\mathrm{w}} = 0.1$. The plots in Fig.~\ref{fig:init_exact_approx} show the
approximation power computing only the first $k \in \{1, 5\}$ terms of the infinite sum. The results show that even the coarsest approximation using only a single summand $k=1$ yields product distributions that are statistically indistinguishable from a Gaussian using a Kolmogorov-Smirnoff test for any factorization depth $D$. Expectedly, the approximation improves with the number of summands $k$.

A significant drawback to the exact Gaussian factorization approach using Lemma~\ref{lemma:init-exact-gaussian} over the proposed \texttt{DWF} initialization is its impractical initialization complexity, rendering the approach unfeasible in practice. This is owed to the accumulating number of additional operations required for each scalar weight, such as drawing multiple random variables per single factor $\omega_d$, computing several constants, summing or multiplying them, and exponentiation. For example, initializing a factorized ResNet-18 with $D=2$ using a vanilla initialization takes $\approx 1 s$, for the proposed \texttt{DWF} initialization $\approx 3 s$, but for the coarsely approximated exact Gaussian factor distribution with $k=1$ around $180$ minutes on an A-4000 GPU with 48GB RAM, despite yielding identical performance to the \texttt{DWF} init. The same network could not be initialized within a time budget of six hours for depths $D>2$.

\vspace{-0.1cm}

\begin{figure}[htbp]
    \centering
    % 0th subfigure
    \subfloat[Only including the first $k=1$ summands.]{%
        \includegraphics[width=0.9\textwidth]{figures/new/rigorous_solution_5D_maxterms1.pdf} 
        \label{fig:maxterm1}
    }\\[0.01em]
    
    % First subfigure
    \subfloat[Only including the first $k=5$ summands.]{%
        \includegraphics[width=0.9\textwidth]{figures/new/rigorous_solution_5D_maxterms5.pdf} 
        \label{fig:maxterm5}
    }\\[0.01em]
    
    % Second subfigure
    %\subfloat[Only including the first $k=50$ summands.]{%
     %   \includegraphics[width=0.86\textwidth]{figures/new/rigorous_solution_5D_maxterms50.pdf} 
      %  \label{fig:maxterm50}
    %}
    
    \caption{ Approximation of a Gaussian product for different truncation values of the infinite sum in \cref{lemma:init-exact-gaussian}. K-S corresponds to the Kolmogorov-Smirnoff test for normality.}
    \label{fig:init_exact_approx}
\end{figure}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%% other figs %%%%%%%%%%%%%%%%

\clearpage

\section{Computational environment and runtime analysis}


\subsection{Computational environment}

Large experiments on ResNet-18 and VGG-19 on datasets CIFAR10, CIFAR100, and Tiny ImageNet were run on an A-100 GPU server with 32GB RAM and 16 CPU cores. Smaller experiments were conducted on a single A-4000 GPU with 48GB RAM or CPU workstations.

\subsection{Runtime analysis}\label{app:runtimes}

Here, we investigate the computational overhead induced by DWF compared to vanilla training. We conducted experiments with WRN-16-8 on CIFAR10 and VGG-19 on CIFAR100 across various batch sizes. Each model was trained for 1000 iterations using SGD. We measured the average wall-clock time per sample and peak GPU memory utilization during training. All experiments were performed on a single A-4000 GPU with 48GB RAM, repeated five times to report means and standard deviations.
Our results, displayed in \cref{fig:runtime-persample-2} and \cref{fig:gpumem-2}, show that the factorization depth $D$ in the DWF model only has a minor impact on computational costs during training. For batch sizes of 256 or higher, both networks exhibit an indistinguishable time per sample comparable to vanilla training across all levels of $D$. At smaller batch sizes, we observe a slight monotonic increase in runtime with greater D. For example, WRN-16-8 with a batch size of 128 and $D=2$ runs approximately 10\% longer than vanilla training, while VGG-19 with a batch size of 64 and $D=4$ shows the largest increase of about 80\%.
These findings demonstrate that DWF training under typical settings incurs only a small additional cost compared to standard training. This contrasts with many sparsification techniques like Iterative Magnitude Pruning \citep{frankle2018lottery}, which can lead to several-fold increases in training time due to multiple cycles of pruning and re-training.

\begin{figure}[ht]
\centering
\includegraphics[width=0.5\textwidth]{figures/new/runtime_combined_2.pdf}
\caption[]{Comparison of wall-clock time per sample for WRN-16-8 (\textbf{left}) and VGG-19 (\textbf{right}) on CIFAR10/100 across factorization depths $D$. Results indicate insignificant runtime overhead for DWF compared to vanilla training, particularly for larger batch sizes where runtime is identical.}
\label{fig:runtime-persample-2}
%\vspace{-0.3cm}
\end{figure}

GPU memory utilization is primarily dependent on batch size, with $D$ having rather small effects in total. In conclusion, besides factorizing the weights into $D$ factors, DWF incurs only a minor additional runtime and memory cost on commonly used convolutional architectures. The minimal increase, especially for typical batch sizes, suggests DWF can be readily integrated into existing training protocols without major changes in computational overhead.


\begin{figure}[ht]
\centering
\includegraphics[width=0.7\textwidth]{figures/new/gpumem_combined_2.pdf}
\caption[]{Peak GPU memory utilization for WRN-16-8 (\textbf{left}) and VGG-19 (\textbf{right}) across depths $D$. The results show that batch training is the dominant factor for memory usage, with only a minimal impact of $D$.}
\label{fig:gpumem-2}
%\vspace{-0.3cm}
\end{figure}


\begin{comment}
\begin{figure}[ht]
\centering
\includegraphics[width=1\textwidth]{figures/new/lenet300100_mnist_combined_plots.pdf}
\caption[]{}
\label{fig:lenet300100-mnist-seeds-pruning}
%\vspace{-0.3cm}
\end{figure}
\end{comment}


\begin{comment}
\begin{figure}[ht]
\centering
\includegraphics[width=1\textwidth]{figures/new/combined-layerwise-cr-plot-CIFAR10.pdf}
\caption[]{}
\label{fig:layerwise-cr-barplot-combined}
\vspace{-0.3cm}
\end{figure}
\end{comment}

%\begin{figure}[ht]
%\centering
%\includegraphics[width=0.5\textwidth]{figures/new/combined-layerwise-cr-plot-CIFAR10.pdf}
%\caption[]{}
%\label{fig:layerwise-cr-barplot-cifar10}
%\vspace{-0.3cm}
%\end{figure}

\begin{comment}
\begin{figure}[ht]
    \centering
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/new/layerwise-cr-plot-ResNet-18-CIFAR-10-4-cr≈1000.pdf}
        \caption{}
        \label{fig:layerwise-cr-barplot-resnet18}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/new/layerwise-cr-plot-VGG-16-CIFAR-10-4-cr≈1000.pdf}
        \caption{}
        \label{fig:layerwise-cr-barplot-vgg16}
    \end{subfigure}
    
    \vspace{0.5cm}
    
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/new/layerwise-cr-plot-ResNet-18-CIFAR10-Synflow-CR1000.pdf}
        \caption{}
        \label{fig:layerwise-cr-barplot-synflow-resnet18}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/new/layerwise-cr-plot-VGG-16-CIFAR10-Synflow-CR1000.pdf}
        \caption{}
        \label{fig:layerwise-cr-barplot-synflow-vgg16}
    \end{subfigure}
    
    \vspace{0.5cm}
    
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/new/layerwise-cr-plot-ResNet-18-CIFAR10-SNIP-CR1000.pdf}
        \caption{}
        \label{fig:layerwise-cr-barplot-snip-resnet18}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/new/layerwise-cr-plot-VGG-16-CIFAR10-SNIP-CR1000.pdf}
        \caption{}
        \label{fig:layerwise-cr-barplot-snip-vgg16}
    \end{subfigure}
    
    \caption{Layerwise compression ratios for different models and methods}
    \label{fig:layerwise-cr-barplot-combined-grid}
\end{figure}
\end{comment}





%\begin{figure}[ht]
%\centering
%\includegraphics[width=0.5\textwidth]{figures/new/layerwise-cr-plot-VGG-16-CIFAR-10-4-high sparsity.pdf}
%\caption[]{}
%\label{fig:layerwise-cr-barplot-vgg16}
%\vspace{-0.3cm}
%\end{figure}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\end{document}
