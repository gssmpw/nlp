
\begin{algorithm}
\small
\caption{\small Training Factorized Neural Networks}
\begin{algorithmic}[1]\label{alg:train}
\STATE \textbf{Input:} 
\STATE \quad Dataset $\mathcal{D} = \{(\bx_i, y_i)\}_{i=1}^n$, network architecture $\mathcal{A}$ with $L$ layers and weights $\w \in \mathbb{R}^p$
\STATE \quad Factorization depth $D \geq 2$, 
\STATE \quad Factor initialization method $\{$\texttt{DWF-Init}, base initialization~$\mathcal{P},\varepsilon\}$,\, (Alg.~\ref{alg:init})
\STATE \quad Training hyperparameters $\{T, |\mathcal{B}|, \text{LRSchedule}\, \{\eta^{(t)}\}_{t=1}^{T}, \lambda \}$
\STATE \quad $\varepsilon_{\text{tiny}}$\quad {\small (e.g., float32 machine epsilon $\approx 1.19\times10^{-7}$}) %\COMMENT{for numerical errors}

\STATE \textbf{Deep Weight Factorization:}
\STATE \quad Factorize the weights $\w$ of $\mathcal{A}$ as:
\STATE \quad $\w \leftarrow \bomega_1 \odot \ldots \odot \bomega_D$ and obtain $f_{\bomega}(\bomega)$ from $f_\wrm(\wrm)$

\STATE \textbf{Initialize weights $\bomega$ of $f_{\bomega}(\bomega)$:}
\STATE \quad $\bomega \leftarrow \texttt{DWF-Init}(\mathcal{A}, D, \varepsilon,\, \text{standard init}\,\mathcal{P}$)

\FOR{each training step $t \in \{0, \dots, T-1\}$}
    \STATE Sample mini-batch $\mathcal{B}^{(t)}=\{(\bx_i, y_i)\}_{i=1}^{|\mathcal{B}|}$ from $\mathcal{D}$ and compute gradient 
    %\STATE Compute network output and loss using $\bomega_1^{(t)}, \dots, \bomega_D^{(t)}$
    %\STATE Calculate the loss:
    %\STATE \quad $\mathcal{L}_{\bomega,\lambda}(\bomega^{(t)}) + \lambda D^{-1}\sum_{d=1}^{D}\big\|\bomega_d^{(t)}\big\|_{2}^2$
    \STATE Update $\bomega_d$ using SGD:
    \STATE \quad $\bomega_d^{(t+1)} \leftarrow \bomega_d^{(t)} - \frac{\eta^{(t)}}{|\mathcal{B}|} \nabla_{\bomega_d} \left( \mathcal{L}_{\bomega,0}(\bomega^{(t)}) + \lambda D^{-1} \sum_{d=1}^{D} \|\bomega_d^{(t)}\|_2^2 \right) \quad \forall \,\, d \in [D]$
    \STATE Update LR:
    \STATE \quad $\eta^{(t+1)} \leftarrow \text{LRSchedule}(t+1)$
\ENDFOR

\STATE \textbf{Post-training factor collapse:}
\STATE \quad Collapse factors to obtain weights for $\mathcal{A}$:
\STATE \quad $\hat{\bomegae} = \bomega_1^{(T)} \odot \ldots \odot \bomega_D^{(T)}$
\STATE \quad Apply numerical mach. epsilon threshold $\varepsilon_{\text{tiny}}$ to remove approx. $0$ weights:
\STATE \quad $\hat{\omegae}_j \leftarrow 0$ \text{ if } $|\hat{\omegae}_j| < \varepsilon_{\text{tiny}} \,\,\, \forall \,\, j \in [p]$
\STATE \quad Transfer sparse weights $\hat{\bomegae}$ back to $\mathcal{A}$
\STATE \textbf{Output:} 
\STATE \quad Sparse collapsed network parameters $\hat{\bomegae}=\hat{\w}$

\end{algorithmic}
\end{algorithm}