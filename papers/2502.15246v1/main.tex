% This is samplepaper.tex, a sample chapter demonstrating the
% LLNCS macro package for Springer Computer Science proceedings;
% Version 2.21 of 2022/01/12
%
\documentclass[runningheads]{llncs}
%
\usepackage[T1]{fontenc}
\usepackage{courier} %% Sets font for listing as Courier.
\usepackage{listings, xcolor}
\usepackage{hyperref}
% \lstset{
%     escapeinside={(*@}{@*)},          % if you want to add LaTeX within your code
% }
\usepackage{booktabs}
% T1 fonts will be used to generate the final print and online PDFs,
% so please use T1 fonts in your manuscript whenever possible.
% Other font encondings may result in incorrect characters.
%
\usepackage{graphicx}
% Used for displaying a sample figure. If possible, figure files should
% be included in EPS format.
%
% If you use the hyperref package, please uncomment the following two lines
% to display URLs in blue roman font according to Springer's eBook style:
%\usepackage{color}
%\renewcommand\UrlFont{\color{blue}\rmfamily}
%\urlstyle{rm}
%
\usepackage{listings, xcolor}
\definecolor{javapurple}{rgb}{0.5,0,0.35} % strings
\definecolor{linenumbergray}{rgb}{0.5,0.5,0.5}
\lstdefinestyle{Java-github}{
        basicstyle=\ttfamily\tiny,
        language=Java,
        commentstyle=\color{linenumbergray},
        stringstyle=\color{javapurple},
        keywordstyle=\color{red},
        morekeywords={@Test},
        morecomment=[s][\color{linenumbergray}]{/**}{*/},
        numbers=left,
        numberstyle=\tiny\color{linenumbergray},
        numbersep=2.5pt,
        xleftmargin=1em,
        moredelim=**[is][\color{javapurple}]{@h@}{@h@},
        morecomment=[f][{\btHL[fill=gitdel]}]-,
        morecomment=[f][{\btHL[fill=gitadd]}]+,
        breaklines = true,
        %escapeinside={(*@}{@*)}
}
\lstdefinestyle{prompt}{
     basicstyle=\ttfamily\tiny,
     language=Html,
     commentstyle=\color{linenumbergray},
     stringstyle=\color{javapurple},
     keywordstyle=\color{red},
     morekeywords={@Test},
     morecomment=[s][\color{linenumbergray}]{/**}{*/},
     numbers=left,
     numberstyle=\tiny\color{linenumbergray},
     numbersep=2.5pt,
     xleftmargin=1em,
     moredelim=**[is][\color{javapurple}]{@h@}{@h@},
     morecomment=[f][{\btHL[fill=gitdel]}]-,
     morecomment=[f][{\btHL[fill=gitadd]}]+,
     breaklines = true, 
     %escapeinside={(*@}{@*)}
}
\begin{document}
%
\title{An approach for API synthesis using large language models}
%
%\titlerunning{Abbreviated paper title}
% If the paper title is too long for the running head, you can set
% an abbreviated paper title here
%
\author{Hua Zhong \and
Shan Jiang \and
Sarfraz Khurshid}
%
\authorrunning{Hua et al.}
% First names are abbreviated in the running head.
% If there are more than two authors, 'et al.' is used.
%
\institute{The University of Texas at Austin, Austin TX 78712, USA}
%
\maketitle              % typeset the header of the contribution
%

\newcommand{\NumTasks}{135}
\newcommand{\NumTasksCompletedOurApproach}{133}

\begin{abstract}
APIs play a pivotal role in modern software development by enabling seamless communication and integration between various systems, applications, and services. Component-based API synthesis is a form of program synthesis that constructs an API by assembling predefined components from a library. Existing API synthesis techniques typically implement dedicated search strategies over bounded spaces of possible implementations, which can be very large and time consuming to explore.
In this paper, we present a novel approach of using large language models (LLMs) in API synthesis.  LLMs offer a foundational technology to capture developer insights and provide an ideal framework for enabling more effective API synthesis.  We perform an experimental evaluation of our approach using \NumTasks~real-world programming tasks, and compare it with FrAngel, a state-of-the-art API synthesis tool.  The experimental results show that our approach completes \NumTasksCompletedOurApproach~of the tasks, and overall outperforms FrAngel. We believe LLMs provide a very useful foundation for tackling the problem of API synthesis, in particular, and program synthesis, in general.

% approach to synthesize complex APIs without the requirement of program components. We employ large language models as a foundational technology to encapsulate our insights within a framework for API synthesis. 

% We successfully evaluated our approach to synthesize complex APIs from realworld programming tasks. Moreover, our approach performs in par with a state-of-the-art API synthesis tool, despite not requiring the additional input of program components. 

\keywords{Component-based Synthesis \and Program Synthesis \and Large Language Models \and Complex APIs}.
\end{abstract}
%
%
%

\section{Introduction}
The increasing complexity of modern software systems has made the correct usage of APIs and dependent libraries a significant challenge for developers\cite{lamothe2021systematic}. Jungloid mining \cite{jungloid} highlights that even experienced developers often spend substantial time identifying a few relevant functions within a library. Similarly, Shi et al. \cite{frangel} observed that programmers frequently explore libraries to locate useful functions, which often need to be combined with intricate control structures, such as loops and conditions, to implement desired functionalities. These challenges complicate program synthesis, as they require not only knowledge of the library's components but also the integration of those components into coherent, functional code.

API synthesis is particularly demanding because it requires an understanding of the intended behavior, which is usually only known to the developer and often lacks formal documentation suitable for automated reasoning. While various specification methods exist - such as logical constraints \cite{10.1145/1993316.1993506,proveri} and executable specifications \cite{mimic,oguide} - synthesizing APIs using input-output examples has emerged as a user-friendly alternative\cite{frangel}. This approach allows developers to communicate intended functionality through input/output examples for synthesis programs. This method is especially beneficial for non-programmers, who can use examples to articulate their requirements without needing technical expertise \cite{stringio}.

Previous approaches to component-based synthesis often rely on domain-specific knowledge to address particular problem classes, such as string manipulation \cite{icmlpbe,stringio} or data transformations \cite{syndstrans,syntable}. And test-driven-synthesis\cite{testdriven} describe a general framework that leverages expert-written domain-specific languages (DSLs). While other approaches, such as loop free synthesis\cite{loopfree} and oracle guided synthesis \cite{oguide}, are domain-agnostic but depend on formal component specifications. But such methods requires knowledge of expected behavior, which may only be known to the developer and may not not exist in a formal language that supports automated reasoning. However, writing formal specifications for every component in software libraries is labor-intensive and often impractical. While these synthesis approaches for complex APIs handle various practical synthesis problems, they are largely limited to creating single basic blocks of code and do not readily handle multiple blocks in the presence of loops (or recursion) and complex tests. A key issue with handling multiple blocks is the very large size of the space of possible method sequences and their combinations.

More recent methods, such as SyPet\cite{sypet}, EdSynth\cite{Edsynth}, and FrAngel\cite{frangel}, have aimed to reduce reliance on DSLs or formal specifications by using black-box execution instead of reasoning about formal semantics. While SyPet struggles with synthesizing programs that involve complex control structures like loops and conditionals, FrAngel introduces a strategy using angelic conditions and EdSynth eliminate this problem by sketching. FrAngel first identifies promising program structures and then resolves the control flow through optimistic evaluation. By iteratively refining control flows that pass test cases, FrAngel succeeds in combining known behaviors with control structures to uncover more complex functionalities \cite{pnondet}. LooPy\cite{loopy} makes use of live execution, a technique that leverages the programmer as an oracle to step over incomplete parts of the loop, which also got rid of the dependence on DSL. However, programmers as oracles also makes LooPy rely on the programmer and cannot automatically synthesis APIs.


In this paper, we propose an approach that leverage LLMs with prompt engineering for systematic study of synthesizing complex APIs. LLMs have recently shown exceptional capabilities in tasks involving both natural language understanding and code synthesis \cite{li2023cctest,nam2024using,jiang2024generating}. Trained on large datasets, including extensive code repositories and documentation, LLMs possess an implicit understanding of programming syntax and semantics. These models can align user inputs with their pre-existing knowledge \cite{liu2024llm} and even address tasks that are uncommon in their training data \cite{schaeffer2024emergent}. Unlike previous synthesis methods, which often rely on domain-specific knowledge or predefined formal specifications, LLMs excel at managing complex, open-ended programming tasks. They can synthesis APIs based on natural language prompts and simple input/output examples, eliminating the need for explicit specifications or DSLs. Furthermore, LLMs handle intricate control structures such as loops and conditionals with ease, making them highly versatile and efficient. By directly interpreting developer intent through input/output examples, LLMs reduce the burden on developers, enabling rapid and adaptive solutions for diverse functionalities.

An exciting consequence of using LLMs is that our approach requires minimal user input (e.g., fewer test cases than FrAngel) compared to other methods and provides richer implementations that include meaningful variable names and comments. The user just provides a few simplified input/output examples and the API signature and LLMs can synthesis correct API. In contrast, previous work like FrAngel have to define a complete set of component API which is the search space. The rich training corpus of LLMs allow them to be used without explicitly having to include all relevant technical details, such as complete lists of allowed APIs (components) to use. Checking program correctness -- a non-trivial task -- is simplified through the use of simple input/out examples, which allow developers to validate that the generated API meets their intended requirements. Developing compelete test cases remains a challenging problem and prior program-by-example (PBE) methods have struggled with corner cases. Our evaluation demonstrates that LLMs, due to their extensive training data and their ability to infer intended functionality from API signature, can handle corner cases with just a few test cases. Our approach simplifies the synthesis process while maintaining reliability and accuracy, offering a practical and accessible solution for developers. Specifically, our approach synthesizes 133 correct APIs out of 135 benchmarks, outperforms previous state-of-the-art technique, FrAngel, in accuracy and readability. 

In addition, to ensure that our method has generalization ability, we use the evaluation dataset in FrAngel which covers different complexity of APIs in different scenario. Besides, we design our additional dataset of 15 new APIs that are not present in open-source projects, which ensures that LLMs are truly capable of synthesizing API implementations based on input/output examples rather than simply reproducing content in their large training set.

This paper makes the following contributions:

\begin{itemize}
    \item {\bf Approach}.  We present a novel approach to use LLMs to do API synthesis.  By combining prompt techniques such as assistant context, chain-of-thought prompting, few-shot learning, and follow-up queries, our approach mitigates the need for exhaustive search and manual component specification.
    \item {\bf Evaluation}. We perform an experimental evaluation of our approach using \NumTasks~real-world programming tasks, and compare it to the state-of-the-art component-based techniques, FrAngel. Our experimental results indicate that our approach achieves over 98.5\% success rate in synthesizing correct, test-passing APIs, outperforming FrAngel in accuracy. Besides, our approach synthesizes readable APIs with meaningful variable names, comments, improving the clarity of the synthesized APIs.
    \item {\bf Artifact}. We release our prompt template in appendix and complete benchmark and experiment results at \href{https://github.com/shanjiang98/api-chatgpt}{https://github.com/shanjiang98/api-chatgpt}. This enables reproducibility of our results and offers a foundation for other researchers to explore LLM-powered program synthesis in additional contexts. 
\end{itemize}


\input{example}

\input{approach}

\input{evaluation}

\section{Related Work}
\textbf{Program synthesis} has been a significant area of research, with various approaches proposed to generate programs efficiently. Traditional methods, such as Transit \cite{transit} and Escher \cite{escher}, rely on bottom-up enumerative synthesis, systematically exploring the program space to identify correct solutions. Brahma \cite{loopfree} introduces an efficient SMT-based encoding for synthesizing straight-line programs that involve multiple assignments to intermediate variables. Component-based synthesizers, such as SyPet \cite{sypet}, focus on generating Java programs from examples by leveraging arbitrary libraries. However, SyPet is limited to synthesizing sequences of method calls and does not support control structures like loops or conditionals. Similarly, Python-based synthesizers such as SnipPy \cite{snippy}, CodeHint \cite{codehint}, TFCoder \cite{tfcoder}, AutoPandas \cite{autopandas}, and Wrex \cite{wrex} primarily target one-liners or sequences of method calls. These tools also provide limited support for control structures, restricting their applicability for more complex program synthesis tasks. While these methods demonstrate the potential of automated program synthesis, their limitations in handling intricate control structures underline the need for more versatile approaches.

\textbf{Synthesis with control structures.} EdSynth \cite{Edsynth} address this by lazily initializing candidates during the execution of provided tests. The execution of partially completed candidates determines the generation of future candidates, making EdSynth particularly effective for synthesis tasks that involve multiple API sequences in both the conditions and bodies of loops or branches.
FrAngel \cite{frangel} is another notable tool that supports component-based synthesis for Java programs with control structures. Unlike many traditional methods, FrAngel relies on function-level specifications and, in principle, does not require users to have a detailed understanding of the algorithm or intermediate variables. However, in practice, to make the synthesis process feasible, users must provide a variety of examples, including base and corner cases. This requirement still necessitates some level of algorithmic knowledge, and FrAngelâ€™s relatively slow performance makes it less suitable for interactive scenarios.
LooPy \cite{loopy}, introduces the concept of an Intermediate State Graph (ISG), which compactly represents a vast space of code snippets composed of multiple assignment statements and conditionals. By engaging the programmer as an oracle to address incomplete parts of the loop, LooPy achieves a balance between automation and interactivity. Its ability to solve a wide range of synthesis tasks at interactive speeds makes it a practical tool for use cases requiring real-time feedback and adjustments.
These tools demonstrate progress in addressing the challenges of synthesis with control structures, but they also highlight trade-offs between usability, required user input, and performance.

\textbf{Large Language Models (LLMs)} have recently shown effectiveness in various software development tasks, including program synthesis\cite{jain2022jigsaw} and test generation\cite{xia2024fuzz4all,jiang2024generating}. By associating document text with code from a large training set, LLMs can generate program code from natural language prompts\cite{jain2022jigsaw,ugare2024improving,spiess2024quality,wang2023large}. Reusable API\cite{reuseableAPI} uses LLMs to generate APIs from code snippets collected from Stack Overflow and shows significant results in identifying API parameters and return types. However, they provide all the code needed and only require LLMs to create new APIs using existing implementations. Test-driven program synthesis remains an under-researched topic. How well do LLMs perform in generating entire APIs with just a few input/output examples that even end users can easily prepare? In this paper, we explore the application of LLMs in generating API implementations and finds that LLMs are effective in understanding test cases and generating viable APIs. We also show that LLMs are able to generate accurate APIs even with an incomplete set of test cases, which is very convenient. To the best of our knowledge, our work is the first systematic study of using LLMs with prompt engineering to synthesis complex APIs.

% \section{Threat to Validity}

\section{Conclusion}
In this paper, we present a novel approach of using large language models (LLMs) in API synthesis.  LLMs offer a foundational technology to capture developer insights and provide an ideal framework for enabling more effective API synthesis. The experimental results show that our approach overall outperforms FrAngel, a state-of-the-art API synthesis tool. Furthermore, our approach produces clear, maintainable code with meaningful variable names and relevant comments. Using minimal user output, our approach offers an easier way for API synthesis. We believe LLMs provide a very useful foundation for tackling the problem of complex API synthesis.






%
% ---- Bibliography ----
%
% BibTeX users should specify bibliography style 'splncs04'.
% References will then be sorted and formatted in the correct style.

\bibliographystyle{splncs04}
\bibliography{main}



% \section{First Section}
% \subsection{A Subsection Sample}
% Please note that the first paragraph of a section or subsection is
% not indented. The first paragraph that follows a table, figure,
% equation etc. does not need an indent, either.

% Subsequent paragraphs, however, are indented.

% \subsubsection{Sample Heading (Third Level)} Only two levels of
% headings should be numbered. Lower level headings remain unnumbered;
% they are formatted as run-in headings.

% \paragraph{Sample Heading (Fourth Level)}
% The contribution should contain no more than four levels of
% headings. Table~\ref{tab1} gives a summary of all heading levels.

% \begin{table}
% \caption{Table captions should be placed above the
% tables.}\label{tab1}
% \begin{tabular}{|l|l|l|}
% \hline
% Heading level &  Example & Font size and style\\
% \hline
% Title (centered) &  {\Large\bfseries Lecture Notes} & 14 point, bold\\
% 1st-level heading &  {\large\bfseries 1 Introduction} & 12 point, bold\\
% 2nd-level heading & {\bfseries 2.1 Printing Area} & 10 point, bold\\
% 3rd-level heading & {\bfseries Run-in Heading in Bold.} Text follows & 10 point, bold\\
% 4th-level heading & {\itshape Lowest Level Heading.} Text follows & 10 point, italic\\
% \hline
% \end{tabular}
% \end{table}


% \noindent Displayed equations are centered and set on a separate
% line.
% \begin{equation}
% x + y = z
% \end{equation}
% Please try to avoid rasterized images for line-art diagrams and
% schemas. Whenever possible, use vector graphics instead (see
% Fig.~\ref{fig1}).

% \begin{figure}
% \includegraphics[width=\textwidth]{fig1.eps}
% \caption{A figure caption is always placed below the illustration.
% Please note that short captions are centered, while long ones are
% justified by the macro package automatically.} \label{fig1}
% \end{figure}

% \begin{theorem}
% This is a sample theorem. The run-in heading is set in bold, while
% the following text appears in italics. Definitions, lemmas,
% propositions, and corollaries are styled the same way.
% \end{theorem}
% %
% % the environments 'definition', 'lemma', 'proposition', 'corollary',
% % 'remark', and 'example' are defined in the LLNCS documentclass as well.
% %
% \begin{proof}
% Proofs, examples, and remarks have the initial word in italics,
% while the following text appears in normal font.
% \end{proof}
% For citations of references, we prefer the use of square brackets
% and consecutive numbers. Citations using labels or the author/year
% convention are also acceptable. The following bibliography provides
% a sample reference list with entries for journal
% articles~\cite{ref_article1}, an LNCS chapter~\cite{ref_lncs1}, a
% book~\cite{ref_book1}, proceedings without editors~\cite{ref_proc1},
% and a homepage~\cite{ref_url1}. Multiple citations are grouped
% \cite{ref_article1,ref_lncs1,ref_book1},
% \cite{ref_article1,ref_book1,ref_proc1,ref_url1}.

% \begin{credits}
% \subsubsection{\ackname} A bold run-in heading in small font size at the end of the paper is
% used for general acknowledgments, for example: This study was funded
% by X (grant number Y).

% \subsubsection{\discintname}
% It is now necessary to declare any competing interests or to specifically
% state that the authors have no competing interests. Please place the
% statement with a bold run-in heading in small font size beneath the
% (optional) acknowledgments\footnote{If EquinOCS, our proceedings submission
% system, is used, then the disclaimer can be provided directly in the system.},
% for example: The authors have no competing interests to declare that are
% relevant to the content of this article. Or: Author A has received research
% grants from Company W. Author B has received a speaker honorarium from
% Company X and owns stock in Company Y. Author C is a member of committee Z.
% \end{credits}

\appendix
\section{Prompt}
\begin{figure}
	\centering
\begin{lstlisting}[style = prompt]
<Role>
    You are a software engineer. You will be given with an Java method signature,
    its return type, and a set of test cases as comments. Your task is to implement
    the Java method into a full implementation to pass the test cases.
</Role>
<Instruction>
    Use the following step by step instruction to solve the problem:
        Step 1 - Import Necessary Libraries
            (1) Identify and include any required import statements, such as Java standard libraries (java.util.*), or open-source libraries (e.g., Apache Commons, Guava) to simplify implementation.
            (2) Ensure that all dependencies are properly referenced to facilitate a smooth compilation.
        Step 2 - Define Any Required Helper Classes or Methods
            (1) If the method requires additional data structures or utility functions, define them before implementing the main method.
            (2) Ensure helper classes or methods are designed efficiently to support modularity and reusability.
        Step 3 - Create a Public Class with an Appropriate Name
            (1) Define a class that logically represents the method's functionality (e.g., APIUtility, StringProcessor, MathHelper).
            (2) Ensure the class is structured according to Java best practices.
        Step 4 - Understand the Problem Statement and Requirements
            (1) Analyze the method signature, return type, and test cases to infer the expected behavior.
            (2) Identify edge cases that may not be explicitly covered by the test cases but are crucial for correctness (e.g., null inputs, empty lists, boundary values).
        Step 5 - Implement the Method to Pass All Test Cases
            (1) Write a clear and efficient implementation of the method to meet the expected input-output requirements.
            (2) Optimize for performance and maintainability while ensuring correctness.
        Step 6 - Validate Edge Cases and Complete Implementation
            (1) Consider additional edge cases beyond the provided test cases (e.g., large inputs, invalid values).
            (2) Refactor the code if necessary to handle all identified cases.
        Step 7 - Write Unit Tests for Each Provided Test Case
            (1) Create a test method for each given test case using JUnit (or another testing framework).
            (2) Ensure test methods cover normal, edge, and erroneous inputs.
        Step 8 - Provide a Main Method for Execution and Validation
            (1) Implement a main method to run and validate the test cases.
            (2) Print test results to confirm correctness and ensure all test cases pass.
            (3) Verify that the code compiles and runs without errors.
</Instruction>
<examples> 
    Here is one example:
    <method>
        List<Integer> GetRange(int start, int end)
    </method>
    <TestCases>
        1. start = 10, end = 9 -> output: []
        2. start = 10, end = 10 -> output: []
        3. start = 10, end = 11 -> output: [10]
        4. start = 10, end = 12 -> output: [10, 11]
        5. start = -2, end = 2 -> output: [-2, -1, 0, 1]
    </TestCases>
    <output>
        List<Integer> GetRange(int start, int end) {
            ArrayList<Integer> range = new ArrayList<Integer>();
            for (int i=0; i+start < end; i++)
                range.add(Integer.valueOf(i+start));
            return range
        }
    </output>
</examples>
Now, give you the following method signature and test cases:
<method>
     double ellipseArea(Ellipse2D ellipse)
</method>
<TestCases>
    1.  Ellipse2D.Double(12.3, -45.6, 7.8, 9) -> 3.9 * 4.5 * Math.PI
    2.  Ellipse2D.Double(12.3, -45.6, 7.8, 2) -> 3.9 * Math.PI
    3.  Ellipse2D.Double(12.3, -45.6, 2, 7.8) -> 3.9 * Math.PI
    4.  Ellipse2D.Double(12.3, -45.6, 2, 2) -> Math.PI
</TestCases>

Please output the results. Only output complete code.
 \end{lstlisting}
 \caption{An example of our prompt input to LLMs}
		\label{fig:figure4}
\end{figure}

\end{document}
