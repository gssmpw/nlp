\section{Related Work}
\paragraph{Fine-Tuning.} Fine-tuning refers to the further updating of a pre-trained model's parameters to adapt it to specific tasks **Devlin, "BERT Pre-training of Deep Bidirectional Transformers for Language Understanding"**__**Peters et al., "Deep Contextualized Word Representations"**. DisCup __ combines a frozen causal language model with an attribute discriminator to optimize control prompts via unlikelihood training. InstructCTG __ achieves controllable text generation by converting constraints into a natural language instruction dataset and fine-tuning the language model on an augmented dataset. The fine-tuning approach balances adaptability and resource efficiency, making it a common choice for enhancing model performance on specific tasks.

\paragraph{Latent Space Manipulation.} Latent space manipulation aims to control text generation by adjusting internal model representations **Goyal et al., "PriorControl: Controllable Text Generation via Reversible Transformations"**__**Liu et al., "MAGIC: Multi-Aspect Controllable Text Generation"**. PriorControl __ uses probability density estimation methods in latent space to effectively manage complex attribute distributions through reversible transformations. MAGIC __ employs disentangled counterfactual augmentation to handle multi-aspect controllable text generation, aiming to balance attribute correlations during training and enhance attribute control during inference by generating counterfactual features in the attribute latent space. These approaches demonstrate flexibility in controlling generation without altering model architecture.


\begin{figure*}[h]
    \centering
    \includegraphics[scale=0.358]{./figs/pipeline.pdf}
    \caption{Illustration of our proposed framework. Our framework extends the traditional LoRA by integrating multiple LoRA modules and employs a learnable gating function to dynamically combine multiple LoRA modules. We use the aspect identifier as the input of the gating function to learn unique parameters for each aspect. $X_{a^t_{\mu}}$ represents the input sequence containing attribute $a^t_{\mu}$ and $H_{a^t_{\mu}}$ is the output hidden state. Only the parameters of LoRAs and the gating function are updated during training.}
    \label{fig:1}
\end{figure*}


\paragraph{Decoding-Time Intervention.} Decoding-time intervention controls the attributes of generated text by manipulating the model's logits or probability distribution during generation. PPLM __ controls text attributes by iteratively adjusting the hidden layer activations of GPT-2 using the gradient of attribute classifiers. GeDi __ uses a discriminator to guide the language model decoding to calculate the classification probability for each next token, effectively controlling text generation. Air-Decoding __ reconstructs attribute distributions to balance the weights between attribute and non-attribute words, effectively generating more fluent and controllable text. Decoding-time intervention enables controllable text generation without training models and is more interpretable.