This section reviews related work on analogy in education, evaluating analogy in HCI, analogy-making with LLMs, and \chirev{LLM-assisted educational systems.} 
\subsection{Analogy in Education}
Analogies help humans understand complex concepts by linking them to familiar ones, making them a valuable tool in educational contexts. 
Many studies~\cite{thagard_analogy_1992,gick_analogical_1980,gick_schema_1983, brown_analogical_1989} have investigated analogical problem solving, where students of various ages solve unfamiliar problems using well-designed analogies. 
Through observational feedback and statistical analysis, researchers have established frameworks and several guidelines for using analogies in education. 
For example, as discussed by~\cite{gick_analogical_1980}, the source of the analogy would share similar relationships with the target, yet originate from a semantically distant field. 
However, such lab studies often involve experimenters posing problems, with students merely solving them without instructional guidance~\cite{brown_analogical_1989}, which diverges from real classroom learning.
Therefore, further research~\cite{vendetti_analogical_2015,richland_analogy_2004,treagust_science_1992,oliva_teaching_2007} have investigated how teachers and students engage with analogies in classroom settings, leading to nuanced insights on the influence of students' age and background and teachers' strategies.
% For example, the age and background of students~\cite{vendetti_analogical_2015}, along with the teacher's strategies~\cite{richland_cognitive_2007}, influence how frequently and effectively analogies are used in the classroom.

Although previous studies have explored the characteristics and use of analogies in education, they have not examined those generated by LLMs, which is crucial given the growing importance of LLM-assisted education~\cite{gao2024fine, Lyu2024evaluating}.
Our work fills this gap by leveraging LLMs to generate analogies tailored to specific education needs, incorporating established characteristics from prior literature and our interviews.
We design human-subjective studies to evaluate their effectiveness in problem-solving tests and classroom environments following prior research.
% \chirev{Finally, we developed a system to examine how LLMs support educators in constructing analogies for real-world applications.}

\subsection{Evaluating Analogy in Human-Computer Interaction}
Analogy has long been studied in HCI for its effectiveness in various context, including algorithms improvement~\cite{bureaucracy2020Pääkkönen,streetlevel2019Alkhatib}, cancer communication~\cite{capturing2024hnatyshyn}, narrative framing~\cite{reelframer2024wang}, enhancing deliberation~\cite{help2024yeo}, communicating standardized effect sizes~\cite{putting2022kim}, and sensemaking of LLM responses~\cite{supporting2024gero}.

Two key research directions about analogies in HCI are for enhancing numerical comprehension through data analogy and fostering creativity.
Data analogies link abstract data to familiar concepts to improve understanding. Researchers evaluate these analogies using controlled experiments and assess effectiveness through subjective ratings like helpfulness~\cite{toput2018riederer, improving2018hullman, generating2016kim, spatharioti_using_2024, chen_beyond_2024}, estimation errors~\cite{toput2018riederer, improving2018hullman}, and correlations between model and human ratings~\cite{spatharioti_using_2024}.
Analogies also facilitate scientific discovery and design. 
In scientific discovery, evaluations involve coding analogy types~\cite{solvent2018chan, kang_augmenting_2022}, calculating similarity metrics~\cite{solvent2018chan}, and conducting think-aloud sessions with scientists~\cite{kang_augmenting_2022}. 
For creative design, analogies are assessed by novelty~\cite{searching2014yu, bilogically2023zhu, bidtrainer2024chen}, quality~\cite{distributed2014yu, bidtrainer2024chen}, relevance and domain distance~\cite{analogymining2018Gilon}, feasibility~\cite{bilogically2023zhu}, and rationality~\cite{bidtrainer2024chen}.
Recently, Ding et al.~\cite{ding_fluid_2023} explored GPT-3's capacity to augment cross-domain analogical reasoning, finding it helpful for creative problem reformulation despite the risks of harmful content.

However, there has been limited exploration of analogy search in HCI for education~\cite{kumar2015stickipedia}. 
While researchers have adopted LLMs to help students and teachers generate novel analogies~\cite{bhavya2024analego}, systematic evaluations of their effectiveness in educational settings are lacking. 
Given the unique cognitive demands of education, existing assessments~\cite{ding_fluid_2023} may not be directly applicable. 
Our work aims to address this gap and offer insights into analogy generation for education.



\subsection{Analogy-making with Language Models}
Analogy is vital for human cognition and has attracted considerable interest from the AI research community. 
Traditionally, studies on analogy-making in AI have concentrated on creating word analogies (\eg, ``king is to man as queen is to woman'') using smaller language models (LMs), e.g., BERT~\cite{devlin_bert_2019} and GPT-2~\cite{radford_language_2019} trained on specific datasets~\cite{turney_combining_2003,mikolov_linguistic_2013,boteanu_solving_2015,gladkova_analogy-based_2016,chen_e-kar_2022, yuan_analogykb_2023}.
With the advancement of LLMs~\cite{ouyang_training_2022,team_gemini_2023,touvron_llama_2023,openai_gpt-4_2023}, there has been a shift toward generating natural language analogies, \ie, free-form analogies~\cite{bhavya_analogy_2022,webb_emergent_2022,ding_fluid_2023,wijesiriwardene_analogical_2023,jiayang_storyanalogy_2023,hu_-context_2023,sultan_parallelparc_2024} and forming structural analogies~\cite{sultan_life_2022,yuan_beneath_2023}.
Researchers typically design prompts manually for free-form analogies to guide LLMs in \chifinal{generating analogies~\cite{bhavya_analogy_2022, webb_emergent_2022}.
For example, Bhavya et al.~\cite{bhavya_analogy_2022} constructed a new dataset including standard science analogies and science analogies from academics and adopted prompt engineering to ask LLMs to generate analogies. 
The results show that LLMs are sensitive to prompt design, temperature, and injected spelling errors, particularly the distinction between questions and imperative statements.
We followed their optimal prompt format for our generation process.
}
%Recent studies on structural analogies employ LLMs to identify mappings between concepts across domains based on relational structures~\cite{yuan_beneath_2023}.
\chirev{
For evaluation of the generation quality of analogy, previous studies have relied on annotators manually evaluating analogies according to established principles of analogy cognition~\cite{sultan_life_2022}. 
}
%\sout{traditional work in the AI research community focuses on manually constructing benchmarks from student exams, formatting them as question-answering tasks to test the capabilities of LMs to recognize word analogies~\cite{mikolov_linguistic_2013,boteanu_solving_2015,gladkova_analogy-based_2016,chen_e-kar_2022}.
%For example, Chen et al.~\cite{chen_e-kar_2022} collect 1,655 analogical reasoning problems sourced from publicly available Civil Service Examinations of China to evaluate the performance of LMs.}
%When addressing free-form and structural analogies, verifying the correctness automatically becomes challenging.
%Therefore,
% For example,  et al.~\cite{_life_2022} described a method where annotators categorize generated analogies into five types: Not analogy, Self-analogy (where entities and their roles are identical), Close-analogy (topics are similar and entities are from related domains), Far-analogy (covering unrelated topics with different entities) and Sub-Analogy (only a part of one entity is analogous to a part of the other).
%give the entity and relation similarity to model the good analogy is with low entity similarity and high relation similarity.
%However, much of this research has focused primarily on assessing if LLMs can produce relevant analogies through human evaluation, without fully exploring their practical use in real-world applications.

In contrast to these approaches, our study is pioneering in investigating how analogies generated by LLMs can help students understand scientific concepts. 
We analyze the characteristics of analogies in educational settings through literature reviews and interviews and incorporate them into prompts for generation. 
Then, we use LLMs to generate educational analogies and evaluate them in real tests, classroom practice, \chirev{and a practical system}.
% \chirev{We also developed a system and conducted a user study to explore the real-world value of LLMs in assisting educators with constructing analogies.}

\subsection{\chirev{LLM-assisted Educational Systems}}
% \ysy{\todo @ysy and @szk add related work from NLP domain and HCI domain}
\chirev{
With the rapid advancement of LLMs, researchers are exploring their potential to develop efficient and practical systems that support students and teachers in educational tasks~\cite{kasneci2023chatgpt,yan2024practical}. 
For students, many studies have focused on creating intelligent tutoring systems powered by LLMs. 
Examples include enabling fully autonomous self-learning pipelines to support self-regulated learning~\cite{gao2024fine} and developing and evaluating LLM-based learning assistants in classroom settings~\cite{kazemitabaar_codeaid_2024,Lyu2024evaluating}.
For teachers, several LLM-based systems are designed to effectively monitor and analyze students' learning activities~\cite{ngoon_classinsight_2024,cflow,vizgroup}.
In addition, researchers aim to assist teachers in creating diverse teaching materials, such as lesson plans~\cite{lessonplanner2024uist}, diagrammatic problems~\cite{edgeworth}, and reading quizzes~\cite{readingquizmaker}.

Our work explores a novel aspect of LLM-driven education: evaluating the effectiveness of LLMs in generating teaching analogies. 
One preliminary research has initially explored generating educational analogies~\cite{bhavya2024analego}, while its system design lacks the support of empirical evidences and fails to address teachers' needs.
Instead, we first conducted a two-stage study to gain insights and empirical evidence and identify needs for teachers and students. 
We then developed and tested a system to support teachers in creating and refining analogies for lesson preparation and discussed future integration with diverse LLM-based educational tools for various users.
% Through two studies, we examined LLM-generated analogies' impact with and without teacher intervention, which provided insights into potential applications for students' self-learning and teachers' lesson preparation. 




% For example, Abd-Alrazaq et al.~\cite{abd2023large} use LLMs to produce clinical case studies, act as virtual test subjects or patients, accelerate research, develop course plans, and provide personalized feedback and assistance.  
% Lee et al.~\cite{lee-etal-2023-peep} design a situational dialogue-based chatbot to help foreign students learn English.
}
