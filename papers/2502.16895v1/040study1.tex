In this section, we detail the participants, data preparation process, stimuli, procedure, and results analysis of Study I.

% We conducted our first study (Study I) to evaluate the effectiveness of LLM-generated analogies in helping students grasp scientific concepts.
% We iteratively refined the prompting strategy to produce high-quality analogies of the scientific concepts that the students were expected to learn.
% The students were then divided into two groups: one with textbook explanations, and another with both LLM-generated analogies and textbook explanations.
% They then completed an exam about scientific concepts to be learned. 
% We evaluated the effectiveness of the analogies by comparing test accuracy and subjective ratings across the groups.

\subsection{Participants} 
\label{sec:study1_participants}
Two classes of freshmen, totaling 49 \chifinal{Chinese} students from a Chinese high school with which we have a scientific research collaboration, participated in Study I. 
Their ages ranged from 15 to 17, with 26 males and 23 females. 
They had recently started high school physics and biology courses. 
Their entrance exam scores and classroom performance suggested a normal cognitive level, and we did not pre-select students based on their abilities.





\subsection{Data Preparation}
\label{sec:study1_data_preparation}

\begin{figure*}[tb]
    \centering
    \includegraphics[width=0.9\linewidth]{figure/revpipeline.pdf}
    \caption{The pipeline for analogy generation to explain scientific concepts for data preparation in Study I.}
    \Description{This figure presents our pipeline for generating and refining analogies to explain scientific concepts, as used in Study I for data preparation. The process begins with collecting scientific concepts and generating initial analogies using LLMs. Errors are then annotated, and the analogies are refined with revised prompts. Next, the most effective analogy is selected automatically, and patterns are annotated for the final selection.}
    \label{fig:pipeline}
\end{figure*}
\input{table/prompt}
\input{table/annotation_result}
\chirev{As shown in Fig.~\ref{fig:pipeline}, we began by manually selecting ten scientific concepts from physics and biology in Chinese high school textbooks. 
Next, we used the advanced LLM, GPT-4o~\cite{openai_gpt-4_2023} (temperature = 0.7) to generate three analogies for each concept with three principles summarized from education research~\cite{hesse1959defining,gentner1983structure,gentner2017analogy} incorporated in the prompt. 
%Tab.~\ref{tab:instruction_prompt} (I) provides the prompt template used for this task.
Three authors independently identified and annotated errors in generated analogies. 
After repeated discussion, the annotators classified the errors into four types: two related to factuality and two to consistency, as follows.
\begin{itemize}
    % \item \textbf{Concept Paradox}: The analogy inaccurately represents the scientific concept, conflicting with established physical phenomena and commonsense knowledge.
    \item \textbf{Analogy Object Paradox}: The objects of the analogy do not align with physical laws or commonsense knowledge.
    \item \textbf{Inappropriate Analogy}: The analogy fails to accurately mirror the concept, leading to misconceptions.
    \item \textbf{Object Confusion}: The same analogy objects are assigned different roles or functions across various contexts.
    \item \textbf{Logical Contradiction}: The syntax within a sentence or paragraph contradicts itself.
\end{itemize}

%\input{table/selection}
The inter-rater reliability among annotators reached Fleiss' Kappa of 0.83 for Analogy Object Paradox, 0.94 for Object Confusion, and 1 for the remaining error codes.
Error annotations in subsequent steps achieved similar reliability.
As shown in the first row of Tab.~\ref{tab:error_rates}, out of the 30 generated analogies, 16 were correct. 
The remaining analogies frequently exhibited the first three error types with one analogy containing logical contradiction.
From these errors, we derived four new principles and added them to the prompt template (Tab.~\ref{tab:instruction_prompt} I) to help GPT-4o avoid these errors.
% We then refined the prompt by incorporating four new principles to help GPT-4 avoid these errors. 
% The revised prompt template is shown in Tab.~\ref{tab:instruction_prompt}.
However, even with these improvements, GPT-4o still made errors. 
To address this, we followed prior AI research~\cite{pan2023automatically,yuan-etal-2023-distilling,liu2024large} and allowed GPT-4o to automatically select the best of the three candidate analogies generated for each concept.
\chifinal{The prompt for analogy selection is shown in Tab.~\ref{tab:instruction_prompt} II.}
As shown in the third row of Tab.~\ref{tab:error_rates}, enabling the model to self-correct improved the accuracy of the analogies.}
%: 


%\paragraph{Step 1: Scientific Concept Collection}
%We first manually selected ten scientific concepts from physics and biology, as outlined in a Chinese high school textbook, which the students had not yet studied. We also collected five exam questions per concept from the books to assess the students' understanding of these concepts in our further experiments.

%\paragraph{Step 2: Plain Analogy Generation}
%To align with the educational process for students, we requested the most advanced LLM, \ie, GPT-4o~\cite{openai_gpt-4_2023}, to generate free-form analogies for each scientific concept. 
%We adhered to the prompt settings described in \cite{bhavya_analogy_2022} for the generation process. 
%Tab.~\ref{tab:instruction_prompt} (I) shows the prompt template used for generating. And examples of these analogies are presented in Fig.~\ref{fig:example}.
%\ysy{We did not include demonstrations in the instruction prompt because demonstrations could induce bias in GPT-4~\cite{pmlr-v139-zhao21c}, thereby degrading the generalization of analogy generation.}
%Specifically, to facilitate more effective analogy generation by GPT-4, we incorporated principles from analogy cognition theory~\cite{hesse1959defining,gentner1983structure,gentner2017analogy} into the instruction prompt. 
%Additionally, we provided GPT-4 with textbook content relevant to the scientific concepts to tailor the analogies to students' learning progress.
%We did not include examples in the instruction prompt because there is no high-quality analogy dataset in the educational domain from which students can learn. 
%Examples could induce bias in GPT-4~\cite{pmlr-v139-zhao21c}, thereby degrading the generalization of analogy generation.

%\paragraph{Step 3: Error Annotation for Generated Analogy}
%\sout{Based on the prompt in step 2, we set the temperature to 0.7 for GPT-4 and generated three analogies for each concept.} 
%Three authors served as annotators to evaluate the quality of the generated analogies by annotating the errors. Initially, the annotators followed two common categories of error classification typically used in LLM outputs~\cite{jang-etal-2022-becel,gekhman-etal-2023-trueteacher,chuang2024dola}: 1) Factual Accuracy: the appropriateness of the analogies for the given concept; 2) Consistency: the coherence of objects within the analogies. 
%The three annotators independently applied this classification to their annotations. After annotating 10 analogies, a discussion about error codes and annotations was conducted. Following three discussions, the annotators categorized the error codes into five types: the first three are factual errors, while the last two are inconsistency errors.


%After repeated discussions, the inter-rater reliability among annotators reached Fleiss' Kappa of 0.83 for Analogy Object Paradox, 0.94 for Object Confusion, and 1 for the remaining error codes.
%As shown in the first row of Tab.~\ref{tab:error_rates}.
%Despite the powerful capabilities of GPT-4, it fails to generate appropriate and satisfactory analogies for scientific concepts with our initial principles. 
%Specifically, concerning the factual aspect, GPT-4 attempts to produce analogies that adhere to physical logic. 
%However, the objects within these analogies often contradict physical laws or commonsense knowledge, resulting in errors related to the analogy objects.

%Moreover, in the generated analogies, the same objects are inconsistently assigned different roles or functions in various contexts, indicating that GPT-4 suffers from object confusion.
%For example, when generating analogies for the photoelectric effect, GPT-4 uses a trampoline. 
%In this analogy, the trampoline represents the metal surface, and children represent electrons. 
%The model initially makes an analogy between the force with which a worker presses the trampoline and the frequency of light, \eg, \textit{``First, if someone presses down on a diving board with a very small force (low-frequency light), the diving board will not have enough elasticity to bounce you back up.''}
%However, in later descriptions, this force is equated to light intensity, \eg, \textit{``The greater the force applied (the higher the intensity of light), the more people will be bounced off the diving board in a given period.''}.



%\paragraph{Step 4: Analogy Generation with Revised Prompt and Second Round of Error Annotation}
%In Step 3, we discovered that GPT-4 cannot generate appropriate analogies directly, as it produces several errors. 
%Therefore, we refined the prompt by incorporating new principles to guide GPT-4 in avoiding these errors, enhancing the quality of the generated analogies. 
%Tab.~\ref{tab:instruction_prompt} (II) details the revised prompt template.
%The three annotators applied the error codes used in Step 3 to the analogies generated in this round.
%After two rounds of discussions, the inter-rater reliability among annotators reached Fleiss' Kappa of 0.84 for Analogy Object Paradox, 0.94 for Inappropriate Analogy, 0.92 for Object Confusion, and 1 for the remaining error codes.
%Based on Tab.~\ref{tab:error_rates}, we can find that the revised prompt enhances the coherence of objects within analogies, thereby minimizing confusion and logical contradictions during generation. 
%However, to accurately identify analogous relationships, the model may force objects to conform to patterns that contradict physical laws or common sense, thereby intensifying the analogy object paradox.

%\paragraph{Step 5: Automatic Analogy Selection from LLMs}

%From step 4, it is evident that the model is still prone to errors despite providing clear and instructive guidelines. 
%Previous research in the AI community has demonstrated that models can select the optimal result from multiple outputs through a process known as self-correction~\cite{pan2023automatically,yuan-etal-2023-distilling,liu2024large}. 
%Therefore, we allowed GPT-4 to choose the best result according to the guidelines from three analogies generated for each concept. 

%Despite clear guidelines, the model in Step 4 still makes errors. 
%To address this, we followed the previous research in the AI community~\cite{pan2023automatically,yuan-etal-2023-distilling,liu2024large} and allowed GPT-4 to choose the best result according to the guidelines from three candidate analogies generated for each concept. 

%The chosen prompts are displayed in Tab.~\ref{tab:instruction_prompt} (III). 
%Results in Tab.~\ref{tab:error_rates} show that enabling the model to self-correct enhances the accuracy of the analogies. 
%However, the model struggles with issues such as object confusion. 
%We hypothesize that this difficulty arises because the objects in the analogies are described in disparate text sections, requiring the model to use contextual cues for differentiation. 
%This necessitates a robust capability for understanding long contexts~\cite{li2024can}. 
%Additionally, as GPT-4 is a decoder-only model, its unidirectional attention mechanism may lead to inconsistency~\cite{}\ysy{\todo Add citations}.

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.9\linewidth]{figure/revexample.pdf}
    \caption{\chirev{The examples of final analogies generated from GPT-4o after iterative generation and annotation in Study I.}}
    \Description{This figure shows examples of analogies generated by GPT-4o after multiple iterations. In the ``Correct'' Analogy category, a satisfying analogy for ``immune response" compares the body to a city protected by a security system. An imaginative analogy for ``wave-particle duality" likens it to an amusement park ride, though less accurate. A non-analogy for the ``Doppler effect" fails to form a true analogy. In the Incorrect Analogy category, the analogy for ``nuclear fission'' confuses objects by comparing neutrons to a knife and seeds, leading to inappropriate and paradoxical explanations.}
    \label{fig:example}
\end{figure*}

%\paragraph{Step 6: Pattern Annotation and Final Analogy Selection}

% From Step 5, we obtained ten concepts with ten analogies generated and then selected by the LLM. 
% We re-evaluated the selected concepts and their corresponding analogies to ensure they were suitable for students to learn through analogies. 
Finally, we further categorized ten analogies selected by the LLM into four distinct groups, as illustrated in Fig.~\ref{fig:example}:
\begin{itemize}
    \item \textbf{Correct and Satisfying Analogy}: Analogies in this category are error-free. 
    The objects in these analogies are realistic, align with common sense, and adhere to physical laws, effectively and vividly illustrating scientific concepts.
    \item \textbf{Correct Analogy with Imagination}: Analogies in this category require envisioning non-existent objects or processes to explain a concept. While logically sound, they demand creative thinking and imagination from students.
    \item \textbf{Correct Non-Analogy}: This is more akin to an example-based explanation than a true analogy and is not generally recognized as an analogy in cognitive science.
    \item \textbf{Incorrect Analogy}: This category includes analogies exhibiting previously identified error types. 
    These analogies are inappropriate for students to refer to, as they do not accurately convey the intended concept.
\end{itemize}


The analogies for the five biological concepts fell under the \textbf{Correct and Satisfying Analogy} category. 
In contrast, the five physical concepts were distributed as follows: three under \textbf{Incorrect Analogy}, one under \textbf{Correct Analogy with Imagination}, and one under \textbf{Correct Non-Analogy}.

We limited the number of concepts and analogies to four to avoid overwhelming students with too much new knowledge in further tests. 
To ensure a balance across subjects and categories, we specifically selected two biological concepts categorized as \textbf{Correct and Satisfying Analogy} and two physical concepts, one each under  \textbf{Correct Analogy with Imagination} and \textbf{Incorrect Analogy}, as shown in the right side of Fig.~\ref{fig:example}.
% , with the final selection and error rates detailed in Tab.~\ref{tab:error_rates}. 
We excluded the one \textbf{Correct Non-Analogy} from further consideration, as it is not typically classified as an analogy.


\subsection{Stimuli}
\label{sec:study1_stimuli}
Since students do not have access to electronic devices and are more familiar with and serious about traditional classroom tests, we conducted offline tests in class.

Based on the data preparation, we printed a test paper and two reference materials. 
The test paper comprises 20 multiple-choice questions, with 5 questions assigned to each of the following 4 concepts: Nuclear Fission and Fusion, Wave-Particle Duality, Blood Sugar Regulation, and Immune Response. 
In addition to selecting answers, students are required to complete a 5-point Likert scale rating for self-confidence to measure their subjective satisfaction.
The first reference material provides textbook explanations for the four concepts, while the second adds LLM-generated analogies before the explanations.
% The first reference material consists of textbook explanations for the four concepts. 
% The second reference material includes LLM-generated analogies followed by textbook explanations for each concept.
The test paper and the reference materials present the concepts in the same order. 
They are highlighted in bold, making it easier for students to find and connect the information with the questions.

\subsection{Procedure}
Then, we conducted an in-class test for students in two classes lasting 30 minutes. 
%The session began with a 5-minute introduction where we introduced ourselves and explained that the test was part of a study on intelligent education assisted by AI. 
We first gave a 5-minute introduction for the background of our test.
%After the introduction, we divided the students into two groups based on their seating, distributed two sets of reference materials to each group, and instructed them to indicate their group number on the test paper (either 1 or 2). 
After the introduction, we randomly divided the students into two groups and distributed two sets of reference materials to each group. 
% We instructed them to indicate their group number on the test paper (either 1 or 2). 
We clarified the meaning of self-confidence rating. 
Under our supervision, each student then independently completed the test using the materials provided in 25 minutes.
%Before the test, we allocated an estimated 20 minutes for completion. Observing the students' progress, we extended the time by five minutes, ensuring they had a sufficient 25 minutes to finish.

After the test, we interviewed four students from each group, totaling eight participants.
Each 2-minute interview earned participants a \$2 gift card.
We asked them about any difficulties during the test and, for those with analogies in their materials, how these helped them answer questions alongside textbook concepts.

\subsection{Results Analysis}
\label{sec:study1_results_analysis}
Our selection criteria excluded test papers with incomplete answers. 
After examining the 49 test papers, we excluded 5 that had more than 5 unanswered questions. 
The remaining 44 fully completed papers, 22 from each group, were considered valid data.
Our analysis followed a top-down approach, starting from the overall test (20 questions) and proceeding to finer levels: subject (10 questions each), concept (5 questions each), and individual questions. For further comparison, students' responses were averaged across questions at the first three levels.

% We computed descriptive statistics to gain overall insights and performed statistical tests to determine significance at each level. 
% We identify differences based on both the descriptive results and p-values.
% Given the discrete nature of accuracy and subjective ratings, we employed the exact Wilcoxon-Mann-Whitney test (using the R package \texttt{coin}), solving for ties in the data as well. 
% The test's null hypothesis assumes that the distribution of responses is the same in both groups; thus, a small p-value suggests a statistical difference in student understanding related to the use of analogies. 
% For individual question accuracy, where student responses followed a binomial distribution, we employed Fisher's exact test (using the R package \texttt{stats}). 
% This test assumes that the proportion of correct responses is consistent across groups, with a small p-value indicating a potential association between analogies and student accuracy. 
% We also calculated Kendall's tau correlation coefficient for all pairs of participant response types within each concept and group, as it is suitable for ordered categorical variables and effectively handles ties. 
% A significance level of $p$<0.05 was used for all tests.

\chirev{
We computed descriptive statistics to gain overall insights and performed statistical tests to determine significance at each level. 
The experiment results include the students' answer accuracy and confidence ratings, both can be regarded as ordinal categorical variables.
Thus, we mostly employed the exact Wilcoxon-Mann-Whitney test (using the R package \texttt{coin}) to evaluate the significance of difference between the two groups. 
For one exception, we employed Fisher's exact test (using the R package \texttt{stats}) on students' answer accuracy at the individual question level, where the accuracy is binary (either 0 or 1). 
In any of the tests, a small p-value indicates a potential association between the use of LLM generated analogies and the students’ outcomes, and a significance level is defined as $p$<0.05 in all tests.
We also calculated Kendall's tau correlation coefficient to assess the relationship between students’ objective answer accuracy and subjective confidence ratings within each concept and group.
% Considering the potential for Type II errors due to our small sample size, we base our analysis on both descriptive statistics and p-values.
}

We summarize our findings as follows. The two groups are referred to as Group T (Textbook explanation only) and Group L (Textbook explanation with LLM-generated analogy), while the interviewed students are denoted as T1-T4 and L1-L4.
% We analyzed 49 test papers and found that 5 had more than 5 unanswered questions. 
% The remaining 44 fully answered papers, with 22 from each group, were considered valid data for further analysis. 
% We first computed descriptive statistics for all groups and observed distinct trends in the responses across different concepts and questions. 
% Then, we employed the exact Wilcoxon-Mann-Whitney test to evaluate the impact of analogies on students' understanding of individual concepts by averaging the responses for the five related questions. 
% % We compared the accuracy rates across concepts using the Friedman test followed by a Nemenyi post-hoc analysis. 
% % We regarded the number of correct answers of each participant and the Likert scale results as ordered categorical variables.
% We calculated Kendall's tau correlation coefficient for all pairs of the participant response types in each concept and group.
% For statistical tests on responses to individual questions, we then conducted further analyses: Fisher's exact test on the association of group and accuracy to determine the impact of analogies on whether students answered individual questions correctly, and exact Wilcoxon-Mann-Whitney tests on the association between group and self-confidence and perceived helpfulness to assess how analogies influenced students' subjective satisfaction.
% % we performed Fisher's exact test to determine the impact of analogies on whether students answered individual questions correctly.
% % We also conducted exact Wilcoxon-Mann-Whitney tests on self-confidence and perceived helpfulness to assess how analogies influenced students' subjective satisfaction with specific questions. 
% % The exact Wilcoxon-Mann-Whitney test is also used to evaluate the impact of analogies on students' understanding of individual concepts by averaging the responses for the five related questions.
% We considered statistical significance at a significance level of $p$ < 0.05 for all the cases.
% We summarize the following findings based on the statistical results of student tests and interviews.
% The two groups are denoted as Group T (Textbook explanation only) and Group L (Textbook explanation with LLM-generated analogy), and students interviewed are denoted as T1-T4 and L1-L4 in the following discussion.


\begin{figure}[t]
    \centering
    \includegraphics[width=1\linewidth]{figure/study1_acc_conf_overall.pdf}
    \caption{Boxplots showing the distribution of answer accuracy and confidence ratings for the two groups and comparing the overall test results and the two subjects. ``M'' represents Mean, ``p'' represents the significance of the association between accuracy and group, determined by the exact Wilcoxon-Mann-Whitney test, and * represents significance ($p$<0.05).}
    \label{fig:study1_acc_conf_overall}
    \Description{This figure includes boxplots showing the distribution of answer accuracy and confidence ratings. The plot has facets of accuracy and confidence rating. In each facet, X-labels are all questions, biological questions, and physical questions. The two boxplots of two groups are juxtaposed inside each label. In the accuracy facet on the left, all boxplots share the same 0 to 1 y-axis. Physics questions have overall higher accuracy with no significant difference between groups. Biology questions have overall lower accuracy with significant differences between groups. All questions are thus in the midst with no significant difference between groups. In the confidence rating facet on the right, all boxplots share the same 0 to 5 y-axis. Physics questions have a mildly higher confidence rating at each quantile while biology questions are mildly lower, but the boxplots from different labels largely overlap. All differences in confidence ratings between groups are significant.}
\end{figure}



\textbf{LLM-generated analogies generally aid problem-solving and have a greater impact on biological concepts than physical concepts.}
As shown in the left of Fig.~\ref{fig:study1_acc_conf_overall}, the overall accuracy for physics questions is higher, while a significant association exists between accuracy and group for biology questions ($p$ = 0.042).
Examining individual questions (Fig.~\ref{fig:study1_acc_question}), there are three questions within the two biological concepts where Group L's accuracy largely exceeds Group T's by more than 0.25.
Besides, a strict Fisher’s exact test shows marginally significant associations between accuracy and group for two questions (Q1 and Q3 of “Immune Response”) ($p$ = 0.067).
However, no such clear difference is seen for the physics questions.
% As shown in Fig.~\ref{fig:study1_acc_question}, Although for all individual questions, there is no significant association between accuracy and group, there are three questions for the two biological concepts where the accuracy of Group L exceeds that of Group T by more than 0.25. 
% Associations between analogy and group in two questions (Q1 and Q3 of ``Immune Response'') are marginally significant ($p$ = 0.067). 
% However, there are no questions with a clear difference for the physical concepts.
In the interviews, all four students from Group L (L1-4) coincidentally explained the role of analogies based on subjects.
They noted that explanations for physical concepts are relatively concise, allowing them to understand directly without analogies. 
In contrast, the lengthy explanations for biological concepts made analogies helpful to ``\textit{get an overview and quickly identify key terms}'', as indicated by L4.
% \chirev{Besides, L1-L4 all stated that biological analogies are of higher quality than physical analogies.}
% Similarly, T1 described the biggest difficulty in answering questions as ``\textit{the concept descriptions of biology are so long that I have no desire to read them.}''
% For these two questions, Q1 focused on the core concept, which was clearly illustrated in the provided analogy. 
% Q3 involved knowledge about autoimmune diseases, which wasn't covered in our provided explanation, leaving Group T confused; T2 even asked, ``\textit{What is the autoimmune disease?}'' during the interview. 
% Group L, however, could identify the correct answer by choosing the option inconsistent with the analogy without considering the other unfamiliar things.

\begin{figure}[t]
    \centering
    \includegraphics[width=1\linewidth]{figure/study1_acc_question.pdf}
    \caption{Heatmap of accuracy differences between Group L and Group T for individual questions, with blue indicating higher accuracy for Group L and red for Group T. Each cell contains a bar chart of the respective accuracies and a p-value representing the significance of the association between accuracy and group, determined by Fisher's exact test. For two of the questions, since all students in both groups answered them correctly, Fisher's exact test is not applicable, and $p$ = N/A.}
    \label{fig:study1_acc_question}
    \Description{This figure includes a Heatmap of accuracy differences between Group L and Group T for individual questions. The cells of heatmap are vertically divided by concepts, and then horizontally divided by the question ID in each concept. For ``Immune Response'' and ``Sugar regulation concept'', student accuracy shows much more significant differences in part of the questions (Q1, Q3, Q4 of ``Immune Response'' and Q3 of ``Sugar regulation concept''). For most questions, Group L had higher accuracy, except for Q3 ``Immune Response'' in which Group L showed clearly lower accuracy. However, for Wave-Particle Duality and Nuclear Fission and Fusion concept, the comparison between student accuracy from each group is ambiguous, as all the superiority or inferiority in accuracy is negligible considering the p-value. }
\vspace*{-10pt}
\end{figure}

\textbf{LLM-generated analogies may negatively affect students' understanding without teacher intervention due to \chirev{errors and missing information in analogies and students' incorrect learning strategies with over-reliance}.}
\chirev{Although some students in Group L identified the analogy of ``Nuclear Fission and Fusion'' as an \textbf{Incorrect Analogy} and noted specific LLMs' hallucination during the interview, Group L's accuracy on all five questions was no higher than Group T's (Fig.~\ref{fig:study1_acc_question}).}
% Therefore, we believe that incorrect analogies may harm students' understanding.
Furthermore, we also found that, although the \chirev{\textbf{Correct and Satisfying Analogies}} slightly improved overall answer accuracy, they could also harm students' understanding in some cases due to missing information.
For example, Group L's accuracy for Q4 of ``Immune Response'' was 0.27 lower than Group T's.
Their incorrect answer choices suggest that some students believed that ``plasma cells can recognize antigens.''
However, the textbook explains that ``antigen-presenting cells such as B cells recognize antigens'' and ``plasma cells release antibodies to eliminate antigens'', while the LLM-generated analogy only includes the latter and omits the former.
\chirev{This may be linked to over-reliance issue, as L1 and L3 described their learning strategies during the interview as ``\textit{reading the analogy first, answering the questions, and not revisiting the textbook if the answers seemed clear from the analogy.}''}
% Based on quantitative results and interviews, we argue that some students may rely on the vague information from the analogy without carefully checking the textbook explanations, leading to incorrect answers.
% This highlights the potential risk that, without teacher intervention, students may mistakenly believe they fully understand a concept after grasping only the incomplete or imprecise information from the analogy.

%Group L's accuracy rate in the five questions of ``Nuclear Fission and Fusion'' was all lower than or equal to that of Group T. 
%While the other three analogies slightly improved overall answer accuracy, they also might harm students' understanding.
%The accuracy of Group L for Q4 of ``Immune Response'' was lower than that of Group T (Mean: 0.50 vs. 0.77).
%According to the answers, many students in Group L believe that ``plasma cells can recognize antigens'' is correct.
%However, the textbook explanations mention that ``antigen-presenting cells such as B cells recognize antigens'' and ``plasma cells release antibodies to eliminate antigens'', while the LLM-generated analogy only includes the latter and omits the former.
%During the interview, L1 and L3 mentioned they ``\textit{read the analogy first and then answered the questions}''.



\textbf{Students subjectively appreciate the correct LLM-generated analogies often with overconfidence.}
We \chirev{were surprised to find} the strongest association between group and self-confidence was for ``Wave-Particle Duality'' ($p = 0.025$) among the four concepts. 
This suggests that students were receptive to the \chirev{\textbf{Correct Analogy with Imagination}}. 
However, there was no significant association between group and answer accuracy for this concept. 
We also observed overconfidence among \chirev{\textbf{Correct and Satisfying Analogies} for} biological concepts: a significant association between group and confidence ratings for Q2 of “Blood Sugar Regulation” ($p$ = 0.034), but none with accuracy ($p$ = 1).
As shown on the right of Fig.~\ref{fig:study1_acc_conf_overall}, there are significant associations between group and confidence ratings for both subjects.
Besides, We found a negligible correlation between accuracy and the confidence rating, as the absolute value of Kendall's tau correlation coefficient between them in each group and concept was $\leq0.2$.
% This indicates that Group L's understanding may not align with their subjective satisfaction, suggesting that LLM-generated analogies could lead students to overestimate their grasp of the concept.

% \uline{\textbf{Summary to results of RQ1:}}
% Overall, we found that LLMs are relatively effective at generating helpful analogies for biological concepts, aiding students in understanding complex and detailed concept explanations. 
% However, LLMs often produce incorrect analogies for abstract physical concepts due to the difficulty of finding real-life counterparts, and such analogies are less effective and necessary for understanding physical concepts. 
% Additionally, students might \chirev{overly} rely on incomplete or imprecise information from analogies \chirev{with incorrect learning strategies}, or overestimate their understanding simply because they grasp the analogy.

\chirev{Overall, our empirical evidences suggest that LLM-generated analogies are currently unsuitable for unsupervised self-learning systems. We will discuss LLMs in supporting students' learning by analogy in Sec.~\ref{sec: discussion_student}.}


