%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Template for USENIX papers.
%
% History:
%
% - TEMPLATE for Usenix papers, specifically to meet requirements of
%   USENIX '05. originally a template for producing IEEE-format
%   articles using LaTeX. written by Matthew Ward, CS Department,
%   Worcester Polytechnic Institute. adapted by David Beazley for his
%   excellent SWIG paper in Proceedings, Tcl 96. turned into a
%   smartass generic template by De Clarke, with thanks to both the
%   above pioneers. Use at your own risk. Complaints to /dev/null.
%   Make it two column with no page numbering, default is 10 point.
%
% - Munged by Fred Douglis <douglis@research.att.com> 10/97 to
%   separate the .sty file from the LaTeX source template, so that
%   people can more easily include the .sty file into an existing
%   document. Also changed to more closely follow the style guidelines
%   as represented by the Word sample file.
%
% - Note that since 2010, USENIX does not require endnotes. If you
%   want foot of page notes, don't include the endnotes package in the
%   usepackage command, below.
% - This version uses the latex2e styles, not the very ancient 2.09
%   stuff.
%
% - Updated July 2018: Text block size changed from 6.5" to 7"
%
% - Updated Dec 2018 for ATC'19:
%
%   * Revised text to pass HotCRP's auto-formatting check, with
%     hotcrp.settings.submission_form.body_font_size=10pt, and
%     hotcrp.settings.submission_form.line_height=12pt
%
%   * Switched from \endnote-s to \footnote-s to match Usenix's policy.
%
%   * \section* => \begin{abstract} ... \end{abstract}
%
%   * Make template self-contained in terms of bibtex entires, to allow
%     this file to be compiled. (And changing refs style to 'plain'.)
%
%   * Make template self-contained in terms of figures, to
%     allow this file to be compiled. 
%
%   * Added packages for hyperref, embedding fonts, and improving
%     appearance.
%   
%   * Removed outdated text.
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass[letterpaper,twocolumn,10pt]{article}
\usepackage{usenix}

% to be able to draw some self-contained figs
\usepackage{tikz}
% \usepackage{amsmath}

% inlined bib file
\usepackage{filecontents}

% Our packages
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
% \usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{subcaption}

% \usepackage[colorlinks=true,urlcolor=black]{hyperref}
% \usepackage[subtle, tracking=normal, leading=normal]{savetrees}

\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}


%% imported packages
\usepackage{balance}
\usepackage{mathrsfs}
\usepackage{bm}
\usepackage{svg}
\usepackage{tabularx}  % added for importing tables
\usepackage{booktabs}  % added for importing tables
\usepackage{algpseudocode}
\usepackage{algorithm}
\usepackage{multirow}


%------
\usepackage{comment}
\usepackage[textwidth=1.5cm,textsize=scriptsize]{todonotes}
\newcommand{\lpasa}[1]{\todo[color=green!20]{{\bf lpasa:} #1}}
\newcommand{\new}[1]{\textcolor{black}{#1}}
\newcommand{\fra}[1]{\textcolor{red}{Fra: #1}}
%-------

\RequirePackage{amsmath}
    \DeclareMathOperator*{\argmax}{arg\,max}
    \DeclareMathOperator*{\argmin}{arg\,min}
    \DeclareMathOperator*{\arginf}{arg\,inf}
    \DeclareMathOperator*{\argsup}{arg\,sup}

% %-------------------------------------------------------------------------------
% \begin{filecontents}{\jobname.bib}
% %-------------------------------------------------------------------------------
% @Book{arpachiDusseau18:osbook,
%   author =       {Arpaci-Dusseau, Remzi H. and Arpaci-Dusseau Andrea C.},
%   title =        {Operating Systems: Three Easy Pieces},
%   publisher =    {Arpaci-Dusseau Books, LLC},
%   year =         2015,
%   edition =      {1.00},
%   note =         {\url{http://pages.cs.wisc.edu/~remzi/OSTEP/}}
% }
% @InProceedings{waldspurger02,
%   author =       {Waldspurger, Carl A.},
%   title =        {Memory resource management in {VMware ESX} server},
%   booktitle =    {USENIX Symposium on Operating System Design and
%                   Implementation (OSDI)},
%   year =         2002,
%   pages =        {181--194},
%   note =         {\url{https://www.usenix.org/legacy/event/osdi02/tech/waldspurger/waldspurger.pdf}}}
% \end{filecontents}

%-------------------------------------------------------------------------------
\begin{document}
%-------------------------------------------------------------------------------

%don't want date printed
\date{}

% make title bold and 14 pt font (Latex default is non-bold, 16 pt)
% \title{\Large \bf Toward the Model Selection Hijacking Adversarial Attack}
% \title{\Large \bf Moshi Moshi? A Model Selection Hijacking Attack for Adversarial Exploitation}
\title{\Large \bf Moshi Moshi? A Model Selection Hijacking Adversarial Attack}

%for single author (just remove % characters)
\author{
{\rm Riccardo Petrucci}\\
University of Padua, Italy
\and
{\rm Luca Pajola}\\
University of Padua, Italy
\and
{\rm Francesco Marchiori}\\
University of Padua, Italy
\and
{\rm Luca Pasa}\\
University of Padua, Italy
\and
{\rm Mauro Conti}\\
University of Padua, Italy
% copy the following lines to add more authors
% \and
% {\rm Name}\\
%Name Institution
} % end author

\maketitle

%-------------------------------------------------------------------------------
\begin{abstract}
%-------------------------------------------------------------------------------
% Model selection in Machine Learning (ML) is of paramount importance as it significantly influences the system's performance, accuracy, and generalizability to novel data.
% The selection of an appropriate model is essential for enabling the ML models to effectively learn from the data, make precise predictions, and adapt to diverse tasks and environments.
% Despite the critical role of model selection in the proper functioning of ML systems, its security from the perspective of adversarial machine learning remains unexplored.
% In the context of the Machine-Learning-as-a-Service (MLaaS) paradigm, this threat is particularly pertinent, as users often delegate the training phase to third-party entities by supplying data and learning strategies. 
% In this case, the issue is twofold: an incorrect model selection can negatively impact both the model's performance and the cost of running it, posing challenges for both the user and the service provider.
\new{
Model selection is a fundamental task in Machine Learning~(ML), focusing on selecting the most suitable model from a pool of candidates by evaluating their performance on specific metrics.
This process ensures optimal performance, computational efficiency, and adaptability to diverse tasks and environments.
Despite its critical role, its security from the perspective of adversarial ML remains unexplored.
This risk is heightened in the Machine-Learning-as-a-Service model, where users delegate the training phase and the model selection process to third-party providers, supplying data and training strategies.
Therefore, attacks on model selection could harm both the user and the provider, undermining model performance and driving up operational costs.
}

% In this work, we investigate \textit{if} and \textit{how} an adversary can tamper the model selection treating the learning and model selection phase as a black box. 
% To this end, we introduce the Model Selection Hijacking Adversarial Attack (MSHAA), a novel approach aimed at manipulating model selection data to yield a model with characteristics advantageous to the adversary.
% Utilizing a framework based on Variational Auto Encoder, we provide evidence that an attacker can induce inefficiencies in ML deployment, manifesting as performance degradation, increased latency, and elevated energy consumption.
% Our experiments employ conventional deep-learning architectures considering benchmark tasks in computer vision and speech recognition.
\new{
% In this work, we present \textbf{MHSAA} (\textbf{M}odel \textbf{S}election \textbf{H}ijacking \textbf{A}dversarial \textbf{A}ttack), the first adversarial attack specifically targeting model selection.
In this work, we present \textbf{MOSHI} (\textbf{MO}del \textbf{S}election \textbf{HI}jacking adversarial attack), the first adversarial attack specifically targeting model selection.
Our novel approach manipulates model selection data to favor the adversary, even without prior knowledge of the system.
Utilizing a framework based on Variational Auto Encoders, we provide evidence that an attacker can induce inefficiencies in ML deployment.
We test our attack on diverse computer vision and speech recognition benchmark tasks and different settings, obtaining an average attack success rate of 75.42\%. % 80.44\%.
%%% NOTA: ASR calcolato su media di Tabs 3+4
% In particular, our attack causes an 88.30\% decrease in generalization capabilities, an 83.33\% increase in latency, and a 12.65\% increase in energy consumption.
In particular, our attack causes an average 88.30\% decrease in generalization capabilities, an 83.33\% increase in latency, and an increase of up to 105.85\% in energy consumption.
%%% NOTA: valori calcolati su media di Tab 5
These results highlight the significant vulnerabilities in model selection processes and their potential impact on real-world applications.
% We publicly release our code and implementation at: \url{https://anonymous.4open.science/r/MOSHI-1518}.
}
\end{abstract}

% 13 pages max
\input{sections/introduction}
\input{sections/related_work}
\input{sections/background}
\input{sections/systemthreatmodel}
% \input{sections/mshaa}
\input{sections/generation}
\input{sections/evaluation}
% \input{sections/settings}
% \input{sections/results}
\input{sections/ablation}
\input{sections/conclusion}

% %-------------------------------------------------------------------------------
% \section*{Availability}
%-------------------------------------------------------------------------------

% %-------------------------------------------------------------------------------
\section*{Ethics Considerations}
% %-------------------------------------------------------------------------------
In this work, we propose a novel adversarial machine learning attack and conduct experiments exclusively in controlled, lab-only environments. Our primary goal is to advance the understanding of adversarial vulnerabilities in machine learning systems and to contribute to the development of more robust defenses. All experiments were carried out with benchmark datasets or models designed for research purposes, ensuring no real-world harm or exploitation of sensitive data. We acknowledge the dual-use nature of adversarial research and emphasize our commitment to ethical guidelines by openly discussing mitigation strategies and encouraging responsible use of this work.
%-------------------------------------------------------------------------------
\section*{Open Science}
% %-------------------------------------------------------------------------------

All the resources required for reproducing the experiments described in this study are provided.
This includes the complete dataset (traditional benchmarks), pre-processing scripts, model training code, and detailed instructions for setting up the computational environment. By ensuring accessibility to these resources, we aim to facilitate transparency, reproducibility, and further research in the field of ML.
All supplementary materials are available in the accompanying repository, providing a comprehensive framework for replicating and validating our findings.
% GitHub repository: \url{https://anonymous.4open.science/r/MOSHI-1518}.

\balance
\bibliographystyle{plain}
\bibliography{bibliography}

% \appendix
% \section{Data Availability}
% All the resources required for reproducing the experiments described in this study are provided. This includes the complete dataset (traditional benchmarks), pre-processing scripts, and model training code, as well as detailed instructions for setting up the computational environment. By ensuring accessibility to these resources, we aim to facilitate transparency, reproducibility, and further research in the field of machine learning. All supplementary materials are available in the accompanying repository, providing a comprehensive framework for replication and validation of our findings. GitHub repository: 
% \url{https://anonymous.4open.science/r/MSHAA-68DD/README.md}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\end{document}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%  LocalWords:  endnotes includegraphics fread ptr nobj noindent
%%  LocalWords:  pdflatex acks