% \section{Results -- OLD}
% MSHAA was tested against sets of models grouped into model grids \textcolor{blue}{set} based on some hyperparameters (e.g. the number of neurons per layer in the MNIST case study) and/or based on the learning rate used for training.\lpasa{il concetto di model grid va un po spiegato, nel senso che noi non parliamo mai di grid search al momento, e quindi non si capisce perche si definisca come griglia e non come set (per come è adesso secondo me è più corretto usare set))} 

% \subsection{Evaluation Metrics}
% In order to qualitatively estimate the effectiveness of the
% attack against the specific model’s grid set considered, we
% introduce two distinct metrics: the \textit{Effectiveness Score Function} (ESF) and the \textit{Attack Success Rate} (ASR).
    
% \paragraph{Effectiveness Score Function}
% % In order to qualitatively estimate the effectiveness of the attack against the specific \textcolor{red}{model's grids} \textcolor{blue}{set} considered, we introduce the \textit{Effectiveness Score Function} (ESF). 
% The ESF goal is to understand the impact of MSHAA on the victim model when deployed. Therefore, with ESF we measure the MSHAA returned model $\tilde{h}_{\mathfrak{c}^*}$ and the legitimate $h_{\mathfrak{c}^*}$ based on the hijack metric $\mathcal{E}$.
% Formally, ESF is defined as follows:
% \begin{equation}
%      \mathscr{S}(\mathfrak{C}) = \frac{
%          \mathcal{E} ( \tilde{h}_{\mathfrak{c}^*}, \mathcal{S}^{Test} ) 
%          - 
%          \mathcal{E} ( h_{\mathfrak{c}^*}, \mathcal{S}^{Test} )
%      }
%      {
%          \underset{\mathfrak{c} \in \mathfrak{C}}{\max} \{\mathcal{E}(h_{\mathfrak{c}}, \mathcal{S}^{Test})\} - 
%          \mathcal{E} (    h_{\mathfrak{c}^*}, \mathcal{S}^{Test}    )
%      }.
% \end{equation}

% ESF takes the difference between a chosen metric $\mathcal{E}$ of the models $\tilde{h}_{\mathfrak{c}^*}$ (see Eq.~\ref{best_poison}) and $h_{\mathfrak{c}^*}$ (see Eq.~\ref{best_val}) normalizing it with the difference between the maximum possible chosen metric of a model $h_\mathfrak{c}$ with $\mathfrak{c} \in \mathfrak{C}$, and again the chosen metric by $h_{\mathfrak{c}^*}$.\\
% This function allows us to evaluate our attack with any metric $\mathcal{E}$, for the purpose of this study, with regards to the hijack metric considered:
% \begin{itemize}
%     \item \texttt{Generalization Metric}. $\mathcal{E}$ is the loss function computed on the original validation set;
%     \item \texttt{Latency Metric}. $\mathcal{E}$ is the latency while processing the test set.
%     \item \texttt{Energy Consumption} or \texttt{$\ell_0$ Norm}. $\mathcal{E}$ is the estimated energy consumed by the model while processing the test set. When testing the attack in the SpeechCommands case, as we could not compute the \texttt{Energy} metric, we evaluated the quality of the attack using the \texttt{$\ell_0$ Norm}, by adopting $\mathcal{E}$ to be the $\ell_0$ Norm.
% \end{itemize}
% This Effectiveness Score function allows us to give a normalized score that sums up the effectiveness of the MSHAA, as a matter of fact:
% \begin{itemize}
%     \item \textit{Successful}, when $ESF > 0$, the MS phase to return a model with higher $\mathcal{E}$ compared to $h_{\mathfrak{c}^*}$. If the score is equal to $1.0$, then the returned model $\tilde{h}_{\mathfrak{c}^*}$ is the one among the considered models to have the highest $\mathcal{E}$.
%     \item \textit{Invariant}, when $ESF = 0.0$, the attack did not sway the MS phase, and the returned model is $\tilde{h}_{\mathfrak{c}^*} = h_{\mathfrak{c}^*}$, \textit{i.e.} the returned model is also the legitimate one. 
%     \item \textit{Unsuccessful}, $ESF < 0.0$ imply that the attack made the MS phase return a model of less $\mathcal{E}$ compared to $h_{\mathfrak{c}^*}$.
% \end{itemize}  

% \paragraph{Attack Success Rate}
% We define the ASR as a simplification of the ESF. In particular, we define the ASR as a boolean variable of a successful or unsuccessful attack, and therefore the $ASR = 1$ if $ESF > 0$, $ASR = 0$ otherwise. 
% With this metric, we are interested in understanding when an attack is successful in terms of increasing the hijack metric. 

% \subsection{Victim \textcolor{red}{Model's Grids}}
% Regardless of the case study at hand, we conduct conducted the attacks by considering each task's whole model's grids but also subsets. 
% For instance, in the MNIST case, we execute the attack when considering the whole grid (\textit{i.e.} the number of layers, number of neurons, learning rate) but also cases where we fixed - for instance - the learning rate and we vary the other two hyper parameters. 
% With this experimental setting, we aim to understand the attack effectiveness at different granularity. 
% Regardless of the case study at hand, we considered the victim models grouped in model grids based on some hyperparameters and/or based on the learning rate used for training. 
% In more detail, the chosen hyperparameter is:
% \begin{itemize}
% 	\item \textbf{MNIST Case}. Number of neurons per layer. Therefore, we obtain 18 sets of 10 models when grouping by learning rate and neurons, and 6 sets of 30 when grouping just by learning rate.
% 	\item \textbf{CIFAR10 Case}. Number of neurons in the second-to-last fully connected layer. Obtaining 6 sets of 7 models grouping by learning rate and neurons, and 2 sets of 21 models when grouping by learning rate.
% 	\item \textbf{SpeechCommands Case}. Number of neurons in the second-to-last fully connected layer. Having 4 sets of 4 models if we group by learning rate and neurons, and 2 sets of 8 models when grouping by just learning rate.
% \end{itemize}

% \subsection{Attacker Knowledge}
% In both white box and black box settings, we assume that the attacker has some different amount of knowledge at his disposal when launching the attack. In other words, we used sets of the models either clean or surrogate (obtained the same way as the aforementioned model grids), to train the HVAE.\\
% All in all, for the case studies at hand, we trained:
% \begin{itemize}
% 	\item \textbf{MNIST Case}. For each of the 6 considered learning rates, we assumed as knowledge possessed by the attacker, the 10 models having 32, 64, or 128 neurons separately and together, amounting to 24 overall HVAE trained (4 HVAE for each learning rate).
% 	\item \textbf{CIFAR10 Case}. For each of the 2 considered learning rates, we assumed as knowledge possessed by the attacker, the 7 models having the second to last fully connected layer with 128, 256, or 512 neurons, taken singularly or together, for a total of 8 HVAE trained.
% 	\item \textbf{SpeechCommands Case}. For each of the 2 considered learning rates, we assumed as knowledge possessed by the attacker, the 4 models having the second to last fully connected layer with 128, 256 neurons, taken singularly or together, for a total of 6 HVAE trained.
% \end{itemize}

\section{Results}
We analyze the impact of the MSHAA attack on the experimental settings previously described.
As MSHAA generates malicious poisoning samples to inject into the validation set, we explore the attack effect in two fashions:
\begin{itemize}
    \item \textit{Full substitution}, where the validation set is entirely substituted by the malicious samples. 
    \item \textit{Partial substitution}, where the validation set combines both legitimate and malicious samples at different rates (\textit{i.e.} 10\%, 20\%, 50\%, 80\%, 100\%). 
\end{itemize}
We measure the attack performance by comparing -- for each grid attack presented in Table~\ref{tab.grid} -- the legitimate model (\textit{i.e.} the model with the lowest loss in the unpoisoned validation set) and the malicious model (\textit{i.e.} the model with the lowest loss in the poisoned validation set). 

\subsection{Full Substitution}
Tables~\ref{tab.asr-wb} and \ref{tab.asr-bb} show the results of MSHAA in both white-box and black-box, respectively. Each table is composed by three sub-tables that represent the three distinct attack grid granularities,  \textit{i.e.} global, grouped-by learning rate, and grouped-by learning rate and number of neurons. For each case, we report the results for the three case studies (\textit{i.e.} MNIST, CIFAR10, and Speech Commands) spanning across the four studied hijack metrics (\textit{i.e} generalization error, latency, energy, and $\ell_0$. 
Furthermore, each row reports the number of sets. \footnote{Each set we trained an ad-hoc HVAE (see Sec.~\ref{ssec.grid}).} Consider Table~\ref{tab.asr-bb} on the MNIST use-case and global granularity: since we have only 1 set, the attack is either successful (100\%) or unsuccessful (0\%).
\par
Overall, we denote strong positive results proving the validity of HVAE, this first approach we proposed to address an MSHAA. 
The attack is perfectly successful (\textit{i.e.} $ASR = 100\%$) in $22/33$ and $23/33$ cases in both white-box and black-box scenarios.
Interestingly, we do not observe differences between white and black box settings, implying that the attack can be very dangerous, yielding high transferability capabilities. 
\par
The proposed attack appears to be extremely dangerous, especially for the generalization: in all the considered cases, our HVAE produced a poisoned validation set that hijacked the model selection, always returning a non-optimal that generalizes less. 
\par
Last, while our proposed attack HVAE shows promising results across different benchmarks, it suffers from the increased complexity of the data. In particular, in the SpeechCommands, the attack fails to produce poisoned validation samples capable of hijacking the model selection.   
%This phenomenon might be due to HVAE's underlying architecture which is not optimal in learning useful representation. 
This phenomenon may be attributed to the underlying architecture of the HVAE, which is not well-suited for learning effective representations in sequential domains.
We believe that future studies might attempt to design ad hoc HVAE for specific tasks (e.g., RAVE for the Speech domain~\cite{caillon2021rave}).
\par
We can therefore conclude that our proposed attack HVAE is effective and can produce strong manipulations to the model selection phase. Furthermore, these results answer our original research question: \textit{can an attacker manipulate the model selection?} The answer is yes, we can produce an MSHAA attack, and an attacker can ideally choose custom hijack metrics.  

\begin{table}[!htpb]
\centering
\footnotesize
\caption{ASR in White-Box Settings at different attack grid granularities.}
\begin{tabular}{c|c|c|c|c|c}  \toprule
\multicolumn{6}{c}{\textit{\textbf{Global}}} \\ \midrule
 & \textit{\# Sets} &\textit{Gener.} & \textit{Latency} & \textit{Energy} & \textit{$\ell_0$} \\ \midrule
MNIST & 1 & 100.0\% & 100.0\% & 100.0\% & 100.0\% \\
CIFAR10 & 1 &100.0\% & 0.0\% & 100.0\% & 100.0\% \\
SpeechC. & 1 & 100.0\% & 0.0\% & N/A & 0.0\% \\ \midrule
\multicolumn{6}{c}{\textit{\textbf{Learning Rate}}} \\ \midrule
MNIST & 6 &  100.0\% & 100.0\% & 66.7\% & 83.3\% \\
CIFAR10 & 2 & 100.0\% & 100.0\% & 100.0\% & 100.0\% \\
SpeechC. & 2 & 100.0\% & 50.0\% & N/A & 0.0\% \\\midrule
\multicolumn{6}{c}{\textit{\textbf{Learning Rate \& \# Neurons}}} \\ \midrule
MNIST & 18 & 100.0\% & 94.4\% & 100.0\% & 100.0\% \\
CIFAR10 & 6 & 100.0\% & 33.3\% & 50.0\% & 50.0\% \\
SpeechC. & 4 & 100.0\% & 50.0\% & N/A & 0.0\% \\ \bottomrule
\end{tabular}
\label{tab.asr-wb}
\end{table}

\begin{table}[!htpb]
\centering
\footnotesize
\caption{ASR in Black-Box Settings at different attack grid granularities.}
\begin{tabular}{c|c|c|c|c|c}  \toprule
\multicolumn{6}{c}{\textit{\textbf{Global}}} \\ \midrule
 & \textit{\# Sets} & \textit{Gener.} & \textit{Latency} & \textit{Energy} & \textit{$\ell_0$} \\ \midrule
MNIST & 1 & 100.0\% & 100.0\% & 100.0\% & 100.0\% \\
CIFAR10 & 1   & 100.0\% & 100.0\% & 100.0\% & 100.0\% \\
SpeechC.  & 1 & 100.0\% & 0.0\% & N/A & 0.0\% \\ \midrule
\multicolumn{6}{c}{\textit{\textbf{Learning Rate}}} \\ \midrule
MNIST & 6 &  100.0\% & 100.0\% & 66.7\% & 83.3\% \\
CIFAR10 & 2 & 100.0\% & 50.0\% & 50.0\% & 100.0\% \\
SpeechC. & 2 & 100.0\% & 50.0\% & N/A & 0.0\% \\ \midrule
\multicolumn{6}{c}{\textit{\textbf{Learning Rate \& \# Neurons}}} \\ \midrule
MNIST & 18 & 100.0\% & 83.3\% & 100.0\% & 100.0\% \\
CIFAR10 & 6 & 83.3\% & 50.0\% & 33.3\% & 50.0\% \\
SpeechC. & 4 & 100.0\% & 50.0\% & N/A & 50.0\% \\ \bottomrule
\end{tabular}
\label{tab.asr-bb}
\end{table}

\subsection{Partial Substitution}
We now analyze the impact on the validation poisoning rate. 
Figures~\ref{fig:pr-impact-lr} and~\ref{fig:pr-impact} show such a trend on the grid attack settings \textit{learning rate} and \textit{learning rate \& \#neurons}, respectively, where we report the three use cases, the four hijack metrics, and both knowledge levels. 
By analyzing the ESF for the first two datasets considered, we can observe that the white-box settings are useful to produce a stronger attack, which is comparable to more traditional poisoning properties. Despite that, we do not observe clear advantages in poisoning the validation set entirely. This result suggests that HVAE can produce an effective attack even by tampering with only a smaller portion of the validation set.\\
On the other hand, in the Speech Commands case study, we observe a more erratic behavior of the mean ESF. This is particularly evident for the $\ell_0$ hijack metric, despite achieving an Attack Success Rate greater than zero, as illustrated in Figures ~\ref{fig:pr-impact-lr} and~\ref{fig:pr-impact}, the mean ESF of these attacks is heavily below 0,  This discrepancy arises because, for some attacks, the ESF values are strongly negative.
We believe that in future works the interplay between MSHAA effectiveness and the poisoning rate should be explored. 

\begin{figure}
    \centering
    \includegraphics[width=0.99\linewidth]{figures/pr_impact_lr.pdf}
    \caption{Poisoning rate impact on the grid attack setting ``\textit{learning rate}'' for both White-Box (WB) and Black-Box (BB) knowledge level. }
    \label{fig:pr-impact-lr}
\end{figure}
 
\begin{figure}
    \centering
    \includegraphics[width=0.99\linewidth]{figures/pr_impact.pdf}
    \caption{Poisoning rate impact on the grid attack setting ``\textit{learning rate \& \#neurons}'' for both White-Box (WB) and Black-Box (BB) knowledge level. }
    \label{fig:pr-impact}
\end{figure}


\subsection{Impact on the Availability}
%\textbf{INTRO per Riccardo. Ricorda di dire che i risultati soon in 100\% val poisoned. Analizza brevemente anche see vedi difference tra white and black box settings.} 
In this section, we present the impact of our attack on the targeted metrics for the specific case study and models considered. 
In Table \ref{tab.impact} we illustrate how MSHAA, by influencing the Model Selection phase, impacts the chosen metrics. This is done by reporting the ratio between the metrics of the model returned by an untampered Model Selection, and the model returned once the attack, conducted by training the HVAE with full knowledge of the victim models, and with full (100\%) substitution of the original validation set with poison data. 
For instance, when considering the white-box knowledge, we observed a malicious MNIST model $\times3$ slower that the legitimate model in terms of latency introduced. 
\par
As we can see from the table, the WB and BB settings perform similarly in both the MNIST and SpeechCommands case studies. The former presents the same model for all the attacks conducted with the various hijack metrics, and the latter shows better performance only for the WB case on the Generalization Metric. Finally, for the CIFAR10 case study, the BB setting performs slightly better, allowing for a small increase in the Latency metric.

\begin{table}[!htpb]
\centering
\footnotesize
\caption{Impact factor on the availability. The results report the impact factor. We only report the values for the \textit{global} attacker grid variant. }
\begin{tabular}{c|c|c|c|c}  \toprule
\multicolumn{5}{c}{\textit{\textbf{White-Box}}} \\ \midrule
 &\textit{Gener.} & \textit{Latency} & \textit{Energy} & \textit{$\ell_0$} \\ \midrule
MNIST  & 21.27 & 3.818 & 1.244 & 6.293 \\
CIFAR10 & 1.120 & 0.769 & 1.009 & 1.027 \\
SpeechC. & 4.136 & 0.797 & N/A & 0.629 \\ \midrule
\multicolumn{5}{c}{\textit{\textbf{Black-box}}} \\ \midrule
MNIST  & 21.27 & 3.818 & 1.244 & 6.293 \\
CIFAR10 & 1.180 & 1.001 & 1.009 & 1.027 \\
SpeechC. & 2.319 & 0.797 & N/A & 0.810 \\ \bottomrule
\end{tabular}
\label{tab.impact}
\end{table}

The study of the impact on the availability of our attacks reconfirms the strong link between our attack and the models for which is performed the model selection, as having a more bigger and diverse set of models allows us to have possibly higher metrics, which allows us to train more effective HVAE and a more impactful attack.

% \subsection{Results -- OLD}
% In order to evaluate the effectiveness of the MSHAA, we tested the attacks stemming from the trained HVAE, by substituting an increasing amount of the clean validation set with generated poisoning data (from substituting 10\% to substituting the whole clean validation set). We then computed the poisoned validation loss on the victim models that share the learning rate with the known models by the adversary.\\
% To evaluate the performance of the attack we divided the trained models into multiple sets, to assert if the attack was successful in swaying the Model Selection phase, by comparing the model with the lowest clean validation loss and the one with the lowest poison validation loss.\\
% To better present the performance of the attack, here we report the percentages of attacks able to achieve an effectiveness score greater than zero with regards to all the launched attacks. The reported percentages refer to attacks conducted substituting completely the validation set with poisonous samples.\\
% By way of example, in Tables \ref{WB-lr-n} and \ref{BB-lr-n}, we are considering the model set obtained by grouping per learning rate and number of neurons, therefore, for the MNIST case, we are testing the 4 HVAEs against the 18 model set for a total of 72 tests. Meanwhile, in Tables \ref{WB-lr} and \ref{BB-lr}, as we are working with the model set obtained by grouping just by the learning rate, in the MNIST case we are testing the 4 attacks against sets, amounting to 24 tests overall.

% \begin{table}[h!] \label{WB-lr-n}
%     \centering
%     \input{tables/attack_evaluation_WB_lr_neuron}
%     \caption{White box results lr \& neurons.}
% \end{table}

% \begin{table}[h!]\label{BB-lr-n}
%     \centering
%     \input{tables/attack_evaluation_BB_lr_neuron}
%     \caption{Black box results lr \& neurons.}
% \end{table}


% \begin{table}[h!]\label{WB-lr}
%     \centering
%     \input{tables/attack_evaluation_WB_lr}
%     \caption{Whitebox results lr.}
% \end{table}

% \begin{table}[h!]\label{BB-lr}
%     \centering
%     \input{tables/attack_evaluation_BB_lr}
%     \caption{Black box results lr.}
% \end{table}


% \subsection{Impact}



% %\textbf{Add some statistics on how the model results in terms of latency, energy, etc. Therefore, you compare based on each metric the value for the legitimate model, the hijacked one, and see how it increases. Therefore, I expect to see an average that says "it's 2.5 times slower than the legitimate model". The results should be represented as in the previous sec, therefore one paragraph for the overall grid, and then 4 distinct tables}
% Finally, we tested an attack with full knowledge of all the models in either the White Box or the Black Box setting, (i.e. for the MNIST case we assume the adversary knows all the trained 180 models). We then tested on sets containing all of the models, regardless of the learning rate used for training and the specific hyper-parameters considered. Following such an attack, we were able to succeed for both in White Box and Black Box Settings, in all tested metrics for the MNIST case. Meanwhile, for the CIFAR10 case, we did fail just on the \texttt{Latency} metric in the white box set. Finally, for the Speech Commands, we were able to succeed only with the \texttt{Generalization} metric.\\
% Now, by way of example, we report the impact of this last attack in the MNIST case, with respect to all of the considered hijack metrics, from all of the 180 trained models, the one that would be normally selected is composed of one layer of 128 neurons, trained at a learning rate of 0.001, having:
% \begin{itemize}
%     \item Clean Validation Loss:  0.000424  
%     \item Latency (s): 0.032640 
%     \item Estimated Energy Consumption (pJ): 2.289672e+07 
%     \item $\ell_0$ Norm: 79.473261 
% \end{itemize}

% Now, for this specific attack, when substituting the whole dataset with poisonous samples generated by our HVAE, for the specific metric considered, we have:
% \begin{itemize}
%     \item \texttt{Generalization Metric}. Launching the attack with this metric, we sway the MS phase into returning a model of 10 layers of 128 neurons trained at a learning rate of 0.005, that had a clean validation loss of 0.009013, spelling an increase of this metric of over 21.257 times.
%     \item \texttt{Latency Metric}. The model returned following this attack is the same as the Generalization metric, and has a latency of 0.124606 s, corresponding to an increase of 3.817 times.
%     \item \texttt{Energy Consumption}. With this metric, the MS returned the same model as before, which brings an estimated energy consumption of 2.848310e+07 pJ, representing an increase of 1.244 compared to the legitimately returned one.
%     \item \texttt{$\ell_0$ Norm}. This metric, again forced the MS to return the same model as before, and in terms of $\ell_0$ norm, we have 500.154926, which signifies an increase of 6.293.
% \end{itemize}
