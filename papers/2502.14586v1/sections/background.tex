\section{Background}
\label{sec:background}

This section describes in depth the basis of supervised learning (Section~\ref{subsec:sl}) and the role of model selection (Section~\ref{subsec:ms}).
We opt for a detailed description aiming to provide readers all the elements to understand the idea behind the attack we propose in this work, the first of its kind. 
This section follows the notation of~\cite{shalev2014understanding, goodfellow2016deep, oneto2020model}.

% \subsection{Principles of Supervised Learning}
\subsection{Supervised Learning}
\label{subsec:sl}
% \subsubsection{Supervised Learning (SL)}
Supervised Learning (SL) is an ML paradigm wherein the training process involves presenting a labeled dataset (experience) to a learning algorithm.
This process results in the development of a model capable of assigning labels to input samples.
The paradigm is composed of the following elements: a learning algorithm $\mathscr{A}$; an arbitrary set of sample that we wish to label $\mathcal{X}$; a set of possible labels $\mathcal{Y}$; $\mathfrak{G}: \mathcal{X} \to \mathcal{Y}$, i.e. a rule which assigns labels $y \in \mathcal{Y}$ to input points $\bm{x} \in \mathcal{X}$, also called labeling or target function, generally unknown to the learner; $\mathcal{D}$, a probability distribution over $\mathcal{X}$, unknown to the learner.
In SL, the aim of our learning algorithm or learner $\mathscr{A}$ is to find a hypothesis $h$ that approximate $\mathfrak{G}$, through another rule $h: \mathcal{X} \to \mathcal{Y}$, which maps inputs $\bm{x} \in \mathcal{X}$ into labels $\hat{y} \in \mathcal{Y}$.
% \par
\subsubsection{Experience}
This goal is carried out by providing to $\mathscr{A}$ the experience, which in the SL framework is a set of samples $\mathcal{\hat{S}} \subseteq \mathcal{X} \times \mathcal{Y}$.
In the real world, we have a limited number of samples.
Thus we will consider $\mathcal{S} \subset \mathcal{\hat{S}}$ to be a finite set, composed of $n$ samples $(\bm{x}_i, y_i)$, where $y_i = \mathfrak{G}(\bm{x}_i)$, and $\bm{x}_i$ is sampled from $\mathcal{X}$ according to the distribution $\mathcal{D}$ for $1 \leq i \leq n$.
Therefore, we can express our learner as $\mathscr{A}: \mathcal{S} \to \mathcal{H}$, where $\mathcal{H}$ is the Hypothesis Space, which constitutes a set of all functions which can be implemented by the ML system ($h \in \mathcal{H}$).
% \par
\subsubsection{Task}
It governs the way the ML algorithm should process each sample $\bm{x} \in \mathcal{X}$, by way of example, remaining in the SL framework, the most common tasks are a \textit{classification} and \textit{regression}.
In classification, $\mathscr{A}$ is asked to find a function $h: \mathcal{X} \to \mathcal{Y}$, specifying which of $|\mathcal{Y}|$ classes%$k$  categories 
some input $\bm{x} \in \mathcal{X}$ belongs to. %, %therefore we have $\mathcal{Y} = \{1, ..., k\}$, discrete and finite, with $k \in \mathbb{N}$.
In regression, $\mathscr{A}$ is asked to find a function $h: \mathcal{X} \to \mathcal{Y}$, which maps inputs $\bm{x} \in \mathcal{X}$ into some numerical value, usually $\mathcal{Y} = \mathbb{R}$, therefore the format of the output is different compared to the classification task.
% \par
\subsubsection{Performance Measure}
The capability of $h$ in approximating $\mathfrak{G}$ is evaluated via a quantitative performance measure usually specific to the task being carried out.
In this work %the performs maeasure refers to the error metric - as 
 we focus on benchmark classification tasks with balanced datasets, thus as a performance measure, we adopted the accuracy, i.e., the proportion of examples for which the model produces incorrect output. %, computed with reference to a loss function $\mathcal{L}: \mathcal{H} \times \mathcal{X} \times \mathcal{Y} \to \mathbb{R}$.
%\lpasa{ho commentato l'ultima parte, perche personalmente non mi torna la connessione tra acc e loss, fate un check, perche che loss e l'acc legate in classificazione non è cosi verovisto che la loss è na softmax}
%\par
Assuming that we are dealing with deterministic rules, %$h \in \mathcal{H}$
we can introduce the concept of \textit{true error} as the probability that $h$ will fail to classify an instance drawn at random according to $\mathcal{D}$:
\begin{equation}
\mathcal{L}_{\mathcal{D}} (h) = \mathbb{E}_{\bm{x} \sim \mathcal{D}}[\mathcal{L}(h, \bm{x},\mathfrak{G}(\bm{x}))].
\end{equation}
%The true error is one of the metrics that give the most useful information on the quality of $h$, allowing us to build Bayes' rule, obtaining the best possible approximation of $\mathfrak{G}$ with reference to a loss function $\mathcal{L}$:
%   \begin{equation}
% h_{Bayes} : \arginf{\mathcal{L}_{\mathcal{D}} (h)}.
%   \end{equation}
However, a general learning algorithm cannot access the probability distribution $\mathcal{D}$. %and an infinitely large set $\mathcal{H}$.%(due to the No-Free-Lunch Theorem), \lpasa{mi sfugge questa connessione}
Thus, it is not possible to compute $\mathcal{L}_{\mathcal{D}} (h)$, and we can only use the learner $\mathscr{A}$, with the known training data $\mathcal{S}$, resulting in:
   \begin{equation}
 h : \mathscr{A}(\mathcal{S}).
   \end{equation}
To measure the performance of our model, we can estimate the true error rate by computing the error rate on unseen data. Therefore, we employ a test set of data:
$\mathcal{S}^{Test} = \{(\bm{x}_j,\mathfrak{G}(\bm{x}_j)): 1 \leq j \leq k, \quad \bm{x}_j \in \mathcal{X} \}$ which is separate from the data used for training the ML system: $\mathcal{S}^{Test} \cap \mathcal{S} = \emptyset $.
Thus, we can define the test error as follows:
\begin{equation}
    \mathcal{L}_{Test} (\mathscr{A}(\mathcal{S}), \mathcal{S}^{Test}) = \mathbb{E}_{(\bm{x},y) \in \mathcal{S}^{Test}}[\mathcal{L}(\mathscr{A}(\mathcal{S}), \bm{x},y)].
\end{equation}

\subsection{Model Selection}
\label{subsec:ms}
\new{Model selection involves identifying the best hypothesis $h$ from a set $\mathcal{H}$ with the goal of minimizing true error through hyperparameters tuning.
This section discusses the role of hyperparameters and the process of selecting most suitable configurations.}
\subsubsection{Hyperparameters}
Any hypothesis $h \in \mathcal{H} $ is characterized by a set of hyperparameters (parameters not adjusted by the learning algorithm itself), which delineate the possible set of functions contained in $\mathcal{H}$, on which $\mathscr{A}$ perform the research of the most suitable function. %from which $\mathscr{A}$ will choose its output.
Therefore, the selection of the best configuration of hyperparameters $\mathfrak{c}$ in a set of possible configurations $\mathfrak{C}$ is an important problem in learning:
which could be generalized to the problem of choosing between different algorithms $\mathscr{A} \in \mathcal{A}$, each characterized by its configuration of hyperparameters $\mathfrak{c}$.
% Formally. %\lpasa{io qui non userei $\mathscr{A} \in \mathcal{A}$, ma $h \in \mathcal{H}$, non faccio la modifica perche non sono certo che la cosa crei problemi al resto del paper. Anche qui, come nel cso del paragrafo precedente $\mathcal{A}$ e $\mathcal{A}$ sono esattamente lo stesso insieme, e usare due nomi per la stessa cosa secondo me crea molta confusione }
\begin{equation}
 \mathcal{A}_\mathfrak{C} = \{\mathscr{A}_{\mathfrak{c}} : \mathscr{A} \in \mathcal{A}, \mathfrak{c} \in \mathfrak{C}_{\mathscr{A}} \},
   \end{equation}
where $\mathfrak{C}_{\mathscr{A}}$ is the set of possible configurations of hyperparameters for learner $\mathscr{A}$.%\lpasa{L'uso di $A$ va rivisto}
% \subsubsection{Model Selection}
\subsubsection{\new{Selection Process}}
%This methodology tries to find the right balance between approximating the labeling function $\mathfrak{G}$ and the estimation of the true error.\lpasa{Anche questa frase non mi pare corretta onestamente, la model selection non considera l'approssimazione, ma si limità a selezionare il modello milgiore su un set di dati non ancora visto, e lo fa perchè la selezione va fatta stimando il true error, e l'unico modo è usare dei dati non considerati in t/t, altrimenti la selezione sarebbe bias rispetto ai dati che consideriamo in training/test.}
% The process of finding the optimal hyperparameter configuration for a given task is known as model selection.
During model selection, the objective is to choose the hypothesis $h$ that minimizes the true error.
Since the true error of a hypothesis cannot be directly computed, we rely on an approximation obtained from a set of unseen data (validation set) drawn from the distribution $\mathcal{D}$. 
This process requires training several models with different hyperparameter configurations $c$ and then approximating the true error for each configuration.
The resulting model with the lowest approximate error will be the candidate chosen to solve the given task.
This procedure is both time and computationally demanding, as the number of possible hyperparameter configurations $c$ is potentially infinite.
A straightforward approach to address this challenge is to use Grid Search.
Grid Search systematically combines and evaluates a predefined set of hyperparameter values.
Although this reduces the complexity by limiting the search space, it can still be computationally expensive, especially when the grid is large or the model is complex.
%Therefore it aims at being able to understand whether a given model fails to obtain a good performance due to overfitting, or underfitting. Overfitting occurs when a learning algorithm fits too closely to its training data while underfitting fails to establish a desirable relationship between the input and output variables. Once the best algorithm $\mathscr{A}_{\mathfrak{c}}$ has been chosen, the final output can be obtained as before: $\mathscr{A}_{\mathfrak{c}}(\mathcal{S}) = h$.
\subsubsection{Resampling Methods}
Performing model selection is often constrained by the amount of available data.
When data is limited, using a resampling method can be helpful, as splitting the dataset into separate training, validation, and test sets of significant size could be detrimental to the training process.
Resampling techniques allow for more efficient use of the data by repeatedly training and validating the model on different subsets, ensuring better performance evaluation without sacrificing valuable data for training.
Two standard techniques for model selection are the Hold-Out method and k-fold cross-validation.
Both rely on resampling the original dataset $\mathcal{S}$ to create the two essential sets needed for model selection: the training set $\mathcal{S}^{Train}$ and the validation set $\mathcal{S}^{Val}$.
%Resampling Methods like Hold-Out or k-Fold Cross Validation rely on the resampling of the original dataset $\mathcal{S}$, with or without replacement, to build two different and independent datasets which take the names of training set $\mathcal{S}^{Train}$ and validation set $\mathcal{S}^{Val}$. 
Formally, the training set is used to train the different algorithms $\mathscr{A}_{\mathfrak{c}} \in \mathcal{A}_\mathfrak{C}$, thus obtaining different candidate rules $\mathscr{A}_{\mathfrak{c}}(\mathcal{S}) = h_{\mathfrak{c}}$, then the latter is employed for estimating the true error of each candidate rule, to choose the best one. 
% \par
In this work, we focus on the Hold-Out procedure. 
It consists of simply performing resampling on $\mathcal{S}$ without replacement and then using the same $\mathcal{S}^{Train}$ to train all the models, and $\mathcal{S}^{Val}$ to select the best candidate rule as the one with minimum validation error:
\begin{equation} \small%\footnotesize
\begin{split}
     \mathcal{L}_{Val} (\mathscr{A}_{\mathfrak{c}}(\mathcal{S}^{Train}), \mathcal{S}^{Val}) = \mathbb{E}_{(\bm{x},y) \in \mathcal{S}^{Val}}[\mathcal{L}(\mathscr{A}_{\mathfrak{c}}(\mathcal{S}^{Train}), \bm{x},y)].
\end{split}
\end{equation}