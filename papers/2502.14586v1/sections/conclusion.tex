\section{Conclusion}
\label{sec:conclusion}

\new{
Model selection is a cornerstone of ML, directly shaping a system's performance, robustness, and generalizability.
Selecting the right model ensures accurate data interpretation, reliable predictions, and adaptability to new scenarios.
Additionally, this paper we demonstrates how model selection can be used to negatively impact the efficiency and scalability of real-world applications, making careful evaluation a critical step in the ML pipeline.
}

\new{
This work addresses a novel question in AML: \textit{can an attacker tamper with the validation set to manipulate model selection arbitrarily?}
To answer this, we introduce the MOSHI (MOdel Selection HIjacking) adversarial attack, the first of its kind, which targets the model selection phase by injecting adversarial samples into the validation set.
This manipulation prioritizes the hijack metric while leaving the victim’s original loss metric and code untouched.
To achieve this goal, we introduce an innovative generative process called the \textit{Hijaking VAE} (HVAE), designed to craft adversarial samples that enable the attack.
Our comprehensive experiments validate the feasibility of arbitrarily influencing model selection, highlighting the broad applicability of MOSHI due to its reliance solely on data injection, independent of the victim’s model or validation methodology.
}

% The importance of model selection in ML cannot be overstated, as it fundamentally influences the system's overall performance, robustness, and generalizability.
% Choosing the right model is essential to ensure that the learning algorithm can accurately interpret the data, make reliable predictions, and adapt to new, unseen scenarios. 
% Moreover, the appropriateness of the model directly impacts the efficiency and scalability of the solution, which are critical for deploying real-world applications.
% Therefore, meticulous consideration and evaluation of different models are crucial steps in the ML pipeline, contributing significantly to the success and reliability of the final solution.

% In this work, we explored a novel research question in adversarial machine learning: \textit{can an attacker tamper the validation set to manipulate the model selection arbitrarily?}
% Starting from this concept, we formulate the MOSHI (MOdel Selection HIjacking) adversarial attack -- the first of its kind. 
% The attack seeks to manipulate the model selection phase to favor a model that meets the attacker’s specifications by injecting carefully crafted adversarial samples into the validation set.
% This influences the selection process to prioritize a target metric, the hijack metric while leaving the victim’s original loss metric and code intact.
% To achieve this goal, we introduce an innovative generative process called the \textit{Hijaking VAE} (HVAE), designed to produce adversarial samples that facilitate our attack when injected into a validation set.
% Our methodology is supported by strong methodological experiments, showing the feasibility of an attacker arbitrarily manipulating the model selection.
% The reliance of MOSHI on simply injecting data without modifying the model selection process ensures the attack's broad applicability, regardless of the model or validation methodology adopted by the victim.