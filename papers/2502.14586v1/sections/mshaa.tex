\section{Model Selection Hijacking Adversarial Attack}
In this section, we introduce the MSHAA, discussing the motivation behind the attack, the methodology employed, and the tools developed to poison the victim’s validation set. We provide a detailed overview of how MSHAA manipulates the model selection process by strategically injecting adversarial samples, ensuring that the attacker's desired model is favored.
\subsection{Motivations}
%\lpasa{se siete d'accordo io useri ML e non AI. di fatto l'AI non ha bisogno della model selection (e chiamre AI il ML è abbstanza sbagliato a mio parare)}
Model selection is a critical step in the ML 
%AI 
development process due to its profound impact on the performance, efficiency, and generalizability of the resulting system. The choice of model determines how well the 
%AI 
ML model can learn from data, adapt to new information, and perform tasks accurately. Selecting an appropriate model involves evaluating various algorithms and architectures to identify the one that best fits the specific problem domain and dataset characteristics. This process ensures that the ML system can achieve optimal performance while minimizing overfitting and underfitting, thereby enhancing its robustness and reliability in real-world applications.
\par
Although extremely critical, works on AML did not consider \textit{if} and \textit{how} the model selection can be tampered with by adversaries for some advantage.  
In this work, we aim to answer this research question by presenting the \textit{Model Selection Hijacking Adversarial Attack} (MSHAA), a novel adversarial vector to ML applications. 
%
\subsection{Adversary Goal}
The goal of our proposed attack -- the \textit{Model Selection Hijacking Adversarial Attack} (MSHAA) -- is to control the model selection phase by injecting malicious crafted samples to the solely validation set.  
In this scope, the adversary goal is to let the model selection phase select the model architecture that suit the best a target metric, named \textit{hijack metric} $m$. 
The hijack metric is arbitrary and can be chosen by the adversary. 
%Last, we assume adversaries cannot alter the standard model selection routine, meaning they cannot alter the loss metric defined by the victim, nor the underling code.This means that with MSHAA we inject samples that alter the validation loss in such a way that the model that fits the best the hijack metric also result with a lower loss compared to the other models in the grid. 
Lastly, we assume that adversaries cannot modify the standard model selection routine, including the loss metric defined by the victim, the underlying code, or the model selection policy. This means that with MSHAA, we inject samples that influence the loss function evaluation on the validation set, ensuring that the model best suited to the hijack metric also achieves a lower loss compared to other models in the validation grid.
In this work, we explore different types of hijack metrics. 

%
\subsubsection{Threat Model}
Suppose the victim has access to a dataset $\mathcal{S}$, divided in train $\mathcal{S}^{Train}$, validation $\mathcal{S}^{Val}$, and testing $\mathcal{S}^{Test}$ sets.%\lpasa{occhio che sta divisione senza Resampling non è affrontata nel cap Background attualmente}
%
The former is used to train several model $h_{\mathfrak{c}}$ grouped in a models %grid 
set identifiable with $\mathfrak{C}$,\footnote{$\mathfrak{c}$ represents the configuration of hyperparameters and $\mathfrak{C}$ the set of all possible configurations of hyperparameters.} the second to perform model selection, and the latter to test the best model performance. %generalization capability.
%The model selection phase will then return the best model $h_{\mathfrak{c}^*}$, which is defined as the model, among the ones trained, that has the lowest validation loss: 
The model selection phase will return the model $h_{\mathfrak{c}^*}$ with the lowest true error estimate on the validation set among those trained:
    \begin{equation}
        \label{best_val}
 h_{\mathfrak{c}^*} = \argmin_{h_{\mathfrak{c}} : \mathfrak{c} \in \mathfrak{C}} \mathcal{L}_{Val}(h_{\mathfrak{c}}, \mathcal{S}^{Val}).
    \end{equation}

In this context, we postulate that the adversary possesses access to the validation set $\mathcal{S}^{Val}$, enabling them to read and modify its data. Consequently, our newly proposed adversarial family, MSHAA, exhibits characteristics akin to data poisoning, as it allows the attacker to alter the victim’s dataset.
In contrast with the traditional poisoning attack, our proposed MSHAA operates solely at the validation set level, leaving the training set unaltered. 
\par
We now outline the adversary knowledge, detailing the information and capabilities that an attacker possesses, as well as the limitations and constraints they face within our threat model.
We explore two distinct attack settings.
\begin{itemize}
    \item \textit{White-box attack}: adversaries with full knowledge of the  trained models $h_{\mathfrak{c}}, \;  \forall c \in \mathcal{C}$ (\textit{i.e., architectures, trained parameters} and they can freely query the model for inference purposes).  
    \item \textit{Black-box attack}: adversaries with knowledge limited to the model's architectures $\mathfrak{C}$ (\textit{i.e.,} the models that will be learned and tested during the model selection phase). 
\end{itemize}
Furthermore, in both scenarios adversaries know $\mathcal{S}^{Train}$ but they cannot tamper it. 
We again recall that -- for both white-box and black box settings -- the attacker is limited to modifying of the only validation set $\mathcal{S}^{Val}$.
A schematic representation of the considered threat model is depicted in Figure~\ref{fig:overview}.

\begin{figure*}[!htpb]
    \centering
    \includegraphics[width=0.65\linewidth]{figures/attack_overview.pdf}
    \caption{Schematic representation of the MSHAA threat model.}
    \label{fig:overview}
\end{figure*}


\subsubsection{Stealthiness}
Poisoning only the validation set allows all the trained models and their parameters not to be modified during the attack. This means that once attacked, the model selection phase, which is carried out in an unmodified way, returns a model that has been regularly trained on clean data, and among all the ones trained is of higher hijack metric, which can be chosen by the adversary. 
%This means that the returned model does not leak any clue about the attack having been carried out, but on the other hand, limits the effectiveness of the attack itself based on the hijack metric of the available models.
This ensures that the selected model reveals no evidence of the attack, as its performance on the victim's loss function remains close to that of models selected using the original validation data. However, it differs in terms of the hijack metric, which is influenced to align with the attacker’s desired outcome.

\subsubsection{Threat Victims}
With MSHAA attack execution might impact different victims through traditional poisoning and backdoor dynamics~\cite{tian2022comprehensive, li2022backdoor}. 
A pertinent example within the context of a threat model is the public dissemination of datasets, which are typically pre-structured into standard partitions (training, validation, and testing splits) to facilitate the reproducibility of experimental outcomes. In such a scenario, tampering with the validation set represents a significant vulnerability, as it undermines the integrity and reliability of the results. 
The attacker provides a compromised dataset to a service provider, to increase the inefficiency of its services by increasing, for instance, the energy consumption or the latency in the predictions, or, conversely, aiming to produce a stealthy poisoning attack by altering the victim's model performance. 
% Furthermore, in the context of the MLaaS paradigm, the attack can be three-fold:

% \begin{itemize}
%     \item The attacker provides a compromised dataset to a service provider, to increase the inefficiency of its services by increasing, for instance, the energy consumption or the latency in the predictions. 
%     \item The victim provides the dataset and a training strategy to the MLaaS provider, whose goal is to train a model with powerful hardware and returned the optimal model to the victim. Here, the MLaaS provider might tamper the 
% \end{itemize}
