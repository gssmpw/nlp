% \section{Generating Hijacking Samples}
\section{\new{Methodology}}
\label{sec:methodology}
%A critical step of the Model Selection Hijacking Adversarial Attack is the generation of adversarial hijacking samples to be inject to the validation set. 
%We now present a novel methodology to design and generate such samples. %\lpasa{occhio che qua sembra che l'unica novelty sia la generzione di esempi!}
% A crucial phase in the Model Selection Hijacking Adversarial Attack involves generating adversarial hijacking samples for injection into the validation set. Among the novelties introduced by this work, we present a new methodology specifically for designing and generating these samples.
\new{
The MOSHI attack operates uniquely by injecting and substituting data points in the validation set with data from $\mathcal{S}^{Val}_{pois}$, disrupting the critical model selection phase without altering the training process or parameters.
This set, which the attacker carefully generates, will be used for the model selection phase, which in turn will return a model $\tilde{h}_{\mathfrak{c}^*}$:
    \begin{equation}
        \label{best_poison}
 \tilde{h}_{\mathfrak{c}^*} = \argmin_{h_{\mathfrak{c}} : \mathfrak{c} \in \mathfrak{C}} \mathcal{L}_{Val}(h_{\mathfrak{c}}, \mathcal{S}^{Val}_{pois}).
    \end{equation}
The selected model $\tilde{h}_{\mathfrak{c}^*}$ is different from $h_{\mathfrak{c}^*}$, as now, the poisoned validation set no longer allows for selecting a better, more generalized, model, but selects one that has a configuration of hyper-parameters which maximizes the hijack metric, chosen by the adversary. 
Thus, a central aspect of this approach involves generating adversarial hijacking samples crafted explicitly for injection into the validation set.
Among the novelties introduced in this work, we present a specialized methodology for designing and generating these samples (Section~\ref{subsec:generation}) and the hijack metrics used in our study (Section~\ref{ssec.hm-theory}).
}

% \subsection{Overview}
% The goal of the adversary is to assume control of the MS phase by injecting and substituting data points in the validation set with data from $\mathcal{S}^{Val}_{pois}$. This set, which is carefully generated by the attacker, will be used for the model selection phase, which in turn will return a model $\tilde{h}_{\mathfrak{c}^*}$:
%     \begin{equation}
%         \label{best_poison}
%  \tilde{h}_{\mathfrak{c}^*} = \argmin_{h_{\mathfrak{c}} : \mathfrak{c} \in \mathfrak{C}} \mathcal{L}_{Val}(h_{\mathfrak{c}}, \mathcal{S}^{Val}_{pois}).
%     \end{equation}

% The selected model $\tilde{h}_{\mathfrak{c}^*}$ is different from $h_{\mathfrak{c}^*}$, as now, the poisoned validation set, no longer allows for selecting a better, more generalized, model, but selects one that has a configuration of hyper-parameters which maximizes the hijack metric, chosen by the adversary. 

% \subsection{Generative Process}
\subsection{\new{Adversarial Sample Generation}}
\label{subsec:generation}
\new{
Although our adversarial sample generation model is based on the Variational Auto Encoder (VAE) architecture (Section~\ref{subsub:vae}), we introduce a variation of the conditional VAE architecture designed for the generation of hijacking samples (Section~\ref{subsub:hvae}).
}
\subsubsection{Variational Auto Encoder (VAE)}
\label{subsub:vae}
We design our generative process using a Variational Auto Encoder (VAE)~\cite{kingma2013auto}, which is an extension of more traditional Autoencoders~\cite{hinton2006reducing}. VAE consists of two modules: first, an \textit{encoder} which learns a \textit{posterior} recognition model $q_{\phi}(z|x)$, encoding an input $x$ to a latent representation $z$; second, a \textit{decoder} that generates samples from the latent space $z$ via the likelihood model $p_{\theta}(x|z)$. $\phi$ and $\theta$ are learning parameters. 
In contrast with standard autoencoders, VAEs enforce a continuous prior distribution $p(z)$, usually set to the Gaussian. This forces the model to encode the entire input distribution to the latent code rather than memorizing single data points. 
Traditional VAEs are trained with the following loss:
\begin{equation} \small
% \begin{split}
    \mathcal{L}_{VAE}(\phi, \theta) = KL(q_{\phi}(z|x) || p(z)) %\\
    -\mathbb{E}_{q_{\phi}(z|x)}(\log p_{\theta}(x|z)), 
% \end{split}
\end{equation}
where $KL$ is the Kullback-Leibler divergence~\cite{kullback1951information} that is a regularizer to keep the posterior distribution close to the prior. The second term is a simple reconstruction loss. 
For the scope of this work, we utilize a Conditional VAE (CVAE) that augments the latent space with information about the true label of a given sample~\cite{sohn2015learning}.  
%
\subsubsection{Hijacking VAE}
\label{subsub:hvae}
We now introduce Hijacking VAE (HVAE), a variation of the more traditional CVAE that is specifically designed to generate hijacking samples to produce $\mathcal{S}^{Val}_{pois}$.
These samples are created in such a way that, when used for computing $\mathcal{L}_{Val}$, the lower the models' hijack metric, the more significant the increase of their validation loss, hence swaying the model selection phase into returning the model that has the highest hijack metric (which has been the least penalized).
We design the HVAE loss function as follows:
    \begin{equation}
        \label{lossMHVAE}
 \mathcal{L}_{\mathrm{HVAE}} = (\mathcal{L}_{\mathrm{rec}} + \mathcal{L}_{\mathrm{KLD}} - Hj_{cost}(\mathfrak{C})) ^ 2.
    \end{equation}
Here, the terms $\mathcal{L}_{\mathrm{rec}}$ and $\mathcal{L}_{\mathrm{KLD}}$ represents the reconstruction loss and the KL divergence, as in the traditional VAE. 
The novel factor of the loss is represented by the third term $Hj_{cost}(\mathfrak{C})$.
This is the pivotal factor of the attack, defined as follows (with $\Lambda = \mathfrak{C}$):

\begin{equation} \label{cost}
     Hj_{cost}(\mathfrak{C}) = \frac{1}{|\mathfrak{C}|}\sum_{\mathfrak{c} \in \mathfrak{C}} \alpha \cdot \mathcal{L}_{Val}(h_{\mathfrak{c}}, \mathcal{S}_{gen})
\end{equation}
with 
\begin{equation} \label{alpha}
 \alpha = \frac
     {\underset{\lambda \in \Lambda}{\max} \{m(h_{\lambda}, \mathcal{S}^{Val})\} - m(h_ {\mathfrak{c}}, \mathcal{S}^{Val})}
     {\underset{\lambda \in \Lambda}{\max} \{m(h_{\lambda}, \mathcal{S}^{Val})\} - \underset{\lambda \in \Lambda}{\min} \{m(h_{\lambda}, \mathcal{S}^{Val})\}}. 
\end{equation}
 
We now explain the rationale behind Equation~\ref{cost}, which is an average of scores that are assigned to each model $\mathfrak{c} \in \mathfrak{C}$. 
The coefficient $\alpha \in \mathbb{R}$ (see Equation~\ref{alpha}) is computed by normalizing the difference between the maximum hijack metric achievable by a model $h_{\lambda}$ with $\lambda \in \Lambda = \mathfrak{C}$ and the metric of the current model.
$\alpha$ yields higher penalties the lower the hijack metric of the model $h_\mathfrak{c}$, reaching 0 if the considered model has the highest metric. This value is fixed for each model and can be computed independently of the HVAE training.
On the opposite, the second term, $\mathcal{L}_{Val}$, assesses the quality of the generative process to produce effective hijacking samples, as it computes the loss of model $h_\mathfrak{c}$ over $S_{gen}$. It is therefore computed at HVAE training time. 
\par
Ideally, we intend to reward higher $Hj_{cost}$, as higher values imply higher losses toward those models with lower hijack metrics.
Therefore, in our loss function, we aim to maximize this value.  
During the training of the HVAE, by minimizing Equation~\ref{lossMHVAE}, we work toward:
\begin{itemize}
    \item diminishing the reconstruction loss $\mathcal{L}_{\mathrm{rec}}$, so that generated samples can resemble the original operations;
    \item diminishing the $\mathcal{L}_{\mathrm{KLD}}$ for obtaining a useful probability distribution;
    \item increasing the hijacking cost function $Hj_{cost}(\mathfrak{C})$. As the penalty value is fixed, by raising Equation~\ref{cost}, we aim at generating samples $\mathcal{S}_{gen}$, which increase the validation loss based on the magnitude of the penalty itself.
    Models with lower hijack metrics incur higher penalties, leading to increased validation loss on the generated samples. This ensures the samples are crafted to produce lower validation loss values for models with the highest hijack metrics.
    % Therefore, those models with lower hijack metrics will have higher penalties, which results in higher validation loss computed on the generated samples. This allows the creation of samples that, when used for evaluating the validation loss of a model, will return a lower value for the ones with the highest hijack metric.
\end{itemize}
% A graphical representation of how  $\mathcal{S}^{Val}_{pois}$ is generated, can be found in Figure~\ref{MHVAE}. 
% By training the HVAE with the objective function Equation~\ref{lossMHVAE}, it is possible to encode a distribution, that is unlike the input samples one -- usually learned by vanilla VAE -- as it governs the generation of samples such that, when injected in the validation set, they can provide a penalty on the validation loss of models at lower hijack metric.
% We report in Algorithm~\ref{alg.HVAE} the HVAE training procedure.
A graphical overview of $\mathcal{S}^{Val}_{pois}$ generation is shown in Figure~\ref{MHVAE}.
By training the HVAE with the objective function in Equation~\ref{lossMHVAE}, the model encodes a distribution distinct from the input samplesâ€™ usual one, enabling the generation of validation samples that penalize models with lower hijack metrics.
The HVAE training procedure is detailed in Algorithm~\ref{alg.HVAE}.

% \begin{figure*}[!htbp]
%     \footnotesize
%     \centering
%     \includesvg[width=.775\textwidth]{figures/MHVAE-v2.drawio}
%     \caption{Schematic representation of the generation process of $\mathcal{S}^{Val}_{pois}$. For simplicity, we reported samples from the MNIST dataset~\cite{lecun2010mnist}.}
%     % \caption{Schematic representation of $\mathcal{S}^{Val}_{pois}$ generation using MNIST samples~\cite{lecun2010mnist} for simplicity.}
%     \label{MHVAE}
% \end{figure*}

\begin{figure*}[!htbp] %% ARXIV
    \footnotesize
    \centering
    \includesvg[width=.775\textwidth]{figures/MHVAE-v2.drawio}
    \caption{Schematic representation of the generation process of $\mathcal{S}^{Val}_{pois}$. For simplicity, we reported samples from the MNIST dataset~\cite{lecun2010mnist}.}
    % \caption{Schematic representation of $\mathcal{S}^{Val}_{pois}$ generation using MNIST samples~\cite{lecun2010mnist} for simplicity.}
    \label{MHVAE}
\end{figure*}

\begin{algorithm}[H]
\footnotesize
    \caption{Hijack VAE Training Algorithm}
    \begin{algorithmic}[1]
        \State \textbf{Input:} HVAE model with random weights, training data $\mathcal{S}$, $\alpha_{\mathfrak{C}}$, $h_{\mathfrak{C}}$, number of epochs $epochs$
        \State \textbf{Output:} Trained HVAE model
        \For{$e \gets 1$ to $epochs$}
            \For{$\bm{x}$, $y$ in $\mathcal{S}$}  % are batches
                \State $\hat{\bm{x}} \gets $ HVAE.decode(HVAE.encode($\bm{x}$))  % reconstruct input
                \State rec\_loss $\gets \mathcal{L}_{\mathrm{rec}}(\bm{x}, \hat{\bm{x}})$  % reconstruction loss
                \State kl\_loss $\gets \mathcal{L}_{\mathrm{KLD}}(\mathrm{HVAE})$  % KLD loss
                \State $\hat{\bm{x}}_{gen} \gets$ HVAE.decode(gaussian\_noise)  % generate samples from randomly sampled noise
                \State generated\_val\_loss $\gets \mathcal{L}_{Val}(h_{\mathfrak{C}}, \hat{\bm{x}}_{gen})$  % validation loss of all knowm models on the generated samples
                \State hijack\_cost $\gets Hj_{cost}(\alpha_{\mathfrak{C}}, \mathrm{generated\_val\_loss})$  % compute hijack cost using the hijack cost penalty & the loss of the generated samples
                \State total\_loss $ \gets(\mathrm{rec\_loss + kl\_loss - hijack\_cost})^2 $  % obtain the total loss
                \State HVAE.backward\_propagation\_step(total\_loss)  % update weights
            \EndFor
        \EndFor
        \State \textbf{return} HVAE
    \end{algorithmic}
    \label{alg.HVAE}
\end{algorithm}

\subsection{Hijack Metric}
\label{ssec.hm-theory}
Generally, the purpose of a hijack metric $m$ is to produce damage to the target victim. 
\new{
We now introduce four distinct hijack metrics that impact an ML system in three different ways, i.e., generalization capabilities (Section~\ref{subsub:generalization}), latency (Section~\ref{subsub:latency}), and energy consumption (Section~\ref{subsub:energy}).
}
Note that MOSHI is not limited to such metrics, and future investigations might define different attack objectives. 

% \subsubsection{Weaken the Generalization Capabilities}
\subsubsection{\new{Generalization Capability Attack}}
\label{subsub:generalization}
This first intuitive hijack metric objective is to impact the victim model overall performance. 
Here, the objective of the attack under this metric is to choose a model that less generalizes to unseen data (e.g., test set), and therefore the result of an underfitting or overfitting training.
%Therefore, this case can be reconducted to the more traditional 
Therefore, this case can be considered a form of the more traditional \textit{model poisoning attack}~\cite{tian2022comprehensive}.
The metric $m$ -- that we named \textit{Generalization Metric} -- can simply compute the loss of a target model on an unseen dataset (\textit{e.g., validation set}). 
%
\subsubsection{Latency Attack}
\label{subsub:latency}
Increased latency in ML predictions can significantly impact the performance and usability of ML systems.
Higher latency leads to delayed responses, which can degrade user experience, particularly in real-time applications such as autonomous driving, financial trading, and interactive systems. Additionally, increased latency can hinder the efficiency of decision-making processes, as timely data processing is crucial for accurate and effective outcomes. This delay can also exacerbate the accumulation of errors, potentially compromising the reliability and accuracy of the ML model's predictions.
Therefore, an attacker might aim to induce the model selection to peak a model that results in slower predictions, on average, when deployed. 
The function $m$ -- that we named \textit{Latency Metric} -- can be designed by observing the time required by a target model to predict a set of unseen datasets (\textit{e.g., validation set}). 
%
\subsubsection{Energy Consumption Attack}
\label{subsub:energy}
Similarly to what is discussed in the motivation of the latency attack, increasing the overall energy consumption might lead to resource exhaustion. 
We inspire this metric based on the \textit{sponge attack}~\cite{shumailov2021sponge}. 
In our work, we consider two distinct metrics that measure energy consumption. 
\begin{itemize}
    \item \textit{Energy Consumption}: an estimation of the energy consumption of the model utilization that can be obtained through the OS energy consumption hosting such model. 
    \item \textit{$\ell_0$ norm}: the $\ell_0$ norm of the activations of the neurons in the network, obtained by summing the non-zero activations of each ReLU Layer in the model when it is processing a sample $\bm{x}$, then computing the mean for all samples $\bm{x} \in \mathcal{X}$. 
\end{itemize}
We opt to include this metric as \cite{cina2022energy} showed, there exists a strong link between the $\ell_0$ norm of a model and its energy consumption.
For instance, we report in Figure~\ref{l0_energy} the observed correlation between these two metrics in our experimental setting (which we will describe in the upcoming section).  

\begin{figure}[!htbp]
    \footnotesize
    \centering
    \includesvg[width = .8\linewidth]{figures/MNIST-normalized-energy-l0-v2}
    % \vspace{-15pt}
    \caption[Histogram comparing $\ell_0$ norm and energy consumption per layer.]{Histogram comparing $\ell_0$ norm and energy consumption per layer on FFNNs from 1 to 10 layers of 32 neurons, trained on MNIST dataset with a learning rate of 0.001.}
    \label{l0_energy}
\end{figure}


\subsection{White-box vs Black-box scenarios}
The HVAE requires knowledge about the target models, as described in Equation~\ref{cost} in the $Hj_{cost}$. Models in the grid are utilized for measuring their performance with the hijack metric and for understanding the quality of the dataset $S_{gen}$ produced by HVAE.
As we previously anticipated, in our work we consider a white-box and black-box case study. In the former, we assume the attacker has access to the exact models of the model grid. In the latter, the attacker has no such knowledge.
However, we assume that the attacker has knowledge about both training and validation sets. We can therefore leverage the \textit{adversarial transferability} of attacks. 
\par
Adversarial transferability in AML refers to the phenomenon where adversarial examples crafted to deceive one ML model can also deceive other models, even if they have different architectures or were trained on different datasets~\cite{demontis2019adversarial, alecci2023your}. This property is significant because it highlights the vulnerability of ML systems to attacks that are not specifically tailored to them, thereby posing a broader security risk.