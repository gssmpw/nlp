\section{Introduction}

Adversarial Machine Learning (AML) represents a rapidly advancing subfield within Artificial Intelligence (AI) and cybersecurity, focusing on studying and mitigating attacks that exploit vulnerabilities in Machine Learning (ML) models.
The seminal work by Barreno et al. in 2006, titled ``\textit{Can machine learning be secure?}''~\cite{barreno2006can}, marked the inception of this critical area of research.
This pioneering study introduced a taxonomy of attacks based on three key properties: security violation, influence, and specificity.
Since then, extensive research has been conducted to explore various aspects of AI security, addressing numerous challenges and significantly enhancing our understanding of safeguarding ML systems against adversarial threats.
\par
Adversarial attacks can be broadly categorized into several families, each targeting different aspects of ML systems
For instance, the \textit{evasion attack} involves manipulating input data to deceive a model into making incorrect predictions~\cite{biggio2013evasion}.
This attack exploits vulnerabilities in the model's decision boundary to achieve misclassification.
\textit{Model poisoning} refers to the deliberate contamination of training data to corrupt the learning process~\cite{biggio2012poisoning}.
This attack aims to degrade the model's performance or manipulate its predictions by introducing maliciously crafted data in the training set during the training phase.
The \textit{membership attack} aims to determine whether a specific data point was part of the model's training dataset~\cite{shokri2017membership}.
This attack exploits the model's overfitting tendencies to infer the presence of individual data samples, potentially compromising data privacy.
\par
With the growing integration of AI into commercial and industrial assets, it is becoming increasingly vital to understand how adversarial attacks can be executed in real-world applications and to evaluate their potential impact.
An example of a popular AI incident is Microsoft's AI chat bot ``Tay''~\cite{wolf2017why}. 
Tay was a chatbot launched on Twitter that was intended to learn from user interactions.
Unfortunately, due to the actions of malicious users, Tay was swiftly influenced to adopt offensive and racist language.
\par
Exploring various attack vectors that could compromise the quality of ML development and deployment is paramount.
% In this article, we address the critical question: \textit{can an attacker influence the model selection phase?}
\new{However, despite its critical role, \textit{no prior work in the literature has considered the possibility of adversarial attacks targeting the model selection phase}.}
Model selection, the process of choosing the most suitable algorithm or model from a pool of candidates for a specific task, is essential for ensuring AI systems' effectiveness and generalization capability.
This issue is especially pressing in the current era, where the Machine-Learning-as-a-Service (MLaaS) paradigm is widely adopted.
% Adversarial attacks, such as the Model Selection Hijacking, could have far-reaching consequences, degrading model performance and significantly impacting aspects like energy consumption and execution costs.
\new{
% Traditionally, model selection aims to optimize the generalization of a model, focusing on its ability to perform well on unseen data. However, if exploited maliciously, this process could serve as a mechanism to impose unintended or even harmful characteristics on the selected model. This misuse represents a significant departure from its conventional role, as model selection is not typically used to infer specific traits into a model, making this exploitation a novel and critical vulnerability.
If extended to target the model selection phase, adversarial attacks could have far-reaching consequences, degrading model performance and significantly impacting energy consumption and execution costs.}
Such attacks could force models to use more computationally expensive algorithms, leading to higher operational costs for both the user and the service provider.
This raises critical challenges, as the increased resource consumption, prolonged task execution, and degraded performance would burden both parties, resulting in elevated costs in running ML models.
Therefore, in this article, we address the critical question: \textit{can an attacker influence the model selection phase?}
% \par
\paragraph{Contribution.}
% \fra{Ho cercato di implementare il consiglio di Mauro di non ripetere troppo le stesse cose nel paragrafo e nei bullet e di dare priorità alla contribution nell'itemize.}
\new{
In this paper, we are the first to present an adversarial attack against the model selection phase.
Our attack called \textbf{MOSHI} (\textbf{MO}del \textbf{S}election \textbf{HI}jacking adversarial attack) targets only the validation set of an ML system, manipulating the model selection process to favor a model with properties advantageous to the attacker.
We generate adversarial samples using a modified conditional Variational Autoencoder (VAE) that optimizes a hijack metric while leaving the victim’s original loss metric and code unaltered.
The result is a scenario where the model that best aligns with the attacker’s hijack metric also appears to minimize the validation loss, ultimately being selected by the model selection process.
Our contributions can be summarized as follows.
}
\begin{itemize}
    \item \new{We introduce MOSHI, the first adversarial attack for model selection hijacking.
    Our attack leverages a VAE optimizing a specifically crafted hijack metric to inject malicious samples in the validation set of an ML system. In our experiments, we demonstrate that MOSHI can undermine critical aspects of ML performance like generalization capability, latency, and energy consumption.}
    \item \new{Our attack methodology considers various scenarios based on the attacker’s level of knowledge, namely \textit{white-box} and \textit{black-box} scenarios.
    These experimental settings demonstrate the effectiveness of our attack and highlight its practical feasibility across a wide range of real-world scenarios.}
    \item \new{We assess the impact of our proposed attack across three distinct benchmarks in computer vision and speech recognition, achieving an average Attack Success Rate (ASR) of 75.42\%.
    % Notably, in our evaluation, our attack results in an 88.30\% reduction in generalization capabilities, an 83.33\% increase in latency, and a 12.65\% rise in energy consumption.
    Notably, in our evaluation, our attack results in an 88.30\% reduction in generalization capabilities, an 83.33\% increase in latency, and a 105.85\% rise in energy consumption (12.65\% when estimated through a simulator, 167.98\% when estimated through $\ell_0$ norm).
    }
    % \item \new{We publicly release our code and implementation at: \url{https://anonymous.4open.science/r/MOSHI-1518}.}
\end{itemize}

\paragraph{Organization.}
\new{
The rest of this paper is organized as follows.
Section~\ref{sec:related} presents an overview of related works in AML, while Section~\ref{sec:background} provides background knowledge on ML systems principles.
We present our system and threat model in Section~\ref{sec:stmodel}, providing motivations and actionable scenarios for our attack.
MOSHI's methodology is detailed in Section~\ref{sec:methodology}, and in Section~\ref{sec:evaluation} we provide our experimental settings and evaluation.
Finally, we discuss our results in Section~\ref{sec:discussion}, and Section~\ref{sec:conclusion} concludes our work.
}
% We present \textit{Model Selection Hijacking Adversarial Attack} (MSHAA), a novel attack of its kind that aims to hijack the model selection phase toward a model with properties desired by an attacker. 
% By injecting carefully crafted adversarial samples into the validation set, the attacker influences the model selection process to favor an architecture that optimizes a target metric, known as the hijack metric, while leaving the victim’s original loss metric and code unaltered.
% To achieve this, we introduce the \textit{Hijacking-VAE} (HVAE), a variant of the traditional Conditional VAE specifically designed to generate hijacking samples tailored to the attacker’s goals.
% Notably, this attack only modifies the validation set, without altering the validation procedure, the model training phase, or the training data itself.
% The result is a scenario where the model that best aligns with the attacker’s hijack metric also appears to minimize the validation loss, ultimately being selected by the model selection process.
% \par
% We consider various scenarios based on different levels of attacker knowledge. In a white-box attack, the adversary possesses full knowledge of the trained models, including architectures, parameters, and the ability to freely query the models for inference. In contrast, in a black-box attack, the attacker’s knowledge is limited to the model architectures used in the selection phase, without access to trained parameters or inference capabilities. These scenarios allow us to evaluate the effectiveness of the Model Selection Hijacking Adversarial Attack under different threat models and attacker constraints. To assess the feasibility of the attack in achieving different objectives, we explore various hijack metrics, including those related to generalization capability, energy consumption, and prediction time costs. We evaluate their potential impacts to understand how different metrics can influence the outcome of the Model Selection Hijacking Adversarial Attack.
% We evaluate the proposed attack using three datasets—two from the field of Computer Vision and one from Speech Recognition. To better assess the attack's effectiveness, we introduce two new metrics: the Effectiveness Score Function (ESF) and the Attack Success Rate (ASR).
% \par
% % \textcolor{red}{ The results obtained are promising, demonstrating the potential of the Model Selection Hijacking Adversarial Attack to effectively manipulate model selection processes across various scenarios.}\lpasa{da estendere}
% We summarize our contributions as follows. 
% \begin{enumerate}
%     \item We define a new adversarial attack named \textit{Model Selection Hijacking Adversarial Attack} (MSHAA), aiming to influence the ML model selection by only affecting the validation set. 
%     \item We propose a novel generative model named \textit{Hijacking-VAE} (HVAE) to generate malicious samples that produce an MSHAA. 
%     \item We demonstrate the feasibility of producing the MSHAA through HVAE with an in-depth experimental setting consisting of three benchmarks spanning from computer vision to speech classification. 
% \end{enumerate}


% The attack consists of injecting malicious samples in the v 