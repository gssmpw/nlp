\section{Related Work}
\label{sec:related}

% \fra{Per togliere la subsec di contribution (as requested by reviewers) ho riassunto key contribution points w.r.t. existing literature sui rispettivi paragraph.}
In this section, we review the adversarial attacks from recent years that are related to the MOSHI attack presented in this work.
These include various approaches aimed at influencing model behavior during training or evaluation, providing context for the novelty and scope of MOSHI in comparison to existing methods.
% We can model three different levels for the knowledge of an attacker:
% \begin{itemize}
%    \item \textit{White-Box Attacks}. The attacker has full knowledge of the ML system: from its architecture, model parameters, training, and testing data. This setting, while unrealistic, is useful as it allows studying an upper bound on the attacker's capabilities.
%    \item \textit{Black-Box Attacks}. The adversary has no knowledge of the inner configurations of the victim's model, but can just feed input data and query its outputs. These attacks are more practical, as they assume no previous knowledge and make use of the legitimate interfaces for accessing the model.
% \end{itemize}

% \subsection{Model Poisoning}
\paragraph{Poisoning Attacks.}
In AML, model poisoning deliberately manipulates the training process by introducing maliciously crafted data into the training dataset~\cite{tian2022comprehensive}.
This manipulation aims to corrupt the model's learning algorithm, leading to compromised performance and erroneous outputs.
Such attacks can subtly alter the decision boundaries, degrade the model's accuracy, or embed specific vulnerabilities, enabling the attacker to influence future predictions or decisions.
Model poisoning exploits the trust in the training data, highlighting the importance of securing and validating data sources in ML workflows.
\new{
Our proposed attack involves poisoning the victim's data but differs significantly from traditional techniques in three key ways: \textit{(i)} MOSHI targets only the validation set, leaving the training set untouched; \textit{(ii)} it does not modify the trained model parameters but manipulates the model selection process to choose a less optimal (yet legitimate) model; and \textit{(iii)} it remains imperceptible, as it introduces no detectable anomalies during training.
This unique design makes MOSHI highly adaptable across various scenarios, as it exclusively impacts validation data without altering any aspect of the victimâ€™s model selection process.
}

% \subsection{Attacks on the Availability}
\paragraph{Availability Attacks.}
Adversarial attacks targeting availability, often called Denial-of-Service (DoS) attacks, aim to disrupt the regular operation of AI systems, rendering them unusable or significantly degrading performance.
These attacks flood the system with excessive, irrelevant, or malicious data, overwhelming its processing capabilities and leading to resource exhaustion.
This can cause delays, reduced accuracy, or complete shutdowns of the AI services, effectively preventing legitimate users from accessing the system.
Examples of attacks are exploiting software vulnerabilities~\cite{xiao2018security}, Camouflage Attack~\cite{xiao2019seeing}, and Zero-Width Attack~\cite{pajola2021fall}.
\new{
Likewise attacks on the availability, MOSHI can introduce an increased utilization of resources.
However, MOSHI objective function is purely guided by a hijack metric measuring model properties like latency, energy consumption, or generalization error.
}

% \subsection{Energy-Latency Attacks}
\paragraph{Energy-Latency Attacks}
During the deployment phase, ML models, particularly Deep Neural Networks (DNNs), exhibit high demands on computational resources and memory capacity.
For this reason, more and more specialized hardware is being developed, such as Big Basin~\cite{hazelwood2018applied}, Project BraiwWave~\cite{chung2018serving}, and Tensor Processing Units (TPUs)~\cite{jouppi2018motivation}.
We refer to these highly specific chipsets as Application-Specific Integrated Circuits (ASIC).
% \par
Shumailov et al.~\cite{shumailov2021sponge} kick-started the AML research trend focusing on the increase in consumption of resources (e.g., energy, latency) of victims' models.
They presented the \textit{sponge examples attack}: the attack generates specific adversarial samples, termed sponge samples, which are introduced to the victim's model.
Unlike previously known attacks, the primary objective of this method is to escalate the latency and energy consumption of the target model, disregarding the prediction accuracy of the samples.
Consequently, the primary goal of this attack is to induce an availability breakdown.
The attack can be carried out by working on either \textit{(i)} the computation dimension, which increases the internal representation dimension to increase the time complexity of computations (useful in Natural Language Processing); \textit{(ii)} data activation sparsity, ss ASICs use runtime data sparsity to increase efficiency (like skipping operations), thus inputs that lead to less sparse activations will lead to more operations being computed (useful in Computer Vision).
% \par
Cina et al.~\cite{cina2022energy} introduced a poisoning attack aiming to increase the latency and energy consumption of a victim's model, without affecting the prediction performance.
The attack is carried out via sponge training (a type of model poisoning), which is a carefully crafted algorithm that minimizes the empirical risk of the training data while maximizing energy consumption.
The algorithm objective is pursued by increasing the model's activation (i.e. $\ell_0$ norm: the number of firing neurons) to undo the advantages of ASIC accelerators.

% \subsection{Contribution}
% The attack proposed in this work -- the \textit{Model Selection Hijacking Adversarial Attack} (MSHAA) -- has similar traits to traditional attacks such as poisoning and availability attacks but offers a new adversarial perspective to the research community. 
% First, we are the first to consider how to manipulate the model selection phase which is a critical stage of ML development. 
% To the best of our knowledge, MSHAA is the first adversarial attack to explore the possibility of negatively influencing the model selection phase of a target model.
% Second, likewise poisoning attacks, MSHAA consists of poisoning the victim's data. However, MSHAA has different properties that make it significantly different from traditional poisoning attacks.
% \begin{itemize}
%     \item MSHAA alters only the validation set, while traditional poisoning attacks tamper the training set. 
%     \item MSHAA does not alter model-trained parameters, unlike model poisoning, since it lets the model selection pick a less optimal -- but legitimate -- model. 
%     \item MSHAA is imperceptible, as the learned model does not show any sign of forging during the training. Similarly, as we do not alter the training data, the traditional model poisoning mechanism cannot be utilized. 
% \end{itemize}
% It is important to note that MSHAA does not depend on specific characteristics of the targeted model, its training process, or model selection policy. Instead, MSHAA solely poisons the validation data without modifying any aspect of the victim's model selection procedure. This design makes MSHAA highly adaptable to any scenario, regardless of the model used or the training and selection techniques employed by the victim.
% Third, likewise attacks on the availability, MSHAA can introduce an increased utilization of resources. However, MSHAA objective function is purely guided by a hijack metric measuring model properties like latency, energy consumption, or generalization error. Therefore, MSHAA is generalizable to the attacker's objective. 