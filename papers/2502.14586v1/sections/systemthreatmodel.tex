\section{System and Threat Model}
\label{sec:stmodel}

\new{
In this section, we describe our system and threat model (Section~\ref{subsec:system} and Section~\ref{subsec:threat}) while also providing different motivations and scenarios of importance for our proposed attack (Section~\ref{subsec:motivation}).
}

\subsection{System Model}
\label{subsec:system}

\new{
The systems we consider reflect a typical ML pipeline that addresses diverse real-world tasks, such as image classification and speech recognition.
These pipelines often involve model selection as a critical phase to ensure the generalization capabilities of the model.
On the other hand, this work demonstrates that poisoning model selection can also be used to influence the behaviors and characteristics of an ML algorithm.
Indeed, different domains often impose distinct demands on model characteristics.
For example, real-time applications like autonomous systems require low-latency models to ensure timely decision-making, while tasks that impact human lives or critical infrastructure prioritize accuracy and reliability above all.
Additionally, deployment scenarios further shape these requirements, as energy-efficient models are indispensable for battery-powered devices operating in resource-constrained environments.
}

\subsection{Threat Model}
\label{subsec:threat}

Our proposed attack aims to control the model selection phase by injecting maliciously crafted samples into the sole validation set.  
In this scope, the adversary goal is to let the model selection phase select the model architecture that suits the best target metric, named \textit{hijack metric} $m$. 
The hijack metric is arbitrary and can be chosen by the adversary.
Lastly, we assume that adversaries cannot modify the standard model selection routine, including the loss metric defined by the victim, the underlying code, or the model selection policy.
This means that with MOSHI, we inject samples that influence the loss function evaluation on the validation set, ensuring that the model best suited to the hijack metric achieves a lower loss than other models in the validation grid.
In this work, we explore different types of hijack metrics.

\paragraph{Assumptions.}
Suppose the victim has access to a dataset $\mathcal{S}$, divided in train $\mathcal{S}^{Train}$, validation $\mathcal{S}^{Val}$, and testing $\mathcal{S}^{Test}$ sets.
The former is used to train several model $h_{\mathfrak{c}}$ grouped in a model set identifiable with $\mathfrak{C}$,\footnote{$\mathfrak{c}$ represents the configuration of hyperparameters and $\mathfrak{C}$ the set of all possible configurations of hyperparameters.} the second to perform model selection, and the latter to test the best model performance.
The model selection phase will return the model $h_{\mathfrak{c}^*}$ with the lowest true error estimate on the validation set among those trained:
    \begin{equation}
        \label{best_val}
 h_{\mathfrak{c}^*} = \argmin_{h_{\mathfrak{c}} : \mathfrak{c} \in \mathfrak{C}} \mathcal{L}_{Val}(h_{\mathfrak{c}}, \mathcal{S}^{Val}).
    \end{equation}
In this context, we postulate that the adversary possesses access to the validation set $\mathcal{S}^{Val}$, enabling them to read and modify its data.
Consequently, our newly proposed adversarial family, MOSHI, exhibits characteristics akin to data poisoning, as it allows the attacker to alter the victim’s dataset.
In contrast with the traditional poisoning attack, our proposed attack operates solely at the validation set level, leaving the training set unaltered.

\paragraph{Adversary Knowledge.}
We now outline the adversary knowledge, detailing an attacker's information and capabilities and the limitations and constraints they face within our threat model.
We explore two distinct attack settings.
\begin{itemize}
    \item \textit{White-Box (WB) attack}: adversaries with full knowledge of the trained models $h_{\mathfrak{c}}, \;  \forall c \in \mathcal{C}$ (i.e., architectures, trained parameters and they can freely query the model for inference purposes). 
    \item \textit{Black-Box (BB) attack}: adversaries with knowledge limited to the model's architectures $\mathfrak{C}$ (i.e., the models that will be learned and tested during the model selection phase). 
\end{itemize}
Furthermore, in both scenarios, adversaries know $\mathcal{S}^{Train}$ but cannot tamper it. 
We again recall that -- for both white-box and black-box settings -- the attacker is limited to modifying the only validation set $\mathcal{S}^{Val}$.
A schematic representation of the considered threat model is depicted in Figure~\ref{fig:overview}.

\begin{figure*}[!htpb]
    \centering
    \includegraphics[width=0.65\linewidth]{figures/attack-overview.pdf}
    \caption{Schematic representation of the MOSHI threat model.}
    \label{fig:overview}
\end{figure*}

\paragraph{Stealthiness.}
Poisoning only the validation set allows all the trained models and their parameters not to be modified during the attack.
This means that once attacked, the model selection phase, which is carried out in an unmodified way, returns a model that has been regularly trained on clean data, and among all the ones trained is of higher hijack metric, which the adversary can choose.
This ensures that the selected model reveals no evidence of the attack, as its performance on the victim's loss function remains close to that of models selected using the original validation data.
However, it differs regarding the hijack metric, which is influenced to align with the attacker’s desired outcome.

\subsection{Motivations and Scenarios}
\label{subsec:motivation}

\new{
The execution of the MOSHI attack can have significant repercussions for various victims by leveraging dynamics similar to traditional poisoning and backdoor attacks~\cite{tian2022comprehensive, li2022backdoor}.
One compelling scenario involves the public dissemination of open-source datasets, which are often pre-structured into standard partitions (training, validation, and testing splits) to ensure the reproducibility of experimental results.
Platforms like Hugging Face\footnote{\url{https://huggingface.co/datasets}} and Kaggle\footnote{\url{https://www.kaggle.com/}} host datasets extensively adopted by the ML community, with some datasets recording hundreds of thousands of downloads.
This popularity underscores their critical role in research and development.
However, several studies have demonstrated the feasibility of poisoning these datasets and successfully degrade model performance~\cite{carlini2024poisoning}.
Furthermore, in MLaaS, service providers often depend on client-supplied datasets, significantly increasing the risk of attackers tampering with the data.
Another scenario arises when service providers themselves act maliciously.
By providing tampered validation sets engineered using MOSHI, they could force clients to select models with higher energy consumption while maintaining optimal generalization capabilities.
This covert manipulation would increase operational costs for users while benefiting the provider financially through inflated billing, all without raising immediate suspicion.
% Similarly, federated learning systems are vulnerable, as adversaries posing as clients can manipulate local data to disrupt global model selection, leading to suboptimal or biased federated models.
These attacks are particularly concerning in critical applications like healthcare or autonomous systems, where inefficiencies can directly impact safety.
Moreover, in edge computing scenarios, compromised validation can result in energy-intensive models, severely affecting battery-powered devices.
By highlighting these vulnerabilities, MOSHI underscores the urgent need for robust countermeasures against adversarial interference in model selection.
}

% Provider colluso con see stesso che da validation set con nostro attacco per fare si che l'energy consumption sia Maggiore e quindi ti billa di più, ma generalization è uguale quindi gg per l'utente

