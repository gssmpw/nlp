\section{Evaluation}
\label{sec:evaluation}

\new{
In this section, we present the experimental settings of our tests (Section~\ref{subsec:experimental}) and the results of our attack (Section~\ref{subsec:results}).
}

\subsection{Experimental Settings}
\label{subsec:experimental}

\new{
We now describe the experimental settings utilized to demonstrate the MOSHI attack.
We first present the datasets and models on which we test our methodology (Section~\ref{subsub:cases}), and then we show our attack grids and HVAE implementations (Section~\ref{ssec.grid}).
Finally, we provide details on the metrics used for hijacking (Section~\ref{subsub:hmetrics}) and for the evaluation (Section~\ref{subsub:emetrics}).
}
The experiments are conducted utilizing Python 3.10 scripts, and standard ML libraries to design deep learning experiments such as PyTorch~\cite{paszke2019pytorch}. 
All experiments are executed on a machine with 32GB RAM with an NVIDIA GeForce RTX 3090.



\subsubsection{Case Studies}
\label{subsub:cases}
We designed three distinct case studies to test the MOSHI attack performance: two yield from the Computer Vision (CV) field, and the remaining one from Speech Recognition (SR), all in the classification task.
We opted to test these two distinct application domains to understand the generalization of the attack in different AI scenarios. 
We now briefly describe the three tasks.\footnote{All datasets were imported using the corresponding PyTorch libraries.} 
\begin{itemize}
    \item \textit{MNIST (CV)~\cite{lecun2010mnist}}. It is a dataset of bilevel $28 \times 28$ handwritten digits divided into 10 classes: one for each digit from 0 to 9. The victim models considered for this case are FeedForward Neural Networks, with hyperparameters grid concerning the number of neurons per layer and the number of layers themselves. 
    \item \textit{CIFAR10 (CV)~\cite{cifar10}}. It is a dataset of colored $32 \times 32$ images of various subjects belonging to 10 classes: airplane, automobile, bird, cat, deer, dog, frog, horse, ship, and truck. In this case study the victim models are DenseNets \cite{huang2017densely}, of which we adapted the implementation done by \cite{PyTorchDenseNet}. The considered hyperparameters grid sees the number of dense blocks and the number of neurons for the fully connected layer which feeds the output one.
    \item \textit{Speech Commands (SR)~\cite{speechcommandsdataset}}. It is a dataset composed of audio files that are about 1 second long (around 16000 frames long) and belong to 35 classes of different commands spoken by different people. Here, the victim models are Convolutional Neural Networks composed of 1D convolutional layers, and the considered hyperparameters are the number of convolutional layers and the width of the fully connected layer that feeds the output one (code adapted from \cite{PyTorchSpeechCommands}).
\end{itemize}
All the models considered are feedforward neural networks, with an output composed of one or more dense layers that map the hidden representations to the output space.
The architecture of the hidden layers varies depending on the specific task. A detailed list of the layers used is provided in Table \ref{tab.hp}.
In total, we trained 472 models, considering both white-box and black-box scenarios, 360 for MNIST, 84 for CIFAR10, and 32 for Speech Commands. 
Figure~\ref{fig:clean_samples} shows two examples for each dataset. Note that for the Speech Commands, we display the frequency spectrum. 

\begin{table}[!htpb]
\centering
\footnotesize
\caption{Hyper parameters grouped by use case. }
\begin{tabular}{cc} \toprule
\multicolumn{2}{c}{\textit{\textbf{MNIST}}} \\ \midrule
    $\#$ layers &     $[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]$        \\
    $\#$ neurons &     $[32, 64, 128]$        \\
    $\#$ learning rate &     $[0.0001, 0.0005, 0.001, 0.005, 0.01, 0.05]$       \\ \midrule
    \multicolumn{2}{c}{\textit{\textbf{CIFAR10}}} \\ \midrule
    $\#$ dense blocks &     $[2, 3, 4, 5, 6, 7, 8]$        \\
    $\#$ neurons dense layer&     $[128, 256, 512]$        \\
    $\#$ learning rate &     $[0.001, 0.005]$       \\ \midrule
    \multicolumn{2}{c}{\textit{\textbf{Speech Commands}}} \\ \midrule
    $\#$ 1D conv layers &     $[2, 4, 6, 8]$        \\
    $\#$ neurons dense layer &     $[128, 256]$        \\
    $\#$ learning rate &     $[0.001, 0.005]$       \\ \bottomrule
\end{tabular}
\label{tab.hp}
\end{table}

% \begin{figure}[!htpb]
%     \centering
%     \includesvg[width=0.5\linewidth]{figures/clean_samples}
%     % \caption{Illustration of samples from $\mathcal{S}^{Val}_{pois}$ for the three tasks: MNIST, CIFAR10, and Speech Commands.}
%     \caption{Illustration of samples from $\mathcal{S}^{Val}$ for the three tasks: MNIST, CIFAR10, and Speech Commands.}
%     \label{fig:clean_samples}
% \end{figure}

\begin{figure}[b]
  \centering
  \begin{subfigure}{0.215\linewidth}
     \centering
     \includesvg[width=\linewidth]{figures/4-MNIST}
     % \includegraphics[width=\textwidth]{}
     \caption{MNIST}
     \label{subfig:4mnist}
  \end{subfigure}
  \hspace{0.05\linewidth}
  \begin{subfigure}{0.215\linewidth}
     \centering
     \includesvg[width=\linewidth]{figures/4-CIFAR}
     \caption{CIFAR10.}
     \label{subfig:4cifar}
  \end{subfigure}
  \hspace{0.05\linewidth}
  \begin{subfigure}{0.215\linewidth}
     \centering
     \includesvg[width=\linewidth]{figures/4-SR}
     \caption{SpeechC.}
     \label{subfig:4sr}
  \end{subfigure}
  \caption{Illustration of samples from $\mathcal{S}^{Val}$.}
  \label{fig:clean_samples}
\end{figure}
 

\subsubsection{Attack Grid}
\label{ssec.grid}
Regardless of the case study at hand, we conduct the attacks by considering each task's whole model's grids but also subsets. 
For instance, in the MNIST case, we execute the attack when considering the whole hyperparameters grid (i.e., the number of layers, number of neurons, learning rate) but also in cases where we fixed - for instance - the learning rate and we vary the other two hyperparameters. 
With this experimental setting, we aim to understand the attack effectiveness at different granularity.
Table~\ref{tab.grid} describes the attack grid. 
For instance, when considering the MNIST scenario, we can execute the attack on the entire combination of hyperparameters (\textit{i.e.} learning rate, number of layers and neurons), consisting in one set containing 180 models, or, when grouping on the learning rate, we obtain 6 distinct sets (one per learning rate) constituted by 30 models ($3$ types of neurons $\times$ $10$ types of number of layers). 

\begin{table}[!htpb]
\centering
\footnotesize
\caption{Attack grid grouped by use case. }
\begin{tabular}{ccc} \toprule
\multicolumn{3}{c}{\textit{\textbf{MNIST}}} \\ \midrule
    \textit{Grouped-by} & \textit{\# sets} & \textit{\# models}\\ \hline
    Global &     1    &  180 \\
    learning rate &     6  &  30   \\
    learning rate \& $\#$ neurons &  18 &  10\\ \midrule
    \multicolumn{3}{c}{\textit{\textbf{CIFAR10}}} \\ \midrule
    Global &  1  &  42 \\
    learning rate &  2   & 21\\
    learning rate \& $\#$ neurons &    6    &   7 \\ \midrule
    \multicolumn{3}{c}{\textit{\textbf{Speech Commands}}} \\ \midrule
    Global &     1   &  16\\
    learning rate &   2  &  8 \\ 
    learning rate \& $\#$ neurons &    4   &  4  \\ \bottomrule
\end{tabular}
\label{tab.grid}
\end{table}
\par
In both white-box and black-box settings, we assume that the attacker has some different amount of knowledge at his disposal when launching the attack.
In other words, we used sets of the models, either clean or surrogate (obtained the same way as the aforementioned attack grid), to train the HVAE.
In total, we trained 50 HVAE in MNIST: 25 sets over the three grouping conditions multiplied by the two knowledge levels i.e., white-box and black-box).
Similarly, we trained 18 and 14 HVAE for CIFAR10 and Speech Commands, respectively. 
\par
Finally, for each dataset, we trained HVAE with different architectures: 
\begin{itemize}
    \item \textit{MNIST}. The HVAE comprises only fully connected layers and is trained using Binary Cross Entropy as a reconstruction loss ($\mathcal{L}_{\mathrm{rec}}$). 
    \item \textit{CIFAR10}. For CIFAR10, the HVAE adopted is composed of 2D convolutional layers, and the adopted reconstruction loss ($\mathcal{L}_{\mathrm{rec}}$) is the Mean Square Error.
    \item \textit{Speech Commands}. The HVAE are made of 1D convolutional layers, with the Mean Square Error as the reconstruction loss ($\mathcal{L}_{\mathrm{rec}}$).
\end{itemize}

\subsubsection{Hijack Metrics}
\label{subsub:hmetrics}
We now describe the implementation details of the hijack metrics described in Section~\ref{ssec.hm-theory}. 
\begin{itemize}
    \item \textit{Generalization Metric}. This hijack metric consists of the loss obtained by our models when processing the original (legitimate) validation set. 
    \item \textit{Latency Metric}. It is obtained by measuring the overall processing time of a given set of input data.
    \item \textit{Energy Consumption}. it is computed by an ASIC simulator \footnote{\url{https://github.com/iliaishacked/sponge_examples}} \footnote{\url{https://github.com/Cinofix/sponge_poisoning_energy_latency_attack}} adapting the functions already developed for \cite{shumailov2021sponge} and \cite{cina2022energy}. As this tool was not compatible with the specific architecture used for the classification models of the SpeechCommands dataset, this was the only case in which we did not consider this specific metric.
    \item \textit{$\ell_0$ Norm}. we computed the mean of the non-zero activations of each ReLU Layer in our models when processing a set of input data.
This metric is closely related to energy consumption and can be considered an alternative that does not rely on ASIC simulators. As a result, it can be applied to any architecture, offering broader applicability.
\end{itemize}

\subsubsection{Evaluation Metrics}
\label{subsub:emetrics}
In order to qualitatively estimate the effectiveness of the
attack against the specific model’s grid set considered, we
introduce two novel distinct metrics: the \textit{Effectiveness Score Function} (ESF) and the \textit{Attack Success Rate} (ASR).
\par
% In order to qualitatively estimate the effectiveness of the attack against the specific \textcolor{red}{model's grids} \textcolor{blue}{set} considered, we introduce the \textit{Effectiveness Score Function} (ESF). 
The ESF goal is to understand the impact of MOSHI on the victim model when deployed.
Therefore, with ESF we measure the attack returned model $\tilde{h}_{\mathfrak{c}^*}$ and the legitimate $h_{\mathfrak{c}^*}$ based on the hijack metric $\mathcal{E}$.
Formally, ESF is defined as follows:
\begin{equation}
     \mathscr{S}(\mathfrak{C}) = \frac{
         \mathcal{E} ( \tilde{h}_{\mathfrak{c}^*}, \mathcal{S}^{Test} ) 
         - 
         \mathcal{E} ( h_{\mathfrak{c}^*}, \mathcal{S}^{Test} )
     }
     {
         \underset{\mathfrak{c} \in \mathfrak{C}}{\max} \{\mathcal{E}(h_{\mathfrak{c}}, \mathcal{S}^{Test})\} - 
         \mathcal{E} (    h_{\mathfrak{c}^*}, \mathcal{S}^{Test}    )
     }.
\end{equation}

ESF takes the difference between a chosen metric $\mathcal{E}$ of the models $\tilde{h}_{\mathfrak{c}^*}$ (see Equation~\ref{best_poison}) and $h_{\mathfrak{c}^*}$ (see Equation~\ref{best_val}) normalizing it with the difference between the maximum possible chosen metric of a model $h_\mathfrak{c}$ with $\mathfrak{c} \in \mathfrak{C}$, and again the chosen metric by $h_{\mathfrak{c}^*}$.\\
This function allows us to evaluate our attack with any metric $\mathcal{E}$, for the purpose of this study, with regards to the hijack metric considered:
\begin{itemize}
    \item \textit{Generalization Metric}. $\mathcal{E}$ is the loss function computed on the original validation set;
    \item \textit{Latency Metric}. $\mathcal{E}$ is the latency while processing the test set.
    \item \textit{Energy Consumption} or \textit{$\ell_0$ Norm}. $\mathcal{E}$ is the estimated energy consumed by the model while processing the test set. When testing the attack in the SpeechCommands case, as we could not compute the \texttt{Energy} metric, we evaluated the quality of the attack using the \texttt{$\ell_0$ Norm}, by adopting $\mathcal{E}$ to be the $\ell_0$ Norm.
\end{itemize}
This Effectiveness Score function allows us to give a normalized score that sums up the effectiveness of the our attack, as a matter of fact:
\begin{itemize}
    \item \textit{Successful}, when $ESF > 0$, the MS phase to return a model with higher $\mathcal{E}$ compared to $h_{\mathfrak{c}^*}$. If the score is equal to $1.0$, then the returned model $\tilde{h}_{\mathfrak{c}^*}$ is the one among the considered models to have the highest $\mathcal{E}$.
    \item \textit{Invariant}, when $ESF = 0.0$, the attack did not sway the MS phase, and the returned model is $\tilde{h}_{\mathfrak{c}^*} = h_{\mathfrak{c}^*}$, \textit{i.e.} the returned model is also the legitimate one. 
    \item \textit{Unsuccessful}, $ESF < 0.0$ imply that the attack made the MS phase return a model of less $\mathcal{E}$ compared to $h_{\mathfrak{c}^*}$.
\end{itemize}  
\par
We define the ASR as a simplification of the ESF. In particular, we define the ASR as a boolean variable of a successful or unsuccessful attack, and therefore the $ASR = 1$ if $ESF > 0$, $ASR = 0$ otherwise. 
With this metric, we are interested in understanding when an attack is successful in terms of increasing the hijack metric. 



\subsection{Results}
\label{subsec:results}

We analyze the impact of the MOSHI attack on the experimental settings previously described.
As our attack generates malicious poisoning samples to inject into the validation set, we explore the attack effect in two fashions.
\begin{itemize}
    \item \textit{Full substitution}, where the validation set is entirely substituted by the malicious samples (Section~\ref{subsub:full}). 
    \item \textit{Partial substitution}, where the validation set combines both legitimate and malicious samples at different rates, i.e. 10\%, 20\%, 50\%, 80\%, 100\% (Section~\ref{subsub:partial}). 
\end{itemize}
In Section~\ref{subsub:impact} we measure the attack performance by comparing -- for each grid attack presented in Table~\ref{tab.grid} -- the legitimate model (i.e., the model with the lowest loss in the unpoisoned validation set) and the malicious model (i.e., the model with the lowest loss in the poisoned validation set). 

\subsubsection{Full Substitution}
\label{subsub:full}

Table~\ref{tab.asr-wb} and Table~\ref{tab.asr-bb} show the results of MOSHI in both white-box and black-box, respectively.
Each table is composed of three sub-tables that represent the three distinct attack grid granularities, i.e., global, grouped-by learning rate, and grouped-by learning rate and number of neurons.
For each case, we report the results for the three case studies (i.e., MNIST, CIFAR10, and Speech Commands) spanning across the four studied hijack metrics (i.e., generalization error, latency, energy, and $\ell_0$). 
Furthermore, each row reports the number of sets.\footnote{Each set we trained an ad-hoc HVAE (see Section~\ref{ssec.grid}).}
Consider Table~\ref{tab.asr-bb} on the MNIST use-case and global granularity: since we have only 1 set, the attack is either successful (100\%) or unsuccessful (0\%).
\par
Overall, we denote strong positive results proving the validity of HVAE. %, this first approach we proposed to address an MSHAA. 
The attack is perfectly successful (i.e., $ASR = 100\%$) in $22/33$ and $23/33$ cases in both white-box and black-box scenarios.
Interestingly, we do not observe differences between white and black box settings, implying that the attack can be very dangerous, yielding high transferability capabilities. 
\new{
HVAE also proves particularly damaging to generalization: in every evaluated instance, the poisoned validation set successfully hijacked the model selection process, consistently leading to the selection of suboptimal models with inferior generalization performance.
This highlights the implications of the proposed attack MOSHI on real-world ML pipelines.
}
% \par
% The proposed attack appears to be extremely dangerous, especially for the generalization: in all the considered cases, our HVAE produced a poisoned validation set that hijacked the model selection, always returning a non-optimal that generalizes less. 
\par
Last, while our proposed attack HVAE shows promising results across different benchmarks, it suffers from the increased complexity of the data.
In particular, in the SpeechCommands, the attack fails to produce poisoned validation samples capable of hijacking the model selection.   
%This phenomenon might be due to HVAE's underlying architecture which is not optimal in learning useful representation. 
This phenomenon may be attributed to the underlying architecture of the HVAE, which is not well-suited for learning effective representations in sequential domains.
We believe that future studies might attempt to design ad hoc HVAE for specific tasks (e.g., RAVE for the speech domain~\cite{caillon2021rave}).
\par
We can conclude that our proposed attack is effective and can produce strong manipulations to the model selection phase. 
Furthermore, these results answer our original research question: \textit{can an attacker manipulate the model selection?}
The answer is yes, we can produce a MOSHI attack, and an attacker can ideally choose custom hijack metrics.
% \new{
% Our results confirm the effectiveness of the proposed attack, demonstrating its ability to manipulate the model selection phase significantly.
% This addresses our initial research question: \textit{Can an attacker manipulate model selection?} The answer is unequivocally yes—an attacker can customize hijack metrics through the MOSHI attack.
% }

\begin{table}[!htpb]
\centering
\footnotesize
% \small
\caption{ASR in White-Box settings. at different attack grid granularities.}
% \caption{ASR in White-Box settings.}
\begin{tabular}{c|c|c|c|c|c}  \toprule
\multicolumn{6}{c}{\textit{\textbf{Global}}} \\ \midrule
 & \textit{\# Sets} &\textit{Gener.} & \textit{Latency} & \textit{Energy} & \textit{$\ell_0$} \\ \midrule
MNIST & 1 & 100.0\% & 100.0\% & 100.0\% & 100.0\% \\
CIFAR10 & 1 &100.0\% & 0.0\% & 100.0\% & 100.0\% \\
SpeechC. & 1 & 100.0\% & 0.0\% & N/A & 0.0\% \\ \midrule
\multicolumn{6}{c}{\textit{\textbf{Learning Rate}}} \\ \midrule
MNIST & 6 &  100.0\% & 100.0\% & 66.7\% & 83.3\% \\
CIFAR10 & 2 & 100.0\% & 100.0\% & 100.0\% & 100.0\% \\
SpeechC. & 2 & 100.0\% & 50.0\% & N/A & 0.0\% \\\midrule
\multicolumn{6}{c}{\textit{\textbf{Learning Rate \& \# Neurons}}} \\ \midrule
MNIST & 18 & 100.0\% & 94.4\% & 100.0\% & 100.0\% \\
CIFAR10 & 6 & 100.0\% & 33.3\% & 50.0\% & 50.0\% \\
SpeechC. & 4 & 100.0\% & 50.0\% & N/A & 0.0\% \\ \bottomrule
\end{tabular}
\label{tab.asr-wb}
\end{table}

\begin{table}[!htpb]
\centering
\footnotesize
\caption{ASR in Black-Box Settings at different attack grid granularities.}
% \caption{ASR in Black-Box settings.}
\begin{tabular}{c|c|c|c|c|c}  \toprule
\multicolumn{6}{c}{\textit{\textbf{Global}}} \\ \midrule
 & \textit{\# Sets} & \textit{Gener.} & \textit{Latency} & \textit{Energy} & \textit{$\ell_0$} \\ \midrule
MNIST & 1 & 100.0\% & 100.0\% & 100.0\% & 100.0\% \\
CIFAR10 & 1   & 100.0\% & 100.0\% & 100.0\% & 100.0\% \\
SpeechC.  & 1 & 100.0\% & 0.0\% & N/A & 0.0\% \\ \midrule
\multicolumn{6}{c}{\textit{\textbf{Learning Rate}}} \\ \midrule
MNIST & 6 &  100.0\% & 100.0\% & 66.7\% & 83.3\% \\
CIFAR10 & 2 & 100.0\% & 50.0\% & 50.0\% & 100.0\% \\
SpeechC. & 2 & 100.0\% & 50.0\% & N/A & 0.0\% \\ \midrule
\multicolumn{6}{c}{\textit{\textbf{Learning Rate \& \# Neurons}}} \\ \midrule
MNIST & 18 & 100.0\% & 83.3\% & 100.0\% & 100.0\% \\
CIFAR10 & 6 & 83.3\% & 50.0\% & 33.3\% & 50.0\% \\
SpeechC. & 4 & 100.0\% & 50.0\% & N/A & 50.0\% \\ \bottomrule
\end{tabular}
\label{tab.asr-bb}
\end{table}

\subsubsection{Partial Substitution}
\label{subsub:partial}

\begin{figure*}[!htpb]
  \centering
  \begin{subfigure}{0.495\linewidth}
    \centering
    \includegraphics[width=\linewidth]{figures/pr-impact-lr.pdf}
    \caption{``\textit{Learning rate}'' attack grid setting.}
    \label{fig:pr-impact-lr}
  \end{subfigure}
  % \hspace{0.05\linewidth}
  \begin{subfigure}{0.495\linewidth}
    \centering
    \includegraphics[width=\linewidth]{figures/pr-impact.pdf}
    \caption{``\textit{Learning rate \& \#neurons}'' attack grid setting.}
    \label{fig:pr-impact}
  \end{subfigure}
  \caption{White-Box (WB) and Black-Box (BB) poisoning rate impact.}
  \label{fig:impact}
\end{figure*}

We now analyze the impact on the validation poisoning rate. 
% Figures~\ref{fig:pr-impact-lr} and~\ref{fig:pr-impact} show such a trend on the grid attack settings \textit{learning rate} and \textit{learning rate \& \#neurons}, respectively, where we report the three use cases, the four hijack metrics, and both knowledge levels. 
Figure~\ref{fig:impact} shows such a trend on the grid attack settings \textit{learning rate} and \textit{learning rate \& \#neurons}, respectively, where we report the three use cases, the four hijack metrics, and both knowledge levels.
By analyzing the ESF for the first two datasets considered, we can observe that the white-box settings help produce a more potent attack comparable to more traditional poisoning properties.
Despite that, we do not observe clear advantages in poisoning the validation set entirely. This result suggests that HVAE can effectively attack even by tampering with only a smaller portion of the validation set.
On the other hand, in the Speech Commands case study, we observe a more erratic behavior of the mean ESF.
This is particularly evident for the $\ell_0$ hijack metric, despite achieving an ASR greater than zero, as illustrated in Figure~\ref{fig:pr-impact-lr} and Figure~\ref{fig:pr-impact}, the mean ESF of these attacks is heavily below 0.
This discrepancy arises because, for some attacks, the ESF values are strongly negative.
We believe that the interplay between MOSHI effectiveness and the poisoning rate should be explored in future works. 

% \begin{figure}
%     \centering
%     \includegraphics[width=0.99\linewidth]{figures/pr_impact_lr.pdf}
%     \caption{Poisoning rate impact on the grid attack setting ``\textit{learning rate}'' for both White-Box (WB) and Black-Box (BB) knowledge level. }
%     \label{fig:pr-impact-lr}
% \end{figure}
 
% \begin{figure}
%     \centering
%     \includegraphics[width=0.99\linewidth]{figures/pr_impact.pdf}
%     \caption{Poisoning rate impact on the grid attack setting ``\textit{learning rate \& \#neurons}'' for both White-Box (WB) and Black-Box (BB) knowledge level. }
%     \label{fig:pr-impact}
% \end{figure}

% \begin{figure*}[!htpb]
%   \centering
%   \begin{subfigure}{0.45\linewidth}
%     \centering
%     \includegraphics[width=0.99\linewidth]{figures/pr_impact_lr.pdf}
%     \caption{``\textit{Learning rate}'' attack grid setting.}
%     \label{fig:pr-impact-lr}
%   \end{subfigure}
%   % \hspace{0.05\linewidth}
%   \begin{subfigure}{0.45\linewidth}
%     \centering
%     \includegraphics[width=0.99\linewidth]{figures/pr_impact.pdf}
%     \caption{``\textit{Learning rate \& \#neurons}'' attack grid setting.}
%     \label{fig:pr-impact}
%   \end{subfigure}
%   \caption{White-Box (WB) and Black-Box (BB) poisoning rate impact.}
%   \label{fig:impact}
% \end{figure*}


% \subsubsection{Impact on the Availability}
\subsubsection{\new{Attack Impact}}
\label{subsub:impact}
%\textbf{INTRO per Riccardo. Ricorda di dire che i risultati soon in 100\% val poisoned. Analizza brevemente anche see vedi difference tra white and black box settings.} 
In this section, we present the impact of our attack on the targeted metrics for the specific case study and models considered. 
In Table~\ref{tab.impact}, we illustrate how MOSHI, by influencing the Model Selection phase, impacts the chosen metrics.
This is done by reporting the ratio between the metrics of the model returned by an untampered Model Selection, and the model returned once the attack, conducted by training the HVAE with full knowledge of the victim models and with full (100\%) substitution of the original validation set with poison data. 
For instance, when considering the white-box knowledge, we observed a malicious MNIST model $\times3$ slower than the legitimate model in terms of latency introduced.
As we can see, the WB and BB settings perform similarly in both the MNIST and SpeechCommands case studies.
The former presents the same model for all the attacks conducted with the various hijack metrics, and the latter shows better performance only for the WB case on the Generalization Metric.
Finally, for the CIFAR10 case study, the BB setting performs slightly better, allowing for a small increase in the Latency metric.

\begin{table}[!htpb]
\centering
\footnotesize
\caption{Impact factor on the availability. The results report the impact factor. We only report the values for the \textit{global} attacker grid variant. }
\begin{tabular}{c|c|c|c|c}  \toprule
\multicolumn{5}{c}{\textit{\textbf{White-Box}}} \\ \midrule
 &\textit{Gener.} & \textit{Latency} & \textit{Energy} & \textit{$\ell_0$} \\ \midrule
MNIST  & 21.27 & 3.818 & 1.244 & 6.293 \\
CIFAR10 & 1.120 & 0.769 & 1.009 & 1.027 \\
SpeechC. & 4.136 & 0.797 & N/A & 0.629 \\ \midrule
\multicolumn{5}{c}{\textit{\textbf{Black-box}}} \\ \midrule
MNIST  & 21.27 & 3.818 & 1.244 & 6.293 \\
CIFAR10 & 1.180 & 1.001 & 1.009 & 1.027 \\
SpeechC. & 2.319 & 0.797 & N/A & 0.810 \\ \bottomrule
\end{tabular}
\label{tab.impact}
\end{table}

% The study of the impact on the availability of our attacks reconfirms the strong link between our attack and the models for which is performed the model selection, as having a more significant and diverse set of models allows us to have possibly higher metrics, which will enable us to train more effective HVAE and a more impactful attack.
\new{
Evaluating our attack's impact on availability reaffirms the strong correlation between the attack's effectiveness and the diversity of models involved in the selection process.
A more extensive and diverse set of models enhances the potential for achieving higher performance metrics, enabling the training of more effective HVAE models and, consequently, executing a more impactful attack.
}