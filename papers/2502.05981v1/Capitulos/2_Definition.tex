\section{Definition of the general method MeLoCoToN}
The core of this work is that every combinatorial problem has an explicit equation that returns its exact solution. In this section we will demonstrate how such an equation is obtained based on the use of tensor networks. This method, which we call \textit{Modulated Logical Combinatorial Tensor Networks} (MLCTN or MeLoCoToN), will consist of four steps:
\begin{enumerate}
    \item Definition of the problem variables and rewriting of the functions.
    \item Creation of the associated classical logical circuit.
    \item Creation of the associated logical tensor network.
    \item Iteration on the tensor network and contraction.
\end{enumerate}

The ideas presented can be sintetized in Fig.~\ref{fig: General Scheme}.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.7\linewidth]{Images/General_Scheme.pdf}
    \caption{General scheme of ideas presented.}
    \label{fig: General Scheme}
\end{figure}

There are three general types of combinatorial problems that we can define. The first are \textit{inversion problems}, which consist in having a function $\gamma$ which associates one output combination to each input combination, and given a known output $\vec{Y}$, we search for the input $\vec{X}$ that generates it, $\vec{Y}=\gamma(\vec{X})$. An example would be the factorization of prime numbers. The second are the \textit{constraint satisfaction problems}, which consist in obtaining a solution that satisfies a set of constraints. An example is the N queens. The third are the \textit{optimization problems}, which consist in having a function which associates a cost to each input, these are not transformed, and obtaining the input with lower cost. An example is the knapsack problem. Both types can be considered combinatorial problems for the purposes of what we will discuss later.

\subsection{Quantum computing explanation of the method}
Before explaining the details of the tensor method, let us explain the quantum motivation for it, for those more familiar with quantum computing. Our method will consist in the simultaneous evaluation of all possible solutions, so that, when measuring the system, it will return with maximum probability the basis state that encodes the optimal solution. In the cases of inversion and constraint satisfaction problems, when measuring the system, only the correct solution can be obtained, while in the cases of optimization problems it can be obtained with higher probability.

For inversion problems, we have two registers. The measurement register, with the qudits we will measure, and the post-selection register, with the qudits we will operate and post-select. 
The system starts with uniform superposition on the qudits from both registers, making a Bell state between each qudit in each register.

\begin{equation}
    \ket{\psi_0} = \bigotimes_{k=0}^{N-1} \left(\sum_{x_k=0}^{d_k-1} \ket{x_k}_{m,k}\ket{x_k}_{p,k}\right)=\sum_{\vec{x}}\left(\ket{\vec{x}}_{m}\ket{\vec{x}}_{p}\right),
\end{equation}
where $\ket{x_k}_{m,k}$ is the state of the $k$-th measurement qudit and $\ket{x_k}_{p,k}$ is the state of the $k$-th post-selection qudit.

After that, we apply an oracle $\mathcal{T}$ on the post-selection register, so that for each base state its state becomes the output associated to the input of that state through the function $\gamma$ to be inverted.
\begin{equation}
    \ket{\psi_1} = \sum_{\vec{x}}\left( \ket{\vec{x}}_{m}\mathcal{T}\ket{\vec{x}}_{p}\right)=\sum_{\vec{x}} \left( \ket{\vec{x}}_{m}\ket{\gamma(\vec{x})}_{p}\right).
\end{equation}
Now, we post-select the state of the qudits in the second register, so that they are only in the state of the known output $\vec{Y}$ of the function we want to invert. This is a non-unitary operation, so we cannot perform it in a quantum system. In this way, in the measurement register, the only state $\ket{\vec{X}}$ that will remain is the one generated by the output that we have post-selected.
\begin{equation}
    \ket{\psi_2} = \left(\mathbb{I}\otimes \ket{\vec{Y}}\bra{\vec{Y}}\right)  \sum_{\vec{x}} \left( \ket{\vec{x}}_{m}\ket{\gamma(\vec{x})}_{p}\right) = \sum_{\vec{x}} \left( \ket{\vec{x}}_{m}\ket{\vec{Y}}\braket{\vec{Y}|\gamma(\vec{x})}_{p}\right) = \ket{\vec{X}}_{m}\ket{\vec{Y}}_p.
\end{equation}
Now, if we measure the first register, we can only get the correct solution.

For optimization and constraint satisfaction problems we only use one register, which starts in uniform superposition
\begin{equation}
    \ket{\psi_0} = \bigotimes_{k=0}^{N-1} \left(\sum_{x_k=0}^{d_k-1} \ket{x_k}\right)=\sum_{\vec{x}}\ket{\vec{x}}.
\end{equation}
After that, we apply an operator that performs an imaginary time evolution~\cite{ITE}, having as Hamiltonian the cost function for each combination
\begin{equation}
    \ket{\psi_1} = \sum_{\vec{x}}e^{-\tau C(\vec{x})}\ket{\vec{x}}.
\end{equation}
Finally, we apply an $\mathcal{R}$ operator that applies the constraints of the problem, cancelling the amplitude of the states that do not satisfy them
\begin{equation}
    \ket{\psi_2} = \sum_{\vec{x}}\mathcal{R}e^{-\tau C(\vec{x})}\ket{\vec{x}} = \sum_{\vec{x}\in R}e^{-\tau C(\vec{x})}\ket{\vec{x}},
\end{equation}
being $R$ the subspace of combinations satisfying the constraints. Again, these are non-unitary operations not directly implementable in a quantum system. After this, the basis state with less associated cost is the most probable state to measure.


\subsection{Definition of the problem variables and rewriting of the functions}

The first step is choosing which variables are going to be optimized to solve the problem, and rewrite it according to these variables. Many problems may have different sets of variables such that solving the problem in one set returns the same solution as solving it in another ones. For example, in the traveling salesman problem we can choose as variables to optimize the vector $\vec{y}$ whose component $y_k$ indicates the time step in which we are at node $k$, or the vector $\vec{x}$ whose component $x_t$ indicates in which node we are at time step $t$. Both formulations are equivalent, but the former allows us to express the cost function, variable transformations and dependencies in the problem in a much simpler way.

Once the variables $\vec{x}$ to optimize have been chosen, we must rewrite the problem in function to these variables. In case of inversion problems, it will be necessary to determine which operations are performed on the input variables to obtain the output variables values. For example, if the problem is to determine two numbers of a set that added result in a certain value, it is possible to take as variables the bits of each number and do the binary addition process until obtaining the output number, also in binary variables. Something similar happens in constraint satisfaction problems, but this time we need to determine when a combination is unfeasible.

In the optimization problems the cost function must be written as a function that receives the values $\vec{x}$ and returns a number. This can be expressed simply as
\begin{equation}
    C(\vec{x}) = C_{x_0,x_1,\dots,x_{N-1}},
\end{equation}
being $C(\cdot)$ the cost function and $C$ its associated cost tensor.

However, to simplify the posterior implementation of the tensor network, the cost function should be expressed as the least-number of variables dependent cost operation. An example is the cost given as a QUBO
\begin{equation}\label{eq: QUBO}
    C(\vec{x}) = \sum_{i,j}Q_{i,j}x_i x_j.
\end{equation}

Another example is the Tensorial Quadratic Unconstrained D-ary Optimization (T-QUDO) formulation, which consists in expressing the cost as a sum of tensor elements of two indexes, these being the values of the variables,
\begin{equation}\label{eq: T QUDO cost}
    C(\vec{x}) = \sum_{k} C_{k,x_{a_k},x_{b_k}},
\end{equation}
where $a_k$ and $b_k$ are the identifiers of the first and second variable involved in the $k$-th term and $\vec{x}$ is a vector of natural values.

In the case of the traveling salesman problem~\cite{TSP_General}, if the variables $x_t$ are defined as the node where we are at time $t$, it can be expressed as
\begin{equation}
    C(\vec{x}) = \sum_{t} C_{x_t,x_{t+1}}.
\end{equation}

Defined the cost function, it is necessary to define the constraints. There are many ways to define the constraints, but a convenient one is to use auxiliary variables that indicate the activation or not of a certain condition. For example, in the traveling salesman problem the constraints are to end at the same node where we start and not to repeat any node. This is
\begin{equation}
\begin{gathered}
    x_0=x_{N},\\
    x_t \neq x_{t'}\ \forall t\neq t'.
\end{gathered}
\end{equation}
Another way of expressing it with auxiliary variables is as follows
\begin{itemize}
    \item $y_r = x_0 \Rightarrow x_N=y_r$ for ending at the start.
    \item $\forall i,t$ for each node $i$ and time step $t$.
    \begin{itemize}
        \item if $i\in \{x_0, x_1, \dots, x_t\} \Rightarrow y_{i,t} = 1$
        \item else $y_{i,t} = 0$
        \item if $y_{i,t} =1 \Rightarrow x_{t'} \neq i\ \forall t'>t$
        \item else $\exists t'>t\ |\ x_{t'}=i$
    \end{itemize}
\end{itemize}
where $y_{i,t}$ takes into account if the node $i$ has being visited in some step up to the step $t$.




\subsection{Creation of the associated classical logical circuit}
Once we have determined the variables that we will use to solve the problem, we have to build a classical 
logical circuit of the problem. The type of circuit to build depends on the type of problem to solve. These circuits make use of what we call \textit{internal signals}, internal information of the circuit, which is not part of the output, comes from some operators and conditions the action of others who receive it. The internal signal is the problem relevant information that is sent between operators. It is the only information they need to perform their operations correctly, and depends directly or indirectly on the problem input. In addition, we can interpret the inputs and outputs that connect to the outside as \textit{external signals}. For this reason, the construction method is called the \textit{Signals Method}. To understand how it works, we will start with the construction for inversion problems, then the CSP and finally the optimization problems.

\subsubsection{Inversion Problem}
For an inversion problem, we have to make a circuit that implements the function to invert, receiving the inputs $\vec{x}$ and returning the corresponding output $\vec{y} = \gamma(\vec{x})$. This can always be done, as it is a known function, making use of a classical logic circuit that transforms the information it receives. This can be implemented by means of fundamental logical operators or by means of more complex ones. We call this circuits \textit{Logical Signal Transformation Circuits} (LSTC), since each operator transforms its input signals into output signals using logical rules. These circuits also serve to solve the problem of calculating $\gamma(\vec{x})$, which we call \textit{forward problems}. Let us give a few examples to make this class of circuits easier to understand.

\paragraph{Sum of two numbers in binary}
$ $

The problem is, given a number $c$, to determine two numbers $a$ and $b$ such that $c=a+b$. That is, we want to invert the addition function. To do this, we use as variables the bits of the numbers. In this way, we will have to build the LSTC that performs the binary addition function.
\begin{figure}[h]
    \centering
    \includegraphics[width=0.7\linewidth]{Images/ADD.pdf}
    \caption{LSTC to add two numbers $a$ and $b$ to obtain a number $c$.}
    \label{fig: ADD circuit}
\end{figure}

The way to build this circuit is shown in Fig.~\ref{fig: ADD circuit}, where each pair of bits, one of $a$ and the other of $b$, enters in each logical operator $ADDb$, which are in charge of doing the part of the global sum corresponding to those bits. If each binary sum is performed in an $ADDb$ operator, we can make them return both the modular sum of the bits and the carry for the sum of the following bits. This carry information is sent in what we call the \textit{internal signal}.

The $ADDb$ operator has three inputs, which are the three bits to be added, and two outputs. The first output is the function $f(x,y,z) =x\oplus y \oplus z$, the modular sum of the 3 bits, marked in orange. The second output is the function $g(x,y,z)=\left\lfloor\frac{x+y+z}{2}\right\rfloor$, which outputs the carry, marked in green. The first output is part of the circuit output, but the second is internal information that is part of the input of the next $ADDb$ operator.
This circuit uses minimal amounts of information to communicate between its parts to obtain the final output, in addition to being small in size. These are two properties necessary for the resulting tensor network to be computable.


\paragraph{Multiply two numbers in binary}
$ $

In this problem we have a number $c$ and we want to obtain two numbers $a$ and $b$ such that $c=a\times b$. To do this, we have to make the LSTC that generates the multiplication of two numbers. We generate it based on conditional binary sums. For this, we have two different internal signals. The first one is the main signal, which keeps track of how much we have added up to a certain point, and that will be the circuit output at the end, and the second signal in each sum keeps track of the carry and the condition. That is, the main signal starts with the value $0$, and the circuit adds to it $b$ if $a_0$ is equal to $1$. That is, it adds $a_0b$.
\begin{figure}
    \centering
    \includegraphics[width=0.7\linewidth]{Images/cADD.pdf}
    \caption{LSTC to add two numbers $a$ and $b$ to obtain a number $c$ if $w=1$.}
    \label{fig: cADD circuit}
\end{figure}

The $ADD$ circuit is similar as before, but now each operator has an extra input, which indicates if the addition is done or not. The $cADD$ circuit is in Fig.~\ref{fig: cADD circuit}. Thus, after doing the first conditional addition, the result is the input of another $cADD$ operator, which has to add the value $b$ multiplied by $2$, but this time if $a_1=1$. With this, the main signal is
\begin{equation}
    r_1 = a_0b + 2a_1b.
\end{equation}
Repeating this step $N$ times gives the signal
\begin{equation}
    r_{N-1} = \sum_{n=0}^{N-1} 2^n a_n b = a\times b=c.
\end{equation}

\begin{figure}
    \centering
    \includegraphics[width=0.7\linewidth]{Images/DOT.pdf}
    \caption{LSTC that performs the multiplication of two numbers $a$ and $b$.}
    \label{fig: DOT circuit}
\end{figure}
As we can see in the circuit in Fig.~\ref{fig: DOT circuit}, there is an \textit{intermediate state} that transforms until finally resulting in the final output. The intermediate state is a class of signal which is internal through the computation, and becomes external at the end of it.

\subsubsection{Constraint Satisfaction Problem}
In these problems the objective is that the circuit receives an input and returns the same value as output only if the combination satisfies the constraints. That is, if the input does not meet the constraints, we will not return any output. To do this, in addition to a set of internal and external signals, we have a value associated with the combination. This number is the \textit{amplitude} of the combination, which we will understand better in the section of optimization problems. Up to this point, it is only an internal number starting at $1$, and in case at some point it is detected that the input violates the constraints, an operator will change it to $0$.

Our circuit is composed of a set of operators that send a set of internal signals to each other, each one being in charge of analyzing a specific part of the input and detecting if any constraint is violated. We call this circuit \textit{Logical Signal Verification Circuits} (LSVC).

\paragraph{Single One Input}
$ $

This problem consists in finding a string of binary numbers such that only one of them is equal to $1$, and the rest are $0$.
\begin{figure}[h]
    \centering
    \includegraphics[width=0.7\linewidth]{Images/One_bit_circuit.pdf}
    \caption{LSVC to determine if a number has only one bit on $1$.}
    \label{fig: One bit}
\end{figure}

To do this, the input to the circuit is the value of each bit, and each one enters in an $ONE$ operator, which determines whether its value is $1$ and how many $1$ have appeared up to that point. They have two inputs and two outputs. The first input is the value of its bit, and the second is the signal indicating how many $1$ have appeared in the combination so far. The first output is the value of its bit, and the second is the signal that tells how many $1$ have appeared in the combination so far, including the current bit. If the operator receives that $1$ has already appeared in the combination, then it will make the amplitude $0$ if the input signal of its bit is also equal to $1$, since there cannot be two $1$ in the combination. The last operator forces that, if it receives that no $1$ has appeared, the amplitude of the combination is $0$ if it does not receive a $1$ on its bit. The circuit is shown in Fig.~\ref{fig: One bit}.



\subsubsection{Optimization Problem}
For optimization problems we have to change the approach slightly. In these problems we are not looking for an output nor an input which only satisfies constraints, but rather each input has an associated cost that we can calculate, but generally do not need to know. We want the state with the lowest associated cost, which satisfies the restrictions. Therefore, we will create a logical circuit whose input and output are the same, but which has a number associated with its state. We can visualize it as an optical circuit that can receive waves at discrete frequencies with a certain amplitude. Then, the circuit, depending on the frequency of that wave, changes its amplitude. Thus, the output of the circuit is a wave of the same frequency, but with a different amplitude. For example, if the circuit receives the value $x$, the output will also be $x$, but the internal value of the state will be $f(x)$. We call this internal value \textit{amplitude}, in analogy to quantum computing. It is important to note that this circuit is NOT a quantum circuit, nor does it work on superposition. It is a classical circuit that, depending on what it receives, amplifies or reduces the amplitude of the internal state. We call these circuits \textit{Logical Signal Modulation Circuits} (LSMC), since each operator transforms the internal signals it receives to modulate the amplitudes of the inputs according to logical rules. 

Due to the properties of the tensors that we will explain later, the changes in amplitude can only be multiplicative. That is, each operator can only multiply the amplitude by a number. This may seem restrictive, but it is sufficient to tackle any problem. Due to the types of existing problems and this restriction, we choose that given an input $\vec{x}$, which is a solution combination, the circuit multiplies its amplitude (initially $1$) by $e^{-\tau C(\vec{x})}$, being $\tau$ a constant. In this way, a combination with higher cost has an associated amplitude exponentially smaller than one with lower cost. This process is called \textit{imaginary time evolution}. This also allows taking advantage of the exponential property
\begin{equation}
    \prod_{i} e^{a_i}= e^{\sum_i a_i}.
\end{equation}
In case of constrained optimization problems, the circuit will also implement the LSVC logics of the constraint satisfaction problems. We will understand it better with three examples.


\paragraph{Linear function}
$ $

This combinatorial optimization problem has a cost function
\begin{equation}
    C(\vec{x})=\sum_{i=0}^{N-1} a_i x_i
\end{equation}
for a set of $a_i\in \mathbb{R}$ values, where $\vec{x}$ is a vector of binary values. The exponential of the cost function can be expressed as
\begin{equation}
    e^{-\tau C(\vec{x})} = \prod_{i=0}^{N-1} e^{-\tau a_i x_i}.
\end{equation}

\begin{figure}
    \centering
    \includegraphics[width=0.7\linewidth]{Images/Linear.pdf}
    \caption{LSMC that multiplies the amplitude of an input $\vec{x}$ by $e^{-\tau \sum_{i=0}^{N-1} a_i x_i}$.}
    \label{fig: Linear circuit}
\end{figure}

As each product only depends on one variable, its LSMC can be expressed as in Fig.~\ref{fig: Linear circuit}. If we multiply the amplitude of each part of the input by a value, the amplitude of the global input is multiplied by the product of all these values. This property allows the circuit to make the amplitude contain information from all the input without the need to transmit all the information at the same time. In this case we have not needed signals between operators as in the case of the addition, but we are going to see a more complicated case.


\paragraph{Quadratic function with a single neighbor in a linear chain}
$ $

This combinatorial optimization problem has a cost function
\begin{equation}
    C(\vec{x})=\sum_{i=0}^{N-1} (Q_{i,i} x_i^2 + Q_{i,i+1} x_ix_{i+1}).
\end{equation}
As before, the exponential can be expressed as products of exponentials. In this case, each operator needs, in addition to the information of the variable that corresponds to it, the value of the previous variable in the chain. The signal that each operator gives to the next one is its variable state.
\begin{figure}
    \centering
    \includegraphics[width=0.7\linewidth]{Images/Quadratic_linear_one_neighbor.pdf}
    \caption{LSMC that multiplies the amplitude of an input $\vec{x}$ by $e^{-\tau \sum_{i=0}^{N-1} (Q_{i,i} x_i^2 + Q_{i,i+1}x_ix_{i+1})}$.}
    \label{fig: Quadratic one neighbor circuit}
\end{figure}

The LSMC is given by Fig.~\ref{fig: Quadratic one neighbor circuit}. As noted above, this circuit is simple, where each functional part depends on few elements, and has a small size.


\paragraph{Natural sum total function problem}
$ $

The combinatorial optimization problem has a cost function
\begin{equation}
    C(\vec{x})=f\left(\sum_{i=0}^{N-1} a_i x_i\right),
\end{equation}
where $a_i\in \mathbb{N}$ and $f(\cdot)$ is some known function. In this case, we can use as a signal the sum $\sum_{i=0}^{m-1} a_i x_i$ up to the $m$-th variable, so that the last operator only does the evolution on the application of $f(\cdot)$ on the signal. This makes the last one operator the only one that multiplies the amplitude. Each previous operator only adds to the signal a value $a_i$ multiplied by its input. In this way, the LSMC is given by Fig.~\ref{fig: Natural sum circuit}.

\begin{figure}
    \centering
    \includegraphics[width=0.7\linewidth]{Images/Natural_sum.pdf}
    \caption{LSMC that multiplies the amplitude of an input $\vec{x}$ by $e^{-\tau f\left(\sum_{i=0}^{N-1} a_i x_i\right)}$.}
    \label{fig: Natural sum circuit}
\end{figure}


\subsection{Creation of the associated logical tensor network}
After understanding the three types of circuits, we have to build the tensor network associated with these circuits. The key of this tensor network is that, given its structure, it is possible to take all possible inputs at once, and return all possible corresponding outputs, with their associated amplitudes. This is because, when tensorizing, the inputs and outputs of the operators become the basis states of tensors. This allows to apply a superposition of all possible inputs, generating their corresponding outputs, and thus propagating the signals through the circuit by means of their entanglement.

Since we have all the possible outputs, in the inversion problems we only have to force the output to be the one we know. That is, having the circuit, we  put as `inverse input' the desired output, making the input of the circuit only the one which generates that output. This will result in a tensor in which the only non-zero elements are in the input values we are looking for. In cases of constraint satisfaction, we have a tensor network that represents a diagonal tensor. The only non-zero elements of this tensor will be those that satisfy the imposed constraints. In the case of optimization problems we have a similar phenomenon. As we have all the possible inputs with their amplitudes associated to their costs, we have a diagonal tensor where each element has the amplitude of that combination, so we only have to look for the one with the highest amplitude.

In both cases, the only thing we have to do is to change each operator of the circuit by a tensor with as many indexes as inputs and outputs the operator has. The values of the output indexes for the non-zero elements depend on the values of the input indexes, following the equations of the outputs of the associated operator. The values of the non-zero elements are the amplitudes of the corresponding operators, which in the case of inversion and contraint satisfaction are always 1. The inputs to the circuit are converted for each of the variables into a vector of ones, which will express the uniform superposition of all possible values of that variable. All tensors are connected to each other in exactly the same way as in the associated circuit, by the same indexes in the same way. We call this process \textit{Circuit Tensorization} (CT), and the resulting tensor network is the \textit{Tensor Logical Circuit} (TLC).

Translated into equations, it means that if we have an operator $U$ with 3 inputs $x,y,z$ and 2 outputs $\mu, \nu$, calculated as $\mu=f(x,y,z),\ \nu=g(x,y,z)$, which multiplies the amplitude of the state by $h(x,y,z)$, then its associated tensor $U$ has as non-zero elements those that satisfy
\begin{equation}
    \begin{gathered}
        \mu=f(x,y,z),\ \nu=g(x,y,z),\\
        U_{x,y,z,\mu,\nu}= h(x,y,z).
    \end{gathered}
\end{equation}
We call this process \textit{Input-Output Indexing} (IOI). As we can see, the relation between the inputs and outputs of each operator implies a constraint on the tensor representing it.

It is important to note that at this point, after creating the TLC of the problem, we are not yet going to contract it. This is because if we were to contract it at this point, we would have a tensor that collects the amplitude for all possible combinations, which is not our objective. The tensor network to be contracted is the one mentioned in the next subsection. We now present a few examples of CT.


\subsubsection{Inversion Problems}
In this case, each tensor transforms the position of the elements of the input $\vec{x}$ to the positions of the elements of the output $\vec{y}$, but not its amplitude. That is, we apply a function on the indexes of the tensor, and not on the values of the elements themselves. There is a function $\gamma$, which we do not need to know explicitly, represented through the LSTC, which receives the input and returns the output. That is, $\vec{y}=\gamma(\vec{x})$. Thus, the tensor network that replaces the circuit is such that, when contracted, it results in a $T$ tensor of elements $T_{x_0,x_1,x_2,\dots, y_0, y_1, y_2, \dots}=1$ only if $\vec{y}=\gamma(\vec{x})$, for all possible values of $\vec{x}$, and otherwise equals $0$. 

In order to force the output to be the correct one $\vec{Y}$ to get the correct input $\vec{X}$, we have to project the tensor on the subspace on that the last indexes of it are $\vec{y}=\vec{Y}$. This is equivalent to performing the contraction operation
\begin{equation}
    T'_{x_0,x_1,x_2,\dots} = \sum_{\vec{y}} T_{x_0,x_1,x_2,\dots, y_0, y_1, y_2, \dots}\delta^{Y_0}_{y_0}\delta^{Y_1}_{y_1}\delta^{Y_2}_{y_2}\dots
\end{equation}
being $\delta^{b}$ a vector of all zero elements except one equals to $1$ at position $b$. To do this, we put in each $k$-th output line a $\delta^{Y_k}$ vector. This causes the only non-zero element of the $T'$ tensor to be $T'_{X_0,X_1,X_2,\dots}=1$, for all the inputs $\vec{X}$ that generate the output $\vec{Y}$.



\paragraph{Sum of two numbers in binary}
$ $

In this case, each operator $ADDb$ must be replaced by its corresponding tensor. The TLC, and the names of the indexes are shown in Fig.~\ref{fig: TN ADD}. Note that the first tensor is different from the others, since it has no index to tell the previous carried, since it is always 0. In this case, the initial $ADDb0$ tensor is a 4-index tensor of $2\times 2\times 2\times 2$ dimensions with non-zero elements $ADDb0_{ij\mu\nu}=1$ when the following is true
\begin{equation}
\begin{gathered}
    \mu = f(i,j,0) =i\oplus j,\\
    \nu = g(i,j,0) = \left\lfloor\frac{i+j}{2}\right\rfloor.
\end{gathered}
\end{equation}

For the rest of the tensors $ADDb$, these have 5 indexes of $2\times 2\times 2\times 2 \times 2$ dimensions, whose non-zero elements $ADDb_{ijk\mu\nu}=1$ are those that satisfy
\begin{equation}
\begin{gathered}
    \mu = f(i,j,k) =i\oplus j \oplus k,\\
    \nu = g(i,j,k) = \left\lfloor\frac{i+j+k}{2}\right\rfloor.
\end{gathered}
\end{equation}

Finally, the vectors $c_b$ are those with their non-zero element at position $c_b$. 
\begin{figure}
    \centering
    \includegraphics[width=0.7\linewidth]{Images/ADD_TN.pdf}
    \caption{TLC of Fig.~\ref{fig: ADD circuit} and its index correspondence for the tensors.}
    \label{fig: TN ADD}
\end{figure}

If we contract this tensor network, the resulting tensor will have its nonzero elements in the positions in which its indexes are those corresponding to the bits of the $a$ and $b$ that generate as a result the $c$ that we have imposed.

\subsubsection{Constraint Satisfaction Problem}
In this case, the tensor network represents a diagonal tensor, in which all non-zero elements are equal to 1 when its indexes indicate a solution compatible with the constraints. Since the tensor is diagonal, we can eliminate half of the indexes, which are going to be a repetition of the other half, so we add a set of ones vectors in each output. This tensors are called the \textit{Plus Vectors} or `+' tensors. Thus, the tensor represented by the tensor network is
\begin{equation}
    T_{x_0,x_1,x_2,\dots} = 1\ \forall \vec{x} \in R.
\end{equation}

\paragraph{Single One Input}
$ $

In this case, each $ONE$ operator is replaced by the corresponding $ONE$ tensor to obtain the tensor network in Fig.~\ref{fig: One bit TN}. The non-zero elements of these tensors are
\begin{equation}
\begin{gathered}
    \mu = \nu =i,\\
    ONE(0)_{i\mu\nu}=1,
\end{gathered}
\end{equation}
\begin{equation}
\begin{gathered}
    \text{if } j=0\Rightarrow \nu = i,\quad\text{ else } \nu=j,\ i=0,\\
    \mu =i,\\
    ONE(k)_{ij\mu\nu}=1,
\end{gathered}
\end{equation}
\begin{equation}
\begin{gathered}
    i = j\oplus 1,\quad\mu =i,\\
    ONE(N-1)_{ij\mu}=1.
\end{gathered}
\end{equation}


\begin{figure}
    \centering
    \includegraphics[width=0.7\linewidth]{Images/One_bit_TN.pdf}
    \caption{TLC of Fig.~\ref{fig: One bit} for the Single One Input problem.}
    \label{fig: One bit TN}
\end{figure}




\subsubsection{Optimization problem}
As before, in optimization problems we do not want to impose a specific output. Our first objective is to create a tensor whose non-zero elements are those in which the input equals the output and whose values are the exponentials of their associated costs. That is, to perform an imaginary time evolution for a diagonal operator. After that, as the input and output information is the same, resulting redundant, we add a set of Plus Vectors in each output. This is equivalent to allowing any output, so nothing change in the problem. Each tensor that replaces an operator has to perform the multiplication of the amplitude that the operator performed. To do this, the element of the tensor associated to the corresponding input and output indexes has to have the value by which the operator multiplied the amplitude. As the contractions multiply the values of the tensor elements, this do the process we imposed on the circuit.

By doing this, the tensor $T$ represented by this tensor network has elements
\begin{equation}
    T_{x_0,x_1,x_2,\dots} = e^{-\tau C(\vec{x})}.
\end{equation}
In case of having constraints in the problem, we only have to add after the optimization circuit, a constraint circuit, so that the tensor represented will be
\begin{equation}
    T_{x_0,x_1,x_2,\dots} = e^{-\tau C(\vec{x})} \ \forall \vec{x} \in R.
\end{equation}

\paragraph{Linear problem}
$ $

In this case, we only have matrices of dimension $2\times 2$, represented in Fig.~\ref{fig: Linear TN}, whose non-zero elements are
\begin{equation}
\begin{gathered}
    \mu=i,\\
    EXPi(a,n)_{i,\mu} = e^{-\tau a_n i}.
\end{gathered}
\end{equation}

\begin{figure}
    \centering
    \includegraphics[width=0.7\linewidth]{Images/Linear_TN.pdf}
    \caption{TLC of Fig.~\ref{fig: Linear circuit} and its index correspondence for the tensors.}
    \label{fig: Linear TN}
\end{figure}


\paragraph{Quadratic function with a single neighbor in a linear chain}
$ $

Unlike the previous case, now we have tensors of 3 and 4 indexes, divided into 3 groups: the initial tensor, the intermediate tensor and the final tensor. The tensor network is represented in Fig.~\ref{fig: Quadratic TN}. All have dimension $2$ in their indexes, with non-zero elements
\begin{equation}
    \begin{gathered}
        \mu=\nu=i,\\
        EXPi(Q,0)_{i\mu\nu}=e^{-\tau Q_{00}i},\\
        EXPi(Q,n)_{ij\mu\nu}=e^{-\tau (Q_{n,n}i^2+Q_{n-1,n}ij)},
    \end{gathered}
\end{equation}
\begin{equation}
    \begin{gathered}
        \mu=i,\\
        EXPi(Q,N-1)_{ij\mu}=e^{-\tau (Q_{N-1,N-1}i^2+Q_{N-2,N-1}ij)}.
    \end{gathered}
\end{equation}
\begin{figure}
    \centering
    \includegraphics[width=0.7\linewidth]{Images/Quadratic_linear_one_neighbor_TN.pdf}
    \caption{TLC of Fig.~\ref{fig: Quadratic one neighbor circuit} and its index correspondence for the tensors.}
    \label{fig: Quadratic TN}
\end{figure}


\paragraph{Natural sum total function problem}
$ $

This case is similar to the previous one, but now the tensors have for their upper and lower indexes a dimension that allows sending any of the possible partial sums. Therefore, the $n$-th tensor has for its upper index a dimension of $\sum_{i=0}^{n-1}a_i$, while for the lower one a dimension of $\sum_{i=0}^{n}a_i$, and for the side ones a dimension of $2$. In the tensor network expressed in Fig.~\ref{fig: Natural sum TN} we have 3 types of tensors, whose non-zero elements are
\begin{equation}
    \begin{gathered}
        \mu=i,\quad \nu=a_0 i,\\
        EXPi(Q,0)_{i\mu\nu}=1,
    \end{gathered}
\end{equation}
\begin{equation}
    \begin{gathered}
        \mu=i,\quad \nu=j+a_n i,\\
        EXPi(Q,n)_{ij\mu\nu}=1,
    \end{gathered}
\end{equation}
\begin{equation}
    \begin{gathered}
        \mu=i,\\
        EXPi(Q,N-1)_{ij\mu}=e^{-\tau f(j+a_{N-1}i)}.
    \end{gathered}
\end{equation}

\begin{figure}
    \centering
    \includegraphics[width=0.7\linewidth]{Images/Natural_sum_TN.pdf}
    \caption{TLC of Fig.~\ref{fig: Natural sum circuit} and its index correspondence for the tensors.}
    \label{fig: Natural sum TN}
\end{figure}

\subsection{Iteration and contraction of the tensor network}
Once we have the tensor network that gives us the result tensor we search, we need to extract the relevant information from it without having to store it in memory. That is, we want to somehow be able to look at a reduced version of it that gives us the information we really need to get the solution. To do this, we are going to determine the correct value of each variable iteratively. Therefore, in each iteration we only want to know what is the value of the $n$-th variable in the optimal combination. To do this we  perform an integral over all the other variables, so that we only have to find the maximum in a vector of exponentially smaller dimension, and that information is included in the next iteration. We call this process \textit{Half Partial Trace}.

This can be visualized as a process similar to that performed when measuring a quantum state. We measure the qubits in order, so that the state of all the qubits that remain to be measured conditions the probability of obtaining a result on the one we are measuring, and the result of the already measured fix and alter the following probabilities in a fixed way. In our case, we do not use the amplitudes in the same way as in quantum mechanics, but the amplitudes are our `probabilities'. Instead of using the density matrix $\rho_{AB}$ of the $|\psi_{AB}\rangle$ state and performing an operation $P_{A,i} = Tr_{B}(\rho_{AB} M_i)$, we apply directly $P_{A,i} = \langle i,+^{\otimes N-1}|\psi_{AB}\rangle$. In~\cite{Escanez_Notation} simplified notation, $P_{A,i} = {}^{0}_{i}\langle +|\psi_{AB}\rangle$.

With this in mind, we will see it more clearly first in the inversion case and then in the optimization case.

\subsubsection{Inversion problem}
For simplicity, we begin by addressing the case in which there is only one solution to the problem. In this case, there is only one input $\vec{X}$ that results in that output $\vec{Y}$. This implies that there is only one non-zero element in the tensor represented by the TLC we have constructed, whose indexes give us the solution. Therefore, we can create a vector of dimension equal to that of the first index of the tensor, that is, of the number of possible values of the first variable. In the $k$-th component of this vector we store the sum of all the elements of the tensor whose first index has the value $k$. Although this operation seems computationally very expensive, it can be performed by putting a Plus Vector in all the indexes except the first one. Since there is only one correct solution, there is only one non-zero element in the tensor, so only one of these sums has a non-zero summand. This makes the position of the non-zero element of the vector match the value of the first index for the non-zero element of the global tensor.

Therefore, to determine the first variable we create the TLC and we connect Plus Vectors in all the indexes except the first one, and the value of the correct variable is the position with a 1 in the vector. To determine the second variable we can do exactly the same, but this time putting Plus Vectors in all the indexes except the second one. We can repeat this process for each variable and we will obtain the solution to the problem.

To generalize to problems in which there are several solutions, we have to take into account that the tensor may have more than one non-zero value. However, our reasoning is the same. In case we want one of the solutions, we only have to take in each iteration as the correct value of the variable the position of any of the non-zero components of the vector, since it indicates that there is a solution that has that value for that variable. One possibility is to choose the position with the largest element. However, since there are several solutions, performing the process exactly the same can lead to mixing several degenerated solutions, resulting in an incorrect result. To avoid this problem, we only need to introduce in each iteration the information of all previous results. That is, in iteration $n$ we have already determined the previous $n-1$ variables, so we can project the tensor to the solutions with the first $n-1$ indexes equal to the values already determined. We can do this just as in the projection onto the output, with vectors having only one non-zero element at the position corresponding to the value we want to impose. We call this \textit{Projection Vectors}. In this way we do not mix solutions. An example of the operation of this method is shown in Fig.~\ref{fig: Inversion iteration}.

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{Images/Inversion_TN_Iteration.pdf}
    \caption{Iterative method for the determination of the solution variables in an inversion problem, for a chain-type tensor network.}
    \label{fig: Inversion iteration}
\end{figure}

In case the variables are binary, instead of evaluating a vector we can evaluate a scalar. If we have only one solution, there are only two possible vectors for a variable: $(1,0)$ if the value has to be $0$ or $(0,1)$ if the value has to be $1$. Therefore, we can connect to the variable we are going to measure a vector $(-1,1)$, which will subtract to the amplitude associated to the value $1$ the amplitude associated to the value $0$. We call this vector \textit{Minus Vector}. If the correct value is $1$, the resulting scalar will be $1$, while if it is $0$, the scalar will be $-1$. In case of having several solutions, we choose $1$ if the scalar is positive or $0$ and $0$ if it is negative.

In the case of not being binary variables, we can perform the same process if we binarize the variable to be determined at the beginning of the circuit. That is, if we do a splitting of the free index so that we have $\log_2(d)$ binary indexes, we can determine the correct value of each of them as if they were different variables. We can also do the equivalent by placing a vector that performs the corresponding additions and subtractions between the groups.
Therefore, the resolution can always be expressed as an equation of the type
\begin{equation}
    x_i = H(\Omega_i),
\end{equation}
being $\Omega_i$ the scalar resulting from contracting the tensor network of the $i$-th variable, and $H$ the Heaviside step function. This way, if $\Omega_i<0$, $x_i=0$, else $x_i=1$. As the value to be introduced in the next iteration of the tensor network will be dependent on the previous one, through the projection tensor to the obtained result, the equation that determines the solution of the combinatorial problem (both the inversion problem and any other) is given by a nesting of Heaviside step functions within tensor networks. The constraint satisfaction problems follow the same mechanism.


\subsubsection{Optimization problems}
For the optimization cases the process is the same as described above, but with a completely different motivation. To begin with, we have to visualize the amplitude map for all possible combinations. Since we have chosen as the amplitude for each combination the negative exponential of its cost multiplied by a constant, the combination with the lowest cost will have the largest amplitude. If we increase the value of $\tau$, the amplitudes of the suboptimal combinations will decrease exponentially faster than the amplitude of the optimal combination. Thus, in the limit $\tau\rightarrow\infty$, if we renormalize the tensor by dividing each element by the sum of all the elements of the tensor, only the amplitude of the optimal combination $\vec{X}$ will remain, exactly as in the inversion case. The limit is
\begin{equation}
    \lim_{\tau\rightarrow\infty} \sum_{\vec{x}\in R}\frac{e^{-\tau C(\vec{x})}}{\sum_{\vec{x}\in R}e^{-\tau C(\vec{x})}}\ket{\vec{x}}=
    \lim_{\tau\rightarrow\infty} \sum_{\vec{x}\in R}\frac{e^{-\tau C(\vec{x})}}{e^{-\tau C(\vec{X})}}\ket{\vec{x}}=
    \lim_{\tau\rightarrow\infty}\sum_{\vec{x}\in R}e^{-\tau (C(\vec{x})-C(\vec{X}))}\ket{\vec{x}} = \ket{\vec{X}}.
\end{equation}
In case of having degeneracy, we will have the case of several solutions and we solve it as presented for inversion problems. The method is presented in Fig.~\ref{fig: Diagonal iteration}.
\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{Images/Diagonal_TN_Iteration.pdf}
    \caption{Iterative method for the determination of the solution variables in an constraint satisfaction and optimization problem, for a chain-type tensor network.}
    \label{fig: Diagonal iteration}
\end{figure}

However, it is not necessary to go to the infinite limit to extract information. For a sufficiently large finite value of $\tau$, the peak amplitude in the optimal combination will be large enough so that, when summing over the other variables values to obtain the vector of amplitudes of the variable we want to determine, this amplitude will be greater than the sum of all the suboptimals. This can be seen analogously to the case of measuring a quantum system in which there is a basis state with a probability much higher than the other basis states. Since the probability of measuring that state is higher, when measuring the first qubit of the system it will be more likely to measure the first bit of the state of maximum probability, and so on with all the qubits. Therefore, with a sufficiently large value of $\tau$, this same procedure is valid if we choose in each iteration as correct the position with the largest element. However, in case of not taking a sufficiently large $\tau$ value, the suboptimal states for an incorrect value of the variable to be determined may have a sum of amplitudes greater than that of the correct value.

To dampen this error, we can replace the sum by a complex sum, changing the Plus Vectors to vectors of complex numbers of unit modulus. We call them the \textit{Phase Vectors}. In this way, the `noise' generated by the amplitudes of the suboptimal ones will not sum in the same direction. Since the amplitude of the optimal combination is always the highest, it is the dominant value in their sum. If we distribute the phases of the numbers evenly, we decrease the probability that all the suboptimal ones are in the opposite phase to the optimum, so they will cancel each other. We call this method \textit{Humbucker}, and it was presented for the first time in~\cite{QUBO_Tridiagonal}.

\subsection{General equation}
In the previous subsections we have presented the method and demonstration of how to create the tensor network that solves the problem, and extracting the solution from it. Now, we can formulate the exact equation that solves any combinatorial problem.

\begin{theorem}
    Given a combinatorial problem, be it an inversion problem, a constraint satisfaction problem or an optimization problem, there is an exact explicit equation for its solution (or solutions).
\end{theorem}

\begin{theorem}
    Given a combinatorial problem, be it an inversion problem, a constraint satisfaction problem or an optimization problem, the exact explicit equation that solves it can be obtained in a polynomial time with respect to the time needed to formulate it.
\end{theorem}

\begin{theorem}
    Due to the symmetries of tensor networks, there are infinitely many equations that solve a combinatorial problem, all equivalent to each other.
\end{theorem}

This equation is obtained by following the steps listed in this section. The steps are
\begin{enumerate}
    \item Choose a set of variables $\vec{x}$ for the problem, which will encode the solution, input and/or output. For simplicity, and without loss of generality, the variables are considered to be binary, although it is general for natural variables.

    \item Build the logical circuit corresponding to the problem. If it is an inversion problem, the LSTC, if it is a constraint satisfaction problem, the LSVC, and if it is an optimization problem, the LSMC. 

    \item Tensorize the logical circuit to obtain the TLC of the problem.

    \item Perform the half partial trace of the TLC, adding the corresponding Minus Vector for the first variable, which we will call $x_0$. This tensor network is equal to the scalar value ${\color{blue} \Omega_0}$. Thus, the correct value of the first variable is
    \begin{equation}\label{eq: first variable}
        x_0={\color{blue} H(\Omega_0)}.
    \end{equation}

    \item To determine the next variable, the half partial trace of the TLC is made by adding now the corresponding Minus Vector for the second variable. This tensor network is a function of the value already determined for $x_0$, since the input index for that variable now has a projection vector at that value. Therefore, the tensor network has a $\delta^{x_0}$ tensor of $\delta^{x_0}_{i}$ components, so all components are zero except for the $x_0$-th component. If we substitute the expression~\ref{eq: first variable}, the tensor is $\delta^{{\color{blue} H(\Omega_0)}}$. The traced TLC in this case is equal to the value ${\color{teal}\Omega_1}$, which, depending on the value of $x_0$, can be expressed as ${\color{teal}\Omega_1(}x_0{\color{teal} )}$, which by substituting the expression~\ref{eq: first variable} becomes ${\color{teal}\Omega_1(}{\color{blue} H(\Omega_0)}{\color{teal} )}$. Thus, the value of the second variable is
    \begin{equation}
        x_1 = {\color{teal} H(\Omega_1(}{\color{blue} H(\Omega_0)}{\color{teal} ))}.
    \end{equation}
    
    \item The third TLC depends on the value of the two previous ones for the same reason as in the previous step, so it is a function
    \begin{equation}
    \Omega_2(x_0,x_1)=\Omega_2({\color{blue} H(\Omega_0)},{\color{teal} H(\Omega_1(}{\color{blue} H(\Omega_0)}{\color{teal} ))}).
    \end{equation}
    This way,
    \begin{equation}
        x_2 = H(\Omega_2({\color{blue} H(\Omega_0)},{\color{teal} H(\Omega_1(}{\color{blue} H(\Omega_0)}{\color{teal} ))})).
    \end{equation}

    \item The correct value of the $n$-th variable is
    \begin{equation}
        x_n = {\color{red}H( \Omega_n(}{\color{blue} H(\Omega_0)},{\color{teal} H(\Omega_1(}{\color{blue} H(\Omega_0)}{\color{teal} ))}{\color{red},\dots,}{\color{violet} H(\Omega_{n-1}}({\color{blue} H(\Omega_0)},{\color{teal} H(\Omega_1(}{\color{blue} H(\Omega_0)}{\color{teal} ))}{\color{violet},\dots ))}{\color{red} ))}.
    \end{equation}
    Since each variable ultimately depends on the initial tensor network, we can say that its value is actually given by a function $\Xi_n$, so that $x_n=\Xi_n({\color{blue}\Omega_0})$.
    \item The solution of the problem always can be expressed as
    \begin{equation}
        \vec{x}=(\Xi_0({\color{blue}\Omega_0}), \Xi_1({\color{blue}\Omega_0}), \Xi_2({\color{blue}\Omega_0}),\dots, \Xi_{N-1}({\color{blue}\Omega_0})).
    \end{equation}
\end{enumerate}

Since the construction of the tensor network is as fast as the construction of the logic circuit, and this can be done in a polynomial time with respect to the formulation of the problem, the equation can be obtained in a polynomial time with respect to the formulation.

Moreover, since between two tensors one can always place an $A$ matrix and its inverse and have the same tensor represented, and the number of possible invertible $A$ matrices is infinite, then there are infinitely many possible TLC, and therefore infinitely many equations for each problem.


Since we already understand how the general method works to obtain the tensor network, and therefore, the equation that solves any combinatorial problem, either inversion, constraint satisfaction or optimization one, we will present a wide range of examples of the application of this method to problems. Due to the large number of examples, many of them with shared concepts, we will explain their logics in a more or less superficial way, providing the form of the tensors and tensor networks to be constructed with examples. In this way, we can be sure to explain the cases and their generalizations without overextending ourselves. In case it generates a lot of interest, concrete cases can be dealt with in greater depth in future versions. It is important to note that most of these methods have not been thoroughly investigated in an attempt to obtain either the simplest formulation or the most efficiently computable tensor network.