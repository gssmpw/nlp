\section{Constraint Satisfaction Problems}
These problems consist in finding a combination that satisfies a set of constraints. They are a large family of problems, with great application and interest.

\subsection{K-colouring}
Given a graph $G$ with $V$ vertices and an adjacency matrix $E_{ij}$, the problem is to find a coloring pattern with $k$ colors of the vertices of the graph such that two vertices connected by an edge cannot have the same color. This problem has been solved with quantum algorithms~\cite{Colouring_Quantum}. This problem is a Constraint Satisfaction Problem.
\begin{figure}[h]
    \centering
    \includegraphics[width=0.9\linewidth]{Images/k_colouring_TN.pdf}
    \caption{a) k-colouring graph with $k=3$ and 10 vertices, b) Tensor Network for the problem to determine the colour of the first vertex.}
    \label{fig: kcolouring}
\end{figure}

To solve this problem we have to make a tensor network with the same shape of the graph, so that each tensor substitutes a vertex and each index of it is an edge of the graph. Each tensor sends to those on its right the value of its color, which it sends through its upper index, and receives the value of those on its left. The tensor only has a non-zero value when the input values are all different from its output value. That is, the non-zero elements of each tensor are
\begin{equation}
    \begin{gathered}
        j_0 = j_1 = \dots= j,\quad i_n\neq j\ \forall n,\\
        C_{j,i_0,i_1,\dots, j_0,j_1, \dots} = 1
    \end{gathered}
\end{equation}

However, each tensor has as many indexes as the vertex has edges, which is not desirable, since there is an excess of information processed at the same time. For this, we obtain its tensor train for every vertex tensor.
\begin{figure}[h]
    \centering
    \includegraphics[width=0.9\linewidth]{Images/k_colouring_simple_TN.pdf}
    \caption{Simplified k-colouring tensor network.}
    \label{fig: kcolouring simple}
\end{figure}

To do this, we continue with the $C$ tensors defined previously, but this time they only have at most 4 indexes. The first is that of the color of the vertex itself, the second is one of input from a previous vertex, and the other two are for the other two types of tensors in the chain. The first new tensor type is the $\delta$, which is a Kronecker delta that only transmits the color signal to the following vertices. The second is the $U$ tensor, which receives through its top index $j_0$ the color of the same vertex from $C$, through its other index $j_1$ transmits the signal to the next $U$ tensor in the chain and from its left index $i$ receives the color of the previous vertex. The new tensor network is shown in Fig.~\ref{fig: kcolouring simple}. The $U$ non-zero elements are
\begin{equation}
    \begin{gathered}
        j_1 = j_0,\quad i\neq j_0,\\
        U_{i,j_0,j_1} = 1
    \end{gathered}
\end{equation}
If we want to know how many possible colorings exist for that graph with that number of colors, we only have to put a Plus Vector in all the indexes of the variables, so that all the amplitudes are added up. The resulting number is precisely the number of possible colorings.

If now the problem to be solved is a k-colouring in which we want to paint the graph with the minimum number of colors possible, we move on to an optimization problem. In this case, we only have to make the cost be of the type
\begin{equation}
    C(\vec{x}) = \sum_{i=0}^{N-1} x_i,
\end{equation}
so that we penalize using higher colors, and therefore, more colors. In this case, the $C$ tensors now apply imaginary time evolution, their new non-zero elements being
\begin{equation}
    \begin{gathered}
        j_0 = j_1 = j,\quad i\neq j,\\
        C_{j,i,j_0,j_1} = e^{-\tau j}.
    \end{gathered}
\end{equation}

If we now want to generalize the problem to one in which the cost depends on the coloring of each vertex, we will have the same network tensor, but with a cost function
\begin{equation}
    C(\vec{x}) = \sum_{i=0}^{N-1} Q_{i,x_i},
\end{equation}
so the non-zero elements of $C$ tensors now are
\begin{equation}
    \begin{gathered}
        j_0 = j_1 = j,\quad i\neq j,\\
        C_{j,i,j_0,j_1} = e^{-\tau Q_{j,x_j}}.
    \end{gathered}
\end{equation}


\subsection{Partition Problem}
Given a set of $N$ integers $S$, this problem consists in dividing it into two sets of numbers $S_1$ and $S_2$ such that the sums of their elements are the same~\cite{Partition}. This problem has been solved with quantum algorithms making use of QUBO formulation~\cite{Partition_Quantum}. To solve this problem, we have $N$ variables, in which $x_i$ indicates whether the $i$-th number $a_i$ belongs to the first set ($0$) or to the second ($1$). To solve it, we make a tensor network exactly like the Integer sum total function problem (Fig.~\ref{fig: Integer Sum TN}). Here, each variable is binary, and the signal to send is $r_m=\sum_{i=0}^{m-1} (-1)^{x_i} a_i  + c$, being $c = \sum_{i=0}^{m-1} a_i$. This means that, if the $i$-th number is in the $S_1$ set, we sum its value, but if it belongs to the $S_2$ set, we substract it. The offset is done to ensure the signal is never negative. The last tensor only allows the state on which the total sum is equal to $c$, because it means that we have the same sum in $S_1$ and $S_2$. The dimension of the signal indexes is $2c$, and the dimension of the variable indexes is $2$. The non-zero elements are

\begin{equation}
     \begin{gathered}
         \mu=i,\quad \nu=(-1)^i a_0+c,\\
         SUM(a,0)_{i,\mu,\nu} = 1,
     \end{gathered}
\end{equation}
\begin{equation}
     \begin{gathered}
         \mu=i,\quad \nu=j+(-1)^i a_k+c,\\
         SUM(a,k)_{i,j,\mu,\nu} = 1,
     \end{gathered}
\end{equation}
\begin{equation}
     \begin{gathered}
         \mu=i,\\
         (-1)^i a_{N-1} + j = c,\\
         EXPi(a,N-1)_{i,j,\mu} = 1.
     \end{gathered}
\end{equation}
By optimizing the tensors to a chain, as in the QUBO/QUDO first neighbor case, we can have a chain of matrices of dimension $(2c)\times (2c)$, which due to their sparsity actually have $O(4c)$ nonzero elements. Since we have $N$ different tensors, we need an $O(cN)$ space to store them. We can contract the tensor network from bottom to top, and reusing intermediate computations. We need $O(c)$ operations on each contraction, so the determination of the first variable requires $O(cN)$ operations. If we take into account that the following variables are determined based on a multiplication of a new first tensor that already has the previous information, the complexity of each step is $O(c)$. This makes the total computational complexity $O(cN)$, having a space complexity of $O(cN)$ due to the storage of the intermediate vectors.




















