\section{Inversion Problems}
Now that we have solved some unconstrained combinatorial optimization problems, we are going to deal with combinatorial inversion problems. These are useful in cryptography to determine if a cryptographic protocol is secure or not, but we will study also other cases.

\subsection{Cybersecurity}
\subsubsection{Prime Factorization}
{\color{red} Subsubsection not available due to paper pending publication.}

% This problem consists in, given a known number $N$ of $n$ bits, resulting from multiplying two unknown prime numbers $p$ and $q$, determining what those two numbers are from $N$. To solve this problem, the LSTC needed is the one in Fig.~\ref{fig: DOT circuit}. However, we can optimize it for the tensor network in different ways. The most fundamental one is to take into account that the smallest number $q$ is going to have at most $\lceil n/2\rceil$ bits, so we can consider half variables for it. The second is that if $N$ is an even number, $q=2$, so we can separate these cases. If $N$ is odd, then $q$ and $p$ are odd, which forces its least significant bit $p_0=q_0=1$. Moreover, to prevent one of the outcomes from being $p=N$ and $q=1$, we impose that $p$ has at most $n-1$ bits, so that it can never reach $N$. Thus, we need $n-1 + \lceil n/2\rceil$ variables. However, since determining $p$ implies determining $q$, we will only need to determine one of the two. For network contraction issues, we choose to determine $p$, thus needing $n-1$ variables to determine.





\subsubsection{SHA-3 inversion}
{\color{red} Subsubsection not available due to paper pending publication.}





\subsection{Systems of linear equations}
The objective is to obtain the vector $\vec{x}$ that satisfies the equation $A\vec{x}=\vec{b}$ , where $A$ is an invertible square matrix and $\vec{b}$ is a vector, both known. To formulate it as a combinatorial inversion problem, we must work with a matrix $A$ of positive integers, a vector $\vec{b}$ of positive integers and that the solution $\vec{x}$ is also of positive integers. However, it can be extended to all integers using the corresponding shifts and mappings.

\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{Images/Linear_Solver.pdf}
    \caption{MeLoCoToN for solving systems of linear equations with a $4\times 4$ matrix $A$. a) Determining the $x_0$. b) Determining the $x_1$. c) Determining the $x_2$.}
    \label{fig: Lineal Solver}
\end{figure}

In this case, we must make a tensor network that performs the product of all possible $\vec{x}$ vectors with the known $A$ matrix, and postselects the final result to that of $\vec{b}$. Therefore, we make a square tensor network, in which each tensor in the grid performs the product and sum with its corresponding $A$ element for that row and column. The tensor network can be seen in Fig.~\ref{fig: Lineal Solver}. The operator $DOT(A,i,j)$ of row $i$ and column $j$ receives by its upper index the value of the component of $x_j$ and by its left index the value of the sum of products up to that point $\sum_{k=0}^{j-1}A_{ik}x_k$. With that information, it returns by its right index the value $\sum_{k=0}^{j}A_{ik}x_k$. It transmits downward the same signal it receives upward. The indexes names are presented in Fig.~\ref{fig: Notation_Linear}. The non-zero elements of each tensor for the $N\times N$ case are
\begin{equation}
    \begin{gathered}
        \mu = A_{i,0} k,\quad \nu=k,\\
        DOT(A,i,0)_{k\mu\nu} = 1,
    \end{gathered}
\end{equation}
\begin{equation}
    \begin{gathered}
        \mu = A_{N-1,0} k,\\
        DOT(A,N-1,0)_{k\mu} = 1,
    \end{gathered}
\end{equation}
\begin{equation}
    \begin{gathered}
        \mu = l+A_{i,j} k,\quad \nu=k,\\
        DOT(A,i,j)_{lk\mu\nu} = 1,
    \end{gathered}
\end{equation}
\begin{equation}
    \begin{gathered}
        \mu = l+A_{N-1,j} k,\\
        DOT(A,N-1,j)_{l,k,\mu} = 1.
    \end{gathered}
\end{equation}
In case some elements of matrix $A$ are equal 0, we can omit their corresponding tensors, and simply pass the signal. This way, for example, if we have a tridiagonal matrix, we only need three diagonals in the tensor network, allowing its contraction more efficiently.
\begin{figure}[h]
    \centering
    \includegraphics[width=0.4\linewidth]{Images/Notation_Linear.pdf}
    \caption{Indexes names for the tensor of the linear solver.}
    \label{fig: Notation_Linear}
\end{figure}



\subsection{Closure finding problem}
This problem consists in finding the closure of a graph. The closure of a graph is the graph that is achieved by iteratively adding edges between non-connected vertices such that the sum of their degrees exceeds or equals the number of vertices $V$. 

\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{Images/Closure_1.pdf}
    \caption{a) Original graph. b) LSTC of one step of the Closure finding problem. c) Operator signals. d) Tensors of the TLC. e) TLC of one step. Each index connecting nodes is two indexes.}
    \label{fig: Closure 1}
\end{figure}

To understand how the tensor network is constructed, and to be able to visualize it, we will work on two levels of tensor complexity. The first is the layer level and the second is the operator level. The tensor network to be constructed is one in which each vertex of the graph (Fig.~\ref{fig: Closure 1} a) is replaced by a tensor of as many double indexes as edges the graph would have if it were completely connected, and on each possible edge there is a tensor with five indexes (Figs.~\ref{fig: Closure 1} b and c). The vertex tensors receive for half of their indexes the signal of whether that edge is activated, and for the other half they communicate how many edges they have activated. The edge tensors send if that edge is activated by two of its indexes to the two vertex tensors that join, and receive from them how many active edges each one has through other two indexes. If that edge was not activated, and the sum of signals it receives equals or exceeds the number of vertices of the graph, it activates the edge, sending the signal through its superior index. The tensor network can be seen in Fig. ~\ref{fig: Closure 1} e and the tensors in Fig.~\ref{fig: Closure 1} d.


As this process must be repeated in several iterations, a layer is placed for each iteration, so that the edge tensors receive the signal from the previous layer, to know if they were activated in the previous iteration, and perform the process. It continues until the process is finished. Finally it is measured.

\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{Images/Closure_1_TT.pdf}
    \caption{a) Operator decomposition for vertex and edge operators. b) Tensor decomposition for vertex and edge tensors.}
    \label{fig: Closure 1 TT}
\end{figure}


Each vertex tensor can be decomposed into a tensor train, where half of the nodes perform the sum of the active edges, and the other half communicate this sum. Each edge tensor is composed of 2 tensors, one that receives the signal of the previous state and communicates it to the vertices, and another that receives the sum signal and makes the modification. We can observe them in Fig.~\ref{fig: Closure 1 TT}. Their non-zero elements are
\begin{equation}
    \begin{gathered}
    \mu = i,\\
    \delta_{i\mu} = 1,
    \end{gathered}
\end{equation}
\begin{equation}
    \begin{gathered}
    \mu =\nu = i,\\
    \delta_{i\mu\nu} = 1,
    \end{gathered}
\end{equation}
\begin{equation}
    \begin{gathered}
    \mu =\nu =\eta= i,\\
    \delta_{i\mu\nu\eta} = 1,
    \end{gathered}
\end{equation}
\begin{equation}
    \begin{gathered}
    \mu = i,\\
    S_{i\mu} = 1,
    \end{gathered}
\end{equation}
\begin{equation}
    \begin{gathered}
    \mu = i+j,\\
    S_{ij\mu} = 1,
    \end{gathered}
\end{equation}
\begin{equation}
    \begin{gathered}
    \text{if } i=0 \text{ or } j+k<V \Rightarrow \mu = 0, \quad\text{else } \mu = 1,\\
    C_{ijk\mu} = 1.
    \end{gathered}
\end{equation}

Up to this point, the problem is a forward problem. However, we can formulate the problem of, given a known closure, obtaining the graphs that have that closure associated with it. For this, the only thing necessary is, with the same transformation layers of the tensor network, to impose by means of Projection Vectors the closure in the output of the TLC, leaving the input free to perform the Half Partial Trace.





































