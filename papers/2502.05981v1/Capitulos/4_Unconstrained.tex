\section{Unconstrained Optimization Problems}
The first group of problems to be addressed are unconstrained combinatorial optimization problems~\cite{Unconstrained}. In these problems we do not need operators that cancel the amplitude of incompatible solutions with some constraints, because they do not exist.
\subsection{QUBO}
The first type of problems are the most famous in the world of quantum computing. These are the QUBO problems, which have quadratic relations between pairs of binary variables. The cost function in these problems is
\begin{equation}\label{eq: cost QUBO}
    C(\vec{x})=\sum_{i}^{N-1}\sum_{j=0}^{i} Q_{ij}x_ix_j,
\end{equation}
where $Q$ is a $N\times N$ matrix that accounts for the relationships between pairs of variables and $\vec{x}$ is a binary vector to optimize. In this problem we want to minimize $C(\vec{x})$. This kind of problems are usually solved with QAOA~\cite{QAOA} or Quantum Annealing~\cite{Annealing}.

In this problem, each variable is related to all the others, so we need a circuit that relates them in pairs. To do this, each variable must have an operator of $N$ connections, which is in charge of receiving the value of that variable through its first input, transmitting it to the other operators through their outputs and receiving the values of the other variables through their other inputs. With this information, the operator carries out the partial evolution.

Since the product $x_i x_j$ is the same as the product $x_j x_i$, we only have to account for this interaction once. Each operator receives as input the signal from the operators of the variables with lower index, and sends its signal to those with higher index. Therefore, the $i$-th operator makes the evolution with
\begin{equation}
    e^{-\tau \sum_{j=0}^{i} Q_{ij}x_ix_j},
\end{equation}

The original circuit is the one shown in Fig.~\ref{fig: QUBO Circuit} a. Its equivalent tensor network is shown in Fig.~\ref{fig: QUBO TN} a. We can notice that having formulated the circuit in this way, we have tensors with a huge number of indexes, although each one is highly sparse. We can also note that we could delegate all the evolution to the last tensor, since it receives all the information of the system, which is undesirable.
\begin{figure}[h]
    \centering
    \includegraphics[width=0.9\linewidth]{Images/QUBO_Circuit.pdf}
    \caption{LSMC for the QUBO/QUDO/T-QUDO problem with 6 variables. a) One operator per variable, b) One operator per pair of variables.}
    \label{fig: QUBO Circuit}
\end{figure}
\begin{figure}
    \centering
    \includegraphics[width=0.9\linewidth]{Images/QUBO_TN.pdf}
    \caption{Tensor Network for the QUBO/QUDO/T-QUDO problem with 6 variables to determine the first variable value. a) One tensor per variable, b) One tensor per pair of variables.}
    \label{fig: QUBO TN}
\end{figure}

To avoid this, and to have a greater separation of the information, we can realize that each tensor sends by its output exactly the same signal, and that the modulations depend each one only on one of the inputs and the input of the variable. Therefore, we can formulate the circuit as in Fig.~\ref{fig: QUBO Circuit} b. Each original operator is replaced by a chain of operators (its tensor train~\cite{Tensor_Train}) that send each other the same signal, and each one receives or sends to the operators of the other variables a single signal. Thus, each operator deals with a single term of the imaginary time evolution, or sending its signal to one of the other operators. The associated tensor network is presented in Fig.~\ref{fig: QUBO TN} b. The $S^{a,b}_{2\times 2\times 2}$ operator corresponds to the $a$-th variable, and receives its value and the state of the $b$-th variable to perform the imaginary time evolution, and passes the signal of $a$-th variable to the next operator of the same variable. The operator $\delta^{a,b}_{2\times 2\times 2}$ corresponds to the variable $a$-th, and sends its state to the operator $S^{b,a}$. The non-zero elements of these tensors are 
\begin{equation}\label{eq: QUBO delta}
    \begin{gathered}
        \mu = \nu = i\\
        \delta^{a,b}_{i,\mu,\nu} = 1\\
        \delta^{a,a+1}_{i,\mu,\nu} = e^{-\tau Q_{a,a}i^2}
    \end{gathered}
\end{equation}
\begin{equation}\label{eq: QUBO S}
    \begin{gathered}
        \nu = i\\
        S^{a,b}_{i,j,\nu} = e^{-\tau Q_{a,b}ij}\\
        S^{N-1,N-2}_{i,j,\nu} = e^{-\tau (Q_{N-1,N-2}ij+Q_{N-1,N-1}i^2)}
    \end{gathered}
\end{equation}
where we have made the last $\delta$ tensor of each variable and the last $S$ tensor of the last variable take into account the diagonal cost term.

If a term $Q_{a,b}$ of the cost matrix is zero, then there is no direct relationship between the variable $x_a$ and the variable $x_b$. Because of this, we can eliminate all zero element connections. This implies that, for all $a,b$ such that $Q_{a,b}=0$ we can eliminate the tensors $\delta_{b,a}$ and $S_{a,b}$ from the circuit and the tensor network.

\begin{figure}
    \centering
    \includegraphics[width=0.9\linewidth]{Images/QUBO_TN_2_neighbor.pdf}
    \caption{Tensor network for determine the first variable in the QUBO/QUDO/T-QUDO problem in a linear chain with 6 variables. a) First neighbor, b) Second neighbor.}
    \label{fig: QUBO Tridiag}
\end{figure}

A particularly interesting case is one in which each variable is only related to the next. That is, a case in which the $Q$ matrix is tridiagonal, which is going to be a case of linear chain neighbor interaction. The circuit in this case can be realized with a single operator for each variable, having the equivalent tensor network shown in Fig.~\ref{fig: QUBO Tridiag} a. As can be seen, this tensor network can be contracted in an efficient way as it is a chain. This construction can be optimized to require less memory and computation time, as studied in \cite{QUBO_Tridiagonal}.  It is also studied for $m$-neighbors in {\color{red} [pending to publish]}.

In the case of having an interaction with the two nearest neighbors, a five-diagonal $Q$ matrix, we have three tensors for each variable, as shown in Fig.~\ref{fig: QUBO Tridiag} b. As in the tridiagonal case, it is possible to contract a more optimal version of this tensor network in polynomial time, as a chain. In the case with interactions reaching up to the $m$-th neighbor, the tensor network has $m$ 3-index tensors for each variable. One can also solve the problem with a linear equivalent tensor network with indexes of dimension $2^m$ or with one in the form of a grid with the upper triangle and part of the lower one cut off.

\begin{figure}
    \centering
    \includegraphics[width=0.9\linewidth]{Images/QUBO_TN_iteration.pdf}
    \caption{Tensor network for the QUBO/QUDO/T-QUDO problem with 6 variables to determine, a) the second variable, b) the third variable.}
    \label{fig: QUBO Iteration}
\end{figure}

In the iterative process of determining the variables, each time we create the network for a variable, we eliminate all the operators of the variables already determined, and in the lines that connect with them we put a vector of zeros with a 1 in the position of the value determined for that variable. The tensor network for the second and third iteration is shown in Fig.~\ref{fig: QUBO Iteration}.

\subsection{QUDO}
The natural extension of the QUBO problem is the QUDO problem, in which the variables $x_i$ are now natural numbers instead of binary. That is, the cost function is still \eqref{eq: cost QUBO}, but now $x_i\in[0,d_i-1]$. This problem can be solved with the same circuit and tensor network that we have presented for the QUBO problem, but increasing the dimensionality of the tensors, so that they can send signals with the dimensionality of the $x_i$. That is, the equations \eqref{eq: QUBO delta} and \eqref{eq: QUBO S} are still the right ones, but now the tensors are of dimension $\delta^{a,b}_{d_a\times d_a\times d_a}$ and $S^{a,b}_{d_a\times d_b\times d_a}$. All other analyses performed for the QUBO case are directly extensible to this one. This one is also optimized for a linear chain with one neighbor in~\cite{QUBO_Tridiagonal}, and for $m$-neighbors in {\color{red} [pending to publish]}.

\subsection{T-QUDO}
Another natural extension is the T-QUDO, where the cost function is expressed in \eqref{eq: T QUDO cost}. However, we can rewrite it in an alternative way that is easier to interpret
\begin{equation}
    C(\vec{x}) = \sum_{i=0}^{N-1}\sum_{j=0}^{i} C_{i,j,x_i,x_j},
\end{equation}
where the cost depends on which variables are involved and their values throught the tensor $C$. The QUDO case is a particular case of this, in which $C_{i,j,x_i,x_j} = Q_{i,j}x_ix_j$.

In this case, the construction of the TLC is exactly the same as in the QUDO case, only changing the value of the tensor elements, to take into account this form of the cost expression. The new values are
\begin{equation}
    \begin{gathered}
        \mu = \nu = i\\
        \delta^{a,b}_{i,\mu,\nu} = 1\\
        \delta^{a,a+1}_{i,\mu,\nu} = e^{-\tau C_{a,a,i,i}}
    \end{gathered}
\end{equation}
\begin{equation}
    \begin{gathered}
        \nu = i\\
        S^{a,b}_{i,j,\nu} = e^{-\tau C_{a,b,i,j}}\\
        S^{N-1,N-2}_{i,j,\nu} = e^{-\tau (C_{N-1,N-2,i,j}+C_{N-1,N-1,i,i})}.
    \end{gathered}
\end{equation}
 This one is also optimized for a linear chain with one neighbor in~\cite{QUBO_Tridiagonal}, and for $m$-neighbors in {\color{red} [pending to publish]}.

\subsection{HOBO, HODO and T-HODO}
The next extension of the problem is one in which the interactions move from couples to larger groups of $M$ variables. That is, the cost function is expressed as
\begin{equation}\label{eq: cost HOBO}
    C(\vec{x})=\sum_{i_0}^{N-1}\sum_{i_1=0}^{i_0}\cdots \sum_{i_{M-1}=0}^{i_{M-2}} Q_{i_0, i_1, \dots, i_{M-1}}\prod_{k=0}^{M-1}x_{i_k}.
\end{equation}
This is a Higher-Order Binary Optimization (HOBO) problem~\cite{VQC_HOBO,HOBO_Quantum_2}. This problem has been addressed with tensor networks before in~\cite{HOBO_TN,HOBOTAN}. This formulation is also valid for the Higher-Order D-ary Optimization (HODO) problems, where $x_i\in [0,d_i]$. As in the QUDO and QUBO cases, it can be generalized to the tensorial case T-HODO with a cost function
\begin{equation}
    C(\vec{x})=\sum_{i_0}^{N-1}\sum_{i_1=0}^{i_0}\cdots \sum_{i_{M-1}=0}^{i_{M-2}} C_{i_0, i_1, \dots, i_{M-1}, x_{i_0}, x_{i_1}, \dots, x_{i_{M-1}}}.
\end{equation}
For simplicity, the terms in which $i_k=i_{k'}$ for $k\neq k'$ will be introduced in the terms in which the index values are not repeated. That is, we eliminate the self-interactions introducing its effect in the interactions with other variables.

In this case we have two options. The first is to mimic the structure previously presented for the quadratic case, making now each tensor $S$ receive $M-1$ inputs from the previous variables to perform the imaginary time evolution. The second one is that each variable has only $\delta$ tensors, and passes its signal to some new intermediate tensors that, receiving each one the $M$ signals, perform the evolution. However, this second approach ends up requiring a larger number of tensors to perform, for practical reasons, the same calculation as the first one.

\begin{figure}
    \centering
    \includegraphics[width=0.9\linewidth]{Images/HOBO_TN_3.pdf}
    \caption{Tensor network for the T-HODO problem with 6 variables and $M=3$ to determine the first variable.}
    \label{fig: T-HODO 3 TN}
\end{figure}

For the first case, the tensor network with $M=3$ is the one presented in Fig.~\ref{fig: T-HODO 3 TN}. In this case we have to add $\delta^3$ tensors, which are Kronecker deltas of 3 indexes, to indicate to all the minimization tensors of each variable the value of the variables involved in its term. The non-zero values of the tensors are
\begin{equation}
    \begin{gathered}
        \mu = \nu = i,\\
        \delta^{a,b}_{i,\mu,\nu} = 1,
    \end{gathered}
\end{equation}
\begin{equation}
    \begin{gathered}
        \nu = i,\\
        S^{a,b_0,b_1,b_2,\dots}_{i,j_0,j_1,\dots,\nu} = e^{-\tau C_{a,b_0,b_1,\dots,i,j_0,j_1,\dots}}.
    \end{gathered}
\end{equation}

\subsection{Integer sum total function problem}
The cost function of this problem is
\begin{equation}
    C(\vec{x})=f\left(\sum_{i=0}^{N-1} a_i x_i\right),
\end{equation}
where $a_i\in \mathbb{Z}$ are fixed cost constants, $x_i\in [0,d_i-1]$ and $f(\cdot)$ is a non-linear function. The reason $f(\cdot)$ is a nonlinear function is that if it were linear, we would be back to the case of the linear cost function problem, which can be solved by finding the minimum of a list.

To solve this problem, the signal is the partial sum of the argument of the function. That is, the signal is $r_m=\sum_{i=0}^{m-1} a_i x_i$. Since tensors can only send information through positive integers, we perform a shift of all signals. If $c_-=-\sum_{i=0}^{N-1} \min(a_i,0) d_i$ is the minimum possible value for the argument, then we define the signal as $r_m=\sum_{i=0}^{m-1} a_i x_i + c_-$. Thus, if $c_+=\sum_{i=0}^{N-1} \max(a_i,0) d_i$ is the maximum value the argument can take, the dimension of the signal is $D=c_-+c_+$. This dimension can be optimized operator by operator taking into account what is the maximum range of values that can be taken up to the $m$-th variable, but for our analysis we will not do so.
\begin{figure}
    \centering
    \includegraphics[width=0.8\linewidth]{Images/Integer_sum_TN.pdf}
    \caption{Tensor network for the Integer sum total function problem with a) lineal argument b) non-linear argument. c) Tensor network for the Nested cost function problem.}
    \label{fig: Integer Sum TN}
\end{figure}

The tensor network that we will have to create is similar to the one shown in Fig.~\ref{fig: Natural sum TN} above, but taking into account this displacement in order to generalize to integer values. It is shown in Fig.~\ref{fig: Integer Sum TN} a. The non zero elements of the tensors $SUM(a,0)_{d_0\times d_0 \times D}$, $SUM(a,k)_{d_k\times D \times d_k \times D}$ and $EXPi(a,N-1)_{d_{N-1}\times d_{N-1}\times D}$ are
\begin{equation}
     \begin{gathered}
         \mu=i,\quad \nu=a_0 i+c_-,\\
         SUM(a,0)_{i,\mu,\nu} = 1,
     \end{gathered}
\end{equation}

\begin{equation}
     \begin{gathered}
         \mu=i,\quad \nu=j+a_k i+c_-,\\
         SUM(a,k)_{i,j,\mu,\nu} = 1,
     \end{gathered}
\end{equation}

\begin{equation}
     \begin{gathered}
         \mu=i,\\
         EXPi(a,N-1)_{i,j,\mu} = e^{-\tau f(j+a_{N-1}i-c_-)}.
     \end{gathered}
\end{equation}

We can further generalize the problem by making each summand of the argument of the function have a tensor dependence instead of being linear with $x_i$, such that
\begin{equation}
    C(\vec{x})=f\left(\sum_{i=0}^{N-1} g_i(x_i)\right) = f\left(\sum_{i=0}^{N-1} g_{i,x_i}\right),
\end{equation}
where $g_i: \mathbb{Z}\rightarrow \mathbb{Z}$ and $g_{i,x_i}$ is its associated tensor of integer numbers. To solve this problem, we start from the same tensor network, but now taking into account the tensors that must add the tensor term instead of directly adding it, it must add the $g_i(x_i)$. Now we define  $c_-=-\sum_{i=0}^{N-1} \min(\min(g_i),0)$ and $c_+=\sum_{i=0}^{N-1} \max(\max(g_i),0)$. The tensor network is shown in Fig.~\ref{fig: Integer Sum TN} b. The tensor values now are
\begin{equation}
     \begin{gathered}
         \mu=i,\quad \nu=g_{0,i}+c_-,\\
         SUM(g,0)_{i,\mu,\nu} = 1
     \end{gathered}
\end{equation}

\begin{equation}
     \begin{gathered}
         \mu=i,\quad \nu=j+g_{k,i}\geq 0,\\
         SUM(g,k)_{i,j,\mu,\nu} = 1
     \end{gathered}
\end{equation}

\begin{equation}
     \begin{gathered}
         \mu=i,\\
         EXPi(g,N-1)_{i,j,\mu} = e^{-\tau f(j+g_{N-1,i}-c_-)}.
     \end{gathered}
\end{equation}


\subsection{Nested cost function problem}
This is a generalization of the previous case. The cost function of this problem is defined as
\begin{equation}
    \begin{gathered}
        C(\vec{x})=Q_{N-1}\\
        Q_i = f_i(x_i,Q_{i-1})=f_{i,x_i,Q_{i-1}}\ \forall i\in[1,N-1],\\
        Q_0 = f_0(x_0,0)=f_{0,x_0,0},
    \end{gathered}
\end{equation}
where $f_i: \mathbb{Z}^2\rightarrow \mathbb{Z}$ and $f_{i,x_i,Q_{i-1}}$ is its associated tensor of integer numbers. In other words, the cost function is a nested function
\begin{equation}
    C(\vec{x})=f_{N-1}(x_{N-1},f_{N-2}(x_{N-2},\dots f_{1}(x_{1},f_0(x_0,0))\dots)).
\end{equation}

To solve this problem, we start from the same tensor network, but now taking into account the tensors that must add the tensor term instead of adding it, it transforms the signal in function of its variable value. Now we define  $c_-=-\min(\min(f),0)$ and $c_+=\max(\max(f),0)$. The tensor network is shown in Fig.~\ref{fig: Integer Sum TN} c. The tensor values now are
\begin{equation}
     \begin{gathered}
         \mu=i,\quad \nu=f_{0,i,0}+c_-,\\
         NEST(f,0)_{i,\mu,\nu} = 1,
     \end{gathered}
\end{equation}

\begin{equation}
     \begin{gathered}
         \mu=i,\quad \nu=f_{k,i,j-c_-}+c_-,\\
         NEST(f,k)_{i,j,\mu,\nu} = 1,
     \end{gathered}
\end{equation}

\begin{equation}
     \begin{gathered}
         \mu=i,\\
         EXPi(f,N-1)_{i,j,\mu} = e^{-\tau f_{N-1,i,j-c_-}}.
     \end{gathered}
\end{equation}