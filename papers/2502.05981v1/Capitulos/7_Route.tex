\section{Route and Graph Optimization problems}
Routing problems are the first constrained optimization problems we will solve. They consist in finding how to join vertexes/edges within a network, directed or undirected, or assign weights in a way that minimizes the cost of the configuration. They are particularly useful in real applications, such as delivery systems, server-to-server connections, board drilling, etc.

Although we are going to deal with interesting and complicated problems, there are others such as the minimal spanning tree that we have not addressed, because we have not found an efficient way to formulate them, so they are left as a task for the reader.

\subsection{Shortest Path Cost Problem}
Given a graph $G$ with $V$ vertices and $E$ edges between them, with an associated cost $E_{ij}$ to go from vertex $i$ to vertex $j$, this problem consists in obtaining the cost of the shortest path from vertex $a$ to vertex $b$. We assume all the $E_{ij}$ are natural numbers. If two vertexes $i,j$ are not connected, $E_{ij}=\infty$. The internal variables are the path vector
\begin{equation}
    \vec{x} = (x_0,x_1,x_2, \dots, x_{N-1}),
\end{equation}
where $x_t$ indicates the vertex on which we are at time step $t$. This follows $x_0=a$ and $x_{N-1}=b$. The cost function is
\begin{equation}
    C(\vec{x})=\sum_{t=0}^{N-2} E_{x_t,x_{t+1}}.
\end{equation}
If we want to generalize the problem, we can add that the network changes at every time step, so the cost function is now
\begin{equation}
    C(\vec{x})=\sum_{t=0}^{N-2} E^t_{x_t,x_{t+1}},
\end{equation}
where $E^t_{i,j}$ is the cost of going from the $i$-th vertex to the $j$-th vertex at time step $t$. In this cost can be added the cost of being at the vertex $x_t$ at time $t$.

Since we are not looking for the route, but rather the cost of the route, we will not determine the route itself. Moreover, since we do not know the number of steps of the optimal path, we make the cost of each vertex going to itself zero. In this way, unnecessary steps will be filled with the traveler at the same vertex, without increasing the cost. This problem is an optimization problem in which we want to know the cost value instead of the combination. We have studied it to show that the method can also solve these kind of problems.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.9\linewidth]{Images/Shortest_Path_TN.pdf}
    \caption{Tensor Network for the a) Shortest Path Cost Problem, b) Shortest Path Problem, and its notation.}
    \label{fig: Shortest Path}
\end{figure}
The tensor network to solve the problem is the one shown in Fig.~\ref{fig: Shortest Path} a, where the correct value of the first and last variable is imposed, and all the others are allowed to take any value. Each COST tensor will receive through its inputs both the value of the previous variable and the cost accumulated so far, and will return through its outputs the value of its variable and the cost after adding the one associated to its variable. The last tensor will return only the cost of the route. In this case, as we know all the costs at the end, it is not necessary to perform an imaginary time evolution, since we will only have to choose the non-zero component in lower position (which implies lower cost) as the solution. Moreover, with the resulting vector we will be able to obtain how many paths, with $N$ steps, have each of the costs.

The non zero elements of the tensors are
\begin{equation}
     \begin{gathered}
         \mu=\nu=i,\quad \eta=0,\\
         COST(E,0)_{i,\mu,\nu,\eta} = 1
     \end{gathered}
\end{equation}

\begin{equation}
     \begin{gathered}
         \mu=\nu=i,\quad \eta=k+E^t_{ji}<\infty,\\
         COST(E,t)_{i,j,k,\mu,\nu,\eta} = 1%e^{-\tau E_{ij}}
     \end{gathered}
\end{equation}

\begin{equation}
     \begin{gathered}
         \mu=i,\quad \eta=k+E^{N-1}_{ji}<\infty,\\
         COST(E,N-1)_{i,j,k,\mu,\eta} = 1%e^{-\tau E_{ij}}.
     \end{gathered}
\end{equation}
The dimension of each of the tensors is:
\begin{itemize}
    \item $i,j,\mu,\nu$: $V,V,V,V$,
    \item $k$ for $COST(E,t)$: $\sum_{n=0}^{t-1} \max_\star(E^n)$ ,
    \item $\eta$ for $COST(E,t)$: $\sum_{n=0}^{t} \max_\star(E^n)$,
\end{itemize}
where $\max_\star(E^n)$ is the maximum finite element of $E^n$. If we want to reduce the dimensionality, we can impose that routes above a certain cost are not considered, which will be our new dimension.

With this same network we can solve the problem of the minimum number of steps to go between two vertexes $a$ and $b$, changing the matrix of edge weights to $\hat{E}^t_{i,j}=1$ for all $t,i,j$ such that $E^t_{i,j}$ is finite.

\subsection{Shortest Path Problem}
This problem is the same as the previous one, but the ultimate goal of the problem is to determine the least costly route, rather than to determine the cost itself. In this case we allow non-integer costs. This is a historically important problem, whose reference algorithm is Dijkstra's algorithm~\cite{Dijkstra1959}. As we are interested in the route and not the cost, the tensor network performs an imaginary time evolution. Each tensor receives the value of the previous variable, performs the evolution with the term of going from the previous vertex to the current one, and returns to the next tensor the value of its variable.

The tensor network is the one shown in Fig.~\ref{fig: Shortest Path} b. The non-zero elements of the tensors are 
\begin{equation}
     \begin{gathered}
         \mu=\nu=i,\\
         EXPi(E,0)_{i,\mu,\nu} = 1,\\
         EXPi(E,t)_{i,j,\mu,\nu} = e^{-\tau E^t_{ji}},
     \end{gathered}
\end{equation}

\begin{equation}
     \begin{gathered}
         \mu=i,\\
         EXPi(E,N-1)_{i,j,\mu} = e^{-\tau E^{N-1}_{ji}}.
     \end{gathered}
\end{equation}

A more in-depth study of the problem solved with this method in a more optimized version is made in {\color{red} [Pending to publish]}.

This problem can also be generalized in a similar way to the TSP we will present in the following subsection, by means of the changes in the tensor network mentioned in \cite{TSP_TN}.

\subsection{Traveling Salesman Problem (TSP)}\label{ssec: TSP}
This is a historically important problem~\cite{TSP_overview,TSP_General}, with a wide range of practical applications, and has been approached in different ways~\cite{TSP,TSP2,TSP_TN,QUBO_TSP}. Given a graph $G$ with $V$ vertices and $E$ edges between them, with an associated cost $E_{ij}$ to go from vertex $i$ to vertex $j$, the problem consists in obtaining the path with a lower associated cost which runs through all the vertexes without repeating any of them, and ends at the same initial vertex. In principle, if the network does not depend on time, we can always choose the last vertex of the path, since the problem is symmetric in time. Thus, we do not consider it as a variable, since we fix $x_{N-1}=N-1$.

To build the tensor network that solves this problem, we have two distinct and connected parts. The first part takes care of the minimization of the cost function, using the same $EXPi$ tensor layer of Fig.~\ref{fig: Shortest Path} b, slightly changing the last tensor. The second part takes care of the nonrepetition constraint, consisting of $N-2$ constraint layers. The tensor layers that apply a particular constraint to the system, without making another modification of its amplitudes, are called \textit{filter layers}.

In this case, the $k$-th constraint layer has as a signal between tensors how many times the $k$-th vertex has appeared in the path. In this way, each tensor receives the value of its variable and the signal. If vertex $k$ has not appeared, if it appears in this variable, it makes the signal 1, otherwise 0. If it has already appeared, if this variable is equal to $k$, it cancels the amplitude of the combination by multiplying it by zero. If the last tensor of the layer receives that the vertex $k$ has not yet appeared, it forces it to appear in this variable, by canceling all the amplitudes of the rest of the possibilities. With this we force the vertex $k$ to appear once and only once. If we apply this to the $V-2$ first vertexes of the graph, since there is only one remaining possibility, it will have to be that of the remaining vertex, so we do not need a layer for the $k=V-2$.
\begin{figure}
    \centering
    \includegraphics[width=0.7\linewidth]{Images/TSP_TN.pdf}
    \caption{Tensor network for the TSP with 6 variables.}
    \label{fig: TSP TN}
\end{figure}

The tensor network of the problem is shown in Fig.~\ref{fig: TSP TN}. The non-zero elements of the tensors are
\begin{equation}
     \begin{gathered}
         \mu=\nu=i,\\
         EXPi(E,0)_{i,\mu,\nu} = e^{-\tau E_{V-1,i}},\\
         EXPi(E,t)_{i,j,\mu,\nu} = e^{-\tau E_{ji}},
     \end{gathered}
\end{equation}
\begin{equation}
     \begin{gathered}
         \mu=i,\\
         EXPi(E,V-2)_{i,j,\mu} = e^{-\tau (E_{ji}+E_{i,V-1})}.
     \end{gathered}
\end{equation}
\begin{equation}
     \begin{gathered}
         \mu=i,\\
         \text{if } i=k\Rightarrow \nu=1 ,\quad \text{ else } \nu=0,\\
         F(k)^0_{i,\mu,\nu} = 1,
     \end{gathered}
\end{equation}
\begin{equation}
     \begin{gathered}
         \mu=i,\\
         \text{if } i=k\Rightarrow j=0, \nu=1 ,\quad \text{ else } \nu=j,\\
         F(k)^t_{i,j,\mu,\nu} = 1,
     \end{gathered}
\end{equation}
\begin{equation}
     \begin{gathered}
         \mu=i,\\
         \text{if } i=k\Rightarrow j=0,\quad \text{ else } j=1,\\
         F(k)^{V-2}_{i,j,\mu} = 1,
     \end{gathered}
\end{equation}

A deeper analysis of this problem and its generalizations is made in the paper \cite{TSP_TN}, where this method is explained in more detail. It can also be generalized to a time-dependent version in the same way as in the shortest path. The longest path problem can be solved with the same tensor network, simply by changing the sign of the exponentials of the imaginary evolution, so that it penalizes the shortest paths.

\subsubsection{Grid Graph TSP}
An interesting particular case is the TSP with a square grid graph as in Fig.~\ref{fig: Grid_TSP_TN} a. In this case it is interesting to formulate the tensor network as a graph in which we replace each vertex by a tensor, and each edge by two indexes. Now our variables $x_i$ is the time step in which we are at vertex $i$.  With this, to perform the imaginary time evolution, each operator needs to know what is the state of its neighbors and itself. In this way, if one of the neighbors has a state one unit lower than its own, this implies that it is the previous vertex in the path, and it has to make the evolution with the cost of going to that vertex until itself. In addition, if there is no neighbor in a justly lower time, the state is destroyed, since it implies that the traveler has `teleported' to the vertex. The same if there is none just above. This applies in a circular way for the first and last time. This forces the route to be continuous and not to cut. In addition, we also eliminate, just in case, the possibilities of same time in the vertexes that are not the previous or the next. For connectivity issues of the tensor network, in this resolution we can only make the costs between vertexes change every time, but not the connectivity between them. That is, it must always be square, or remove connections with respect to it, but not introduce diagonals or higher distance connections.

\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{Images/Grid_TSP_TN.pdf}
    \caption{Graph of the TSP in square grid and its associated tensor network.}
    \label{fig: Grid_TSP_TN}
\end{figure}

In this way, each operator receives 4 inputs and its state and sends 4 equal outputs, which are its state. In tensorization, this implies that the tensors have 9 indexes of dimension $V$. This can be simplified as in the case of k-colouring, but let us treat it with the denser version. The non-zero elements of the $S^k$ tensor of the network for the $k$-variable are
\begin{equation}
     \begin{gathered}
        j_{up}=j_{down}=j_{left}=j_{right}=j,\\
        (i_{up} = j-1 \text{ or } i_{down} = j-1 \text{ or } i_{left} = j-1 \text{ or } i_{right}= j-1),\\
        (i_{up} = j+1 \text{ or } i_{down} = j+1 \text{ or } i_{left} = j+1 \text{ or } i_{right}= j+1),\\
        i_{pos} \neq i_{pos'}\ \forall pos\neq pos',\\
        S^k_{i_{up},i_{down},i_{left},i_{right},j,j_{up},j_{down},j_{left},j_{right}} = e^{-\tau E_{y,k}},
     \end{gathered}
\end{equation}
being $i_{up}$, $i_{down}$, $i_{left}$ and $i_{right}$ the inputs, $j$, $j_{up}$, $j_{down}$, $j_{left}$ and $j_{right}$ are the outputs and $y$ the vertex with the $j-1$ value. All tensors with less amount of indexes follow the same logic. It is important to note that the index $i_{pos}$ of a tensor is the $j_{pos'}$ of the tensor to which it is attached, $pos'$ being the inverse position of $pos$.

If the lattice has dimensions $N\times M$, being $M\geq N$, we have to contract it from right to left. In a simplified version, where each tensor already has only 8 indexes, it is easy to verify that the computational complexity of this contraction results in $O(V^{2N+5})$ in time, with $O(V^{2N+2})$ in space, without sparsity considerations. By reusing intermediate calculations, the total time complexity is the same, but space complexity $O(V^{2N+3})$. In the worst case, when $M=N=\sqrt{V}$, the time complexity is $O(V^{2\sqrt{V}+5})$ and the space complexity is $O(V^{2\sqrt{V}+3})$. This result shows that every TSP can be solved with a time complexity of $O(V^{2L})$, being $L$ the dimension of the longest section perpendicular to the longest dimension line of the graph.

In case there are holes in the grid such that the vertexes are connected through it, it will only be necessary to replace the tensors of the non-existent vertexes by a $Pass$ tensor whose non-zero elements are
\begin{equation}
     \begin{gathered}
        j_{up}=j_{down}, j_{left}=j_{right},\quad i_{up}=i_{down}, i_{left}=i_{right},\\
        Pass^k_{i_{up},i_{down},i_{left},i_{right},j_{up},j_{down},j_{left},j_{right}} = 1.
     \end{gathered}
\end{equation}
This tensor is responsible for passing the information from one vertex to another through the hole.

If there are holes without internal connections, simply remove both the corresponding vertex tensor and the vertex connections. The computational complexity in both cases remains the same as shown above.

\subsection{Vehicle routing problem}
This problem consists in traversing an entire graph $G$ with a number $M$ of vehicles without repeating any vertex, all leaving and returning to the same vertex, and with the minimum possible global total route cost~\cite{VRP}. This is a generalization of the TSP. This problem has been approached with quantum computing~\cite{VRP_Quantum}. In this case, instead of having a set of variables $x_t$ indicating the vertex at time step $t$, one has a set of variables $x_{v,t}$, indicating the vertex at which the vehicle $v$ is located at time step $t$.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.7\linewidth]{Images/VRP_TN.pdf}
    \caption{Tensor network for the Vehicle routing problem with two vehicles and 6 vertexes. In this case, each vehicle must visit exactly 3 vertexes.}
    \label{fig: VRP TN}
\end{figure}

To approach it with a tensor network, we only need to take the same tensor network as for the TSP (and any of its variants) for each of the vehicles. Since between them they have to cover all the vertices of the network, and pass through each one only once, the constraint layers are joined to each other, so that the constraint signal passes from the network of vehicle $n$ to that of vehicle $n+1$. The tensor network is shown in Fig.~\ref{fig: VRP TN}. It is important to note that, in case we do not want the vehicles to visit a specific number of vertices each, we must make it so that each one can visit all of them, but set a zero cost for staying at the origin vertex.

In this way, the computational complexity is exactly the same as that of the TSP presented above, and the same for all generalizations. Moreover, if we change the evolution layers between vehicles, we can generalize to the case in which the networks are different for each vehicle. We can also model the cost of using new vehicles by adding an extra cost for leaving the origin node.


\subsection{Chinese postman problem}
Given a graph $G$ of $V$ vertices and $E$ edges with costs $E_{ij}$ to go from vertex $i$ to vertex $j$, the problem consists in finding the least costly path of $T$ steps that traverses each edge at least once.

To solve this problem, the variables are a vector $\vec{x}$ where the $x_t$ component indicates on which edge the traveler is at time-step $t$. This makes the cost function dependent only on each variable
\begin{equation}
    C(\vec{x})=\sum_{t=0}^{T-1}E_{V({x_t})}, 
\end{equation}
being $V(k)$ the pair $(i,j)$ of vertexes connected by edge $k$. 

With this in mind, we have to impose the continuity constraint so that the path is a single chain. In order to apply it minimally, each edge is split into two edges, one to go from $i$ to $j$ and one to go from $j$ to $i$. The pair of edges is obtained by the function $E(i,j)$. If one of the original edges had only one possible direction, then a new one is not created for the nonexistent direction. Thus, if at time $t$ we are on edge $k$, at time $t+1$ we can only be on the edges $D(k)$ that leave from the destination vertex of $k$. This causes $x_t\in [0,2E-1]$.

Since each edge has to appear at least once, there is also the constraint that
\begin{equation}
    \exists t\ |\ x_t \in E(i,j)\ \forall i,j \in [0,V-1].
\end{equation}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.7\linewidth]{Images/Chinese_TN.pdf}
    \caption{Tensor Network that solves the Chinese postman problem with 5 edges and 6 time steps.}
    \label{fig: Chinese TN}
\end{figure}

The tensor network we create is the same as the tensor network of the TSP in structure, but changing the tensors involved. It is shown in Fig.~\ref{fig: Chinese TN}. The first layer of tensors $S$ takes care of the evolution in imaginary time and the superposition at the same time. The second tensor layer $J$ handles the connectivity between parts of the path. The next $F$ counting layers take care of guaranteeing that each original edge is traversed at least once. The non-zero elements of each tensor are
\begin{equation}
        S(t)_i = e^{-\tau E_{V(i)}}.
\end{equation}
\begin{equation}
     \begin{gathered}
        \mu=\nu=i,\\
        J(0)_{i,\mu,\nu} = 1,
     \end{gathered}
\end{equation}
\begin{equation}
     \begin{gathered}
        i=D(j),\quad \mu=\nu=i,\\
        J(t)_{i,\mu,j,\nu} = 1,
     \end{gathered}
\end{equation}
\begin{equation}
     \begin{gathered}
        i=D(j),\quad \mu=i,\\
        J(T-1)_{i,\mu,j} = 1,
     \end{gathered}
\end{equation}
\begin{equation}
     \begin{gathered}
        \mu=i,\\
        \text{if } i=n\Rightarrow \nu=1,\quad \text{ else } \nu=0,\\
        F(n)^0_{i,\mu,\nu} = 1,
     \end{gathered}
\end{equation}
\begin{equation}
     \begin{gathered}
        \mu=i,\\
        \text{if } i=n\Rightarrow \nu=1,\quad \text{ else } \nu=j,\\
        F(n)^t_{i,\mu,j,\nu} = 1,
     \end{gathered}
\end{equation}
\begin{equation}
     \begin{gathered}
        \mu=i,\\
        \text{if } j=0\Rightarrow i=n,\\
        F(n)^{T-1}_{i,\mu,j} = 1.
     \end{gathered}
\end{equation}

If we want to address a generalization in which the weights of each edge change at each time step, we can do so by changing the cost function to
\begin{equation}
    C(\vec{x})=\sum_{t=0}^{T-1}E^t_{V({x_t})} .
\end{equation}

If we want each edge $(i,j)$ to appear between $N^0_{(i,j)}$ and $N^f_{(i,j)}$ times, we just have to change the constraint layers as shown in \cite{TSP_TN}. The new tensors are
\begin{equation}
        S(t)_i = e^{-\tau E^t_{V(i)}}.
\end{equation}
\begin{equation}
     \begin{gathered}
        \mu=i,\\
        \text{if } i=n\Rightarrow \nu=\min(j+1,N^f_n),\quad \text{ else } \nu=j,\\
        F(n)^t_{i,\mu,j,\nu} = 1.
     \end{gathered}
\end{equation}
\begin{equation}
     \begin{gathered}
        \mu=i\\
        N^0_n-1\leq j\leq N^f_n,\\
        \text{if } i=n\Rightarrow j<N^f_n,\\
        \text{if } j=N^0_n-1\Rightarrow i=n,\\
        F(n)^{T-1}_{i,\mu,j} = 1.
     \end{gathered}
\end{equation}



\subsection{Minimal Cost Closure Problem}
In this problem a closure of a directed graph is defined as a set of vertices $C$ such that none of its edges points to a vertex outside $C$. Given a weighted directed graph, the objective is to find the closure with the smallest associated weight. This problem has been solved with quantum computing algorithms~\cite{Postman_Quantum}.

\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{Images/Closure_2.pdf}
    \caption{a) Directed graph. b) Tensor Network for its Minimal Cost Closure Problem. c) Tensor Train decomposition of the operator.}
    \label{fig: Closure 2}
\end{figure}

For this problem, we replace each vertex of the original graph by a tensor with an index for each edge of the vertex, and an index to indicate whether that vertex belongs to the closure. Each tensor will consider the superposition of whether or not it is activated (its vertex belongs to the closure). If a tensor is activated, it performs the evolution in imaginary time of the edges coming out of it. In addition, it sends by the indexes corresponding to these the signal that it is activated. If it is not activated, it does not perform the evolution, and sends the signal that it is deactivated. In case that any of the incoming edges communicates an activated signal, the tensors that receive it will also be activated and will make the evolution in imaginary time, sending its signal to the following ones. If it does not receive any activation, it may or may not be activated. The tensor network can be seen in Fig.~\ref{fig: Closure 2} b. Each tensor has $x_i$ indexes for its inputs, $y_i$ indexes for its outputs and $z$ index for its state. It can be decomposed in a tensor train of $\delta$ tensor to pass the signal of its own activation, a $S$ tensor which determines if the state has to be $1$ and performs the imaginary time evolution, and $M$ tensors, which determines if some previous vertex is active. Its non-zero elements are
\begin{equation}
    \begin{gathered}
    \mu =\nu = i,\\
    \delta_{i\mu\nu} = 1,
    \end{gathered}
\end{equation}
\begin{equation}
    \begin{gathered}
    \text{if } i=0\Rightarrow \mu\in\{0,1\},\quad \text{ else } \mu=1,\\
    \nu = \mu,\\
    S^k_{i\mu\nu} = e^{-\tau \mu\sum_{j\in D(k)} E_{k,j}},
    \end{gathered}
\end{equation}
\begin{equation}
    \begin{gathered}
    \mu =\max(i,j),\\
    M_{ij\mu} = 1,
    \end{gathered}
\end{equation}
being $D(k)$ the set of vertexes with an edge that starts in $k$-th vertex and $E_{k,j}$ the cost of the edge connecting $k$ with $j$. If we want to take into account only the number of vertexes in the closure, we can fix all the $E_{i,j}=1$.




\subsection{Maximum Flow Problem}
This problem formulated in 1954~\cite{Maximum_Flow} consists in finding the maximum flow routing for a flow network. A flow network is a directed graph of $V$ vertices and $E$ edges, with a source $S$ and a sink $T$. The capacity $E_{ab}$ of an edge going from vertex $a$ to $b$ is the maximum amount of flow it can let through the edge. The flux is a function $f$ of the edges it satisfies:
\begin{itemize}
    \item The flow through an edge from $a$ to $b$ cannot exceed its capacity. That is, $f_{ab}\leq E_{ab}$.
    \item The sum of flows entering a vertex must be equal to the sum of flows leaving it, except for the source and sink. That is, $$\sum_{a: (a,b)\in E, f_{ab}>0} f_{ab} = \sum_{a: (b,a)\in E, f_{ba}>0} f_{ba},\ \forall b\in V\backslash \{S,T\}.$$
\end{itemize}
The value of flow is the amount of flow going from the source $S$ to the sink $T$. That is,
\begin{equation}
    |f| = \sum_{b: (S,b)\in E} f_{Sb} = \sum_{a: (a,T)\in E} f_{aT}
\end{equation}
The maximum flow problem is to route as much flow as possible from the source to the sink. That is, obtain the maximum $|f|$ possible. It has been solved with quantum annealing~\cite{Max_Flow_Quantum}.

To solve this problem with integer flows and capacities, we make a tensor network with the same structure as the network, where each vertex is a tensor and each edge an index. In addition, we add a tensor on each edge. The vertex tensors receive by their input indexes the amount of flow entering the vertex through that edge, and return by their output indexes the amount of outgoing flow through that vertex. These tensors impose the flow continuity limit constraints, while the dimension of their indexes already impose the capacity limits. The edge tensors are used to determine the amount of flow crossing that edge. To avoid an excessive use of tensors, they can be placed only on the edge to be determined in that step, as a Kronecker delta of 3 indexes, and on the edges already determined, place two Projector Vectors, one in each direction of the edge. The source tensor performs an imaginary time evolution, so that the combination with a higher flux has a higher amplitude. The indexes between vertex $a$ and vertex $b$ are of dimension $f_{a,b}+1$, to take into account the capacity limit of the edge joining them, and the possibility of having null flow. We can see an example of this tensor network in Fig.~\ref{fig: Maximum Flow}.  

\begin{figure}
    \centering
    \includegraphics[width=0.9\linewidth]{Images/Maximum_Flow.pdf}
    \caption{a) Maximum Flow graph, b) Tensor Network to determine the upper edge flow after determine the flow of the below edge.}
    \label{fig: Maximum Flow}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{Images/Maximum_Flow_Decomposition.pdf}
    \caption{Decomposition of the vertex tensors.}
    \label{fig: Maximum Flow Decomp}
\end{figure}


All the vertex, source and sink tensors can be decomposed as shown in Fig.~\ref{fig: Maximum Flow Decomp}. The non-zero elements of the tensors are
\begin{equation}
    \begin{gathered}
        \mu = \nu = i,\\
        \delta_{i\mu\nu} = 1,
    \end{gathered}
\end{equation}
\begin{equation}
    \begin{gathered}
        \mu = i + j,\\
        SUM_{ij\mu} = 1,
    \end{gathered}
\end{equation}
\begin{equation}
    \begin{gathered}
        EXP_{i} = e^{\tau i},
    \end{gathered}
\end{equation}
\begin{equation}
    \begin{gathered}
        \mu = i-j,\\
        SUB_{ij\mu} = 1,
    \end{gathered}
\end{equation}
\begin{equation}
    \begin{gathered}
        \mu= i,\\
        SUBf_{i\mu} = 1.
    \end{gathered}
\end{equation}


\subsection{Maximum Independent Set Problem}
This problem consists in, given a graph $G$ of $V$ vertices and $E$ edges, finding the largest possible independent set. An independent set is a set of vertices of the graph such that no two vertices of the graph share an edge. This problem has been approached with quantum techniques~\cite{Independet_Quantum}.

To solve this problem, we only need to replace each vertex by a tensor and each edge by an index. Each vertex tensor represents a variable. If that variable is $0$, then the vertex does not belong to the independent set, and if it is $1$, then it belongs to the independent set. It is the same TLC as in Fig.~\ref{fig: kcolouring}. The operation of the tensors is exactly the same as in the case of k-colouring, with $k=2$ colors, being that the color $0$ can be repeated. Therefore, all index dimensions in the tensor network are 2. The decomposition of the tensors is the same, but now the U-tensors do not prevent the repetition of the $0$ state. In addition, now the $C$-tensors perform an imaginary time evolution for the $1$-state, to favor combinations with the largest number of vertices in the independent set. In this way, the non-zero elements of the new tensors are
\begin{equation}
    \begin{gathered}
        j_0 = j_1 = j,\\
        \text{if } i=0\Rightarrow j\in\{0,1\},\quad \text{ else } j=0,\\
        C_{j,i, j_0,j_1} = e^{\tau j},
    \end{gathered}
\end{equation}
\begin{equation}
    \begin{gathered}
        \text{if } i=0\Rightarrow j_0\in\{0,1\},\quad \text{ else } j_0=0,\\
        j_1 = j_0,\\
        U_{i,j_0,j_1} = 1.
    \end{gathered}
\end{equation}


\subsection{Minimum Vertex Cover Problem}
This problem consists in, given a graph $G$ with $V$ vertices and $E$ edges, finding the vertex cover with the smallest possible number of vertices~\cite{Vertex_Cover}. A vertex cover is the set of vertices such that every edge of the graph has at least one of its ends inside the set. This problem has been solved with quantum algorithms~\cite{Vertex_Cover_Quantum}, making use of Grover search algorithm~\cite{Grover}.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.9\linewidth]{Images/Vertex_cover_TN.pdf}
    \caption{a) Vertex Cover Solution, b) Tensor Network for the Minimum Vertex Cover Problem.}
    \label{fig: Vertex Cover}
\end{figure}

To solve this problem, our variables $x_i$ indicate whether the $i$-th vertex belongs to the vertex cover. If $x_i=1$ it belongs to the vertex cover, otherwise it does not. Each vertex is replaced by a vertex tensor with one index per edge connected to that vertex, and an $A$-tensor is placed on each edge. The tensor network is shown in Fig.~\ref{fig: Vertex Cover}. The vertex tensors act as Kronecker deltas, sending as a signal the value of that vertex. To minimize the number of vertices in the vertex cover, they perform an imaginary time evolution that reduces the amplitude for each $1$-valued variable. This implies that this tensor is of the type $e^{-\tau i} \delta_{i,j_0,j_1,j_2,j_2,\dots}$. As always, an $n$-index delta can be decomposed into $n-2$ $3$-index deltas and two $2$-index deltas. The $A$ tensors ensure that its edge must have at least one end in the cover. Therefore, its non-zero elements are
\begin{equation}
    \begin{gathered}
        \mu \leq \max(0,1-i),\\
        A_{i\mu} = 1.
    \end{gathered}
\end{equation}



\subsection{Dominating set problem}
This problem consists in finding a dominating set for a graph $G$. A dominant set is a subset $D$ of its vertices, such that any vertex of $G$ is in or has a neighbor in $D$. The variable $x_i$ indicates whether the $i$-th vertex belongs to the dominating set or not. This problem has been solved with quantum algorithms~\cite{Dominant_Grover,Dominant_Set_Quantum}.

For solving this problem, we need a tensor network as the presented in Fig.~\ref{fig: kcolouring}. Each vertex of the graph is transformed into a vertex tensor and each edge into two indexes. Each vertex tensor sends to the adjacent ones its state through its output indexes, and receives the state of each of them through its input indexes. To guarantee that each node is or is adjacent to a vertex of the dominating set, the state is removed only if this tensor receives that all its neighbors and itself are in $0$. That is, the non-zero elements of a vertex tensor $C$ are
\begin{equation}
    \begin{gathered}
    i_0 = i_1 = \dots = i,\quad \sum_k j_k + i\geq 1,\\ 
        C_{i,i_0,i_1,\dots, j_0, j_1,\dots} = 1.
    \end{gathered}
\end{equation}
If we want to find the smallest dominating set, we can apply the cost function
\begin{equation}
    C(\vec{x}) = \sum_i c_i x_i,
\end{equation}
being $c_i$ the cost of having the $i$-vertex in the dominating set. In this case, the $C^a$ tensor for the $a$-th vertex is
\begin{equation}
    \begin{gathered}
    i_0 = i_1 = \dots = i,\quad \sum_k j_k + i\geq 1,\\ 
        C^a_{i,i_0,i_1,\dots, j_0, j_1,\dots} = e^{-\tau c_a i}.
    \end{gathered}
\end{equation}





