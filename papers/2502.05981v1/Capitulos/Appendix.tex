
\section{Frequently Asked Questions}

\paragraph{Does this work imply that P=NP?}

No, this work implies that the explicit equation that solves a problem can be obtained in polynomial time, not that it is also computable in polynomial time. Moreover, polynomial time is not always with respect to the size of the problem, but is with respect to the time required to properly formulate the problem.

\paragraph{Does this work apply to all possible problems?}

This work applies to all well-formulated combinatorial problems. That is, to those in which the problem data are known and exact. For example, in the TSP, if we know exactly the edge costs, or in the CSP, if we know exactly the data imposing the constraints. Even so, the method is easily adaptable to situations in which the correct design values of the problem are not known exactly. It can even be adapted to Inverse Combinatorial Optimization~\cite{Inverse_comb}.

\paragraph{Is this formulation useful for real problems?}

Yes, provided that the problems have characteristics such that they allow efficient contraction of TLC or can be efficiently approximated by the Motion Onion.

\paragraph{In which situations is this method interesting?}

It is interesting especially in cases where there is no known algorithm to solve the problem, and when you want to perform a different mathematical analysis of the problem.

\paragraph{Which libraries are recommended to put it into practice?}

Any library that allows the use and contraction of tensors. An example in Python can be tensornetwork, tensorkrowch or Quimb.

\paragraph{Is the method approximable?}

Yes, it can be approximated both by removing logical layers, implementing them in the iteration, and by contracting the tensor network with approximate representations.

\paragraph{Is this method variational?}

No, it is a method that does not require training or derivation. It returns the solution directly.

\paragraph{I think something is missing or could be better explained.}

The reader is free to contact me directly to give feedback on the work, which may include further corrections in future versions of the document.







\newpage

\section{Glossary}

\begin{table}[h]
    \centering
    \begin{tabular}{|p{4cm}|l|p{8cm}|}
         \hline
         Term & Acronym & Meaning \\
         \hline
         Inversion Problem & - & Problem in which the objective is to obtain the input that results in a known output by means of a function.\\
         \hline
         Forward Problem & - & Problem in which the objective is to obtain the output of a known input by means of a function.\\
         \hline
         Modulated Logical Combinatorial Tensor Networks& MeLoCoToN & Algorithm that allows to obtain the tensor network equation that solves a combinatorial problem, by creating a logic circuit and its tensorization and iteration.\\
         \hline
         Signals Method & - & Construction method to build the classical logical circuits for the problems, based on the use of signals.\\
         \hline
         Logical Signal Transformation Circuits & LSTC & A logical circuit that, given an input, returns the corresponding output of a certain function. \\
         \hline
         Logical Signal Verification Circuits & LSVC & A logical circuit composed of a set of operators that send a set of internal signals to each other, each one being in charge of analyzing a specific part of the input and detecting if any constraint is violated.\\
         \hline
         Logical Signal Modulation Circuits & LSMC & Logic circuit that, given a certain input, returns the same, but alters an associated internal number according to the cost of the input. \\
         \hline
         Circuit Tensorization& CT & Conversion of a logic circuit to a tensor network by transforming its operators to constrained tensors. \\
         \hline
         Tensor Logical Circuit & TLC & Tensor network associated with a logic circuit after tensorization. \\
         \hline
         Input-Output Indexing & IOI & Transformation of the inputs and outputs of the operators to the subscripts of their associated tensors. \\
         \hline
         Half Partial Trace & - & Sum over all possible inputs or outputs for a logic circuit, leaving a single input free. Equivalent to measurement in a formalism in which the probabilities are directly the amplitudes.\\
         \hline
         Humbucker & - & Method to reduce the effect of the amplitudes of suboptimal combinations by multiplying them by a complex phase, causing them to cancel each other out when added together. \\
         \hline
         Motion Onion & -  & Group of methods to make the equation computation lighter. \\
         \hline
         Tensorial Quadratic Unconstrained D-ary Optimization & T-QUDO  & Formulation with a cost function $C(\vec{x})=\sum_{k} C_{k,x_{a_k},x_{b_k}}$ \\
         \hline
    \end{tabular}
    \caption{Glossary of new paper terms.}
    \label{tab: Glossary new}
\end{table}


\begin{table}[h]
    \centering
    \begin{tabular}{|p{4cm}|l|p{8cm}|}
         \hline
         Term & Acronym & Meaning \\
         \hline
         External Signal & - & Information of the circuit that serves as input or output, and determines the combination evaluated.\\
         \hline
         Internal Signal & - & Internal information of the circuit, which is not part of the output, comes from some operators and conditions the action of others who receive it.\\
         \hline
         Intermediate State & - & Internal signal of the circuit that is transformed along the circuit. It may ultimately be the output signal. \\
         \hline
         Amplitude & - & Internal number associated with an input in a logic circuit. Analogous to quantum amplitude. \\
         \hline
         Plus Vector (`+' tensor) & - & Vector with all its elements equal to 1.\\
         \hline
         Minus Vector (`-' tensor) & - & Vector $(-1,1)$.\\
         \hline
         Projection Vector (`$x_i$' tensor) & - & Vector that has only one non-zero element at the position corresponding to the value we want to impose. In other words, a vector of all zeros, except at position $x_i$ we impose.\\
         \hline
         Phase Vector (`P' tensor) & - & Vector with all its elements with module 1 and complex phase uniformly distributed.\\
         \hline
         Transmision Tensor Chain & TTC & A chain of tensors that share and transmit the same signal, and serve as a division of a single logical tensor with more indexes.\\
         \hline
    \end{tabular}
    \caption{Glossary of new paper objects.}
    \label{tab: Glossary new objects}
\end{table}




\begin{table}[h]
    \centering
    \begin{tabular}{|p{4cm}|l|p{8cm}|}
         \hline
         Term & Acronym & Meaning \\
         \hline
         Constraint Satisfaction Problem & CSP & Problem which consist in finding a solution
that satisfies a set of restrictions.\\
         \hline
         Optimization Problem & - & Problem in which the objective is to obtain the combination that has the lowest or highest associated value of a certain function.\\
         \hline
         Cost function & - & Function that determines how bad or good a combination is for a given problem.\\
         \hline
         Imaginary Time Evolution & ITE & Technique consisting of evolving a quantum state with its hamiltonian by making the transformation of time to an imaginary time, making the exponential real. \\
         \hline
    \end{tabular}
    \caption{Glossary of previous paper terms.}
    \label{tab: Glossary old}
\end{table}


\newpage
$ $

\newpage

\section{Tensorial Notation}
To simplify the understanding of the present and future works on tensor networks, both logical and general, we propose a new notation. The purpose of this notation is to make defining sparse tensors with logics clearer, and to make defining large tensor networks with a certain structure simpler at the graphical level.

\subsection{Notation for logical tensors}
The definition of logical tensors can be complicated and unreadable, especially in sparse cases. We have therefore taken a simpler notation to express them.

Take a 4-index $T$ tensor with dimensions $d_0$, $d_1$, $d_2$ and $d_3$, which has only non-zero elements given by a function of the indexes, at the positions given by another function of one or more indexes. This tensor can be expressed in four blocks of information: the name, the indexes, the auxiliaries and the values.

The first thing we will do is to express the dimensions of the tensor in its name, as subscript of products of dimensions. For example, in this tensor its name is
\begin{equation}
    T_{d_0\times d_1\times d_2\times d_3}.
\end{equation}

Each value of the subscripts indicates the dimension of the index of the corresponding position. For when we define the elements of the tensor, we will not add extra indexes, but replace these subscripts by the indexes of the tensor.

We differentiate the independent indexes, which run through all the values of its dimension, from the dependent indexes, whose values are obtained from the first ones. The independent ones will be expressed in roman letters, while the dependent ones will be expressed by means of greek letters. The general way of expressing them is
\begin{equation}
    \begin{gathered}
        \mu = f_\mu(i,j),\\
        \nu = f_\nu(i,j).
    \end{gathered}
\end{equation}

If we had that the tensor has an input value $n$ which creates an internal auxiliary variable $y$ to be introduced to the functions, it can be added as
\begin{equation}
    \begin{gathered}
        y = g_y(n),\\
        \mu = f_\mu(i,j),\\
        \nu = f_\nu(i,j).
    \end{gathered}
\end{equation}

The non-zero elements of the tensor will be those whose indexes satisfy the above equations, and will be obtained by a series of functions. An example is 
\begin{equation}
    T_{ij\mu\nu} =
    \begin{cases} 
    t_1(i,j,\mu,\nu)&\text{ if }  h_1(i,j,\mu,\nu)>0,\\
    t_2(i,j,\mu,\nu)&\text{ if }  h_2(i,j,\mu,\nu)>0,\\
    t_3(i,j,\mu,\nu)&\text{ else }.
   \end{cases}
\end{equation}

Thus, a tensor in general will be expressed as
\begin{align*}
    &T_{d_0\times d_1\times d_2\times d_3\times \dots\times d_N}\\
    &\begin{gathered}
        y = g_y(n,m,\dots),\\
        \vdots\\
        \mu = f_\mu(i,j,\dots),\\
        \nu = f_\nu(i,j,\dots),\\
        \vdots
    \end{gathered}\\
    T_{ij\dots \mu\nu\dots} =&
    \begin{cases} 
    t_1(i,j,\dots,\mu,\nu,\dots)&\text{ if }  h_1(i,j,\dots,\mu,\nu,\dots)>0,\\
    t_2(i,j,\dots,\mu,\nu,\dots)&\text{ if }  h_2(i,j,\dots,\mu,\nu,\dots)>0,\\
    \vdots\\
    t_M(i,j,\dots,\mu,\nu,\dots)&\text{ else }.
   \end{cases}
\end{align*}

A particular case is a tensor $T^m$ of $N$ indexes in which the non-zero elements are in the elements where the rule is
\begin{align*}
    y_j& = m^j\\
    i_n& = \sum_{j} \left(\omega_j i_j -y_j\right) 
\end{align*}
and its elements have the value
\begin{equation}
        T^m_{i_0,i_1,i_2,\dots,i_{N-1}} = 
    \begin{cases}
        \sum_{j} e^{\beta i_j+m} \text{ if } \sum_j i_j -50 > 0,\\
        \sum_{j} e^{\beta i_j-m} \text{ if } \sum_j i_j -100 > 0,\\
        1.
    \end{cases}
\end{equation}

This tensor can be expressed as
\begin{align*}
    &T^m_{d_0\times d_1\times d_2\times d_3\times \dots\times d_N}\\
    &\begin{gathered}
        y_j = m^j\\
        i_n = \sum_{j} \left(\omega_j i_j -y_j\right)
    \end{gathered}\\
     T^m_{i_0,i_1,i_2,\dots,i_{N-1}} =& 
    \begin{cases}
        \sum_{j} e^{\beta i_j+m} \text{ if } \sum_j i_j -50 > 0,\\
        \sum_{j} e^{\beta i_j-m} \text{ if } \sum_j i_j -100 > 0,\\
        1.
    \end{cases}
\end{align*}

\newpage

\subsection{Chemistry-inspired Tensor Network Notation}
This notation is inspired by the way compounds are simplified in chemistry by means of the skeletal structure. In our case, we will do the same with our tensor networks, suppressing unnecessary nodes to lighten their representation.

The motivation for this notation is that in various tensor networks, especially two-dimensional ones, representing the nodes in the figures is unnecessary and time-consuming and does not add information. Moreover, for very large tensor networks, it can blur the images.

Our notation is based on four fundamental points.
\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{Images/Notation_TN_Chemistry.pdf}
    \caption{Representation of standard tensor network vs. its equivalent in simplified notation. a) Normal nodes. b) Chain nodes. c) Index over nodes. d) End nodes.}
    \label{fig: Notation TN}
\end{figure}

The first is the elimination of nodes in tensors with more than one index. In these cases, it will be assumed that there is a tensor whenever there are two intersecting straight lines. This is shown in Fig.~\ref{fig: Notation TN} a. 

The second is that, when a tensor has only two indexes, it will be represented as an angle, so that a linear chain remains as a chain of angles. This is shown in Fig.~\ref{fig: Notation TN} b.

The third is that if an index has to connect two nodes, so that it crosses another index, this will be represented by a curved line. In this way, we will know that the only point where there can be a node is at the ends of the curved line. This is shown in Fig.~\ref{fig: Notation TN} c.

The fourth is the non-elimination of the nodes of a single index, since if we were to eliminate them we would not be able to differentiate them from a free index. This is shown in Fig.~\ref{fig: Notation TN} d.

\newpage


\section{Common tensors and layers}
In this section we will present a series of tensors and layers of tensors that can appear in several different types of problems.

\subsection{Initialization Tensors}
These vectors are responsible for initialization, and half partial trace in some cases. We have the Plus Vector
\begin{equation}
    \begin{gathered}
        \forall i\in d,\\
        +_{i} = 1,
    \end{gathered}
\end{equation}
the Minus Vector
\begin{equation}
        - = (-1,1),
\end{equation}
the Plus Vector with local imaginary time evolution
\begin{equation}
    \begin{gathered}
        \forall i\in d,\\
        +_{i} = e^{-\tau C_i},
    \end{gathered}
\end{equation}
the Plus Vector with humbucker
\begin{equation}
    \begin{gathered}
        \forall j\in d,\\
        +_{j} = e^{2\pi i j/d},
    \end{gathered}
\end{equation}

the Plus Vector with humbucker and local imaginary time evolution
\begin{equation}
    \begin{gathered}
        \forall j\in d,\\
        +_{j} = e^{2\pi i j/d}e^{-\tau C_j},
    \end{gathered}
\end{equation}

and the projection tensor at the value $a$
\begin{equation}
    \begin{gathered}
        i=a,\\
        \delta^{a}_{i} = 1.
    \end{gathered}
\end{equation}


\subsection{Kronecker Delta}

This tensor is responsible for transmitting the same signal in all directions, so that they have to be common to each other. Its non-zero elements are
\begin{equation}
    \begin{gathered}
        i_0 = i_1 = \dots = i_{N-1},\\
        \delta_{i_0, i_1, \dots, i_{N-1}} = 1. 
    \end{gathered}
\end{equation}
It can be decomposed into a train tensor of $N$ Kronecker deltas of two and three indexes as in Fig.~\ref{fig: Delta}.

\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{Images/Delta.pdf}
    \caption{Kronecker delta decomposition in tensor train.}
    \label{fig: Delta}
\end{figure}

\subsection{Pass Tensor}
This 4-index tensor allows the signal to pass from top to bottom and from left to right without any interaction between them. That is, it is like having two identities acting as wires in both directions, as shown in Fig.~\ref{fig: Pass}. Its non-zero elements are
\begin{equation}
    \begin{gathered}
        \mu = i,\quad \nu = j,\\
        Pass_{i,j,\mu,\nu} = 1. 
    \end{gathered}
\end{equation}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.5\linewidth]{Images/Pass.pdf}
    \caption{Pass tensor decomposition.}
    \label{fig: Pass}
\end{figure}

\subsection{Counting layer}
This layer is responsible for counting how many times the value $a$ has appeared in a set of variables, and removing the state if it has appeared more than $N_a$ times. It is a Matrix Product Operator (MPO) layer, and its non-zero elements are
\begin{equation}
    \begin{gathered}
        \mu = i,\ \nu= \delta^{a}_{i},\\
        F(a)^0_{i,\mu,\nu} = 1. 
    \end{gathered}
\end{equation}
\begin{equation}
    \begin{gathered}
        \mu = i,\quad \nu= j+\delta^{a}_{i}\leq N_a,\\
        F(a)^k_{i,j,\mu,\nu} = 1. 
    \end{gathered}
\end{equation}
\begin{equation}
    \begin{gathered}
        \mu = i,\quad j+\delta^{a}_{i}\leq N_a,\\
        F(a)^{N-1}_{i,j,\mu} = 1. 
    \end{gathered}
\end{equation}

\subsection{Single Repetition Layer}
This layer ensures that a certain value $a$ appears $N_a$ and only $N_a$ times in the set of variables. Its non-zero elements are 
\begin{equation}
    \begin{gathered}
        \mu = i,\quad \nu= \delta^{a}_{i},\\
        F(a)^0_{i,\mu,\nu} = 1. 
    \end{gathered}
\end{equation}
\begin{equation}
    \begin{gathered}
        \mu = i,\quad \nu= j+\delta^{a}_{i}\leq N_a,\\
        F(a)^k_{i,j,\mu,\nu} = 1. 
    \end{gathered}
\end{equation}
\begin{equation}
    \begin{gathered}
        \mu = i,\quad j+\delta^{a}_{i}= N_a,\\
        F(a)^{N-1}_{i,j,\mu} = 1. 
    \end{gathered}
\end{equation}



