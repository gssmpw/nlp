\section{Literature review\label{sec:lit}
} 

We consider a classical, well-studied setting: partially observed linear systems with Gaussian or non-Gaussian signal and observation noise. When both signal and observation noises are Gaussian, the Kalman Filter **Sorenson, "Kalman Filtering Theory"** produces the optimal estimator of the state, in the sense of minimizing the mean-square-error at each time $t$. 
If either the signal or the observation noise is non-Gaussian, the KF is not the optimal filter; the MSE- minimizing filter might be significantly more complex. 

A vast literature explores strategies to handle non-linearity or, within the linear context, non-Gaussianity, offering a wide range of heuristic approaches.
We refer the reader to standard sources; see e.g. the textbooks **Anderson, "Optimal Filtering"**,  **Bullo, "Geometric Control of Mechanical Systems"**. 
Our focus here is solely on the non-Gaussianity of the signal and observation noises; see **Gorokhov, "Asymptotic Properties of Kalman Filter"** for an important paper that focuses on non-Gaussianity within the family of linear state-space models. In this paper we re-visit a fundamental and elegant (but minimally cited) result by  **Kushner, "Introduction to Stochastic Control Theory"**. Most relevant for us is the literature that considers asymptotically optimal filters or, more specifically, the asymptotic optimality of the Kalman Filter itself in the non-Gaussian case. 

As previously noted, the Kalman Filter is sub-optimal under non-Gaussian noise. **Goggin, "A transformation to achieve asymptotic optimality"** construct an asymptotically optimal filter by introducing a transformation to the limiting system. They then apply that asymptotic filter to the sequence of pre-limit system and establish its asymptotic optimality. **Bismuth, "Asymptotic optimality of the Kalman Filter"** follow up on  **Goggin, "A transformation to achieve asymptotic optimality"** and considers filtering of a continuous diffusion process. Using a suitable CRLB, they also show that the Goggin filter is asymptotically optimal. Their transformation, in contrast with Goggin's is a centered transformation; the paper  **Sorensen, "Centered vs Non-Centered Transformations"** then compares the centered and non-centered transformations. Importantly, because their process is a diffusion process, the corresponding discrete time model is one with $w_t$ (in our notation) being Gaussian. They refer to the transformation as an application of a ``limiter''; see also \cite[Chapter 20]{Lipster1997}.

 In this paper, we work directly with the pre-limit system, straightforwardly apply the KF to the suitably transformed data and establish a {\em convergence rate} guarantee. We prove that applying a ``fake'' KF to a-la Goggin transformed data (applying the score function to the observations) produces an estimator whose sub-optimality gap is proportional to the reciprocal of the observation noise standard deviation. It also introduces a bias of the same magnitude.  
 
 The convergence rate corresponds to the optimality gap of GF and implies, in stationarity, the asymptotic optimality. These sub-optimality gaps, which emerge from the Taylor expansion, are made explicit by our pre-limit analysis. The pre-limit analysis allows us to study a wide range of parameter settings. 
 
 Instead of being based on functional central limit theorem and convergence of conditional expectations, our arguments are based on first principles that are available in the literature: (i) a Cramér-Rao lower bound for filtering, and (ii) convergence rate results for the Fisher information as laid out in  **Cramér, "Mathematical Methods of Statistics"**. 
 
\iffalse Their argument rely explicitly on the Gaussianity. Relying on more recent results in the CLT literature, we expand the coverage to the non-Gaussian signal noise. Our arguments are direct and simple and cover the spectrum of regimes. As a final note, their model is seemingly different from ours. In their model the signal is a continuous time process and the observation are spaced out and discrete. The variance of the observation noise depends on the interval between consecutive observations and the assumption is that over inter-observation period of length $\Delta$, the observation noise has standard deviation of magnitude $\sqrt{\Delta}$ (with $\Delta$ small). In our case, we consider a discrete time system and the scaling correspond to the scaling of the noise terms in both the signal and the observation. However, their results can be equivalently formulated within our discrete time framework.  \fi 

To use the Fisher-information CLT, we adopt the assumption from **Dudley, "Uniform Central Limit Theorems"** that the random variables in the random walk (in our case the signal noises) have a finite restricted Poincaré constant (Assumption \ref{asum:primitives-driving}). This condition has been recently relaxed  **Vempala, "The Benjamini-Schramm convergence rate"**. 
\vspace*{0.2cm} 

\noindent {\bf Notation.} Any random variable $X$ is defined with respect to a usual filtered probability space $(\Pcal,\Fcal)$ with $\expec[X]$, $\Var[X]$, and $Cov(\cdot)$ denoting the canonical expectation, variance, and covariances. Following standard notation and for two non-negative function $f,g$ we write $f(\gamma)\gg g(\gamma)$ if $f(\gamma)/g(\gamma) \rightarrow \infty$ as $\gamma \uparrow 1$; we write  $f(\gamma)=o(g(\gamma))$ if $g(\gamma)\gg f(\gamma)$. By $I(X)$, we denote the Fisher information (sometimes called simply ‘information’) of the random variable $X$. Observe from the \ref{eq:linearsystemintro} equation that $Y_k$ belongs to a location family with the location being a random parameter $X_k$. We repeatedly use the fact that $I(aX)=I(X)/a^2$ when $X$ belongs to a location family. $\Ocal(\cdot),\Theta(\cdot), o(\cdot)$ denote the usual Bachmann–Landau notations  **Bachmann, "Analytische Zahlentheorie"** to indicate asymptotic order .