
%% bare_jrnl.tex
%% V1.4b
%% 2015/08/26
%% by Michael Shell
%% see http://www.michaelshell.org/
%% for current contact information.
%%
%% This is a skeleton file demonstrating the use of IEEEtran.cls
%% (requires IEEEtran.cls version 1.8b or later) with an IEEE
%% journal paper.
%%
%% Support sites:
%% http://www.michaelshell.org/tex/ieeetran/
%% http://www.ctan.org/pkg/ieeetran
%% and
%% http://www.ieee.org/

%%*************************************************************************
%% Legal Notice:
%% This code is offered as-is without any warranty either expressed or
%% implied; without even the implied warranty of MERCHANTABILITY or
%% FITNESS FOR A PARTICULAR PURPOSE! 
%% User assumes all risk.
%% In no event shall the IEEE or any contributor to this code be liable for
%% any damages or losses, including, but not limited to, incidental,
%% consequential, or any other damages, resulting from the use or misuse
%% of any information contained here.
%%
%% All comments are the opinions of their respective authors and are not
%% necessarily endorsed by the IEEE.
%%
%% This work is distributed under the LaTeX Project Public License (LPPL)
%% ( http://www.latex-project.org/ ) version 1.3, and may be freely used,
%% distributed and modified. A copy of the LPPL, version 1.3, is included
%% in the base LaTeX documentation of all distributions of LaTeX released
%% 2003/12/01 or later.
%% Retain all contribution notices and credits.
%% ** Modified files should be clearly indicated as such, including  **
%% ** renaming them and changing author support contact information. **
%%*************************************************************************


% *** Authors should verify (and, if needed, correct) their LaTeX system  ***
% *** with the testflow diagnostic prior to trusting their LaTeX platform ***
% *** with production work. The IEEE's font choices and paper sizes can   ***
% *** trigger bugs that do not appear when using other class files.       ***                          ***
% The testflow support page is at:
% http://www.michaelshell.org/tex/testflow/

\documentclass[lettersize,onecolumn]{IEEEtran}
%\documentclass[lettersize,journal]{IEEEtran}
\usepackage{amsmath,amsfonts}
\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{array}
\usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
\usepackage{textcomp}
\usepackage{stfloats}
\usepackage{url}
\usepackage{verbatim}
\usepackage{graphicx}
% \usepackage{cite}
\hyphenation{op-tical net-works semi-conduc-tor IEEE-Xplore}
% correct bad hyphenation here
% \hyphenation{op-tical net-works semi-conduc-tor}
\usepackage[english]{babel}

% Set page size and margins
% Replace `letterpaper' with `a4paper' for UK/EU standard size
% \usepackage[letterpaper,top=2cm,bottom=2cm,left=3cm,right=3cm,marginparwidth=1.75cm]{geometry}
% Useful packages
\usepackage{amsmath,amsthm,amsfonts}
\usepackage{graphicx}
\usepackage{bm}
\usepackage{xcolor}
\usepackage[numbers]{natbib}
% \usepackage[style=IEEEtran]{biblatex}
\bibliographystyle{IEEEtran}
% \usepackage[colorlinks=true, allcolors=blue]{hyperref}
\usepackage{cleveref}
\usepackage{nicefrac}
\usepackage{bbm}
\usepackage{tikz}
\usepackage{soul}
% \usepackage{svg} % For SVG image support
% \tikzset{every picture/.style={line width=0.75pt}} %set default line width to 0.75pt   
\newcommand{\convindist}{\overset{\Lcal}{\to}}
\newcommand{\hX}{\widehat{X}}
\newcommand{\Ex}{\mathbbm{E}}
\newcommand{\calO}{\mathcal{O}}
\newcommand{\calV}{\mathcal{V}}
\newcommand{\calW}{\mathcal{W}}

\input{commands.tex}
% \newcommand{\imon}[1]{{\color{purple} #1}}



\begin{document}
%
% paper title
% Titles are generally capitalized except for words such as a, an, and, as,
% at, but, by, for, in, nor, of, on, or, the, to and up, which are usually
% not capitalized unless they are the first or last word of the title.
% Linebreaks \\ can be used within to get better formatting as desired.
% Do not put math or special symbols in the title.
\title{Goggin's corrected Kalman Filter: Guarantees and Filtering Regimes}
%
%
% author names and IEEE memberships
% note positions of commas and nonbreaking spaces ( ~ ) LaTeX will not break
% a structure at a ~ so this keeps an author's name from being broken across
% two lines.
% use \thanks{} to gain access to the first footnote area
% a separate \thanks must be used for each paragraph as LaTeX2e's \thanks
% was not built to handle multiple paragraphs
%

\author{Imon~Banerjee,~%\IEEEmembership{Member,~IEEE,}
 %       John~Doe,~\IEEEmembership{Fellow,~OSA,}
        and~Itai~Gurvich%~\IEEEmembership{Life~Fellow,~IEEE}% <-this % stops a space
\thanks{I. Banerjee is with the Department
of Industrial Engineering and Management Science, Northwestern University, Evanston,
IL, 60208 USA e-mail: (imon.banerjee@northwestern.edu).}% <-this % stops a space
\thanks{I. Gurvich is with the Kellogg School of Management, Northwestern University, Evanston,
IL, 60208 USA e-mail}% <-this % stops a space
\thanks{}}

% note the % following the last \IEEEmembership and also \thanks - 
% these prevent an unwanted space from occurring between the last author name
% and the end of the author line. i.e., if you had this:
% 
% \author{....lastname \thanks{...} \thanks{...} }
%                     ^------------^------------^----Do not want these spaces!
%
% a space would be appended to the last name and could cause every name on that
% line to be shifted left slightly. This is one of those "LaTeX things". For
% instance, "\textbf{A} \textbf{B}" will typeset as "A B" not "AB". To get
% "AB" then you have to do: "\textbf{A}\textbf{B}"
% \thanks is no different in this regard, so shield the last } of each \thanks
% that ends a line with a % and do not let a space in before the next \thanks.
% Spaces after \IEEEmembership other than the last one are OK (and needed) as
% you are supposed to have spaces between the names. For what it is worth,
% this is a minor point as most people would not even notice if the said evil
% space somehow managed to creep in.



% The paper headers
\markboth{Submitted to \textit{IEEE Transactions on Information Theory}}%
{Shell \MakeLowercase{\textit{et al.}}: Bare Demo of IEEEtran.cls for IEEE Journals}
% The only time the second header will appear is for the odd numbered pages
% after the title page when using the twoside option.
% 
% *** Note that you probably will NOT want to include the author's ***
% *** name in the headers of peer review papers.                   ***
% You can use \ifCLASSOPTIONpeerreview for conditional compilation here if
% you desire.




% If you want to put a publisher's ID mark on the page you can do it like
% this:
%\IEEEpubid{0000--0000/00\$00.00~\copyright~2015 IEEE}
% Remember, if you use this you must call \IEEEpubidadjcol in the second
% column for its text to clear the IEEEpubid mark.



% use for special paper notices
%\IEEEspecialpapernotice{(Invited Paper)}




% make the title area
\maketitle

% As a general rule, do not put math, special symbols or citations
% in the abstract or keywords.
\begin{abstract}
In this paper we revisit a non-linear filter for {\em non-Gaussian} noises that was introduced in \cite{goggin1992convergence}. Goggin proved that transforming the observations by the score function and then applying the Kalman Filter (KF) to the transformed observations results in an asymptotically optimal filter. In the current paper, we study the convergence rate of Goggin's filter in a pre-limit setting that allows us to study a range of signal-to-noise regimes which includes, as a special case, Goggin's setting. Our guarantees are explicit in the level of observation noise, and unlike most other works in filtering, we do not assume Gaussianity of the noises. 

Our proofs build on combining  simple tools from two separate literature streams. One is a general posterior Cramér-Rao lower bound for filtering. The other is convergence-rate bounds in the Fisher information central limit theorem.

Along the way, we also study filtering regimes for linear state-space models, characterizing clearly degenerate regimes---where trivial filters are nearly optimal---and a {\em balanced} regime, which is where Goggin's filter has the most value. \footnote{This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible.}

\end{abstract}

% Note that keywords are not normally used for peerreview papers.
% \begin{IEEEkeywords}
% Spaceholder
% \end{IEEEkeywords}






% For peer review papers, you can put extra information on the cover
% page as needed:
% \ifCLASSOPTIONpeerreview
% \begin{center} \bfseries EDICS Category: 3-BBND \end{center}
% \fi
%
% For peerreview papers, this IEEEtran command inserts a page break and
% creates the second title. It will be ignored for other modes.
\IEEEpeerreviewmaketitle

\section{Introduction} 

The setting of this paper is the simplest sequential estimation (or filtering) problem. Specifically, we revisit the linear state space model where one must estimate the unobserved state process $(X_t, t \in \mathbb{N})$ from noisy observations $(Y_t, t \in \mathbb{N})$:
\begin{equation}\begin{split}  
X_{t+1} & = \gamma X_t + \sigma_w {\bar w}_t, ~t\in \{0\}\cup \mathbb{N},\\
Y_t& = X_t+ \sigma_v \bar v_t,~t\in\mathbb{N},\end{split} \tag{Linear System}\label{eq:linearsystemintro}
\end{equation}
where $\bar w_t$ and $\bar v_t$ are i.i.d. zero-mean random variables with $Var(\bar w_t)=Var(\bar v_t)=1$


At each time $t$, an estimator $\hat{x}_t$ of the state is a mapping from $(Y_1,\ldots,Y_t)$ to $\mathbb{R}$. With Gaussian $\bar w$ and $\bar v$, the sequential estimator produced by the Kalman Filter (KF) is unbiased (that is, $\Ex[\hat{x}_t]=\Ex[X_t]$) and its mean squared error (MSE), $\mathbb{E}[(\hat{x}_t - X_t)^2]$ is minimal among {\em all} possible such estimators; making it the minimum variance unbiased estimator (MVUE). The KF is no longer MVUE if either $w$ or $v$ are non-Gaussian. In that case, optimality is maintained only within the smaller class of linear estimators. 

Our paper zooms in on this non-Gaussianity and explores it within a non-asymptotic framework, which we use to uncover ``filtering regimes'', (re)introduce an asymptotically optimal filter from \cite{goggin1992convergence}, and develop {\em non-asymptotic} guarantees for its steady-state performance. 
An immediate pre-asymptotic interpretation of Goggin's asymptotic framework is as follows: one transforms the observations $Y_t$ to the observations $\sigma_v\phi\left(\frac{Y_t}{\sigma_v}\right)$---where $\phi$ is the negative score function for $v$, given by $\phi = -h'/h$, $h$ being the density of $ \bar v$--and subsequently ``pretends'' the underlying system obeys the linear structure
\begin{equation}\begin{split} 
X_{t+1} & = \gamma X_t + \sigma_w w_t,\\
Z_t& = I(v) X_t+ \sigma_v \phi(v_t),\end{split} \label{eq:modified}
\end{equation}

Given this linear system, one applies the simple KF. Thus, this (non-linear) filter---which we refer to as the {\em Goggin filter} (GF)---is linear in the modified ``fake'' observations and maintains the practical appeal of the KF. 

Goggin's analysis is grounded in a a change of measure (likelihood) view. Useful for our purposes is to ground the GF in a Taylor expansion and the Cramér-Rao lower bound (CRLB). 

To that end, consider the case where $X_0=x_0$ is deterministic and $\gamma=\sigma_w=0$. This then becomes the problem of estimating a (location) parameter $x_0$ from noisy observations $Y_t = x_0+\sigma_v \bar v_t, t\in\mathbb{N}$. It is a simple and well-known fact that the average 
$$\hat{x}^A_t= \frac{1}{t}\sum_{s=1}^{t}Y_s,$$
is unbiased but not MVUE unless $v$ is Gaussian.
If $v$ is gaussian then, $\hat{x}^A_t$ achieves the Cramer-Rao lower bound
$$Var(\hat{x}^A_t) = \frac{\sigma_v^2}{t}> \frac{\sigma^2}{t I(v)} = CRLB.$$ 
Considering instead, the estimator
$$\hat{x}_t = \frac{\sigma_v}{tI(\bar v)}\sum_{s=1}^t \phi(Y_t/\sigma_v),$$ where $\phi = -h'/h$ is the (negative of the) score function of $\bar v$; $h$ is its density. Then, 
$$\Ex[\hat{x}_t] = \frac{\sigma_v}{I(\bar v) }\Ex[\phi(Y/\sigma_v)], ~~~Var(\hat{x}_t) = \frac{\sigma_v^2}{tI^2(\bar v)}Var(\phi(Y/\sigma_v)).$$ 
Informally applying a Taylor expansion to $\phi(Y/\sigma_v)$ we have 
$$\sigma_v\phi(Y_t/\sigma_v) = \sigma_v\phi(\bar v_t+x_0/\sigma_v) \approx \sigma_v\phi(\bar v_t) + \Ex[\phi'(\bar v_t)]x_0.$$ 
Because $-\phi$ is the score function of $v$, $\Ex[\phi(v_t)]=0, \Ex[\phi'(v_t)] = Var(\phi(v_t))= I(v)$ so that 
$$ \Ex[\hat{x}_t]\approx x_0, \ Var(\hat{x}_t) \approx \frac{\sigma_v^2}{tI(\bar v)}=CRLB.$$ 
Hence, this estimator {\em approximately} achieves the CRLB with a bias and suboptimality (relative to variance) that depends on the second order term in the Taylor expansion. 

At the most basic level, our paper strengthens Goggin's \cite{goggin1992convergence} limit-theorem result  by providing steady state convergence rates based on first principles. But we also go beyond Goggin's limit theorem to cover a wider range of parameter settings, or {\em regimes}. Specifically, Goggin's analysis---based on limit theorems for conditional expectations---focuses on the case where the best possible mean squared error, $MSE^{\star}$, is of the same order of magnitude of the variance of the (unobserved) process; informally, $MSE^{\star}\approx \Ex[X_t^2]$. 

We partition the space of parameters $(\gamma,\sigma_v,\sigma_w)$ into filtering regimes defined in terms of the relationship between $\sigma_w$ and $\sigma_v$\footnote{$\gamma$ is then set so that the process is ``normalized''; see more in Section \ref{sec:model}}. Goggin's setting is on the boundary of the balanced regime: 
\begin{equation} 
\sigma_w\sigma_v\leq  1,\mbox{ and } \sigma_w\leq \sigma_v \tag{Balanced} 
\end{equation} 
In \cite{goggin1992convergence} and follow-up work (see Section \ref{sec:lit}) $\sigma_w\sigma_v\approx 1$.  

Our main result is that, in this balanced regime, the MSE of the Goggin filter matches the optimal MSE up to an error that is reciprocal in $\sigma_v$. That is, 
\[
MSE \text{ of Goggin's filter}= MSE^{\star} + \calO\left(\frac{1}{\sigma_v}\right).
\]

The $MSE^{\star}$ is itself of the order of $\sigma_v\sigma_w$; see our Lemma \ref{lem:balanced}. Therefore Goggin's filter is first order accurate:
\begin{align*}
    \frac{MSE \text{ of Goggin's filter}}{MSE^\star} = 1 + \calO\left(\sigma_w\right);\numberthis\label{eq:optintro}
\end{align*} the percent error is equivalent to one-step signal noise. 

The study of these pre-limit guarantees beyond the setting of the balanced regime motivates a broader thinking of filtering regimes. Take, as an example, the case where
\[ 
\sigma_v\sigma_w\gg 1.\tag{Large Noise}
\] 
Here the observation noise is excessive and using $\Ex[X_t]$ as the estimate is almost as good as any more sophisticated filter; see our Lemma \ref{lem:toonoisy}.

Intuitively, when the noise is excessively large, the observations become uninformative. The proof of this claim is, however, not obvious because the CRLB is too loose in this setting. 


This looseness of the CRLB is also true for the other extreme/degenerate regime, 
\[ 
\sigma_v\ll \sigma_w. \tag{Small Noise}
\]

We prove that, in this regime, taking the actual observation as the estimator---$\hat{x}_t=Y_t$ is nearly optimal. Intuitively, when the observation noise is very small the observation is as good as any other estimator. 


Returning to \cref{eq:optintro}, we note that the sub-optimality of the GF is meaningful if 
\begin{equation}\frac{1}{\sigma_v}\ll  \sigma_v\sigma_w\leq 1 \end{equation} For these parameters, the sub-optimality induced by the GF is order-of-magnitude smaller than the best possible MSE. Outside of this, when \begin{equation} \sigma_v\sigma_w\leq \frac{1}{\sigma_v},\tag{Low noise} \end{equation} our bounds for the GF are as bad (or as good) as the guarantees for the straightforward application of the KF. Our analysis suggests that this is because, in this regime, the linear approximation that lies at the heart of GF is too loose. \vspace*{0.5cm}


{\bf Our analysis.} Our proofs build a bridge between two literature streams and brings together two existing building blocks to produce a useful lower bound on the best possible stationary MSE. Given the lower bound, the proof that GF's MSE achieves said lower bound is based on a Taylor expansion argument. 

The first of the two building blocks is a Cramér Rao lower bound (CRLB) for sequential estimation. The second is a result on the convergence of the Fisher information in a central-limit-theorem scaling. In the straighforward application of the CRLB the Fisher information of both the observation noise $v$ and the signal noise $w$ would appear; this Fisher information is
--with the exception of the Gaussian case---different than the reciprocal of the variance. In our lower bound, we  keep the Fisher information for the observation noise $\sigma_v\bar v$, but the signal noise $\sigma_w\bar w_t$ is aggregated, across periods, through a Fisher information CLT. 

This ``magic'' cannot be equally applied to the observation noise without compromising significantly the tightness of the lower bound. This is because in the ``balanced'' regime, the CLT error is too big for the observation noise but ``just right'' for the signal noise. 



\section{Literature review\label{sec:lit}} 

We consider a classical, well-studied setting: partially observed linear systems with Gaussian or non-Gaussian signal and observation noise. When both signal and observation noises are Gaussian, the Kalman Filter \cite{kalman1960new} produces the optimal estimator of the state, in the sense of minimizing the mean-square-error at each time $t$. 
If either the signal or the observation noise is non-Gaussian, the KF is not the optimal filter; the MSE- minimizing filter might be significantly more complex. 

A vast literature explores strategies to handle non-linearity or, within the linear context, non-Gaussianity, offering a wide range of heuristic approaches.
We refer the reader to standard sources; see e.g. the textbooks \cite{sarkka2023bayesian,krishnamurthy2016partially}. 
Our focus here is solely on the non-Gaussianity of the signal and observation noises; see 
 \cite{Kitagawa1987} for an important paper that focuses on non-Gaussianity within the family of linear state-space models. In this paper we re-visit a fundamental and elegant (but minimally cited) result by  \cite{goggin1992convergence}. Most relevant for us is the literature that considers asymptotically optimal filters or, more specifically, the asymptotic optimality of the Kalman Filter itself in the non-Gaussian case. 

As previously noted, the Kalman Filter is sub-optimal under non-Gaussian noise. \cite{liptser1996robust} construct an asymptotically optimal filter by introducing a transformation to the limiting system. They then apply that asymptotic filter to the sequence of pre-limit system and establish its asymptotic optimality. \cite{Bobrovsky2001} follow up on \cite{goggin1992convergence} and considers filtering of a continuous diffusion process. Using a suitable CRLB, they also show that the Goggin filter is asymptotically optimal. Their transformation, in contrast with Goggin's is a centered transformation; the paper \cite{Liptser1998}  then compares the centered and non-centered transformations. Importantly, because their process is a diffusion process, the corresponding discrete time model is one with $w_t$ (in our notation) being Gaussian. They refer to the transformation as an application of a ``limiter''; see also \cite[Chatper 20]{Lipster1997}.

 In this paper, we work directly with the pre-limit system, straightforwardly apply the KF to the suitably transformed data and establish a {\em convergence rate} guarantee. We prove that applying a ``fake'' KF to a-la Goggin transformed data (applying the score function to the observations) produces an estimator whose sub-optimality gap is proportional to the reciprocal of the observation noise standard deviation. It also introduces a bias of the same magnitude.  
 
 The convergence rate corresponds to the optimality gap of GF and implies, in stationarity, the asymptotic optimality. These sub-optimality gaps, which emerge from the Taylor expansion, are made explicit by our pre-limit analysis. The pre-limit analysis allows us to study a wide range of parameter settings. 
 
 Instead of being based on functional central limit theorem and convergence of conditional expectations, our arguments are based on first principles that are available in the literature: (i) a Cramér-Rao lower bound for filtering, and (ii) convergence rate results for the Fisher information as laid out in \cite{johnson2004information}. 
 
\iffalse Their argument rely explicitly on the Gaussianity. Relying on more recent results in the CLT literature, we expand the coverage to the non-Gaussian signal noise. Our arguments are direct and simple and cover the spectrum of regimes. As a final note, their model is seemingly different from ours. In their model the signal is a continuous time process and the observation are spaced out and discrete. The variance of the observation noise depends on the interval between consecutive observations and the assumption is that over inter-observation period of length $\Delta$, the observation noise has standard deviation of magnitude $\sqrt{\Delta}$ (with $\Delta$ small). In our case, we consider a discrete time system and the scaling correspond to the scaling of the noise terms in both the signal and the observation. However, their results can be equivalently formulated within our discrete time framework.  \fi 

To use the Fisher-information CLT, we adopt the assumption from \cite{johnson2004information} that the random variables in the random walk (in our case the signal noises) have a finite restricted Poincaré constant (Assumption \ref{asum:primitives-driving}). This condition has been recently relaxed \cite{bobkov_Fisher_2014}. 
\vspace*{0.2cm} 

\noindent {\bf Notation.} Any random variable $X$ is defined with respect to a usual filtered probability space $(\Pcal,\Fcal)$ with $\expec[X]$, $\Var[X]$, and $Cov(\cdot)$ denoting the canonical expectation, variance, and covariances. Following standard notation and for two non-negative function $f,g$ we write $f(\gamma)\gg g(\gamma)$ if $f(\gamma)/g(\gamma) \rightarrow \infty$ as $\gamma \uparrow 1$; we write  $f(\gamma)=o(g(\gamma))$ if $g(\gamma)\gg f(\gamma)$. By $I(X)$, we denote the Fisher information (sometimes called simply ‘information’) of the random variable $X$. Observe from the \ref{eq:linearsystemintro} equation that $Y_k$ belongs to a location family with the location being a random parameter $X_k$. We repeatedly use the fact that $I(aX)=I(X)/a^2$ when $X$ belongs to a location family. $\Ocal(\cdot),\Theta(\cdot), o(\cdot)$ denote the usual Bachmann–Landau notations \cite{cormen_introduction_2022} to indicate asymptotic order  .



\section{Pre-requisite: Filtering regimes\label{sec:model}} 

Consider the linear state space model  \eqref{eq:linearsystemintro}. To place this model within an asymptotic framework we will assume that $\gamma\uparrow 1$ from below; $\gamma<1$ as we focus on the stationary setting. Filtering regimes differ in the way in which the other model parameters, specifically $\sigma_v=\sigma_v(\gamma)$ and $\sigma_w=\sigma_w(\gamma)$ scale as $\gamma$ approaches $1$. 
 
$MSE^{\star}$ is the best possible steady-state MSE across all estimators $\hat{x}_t=\pi({\bf Y}_t)$ where ${\bf Y}_t=(Y_1,\ldots,Y_t)$. That is, 
$$MSE^{\ast} =  \inf_{\pi}\liminf_{t\uparrow \infty}\Ex[(\pi({\bf Y}_t)-X_t)^2].$$

The expectation here is with respect to both the realization of the vector ${\bf Y}$, the value $X_t$ of the process and the prior on $X_0$ at time $t=0$. 

 Effectively, it suffices to consider the two parameters $\sigma_v,\sigma_w$. This is because, given $\gamma,\sigma_w(\gamma)$, we can define a new process 
$$\xi_t=\frac{X_t\sqrt{1-\gamma}}{\sigma_w}$$
where we have suppressed the dependence of $\sigma$ on $\gamma$ for convenience. This process satisfies the dynamics  
$$\xi_{t+1} = \gamma \xi_t +\sqrt{1-\gamma}\bar{w}_t, $$ which has, in steady-state, $\Ex[\xi^2]\approx 1/2$ as $\gamma\uparrow 1$. The observations are suitably transformed to get the process $$Y^{\xi}_t = \sqrt{1-\gamma}\sigma_w=   \xi_t+ \sigma^{\xi}_v\bar{v}_t,$$ where $\sigma_v^{\xi} = \sigma_v\frac{\sqrt{1-\gamma}}{\sigma_w}$.

If an estimator is optimal for this (re)scaled linear system, its linear transformation is optimal for the original linear system. In what follows we maintain the explicit dependence $\sigma_w$ and $\gamma$ but impose this re-scaling indirectly by assuming (without loss of generality) that 

\begin{equation} \frac{\sigma_w^2(\gamma)}{1-\gamma^2}=\Theta(1),\label{eq:constantX}\end{equation} In turn, we also have that  \begin{equation}  (1-\gamma^2) =o\left(\frac{\sigma_w^2(\gamma)}{1-\gamma^2}\right),\label{eq:constantX2}\end{equation}  as $\gamma\uparrow 1$. 

Our first order of business is to characterize the degenerate regimes. %{\color{blue} The proof of this lemma can be found in Section \ref{sec:prf-lemtoonoisy}}

\begin{lemma}[degenerate regime A] Suppose, in addition to \eqref{eq:constantX}, that  \begin{equation} (1-\gamma^2)\sigma_v^2(\gamma)\gg  \frac{\sigma_w^2(\gamma)}{1-\gamma^2}.\label{eq:toonoisy}\end{equation} Then 
  $$ MSE^{\star} \geq \frac{\sigma_{w}^2(\gamma)}{1-\gamma^2} + o\left(\frac{\sigma_{w}^2(\gamma)}{1-\gamma^2}\right),$$ and the lower bound is achieved by the estimator $\hat{x}_t^1 = \Ex[X_t]$.   
\label{lem:toonoisy}
  \end{lemma} 

\noindent All lemma proofs appear in the appendix. 

Lemma \ref{lem:toonoisy} is conceptually intuitive: what better can one do if the noise is too big than  take the expectation as the estimate. But proving this requires a bit of work, as the CRLB---where $w_t$ is non-Gaussian---will have the Fisher information of $w$ (instead of $1/\sigma_w^2$). The proof thus relies on a CLT for the Fisher information on which we rely also later in this paper. 
  
  It is also worth noting that $\frac{\sigma_w^2(\gamma)}{\sigma_v^2(\gamma)(1-\gamma^2)}$ is what is often referred to as the Signal-to-Noise Ratio (SNR). The requirement in \eqref{eq:toonoisy} is that the SNR is smaller than $1-\gamma^2.$  
  
  \vspace*{0.2cm}  

 
\begin{lemma}[degenerate regime B] Suppose that 
$\bar{w}_t$ has a Lipschitz continuous density on the real line that $\sigma_v^2(\gamma)=o(\sigma_w^2(\gamma))$. Then, 
    $$MSE^{\star} \geq \sigma_v^2(\gamma) + o(\sigma_v^2(\gamma)).$$ The lower bound is achieved by taking at time $t$ the estimator $\hat{x}_t^2 = Y_t$.
    \label{lem:nonoise}
   
\end{lemma} 

The intuition is, again, clear: if the observation noise is small enough, one should use the observation as the estimate of the state. The math is not direct because the CRLB is too loose in this setting. Instead we resort to a simple Le-Cam type analysis that, through a study of the posterior probabilities, shows that one cannot do (much) better than using the ``current'' observation alone.  

The two degenerate regimes, lead to the balanced regime. Here, the balance between observation noise and process/signal variance is such that there is real benefit to adaptive filters that use historical observations. 

  \begin{lemma}[balanced regime] If $\sigma_w^2(\gamma)=\calO(\sigma_v^2(\gamma))$
and
$$(1-\gamma^2)\sigma_v^2(\gamma)=\calO\left(\frac{\sigma_w^2(\gamma)}{1-\gamma^2}\right),$$ then both estimators in Lemmas \ref{lem:toonoisy} and \ref{lem:nonoise} are strictly sub-optimal: Letting $MSE^{KF}$ be the steady-state MSE of the (itself suboptimal) Kalman Filter we have 
$$\frac{\sigma_w^2(\gamma)}{1-\gamma^2} - MSE^{KF}  = \Omega  \left(\frac{\sigma_w^2(\gamma)}{1-\gamma^2}\right), ~~ \sigma_v^2(\gamma)- MSE^{KF}=\Omega(\sigma_v^2(\gamma)).$$

\label{lem:balanced} 
  \end{lemma} 

Goggin's asymptotic regime (also in follow-up to his work; see Section \ref{sec:lit}) lies at the boundary of this balanced regime above where 
\[ (1-\gamma^2)\sigma_v^2=\Theta\left(\frac{\sigma_w^2(\gamma)}{1-\gamma^2}\right). \tag{Goggin's limit-theorem regime}
\]

To be consistent with and facilitate the mapping to antecedent literature we use a scalar $N$ and let 
$$\gamma=\gamma(N) = 1-\frac{1}{N},~~ \sigma_w(N)=\sigma_{\bar w}\frac{1}{\sqrt{N}},$$ where $\sigma_{\bar w}>0$ does not scale with $N$. With this choice the signal process is an ``order 1'' process which, as we highlighted above, can be assumed without loss of generality. To allow for different levels of observation noise we use 
$$\sigma_v(N) = s_N \sigma_{\bar{v}},$$ where $\sigma_{\bar v}>0$ does not scale with $N$. Ultimately, the system that we study has
\begin{align*}
    X_{t+1} & = \underbrace{\lp 1-\frac{1}{N}\rp}_{\gamma}X_t + \underbrace{\frac{\sigma_{\bar w}}{\sqrt{N}} \bar w_{t}}_{=: w_{t} }\\
    Y_{t} & = X_t +\underbrace{s_N\bar v_t}_{=: v_t},
\end{align*} where $\bar v_t$ (respectively $\bar w_t$) are i.i.d (non-Gaussian) zero-mean random variables and with standard deviation $\sigma_{\bar v}$ (respectively $\sigma_{\bar w}$).\footnote{\cite{goggin1992convergence} uses the observation process $\bar{Y}_t: = (1/N) Y_t= \frac{1}{N}X_t+ \bar v_t/\sqrt{N}$. We equivalently multiply both sides by $N$.} 

\section{The main result} 

We assume that $\bar{v}$ has a density $h$ that is twice continuously differentiable; $\phi = -h'/h$ is then the negative of the score function. It satisfies 
$$\Ex[\phi(\bar{v})]=0, \mbox{ and } Var(\phi(\bar v))=I(\bar v),$$ where $I(\bar v)$ is the Fisher information of the distribution $h$ (sometimes referred to as the location information). We use $I(\bar{w})$ for the information of $\bar{w}$; to avoid technical difficulties, we assume that $\bar w$ has a continuously differentiable density. Our first assumption is on the observation noise $\bar v$:

\begin{assumption}[observation-noise distribution] The variable $\bar{v}$ has a finite fourth moment and a density which we term as $h$. We assume that $h$ is four-times continuously differentiable. Also, $\phi=-h'/h$ is integrable, in which case, $\Ex[\phi(\bar{v})]=0$ and $Var(\phi(\bar{v}))=\Ex[\phi'(\bar{v})]=I(\bar{v})$. Finally, we assume that  $\|\phi''\|_{\infty}<\infty$. \label{asum:primitives-observation}\end{assumption} 

To apply the information inequality in \cite{johnson2004information} to the signal noise, we need a bounded restricted Poincaré constant of the random variable $\bar{w}$; it is defined as follows 
$$R_{\bar w}^* = \sup_{g\in H_1^*(\bar w)}\frac{\Ex[g^2(\bar w )]}{\Ex[g'(\bar w)]^2},$$ where 
$H_1^*(\bar w)$ is the space of absolutely continuous functions $g$ such that $Var(g(\bar w))>0$,$\Ex[g(\bar w)]=0$, $\Ex[g^2(\bar w)]<\infty$ and $\Ex[g'(\bar w)]=0$. For $\bar w$ that is normal this constant is precisely $\sigma_{\bar w}^2/2$. Our second assumption is on the signal noise $\bar w$.

\begin{assumption}[signal-noise distribution] \label{asum:primitives-driving} The signal-noise random variable $\bar{w}$ has a finite restricted Poincaré constant: $R_{\bar w}^*<\infty$. 
\end{assumption} 
\begin{remark} It is known \cite[Theorem 2]{borovkov1984inequality} that having $R_{\bar w}^*<\infty$ automatically implies that all moments of $\bar w$ exists. This is a strong requirement. We impose it so that we can use results from  \cite{Johnson2004,johnson2004information} for independent but {\bf not} identically-distributed random variables. In \cite{Bobkov2014,Johnson2020}, this assumption was relaxed for i.i.d.\ random variables, but to the best of our knowledge, no equivalent relaxation exists for the non-i.i.d.\ case (Proposition \ref{prop:johnson_inforbd}). Any such relaxations would immediately apply to our own results. \hfill \bsq \vspace*{0.2cm}
\end{remark}


\textbf{Goggin's filter:} The setup for Goggin's filter starts with the nonlinear transformation of the observations
$\sqrt{s_N}\phi\left(\nicefrac{Y_t}{s_N}\right).$ Subsequently, a Taylor-series linearization is applied to the previous transformation to obtain variables $Z_t$
\begin{equation} 
Z_t := I(\bar v_t) X_t +s_N\phi(\bar v_t).\label{eq:hatY}
\end{equation} 
If $v$ is Gaussian, then $Z_t = I(\bar{v})X_t+ \sqrt{N}I(\bar{v})\bar{v}_t$, and this linearization is exact. 



As in Goggin's \cite{goggin1992convergence} analysis, while we transform the observation noise, we do not modify the signal noise sequence. For that noise, an aggregation effect, as we will prove, ensures that the cost of assuming Gaussianity remains suitably small. 

\begin{definition}[Goggin's Filter ({\bf GF})]\label{def:goggin-filter} In each period $t$, the estimator is given by:
$$\hat{x}_t = (1-K_t I(\bar{v}))\gamma \hat{x}_{t-1} + K_tZ_t$$
where $$K_t=\frac{(\gamma^2 P_{t-1} + q)I(\bar{v})}{I^2(\bar{v})(\gamma^2 P_{t-1} +q)+r},$$ and
$$P_t = \frac{r(\gamma^2 P_{t-1} + q)}{I^2(\bar{v})(\gamma ^2 P_{t-1}+q)+r}, ~r=Var(v_t) = s_N^2 I(\bar{v}),\ q=Var(w_t) = \frac{1}{N}\sigma_{\bar w}^2.$$  
\end{definition}

Our main theorem, below, shows how closely its MSE approaches the optimal $MSE^\star$ scaling;  see also Remark \ref{remark:KFvsGF}.

\begin{theorem} Let $\hat{x}_t$ be the (random) estimator produced by Goggin's filter. Then, in stationarity, 
\begin{align} 
\Ex[\hat{x}_t-X_t] &= \calO\left(\frac{1}{s_N}\right) \tag{Bias} \\
\Ex[(\hat{x}_t-X_t)^2]& = MSE^{\star}+\calO\left(\frac{1}{s_N}\right),\tag{Variance} \end{align} where $MSE^{\star}$ is the best possible MSE. Moreover, $MSE^{\star} = \Theta(s_N/\sqrt{N})$ so that 
$$\frac{\Ex[(\hat{x}_t-X_t)^2]}{MSE^{\star}}= 1 + \calO\left(\frac{\sqrt{N}}{s_N^2}\right).$$\label{thm:main}
\end{theorem}
\begin{remark}[$s_N\ll N^{1/4}$] \label{rem:N14} A phase transition for the relative error occurs when $s_N=N^{1/4}$. At this noise level, the bound on the relative error of the Goggin filter is of the same order as the lower bound $MSE^{\star}$. Whether this is a fundamental property of the filter and whether a matching lower bound exists remains an open question. We note that the lower bound in Theorem \ref{thm:CRLB} does not depend on this choice. 

 \cite{Liptser1998} consider the case $s_N=\sqrt{N}$,
a centered ``limiter'' which leads to better numerical performance. Yet, it is not clear that such centering will extend the optimality of GF to $s_N\ll N^{1/4}$. We believe that examining the higher-order terms of the Taylor expansion in \cref{eq:hatY} might be a natural first step. However, the upper bound proof does not seem easily modifiable to higher-order expansions, and we leave such explorations for future work. \hfill \bsq\vspace*{0.2cm} \end{remark} 


\begin{remark}[On the scaling of $MSE^{\star}$]\label{remark:KFvsGF}
Achieving the optimal scaling of the $MSE^{\star}$, i.e., of $\Theta(s_N/\sqrt{N})$ is not where adaptive filters---in this case KF vs. GF---differ. Instead, they differ only in how close they get to the correct constant multiple of $s_N/\sqrt{N}$. Indeed, a naive averaging  filter already achieves the optimal $MSE^{\star}$ scaling. The {\textbf{naive}} filter is defined as follows: for all $t \in [k\tau,(k+1)\tau)$ we set
\begin{align*}
\hat{x}_t = \frac{1}{\tau}\sum_{s=(k-1)\tau+1}^{k\tau}Y_s.    
\end{align*}
The MSE of this estimator is given by $$\Ex[(\hat{x}_t-X_t)^2] = \frac{1}{\tau^2}\Ex\left[\left(\sum_{s=(k-1)\tau+1}^{k\tau}(X_s-X_t)\right)^2 \right] +\frac{1}{\tau}Var(v_1)= \Theta\left(\frac{\tau}{N}\right) +  \frac{1}{\tau}s_N^2\sigma_{\bar v}^2$$ 
To achieve the $MSE^{\star}$ scaling of $s_N/\sqrt{N}$ with this naive filter, a batch size $\tau=\Omega(s_N\sqrt{N})$ suffices. \hfill \bsq \vspace*{0.2cm}
\end{remark} 
 
In terms of the primitives used in the introduction, $\sigma_v = s_N\sigma_{\bar v}$ and $\sigma_w = \frac{1}{\sqrt{N}}\sigma_{\bar w}$, and Theorem \ref{thm:main} can be restated as 
$$\Ex[\hat{x}_t-X_t] = \calO\lp\frac{1}{\sigma_v}\rp,\Ex[(\hat{x}_t-X_t)^2]=MSE^{\star} + \calO\lp \frac{1}{\sigma_v}\rp,$$
while $$MSE^\star = \Theta(\sigma_v\sigma_w),$$ so that the relative deviation satisfies 

$$\frac{\Ex[(\hat{x}_t-X_t)^2]}{MSE^{\star}}= 1 + \calO\left(\frac{1}{\sigma_w \sigma_v^2}\right).$$

\vspace*{0.2cm} 
\paragraph{Asymptotic optimality:} As a complement to Remark \ref{rem:N14}, observe that a straightforward implication of Theorem \ref{thm:main} is the steady-state asymptotic optimality for any $s_N\gg N^{1/4}$:
$$\mbox{ Bias } \rightarrow 0,\mbox{ and } \frac{MSE}{MSE^{\star}}\rightarrow 1, \mbox{ as }N\uparrow \infty.$$

\paragraph{Improvement over the Regular Kalman filter:} Finally, while implicitly clear from earlier literature, the next lemma formally establishes that Goggin's filter offers a substantial improvement over KF. Specifically, for $s_N\gg N^{1/4}$ Goggin's filter has an error that is $o(MSE^\star)$ but the KF has an error that is $\Omega(MSE^\star)$.

\begin{lemma} Suppose that $s_N\gg N^{1/4}$ and let $MSE^{KF}$ be the steady-state MSE of the Kalman Filter. Then, 
 $$MSE^{KF} \geq MSE^\star + \Omega\left(MSE^\star\right)$$
\label{lem:KF}
 \end{lemma} 

\subsection{Outline of the Proof}

We divided the proof of Theorem \ref{thm:main} into two parts. In Section \ref{section:lowerbound}, we establish the lower bound. To do that, we fix a batch period $\tau$ and prove the lower bound for the estimates $\hat x_{\tau},\hat x_{2\tau},\dots,$ at batch periods. The proof for any time $t$ in stationarity follows immediately. Our strategy for proving the lower bound in these batch periods is to apply Johnson's \cite{johnson2004information} inequality on the information of averages (Proposition \ref{prop:johnson_inforbd}) on the batched variant of Kalman filters (as developed by \cite{tichavsky1998posterior}). Proposition \ref{prop:johnson_inforbd} is where boundedness assumptions on the restricted Poincaré constants (Assumption \ref{asum:primitives-observation}) are used. 

The upper bound is established using first principles in Section \ref{sec:upperbound} and does not require  batching. We start with Goggin's filter as defined in Definition \ref{def:goggin-filter}, and---after a Taylor series expansion of $s_N\phi(Y_t/s_N)$---carefully derive upper bounds for each remainder term. Our tools for this analysis are Lemma \ref{lem:KFPXg} which provides an upper bound on the variance $P_t$ and the Kalman gain $K_t$; along with Lemma \ref{lem:remainderterms1}, which provides an upper bound on additional cross terms arriving from Taylor expansions.

With the broad brush-strokes of the proof in place, we turn to the lower-bound proof. 

\section{The Cramér-Rao Lower bound at batch periods}\label{section:lowerbound}

Consider the linear system at times $k\tau,k\in\mathbb{N}\cup \{0\}$. It is a simple matter that this system satisfies the linear dynamics
\begin{align*} X_{k}^B & = \gamma^{\tau} X_{k-1}^B + \sum_{s\in [\tau]}\gamma^{s-1} w_{\tau k-s} = \gamma^{\tau} X_{k-1}^B+\frac{1}{\sqrt{N}}\sum_{s\in [\tau]}\gamma^{s-1} \bar w_{\tau k-s}\\& = 
\gamma^{\tau} X_{k-1}^B + \mathcal{W}_k,
\end{align*} where $X_k^B=X_{\tau k}$, $\gamma=(1-1/N)$, and we define $$\mathcal{W}_k =\frac{1}{\sqrt{N}} \sum_{s\in [\tau]}\gamma^{s-1} w_{\tau k-s}, \mbox{ so that } Var(\calW) = \frac{1}{N} \frac{1-\gamma^{\tau}}{1-\gamma}\sigma_{\bar w}.$$ Finally, $X_0^B = X_0$. 

This linear process has, at time (batch) $k$, the vector-valued observation 
$$Y_k^B = {\bf e} X_k^B + \mathcal{V}_k\in\mathbb{R}^{\tau},$$

where ${\bf e}$ is the column vector of $1$'s and 
$\mathcal{V}_k$ has entries $$\mathcal{V}_{k,t}=\gamma^{\tau k-t}s_N\bar v_t-\frac{1}{\sqrt{N}} \sum_{s=1}^{\tau k-t}\gamma^{s-1} \bar{w}_{\tau k-s}, ~~~ t\in [\tau(k-1)+1,\ldots,\tau k],$$ 
where we define $\sum_{s=1}^0=0$. 

Overall, we are considering the system $(X_k^B, Y_k^B)$ following the dynamics 
\begin{align} \label{eq:batch1} 
X_k^B & = \gamma^{\tau} X_{k-1}^B + \mathcal{W}_k
\\Y_k^B &=  {\bf e} X_k^B + \mathcal{V}_k.%\label{eq:batch2} 
\end{align}

The Cramér Rao lower bound (CRLB), bounds the minimal variance any estimator could have in terms of the Fisher information of the parameter being estimated. In filtering, the parameter being estimated is the (random) state (a ``location parameter'') and the context is Bayesian. The VanTrees inequality is the analog for the random parameter Bayesian context \cite[Page 263]{VanTrees2001} and, relying on it, CRLB-type bounds for sequential estimation (filtering) have been established in the literature; we use these. 

\cite{tichavsky1998posterior} (see additional references in \cite{Bergman2001}) develop a recursion to compute the information for estimating $X_t$ from the observations ${\bf Y}_t = (Y_1,\ldots,Y_t)$  
$$J_t = \Ex\left[-\frac{\partial^2}{(\partial X_t)^2}\log p(X_t,{\bf Y}_t)\right],$$ where $p(X_t,{\bf Y}_t)$ is the joint density of $X_t$ and the vector of observations. 

 Specialized to batch periods, \cite[Proposition 1]{tichavsky1998posterior} shows that $J_k:=J_{k\tau}=I(X_{k\tau})$ satisfies the recursion 
\begin{align*} J_{k+1} & = I(\calW)+{\bf e}' I(\calV){\bf e}-\gamma^{2\tau} I^2(\calW)(J_k+\gamma^{2\tau}I(\calW))^{-1}\\
& = {\bf e}'I(\calV){\bf e}+\frac{J_kI(\calW)}{J_k+\gamma^{2\tau}I(\calW)}, 
\end{align*} with $J_0=I(X_0)$. For all $k$, 
$$MSE^{\ast}_k \geq \frac{1}{J_k}$$

In the theorem below, we approximate $J_k$, proving that the Fisher information for the signal noise can be replaced by the reciprocal of the variance---as if these noises are Gaussian. 

\begin{theorem}\label{thm:batch-mse} Let $MSE_k^{\star,B}$ be the best possible MSE for estimating $X_k^B$. Then, for $\tau=\calO(\min\{s_N^2,N\})=\calO(N)$, 
\[ 
MSE_{\infty}^{\star,B} \geq \frac{1}{\bar{J}_{\infty}} +\calO\left(\frac{1}{\tau}\right)\numberthis\label{eq:MSELB}
\]
where $\bar{J}_\infty$ is the stationary point of the recursion 
$$
\bar{J}_{k+1} = {\bf e}'I(\mathbb{V}_k){\bf e} + \frac{\bar{J}_k\frac{1}{Var(\calW_k)}}{\bar{J_k}+\gamma^{2\tau}\frac{1}{Var(\calW_k)} },$$ and has 
$$\bar{J}_{\infty} = \Theta\left(1+\frac{\sqrt{N}}{s_N}\right).$$
Finally, if $\bar w_t$ is normally distributed there is no approximation in \eqref{eq:MSELB}: $MSE^{\star} \geq \bar{J}_{\infty}^{-1}$. 

\label{thm:CRLB}

\end{theorem} 
\vspace*{0.4cm} 

We will later show that the GF's MSE achieves the lower bound in Theorem \ref{thm:CRLB} up to an error of size $1/s_N$. Setting $\tau=s_N$ in \ref{thm:CRLB} will give us matching upper and lower bounds. 

\vspace*{0.5cm}


\begin{proofof}{Theorem \ref{thm:CRLB}.} Let us decompose $\mathcal{V}_k=\mathbb{V}_k^-+\mathbb{W}_k+{\bf e}_ks_N\bar{v}_{\tau k}$ where ${\bf e}_k$ is the unit vector that has $1$ in the $\tau$'s entry and $0$ elsewhere, and 
\begin{align*}
\mathbb{V}_k^-&=\left(\gamma^{\tau k-t}s_N\bar{v}_t,~ t\in [\tau(k-1)+1,\ldots,\tau k-1]\right),~~ \mathbb{V}_k=[\mathbb{V}^{k-1},s_N\bar{v}_{\tau k}]\\ \mathbb{W}_k& = \left(-\frac{1}{\sqrt{N}} \sum_{s=1}^{\tau k-t}\gamma^{s-1} \bar{w}_{\tau k-s}, ~ t\in [\tau(k-1)+1,\ldots,\tau k-1]\right).\numberthis \label{eq:v_kw_k}
\end{align*}

Both vectors, $\mathbb{V}_k^-$ and $\mathbb{W}_k$, are of dimension $\tau-1$. The vector $\mathbb{V}_k^-$ has independent components but the components of $\mathbb{W}_k$
are dependent;  $\mathbb{V}_k^-$ and $\mathbb{W}_k$ are independent of each other. The information matrix of $\calV$ is then the block matrix
$$I(\calV) = \left[\begin{array}{cc}  I(\mathbb{V}^{-}+\mathbb{W}) & 0 \\
0 & I(\bar v)/s_N^2
\end{array}\right]$$ 

The Fisher information matrix is the covariance matrix of the individual score functions; it is trivially positive semi definite. The Fisher information matrix $I(\mathbb{V}_k^-)$ is diagonal, with the $t^{th}$ element equal to $\frac{I(\bar v)}{\gamma^{2(\tau k-t)}s_N^2}$; it is invertible and positive definite. The vector $\mathbb{W}_k$ is an invertible linear transformation of the vector ${\bf w}_k = (\bar w_{\tau(k-1)+1},\ldots \bar{w}_{\tau k -1})$. Hence $I(\mathbb{W})= (A^{-1})'I({\bf  w})A^{-1}$ and is invertible. 

$\mathbb{V}^{-} + \mathbb{W}$ is itself a linear (though not invertible) transformation of the vector $2\tau$ dimensional vector $({\bf v}_k,{\bf w}_k)$ where ${\bf w}_k$ is as defined above and ${\bf v}= (\gamma^{\tau k-t} s_N^2 \bar v_{t}, t\in [\tau(k-1),\ldots,\tau k-1])$; that is $$\mathbb{V}^{-}+\mathbb{W}= B({\bf v}_k,{\bf w}_k), \mbox{ where } B:=[\mathbb{I}, A],$$ with $\mathbb{I}$ the $\tau\times \tau$ identity matrix. The matrix $B$ has full row rank. 

A version of the Fisher information (convolution) inequality \cite[Corollary 1]{zamir} (see also \cite{bercher2002matrixfisher}
%,bercher2003fisherinfo
) states that 
$$I(\mathbb{V}^{-}+\mathbb{W}) \leq (BI^{-1}({\bf v}_k,{\bf w}_k)B^T)^{-1},$$ because all random variables in $({\bf v}_k,{\bf w}_k)$ are independent of each other, $I({\bf v}_k,{\bf w}_k)$ is a diagonal matrix 
with the first $\tau-1$ entries equal to $\gamma^{\tau k-t}s_N^2$ for the $\tau k-t$ entry and the latter equal to $N/(\sigma_{\bar w}^2I(\bar w))$.

We thus have that 
\begin{align*} &BI({\bf v}_k,{\bf w}_k)B^T = I^{-1}({\bf v}_k) + AI^{-1}({\bf w}_k)A^T=I^{-1}(\mathbb{V}_k^-) + AI^{-1}({\bf w}_k)A^T.
\end{align*} 

Because $A$ is invertible and $I({\bf w}_k)$ has strictly positive diagonal entries, the matrix $AI({\bf w}_k)A^T$ is positive definite (PD). We conclude that  
\begin{align*} 
I(\mathbb{V}_k^-+\mathbb{W}_k)\leq (BI^{-1}({\bf v}_k,{\bf w}_k)B^T)^{-1} = (I^{-1}(\mathbb{V}_k^-) + AI^{-1}({\bf w}_k)A^T)^{-1} \leq I(\mathbb{V}_k^-).\end{align*} In the last inequality we used a fact for PD matrices (that $(A+B)^{-1} \leq A^{-1}, (A+B)^{-1}\leq B^{-1}$).

We arrived at $$I(\calV_k) = \left[\begin{array}{cc}  I(\mathbb{V}_k^-+\mathbb{W}_k) & 0 \\
0 & I(\bar v)/s_N^2
\end{array}\right]\leq  \left[\begin{array}{cc}  I(\mathbb{V}_k^-) & 0 \\
0 & I(\bar v)/s_N^2
\end{array}\right]$$ 
and, by the definition of PSD, that $${\bf e}'I(\calV_k){\bf e}\leq {\bf e}'I(\mathbb{V}_k){\bf e};$$ here $\mathbb{V}_k=[\mathbb{V}_k^-,s_N\bar{v}_{\tau k}]$. In turn, 
$$J_{k+1} \leq {\bf e
}'I(\mathbb{V}_k){\bf e}+ \frac{J_kI(\calW_k)}{J_k+\gamma^{2\tau}I(\calW_k)}.$$

 Letting $\tilde{J}$ solve the recursion 
$$\tilde{J}_{k+1} = {\bf e
}'I(\mathbb{V}_k){\bf e}+ \frac{\tilde{J}_kI(\calW_k)}{\tilde{J}_k+\gamma^{2\tau}I(\calW_k)},$$ a simple induction shows that, with $J_0=\tilde{J}_0$, $J_k\leq \tilde{J}_k$ for all $k$. In particular, 
$$MSE^{\star}_k\geq \frac{1}{\tilde{J}_k}.$$

Let $\bar{J}$, as in the statement of the theorem, be the solution of the recursion 
$$\bar{J}_{k+1} = {\bf e}'I(\mathbb{V}){\bf e} + \frac{\bar{J}_k\frac{1}{Var(\calW)}}{\bar{J_k}+\gamma^{2\tau} \frac{1}{Var(\calW)}}.$$ Theorem \ref{thm:CRLB} now only requires that we prove that 
$$|\tilde{J}_{\infty}-\bar J_{\infty}|=\Theta\left(\frac{1}{\tau}\lp 1+\frac{\sqrt{N}}{s_N}\rp\right),~~\bar{J}_{\infty}=\left(1+\frac{\sqrt{N}}{s_N}\right),$$ which would guarantee that 
$$\left|\frac{1}{\tilde J_{\infty}}-\frac{1}{\bar J_{\infty}}\right|=\calO\left(\frac{1}{\tau}\right)$$
$$MSE^{\star}\geq \frac{1}{J_k}\geq \frac{1}{\tilde J_k}\geq \frac{1}{\bar{J}_k} + \calO\left(\frac{1}{\tau}\right).$$

We introduce the following Lemma.
\begin{lemma}[Fisher information CLT for $\calW$] 
$$1\leq I(\calW)Var(\calW) = 1 + \calO\left(\frac{1}{\tau}\right).$$
\label{lem:CLT_gn}
\end{lemma} 
Since the information of a vector of independent random variables is the sum of the individual-variable Fisher information, we have that 
\begin{align*}
   {\bf e}' I(\mathbb{V}_k){\bf e} & = \frac{1}{s_N^2}I(\bar v)\sum_{s=1}^{\tau }\gamma^{s-1}\\ 
    & = \frac{1}{s_N^2}I(\bar v) \frac{1-\gamma^\tau}{1-\gamma}=\frac{\tau}{s_N^2}I(\bar{v})+\calO\left(\frac{1}{s_N^2}\frac{\tau^2}{N}\right)=\Theta\left(\frac{\tau}{s_N^2}\right)\numberthis\label{eq:VBB-infobound}.
\end{align*}
The final conclusion that  ${\bf e}' I(\mathbb{V}_k){\bf e}=\Theta(\tau/s_N^2)$ holds for all $\tau\leq N$. 

Let $\delta=\delta_{N,\tau} = I(\calW)Var(\calW)-1$. Notice that $\delta>0$ for all distributions except for the Gaussian case where $I(\calW)Var(\calW)-1=0$. Then, $I(\calW) = \frac{1}{Var(\calW)}(1+\delta)$. By Lemma \ref{lem:CLT_gn} $\delta=\calO(1/\tau)$. We are comparing the recursion
\begin{align}
\bar{J}_{k+1} = {\bf e}'I(\mathbb{V}_k){\bf e} + \frac{\bar{J}_k\frac{1}{Var(\calW_k)}}{\bar{J_k}+\gamma^{2\tau}\frac{1}{Var(\calW_k)} },\numberthis\label{eq:approximate-recursion}
\end{align} to the recursion
\begin{align}\tilde{J}_{k+1} & = {\bf e
}'I(\mathbb{V}_k){\bf e}+ \frac{\tilde{J}_kI(\calW_k)}{\tilde{J}_k+\gamma^{2\tau}I(\calW_k)}\nonumber\\ & ={\bf e}'I(\mathbb{V}_k){\bf e}+
\frac{\tilde{J}_k\frac{1}{Var(\calW_k)}(1+\delta)}{\tilde{J}_k+\gamma^{2\tau}\frac{1}{Var(\calW_k)}(1+\delta)}.
\end{align} 

To bound $|\bar J_{\infty}-\tilde J_{\infty}|$, we solve for the steady-state in both equations. The roots of the equation 
\begin{align*}
    x = a+\frac{xb}{x+bc}
\end{align*}
are given by \( x = \frac{a+b-bc\pm\sqrt{(a+b-bc)^2+4abc}}{2} \). Since \(J\)'s are positive, we only take the positive root.

Since \(\Vbb_k\) and \(\Wcal_k\) are stationary with respect to \(k\), we substitute \( a={\bf e}'I(\mathbb{V}_k){\bf e} \), \( b=\frac{1}{Var(\calW)} \), and \( c=\gamma^{2\tau} \) in the equation for $\bar{J}$ and and \( a'={\bf e}'I(\mathbb{V}_k){\bf e} \), \( b'=\frac{(1+\delta)}{Var(\calW)} \), and \( c'=\gamma^{2\tau} \) in that for $\tilde{J}$. We then get the equation
\begin{align*}
    \bar J & = \frac{{\bf e}'I(\mathbb{V}){\bf e}+\frac{1-\gamma^{2\tau}}{Var(\calW)}+\sqrt{\bigl({\bf e}'I(\mathbb{V}){\bf e}+\frac{1-\gamma^{2\tau}}{Var(\calW)} \bigr)^2+4{\bf e}'I(\mathbb{V}){\bf e}\gamma^{2\tau}\frac{1}{Var(\calW)}}}{2}\\
    & =  \frac{{\bf e}'I(\mathbb{V}){\bf e}+\frac{1-\gamma^{2\tau}}{Var(\calW)}+\sqrt{A}}{2}, \\
    \tilde J & = \frac{{\bf e}'I(\mathbb{V}){\bf e}+\frac{(1+\delta)(1-\gamma^{2\tau})}{Var(\calW)}+\sqrt{\bigl({\bf e}'I(\mathbb{V}){\bf e}+\frac{(1+\delta)(1-\gamma^{2\tau})}{Var(\calW)} \bigr)^2+4{\bf e}'I(\mathbb{V}){\bf e}\frac{\gamma^{2\tau}(1+\delta)}{Var(\calW)}}}{2}\\
    & = \frac{{\bf e}'I(\mathbb{V}){\bf e}+\frac{(1+\delta)(1-\gamma^{2\tau}}{Var(\calW)}+\sqrt{B}}{2}.
\end{align*} Subtracting, we have
\begin{align*}
    |\bar J-\tilde J| & \leq \frac{\lvert (1-\gamma^{2\tau})\bigl(\frac{1}{Var(\calW)}-\frac{(1+\delta)}{Var(\calW)}\bigr)\rvert}{2}+\left\lvert\frac{\text{Term}}{2}\right\rvert.
\end{align*}
For the first term 
\begin{align*}
    \frac{\lv(1-\gamma^{2\tau})\lp\frac{1}{Var(\calW)}-\frac{(1+\delta)}{Var(\calW)}\rp\rv}{2} & =\frac{1}{2}\left\lvert \frac{(1-\gamma^{2\tau})\delta}{Var(\calW)}\right\rvert\\
    \text{(since $Var(\calW)=\nicefrac{(1-\gamma^{2\tau})\sigma_{\bar w}^2}{N(1-\gamma^2)}$)}\qquad \qquad  & = \Ocal(\delta)=\calO\left(\delta\lp(1+\frac{\sqrt{N}}{s_N}\rp\right).\numberthis\label{eq:J-diffterm1}
\end{align*}

The second term arises as half of the difference \(|\sqrt A-\sqrt B|\), which can now be rationalized by multiplying and dividing by \(|\sqrt A+\sqrt B|\). We can write
\begin{align*}
    \text{Term} & = \frac{C^2-D^2+4{\bf e}'I(\mathbb{V}){\bf e}\gamma^{2\tau}\left(\frac{1}{Var(\calW)}-\frac{1+\delta}{Var(\calW)}\right)}{\sqrt{A}+\sqrt{B}}.\numberthis\label{eq:J-diffterm2}
\end{align*}
Where
\begin{align*}
    C^2-D^2 & = \left({\bf e}'I(\mathbb{V}){\bf e}+\frac{1-\gamma^{2\tau}}{Var(\calW)} \right)^2-\lp{\bf e}'I(\mathbb{V}){\bf e}+\frac{(1+\delta)(1-\gamma^{2\tau})}{Var(\calW)} \rp^2\\
    & = (1-\gamma^{2\tau})\left(\frac{1}{Var(\calW)}-\frac{1+\delta}{Var(\calW)}\right)(C+D).
\end{align*}

Observe trivially that 
\begin{align*}
\sqrt{A} & =\sqrt{\bigl({\bf e}'I(\mathbb{V}){\bf e}+\tfrac{1-\gamma^{2\tau}}{Var(\calW)} \bigr)^2+4{\bf e}'I(\mathbb{V}){\bf e}\gamma^{2\tau}\frac{1}{Var(\calW)}}\\
& \geq {\bf e}'I(\mathbb{V}){\bf e}+\frac{1}{Var(\calW)} -\gamma^{2\tau}\frac{1}{Var(\calW)}\\
& = C.    
\end{align*}
Similarly \(\sqrt{B}\geq D\) and we get that
\begin{align*}
    \frac{C^2-D^2}{\sqrt{A}+\sqrt{B}}\leq (1-\gamma^{2\tau})\left(\frac{1}{Var(\calW)}-\frac{1+\delta}{Var(\calW)}\right)\leq \Ocal(\delta)=\calO\left(\delta\lp(1+\frac{\sqrt{N}}{s_N}\rp\right)
    .\numberthis\label{eq:J-diffterm3}
\end{align*}
The final term can be bounded as
\[
\frac{4{\bf e}'I(\mathbb{V}){\bf e}\gamma^{2\tau}\left(\frac{\delta}{Var(\calW)}\right)}{\sqrt{A}+\sqrt{B}}\leq\frac{4{\bf e}'I(\mathbb{V}){\bf e}\gamma^{2\tau}\left(\frac{\delta}{Var(\calW)}\right)}{\sqrt{A}}.
\]
We trivially lower bound A with $4{\bf e}'I(\mathbb{V}){\bf e}\gamma^{2\tau}\left(\nicefrac{1}{Var(\calW)}\right)$ to get that
\begin{align*}
    \frac{4{\bf e}'I(\mathbb{V}){\bf e}\gamma^{2\tau}\left(\frac{\delta}{Var(\calW)}\right)}{\sqrt{A}} & \leq \delta  \sqrt{4{\bf e}'I(\mathbb{V}){\bf e}\gamma^{2\tau}\left(\frac{1}{Var(\calW)}\right)}
\end{align*}
Substituting the upper bound of $e'I(\Vbb)e$ from \cref{eq:VBB-infobound}, and the value of $Var(\calW)$, we now get
\begin{align*}
    \delta  \sqrt{4{\bf e}'I(\mathbb{V}){\bf e}\gamma^{2\tau}\left(\frac{1}{Var(\calW)}\right)} \leq \delta \calO\lp\sqrt{\gamma^{2\tau}\frac{2\tau}{s_N^2}\frac{N}{\sigma_{\bar w}^2}\frac{1-\gamma^2}{1-\gamma^{2\tau}}}\rp,
\end{align*} 
where observe that we upper bounded  $\tau^2/(s_N^2N)\leq (\tau/s_N^2)(\tau/N) = \calO(\tau/s_N^2)$ using $\tau=\calO(N)$. We now upper bound $\gamma^{2\tau}$ by $1$, and note that $1/Var(\calW) = (N/\sigma_{\bar w}^2)({1-\gamma^2})/({1-\gamma^{2\tau}})=\calO(N/\tau)$\footnote{This holds for all $\tau\leq N$ including $\tau=N$}. We then get,
\begin{align*}
    \delta \sqrt{\gamma^{2\tau}\frac{2\tau}{s_N^2}\frac{1-\gamma^2}{\sigma_{\bar w}^2(1-\gamma^{2\tau})}}=\Ocal\lp \delta \frac{\sqrt{N}}{s_N}\rp=\calO\lp \delta\lp 1+ \frac{\sqrt{N}}{s_N}\rp\rp.
\end{align*}
Trivially upper bounding $\delta\leq \delta \frac{\sqrt{N}}{s_N}$ in equations (\ref{eq:J-diffterm1}) and (\ref{eq:J-diffterm3}), we have the required result. To complete the proof of Theorem \ref{thm:CRLB} we need to establish the order of magnitude of $\bar{J}$:
$$\bar{J}_{\infty}=\Theta\left(1+\frac{\sqrt{N}}{s_N}\right),\tilde{J}_k=\Theta\left(1+\frac{\sqrt{N}}{s_N}\right).$$ We only need to prove the first of these as the second follows from the, already established, fact that $|\bar{J}_{\infty}-\tilde{J}_{\infty}|=\calO((1/\tau)(1+\sqrt{N}/s_N)).$

Recall 
\begin{align*}
    \bar J & = \frac{{\bf e}'I(\mathbb{V}){\bf e}+\frac{1}{Var(\calW)}(1-\gamma^{2\tau})+\sqrt{\bigl({\bf e}'I(\mathbb{V}){\bf e}+\frac{1}{Var(\calW)}(1-\gamma^{2\tau})\bigr)^2+4{\bf e}'I(\mathbb{V}){\bf e}\gamma^{2\tau}\frac{1}{Var(\calW)}}}{2},\end{align*}  and that $$(1-\gamma^{2\tau} )/Var(\calW) = (1-\gamma^2)\frac{N}{\sigma_{\bar w}^2} = \frac{2}{\sigma_{\bar{w}}^2}+o(1) = \Theta \left(1\right), ~{\bf e}'I(\mathbb{V}){\bf e}=\Theta\left(\frac{\tau}{s_N^2}\right).$$

Thus,
$$\frac{{\bf e}'I(\mathbb{V}){\bf e}+\frac{1}{Var(\calW)}(1-\gamma^{2\tau})}{2}=\Theta\left(1+\frac{\tau}{s_N^2}\right)=\Theta(1),$$ where the last equality follows using $\tau=\calO(s_N^2)$. Next, using the same ingredients as well as $\gamma^{2\tau}=(1-1/N)^{2\tau}=\Omega(1)$ for any $\tau=\calO(N)$, we have that 
$$4{\bf e}'I(\mathbb{V}){\bf e}\gamma^{2\tau}\frac{1}{Var(\calW)}=\Theta\left(\frac{\tau}{s_N^2}\frac{N}{\tau}\right)=\Theta\lp\frac{N}{s_N^2}\rp,$$ so that overall 

$$\sqrt{\bigl({\bf e}'I(\mathbb{V}){\bf e}+\frac{1}{Var(\calW)}(1-\gamma^{2\tau})\bigr)^2+4{\bf e}'I(\mathbb{V}){\bf e}\gamma^{2\tau}\frac{1}{Var(\calW)}}=\Theta\left(1+\frac{\sqrt{N}}{s_N}\right),$$ as stated. 

Finally, we note that if $w_t$ is Gaussian, $\delta\equiv 0$ and $I(\calW)=1/Var(\calW)$ so we have $MSE^{\star}\geq \bar{J}_{\infty}^{-1}$ without any approximation error.\end{proofof}

The corollary below states that the lower bound is not only for the ``batched'' times but holds in stationarity for the period-by-period MSE. In applying Theorem \ref{thm:CRLB}, we take $\tau=N$. 

\begin{corollary}
Let $MSE_t^{\star}$ be the best possible MSE for estimating $X_t$. Then, for $\tau=\calO(\min\{s_N^2,N\})$,
$$ MSE_{\infty}^{\star} \geq \frac{1}{\bar{J}_{\infty}} +\calO\left(\frac{1}{\tau}\right),$$ where $\bar{J}_{\infty}$ is as in Theorem \ref{thm:CRLB}.
    
\end{corollary}
\begin{proof}
In our construction we can take the first batch to be of a different size. Namely, fix $m\in [\tau]$ and let $X_1^B = X_m$, For $l\geq 1$, let $X_l^B= X_{m+(l-1)\tau}$. For each fixed $m$, it holds 
$\bar{J}_{\infty,m}$ is the same as, $\bar{J}_{\infty}$ (which is the special case with $m=\tau$). Thus, as in Theorem \ref{thm:CRLB}, $\lim_{k\uparrow \infty}MSE_{k,m}^{\star}\geq \frac{1}{\bar{J}_{\infty}}+\calO(1/\tau)$. 

Fix a time $t\geq \tau$ and let $m(t),k(t)$, be such that $m(t)+k(t)\tau=t$ for some $k(t)$; in which case $k(t)=t-m(t)$. Then, $MSE_t^{\star}= MSE_{k(t),m(t)}^{\star}
\geq \min_{m\in\tau}MSE_{k(t),m}^{\star}.$ In turn, 
$$\liminf_{t\uparrow \infty} MSE_t^{\star}\geq \lim_{k\uparrow \infty}\min_{m\in[\tau]}MSE^{\star}_{k,m}\geq \frac{1}{\bar{J}_{\infty}}+\calO(1/\tau).$$
\end{proof}




\section{The Goggin filter: Performance upper bound}\label{sec:upperbound}
The transformed observation in time $t$, recall, is 
\begin{align*}Z_{t}&:=s_N \phi \left( \frac{1}{s_N}Y_t\right) = s_N \phi \left( \frac{1}{s_N}X_t + \bar{v}_t\right)\end{align*} 

We apply KF pretending that this observation actually satisfies the linear relation that arises from a Taylor expansion. That is, that the observation is
$$\hY_t = I(\bar v) X_t + s_N\phi(\bar{v}_t).$$

From the viewpoint of applying the KF we are considering the system $(X_t, \hY_t)$ following the dynamics 
\begin{align} %\label{eq:batch1} 
X_t = \gamma X_{t-1} + \frac{1}{\sqrt{N}}\bar{w}_t
\\\hY_t = I(\bar v) X_t + s_N\phi(\bar{v}_t).\label{eq:batch2} 
\end{align} The approximation of $Z_t$ by $\hat{Y}_t$ has implications to both bias and MSE. To study these, let us first spell out the KF estimator update. 
Let $Q=\frac{1}{N}\sigma_{\bar w}^2$ (this is a scalar) and let $R=Var(s_N\phi(\bar v))=s_N^2I(\bar{v})$. Then, the KF $P_t$ obeys the variance update recursion 
\begin{align*}
    P_t & = \left(1-\frac{(\gamma^{2}P_{t-1}+Q)I^2(\bar{v})}{I^2(\bar{v})( \gamma^{2}P_{t-1}+Q)+R)}\right)(\gamma^{2}P_{t-1} + Q)\numberthis \label{eq:VU}   
\end{align*}
 

Notice that this recursion, because of the Taylor expansion, {\bf does not} capture the MSE. It is merely a construct for the sake of applying KF as an algorithm.

The estimator at time $t$ is given --- as a function of the estimator at $t-1$ and the observation $Z_{t}$ --- by 
\begin{align*}\hat{x}_t =& \left(1-(\gamma^{2}P_{t-1}+Q)I^2(\bar{v}){\bf e}'(I^2(\bar{v})( \gamma^{2\tau}P_{t-1}+Q)+R)^{-1}\right)\gamma \hat{x}_{t-1} \\
&+(\gamma^{2}P_{t-1}+Q)I(\bar{v})(I^2(\bar{v})(\gamma^{2}P_{k-1}+Q)+R)^{-1})Z_t.
\end{align*} 
Noticing that $\gamma^{\tau}\Ex[\hat{x}_{t-1}]=\Ex[X_t]+\gamma\Ex[\hat{x}_{t-1}-X_{t-1}]$---if $\Ex[Z_t|X_t]$ was indeed $I(\bar{v})X_t$---we would have that 
$$\Ex[\hat{x}_t-X_t]=\left(1-(\gamma^{2}P_{k-1}+Q)I^2(\bar{v})(I^2(\bar{v})(\gamma^{2}P_{t-1}+Q)+R)^{-1}\right)\gamma\Ex[\hat{x}_{t-1}-x_{t-1}].$$ Thus, recursively, if the initial estimate is (at $t=0$) is unbiased the estimates remain unbiased. In reality, because of the Taylor expansion, we have an extra term. This term we investigate next. 
$$Z_{t}=s_N\phi\left(\frac{1}{s_N}X_t +\bar{v}_t\right)=
s_N\phi(\bar{v}_t)+\Ex[\phi'(\bar{v}_t)]X_t+\mathcal{R}_{t}
$$
where \begin{align*} \mathcal{R}_{t}  &= s_N\phi\left(\frac{1}{s_N}X_t +\bar{v}_t\right) - 
s_N\phi(\bar{v}_t)-\Ex[\phi'(\bar{v}_t)]X_t
\\ & = X_t(\phi'(\bar{v}_t)-\Ex[\phi'(\bar{v}_t)
]) \pm\frac{1}{2}\|\phi''\|_{\infty} \frac{1}{s_N}(X_t)^2 \numberthis\label{eq:R_t}
\end{align*}
Notice that $$\Ex[\calR_t]=\calO\left(\frac{1}{s_N} \Ex[X_t^2]\right).$$ 

\begin{lemma} 
In stationarity, $P_{t-1}=\Theta(s_N/\sqrt{N})$, 
$$K_t = (\gamma^{2}P_{t-1}+Q)I(\bar{v})(\gamma^{2}P_{t-1}+Q+R)^{-1}=\Theta\left(\frac{1}{s_N\sqrt{N}}\right),$$ and for all $t$
$$\Ex[(X_t)^2]=\calO(1).$$\label{lem:KFPXg}
\end{lemma} 

Consider now the coupled equations
\begin{align*} \hat{x}_t& =(1-K_t I(\bar{v}))\gamma \hat{x}_{t-1} +K_tZ_t\\
\tilde{x}_t & = (1-K_t I(\bar{v}))\gamma \tilde{x}_{t-1} +K_t(I(\bar{v})X_t + s_N\phi(\bar{v}_t)).
\end{align*} 

Then, $$\hat{x}_t - \tilde{x}_t = (1-K_t I(\bar{v}))\gamma (\hat{x}_{t-1}-\tilde{x}_{t-1})+K_t\calR_t.$$
so that 
$$\Ex[\hx_t-\tilde{x}_{t}] =  (1-K_tI(\bar{v}))\gamma\Ex[\hx_{t-1}-\tilde{x}_{t-1}] + \calO\left(\frac{1}{s_N^2\sqrt{N}}\right).$$

Let $f(t):=\expec[\hat x_t-\tilde x_t]$. Using Lemma \ref{lem:KFPXg}, we have the following recursion
\begin{align*}
    f(t) = \Ocal\lp \lp1-\frac{I(\bar v)}{s_N\sqrt{N}} \rp f(t-1) + \frac{1}{s_N^2\sqrt{N}}\rp.
\end{align*}
Applying this recursion repeatedly, we get 
\begin{align*}
    f(t) = \Ocal\lp\frac{1}{s_N^2\sqrt{N}}\sum_{i=0}^{t-1}\lp1-\frac{I(\bar v)}{s_N\sqrt{N}} \rp^i\rp+\Ocal(f(0)).
\end{align*}
$f(0)$ can be set to $0$ by initialisation. Completing the summation of the geometric series leads us to 
\begin{align*}
    f(t) = \Ocal\lp\frac{1}{s_N}\rp.
\end{align*}

We conclude that,
$$\Ex[\hat{x}_t-\tilde{x}_t]=\calO(1/s_N).$$  Finally, notice that $\Ex[\tilde{x}_t-X_t]=0$, so that 

\begin{align*}
    \Ex[\hat{x}_t-X_t]=\calO(1/s_N)\numberthis\label{eq:bias-bound}
\end{align*}

We turn to the variance update. Recall that the KF estimator is constructed recursively as 
$$\hx_t = (1-K_t I(\bar{v}))\gamma\hx_{t-1}+K_t Z_t,$$ where 
$$K_t:=(\gamma^{2}P_{t-1}+Q)I^2(\bar{v})(I^2(\bar{v})(\gamma^{2}P_{t-1}+Q)+R)^{-1},\mbox{ and } H = I(\bar{v}).$$
First, using the explicit expression for $Z_t$ and re-arranging terms we have 
\begin{align*} 
X_t - (1-K_tH)\gamma\hx_{t-1} - K_tZ_t & = (1-K_tH) (X_t-\gamma\hx_{t-1})-K_t\calV_t - K_t\calR_t\\ & = 
(1-K_tH)[\gamma(x_{t-1}-\hx_{t-1})+\calW_t] - K_t\calV_t-K_t\calR_t
\end{align*} 
where in the second equality we use $X_t = \gamma X_{t-1}+\calW_t$.

Thus, we have that \begin{align} Var(X_t-\hat{x}_t) & = (1-K_tI(\bar{v}))^2 [\gamma^{2}Var(X_{t-1}-\hx_{t-1}) + Var(\calW_t)] 
+K_t^2 Var(\calV_t)  \nonumber \\& + K_t^2Var(\calR_t) + 2Cov(K_t\calR_t,A_t), \label{eq:decompVar}
\end{align}
where $$A_t:=  
(1-K_tI(\bar{v}))[\gamma Var(X_{t-1}-\hx_{t-1})+\calW_t] - K_t\calV_t.$$
Using $\hat P_t := Var(X_t-\hat x_t) $, $Q_t:= Var(\calW_t)$ and $R_t := Cov(\calV_t)$,  \eqref{eq:decompVar} can be rewritten as
\begin{align*}
    \hat{P}_t = (1-K_tI(\bar{v}))^2 (\gamma^{2} \hat{P}_{t-1}+ Q_t) +K_t^2R_t+ K_t^2Var(\calR_t) + 2Cov(K_t\calR_t,A_t).
\end{align*}
The first two terms in the previous equation correspond to the Joseph form of variance update equation of a standard Kalman-Filter.
The second two terms are the contribution of the approximation to the recursive update. We bound these terms using the following Lemma \ref{lem:remainderterms1} which is proved in Section \ref{sec:prf-remainder}.
\begin{lemma} 
$$K_t^2Var(\calR_t) + 2Cov(K_t\calR_t,A_t) = \calO\left(\frac{1}{s_N^2\sqrt{N}}\right).$$ \label{lem:remainderterms1}
\end{lemma}
We are then left with
\begin{align*}
    \hat{P}_t = (1-K_tI(\bar{v}))^2 (\gamma^{2} \hat{P}_{t-1}+ Q_t) +K_t^2R_t+ \Ocal\lp\frac{1}{s_N^2\sqrt{N}}\rp.
\end{align*}
Comparing this with
\begin{align*}
    P_t = (1-K_tI(\bar{v}))^2 (\gamma^{2} P_{t-1}+ Q_t) +K_t^2R_t
\end{align*}
we observe
\begin{align*}
    P_t-\hat{P}_t = (1-K_tI(\bar{v}))^2(\gamma^{2}(P_{t-1}-\hat{P}_{t-1})) + \calO\lp\frac{1}{\sqrt{N}s_N^2}\rp.
\end{align*}
Recall from Lemma \ref{lem:KFPXg} that, in stationarity, $K_t=\Theta(1/(s_N\sqrt{N}))$. Substituting this bound in the previous equation we have for some positive real number $\eta$
\begin{align*}
     P_t-\hat{P}_t = \gamma^{2}\lp 1-\frac{I(\bar{v})\eta}{s_N\sqrt{N}}\rp^2(P_{t-1}-\hat{P}_{t-1}) + \calO\lp\frac{1}{\sqrt{N}s_N^2}\rp.
\end{align*}

{\color{black} As before,} setting $f(t):= |P_t-\hat{P}_t|$ we then have the relationship 
\begin{align*}
     f(t) \leq \gamma^{2}\lp 1-\frac{I(\bar{v})\eta}{s_N\sqrt{N}}\rp^2 f(t-1) + \calO\lp\frac{1}{\sqrt{N}s_N^2}\rp.
\end{align*}
{
Using the previous recursion repeatedly, we get
\begin{align*}
    f(t) & \leq \Ocal\lp\frac{1}{\sqrt{N}s_N^2}\sum_{i=0}^{t-1}\gamma^{2i}\lp 1-\frac{I(\bar{v})\eta}{s_N\sqrt{N}}\rp^{2i}+ \rp+\Ocal(f(0))\\
    &\leq  \calO\left(\frac{\nicefrac{1}{\sqrt{N}s_N^2}}{1-\gamma^{2}\lp 1-\frac{I(\bar{v})\eta}{s_N\sqrt{N}}\rp^2}\right)
\end{align*}
since $f(0)=0$ due from initialisation.
}
Expanding the denominator, we find that the leading term is $1/s_N\sqrt{N}$. Therefore,  we conclude that, in stationarity, 
\begin{align*}
    |P_t-\hat{P}_t| & = \Ocal\lp\frac{1}{s_N}\rp.
\end{align*}  

Next, we argue that in stationarity $P_t$ achieves the $MSE$ lower bound in Theorem \ref{thm:main}. That is,
\begin{align*}
    P_t\leq \frac{1}{\bar{J}_{\infty}}+\calO\left(\frac{1}{s_N}\right)\leq MSE^{\star}+\calO\left(\frac{1}{s_N}\right).
\end{align*}
To that end, let us re-write the equation for $P_t$ in its its standard form 
$$P_t = \frac{R_t(\gamma^2P_{t-1}+Q_t)}{I(\bar{v})^2 (\gamma^2 P_{t-1}+Q_t)+R_t  }.$$
$\JGF_t= 1/P_t$ is then given by 
\begin{align*}
    \JGF_{t+1} = \frac{1}{s_N^2}I(\bar v) + \frac{\JGF_{t}N/\sigma_{\bar w}^2}{\JGF_{t} + \gamma^2N/\sigma_{\bar w}^2}.
\end{align*}
Its stationary value is the solution to the equation 
$$ %\label{eq:JGF}
(\JGF)^2 -\left((1-\gamma^{2})\frac{N}{\sigma_{\bar w}^2}+\frac{1}{s_N^2}I(\bar{v})\right)\JGF -\frac{\gamma^2 N}{\sigma_{\bar w}^2s_N^2}I(\bar v)=0,$$
which has the positive solution 
\[ %\label{eq:JGF} 
\JGF =\frac{1}{2}\lb\left((1-\gamma^{2})\frac{N}{\sigma_{\bar w}^2}+\frac{1}{s_N^2}I(\bar{v})\right)+\sqrt{\left(1-\gamma^{2})\frac{N}{\sigma_{\bar w}^2}+\frac{1}{s_N^2}I(\bar{v})\right)^2 + 4\frac{\gamma^2 N}{\sigma_{\bar w}^2s_N^2}}\rb
\]

We compare the stationary value for this equation with that for $\bar{J}$ given by \begin{align*}
\bar{J}_{k+1} = {\bf e}'I(\mathbb{V}_k){\bf e} + \frac{\bar{J}_k\frac{1}{Var(\calW_k)}}{\bar{J_k}+\gamma^{2\tau}\frac{1}{Var(\calW_k)} },%\numberthis\label{eq:approximate-recursion}
\end{align*} 
and establish that 
\[
|\bar{J}-\JGF| = \calO\left(\frac{1}{\tau}\right).\label{eq:Jgap}
\] In turn $$P_t-\frac{1}{\bar{J}}=\calO\left(\frac{1}{\tau}\right),$$ which, with $\tau=s_N$ proves the upper bound. 

The steady-state value $\bar{J}$ of $\bar{J}_k$ solves the quadratic equation (below $\Sigma^{-1} = 1/Var(\calW)$ and $\bar{I} = {\bf e}'I(\mathbb{V}){\bf e}$) 
$$\bar{J}^2 -\left((1-\gamma^{2\tau})\Sigma^{-1}+\bar{I}\right)\bar{J}-\gamma^{2\tau} \Sigma^{-1}\bar{I} = 0,$$ which has the solution 
$$\bar{J} = \frac{1}{2}\left[(1-\gamma^{2\tau})\Sigma^{-1}+\bar{I} + \sqrt{ ((1-\gamma^{2\tau})\Sigma^{-1}+\bar{I})^2 + 4 \gamma^{2\tau} \Sigma^{-1}\bar{I}}\right],$$ so that
$$|\bar{J}-\JGF| \leq A + B,$$
where
\begin{align*}
  A & = \frac{1}{2}\left|(1-\gamma^{2\tau})\Sigma^{-1}+\bar{I} - (1-\gamma^2)\frac{N}{\sigma_{\bar w}^2} -\frac{1}{s_N^2}I(\bar v)\right| \text{ and, }\\
  B & = \frac{1}{2}\left|\underbrace{\sqrt{ ((1-\gamma^{2\tau})\Sigma^{-1}+\bar{I})^2 + 4 \gamma^{2\tau} \Sigma^{-1}\bar{I}}}_{B_1}-\underbrace{\sqrt{ \left((1-\gamma^{2})\frac{N}{\sigma_{\bar w}^2} +\frac{1}{s_N^2}I(\bar v)\right)^2 + 4 \gamma^{2}\frac{N}{\sigma_{\bar w}^2s_N}I(\bar v)}}_{B_2}\right|.
\end{align*}

Noting  that $(1-\gamma^{2\tau}\Sigma^{-1})= (1-\gamma^2)\frac{N}{\sigma_{\bar w}^2}$ and $\bar{I} = \Theta(\frac{\tau}{s_N^2})$ so that
\begin{align*}
  A & = \calO\lp\frac{\tau}{s_N^2}\rp=\calO\lp \frac{1}{s_N}\rp, \text{ and } B = \frac{1}{2}\left|\frac{B_1^2-B_2^2}{B_1+B_2}\right|.
\end{align*}
Recalling that $(1-\gamma^{2\tau}\Sigma^{-1})=(1-\gamma^2)N/\sigma_{\bar w}^2,$ and that $\bar I=\Theta(\tau/s_N^2)$ we have, for $\tau=s_N$ that 
$$\left|((1-\gamma^{2\tau})\Sigma^{-1}+\bar{I})^2- ((1-\gamma^{2})\frac{N}{\sigma_{\bar w}^2} \frac{1}{s_N^2}I(\bar v))^2\right|=\calO\lp\frac{1}{s_N}\rp.$$
Similarly, 
$$\left|4\gamma^{2\tau}\Sigma^{-1}\bar{I} - 4\gamma^2\frac{N}{s_N^2\sigma_{\bar w}^2}I(\bar v)\right|=\calO\lp \frac{\tau}{s_N^2}\rp=\calO\lp\frac{1}{s_N}\rp.$$
Thus, we have that 
$$B = \Theta\lp \frac{1/s_N}{\sqrt{N}/s_N}\rp = \calO\lp \frac{1}{\sqrt{N}}\rp=\calO\lp\frac{1}{s_N}\rp.$$



\vspace{2 pt}
\iffalse
Recall that $Q_t:= Var(\calW_t)$ and $R_t = Cov(\calV_t)$. We drop the subscript in $t$ for convenience. The first row \eqref{eq:decompVar} is the standard KF update (the so-called Joseph form)
$P_t= (1-K_tH)^2 (\gamma^{2} P_{k-1}+ Q) +K_tCov(R)K_t^T.$ The terms in the second row are the contribution of the approximation to the recursive update. These are the terms we bound next. We first write Lemma \ref{lem:remainderterms1} which is proved in Section \ref{sec:prf-remainder}


Letting $\hat{P}_t=Var(X_t-\hx_t)$ we have 
$$\hat{P}_t = (1-K_tH)^2 (\gamma^{2} \hat{P}_{t-1}+ Q) +K_t^2Var(R) + \calO(\frac{1}{\sqrt{N}s_N^2}).$$ 

Let us compare this to $P_t$ satisfying the equation 
$$P_t = (1-K_tH)^2 (\gamma^{2} P_{t-1}+ Q) +K_t^2Var(R).$$ Then, 
$$P_t-\hat{P}_t = (1-K_tH)^2(\gamma^{2}(P_{t-1}-\hat{P}_{t-1})) + \calO(1/(\sqrt{N}s_N^2)).$$ Using the fact that $K_t=\Theta(1/(s_N\sqrt{N}))$
 
$$|P_t-\hat{P}_t| \leq \gamma^{2}\left(1-\eta\frac{1}{\sqrt{N}s_N}\right)|P_{t-1}-\hat{P}_{t-1}| + \calO(1/(\sqrt{N} s_N^2)).$$ We then have that, in stationarity, 
$$|P_t-\hat{P}_t|=\calO(1/s_N).$$

Finally, we argue that $P_t$ achieves, in stationarity, the $MSE$ lower bound in Theorem \ref{thm:main}. That is, that 
$$P_t\leq \frac{1}{\bar{J}_{\infty}}+\calO\left(\frac{1}{s_N}\right)\leq MSE^{\star}+\calO\left(\frac{1}{s_N}\right).$$ 
To that end, let us re-write the equation for $P_t$ in its its standard form 
$$P_t = \frac{R(\gamma^2P_{t-1}+Q)}{H^2 (\gamma^2 P_{t-1}+Q)+R  }.$$
$\JGF_t= 1/P_t$ is then given by (spelling out the terms like $H=I(\bar{v})$) 
$$\JGF_{t+1} = \frac{1}{s_N^2}I(\bar v) + \frac{\JGF_{t}N/\sigma_{\bar w}^2}{\JGF_{t} + \gamma^2N/\sigma_{\bar w}^2}$$

Its stationary value is the solution to the equation 
$$ %\label{eq:JGF}
(\JGF)^2 -\left((1-\gamma^{2})\frac{N}{\sigma_{\bar w}^2}+\frac{1}{s_N^2}I(\bar{v})\right)\JGF -\frac{\gamma^2 N}{\sigma_{\bar w}^2s_N^2}I(\bar v)=0,$$
which has the positive solution 
\[
%\label{eq:JGF} 
\JGF =\frac{1}{2}\lb\left((1-\gamma^{2})\frac{N}{\sigma_{\bar w}^2}+\frac{1}{s_N^2}I(\bar{v})\right)+\sqrt{\left(1-\gamma^{2})\frac{N}{\sigma_{\bar w}^2}+\frac{1}{s_N^2}I(\bar{v})\right)^2 + 4\frac{\gamma^2 N}{\sigma_{\bar w}^2s_N^2}}\rb
\]

We will compare the stationary value for this equation with that for $\bar{J}$ given by 


\begin{align*}
\bar{J}_{k+1} = {\bf e}'I(\mathbb{V}_k){\bf e} + \frac{\bar{J}_k\frac{1}{Var(\calW_k)}}{\bar{J_k}+\gamma^{2\tau}\frac{1}{Var(\calW_k)} }.
\end{align*} 


We will show that 
\be |\bar{J}-\JGF| = \calO\left(\frac{1}{\tau}\right)\label{eq:Jgap}\ee and in turn $$P_t-\frac{1}{\bar{J}}=\calO\left(\frac{1}{\tau}\right),$$ which, with $\tau=s_N$ proves the upper bound. 

The steady-state value $\bar{J}$ of $\bar{J}_k$ solves the quadratic equation (below $\Sigma^{-1} = 1/Var(\calW)$ and $\bar{I} = {\bf e}'I(\mathbb{V}){\bf e}$) 
$$\bar{J}^2 -\left((1-\gamma^{2\tau})\Sigma^{-1}+\bar{I}\right)\bar{J}-\gamma^{2\tau} \Sigma^{-1}\bar{I} = 0.$$ 

This has the solution 

$$\bar{J} = \frac{1}{2}\left[(1-\gamma^{2\tau})\Sigma^{-1}+\bar{I} + \sqrt{ ((1-\gamma^{2\tau})\Sigma^{-1}+\bar{I})^2 + 4 \gamma^{2\tau} \Sigma^{-1}\bar{I}}\right]$$

$$|\bar{J}-\JGF| \leq A + B$$

where $$A =\frac{1}{2}\left|(1-\gamma^{2\tau})\Sigma^{-1}+\bar{I} - (1-\gamma^2)\frac{N}{\sigma_{\bar w}^2} -\frac{1}{s_N^2}I(\bar v)\right|$$

$$B = \frac{1}{2}\left|\underbrace{\sqrt{ ((1-\gamma^{2\tau})\Sigma^{-1}+\bar{I})^2 + 4 \gamma^{2\tau} \Sigma^{-1}\bar{I}}}_{B_1}-\underbrace{\sqrt{ \left((1-\gamma^{2})\frac{N}{\sigma_{\bar w}^2} +\frac{1}{s_N^2}I(\bar v)\right)^2 + 4 \gamma^{2}\frac{N}{\sigma_{\bar w}^2s_N}I(\bar v)}}_{B_2}\right|$$

Note now that $(1-\gamma^{2\tau}\Sigma^{-1} = (1-\gamma^2)\frac{N}{\sigma_{\bar w}^2}$ and $\bar{I} = \Theta(\frac{\tau}{s_N^2})$ so that $$A=\calO\lp\frac{\tau}{s_N^2}\rp=\calO\lp \frac{1}{s_N}\rp.$$



$$B=\frac{1}{2}\left|\frac{B_1^2-B_2^2}{B_1+B_2}\right|$$

Recalling that $(1-\gamma^{2\tau}\Sigma^{-1}=(1-\gamma^2)N/\sigma_{\bar w}^2,$ and that $\bar I=\Theta(\tau/s_N^2)$ we have, for $\tau=s_N$ that 
$$\left|((1-\gamma^{2\tau})\Sigma^{-1}+\bar{I})^2- ((1-\gamma^{2})\frac{N}{\sigma_{\bar w}^2} \frac{1}{s_N^2}I(\bar v))^2\right|=\calO\lp\frac{1}{s_N}\rp.$$
Similarly, 
$$\left|4\gamma^{2\tau}\Sigma^{-1}\bar{I} - 4\gamma^2\frac{N}{s_N^2\sigma_{\bar w}^2}I(\bar v)\right|=\calO\lp \frac{\tau}{s_N^2}\rp=\calO\lp\frac{1}{s_N}\rp.$$

Thus, we have that 
$$B = \Theta\lp \frac{1/s_N}{\sqrt{N}/s_N}\rp = \calO\lp \frac{1}{\sqrt{N}}\rp=\calO\lp\frac{1}{s_N}\rp.$$
\eProof 

\fi
\vspace{2 pt}



\section{Concluding Remarks} 

In this paper we re-visited the development of simple filters for the non-Gaussian state space model. For these, we mapped filtering regimes,  identifying and focusing attention on a {\em balanced regime}. In this balanced regime, we characterized the sub-optimality rates---as an explicit function of the observation noise---of the Goggin filter which is one where the observation are transformed by the score function of the observation noise. 

Our paper complements and renews attention to earlier literature on the non-Gaussian linear state space model and, of course, is not the last word. An important open problem remains the boundary (between parameter families) where the linearized Goggin filter no-longer offers a satisfying approximation; recall Remark \ref{rem:N14}. It remains an interesting question whether there are simple corrections to the filter that would expand coverage to a lower-noise ($\leq N^{1/4}$) regime. 

Goggin's most general results allow for some non-linear systems. For these, his result does not suggest a concrete filter. Instead, it shows that the optimal filter (which is the conditional expectation of the state given the expectations) converges to the conditional expectation in a suitably defined filtering problem for a non-linear diffusion process. To the extent that tractable filters exist for that diffusion filtering problem, it is plausible that our analysis can extend to establish sub-optimality gaps for some non-linear problems. 

In going forward, key is the fact that our analysis relies on a Cramér-Rao lower bound and central limit theorems for the Fisher information. This approach, which does not rely on limit theorems, maybe useful in expanding the coverage from estimation to control, namely into POMDPs. The most accessible among these with this infrastructure is the quadratic regulator problem. In the Gaussian case, the solution exhibits a so-called separation principle  has where the filtering component is the Kalman Filter. The results and analysis in the current paper might be useful for studying the effect of non-Gaussianity and how it interacts with both the stochastic primitives and the cost parameters. 

% \section*{Acknowledgement}

\section*{Acknowledgements}

The authors are grateful to Vikram Krishnamurthy for his help during the development of this manuscript. Imon Banerjee acknowledges IEMS Alumni Fellowship at Northwestern University for funding during the period at which this research was conducted.

\appendices

\section{Proofs of Lemmas \ref{lem:toonoisy}, \ref{lem:nonoise}, \ref{lem:balanced}, \ref{lem:KF}}
  \begin{proofof}{Lemma \ref{lem:toonoisy}} First, we claim that, in stationarity, $MSE^{\star}\geq 1/J$ where  
  $$J:=\frac{1}{2}\left((R+(1-\gamma^2)Q) + \sqrt{(R+(1-\gamma^2)Q)^2+4\gamma^2 QR}\right),$$ where $Q := I(w)=I(\bar w)/\sigma_{ w}^2$ and $R:=I(v)=I(\bar v)/\sigma_{v}^2$. 
  
By the assumption of the lemma $R= o(Q(1-\gamma^2))$ and $QR=o(Q(1-\gamma^2))$ so that
$$\sqrt{(R+(1-\gamma^2)Q)^2+4\gamma^2 QR}\leq R+(1-\gamma^2)Q+ 2\gamma \sqrt{QR} = (1-\gamma^2)Q+o((1-\gamma^2)Q).$$ Thus, 
$$J\leq (1-\gamma^2)Q +o((1-\gamma^2)Q),$$
so that 
$$MSE^{\star}\geq \frac{1/Q}{1-\gamma^2} + o\left(\frac{1/Q}{1-\gamma^2}\right).$$ 
But, recall, $1/Q=1/I(w)< \sigma_w^2$ with equality only in the Gaussian case. 

To establish the result as stated, we resort to a central-limit-theorem argument. We use the notation of our scaling where $\sigma_w^2=(1/N)\sigma_{\bar w}^2$ and $(1-\gamma)=1/N$. In Theorem \ref{thm:CRLB} set $\tau=N$. From there it follows that 
$$MSE^{\star} \geq \frac{1}{\bar{J}_{\infty}} + \calO\lp\frac{1}{\tau}\rp.$$ 
It remains then only to prove that $\bar{J}_{\infty}$ converge to $\sigma_{\bar w}^2/2 = \frac{\sigma_w^2}{1-\gamma^2} + o\left(
\frac{\sigma_w^2}{1-\gamma^2}\right).
.$
To see this recall that 
\begin{align*}
    \bar J_{\infty} & = \frac{{\bf e}'I(\mathbb{V}){\bf e}+\frac{1}{Var(\calW)}(1-\gamma^{2\tau})+\sqrt{\bigl({\bf e}'I(\mathbb{V}){\bf e}+\frac{1}{Var(\calW)}(1-\gamma^{2\tau})\bigr)^2+4{\bf e}'I(\mathbb{V}){\bf e}\gamma^{2\tau}\frac{1}{Var(\calW)}}}{2}
\end{align*}
With $\tau=N$, we have 
$$\frac{1}{Var(\calW)}(1-\gamma^{2\tau}) = (1-\gamma^2)\frac{N}{\sigma_{\bar w}^2} = \frac{1-\gamma^2}{\sigma_w^2}=\frac{1-\gamma^2}{\Var(w_t)}.$$
Recalling that, in the setting of this lemma, $\frac{1}{s_N^2}\sigma_{\bar v}^2 = \Theta((1-\gamma^2)\sigma_v^2) \gg \sigma_w^2/(1-\gamma^2) =\Theta(1) 
$ so that $s_N\gg \sqrt{N}$ we have with $\tau=N$ that $${\bf e}I(\mathbb{V}){\bf e}=\Theta\lp \frac{\tau}{s_N^2}\rp= o(1).$$ In particular, both $$\frac{{\bf e}'I(\mathbb{V}){\bf e}}{(1-\gamma^2)/Var(w_t)}=o(1),\mbox{ and } \frac{4{\bf e}'I(\mathbb{V}){\bf e}\gamma^{2\tau}\frac{1}{Var(\calW)}}{(1-\gamma^2)/Var(w_t)}=o(1)$$ In the last equality we used the fact that $(1-\gamma^2)/Var(w_t)=\Theta(1)$ and $1/Var(\calW) = \Theta(N/\tau)=\Theta(1)$ for $\tau=N$. Thus, 

$$\frac{\bar J}{(1-\gamma^2)/Var(w_t)}=\frac{o(1)+1 + \sqrt{(1+o(1))^2 + o(1)} }{2}=1+o(1),$$ and we may conclude that 
$$\bar{J} = \frac{Var(w_t)}{1-\gamma^2} + o\lp \frac{Var(w_t)}{1-\gamma^2}\rp,$$ as required. 

\end{proofof}


  \begin{proofof}{Lemma \ref{lem:nonoise}} Dividing $J/R$ we get 
  $$\frac{J}{R} = \frac{1}{2}\lp 1 + (1-\gamma^2)\frac{Q}{R} + \sqrt{\lp1+(1-\gamma^2)\frac{Q}{R}\rp^2+4\gamma^2\frac{Q}{R}}\rp.$$ Under the assumptions of this lemma $Q=o(R)$, so that $J/R = 1+o(1)$ and, in turn, $J=R+o(R)$. Recalling that $R=I(\bar v)/\sigma_{\bar v}^2$ we have that 
  $MSE^{\star}\geq \sigma_{v}^2I^{-1}(\bar v)+ o(\sigma_{v}^2).$
  
To prove the stronger result, that $MSE^{\star}\geq \sigma_{v}^2 + o(\sigma_v^2),$ we need a different argument, the key in which is the following lemma that we prove in Section \ref{sec:prf-lemlecam}. 

\begin{lemma}\label{lem:lecamapp} Suppose we have an observation of a parameter $\theta\in\mathbb{R}$:
 $$Z = \theta + \zeta_1, $$ where $\zeta_1=\sigma_1\bar{\zeta}$ is a real valued random variable. Suppose further that $\sigma_1>0$ and $\bar{\zeta}$ is some zero mean, unit variance continuous valued random variable with finite fourth moment. We also assume that the prior $\pi(\theta)$ is Lipschitz continuous and has $\pi(0)>0$. Then, for any estimator $\hat{\theta}$ of $\theta$, as $\sigma_1^2\downarrow 0$, 
$$\Ex[(\hat{\theta}-\theta)^2]\geq \sigma_1^2 +o(\sigma_1^2), $$ where, the expectation is over the joint distribution of the prior and the observation.
\end{lemma} 
\vspace*{0.2cm} 
  Equipped with this lemma, let us consider the linear system 
\begin{align*} X_{t+1}&=\gamma X_t+\sigma_w\bar{w}_t, \\
Y_{t} & = X_t + \sigma_v \bar{v}_t\end{align*} 
Let us scale this system by dividing by $\sigma_w$, then we have the system (with some abuse of notation we keep $X$ and $Y$)
\begin{align*} X_{t+1}&=\gamma X_t+\bar{w}_t, \\
Y_{t} & = X_t + \frac{\sigma_v}{\sigma_w} \bar{v}_t\end{align*} 
Recall that, by assumption,  $\bar{w}_t$ is Lipschitz continuous and, let $L_{\bar w}$ be the Lipschitz constant. 

{
Given a fixed (arbitrary) filter, fix a time $t-1$, and let $\hat{X}_{t-1}$ be the estimate of $X_{t-1}$ at time $t-1$. 
}
Then, the prior at time $t$ (before the observation at time $t$) is given by by the convolution of the independent variables
$\gamma \hat{X}_{t-1}$ and $\bar{w}_{t-1}$. That is
$$\hat{X}_{t|t-1} = \gamma\hat{X}_{t-1} +\bar{w}_{t-1}.$$

We claim that this convolution inhereits the Lipschitz constant and bound on the density from $\bar{w}_t$. Indeed, take two continuous random variables $C_1,C_2$ such that $C_2$ has a Lipschitz continuous and bounded density  $f_{2}$. Then, 
$$f_{1+2}(c) = \int_{c_1}f_2(c-c_1)f_1(c_1)dc_1\leq |f_2|_{\infty}\int_{c_1}f_1{c_1}dc_1=|f_2|_{\infty}.$$

Similarly, 
\begin{align*} |f_{1+2}(c)-f_{1+2}(c)|& =\left| \int_{c_1}f_2(c-c_1)f_1(c_1)dc_1-\int_{c_1}f_2(c'-c_1)f_1(c_1)dc_1\right|
\\ & \leq   \int_{c_1}|f_2(c-c_1)-f_2(c'-c_1)|f_1(c_1)dc_1\leq L_2|c-c'|,
\end{align*}  where $L_2$ is the Lipschitz constant of $f_2$. 

Notice that continuity of $C_1$ is not needed above. The argument can be repeated almost identically if $C_1$ is a discrete (or mixed) random variable. 

In order to apply Lemma \ref{lem:lecamapp}, we next verify that the density of $\hat X_{t|t-1}$ is positive at $0$. 
Following the line of arguments before, let $f_1$ and $f_2$ be both positive at $0$. Just for simplicity, we also assume that they are continuous. Then $\exists \ \epsilon>0$ such that $f_1(x)$ and $f_2(x)$ are positive for all $x\in(-\epsilon,\epsilon)$. We get 
\begin{align*}
    f_{1+2}(0) = \int_{c_1}f_2(-c_1)f_1(c_1)dc_1>\int_{-\epsilon}^\epsilon f_2(-c_1)f_1(c_1)dc_1>0.
\end{align*}

We conclude that the distribution $\hat{X}_{t|t-1}$ inherits the boundedness, positivity at $0$, and Lipschitz continuity of the density from $\bar{w}_t$.

With the observation at time $t$ being $Y_t=X_t+\sigma_v \bar{v}_t$, we are in the position to apply Lemma \ref{lem:lecamapp}. This concludes the proof. 

\end{proofof} 




\begin{proofof}{Lemma \ref{lem:balanced}} Notice that for the KF, the information $J=1/P$ follows the same solution as in the previous lemmas but with $Q=1/\sigma_{w}^2$ and $R=1/\sigma_v^2$. 
$$J\geq \frac{1}{2}\lp (1-\gamma^2)Q + \sqrt{((1-\gamma^2)Q)^2 + 4\gamma^2 RQ}\rp. $$ so that
$$\frac{J}{(1-\gamma^2)Q} \geq 1+ \sqrt{1+ \frac{4\gamma^2 R}{(1-\gamma^2)^2Q}}$$

By the assumptions of the lemma 
$R/(1-\gamma^2)=\Omega(Q(1-\gamma^2))$ and, in turn, $R/Q=\Omega((1-\gamma^2)^2)$ and we conclude that 
$$\frac{J}{(1-\gamma^2)Q} \geq \frac{1}{2}\left(1+ \sqrt{1+\Omega(1)}\right) = 1+\Theta(1).$$
Therefore, there exists $\delta>0$ such that $J\geq (1+\delta)Q(1-\gamma^2)$ so that for some $\epsilon>0$ 
$$P\leq (1-\epsilon)\frac{\sigma_w^2}{1-\gamma^2},$$
or, equivalently, 
$$\frac{\sigma_w^2}{1-\gamma^2} - MSE^{KF}=\Omega\lp \frac{\sigma_w^2}{1-\gamma^2}\rp.$$ 
The argument is almost identical for the other bound by noting that 
$$\frac{J}{R}\geq \frac{1}{2}\lp 1 + \sqrt{1 + 4\gamma^2 \frac{Q}{R}}\rp. $$
Because, by assumption $\sigma_w^2 =\calO(\sigma_v^2)$ we have that 
$Q/R = \sigma_v^2/\sigma_w^2=\Omega(1)$ and conclude as before that $J\geq (1+\delta)R$ so that $P\leq (1-\epsilon)\sigma_v^2$ and 
$$\sigma_v^2 -MSE^{KF} =\Omega(\sigma_v^2).$$
\end{proofof}




\noindent \begin{proofof}{Lemma \ref{lem:KF}} Let $P_t^{KF}$ be the MSE of the KF after $t$ steps. Then, $J_t^{KF}$ satisfies the evolutions 

$$J_t^{KF} = \frac{1}{\sigma_{\bar v}^2 s_N^2} + \frac{J_t^{KF}/Q}{J_t^{KF}+\gamma^2/Q}.$$

Our lower bound $\bar{J}_t$ satisfies 

$$\bar{J}_t = \frac{I(\bar{v})}{s_N^2} +  \frac{\bar{J}_t/Q}{\bar{J}_t+\gamma^2/Q}$$

Among random variables with the same variance, the Fisher information is minimized by the Gaussian with the given variance \cite{johnson2004information}. In other words, for a random variable $\bar v$ with variance $\sigma_{\bar v}^2$, $I(\bar{v}) \geq \frac{1}{\sigma_{\bar v}^2}$ and the equality is achieved only with the Gaussian distribution. In turn, if $\bar{v}$ is non-gaussian we can define a strictly positive the number (and not dependent on N) $$\eta = I(\bar{v})-\frac{1}{\sigma_{\bar v}^2}>0.$$

Let us consider both equations in steady-state and compare their solutions. 

$$(J^{KF})^2-J^{KF}\left((1-\gamma^2)/Q+\frac{a}{s_N^2}\right)-\frac{\gamma^2}{Q}\frac{a}{s_N^2}=0$$ $$(\bar{J})^2-\bar{J}\left((1-\gamma^2)/Q+\frac{b}{s_N^2}\right)-\frac{\gamma^2}{Q} \frac{b}{s_N^2} =0$$ 
where $a = \frac{1}{\sigma_{\bar v}^2}< b= I(\bar{v})$.

Recalling that $Q=1/N\sigma_{\bar w}^2$ and that $1-\gamma^2 = 2/N + o(2/N)$, we have that both $(1-\gamma^2)/Q + a/s_N^2 =\Theta(1)$ and $(1-\gamma^2)/Q + b/s_N^2 = \Theta(1)$, as well as both $\frac{\gamma^2}{Q}a/s_N^2 = \Omega(N/s_N^2)=\Omega(1),\frac{\gamma^2}{Q}b/s_N^2=\Omega(1)$ for all $s_N\leq \sqrt{N}$. Solving the quadratic equation and taking a Taylor expansion of the square-root term around $((1-\gamma^2)/Q)^2 + 4\frac{\gamma^2}{Q}\frac{a}{s_N^2}$  gives 

\begin{align*} J^{KF} & = \frac{1}{2}\left((1-\gamma^2)/Q + \sqrt{((1-\gamma^2)/Q)^2 + 4\frac{\gamma^2}{Q}\frac{a}{s_N^2}}\right) + o\left(\sqrt{\frac{\gamma^2}{Q} \frac{a}{s_N^2}}\right), 
\\\bar{J}&= \frac{1}{2}\left((1-\gamma^2)/Q + \sqrt{((1-\gamma^2)/Q)^2 + 4\frac{\gamma^2}{Q}\frac{b}{s_N^2}}\right)+ o\left(\sqrt{\frac{\gamma^2}{Q} \frac{b}{s_N^2}}\right)\end{align*}

In turn, recalling that $\eta = b-a >0$, 
$$\bar{J}-J^{KF} = \Omega \left(\frac{\sqrt{N}}{s_N}\right),$$
so that $$\frac{1}{J^{KF}}-\frac{1}{\bar{J}}=\Omega\left(\frac{s_N}{\sqrt{N}}\right),$$ as stated. \end{proofof} 
\section{Proofs of Auxillary Lemmas}
\subsection{Proof of Lemma \ref{lem:CLT_gn}}~\label{sec:prf-lemcltgn}
\begin{proofof}{Lemma \ref{lem:CLT_gn}}
We first recall from \cref{eq:batch1} that 
\[
\Wcal_k = \frac{1}{\sqrt{N}}\sum_{t\in [\tau]}\gamma^{t-1} \bar w_{\tau k-t} = \frac{1}{\sqrt{N}}\sum_{t\in [\tau]}\gamma^{t-1}\bar w_{\tau k-t}.
\] 
Thus,
\begin{align*}
   \Var(\Wcal_k)  = \frac{1}{N}\sum_{t\in [\tau]}\gamma^{2(t-1)} \sigma_{\bar w}^2 = \frac{1-\gamma^{2\tau}}{1-\gamma^2}\sigma_{\bar w}^2= \frac{\tau}{N}\sigma_{\bar w}^2+\mathcal{O}(\tau/N^{2})\numberthis\label{eq:cltgnprf-eq2}.
\end{align*}

The standardized Fisher information for a random variable $X$ is given by 
$$J_{st}(X) = Var(X)I(X)-1.$$



\begin{lemma}[Proposition 3.2 in \cite{johnson2004information}]\label{prop:johnson_inforbd}
    Let $X_1\dots,X_n$ be independent, $0$ mean random variables with variances $\sigma_1^2,\dots,\sigma_n^2$ respectively. Then, the standardised Fisher information $J_{st}$ satisfies
    \[
    J_{st}\lp\frac{X_1+\dots+X_n}{\sqrt{n}}\rp\leq \sum_{i=1}^n \frac{\alpha_i}{1+c_i} J_{st}(X_i)
    \]
    where
    \[
    \alpha_i=\sigma_i^2/(\sum \sigma_i^2) \text{ and } c_i = \frac{1}{2R^\star (X_i)}\sum_{j\neq i}\frac{1}{I(X_j)}.
    \]
\end{lemma}
We substitute  $n=\tau$, $X_i = \gamma^{i-1}\bar{w}_{\tau k -i}$ in Lemma \ref{prop:johnson_inforbd}. 
\begin{align*}
    \alpha_i & = \gamma^{2(i-1)}/(\sum_{i\in[\tau]}\gamma^{2(i-1)})= \frac{\gamma^{2(i-1)}(1-\gamma^2)}{1-\gamma^{2\tau}}\leq 2/\tau \\
    c_i & = \frac{1}{2R^\star (\gamma^{i-1}\bar{w}_{\tau k -i})}\sum_{j\neq i}\frac{1}{I(\gamma^{j-1}\bar{w}_{\tau k -j})}\\ 
    &= \frac{1}{2\gamma^{2(i-1)}R^\star (\bar{w}_{\tau k -i})}\sum_{j\neq i}\frac{\gamma^{2(j-1)}}{I(\bar{w}_{\tau k -j})}\\
    & =  \frac{1}{2\gamma^{2i}R^\star (\bar{w}_1)}\sum_{j\neq i}\frac{\gamma^{2j}}{I(\bar{w}_1)}, 
\end{align*}
where the second-to-last equality follows since $R^\star(\gamma^t \bar{w})=\gamma^{2t}R^\star(\bar{w})$ by \cite[Theorem 2, item v]{borovkov1984inequality}.
Now we have, 
\begin{align*}
     \frac{1}{2\gamma^{2(i-1)}R^\star (\bar{w}_1)}\sum_{j\neq i}\frac{\gamma^{2(j-1)}}{I(\bar{w}_1)} = \frac{1}{2R^\star (\bar{w}_1)I(\bar{w}_1)}\lp \underbrace{\frac{1-\gamma^{2\tau}}{\gamma^{2(i-1)}(1-\gamma^2)}}_{=:\text{Term}}-1 \rp.\numberthis\label{eq:prfcltgn-eq1}
\end{align*}

Recall that $\gamma=1-1/N$ and notice that $\gamma^{2\tau} = 1-2\tau/N + o(\tau^2/N^2) = 1-2\tau/N + o(1/N)$ and similarly $\gamma^2 = 1-2/N + o(1/N)$. Together with $\gamma^{2i}\leq 1$, we have for all $N$ large because $\tau\gg 1$ that 
$$\frac{1-\gamma^{2\tau}}{\gamma^{2i}(1-\gamma^2)}-1=\frac{\tau/N + o(1/N)}{1/N +o(1/N)}-1 \geq \frac{1}{2}\tau.$$ 

For future use, we we note that this conclusion holds also if $\tau=N$. In that case, because $\gamma^{2\tau}=e^{-2}+o(1)$ so that 
$$\frac{1-\gamma^{2\tau}}{\gamma^{2(i-1)}(1-\gamma^2)}-1=\Theta(N)=\Theta(\tau). $$


Overall, we have that \begin{align*}
    \frac{\alpha_i}{1+c_i} & \leq \frac{2}{\tau} \frac{1}{1+\frac{1}{4 R^\star (\bar{w}_1)I(\bar{w}_1)}\tau }\leq \frac{8R^\star (\bar{w}_1)I(\bar{w}_1)}{\tau^2}
\end{align*}
As before, we note that $J_{st}(\gamma^{i-1}\bar{w}_{\tau k -i})=J_{st}(\bar{w}_{\tau k -i})=\sigma_{\bar w}^2I(\bar w_1)-1$. Therefore,
\begin{align*}
    \sum_{i\in [\tau]}\frac{\alpha_1}{1+c_i}J_{st}(\gamma^i\bar{w}_{\tau k -i-1})\leq \frac{8R^\star (\bar{w}_1)I(\bar{w}_1)}{\tau^2}=\Ocal(1/\tau),
\end{align*}
and we get $I(\Wcal)\leq \nicefrac{1}{\Var(\Wcal)}+\Ocal(1/\tau)$, which completes the proof.
\end{proofof}

\subsection{Proof of Lemma \ref{lem:KFPXg}.}\label{sec:prf-KFPXg}
\begin{proofof}{Lemma \ref{lem:KFPXg}}

Recall from \cref{eq:VU} that with $Q=\frac{1}{N}\sigma_{\bar w}^2$ and $R=s_N^2I(\bar{v})$
\begin{align*}
    P_t & = \left(1-\frac{(\gamma^{2}P_{t-1}+Q)I^2(\bar{v})}{I^2(\bar{v})( \gamma^{2}P_{t-1}+Q)+R)}\right)(\gamma^{2}P_{t-1} + Q)\\
    & = \frac{(\gamma^2P_{t-1}+Q)R}{R+I^2(\bar v)(\gamma^2P_{t-1}+Q)}.
\end{align*}
In stationarity this admits the steady-state solution 
\begin{align*}
    P & = \frac{(\gamma^2P+Q)R}{R+I^2(\bar v)(\gamma^2P+Q)}.
\end{align*}
To solve this quadratic equation, observe that
\begin{align*}
    x=\frac{ax+b}{c+dx}\ \text{ admits the positive solution }\ x & = \frac{\sqrt{(c-a)^2+4bd}-(c-a)}{2d}\\
    & = \frac{2b}{\sqrt{(c-a)^2+4bd}+(c-a)}.
\end{align*}
We substitute $a=\gamma^2$, $b=Q$, $c=1+I^2(\bar v)Q/R$, $d=I^2(\bar v)\gamma^2$, and then further substitute $Q=\frac{1}{N}\sigma_{\bar w}^2$ and $R=s_N^2I(\bar v)$. Observe that $b=\Theta(1/N)$ and 
\begin{align*}
    \sqrt{(c-a)^2+4bd}+(c-a) = \Ocal(1/s_N\sqrt{N}).
\end{align*}
Therefore, $P=\Ocal(s_N/\sqrt{N})$. To see the lower bound, observe that if $s_N^2=o(N)$, then $\sqrt{N}/s_N\uparrow\infty$ and 
\[
\frac{P}{s_N/\sqrt{N}}\rightarrow 1.
\]
In either case, $P=\Theta(s_N/\sqrt{N})$ and 
\begin{align*}
    J^{GF} = \Theta\lp \frac{\sqrt{N}}{s_N}\rp.
\end{align*}
This in turn implies that in stationarity,
\begin{align*}
    K_t = (\gamma^{2}P_{t-1}+Q)I^2(\bar{v})(I^2(\bar{v})(\gamma^{2}P_{t-1}+Q)+R)^{-1} = \frac{I^2(\bar{v})(\gamma^{2}P_{t-1}+Q)}{R+I^2(\bar{v})(\gamma^{2}P_{t-1}+Q)}=\Theta\left(\frac{1}{s_N\sqrt{N}}\right).
\end{align*}

This completes the first part of the proof. The upper bound on the variance is a well known fact for the AR(1) process. We include a proof for completeness. Observe that,
\begin{align*}
    \Var(X_t) & = \Var \lp\sum_{i=0}^t \gamma^t \frac{\bar w_i}{\sqrt{N}}\rp\\
    & = \frac{1-\gamma^{2t}}{1-\gamma^2}\times \frac{\Var(\bar w_1)}{N}\\
    & \leq \frac{\sigma_{\bar w}^2}{N(1-\gamma^2)}.
\end{align*}
Recall that $\gamma = 1-1/\sqrt{N}$, so that $N(1-\gamma^2)=\Theta(1)$. This completes the proof.
\end{proofof}
\subsection{ Proof of Lemma \ref{lem:remainderterms1}}\label{sec:prf-remainder}

Recall from \cref{eq:R_t} that
\begin{align*} \calR_t  & =\underbrace{X_t(\phi'(\bar{v}_t)-\Ex[\phi'(\bar{v}_t)]
])}_{\text{Term 1}} \pm\underbrace{\frac{1}{2}\|\phi''\|_{\infty} \frac{1}{s_N}(X_t)^2}_{\text{Term 2}} \\
A_t & =  
(1-K_tI(\bar v))[\gamma(X_{t-1}-\hx_{t-1})+\bar w_t] - K_t\bar v_t.
\end{align*} 


We first show that the variance of $\Rcal_t$ is $\Ocal(1)$. 
\begin{align*}
    Var(\Rcal_t) = Var(\text{Term 1})+Var(\text{Term 2})+2Cov(\text{Term 1},\text{Term 2}).
\end{align*}
Observe from the independence of $X_t$ and $\bar v_t$ that $Cov(\text{Term 1},\text{Term 2})\propto\expec[X_t^3]\expec\lb\phi'(\bar{v}_t)-\Ex[\phi'(\bar{v}_t)]\rb=0$. So it is sufficient to bound the variance terms.  Again, because $\bar v_t$ is independent of $X_t$, and because $\expec[\text{Term 1}]=0$, we get
\begin{align*}
  Var(X_t(\phi'(\bar{v}_t)-\Ex[\phi'(\bar{v}_t)])) = \Ex[X_t^2(\phi'(\bar{v}_t)-\Ex[\phi'(\bar{v}_t)])^2]=\Ex[X_t^2]Var(\phi'(\bar{v}_t))=\calO(1).
\end{align*}
To upper bound Term 2 we only need to bound the fourth moment of $X_t$. Observe the $X_t = \sum_{i=0}^t\gamma^i \bar w_i/\sqrt{N}$. A sufficient condition for the fourth moment of $X_t$ to be bounded is then to have the fourth moments of $\bar w_i$ to be bounded, which happens via Assumption \ref{asum:primitives-driving} (and the subsequent remark).

We turn to $Cov(K_t\Rcal_t,A_t)$. Because $K_t$ is non-random, it suffices to consider $Cov(\Rcal_t,A_t)$.
Observe that, 
\[
Cov(\Rcal_t,A_t) = \expec[\Rcal_tA_t]-\expec[\Rcal_t]\expec[A_t].
\]
And we bound each of the previous terms separately, beginning with $\expec[\Rcal_t]$.

By an argument similar to the one in the previous part,
\begin{align*}
    \expec[\Rcal_t]\propto \expec[X_t^2]=\Var(X_t)=\Ocal\lp\frac{1}{s_N}\rp.
\end{align*}

We turn to $\expec[A_t]$. $K_t$ is non-random and $\expec[\bar v_t]=\expec[\bar w_t]=0$. Thus,
\begin{align*}
    \expec[A_t] = (1-K_t I(\bar v))\lp\gamma\expec[X_{t-1}-\hat x_{t-1}]\rp.
\end{align*}
It follows from \cref{eq:bias-bound} that the bias $\expec[X_{t-1}-\hat x_{t-1}]$ is $\Ocal(1/s_N)$. It also follows from Lemma \ref{lem:KFPXg} that $1-K_tI(\bar v)=\Ocal(1)$. Combining, it follows that $\expec[\Rcal_t]\expec[A_t]=\Ocal(1/s_N^2)$. We underline the fact our arguments used Lemma \ref{lem:KFPXg}, which is proved independent of Lemma \ref{lem:remainderterms1}. Thus, they are not circular. We now turn to $\expec[\Rcal_tA_t]$. 

 To bound $Cov(\Rcal_t,A_t)$, we now only need to bound $\expec[\Rcal_tA_t]$.
\begin{align} \Ex[\calR_tA_t]  = & \Ex\left[X_t(\phi'(\bar{v}_t)-\Ex[\phi'(\bar{v}_t)])(1-K_tI(\bar v))\gamma(X_{t-1}-\hat{x}_{t-1})\right] + \tag{i}\\
&   \frac{1}{\sqrt{N}}\Ex\left[X_t(\phi'(\bar{v}_t)-\Ex[\phi'(\bar{v}_t)])(1-K_tI(\bar v))\bar w_t\right]+ \tag{ii}\\
& s_N\Ex\left[X_t(\phi'(\bar{v}_t)-\Ex[\phi'(\bar{v}_t)])(1-K_tI(\bar v))K_t\bar v_t\right]\pm \tag{iii}\\
& \frac{1}{2 s_N}\|\phi''\|_{\infty}\Ex\left[X_t^2(1-K_tI(\bar v))\gamma(X_{t-1}-\hat{x}_{t-1})\right] + \tag{iv}\\ 
& \frac{1}{2 s_N}\|\phi''\|_{\infty}\Ex\left[X_t^2(1-K_tI(\bar v))\bar w_t\right] + \tag{v}\\
& \frac{1}{2 s_N}\|\phi''\|_{\infty}\Ex\left[X_t^2(1-K_tI(\bar v))K_t\bar v_t\right].  \tag{vi}
\end{align} 

Using the independence of $\bar v_t$, and using the fact $\expec[\phi(\bar v_t)-\expec\phi(\bar v_t)]=0$, it follows that (i) and (ii) are $0$. In (iii), $X_t$ is independent of $\bar v_t$ and $\expec[X_t]=0$. Thus (iii) becomes $0$. $X_t$ only depends on $X_{t-1}$, and $\bar w_{t-1}$, and thus is independent of $\bar w_t$. Thus (v) is $0$. $X_t$ is also independent of $\bar v_t$, which makes (vi) $0$. Finally, for item (iv) 
\begin{align*} \frac{1}{2 s_N}\|\phi''\|_{\infty}\Ex\left[X_t^2(1-K_tI(\bar v))\gamma(X_{t-1}-\hat{x}_{t-1})\right] & \leq \frac{1}{s_N}(1-K_tI(\bar v))\gamma\sqrt{\Ex[X_t^4]}\sqrt{Var(X_{t-1}-\hat{x}_{t-1})}\\ 
& \leq \frac{1}{s_N}(1-K_tI(\bar v))\gamma\sqrt{\Ex[X_t^4]}(1+Var(X_{t-1}-\hat{x}_{t-1}))\\
& =\calO\left(\frac{1}{s_N}(1+Var(X_{t-1}-\hat{x}_{t-1}))\right)\\
& =\calO\lp \frac{1}{s_N}\lp 1+P_{t-1} \rp \rp
\end{align*} 
Thus $K_tCov(\Rcal_t,A_t)\leq \Ocal\lp (1+P_{t-1})/(s_N^2\sqrt{N}) \rp$.
The rest of the proof now follows using the bound on $P_{t-1}$ from Lemma \ref{lem:KFPXg}.  \vspace{0.5cm}

\subsection{Proof of Lemma \ref{lem:lecamapp}}\label{sec:prf-lemlecam}

\begin{proofof}{Lemma \ref{lem:lecamapp}}


Let us assume without loss of generality that $\theta=0$ and let $f_{\tau}^1(z_1):=f_{\zeta_1}(z_1-\tau)$ denote the density of $Z_1$ when $\theta=\tau$. Using Bayes rule
\begin{align*}
   \mathbb{P}[\tau(Z_1)=\tau]=\frac{f_{\tau}^1(z_1)\pi(\tau)}{\int_{\tau}f_{\tau}^1(z_1)\pi(\tau)d\tau}.\numberthis \label{eq:bayes} 
\end{align*}
Under the hypothesis of the lemma, there exists $L_\pi$ such that 
\begin{align*}
    \sup_{\tau_1,\tau_2}\frac{|\pi(\tau_2)-\pi(\tau_1)|}{|\tau_2-\tau_1|}\leq L_\pi
\end{align*}
which implies for any $\tau$, $|\pi(\tau)-\pi(0)| \leq |\tau |L_\pi$. Then, $$f_{\tau}^1(z_1)\pi(\tau)= f_{\tau}^1(z_1)\pi(0) \pm L_{\pi} |\tau| f_{\tau}^1(z_1),$$
and 
$$\int_{\tau} f_{\tau}^1(z_1)\pi(\tau)d\tau= \pi(0)\int_{\tau}f_{\tau}^1(z_1)d\tau \pm L_{\pi} \int_{\tau}|\tau| f_{\tau}^1(z_1)d\tau.$$

Let $b=o(1)$. For $z_1\in [-b,b]$ we have 
$$\int_{\tau}|\tau| f_{\tau}^1(z_1)d\tau =
\int_{y}|z_1-y| f_{\zeta_1}(y)dy\leq z_1 + \int_y |y|f_{\zeta_1}(y)dy.$$
$z_1$ is $o(1)$, and by Cauchy-Schzwarz inequality, $\int_y |y|f_{\zeta_1}(y)dy\leq \sigma_1=o(1)$. Therefore, we get
\[
\int_{\tau} f_{\tau}^1(z_1)\pi(\tau)d\tau = o(1).
\]

Because $\int_{\tau}f_{\tau}^1(z_1)d\tau = 1$ we have for for $z_1\in [-b,b]$
\begin{align*}
\frac{f_{\tau}^1(z_1)\pi(\tau)}{\int_{\tau}f_{\tau}^1(z_1)\pi(\tau)d\tau}   & = 
\frac{f_{\tau}^1(z_1)}{\int_{\tau}f_{\tau}^1(z_1)d\tau+o(1)}\pm \frac{L_{\pi}|\tau|f_{\tau}^1(z_1)}{\int_{\tau}f_{\tau}^1(z_1)d\tau+o(1)}\\
& =\frac{f_{\tau}^1(z_1)}{\int_{\tau}f_{\tau}^1(z_1)d\tau}(1+o(1))\pm \frac{L_{\pi}}{\pi(0)}|\tau|\frac{f_{\tau}^1(z_1)}{\int_{\tau}f_{\tau}^1(z_1)d\tau}(1+o(1)) \\
& =f_{\zeta_1}(z_1-\tau)(1+o(1))\pm  2\frac{L_{\pi}}{\pi(0)}|\tau|f_{\zeta_1}(z_1-\tau).
\end{align*}
Overall for $z_1\in[-b,b]$, 
integrating over $\tau$ we have for $z_1\in [-b,b]$  \begin{align*} \Ex[\tau(Z_1)|Z_1=z_1] & = \int_{\tau}\tau f_{\zeta_1}(z_1-\tau)(1+o(1))d\tau \pm 2\frac{L_{\pi}}{\pi(0)}\int_{\tau}|\tau|^2f_{\zeta_1}(z_1-\tau)d\tau  \\ & = z_1(1+o(1))+\calO(z_1^2)=z_1(1+o(1)),\end{align*} where the last equality follows recalling that $b=o(1)$. 

Finally, because $\zeta_1 = \sigma_1\bar{\zeta}$ (with $\bar{\zeta}$ having a fourth finite moment), we can choose $b=o(1)\gg \sigma_1$ so that $\Ex[z_1^21\{|z_1|>b\}] =o(\sigma_1^2).$ Thus, (recall $\theta=0$)
\begin{align*} \Ex[(\hat{\theta}-\theta)^2]& \geq \int_{z_1\in [-b,b]}z_1^2 f_{\zeta_1}(z_1)(1+o(1)) dz_1  = \sigma_1^2(1+o(1)),
\end{align*} as stated. 
\end{proofof}

% \section{Introduction}
% The very first letter is a 2 line initial drop letter followed
% by the rest of the first word in caps.
% 
% form to use if the first word consists of a single letter:
% \IEEEPARstart{A}{demo} file is ....
% 
% form to use if you need the single drop letter followed by
% normal text (unknown if ever used by the IEEE):
% \IEEEPARstart{A}{}demo file is ....
% 
% Some journals put the first two words in caps:
% \IEEEPARstart{T}{his demo} file is ....
% 
% Here we have the typical use of a "T" for an initial drop letter
% and "HIS" in caps to complete the first word.
% \IEEEPARstart{T}{his} demo file is intended to serve as a ``starter file''
% for IEEE journal papers produced under \LaTeX\ using
% IEEEtran.cls version 1.8b and later.
% You must have at least 2 lines in the paragraph with the drop letter
% (should never be an issue)
% I wish you the best of success.

% \hfill mds
 
% \hfill August 26, 2015

% \subsection{Subsection Heading Here}
% Subsection text here.

% needed in second column of first page if using \IEEEpubid
%\IEEEpubidadjcol

% \subsubsection{Subsubsection Heading Here}
% Subsubsection text here.


% An example of a floating figure using the graphicx package.
% Note that \label must occur AFTER (or within) \caption.
% For figures, \caption should occur after the \includegraphics.
% Note that IEEEtran v1.7 and later has special internal code that
% is designed to preserve the operation of \label within \caption
% even when the captionsoff option is in effect. However, because
% of issues like this, it may be the safest practice to put all your
% \label just after \caption rather than within \caption{}.
%
% Reminder: the "draftcls" or "draftclsnofoot", not "draft", class
% option should be used if it is desired that the figures are to be
% displayed while in draft mode.
%
%\begin{figure}[!t]
%\centering
%\includegraphics[width=2.5in]{myfigure}
% where an .eps filename suffix will be assumed under latex, 
% and a .pdf suffix will be assumed for pdflatex; or what has been declared
% via \DeclareGraphicsExtensions.
%\caption{Simulation results for the network.}
%\label{fig_sim}
%\end{figure}

% Note that the IEEE typically puts floats only at the top, even when this
% results in a large percentage of a column being occupied by floats.


% An example of a double column floating figure using two subfigures.
% (The subfig.sty package must be loaded for this to work.)
% The subfigure \label commands are set within each subfloat command,
% and the \label for the overall figure must come after \caption.
% \hfil is used as a separator to get equal spacing.
% Watch out that the combined width of all the subfigures on a 
% line do not exceed the text width or a line break will occur.
%
%\begin{figure*}[!t]
%\centering
%\subfloat[Case I]{\includegraphics[width=2.5in]{box}%
%\label{fig_first_case}}
%\hfil
%\subfloat[Case II]{\includegraphics[width=2.5in]{box}%
%\label{fig_second_case}}
%\caption{Simulation results for the network.}
%\label{fig_sim}
%\end{figure*}
%
% Note that often IEEE papers with subfigures do not employ subfigure
% captions (using the optional argument to \subfloat[]), but instead will
% reference/describe all of them (a), (b), etc., within the main caption.
% Be aware that for subfig.sty to generate the (a), (b), etc., subfigure
% labels, the optional argument to \subfloat must be present. If a
% subcaption is not desired, just leave its contents blank,
% e.g., \subfloat[].


% An example of a floating table. Note that, for IEEE style tables, the
% \caption command should come BEFORE the table and, given that table
% captions serve much like titles, are usually capitalized except for words
% such as a, an, and, as, at, but, by, for, in, nor, of, on, or, the, to
% and up, which are usually not capitalized unless they are the first or
% last word of the caption. Table text will default to \footnotesize as
% the IEEE normally uses this smaller font for tables.
% The \label must come after \caption as always.
%
%\begin{table}[!t]
%% increase table row spacing, adjust to taste
%\renewcommand{\arraystretch}{1.3}
% if using array.sty, it might be a good idea to tweak the value of
% \extrarowheight as needed to properly center the text within the cells
%\caption{An Example of a Table}
%\label{table_example}
%\centering
%% Some packages, such as MDW tools, offer better commands for making tables
%% than the plain LaTeX2e tabular which is used here.
%\begin{tabular}{|c||c|}
%\hline
%One & Two\\
%\hline
%Three & Four\\
%\hline
%\end{tabular}
%\end{table}


% Note that the IEEE does not put floats in the very first column
% - or typically anywhere on the first page for that matter. Also,
% in-text middle ("here") positioning is typically not used, but it
% is allowed and encouraged for Computer Society conferences (but
% not Computer Society journals). Most IEEE journals/conferences use
% top floats exclusively. 
% Note that, LaTeX2e, unlike IEEE journals/conferences, places
% footnotes above bottom floats. This can be corrected via the
% \fnbelowfloat command of the stfloats package.




% \section{Conclusion}
% The conclusion goes here.





% if have a single appendix:
%\appendix[Proof of the Zonklar Equations]
% or
%\appendix  % for no appendix heading
% do not use \section anymore after \appendix, only \section*
% is possibly needed

% use appendices with more than one appendix
% then use \section to start each appendix
% you must declare a \section before using any
% \subsection or using \label (\appendices by itself
% starts a section numbered zero.)
%


% \section{Proof of the First Zonklar Equation}
% Appendix one text goes here.

% you can choose not to have a title for an appendix
% if you want by leaving the argument blank
% \section{}
% Appendix two text goes here.


% use section* for acknowledgment
% \section*{Acknowledgment}


% The authors would like to thank...


% Can use something like this to put references on a page
% by themselves when using endfloat and the captionsoff option.
\ifCLASSOPTIONcaptionsoff
  \newpage
\fi



% trigger a \newpage just before the given reference
% number - used to balance the columns on the last page
% adjust value as needed - may need to be readjusted if
% the document is modified later
%\IEEEtriggeratref{8}
% The "triggered" command can be changed if desired:
%\IEEEtriggercmd{\enlargethispage{-5in}}

% references section

% can use a bibliography generated by BibTeX as a .bbl file
% BibTeX documentation can be easily obtained at:
% http://mirror.ctan.org/biblio/bibtex/contrib/doc/
% The IEEEtran BibTeX style support page is at:
% http://www.michaelshell.org/tex/ieeetran/bibtex/
%\bibliographystyle{IEEEtran}
% argument is your BibTeX string definitions and bibliography database(s)
%\bibliography{IEEEabrv,../bib/paper}
%
% <OR> manually copy in the resultant .bbl file
% set second argument of \begin to the number of references
% (used to reserve space for the reference number labels box)


% \begin{thebibliography}{1}

% \bibitem{IEEEhowto:kopka}
% H.~Kopka and P.~W. Daly, \emph{A Guide to \LaTeX}, 3rd~ed.\hskip 1em plus
%   0.5em minus 0.4em\relax Harlow, England: Addison-Wesley, 1999.

% \end{thebibliography}

% biography section
% 
% If you have an EPS/PDF photo (graphicx package needed) extra braces are
% needed around the contents of the optional argument to biography to prevent
% the LaTeX parser from getting confused when it sees the complicated
% \includegraphics command within an optional argument. (You could create
% your own custom macro containing the \includegraphics command to make things
% simpler here.)
%\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{mshell}}]{Michael Shell}
% or if you just want to reserve a space for a photo:

% \begin{IEEEbiography}{Michael Shell}
% Biography text here.
% \end{IEEEbiography}

% % if you will not have a photo at all:
% \begin{IEEEbiographynophoto}{John Doe}
% Biography text here.
% \end{IEEEbiographynophoto}

% insert where needed to balance the two columns on the last page with
% biographies
%\newpage

% \begin{IEEEbiographynophoto}{Jane Doe}
% Biography text here.
% \end{IEEEbiographynophoto}

% You can push biographies down or up by placing
% a \vfill before or after them. The appropriate
% use of \vfill depends on what kind of text is
% on the last page and whether or not the columns
% are being equalized.

%\vfill

% Can be used to pull up biographies so that the bottom of the last one
% is flush with the other column.
%\enlargethispage{-5in}
\sloppy
\bibliography{biblio,itaibib}
\fussy
% that's all folks
\end{document}


