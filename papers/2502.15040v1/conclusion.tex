\section{Conclusion}

When faced with a long report generation task, Medical Multimodal Large Language Models may exhibit biases and hallucinate details. We have introduced an entity probing method to examine these details, and shown that V-RAG improves entity probing accuracy for both frequent and rare entities. The lack of multi-image support in mainstream models has been a barrier to the adoption of V-RAG, leading almost all prior work to work only with the text corresponding to similar images.  Our special image-and-text fine-tuning tasks pave the way for multi-image-trained and single-image-trained models to become capable or more powerful at V-RAG, and we have shown that the use of both retrieved text and retrieved images benefits entity probing performance. Downstream, revision using entity probing with V-RAG can increase a report's accuracy on clinical details, improving the RadGraph-F1 score of a generated report.  Our research contributes towards more medically trustworthy MLLMs for healthcare applications.

