

\begin{figure*}[t]
    \centering
    \setlength{\abovecaptionskip}{1mm}
    \includegraphics[width=\linewidth]{figure/fig2.png}
    \caption{Fine-tuning tasks to make Med-MLLM V-RAG-capable by (a) improving image-and-text association abilities, (b) focusing on specific images, and (c) making decisions using extracted similar data.}
    \vspace{-3mm}
    \label{fig:tasks}
\end{figure*}










\section{V-RAG with Existing multi-image-trained Med-MLLMs}
\label{sec:v-rag-method}
Figure~\ref{fig:fig1} illustrates the V-RAG framework. This section details each component and explains how we enhance model performance during V-RAG.


\subsection{Multimodal Retrieval}

We aim to retrieve images and corresponding textual descriptions that match the features of target medical images. These references, rich in visual and textual medical details, guide response generation for the medical image.
To extract embeddings, we employ BiomedCLIP~\cite{Zhang2023BiomedCLIPAM}, which provides robust representations across a diverse range of biomedical image types. For a given medical image $X_{img}$, we extract its image embedding $\mathcal{E}_{img} \in \mathbb{R}^{d}$, with $d$ representing the dimension (i.e., 512 for BiomedCLIP), and store it in memory $\mathcal{M}$ for retrieval.

To facilitate efficient search operations during the inference phase, we construct the memory $\mathcal{M}$ using FAISS~\cite{douze2024faiss}, a vector storage and retrieval system that utilizes GPU computation. Instead of exact kNN search, we employ an approximate kNN search using the Hierarchical Navigable Small World (HNSW) algorithm~\cite{Malkov2016EfficientAR} to identify the top-$k$ nearest neighbors, effectively retrieving the images in $\mathcal{M}$ most similar to a given query image.


\subsection{Inference with V-RAG}

In the inference stage, we first encode the query image $X_{q}$ to obtain its corresponding image embedding. We then retrieve the top-$k$ images in $\mathcal{M}$; the retrieved set of similar images and their reports are represented as $(I_{1}, ... I_{k})$ and $(R_{1}, ..., R_{k})$.
We then use the retrievals to guide the generation of Med-MLLM for the query image by appending each reference before the question, following this prompt guidance:
``\texttt{...This is the i-th similar image and its report for your reference. [Reference]$_i$... Answer the question with only the word yes or no. Do not provide explanations. According to the last query image and the reference images and reports, [Question] [Query Image]}", where \texttt{[References]$_i$} is structured as \texttt{[($I_i$, $R_i$)]}.




\subsection{Enhancing Med-MLLMs for V-RAG}
Some MLLMs may lack the training to distinguish information from multiple images.
To address this, we introduce three fine-tuning tasks to enhance image-text association in the V-RAG process.
Given a dataset of images paired with captions or reports, we define the original dataset as $S = {(\texttt{img}_{i}, \texttt{P}_i, \texttt{A}_i)}|^{N}_{i=1}$, where $\texttt{img}_{i}$ denotes the $i$-th image, $\texttt{P}_i$ and $\texttt{A}_i$ represent the prompt and the answer, respectively, and $N$ is the total number of samples.
We then construct fine-tuning tasks on this dataset with our designed objectives as follows.


\paragraph{Image-text awareness task.}
We aim to enhance Med-MLLM's image-and-text association ability by training the model to identify the relevant image corresponding to provided text from multiple images.
To achieve this, we construct a multi-image dataset, $M_{position}$, from dataset $S$, to ask the model to identify the position of the image related to the given text, as depicted in Figure~\ref{fig:tasks}(a).
First, we randomly select $K$ images (where $K$ ranges from 1 to 5 in our case) and form the image collection $(\texttt{img}_{i_1}, ..., \texttt{img}_{i_K})$.
Next, we choose an integer $j$ from $[1,K]$ and retrieve the textual document $\texttt{R}_{i_j}$, corresponding to $\texttt{img}_{i_j}$.
We then collect $M_{position}$ using $\{(\texttt{img}_{i_1}, \texttt{img}_{i_2}, ..., \texttt{img}_{i_K}, \texttt{P}^{'}_{i_j}, \texttt{A}^{'}_{i_j})\}$. 
Here, $\texttt{P}^{'}_{i_j}$ is a newly formulated prompt designed to ask a position-based question in addition to the original question $\texttt{P}_{i_j}$, associating $\texttt{A}_{i_j}$ with the provided images.
For example, ``\texttt{What image from 1 to $K$ does this $\texttt{A}_{i_j}$ correspond to? $\texttt{P}_{i_j}$}''.
$\texttt{A}^{'}_{i_j}$ is the answer indicating the position of ${\texttt{img}_{i_j}}$ among the provided images, for example, ``\texttt{The $j$-th image.}''



\paragraph{Image-focus task.}
In this task (Figure~\ref{fig:tasks}(b)), we aim to direct Med-MLLM to focus on one specific image from a set of multiple images and subsequently perform text generation based on that image, thereby improving performance by minimizing distractions from other visual inputs.
To achieve this, we create another dataset, $M_{focus}$, also from image dataset $S$.
We start by randomly selecting $K$ images from $S$ to form the collection $(\texttt{img}_{i_1}, ..., \texttt{img}_{i_K})$, and then choose an integer $j$ from $[1,K]$.
We then collect $\{(\texttt{img}_{i_1}, ..., \texttt{img}_{i_K}, \texttt{P}^{''}_{i_j}, \texttt{A}_{i_j})\}$ to form $M_{focus}$, where  $\texttt{P}^{''}_{i_j}$ is a new prompt designed to help the model focus on our specified image, $\texttt{img}_{i_j}$, and pose the original question $\texttt{P}_{i_j}$ for that image.
For example in Figure~\ref{fig:tasks}(b), the new prompt $\texttt{P}^{''}_{i_j}$ is ``\texttt{Focus on the $j$-th image, {P}$_{i_j}$.}'', where $\texttt{P}_{i_j}$ is the original prompt that asks for a finding/report to be generated from a given image.

\paragraph{Strategies to make easier learning tasks.}
Various conditions may be applied to the random selection of images for both image-text awareness and image-focus tasks.
For example, when the image dataset $S$ consists of images $\texttt{img}_i$ with radiology reports $\texttt{A}_i$, we  require that the selected report $\texttt{A}_{i_j}$ for the focus image contains at least one CheXpert~\cite{Irvin2019CheXpertAL} label that is distinct from those in the other reports $\{A_{{i_m}}\}|^{K}_{m = 1, m \neq j}$.
This strategy simplifies the learning task by ensuring that there are no alternative images to which the report could apply equally well.
For easier and more diverse datasets, such a strategy may not be necessary.


\paragraph{Learning from extracted similar information task.}
We aim to assist Med-MLLM in decision-making by using extracted similar information during V-RAG. 
To do so, we simulate the V-RAG scenario and construct a multi-image dataset, $M_{vrag}$.
Given a query image $\texttt{img}_{q}$ in the validation set, we search for the top-$K$ similar images ($\texttt{img}_{q_1}$, ... , $\texttt{img}_{q_K}$) from memory $\mathcal{M}$, pairing them with their corresponding documents ($\texttt{A}_{q_1}$, ..., $\texttt{A}_{q_K}$).
We then conduct $M_{vrag}$ using $\{(\texttt{img}_{q_1},  \texttt{A}_{q_1}, ..., \texttt{img}_{q_K}, \texttt{A}_{q_K}), \texttt{img}_{q}, \texttt{P}^{'''}_{q}, \texttt{A}_{q}\}$.
Here, $ \texttt{A}_{q}$ is the answer for query image and $\texttt{P}^{'''}_{q}$ is a new prompt designed to supply related information alongside the original question $\texttt{P}_{q}$.
Taking disease entity probing as example (in Figure~\ref{fig:tasks}(c)), $\texttt{P}^{'''}_{q}$ can be ``\texttt{Based on the query image, and the similar images and their reports: $(\texttt{img}_{q_1},  \texttt{A}_{q_1}, ..., \texttt{img}_{q_K}, \texttt{A}_{q_K})$, $\texttt{P}_{q}$},'' and $\texttt{P}_{q}$ is ``\texttt{Does the patient have [disease entity]?}''


