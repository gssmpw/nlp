\begin{table}[]
\begin{center}
\scalebox{0.78}{
\begin{tabular}{cccc}
\hline
\multicolumn{1}{c}{\multirow{2}{*}{\textbf{Model}}} & \multicolumn{3}{c}{\textbf{MIMIC-CXR}} \\
\cline{2-4} 
\multicolumn{1}{c}{} & Precision & Recall & F1  \\ \hline
 LLaVA$_S$ &  \textbf{0.953}     &   0.475    &  0.604    \\ \hline
 LLaVA$_{\{M_{vrag}\}}$ & 0.914 &	0.867 &	0.852 \\
LLaVA$_{\{M_{focus} + M_{vrag}\}}$& 0.908 &	0.903 & 0.859 \\
LLaVA$_{\{M_{position} + M_{vrag}\}}$& 0.908	& {0.910} &	0.862\\
 LLaVA$_{\{M_{position} + M_{focus} + M_{vrag}\}}$ & 0.897 &	\textbf{0.944} &	\textbf{0.870}
\\\hline
		
	
			
\end{tabular}
}
\caption{Entity probing results for single-image-trained MLLM and MLLM enhanced with our proposed fine-tuning tasks. The superiority indicates that our tasks effectively make a MLLM V-RAG-capable.}
\vspace{-5mm}
\label{tb:llava}

\end{center}
\end{table}