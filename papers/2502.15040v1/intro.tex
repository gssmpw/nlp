\section{Introduction}
Recent advances in Multimodal Large Language Models (MLLMs)~\cite{Achiam2023GPT4TR, Liu2023ImprovedBW} have demonstrated impressive capabilities in complex vision-and-text tasks, showing significant potential in specialized domains.
In healthcare, the development of Medical MLLMs (Med-MLLMs)~\cite{Li2023LLaVAMedTA, Wu2023TowardsGF} can support clinical decision-making processes, with the potential to enhance physician efficiency and improve patient health outcomes.
However, numerous studies have demonstrated that MLLMs are prone to hallucination~\cite{Li2023EvaluatingOH, Bai2024HallucinationOM, Huang2024VisualHO}.
The hallucination tendency of MLLM's has been demonstrated on Med-MLLM's
as well \citep{wu2024hallucinationbenchmarkmedicalvisual}.
This is particularly concerning in the healthcare scenario, as depicted in Figure~\ref{fig:fig1}, where even a few wrong tokens in text can lead to significant misinterpretations, affecting medical diagnoses, treatment plans, and patient outcomes~\cite{Pal2024GeminiGT}.

\begin{figure}[t]
    \centering
    \setlength{\abovecaptionskip}{1mm}
    \includegraphics[width=0.9\linewidth]{figure/fig1.png}
    \caption{(Up) Hallucination issue of Med-MLLM. (Down) Framework of V-RAG to improve Med-MLLM.}
    \vspace{-5mm}
    \label{fig:fig1}
\end{figure}


\begin{figure}[t]
\centering
    \setlength{\abovecaptionskip}{1mm}
    \includegraphics[width=0.9\linewidth]{figure/entityprobing.pdf}
    \caption{Entity probing asks entity-based questions to an MLLM and compares predictions against answers grounded in an LLM's interpretation of a reference caption.}
    \vspace{-5mm}
    \label{fig:entityprobing}
\end{figure}

Retrieval-Augmented Generation (RAG)~\cite{Lewis2020RetrievalAugmentedGF} has become a prominent approach to mitigate the hallucination problem in Large Language Models (LLMs) by grounding text generation in retrieved knowledge relevant to a given query. 
Besides grounding, RAG potentially supplements the knowledge in a model's parameters with knowledge present in a corpus, enabling open book question answering to exceed closed book performance.
Several prior works~\cite{Sarto2024TowardsRA,Liu2024RARRA, Zhou2024Img2LocRI} have explored text-based RAG in MLLMs.
This approach assumes that using text documents associated with images similar to the query image can effectively augment the model, treating the retrieved images as perfectly interchangeable with the query image.
However, this assumption is not always accurate. In this work, we study \textbf{Visual-RAG (V-RAG)}, which considers not only the associated text from retrieved similar images but also the similar images themselves to provide more accurate responses to the given instruction. By incorporating both modalities, V-RAG allows the model to determine what is truly important from the retrieved content, enhancing its ability to deliver more contextually relevant answers, as illustrated in Figure~\ref{fig:fig1}.

With certain multi-image-trained Med-MLLMs, we see that V-RAG improves a detailed understanding of an image beyond what is possible with text-based RAG techniques.
We demonstrate this through \textbf{entity probing}.
Entity probing presents an image to an MLLM and asks yes/no questions about disease entities, and compares predictions against answers grounded in an LLM's interpretation of a reference report or caption (Figure~\ref{fig:entityprobing}).
Entity probing gives us a clinical perspective on text generations across medical domains which is not captured by natural language generation metrics such as ROUGE, while avoiding sensitivity to entity phrasing.
We show that V-RAG, as an inference technique applied to carefully selected Med-MLLMs trained on multi-image datasets, enhances understanding more effectively than original Med-MLLMs and previous text-based RAG systems.


To improve the model's multimodal understanding when presented with rich retrievals, we design a general fine-tuning technique to boost Med-MLLM capabilities in V-RAG. 
This approach strengthens image-text comprehension and enables effective learning from similar resources retrieved during multimodal queries.
It benefits not only Med-MLLMs trained on multi-image dataset but also single-image-trained models to leverage multi-image inputs in V-RAG, thereby improving performance.
This frees researchers from relying on specific pre-trained models that may not be aligned with their task in order to use V-RAG, allowing V-RAG to be applied to any model and dataset of interest.  
Our key contributions are summarized as follows: 

\begin{itemize}
\item We analyze hallucinations in MLLMs on chest X-ray report generation and medical image captioning datasets through entity probing, showing that V-RAG mitigates hallucinations more effectively than baseline RAG techniques. These benefits extend to both frequent and rare entities.

\item  To enhance Med-MLLMs' multimodal comprehension with V-RAG, we introduce general image-text fine-tuning tasks to boost model performance and improve their understanding when multimodal retrievals are presented. These tasks enable an MLLM originally trained with single images to become capable of V-RAG using multiple retrieved images.

\item We show that entity probing with V-RAG can be used to revise
chest X-ray reports to contain fewer hallucinations and have better 
detailed accuracy, as measured by RadGraph-F1 score.

\end{itemize}


