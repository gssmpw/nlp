\section{Making a single-image-trained MLLM V-RAG-Capable}
\label{sec:single-image}
We first define a single-image dataset and then detail our proposed tasks that extend models trained on single-image datasets to handle multi-image inputs.



\begin{figure*}[t]
    \centering
    \setlength{\abovecaptionskip}{1mm}
    \includegraphics[width=\linewidth]{LaTeX/figure/fig2.png}
    \caption{Fine-tuning tasks to make Med-MLLM V-RAG-capable by (a) improving image-and-text association abilities, (b) focusing on specific images, and (c) making decisions using extracted similar data.}
    \label{fig:tasks}
\end{figure*}



\paragraph{Preliminaries.}
Many recent Med-MLLMs are trained on single image datasets. 
These datasets are constructed with an image paired with a text description, either a caption or a report, for each image. 
We define this dataset as $S = \{(\texttt{img}_{i}, \texttt{P}_i, \texttt{A}_i)\}|^{N}_{i=1}$, where $\texttt{img}_{i}$ denotes the $i$-th image, $\texttt{P}_i$ and $\texttt{A}_i$ represent the prompt and the answer, and $N$ is the total number of samples.

\paragraph{Image-text awareness task.}
We aim to enhance Med-MLLM's image-and-text association ability by training the model to identify the relevant image corresponding to provided text from multiple images.
To achieve this, we construct a multi-image dataset, $M_{position}$, from single image dataset $S$, to ask the model to identify the position of the image related to the given text, as depicted in Figure~\ref{fig:tasks}(a).
First, we choose $K$ integers from the range $[1,N]$ to form the set $(i_1,..., i_k, ..., i_K)$ and then select an integer $j$ from $[1,K]$.
Next, we extract the corresponding images based on the integer set from the single image dataset and collect $(\texttt{img}_{i_1}, \texttt{img}_{i_2}, ..., \texttt{img}_{i_K}).$
Based on the selected $j$, we retrieve the textual document $\texttt{R}_{i_j}$, corresponding to $\texttt{img}_{i_j}$.
We then collect $M_{aware}$ using $\{(\texttt{img}_{i_1}, \texttt{img}_{i_2}, ..., \texttt{img}_{i_K}, \texttt{P}^{'}_{i_j}, \texttt{A}^{'}_{i_j})\}$. 
Here, $\texttt{P}^{'}_{i_j}$ is a newly formulated prompt designed to ask a position-based question in addition to the original question $\texttt{P}_{i_j}$, associating $\texttt{A}_{i_j}$ with the provided images.
For example, ``\texttt{What image from 1 to $K$ does this $\texttt{A}_{i_j}$ correspond to? $\texttt{P}_{i_j}$}''.
$\texttt{A}^{'}_{i_j}$ is the answer indicating the position of ${\texttt{img}_{i_j}}$ among the provided images, for example, ``\texttt{The $j$-th image.}''


\paragraph{Image-focus task.}
In this task (Figure~\ref{fig:tasks}(b)), we aim to direct Med-MLLM to focus on one specific image from a set of multiple images and subsequently perform text generation based on that image, thereby improving performance by minimizing distractions from other visual inputs.
To achieve this, we create another dataset, $M_{focus}$, also from single image dataset $S$.
We start by randomly selecting $K$ images from $S$ to form the collection $(\texttt{img}_{i_1}, ..., \texttt{img}_{i_K})$, and then choose an integer $j$ from $[1,K]$.
We then collect $\{(\texttt{img}_{i_1}, ..., \texttt{img}_{i_K}, \texttt{P}^{''}_{i_j}, \texttt{A}_{i_j})\}$ to form $M_{focus}$, where  $\texttt{P}^{''}_{i_j}$ is a new prompt designed to help the model focus on our specified image, $\texttt{img}_{i_j}$, and pose the original question $\texttt{P}_{i_j}$ for that image.
For example, $\texttt{P}^{''}_{i_j}$ could be ``\texttt{Focus on the $j$-th image, {P}$_{i_j}$.}''


\paragraph{Strategies to make easier learning tasks.}
Various conditions may be applied to the random selection of images for both the image-text awareness and image-focus tasks.
For example, when the single image dataset $S$ consists of images $\texttt{img}_i$ with radiology reports $\texttt{A}_i$, we may require that the selected report $\texttt{A}_{i_j}$ for the focus image contains at least one CheXpert~\cite{Irvin2019CheXpertAL} label that is distinct from those in the other reports $\{A_{{i_m}}\}|^{K}_{m = 1, m \neq j}$.
This strategy can make the learning task easier by ensuring that there are no alternative images to which the report could apply equally well.
For easier and more diverse datasets, such a strategy may not be necessary.


\paragraph{Learning from extracted similar information tasks.}
We aim to assist Med-MLLM in making decisions with extracted similar information during V-RAG. To do so, we simulate the scenario of V-RAG and construct a multi-image dataset, $M_{vrag}$.
Given a query image $\texttt{img}_{q}$ in the validation set, we search for the top-$K$ similar images ($\texttt{img}_{q_1}$, ... , $\texttt{img}_{q_K}$) in the training set, pairing them with their corresponding answers ($\texttt{A}_{q_1}$, ..., $\texttt{A}_{q_K}$).
We then conduct $M_{vrag}$ using $\{(\texttt{img}_{q_1},  \texttt{A}_{q_1}, ..., \texttt{img}_{q_K}, \texttt{A}_{q_K}), \texttt{img}_{q}, \texttt{P}^{'''}_{q}, \texttt{A}_{q}\}$.
Here, $ \texttt{A}_{q}$ is the answer for query image and $\texttt{P}^{'''}_{q}$ is a crafted prompt intended to supply related information in conjunction with its original question $\texttt{P}_{q}$.
Taking the example of entity probing task, as illustrated in Figure~\ref{fig:tasks}(c), $\texttt{P}^{'''}_{q}$ can be ``\texttt{Based on the query image, incorporate the extracted similar information, $\texttt{P}_{q}$}'', and $\texttt{P}_{q}$ is ``\texttt{Does the patient have [disease entity]?}''






\section{V-RAG with Existing multi-image-trained Med-MLLMs}

\subsection{Multimodal Retrieval}

We aim to retrieve images and their corresponding textual descriptions that match the features of the target medical images. These references, rich in visual and textual medical facts, guide the generation of responses for the medical image. To extract embeddings, we employ BiomedCLIP~\cite{Zhang2023BiomedCLIPAM}, which provides robust representations across a diverse range of biomedical image types. For a given medical image $X_{img}$, we extract its image embedding $\mathcal{E}_{img} \in \mathbb{R}^{d}$, with $d$ representing the dimension (i.e., 512 for BiomedCLIP), and store it in memory $\mathcal{M}$ for retrieval.

To facilitate efficient search operations during the inference phase, we construct the memory $\mathcal{M}$ using FAISS~\cite{douze2024faiss}, a vector-based data storage system that utilizes flat indexes and GPU computation. Instead of exact kNN search, we employ an approximate kNN search using the Hierarchical Navigable Small World (HNSW) algorithm~\cite{Malkov2016EfficientAR} to identify the top-$k$ nearest neighbors, effectively retrieving the images in $\mathcal{M}$ most similar to a given query image.


\subsection{Inference with V-RAG}

In the inference stage, we first encode the query image $X_{q}$ to obtain its corresponding image embedding. We then retrieve the top-$k$ images in $\mathcal{M}$; the retrieved set of similar images and their reports are represented as $(I_{1}, ... I_{k})$ and $(R_{1}, ..., R_{k})$.
We then use the retrievals to guide the generation of Med-MLLM for the query image by appending each reference before the question, following this prompt guidance:
``\texttt{...This is the i-th similar image and its report for your reference. [Reference]$_i$... Answer the question with only the word yes or no. Do not provide explanations. According to the last query image and the reference images and reports, [Question] [Query Image]}", where \texttt{[References]$_i$} is structured as \texttt{[($I_i$, $R_i$)]}.




\subsection{Enhancing Med-MLLMs for V-RAG}
Directly presenting multiple images and reports to Med-MLLMs may also lead to confusion about which resources to utilize for optimal generation. 
Therefore, we fine-tune the Med-MLLMs to enhance their image-text awareness capabilities by introducing the fine-tuning tasks detailed in Section~\ref{sec:single-image} and Figure~\ref{fig:tasks}, applying them to the existing multi-image-trained Med-MLLMs.

For the image-text awareness task, we first randomly select 2-5 images for each sample and extract one text document ($\texttt{A}_{j}$) from them, which we then provide to the Med-MLLM. We conduct the question-answer pair fine-tuning for this task using the following format:
``\texttt{Question: $(\texttt{img}_{1}, \texttt{img}_{2}, ... \texttt{img}_{K})$,  Which image does the following report correspond to? Report: $\texttt{A}_{j}$; Answer: The j-th image.}''
Similarly, for the image-focus task, we randomly select 2-5 images and ask the model to focus on a specific image to generate a corresponding text document ($\texttt{A}_{j}$). We then prepare samples for fine-tuning on this task in the following format:
``\texttt{Question: $(\texttt{img}_{1}, \texttt{img}_{2}, ... \texttt{img}_{K})$, Focus on the j-th image. What is the finding of this image?; Answer: $\texttt{A}_{j}$.}''

To conduct the third fine-tuning task for Med-MLLM, we use the validation image $(\texttt{img}_{val})$ and extract the top-$k$ similar images from memory $\mathcal{M}$, with $k$ randomly selected from 1 to 5. We pair the extracted images ($\texttt{img}_{1}$ to $\texttt{img}_{k}$) and their corresponding documents ($\texttt{A}_{1}$ to $\texttt{A}_{k}$) together with the original validation image as the sources for the question. For the answer, we extract the ground truth for entity probing from this validation image. The prompt format we consider is as follows:
``\texttt{Question: (...$\texttt{img}_{i}$; This is the $i$-th similar image's report for your reference. Report: $\texttt{A}_{i}$...), [$\texttt{img}_{val}$], Based on the final query image, the similar images, and their reports, Does the patient have [disease entity]?' Answer: [Yes/No].}''

