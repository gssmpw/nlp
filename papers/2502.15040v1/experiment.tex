\section{Experiment}

\subsection{Experimental Setups}

% We utilize RadFM~\cite{Wu2023TowardsGF} as the backbone Med-MLLM due to its ability to take multiple images as input.
% To enhance the performance of Med-MLLM, we prepared 6,000 data points for each of the image-text awareness and rare disease visual-RAG tasks. We utilized LoRA~\cite{Hu2021LoRALA} to fine-tune Med-MLLM with a learning rate of 5e-5 for these tasks.
We selected RadFM~\cite{Wu2023TowardsGF}, an existing multi-image-trained Med-MLLM, as our base model to evaluate the effectiveness of V-RAG and our proposed fine-tuning tasks on multi-image-trained models. To assess the capability of making single-image-trained MLLMs V-RAG capable, we utilized LLaVA~\cite{Liu2023VisualIT} as the backbone model. We employed LoRA~\cite{Hu2021LoRALA} to fine-tune both LLaVA and RadFM on our designed tasks, applying a learning rate of 5e-5 for all fine-tuning tasks.




\subsection{Baselines}

We compare our method with the original Med-MLLM, RadFM, which does not include retrievals, with other baselines that do.
RAT~\cite{Sarto2024TowardsRA} and Img2Loc~\cite{Zhou2024Img2LocRI} are
identical methods which incorporate text associated with retrieved
similar images into the prompt.
RAR~\cite{Liu2024RARRA} also incorporates the text associated with retrieved
similar images, but it re-ranks those texts using the MLLM before generation.
We set $k = 5$ as the number of retrievals for every RAG-based method.

\input{table/overall_result_no_indication}



\subsection{Datasets and Evaluation Metrics}

\subsubsection{Entity Probing}

We utilize two medical vision-language datasets: MIMIC-CXR~\cite{Johnson2019MIMICCXRAD}, containing chest X-ray images for radiology, and MultiCaRe~\cite{multicaredataset}, offering a variety of images across medical specialties.
We follow the official data split for MIMIC-CXR and randomly split MultiCaRe into train, validation, and test sets with a ratio of 8:1:1.
To construct VQA pairs for disease entity probing, we employ a biomedical named entity recognition (NER) model~\footnote{Stanza i2b2: https://stanfordnlp.github.io/stanza/biomed.html }~\cite{zhang2021biomedical} to extract all disease entities from the dataset's reports. 
We input these reports into LLMs (in our case, Llama-2 7B) to create closed-ended QA data with yes or no answers. 
For example, we ask ``\texttt{Does the patient have [disease entity] based on the report: [Report]?}", with answers formatted as \texttt{Yes/No}, simplifying error analysis. 
The use of LLMs allows for interpreting complex semantic structures within the text to accurately deduce potential answers. 
For instance, given the \texttt{[Report]}: ``An upper GI series on post-operative day 5 showing the duodenum ruling out stenosis.'' and \texttt{[disease entity]}: ``\texttt{stenosis}'', the LLM correctly answers ``\texttt{No.}''
By sampling segments from a medical report, we generate a sequence of concise, closed-ended questions paired with LLM-generated answers. The VQA dataset is then formed by associating these disease probing QA pairs with the original medical images.

For example, in MIMIC-CXR, we exclude entities in the ``INDICATION'' section of the report, as these reflect patient history or the reason for conducting the evaluation rather than X-ray findings.
Across both datasets, we found that less frequent entities are often already covered by more frequent ones (e.g., ``\texttt{right lower lobe atelectasis}'' as a particular kind of ``\texttt{atelectasis}''). Therefore, we map each entity to its shortest terminal subphrase occurring as an entity in the training set, to reduce redundancy and clarify entity frequency. 
For each test set of MIMIC-CXR and MultiCaRe, we parse 9,411 and 21,653 VQA pairs, respectively, with 385 and 10,434 distinct entities.
We use Precision, Recall, and F1 Score as the primary metrics to evaluate answer correctness in disease entity probing.

\subsubsection{Report generation}

We apply disease entity probing with V-RAG to mitigate hallucinations in generated text through a rewrite strategy. After a Med-MLLM generates an initial report of findings for an X-ray, the NER model extracts all disease entities from the generated report and from the reports of the $k$ most similar images.  For each entity, the query image is probed using the Med-MLLM with V-RAG.

The originally generated report and entity probing results are input to
a text-only LLM (Llama 3.1 70B chat), with the prompt:
\texttt{
Consider the following chest X-ray report from a junior radiologist:
-----begin report-----
[REPORT]
-----end report-----
A senior radiologist has inspected the X-ray image and answered the following questions:
-----begin questions----
[QUESTIONS AND ANSWERS]
-----end questions-----
Please rewrite the junior radiologist's report to reflect the senior radiologist's answers.}
We measure RadGraph-F1 scores \citep{delbrouck-etal-2024-radgraph}
of the findings of the original and revised reports.

