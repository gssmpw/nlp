\section{Related Work}
\paragraph{Medical Multimodal Large Language Models.}
Substantial advancements have been made in adapting MLLMs to medical imaging____. The primary focus has been on training these models for radiology tasks using medical images (like X-rays, MRIs, and CT scans) along with their textual descriptions/reports.
____ used GPT-4 to generate instruction-following data for fine-tuning, improving MLLMs' conversational ability for open-ended biomedical image inquiries.
% Researchers have developed information extraction frameworks to efficiently structure data from radiology reports tied to these images____. \yun{}
____ developed a foundation model for chest X-Ray interpretation with an image-text bridger to align modalities.
% Others have explored the depths of open-ended medical dialogues by using sophisticated, instruction-based datasets____.
However, we found that these medical multimodal foundation models still suffer from hallucinations.
We aim to mitigate this issue in Med-MLLMs through a visual-based Retrieval-Augmented Generation (RAG) approach, enabling these models to generate factually accurate answers.



\paragraph{Retrieval-Augmented Generation (RAG).} 
% RAG____ mitigates hallucination in LLMs by retrieving and integrating domain-specific knowledge from external databases, enhancing text generation with accurately aligned information and demonstrating their effectiveness in addressing this challenge____.
% However, RAG techniques have received minimal exploration in MLLMs. 
% Prior studies primarily enhance image captioning by reranking labels of retrieved images____ or directly incorporating texts from these images into prompts to improve generation____. 
% In healthcare, researchers have developed domain-specific retrieval pipelines____ and explored the optimal number of retrievals____ to ensure the factuality of Med-MLLMs.
% Yet, all these previous work retrieve top-k similar images using the query image, and only consider the text/label associated with the retrieved images.
% Without utilizing the rich visual information of retrieved images, MLLMs fail to learn the intricate relationship between image and text, potentially limiting RAG's performance.
RAG____ mitigates hallucination in LLMs by retrieving and integrating domain-specific knowledge from external databases, enhancing text generation with accurate, aligned information and effectively addressing this challenge____.
Despite RAG's popularity, very few studies have applied RAG to MLLMs. 
Prior studies primarily enhance image captioning by reranking labels of retrieved images____ or directly incorporating texts from these images into prompts to improve generation____. 
In healthcare, researchers have developed domain-specific retrieval pipelines____ and explored the optimal number of retrievals____ to ensure the factuality of Med-MLLMs.
All these previous works retrieve similar images based on the query image but consider only the text/label associated with the retrieved images.
Thus these methods assume that the retrieved images are perfectly interchangeable with the query image, which is not always the case. 

A more effective approach might involve comparing the query image with retrieved images and their reports, allowing the model to identify what is truly relevant for generation.  This is the ``V-RAG'' method of our paper.
____ attempted a similar approach with ``Coarse (I+T),'' though it performed worse than using only associated texts (``Coarse (T)'' in their Table 6), which they noted was likely due to limited multi-image reasoning in the MLLMs they considered. 
We address this by analyzing MLLMs trained for multi-image reasoning, and also by introducing an architecture and fine-tuning method to make single-image-trained MLLMs ``V-RAG-capable,'' enabling them to benefit from this approach.