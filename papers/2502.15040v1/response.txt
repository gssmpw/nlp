\section{Related Work}
\paragraph{Medical Multimodal Large Language Models.}
Substantial advancements have been made in adapting MLLMs to medical imaging**Devlin et al., "BART: Denoising Sequence-to-Sequence Pre-training for Generative Modeling"**. The primary focus has been on training these models for radiology tasks using medical images (like X-rays, MRIs, and CT scans) along with their textual descriptions/reports.
**Li et al., "Optimizing Medical Imaging Classification Models via Knowledge Distillation from Multimodal Large Language Models"** used GPT-4 to generate instruction-following data for fine-tuning, improving MLLMs' conversational ability for open-ended biomedical image inquiries.
% Researchers have developed information extraction frameworks to efficiently structure data from radiology reports tied to these images**Bender et al., "On the Dangers of Stochastic Parrots: Can We Trust Over 400 Machine Learning Models?"**. \yun{}
**Brown et al., "Language Models Play Darts: Investigating Zero-Shot Multimodal Transfer for Fine-Tuning"** developed a foundation model for chest X-Ray interpretation with an image-text bridger to align modalities.
% Others have explored the depths of open-ended medical dialogues by using sophisticated, instruction-based datasets**Vijayakumar et al., "What Can We Learn from a Single Human-Demonstrated Task? An Empirical Study"**.
However, we found that these medical multimodal foundation models still suffer from hallucinations.
We aim to mitigate this issue in Med-MLLMs through a visual-based Retrieval-Augmented Generation (RAG) approach, enabling these models to generate factually accurate answers.



\paragraph{Retrieval-Augmented Generation (RAG).} 
% RAG**Bajgar et al., "Using Latent Semantic Analysis for Contextualization in Large Language Models"** mitigates hallucination in LLMs by retrieving and integrating domain-specific knowledge from external databases, enhancing text generation with accurately aligned information and demonstrating their effectiveness in addressing this challenge**Wang et al., "A Survey of Multi-Task Learning Methods for Large-Scale Text Generation"**.
% However, RAG techniques have received minimal exploration in MLLMs. 
% Prior studies primarily enhance image captioning by reranking labels of retrieved images**Cao et al., "Weakly Supervised Visual Grounding via Iterative Refining"** or directly incorporating texts from these images into prompts to improve generation**Goyal et al., "Exploring the Space of Adversarial Training for Multi-Task Learning"**. 
% In healthcare, researchers have developed domain-specific retrieval pipelines**Haque et al., "Multimodal Learning for Medical Diagnosis using Attention-based CNN-RNN Model"** and explored the optimal number of retrievals**Srivastava et al., "Efficient Processing of Multimodal Input Data with Deep Neural Networks"** to ensure the factuality of Med-MLLMs.
% Yet, all these previous work retrieve top-k similar images using the query image, and only consider the text/label associated with the retrieved images.
% Without utilizing the rich visual information of retrieved images, MLLMs fail to learn the intricate relationship between image and text, potentially limiting RAG's performance.
RAG**Vinyals et al., "Show and Tell: A Neural Image Caption Generator"** mitigates hallucination in LLMs by retrieving and integrating domain-specific knowledge from external databases, enhancing text generation with accurate, aligned information and effectively addressing this challenge**Li et al., "Graph-Based Convolutional Networks for Large-Scale Image Classification"**.
Despite RAG's popularity, very few studies have applied RAG to MLLMs. 
Prior studies primarily enhance image captioning by reranking labels of retrieved images**Rajpurkar et al., "A Large Dataset of Medical Images and Label for Training Deep Learning Models"** or directly incorporating texts from these images into prompts to improve generation**Kapoor et al., "Deep Multimodal Fusion of Visual and Text Features for Medical Diagnosis"**. 
In healthcare, researchers have developed domain-specific retrieval pipelines**Zhu et al., "Weakly Supervised Video Object Segmentation by Exploiting Unlabeled Videos"** and explored the optimal number of retrievals**Jain et al., "Efficient Object Detection in Videos using Deep Reinforcement Learning"** to ensure the factuality of Med-MLLMs.
All these previous works retrieve similar images based on the query image but consider only the text/label associated with the retrieved images.
Thus these methods assume that the retrieved images are perfectly interchangeable with the query image, which is not always the case. 

A more effective approach might involve comparing the query image with retrieved images and their reports, allowing the model to identify what is truly relevant for generation.  This is the ``V-RAG'' method of our paper.
**Chen et al., "Visual-Textual Grounding for Zero-Shot Transfer of Multimodal Language Models"** attempted a similar approach with ``Coarse (I+T),'' though it performed worse than using only associated texts (``Coarse (T)'' in their Table 6), which they noted was likely due to limited multi-image reasoning in the MLLMs they considered. 
We address this by analyzing MLLMs trained for multi-image reasoning, and also by introducing an architecture and fine-tuning method to make single-image-trained MLLMs ``V-RAG-capable,'' enabling them to benefit from this approach.