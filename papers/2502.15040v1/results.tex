\section{Evaluation Results}


\subsection{Overall performance for existing multi-image-trained Med-MLLMs}
We first evaluate V-RAG's performance for existing Med-MLLM that originally trained on multi-image datasets. Table~\ref{tb:overall} shows entity probing results comparing our method to baselines.
Across both datasets, V-RAG outperforms text-only RAG baselines in F1 scores. This improves the model's ability to extract relevant information for decision-making. Furthermore, with our proposed fine-tuning tasks, V-RAG (fine-tuned) achieves superior F1 scores over both baselines and the un-fine-tuned version. This shows that we have significantly enhanced Med-MLLM's capabilities by equipping it with robust image-text association skills.




\subsection{Ablation study}
We now conduct ablation studies to better understand our proposed method across various configurations.

\paragraph{Multimodal retrieval.} 
Table~\ref{tb:modality} shows the F1 scores of RAG (top-5) across different retrieval modalities. We observe that providing only similar images without text makes it challenging for Med-MLLM to extract entity information from visuals, though it offers marginal improvements over Med-MLLM without RAG. Adding text for similar images significantly enhances performance, highlighting the rich information provided by texts in entity probing.
By integrating both modalities, V-RAG effectively links retrieved texts and images, enabling more comprehensive decision-making and achieving the best performance. This underscores the importance of multimodal retrieval in V-RAG, rather than relying solely on text as most existing MLLM baselines.

\input{table/ablation_modality}


\paragraph{Fine-tuning tasks for V-RAG.}
To enhance V-RAG's performance, we proposed three fine-tuning tasks for Med-MLLM, each with 6,000 instances. In Table~\ref{tb:ablationfinetune}, we examine how different combinations of these tasks impact performance.
% Compared to V-RAG without fine-tuning, we first enable Med-MLLM to learn from extracted similar information using only the $M_{vrag}$ dataset.
% The observed performance gains demonstrate that we can enhance the model's understanding of downstream V-RAG tasks. 
Initially, using only the $M_{vrag}$ dataset, we enable Med-MLLM to learn from extracted similar information, yielding performance gains that enhance the model's understanding of downstream V-RAG tasks.
% $M_{position}$ and $M_{focus}$, we observe that each contributes additional gains to V-RAG. We also found that $M_{position}$ offers more benefits to the model since $M_{focus}$, which requires the model to generate a full medical report, is more challenging to learn with the same quantity of data.
Adding the image-text association tasks $M_{position}$ and $M_{focus}$ provides further gains, with $M_{position}$ offering more benefits due to the complexity of $M_{focus}$, which involves generating a full medical report and is more challenging to learn with limited data.

\input{table/ablation_finetune}


\subsection{Analysis of entities across frequency levels}
In addition to analyzing the overall entities in the test set, we conducted an analysis to see how they differ in appearance. We categorized the entities from the test set into the most frequent 50 and the less frequent ones, analyzing their performance separately. Rare entities were almost exclusively found in positive contexts, which created a label imbalance. To address this, we balanced the test sets for rare entities by adding additional negative probing questions for each entity until the number of positive examples equaled the number of negative examples. Negative examples were paired with a randomly chosen image, and we verified using Llama-2 that the associated report did not suggest the presence of the entity. We tested 1,000 samples for both frequent and rare entities across two datasets.
Figure~\ref{fig:freq} shows the F1 scores for each test set. Our V-RAG method outperforms both the original method and the RAG baselines in both settings. The improvement of V-RAG over other methods in the rare entity setting demonstrates the practical utility of our approach, emphasizing its effectiveness in utilizing information from multiple modalities to answer queries that neither the original model nor text-based RAG methods could address.


\begin{figure}[t]
    \centering
    \setlength{\abovecaptionskip}{1mm}
    \includegraphics[width=0.9\linewidth]{figure/histogram.png}
    \caption{F1 performance of each method on disease entities with different frequencies. The superior performance of our method, particularly in probing rare entities, demonstrates its effectiveness and applicability in real-world scenarios.}
    % \vspace{-3mm}
    \label{fig:freq}
\end{figure}


% \begin{figure*}[t]
%     \centering
%     \setlength{\abovecaptionskip}{1mm}
%     \includegraphics[width=\linewidth]{LaTeX/figure/error_analysis.png}
%     \caption{An error made by V-RAG when probing for rare disease entities. V-RAG  retrieves similar images based on larger objects, such as catheters (highlighted in green in the reports). The retrieved image and report erroneously contain the target entity (text in blue), despite its absence in the ground truth report.}
%     \label{fig:error}
% \end{figure*}

% \subsection{Error Analysis}
% To further understand our method, we have identified some potential mistakes that could occur.
% Figure~\ref{fig:error} displays an example from the rare entity test set of the MIMIC-CXR dataset. The rare entities we aimed to probe did not have negative ground truth data available. Therefore, we randomly selected an image for this rare entity and verified the absence of this entity in the image's report using Llama 2. Based on the query image, V-RAG extracts similar images, and we observed that this extraction may be influenced by larger objects, such as catheters (indicated in green texts), in the X-ray images. The reports of these similar images could contain the rare entity we need to probe, potentially leading V-RAG to incorrectly answer `Yes' to a negative sample. In the future, it would be beneficial to consider more granular retrieval methods to enhance the precision and effectiveness of V-RAG.






\input{table/llava}
\subsection{Can we make a single-image-trained MLLM V-RAG-capable?}
% We now evaluate the effectiveness of the objectives proposed in Section 3 for enabling the single-image-trained MLLM V-RAG capable.
% We conduct a single-image dataset $S$ using all the single-image-report pairs from the MIMIC-CXR train set, resulting in 100,098 samples. 
% We then fine-tune LLaVA~\cite{Liu2023ImprovedBW} on $S$ using LoRA for one epoch, resulting in a single-image MLLM denoted as LLaVA$_S$.
% Building on LLaVA$_S$, we further fine-tune using the tasks introduced in Section 3 to enable its multi-image capabilities. 
% Extending from single-image dataset $S$, we extract 10k samples for each fine-tuning task, creating the multi-image datasets $M_{position}$, $M_{focus}$, and $M_{vrag}$.
% The models fine-tuned on these datasets are denoted as LLaVA$_{\{task\}}$.
After observing the performance gains of V-RAG and our fine-tuning methods on multi-image pre-trained Med-MLLMs, we now explore whether single-image-trained MLLMs can also be enabled to perform V-RAG.
We extract all single image-text pairs from the MIMIC-CXR training set to create the single-image dataset $S$, resulting in 100,098 samples.
We then fine-tune LLaVA-v1.5-7B with Vicuna backbone~\cite{Liu2023ImprovedBW} on $S$ using LoRA for one epoch, resulting in a single-image Med-MLLM denoted as LLaVA$_S$.
From the single-image dataset $S$, we extract 10k samples for each fine-tuning task in Section~\ref{sec:v-rag-method}, creating the multi-image datasets $M_{position}$, $M_{focus}$, and $M_{vrag}$.
We then fine-tune LLaVA$_S$ on these tasks, producing the model LLaVA$_{\{task\}}$.


To evaluate our idea, we conducted entity probing on the MIMIC-CXR test set. For the single-image model LLaVA$_S$, we input a single test image to probe for a disease entity.
For the multi-image model LLaVA$_{\{task\}}$, we implemented V-RAG to assess its performance and determine if it can be effectively V-RAG capable with our designed tasks.
We set the context length of LLaVA to be 4096 and consider the top-3 retrievals for LLaVA$_{\{task\}}$ when performing V-RAG.

Table~\ref{tb:llava} shows the entity probing performance of single-image-trained MLLM and MLLM with multi-image capabilities resulting from our proposed fine-tuning tasks. 
For the single-image model LLaVA$_S$, we input a single test image and tasked the model with probing for a disease entity based on the given image. 
For the multi-image model LLaVA$_{\{task\}}$, we perform V-RAG to assess its performance. We set the context length of LLaVA to be 4096 and consider the top-3 retrievals for LLaVA$_{\{task\}}$ when performing V-RAG.
Results demonstrate that, with the support of our designed fine-tuning tasks, we enable the single-image-trained MLLM to effectively perform V-RAG.


% Table~\ref{tb:llava} shows the entity probing performance of single-image-trained MLLM and MLLM with multi-image capabilities resulting from our proposed fine-tuning tasks. 
% By comparing LLaVA$_S$ and LLaVA$_{\{M_{position} + M_{focus} + M_{vrag}\}}$, we demonstrate that, with the support of all three tasks, we have enabled the MLLM to effectively perform V-RAG, utilizing multiple images and documents to enhance probing capabilities for the query image.
% We can also observe how each task influences performance and have identified the superiority of each. These results demonstrate that we can make a single-image-trained MLLM, LLaVA in our case, V-RAG-capable.

\subsection{Improving generated reports}

\input{table/reportgen}

% Entity probing is not generally the final task for an MLLM, so it is important to show the utility of V-RAG for report generation.
We have shown that disease entity probing provides a valuable clinical perspective on model outputs. 
However, since entity probing is typically not the final task for an MLLM, it is essential to demonstrate the utility of V-RAG in report generation.
We find that our strategy using Llama 3.1 70B Chat to rewrite the
generated reports using the V-RAG entity probing results yields 19\% relative
improvements in the simple and partial RadGraph-F1, compared to
the original findings, as shown in Table~\ref{tab:reportgen}.
These results highlight the practical benefits of V-RAG-enhanced entity probing, demonstrating its value not only in probing accuracy but also in improving the accuracy of generated medical reports.

