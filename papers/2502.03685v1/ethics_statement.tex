We adhere to the ICLR Code of Ethics. Additionally we confirm that our experiments use only public datasets.
The algorithm introduced in this work is a general-purpose algorithm for directing LLMs to generate text satisfying arbitrary constraints. \
Thus it is possible to define malicious constraints that cause LLMs to produce harmful text. 
Similar to the work done in \citep{guo2024cold}, it may be possible to apply our algorithm towards jail-breaking LLMs and causing them to produce harmful text. 
Previous works have demonstrated that it is possible to induce harmful behavior in LLMs via various attacks \citep{he2024talk, liu2023prompt, schwinn2023adversarial}. 