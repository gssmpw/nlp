Our goal is to formulate the target distribution in such a way that enables both auto-regressive biasing and discrete sampling, allowing for superior exploration of the discrete space of fluent and controlled responses.
However, existing formulations will not allow for both: while \citet{kumar2022gradient} introduces a framework that allows for direct sampling of word embedding, it is non auto-regressive; and while \citet{liu2023bolt} introduces a method that allows for auto-regressive generation, the bias vectors used are continuous. Thus here we introduce a new formulation. 

\paragraph{Target Discrete Distribution} 
In order to ensure the constraint satisfaction of the output sequence $Y$, we introduce an auxiliary variable $B = \{b_1, b_2 \dots b_n\}$, where each $b_i \in V$. We refer to this sequence as the bias sequence or bias tokens. 
First, we define the joint distribution over $Y, B$ conditioned on the prompt $X$: 
\begin{align}
\label{eq:dab-target-dist}
    P(Y, B | X) \propto P^{LM}(Y | X, B) \exp(f(B | X)).
\end{align}
Here, $f(B | X)$ represents the constraint, with larger values of \(f(B | X) \) indicating better satisfaction of the constraint. $P^{LM}$ refers to the language model distribution conditioned on $X, B$. 

\paragraph{Marginal Distribution $P(Y | X)$}
The marginal distribution of \( Y \) is:
\[
P(Y | X) = \sum_{B \in |V|^d} P(Y | X, B) P(B | X) =  \sum_{B \in |V|^d} P(Y | X, B) \frac{\exp(f(B | X))}{Z_B}.
\]
This formulation expresses the probability of obtaining the response \( Y \), taking into account all possible biases \( B \). The response \( Y \) drawn from this marginal distribution will be both fluent (due to the term \( P^{LM}(Y | X, B) \)) and highly satisfying of the constraints (due to \( \exp(f(B | X)) \)). The bias variable \( B \) helps balance these two aspects. As the distribution over $Y$ incorporates both $P^{LM}$ and the external constraint, probable sequences under this distribution will be both fluent and satisfactory. This ensures that the generated response $Y$ from our algorithm has the desired properties.

\paragraph{Marginal Distribution $P(B|X)$}

The marginal distribution of \( B \) is given by:
\begin{align}
    p(B|X) = \frac{\exp(f(B | X))}{Z_B}
\end{align}
This indicates that highly probable values of $B$ will satisfy the external constraint. However, since the distribution does not incorporate the language model $P^{LM}$, the sequence of $B$ may not be fluent. For this reason, we use $B$ as a ``guide'' sequence as opposed to using it as the response. 

\paragraph{Conditional Distributions}
Given the joint distribution defined in \eqref{eq:dab-target-dist}, the conditional distribution of \( Y \) is $P(Y | B, X) = P^{LM}(Y | X, B)$. 
Furthermore, the conditional distribution of $B$ is:
\[
P(B | X, Y) = \frac{P^{LM}(Y | X, B) \exp(f(B | X))}{P(Y|X)} \propto P^{LM}(Y | X, B) \exp(f(B | X))
\]

% \paragraph{Why introducing the bias $B$?} 
\paragraph{Advantages of Joint Distribution}
% We introduce a bias term $B$ and define our target distribution jointly over $Y, B$. In contrast to the typical target shown in equation \eqref{eq:prior-energy-def}, our formulation enables the use of both auto-regressive generation and gradient-based discrete sampling, ensuring both fluency and constraint satisfaction. 

The primary motivation behind our framework is the observation that fluency is best satisfied through auto-regressive generation, and gradient-based sampling efficiently finds responses that satisfy constraints. By framing the problem as a joint distribution of $Y, B$, we enable the use of both methods. The typical target shown in equation \eqref{eq:prior-energy-def} does not support auto-regressive generation and significantly compromises fluency. 
