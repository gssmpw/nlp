Here we discuss various advantages of discrete sampling in the context of auto-regressive biasing. First, we demonstrate that discrete sampling enables a quicker and more thorough exploration of potential output sequences $Y$. We then describe how discrete sampling solves the stability issue discussed in \citet{liu2023bolt}. Finally, we show that discrete sampling makes use of simpler gradient computations, resulting in a more efficient decoding algorithm.

\paragraph{Exploration of State Space}
Discrete sampling enables DAB to explore the output space more effectively than continuous methods.
We hypothesize that discrete sampling enables more directional and substantial changes to the bias vector, resulting in more token changes in the output sequence across sampling steps. 
We compare with BOLT, a continuous auto-regressive biasing algorithm \citep{liu2023bolt}. 
We examine the performance of BOLT both with and without the normalizing factor defined in \eqref{eq:dab-normalize}. 
We include the comparison of hops across 50 steps in Figure \ref{fig:samplespace_explore}a.
These results show that our method updates substantially more sequence positions across all sampling steps than either variant of BOLT. 

Next, we measure how comprehensively each method explores the sample space of potential sequences. 
For each sequence position, we maintain a record of tokens encountered throughout the sampling process and compute the number of unique tokens within this set. 
Figure \ref{fig:samplespace_explore}b shows the average unique tokens per sequence position for all three algorithms. 
These results indicate that our method samples more unique tokens for each sequence position than either variant of BOLT, demonstrating more comprehensive exploration. 
Collectively, these findings confirm that discrete sampling enables faster, more thorough, and thus more effective exploration of the sample space of potential sequences. 
\paragraph{Sampling Stability}
Discrete sampling allows DAB to have superior stability across sampling steps when compared to continuous methods. 
We show this in Figure \ref{fig:samplespace_explore}c, where we track the average perplexity of the batch at each time step. 
While BOLT faces deteriorating perplexity, DAB remains stable throughout the sampling process. 

We attribute this instability to the difficulty of applying continuous sampling techniques to a discrete domain as discussed in \citet{grathwohl2021gwg}. 
As a result of BOLT's misalignment between the sampling domain and target domain, the energy landscape is too complex to navigate with gradient information.
This results in the divergence seen in Figure \ref{fig:samplespace_explore}c. 

\input{Main_Body/disc_bias_precision_fig}

Our algorithm avoids this entirely as we perform direct sampling on the discrete token space. 
Since we define the sampling domain and target domain to be the same, our algorithm enjoys superior stability throughout all sampling steps. 
This improvement in algorithmic stability removes the need for implementing early-stopping and to carefully tune the number of sampling steps. 
\vspace{-1em}
\input{Experiments/speed_table} 
\vspace{-0.8em}
\paragraph{Improved Efficiency} Discrete sampling enables our algorithm to use simpler gradient computations that provide a computational advantage over continuous sampling methods. 
To evaluate our algorithm's efficiency, we compare the tokens per second of our method to BOLT. 
We also measure the time-cost for computing the bias term for each method. 
We put the results in Table \ref{table:speed-table}, where we observe that our method has over \textbf{2x} the tokens per second output when compared against BOLT. 
Our algorithm achieves this computational advantage as a result of computing the gradient with respect to $\hat{B} = \hat{Y}$, which removes the need to backpropagate through auto-regressive generation. Computing the gradient of $f$ with respect to a continuous bias term $\tilde{B}$ requires first computing $\pderiv{f}{\hat{Y}}$ and then $\pderiv{\hat{Y}}{\tilde{B}}$.
Since each one-hot vector in $\hat{Y}$ is influenced by previous bias terms, the latter term requires backpropagation through auto-regressive generation. 
Simply initializing $\tilde{B}=\hat{Y}$ will not work in continuous sampling because the incremental updates will keep $\tilde{B}$ close to the original $\hat{Y}$. 
In contrast, our method uses gradients to identify which tokens will increase constraint satisfaction and directly samples them, enabling substantial change from the original sequence while incorporating information from the external constraint. 
While continuous sampling cannot exploit this computational shortcut and maintain constraint satisfaction, gradient-based discrete sampling achieves both simultaneously. 
