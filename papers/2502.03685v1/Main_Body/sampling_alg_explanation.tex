Sampling directly from the joint distributions of $Y, B$ is challenging. We propose to use Gibbs sampling to sample from the target distribution by alternatively sampling from $P(Y | X, B)$ and $P(B | X, Y)$. First, we describe how our proposed algorithm samples from each conditional distribution. We then introduce the complete sampling algorithm along with some intuition as to how it ensures satisfactory and fluent generations. 

\paragraph{Sampling from $P(B | X, Y)$} 
The conditional distribution for $P(B | X, Y)$ includes the term $P^{LM}(Y | X, B)$. This is to ensure that the sampled $B$ results in output sequences that are high in likelihood under the base model distribution. 
However, directly computing \( P^{LM}(Y | X, B) \) for all possible values of \( B \) is intractable. 
By noting that this term encourages the selection of \( B \) that is consistent with the observed \( Y \), we can infer this property is naturally satisfied when $B$ is close to $Y$. 
Thus, we approximate \( P^{LM}(Y | X, B) \) by performing a single MCMC step with the initial state set to $Y$. 

In order to sample this discrete sequence of tokens, we apply the Discrete Langevin Proposal (DLP) introduced in \citep{zhang2022langevinlike}. 
After initializing $B$ as $B = Y$, and representing the sequence as a sequence of one-hot vectors $\hat{B} = \{\hat{b}_1, \hat{b_2} \dots \hat{b}_n\}$, we execute a single step of DLP with the target distribution being $\exp(f(B | X))$. Below we include the proposal distribution for position $i$:
\begin{align}
\label{eq:dlp_prop}
    b'_i \sim \categorical\left(\underset{j \in |V|}{\softmax} \left( \frac{1}{\tau} (\nabla f(\hat{B} | X))_{i,j} (1 - \hat{b}_{i,j}) \right) \right)
\end{align}
Here, $\tau$ is a temperature hyper-parameter that controls the sharpness of the proposal distribution, $(\nabla f(\hat{B} | X))_{i,j}$ is the $j$th component of the $i$th gradient vector, $\hat{b}_{i,j}$ represents the $j$th component of the one-hot vector $\hat{b}_i$, and $b'_i$ is the token we sample from the distribution over $V$. 
For more details on the application of DLP and the gradient compution, see Appendix \ref{appndx:dlp_proposal}.
Unlike the algorithm presented in \citet{liu2023bolt}, we do not require the use of straight through estimation (STE) \citep{bengio2013estimating} as we differentiate directly with respect to $Y$.
Note that this proposal function can be computed for all sequence positions in parallel. 
We refer to this proposal function as $q_\tau(\cdot | B)$. 

\paragraph{Sampling from $P(Y | X, B)$}
Our goal is to sample from $P(Y | X, B)$ using biased auto-regressive generation, similar to \eqref{eq:bolt-auto-reg}. 
In order to do so, we must map the sequence of bias \textit{tokens} $B$ to a sequence of bias \textit{vectors} $\tilde{B}$.  
The ideal bias vector should reflect the difference in meaning between each token in the vocabulary space $V$ and the sampled token.
To accomplish this, we penalize each token based on the distance to the sampled bias token within the embedding space, as static embeddings reflect semantic meaning \citep{mikolov2013efficient, pennington2014glove, mikolov2013distributed}. Given a bias token $b_i$, embedding table $M$, we define the $j$th coordinate value corresponding to token $v_j$ as follows: 
\begin{align}
\label{eq:bias-vec-def}
    \tilde{b}_{i,j} = || Mb_i - Mv_j||^2_2 
\end{align}
This yields a $|V|$ dimensional vector that can be added to the auto-regressive logits $\tilde{y_i}$. 
When adding the bias term to $\tilde{y_i}$, we also incorporate both a weight term $w_i$ and a normalizing factor $r_i$. While $w_i$ is a hyper-parameter, $r_i$ normalizes the bias vector at the $i$ position to have the same norm as $\tilde{y_i}$. We define the normalizing factor as follows: 
\begin{align}
    \label{eq:dab-normalize}
    r_i = \frac{|| \tilde{y_i} ||_2}{|| \tilde{b}_i ||_2}
\end{align}
We note that while this normalizing factor can also be applied to BOLT and may improve its results, the modified BOLT still underperforms compared to our method. 

We formally define our biased auto-regressive generation as follows: 
\begin{align}
    \label{eq:dab-auto-reg}
    y_i = \argmax_{j \in |V|} \left( \tilde{y}_{i, j} - w_i \cdot r_i \cdot \tilde{b}_{i, j}\right). 
\end{align}
Intuitively, this returns the token corresponding to the maximal coordinate of the biased distribution. Repeating this $n$ times results in the updated response sequence $Y$. 
\input{Main_Body/diagram_fig}

\paragraph{Text Generation Algorithm} We provide a visualization of our algorithm in Figure \ref{fig:diagram}, and include the full algorithm in Appendix \ref{appndx:algrthm-details}. Given some prompt, we first generate some initial auto-regressive generation $Y_1$, with the initial bias vector set to $\Vec{0}$. After obtaining $Y$, we sample from the conditional distribution over $B$ to obtain a sequence of bias tokens. We then use \eqref{eq:bias-vec-def} to compute the new bias vector to use for biased auto-regressive generation. 
We repeat this alternative sampling process for several iterations, returning the sample that best satisfies the constraint at the end as commonly done in the literature \citep{kumar2022gradient,liu2023bolt}. 
For a discussion on the hyper-parameters of our algorithm, see Appendix \ref{appndx:ablation}. 