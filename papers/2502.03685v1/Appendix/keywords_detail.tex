\paragraph{Experiment Design}
We largely follow the experimental design introduced in \citet{liu2023bolt}. We list the topics and keywords in \ref{table:kw-topics}. 

\paragraph{Constraint Function} Following \citet{liu2023bolt}, \citet{qin2022cold}, we use the differentiable BLEU score introduced by \citet{liu-etal-2022-dont}. 
This function measures the uni-gram similarity between the generated sentences and the target key-words, using an operation very similar to convolution. 


\input{Appendix/kw_topic_table}
\paragraph{Reference Text Generation} We use GPT-4o to generate high-quality reference text to use in the BertScore computation. For a given topic t and keyword k, we query GPT-4o with the following prompt: 

\textit{Given the topic t and the keyword k, write 30 different, unique sentences using the keyword and relevant to the topic.}

We do this for each topic and for every keyword for that topic. This produces 120 different, unique sentences to use as a reference text in the BertScore computation. 

\paragraph{BertScore Computation Details}
We use the BertScore computation introduced in \citet{zhang2020bertscoreevaluatingtextgeneration} to evaluate the topicality of the generations. Since BertScore relies on the contextualized embedding of the candidate generations and the reference text, this provides insight into how well the methods use the keyword in the desired context. 

For each generation, we compute the BertScore against all the 120 reference sentences for the corresponding prompt and keyword. Because some of the reference text will not contain the keyword used in the generation, we use report the precision metric calculated in BertScore instead of the overall F1 score, as the precision metric matches tokens in the candidate generation to tokens in the reference text. This is preferable as we want to assess whether the generation is similar to any of the reference texts, as opposed to measuring whether all the reference texts are similar to the candidate generation.  

\paragraph{Implementation Details}
We found that in order to obtain good results with DAB on this task, it was necessary to include a string containing the keywords prior to the prompt. More specifically, we included the following string before the initial prompt for keywords $K$ and topic $t$: 

\textit{Include the following keywords: K relevant to t.}

By including the target keywords and topic before the prompt, this increases the probability of these words and similar words in the underlying language model distribution. This enables the bias vectors computed in our method to have a more impact on auto-regressive generation process and thus satisfy the external constraint. 

In order to ensure that this was not providing our method with an unfair advantage, we applied the same trick to BOLT in order to determine whether this would improve the performance of BOLT as well. We provide results in Table \ref{table:kw-prompted-comp}. 
\input{Appendix/bolt-prompt-comp}

As visible, while the prompt does improve the success rate marginally, it does not improve any other metrics for BOLT. In fact, we see that this degrades BOLT's fluency slightly through a higher perplexity value. 
\paragraph{Examples} In Table \ref{appendix:tab:kw-gens} we show examples of generations for the various samplers we examine. 
\input{Appendix/generations_kw}
