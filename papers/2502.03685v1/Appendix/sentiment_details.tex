\paragraph{Experiment Design} We use the same experimental design from \citet{liu2023bolt}, where the sampler uses an internal classifier to produce the generations. The internal model is a RoBERTA with GPT2-Large Embeddings fine-tuned on the yelp polarity dataset. We use two external models to provide additional evaluation: we use another RoBERTA trained on the same dataset but with the original embeddings, as well as a RoBERTa fine-tuned on Stanford Sentiment Treebank 2. 

We include the hyper-parameters we use for DAB in Table \ref{appndx:tab:exp-hyperparam}. For the baselines, we run the code within their codebase. While we minimize the changes made to the original code, we note that there are some necessary modifications needed in order to ensure that the experimental setting is consistent across all methods evaluated. This due to the fact that all the evaluated methods consider similar but slightly different experiments from ours in their original work \citep{qin2022cold, liu2023bolt, han2023lm, kumar2022gradient}.  

In regards to LM-Steer, which requires training data, we train the steering matrix using the SST-2 dataset, as done in \citet{han2023lm}. While this is a different dataset from what was used to fine-tune the internal classifiers for the EBM sampling methods, we choose this dataset as obtained worse results when training the steer matrix on yelp polarity. Furthermore, we include an external classifier fine-tuned on SST-2 to use as an evaluation criteria. This makes our experiments fair, as all the methods are evaluated with classifiers that are fine-tuned on a different dataset than used for sampling. Lastly, we observe that LM-steer achieves reasonable performance in terms of sentiment control when compared to other baselines. 

Here we list the prompts we use for this experiment: 

\paragraph{External Constraint} To represent the internal constraint, we use a RoBERTA with GPT-2 large embeddings fine-tuned on Yelp-Polarity for COLD, BOLT, MuCOLA, and DAB. We train this model following the codebase of \citet{liu2023bolt}. Since we require the embedding table to be the same between the base LM, we use the GPT2-large embeddings for the classifier, as done in \citet{liu2023bolt, kumar2022gradient}. 
% The final classifier achieves an accuracy of $96\%$ on the hold-out. 

We use a slightly different function to represent the constraint imposed by the fine-tuned model when compared to BOLT. Given the discriminator $h: |V| \to \mathbf{R}^2$, where the results represent the logits for both the desired class $c_{+}$ and the undesired class $c_{-}$, we define the final constraint function as follows: 
\begin{align*}
    f(Y) = (h(Y)_{+} - h(Y)_{-})
\end{align*}
Intuitively, this pushes the unnormalized logits between the desired class and the opposite class away from each other.

This differs from the constraint function in BOLT, which is the typical cross-entropy loss of the discriminator logits where the correct label is the desired sentiment: 
\begin{align*}
    f(Y) &= \log \softmax (h(Y)_{+})
\end{align*}

We find that our formulation of the constraint function enables more effective gradients for our specific method. 
Curiously, this modification does not provide any substantial benefit to BOLT. 
It is possible that the $\log \softmax$ of BOLT's method smooths out the directional information of the gradient. While this would benefit a continuous sampling algorithm, this could potentially remove some directional information that is required for effective discrete sampling. 

\paragraph{Example Generations} In Table \ref{appendix:tab:senti-gens} we include examples of generations for all methods evaluated.  
\input{Appendix/generations_senti}