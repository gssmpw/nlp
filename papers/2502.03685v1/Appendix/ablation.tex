
To provide further insight into our algorithm, we present an ablation study over the important hyper-parameters. We demonstrate the robustness of our algorithm to various settings, as well as the hyper-parameters that are important towards good performance. We include the results in Figure \ref{fig:abl-res}.

\paragraph{Bias Weight} As visible in Figure \ref{fig:abl-res}a, increasing the weight term in \eqref{eq:dab-auto-reg} leads to increased control over generation at the expense of fluency. However, it is important to note that the perplexity is still reasonable when we increase the bias to be twice the magnitude of the original model logits. 

\paragraph{Proposal Temperature} In Figure \ref{fig:abl-res}b, we examine how varying the temperature $\tau$ in \eqref{eq:dlp_prop} effects the performance of our algorithm. Intuitively, $\tau$ controls the ``sharpness" of the proposal distribution, with larger values correspending to flatter peaks and lower values correspending to sharper peaks. This effectively controls the degree of exploration v.s exploitation in the DLP sampler. We see this relation in the results shown in Figure \ref{fig:abl-res}b, where higher values result in lower control towards the desired sentiment as a result of increased exploration. We also see that there needs to a certain amount of exploration in order to find satisfactory generations, as decreasing the temperature too much results in lower control values as well. 

\paragraph{Top-k Value} In order to further ensure fluency and constraint satisfaction of our algorithm, we restrict the DLP proposal in \eqref{eq:dlp_prop} to sample only from the Top-$k$ tokens for each position as indicated by the base language model, where $k$ is a hyper-parameter that can be tuned. In Figure \ref{fig:abl-res}c, we observe a similar tradeoff between exploration and exploitation: if the $k$ value is too high, than the algorithm will not exert as much control over the generation process. If the $k$ value is too low, then the algorithm won't be able to explore enough sequence combinations to find good modes. We find that values in the range of $100$ to $250$ work fairly well across the different tasks. 

\paragraph{Algorithmic Robustness} Throughout the ablation, it is clear that DAB is capable of achieving strong performance across a range of reasonable hyper-parameter values.
Furthermore, this performance does not come at the cost of fluency -- the only hyper-parameter that enables for a tradeoff between fluency and control is the weight value, and here we see that even large values for this hyper-parameter result in reasonable perplexity compared to the results in Table \ref{table:megatable}. 
Thus we see that our algorithm is fairly robust to various hyper-parameter settings. 
 
\input{Appendix/abl_res_fig}