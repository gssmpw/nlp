\section{Discrete Langevin Proposal}\label{appndx:dlp_proposal}
Our proposed controlled text generation leverages the gradient-based discrete sampling algorithm in \citet{zhang2022langevinlike}, which is further investigated by \citet{pynadath2024gradientbaseddiscretesamplingautomatic}. Using the same notation as in the Main Body of the paper, we put the original proposal distribution from \citet{zhang2022langevinlike} below:
\[
\text{Categorical} \left( \underset{j \in |V|}{\softmax} \left( \frac{1}{2} \nabla f(\hat{B} | X)_i (\text{Onehot}_j - \hat{b}_i) - \frac{||\text{Onehot}_j - \hat{b}_i ||^2_2}{2\alpha}\right) \right)
\]
Here, $\hat{b}_i$ corresponds to the one-hot vector in sequence position $i$. Similarly, $\text{Onehot}_j$ corresponds to the one-hot vector for the $j$th token in $V$. This proposal function defines a distribution over the vocabulary for the $i$th sequence position in the sequence by taking the softmax over all possible tokens.

As discussed in \citet{pynadath2024gradientbaseddiscretesamplingautomatic}, this proposal is locally balanced, or optimal for very small step-sizes. For the task of controlled text generation, we would prefer a proposal function that is optimal for large step-sizes, which allow for superior exploration of the space of potential sequences. The globally balanced proposal can be written as follows: 
\[
\text{Categorical} \left( \underset{j \in |V|}{\softmax} \left( \nabla f(\hat{B} | X)_i (\text{Onehot}_j- \hat{b}_i) \right) \right)
\]
In terms of the gradient computation, the one-hot representation  enables the use of automatic differentiation packages to compute $\nabla f(\hat{B} | X)$. We observe that the term 
$(\text{Onehot}_j - \hat{b}_i)$ corresponds to the distance between the proposed token $j$ and the original token $b_i$. We choose to represent this distance term as hamming distance, given the discrete nature of the space we wish to sample. For a token $j$, the hamming distance to the original token in position $i$ is 0 if the $j$th coordinate $\hat{b}_{ij} = 1$ as they are the same token; and 1 if the $j$th coordinate is 0. Thus we can represent the distances between the tokens as $1 - \hat{b}_{ij}$. This leads us to the proposal function in \ref{eq:dlp_prop}, which we place below for convenience: 
\[
b'_i \sim
    \categorical\left(\underset{j \in V}{\softmax} \left( \frac{1}{\tau} (\nabla f(\hat{B} | X))_{ij} (1 - \hat{b}_{ij}) \right) \right)
\]
Here, $b'_i$ refers to the token we sample from the categorical distribution over $V$. 

\section{Algorithmic Details} \label{appndx:algrthm-details}
Here we provide the full pseudo-code for our algorithm. 
\input{Main_Body/sampling_alg}

DAB takes as input the external constraint $f$, the base language model $P^{LM}$, prompt $X$, number of steps $s$, sequence length $n$, and embedding table $M$. Given these inputs, our proposed algorithm alternates between auto-regressively generating the response sequence and sampling the bias sequence using Discrete Langevin Proposal (DLP) \citep{zhang2022langevinlike}. 

\section{Ablation Study}
\label{appndx:ablation}
\input{Appendix/ablation}

\subsection{Efficiency}
\label{appndx:efficiency}
\input{Appendix/efficiency}

\section{Experimental Details}
Here we include additional details on the experiment setup. We provide the hyper-parameter settings for our algorithm  for each experiment in Table \ref{appndx:tab:exp-hyperparam}. It should be noted that for Sampling Steps, we pick values to maintain roughly the same time cost as BOLT: given that our algorithm is roughly twice as fast, we use around twice the number of sampling steps. Furthermore, given the use of early stopping in BOLT, further computational budget doesn't necessarily provide any advantage. 

For the weight value, we use a schedule by \citet{liu2023bolt} as it was shown to be effective in terms of incorporating the bias term into auto-regressive generation. Thus for each position $t$, we have $w_t = w(1 - \frac{t}{L})$, where $w$ is the value we put in Table \ref{appndx:tab:exp-hyperparam}. 
\input{Appendix/hyper_param_table}

\subsection{Fluency Metrics}
\label{appndx:fluency-metrics}
Here we provide more details as to the metrics we use to evaluate the fluency of text generations. 
\paragraph{CoLA Score} To assess the grammatical correctness of a generation, we use a fine-tuned RoBERTa model from \citet{morris2020textattack} to predict the probability of the sample being labelled as grammatically correct. While a similar metric was used in \citet{kumar2022gradient}, we compute the average predicted probability as opposed to the percentage over generations predicted as fluent since this provides more insight into the degree of grammatical correctness. 

\paragraph{Repeated Tri-grams} To compute the number of repeated tri-grams, we simply count all the tri-grams that were repeated and divide them by the total number of tri-grams per generation. We show the average across all generations for each metric. 

\paragraph{Perplexity} For perplexity, we use the built-in function within the Hugging Face evaluate package to compute the perplexity of each generation according to GPT2-XL \citep{wolf2020huggingfacestransformersstateoftheartnatural}. We show the perplexity of the \textbf{entire} generation, as opposed to conditioning on the prompt as done in \citet{han2023lm, kumar2022gradient, liu2023bolt}. 

\subsection{Sentiment Controlled Generation}
\label{appndx:senti-details}
\input{Appendix/sentiment_details}

\subsection{Toxicity Controlled Generation}\textcolor{red}{Content Warning: The following section contains examples of LLM generations that some may find disturbing or offensive, including content that is hateful or violent in nature}

\label{appndx:toxicity-details}
\input{Appendix/detoxic_details}

\subsection{Keyword Controlled Generation}
\label{appndx:keywords-details}
\input{Appendix/keywords_detail}
