\paragraph{}

\paragraph{Efficiency Improvements}
Here we further discuss the comparison of efficiency between BOLT and DAB.
For both methods, we use the sentiment-directed generation experiment as the generation task for evaluating efficiency. 
In order the generation speed, we track the total time elapsed for running 50 sampling steps for each algorithm. 
\paragraph{Tokens per second} Given this time $t$, sequence length $n$, and iterations per algorithm $s$, we compute the following: 
\begin{align}
    \text{TokensPerSecond} = \frac{n \cdot s}{t}
\end{align}

For computing the cost per bias sampling for both algorithms, we time only the operations that compute the gradient of the loss and update the bias term. We take the average of this over 50 sampling steps and 15 prompts. 

\paragraph{Bias Sampling Cost} We use the AutoGrad profiler within Pytorch to count the operations involved in both BOLT and DAB's gradient computation. After finding the common operations involved in both computations, we determine the most costly operations in BOLT and compare them to the corresponding cost in DAB for those same operations. We show the number of calls as well as the total time spent on GPU for BOLT's three most costly operations in \ref{table:common_grad_computations}. It should be noted that using the profiler increases the run-time of all GPU operations, hence why the values will not correspond to the total run times recorded in \ref{table:speed-table}.
\input{Appendix/grad_computation_table}

As visible, the gradient computation for BOLT is significantly more expensive than the gradient computation for DAB. 