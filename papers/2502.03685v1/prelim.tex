\paragraph{Task Definition} Let $P^{LM}$ denote some pre-trained language model, $V$ denote the set of tokens in the model vocabulary, and $f: |V|^n \to \mathbb{R}$ represent an external constraint where higher values correspond to better constraint satisfaction. We will use $Y = \{y_1, y_2 \dots y_n\}$ to refer to the sequence of tokens in the response. 

We assume $P^{LM}$ is an auto-regressive transformer as used in previous work \citep{qin2022cold, kumar2022gradient, liu2023bolt}. Given some initial prompt $X$, we define the auto-regressive distribution for the $i$th position as $ \tilde{y}_i = P^{LM} ( \cdot | y_{<i}, X)$. This forms a distribution over the vocabulary $V$ and can be represented as a $|V|$ dimensional logit vector. 

We define the task of controlled language generation as generating a sequence of $n$ tokens $Y = \{y_1, y_2, \dots y_n\}$ from some initial prompt $X = \{x_1, x_2 \dots x_m\}$ that is both high in likelihood under the language model and high in terms of constraint satisfaction. We can compute the likelihood of the generation under the pre-trained language model for a sequence of length $n$ as follows: 
\begin{align}
    P^{LM}(Y | X) = \Pi_{i < n} \ P^{LM}(y_i | y_{<i}, X).
\end{align}

Previous works have framed this problem as sampling from an unnormalized distribution, commonly referred to as an energy based model (EBM) \citep{mireshghallah-etal-2022-mix, qin2022cold, kumar2022gradient, liu2023bolt}. The \textit{energy function} defines an unnormalized distribution and is typically defined as follows: 
\begin{align}
    E(Y) = \lambda_1 \log P^{LM} (Y | X) + \lambda_2 f(Y|X)
    \label{eq:prior-energy-def}.
\end{align}
\paragraph{Non Auto-regressive Generation} As the denominator, or partition function, requires computing the energy of all possible sequences, it is intractable to directly sample from $\pi$. Previous works address this by applying Langevin dynamics as it only requires gradients of the energy function $E$ \citep{qin2022cold, kumar2022gradient}. 
Specifically, they use some continuous representation of the current sample $\tilde{Y}_t$ and a learning rate $\gamma$ to define the following update step:  
\begin{align}
    \tilde{Y}_{t+1} = \tilde{Y}_{t} + \gamma \nabla_{\tilde{Y}} E(\tilde{Y}_t) + \epsilon, \epsilon \sim \mathcal{N}(0, \sigma^2 I)
    \label{eq:grad-update-lang}
\end{align}

Non-autoregressive generation methods typically rely on some form of filtering or projection to ensure that the continuous generation can be mapped back into the discrete token space $V$. In \citet{qin2022cold}, the final generation is filtered using a top-k mask, where the top-k indices are obtained from the base language model $P^{LM}$. In \citet{kumar2022gradient}, their proposed algorithm performs the update in the embedding space and projects the resulting vector onto the set of token embeddings for the base language model.

\paragraph{Biased Auto-regressive Generation}
\citet{liu2023bolt} introduced a method that samples a bias term from the target distribution and incorporates it into auto-regressive generation. While the sampling step of the bias-term is similar to \eqref{eq:grad-update-lang} in the embedding space, they skip the projection step of \citet{kumar2022gradient} and modify the auto-regressive step as follows: 
\begin{align}
    y_i = \argmax_{j \in |V|} \left( \tilde{y}_{i, j} + w_i \cdot (b_{i}M^T)_j \right) 
    \label{eq:bolt-auto-reg}
\end{align}
Here, $i$ is the sequence position, $\tilde{y}_i$ is the initial logit vector, $M$ is the embedding table for the language model $P^{LM}$, $b_i$ is the bias vector in the embedding space, $w_i$ is the weight value, and $j$ refers to the logit coordinate.
$b_i M^T$ refers to the transformation of the bias vector $b_i$ to a logit vector. 
\citet{liu2023bolt} demonstrates that sampling a bias term to direct auto-regressive generation enables quicker convergence to satisfactory generation, as opposed to non auto-regressive generation. 
However, they also mention the undesirable trade-off between control towards constraint satisfaction and fluency, which our work addresses. 