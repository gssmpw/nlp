\documentclass{article} % For LaTeX2e
\usepackage{Styling/iclr2025_conference,times}

\usepackage{amsthm}
\usepackage{algorithm}
\usepackage{amsmath} % For \arg\max
\usepackage{xcolor}         % colors
\usepackage{booktabs}
\usepackage{tabularx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{graphicx}
\usepackage{multirow}
\usepackage{wrapfig}
\usepackage{makecell}
\usepackage{nicefrac,xfrac}
\usepackage{array}
\usepackage{makecell}
\usepackage{tikz}
\usetikzlibrary{matrix,shapes,arrows,positioning,chains,backgrounds,shapes.geometric}

\usepackage{colortbl}

\input{Styling/math_commands}
\input{Main_Body/drawing_commands}

\usepackage{hyperref}
\usepackage{url}

\title{Controlled LLM Decoding via Discrete \\Auto-regressive Biasing}

\author{Patrick Pynadath, Ruqi Zhang \\
Department of Computer Science\\
Purdue University\\
West Lafayette, Indiana, 47906, USA \\
\texttt{\{ppynadat, ruqiz\}@purdue.edu} \\
}
\DeclareMathOperator*{\categorical}{Categorical}

\newcommand{\RETURN}[1]{\State \textbf{return} #1}
\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}
\newcommand{\LineComment}[1]{{\color{blue}\textit{$\triangleright$ #1}}}
\newcommand{\FullLineComment}[1]{\Statex {\color{blue}\textit{$\triangleright$ #1}}}
\newcommand{\ruqi}[1]{{\textcolor{red}{[rz: #1]}}}
\newcommand{\pderiv}[2]{{\partial #1}/{\partial #2}}
\newcommand{\pat}[1]{{\textcolor{blue}{#1}}}

\iclrfinalcopy 
\begin{document}


\maketitle
\begin{abstract}
Controlled text generation allows for enforcing user-defined constraints on large language model outputs, an increasingly important field as LLMs become more prevalent in everyday life. One common approach uses energy-based decoding, which defines a target distribution through an energy function that combines multiple constraints into a weighted average. However, these methods often struggle to balance fluency with constraint satisfaction, even with extensive tuning of the energy function's coefficients. In this paper, we identify that this suboptimal balance arises from sampling in continuous space rather than the natural discrete space of text tokens. To address this, we propose \emph{Discrete Auto-regressive Biasing}, a controlled decoding algorithm that leverages gradients while operating entirely in the discrete text domain.
Specifically, we introduce a new formulation for controlled text generation by defining a joint distribution over the generated sequence and an auxiliary bias sequence. To efficiently sample from this joint distribution, we propose a Langevin-within-Gibbs sampling algorithm using gradient-based discrete MCMC. Our method significantly improves constraint satisfaction while maintaining comparable or better fluency, all with even lower computational costs. We demonstrate the advantages of our controlled decoding method on sentiment control, language detoxification, and keyword-guided generation. We make our code available at the following url: \url{https://github.com/patrickpynadath1/dab}.
\end{abstract}

\section{Introduction}
\input{Main_Body/intuitive_graph}
\input{intro}

\section{Related Work}
\input{related}

\section{Preliminaries}
\input{prelim}

\section{Discrete Autoregressive Biasing}
\input{Main_Body/main_body}
\section{Experiments}
\input{Experiments/exp_overview}
\subsection{Sentiment-Controlled Generation}
\input{Experiments/sentiment}
\input{Experiments/megatable}
\subsection{Toxicity Avoidance}
\input{Experiments/detoxify}
\subsection{Keyword-guided Generation}
\input{Experiments/keywords}
\section{Conclusion}
\input{discussion}
\section*{Ethics}
\input{ethics_statement}
\section*{Reproducibility}
\input{reproducibility_statement}
% \newpage

\bibliography{iclr2025_conference}
\bibliographystyle{Styling/iclr2025_conference}
\newpage
\appendix
\input{Appendix/appendix_main}
\end{document}
