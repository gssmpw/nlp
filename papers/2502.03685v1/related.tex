\subsection{Language Models as EBMs}
Energy-based models (EBMs) have been a popular framework used to study the problem of inference-time controlled text generation. 
Initial works focused on the application of Gibbs sampling to encoder-based architectures such as BERT \citep{wang2019bert, goyal2021exposing, mireshghallah-etal-2022-mix}. 

While these methods are similar to ours in that they perform discrete sampling, our work differs in several key ways: (1) Our work leverages gradient information for a more informed exploration of the energy landscape.
(2) Our Gibbs sampling alternates between the response and bias, updating the entire response at once. whereas previous Gibbs sampling approaches only update a single token at a time. 
(3) Our method leverages auto-regressive generation within decoder architectures, whereas these works can only be used with encoder architectures.

More recent works have applied gradient-based sampling methods to more efficiently navigate the energy landscape. \citet{qin2022cold} that uses Langevin dynamics in the logit space followed by top-k masking; \citet{kumar2022gradient} uses Langevin dynamics in the embedding space followed by a projection to the embedding space; and \citet{liu2023bolt} uses the ADAM optimizer with Gaussian noise to obtain bias terms for biased auto-regressive generation. 

Our approach differs from these works in that it relies on discrete sampling rather than continuous sampling or optimization. By operating directly in the natural discrete space of text tokens, our method not only improves the trade-off between fluency and constraint satisfaction but also reduces decoding costs.

Beyond MCMC methods, it is also possible to apply some modified rejection sampling to proposal algorithm to improve approximate sampling of the target EBM \citep{eikema2022an}. Additionally, there is also a line of research devoted to fine-tuning LLMs to align with an EBM defined in terms of the base LM and external constraints \citep{khalifa2020distributional, korbak2022controlling, kruszewski-etal-2023-disco}. We choose to focus on inference-time algorithms as they provide more flexibility and avoid long training runs.
\subsection{Alternative Controlled Generation Approaches}
While the EBM framework is popular within the field of controlled text generation, there are several alternative approaches. In terms of previous inference time algorithms, many works rely on specially trained auxiliary models to provide token-level guidance \citep{krause2020gedi, yang2021fudge, liu-etal-2021-dexperts, meng2022controllable, kim2022critic} or query a standard text classifier multiple times per generated token \citep{dekoninck2023controlled, sitdikov2022classifiers}. Other works apply gradient-based optimization methods to improve constraint satisfaction \citep{qin2020back, dathathri2020plugplaylanguagemodels}. 

Recently, \citet{han2023lm} introduces LM-Steer, a method for learning linear transformations to influence language model generation. This is similar to the BOLT algorithm \citet{liu2023bolt} as both alter the embedding representations from auto-regressive generation by some learned operation. Whereas \citet{liu2023bolt} is framed as a decoding-time algorithm, LM-Steer requires training data in order to learn the linear transformation. In general, these methods either suffer from the steep trade-off between fluency and control previously mentioned, require separate training or fine-tuning for the constraint LM, or are computationally expensive.

\subsection{Gradient-Based Discrete Sampling}
Recent works have demonstrated the benefits of leveraging gradient information for sampling over discrete spaces \citep{grathwohl2021gwg, sun2023anyscale, goshvadi2023discs, sun2023discrete, pynadath2024gradientbaseddiscretesamplingautomatic}. 
Our method uses the sampling algorithm introduced in \citet{zhang2022langevinlike}, which can be viewed as the analog of Langevin dynamics \citep{robertsstramer2002langevinmh} in discrete spaces. 
Our paper may be of interest to this field as it is the first to explicitly link discrete gradient-based sampling with controlled language model generation.