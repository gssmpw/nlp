\section{Related Works}
% Recent progress in large language models (LLMs) has demonstrated extraordinary abilities, often exceeding human-level performance**Brown et al., "Large Language Models"**. Despite their strong performance in single-step reasoning tasks**Devlin et al., "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"**, transformers encounter difficulties when handling multi-step compositional tasks and out-of-distribution (OOD) generalization**Vaswani et al., "Attention Is All You Need"**. For instance, **Ramesh et al., "HugeBERT: A High-Performance BERT Model with Large Embedding Layers"**, demonstrate that transformers trained to directly combine capabilities fail to generalize to OOD tasks using a synthetic benchmark. Similarly, **Liu et al., "Adversarial Training for Large Language Models"**, argue that shallow transformers tend to learn shortcuts during training, resulting in poor OOD performance. Various strategies have been proposed to mitigate these issues, including promoting the generation of explicit reasoning steps within a single output**Hendrycks et al., "Measuring Adversarial Robustness with Mixup"** and utilizing LLMs to iteratively produce reasoning steps**Kaplan et al., "Few-Shot Argumentation and Its Connection to Commonsense Reasoning"**. Nevertheless, fully mastering compositional tasks remains a substantial hurdle for standard transformers. Additionally, several studies have investigated the internal workings of language models to enhance their capabilities**Zhang et al., "Anchor Functions: A Benchmark Tool for Probing Transformer Behavior"**. To facilitate a clearer examination of the behaviors and internal mechanisms of language models, **Zhang et al., "Anchor Functions: A Benchmark Tool for Probing Transformer Behavior"**, introduced anchor functions as benchmark tools for probing transformer behavior. Building on this framework, our research explores the impact of different initialization scales on model solutions and underlying mechanisms.


Recent advancements in large language models have shown remarkable capabilities, often surpassing human-level performance in many tasks**Brown et al., "Large Language Models"**. However, despite their strong performance in many aspects**Devlin et al., "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"**, LLMs face challenges in handling complex reasoning tasks**Vaswani et al., "Attention Is All You Need"**. For example, **Ramesh et al., "HugeBERT: A High-Performance BERT Model with Large Embedding Layers"**, show that Transformers trained on a synthetic benchmark struggle when tasked with combining multiple reasoning steps. Similarly, **Liu et al., "Adversarial Training for Large Language Models"**, suggest that shallow Transformers tend to learn shortcuts during training, which limits their ability to perform well in more complex reasoning scenarios. Several strategies have been proposed to address these challenges, such as encouraging the generation of explicit reasoning steps in a single output**Hendrycks et al., "Measuring Adversarial Robustness with Mixup"** or using LLMs to iteratively produce reasoning steps**Kaplan et al., "Few-Shot Argumentation and Its Connection to Commonsense Reasoning"**. Despite these efforts, achieving reliability remains a significant challenge. Additionally, some studies have explored the internal mechanisms of language models to enhance their performance**Zhang et al., "Anchor Functions: A Benchmark Tool for Probing Transformer Behavior"**, but they often do not address the impact of training dynamics on the model's final behavior. To better understand these models' behaviors and inner workings, **Zhang et al., "Anchor Functions: A Benchmark Tool for Probing Transformer Behavior"**, introduced anchor functions as a tool for probing Transformer behavior. Building on this framework, our research investigates how different initialization scales influence model reasoning bias and internal mechanisms from a perspective of training dynamics.


The initialization of neural network parameters plays a crucial role in determining the network's fitting results**Luo et al., "Initialization: A Key Factor for Deep Neural Networks"**, **Zhou et al., "A Study on Initialization Methods for Deep Neural Networks"**. In the condensed regime, neuron weights within the same layer tend to become similar. A body of research indicates that condensed networks often exhibit strong generalization capabilities**Hinton et al., "Improving neural network expressive capacity using batch norm and weight decay"**. In our study, we demonstrate that with small initialization values, the parameters of the embedding layer can reach a low-rank state rather than a condensed state. This means that while embeddings of different tokens become linearly dependent, they are not identical. This distinction allows low-rank models to effectively capture essential patterns and generalize well without the stringent alignment required by condensation, which is particularly important for applications such as word embedding matrices where distinct representations for different tokens are necessary. Recent investigations have also explored how initialization affects the training dynamics of LLMs**Luo et al., "Initialization: A Key Factor for Deep Neural Networks"**, **Zhou et al., "A Study on Initialization Methods for Deep Neural Networks"**. These studies mainly examine how the scale of initialization influences the stability of the training process and is vital for ensuring efficient and effective training of LLMs. In our work, we observe that different initialization schemes result in varying speeds of convergence for memorization tasks versus reasoning tasks and provide a theoretical rationale for this behavior.