@inproceedings{arora2019exact,
  title={On exact computation with an infinitely wide neural net},
  author={Arora, Sanjeev and Du, Simon S and Hu, Wei and Li, Zhiyuan and Salakhutdinov, Russ R and Wang, Ruosong},
  booktitle={Advances in Neural Information Processing Systems},
  pages={8141--8150},
  year={2019}
}

@incollection{chizat_global_2018,
	title = {On the {Global} {Convergence} of {Gradient} {Descent} for {Over}-parameterized {Models} using {Optimal} {Transport}}, 
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 31}, 
	author = {Chizat, Lénaïc and Bach, Francis}, 
	year = {2018},
	pages = {3036--3046}, 
}

@article{creswell2022faithful,
  title={Faithful reasoning using large language models},
  author={Creswell, Antonia and Shanahan, Murray},
  journal={arXiv preprint arXiv:2208.14271},
  year={2022}
}

@article{creswell2022selection,
  title={Selection-inference: Exploiting large language models for interpretable logical reasoning},
  author={Creswell, Antonia and Shanahan, Murray and Higgins, Irina},
  journal={arXiv preprint arXiv:2205.09712},
  year={2022}
}

@article{csordas2021neural,
  title={The neural data router: Adaptive control flow in transformers improves systematic generalization},
  author={Csord{\'a}s, R{\'o}bert and Irie, Kazuki and Schmidhuber, J{\"u}rgen},
  journal={arXiv preprint arXiv:2110.07732},
  year={2021}
}

@article{csordas2022ctl,
  title={CTL++: Evaluating Generalization on Never-Seen Compositional Patterns of Known Functions, and Compatibility of Neural Representations},
  author={Csord{\'a}s, R{\'o}bert and Irie, Kazuki and Schmidhuber, J{\"u}rgen},
  journal={arXiv preprint arXiv:2210.06350},
  year={2022}
}

@article{dziri2024faith,
  title={Faith and fate: Limits of transformers on compositionality},
  author={Dziri, Nouha and Lu, Ximing and Sclar, Melanie and Li, Xiang Lorraine and Jiang, Liwei and Lin, Bill Yuchen and Welleck, Sean and West, Peter and Bhagavatula, Chandra and Le Bras, Ronan and others},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{e2020comparative,
  title={A comparative analysis of optimization and generalization properties of two-layer neural network and random feature models under gradient descent dynamics.},
  author={E, Weinan and Ma, Chao and Wu, Lei },
  journal={Sci. China Math.}, 
  volume = {63},
  year={2020} 
}

@article{fu2022does,
  title={How does gpt obtain its ability? tracing emergent abilities of language models to their sources},
  author={Fu, Yao and Peng, Hao and Khot, Tushar},
  journal={Yao Fu’s Notion},
  year={2022}
}

@inproceedings{huang2020improving,
  title={Improving transformer optimization through better initialization},
  author={Huang, Xiao Shi and Perez, Felipe and Ba, Jimmy and Volkovs, Maksims},
  booktitle={International Conference on Machine Learning},
  pages={4475--4483},
  year={2020},
  organization={PMLR}
}

@article{hupkes2018learning,
  title={Learning compositionally through attentive guidance},
  author={Hupkes, Dieuwke and Singh, Anand and Korrel, Kris and Kruszewski, German and Bruni, Elia},
  journal={arXiv preprint arXiv:1805.09657},
  year={2018}
}

@incollection{jacot_neural_2018,
	title = {Neural {Tangent} {Kernel}: {Convergence} and {Generalization} in {Neural} {Networks}}, 
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 31}, 
	author = {Jacot, Arthur and Gabriel, Franck and Hongler, Clement}, 
	year = {2018},
	pages = {8571--8580}, 
}

@article{lepori2023break,
  title={Break It Down: Evidence for Structural Compositionality in Neural Networks},
  author={Lepori, Michael A and Serre, Thomas and Pavlick, Ellie},
  journal={arXiv preprint arXiv:2301.10884},
  year={2023}
}

@article{liu2020understanding,
  title={Understanding the difficulty of training transformers},
  author={Liu, Liyuan and Liu, Xiaodong and Gao, Jianfeng and Chen, Weizhu and Han, Jiawei},
  journal={arXiv preprint arXiv:2004.08249},
  year={2020}
}

@article{liu2022transformers,
  title={Transformers learn shortcuts to automata},
  author={Liu, Bingbin and Ash, Jordan T and Goel, Surbhi and Krishnamurthy, Akshay and Zhang, Cyril},
  journal={arXiv preprint arXiv:2210.10749},
  year={2022}
}

@article{luo2021phase,
  title={Phase diagram for two-layer relu neural networks at infinite-width limit},
  author={Luo, Tao and Xu, Zhi-Qin John and Ma, Zheng and Zhang, Yaoyu},
  journal={Journal of Machine Learning Research},
  volume={22},
  number={71},
  pages={1--47},
  year={2021}
}

@article{mei_mean_2018,
	title = {A mean field view of the landscape of two-layer neural networks},
	volume = {115}, 
	doi = {10.1073/pnas.1806579115}, 
	number = {33}, 
	journal = {Proceedings of the National Academy of Sciences},
	author = {Mei, Song and Montanari, Andrea and Nguyen, Phan-Minh}, 
	year = {2018}, 
	pages = {E7665--E7671}, 
}

@article{okawa2023compositional,
  title={Compositional Abilities Emerge Multiplicatively: Exploring Diffusion Models on a Synthetic Task},
  author={Okawa, Maya and Lubana, Ekdeep Singh and Dick, Robert P and Tanaka, Hidenori},
  journal={arXiv preprint arXiv:2310.09336},
  year={2023}
}

@article{ramesh2023capable,
  title={How capable can a transformer become? a study on synthetic, interpretable tasks},
  author={Ramesh, Rahul and Khona, Mikail and Dick, Robert P and Tanaka, Hidenori and Lubana, Ekdeep Singh},
  journal={arXiv preprint arXiv:2311.12997},
  year={2023}
}

@incollection{rotskoff_parameters_2018,
	title = {Parameters as interacting particles: long time convergence and asymptotic error scaling of neural networks}, 
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 31}, 
	author = {Rotskoff, Grant and Vanden-Eijnden, Eric}, 
	year = {2018},
	pages = {7146--7155}, 
}

@article{sirignano_mean_2020,
	title = {Mean field analysis of neural networks: {A} central limit theorem},
	volume = {130}, 
	doi = {10.1016/j.spa.2019.06.003}, 
	number = {3}, 
	journal = {Stochastic Processes and their Applications},
	author = {Sirignano, Justin and Spiliopoulos, Konstantinos}, 
	year = {2020},
	pages = {1820--1852}, 
}

@article{srivastava2022beyond,
  title={Beyond the imitation game: Quantifying and extrapolating the capabilities of language models},
  author={Srivastava, Aarohi and Rastogi, Abhinav and Rao, Abhishek and Shoeb, Abu Awal Md and Abid, Abubakar and Fisch, Adam and Brown, Adam R and Santoro, Adam and Gupta, Aditya and Garriga-Alonso, Adri{\`a} and others},
  journal={arXiv preprint arXiv:2206.04615},
  year={2022}
}

@inproceedings{trockman2023mimetic,
  title={Mimetic initialization of self-attention layers},
  author={Trockman, Asher and Kolter, J Zico},
  booktitle={International Conference on Machine Learning},
  pages={34456--34468},
  year={2023},
  organization={PMLR}
}

@article{wang2023label,
  title={Label words are anchors: An information flow perspective for understanding in-context learning},
  author={Wang, Lean and Li, Lei and Dai, Damai and Chen, Deli and Zhou, Hao and Meng, Fandong and Zhou, Jie and Sun, Xu},
  journal={arXiv preprint arXiv:2305.14160},
  year={2023}
}

@article{wang2024deepnet,
  title={Deepnet: Scaling transformers to 1,000 layers},
  author={Wang, Hongyu and Ma, Shuming and Dong, Li and Huang, Shaohan and Zhang, Dongdong and Wei, Furu},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  year={2024},
  publisher={IEEE}
}

@article{wang2024improving,
  title={Improving Generalization and Convergence by Enhancing Implicit Regularization},
  author={Wang, Mingze and He, Haotian and Wang, Jinbo and Wang, Zilin and Huang, Guanhua and Xiong, Feiyu and Li, Zhiyu and Wu, Lei and others},
  journal={arXiv preprint arXiv:2405.20763},
  year={2024}
}

@article{wang2024towards,
  title={Towards understanding how transformer perform multi-step reasoning with matching operation},
  author={Wang, Zhiwei and Wang, Yunji and Zhang, Zhongwang and Zhou, Zhangchen and Jin, Hui and Hu, Tianyang and Sun, Jiacheng and Li, Zhenguo and Zhang, Yaoyu and Xu, Zhi-Qin John},
  journal={arXiv preprint arXiv:2405.15302},
  year={2024}
}

@article{wang2024understanding,
  title={Understanding the Expressive Power and Mechanisms of Transformer for Sequence Modeling},
  author={Wang, Mingze and others},
  journal={arXiv preprint arXiv:2402.00522},
  year={2024}
}

@article{wei2022chain,
  title={Chain of thought prompting elicits reasoning in large language models},
  author={Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Chi, Ed and Le, Quoc and Zhou, Denny},
  journal={arXiv preprint arXiv:2201.11903},
  year={2022}
}

@article{wei2022emergent,
  title={Emergent abilities of large language models},
  author={Wei, Jason and Tay, Yi and Bommasani, Rishi and Raffel, Colin and Zoph, Barret and Borgeaud, Sebastian and Yogatama, Dani and Bosma, Maarten and Zhou, Denny and Metzler, Donald and others},
  journal={arXiv preprint arXiv:2206.07682},
  year={2022}
}

@article{williams_gradient_2019,
  author    = {Francis Williams and
               Matthew Trager and
               Cl{\'{a}}udio T. Silva and
               Daniele Panozzo and
               Denis Zorin and
               Joan Bruna},
  title     = {Gradient Dynamics of Shallow Univariate ReLU Networks},
  journal   = {CoRR},
  volume    = {abs/1906.07842},
  year      = {2019},
  url       = {http://arxiv.org/abs/1906.07842},
  archivePrefix = {arXiv},
  eprint    = {1906.07842},
  timestamp = {Mon, 24 Jun 2019 17:28:45 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1906-07842.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{yun2022vision,
  title={Do Vision-Language Pretrained Models Learn Composable Primitive Concepts?},
  author={Yun, Tian and Bhalla, Usha and Pavlick, Ellie and Sun, Chen},
  journal={arXiv preprint arXiv:2203.17271},
  year={2022}
}

@article{zhang2019improving,
  title={Improving deep transformer with depth-scaled initialization and merged attention},
  author={Zhang, Biao and Titov, Ivan and Sennrich, Rico},
  journal={arXiv preprint arXiv:1908.11365},
  year={2019}
}

@article{zhang2022linear,
  title={Linear Stability Hypothesis and Rank Stratification for Nonlinear Models},
  author={Zhang, Yaoyu and Zhang, Zhongwang and Zhang, Leyang and Bai, Zhiwei and Luo, Tao and Xu, Zhi-Qin John},
  journal={arXiv preprint arXiv:2211.11623},
  year={2022}
}

@article{zhang2023loss,
  title={Loss Spike in Training Neural Networks},
  author={Zhang, Zhongwang and Xu, Zhi-Qin John},
  journal={arXiv preprint arXiv:2305.12133},
  year={2023}
}

@article{zhang2023stochastic,
  title={Stochastic Modified Equations and Dynamics of Dropout Algorithm},
  author={Zhang, Zhongwang and Li, Yuqing and Luo, Tao and Xu, Zhi-Qin John},
  journal={arXiv preprint arXiv:2305.15850},
  year={2023}
}

@article{zhang2024anchor,
  title={Anchor function: a type of benchmark functions for studying language models},
  author={Zhang, Zhongwang and Wang, Zhiwei and Yao, Junjie and Zhou, Zhangchen and Li, Xiaolong and {E}, Weinan and Xu, Zhi-Qin John},
  journal={arXiv preprint arXiv:2401.08309},
  year={2024}
}

@article{zhang_type_2019,
	title = {A type of generalization error induced by initialization in deep neural networks}, 
	journal = {arXiv:1905.07777 [cs, stat]},
	author = {Zhang, Yaoyu and Xu, Zhi-Qin John and Luo, Tao and Ma, Zheng}, 
	year = {2019},  
}

@article{zhou2022empirical,
  title={Empirical Phase Diagram for Three-layer Neural Networks with Infinite Width},
  author={Zhou, Hanxu and Zhou, Qixuan and Jin, Zhenyuan and Luo, Tao and Zhang, Yaoyu and Xu, Zhi-Qin John},
  journal={Advances in Neural Information Processing Systems},
  year={2022}
}

@article{zhu2021gradinit,
  title={Gradinit: Learning to initialize neural networks for stable and efficient training},
  author={Zhu, Chen and Ni, Renkun and Xu, Zheng and Kong, Kezhi and Huang, W Ronny and Goldstein, Tom},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={16410--16422},
  year={2021}
}

