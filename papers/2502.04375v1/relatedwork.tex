\section{Related Works}
% Recent progress in large language models (LLMs) has demonstrated extraordinary abilities, often exceeding human-level performance~\cite{fu2022does, wei2022emergent}. Despite their strong performance in single-step reasoning tasks~\cite{srivastava2022beyond}, transformers encounter difficulties when handling multi-step compositional tasks and out-of-distribution (OOD) generalization~\cite{csordas2021neural, dziri2024faith, hupkes2018learning, lepori2023break, okawa2023compositional, yun2022vision, wang2024towards, csordas2022ctl}. For instance, Ramesh et al.~\cite{ramesh2023capable} demonstrate that transformers trained to directly combine capabilities fail to generalize to OOD tasks using a synthetic benchmark. Similarly, Liu et al.~\cite{liu2022transformers} argue that shallow transformers tend to learn shortcuts during training, resulting in poor OOD performance. Various strategies have been proposed to mitigate these issues, including promoting the generation of explicit reasoning steps within a single output~\cite{wei2022chain} and utilizing LLMs to iteratively produce reasoning steps~\cite{creswell2022selection, creswell2022faithful}. Nevertheless, fully mastering compositional tasks remains a substantial hurdle for standard transformers. Additionally, several studies have investigated the internal workings of language models to enhance their capabilities~\cite{wang2024improving, wang2024understanding, wang2023label}. To facilitate a clearer examination of the behaviors and internal mechanisms of language models, Zhang et al.~\cite{zhang2024anchor} introduced anchor functions as benchmark tools for probing transformer behavior. Building on this framework, our research explores the impact of different initialization scales on model solutions and underlying mechanisms.


Recent advancements in large language models have shown remarkable capabilities, often surpassing human-level performance in many tasks~\cite{fu2022does, wei2022emergent}. However, despite their strong performance in many aspects~\cite{srivastava2022beyond}, LLMs face challenges in handling complex reasoning tasks~\cite{csordas2021neural, dziri2024faith, hupkes2018learning, lepori2023break, okawa2023compositional, yun2022vision, wang2024towards, csordas2022ctl}. For example, Ramesh et al.~\cite{ramesh2023capable} show that Transformers trained on a synthetic benchmark struggle when tasked with combining multiple reasoning steps. Similarly, Liu et al.~\cite{liu2022transformers} suggest that shallow Transformers tend to learn shortcuts during training, which limits their ability to perform well in more complex reasoning scenarios. Several strategies have been proposed to address these challenges, such as encouraging the generation of explicit reasoning steps in a single output~\cite{wei2022chain} or using LLMs to iteratively produce reasoning steps~\cite{creswell2022selection, creswell2022faithful}. Despite these efforts, achieving reliability remains a significant challenge. Additionally, some studies have explored the internal mechanisms of language models to enhance their performance~\cite{wang2024improving, wang2024understanding, wang2023label}, but they often do not address the impact of training dynamics on the model's final behavior. To better understand these models' behaviors and inner workings, Zhang et al.~\cite{zhang2024anchor} introduced anchor functions as a tool for probing Transformer behavior. Building on this framework, our research investigates how different initialization scales influence model reasoning bias and internal mechanisms from a perspective of training dynamics.


The initialization of neural network parameters plays a crucial role in determining the network's fitting results~\cite{arora2019exact, chizat_global_2018, zhang_type_2019, e2020comparative, jacot_neural_2018, mei_mean_2018, rotskoff_parameters_2018, sirignano_mean_2020, williams_gradient_2019}. Luo et al.~\cite{luo2021phase} and Zhou et al.~\cite{zhou2022empirical} primarily identify the linear and condensed regimes in wide ReLU neural networks. In the condensed regime, neuron weights within the same layer tend to become similar. A body of research indicates that condensed networks often exhibit strong generalization capabilities~\cite{zhang2022linear, zhang2023loss, zhang2023stochastic, zhang2024implicit}. In our study, we demonstrate that with small initialization values, the parameters of the embedding layer can reach a low-rank state rather than a condensed state. This means that while embeddings of different tokens become linearly dependent, they are not identical. This distinction allows low-rank models to effectively capture essential patterns and generalize well without the stringent alignment required by condensation, which is particularly important for applications such as word embedding matrices where distinct representations for different tokens are necessary. Recent investigations have also explored how initialization affects the training dynamics of LLMs~\cite{huang2020improving, liu2020understanding, trockman2023mimetic, wang2024deepnet, zhang2019improving, zhu2021gradinit}. These studies mainly examine how the scale of initialization influences the stability of the training process and is vital for ensuring efficient and effective training of LLMs. In our work, we observe that different initialization schemes result in varying speeds of convergence for memorization tasks versus reasoning tasks and provide a theoretical rationale for this behavior.