\documentclass{article}
% \documentclass[draft]{article}
\usepackage{arxiv}
% Language setting
% Replace `english' with e.g. `spanish' to change the document language
\usepackage[english]{babel}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage[colorlinks=true, allcolors=blue]{hyperref}
\urlstyle{tt}
\newcommand{\email}[1]{\href{mailto:#1}{\tt{\nolinkurl{#1}}}}
\newcommand{\orcid}[1]{ORCID: \href{https://orcid.org/#1}{\tt{\nolinkurl{#1}}}}
\usepackage{authblk}

% Set page size and margins
% Replace `letterpaper' with `a4paper' for UK/EU standard size
% \usepackage[letterpaper,top=2cm,bottom=2cm,left=3cm,right=3cm,marginparwidth=1.75cm]{geometry}

% Useful packages
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{subfig}
\usepackage{float}
\usepackage[colorlinks=true, allcolors=blue]{hyperref}
\usepackage{array}
\usepackage{tabularx} % 加载tabularx宏包
% \usepackage{xcolor}
\usepackage{mlmath}
\usepackage{longtable} % 使用longtable宏包
\usepackage[table]{xcolor} % 使用xcolor宏包，带table选项
\usepackage{multirow} % 引入multirow宏包
\newcommand{\red}[1]{{\color{red}{#1}}}
\newcommand{\blue}[1]{{\color{blue}{#1}}}


\usepackage{amsmath,amsfonts,amssymb,mathtools,amsthm}

\newtheorem{problem}{Problem}
\newtheorem{prob}{Problem}

\newtheorem{theorem}{Theorem}
\newtheorem*{theorem*}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem*{lemma*}{Lemma}
\newtheorem{property}{Property}
\newtheorem*{property*}{Property}
\newtheorem{remark}{Remark}
\newtheorem{definition}{Definition}
\newtheorem{corollary}{Corollary}
\newtheorem{assumption}{Assumption}
\newtheorem*{assumption*}{Assumption}
\newtheorem*{prop*}{Proposition}
\newtheorem{prop}{Proposition}
\newtheorem{setting}{Setting}
\newtheorem*{setting*}{Setting}
\newtheorem{example}{Example}
\newtheorem{notation}{Notation}
\title{An analysis for reasoning bias of language models with small initialization}
% \title{Embedding State Space and task Preference of Small Initialization }
\author[1,2]{Junjie Yao}
\author[1,2,*]{Zhongwang Zhang}
\author[1,2,3,4,5,*]{Zhi-Qin John Xu}
\affil[1]{Institute of Natural Sciences, MOE-LSC, Shanghai Jiao Tong University}
\affil[2]{School of Mathematical Sciences, Shanghai Jiao Tong University}
\affil[3]{School of Artificial Intelligence, Shanghai Jiao Tong University}
\affil[4]{Key Laboratory of Marine Intelligent Equipment and System, Ministry of Education, P.R. China}
\affil[5]{Center for LLM, Institute for Advanced Algorithms Research, Shanghai}
\affil[*]{Corresponding author: \email{0123zzw666@sjtu.edu.cn}, \email{xuzhiqin@sjtu.edu.cn}}
\begin{document}
\maketitle
\begin{abstract}
Transformer-based Large Language Models (LLMs) have revolutionized Natural Language Processing by demonstrating exceptional performance across diverse tasks. This study investigates the impact of the parameter initialization scale on the training behavior and task preferences of LLMs. We discover that smaller initialization scales encourage models to favor reasoning tasks, whereas larger initialization scales lead to a preference for memorization tasks. We validate this reasoning bias via real datasets and meticulously designed anchor functions. Further analysis of initial training dynamics suggests that specific model components, particularly the embedding space and self-attention mechanisms, play pivotal roles in shaping these learning biases. We provide a theoretical framework from the perspective of model training dynamics to explain these phenomena. Additionally, experiments on real-world language tasks corroborate our theoretical insights. This work enhances our understanding of how initialization strategies influence LLM performance on reasoning tasks and offers valuable guidelines for training models.
\end{abstract}

% Large Language Models (LLMs) have revolutionized Natural Language Processing by demonstrating exceptional performance across diverse tasks. This study investigates the impact of parameter initialization scale on the training behavior and task preferences of LLMs. We discover that varying the initialization scale significantly alters how models engage with tasks: smaller initialization scales encourage models to identify intrinsic data patterns, thereby favoring reasoning tasks, whereas larger initialization scales lead models to memorize input-output mappings, resulting in a preference for memory-based tasks. To validate these findings, we design a set of fundamental tasks based on anchor functions, which effectively differentiate model behaviors under different initialization scales. Further analysis of the embedding space and self-attention modules reveals that small initialization scales induce a hierarchically structured embedding space for reasoning anchors and a highly similar embedding space for memory anchors, making it difficult for models to distinguish between different anchors. We provide a theoretical framework from the perspective of model training dynamics to explain these phenomena. Additionally, experiments on real-world language tasks corroborate our theoretical insights, underscoring the critical role of initialization scale in shaping LLM training dynamics and task preferences. This work enhances our understanding of how initialization strategies influence LLM performance and offers valuable guidelines for optimizing model training protocols.

\section{Introduction}
With the rapid advancement of deep learning technologies, Large Language Models have achieved remarkable success in the field of Natural Language Processing (NLP). These models have demonstrated exceptional capabilities across a wide range of tasks, from text generation to complex reasoning. Reasoning, in particular, is a critical ability for LLMs. A number of studies have focused on improving the reasoning ability of these models through data-driven approaches, such as RHO-1~\cite{lin2024not} and Phi-4~\cite{abdin2024phi4technicalreport}. However, there remains an ongoing debate as to whether LLMs genuinely learn the underlying logical rules or merely mimic patterns observed in the data~\cite{marcus2003algebraic, smolensky2022neurocompositional}.

\begin{figure}[hbpt]
    \centering
    \includegraphics[width=0.6\linewidth]{figure/PrOntoQA/qa_TinyStory.png}
    % \vspace{-15pt}
    \caption{Comparison of training loss between PrOntoQA and TinyStories in one next-token prediction training for this mix dataset. The red line represents the training loss on the PrOntoQA dataset, while the blue line depicts the training loss on the TinyStories dataset.}
    \label{fig:loss_PrOntoQA}
\end{figure}

An alternative approach to enhancing the reasoning ability of LLMs focuses on the model architecture and its training process. In one such study examining the use of Transformers to model compositional functions, it was observed that the scale of model parameter initialization significantly impacts the model's reasoning behavior~\cite{zhang2024initialization, zhang2025complexity}. Specifically, smaller initialization scales bias the model toward fitting the data by learning primitive-level functions and compositional rules, whereas larger initialization scales tend to encourage memorization of input-output mappings. A qualitative rationale for this phenomenon has been proposed: with a small initialization, a well-documented effect known as neuron condensation emerges during training~\cite{luo2021phase,zhou2022empirical,zhang2022linear,zhang2023loss,zhang2023stochastic,zhang2024implicit}. This phenomenon suggests that neurons within the same layer tend to behave similarly, promoting data fitting with the least possible complexity. To achieve a low-complexity result, the model must learn a minimal set of rules leading to capture the intrinsic primitive functions and compositional rules. However, this rationale does not reveal a critical question: how the optimization process, together with the Transformer structure, can achieve reasoning solutions with small initialization.
% In contrast to memorizing all input-output mappings, learning intrinsic primitive functions and compositional rules offers a much lower-cost approach. Consequently, reasoning-based solutions are preferred when initialization is small.


In this work, we identify a reasoning bias during the training of neural networks that learn natural language when initialized with small parameter scales. To illustrate this phenomenon, we employ a GPT-2 model~\cite{GPT2} to train on a mixed dataset comprising two types of language data with distinct levels of reasoning complexity, within a single next-token prediction training framework. The first dataset, PrOntoQA~\cite{PrOntoQA}, consists of question-answering examples that include chains of thought, which explicitly describe the reasoning necessary to answer the questions correctly. The second dataset, TinyStories~\cite{tinystories}, is a synthetic corpus of short stories containing only words typically understood by children aged 3 to 4 years. As shown in Figure~\ref{fig:loss_PrOntoQA}, the training loss for PrOntoQA decreases significantly faster than for TinyStories, suggesting that the model encounters and learns the reasoning patterns more readily.



% In this work, we find that there is a reasoning bias during the training for neural networks learning natural language with small initialization. As an example, we use a GPT2 to learn two types of language data with distinct reasoning degrees in one next-token prediction training. One is PrOntoQA, which contains question-answering examples with chains-of-thought that describe the reasoning required to answer the questions correctly. The other is TinyStories, a synthetic dataset of short stories that only contain words that a typical 3 to 4-year-olds usually understand. As shown in Fig. [??], the training loss for PrOntoQA decreases much faster than TinyStories. 

% We uncover a key mechanism that reasoning tasks are learned first due to its related tokens are differentiated in the embedding space with task-dependent positions earlier in the training. We validate this mechanism through synthetic data and real-world datasets. In addition, we theoretically show that the evolution of token embedding depends on the distribution of sample labels. Since each token is encoded by one-hot vector, the embedding of this token is tuned via the loss of samples that contain the token. Therefore, samples can be different for training different token embeddings. For memory tasks, the label for each token is quite random without particular rules, therefore, the distributions of labels for memory tokens are similar, thus, the embedding for memory tokens are hard to be differentiated in early training. In contrast, the distributions of labels for reasoning tokens are often different, leading to different learning of the embedding of reason tokens. These analysis are expanded through a simplified model with MLP and embedding structure, followed by an analysis for a two-layer transformer model.
We uncover a key mechanism whereby reasoning tasks are learned earlier during training because the tokens associated with these tasks become more differentiated in the embedding space at an early stage of the training process. We validate this mechanism using both synthetic data and real-world datasets. Furthermore, we provide a theoretical explanation for the evolution of token embeddings, which depends on the distribution of sample labels. Since each token is encoded as a one-hot vector, its embedding is adjusted based on the loss associated with the labels of that token. Consequently, different label distributions can lead to distinct learning behaviors for token embeddings. For memory tasks, the labels associated with each token are typically random and lack explicit structure, which results in similar distributions for different memory token labels. As a result, the embeddings for memory tokens are difficult to differentiate in the early stages of training. In contrast, reasoning tokens often exhibit distinct label distributions, leading to more differentiated embedding vectors for these tokens. These insights are elaborated through a simplified model using a multi-layer perceptron (MLP) and embedding structure, followed by an analysis of a Transformer model.


% However, the training behavior of LLMs is significantly influenced by the initialization of their parameters, particularly the scale of parameter initialization. Parameter initialization not only affects the convergence speed of the model but also has profound implications on the model’s final performance and task preferences. While existing studies have explored the impact of initialization on model training, the specific mechanisms by which different initialization scales lead to varying task preferences remain insufficiently understood.

% In this study, we systematically investigate how different parameter initialization scales affect the training behaviors of LLMs. Our experimental results reveal that the scale of initialization has a substantial impact on the model’s task preferences. Specifically, when the initialization scale is reduced, the model tends to seek intrinsic patterns within the data, thereby exhibiting a stronger preference for reasoning tasks. Conversely, when the initialization scale is increased, the model is more inclined to memorize all input-output mappings, showing a preference for memory-based tasks. To validate this phenomenon, we designed a set of fundamental tasks based on anchor functions. These tasks effectively differentiate the training behaviors of models under varying initialization scales, thereby confirming the influence of initialization scale on task preference.

% To delve deeper into the underlying reasons for the model’s preference for reasoning tasks under small initialization scales, we analyze the reasoning mechanisms within different modules of the model, including the embedding space and the self-attention modules. Our findings indicate that with small initialization scales, the embedding space for reasoning anchors rapidly develops a hierarchically structured organization, whereas the embedding space for memory anchors exhibits high similarity. This high similarity makes it challenging for the model to distinguish between different anchors, thereby favoring reasoning over memorization.

% Building on these observations, we provide a theoretical analysis from the perspective of model training dynamics to explain the emergence of these mechanisms. Our analysis elucidates how the initialization scale influences the structure of the embedding space and the functioning of the self-attention mechanisms, ultimately determining the model’s task preference. To further substantiate our theoretical findings, we conducted experiments on real-world language tasks. The results of these experiments consistently support our theoretical claims, reinforcing the critical role of initialization scale in shaping model training behaviors.

The primary contribution of this research lies in uncovering the profound impact of the parameter initialization scale on the training behavior and task preferences of LLMs. By combining theoretical analysis with empirical evidence, we enhance the understanding of LLM training dynamics and provide new insights for optimizing model initialization strategies.  

\section{Related Works}

% Recent progress in large language models (LLMs) has demonstrated extraordinary abilities, often exceeding human-level performance~\cite{fu2022does, wei2022emergent}. Despite their strong performance in single-step reasoning tasks~\cite{srivastava2022beyond}, transformers encounter difficulties when handling multi-step compositional tasks and out-of-distribution (OOD) generalization~\cite{csordas2021neural, dziri2024faith, hupkes2018learning, lepori2023break, okawa2023compositional, yun2022vision, wang2024towards, csordas2022ctl}. For instance, Ramesh et al.~\cite{ramesh2023capable} demonstrate that transformers trained to directly combine capabilities fail to generalize to OOD tasks using a synthetic benchmark. Similarly, Liu et al.~\cite{liu2022transformers} argue that shallow transformers tend to learn shortcuts during training, resulting in poor OOD performance. Various strategies have been proposed to mitigate these issues, including promoting the generation of explicit reasoning steps within a single output~\cite{wei2022chain} and utilizing LLMs to iteratively produce reasoning steps~\cite{creswell2022selection, creswell2022faithful}. Nevertheless, fully mastering compositional tasks remains a substantial hurdle for standard transformers. Additionally, several studies have investigated the internal workings of language models to enhance their capabilities~\cite{wang2024improving, wang2024understanding, wang2023label}. To facilitate a clearer examination of the behaviors and internal mechanisms of language models, Zhang et al.~\cite{zhang2024anchor} introduced anchor functions as benchmark tools for probing transformer behavior. Building on this framework, our research explores the impact of different initialization scales on model solutions and underlying mechanisms.


Recent advancements in large language models have shown remarkable capabilities, often surpassing human-level performance in many tasks~\cite{fu2022does, wei2022emergent}. However, despite their strong performance in many aspects~\cite{srivastava2022beyond}, LLMs face challenges in handling complex reasoning tasks~\cite{csordas2021neural, dziri2024faith, hupkes2018learning, lepori2023break, okawa2023compositional, yun2022vision, wang2024towards, csordas2022ctl}. For example, Ramesh et al.~\cite{ramesh2023capable} show that Transformers trained on a synthetic benchmark struggle when tasked with combining multiple reasoning steps. Similarly, Liu et al.~\cite{liu2022transformers} suggest that shallow Transformers tend to learn shortcuts during training, which limits their ability to perform well in more complex reasoning scenarios. Several strategies have been proposed to address these challenges, such as encouraging the generation of explicit reasoning steps in a single output~\cite{wei2022chain} or using LLMs to iteratively produce reasoning steps~\cite{creswell2022selection, creswell2022faithful}. Despite these efforts, achieving reliability remains a significant challenge. Additionally, some studies have explored the internal mechanisms of language models to enhance their performance~\cite{wang2024improving, wang2024understanding, wang2023label}, but they often do not address the impact of training dynamics on the model's final behavior. To better understand these models' behaviors and inner workings, Zhang et al.~\cite{zhang2024anchor} introduced anchor functions as a tool for probing Transformer behavior. Building on this framework, our research investigates how different initialization scales influence model reasoning bias and internal mechanisms from a perspective of training dynamics.


The initialization of neural network parameters plays a crucial role in determining the network's fitting results~\cite{arora2019exact, chizat_global_2018, zhang_type_2019, e2020comparative, jacot_neural_2018, mei_mean_2018, rotskoff_parameters_2018, sirignano_mean_2020, williams_gradient_2019}. Luo et al.~\cite{luo2021phase} and Zhou et al.~\cite{zhou2022empirical} primarily identify the linear and condensed regimes in wide ReLU neural networks. In the condensed regime, neuron weights within the same layer tend to become similar. A body of research indicates that condensed networks often exhibit strong generalization capabilities~\cite{zhang2022linear, zhang2023loss, zhang2023stochastic, zhang2024implicit}. In our study, we demonstrate that with small initialization values, the parameters of the embedding layer can reach a low-rank state rather than a condensed state. This means that while embeddings of different tokens become linearly dependent, they are not identical. This distinction allows low-rank models to effectively capture essential patterns and generalize well without the stringent alignment required by condensation, which is particularly important for applications such as word embedding matrices where distinct representations for different tokens are necessary. Recent investigations have also explored how initialization affects the training dynamics of LLMs~\cite{huang2020improving, liu2020understanding, trockman2023mimetic, wang2024deepnet, zhang2019improving, zhu2021gradinit}. These studies mainly examine how the scale of initialization influences the stability of the training process and is vital for ensuring efficient and effective training of LLMs. In our work, we observe that different initialization schemes result in varying speeds of convergence for memorization tasks versus reasoning tasks and provide a theoretical rationale for this behavior.


\section{Preliminary}
\subsection{Synthetic Composition Task}
To study the task bias during the training, we use the concept of anchor function~\cite{zhang2024anchor} to construct a dataset that contains tasks of different reasoning complexities. We consider all tokens belonging to positive integers. A set of tokens are designated as anchors, denoted as $\fA:=\{a\in\sN^+|\alpha_{\min}\leq a\leq \alpha_{\max}\}$, where each anchor represents an addition/randomness operation in this work. Another set of tokens are designated as keys, denoted as $\fZ:=\{z\in\sN^+|\zeta_{\min}\leq z\leq\zeta_{\max}\}$ with the assumption that $\fZ \cap \fA = \emptyset$. For convenience, we denote $N_{\fZ}=\zeta_{\max}-\zeta_{\min}+1$ and $N_{\fA}=\alpha_{\max}-\alpha_{\min}+1$. 

This section introduces two types of sequence mappings. The first step involves constructing a sequence of positive integers with length $L$, represented as:
\begin{equation}    
\begin{aligned}
    &\fX^{\left(q,L\right)}=\left\{X\middle|X=\left[z_1,\cdots,z_p,a_{p+1},\cdots,a_{p+q},z_{p+q+1},\cdots,z_L\right],z_i\in\fZ, a_i\in \fA \right\},
\end{aligned}
\end{equation}
 % where $z_i\in\fZ:=\{z\in\sN^+|\zeta_{\min}\leq z\leq\zeta_{\max}\}, a_i\in \fA:=\{a\in\sN^+|\alpha^{\rm rsn}_{\min}\leq a\leq\alpha^{\rm rsn}_{\max}\}$. 
 % The subsets $\fZ$ and $\fA$ are referred to as the key set and anchor set, respectively, with the assumption that $\fZ \cap \fA = \emptyset$. 
 We define $q$ as the number of anchors in the sequence, and $p$ as the index of the element immediately preceding the first anchor element $a_{p+1}$ in the sequence.
 % For any element $s$ belonging to the key set or anchor set, it's denoted by $C$-dimensional one-hot representation $\ve^s \in \{0, 1\}^C$, where the $s$-th element equals 1, and all other elements are 0.

For a given sequence $X\in\fX^{\left(q,L\right)}$, we define its key-anchor combination as $(z_p,a_{p+1},\cdots,a_{p+q})$, which is denoted concisely as pair $(z_p,\va)$, and other keys are regarded as noise in this input sequence. The anchor set $\fA$ is divided into two subsets, i.e., reasoning anchor set $\fA_{\rm rsn}$ and memory anchor set $\fA_{\rm mem}$, where $\fA=\fA_{\rm rsn}\cup\fA_{\rm mem}$ and $\fA_{\rm rsn}\cap\fA_{\rm mem}=\emptyset$.

\paragraph{Reasoning mapping.} For any $X$ with $a_{p+i}\in\fA_{\rm rsn}, i=1,\cdots,q$, we define the following mapping as a reasoning mapping
\begin{align*}
    \fF_{\rm rsn}(X) = z_p + \sum_{i=1}^q a_{p+i}.
\end{align*}
% This definition means that we use the basic operation to describe the inference.
%$\vY\in\sN^{|\fZ|\times|\fA|\times|\fA|\cdots|\fA|}$
\paragraph{Memory mapping.} 
For any key-anchor pair $\left(z_p,\va\right)$, where each element in $\va$ belongs to $\fA_{\rm mem}$, we randomly sample a number $y^{\left(z_p,\va\right)}$ from $\fZ$ as the memory mapping label of any sequence $X$ containing $\left(z_p,\va\right)$, i.e.,
\begin{align*}
    \mathcal{F}_{\rm mem}(X) = y^{\left(z_p,\va\right)},\quad \forall X \text{ contains } \left(z_p,\va\right).
\end{align*}

% \paragraph{Memory Mapping} 
% For any possible key-anchor pair $\left(z_p,a_{p+1},\cdots,a_{p+q}\right)\in \fZ\times\fA^{(T)}$, we randomly sample a number $y^{\left(z_p,a_{p+1},\cdots,a_{p+q}\right)}$ from $\fZ$ as memory mapping label of any sequence $X$ containing $[z_p,a_{p+1},\cdots,a_{p+q}]$, i.e.
% \begin{align*}
%     \mathcal{F}_{\rm mem}(X) = y^{\left(z_p,a_{p+1},\cdots,a_{p+q}\right)}.
% \end{align*}
% Specifically, define the label tensor $\vY\in\sN^{|\fZ|\times|\fA|^{(T)}},\vY_$ and each element of $\vY$ is randomly sampled from $[\eta_{\min},\eta_{\max}]\subset\sN^+$. We define the memory mapping as
% \begin{align*}
%     \mathcal{F}_{\rm mem}(X) = \vY_{(z_p-\zeta_{\min},a_{p+1}-\alpha^{\rm rsn}_{\min},\cdots,a_{p+q}-\alpha^{\rm rsn}_{\min})}.
% \end{align*}
% We denote the anchor sets of reasoning mapping and memory mapping by $\fA_{\rm rsn}$ and $\fA_{\rm mem}$, respectively, and assume that $\fA_{\rm rsn}\cap\fA_{\rm mem}=\emptyset$.
% This shows that we define memory mapping as random mapping.

% \paragraph{Variant Mapping} We combine the previous two mappings to define their variant
% \begin{align*}
%     \mathcal{F}_(z_x,a_1,\cdots,a_m) = z_x + f(a_{i1},a_{i2},\cdots,a_{iM}) + y_{J}
% \end{align*}
% where $J$ is the indicator of $\{1,2,\cdots,m\}/\{i1,i2,\cdots,iM\}$.
% \subsection{Task Complexity}
% In Memory-Inference mapping, the task complexity depends on the norm of the random mapping consisted in the mapping. If the mapping can be formulated as
% \begin{align*}
%     \mathcal{F}(z_x,a_1,\cdots,a_m) = z_x + f(a_{i1},a_{i2},\cdots,a_{iM}) + y_{J}
% \end{align*}
% Then we define its complexity
% \begin{align*}
%     C = |\{y_J\}|
% \end{align*}
% It's noted that the complexity would make sense while several mappings are trained by the model in the same time. 
A detailed description is provided in Appendix~\ref{app:dataset}.
\subsection{Dataset Setup}
In this study, we denote a data pair as $\left(X,y\right)$, where $X$ represents the input sequence and $y$ corresponds to its associated label. We define $\vy$ as the one-hot encoded representation of $y$ for convenience. For memory mapping, all data are contained within the training set $\fD_{\rm mem}$, and no test set is employed, as the generalization is not considered in this framework. For reasoning mapping, we define a set of masked anchor combinations $\fM = $$\left\{\left(a_{p+1},a_{p+2},\cdots,a_{p+q}\right)\mid a_{p+i}\in\fA_{\rm rsn},i=1,\cdots,q\right\}$ and designate all sequences containing any masked combination $\left(a_{p+1}, a_{p+2}, \cdots, a_{p+q}\right)\in \fM$ as the test set $\fD_{\rm rsn,test}$ and set the rest sequence as $\fD_{\rm rsn,train}$. The training set is $\fD_{\rm train}=\fD_{\rm mem}\cup\fD_{\rm rsn,train}$.

% To evaluate the model’s learning bias, we compare its performance across these datasets. If the decrease in the model’s loss on $\fD_{\rm mem}$ is nearly identical to the decrease on $\fD_{\rm rsn,train}$ while remaining unchanged on $\fD_{\rm rsn,test}$, the model is classified as exhibiting a memory bias. Conversely, if the losses on both $\fD_{\rm rsn,train}$ and $\fD_{\rm rsn,test}$ decrease at similar rates but substantially faster than on $\fD_{\rm mem}$, the model is classified as exhibiting a reasoning bias.

\subsection{Model Architecture}
We give the formulation of the embedding space and self-attention module here for notation convenience. Let $d_{\rm vob},d_m,d_k$ denote the vocabulary size, embedding space dimension, and query-key-value projection dimension, respectively. For any token $s$, denote its one-hot vector by $\ve^s\in\sR^{1\times d_{\rm vob}}$. The embedding vector of $s$ is $\vw^{{\rm emb},s}=\ve^s\vW^{\rm emb}$ where $\vW^{\rm emb}\in\sR^{d_{\rm vob}\times d_m}$ is the embedding matrix. Additionally, the self-attention operator ${\rm Attn}$ on any embedding sequence $\vX\in\sR^{L\times d_m}$ is defined as:
\begin{align}    
    {\rm Attn}\left(\vX\right)&=g\left({\rm mask}\left(\frac{\vX\vW^Q\vW^{KT}\vX^T}{\sqrt{d_k}}\right)\right),\\
    \vO&={\rm Attn}\left(\vX\right)\vX\vW^V\vW^O,
\end{align}
where $g\left(\cdot\right)$ is the softmax operator and $T$ means the matrix transposition. $\vW^{Q},\vW^K,\vW^V$$\in\sR^{d_m\times d_k}$ are the query, key and value projection matrices, respectively. $\vW^O\in\sR^{d_k\times d_{m}}$ represents the output projection matrix. 
The detailed expression of multilayer Transformer models can be found in Appendix~\ref{sec:model_arc}.

\subsection{Parameter Initialization}
Given any trainable parameter matrix $\vW\in\sR^{d_1\times d_2}$, where $d_1$ and $d_2$ denote the input and output dimensions, respectively, its elements are initialized according to a normal distribution:
\begin{equation*}
    \vW_{i,j} \sim \mathcal{N}\left(0,\left(d_2^{-\gamma}\right)^2\right),
\end{equation*}
where $\gamma$ is the initialization rate. Specifically, the initialization scale decreases as $\gamma$ increases.
Note that $\gamma=0.5$ is commonly used in many default initialization methods, such as LeCun initialization~\cite{LeCun1998} and He initialization~\cite{he2015delving}. As the network width towards infinity~\cite{luo2021phase,zhou2022empirical}, the training of the network with $\gamma>0.5$ exhibits significant non-linear characteristics, i.e., condensation. Therefore, initialization scales with $\gamma>0.5$ are generally considered small. 

% \subsection{Training method}
% We approach the synthetic composition task through a last-token prediction mechanism, where the prediction loss is computed solely for the last token. For the real language task, we employ a next-token prediction method, where the model predicts the subsequent token in the sequence.

\section{Result}
In this section, we present empirical evidence of a reasoning bias during the training of Transformers with small initialization by utilizing composite anchors. To further explore this phenomenon, we introduce a simplified model consisting of an embedding layer and a multi-layer perceptron, which reproduces the reasoning bias and enables theoretical analysis. A key mechanism underlying this bias is that the training behavior of each token's embedding depends on the label distribution of the samples containing that token. For reasoning anchors, the label distributions typically exhibit greater variability compared to memory anchors, leading to the more rapid differentiation of their embeddings early in training. Additionally, we extend our analysis to the Transformer architecture to demonstrate the generalizability of this effect. 
\begin{figure*}[htpb]%重画
    \centering
    \includegraphics[width=1\linewidth]{figure/loss.png}
    % \vspace{-10pt}
    \caption{A: Loss and prediction accuracy of the models on different datasets under varying initialization scales ($\gamma = 0.3, 0.5, 0.8$). The top row depicts the evolution of the loss during training for three datasets: $\fD_{\rm mem}$ (blue lines), $\fD_{\rm rsn,train}$ (purple lines), and $\fD_{\rm rsn,test}$ (orange lines). The bottom row presents the corresponding prediction accuracies for these datasets. Each column represents results obtained with different initialization scales. B: Prediction accuracy of Emb-MLP under initialization rate $\gamma=0.3$ and $\gamma=0.8$.}
    \label{fig:lossgap}
\end{figure*}
\subsection{Reasoning Bias in Transformer with Composite Anchor Functions}
In our experiment, we set that $q=2, L=9$. The dataset is constructed with the following configurations: $\fZ=\{21,\cdots,120\},\fA_{\rm mem}=\{1,\cdots,10\},\fA_{\rm rsn}=\{11,\cdots,20\}$ and $\fM=\{(11,13),(13,11)\}$. The dataset contains 200000 data pairs, ensuring an equal number of samples for each anchor pair. The loss function employed is Cross Entropy and the optimization algorithm used is AdamW. The model architecture comprises a decoder-only Transformer structure with 2 layers and a single attention head. We train the model under different initialization scales with $\gamma=0.3,0.5,0.8$ utilizing the last token prediction method. Additional details about the experimental setup can be found in Appendix~\ref{app:setup}.

% \subsection{Phenomenon}
To investigate the impact on training behavior under varying initialization scales, we analyze the dynamics of loss and prediction accuracy on $\fD_{\rm mem},\fD_{\rm rsn,train}$ and $\fD_{\rm rsn,test}$. As illustrated in Figure \ref{fig:lossgap}A, for $\gamma = 0.3$, the losses on \(\fD_{\rm mem}\) and \(\fD_{\rm rsn,train}\) decrease at nearly identical rates, while the loss on \(\fD_{\rm rsn,test}\) remains effectively unchanged. This observation suggests that the model primarily memorizes the training data in this setting. In contrast, when $\gamma = 0.8$, the losses on \(\fD_{\rm rsn,train}\) and \(\fD_{\rm rsn,test}\) decrease significantly faster than the loss on \(\fD_{\rm mem}\). This behavior indicates a shift towards a reasoning bias in the model. These findings reveal that the model's learning bias is influenced by the initialization scale: as the initialization scale decreases, the model exhibits a progressively stronger reasoning bias.


%The results demonstrate that the initialization scale influences the model’s learning bias: a larger initialization scale favors a memory-oriented learning mode, whereas a small initialization scale favors a reasoning-oriented learning mode.
\subsection{Simplified Model: Phenomena and Analysis}
To further investigate the underlying cause of the reasoning bias under a small initialization scale, we begin by analyzing the embedding space. Specifically, we employ a two-layer fully connected network to address a particular task, where $p\equiv1$ and $L=q+1$. The network structure is defined as follows:
\begin{definition}
    Given that $\vW^{(1)}\in \sR^{d_m\times d_f},\vW^{(2)}\in \sR^{d_f\times d_{\rm vob}}$, and $\sigma$ as the activation function. Given any sequence $X\in \fX^{(q,q+1)}$, we define the Embedding-MLP model (Emb-MLP) $\vG_{\vtheta}$ as
    \begin{align*}
        \vG_{\vtheta}\left(X\right):=\sigma\left(\sum_{s\in X}\vw^{{\rm emb},s}\vW^{(1)}\right)\vW^{(2)}.
    \end{align*}    
\end{definition}
Comparing with a large initialization scale $(\gamma = 0.3)$, a noticeable reasoning bias can still be observed in Figure \ref{fig:lossgap}B for a small initialization scale $(\gamma = 0.8)$.
% \begin{figure}[htbp]
    % \centering
%     \includegraphics[width=1\linewidth]{figure/FNN/FNN-accplot.png}
%     \caption{Prediction accuracy of Embedding-MLP on three types of datasets under initialization rate $\gamma=0.3$ and $\gamma=0.8$. This result gives the same conclusion as Figure~\ref{fig:lossgap}.}
%     \label{fig:bias-emb-mlp}
% \end{figure}

To investigate the causes of the reasoning bias under small initialization for such a simplified model, it's critical to understand the structure of the embedding space. Figure~\ref{fig:emb-mlp-embeddings}A depicts the cosine similarity matrices for embeddings of memory anchors and reasoning anchors at epochs 50 and 900. The results reveal that the cosine similarity between reasoning anchors $s_i,s_j$ decreases with the increase of $|s_i-s_j|$, suggesting that reasoning anchors quickly establish a hierarchical structure within the embedding space. In contrast, the memory anchors exhibit consistently high similarity and alignment, leading to a lack of differentiation among them. Nevertheless, given that the model needs to learn more primitive-level mappings for memory mapping than reasoning mapping, the embedding space associated with memory mapping should, in principle, exhibit greater complexity and variability. This phenomenon reveals that the primary challenge preventing the model from effectively learning memory mapping could highly possibly lie in its difficulty in identifying and differentiating between individual anchors.
\begin{figure}[htp]
    \centering
    \includegraphics[width=0.8\linewidth]{figure/FNN/embedding-init0.8-v2.png}
    \caption{A: Cosine similarity matrices for memory (top row) and reasoning (bottom row) anchors at epoch 50 (left) and epoch 900 (right) of a model initialized with $\gamma=0.8$. B: Distribution of $\vP^{s}-\frac{1}{d_{\rm vob}}\bm{1}$ for different reasoning anchor $s$. C: Cosine similarity between $\vP^{s_i}-\frac{1}{d_{\rm vob}}\bm{1}$ and $\vP^{s_j}-\frac{1}{d_{\rm vob}}\bm{1}$ for any reasoning anchor $s_i,s_j$, exhibiting a similar structure to the embedding space of reasoning anchors observed in A.}
    \label{fig:emb-mlp-embeddings}
\end{figure}
% During the early stages of training, the embedding vectors for memory anchors display high similarity, with nearly identical directions and norms, indicating a lack of differentiation. In contrast, the embedding vectors for reasoning anchors exhibit a distinct hierarchical structure, characterized by diverse directions and varying norms, even at the early stage.

This phenomenon can be interpreted through the training dynamics. To facilitate our analysis, we give the following assumption~\cite{CSIAMchen}:
\begin{assumption}\label{assump:activation}
    The activation function $\sigma\in \mathcal{C}^2(\mathbb{R})$, and there exists a universal constant $C_L > 0$ such that its first and second derivatives satisfy $        ||\sigma^{\prime}(\cdot)||_{\infty}\leq C_L,||\sigma^{\prime\prime}(\cdot)||_{\infty}\leq C_L.$ Moreover, $\sigma(0) = 0,\sigma^{\prime}(0) = 1$.
\end{assumption}
For any token $s$, let $\left\{\left(X^{s,i},y^{s,i}\right)\right\}_{i=1}^{n_{s}}$ denote all input sequences containing $s$ and corresponding labels, where $n_{s}$ means the appearance times of $s$ (For simplicity, if $X$ contains two $s$, then $X$ and its corresponding label appear twice). As the initialization scale decreases, with Assumption~\ref{assump:activation}, we have  
$\sigma^{\prime}\left(\sum_{x\in X^i}\vw^{{\rm{emb}},x}\vW^{(1)}\right)=\bm{1}, {\rm softmax}\left(\vG_{\vtheta}\left(X^{s,i}\right)\right)=\frac{1}{d_{\rm{vob}}}\bm{1}$, where $\textbf{1}\in\sR^{1\times d_{\rm vob}}$ means the vector with all elements equal to 1. Then the gradient flow  of \( \vw^{{\rm emb},s} \) could be approximated by the limit formulation, i.e.,
\begin{equation}\label{eq:gf_emb_g_smallinit}
    \frac{d\vw^{{\rm emb},s}}{dt}=\frac{1}{n}\sum_{i=1}^{n_s}\left(\vy^{s,i}-\frac{1}{d_{\rm vob}}\bm{1}\right)\vW^{(2)T}\vW^{(1)T},
\end{equation}
where $n$ represents the count of all tokens. We consider $n\rightarrow\infty$ to obtain the asymptotic form of the following gradient flow.
\begin{prop}\label{prop:E_emb_g}
    For any token $s$, denote $Y^s$ as a random variable, which takes values randomly from the label of any input sequence that contains token $s$.  In the limit $n\rightarrow\infty$, we define $\vP^s$ with its $i$-th element as the probability of $Y^s=i$, i.e., $\vP^s_i=\mathbb{P}\left(Y^s=i\right)$. Assume the ratio of the token $s$ in the whole dataset $r_s:=\frac{n_s}{n}$ remains constant, then ~\eqref{eq:gf_emb_g_smallinit} can be approximated as:
    \begin{equation}\label{eq:gf_emb_g_limit}
        \frac{d\vw^{{\rm emb},s}}{dt}=r_s\left(\vP^s-\frac{1}{d_{\rm vob}}\bm{1}\right)\vW^{(2)T}\vW^{(1)T}.
    \end{equation}
\end{prop}
Proposition~\ref{prop:E_emb_g} demonstrates that for any token $s$, its embedding vector is dominated by the distribution of $Y^s$ which indicates that it's significant to discuss the distribution $Y^s$ for different tokens. Firstly, we define the following random variables ($\mathcal{U}$ for discrete uniform distribution):
\begin{equation}
    Z\sim \mathcal{U}\left(\fZ\right),\quad A_j\sim \mathcal{U}\left(\fA_{\rm rsn}\right),\quad j=1,2,\cdots,q.
\end{equation}
Then we have the following results:
\begin{equation}\label{eq:dist_mem}
    \mathbb{P}\left(Y^{s}=i\mid s\in\fA_{\rm mem}\right) = \frac{1}{N_{\fZ}}\delta_{i\in\fZ},
\end{equation}
% for any $s\in\fA_{\rm mem}$, where $\delta_{i\in\fZ}=1$ if $i\in\fZ$ otherwise 0. Besides, for any $s\in\fA_{\rm rsn}$, we have:
and 
\begin{equation}\label{eq:dist_rsn}
\begin{aligned}
    \mathbb{P}\left(Y^{s}=i\mid s\in\fA_{\rm rsn}\right)=\mathbb{P}\left(Z+\sum_{j=1}^{q-1}A_j=i-s\mid s\in\fA_{\rm rsn}\right).
\end{aligned}
\end{equation}

Equation~\eqref{eq:dist_mem} reveals that the information to different memory anchors is identical such that the embedding space of memory anchors exhibits a high similarity. However, \eqref{eq:dist_rsn} demonstrates that the
 distribution of $Y^s$ exhibits shifts in the mean values that depend on the specific $s$ for any $s\in\fA_{\rm rsn}$. Figure \ref{fig:emb-mlp-embeddings}B and \ref{fig:emb-mlp-embeddings}C visualize the distribution of $\vP^s-\frac{1}{d_{\rm vob}}\bm{1}$ and the resulting cosine similarity among different reasoning anchors $s$, suggesting that the labels' distributions play a critical role in establishing the embedding structure of reasoning anchors during the early stages of training, facilitating the differentiation among the embeddings associated with different reasoning anchors. 
 The detailed formulations can be found in Appendix~\ref{sec:dist_ys}.
 % The technical conditions and detailed analysis are provided in Appendix~\ref{app:theory_of_emd&dist} for further reference.
% \begin{figure}[htpb]
%     \centering
%     \includegraphics[width=0.7\linewidth]{figure/FNN/plot_R.png}
%     \caption{Distribution of $\vP^s$ and cosine similarity among different $s$}
%     \label{fig:P_alpha}
% \end{figure}


\subsection{Transformer with General Task}
In the previous section, we investigate the key mechanisms driving the learning bias of Emb-MLP and analyze the dynamics of its embedding space. However, when applied to a general sequence containing some degree of noise, i.e., $L>q+1$, we find the MLP model fails to perform effectively due to its inability to extract critical tokens $z_p, a_{p+1},$ and $a_{p+2}$. In contrast, Transformer architectures overcome this limitation through self-attention mechanisms, which can identify the key and anchor elements and propagate their information.


In the following section, we conduct an in-depth analysis of the Transformer's characteristics and processing mechanisms under a small initialization scale. Specifically, we investigate whether the embedding space exhibits similar phenomena to those observed in Emb-MLP and assess how the model captures critical information from the input sequence. 
% To clarify the underlying mechanism of the model, we set $\fA_{\rm mem}=\{1,2,\cdots,10\},\fA_{\rm rsn}=\{11,12,\cdots,20\},\fM=\{(11,13),(13,11)\}$, and train a Transformer model initialized with $\gamma=0.8$. 


\paragraph{Embedding space.}
The embedding space of the Transformer exhibits a phenomenon similar to that observed in the Emb-MLP. Figure~\ref{fig:embeding-transformer}A illustrates the cosine similarity among different anchors' embedding vectors, revealing distinct patterns for reasoning and memory tasks. Reasoning anchors display a hierarchical structure, the further distance, the smaller similarity, suggesting a clearer organization within the embedding space. In contrast, memory anchors exhibit high similarity and alignment. Additionally, we apply Principal Component Analysis (PCA) to the entire embedding space to examine its structural properties. The results in Figure~\ref{fig:embeding-transformer}B reveal a strong inherent numerical order. This structure is particularly advantageous for reasoning tasks, as it supports the model’s capacity to generalize based on the underlying numerical relationships.
\begin{figure*}[htbp]
    \centering
    \includegraphics[width=1\linewidth]{figure/embedding/embedding_anchor-v3-icml.png}
    \caption{Embedding structure of a Transformer model with small initialization scale. A: Cosine similarity matrices for memory (top) and reasoning (bottom) anchors at epoch 200 (left) and epoch 900 (right). B: Visualization of the embedding space projected onto the first two principal components computed via PCA. C: Cosine similarity between the constructed embedding vectors of reasoning anchors $\tilde{\vw}^{\rm emb,s}$ (see ~\eqref{eq:estimation_wemb}) as derived in Theorem~\ref{thm:emb_reconstruct} (top) and Cosine similarity comparison between experimental results with theoretical approximations where $s_i=15$ (bottom). D: PCA projection of the constructed embedding space $\tilde{\vw}^{\rm emb,s}$ for $s\in\fZ$ (top) and $s\in\fA_{\rm rsn}$ (bottom) onto the first two principal components.}
    \label{fig:embeding-transformer}
\end{figure*}

\paragraph{First attention module.}
As illustrated in Figure~\ref{fig:first_attn}, the first attention matrix approaches the behavior of an average operator when the initialization scale decreases, such that $\left({\rm Attn}\left(\vX\right)\vV\right)_i=\frac{1}{i}\sum_{j\leq i}\vV_j$, where $\vV=\vX\vW^{V}$. Consequently, each token aggregates information from all preceding tokens. Additionally, the largest singular value of $\vW^{V}$ is significantly larger than the remaining singular values, and its corresponding singular vector is aligned closely with the embedding vectors of reasoning anchors, but nearly orthogonal to those of memory anchors. These phenomena suggest reasoning anchors are prominently captured by $\vW^V$ and subsequently propagated to all subsequent tokens in the sequence via the average operation. However, the memory anchors are not distinctly identified, indicating that the model faces challenges in capturing significant information from a memory sequence effectively. More analysis of $\vW^V$ can be found in Appendix~\ref{app:W_V}.

\begin{figure*}[htbp]
    \centering
    \includegraphics[width=1\linewidth]{figure/first_attention/first_attention.png}
    \caption{Characteristics of the first attention module under small initialization ($\gamma=0.8$). A: Heatmap of the attention matrix for a random sample. B: Distribution of the relative error between attention $A_{jk}$ and $\frac{1}{j}$ across all training sequences. C: Distribution of singular values of $\vW^{V}$. D: Cosine similarity between the left singular vectors and average embedding vectors of the anchors. } 
    \label{fig:first_attn}
\end{figure*}
% Based on these observations, we propose a formal definition of the attention operator as follows:
% \begin{definition}\label{def:AVA}
%     Given a anchor set $\fA$, it's average embedding vector is formulated as $\overline{\vw^{\rm emb}}_{\fA}=\frac{1}{|\fA|}\sum_{a\in\fA}\vw^{{\rm emb},a}$. Given the singular value $\tilde{\lambda}_V$ and singular vector $\vv$, we define the $\fA\text{-}\vW_{\vV}$ as $\tilde{\lambda}_V\overline{\vw^{\rm emb}}_{\fA}\vv^T$. Given a sequence $X$ with embedding sequence $\vW^{{\rm emb},x}$, we define the $\fA$-attention operator $AV_{\fA}$ as
%     \begin{align*}
%     AV_{\fA}(X;\tilde{\lambda}_V,\vv)_j &= \tilde{\lambda}_V\left(\frac{1}{j}\sum_{i\leq j}\vW^{{\rm emb},x}_i\right)^T\overline{\vw^{\rm emb}}_{\fA}\vv.
% \end{align*}
% \end{definition}

% %???写下面这个定义每一个部分的作用，为什么要定义这个东西。
% The first attention module can be interpreted as an $AV_{\fA_{\rm rsn}}$ operator. Assume that $\vw^{{\rm emb},s_{\rm rsn}}\perp \vw^{{\rm emb},s_{\rm mem}}$ for any $s_{\rm rsn}\in\fA_{\rm rsn},s_{\rm mem}\in\fA_{\rm mem}$,
% and substitute the reasoning sequence $X^{\rm rsn}$ and memory sequence $X^{\rm mem}$, respectively, into this formulation, we derive the following results:
% \begin{align}
%     AV_{\fA_{\rm rsn}}\left(X^{\rm mem};\tilde{\lambda}_V,\vv\right)_j &= \bm{0},\\
%     AV_{\fA_{\rm rsn}}\left(X^{\rm rsn};\tilde{\lambda}_V,\vv\right)_j &= \left\{\begin{aligned}
%         &\bm{0},\quad j\leq p,\\
%         &\tilde{\lambda}_V\frac{1}{j}\left(\sum_{i=p+1}^{\min(j,p+q)}\vW^{{\rm emb},X^{\rm rsn}}_i\right)^T\overline{\vw^{\rm emb}}_{\fA_{\rm rsn}}\vv,\quad p<j\leq L
%     \end{aligned}\right.
% \end{align}
% Where $j$ means the row index. This mechanism enables the model to easily identify reasoning anchors and augment them with additional information through $\vv$, facilitating their extraction and processing in the subsequent layer. 

\paragraph{Second attention module.}
The second attention module functions to extract the key, and propagate its information to the final position in the sequence. This is facilitated through the use of position embeddings. Since this mechanism is applicable to both memory and reasoning tasks, a detailed explanation is provided in Appendix~\ref{sec:second_attention_app}.

\paragraph{Theoretical analysis.} Based on the observations from the experiments, we extract the sketch component of the model, which is crucial to its learning preferences, and analyze the underlying mechanisms for its occurrence. We define the following one-layer Transformer model: 
\begin{definition}[One-layer Transformer]\label{def:1_layer_transformer}
    Let $d_f\in\sN^+$ denotes the hidden layer of the feedforward neural network (FNN). For any $X\in\fX^{\left(q,L\right)}$, denote ${\rm Attn}\left(\vW^{{\rm emb},X}\right)$ by $\vA$, then we define $\vf_{\vtheta}:\fX^{\left(q,L\right)}\rightarrow \sR^{d_m}$ as follows:
    \begin{equation} 
\begin{aligned}
    \vf_{\vtheta}(X) = \sigma((\vA_{L,:}\vW^{{\rm emb},X}\vW^V\vW^O+\vW^{{\rm emb},X}_{L,:})\vW^{f1})\vW^{f2}+ \vA_{L,:}\vW^{{\rm emb},X}\vW^V\vW^O+\vW^{{\rm emb},X}_{L,:}.
\end{aligned}
    \end{equation}
Where $\vW^{f1}\in\mathbb{R}^{d_m\times d_f},\vW^{f2}\in\mathbb{R}^{d_f\times d_m}$ are the feedforward layer projection matrices. The subscript $L$ in $\vA_{L,:}$ and $\vW^{{\rm emb},X}_{L,:}$ denotes the $L$-th row.
\end{definition}
Definition~\ref{def:1_layer_transformer} is introduced to facilitate the theoretical analysis, excluding the Layer Normalization and the final projection operator, as they do not impact our results. 
% \begin{definition}[One-layer Transformer]\label{def:1_layer_transformer}
%     Let $d_f\in\sN^+$ denotes the hidden layer of the feedforward neural network (FNN). For any $X\in\fX^{\left(q,L\right)}$, we define $\vf_{\vtheta}:\fX^{\left(q,L\right)}\rightarrow \sR^{d_m}$ as follows:
%     \begin{equation} 
% \begin{aligned}
%     \vf_{\vtheta}(X) =& \sigma((\vA_{L,:}\vW^{{\rm emb},X}\vW^V\vW^O+\vW^{{\rm emb},X}_{L,:})\vW^{f1})\vW^{f2} \\+& \vA_{L,:}\vW^{{\rm emb},X}\vW^V\vW^O+\vW^{{\rm emb},X}_{L,:}.
% \end{aligned}
%     \end{equation}
% Where $\vW^{{\rm emb},X}\in\sR^{L\times d_m}$ represents the embedding sequecne of $X$. $\vW^V\in \mathbb{R}^{d_m\times d_k},\vW^O\in\mathbb{R}^{d_k\times d_m}$ are the value and output projection matrices, respectively, and $\vW^{f1}\in\mathbb{R}^{d_m\times d_f},\vW^{f2}\in\mathbb{R}^{d_f\times d_m}$ are the feedforward layer projection matrices. The attention operator $\vA\in \sR^{L\times L}$ is expressed as
% \begin{align*}
%     \vA=softmax\left(mask\left(\frac{\vW^{{\rm emb},X}\vW^Q\vW^{KT}\vW^{{\rm emb},XT}}{\sqrt{d_k}}\right)\right),
% \end{align*}
% where $\vW^{Q},\vW^K\in\sR^{d_m\times d_k}$ are the query and key projection matrices, respectively. The subscript $L$ in $\vA_{L,:}$ and $\vW^{{\rm emb},X}_{L,:}$ denotes the $L$-th row.
% \end{definition}

As we observed, with a small initialization scale, the attention operator $\vA$ can be interpreted as an average operator. Specifically, we have
\begin{lemma}\label{lem:attention}%改成依概率收敛。
    For any $\varepsilon\in(0,1]$, there exists $C>0$ such that for any $\gamma>C$, the elements of $\vA$ at initialization, denoted by $\vA_{i,j}$, satisfy $\left|\vA_{i,j}-\frac{1}{i}\right|\leq\varepsilon$ for any $i\leq j$ with probability at least $1-\varepsilon$.
\end{lemma}
% By Lemma \ref{lem:attention}, we define the average embedding $\overline{\vW}^{{\rm emb},X}:=\vA_{L,:}\vW^{{\rm emb},X}=\frac{1}{L}\sum_{l=1}^L\vW^{{\rm emb},X}_l$. Then, $\vf_{\vtheta}$ can be rewritten as:
% \begin{equation}
% \begin{aligned}
%     \vf_{\vtheta}(X) = &\sigma((\overline{\vW}^{{\rm emb},X}\vW^V\vW^O+\vW^{{\rm emb},X}_{L,:})\vW^{f1})\vW^{f2} \\+& \overline{\vW}^{{\rm emb},X}\vW^V\vW^O+\vW^{{\rm emb},X}_{L,:}.
% \end{aligned}
% \end{equation}

 Denote that $\vW^f=\vW^{f1}\vW^{f2},\vW^{VO}=\vW^V\vW^O$ and $\tilde{\vW}=\left(\vW^{f,T}+\vI\right)\left(\vW^{VO,T}+\vI\right)$, where the identity matrix $\vI$ comes from the resnet. Using techniques similar to those employed in the previous section, we derive the gradient flow of $\vw^{{\rm emb},s}$ under small initialization scales as follows:

 \begin{prop}\label{prop:emb_mem_tran}
     For any $s\in\fA_{\rm mem}$, let $n,\gamma\rightarrow\infty$, with Assumption~\ref{assump:activation} we have the following result:
     \begin{equation}
        \frac{d\vw^{{\rm emb},s}}{dt}=\frac{r_s}{L}\left(\frac{\vdelta^\fZ}{N_{\fZ}}-\frac{1}{d_m}\bm{1}\right)\tilde{\vW}.
     \end{equation}
\end{prop}
  \begin{prop}\label{prop:emb_rsn_tran}
      For any $s\in\fA_{\rm rsn}$, let $n,\gamma\rightarrow\infty$, with Assumption~\ref{assump:activation} we have the following result:
     \begin{equation}
        \frac{d\vw^{{\rm emb},s}}{dt}=\frac{r_s}{L}\left(\vP^{s}-\frac{1}{d_m}\bm{1}\right)\tilde{\vW},
     \end{equation}
     where the $i$-th element of $\vP^s$ is defined as $\vP^{s}_i=\mathbb{P}\left(Z+\sum_{j=1}^{q-1}A_j=i-s\mid s\in\fA_{\rm rsn}\right)$.
 \end{prop}
 Furthermore, we utilize the normal distribution to approximate the distribution of $Y^{s},s\in\fA_{\rm rsn}$ and give an approximation of $\vw^{{\rm emb},s}$ to describe the overall structure and internal relationships within the embedding space observed in real-world training scenarios.
\begin{theorem}\label{thm:emb_reconstruct}
    let $n\rightarrow\infty$, define the learning rate $\eta$ and assume that $L-q=O(1),\frac{r_s\eta}{L}=O(1)$ and $||\vw^{\rm emb}||_{\infty}\leq O\left(d_m^{-\gamma}\right)$ at initialization. We propose the approximation of $\vw^{{\rm emb},s},s\in\fA_{\rm rsn}$ by
    \begin{equation}\label{eq:estimation_wemb}
        \tilde{\vw}^{{\rm emb},s}_j=C_1\left(C_2e^{-\frac{(j-s)^2}{2\sigma_P}}-\frac{1}{d_m}\right)+\varepsilon,
    \end{equation}
    where $C_1,C_2,\sigma_P$ are constants depending on $r_s,\eta,L,q$ and  $\varepsilon\sim\fN(0,(d_m^{-\gamma})^2)$. Then we have the following result
    \begin{equation}
    \begin{aligned}
        \sup_{i,j}&{\left|\left(\tilde{\vw}^{{\rm emb},s_j},\tilde{\vw}^{{\rm emb},s_i}\right)-\left({\vw}^{{\rm emb},s_j},{\vw}^{{\rm emb},s_i}\right)\right|}\\ &\leq O\left(d_m^{1-\gamma}\left(q^{-\frac{1}{2}}+d_m^{-\gamma}\right)\right),
    \end{aligned}
    \end{equation}
    where $\left(.,.\right)$ denotes the inner production.
\end{theorem}

Additionally, for any key $z\in\fZ$ and $\fA_{mem}$, we could have a similar result.
     % \begin{align*}
     %     % \vw^{{\rm emb},z}_i=&\frac{r_s\eta}{L}\left(\frac{1}{2N_{\fZ}}+\frac{1}{2\left(L-T\right)}\left(\mathbb{P}\left(z+\sum_{j=1}^TA_j=i\right)+\left(L-T-1\right)\mathbb{P}\left(Z+\sum_{j=1}^TA_j=i\right)\right)-\frac{1}{d_m}\right)+\varepsilon\\
     %     % =&\frac{r_s\eta}{2L\left(L-T\right)}\mathbb{P}\left(z+\sum_{j=1}^TA_j=i\right)+\frac{r_s\eta}{L}\left(\frac{1}{2N_{\fZ}}+\frac{L-T-1}{2\left(L-T\right)}\mathbb{P}\left(Z+\sum_{j=1}^TA_j=i\right)-\frac{1}{d_m}\right)+\varepsilon\\
     %     &\vw^{{\rm emb},z}_i=\frac{r_s\eta C}{2L\left(L-T\right)}e^{-\frac{\left(i-z-\tilde{\mu}\right)^2}{2\tilde{\sigma_P}}}\\&+\frac{r_s\eta}{L}\left(\frac{1}{2N_{\fZ}}+\frac{L-T-1}{2\left(L-T\right)}\mathbb{P}\left(Z+\sum_{j=1}^TA_j=i\right)-\frac{1}{d_m}\right)+\varepsilon
     % \end{align*}
To validate our theory analysis, we set the detailed formulation of reasoning anchors and keys as
\begin{equation}
\begin{aligned}
    &\tilde{\vw}^{{\rm emb},s}_i=e^{-\frac{\left(i-s\right)^2}{12}}-\frac{1}{d_m}+\varepsilon,\quad s\in\fA_{\rm rsn},\\
    &\tilde{\vw}_i^{{\rm emb},s}=e^{-\frac{\left(i-s\right)^2}{12}}+\varepsilon,\quad s\in\fZ.
\end{aligned}
\end{equation}
 Figure~\ref{fig:embeding-transformer}C exhibits the cosine similarity among the $\tilde{\vw}^{{\rm emb},s}$ for any $s\in\fA_{\rm rsn}$ (top) and compare $\cos\left(\vw^{\rm emb,15},\vw^{\rm emb,s_j}\right)$ in real training process with the theoretical approximation $\cos\left(\tilde{\vw}^{\rm emb,15},\tilde{\vw}^{\rm emb,s_j}\right)$ (bottom, a complete comparison is provided in Appendix~\ref{app:validation_of_theory}). Figure~\ref{fig:embeding-transformer}D presents the PCA projection of $\vw^{{\rm emb},s}$ for $s\in\fA_{\rm rsn}$ and $\fZ$, respectively. These visualizations exhibit a strong alignment with the experimental observations, thereby substantiating the validity of our analysis.
% Consider the linear expansion of the parameter, i.e. $\vw^{{\rm emb},s}=\vw^{{\rm emb},s}_{t_0}+\frac{d\vw^{{\rm emb},s}}{dt}t$, we can compute the inner dot between different $s$
% \begin{equation}\label{eq:inner_dot}
% \begin{aligned}
% \left(\vw^{{\rm emb},s_1},\vw^{{\rm emb},s_2}\right)=&\left(\vw^{{\rm emb},s_1}_{t_0},\vw^{{\rm emb},s_2}_{t0}\right)+\left(\vw^{{\rm emb},s_1}_{t_0},\frac{d\vw^{{\rm emb},s_2}}{dt}t\right)+\left(\vw^{{\rm emb},s_2}_{t_0},\frac{d\vw^{{\rm emb},s_1}}{dt}t\right)+\\
% &\left(\frac{d\vw^{{\rm emb},s_1}}{dt}t,\frac{d\vw^{{\rm emb},s_2}}{dt}t\right)\\
% =&O\left(||\vw^{\rm emb}||_2^2\right)+O\left(||\vw^{\rm emb}||_1\right)+\frac{r_{s_i}r_{s_j}t^2}{L^2}\mathbb{E}_{Y^{s_i}}\left[\vy^{s_i}-\frac{1}{d_m}\bm{1}\right]^T\mathbb{E}_{Y^{s_j}}\left[\vy^{s_j}-\frac{1}{d_m}\bm{1}\right]&\\
% % =&\frac{r_{s_i}r_{s_j}t^2}{L^2}\mathbb{E}_{y^{s_i}}\left[\vy^{s_i}-\frac{1}{d_m}\bm{1}\right]^T\mathbb{E}_{y^{s_j}}\left[\vy^{s_j}-\frac{1}{d_m}\bm{1}\right]+O(\vw^T\bm{1})
% \end{aligned}
% \end{equation}

 

\subsection{Embedding of Real Language Tasks}
To further validate our analysis, we examine the embedding space of the GPT-2 model which we train on the PrOntoQA dataset and TinyStories dataset during the early stages of training. Figure~\ref{fig:realtask} reveals that the embeddings of tokens in PrOntoQA are significantly more distinguishable from each other compared to the tokens in TinyStories. The average cosine similarity among the PrOntoQA is 0.123 while 0.531 among the TinyStories. These results provide strong support for our analysis, highlighting the impact of embedding distinguishability on training preference.
\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.6\linewidth]{figure/PrOntoQA/emb_cotstory.png}
    \caption{Cosine similarity among embedding space of PrOntoQA dataset and TinyStories dataset. }
    \label{fig:realtask}
\end{figure}

% \section{Discussion}

\subsection{Effect of Label's Distribution}
Previous sections reveal that under small initialization, the distribution of labels plays a pivotal role in shaping the embedding space of tokens and regulating the model’s training dynamics. To more intuitively demonstrate the impact of the label distribution of each token on its output, we designed four groups of memory mappings. The label ranges of the four groups are set to $\{30,\cdots,29+20\times i\},i=1,2,3,4$. The right picture of Figure \ref{fig:label} illustrates the distribution of the model's outputs for each group during the early stages of training. Notably, it can be observed that even at this initial stage, when the model's accuracy is still relatively low, its outputs do not exceed the range of the label distributions. This highlights the critical influence of label distributions on the token structure, which in turn significantly impacts the model’s outputs. Additionally, we compare two memory tasks with differing label distributions. In the first task, denoted $\fF_{mem,1}$, for any key-anchor pair $(z_p,\va)$, the label $y^{(z_p,\va)}$ is randomly sampled from $\fZ$. In the second task $\fF_{mem,2}$, $y^{(z_p,\va)}$ is randomly sampled from $\left\{z_p-\sum_{i=p+1}^{p+q}a_i,\cdots,z_p+\sum_{i=p+1}^{p+q}a_i\right\}$. While both tasks are clearly memory tasks, the label distributions in $\fF_{mem,2}$ vary depending on the anchor. As shown in the left picture of Figure \ref{fig:label}, the learning rate for $\fF_{mem,2}$ is significantly faster than that for $\fF_{mem,1}$. This observation underscores the crucial role that label distribution plays in the model's learning process.
\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.8\linewidth]{figure/label/plot_label.png}
    \caption{Left: Distribution of targets and predictions in 4 groups of memory tasks. Red represents the target distribution in each group and blue represents the prediction distribution. Right: Learning speed comparison for $\fF_{\rm mem,1}$ (red) and $\fF_{\rm mem,2}$ (blue).}
    \label{fig:label}
\end{figure}

\section{Conclusions}
In this paper, we investigate the underlying mechanism of which small initialization scales promote a reasoning preference in language models. Our findings suggest that the label distribution of tokens plays a pivotal role in shaping the embedding space, thereby influencing the learning dynamics and task complexity. Our result can be readily extended to the next-token prediction training and obtain similar results.  This perspective is supported through a combination of experimental observations and theoretical analysis, providing a deeper understanding of how initialization strategies impact task-specific behavior in language models. 


% \section*{Impact Statement}
% Our works provide new insights into the intrinsic mechanisms underlying the reasoning bias of language models under small initialization scales, as well as the training behavior of individual modules within the model architecture. These findings not only contribute to understanding the model behavior and training mechanisms, but also help with optimizing model initialization strategies and designing novel algorithms to enhance the reasoning capabilities of language models.


% \bibliographystyle{icml2025}
\bibliographystyle{plain}
% \bibliographystyle{IEEEtran}
\bibliography{ref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn

\section{Basic Settings}\label{app:setup}

\subsection{Synthetic Compositional Task}\label{app:dataset}
To enhance comprehension of the synthetic compositional task, we provide a detailed explanation using specific examples. Figure~\ref{fig:task} illustrates the precise formulation of the dataset, where each input sequence comprises noise, key, and anchor tokens. The key-anchor pair may occur at any position within the sequence. The label is independent of both the noise tokens and the position of the key-anchor pair within the sequence but is determined solely by the value of the key-anchor pair.
\begin{figure}[hbpt]
    \centering
    \includegraphics[width=1\linewidth]{figure/appendix/task_plot.png}
    \caption{Schematic diagram of the synthetic composition task. The gray-shaded area illustrates the specific setup used in this example. Each block represents a token within the input sequence, with different face colors indicating distinct token types (blue: noise, orange: key, green: anchor). Each row corresponds to an input sequence paired with its respective label. The left section depicts four examples of memory mapping, while the right section presents four examples of reasoning mapping. }
    \label{fig:task}
\end{figure}


\subsection{Transformer Architecture}\label{sec:model_arc}
For any sequence $X\in\fX^{\left(q,L\right)}$, we denote its one-hot vector by $\ve^X$. The word embedding $\vW^{\mathrm{emb}}$ and the input to the first Transformer block $X^{(1)}$ is calculated as:
\begin{equation}
\vW^{\mathrm{emb},X} = \ve^X\vW^{\mathrm{emb}},\text{\quad} \vX^{(1)} = \vW^{\mathrm{emb},X} + \vW^{\mathrm{pos}},
\end{equation}
where $\vW^{\mathrm{pos}}$ is a trainable positional vector. For the $l$-th layer, the $\vQ, \vK, \vV$ are defined as:
\begin{equation}
    \vQ^{(l)} = \vX^{(l)}\vW^{Q(l)},  \quad \vK^{(l)} = \vX^{(l)}\vW^{K(l)}, \quad \vV^{(l)} = \vX^{(l)}\vW^{V(l)}.
\end{equation}
The attention matrix ${\rm Attn}^{(l)}$ and its subsequent output  $\vX^{\mathrm{qkv}(l)}$ for the $l$-th layer is computed as:
\begin{equation}
    {\rm Attn}^{(l)} = \mathrm{softmax}\left({\rm mask}\left(\frac{\vQ^{(l)}\vK^{(l)T}}{\sqrt{d_k}}\right)\right) , \quad  \vX^{\mathrm{qkv}(l)} = {\rm Attn}^{(l)} \vV^{(l)}.
\end{equation}
The output of the $l$-th attention layer is obtained as:
\begin{equation}
    \vX^{\mathrm{ao}(l)} = \text{LN}\left(\vX^{(l)} + \vX^{\mathrm{qkv}(l)}\vW^{O(l)}\right), \quad \vX^{(l+1)}:=\vX^{\mathrm{do}(l)}=\text{LN}\left(\text{MLP}\left(\vX^{\mathrm{ao}(l)}\right)+\vX^{\mathrm{ao}(l)}\right), 
\end{equation}
% The output of the $l$-th decoder layer $X^{\mathrm{do}(l)}$ (also denoted as $X^{(l+1)}$) is then obtained by applying the two-layer ReLU FNN to $X^{\mathrm{ao}(l)}$ followed by layer normalization and residual connection. 
where ``LN'' refers to Layer Normalization. The final output is obtained by projecting the output of the last layer $\vX^{\mathrm{do}(L)}$ using a linear projection layer, followed by a softmax operation and argmax to obtain the predicted token.

\subsection{Experimental Setups}
For those experiments about the Transformer structure, we train three Transformer models on a dataset of 200,000 samples, with each input sequence having a fixed length of 9 tokens. The vocabulary size $d_{\rm vob}$ is set to 200, and the model architecture includes an embedding dimension $d_m$ of 200, a feedforward dimension $d_f$ of 512, and query-key-value projection dimension $d_k$ of 64. The Transformer-based model uses 2 decoder layers with 1 attention head per layer. The training is conducted for 1000 epochs with a batch size of 100, and gradient clipping is applied with a maximum norm of 1. The AdamW optimizer is employed with an initial learning rate of $1 \times 10^{-5}$. The initialization rates of the three models are $\gamma=0.3,0.5,0.8$.

For those experiments related to Emb-MLP, we train three Emb-MLP models with $d_{\rm vob}=200,d_m=200,d_f=512$ and initialization scales $\gamma=0.3,0.5,0.8$. We employed a dataset comprising $1,000,000$ data pairs. We set that $\fA_{\rm mem}=\left\{1,2,\cdots,10\right\},\fA_{\rm rsn}=\left\{11,12,\cdots,20\right\},\fZ=\left\{21,22,\cdots,120\right\},\fM=\left\{\left(11,13\right),\left(13,11\right)\right\}$. The initial learning rate is $5\times 10^{-6}$ and all other training setups remain consistent with those described in the first paragraph.

For Figure~\ref{fig:loss_PrOntoQA} and Figure~\ref{fig:realtask}, we use a GPT-2 model with an initialization scale $\gamma=0.8$. The dataset contains 10,000 data sequences, with half of them sourced from PrOntoQA and the other half from TinyStories. The AdamW optimizer is employed with an initial learning rate of $1 \times 10^{-5}$. The model is trained for 200 epochs, ensuring that the loss for both datasets decreases to a similar level.




\section{Theory Details}


\subsection{Proof of Proposition~\ref{prop:E_emb_g}}
\begin{lemma}\label{lem:gf_emb_G}
   For any token \( s \), let $\{\left(X^{s,i},y^{s,i}\right)\}_{i=1}^{n_{s}}$ denote all input sequences containing $s$ and corresponding labels. The gradient flow of \( \vw^{{\rm emb},s} \) can be expressed as:
\begin{equation}
\frac{d\vw^{{\rm emb},s}}{dt} = \frac{1}{n}\sum_{i=1}^{n_s}\vW^{(1),T}\left(\vW^{(2),T}(\vy^{s,i} - \vp^{s,i})\right)\odot \sigma^{\prime}(\vW^{(1)}\vw^{{\rm emb},s}),
\end{equation}

where $\vp^{s,i}=\rm{softmax}\left(\vG_{\vtheta}\left(X^{s,i}\right)\right)$, $\sigma^{\prime}$ denotes the derivative of $\sigma$ and $n$ means the count of all training data. $\odot$ represents the elements-wise production.
\end{lemma}
\begin{proof}
For any data pair $\left(X^{s,i},y^{s,i}\right)$, the cross entropy function $R$ could be expressed as:
\begin{align*}
    R\left(X^{s,i}\right)=-\log \frac{e^{\vG_{\vtheta}\left(X^{s,i}\right)_{y^{s,i}}}}{\sum_{j=1}^{d_{\rm vob}}e^{\vG_{\vtheta}\left(X^{s,i}\right)_j}},
\end{align*}
where the subscript $j$ represents the element index. Then the derivative of $R$ respect with $\vw^{{\rm emb},s}$ can be expressed as:
\begin{align*}
    \frac{\partial R\left(X^{s,i}\right)}{\partial\vw^{{\rm emb},s}}&=\sum_{j=1}^{d_{\rm vob}}\frac{\partial R\left(X^{s,i}\right)}{\vG_{\vtheta}\left(X^{s,i}\right)_j}\frac{\partial \vG_{\vtheta}\left(X^{s,i}\right)_j}{\partial\vw^{{\rm emb},s}}=\sum_{j=1}^{d_{\rm vob}}(\vp^{s,i}_j-\vy^{s,i}_j)\frac{\partial \vG_{\vtheta}\left(X^{s,i}\right)_j}{\partial\vw^{{\rm emb},s}}.\\
\end{align*}
Using the trace theorem, we obtain:
\begin{align*}
    d  \vG_{\vtheta}\left(X^{s,i}\right)_j&= \text{tr}\left(\vG_{\vtheta}\left(X^{s,i}\right)_j\right)=\text{tr}\left(d\sigma\left(\sum_{x\in X^{s,i}}\vw^{{\rm emb},x}\vW^{(1)}\right)\vW^{(2)}_{:,j}\right)\\
    &=\text{tr}\left(\sigma^{\prime}\left(\sum_{x\in X^{s,i}}\vw^{{\rm emb},x}\vW^{(1)}\right)\odot \left(d\vw^{{\rm emb},s}\vW^{(1)}\right)\vW^{(2)}_{:,j}\right)\\
    &=\text{tr}\left(\vW^{(1)}\left(\vW^{(2)}_{:,j}\odot\sigma^{\prime}\left(\sum_{x\in X^{s,i}}\vw^{{\rm emb},x}\vW^{(1)}\right)^T\right)d\vw^{{\rm emb},s}\right).
\end{align*}
Then we have
\begin{align*}
    \frac{\partial R\left(X^{s,i}\right)}{\partial\vw^{{\rm emb},s}}&=\sum_{j=1}^{d_{\rm vob}}\left(\vp^{s,i}_j-\vy^{s,i}_j\right)\left(\vW^{(2)T}_{:,j}\odot\sigma^{\prime}\left(\sum_{x\in X^{s,i}}\vw^{{\rm emb},x}\vW^{(1)}\right)\right)\vW^{(1)T}\\
    &=\left(\left(\vp^{s,i}-\vy^{s,i}\right)\vW^{(2)T}\right)\odot\sigma^{\prime}\left(\sum_{x\in X^{s,i}}\vw^{{\rm emb},x}\vW^{(1)}\right)\vW^{(1)T},
\end{align*}
and furthermore
\begin{align*}
    \frac{d\vw^{{\rm emb},s}}{dt} = -\frac{1}{n}\sum_{i=1}^{n_s}\frac{\partial R\left(X^{s,i}\right)}{\partial\vw^{{\rm emb},s}}=\frac{1}{n}\sum_{i=1}^{n_s}\left(\left(\vp^{s,i}-\vy^{s,i}\right)\vW^{(2)T}\right)\odot\sigma^{\prime}\left(\sum_{x\in X^{s,i}}\vw^{{\rm emb},x}\vW^{(1)}\right)\vW^{(1)T}.
\end{align*}
\end{proof}
 As the initialization scale decreases $\gamma\rightarrow0$, with the Assumption~\ref{assump:activation}, we have that 
$\sigma^{\prime}\left(\sum_{x\in X^{s,i}}\vw^{{\rm emb},x}\vW^{(1)}\right)=\bm{1}, \text{softmax}\left(\vG_{\vtheta}\left(X^{s,i}\right)\right)=\frac{1}{d_{\rm vob}}\bm{1}$, where $\textbf{1}\in\sR^{1\times d_{\rm vob}}$ means the vector with all elements equal to 1. Then the gradient flow  of \( \vw^{{\rm emb},s} \) could be approximated by the limit formulation, i.e.
\begin{equation}\label{eq:gf_emb_g_smallinit_app}
    \frac{d\vw^{{\rm emb},s}}{dt}=\frac{1}{n}\sum_{i=1}^{n_s}\left(\vy^{s,i}-\frac{1}{d_{\rm vob}}\bm{1}\right)\vW^{(2)T}\vW^{(1)T}.
\end{equation}
Consider $n\rightarrow\infty$ and denote the random variable $Y^s$ which follows the distribution of $\left\{y^{s,i}\right\}_{i=1}^{n_s}$,  then we obtain the asymptotic form by the law of large number
\begin{equation*}
    \frac{1}{n}\sum_{i=1}^{n_s}\vy^{s,i}=r_s\mathbb{E}_{Y^s}\left[\vY^s\right]=r_s\vP^s,
\end{equation*}
where $\vY^s$ is the one-hot representation of $Y^s$ and the $i$-th element of $\vP^s$ is $\vP^s_i=\mathbb{P}\left(Y^s=i\right)$. Then we obtain the Proposition~\ref{prop:E_emb_g}.

\subsection{Distribution of $Y^s$}\label{sec:dist_ys}


\paragraph{Memory anchor.}Since we select a label for any key-anchor pair randomly from $\mathcal{U}\left(\fZ\right)$, the label $s$ meets would follow the same distribution for any $s\in\fA_{\rm mem}$. specifically, we have
\begin{equation}
    \mathbb{P}\left(Y^{s}=i\right) = \frac{1}{N_{\fZ}}\delta_{i\in\fZ},
\end{equation}
where $\delta_{i\in\fZ}=1$ if $i\in\delta_{\fZ}$ otherwise 0.

\paragraph{Reasoning anchor.}For any reasoning anchor $s\in\fA_{\rm rsn}$, we assume that the other elements of a key-anchor pair containing $s$ is $z,a_1,a_2,\cdots,a_{q-1}$. Since the other elements are randomly chosen from the corresponding scope, the label could be represented as $y^s=s + z+\sum_{j=1}^{q-1}a_j$. Then $Y^s$ follows the distribution $s+Z+\sum_{j=1}^{q-1}A_j$, then we have
\begin{equation}
    \begin{aligned}    \mathbb{P}\left(Y^s=i\right)&=\mathbb{P}\left(Z+\sum_{j=1}^{q-1}A_j=i-s\right)\\&=\sum_{\zeta=\zeta_{\min}}^{\zeta_{\max}}{\mathbb{P}\left(Z=\zeta\right)\mathbb{P}\left(\sum_{j=1}^{q-1}A_j=i-s-\zeta\right)}\\
    &=\frac{1}{N_{\fZ}}\frac{1}{N_{\fA_{\rm rsn}}^{q-1}}\sum_{\zeta=\zeta_{\min}}^{\zeta_{\max}}\binom{q-1}{i-s-\zeta-(q-1)\alpha^{\rm rsn}_{\min}}_{N_{\fA_{\rm rsn}}},
\end{aligned}
\end{equation}

where the combination number $\binom{n}{j}_{k+1}$ can be defined by $\left(1+x+\cdots+x^k\right)^n=\sum_{j=0}^{kn}\binom{n}{j}_{k+1}x^j$~\cite{distributionofsumuniform}.
Specifically, when $q=2$, we have the following result:
\begin{align*}          \mathbb{P}\left(Y^s=i\right)=&\sum_{\zeta=\zeta_{\min}}^{\zeta=\zeta_{\max}}    \mathbb{P}\left(Z=\zeta\right)\mathbb{P}\left(A_1=i-s-\zeta\right)\\
    =&\left\{\begin{aligned}
        &\sum_{\zeta=\zeta_{\min}}^{i-s-\alpha^{\rm rsn}_{\min}}\frac{1}{N_{\fZ}N_{\fA_{\rm rsn}}},\quad i=\zeta_{\min}+\alpha^{\rm rsn}_{\min}+s,\cdots,\zeta_{\min}+\alpha^{\rm rsn}_{\max}+s,\\
        &\sum_{\zeta=i-s-\alpha^{\rm rsn}_{\max}}^{i-s-\alpha^{\rm rsn}_{\min}}\frac{1}{N_{\fZ}N_{\fA_{\rm rsn}}},\quad i=\zeta_{\min}+\alpha^{\rm rsn}_{\max}+1+s,\cdots,\zeta_{\max}+\alpha^{\rm rsn}_{\min}+s,\\
        &\sum_{\zeta=i-s-\alpha^{\rm rsn}_{\max}}^{\zeta_{\max}}\frac{1}{N_{\fZ}N_{\fA_{\rm rsn}}},\quad i=\zeta_{\max}+\alpha^{\rm rsn}_{\min}+1+s,\cdots,\zeta_{\max}+\alpha^{\rm rsn}_{\max}+s.
    \end{aligned}\right.\\
    =&\left\{\begin{aligned}
        &\frac{i-s-\alpha^{\rm rsn}_{\min}-\zeta_{\min}+1}{N_{\fZ}N_{\fA_{\rm rsn}}},\quad i=\zeta_{\min}+\alpha^{\rm rsn}_{\min}+s,\cdots,\zeta_{\min}+\alpha^{\rm rsn}_{\max}+s,\\
        &\frac{1}{N_{\fZ}},\quad i=\zeta_{\min}+\alpha^{\rm rsn}_{\max}+1+s,\cdots,\zeta_{\max}+\alpha^{\rm rsn}_{\min}+s,\\
        &\frac{\zeta_{\max}+\alpha^{\rm rsn}_{\max}-i+s+1}{N_{\fZ}N_{\fA_{\rm rsn}}},\quad i=\zeta_{\max}+\alpha^{\rm rsn}_{\min}+1+s,\cdots,\zeta_{\max}+\alpha^{\rm rsn}_{\max}+s.
    \end{aligned}\right.
\end{align*}

% When $T$ becomes enough large, by the central limit theorem the distribution of $Y^s$ can be interpreted with
% \begin{align*}
%     \mathbb{P}\left(Y^s=i\right)=C^{s}_ie^{-\frac{\left(i-\mu^{s}\right)^2}{2\sigma^{s}}} 
% \end{align*}
% where $\mu^{s}=(T-1)\frac{\alpha^{\rm rsn}_{\max}+\alpha^{\rm rsn}_{\min}}{2}+\frac{\zeta_{\max}+\zeta_{\min}}{2}+s$, $\sigma^{s}$ is independent with $s$ and $C^{s}_i$ is the normalization coefficient. If we consider the statistic characteristic and the relation between different $s$ we can do an axis shift and assume that
% \begin{equation}
%     \mathbb{P}\left(Y^s=i\right)=C_i e^{-\frac{\left(i-s\right)^2}{2\sigma}}
% \end{equation}
\paragraph{Key.} For any $s\in\fZ$, its labels come from two parts, memory mapping and reasoning mapping. In the memory mapping, $z$ meets each token in $\fZ$ with the same probability $\frac{1}{N_{\fZ}}$. In the reasoning mapping, the label $y^s=s+\sum_{i=1}^{q}a_i$. Assume that the ratio of memory mapping is identified with reasoning mapping, then we have
\begin{align}
    \mathbb{P}(Y^s=i)&=\frac{1}{2}\left(\mathbb{P}_{\rm mem}\left(Y^s=i\right)+\mathbb{P}_{\rm rsn}\left(Y^s=i\right)\right)\\
    &=\frac{1}{2}\left(\frac{1}{N_{\fZ}}+\mathbb{P}\left(s+\sum_{j=1}^qA_j=i\right)\right)\\
    &=\frac{1}{2}\left(\frac{1}{N_{\fZ}}+\frac{1}{N_{\fA_{\rm rsn}}^q}\binom{q}{i-s-q\alpha^{\rm rsn}_{\min}}_{N_{\fA_{\rm rsn}}}\right).
\end{align}
Specifically, when $q=2$
\begin{align*}
    \mathbb{P}\left(Y^s=i\right)=\frac{1}{2}\left(\frac{1}{N_{\fZ}}+\frac{N_{\fA_{\rm rsn}}-\left|\alpha^{\rm rsn}_{\max}-\alpha^{\rm rsn}_{\min}-i+s\right|}{N_{\fA_{\rm rsn}}^2}\right).
\end{align*}
Generally, consider the usual sequence containing some noise. Then the label consists of a third part when $z$ appears as a noise. With a similar method, we have
\begin{align*}
    \mathbb{P}_{noise}\left(Y^s=i\right)&=\frac{1}{2}\left(\mathbb{P}_{noise,mem}\left(Y^s=i\right)+ \mathbb{P}_{noise,rsn}\left(Y^s=i\right)\right)\\
    &=\frac{1}{2}\left(\frac{1}{N_{\fZ}}+\mathbb{P}\left(Z+\sum_{j=1}^qA_j=i\right)\right)\\
    &=\frac{1}{2}\left(\frac{1}{N_{\fZ}}+\frac{1}{N_{\fZ}}\frac{1}{N_{\fA_{\rm rsn}}^{q}}\sum_{\zeta=\zeta_{\min}}^{\zeta_{\max}}\binom{q}{i-\zeta-q\alpha^{\rm rsn}_{\min}}_{N_{\fA_{\rm rsn}}}\right).
\end{align*}
Combine them together, we have in the general setting $\fX^{\left(q,L\right)}$, we have that 
\begin{align*}
    \mathbb{P}(Y^s=i)=&\frac{1}{2\left(L-q\right)}\left(\frac{1}{N_{\fZ}}+\mathbb{P}\left(s+\sum_{j=1}^qA_j=i\right)\right)+\frac{L-q-1}{2\left(L-q\right)}\left(\frac{1}{N_{\fZ}}+\mathbb{P}\left(Z+\sum_{j=1}^qA_j=i\right)\right)\\
    =&\frac{1}{2N_{\fZ}}+\frac{1}{2\left(L-q\right)}\left(\mathbb{P}\left(s+\sum_{j=1}^qA_j=i\right)+\left(L-q-1\right)\mathbb{P}\left(Z+\sum_{j=1}^qA_j=i\right)\right)\\
    =&\frac{1}{2N_{\fZ}}+\frac{1}{2\left(L-q\right)N_{\fA_{\rm rsn}}^{q}}\left(\binom{q}{i-s-q\alpha^{\rm rsn}_{\min}}_{N_{\fA_{\rm rsn}}}+\left(L-q-1\right)\frac{1}{N_{\fZ}}\sum_{\zeta=\zeta_{\min}}^{\zeta_{\max}}\binom{q}{i-\zeta-q\alpha^{\rm rsn}_{\min}}_{N_{\fA_{\rm rsn}}}\right).
\end{align*}
\subsection{Gradient Flow of Embedding Space in Emb-MLP}
With the discussion in Section~\ref{sec:dist_ys}, we obtain the detailed formulation of \eqref{eq:gf_emb_g_limit} for different anchors of different tasks. Specifically, we have the following result: 
\begin{corollary}\label{cor:emb_mem}
    Given any $s \in \fA_{\rm mem}$, assume that $n\rightarrow\infty$ and assume the ratio of sequences containing $s$ in the training set $r_s$ keeps constant, then we have
    \begin{align}\label{eq:emb_mem}
    \frac{d\vw^{{\rm emb},s}}{dt} &= r_s\left(\frac{\vdelta^{\fZ}}{N_{\fZ}}-\frac{1}{d_{\rm vob}}\textbf{1}\right)\vW^{(2)T}\vW^{(1)T},
\end{align}
where $\vdelta^{\fZ}\in\sR^d$ is a vector with elements equal to 1 for indices belonging to \(\fZ\), and 0 otherwise.
\end{corollary}
\begin{corollary}\label{cor:emb_rsn}
 Given any $s \in \fA_{\rm rsn}$, assume that $n\rightarrow\infty$ and the ratio of sequences containing $s$ in the training set $r_s$ remains constant. Then, the gradient flow of the embedding vector corresponding to $s$ is given by:
    \begin{align}\label{eq:emb_rsn}
    \frac{d\vw^{{\rm emb},s}}{dt} &= r_s\left(\vP^{s}-\frac{1}{d_{\rm vob}}\textbf{1}\right)\vW^{(2)T}\vW^{(1)T},
\end{align}
 where $\vP^{s}\in\sR^d$ is a probability vector whose i-th element is $\vP^{s}_i=\frac{1}{N_{\fZ}}\frac{1}{N_{\fA_{\rm rsn}}^{q-1}}\sum_{\zeta=\zeta_{\min}}^{\zeta_{\max}}\binom{q-1}{i-s-\zeta-(q-1)\alpha^{\rm rsn}_{\min}}_{N_{\fA_{\rm rsn}}}$.
\end{corollary}

\subsection{Proof of Lemma~\ref{lem:attention}}
\begin{proof}
    We assume that $\vW^{{\rm emb},X}_{i,j}\sim\mathcal{N}\left(0,\left(d_m^{-\gamma}\right)^2\right),\vW^Q_{i,j}\sim\mathcal{N}\left(0,\left(d_k^{-\gamma}\right)^2\right),\vW^K_{i,j}\sim\mathcal{N}\left(0,\left(d_k^{-\gamma}\right)^2\right)$. 
We have that
\begin{align*}
    \left(\vW^{{\rm emb},X}\vW^Q\vW^{KT}\vW^{{\rm emb},X,T}\right)_{i,j} &= \sum_{k=1}^{d_m}\sum_{l=1}^{d_m}\vW^{{\rm emb},X}_{i,k}\left(\sum_{p=1}^{d_k}\vW^{Q}_{k,p}\vW^{K}_{l,p}\right)\vW^{{\rm emb},X}_{j,l}\\
    &=\sum_{k=1}^{d_m}\sum_{l=1}^{d_m}\sum_{p=1}^{d_k}\vW^{{\rm emb},X}_{i,k}\vW^{Q}_{k,p}\vW^{K}_{l,p}\vW^{{\rm emb},X}_{j,l}\\
    &\sim \mathcal{N}\left(0,\left(\frac{d_m^2d_k}{2\left(d_m^\gamma+d_k^\gamma\right)}\right)^2\right).
\end{align*}
So that the attention operator
\begin{align*}
    \frac{\left(\vW^{{\rm emb},X}\vW^Q\vW^{KT}\vW^{{\rm emb},X,T}\right)_{i,j}}{\sqrt{d_k}}\sim \mathcal{N}\left(0,\left(\frac{d_m^2\sqrt{d_k}}{2\left(d_m^\gamma+d_k^\gamma\right)}\right)^2\right).
\end{align*}
Utilizing the Chebyshev's Inequality, then we have 
\begin{align*}
    \mathbb{P}\left(\frac{\left|\left(\vW^{{\rm emb},X}\vW^Q\vW^{KT}\vW^{{\rm emb},X,T}\right)_{i,j}\right|}{\sqrt{d_k}}>\delta\right)&\leq\frac{d_m^4d_k}{4\delta^2\left(d_m^\gamma+d_k^{\gamma}\right)^2},
\end{align*}
  for any $\delta>0$. Given any $\varepsilon\in\left(0,1\right]$, let $C=\frac{1}{2}\log_{d_m+d_k}{\frac{d_m^4d_k}{4\delta^2\varepsilon}}$, then for any $\gamma>C$, we have 
\begin{align*}
    \mathbb{P}\left(\frac{\left|\left(\vW^{{\rm emb},X}\vW_Q\vW_K^T\vW^{{\rm emb},X,T}\right)_{i,j}\right|}{\sqrt{d_k}}>\delta\right)&\leq\frac{d_m^4d_k}{4\delta^2\left(d_m^\gamma+d_k^{\gamma}\right)^2}\leq\varepsilon,
\end{align*}
which implies that $\vA_{i,j}\xrightarrow{P}\frac{1}{i},$ for any $i\leq j$ as $\gamma\rightarrow \infty$.
\end{proof}
\subsection{Proof of Proposition~\ref{prop:emb_mem_tran},~\ref{prop:emb_rsn_tran}}
 For convenience in further analysis, we introduce the following notations $\vH^{s,i}:=(\overline{\vW}^{{\rm emb},X^{s,i}}\vW^V\vW^O+\vW^{{\rm emb},X^{s,i}}_L)\vW^{f1},\vW^{VO}=\vW^V\vW^O,\vW^f=\vW^{f1}\vW^{f2},\vp^{s,i}=\text{softmax}(\vf_{\vtheta}(X^{s,i}))$. Firstly, we have the following result:
\begin{lemma}\label{lem:gf_emb_f}
    Given any token $s$, the gradient flow of $\vw^{{\rm emb},s}$ can be expressed as
\begin{align*}
    \frac{d\vw^{{\rm emb},s}}{dt}= &-\frac{1}{n}\left(\sum_{i=1}^{n_{s}}\frac{1}{L}\left(\left(\vp^{s,i}-\vy^{s,i}\right)\vW^{f,T}\vW^{VO,T}\odot\sigma^{\prime}\left(\vH^{s,i}\right)^T+\left(\vp^{s,i}-\vy^{s,i}\right)\vW^{VO,T}\right)\right.\\
    &\left.\qquad+\sum_{i=1}^{\tilde{n}_{s}}\left(\vp^{s,i}-\vy^{s,i}\right)\vW^{f,T}\odot\sigma^{\prime}\left(\vH^{s,i}\right)^T+\left(\vp^{s,i}-\vy^{s,i}\right)\right),
\end{align*}
where $\tilde{n}_{s}$ denotes the time $s$ appears in the final position of a sequence.
\end{lemma}
 \begin{proof}
%     \begin{align*}
%     \frac{\partial L}{\partial x}=(\frac{\partial z}{\partial x})^T(\frac{\partial L}{\partial z})
% \end{align*}
For any data pair $\left(X^{s,i},y^{s,i}\right)$, we have
\begin{align*}
    d\vf_{\vtheta}(X^{s,i})_j &= d\left(\sigma\left(\vH^{s,i}\right)\vW^{f2}_{:,j} + \overline{\vW}^{{\rm emb},X^{s,i}}\vW^{VO}_{:,j}+\vW^{{\rm emb},X^{s,i}}_{L,j}\right)\\
    &= \sigma^{\prime}\left(\vH^{s,i}\right)\odot \left(d \overline{\vW}^{{\rm emb},X^{s,i}}\vW^{VO}\vW^{f1}+d\vW^{{\rm emb},X^{s,i}}_L\vW^{f1}\right)\vW^{f2}_{:,j}+d \overline{\vW}^{{\rm emb},X^{s,i}}\vW^{VO}_{:,j}+d\vW^{{\rm emb},X^{s,i}}_{L,j}.
\end{align*}
By the trace theorem, we have
\begin{align*}
    d\vf_{\vtheta}\left(X^{s,i}\right)_j &= \text{tr}\left(d\vf_{\theta}\left(X^{s,i}\right)_j\right)\\
    &= \text{tr}\left(\vW^{VO}\vW^{f1}\left(\vW^{f2}_{:,j}\odot\sigma^{\prime}\left(\vH^{s,i}\right)^T\right) d\overline{\vW}^{{\rm emb},X^{s,i}}\right)+\text{tr}\left(\vW^{f1}\left(\vW^{f2}_{:,j}\odot\sigma^{\prime}\left(\vH^{s,i}\right)^T\right) d\vW^{{\rm emb},X^{s,i}}_{L,:}\right)\\
    &\quad+\text{tr}\left(\vW^{VO}_{:,j}d\overline{\vW}^{{\rm emb},X^{s,i}}\right)+\text{tr}\left(d\vW^{{\rm emb},X^{s,i}}_{L,j}\right)\\
    & = \text{tr}\left(\left(\vW^{VO}\vW^{f1}\left(\vW^{f2}_{:,j}\odot\sigma^{\prime}\left(\vH^{s,i}\right)^T\right)+\vW^{VO}_{:,j}\right)d\overline{\vW}^{{\rm emb},X^{s,i}}\right) + \text{tr}\left(\left(\vW^{f2}_{:,j}\odot\sigma^{\prime}\left(\vH^{s,i}\right)^T+\bm{1}\right) d\vW^{{\rm emb},X^{s,i}}_{L,:}\right).
    \end{align*}
Utilizing the chain rule, we have
\begin{align*}
    &\frac{\partial R\left(X^{s,i},y^{s,i}\right)}{\partial \vW^{{\rm emb},s}} = \sum_{j=1}^{d_m} \frac{\partial R\left(X^{s,i},y^{s,i}\right)}{\partial \vf_{\vtheta}\left(X^{s,i}\right)_j}\frac{\partial \vf_{\vtheta}\left(X^{s,i}\right)_j}{\partial \vW^{{\rm emb},s}}=\sum_{j=1}^{d_m} \left(\vp^{s,i}_j-\vy^{s,i}_j\right)\frac{\partial \vf_{\vtheta}\left(X^{s,i}\right)_j}{\partial \vW^{{\rm emb},s}}\\
   &= \left\{
    \begin{aligned}
        &\frac{1}{L}\left(\left(\left(\vp^{s,i}-\vy^{s,i}\right)\vW^{f2,T}\odot\sigma^{\prime}\left(\vH^{s,i}\right)\right)\vW^{f1,T}\vW^{VO,T}+\left(\vp^{s,i}-\vy^{s,i}\right)\vW^{VO,T}\right)\\&+\left(\left(\vp^{s,i}-\vy^{s,i}\right)\vW^{f2,T}\odot\sigma^{\prime}\left(\vH^{s,i}\right)\right)\vW^{f1,T}+\left(\vp^{s,i}-\vy^{s,i}\right),\qquad\qquad s \text{   occurs on last position,}\\
        &\frac{1}{L}\left(\left(\left(\vp^{s,i}-\vy^{s,i}\right)\vW^{f2,T}\odot\sigma^{\prime}\left(\vH^{s,i}\right)\right)\vW^{f1,T}\vW^{VO,T}+\left(\vp^{s,i}-\vy^{s,i}\right)\vW^{VO,T}\right),\quad\text{otherwise.}
    \end{aligned}
   \right.
\end{align*}
Then we obtain the gradient flow as
\begin{align*}
    \frac{d\vw^{{\rm emb},s}}{dt}= &-\frac{1}{n}\left(\sum_{i=1}^{n_{s}}\frac{1}{L}\left(\left(\vp^{s,i}-\vy^{s,i}\right)\vW^{f2,T}\odot\sigma^{\prime}\left(\vH^{s,i}\right)\vW^{f1,T}\vW^{VO,T}+\left(\vp^{s,i}-\vy^{s,i}\right)\vW^{VO,T}\right)\right.\\
    &\left.\qquad+\sum_{i=1}^{\tilde{n}_{s}}\left(\vp^{s,i}-\vy^{s,i}\right)\vW^{f2,T}\odot\sigma^{\prime}\left(\vH^{s,i}\right)^T\vW^{f1,T}+\left(\vp^{s,i}-\vy^{s,i}\right)\right).
\end{align*}
\end{proof}

As the initialization scales decrease to zero, we derive the gradient flow under a small initialization scale as follows via Assumption~\ref{assump:activation}.
\begin{equation}\label{eq:gf_emb_f}
\begin{aligned}
    &\frac{d\vw^{{\rm emb},s}}{dt}= \frac{1}{n}\left(\sum_{i=1}^{n_{s}}\frac{1}{L}\left(\vy^{s,i}-\frac{1}{d_m}\bm{1}\right)\left(\vW^{VO}\vW^f+\vW^{VO}\right)^T\right.\\
    &\left.\qquad+\sum_{i=1}^{\tilde{n}_{s}}\left(\vy^{s,i}-\frac{1}{d_m}\bm{1}\right)\vW^{f,T}+\left(\vy^{s,i}-\frac{1}{d_m}\bm{1}\right)\right).
\end{aligned}
\end{equation}
We consider the ideal condition $n\rightarrow\infty$, with the law of large number, ~\eqref{eq:gf_emb_f} can be approximated as follow: 
    \begin{equation}
        \frac{d\vw^{{\rm emb},s}}{dt}=\frac{r_s}{L}\mathbb{E}_{Y^s}\left[\vY^s-\frac{1}{d_m}\bm{1}\right]\tilde{\vW}.
    \end{equation}
With the distribution we discussed in Section~\ref{sec:dist_ys}, we complete the proof of Proposition~\ref{prop:emb_mem_tran},~\ref{prop:emb_rsn_tran}.

\subsection{Proof of Theorem~\ref{thm:emb_reconstruct}}
\begin{proof}
     Consider the linear expansion of $\vw^{{\rm emb},s}=\vw^{{\rm emb},s}_{t_0}+\frac{d \vw^{{\rm emb},s}}{dt}\eta$ where $\vw^{{\rm emb},s}_{t_0}$ is the initialization of $\vw^{{\rm emb},s}$, then we have
     \begin{align}
         \vw^{{\rm emb},s}=&\vw^{{\rm emb},s}_{t_0}+\frac{d\vw^{{\rm emb},s}}{dt}\eta\\
         =& \vw^{{\rm emb},s}_{t_0}+\frac{r_s\eta}{L}\mathbb{E}_{Y^s}\left[\vy^s-\frac{1}{d_m}\bm{1}\right]\left(\vW^{f,T}+\vI\right)\left(\vW^{VO,T}+\vI\right)\\
         =&\frac{r_s\eta}{L}\mathbb{E}_{Y^s}\left[\vy^s-\frac{1}{d_m}\bm{1}\right]+\vw^{{\rm emb},s}_{t_0}+O\left(d_m^{-2\gamma}\bm{1}\right).
     \end{align}
     For any $s\in\fA_{\rm rsn}$, the formulation can be rewritten as 
     \begin{equation}
         \vw^{{\rm emb},s}_i=\frac{r_s\eta}{L}\left(\mathbb{P}\left(s+Z+\sum_{j=1}^{q-1}A_j=i\right)-\frac{1}{d_m}\right)+\varepsilon,
     \end{equation}
     where $\varepsilon\sim\fN\left(0,\left(d_m^{\gamma}\right)^2\right)$. Let $q$ enlarge enough, then $\mathbb{P}\left(s+Z+\sum_{j=1}^{T-1}A_j=i\right)$ can be approximated by the following formulation using the Berry-Esseen central limit theorem 
     \begin{equation}
         % \mathbb{P}\left(s+Z+\sum_{j=1}^{q-1}=i\right)=\frac{1}{\sqrt{2\pi}\sigma_P}e^{-\frac{(i-s-\mu)^2}{2\sigma_P}}+O\left(T^{-\frac{1}{2}}\right)
         \sup_i\left|\mathbb{P}\left(s+Z+\sum_{j=1}^{q-1}=i\right)-\frac{1}{\sqrt{2\pi}\sigma_P}e^{-\frac{(i-s-\mu)^2}{2\sigma_P}}\right|\leq O\left(q^{-\frac{1}{2}}\right),
     \end{equation}
    where $\mu$ and $\sigma_P$ is the expectation and standard deviation of $Z+\sum_{j=1}^{q-1}A_j$. Denote that $\tilde{\vw}^{{\rm emb},s}_i=\frac{r_s\eta}{L}\left(\frac{1}{\sqrt{2\pi}\sigma_P}e^{-\frac{(i-s-\mu)^2}{2\sigma_P}}-\frac{1}{d_m}\right)+\varepsilon$, then we have:
    \begin{equation*}
        \sup_i \left|\tilde{\vw}^{{\rm emb},s}_i-\vw^{{\rm emb},s}_i\right|\leq O\left(q^{-\frac{1}{2}}+d_m^{-\gamma}\right).
    \end{equation*}
    Then the difference in inner production can be derived as follows:
    \begin{align*}
        \sup_{i,j}\left|\left(\tilde{\vw}^{{\rm emb},s_j},\tilde{\vw}^{{\rm emb},s_i}\right)-\left({\vw}^{{\rm emb},s_j},{\vw}^{{\rm emb},s_i}\right)\right|&=\sup_{i,j}\left|\sum_k\tilde{\vw}^{{\rm emb},s_j}_k\tilde{\vw}^{{\rm emb},s_i}_k-\vw^{{\rm emb},s_j}_k\vw^{{\rm emb},s_i}_k\right|\\
        &\leq \sum_{k}\sup_{i,j}\left|\tilde{\vw}^{{\rm emb},s_j}_k\tilde{\vw}^{{\rm emb},s_i}_k-\vw^{{\rm emb},s_j}_k\vw^{{\rm emb},s_i}_k\right|\\
        &\leq O\left(d_m^{1-\gamma}\left(q^{-\frac{1}{2}}+d_m^{-\gamma}\right)\right).
    \end{align*}
     Since an axis transformation does not affect the inner product, we set $\tilde{i}=i-\mu$, then we complete the proof of Theorem~\ref{thm:emb_reconstruct}.
 \end{proof}

\subsection{Validation of Theorem~\ref{thm:emb_reconstruct}}\label{app:validation_of_theory}
To verify the validity and generality of our theoretical analysis, we compare the cosine similarity of the embedding vectors within the reasoning anchors between experimental results and theoretical approximations. The results in Figure~\ref{fig:theory_app} demonstrate that our theoretical estimates align well with the experimental results in most cases, with discrepancies observed only when $|s_i-s_j|$ becomes large, likely due to the omission of higher-order terms.
\begin{figure}[htbp]
    \centering
    \includegraphics[width=1\linewidth]{figure/appendix/theory_app.png}
    \caption{Cosine similarity comparison between experimental results $\cos\left(\vw^{{\rm emb},s_i},\vw^{{\rm emb},s_j}\right)$ with theoretical approximations $\cos\left(\tilde{\vw}^{{\rm emb},s_i},\tilde{\vw}^{{\rm emb},s_j}\right)$ (see ~\eqref{eq:estimation_wemb}), for any $s_i,s_j\in\fA_{\rm rsn}$. }
    \label{fig:theory_app}
\end{figure}


\subsection{Discussion about $W^V$}\label{app:W_V}
For the phenomenon where the $\vW^V$ of the first attention module exhibits a preference for capturing the reasoning anchors, a general theoretical explanation would require a more comprehensive and sophisticated analysis. However, a similar result can be derived under a special and constrained condition.
\begin{theorem}\label{thm:WV}
    Let $n,\gamma\rightarrow\infty$, $N_{\fZ}=d_m$, Define that $A\sim  \mathcal{U}\left(\fA_{\rm rsn}\right)$ and $Y$ as a random variable which randomly takes value from the whole dataset's labels, then we have the following result:
    \begin{equation*}
            \frac{d\vW^V}{dt}=\frac{1}{2}\mathbb{E}_{A}\vw^{{\rm emb},A}\mathbb{E}_{Y}\left[\vY-\frac{1}{d_m}\bm{1}\right]^T\vW^O\left(\vW^f+I\right).
    \end{equation*}
\end{theorem}
Theorem~\ref{thm:WV} highlights that $\vW^V$ inherently demonstrates a preference for reasoning tasks, thereby enhancing its ability to capture information associated with reasoning anchors.
\begin{proof}
 Firstly we have the following formulation:
\begin{lemma}\label{lem:gf_wv_f}
    Given the dataset $\{(X^i,y^i)\}_{i=1}^n$, the gradient flow of $\vW^V$ can be expressed as follow:
    \begin{equation}
    \begin{aligned}
        \frac{d\vW^V}{dt} =-\frac{1}{n}\sum_{i=1}^n\left(\sigma^{\prime}\left(\vH^{i}\right)\odot\overline{\vW}^{{\rm emb},X^i}\right)^T\left(\vp^i-\vy^i\right)\left(\vW^O\vW^{f}\right)^T+\overline{\vW}^{{\rm emb},X^i,T}\left(\vp^i-\vy^i\right)\vW^{O,T}.
    \end{aligned}
    \end{equation}
\end{lemma}
\begin{proof}
    For each  data pair $\left(X^i,y^i\right)$, we have
    \begin{align*}
    \vf_{\vtheta}\left(X^i\right)_j &= \sigma\left(\left(\vA_{L,:}\vW^{{\rm emb},X^i}\vW^V\vW^O+\vW^{{\rm emb},X^i}_{L,:}\right)\vW^{f1}\right)\vW^{f2}_{:,j} + \vA_{L,:}\vW^{{\rm emb},X^i}\vW^V\vW^O_{:,j}+\vW^{{\rm emb},X^i}_{L,j}.
    \end{align*}
Compute its differential, we have
    \begin{align*}
    d \vf_{\vtheta}\left(\vW^{{\rm emb},X^i}\right)_j &= d\left(\sigma\left(\left(\overline{\vW}^{{\rm emb},X^i}\vW^V\vW^O+\vW^{{\rm emb},X^i}_{L,:}\right)\vW^{f1}\right)\vW^{f2}_{:,j} + d\overline{\vW}^{{\rm emb},X^i}\vW^V\vW^O_{:,j}\right)\\
    &=\sigma^{\prime}\left(\vH^i\right)\odot d\left(\left(\overline{\vW}^{{\rm emb},X^i}\vW^V\vW^O+\vW^{{\rm emb},X^i}_{L,:}\right)\vW^{f1}\right)\vW^{f2}_{:,j}+\overline{\vW}^{{\rm emb},X^i}d\vW^V\vW^O_{:,j}\\
    &=\sigma^{\prime}\left(\vH^i\right)\odot \overline{\vW}^{{\rm emb},X^i}d\vW^V\vW^O\vW^{f1}\vW^{f2}_{:,j}+\overline{\vW}^{{\rm emb},X^i}d\vW^V\vW^O_{:,j}.
    \end{align*}
    Using the trace theorem 
    \begin{align*}
    d\vf_{\vtheta}\left(X^i\right)_j = \text{tr}\left(d\vf_{\vtheta}\left(X^i\right)_j\right)& = \text{tr}\left(\sigma^{\prime}\left(\vH^i\right)\odot \overline{\vW}^{{\rm emb},X^i}d\vW^V\vW^O\vW^{f}_{:,j}\right) + \text{tr}\left(\overline{\vW}^{{\rm emb},X^i}d\vW^V\vW^O_{:,j}\right)\\
    & = \text{tr}\left(\vW^O\vW^{f}_{:,j}\sigma^{\prime}\left(\vH^i\right)\odot \overline{\vW}^{{\rm emb},X^i}d\vW^V\right) + \text{tr}\left(\vW^O_{:,j}\overline{\vW}^{{\rm emb},X^i}d\vW^V\right)\\
    & = \text{tr}\left(\left(\vW^O\vW^{f}_{:,j}\sigma^{\prime}\left(\vH^i\right)\odot\overline{\vW}^{{\rm emb},X^i}+\vW^O_{:,j}\overline{\vW}^{{\rm emb},X^i}\right)d\vW^V\right),
    \end{align*}
    which suggests that
    \begin{align*}
    \frac{\partial \vf_{\vtheta}\left(X^i\right)_j}{\partial \vW^V} = \left(\vW^O\vW^{f}_{:,j}\sigma^{\prime}\left(\vH^i\right)\odot\overline{\vW}^{{\rm emb},X^i}+\vW^O_{:,j}\overline{\vW}^{{\rm emb},X^i}\right)^T.
    \end{align*}
    Utilizing the chain rule, we have
    \begin{align*}
    \frac{\partial R\left(X^i\right)}{\partial \vW^V} &= \sum_{j=1}^{d_m} \frac{\partial R\left(X^i\right)}{\partial  \vf_{\vtheta}\left(X^i\right)_j}\frac{\partial  \vf_{\vtheta}\left(X^i\right)_j}{\partial \vW^V}\\
    & = \sum_{j=1}^{d_m}\left(\vp^i_j-\vy^i_j\right)\left(\vW^O\vW^{f}_{:,j}\sigma^{\prime}\left(\vH^i\right)\odot\overline{\vW}^{{\rm emb},X^i}+\vW^O_{:,j}\overline{\vW}^{{\rm emb},X^i}\right)^T\\
    & = \left(\sigma^{\prime}\left(\vH^i\right)\odot\overline{\vW}^{{\rm emb},X^i}\right)^T\left(\vp^i-\vy^i\right)\left(\vW^O\vW^{f}\right)^T+\overline{\vW}^{{\rm emb},X^i,T}\left(\vp^i-\vy^i\right)\vW^{O,T},
\end{align*}
where $\vp^i=\text{softmax}(\vf_{\vtheta}(X^i))$. Then gradient flow of $\vW^V$ can be expressed as 
\begin{align*}
    \frac{d\vW^V}{dt} &= -\frac{1}{n}\sum_{i=1}^n\frac{\partial R\left(X^i\right)}{\partial \vW^V}\\
    &=-\frac{1}{n}\sum_{i=1}^n\left(\sigma^{\prime}\left(\vH^{i}\right)\odot\overline{\vW}^{{\rm emb},X^i}\right)^T\left(\vp^i-\vy^i\right)\left(\vW^O\vW^{f}\right)^T+\overline{\vW}^{{\rm emb},X^i,T}\left(\vp^i-\vy^i\right)\vW^{O,T}.
\end{align*}
\end{proof}
 When initialized with a small scale, the gradient flow for $\vW^V$ could be interpreted by:
\begin{align}
    \frac{d\vW^V}{dt} &= \frac{1}{n}\sum_{i=1}^n\overline{\vW}^{{\rm emb},X^i}\left(\vy^i-\frac{1}{d_m}\bm{1}\right)^T\left(\vW^O\vW^{f}+\vW^O\right)^T\\
    &=\frac{1}{nL}\sum_{i=1}^n\sum_{j=1}^L\vw^{\rm emb}_{(i,j)}\left(\vy^i-\frac{1}{d_m}\bm{1}\right)^T\left(\vW^O\vW^{f}+\vW^O\right)^T,
\end{align}
where $\vw^{\rm emb}_{(i,j)}$ denotes the $j$-th element of $\vW^{{\rm emb},X^i}$. In this formulation, there are $nL$ tokens, and we reorder all tokens along with their corresponding labels. Let $\vw^{{\rm emb},s_i}$ denote the embedding vector of the $i$-th token, and let $y^{s_i}$ be its corresponding label. Consequently, the gradient flow can be expressed as:
\begin{equation}\label{eq:gf_WV_f}
    \frac{d\vW^V}{dt} =\frac{1}{N}\sum_{i=1}^N\vw^{{\rm emb},s_i}\left(\vy^{s_i}-\frac{1}{d_m}\bm{1}\right)^T\left(\vW^O\vW^{f}+\vW^O\right)^T.
\end{equation}
If we interpret $\vw^{{\rm emb},s_i}$ by its linear expansion $\vw^{{\rm emb},s_i}_{t_0}+\frac{d\vw^{{\rm emb},s_i}}{dt}\eta$, we obtain that
\begin{align*}
    \frac{d\vW^V}{dt}=&\frac{1}{N}\sum_{i=1}^N\left(\vw^{\text{emb},s_i}_{t_0}+\frac{d\vw^{{\rm emb},s_i}}{dt}\eta\right)\left(\vy^{s_i}-\frac{1}{d_m}\bm{1}\right)^T\left(\vW^O\vW^{f}+\vW^O\right)^T\\
    =&\frac{1}{N}\sum_{i=1}^N\left(\vw^{\text{emb},s_i}_{t_0}+\eta\frac{r_{s_i}}{L}\mathbb{E}_{Y^{s_i}}\left[\vY^{s_i}-\frac{1}{d_m}\bm{1}\right]\left(\vW^{f,T}+\vI\right)\left(\vW^{VO,T}+\vI\right)\right)\left(\vy^{s_i}-\frac{1}{d_m}\bm{1}\right)^T\left(\vW^O\vW^{f}+\vW^O\right)^T.
    % =&\frac{1}{N}\sum_{i=1}^N\frac{r_s\eta}{L}\mathbb{E}_{Y^s}\left[\vy^s-\frac{1}{d_m}\bm{1}\right]
    % =&\mathbb{E}_{s_i,Y^s}\left[\vw^{{\rm emb},s}_{t_0}+\frac{r_s}{L}\mathbb{E}_{\vy^s}\left[\vy^s-\frac{1}{d_m}\bm{1}\right]\left(\left(\vW^f\right)^T+\vI\right)\left(\left(\vW^{VO}\right)^T+\vI\right)\right]\left(\vy^s-\frac{1}{d_m}\bm{1}\right)^T\left(\vW^O\vW^f+\vW^O\right)
\end{align*}
Let $\vW^1=\left(\left(\vW^f\right)^T+\vI\right)\left(\left(\vW^{VO}\right)^T+\vI\right),\vW^2=\left(\vW^O\vW^f+\vW^O\right)^T$, then the formulation could be rewritten as
\begin{align*}
    \frac{d\vW^V}{dt}=&\frac{1}{N}\sum_{i=1}^N\left(\vw^{\text{emb},s_i}_{t_0}+\eta\frac{r_{s_i}}{L}\mathbb{E}_{Y^{s_i}}\left[\vY^{s_i}-\frac{1}{d_m}\bm{1}\right]\vW^1\right)\left(\vy^{s_i}-\frac{1}{d_m}\bm{1}\right)^T\vW^{2}\\
    =&\mathbb{E}_{s_i,Y}\left[\left(\vw^{\text{emb},s_i}_{t_0}+\eta\frac{r_{s_i}}{L}\mathbb{E}_{Y^{s_i}}\left[\vY^{s_i}-\frac{1}{d_m}\bm{1}\right]\vW^1\right)\left(\vy^{s_i}-\frac{1}{d_m}\bm{1}\right)^T\vW^{2}\right]\\
    =&\mathbb{E}_{s_i,Y}\left[\eta\frac{r_{s_i}}{L}\mathbb{E}_{Y^{s_i}}\left[\vY^{s_i}-\frac{1}{d_m}\bm{1}\right]\vW^1\left(\vY^{s_i}-\frac{1}{d_m}\bm{1}\right)^T\vW^{2}\right]+\mathbb{E}_{s_i,Y^{s_i}}\left[\vw^{\text{emb},s_i}_{t_0}\left(\vY^{s_i}-\frac{1}{d_m}\bm{1}\right)^T\vW^{2}\right]\\
    =&\frac{r_s\eta}{L}\mathbb{E}_{s_i}\left[\mathbb{E}_{Y^{s_i}}\left[\vY^{s_i}-\frac{1}{d_m}\bm{1}\right]\vW^1\mathbb{E}_{Y^{s_i}}\left[\vY^{s_i}-\frac{1}{d_m}\bm{1}\right]^T\vW^{2}\right]+\mathbb{E}_{s_i,Y^{s_i}}\left[\vw^{\text{emb},s_i}_{t_0}\left(\vY^{s_i}-\frac{1}{d_m}\bm{1}\right)^T\vW^{2}\right]\\
    =&\frac{r_s\eta}{L}\mathbb{E}_{Y}\left[\vY-\frac{1}{d_m}\bm{1}\right]\vW^1\mathbb{E}_{Y}\left[\vY-\frac{1}{d_m}\bm{1}\right]^T\vW^{2}+\mathbb{E}_{s_i,Y^{s_i}}\left[\vw^{\text{emb},s_i}_{t_0}\left(\vY^{s_i}-\frac{1}{d_m}\bm{1}\right)^T\vW^{2}\right].
\end{align*}
% Since all parameters are initialized with sufficiently small scale, we could ignore all higher order and simplify this formulation as
% \begin{align*}
%     \frac{d\vW^V}{dt}&=\mathbb{E}_{s_i,Y^s}\left[\frac{r_st}{L}\mathbb{E}_{\vy^s}\left[\vy^s-\frac{1}{d_m}\bm{1}\right]\vW^1\left(\vy^s-\frac{1}{d_m}\bm{1}\right)^T\right]\vW^2+o(\vepsilon)\\
%     &=\frac{r_st}{L}\mathbb{E}_{\vy}\left[\vy-\frac{1}{d_m}\bm{1}\right]\vW^1\mathbb{E}_{\vy}\left[\vy-\frac{1}{d_m}\bm{1}\right]^T\vW^2+o\left(\vepsilon\right)
% \end{align*}
While $\mathbb{E}_{Y}\left[\vY-\frac{1}{d_m}\bm{1}\right]_i=\mathbb{P}\left(Y=i\right)-\frac{1}{d_m}$, using the discussion in section \ref{sec:dist_ys}, Let $Z\sim \fU\left(\fZ\right),A_1,\cdots A_q\sim \fU\left(\fA_{\rm rsn}\right)$ we have
\begin{align*}
    \mathbb{P}\left(Y=i\right)-\frac{1}{d_m}&=\frac{1}{2}\left(\mathbb{P}_{\rm mem}\left(Y=i\right)+\mathbb{P}_{\rm rsn}\left(Y=i\right)\right)-\frac{1}{d_m}\\
    &=\frac{1}{2}\left(\frac{\delta_{i\in\fZ}}{N_{\fZ}}+\mathbb{P}\left(Z+\sum_{j=1}^qA_j=i\right)\right)-\frac{1}{d_m}\\
    &=\frac{1}{2}\left(\mathbb{P}\left(Z+\sum_{j=1}^qA_j=i\right)-\frac{1}{d_m}\right) + \frac{1}{2}\left(\frac{\delta_{i\in\fZ}}{N_{\fZ}}-\frac{1}{d_m}\right)\\
    &=\frac{1}{2}\left(\mathbb{E}_{A_1}\left[\mathbb{P}\left(Z+\sum_{j=1}^qA_j=i\mid A_1=a\right)\right]-\frac{1}{d_m}\right) + \frac{1}{2}\left(\frac{\delta_{i\in\fZ}}{N_{\fZ}}-\frac{1}{d_m}\right)\\
    &=\frac{1}{2}\mathbb{E}_{A_1}\left[\mathbb{P}\left(Z+\sum_{j=1}^qA_j=i|A_1=a\right)-\frac{1}{d_m}\right]+ \frac{1}{2}\left(\frac{\delta_{i\in\fZ}}{N_{\fZ}}-\frac{1}{d_m}\right)\\
    &=\frac{1}{2}\mathbb{E}_{A_1}\left[\mathbb{E}_{Y^s}\left[\vy^s-\frac{1}{d_m}\right]_i\right].
\end{align*}
Then we have that
\begin{align*}
    &\frac{d\vW^V}{dt}=\frac{r\eta}{2L}\mathbb{E}_{A_1}\left[\mathbb{E}_{Y^s}\left[\vY^s-\frac{1}{d_m}\bm{1}\right]\right]\vW^1\mathbb{E}_{Y}\left[\vY-\frac{1}{d_m}\bm{1}\right]^T\vW^{2}+\mathbb{E}_{s_i,Y^{s_i}}\left[\vw^{\text{emb},s_i}_{t_0}\left(\vY^{s_i}-\frac{1}{d_m}\bm{1}\right)^T\vW^{2}\right]\\
    &=\frac{1}{2}\mathbb{E}_{A_1}\left[\vw^{\text{emb},A}\right]\mathbb{E}_{Y}\left[\vY-\frac{1}{d_m}\bm{1}\right]^T\vW^{2}+O\left(d_m^{-4\gamma}\bm{1}\right).
\end{align*}
% Since $\frac{d\vw^{{\rm emb},s}}{dt}=\frac{r_s}{L}\mathbb{E}_{y^s}\left[\vy^s-\frac{1}{d_m}\right]\vW^1$, we obtain that
% \begin{align*}
%     \frac{d\vW^V}{dt}=\frac{1}{2}\left(\mathbb{E}_{A_1}\left[\vw^{{\rm emb},s}\right]+\frac{d_m-N_{\fZ}}{N_{\fZ}d_m}\bm{1}\vW^1\right)\mathbb{E}_{\vy}\left[\vy-\frac{1}{d_m}\bm{1}\right]^T\vW^2 + o(\vepsilon)
% \end{align*}
% assume that $N_{\fZ}\rightarrow d_m$, then
% \begin{align*}
%     \frac{d\vW^V}{dt}=\frac{1}{2}\mathbb{E}_{s}\left[\vw^{{\rm emb},s}\right]\mathbb{E}_{\vy}\left[\vy-\frac{1}{d_m}\bm{1}\right]^T\vW^2 + o(\vepsilon)
% \end{align*}
\end{proof}

\newpage
\section{Mechanisms under Varying Initialization Scales}\label{app:model_chara}
\subsection{Embedding Space of Emb-MLP}
Figure~\ref{fig:emb_g_app} exhibits the cosine similarity within the embedding space of Emb-MLP models with initialization rates $\gamma=0.3$ and $\gamma=0.5$. The results indicate that under a large initialization scale, the embedding space of the model becomes less influenced by the label distributions and instead relies predominantly on orthogonality to differentiate all tokens. This mechanism neglects the intrinsic relationships among tokens, leading to a loss of generalization.
\begin{figure}[htbp]
    \centering
    \includegraphics[width=1\linewidth]{figure/appendix/embedding-appendix.png}
    \caption{Cosine similarity among different anchors' embedding vectors of Emb-MLP under initialization rates $\gamma=0.3,0.5$ for memory anchors (top row) and reasoning anchors (bottom row).}
    \label{fig:emb_g_app}
\end{figure}
% \begin{figure}[htbp]
%     \centering
%     \includegraphics[width=1\linewidth,height=0.4\linewidth]{figure/appendix/embedding-init0.5.png}
%     \caption{Embedding space of $G$ with initialization rate $\gamma=0.5$.}
%     \label{fig:emb_g_0.5}
% \end{figure}
\subsection{Embedding Space of Transformer}
Figure~\ref{fig:emb_f_app} exhibits the structure of the Transformer's embedding space with $\gamma=0.3$ and $\gamma=0.5$. The left and middle panels exhibit the cosine similarity within the embedding space of memory anchors an reasoning anchors, demonstrating that a larger initialization scale promotes orthogonality among embedding vectors. The right panel presents the PCA projection of the embedding space, suggesting that under a large initialization scale, the embedding space lacks a meaningful structure conducive to learning the reasoning mapping. These findings suggest that a large initialization scale encourages differentiation of tokens primarily through orthogonality, taking tokens as independent from the others and neglecting intrinsic token relationships, and ultimately impairing the generalization capability.
\begin{figure}[htbp]
    \centering
    \includegraphics[width=1\linewidth]{figure/appendix/embedding_anchor_appendix.png}
    \caption{Characteristic of embedding space of Transformer with initialization rates $\gamma=0.3,0.5$. The left and middle panels depict the cosine similarity among embedding vectors of memory anchors and reasoning anchors at epochs 200 and 900. The right panel shows a PCA projection of the embedding space with the key and reasoning anchors.}
    \label{fig:emb_f_app}
\end{figure}
% \begin{figure}[htbp]
%     \centering
%     \includegraphics[width=0.9\linewidth]{figure/appendix/embedding_anchor_50.png}
%     \caption{Embedding space of Transformer with initialization rate $\gamma=0.5$.}
%     \label{fig:emb_f_0.5}
% \end{figure}

\subsection{The First Attention Module of Transformer}
Figure~\ref{fig:attn_f_app} exhibits the structure of the first attention module with $\gamma=0.3$ and $\gamma=0.5$.  The comparison reveals that a larger initialization scale results in a more complex attention mechanism, which exhibits no specific preference for any particular task.
\begin{figure}[htbp]
    \centering
    \includegraphics[width=1\linewidth]{figure/appendix/first_attention_appendix.png}
    \caption{Characteristics of the first attention module of Transformers with initialization rates $\gamma=0.3$ (top row) and $\gamma=0.5$ (bottom row). A: Heatmap of the attention matrix for a random sample. B: Distribution of the relative error between attention $A_{jk}$ and $\frac{1}{j}$ across all training sequences. C: Distribution of singular values of $\vW^{V}$. D: Cosine similarity between the left singular vectors and average embedding vectors of the anchors. }
    \label{fig:attn_f_app}
\end{figure}
% \begin{figure}[htbp]
%     \centering
%     \includegraphics[width=1\linewidth]{figure/appendix/first_attention_50.png}
%     \caption{First attention module of $f_{\theta}$ with initialization rate $\gamma=0.5$.}
%     \label{fig:attn_f_0.5}
% \end{figure}
\subsection{Low-rank Phenomena of Transformer}
Figure~\ref{fig:low_rank_W} illustrates the distribution of singular value across different parameter matrices under varying initialization scales. The results reveal that as the initialization scale decreases, the parameter matrices exhibit a pronounced low-rank structure, which in turn facilitates a simpler learning mode.
\begin{figure}[htbp]
    \centering
    \includegraphics[width=1\linewidth]{figure/appendix/W_svd_all.png}
    \caption{The distribution of singular value in different parameter matrices under different initialization scales. We denote the singular value vector by $S$ and the $i$-th largest singular value by $s_i$.}
    \label{fig:low_rank_W}
\end{figure}
% \section{Analysis of the embedding space}\label{app:theory_of_emd&dist}



% \newpage
\section{The Second Attention Module}\label{sec:second_attention_app}
The function of the second attention module is to extract the key preceding the anchors $z_{p}$, and transfer its information to the last position. Figure~\ref{fig:second_attn}A depicts the last row of the attention matrix before applying softmax, whose variation trend with respect to position index $i$ can be divided into three parts: (1) for $i \leq p$, the attention increases progressively as $i$ increases; (2) for $p+1 \leq i \leq p+q$, the attention exhibits a slight decrease; and (3) for $i \geq p+q$, the attention drops sharply.
\begin{figure}[htpb]
    \centering
    \includegraphics[width=1\linewidth]{figure/appendix/second_attention.png}

    \caption{Characteristic of the second attention module. A: The last row of the second attention matrix (without applying softmax) for a randomly selected sequence. B: A heatmap of the second attention matrix for the same sequence. C: The last row of the matrix $\vQ\left(\vW^{\rm pos}\vW^K\right)^T/\sqrt{d_k}$ immediately before the final token. D: The cosine similarity of positional embeddings $\cos\left(\vw^{{\rm pos},i},\vw^{{\rm pos},j}\right)$ for $i,j=1,2,\cdots,L-1$.}
    \label{fig:second_attn}
\end{figure}

Positional encoding plays a crucial role in this step. Figure~\ref{fig:second_attn}C illustrates the last row of the attention matrix after substituting $\vK$ with $\vW^{\text{pos}}\vW^{K}$, suggesting a increasing trend with the position index. Note that we only present the index immediately before the final token, as the key does not appear at the last position and thus is not required to adhere to the same pattern. Furthermore, since the reasoning anchor and the tokens following it are augmented with reasoning anchor's information in the first attention module, this information can be utilized to reduce the attention for tokens after the token.
We construct a detailed mechanism about this in ~\ref{app:second_attn_mechanism}.

\section{Reconstruction Mechanism for Information Capturing}
To verify our observation is significant for information capturing for the Transformer model, we reconstruct the embedding space, the first attention module, and the second attention module and exhibit the process of extracting the key-anchor pair from a reasoning sequence.

\subsection{Embedding Space}
\begin{assumption} [Word Embedding]\label{word_embedding}
   We assume the embedding space has the following properties:
\begin{enumerate}
    \item $\cos\left(\vw^{{\rm emb},s_{\rm mem}},\vw^{{\rm emb},s_{\rm rsn}}\right)=0, \cos\left(\vw^{{\rm emb},s_{\rm rsn}},\vw^{{\rm emb},s_{\rm key}}\right)=0,\quad  \forall s_{\rm mem}\in\fA_{\rm mem},s_{\rm rsn}\in\fA_{\rm rsn},s_{\rm key}\in \fZ$.
    \item Fix any $s_1\in\fZ$, $\cos\left(\vw^{{\rm emb},s_1},\vw^{{\rm emb},s_2}\right)  \geq\cos\left(\vw^{{\rm emb},s_1},\vw^{{\rm emb},s_3}\right) \text{ if }\left|s_1-s_2\right|\leq\left|s_1-s_3\right|, \quad \forall s_{2},s_{3}\in \fZ$.
    \item There exists a universal constant $C_{\vw}<\infty$ such that $||\vw^{{\rm emb},s}||_{\infty}\leq C_{\vw}$ for any token $s$.
\end{enumerate}
\end{assumption}
% \begin{proof}
%     Since $\fZ,\fA_{\rm mem},\fA_{\rm rsn}$ are pairwise disjoint, given any $\sigma_1,\sigma_2>0, \epsilon>0$, we set that
% \begin{align*}
%     \vw^{{\rm emb},z}_{i}=e^{-\frac{(z-i)^2}{8}}\delta_{i\in\fZ}+\epsilon,\quad \vw^{{\rm emb},s}_{i} = e^{-\frac{(s-i)^2}{8}}\delta_{i\in\fZ\cup\fA_{\rm rsn}}-\frac{1}{d_m}\delta_{i\in\fZ}+\epsilon
% \end{align*}
%     For any $z\in\fZ,a_{\rm rsn}\in\fA_{\rm rsn}$. Let $\vx_0$ be an arbitrary random vector, and let $\theta>0$. Define $\vR^{\theta}_{(i,j)}$ as the rotation matrix representing a rotation by $\theta$ degrees in the plane spanned by the $i$-th and $j$-th coordinate axes. We then set $\vx_{(mem,0)}=\vx_0\textbf{1}_{\fA_{\rm mem}}$, which serves as the embedding vector of $s_{mem,\min}$ and recursively define $\vx_{(mem,j)}=\vR^{\theta}_{(j-1,j)}\vx_{(mem,j-1)}$ for $j= 1,2,\cdots,s_{mem,\max}$. Under this construction, the embedding space satisfies all required properties.
% \end{proof}

In addition to word embeddings, position embeddings should be effectively utilized, as they play a critical role in the functionality of the second attention module. Here, we propose some assumptions about the relationship between word embeddings and position embeddings, with further characteristics to be elaborated upon later.
\begin{assumption}[Position Embedding 1]
Given any position embedding vector $\vw^{{\rm pos},i}$ where $i$ denotes the position index, we assume that $\vw^{\rm{pos},i}\perp \vw^{{\rm emb},s} $ for all $i=1,2,\cdots L$ and $s\in\fZ\cup\fA_{\rm rsn}\cup\fA_{\rm mem}$.
\end{assumption}
\begin{assumption}[Position Embedding 2]\label{assump:pe2}
    We assume that $\cos\left(\vw^{{\rm pos},i},\vw^{{\rm pos},j}\right)=\cos\frac{|i-j|}{L}\pi$ and $||\vw^{{\rm pos},i}||=1$ for any $i,j\in [1,L]$.
\end{assumption}

Given any sequence $X$, the output of the embedding layer is 
\begin{equation*}
    \vX^{(1)}=\ve^X\vW^{\rm emb}+\vW^{\rm pos}.
\end{equation*}

\subsection{First Attention Module}
In the first attention module, due to the impact of small initialization, the attention matrix  $\vA^{(1)}$ functions as an average operator. Specifically, the result of the first attention module can be interpreted as
\begin{equation}
    \left({\rm Attn}^{(1)}\left(\vX^{(1)}\right) \vX^{(1)}\vW^{V(1)}\right)_j=\frac{1}{j}\left(\sum_{i\leq j}\vX^{(1)}_{i,:}\right)\vW^{V(1)}.
\end{equation}
Furthermore, performing singular value decomposition (SVD) on the value projection matrix $\vW^{V(1)}$ reveals that its largest singular value is significantly greater than the remaining singular values. The left singular vector corresponding to the largest singular value $\vW^{V(1)}$ is highly similar to the embedding vectors of the reasoning anchors which indicates that $\vW^{V(1)}$ can be approximated by
\begin{equation}
    \vW^{V(1)}=\lambda_V\left(\frac{1}{||\sum_{s\in\fA_{\rm rsn}}\vW^{{\rm emb},s}||_2}\sum_{s\in\fA_{\rm rsn}}\vW^{{\rm emb},s}\right)^T\vv,
\end{equation}
where $\lambda_{V}$ is the singular value and $\vv\in\sR^{1\times d_k}$ denotes the right singular vector. Since $\vw^{{\rm emb},s_{\rm rsn}}\perp \vw^{{\rm emb},s_{\rm mem}}$ for any $s_{\rm rsn}\in\fA_{\rm rsn},s_{\rm mem}\in\fA_{\rm mem}$ and $\vw^{\rm emb}\perp\vw^{\rm pos}$. Then the result of the attention operator can be interpreted as
\begin{equation}
    \left({\rm Attn}^{(1)}\left(\vX^{(1)}\right) \vX^{(1)}\vW^{V(1)}\right)_{j,:}=\frac{\tilde{\lambda}_V}{j}\left(\sum_{i\leq j}\vW^{{\rm emb},X}_{i,:}\right)\overline{\vw}^{{\rm emb},T}_{\fA_{\rm rsn}}\vv,
\end{equation}
where $\overline{\vw}^{\rm emb}_{\fA_{\rm rsn}}=\sum_{s\in\fA_{\rm rsn}}\vw^{{\rm emb},s},\tilde{\lambda}_V=\frac{\lambda_V}{||\overline{\vw^{\rm emb}}_{\fA_{\rm rsn}}||}$. Substituting the reasoning sequence $X^{\rm rsn}$ and memory sequence $X^{\rm mem}$, respectively, into this formulation, we derive the following results:
\begin{align}
    \left({\rm Attn}^{(1)} \vX^{{\rm mem},(1)}\vW^{V(1)}\right)_{j,:} &= 0,\\
    \left({\rm Attn}^{(1)} X^{{\rm rsn},(1)}\vW^{V(1)}\right)_{j,:} &= \left\{\begin{aligned}
        &\bm{0},\quad j\leq p,\\
        &\frac{\tilde{\lambda}_V}{j}\left(\sum_{i=p+1}^{\min(j,p+q)}\vW^{{\rm emb},X^{\rm rsn}}_i\right)\overline{\vw}^{{\rm emb},T}_{\fA_{\rm rsn}}\vv,\quad p<j\leq L,
    \end{aligned}\right.
\end{align}
where $j$ means the row index. Thus, all tokens following the reasoning anchor are effectively “tagged,” facilitating the identification of the anchor. Define the output of the first attention module is as follows:
\begin{align*}
    \vX^{(2)} &= \vX^{(1)}+{\rm Attn}^{(1)}\left(\vX^{(1)}\right) \vX^{(1)}\vW^{V(1)}.
\end{align*}
Under the Assumption~\ref{word_embedding}, we can formulate the output of the reasoning sequence further
\begin{align*}
    \vX^{{\rm rsn},(2)}_{j,:}=\left\{
    \begin{aligned}
        &\vw^{{\rm emb},z_j}+\vw^{{\rm pos},j},\quad j\leq p;\\
        &\vw^{{\rm emb},a_j}+\vw^{{\rm pos},j}+\frac{\tilde{\lambda}_V}{j}\left(\sum_{i=p+1}^j\vw^{{\rm emb},a_i}\right)\overline{\vw}^{{\rm emb},T}_{\fA_{\rm rsn}}\vv,\quad p+1\leq j\leq p+q;\\
        &\vw^{{\rm emb},z_j}+\vw^{{\rm pos},j}+\frac{\tilde{\lambda}_V}{j}\left(\sum_{i=p+1}^{p+q}\vw^{{\rm emb},a_i}\right)\overline{\vw}^{{\rm emb},T}_{\fA_{\rm rsn}}\vv,\quad p+q+1\leq j\leq L.
    \end{aligned}
    \right.
\end{align*}

\subsection{Second Attention Module}\label{app:second_attn_mechanism}
We observe that the first attention module introduces additional information to all tokens with indices $j\geq p+1$. The subsequent challenge is to identify the reasoning tokens and the key token, and effectively propagate their information to the last position in the sequence. To achieve this, we construct the following attention distribution and demonstrate its properties:
\begin{definition}[Cliff Sequence]
Given a sequence \( \vl \in \mathbb{R}^L \), we define \( \vl \) as a $(p,q)$-cliff sequence if there exists $p,L\in\sN^+$ such that $\vl$ satisfies the following conditions:
\begin{enumerate}
    \item (Increasing Segment) \( \vl_{i+1} > \vl_i \) for all \( i < p \);
    \item (Plateau) \( \frac{\vl_{p-1}+\vl_p}{2} \leq \vl_{p+1} , \cdots , \vl_{p+q}\leq\vl_p \);
    \item (Descending Segment) \( \vl_i < \vl_1 \) for all \( p+q < i \leq L \).
\end{enumerate}
\end{definition}
It is evident that if the attention of the last token forms a $(p,q)$-cliff sequence, it can effectively capture the information of the tokens and the key. Specifically, we have following results to illustrate its feasibility.
\begin{theorem}
     For any $\varepsilon > 0$, there exists a $(p,q)$-cliff sequence $\vl$ with norm $C$ such that ${\rm softmax}(\vl)_i \leq \varepsilon$ for any $i\in[1,p-1]\cup[p+q+1,L]$.
\end{theorem}
\begin{proof}
    It's evident that we just need to illustrate ${\rm softmax}(\vl)_{p-1}\rightarrow 0$ as $C\rightarrow\infty$. Denote that $\vl=C\tilde{\vl}$, then we have
    \begin{align*}
        {\rm softmax}\left(\vl\right)_{p-1}&=\frac{e^{C\tilde{\vl}_{p-1}}}{\sum_{j=1}^Le^{C\tilde{\vl}_{j}}}\\
        &=\frac{1}{\sum_{j\in[1, p-1]\cup[p+q+1,L]}e^{C\left(\tilde{\vl}_i-\tilde{\vl}_{p-1}\right)}+\sum_{j\in[p,p+q]}e^{C\left(\tilde{\vl}_i-\tilde{\vl}_{p-1}\right)}}.
    \end{align*}
    Since that $\tilde{\vl}_j\leq\tilde{\vl}_{p-1}$ for any $j\in[1, p-1]\cup[p+q+1,L]$ and $\tilde{\vl}_j\geq\tilde{\vl}_{p-1}$ for any $j\in[p,p+q]$, so we have
    \begin{align*}
        \lim_{C\rightarrow\infty}\sum_{j\in[1, p-1]\cup[p+q+1,L]}e^{C\left(\tilde{\vl}_i-\tilde{\vl}_{p-1}\right)}=0\quad\text{ and } \lim_{C\rightarrow\infty}\sum_{j\in[p,p+q]}e^{C\left(\tilde{\vl}_i-\tilde{\vl}_{p-1}\right)}=\infty  ,
    \end{align*}
    then $\rm{softmax}\left(\vl\right)_{p-1}\rightarrow 0$.
\end{proof}


\begin{theorem}\label{thm:cliff}
    There exists a real matrix $\tilde{\vA}\in \sR^{d_m\times d_m}$ such that $\vX^{\rm rsn,(2)}_{L,:}\tilde{\vA}\vX^{{\rm rsn,(2)},T}$ is a $\left(p,q\right)$-cliff sequence.
\end{theorem}
\begin{proof}
    Assume that $\tilde{\vA}=||\vv||^4\vpi\left(\text{span}\{\vw^{pos}\}\right)-\mu\vv^T\vv,\mu>0$, then we have
    % \begin{align*}
    %     \vX^{\rm rsn,(2)}_{L,:}\tilde{\vA}&=\left(\vw^{{\rm emb},z_L}+\vw^{{\rm pos},L}+\frac{\tilde{\lambda}_V}{L}\left(\sum_{i=p+1}^{p+q}\vw^{{\rm emb},a_i}\right)\overline{\vw}^{{\rm emb},T}_{\fA_{\rm rsn}}\vv\right)\left(||\vv||^4\vpi\left(\text{span}\{\vw^{\rm pos}\}\right)-\mu\vv^T\vv\right)\\
    %     &=||\vv||^4\vw^{{\rm pos},L}-\frac{\tilde{\lambda}_V\mu}{L}\left(\sum_{i=p+1}^{p+q}\vw^{{\rm emb},{a_i}}\right)\overline{\vw}^{{\rm emb},T}_{\fA_{\rm rsn}} ||\vv||_2^2\vv
    % \end{align*}
    \begin{align*}
        \vX^{\rm rsn,(2)}_{L,:}\tilde{\vA}\vX^{{\rm rsn,(2)},T}&=\left\{
    \begin{aligned}
        &\left(\vw^{{\rm pos},L},\vw^{{\rm pos},j}\right)||\vv||_2^4,\quad j\leq q,\\
        & \left(\vw^{{\rm pos},L},\vw^{{\rm pos},j}\right)||\vv||_2^4-\frac{\tilde{\lambda}_V \mu}{L}\left(\sum_{i=p+1}^{p+q}\vw^{{\rm emb},a_i},\overline{\vw}^{{\rm emb}}_{\fA_{\rm rsn}} \right)\left(\vv,\vw^{{\rm emb},a_j}\right)||\vv||^2\\
        &-\frac{\tilde{\lambda}_V^2 \mu}{jL}\left(\sum_{i=p+1}^{p+q}\vw^{{\rm emb},a_i},\overline{\vw}^{\rm emb}_{\fA_{\rm rsn}}\right)\left(\sum_{i=p+1}^{j}\vw^{{\rm emb},a_i},\overline{\vw}^{\rm emb}_{\fA_{\rm rsn}}\right)||\vv||^4,\quad p+1\leq j\leq p+q,\\
        & \left(\vw^{{\rm pos},L},\vw^{{\rm pos},j}\right)||\vv||^4-\frac{\tilde{\lambda}_V^2\mu}{jL}\left(\sum_{i=p+1}^{p+q}\vw^{{\rm emb},a_i},\overline{\vw}^{\rm emb}_{\fA_{\rm rsn}} \right)^2||\vv||^4,\quad p+q+1\leq j\leq L.
    \end{aligned}
    \right.
    \end{align*}
Applying the Assumption~\ref{assump:pe2} on the position embedding, then we have the following result:
\begin{align*}
        &\frac{1}{||\vv||^4}\vX^{\rm rsn,(2)}_{L,:}\tilde{\vA}\vX^{{\rm rsn,(2)},T}\\
        =&\left\{
    \begin{aligned}
        &\cos\left(1-\frac{j}{L}\right)\pi,\quad j\leq p,\\
        & \cos\left(1-\frac{j}{L}\right)\pi-\frac{\tilde{\lambda}_V^2 \mu}{jL}\left(\sum_{i=p+1}^{p+q}\vw^{{\rm emb},a_i},\overline{\vw}^{\rm emb}_{\fA_{\rm rsn}}\right) \left(\frac{j||\vw^{{\rm emb},a_j}||}{\tilde{\lambda}_V||\vv||}\cos\left(\vv,\vw^{{\rm emb},a_j}\right)\right.\\
        \qquad &\left.+\left(\sum_{i=p+1}^{j}\vw^{{\rm emb},a_i},\overline{\vw}^{\rm emb}_{\fA_{\rm rsn}}\right)\right),\quad p+1\leq j\leq p+q,\\
        & \cos\left(1-\frac{j}{L}\right)\pi-\frac{\tilde{\lambda}_V^2\mu}{jL}\left(\sum_{i=p+1}^{p+q}\vw^{{\rm emb},a_i},\overline{\vw}^{\rm emb}_{\fA_{\rm rsn}}\right) ^2,\quad p+q+1\leq j\leq L.
    \end{aligned}
    \right.
    \end{align*}
To satisfy the Increasing Segment condition, we need that: 
\begin{align*}
    &\cos\left(1-\frac{j}{L}\right)\pi-\frac{\tilde{\lambda}_V^2 \mu}{jL}\left(\sum_{i=p+1}^{p+q}\vw^{{\rm emb},a_i},\overline{\vw}^{\rm emb}_{\fA_{\rm rsn}}\right) \left(\frac{j||\vw^{{\rm emb},a_j}||}{\tilde{\lambda}_V||\vv||}\cos\left(\vv,\vw^{{\rm emb},a_j}\right)+\left(\sum_{i=p+1}^{j}\vw^{{\rm emb},a_i},\overline{\vw}^{\rm emb}_{\fA_{\rm rsn}}\right)\right)\\
    \geq&\frac{1}{2}\cos\left(1-\frac{p}{L}\right)\pi+\frac{1}{2}\cos\left(1-\frac{p-1}{L}\right)\pi,\qquad \qquad \text{for any } p\in\left[1,L-q\right],j\in\left[p+1,p+q\right].
\end{align*}
% Since we have that:
% \begin{align*}
%     \frac{1}{2}\cos\left(1-\frac{p}{L}\right)\pi+\frac{1}{2}\cos\left(1-\frac{p-1}{L}\right)\pi=-\cos\left(\frac{2p-1}{2L}\pi\right)\cos\left(\frac{\pi}{2L}\right)\leq-\cos\left(\frac{2\left(L-q\right)-1}{2L}\pi\right)\cos\left(\frac{\pi}{2L}\right).
% \end{align*}
% Then we have
% \begin{align*}
%     &\frac{\tilde{\lambda}_V^2 \mu}{j}\left(\sum_{i=p+1}^{p+q}\vw^{{\rm emb},a_i},\overline{\vw}^{\rm emb}_{\fA_{\rm rsn}}\right) \left(\frac{j||\vw^{{\rm emb},a_j}||}{\tilde{\lambda}_V||\vv||}\cos\left(\vv,\vw^{{\rm emb},a_j}\right)+\left(\sum_{i=p+1}^{j}\vw^{{\rm emb},a_i},\overline{\vw}^{\rm emb}_{\fA_{\rm rsn}}\right)\right)\\
%     \leq&-L+L\cos\left(\frac{2\left(L-q\right)-1}{2L}\pi\right)\cos\left(\frac{\pi}{2L}\right).
% \end{align*}
Denote that:
\begin{equation}
\tilde{M}:=\max_{p,q}\left(\sum_{i=p+1}^{p+q}\vw^{{\rm emb},a_i},\overline{\vw}^{\rm emb}_{\fA_{\rm rsn}}\right),\qquad \tilde{m}:=\min_{p,q}\left(\sum_{i=p+1}^{p+q}\vw^{{\rm emb},a_i},\overline{\vw}^{\rm emb}_{\fA_{\rm rsn}}\right).
\end{equation}
Then we have
\begin{align*}
    &\frac{\tilde{\lambda}_V^2\mu}{jL}\tilde{M}\left(\frac{j||\vw^{{\rm emb},a_j}||}{\tilde{\lambda}_V||\vv||}\cos\left(\vv,\vw^{{\rm emb},a_j}\right)+\tilde{M}\right)\leq-\cos\left(\frac{p+1}{L}\right)\pi+\frac{1}{2}\cos\left(\frac{p}{L}\right)\pi+\frac{1}{2}\cos\left(\frac{p-1}{L}\right)\pi\\
    \rightarrow & \frac{\tilde{\lambda}_V^2\mu}{jL}\tilde{M}\left(\frac{j||\vw^{{\rm emb},a_j}||}{\tilde{\lambda}_V||\vv||}\cos\left(\vv,\vw^{{\rm emb},a_j}\right)+\tilde{M}\right)\\
    &\leq\sqrt{\left(\frac{1}{2}\left(1-\cos\left(\frac{\pi}{L}\right)\right)\right)^2+\left(\frac{3}{2}\sin\left(\frac{\pi}{L}\right)\right)^2}\cos\left(\frac{L-1}{L}\pi-\arctan{\frac{3\sin\left(\frac{\pi}{L}\right)}{1-\cos\left(\frac{\pi}{L}\right)}}\right).
\end{align*}
%\frac{1}{2}\cos\left(1-\frac{p}{L}\right)\pi+\frac{1}{2}\cos\left(1-\frac{p-1}{L}\right)\pi\geq\cos\left(\frac{\pi}{2L}\right)
Denote the right side by $C_M$, and simplify it with
\begin{equation}\label{eq:condition1}
    \frac{||\vw^{{\rm emb},a_j}||}{\tilde{\lambda}_V||\vv||}\cos\left(\vv,\vw^{{\rm emb},a_j}\right)\leq \frac{LC_M}{\tilde{\lambda}_V^2\mu\tilde{M} }-\frac{\tilde{M}}{L}.
\end{equation}
For another side, we assume that:
\begin{align*}
    \cos\left(1-\frac{j}{L}\right)\pi-\frac{\tilde{\lambda}_V^2 \mu}{jL}\left(\sum_{i=p+1}^{p+q}\vw^{{\rm emb},a_i},\overline{\vw}^{\rm emb}_{\fA_{\rm rsn}}\right) \left(\frac{j||\vw^{{\rm emb},a_j}||}{\tilde{\lambda}_V||\vv||}\cos\left(\vv,\vw^{{\rm emb},a_j}\right)+\left(\sum_{i=p+1}^{j}\vw^{{\rm emb},a_i},\overline{\vw}^{\rm emb}_{\fA_{\rm rsn}}\right)\right)\leq \cos\left(1-\frac{p}{L}\right)\pi,
\end{align*}
which implies that:
\begin{align*}
    -\frac{\tilde{\lambda}_V^2 \mu}{jL}\left(\sum_{i=p+1}^{p+q}\vw^{{\rm emb},a_i},\overline{\vw}^{\rm emb}_{\fA_{\rm rsn}}\right) \left(\frac{j||\vw^{{\rm emb},a_j}||}{\tilde{\lambda}_V||\vv||}\cos\left(\vv,\vw^{{\rm emb},a_j}\right)+\left(\sum_{i=p+1}^{j}\vw^{{\rm emb},a_i},\overline{\vw}^{\rm emb}_{\fA_{\rm rsn}}\right)\right)&\leq -2\sin\left(\frac{2p+q}{2L}\pi\right)\sin\left(\frac{q}{2L}\pi\right).\\
    % &=-2\sin\left(\frac{2p+q}{2L}\pi\right)\sin\left(\frac{q}{2L}\pi\right).
\end{align*}
We have that:
\begin{align}\label{eq:condition2}
    % \frac{\tilde{\lambda}_V^2 \mu}{jL}\tilde{m}\left(\frac{j||\vw^{{\rm emb},a_j}||}{\tilde{\lambda}_V||\vv||}\cos\left(\vv,\vw^{{\rm emb},a_j}\right)+\tilde{m}\right) \geq 2\sin\left(\frac{q}{2L}\pi\right)\\
    \frac{||\vw^{{\rm emb},a_j}||}{\tilde{\lambda}_V||\vv||}\cos\left(\vv,\vw^{{\rm emb},a_j}\right)\geq \frac{LC_m}{\tilde{\lambda}_V^2\mu\tilde{m}}-\frac{\tilde{m}}{L}.
\end{align}
These two conditions give the norm and direction scope of $\vv$. For the Descending Segment condition, we have that

    % \rightarrow  \frac{j}\leq{\tilde{\lambda}_V}\cos(\vv,\vx_{a_j}^{\text{emb}})+\left(\sum_{i=p+1}^{j}\vx_{a_i}^{\text{emb}}\right)^T\overline{\vx}_{\fA_{\rm rsn}}=\frac{2L}{\tilde{\lambda}_V^2\mu\left(\sum_{i=p+1}^{p+q}\vx_{a_i}^{\text{emb}}\right)^T\overline{\vx}_{\fA_{\rm rsn}}} j\sin\frac{j+q}{2L}\pi\sin\frac{j-q}{2L}\pi\\
\begin{equation}\label{eq:condition3}
\begin{aligned}
    &\cos\left(1-\frac{1}{L}\right)\pi>\cos\left(1-\frac{j}{L}\right)\pi-\frac{\tilde{\lambda}_V^2\mu}{jL}\left(\sum_{i=p+1}^{p+q}\vw^{{\rm emb},a_i},\overline{\vw}^{\rm emb}_{\fA_{\rm rsn}}\right) ^2\\
    \rightarrow &\tilde{\lambda}_V^2\mu > jL\left( \cos\left(\frac{\pi}{L}\right)-\cos\left(\frac{j\pi}{L}\right)\right)\left(\sum_{i=p+1}^{p+q}\vw^{{\rm emb},a_i},\overline{\vw}^{\rm emb}_{\fA_{\rm rsn}}\right) ^{-2}\\
    \rightarrow & \tilde{\lambda}_V^2\mu >L^2\left(1+\cos\left(\frac{\pi}{L}\right)\right)\tilde{m}^{-2}.
\end{aligned}
\end{equation}

With~\eqref{eq:condition1},\eqref{eq:condition2}, and~\eqref{eq:condition3}, we could give a range of $\tilde{\lambda}_V,\mu$ and the norm and direction of $\vv$ which makes $\vX^{\rm rsn,(2)}_{L,:}\tilde{\vA}\vX^{{\rm rsn,(2)},T}$ is a $\left(p,q\right)$-cliff sequence.

% Since that 
% \begin{align*}
%     jL\left( \cos\left(\frac{\pi}{L}\right)-\cos\left(\frac{j\pi}{L}\right)\right)\left[\left(\sum_{i=p+1}^{p+q}\vw^{{\rm emb},a_i}\right)^T\overline{\vw^{\rm emb}}_{\fA_{\rm rsn}} \right]^{-2} \leq L^2\left(1+\cos\left(\frac{\pi}{L}\right)\right)\left[\left(\sum_{i=p+1}^{p+q}\vw^{{\rm emb},a_i}\right)^T\overline{\vw^{\rm emb}}_{\fA_{\rm rsn}} \right]^{-2}
% \end{align*}
% % So we have $\tilde{\lambda}_V^2\mu> L^2\left(1+\cos\left(\frac{\pi}{2L}\right)\right)\left(\frac{T}{N_{\fA_{rsrn}}}\right)^{-2}\max_{a_i\in\fA_{\rm rsn}}||\vw^{{\rm emb},a_i}||$, then
% So we have $\tilde{\lambda}_V^2\mu>L^2\left(1+\cos\left(\frac{\pi}{L}\right)\right)\left[\left(\sum_{i=p+1}^{p+q}\vx_{a_i}^{\rm{emb}}\right)^T\overline{\vx}_{\fA_{\rm rsn}} \right]^{-2}$, then
% \begin{align*}
%     \frac{1}{\tilde{\lambda}_V}\cos(\vv,\vw^{{\rm emb},a_j})&<\frac{\left(-1-\cos\left(\frac{\pi}{2L}\right)\right)\left(\sum_{i=j+1}^{p+q}\vw^{{\rm emb},a_i}\right)^T\overline{\vw^{\rm emb}}_{\fA_{\rm rsn}}}{\left(1+\cos\left(\frac{\pi}{L}\right)\right)L}-\frac{1}{j}\left(\sum_{i=p+1}^{j}\vw^{{\rm emb},{a_i}}\right)^T\overline{\vw^{\rm emb}}_{\fA_{\rm rsn}}\\
%     &<-\left(\frac{1+\cos\left(\frac{\pi}{2L}\right)}{\left(1+\cos\left(\frac{\pi}{L}\right)\right)L}+\frac{1}{2}\right)\frac{T}{N_{\fA_{\rm rsn}}}\max_{a_i\in\fA_{\rm rsn}}||\vw^{{\rm emb},a_i}||^2\\
% \end{align*}
% In the other side $\tilde{\lambda}_V^2\mu>L^2\left(1+\cos\left(\frac{\pi}{L}\right)\right)\left(\frac{T}{N_{\fA_{\rm rsn}}}\right)^{-2}\max_{a_i\in\fA_{\rm rsn}}||\vw^{{\rm emb},a_i}||$. Then $\frac{1}{||\vv||^4}\vX_L^{(1)}\tilde{\vA}\vX^{(1)T}$ is a ($p,T$)-cliff sequence and so is $\vX_L^{(1)}\tilde{\vA}\vX^{(1)T}$

\end{proof}


\end{document}