@inproceedings{
lin2024not,
title={Not All Tokens Are What You Need for Pretraining},
author={Zhenghao Lin and Zhibin Gou and Yeyun Gong and Xiao Liu and yelong shen and Ruochen Xu and Chen Lin and Yujiu Yang and Jian Jiao and Nan Duan and Weizhu Chen},
booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},
year={2024},
url={https://openreview.net/forum?id=0NMzBwqaAJ}
}
@article{distributionofsumuniform,
author = {Caiado, Camila and Rathie, Pushpa},
year = {2007},
month = {01},
pages = {},
title = {Polynomial coefficients and distribution of the sum of discrete uniform variables.}
}
@misc{adamw,
      title={Decoupled Weight Decay Regularization}, 
      author={Ilya Loshchilov and Frank Hutter},
      year={2019},
      eprint={1711.05101},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1711.05101}, 
}
 @Article{CSIAMchen, author = {Chen , Zheng-An and Li , Yuqing and Luo , Tao and Zhou , Zhangchen and Xu , Zhi-Qin John}, title = {Phase Diagram of Initial Condensation for Two-Layer Neural Networks}, journal = {CSIAM Transactions on Applied Mathematics}, year = {2024}, volume = {5}, number = {3}, pages = {448--514}, issn = {2708-0579}, doi = {https://doi.org/10.4208/csiam-am.SO-2023-0016}, url = {http://global-sci.org/intro/article_detail/csiam-am/23306.html} }

@inproceedings{GPT2,
  title={Language Models are Unsupervised Multitask Learners},
  author={Alec Radford and Jeff Wu and Rewon Child and David Luan and Dario Amodei and Ilya Sutskever},
  year={2019},
  url={https://api.semanticscholar.org/CorpusID:160025533}
}
@inproceedings{
  PrOntoQA,
  title={Language Models Are Greedy Reasoners: A Systematic Formal Analysis of Chain-of-Thought},
  author={Abulhair Saparov and He He},
  booktitle={The Eleventh International Conference on Learning Representations},
  year={2023},
  url={https://openreview.net/forum?id=qFVVBzXxR2V}
}

@misc{tinystories,
      title={TinyStories: How Small Can Language Models Be and Still Speak Coherent English?}, 
      author={Ronen Eldan and Yuanzhi Li},
      year={2023},
      eprint={2305.07759},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2305.07759}, 
}

@misc{abdin2024phi4technicalreport,
      title={Phi-4 Technical Report}, 
      author={Marah Abdin and Jyoti Aneja and Harkirat Behl and SÃ©bastien Bubeck and Ronen Eldan and Suriya Gunasekar and Michael Harrison and Russell J. Hewett and Mojan Javaheripi and Piero Kauffmann and James R. Lee and Yin Tat Lee and Yuanzhi Li and Weishung Liu and Caio C. T. Mendes and Anh Nguyen and Eric Price and Gustavo de Rosa and Olli Saarikivi and Adil Salim and Shital Shah and Xin Wang and Rachel Ward and Yue Wu and Dingli Yu and Cyril Zhang and Yi Zhang},
      year={2024},
      eprint={2412.08905},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2412.08905}, 
}

@misc{he2015delvingdeeprectifierssurpassing,
      title={Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification}, 
      author={Kaiming He and Xiangyu Zhang and Shaoqing Ren and Jian Sun},
      year={2015},
      eprint={1502.01852},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/1502.01852}, 
}
@Inbook{LeCun1998,
author="LeCun, Yann
and Bottou, Leon
and Orr, Genevieve B.
and M{\"u}ller, Klaus -Robert",
editor="Orr, Genevieve B.
and M{\"u}ller, Klaus-Robert",
title="Efficient BackProp",
bookTitle="Neural Networks: Tricks of the Trade",
year="1998",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="9--50",
abstract="The convergence of back-propagation learning is analyzed so as to explain common phenomenon observedb y practitioners. Many undesirable behaviors of backprop can be avoided with tricks that are rarely exposedin serious technical publications. This paper gives some of those tricks, ando.ers explanations of why they work. Many authors have suggested that second-order optimization methods are advantageous for neural net training. It is shown that most ``classical'' second-order methods are impractical for large neural networks. A few methods are proposed that do not have these limitations.",
isbn="978-3-540-49430-0",
doi="10.1007/3-540-49430-8_2",
url="https://doi.org/10.1007/3-540-49430-8_2"
}


@misc{zhang2024anchorfunctiontypebenchmark,
      title={Anchor function: a type of benchmark functions for studying language models}, 
      author={Zhongwang Zhang and Zhiwei Wang and Junjie Yao and Zhangchen Zhou and Xiaolong Li and Weinan E and Zhi-Qin John Xu},
      year={2024},
      eprint={2401.08309},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2401.08309}, 
}

@article{zhang2024anchor,
  title={Anchor function: a type of benchmark functions for studying language models},
  author={Zhang, Zhongwang and Wang, Zhiwei and Yao, Junjie and Zhou, Zhangchen and Li, Xiaolong and {E}, Weinan and Xu, Zhi-Qin John},
  journal={arXiv preprint arXiv:2401.08309},
  year={2024}
}

@article{jastrzkebski2018relation,
  title={On the relation between the sharpest directions of DNN loss and the SGD step length},
  author={Jastrzebski, Stanislaw and Kenton, Zachary and Ballas, Nicolas and Fischer, Asja and Bengio, Yoshua and Storkey, Amos},
  journal={arXiv preprint arXiv:1807.05031},
  year={2018}
}

@article{wager2014altitude,
  title={Altitude training: Strong bounds for single-layer dropout},
  author={Wager, Stefan and Fithian, William and Wang, Sida and Liang, Percy S},
  journal={Advances in Neural Information Processing Systems},
  volume={27},
  year={2014}
}
@inproceedings{cavazza2018dropout,
  title={Dropout as a low-rank regularizer for matrix factorization},
  author={Cavazza, Jacopo and Morerio, Pietro and Haeffele, Benjamin and Lane, Connor and Murino, Vittorio and Vidal, Rene},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={435--444},
  year={2018},
  organization={PMLR}
}
@inproceedings{pal2020regularization,
  title={On the regularization properties of structured dropout},
  author={Pal, Ambar and Lane, Connor and Vidal, Ren{\'e} and Haeffele, Benjamin D},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={7671--7679},
  year={2020}
}
@article{baldi2013understanding,
  title={Understanding dropout},
  author={Baldi, Pierre and Sadowski, Peter J},
  journal={Advances in neural information processing systems},
  volume={26},
  year={2013}
}
@inproceedings{mianjy2018implicit,
  title={On the implicit bias of dropout},
  author={Mianjy, Poorya and Arora, Raman and Vidal, Rene},
  booktitle={International Conference on Machine Learning},
  pages={3540--3548},
  year={2018},
  organization={PMLR}
}
@article{wu2018sgd,
  title={How sgd selects the global minima in over-parameterized learning: A dynamical stability perspective},
  author={Wu, Lei and Ma, Chao and E, Weinan},
  journal={Advances in Neural Information Processing Systems},
  volume={31},
  year={2018}
}
@article{jastrzkebski2017three,
  title={Three factors influencing minima in sgd},
  author={Jastrzebski, Stanislaw and Kenton, Zachary and Arpit, Devansh and Ballas, Nicolas and Fischer, Asja and Bengio, Yoshua and Storkey, Amos},
  journal={arXiv preprint arXiv:1711.04623},
  year={2017}
}

@article{rahaman2018spectral,
  title={On the Spectral Bias of Deep Neural Networks},
  author={Rahaman, Nasim and Arpit, Devansh and Baratin, Aristide and Draxler, Felix and Lin, Min and Hamprecht, Fred A and Bengio, Yoshua and Courville, Aaron},
  journal={International Conference on Machine Learning},
  year={2019}
}
@article{li2017visualizing,
  title={Visualizing the loss landscape of neural nets},
  author={Li, Hao and Xu, Zheng and Taylor, Gavin and Studer, Christoph and Goldstein, Tom},
  journal={arXiv preprint arXiv:1712.09913},
  year={2017}
}
@article{hairer2003geometric,
  title={Geometric numerical integration illustrated by the St{\"o}rmer--Verlet method},
  author={Hairer, Ernst and Lubich, Christian and Wanner, Gerhard},
  journal={Acta numerica},
  volume={12},
  pages={399--450},
  year={2003},
  publisher={Cambridge University Press}
}
@article{cao2019generalization,
  title={Generalization bounds of stochastic gradient descent for wide and deep neural networks},
  author={Cao, Yuan and Gu, Quanquan},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  pages={10836--10846},
  year={2019}
}

@inproceedings{cao2019towards,
  title     = {Towards Understanding the Spectral Bias of Deep Learning},
  author    = {Cao, Yuan and Fang, Zhiying and Wu, Yue and Zhou, Ding-Xuan and Gu, Quanquan},
  booktitle = {Proceedings of the Thirtieth International Joint Conference on
               Artificial Intelligence, {IJCAI-21}},
  pages     = {2205--2211},
  year      = {2021},
  month     = {8}
}

@article{basri2019convergence,
  title={The convergence rate of neural networks for learned functions of different frequencies},
  author={Ronen, Basri and Jacobs, David and Kasten, Yoni and Kritchman, Shira},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  pages={4761--4771},
  year={2019}
}
@inproceedings{basri2020frequency,
  title={Frequency bias in neural networks for input of non-uniform density},
  author={Basri, Ronen and Galun, Meirav and Geifman, Amnon and Jacobs, David and Kasten, Yoni and Kritchman, Shira},
  booktitle={International Conference on Machine Learning},
  pages={685--694},
  year={2020},
  organization={PMLR}
}
@inproceedings{lengerich2022dropout,
  title={Dropout as a regularizer of interaction effects},
  author={Lengerich, Benjamin J and Xing, Eric and Caruana, Rich},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={7550--7564},
  year={2022},
  organization={PMLR}
}
@inproceedings{yaida2018fluctuation,
  title={Fluctuation-dissipation relations for stochastic gradient descent},
  author={Yaida, Sho},
  booktitle={International Conference on Learning Representations},
  year={2018}
}

@article{feng2021inverse,
  title={The inverse variance--flatness relation in stochastic gradient descent is critical for finding flat minima},
  author={Feng, Yu and Tu, Yuhai},
  journal={Proceedings of the National Academy of Sciences},
  volume={118},
  number={9},
  year={2021},
  publisher={National Acad Sciences}
}

@inproceedings{he2016deep,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={770--778},
  year={2016}
}

@inproceedings{glorot2010understanding,
  title={Understanding the difficulty of training deep feedforward neural networks},
  author={Glorot, Xavier and Bengio, Yoshua},
  booktitle={Proceedings of the thirteenth international conference on artificial intelligence and statistics},
  pages={249--256},
  year={2010}
}

@incollection{Bengio+chapter2007,
author = {Bengio, Yoshua and LeCun, Yann},
booktitle = {Large Scale Kernel Machines},
publisher = {MIT Press},
title = {Scaling Learning Algorithms Towards {AI}},
year = {2007}
}
@article{maennel2018gradient,
	title={Gradient descent quantizes relu network features},
	author={Maennel, Hartmut and Bousquet, Olivier and Gelly, Sylvain},
	journal={arXiv preprint arXiv:1803.08367},
	year={2018}
}
@article{Hinton06,
author = {Hinton, Geoffrey E. and Osindero, Simon and Teh, Yee Whye},
journal = {Neural Computation},
pages = {1527--1554},
title = {A Fast Learning Algorithm for Deep Belief Nets},
volume = {18},
year = {2006}
}
@article{zhou2021towards,
  title={Towards Understanding the Condensation of Neural Networks at Initial Training},
  author={Zhou, Hanxu and Zhou, Qixuan and Luo, Tao and Zhang, Yaoyu and Xu, Zhi-Qin John},
  journal={arXiv preprint arXiv:2105.11686},
  year={2021}
}
@article{pellegrini2020analytic,
	title={An analytic theory of shallow networks dynamics for hinge loss classification},
	author={Pellegrini, Franco and Biroli, Giulio},
	journal={Advances in Neural Information Processing Systems},
	volume={33},
	year={2020}
}
@book{goodfellow2016deep,
title={Deep learning},
author={Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron and Bengio, Yoshua},
volume={1},
year={2016},
publisher={MIT Press}
}

@inproceedings{jacot2018neural,
  author    = {Arthur Jacot and
               Cl{\'{e}}ment Hongler and
               Franck Gabriel},
  title     = {Neural Tangent Kernel: Convergence and Generalization in Neural Networks},
  booktitle = {Advances in Neural Information Processing Systems},
  pages     = {8580--8589},
  year      = {2018},
}
@article{srivastava2014dropout,
  title={Dropout: a simple way to prevent neural networks from overfitting},
  author={Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
  journal={The journal of machine learning research},
  volume={15},
  number={1},
  pages={1929--1958},
  year={2014},
  publisher={JMLR. org}
}
@article{hinton2012improving,
  title={Improving neural networks by preventing co-adaptation of feature detectors},
  author={Hinton, Geoffrey E and Srivastava, Nitish and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan R},
  journal={arXiv preprint arXiv:1207.0580},
  year={2012}
}

@article{keskar2016large,
  title={On large-batch training for deep learning: Generalization gap and sharp minima},
  author={Keskar, Nitish Shirish and Mudigere, Dheevatsa and Nocedal, Jorge and Smelyanskiy, Mikhail and Tang, Ping Tak Peter},
  journal={arXiv preprint arXiv:1609.04836},
  year={2016}
}
@article{neyshabur2017exploring,
  title={Exploring generalization in deep learning},
  author={Neyshabur, Behnam and Bhojanapalli, Srinadh and McAllester, David and Srebro, Nathan},
  journal={arXiv preprint arXiv:1706.08947},
  year={2017}
}
@article{zhu2018anisotropic,
  title={The anisotropic noise in stochastic gradient descent: Its behavior of escaping from sharp minima and regularization effects},
  author={Zhu, Zhanxing and Wu, Jingfeng and Yu, Bing and Wu, Lei and Ma, Jinwen},
  journal={arXiv preprint arXiv:1803.00195},
  year={2018}
}
@article{zhang2022embedding,
  title={Embedding Principle: a hierarchical structure of loss landscape of deep neural networks},
  author={Zhang, Yaoyu and Li, Yuqing and Zhang, Zhongwang and Luo, Tao and Xu, Zhi-Qin John},
  journal={Journal of Machine Learning vol},
  volume={1},
  pages={1--45},
  year={2022}
}
@article{zhang2021mod,
  title={MOD-Net: A Machine Learning Approach via Model-Operator-Data Network for Solving PDEs},
  author={Zhang, Lulu and Luo, Tao and Zhang, Yaoyu and Xu, Zhi-Qin John and Ma, Zheng},
  journal={arXiv preprint arXiv:2107.03673},
  year={2021}
}
@article{leng2021force,
  title={Force-in-domain GAN inversion},
  author={Leng, Guangjie and Zhu, Yeku and Xu, Zhi-Qin John},
  journal={arXiv preprint arXiv:2107.06050},
  year={2021}
}

@article{xu2021towards,
  title={Towards Understanding the Condensation of Two-layer Neural Networks at Initial Training},
  author={Xu, Zhi-Qin John and Zhou, Hanxu and Luo, Tao and Zhang, Yaoyu},
  journal={arXiv preprint arXiv:2105.11686},
  year={2021}
}

@article{ma2021frequency,
  title={Frequency Principle in Deep Learning Beyond Gradient-descent-based Training},
  author={Ma, Yuheng and Xu, Zhi-Qin John and Zhang, Jiwei},
  journal={arXiv preprint arXiv:2101.00747},
  year={2021}
}
@article{wang2020implicit,
  title={Implicit bias with Ritz-Galerkin method in understanding deep learning for solving PDEs},
  author={Wang, Jihong and Xu, Zhi-Qin John and Zhang, Jiwei and Zhang, Yaoyu},
  journal={arXiv preprint arXiv:2002.07989},
  year={2020}
}
@article{luo2020fourier,
  title={Fourier-domain Variational Formulation and Its Well-posedness for Supervised Learning},
  author={Luo, Tao and Ma, Zheng and Wang, Zhiwei and Xu, Zhi-Qin John and Zhang, Yaoyu},
  journal={arXiv preprint arXiv:2012.03238},
  year={2020}
}
@Article{luo2020theory,
  author  = {Luo, Tao and Ma, Zheng and Xu, Zhi-Qin John and Zhang, Yaoyu},
  journal = {arXiv preprint arXiv:2010.08153},
  title   = {On the exact computation of linear frequency principle dynamics and its generalization},
  year    = {2020},
}
@article{li2020multi,
  title={A Multi-Scale DNN Algorithm for Nonlinear Elliptic Equations with Multiple Scales},
  author={Li, Xi-An and Xu, Zhi-Qin John and Zhang, Lei},
  journal={Communications in Computational Physics},
  volume={28},
  number={5},
  pages={1886--1906},
  year={2020}
}
@article{zhang2021embedding,
  title={Embedding principle of loss landscape of deep neural networks},
  author={Zhang, Yaoyu and Zhang, Zhongwang and Luo, Tao and Xu, Zhiqin J},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={14848--14859},
  year={2021}
}
@article{bai2022embedding,
  title={Embedding Principle in Depth for the Loss Landscape Analysis of Deep Neural Networks},
  author={Bai, Zhiwei and Luo, Tao and Xu, Zhi-Qin John and Zhang, Yaoyu},
  journal={arXiv preprint arXiv:2205.13283},
  year={2022}
}
@inproceedings{xu2020deep,
  title={Deep Frequency Principle Towards Understanding Why Deeper Learning Is Faster},
  author={Xu, Zhi-Qin John and Zhou, Hanxu},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={35},
  number={12},
  year={2021}
}
@article{luo2021phase,
  title={Phase diagram for two-layer relu neural networks at infinite-width limit},
  author={Luo, Tao and Xu, Zhi-Qin John and Ma, Zheng and Zhang, Yaoyu},
  journal={Journal of Machine Learning Research},
  volume={22},
  number={71},
  pages={1--47},
  year={2021}
}
@article{liu2020multi,
  title={Multi-Scale Deep Neural Network (MscaleDNN) for Solving Poisson-Boltzmann Equation in Complex Domains},
  author={Liu, Ziqi and Cai, Wei and Xu, Zhi-Qin John},
  journal={Communications in Computational Physics},
  volume={28},
  number={5},
  pages={1970--2001},
  year={2020}
}

@article{cai2019multi,
  title={Multi-scale Deep Neural Networks for Solving High Dimensional PDEs},
  author={Cai, Wei and Xu, Zhi-Qin John},
  journal={arXiv preprint arXiv:1910.11710},
  year={2019}
}

@article{xu2022overview,
  title={Overview frequency principle/spectral bias in deep learning},
  author={Xu, Zhi-Qin John and Zhang, Yaoyu and Luo, Tao},
  journal={arXiv preprint arXiv:2201.07395},
  year={2022}
}
@article{luo2019theory,
  title={Theory of the Frequency Principle for General Deep Neural Networks},
  author={Luo, Tao and Ma, Zheng and Xu, Zhi-Qin John and   Zhang, Yaoyu},
  journal = {CSIAM Transactions on Applied Mathematics},
  year = {2021},
  volume = {2},
  number = {3},
  pages = {484--507},
}

@article{zhang2019explicitizing,
  title={Explicitizing an Implicit Bias of the Frequency Principle in Two-layer Neural Networks},
  author={Zhang, Yaoyu and Xu, Zhi-Qin John and Luo, Tao and Ma, Zheng},
  journal={arXiv preprint arXiv:1905.10264},
  year={2019}
}
@inproceedings{zhang2019type,
  title={A type of generalization error induced by initialization in deep neural networks},
  author={Zhang, Yaoyu and Xu, Zhi-Qin John and Luo, Tao and Ma, Zheng},
  booktitle={Mathematical and Scientific Machine Learning},
  pages={144--164},
  year={2020},
  organization={PMLR}
}
@article{xu2019frequency,
  title={Frequency principle: Fourier analysis sheds light on deep neural networks},
  author={Xu, Zhi-Qin John and Zhang, Yaoyu and Luo, Tao and Xiao, Yanyang and Ma, Zheng},
  journal = {Communications in Computational Physics},
  year = {2020},
  volume = {28},
  number = {5},
  pages = {1746--1767},
}

@article{xu2018frequency,
  title={Frequency principle in deep learning with general loss functions and its potential application},
  author={Xu, Zhi-Qin John},
  journal={arXiv preprint arXiv:1811.10146},
  year={2018}
}
@article{xu2018understanding,
  title={Understanding training and generalization in deep learning by fourier analysis},
  author={Xu, Zhiqin John},
  journal={arXiv preprint arXiv:1808.04295},
  year={2018}
}


@article{xu2021towards,
  title={Towards Understanding the Condensation of Two-layer Neural Networks at Initial Training},
  author={Xu, Zhi-Qin John and Zhou, Hanxu and Luo, Tao and Zhang, Yaoyu},
  journal={arXiv preprint arXiv:2105.11686},
  year={2021}
}
@article{zhang2021linear,
  title={A Linear Frequency Principle Model to Understand the Absence of Overfitting in Neural Networks},
  author={Zhang, Yaoyu and Luo, Tao and Ma, Zheng and Xu, Zhi-Qin John},
  journal={Chinese Physics Letters},
  volume={38},
  number={3},
  pages={038701},
  year={2021},
  publisher={IOP Publishing}
}
@article{ma2021frequency,
  title={Frequency Principle in Deep Learning Beyond Gradient-descent-based Training},
  author={Ma, Yuheng and Xu, Zhi-Qin John and Zhang, Jiwei},
  journal={arXiv preprint arXiv:2101.00747},
  year={2021}
}
@article{wang2020implicit,
  title={Implicit bias with Ritz-Galerkin method in understanding deep learning for solving PDEs},
  author={Wang, Jihong and Xu, Zhi-Qin John and Zhang, Jiwei and Zhang, Yaoyu},
  journal={arXiv preprint arXiv:2002.07989},
  year={2020}
}
@article{luo2020fourier,
  title={Fourier-domain Variational Formulation and Its Well-posedness for Supervised Learning},
  author={Luo, Tao and Ma, Zheng and Wang, Zhiwei and Xu, Zhi-Qin John and Zhang, Yaoyu},
  journal={arXiv preprint arXiv:2012.03238},
  year={2020}
}
@Article{luo2020theory,
  author  = {Luo, Tao and Ma, Zheng and Xu, Zhi-Qin John and Zhang, Yaoyu},
  journal = {arXiv preprint arXiv:2010.08153},
  title   = {On the exact computation of linear frequency principle dynamics and its generalization},
  year    = {2020},
}
@article{li2020multi,
  title={A Multi-Scale DNN Algorithm for Nonlinear Elliptic Equations with Multiple Scales},
  author={Li, Xi-An and Xu, Zhi-Qin John and Zhang, Lei},
  journal={Communications in Computational Physics},
  volume={28},
  number={5},
  pages={1886--1906},
  year={2020}
}

@inproceedings{xu2020deep,
  title={Deep Frequency Principle Towards Understanding Why Deeper Learning Is Faster},
  author={Xu, Zhi-Qin John and Zhou, Hanxu},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={35},
  number={12},
  year={2021}
}

@article{liu2020multi,
  title={Multi-Scale Deep Neural Network (MscaleDNN) for Solving Poisson-Boltzmann Equation in Complex Domains},
  author={Liu, Ziqi and Cai, Wei and Xu, Zhi-Qin John},
  journal={Communications in Computational Physics},
  volume={28},
  number={5},
  pages={1970--2001},
  year={2020}
}

@article{cai2019multi,
  title={Multi-scale Deep Neural Networks for Solving High Dimensional PDEs},
  author={Cai, Wei and Xu, Zhi-Qin John},
  journal={arXiv preprint arXiv:1910.11710},
  year={2019}
}



@article{zhang2019explicitizing,
  title={Explicitizing an Implicit Bias of the Frequency Principle in Two-layer Neural Networks},
  author={Zhang, Yaoyu and Xu, Zhi-Qin John and Luo, Tao and Ma, Zheng},
  journal={arXiv preprint arXiv:1905.10264},
  year={2019}
}
@inproceedings{zhang2019type,
  title={A type of generalization error induced by initialization in deep neural networks},
  author={Zhang, Yaoyu and Xu, Zhi-Qin John and Luo, Tao and Ma, Zheng},
  booktitle={Mathematical and Scientific Machine Learning},
  pages={144--164},
  year={2020},
  organization={PMLR}
}


@article{xu2018frequency,
  title={Frequency principle in deep learning with general loss functions and its potential application},
  author={Xu, Zhi-Qin John},
  journal={arXiv preprint arXiv:1811.10146},
  year={2018}
}
@article{xu2018understanding,
  title={Understanding training and generalization in deep learning by fourier analysis},
  author={Xu, Zhiqin John},
  journal={arXiv preprint arXiv:1808.04295},
  year={2018}
}
@inproceedings{xu2019training,
  title={Training behavior of deep neural network in frequency domain},
  author={Xu, Zhi-Qin John and Zhang, Yaoyu and Xiao, Yanyang},
  booktitle={International Conference on Neural Information Processing},
  pages={264--274},
  year={2019},
  organization={Springer}
}
@article{xu2020neural,
  title={Neural networks of different species, brain areas and states can be characterized by the probability polling state},
  author={Xu, Zhi-Qin John and Gu, Xiaowei and Li, Chengyu and Cai, David and Zhou, Douglas and McLaughlin, David W},
  journal={European Journal of Neuroscience},
  volume={52},
  number={7},
  pages={3790--3802},
  year={2020},
  publisher={Wiley Online Library}
}
@article{xu2018swift,
  title={Swift Two-sample Test on High-dimensional Neural Spiking Data},
  author={Xu, Zhi-Qin John and Zhou, Douglas and Cai, David},
  journal={arXiv preprint arXiv:1811.12314},
  year={2018}
}

@article{xu2019cautionary,
  title={A cautionary tale of entropic criteria in assessing the validity of the maximum entropy principle},
  author={Xu, Zhi-Qin John and Xu, Fang and Bi, Guoqiang and Zhou, Douglas and Cai, David},
  journal={EPL (Europhysics Letters)},
  volume={126},
  number={3},
  pages={38005},
  year={2019},
  publisher={IOP Publishing}
}
@article{xu2019maximum,
  title={Maximum entropy principle analysis in network systems with short-time recordings},
  author={Xu, Zhi-Qin John and Crodelle, Jennifer and Zhou, Douglas and Cai, David},
  journal={Physical Review E},
  volume={99},
  number={2},
  pages={022409},
  year={2019},
  publisher={APS}
}
@article{xu2019dynamical,
  title={Dynamical and coupling structure of pulse-coupled networks in maximum entropy analysis},
  author={Xu, Zhi-Qin John and Zhou, Douglas and Cai, David},
  journal={Entropy},
  volume={21},
  number={1},
  pages={76},
  year={2019},
  publisher={Multidisciplinary Digital Publishing Institute}
}
@article{xu2017dynamical,
  title={A dynamical state underlying the second order maximum entropy principle in neuronal networks},
  author={Xu, Zhi-Qin John and Bi, Guoqiang and Zhou, Douglas and Cai, David},
  journal={Communications in Mathematical Sciences},
  volume={15},
  number={3},
  pages={665--692},
  year={2017},
  publisher={International Press of Boston}
}
@article{zhou2014granger,
  title={Granger causality network reconstruction of conductance-based integrate-and-fire neuronal systems},
  author={Zhou, Douglas and Xiao, Yanyang and Zhang, Yaoyu and Xu, Zhiqin and Cai, David},
  journal={PloS one},
  volume={9},
  number={2},
  pages={e87636},
  year={2014},
  publisher={Public Library of Science San Francisco, USA}
}
@article{zhou2013causal,
  title={Causal and structural connectivity of pulse-coupled nonlinear networks},
  author={Zhou, Douglas and Xiao, Yanyang and Zhang, Yaoyu and Xu, Zhiqin and Cai, David},
  journal={Physical review letters},
  volume={111},
  number={5},
  pages={054102},
  year={2013},
  publisher={APS}
}
@article{shwartz2017opening,
  title={Opening the black box of deep neural networks via information},
  author={Shwartz-Ziv, Ravid and Tishby, Naftali},
  journal={arXiv preprint arXiv:1703.00810},
  year={2017}
}

@inproceedings{he2015delving,
  title={Delving deep into rectifiers: Surpassing human-level performance on imagenet classification},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={1026--1034},
  year={2015}
}
@inproceedings{kingma2015adam,
  title={Adam: A Method for Stochastic Optimization},
  author={Kingma, Diederik P and Ba, Jimmy},
  booktitle={ICLR (Poster)},
  year={2015}
}

@article{simonyan2014very,
  title={Very deep convolutional networks for large-scale image recognition},
  author={Simonyan, Karen and Zisserman, Andrew},
  journal={arXiv preprint arXiv:1409.1556},
  year={2014}
}

@article{skorokhodov2019loss,
  title={Loss landscape sightseeing with multi-point optimization},
  author={Skorokhodov, Ivan and Burtsev, Mikhail},
  journal={arXiv preprint arXiv:1910.03867},
  year={2019}
}
@article{an1996effects,
  title={The effects of adding noise during backpropagation training on a generalization performance},
  author={An, Guozhong},
  journal={Neural computation},
  volume={8},
  number={3},
  pages={643--674},
  year={1996},
  publisher={MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~â¦}
}



@article{wager2013dropout,
  title={Dropout training as adaptive regularization},
  author={Wager, Stefan and Wang, Sida and Liang, Percy S},
  journal={Advances in neural information processing systems},
  volume={26},
  pages={351--359},
  year={2013}
}

@article{mcallester2013pac,
  title={A PAC-Bayesian tutorial with a dropout bound},
  author={McAllester, David},
  journal={arXiv preprint arXiv:1307.2118},
  year={2013}
}

@inproceedings{wan2013regularization,
  title={Regularization of neural networks using dropconnect},
  author={Wan, Li and Zeiler, Matthew and Zhang, Sixin and Lecun, Yann and Fergus, Rob},
  booktitle={In Proceedings of the International Conference on Machine learning},
  year={2013},
  organization={Citeseer}
}

@inproceedings{mou2018dropout,
  title={Dropout training, data-dependent regularization, and generalization bounds},
  author={Mou, Wenlong and Zhou, Yuchen and Gao, Jun and Wang, Liwei},
  booktitle={International conference on machine learning},
  pages={3645--3653},
  year={2018},
  organization={PMLR}
}



@inproceedings{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  booktitle={Advances in neural information processing systems},
  pages={5998--6008},
  year={2017}
}

@inproceedings{elliott2016multi30k,
  title={Multi30K: Multilingual English-German Image Descriptions},
  author={Elliott, Desmond and Frank, Stella and Sima'an, Khalil and Specia, Lucia},
  booktitle={5th Workshop on Vision and Language},
  pages={70--74},
  year={2016},
  organization={Association for Computational Linguistics (ACL)}
}

@article{krizhevsky2009learning,
  title={Learning multiple layers of features from tiny images},
  author={Krizhevsky, Alex and others},
  year={2009},
  publisher={Citeseer}
}

@article{lecun1998gradient,
  title={Gradient-based learning applied to document recognition},
  author={LeCun, Yann and Bottou, L{\'e}on and Bengio, Yoshua and Haffner, Patrick},
  journal={Proceedings of the IEEE},
  volume={86},
  number={11},
  pages={2278--2324},
  year={1998},
  publisher={Ieee}
}
@article{li2019towards,
  title={Towards explaining the regularization effect of initial large learning rate in training neural networks},
  author={Li, Yuanzhi and Wei, Colin and Ma, Tengyu},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}

@inproceedings{wei2020implicit,
  title={The implicit and explicit regularization effects of dropout},
  author={Wei, Colin and Kakade, Sham and Ma, Tengyu},
  booktitle={International Conference on Machine Learning},
  pages={10181--10192},
  year={2020},
  organization={PMLR}
}

@inproceedings{smith2020origin,
  title={On the Origin of Implicit Regularization in Stochastic Gradient Descent},
  author={Smith, Samuel L and Dherin, Benoit and Barrett, David and De, Soham},
  booktitle={International Conference on Learning Representations},
  year={2020}
}

@inproceedings{barrett2020implicit,
  title={Implicit Gradient Regularization},
  author={Barrett, David and Dherin, Benoit},
  booktitle={International Conference on Learning Representations},
  year={2020}
}

@article{zhang2021variance,
  title={A variance principle explains why dropout finds flatter minima},
  author={Zhang, Zhongwang and Zhou, Hanxu and Xu, Zhi-Qin John},
  journal={arXiv preprint arXiv:2111.01022},
  year={2021}
}

@article{yoshida2017spectral,
  title={Spectral norm regularization for improving the generalizability of deep learning},
  author={Yoshida, Yuichi and Miyato, Takeru},
  journal={arXiv preprint arXiv:1705.10941},
  year={2017}
}

@article{kawaguchi2017generalization,
  title={Generalization in deep learning},
  author={Kawaguchi, Kenji and Kaelbling, Leslie Pack and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1710.05468},
  year={2017}
}

@inproceedings{yang2022generalization,
  title={Generalization and memorization: The bias potential model},
  author={Yang, Hongkang and Weinan, E},
  booktitle={Mathematical and Scientific Machine Learning},
  pages={1013--1043},
  year={2022},
  organization={PMLR}
}
@article{zhang2021understanding,
  title={Understanding deep learning (still) requires rethinking generalization},
  author={Zhang, Chiyuan and Bengio, Samy and Hardt, Moritz and Recht, Benjamin and Vinyals, Oriol},
  journal={Communications of the ACM},
  volume={64},
  number={3},
  pages={107--115},
  year={2021},
  publisher={ACM New York, NY, USA}
}


@inproceedings{tan2019efficientnet,
  title={Efficientnet: Rethinking model scaling for convolutional neural networks},
  author={Tan, Mingxing and Le, Quoc},
  booktitle={International conference on machine learning},
  pages={6105--6114},
  year={2019},
  organization={PMLR}
}

@article{helmbold2015inductive,
  title={On the inductive bias of dropout},
  author={Helmbold, David P and Long, Philip M},
  journal={The Journal of Machine Learning Research},
  volume={16},
  number={1},
  pages={3403--3454},
  year={2015},
  publisher={JMLR. org}
}

@inproceedings{gunasekar2018characterizing,
  title={Characterizing implicit bias in terms of optimization geometry},
  author={Gunasekar, Suriya and Lee, Jason and Soudry, Daniel and Srebro, Nathan},
  booktitle={International Conference on Machine Learning},
  pages={1832--1841},
  year={2018},
  organization={PMLR}
}

@inproceedings{arora2021dropout,
  title={Dropout: Explicit forms and capacity control},
  author={Arora, Raman and Bartlett, Peter and Mianjy, Poorya and Srebro, Nathan},
  booktitle={International Conference on Machine Learning},
  pages={351--361},
  year={2021},
  organization={PMLR}
}

@article{hairer2006geometric,
  title={Geometric numerical integration},
  author={Hairer, Ernst and Hochbruck, Marlis and Iserles, Arieh and Lubich, Christian},
  journal={Oberwolfach Reports},
  volume={3},
  number={1},
  pages={805--882},
  year={2006}
}

@article{papyan2019measurements,
  title={Measurements of three-level hierarchical structure in the outliers in the spectrum of deepnet hessians},
  author={Papyan, Vardan},
  journal={arXiv preprint arXiv:1901.08244},
  year={2019}
}

@article{papyan2018full,
  title={The full spectrum of deepnet hessians at scale: Dynamics with sgd training and sample size},
  author={Papyan, Vardan},
  journal={arXiv preprint arXiv:1811.07062},
  year={2018}
}

@article{zhou2022empirical,
  title={Empirical Phase Diagram for Three-layer Neural Networks with Infinite Width},
  author={Zhou, Hanxu and Zhou, Qixuan and Jin, Zhenyuan and Luo, Tao and Zhang, Yaoyu and Xu, Zhi-Qin John},
  journal={Advances in Neural Information Processing Systems},
  year={2022}
}

@article{zhang2022linear,
  title={Linear Stability Hypothesis and Rank Stratification for Nonlinear Models},
  author={Zhang, Yaoyu and Zhang, Zhongwang and Zhang, Leyang and Bai, Zhiwei and Luo, Tao and Xu, Zhi-Qin John},
  journal={arXiv preprint arXiv:2211.11623},
  year={2022}
}

@inproceedings{blanc2020implicit,
  title={Implicit regularization for deep neural networks driven by an ornstein-uhlenbeck like process},
  author={Blanc, Guy and Gupta, Neha and Valiant, Gregory and Valiant, Paul},
  booktitle={Conference on learning theory},
  pages={483--513},
  year={2020},
  organization={PMLR}
}

@article{mianjy2020convergence,
  title={On convergence and generalization of dropout training},
  author={Mianjy, Poorya and Arora, Raman},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={21151--21161},
  year={2020}
}

@article{bank2020etf,
  title={An ETF view of dropout regularization},
  author={Bank, Dor and Giryes, Raja},
  journal={British Machine Vision Conference},
  year={2020}
}

@inproceedings{mirzadeh2020dropout,
  title={Dropout as an implicit gating mechanism for continual learning},
  author={Mirzadeh, Seyed Iman and Farajtabar, Mehrdad and Ghasemzadeh, Hassan},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops},
  pages={232--233},
  year={2020}
}

@article{blanchet2023dropout,
  title={Dropout training is distributionally robust optimal},
  author={Blanchet, Jos{\'e} and Kang, Yang and Olea, Jos{\'e} Luis Montiel and Nguyen, Viet Anh and Zhang, Xuhui},
  journal={Journal of Machine Learning Research},
  volume={24},
  number={180},
  pages={1--60},
  year={2023}
}

@inproceedings{zhai2018adaptive,
  title={Adaptive dropout with rademacher complexity regularization},
  author={Zhai, Ke and Wang, Huan},
  booktitle={International conference on learning representations},
  year={2018}
}

@article{senen2020almost,
  title={Almost sure convergence of dropout algorithms for neural networks},
  author={Senen-Cerda, Albert and Sanders, Jaron},
  journal={arXiv preprint arXiv:2002.02247},
  year={2020}
}

@article{zhang2023stochastic,
  title={Stochastic Modified Equations and Dynamics of Dropout Algorithm},
  author={Zhang, Zhongwang and Li, Yuqing and Luo, Tao and Xu, Zhi-Qin John},
  journal={arXiv preprint arXiv:2305.15850},
  year={2023}
}

@article{wei2022emergent,
  title={Emergent abilities of large language models},
  author={Wei, Jason and Tay, Yi and Bommasani, Rishi and Raffel, Colin and Zoph, Barret and Borgeaud, Sebastian and Yogatama, Dani and Bosma, Maarten and Zhou, Denny and Metzler, Donald and others},
  journal={arXiv preprint arXiv:2206.07682},
  year={2022}
}

@article{fu2022does,
  title={How does gpt obtain its ability? tracing emergent abilities of language models to their sources},
  author={Fu, Yao and Peng, Hao and Khot, Tushar},
  journal={Yao Fuâs Notion},
  year={2022}
}

@article{srivastava2022beyond,
  title={Beyond the imitation game: Quantifying and extrapolating the capabilities of language models},
  author={Srivastava, Aarohi and Rastogi, Abhinav and Rao, Abhishek and Shoeb, Abu Awal Md and Abid, Abubakar and Fisch, Adam and Brown, Adam R and Santoro, Adam and Gupta, Aditya and Garriga-Alonso, Adri{\`a} and others},
  journal={arXiv preprint arXiv:2206.04615},
  year={2022}
}
@article{hupkes2018learning,
  title={Learning compositionally through attentive guidance},
  author={Hupkes, Dieuwke and Singh, Anand and Korrel, Kris and Kruszewski, German and Bruni, Elia},
  journal={arXiv preprint arXiv:1805.09657},
  year={2018}
}

@article{csordas2021neural,
  title={The neural data router: Adaptive control flow in transformers improves systematic generalization},
  author={Csord{\'a}s, R{\'o}bert and Irie, Kazuki and Schmidhuber, J{\"u}rgen},
  journal={arXiv preprint arXiv:2110.07732},
  year={2021}
}

@article{csordas2022ctl,
  title={CTL++: Evaluating Generalization on Never-Seen Compositional Patterns of Known Functions, and Compatibility of Neural Representations},
  author={Csord{\'a}s, R{\'o}bert and Irie, Kazuki and Schmidhuber, J{\"u}rgen},
  journal={arXiv preprint arXiv:2210.06350},
  year={2022}
}


@article{yun2022vision,
  title={Do Vision-Language Pretrained Models Learn Composable Primitive Concepts?},
  author={Yun, Tian and Bhalla, Usha and Pavlick, Ellie and Sun, Chen},
  journal={arXiv preprint arXiv:2203.17271},
  year={2022}
}

@article{lepori2023break,
  title={Break It Down: Evidence for Structural Compositionality in Neural Networks},
  author={Lepori, Michael A and Serre, Thomas and Pavlick, Ellie},
  journal={arXiv preprint arXiv:2301.10884},
  year={2023}
}

@article{okawa2023compositional,
  title={Compositional Abilities Emerge Multiplicatively: Exploring Diffusion Models on a Synthetic Task},
  author={Okawa, Maya and Lubana, Ekdeep Singh and Dick, Robert P and Tanaka, Hidenori},
  journal={arXiv preprint arXiv:2310.09336},
  year={2023}
}

@article{dziri2024faith,
  title={Faith and fate: Limits of transformers on compositionality},
  author={Dziri, Nouha and Lu, Ximing and Sclar, Melanie and Li, Xiang Lorraine and Jiang, Liwei and Lin, Bill Yuchen and Welleck, Sean and West, Peter and Bhagavatula, Chandra and Le Bras, Ronan and others},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{betz2020critical,
  title={Critical thinking for language models},
  author={Betz, Gregor and Voigt, Christian and Richardson, Kyle},
  journal={arXiv preprint arXiv:2009.07185},
  year={2020}
}

@article{wei2022chain,
  title={Chain of thought prompting elicits reasoning in large language models},
  author={Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Chi, Ed and Le, Quoc and Zhou, Denny},
  journal={arXiv preprint arXiv:2201.11903},
  year={2022}
}

@article{liu2022transformers,
  title={Transformers learn shortcuts to automata},
  author={Liu, Bingbin and Ash, Jordan T and Goel, Surbhi and Krishnamurthy, Akshay and Zhang, Cyril},
  journal={arXiv preprint arXiv:2210.10749},
  year={2022}
}
@article{creswell2022selection,
  title={Selection-inference: Exploiting large language models for interpretable logical reasoning},
  author={Creswell, Antonia and Shanahan, Murray and Higgins, Irina},
  journal={arXiv preprint arXiv:2205.09712},
  year={2022}
}

@article{creswell2022faithful,
  title={Faithful reasoning using large language models},
  author={Creswell, Antonia and Shanahan, Murray},
  journal={arXiv preprint arXiv:2208.14271},
  year={2022}
}

@article{ramesh2023capable,
  title={How capable can a transformer become? a study on synthetic, interpretable tasks},
  author={Ramesh, Rahul and Khona, Mikail and Dick, Robert P and Tanaka, Hidenori and Lubana, Ekdeep Singh},
  journal={arXiv preprint arXiv:2311.12997},
  year={2023}
}

% Encoding: UTF-8
@article{williams_gradient_2019,
  author    = {Francis Williams and
               Matthew Trager and
               Cl{\'{a}}udio T. Silva and
               Daniele Panozzo and
               Denis Zorin and
               Joan Bruna},
  title     = {Gradient Dynamics of Shallow Univariate ReLU Networks},
  journal   = {CoRR},
  volume    = {abs/1906.07842},
  year      = {2019},
  url       = {http://arxiv.org/abs/1906.07842},
  archivePrefix = {arXiv},
  eprint    = {1906.07842},
  timestamp = {Mon, 24 Jun 2019 17:28:45 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1906-07842.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@incollection{aubin_committee_2018,
	title = {The committee machine: {Computational} to statistical gaps in learning a two-layers neural network},   
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 31}, 
	author = {Aubin, Benjamin and Maillard, Antoine and barbier, jean and Krzakala, Florent and Macris, Nicolas and ZdeborovÃ¡, Lenka}, 
	year = {2018},
	pages = {3223--3234}, 
}

@article{jacot2019freeze,
  title={Freeze and chaos for dnns: an NTK view of batch normalization, checkerboard and boundary effects},
  author={Jacot, Arthur and Gabriel, Franck and Hongler, Cl{\'e}ment},
  journal={arXiv preprint arXiv:1907.05715},
  year={2019}
}
@inproceedings{allender1996circuit,
  title={Circuit complexity before the dawn of the new millennium},
  author={Allender, Eric},
  booktitle={International Conference on Foundations of Software Technology and Theoretical Computer Science},
  pages={1--18},
  year={1996},
  organization={Springer}
}
@book{minsky2017perceptrons,
  title={Perceptrons: An introduction to computational geometry},
  author={Minsky, Marvin and Papert, Seymour A},
  year={2017},
  publisher={MIT press}
}
@article{breiman1995reflections,
  title={Reflections after refereeing papers for NIPS},
  author={Breiman, Leo},
  journal={The Mathematics of Generalization},
  pages={11--15},
  volume = {XX},
  year={1995},
  publisher={Addison Wesley Reading, MA}
}

@article{zdeborova2020understanding,
  title={Understanding deep learning is also a job for physicists},
  author={Zdeborov{\'a}, Lenka},
  journal={Nature Physics},
  volume = {16},
  issn = {1745-2481}, 
  doi = {10.1038/s41567-020-0929-2}, 
  pages={1--3},
  year={2020},
  publisher={Nature Publishing Group}
}
@inproceedings{basri2019convergence,
  title={The convergence rate of neural networks for learned functions of different frequencies},
  author={Ronen, Basri and Jacobs, David and Kasten, Yoni and Kritchman, Shira},
  booktitle={Advances in Neural Information Processing Systems},
  pages={4763--4772},
  year={2019}
}
@inproceedings{valle2018deep,
  title={Deep learning generalizes because the parameter-function map is biased towards simple functions},
  author={Valle-Perez, Guillermo and Camargo, Chico Q and Louis, Ard A},
  booktitle={The International Conference on Learning Representations},
  year={2019}
}

@incollection{kalimeris_sgd_2019,
	title = {{SGD} on {Neural} {Networks} {Learns} {Functions} of {Increasing} {Complexity}},  
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 32},
	author = {Kalimeris, Dimitris and Kaplun, Gal and Nakkiran, Preetum and Edelman, Benjamin and Yang, Tristan and Barak, Boaz and Zhang, Haofeng}, 
	year = {2019},
	pages = {3496--3506}, 
}

@inproceedings{arpit2017closer,
  title={A closer look at memorization in deep networks},
  author={Arpit, Devansh and Jastrzbski, Stanislaw and Ballas, Nicolas and Krueger, David and Bengio, Emmanuel and Kanwal, Maxinder S and Maharaj, Tegan and Fischer, Asja and Courville, Aaron and Bengio, Yoshua and others},
  booktitle={Proceedings of the 34th International Conference on Machine Learning-Volume 70},
  pages={233--242},
  year={2017}, 
}
@article{aurisano_convolutional_2016,
	title = {A convolutional neural network neutrino event classifier},
	volume = {11},
	issn = {1748-0221}, 
	doi = {10.1088/1748-0221/11/09/P09001}, 
	number = {09}, 
	journal = {Journal of Instrumentation},
	author = {Aurisano, A. and Radovic, A. and Rocco, D. and Himmel, A. and Messier, M. D. and Niner, E. and Pawloski, G. and Psihas, F. and Sousa, A. and Vahle, P.}, 
	year = {2016}, 
	pages = {P09001--P09001}, 
}

@article{bahri_statistical_2020,
	title = {Statistical {Mechanics} of {Deep} {Learning}},
	volume = {11}, 
	doi = {10.1146/annurev-conmatphys-031119-050745}, 
	number = {1}, 
	journal = {Annual Review of Condensed Matter Physics},
	author = {Bahri, Yasaman and Kadmon, Jonathan and Pennington, Jeffrey and Schoenholz, Sam S. and Sohl-Dickstein, Jascha and Ganguli, Surya},
	year = {2020}, 
	pages = {501--528}, 
}
@article{woodworth2020kernel,
  title={Kernel and rich regimes in overparametrized models},
  author={Woodworth, Blake and Gunasekar, Suriya and Lee, Jason D and Moroshko, Edward and Savarese, Pedro and Golan, Itay and Soudry, Daniel and Srebro, Nathan},
  journal={arXiv preprint arXiv:2002.09277},
  year={2020}
}
@article{biland_frequency-aware_2019,
	title = {Frequency-{Aware} {Reconstruction} of {Fluid} {Simulations} with {Generative} {Networks}}, 
	journal = {arXiv:1912.08776 [physics, stat]},
	author = {Biland, Simon and Azevedo, Vinicius C. and Kim, Byungsoo and Solenthaler, Barbara}, 
	year = {2019},  
}

@article{cai_approximating_2018,
	title = {Approximating quantum many-body wave functions using artificial neural networks},
	volume = {97},
	issn = {2469-9950, 2469-9969},
	url = {https://link.aps.org/doi/10.1103/PhysRevB.97.035116},
	doi = {10.1103/PhysRevB.97.035116},
	language = {en},
	number = {3},
	urldate = {2020-06-22},
	journal = {Physical Review B},
	author = {Cai, Zi and Liu, Jinguo},
	month = jan,
	year = {2018},
	pages = {035116},
}
@article{ma2020quenching,
  title={The Quenching-Activation Behavior of the Gradient Descent Dynamics for Two-layer Neural Network Models},
  author={Ma, Chao and Wu, Lei and E, Weinan},
  journal={arXiv preprint arXiv:2006.14450},
  year={2020}
}

@article{maennel2018gradient,
  title={Gradient descent quantizes relu network features},
  author={Maennel, Hartmut and Bousquet, Olivier and Gelly, Sylvain},
  journal={arXiv preprint arXiv:1803.08367},
  year={2018}
}

@article{zhang2019explicitizing,
  title={Explicitizing an implicit bias of the frequency principle in two-layer neural networks},
  author={Zhang, Yaoyu and Xu, Zhi-Qin John and Luo, Tao and Ma, Zheng},
  journal={arXiv preprint arXiv:1905.10264},
  year={2019}
}
@article{chizat2018note,
  title={A note on lazy training in supervised differentiable programming},
  author={Chizat, Lenaic and Bach, Francis},
  journal={arXiv preprint arXiv:1812.07956},
  volume={1},
  year={2018}
}
@inproceedings{chizat2019lazy,
  title={On lazy training in differentiable programming},
  author={Chizat, Lenaic and Oyallon, Edouard and Bach, Francis},
  booktitle={Advances in Neural Information Processing Systems},
  pages={2937--2947},
  year={2019}
}
@article{e2020comparative,
  title={A comparative analysis of optimization and generalization properties of two-layer neural network and random feature models under gradient descent dynamics.},
  author={E, Weinan and Ma, Chao and Wu, Lei },
  journal={Sci. China Math.}, 
  volume = {63},
  year={2020} 
}


@inproceedings{arora2019exact,
  title={On exact computation with an infinitely wide neural net},
  author={Arora, Sanjeev and Du, Simon S and Hu, Wei and Li, Zhiyuan and Salakhutdinov, Russ R and Wang, Ruosong},
  booktitle={Advances in Neural Information Processing Systems},
  pages={8141--8150},
  year={2019}
}
@article{nye2018efficient,
  title={Are efficient deep representations learnable?},
  author={Nye, Maxwell and Saxe, Andrew},
  journal={arXiv preprint arXiv:1807.06399},
  year={2018}
}

@article{cybenko1989approximation,
  title={Approximation by superpositions of a sigmoidal function},
  author={Cybenko, George},
  journal={Mathematics of control, signals and systems},
  volume={2},
  number={4},
  pages={303--314},
  year={1989},
  publisher={Springer}
}
@article{cai_phase_2019,
	title = {A {Phase} {Shift} {Deep} {Neural} {Network} for {High} {Frequency} {Approximation} and {Wave} {Problems}}, 
	journal = {arXiv:1909.11759 [cs, math]},
	author = {Cai, Wei and Li, Xiaoguang and Liu, Lizuo}, 
	year = {2019}, 
}

@article{cao_towards_2020,
	title = {Towards {Understanding} the {Spectral} {Bias} of {Deep} {Learning}}, 
	journal = {arXiv:1912.01198 [cs, stat]},
	author = {Cao, Yuan and Fang, Zhiying and Wu, Yue and Zhou, Ding-Xuan and Gu, Quanquan}, 
	year = {2020},    
}

@article{carleo_machine_2019,
	title = {Machine learning and the physical sciences},
	volume = {91}, 
	doi = {10.1103/RevModPhys.91.045002}, 
	number = {4}, 
	journal = {Reviews of Modern Physics},
	author = {Carleo, Giuseppe and Cirac, Ignacio and Cranmer, Kyle and Daudet, Laurent and Schuld, Maria and Tishby, Naftali and Vogt-Maranto, Leslie and ZdeborovÃ¡, Lenka}, 
	year = {2019}, 
	pages = {045002}, 
}

@incollection{chizat_global_2018,
	title = {On the {Global} {Convergence} of {Gradient} {Descent} for {Over}-parameterized {Models} using {Optimal} {Transport}}, 
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 31}, 
	author = {Chizat, LÃ©naÃ¯c and Bach, Francis}, 
	year = {2018},
	pages = {3036--3046}, 
}

@inproceedings{choromanska_loss_2015,
  title={The loss surfaces of multilayer networks},
  author={Choromanska, Anna and Henaff, Mikael and Mathieu, Michael and Arous, G{\'e}rard Ben and LeCun, Yann},
  booktitle={Artificial Intelligence and Statistics},
  pages={192--204},
  year={2015}
}
@article{e_priori_2019,
	title = {A {Priori} {Estimates} of the {Population} {Risk} for {Two}-layer {Neural} {Networks}},
	volume = {17}, 
	doi = {10.4310/CMS.2019.v17.n5.a11}, 
	number = {5}, 
	journal = {Communications in Mathematical Sciences},
	author = {E, Weinan and Ma, Chao and Wu, Lei},
	year = {2019},  
	pages = {1407--1425}, 
}

@book{engel_statistical_2001,
	title = {Statistical {Mechanics} of {Learning}}, 
	author = {Engel, A. and Broeck, C. Van den}, 
	year = {2001},
	note = {Google-Books-ID: qVo4IT9ByfQC}, 
}

@inproceedings{eykholt_robust_2018,
	title = {Robust {Physical}-{World} {Attacks} on {Deep} {Learning} {Visual} {Classification}},
	doi = {10.1109/CVPR.2018.00175}, 
	booktitle = {2018 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Eykholt, Kevin and Evtimov, Ivan and Fernandes, Earlence and Li, Bo and Rahmati, Amir and Xiao, Chaowei and Prakash, Atul and Kohno, Tadayoshi and Song, Dawn}, 
	year = {2018}, 
	pages = {1625--1634}
} 

@article{guest_deep_2018,
	title = {Deep {Learning} and {Its} {Application} to {LHC} {Physics}},
	volume = {68}, 
	doi = {10.1146/annurev-nucl-101917-021019}, 
	number = {1}, 
	journal = {Annual Review of Nuclear and Particle Science},
	author = {Guest, Dan and Cranmer, Kyle and Whiteson, Daniel},
	year = {2018},
	pages = {161--181}, 
}

@incollection{jacot_neural_2018,
	title = {Neural {Tangent} {Kernel}: {Convergence} and {Generalization} in {Neural} {Networks}}, 
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 31}, 
	author = {Jacot, Arthur and Gabriel, Franck and Hongler, Clement}, 
	year = {2018},
	pages = {8571--8580}, 
}

@article{jagtap_adaptive_2020,
	title = {Adaptive activation functions accelerate convergence in deep and physics-informed neural networks},
	volume = {404}, 
	doi = {10.1016/j.jcp.2019.109136}, 
	journal = {Journal of Computational Physics},
	author = {Jagtap, Ameya D. and Kawaguchi, Kenji and Karniadakis, George Em}, 
	year = {2020}, 
	pages = {109136}, 
}

@inproceedings{lampinen_analytic_2019,
	title = {An analytic theory of generalization dynamics and transfer learning in deep linear networks}, 
	booktitle = {The International Conference on Learning Representations},
	author = {Lampinen, Andrew K. and Ganguli, Surya}, 
	year = {2019}, 
}

@article{lecun_deep_2015,
	title = {Deep learning},
	volume = {521},  
	doi = {10.1038/nature14539}, 
	number = {7553},
	urldate = {2020-05-09},
	journal = {Nature},
	author = {LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey}, 
	year = {2015}, 
	pages = {436--444}, 
}
@inproceedings{saxe_exact_2013,
	title = {Exact solutions to the nonlinear dynamics of learning in deep linear neural networks}, 
	author = {Saxe, Andrew M. and McClelland, James L. and Ganguli, Surya}, 
	year = {2014}, 
	booktitle = {The International Conference on Learning Representations}
}
@incollection{lee_wide_2019,
	title = {Wide {Neural} {Networks} of {Any} {Depth} {Evolve} as {Linear} {Models} {Under} {Gradient} {Descent}}, 
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 32}, 
	author = {Lee, Jaehoon and Xiao, Lechao and Schoenholz, Samuel and Bahri, Yasaman and Novak, Roman and Sohl-Dickstein, Jascha and Pennington, Jeffrey}, 
	year = {2019},
	pages = {8572--8583}, 
}

@article{levine_quantum_2019,
	title = {Quantum {Entanglement} in {Deep} {Learning} {Architectures}},
	volume = {122}, 
	doi = {10.1103/PhysRevLett.122.065301}, 
	number = {6}, 
	journal = {Physical Review Letters},
	author = {Levine, Yoav and Sharir, Or and Cohen, Nadav and Shashua, Amnon}, 
	year = {2019}, 
	pages = {065301}, 
}

@article{luo_theory_2019,
	title = {Theory of the {Frequency} {Principle} for {General} {Deep} {Neural} {Networks}}, 
	journal = {arXiv:1906.09235 [cs, math, stat]},
	author = {Luo, Tao and Ma, Zheng and Xu, Zhi-Qin John and Zhang, Yaoyu}, 
	year = {2019},  
}

@article{mehta_high-bias_2019, 
	title = {A high-bias, low-variance introduction to {Machine} {Learning} for physicists},
	volume = {810}, 
	doi = {10.1016/j.physrep.2019.03.001}, 
	journal = {Physics Reports},
	author = {Mehta, Pankaj and Bukov, Marin and Wang, Ching-Hao and Day, Alexandre G. R. and Richardson, Clint and Fisher, Charles K. and Schwab, David J.}, 
	year = {2019},
	pages = {1--124}, 
}

@article{mei_mean_2018,
	title = {A mean field view of the landscape of two-layer neural networks},
	volume = {115}, 
	doi = {10.1073/pnas.1806579115}, 
	number = {33}, 
	journal = {Proceedings of the National Academy of Sciences},
	author = {Mei, Song and Montanari, Andrea and Nguyen, Phan-Minh}, 
	year = {2018}, 
	pages = {E7665--E7671}, 
}

@article{rabinowitz_meta-learners_2019,
	title = {Meta-learners' learning dynamics are unlike learners'}, 
	journal = {arXiv:1905.01320 [cs, stat]},
	author = {Rabinowitz, Neil C.}, 
	year = {2019},  
}

@article{radovic_machine_2018,
	title = {Machine learning at the energy and intensity frontiers of particle physics},
	volume = {560}, 
	doi = {10.1038/s41586-018-0361-2}, 
	number = {7716},
	urldate = {2020-05-09},
	journal = {Nature},
	author = {Radovic, Alexander and Williams, Mike and Rousseau, David and Kagan, Michael and Bonacorsi, Daniele and Himmel, Alexander and Aurisano, Adam and Terao, Kazuhiro and Wongjirad, Taritree}, 
	year = {2018}, 
	pages = {41--48}, 
}

@inproceedings{rahaman_spectral_2019,
	title = {On the {Spectral} {Bias} of {Neural} {Networks}}, 
	booktitle = {International {Conference} on {Machine} {Learning}},
	author = {Rahaman, Nasim and Baratin, Aristide and Arpit, Devansh and Draxler, Felix and Lin, Min and Hamprecht, Fred and Bengio, Yoshua and Courville, Aaron}, 
	year = {2019}, 
	pages = {5301--5310}, 
}

@incollection{ronen_convergence_2019,
	title = {The {Convergence} {Rate} of {Neural} {Networks} for {Learned} {Functions} of {Different} {Frequencies}}, 
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 32}, 
	author = {Ronen, Basri and Jacobs, David and Kasten, Yoni and Kritchman, Shira}, 
	year = {2019},
	pages = {4761--4771}, 
}

@incollection{rotskoff_parameters_2018,
	title = {Parameters as interacting particles: long time convergence and asymptotic error scaling of neural networks}, 
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 31}, 
	author = {Rotskoff, Grant and Vanden-Eijnden, Eric}, 
	year = {2018},
	pages = {7146--7155}, 
}

@article{saxe_information_2019,
	title = {On the information bottleneck theory of deep learning},
	volume = {2019}, 
	doi = {10.1088/1742-5468/ab3985}, 
	number = {12}, 
	journal = {Journal of Statistical Mechanics: Theory and Experiment},
	author = {Saxe, Andrew M. and Bansal, Yamini and Dapello, Joel and Advani, Madhu and Kolchinsky, Artemy and Tracey, Brendan D. and Cox, David D.}, 
	year = {2019}, 
	pages = {124020}, 
}

@article{saxe_exact_2014,
	title = {Exact solutions to the nonlinear dynamics of learning in deep linear neural networks}, 
	journal = {arXiv:1312.6120 [cond-mat, q-bio, stat]},
	author = {Saxe, Andrew M. and McClelland, James L. and Ganguli, Surya}, 
	year = {2014},  
}

@article{sirignano_mean_2020,
	title = {Mean field analysis of neural networks: {A} central limit theorem},
	volume = {130}, 
	doi = {10.1016/j.spa.2019.06.003}, 
	number = {3}, 
	journal = {Stochastic Processes and their Applications},
	author = {Sirignano, Justin and Spiliopoulos, Konstantinos}, 
	year = {2020},
	pages = {1820--1852}, 
}

@article{e_machine_2019,
	title = {Machine {Learning} from a {Continuous} {Viewpoint}}, 
	journal = {arXiv:1912.12777 [cs, math, stat]},
	author = {E, Weinan and Ma, Chao and Wu, Lei}, 
	year = {2019},    
}

@inproceedings{xu_training_2019, 
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Training {Behavior} of {Deep} {Neural} {Network} in {Frequency} {Domain}}, 
	doi = {10.1007/978-3-030-36708-4_22}, 
	booktitle = {Neural {Information} {Processing}}, 
	author = {Xu, Zhi-Qin John and Zhang, Yaoyu and Xiao, Yanyang}, 
	year = {2019}, 
	pages = {264--274}, 
}

@article{xu_frequency_2019,
	title = {Frequency {Principle}: {Fourier} {Analysis} {Sheds} {Light} on {Deep} {Neural} {Networks}}, 
	journal = {arXiv:1901.06523 [cs, stat]},
	author = {Xu, Zhi-Qin John and Zhang, Yaoyu and Luo, Tao and Xiao, Yanyang and Ma, Zheng}, 
	year = {2019},  
}

@article{yang_fine-grained_2020,
	title = {A {Fine}-{Grained} {Spectral} {Perspective} on {Neural} {Networks}}, 
	journal = {arXiv:1907.10599 [cs, stat]},
	author = {Yang, Greg and Salman, Hadi},  
	year = {2020},  
}

@inproceedings{zhang_understanding_2017,
	title = {Understanding deep learning requires rethinking generalization}, 
	booktitle = {The International Conference on Learning Representations},
	author = {Zhang, Chiyuan and Bengio, Samy and Hardt, Moritz and Recht, Benjamin and Vinyals, Oriol},
	year = {2017}, 
}

@article{zhang_deep_2018,
	title = {Deep {Potential} {Molecular} {Dynamics}: {A} {Scalable} {Model} with the {Accuracy} of {Quantum} {Mechanics}},
	volume = {120}, 
	doi = {10.1103/PhysRevLett.120.143001}, 
	number = {14}, 
	journal = {Physical Review Letters},
	author = {Zhang, Linfeng and Han, Jiequn and Wang, Han and Car, Roberto and E, Weinan}, 
	year = {2018}, 
	pages = {143001}, 
}

@article{zhang_type_2019,
	title = {A type of generalization error induced by initialization in deep neural networks}, 
	journal = {arXiv:1905.07777 [cs, stat]},
	author = {Zhang, Yaoyu and Xu, Zhi-Qin John and Luo, Tao and Ma, Zheng}, 
	year = {2019},  
}

@article{zhen_nonlinear_2018,
	title = {Nonlinear {Collaborative} {Scheme} for {Deep} {Neural} {Networks}}, 
	journal = {arXiv:1811.01316 [cs, stat]},
	author = {Zhen, Hui-Ling and Lin, Xi and Tang, Alan Z. and Li, Zhenhua and Zhang, Qingfu and Kwong, Sam}, 
	year = {2018},  
}

@article{dyson_meeting_2004,
	title = {A meeting with {Enrico} {Fermi}},
	volume = {427},
	copyright = {2004 Nature Publishing Group},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/427297a},
	doi = {10.1038/427297a},
	abstract = {How one intuitive physicist rescued a team from fruitless research.},
	language = {en},
	number = {6972},
	urldate = {2020-06-15},
	journal = {Nature},
	author = {Dyson, Freeman},
	month = jan,
	year = {2004}, 
	pages = {297--297},
}
@InCollection{lecun2012efficient,
  author    = {LeCun, Yann A and Bottou, L{\'e}on and Orr, Genevieve B and M{\"u}ller, Klaus-Robert},
  title     = {Efficient backprop},
  booktitle = {Neural networks: Tricks of the trade},
  publisher = {Springer},
  year      = {2012},
  pages     = {9--48},
}

@InProceedings{glorot2010understanding,
  author    = {Glorot, Xavier and Bengio, Yoshua},
  title     = {Understanding the difficulty of training deep feedforward neural networks},
  booktitle = {Proceedings of the thirteenth international conference on artificial intelligence and statistics},
  year      = {2010},
  pages     = {249--256},
}


@Book{vershynin2018high,
  title     = {High-dimensional probability: An introduction with applications in data science},
  publisher = {Cambridge university press},
  year      = {2018},
  author    = {Vershynin, Roman},
  volume    = {47},
}

@Comment{jabref-meta: databaseType:bibtex;}


@article{zhang2024implicit,
  title={Implicit regularization of dropout},
  author={Zhang, Zhongwang and Xu, Zhi-Qin John},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  year={2024},
  publisher={IEEE}
}

@article{liu2020understanding,
  title={Understanding the difficulty of training transformers},
  author={Liu, Liyuan and Liu, Xiaodong and Gao, Jianfeng and Chen, Weizhu and Han, Jiawei},
  journal={arXiv preprint arXiv:2004.08249},
  year={2020}
}

@article{zhu2021gradinit,
  title={Gradinit: Learning to initialize neural networks for stable and efficient training},
  author={Zhu, Chen and Ni, Renkun and Xu, Zheng and Kong, Kezhi and Huang, W Ronny and Goldstein, Tom},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={16410--16422},
  year={2021}
}

@inproceedings{trockman2023mimetic,
  title={Mimetic initialization of self-attention layers},
  author={Trockman, Asher and Kolter, J Zico},
  booktitle={International Conference on Machine Learning},
  pages={34456--34468},
  year={2023},
  organization={PMLR}
}

@article{zhang2019improving,
  title={Improving deep transformer with depth-scaled initialization and merged attention},
  author={Zhang, Biao and Titov, Ivan and Sennrich, Rico},
  journal={arXiv preprint arXiv:1908.11365},
  year={2019}
}

@inproceedings{huang2020improving,
  title={Improving transformer optimization through better initialization},
  author={Huang, Xiao Shi and Perez, Felipe and Ba, Jimmy and Volkovs, Maksims},
  booktitle={International Conference on Machine Learning},
  pages={4475--4483},
  year={2020},
  organization={PMLR}
}

@article{wang2024deepnet,
  title={Deepnet: Scaling transformers to 1,000 layers},
  author={Wang, Hongyu and Ma, Shuming and Dong, Li and Huang, Shaohan and Zhang, Dongdong and Wei, Furu},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  year={2024},
  publisher={IEEE}
}


@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}

@article{bubeck2023sparks,
  title={Sparks of artificial general intelligence: Early experiments with gpt-4},
  author={Bubeck, S{\'e}bastien and Chandrasekaran, Varun and Eldan, Ronen and Gehrke, Johannes and Horvitz, Eric and Kamar, Ece and Lee, Peter and Lee, Yin Tat and Li, Yuanzhi and Lundberg, Scott and others},
  journal={arXiv preprint arXiv:2303.12712},
  year={2023}
}

@article{choi2021chatgpt,
  title={ChatGPT goes to law school},
  author={Choi, Jonathan H and Hickman, Kristin E and Monahan, Amy B and Schwarcz, Daniel},
  journal={J. Legal Educ.},
  volume={71},
  pages={387},
  year={2021},
  publisher={HeinOnline}
}

@article{teubner2023welcome,
  title={Welcome to the era of chatgpt et al. the prospects of large language models},
  author={Teubner, Timm and Flath, Christoph M and Weinhardt, Christof and van der Aalst, Wil and Hinz, Oliver},
  journal={Business \& Information Systems Engineering},
  volume={65},
  number={2},
  pages={95--101},
  year={2023},
  publisher={Springer}
}

@article{achiam2023gpt,
  title={Gpt-4 technical report},
  author={Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and others},
  journal={arXiv preprint arXiv:2303.08774},
  year={2023}
}

@book{marcus2003algebraic,
  title={The algebraic mind: Integrating connectionism and cognitive science},
  author={Marcus, Gary F},
  year={2003},
  publisher={MIT press}
}
@article{smolensky2022neurocompositional,
  title={Neurocompositional computing: From the Central Paradox of Cognition to a new generation of AI systems},
  author={Smolensky, Paul and McCoy, Richard and Fernandez, Roland and Goldrick, Matthew and Gao, Jianfeng},
  journal={AI Magazine},
  volume={43},
  number={3},
  pages={308--322},
  year={2022}
}

@article{radford2019language,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  journal={OpenAI blog},
  volume={1},
  number={8},
  pages={9},
  year={2019}
}

@article{power2022grokking,
  title={Grokking: Generalization beyond overfitting on small algorithmic datasets},
  author={Power, Alethea and Burda, Yuri and Edwards, Harri and Babuschkin, Igor and Misra, Vedant},
  journal={arXiv preprint arXiv:2201.02177},
  year={2022}
}

@inproceedings{liu2022omnigrok,
  title={Omnigrok: Grokking beyond algorithmic data},
  author={Liu, Ziming and Michaud, Eric J and Tegmark, Max},
  booktitle={The Eleventh International Conference on Learning Representations},
  year={2022}
}

@article{wang2024grokked,
  title={Grokked Transformers are Implicit Reasoners: A Mechanistic Journey to the Edge of Generalization},
  author={Wang, Boshi and Yue, Xiang and Su, Yu and Sun, Huan},
  journal={arXiv preprint arXiv:2405.15071},
  year={2024}
}

@inproceedings{gopalani2024transformers,
  title={How Do Transformers Fill in the Blanks? A Case Study on Matrix Completion},
  author={Gopalani, Pulkit and Lubana, Ekdeep Singh and Hu, Wei},
  booktitle={ICML 2024 Workshop on Mechanistic Interpretability},
  year={2024}
}

@article{wang2024improving,
  title={Improving Generalization and Convergence by Enhancing Implicit Regularization},
  author={Wang, Mingze and He, Haotian and Wang, Jinbo and Wang, Zilin and Huang, Guanhua and Xiong, Feiyu and Li, Zhiyu and Wu, Lei and others},
  journal={arXiv preprint arXiv:2405.20763},
  year={2024}
}

@article{wang2024understanding,
  title={Understanding the Expressive Power and Mechanisms of Transformer for Sequence Modeling},
  author={Wang, Mingze and others},
  journal={arXiv preprint arXiv:2402.00522},
  year={2024}
}

@article{wang2023label,
  title={Label words are anchors: An information flow perspective for understanding in-context learning},
  author={Wang, Lean and Li, Lei and Dai, Damai and Chen, Deli and Zhou, Hao and Meng, Fandong and Zhou, Jie and Sun, Xu},
  journal={arXiv preprint arXiv:2305.14160},
  year={2023}
}

@article{cao2024graphinsight,
  title={GraphInsight: Unlocking Insights in Large Language Models for Graph Structure Understanding},
  author={Cao, Yukun and Han, Shuo and Gao, Zengyi and Ding, Zezhong and Xie, Xike and Zhou, S Kevin},
  journal={arXiv preprint arXiv:2409.03258},
  year={2024}
}

@inproceedings{lake2018generalization,
  title={Generalization without systematicity: On the compositional skills of sequence-to-sequence recurrent networks},
  author={Lake, Brenden and Baroni, Marco},
  booktitle={International conference on machine learning},
  pages={2873--2882},
  year={2018},
  organization={PMLR}
}

@inproceedings{kim2020cogs,
  title={COGS: A compositional generalization challenge based on semantic interpretation},
  author={Kim, Najoung and Linzen, Tal},
  booktitle={Proceedings of the 2020 conference on empirical methods in natural language processing (emnlp)},
  pages={9087--9105},
  year={2020}
}

@inproceedings{saparovlanguage,
  title={Language Models Are Greedy Reasoners: A Systematic Formal Analysis of Chain-of-Thought},
  author={Saparov, Abulhair and He, He},
  booktitle={The Eleventh International Conference on Learning Representations},
  year={2023}
}

@inproceedings{hucase,
  title={Case-Based or Rule-Based: How Do Transformers Do the Math?},
  author={Hu, Yi and Tang, Xiaojuan and Yang, Haotong and Zhang, Muhan},
  booktitle={Forty-first International Conference on Machine Learning},
  year={2024}
}

@article{zhang2023loss,
  title={Loss Spike in Training Neural Networks},
  author={Zhang, Zhongwang and Xu, Zhi-Qin John},
  journal={arXiv preprint arXiv:2305.12133},
  year={2023}
}

@article{rudelson2007sampling,
  title={Sampling from large matrices: An approach through geometric functional analysis},
  author={Rudelson, Mark and Vershynin, Roman},
  journal={Journal of the ACM (JACM)},
  volume={54},
  number={4},
  pages={21--es},
  year={2007},
  publisher={ACM New York, NY, USA}
}

@article{tropp2015introduction,
  title={An introduction to matrix concentration inequalities},
  author={Tropp, Joel A and others},
  journal={Foundations and Trends{\textregistered} in Machine Learning},
  volume={8},
  number={1-2},
  pages={1--230},
  year={2015},
  publisher={Now Publishers, Inc.}
}

@article{marcus2022very,
  title={A very preliminary analysis of DALL-E 2},
  author={Marcus, Gary and Davis, Ernest and Aaronson, Scott},
  journal={arXiv preprint arXiv:2204.13807},
  year={2022}
}

@article{leivada2023dall,
  title={DALL{\textperiodcentered} E 2 fails to reliably capture common syntactic processes},
  author={Leivada, Evelina and Murphy, Elliot and Marcus, Gary},
  journal={Social Sciences \& Humanities Open},
  volume={8},
  number={1},
  pages={100648},
  year={2023},
  publisher={Elsevier}
}

@article{conwell2022testing,
  title={Testing relational understanding in text-guided image generation},
  author={Conwell, Colin and Ullman, Tomer},
  journal={arXiv preprint arXiv:2208.00005},
  year={2022}
}

@article{gokhale2022benchmarking,
  title={Benchmarking spatial relationships in text-to-image generation},
  author={Gokhale, Tejas and Palangi, Hamid and Nushi, Besmira and Vineet, Vibhav and Horvitz, Eric and Kamar, Ece and Baral, Chitta and Yang, Yezhou},
  journal={arXiv preprint arXiv:2212.10015},
  year={2022}
}

@inproceedings{du2023reduce,
  title={Reduce, reuse, recycle: Compositional generation with energy-based diffusion models and mcmc},
  author={Du, Yilun and Durkan, Conor and Strudel, Robin and Tenenbaum, Joshua B and Dieleman, Sander and Fergus, Rob and Sohl-Dickstein, Jascha and Doucet, Arnaud and Grathwohl, Will Sussman},
  booktitle={International conference on machine learning},
  pages={8489--8510},
  year={2023},
  organization={PMLR}
}

@inproceedings{liu2022compositional,
  title={Compositional visual generation with composable diffusion models},
  author={Liu, Nan and Li, Shuang and Du, Yilun and Torralba, Antonio and Tenenbaum, Joshua B},
  booktitle={European Conference on Computer Vision},
  pages={423--439},
  year={2022},
  organization={Springer}
}

@inproceedings{fengtraining,
  title={Training-Free Structured Diffusion Guidance for Compositional Text-to-Image Synthesis},
  author={Feng, Weixi and He, Xuehai and Fu, Tsu-Jui and Jampani, Varun and Akula, Arjun Reddy and Narayana, Pradyumna and Basu, Sugato and Wang, Xin Eric and Wang, William Yang},
  booktitle={The Eleventh International Conference on Learning Representations}
}

@article{okawa2024compositional,
  title={Compositional abilities emerge multiplicatively: Exploring diffusion models on a synthetic task},
  author={Okawa, Maya and Lubana, Ekdeep S and Dick, Robert and Tanaka, Hidenori},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{liang2024diffusion,
  title={How Diffusion Models Learn to Factorize and Compose},
  author={Liang, Qiyao and Liu, Ziming and Ostrow, Mitchell and Fiete, Ila},
  journal={arXiv preprint arXiv:2408.13256},
  year={2024}
}

@article{yang2024dynamics,
  title={Dynamics of Concept Learning and Compositional Generalization},
  author={Yang, Yongyi and Park, Core Francisco and Lubana, Ekdeep Singh and Okawa, Maya and Hu, Wei and Tanaka, Hidenori},
  journal={arXiv preprint arXiv:2410.08309},
  year={2024}
}

@inproceedings{parkemergence,
  title={Emergence of Hidden Capabilities: Exploring Learning Dynamics in Concept Space},
  author={Park, Core Francisco and Okawa, Maya and Lee, Andrew and Lubana, Ekdeep Singh and Tanaka, Hidenori},
  booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems}
}


@article{fodor1988connectionism,
  title={Connectionism and cognitive architecture: A critical analysis},
  author={Fodor, Jerry A and Pylyshyn, Zenon W},
  journal={Cognition},
  volume={28},
  number={1-2},
  pages={3--71},
  year={1988},
  publisher={Elsevier}
}

@inproceedings{keysers2019measuring,
  title={Measuring Compositional Generalization: A Comprehensive Method on Realistic Data},
  author={Keysers, Daniel and Sch{\"a}rli, Nathanael and Scales, Nathan and Buisman, Hylke and Furrer, Daniel and Kashubin, Sergii and Momchev, Nikola and Sinopalnikov, Danila and Stafiniak, Lukasz and Tihon, Tibor and others},
  booktitle={International Conference on Learning Representations},
  year={2019}
}

@inproceedings{yu2020assessing,
  title={Assessing Phrasal Representation and Composition in Transformers},
  author={Yu, Lang and Ettinger, Allyson},
  booktitle={Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  pages={4896--4907},
  year={2020}
}

@article{hupkes2020compositionality,
  title={Compositionality decomposed: How do neural networks generalise?},
  author={Hupkes, Dieuwke and Dankers, Verna and Mul, Mathijs and Bruni, Elia},
  journal={Journal of Artificial Intelligence Research},
  volume={67},
  pages={757--795},
  year={2020}
}

@inproceedings{press2023measuring,
  title={Measuring and Narrowing the Compositionality Gap in Language Models},
  author={Press, Ofir and Zhang, Muru and Min, Sewon and Schmidt, Ludwig and Smith, Noah A and Lewis, Mike},
  booktitle={Findings of the Association for Computational Linguistics: EMNLP 2023},
  pages={5687--5711},
  year={2023}
}

@misc{cerebras2023slimpajama,
  author = {Soboleva, Daria and Al-Khateeb, Faisal and Myers, Robert and Steeves, Jacob R and Hestness, Joel and Dey, Nolan},
  title = {{SlimPajama: A 627B token cleaned and deduplicated version of RedPajama}},
  month = June,
  year = 2023,
  howpublished = {\url{https://www.cerebras.net/blog/slimpajama-a-627b-token-cleaned-and-deduplicated-version-of-redpajama}},
  url = {https://huggingface.co/datasets/cerebras/SlimPajama-627B},
}

@article{wang2024towards,
  title={Towards understanding how transformer perform multi-step reasoning with matching operation},
  author={Wang, Zhiwei and Wang, Yunji and Zhang, Zhongwang and Zhou, Zhangchen and Jin, Hui and Hu, Tianyang and Sun, Jiacheng and Li, Zhenguo and Zhang, Yaoyu and Xu, Zhi-Qin John},
  journal={arXiv preprint arXiv:2405.15302},
  year={2024}
}

@inproceedings{
zhang2024initialization,
title={Initialization is Critical to Whether Transformers Fit Composite Functions by Reasoning or Memorizing},
author={Zhongwang Zhang and Pengxiao Lin and Zhiwei Wang and Yaoyu Zhang and Zhi-Qin John Xu},
booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},
year={2024}
}

@article{zhang2025complexity,
  title={Complexity Control Facilitates Reasoning-Based Compositional Generalization in Transformers},
  author={Zhang, Zhongwang and Lin, Pengxiao and Wang, Zhiwei and Zhang, Yaoyu and Xu, Zhi-Qin John},
  journal={arXiv preprint arXiv:2501.08537},
  year={2025}
}