[
  {
    "index": 0,
    "papers": [
      {
        "key": "saxton2019analysing",
        "author": "Saxton, David and Grefenstette, Edward and Hill, Felix and Kohli, Pushmeet",
        "title": "Analysing mathematical reasoning abilities of neural models"
      },
      {
        "key": "yu2023metamath",
        "author": "Yu, Longhui and Jiang, Weisen and Shi, Han and Yu, Jincheng and Liu, Zhengying and Zhang, Yu and Kwok, James T and Li, Zhenguo and Weller, Adrian and Liu, Weiyang",
        "title": "Metamath: Bootstrap your own mathematical questions for large language models"
      },
      {
        "key": "meidani2023snip",
        "author": "Meidani, Kazem and Shojaee, Parshin and Reddy, Chandan K and Farimani, Amir Barati",
        "title": "Snip: Bridging mathematical symbolic and numeric realms with unified pre-training"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "tan2024language",
        "author": "Tan, Mingtian and Merrill, Mike A and Gupta, Vinayak and Althoff, Tim and Hartvigsen, Thomas",
        "title": "Are language models actually useful for time series forecasting?"
      },
      {
        "key": "merrill2024language",
        "author": "Merrill, Mike A and Tan, Mingtian and Gupta, Vinayak and Hartvigsen, Tom and Althoff, Tim",
        "title": "Language Models Still Struggle to Zero-shot Reason about Time Series"
      },
      {
        "key": "ma2024survey",
        "author": "Ma, Qianli and Liu, Zhen and Zheng, Zhenjing and Huang, Ziyang and Zhu, Siying and Yu, Zhongzhong and Kwok, James T",
        "title": "A survey on time-series pre-trained models"
      },
      {
        "key": "zhou2023one",
        "author": "Zhou, Tian and Niu, Peisong and Sun, Liang and Jin, Rong and others",
        "title": "One fits all: Power general time series analysis by pretrained lm"
      },
      {
        "key": "liu2024taming",
        "author": "Liu, Peiyuan and Guo, Hang and Dai, Tao and Li, Naiqi and Bao, Jigang and Ren, Xudong and Jiang, Yong and Xia, Shu-Tao",
        "title": "Taming Pre-trained LLMs for Generalised Time Series Forecasting via Cross-modal Knowledge Distillation"
      },
      {
        "key": "jin2023time",
        "author": "Jin, Ming and Wang, Shiyu and Ma, Lintao and Chu, Zhixuan and Zhang, James Y and Shi, Xiaoming and Chen, Pin-Yu and Liang, Yuxuan and Li, Yuan-Fang and Pan, Shirui and others",
        "title": "Time-llm: Time series forecasting by reprogramming large language models"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "mcleish2024benchmarking",
        "author": "McLeish, Sean and Schwarzschild, Avi and Goldstein, Tom",
        "title": "Benchmarking ChatGPT on Algorithmic Reasoning"
      },
      {
        "key": "liu2024llms",
        "author": "Liu, Xiao and Wu, Zirui and Wu, Xueqing and Lu, Pan and Chang, Kai-Wei and Feng, Yansong",
        "title": "Are llms capable of data-based statistical and causal reasoning? benchmarking advanced quantitative reasoning with data"
      },
      {
        "key": "chen2023theoremqa",
        "author": "Chen, Wenhu and Yin, Ming and Ku, Max and Lu, Pan and Wan, Yixin and Ma, Xueguang and Xu, Jianyu and Wang, Xinyi and Xia, Tony",
        "title": "Theoremqa: A theorem-driven question answering dataset"
      },
      {
        "key": "jin2024cladder",
        "author": "Jin, Zhijing and Chen, Yuen and Leeb, Felix and Gresele, Luigi and Kamal, Ojasv and Lyu, Zhiheng and Blin, Kevin and Gonzalez Adauto, Fernando and Kleiman-Weiner, Max and Sachan, Mrinmaya and others",
        "title": "Cladder: A benchmark to assess causal reasoning capabilities of language models"
      },
      {
        "key": "cobbe2021training",
        "author": "Cobbe, Karl and Kosaraju, Vineet and Bavarian, Mohammad and Chen, Mark and Jun, Heewoo and Kaiser, Lukasz and Plappert, Matthias and Tworek, Jerry and Hilton, Jacob and Nakano, Reiichiro and others",
        "title": "Training verifiers to solve math word problems"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "gao2024raw",
        "author": "Gao, Yanjun and Myers, Skatje and Chen, Shan and Dligach, Dmitriy and Miller, Timothy A and Bitterman, Danielle and Churpek, Matthew and Afshar, Majid",
        "title": "When Raw Data Prevails: Are Large Language Model Embeddings Effective in Numerical Data Representation for Medical Machine Learning Applications?"
      },
      {
        "key": "fang2024large",
        "author": "Fang, Xi and Xu, Weijie and Anting Tan, Fiona and Zhang, Jiani and Hu, Ziqing and Qi, Yanjun and Nickleach, Scott and Socolinsky, Diego and Sengamedu, Srinivasan and Faloutsos, Christos",
        "title": "Large language models on tabular data--a survey"
      },
      {
        "key": "sahakyan2021explainable",
        "author": "Sahakyan, Maria and Aung, Zeyar and Rahwan, Talal",
        "title": "Explainable artificial intelligence for tabular data: A survey"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "dziri2024faith",
        "author": "Dziri, Nouha and Lu, Ximing and Sclar, Melanie and Li, Xiang Lorraine and Jiang, Liwei and Lin, Bill Yuchen and Welleck, Sean and West, Peter and Bhagavatula, Chandra and Le Bras, Ronan and others",
        "title": "Faith and fate: Limits of transformers on compositionality"
      },
      {
        "key": "feng2024numerical",
        "author": "Feng, Guhao and Yang, Kai and Gu, Yuntian and Ai, Xinyue and Luo, Shengjie and Sun, Jiacheng and He, Di and Li, Zhenguo and Wang, Liwei",
        "title": "How numerical precision affects mathematical reasoning capabilities of llms"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "nye2021show",
        "author": "Nye, Maxwell and Andreassen, Anders Johan and Gur-Ari, Guy and Michalewski, Henryk and Austin, Jacob and Bieber, David and Dohan, David and Lewkowycz, Aitor and Bosma, Maarten and Luan, David and others",
        "title": "Show your work: Scratchpads for intermediate computation with language models"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "lee2023teaching",
        "author": "Lee, Nayoung and Sreenivasan, Kartik and Lee, Jason D and Lee, Kangwook and Papailiopoulos, Dimitris",
        "title": "Teaching arithmetic to small transformers"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "golkar2023xval",
        "author": "Golkar, Siavash and Pettee, Mariel and Eickenberg, Michael and Bietti, Alberto and Cranmer, Miles and Krawezik, Geraud and Lanusse, Francois and McCabe, Michael and Ohana, Ruben and Parker, Liam and others",
        "title": "xval: A continuous number encoding for large language models"
      },
      {
        "key": "sundararaman2020methods",
        "author": "Sundararaman, Dhanasekar and Si, Shijing and Subramanian, Vivek and Wang, Guoyin and Hazarika, Devamanyu and Carin, Lawrence",
        "title": "Methods for numeracy-preserving word embeddings"
      },
      {
        "key": "jiang2019learning",
        "author": "Jiang, Chengyue and Nian, Zhonglin and Guo, Kaihao and Chu, Shanbo and Zhao, Yinggong and Shen, Libin and Tu, Kewei",
        "title": "Learning numeral embeddings"
      },
      {
        "key": "sivakumar2024leverage",
        "author": "Sivakumar, Jasivan Alex and Moosavi, Nafise Sadat",
        "title": "How to Leverage Digit Embeddings to Represent Numbers?"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "mcleish2024transformers",
        "author": "McLeish, Sean and Bansal, Arpit and Stein, Alex and Jain, Neel and Kirchenbauer, John and Bartoldson, Brian R and Kailkhura, Bhavya and Bhatele, Abhinav and Geiping, Jonas and Schwarzschild, Avi and others",
        "title": "Transformers Can Do Arithmetic with the Right Embeddings"
      },
      {
        "key": "shen2023positional",
        "author": "Shen, Ruoqi and Bubeck, S{\\'e}bastien and Eldan, Ronen and Lee, Yin Tat and Li, Yuanzhi and Zhang, Yi",
        "title": "Positional description matters for transformers arithmetic"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "nogueira2021investigating",
        "author": "Nogueira, Rodrigo and Jiang, Zhiying and Lin, Jimmy",
        "title": "Investigating the limitations of transformers with simple arithmetic tasks"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "zhou2024scaling",
        "author": "Zhou, Zhejian and Wang, Jiayu and Lin, Dahua and Chen, Kai",
        "title": "Scaling Behavior for Large Language Models regarding Numeral Systems: An Example using Pythia"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "thawani2021representing",
        "author": "Thawani, Avijit and Pujara, Jay and Szekely, Pedro A and Ilievski, Filip",
        "title": "Representing numbers in NLP: a survey and a vision"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "jiang2019learning",
        "author": "Jiang, Chengyue and Nian, Zhonglin and Guo, Kaihao and Chu, Shanbo and Zhao, Yinggong and Shen, Libin and Tu, Kewei",
        "title": "Learning numeral embeddings"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "olshausen1997sparse",
        "author": "Olshausen, Bruno A and Field, David J",
        "title": "Sparse coding with an overcomplete basis set: A strategy employed by V1?"
      },
      {
        "key": "olah2020overview",
        "author": "Olah, Chris and Cammarata, Nick and Schubert, Ludwig and Goh, Gabriel and Petrov, Michael and Carter, Shan",
        "title": "An overview of early vision in inceptionv1"
      },
      {
        "key": "fiquet2024polar",
        "author": "Fiquet, Pierre-{\\'E}tienne and Simoncelli, Eero",
        "title": "A polar prediction model for learning to represent visual transformations"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "bai2022improving",
        "author": "Bai, Jiawang and Yuan, Li and Xia, Shu-Tao and Yan, Shuicheng and Li, Zhifeng and Liu, Wei",
        "title": "Improving vision transformers by revisiting high-frequency components"
      },
      {
        "key": "tancik2020fourier",
        "author": "Tancik, Matthew and Srinivasan, Pratul and Mildenhall, Ben and Fridovich-Keil, Sara and Raghavan, Nithin and Singhal, Utkarsh and Ramamoorthi, Ravi and Barron, Jonathan and Ng, Ren",
        "title": "Fourier features let networks learn high frequency functions in low dimensional domains"
      }
    ]
  },
  {
    "index": 15,
    "papers": [
      {
        "key": "tancik2020fourier",
        "author": "Tancik, Matthew and Srinivasan, Pratul and Mildenhall, Ben and Fridovich-Keil, Sara and Raghavan, Nithin and Singhal, Utkarsh and Ramamoorthi, Ravi and Barron, Jonathan and Ng, Ren",
        "title": "Fourier features let networks learn high frequency functions in low dimensional domains"
      },
      {
        "key": "he2024frequency",
        "author": "He, Keji and Si, Chenyang and Lu, Zhihe and Huang, Yan and Wang, Liang and Wang, Xinchao",
        "title": "Frequency-enhanced data augmentation for vision-and-language navigation"
      }
    ]
  },
  {
    "index": 16,
    "papers": [
      {
        "key": "nanda2023progress",
        "author": "Nanda, Neel and Chan, Lawrence and Lieberum, Tom and Smith, Jess and Steinhardt, Jacob",
        "title": "Progress measures for grokking via mechanistic interpretability"
      },
      {
        "key": "gu2024fourier",
        "author": "Gu, Jiuxiang and Li, Chenyang and Liang, Yingyu and Shi, Zhenmei and Song, Zhao and Zhou, Tianyi",
        "title": "Fourier circuits in neural networks: Unlocking the potential of large language models in mathematical reasoning and modular arithmetic"
      }
    ]
  },
  {
    "index": 17,
    "papers": [
      {
        "key": "zhou2024pre",
        "author": "Zhou, Tianyi and Fu, Deqing and Sharan, Vatsal and Jia, Robin",
        "title": "Pre-trained Large Language Models Use Fourier Features to Compute Addition"
      }
    ]
  }
]