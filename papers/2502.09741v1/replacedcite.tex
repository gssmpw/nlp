\section{Related Work}
\paragraph{Arithmetic and Number-Related Tasks in LLMs.}
Addressing number-related tasks through next-token prediction remains a significant challenge, such as arithmetic ____,  time-series prediction ____, quantitative reasoning ____, and handling tabular data ____.
Despite advancements in transformer-based models, LLMs such as GPT-3 and GPT-4, with billions of parameters, struggle to solve simple arithmetic problems involving multi-digit addition and multiplication across multiple forward passes ____, even when using scratchpads ____. 

____ demonstrate that smaller transformer models can successfully handle multiplication when equipped with carefully designed scratchpads.
Other approaches, such as those proposed by ____, introduce number embedding methods to enhance model performance on number-related tasks. However, the range of numbers these methods can accurately represent is typically limited to fewer than five digits.
Additionally, a line of research ____ incorporates the positional information of digits into embeddings or adds it as extra tokens ____. ____ shows that digit-wise tokenization is better than subword tokenization. ____ explores encoding strategies like digit-by-digit, scientific notation, and base-10 formats, while ____ maps numbers to finite ``prototype numerals.'' These methods help the model align digits of equal significance but often require digit-wise tokenization or introduce additional tokens, reducing training and prediction efficiency.
In contrast, the method proposed in this paper precisely encodes all numbers as a single token, eliminating range limitations and avoiding the efficiency drawbacks associated with previous approaches.

 


\paragraph{Fourier Features.}
Fourier features are commonly observed in image models, particularly in the early layers of vision models ____. These features enable the model to detect edges, textures, and other spatial patterns effectively. 
However, Transformers struggle to capture high-frequency components in ____. Augmenting data with high-frequency components or explicitly encoding coordinates using Fourier features has been demonstrated to improve model performance ____. 
In modular addition tasks, studies have revealed that after “grokking”, a one-layer Transformer can learn to solve the task perfectly by leveraging Fourier features ____. Furthermore, ____  demonstrates that LLMs naturally encode numbers using Fourier features during pretraining, leveraging these representations for arithmetic tasks. Building on this insight, we propose encoding numbers precisely through Fourier features by representing residues in various modular groups, enabling algebraic operations to be performed in a component-wise, parallel manner.

\ifdefined\isarxiv

\else
\begin{algorithm*}[!ht]
\caption{Fourier Number Embedding (FoNE) Algorithm}\label{alg:fne_algorithm_fixed}
    \begin{algorithmic}[1]
    \Procedure{\textsc{FourierNumberEmbedding}}{$x \in \R, m \in \Z_{\geq 0}, n \in \Z_{\geq 0}, d \in \Z_{> 0}$} 
    \State{\textbf{Inputs}: Number $x$, integer digit length $m$, decimal digit length $n$, embedding dimension $d$}
    \State Initialize empty embedding vector $\text{FoNE} \gets []$
    \For{$i = -n+1 \to m$} \Comment{Loop over all scales from $10^{-n+1}$ to $10^m$}
        \State $T_i \gets 10^i$ \Comment{Set the period for the current scale}
        \State $\phi(x, T_i) \gets (\cos(\tfrac{2\pi}{T_i} x), \sin(\tfrac{2\pi}{T_i} x))$ \Comment{Compute the circular embedding for scale $T_i$}
        \State Append $\phi(x, T_i)$ to $\text{FoNE}$ \Comment{Add the embedding for this scale to the result}
    \EndFor
    \While{$\text{Length}(\text{FoNE}) < d$} \Comment{Ensure embedding dimension matches the target}
        \State Append $0$ to $\text{FoNE}$ \Comment{Zero-pad}
    \EndWhile
    \State \Return $\text{FoNE}$ 
    \EndProcedure
    \end{algorithmic}
\end{algorithm*}
\fi