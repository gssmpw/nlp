\section{Discussion}\label{sec:discussion}

\textit{\textbf{Q1:}} \textbf{Why do we choose components with periods that are multiples of $10$, i.e., $10, 100, 1000 \cdots$?}


As shown in \citet{zhou2024pre} and Figure \ref{fig:embedding_fourier_models}, pre-trained LLMs trained on diverse datasets and strategies consistently learn nearly identical key frequency components. These components have periods of $10$ and its divisors, such as $2$ and $5$. Since $\bmod 10$ can already represent a single digit, we believe that $\bmod 2$ and $\bmod 5$ contribute to enhancing robustness.
Models trained on real-world text data—where numbers are almost always expressed in decimal—commonly learn frequency components that correspond to \(\bmod 10\).
We could, in principle, choose different bases (such as 2, 16, etc.), and if the training data contained enough examples in those representations, the model might well learn those frequencies too. However, most large language models primarily encounter numbers in base 10. 

\ifdefined\isarxiv
\begin{figure}[t]
  \centering
    \subfigure[Test accuracy of 60-digit addition with FoNE]{
\includegraphics[width=0.46\textwidth]{figures/results/abacus60.png} 
  }
  \subfigure[Impact of combining FoNE with Abacus embedding]{
    \includegraphics[width=0.46\textwidth]{figures/results/lengendiff.png}
  }
  \caption{
(a) Average accuracy of an 8-layer transformer model on 60-digit addition tasks using FoNE for chunked input.
(b) Performance improvements achieved by combining FoNE with the Abacus embedding method across various random seeds. The transformer is trained on addition tasks with up to 10-digits  numbers (represented by the smaller square)  and tested up to 50-digit numbers.} 
    \label{fig:lengen} 
\end{figure}
\else
\begin{figure}[t]
  \centering
    \subfigure[Test accuracy of 60-digit addition with FoNE]{
\includegraphics[width=0.4\textwidth]{figures/results/abacus60.png} 
  }\vspace{-3mm}\\
  \subfigure[Impact of combining FoNE with Abacus embedding]{
    \includegraphics[width=0.4\textwidth]{figures/results/lengendiff.png}
  }
  \vspace{-3mm}
  \caption{
(a) Average accuracy of an 8-layer transformer model on 60-digit addition tasks using FoNE for chunked input.
(b) Performance improvements achieved by combining FoNE with the Abacus embedding method across various random seeds. The transformer is trained on addition tasks with up to 10-digits  numbers (represented by the smaller square)  and tested up to 50-digit numbers.} 
\vspace{-3mm}
    \label{fig:lengen} 
\end{figure}
\fi


\ifdefined\isarxiv
\vspace{3mm}
\fi
\noindent\textit{\textbf{Q2:}} \textbf{How does the FoNE handle numbers with longer digit sequences?}

The maximum digit length that a \texttt{float64} data type can represent is 15 digits. When \( x \) exceeds 15 digits in length, applying \( \text{FoNE}(x) \) directly may result in a loss of precision. To address this, \( x \) can be divided into smaller chunks, and \( \text{FoNE} \) can be applied to each chunk independently. For example, \( x \) can be split into groups of five digits. The FoNE can then be calculated for each chunk, resulting in a representation of length 10 per chunk, as each digit is encoded in two dimensions. These embeddings are subsequently concatenated to obtain the final number embedding for \( x \).

By using this method, as shown in Figure~\ref{fig:lengen}(a), an 8-layer transformer trained on 60-digit addition achieved an average accuracy of 97.42\% across different operand length with just one forward pass. This demonstrates the effectiveness of FoNE in handling long sequences.

\ifdefined\isarxiv
\vspace{3mm}
\fi
\noindent\textit{\textbf{Q3:}} \textbf{Can FoNE combine with other positional embedding?}

Yes, FoNE can be combined with other positional embedding methods. For instance, we integrated FoNE with the Abacus embedding method \cite{mcleish2024transformers}, which operates on digit-wise tokenization. In this setup, the embeddings for each digit (0--9) are replaced with their corresponding Fourier Number Embeddings.

We trained an 8-layer transformer model on integer addition tasks with up to 10 digits and tested it on addition tasks involving up to 50-digit numbers. The results, as illustrated in Figure~\ref{fig:lengen}(b) and Figure~\ref{fig:diff} in Appendix~\ref{app:abacus}, show that incorporating FoNE consistently improves the performance of the Abacus method across various random seeds. This highlights the complementary benefits of combining FoNE with other positional embedding strategies.

\ifdefined\isarxiv
\vspace{3mm}
\fi
\noindent\textit{\textbf{Q4:}} \textbf{Will FoNE affect the semantic meaning of numbers like years?}


As discussed by \citet{meng2022locating}, the semantic meaning or memory of tokens is often inferred from the MLP layers in transformer models. Since LLMs are typically equipped with sufficient capacity, the precise numerical embedding of numbers takes precedence over encoding their semantic meanings directly within the embeddings. Moreover, as noted by \citet{yao2024knowledge}, LLMs are capable of developing specialized circuits to handle different query types. Consequently, FoNE is designed to provide accurate numerical representations while allowing the model's architecture to manage semantic contexts independently. An important future direction is the integration of FoNE with pre-trained LLMs, enabling the efficient adoptation of numerical representations within existing large-scale models. This approach could enhance numerical reasoning capabilities while leveraging the extensive knowledge and contextual understanding embedded in pre-trained LLMs.

