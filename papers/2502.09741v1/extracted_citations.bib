@inproceedings{bai2022improving,
  title={Improving vision transformers by revisiting high-frequency components},
  author={Bai, Jiawang and Yuan, Li and Xia, Shu-Tao and Yan, Shuicheng and Li, Zhifeng and Liu, Wei},
  booktitle={European Conference on Computer Vision},
  pages={1--18},
  year={2022},
  organization={Springer}
}

@inproceedings{chen2023theoremqa,
  title={Theoremqa: A theorem-driven question answering dataset},
  author={Chen, Wenhu and Yin, Ming and Ku, Max and Lu, Pan and Wan, Yixin and Ma, Xueguang and Xu, Jianyu and Wang, Xinyi and Xia, Tony},
  booktitle={Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing},
  pages={7889--7901},
  year={2023}
}

@article{cobbe2021training,
  title={Training verifiers to solve math word problems},
  author={Cobbe, Karl and Kosaraju, Vineet and Bavarian, Mohammad and Chen, Mark and Jun, Heewoo and Kaiser, Lukasz and Plappert, Matthias and Tworek, Jerry and Hilton, Jacob and Nakano, Reiichiro and others},
  journal={arXiv preprint arXiv:2110.14168},
  year={2021}
}

@article{dziri2024faith,
  title={Faith and fate: Limits of transformers on compositionality},
  author={Dziri, Nouha and Lu, Ximing and Sclar, Melanie and Li, Xiang Lorraine and Jiang, Liwei and Lin, Bill Yuchen and Welleck, Sean and West, Peter and Bhagavatula, Chandra and Le Bras, Ronan and others},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{fang2024large,
  title={Large language models on tabular data--a survey},
  author={Fang, Xi and Xu, Weijie and Anting Tan, Fiona and Zhang, Jiani and Hu, Ziqing and Qi, Yanjun and Nickleach, Scott and Socolinsky, Diego and Sengamedu, Srinivasan and Faloutsos, Christos},
  journal={arXiv e-prints},
  pages={arXiv--2402},
  year={2024}
}

@article{feng2024numerical,
  title={How numerical precision affects mathematical reasoning capabilities of llms},
  author={Feng, Guhao and Yang, Kai and Gu, Yuntian and Ai, Xinyue and Luo, Shengjie and Sun, Jiacheng and He, Di and Li, Zhenguo and Wang, Liwei},
  journal={arXiv preprint arXiv:2410.13857},
  year={2024}
}

@article{fiquet2024polar,
  title={A polar prediction model for learning to represent visual transformations},
  author={Fiquet, Pierre-{\'E}tienne and Simoncelli, Eero},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{gao2024raw,
  title={When Raw Data Prevails: Are Large Language Model Embeddings Effective in Numerical Data Representation for Medical Machine Learning Applications?},
  author={Gao, Yanjun and Myers, Skatje and Chen, Shan and Dligach, Dmitriy and Miller, Timothy A and Bitterman, Danielle and Churpek, Matthew and Afshar, Majid},
  journal={arXiv preprint arXiv:2408.11854},
  year={2024}
}

@article{golkar2023xval,
  title={xval: A continuous number encoding for large language models},
  author={Golkar, Siavash and Pettee, Mariel and Eickenberg, Michael and Bietti, Alberto and Cranmer, Miles and Krawezik, Geraud and Lanusse, Francois and McCabe, Michael and Ohana, Ruben and Parker, Liam and others},
  journal={arXiv preprint arXiv:2310.02989},
  year={2023}
}

@article{gu2024fourier,
  title={Fourier circuits in neural networks: Unlocking the potential of large language models in mathematical reasoning and modular arithmetic},
  author={Gu, Jiuxiang and Li, Chenyang and Liang, Yingyu and Shi, Zhenmei and Song, Zhao and Zhou, Tianyi},
  journal={arXiv preprint arXiv:2402.09469},
  year={2024}
}

@article{he2024frequency,
  title={Frequency-enhanced data augmentation for vision-and-language navigation},
  author={He, Keji and Si, Chenyang and Lu, Zhihe and Huang, Yan and Wang, Liang and Wang, Xinchao},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{jiang2019learning,
  title={Learning numeral embeddings},
  author={Jiang, Chengyue and Nian, Zhonglin and Guo, Kaihao and Chu, Shanbo and Zhao, Yinggong and Shen, Libin and Tu, Kewei},
  journal={arXiv preprint arXiv:2001.00003},
  year={2019}
}

@article{jin2023time,
  title={Time-llm: Time series forecasting by reprogramming large language models},
  author={Jin, Ming and Wang, Shiyu and Ma, Lintao and Chu, Zhixuan and Zhang, James Y and Shi, Xiaoming and Chen, Pin-Yu and Liang, Yuxuan and Li, Yuan-Fang and Pan, Shirui and others},
  journal={arXiv preprint arXiv:2310.01728},
  year={2023}
}

@article{jin2024cladder,
  title={Cladder: A benchmark to assess causal reasoning capabilities of language models},
  author={Jin, Zhijing and Chen, Yuen and Leeb, Felix and Gresele, Luigi and Kamal, Ojasv and Lyu, Zhiheng and Blin, Kevin and Gonzalez Adauto, Fernando and Kleiman-Weiner, Max and Sachan, Mrinmaya and others},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{lee2023teaching,
  title={Teaching arithmetic to small transformers},
  author={Lee, Nayoung and Sreenivasan, Kartik and Lee, Jason D and Lee, Kangwook and Papailiopoulos, Dimitris},
  journal={arXiv preprint arXiv:2307.03381},
  year={2023}
}

@article{liu2024llms,
  title={Are llms capable of data-based statistical and causal reasoning? benchmarking advanced quantitative reasoning with data},
  author={Liu, Xiao and Wu, Zirui and Wu, Xueqing and Lu, Pan and Chang, Kai-Wei and Feng, Yansong},
  journal={arXiv preprint arXiv:2402.17644},
  year={2024}
}

@article{liu2024taming,
  title={Taming Pre-trained LLMs for Generalised Time Series Forecasting via Cross-modal Knowledge Distillation},
  author={Liu, Peiyuan and Guo, Hang and Dai, Tao and Li, Naiqi and Bao, Jigang and Ren, Xudong and Jiang, Yong and Xia, Shu-Tao},
  journal={arXiv preprint arXiv:2403.07300},
  year={2024}
}

@article{ma2024survey,
  title={A survey on time-series pre-trained models},
  author={Ma, Qianli and Liu, Zhen and Zheng, Zhenjing and Huang, Ziyang and Zhu, Siying and Yu, Zhongzhong and Kwok, James T},
  journal={IEEE Transactions on Knowledge and Data Engineering},
  year={2024},
  publisher={IEEE}
}

@article{mcleish2024benchmarking,
  title={Benchmarking ChatGPT on Algorithmic Reasoning},
  author={McLeish, Sean and Schwarzschild, Avi and Goldstein, Tom},
  journal={arXiv preprint arXiv:2404.03441},
  year={2024}
}

@article{mcleish2024transformers,
  title={Transformers Can Do Arithmetic with the Right Embeddings},
  author={McLeish, Sean and Bansal, Arpit and Stein, Alex and Jain, Neel and Kirchenbauer, John and Bartoldson, Brian R and Kailkhura, Bhavya and Bhatele, Abhinav and Geiping, Jonas and Schwarzschild, Avi and others},
  journal={arXiv preprint arXiv:2405.17399},
  year={2024}
}

@article{meidani2023snip,
  title={Snip: Bridging mathematical symbolic and numeric realms with unified pre-training},
  author={Meidani, Kazem and Shojaee, Parshin and Reddy, Chandan K and Farimani, Amir Barati},
  journal={arXiv preprint arXiv:2310.02227},
  year={2023}
}

@article{merrill2024language,
  title={Language Models Still Struggle to Zero-shot Reason about Time Series},
  author={Merrill, Mike A and Tan, Mingtian and Gupta, Vinayak and Hartvigsen, Tom and Althoff, Tim},
  journal={arXiv preprint arXiv:2404.11757},
  year={2024}
}

@article{nanda2023progress,
  title={Progress measures for grokking via mechanistic interpretability},
  author={Nanda, Neel and Chan, Lawrence and Lieberum, Tom and Smith, Jess and Steinhardt, Jacob},
  journal={arXiv preprint arXiv:2301.05217},
  year={2023}
}

@article{nogueira2021investigating,
  title={Investigating the limitations of transformers with simple arithmetic tasks},
  author={Nogueira, Rodrigo and Jiang, Zhiying and Lin, Jimmy},
  journal={arXiv preprint arXiv:2102.13019},
  year={2021}
}

@article{nye2021show,
  title={Show your work: Scratchpads for intermediate computation with language models},
  author={Nye, Maxwell and Andreassen, Anders Johan and Gur-Ari, Guy and Michalewski, Henryk and Austin, Jacob and Bieber, David and Dohan, David and Lewkowycz, Aitor and Bosma, Maarten and Luan, David and others},
  journal={arXiv preprint arXiv:2112.00114},
  year={2021}
}

@article{olah2020overview,
  title={An overview of early vision in inceptionv1},
  author={Olah, Chris and Cammarata, Nick and Schubert, Ludwig and Goh, Gabriel and Petrov, Michael and Carter, Shan},
  journal={Distill},
  volume={5},
  number={4},
  pages={e00024--002},
  year={2020}
}

@article{olshausen1997sparse,
  title={Sparse coding with an overcomplete basis set: A strategy employed by V1?},
  author={Olshausen, Bruno A and Field, David J},
  journal={Vision research},
  volume={37},
  number={23},
  pages={3311--3325},
  year={1997},
  publisher={Elsevier}
}

@article{sahakyan2021explainable,
  title={Explainable artificial intelligence for tabular data: A survey},
  author={Sahakyan, Maria and Aung, Zeyar and Rahwan, Talal},
  journal={IEEE access},
  volume={9},
  pages={135392--135422},
  year={2021},
  publisher={IEEE}
}

@article{saxton2019analysing,
  title={Analysing mathematical reasoning abilities of neural models},
  author={Saxton, David and Grefenstette, Edward and Hill, Felix and Kohli, Pushmeet},
  journal={arXiv preprint arXiv:1904.01557},
  year={2019}
}

@article{shen2023positional,
  title={Positional description matters for transformers arithmetic},
  author={Shen, Ruoqi and Bubeck, S{\'e}bastien and Eldan, Ronen and Lee, Yin Tat and Li, Yuanzhi and Zhang, Yi},
  journal={arXiv preprint arXiv:2311.14737},
  year={2023}
}

@article{sivakumar2024leverage,
  title={How to Leverage Digit Embeddings to Represent Numbers?},
  author={Sivakumar, Jasivan Alex and Moosavi, Nafise Sadat},
  journal={arXiv preprint arXiv:2407.00894},
  year={2024}
}

@inproceedings{sundararaman2020methods,
  title={Methods for numeracy-preserving word embeddings},
  author={Sundararaman, Dhanasekar and Si, Shijing and Subramanian, Vivek and Wang, Guoyin and Hazarika, Devamanyu and Carin, Lawrence},
  booktitle={Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  pages={4742--4753},
  year={2020}
}

@article{tan2024language,
  title={Are language models actually useful for time series forecasting?},
  author={Tan, Mingtian and Merrill, Mike A and Gupta, Vinayak and Althoff, Tim and Hartvigsen, Thomas},
  journal={arXiv preprint arXiv:2406.16964},
  year={2024}
}

@article{tancik2020fourier,
  title={Fourier features let networks learn high frequency functions in low dimensional domains},
  author={Tancik, Matthew and Srinivasan, Pratul and Mildenhall, Ben and Fridovich-Keil, Sara and Raghavan, Nithin and Singhal, Utkarsh and Ramamoorthi, Ravi and Barron, Jonathan and Ng, Ren},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={7537--7547},
  year={2020}
}

@article{thawani2021representing,
  title={Representing numbers in NLP: a survey and a vision},
  author={Thawani, Avijit and Pujara, Jay and Szekely, Pedro A and Ilievski, Filip},
  journal={arXiv preprint arXiv:2103.13136},
  year={2021}
}

@article{yu2023metamath,
  title={Metamath: Bootstrap your own mathematical questions for large language models},
  author={Yu, Longhui and Jiang, Weisen and Shi, Han and Yu, Jincheng and Liu, Zhengying and Zhang, Yu and Kwok, James T and Li, Zhenguo and Weller, Adrian and Liu, Weiyang},
  journal={arXiv preprint arXiv:2309.12284},
  year={2023}
}

@article{zhou2023one,
  title={One fits all: Power general time series analysis by pretrained lm},
  author={Zhou, Tian and Niu, Peisong and Sun, Liang and Jin, Rong and others},
  journal={Advances in neural information processing systems},
  volume={36},
  pages={43322--43355},
  year={2023}
}

@article{zhou2024pre,
  title={Pre-trained Large Language Models Use Fourier Features to Compute Addition},
  author={Zhou, Tianyi and Fu, Deqing and Sharan, Vatsal and Jia, Robin},
  journal={arXiv preprint arXiv:2406.03445},
  year={2024}
}

@article{zhou2024scaling,
  title={Scaling Behavior for Large Language Models regarding Numeral Systems: An Example using Pythia},
  author={Zhou, Zhejian and Wang, Jiayu and Lin, Dahua and Chen, Kai},
  journal={arXiv preprint arXiv:2409.17391},
  year={2024}
}

