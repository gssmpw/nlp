\paragraph{Roadmap} 
In Appendix \ref{app:fourier_final_loss_prediction}, we provide the detailed algorithm for computing the final loss and making number predictions.  
In Appendix \ref{app:class}, we present the results of the binary classification task.  
In Appendix \ref{app:pre_proof}, we provide the preliminaries and the missing proofs from the main paper.  
In Appendix \ref{app:moreevidence}, we offer additional evidence of the existence of Fourier features in pre-trained LLMs.  
In Appendix \ref{app:60digit}, we demonstrate the ability of Transformers to solve long-sequence addition using FoNE.  
In Appendix \ref{app:abacus}, we show how FoNE, combined with Abacus embedding, improves the results.  
In Appendix \ref{app:exp}, we provide additional experimental settings that were missing from the main paper.  
In Appendix \ref{sec:gpt2}, we show that our method produces similar results on the GPT-2 Large model.  
In Appendix \ref{app:r2}, we present our method's performance on the $R^2$ metric.  


\section{Fourier Number Final Loss \& Prediction}\label{app:fourier_final_loss_prediction}
In this section, we provide the detail algorithm of how we get the final loss and final prediction as defined in Section \ref{sec:fne}.

\begin{algorithm*}[!htbp]
\caption{Fourier Number Final Loss \& Prediction}
\label{alg:fourier_final_loss_prediction}
\begin{algorithmic}[1]

    %--------------------------------------------------
    % Final Loss
    %--------------------------------------------------
    \Function{FourierNumberFinalLoss}{$h, y, m, n$}
    \Comment{\textit{Compute average loss}}
        \State $\text{totalLoss} \gets 0$
        \State $\mathcal{I} \gets [m+n]$
        \For{$i \in \mathcal{I}$} 
            \State $\text{digitLoss} \gets \Call{FourierNumberLossFunction}{h,\ y,\ i}$
            \State $\text{totalLoss} \gets \text{totalLoss} + \text{digitLoss}$
        \EndFor
        \State $\text{finalLoss} \gets \frac{\text{totalLoss}}{|\mathcal{I}|}$
        \Comment{Average over all digit positions}
        \State \Return $\text{finalLoss}$
    \EndFunction

    %--------------------------------------------------
    % Final Prediction
    %--------------------------------------------------
    \Function{FourierNumberFinalPrediction}{$h, m, n$}
    \Comment{\textit{Compute final prediction}}
        \State $\hat{y} \gets 0$
        \State $\mathcal{I}_\text{frac} \gets [0, \dots, n-1]$ \Comment{Fractional digit indices}
        \State $\mathcal{I}_\text{int} \gets [n, \dots, m+n-1]$ \Comment{Integer digit indices}
        
        \For{$i \in \mathcal{I}_\text{frac}$} 
            \State $\text{logits}_i \gets 
                \bigl[h[2i], h[2i+1]\bigr]
                \cdot
                \bigl[\phi(j,10)\bigr]_{j=0,\dots,9}$
            \State $\hat{y}_i \gets \arg\max_{j \in \{0, \dots, 9\}} \text{logits}_i[j]$
            \State $\hat{y} \gets \hat{y} + \hat{y}_i \cdot 10^{-(n-i)}$ \Comment{Scale fractional part by $10^{-(n-i)}$}
        \EndFor
        
        \For{$j \in \mathcal{I}_\text{int}$}
            \State $\text{logits}_j \gets 
                \bigl[h[2j], h[2j+1]\bigr]
                \cdot
                \bigl[\phi(j,10)\bigr]_{j=0,\dots,9}$
            \State $\hat{y}_j \gets \arg\max_{j \in \{0, \dots, 9\}} \text{logits}_j[j]$
            \State $\hat{y} \gets \hat{y} + \hat{y}_j \cdot 10^{j-n}$ \Comment{Scale integer part by $10^{j}$}
        \EndFor
        
        \State \Return $\hat{y}$
    \EndFunction

\end{algorithmic}
\end{algorithm*}

\newpage
\section{FoNE on Binary Classification Task}\label{app:class}

In this section, we demonstrate that FoNE outperforms other methods on binary classification tasks, benefiting from its precise representation.

\paragraph{Dataset} Each example in the dataset is formatted as \texttt{[num1,num2,num3]}, where the integers \texttt{num1}, \texttt{num2}, and \texttt{num3} are sorted in ascending order (\(num1 \leq num2 \leq num3\)) to ensure uniqueness and eliminate duplicate representations of the same combination. The integers are uniformly sampled from the range \([0, 1000]\). The label for each example is determined by evaluating the linear equation 
\[
a \cdot \texttt{num1} + b \cdot \texttt{num2} + c \cdot \texttt{num3} - d,
\] 
using predefined coefficients  \(a=1.5\), \(b=-2\), \(c=0.5\), and \(d=10\) and \(a=1.5\), \(b=-2\), \(c=0.5\), and \(d=-190\).
If the result is greater than zero, the label is assigned as \texttt{1}; otherwise, it is assigned as \texttt{0}. The dataset is divided into training, validation, and test subsets, as outlined in Table \ref{tab:dataset_sizes}.
 \begin{figure}[!htbp]
\centering
\subfigure[Accuracy vs. Training Data Size]{\includegraphics[width=0.45\textwidth]{figures/results/classdataacc.png}}
\hspace{2mm}
\subfigure[Accuracy vs. Model Size]{\includegraphics[width=0.45\textwidth]{figures/results/classmodelacc.png}}
\caption{
We train Llama-3.2-1B from scratch with random initialization using different number embedding methods on number classification where $d=10$. The test accuracy is compared across varying data sizes and model sizes.
}
\label{fig:classacc}
\end{figure}
 \begin{figure}[!htbp]
\centering
\subfigure[Accuracy vs. Training Data Size]{\includegraphics[width=0.45\textwidth]{figures/results/class2dataacc.png}}
\hspace{2mm}
\subfigure[Accuracy vs. Model Size]{\includegraphics[width=0.45\textwidth]{figures/results/class2modelacc.png}}
\caption{
We train Llama-3.2-1B from scratch with random initialization using different number embedding methods on number classification where $d=-190$. The test accuracy is compared across varying data sizes and model sizes.
}
\label{fig:classacc2}
\end{figure}
\newpage
\section{Preliminaries and Missing Proof}\label{app:pre_proof}
\subsection{Preliminaries}
In this section, we provide the necessary mathematical definitions and concepts used throughout the paper.

\paragraph{Period and Frequency.} 
A function \( f(x) \) is periodic with period \( T > 0 \) if \( f(x + T) = f(x) \) for all \( x \). The period \( T \) represents the smallest positive value for which the function repeats. The frequency \( f \) of a periodic function is the reciprocal of its period, \( f = \frac{1}{T} \), and describes the number of cycles completed in one unit interval. For the sine and cosine functions \( \cos\bigl(\frac{2\pi}{T}x\bigr) \) and \( \sin\bigl(\frac{2\pi}{T}x\bigr) \), the period is \( T \).

\paragraph{Unit Circle.}
The unit circle is the set of points in the plane at a distance of 1 from the origin, given by \( x^2 + y^2 = 1 \). The coordinates of points on the unit circle can be parameterized as \( (\cos\theta, \sin\theta) \), where \( \theta \) is the angle measured counterclockwise from the positive \( x \)-axis. For any angle \( \theta \), \( \cos\theta \) represents the \( x \)-coordinate, and \( \sin\theta \) represents the \( y \)-coordinate.

\paragraph{Two-Argument Inverse Tangent.}
The two-argument inverse tangent function, \( \operatorname{atan2}(y, x) \), determines the angle \( \theta \) (modulo \( 2\pi \)) given the coordinates \( (x, y) = (\cos\theta, \sin\theta) \). Specifically,
\[
\theta = \operatorname{atan2}(y, x),
\]
which resolves the angle \( \theta \) uniquely based on the signs of \( x \) and \( y \).

\paragraph{Modular Arithmetic.}
Modular arithmetic considers equivalence classes of numbers under a modulus \( T > 0 \). For integers \( a \) and \( b \), \( a \equiv b \pmod{T} \) if \( T \mid (a - b) \), meaning \( a \) and \( b \) differ by an integer multiple of \( T \). 

\paragraph{Fourier Representation.}
Periodic functions with period \( T \) can be represented using the fundamental frequencies \( \frac{2\pi}{T} \). For example, the embeddings \( \bigl(\cos\bigl(\frac{2\pi}{T}x\bigr), \sin\bigl(\frac{2\pi}{T}x\bigr)\bigr) \) capture the periodicity of \( x \) modulo \( T \) by mapping it to a unique point on the unit circle.

\subsection{Missing Proof}
In this section, we provide some missing proofs.
\begin{lemma}[Formal version of Lemma \ref{lem:fne_preserve_numeracy:informal}]\label{lem:fne_preserve_numeracy:formal}
    Given the pair $\bigl(\cos(\tfrac{2\pi}{T}x), \sin(\tfrac{2\pi}{T}x)\bigr)$, we can recover $x \bmod T$.
\end{lemma}

\begin{proof}
Let $\theta = \frac{2\pi}{T} \, x$. Then the given pair becomes 
\[
\bigl(\cos(\theta), \sin(\theta)\bigr).
\]
From this pair, one can recover $\theta$ uniquely modulo $2\pi$. Concretely, $\theta$ can be obtained (modulo $2\pi$) using the two-argument inverse tangent function:
\[
\theta \equiv \operatorname{atan2}\bigl(\sin(\theta), \cos(\theta)\bigr) \quad (\bmod \; 2\pi).
\]
Since $\theta = \frac{2\pi}{T} \, x$, we have
\[
x \;=\; \frac{T}{2\pi} \, \theta.
\]
Hence $x$ is determined up to integer multiples of $T$, i.e., $x \bmod T$. 

In other words, if
\[
\bigl(\cos(\tfrac{2\pi}{T}x_1), \sin(\tfrac{2\pi}{T}x_1)\bigr)
\;=\;
\bigl(\cos(\tfrac{2\pi}{T}x_2), \sin(\tfrac{2\pi}{T}x_2)\bigr),
\]
then $\frac{2\pi}{T} x_1 \equiv \frac{2\pi}{T} x_2 \pmod{2\pi}$, which implies $x_1 \equiv x_2 \pmod{T}$. Therefore, from the pair $\bigl(\cos(\tfrac{2\pi}{T}x), \sin(\tfrac{2\pi}{T}x)\bigr)$, we can indeed recover $x \bmod T$.
\end{proof}
\begin{lemma}[Layer-Normalized FoNE Preserves Numeracy]\label{lem:fne_preserve_numeracy_layer_norm}
    Given a number's Layer-Normalized Fourier Number Embedding $\mathrm{LN}(\FoNE(x) + \mathbf{p})$, where $\FoNE(x)$ is the Fourier Number Embedding of $x$ and $\mathbf{p}$ is an orthogonal positional encoding vector, assume the mean of $\FoNE(x) + \mathbf{p}$ is $0$. Let $m$ be the integer digit length of $x$ and $n$ be the decimal digit length of $x$. Then, using Lemma~\ref{lem:fne_preserve_numeracy:informal}, we can recover $x \bmod 10^{i}$ for each integer $i$ in the range $-n+1$ to $m$.
\end{lemma}

\begin{proof}
Assume the mean of $\mathbf{x} = \FoNE(x) + \mathbf{p}$ is $0$, i.e., $\mu = 0$. Under this assumption, LayerNorm simplifies to:
\[
\mathrm{LN}(\mathbf{x}) = \frac{\mathbf{x}}{\sigma},
\]
where $\sigma$ is the standard deviation of $\mathbf{x}$.

Let $\mathbf{u} = \FoNE(x)$ encode the scalar $x$, and let $\mathbf{p}$ be an orthogonal positional encoding vector such that:
\[
\|\mathbf{u}\| = \|\mathbf{p}\| = 1 \quad \text{and} \quad \mathbf{u} \cdot \mathbf{p} = 0.
\]
Then, the input to LayerNorm is:
\[
\mathbf{x} = \mathbf{u} + \mathbf{p}.
\]

The standard deviation $\sigma$ of $\mathbf{x}$ is given by:
\[
\sigma = \sqrt{\frac{1}{d} \sum_{i=1}^d (\mathbf{x}_i - \mu)^2},
\]
where $d$ is the dimensionality of $\mathbf{x}$. Since $\mu = 0$, this simplifies to:
\[
\sigma = \sqrt{\frac{1}{d} \|\mathbf{x}\|^2}.
\]
Substitute $\mathbf{x} = \mathbf{u} + \mathbf{p}$:
\[
\|\mathbf{x}\|^2 = \|\mathbf{u} + \mathbf{p}\|^2 = \|\mathbf{u}\|^2 + \|\mathbf{p}\|^2 + 2\mathbf{u} \cdot \mathbf{p}.
\]
By orthogonality and unit norm, $\mathbf{u} \cdot \mathbf{p} = 0$, $\|\mathbf{u}\|^2 = 1$, and $\|\mathbf{p}\|^2 = 1$. Thus:
\[
\|\mathbf{x}\|^2 = 1 + 1 + 0 = 2.
\]
Therefore:
\[
\sigma = \sqrt{\frac{1}{d} \cdot 2} = \sqrt{\frac{2}{d}}.
\]

The LayerNorm operation simplifies to:
\[
\mathrm{LN}(\mathbf{x}) = \frac{\mathbf{x}}{\sigma} = \frac{\mathbf{u} + \mathbf{p}}{\sqrt{\frac{2}{d}}} = \sqrt{\frac{d}{2}} (\mathbf{u} + \mathbf{p}).
\]
This rescales $\mathbf{u}$ and $\mathbf{p}$ by a factor of $\sqrt{\frac{d}{2}}$.

The key observation is that LayerNorm applies a \textbf{uniform scaling} to all components of $\mathbf{x}$. Since $\mathbf{u}$ and $\mathbf{p}$ are orthogonal and their relative directions are preserved, the numerical relationships encoded in $\mathbf{u}$ (which represent $x$) are preserved up to a scaling factor. 

By Lemma~\ref{lem:fne_preserve_numeracy:informal}, the numeracy of $x$ is preserved. This means we can recover $x \bmod 10^i$ for all $i$ in the range $-n+1 \leq i \leq m$, as the normalized embedding retains the necessary information about $x$.

\end{proof}
The same result holds for RMSNorm because it also applies a uniform scaling (based on the root mean square of the input) while preserving the relative directions of the embedding components, thus maintaining the numeracy of $x$.
\newpage
\section{More evidence about Fourier features}
\label{app:moreevidence}
\subsection{Emergence of Fourier Features during Pre-training}
We follow \citet{zhou2024pre} and conduct the same Fourier analysis on Pythia model. In Figure \ref{fig:pythia_number_checkpoints}, we show how Pythia gradually learns the Fourier features during pre-training. With different model size, the model gradually learn the same frequency components.
\begin{figure*}[htbp]
  \centering
  \includegraphics[width=1.0\textwidth]{figures/pretrained_embeddings/pythia_number_embedding_checkpoints.png}
  \caption{Fourier analysis of the Pythia model's number embeddings across pre-training checkpoints. The figure illustrates how the Fourier features are progressively learned during pre-training, showing the emergence of specific frequency components. Models of varying sizes exhibit a similar trend, gradually learning the same frequency components over time.}
  \label{fig:pythia_number_checkpoints}
\end{figure*}

We extend the work of \citet{zhou2024pre} to other pre-trained LLMs and observe similar findings: pre-trained LLMs, regardless of the dataset used, tend to learn the same outlier frequency components.
\begin{figure}[!htbp]
\centering
\begin{tabular}{cc}
\subfigure[pre-trained Pythia]{\includegraphics[width=0.45\textwidth]{figures/pretrained_embeddings/pythia_number_embedding_last.png}} &
\subfigure[fine-tuned Llama3.2]{\includegraphics[width=0.45\textwidth]{figures/pretrained_embeddings/llama3.2.png}} \\
\subfigure[pre-trained OPT]{\includegraphics[width=0.45\textwidth]{figures/pretrained_embeddings/opt-1.3b.png}} &
\subfigure[pre-trained GPT2]{\includegraphics[width=0.45\textwidth]{figures/pretrained_embeddings/gpt2-large.png}}
\end{tabular}
\caption{Number embedding in Fourier space for different pre-trained models.}
\label{fig:embedding_fourier_models}
\end{figure}


% \subsection{Different Key Frequency Components }

% If we train one-layer transformer from scratch on $\bmod$ 113 task. We find that the model internally develop Fourier Features to solve modular addition. The components it use are different for different random seeds. However, for pre-trained LLMs the number embedding always have almost the same outliner components with period $2,2.5,5,10$.

%  \begin{figure*}[!htbp]
% \centering
% \subfigure[Number Embedding in Fourier Space]{\includegraphics[width=0.48\textwidth]{figures/modaddition/113-1.png}}
% \hspace{2mm}
% \subfigure[Training Process]{\includegraphics[width=0.48\textwidth]{figures/modaddition/113-3.png}}\\
% \subfigure[Number Embedding in Fourier Space]{\includegraphics[width=0.48\textwidth]{figures/modaddition/113-5.png}}
% \hspace{2mm}
% \subfigure[Training Process]{\includegraphics[width=0.48\textwidth]{figures/modaddition/113-7.png}}
% \caption{
% 1
% }
% \label{fig:113-1}
% \end{figure*}

%  \begin{figure*}[!htbp]
% \centering
% \subfigure[Number Embedding in Fourier Space]{\includegraphics[width=0.48\textwidth]{figures/modaddition/113-2.png}}
% \hspace{2mm}
% \subfigure[Number Embedding in Fourier Space]{\includegraphics[width=0.48\textwidth]{figures/modaddition/113-4.png}}\\
% \subfigure[Number Embedding in Fourier Space]{\includegraphics[width=0.48\textwidth]{figures/modaddition/113-6.png}}
% \hspace{2mm}
% \subfigure[Number Embedding in Fourier Space]{\includegraphics[width=0.48\textwidth]{figures/modaddition/113-8.png}}
% \caption{
% ???
% }
% \label{fig:113-2}
% \end{figure*}



\newpage


\newpage
\section{FoNE for 60-digit Integer Addition in One Forward Pass}\label{app:60digit}
 \begin{figure*}[!htbp]
    \centering
    \includegraphics[width=0.5\linewidth]{figures/results/6060addition.png}
    \caption{ Accuracy of an 8-layer transformer on 60-digit addition tasks, illustrating the effectiveness of FoNE embeddings in handling long sequences. The model achieves an average accuracy of 97.42\% across different operand lengths, showcasing its capability in numerical precision and sequence representation.}
    \label{fig:6060addition}
\end{figure*}

As discussed in Section \ref{sec:discussion}, the maximum digit length that a \texttt{float64} data type can precisely represent is 15 digits. Consequently, even if we convert numbers to \texttt{float64} and then back to \texttt{float16} to match the model weight precisionm it still introduce numerical inaccuracies when the input \( x \) exceeds 15 digits. To mitigate this issue, we process \( x \) by dividing it into smaller chunks, allowing the FoNE to operate effectively without precision loss.

Specifically, \( x \) is split into groups of five digits, and \( \text{FoNE} \) is applied independently to each chunk. Each digit within a chunk is encoded into two dimensions, resulting in an embedding of length 10 per chunk. These chunk embeddings are then concatenated to form the final representation of \( x \). This method ensures that even for long inputs, the FoNE still preserve the numeracy of the numbers.

We adopt the data generation approach from \cite{mcleish2024transformers}, which includes all combinations of operand lengths \((i, j)\) up to a maximum length \(k\), generating 20 million stratified samples to ensure balanced representation across all length pairs. 
Training is conducted using a language model cramming approach \citep{geiping2023cramming}, constrained to 8 exaFLOP (equivalent to 24 hours of training on a single Nvidia RTX A6000 GPU). 
Using this strategy, as depicted in Figure~\ref{fig:lengen}(a), an 8-layer transformer trained on 60-digit addition achieves an average accuracy of 97.42\% across various operand lengths in just one forward pass. This result underscores the effectiveness of the \( \text{FoNE} \) in processing long numbers with high precision and computational efficiency in just one forward pass.

\section{Combine FoNE with Abacus}\label{app:abacus}

We train decoder-only causal language models to solve arithmetic problems, following the setup described in \citet{mcleish2024transformers}. Inputs are formatted in a least-significant-digit-first order (e.g., \(98282 + 3859172 = 2787472\)), without padding between digits or operands. The training dataset includes all combinations of operand lengths \((i, j)\) up to a maximum length \(k\), with 20 million stratified samples ensuring balanced representation across all length pairs.

For input representation, we combine Fourier Number Embeddings (FoNE) with the Abacus method \cite{mcleish2024transformers}. That each digit is embedded with FoNE. Training is conducted using a language model cramming approach \citep{geiping2023cramming}, constrained to 8 exaFLOP (equivalent to 24 hours of training on a single Nvidia RTX A6000 GPU). 

We train and evaluate the models across three runs, each with a different random seed, as shown in Figure \ref{fig:diff}. Results indicate that incorporating FoNE enables the Abacus method to achieve better generalization and higher accuracy.

\begin{figure}
    \centering
    \includegraphics[width=0.8\linewidth]{figures/results/diff1.png}
    \caption{Heatmaps of accuracy percentages for ``FoNE+Abacus" (left column) and ``Abacus" (right column) across three different random seeds. Each heatmap represents accuracy as a function of the first and second number lengths, with lighter blue shades indicating higher accuracy. The color scale ranges from white (low accuracy) to blue (high accuracy). These visualizations highlight FoNE can combine with Abacus to improve performance.}
    \label{fig:diff}
\end{figure}




\newpage
\section{Experiment Setting}\label{app:exp}
In this section, we provide the experiments settings that we used in the Section \ref{sec:exp_setting}.

Learning rates were determined through an extensive search, with the best rates selected separately for each method based on validation performance. Final training hyperparameters include a learning rate of \(0.005\) for regular and FoNE methods, and \(0.0001\) for the xVal method, a batch size of \(512\), and \(100\) epochs.


\begin{table}[ht]
\centering
\begin{tabular}{|c|c|c|c|}
\hline
\textbf{Dataset} & \textbf{Train Size} & \textbf{Validation Size} & \textbf{Test Size} \\ \hline
6-digit decimal addition & 720,000 & 80,000 & 200,000 \\ \hline
6-digit integer addition &  720,000 & 80,000 & 200,000 \\ \hline
5-digit integer subtract &  720,000 & 80,000 & 200,000 \\ \hline
3-digit integer multiplication &  360,000 &  40,000 & 100,000 \\ \hline
4-digit integer multiplication &  720,000 & 80,000 & 200,000\\ \hline
classification &  720,00 & 80,00 & 200,00\\ \hline
\end{tabular}
\caption{Dataset Sizes for Training, Testing, and Validation}
\label{tab:dataset_sizes}
\end{table}


\begin{table}[ht]
\centering
\small
\begin{tabular}{|c|c|c|}
\hline
\textbf{Dataset} & \textbf{Model Size for Varying Data Size} & \textbf{Data Size for Varying Model Size} \\ \hline
6-digit decimal addition & 37.55M & 200,000 \\ \hline
6-digit integer addition &  37.55M & 200,000\\ \hline
5-digit integer subtract &  37.55M & 200,000 \\ \hline
3-digit integer multiplication &  37.55M &  360,000  \\ \hline
4-digit integer multiplication &  37.55M & 360,000\\ \hline
4-digit integer multiplication &  37.55M & 360,000\\ \hline
classification &  37.55M & 50,000\\ \hline
\end{tabular}
\caption{Dataset and Configuration Sizes for Model and Data Variation Experiments}
\label{tab:varysizes}
\end{table}



\begin{table}[ht]
\centering
\footnotesize
\begin{tabular}{|c|c|c|c|c|c|}
\hline
\textbf{Model} & \textbf{Hidden Size} & \textbf{Intermediate Size} & \textbf{\# Hidden Layers} & \textbf{\# Attention Heads} & \textbf{\# Key-Value Heads} \\ \hline
1 & 64  & 256  & 1 & 4  & 2 \\ \hline
2 & 128 & 512  & 2 & 4  & 2 \\ \hline
3 & 192 & 768  & 3 & 6  & 3 \\ \hline
4 & 256 & 1024 & 4 & 8  & 4 \\ \hline
5 & 320 & 1280 & 5 & 8  & 4 \\ \hline
6 & 384 & 1536 & 6 & 8  & 4 \\ \hline
\end{tabular}
\caption{Model Configuration Table}
\label{tab:model_config}
\end{table}

\subsection{Ablation Study}
In this section, we present the mispredictions of the model trained with an FoNE, where the periods are multiples of $5$ instead of $10$. Table \ref{tab:misprediction} demonstrates that, for each digit, the mispredictions consistently deviate from the true labels by $5$.


\begin{table}[ht]
\centering
\caption{Mispredictions in the Final Evaluation with when we embed each digit with only $\bmod 5$.}
\label{tab:misprediction}
\begin{tabular}{|c|c|c|}
\hline
\textbf{Index} & \textbf{Predicted Value} & \textbf{Actual Value} \\ \hline
1 & 934.03 & 934.585 \\ \hline
2 & 3.009 & 558.509 \\ \hline
3 & 912.311 & 917.366 \\ \hline
4 & 6201.003 & 1756.008 \\ \hline
5 & 1240.34 & 1290.84 \\ \hline
\end{tabular}
\end{table}

We also present the model's mispredictions in Table \ref{tab:misprediction2}, where each digit is encoded into a separate dimension of the embedding. For example, the number \(567\) is represented as \([5,6,7]\). During training, we compute the RMSE loss between the last hidden states and the labels. During prediction, we interpret each entry in the last hidden state as a single digit.
\begin{table}[ht]
\centering
\caption{Mispredictions in the Final Evaluation when directly encoding numbers into their embeddings.}
\label{tab:misprediction2}
\begin{tabular}{|c|c|c|}
\hline
\textbf{Index} & \textbf{Predicted Value} & \textbf{Actual Value} \\ \hline
1  & 883.888 & 993.999  \\ \hline
2  & 787.878  & 898.989  \\ \hline
3  & 888.758 & 989.759  \\ \hline
4  & 748.785 & 849.895  \\ \hline
5  & 677.677  & 688.788  \\ \hline
10 & 1179.488 & 1189.499 \\ \hline
\end{tabular}
\end{table}

\newpage

\section{Similar Results on GPT2-Large Based Experiments}\label{sec:gpt2}
We conduct the same experiments on decimal addition using a GPT-2 Large-based model. The results indicate that changing the model architecture does not affect the outcomes. For instance, GPT-2 Large employs LayerNorm, while Llama 3.2 uses RMSNorm.


 \begin{figure*}[!htbp]
\centering
\subfigure[ 6-digit decimal addition: Accuracy vs. Training Data Size]{\includegraphics[width=0.45\textwidth]{figures/results/add2datagpt.png}}
\hspace{2mm}
\subfigure[6-digit decimal addition: Accuracy vs. Model Size]{\includegraphics[width=0.45\textwidth]{figures/results/add2modelgpt.png}}
\caption{
We train GPT2-Large from scratch with random initialization using different number embedding methods on 6-digit decimal addition. The test accuracy is compared across varying data sizes and model sizes.
}
\label{fig:gpt}
\end{figure*}

\newpage
\section{$R^2$ comparison}\label{app:r2}

xVal \cite{golkar2023xval} performs well on the $R^2$ metric 
\begin{align*}
R^2 = 1 - \frac{\sum_{i=1}^n (y_i - \hat{y}_i)^2}{\sum_{i=1}^n (y_i - \bar{y})^2},
\end{align*}
because it uses RMSE as its loss function. However, we demonstrate that FoNE outperforms xVal on $R^2$ in most tasks. We show the final $R^2$ on test dataset in our experiments(Section \ref{sec:exp_results}).

 \begin{figure*}[!htbp]
\centering
\subfigure[Data size vs. Accuracy]{\includegraphics[width=0.45\textwidth]{figures/results/adddecimaldatar2.png}}
\hspace{2mm}
\subfigure[Model size vs. Accuracy]{\includegraphics[width=0.45\textwidth]{figures/results/adddecimalmodelr2.png}}
\caption{
Comparison of $R^2$ trends for 6-digit decimal addition with respect to model size and data size.
}
\label{fig:r2decimal}
\end{figure*}


\begin{figure*}[!htbp]
  \centering
  \subfigure[6-digit integer addition: Model\&Data size vs. Accuracy]{
    \begin{minipage}{0.48\textwidth}
      \centering
      \includegraphics[width=0.49\textwidth]{figures/results/add2datar2.png}
    \hspace{-2mm}
      \includegraphics[width=0.49\textwidth]{figures/results/add2modelr2.png}
    \end{minipage}
    \label{fig:decimal_addition_r2}
  }
  \hspace{-2mm}
    \subfigure[5-digit integer addition: Model\&Data size vs. Accuracy]{
    \begin{minipage}{0.48\textwidth}
      \centering
      \includegraphics[width=0.49\textwidth]{figures/results/adddatar2.png}
      \hspace{-2mm}
      \includegraphics[width=0.49\textwidth]{figures/results/addmodelr2.png}
    \end{minipage}
    \label{fig:addition_r2}
  }
  \hspace{-2mm}\\
  \subfigure[5-digit integer subtraction: Model\&Data size vs. Accuracy]{
    \begin{minipage}{0.48\textwidth}
      \centering
      \includegraphics[width=0.49\textwidth]{figures/results/subdatar2.png}
      \hspace{-2mm}
      \includegraphics[width=0.49\textwidth]{figures/results/submodelr2.png}
    \end{minipage}
    \label{fig:subtraction_r2}
  }
  \hspace{-2mm}
  \subfigure[3-digit integer multiplication: Model\&Data size vs. Accuracy]{
    \begin{minipage}{0.48\textwidth}
      \centering
      \includegraphics[width=0.49\textwidth]{figures/results/muldatar2.png}
      \hspace{-2mm}
      \includegraphics[width=0.49\textwidth]{figures/results/mulmodelr2.png}
    \end{minipage}
    \label{fig:multiplication_r2}
  }
  \caption{Comparison of $R^2$ trends for various arithmetic tasks with respect to model size and data size.}
  \label{fig:result_r2} 
\end{figure*}


