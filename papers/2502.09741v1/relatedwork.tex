\section{Related Work}
\paragraph{Arithmetic and Number-Related Tasks in LLMs.}
Addressing number-related tasks through next-token prediction remains a significant challenge, such as arithmetic \cite{saxton2019analysing, yu2023metamath, meidani2023snip},  time-series prediction \cite{tan2024language,merrill2024language,ma2024survey, zhou2023one,liu2024taming,jin2023time}, quantitative reasoning \cite{mcleish2024benchmarking,liu2024llms,chen2023theoremqa,jin2024cladder,cobbe2021training}, and handling tabular data \cite{gao2024raw,fang2024large,sahakyan2021explainable}.
Despite advancements in transformer-based models, LLMs such as GPT-3 and GPT-4, with billions of parameters, struggle to solve simple arithmetic problems involving multi-digit addition and multiplication across multiple forward passes \cite{dziri2024faith, feng2024numerical}, even when using scratchpads \cite{nye2021show}. 

\citet{lee2023teaching} demonstrate that smaller transformer models can successfully handle multiplication when equipped with carefully designed scratchpads.
Other approaches, such as those proposed by \citet{golkar2023xval, sundararaman2020methods, jiang2019learning,sivakumar2024leverage}, introduce number embedding methods to enhance model performance on number-related tasks. However, the range of numbers these methods can accurately represent is typically limited to fewer than five digits.
Additionally, a line of research \cite{mcleish2024transformers, shen2023positional} incorporates the positional information of digits into embeddings or adds it as extra tokens \cite{nogueira2021investigating}. \citet{zhou2024scaling} shows that digit-wise tokenization is better than subword tokenization. \citet{thawani2021representing} explores encoding strategies like digit-by-digit, scientific notation, and base-10 formats, while \citet{jiang2019learning} maps numbers to finite ``prototype numerals.'' These methods help the model align digits of equal significance but often require digit-wise tokenization or introduce additional tokens, reducing training and prediction efficiency.
In contrast, the method proposed in this paper precisely encodes all numbers as a single token, eliminating range limitations and avoiding the efficiency drawbacks associated with previous approaches.

 


\paragraph{Fourier Features.}
Fourier features are commonly observed in image models, particularly in the early layers of vision models \cite{olshausen1997sparse,olah2020overview,fiquet2024polar}. These features enable the model to detect edges, textures, and other spatial patterns effectively. 
However, Transformers struggle to capture high-frequency components in \cite{bai2022improving,tancik2020fourier}. Augmenting data with high-frequency components or explicitly encoding coordinates using Fourier features has been demonstrated to improve model performance \cite{tancik2020fourier,he2024frequency}. 
In modular addition tasks, studies have revealed that after “grokking”, a one-layer Transformer can learn to solve the task perfectly by leveraging Fourier features \cite{nanda2023progress,gu2024fourier}. Furthermore, \citet{zhou2024pre}  demonstrates that LLMs naturally encode numbers using Fourier features during pretraining, leveraging these representations for arithmetic tasks. Building on this insight, we propose encoding numbers precisely through Fourier features by representing residues in various modular groups, enabling algebraic operations to be performed in a component-wise, parallel manner.

\ifdefined\isarxiv

\else
\begin{algorithm*}[!ht]
\caption{Fourier Number Embedding (FoNE) Algorithm}\label{alg:fne_algorithm_fixed}
    \begin{algorithmic}[1]
    \Procedure{\textsc{FourierNumberEmbedding}}{$x \in \R, m \in \Z_{\geq 0}, n \in \Z_{\geq 0}, d \in \Z_{> 0}$} 
    \State{\textbf{Inputs}: Number $x$, integer digit length $m$, decimal digit length $n$, embedding dimension $d$}
    \State Initialize empty embedding vector $\text{FoNE} \gets []$
    \For{$i = -n+1 \to m$} \Comment{Loop over all scales from $10^{-n+1}$ to $10^m$}
        \State $T_i \gets 10^i$ \Comment{Set the period for the current scale}
        \State $\phi(x, T_i) \gets (\cos(\tfrac{2\pi}{T_i} x), \sin(\tfrac{2\pi}{T_i} x))$ \Comment{Compute the circular embedding for scale $T_i$}
        \State Append $\phi(x, T_i)$ to $\text{FoNE}$ \Comment{Add the embedding for this scale to the result}
    \EndFor
    \While{$\text{Length}(\text{FoNE}) < d$} \Comment{Ensure embedding dimension matches the target}
        \State Append $0$ to $\text{FoNE}$ \Comment{Zero-pad}
    \EndWhile
    \State \Return $\text{FoNE}$ 
    \EndProcedure
    \end{algorithmic}
\end{algorithm*}
\fi