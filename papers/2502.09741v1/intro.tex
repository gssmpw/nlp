\section{Introduction}

\ifdefined\isarxiv
\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.99\linewidth]{figures/teaserarxiv.pdf}
    \caption{(a) We extract all the numbers from the input sequence.
     (b) For each number, we use FoNE to directly map the number to its embedding. The first two entries in the embedding represent \( 18 \bmod 10 \), while the next two entries represent \( 18 \bmod 100 \).
     (c) We pad the FoNE with zeros, add it to the word embeddings, and then feed the combined embeddings into the model.
     (d) For each digit, we take every two entries from the last hidden state and find the number whose representation is closest to these two entries.}
    \label{fig:teaser1}
\end{figure}
\else
\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.96\linewidth]{figures/teaser1.pdf}
    \caption{(a) We extract all the numbers from the input sequence.
     (b) For each number, we use FoNE to directly map the number to its embedding. The first two entries in the embedding represent \( 18 \bmod 10 \), while the next two entries represent \( 18 \bmod 100 \).
     (c) We pad the FoNE with zeros, add it to the word embeddings, and then feed the combined embeddings into the model.
     (d) For each digit, we take every two entries from the last hidden state and find the number whose representation is closest to these two entries.}
    \label{fig:teaser1}
    \vspace{-6mm}
\end{figure}
\fi


Large language models (LLMs) require precise representations of numerical data to perform number-related tasks effectively. 
However, since LLMs treat numbers just like any other token, embeddings of numerical tokens do not systematically capture important numerical features. As a result, it is challenging for even billion-parameter models to achieve perfect accuracy in solving simple arithmetic tasks \cite{saxton2019analysing,dziri2024faith,lee2023teaching,shen2023positional,zhou2023algorithms}.

While generating code can be a useful workaround, relying solely on this capability highlights a fundamental limitation: without a proper understanding of numbers, the model cannot fully grasp concepts critical to domains like mathematical theorems, physics laws, or quantitative reasoning. 
Even with approaches like Chain-of-Thought (CoT) prompting, it is important to have a perfect accuracy in solving basic arithmetic tasks to build a strong foundation for more complex reasoning. 

Standard tokenization approaches, such as subword tokenization (e.g., GPT-4o \cite{achiam2023gpt}, Llama-3 \cite{dubey2024llama}, Phi-2 \cite{abdin2024phi}) or digit-wise tokenization (e.g., Llama-2 \cite{touvron2023llama}, Mistral \cite{jiang2023mistral}), force the model to aggregate multiple tokens to understand numbers and introduces inefficiencies by tokenizing one number to multiple tokens. However, this inefficiency in tokenizing numbers leads to larger challenges when it comes to their representation. Numbers, unlike words, require systematic, frequency-agnostic representations, yet LLMs often exhibit a frequency bias in arithmetic tasks \cite{razeghi2022impact}, predicting numbers based on training data prevalence rather than adhering to their mathematical properties.



We draw inspiration from interpretability analyses of LLMs, which reveal that  models internally develop Fourier-like features. Specifically, pre-trained models embed number tokens using a sparse set of features in the Fourier domain \cite{zhou2024pre}.
These features enable the representation of numbers capturing both the magnitude and exact values of numbers, which are critical for solving arithmetic tasks \cite{zhou2024pre}. However, due to limitations in sub-word tokenization, current LLMs struggle to extend this mechanism to larger numbers, highlighting the need for more systematic approaches for numerical representation.

In this paper, we propose a novel approach called Fourier Number Embedding (FoNE), which directly maps numbers into their Fourier representations, bypassing the tokenization step entirely. By representing each digit using \textit{cosine and sine functions with different periods}, as shown in Figure~\ref{fig:teaser1}(b), FoNE ensures precise representation of numbers. This approach encodes the modular relationship of each digit through periodic functions, enabling recovery of \( x \bmod 10^i \) for all digits \( i \). Note that, for FoNE, each digit is represented using only two dimensions in the embedding vector. This compact design not only reduces computational overhead but also creates opportunities for future extensions by incorporating additional features to better capture numeric properties. By embedding and predicting numbers directly as single tokens, our method eliminates the need for multiple forward passes and token aggregation, significantly enhancing computational efficiency. Furthermore, we provide a theoretical justification for why FoNE can represent numbers accurately as single tokens, leveraging the modular encoding properties of trigonometric functions to ensure exact recovery of each digit through periodic embeddings.

Beyond theoretical justifications, we demonstrate the effectiveness of FoNE through extensive experiments on a range of arithmetic tasks, including addition, subtraction, and multiplication. Our results show that FoNE is the only approach that 
can achieve perfect accuracy on multiple arithmetic tasks while requiring significantly less training data and fewer model parameters compared to existing methods. Moreover, FoNE offers faster training and inference times by encoding each number into a single token, thereby reducing the computational overhead. These findings underscore FoNEâ€™s capacity to represent and manipulate numerical data both efficiently and precisely within large language models.





