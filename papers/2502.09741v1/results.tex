\section{Empirical Evaluation}

\ifdefined\isarxiv
\else
 \begin{figure*}[!ht]
\centering
\subfigure[ 6-digit decimal addition: Accuracy vs. Training Data Size]{\includegraphics[width=0.45\textwidth]{figures/results/adddecimaldataacc.png}}
\hspace{2mm}
\subfigure[ 6-digit decimal addition: Accuracy vs. Model Size]{\includegraphics[width=0.45\textwidth]{figures/results/adddecimalmodelacc.png}}
\caption{
We train Llama-3.2-1B from scratch with random initialization using different number embedding methods on 6-digit decimal addition. The test accuracy is compared across varying data sizes and model sizes.
}
\label{fig:6digitdecimalacc}
\end{figure*}

\begin{figure*}[!ht]
  \centering
  \subfigure[6-digit integer addition: Model\&Data size vs. Accuracy]{
    \begin{minipage}{0.48\textwidth}
      \centering
      \includegraphics[width=0.49\textwidth]{figures/results/add1dataacc_small.png}
      \hspace{-2mm}
      \includegraphics[width=0.49\textwidth]{figures/results/add1modelacc_small.png}
    \end{minipage}
    \label{fig:addition}
  }
  \hspace{-2mm}
  \subfigure[5-digit integer subtraction: Model\&Data size vs. Accuracy]{
    \begin{minipage}{0.48\textwidth}
      \centering
      \includegraphics[width=0.49\textwidth]{figures/results/sub1dataacc_small.png}
      \hspace{-2mm}
      \includegraphics[width=0.49\textwidth]{figures/results/sub1modelacc_small.png}
    \end{minipage}
    \label{fig:decimal_addition}
  }
  \hspace{-2mm}
  \subfigure[3-digit integer multiplication: Model\&Data size vs. Accuracy]{
    \begin{minipage}{0.48\textwidth}
      \centering
      \includegraphics[width=0.49\textwidth]{figures/results/mul1dataacc_small.png}
      \hspace{-2mm}
      \includegraphics[width=0.49\textwidth]{figures/results/mul1modelacc_small.png}
    \end{minipage}
    \label{fig:subtraction}
  }
  \hspace{-2mm}
  \subfigure[4-digit integer multiplication: Model\&Data size vs. Accuracy]{
    \begin{minipage}{0.48\textwidth}
      \centering
      \includegraphics[width=0.49\textwidth]{figures/results/mul2dataacc_small.png}
      \hspace{-2mm}
      \includegraphics[width=0.49\textwidth]{figures/results/mul2modelacc_small.png}
    \end{minipage}
    \label{fig:multiplication}
  }
  \caption{Comparison of accuracy trends for various arithmetic tasks with respect to model size and data size.}
  \label{fig:result} 
\end{figure*}
\fi


\ifdefined\isarxiv
\else
\begin{table*}[h!]
\centering
\setlength{\tabcolsep}{3pt}  % Reduce horizontal spacing between columns
\caption{Training and inference efficiency comparison across three arithmetic tasks. The times are reported in minutes (\('\)) and seconds (\(''\)).}
\begin{tabular}{lcccc cccc cccc}
\toprule
 & \multicolumn{4}{c}{\textbf{Decimal Addition}} & \multicolumn{4}{c}{\textbf{Subtraction}} & \multicolumn{4}{c}{\textbf{Multiplication}} \\
\cmidrule(lr){2-5}\cmidrule(lr){6-9}\cmidrule(lr){10-13}
\textbf{Method} & \textbf{Train Time} & \textbf{Test Time} & \textbf{Tokens} & \textbf{Accuracy} & \textbf{Train.} & \textbf{Test.} & \textbf{Toks.} & \textbf{Acc.} & \textbf{Train.} & \textbf{Test.} & \textbf{Toks.} & \textbf{Acc.} \\
\midrule
Ours       & 3\(^{\prime}\)18\(^{\prime\prime}\) & 29\(^{\prime\prime}\) & 1 & 100 &
             2\(^{\prime}\)42\(^{\prime\prime}\) & 29\(^{\prime\prime}\) & 1 &  100 &
             2\(^{\prime}\)56\(^{\prime\prime}\) & 33\(^{\prime\prime}\) & 1 &  98.56   \\
Digit-wise & 11\(^{\prime}\)48\(^{\prime\prime}\) & 1\(^{\prime}\)25\(^{\prime\prime}\) & 7 & 99.85 &
             9\(^{\prime}\)41\(^{\prime\prime}\) & 1\(^{\prime}\)15\(^{\prime\prime}\) & 5 & 99.71 &
             10\(^{\prime}\)11\(^{\prime\prime}\) & 1\(^{\prime}\)18\(^{\prime\prime}\) & 8 &   81.21 \\
Subword    & 6\(^{\prime}\)46\(^{\prime\prime}\) & 58\(^{\prime\prime}\) & 3 & 97.94 &
             5\(^{\prime}\)47\(^{\prime\prime}\) & 54\(^{\prime\prime}\) & 2 & 91.66 &
             6\(^{\prime}\)20\(^{\prime\prime}\) & 58\(^{\prime\prime}\) & 3 & 8.05  \\
\textsc{xVal}       & 3\(^{\prime}\)17\(^{\prime\prime}\) & 27\(^{\prime\prime}\) & 1 & 0.44 &
             2\(^{\prime}\)54\(^{\prime\prime}\) & 27\(^{\prime\prime}\) & 1 & 3.41 &
             2\(^{\prime}\)56\(^{\prime\prime}\) & 26\(^{\prime\prime}\) & 1 & 0  \\
\bottomrule
\end{tabular}

\label{tab:time_comparison}
\end{table*}

\fi




\subsection{Experiment Setting}
\label{sec:exp_setting}

We evaluate the performance of our proposed \textit{Fourier Number Embedding (FoNE)} method on arithmetic tasks designed to benchmark different number embedding methods. The dataset includes tasks such as 6-digit integer addition, 6-digit decimal addition (with 3 digits after the decimal), 5-digit integer subtraction, 3-digit integer multiplication, and 4-digit integer multiplication. These tasks are curated to measure model capabilities in accurate numeric computation. All experiments were conducted using an NVIDIA A6000.

\paragraph{Dataset.}
Each example in the dataset is formatted as \texttt{[operand a][operator][operand b]=}, where the operands \texttt{a} and \texttt{b} are sampled based on the operation type. For addition and multiplication, we ensure \(a \leq b\) to avoid duplication (e.g., \(a + b\) and \(b + a\) are treated as identical and included only once). For subtraction, we enforce \(a \geq b\) to ensure non-negative results. 
For an \(x\)-digit operands dataset, each operand can have up to \(x\) digits. The dataset is divided into training, validation, and test subsets as shown in Table \ref{tab:dataset_sizes} in Appendix \ref{app:exp}.


\paragraph{Baselines.} We compare our proposed FoNE method against several baseline methods for numeric embeddings. First, we consider digit-wise tokenization, where each digit in a number is treated as an individual token. Second, we evaluate subword tokenization, where numeric values are tokenized into subword units based on the Llama3.2-1b tokenizer's default vocabulary. Third, we include the \textsc{xVal} method \cite{golkar2023xval}, which leverages explicit value-based representations for numeric computation. As \textsc{xVal} predict floating point numbers, predictions are rounded to calculate accuracy. Finally, we fine-tune pre-trained LLMs on the same dataset for comparison.

\paragraph{Setup.} Our experiments involve multiple configurations of transformer-based models. Specifically, we use \texttt{Llama-3.2-1B-Instruct} as the base model. Models were evaluated across varying sizes, ranging from small to large architectures as defined in Table \ref{tab:model_config}. In Appendix \ref{sec:gpt2}, we conduct experiments on \texttt{GPT-2-Large} and have the same results.

Learning rates were determined through an extensive search, with the best rates selected separately for each method based on the validation performance. 
Model evaluation used exact match accuracy to assess numeric prediction correctness. All models were trained from random initialization. We varied the training data size by uniformly sampling subsets and adjusted model sizes to compare accuracy across methods.

\subsection{Experiment Results}
\label{sec:exp_results}
\ifdefined\isarxiv
 \begin{figure*}[!ht]
\centering
\subfigure[ 6-digit decimal addition: Acc. vs. Training Data Size]{\includegraphics[width=0.49\textwidth]{figures/results/adddecimaldataacc.png}}
\subfigure[ 6-digit decimal addition: Acc. vs. Model Size]{\includegraphics[width=0.49\textwidth]{figures/results/adddecimalmodelacc.png}}
\caption{
We train Llama-3.2-1B from scratch with random initialization using different number embedding methods on 6-digit decimal addition. The test accuracy is compared across varying data sizes and model sizes.
}
\label{fig:6digitdecimalacc}
\end{figure*}

\begin{figure*}[!ht]
  \centering
  \subfigure[6-digit integer addition: Model\&Data size vs. Acc.]{
    \begin{minipage}{0.48\textwidth}
      \centering
      \includegraphics[width=0.49\textwidth]{figures/results/add1dataacc_small.png}
      \hspace{-2mm}
      \includegraphics[width=0.49\textwidth]{figures/results/add1modelacc_small.png}
    \end{minipage}
    \label{fig:addition}
  } 
  \hspace{-1mm}
  \subfigure[5-digit integer subtraction: Model\&Data size vs. Acc.]{
    \begin{minipage}{0.48\textwidth}
      \centering
      \includegraphics[width=0.49\textwidth]{figures/results/sub1dataacc_small.png}
      \hspace{-2mm}
      \includegraphics[width=0.49\textwidth]{figures/results/sub1modelacc_small.png}
    \end{minipage}
    \label{fig:decimal_addition}
  }
  \hspace{-1mm}
  \subfigure[3-digit integer multiplication: Model\&Data size vs. Acc.]{
    \begin{minipage}{0.48\textwidth}
      \centering
      \includegraphics[width=0.49\textwidth]{figures/results/mul1dataacc_small.png}
      \hspace{-2mm}
      \includegraphics[width=0.49\textwidth]{figures/results/mul1modelacc_small.png}
    \end{minipage}
    \label{fig:subtraction}
  }
  \hspace{-1mm}
  \subfigure[4-digit integer multiplication: Model\&Data size vs. Acc.]{
    \begin{minipage}{0.48\textwidth}
      \centering
      \includegraphics[width=0.49\textwidth]{figures/results/mul2dataacc_small.png}
      \hspace{-2mm}
      \includegraphics[width=0.49\textwidth]{figures/results/mul2modelacc_small.png}
    \end{minipage}
    \label{fig:multiplication}
  }
  \caption{Comparison of accuracy trends for various arithmetic tasks with respect to model size and data size.}
  \label{fig:result} 
\end{figure*}

\else
\fi

\paragraph{Data Efficiency.}
Figure~\ref{fig:6digitdecimalacc}(a) illustrates the accuracy trends of different embedding methods as the data size increases for the 6-digit decimal addition task. 
Remarkably, our model achieves \(99\%\) accuracy with just \(6,400\) training samples and $37.55$ million parameters, requiring \(64\times\) less training data than traditional embedding methods (\(409,600 / 6,400 = 64\)). Even with only \(3,200\) training samples, our method outperforms the fine-tuned Llama-3.2 model. Additionally, it achieves perfect accuracy with \(51,200\) training samples.

Beyond synthetic tasks, our approach also improves data efficiency in real-world scenarios. For instance, FoNE requires only $149.25$ tokens on average to represent numerical values from a table in the WikiTableQuestions dataset \citep{pasupat-liang-2015-compositional}, compared to $329.7$ tokens used by a digit-wise tokenizer. This significant reduction in token usage highlights the efficiency of our method in encoding numerical data, making it more scalable for practical applications.

\paragraph{Parameter Efficiency.}
Figure~\ref{fig:6digitdecimalacc}(b) shows the accuracy trends of different embedding methods as the model size increases for the 6-digit decimal addition task. Our method achieves \(97\%\) accuracy with just \(1\) layer and \(8.31\) million parameters using $200k$ examples for training. Furthermore, with \(26.62\) million parameters, it surpasses the fine-tuned Llama-3.2 model and achieves \(100\%\) accuracy.

\paragraph{Different Tasks.} We conducted the same experiments across all different datasets. As shown in Figure~\ref{fig:result}, our method consistently demonstrates superior data and parameter efficiency compared to other approaches. Notably, it is the only method that achieves perfect accuracy on 6-digit decimal addition, 6-digit integer addition, 5-digit subtraction, and 3-digit multiplication. We also show that our method performs better in a binary classification task that involves numerical values. Specifically, the task requires predicting a label based on a linear equation applied to three integers. Due to space limitations, we defer the details to Appendix~\ref{app:class}.


\paragraph{Training and Inference Efficiency.}
Table \ref{tab:time_comparison} compares the training and test times used for one epoch across different embedding methods. Our method is consistently faster than digit-wise and subword embedding methods, as it uses one token to embed each number. Compared with \textsc{xVal}, our method consistently achieves higher accuracy. Additionally, we show the number of tokens required to tokenize the maximum number for different methods, highlighting the efficiency of our approach.


\ifdefined\isarxiv
\begin{table*}[h!]
\centering
\small
\setlength{\tabcolsep}{2pt}  % Reduce horizontal spacing between columns
\caption{Training and inference efficiency comparison across three arithmetic tasks. The times are reported in minutes (\('\)) and seconds (\(''\)).}
\begin{tabular}{lcccc cccc cccc}
\toprule
 & \multicolumn{4}{c}{\textbf{Decimal Addition}} & \multicolumn{4}{c}{\textbf{Subtraction}} & \multicolumn{4}{c}{\textbf{Multiplication}} \\
\cmidrule(lr){2-5}\cmidrule(lr){6-9}\cmidrule(lr){10-13}
\textbf{Method} & \textbf{Train Time} & \textbf{Test Time} & \textbf{Tokens} & \textbf{Accuracy} & \textbf{Train.} & \textbf{Test.} & \textbf{Toks.} & \textbf{Acc.} & \textbf{Train.} & \textbf{Test.} & \textbf{Toks.} & \textbf{Acc.} \\
\midrule
Ours       & 3\(^{\prime}\)18\(^{\prime\prime}\) & 29\(^{\prime\prime}\) & 1 & 100 &
             2\(^{\prime}\)42\(^{\prime\prime}\) & 29\(^{\prime\prime}\) & 1 &  100 &
             2\(^{\prime}\)56\(^{\prime\prime}\) & 33\(^{\prime\prime}\) & 1 &  98.56   \\
Digit-wise & 11\(^{\prime}\)48\(^{\prime\prime}\) & 1\(^{\prime}\)25\(^{\prime\prime}\) & 7 & 99.85 &
             9\(^{\prime}\)41\(^{\prime\prime}\) & 1\(^{\prime}\)15\(^{\prime\prime}\) & 5 & 99.71 &
             10\(^{\prime}\)11\(^{\prime\prime}\) & 1\(^{\prime}\)18\(^{\prime\prime}\) & 8 &   81.21 \\
Subword    & 6\(^{\prime}\)46\(^{\prime\prime}\) & 58\(^{\prime\prime}\) & 3 & 97.94 &
             5\(^{\prime}\)47\(^{\prime\prime}\) & 54\(^{\prime\prime}\) & 2 & 91.66 &
             6\(^{\prime}\)20\(^{\prime\prime}\) & 58\(^{\prime\prime}\) & 3 & 8.05  \\
\textsc{xVal}       & 3\(^{\prime}\)17\(^{\prime\prime}\) & 27\(^{\prime\prime}\) & 1 & 0.44 &
             2\(^{\prime}\)54\(^{\prime\prime}\) & 27\(^{\prime\prime}\) & 1 & 3.41 &
             2\(^{\prime}\)56\(^{\prime\prime}\) & 26\(^{\prime\prime}\) & 1 & 0  \\
\bottomrule
\end{tabular}

\label{tab:time_comparison}
\end{table*}

\else
\fi
\subsection{Ablation Studies}\label{sec:ablation}

\paragraph{Linear Layer after FoNE.}
As discussed in Section \ref{sec:fne}, we evaluate the use of a linear layer applied after FoNE and compare it with the approach of appending zeros to align the embedding dimensions with the model's input requirements. As shown in Table \ref{tab:ablation_linear}, both configurations achieve almost the same accuracy. Hence, either way can be used to align FoNE with the embedding dimension.

\ifdefined\isarxiv
\begin{table}[!ht]
\centering
\setlength{\tabcolsep}{4pt} % Reduce column spacing
\caption{Accuracy Comparison Across Datasets}
\label{tab:ablation_linear}
\begin{tabular}{lcc}
\toprule
\textbf{Task} & \textbf{Linear Layer} & \textbf{Zero Padding} \\
\midrule
Decimal Addition  & 100\%  & 100\%  \\
Integer Addition  & 100\%  & 100\%  \\
Multiplication    & 99.95\% & 99.91\% \\
Subtraction       & 100\%  & 100\%  \\
\bottomrule
\end{tabular}
\end{table}

\else
\vspace{-5mm}
\begin{table}[!ht]
\centering
\small % Reduce font size
\setlength{\tabcolsep}{4pt} % Reduce column spacing
\caption{Accuracy Comparison Across Datasets}
\label{tab:ablation_linear}
\begin{tabular}{lcc}
\toprule
\textbf{Task} & \textbf{Linear Layer} & \textbf{Zero Padding} \\
\midrule
Decimal Addition  & 100\%  & 100\%  \\
Integer Addition  & 100\%  & 100\%  \\
Multiplication    & 99.95\% & 99.91\% \\
Subtraction       & 100\%  & 100\%  \\
\bottomrule
\end{tabular}
\vspace{-1.5mm} % Reduce space after table
\end{table}

\fi



\paragraph{Effect of Different Periods.}
As discussed in Section \ref{sec:fne}, the modular group captures the necessary information for each digit, ensuring the effectiveness of our approach. We test the model with base periods of $[2, 5, 10]$, $[5]$, and $[7]$, as shown in Table \ref{tab:modular_task_comparison}. The $[2, 5, 10]$ configuration achieves accuracy comparable to that of the $10$-period setup across different datasets. In this paper, we choose single $10$ to make it more parameter efficient. However, configurations using only $\bmod 5$ or $\bmod 7$ exhibit significantly lower accuracy. This is because neither $\bmod 5$ nor $\bmod 7$ can fully represent the required information for all digits.

\ifdefined\isarxiv

\begin{table}[ht]
\centering
\setlength{\tabcolsep}{4pt} % Reduce column spacing
\caption{Accuracy Comparison Across Datasets and Periods}
\label{tab:modular_task_comparison}
\begin{tabular}{lcccc}
\toprule
\textbf{Dataset} & \textbf{2,5,10} & \textbf{10} & \textbf{5} & \textbf{7} \\ 
\midrule
Decimal Addition  & 100   & 100   & 1.52 & 3.64 \\ 
Integer Addition  & 100   & 100   & 1.55 & 0.02 \\ 
Multiplication    & 99.99 & 99.95 & 3.67 & 1.91 \\ 
Subtraction       & 100   & 100   & 4.64 & 0.24 \\ 
\bottomrule
\end{tabular}
\end{table}
\else

\begin{table}[ht]
\centering
\small % Reduce font size
\setlength{\tabcolsep}{4pt} % Reduce column spacing
% \vspace{-3mm}
\caption{Accuracy Comparison Across Datasets and Periods}
\label{tab:modular_task_comparison}
\begin{tabular}{lcccc}
\toprule
\textbf{Dataset} & \textbf{2,5,10} & \textbf{10} & \textbf{5} & \textbf{7} \\ 
\midrule
Decimal Addition  & 100   & 100   & 1.52 & 3.64 \\ 
Integer Addition  & 100   & 100   & 1.55 & 0.02 \\ 
Multiplication    & 99.99 & 99.95 & 3.67 & 1.91 \\ 
Subtraction       & 100   & 100   & 4.64 & 0.24 \\ 
\bottomrule
\end{tabular}
\vspace{-5mm} 
\end{table}
\fi

The mispredictions are attributed to the absence of critical modular results. As illustrated in Table \ref{tab:misprediction} in Appendix \ref{app:exp}, in the decimal addition task, using only a $\bmod 5$ representation prevents the model from distinguishing between certain digits, such as $2$ and $7$, which results in errors. 

\paragraph{Necessity of Sine and Cosine Encoding.} 
A natural question arises: are sinusoidal encodings truly necessary for arithmetic tasks? One could directly encode each digit into a separate dimension of the embedding, representing a number like 567 as \([5,6,7]\). However, this approach fails to achieve perfect accuracy. For instance, numbers such as $999$ and $888$ become nearly indistinguishable after layer normalization, which reduces their differences and can lead to confusion during training. We evaluate this direct encoding method on 6-digit decimal addition and, after performing a learning rate search, find that the best accuracy is 99.3\% with a learning rate of 0.01 and training for 100 epochs. In contrast, FoNE  achieves better accuracy in just 6 epochs with the same dataset and model size. This suggests that naive direct encoding does not adequately preserve numerical distinctions for reliable arithmetic operations. As illustrated in Table \ref{tab:misprediction2} in the appendix, the model frequently mispredicts 8 as 9, further demonstrating the limitations of direct encoding in preserving numerical structure.
