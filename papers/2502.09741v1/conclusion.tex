\vspace{-2mm}
\section{Conclusion}
In this paper, we introduced FoNE, a novel method for representing numbers in the embedding space of LLMs. By leveraging Fourier features, FoNE directly maps numbers into a compact and precise representation, bypassing tokenization inefficiencies and preserving essential numerical properties.

FoNE has significant implications for pre-training LLMs. By incorporating FoNE, models can develop a robust understanding of numerical concepts, addressing a fundamental limitation in current architectures. We expect this capability extends beyond simple arithmetic to support a wide range of number-related tasks, including time-series analysis, quantitative reasoning, and complex operations in fields like physics and mathematics.

By integrating FoNE into pre-training strategies, future models can overcome the limitations of existing tokenization schemes, achieve greater computational efficiency. We believe FoNE represents a \vsreplace{transformative}{significant} step toward equipping LLMs with the tools necessary to tackle complex challenges with precision and rigor.


\ifdefined\isarxiv
\else
\section*{Impact Statement}
This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here.
\fi