\section{Related Works}
Current attacks on model merging primarily focus on injecting backdoor triggers to mislead the model into making incorrect decisions. Among them, LoRA-as-an-Attack **Wang, "LoRA as an Adversarial Attack"** and **Zhao, "Backdoor Attacks through Lateral Regularization"** leveraged LoRA to implant backdoor triggers, while **BadMerging** and **DAM** study injected backdoors through fine-tuning. Additionally, the study by **He, "On the Robustness of Model Merging with Misaligned Language Models"** revealed that merging a misaligned LLM will break the merged LLM safe alignment. In privacy attacks,  **Bai, "Phishing Privacy through Fine-Tuning in Pre-training Stage"** considered teaching LLMs to phish privacy during the pre-training stage, while **Li, "Private Data Leaks via Model Merging"** and **Zhang, "Fine-tuning Attacks on Language Models for Stealing User’s Privacy"** focused on the fine-tuning stage. This paper focuses on stealing the user’s privacy in model merging.