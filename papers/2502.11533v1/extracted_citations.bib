@inproceedings{chen2024janus,
  title={The janus interface: How fine-tuning in large language models amplifies the privacy risks},
  author={Chen, Xiaoyi and Tang, Siyuan and Zhu, Rui and Yan, Shijun and Jin, Lei and Wang, Zihao and Su, Liya and Zhang, Zhikun and Wang, XiaoFeng and Tang, Haixu},
  booktitle={Proceedings of the 2024 on ACM SIGSAC Conference on Computer and Communications Security},
  pages={1285--1299},
  year={2024}
}

@Article{electronics13142858,
AUTHOR = {He, Jiaming and Hou, Guanyu and Jia, Xinyue and Chen, Yangyang and Liao, Wenqi and Zhou, Yinhang and Zhou, Rang},
TITLE = {Data Stealing Attacks against Large Language Models via Backdooring},
JOURNAL = {Electronics},
VOLUME = {13},
YEAR = {2024},
NUMBER = {14},
ARTICLE-NUMBER = {2858},
URL = {https://www.mdpi.com/2079-9292/13/14/2858},
ISSN = {2079-9292},
ABSTRACT = {Large language models (LLMs) have gained immense attention and are being increasingly applied in various domains. However, this technological leap forward poses serious security and privacy concerns. This paper explores a novel approach to data stealing attacks by introducing an adaptive method to extract private training data from pre-trained LLMs via backdooring. Our method mainly focuses on the scenario of model customization and is conducted in two phases, including backdoor training and backdoor activation, which allow for the extraction of private information without prior knowledge of the modelâ€™s architecture or training data. During the model customization stage, attackers inject the backdoor into the pre-trained LLM by poisoning a small ratio of the training dataset. During the inference stage, attackers can extract private information from the third-party knowledge database by incorporating the pre-defined backdoor trigger. Our method leverages the customization process of LLMs, injecting a stealthy backdoor that can be triggered after deployment to retrieve private data. We demonstrate the effectiveness of our proposed attack through extensive experiments, achieving a notable attack success rate. Extensive experiments demonstrate the effectiveness of our stealing attack in popular LLM architectures, as well as stealthiness during normal inference.},
DOI = {10.3390/electronics13142858}
}

@article{hammoud2024model,
  title={Model Merging and Safety Alignment: One Bad Model Spoils the Bunch},
  author={Hammoud, Hasan Abed Al Kader and Michieli, Umberto and Pizzati, Fabio and Torr, Philip and Bibi, Adel and Ghanem, Bernard and Ozay, Mete},
  journal={arXiv preprint arXiv:2406.14563},
  year={2024}
}

@article{liu2024lora,
  title={LoRA-as-an-Attack! Piercing LLM Safety Under The Share-and-Play Scenario},
  author={Liu, Hongyi and Liu, Zirui and Tang, Ruixiang and Yuan, Jiayi and Zhong, Shaochen and Chuang, Yu-Neng and Li, Li and Chen, Rui and Hu, Xia},
  journal={arXiv preprint arXiv:2403.00108},
  year={2024}
}

@article{panda2024teach,
  title={Teach llms to phish: Stealing private information from language models},
  author={Panda, Ashwinee and Choquette-Choo, Christopher A and Zhang, Zhengming and Yang, Yaoqing and Mittal, Prateek},
  journal={arXiv preprint arXiv:2403.00871},
  year={2024}
}

@article{yang2024mitigating,
  title={Mitigating the Backdoor Effect for Multi-Task Model Merging via Safety-Aware Subspace},
  author={Yang, Jinluan and Tang, Anke and Zhu, Didi and Chen, Zhengyu and Shen, Li and Wu, Fei},
  journal={arXiv preprint arXiv:2410.13910},
  year={2024}
}

@article{yin2024lobam,
  title={LoBAM: LoRA-Based Backdoor Attack on Model Merging},
  author={Yin, Ming and Zhang, Jingyang and Sun, Jingwei and Fang, Minghong and Li, Hai and Chen, Yiran},
  journal={arXiv preprint arXiv:2411.16746},
  year={2024}
}

@inproceedings{zhang2024badmerging,
  title={Badmerging: Backdoor attacks against model merging},
  author={Zhang, Jinghuai and Chi, Jianfeng and Li, Zheng and Cai, Kunlin and Zhang, Yang and Tian, Yuan},
  booktitle={Proceedings of the 2024 on ACM SIGSAC Conference on Computer and Communications Security},
  pages={4450--4464},
  year={2024}
}

