% Phishing Merging

@article{yang2024model,
  title={Model merging in llms, mllms, and beyond: Methods, theories, applications and opportunities},
  author={Yang, Enneng and Shen, Li and Guo, Guibing and Wang, Xingwei and Cao, Xiaochun and Zhang, Jie and Tao, Dacheng},
  journal={arXiv preprint arXiv:2408.07666},
  year={2024}
}

@inproceedings{wortsman2022model,
  title={Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time},
  author={Wortsman, Mitchell and Ilharco, Gabriel and Gadre, Samir Ya and Roelofs, Rebecca and Gontijo-Lopes, Raphael and Morcos, Ari S and Namkoong, Hongseok and Farhadi, Ali and Carmon, Yair and Kornblith, Simon and others},
  booktitle={International conference on machine learning},
  pages={23965--23998},
  year={2022},
  organization={PMLR}
}

@article{yang2023adamerging,
  title={Adamerging: Adaptive model merging for multi-task learning},
  author={Yang, Enneng and Wang, Zhenyi and Shen, Li and Liu, Shiwei and Guo, Guibing and Wang, Xingwei and Tao, Dacheng},
  journal={arXiv preprint arXiv:2310.02575},
  year={2023}
}

@article{zhou2024metagpt,
  title={MetaGPT: Merging Large Language Models Using Model Exclusive Task Arithmetic},
  author={Zhou, Yuyan and Song, Liang and Wang, Bingning and Chen, Weipeng},
  journal={arXiv preprint arXiv:2406.11385},
  year={2024}
}

@article{ilharco2022editing,
  title={Editing models with task arithmetic},
  author={Ilharco, Gabriel and Ribeiro, Marco Tulio and Wortsman, Mitchell and Gururangan, Suchin and Schmidt, Ludwig and Hajishirzi, Hannaneh and Farhadi, Ali},
  journal={arXiv preprint arXiv:2212.04089},
  year={2022}
}

@article{yadav2024ties,
  title={Ties-merging: Resolving interference when merging models},
  author={Yadav, Prateek and Tam, Derek and Choshen, Leshem and Raffel, Colin A and Bansal, Mohit},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{ortiz2305task,
  title={Task arithmetic in the tangent space: Improved editing of pre-trained models, 2023},
  author={Ortiz-Jimenez, Guillermo and Favero, Alessandro and Frossard, Pascal},
  journal={URL http://arxiv. org/abs/2305.12827},
  year={2023}
}

@inproceedings{yu2024language,
  title={Language models are super mario: Absorbing abilities from homologous models as a free lunch},
  author={Yu, Le and Yu, Bowen and Yu, Haiyang and Huang, Fei and Li, Yongbin},
  booktitle={Forty-first International Conference on Machine Learning},
  year={2024}
}

@article{du2024parameter,
  title={Parameter competition balancing for model merging},
  author={Du, Guodong and Lee, Junlin and Li, Jing and Jiang, Runhua and Guo, Yifei and Yu, Shuyang and Liu, Hanting and Goh, Sim Kuan and Tang, Ho-Kin and He, Daojing and others},
  journal={arXiv preprint arXiv:2410.02396},
  year={2024}
}

@article{deep2024della,
  title={DELLA-Merging: Reducing Interference in Model Merging through Magnitude-Based Sampling},
  author={Deep, Pala Tej and Bhardwaj, Rishabh and Poria, Soujanya},
  journal={arXiv preprint arXiv:2406.11617},
  year={2024}
}

@article{akiba2024evolutionary,
  title={Evolutionary optimization of model merging recipes},
  author={Akiba, Takuya and Shing, Makoto and Tang, Yujin and Sun, Qi and Ha, David},
  journal={arXiv preprint arXiv:2403.13187},
  year={2024}
}

@inproceedings{huang-etal-2024-chat,
    title = "Chat Vector: A Simple Approach to Equip {LLM}s with Instruction Following and Model Alignment in New Languages",
    author = "Huang, Shih-Cheng  and
      Li, Pin-Zu  and
      Hsu, Yu-chi  and
      Chen, Kuang-Ming  and
      Lin, Yu Tung  and
      Hsiao, Shih-Kai  and
      Tsai, Richard  and
      Lee, Hung-yi",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.acl-long.590/",
    doi = "10.18653/v1/2024.acl-long.590",
    pages = "10943--10959",
}

@article{yang2024mitigating,
  title={Mitigating the Backdoor Effect for Multi-Task Model Merging via Safety-Aware Subspace},
  author={Yang, Jinluan and Tang, Anke and Zhu, Didi and Chen, Zhengyu and Shen, Li and Wu, Fei},
  journal={arXiv preprint arXiv:2410.13910},
  year={2024}
}

@inproceedings{zhang2024badmerging,
  title={Badmerging: Backdoor attacks against model merging},
  author={Zhang, Jinghuai and Chi, Jianfeng and Li, Zheng and Cai, Kunlin and Zhang, Yang and Tian, Yuan},
  booktitle={Proceedings of the 2024 on ACM SIGSAC Conference on Computer and Communications Security},
  pages={4450--4464},
  year={2024}
}

@article{minaee2024large,
  title={Large language models: A survey},
  author={Minaee, Shervin and Mikolov, Tomas and Nikzad, Narjes and Chenaghlu, Meysam and Socher, Richard and Amatriain, Xavier and Gao, Jianfeng},
  journal={arXiv preprint arXiv:2402.06196},
  year={2024}
}


@article{tam2024remm,
  title={Realistic Evaluation of Model Merging for Compositional Generalization},
  author={Tam, Derek and Kant, Yash and Lester, Brian and Gilitschenski, Igor and Raffel, Colin},
  journal={arXiv preprint arXiv:2409.18314},
  year={2024}
}

@article{li2024llm,
  title={Llm-pbe: Assessing data privacy in large language models},
  author={Li, Qinbin and Hong, Junyuan and Xie, Chulin and Tan, Jeffrey and Xin, Rachel and Hou, Junyi and Yin, Xavier and Wang, Zhun and Hendrycks, Dan and Wang, Zhangyang and others},
  journal={arXiv preprint arXiv:2408.12787},
  year={2024}
}

@article{huang2022large,
  title={Are large pre-trained language models leaking your personal information?},
  author={Huang, Jie and Shao, Hanyin and Chang, Kevin Chen-Chuan},
  journal={arXiv preprint arXiv:2205.12628},
  year={2022}
}

@article{kim2024propile,
  title={Propile: Probing privacy leakage in large language models},
  author={Kim, Siwon and Yun, Sangdoo and Lee, Hwaran and Gubri, Martin and Yoon, Sungroh and Oh, Seong Joon},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{panda2024teach,
  title={Teach llms to phish: Stealing private information from language models},
  author={Panda, Ashwinee and Choquette-Choo, Christopher A and Zhang, Zhengming and Yang, Yaoqing and Mittal, Prateek},
  journal={arXiv preprint arXiv:2403.00871},
  year={2024}
}

@article{mattern2023membership,
  title={Membership inference attacks against language models via neighbourhood comparison},
  author={Mattern, Justus and Mireshghallah, Fatemehsadat and Jin, Zhijing and Sch{\"o}lkopf, Bernhard and Sachan, Mrinmaya and Berg-Kirkpatrick, Taylor},
  journal={arXiv preprint arXiv:2305.18462},
  year={2023}
}

@inproceedings{mireshghallah2022empirical,
  title={An empirical analysis of memorization in fine-tuned autoregressive language models},
  author={Mireshghallah, Fatemehsadat and Uniyal, Archit and Wang, Tianhao and Evans, David K and Berg-Kirkpatrick, Taylor},
  booktitle={Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing},
  pages={1816--1826},
  year={2022}
}

@misc{fu2024practicalmembershipinferenceattacks,
      title={Practical Membership Inference Attacks against Fine-tuned Large Language Models via Self-prompt Calibration}, 
      author={Wenjie Fu and Huandong Wang and Chen Gao and Guanghua Liu and Yong Li and Tao Jiang},
      year={2024},
      eprint={2311.06062},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2311.06062}, 
}

@article{qin2024infobench,
  title={Infobench: Evaluating instruction following ability in large language models},
  author={Qin, Yiwei and Song, Kaiqiang and Hu, Yebowen and Yao, Wenlin and Cho, Sangwoo and Wang, Xiaoyang and Wu, Xuansheng and Liu, Fei and Liu, Pengfei and Yu, Dong},
  journal={arXiv preprint arXiv:2401.03601},
  year={2024}
}

@article{wu2024thinking,
  title={Thinking LLMs: General Instruction Following with Thought Generation},
  author={Wu, Tianhao and Lan, Janice and Yuan, Weizhe and Jiao, Jiantao and Weston, Jason and Sukhbaatar, Sainbayar},
  journal={arXiv preprint arXiv:2410.10630},
  year={2024}
}

@article{wei2022chain,
  title={Chain-of-thought prompting elicits reasoning in large language models},
  author={Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Xia, Fei and Chi, Ed and Le, Quoc V and Zhou, Denny and others},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={24824--24837},
  year={2022}
}

@article{kaushik2021understanding,
  title={Understanding catastrophic forgetting and remembering in continual learning with optimal relevance mapping},
  author={Kaushik, Prakhar and Gain, Alex and Kortylewski, Adam and Yuille, Alan},
  journal={arXiv preprint arXiv:2102.11343},
  year={2021}
}

@article{lu2024versatune,
  title={VersaTune: Fine-Tuning Multi-Ability LLMs Efficiently},
  author={Lu, Keer and Zhao, Keshi and Liang, Zheng and Pan, Da and Zhang, Shusen and Wu, Xin and Chen, Weipeng and Zhou, Zenan and Dong, Guosheng and Cui, Bin and others},
  journal={arXiv preprint arXiv:2411.11266},
  year={2024}
}

@article{yan2024protecting,
  title={On protecting the data privacy of large language models (llms): A survey},
  author={Yan, Biwei and Li, Kun and Xu, Minghui and Dong, Yueyan and Zhang, Yue and Ren, Zhaochun and Cheng, Xiuzhen},
  journal={arXiv preprint arXiv:2403.05156},
  year={2024}
}

@inproceedings{carlini2021extracting,
  title={Extracting training data from large language models},
  author={Carlini, Nicholas and Tramer, Florian and Wallace, Eric and Jagielski, Matthew and Herbert-Voss, Ariel and Lee, Katherine and Roberts, Adam and Brown, Tom and Song, Dawn and Erlingsson, Ulfar and others},
  booktitle={30th USENIX Security Symposium (USENIX Security 21)},
  pages={2633--2650},
  year={2021}
}

@inproceedings{lukas2023analyzing,
  title={Analyzing leakage of personally identifiable information in language models},
  author={Lukas, Nils and Salem, Ahmed and Sim, Robert and Tople, Shruti and Wutschitz, Lukas and Zanella-B{\'e}guelin, Santiago},
  booktitle={2023 IEEE Symposium on Security and Privacy (SP)},
  pages={346--363},
  year={2023},
  organization={IEEE}
}


@article{zhang2023instruction,
  title={Instruction tuning for large language models: A survey},
  author={Zhang, Shengyu and Dong, Linfeng and Li, Xiaoya and Zhang, Sen and Sun, Xiaofei and Wang, Shuhe and Li, Jiwei and Hu, Runyi and Zhang, Tianwei and Wu, Fei and others},
  journal={arXiv preprint arXiv:2308.10792},
  year={2023}
}

@inproceedings{chen2024janus,
  title={The janus interface: How fine-tuning in large language models amplifies the privacy risks},
  author={Chen, Xiaoyi and Tang, Siyuan and Zhu, Rui and Yan, Shijun and Jin, Lei and Wang, Zihao and Su, Liya and Zhang, Zhikun and Wang, XiaoFeng and Tang, Haixu},
  booktitle={Proceedings of the 2024 on ACM SIGSAC Conference on Computer and Communications Security},
  pages={1285--1299},
  year={2024}
}

@article{kaneko2024sampling,
  title={Sampling-based Pseudo-Likelihood for Membership Inference Attacks},
  author={Kaneko, Masahiro and Ma, Youmi and Wata, Yuki and Okazaki, Naoaki},
  journal={arXiv preprint arXiv:2404.11262},
  year={2024}
}

@inproceedings{amini-etal-2019-mathqa,
    title = "{M}ath{QA}: Towards Interpretable Math Word Problem Solving with Operation-Based Formalisms",
    author = "Amini, Aida  and
      Gabriel, Saadia  and
      Lin, Shanchuan  and
      Koncel-Kedziorski, Rik  and
      Choi, Yejin  and
      Hajishirzi, Hannaneh",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N19-1245",
    doi = "10.18653/v1/N19-1245",
    pages = "2357--2367",
}

@article{jin2021disease,
  title={What disease does this patient have? a large-scale open domain question answering dataset from medical exams},
  author={Jin, Di and Pan, Eileen and Oufattole, Nassim and Weng, Wei-Hung and Fang, Hanyi and Szolovits, Peter},
  journal={Applied Sciences},
  volume={11},
  number={14},
  pages={6421},
  year={2021},
  publisher={MDPI}
}

@misc{codealpaca,
  author = {Sahil Chaudhary},
  title = {Code Alpaca: An Instruction-following LLaMA model for code generation},
  year = {2023},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/sahil280114/codealpaca}},
}

@misc{chen2021evaluating,
      title={Evaluating Large Language Models Trained on Code},
      author={Mark Chen and Jerry Tworek and Heewoo Jun and Qiming Yuan and Henrique Ponde de Oliveira Pinto and Jared Kaplan and Harri Edwards and Yuri Burda and Nicholas Joseph and Greg Brockman and Alex Ray and Raul Puri and Gretchen Krueger and Michael Petrov and Heidy Khlaaf and Girish Sastry and Pamela Mishkin and Brooke Chan and Scott Gray and Nick Ryder and Mikhail Pavlov and Alethea Power and Lukasz Kaiser and Mohammad Bavarian and Clemens Winter and Philippe Tillet and Felipe Petroski Such and Dave Cummings and Matthias Plappert and Fotios Chantzis and Elizabeth Barnes and Ariel Herbert-Voss and William Hebgen Guss and Alex Nichol and Alex Paino and Nikolas Tezak and Jie Tang and Igor Babuschkin and Suchir Balaji and Shantanu Jain and William Saunders and Christopher Hesse and Andrew N. Carr and Jan Leike and Josh Achiam and Vedant Misra and Evan Morikawa and Alec Radford and Matthew Knight and Miles Brundage and Mira Murati and Katie Mayer and Peter Welinder and Bob McGrew and Dario Amodei and Sam McCandlish and Ilya Sutskever and Wojciech Zaremba},
      year={2021},
      eprint={2107.03374},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{yin2024lobam,
  title={LoBAM: LoRA-Based Backdoor Attack on Model Merging},
  author={Yin, Ming and Zhang, Jingyang and Sun, Jingwei and Fang, Minghong and Li, Hai and Chen, Yiran},
  journal={arXiv preprint arXiv:2411.16746},
  year={2024}
}

@article{liu2024lora,
  title={LoRA-as-an-Attack! Piercing LLM Safety Under The Share-and-Play Scenario},
  author={Liu, Hongyi and Liu, Zirui and Tang, Ruixiang and Yuan, Jiayi and Zhong, Shaochen and Chuang, Yu-Neng and Li, Li and Chen, Rui and Hu, Xia},
  journal={arXiv preprint arXiv:2403.00108},
  year={2024}
}

@article{hammoud2024model,
  title={Model Merging and Safety Alignment: One Bad Model Spoils the Bunch},
  author={Hammoud, Hasan Abed Al Kader and Michieli, Umberto and Pizzati, Fabio and Torr, Philip and Bibi, Adel and Ghanem, Bernard and Ozay, Mete},
  journal={arXiv preprint arXiv:2406.14563},
  year={2024}
}

@Article{electronics13142858,
AUTHOR = {He, Jiaming and Hou, Guanyu and Jia, Xinyue and Chen, Yangyang and Liao, Wenqi and Zhou, Yinhang and Zhou, Rang},
TITLE = {Data Stealing Attacks against Large Language Models via Backdooring},
JOURNAL = {Electronics},
VOLUME = {13},
YEAR = {2024},
NUMBER = {14},
ARTICLE-NUMBER = {2858},
URL = {https://www.mdpi.com/2079-9292/13/14/2858},
ISSN = {2079-9292},
ABSTRACT = {Large language models (LLMs) have gained immense attention and are being increasingly applied in various domains. However, this technological leap forward poses serious security and privacy concerns. This paper explores a novel approach to data stealing attacks by introducing an adaptive method to extract private training data from pre-trained LLMs via backdooring. Our method mainly focuses on the scenario of model customization and is conducted in two phases, including backdoor training and backdoor activation, which allow for the extraction of private information without prior knowledge of the model’s architecture or training data. During the model customization stage, attackers inject the backdoor into the pre-trained LLM by poisoning a small ratio of the training dataset. During the inference stage, attackers can extract private information from the third-party knowledge database by incorporating the pre-defined backdoor trigger. Our method leverages the customization process of LLMs, injecting a stealthy backdoor that can be triggered after deployment to retrieve private data. We demonstrate the effectiveness of our proposed attack through extensive experiments, achieving a notable attack success rate. Extensive experiments demonstrate the effectiveness of our stealing attack in popular LLM architectures, as well as stealthiness during normal inference.},
DOI = {10.3390/electronics13142858}
}

@article{chen2024multi,
  title={Multi-task learning in natural language processing: An overview},
  author={Chen, Shijie and Zhang, Yu and Yang, Qiang},
  journal={ACM Computing Surveys},
  volume={56},
  number={12},
  pages={1--32},
  year={2024},
  publisher={ACM New York, NY}
}

@article{long2024llms,
  title={On llms-driven synthetic data generation, curation, and evaluation: A survey},
  author={Long, Lin and Wang, Rui and Xiao, Ruixuan and Zhao, Junbo and Ding, Xiao and Chen, Gang and Wang, Haobo},
  journal={arXiv preprint arXiv:2406.15126},
  year={2024}
}

@inproceedings{kwon2023efficient,
  title={Efficient memory management for large language model serving with pagedattention},
  author={Kwon, Woosuk and Li, Zhuohan and Zhuang, Siyuan and Sheng, Ying and Zheng, Lianmin and Yu, Cody Hao and Gonzalez, Joseph and Zhang, Hao and Stoica, Ion},
  booktitle={Proceedings of the 29th Symposium on Operating Systems Principles},
  pages={611--626},
  year={2023}
}

@article{uppaal2024detox,
  title={Detox: Toxic subspace projection for model editing},
  author={Uppaal, Rheeya and Dey, Apratim and He, Yiting and Zhong, Yiqiao and Hu, Junjie},
  journal={arXiv e-prints},
  pages={arXiv--2405},
  year={2024}
}

@article{zhang2024reef,
  title={Reef: Representation encoding fingerprints for large language models},
  author={Zhang, Jie and Liu, Dongrui and Qian, Chen and Zhang, Linfeng and Liu, Yong and Qiao, Yu and Shao, Jing},
  journal={arXiv preprint arXiv:2410.14273},
  year={2024}
}

@article{yamabe2024mergeprint,
  title={MergePrint: Robust Fingerprinting against Merging Large Language Models},
  author={Yamabe, Shojiro and Takahashi, Tsubasa and Waseda, Futa and Wataoka, Koki},
  journal={arXiv preprint arXiv:2410.08604},
  year={2024}
}






% Use this file for citations not found in the ACL Anthology (contained in "anthology.bib").

@book{Aho:72,
    author  = {Alfred V. Aho and Jeffrey D. Ullman},
    title   = {The Theory of Parsing, Translation and Compiling},
    year    = "1972",
    volume  = "1",
    publisher = {Prentice-Hall},
    address = {Englewood Cliffs, NJ}
}

@book{APA:83,
    author  = {{American Psychological Association}},
    title   = {Publications Manual},
    year    = "1983",
    publisher = {American Psychological Association},
    address = {Washington, DC}
}

@article{Chandra:81,
	author = {Ashok K. Chandra and Dexter C. Kozen and Larry J. Stockmeyer},
	year = "1981",
	title = {Alternation},
	journal = {Journal of the Association for Computing Machinery},
	volume = "28",
	number = "1",
	pages = "114--133",
	doi = "10.1145/322234.322243",
}

@inproceedings{andrew2007scalable,
  title={Scalable training of {L1}-regularized log-linear models},
  author={Andrew, Galen and Gao, Jianfeng},
  booktitle={Proceedings of the 24th International Conference on Machine Learning},
  pages={33--40},
  year={2007},
}

@book{Gusfield:97,
    author  = {Dan Gusfield},
    title   = {Algorithms on Strings, Trees and Sequences},
    year    = "1997",
    publisher = {Cambridge University Press},
    address = {Cambridge, UK}
}

@article{rasooli-tetrault-2015,
    author    = {Mohammad Sadegh Rasooli and Joel R. Tetreault},
    title     = {Yara Parser: {A} Fast and Accurate Dependency Parser},
    journal   = {Computing Research Repository},
    volume    = {arXiv:1503.06733},
    year      = {2015},
    url       = {http://arxiv.org/abs/1503.06733},
    note    = {version 2}
}

@article{Ando2005,
	Acmid = {1194905},
	Author = {Ando, Rie Kubota and Zhang, Tong},
	Issn = {1532-4435},
	Issue_Date = {12/1/2005},
	Journal = {Journal of Machine Learning Research},
	Month = dec,
	Numpages = {37},
	Pages = {1817--1853},
	Publisher = {JMLR.org},
	Title = {A Framework for Learning Predictive Structures from Multiple Tasks and Unlabeled Data},
	Volume = {6},
	Year = {2005}
}
