\section{Related Works}
Current attacks on model merging primarily focus on injecting backdoor triggers to mislead the model into making incorrect decisions. Among them, LoRA-as-an-Attack \cite{yin2024lobam} and LoBAM \citet{liu2024lora} leveraged LoRA to implant backdoor triggers, while BadMerging \cite{zhang2024badmerging} and DAM \cite{yang2024mitigating} study injected backdoors through fine-tuning. Additionally, the study by \citet{hammoud2024model} revealed that merging a misaligned LLM will break the merged LLM safe alignment. In privacy attacks,  \citet{panda2024teach} considered teaching LLMs to phish privacy during the pre-training stage, while \citet{chen2024janus} and \citet{electronics13142858} focused on the fine-tuning stage. This paper focuses on stealing the userâ€™s privacy in model merging.