%\section{Generalized iterative conditional estimation for model identification with form selection}
\section{Method: Generalized Iterative Conditional Estimation (GICE) for model identification with form selection}
\label{sec:Iterative conditional estimation for model identification with form selection}

Let $\x_1^N = \seq{\x_1, \cdots, \x_N}$ be the samples from a CBMM \eqref{eq:CBMMs with params}. Their subgroup labels $z_1^N=\seq{z_1,\cdots,z_N}$ are hidden. Given the component number $K$, the model identification means 
solving the following problems:
\begin{enumerate}
    \item For each \blueCN{marginal} $f_{k,d}$, $d\in\set{1,\cdots,D}$, $k\in\set{1,\cdots,K}$, find its best-fit form $H_{k,d}$, and estimate its associate parameters $\theta_{k,d}$ from a list of candidate forms $\H = \set{H_1,\cdots,H_L}$. Each form $H_l$, $l\in\set{1,\cdots,L}$ is a parametric set of distributions $H_l=\set{f_{\theta\seq{l}}}_{\blueC{l\in\set{1,\cdots,L}}}$.
    \item For each copula $c_k$, $k\in\set{1,\cdots,K}$, find its best-fit form $G_{k}$, and estimate its associate parameters $\alpha_k$ from a list of candidate forms $\G = \set{G_1,\cdots,G_M}$. Each form $G_m$, $m\in\set{1,\cdots,M}$ is a parametric set of copulas $G_m=\set{c_{\alpha\seq{m}}}_{\blueC{m\in\set{1,\cdots,M}}}$.
    \item Estimate component coefficients $\bspi=\set{\pi_k}_{k=1}^K$ related to the hidden cluster labels $z_1^N$.
\end{enumerate}
We tackle the identification problem of the CBMMs through a Generalised ICE (GICE) framework with copula selection.  
ICE is an iterative method for parameter estimation in case of incomplete data, under two gentle assumptions: (i) there exist estimators for the parameters from the complete data ($\x_1^N, z_1^N$), and (ii) the ability to generate samples of $Z$ from $\pdfc{z}{\x}$ \cite{pieczynski2007convergence}. %\cite{image1992statistical, pieczynski2007convergence}. 
The ICE resembles \blueC{the} EM algorithm, but it does not aim to maximize the likelihood. It is generally easier to implement compared to EM, especially when facing complex \blueC{models. Besides, the} ``maximization'' step of EM often meets difficulties. 

To apply the ICE framework to our problem, we accept the following assumptions:
\begin{enumerate}
    \item There exists an estimator $\hat{\theta}(l)$ for estimating the parameters of the candidate marginal form $H_l$ , $\forall l\in\set{1,\cdots,L}$ from univariate data samples, and an estimator $\hat{\alpha}(m)$ for estimating the parameters of the
    candidate copula form $G_m$, $\forall m\in\set{1,\cdots,M}$ from multivariate data samples.
    \item There exist two decision rules, $\blueC{\Delta}^1$ deciding the best-fit marginal form from $\H$ which fits the given univariate samples $\seq{y_1,\cdots,y_Q}$, and $\blueC{\Delta}^2$ deciding the best-fit copula from $\G$ which fits the given multivariate samples $\seq{\y_1,\cdots,\y_Q}$. $Q$ denotes the number of samples.
\end{enumerate}

The first assumption is required by the original ICE, and the second assumption is added to enable the form selection of the algorithm. A two-step parameter estimation is considered in the first assumption to separate the marginal and the copula estimators. When estimating the parameters of a joint distribution, we first \blueC{need to define the number of clusters, then} estimate the marginal parameters, \blueC{and finally} estimate the copula parameters among estimated marginal CDFs. In consequence, the form selection of \blueCN{marginals} and copula can also be performed separately, which avoids the form combination issue involved when using simultaneous estimators.

\blueC{The proposed GICE algorithm starts from an initial guess of cluster distributions, then iteratively updates the distribution forms as well as the parameters based on grouped data with simulated cluster labels.}
More precisely, GICE starts with an initialization of parameters $\bsTheta^{(i=0)}=\set{\pi_k^{(0)}, \alpha_k^{(0)}, \bstheta_k^{(0)}}_{k=1}^K$, then iteratively runs the following steps: 
\begin{enumerate}
    \item \blueC{At iteration $i$, c}ompute the posteriors of the hidden variable $\pdfc{\zn}{\xn; \bsTheta^{(i-1)}}$, for $n=1,\cdots,N$ by:
    \begin{equation}
        \label{eq:GICE posterior of z}
        \pdfc{\zn=k}{\xn; \bsTheta^{(i-1)}} = \frac{\pi_k^{(i-1)} \pdfc{\xn}{\zn; \alpha_k^{(i-1)}, \bstheta_k^{(i-1)}} }{\sum_{k=1}^K \pi_k^{(i-1)} \pdfc{\xn}{\zn; \alpha_k^{(i-1)}, \bstheta_k^{(i-1)}}},
    \end{equation}
    then simulate $T$ times the hidden labels $\zunN$ \blueC{according to \eqref{eq:GICE posterior of z}}, name all the realizations $\hat{\bsz} = \seq{\hat{\bsz}^1,\cdots,\hat{\bsz}^T}$, with each realization set $\hat{\bsz}^t = \seq{\hat{z}_1^t,\cdots,\hat{z}_N^t}$, $t\in\set{1,\cdots,T}$.
    \item For each component $k$, \blueC{create the subgroup of samples by gathering the samples with their corresponding labels $\hat{\bsz}=k$. Then, for each created subgroup, select its marginal and copula forms from candidate lists $\H$, $\G$, and estimate their corresponding parameters $\bstheta_k$, $\alpha_k$ by executing:}
    \begin{enumerate}
        \item For all $t\in\set{1,\cdots,T}$, group data $\seq{\x_1^N}_{\hat{\z}^t=k}$ by taking the subset of $\x_1^N$ formed with $\x_n$ whose cluster label $\hat{z}_n^t=k$, then fuse the $T$     
        subsets corresponding to the same $k$ to obtain the entire subgroup samples:
        \begin{equation}
            \label{eq:GICE subgroup samples}
             \seq{\x_1^N}_{\hat{\bsz}=k} = \seq{\seq{\x_1^N}_{\hat{\z}^1=k},\cdots,\seq{\x_1^N}_{\hat{\z}^T=k}}.
        \end{equation}
        \item Estimate the coefficient of the $k$-th component by:
            \begin{equation}
            \label{eq:GICE update pi_k}
                \hat{\pi}_k = \blueCN{\frac{1}{NT}}\sum_{t=1}^T\sum_{n=1}^N\Card{\hat{z}_n^t=k},
            \end{equation}
        where $Card$ computes the cardinality of a given set.
        \item For all $d\in\set{1,\cdots,D}$, and all $l\in\set{1,\cdots,L}$, fit the marginal form $H_l$ to $\seq{x_1^N}_{\hat{\bsz}=k}^d$
        \greenC{(namely, the $d$-th dimension of $\seq{x_1^N}_{\hat{\bsz}=k}$)} by predefined marginal estimator, and get candidate parameters $\hat{\theta}\seq{l}\interv{\seq{x_1^N}_{\hat{\bsz}=k}^d}$. Then, for each dimension $d$, choose an element $\hat{\theta}_{k,d}$ from $\set{\hat{\theta}\seq{1}\interv{\seq{x_1^N}_{\hat{\bsz}=k}^d},\cdots,\hat{\theta}\seq{L}\interv{\seq{x_1^N}_{\hat{\bsz}=k}^d}}$ by applying $\blueC{\Delta}^1$ to the samples $\seq{x_1^N}_{\hat{\bsz}=k}^d$.
        This step corresponds to the Algorithm \ref{alg:MarginEstimation}.
        \item For each $m\in\set{1,\cdots,M}$, fit the copula form $G_l$ to $\seq{\x_1^N}_{\hat{\bsz}=k}$ by predefined copula estimator, and get the candidate copula parameters $\hat{\alpha}\seq{m}\interv{\seq{\x_1^N}_{\hat{\bsz}=k}}$. Then choose an element $\hat{\alpha}_k$ from $\set{\hat{\alpha}\seq{1}\interv{\seq{\x_1^N}_{\hat{\bsz}=k}},\cdots,\hat{\alpha}\seq{M}\interv{\seq{\x_1^N}_{\hat{\bsz}=k}}}$ by applying $\blueC{\Delta}^2$ to the samples $\seq{\x_1^N}_{\hat{\bsz}=k}$.
        This step corresponds to the Algorithm \ref{alg:CopulaEstimation}.
    \end{enumerate}
    \item Update the parameters $\bsTheta^{(i)}$ by $\set{\hat{\pi}_k, \hat{\alpha}_k, \hat{\bstheta}_k}_{k=1}^K$, where $\hat{\bstheta}_k=\set{\hat{\theta}_{k,d}}_{d=1}^D$.
\end{enumerate}
The above steps are iterated $iterMax$ times until convergence. The \blueCN{whole procedure is summarized} in Algorithm \ref{alg:ICE for CBMMs}.

\IncMargin{1em}
\begin{algorithm}[t]
    \caption{GICE for CBMMs}
    \label{alg:ICE for CBMMs}
    \SetKwInOut{Input}{input}\SetKwInOut{Output}{output}
    \SetKwFunction{Initialization}{Initialization}
    \SetKwFunction{MarginEstimation}{MarginEstimation}
    \SetKwFunction{CopulaEstimation}{CopulaEstimation}
    \Input{$\xunN$, $K$, $iterMax$, \blueC{$T$}, $\H$, $\G$, $\{\hat{\theta}\seq{l}\}_{l=1}^L$, $\set{\hat{\alpha}\seq{m}}_{m=1}^M$, $\blueC{\Delta}^1$, $\blueC{\Delta}^2$}
    \Output{$\bsTheta^{iterMax}$}
    \BlankLine
    $\bsTheta^{(0)}$ = \Initialization{$\xunN$, $K$}\;
    \For{$i\leftarrow1$ \KwTo $iterMax$}{
    simulate \blueC{$\hat{\bsz} = \seq{\hat{\bsz}^1,\cdots,\hat{\bsz}^T}$} according to \eqref{eq:GICE posterior of z} using $\bsTheta^{(i)}$\;
    \For{$k\leftarrow1$ \KwTo $K$}{
        get subgroup sample $\seq{x_1^N}_{\hat{\bsz}=k}$ according to \eqref{eq:GICE subgroup samples}\;
        update $\hat{\pi}_k$ by \eqref{eq:GICE update pi_k}\;
        $\hat{\alpha}_k$ = \MarginEstimation{$\seq{\x_1^N}_{\hat{\bsz}=k}$, $\H$, $\{\hat{\theta}\seq{l}\}_{l=1}^L$, $\blueC{\Delta}^1$}\;
        $\hat{\bstheta}_k$ = \CopulaEstimation{$\seq{\x_1^N}_{\hat{\bsz}=k}$, $\G$, $\set{\hat{\alpha}\seq{m}}_{m=1}^M$, $\blueC{\Delta}^2$}\; 
        }
    $\bsTheta^{(i)}\leftarrow$ $\set{\hat{\pi}_k,\hat{\alpha}_k,\hat{\bstheta}_k}_{k=1}^K$ 
    }
\end{algorithm}
\DecMargin{1em} 

\IncMargin{1em}
\begin{algorithm}[t]
    \caption{Margin Estimation}
    \label{alg:MarginEstimation}
    \SetKwInOut{Input}{input}\SetKwInOut{Output}{output}
    \Input{$\seq{\x_1^N}_{\hat{\bsz}=k}$, $\H$, $\{\hat{\theta}\seq{l}\}_{l=1}^L$, $\blueC{\Delta}^1$}
    \Output{$\hat{\bstheta}_k$}
    \For{$d=1\leftarrow$ \KwTo $D$}{
        $\theta_{tmp}\leftarrow\set{\ }$\;
        \For{$l=1\leftarrow$ \KwTo $L$}{
            $\theta_{tmp} \leftarrow \theta_{tmp} + \hat{\theta}\seq{l}\interv{\seq{x_1^N}_{\hat{\bsz}=k}^d}$
        }
        $\hat{\theta}_{k,d} \leftarrow$ the unique element $\in\theta_{tmp}$ decided by $\blueC{\Delta}^1$
    }
\end{algorithm}
\DecMargin{1em} 

\begin{algorithm}[htbp]
    \caption{Copula Estimation}
    \label{alg:CopulaEstimation}
    \SetKwInOut{Input}{input}\SetKwInOut{Output}{output}
    \Input{$\seq{\x_1^N}_{\hat{\bsz}=k}$, $\G$, $\set{\hat{\alpha}\seq{m}}_{m=1}^M$, $\blueC{\Delta}^2$}
    \Output{$\hat{\alpha}_k$}
    $\alpha_{tmp}\leftarrow\set{\ }$\;
%    \For{$l=1\leftarrow$ \KwTo $L$}{
    \For{$\blueC{m}=1\leftarrow$ \KwTo $\blueC{M}$}{
        $\alpha_{tmp} \leftarrow \alpha_{tmp} + \hat{\alpha}\seq{m}\interv{\seq{\x_1^N}_{\hat{\bsz}=k}}$
    }
    $\hat{\alpha}_k\leftarrow$ the unique element $\in\alpha_{tmp}$ decided by $\blueC{\Delta}^2$
\end{algorithm}

\subsection{Computational details}
\label{sec:Computational details}

\subsubsection{Initialization of parameters}
\label{sec:Initialization of parameters}

There are various techniques to initialize the parameters $\bsTheta^{0}$ corresponding to \blueCN{S}tep 1 in \blueCN{A}lgorithm \ref{alg:ICE for CBMMs}. \blueC{C}onsidering methods with hard assignment, one can simulate the hidden labels $\hat{\bsz}$ randomly or through K-Means, %\cite{macqueen1967some}, 
then run \blueCN{S}teps 4-9 to obtain the $\bsTheta^{0}$. Considering soft assignment, simpler mixtures can be applied, and the learnt parameters of the mixture are taken as $\bsTheta^{0}$. %with slight parameter conversion. 
For instance, using GMM and converting the learnt means and covariances to marginal and copula parameters. K-Means and GMM initialization are both tested in this work, as demonstrated in \blueC{S}ection \ref{sec:Test on simulated data of Non-Gaussian CBMM}.

\subsubsection{Estimators and decision rules}
\label{sec:Estimators and decision rules}
The ICE framework is general and allows different choices of estimators and decision rules for assumptions 1-2. For marginal distributions we can use the Maximum Likelihood \blueCN{Estimator (MLE)}:
\begin{equation}
    \label{eq:marginal estimator}
    \hat{\theta}\seq{l}=\argmax{\blueC{\theta\seq{l}}}\interv{\sum_{q=1}^Q \Ln \seq{f_{\theta\seq{l}}\seq{y_q}}},
\end{equation}
%\blueC{The method of moments can also be chosen as estimator when clusters are of large sample size} 
\blueCN{Alternatively, one could also use the method of moments when the sample size of the clusters is large} \cite{fujimaki2011online}. Similarly for copulas, parameters can be estimated through ML:
\begin{equation}
    \label{eq:copula estimator}
    \hat{\alpha}\seq{m}=\argmax{\blueC{\alpha\seq{m}}}\interv{\sum_{q=1}^Q\Ln\seq{ c_{\alpha\seq{m}}\seq{\hat{\u}_q}}},
\end{equation}
where $\hat{\u}_q = \seq{\hat{u}_q^1,\cdots,\hat{u}_q^D}$ represents the pseudo samples of the copula which are the estimated marginal CDFs of samples $\set{y_q^1,\cdots,y_q^D}$. There are basically two ways to get these pseudo samples: (i) the parametric way, using the estimated parameters of marginal distributions to compute the $\hat{\u}$ \blueC{through their CDF formulas}, and (ii) the non-parametric way, estimating $\hat{\u}$ empirically \blueC{by $\hat{F}(y) = \frac{1}{Q}\sum_{q=1}^{Q}\mathbb{1}_{Y \leq y}$}. \blueC{The ML estimator of copula \eqref{eq:copula estimator} taking $\hat{\u}$ generated parametrically refers to the Inference Function for Margin (IFM) method \cite{joe1996estimation}, while the estimator taking  $\hat{\u}$ generated empirically is called Pseudo Maximum Likelihood (PML) method  \cite{genest1995semiparametric}}\blueCN{.}
Alternatively, copula parameters can also be estimated through the method of moments by means
of the empirical estimation of Kendallâ€™s tau \cite{kendall1938new}. 
In this work, ML is chosen as marginal estimator and PML is chosen as copula estimator for all candidate forms. \blueCN{When} using the method of moments approach, one should pay attention to the chosen order of moments, and the relation between parameters and moments which may not exist in close form for some candidate forms.

The minimization of the Kolmogorov distance is adopted as decision rule for both $\blueC{\Delta}^1$ and $\blueC{\Delta}^2$:
\begin{align}
    \label{eq:D1}
    \blueC{\Delta}^1\seq{y_1^Q} &= \arginf{l\in\set{1,\cdots,L}}\interv{\argsup{y\in\set{y_1^d,\cdots,y_Q^d}}\left| F^d\seq{y} - F^d_{\hat{\theta}\seq{l}}\seq{y}\right|}, \\
    \label{eq:D2}
    \blueC{\Delta}^2\seq{\y_1^Q} &= \arginf{m\in\set{1,\cdots,M}}\interv{\argsup{\y\in\set{\y_1,\cdots,\y_Q}}\left| F\seq{\y} - F_{\hat{\alpha}\seq{m}}\seq{\y}\right|},
\end{align}
where $F^d\seq{y}$ and $F\seq{\y}$ are approximated by empirical CDFs. Other possible decision rules can be ML (PML for $\blueC{\Delta}^2$ \cite{derrode2016unsupervised}), or some particular rules for specific families of candidate forms. For example, $\blueC{\Delta}^1$ can be based on ``skewness'' and ``kurtosis'' if all candidate marginal forms belong to the Pearson family \cite{delignon1997estimation}.

\subsection{Convergence}
\label{sec:Convergence}
It is difficult to study theoretically the convergence of GICE, as the algorithm allows different combinations of estimators and decision rules. Nevertheless, 
it is
easy to trace a certain convergence index at each iteration in order to monitor the performance. For instance, the Kolmogorov distance between empirical CDFs and the learnt CBMM CDFs of the given data:
\begin{equation}
    \label{eq:convergence index}
    d\seq{F,F_{\hat{\bsTheta}}} = \argsup{\x\in\set{\x_1,\cdots,\x_N}}\left| F\seq{\x} - F_{\hat{\bsTheta}}\seq{\x}\right|,
\end{equation}
where $F_{\hat{\bsTheta}}\seq{\x}=\sum_{k=1}^K \hat{\pi}_k F_{\hat{\alpha}_k,\hat{\bstheta}_k}\seq{\x}$.
\blueCN{In practice, one should choose a large enough $iterMax$ to achieve convergence, or stop the iterations when a chosen criterion is met. For example, when no} change of the estimated marginal and copula forms is observed between two iterations, and the difference of the estimated parameters is within some predefined threshold. 

\blueC{Besides the $iterMax$ parameter, the realisation time parameter $T$ also plays an important role on the convergence of GICE by affecting its smoothness. Increasing $T$ increases the size of simulated samples, based on which the distribution forms and parameters are estimated. Thus, choosing a larger $T$ leads to a smoother path of convergence. Further discussion on $T$ can be found with in the synthetic experiments (Section \ref{sec:Test on simulated data of Non-Gaussian CBMM}).}
