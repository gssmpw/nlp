\subsection{\blueC{Experiments on real data: MNIST database}}
\label{subsec:MNIST}

\blueC{This experiment expands the previous evaluation of GICE to 
%the real situation 
a real dataset of more than two clusters ($K=10$).
We tested our method on the entire MNIST database \cite{Lecun:1998} \greenC{(N=70000 samples)}, which consists of $28 \times 28$ pixels grayscale images of handwritten digits from $0$ to $9$, therefore consisting of 10 classes.}

To minimize the challenges related to analyzing such high-dimensional data, \blueC{the Uniform Manifold Approximation and Projection (UMAP) algorithm \cite{mcinnes2018umap} was used as a preprocessing step to project data into a 2D space before applying the clustering methods. UMAP can preserve both the local and global structure of data, and improves the performance of downstream clustering methods on the MNIST database, as already illustrated in several published studies. For example, \cite{allaoui2020considerably} reports the improvements of three UMAP preprocessed clustering algorithms K-Means, HDBSCAN \cite{campello2013density} and GMM. \cite{mcconville2021n2d} layers up UMAP, auto-encoder and GMM into a method called ``N2D'', and achieves a clustering accuracy of 0.979 on MNIST.} 
UMAP is sensitive to its parameter K-Nearest Neighbors (KNN), which controls the balance of local versus global structure in the data. A large KNN risks to merge the real clusters together, while a small one may lead to many subgroups of samples in the projected space. 
\blueC{In this experiment, KNN$=30$ was chosen as suggested by the documentation of UMAP to avoid producing fine grained cluster structure that may be more a result of noise patterns.}

\begin{figure}[t]
\centering
\begin{subfigure}[t]{0.5\textwidth}
  \centering
  \includegraphics[width=0.815\textwidth]{Figures/MNIST_estimatedPDF_GMM_adjusted.png}
  \captionsetup{width=.9\linewidth}
  \caption{\blueC{Cluster densities found by GMM-EM, accuracy=0.825, Kolmogorov distance=0.029.}}
  \label{fig:MNIST density GMM}
\end{subfigure}%
%\\
\begin{subfigure}[t]{0.5\textwidth}
  \centering
  \includegraphics[width=0.95\textwidth]{Figures/MNIST_estimatedPDF_CBMM_adjusted.png}
  \captionsetup{width=.95\linewidth}
  \caption{\blueC{Cluster densities found by CBMM-GICE, accuracy=0.967, Kolmogorov distance=0.011.}}
  \label{fig:MNIST density CBMM}
\end{subfigure}
\caption{\blueC{Experiment on the MNIST dataset (\greenC{N=70000 samples, }10 clusters, 2D projection obtained by UMAP with KNN=30), to evaluate the performance of GICE on more than two clusters. Each point corresponds to the image of a digit, colored by its corresponding ground truth label.}}
\label{fig:MNIST density estimation}
\end{figure}

\begin{table}[b]
    \centering
    \resizebox{0.7\columnwidth}{!}{
    \begin{tabular}{c||c|c|c||c|c|c|}
        \cline{2-7}
        & \multicolumn{3}{c||}{\blueC{Accuracy}} & \multicolumn{3}{|c|}{\blueC{Kolmogorov distance}} \\
        \cline{2-7}
        & \blueC{average} & \blueC{min} & \blueC{max} & \blueC{average} & \blueC{min} & \blueC{max} \\
        \hline
        \blueC{GMM-EN} & \blueC{0.824} & \blueC{0.690} & \blueC{0.956} & \blueC{0.025} & \blueC{0.013} & \blueC{0.055}\\
        \hline
        \blueC{CBMM-GICE} & \blueC{0.848} & \blueC{0.716} & \blueC{0.967} & \blueC{0.021} & \blueC{0.011} & \blueC{0.029}\\
        \hline
    \end{tabular}}
    \caption{\blueC{Performance of GMM-EM and CBMM-GICE on the MNIST dataset based on 20 repeated experimentation (100 max iterations for GMM and GICE, GICE realization time $T$=10, initialization: GMM).}}
    \label{tab:Performance of GMM-EM and CBMM-GICE on MNIST dataset}
\end{table}

\blueC{Table \ref{tab:Performance of GMM-EM and CBMM-GICE on MNIST dataset} reports the clustering results of 20 repeated experiments on the projected (2D) MNIST data through CBMM-GICE and GMM-EM. For easy comparison to the existing results provided in other studies, we report the accuracy as measure of performance instead of the error ratio we used in previous sections. 
We observe that both algorithms manage to identify the relevant clusters, whereas non-Gaussian distributions are slightly more desirable with better goodness of fit. The GMM-EM performance is quite close to the one (0.825) reported in \cite{mcconville2021n2d}. 
Figure \ref{fig:MNIST density estimation} shows the \blueC{2D} latent space estimated by UMAP, corresponding to the best result of GICE-CBMM in the table. Each point corresponds to the image of a digit, colored by its corresponding ground truth label. Most of the clusters are already separated by UMAP, their tight distribution offers a good condition for GMM estimation. Dealing with the overlapping clusters is more challenging, and in this experiment, GMM mistook the two clusters in the center as one entire Gaussian, and force-created two subgroups in the cluster at the lower-left corner, as illustrated in Figure \ref{fig:MNIST density GMM}.}

%\todo[inline]{ND2FZ: I don't understand the color of dots in the figure: I only see green and red dots, but colored clusters. \\ \greenC{FZ: Each cluster is with around 7000 subjects and so tightly distributed that we can't see subjects individually, the visible subjects in red and green are mislocated subjects far from their own cluster centers.} \\ ND: ok I understand ! Not so easy to convince although we're not doing classification. What about: 1) having a colorbar indicating the color for each digit? (may the three close clusters correspond to similar digits?) + 2) having e.g. 25\% or 50\% opacity for each dot? This won't hide the misclassified cases, but will better render that we have many well-identified cases in each cluster.\\ \greenC{I modified the size and opacity of each point, also a colorbar is added, it seems better?}}







