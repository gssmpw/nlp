\section{Introduction}
\label{sec:Introduction}

Finite mixture models are very relevant tools to model the distribution of data samples in a population, and in particular for unsupervised learning problems such as clustering, outlier detection, or even generating new samples from a given population \cite{mclachlan2019finite}. %\cite{mclachlan1988mixture, fruhwirth2006finite, mclachlan2019finite}.
The most widely applied mixture models are Gaussian Mixture Models (GMMs), which
assume that each subgroup of the population follows a specific multivariate Gaussian distribution. GMMs can be identified through the Expectation-Maximization (EM) algorithm which iteratively finds the local maximum data likelihood \cite{mclachlan2007algorithm}.
GMMs have been commonly chosen mainly because they offer a closed-form solution for the maximization step in the EM algorithm, and better interpretability thanks to its easy marginalization which allows visualizing the model in a given dimension. 
However, GMMs are obviously not a good choice for fitting a population composed of non-elliptical clusters as often found in medical \blueCN{applications \cite{Banfield_1993}}, since they may overestimate the number of components to approximate the real distribution \blueCN{\cite{Baudry_2010}}. 
%\todo{ND2FZ: could we think of an illustrative figure, with a synthetic population we design that would lead to 2 clusters with a GMM, and a single one with your model?}
Mixtures of alternative component densities have been explored during last decades for specific non-Gaussian application situations. For example, Weibull mixtures for learning the distribution of the cotton fiber length \cite{kuang2015generating}, Gamma or Beta mixtures for SAR image modeling \cite{ma2011bayesian, liu2019bayesian}, Dirichlet mixtures for skin color modeling \cite{bouguila2004unsupervised}, and Student's t mixtures for image segmentation \cite{sfikas2007robust}. There have also been attempts for more flexible component distribution by introducing skewness \cite{lee2014finite}, %\cite{lin2009maximum, lee2014finite}, 
or multidimensional scale variables \cite{forbes2014new}. %\cite{forbes2014new, arnaud2018fully}. 

In spite of the added flexibility offered by the above mixture models, they assume that i) all the components follow the same family of distribution, ii) all dimensions of subgroups follow the same family of marginal distribution. Some heterogeneous components mixtures have been proposed to overcome the first constraint \cite{li2016statistical, huang2019probability}. However, the second assumption may also not be satisfied in practice when performing multivariate analysis. For example in medical analytics, the blood pressure usually follows a normal or skewed normal distribution while the medical cost usually follows a log-normal distribution or a mixture of log-normal and normal distributions \cite{fujimaki2011online}. Similarly, as in one of the application examples used in this paper, myocardial infarct shapes across subjects and image slices do not necessarily follow a normal distribution, as these are constrained by the anatomy and the physiology of ischemia-reperfusion mechanisms \cite{Duchateau:FrontCV:20233}.
To address this issue, ``copulas'' have been introduced into the mixture model family \cite{nelsen2007introduction}. %\cite{fujimaki2011online, roy2014pair}. %\cite{jajuga2006copula, fujimaki2011online, roy2014pair}. 
According to Sklar's theorem, any multivariate joint distribution can be decomposed into its marginal distributions and a copula which represents the dependency structure among variables from their marginal distributions \cite{sklar1959fonctions}. %\cite{sklar1959fonctions, nelsen2007introduction}. 
The resulting copula-based mixture models (CBMMs), alternatively named ``Mixture models with Heterogeneous Marginals and Copulas (MHMC)'' \cite{fujimaki2011online}, allow us to model both heterogeneous types of marginal distributions and heterogeneous dependencies among variables separately, thus offers more flexibility and better approximation to the density of data in practice. The name ``copula-based mixture models'' is preferred here to stay brief as no confusion will be introduced.

Although the copula-based mixtures are very flexible for density estimation, \blueC{they} are not much explored so far for applications due to the difficulty of selecting marginal distributions and copulas.
Most of the work determines the forms of \blueCN{marginals} and copulas intuitively or by data analysis such as a histogram or quantile-quantile plot, then estimate the model parameters through an EM-like iterative algorithm \cite{roy2014pair}. %\cite{roy2014pair, sahin2022vine}. 
A classical way to deal with this model selection problem is to fit all combinations of possible marginal and copula forms \blueC{from a finite set of models}, then select the best-fit combination according to some criterion such as Bayesian Information Criterion (BIC). %\cite{schwarz1978estimating}. %\cite{schwarz1978estimating, wit2012all}. 
This two-step decision becomes computational-heavy when facing high-dimensional data given many candidate forms.  
\blueC{According to \cite{kosmidis2016model}, one needs to repeat the parameter estimation for $\binom{d + K -1}{K}$ combinations to finally choose the best-fit CBMM of $K$ components from a dictionary of $d = L^D M$ multivariate distributions forms,
for $D$-dimensional data,
with a candidate list of $L$ marginal forms and $M$ copula forms.} 

\subsubsection*{\blueC{Proposed approach and contributions}}

In this study, we propose an iterative algorithm for the identification of CBMMs \blueC{that overcomes these limitations. It estimates} optimized marginal and copula forms from a list of candidates as well as their parameters, by updating both the forms and parameters at each iteration. The proposed algorithm is a Generalized ICE (GICE) 
\cite{pieczynski2007convergence}
%\cite{pieczynski2007convergence, derrode2007unsupervised}
%\cite{image1992statistical, pieczynski2007convergence, provost2004hierarchical, derrode2007unsupervised} 
adapted for CBMMs inspired by the GICE algorithm for switching hidden Markov model identification \cite{zheng2020semi, derrode2016unsupervised}.
\blueC{It represents an original strategy for CBMM identification, which we thoroughly evaluate on synthetic and real data, including \blueCN{large populations from synthetic toy configurations and a popular public dataset, }and a real clinical clustering problem in medical imaging \blueCN{(infarct shape analysis from magnetic resonance images)}.}

%To the best of our knowledge, the only existing work that also considers updating the form until the convergence of \blueC{the} algorithm for CBMM identification is the ``Online EMDL'' from Fujimaki et al. \cite{fujimaki2011online}. It is quite an ``all-in-one'' algorithm which estimates not only the marginal and copula forms but also the component number in an online manner. Even so, only three candidate marginal forms from exponential family and Gaussian copulas are explored in their study, which still limits the flexibility of the resulting mixture. %\cite{berg1995indeterminate}\todo{ND2FZ: should this ref come before the dot?} 

\greenC{Similar works that also considers updating the form until the convergence of the algorithm for CBMM identification include the ``Online EMDL''\cite{fujimaki2011online} and ``VCMM''\cite{sahin2022vine}.  
Online EMDL estimates not only the marginal and copula forms but also the component number in an online manner. Even so, only three candidate marginal forms from exponential family and Gaussian copulas are explored in their study, which still limits the flexibility of the resulting mixture. VCMM is similar to GICE, while having the advantage that it can be applied on higher-dimensional data by incorporating a vine copula structure. VCMM estimates the marginal parameters at the beginning and the end of the algorithm instead of updating them continuously throughout the iterations as in GICE.}

\greenC{It is worth mentioning that recent studies have explored other flexible distribution models, such as neural copulas\cite{li2023nn} and Spline Quasi-interpolation-based methods\cite{tamborrino2024empirical}. In this article, we prefer to focus on the estimation of the conventional CBMM due to its interpretability in medical applications.}

%It is worth mentioning that there are studies on other types of copula-based models for clustering. For instance, the ``CoClust'' models inter-cluster dependence through copulas while considering no intra-cluster dependence, which is a reverse assumption of CBMMs \cite{di2012copula}. %\cite{di2012copula, di2019clustering}.

\blueC{Our} paper is structured as follows. Section \ref{sec:Copula-based mixture models} presents the CBMM and specifies the identification problem.
Section \ref{sec:Iterative conditional estimation for model identification with form selection} describes the proposed iterative conditional estimation for CBMM identification, discusses the details of its configuration and convergence.
In Section \ref{sec:Experiments}, the proposed algorithm is tested on synthetic data for different scenarios considering different method configurations \blueC{(2 clusters)}, \blueC{\blueCN{on the entire} MNIST database (10 clusters),} and then applied to real cardiac image clustering. 
Finally, Section \ref{sec:conclusion} outlines the conclusions and proposals for further research.