@inproceedings{ASL_DeafChild_HearingParent,
author = {Hossain, Ekram and Bao, Ashley and Newman, Kaleb Slater and Mann, Madeleine and Wang, Hecong and Li, Yifan and Kurumada, Chigusa and Hall, Wyatte and Bai, Zhen},
title = {Supporting ASL Communication Between Hearing Parents and Deaf Children},
year = {2023},
isbn = {9798400702204},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597638.3614511},
doi = {10.1145/3597638.3614511},
abstract = {The vast majority of deaf or hard-of-hearing (DHH) children are born to hearing parents. Due to a lack of immersive exposure to their natural language - sign language - they are at severe risk of language deprivation. In response to this challenge, this paper presents a novel computer-mediated communication platform named Tabletop Interactive Play System (TIPS). It serves as a test-bed to investigate technical and ethical solutions that enable hearing parents to use American Sign Language (ASL) during face-to-face play with their with their DHH children. The TIPS platform offers a variety of user options in three key aspects: (1) ASL recommendation in alignment with hearing parents’ real-time speech; (2) ASL display through different form-factors (Augmented Reality (AR) projection, tablet, and smart glasses); and (3) autonomy support to enhance users’ sense of agency and trust in the system. In this paper, we will describe the system’s design, implementation, and preliminary evaluation results.},
booktitle = {Proceedings of the 25th International ACM SIGACCESS Conference on Computers and Accessibility},
articleno = {61},
numpages = {5},
keywords = {American Sign Language, Augmented Reality, Computer-Mediated Communication, Deaf or Hard-of-Hearing, Parent-Child Interaction},
location = {New York, NY, USA},
series = {ASSETS '23}
}

@inproceedings{Das_ProvenanceToAberrations,
author = {Das, Maitraye and Fiannaca, Alexander J. and Morris, Meredith Ringel and Kane, Shaun K. and Bennett, Cynthia L.},
title = {From Provenance to Aberrations: Image Creator and Screen Reader User Perspectives on Alt Text for AI-Generated Images},
year = {2024},
isbn = {9798400703300},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613904.3642325},
doi = {10.1145/3613904.3642325},
abstract = {AI-generated images are proliferating as a new visual medium. However, state-of-the-art image generation models do not output alternative (alt) text with their images, rendering them largely inaccessible to screen reader users (SRUs). Moreover, less is known about what information would be most desirable to SRUs in this new medium. To address this, we invited AI image creators and SRUs to evaluate alt text prepared from various sources and write their own alt text for AI images. Our mixed-methods analysis makes three contributions. First, we highlight creators’ perspectives on alt text, as creators are well-positioned to write descriptions of their images. Second, we illustrate SRUs’ alt text needs particular to the emerging medium of AI images. Finally, we discuss the promises and pitfalls of utilizing text prompts written as input for AI models in alt text generation, and areas where broader digital accessibility guidelines could expand to account for AI images.},
booktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},
articleno = {900},
numpages = {21},
keywords = {AI art, Accessibility, Alt text, Blind, Screen reader users, Text-to-Image},
location = {Honolulu, HI, USA},
series = {CHI '24}
}

@inproceedings{Dietz_ContextQ,
author = {Dietz Smith, Griffin and Prasad, Siddhartha and Davidson, Matt J. and Findlater, Leah and Shapiro, R. Benjamin},
title = {ContextQ: Generated Questions to Support Meaningful Parent-Child Dialogue While Co-Reading},
year = {2024},
isbn = {9798400704420},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3628516.3655809},
doi = {10.1145/3628516.3655809},
abstract = {Much of early literacy education happens at home with caretakers reading books to young children. Prior research demonstrates how having dialogue with children during co-reading can develop critical reading readiness skills, but most adult readers are unsure if and how to lead effective conversations. We present ContextQ, a tablet-based reading application to unobtrusively present auto-generated dialogic questions to caretakers to support this dialogic reading practice. An ablation study demonstrates how our method of encoding educator expertise into the question generation pipeline can produce high-quality output; and through a user study with 12 parent-child dyads (child age: 4–6), we demonstrate that this system can serve as a guide for parents in leading contextually meaningful dialogue, leading to significantly more conversational turns from both the parent and the child and deeper conversations with connections to the child’s everyday life.},
booktitle = {Proceedings of the 23rd Annual ACM Interaction Design and Children Conference},
pages = {408–423},
numpages = {16},
keywords = {co-reading, dialogic reading, large language models, literacy, parent-child interaction, question generation},
location = {Delft, Netherlands},
series = {IDC '24}
}

@inproceedings{Glazko_Autoethnography,
author = {Glazko, Kate S and Yamagami, Momona and Desai, Aashaka and Mack, Kelly Avery and Potluri, Venkatesh and Xu, Xuhai and Mankoff, Jennifer},
title = {An Autoethnographic Case Study of Generative Artificial Intelligence's Utility for Accessibility},
year = {2023},
isbn = {9798400702204},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597638.3614548},
doi = {10.1145/3597638.3614548},
abstract = {With the recent rapid rise in Generative Artificial Intelligence (GAI) tools, it is imperative that we understand their impact on people with disabilities, both positive and negative. However, although we know that AI in general poses both risks and opportunities for people with disabilities, little is known specifically about GAI in particular. To address this, we conducted a three-month autoethnography of our use of GAI to meet personal and professional needs as a team of researchers with and without disabilities. Our findings demonstrate a wide variety of potential accessibility-related uses for GAI while also highlighting concerns around verifiability, training data, ableism, and false promises.},
booktitle = {Proceedings of the 25th International ACM SIGACCESS Conference on Computers and Accessibility},
articleno = {99},
numpages = {8},
keywords = {ableism, accessibility, auto-ethnography, generative artificial intelligence},
location = {New York, NY, USA},
series = {ASSETS '23}
}

@inproceedings{Gubbi_ContextAware,
author = {Gubbi Mohanbabu, Ananya and Pavel, Amy},
title = {Context-Aware Image Descriptions for Web Accessibility},
year = {2024},
isbn = {9798400706776},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3663548.3675658},
doi = {10.1145/3663548.3675658},
abstract = {Blind and low vision (BLV) internet users access images on the web via text descriptions. New vision-to-language models such as GPT-V, Gemini, and LLaVa can now provide detailed image descriptions on-demand. While prior research and guidelines state that BLV audiences’ information preferences depend on the context of the image, existing tools for accessing vision-to-language models provide only context-free image descriptions by generating descriptions for the image alone without considering the surrounding webpage context. To explore how to integrate image context into image descriptions, we designed a Chrome Extension that automatically extracts webpage context to inform GPT-4V-generated image descriptions. We gained feedback from 12 BLV participants in a user study comparing typical context-free image descriptions to context-aware image descriptions. We then further evaluated our context-informed image descriptions with a technical evaluation. Our user evaluation demonstrates that BLV participants frequently prefer context-aware descriptions to context-free descriptions. BLV participants also rate context-aware descriptions significantly higher in quality, imaginability, relevance, and plausibility. All participants shared that they wanted to use context-aware descriptions in the future and highlighted the potential for use in online shopping, social media, news, and personal interest blogs.},
booktitle = {Proceedings of the 26th International ACM SIGACCESS Conference on Computers and Accessibility},
articleno = {62},
numpages = {17},
keywords = {Accessibility, Context Awareness, Image Descriptions, Text;},
location = {St. John's, NL, Canada},
series = {ASSETS '24}
}

@inproceedings{Hossain_ContextResponsiveASL,
author = {Hossain, Ekram and Cahoon, Merritt Lee and Liu, Yao and Kurumada, Chigusa and Bai, Zhen},
title = {Context-responsive ASL Recommendation for Parent-Child Interaction},
year = {2022},
isbn = {9781450392587},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3517428.3550366},
doi = {10.1145/3517428.3550366},
abstract = {Parental language input in early childhood plays a critical role in lifelong neuro-cognitive and social development. Deaf and Hard of Hearing (DHH) children are often at risk of language deprivation due to hearing parents’ limited knowledge of sign language - the natural language for DHH children at birth. To offer an immersive sign language environment for DHH children, we designed a novel computer-mediated communication technology named Table Top Interactive System (TIPS). It aims to provide context-responsive recommendation of American Sign Language (ASL) in real-time for hearing parents during face-to-face joint play with their DHH children. The system emphasizes supporting parent autonomy by adapting ASL recommendations using parent’s speech during play, and minimizes obtrusion for face-to-face interaction through an Augmented Reality (AR) display. This paper describes the design and development of an initial working prototype of TIPS and preliminary results of the system’s efficiency regarding system latency and accuracy for ASL recommendation and visualization. Next, we plan to conduct a user study to gather expert and parent feedback about the system design and ASL recommendation strategies for long-term and personalized usage.},
booktitle = {Proceedings of the 24th International ACM SIGACCESS Conference on Computers and Accessibility},
articleno = {76},
numpages = {5},
keywords = {American Sign Language, Augmented Reality, Computer-mediated communication, Parent-Child Interaction},
location = {Athens, Greece},
series = {ASSETS '22}
}

@inproceedings{Huh_GenAssist,
author = {Huh, Mina and Peng, Yi-Hao and Pavel, Amy},
title = {GenAssist: Making Image Generation Accessible},
year = {2023},
isbn = {9798400701320},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3586183.3606735},
doi = {10.1145/3586183.3606735},
abstract = {Blind and low vision (BLV) creators use images to communicate with sighted audiences. However, creating or retrieving images is challenging for BLV creators as it is difficult to use authoring tools or assess image search results. Thus, creators limit the types of images they create or recruit sighted collaborators. While text-to-image generation models let creators generate high-fidelity images based on a text description (i.e. prompt), it is difficult to assess the content and quality of generated images. We present GenAssist, a system to make text-to-image generation accessible. Using our interface, creators can verify whether generated image candidates followed the prompt, access additional details in the image not specified in the prompt, and skim a summary of similarities and differences between image candidates. To power the interface, GenAssist uses a large language model to generate visual questions, vision-language models to extract answers, and a large language model to summarize the results. Our study with 12 BLV creators demonstrated that GenAssist enables and simplifies the process of image selection and generation, making visual authoring more accessible to all.},
booktitle = {Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology},
articleno = {38},
numpages = {17},
keywords = {Accessibility, Creativity Support Tools, Generative AI, Image Generation},
location = {San Francisco, CA, USA},
series = {UIST '23}
}

@inproceedings{Kim_DescriptionsForComics,
author = {Kim, Suhyun and Lee, Semin and Kim, Kyungok and Oh, Uran},
title = {Utilizing a Dense Video Captioning Technique for Generating Image Descriptions of Comics for People with Visual Impairments},
year = {2024},
isbn = {9798400705083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3640543.3645154},
doi = {10.1145/3640543.3645154},
abstract = {To improve the accessibility of visual figures, auto-generation of text description of individual images has been studied. However, it cannot be directly applied to comics as the descriptions can be redundant as similar scenes appear in a row. To address this issue, we propose generating the descriptions per group of related images and demonstrate how an dense captioning technique for videos can be utilized for this purpose and ways to improve its performance. To assess the effectiveness of our approach and to identify factors affecting the quality of text descriptions of comics, we conducted a preliminary study with 3 sighted evaluators and a main user study with 12 participants with visual impairments. The results show that text descriptions generated per group of images are perceived to be better than those generated per image in terms of accuracy, clarity, understandability, length, informativeness and preference for sighted groups, when annotator is human. In the same conditions, when the annotator is AI, it exhibited better performance in terms of length. Also, people with visual impairments prefer group descriptions because of conciseness, smooth connectivity of sentences, and non-repetitive features. Based on the findings, we provide design recommendations for generating accessible comic descriptions at a scale for blind users.},
booktitle = {Proceedings of the 29th International Conference on Intelligent User Interfaces},
pages = {750–760},
numpages = {11},
keywords = {comics, dense video captioning, image description, people with visual impairment},
location = {Greenville, SC, USA},
series = {IUI '24}
}

@inproceedings{Lee_RSAs,
author = {Lee, Sooyeon and Yu, Rui and Xie, Jingyi and Billah, Syed Masum and Carroll, John M.},
title = {Opportunities for Human-AI Collaboration in Remote Sighted Assistance},
year = {2022},
isbn = {9781450391443},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3490099.3511113},
doi = {10.1145/3490099.3511113},
abstract = {Remote sighted assistance (RSA) has emerged as a conversational assistive technology for people with visual impairments (VI), where remote sighted agents provide realtime navigational assistance to users with visual impairments via video-chat-like communication. In this paper, we conducted a literature review and interviewed 12 RSA users to comprehensively understand technical and navigational challenges in RSA for both the agents and users. Technical challenges are organized into four categories: agents’ difficulties in orienting and localizing the users; acquiring the users’ surroundings and detecting obstacles; delivering information and understanding user-specific situations; and coping with a poor network connection. Navigational challenges are presented in 15 real-world scenarios (8 outdoor, 7 indoor) for the users. Prior work indicates that computer vision (CV) technologies, especially interactive 3D maps and realtime localization, can address a subset of these challenges. However, we argue that addressing the full spectrum of these challenges warrants new development in Human-CV collaboration, which we formalize as five emerging problems: making object recognition and obstacle avoidance algorithms blind-aware; localizing users under poor networks; recognizing digital content on LCD screens; recognizing texts on irregular surfaces; and predicting the trajectory of out-of-frame pedestrians or objects. Addressing these problems can advance computer vision research and usher into the next generation of RSA service.},
booktitle = {Proceedings of the 27th International Conference on Intelligent User Interfaces},
pages = {63–78},
numpages = {16},
keywords = {3D maps, RSA, artificial intelligence, augmented reality, blind, camera, computer vision, conversational assistance, navigation, people with visual impairments, remote sighted assistance, smartphone},
location = {Helsinki, Finland},
series = {IUI '22}
}

@inproceedings{Lin_FishScales,
author = {Lin, Grace C. and Schoenfeld, Ilana and Thompson, Meredith and Xia, Yiting and Uz-Bilgin, Cigdem and Leech, Kathryn},
title = {”What color are the fish’s scales?” Exploring parents’ and children’s natural interactions with a child-friendly virtual agent during storybook reading},
year = {2022},
isbn = {9781450391979},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3501712.3529734},
doi = {10.1145/3501712.3529734},
abstract = {With increasing integration of AI-powered agents into educational technologies available to families with young children, the landscape of how caregivers and children interact together or separately with these technologies is underexplored. Understanding the nature of these interactions could critically inform the design of educational technologies to facilitate children’s learning, research methods for use in future studies involving adult-child dyad technology use, and policy decisions regarding the use of educational technology with young children. In this study, we explored the natural interactions among parent, child, and a child-friendly virtual rabbit character named Floppy. Floppy is a virtual agent in a Smart Speaker app that models adult dialogic reading and conversational strategies for use with young children (ages 4-6 years). Over a span of four to six weeks, 18 parent-child dyads read The Rainbow Fish, a classic children’s book by Marcus Pfister, during 24 at-home, remote sessions with Floppy. Of the 189 conversations generated during this time, 125 were initiated by a prompt spoken by Floppy. Though there were some variations among the dyads, across all conversations, parent-driven interactions made up 63\% of the conversations, followed by child-driven conversations at 15.3\%, Floppy-driven at 14.3\%, and Floppy-and-parent-driven at 7.4\%. A select few parents were more comfortable having their children interact directly with Floppy, whereas the majority of the parents would direct children’s attention back to themselves or help children understand the questions by repeating or reformulating Floppy’s prompts. More than half of the parents reported that their children formed emotional connections with the virtual character. These findings point to a need to clearly define the role of virtual agents, even ones with limited AI, in this type of triadic interaction.},
booktitle = {Proceedings of the 21st Annual ACM Interaction Design and Children Conference},
pages = {185–195},
numpages = {11},
keywords = {virtual agent, triadic interaction, early literacy},
location = {Braga, Portugal},
series = {IDC '22}
}

@inproceedings{Morrison_FindMyThings,
author = {Morrison, Cecily and Grayson, Martin and Marques, Rita Faia and Massiceti, Daniela and Longden, Camilla and Wen, Linda and Cutrell, Edward},
title = {Understanding Personalized Accessibility through Teachable AI: Designing and Evaluating Find My Things for People who are Blind or Low Vision},
year = {2023},
isbn = {9798400702204},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597638.3608395},
doi = {10.1145/3597638.3608395},
abstract = {The opportunity for artificial intelligence, or AI, to enable accessibility is rapidly growing, but widely impactful applications can be challenging to build given the diversity of user need within and across disability communities. Teachable AI systems give users with disabilities a way to leverage the power of AI to personalize applications for their own specific needs, as long as the effort of providing examples is balanced with the benefit of the personalization received. As an example, this paper presents the design and evaluation of Find My Things, an end-to-end application that can be taught by people who are blind or low vision to find their personal things. Through synthesis of the design process, this paper offers design considerations for the teaching loop that is so critical to realizing the power of teachable AI for accessibility.},
booktitle = {Proceedings of the 25th International ACM SIGACCESS Conference on Computers and Accessibility},
articleno = {31},
numpages = {12},
keywords = {Accessibility, Artificial Intelligence, Teachable AI},
location = {New York, NY, USA},
series = {ASSETS '23}
}

@inproceedings{Park_AllAboutPictures_2023,
author = {Park, Sohyeon and Cassidy, Cameron Tyler and Branham, Stacy M.},
title = {“It’s All About the Pictures:” Understanding How Parents/Guardians With Visual Impairments Co-Read With Their Child(ren)},
year = {2023},
isbn = {9798400702204},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597638.3614488},
doi = {10.1145/3597638.3614488},
abstract = {Co-reading, an activity where adults collaboratively read books with child(ren), is important for literacy learning and forming human connection. However, parents and guardians with visual impairments do not experience the same level of access to resources when co-reading with their child(ren) as their sighted counterparts, especially as regards images in children’s books. Through conducting an interview study with five visually impaired parents/guardians, we illuminate the importance parents place on images in children’s books, how they access visual information in children’s print books, and the potential of smart speakers in assisting their existing co-reading practices.},
booktitle = {Proceedings of the 25th International ACM SIGACCESS Conference on Computers and Accessibility},
articleno = {88},
numpages = {4},
keywords = {accessibility, blind or low vision, co-reading, parents/caregivers},
location = {New York, NY, USA},
series = {ASSETS '23}
}

@inproceedings{Singh_FigureA11y,
author = {Singh, Nikhil and Wang, Lucy Lu and Bragg, Jonathan},
title = {FigurA11y: AI Assistance for Writing Scientific Alt Text},
year = {2024},
isbn = {9798400705083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3640543.3645212},
doi = {10.1145/3640543.3645212},
abstract = {High-quality alt text is crucial for making scientific figures accessible to blind and low-vision readers. Crafting complete, accurate alt text is challenging even for domain experts, as published figures often depict complex visual information and readers have varied informational needs. These challenges, along with high diversity in figure types and domain-specific details, also limit the usefulness of fully automated approaches. Consequently, the prevalence of high-quality alt text is very low in scientific papers today. We investigate whether and how human-AI collaborative editing systems can help address the difficulty of writing high-quality alt text for complex scientific figures. We present FigurA11y, an interactive system that generates draft alt text and provides suggestions for author revisions using a pipeline driven by extracted figure and paper metadata. We test two versions, motivated by prior work on visual accessibility and writing support. The base Draft+Revise&nbsp;version provides authors with an automatically generated draft description to revise, along with extracted figure metadata and figure-specific alt text guidelines to support the revision process. The full Interactive Assistance&nbsp;version further adds contextualized suggestions: text snippets to iteratively produce descriptions, and hypothetical user questions with possible answers to reveal potential ambiguities and resolutions. In a study of authors (N=14), we found the system assisted them in efficiently producing descriptive alt text. Generated drafts and interface elements enabled authors to quickly initiate and edit detailed descriptions. Additionally, interactive suggestions from the full system prompted more iteration and highlighted aspects for authors to consider, resulting in greater deviation from the drafts without increased average cognitive load or manual effort.},
booktitle = {Proceedings of the 29th International Conference on Intelligent User Interfaces},
pages = {886–906},
numpages = {21},
keywords = {Accessibility, Alt text, Human-AI interaction, Image descriptions, Large language models, Natural language generation, Scientific figures, Writing assistance systems},
location = {Greenville, SC, USA},
series = {IUI '24}
}

@inproceedings{Xu_Bilingual,
author = {Xu, Ying and He, Kunlei and Vigil, Valery and Ojeda-Ramirez, Santiago and Liu, Xuechen and Levine, Julian and Cervera, Kelsyann and Warschauer, Mark},
title = {“Rosita Reads With My Family”: Developing A Bilingual Conversational Agent to Support Parent-Child Shared Reading},
year = {2023},
isbn = {9798400701313},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3585088.3589354},
doi = {10.1145/3585088.3589354},
abstract = {Bilingual children have unique needs for school readiness as they navigate between two languages and cultures. A supportive home language environment, where children are frequently exposed to language through conversation and reading, can positively impact their language development and prepare them for school. However, current conversational agents and e-books designed for children do not typically take into account the cultural and linguistic needs of bilingual children and do not involve parents. This project presents the development of a bilingual conversational agent and accompanying e-book, designed to support parent-child interactions and promote language development for Latinx Spanish-English bilingual children. Results from a user study indicate that the bilingual agent effectively engages children verbally and encourages parental involvement in reading processes. The study also provides design insights for creating conversational agents for bilingual children.},
booktitle = {Proceedings of the 22nd Annual ACM Interaction Design and Children Conference},
pages = {160–172},
numpages = {13},
keywords = {Latinx, bilingualism, conversational agent, culturally-responsive design, parent-child interaction, shared reading},
location = {Chicago, IL, USA},
series = {IDC '23}
}

@inproceedings{Zhang_StoryBuddy,
author = {Zhang, Zheng and Xu, Ying and Wang, Yanhao and Yao, Bingsheng and Ritchie, Daniel and Wu, Tongshuang and Yu, Mo and Wang, Dakuo and Li, Toby Jia-Jun},
title = {StoryBuddy: A Human-AI Collaborative Chatbot for Parent-Child Interactive Storytelling with Flexible Parental Involvement},
year = {2022},
isbn = {9781450391573},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3491102.3517479},
doi = {10.1145/3491102.3517479},
abstract = {Despite its benefits for children’s skill development and parent-child bonding, many parents do not often engage in interactive storytelling by having story-related dialogues with their child due to limited availability or challenges in coming up with appropriate questions. While recent advances made AI generation of questions from stories possible, the fully-automated approach excludes parent involvement, disregards educational goals, and underoptimizes for child engagement. Informed by need-finding interviews and participatory design (PD) results, we developed StoryBuddy, an AI-enabled system for parents to create interactive storytelling experiences. StoryBuddy’s design highlighted the need for accommodating dynamic user needs between the desire for parent involvement and parent-child bonding and the goal of minimizing parent intervention when busy. The PD revealed varied assessment and educational goals of parents, which StoryBuddy addressed by supporting configuring question types and tracking child progress. A user study validated StoryBuddy’s usability and suggested design insights for future parent-AI collaboration systems.},
booktitle = {Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems},
articleno = {218},
numpages = {21},
keywords = {child-agent interactions, co-reading, dialogic reading, human-AI collaboration, interactive storytelling, voice user interfaces},
location = {New Orleans, LA, USA},
series = {CHI '22}
}

@misc{be_my_ai_2024,  title={Introducing: Be My AI},  url={https://www.bemyeyes.com/blog/introducing-be-my-ai},  year={2024},  month={Apr} }

@misc{be_my_eyes_2024,  title={The story about Be My Eyes},  url={https://www.bemyeyes.com/about},  year={2024},  month={Apr} }

@inproceedings{bennett_itscomplicated,
author = {Bennett, Cynthia L. and Gleason, Cole and Scheuerman, Morgan Klaus and Bigham, Jeffrey P. and Guo, Anhong and To, Alexandra},
title = {“It’s Complicated”: Negotiating Accessibility and (Mis)Representation in Image Descriptions of Race, Gender, and Disability},
year = {2021},
isbn = {9781450380966},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411764.3445498},
doi = {10.1145/3411764.3445498},
abstract = {Content creators are instructed to write textual descriptions of visual content to make it accessible; yet existing guidelines lack specifics on how to write about people’s appearance, particularly while remaining mindful of consequences of (mis)representation. In this paper, we report on interviews with screen reader users who were also Black, Indigenous, People of Color, Non-binary, and/or Transgender on their current image description practices and preferences, and experiences negotiating theirs and others’ appearances non-visually. We discuss these perspectives, and the ethics of humans and AI describing appearance characteristics that may convey the race, gender, and disabilities of those photographed. In turn, we share considerations for more carefully describing appearance, and contexts in which such information is perceived salient. Finally, we offer tensions and questions for accessibility research to equitably consider politics and ecosystems in which technologies will embed, such as potential risks of human and AI biases amplifying through image descriptions.},
booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},
articleno = {375},
numpages = {19},
keywords = {AI, Accessibility, Blind, Disability, Gender, Image descriptions, Race, Visual impairments},
location = {Yokohama, Japan},
series = {CHI '21}
}

@inproceedings{chhedakothary2024,
author = {Chheda-Kothary, Arnavi and Wobbrock, Jacob O. and Froehlich, Jon E.},
title = {Engaging with Children's Artwork in Mixed Visual-Ability Families},
year = {2024},
isbn = {9798400706776},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3663548.3675613},
doi = {10.1145/3663548.3675613},
abstract = {We present two studies exploring how blind or low-vision (BLV) family members engage with their sighted children’s artwork, strategies to support understanding and interpretation, and the potential role of technology, such as AI, therein. Our first study involved 14 BLV individuals, and the second included five groups of BLV individuals with their children. Through semi-structured interviews with AI descriptions of children’s artwork and multi-sensory design probes, we found that BLV family members value artwork engagement as a bonding opportunity, preferring the child’s storytelling and interpretation over other nonvisual representations. Additionally, despite some inaccuracies, BLV family members felt that AI-generated descriptions could facilitate dialogue with their children and aid self-guided art discovery. We close with specific design considerations for supporting artwork engagement in mixed visual-ability families, including enabling artwork access through various methods, supporting children’s corrections of AI output, and distinctions in context vs. content and interpretation vs. description of children’s artwork.},
booktitle = {Proceedings of the 26th International ACM SIGACCESS Conference on Computers and Accessibility},
articleno = {3},
numpages = {19},
keywords = {AI, Accessibility, blind or low-vision, children’s artwork, mixed-ability families},
location = {St. John's, NL, Canada},
series = {ASSETS '24}
}

@inproceedings{cuddlingup,
author = {Cassidy, Cameron Tyler and Figueira, Isabela and Park, Sohyeon and Kim, Jin Seo and Edwards, Emory James and Branham, Stacy Marie},
title = {Cuddling Up With a Print-Braille Book: How Intimacy and Access Shape Parents' Reading Practices with Children},
year = {2024},
isbn = {9798400703300},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613904.3642763},
doi = {10.1145/3613904.3642763},
abstract = {Like many parents, visually impaired parents (VIPs) read books with their children. However, research on accessible reading technologies predominantly focuses on blind adults reading alone or sighted adults reading with blind children, such that the motivations, strategies, and needs of blind parents reading with their sighted children are still largely undocumented. To address this gap, we interviewed 13 VIPs with young children. We found that VIPs (1) sought familial intimacy through reading with their child, often prioritizing intimacy over their own access needs, (2) took on many types of access labor to read with their children, and (3) desired novel assistive technologies (ATs) for reading that prioritize intimacy while reducing access labor. We contribute the notion of Intimate AT, along with a demonstrative design space, which together constitute a new design paradigm that draws attention to intimacy as a facet of both independently and collaboratively accessible ATs.},
booktitle = {Proceedings of the CHI Conference on Human Factors in Computing Systems},
articleno = {125},
numpages = {15},
keywords = {accessibility, blind, books, co-reading, eBooks, low vision, voice assistants},
location = {Honolulu, HI, USA},
series = {CHI '24}
}

@inproceedings{darth_vader,
author = {Newman, Michele and Sun, Kaiwen and Dalla Gasperina, Ilena B and Shin, Grace Y. and Pedraja, Matthew Kyle and Kanchi, Ritesh and Song, Maia B. and Li, Rannie and Lee, Jin Ha and Yip, Jason},
title = {"I want it to talk like Darth Vader": Helping Children Construct Creative Self-Efficacy with Generative AI},
year = {2024},
isbn = {9798400703300},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613904.3642492},
doi = {10.1145/3613904.3642492},
abstract = {The emergence of generative artificial intelligence (GenAI) has ignited discussions surrounding its potential to enhance creative pursuits. However, distinctions between children’s and adult’s creative needs exist, which is important when considering the possibility of GenAI for children’s creative usage. Building upon work in Human-Computer Interaction (HCI), fostering children’s computational thinking skills, this study explores interactions between children (aged 7-13) and GenAI tools through methods of participatory design. We seek to answer two questions: (1) How do children in co-design workshops perceive GenAI tools and their usage for creative works? and (2) How do children navigate the creative process while using GenAI tools? How might these interactions support their confidence in their ability to create? Our findings contribute a model that describes the potential contexts underpinning child-GenAI creative interactions and explores implications of this model for theories of creativity, design, and use of GenAI as a constructionist tool for creative self-efficacy.},
booktitle = {Proceedings of the CHI Conference on Human Factors in Computing Systems},
articleno = {117},
numpages = {18},
keywords = {Artificial Intelligence, Children, Co-Design, Constructionism, Creativity, Participatory Design},
location = {Honolulu, HI, USA},
series = {CHI '24}
}

@inproceedings{gonzalez_CHI,
author = {Gonzalez Penuela, Ricardo E and Collins, Jazmin and Bennett, Cynthia and Azenkot, Shiri},
title = {Investigating Use Cases of AI-Powered Scene Description Applications for Blind and Low Vision People},
year = {2024},
isbn = {9798400703300},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613904.3642211},
doi = {10.1145/3613904.3642211},
abstract = {"Scene description" applications that describe visual content in a photo are useful daily tools for blind and low vision (BLV) people. Researchers have studied their use, but they have only explored those that leverage remote sighted assistants; little is known about applications that use AI to generate their descriptions. Thus, to investigate their use cases, we conducted a two-week diary study where 16 BLV participants used an AI-powered scene description application we designed. Through their diary entries and follow-up interviews, users shared their information goals and assessments of the visual descriptions they received. We analyzed the entries and found frequent use cases, such as identifying visual features of known objects, and surprising ones, such as avoiding contact with dangerous objects. We also found users scored the descriptions relatively low on average, 2.76 out of 5 (SD=1.49) for satisfaction and 2.43 out of 4 (SD=1.16) for trust, showing that descriptions still need significant improvements to deliver satisfying and trustworthy experiences. We discuss future opportunities for AI as it becomes a more powerful accessibility tool for BLV users.},
booktitle = {Proceedings of the CHI Conference on Human Factors in Computing Systems},
articleno = {901},
numpages = {21},
keywords = {AI, Assistive Technology, Blind and Low Vision People, Computer Vision, Diary Study, Scene Description, Use cases},
location = {Honolulu, HI, USA},
series = {CHI '24}
}

@inproceedings{hwang_creative,
author = {Hwang, Angel Hsing-Chi},
title = {Too Late to be Creative? AI-Empowered Tools in Creative Processes},
year = {2022},
isbn = {9781450391566},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3491101.3503549},
doi = {10.1145/3491101.3503549},
abstract = {The present case study examines the product landscape of current AI-empowered co-creative tools. Specifically, I review literature in both creativity and HCI research and investigate how these tools support different stages in humans’ creative processes and how common challenges in human-AI interaction (HAII) are addressed. I find these AI-driven tools mostly support the generation and execution of ideas and are less involved in the early stages of co-creation. Moreover, HAII challenges identified in other fields receive little attention in the creative domain. Based on a synthetic analysis, I elaborate on how future tools can leverage the ”non-human” quality of AI to achieve innovation through a more human-centered, collaborative journey.},
booktitle = {Extended Abstracts of the 2022 CHI Conference on Human Factors in Computing Systems},
articleno = {38},
numpages = {9},
keywords = {human-AI interaction, creativity support tool, creativity},
location = {New Orleans, LA, USA},
series = {CHI EA '22}
}

@inproceedings{jain2024streetnavleveragingstreetcameras,
author = {Jain, Gaurav and Hindi, Basel and Zhang, Zihao and Srinivasula, Koushik and Xie, Mingyu and Ghasemi, Mahshid and Weiner, Daniel and Paris, Sophie Ana and Xu, Xin Yi Therese and Malcolm, Michael and Turkcan, Mehmet Kerem and Ghaderi, Javad and Kostic, Zoran and Zussman, Gil and Smith, Brian A.},
title = {StreetNav: Leveraging Street Cameras to Support Precise Outdoor Navigation for Blind Pedestrians},
year = {2024},
isbn = {9798400706288},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3654777.3676333},
doi = {10.1145/3654777.3676333},
abstract = {Blind and low-vision (BLV) people rely on GPS-based systems for outdoor navigation. GPS’s inaccuracy, however, causes them to veer off track, run into obstacles, and struggle to reach precise destinations. While prior work has made precise navigation possible indoors via hardware installations, enabling this outdoors remains a challenge. Interestingly, many outdoor environments are already instrumented with hardware such as street cameras. In this work, we explore the idea of repurposing existing street cameras for outdoor navigation. Our community-driven approach considers both technical and sociotechnical concerns through engagements with various stakeholders: BLV users, residents, business owners, and Community Board leadership. The resulting system, StreetNav, processes a camera’s video feed using computer vision and gives BLV pedestrians real-time navigation assistance. Our evaluations show that StreetNav guides users more precisely than GPS, but its technical performance is sensitive to environmental occlusions and distance from the camera. We discuss future implications for deploying such systems at scale.},
booktitle = {Proceedings of the 37th Annual ACM Symposium on User Interface Software and Technology},
articleno = {139},
numpages = {21},
keywords = {Visual impairments, computer vision, outdoor navigation, street camera},
location = {Pittsburgh, PA, USA},
series = {UIST '24}
}

