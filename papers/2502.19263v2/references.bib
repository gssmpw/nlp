@inproceedings{Stangl3DPrintIDC14,
author = {Stangl, Abigale and Kim, Jeeeun and Yeh, Tom},
title = {3D printed tactile picture books for children with visual impairments: a design probe},
year = {2014},
isbn = {9781450322720},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2593968.2610482},
doi = {10.1145/2593968.2610482},
abstract = {Young children with visual impairments greatly benefit from tactile graphics (illustrations, images, puzzles, objects) during their learning processes. In this paper we present insight about using a 3D printed tactile picture book as a design probe. This has allowed us to identify and engage stakeholders in our research on improving the technical and human processes required for creating 3D printed tactile pictures, and cultivate a community of practice around these processes. We also contribute insight about how our inperson and digital methods of interacting with teachers, parents, and other professionals dedicated to supporting children with visual impairments contributes to research practices.},
booktitle = {Proceedings of the 2014 Conference on Interaction Design and Children},
pages = {321–324},
numpages = {4},
keywords = {tactile picture books, tactile graphics, design probe, communities of practice, children, blind},
location = {Aarhus, Denmark},
series = {IDC '14}
}

@inproceedings{GAI_DesignPrinciples,
author = {Weisz, Justin D. and He, Jessica and Muller, Michael and Hoefer, Gabriela and Miles, Rachel and Geyer, Werner},
title = {Design Principles for Generative AI Applications},
year = {2024},
isbn = {9798400703300},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613904.3642466},
doi = {10.1145/3613904.3642466},
abstract = {Generative AI applications present unique design challenges. As generative AI technologies are increasingly being incorporated into mainstream applications, there is an urgent need for guidance on how to design user experiences that foster effective and safe use. We present six principles for the design of generative AI applications that address unique characteristics of generative AI UX and offer new interpretations and extensions of known issues in the design of AI applications. Each principle is coupled with a set of design strategies for implementing that principle via UX capabilities or through the design process. The principles and strategies were developed through an iterative process involving literature review, feedback from design practitioners, validation against real-world generative AI applications, and incorporation into the design process of two generative AI applications. We anticipate the principles to usefully inform the design of generative AI applications by driving actionable design recommendations.},
booktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},
articleno = {378},
numpages = {22},
keywords = {Generative AI, design principles, foundation models, human-centered AI},
location = {Honolulu, HI, USA},
series = {CHI '24}
}

@article{OrdinalRegression,
 ISSN = {0006341X, 15410420},
 URL = {http://www.jstor.org/stable/2533433},
 abstract = {A random-effects ordinal regression model is proposed for analysis of clustered or longitudinal ordinal response data. This model is developed for both the probit and logistic response functions. The threshold concept is used, in which it is assumed that the observed ordered category is determined by the value of a latent unobservable continuous response that follows a linear regression model incorporating random effects. A maximum marginal likelihood (MML) solution is described using Gauss-Hermite quadrature to numerically integrate over the distribution of random effects. An analysis of a dataset where students are clustered or nested within classrooms is used to illustrate features of random-effects analysis of clustered ordinal data, while an analysis of a longitudinal dataset where psychiatric patients are repeatedly rated as to their severity is used to illustrate features of the random-effects approach for longitudinal ordinal data.},
 author = {Donald Hedeker and Robert D. Gibbons},
 journal = {Biometrics},
 number = {4},
 pages = {933--944},
 publisher = {International Biometric Society},
 title = {A Random-Effects Ordinal Regression Model for Multilevel Analysis},
 urldate = {2024-10-10},
 volume = {50},
 year = {1994}
}


@article{GLMM,
author = {N. E. Breslow and D. G. Clayton},
title = {Approximate Inference in Generalized Linear Mixed Models},
journal = {Journal of the American Statistical Association},
volume = {88},
number = {421},
pages = {9--25},
year = {1993},
publisher = {ASA Website},
doi = {10.1080/01621459.1993.10594284},
URL = {  
    https://doi.org/10.1080/01621459.1993.10594284

},
eprint = { 
   https://doi.org/10.1080/01621459.1993.10594284
}
}

@inproceedings{MagicCamera,
author = {Zhao, Yijun and Cheng, Yiming and Ding, Shiying and Fang, Yan and Cao, Wei and Liu, Ke and Cao, Jiacheng},
title = {Magic Camera: An AI Drawing Game Supporting Instantaneous Story Creation for Children},
year = {2024},
isbn = {9798400704420},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3628516.3659386},
doi = {10.1145/3628516.3659386},
abstract = {Storytelling plays a crucial role in the development of children’s comprehensive abilities. This paper presents an innovative AI drawing game designed to enhance children’s capabilities in using metaphors and storytelling. The game employs a unique approach, allowing children to capture scenes or objects from the real world using a built-in camera and describe them via voice input, which then generates corresponding images on the interface. The AI drawing module of the game, developed based on the LCM model, converts children’s descriptions into images in real time, offering a dynamic and almost instantaneous storytelling experience. This study explores the impact of the game on children’s creativity and narrative skills, presenting a novel way of integrating AI into child development.},
booktitle = {Proceedings of the 23rd Annual ACM Interaction Design and Children Conference},
pages = {738–743},
numpages = {6},
keywords = {Children, Creativity support tool, Human-AI collaboration, Storytelling},
location = {Delft, Netherlands},
series = {IDC '24}
}

@inproceedings{cuddlingup,
author = {Cassidy, Cameron Tyler and Figueira, Isabela and Park, Sohyeon and Kim, Jin Seo and Edwards, Emory James and Branham, Stacy Marie},
title = {Cuddling Up With a Print-Braille Book: How Intimacy and Access Shape Parents' Reading Practices with Children},
year = {2024},
isbn = {9798400703300},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613904.3642763},
doi = {10.1145/3613904.3642763},
abstract = {Like many parents, visually impaired parents (VIPs) read books with their children. However, research on accessible reading technologies predominantly focuses on blind adults reading alone or sighted adults reading with blind children, such that the motivations, strategies, and needs of blind parents reading with their sighted children are still largely undocumented. To address this gap, we interviewed 13 VIPs with young children. We found that VIPs (1) sought familial intimacy through reading with their child, often prioritizing intimacy over their own access needs, (2) took on many types of access labor to read with their children, and (3) desired novel assistive technologies (ATs) for reading that prioritize intimacy while reducing access labor. We contribute the notion of Intimate AT, along with a demonstrative design space, which together constitute a new design paradigm that draws attention to intimacy as a facet of both independently and collaboratively accessible ATs.},
booktitle = {Proceedings of the CHI Conference on Human Factors in Computing Systems},
articleno = {125},
numpages = {15},
keywords = {accessibility, blind, books, co-reading, eBooks, low vision, voice assistants},
location = {Honolulu, HI, USA},
series = {CHI '24}
}

@inproceedings{chhedakothary2024,
author = {Chheda-Kothary, Arnavi and Wobbrock, Jacob O. and Froehlich, Jon E.},
title = {Engaging with Children's Artwork in Mixed Visual-Ability Families},
year = {2024},
isbn = {9798400706776},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3663548.3675613},
doi = {10.1145/3663548.3675613},
abstract = {We present two studies exploring how blind or low-vision (BLV) family members engage with their sighted children’s artwork, strategies to support understanding and interpretation, and the potential role of technology, such as AI, therein. Our first study involved 14 BLV individuals, and the second included five groups of BLV individuals with their children. Through semi-structured interviews with AI descriptions of children’s artwork and multi-sensory design probes, we found that BLV family members value artwork engagement as a bonding opportunity, preferring the child’s storytelling and interpretation over other nonvisual representations. Additionally, despite some inaccuracies, BLV family members felt that AI-generated descriptions could facilitate dialogue with their children and aid self-guided art discovery. We close with specific design considerations for supporting artwork engagement in mixed visual-ability families, including enabling artwork access through various methods, supporting children’s corrections of AI output, and distinctions in context vs. content and interpretation vs. description of children’s artwork.},
booktitle = {Proceedings of the 26th International ACM SIGACCESS Conference on Computers and Accessibility},
articleno = {3},
numpages = {19},
keywords = {AI, Accessibility, blind or low-vision, children’s artwork, mixed-ability families},
location = {St. John's, NL, Canada},
series = {ASSETS '24}
}

@inproceedings{XuTeslaTouchCHI2011,
author = {Xu, Cheng and Israr, Ali and Poupyrev, Ivan and Bau, Olivier and Harrison, Chris},
title = {Tactile display for the visually impaired using TeslaTouch},
year = {2011},
isbn = {9781450302685},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1979742.1979705},
doi = {10.1145/1979742.1979705},
abstract = {TeslaTouch is a technology that provides tactile sensation to moving fingers on touch screens. Based on TeslaTouch, we have developed applications for the visually impaired to interpret and create 2D tactile information. In this paper, we demonstrate these applications, present observations from the interaction, and discuss TeslaTouch's potential in supporting communication among visually impaired individuals.},
booktitle = {CHI '11 Extended Abstracts on Human Factors in Computing Systems},
pages = {317–322},
numpages = {6},
keywords = {TeslaTouch, assistive technology, tactile display, visual impairment},
location = {Vancouver, BC, Canada},
series = {CHI EA '11}
}

@inproceedings{RUBICON,
author = {Biyani, Param and Bajpai, Yasharth and Radhakrishna, Arjun and Soares, Gustavo and Gulwani, Sumit},
title = {RUBICON: Rubric-Based Evaluation of Domain-Specific Human AI Conversations},
year = {2024},
isbn = {9798400706851},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3664646.3664778},
doi = {10.1145/3664646.3664778},
abstract = {Evaluating conversational assistants, such as GitHub Copilot Chat, poses a significant challenge for tool builders in the domain of Software Engineering. These assistants rely on language models and chat-based user experiences, rendering their evaluation with respect to the quality of the Human-AI conversations complicated. Existing general-purpose metrics for measuring conversational quality found in literature are inadequate for appraising domain-specific dialogues due to their lack of contextual sensitivity.
 
In this paper, we present RUBICON, a technique for evaluating domain-specific Human-AI conversations. RUBICON leverages large language models to generate candidate rubrics for assessing conversation quality and employs a selection process to choose the subset of rubrics based on their performance in scoring conversations. In our experiments, RUBICON effectively learns to differentiate conversation quality, achieving higher accuracy and yield rates than existing baselines.},
booktitle = {Proceedings of the 1st ACM International Conference on AI-Powered Software},
pages = {161–169},
numpages = {9},
keywords = {AI-assisted Programming, Conversation Evaluation, Conversational AI, Evaluation Metrics, Human-AI interaction, User Satisfaction},
location = {Porto de Galinhas, Brazil},
series = {AIware 2024}
}

@article{parnami2022learning,
  title={Learning from few examples: A summary of approaches to few-shot learning},
  author={Parnami, Archit and Lee, Minwoo},
  journal={arXiv preprint arXiv:2203.04291},
  year={2022}
}

@misc{HELM_MMLU, url={https://crfm.stanford.edu/helm/mmlu/latest/}, year={2024}, month={August}}


@misc{GPT-4_2023, url={https://openai.com/index/gpt-4-research/}, journal={OpenAI}, year={2023}, month=mar }

@misc{(HELM), url={https://crfm.stanford.edu/helm/image2struct/latest/}, journal={Center for Research on Foundation Models}, year={2024}, month={August} }


@article{Gupta2022GRITGR,
  title={GRIT: General Robust Image Task Benchmark},
  author={Tanmay Gupta and Ryan Marten and Aniruddha Kembhavi and Derek Hoiem},
  journal={ArXiv},
  year={2022},
  volume={abs/2204.13653},
  url={https://api.semanticscholar.org/CorpusID:248427081}
}

@misc{Lynch_2012, title={Why is art important in schools}, url={https://www.pbs.org/parents/thrive/the-importance-of-art-in-child-development}, journal={PBS}, publisher={Public Broadcasting Service}, author={Lynch, Grace}, year={2012}, month={May}} 

@misc{Rymanowicz_2021, title={The art of creating: Why art is important for early childhood development}, url={https://www.canr.msu.edu/news/the_art_of_creating_why_art_is_important_for_early_childhood_development}, journal={MSU Extension}, author={Rymanowicz, Kylie}, year={2021}, month={Mar}} 

 @misc{importance_for_early_development_2024,  title={The Importance of Arts and Crafts for Early Childhood Development},  url={https://www.cmosc.org/benefits-of-art-during-early-childhood/},  year={2023},  month={Feb} }

 @misc{how_parents_can_help_2024,  title={Drawing pictures is great for children’s development – here’s how parents can help},  url={http://theconversation.com/drawing-pictures-is-great-for-childrens-development-heres-how-parents-can-help-202650},  year={2023},  month={Apr} }

 @misc{dotpad,  title={Dot Module 20 | Dot Inc.},  url={http://www.dotincorp.com/page/32?gbn2=DotPad},  year={2024},  month={Apr} }

 @misc{gemini_2024,  title={Google’s AI now goes by a new name: Gemini},  url={https://www.theverge.com/2024/2/8/24065553/google-gemini-ios-android-app-duet-bard},  year={2024},  month={Apr} }

 @misc{llava,  title={LLaVA},  url={https://llava-vl.github.io/}, year={2024},  month={Apr} }

@misc{bing_ai, title={What is Microsoft’s Bing AI chatbot?}, url={https://www.lifewire.com/what-is-bing-ai-chatbot-7371141}, journal={Lifewire}, publisher={Lifewire}, author={Laukkonen, Jeremy}, year={2023}, month={Sep}} 

 @misc{seeing_ai,  title={Seeing AI - Talking Camera for the Blind},  url={https://www.seeingai.com/},  year={2024},  month={Apr} }

 @misc{be_my_ai_2024,  title={Introducing: Be My AI},  url={https://www.bemyeyes.com/blog/introducing-be-my-ai},  year={2024},  month={Apr} }

 @misc{explore_gpts_2024,  title={Explore GPTs},  url={https://chat.openai.com/gpts},  year={2024},  month={Apr} }

 @misc{be_my_eyes_2024,  title={The story about Be My Eyes},  url={https://www.bemyeyes.com/about},  year={2024},  month={Apr} }

 @misc{jaws_software_2024,  title={What's New in JAWS 2024 Screen Reading Software},  url={https://support.freedomscientific.com/downloads/jaws/JAWSWhatsNew},  journal={Freedom Scientific},  year={2024},  month={Mar} }

@misc{voiceoverios,
    title={{Learn VoiceOver gestures on iPhone}},
    author={{Apple}},
    year={Retrieved September 2022},
    url={https://support.apple.com/guide/iphone/learn-voiceover-gestures-iph3e2e2281/ios}}

@misc{hig_accessibility,
    title={{Accessibility | Apple Developer Documentation}},
    author={{Apple}},
    year={Retrieved October 2024},
    url={https://developer.apple.com/design/human-interface-guidelines/accessibility}
}


@misc{speech_framework,
    title={{Speech | Apple Developer Documentation}},
    author={{Apple}},
    year={Retrieved October 2024},
    url={https://developer.apple.com/documentation/speech/}
}

@misc{openai_assistants,
    title={{Assistants overview - OpenAI API}},
    author={{OpenAI}},
    year={Retrieved October 2024},
    url={https://platform.openai.com/docs/assistants/overview}
}

@misc{swiftui,
    title={{SwiftUI}},
    author={{Apple}},
    year={Retrieved October 2024},
    url={https://developer.apple.com/xcode/swiftui/}
}

@misc{uikit,
    title={{UIKit}},
    author={{UIKit}},
    year={Retrieved October 2024},
    url={https://getuikit.com/}
}

@misc{Claude35, title={{Introducing Claude 3.5 Sonnet}}, url={https://www.anthropic.com/news/claude-3-5-sonnet}, year={Retrieved October 2024}}

@misc{Gemini, title={{Gemini API}}, url={https://ai.google.dev/gemini-api?gad_source=1&gclid=Cj0KCQjw6oi4BhD1ARIsAL6pox0YMdXRnjPCnqOo6h8YnwZcGHcvNgrxcpfXsbJlPc46krAAmSfKAN4aArM2EALw_wcB}, year={Retrieved October 2024} }

@misc{GPT-4Turbo, title={{Models - OpenAI API | GPT-4 Turbo and GPT-4}}, url={https://platform.openai.com/docs/models/gpt-4-turbo-and-gpt-4}, year={Retrieved October 2024}}

@misc{GPT-4o, title={{Models - OpenAI API | GPT-4o}}, url={https://platform.openai.com/docs/models/gpt-4o}, year={Retrieved October 2024}}

@misc{openai_vision,
    title={{Vision - OpenAI API}},
    author={{OpenAI}},
    year={Retrieved October 2024},
    url={https://platform.openai.com/docs/guides/vision}
}

@misc{openai_models,
    title={{Models - OpenAI}},
    author={{OpenAI}},
    year={Retrieved October 2024},
    url={https://platform.openai.com/docs/models}
}

@article{likert1932technique,
  title={A technique for the measurement of attitudes.},
  author={Likert, Rensis},
  journal={Archives of psychology},
  year={1932}
}

@JournalArticle{ Sakr2017,
        title={ Parent-Child Moments of Meeting in Art-Making with Collage, iPad, Tuxpaint and Crayons },
        authors={ Mona Sakr Natalia Kucirkova },
        journal={ International Journal of Education and the Arts },
        year={ 2017 },
        publisher={ International Journal of Education & the Arts. 1310 South 6th Street, Champaign, IL 61820. Tel: 402-472-9958; Fax: 402-472-2837; Web site: http://www.ijea.org },
        volume={ 18 },
        
        number={ 2 },
      }

@JournalArticle{ Shamri_2018,
        title={ The Efficiency of Art-Based Interventions in Parental Training. },
        authors={ Liat Shamri Zeevi and Dafna Regev and Joseph Guttmann },
        journal={ Frontiers in Psychology },
        year={ 2018 },
        publisher={ Frontiers Media SA },
        volume={ 9 },
        pages={ 1495-1495 },
        
        doi={ 10.3389/FPSYG.2018.01495 },  
      }

@inproceedings{jain2024streetnavleveragingstreetcameras,
author = {Jain, Gaurav and Hindi, Basel and Zhang, Zihao and Srinivasula, Koushik and Xie, Mingyu and Ghasemi, Mahshid and Weiner, Daniel and Paris, Sophie Ana and Xu, Xin Yi Therese and Malcolm, Michael and Turkcan, Mehmet Kerem and Ghaderi, Javad and Kostic, Zoran and Zussman, Gil and Smith, Brian A.},
title = {StreetNav: Leveraging Street Cameras to Support Precise Outdoor Navigation for Blind Pedestrians},
year = {2024},
isbn = {9798400706288},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3654777.3676333},
doi = {10.1145/3654777.3676333},
abstract = {Blind and low-vision (BLV) people rely on GPS-based systems for outdoor navigation. GPS’s inaccuracy, however, causes them to veer off track, run into obstacles, and struggle to reach precise destinations. While prior work has made precise navigation possible indoors via hardware installations, enabling this outdoors remains a challenge. Interestingly, many outdoor environments are already instrumented with hardware such as street cameras. In this work, we explore the idea of repurposing existing street cameras for outdoor navigation. Our community-driven approach considers both technical and sociotechnical concerns through engagements with various stakeholders: BLV users, residents, business owners, and Community Board leadership. The resulting system, StreetNav, processes a camera’s video feed using computer vision and gives BLV pedestrians real-time navigation assistance. Our evaluations show that StreetNav guides users more precisely than GPS, but its technical performance is sensitive to environmental occlusions and distance from the camera. We discuss future implications for deploying such systems at scale.},
booktitle = {Proceedings of the 37th Annual ACM Symposium on User Interface Software and Technology},
articleno = {139},
numpages = {21},
keywords = {Visual impairments, computer vision, outdoor navigation, street camera},
location = {Pittsburgh, PA, USA},
series = {UIST '24}
}

@inproceedings{Grayson_DynamicAI,
author = {Grayson, Martin and Thieme, Anja and Marques, Rita and Massiceti, Daniela and Cutrell, Ed and Morrison, Cecily},
title = {A Dynamic AI System for Extending the Capabilities of Blind People},
year = {2020},
isbn = {9781450368193},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3334480.3383142},
doi = {10.1145/3334480.3383142},
abstract = {We introduce an advanced computer vision-based AI system that offers people with vision impairments (VI) dynamic, in-situ access to information about the location, identity and gaze-direction of other people nearby. Our AI system utilizes the camera technology of a head-worn HoloLens device, which captures a near 180° field-of-view surrounding the person who is wearing it. Captured images are then processed by multiple state-of-the-art perception algorithms whose outputs are integrated into a real-time tracking model of all people that the system detected. Users can receive information about those people acoustically (via spatialized audio) using a wrist-worn input controller. Having such dynamic access to information, through AI system interactions, enables people with VI to: develop their communication skills; more easily focus on others; and be more confident in their social interactions. Thus, our work explores how AI systems can serve as a useful resource for humans, helping expand their agency to develop new or extend existing skills.},
booktitle = {Extended Abstracts of the 2020 CHI Conference on Human Factors in Computing Systems},
pages = {1–4},
numpages = {4},
keywords = {AI system, accessibility, blindness, design + AI, disability, inclusive design, innovation, visual impairment},
location = {Honolulu, HI, USA},
series = {CHI EA '20}
}

@inproceedings{Morrison_FindMyThings,
author = {Morrison, Cecily and Grayson, Martin and Marques, Rita Faia and Massiceti, Daniela and Longden, Camilla and Wen, Linda and Cutrell, Edward},
title = {Understanding Personalized Accessibility through Teachable AI: Designing and Evaluating Find My Things for People who are Blind or Low Vision},
year = {2023},
isbn = {9798400702204},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597638.3608395},
doi = {10.1145/3597638.3608395},
abstract = {The opportunity for artificial intelligence, or AI, to enable accessibility is rapidly growing, but widely impactful applications can be challenging to build given the diversity of user need within and across disability communities. Teachable AI systems give users with disabilities a way to leverage the power of AI to personalize applications for their own specific needs, as long as the effort of providing examples is balanced with the benefit of the personalization received. As an example, this paper presents the design and evaluation of Find My Things, an end-to-end application that can be taught by people who are blind or low vision to find their personal things. Through synthesis of the design process, this paper offers design considerations for the teaching loop that is so critical to realizing the power of teachable AI for accessibility.},
booktitle = {Proceedings of the 25th International ACM SIGACCESS Conference on Computers and Accessibility},
articleno = {31},
numpages = {12},
keywords = {Accessibility, Artificial Intelligence, Teachable AI},
location = {New York, NY, USA},
series = {ASSETS '23}
}

@inproceedings{Park_AllAboutPictures_2023,
author = {Park, Sohyeon and Cassidy, Cameron Tyler and Branham, Stacy M.},
title = {“It’s All About the Pictures:” Understanding How Parents/Guardians With Visual Impairments Co-Read With Their Child(ren)},
year = {2023},
isbn = {9798400702204},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597638.3614488},
doi = {10.1145/3597638.3614488},
abstract = {Co-reading, an activity where adults collaboratively read books with child(ren), is important for literacy learning and forming human connection. However, parents and guardians with visual impairments do not experience the same level of access to resources when co-reading with their child(ren) as their sighted counterparts, especially as regards images in children’s books. Through conducting an interview study with five visually impaired parents/guardians, we illuminate the importance parents place on images in children’s books, how they access visual information in children’s print books, and the potential of smart speakers in assisting their existing co-reading practices.},
booktitle = {Proceedings of the 25th International ACM SIGACCESS Conference on Computers and Accessibility},
articleno = {88},
numpages = {4},
keywords = {accessibility, blind or low vision, co-reading, parents/caregivers},
location = {New York, NY, USA},
series = {ASSETS '23}
}

@inproceedings{Storer_Branham,
author = {Storer, Kevin M. and Branham, Stacy M.},
title = {"That's the Way Sighted People Do It": What Blind Parents Can Teach Technology Designers About Co-Reading with Children},
year = {2019},
isbn = {9781450358507},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3322276.3322374},
doi = {10.1145/3322276.3322374},
abstract = {Co-reading (when parents read aloud with their children) is an important literacy development activity for children. HCI has begun to explore how technology might support children in co-reading, but little empirical work examines how parents currently co-read, and no work examines how people with visual impairments (PWVI) co-read. PWVIs' perspectives offer unique insights into co-reading, as PWVI often read differently from their children, and (Braille) literacy holds particular cultural significance for PWVI. We observed discussions of co-reading practices in a blind parenting forum on Facebook, to establish a grounded understanding of how and why PWVI co-read. We found that PWVIs' co-reading practices were highly diverse and affected by a variety of socio-technical concerns - and visual ability was less influential than other factors like ability to read Braille, presence of social supports, and children's literacy. Our findings show that PWVI have valuable insights into co-reading, which could help technologies in this space better meet the needs of parents and children, with and without disabilities.},
booktitle = {Proceedings of the 2019 on Designing Interactive Systems Conference},
pages = {385–398},
numpages = {14},
keywords = {assistive technology, blind parents, co-reading, literacy},
location = {San Diego, CA, USA},
series = {DIS '19}
}

@misc{specializing_llms,  title={A three-step design pattern for specializing LLMs},  url={https://cloud.google.com/blog/products/ai-machine-learning/three-step-design-pattern-for-specializing-llms}, year={2024},  month={March}  }

@article{Phutane_2022,
author = {Phutane, Mahika and Wright, Julie and Castro, Brenda Veronica and Shi, Lei and Stern, Simone R. and Lawson, Holly M. and Azenkot, Shiri},
title = {Tactile Materials in Practice: Understanding the Experiences of Teachers of the Visually Impaired},
year = {2022},
issue_date = {September 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {3},
issn = {1936-7228},
url = {https://doi.org/10.1145/3508364},
doi = {10.1145/3508364},
abstract = {Teachers of the visually impaired (TVIs) regularly present tactile materials (tactile graphics, 3D models, and real objects) to students with vision impairments. Researchers have been increasingly interested in designing tools to support the use of tactile materials, but we still lack an in-depth understanding of how tactile materials are created and used in practice today. To address this gap, we conducted interviews with 21 TVIs and a 3-week diary study with eight of them. We found that tactile materials were regularly used for academic as well as non-academic concepts like tactile literacy, motor ability, and spatial awareness. Real objects and 3D models served as “stepping stones” to tactile graphics and our participants preferred to teach with 3D models, despite finding them difficult to create, obtain, and modify. Use of certain materials also carried social implications; participants selected materials that fostered student independence and allow classroom inclusion. We contribute design considerations, encouraging future work on tactile materials to enable student and TVI co-creation, facilitate rapid prototyping, and promote movement and spatial awareness. To support future research in this area, our paper provides a fundamental understanding of current practices. We bridge these practices to established pedagogical approaches and highlight opportunities for growth regarding this important genre of educational materials.},
journal = {ACM Trans. Access. Comput.},
month = {jul},
articleno = {17},
numpages = {34},
keywords = {3D models, tactile graphics, visual impairments, Tactile materials}
}

@inproceedings{Bennett_Interdependence,
author = {Bennett, Cynthia L. and Brady, Erin and Branham, Stacy M.},
title = {Interdependence as a Frame for Assistive Technology Research and Design},
year = {2018},
isbn = {9781450356503},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3234695.3236348},
doi = {10.1145/3234695.3236348},
abstract = {In this paper, we describe interdependence for assistive technology design, a frame developed to complement the traditional focus on independence in the Assistive Technology field. Interdependence emphasizes collaborative access and people with disabilities' important and often understated contribution in these efforts. We lay the foundation of this frame with literature from the academic discipline of Disability Studies and popular media contributed by contemporary disability justice activists. Then, drawing on cases from our own work, we show how the interdependence frame (1) synthesizes findings from a growing body of research in the Assistive Technology field and (2) helps us orient to additional technology design opportunities. We position interdependence as one possible orientation to, not a prescription for, research and design practice--one that opens new design possibilities and affirms our commitment to equal access for people with disabilities.},
booktitle = {Proceedings of the 20th International ACM SIGACCESS Conference on Computers and Accessibility},
pages = {161–173},
numpages = {13},
keywords = {assistive technology design, interdependence},
location = {Galway, Ireland},
series = {ASSETS '18}
}

@inproceedings{Branham_Kane_HomeSpaces,
author = {Branham, Stacy M. and Kane, Shaun K.},
title = {Collaborative Accessibility: How Blind and Sighted Companions Co-Create Accessible Home Spaces},
year = {2015},
isbn = {9781450331456},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2702123.2702511},
doi = {10.1145/2702123.2702511},
abstract = {In recent decades, great technological strides have been made toward enabling people who are blind to live independent, successful lives. However, there has been relatively little progress towards understanding the social, collaborative needs of this population, particularly in the domestic setting. We conducted semi-structured interviews in the homes of 10 pairs of close companions in which one partner was blind and one was not. We found that partners engaged in collaborative accessibility by taking active roles in co-creating an accessible environment. Due to their different visual abilities, however, partners sometimes encountered difficulties managing divergent needs and engaging in shared experiences. We describe outstanding challenges to creating accessible shared home spaces and outline new research and technology opportunities for supporting collaborative accessibility in the home.},
booktitle = {Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems},
pages = {2373–2382},
numpages = {10},
keywords = {vision impairment, interpersonal relationships, home, collaboration, blindness, accessibility},
location = {Seoul, Republic of Korea},
series = {CHI '15}
}

@inproceedings{Branham_Kane_Workplaces,
author = {Branham, Stacy M. and Kane, Shaun K.},
title = {The Invisible Work of Accessibility: How Blind Employees Manage Accessibility in Mixed-Ability Workplaces},
year = {2015},
isbn = {9781450334006},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2700648.2809864},
doi = {10.1145/2700648.2809864},
abstract = {Over the past century, people who are blind and their allies have developed successful public policies and technologies in support of creating more accessible workplaces. However, simply creating accessible technologies does not guarantee that these will be available or adopted. Because much work occurs within shared workspaces, decisions about assistive technology use may be mediated by social interactions with, and expectations of, sighted coworkers. We present findings from a qualitative field study of five workplaces from the perspective of blind employees. Although all participants were effective employees, they expressed that working in a predominantly sighted office environment produces impediments to a blind person's independence and to their integration as an equal coworker. We describe strategies employed by our participants to create and maintain an accessible workplace and present suggestions for future technology that better supports blind workers as equal peers in the workplace.},
booktitle = {Proceedings of the 17th International ACM SIGACCESS Conference on Computers \& Accessibility},
pages = {163–171},
numpages = {9},
keywords = {assistive technology, blindness, collaborative accessibility, vision impairment, workplace},
location = {Lisbon, Portugal},
series = {ASSETS '15}
}

@inproceedings{Williams_LetTheCaneHit,
author = {Williams, Michele A. and Galbraith, Caroline and Kane, Shaun K. and Hurst, Amy},
title = {"just let the cane hit it": how the blind and sighted see navigation differently},
year = {2014},
isbn = {9781450327206},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2661334.2661380},
doi = {10.1145/2661334.2661380},
abstract = {Sighted people often have the best of intentions when they want to help a blind person navigate, but their well meaning is also often coupled with a lack of knowledge and understanding about how a person navigates without vision. As a result what sighted people think is the right feedback is too often the wrong feedback to give to a person with a visual impairment. Understanding how to provide feedback to blind navigators is crucial to the design of assistive technologies for navigation. In our research investigating the design of a personal pedestrian navigation device, we observed firsthand the ways that sighted people seemingly misunderstand how many blind people navigate when using a white cane mobility aid. Throughout our qualitative end user studies that included focus groups and observations (including couple-based observations with a close companion) we gathered data that explicitly shows how the language and understanding of sighted vs. blind pedestrians differs greatly and even how it can be dangerous when people interfere in the wrong way. From our findings we discuss why it is difficult for a blind person to navigate like a sighted person to ensure designers are aware of the difficulties and designing with new training in mind, not simply designing from their own point of view. We also want to encourage advocacy and empathy amongst the sighted community towards this activity of walking around independently.},
booktitle = {Proceedings of the 16th International ACM SIGACCESS Conference on Computers \& Accessibility},
pages = {217–224},
numpages = {8},
keywords = {white cane, empathy, blind navigation},
location = {Rochester, New York, USA},
series = {ASSETS '14}
}

@inproceedings{McDonnell_EasierHarder,
author = {McDonnell, Emma J and Moon, Soo Hyun and Jiang, Lucy and Goodman, Steven M. and Kushalnagar, Raja and Froehlich, Jon E. and Findlater, Leah},
title = {“Easier or Harder, Depending on Who the Hearing Person Is”: Codesigning Videoconferencing Tools for Small Groups with Mixed Hearing Status},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3580809},
doi = {10.1145/3544548.3580809},
abstract = {With improvements in automated speech recognition and increased use of videoconferencing, real-time captioning has changed significantly. This shift toward broadly available but less accurate captioning invites exploration of the role hearing conversation partners play in shaping the accessibility of a conversation to d/Deaf and hard of hearing (DHH) captioning users. While recent work has explored DHH individuals’ videoconferencing experiences with captioning, we focus on established groups’ current practices and priorities for future tools to support more accessible online conversations. Our study consists of three codesign sessions, conducted with four groups (17 participants total, 10 DHH, 7 hearing). We found that established groups crafted social accessibility norms that met their relational contexts. We also identify promising directions for future captioning design, including the need to standardize speaker identification and customization, opportunities to provide behavioral feedback during a conversation, and ways that videoconferencing platforms could enable groups to set and share norms.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {780},
numpages = {15},
keywords = {Accessibility, Captioning, Videoconferencing, d/Deaf and hard of hearing},
location = {Hamburg, Germany},
series = {CHI '23}
}

@article{Phutane_Tactile,
author = {Phutane, Mahika and Wright, Julie and Castro, Brenda Veronica and Shi, Lei and Stern, Simone R. and Lawson, Holly M. and Azenkot, Shiri},
title = {Tactile Materials in Practice: Understanding the Experiences of Teachers of the Visually Impaired},
year = {2022},
issue_date = {September 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {3},
issn = {1936-7228},
url = {https://doi.org/10.1145/3508364},
doi = {10.1145/3508364},
abstract = {Teachers of the visually impaired (TVIs) regularly present tactile materials (tactile graphics, 3D models, and real objects) to students with vision impairments. Researchers have been increasingly interested in designing tools to support the use of tactile materials, but we still lack an in-depth understanding of how tactile materials are created and used in practice today. To address this gap, we conducted interviews with 21 TVIs and a 3-week diary study with eight of them. We found that tactile materials were regularly used for academic as well as non-academic concepts like tactile literacy, motor ability, and spatial awareness. Real objects and 3D models served as “stepping stones” to tactile graphics and our participants preferred to teach with 3D models, despite finding them difficult to create, obtain, and modify. Use of certain materials also carried social implications; participants selected materials that fostered student independence and allow classroom inclusion. We contribute design considerations, encouraging future work on tactile materials to enable student and TVI co-creation, facilitate rapid prototyping, and promote movement and spatial awareness. To support future research in this area, our paper provides a fundamental understanding of current practices. We bridge these practices to established pedagogical approaches and highlight opportunities for growth regarding this important genre of educational materials.},
journal = {ACM Trans. Access. Comput.},
month = {jul},
articleno = {17},
numpages = {34},
keywords = {Tactile materials, visual impairments, tactile graphics, 3D models}
}

@inproceedings{savidis1995developing,
  title={Developing dual user interfaces for integrating blind and sighted users: the HOMER UIMS},
  author={Savidis, Anthony and Stephanidis, Constantine},
  booktitle={Proceedings of the SIGCHI conference on Human Factors in Computing Systems},
  pages={106--113},
  year={1995}
}

@inproceedings{Plimmer_Multimodal,
author = {Plimmer, Beryl and Crossan, Andrew and Brewster, Stephen A. and Blagojevic, Rachel},
title = {Multimodal collaborative handwriting training for visually-impaired people},
year = {2008},
isbn = {9781605580111},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1357054.1357119},
doi = {10.1145/1357054.1357119},
abstract = {"McSig" is a multimodal teaching and learning environ-ment for visually-impaired students to learn character shapes, handwriting and signatures collaboratively with their teachers. It combines haptic and audio output to realize the teacher's pen input in parallel non-visual modalities. McSig is intended for teaching visually-impaired children how to handwrite characters (and from that signatures), something that is very difficult without visual feedback. We conducted an evaluation with eight visually-impaired children with a pretest to assess their current skills with a set of character shapes, a training phase using McSig and then a post-test of the same character shapes to see if there were any improvements. The children could all use McSig and we saw significant improvements in the character shapes drawn, particularly by the completely blind children (many of whom could draw almost none of the characters before the test). In particular, the blind participants all expressed enjoyment and excitement about the system and using a computer to learn to handwrite.},
booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
pages = {393–402},
numpages = {10},
keywords = {visually-impaired users, signature training, multimodal interface design, haptic trajectory playback},
location = {Florence, Italy},
series = {CHI '08}
}

@inproceedings{Carmien_EndUserProgramming,
author = {Carmien, Stefan},
title = {End user programming and context responsiveness in handheld prompting systems for persons with cognitive disabilities and caregivers},
year = {2005},
isbn = {1595930027},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1056808.1056889},
doi = {10.1145/1056808.1056889},
abstract = {Providing instructions via handheld prompters holds much promise for supporting independence for persons with cognitive disabilities. Because users of these tools are paired - caregivers who make scripts and a person with cognitive disabilities who uses them - designing such a system presents unique meta-design problems. The problems of changing content and configuration on a handheld computer, as needs and abilities change of the users with cognitive disabilities, produce a critical need for end-user programming tools. This paper describes the design and testing of the MAPS (Memory Aiding Prompting System) system, consisting of a handheld prompter and a multimedia editing tool for script creation, storage, and modification. The unique meta-design challenges of supporting end-user programming of context-responsive systems, and its broader implications, are presented.},
booktitle = {CHI '05 Extended Abstracts on Human Factors in Computing Systems},
pages = {1252–1255},
numpages = {4},
keywords = {assistive technology, context aware mobile systems, end-user programming, technology abandonment},
location = {Portland, OR, USA},
series = {CHI EA '05}
}

@inproceedings{Cullen_StoryMapping,
author = {Cullen, Clare and Metatla, Oussama},
title = {Co-designing Inclusive Multisensory Story Mapping with Children with Mixed Visual Abilities},
year = {2019},
isbn = {9781450366908},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3311927.3323146},
doi = {10.1145/3311927.3323146},
abstract = {Story mapping is used in schools to promote children's understanding of stories and narrative structure. As a collaborative activity, it can support creativity and facilitate group interaction. However, most techniques used in primary schools rely on visual materials, which creates a barrier to learning for children with visual impairments (VI). To address this, we set out to design a collaborative story mapping tool with a group of children with mixed visual abilities and their teaching assistants. Using co-design approaches over ten workshops, we designed and prototyped different ideas for engaging children in storytelling and design. We present our co-design process and findings, and the resulting story mapping system. We outline how using multisensory elements can facilitate creativity and collaboration to help children with mixed visual abilities create and share stories together, and support learning and social inclusion of VI children in mainstream classrooms.},
booktitle = {Proceedings of the 18th ACM International Conference on Interaction Design and Children},
pages = {361–373},
numpages = {13},
keywords = {Tangibles, Storytelling, Multisensory, Mixed Abilities, Inclusion, Education, Co-Design, Children},
location = {Boise, ID, USA},
series = {IDC '19}
}

@inproceedings{Pires_CoDesignInclusiveEdu,
author = {Pires, Ana Cristina and Neto, Isabel and Brul\'{e}, Emeline and Malinverni, Laura and Metatla, Oussama and Hourcade, Juan Pablo},
title = {Co-Designing with Mixed-Ability Groups of Children to Promote Inclusive Education},
year = {2022},
isbn = {9781450391979},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3501712.3536389},
doi = {10.1145/3501712.3536389},
abstract = {In this half-day workshop, we will explore how to co-design technology in inclusive classrooms where children have diverse sensory, motor, cognitive or behavioral abilities. We will discuss barriers and opportunities in co-designing for inclusion, exploring techniques and tools to support learning in a collaborative environment. We encourage researchers, educators, parents, and other stakeholders to participate and provide their expertise and know-how in improving these environments, with an aim to support both inclusion and collaboration; and children’s exploration of their own interests and approaches to learning. We seek to better understand research experiences in these environments, co-design techniques that were successfully used, and what they can teach the broader field of interaction design for children.},
booktitle = {Proceedings of the 21st Annual ACM Interaction Design and Children Conference},
pages = {715–718},
numpages = {4},
keywords = {Mixed Abilities., Inclusive Education, Co-design with Children},
location = {Braga, Portugal},
series = {IDC '22}
}

@inproceedings{Butler_MultiSensory,
author = {Butler, Matthew and Holloway, Leona and Marriott, Kim},
title = {A Closer Look: Multi-Sensory Accessible Art Translations},
year = {2019},
isbn = {9781450366762},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3308561.3354617},
doi = {10.1145/3308561.3354617},
abstract = {Providing people who are blind or have low vision with accessible versions of artworks is important not just for equity, but also for inclusion, greater engagement with the community at large, and raising awareness about these issues. In 2018, a value-sensitive design methodology was used with the Bendigo Art Gallery and key stakeholders to develop a model that provides three different ways of accessing the gallery, depending upon visual acuity and mobility: virtual tours, self-guided tours and guided tours. As a pilot implementation of the model, we developed different tactile representations of key artworks using tactile graphics, laser-cut layered graphics, 3D printed models, soundscapes, role plays, and a website featuring information and representations requested by workshop participants. To highlight the work, this paper will present two of the key works in more detail to highlight different representations that should be considered when presenting accessible artworks.},
booktitle = {Proceedings of the 21st International ACM SIGACCESS Conference on Computers and Accessibility},
pages = {594–596},
numpages = {3},
keywords = {3d printing, accessibility, art, blindness, low vision},
location = {Pittsburgh, PA, USA},
series = {ASSETS '19}
}

@inproceedings{Asakawa_IndependentMuseumExperience,
author = {Asakawa, Saki and Guerreiro, Jo\~{a}o and Sato, Daisuke and Takagi, Hironobu and Ahmetovic, Dragan and Gonzalez, Desi and Kitani, Kris M. and Asakawa, Chieko},
title = {An Independent and Interactive Museum Experience for Blind People},
year = {2019},
isbn = {9781450367165},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3315002.3317557},
doi = {10.1145/3315002.3317557},
abstract = {Museums are gradually becoming more accessible to blind people, who have shown interest in visiting museums and in appreciating visual art. Yet, their ability to visit museums is still dependent on the assistance they get from their family and friends or from the museum personnel. Based on this observation and on prior research, we developed a solution to support an independent, interactive museum experience that uses the continuous tracking of the user's location and orientation to enable a seamless interaction between Navigation and Art Appreciation. Accurate localization and context-awareness allow for turn-by-turn guidance (Navigation Mode), as well as detailed audio content when facing an artwork within close proximity (Art Appreciation Mode). In order to evaluate our system, we installed it at The Andy Warhol Museum in Pittsburgh and conducted a user study where nine blind participants followed routes of interest while learning about the artworks. We found that all participants were able to follow the intended path, immediately grasped how to switch between Navigation and Art Appreciation modes, and valued listening to the audio content in front of each artwork. Also, they showed high satisfaction and an increased motivation to visit museums more often.},
booktitle = {Proceedings of the 16th International Web for All Conference},
articleno = {30},
numpages = {9},
keywords = {Museum accessibility, art appreciation, indoor navigation, interactive space, non-visual interaction, visual impairments},
location = {San Francisco, CA, USA},
series = {W4A '19}
}

@inproceedings{Gomes_ArtA11y_Photography,
author = {Gomes, Arlindo and Soares, Cristiana and Sim\~{o}es, Francisco and Correia, Walter},
title = {Art accessibility for blind people: a process to create and understand photography},
year = {2019},
isbn = {9781450369718},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3357155.3358456},
doi = {10.1145/3357155.3358456},
abstract = {Cultural spaces like art galleries or museums are, in general, not accessible to blind or low vision visitors. Most of them do not have haptic support or when it has, the users do not have the experience to understand adapted art. This paper proposes a process to improve accessibility to visual art by for blind people improving knowledge on the creation and interpretation of photography. Initially, a photography workshop was applied to a group of three individuals with different levels of visual impairment. In the first part, researchers explained the basic concepts of photography like composing, lighting, and camera. Then, the participants choose their subjects and captured images. After the workshop, the research team built six haptic plaster pieces. Later, a final meeting and a public exhibition took place to collect users feedback and show their final results. With this process, we provide a path to improve the autonomy of blind people on image reading, photographic production, and haptic perception, giving them tools to interpret not just their captured images, but different visual art, using their own experiences. This work can also be extended to understand the challenges of image/haptic based interfaces for people with low vision.},
booktitle = {Proceedings of the 18th Brazilian Symposium on Human Factors in Computing Systems},
articleno = {13},
numpages = {9},
keywords = {accessibility, art, blind, haptic, low-vision, photography},
location = {Vit\'{o}ria, Esp\'{\i}rito Santo, Brazil},
series = {IHC '19}
}

@inproceedings{Bartolome_ArtMultimodalExploration,
author = {Iranzo Bartolome, Jorge and Cavazos Quero, Luis and Kim, Sunhee and Um, Myung-Yong and Cho, Jundong},
title = {Exploring Art with a Voice Controlled Multimodal Guide for Blind People},
year = {2019},
isbn = {9781450361965},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3294109.3300994},
doi = {10.1145/3294109.3300994},
abstract = {There is an increasing concern to improve the accessibility of artworks for blind people. Much of the effort has been focused on helping the visually impaired people to access the exhibition facilities, but the works of art hosted there are still difficult to experience for them. Particularly, the appreciation of visual artworks is hindered as blind visitors are not allowed to touch them in order to conserve their aesthetics and value. In this work we explore our findings using a prototype of a voice interactive multimodal guide designed to improve the accessibility of visual works of arts, such as paintings, for the blind people. The prototype identifies tactile gestures and voice commands that trigger audio descriptions and sounds while a person explores a 2.5D tactile representation of the artwork placed on the top surface of the prototype. Our preliminary findings include the results of eight user tests and Likert-type surveys.},
booktitle = {Proceedings of the Thirteenth International Conference on Tangible, Embedded, and Embodied Interaction},
pages = {383–390},
numpages = {8},
keywords = {voice interaction, multimodal guide, human computer interaction, blind people, assistive technologies},
location = {Tempe, Arizona, USA},
series = {TEI '19}
}

@inproceedings{Holloway_MakingSenseOfArt,
author = {Holloway, Leona and Marriott, Kim and Butler, Matthew and Borning, Alan},
title = {Making Sense of Art: Access for Gallery Visitors with Vision Impairments},
year = {2019},
isbn = {9781450359702},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3290605.3300250},
doi = {10.1145/3290605.3300250},
abstract = {While there is widespread recognition of the need to provide people with vision impairments (PVI) equitable access to cultural institutions such as art galleries, this is not easy. We present the results of a collaboration with a regional art gallery who wished to open their collection to PVIs in the local community. We describe a novel model that provides three different ways of accessing the gallery, depending upon visual acuity and mobility: virtual tours, self-guided tours and guided tours. As far as possible the model supports autonomous exploration by PVIs. It was informed by a value sensitive design exploration of the values and value conflicts of the primary stakeholders.},
booktitle = {Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems},
pages = {1–12},
numpages = {12},
keywords = {vision impairment, value sensitive design, blindness, art, accessibility, 3d printing},
location = {Glasgow, Scotland Uk},
series = {CHI '19}
}

@inproceedings{Butler_SystematicReviewOfTactile,
author = {Butler, Matthew and Holloway, Leona M and Reinders, Samuel and Goncu, Cagatay and Marriott, Kim},
title = {Technology Developments in Touch-Based Accessible Graphics: A Systematic Review of Research 2010-2020},
year = {2021},
isbn = {9781450380966},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411764.3445207},
doi = {10.1145/3411764.3445207},
abstract = {This paper presents a systematic literature review of 292 publications from 97 unique venues on touch-based graphics for people who are blind or have low vision, from 2010 to mid-2020. It is the first review of its kind on touch-based accessible graphics. It is timely because it allows us to assess the impact of new technologies such as commodity 3D printing and low-cost electronics on the production and presentation of accessible graphics. As expected our review shows an increase in publications from 2014 that we can attribute to these developments. It also reveals the need to: broaden application areas, especially to the workplace; broaden end-user participation throughout the full design process; and conduct more in situ evaluation. This work is linked to an online living resource to be shared with the wider community.},
booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},
articleno = {278},
numpages = {15},
keywords = {Assistive Technology, Blind, Low Vision, Systematic Literature Review, Tactile Graphics},
location = {Yokohama, Japan},
series = {CHI '21}
}

@inproceedings{Ahmetovic_TouchScreenExplorationOfArtwork,
author = {Ahmetovic, Dragan and Kwon, Nahyun and Oh, Uran and Bernareggi, Cristian and Mascetti, Sergio},
title = {Touch Screen Exploration of Visual Artwork for Blind People},
year = {2021},
isbn = {9781450383127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442381.3449871},
doi = {10.1145/3442381.3449871},
abstract = {This paper investigates how touchscreen exploration and verbal feedback can be used to support blind people to access visual artwork. We present two artwork exploration modalities. The first one, attribute-based exploration, extends prior work on touchscreen image accessibility, and provides fine-grained segmentation of artwork visual elements; when the user touches an element, the associated attributes are read. The second one, hierarchical exploration, is designed with domain experts and provides multi-level segmentation of the artwork; the user initially accesses a general description of the entire artwork and then explores a coarse segmentation of the visual elements with the corresponding high-level descriptions; once selected, coarse segments are subdivided into fine-grained ones, which the user can access for more detailed descriptions. The two exploration modalities, implemented as a mobile web app, were evaluated through a user study with 10 blind participants. Both modalities were appreciated by the participants. Attribute-based exploration is perceived to be easier to access. Instead, the hierarchical exploration was considered more understandable, useful, interesting and captivating, and the participants remembered more details about the artwork with this modality. Participants commented that the two modalities work well together and therefore both should be made available.},
booktitle = {Proceedings of the Web Conference 2021},
pages = {2781–2791},
numpages = {11},
keywords = {Art accessibility, Audio feedback., Blindness, Touch screen},
location = {Ljubljana, Slovenia},
series = {WWW '21}
}

@inproceedings{Li_UnderstandingVisualArtsExperiences,
author = {Li, Franklin Mingzhe and Zhang, Lotus and Bandukda, Maryam and Stangl, Abigale and Shinohara, Kristen and Findlater, Leah and Carrington, Patrick},
title = {Understanding Visual Arts Experiences of Blind People},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3580941},
doi = {10.1145/3544548.3580941},
abstract = {Visual arts play an important role in cultural life and provide access to social heritage and self-enrichment, but most visual arts are inaccessible to blind people. Researchers have explored different ways to enhance blind people’s access to visual arts (e.g., audio descriptions, tactile graphics). However, how blind people adopt these methods remains unknown. We conducted semi-structured interviews with 15 blind visual arts patrons to understand how they engage with visual artwork and the factors that influence their adoption of visual arts access methods. We further examined interview insights in a follow-up survey (N=220). We present: 1) current practices and challenges of accessing visual artwork in-person and online (e.g., Zoom tour), 2) motivation and cognition of perceiving visual arts (e.g., imagination), and 3) implications for designing visual arts access methods. Overall, our findings provide a roadmap for technology-based support for blind people’s visual arts experiences.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {60},
numpages = {21},
keywords = {Accessibility, Assistive technology, Blind people, Mixed-methods study, Visual arts},
location = {Hamburg, Germany},
series = {CHI '23}
}

@inproceedings{Lee_ImageExplorer,
author = {Lee, Jaewook and Herskovitz, Jaylin and Peng, Yi-Hao and Guo, Anhong},
title = {ImageExplorer: Multi-Layered Touch Exploration to Encourage Skepticism Towards Imperfect AI-Generated Image Captions},
year = {2022},
isbn = {9781450391573},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3491102.3501966},
doi = {10.1145/3491102.3501966},
abstract = {Blind users rely on alternative text (alt-text) to understand an image; however, alt-text is often missing. AI-generated captions are a more scalable alternative, but they often miss crucial details or are completely incorrect, which users may still falsely trust. In this work, we sought to determine how additional information could help users better judge the correctness of AI-generated captions. We developed&nbsp;ImageExplorer, a touch-based multi-layered image exploration system that allows users to explore the spatial layout and information hierarchies of images, and compared it with popular text-based (Facebook) and touch-based (Seeing AI) image exploration systems in a study with 12 blind participants. We found that exploration was generally successful in encouraging skepticism towards imperfect captions. Moreover, many participants preferred&nbsp;ImageExplorer for its multi-layered and spatial information presentation, and Facebook for its summary and ease of use. Finally, we identify design improvements for effective and explainable image exploration systems for blind users.},
booktitle = {Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems},
articleno = {462},
numpages = {15},
keywords = {visual impairment, touch exploration, screen reader, imperfect AI, encourage skepticism, alternative text, alt text, accessibility, Blind, Automatic image captioning},
location = {New Orleans, LA, USA},
series = {CHI '22}
}

@inproceedings{Nair_ImageAssist,
author = {Nair, Vishnu and Zhu, Hanxiu 'Hazel' and Smith, Brian A.},
title = {ImageAssist: Tools for Enhancing Touchscreen-Based Image Exploration Systems for Blind and Low Vision Users},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3581302},
doi = {10.1145/3544548.3581302},
abstract = {Blind and low vision (BLV) users often rely on alt text to understand what a digital image is showing. However, recent research has investigated how touch-based image exploration on touchscreens can supplement alt text. Touchscreen-based image exploration systems allow BLV users to deeply understand images while granting a strong sense of agency. Yet, prior work has found that these systems require a lot of effort to use, and little work has been done to explore these systems’ bottlenecks on a deeper level and propose solutions to these issues. To address this, we present ImageAssist, a set of three tools that assist BLV users through the process of exploring images by touch — scaffolding the exploration process. We perform a series of studies with BLV users to design and evaluate ImageAssist, and our findings reveal several implications for image exploration tools for BLV users.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {76},
numpages = {17},
keywords = {alt text, smartphone-based accessibility tools, touchscreen-based image exploration tools, visual impairments},
location = {Hamburg, Germany},
series = {CHI '23}
}

@inproceedings{Bellscheidt_AuthoringAltText,
author = {Bellscheidt, Selah and Metcalf, Hazel and Pham, Di and Elglaly, Yasmine N.},
title = {Building the Habit of Authoring Alt Text: Design for Making a Change},
year = {2023},
isbn = {9798400702204},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597638.3614495},
doi = {10.1145/3597638.3614495},
abstract = {Alternative (alt) text is essential for the accessibility of images to screen reader users. Alt text can be authored by end users or generated automatically using machine learning. Automated methods, while fast and cheap, do not result in high quality alt text. Meanwhile, the large number of images with no alt text on social media indicates that many users do not author alt text for the images they upload. Little is known about whether design elements that motivate and support users are integrated into alt text user interfaces (UIs). We reviewed the current state of alt text UIs in literature, and characterized their design according to behavior change and accountability design factors. We found that the majority of reviewed UIs contained general prompts with no specific motivator or guidance. Many of the UIs did not incorporate design elements to encourage accountability or behavior change. We recommend that future alt text authoring tools incorporate design elements which decrease the user’s cognitive effort, employ motivational and well-timed triggers, and leverage user accountability through alt text visibility and its association with the user’s social presence.},
booktitle = {Proceedings of the 25th International ACM SIGACCESS Conference on Computers and Accessibility},
articleno = {65},
numpages = {5},
keywords = {accountability, alt text, behavior change, image accessibility},
location = {New York, NY, USA},
series = {ASSETS '23}
}

@inproceedings{Morris_RichRepresentations,
author = {Morris, Meredith Ringel and Johnson, Jazette and Bennett, Cynthia L. and Cutrell, Edward},
title = {Rich Representations of Visual Content for Screen Reader Users},
year = {2018},
isbn = {9781450356206},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3173574.3173633},
doi = {10.1145/3173574.3173633},
abstract = {Alt text (short for "alternative text") is descriptive text associated with an image in HTML and other document formats. Screen reader technologies speak the alt text aloud to people who are visually impaired. Introduced with HTML 2.0 in 1995, the alt attribute has not evolved despite significant changes in technology over the past two decades. In light of the expanding volume, purpose, and importance of digital imagery, we reflect on how alt text could be supplemented to offer a richer experience of visual content to screen reader users. Our contributions include articulating the design space of representations of visual content for screen reader users, prototypes illustrating several points within this design space, and evaluations of several of these new image representations with people who are blind. We close by discussing the implications of our taxonomy, prototypes, and user study findings.},
booktitle = {Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems},
pages = {1–11},
numpages = {11},
keywords = {visual impairment, screen readers, captions, blindness, alternative text, alt text, accessibility},
location = {Montreal QC, Canada},
series = {CHI '18}
}

@misc{effective_alt_text,  title={Everything you need to know to write effective alt text - Microsoft Support},  url={https://support.microsoft.com/en-us/office/everything-you-need-to-know-to-write-effective-alt-text-df98f884-ca3d-456c-807b-1a1fa82f5dc2}, year={2024},  month={Apr} }

@inproceedings{Shi_MagicTouch,
author = {Shi, Lei and McLachlan, Ross and Zhao, Yuhang and Azenkot, Shiri},
title = {Magic Touch: Interacting with 3D Printed Graphics},
year = {2016},
isbn = {9781450341240},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2982142.2982153},
doi = {10.1145/2982142.2982153},
abstract = {Graphics like maps and models are important learning materials. With recently developed projects, we can use 3D printers to make tactile graphics that are more accessible to blind people. However, current 3D printed graphics can only convey limited information through their shapes and textures. We present Magic Touch, a computer vision-based system that augments printed graphics with audio files associated with specific locations, or hotspots, on the model. A user can access an audio file associated with a hotspot by touching it with a pointing gesture. The system detects the user's gesture and determines the hotspot location with computer vision algorithms by comparing a video feed of the user's interaction with the digital representation of the model and its hotspots. To enable MT, a model designer must add a single tracker with fiducial tags to a model. After the tracker is added, MT only requires an RGB camera, so it can be easily deployed on many devices such as mobile phones, laptops and smart glasses.},
booktitle = {Proceedings of the 18th International ACM SIGACCESS Conference on Computers and Accessibility},
pages = {329–330},
numpages = {2},
keywords = {visual impairments, computer vision, 3d printing},
location = {Reno, Nevada, USA},
series = {ASSETS '16}
}

 @misc{snowball_sampling,  title={Snowball Sampling},  url={https://research.oregonstate.edu/irb/policies-and-guidance-investigators/guidance/snowball-sampling}, year={2010},  month={Sep} }

 @misc{diy_embossing,  title={Discover a Simple DIY Technique for Embossing Tactile Graphics - 100 Times Cheaper Than Other Methods},  url={https://tactileimages.org/en/blog-en/discover-a-simple-diy-technique-for-embossing-tactile-graphics-100-times-cheaper-than-other-methods/} }

@inproceedings{Herskovitz_HackingSwitchingCombining,
author = {Herskovitz, Jaylin and Xu, Andi and Alharbi, Rahaf and Guo, Anhong},
title = {Hacking, Switching, Combining: Understanding and Supporting DIY Assistive Technology Design by Blind People},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3581249},
doi = {10.1145/3544548.3581249},
abstract = {Existing assistive technologies (AT) often fail to support the unique needs of blind and visually impaired (BVI) people. Thus, BVI people have become domain experts in customizing and ‘hacking’ AT, creatively suiting their needs. We aim to understand this behavior in depth, and how BVI people envision creating future DIY personalized AT. We conducted a multi-part qualitative study with 12 blind participants: an interview on unique uses of AT, a two-week diary study to log use cases, and a scenario-based design session to imagine creating future technologies. We found that participants work to design new AT both implicitly through creative use cases, and explicitly through regular ideation and development. Participants envisioned creating a variety of new technologies, and we summarize expected benefits and concerns of using a DIY technology approach. From our results, we present design considerations for future DIY technology systems to support existing customization and ‘hacking’ behaviors.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {57},
numpages = {17},
keywords = {Accessibility, Assistive technology, Blind, Design, Do-It-Yourself, Interview, Visual impairment},
location = {Hamburg, Germany},
series = {CHI '23}
}

@inproceedings{Sharif_VoxLens,
author = {Sharif, Ather and Wang, Olivia H. and Muongchan, Alida T. and Reinecke, Katharina and Wobbrock, Jacob O.},
title = {VoxLens: Making Online Data Visualizations Accessible with an Interactive JavaScript Plug-In},
year = {2022},
isbn = {9781450391573},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3491102.3517431},
doi = {10.1145/3491102.3517431},
abstract = {JavaScript visualization libraries are widely used to create online data visualizations but provide limited access to their information for screen-reader users. Building on prior findings about the experiences of screen-reader users with online data visualizations, we present VoxLens, an open-source JavaScript plug-in that—with a single line of code—improves the accessibility of online data visualizations for screen-reader users using a multi-modal approach. Specifically, VoxLens enables screen-reader users to obtain a holistic summary of presented information, play sonified versions of the data, and interact with visualizations in a “drill-down” manner using voice-activated commands. Through task-based experiments with 21 screen-reader users, we show that VoxLens improves the accuracy of information extraction and interaction time by 122\% and 36\%, respectively, over existing conventional interaction with online data visualizations. Our interviews with screen-reader users suggest that VoxLens is a “game-changer” in making online data visualizations accessible to screen-reader users, saving them time and effort.},
booktitle = {Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems},
articleno = {478},
numpages = {19},
keywords = {Visualizations, accessibility, blind, low-vision., screen readers, voice-based interaction},
location = {New Orleans, LA, USA},
series = {CHI '22}
}

@inproceedings{Zuniga_LowCostDIY,
author = {Zuniga-Zabala, Maria Fernanda and Guerra-Gomez, John Alexis},
title = {Lessons Learned Building Low-Cost DIY Tactile Graphics and Conducting a Tactile Drawing Club in Colombia During COVID-19},
year = {2022},
isbn = {9781450391566},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3491101.3503559},
doi = {10.1145/3491101.3503559},
abstract = {Perceiving images and drawing are fundamental parts of human life, and thus access to them should be a universal right. However, there is a large breach for people with visual impairments to access diverse graphics, let alone drawing. There are several techniques of tactile graphics, such as swell paper, Braille embossing, and thermoform that help to alleviate this gap. However, in developing countries, the high cost and lack of access make them impractical. In this work, we describe our experience improving access to tactile graphics and drawing in Colombia. We created low-cost, effective and efficient, tactile graphics and drawing techniques that improve on current solutions. These techniques were created from the best practices of two projects adapting pieces from the Colombian art heritage&nbsp;[52, 53] for blind and visually impaired people. They were then applied to a third project: running a virtual tactile drawing club with blind and visually impaired participants in the middle of the COVID-19 pandemic. The lessons learned from these experiences are presented in this paper with the hope they can help the community democratize access to tactile graphics.},
booktitle = {Extended Abstracts of the 2022 CHI Conference on Human Factors in Computing Systems},
articleno = {30},
numpages = {10},
keywords = {Blind, Blind Embossing, DIY, Home, Non-visual Interaction, Remote Participation, Tactile, Tactile Drawing, Tactile Graphics, Visual Impairments},
location = {New Orleans, LA, USA},
series = {CHI EA '22}
}

@inproceedings{Mack_HighQualityAltText,
author = {Mack, Kelly and Cutrell, Edward and Lee, Bongshin and Morris, Meredith Ringel},
title = {Designing Tools for High-Quality Alt Text Authoring},
year = {2021},
isbn = {9781450383066},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3441852.3471207},
doi = {10.1145/3441852.3471207},
abstract = {Alternative (alt) text provides access to descriptions of digital images for people who use screen readers. While prior work studied screen reader users’ (SRUs’) preferences about alt text and automatic alt text (i.e., alt text generated by artificial intelligence), little work examined the alt text author’s experience composing or editing these descriptions. We built two types of prototype interfaces for two tasks: authoring alt text and providing feedback on automatic alt text. Through combined interview-usability testing sessions with alt text authors and interviews with SRUs, we tested the effectiveness of our prototypes in the context of Microsoft PowerPoint. Our results suggest that authoring interfaces that support authors in choosing what to include in their descriptions result in higher quality alt text. The feedback interfaces highlighted considerable differences in the perceptions of authors and SRUs regarding “high-quality” alt text. Finally, authors crafted significantly lower quality alt text when starting from the automatic alt text compared to starting from a blank box. We discuss the implications of these results on applications that support alt text.},
booktitle = {Proceedings of the 23rd International ACM SIGACCESS Conference on Computers and Accessibility},
articleno = {23},
numpages = {14},
keywords = {alt text, alt text authoring, feedback on automatic alt text, high-quality alt text, screen reader users},
location = {Virtual Event, USA},
series = {ASSETS '21}
}

@article{Edwards_HowAltTextGetsMade,
author = {Edwards, Emory J. and Gilbert, Michael and Blank, Emily and Branham, Stacy M.},
title = {How the Alt Text Gets Made: What Roles and Processes of Alt Text Creation Can Teach Us About
Inclusive Imagery},
year = {2023},
issue_date = {June 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {2},
issn = {1936-7228},
url = {https://doi.org/10.1145/3587469},
doi = {10.1145/3587469},
abstract = {Many studies within Accessible Computing have investigated image accessibility, from what should be included in alternative text (alt text), to possible automated, human-in-the-loop, or crowdsourced approaches to alt text generation. However, the processes through which practitioners make alt text in situ have rarely been discussed. Through interviews with three artists and three accessibility practitioners working with Google, as well as 25 end users, we identify four processes of alt text creation used by this company—The User-Evaluation Process, The Lone Writer Process, The Team Write-A-Thon Process, and The Artist-Writer Process—and unpack their potential strengths and weaknesses as they relate to access and inclusive imagery. We conclude with a discussion of what alt text researchers and industry professionals can learn from considering alt text in situ, including opportunities to support user feedback, cross-contributor consistency, and organizational or technical changes to production processes.},
journal = {ACM Trans. Access. Comput.},
month = {jul},
articleno = {18},
numpages = {28},
keywords = {image accessibility, inclusive imagery, accessibility practitioners, Alt text creation processes}
}

@inproceedings{Singh_FigureA11y,
author = {Singh, Nikhil and Wang, Lucy Lu and Bragg, Jonathan},
title = {FigurA11y: AI Assistance for Writing Scientific Alt Text},
year = {2024},
isbn = {9798400705083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3640543.3645212},
doi = {10.1145/3640543.3645212},
abstract = {High-quality alt text is crucial for making scientific figures accessible to blind and low-vision readers. Crafting complete, accurate alt text is challenging even for domain experts, as published figures often depict complex visual information and readers have varied informational needs. These challenges, along with high diversity in figure types and domain-specific details, also limit the usefulness of fully automated approaches. Consequently, the prevalence of high-quality alt text is very low in scientific papers today. We investigate whether and how human-AI collaborative editing systems can help address the difficulty of writing high-quality alt text for complex scientific figures. We present FigurA11y, an interactive system that generates draft alt text and provides suggestions for author revisions using a pipeline driven by extracted figure and paper metadata. We test two versions, motivated by prior work on visual accessibility and writing support. The base Draft+Revise&nbsp;version provides authors with an automatically generated draft description to revise, along with extracted figure metadata and figure-specific alt text guidelines to support the revision process. The full Interactive Assistance&nbsp;version further adds contextualized suggestions: text snippets to iteratively produce descriptions, and hypothetical user questions with possible answers to reveal potential ambiguities and resolutions. In a study of authors (N=14), we found the system assisted them in efficiently producing descriptive alt text. Generated drafts and interface elements enabled authors to quickly initiate and edit detailed descriptions. Additionally, interactive suggestions from the full system prompted more iteration and highlighted aspects for authors to consider, resulting in greater deviation from the drafts without increased average cognitive load or manual effort.},
booktitle = {Proceedings of the 29th International Conference on Intelligent User Interfaces},
pages = {886–906},
numpages = {21},
keywords = {Accessibility, Alt text, Human-AI interaction, Image descriptions, Large language models, Natural language generation, Scientific figures, Writing assistance systems},
location = {Greenville, SC, USA},
series = {IUI '24}
}

@inproceedings{WilliamsDeGreef_QualityAltTextInComputing,
author = {Williams, Candace and de Greef, Lilian and Harris, Ed and Findlater, Leah and Pavel, Amy and Bennett, Cynthia},
title = {Toward supporting quality alt text in computing publications},
year = {2022},
isbn = {9781450391702},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3493612.3520449},
doi = {10.1145/3493612.3520449},
abstract = {While researchers have examined alternative (alt) text for social media and news contexts, few have studied the status and challenges for authoring alt text of figures in computing-related publications. These figures are distinct, often conveying dense visual information, and may necessitate unique accessibility solutions. Accordingly, we explored how to support authors in creating alt text in computing publications---specifically in the field of human-computer interaction (HCI). We conducted two studies: (1) an analysis of 300 recently published figures at a general HCI conference (ACM CHI), and (2) interviews with 10 researchers in HCI and related fields who have varying levels of experience writing alt text. Our findings characterize the prevalence, quality, and patterns of recent figure alt text and captions. We further identify challenges authors encounter, describing their workflow barriers and confusions around how to compose alt text for complex figures. We conclude by outlining a research agenda on process, education, and tooling opportunities to improve alt text in computing-related publications.},
booktitle = {Proceedings of the 19th International Web for All Conference},
articleno = {20},
numpages = {12},
keywords = {vision impairment, screen readers, scientific figures, data representations, blind, alt text, accessibility},
location = {Lyon, France},
series = {W4A '22}
}

@inproceedings{Kim_DescriptionsForComics,
author = {Kim, Suhyun and Lee, Semin and Kim, Kyungok and Oh, Uran},
title = {Utilizing a Dense Video Captioning Technique for Generating Image Descriptions of Comics for People with Visual Impairments},
year = {2024},
isbn = {9798400705083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3640543.3645154},
doi = {10.1145/3640543.3645154},
abstract = {To improve the accessibility of visual figures, auto-generation of text description of individual images has been studied. However, it cannot be directly applied to comics as the descriptions can be redundant as similar scenes appear in a row. To address this issue, we propose generating the descriptions per group of related images and demonstrate how an dense captioning technique for videos can be utilized for this purpose and ways to improve its performance. To assess the effectiveness of our approach and to identify factors affecting the quality of text descriptions of comics, we conducted a preliminary study with 3 sighted evaluators and a main user study with 12 participants with visual impairments. The results show that text descriptions generated per group of images are perceived to be better than those generated per image in terms of accuracy, clarity, understandability, length, informativeness and preference for sighted groups, when annotator is human. In the same conditions, when the annotator is AI, it exhibited better performance in terms of length. Also, people with visual impairments prefer group descriptions because of conciseness, smooth connectivity of sentences, and non-repetitive features. Based on the findings, we provide design recommendations for generating accessible comic descriptions at a scale for blind users.},
booktitle = {Proceedings of the 29th International Conference on Intelligent User Interfaces},
pages = {750–760},
numpages = {11},
keywords = {comics, dense video captioning, image description, people with visual impairment},
location = {Greenville, SC, USA},
series = {IUI '24}
}

@inproceedings{Cavazos_MultiModalArtA11y,
author = {Cavazos Quero, Luis and Iranzo Bartolom\'{e}, Jorge and Lee, Seonggu and Han, En and Kim, Sunhee and Cho, Jundong},
title = {An Interactive Multimodal Guide to Improve Art Accessibility for Blind People},
year = {2018},
isbn = {9781450356503},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3234695.3241033},
doi = {10.1145/3234695.3241033},
abstract = {The development of 3D printing technology has improved the engagement of the visually impaired people when experiencing two-dimensional visual artworks. However, it is still difficult to explore, experience and get a clear understanding. We introduce an interactive multimodal guide in which a 3D printed 2.5D representation of a painting can be explored by touch. Touching determined features in the representation triggers localized verbal, audio, wind, and light/heat feedback events that convey spatial and semantic information. In this work we present a working prototype developed through three sessions using a participatory design approach.},
booktitle = {Proceedings of the 20th International ACM SIGACCESS Conference on Computers and Accessibility},
pages = {346–348},
numpages = {3},
keywords = {multimodal guide, blind people, assitive technologies, 3d printing},
location = {Galway, Ireland},
series = {ASSETS '18}
}

 @misc{oko,  title={OKO App Leverages AI To Help Blind Pedestrians Recognize Traffic Signals},  url={https://www.forbes.com/sites/gusalexiou/2023/08/10/oko-app-deploys-ai-to-make-crossing-the-street-safer-for-blind-pedestrians/},  year={2023},  month={Aug} }

@inproceedings{Huh_GenAssist,
author = {Huh, Mina and Peng, Yi-Hao and Pavel, Amy},
title = {GenAssist: Making Image Generation Accessible},
year = {2023},
isbn = {9798400701320},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3586183.3606735},
doi = {10.1145/3586183.3606735},
abstract = {Blind and low vision (BLV) creators use images to communicate with sighted audiences. However, creating or retrieving images is challenging for BLV creators as it is difficult to use authoring tools or assess image search results. Thus, creators limit the types of images they create or recruit sighted collaborators. While text-to-image generation models let creators generate high-fidelity images based on a text description (i.e. prompt), it is difficult to assess the content and quality of generated images. We present GenAssist, a system to make text-to-image generation accessible. Using our interface, creators can verify whether generated image candidates followed the prompt, access additional details in the image not specified in the prompt, and skim a summary of similarities and differences between image candidates. To power the interface, GenAssist uses a large language model to generate visual questions, vision-language models to extract answers, and a large language model to summarize the results. Our study with 12 BLV creators demonstrated that GenAssist enables and simplifies the process of image selection and generation, making visual authoring more accessible to all.},
booktitle = {Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology},
articleno = {38},
numpages = {17},
keywords = {Accessibility, Creativity Support Tools, Generative AI, Image Generation},
location = {San Francisco, CA, USA},
series = {UIST '23}
}

@inproceedings{Lee_RSAs,
author = {Lee, Sooyeon and Yu, Rui and Xie, Jingyi and Billah, Syed Masum and Carroll, John M.},
title = {Opportunities for Human-AI Collaboration in Remote Sighted Assistance},
year = {2022},
isbn = {9781450391443},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3490099.3511113},
doi = {10.1145/3490099.3511113},
abstract = {Remote sighted assistance (RSA) has emerged as a conversational assistive technology for people with visual impairments (VI), where remote sighted agents provide realtime navigational assistance to users with visual impairments via video-chat-like communication. In this paper, we conducted a literature review and interviewed 12 RSA users to comprehensively understand technical and navigational challenges in RSA for both the agents and users. Technical challenges are organized into four categories: agents’ difficulties in orienting and localizing the users; acquiring the users’ surroundings and detecting obstacles; delivering information and understanding user-specific situations; and coping with a poor network connection. Navigational challenges are presented in 15 real-world scenarios (8 outdoor, 7 indoor) for the users. Prior work indicates that computer vision (CV) technologies, especially interactive 3D maps and realtime localization, can address a subset of these challenges. However, we argue that addressing the full spectrum of these challenges warrants new development in Human-CV collaboration, which we formalize as five emerging problems: making object recognition and obstacle avoidance algorithms blind-aware; localizing users under poor networks; recognizing digital content on LCD screens; recognizing texts on irregular surfaces; and predicting the trajectory of out-of-frame pedestrians or objects. Addressing these problems can advance computer vision research and usher into the next generation of RSA service.},
booktitle = {Proceedings of the 27th International Conference on Intelligent User Interfaces},
pages = {63–78},
numpages = {16},
keywords = {3D maps, RSA, artificial intelligence, augmented reality, blind, camera, computer vision, conversational assistance, navigation, people with visual impairments, remote sighted assistance, smartphone},
location = {Helsinki, Finland},
series = {IUI '22}
}

@inproceedings{hwang_creative,
author = {Hwang, Angel Hsing-Chi},
title = {Too Late to be Creative? AI-Empowered Tools in Creative Processes},
year = {2022},
isbn = {9781450391566},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3491101.3503549},
doi = {10.1145/3491101.3503549},
abstract = {The present case study examines the product landscape of current AI-empowered co-creative tools. Specifically, I review literature in both creativity and HCI research and investigate how these tools support different stages in humans’ creative processes and how common challenges in human-AI interaction (HAII) are addressed. I find these AI-driven tools mostly support the generation and execution of ideas and are less involved in the early stages of co-creation. Moreover, HAII challenges identified in other fields receive little attention in the creative domain. Based on a synthetic analysis, I elaborate on how future tools can leverage the ”non-human” quality of AI to achieve innovation through a more human-centered, collaborative journey.},
booktitle = {Extended Abstracts of the 2022 CHI Conference on Human Factors in Computing Systems},
articleno = {38},
numpages = {9},
keywords = {human-AI interaction, creativity support tool, creativity},
location = {New Orleans, LA, USA},
series = {CHI EA '22}
}

@inproceedings{Mohanbabu2024ContextAwareID,
  title={Context-Aware Image Descriptions for Web Accessibility},
  author={Ananya Gubbi Mohanbabu and Amy Pavel},
  year={2024},
  url={https://api.semanticscholar.org/CorpusID:272423782}
}

@misc{Galarce_Kennedy_2016, title={More than Pretty Pictures}, url={https://ascd.org/el/articles/more-than-pretty-pictures}, journal={ACSD.org}, author={Galarce, Patricia Crain de and Kennedy, Kathleen}, year={2016}, month=apr }

@article{Naik2023ContextVQATC,
  title={Context-VQA: Towards Context-Aware and Purposeful Visual Question Answering},
  author={Nandita Shankar Naik and Christopher Potts and Elisa Kreiss},
  journal={2023 IEEE/CVF International Conference on Computer Vision Workshops (ICCVW)},
  year={2023},
  pages={2813-2817},
  url={https://api.semanticscholar.org/CorpusID:260334278}
}

@article{Gates_SubjectiveAssessment,
author = {Leslie Gates},
title = {Embracing Subjective Assessment Practices: Recommendations for Art Educators},
journal = {Art Education},
volume = {70},
number = {1},
pages = {23--28},
year = {2017},
publisher = {Routledge},
doi = {10.1080/00043125.2017.1247565},
URL = { 
    
        https://doi.org/10.1080/00043125.2017.1247565
},
eprint = { 
        https://doi.org/10.1080/00043125.2017.1247565
}
}

@inproceedings{VQAsk,
author = {De Marsico, Maria and Giacanelli, Chiara and Manganaro, Clizia Giorgia and Palma, Alessio and Santoro, Davide},
title = {VQAsk: a multimodal Android GPT-based application to help blind users visualize pictures},
year = {2024},
isbn = {9798400717642},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3656650.3656677},
doi = {10.1145/3656650.3656677},
abstract = {VQAsk is an Android application that helps visually impaired users to get information about images framed by their smartphones. It enables to interact with one’s photographs or the surrounding visual environment through a question-and-answer interface integrating three modalities: speech interaction, haptic feedback that facilitates navigation and interaction, and sight. VQAsk is primarily designed to help visually impaired users mentally visualize what they cannot see, but it can also accommodate users with varying levels of visual ability. To this aim, it embeds advanced NLP and Computer Vision techniques to answer all user questions about the image on the cell screen. Image processing is enhanced by background removal through advanced segmentation models that identify important image elements. The outcomes of a testing phase confirmed the importance of this project as a first attempt at using AI-supported multimodality to enhance visually impaired users’ experience.},
booktitle = {Proceedings of the 2024 International Conference on Advanced Visual Interfaces},
articleno = {39},
numpages = {5},
keywords = {Visual Question Answering, natural language processing and computer vision for scene interpretation, visually impaired users},
location = {Arenzano, Genoa, Italy},
series = {AVI '24}
}

@inproceedings{Chen_AudioText,
author = {Chen, Si and Wang, Dennis and Huang, Yun},
title = {Exploring the Complementary Features of Audio and Text Notes for Video-based Learning in Mobile Settings},
year = {2021},
isbn = {9781450380959},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411763.3451801},
doi = {10.1145/3411763.3451801},
abstract = {In this paper, we compared audio and text note-taking behaviors for video-based learning in various mobile settings. We designed and implemented a note-taking tool and conducted a task-based study to examine how users took audio and text notes differently. Our results show that participants’ audio notes were significantly longer than text notes; longer audio notes were taken to capture unfamiliar video content and participants’ emotions. However, audio notes also raised several privacy concerns. Text notes allowed participants to revise for better accuracy and deeper reflection. Our findings of the complementary features of audio and text notes for video-based learning shed light on designing future note-taking tools that can be used to facilitate learning in varied mobile settings.},
booktitle = {Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems},
articleno = {310},
numpages = {7},
keywords = {Audio interaction, Mixed-method study, Note-taking},
location = {Yokohama, Japan},
series = {CHI EA '21}
}

@inproceedings{darth_vader,
author = {Newman, Michele and Sun, Kaiwen and Dalla Gasperina, Ilena B and Shin, Grace Y. and Pedraja, Matthew Kyle and Kanchi, Ritesh and Song, Maia B. and Li, Rannie and Lee, Jin Ha and Yip, Jason},
title = {"I want it to talk like Darth Vader": Helping Children Construct Creative Self-Efficacy with Generative AI},
year = {2024},
isbn = {9798400703300},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613904.3642492},
doi = {10.1145/3613904.3642492},
abstract = {The emergence of generative artificial intelligence (GenAI) has ignited discussions surrounding its potential to enhance creative pursuits. However, distinctions between children’s and adult’s creative needs exist, which is important when considering the possibility of GenAI for children’s creative usage. Building upon work in Human-Computer Interaction (HCI), fostering children’s computational thinking skills, this study explores interactions between children (aged 7-13) and GenAI tools through methods of participatory design. We seek to answer two questions: (1) How do children in co-design workshops perceive GenAI tools and their usage for creative works? and (2) How do children navigate the creative process while using GenAI tools? How might these interactions support their confidence in their ability to create? Our findings contribute a model that describes the potential contexts underpinning child-GenAI creative interactions and explores implications of this model for theories of creativity, design, and use of GenAI as a constructionist tool for creative self-efficacy.},
booktitle = {Proceedings of the CHI Conference on Human Factors in Computing Systems},
articleno = {117},
numpages = {18},
keywords = {Artificial Intelligence, Children, Co-Design, Constructionism, Creativity, Participatory Design},
location = {Honolulu, HI, USA},
series = {CHI '24}
}

@inproceedings{Das_ProvenanceToAberrations,
author = {Das, Maitraye and Fiannaca, Alexander J. and Morris, Meredith Ringel and Kane, Shaun K. and Bennett, Cynthia L.},
title = {From Provenance to Aberrations: Image Creator and Screen Reader User Perspectives on Alt Text for AI-Generated Images},
year = {2024},
isbn = {9798400703300},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613904.3642325},
doi = {10.1145/3613904.3642325},
abstract = {AI-generated images are proliferating as a new visual medium. However, state-of-the-art image generation models do not output alternative (alt) text with their images, rendering them largely inaccessible to screen reader users (SRUs). Moreover, less is known about what information would be most desirable to SRUs in this new medium. To address this, we invited AI image creators and SRUs to evaluate alt text prepared from various sources and write their own alt text for AI images. Our mixed-methods analysis makes three contributions. First, we highlight creators’ perspectives on alt text, as creators are well-positioned to write descriptions of their images. Second, we illustrate SRUs’ alt text needs particular to the emerging medium of AI images. Finally, we discuss the promises and pitfalls of utilizing text prompts written as input for AI models in alt text generation, and areas where broader digital accessibility guidelines could expand to account for AI images.},
booktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},
articleno = {900},
numpages = {21},
keywords = {AI art, Accessibility, Alt text, Blind, Screen reader users, Text-to-Image},
location = {Honolulu, HI, USA},
series = {CHI '24}
}

@inproceedings{bennett_itscomplicated,
author = {Bennett, Cynthia L. and Gleason, Cole and Scheuerman, Morgan Klaus and Bigham, Jeffrey P. and Guo, Anhong and To, Alexandra},
title = {“It’s Complicated”: Negotiating Accessibility and (Mis)Representation in Image Descriptions of Race, Gender, and Disability},
year = {2021},
isbn = {9781450380966},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411764.3445498},
doi = {10.1145/3411764.3445498},
abstract = {Content creators are instructed to write textual descriptions of visual content to make it accessible; yet existing guidelines lack specifics on how to write about people’s appearance, particularly while remaining mindful of consequences of (mis)representation. In this paper, we report on interviews with screen reader users who were also Black, Indigenous, People of Color, Non-binary, and/or Transgender on their current image description practices and preferences, and experiences negotiating theirs and others’ appearances non-visually. We discuss these perspectives, and the ethics of humans and AI describing appearance characteristics that may convey the race, gender, and disabilities of those photographed. In turn, we share considerations for more carefully describing appearance, and contexts in which such information is perceived salient. Finally, we offer tensions and questions for accessibility research to equitably consider politics and ecosystems in which technologies will embed, such as potential risks of human and AI biases amplifying through image descriptions.},
booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},
articleno = {375},
numpages = {19},
keywords = {AI, Accessibility, Blind, Disability, Gender, Image descriptions, Race, Visual impairments},
location = {Yokohama, Japan},
series = {CHI '21}
}

@inproceedings{gonzalez_CHI,
author = {Gonzalez Penuela, Ricardo E and Collins, Jazmin and Bennett, Cynthia and Azenkot, Shiri},
title = {Investigating Use Cases of AI-Powered Scene Description Applications for Blind and Low Vision People},
year = {2024},
isbn = {9798400703300},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613904.3642211},
doi = {10.1145/3613904.3642211},
abstract = {"Scene description" applications that describe visual content in a photo are useful daily tools for blind and low vision (BLV) people. Researchers have studied their use, but they have only explored those that leverage remote sighted assistants; little is known about applications that use AI to generate their descriptions. Thus, to investigate their use cases, we conducted a two-week diary study where 16 BLV participants used an AI-powered scene description application we designed. Through their diary entries and follow-up interviews, users shared their information goals and assessments of the visual descriptions they received. We analyzed the entries and found frequent use cases, such as identifying visual features of known objects, and surprising ones, such as avoiding contact with dangerous objects. We also found users scored the descriptions relatively low on average, 2.76 out of 5 (SD=1.49) for satisfaction and 2.43 out of 4 (SD=1.16) for trust, showing that descriptions still need significant improvements to deliver satisfying and trustworthy experiences. We discuss future opportunities for AI as it becomes a more powerful accessibility tool for BLV users.},
booktitle = {Proceedings of the CHI Conference on Human Factors in Computing Systems},
articleno = {901},
numpages = {21},
keywords = {AI, Assistive Technology, Blind and Low Vision People, Computer Vision, Diary Study, Scene Description, Use cases},
location = {Honolulu, HI, USA},
series = {CHI '24}
}

@inproceedings{Dietz_ContextQ,
author = {Dietz Smith, Griffin and Prasad, Siddhartha and Davidson, Matt J. and Findlater, Leah and Shapiro, R. Benjamin},
title = {ContextQ: Generated Questions to Support Meaningful Parent-Child Dialogue While Co-Reading},
year = {2024},
isbn = {9798400704420},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3628516.3655809},
doi = {10.1145/3628516.3655809},
abstract = {Much of early literacy education happens at home with caretakers reading books to young children. Prior research demonstrates how having dialogue with children during co-reading can develop critical reading readiness skills, but most adult readers are unsure if and how to lead effective conversations. We present ContextQ, a tablet-based reading application to unobtrusively present auto-generated dialogic questions to caretakers to support this dialogic reading practice. An ablation study demonstrates how our method of encoding educator expertise into the question generation pipeline can produce high-quality output; and through a user study with 12 parent-child dyads (child age: 4–6), we demonstrate that this system can serve as a guide for parents in leading contextually meaningful dialogue, leading to significantly more conversational turns from both the parent and the child and deeper conversations with connections to the child’s everyday life.},
booktitle = {Proceedings of the 23rd Annual ACM Interaction Design and Children Conference},
pages = {408–423},
numpages = {16},
keywords = {co-reading, dialogic reading, large language models, literacy, parent-child interaction, question generation},
location = {Delft, Netherlands},
series = {IDC '24}
}

@inproceedings{Luebs_ExpertCraftingPractices,
author = {Luebs, Wenhao and Tigwell, Garreth and Shinohara, Kristen},
year = {2024},
month = {05},
pages = {},
title = {Understanding Expert Crafting Practices of Blind and Low Vision Creatives},
doi = {10.1145/3613905.3650960}
}

@inproceedings{Li_TactileTemplates,
author = {Li, Jingyi and Kim, Son and Miele, Joshua A. and Agrawala, Maneesh and Follmer, Sean},
title = {Editing Spatial Layouts through Tactile Templates for People with Visual Impairments},
year = {2019},
isbn = {9781450359702},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3290605.3300436},
doi = {10.1145/3290605.3300436},
abstract = {Spatial layout is a key component in graphic design. While people who are blind or visually impaired (BVI) can use screen readers or magnifiers to access digital content, these tools fail to fully communicate the content's graphic design information. Through semi-structured interviews and contextual inquiries, we identify the lack of this information and feedback as major challenges in understanding and editing layouts. Guided by these insights and a co-design process with a blind hobbyist web developer, we developed an interactive, multimodal authoring tool that lets blind people understand spatial relationships between elements and modify layout templates. Our tool automatically generates tactile print-outs of a web page's layout, which users overlay on top of a tablet that runs our self-voicing digital design tool. We conclude with design considerations grounded in user feedback for improving the accessibility of spatially encoded information and developing tools for BVI authors.},
booktitle = {Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems},
pages = {1–11},
numpages = {11},
keywords = {accessibility, accessible design tools, accessible web design, blindness, layout design, multimodal interfaces, tactile overlays, templates, visual impairments},
location = {Glasgow, Scotland Uk},
series = {CHI '19}
}

@inproceedings{Lin_FishScales,
author = {Lin, Grace C. and Schoenfeld, Ilana and Thompson, Meredith and Xia, Yiting and Uz-Bilgin, Cigdem and Leech, Kathryn},
title = {”What color are the fish’s scales?” Exploring parents’ and children’s natural interactions with a child-friendly virtual agent during storybook reading},
year = {2022},
isbn = {9781450391979},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3501712.3529734},
doi = {10.1145/3501712.3529734},
abstract = {With increasing integration of AI-powered agents into educational technologies available to families with young children, the landscape of how caregivers and children interact together or separately with these technologies is underexplored. Understanding the nature of these interactions could critically inform the design of educational technologies to facilitate children’s learning, research methods for use in future studies involving adult-child dyad technology use, and policy decisions regarding the use of educational technology with young children. In this study, we explored the natural interactions among parent, child, and a child-friendly virtual rabbit character named Floppy. Floppy is a virtual agent in a Smart Speaker app that models adult dialogic reading and conversational strategies for use with young children (ages 4-6 years). Over a span of four to six weeks, 18 parent-child dyads read The Rainbow Fish, a classic children’s book by Marcus Pfister, during 24 at-home, remote sessions with Floppy. Of the 189 conversations generated during this time, 125 were initiated by a prompt spoken by Floppy. Though there were some variations among the dyads, across all conversations, parent-driven interactions made up 63\% of the conversations, followed by child-driven conversations at 15.3\%, Floppy-driven at 14.3\%, and Floppy-and-parent-driven at 7.4\%. A select few parents were more comfortable having their children interact directly with Floppy, whereas the majority of the parents would direct children’s attention back to themselves or help children understand the questions by repeating or reformulating Floppy’s prompts. More than half of the parents reported that their children formed emotional connections with the virtual character. These findings point to a need to clearly define the role of virtual agents, even ones with limited AI, in this type of triadic interaction.},
booktitle = {Proceedings of the 21st Annual ACM Interaction Design and Children Conference},
pages = {185–195},
numpages = {11},
keywords = {virtual agent, triadic interaction, early literacy},
location = {Braga, Portugal},
series = {IDC '22}
}

@inproceedings{ASL_DeafChild_HearingParent,
author = {Hossain, Ekram and Bao, Ashley and Newman, Kaleb Slater and Mann, Madeleine and Wang, Hecong and Li, Yifan and Kurumada, Chigusa and Hall, Wyatte and Bai, Zhen},
title = {Supporting ASL Communication Between Hearing Parents and Deaf Children},
year = {2023},
isbn = {9798400702204},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597638.3614511},
doi = {10.1145/3597638.3614511},
abstract = {The vast majority of deaf or hard-of-hearing (DHH) children are born to hearing parents. Due to a lack of immersive exposure to their natural language - sign language - they are at severe risk of language deprivation. In response to this challenge, this paper presents a novel computer-mediated communication platform named Tabletop Interactive Play System (TIPS). It serves as a test-bed to investigate technical and ethical solutions that enable hearing parents to use American Sign Language (ASL) during face-to-face play with their with their DHH children. The TIPS platform offers a variety of user options in three key aspects: (1) ASL recommendation in alignment with hearing parents’ real-time speech; (2) ASL display through different form-factors (Augmented Reality (AR) projection, tablet, and smart glasses); and (3) autonomy support to enhance users’ sense of agency and trust in the system. In this paper, we will describe the system’s design, implementation, and preliminary evaluation results.},
booktitle = {Proceedings of the 25th International ACM SIGACCESS Conference on Computers and Accessibility},
articleno = {61},
numpages = {5},
keywords = {American Sign Language, Augmented Reality, Computer-Mediated Communication, Deaf or Hard-of-Hearing, Parent-Child Interaction},
location = {New York, NY, USA},
series = {ASSETS '23}
}

@inproceedings{Hossain_ContextResponsiveASL,
author = {Hossain, Ekram and Cahoon, Merritt Lee and Liu, Yao and Kurumada, Chigusa and Bai, Zhen},
title = {Context-responsive ASL Recommendation for Parent-Child Interaction},
year = {2022},
isbn = {9781450392587},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3517428.3550366},
doi = {10.1145/3517428.3550366},
abstract = {Parental language input in early childhood plays a critical role in lifelong neuro-cognitive and social development. Deaf and Hard of Hearing (DHH) children are often at risk of language deprivation due to hearing parents’ limited knowledge of sign language - the natural language for DHH children at birth. To offer an immersive sign language environment for DHH children, we designed a novel computer-mediated communication technology named Table Top Interactive System (TIPS). It aims to provide context-responsive recommendation of American Sign Language (ASL) in real-time for hearing parents during face-to-face joint play with their DHH children. The system emphasizes supporting parent autonomy by adapting ASL recommendations using parent’s speech during play, and minimizes obtrusion for face-to-face interaction through an Augmented Reality (AR) display. This paper describes the design and development of an initial working prototype of TIPS and preliminary results of the system’s efficiency regarding system latency and accuracy for ASL recommendation and visualization. Next, we plan to conduct a user study to gather expert and parent feedback about the system design and ASL recommendation strategies for long-term and personalized usage.},
booktitle = {Proceedings of the 24th International ACM SIGACCESS Conference on Computers and Accessibility},
articleno = {76},
numpages = {5},
keywords = {American Sign Language, Augmented Reality, Computer-mediated communication, Parent-Child Interaction},
location = {Athens, Greece},
series = {ASSETS '22}
}

@inproceedings{Gubbi_ContextAware,
author = {Gubbi Mohanbabu, Ananya and Pavel, Amy},
title = {Context-Aware Image Descriptions for Web Accessibility},
year = {2024},
isbn = {9798400706776},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3663548.3675658},
doi = {10.1145/3663548.3675658},
abstract = {Blind and low vision (BLV) internet users access images on the web via text descriptions. New vision-to-language models such as GPT-V, Gemini, and LLaVa can now provide detailed image descriptions on-demand. While prior research and guidelines state that BLV audiences’ information preferences depend on the context of the image, existing tools for accessing vision-to-language models provide only context-free image descriptions by generating descriptions for the image alone without considering the surrounding webpage context. To explore how to integrate image context into image descriptions, we designed a Chrome Extension that automatically extracts webpage context to inform GPT-4V-generated image descriptions. We gained feedback from 12 BLV participants in a user study comparing typical context-free image descriptions to context-aware image descriptions. We then further evaluated our context-informed image descriptions with a technical evaluation. Our user evaluation demonstrates that BLV participants frequently prefer context-aware descriptions to context-free descriptions. BLV participants also rate context-aware descriptions significantly higher in quality, imaginability, relevance, and plausibility. All participants shared that they wanted to use context-aware descriptions in the future and highlighted the potential for use in online shopping, social media, news, and personal interest blogs.},
booktitle = {Proceedings of the 26th International ACM SIGACCESS Conference on Computers and Accessibility},
articleno = {62},
numpages = {17},
keywords = {Accessibility, Context Awareness, Image Descriptions, Text;},
location = {St. John's, NL, Canada},
series = {ASSETS '24}
}

@inproceedings{Glazko_Autoethnography,
author = {Glazko, Kate S and Yamagami, Momona and Desai, Aashaka and Mack, Kelly Avery and Potluri, Venkatesh and Xu, Xuhai and Mankoff, Jennifer},
title = {An Autoethnographic Case Study of Generative Artificial Intelligence's Utility for Accessibility},
year = {2023},
isbn = {9798400702204},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597638.3614548},
doi = {10.1145/3597638.3614548},
abstract = {With the recent rapid rise in Generative Artificial Intelligence (GAI) tools, it is imperative that we understand their impact on people with disabilities, both positive and negative. However, although we know that AI in general poses both risks and opportunities for people with disabilities, little is known specifically about GAI in particular. To address this, we conducted a three-month autoethnography of our use of GAI to meet personal and professional needs as a team of researchers with and without disabilities. Our findings demonstrate a wide variety of potential accessibility-related uses for GAI while also highlighting concerns around verifiability, training data, ableism, and false promises.},
booktitle = {Proceedings of the 25th International ACM SIGACCESS Conference on Computers and Accessibility},
articleno = {99},
numpages = {8},
keywords = {ableism, accessibility, auto-ethnography, generative artificial intelligence},
location = {New York, NY, USA},
series = {ASSETS '23}
}

@article{PeerDebriefing,  title={Peer Debriefing in Qualitative Research: Emerging Operational Models},  volume={4},  url={http://journals.sagepub.com/doi/10.1177/107780049800400208},  DOI={10.1177/107780049800400208},  number={2},  publisher={SAGE Publications},  author={Spall, Sharon},  year={1998},  month={Jan},  pages={280–292} }

@inproceedings{Zhang_StoryBuddy,
author = {Zhang, Zheng and Xu, Ying and Wang, Yanhao and Yao, Bingsheng and Ritchie, Daniel and Wu, Tongshuang and Yu, Mo and Wang, Dakuo and Li, Toby Jia-Jun},
title = {StoryBuddy: A Human-AI Collaborative Chatbot for Parent-Child Interactive Storytelling with Flexible Parental Involvement},
year = {2022},
isbn = {9781450391573},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3491102.3517479},
doi = {10.1145/3491102.3517479},
abstract = {Despite its benefits for children’s skill development and parent-child bonding, many parents do not often engage in interactive storytelling by having story-related dialogues with their child due to limited availability or challenges in coming up with appropriate questions. While recent advances made AI generation of questions from stories possible, the fully-automated approach excludes parent involvement, disregards educational goals, and underoptimizes for child engagement. Informed by need-finding interviews and participatory design (PD) results, we developed StoryBuddy, an AI-enabled system for parents to create interactive storytelling experiences. StoryBuddy’s design highlighted the need for accommodating dynamic user needs between the desire for parent involvement and parent-child bonding and the goal of minimizing parent intervention when busy. The PD revealed varied assessment and educational goals of parents, which StoryBuddy addressed by supporting configuring question types and tracking child progress. A user study validated StoryBuddy’s usability and suggested design insights for future parent-AI collaboration systems.},
booktitle = {Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems},
articleno = {218},
numpages = {21},
keywords = {child-agent interactions, co-reading, dialogic reading, human-AI collaboration, interactive storytelling, voice user interfaces},
location = {New Orleans, LA, USA},
series = {CHI '22}
}

@inproceedings{FloresSaviaga_InclusivePortraits,
author = {Flores-Saviaga, Claudia and Curtis, Christopher and Savage, Saiph},
title = {Inclusive Portraits: Race-Aware Human-in-the-Loop Technology},
year = {2023},
isbn = {9798400703812},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3617694.3623235},
doi = {10.1145/3617694.3623235},
abstract = {AI has revolutionized the processing of various services, including the automatic facial verification of people. Automated approaches have demonstrated their speed and efficiency in verifying a large volume of faces, but they can face challenges when processing content from certain communities, including communities of people of color. This challenge has prompted the adoption of "human-in-the-loop" (HITL) approaches, where human workers collaborate with the AI to minimize errors. However, most HITL approaches do not consider workers’ individual characteristics and backgrounds. This paper proposes a new approach, called Inclusive Portraits (IP), that connects with social theories around race to design a racially-aware human-in-the-loop system. Our experiments have provided evidence that incorporating race into human-in-the-loop (HITL) systems for facial verification can significantly enhance performance, especially for services delivered to people of color. Our findings also highlight the importance of considering individual worker characteristics in the design of HITL systems, rather than treating workers as a homogenous group. Our research has significant design implications for developing AI-enhanced services that are more inclusive and equitable.},
booktitle = {Proceedings of the 3rd ACM Conference on Equity and Access in Algorithms, Mechanisms, and Optimization},
articleno = {15},
numpages = {11},
location = {Boston, MA, USA},
series = {EAAMO '23}
}

@inproceedings{Mack_AnticipateAndAdjust,
author = {Mack, Kelly and McDonnell, Emma and Potluri, Venkatesh and Xu, Maggie and Zabala, Jailyn and Bigham, Jeffrey and Mankoff, Jennifer and Bennett, Cynthia},
title = {Anticipate and Adjust: Cultivating Access in Human-Centered Methods},
year = {2022},
isbn = {9781450391573},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3491102.3501882},
doi = {10.1145/3491102.3501882},
abstract = {Methods are fundamental to doing research and can directly impact who is included in scientific advances. Given accessibility research's increasing popularity and pervasive barriers to conducting and participating in research experienced by people with disabilities, it is critical to ask how methods are made accessible. Yet papers rarely describe their methods in detail. This paper reports on 17 interviews with accessibility experts about how they include both facilitators and participants with disabilities in popular user research methods. Our findings offer strategies for anticipating access needs while remaining flexible and responsive to unexpected access barriers. We emphasize the importance of considering accessibility at all stages of the research process, and contextualize access work in recent disability and accessibility literature. We explore how technology or processes could reflect a norm of accessibility. Finally, we discuss how various needs intersect and conflict and offer a practical structure for planning accessible research.},
booktitle = {Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems},
articleno = {603},
numpages = {18},
keywords = {Human-Centered methods, accessibility, disability, user research},
location = {New Orleans, LA, USA},
series = {CHI '22}
}

@article{Bosgraaf2020,
  author  = {Bosgraaf, L. and Spreen, M. and Pattiselanno, K. and van Hooren, S.},
  title   = {Art Therapy for Psychosocial Problems in Children and Adolescents: A Systematic Narrative Review on Art Therapeutic Means and Forms of Expression, Therapist Behavior, and Supposed Mechanisms of Change},
  journal = {Frontiers in Psychology},
  year    = {2020},
  volume  = {11},
  pages   = {584685},
  doi     = {10.3389/fpsyg.2020.584685},
  pmid    = {33132993},
  pmcid   = {PMC7578380},
  date    = {2020-10-08}
}


@inproceedings{ThematicAnalysisHCI,
author = {Bowman, Robert and Nadal, Camille and Morrissey, Kellie and Thieme, Anja and Doherty, Gavin},
title = {Using Thematic Analysis in Healthcare HCI at CHI: A Scoping Review},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3581203},
doi = {10.1145/3544548.3581203},
abstract = {CHI papers researching healthcare human-computer interaction (HCI) are increasingly reporting the use of “thematic analysis” (TA). TA refers to a range of flexible and evolving approaches for qualitative data analysis. Its increased use demonstrates a change in research practices, and with that the emergence of new local standards. We need to understand and reflect upon these emerging local practices, including departures from what is advocated as quality TA practice more generally. Toward this, we conducted a scoping review of a decade of CHI publications (2012&nbsp;–&nbsp;2021) that researched healthcare and termed their analysis approach “thematic analysis”; 78 papers reporting a total of 100 TAs were included. We contribute a description of 1) the contexts in which TA is being used, 2) the TA approaches being conducted, and 3) how TA is being reported. Drawing on this, we discuss opportunities to improve research practice when using TA in healthcare HCI.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {491},
numpages = {18},
keywords = {healthcare, qualitative research, thematic analysis},
location = {Hamburg, Germany},
series = {CHI '23}
}

@inproceedings{Xu_Bilingual,
author = {Xu, Ying and He, Kunlei and Vigil, Valery and Ojeda-Ramirez, Santiago and Liu, Xuechen and Levine, Julian and Cervera, Kelsyann and Warschauer, Mark},
title = {“Rosita Reads With My Family”: Developing A Bilingual Conversational Agent to Support Parent-Child Shared Reading},
year = {2023},
isbn = {9798400701313},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3585088.3589354},
doi = {10.1145/3585088.3589354},
abstract = {Bilingual children have unique needs for school readiness as they navigate between two languages and cultures. A supportive home language environment, where children are frequently exposed to language through conversation and reading, can positively impact their language development and prepare them for school. However, current conversational agents and e-books designed for children do not typically take into account the cultural and linguistic needs of bilingual children and do not involve parents. This project presents the development of a bilingual conversational agent and accompanying e-book, designed to support parent-child interactions and promote language development for Latinx Spanish-English bilingual children. Results from a user study indicate that the bilingual agent effectively engages children verbally and encourages parental involvement in reading processes. The study also provides design insights for creating conversational agents for bilingual children.},
booktitle = {Proceedings of the 22nd Annual ACM Interaction Design and Children Conference},
pages = {160–172},
numpages = {13},
keywords = {Latinx, bilingualism, conversational agent, culturally-responsive design, parent-child interaction, shared reading},
location = {Chicago, IL, USA},
series = {IDC '23}
}

@article{Braune_And_Clarke,
  title={Using thematic analysis in psychology},
  author={Braun, Virginia and Clarke, Victoria},
  journal={Qualitative Research in Psychology},
  volume={3},
  number={2},
  pages={77--101},
  year={2006},
  publisher={Taylor \& Francis},
  doi={10.1191/1478088706qp063oa},
  url={http://www.tandfonline.com/doi/abs/10.1191/1478088706qp063oa}
}

@book{braun2012thematic,
  title={Thematic analysis.},
  author={Braun, Virginia and Clarke, Victoria},
  year={2012},
  publisher={American Psychological Association}
}

@inproceedings{Jokela,
author = {Jokela, Tero and Lucero, Andr\'{e}s},
title = {MixedNotes: a digital tool to prepare physical notes for affinity diagramming},
year = {2014},
isbn = {9781450330060},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2676467.2676478},
doi = {10.1145/2676467.2676478},
abstract = {Affinity Diagramming is a technique to organize and make sense of qualitative data. It is commonly used in Contextual Design and HCI research. However, preparing notes for and building an Affinity Diagram remains a laborious process, with a wide variety of different approaches and practices. In this paper, we present MixedNotes, a novel technique to prepare physical paper notes for Affinity Diagramming, and a software tool to support this technique. The technique has been tested with large real-life Affinity Diagrams with overall positive results.},
booktitle = {Proceedings of the 18th International Academic MindTrek Conference: Media Business, Management, Content \& Services},
pages = {3–6},
numpages = {4},
keywords = {contextual design, affinity diagram, KJ method},
location = {Tampere, Finland},
series = {AcademicMindTrek '14}
}

@article{Bai2023ParticipatoryDO,  title={Participatory Design of AI with Children: Reflections on IDC Design Challenge}, author={Zhen Bai and Fran Power Judd and Naomi Polinsky and Elmira Yadollahi},
  journal={ArXiv}, url={http://arxiv.org/abs/2304.09091},  DOI={10.48550/arXiv.2304.09091},  year={2023},  month={Apr} }

@inproceedings{Druga_InclusiveAILiteracy,
author = {Druga, Stefania and Vu, Sarah T. and Likhith, Eesh and Qiu, Tammy},
title = {Inclusive AI literacy for kids around the world},
year = {2019},
isbn = {9781450362443},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3311890.3311904},
doi = {10.1145/3311890.3311904},
abstract = {We observed how 102 children (7-12 years old), from four different countries (U.S.A, Germany, Denmark, and Sweden), imagine smart devices and toys of the future and how they perceive current AI technologies. Children outside of U.S.A were overall more critical of these technologies and less exposed to them. The way children collaborated and communicated while describing their AI perception and expectations were influenced both by their social-economical and cultural background. Children in low and medium SES schools and centers were better are collaborating compared to high SES children, but had a harder time advancing because they had less experience with coding and interacting with these technologies. Children in high SES schools and centers had troubles collaborating initially but displayed a stronger understanding of AI concepts. Based on our initial findings we propose a series of guidelines for designing future hands-on learning activities with smart toys and AI devices for K8 students.},
booktitle = {Proceedings of FabLearn 2019},
pages = {104–111},
numpages = {8},
keywords = {Inclusive education, Child-Agent Interaction, AI literacy},
location = {New York, NY, USA},
series = {FL2019}
}

@inproceedings{Dietz2023TheoryOA,
  title={Theory of AI Mind: How adults and children reason about the “mental states” of conversational AI},
  author={Griffin Dietz and Joseph Outa and Lauren Lowe and James A. Landay and Hyowon Gweon},
  year={2023},
  url={https://api.semanticscholar.org/CorpusID:259325584}
}

@inproceedings{Bae_TouchingInformation,
author = {Bae, Sandra and Yang, Ruhan and Gyory, Peter and Uhr, Julia and Szafir, Danielle Albers and Do, Ellen Yi-Luen},
title = {Touching Information with DIY Paper Charts \& AR Markers},
year = {2021},
isbn = {9781450384520},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3459990.3465191},
doi = {10.1145/3459990.3465191},
abstract = {Fostering data literacy has largely been the domain of formal educational systems and export-oriented tools. Informal educational approaches, such as games or family activities, may overcome barriers to engaging with data by fostering data literacy through casual engagement. This work in progress explores how informal learning through creation and play with interactive data representations (physicalizations) can foster increased literacy and engagement with data. We outline a series of DIY paper charts using AR markers and everyday materials to help children interact and explore data through the creative process of making.},
booktitle = {Proceedings of the 20th Annual ACM Interaction Design and Children Conference},
pages = {433–438},
numpages = {6},
keywords = {DIY, data literacy, data physicalization},
location = {Athens, Greece},
series = {IDC '21}
}

@inproceedings{Lim_CommunicationWithAI,
author = {Lim, Hajin},
title = {Design for Computer-Mediated Multilingual Communication with AI Support},
year = {2018},
isbn = {9781450360180},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3272973.3272982},
doi = {10.1145/3272973.3272982},
abstract = {Machine translation is often not enough for people to engage across languages due to translation errors and lack of cultural background. In addressing these challenges, my dissertation explores how AI-augmented analytics can improve computer-mediated communication between speakers of different native languages. First, to support better sense making of foreign language posts in social media, I designed SenseTrans, a tool that adds contextual information using AI-analytics such as sentiment analysis. In my future work, I intend to explore 1) how people perceive, interpret and make use of AI-generated information, and 2) how AI-augmented analytics could be applied to other settings.},
booktitle = {Companion of the 2018 ACM Conference on Computer Supported Cooperative Work and Social Computing},
pages = {93–96},
numpages = {4},
keywords = {ai-assisted communication, ai-augmented communication, cross-lingual communication, machine translation, multilingual communication, sense making, social media, social network sites},
location = {Jersey City, NJ, USA},
series = {CSCW '18 Companion}
}

@inproceedings{Chen_RecommenderSys,
author = {Chen, Yuyao},
title = {Analysis on the Impact of Recommender Systems on Consumer Decision: Making on China's Online Shopping Platforms},
year = {2022},
isbn = {9781450396523},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3537693.3537734},
doi = {10.1145/3537693.3537734},
abstract = {With the popularization of online shopping platforms across China and in order to maximize profits, online shopping platforms developed various recommender systems (RS), which played an important role in consumers’ decision-making process. Therefore, the impact of RS on consumers’ decision-making process is worthy of studying. In this paper, the RS of online shopping platforms and two examples of them were introduced. This paper firstly analyzed the ethical challenges related to the RS, including algorithmic opacity, privacy concerns, and bias and behavior manipulation and provided solutions accordingly. Besides, the paper through a method of online questionnaire and data analysis, illustrated RS’ ability to influence consumers’ decision making by providing them with personalized recommendations during online shopping. In addition, with the help of an online questionnaire filled out by more than 128 Chinese consumers across different ages and occupations, the author also analyzed people's attitudes towards the RS. According to the research results, this overall finding implies that recommender systems in contemporary electronic commerce are not merely decision aids for lowering search costs, but may also play a substantial part in economic decision making. A randomised trial strategy was used in this work to support the idea that internet recommendations may influence willingness-to-pay judgements, even when the two are tested on distinct scales.},
booktitle = {Proceedings of the 6th International Conference on E-Commerce, E-Business and E-Government},
pages = {29–33},
numpages = {5},
keywords = {Taobao, Recommender systems, Recommendation systems, Pinduoduo, Online shopping, Ethical challenges},
location = {Plymouth, United Kingdom},
series = {ICEEG '22}
}

@inproceedings{Zimmerman_DevProductivity,
author = {Zimmermann, Thomas},
title = {The Incredible Machine: Developer Productivity and the Impact of AI on Productivity (Keynote)},
year = {2024},
isbn = {9798400706585},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3663529.3674721},
doi = {10.1145/3663529.3674721},
abstract = {Developer productivity is about more than an individual’s activity levels or the efficiency of the engineering systems, and it cannot be measured by a single metric or dimension. In this talk, I will discuss a decade of my productivity research. I will show how to use the SPACE framework to measure developer productivity across multiple dimensions to better understand productivity in practice. I will also discuss common myths around developer productivity and propose a collection of sample metrics to navigate around those pitfalls. Measuring developer productivity at Microsoft has allowed us to build new insights about the challenges remote work has introduced for software engineers, and how to overcome many of those challenges moving forward into a new future of work. Finally, I will talk about how I expect that the AI revolution will change developers and their productivity.},
booktitle = {Companion Proceedings of the 32nd ACM International Conference on the Foundations of Software Engineering},
pages = {1},
numpages = {1},
keywords = {artificial intelligence, developer productivity, software engineering},
location = {Porto de Galinhas, Brazil},
series = {FSE 2024}
}

@inproceedings{Yuksel_Video_HumanInTheLoop,
author = {Yuksel, Beste F. and Kim, Soo Jung and Jin, Seung Jung and Lee, Joshua Junhee and Fazli, Pooyan and Mathur, Umang and Bisht, Vaishali and Yoon, Ilmi and Siu, Yue-Ting and Miele, Joshua A.},
title = {Increasing Video Accessibility for Visually Impaired Users with Human-in-the-Loop Machine Learning},
year = {2020},
isbn = {9781450368193},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3334480.3382821},
doi = {10.1145/3334480.3382821},
abstract = {Video accessibility is crucial for blind and visually impaired individuals for education, employment, and entertainment purposes. However, professional video descriptions are costly and time-consuming. Volunteer-created video descriptions could be a promising alternative, however, they can vary in quality and can be intimidating for novice describers. We developed a Human-in-the-Loop Machine Learning (HILML) approach to video description by automating video text generation and scene segmentation while allowing humans to edit the output. Our HILML system was significantly faster and easier to use for first-time video describers compared to a human-only control condition with no machine learning assistance. The quality of the video descriptions and understanding of the topic created by the HILML system compared to the human-only condition were rated as being significantly higher by blind and visually impaired users.},
booktitle = {Extended Abstracts of the 2020 CHI Conference on Human Factors in Computing Systems},
pages = {1–9},
numpages = {9},
keywords = {blind users, human-in-the-loop, machine learning, video accessibility, video description, visually impaired users},
location = {Honolulu, HI, USA},
series = {CHI EA '20}
}

@inproceedings{Bodi_Video,
author = {Bodi, Aditya and Fazli, Pooyan and Ihorn, Shasta and Siu, Yue-Ting and Scott, Andrew T and Narins, Lothar and Kant, Yash and Das, Abhishek and Yoon, Ilmi},
title = {Automated Video Description for Blind and Low Vision Users},
year = {2021},
isbn = {9781450380959},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411763.3451810},
doi = {10.1145/3411763.3451810},
abstract = {Video accessibility is crucial for blind and low vision users for equitable engagements in education, employment, and entertainment. Despite the availability of professional description services and tools for amateur description, most human-generated descriptions are expensive and time consuming, and the rate of human-generated descriptions simply cannot match the speed of video production. To overcome the increasing gaps in video accessibility, we developed a system to automatically generate descriptions for videos and answer blind and low vision users’ queries on the videos. Results from a pilot study with eight blind video aficionados indicate the promise of this system for meeting needs for immediate access to videos and validate our efforts in developing tools in partnership with the individuals we aim to benefit. Though the results must be interpreted with caution due to the small sample size, participants overall reported high levels of satisfaction with the system, and all preferred use of the system over no support at all.},
booktitle = {Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems},
articleno = {230},
numpages = {7},
keywords = {Artificial Intelligence, Blind and Low Vision Users, Video Accessibility, Video Description},
location = {Yokohama, Japan},
series = {CHI EA '21}
}

@inproceedings{potluri_VisualSemantics,
author = {Potluri, Venkatesh and Grindeland, Tadashi E and Froehlich, Jon E. and Mankoff, Jennifer},
title = {Examining Visual Semantic Understanding in Blind and Low-Vision Technology Users},
year = {2021},
isbn = {9781450380966},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411764.3445040},
doi = {10.1145/3411764.3445040},
abstract = {Visual semantics provide spatial information like size, shape, and position, which are necessary to understand and efficiently use interfaces and documents. Yet little is known about whether blind and low-vision (BLV) technology users want to interact with visual affordances, and, if so, for which task scenarios. In this work, through semi-structured and task-based interviews, we explore preferences, interest levels, and use of visual semantics among BLV technology users across two device platforms (smartphones and laptops), and information seeking and interactions common in apps and web browsing. Findings show that participants could benefit from access to visual semantics for collaboration, navigation, and design. To learn this information, our participants used trial and error, sighted assistance, and features in existing screen reading technology like touch exploration. Finally, we found that missing information and inconsistent screen reader representations of user interfaces hinder learning. We discuss potential applications and future work to equip BLV users with necessary information to engage with visual semantics.},
booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},
articleno = {35},
numpages = {14},
keywords = {visual design, blind and low-vision creators, Accessibility},
location = {Yokohama, Japan},
series = {CHI '21}
}