\section{Background and Related Work}
Our work builds on prior research in AI-based technology to promote family interactions, AI tools for BLV people to enable access to visual content, and Human-AI systems that enable corrections and context augmentations to AI output. 

\subsection{AI in Family Interactions}
An emerging body of work explores the role of AI in family interactions, specifically to augment interactions between parents and children. AI-supported co-reading scenarios are a prominent theme \cite{Zhang_StoryBuddy, Dietz_ContextQ, Lin_FishScales, Xu_Bilingual}. \citet{Dietz_ContextQ} present \textit{ContextQ}, a system supporting parents and children through AI-generated questions to promote dialogue and conversation while co-reading. \citet{Zhang_StoryBuddy}'s \textit{StoryBuddy} similarly presents automated questions throughout the reading of a story, but enables children's interaction with the AI agent both in the presence and in the absence of parents. Additionally, \textit{StoryBuddy} tracks developmental progress through children's responses to these AI-generated questions. \citet{Xu_Bilingual} also employ a bilingual conversational agent to promote language literacy while continuing to foster dialogue and parent-child connections while co-reading. These systems and their study findings inform our design guidelines for ArtInsight---particularly to center the connection between BLV family members and their children while using technology for artwork interpretation.

A smaller body of work investigates the use of AI in mixed-ability family settings. \citet{ASL_DeafChild_HearingParent} leverage AI to enable ASL word retrieval, fostering communication between Hearing parents and their Deaf or Hard-of-Hearing (DHH) children. A related work looks at context-responsive ASL recommendations between Hearing parents and their DHH children \cite{Hossain_ContextResponsiveASL}. \citet{Park_AllAboutPictures_2023} propose smart speakers for BLV parents and their sighted children to co-read picture books. \citet{cuddlingup} discuss researching unobtrusive AI systems that understand both images and text to also support co-reading in mixed-ability families. The researchers also coin the term \textit{``Intimate Assistive Technology''} \cite{cuddlingup}, defined as technology that \textit{``enables individual or collaborative access and fosters interpersonal connection building.''} Our work builds on these past works by expanding the \textit{Intimate Assistive Technology} space to include AI-powered systems that foster increased artwork engagement between BLV family members (\textit{i.e.,} broader than parents) and their sighted children.

\subsection{AI-Powered Tools for BLV People}
Researchers and product teams alike are investigating AI tools for BLV people to navigate both physical and digital spaces. Applications such as \textit{Be My Eyes} \cite{be_my_eyes_2024} and \textit{Seeing AI} now use state-of-the-art AI models for BLV users to take and analyze photos of the physical world around them. \textit{Be My Eyes}, an application experience that is best known for connecting BLV people to human volunteers for sighted assistance, calls this new AI mode \textit{Be My AI} \cite{be_my_ai_2024}.  

One category of research explores \textit{how} BLV people use existing AI systems, such as Be My AI or AI-generated alternative text for images, as well as BLV people's attitudes towards the output of these AI systems \cite{gonzalez_CHI, Das_ProvenanceToAberrations, bennett_itscomplicated, Glazko_Autoethnography}. \citet{gonzalez_CHI} investigate responses to the trustworthiness of AI descriptions in current tools. \citet{bennett_itscomplicated} explore the risks of AI bias when AI is used to auto-describe images, specifically photographs. Through an auto-ethnographic study, \citet{Glazko_Autoethnography} highlight how a BLV researcher wants to use AI for writing and validating code. While these research efforts provide a foundation for future AI-based visual artifact understanding systems, much of this prior research focuses on BLV adults' \textit{individual} usage of and interaction with AI.

Most relevant to our work, \citet{chhedakothary2024} performed a formative study exploring mixed visual-ability families' reactions to existing AI tools when used for interpreting children's artwork, finding generally positive reactions to AI descriptions. However, the researchers also highlight that families want to \textit{correct} inaccurate AI interpretations, as well as prevent reductive or overly simplistic AI language when describing their children's artwork. As formative work, \citet{chhedakothary2024} further inform our design guidelines for the ArtInsight system. 

Another category of recent research involves \textit{implementing} novel AI-based systems for BLV people to engage with visual content \cite{Huh_GenAssist, Kim_DescriptionsForComics}. \citet{Huh_GenAssist} enable accessible ways for BLV people to create AI-generated images, and \citet{Kim_DescriptionsForComics} compare AI and human annotations of comic strip descriptions. Both of these works leverage AI for the accessibility of different types of visual artifacts, but the focus of ArtInsight as a system for children's artwork interpretation remains unique. Additional research leverages prototypical systems using state-of-the-art AI models for physical world navigation tasks, such as helping BLV people find their personal belongings \cite{Morrison_FindMyThings} and with street crossings \cite{jain2024streetnavleveragingstreetcameras}. A technique that \citet{Morrison_FindMyThings} employ for training AI models to recognize BLV people's personal items is few-shot learning, which allows AI models to \textit{``make accurate predictions by training on a very small number of labeled examples.''} \footnote{https://www.ibm.com/topics/few-shot-learning} We similarly employ few-shot learning to evaluate the effectiveness of ArtInsight's prompt-engineered AI model across a small dataset of example children's artworks.

\subsection{Augmenting AI Descriptions with Context and Human Edits}
As \citet{chhedakothary2024} and \citet{bennett_itscomplicated} report, there is a desire by BLV individuals and their families to feed context and corrections to AI descriptions of visuals such as artwork or images of people. Based on this, we ground our work in prior research in context-aware AI \cite{Gubbi_ContextAware} as well as enabling human corrections or additions to AI descriptions \cite{be_my_eyes_2024, darth_vader}. We draw on efforts in the realm of BLV accessibility \cite{Lee_RSAs, be_my_eyes_2024, Gubbi_ContextAware, Singh_FigureA11y} as well as in the broader Human-AI research space \cite{darth_vader, hwang_creative}.

Even after its AI integration, \textit{Be My Eyes} continues to support human sighted assistance---Be My AI users can connect with human volunteers for further help. \citet{Gubbi_ContextAware} take a fully automated approach, creating a pipeline to extract meaningful and relevant data from websites to provide users with context-aware image descriptions. \citet{darth_vader} explore children's attitudes towards AI, and report findings that children want to provide added context to AI when it is incorrect or it misinterprets their request. We evaluate these different approaches as considerations for how ArtInsight should augment AI descriptions of artwork with the children's \textit{i.e.,} the artist's context and interpretation of their art.