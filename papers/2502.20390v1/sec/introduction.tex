\section{Introduction}

Animating human-object interactions is a challenging and time-consuming task even for skilled animators. It requires a deep understanding of physics and meticulous attention to detail to create natural and convincing interactions. While Motion Capture (MoCap) data provides references, animators often need to correct contact errors caused by sensor limitations and occlusions between humans and objects. However, this process remains unscalable, as refining a single motion demands a delicate balance between preserving the captured data and ensuring its physical plausibility.

Physics-based human motion imitation~\cite{lee2010data,peng2018deepmimic} offers an alternative approach to improving motion fidelity, by training control policies to mimic reference MoCap data within a physics simulator. However, scaling up human-object interaction imitation presents significant challenges: (\textbf{i}) \textcolor{Sepia}{\textit{MoCap Imperfection}}: Contact artifacts are common, causing expected contacts to fluctuate instead of maintaining consistent zero distance, often due to MoCap limitations or missing hand capture~\cite{bhatnagar22behave,li2023object}. Accurately imitating MoCap kinematics can result in unrealistic dynamics in simulation. Moreover, HOI datasets often include diverse human shapes, requiring motion retargeting to adapt movements across different human models while preserving interaction dynamics. This retargeting process is imperfect and can introduce new contact artifacts or exacerbate existing ones.
(\textbf{ii}) \textcolor{Violet}{\textit{Scaling-up}}: Although large-scale motion imitation has been explored in previous works~\cite{yuan2020residual, luo2023perpetual, tessler2024maskedmimic, won2020scalable}, it remains largely underexplored for whole-body interactions involving dynamic and diverse objects.

In this paper, we aim to utilize rich yet imperfect motion capture interaction datasets to train a control policy capable of learning diverse motor skills while enhancing the plausibility of these actions by correcting errors, such as inaccurate hand motions and faulty contacts. Our approach is grounded on the key insight of tackling the challenges of \textcolor{Sepia}{\textit{skill perfection}} and \textcolor{Violet}{\textit{skill integration}} progressively. We implement a curriculum-based teacher-student distillation framework, where multiple teacher policies focus on imitating and refining small subsets of interactions, and a student policy integrates these skills from the teachers.

Instead of relying on curated data that covers a limited range of actions~\cite{luo2024grasping,braun2023physically}, we employ multiple teacher policies trained on a diverse set of imperfect interaction data and address two key challenges: \textit{retargeting} and \textit{recovering}.
First, we unify all training policies to a canonical human model, by embedding HOI retargeting directly into the imitation. This is achieved by reframing the policy learning to optimize both imitation and retargeting objectives.
Second, our teacher policies refine interaction motion through learning from it, as accurate contact dynamics enforced by a physics simulator inherently correct inaccuracies in the reference kinematics. To support this, we introduce tailored contact-guided reward and optimize trajectory collection, enabling effective skill imitation despite MoCap errors.

Introducing teacher policies offers several key benefits. By leveraging teacher rollouts, we effectively distill raw MoCap data into refined HOI references with a unified embodiment and enhanced physical fidelity. These refined references guide the subsequent student policy training, reducing the negative impact of errors in the original MoCap data.
A major hurdle in scaling motion imitation is the sample inefficiency of Reinforcement Learning (RL), which can lead to prohibitively long training times. Our teacher-student approach mitigates this through a \textit{space-time trade-off}: multiple teacher policies are trained in parallel on smaller, more manageable data subsets, and their expertise is then \textit{distilled} into a single student policy. We begin with demonstration-based distillation to bootstrap PPO~\cite{schulman2017proximal} updates, reducing reliance on pure trial and error and enabling more effective scaling. As training progresses, the student gradually shifts from heavy demonstration guidance to increased RL updates, ultimately surpassing simple demonstration memorization. This mirrors alignment strategies in Large Language Models (LLMs), where demonstration-based pretraining is refined through RL fine-tuning~\cite{ouyang2022training,touvron2023llama}.


To summarize, our contributions are as follows:  
(\textbf{i}) We introduce \textit{InterMimic}, which, to the best of our knowledge, is the \textit{first} framework designed to train physically simulated humans to develop \textit{a wide range of whole-body} motor skills for interacting with \textit{diverse} and \textit{dynamic} objects, extending beyond traditional grasping tasks.  
(\textbf{ii}) We develop a teacher-student training strategy, where teacher policies provide a unified solution to address the challenges of retargeting and refining in HOI imitation. The student distillation introduces a scalable solution by leveraging a space-time trade-off.  
(\textbf{iii}) We demonstrate that our unified framework, \textit{InterMimic}, as illustrated in Figure~\ref{fig:teaser}, effectively handles versatile physics-based interaction animation, recovering motions with realistic and physically plausible details. Notably, by combining kinematic generators with \textit{InterMimic}, we enable a physics-based agent to achieve tasks such as interaction prediction and text-to-interaction generation.
