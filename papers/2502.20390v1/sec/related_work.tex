\section{Related Work}
Significant progress has been made in human interaction animation and control, with advancements in areas such as human-human interactions~\cite{xu2023stochastic,ghosh2023remos,liu2024physreaction,wang2023intercontrol,xu2023actformer,liu2023interactive,huang2024closely, khirodkar2024harmony4d, li2024interdance}, hand-object interactions~\cite{sueda2008musculotendon, wheatland2015state, ye2012synthesis, zhang2021manipnet, ye2023affordance, zheng2023cams, zhou2022toch, cao2024multi, zhang2024manidext, ma2024diff, tian2024gaze, christen2024diffh2o, park2025learningtransferhumanhand, tian2024gaze, akkerman2024interdyn, yang2022learning, wang2024furelise, xu2024synchronize}, single-frame interactions~\cite{xie2022chore,zhang2020perceiving,wang2022reconstructing,petrov2023object,hou2023compositional,kim2023ncho,xusemantic,yang2024person,dai2024interfusion, li2024genzi,yang2023lemon,kim2024zero,yang2024egochoir, yang2024f, zhang2023ins, zhang2024ood, zhao2022compositional,wu2024dice}, human interactions with static scenes~\cite{hassan2021stochastic, wang2021synthesizing, wang2022towards, chao2021learning, pan2023synthesizing, xiao2024unified, tessler2024maskedmimic, lee2023locomotion, wang2024sims, li2024zerohsi, zhang2024scenic, louharmonizing, kim2025beyond, cong2024laserhuman, zhang2023roam, cen2024generating, ma2024contact, Zhao:ICCV:2023}, and real-world humanoid control for object manipulation~\cite{gu2025humanoidlocomotionmanipulationcurrent, sferrazza2024humanoidbench, he2024omnih2o, fu2024humanplus, gao2024bi, cheng2024open, ze2024generalizable, chernyadev2024bigym, wang2024grutopia, jiang2024dexmimicgen, lu2024mobile, zhang2024wococo, he2024learning, chu2024large, ben2024homie,ding2025humanoidvlauniversalhumanoidcontrol,li2024okami,kareer2024egomimicscalingimitationlearning,ji2024exbody2,dao2024sim,liu2024opt2skill}. In this section, we focus on recent studies on \textit{whole-body} interaction animation, particularly involving \textit{dynamic objects}.

\subsection{Kinematic Interaction Animation}
Generating human interactions has been a long-standing topic in animation and computer graphics~\cite{lee2006motion, gleicher1997motion}. Significant advances in character animation~\cite{chen2024motionclr, zhong2025smoodi, dai2024motionlcm, xie2023omnicontrol, li2024unimotion, liu2024programmable, zhao2024dart, barquero2024seamless} have emerged with the advent of deep learning, \eg, including phase-function-based methods~\cite{holden2017phase} that enable object interactions like carrying a box~\cite{starke2019neural} or playing basketball~\cite{starke2020local}. This is extended to more diverse but static objects approaching~\cite{zhang2022couch, wu2022saga, taheri2022goal, kulkarni2023nifty}.
Subsequent efforts~\cite{ghosh2022imos, razali2023action, li2023controllable, wu2024human, li2023task, jiang2024scaling, jiang2024autonomous, lu2024choice} integrate object motion into interactions but remain constrained by assuming that interactions occur primarily through the hands. To address this, recent developments~\cite{corona2020context, 9714029, xu2023interdiff, peng2023hoi, diller2023cg, wu2024thor, daiya2024collage, song2024hoianimator, david, xu2024interdreamer, he2024syncdiff} introduce interactions in a fashion of whole-body loco-manipulation that engages multiple body parts in contact. However, these methods often suffer from physical inaccuracies, such as floating contacts and penetrations, while they generate only body motion without considering hand motion~\cite{lu2023humantomato} or dexterity.
In this work, we address physical inaccuracies by refining imperfect kinematic generation through physics simulation, with InterDiff~\cite{xu2023interdiff} and HOI-Diff~\cite{peng2023hoi} serving as motion planning for loco-manipulation that bridges high-level decision-making (\eg, text instruction) with low-level execution.

\subsection{Physics-based Interaction Animation}
Physics-based methods generate motion through motor control policies within a physics simulator, \eg, achieved via deep reinforcement learning to track reference motions~\cite{peng2018deepmimic}. These policies are directly applicable for executing simple interactions, such as punching or striking an object~\cite{peng2022ase, tessler2023calm, cui2024anyskill, tevet2024closd}. To achieve more complex interactions, early studies focus on specific scenarios, including notable sports-related~\cite{luo2024smplolympics} examples such as basketball~\cite{wang2023physhoi, liu2018learning, wang2024skillmimic}, skating~\cite{liu2017learning}, soccer~\cite{xie2022learning, hong2019physics, liu2022motor, haarnoja2024learning, cui2024anyskill}, tennis~\cite{zhang2023learning}, table tennis~\cite{wang2024strategy}, as well as tasks proposed in~\cite{bae2023pmp}.
Research also demonstrates flexibility in more general but simpler box carrying tasks~\cite{zhang2023simulation}. These advancements are achieved through the integration of multiple control policies~\cite{merel2020catch}, the use of adversarial motion priors~\cite{hassan2023synthesizing, peng2021amp,gao2024coohoi}, and imitating diverse kinematic generations~\cite{xie2023hierarchical,wu2024human}.
However, these methods train their policies in a \textit{non-scalable} manner, with each policy handling only specific object types or actions. In pursuit of a single, scalable policy to enable multiple interaction skills, existing methods either rely on fixed interaction patterns, such as approaching and grasping objects by following predefined trajectories~\cite{braun2023physically}, or remain confined to single-hand grasping actions~\cite{luo2024grasping}. Additionally, they depend on highly curated data from the GRAB dataset~\cite{taheri2020grab}, which, despite its high quality, primarily features low-dynamic full-body motion and only small-sized objects. More recently proposed datasets~\cite{bhatnagar22behave, jiang2022chairs, huang2022intercap, zhang2023neuraldome, fan2023arctic, li2023object, zhao2023im, kim2024parahome, wu2024himo, zhang2024core4d, xie2024intertrack, zhang2024hoi, zhang2024force, liu2024mimicking, liu2024taco} offer richer full-body interactions with objects in diverse shapes but contain significant artifacts that challenge existing motion imitation approaches. We process data from OMOMO~\cite{li2023object}, BEHAVE~\cite{bhatnagar22behave}, HODome~\cite{zhang2023neuraldome}, IMHD~\cite{zhao2023im}, and HIMO~\cite{wu2024himo} into the simulator, demonstrating InterMimicâ€™s \textit{scalability} for diverse interactions and \textit{robustness} to MoCap artifacts.