\clearpage
\setcounter{page}{1}
\maketitlesupplementary


\setcounter{table}{0}
\renewcommand{\thetable}{\Alph{table}}
\renewcommand*{\theHtable}{\thetable}
\setcounter{figure}{0}
\renewcommand{\thefigure}{\Alph{figure}}
\renewcommand*{\theHfigure}{\thefigure}
\setcounter{section}{0}
\renewcommand{\thesection}{\Alph{section}}
\renewcommand*{\theHsection}{\thesection}

\begin{strip}\centering
\vspace{-2em}
\includegraphics[width=\textwidth]{figures/rgb_env0_frame00470.png}
\captionof{figure}{InterMimic enables simulated humans to perform physical interactions, featuring scalable skill learning covering diverse objects.
\label{fig:teaser_supp}}
\vspace{-1em}
\end{strip}

\noindent In this supplementary, we provide additional method details and experimental setups:
\begin{enumerate}[label=(\textbf{\roman*})]
    \item \textbf{Demo Video.} A demonstration video (with a screenshot in Figure~\ref{fig:teaser_supp}) is provided at \href{https://sirui-xu.github.io/InterMimic/assets/demo.mp4}{demo.mp4}, as described in Sec.~\ref{sec:demo}.
    \item \textbf{Simulation Setup.} The environment configuration for physical HOI simulations is introduced in Sec.~\ref{sec:phys_para}.
    \item \textbf{Reference Contact Labels.} Additional information on obtaining the reference contact label \(\hat{\boldsymbol{c}}_t\) is detailed in Sec.~\ref{sec:repre_supp}.
    \item \textbf{Reward Formulation.} A comprehensive explanation of the reward design is provided in Sec.~\ref{sec:reward}.
    \item \textbf{Physical State Initialization \& Interaction Early Termination.} Further insights into these mechanisms are discussed in Sec.~\ref{sec:psi_supp}.
    \item \textbf{Implementation Details.} This section (Sec.~\ref{sec:training}) covers reframing our method for interaction prediction and text-guided interaction generation, as well as translating MoCap interactions into humanoid robot skills.
    \item \textbf{Additional Experiments.} Sec.~\ref{sec:add_exp} presents further qualitative results and analyzes failure cases.
    \item \textbf{Limitations and Societal Impact.} Finally, we examine the limitations of InterMimic and its potential societal implications in Sec.~\ref{sec:discuss}.
\end{enumerate}

\noindent We will release the code for this project at \href{https://sirui-xu.github.io/InterMimic}{our webpage}.

\etocdepthtag.toc{mtappendix}
\etocsettagdepth{mtchapter}{none}
\etocsettagdepth{mtappendix}{subsection}
{
  \hypersetup{
    linkcolor = black
  }
  \tableofcontents
}


\section{Demo Video} \label{sec:demo}

In addition to the qualitative results presented in the main paper, we provide a demo video \href{https://sirui-xu.github.io/InterMimic/assets/demo.mp4}{(demo.mp4)} for more detailed visualizations of the tasks, further illustrating the efficacy of our approach. The demo video conveys the following key points:

\begin{enumerate}[label=(\textbf{\roman*})]
\item Our teacher policy can imitate highly dynamic and long-term interactions, both of which are inherently challenging.

\item We visualize the effectiveness of our teacher policy in \textit{HOI retargeting}. Given MoCap references for humans, we successfully transfer these tasks to a humanoid robot, tolerating embodiment differences.

\item Our method corrects errors in reference interactions, addressing contact penetration, floating, and jittering issues. This demonstrates how teacher-based reference distillation can provide cleaner data for student policy training.

\item The baseline method PhysHOI~\cite{wang2023physhoi} \textit{fails} on sequences our approach successfully imitates, complementing Figure~\ref{fig:compare} in the main paper.

\item Our student policy exhibits strong scalability, effectively learning from hours of data across diverse objects and interaction skills.

\item The framework grants the student policy \textit{zero-shot} generalizability, enabling direct application to text-to-HOI, interaction prediction, and interactions with new skills or objects -- even multiple objects not present in the training set.
\end{enumerate}

\begin{figure}
    \centering
    \includegraphics[width=\columnwidth]{figures/object.pdf}
    \caption{Visualization of the objects from OMOMO~\cite{li2023object}, each decomposed into 64 convex hulls for simulation.
    }
    \label{fig:object}
\end{figure}

\section{Setup of Physical Interaction Simulation} \label{sec:phys_para}
The reference data represent humans using the SMPL models~\cite{MANO, SMPL-X:2019}. For simulation, we convert these models into box and cylindrical rigid bodies following~\cite{yuan2021simpoe, luo2023perpetual}. Objects are also converted into simulation models through convex decomposition, as illustrated in Figure~\ref{fig:object}. We summarize the physics parameters for our task in Table~\ref{tab:physics_hyper}. We follow the physics parameters for the human as specified in~\cite{wang2023physhoi, wang2024skillmimic}, with the exception of the specialized range of motion (RoM) for hands, detailed in Table~\ref{tab:rom}. Our range of motion (RoM) setting is biologically inspired: finger flexion and extension (bending and straightening) are fully activated. However, unlike the real human, the abduction and adduction of the Metacarpophalangeal (MCP) joint are constrained to minimize the risk of finger interpenetration, in the absence of the correct reference hand pose for guidance. The rationale for these RoM settings is discussed in Sec.\ref{sec:teacher} of the main paper and Sec.\ref{sec:hand} of the supplementary.
\begin{table}
  \begin{tabular}{l|l}
    \toprule
    Hyperparameter & Value \\
    \midrule
    Sim $dt$ & $1/60$s\\
    Control $dt$ & $1/30$s\\
    Number of envs & 8192 \\
    \midrule
    Number of substeps & 2 \\
    Number of pos iterations & 4 \\
    Number of vel iterations & 0 \\
    Contact offset & 0.02 \\
    Rest offset & 0.0 \\
    Max depenetration velocity & 100 \\
    \midrule
    Object \& ground restitution & 0.7 \\
    Object \& ground friction & 0.9 \\ 
    Object density & 200 \\
    Object max convex hulls & 64 \\
  \bottomrule
\end{tabular}\caption{Simulation hyperparameters used in Isaac Gym~\cite{makoviychuk2021isaac}.}
\label{tab:physics_hyper}
\end{table}

\begin{table}
  \begin{tabular}{l|l|l}
    \toprule
    Joint & x-dim & y \& z-dim \\
    \midrule
    MCP & $[-55.625^\circ, 55.625^\circ]$ & \multirow{3}{*}{$[-5.625^\circ, 5.625^\circ]$} \\
    PIP & $[-55.625^\circ, 55.625^\circ]$ &  \\
    DIP & $[-5.625^\circ, 90.000^\circ]$ &  \\
    \midrule
    CMC &$[-55.625^\circ, 55.625^\circ]$&$[-55.625^\circ, 55.625^\circ]$\\
    MCP &$[-5.625^\circ, 5.625^\circ]$&$[-5.625^\circ, 5.625^\circ]$\\
    IP &$[-5.625^\circ, 90.000^\circ]$&$[-5.625^\circ, 5.625^\circ]$\\
  \bottomrule
\end{tabular}
\caption{We constrain the Range of Motion (RoM) for the joints in the index, middle, ring, and pinky fingers including: the MCP (Metacarpophalangeal) joint where the finger meets the hand, the PIP (Proximal Interphalangeal) joint as the middle joint, and the DIP (Distal Interphalangeal) joint closest to the fingertip. For the thumb, we consider the CMC (Carpometacarpal) joint at the base in the palm, the MCP connecting the thumb to the hand, and the IP (Interphalangeal) joint within the thumb. The coordinates for describing these RoMs are based on the human model from~\cite{luo2023perpetual}.}
\label{tab:rom}
\end{table}
\section{Reference Contact} \label{sec:repre_supp}
In this section, we detail how we extract the reference contact that formulates the state and the reward as discussed in Sec.~\ref{sec:state} of the main paper. 
One solution involves loading the HOI data into the simulation, replaying the data, and using the force detector in Isaac Gym~\cite{makoviychuk2021isaac} to identify contact, as suggested by~\cite{wang2023physhoi}. However, this approach is ineffective for imperfect MoCap data; for instance, the force detector fails to capture contact when floating artifacts occur. To address this limitation, we propose solutions tailored differently for teacher and student training:

\noindent\textbf{Reference contact for the student.} We query the force detector from distilled reference in the simulation rather than from MoCap data replay, as the teacher policy is capable of correcting artifacts.

\noindent\textbf{Reference contact for teachers.} To account for contact distance variances, we determine reference contact based on inferred dynamics from kinematics, as outlined below.

\subsection{Inferring Reference Dynamics} \label{sec:contact_lab}
By analyzing the objectâ€™s acceleration over time, we can approximate external forces without depending on simulated dynamics. We assume human-object interaction occurs if any of these conditions hold:
(\textbf{i}) The object is airborne, but its acceleration deviates significantly from gravitational acceleration, indicating that an external force, \eg, human interaction is acting upon it.
(\textbf{ii}) The object is on the ground but not static, and its acceleration significantly differs from what is expected due to friction alone, suggesting additional force input.
(\textbf{iii}) The minimum distance between the human and object vertices is below 0.01 meters.

When any condition is met, we define the contact threshold $\sigma$ as the minimum distance between the human and object vertices, plus 0.005 meters. This adaptive threshold is essential for accommodating contact distance variations in the ground truth MoCap data. For example, the contact promotion marker is defined as $\hat{\boldsymbol{c}}_b[i] = \|\hat{\boldsymbol{d}}[i]\| < \sigma$, where $i$ is the index of human rigid bodies. We integrate $\hat{\boldsymbol{c}}_b$ into the contact promotion reward $R^c_b$, as introduced in Sec.~\ref{sec:teacher} of the main paper and detailed in Sec.~\ref{sec:contact_reward} of supplementary. $\hat{\boldsymbol{d}}$ is the joint-to-object vectors as defined in Sec.~\ref{sec:state}.

\section{Additional Details on Reward} \label{sec:reward}
In this section, we provide further details about the reward function used for policy training. Specifically, we describe how we balance the components of the embodiment-aware reward, formulate the contact and energy rewards, address hand interaction recovery, and explain the process of integrating all rewards into a unified scalar.

\subsection{Embodiment-Aware Reward}
We formulate the weight \(\boldsymbol{w}_d\), introduced in Sec.~\ref{sec:teacher} of the main paper, for balancing the embodiment-aware reward:
\begin{equation}
    \boldsymbol w_d[i] = 0.5 \times \frac{1 / \|\boldsymbol{d}[i]\|^2}{\sum_{i} 1 / \|\boldsymbol{d}[i]\|^2} + 0.5 \times \frac{1 / \|\hat{\boldsymbol{d}}[i]\|^2}{\sum_{i} 1 / \|\hat{\boldsymbol{d}}[i]\|^2},
    \label{eq:edge_weighting_function}
\end{equation}
where \(i\) is the joint index, and \(\boldsymbol{d}\) and \(\hat{\boldsymbol{d}}\) are vectors from the human joint to the object surface for simulation and reference, respectively, as defined in Sec.~\ref{sec:state} of the main paper. The value $\|\boldsymbol{d}[i]\|^2$ and $\|\hat{\boldsymbol{d}}[i]\|^2$ are clipped by a small positive value to prevent division by zero.

Our joint position and rotation tracking rewards, \(R^h_p\) and \(R^h_{\theta}\), include both body and hand joints, even for imitating datasets such as~\cite{bhatnagar22behave,li2023object} which present hands always in flat or mean poses. This encourages hands to maintain a reasonable default pose when the contact reward is not activated.

\subsection{Contact Reward} \label{sec:contact_reward}
The contact promotion cost function $E^{c}_b$ is designed to encourage highly probable contact, as highlighted by the red regions in Figure~\ref{fig:contact_label}(i) of the main paper. This reward utilizes the adaptive contact marker \(\hat{\boldsymbol{c}}_b\), described in Sec.~\ref{sec:contact_lab},
\begin{align}
E^{c}_b = \sum\|\hat{\boldsymbol{c}}_b - \boldsymbol{c}\| \odot \hat{\boldsymbol{c}}_b,
\end{align}
where $\boldsymbol{c}$ is the simulated contact extracted from the force detected, as introduced in Sec.~\ref{sec:state} of the main paper.

Contact penalties, applied to the blue regions in Figure~\ref{fig:contact_label}(i) of the main paper, are defined using a larger and fixed threshold of \(\sigma_p = 0.1\). Specifically, \(\hat{\boldsymbol{c}}_p[i] = (\|\hat{\boldsymbol{d}}[i]\| > \sigma_p) \land \neg \hat{\boldsymbol{c}}_g[i]\), where $\|\hat{\boldsymbol{d}}[i]\|$ is the distance between joint $i$ and the object surface in the reference interaction as defined in Sec.~\ref{sec:state} of the main paper, and the negation $\neg$ of \(\hat{\boldsymbol{c}}_g[i]\) indicates the rigid body part $i$ that is not in contact with the ground. The cost of penalty is then calculated as:
\begin{align}
E^c_p = \sum\|\boldsymbol{c}\| \odot \hat{\boldsymbol{c}}_p.
\end{align}

\subsection{Hand Interaction Recovery}\label{sec:hand}
Our hand contact guidance is defined as:
\begin{align}
E^c_h &= \sum\|\boldsymbol{c}^{\mathrm{lhand}} - \hat{\boldsymbol{c}}^{\mathrm{lhand}}\| \odot \hat{\boldsymbol{c}}^{\mathrm{lhand}} \\ &+ \|\boldsymbol{c}^{\mathrm{rhand}} - \hat{\boldsymbol{c}}^{\mathrm{rhand}}\| \odot \hat{\boldsymbol{c}}^{\mathrm{rhand}},
\end{align}
where \(\boldsymbol{c}^{\mathrm{lhand}}\) and \(\boldsymbol{c}^{\mathrm{rhand}}\) represent contact labels for rigid body components of the hands. The reference contact markers, \(\hat{\boldsymbol{c}}^{\mathrm{lhand}}\) and \(\hat{\boldsymbol{c}}^{\mathrm{rhand}}\), are defined when any hand vertices are within an adaptive threshold distance $\sigma$ to the objects, as described in Sec.~\ref{sec:contact_lab} of supplementary. 
To avoid overly aggressive hand contact that could lead to unrealistic poses, we impose range of motion constraints for the hand, as shown in Table~\ref{tab:rom}, ensuring that RL-explored grasping remains biologically realistic.

\subsection{Energy Reward} \label{sec:energy}
We define the energy cost as $E^{e}_h = \sum\|\boldsymbol{a}_h\|$, $E^e_{o} = \sum\|\boldsymbol{a}_o\|$, and $E^e_{c} = \max\|\boldsymbol{f}\|$,
where \( \boldsymbol{a}_h \) represents the acceleration of human joints, \( \boldsymbol{a}_o \) the object's acceleration, and \( \boldsymbol{f} \) the force detected on human rigid bodies. Applying them penalizes abrupt contact and promotes smooth interactions.

\subsection{Reward Aggregation}
We define the weights for each cost function, including \(E^h_p\), \(E^h_{\theta}\), \(E_d\), \(E^o_p\), and \(E^o_{\theta}\), as described in Sec.~\ref{sec:teacher} of the main paper, along with \(E^c_b\), \(E^c_p\), \(E^c_h\), \(E^e_h\), \(E^e_o\), and \(E^e_c\) detailed in supplementary as \((\lambda^h_p, \lambda^h_{\theta}, \lambda_d, \lambda^o_p, \lambda^o_\theta, \lambda_{c_b}, \lambda_{c_p}, \lambda_{c_h}, \lambda^h_e, \lambda^o_e, \lambda^f_e)\). The final aggregated reward is computed as:
\(
R = \exp( -\lambda^h_{\theta} E^h_{\theta} - \lambda^h_p E^h_p - \lambda^o_{\theta} E^o_{\theta} - \lambda^o_p E^o_p - \lambda_d E_d - \lambda_{c_b} E^c_b - \lambda_{c_p} E^c_p - \lambda_{c_h} E^c_h - \lambda^h_e E^e_h - \lambda^o_e E^e_o - \lambda^f_e E^e_c ).
\), following a multiplication of the exponential structure, as suggested in~\cite{won2020scalable,park2019learning}.

\section{Additional Details on Trajectory Collection} 
\label{sec:psi_supp}

\subsection{Interaction Early Termination}
In Sec.~\ref{sec:teacher} of the main paper, we introduce the termination conditions defined for human-object interaction. Additionally, we use three conditions general for single human imitation as follows:
(\textbf{i}) The joints are, on average, more than 0.5 meters from their reference.
(\textbf{ii}) The root joint is under the height of 0.15.
(\textbf{iii}) The episode ends after 300 frames, as the maximum episode length (also specified in Table~\ref{tab:ppo_hype}).

\subsection{Physical State Initialization}
\begin{figure}
    \centering
    \includegraphics[width=\columnwidth]{figures/PSI.png}
    \caption{A sanity check on why Reference State Initialization (RSI)~\cite{peng2018deepmimic} can fail: we use a bar representing the reference interaction sequence that the policy imitates, where red regions indicate that initializing in those regions leads to immediate failure, while green regions signify that successful initialization is possible. There may be periods, shown as two gray blocks, where the policy cannot collect trajectories for updates (\ie, unreachable regions), as the successful rollout cannot cover large failed RSI region given the fixed length of the rollout. In real scenarios, rollouts can be suboptimal and terminated prematurely, preventing the policy from collecting sufficient trajectories for challenging periods that extend beyond the boundaries illustrated by the gray blocks.
    }
    \label{fig:psi_sanity}
\end{figure}

\noindent\textbf{Limitations of RSI.} Figure~\ref{fig:psi_sanity} illustrates why Reference State Initialization (RSI)~\cite{peng2018deepmimic} is suboptimal for interaction imitation with imperfect MoCap data. In single-person MoCap scenarios, where failures are less frequent, RSI performs well; however, in the presence of MoCap errors, RSI leads to reduced experience collection, ultimately undermining performance.

\noindent\textbf{Does Interaction Early Termination Help?} While early termination can filter out poor initial states, excessive initialization failures lead to frequent simulation resets that significantly slow down training. Consequently, the agent spends more time restarting simulations rather than engaging in productive learning.

\noindent\textbf{Step-by-step details} to complement Sec.~\ref{sec:teacher} of the main paper:
(\textbf{i})
PSI begins by creating an initialization buffer that stores a collection of reference states from motion capture data and simulation states from previous rollouts. This buffer is used to select initialization states for future rollouts.
(\textbf{ii})
For each new rollout, an initialization state is randomly selected from the buffer.
(\textbf{iii})
Using the current policy, the agent performs rollouts in the simulation environment by taking actions, transitioning through states, and receiving rewards.
(\textbf{iv})
After each rollout, the collected trajectories are evaluated based on their expected discounted rewards to update the critic network. Trajectories with expected rewards above a defined threshold are added to the PSI buffer, while older or lower-quality trajectories are removed to maintain the buffer's size and quality.
We apply PSI in a sparse manner to enhance training efficiency, with a probability of 0.005 for updating the buffer for each rollout.

\section{Additional Implementation Details}\label{sec:training}
In Figures~\ref{fig:text2hoi} and \ref{fig:interdiff}, we illustrate the framework that integrates the kinematic generators with our InterMimic -- let the policy use the kinematic output as the input reference to imitate. Table~\ref{tab:ppo_hype} lists the hyperparameters used during the PPO~\cite{schulman2017proximal}. The weights for the reward function \((\lambda^h_p, \lambda^h_{\theta}, \lambda_d, \lambda^o_p, \lambda^o_\theta, \lambda_{c_b}, \lambda_{c_p}, \lambda_{c_h}, \lambda^h_e, \lambda^o_e, \lambda^f_e)\) are set as $(30, 2.5, 5, 0.1, 5, 5, 5, 3, 2\times10^{-5}, 2\times10^{-5}, 10^{-9})$. 

For evaluation on the OMOMO~\cite{li2023object} dataset, we use Subject 9 as the base model, with teacher policies retargeting interactions from other subjects into this base.

Similar to existing motion imitation approaches~\cite{peng2018deepmimic}, we use API in Isaac Gym~\cite{makoviychuk2021isaac} to initialize the first frame to match the first reference frame -- whether it comes from MoCap or kinematic generation methods. The subsequent sequence is then simulated based on the starting frame.

For learning interaction skills on a humanoid robot~\cite{unitreeg1,inspire} from SMPL-X~\cite{SMPL-X:2019} data, we bypass external retargeting and directly learn, highlighting our frameworkâ€™s integrated ability for both retargeting and imitation. Note that we model each Inspire hand with 12 actuators using PD control, without accounting for the mimic joint present in the actual setup, which could be inapplicable in real deployment. Due to the embodiment gap, the humanoid cannot be initialized to match the first SMPL-X frame. Thus, we adopt a two-stage approach: during the first 15 frames, the policy learns to stand and approach the referenceâ€™s initial pose, establishing a basis for subsequent tracking. Afterward, the policy transitions to track the reference. We rewrite the position and rotation rewards for the robotâ€™s joints mapped to SMPL-X joints. We do not use the contact reward as we disable the self-collision, since the human reference now cannot ensure proper collision constraints for the humanoid robot. To mitigate the impact of contact artifacts in MoCap data without relying on a contact reward, we leverage teacher distillation references for training.

For interactions involving multiple objects, our framework remains unchanged except for the state and reward components related to the objects, such as \(\{{\boldsymbol \theta}_t^o,  \boldsymbol p_t^o, {\boldsymbol \omega}_t^o,  \boldsymbol v_t^o\}\), \(\boldsymbol{d}_t\), and the rewards \(R^o_p\), \(R^o_{\theta}\), and \(R_d\), which now include multiple components to represent multiple objects.


\begin{figure}
    \centering
    \includegraphics[width=\columnwidth]{figures/text2hoi.pdf}
    \caption{Overview of integrating HOI-Diff~\cite{peng2023hoi} with InterMimic to perform text-guided interaction generation, \ie, generating interaction sequences based on text input.
    }
    \label{fig:text2hoi}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=\columnwidth]{figures/interdiff.pdf}
    \caption{Overview of integrating InterDiff~\cite{xu2023interdiff} with InterMimic to perform interaction prediction, \ie, generating future interactions based on past interaction frames.
    }
    \label{fig:interdiff}
\end{figure}

\begin{table}[h]
  \begin{tabular}{l|l}
    \toprule
    Hyperparameters & value \\
    \midrule
    Action distribution & 153D Continuous\\
    Discount factor $\gamma$    & 0.99\\
    Generalized advantage estimation $\lambda$    & 0.95\\
    Entropy regularization coefficient & 0.0\\
    Optimizer & Adam~\cite{Adam} \\
    Learning rate (Actor) & 2e-5 \\
    Learning rate (Critic) & 1e-4 \\
    Minibatch size & 16384 \\
    Horizon length $H$ & 32 \\
    Action bounds loss coefficient & 10 \\
    Maximum episode length & 300 \\
  \bottomrule
\end{tabular}
\caption{Hyperparamters for training teacher and student policies.}
\label{tab:ppo_hype}
\end{table}



\section{Additional Experiemental Results}\label{sec:add_exp}
In this section, we introduce experimental results that are not included in the main paper due to space limit. 

\noindent\textbf{Failure Cases.} In Figure~\ref{fig:error}, we illustrate an example where our teacher policies fail to perform successful imitation. Despite the strong adaptability of our policies, as demonstrated in Figures~\ref{fig:teaser} and \ref{fig:obj_rot}, where they effectively correct reference errors, there are limitations when encountering too many errors. Since the reward design inherently prioritizes reference tracking, excessive errors in the reference inevitably result in failures.


\noindent\textbf{HOI Retargeting.} Figure~\ref{fig:retargeting} shows that our teacher policies, trained on reference data for a specific body shape, can successfully drive a human model with a body shape different from the reference in the simulator to accomplish the same task, albeit with slightly varied trajectories. This result highlights the effectiveness of our design, which integrates retargeting into interaction imitation.

\begin{figure}
    \centering
    \includegraphics[width=\columnwidth]{figures/error.pdf}
    \caption{For certain reference from OMOMO~\cite{li2023object}, the hand is incorrectly flipped, which leads to the failure of the teacher policy. We exclude such data when training the student policy.
    }
    \label{fig:error}
\end{figure}


\begin{figure}
    \centering
    \includegraphics[width=\columnwidth]{figures/retargeting.pdf}
    \caption{Comparison between the reference interaction (human shown in green) and the simulated interaction (human shown in yellow) demonstrates that, despite the different body shapes, the simulated human driven by InterMimic successfully accomplishes the same task with different trajectories, highlighting the effectiveness of our imitation as retargeting.
    }
    \label{fig:retargeting}
\end{figure}

\section{Discussion} \label{sec:discuss}
\noindent\textbf{Limitations and Future Work.} One limitation, as discussed in Sec.~\ref{sec:add_exp} and illustrated in Figure~\ref{fig:error}, is that our method struggles to fully correct MoCap data with significant errors. However, it also underscores a strength of our teacher-student framework: teacher policies filter out data that are too corrupted to imitate, allowing the student policy to concentrate on learning from viable samples and avoid wasting training effort on unrecoverable data.

The policy sometimes results in unnatural object support, where the human produces penetration rather than relying on friction. While we mitigate this issue by setting a high maximum depenetration velocity in simulation (See Table~\ref{tab:physics_hyper}) and applying a contact-based energy (See Sec.~\ref{sec:energy}) to discourage large forces that could cause penetration, it does not entirely solve the problem. A potential solution could involve using a signed distance-based penetration score as a criterion for early termination.

The hand interaction recovery method is effective for the tasks explored in this paper. For tasks requiring dexterity with detailed finger motions, its benefits may be limited.

Additionally, while our method demonstrates good scalability by effectively training on hours of MoCap data involving different objects and generalizing to unseen skills and object geometries, its performance could be further improved with a larger dataset. Incorporating more diverse objects~\cite{xie2024intertrack} would likely further enhance InterMimic's zero-shot generalization capabilities.

\noindent\textbf{Potential Negative Societal Impact.}
Our approach has the potential to generate vivid human-object interaction sequences, which, if misused, could lead to negative societal impacts, with the risk of creating misleading content by depicting individuals in fabricated scenarios. However, our model is designed with privacy in mind -- it employs an abstract representation, using simple geometric shapes like boxes and cylinders to depict different parts. This abstraction reduces the inclusion of personally identifiable features, making it less likely for our synthesized data to be misused in ways that compromise individual identities.