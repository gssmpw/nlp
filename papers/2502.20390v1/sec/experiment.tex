\section{Experiments}
We evaluate teacher policies on their ability to imitate imperfect HOI references, and assess the entire teacher-student framework for scalability to large-scale data and zero-shot generalization across various scenarios. Additional experiments are provided in Sec.~\ref{sec:add_exp} of supplementary.

\noindent\textbf{Datasets.} 
We use the following datasets: OMOMO~\cite{li2023object}, BEHAVE~\cite{bhatnagar22behave}, HODome~\cite{zhang2023neuraldome}, IMHD~\cite{zhao2023im}, and HIMO~\cite{wu2024himo}. OMOMO, containing 15 objects and approximately 10 hours of data, is our primary dataset for evaluating the full teacher-student distillation framework for its scale. We train 17 teacher policies, one per subject, with subject 14 reserved as the test set and the remaining data used for training the student policy. A small portion of data is discarded after teacher imitation due to severe MoCap errors that could not be corrected (see Sec.~\ref{sec:training} and Sec.~\ref{sec:discuss} of the supplementary).
Additional datasets are used to evaluate teacher policies in various MoCap scenarios with different error levels and interaction types. We focus on highly dynamic motions (Figure~\ref{fig:teaser}) and interactions involving multiple body parts (Figure~\ref{fig:compare}), while excluding scenarios such as carrying a bag with a strap, since the simulator~\cite{makoviychuk2021isaac} used lacks full support for soft bodies.

\noindent\textbf{Metrics.}
We use the following metrics: (\textbf{i}) \textit{Success Rate} is defined as the proportion of references that the policy successfully imitates, averaged over all references, while (\textbf{ii}) \textit{Duration} is the time (in seconds) that the imitation is maintained without triggering the interaction early termination conditions introduced in Sec.~\ref{sec:teacher}. (\textbf{iii}) \textit{Human Tracking Error} ($E_h$), which measures the per-joint position error (cm) between the simulated and reference human (excluding hand joints for BEHAVE~\cite{bhatnagar22behave} and OMOMO~\cite{li2023object} due to inaccuracy), and (\textbf{iv}) the \textit{Object Tracking Error} ($E_o$), which measures the per-point position error (cm) between the simulated and reference object. Both errors are averaged over the duration of the imitation.

\noindent\textbf{Baselines.} 
To facilitate fair comparisons, we downgrade our method for teacher policy evaluation to imitate either a single MoCap clip (Figure~\ref{fig:compare}) or multiple clips with a single object (Table~\ref{table:comparison}), enabling direct comparison with PhysHOI~\cite{wang2023physhoi} and SkillMimic~\cite{wang2024skillmimic} (Sec.~\ref{sec:quan} and \ref{sec:qual}). Due to the lack of established baselines for large-scale HOI imitation, we adapt the following variants for comparison with our student policy (Sec.~\ref{sec: ablation}): (\textbf{i}) \textbf{PPO}~\cite{schulman2017proximal} trains an imitation policy from scratch, following~\cite{peng2018deepmimic}. We experiment with both versions, with and without \textit{reference distillation}; (\textbf{ii}) \textbf{DAgger}~\cite{ross2011reduction} distills the student without RL fine-tuning, a process we refer to as \textit{policy distillation}.

\begin{figure}
    \centering
    \includegraphics[width=\columnwidth]{figures/compare.pdf}
    \caption{Qualitative comparison between PhysHOI~\cite{wang2023physhoi} (top), the reference motion (middle) from the BEHAVE~\cite{bhatnagar22behave} dataset, and the interaction refined by our teacher trained on it (bottom). InterMimic faithfully imitates the interactions involving multiple body parts while correcting errors in the original reference.}
    \label{fig:compare}
\end{figure}
\noindent\textbf{Implementation Details.} The control policies operate at 30 Hz and are trained using the \textit{Isaac Gym} simulator~\cite{makoviychuk2021isaac}. Teacher policies are implemented as MLPs with hidden layers of sizes 1024, 1024, and 512. The student policy is implemented as a three-layer Transformer encoder with 4 heads, a hidden size of 256, and a feed-forward layer of 512. The critics are also modeled as MLPs with the same architecture as the teacher policies.
To integrate the student policy with kinematic generators, including text-to-HOI~\cite{peng2023hoi} and future interaction prediction~\cite{xu2023interdiff}, we train these models using reference data distilled by the teacher policies from the OMOMO~\cite{li2023object} dataset, following the same train-test split as the student policy training. For the text-to-HOI model, we train it to generate 10 seconds of motion and use 24 generated samples for evaluation, while for future interaction prediction, the model generates 25 future frames given 10 past frames and we use 60 generated samples for evaluation. See Sec.~\ref{sec:training} of the supplementary.


\begin{figure}
    \centering
    \includegraphics[width=\columnwidth]{figures/obj_rot.pdf}
    \caption{We recover plausible object rotations (bottom) that are challenging for motion capture due to the equivariant geometries of objects, which result in the object sliding on the ground (top).
    }
    \label{fig:obj_rot}
\end{figure}
\input{table/comparison}

\subsection{Quantitative Evaluation}\label{sec:quan}
\input{table/student}
Table~\ref{table:comparison} shows that the baseline struggles with MoCap imperfections, \eg, incorrect hand positioning, and thus results in clearly shorter tracking durations. In contrast, our method maintains reference tracking for longer durations and produces interactions that closely match the reference. Table~\ref{table:student} shows that our method consistently outperforms baselines in both training data imitation and out-of-distribution generalization, including interactions from the test set and from kinematic generations. We discuss the effectiveness of specific design choices in Sec.~\ref{sec: ablation}.


\subsection{Qualitative Evaluation}\label{sec:qual}
Figure~\ref{fig:compare} shows a representative sequence from the experiment in Table~\ref{table:comparison}, illustrating how our teacher policy corrects interactions that PhysHOI~\cite{wang2023physhoi} fails to track robustly -- our method effectively withstands and corrects incorrect hand positioning and floating contacts in the reference. Beyond obvious errors, our method also rectifies the rotation of symmetric objects that MoCap inaccurately depicts as sliding along the ground, shown in Figure~\ref{fig:obj_rot}. Figure~\ref{fig:zero} presents additional examples that complement Figure~\ref{fig:teaser}, demonstrating how our approach integrates with kinematic generators for future interaction prediction and text-to-interaction synthesis. This zero-shot generalization extends to novel objects unseen during training (Figure~\ref{fig:zero2}), highlighting the effectiveness of our object geometry and contact-encoded representation, as well as the large-scale training.


\begin{figure}
    \centering
    \includegraphics[width=\columnwidth]{figures/text-predict.png}
    \caption{Zero-shot integration with a text-to-HOI model HOI-Diff~\cite{peng2023hoi} (\textbf{Top}), using `Kick the large box' as the prompt, and an interaction prediction model InterDiff~\cite{xu2023interdiff} (\textbf{Bottom}), where gray meshes are past states and colored illustrate future generations.
    }
    \label{fig:zero}
\end{figure}

\subsection{Ablation Study} \label{sec: ablation}

\noindent\textbf{Effectiveness of PSI and IET.}
We conduct an ablation study, as demonstrated in Table~\ref{table:comparison}, comparing the full approach to ``Ours w/o PSI''. The results validate that Physical State Initialization (PSI) is effective by mitigating inaccuracies in the motion capture data. We also observe reduced effectiveness without our interaction early termination, as training often spends rollouts on irrelevant periods.

\noindent\textbf{Effectiveness of Reference Distillation.} 
Compared to directly scaling imitation from MoCap with potential imperfections (line 1 in Table~\ref{table:student}), using references refined by the teacher policy (line 3) achieves consistently better performance on all metrics. The improvement is even more pronounced on the test set, where, without reference distillation, the policy struggles with unseen shapes, while retargeting by reference distillation eliminates the difficulty.


\noindent\textbf{Effectiveness of Joint PPO and DAgger Updates.} As shown in Table~\ref{table:student}, training a policy from scratch (line 3) or relying solely on policy distillation (DAgger, line 2) fails to achieve optimal performance. While supervised skill learning lays the groundwork, additional PPO fine-tuning is crucial for resolving conflicts among teacher policies. This is important because our subject-based clustering may not effectively distinguish between different interaction patterns, and ambiguity arises when multiple teachers produce different actions for similar motions.

\noindent\textbf{Effectiveness of Transformer for Policy Learning.}  
From Table~\ref{table:student}, we see that using a Transformer policy (line 5) outperforms MLP-based approaches, particularly on the test set and out-of-distribution cases generated by the kinematic model. We attribute this to the Transformer's inductive bias in sequential modeling and its capacity to incorporate longer-term observations, enabling it to handle complex spatio-temporal dependencies more effectively.

\begin{figure}
    \centering
    \includegraphics[width=\columnwidth]{figures/object_gen.pdf}
    \caption{Zero-shot generalization of our student policy on novel objects from BEHAVE~\cite{bhatnagar22behave} and HODome~\cite{zhang2023neuraldome}.
    }
    \label{fig:zero2}
\end{figure}
