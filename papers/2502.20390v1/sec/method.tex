\begin{figure}
    \centering
    \includegraphics[width=\columnwidth]{figures/method_new_new.pdf}
    \caption{Our two-stage pipeline: (\textbf{i}) training each teacher policy (MLP) on a small data subset with initialization corrected via Physical State Initialization (PSI), and (\textbf{ii}) freezing the teacher policies to provide refined references for training a student policy (Transformer). The student leverages teacher supervision for effective scaling and is fine-tuned through RL.
    }
    \label{fig:method}
\end{figure}
\section{Methodology}
\noindent\textbf{Task Formulation.}
The goal of human-object interaction (HOI) imitation is to learn a policy \(\pi\) that produces simulated human-object motion \(\{\boldsymbol{q}_t\}_{t=1}^T\) closely matching a ground-truth reference \(\{\hat{\boldsymbol{q}}_t\}_{t=1}^T\) derived from large-scale MoCap data. Given the geometries of the human and objects, the policy should also compensate for missing or inaccurate details in the dataset.  
Each pose \(\boldsymbol{q}_t\) has two components: the human pose \(\boldsymbol{q}^h_t\) and the object pose \(\boldsymbol{q}^o_t\). The human pose is defined as \(\boldsymbol{q}^h_t = \{\boldsymbol{\theta}^h_t, \boldsymbol{p}^h_t\}\), where \(\boldsymbol{\theta}^h_t \in \mathbb{R}^{51 \times 3}\) represents the joint rotations, and \(\boldsymbol{p}^h_t \in \mathbb{R}^{51 \times 3}\) specifies the joint positions. Specifically, our human model includes 30 hand joints and 21 joints for the rest of the body.
The object pose \(\boldsymbol{q}^o_t\) is represented as \(\{\boldsymbol{\theta}^o_t, \boldsymbol{p}^o_t\}\), where \(\boldsymbol{\theta}^o_t \in \mathbb{R}^{3}\) denotes the object’s orientation and \(\boldsymbol{p}^o_t \in \mathbb{R}^{3}\) the position.
All simulation states have corresponding ground-truth values, denoted by the hat symbol. For instance, the reference object rotation is \(\{\hat{\boldsymbol{\theta}}^o_t\}_{t=1}^T\). The environmental setup for the simulation is detailed in Sec.~\ref{sec:phys_para} of supplementary.

\noindent\textbf{Overview.}
We formulate interaction imitation as a Markov Decision Process (MDP), defined by states, actions, simulator-provided transition dynamics, and a reward function.  
Figure~\ref{fig:method} illustrates our two-stage framework: (i) training teacher policies \(\pi^{(T)}\) on small skill subsets, and (ii) distilling these teachers into a scalable student policy \(\pi^{(S)}\) for large-scale skill learning.  
In Sec.~\ref{sec:state}, we define the states \(\boldsymbol{s}_t\) and actions \(\boldsymbol{a}_t\), applicable to both teacher \(\pi^{(T)}\) and student \(\pi^{(S)}\) policies.  
In Sec.~\ref{sec:teacher}, we describe how teacher policies are trained via RL, focusing on reward designs that facilitate retargeting, as well as techniques that mitigate the impact of imperfections in the reference data.  
Sec.~\ref{sec:student} details the subsequent distillation of teachers into a scalable student policy, leveraging both RL and learning from demonstration.

\subsection{Policy Representation} \label{sec:state}
\noindent\textbf{State.}
The state \(\boldsymbol{s}_t\), which serves as input to the policy, comprises two components \(\boldsymbol{s}_t = \{\boldsymbol{s}_t^s, \boldsymbol{s}_t^g\}\). The first part, \(\boldsymbol{s}_t^s\), contains human proprioception and object observations, expressed as,
 $\{\{\boldsymbol{\theta}_t^h, \boldsymbol p_t^h, \boldsymbol{\omega}_t^h, \boldsymbol v_t^h\}, \{\boldsymbol{\theta}_t^o, \boldsymbol p_t^o, \boldsymbol{\omega}_t^o, \boldsymbol v_t^o\}, \{\boldsymbol{d}_t, \boldsymbol{c}_t\}\},$
where $\{\boldsymbol{\theta}_t^h, \boldsymbol p_t^h, \boldsymbol{\omega}_t^h, \boldsymbol v_t^h\}$ represent the rotation, position, angular velocity, and velocity of all joints, respectively, while $\{{\boldsymbol \theta}_t^o,  \boldsymbol p_t^o, {\boldsymbol \omega}_t^o,  \boldsymbol v_t^o\}$ represent the orientation, location, velocity, and angular velocity of the object, respectively. Motivated by~\cite{christen2022d}, we include object geometry and whole-body haptic sensing from two elements: (\textbf{i}) $\boldsymbol{d}_t$, vectors from human joints to their nearest points on each object surface; and (\textbf{ii}) $\boldsymbol{c}_t$, contact markers indicating whether the human’s rigid body parts experience applied forces; this serves as simplified tactile or force sensing -- an important multi-modal input in robot manipulation tasks~\cite{dahiya2009tactile, yu2024mimictouch, si2024difftactile, huang20243d}. The goal state $\boldsymbol s_{t}^g = \{\boldsymbol s_{t, t+k}^g\}_{k \in K}$ integrates reference poses from the ground truth motion, where $\boldsymbol s_{t, t+k}^g$ is defined as,
\begin{equation}\label{eq:state}
\begin{aligned} 
 \{\{\hat{\boldsymbol{\theta}}_{t+k}^h \ominus \boldsymbol{\theta}_t^h, \hat{\boldsymbol p}_{t+k}^h - \boldsymbol p_t^h\}, \{\hat{{\boldsymbol \theta}}_{t+k}^o \ominus {\boldsymbol \theta}_t^o, \hat{ \boldsymbol p}_{t+k}^o -  \boldsymbol p_t^o\}, \\\{\hat{\boldsymbol{d}}_{t+k} - \boldsymbol{d}_t, \hat{\boldsymbol{c}}_{t+k} - \boldsymbol{c}_t\}, \{\hat{\boldsymbol{\theta}}_{t+k}^h, \hat{\boldsymbol p}_{t+k}^h, \hat{{\boldsymbol \theta}}_{t+k}^o, \hat{\boldsymbol p}_{t+k}^o \}\},
\end{aligned} 
\end{equation}
where $\hat{\boldsymbol{\theta}}_{t+k}^h, \hat{\boldsymbol p}_{t+k}^h, \hat{\boldsymbol{d}}_{t+k}, \hat{\boldsymbol{c}}_{t+k}$ represent the reference information at time step $t+k$, $\ominus$ denotes the calculation of rotation difference.
All continuous elements of $\boldsymbol{s}_t$ are normalized relative to the current direction of view of the human and the position of the root~\cite{peng2018deepmimic}. 

We extract reference contact markers \(\hat{\boldsymbol{c}}_{t+k}\) by inferring dynamic information, in addition to inaccurate contact distances, specifically by analyzing the object's acceleration to detect human-induced forces. 
To accommodate the variability in contact distances observed in reference motion, we discretize reference contact markers using varying distance thresholds, as illustrated in Fig.~\ref{fig:contact_label}(\textbf{i}). The neutral areas serve as buffer zones, avoiding the penalization or enforcement of strict contact.
See Sec.~\ref{sec:repre_supp} of supplementary for details.

\noindent\textbf{Action.}
Our human model has 51 actuated joints, defining an action space of $\boldsymbol{a}_t \in \mathbb{R}^{51\times3}$. These actions are specified as joint PD targets using the exponential map and are converted into torques applied to each of the human joints.

\subsection{Imitation as Perfecting}\label{sec:teacher}
The teacher policy \(\pi^{(\text{T})}\) is trained via RL to maximize the expected discounted reward by comparing simulated states against potentially erroneous reference states. The training involves: (\textbf{i}) trajectory collection, where we explain how trajectories are initialized and terminated. (\textbf{ii}) policy updating, where collected trajectories and their associated rewards are used to refine the policy. In this section, we elaborate on our reward design and how we optimize trajectory collection to mitigate the impact of reference inaccuracies.

\noindent\textbf{Imitation as Retargeting.}
We tailor teacher policies to each human subject, while all policies share the same base human model. This serves the retargeting purpose by converting HOIs from different human shapes into a unified base shape. Although motion imitation does not necessarily require a unified human model~\cite{won2019learning,luo2023perpetual}, our approach offers two benefits:
(i) It enhances integration with kinematic generation methods, which generally perform better on a single, unified shape~\cite{guo2022generating, tevet2023human}.
(ii) It demonstrates possible integration with real-world humanoid deployment, which requires retargeting to a consistent physical embodiment. In Figure~\ref{fig:teaser}, our method translates MoCap data into motor skills on a Unitree G1~\cite{unitreeg1} with two Inspire hands~\cite{inspire}, all without external retargeting in complex contact-rich scenarios. See Sec.~\ref{sec:training} of the supplementary for additional details.

Human~\cite{villegas2021contact} or HOI~\cite{kim2016retargeting} retargeting can be formulated as an optimization problem. Inverse Kinematics (IK) methods, such as those based on quadratic programming~\cite{kraft1994algorithm}, demonstrate effectiveness in simplified scenarios but remain underexplored for motions featuring intricate object interactions. RL, by contrast, solves the optimization by maximizing an expected cumulative reward, prompting us to investigate whether RL-driven HOI imitation can be used for HOI retargeting. This extends existing physics-based retargeting approaches, which either omit object interactions~\cite{reda2023physics} or are non-scalable with a single reference~\cite{zhang2023simulation}.

While the kinematics should differ due to the embodiment gap, we argue that the \textit{dynamics} between human and object should remain \textit{invariant}. Thus, we define rewards to include an embodiment-aware component that loosely aligns the simulated kinematics with the reference interaction, and an embodiment-agnostic reward component that encourages dynamics to be close to the reference.

\noindent\textbf{Embodiment-Aware Reward.} When the human and object are far apart, retargeting should prioritize capturing rotational motion, whereas when they are close, accurate position tracking becomes crucial for achieving contact. To reflect this, we define the weights \( \boldsymbol{w}_d \) that are inversely proportional to the distances between joints and the object~\cite{zhang2023simulation}. The reward thus includes cost functions for joint position \( E_p^h = \langle \boldsymbol\Delta^h_{p}, \boldsymbol{w}_d \rangle\), rotation \( E_{\theta}^h = \langle \boldsymbol\Delta^h_{\theta}, \boldsymbol 1 - \boldsymbol{w}_d \rangle\), and interaction tracking \( E_d = \langle \boldsymbol\Delta_{d}, \boldsymbol{w}_d \rangle \), where \(\langle \cdot, \cdot \rangle\) is the inner product, $\boldsymbol\Delta^h_{p}[i]=\|\hat{\boldsymbol{p}}^h[i] - \boldsymbol{p}^h[i]\|$, $\boldsymbol\Delta^h_{\theta}[i]=\|\hat{\boldsymbol{\theta}}^h[i] \ominus \boldsymbol{\theta}^h[i]\|$, and $\boldsymbol\Delta_{d}[i]=\|\hat{\boldsymbol{d}}[i] - \boldsymbol{d}[i]\|$ represent the displacement for the variables defined in Sec.~\ref{sec:state} with timestep $t$ omitted. The formulation of \( \boldsymbol{w}_d \) is provided in Sec.~\ref{sec:reward} of supplementary. The reward to be maximized can be formulated as $\exp(-\lambda E)$ for each cost function $E$ with a specific hyperparameter $\lambda$. Details can be found in Sec.~\ref{sec:reward}.

\noindent\textbf{Embodiment-Agnostic Reward.} The reward includes components for object tracking and contact tracking. The object tracking cost is defined for position $E^o_p = \|\hat{\boldsymbol{p}}^{o} -  \boldsymbol p^{o}\|$ and rotation $E^o_{\theta} = \|\hat{\boldsymbol\theta}^{o} - {\boldsymbol\theta}^{o}\|$, with all values normalized to the human's current position and direction.
\begin{figure}
    \centering
    \includegraphics[width=\columnwidth]{figures/contact_label.pdf}
    \caption{(\textbf{i}) Visualization of reference contact markers that accommodate varied contact distances: \textcolor{red}{red} to promote contact, \textcolor{green}{green} for neutral areas where contact is neither promoted nor penalized, and \textcolor{blue}{blue} to penalize contact. (\textbf{ii}) Initializing the rollout with reference (RSI) or reference corrected via simulation (PSI).
    }
    \label{fig:contact_label}
\end{figure}
The contact tracking reward comprises two cost functions: body contact promotion \( E^c_b \) and penalty \( E^c_p \), both aligning the simulated contact \( \boldsymbol{c} \) with reference markers \( \hat{\boldsymbol{c}} \), as shown in Figure~\ref{fig:contact_label}. We define three contact levels -- promotion, penalty, and neutral -- to accommodate potential inaccuracies in reference contact distances. The detailed formulation can be found in Sec.~\ref{sec:reward} of the supplementary.
Since the physics engine does not differentiate between object, ground, and self-contact, we adopt two strategies:
\textbf{(i)} we model foot-ground contact promotion and penalty. This ensures proper foot lifting during cyclic walking and mitigates foot hobbling.
\textbf{(ii)} We allow self-collision to avoid self-contact promotion but to promote object interaction. This poses minimal risk as the policy is guided by MoCap reference, which, although lacking perfect contact accuracy, rarely shows self-penetration. For humanoid robots with embodiments that differ from the MoCap reference and require real-world applicability, we disable self-collision, as discussed in Sec.~\ref{sec:training}.
% Unlike \cite{wang2023physhoi,wang2024skillmimic}, we consider contacts for every body part.

We introduce energy consumption rewards~\cite{yu2018learning} to penalize large human or object jitters, with a proposed contact energy penalizing abrupt contact to promote compliant interactions. See Sec.~\ref{sec:reward} of supplementary for more details.

\noindent\textbf{Hand Interaction Discovery.}
We use data with average or flattened hand poses~\cite{bhatnagar22behave,li2023object}, which makes accurate object manipulation imitation challenging. To address this, we activate a reference contact marker for any hand part when a \textit{fingertip} or \textit{palm} is near an object. Given tasks that do not demand high dexterity, employing a contact-promoting reward with this marker enables policies to develop effective hand interaction strategies, leveraging the exploratory nature of RL. Additionally, we constrain the range of motion (RoM) of the hands to ensure natural movement. See Sec.~\ref{sec:reward} and Sec.~\ref{sec:phys_para} of the supplementary for further details.

\noindent\textbf{Policy Learning.}
Following~\cite{peng2018deepmimic}, the control policy $\pi$ is trained using PPO~\cite{schulman2017proximal} with the policy gradient $L(\psi) = \mathbb{E}_t [ \min(r_t(\psi) {A}_t, \text{clip}(r_t(\psi), 1-\epsilon, 1+\epsilon) {A}_t).$ $\psi$ are the parameters of $\pi$ and \( r_t(\psi) \) quantifies the difference in action likelihoods between updated and old policies. \( \epsilon \) is a small constant, and \({A}_t \) is the advantage estimate given by the generalized advantage estimator GAE($\lambda$)~\cite{schulman2015high}. 


\noindent\textbf{Physical State Initialization.}
Learning later-phase motion can be essential for policies to achieve high rewards during earlier phases, compared to incrementally learning from the starting phase. 
Thus, Reference State Initialization (RSI)~\cite{peng2018deepmimic} sets the current pose ${\boldsymbol{q}}_t$ to a reference pose $\hat{\boldsymbol{q}}_t$ at a random timestep $t$, for initializing the rollout.
However, initializing with the imperfect reference can introduce \textit{critical artifacts}, such as contact floating or incorrect hand motion, leading to unrecoverable failures, \eg, object falling, as depicted in Figure~\ref{fig:contact_label}(ii). These issues render many initializations ineffective, limiting training on certain interaction phases since successful rollouts may not reach them before the maximum length. The problem is exacerbated by the use of prioritized sampling~\cite{luo2023universal,xie2022learning,won2019learning,tessler2024maskedmimic}, which favors high-failure-rate initializations.

To address the need for higher-quality reference initialization, we propose \textit{Physical State Initialization} (PSI). As illustrated in Figure~\ref{fig:method}, PSI begins by creating an initialization buffer that stores reference states from MoCap and simulation states from prior rollouts. For each new rollout, an initial state is randomly selected from this buffer, which increases the likelihood of starting from advantageous positions. Once a rollout is completed, trajectories are evaluated based on their expected discounted rewards; those above a certain threshold are added to the buffer using a first-in-first-out (FIFO) strategy, while older or lower-quality trajectories are discarded. This selective reintroduction of high-value states for initialization helps maintain stable policy updates. We apply PSI in a sparse manner to ensure training efficiency. As shown in Figure~\ref{fig:contact_label}(ii), PSI can collect trajectories for policy update that RSI does not effectively utilize. Further details are provided in Sec.~\ref{sec:psi_supp} of the supplementary.

\noindent\textbf{Interaction Early Termination.}
Early Termination (ET)~\cite{peng2018deepmimic} is commonly used in motion imitation, ending an episode when a body part makes unplanned ground contact or when the character deviates significantly from the reference~\cite{luo2023perpetual}, thus stopping the policy from overvaluing invalid transitions. However, additional conditions should be considered for human-object interactions. We propose \textit{Interaction Early Termination} (IET), which supplements ET with three extra checks:
(\textbf{i}) Object points deviate from their references by more than 0.5\,m on average.
(\textbf{ii}) Weighted average distances between the character’s joints and the object surface exceed 0.5\,m from the reference.
(\textbf{iii}) Any required body-object contact is lost for over 10 consecutive frames.
Full conditions are detailed in Sec.~\ref{sec:psi_supp} of the supplementary.

\subsection{Imitation with Distillation}\label{sec:student}
As shown in Figure~\ref{fig:method}, after training the teacher policies on data from each subject (Sec.~\ref{sec:teacher}), we aggregate them to train a student policy $\pi^{(\text{S})}$ to master all skills. As outlined in Algorithm~\ref{algo:dis}, the combined teacher policies, denoted by $\pi^{(\text{T})}$ for brevity, serves dual roles by providing state-action trajectories $(\boldsymbol s^{(\text{T})}, \boldsymbol a^{(\text{T})})$: (\textbf{i}) the state $\boldsymbol s^{(\text{T})}$ for reference distillation, and (\textbf{ii}) the action $\boldsymbol a^{(\text{T})}$ for policy distillation.

\noindent\textbf{Reference Distillation.}
Noisy MoCap data can hinder policy learning, especially at larger scales. In contrast, teacher 
policies trained on smaller-scale data effectively address these issues by correcting contact artifacts, refining hand placements, and recovering missing details (see Figures~\ref{fig:teaser} and \ref{fig:obj_rot}). To fully leverage teacher policies, we use their rollouts as references for defining the student policy’s goal state and reward functions, distinguishing our approach from distillation based on only action output.

\noindent\textbf{Policy Distillation.} We also apply distillation on action outputs, which we view as crucial for scaling policies to large datasets. In essence, we trade space for time: teacher policies are trained in parallel on smaller data subsets, allowing the student policy to scale through distillation.
Following Algorithm~\ref{algo:dis}, we begin with Behavior Cloning (BC)~\cite{juravsky2024superpadl,wagener2022mocapact} and then use RL fine-tuning to go beyond demonstration memorization, an approach common in LLM alignment~\cite{ouyang2022training,touvron2023llama}. Inspired by~\cite{rajeswaran2017learning,smith2023learning}, we integrate BC into online policy updates and adopt a staged schedule: we start with DAgger~\cite{ross2011reduction} and gradually transition to PPO. Throughout, the critic is continuously trained with the reward from Sec.~\ref{sec:teacher}. This RL fine-tuning phase is crucial because teacher policies may behave differently when performing similar skills, and simple BC can lead to suboptimal ``averaging'' behavior, where RL fine-tuning helps the student policy converge on optimal solutions.


\subsection{Architecture}
We set the keyframe indices \(K\) (Sec.~\ref{sec:state}, Eq.~\ref{eq:state}) to \(\{1, 16\}\) for the teacher policies and \(\{1, 2, 4, 16\}\) for the student policy. The broader observation window for the student policy helps it better distinguish different skills with larger-scale data. Teacher policies employ MLPs, common in physics-based animation~\cite{peng2018deepmimic}, while the student policy handles higher-dimensional observations, for which MLPs are less effective. Thus, we use a transformer~\cite{transformer} architecture for sequential modeling~\cite{tessler2024maskedmimic}, as shown in Figure~\ref{fig:method}.

\begin{algorithm}
    \caption{Distillation with RL Fine-tuning}
    \label{algo:dis}
    \begin{algorithmic}[1]
    \State \textbf{Input}: A composite policy $\pi^{(\text{T})}$ integrated from individual teacher policies, student policy parameters $\boldsymbol{\psi}$, student value function parameters $\boldsymbol{\phi}$, schedule hyperparameter $\beta$ for DAgger, horizon length $H$ for PPO
    \For{$t = 0, 1, 2, \ldots$ } 
        \For{$h$ from $1$ to $H$}
            \State Sample a variable $u \sim \text{Uniform}(0, 1)$
            \State Collect $\boldsymbol s^{(\text{T})}, \boldsymbol a^{(\text{T})}$ from teacher $\pi^{(\text{T})}$
            \State Obtain the refined reference from $\boldsymbol s^{(\text{T})}$ to define $\boldsymbol{s}^{(\text{S})}$ and $r(\cdot)$, obtain $\boldsymbol a^{(\text{S})}$ from $\pi^{(\text{S})}_{\boldsymbol\phi}(\boldsymbol a^{(\text{S})}|\boldsymbol{s}^{(\text{S})})$.
            \If{$u \leq \frac{t}{\beta}$} \Comment{Use the teacher}
                \State Given $\boldsymbol s^{(\text{S})}$, execute $\boldsymbol a^{(\text{S})}$, observe $\boldsymbol s'^{(\text{S})}, r$
            \Else \Comment{Use the student}
                \State Given $\boldsymbol s^{(\text{S})}$, execute $\boldsymbol a^{(\text{T})}$, observe $\boldsymbol s'^{(\text{S})}, r$
            \EndIf
            \State Store the transition $(\boldsymbol s^{(\text{S})}, \boldsymbol s'^{(\text{S})}, \boldsymbol a^{(\text{S})}, \boldsymbol a^{(\text{T})}, r)$
        \EndFor
    \State Update $\boldsymbol\phi$ with TD($\lambda$)
    \State Compute PPO objective: $L(\psi)$
    \State Compute $J(\psi) = \|\boldsymbol a^{(\text{S})} - \boldsymbol a^{(\text{T})}\|$
    \State Compute the weight: $w = \min(\frac{t}{\beta}, 1)$
    \State Update $\psi$ by gradient: $\nabla_{\psi} (wL(\psi) + (1-w) J(\psi))$
    \EndFor
    \end{algorithmic}
\end{algorithm}