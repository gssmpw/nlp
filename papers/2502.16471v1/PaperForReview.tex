% WACV 2025 Paper Template
% based on the WACV 2024 template, which is
% based on the CVPR 2023 template (https://media.icml.cc/Conferences/CVPR2023/cvpr2023-author_kit-v1_1-1.zip) with 2-track changes from the WACV 2023 template (https://github.com/wacv-pcs/WACV-2023-Author-Kit)
% based on the CVPR template provided by Ming-Ming Cheng (https://github.com/MCG-NKU/CVPR_Template)
% modified and extended by Stefan Roth (stefan.roth@NOSPAMtu-darmstadt.de)

\documentclass[10pt,twocolumn,letterpaper]{article}

%%%%%%%%% PAPER TYPE  - PLEASE UPDATE FOR FINAL VERSION
% \usepackage[review,algorithms]{wacv}      % To produce the REVIEW version for the algorithms track
%\usepackage[review,applications]{wacv}      % To produce the REVIEW version for the applications track
\usepackage{wacv}              % To produce the CAMERA-READY version
%\usepackage[pagenumbers]{wacv} % To force page numbers, e.g. for an arXiv version

% Include other packages here, before hyperref.
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{adjustbox}
\usepackage{float}


\usepackage{algpseudocode}
\usepackage[linesnumbered,ruled,vlined]{algorithm2e}

% It is strongly recommended to use hyperref, especially for the review version.
% hyperref with option pagebackref eases the reviewers' job.
% Please disable hyperref *only* if you encounter grave issues, e.g. with the
% file validation for the camera-ready version.
%
% If you comment hyperref and then uncomment it, you should delete
% ReviewTempalte.aux before re-running LaTeX.
% (Or just hit 'q' on the first LaTeX run, let it finish, and you
%  should be clear).
\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}


% Support for easy cross-referencing
\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}


%%%%%%%%% PAPER ID  - PLEASE UPDATE
\def\wacvPaperID{1734} % *** Enter the WACV Paper ID here
\def\confName{WACV}
\def\confYear{2025}


\begin{document}

%%%%%%%%% TITLE - PLEASE UPDATE
\title{Feature Space Perturbation: A Panacea to Enhanced Transferability Estimation}

% \author{Prafful Kumar Khoba\\
% UQ–IITD Research Academy\\
% Delhi, India\\
% {\tt\small qiz228274@iitd.ac.in}
% % For a paper whose authors are all at the same institution,
% % omit the following lines up until the closing ``}''.
% % Additional authors and addresses can be added with ``\and'',
% % just like the second author.
% % To save space, use either the email address or home page, not both
% \and
% Zijian Wang\\
% The University of Queensland\\
% Brisbane, Australia\\
% {\tt\small zijian.wang@uq.edu.au}
% \and
% Chetan Arora\\
% IIT Delhi\\
% Delhi, India\\
% {\tt\small chetan@cse.iitd.ac.in}
% \and
% Mahsa Baktashmotlagh\\
% The University of Queensland\\
% Brisbane, Australia\\
% {\tt\small m.baktashmotlagh@uq.edu.au}}

\author{
Prafful Kumar Khoba$^{1}$ \quad Zijian Wang$^{2}$ \quad Chetan Arora$^{3}$ \quad Mahsa Baktashmotlagh$^{2}$ \\
$^{1}$UQ–IITD Research Academy, New Delhi, India \\
$^{2}$The University of Queensland, Brisbane, Australia \\
$^{3}$Indian Institute of Technology Delhi, New Delhi, India \\
{\tt\small qiz228274@iitd.ac.in, zijian.wang@uq.edu.au, chetan@cse.iitd.ac.in, m.baktashmotlagh@uq.edu.au}
}

\maketitle

\begin{figure*}[!h]
  \centering
  \includegraphics[width=\textwidth]{images/Teaser.pdf}
  \caption{Illustration of our feature perturbation method for transferability estimation. (a) provides a flowchart outlining the process of enhancing transferability estimation, with bold elements representing our perturbation steps. The remaining part of the flowchart illustrates the process traditionally employed in existing transferability estimation work. (b) shows initial embeddings with significant inter-class separation and compact intra-class clustering, typical of supervised models. (c) displays embeddings after our feature perturbation.
  (d) and (e) present actual correlation charts and weighted Kendall correlation coefficient $\tau_w$ on the pets dataset. Correlation chart depicts the predicted rankings versus actual rankings before and after perturbations, where each symbol in these charts represents a model. The shift from lower to higher correlation values highlights the improved accuracy of model rankings after applying our perturbation method.}
  \label{fig:teaser}
  \vspace{-1em}
\end{figure*}


%%%%%%%%% ABSTRACT
\begin{abstract}
    Leveraging a transferability estimation metric facilitates the non-trivial challenge of selecting the optimal model for the downstream task from a pool of pre-trained models. Most existing metrics primarily focus on identifying the statistical relationship between feature embeddings and the corresponding labels within the target dataset, but overlook crucial aspect of model robustness. This oversight may limit their effectiveness in accurately ranking pre-trained models. To address this limitation, we introduce a feature perturbation method that enhances the transferability estimation process by systematically altering the feature space. Our method includes a Spread operation that increases intra-class variability, adding complexity within classes, and an Attract operation that minimizes the distances between different classes, thereby blurring the class boundaries.
   Through extensive experimentation, we demonstrate the efficacy of our feature perturbation method in providing a more precise and robust estimation of model transferability. Notably, the existing LogMe method exhibited a significant improvement, showing a 28.84$\%$ increase in performance after applying our feature perturbation method. The implementation is available at \url{https://github.com/prafful-kumar/enhancing_TE.git}
\end{abstract}

\section{Introduction}


Transfer learning enables the application of knowledge acquired from one task to enhance performance on another with only minimal additional training, typically through fine-tuning \cite{zhuang2020comprehensive}. At the core of this approach is the challenge of transferability estimation, which aims to predict how well the pre-trained models adapt when applied to new, target datasets without fine-tuning. However, the main challenge lies in determining the most appropriate model for a specific task without individually fine-tuning each candidate model, which can be a resource-intensive and time-consuming task, especially for large target datasets.

To address this issue, researchers have proposed various metrics \cite{nguyen2020leep, tran2019transferability, li2021ranking,you2021logme,shao2022not,DBLP:conf/iccv/WangLZHB23,Pandy2022TransferabilityEstimation, zhang2024model} based on the distinctive properties of the pre-trained model, known as the transferability estimation metric. The core idea behind many of these metrics is to establish a statistical relationship between the feature embedding of pre-trained models and the labels of the samples, thereby generating a transferability score for each model. Based on the score, these metrics aim to predict the actual rank of these models on the target dataset, with the goal of achieving a high correlation between the predicted rank and the actual rank, as shown in Fig. \ref{fig:teaser}(a). However, while these metrics effectively measure adaptability, they often overlook how models handle disruptions in the structure of the embeddings, which is crucial for assessing their robustness. To fill this gap, we propose a novel approach that perturbs both the intra-class (through the spread operation) and inter-class (through the attract operation) structures of the embeddings.
By introducing this targeted perturbation, we aim to provide a more comprehensive and realistic assessment of a model's resilience and adaptability. This method not only improves the accuracy of transferability estimations but also ensures that the selected models are genuinely capable of performing reliably in diverse and dynamic environments.


To illustrate how robustness to perturbations impacts transferability estimation, consider a toy example depicted in Fig. \ref{fig:teaser}.
The figure visually illustrates how embeddings respond to perturbations and its impact on model transferability. Initially, the embeddings display clear inter-class separation and compact intra-class configurations Fig. \ref{fig:teaser}(a), typical of supervised models' structured embeddings. After our feature perturbation method is applied Fig. \ref{fig:teaser}(b), these embeddings show expanded intra-class distributions and reduced inter-class separations, highlighting how perturbations disrupt the standard embedding structure. As traditional metrics rely adaptability of the features, they tend to produce lower transferability scores post-perturbation for all the models. Yet, the degree of score reduction varies significantly among the models. This is evident on the x-axis of the correlation chart for the pets dataset in Fig. \ref{fig:teaser}(d,e), as referenced from the results section \ref{sect:result}. For instance, if one model’s score drops drastically compared to others, it indicates a lower robustness to perturbations. This substantial decrease suggests that the model’s embeddings are overly sensitive to changes, potentially undermining its performance when applied to new datasets.
This leads to a more accurate relative ranking of these models. %These improvements are detailed in the experimental section \ref{sec:exp}, highlighting how perturbations refine the correlation between predicted and actual model rankings.


While most of the prior works focus on estimating the transferability of models under the vanilla fine-tuning schema, we argue that it may not be sufficient to address a broader use case. Recent literature~\cite{kirichenko2022dfr, kumar2022fine} highlights that while vanilla fine-tuning can achieve higher in-distribution (ID) test accuracy, it sacrifices the out-of-distribution (OOD) robustness compared with alternative fine-tuning strategy (\textit{i.e.}, linear probing). The trade-off between ID and OOD test accuracy urges transferability estimation metrics to consider a wider range of fine-tuning schemes. Therefore, in this work, we assess the transferability estimation metric for three distinct fine-tuning strategies: \textit{vanilla fine-tuning}, which updates all model parameters; \textit{last block fine-tuning (LBFT)}, which updates only the parameters of the last block and final linear layer; and \textit{linear fine-tuning (LFT)}, which focuses on updating the last fully connected layer. In summary, our contribution is threefold:

% Note that we avoid applying our perturbation technique to self-supervised models. This is because self-supervised models do not rely on labeled data to learn representations \cite{sela, simclr-v1, simclr-v2, byol, infomin}. Whereas our feature perturbation method, which relies on class label information, may disrupt the meaningful structures they learn by erasing valuable geometric configurations of the embeddings. Unlike supervised models, which follow a discriminative pattern, self-supervised models do not, as shown in Fig. \ref{fig:ss_vs_s}. This complexity makes it difficult to create an effective perturbation strategy for self-supervised models. As a result, our research is dedicated to improving transferability estimation for supervised models only. However, we have developed LDA-based metrics for self-supervised models that surpass many of the previous transferability estimation metrics. In summary, our contribution is threefold:

\begin{itemize}
    \item  
    Our feature perturbation method effectively perturbs the feature embedding spaces of pre-trained supervised models, significantly improving the rank estimation of these pre-trained models by transferability estimation metric. This approach led to a notable 28.84$\%$ performance boost in the LogMe \cite{you2021logme} method.
    \item 
    Our findings highlight that existing metrics are primarily effective for vanilla fine-tuning, pointing out the necessity for more adaptable solutions for diverse fine-tuning techniques. Our feature perturbation method significantly enhances transferability estimation across key fine-tuning strategies, specifically vanilla and LBFT. Its effectiveness across these strategies and compatibility with all current transferability estimation metrics underscores its broad applicability and versatility. 
    \item 
   Unlike supervised models, self-supervised models do not rely on labeled data and are particularly sensitive to disruptions in their geometric embedding structures. Therefore, we avoid applying our class-based perturbation strategy to these models. Instead, we have developed a Linear Discriminant Analysis \cite{balakrishnama1998linear} (LDA)-based metric, specifically tailored for self-supervised models, that significantly outperforms traditional baselines across all fine-tuning variants.

\end{itemize}
% Our analysis demonstrates the superior performance of simpler architectures, like Linear Discriminant Analysis (LDA) \cite{balakrishnama1998linear}, over state-of-the-art methods for self-supervised models across all fine-tuning variants. 


\section{Related Work}

% \subsection{Transferability estimation techniques}

Prior work can be categories into label-based method and source-embedding based method. Log Expected Empirical Predictor (LEEP) \cite{nguyen2020leep} and Negative Conditional Entropy (NCE) \cite{tran2019transferability} are two transferability estimation metrics that rely on information from both the source and target label sets. % One drawback of label-comparison based methods is their sensitivity to changes in the model's head (the last layer), which can affect the output scores of these metrics. Instead of using the head of the pre-trained model, source-embedding based approaches tend to use the feature extractor for creating transferability estimation metric.
Passing target data through a pre-trained model's feature extractor yields target embeddings for source-embedding methods. These methods include $\mathcal{N}$LEEP \cite{li2021ranking}, LogMe \cite{you2021logme}, SFDA \cite{shao2022not}, NCTI \cite{DBLP:conf/iccv/WangLZHB23}, and GBC \cite{Pandy2022TransferabilityEstimation}.


$\mathcal{N}$LEEP \cite{li2021ranking} is build to remove the drawbacks of LEEP, i.e., it does not uses source head. It utilizes Principal Component Analysis (PCA) \cite{wold1987principal} to reduce the dimensionality of the data, followed by fitting a Gaussian Mixture Model (GMM) \cite{reynolds2009gaussian} to the target data embedding. LogMe \cite{you2021logme} assesses model transferability by modeling each target label as a linear model with Gaussian noise using maximum evidence to evaluate how well pre-trained model features fit target labels. GBC \cite{Pandy2022TransferabilityEstimation} metric evaluates how much classes in the target data's embedding space overlap; a higher GBC value indicates greater overlap. SFDA, presented in \cite{shao2022not}, is a transferability metric which employs Fisher Discriminant Analysis (FDA) \cite{mika1999fisher} and introduces ConfMix. FDA works by finding a linear data transformation that maximizes the separation between classes using within-class scatter and between-class scatter followed by applying ConfMix, a mechanism to generate challenging negative samples. NCTI \cite{DBLP:conf/iccv/WangLZHB23} metric quantifies the gap between a model's embedding and the ideal neural collapse embedding. 

% A notable attempt by Li et al. \cite{li2023exploring} introduces the Potential Energy Decline (PED) approach, a physics-inspired method aimed at better capturing the dynamics of model adaptability by modeling the interactive forces that influence the fine-tuning process. \cite{menta2024active} enhances transferability estimation by selecting the most informative subset from the target dataset using the pre-trained model's internal and output representations. However, none of the existing works adequately address the aspect of robustness in transferability estimation. Our study introduces a vital perspective, crucial for developing more reliable transferability estimation metrics that effectively incorporate both adaptability and robustness.

Li et al. \cite{li2023exploring} proposed Potential Energy Decline (PED), a physics-inspired method modeling interactive forces to capture model adaptability during fine-tuning. Menta et al. \cite{menta2024active} improved transferability estimation by selecting informative subsets of the target dataset using pre-trained model representations. However, existing methods overlook robustness in transferability estimation. Our study addresses this gap by introducing a framework that integrates adaptability and robustness for more reliable transferability metrics.
\section{Methodology}

In this section, we outline the problem definition, our proposed method, transferability estimation metrics, and the evaluation criteria for model selection. For the sake of simplicity, we use image classification as our primary task throughout the paper.

\subsection{Preliminaries}

\textbf{Problem definition:}
Let $\mathcal{T} = \{X,Y\}$ represent the target dataset, and $\{\phi_l\}_{l=1}^L$ denote $L$ the pre-trained feature-extractors, with $l$ being the index. Using transferability estimation, given by $\mathcal{M}$, our goal is to rank these pre-trained models based on their performance on a given target dataset. We assess the transferability of each pre-trained model using a specific metric $\mathcal{M}$ that produces a numerical score, denoted as $T_l$. Essentially, a higher score for $T_l$ implies that the model $\phi_l$ is more likely to perform effectively on the given target dataset. 

\noindent\textbf{Ground truth:}
To establish a reliable basis for comparing pre-trained models, a process of fine-tuning each model on the target dataset is conducted, accompanied by a thorough exploration of different hyperparameters (i.e., learning rate and weight decay). Test accuracy obtained from this fine-tuning is then utilized as a `ground truth' for the ranking of these models. The fine-tuning performance of each model is notated as $\{G_l\}_{l=1}^L$, serving as a benchmark for evaluating the model ranking by transferability estimation metric.


\subsection{Proposed Approach}
\label{sec:SA}


Source-embedding based transferability estimation metrics\cite{shao2022not, DBLP:conf/iccv/WangLZHB23,li2021ranking,Pandy2022TransferabilityEstimation,you2021logme} leverage features extracted from pre-trained models to estimate their adaptability to a downstream task. Our proposed feature perturbation method systematically tests the robustness of model embeddings by applying controlled perturbations. Recognizing that the embedding structure varies with the feature extractor and the target dataset, our method avoids a one-size-fits-all perturbation. Instead, it dynamically adjusts the perturbation magnitude to preserve the meaningful structure of embeddings. We achieve this through two operations called spread and attract (SA), which are described below:



\noindent\textbf{Spread operation:} One of the desirable properties of features in supervised learning is high intra-class compactness \cite{bengio2013representation}. The Spread operation deliberately perturbs the internal structure of each class by increasing the intra-class variance, effectively decreasing intra-class compactness. This perturbation pushes examples that were previously near the centroid—and thus easier to classify—further away, increasing their variability and making them more challenging to classify correctly. Specifically, for every class, we compute the centroid of the class \( \mathbf{C}_u\) as given by Eq. \ref{eq:centroid} and displace each data point uniformly away from its centroid, given by:



\begin{equation}
\mathbf{\hat{X}}_{\text{spread}, u} = \mathbf{\hat{X}}_u + \left( \frac{\mathbf{\hat{X}}_u - \mathbf{C}_u}{\|\mathbf{\hat{X}}_u - \mathbf{C}_u\|_2} \right)
\label{eq:spread}
\end{equation}


\begin{equation}
\mathbf{C}_u = \frac{1}{n_u} \sum_{i=1}^{n_u} \mathbf{\hat{X}}_{u,i}
\label{eq:centroid}
\end{equation}

where \(\mathbf{\hat{X}}_{\text{spread}, u}\) represents the feature embeddings for class \(u\) after the spread operation, \(\mathbf{C}_u\) is the centroid of class \(u\), \(\mathbf{\hat{X}}_{u}\) is the reduced-dimensionality embedding of class \(u\) obtained after applying PCA on the extracted feature embeddings, and \( n_u \) is the number of samples in class \( u \).


\noindent\textbf{Attract operation:}  Another desirable property of features in supervised learning is high inter-class separability\cite{bengio2013representation}. Supervised models for classification tasks generally learn clear boundaries between classes in feature space. The attract operation aims to diminish this separation by adjusting class embeddings according to the distances between their centroids and variance, thereby perturbing the feature space to create closer inter-class proximities. This operation tests the resilience of the model's embeddings in scenarios where class boundaries are intentionally blurred.
The attract operation is formulated as:

\begin{equation}
\mathbf{\hat{X}}_{\text{attract}_{u}} = \mathbf{\hat{X}}_{\text{spread}_{u}} + \alpha \cdot \mathbf{Disp}_{uv}
\label{eq:attract}
\end{equation}

where, \(\mathbf{\hat{X}}_{\text{attract}_{u}}\) is the feature embedding of the class \textit{u} after applying the attract method to \(\mathbf{\hat{X}}_{\text{spread}_{u}}\), 
$\alpha$ is a hyper-parameter that modulates the magnitude of the displacement towards the other class centroids, and $\mathbf{Disp}_{uv}$ is: 

\begin{equation}
\mathbf{Disp}_{uv} = \sum_{v \neq u} \left( \frac{\mathbf{D}_{uv}}{\|\mathbf{D}_{uv}\|_2} \right) \cdot \left( \|\mathbf{D}_{uv}\|_2 - (\sigma \cdot R_u + \sigma \cdot R_v) \right) \\ \nonumber
\label{eq:displacement}
\end{equation} 

\begin{equation}
R_u = \sqrt{\frac{1}{n_u} \sum_{i=1}^{n_u} \| \mathbf{X}_{u,i} - \mathbf{C}_u \|^2}
\label{eq:standard_deviation}
\end{equation}

Here, $\mathbf{D}_{uv} = \mathbf{C}_u - \mathbf{C}_v$ is the distance vector between the centroids \(\mathbf{C}\) of two different classes, \(u\) and \(v\). $\sigma$ is a hyper-parameter that adjusts the sensitivity of the displacement to the variability within class embeddings, as measured by $R_u$ and $R_v$. 
Attract operation ensures that the perturbations are proportionate to the natural variability within and between the classes. This approach helps to preserve the meaningful structure of the embeddings while testing their resilience against perturbations.



\begin{algorithm} [!h]
\caption{SA algorithm}
\label{alg:SA}
\KwIn{Target dataset $\mathcal{T}=\{X,Y\}$, pre-trained models $\{\phi_l\}_{l=1}^L$, hyper-parameters $\sigma$, $\alpha$, transferability metric $\mathcal{M}$}
\KwOut{Transferability scores $T_l$ for each model}
\For{$l=1$ \textbf{to} $L$}{
    $\hat{X} \leftarrow$ PCA($\phi_l(X)$) \\
    \For{each class $k$ in $Y$}{
        $C_k \leftarrow$ Compute centroid of $\hat{X}_k$ \\
        \For{each point $X_{k,i}$ in class $k$}{
            $D_{k,i} \leftarrow \frac{X_{k,i} - C_k}{\|X_{k,i} - C_k\|_2}$ \\
            $X_{\text{spread}_{k,i}} \leftarrow X_{k,i} + D_{k,i}$ \tcp{Spread}
        }
    }
    \For{each class pair $(u, v)$}{
        $D_{uv} \leftarrow C_u - C_v$ \\
        $R_u \leftarrow \| \text{Std}(\hat{X}_u) \|_2$, $R_v \leftarrow \| \text{Std}(\hat{X}_v) \|_2$ \\
        \For{each $X_{\text{spread}_{u,i}}$ in class $u$}{
            $Disp_{uv,i} \leftarrow \sum_{v \neq u} \frac{D_{uv}}{\|D_{uv}\|_2} \cdot (\|D_{uv}\|_2 - (\sigma \cdot R_u + \sigma \cdot R_v))$ \\
            $X_{\text{attract}_{u,i}} \leftarrow X_{\text{spread}_{u,i}} + \alpha \cdot Disp_{uv,i}$ \tcp{Attract}
        }
    }
    $T_l \leftarrow \mathcal{M}(X_{\text{attract}}, Y)$
}
Rank models based on $T_l$.
\end{algorithm}


 \noindent\textbf{The importance of controlled perturbation:}
Fig. \ref{fig:hyp}(b) demonstrates an optimal level of perturbation that preserves the necessary balance and structural integrity required for accurate transferability estimation. Conversely, Fig. \ref{fig:hyp}(c) shows how suboptimal perturbation settings can result in excessive alterations, complicating the feature space and hindering transferability assessment. These examples underline the crucial role of carefully calibrated perturbations in ensuring reliable model evaluations.


\begin{figure}[!h]
  \centering
  \includegraphics[width=0.90\columnwidth]{images/new_hyperparameter_variation.pdf}
  \vspace{-2ex}
  \caption{Demonstrating the importance of controlled perturbation in feature space manipulation, using a toy example. (a) Represents the initial target embedding. (b) depict an appropriate amount of feature perturbation, while (c) demonstrate excessive levels of feature perturbation.}
  \label{fig:hyp}
  \vspace{-1ex}
\end{figure}

\subsection{Overall Objective}

Our feature perturbation method systematically alters the feature embedding representation for target dataset $\mathcal{T}=\{X,Y\}$ using $\{\phi_l\}_{l=1}^L$ feature extractor of the pre-trained model. We then compute the transferability score for each model $\{T_l\}_{l=1}^L$ using the metric $\mathcal{M}$, which assesses the perturbed embeddings $\mathbf{\hat{X}}_{\text{attract}}$ against the target labels $\mathbf{Y}$, as shown in the equation below. 
\begin{equation}
\label{eq:transferability_estimation}
    T_l = \mathcal{M}(\mathbf{\hat{X}}_{\text{attract}}, \mathbf{Y})
\end{equation}
For each feature extractor $\phi_l$, the algorithm extracts feature embeddings and applies PCA to get a reduced-dimensionality representation, $\hat{X}$. This step is crucial for managing computational complexity and focusing on the most informative features of the embeddings. The algorithm for our perturbation method is given in Algorithm \ref{alg:SA}. 
% Essentially, a higher score for $T_l$ implies that the model $\phi_l$ is more likely to perform effectively on the given target dataset.


% \textcolor{red}{Choose one of the algorithm (see code editor!)}

% \begin{algorithm} [!t]
% \caption{Option 2: Detailed Feature perturbation algorithm}
% \label{alg:SA}
% \KwIn{Target dataset $\mathcal{T}=\{X,Y\}$; Pre-trained models $\{\phi_l\}_{l=1}^L$; Hyper-parameter: Difficulty enhance parameters $\sigma$; TE metric $\mathcal{M}$;}
% \KwOut{$T_l$ metric score for each model}
% \For{$l=1$ {\bfseries to} $L$}{
%     Extract feature embedding from model $\phi_l$ and apply PCA to get $\hat{X}$\\
%     Apply Spread operation: \\
%     \For{each class $k$ in $Y$}{
%         Calculate the class centroid $C_k$: $C_k = \frac{1}{N_k} \sum_{i=1}^{N_k} \hat{X}_{k,i}$ \\
%         Apply Spread operation to class $k$: \\
%         \For{each point $X_{k,i}$ in class $k$}{
%             Calculate normalized direction vector $D_{k,i}$: 
%             $D_{k,i} = \frac{\hat{X}_{k,i} - C_k}{\|\hat{X}_{k,i} - C_k\|_2}$ \\
%             Update embedding: 
%             $\hat{X}_{\text{spread}_{k,i}} = \hat{X}_{k,i} + D_{k,i}$ \\
%         }
%     }
  
%     % Combine all spread points into $X_{\text{spread}}$ \\
%     Apply Attract operation: \\
%     \For{each pair of different classes $(u, v)$}{
%         % Calculate standard deviation $R_u, R_v = \sqrt{\text{var\(\hat{X}_spread\)}_u}, \sqrt{\text{var\(\hat{X}_spread\)}_v}$ \\
%         Calculate $L2$-norm of standard deviation $(SD)$ for each class 
%          $R_u = \| SD_u \|_2, \quad R_v = \| SD_v \|_2$  \\
%         % Spread $R_u = \|\hat{X}_u\|_2, \quad R_v = \|\hat{X}_v\|_2$ \\
%         % Calculate standard deviation $R_u, R_v = \sqrt{\frac{1}{N_u}\sum_{i=1}^{N_u} \|\hat{X}_{\text{spread}_{u,i}} - C_u\|_2^2}, \sqrt{\frac{1}{N_v}\sum_{i=1}^{N_v} \|\hat{X}_{\text{spread}_{v,i}} - C_v\|_2^2}$ \\

%         Calculate distance vector $D_{uv}$: $D_{uv} = C_u - C_v$ \\
%         \For{each point $\hat{X}_{\text{spread}_{u,i}}$ in class $u$}{
%             Calculate attraction displacement $Disp_{uv,i}$: $Disp_{uv,i} = \sum_{v \neq u} \left( \frac{D_{uv}}{\|D_{uv}\|_2} \right) \cdot \left( \|D_{uv}\|_2 - (\sigma \cdot R_u + \sigma \cdot R_v) \right)$ \\
%             Update embedding: $\hat{X}_{\text{attract}_{u,i}} = \hat{X}_{\text{spread}_{u,i}} + \alpha \cdot Disp_{uv,i}$ \\
%         }
%     }
%     $T_l = \mathcal{M}(\hat{X}_{\text{attract}},Y)$
  
% }
% Rank the models based on the $T_l$ metric score.
% \end{algorithm}


% \vspace{-1ex}

% \textcolor{red}{can we make this para better?}
% The combined effects of the spread and attract operations create a perturbed feature space that rigorously tests the models' ability to maintain distinct and organized class representations under altered conditions. This serves as a rigorous test of their robustness. Models that continue to exhibit clear, discernible features despite these perturbations demonstrate a high level of resilience and are likely to receive higher rankings, thereby improving the accuracy of transferability estimations. Moreover, our feature perturbation method not only enhances the transferability estimation process by rigorously testing and proving the endurance of model embeddings under stress but also ensures that models identified for their robustness are truly capable of adapting to diverse and challenging scenarios. This approach not only refines the evaluation metrics but also significantly boosts confidence in the practical application of these models.


% vanilla supervised
\begin{table*}[t!]\setlength\tabcolsep{5pt}
\footnotesize
\centering
\caption{Performance comparison (average weighted Kendall $\tau_w$) between original and enhanced frameworks for vanilla fine-tuning on supervised models. For a pair of rows, the original metric is presented first followed by the corresponding enhanced metric. Best results are highlighted in bold. Our framework consistently outperforms the original framework across all metrics.} 
\label{tab:sup_fine-tune}
\begin{adjustbox}{max width=\textwidth}
\begin{tabular}{l|ccccccccccc|c}
    \toprule
    Method          & Aircraft & Caltech-101 & Cars  & CIFAR10 & CIFAR100 & DTD   & Flowers & Food-101 & Pets  & Sun & VOC & Average \\ 
    \midrule
    $\mathcal{N}$LEEP\cite{li2021ranking}           & -0.449   & \textbf{0.769}       & 0.602 & 0.783   & 0.717    & 0.796 & 0.295   & 0.581    & 0.511 & \textbf{0.944} & 0.710 & 0.569 \\
    \textbf{SA} + $\mathcal{N}$LEEP& \textbf{0.236}   & 0.626       & \textbf{0.763} & \textbf{0.910}    & \textbf{0.843}    & \textbf{0.836} & \textbf{0.435}   & \textbf{0.657}   & \textbf{0.829} & 0.790 & \textbf{0.828}   & \textbf{0.704} \\
    \hline
    LogME\cite{you2021logme}           & 0.439    & 0.497       & \textbf{0.605} & 0.852   & 0.725    & 0.700 & 0.147   & 0.385    & 0.411 & 0.511 & 0.695 & 0.542 \\
    \textbf{SA} + LogME & \textbf{0.442}    & \textbf{0.655}       & 0.603 & \textbf{0.924}  & \textbf{0.855}    & \textbf{0.784} & \textbf{0.743}   & \textbf{0.665}    & \textbf{0.447} & \textbf{0.788} & \textbf{0.782} & \textbf{0.698} \\
    \hline
    GBC\cite{Pandy2022TransferabilityEstimation}             & \textbf{0.423}    & 0.213       & \textbf{0.617} & 0.735   & 0.664    & 0.703 & 0.214   & 0.548    & 0.514 & 0.271 & 0.743 & 0.513 \\
    \textbf{SA} + GBC   & -0.110    & \textbf{0.473}       & 0.591 & \textbf{0.928}   & \textbf{0.789}    & \textbf{0.713} & \textbf{0.510}  & \textbf{0.711}   & \textbf{0.651} & \textbf{0.803} & \textbf{0.787} & \textbf{0.622} \\
    \hline
    SFDA\cite{shao2022not}            & \textbf{0.614}    & \textbf{0.615}       & 0.574 & \textbf{0.949}   & 0.866    & 0.575 & 0.492   & \textbf{0.815}    & 0.545 & 0.558 & 0.671 & 0.661 \\
    \textbf{SA} + SFDA  & 0.414    & 0.598       & \textbf{0.801} & 0.901   & \textbf{0.908}    & \textbf{0.816} & \textbf{0.736}   & 0.681    & \textbf{0.865} & \textbf{0.790} & \textbf{0.816} & \textbf{0.756} \\
    \hline
    NCTI\cite{DBLP:conf/iccv/WangLZHB23}            & 0.496     & \textbf{0.492}      & 0.662 & 0.843   & \textbf{0.879}    & 0.616 & 0.541   & \textbf{0.773}    & \textbf{0.867} & 0.756 & 0.741 & 0.697 \\
    \textbf{SA} + NCTI  & \textbf{0.872}   & 0.483       & \textbf{0.805} & \textbf{0.843}   & 0.878    & \textbf{0.776} & \textbf{0.714}   & 0.619    & 0.856 & \textbf{0.790} & \textbf{0.800}   & \textbf{0.767} \\
    \bottomrule
\end{tabular}
\end{adjustbox}
\end{table*}

% LBFT supervised
\begin{table*}[h]\setlength\tabcolsep{5pt}
\footnotesize
\centering
\caption{Performance comparison (average weighted Kendall $\tau_w$) between original and enhanced frameworks for LBFT on supervised models. For a pair of rows, the original metric is presented first followed by the corresponding enhanced metric. The optimal outcomes are emphasized in bold. Our framework consistently exceeds the performance of the original framework across all metrics.} 
\label{tab:sup_conv-ft}
\begin{adjustbox}{max width=\textwidth}
\begin{tabular}{l|ccccccccccc|c}
    \toprule
    Method & Aircraft & Caltech-101 & Cars & CIFAR10 & CIFAR100 & DTD & Flowers & Food-101 & Pets & Sun & VOC & Average \\ \midrule
    $\mathcal{N}$LEEP\cite{li2021ranking} & -0.415 & 0.370 & 0.159 & 0.732 & 0.803 & 0.593 & -0.035 & 0.667 & 0.505 & \textbf{0.691} & 0.512 & 0.417 \\
    \textbf{SA} + $\mathcal{N}$LEEP & \textbf{0.305} & \textbf{0.735} & \textbf{0.848} & \textbf{0.757} & \textbf{0.853} & \textbf{0.629} & \textbf{0.020} & \textbf{0.742} & \textbf{0.529} & 0.641 & \textbf{0.777} & \textbf{0.621} \\ 
    \hline
    LogME\cite{you2021logme} & \textbf{0.386} & \textbf{0.577} & 0.453 & \textbf{0.789} & 0.640 & \textbf{0.715} & 0.286 & 0.690 & 0.192 & \textbf{0.627} & 0.222 & 0.507 \\
    \textbf{SA} + LogME & 0.223 & 0.260 & \textbf{0.633} & 0.739 & \textbf{0.831} & 0.548 & \textbf{0.458} & \textbf{0.783} & \textbf{0.233} & 0.622 & \textbf{0.707} & \textbf{0.548} \\
    \hline
     GBC\cite{Pandy2022TransferabilityEstimation} & \textbf{0.676} & 0.076 & 0.476 & 0.631 & 0.751 & 0.612 &\textbf{ 0.176} & \textbf{0.790} & \textbf{0.349} & 0.395 & 0.210 & 0.467 \\
    \textbf{SA} + GBC & -0.164 & \textbf{0.541} & \textbf{0.626} & \textbf{0.656} & \textbf{0.767} & \textbf{0.650} & 0.133 & 0.749 & 0.149 & \textbf{0.641} & \textbf{0.777} & \textbf{0.502} \\
    \hline
    SFDA\cite{shao2022not} & \textbf{0.395} & 0.432 & 0.324 & 0.702 & 0.671 & 0.585 & \textbf{0.414} & 0.553 & 0.372 & 0.393 & 0.142 & 0.453 \\
    \textbf{SA} + SFDA & 0.174 & \textbf{0.754} & \textbf{0.864} & \textbf{0.765} & \textbf{0.775} & \textbf{0.768} & 0.361 & \textbf{0.708} & \textbf{0.698} & \textbf{0.610} & \textbf{0.764} & \textbf{0.658} \\
    \hline
    NCTI\cite{DBLP:conf/iccv/WangLZHB23} & 0.366 &0.441& 0.447 & 0.728 & 0.760 & 0.395 & 0.150 & 0.637 & \textbf{0.766} & \textbf{0.848} & 0.388 & 0.539 \\
    \textbf{SA} + NCTI & \textbf{0.620} & \textbf{0.652} & \textbf{0.942} & \textbf{0.739} &\textbf{ 0.789} & \textbf{0.568} & \textbf{0.415} & \textbf{0.730} & 0.685 & 0.641 & \textbf{0.792} & \textbf{0.688} \\ 
    \bottomrule
\end{tabular}
\end{adjustbox}
\end{table*}


 % linear probing supervised
\begin{table*}[h]\setlength\tabcolsep{5pt}
\footnotesize
\centering
\caption{Comparison (average weighted Kendall $\tau_w$) between original and enhanced frameworks for LFT on supervised models. In each pair of rows, the original metric is listed first, followed by the corresponding enhanced metric. The superior results are emphasized in bold.} 
\label{tab:sup_lp}
\begin{adjustbox}{max width=\textwidth}
\begin{tabular}{l|ccccccccccc|c}
    \toprule
    Method & Aircraft & Caltech-101 & Cars & CIFAR10 & CIFAR100 & DTD & Flowers & Food-101 & Pets & Sun & VOC & Average \\ \midrule
    $\mathcal{N}$LEEP\cite{li2021ranking} & 0.422 & 0.685 & \textbf{0.747} & 0.558 & 0.509 & \textbf{0.790} & \textbf{0.378} & 0.512 & \textbf{0.717} & \textbf{0.746} & 0.698 &\textbf{ 0.615} \\
    \textbf{SA} + $\mathcal{N}$LEEP & \textbf{0.615} & \textbf{0.760} & 0.328 & \textbf{0.831} & \textbf{0.705} & 0.635 & 0.285 & \textbf{0.615} & 0.677 & 0.244 & \textbf{0.824} & 0.592 \\
    \hline
    LogME\cite{you2021logme} & \textbf{0.127} & 0.247 & \textbf{0.144} & 0.490 & 0.359 & \textbf{0.721} & 0.170 & 0.229 & -0.128 & 0.198 & 0.470 & 0.275 \\
    \textbf{SA} + LogME & -0.129 & \textbf{0.363} & 0.073 & \textbf{0.907} & \textbf{0.883} & 0.634 & \textbf{0.465} & \textbf{0.625} & \textbf{0.178} & \textbf{0.262} & \textbf{0.689} & \textbf{0.450} \\
    \hline
     GBC\cite{Pandy2022TransferabilityEstimation} & \textbf{-0.048} & 0.266 & \textbf{0.124} & 0.359 & 0.373 & 0.440 & 0.121 & 0.330 & \textbf{0.209} & 0.228 & 0.531 & 0.267 \\
    \textbf{SA} + GBC & -0.308 & \textbf{0.628} & 0.044 & \textbf{0.935} & \textbf{0.818} & \textbf{0.758} & \textbf{0.304} & \textbf{0.694} & 0.096 & \textbf{0.244} & \textbf{0.824} & \textbf{0.457} \\
    \hline
    SFDA\cite{shao2022not} & 0.320 & 0.475 & \textbf{0.508} & 0.611 & 0.558 & 0.380 & \textbf{0.429} & \textbf{0.709} & 0.119 & \textbf{0.574} & 0.432 & 0.465 \\
    \textbf{SA} + SFDA & \textbf{0.585} & \textbf{0.934} & 0.381 & \textbf{0.882} & \textbf{0.905} & \textbf{0.855} & 0.307 & 0.684 & \textbf{0.751} & 0.251 & \textbf{0.812} & \textbf{0.667} \\
    \hline
    NCTI\cite{DBLP:conf/iccv/WangLZHB23} & \textbf{0.656} & 0.775 & \textbf{0.572} & 0.627 & 0.636 & 0.526 & \textbf{0.589} & \textbf{0.675} & 0.431 &\textbf{ 0.521} & 0.667 & 0.607 \\
    \textbf{SA} + NCTI & 0.182 & \textbf{0.833} & 0.381 & \textbf{0.907} & \textbf{0.840} & \textbf{0.738} & 0.423 & 0.635 & \textbf{0.706} & 0.244 & \textbf{0.839} & \textbf{0.611} \\
    \bottomrule
\end{tabular}
\end{adjustbox}
\end{table*}

\section{Experiments}
\label{sec:exp}

This section is organized into five distinct parts to evaluate the proposed feature perturbation method. Section \ref{sect:setup} outlines the experimental setup; Section \ref{sect:result} presents the results; Section \ref{sect:ablation} covers the ablation study; Section \ref{sect:hyperparameter} examines hyper-parameter sensitivity; Section \ref{sect:time} analyzes time complexity; and section \ref{sect:self-supervised} presents the result on self-supervised architectures.

\subsection{Experiment Setup}
\label{sect:setup}

\noindent\textbf{Datasets.} Our study utilizes a diverse collection of datasets commonly used in transferability estimation research \cite{shao2022not}. The collection contains fine-grained object classification dataset (\textit{i.e.}, Oxford-102 Flowers \cite{nilsback2008automated}, Food-101 \cite{bossard2014food}, Stanford Cars \cite{cars}, FGVC Aircraft \cite{maji2013fine}, Oxford-IIIT Pets \cite{parkhi2012cats}),
coarse-grained object classification dataset (\textit{i.e.}, Caltech-101 \cite{fei2004learning}, Cifar-10 \cite{cifar}, Cifar-100 \cite{cifar}, Voc2007 \cite{pascal-voc-2007} ), 
one scene classification dataset (\textit{i.e.}, Sun397 \cite{xiao2010sun}), and 
one texture classification dataset (\textit{i.e.}, DTD \cite{cimpoi2014describing}). These datasets provide a broad spectrum of challenges under various scenarios.


\subsection{Experimental Results}
\label{sect:result}

\noindent\textbf{Overview.} To evaluate the performance of transferability estimation metrics, we initiate the model pool by following the existing work~\cite{shao2022not}. Specifically, the model pool consists of 11 ImageNet pre-trained architectures, including InceptionV1 \cite{goingdeeperwithconvolutions}, InceptionV3 \cite{rethinkingtheinceptionarchitectureforcv}, ResNet50 \cite{deepresidulalearningforimagerecognition}, ResNet101 \cite{deepresidulalearningforimagerecognition}, ResNet152 \cite{deepresidulalearningforimagerecognition}, DenseNet121 \cite{denselyconnectedconvolutionalnetworks}, DenseNet169 \cite{denselyconnectedconvolutionalnetworks}, DenseNet201 \cite{denselyconnectedconvolutionalnetworks}, MobileNetV2 \cite{mobilenetv2invertedresidualsandlinearbottlenecks}, and NASNet-A Mobile \cite{mnasnetplatformawareneuralarchitecturesearchformobile}. 


\noindent\textbf{Ground truth and correlation measurement.}
We follow the grid search described in \cite{shao2022not}, which selects the learning rates from $\{10^{-1}, 10^{-2}, 10^{-3}, 10^{-4}\}$ and weight decay parameters from $\{10^{-6}, 10^{-5}, 10^{-4}, 10^{-3}\}$. To ensure the robustness and reliability of our evaluation, we execute the code using five distinct seeds for each experiment and then take the average of target accuracy. For evaluation, we use weighted Kendall \cite{kendall1938new} correlation coefficient $\tau_w$ because it assigns weights to the concordant and discordant pairs based on their positions in the ranking; A high positive $\tau_w$ value indicates a strong correlation, while a negative value implies an inverse correlation.


\noindent\textbf{Performance comparison on vanilla fine-tuning.} In this part, we consider vanilla fine-tuning, which updates all parameters during the training process. We report the experimental results in Table \ref{tab:sup_fine-tune}. As can be seen from the table, SA feature perturbation method demonstrates notable improvements in transferability estimation across all previous metrics. In particular, our proposed method achieves a relative improvement of 23.86 \% on $\mathcal{N}$LEEP, 28.84\% on LogMe, 21.27\% on GBC, 14.46\% on SFDA, 10.04\% on NCTI, and with the LogMe metric benefiting the most from this approach. The table reports an average improvement of 19.69\% on previous SOTA transferability estimation methods, indicating the effectiveness of our approach.

% \textcolor{red}{should we remove this para. This figure and para can develop some question in the minds of reviewers}
Meanwhile, our perturbation strategy demonstrates variably beneficial outcomes in a few datasets. For example, on Aircraft, our methods can only improve three out of five baseline metrics. We argue that embedding structure for a few datasets contains class overlap causing mixed improvements for various metrics. To understand the intrinsic difference of these datasets, we randomly sample three classes and visualize the feature distribution of (a) variably beneficial datasets and (b) consistent beneficial datasets in Fig. \ref{fig:result_embed}. One can see from the figure that the class-conditional feature distribution largely overlaps with each other. A similar pattern can be seen in other datasets like Caltech-101 and Cars. Given the well-separated feature embeddings in set (b), our perturbation method has a more pronounced effect in enhancing transferability estimation. This improvement is evident in the column corresponding to the dataset in Table \ref{tab:sup_fine-tune}. %Additional results and detailed analyses are provided in the supplementary material.

\begin{figure}[!t]
  \centering
  \includegraphics[width=\columnwidth]{images/result_viz_updated_color.pdf}
  \caption{Visualization of ResNet50 target embeddings before feature perturbation (best viewed in color): (a) Represents datasets exhibiting mixed improvement for various metrics, shown in Table \ref{tab:sup_fine-tune}. The presence of class overlap in (a) contributes to the varied performance across metrics. In contrast, (b) depicts datasets demonstrating consistent improvement across all other evaluated metrics, facilitated by well-separated embeddings. This distinction underscores the role of embedding structure in the estimation.}
  \label{fig:result_embed}
  \vspace{-1ex}
  %\vspace{-2em}
\end{figure}


\noindent\textbf{Performance comparison on fine-tuning variants.} In this work, we study the performance of transferability estimation metrics under two alternative fine-tuning strategies, namely LBFT and LFT. LBFT updates the last convolutional block and the final linear layer of the network, and LFT only updates the final linear layer with all other parameters frozen in the training process. More details can be found in the supplementary material. 

Table \ref{tab:sup_conv-ft} shows the ranking performance of the models trained under the LBFT strategy.  Although the order of rank of the estimation metrics remains largely consistent between vanilla fine-tuning and LBFT, the overall ranking correlation drops from 0.59 to 0.47. This suggests that previous metrics could be vulnerable when applied to LBFT. Following SA feature perturbation, there is a substantial performance boost of 27.47\% for LBFT, proving the effectiveness of our feature perturbation method in enhancing accurate transferability estimation specifically for LBFT scenarios.

Compared to vanilla fine-tuning, LFT can be deployed faster, with improved performance on out-of-distribution (OOD) test samples. The ranking performance under the LFT strategy is shown in Table.~\ref{tab:sup_lp}. While a strategy like LFT does not change the structure of the feature space during training, our approach can still achieve a predominantly beneficial result. SA feature perturbation strategy improves three out of five comparison metrics, scoring 0.142 weighted Kendall $\tau_w$ increase. The results reflect that our approach is suitable to adopt in the LFT use case.

\begin{table*}[!tbh]
\setlength\tabcolsep{5pt}
\footnotesize
\centering
\caption{Performance comparison (average weighted Kendall $\tau_w$) for vanilla fine-tuning on self-supervised models.  In each column, the best results are highlighted in bold. Remarkably, LDA achieves the highest overall average weighted Kendall’s $\tau_w$ score.} 
\label{tab:ss_vanilla-ft}
\begin{adjustbox}{max width=\textwidth}
\begin{tabular}{l|ccccccccccc|c}
    \toprule
    Method & Aircraft & Caltech-101 & Cars & CIFAR10 & CIFAR100 & DTD & Flowers & Food-101 & Pets & Sun397 & VOC & Average \\ 
    \midrule
    $\mathcal{N}$LEEP\cite{li2021ranking} & -0.029 & 0.631 & 0.358 & 0.074 & 0.276 & 0.641 & 0.585 & 0.544 & \textbf{0.836} & 0.735 & -0.076 & 0.416 \\
    LogME\cite{you2021logme} & 0.223 & 0.387 & 0.387 & 0.295 & -0.028 & 0.627 & 0.718 & 0.570 & 0.704 & 0.217 & 0.121 & 0.384 \\
    GBC\cite{Pandy2022TransferabilityEstimation} & 0.070 & 0.417 & 0.464 & -0.054 & 0.237 & 0.317 & 0.701 & 0.729 & 0.484 & 0.539 & 0.161 & 0.370 \\
    SFDA\cite{shao2022not} & \textbf{0.254} & 0.526 & 0.553 & 0.619 & 0.548 & 0.815 & \textbf{0.847} & 0.685 & 0.556 & 0.732 & 0.532 & 0.606 \\
    NCTI\cite{DBLP:conf/iccv/WangLZHB23} & 0.035 & 0.643 & \textbf{0.724} & 0.546 & 0.533 & 0.715 & 0.705 & 0.892 & 0.767 & 0.697 & 0.547 & 0.619 \\
    LDA & 0.058 & \textbf{0.708} & 0.720 & \textbf{0.707} & \textbf{0.692} & \textbf{0.913} & 0.779 & \textbf{0.944} & 0.540 & \textbf{0.892} & \textbf{0.723} & \textbf{0.698} \\
    \bottomrule
\end{tabular}
\end{adjustbox}
\end{table*} 


% selfsup linear probe
\begin{table*}[!tbh] 
    \setlength\tabcolsep{5pt}
    \footnotesize
    \centering
    \caption{Performance comparison (average weighted Kendall $\tau_w$) for LFT on self-supervised models. 
    The highest performing $\tau_w$ value in each column are highlighted in bold. LDA achieves the highest overall average weighted Kendall $\tau_w$ score.} 
    \label{tab:ss_lp}
    \begin{adjustbox}{max width=\textwidth}
    \begin{tabular}{l|ccccccccccc|c}
        \toprule
        Method & Aircraft & Caltech-101 & Cars & CIFAR10 & CIFAR100 & DTD & Flowers & Food-101 & Pets & Sun397 & VOC & Average \\
        \midrule
        $\mathcal{N}$LEEP\cite{li2021ranking} & 0.446 & 0.654 & 0.537 & -0.044 & 0.210 & 0.668 & 0.633 & 0.519 & 0.608 & 0.275 & 0.126 & 0.421 \\
        LogME\cite{you2021logme} & 0.569 & 0.350 & 0.629 & -0.083 & -0.250 & 0.655 & 0.653 & 0.518 & 0.487 & -0.178 & 0.037 & 0.308 \\
        GBC\cite{Pandy2022TransferabilityEstimation} & 0.498 & 0.446 & 0.717 & -0.093 & 0.147 & 0.504 & 0.702 & 0.590 & 0.422 & 0.219 & 0.320 & 0.407 \\
        SFDA\cite{shao2022not} & 0.685 & 0.582 & 0.813 & 0.275 & 0.138 & 0.717 & 0.705 & 0.693 & 0.681 & 0.426 & 0.633 & 0.577 \\
        NCTI\cite{DBLP:conf/iccv/WangLZHB23} & 0.842 & 0.661 & \textbf{0.917} & 0.275 & 0.473 & 0.699 & 0.683 & 0.846 & \textbf{0.846} & 0.308 & 0.670 & 0.656 \\
        LDA & \textbf{0.903} & \textbf{0.764} & 0.800 & \textbf{0.598} & \textbf{0.497} & \textbf{0.807} & \textbf{0.845} & \textbf{0.867} & 0.748 & \textbf{0.656} & \textbf{0.823} & \textbf{0.755} \\
        \bottomrule
    \end{tabular}
    \end{adjustbox} \vspace{-1ex}
\end{table*}


\subsection{Ablation Study}
\label{sect:ablation}

To validate the effectiveness of each component in the proposed methods, we conduct an ablation study of the Spread and Attract operation on four different baseline transferability estimation metrics (\textit{i.e.,} NCTI, SFDA, LogMe, and GBC). We show the experimental results in Fig. \ref{fig:ablation}. From the figure, we can observe that the application of both the Spread and Attract operations brings improvement in ranking correlations. On average, the Spread operation yields a 10.57\% improvement, while the Attract operation provides a 14.57\% improvement. The Attract operation's superior performance can be attributed to its sophisticated approach that accounts for both the intra-class coherence and the inter-class separations before perturbation. Specifically, as defined in Eq \ref{eq:attract}, the Attract operation applies changes to the embeddings after considering the distance between class centroids (\(\mathbf{D}_{uv}\)) and the L2 norm of standard deviations within the classes (\(R_u\) and \(R_v\)). This allows for perturbations that are informed by a holistic view of the entire feature space, maintaining a delicate balance between disrupting and preserving the structural integrity essential for accurate class differentiation.
Moreover, the most substantial enhancement is observed when both operations are applied sequentially: first applying the Spread operation followed by the Attract operation. This sequential application leads to an average improvement of 17.82\% over the originally obtained weighted Kendall coefficient. This demonstrates that the synergistic effect of applying both operations sequentially is significantly greater than the impact of each operation when applied independently.


\begin{figure}
    \centering
    \includegraphics[width=0.9\columnwidth]{images/improvement-separate.pdf}
    \vspace{-2ex}
    \caption{This figure demonstrates a bar chart that illustrates the performance improvement of various operations of feature perturbation over the original baseline. Each metric is represented by four bars, corresponding to different operations: Original, Spread, Attract, and Combined Spread-Attract, illustrating that the combined approach significantly outperform others.}
    \label{fig:ablation}
    \vspace{-3ex}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=\columnwidth]{images/sensitivity-Page-2.png}
    \vspace{-4ex}
    \caption{Hyper-parameter sensitivity analysis: The left figure showcases consistent performance ($\tau_w$) across a wide range of $\alpha$ values, at optimum $\sigma$. This observation indicates an insensitivity to hyper-parameter changes. On the other hand, the right figure illustrates limited variance in performance ($\tau_w$) across a broad spectrum of $\sigma$ values, ranging from 0.5 to 0.9, at optimum $\alpha$. %The minimal performance fluctuation observed across diverse parameter ranges underscores the algorithm's resilience, thereby emphasizing its robustness and reliability.
    } 
    \label{fig:sensitivity}
    \vspace{-3ex}
\end{figure}


\subsection{Hyper-parameter Sensitivity Analysis}
\label{sect:hyperparameter}

In this section, we assess the impact of key hyperparameters, namely $\sigma$ and $\alpha$, which govern the magnitude of perturbation within our method. The sensitivity analysis is conducted on two best-performed transferability estimation metrics (\textit{i.e.,} SFDA and NCTI) after applying the proposed SA feature perturbation technique. Fig. \ref{fig:sensitivity} shows the ranking correlation under different $\sigma$ and $\alpha$. The minimal variations in performance across a wide range of hyper-parameters highlight the algorithm's resilience, showcasing its robustness and reliability. We note that we fix the value of one hyper-parameter to tune the other one. From the figure, we can see that a similar performance trend is presented for both metrics. Specifically, the optimal performance is achieved at $\alpha$ = 0.005 and $\sigma$ = 0.6 for metrics. The analysis results demonstrate that the recommended hyper-parameters are versatile. 




\subsection{Time Complexity}
\label{sect:time}

Our feature perturbation strategy improves ranking correlation while maintaining the same level of time complexity, as shown in Fig. \ref{fig:time}. The impact of our feature perturbation method on the time complexity of various transferability estimation metrics has yielded mixed outcomes. While some metrics have experienced a reduction in time complexity, others have seen an increase, reflecting the effect of our feature perturbation method on different evaluation approaches.
Metrics that rely heavily on the dimensionality of features such as $\mathcal{N}$LEEP, benefit from our method's dimensionality reduction, which reduces processing time. However, metrics that are less dependent on feature dimensionality do not experience the same reductions in time complexity. In fact, the introduction of SA perturbation adds computational steps, slightly increasing overall time requirements.


\begin{figure}
    \centering
    \includegraphics[width=0.9\columnwidth]{images/time_separate.pdf}
        \vspace{-2ex}
    \caption{This figure compare the time complexity before and after applying feature perturbation techniques to vanilla fine-tuning.} 
    \label{fig:time}
    \vspace{-1ex}
\end{figure}

\subsection{Performance on Self-supervised Models}
\label{sect:self-supervised}

To evaluate the transferability estimation on the self-supervised models, we construct the pool with ResNet50 \cite{deepresidulalearningforimagerecognition} pretrained on various self-supervised methods, spanning BYOL \cite{byol}, Infomin \cite{infomin}, PCL-v1 \cite{pcl}, PCL-v2 \cite{pcl}, Sela-v2 \cite{sela}, InsDis \cite{indis}, SimCLR-v1 \cite{simclr-v1}, SimCLR-v2 \cite{simclr-v2}, MoCo-v1 \cite{moco-v1}, MoCo-v2 \cite{moco-v2}, DeepCluster-v2 \cite{deepcluster}, and SWAV \cite{swav}. In self-supervised tasks, we develop a baseline method, which leverages LDA to predict class probabilities for each sample and accumulate the probability of the correct class corresponding to samples as the transferability score. Further details can be found in supplementary material.


LDA-based metric achieves impressive results on transferability estimation of self-supervised models as shown in Table. \ref{tab:ss_vanilla-ft} and Table. \ref{tab:ss_lp}. For both vanilla fine-tuning (Table. \ref{tab:ss_vanilla-ft}) and LFT (Table. \ref{tab:ss_lp}), LDA-based metric demonstrates a 12.7\% and 15.06\% higher average weighted Kendall coefficient compared to the best performing SOTA. The superior performance of the LDA-based metric compared to SOTA metrics across all fine-tuning types emphasizes a key insight: previous metrics have been primarily designed with supervised models in mind, overlooking the unique characteristics and requirements of self-supervised models.  To understand why the LDA-based metric can outperform SOTA on self-supervised estimation tasks, we study the difference in feature geometry generated by the self-supervised and supervised models. The t-SNE feature visualization can be found in Fig. \ref{fig:ss_vs_s}. Without semantic supervision, the self-supervised models present a more heterogeneous feature space than that of the supervised models. This finding also indicates that when estimating the transferability of models with diverse, less discriminative feature space, the existing transferability estimation metric could be vulnerable. On the other hand, the capability of the LDA metric to reflect the transferability of less discriminative feature spaces demonstrates a foundation for developing transferability estimation matrics for self-supervised models.



\begin{figure}[t!]
  \centering
  \includegraphics[width=\columnwidth]{images/updated_embedding.pdf}
  \caption{Comparison of CIFAR-10 embedding structures: (a) illustrates the embedding structure derived from a self-supervised model, while (b) depicts the embedding structure from a supervised model. Self-supervised models, learning without explicit label guidance, tend to capture more abstract relationships in the data, leading to embeddings with diverse patterns. In contrast, supervised models emphasize class separation, leading to a similar embedding structure for different models.}
  \label{fig:ss_vs_s}
  \vspace{-2ex}
\end{figure}


\section{Conclusions and Discussions}

The introduction and evaluation of our feature perturbation method represent a significant advancement in the field of transferability estimation. Through a comprehensive analysis, we have observed that our feature perturbation method not only enhances the accuracy of existing transferability metrics across various fine-tuning methods but also introduces a vital aspect of robustness evaluation. This additional layer of analysis provides more precise rankings by assessing the resilience of model embeddings to perturbations, ensuring that the best model is robust and transferable to new, target datasets.
Specifically, our method has shown to significantly improve metrics such as $\mathcal{N}$LEEP, LogMe, GBC, SFDA, and NCTI with varied effects on time complexity, indicating its capacity to optimize computational efficiency in certain scenarios.


We provide insights into the disparities in embedding structures between self-supervised and supervised models, emphasizing the need for carefully tailored transferability estimation metrics for both model types. Our results reveal that an LDA-based metric outperforms SOTA across all fine-tuning variants for self-supervised tasks. This highlights an opportunity for the community to develop more adaptable and accurate transferability estimation metrics. 

\section*{Acknowledgment}

This work was supported in part by the Australian Research Council (FT230100426).

%%%%%%%%% REFERENCES
{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\clearpage  % Ensures supplementary material starts on a new page

\appendix
\section*{Supplementary Material}
\input{supplement}


\end{document}
