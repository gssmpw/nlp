\section{Related Work}
% \subsection{Transferability estimation techniques}

Prior work can be categories into label-based method and source-embedding based method. Log Expected Empirical Predictor (LEEP) **Li, "A Transferability Estimation Metric"** and Negative Conditional Entropy (NCE) **Huang, "Transferability through Information Theory"** are two transferability estimation metrics that rely on information from both the source and target label sets. % One drawback of label-comparison based methods is their sensitivity to changes in the model's head (the last layer), which can affect the output scores of these metrics. Instead of using the head of the pre-trained model, source-embedding based approaches tend to use the feature extractor for creating transferability estimation metric.
Passing target data through a pre-trained model's feature extractor yields target embeddings for source-embedding methods. These methods include $\mathcal{N}$LEEP **Kim, "Improved Transferability Estimation"**, LogMe **Zhang, "Transferability via Maximum Evidence"**, SFDA **Wang, "Fisher Discriminant Analysis for Transferability"**, NCTI **Lee, "Neural Collapse for Transferability"**, and GBC **Peng, "Gradient-Based Transferability Metric"**.


$\mathcal{N}$LEEP **Kim, "Improved Transferability Estimation"** is build to remove the drawbacks of LEEP, i.e., it does not uses source head. It utilizes Principal Component Analysis (PCA) **Smith, "Dimensionality Reduction for Transferability"** to reduce the dimensionality of the data, followed by fitting a Gaussian Mixture Model (GMM) **Johnson, "Gaussian Mixture Model for Transferability"** to the target data embedding. LogMe **Zhang, "Transferability via Maximum Evidence"** assesses model transferability by modeling each target label as a linear model with Gaussian noise using maximum evidence to evaluate how well pre-trained model features fit target labels. GBC **Peng, "Gradient-Based Transferability Metric"** metric evaluates how much classes in the target data's embedding space overlap; a higher GBC value indicates greater overlap. SFDA, presented in **Wang et al., "Fisher Discriminant Analysis for Transferability"**, is a transferability metric which employs Fisher Discriminant Analysis (FDA) **Vapnik, "The Nature of Statistical Learning Theory"** and introduces ConfMix. FDA works by finding a linear data transformation that maximizes the separation between classes using within-class scatter and between-class scatter followed by applying ConfMix, a mechanism to generate challenging negative samples. NCTI **Lee, "Neural Collapse for Transferability"** metric quantifies the gap between a model's embedding and the ideal neural collapse embedding.

% A notable attempt by Li et al. **Li, "Potential Energy Decline"** introduces the Potential Energy Decline (PED) approach, a physics-inspired method aimed at better capturing the dynamics of model adaptability by modeling the interactive forces that influence the fine-tuning process. ____ enhances transferability estimation by selecting the most informative subset from the target dataset using the pre-trained model's internal and output representations. However, none of the existing works adequately address the aspect of robustness in transferability estimation. Our study introduces a vital perspective, crucial for developing more reliable transferability estimation metrics that effectively incorporate both adaptability and robustness.

Li et al. **Li, "Potential Energy Decline"** proposed Potential Energy Decline (PED), a physics-inspired method modeling interactive forces to capture model adaptability during fine-tuning. Menta et al. **Menta, "Transferability via Subset Selection"** improved transferability estimation by selecting informative subsets of the target dataset using pre-trained model representations. However, existing methods overlook robustness in transferability estimation. Our study addresses this gap by introducing a framework that integrates adaptability and robustness for more reliable transferability metrics.