\section{Discussion}

\subsection{Implications to LLM-powered Systems for Remote Patient Monitoring}
Through our design and review process, we identify three major components that are crucial for LLM integration into clinical RPM systems: 
(1) \textit{a descriptive protocol}, such as a flow diagram or a set of text-based instructions, to guide interactions for collecting patient health information; 
(2) \textit{a comprehensive list of key information} that the LLM is expected to cover; and 
(3) \textit{a priority mapping framework} to assist clinical staff in reviewing information or determining future actions.
With these elements, LLMs can effectively incorporate domain-specific clinical guidelines and needs into remote patient monitoring systems, thereby providing robust support for clinical workflows.

Prior studies have highlighted the importance of aligning digital health tools with clinical workflows to enhance their adoption and efficacy \cite{staras2021using, marwaha2022deploying}, with some exploration of clinical integration only on decision-support or documentation systems~\cite{chen2024burextract, rajashekar2024human}.
Our \projectname{} system, extending the RPM systems beyond general inquiries, integrates domain-specific clinical information needs to enhance its ability to provide timely and relevant patient monitoring. We picture that such LLM-powered systems may impact the RPM work from the following perspectives.








\textit{Long-term Impact to Patient Care}
In clinical practice, the integration of LLMs may significantly alter standardized workflows. 
For example, follow-up phone calls made by nurse practitioners or patient self-reporting may be supplemented with LLM-assisted tools based on (1) the descriptive protocol, then reducing the time and effort required for routine monitoring and data collection with (3) priority mapping framework. This revised workflow would involve a combination of LLM-supported monitoring and human-led clinical visits, with providers remaining central to decision-making and patient care. 
Still, in this transformation, designers must carefully evaluate which steps can be simplified and which should remain integral. 
For instance, face-to-face visits and physical examinations should continue to play a critical role in patient care, ensuring the reliability and safety of high-stakes clinical decisions.



Apart from supporting clinical work in RPM, LLMs may also enhance \textit{patient education} beyond existing question-answer chatbots~\cite{hao2024outlining} with (2) a comprehensive list of key questions. By engaging patients in scaffolded, professionally guided conversations, LLMs could help reinforce awareness of critical symptoms and health conditions during daily monitoring. This approach has the potential to encourage patients to adopt more proactive and informed health management practices. Long-term health monitoring could potentially shift the focus from reactive, short-term responses to sustained, preventative care.











\paragraph{Engage Clinical Stakeholders when Facing Design Conflicts}
As we integrated domain-specific knowledge into our system design, we observed differing perspectives between cancer care providers and HCI researchers and designers. For instance, since there were no written clinical coloring guidelines available, our research team first followed common mental models and design conventions to use the colors red, yellow, and green to indicate the severity of symptoms. Unexpectedly, the provider participants \textit{completely disagreed} with this design, arguing that colors should be used to note the importance of different symptoms, while numbers should indicate severity.
This feedback prompted adjustments to our initial designs to better align with clinical needs. 
Thus, we suggest that future designs of LLM-powered systems for RPM should always closely engage clinical stakeholders in reviewing and testing the design so that they are clinically effective. Furthermore, integrating recent HCI advancements in data visualization, collaboration, and time management~\cite{lin2024hevelius, branco2024co} can further enhance these systems. Future research could consider incorporating information about providers' schedules, digital tools, and collaborative workflows to support both decision-making and managing the complexities of GI cancer recovery. 




\paragraph{Challenges and Needs in LLM-powered Conversational RPM}
During our design process, we also identified several limitations and opportunities for improving LLM systems in clinical applications. One key challenge is the limited availability of open and standardized clinical guidelines. Every disease has unique requirements, and existing resources often lack the depth needed for generalizable solutions. Expanding digitally curatable datasets, such as structured text or annotated images, could address this gap and enable broader applicability of such systems across diverse clinical scenarios. 
Another challenge lies in patient interaction, as many symptoms and expressions are highly personal and subjective, sometimes even non-lexical~\cite{tran2023mm}. Researchers could explore different ways to collect and validate patient expressions, such as novel interaction techniques, conversation design, or sound processing algorithms.

\paragraph{Multi-modal RPM with LLM as a Central Component}
Beyond our design process, we also picture further development of LLM-powered RPM systems. The domain-specific nature of clinical scenarios influences how clinical guidelines should be integrated with LLMs. In our design, we focused on using an LLM-based conversational agent (CA) for collecting patient-reported data in postoperative GI cancer care, where symptoms are primarily internal and can be described through language. However, for other conditions, such as cardiovascular disease, incorporating additional data collection devices, such as mobile sensors, could be essential. Recent advancements in ubiquitous computing, such as actionable sensing and detection~\cite{adler2024beyond}, could be integrated into the RPM process. In such cases, LLMs could serve as a central component for managing and analyzing multimodal data, processing diverse inputs and presenting actionable insights to providers through an interface.
LLMs are particularly suited for non-emergency, long-term monitoring scenarios. In ubiquitous computing and patient monitoring contexts, LLMs can leverage clinical guidelines to analyze and synthesize data from multiple modalities, enhancing the comprehensiveness and effectiveness of patient management systems.


\subsection{Responsible AI Systems for Remote Patient Monitoring}
\subsubsection{Accountability and Responsibility Boundary in AI Responses}
Researchers have outlined principles for responsible AI, emphasizing fairness, explainability, and accountability~\cite{dignum2019responsible, arrieta2020explainable}, which in clinical contexts extend to opacity, responsibility, and reliability~\cite{smith2021clinical}. 
Building on these foundations, our study demonstrates practical implementations of responsible AI in RPM. During our PD sessions and user studies, the participant feedback emphasized the importance of accountability and objectivity, while ensuring that users are clearly informed of the system’s capability boundaries. Future LLM-powered RPM systems should emphasize a section specifying responsibilities and boundaries of LLM agents; there could also be a checking step before the agents return the responses to users to filter out risky content.



\subsubsection{Caution against Over-Reliance on AI}

Prior work has shown that AI can streamline healthcare workflows and reduce provider burden~\cite{yildirimMultimodalHealthcareAI2024}, but our findings highlight the risks of \textit{over-reliance on AI} in RPM, particularly when patients underestimate symptom severity. This can lead to AI-generated recommendations that overlook critical health issues, emphasizing the need for human oversight in system design.
Our study revealed \textit{differing attitude}s toward AI-generated clinical suggestions: GI cancer patients were less concerned about ethical risks, while providers exercised greater caution. This suggests patients may lack awareness of potential dangers, underscoring the importance of stricter, safety-focused approaches. To address these risks, our design strategies emphasized the safety of AI-generated content and ensured the availability of original logs for review. Based on prior research~\cite{kerasidouTrustRelianceMedical2022}, we advocate for rigorous provider inspection of AI outputs, especially in high-stakes healthcare contexts. 
Approaches like human-AI annotation for LLM label verification~\cite{wang2024human} could help ensure AI suggestions are properly reviewed and interpreted to avoid misdiagnoses. Ultimately, system designers must recognize that automation does not always equate to better care, and in many cases, human-AI collaboration may be the optimal solution for AI-supported healthcare workflows.



\subsubsection{Privacy and Security Concerns and Privacy-Utility Trade-Off}

As discussed in prior work and by our participants, how an LLM-powered system in healthcare may properly handle sensitive and private information must be addressed~\cite{murdoch2021privacy,cohen2019big}. Existing work has discussed the risks of AI in creating bias among patients or producing hallucination~\cite{choudhury2020role,lee2023benefits,haupt2023ai}
With our provider and patient insights, we highlight that LLM-powered systems must prioritize patient anonymization and proactively warn patients of the associated risks for revealing personal information. Furthermore, we notice a potential \textit{trade-off} between protecting patient privacy and system utility. During user studies, patients unintentionally share more personal data due to the natural, open-ended conversation with LLM CA. Although such detailed patient information may promote system utility through a more customized and personalized conversation experience as well as LLM summaries, more personal information increase privacy and security risks. Thus, system designers should consider the privacy-utility trade-off and adjust balance according to clinical regulations and user acceptance.

\subsection{Expectations for Real-world Deployment}

In response to our evaluation findings, participants expressed a strong interest in the deeper integration of \projectname{} with existing telehealth platforms to enhance the capabilities of RPM. The LLM conversation, visualization and LLM-generated analysis could be made available to patients or their caregivers for review; further, patients and providers may leverage the conversation log to initiate or enrich secure messaging communication or appointments. This suggestion for further patient engagement aligns with the growing trend towards patient ``self-management'' in clinical practice~\cite{salmiharnessing}, emphasizing the importance of patient engagement in their own health processes.

To further tailor the system to individual needs, future systems could include customization and personalization features that consider the progression of a patient’s recovery timeline. Firstly, some EHR data such as current medication, demographics, and side effects or common surgery complications could be integrated to LLM protocols. Also, \projectname{} could dynamically adjust its interaction protocols—altering the frequency and depth of questions based on how far a patient is post-surgery, or vary in communication styles~\cite{wester2024facing}. This adaptive approach could significantly improve patient engagement and adherence to care plans.

Prior to a broader deployment, comprehensive instructional materials for patients and training programs for providers will be essential. These resources will ensure that all users are well-prepared to utilize the system effectively, understanding both its capabilities and limitations. Moreover, extending \projectname{} to other clinical settings and health conditions could enhance its utility and provide valuable insights into its adaptability and scalability.


\subsection{Limitations}
We acknowledge several limitations of our work. Firstly, the design of our system \projectname{} focuses on postoperative GI cancer. Although the system is potentially applicable to other RPM scenarios that involve domain-specific or time-sensitive information collection or review, it may not generalize well to other clinical scenarios such as other cancers that do not require surgical intervention or involve multiple postoperative complications.
Secondly, we acknowledge limitations in our participant selection. Since the system primarily aims to assist clinical work, our study focuses more on clinical staff and less on postoperative GI cancer patients. The small sample size limits our study to a preliminary assessment rather than a rigorous deployment. The recruitment was difficult as many postoperative patients are in vulnerable conditions and prefer to focus on recovery, while clinical staff may lack GI cancer care experience or availability for multiple study sessions. Additionally, some evaluation participants were involved in the formative study, which could introduce positive bias. However, the final design and implementation of \projectname{} were not disclosed before the user study, and our evaluation focused on participants' behavior and attitudes toward the system rather than specific features. In the future, we plan to engage more healthcare professionals and GI cancer patients for further evaluation.
Finally, this work does not involve deployment in a real-world setting. To address these gaps, we are recruiting GI cancer patients prior to their surgery and subsequently deploy the system for their postoperative recovery. This future study aims to evaluate the system's effectiveness from the user's perspective and refine our approach based on direct patient feedback.
