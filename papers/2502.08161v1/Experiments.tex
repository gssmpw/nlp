%To illustrate the effectiveness of DecMix sampling, We conducted experiments on three recommendation task datasets with three mainstream Graph Neural Networks. In addition, to demonstrate the action interval and applicable parameters of DecMix sampling, we conducted ablation studys on the Amazon-Book dataset. 
In this section,  we evaluate the performances of the proposed sampling method on three benchmark datasets with three representative GNN-based recommendation models. Furthermore, to analyze the MixDec Sampling method, we perform the hyperparameter study and the ablation study. Specifically, we intend to address the four research questions listed below:
\begin{itemize}
    \item \textbf{Q1.} Are there improvements in the effect of applying MixDec Sampling to representative GNN-based recommendation models? (See \ref{Q1}).
    \item \textbf{Q2.} Whether the proposed sampling method can continuously and steadily improve the performances when plugged
into existing GNN-based recommendation models? (See \ref{Q2}).
    \item \textbf{Q3.} How does MixDec Sampling method perform for nodes with few neighbors? (See \ref{Q3}).
    \item \textbf{Q4.} How do hyperparameters affect the performance of the proposed method? (See \ref{Q4}).
\end{itemize}
\subsection{Experimental Settings}
%We compare mixed sampling with negative sampling based on the following model: GraphSAGE \cite{graphsage}, GCN \cite{gcn}, GAT \cite{gat}. Moreover, we compare several components of Mix Sampling: Mixup Sampling, Decay Sampling, and MIXDEC Sampling with negative sampling as well. 
\subsubsection{\textbf{Dataset}}
We use three public benchmark datasets, Amazon-Book, Yelp2018, and Last-FM to evaluate our method.
Each dataset contains users, items, and interactions between them. To guarantee the quality of datasets, we only keep users and items with more than ten interactions.

\begin{table}[t]
\centering
\renewcommand\arraystretch{2} 
\caption{Statistics of the datasets.}
\label{tab:dataset}
\resizebox{\linewidth}{!}{
\begin{tabular}{c|cccc}

\hline
Dataset  & \#Users  & \#Items & \#Interactions & Density \\ \hline \hline
Amazon-Book   & 70,679 & 24,915 & 847,733    & 0.000048 \\ \hline
Yelp2018 & 45,919   & 45,538 &  1,185,068     & 0.000566 \\ \hline
Last-FM  & 23,566  & 48,123  &  3,034,796    &  0.002676\\ \hline
\end{tabular}}
\end{table}

% We performed ablation experiments with Mixup Sampling, Decay Sampling and MixDec Sampling modules with Negative Sampling. The studies were subjected to three datasets (\ie, Amazon-Book, Yelp2018, Last-FM) utilizing three typical GNN models (\ie, GraphSAGE, GCN, GAT) in the field of recommendation system. MixDec has enhanced the generalization and accuracy of the models, as calculated by the MRR and Hit@30 metrics.

\begin{table*}[t]
\centering
\renewcommand\arraystretch{2}
\caption{Performance Comparison Results.}
\label{result}

\resizebox{\textwidth}{!}{
\begin{tabular}{c|c|ccc|ccc|ccc}
\hline
\multirow{2}{*}{Evaluation Metrics} & \multirow{2}{*}{Dataset} & \multicolumn{3}{c|}{Amazon-Book} & \multicolumn{3}{c|}{Yelp2018} & \multicolumn{3}{c}{Last-FM} \\
                        &                      & GraphSAGE & GCN & GAT & GraphSAGE & GCN & GAT & GraphSAGE & GCN & GAT \\ \hline \hline
\multirow{5}{*}{MRR} &Uniform Negative Sampling   &0.1821 & 0.1480& 0.1652 & 0.1754 & 0.1682& 0.1872&  0.1627&  0.1409 &0.1834 \\
&Mixup Sampling & 0.1899 & 0.1551& 0.1722 & 0.1805 & 0.1837& \textbf{0.2018}&  0.1705&  0.1466 &0.1820 \\ 

 &Decay Sampling  & 0.1928 & 0.1747& 0.1757 & 0.1814 & \textbf{0.1842}& 0.1926&  \textbf{0.2353}&  \textbf{0.1892} &0.1895 \\ 

 &MixDec Sampling  & \textbf{0.1940} & \textbf{0.1770}& \textbf{0.1777}& 0.1824 & 0.1822 & 0.1972&  0.2280&  0.1840 &\textbf{0.1904} \\

 &\textbf{Improvement}  & \textbf{6.54\%} & \textbf{19.59\%}& \textbf{7.57\%}& \textbf{3.99\%} & \textbf{9.51\%}& \textbf{7.79\%}&  \textbf{44.62\%}&  \textbf{34.27\%}&\textbf{3.81\%} \\\hline
 \multirow{5}{*}{Hit@30} &Uniform Negative Sampling   &0.5535 &0.4956 & 0.5102 & 0.6192 & 0.5923& 0.6884&  0.5524& 0.4992 &0.5956 \\
%  
&Mixup Sampling & 0.5520 & 0.5152 & 0.5450 & 0.5302 & \textbf{0.6370} & 0.7098& 0.5758&  0.5590&  0.5923  \\

 &Decay Sampling  & 0.5653 & 0.5295 & 0.5304 & 0.6307& 0.6334& 0.7057& \textbf{0.6091}&  \textbf{0.5642} & 0.6012 \\

 &MixDec Sampling  & \textbf{0.5720} & \textbf{0.5352} & \textbf{0.5574} & \textbf{0.6340} & 0.6320& \textbf{0.7132}&  0.6078& 0.5596&\textbf{0.6031}\\ 

 &\textbf{Improvement } & \textbf{3.34\%} & \textbf{7.99\%}&\textbf{ 9.25\%}& \textbf{2.39\%} & \textbf{7.54\%}& \textbf{3.60\%}& \textbf{10.26\%}&  \textbf{13.02\%} &\textbf{1.25\%} \\\hline
\end{tabular}%
}
\end{table*}

\begin{itemize}
    \item \textbf{Amazon-Book}\cite{amazon}: Amazon-Book is a widely used dataset for product recommendation.
    \item \textbf{Last-FM}\cite{last-fm}: This is the music listening dataset collected from Last-FM online music systems. We take the subset of the dataset where the timestamp is from Jan, 2015 to June, 2015. 
    \item \textbf{Yelp2018}\cite{yelp}: This dataset is adopted from the 2018 edition of the Yelp challenge. %including local businesses, such as restaurants and bars.
\end{itemize}

We summarize the statistics of the three datasets in Table~\ref{tab:dataset}. 
The table contains the number of  user nodes, item nodes, and interactions in graphs, with the density of graphs. We construct user-item bipartite graphs, the same as \cite{pinsage, wang2019kgat, huang2021mixgcf}.
%The table contains the number of different node types in the graph, the number of interactions and the density of graphs. The graph structure is the same as the user and item node structure in KGAT \cite{wang2019kgat}, and the settings of training set and test set are also the same as KGAT : 
For each dataset, we randomly select 80\% of each user's interaction history as the training set, and use the rest as the test set. From the training set, we randomly select 10\% of the interactions as the validation set to tune the hyperparameters. 
%For each observed user interaction with an item, we treat it as a positive instance and then perform a negative sampling strategy to pair it with a negative item that the user has not consumed before. 
We use observed user-item interactions as positive examples and use Uniform Negative Sampling to sample unconsumed items for users.
On this basis, more soft link sample pairs are sampled using our Mixup Sampling, Decay Sampling, and MixDec Sampling methods, which can clearly demonstrate that the performance improvement comes from our sampling method.

\subsubsection{\textbf{Baseline Models}}

We use three representative GNN-based recommendation models, \ie, GraphSAGE, GCN, and GAT, as the base models for our experiments \cite{wu2020gnnsurvey}, respectively, and standardize on a three-layer structure.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 多次出现还需要引用吗？
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{itemize}
\item \textbf{GraphSage}~\cite{graphsage}
is a framework for inductive representation learning on large graphs. GraphSAGE is used to generate low-dimensional vector representations for nodes and is especially useful for graphs that have rich node attribute information.

\item \textbf{GCN}~\cite{gcn}
is a type of convolutional neural network that can work directly on graphs and take advantage of their structural information.


\item \textbf{GAT}~\cite{gat}
is a neural network architecture that leverages masked self-attentional layers to assign weights to the different nodes of the aggregation process.
\end{itemize}



\subsubsection{\textbf{Evaluation Metrics}}
To evaluate the performance for recommendation task, Hit@K \cite{hitk} and mean reciprocal ranking (MRR) \cite{mrr} serve as evaluation methodologies. 
%To ensure the generalizability of the experimental results, we use the most basic bipartite graph construction of user-item.  
\begin{itemize}
    \item \textbf{MRR} is a metric that evaluates the returned sorted list for recommendation tasks. MRR assigns different scores to different ranks of item v in the ranked list $R$ of the inner product of \(u\) and \(v\), which is defined as follows: 
    \begin{equation}\label{mrr}
        \text{MRR}=\frac{1}{|D_\text{candi}|} \sum_{(u_i,v_i)\in D_\text{candi}}\frac{1}{\text{rank}_{v_i}},
    \end{equation}
    where $\text{rank}_{v_i}$ is the rank of $v_i$ in the ranked list of user-item pair.
    \item \textbf{Hit@K} is a coarse granularity evaluation metric for recommendation task. Hit@K assigns same score to the first K of the ranked list $R$, which is defined as follows:
    \begin{equation}\label{hitk}
        \text{Hit@K}=\frac{\text{\#hit@K}}{|D_\text{candi}|}.
    \end{equation}
\end{itemize}
%For user-item bipartite graphs, the $D_{test}$ is the test set include each user-item pair (u,v), and the settings are the same as \cite{NCE}: For each user-item pair(u, v) in the test set, we randomly select 499 additional items $\{v_0',. . . . . . ,v'_{498}\}$ which are never showed to queried user u. 
The $D_\text{candi}$ is the candidate set of items when predicting the favorite item for each user.
This set consists of one ground truth and 499 randomly selected items from which the user has not interacted.
In our experiment, we set $K$ as 30.





\subsubsection{\textbf{Implementation Details}}
All models are implemented in the PyTorch framework \cite{paszke2019pytorch}, and the Adam optimizer\cite{kingma2014adam} is adopted. Uniform Negative Sampling\cite{negsamp}, Mixup Sampling, Decay Sampling, and MixDec Sampling share an identical implementation of the aggregator, mini-batch iterator, and loss function to provide a fair comparison. For the other basic settings: the embedding dimension of the node is 128, the number of training epochs is 2,500, the number $c$ in Uniform Negative Sampling is 20 and $c_m$ and $c_d$ in MixDec Sampling is 5. Source code is relased in \url{ https://github.com/a2093930/MixDec-Sampling}.

\subsection{Results and Analysis}
We replace the Uniform Negative Sampling with our approaches (\ie, Mixup Sampling, Decay Sampling, and MixDec Sampling) on representative GNN-based recommendation models and evaluate their performance on three recommendation benchmarks.


\subsubsection{\textbf{Performance Comparison (for Q1)}} \label{Q1}
Benchmarks are shown in Table \ref{result} that includes GNN-based recommendation models and the proposed sampling approach implemented. The following are the findings:

\begin{itemize}
\item It's hardly surprising that the Uniform Negative Sampling approach from the GNN model performs the worst in all evaluations, since it simply interprets the relationship between nodes as either hard positive or hard negative pairs.
%\item Our method MixDec Sampling achieves the best performance on most of the metrics demonstrating the effectiveness of our soft link based sampling.
\item Our method is the most effective in terms of all metrics. Graph augmentation with synthetic nodes and links, along with soft link-based sampling, proves its efficacy in this scenario.
\item In most cases, Decay Sampling outperforms Mixup Sampling, demonstrating that it is crucial to maintain structural information during sampling.
\item High-density graphs benefit more from Decay Sampling's performance. Decay Sampling, in particular, improves MRR scores by up to 44.62\% and Hit@30 scores by up to 13.02\% on the Last-FM dataset. Mixup Sampling performs significantly better on graphs with low-density, such as the Amazon-Book dataset and the Yelp2018 dataset.
\end{itemize}

\subsubsection{\textbf{Generalization Ability (for Q2)}}\label{Q2}
Table \ref{result} shows that our strategy consistently outperforms Uniform Negative Sampling on all three GNN-based recommendation models, proving its generalizability.
In comparison to GraphSAGE and GAT, MixDec Sampling on GCN is far superior. This is due to the fact that GCN's preliminary performance on the most of results is the lowest.
Our strategy considerably improves the less effective GNN-based model.


\subsubsection{\textbf{Performance Analysis on Different Density Graphs (for Q3)}}\label{Q3}
In order to evaluate the performance of our method on nodes with few neighbors, we drop edges in the training set in multiple proportions to generate subgraphs with different degrees of sparseness.
We completed this experiment with GCN, we set the dropping ratio to be 0\%, 20\%, 50\%, and 70\% on Amazon-Books for Uniform Negative Sampling, Mixup Sampling and MixDec Sampling.
The results are presented in Table \ref{Px}. The observations are as followed:
 \begin{itemize}
     \item When the dropping ratio is 0\%, 20\% and 50\%, the improvement of Mixup Sampling compared to Uniform Negative Sampling gradually increases with the decrease of graph density.
     Mixup Sampling has more significant improvements in the case of fewer neighbors, which illustrates the effectiveness of Mixup Sampling on nodes with few neighbors.
     \item When the dropping ratio is less than 0.7, the improvement of MixDec Sampling relative to Uniform Negative Sampling gradually increases with the increase of graph density, while Mixup Sampling exhibits the opposite trend. This shows that Decay Sampling performs better in the density graph, which is also reflected in the performance comparison results in Table \ref{result}. Overall, MixDec Sampling still performs significantly better than Uniform Negative Sampling and Mixup Sampling with a large dropping ratio, such as 50\%.
     \item When the dropping ratio is set to 0.7, each user in the graph has an average of 3.59 neighbors. Due to the low density of the graph, the effective information is limited. Therefore, compared to the higher graph density, the improvement of Mixup Sampling and MixDec Sampling becomes less.
 \end{itemize}
%为了实验我们的方法对冷启node的效果，我们采用对边按比例drop的方式，生成不同稀疏程度的子图，从而模拟当节点的邻居较少的情况。
% 为了探究DecMix Sampling 及 Mixup Sampling 在稀疏图和冷启动上的表现，我们对 Amazon-Books进行了不同图密度的子图上做了消融实验，结果展示在表 \ref{Px}中，$Px =\{0.3, 0.5, 0.8, 1\}$是子图与原图边数量之比。我们有以下观察：
% \begin{itemize}
%     \item Mixup Sampling has a more significant boost in the case of fewer samples, which illustrates the effectiveness of Mixup Sampling in the case of cold starts.
%     \item $Px$ 大于0.3时, MixDec Sampling 的相对提升随图密度增加而增加，Mixup Sampling的相对提升随图密度增加而减小。这说明了Decay Sampling在稠密度图表现更好，而Mixup samping适应于稀疏图（冷启）。这验证了我们的motivation。 对于px=0.3我们认为这是由于图密度过低，每个用户平均约3.59条交互，图密度过低无法，有效信息太少，因此Mixup Sampling 和 Mixdec sampling相对提升不明显。
% \end{itemize}

\begin{table}[t]
\centering
\renewcommand\arraystretch{2} 
\caption{ Performance Results on Different Density Graphs}
\label{Px}
\resizebox{\linewidth}{!}{
\begin{tabular}{c|cccc}
\hline
Dropping Ratio & 70\% & 50\% & 20\%  & 0\% \\ \hline \hline
Uniform Negative Sampling& 0.0986  & 0.1145 &0.1304 &0.1480  \\ \hline
Mixup Sampling &0.1034&0.1221& 0.1382 &0.1551\\ 
\textbf{Improvement} &\textbf{4.86\%} & \textbf{6.63\%}& \textbf{5.98\%}&\textbf{4.79\%}\\ \hline
MixDec Sampling & 0.1041& 0.1302  &0.1517&0.1770\\ 
\textbf{Improvement}&  \textbf{5.57\%}& \textbf{13.71\%} & \textbf{16.33\%} &\textbf{19.59\%}\\ \hline
\end{tabular}
}
\end{table}

% last fm    0.5    decay +mix 1431 1525  mix  1430 
%           0.2                 1203
%          0.3                  0.1397
%           0.8                    0.1578         1495

%Table \ref{result} shows that for the two components of MixDec Sampling: Mixup Sampling and Decay Sampling, 
%We find that the improvement effect of MixDec Sampling on the three datasets is negatively correlated with the graph density, and for the minimal density dataset Last-Fm, the improvement effect of DecMix is much better than that of the maximum density dataset Yelp2018. We believe this is because the node distribution is more discrete when the graph density is small, and the determination of the soft link $L$ by Decay sampling can assist model learning greater and make the distribution of similar nodes in the graph more clustered. 

\subsubsection{\textbf{Efficiency Comparison}}

% 这里用 sec 表示秒, 打上千分位符
Take GCN as an example, the comparison results of time consumption for training 2,500 epochs of Uniform Negative Sampling, Mixup Sampling, Decay Sampling and MixDec Sampling are shown in Table \ref{TC}.
Uniform Negative Sampling is the fastest on all datasets, because our three samplings all have to sample negative items.
Among the two essential components of MixDec Sampling, Decay Sampling has the shorter time consumption. 
Due to including Uniform Negative Sampling, Mixup Sampling, and Decay Sampling, MixDec Sampling takes the longest time of all methods.
Overall, MixDec Sampling contributes no more than 30 percent to the total time consumption of Uniform Negative Sampling.
Thus, our method does not increase too much time consumption.
%Take GCN as an example, the time required for training 2,500 epochs of Negative Sampling, Mixup Sampling, Decay Sampling and MixDec Sampling are approximately 5,500 sec, 6,323 sec, 6,180 sec and 6,665 sec for Amazon-Book; 7,863 sec,9,214 sec ,8,124 sec and for Yelp-2018; and 18,050 sec, 20,160 sec, 19,183 sec and 21,220 approximately as for Last-FM. 

%The comparison results of time consumption are shown in Table \ref{TC}. 


\begin{table}[t]
\centering
\renewcommand\arraystretch{2} 
\caption{ Efficiency Comparison}
\label{TC}
\resizebox{\linewidth}{!}{
\begin{tabular}{c|ccc}
\hline
Runtime (sec.) & Amazon-Books & Yelp-2018 &Last-FM  \\ \hline \hline
Uniform Negative Sampling& 5,500 & 7,863 &18,050 \\ \hline
Mixup Sampling &6,323 &9,214 & 20,160 \\ \hline
Decay Sampling &6,180 &8,124 & 19,183 \\ \hline
MixDec Sampling & 6,665 &9,652   &21,220\\ \hline

\end{tabular}
}
\end{table}
\subsection{\textbf{Parameter Study (for Q4)}}\label{Q4}


\subsubsection{\textbf{Impact of Sampling Distribution}}
%In contrast to negative sampling, Mixup Sampling relies on the distribution function.

%\textbf{Fixed sampling parameters:} Our first sampling parameter used is a fixed $\lambda=\{0.5,0.9,0.99,0.1,0.01\}$, the purpose of this approach is to evaluate which $\lambda$ are effective for mix sampling. We found that since all nodes are visible on the test set, and the relationship between nodes that is to be judged, $\lambda$ can achieve better results when it is closer to 0 or 1.

%\subsubsection{$\alpha $ and $ \beta$}

\begin{figure}[t]
    \centering
    \includegraphics[width=1.0\linewidth]{img/beta3.pdf}
    \caption{Distributions of Different $\text{Beta}(\alpha,\beta)$. The x-axis represents the value of $\lambda$, and the y-axis represents the corresponding probability density.}
    \label{fig:beta}
\end{figure}

We performed parameter sensitive experiments for \(\alpha\) and \(\beta\) of Beta distribution on the Amazon-Book dataset utilizing GCN to discover the optimal sampling distribution, and the results are shown in Table \ref{tab:lambda}. 
%The parameters $\alpha$ and $\beta$ determine $\lambda \sim \text{Beta}(\alpha, \beta)$. 
We choose $\alpha$, $\beta$ from $\{0.2,0.5, 0.8, 1, 5, 8\}$. Fig. \ref{fig:beta} describes multiple particular $\text{Beta}(\alpha,\beta)$ distributions to highlight how varied $\alpha$ and $\beta$ impacts the sampling distribution.


\begin{table}[t]
\centering
\renewcommand\arraystretch{2} 
\caption{ Impact of Beta Distribution}
\label{tab:lambda}
\resizebox{\linewidth}{!}{
\begin{tabular}{c|cccccc}
\hline
$\alpha \setminus \beta$& 0.2  & 0.5 & 0.8&1&5&8  \\ \hline \hline
0.2   & 0.1442 & \textbf{0.1570}& 0.1520 &0.1498&0.1419&0.1472   \\ \hline
0.5 &0.1455&0.1445 & 0.1401  &0.1400&0.1543&0.1487\\ \hline
0.8  & 0.1440  &0.1401  & 0.1451  &0.1512&0.1454&0.1502    \\ \hline
1 & 0.1362   & 0.1420  & 0.1518 &0.1504&0.1514&0.1560 \\ \hline
5 & 0.1503 & 0.1346 & 0.1348   &0.1369&0.1551&0.1550 \\ \hline
8  & 0.1466   & 0.1408  & 0.1375   &0.1357&0.1494&0.1522   \\ \hline
\end{tabular}
}
\end{table}



\begin{enumerate}[i.]
    \item (a) When both \(\alpha\) and \(\beta\) are less than 1 (\eg, $\alpha=0.2$, $\beta=0.2$), $\lambda \sim \text{Beta}(\alpha, \beta)$ is concentrated at 0 or 1. (b) When one of the values of \(\alpha\) and \(\beta\)  is greater than 1, \(\lambda\) changes monotonically (\eg, \(\lambda\) decreases monotonically when $\alpha=0.2$, $\beta=5$, and increases monotonically when $\alpha=8$ and $\beta=0.2$). (c) When both \(\alpha\) and \(\beta\)  are greater than 1 (\eg, $\alpha=5$, $\beta=5$), \(\lambda\) is centrally distributed between 0 and 1.
    
    \item As shown in Table \ref{tab:lambda}, $\alpha = 0.2$ and $\beta= 0.5$ achieves the best performance, while the corresponding value of $\lambda$ is around 0 and 1 according to Fig. \ref{fig:beta}. Similar results can be obtained with $\alpha = 0.2$ and $\beta= 0.8$. It illustrate that Mixup works better when $\lambda$ is around 0 or 1, suggesting that Mixup Sampling is more suited for producing small or large soft links.
    \item When $\alpha$ is 1 and $\beta$ is 0.2, most corresponding value of $\lambda$ is around 1, which has a poor performance.  This shows that Mixup Sampling is not suitable for all the $\lambda$ values close to 1.
\end{enumerate}

%  alpha beta 都小于等于1---》lambda 集中于 0  or 1,一个大于，一个小于 ：单调变化 ，都大于集中于0-1间某个值
% 在lambda 集中0 or 1和 在0 -1上单调递减效果更好 ，集中于 1效果差，总体来说beta大一点好 ： Mixup 适合生成 接近1 和postive 信息比较少的软连接




\subsubsection{\textbf{Impact of Decay Parameters}}

To study the contribution of $\rho$ and $k$ to Decay Sampling, we conduct the parameter sensitive experiment on the Amazon-Book dataset with GCN by choosing $\rho$ from $\{0,0.3, 0.5, 0.8, 1\}$ and $k$ from $\{100,200,300,400,500\}$. As shown in Table \ref{decay},  the best result is yielded when $\rho=0.5$ and $k=500$. The observations are as followed:
\begin{itemize}
    \item The parameter $\rho$ is used to project the weights of the soft link to $\left[\rho,1\right]$. If the parameter $\rho$ is set to a large value (\eg, 0.8, 1), the hierarchy between decay nodes will be obscured. If set $\rho$ to a small value (\eg, 0, 0.3), the decay node will not get sufficient positive information.  Experimental results show that the closer the value of $\rho$ is to 0.5, the better the result achieved. 
    \item The parameter $k$ indicates the number of decay items have been selected. When the value of $\rho$ is appropriate, the model performance improves with the increase of $k$~(Due to limited memory, we did not try larger values for $k$). Upon observation, the model performs optimally when $\rho$ is equal to 0.5 and $k$ is around 500. When $k$ and $\rho$ are both small (\eg $\rho=0$, $k=100$), the performance is the worst.
%    \item In short, As a result, treating such items as decay samples is preferable to merely considering them as negative samples. The larger the number of $D$ for various $\rho$, the better the model performs.
\end{itemize}



\begin{table}[t]
\centering
\renewcommand\arraystretch{2    }
\caption{Impact of Decay Parameters}
\label{decay}
% \resizebox{\linewidth}{!}{
\begin{tabular}{c|ccccc}
\hline
$\rho \setminus k$  & 100  & 200 & 300 & 400 &500 \\ \hline \hline
0   & 0.1173 &0.1304 & 0.1310  & 0.1269&0.1256 \\ \hline
0.3 & 0.1396 & 0.1405 &0.1517   & 0.1454&0.1480 \\ \hline
0.5  &0.1492   & 0.1511  & 0.1641 &0.1675& \textbf{0.1747}\\ \hline
0.8 & 0.1433   & 0.1452  & 0.1405& 0.1400&0.1442\\ \hline
1& 0.1450 & 0.1453  & 0.1363   & 0.1389 &0.1403 \\ \hline
\end{tabular}
% }
\end{table}

% ？等实验


% \begin{table*}[t]
% \centering
% \renewcommand\arraystretch{2} 
% \caption{Statistics of the datasets. }
% \label{result}
% \resizebox{\textwidth}{!}{
% \begin{tabular}{c|c|cccc|cccc|cccc}
% \hline
% Evaluation Metrics&Dataset  & & Amazon-Book & & &  & Yelp2018  & & &  &Last-FM & & \\ \hline \hline
%       & & GraphSAGE & GCN& GAT & GraphSAGE& GCN& GAT&  SAGE &  GCN &GAT \\
%  &Negative Sampling   &0.1821 & 0.1457& 0.1652 & 0.1754 & 0.1682& 0.1872&  0.1627&  0.1388 &0.1834 \\
% Mrr&Mixup Sampling & 0.1899 & 0.1530& 0.1722 & 0.1805 & 0.1797& 0.2028&  0.1705&  0.1466 &0.1820 \\ 
% % \cmidrule(r){2-14}
%  &Decay Sampling  & 0.1928 & 0.1747& 0.1757 & GraphSAGE & 0.1842& 0.1670&  \textbf{0.2353}&  \textbf{0.1892} &0.1820 \\ 
% %  \cmidrule(r){2-14}
%  &MixDEC Sampling  & \textbf{0.1940} & \textbf{0.1770}& 0.1777& 0.1569 & 0.1791& GAT&  0.2280&  \textbf{0.1770} &0.1584 \\\hline
%  &Improvement  & \textbf{6. 54\%} & \textbf{21. 5\%}& \textbf{7. 57\%}& \textbf{44. 62\%} & \textbf{27. 52\%}& GAT&  \textbf{44. 62\%}&  GCN &GAT \\\hline
%   &Negative Sampling   &0.5535 &0.4856 & 0.5102 & 0.1686 & GCN& 0.6884&  0.5524&  GCN &0.5956 \\
% %   \cmidrule(r){2-14}
% Hit@30&Mixup Sampling & 0.5520 & 0.5029 & 0.5450 & 0.5302 & 0.6341 & GCN& 0.7098&  0.5590&  0.5173 &0.5950 \\
% % \cmidrule(r){2-14}
%  &Decay Sampling  & 0.5653 & 0.5295 & 0.5304 & GraphSAGE & 0.6334& 0.6842&  0.6091&  0.5642 &0.5568 \\
% %  \cmidrule(r){2-14}
%  &MixDEC Sampling  & \textbf{0.5720} & \textbf{0.5352} & \textbf{0.5574} & 0.5780 & 0.6320& GAT&  0.6078& 0.5596&0.5568 \\\hline
%  &Improvement  & \textbf{3. 34\%} & \textbf{10.2\%}&\textbf{ 9. 25\%}& GraphSAGE & GCN& GAT&  GraphSAGE&  GCN &GAT \\\hline
% \end{tabular}}
% \end{table*}
