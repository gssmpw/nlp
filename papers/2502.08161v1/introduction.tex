%介绍推荐和gnn
With the rapid increase of information on the Internet, recommender systems are widely used in e-commerce, social media, \textit{etc}. They provide\textbf{} users with a personalized flow of information based on their interaction history. Since the data of the recommender system naturally have a graph structure\cite{wu2020gnnsurvey}, a large number of recommendation methods that built based on graph neural network~(GNN) have emerged in recent years, such as GCN \cite{gcn},  GraphSAGE\cite{graphsage}, GAT\cite{gat}, Pinsage \cite{pinsage}, and LightGCN \cite{he2020lightgcn}.

%gnn的基本流程
The typical setup of the GNN-based recommendation method is as follows: 
1) firstly, a graph that reveals the user's interaction behavior is constructed, where each node is a user or an item; 2) subsequently, multiple propagation layers are applied to learn the user and item embeddings. Each propagation layer aggregates neighbor features of a current node to derive its embedding; 3) finally, the overall pipeline is supervised by a loss function that aims to pull embeddings of positive user-item pairs to be closer, while pushing embeddings of negative pairs away.


%现有的negative sampling方法
%As negative sampling plays an important role in optimizing the affinity between user and item under the GNN-based methods, some negative sampling methods are proposed recently.
%For example, Bayesian Personalized Ranking \cite{rendle2012bpr} proposes unified sampling negative sample.
Negative sampling plays an important role in optimizing the affinity between user and item under the GNN-based methods.
Commonly, a uniform distribution is used for negative sampling \cite{he2020lightgcn, rendle2012bpr}.
To improve the effect, some negative sampling methods are recently proposed.
For example, PinSage \cite{pinsage} and DNS \cite{dns} focused on selecting hard negative samples for improving the ability of dealing with difficult samples. 
Generative adversarial network~(GAN) based methods, such as IRGAN \cite{wang2017irgan}, AdvIR \cite{park2019adversarial}, KBGAN \cite{cai2017kbgan}, generated negative samples through GAN.


%存在的问题 
%Negative sampling\cite{negsamp} is an important component of graph representation learning, which was first proposed to deal with tasks related to natural language processing (NLP). This is an efficient way to compute partition functions for unnormalized distributions to assist training word2vec\cite{word2vec}.

%The distributed representation of nodes in vector space helps the learning algorithm to obtain better performance in graph representation learning by clustering the connected nodes. In tasks related to graph representation learning, the mainstream graph representation learning algorithms include traditional network embedding methods (\eg, DeepWalk \cite{perozzi2014deepwalk}, LINE \cite{tang2015line}) and graph neural networks (\eg, GCN \cite{gcn}, GraphSAGE \cite{graphsage}) that transform nodes in the graph into low-dimensional embedding to optimize related tasks, such as node classification \cite{perozzi2014deepwalk}, link prediction \cite{linkpre}, and recommendation \cite{recom}.
% 介绍图表示学习
%Most graph representations learn to use the Sampled Noise Contrast Estimation (SampledNCE)\cite{NCE} framework by trainable encoders, positive sampler and negative sampler, to sample any given node positively and negatively, respectively. The encoders are trained by positive sampling and negative sampling and the embedding of the nodes in the graph is generated using the corresponding encoders.
% 介绍负采样和问题

Despite the success of existing negative sampling methods, some limitations still exist. 
%Firstly, a node in a graph is typically connected by multi-hop neighbors, thus each pair of nodes naturally forms a soft proximity relationship. However, existing methods select negative samples simply based on the direct interaction between two nodes, leading to the loss of structural information.
Firstly, in these methods, the relationship between two nodes is simply treated as either hard positive pairs or hard negative pairs.
However, nodes in a graph are typically connected by multi-hop neighbors, thus each pair of nodes naturally forms a soft proximity relationship.  Degenerating the original soft proximity relationship between nodes into a binary classification problem will obviously lead to the loss of structural information.
%Negative sampling restricts the relationship between two nodes to positive and negative pairs.
%However, for the graph structure, nodes are connected by multi-hop neighbors, which naturally form a network structure. Degenerating the original soft proximity relationship between nodes into a binary classification task will obviously lead to the loss of structural information.
In addition, most of the existing negative sampling methods \cite{pinsage, dns, he2020lightgcn, wang2017irgan} mainly focused on sampling for constructing negative pairs, while lacking the generation of positive sample pairs. 
This may result in inadequate training of nodes with few neighbors.
% , since there are not enough neighbors for building positive pairs.
% This may lead to insufficient training of nodes with few neighbors due to lacking sufficient neighbors for constructing positive pairs.
% However, for each node, it is not reasonable to simply mark all other nodes as negative samples. Especially for less linked nodes, this makes it impossible to present a hierarchical distribution of other nodes for the embedding of these nodes. For example, the node's closer neighbors should get more positive information in the negative sampling process than its more distant neighbors. And nodes with different degrees may also represent different degrees of linkage when presented as positive and negative samples. The existing methods mainly cope with the above problem by different sampling strategies, but this does not change the relationship representation strategy between nodes in principle.
%However, for the process of negative sampling, First, it ignores the structure of the graph and simply simplifies the node pairs as positive or negative. Second, most of the modifications to the negative sampling are focused on the construction of difficult negative samples, while the construction of positive samples is neglected, which makes the distance between nodes in the graph not well represented.
% 问题
%In consideration of the above issues, this inspires us whether we should simply represent the relationships between nodes in positive or negative? is there a more accurate representation? how to ensure the correctness of finer representations and how to achieve this finer representation? And if a more accurate representation of the relationship between nodes can be obtained, whether the loss function in the current training needs to be changed?

% 思想来源
%Recently, Mixup \cite{mixup} and its variants \cite{manifold} propose to augment samples based on linear interpolation. They synthesize samples with soft labels through interpolation of samples, which have been theoretically and empirically demonstrated as an effective data augmentation paradigm that can improve the generalization and robustness of deep neural networks in image recognition \cite{mixup,manifold,zhang2020does}, natural language processing \cite{wordmixup,seqmix,guo2020nonlinear} ,  graph classification \cite{guo2021ifmixup,G-Mixup,nandgmixup}, and node classification \cite{wu2021graphmixup,nandgmixup}.


% 方法
 To overcome the above limitations, we propose a novel sampling method, namely \textbf{MixDec Sampling}, for GNN-based recommender systems~(see Fig. \ref{fig:arc} for an overview).
 \emph{MixDec Sampling} consists of Mixup Sampling module and Decay Sampling module. Firstly, the \textbf{Mixup Sampling} module augments node features of a graph by synthesizing new nodes and soft links.  It linearly mixes the features of positive samples and negative samples of each anchored node based on the Beta distribution, and fuses their links accordingly. Thus, nodes with few neighbors can be trained with more sufficient number of sample pairs. The \textbf{Decay Sampling} module generates soft links for each pair of nodes to strengthen the digestion of graph structure information in node embedding learning. The weights of links between nodes decay with their distance in a Breadth First Search (BFS) manner. In this way, the 
structural relationships between nodes are enriched. The Decay Sampling module cooperates with the Mixup Sampling module to boost the central idea of GNN---fusing features of neighbor nodes, meanwhile, utilizing the graph structure information.

%MixDec incorporates the structural information into the node embedding learning via  a soft link mechanism, and synthesizes new nodes and links to improve the cold-start node embedding learning. 
%Specifically, our MixDec boost the  central idea of GNN---fusing node features, meanwhile, considering the graph structure information---by designing three strategies:
%1) \emph{Mixup Sampling} that focuses on augmenting the graph by generating synthetic nodes and links. It linearly mixes the features of positive samples and negative samples of each anchored node based on the Beta distribution, and fuses their links accordingly.
%2) \emph{Decay Sampling} that strengthens the digestion of graph structure information by generating soft links for node embedding learning. The weights of links between nodes decay with their distance in a breadth-first search.
%3) \emph{MixDec Sampling} is the joint sampling of Mixup Sampling and Decay Sampling.
%It preserves the node link weights from Decay Sampling and uses the Mixup Sampling method to synthesize nodes and links.

%Inspired by Mixup, we propose a soft link-based sampling method \textbf{MixDec} for GNN-based recommender systems, which generates synthetic soft links between real nodes and synthetic nodes.
%Instead of using positive and negative sample pairs, MixDec provides a soft link to measure the proximity relationship between nodes, and the synthesized nodes also provide data augmentation for cold-start nodes.
%Since GNN focuses on fusing node features and graph structures, MixDec designs three strategies: 
%1) \emph{Mixup Sampling} focuses on node features to generate synthetic nodes and links. It linearly mixes the features of the positive samples and negative samples of the current node based on the Beta distribution, and fuses their links accordingly.
%2) \emph{Decay Sampling} pays attention to the graph structure information to generate soft links. The link values between nodes decay according to the distance of the Breadth First Search (BFS).
%3) \emph{MixDec Sampling} is the joint sampling of Mixup and Decay.
%It preserves the node link values from Decay Sampling and uses the Mixup Sampling method to synthesize nodes and links.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% For presentation, use this one.

% \begin{figure*}
%     \centering
    
%     \includegraphics[width=\linewidth]{img/arc.pdf}
%     \caption{An overview of the MixDec sampling. MixDec consists of Mixup Sampling and Decay Sampling. 1) Based on sampled positive and negative items for the anchored user, Mixup Sampling linearly mixes $<$positive item, positive item$>$ pairs and $<$positive item, negative item$>$ pairs respectively to generate synthetic items. Then, The soft link between synthetic items and user is created. 2) In Decay sampling, the decay item is the neighbor within k-hop for the anchored user. A soft link between $<$user, decay item$>$ will be built.  In MixDec, Mixup Samples and Decay Samples will be integrated.}
%     \label{fig:arc}
% \end{figure*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{figure*}
    \centering
    
    \includegraphics[width=\linewidth]{img/arc_ugly.pdf}
    \caption{An overview of the MixDec sampling. MixDec consists of Mixup Sampling and Decay Sampling. 1) Based on sampled positive and negative items for the anchored user, Mixup Sampling linearly mixes $<$positive item, positive item$>$ pairs and $<$positive item, negative item$>$ pairs respectively to generate synthetic items. Then, the soft link is created between synthetic items and the user. 2) In Decay sampling, the decay item is the neighbor within the 
   l-hop for the anchored user. A soft link between $<$user, decay item$>$ will be built. In MixDec, Mixup Samples and Decay Samples will be integrated.}
    \label{fig:arc}
\end{figure*}
%Fig. \ref{fig:arc} shows an overview of DecMix Sampling, in which the sampling of each user node is from four types: positive, decay, negative and mixup, and the node relationships is represented by soft links.

%This inspired us that linear interpolation can introduce synthetic samples and corresponding soft links without perturbing the original positive and negative links in the graph. We also consider for the links in the graph, the soft links are set according to the distance between the nodes.
%In this paper, we propose DecMix Sampling, which is a sampling method enables a more accurate representation of the links between nodes by turning the hard links between nodes into soft links and It is consisted of Mixup Sampling and Decay Sampling together.
%In the first part, we propose \textbf{Mixup sampling}, which Mixup the positive and negative samples of each node to construct synthetic samples and corresponding soft links. 
%Specifically, we first transform the links between nodes from hard links (positive or negative) to soft links. Second, we construct mixed samples and soft links by sampling negative samples for each positive node pair and mixup the link. Finally, We synthesize the mixed samples based on the appropriate Beta distribution.

%In the second part, in order to further accurate the links between nodes, we also propose \textbf{Decay Sampling}, a sampling method in which the link values of all node pairs in the entire graph structure are set based on distance. In this sampling method, the link values between nodes decay according to the distance of the Breadth First Search (BFS). It is different from the negative sampling, where visible interactions are positive instances and invisible interactions or randomly sampled instances are negative instances. In the specific implementation, we perform BFS of each node with depth K, which is the number of layers of the graph neural network aggregation, and design a searchable table of link values for the nodes in the graph based on the distance and the decayed ranking.

%In the last part, we synthesize Mixup Sampling and Deacy Sampling into \textbf{MixDec Sampling}, and in MixDec Sampling, we preserve the node link values from the Decay Sampling and synthesizing the nodes and links using the Mixup Sampling method to jointly optimize the model.


%实验
To evaluate the effectiveness of our method, we conduct experiments by applying MixDec to representative GNN-based recommendation models (\eg GraphSAGE \cite{graphsage}, GCN \cite{gcn}, GAT \cite{gat}) and evaluate their performance on three recommendation benchmarks: Amazon-Book\footnote{Amazon-book, 2015. \url{http://jmcauley.ucsd.edu/data/amazon}}, Last-FM\footnote{Last-fm, 2015. \url{https://grouplens.org/datasets/hetrec-2011}}, and Yelp2018\footnote{Yelp, 2015. \url{https://www.yelp.com/dataset}}. 
The experimental results show that GNN-based models equipped with MixDec sampling method significantly and consistently outperform those equipped with the default negative sampling on various benchmarks.
In specific, we achieve average increases of 18.3\% for GraphSAGE and 21.1\% for GCN in terms of Mean Reciprocal Rank(MRR).
%The experimental results demonstrate that by replacing the original default negative sampling with our three strategies Mixup Sampling, Decay Samping, and MixDec Sampling, the three typical GNN-based recommendation models have consistently and significantly improved, such as average increase of 18\% for GraphSAGE and 27.7\% for GCN in term of MRR. In addition, experiments show that most results of MixDec Sampling are improved over Mixup Sampling and Decay Samping.


In summary, our work has the following contributions:
\begin{enumerate}
    \item To the best of our knowledge, we are the first to model relationships between nodes by soft links for sampling in GNN-based recommender systems, rather than 
    simply treats the relationship between nodes as either hard positive pairs or hard negative pairs. %rather than just focusing on how to sample positive and negative pairs.
    \item We propose a novel sampling method, namely MixDec Sampling, for training GNN-based recommendation models. The proposed MixDec Sampling is able to boost the ability in learning meaningful embeddings from features and structural information of a user-item graph.
    \item Moreover, the proposed MixDec can be naturally plugged into existing GNN-based recommendation models.
    %We propose the MixDec method to generate synthetic nodes and soft edges between nodes from two perspectives of node feature and graph structure. Our method can be naturally plugged into existing gnn-based recommendation models.
    \item We empirically demonstrate that the proposed MixDec Sampling can significantly and consistently improve the recommendation quality of several popular GNN-based models on various recommendation benchmarks.
    %Comprehensive empirical studies demonstrate significant improvements on three typical recommender datasets and consistent improvements across multiple representative GNN models to demonstrate the effectiveness of the proposed method.
\end{enumerate}