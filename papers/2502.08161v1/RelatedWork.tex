% In this section, we will first review graph neural networks (GNNs) briefly and introduce its application in link prediction tasks. Then, we will present the current prevalent approaches of negative sampling in GNNs and their limitations.
In this section, we first provide a quick overview of GNNs and discussing their application in Recommendation tasks. Then, we introduce the current prevalent methods to negative sampling in GNNs, and their limitations. Finally, we introduce the data augmentation method of linear interpolation represented by Mixup \cite{mixup} and make a summary.

\subsection{GNNs for Recommendation}
In recent years, there has been a surge of research interest in developing varieties of GNNs, specialized deep learning architectures for dealing with graph-structured data. GNNs leverage the structure of the data as computational graph, allowing the information to propagate across the edges of graphs. A GNN usually consists of 1) graph convolution layers which extract local substructure features for individual nodes, and 2) a graph aggregation layer which aggregates node-level features into a graph-level feature vector.
Kipf \etal \cite{gcn} designed GCN by approximating localized 1-order spectral convolution.
Hamilton \etal \cite{graphsage} subsequently improved GCN which alleviate the receptive field expansion by sampling neighbors.
FastGCN \cite{fastgcn} further improved the sampling algorithm and adopts importance sampling in each layer.

Link prediction aims to find missing links or predict the likelihood of future links. Most of the contemporary approaches of link prediction focus on homogeneous networks where the object and the link are of single (same) types such as author collaboration networks. These networks comprise less information which may causes less accuracy for the prediction task. In heterogeneous networks, the underlying assumption of a single type of object and links does not hold good. Such networks contain different types of objects as well as links that carry more information compared to homogeneous networks and hence more fruitful to link prediction.

Huang \etal \cite{chen2005link} and Li \etal \cite{li2014recommendation} proposed approaches, where the recommender system (user–item recommendation) is represented as a bipartite graph, and employed basic link prediction approaches for the items recommendation. Sun \etal \cite{sun2009rankclus, sun2009ranking} coined the concept of heterogeneous information network (HIN) and subsequently meta path concept \cite{sun2011pathsim}, since then it becomes popular among researchers. Yang \etal \cite{yang2012predicting} proposed a new topological feature, namely multi-relational influence propagation to capture the correlation between different types of links and further incorporate temporal features to improve link prediction accuracy. Davis \etal \cite{davis2011multi} proposed a novel probabilistic framework. Their approach is based on the idea that the non-existing node pair forms a partial triad with their common neighbor, and their probabilistic weight is based on such triad census. Then the prediction score is computed for each link type by adding such weights.


\subsection{Negative Sampling}
Negative sampling is firstly proposed to speed up skip-gram training in Word2Vec \cite{word2vec}. Negative sampling has a significant impact on the performance of networks in GNN-based tasks, such as recommender systems and link prediction. As an example, for the link prediction and user recommendation scenarios, the recommendation model relies mostly on historical feedback from users to model their preferences. We learn the representation of users and items by feeding both positive and negative samples. In general, the interaction between users and items is treated as a set of implicit feedbacks. With implicit feedback, the database is not explicitly labeled, thus we have generally assumed all items that have interacted with the user are positive samples, and vice versa are negative samples. The approach to select negative samples can be broadly classified into heuristic negative sampling and model-based negative sampling.

Heuristic negative sampling focuses on sampling by setting some heuristic rules. Bayesian Personalized Ranking (BPR) \cite{rendle2012bpr} with negative sampling by uniform distribution over equal probabilities. The BPR algorithm has straightforward strategy and avoids introducing new biases in the sampling process, which is a widely used method. Popularity-biased Negative Sampling (PNS) \cite{chen2017sampling} applied the popularity of an item as the sampling weight, with the more popular the item, the more likely it is to be selected. The PNS increased the informativeness during sampling, but as its sampling distribution is calculated in advance, the sampling distribution does not change accordingly in the model training process \cite{rendle2014improving}. Therefore, the informativeness from negative sampling will decrease after several training sessions.

Model-based negative sampling aggregates the structural information of the model and obtains more high-quality negative samples. Dynamically Negative Sampling (DNS) \cite{zhang2013optimizing} dynamically changed the sampling distribution depending on the network, and the highest rated ones in the model were selected as negative samples for network training each time. MixGCF \cite{huang2021mixgcf} was a hop-wise sampling method which samples the representations of each hop among the negative. Besides, MixGCF applied the positive mixing to improve the quality of negative candidates. IRGAN \cite{wang2017irgan} used the idea of GAN for the first time in the field of information retrieval to perform negative sampling. The sampler performs as a generator and samples the negative to confuse the recommender. Then, \cite{ding2019reinforced, park2019adversarial} optimizing and improving the IRGAN in terms of efficiency and performance. To address the problem of inability to distinguish between the false negative and the hard negative when sampling, SRNS \cite{ding2020simplify} utilized the observed statistical features as a priori knowledge to separate the false negative and the hard negative, with a DNS-like structure for sampling to ensure the sampling quality and the robustness of the network.

\subsection{Mixup}

Interpolation-based data augmentation was proposed in Mixup \cite{mixup}. Mixup extends the training data by training a neural network on convex combinations of pairs of examples and their labels. Mixup has achieved relative success in many computer vision tasks. Mixup variants\cite{manifold,wordmixup,xie2022global} used interpolation in the hidden representation to capture higher-level information and obtain smoother decision boundaries. Recently, more researchers have focused on utilizing Mixup to improve the model’s performance in tasks with GNNs. GraphMixup\cite{wu2021graphmixup} presents a mixup-based framework for improving class-imbalanced node classification on graphs. MixGCF \cite{huang2021mixgcf} design the hop mixing technique to synthesize hard negatives. For graph classification, \cite{guo2021ifmixup} propose a simple input mixing schema for Mixup on graph, coined ifMixup, and they theoretically prove that, ifMixup guarantees that the mixed graphs are manifold intrusion free. G-Mixup augment graphs for graph classification by interpolating the generator (\ie, graphon) of different classes of graphs.

Mix sampling is inspired by mixup and negative sampling, converting nodal relations into soft relations and optimizing the model by linear interpolation and $\text{Beta}(\alpha,\beta)$ distribution.