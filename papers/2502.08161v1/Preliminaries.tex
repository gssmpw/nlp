In this section,  we introduce related concepts including GNN for recommendation, negative sampling, and Mixup \cite{mixup}.

\subsection{Graph Neural Networks for Recommendation}
 Recommendation is the most important technology in many e-commerce platforms, which has evolved from collaborative filtering to graph-based models. Graph-based recommendation represents all users and items by embedding and recommending items with maximum similarity score (by a inner product operation) for a given user. Here, we briefly describe the pipeline of GNN-based representation learning, including aggregation and optimization with negative sampling.

GNNs learn distributed vectors of nodes by leveraging node features and the graph structure. 
The neighborhood aggregation follows the ``message passing'' mechanism, which iteratively updates a node's embedding $h$ by aggregating the embeddings of its neighbors. Formally, the embedding $h_i^l$ of node $i$ in the $l$-th layer of GNN is defined as:
% GNNs use graph structures and node features to learn distributed vectors to represent graph information. Learning follows the "message passing" mechanism of neighborhood aggregation by iteratively updating a node's embedding $h$ by aggregating the embeddings of its neighbors. Formally, the representation $h_k^i$ of node $i$ in the $k$th layer of GNN is defined as:
 \begin{equation} \label{agg}
     \small
     h_i^l= \sigma\left(\text{AGG}\left(h_{i}^{l-1}, h_{j}^{l-1} \mid j \in N_{(i)},W_l\right)\right),
 \end{equation}
where the \(\sigma\) is activation function, $W_l$ denotes the trainable weights at layer l, $N_{(i)}$ denotes all nodes adjacent to $i$, $\text{AGG}$ is an aggregation function implemented by specific GNN model (\eg GraphSAGE, GCN, GAT, \etc), and $h_i^0$ is typically initialized as the input node feature $v_i$.

 
\subsection{Negative Sampling}

Negative sampling \cite{negsamp} is firstly proposed to serve as a simplified version of Noise Contrastive Estimation\cite{NCE}, which is an efficient way to compute the partition function of an unnormalized distribution to accelerate the training of Word2Vec\cite{word2vec}. The GNN has different non-Euclidean encoder layers with the following negative sampling objective:

\begin{equation}\label{negsampling}
    \mathcal{L} = \log(\sigma (e_{v_i}^Te_{v_p}))+\sum^{c}_{j=1}\mathbb{E}_{v_j\sim P_n(v)}  \log(1-\sigma (e_{v_i}^Te_{v_j})),
\end{equation}
where $v_i$ is a node in the graph, $v_p$ is sampled from the positive distribution of node $v_i$, $v_j$ is sampled from the negative distribution of node $v_i$, $e$ represents the embedding of the node, $\sigma$ represents the sigmoid function, $c$ represents the number of negative samples for each positive sample pair. 
%So Negative Sampling are free to simplify NCE as long as the vector representations retain their quality, it is an effective method to calculate the partition function of unnormalized distribution.


\subsection{Mixup}


\textbf{Mixup\cite{mixup}} is an simple yet effective data augmentation method that is originally proposed for image classification tasks. 
Mathematically, let $(x, y)$ denotes a sample of training data, where $x$ is the raw input samples and $y$ represents the one-hot label of $x$, the Mixup generates synthetic training samples $(\tilde{x}, \tilde{y})$  as follows:
\begin{equation}
% \vspace{-0.2cm}
\begin{split}
% \setlength\abovedisplayskip{0cm}
& \tilde{x}=\lambda x_{i}+(1-\lambda) x_{j}, \\
% \setlength\belowdisplayskip{0cm}
% \setlength\abovedisplayskip{0cm}
& \tilde{y}=\lambda y_{i}+(1-\lambda) y_{j}. \\
% \setlength\belowdisplayskip{0cm}
\end{split}
% \vspace{-0.2cm}
\end{equation}
It generates new samples by using linear interpolations to mix different images and their labels.