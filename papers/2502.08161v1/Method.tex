%In this section,  We introduced negative sampling in \{PRELIMINARIES\} \ref{prel}, how GNN is unified into the negative sampling framework and Mixup. 
%Then we introduce the specific methods in \{MixDec Sampling\} \ref{MixS}, which contains three parts: Mixup Sampling, Decay Sampling, MixDec Sampling. Finally, we explore other alternative components in the MixDec method in \{Sampling distribution and loss function \} \ref{other}.

%  \begin{figure*}
% \centering

% \includegraphics[angle=0, width=\textwidth]{img/architecture.pdf} 
% \caption{The network architecture. MixDec Sampling is divided into two parts, Decay sampling and Mix sampling. In the Decay sampling phase, the network starts from user \(u\) and finds the nodes directly connected to \(u\) as positive items. The next connected item from these positive samples are the decay items, while the rest of the items in the graph are negative items. In the Mix sampling phase, we aggregate the positive items, decay item and negative items to get Mixup decay items.} 
% \label{fig:arch} 
% \end{figure*}

% \subsection{\textbf{PRELIMINARIES}}\label{prel}

% \subsubsection{Graph Neural Networks for Recommendation:}
%  Recommendation is the most important technology in many e-commerce platforms, which has evolved from collaborative filtering to graph-based models. Graph-based recommendation represent all users and items by embedding and recommending items with maximum inner product for a given user.Then we briefly describe the process of graph neural network representation learning, including aggregation and optimization with negative sampling.
 
%  \subsubsection{Aggregation and pooling}
%  GNNs use graph structures and node features to learn distributed vectors to represent graph information. Learning follows the "message passing" mechanism of neighborhood aggregation by iteratively updating a node's embedding $h$ by aggregating the embeddings of its neighbors. Formally, the representation $h_k^i$ of node $i$ in the $k$th layer of GNN is defined as:
%  \begin{equation}
%      \small
%      h_i^k= \text{LeakyReLU}(AGG(h_{i}^{k-1}, h_{j}^{k-1} \mid j \in N_{(i),W_k})
%  \end{equation}
 
%   where we set the activation function set as LeakyReLU \cite{leaky}; $W_k$ denotes the trainable weights at layer k, $N_(i)$ denotes all nodes adjacent to $i$, and $AGG$ is an aggregation function implemented by specific GNN model (\eg GraphSAGE, GCN). And $h_i^0$ is typically initialized as the input node feature $v_i$.

 
% \subsubsection{Negative Sampling}

% Negative sampling \cite{negsamp} is firstly proposed to serve as a simplified version of Noise Contrastive Estimation (NCE) \cite{NCE}, which is an efficient way to compute the partition function of an unnormalized distribution to accelerate the training of word2vec\cite{word2vec}. The graph neural network has different non-Euclidean encoder layers with the following corresponding negative sampling objective:

% \begin{equation}\label{negsampling}
%     \mathcal{L}=\frac{1}{T}\sum_{i=1}^{T} log(\sigma (e_V^Te_P))+\frac{1}{T}\sum_{i=1}^{KT} log(1-\sigma (e_V^Te_N))
% \end{equation}
% where $V$ is a node in the graph, P is sampled from the positive distribution of node $V$, and $N$ is sampled from the negative distribution of node $V$. $e$ represents the embedding of the node, $\sigma$ represents the sigmoid function, $
% K$ represents the number of negative samples for each positive sample pair, and $T$ represents the total number of positive samples of the node. So Negative Sampling are free to simplify NCE as long as the vector representations retain their quality, it is an effective method to calculate the partition function of unnormalized distribution.


% \subsubsection{Mixup}


% \textbf{Mixup\cite{mixup}} is the first data augmentation method proposed for image classification tasks that implements linear interpolations to mix different images and their labels to generate new samples in order to train models to recognize image features and classification in complex situations, it approximates to adding noise to the image.
% In short, let $(x, y)$ denote a sample of training data, where $x$ is the raw input samples and $y$ represents the one-hot label of $x$, the Mixup generates synthetic training samples $(\tilde{x}, \tilde{y})$ can be formulated as follows:
% \begin{equation}
% % \vspace{-0.2cm}
% \begin{split}
% % \setlength\abovedisplayskip{0cm}
% & \tilde{x}=\lambda x_{i}+(1-\lambda) x_{j}, \\
% % \setlength\belowdisplayskip{0cm}
% % \setlength\abovedisplayskip{0cm}
% & \tilde{y}=\lambda y_{i}+(1-\lambda) y_{j}, \\
% % \setlength\belowdisplayskip{0cm}
% \end{split}
% % \vspace{-0.2cm}
% \end{equation}


In this section, we propose MixDec Sampling, a soft link-based sampling method for GNN-based recommender systems. It can be plugged into existing GNN-based recommendation models, such as GraphSAGE, GCN and GAT.

An overview workflow of the MixDec Samping is illustrated in Fig. \ref{fig:arc}. Our MixDec Sampling boosts the GNN-based recommendation models in two aspects: (i) augmenting node features of a user-item graph by synthesizing new nodes and soft links, thus nodes with few neighbors can be trained with more sufficient number of samples; (ii) modeling the sampling relationships between nodes through soft links, where each link is weighted according to their distance in a Breadth First Search (BFS) manner. In this way, the richness of relationships between nodes is increased.
%Instead of simply treating the relationship between nodes as either hard positive pairs or hard negative pairs, MixDec Sampling incorporates the structural information into the node embedding learning via a soft link mechanism, and synthesizes new nodes and links to improve the cold-started node embedding learning. 
%We present an overview of the MixDec Sampling in Fig \ref{fig:arc}. 
%Specifically, MixDec Sampling consists of a Mixup Sampling module and a Decay Sampling module.
%\emph{Mixup Sampling} that focuses on augmenting the graph by generating synthetic nodes and links. It linearly mixes the features of positive samples and negative samples of each anchored node based on the Beta distribution, and fuses their links accordingly.
%\emph{Decay Sampling} that strengthens the digestion of graph structure information by generating soft links for node embedding learning. The weights of links between nodes decay with their distance in a breadth-first search.
%MixDec Sampling is the joint sampling of Mixup Sampling and Decay Sampling.

%In conventional negative sampling, the relationship between two nodes are treat as either hard positive pairs or hard negative pairs by whether their interaction is. If there is interaction between nodes, their link weight is 1, and vice versa is 0. In MixDec Sampling, We first transform the links between nodes into soft links with weights between 0 and 1, and then use different sampling strategies to derive a more accurate representation of the soft links.
% 解释图片

%The purpose of MixDec sampling is to design a more accurate representation of the links in order to more accurately represent the distances and relationships between nodes.


\subsection{\textbf{Mixup Sampling}}

\begin{figure}[t]
    \centering
    
    % \includegraphics[width=.7\linewidth]{img/mixup_sample.pdf}
    \includegraphics[width=.8\linewidth]{img/new_mixup_sample.pdf}
    \caption{An illustration of the Mixup sampling. User \(U\) is the anchored node.  Due to directly connected to user \(U\),  \(P_1\) and \(P_2\) are positive samples. N is a negative sample. \(M_1\) and \(M_2\) are synthetic items, generated by mixing the feature of $<P_1, P_2>$ pair and $<P_1, N>$ pair according to the parameter \(\lambda_1\) and \(\lambda_2\), respectively. Then, we create a soft link with weight 1 between $<U, M_1>$ pair, and another soft link with weight $1-\lambda_2$ between $<U, M_2>$ pair. }
    \label{fig:mix_sam}
\end{figure}

To improve the embedding learning of nodes with few neighbors, we perform data augmentation on the node features in the graph by generating synthetic nodes and soft links. Mixup \cite{mixup} is a linear interpolation based data augmentation method with soft labels, which has been theoretically and empirically demonstrated the generalization and robustness.
Inspired by Mixup, we linearly mixes the features of positive and negative samples of each anchored node, and fuses their links accordingly.
 

For each anchored node in the graph, 
we set the weight of positively sampled nodes to one, and the weight of negatively sampled nodes to zero.
%we assign nodes sampled from positive links weighted by one and nodes from negative links weighted by zero.
%Specifically, for each anchored node in the graph, the positive and negative sampled nodes are transformed into link weighted by 1 and 0, respectively.
Then, we apply Mixup to positive and negative pairs to generate new pairs.
Specifically, each generated pair can be composed of two positive samples, or a positive sample and a negative sample.
The synthetic node and its link to the anchored node are obtained by Beta$(\alpha, \beta)$-based linear interpolation of the sampled nodes and weights of links, which is formalized as:

\begin{equation}\label{mix}
\begin{array}{c}
e_{s} = \lambda e_{i}+(1-\lambda) e_{j}, \\
w_{s} = \lambda w_{i}+(1-\lambda)w_{j}, \\
\end{array}
\end{equation}
where $e$ denotes the embedding of the node, $w$ represents the weight of the link between the anchored node and the another node. $s$ denotes the synthetic node, $i$ and $j$ denote the nodes obtained by positive and negative sampling of the anchored node and $\lambda \sim \text{Beta}(\alpha, \beta)$. An illustration of Mixup Sampling as shown in Fig. \ref{fig:mix_sam}. 


The basic negative sampling loss $\mathcal{L}_{ns}$ is as follows:
\begin{equation}
\small
\mathcal{L}_{ns}=\frac{1}{N}\sum_{(v,n) \in\mathcal{O}} \log\left(\sigma\left ( e_v^T e_{n^+} \right )\right ) + \log\left(\sigma\left (-e_v^T e_{n^-}\right )\right ),
\end{equation}
where $N$ is the number of nodes in the graph, $ \mathcal{O} =\{(v,n) | (v,n^{+})\in \mathcal{R^{+}}, (v,n^{-})\in \mathcal{R^{-}}\} $ denotes the training set, \(\mathcal{R^{+}}\) indicates the observed~(positive) interactions between the anchored node $v$ and sampled node $n$, \(\mathcal{R^{-}}\) is the unobserved~(negative) interaction set. The loss of Mixup Sampling is then as follows:

\begin{equation}
\mathcal{L}_{m}=\frac{1}{N}\sum_{(v,s) \in\mathcal{O}_m} g\left(\sigma\left(e_v^T e_{s}\right), w_s\right),
\end{equation}
where $g$ is a loss function, $ \mathcal{O}_m$ denotes the Mixup training set,  which is the Mix interaction by (\ref{mix}).
%$K$ and $K_{m}$ are the number of negative and mixed sampled for each positive interactions.

For Mixup Sampling, the complete objective for optimization is defined as:
\begin{equation}
\begin{array}{c}

\mathcal{L}_{mix}=\mathcal{L}_{ns}+ \mathcal{L}_{m}.

\end{array}
\end{equation}




%It is worth noting that in the fields of computer vision~(CV) and natural language processing~(NLP), instances in the test set are not visible in the training phase. However, in recommender systems, the purpose of testing is to find potential linking relationships, therefore most of the nodes in the testing and training phases are the same. Therefore, for the recommendation, we use a different $\text{Beta}(\alpha,\beta)$ distribution from NLP and CV as a coefficient of mix sampling.

\subsection{\textbf{Decay Sampling}}

\begin{figure}[t]
    \centering
    
    % \includegraphics[width=.7\linewidth]{img/decay_sample.pdf}
    \includegraphics[width=.8\linewidth]{img/new_decay_sample.pdf}
    \caption{An illustration of the Decay sampling. \(D_1\) and \(D_2\) are decay items, which are the second-hop items found by anchored User \(U\) through BFS. Then a soft link is built between $<D_1,U>$ and $<D_2,U>$. Using soft link, the structural information of the graph is effectively preserved.}
    \label{fig:decay_sam}
\end{figure}


To preserve the structure information during the sampling process, we propose Decay Sampling that strengthens the digestion of graph structure information via a soft link mechanism.
The weights of links between nodes decay with their distance in a Breadth First Search~(BFS) manner.
An illustration of Decay Sampling is shown in Fig. \ref{fig:decay_sam}.

%In addition to using mixup sampling to improve accuracy, we propose Decay Sampling, which calculates the weight of soft links directly on the network structure based on the distance between nodes (see Fig. \ref{fig:decay_sam}). Experiments have demonstrated that this approach is the most efficient component of MixDec Sampling for model enhancement.

For each anchored node in the graph, we compute the decay weights of links between it and its neighbors within  \(l\)-hop based on BFS.
The weight of links is designed based on the number of reachable paths between the anchored node and its \(l\)-hop neighbors. 
The hop number \(l\) of BFS is the same as the number of aggregation layers in the GNN model. 
The weight of link \(w_d\) connecting the anchored node to the sampled node is defined as follows:

\begin{equation}\label{link}
    w_{d}=\rho+(1- \rho)\frac{r_d}{r_\text{max}},
\end{equation}
where $d$ is one of the non-positive sample of neighbors within \(l\)-hop reached by the anchored node $v$ on BFS paths, $w_{d}$ is the link weight between node $d$ and the anchored node, $\rho$ is used to map the weights of $w_{d}$ to $\left[\rho,1\right]$, $r_d$ is the number of pathways from anchored node $v$ to node $d$, and $r_\text{max}$ is the maximum value of all the \(r\) within \(l\)-hop by node $v$. 



We sort the sampled nodes by defined decay weights and intercept Top $k$. Meanwhile, the link weights of the rest nodes are set to 0. 
%All sampled pairs are trained with the corresponding weighted links obtained by the BFS.
Fig. \ref{fig:decayf} shows an example of BFS-based dacay weight.

\begin{figure}[t]
\centering
% \includegraphics[width=\linewidth]{img/BFS.pdf}
\includegraphics[width=.8\linewidth]{img/new_bfs.pdf}
\caption{An example of BFS-based decay weights of soft links. For the anchored user, items with direct interactions are regarded as positive items, and decay items are 3-hop neighbors. For these four decay items, the number of pathways from the user to the decay items, as obtained by BFS, is 1, 1, 3 and 2. Correspondingly, the decay weights of soft links are 0.66, 0.66, 1.00 and 0.83, when  the hyperparameter \(\rho\) is set to 0.5.} 
\label{fig:decayf} 
\end{figure}
%For the four decay items in Fig. \ref{fig:decayf}, the number of pathways from the user to the decay items, as obtained by BFS, is 1, 2, 4, and 1. Correspondingly, the decay weights of soft links are 0.625, 0.875, 1, and 0.625, when the hyperparameter \(\rho\) is set to 0.5.

The Decay Sampling part loss is defined as follows:
 
\begin{equation}
\mathcal{L}_{d}= \frac{1}{N}\sum_{(v,d) \in\mathcal{O}_d} g\left(\sigma\left(e_v^T e_{d}\right), w_d\right),
\end{equation}
while the complete objective of Decay Sampling for optimization is as follows:

\begin{equation}\label{linkdecay}
\mathcal{L}_{dec}=\mathcal{L}_{ns}+ \mathcal{L}_{d},
\end{equation}
where $(v,d)\in \mathcal{O}_d$, $\mathcal{O}_d $ is the decay item set of each anchored node. 


The time complexity of the BFS is $O(N ^{l+1})$.
Graphs in practical applications are not that dense, so the complexity is lower than $O( N^{l+1})$.
%Since the hop number of BFS is $l$ and the sparsity of graph structure in the real implementation, the actual time complexity will be significantly less than $O(N\cdot E)$. 
In particular, for a User-Item graph, suppose the number of user nodes is $N_u$, the number of item nodes is $N_i$ and $E$ is the number of edges in the graph, the time complexity $T$ is approximately:

\begin{equation}
    T=
    % P_u\times\frac{E}{P_u}\times\frac{E}{P_i}\times\frac{E}{P_u}\times...=E\prod_{(i,u)\in V}^{\left \lfloor L/2\right \rfloor}  \frac{E}{P_i}\cdot\frac{E}{P_u}=
    E\prod_{(i,u)\in V}^{\left \lfloor l/2\right \rfloor}  \frac{E}{N_i}\cdot\frac{E}{N_u}=\frac{E^l}{N_i^{\left\lfloor l/2\right\rfloor}N_u^{\left\lfloor l/2\right\rfloor}},
\end{equation}
% \[C=P_u\prod_{i\in V}\prod_{u\in U} \frac{E}{P_i} \cdot\frac{E}{P_u} \]
where ${E}/{N_u}$ is the average number of user-item edges for each user node, ${E}/{N_i}$ is the average number of item-user edges for each item node and $l$ is odd. 

It is worth noting that the amount of data in the recommendation task is huge, \eg items may be viewed by millions or ten million users, which will cause the BFS to take too long to run.
Since random walks \cite{perozzi2014deepwalk} on graph has been used to estimate the reachability probability from anchored node to other nodes, the weight of the soft link in the above BFS can be approximated by random walk for large-scale graphs.
%Thus the corresponding soft link $L_d$ of the $Largest-T$ nodes obtained by the above BFS can also be approximated by random walk \cite{perozzi2014deepwalk}. 
Specifically, we randomly walk $l$ steps starting from each anchored node. Then, $r_d$ in (\ref{linkdecay}) is the number of occurrences of sampled node $d$ in the walk sequences for the anchored node $v$.
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%重新组织一下
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%Specifically,  each node samples $Z$ nodes based on the probability of random walk in the graph, and replaces $P$ in equation \ref{linkdecay} with the number of occurrences $P_z$ for each node $Z$ to determine the value of the soft link $L_d$.


\subsection{\textbf{MixDec Sampling}}

\begin{figure}[t]
    \centering
    
    % \includegraphics[width=.7\linewidth]{img/mixdec_sample.pdf}
    \includegraphics[width=.8\linewidth]{img/new_mixdec_sample.pdf}
    \caption{An illustration of the MixDec sampling. For example, in the figure, \(P_1\), \(P_2\), \(N\), \(M_1\) and \(M_2\) are the same as the Mixup sampling phase \ref{fig:mix_sam}, and \(D\) is the node searched by BFS in the Decay sampling phase. We synthesize \(M_3\) by mixing $<P_1, D>$ pair, and also build a soft link between $<U, M_3>$. MixDec sampling integrates Mixup sampling and Decay sampling, and the structure and feature information of the graph will be reserved.}
    \label{fig:mixdec_sam}
\end{figure}

MixDec Sampling is a joint sampling method of Mixup Sampling and Decay Sampling.
Mixup Sampling synthesizes the nodes and links, and then Decay Sampling preserves the graph's structure with soft links.
An illustration of MixDec Sampling is shown in Fig. \ref{fig:mixdec_sam}.
Formally, a GNN-based model is trained according to the following loss function:

\begin{equation}\label{loss}
\mathcal{L}=\mathcal{L}_{ns}+ \mathcal{L}_{m} + \mathcal{L}_{d},
\end{equation}
where $\mathcal{L}_{ns}$ is  basic negative sampling loss, $\mathcal{L}_{m}$ is the Mixup Sampling part loss, $\mathcal{L}_{d}$ is the Decay Sampling part loss, respectively.
Unlike Mixup Sampling,
each node pair of MixDec Sampling includes decay nodes, that is, node pairs of positive and positive, node pairs of positive and negative, node pairs of positive and decay. The MixDec Sampling is summarized in Algorithm \ref{alg}.
%where $(V,N^{d\pm})\in \mathcal{R}^{d\pm}$, $\mathcal{R}^{d\pm} $ is the MixDec interaction set of each node by equation \ref{mix}.


\subsection{\textbf{Loss function}}\label{other}


\begin{figure}[t]
  
  \renewcommand{\algorithmicrequire}{\textbf{Input:}}
  \renewcommand{\algorithmicensure}{\textbf{Output:}}
  \removelatexerror
  
  \begin{algorithm}[H]
    \caption{The train process with MixDec Sampling}\label{alg}
    \begin{algorithmic}[1]
        \REQUIRE Graph $G_{train}$, $G_{test}$, Number of Samples $c, c_m, c_d$. \\the training set $\mathcal{O}$, epochs.
        % \ENSURE MRR and Hit@K
        \STATE Initialize nodes feature $H^0$, Model $M_{\theta}$, Loss $\mathcal{L} = 0$.
        \FOR{$\mathit{each \; user}$ $\mathit{v}$ in $G_\text{train}$}
            \STATE Get the set $\mathcal{O}_d$ of the decay item and soft link $w_{d}$ of $\mathit{v}$  based on BFS and (\ref{link}).
        \ENDFOR
        \FOR {$\mathit{epoch}$ in $\mathit{epochs}$}
            %\STATE Initialize loss $\mathcal{L} = 0$.
            \STATE Obtain node features $H^k$ by aggregation of (\ref{agg}).
            \FOR{$\mathit{each \; <user,item> }$ pair in $G_{train}$} % For 语句，需要和EndFor对应
  \STATE Sample $c$ nodes $\mathit{n}^-$ from $\mathcal{R}^-$, $c_m$ node $j$ from $\mathcal{O}_m$, $c_d$ nodes $d$ and soft link $w_d$ from $ \mathcal{O}_d$.
       \STATE Get synthetic nodes $s$, synthetic feature $H_s$ and soft link $w_s$ by Mix sampled nodes based on Mixup Sampling defined in (\ref{mix}).
            \ENDFOR
    \STATE Descending the gradients $\nabla_{\theta}\mathcal{L}$ by (\ref{loss}) and update $\theta$.
        \ENDFOR

    \end{algorithmic}
  \end{algorithm}
  
\end{figure}

%To find the appropriate loss function for mix sampling, we tried the following loss functions.

%\begin{enumerate}
%    \item  Cross Entropy loss: The cross entropy loss makes the loss of links except positive and negative links cannot be close to 0.
%    \item Mean Squared Error~(MSE) and Mean Absolute Error~(MAE): MSE and MAE can make the loss of soft links close to 0. As the link weight is between 0 and 1, MSE will makes the loss invisible, the MAE is preferred.
%\end{enumerate}

We use Mean Absolute Error~(MAE) as the loss function $g$ of MixDec Sampling. Then, the Mixup Sampling part loss is formalized as:
\begin{equation}
\mathcal{L}_{m}=\frac{1}{N}\sum_{(v,s) \in\mathcal{O}_m} \left|\sigma\left(e_v^T e_{s}\right)-w_s\right|,
\end{equation}
and the Decay Sampling part loss is formalized as:
\begin{equation}
\mathcal{L}_{d}= \frac{1}{N}\sum_{(v,d) \in\mathcal{O}_d} \left|\sigma\left(e_v^T e_{d}\right)-w_d\right|.
\end{equation}

%\textbf{Link values of BFS:} Besides the link value determination method \ref{link} above, we have devised several link value determination methods as follows.
%\begin{equation}\label{e}
%     L_d(V_N)=\rho+(1- \rho)\frac{\sqrt{P_N}}{\sqrt{P_{max}}},
%\end{equation}
%
%\begin{equation}\label{hop}
%%\end{equation}
%where $hop$ is the shortest path between orginal node and target node. Method \ref{link} has a more even distribution of link values compared to method \ref{e}, while method \ref{hop} is simple to implement but does not differentiate the links of different nodes with the same distance. So Method \ref{link} is the best way to distinguish links compared to equation \ref{e} and equation \ref{hop}. In addition, in the case of source users with insufficient number of reachable nodes, we randomly select other nodes in the graph and set the link to $\rho$ and padding its decay table