
\documentclass{article}
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

\usepackage{hyperref}

\newcommand{\theHalgorithm}{\arabic{algorithm}}
\newcommand{\method}{\textsc{RelGNN}} 

\newcommand{\tianlang}[1]{{\color{cyan} Tianlang: #1}}
\newcommand{\charilaos}[1]{{\color{red} Charilaos: #1}}
\newcommand{\jure}[1]{{\color{blue} J: #1}}

\usepackage[accepted]{arxiv}


\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{xspace}

\usepackage[capitalize,noabbrev]{cleveref}


\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}


\usepackage[textsize=tiny]{todonotes}
\usepackage{catchfile}
\usepackage{makecell}
\usepackage{multirow}
\usepackage{multicol}
\usepackage{bm}
\usepackage{subcaption}


\icmltitlerunning{\method: Composite Message Passing for Relational Deep Learning}

\begin{document}

\twocolumn[
\icmltitle{
\method: Composite Message Passing for Relational Deep Learning
}


\begin{icmlauthorlist}
\icmlauthor{Tianlang Chen}{stanford}
\icmlauthor{Charilaos Kanatsoulis}{stanford}
\icmlauthor{Jure Leskovec}{stanford}
\end{icmlauthorlist}

\icmlaffiliation{stanford}{Computer Science Department, Stanford University}

\icmlcorrespondingauthor{Tianlang Chen}{tlchen@cs.stanford.edu}

\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

\printAffiliationsAndNotice{} 

\newcommand{\xhdr}[1]{\paragraph{#1.}\xspace}
\newcommand{\relbench}{\textsc{RelBench}\xspace}

\newcommand{\amazon}{\textit{rel-amazon}\xspace}
\newcommand{\amazonn}{\texttt{rel-amazon}\xspace}
\newcommand{\userChurn}{\texttt{user-churn}\xspace}
\newcommand{\userLtv}{\texttt{user-ltv}\xspace}
\newcommand{\itemChurn}{\texttt{item-churn}\xspace}
\newcommand{\itemLtv}{\texttt{item-ltv}\xspace}
\newcommand{\userItemPurchase}{\texttt{user-item-purchase}\xspace}
\newcommand{\userItemRate}{\texttt{user-item-rate}\xspace}
\newcommand{\userItemReview}{\texttt{user-item-review}\xspace}
\newcommand{\customer}{\textit{customer}\xspace}
\newcommand{\transaction}{\textit{transaction}\xspace}
\newcommand{\items}{\textit{item}\xspace}

\newcommand{\fone}{\texttt{rel-f1}\xspace}
\newcommand{\driverPosition}{\texttt{driver-position}\xspace}
\newcommand{\driverConstructorResult}{\texttt{driver-constructor-result}\xspace}
\newcommand{\driverDNF}{\texttt{driver-dnf}\xspace}
\newcommand{\driverTopThree}{\texttt{driver-top3}\xspace}
\newcommand{\race}{\textit{races}\xspace}
\newcommand{\driver}{\textit{drivers}\xspace}
\newcommand{\constructor}{\textit{constructors}\xspace}
\newcommand{\standing}{\textit{standings}\xspace}
\newcommand{\circuits}{\textit{circuits}\xspace}
\newcommand{\res}{\textit{results}\xspace}

\newcommand{\handm}{\texttt{rel-hm}\xspace}
\newcommand{\itemSales}{\texttt{item-sales}\xspace}

\newcommand{\event}{\texttt{rel-event}\xspace}
\newcommand{\userAttendance}{\texttt{user-attendance}\xspace}
\newcommand{\userIgnore}{\texttt{user-ignore}\xspace}
\newcommand{\userRepeat}{\texttt{user-repeat}\xspace}

\newcommand{\avito}{\texttt{rel-avito}\xspace}
\newcommand{\userClick}{\texttt{user-clicks}\xspace}
\newcommand{\userVisit}{\texttt{user-visits}\xspace}
\newcommand{\userAdVisit}{\texttt{user-ad-visit}\xspace}
\newcommand{\adsCTR}{\texttt{ad-ctr}\xspace}

\newcommand{\stackex}{\texttt{rel-stack}\xspace}
\newcommand{\userEngage}{\texttt{user-engagement}\xspace}
\newcommand{\postVotes}{\texttt{post-votes}\xspace}
\newcommand{\userBadge}{\texttt{user-badge}\xspace}
\newcommand{\userPostComment}{\texttt{user-post-comment}\xspace}
\newcommand{\postPostLinked}{\texttt{post-post-related}\xspace}
\newcommand{\posts}{\texttt{posts}\xspace}

\newcommand{\trials}{\texttt{rel-trial}\xspace}
\newcommand{\studyOutcome}{\texttt{study-outcome}\xspace}
\newcommand{\studyAdverse}{\texttt{study-adverse}\xspace}
\newcommand{\studyWithdrawal}{\texttt{study-withdrawal}\xspace}
\newcommand{\facilitySuccess}{\texttt{site-success}\xspace}
\newcommand{\sponsorConditionRec}{\texttt{condition-sponsor-run}\xspace}
\newcommand{\sponsorFacilityRec}{\texttt{site-sponsor-run}\xspace}

\newcommand{\src}{\texttt{src}\xspace}
\newcommand{\midd}{\texttt{mid}\xspace}
\newcommand{\dst}{\texttt{dst}\xspace}
\newcommand{\fuse}{\texttt{fuse}\xspace}
\newcommand{\tea}{\texttt{T}\xspace}

\newcommand{\tbl}[1]{\textsc{#1}\xspace}

\newcommand{\mr}[2]{\multirow{#1}{*}{#2}}
\newcommand{\mc}[3]{\multicolumn{#1}{#2}{#3}}

\begin{abstract}
Predictive tasks on relational databases are critical in real-world applications spanning e-commerce, healthcare, and social media. To address these tasks effectively, Relational Deep Learning (RDL) encodes relational data as graphs, enabling Graph Neural Networks (GNNs) to exploit relational structures for improved predictions. However, existing heterogeneous GNNs often overlook the intrinsic structural properties of relational databases, leading to modeling inefficiencies. Here we introduce \method, a novel GNN framework specifically designed to capture the unique characteristics of relational databases. At the core of our approach is the introduction of atomic routes, which are sequences of nodes forming high-order tripartite structures. Building upon these atomic routes, \method~designs new composite message passing mechanisms between heterogeneous nodes, allowing direct single-hop interactions between them. This approach avoids redundant aggregations and mitigates information entanglement, ultimately leading to more efficient and accurate predictive modeling. \method~is evaluated on 30 diverse real-world tasks from \relbench~\cite{fey2023relational}, and consistently achieves state-of-the-art accuracy with up to 25\% improvement.
\end{abstract}
 




\section{Introduction}\label{intro}

Predictive modeling over relational data (multiple tables connected via primary-foreign key relations) is central to numerous real-world applications: e-commerce platforms forecast product demand, music streaming services personalize recommendations, and financial institutions assess credit risk. The common strategy to tackle these predictive tasks involves classical tabular machine learning approaches~\citep{chen2016xgboost} that often require flattening relational data into a single table through manual feature engineering~\citep{kaggle-survey}. This approach is not only labor-intensive but also leads to a substantial loss of predictive signal, as it oversimplifies the interconnected structure of relational data during the flattening process.

To overcome these limitations, \citet{fey2023relational} introduced Relational Deep Learning (RDL), a framework that enables end-to-end trainable neural networks to perform predictive modeling directly on relational databases by leveraging their inherent structure. In RDL, relational data is represented as a \textit{graph}, where each entity is represented as a node, and the primary-foreign key links between entities define the edges. This graph-based representation allows Graph Neural Networks (GNNs) ~\citep{gilmer2017mpgnn, hamilton2017inductive} to serve as predictive models, capturing complex relational dependencies that traditional methods overlook. Complementing this advancement, \relbench ~\citep{robinson2024relbench} provides the first comprehensive benchmark for evaluating and developing RDL models. By addressing the shortcomings of tabular approaches, RDL not only enhances predictive performance but also establishes a new paradigm in machine learning—one where end-to-end learning frameworks are applied to solve predictive problems on relational data.


Designing effective GNN models for RDL is essential for tackling predictive tasks and supporting critical real-world applications. Relational graphs are large-scale heterogeneous networks whose structure evolves dynamically over time. Existing approaches typically apply standard heterogeneous GNNs directly to these graphs \cite{robinson2024relbench}. However, this can be suboptimal, as these models fail to fully capture the unique structural and relational properties inherent in relational databases. Standard heterogeneous GNNs \cite{schlichtkrull2018relational,hu2020heterogeneous} are designed for general heterogeneous graphs, where edge types encode direct semantic interactions between entities. In contrast, relational data graphs are structured around primary-foreign key relationships, which define the connectivity between tables rather than semantic meaning. This fundamental distinction significantly affects how information propagates, highlighting the need for models specifically tailored to the structural characteristics of relational databases.

\begin{figure*}[!t]
    \centering
\includegraphics[width=\textwidth]{figures/fig-intro.pdf}
    \caption{An illustration of the key concepts in our method. \textbf{(a)} The primary-foreign key relation of the \fone dataset. Arrows point from a node with a foreign key to the node with the corresponding primary key. Nodes with zero or one foreign key are marked in blue, and the corresponding foreign key is illustrated by a solid line. Nodes with two or more foreign keys are marked in purple, and the corresponding foreign keys are illustrated by dotted lines. \textbf{(b)} An example of the bridge structure, where \standing node is a bridge node. \textbf{(c)} An example of the hub structure, where \res node is a hub node. \textbf{(d)} Examples of three atomic routes, where the nodes within each box constitute a distinct atomic route.}
    \label{fig:main}
\end{figure*}

Here we propose \method, a novel graph attention framework that introduces a composite message passing mechanism to fully exploit the unique structural properties of relational data. \method~builds upon a key observation: Primary-foreign key interactions in relational graphs give rise to two frequent substructures: (i) bridge nodes (cf. Fig. \ref{fig:main}(b)), which have exactly two foreign keys and form tripartite structures of the form  \(\textit{(node-type 3} \leftarrow \textit{node-type 1} \rightarrow \textit{node-type 2)}\) and (ii) hub nodes (cf. Fig. \ref{fig:main}(c)), which possess three or more foreign keys, forming star-shaped subgraphs. One major limitation of these node-types is that bridge and hub nodes often function only as intermediate aggregators, enabling interactions among their neighbors. As a result, conventional multi-hop message passing architectures can suffer from oversmoothing or modeling inefficiencies, since repeated aggregation at these central nodes dilutes meaningful relationships.
Beyond these inefficiencies, in configuration (ii), star-shaped connectivity at the schema level induces a hidden second-order clique structure in the data graph—an aspect that standard modeling approaches fail to exploit effectively.


Our \method~introduces the notion of \textit{atomic routes} as the foundation of its graph attention mechanism. An atomic route is a sequence of node-types that jointly form a complete information exchange (cf. Fig. \ref{fig:main}(d)). We distinguish two scenarios: (i) when a table has a single foreign key referencing another table, and (ii) when a table has multiple foreign keys, acting as a bridge or hub node. In the first case, an atomic route consists of just two node-types and the edge connecting them, whereas in the second case, it comprises a single-hop path grouping the source node, the bridge (or hub) node, and the destination node, thereby capturing all relevant interactions.
Although reminiscent of meta-paths in conventional heterogeneous graphs, atomic routes differ fundamentally in their construction: while meta-paths typically rely on domain expert knowledge, atomic routes are systematically derived from primary–foreign key relationships in relational data. Building upon these routes, \method~designs \emph{composite message passing and attention mechanisms} to avoid aggregating irrelevant information, thereby preventing noise accumulation and oversmoothing. Moreover, unlike standard heterogeneous GNNs that require multiple hops for complete information exchange, \method~enables direct, single-step communication between the relevant node-types, efficiently extracting critical predictive signals.

We assess the performance of the proposed \method~across all tasks in \relbench~\citep{fey2023relational}, a benchmark which spans seven diverse relational databases covering e-commerce, social networks, sports, and medical platforms. \relbench~features 30 real-world predictive tasks cast as entity classification, entity regression, and recommendation. \method~surpasses all baselines on 27 of the 30 tasks while performing comparably on the remaining three. Notably, \method~achieves more than a 4\% improvement over a standard heterogeneous GNN in 15 out of 30 tasks, and provides up to a 25\% improvement on the \texttt{site-success} regression task in the \texttt{rel-trial} database.

\begin{figure*}[!t]
    \centering
\includegraphics[width=\textwidth]{figures/fig-concept.pdf}
\vspace{-5pt}
    \caption{Illustration of key concepts of RDL. \textbf{(a)} An example relational database. \textbf{(b)} Relational tables connected by primary-foreign key relations. \textbf{(c)} Relational entity graph built from relational tables. \textbf{(d)} Subgraph sampled with termporal neighbor sampling. Figures from ~\citet{fey2023relational}.
    }
    \label{fig: concept}
    \vspace{-5pt}
\end{figure*}

\section{Preliminaries}
\label{sec: prelim}

\subsection{Relational Database}
\label{sec:notation}

A relational database $(\mathcal{T}, \mathcal{L})$ consists of a set of tables $\mathcal{T} = \{T_1, \ldots, T_n\}$ and a set of links between them $\mathcal{L} \subseteq \mathcal{T} \times \mathcal{T}$. Each table is a set $T = \{v_1, ..., v_{n_T}\}$, where the elements $v_i \in T$ are called rows or entities. Each entity $v \in T$ has a unique \textbf{primary key} $p_v$ that distinguishes it from other entities within the table. An entity may also have one or more \textbf{foreign keys}, where each foreign key $\mathcal K_v\subseteq \{p_{v'}:  v' \in T'\text{ and } (T, T') \in \mathcal L \}$ defines a link between element $v\in T$ to elements $v' \in T'$, where $p_{v'}$ is the primary key of an entity $v'$ in table $T'$. 
Besides, an entity may have several attributes $x_v$, which represent the informational content of the entity, and an optional timestamp $t_v$, indicating when an event occurred. 
Both primary keys and foreign keys, as well as attributes and the optional timestamp, are columns of the table.
A link $L = (T_{\rm fkey}$, $T_{\rm pkey})$ between tables exists if a foreign key column in $T_{\rm fkey}$ references a primary key column of $T_{\rm pkey}$. 

For example, in Figue \ref{fig: concept}(a), the \tbl{Transactions} table  has a primary key (\tbl{TransactionID}), two foreign keys (\tbl{ProductID} and \tbl{CustomerID}), one attribute (\tbl{Price}), and a timestamp (\tbl{Timestamp}). Similarly, the \tbl{Products} table has a primary key (\tbl{ProductID}), no foreign keys, three attributes (\tbl{Description}, \tbl{Image} and \tbl{Size}), and no timestamp. The links between tables are illustrated by black connecting lines.


\subsection{Relational Deep Learning}
\label{sec:rdl}
\citet{fey2023relational} proposed Relational Deep Learning (RDL), a framework that enables end-to-end trainable neural network models to tackle predictive tasks on relational databases. In RDL, relational tables are represented as a temporal, heterogeneous \textit{graph}, where each table corresponds to a node-type, each entity (row) corresponds to a node, and primary-foreign key links define the edges (See Figure \ref{fig: concept} b, c). The information carried by each entity, specified by the columns of the table, is extracted as the initial embedding for the corresponding node. This representation preserves all the information and predictive signals in relational data.

It is important to note that relational data evolves over time as events occur. This temporal aspect is captured by the (optional) timestamp $t_v$ attached to each entity $v$. For instance, each transaction in the \tbl{Transactions} table has a timestamp. Many predictive tasks involve forecasting future events, such as predicting the total sales of a product in the next week. Consequently, it is crucial to treat time as an important component in RDL. Specifically, temporal neighbor sampling~\citep{hamilton2017inductive, fey2023relational} is employed to construct subgraphs around entity nodes at specific seed times determined by the corresponding tasks, with nodes from future timestamps excluded during the sampling to prevent information leakage from future events during training. After that, GNNs can be trained end-to-end on these temporally sampled subgraphs, eliminating the need for manual feature engineering (See Figure \ref{fig: concept} d).



\subsection{Meta-path in Heterogeneous Graphs}
Meta-path ~\citep{sun2011pathsim} is widely used in heterogeneous graphs to capture semantic relationships between different types of entities ~\citep{shang2016meta,dong2017metapath2vec,hu2018leveraging,shi2018easing,wang2019heterogeneous,fu2020magnn}. A meta-path is a sequence of node and edge types in a heterogeneous graph, often designed using expert knowledge to capture meaningful relational patterns. For example, in an academic graph, a meta-path can be defined as "Author-Paper-Author" to model co-authorship relations, or "Author-Paper-Conference-Paper-Author" to capture co-conference relations.
Despite its widespread use, the meta-path has notable limitations ~\citep{shi2016survey,hu2020heterogeneous,GNNBook-ch16-shi}. It requires manual selection, which depends on domain expertise and can be biased, leading to suboptimal performance if an inappropriate meta-path is chosen. Additionally, meta-path lacks flexibility, as it cannot easily adapt to changes in graph structure or new relationships. Moreover, designing an effective set of meta-paths for complex graphs can be time-consuming and may fail to capture all relevant interactions, limiting its expressiveness.

\section{Method}
\label{sec: method}


\subsection{Challenges in Message Passing for Relational Deep Learning}\label{sec: method-challenge}
Message passing in heterogeneous graphs is designed to aggregate information across nodes and edges of different types. However, relational data graphs exhibit distinct structural properties that set them apart from general heterogeneous graphs. Specifically, in a heterogeneous graph, the fundamental unit of structure is a relation type, typically represented as a triplet of the form \textit{(node-type 1, edge-type, node-type 2)}. For example, consider a scenario where a customer purchases an item. This interaction can be modeled as a heterogeneous graph with the triplet \textit{(customer, transaction, item)}, where a message originating from a customer node is propagated to an item node via a transaction edge in a single hop.  

In contrast, relational data graphs are structured differently. While they are heterogeneous and contain multiple node-types, their edge types are not defined based on semantic interactions but rather by primary-foreign key relationships. Consequently, the fundamental structural unit is not necessarily a triplet but a direct pairwise relationship between nodes, represented as \textit{(node-type 1, node-type 2)}. In a relational data graph, the \textit{(customer, transaction, item)} triplet is modeled by three nodes: a \customer node, a \transaction node, and an \items node, and pairwise interactions between them i.e., \textit{(customer, transaction)} and \textit{(transaction, item)}. This distinction is critical in both modeling and analysis and requires further investigation. 

Our first observation is node-types in relational graphs can be broadly classified into two categories: (i) those with zero or one foreign key and (ii) those with two or more foreign keys. To illustrate, consider the \fone schema, which tracks all-time Formula 1 racing data since 1950 (see Fig.~\ref{fig:main}). In this schema, \textit{Constructor}, \textit{races}, and \textit{drivers} each have zero foreign keys, while \textit{circuits} has one. By contrast, \textit{constructor\_standings}, \textit{constructor\_results}, and \textit{standings} each have two foreign keys, and \textit{results} and \textit{qualifying} each have three. Nodes with two or more foreign keys are highlighted in purple. Subgraphs containing only node-types with zero or one foreign key exhibit no special structural patterns and can be treated as general heterogeneous graphs. However, subgraphs that include node-types with multiple foreign keys reveal unique structures, necessitating further analysis.

\textbf{Node-type with two foreign keys (bridge):} When a node-type has two foreign keys, it forms a subgraph of the form \(\textit{(node-type 3} \leftarrow \textit{node-type 1} \rightarrow \textit{node-type 2)}\), creating a local tripartite structure among these three node-types. In this configuration, \(\textit{node-type 1}\) simply acts as a aggregating bridge between \(\textit{node-type 2}\) and \(\textit{node-type 3}\). Consequently, a two-hop communication path can be redundant, leading to unnecessary aggregation and modeling inefficiencies. 
Under standard heterogeneous GNNs, irrelevant information often gets entangled during message passing, which dilutes the predictive signal. For instance, consider how “a driver achieved a certain standing in a race” is passed from a source \(\race\) node to a destination \(\driver\) node via an intermediate \(\standing\) node (Fig.~\ref{fig:main}). In a typical two-hop scheme, all neighbors of the \(\standing\) node—including unrelated \(\constructor\) nodes—contribute noise during the first message passing step. Furthermore, information from the \(\driver\) node is unnecessarily duplicated when it propagates through this intermediate node.
As we show in the next subsection, modeling this triplet of nodes as an atomic route enables direct one-hop message passing between the relevant node-types—\(\textit{node-type 2}\) and \(\textit{node-type 3}\)—without any loss of information or risk of oversmoothing. 





\textbf{Node-type with multiple (three or more) foreign keys (hub):} When node-types have three or more foreign keys, they form star-shaped subgraphs that serve as communication hubs, bridging multiple node-types. For example, as shown in Fig.~\ref{fig:main}, the \textit{results} node mediates interactions among \constructor-\race, \race-\driver, and \constructor-\driver. These hub nodes inherit the same inefficiencies seen in the two-foreign-key case, where two-hop message passing often leads to redundant information aggregation. Furthermore, the star-shaped connectivity at the schema level induces a hidden second-order clique structure in the data graph—an aspect that standard modeling approaches fail to exploit effectively. Our proposed approach, which is explained next, can leverage these hidden cliques, which form a critical substructure in many high-impact domain graphs. This transition from star-like to clique-like patterns substantially increases connectivity density, reshaping information propagation and complicating message passing dynamics.



\subsection{Atomic Routes in Relational Deep Learning}
\label{sec: method-atomic}
To address the challenges outlined above, we introduce the concept of atomic routes.

\begin{definition}[Atomic Route]\label{def:atomic_routes} An \emph{atomic route} is a sequence of node-types that form a composite path between the starting and ending node-type. We distinguish two cases: \begin{enumerate} \item \textbf{Single foreign key.} If a table has exactly one foreign key to another table, the atomic route is an edge connecting the primary-key node to the foreign-key node. \item \textbf{Multiple foreign keys.} If a table has multiple foreign keys, the atomic route is a hyperedge connecting pairs of foreign-key nodes via the primary-key node. \end{enumerate} \end{definition}
Fig. \ref{fig:main} and \ref{fig:atomic_routes} illustrates the primary-foreign key relationships in the \fone dataset and the atomic routes derived from these relationships. For instance, the \circuits table has only one foreign key, which points to the \race table, forming atomic routes (\circuits $\to$ \race) and (\race $\to$ \circuits). In contrast, the \standing table has two foreign keys connecting it to both the \driver and \race tables. This results in atomic routes (\driver $\to$ \standing $\to$ \race) and (\race $\to$ \standing $\to$ \driver). These routes capture the necessary interactions among multiple entities within a single step.

\textbf{Atomic routes vs. Meta-paths:} Following Definition \ref{def:atomic_routes}, atomic routes are constructed solely based on primary–foreign key relationships among tables, allowing them to be derived systematically without requiring expert knowledge or manual intervention. This ensures both broad applicability and scalability across diverse datasets. In contrast, meta-paths in general heterogeneous graphs often require manual specification, risk introducing bias through selective design, and may overlook crucial relational contexts. While meta-paths aim to highlight specific patterns, atomic routes serve a fundamentally different role: to systematically and comprehensively capture all essential interactions inherent in relational data.


\subsection{Composite Message Passing for Relational Deep Learning}
\label{sec: method-composite}
In this subsection we build upon the concept of atomic routes and design composite message passing mechanismsfor RDL. We begin this discussion by applying a standard heterogeneous GNN on a subgraph encoding tables with multiple foreign keys. We assign \src, \dst, and \midd to represent nodes corresponding to source, destination, and intermediate node-types, respectively. 
In standard heterogeneous GNNs, it takes two steps to complete the full information exchange. In the first step, each \midd node aggregates information from all its neighbor nodes:
\begin{equation}
\label{eq_vanilla}
      \mathbf{h}_{\midd}^{(l+1)}=\operatorname{UPD}(\{\!\{ \mathbf{m}_{R}^{(l+1)}|\forall R=(\tea,\  \phi(\midd))\in\mathcal{R} \}\!\}),
\end{equation}
where
\begin{equation*}
    \mathbf{m}_{R}^{(l+1)}=\operatorname{AGGR}(\mathbf{h}_{\midd}^{(l)},\{\!\{ \mathbf{h}_{u}^{(l)}| \phi(u)=\tea \}\!\}),
\end{equation*}
$\mathbf{h}_v^{(l)}$ denotes the embedding of node $v$ at the $l$-th layer, $\operatorname{UPD}$ and $\operatorname{AGGR}$ are arbitrary differentiable functions with optimizable parameters, $\{\!\{\cdot\}\!\}$ denotes a permutation invariant set aggregator (e.g. mean, sum), $\mathcal{R}$ denotes the edge set consisting of pairs of node-types connected through primary-foreign key relationships and $\phi(\cdot)$ denotes a function mapping a node to its corresponding node-type.
Then in the second step, the message passed from \midd to \dst is 
\begin{equation}
\label{eq_vanilla_2}
    \mathbf{m}_{(\midd,\ \dst)}^{(l+2)}=\operatorname{AGGR}(\mathbf{h}_{\dst}^{(l+1)},\{\!\{ \mathbf{h}_{\midd}^{(l+1)} \}\!\})
\end{equation}
Note that in Eq. (\ref{eq_vanilla}), \tea represents all node-types connected to the intermediate node-type. Therefore, in addition to the information from the source node-type, information from other irrelevant node-types connected to the intermediate node-type is also aggregated and entangled during this step. Specifically, information from \dst  is also passed to \midd in this step, and it is subsequently passed back to \dst again in Eq. (\ref{eq_vanilla_2}), leading to redundancy, as discussed in Section \ref{sec: method-challenge}.

To avoid these modeling inefficiencies we propose a novel composite message passing scheme based on atomic routes.
\begin{equation}
\label{eq-relgnn}
    \mathbf{m}_{(\dst,\midd,\src)}^{(l+1)}=\operatorname{AGGR}(\mathbf{h}_{\dst}^{(l)},\{\!\{ \operatorname{FUSE}(\mathbf{h}_{\midd}^{(l)} ,\mathbf{h}_{\src}^{(l)} ) \}\!\})
\end{equation}
Eq. (\ref{eq-relgnn}) describes an information exchange from \src via \midd to \dst that is completed within a single step. As a result, there is no extraneous information entangled in the process. In summary, our approach effectively tackles the challenges that standard heterogeneous GNNs may encounter, such as multiple steps needed for complete information exchange and entangled information during message passing.


\begin{figure}[!t]
    \centering
\includegraphics[width=\linewidth]{figures/fig-routes.pdf}
    \caption{All the atomic routes derived from it of \fone dataset. The primary-foreign key relations of \fone is illustrated in Figure \ref{fig:main} (a).}
    \label{fig:atomic_routes}
    \vspace{-10pt}
\end{figure}




\subsection{\method: Composite Message Passing with Atomic Routes}
\label{sec: method-relgnn}
The introduction of atomic routes and composite message passing enables the design of new architectures specifically tailored to relational data graphs. Eq. (\ref{eq-relgnn}) admits multiple instantiations, offering a flexible framework for message passing using atomic routes. Here, we propose \method, a simple yet effective instantiation.
When multiple foreign keys are involved, \method~instantiates Eq. (\ref{eq-relgnn}) as follows: for each \midd node, it fuses information from each \src node connected via a primary–foreign key relationship. Then \(\operatorname{FUSE}(\cdot)\) is implemented as a linear combination:
\begin{equation}
\label{eq:fuse}
\operatorname{FUSE}(\mathbf{h}_{\midd}^{(l)},\mathbf{h}_{\src}^{(l)} )=\mathbf{W}_1\mathbf{h}_{\midd}^{(l)}+\mathbf{W}_2\mathbf{h}_{\src}^{(l)}.
\end{equation}
Note that, due to the nature of foreign keys, each \midd node is connected to only one \src node.
Then, \method~instantiates  $\operatorname{AGGR(\cdot)}$ with the standard multi-head attention mechanism ~\citep{vaswani2017attention, transformerconv}, where embeddings from destination nodes serve as queries, and embeddings derived from the fusion operation in Eq. (\ref{eq:fuse})  serve as keys and values. Let $\mathbf{h}_{\fuse}^{(l)}:=\operatorname{FUSE}(\mathbf{h}_{\midd}^{(l)},\mathbf{h}_{\src}^{(l)} )$ as defined in Eq. {\ref{eq:fuse}}.  $\operatorname{AGGR(\cdot)}$ is realized as:

\begin{align}
\label{eq:attn}
    \operatorname{AGGR}(\mathbf{h}_{\dst}^{(l)},\{\!\{\mathbf{h}_{\fuse}^{(l)}\}\!\})= \mathbf{W}_{\operatorname{proj}}\mathbf{h}_{\dst}^{(l)}\nonumber\\+\sum_{\fuse\in \mathcal{N}(\dst)}\alpha_{\dst,\fuse}\mathbf{W}_V\mathbf{h}_{\fuse}^{(l)},
\end{align}
where the attention coefficients $\alpha_{\dst,\fuse}$ are computed via multi-head attention (with the multi-head notation omitted for brevity):
\begin{equation*}
    \alpha_{\dst,\fuse} = \textrm{softmax} \left(
        \frac{(\mathbf{W}_Q\mathbf{h}_{\dst}^{(l)})^{\top} (\mathbf{W}_K\mathbf{h}_{\fuse}^{(l)})}
        {\sqrt{d}} \right).
\end{equation*}

In cases where tables with multiple foreign keys are not present, there is only a source and destination node-type, so the fusion operation is not needed. We directly substitute $\mathbf{h}_{\fuse}$  with $\mathbf{h}_{\src}$  in Eq. (\ref{eq:attn}):
\begin{equation}
    \label{eq:dim-dim}
      \mathbf{m}_{(\dst,\src)}^{(l+1)}=\operatorname{AGGR}(\mathbf{h}_{\dst}^{(l)},\{\!\{\mathbf{h}_{\src}^{(l)}  \}\!\})
\end{equation}

We use different weight matrices in Eq. (\ref{eq:fuse})  and Eq. (\ref{eq:attn}) for each atomic route, enabling \method~to capture different types of information across routes. 

Finally, the destination node aggregates information from all atomic routes related to it using a simple summation and updates its embedding:
\begin{equation}
    \mathbf{h}_{\dst}^{(l+1)}=\sum_{t\in\mathcal{T}(\dst)}\mathbf{m}_t^{(l+1)},
\end{equation}
where $\mathcal{T}(\dst)$ denotes the set of atomic routes with \dst as the destination node, and $\mathbf{m}_t^{(l+1)}$ denotes the message from atomic route $t$, as defined in  Eq. (\ref{eq-relgnn}) or Eq. (\ref{eq:dim-dim}). Note that this final update operation will not lead to information entanglement, as the model learns distinct weight matrices for each atomic route, therefore learning to assign an appropriate weights to message from each atomic route during the summation.


\section{Experiments}
\label{sec: exp}
We evaluated \method~on \relbench ~\citep{robinson2024relbench}, a  public benchmark designed for predictive tasks over relational databases using GNNs. 
\relbench offers a diverse collection of real-world relational databases and realistic predictive tasks. The benchmark spans 7 datasets, each carefully processed from real-world sources across a wide range of domains, including e-commerce, social networks, medical records, Q\&A platform and sports. These datasets vary significantly in size, with differences in the number of rows, columns, and tables, serving as a challenging and comprehensive benchmark for RDL model evaluation. See Appendix ~\ref{app: dataset} for the description and detailed statistics of each dataset.

\relbench  introduces 30 predictive tasks covering a wide range of real-world use cases, grouped into three representative types: entity classification (Section \ref{sec:node_cls}), entity regression  (Section~\ref{sec:node_reg}), and recommendation  (Section~\ref{sec:rec}). 
These tasks are designed to reflect practical applications, such as predicting event attendance, estimating sales of an item, and recommending posts to users. The data is split temporally, with models trained on data from earlier time periods and tested on data from future time periods. The tasks vary significantly in the number of entities in the train/validation/test split and the proportion of test entities encountered during training. Description and detailed description of each task can be found in Appendix ~\ref{app: task}.


We follow the implementation of RDL in \citet{robinson2024relbench}. As introduced in Sec. \ref{sec:rdl},  relational data is transformed into heterogeneous temporal graphs, and temporal neighbor sampling is employed, creating subgraphs on which the model is trained. The initial node embeddings are extracted from raw table feature using PyTorch Frame ~\citep{hu2024pytorch}. These embeddings are subsequently fed into a GNN, and the resulting node embeddings are passed into a prediction head specific to the type of task to produce the final output.
For baselines, we compare with the heterogeneous GraphSAGE ~\citep{hamilton2017inductive,fey2019fast,robinson2024relbench} used in the original \relbench paper. To ensure a fair comparison, we maintain identical settings, including temporal neighbor sampling algorithm, initial node embeddings extraction model, the prediction head, and the loss function. Additionally, we incorporate a Light Gradient Boosting Machine (LightGBM) ~\citep{ke2017lightgbm} as an additional non-RDL baseline, which is applied directly to the raw entity table features, following the setting in \citet{robinson2024relbench}.


\subsection{Entity Classification}
\label{sec:node_cls}

The entity classification task involves predicting binary labels for a given entity at a specific seed time. The performance is evaluated with the ROC-AUC ~\citep{hanley1983method} metric, where higher values indicate better performance. The prediction head for this task consists of an MLP applied to the entity embeddings generated by the GNN. The model is trained using binary cross-entropy loss, and results are averaged over five different seeds.

Table~\ref{tab:cls}  presents the results, along with the relative improvement of \method~over the standard heterogeneous GNN. \method~outperforms the baselines on 10 out of 12 tasks and achieves comparable performance on the remaining two. Notably, the relative gain is more significant on datasets with a more complex primary-foreign key structure (e.g., \fone; see Appendix~\ref{app: vis} for visualizations) compared to datasets with simpler structures (e.g., \amazonn and \handm). This suggests that the improvements stem from \method’s ability to better capture distinct structural characteristics of relational data graphs. 
On the \stackex dataset, \method~does not achieve significant improvements. A potential cause might be the unique self-loop structure, where primary-foreign key links connect nodes of the same type (\posts)—a pattern not present in other datasets. Treating these self-loops the same way as primary-foreign key links between different node-types may not be optimal. Developing better techniques to handle self-loop structures in relational graphs is an interesting direction for future work.


\begin{table}
    \caption{Entity classification results (ROC-AUC(\%), higher is better) on \relbench test set. Best values are in bold.
    }
    \centering
    \resizebox{\linewidth}{!}{
    \tiny
    \CatchFileDef{\tabledata}{tables/node_classification.tex}{}
    \begin{tabular}{llrrrr}
    \toprule
    \textbf{Dataset} & \textbf{Task} & \textbf{LightGBM} & \makecell{\textbf{Hetero-}\\\textbf{GNN}} & \makecell{\textbf{\method}\\\textbf{(ours)}} & \makecell{\textbf{Relative}\\\textbf{Gain}} \\
    \midrule
    \tabledata
    \bottomrule
    \end{tabular}
    } % resize
    \label{tab:cls}
\end{table}

\subsection{Entity Regression}
\label{sec:node_reg}

Entity regression task requires predicting numerical labels for an entity at a specific seed time. The evaluation metric is Mean Absolute Error (MAE), where lower values indicate better performance. Similar to the classification task, the prediction head is an MLP applied to the entity embeddings generated by the GNN, and the model is trained using L1 loss. Results are averaged over five seeds.

Table~\ref{tab: reg} presents the results and the relative gain of our model over the standard heterogeneous GNN. \method~outperforms the baselines on 8 out of 9 tasks and achieves comparable performance on the remaining one. As in the classification task, improvements are more pronounced on datasets with more complex primary-foreign key structures, highlighting \method’s effectiveness in modeling relational dependencies. The performance gain on \stackex is limited, as discussed in Section~\ref{sec:node_cls}.
We also observe varying performance across tasks in the \trials dataset. A potential cause might be the inherent limitation of the prediction head. The original \relbench paper identified issues with the entity regression prediction head, suggesting that the suboptimal performance on \studyAdverse may result from the prediction head rather than the GNN model itself ~\citep{robinson2024relbench}. Notably, \studyAdverse requires estimating the number of severely affected patients (unbounded prediction), whereas \facilitySuccess involves predicting the success rate (bounded prediction). One possible explanation is that unbounded predictions may be more challenging given the inherently constrained design of the regression head. Future work could explore alternative prediction heads better suited for entity regression tasks. 


\begin{table}
\caption{Entity regression results (MAE, lower is better) on \relbench test set. Best values are in bold.
}
\centering
\setlength{\tabcolsep}{5pt}
\resizebox{\linewidth}{!}{
\tiny
\CatchFileDef{\tabledata}{tables/node_regression.tex}{}
\begin{tabular}{llrrrr}
    \toprule
    \textbf{Dataset} & \textbf{Task} & \textbf{LightGBM} & \makecell{\textbf{Hetero-}\\\textbf{GNN}} & \makecell{\textbf{\method}\\\textbf{(ours))}} & \makecell{\textbf{Relative}\\\textbf{Gain}} \\
\midrule
\tabledata
\bottomrule
\end{tabular}
} % resize
\label{tab: reg}
\end{table}

\subsection{Recommendation}
\label{sec:rec}

\looseness=-1 Recommendation tasks involve predicting a ranked list of top $K$ target entities for a given source entity at a specific seed time,  where $K$ is pre-defined for each task in \relbench. This requires computing pairwise scores between source and target nodes. Following the original \relbench  implementation, we employ two representative prediction heads: two-tower GNN ~\citep{wang2019neural} and identity-aware GNN (ID-GNN) ~\citep{you2021identity}. The two-tower GNN calculates pairwise scores through the inner product of source and target node embeddings and is trained using the Bayesian Personalized Ranking loss ~\citep{rendle2012bpr}. ID-GNN computes scores by applying an MLP to the embedding of the target entity within a subgraph sampled for each source entity and is trained using binary cross-entropy loss. Consistent with the original implementation, we use two-tower GNN for the \amazonn dataset and ID-GNN for the remaining datasets. The evaluation metric is Mean Average Precision (MAP) @$K$, where higher values indicate better performance.  Results are averaged over five seeds.

Results are presented in Table~\ref{tab:link results}. 
\method~ achieves better or same performance compared to baselines on all 9 tasks.
One limitation of the ID-GNN prediction head is that the MLP is applied to target entities sampled within the subgraph around the source node. This restricts the final recommendation list for a source node to targets that were sampled within its local subgraph. In the RDL setting, where past interactions are used to predict future ones, this restriction limits recommendations to target nodes that have previously interacted with the source node. This can be problematic in certain cases. For example, in the \handm dataset, the task is to predict a list of articles a customer will purchase in the next week. If candidate articles are limited to those appearing in the customer's subgraph, the model can only recommend articles the customer has already purchased, ignoring new ones. Since customers rarely repurchase the same article, this creates an inherent limitation. This issue was identified by~\citep{yuan2024contextgnn}, who proposed a quantitative metric, locality score, to measure its impact on model performance. The locality score is defined as the fraction of ground-truth target nodes that fall within the source node’s subgraph for each task. We observe that our improvements are more significant on tasks with a higher locality score (e.g., tasks in \stackex, where users are more likely to interact with posts they have engaged with previously) compared to tasks with a lower locality score (e.g., tasks in \handm and \trials). Developing a more effective prediction head and framework for recommendation tasks is an interesting direction for future work.

\begin{table}
\caption{Recommendation results (MAP(\%), higher is better) on \relbench test set. Best values are in bold.
}
\centering
\resizebox{\linewidth}{!}{
\tiny
\CatchFileDef{\tabledata}{tables/link_prediction.tex}{}
\begin{tabular}{llrrrr}
    \toprule
    \textbf{Dataset} & \textbf{Task} & \textbf{LightGBM} & \makecell{\textbf{Hetero-}\\\textbf{GNN}} & \makecell{\textbf{\method}\\\textbf{(ours)}} & \makecell{\textbf{Relative}\\\textbf{Gain}} \\
\midrule
\tabledata
\bottomrule
\end{tabular}
} % resize
\label{tab:link results}
\end{table}


\section{Related Work}
\label{sec: related_work}
\xhdr{Deep Learning on Relational Data} 
Several works have explored the use of GNNs for learning on relational data ~\citep{schlichtkrull2018relational, cvitkovic2020supervisedrd, vsir2021deep, zahradnik2023deep}. These works investigated different GNN architectures that utilize the relational structure. More recently, \citet{fey2023relational} introduced Relational Deep Learning (RDL) (see Sec. \ref{sec:rdl}), establishing a new subfield of machine learning. RDL has enabled various research opportunities, such as advancements in relational graph construction algorithms, GNN architectures, and task-specific prediction heads. \citet{yuan2024contextgnn} focused on improving task-specific prediction heads for recommendation tasks, addressing limitations in the currently employed two-tower and pair-wise prediction heads. In contrast, our work focuses on improving GNN architectures applied to all task types to generate node embeddings, offering an orthogonal contribution. In addition to GNNs, \citet{wydmuch2024tackling} proposed leveraging large language models (LLMs) to address predictive tasks in RDL. 
\xhdr{Distinction Between RDL and Knowledge Graphs} 
The literature of knowledge graphs ~\citep{{bordes2013translating,wang2014knowledge,wang2017knowledge}} differs from RDL in terms of the tasks being tackled. Knowledge graph models mainly focus on \emph{completion} tasks like predicting missing entities (e.g., Q: Who is the author of "Harry Potter"? A: J.K. Rowling) or missing relationships (Q: Did Yoshua Bengio win a Turing Award? A: Yes). 
In contrast, RDL focuses on making \emph{predictions} about entities or groups of entities (e.g., Will a customer churn in the next month? How much will a customer spend in the upcoming week?) 

\section{Conclusion}
\label{sec: conclusion}
In this paper, we introduced \method, a novel graph neural network framework specifically designed to address the structural inefficiencies of existing heterogeneous GNNs for relational databases. By leveraging atomic routes—which capture high-order tripartite structures—we designed a composite message passing mechanism that enables direct single-hop interactions between heterogeneous nodes. This avoids redundant aggregation and mitigates information entanglement, leading to more efficient and accurate predictive modeling.
Through extensive evaluation on \relbench, a diverse benchmark covering 30 predictive tasks across seven relational databases, \method\ consistently outperforms state-of-the-art baselines, achieving up to a 25\% improvement in predictive accuracy. Our findings emphasize the limitations of conventional heterogeneous GNNs when applied to relational data and highlight the necessity of models that explicitly account for primary-foreign key relationships.

\section*{Impact Statement}
This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here.

\section*{Acknowledgments}
We thank Rishabh Ranjan and Vijay Prakash Dwivedi for discussions and for providing feedback on our manuscript.
We also gratefully acknowledge the support of
NSF under Nos. OAC-1835598 (CINES), CCF-1918940 (Expeditions), DMS-2327709 (IHBEM), IIS-2403318 (III);
Stanford Data Applications Initiative,
Wu Tsai Neurosciences Institute,
Stanford Institute for Human-Centered AI,
Chan Zuckerberg Initiative,
Amazon, Genentech, GSK, Hitachi, SAP, and UCB.

The content is solely the responsibility of the authors and does not necessarily represent the official views of the funding entities.


\bibliography{main}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn
\section{\relbench Details}
\label{app: details}


\begin{table}[t]
  \centering
  \caption{{Statistics of \relbench.}  }
  \label{tab:stats}
  \setlength{\tabcolsep}{4pt}
  \tiny
  \begin{tabular}{lllcccll}
    \toprule
      \mr{2}{\textbf{Dataset}} & \mr{2}{\textbf{Task name}} & \mr{2}{\textbf{Task type}} & \mc{3}{c}{\textbf{\#Rows of training table}} & \#Unique & \%train/test  \\
      & & & Train & Validation & Test & Entities & Entity Overlap   \\
    \midrule
      \mr{6}{\amazonn}
      & \userChurn & classification & 4,732,555 & 409,792 & 351,885 & 1,585,983 & 88.0  \\
      & \itemChurn & classification & 2,559,264 & 177,689 & 166,842 & 416,352 & 93.1   \\
      & \userLtv & regression  & 4,732,555 & 409,792 & 351,885 & 1,585,983 & 88.0  \\
      & \itemLtv & regression   & 2,707,679 & 166,978 & 178,334 & 427,537 & 93.5 \\
      & \userItemPurchase & recommendation   & 5,112,803 & 351,876 & 393,985 & 1,632,909 & 87.4 \\
      & \userItemRate & recommendation  & 3,667,157 & 257,939 & 292,609 & 1,481,360 & 81.0 \\
      & \userItemReview & recommendation  & 2,324,177 & 116,970 & 127,021 & 894,136 & 74.1 \\
    \midrule
    \mr{4}{\avito}
    & \userClick & classification & 59,454 & 21,183 & 47,996 & 66,449 & 45.3 \\
    & \userVisit & classification & 86,619 & 29,979 & 36,129 & 63,405 & 64.6 \\
    & \adsCTR & regression & 5,100 & 1,766 & 1,816 & 4,997 & 59.8 \\
    & \userAdVisit & recommendation & 86,616 & 29,979 & 36,129 & 63,402 & 64.6 \\
    \midrule
    \mr{3}{\event}
    & \userRepeat & classification & 3,842 & 268 & 246 &1,514 & 11.5\\
    & \userIgnore & classification & 19,239 & 4,185 & 4,010 &9,799 & 21.1\\
    & \userAttendance & regression & 19,261 & 2,014 & 2,006 &9,694 & 14.6\\
    \midrule
    \mr{3}{\fone}
      & \driverDNF & classification & 11,411 & 566 & 702 & 821 & 50.0  \\
      & \driverTopThree & classification  & 1,353 & 588 & 726 & 134 & 50.0  \\
      & \driverPosition & regression & 7,453 & 499 & 760 & 826 & 44.6  \\
    \midrule
    \mr{3}{\handm}
      & \userChurn & classification  & 3,871,410 & 76,556 & 74,575 & 1,002,984 & 89.7  \\
      & \itemSales & regression & 5,488,184 & 105,542 & 105,542 & 105,542 & 100.0  \\
      & \userItemPurchase & recommendation & 3,878,451 & 74,575 & 67,144 & 1,004,046 & 89.2 \\
    \midrule
    \mr{5}{\stackex}
      & \userEngage & classification & 1,360,850 & 85,838 & 88,137 & 88,137 & 97.4  \\
      & \userBadge & classification  & 3,386,276 & 247,398 & 255,360 & 255,360 & 96.9  \\
      & \postVotes & regression  & 2,453,921 & 156,216 & 160,903 & 160,903 & 97.1  \\
      & \userPostComment & recommendation  & 21,239 & 825 & 758 & 11,453 & 59.9 \\
      & \postPostLinked & recommendation  & 5,855 & 226 & 258 & 5,924 & 8.5  \\
    \midrule
    \mr{5}{\trials}
      & \studyOutcome & classification  & 11,994 & 960 & 825 & 13,779 & 0.0  \\
      & \studyAdverse & regression  & 43,335 & 3,596 & 3,098 & 50,029 & 0.0  \\
      & \facilitySuccess & regression  & 151,407 & 19,740 & 22,617 & 129,542 & 42.0  \\
      & \sponsorConditionRec & recommendation  & 36,934 & 2,081 & 2,057 & 3,956 & 98.4 \\
      & \sponsorFacilityRec & recommendation  & 669,310 & 37,003 & 27,428 & 445,513 & 48.3  \\
   \bottomrule
  \end{tabular}
\end{table}



In this section, we provide a detailed description and related statistics of  \relbench . Table~\ref{tab:stats} provides detailed statistics for each dataset and task.


\subsection{Datasets}
\label{app: dataset}
\relbench consists of 7 datasets, covering a diverse range of domains and scales. Below is a detailed description for each dataset.

\xhdr{\amazonn} The Amazon E-commerce dataset contains product, user, and review interactions on Amazon's platform. It includes product metadata (e.g., price, category), review details (e.g., rating, text), and user engagement. 

\xhdr{\avito} Avito, a major online marketplace, facilitates buying and selling across categories such as real estate, vehicles, and consumer goods. This dataset contains user search queries, ad characteristics, and additional contextual data for developing predictive models.

\xhdr{\event} The Event Recommendation dataset is derived from Hangtime, a mobile app that tracks users' social plans. It contains user interactions, event metadata, demographic information, and social network connections, offering insights into how social relationships influence user behavior.

\xhdr{\fone} The F1 dataset records comprehensive Formula 1 racing data since 1950, covering drivers, constructors, engine and tire manufacturers, and race circuits). It includes historical race results, season standings, and granular data on practice sessions, qualifying rounds, sprints, and pit stops.

\xhdr{\handm} The H\&M dataset captures customer and product interactions from the retailer’s e-commerce platform. It includes metadata on customers and products (e.g., demographic attributes, product descriptions), and purchase histories.

\xhdr{\stackex} Stack Exchange is a network of Q\&A websites where users earn reputation based on contributions. The dataset contains detailed activity logs, including user biographies, posts, comments, edit histories, votes, and linked questions. 

\xhdr{\trials} The clinical trial dataset, sourced from the AACT initiative, aggregates study protocols and results. It includes trial design details, participant demographics, intervention specifics, and outcome measures, serving as a valuable resource for medical research and policy analysis.


\subsection{Tasks}
\label{app: task}

The following list outlines the description predictive tasks included in \relbench. 
\begin{enumerate}
\item \amazonn

    \begin{enumerate}
    \item \userChurn: Predict whether a user will stop reviewing products within the next three months.
    \item \itemChurn: Predict whether a product will receive no reviews in the next three months.
    \item \userLtv: Estimate the total dollar value of products a user will purchase and review over the next three months.

    \item \itemLtv: Estimate the total dollar value of purchases and reviews a product will receive in the next three months.

    \item \userItemPurchase: Predict the set of items a user will purchase in the next three months.
    \item \userItemRate: Predict the set of items a user will purchase and rate five stars in the next three months.
    \item \userItemReview: Predict the set of distinct items a user will purchase and write a detailed review for in the next three months.
    \end{enumerate}
    
\item \avito

    
    \begin{enumerate}
    \item \userVisit: Predict if a user will interact with multiple ads within next four days.
    \item \userClick: Predict if a user will engage with more than one ad by clicking within next four days.
    \item \adsCTR: Estimate the click-through rate for an ad, assuming it receives a click within four days.
    \item \userAdVisit: Predict the list of ads a user will visit within next four days.
    \end{enumerate}

\item \event
    \begin{enumerate}
    \item \userAttendance: Predict the number of events a user will RSVP "yes" or "maybe" to in the next seven days.
    \item \userRepeat: Predict whether a user will attend an event (by responding "yes" or "maybe") in the next seven days, given they attended an event in the last 14 days.
    \item \userIgnore: Predict whether a user will ignore more than two event invitations in the next seven days.
    \end{enumerate}

    
\item \fone

    Node-level tasks:
    \begin{enumerate}
    \item \driverDNF: Predict whether a driver will fail to finish a race within the next month.
    \item \driverTopThree: Predict if a driver will secure a top-three qualifying position in a race within the next month.
    \item \driverPosition: Predict a driver's average finishing placement across all races in the next two months.
    \end{enumerate}


\item \handm

    Node-level tasks:
    \begin{enumerate}
    \item \userChurn: Predict if a customer will become inactive (no transactions) in the next week.
    \item \itemSales: Predict total revenue generated by an article in the upcoming week.
    \item \userItemPurchase: Predict the list of articles a customer will over the next seven days.
    \end{enumerate}

\item \stackex

    \begin{enumerate}
    \item \userEngage: Predict whether a user will participate by voting, posting, or commenting within the next three months.
    \item \userBadge: Predict if a user will earn a new badge within the next three months.
    \item \postVotes: Predict the number of votes a user’s post will receive over the next three months.
    \item \userPostComment: Predict which existing posts a user will comment on in the next two years.
    \item \postPostLinked: Identify a list of existing posts that will be linked to a given post within the next two years.
    \end{enumerate}

\item \trials

    \begin{enumerate}
    \item \studyOutcome: Predict if a clinical trial will meet its primary outcome within the next year.
    \item \studyAdverse: Estimate the number of patients who will experience severe adverse events or death in a clinical trial over the next year.
    \item \facilitySuccess: Predict the success rate of a trial site in the next year.
    \item \sponsorConditionRec: Predict which sponsors will be associated with a particular condition.
    \item \sponsorFacilityRec: Predict whether a specific sponsor will conduct a trial at a given facility.
    \end{enumerate}
    

\end{enumerate}





\section{Visualization of Primary-Foreign Key Relationships}
\label{app: vis}

In this section, we visualize the primary-foreign key relationships of all datasets in \relbench (\fone is visualized in Figure ~\ref{fig:main}).


\begin{figure}[htbp]
    \centering
    \begin{minipage}{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/amazon.pdf}
    \caption{The primary-foreign key relationship of \amazonn dataset.}
    \end{minipage}
    \hfill
    \begin{minipage}{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/hm.pdf}
    \caption{The primary-foreign key relationship of \handm dataset.}
    \end{minipage}
\end{figure}

\begin{figure}[htbp]
    \centering
    \begin{minipage}{0.45\textwidth}
        \includegraphics[width=\textwidth]{figures/avito.pdf}
    \caption{The primary-foreign key relationship of \avito dataset.}
    \end{minipage}
    \hfill
    \begin{minipage}{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/event.pdf}
    \caption{The primary-foreign key relationship of \event dataset.}
    \end{minipage}
\end{figure}


\begin{figure}[h]
    \centering
    \begin{minipage}{0.45\textwidth}
        \includegraphics[width=\textwidth]{figures/stack.pdf}
    \caption{The primary-foreign key relationship of \stackex dataset.}
    \end{minipage}
    \hfill
    \begin{minipage}{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/trial.pdf}
    \caption{The primary-foreign key relationship of \trials dataset.}
    \end{minipage}
\end{figure}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}

