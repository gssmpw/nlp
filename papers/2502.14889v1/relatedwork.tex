\section{Related Work}
\subsection{Contrastive Language-Image Pretraining (CLIP)}
\cite{radford2021learning} introduced CLIP, which learns multimodal embeddings of images and text by training image and text encoders on large-scale image-text paired data. This enables CLIP to establish connections between the two modalities within a unified embedding space, facilitating zero-shot transfer, where the model can make predictions based on natural language descriptions without relying on task-specific labeled data. However, the complexity of these multimodal tasks necessitates a focus on interpretability to ensure that the model’s decisions are grounded in meaningful features. Studying the interpretability of CLIP helps verify whether the model genuinely understands the relationship between vision and language, as opposed to relying on spurious correlations in the data.

\subsection{Traditional Interpretability Methods}
Traditional interpretability methods for deep learning models were initially designed for unimodal tasks. Early methods, such as Saliency Maps, generate fine-grained heatmaps by computing the gradient of the model's output with respect to input pixels. However, these methods are sensitive to noise and often yield coarse explanations. Grad-CAM~\citep{selvaraju2017grad}, by computing the gradient of activation maps in convolutional layers, produces class-specific heatmaps, making the explanations more intuitive, particularly for convolutional neural networks (CNNs). RISE~\citep{Petsiuk2018rise} further advances the field by introducing a black-box method that applies random masks to different regions of the input image, observes changes in the model's output, and generates heatmaps that account for both global and local interpretability. RISE’s advantage lies in its model-agnostic nature, making it applicable to any architecture. However, due to its reliance on random sampling, it is computationally expensive and may introduce some noise. LIME~\cite{ribeiro2016should}, another black-box method, perturbs the input locally and trains a surrogate model to provide locally linear explanations, making it applicable to any model, though it can sometimes produce inaccurate explanations in complex tasks. Overall, these interpretability methods designed for unimodal tasks do not perform well in multimodal tasks, as we demonstrate in our experiments.%Overall, these methods have gradually evolved from gradient-based analysis dependent on internal model structures to fully black-box interpretability techniques.

With the introduction of the Sensitivity Axiom and Implementation Invariance Axiom by \cite{sundararajan2017axiomatic}, point-wise interpretable methods have rapidly evolved. The Sensitivity Axiom requires the sensitivity of a model's output to align with its attribution values, while the Implementation Invariance Axiom demands that functionally equivalent models yield the same attribution results, regardless of implementation. Currently, the most advanced attribution methods based on adversarial attacks~\cite{jin2024benchmarking}, such as AGI~\citep{pan2021explaining} and MFABA~\citep{zhu2024mfaba}, satisfy both axioms and have shown strong interpretability for traditional CNN models. However, these methods have not been optimized for multimodal tasks, and directly modifying their loss functions for multimodal tasks is infeasible. Moreover, they are primarily designed for unimodal tasks, rely on downstream tasks for explanations, and lack adaptation to multimodal contexts. As a result, while traditional interpretability methods have made progress in unimodal tasks, there remains a significant gap in addressing multimodal tasks and novel models like CLIP, requiring further optimization and extension.

\subsection{Interpretability Methods for multimodal tasks}
Currently, existing interpretability methods for multimodal tasks still exhibit several limitations that require improvement, as shown in Table~\ref{tab.comp}. In the following sections, we will provide a detailed explanation of the causes and effects of these limitations.

\textbf{No Extra Example} indicates that no additional samples are required during the interpretation process, which is crucial because in real-world scenarios, we do not know what samples to select, nor can we explain why a particular pair of samples are correlated. For instance, if we aim to explain which parts of an image depict a cat, the image in the CLIP model already exhibits high activation with respect to the text \textit{cat}. Therefore, we should not need to reference 100 additional images of cats and 100 images without cats. A well-trained model that already understands the semantics should not require such sampling. \textbf{No Randomness} means that the calculation process involves no randomness, as randomness reduces trust in the interpretability method. \textbf{No Specific Structure} means that the method does not depend on a particular model structure. \textbf{No Info Loss} ensures that no information is lost during the interpretation process, such as interpreting only a subset of the model's output. \textbf{Current Model} indicates that the method explains the model as it currently exists, without constructing a new model for interpretation. \textbf{No Downstream Task} means that no downstream tasks are required for the explanation process.
\begin{table}[htpb]
\centering
\caption{Comparison of interpretability methods based on several criteria: whether they require no extra examples, no randomness, need no specific structure, avoid information loss, explain the current model, and don't rely on downstream tasks.}
\label{tab.comp}
\resizebox{\textwidth}{!}{%
\begin{tabular}{@{}lcccccc@{}}
\toprule
\textbf{Method}                       & \textbf{No Extra Example} &  \textbf{No Randomness}&\textbf{No Specific Structure} & \textbf{No Info Loss} & \textbf{Current Model} & \textbf{No Downstream Task} \\ \midrule
COCOA                                 & {\LARGE $\circ$}          &  {\LARGE $\circ$}&{\LARGE $\bullet$}             & {\LARGE $\circ$}      & {\LARGE $\bullet$}     & {\LARGE $\bullet$}          \\
TEXTSPAN                              & {\LARGE $\circ$}          &  {\LARGE $\circ$}&{\LARGE $\circ$}               & {\LARGE $\circ$}      & {\LARGE $\bullet$}     & {\LARGE $\bullet$}          \\
\cite{hossainexplaining} & {\LARGE $\circ$}          &  {\LARGE $\circ$}&{\LARGE $\bullet$}             & {\LARGE $\circ$}      & {\LARGE $\bullet$}     & {\LARGE $\bullet$}          \\
LICO                                  & {\LARGE $\circ$}          &  {\LARGE $\circ$}&{\LARGE $\bullet$}             & {\LARGE $\circ$}      & {\LARGE $\circ$}       & {\LARGE $\circ$}            \\
FALCON                                & {\LARGE $\circ$}          &  {\LARGE $\circ$}&{\LARGE $\bullet$}             & {\LARGE $\bullet$}    & {\LARGE $\bullet$}     & {\LARGE $\bullet$}          \\
 M2IB                                  & {\LARGE $\bullet$}          & {\LARGE $\circ$}& {\LARGE $\bullet$}             & {\LARGE $\circ$}      & {\LARGE $\bullet$}     &{\LARGE $\bullet$}          \\
NIB (Ours)                            & {\LARGE $\bullet$}        &  {\LARGE $\bullet$}        &{\LARGE $\bullet$}             & {\LARGE $\bullet$}    & {\LARGE $\bullet$}     & {\LARGE $\bullet$}          \\ \bottomrule
\end{tabular}%
}
\end{table}


M2IB~\citep{wang2023visual} introduced a multimodal information bottleneck method aimed at explaining the decision-making process of vision-language pre-trained models by compressing task-irrelevant information to highlight key predictive features. However, this approach introduces additional complexity, which will be analyzed further when discussing the IBP theory. Similarly, COCOA~\citep{lin2022contrastive} extended Integrated Gradients (IG) to multimodal tasks by incorporating positive and negative sample pairs in its loss function, but this requires sampling additional relevant examples, introducing extraneous information that may not be directly relevant to explaining the current sample.

Other methods like TEXTSPAN~\citep{gandelsman2023interpreting} and \cite{hossainexplaining} also suffer from sample dependency. TEXTSPAN requires constructing a specific text set to calculate similarity with the image, limiting its scope to predefined sets, while \cite{hossainexplaining} relies on selecting training data samples based on L2 distance in the embedding space, which is not always feasible in practical settings. LICO~\citep{lei2024lico} attempts to create an interpretable model by retraining it to maintain feature relationships between text and image, but this results in explaining the newly trained model rather than the original one, and randomness is introduced through batch sampling. FALCON~\citep{kalibhat2023identifying} explains each dimension in the feature space by finding images that highly activate a specific feature, but this approach does not provide explanations for individual samples, limiting its applicability. Overall, many of these methods face challenges such as reliance on additional samples, randomness, or structural dependencies, making them less suitable for clear, direct explanations of pre-trained models.