\documentclass[journal]{IEEEtai}

\usepackage[colorlinks,urlcolor=blue,linkcolor=blue,citecolor=blue]{hyperref}

\usepackage{color,array}

\usepackage{graphicx}

%% \jvol{XX}
%% \jnum{XX}
%% \paper{1234567}
%% \pubyear{2020}
%% \publisheddate{xxxx 00, 0000}
%% \currentdate{xxxx 00, 0000}
%% \doiinfo{TQE.2020.Doi Number}

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\setcounter{page}{1}
%% \setcounter{secnumdepth}{0}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{amsmath}
\usepackage{booktabs}
\usepackage{multirow} 
\usepackage[table]{xcolor}
\setlength{\pdfpagewidth}{8.5in} % DO NOT CHANGE THIS
\setlength{\pdfpageheight}{11in} % DO NOT CHANGE THIS
\definecolor{cGreen}{RGB}{100,180,100}
\definecolor{cRed}{RGB}{220,50,0}
\definecolor{Klein_Blue}{rgb}{0.0, 0.129, 0.6}
\usepackage{newfloat}
\usepackage{listings}
\usepackage{amssymb}
\usepackage{cite}

\begin{document}


\title{Adaptive Perception for Unified Visual Multi-modal Object Tracking} 


\author{Xiantao Hu, Bineng Zhong$^{*}$, Qihua Liang, Zhiyi Mo, Liangtao Shi, Ying Tai, Jian Yang 

\thanks{Xiantao Hu, Bineng Zhong, Qihua Liang and  Liangtao Shi are with the Guangxi Key Lab of Multi-Source Information Mining \& Security, Guangxi Normal University, Guilin 541004, China. }
 \thanks{Zhiyi Mo is currently a Professor in the School of Data Science and Software Engineering, Wuzhou University, Wuzhou 543002, China.}
\thanks{Ying Tai is with the School of  intelligence Science and Technonolgy, Nanjing University, Nanjing 210008, China.}
\thanks{jian Yang is with PCA-Lab, School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing 210094,  China.}
% \thanks{S. B. Author, Jr., was with Rice University, Houston, TX 77005 USA. He is now with the Department of Physics, Colorado State University, Fort Collins, CO 80523 USA (e-mail: author@lamar.colostate.edu).}
% \thanks{T. C. Author is with the Electrical Engineering Department, University of Colorado, Boulder, CO 80309 USA, on leave from the National Research Institute for Metals, Tsukuba, Japan (e-mail: author@nrim.go.jp).}
% \thanks{This paragraph will include the Associate Editor who handled your paper.}

}

% \markboth{Journal of IEEE Transactions on Artificial Intelligence, Vol. 00, No. 0, Month 2020}
% {*** \MakeLowercase{\textit{et al.}}: Bare Demo of IEEEtai.cls for IEEE Journals of IEEE Transactions on Artificial Intelligence}

\maketitle

\begin{abstract}
Recently, many multi-modal trackers prioritize RGB as the dominant modality, treating other modalities as auxiliary, and fine-tuning separately various multi-modal tasks. This imbalance in modality dependence limits the ability of methods to dynamically utilize complementary information from each modality in complex scenarios, making it challenging to fully perceive the advantages of multi-modal. As a result, a unified parameter model often underperforms in various multi-modal tracking tasks. To address this issue, we propose APTrack, a novel unified tracker designed for multi-modal adaptive perception. Unlike previous methods, APTrack explores a unified representation through an equal modeling strategy. This strategy allows the model to dynamically adapt to various modalities and tasks without requiring additional fine-tuning between different tasks. Moreover, our tracker integrates an adaptive modality interaction (AMI) module that efficiently bridges cross-modality interactions by generating learnable tokens. Experiments conducted on five diverse multi-modal datasets (RGBT234, LasHeR, VisEvent, DepthTrack, and VOT-RGBD2022) demonstrate that APTrack not only surpasses existing state-of-the-art unified multi-modal trackers but also outperforms trackers designed for specific multi-modal tasks.
\end{abstract}

\begin{IEEEImpStatement}
Multi-modal tracking is a popular research topic in the field of computer vision. However, most of the current mainstream multi-modal trackers cannot efficiently use the unified parameters of a single model for tracking. By constructing an adaptively perceiving network, my tracker can effectively adapt to the situation where the modal advantages are constantly changing. It improves the accuracy and robustness of multi-modal tracking in complex scenes.
% The impact statement should not exceeed 150 words. This section offers an example that is expanded to have only and just 150 words to demonstrate the point. Here is an example on how to write an appropriate impact statement: Chatbots are a popular technology in online interaction. They reduce the load on human support teams and offer continuous 24-7 support to customers. However, recent usability research has demonstrated that 30\% of customers are unhappy with current chatbots due to their poor conversational capabilities and inability to emotionally engage customers. The natural language algorithms we introduce in this paper overcame these limitations. With a significant increase in user satisfaction to 92\% after adopting our algorithms, the technology is ready to support users in a wide variety of applications including government front shops, automatic tellers, and the gaming industry. It could offer an alternative way of interaction for some physically disable users.
\end{IEEEImpStatement}

\begin{IEEEkeywords}
Visual Object Tracking, Multi-modal Tracking.
\end{IEEEkeywords}



\section{Introduction}

\IEEEPARstart{V}{isual} Object Tracking (VOT) is a core task in the field of computer vision, defined as accurately locating any target in subsequent videos using an initial bounding box. It is essential for various tasks~\cite{fang2024guided, lian2023multi, bao2022emotion, anSHaRPoseSparseHighResolution2024, zhang2024few, yan2023dynamic, zheng2023curricular, ning20253d}. Current RGB-based trackers~\cite{aqatrack,evptrack,seqtrack,artrack,FFTrack,siamban,swintrack,CDTrack, xie2024robust} have achieved remarkable results, but inherent limitations in RGB imaging lead to suboptimal performance in challenging and complex scenes.
Therefore, some studies have begun exploring unified multi-modal trackers~\cite{vipt,untrack,onetracker} to fully exploit the complementary strengths of different modalities. These trackers primarily focus on developing a multi-modal framework based on the dominant-auxiliary design principle, aiming to effectively integrate information from various modalities—such as thermal infrared (TIR), event, and depth data into RGB features to enhance tracking performance.

\begin{figure}[t]
  \centering
    \includegraphics[width=1\linewidth,height=4.5cm]
    {fig/fig1.pdf}
   \caption{
APTrack is a unified multi-modal tracker that can be applied to various RGB-X tasks (such as RGB-Depth, RGB-TIR, and RGB-Event) with a unified parameters. }
   \label{fig:simple}
\end{figure}

Treating RGB as the dominant modality and others as auxiliary leads the model to overly rely on RGB, neglecting the importance of auxiliary modalities.
This imbalance prevents TIR, event, and depth modalities, which should dominate in challenging scenarios such as low-light conditions, rapid movement, and occlusion, from fully exerting their function.
This imbalance between modalities during the integration of different modal information can lead to reduced perception efficiency, making it more challenging to coordinate and fuse multi-modal features. This challenge hinders the performance of trackers based on unified parameters in handling multi-modal tasks, thereby limiting the model's effectiveness in practical applications and posing an obstacle to achieving a truly unified multi-modal framework.

To address these issues, we propose a unified multi-modal tracker that eliminates the need for a dominant modality. Instead of the traditional dominant-auxiliary approach, our method integrates multiple modalities equally. In other words, APTrack employs the strategy of adaptive perception. It can flexibly adjust the perception and processing based on the inherent features of different modalities and the actual conditions of specific input data. This strategy ensures that each modality is treated equally, allowing them to fully exert their unique characteristics.
For example, in some extremely complex scenarios, the information value contributed by different modalities will change. The adaptive perception mechanism does not rely on fixed weight settings to favor a certain modality, but is able to achieve a balanced utilization of the advantages of each modality.
% This approach promotes consistent representation of different modalities in the shared latent space, thereby enhancing the model's performance in complex scenarios.
Furthermore, to further enhance inter-modality correlation and improve modality alignment, we designed an adaptive modality interaction (AMI) module. This module explores a more efficient method of features interaction, enabling effective feature fusion with fewer tokens. Firstly, by learning the learnable tokens for each modality, the model can reduce redundant information and computational load in modality interaction. 
The learning process of these learnable tokens is dynamic, and it can be adjusted adaptively according to the change of input features, without setting fixed markers in advance. Additionally, the global modal perceptor integrates the advantages of each modality by learnable tokens, constructing an effective information flow during the interaction calculation.
% the learnable tokens learnable by the global modal perceptor set integrate the advantages of each modality, thereby constructing an effective information flow during the interaction calculation.
This flow is then embedded into the modality features, which enhances modality alignment and facilitates the mutual transmission of advantages between modalities.
% Secondly, during the interaction computation, we partition the tokens learned from two modalities into guiding modal tokens and guided modal tokens. By constructing guiding modality attention maps, the model can combine the information of the guiding modality with the information of the guided modality to produce an effective information flow. Then the information flow will be embedded into modal features, which will promote modal alignment and mutual transmission of modal advantages.

 
In summary, our contributions are as follows:
\begin{itemize}
\item We proposed APTrack, a unified multi-modal tracker that integrates multiple modalities through adaptive perception, eliminating the need for complex multitasking fine-tuning.
\item An adaptive modality interaction module (AMI) is introduced to dynamically adjust interaction features. This enables effective information transmission between modalities using fewer tokens, thereby improving the tracker's performance.
\item The proposed tracker undergoes extensive experimental validation on multiple benchmark datasets. It demonstrates leading performance in uniform training scenarios across multi-modal datasets, showcasing its potential and advantages in the field of multi-modal tracking.
\end{itemize}

\section{Related Works}
\subsection{Unified Multi-modal Tracking}
Recent RGB-based object trackers~\cite{simtrack,mixformer,ostrack,stark,transt} achieved significant advancements in large-scale benchmark tests~\cite{lasot,got10k,trackingnet}. However, the unstable reliability of RGB causes these types of trackers to struggle with complex scenarios such as low-light conditions. Meanwhile, with the development of multi-modal imaging technology and the reduction in costs, an increasing number of multi-modal datasets, such as RGB-T~\cite{lasher,rgbt234}, RGB-E~\cite{visevent}, and RGB-D~\cite{depthtrack,vot-rgbd}, provide possibilities for developing more robust trackers. 
Several works explored how to effectively integrate RGB with another modality. Specifically, TBSI~\cite{tbsi} model used a pair of template features as a mediator to bridge the relationship between TIR search features and RGB search features. BAT~\cite{bat} froze the backbone network and used adapters to transfer features from one modality to another, achieving adaptive fusion between RGB and TIR. STNet~\cite{stnet} combines event images and RGB images from both spatial and temporal perspectives. 
Besides some works have begun exploring unified multi-modal tracking architectures. Among them, ViPT~\cite{vipt} was an early adopter of the dominant-auxiliary modality method to fine-tune multiple multi-modal tracking tasks. OneTracker~\cite{onetracker} utilized large-scale benchmark training of the foundation tracker and incorporated auxiliary branches into the trained model to provide visual cues. On the other hand, Un-Track~\cite{untrack} achieved unified training for multi-modal tasks through low-rank factorization and reconstruction techniques. 
However, these methods still exhibit gaps compared to models specifically designed for single multi-modal~\cite{tbsi,bat,mctrack,hu2024exploiting} tasks.
% The main reason is the adoption of a dominant-auxiliary modalities modeling approach, making the models more sensitive to features from the dominant modality while insufficiently attending to features from the auxiliary modalities, thus failing to effectively align multi-modal features.

\begin{figure*}
    \centering
    \includegraphics[height=9.5cm]{fig/structure.pdf}
    \caption{The overall structure of APTrack. APTrack is composed of shared embedding, shared transformer block, AMI and Head. The method of modal processing in this model is completely consistent, and there is no need for extra processing for a certain model, which makes the modal features can be aligned adaptively. In addition, AMI can transfer the advantages of modalities to each other.
 }
  \label{fig:struct} 
\end{figure*}


\subsection{Adaptive Perception}
Recent research~\cite{imagebind,mutex,unified,clip} has shown that through extensive training on vast amounts of datasets, models can acquire the capability to align multiple modalities. However, due to the scarcity of multi-modal tracking datasets, it is not feasible to rely on large-scale data for training.
Therefore, recently unified multi-modal tracker~\cite{vipt,untrack,onetracker,sdstrack} typically rely on the weights of pre-trained RGB-based tracker, optimizing multi-modal task performance by aligning auxiliary modality features with RGB features. However, simply mapping auxiliary modalities to the RGB latent space is insufficient to fully harness their potential, making it challenging to effectively perceive and leverage the advantages of multi-modality. Furthermore, in the modeling part of its shared weights encoder, SDSTrack~\cite{sdstrack} has a bias in the asymmetrical adapter modeling of the X modality and focuses on multitask fine-tuning, while lacking exploration of the application of adaptive perception in multi-modal tasks under uniform parameters. Moreover, research~\cite{learning_li,siamese_av,unified_av}  has shown that using shared-weight local networks for multi-modal feature perception provides significant advantages, particularly in consistent modeling at certain stages. In RGB-T trackers~\cite{tbsi,bat}, this shared feature extraction network has shown a significant performance improvement by virtue of adaptive perception. Based on this observation, we pose a critical question: Can adaptive perception further enhance the performance of unified multi-modal trackers? To explore this, we investigate a strategy that avoids fixing a dominant modality and adopts a consistent modeling approach, aiming to improve the performance of unified multi-modal trackers without the need for separate fine-tuning for each task. In other words, rather than focusing on a single task, our goal is to focus on improving the performance of the unified parameter model to enable its easy deployment in diverse scenarios.

% Therefore, a common practice is to fine-tune tracking models pretrained on RGB for multi-modal tasks. 
% Recent methods~\cite{vipt,untrack,oneTracker,sdstrack} aim to fully leverage the potential of pretraining on RGB by typically freezing the parameters of the RGB extraction network. They then add additional modules to extract auxiliary modality features and fuse them into RGB. These method unilaterally aligns auxiliary modalities to RGB to ensure the effectiveness of the model in multi-modal tasks. However, simply mapping auxiliary modalities to the potential space of RGB does not fully exploit the potential of auxiliary modalities. Meanwhile, 
% recent studies~\cite{learning_li,siamese_av,unified_av} have demonstrated the effectiveness of shared-weight feature partial networks in perceiving multi-modal features. However, this approach has not been fully explored in multi-modal tracking frameworks. Inspired by these findings, we further investigated the potential of a consistent modeling strategy for achieving unified multi-modal tracking through adaptive perception. 

% some studies have demonstrated the effectiveness of methods that share weights across modalities for features alignment. 
% Furthermore, the alignment between multi-modal features becomes more challenging due to the frozen RGB feature extraction network, leading to the model's inability to fully utilize multi-modal features. 

 % By maximizing the distance between different semantic features in the potential space while minimizing the distance between the same semantic features, the model can better align multi-modal features in the potential space. 
 % By exploring fully symmetric modeling of multi-modal methods, we have demonstrated that well-designed modality alignment strategies can effectively achieve unified multi-modal representation learning. This unified model surpasses algorithms designed for specific multi-modal tasks in both fine-tuning for joint training.

 

\section{Methodology}
\subsection{Overview}
We introduce a adaptive perception tracking method named APTrack. The overall structure, as illustrated in Fig.~\ref{fig:struct}, shows that APTrack uses an equal modeling strategy, which enables it to quickly and efficiently align features between different modalities, thereby improving the accuracy and stability of multi-modal tracking. Additionally, we introduce an efficient modality interaction module that fully utilizes the advantages of each modality for modality guided. Below, we'll denote modalities other than RGB with the symbol $x$, which could encompass depth, TIR and event data.
\subsection{Adaptive Perception Network}
 % the input of APTrack includes two modalities along with their initial target states and consecutive video frames for tracking. 
% Different from the input of RGB tracking,
% APTrack allow the model to autonomously choose the optimal modality based on the input by treating the two input modalities consistently without introducing additional bias. 
APTrack employs a consistent modeling approach through the design of adaptive perception to ensure that no individual bias is introduced to any modality. This strategy allows the model to be consistent in processing both input modalities, thus autonomously selecting the best modality based on the input without introducing additional bias.
This approach allows for greater flexibility in adapting to the effectiveness of each modality in different environments.
The input to APTrack comprise two template images $t^{x} \in \mathbb{R}^{3 \times H_t \times W_t}$ and $t^{r} \in \mathbb{R}^{3 \times H_t \times W_t}$ , and two search images  $s^{x} \in \mathbb{R}^{3 \times H_s \times W_s} $ and $s^{r} \in \mathbb{R}^{3 \times H_s \times W_s}$.  
Before the tracking process, the search frames and template frames of the two modalities are tokenized ($t_{x} \in \mathbb{R}^{N_t \times (3 \cdot P^2)}$ , $t_{r} \in \mathbb{R}^{N_t \times (3 \cdot P^2)}$ , $s_{x} \in \mathbb{R}^{N_s \times (3 \cdot P^2)} $ and $s_{r} \in \mathbb{R}^{N_s \times (3 \cdot P^2)}$). Where $P \times P$ is the resolution of each patch. $N_s$ and $N_t$ represent the number of patches for the search and template images, respectively.
Then the same modal data splicing together (that is, $H_ {x} = (t_ {x}, s_ {x}) $ and $H_ {r} = (t_ {r}, s_ {r}) $), and the use of a trainable linear layer with shared weights is employed to project both $H_{x}$ and $H_{r}$ into a D-dimensional space.
% Then data from the same modality are spliced together(ie, $H_{x} = (t_{x},s_{x})$ and $H_{r} = (t_{r},s_{r})$) and separately input into the feature extraction network. Subsequently, a trainable linear layer with identical weights is employed to project both $H_{x}$ and $H_{r}$ into a D-dimensional space. 
Additionally, the same learnable location embedding $P$ is added to generate the RGB tokens $H_{r}^0$ and the X modality tokens $H_{x}^0$. The calculation process is as follows:

    \begin{equation}  H_{r}^0 = [t_{r}^{1}{E};\cdots;t_{r}^{N1}{E};s_{r}^{1}{E};\cdots;s_{r}^{N2}{E}] + p,
    \label{eq:eq2}
    \end{equation}
   \begin{equation}  H_{x}^0 = [t_{x}^{1}{E};\cdots;t_{x}^{N1}{E};s_{x}^{1}{E};\cdots;s_{x}^{N2}{E}] + p ,  \label{eq:eq1} \end{equation}
where ${E} \in \mathbb{R}^{\left(3 \cdot P^{2}\right) \times D}$, $P \in {\mathbb{R}^{N \times D}}$, $N1$ is the number of template tokens and $N2$ is the number of search tokens.

Then $H_{r}^0$  and $H_{x}^0$ will be fed into the transformer block which consists of Multi-head Self-Attention (MSA), LayerNorm (LN), MultiLayer Perceptron (MLP). In this part each layer of the block will also share the weights and process the two modalities separately, the computation process is as follows:

  \begin{equation}
  {h^l} = {H^{l - 1}} + MSA(LN({H^{l - 1}})),
  \end{equation}
  \begin{equation}
    {H^l} = {h^l} + MLP(LN({h^l})),
  \end{equation}
where ${H^{l - 1}} \in [{{H_{r}^{l - 1}},{H_{x}^{l - 1}}}]$ and ${H^{l}} \in [{{H_{r}^{l}},{H_{x}^{l}}}]$. $l$ represent the tansformer block layer number.

In addition, independent modal extraction, lack of communication between modalities, will reduce the multi-modal synergy. So we add adaptive modality interaction module after part of transformer block. The module remains consistent in handling both modes. The purpose of this is to realize the effective transfer of advantages without favoring a certain modality, so that the model can combine the advantages of other models  in the process of interaction.
 \begin{equation}
 {H_r}^l = {H_r}^l + {\rm{AMI(}}H^l_r,H_x^l). \end{equation}
 \begin{equation}
 {H_x}^l = {H_x}^l + {\rm{AMI(}}H^l_x,H_r^l). \end{equation}





\subsection{Adaptive Modality Interaction}
 AMI module mainly includes three stages: token learning, global modal perception and token embedding. Among them, learning token is to reduce the amount of calculation and redundant information, while global modal perceptor is to build an information bridge between modalities. In addition, token embedding is to transfer the information obtained from interaction to modal features. The token learning and
token embedding constitute learnable token mechanism.

\noindent \textbf{Token Learning.} Full-sequence tokens interaction will bring more interaction computation and introduce redundant information. At this stage, the model will learning few tokens through modal features. Specifically, a set of adaptive learnable token is used to significantly reduce the amount of computation in the interaction phase while aggregating features. The model learns the spatial attention weight generated by the input $X$ as the condition, and multiplies it with the $X$ itself. The calculation process is as follows:
 \begin{equation}
F^i =  \gamma ({\alpha _i}({X}))^T \times  X,
\end{equation}
where $F^i$ is the i-th learnable tokens. Linear function $\alpha _i$ is used for intermediate weight tensor computed, $\gamma (\cdot )$ is softmax function. Both modalities will be calculated, and the respective learned token are spliced to generate a new feature sequence $F_{r} \in \mathbb{R}^{N_{t} \times C}$ and $F_{x} \in \mathbb{R}^{N_{t} \times C}$. $ {N_{t}} $ is the number of learnable tokens, and C is the number of channel.

The learnable tokens are used to construct modal interactive information. In this way, subsequent operations require only a small number of tokens to perform efficient modal interaction, significantly reducing the amount of computation.


\begin{figure}[t]
  \centering
    \includegraphics[width=1\linewidth]
    {fig/perceptor.pdf}
   \caption{The detailed architecture of Global Modal Perceptor. These perceptors use attention mechanisms to enforce multi-modal global attention. }
   \label{fig:perceptor}
\end{figure}


\noindent \textbf{Global Modal Perceptor.} In this stage, we designed a global modal preceptor for cross-modal interaction, where the learnable tokens serve as the input to the preceptor. As shown in Fig.~\ref{fig:perceptor}, the preceptor includes two core attention mechanisms: (1) Query-guided cross-modal attention (Q-Attention) and (2) Key-Value-guided cross-modal attention (KV-Attention). Within the preceptor, the two modalities interact by alternately serving as the input and other modal. 
 % 
 Q-Attention performs cross-modal attention by merging the query vectors of the current modality with those of the other modality. This mechanism generates new query vectors $\hat{Q}$ to focus on important information within the input modal. KV-Attention generates the $\hat{K}$ and $\hat{V}$ vectors by merging the key and value vectors from both modalities, thereby facilitating the retrieval of complementary information between them. The process can be described as follows:
 
 \begin{equation}
\begin{gathered}
W^Q = Attention(\hat{Q},K,V), \\
W^{KV} = Attention(Q,\hat{K},\hat{V})
  \end{gathered}
\end{equation}

Subsequently, $W^Q$ and $ W^{KV}$ are fused through a linear layer and fed back into the input learnable tokens, enhancing the information flow with augmented modal information. 
Importantly, this process is reciprocal, with each modality taking turns as the input modality for independent computation.
\begin{table*}[t]\footnotesize
  \centering
  % \vspace{-2mm}
    \caption{State-of-the-art comparison on LasHeR~\cite{lasher} (RGB-T), VisEvent~\cite{visevent} (RGB-E), and DepthTrack~\cite{depthtrack} (RGB-D) benchmarks. We use \textcolor{gray}{gray} color to denote our trackers. The best three real-time results are shown in \textbf{\textcolor{cRed}{red}}, \textcolor{blue}{blue} and \textcolor{cGreen}{green} fonts.}
  \setlength{\tabcolsep}{3mm}{  
  \resizebox{0.9\textwidth}{!}{
  \begin{tabular}{c|c|c|c cc c cc c ccc}
    \toprule
    & \multirow{2}*{Method} & \multirow{2}*{Source} &  \multicolumn{2}{c}{LasHeR} & & \multicolumn{2}{c}{VisEvent} & &   \multicolumn{3}{c}{DepthTrack} \\
    \cline{4-5}
    \cline{7-8}
    \cline{10-12}
    % \cline{15-17}
   & & & Suc. &Pr. & &Suc. &Pre.  &  & F-score&Re. &Pr. \\
    \midrule[0.5pt]
    \multirow{7}*{\rotatebox{90}{Unified}}
     & \cellcolor{gray!15}APTrack &\cellcolor{gray!15}Ours &\cellcolor{gray!15}\textcolor{cRed}{58.9} &\cellcolor{gray!15}\textcolor{cRed}{74.1} &\cellcolor{gray!15} &\cellcolor{gray!15}\textcolor{cRed}{61.8} &\cellcolor{gray!15}\textcolor{blue}{78.5} &\cellcolor{gray!15} &\cellcolor{gray!15}\textcolor{cRed}{62.1} &\cellcolor{gray!15}\textcolor{cRed}{61.9} &\cellcolor{gray!15}\textcolor{cRed}{62.3}\\
     & Un-Track~\cite{untrack} &CVPR24 &51.3 &64.6 & &58.9 &75.5 & &\textcolor{cGreen}{61.0} &\textcolor{blue}{61.0} &\textcolor{cGreen}{61.0}\\
     & SeqTrack~\cite{seqtrack} &CVPR23 &49.0 &60.8 & &50.4 &66.5 & &59.0 &60.0 &58.0\\
     & ViPT~\cite{vipt} &CVPR23 &49.0 &60.8& &57.9 &74.0 & &56.1 &56.2 &56.0\\
     & OSTrack~\cite{ostrack} &ECCV22 &42.2 &53.0 & &52.5 &69.1 & &56.9 &58.2 &55.7\\
     & AiATrack~\cite{aiatrack} &ECCV22  &36.5 &46.3 & &44.4 &62.6 & &51.5 &52.6 &50.5\\
     & Stark~\cite{stark} &ICCV21 &33.3 &41.8 & &52.5 &44.8 & &39.7 &40.6 &38.8\\
     \midrule[0.5pt]
      \multirow{7}*{\rotatebox{90}{Specific}}
      & AINet~\cite{AINet} &AAAI25 &\textcolor{blue}{58.2} &\textcolor{blue}{73.0} & &- &- & &- &- &- \\
      & CAFormer~\cite{CAFormer} &AAAI25 &55.6 &70.0 & &- &- & &- &- &- \\
    & OneTracker~\cite{onetracker} &CVPR24 &53.8 &66.5 & &\textcolor{blue}{60.8} &\textcolor{cRed}{78.6} & &60.9 &60.4 &60.7\\
     & SDSTrack~\cite{sdstrack} &CVPR24 &53.1 &66.7 & &\textcolor{cGreen}{59.7} &\textcolor{cGreen}{76.7} & &\textcolor{blue}{61.4} &\textcolor{cGreen}{60.9} &\textcolor{blue}{61.9}\\
     & BAT~\cite{bat} &AAAI24 &56.3 &70.2 & &- &- & &- &- &-\\
     & GMMT~\cite{gmmt} &AAAI24 &\textcolor{cGreen}{56.6} &\textcolor{cGreen}{70.7} & &- &- & &- &- &-\\
     & Depthrefiner ~\cite{dualrgbd_depthrefiner}  &ICME24 &- &- & &- &- & &51.0 &50.7 &51.3 \\
     & MMHT~\cite{dualevent_mmht} &Arxiv24 &- &- & &55.3 &73.4 & &- &- &- \\
     & ViPT~\cite{vipt} &  CVPR23 &52.5 &65.1 & &59.2 &75.8 & &59.4 &59.6 &59.2\\
     & TBSI~\cite{tbsi} &CVPR23 &56.3 &70.5 & &- &- & &- &- &-\\
     & ProTrack~\cite{protrack} &  MM22 &42.0 &53.8 & &47.1 &63.2 & &57.8 &57.3 &58.3\\
\bottomrule

\end{tabular}
  
}


  \label{tab:rgb-x}
}
\vspace{-2.5mm}
\end{table*}


   \begin{table*}[t]
    \centering
    \caption{Overall performance on VOT-RGBD2022~\cite{vot-rgbd}. The best three results are highlighted in {\color{cRed}red}, {\color{blue}blue}, {\color{cGreen}green}, respectively.}
    \resizebox{1\linewidth}{!}{
    % \setlength{\tabcolsep}{1mm}
    \begin{tabular}{c|cccccc|cccccc}
     \toprule
    & \multicolumn{6}{c|}{Specific Parameters} & \multicolumn{6}{c}{Unified Parameters}\\
    \toprule
       \multirow{2}*{Method} &DeT &SPT &ProTrack &ViPT  &OneTracker &SDSTrack &Stark &AiATrack  &OSTrack  &SeqTrack  &Un-Track   &APTrack \\
       &\cite{det} &\cite{spt} &\cite{protrack} &\cite{vipt} &\cite{onetracker} &\cite{sdstrack} &\cite{stark} &\cite{aiatrack} &\cite{ostrack} &\cite{seqtrack} &\cite{untrack} &\textbf{(Ours)}
       \\ 
       \toprule
       EAO($\uparrow$) &65.7 &65.1 &65.1 &72.1 &{\color{cGreen}72.7} &{\color{blue}72.8} &44.5 &64.1
       &66.6 &67.9 &71.8 &{\color{cRed}77.4}\\ 
       Accuracy($\uparrow$)&76.0 & 79.8 &80.1 &81.5&{\color{cGreen}81.9}&81.2 &71.4 &76.9
       
       &80.8&80.2 &{\color{blue}82.0} &{\color{cRed}82.1}\\  
        Robustness($\uparrow$) &84.5 &85.1 &80.2 &87.1 &{\color{cGreen}87.2}&{\color{blue}88.3}
        &59.8 &83.2
        
        &81.4 &84.6 &86.4 &{\color{cRed}93.4}\\
       % &~\cite{drefine} &~\cite{aiatrack} &~\cite{spt} &~\cite{protrack}   &~\cite{stark}      &~\cite{seqtrack}  &~\cite{untrack}   &~\cite{vipt}   &~\cite{oneTracker}  &~\cite{sdstrack}  &(ours)
       %  &(ours) \\
   %  \midrule
   %  EAO($\uparrow$) &0.592 &0.641 &0.651 &0.651  &0.666  &0.679 &0.718 &0.721 & 0.727      &{\color{green}0.728} &  {\color{red}0.768} & {\color{blue}0.766} \\
   % Accuracy($\uparrow$) &0.775 &0.769 &0.798 &0.801  &0.808  &0.802 &{\color{red}0.820} &{\color{green}0.815} & {\color{blue}0.819}       &0.812 &{\color{blue}0.819} & {\color{blue}0.819}  \\
   %     Robustness($\uparrow$) &0.760 &0.832 &0.851 &0.802  &0.814  &0.846  &0.864 &0.871 & 0.872       &{\color{green}0.883} &  {\color{red}0.925} & {\color{blue}0.924}  \\
    \bottomrule
    \end{tabular}
    }        
    \label{tab:vot-rgbd2022}
    \end{table*}

\noindent \textbf{Token Embedding.} Modal interaction is essentially a process of mutual notification. One modality will inform another modality of its own advantageous information about the target, and the other modality will integrate the advantageous information with its own mined features. After we use the learnable tokens to generate interactive information, we need to embed the information into the input features of the AMI module. We use $F \in \mathbb{R}^{{N_t} \times C}$  to refer to the interactive information of ones modal, and $H^{l+1} \in \mathbb{R}^{{N} \times C}$ to represent the feature sequence with N number of token input into the AMI. Specifically, we embed learnable tokens into $H^{l}$ by learning to combine tokens at each spatial location.
\begin{equation}
{H^{l + 1}} = {B_w}F + {H^{l}},
 \end{equation}
where ${B_w \in \mathbb{R}^{N_t \times N_t}}$ is implemented through simple linear layer and a softmax function, it is the spatial weight tensor.



\subsection{Prediction Head and Loss Function}
\noindent \textbf{Prediction Head.} Because of the adaptive perception modeling method and effective modal interaction strategy, the model can accurately extract the multi-modal alignment feature.
So it is only necessary to simply splice the two modal features, and accurate target positioning can be realized in the prediction head. 
Specifically, we employ conventional classification heads and bounding box regression. The classification score map, denoted by $ P \in \mathbb{R}^{ \frac{H_{s}}{P} \times \frac{W_{s}}{P}}$, the local offset, symbolized as $  O \in \mathbb{R}^{2 \times \frac{H_{s}}{P} \times \frac{W_{s}}{P}}$, and the bounding box size, represented by $  S \in \mathbb{R}^{ 2 \times \frac{H_{s}}{P} \times \frac{W_{s}}{P}}$, are all obtained by the prediction modules, respectively. \
Obtain accurate tracking through rough localization, offset, and box size,as
 \begin{equation} 
    \begin{aligned}
     & x = x_{d} + O(0,x_{d},y_{d}), \\
     & y = y_{d} + O(1,x_{d},y_{d}), \\
     & w = S(0,x_{d},y_{d}), \\
     & h = S(1,x_{d},y_{d}), \\
    \end{aligned}
    \label{eq:bbox}
     \end{equation}
where $(x_{d},y_{d})$ denotes the coordinates which is the highest
score of classification score map.

\noindent \textbf{Loss Function.} We select focal loss~\cite{focal_loss} as our classification loss, $L_{cls}$, while assigning the $L_1$ loss and GIoU loss~\cite{giou} as our regression loss types. Consequently, the comprehensive loss L is determined as follows:
    \begin{equation}
    L = L_{cls} + \lambda_{1}L_{1} + \lambda_{2}L_{GIoU},
    \end{equation}
where $\lambda_{1}=5 $ and  $\lambda_{2}=2 $ are the regularization parameters.
 




\begin{figure*}[t]
  \centering
    \includegraphics[width=1\linewidth]
    {fig/tracker_contrast.pdf}
   \caption{Visual result of RGB-T. The three sequences from top to bottom represent the following scenarios: both modalities exhibit low effectiveness, the RGB modality is less effective than the TIR modality, and the TIR modality is less effective than the RGB modality. }
   \label{fig:tracker_contrast}
\end{figure*}





\begin{figure}
  \centering
    \includegraphics[width=1\linewidth,height=5cm]
    {fig/RGBT234.pdf}
   \caption{More MPR/MSR comparisons on RGBT234. }
   \label{fig:rgbt234}
\end{figure}


\section{Experiment}
\subsection{Implementation Details}

To establish a truly unified multi-modal tracking framework, we developed a versatile RGB-X tracker capable of flexibly addressing a range of multi-modal tasks, including RGB-T, RGB-D, and RGB-E tracking. The training process incorporated the VisEvent~\cite{visevent}, DepthTrack~\cite{depthtrack}, and LasHeR~\cite{lasher} datasets. We conducted the training on four NVIDIA GeForce RTX 4090 GPUs, running for 20 epochs. Each epoch comprised 60,000 sample pairs with a batch size of 16, using AdamW~\cite{adamw} as the optimizer. The initial learning rate was set at 1e-4 for the AMI module and 1e-5 for other parameters. After 10 epochs, the learning rate was reduced by a factor of 10. It is worth noting that our tracker employs a dual-template mechanism, which consists of an initial template frame and a dynamic template frame. By default, the dynamic template will be updated when the update interval reaches 5 and the classification score exceeds 0.65. Specifically, the initial template frame remains constant during the tracking process, while the dynamic template frame is updated according to the confidence level of the tracking results.

% VisEvent~\cite{visevent} is used for the training of RGB-E task; and Depthtrack~\cite{depthtrack} is used for the training of RGB-D task. And in the joint training scheme, we use the above three data together as training data. In order to demonstrate the effectiveness of APTrack, we did not employ additional tuning for a specific task, but used the same training scheme for both specific fine-tuning and joint training. Specifically, we trained the model on four NVIDIA 4090 GPUs for a total of 20 epochs. Each epoch contains 60,000 sample pairs with a batch size of 16, and we used AdamW~\cite{adamw} as the optimizer. We set the initial learning rate of the AMI module to 1e-4 and other parameters to 1e-5. After 10 epochs, we reduced the learning rate by a factor of 10. 
 We tested our model on an NVIDIA GeForce RTX 4090, achieving a runtime speed of 50.5 FPS. A single model with unified parameters was used for multitasking evaluation. For the RGB-T task, we tested on LasHeR~\cite{lasher} and RGBT234~\cite{rgbt234}. The RGB-E task was evaluated using VisEvent~\cite{visevent}, while the RGB-D task was tested on DepthTrack~\cite{depthtrack} and VOT-RGBD2022~\cite{vot-rgbd}. For comparison purposes, we categorize the models into two types: Specific parameter models that require fine-tuning for specific tasks, and unified parameter models that do not require any fine-tuning. 



    
\subsection{Comparison with State-of-the-arts}

\noindent \textbf{LasHeR}. LasHeR~\cite{lasher} benchmark is a large RGB-T tracking dataset containing 1224 aligned sequences with a total of over 2$\times$730K frames, caputered in various types of imaging platforms.
This dataset covers a wide range of object categories, camera perspectives, scene complexities, and environmental factors, while taking into account various real-world challenges.  We compared with recent high-performance multi-modal trackers, including AINet~\cite{AINet}, CAFormer~\cite{CAFormer}, GMMT~\cite{gmmt}, BAT~\cite{bat}, TBSI~\cite{tbsi}, ViPT~\cite{vipt}, OneTracker~\cite{onetracker}, SDSTrack~\cite{sdstrack}, ProTrack~\cite{protrack} and Un-Track~\cite{untrack}. It is worth noting that the GMMT~\cite{gmmt}, BAT~\cite{bat} and TBSI~\cite{tbsi} is specifically designed for RGB-T tracking tasks.
As shown in Tab.~\ref{tab:rgb-x}, APTrack outperformed AINet~\cite{AINet} in both precision and success, achieving 1.1\% and 0.7\% improvements, respectively. The results from the large-scale RGB-T dataset LasHeR~\cite{lasher} not only verifies the validity of our model, but also fully demonstrates that adaptive perception modeling method and effective modal interaction module can significantly boost performance.  
% In addition, in the multi-modal joint training scenario, our Un-APTrack exceeds the Unified model UnTrack(uni) by 8.7\% in precision. At the same time, in certain fine-tuning scenarios, our APTrack significantly exceeds the unified model UnTrack 8.2\% in precision. 

\noindent \textbf{VisEvent.} VisEvent~\cite{visevent} contains 500 training video sequences and 320 testing video sequences, which is currently the largest RGB-E dataset. In VisEvent~\cite{visevent}, we compared with recent high-performance multi-modal trackers, including ViPT~\cite{vipt}, SDSTrack~\cite{sdstrack}, ProTrack~\cite{protrack} OneTracker~\cite{onetracker}, Depthrefiner~\cite{dualrgbd_depthrefiner} and Un-Track~\cite{untrack}.
As shown in Tab.~\ref{tab:rgb-x}, under the same strategy of unified parameters model, our tracker exceeds Un-Track~\cite{untrack} by 2.9\% in success. This is due to the fact that our network does not have a fixed pattern of dominant-auxiliary modes, allowing the model to adaptively make choices of modes in complex changing dynamic scenarios.
% under the same strategy of fine-tuning the multi-modal task separately, our tracker exceeds OneTracker by 3.7\% in Precision, and also achieves a 1.5\% increase in success.

\noindent \textbf{DepthTrack.} DepthTrack~\cite{depthtrack} is a long-time tracking dataset in which the average sequence length is 1473 frames. The dataset covers 200 sequences, 40 scenes and 90 target objects.
In Depthtrack~\cite{depthtrack}, precision (Pr), recall (Re), and F-score are used to measure the tracker performances. where F-score, calculated by $f = \frac{2RePr}{Re+Pr}$, is the primary measure. In DepthTrack~\cite{depthtrack}, we compared with recent high-performance multi-modal trackers, including SDSTrack~\cite{sdstrack},  ViPT~\cite{vipt}, ProTrack~\cite{protrack} OneTracker~\cite{onetracker}, MMHT~\cite{dualevent_mmht}  and Un-Track~\cite{untrack}.
As shown in Tab.~\ref{tab:rgb-x}, compared to SDSTrack~\cite{sdstrack}, APTrack benefits from the ability to adaptively gain modal advantage in complex and variable dynamic scenes, achieving a 0.7% increase in F - score, a 1.0% increase in recall and a 0.4% increase in precision.

 
 \noindent \textbf{RGBT234.} The RGBT234~\cite{rgbt234} benchmark extends the RGBT210~\cite{rgbt210} dataset, providing enriched annotations and a wider array of environmental challenges.
It comprises a total of 234 sequences, consisting of 234K highly aligned pairs of RGBT videos with a maximum of 8K frames per sequence, and offers 12 attributes. 
Due to inconsistencies between the ground truths of RGB and TIR in this dataset, the evaluation methodology of RGBT210 is discarded. Instead, the benchmark employs the Maximum Precision Rate (MPR) and Maximum Success Rate (MSR) as assessment metrics, offering a more comprehensive evaluation of tracker performance. 
Specifically, for each frame, accuracy is determined by calculating the Euclidean distance between the predicted bounding box and the ground truth separately in RGB and TIR modalities, selecting the smaller distance for computation. 
As shown in Fig.~\ref{fig:rgbt234}, in the RGB-T domain, only APTrack outperforms the models specifically designed for RGB-T among the unified models. Specifically, our tracker outperformed GMMT~\cite{gmmt} by 1.4\% in MSR.


 \noindent \textbf{VOT-RGBD2022}. VOT-RGBD2022~\cite{vot-rgbd} consists of 127 short RGB-D sequences. It employs Accuracy, Robustness, and Expected Average Overlap (EAO) as evaluation metrics. As shown in Tab.~\ref{tab:vot-rgbd2022}, APTrack exceeds the SDStrack~\cite{sdstrack} by 4.6\% in EAO.


% \textbf{Token Update Analysis.}


% \begin{table}
%   \centering


%   \begin{tabular}{c|c|cc}
% \toprule
% Variants &Template Update  &Precision &Success \\
% \midrule
% 1 &$\times$  &0.732 &0.582 \\
% 2 &\checkmark    &0.749 &0.595 \\
% \midrule
% \end{tabular}


% \caption{Ablation studies on the effect of various components on LasHeR~\cite{lasher} testing set. CA, MA,LT denote cross attention, mutialmodal attention, and learnable token.}
% \label{tab:ablation_components}
% \end{table}



% Furthermore, as Fig.~\ref{fig:attribute} shows the results of attribute evaluation, demonstrating that our tracker outperforms other tracking methods on multiple challenge attributes. These results indicate that a fully symmetric mechanism helps the model learn the ability to align multi-modal features in latent space and improve the accuracy of target localization in auxiliary tracking scenarios.
\begin{table}
  \centering
\caption{Ablation studies on the effect of various components on LasHeR~\cite{lasher} testing set. AP, GMP, LT denote adaptive perception structure, global modal perceptor, and learnable token mechanism (include token learning and token embedding). We use \textcolor{gray}{gray} color to denote our final trackers setting.}
 \resizebox{1\linewidth}{!}{
  \setlength{\tabcolsep}{2mm}{
  \begin{tabular}{c|cccc|cc}
\toprule
Variants &Baseline &AP  & GMP  & LT &Precision &Success \\
\midrule
1 &\checkmark  &- &- &- &65.7 &52.3 \\
2 &\checkmark  & \checkmark   &- &- &71.3 &56.9 \\
3 &-   &\checkmark & \checkmark  &- &72.9 &58.7\\
4 &-  &\checkmark  & -  &\checkmark  &73.1 &58.1 \\
\cellcolor{gray!15}5 &\cellcolor{gray!15}-  &\cellcolor{gray!15}\checkmark  & \cellcolor{gray!15}\checkmark  &\cellcolor{gray!15}\checkmark  &\cellcolor{gray!15}74.1 &\cellcolor{gray!15}58.9 \\
\midrule
\end{tabular}
}
}

\label{tab:ablation_components}
\end{table}




\begin{table}
  \centering
\caption{Ablation studies about number of learnable tokens. We use \textcolor{gray}{gray} color to denote our final trackers setting. }
 \resizebox{0.6\linewidth}{!}{
  \setlength{\tabcolsep}{2mm}{
  \begin{tabular}{c|cc}
\toprule
  Number   &Precision &Success \\

\midrule
 0 &72.9 &58.7\\
16 &73.2 &58.3\\
\cellcolor{gray!15}32 &\cellcolor{gray!15}74.1 &\cellcolor{gray!15}58.9\\
64 &73.9 &58.6 \\ 
\midrule
\end{tabular}
}}

\label{tab:number}
\end{table}

\begin{table}
  \centering
  \caption{Comparison of inference speed with sota modal. All methods are tested in an environment identical to our APTrack. }
 % \scalebox{1}{
  \resizebox{0.75\linewidth}{!}{
  \begin{tabular}{c|c|c|cc}
\toprule
Methond  &FPS  &Precision &Success \\

\midrule
ViPT~\cite{vipt} &41.5  &65.1 &52.5\\
SDSTrack~\cite{sdstrack} &23.5 &66.5 &53.1\\
BAT~\cite{bat} &47.8 &70.2 &56.3\\
TBSI~\cite{tbsi} &48.0 &70.2 &56.5\\
\midrule
APTrack (ours) &\textbf{50.5} &\textbf{74.1} &\textbf{58.9}\\
\midrule
\end{tabular}
}

\label{tab:fps}
\end{table}




\begin{figure*}[t]
  \centering
    \includegraphics[width=1\linewidth,height=10.5cm]
    {fig/tracker_contrast_other.pdf}
   \caption{Visualization comparison of our method with other unified multi-modal trackers on RGB-E and RGB-D tasks. Four challenging scenarios including occlusion, similar object interference, cluttered backgrounds and small targets are shown in from top to bottom. }
   \label{fig:contrast}
\end{figure*}





% 因为我们的模型具有统一的特性，即同一个模型参数可以适配多个任务。因此在本节

\subsection{Exploration Study and Analysis.}
Our model exhibits a unified characteristic, meaning that the same set of parameters can be adapted to multiple tasks. In this section, we refer to Un-Track~\cite{untrack} to perform ablation experiments on a single multi-modal task, specifically we will perform ablation experiments and explorations on the LasHeR~\cite{lasher} test set to validate the model components.
 % In this section, we conduct ablation experiments on the LasHeR testing set~\cite{lasher}. 

\noindent \textbf{Components Analysis.} 
% In or. 
We used the dual-template ViPT~\cite{vipt} as our baseline.
As summarized in Tab.~\ref{tab:ablation_components}, the adaptive perception structure achieved a 4.3\% improvement in success rate due to the absence of bias towards any modality. This improvement is attributed to the fact that in complex environments, the effectiveness of each modality is not constant. The model, through self-selection, can flexibly adapt to the varying importance of different modalities, thereby enhancing overall tracking performance. Moreover, the cross-modal information flow generated by our global modal preceptor significantly enhances performance. Additionally, the learnable token mechanism, by learning more representative features, synergizes with the preceptor, further boosting the model's accuracy.

% we propsed interaction strategy is effective in improving the tracker's ability for multi-modal synergies. Although the traditional cross attention mechanism can mention to improve the model performance, our proposed multi-modal attention can achieve better modal interactions.
% Specifically, the multi-modal attention we proposed can bring about 0.7\% performance in precission improvement compared to the traditional cross attention. This is because the multi-modal attention realizes modal interaction through modal advantage guidance instead of using similarity query. In addition, the method of Learnable token mechanism including token learning and token embeding has also been proven to be effective. 
% % These components are fully symmetrical when handling multiple modalities.



\begin{figure*}
  \centering
    \includegraphics[width=0.85\linewidth,height=6.5cm]
    {fig/heat.pdf}
   \caption{Visualization of the attention maps for a representative pair sequence. The modal dominance of these two sequences is not consistent, specifically (a) the sequence with superior RGB imaging has better target features and (b) it is the TIR that has better target features.}
   \label{fig:heat}
\end{figure*}


\noindent \textbf{Number of learnable Tokens Analysis.}
We analyze the ratio of learnable token to the original token.
Learning 0 tokens indicates that no learnable token mechanism is applied, and the model directly performs feature interaction. As shown in the Tab.~\ref{tab:number}, even a small number of learnable tokens can enhance performance, as reducing the number of tokens helps aggregate information, minimize redundancy, and avoid unnecessary interactions. However, if the number of learnable tokens is too small, it may not adequately capture the target's features, thus limiting performance improvements.

% As shown in Tab.~\ref{tab:number}, the strategy that only adds learnable token can get a slight increase, while the strategy that does not add learnable token can get a 0.8\% increase by appropriately reducing the ratio of token number. This is because reducing the number of tokens is a process of aggregation. It reduces redundant information and avoids unnecessary interactions.

% \textbf{Learning features and mapping features Analysis.} 
% In order to explore whether all image feature tokens are required for modal interaction, we conducted experiments on the corresponding image tokens of token learning and token embeding. 
% All-All indicates a token that uses both the template feature and the search feature in learning stage and embeding stage. Search-All means using the search feature token in token learning stage and All feature tokens in token embeding stage. As shown in Tab.~\ref{tab:number}, tokens in the search region, as input to token learning stage, can improve the performance of the model. This is because, by inputting only the search region, the model interactively extracts target features about the search region. This way, compared with inputting all tokens and template tokens, it also needs fewer steps to convert interactive information into target positioning features. In addition, using All tokens as input to token embeding stage, which can assist the potential spatial alignment of modalities, has  0.6\% increase in precission compared to search as input.
\begin{figure}
  \centering
    \includegraphics[width=0.9\linewidth]
    {fig/attribute.pdf}
   \caption{Success rate of different attributes on the LasHeR~\cite{lasher} test set. }
   \label{fig:attribute}
\end{figure}



% \textbf{Comparison on inference speed.}
% We compare the inference speed of APTrack with the sota method of previous open source code. As summarized in Tab.~\ref{tab:fps}, APTrack is able to achieve real-time speed (28.7 FPS), which is slightly superior to the model of the dominant-auxiliary method (ViPT~\cite{vipt} achieves 28.3 FPS).

\noindent \textbf{Qualitative Comparison.} In order to comprehensively analyze the performance of our method in various scenarios, we performed attribute evaluation as well as visualization on the LasHeR~\cite{lasher} dataset.
As shown in Fig.~\ref{fig:tracker_contrast}, with different cases of modal validity, our model can track the target better. And, in Fig.~\ref{fig:attribute},  APTrack can surpass the current unified tracking model in many challenging scenarios.  
Specifically, under low-light conditions, the TIR modality typically outperforms the RGB modality. Since we did not prioritize the RGB modality but instead allowed the model to adaptively perceive and select the more effective modality, performance improvements were achieved. Moreover, the last two sequences in Fig.~\ref{fig:tracker_contrast} further validate this observation. In terms of speed, as shown in Tab.~\ref{tab:fps}, despite utilizing dual template frames, our model maintains a speed advantage by circumventing complex modal interactions.
% including  frame lost, fast motion, high illumination, low resolution, motion blur,  partial occlusion, similar appearance, scale Variation, thermal crossover, aspect ratio change , Camera Moving , deformation. 

\noindent \textbf{More visualization} To evaluate APTrack's generalization capability in RGB-D and RGB-E tasks, we conducted a visual analysis in four challenging scenarios: occlusion, similar object interference, cluttered backgrounds, and small targets. This is shown in Fig.~\ref{fig:contrast} from top to bottom. In the occlusion scenario, APTrack successfully captured the features of the occluded target by adaptively selecting the modality, thereby avoiding target loss, which is a common issue in traditional methods that rely on a fixed modality. In situations involving similar object interference, APTrack demonstrated its ability to flexibly choose the modality that best distinguishes the target from the interference, thereby enhancing target recognition and avoiding tracking errors. When faced with cluttered backgrounds, APTrack effectively separated the target from the background by adaptively selecting the modality that best highlights the target, ensuring accurate tracking in complex environments. For small targets, APTrack enhanced the perception of minute targets by integrating the strengths of multiple modalities. These results highlight APTrack's robustness and generalization ability in complex scenarios, attributed to its approach of adaptively selecting and integrating various modality features, rather than relying on a fixed dominant modality.

Additionally, we conducted a visual analysis of the attention maps for the search area to illustrate two challenging sequences. Fig.~\ref{fig:heat}~(a) shows that, under the RGB modality, the model's target imaging performance is superior compared to the TIR modality, while Fig.~\ref{fig:heat}~(b) indicates that the imaging performance is more prominent under the TIR modality. As shown in Fig.~\ref{fig:heat}, by not fixing any modality during the modeling process, APTrack can flexibly adjust its focus and fully leverage the complementary advantages of each modality, resulting in precise target attention in complex scenarios.

%  Since we treat RGB and TIR equally instead of favoring RGB, we achieve better performance.  
% Specifically in low-light scenes, RGB tends to fail, in addition to TIR's playing a more important role.
% This is the reason why APTrack achieves high performance on several multi-modal tasks. 

% \textbf{Speed Performance.} In order to comprehensively analyze the performance of our method in various scenarios, we evaluate our method on different attributes of lasher data set, including  frame lost, fast motion, high illumination, low resolution, motion blur,  partial occlusion, similar appearance, scale Variation, thermal crossover, aspect ratio change , Camera Moving , deformation. As shown in Fig.~\ref{fig:attribute}, thanks to the full symmetric network and efficient modal interaction strategy, APTrack can surpass the current unified tracking model in many challenging scenarios. Especially in low-light scenes, RGB tends to fail, in addition to TIR's playing a more important role. Since we treat RGB and TIR equally instead of favoring RGB, we achieve better performance. This is the reason why APTrack achieves high performance on several multi-modal tasks.
% % Especially in low-light scenes TIR plays a more important role when RGB fails. since we treat RGB and TIR equally, so we achieve better performance. This is why APTrack has achieved high performance on multiple multi-modal tasks.
% 

% Especially in low-light scenes, RGB is easy to fail, and TIR plays a more important role. Because we treat RGB and TIR equally, instead of favoring RGB, we achieve better performance. This is also the reason why APTrack achieves high performance on multiple multi-modal tasks.
% 
  
% \textbf{Visualization of Attention Map.} In order to explore how the AMI modules mutually enhance the target features in the search area, we visualize the attention maps about the target in the search area before and after modal interaction. As shown in Fig.~\ref{fig:heatmap}, we can observe that not RGB and TIR can maintain target attention in all scenarios, and at the same time, our interaction method can transfer the advantages of one modality to another modality, so that the accepting modality enhances the feature representation about the target. 
% For example, in the first line of the Fig.~\ref{fig:heatmap}, the RGB modal features do not accurately focus on the target, but TIR focuses on the target more accurately. Then these more correct advantages of attention will be passed to RGB through TIR, enhancing the attention of the RGB modal branch to the target.
% For example, in the first row of the Fig.~\ref{fig:heatmap}, the RGB modal features do not pay much attention to the target, but the TIR pays attention to the target, and is passed to RGB through TIR to enhance the search ability of the RGB modal branch about the target. 



\section{Conclusions}

In this work, we investigated methods to enhance the performance of unified parameter trackers. Specifically, our proposed APTrack uniformly processes all modalities without additional optimization for specific ones, enabling the model to self-perceive and select modalities. Additionally, the designed AMI module effectively reduces redundancy and computational load, while the global modal preceptor facilitates modality alignment and advantage transmission. APTrack demonstrates outstanding performance across five benchmarks in three multi-modal tasks, showcasing its superior capabilities.

\textit{Limitation.} Adaptive perception has significant advantages: on the one hand, it eliminates the need to deliberately select the dominant-auxiliary modes, which makes the use of modes more flexible; on the other hand, it can effectively improve the performance of multi-modal tasks. However, there are some limitations in this adaptive perception approach, as it needs to give the same degree of processing to both modes, which will lead to an increase in the amount of computation to a certain extent, and put forward a higher demand for computational resources. Besides, in the natural language tracking task, natural language is different from image features and cannot be directly applied to the model we designed. In our follow-up work, we plan to use natural language as an independent prompt to assist the RGB-X tracking task, to further explore its potential application and optimize the tracking effect.

% If your paper is intended for a conference, please contact your conference editor concerning acceptable word processor formats for your particular conference.

% \section{Guidelines for Manuscript Preparation}

% When you open trans\_jour.docx, select ``Page Layout'' from the ``View'' menu in the menu bar (View $|$ Page Layout), (these instructions assume MS 6.0. Some versions may have alternate ways to access the same functionalities noted here). Then, type over sections of trans\_jour.docx or cut and paste from another document and use markup styles. The pull-down style menu is at the left of the Formatting Toolbar at the top of your {\it Word} window (for example, the style at this point in the document is ``Text''). Highlight a section that you want to designate with a certain style, and then select the appropriate name on the style menu. The style will adjust your fonts and line spacing. Do not change the font sizes or line spacing to squeeze more text into a limited number of pages. Use italics for emphasis; do not underline.

% To insert images in {\it Word}, position the cursor at the insertion point and either use Insert $|$ Picture $|$ From File or copy the image to the Windows clipboard and then Edit $|$ Paste Special $|$ Picture (with ``float over text'' unchecked). 

% IEEE will do the final formatting of your paper. If your paper is intended for a conference, please observe the conference page limits.

% \subsection{Abbreviations and Acronyms}

% Define abbreviations and acronyms the first time they are used in the text, even after they have already been defined in the abstract. Abbreviations such as IEEE, SI, ac, and dc do not have to be defined. Abbreviations that incorporate periods should not have spaces: write ``C.N.R.S.,'' not ``C. N. R. S.'' Do not use abbreviations in the title unless they are unavoidable (for example, ``IEEE'' in the title of this article).

% \subsection{Other Recommendations}

% Use one space after periods and colons. Hyphenate complex modifiers: ``zero-field-cooled magnetization.'' Avoid dangling participles, such as, ``Using (1), the potential was calculated.'' [It is not clear who or what used (1).] Write instead, ``The potential was calculated by using (1),'' or ``Using (1), we calculated the potential.''

% Use a zero before decimal points: ``0.25,'' not ``.25.'' Use ``cm$^3$,'' not ``cc.'' Indicate sample dimensions as ``0.1~cm~$\times$ 0.2 cm,'' not ``0.1 $\times$ 0.2 cm$^2$.'' The abbreviation for ``seconds'' is ``s,'' not ``sec.'' Use ``Wb/m$^2$'' or ``webers per square meter,'' not ``webers/m$^2$.'' When expressing a range of values, write ``7 to 9'' or ``7-9,'' not ``7$\sim$9.''

% A parenthetical statement at the end of a sentence is punctuated outside of the closing parenthesis (like this). (A parenthetical sentence is punctuated within the parentheses.) In American English, periods and commas are within quotation marks, like ``this period.'' Other punctuation is ``outside''$!$ Avoid contractions; for example, write ``do not'' instead of ``don't.'' The serial comma is preferred: ``A, B, and C'' instead of ``A, B and C.''

% If you wish, you may write in the first person singular or plural and use the active voice (``I observed that ...'' or ``We observed that ...'' instead of ``It was observed that ...''). Remember to check spelling. If your native language is not English, please get a native English-speaking colleague to carefully proofread your paper.

% \section{Math}

% If you are using {\it Word}, use either the Microsoft Equation Editor or the {\it MathType} add-on (http://www.mathtype.com) for equations in your paper (Insert $|$ Object $|$ Create New $|$ Microsoft Equation {\it or} MathType Equation). ``Float over text'' should {\it not} be selected.

% \subsection{Equations}

% Number equations consecutively with equation numbers in parentheses flush with the right margin, as in (1). First use the equation editor to create the equation. Then select the ``Equation'' markup style. Press the tab key and write the equation number in parentheses. To make your equations more compact, you may use the solidus ( / ), the exp function, or appropriate exponents. Use parentheses to avoid ambiguities in denominators. Punctuate equations when they are part of a sentence, as in
% \begin{equation}
% \!
% \end{equation}
% Be sure that the symbols in your equation have been defined before the equation appears or immediately following. Italicize symbols ($T$ might refer to temperature, but T is the unit tesla). Refer to ``(1),'' not ``Eq. (1)'' or ``equation~(1),'' except at the beginning of a sentence: ``Equation (1) is ....''


% \section{Units}

% Use either SI (MKS) or CGS as primary units. (SI units are strongly encouraged.) English units may be used as secondary units (in parentheses). This applies to papers in data storage. For example, write ``15 Gb/cm$^2$ (100 Gb/in$^2$).'' An exception is when English units are used as identifiers in trade, such as ``3$^1${/}$_2$-in disk drive.'' Avoid combining SI and CGS units, such as current in amperes and magnetic field in oersteds. This often leads to confusion because equations do not balance dimensionally. If you must use mixed units, clearly state the units for each quantity in an equation.

% The SI unit for magnetic field strength $H$ is A/m. However, if you wish to use units of T, either refer to magnetic flux density $B$ or magnetic field strength symbolized as $\mu_0$H. Use the center dot to separate compound units, e.g., ``A$\cdot$m$^2$.''

% \section{Some Common Mistakes}

% The word ``data'' is plural, not singular. The subscript for the permeability of vacuum $\mu_0$ is zero, not a lowercase letter ``o.'' The term for residual magnetization is ``remanence''; the adjective is ``remanent''; do not write ``remnance'' or ``remnant.'' Use the word ``micrometer'' instead of ``micron.'' A graph within a graph is an ``inset,'' not an ``insert.'' The word ``alternatively'' is preferred to the word ``alternately'' (unless you really mean something that alternates). Use the word ``whereas'' instead of ``while'' (unless you are referring to simultaneous events). Do not use the word ``essentially'' to mean ``approximately'' or ``effectively.'' Do not use the word ``issue'' as a euphemism for ``problem.'' When compositions are not specified, separate chemical symbols by en-dashes; for example, ``NiMn'' indicates the intermetallic compound Ni$_{0.5}$Mn$_{0.5}$ whereas ``Ni--Mn'' indicates an alloy of some composition Ni$_{\rm x}$Mn$_{1-{\rm x}}$.

% Be aware of the different meanings of the homophones ``affect'' (usually a verb) and ``effect'' (usually a noun), ``complement'' and ``compliment,'' ``discreet'' and ``discrete,'' ``principal'' (e.g., ``principal investigator'') and ``principle'' (e.g., ``principle of measurement''). Do not confuse ``imply'' and\break ``infer.'' 

% Prefixes such as ``non,'' ``sub,'' ``micro,'' ``multi,'' and ``ultra'' are not independent words; they should be joined to the words they modify, usually without a hyphen. There is no period after the ``et'' in the Latin abbreviation ``{\it et al}.'' (it is also italicized). The abbreviation ``i.e.,'' means ``that is,'' and the abbreviation ``e.g.,'' means ``for example'' (these abbreviations are not italicized).

% A general IEEE styleguide is available at www.ieee.org/authortools.

% \section{Guidelines For Graphics Preparation and Submission}

% \subsection{Types of Graphics}

% The following list outlines the different types of graphics published in IEEE journals. They are categorized based on their construction, and use of color / shades\vadjust{\pagebreak} of gray:

% \begin{enumerate}
% \item[{\it 1)}]{\it Color/Grayscale Figures}

% Figures that are meant to appear in color, or shades of black/gray. Such figures may include photographs, illustrations, multicolor graphs, and flowcharts.

% \item[{\it 2)}]{\it Line Art Figures}

% Figures that are composed of only black lines and shapes. These figures should have no shades or half-tones of gray, only black and white.

% \item[{\it 3)}]{\it Author Photos}

% Head and shoulders shots of authors that appear at the end of our papers.

% \item[{\it 4)}]{\it Tables}

% Data charts which are typically black and white, but sometimes include color.
% \end{enumerate}
% \begin{figure}
% \centerline{\includegraphics[width=18.5pc]{fig1.png}}
% \caption{Magnetization as a function of applied field. Note that ``Fig.'' is abbreviated. There is a period after the figure number, followed by two spaces. It is good practice to explain the significance of the figure in the caption.}
% \end{figure}

% \begin{table}
% \caption{Units for Magnetic Properties}
% \label{table}
% \tablefont%
% \setlength{\tabcolsep}{3pt}
% \begin{tabular*}{21pc}{@{}|p{23pt}|p{81pt}<{\raggedright\hangindent6pt}|p{123pt}<{\raggedright\hangindent6pt}|@{}}
% \hline
% Symbol& 
% Quantity& 
% Conversion from Gaussian and \par CGS EMU to SI$^{\mathrm{a}}$ \\
% \hline\\[-17pt]
% &&\\
% $\Phi $& 
% Magnetic flux& 
% 1 Mx $\to  10^{-8}$ Wb $= 10^{-8}$ V$\,\cdot\,$s \\
% $B$& 
% Magnetic flux density, magnetic induction& 
% 1 G $\to  10^{-4}$ T $= 10^{-4}$~Wb/m$^{2}$ \\
% $H$& 
% Magnetic field strength& 
% 1 Oe $\to  10^{-3}/(4\pi )$ A/m \\
% $m$& 
% Magnetic moment& 
% 1 erg/G $=$ 1 emu\\ && $\to 10^{-3}$ A $\cdot$ m$^{2} = 10^{-3}$ J/T \\
% $M$& 
% Magnetization& 
% 1 erg/(G $\cdot$ cm$^{3}) =$ 1 emu/cm$^{3}$  $\to 10^{-3}$ A/m \\
% 4$\pi M$& 
% Magnetization& 
% 1 G $\to  10^{-3}/(4\pi )$ A/m \\
% $\sigma $& 
% Specific magnetization& 
% 1~erg/(G$\,{\cdot}\,$g)$\,{=}\,$1~emu/g$\,{\to}\,$1~A$\,{\cdot}\,$m$^{2}$/kg \\
% $j$& 
% Magnetic dipole  moment& 
% 1 erg/G $=$ 1 emu\par $\to 4\pi \times  10^{-10}$ Wb $\cdot$ m \\
% $J$& 
% Magnetic polarization& 
% 1 erg/(G $\cdot$ cm$^{3}) =$ 1 emu/cm$^{3}$ \\ && $\to 4\pi \times  10^{-4}$ T \\
% $\chi , \kappa $& 
% Susceptibility& 
% 1 $\to  4\pi $ \\
% $\chi_{\rho }$& 
% Mass susceptibility& 
% 1 cm$^{3}$/g $\to  4\pi \times  10^{-3}$ m$^{3}$/kg \\
% $\mu $& 
% Permeability& 
% 1 $\to  4\pi \times  10^{-7}$ H/m\par  $= 4\pi \times  10^{-7}$ Wb/(A $\cdot$ m) \\
% $\mu_{r}$& 
% Relative permeability& 
% $\mu \to \mu_{r}$ \\
% $w, W$& 
% Energy density& 
% 1 erg/cm$^{3} \to  10^{-1}$ J/m$^{3}$ \\
% $N, D$& 
% Demagnetizing factor& 
% 1 $\to  1/(4\pi )$ \\
% \hline
% \multicolumn{3}{l}{}\\[-5pt]
% \multicolumn{3}{@{}p{21pc}@{}}{\hspace*{9pt}Vertical lines are optional in tables. Statements that serve as captions for 
% the entire table do not need footnote letters. }\\
% \multicolumn{3}{@{}p{21pc}@{}}{\hspace*{9pt}$^{\mathrm{a}}$Gaussian units are the same as cg emu for magnetostatics; Mx 
% $=$ maxwell, G $=$ gauss, Oe $=$ oersted; Wb $=$ weber, V $=$ volt, s $=$ 
% second, T $=$ tesla, m $=$ meter, A $=$ ampere, J $=$ joule, kg $=$ 
% kilogram, H $=$ henry.}
% \end{tabular*}
% \label{tab1}
% \end{table}



% \subsection{Multipart Figures}

% Figures compiled of more than one sub-figure presented side-by-side, or stacked. If a multipart figure is made up of multiple figure types (one part is lineart, and another is grayscale or color) the figure should meet the stricter guidelines.

% \subsection{File Formats for Graphics}

% Format and save your graphics using a suitable graphics processing program that will allow you to create the images as PostScript (PS), Encapsulated PostScript (.EPS), Tagged Image File Format (.TIFF), Portable Document Format (.PDF), or Portable Network Graphics (.PNG) sizes them, and adjusts the resolution settings. If you created your source files in one of the following programs you will be able to submit the graphics without converting to a PS, EPS, TIFF, PDF, or PNG file: Microsoft Word, Microsoft PowerPoint, or Microsoft Excel. Though it is not required, it is strongly recommended that these files be saved in PDF format rather than DOC, XLS, or PPT. Doing so will protect your figures from common font and arrow stroke issues that occur when working on the files across multiple platforms. When submitting your final paper, your graphics should all be submitted individually in one of these formats along with the manuscript.


% \subsection{Sizing of Graphics}

% Most charts, graphs, and tables are one column wide (3.5 inches / 88 millimeters / 21 picas) or page wide (7.16 inches / 181 millimeters / 43 picas). The maximum depth a graphic can be is 8.5 inches (216 millimeters / 54 picas). When choosing the depth of a graphic, please allow space for a caption. Figures can be sized between column and page widths if the author chooses, however it is recommended that figures are not sized less than column width unless when necessary.

% There is currently one publication with column measurements that do not coincide with those listed above. Proceedings of the IEEE has a column measurement of 3.25 inches (82.5 millimeters / 19.5 picas). 

% The final printed size of author photographs is exactly 1 inch wide by 1.25 inches tall (25.4 millimeters $\times$ 31.75 millimeters / 6 picas $\times$ 7.5 picas). Author photos printed in editorials measure 1.59 inches wide by 2 inches tall (40 millimeters $\times$ 50 millimeters / 9.5 picas $\times$ 12 picas).

% \subsection{Resolution}

% The proper resolution of your figures will depend on the type of figure it is as defined in the ``Types of Figures'' section. Author photographs, color, and grayscale figures should be at least 300dpi. Line art, including tables should be a minimum of 600dpi.

% \subsection{Vector Art}

% In order to preserve the figures' integrity across multiple computer platforms, we accept files in the following formats: .EPS/.PDF/.PS. All fonts must be embedded or text converted to outlines in order to achieve the best-quality results.

% \subsection{Color Space}

% The term color space refers to the entire sum of colors that can be represented within the said medium. For our purposes, the three main color spaces are Grayscale, RGB (red/green/blue) and CMYK (cyan/magenta/yellow/black). RGB is generally used with on-screen graphics, whereas CMYK is used for printing purposes.

% All color figures should be generated in RGB or CMYK color space. Grayscale images should be submitted in Grayscale color space. Line art may be provided in grayscale OR bitmap colorspace. Note that ``bitmap colorspace'' and ``bitmap file format'' are not the same thing. When bitmap color space is selected, .TIF/.TIFF/.PNG are the recommended file formats.

% \subsection{Accepted Fonts Within Figures}

% When preparing your graphics IEEE suggests that you use of one of the following Open Type fonts: Times New Roman, Helvetica, Arial, Cambria, and Symbol. If you are supplying EPS, PS, or PDF files all fonts must be embedded. Some fonts may only be native to your operating system; without the fonts embedded, parts of the graphic may be distorted or missing.

% A safe option when finalizing your figures is to strip out the fonts before you save the files, creating ``outline'' type. This converts fonts to artwork what will appear uniformly on any screen.

% \subsection{Using Labels Within Figures}

% \subsubsection{Figure Axis Labels}

% Figure axis labels are often a source of confusion. Use words rather than symbols. As an example, write the quantity ``Magnetization,'' or ``Magnetization $M$,'' not just ``$M$.'' Put units in parentheses. Do not label axes only with units. As in Fig. 1, for example, write ``Magnetization (A/m)'' or ``Magnetization (A$\cdot$m$^{-1}$),'' not just ``A/m.'' Do not label axes with a ratio of quantities and units. For example, write ``Temperature (K),'' not ``Temperature/K.''
 
% Multipliers can be especially confusing. Write ``Magnetization (kA/m)'' or ``Magnetization (10$^3$ A/m).'' Do not write ``Magnetization (A/m) $\times$ 1000'' because the reader would not know whether the top axis label in Fig. 1 meant 16000 A/m or 0.016 A/m. Figure labels should be legible, approximately 8 to 10 point type.

% \subsubsection{Subfigure Labels in Multipart Figures and Tables}

% Multipart figures should be combined and labeled before final submission. Labels should appear centered below each subfigure in 8 point Times New Roman font in the format of (a) (b) (c). 

% \subsection{File Naming}

% Figures (line artwork or photographs) should be named starting with the first 5 letters of the author’s last name. The next characters in the filename should be the number that represents the sequential location of this image in your article. For example, in author ``Anderson's'' paper, the first three figures would be named ander1.tif, ander2.tif, and ander3.ps.

% Tables should contain only the body of the table (not the caption) and should be named similarly to figures, except that `.t' is inserted in-between the author's name and the table number. For example, author Anderson's first three tables would be named ander.t1.tif, ander.t2.ps, ander.t3.eps.

% Author photographs should be named using the first five characters of the pictured author's last name. For example, four author photographs for a paper may be named: oppen.ps, moshc.tif, chen.eps, and duran.pdf.

% If two authors or more have the same last name, their first initial(s) can be substituted for the fifth, fourth, third... letters of their surname until the degree where there is differentiation. For example, two authors Michael and Monica Oppenheimer's photos would be named oppmi.tif, and oppmo.eps.

% \subsection{Referencing a Figure or Table Within Your Paper}
 
% When referencing your figures and tables within your paper, use the abbreviation ``Fig.'' even at the beginning of a sentence. Do not abbreviate ``Table.'' Tables should be numbered with Roman Numerals.

% \subsection{Checking Your Figures: The IEEE Graphics Analyzer}

% The IEEE Graphics Analyzer enables authors to pre-screen their graphics for compliance with IEEE Transactions and Journals standards before submission. The online tool, located at \underline{http://graphicsqc.ieee.org/}, allows authors to upload their graphics in order to check that each file is the correct file format, resolution, size and colorspace; that no fonts are missing or corrupt; that figures are not compiled in layers or have transparency, and that they are named according to the IEEE Transactions and Journals naming convention. At the end of this automated process, authors are provided with a detailed report on each graphic within the web applet, as well as by email.

% For more information on using the Graphics Analyzer or any other graphics related topic, contact the IEEE Graphics Help Desk by e-mail at \underline{graphics@ieee.org}.

% \subsection{Submitting Your Graphics}

% Because IEEE will do the final formatting of your paper, you do not need to position figures and tables at the top and bottom of each column. In fact, all figures, figure captions, and tables can be placed at the end of your paper. In addition to, or even in lieu of submitting figures within your final manuscript, figures should be submitted individually, separate from the manuscript in one of the file formats listed above in section VI-J. Place figure captions below the figures; place table titles above the tables. Please do not include captions as part of the figures, or put them in ``text boxes'' linked to the figures. Also, do not place borders around the outside of your figures.


% \subsection{Color Processing / Printing in IEEE Journals}

% All IEEE Transactions, Journals, and Letters allow an author to publish color figures on IEEE {\it Xplore}$^{\scriptsize\textregistered}$ at no charge, and automatically convert them to grayscale for print versions. In most journals, figures and tables may alternatively be printed in color if an author chooses to do so. Please note that this service comes at an extra expense to the author. If you intend to have print color graphics, include a note with your final paper indicating which figures or tables you would like to be handled that way, and stating that you are willing to pay the additional fee.

% \section{Conclusion}

% A conclusion section is not required. Although a conclusion may review the main points of the paper, do not replicate the abstract as the conclusion. A conclusion might elaborate on the importance of the work or suggest applications and extensions.

% \section*{Appendix}

% Appendixes, if needed, appear before the acknowledgment.

% \section*{Acknowledgment}

% The preferred spelling of the word ``acknowledgment'' in American English is without an ``e'' after the ``g.'' Use the singular heading even if you have many acknowledgments. Avoid expressions such as ``One of us (S.B.A.) would like to thank ... .'' Instead, write ``F. A. Author thanks ... .'' In most cases, sponsor and financial support acknowledgments are placed in the unnumbered footnote on the first page, not here.

% \section*{References and Footnotes}

% \subsection{References}

% References need not be cited in text. When they are, they appear on the line, in square brackets, inside the punctuation. Multiple references are each numbered with separate brackets. When citing a section in a book, please give the relevant page numbers. In text, refer simply to the reference number. Do not use ``Ref.'' or ``reference'' except at the beginning of a sentence: ``Reference [3] shows~....'' Please do not use automatic endnotes in {\it Word}, rather, type the reference list at the end of the paper using the ``References''  style.

% Reference numbers are set flush left and form a column of their own, hanging out beyond the body of the reference. The reference numbers are on the line, enclosed in square brackets. In all references, the given name of the author or editor is abbreviated to the initial only and precedes the last name. Use them all; use {\it et al}. only if names are not given. Use commas around Jr., Sr., and III in names. Abbreviate conference titles. When citing IEEE transactions, provide the issue number, page range, volume number, year, and/or month if available. When referencing a patent, provide the day and the month of issue, or application. References may not include all information; please obtain and include relevant information. Do not combine references. There must be only one reference with each number. If there is a URL included with the print reference, it can be included at the end of the reference. 

% Other than books, capitalize only the first word in a paper title, except for proper nouns and element symbols. For papers published in translation journals, please give the English citation first, followed by the original foreign-language citation See the end of this document for formats and examples of common references. For a complete discussion of references and their formats, see the IEEE style manual at \underline{www.ieee.org/authortools}.

% \subsection{Footnotes}

% Number footnotes separately in superscripts (Insert $|$ Footnote).\footnote{It is recommended that footnotes be avoided (except for the unnumbered footnote with the receipt date on the first page). Instead, try to integrate the footnote information into the text.}  Place the actual footnote at the bottom of the column in which it is cited; do not put footnotes in the reference list (endnotes). Use letters for table footnotes (see Table I). 

% \section{Submitting Your Paper for Review}

% \subsection{Review Stage Using Word 6.0 or Higher}

% If you want to submit your file with one column electronically, please do the following:

% \begin{itemize}
% \item[--\kern-4pt]First, click on the View menu and choose Print Layout.

% \item[--\kern-4pt]Second, place your cursor in the first paragraph. Go to the Format menu, choose Columns, choose one column Layout, and choose ``apply to whole document” from the dropdown menu.

% \item[--\kern-4pt]Third, click and drag the right margin bar to just over 4 inches in width.
% \end{itemize}

% The graphics will stay in the ``second'' column, but you can drag them to the first column. Make the graphic wider to push out any text that may try to fill in next to the graphic.

% \subsection{Final Stage Using Word 6.0}

% When you submit your final version (after your paper has been accepted), print it in two-column format, including figures and tables. You must also send your final manuscript on a disk, via e-mail, or through a Web manuscript submission system as directed by the society contact. You may use {\it Zip} for large files, or compress files using {\it Compress}, {\it Pkzip}, {\it Stuffit}, or {\it Gzip}. 

% Also, send a sheet of paper or PDF with complete contact information for all authors. Include full mailing addresses, telephone numbers, fax numbers, and e-mail addresses. This information will be used to send each author a complimentary copy of the journal in which the paper appears. In addition, designate one author as the ``corresponding author.'' This is the author to whom proofs of the paper will be sent. Proofs are sent to the corresponding author only.

% \subsection{Review Stage Using Scholarone$^{\scriptsize\textregistered}$ Manuscripts}

% Contributions to the Transactions, Journals, and Letters may be submitted electronically on IEEE's on-line manuscript submission and peer-review system, ScholarOne$^{\scriptsize\textregistered}$ Manuscripts. You can get a listing of the publications that participate in ScholarOne at \underline{http://www.ieee.org/publications\_} \underline{standards/publications/authors/authors\_submission.html} First check if you have an existing account. If there is none, please create a new account. After logging in, go to your Author Center and click ``Submit First Draft of a New Manuscript.'' 



% Along with other information, you will be asked to select the subject from a pull-down list. Depending on the journal, there are various steps to the submission process; you must complete all steps for a complete submission. At the end of each step you must click ``Save and Continue''; just uploading the paper is not sufficient. After the last step, you should see a confirmation that the submission is complete. You should also receive an e-mail confirmation. For inquiries regarding the submission of your paper on ScholarOne Manuscripts, please contact oprs-support@ieee.org or call +1 732 465 5861.

% ScholarOne Manuscripts will accept files for review in various formats. Please check the guidelines of the specific journal for which you plan to submit. 

% You will be asked to file an electronic copyright form immediately upon completing the submission process (authors are responsible for obtaining any security clearances). Failure to submit the electronic copyright could result in publishing delays later. You will also have the opportunity to designate your article as ``open access'' if you agree to pay the IEEE open access fee. 

% \subsection{Final Stage Using Scholarone Manuscripts}

% Upon acceptance, you will receive an email with specific instructions regarding the submission of your final files. To avoid any delays in publication, please be sure to follow these instructions. Most journals require that final submissions be uploaded through ScholarOne Manuscripts, although some may still accept final submissions via email. Final submissions should include source files of your accepted manuscript, high quality graphic files, and a formatted pdf file. If you have any questions regarding the final submission process, please contact the administrative contact for the journal. 

% In addition to this, upload a file with complete contact information for all authors. Include full mailing addresses, telephone numbers, fax numbers, and e-mail addresses. Designate the author who submitted the manuscript on ScholarOne Manuscripts as the ``corresponding author.'' This is the only author to whom proofs of the paper will be sent. 

% \subsection{Copyright Form}

% Authors must submit an electronic IEEE Copyright Form (eCF) upon submitting their final manuscript files. You can access the eCF system through your manuscript submission system or through the Author Gateway. You are responsible for obtaining any necessary approvals and/or security clearances. For additional information on intellectual property rights, visit the IEEE Intellectual Property Rights department web page at \underline{http://www.ieee.org/publications\_} \underline{standards/publications/rights/index.html}. 

% \section{IEEE Publishing Policy}

% The general IEEE policy requires that authors should only submit original work that has neither appeared elsewhere for publication, nor is under review for another refereed publication. The submitting author must disclose all prior publication(s) and current submissions when submitting a manuscript. Do not publish ``preliminary'' data or results. The submitting author is responsible for obtaining agreement of all coauthors and any consent required from employers or sponsors before submitting an article. The IEEE Transactions and Journals Department strongly discourages courtesy authorship; it is the obligation of the authors to cite only relevant prior work.

% The IEEE Transactions and Journals does not publish conference records or proceedings, but can publish articles related to conferences that have undergone rigorous peer review. Minimally, two reviews are required for every article submitted for peer review.

% \section{Publication Principles}

% The two types of contents of that are published are; 1) peer-reviewed and 2) archival. The Transactions and Journals Department publishes scholarly articles of archival value as well as tutorial expositions and critical reviews of classical subjects and topics of current interest. 

% Authors should consider the following points:
% \begin{enumerate}
% \item[1)]Technical papers submitted for publication must advance the state of knowledge and must cite relevant prior work. 

% \item[2)]The length of a submitted paper should be commensurate with the importance, or appropriate to the complexity, of the work. For example, an obvious extension of previously published work might not be appropriate for publication or might be adequately treated in just a few pages.

% \item[3)]Authors must convince both peer reviewers and the editors of the scientific and technical merit of a paper; the standards of proof are higher when extraordinary or unexpected results are reported. 

% \item[4)]Because replication is required for scientific progress, papers submitted for publication must provide sufficient information to allow readers to perform similar experiments or calculations and use the reported results. Although not everything need be disclosed, a paper must contain new, useable, and fully described information. For example, a specimen's chemical composition need not be reported if the main purpose of a paper is to introduce a new measurement technique. Authors should expect to be challenged by reviewers if the results are not supported by adequate data and critical details.

% \item[5)]Papers that describe ongoing work or announce the latest technical achievement, which are suitable for presentation at a professional conference, may not be appropriate for publication.
% \end{enumerate}

% \section*{References}
% Generated by IEEEtran.bst, version: 1.14 (2015/08/26)
\begin{thebibliography}{10}
\providecommand{\url}[1]{#1}
\csname url@samestyle\endcsname
\providecommand{\newblock}{\relax}
\providecommand{\bibinfo}[2]{#2}
\providecommand{\BIBentrySTDinterwordspacing}{\spaceskip=0pt\relax}
\providecommand{\BIBentryALTinterwordstretchfactor}{4}
\providecommand{\BIBentryALTinterwordspacing}{\spaceskip=\fontdimen2\font plus
\BIBentryALTinterwordstretchfactor\fontdimen3\font minus
  \fontdimen4\font\relax}
\providecommand{\BIBforeignlanguage}[2]{{%
\expandafter\ifx\csname l@#1\endcsname\relax
\typeout{** WARNING: IEEEtran.bst: No hyphenation pattern has been}%
\typeout{** loaded for the language `#1'. Using the pattern for}%
\typeout{** the default language instead.}%
\else
\language=\csname l@#1\endcsname
\fi
#2}}
\providecommand{\BIBdecl}{\relax}
\BIBdecl

\bibitem{simtrack}
B.~Chen, P.~Li, L.~Bai, L.~Qiao, Q.~Shen, B.~Li, W.~Gan, W.~Wu, and W.~Ouyang,
  ``Backbone is all your need: {A} simplified architecture for visual object
  tracking,'' in \emph{{ECCV} {(22)}}, ser. Lecture Notes in Computer Science,
  vol. 13682.\hskip 1em plus 0.5em minus 0.4em\relax Springer, 2022, pp.
  375--392.

\bibitem{mixformer}
Y.~Cui, C.~Jiang, L.~Wang, and G.~Wu, ``Mixformer: End-to-end tracking with
  iterative mixed attention,'' in \emph{{CVPR}}.\hskip 1em plus 0.5em minus
  0.4em\relax {IEEE}, 2022, pp. 13\,598--13\,608.

\bibitem{ostrack}
B.~Ye, H.~Chang, B.~Ma, S.~Shan, and X.~Chen, ``Joint feature learning and
  relation modeling for tracking: {A} one-stream framework,'' in \emph{{ECCV}
  {(22)}}, ser. Lecture Notes in Computer Science, vol. 13682.\hskip 1em plus
  0.5em minus 0.4em\relax Springer, 2022, pp. 341--357.

\bibitem{seqtrack}
X.~Chen, H.~Peng, D.~Wang, H.~Lu, and H.~Hu, ``Seqtrack: Sequence to sequence
  learning for visual object tracking,'' in \emph{{CVPR}}.\hskip 1em plus 0.5em
  minus 0.4em\relax {IEEE}, 2023, pp. 14\,572--14\,581.

\bibitem{swintrack}
L.~Lin, H.~Fan, Z.~Zhang, Y.~Xu, and H.~Ling, ``Swintrack: {A} simple and
  strong baseline for transformer tracking,'' in \emph{NeurIPS}, 2022.

\bibitem{artrack}
X.~Wei, Y.~Bai, Y.~Zheng, D.~Shi, and Y.~Gong, ``Autoregressive visual
  tracking,'' in \emph{{CVPR}}.\hskip 1em plus 0.5em minus 0.4em\relax {IEEE},
  2023, pp. 9697--9706.

\bibitem{vipt}
J.~Zhu, S.~Lai, X.~Chen, D.~Wang, and H.~Lu, ``Visual prompt multi-modal
  tracking,'' in \emph{{CVPR}}.\hskip 1em plus 0.5em minus 0.4em\relax {IEEE},
  2023, pp. 9516--9526.

\bibitem{untrack}
Z.~Wu, J.~Zheng, X.~Ren, F.-A. Vasluianu, C.~Ma, D.~P. Paudel, L.~Van~Gool, and
  R.~Timofte, ``Single-model and any-modality for video object tracking,''
  \emph{arXiv preprint arXiv:2311.15851}, 2023.

\bibitem{xie2024robust}
J.~Xie, B.~Zhong, Q.~Liang, N.~Li, Z.~Mo, and S.~Song, ``Robust tracking via
  mamba-based context-aware token learning,'' \emph{arXiv preprint
  arXiv:2412.13611}, 2024.

\bibitem{hu2024exploiting}
X.~Hu, Y.~Tai, X.~Zhao, C.~Zhao, Z.~Zhang, J.~Li, B.~Zhong, and J.~Yang,
  ``Exploiting multimodal spatial-temporal patterns for video object
  tracking,'' \emph{arXiv preprint arXiv:2412.15691}, 2024.

\bibitem{zheng2023curricular}
Y.~Zheng, J.~Zhan, S.~He, J.~Dong, and Y.~Du, ``Curricular contrastive
  regularization for physics-aware single image dehazing,'' in
  \emph{Proceedings of the IEEE/CVF conference on computer vision and pattern
  recognition}, 2023, pp. 5785--5794.

\bibitem{yan2023dynamic}
M.~Yan, J.~Qian, R.~Wang, S.~Gao, and J.~Yang, ``Dynamic group difference
  coding based on thermal infrared face image for fever screening,'' \emph{IEEE
  Transactions on Instrumentation and Measurement}, vol.~72, pp. 1--13, 2023.

\bibitem{zhang2024few}
H.~Zhang, S.~Chen, L.~Luo, and J.~Yang, ``Few-shot learning with long-tailed
  labels,'' \emph{Pattern Recognition}, vol. 156, p. 110806, 2024.

\bibitem{anSHaRPoseSparseHighResolution2024}
X.~An, L.~Zhao, C.~Gong, N.~Wang, D.~Wang, and J.~Yang, ``{{SHaRPose}}:
  {{Sparse High-Resolution Representation}} for {{Human Pose Estimation}},''
  \emph{Proceedings of the AAAI Conference on Artificial Intelligence},
  vol.~38, no.~2, pp. 691--699, Mar. 2024.

\bibitem{bao2022emotion}
J.~Bao, X.~Tao, and Y.~Zhou, ``An emotion recognition method based on eye
  movement and audiovisual features in mooc learning environment,'' \emph{IEEE
  Transactions on Computational Social Systems}, vol.~11, no.~1, pp. 171--183,
  2022.
  
\bibitem{ning20253d}
E.~Ning, W.~Li, J.~Fang, J.~Yuan, Q.~Duan, and G.~Wang, ``3d-guided
  multi-feature semantic enhancement network for person re-id,''
  \emph{Information Fusion}, vol. 117, p. 102863, 2025.
  
\bibitem{lian2023multi}
J.~Lian, D.-H. Wang, Y.~Wu, and S.~Zhu, ``Multi-branch enhanced discriminative
  network for vehicle re-identification,'' \emph{IEEE Transactions on
  Intelligent Transportation Systems}, 2023.



\bibitem{fang2024guided}
W.~Fang, J.~Fan, Y.~Zheng, J.~Weng, Y.~Tai, and J.~Li, ``Guided real image
  dehazing using ycbcr color space,'' \emph{arXiv preprint arXiv:2412.17496},
  2024.

\bibitem{onetracker}
L.~Hong, S.~Yan, R.~Zhang, W.~Li, X.~Zhou, P.~Guo, K.~Jiang, Y.~Chen, J.~Li,
  Z.~Chen \emph{et~al.}, ``Onetracker: Unifying visual object tracking with
  foundation models and efficient tuning,'' \emph{arXiv preprint
  arXiv:2403.09634}, 2024.

\bibitem{protrack}
J.~Yang, Z.~Li, F.~Zheng, A.~Leonardis, and J.~Song, ``Prompting for
  multi-modal tracking,'' in \emph{{ACM} Multimedia}.\hskip 1em plus 0.5em
  minus 0.4em\relax {ACM}, 2022, pp. 3492--3500.

\bibitem{sdstrack}
X.~Hou, J.~Xing, Y.~Qian, Y.~Guo, S.~Xin, J.~Chen, K.~Tang, M.~Wang, Z.~Jiang,
  L.~Liu \emph{et~al.}, ``Sdstrack: Self-distillation symmetric adapter
  learning for multi-modal visual object tracking,'' \emph{arXiv preprint
  arXiv:2403.16002}, 2024.

\bibitem{tbsi}
T.~Hui, Z.~Xun, F.~Peng, J.~Huang, X.~Wei, X.~Wei, J.~Dai, J.~Han, and S.~Liu,
  ``Bridging search region interaction with template for {RGB-T} tracking,'' in
  \emph{{CVPR}}.\hskip 1em plus 0.5em minus 0.4em\relax {IEEE}, 2023, pp.
  13\,630--13\,639.

\bibitem{bat}
B.~Cao, J.~Guo, P.~Zhu, and Q.~Hu, ``Bi-directional adapter for multimodal
  tracking,'' in \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, vol.~38, no.~2, 2024, pp. 927--935.

\bibitem{stnet}
J.~Zhang, B.~Dong, H.~Zhang, J.~Ding, F.~Heide, B.~Yin, and X.~Yang, ``Spiking
  transformers for event-based single object tracking,'' in
  \emph{{CVPR}}.\hskip 1em plus 0.5em minus 0.4em\relax {IEEE}, 2022, pp.
  8791--8800.

\bibitem{lasot}
H.~Fan, L.~Lin, F.~Yang, P.~Chu, G.~Deng, S.~Yu, H.~Bai, Y.~Xu, C.~Liao, and
  H.~Ling, ``Lasot: {A} high-quality benchmark for large-scale single object
  tracking,'' in \emph{{CVPR}}.\hskip 1em plus 0.5em minus 0.4em\relax Computer
  Vision Foundation / {IEEE}, 2019, pp. 5374--5383.

\bibitem{got10k}
L.~Huang, X.~Zhao, and K.~Huang, ``Got-10k: {A} large high-diversity benchmark
  for generic object tracking in the wild,'' \emph{{IEEE} Trans. Pattern Anal.
  Mach. Intell.}, vol.~43, no.~5, pp. 1562--1577, 2021.

\bibitem{trackingnet}
M.~M{\"{u}}ller, A.~Bibi, S.~Giancola, S.~Al{-}Subaihi, and B.~Ghanem,
  ``Trackingnet: {A} large-scale dataset and benchmark for object tracking in
  the wild,'' in \emph{{ECCV} {(1)}}, ser. Lecture Notes in Computer Science,
  vol. 11205.\hskip 1em plus 0.5em minus 0.4em\relax Springer, 2018, pp.
  310--327.

\bibitem{lasher}
C.~Li, W.~Xue, Y.~Jia, Z.~Qu, B.~Luo, J.~Tang, and D.~Sun, ``Lasher: {A}
  large-scale high-diversity benchmark for {RGBT} tracking,'' \emph{{IEEE}
  Trans. Image Process.}, vol.~31, pp. 392--404, 2022.

\bibitem{rgbt234}
C.~Li, X.~Liang, Y.~Lu, N.~Zhao, and J.~Tang, ``{RGB-T} object tracking:
  Benchmark and baseline,'' \emph{Pattern Recognit.}, vol.~96, 2019.

\bibitem{depthtrack}
S.~Yan, J.~Yang, J.~K{\"a}pyl{\"a}, F.~Zheng, A.~Leonardis, and J.-K.
  K{\"a}m{\"a}r{\"a}inen, ``Depthtrack: Unveiling the power of rgbd tracking,''
  in \emph{Proceedings of the IEEE/CVF International Conference on Computer
  Vision}, 2021, pp. 10\,725--10\,733.

\bibitem{vot-rgbd}
M.~Kristan, J.~Matas, A.~Leonardis, M.~Felsberg, L.~Cehovin, G.~Fernandez,
  T.~Vojir, G.~Hager, G.~Nebehay, and R.~Pflugfelder, ``The visual object
  tracking vot2015 challenge results,'' in \emph{Proceedings of the IEEE
  international conference on computer vision workshops}, 2015, pp. 1--23.

\bibitem{visevent}
X.~Wang, J.~Li, L.~Zhu, Z.~Zhang, Z.~Chen, X.~Li, Y.~Wang, Y.~Tian, and F.~Wu,
  ``Visevent: Reliable object tracking via collaboration of frame and event
  flows,'' \emph{IEEE Transactions on Cybernetics}, 2023.


\bibitem{unified}
J.~Lu, C.~Clark, R.~Zellers, R.~Mottaghi, and A.~Kembhavi, ``Unified-io: A
  unified model for vision, language, and multi-modal tasks,'' in \emph{The
  Eleventh International Conference on Learning Representations}, 2022.

\bibitem{imagebind}
R.~Girdhar, A.~El-Nouby, Z.~Liu, M.~Singh, K.~V. Alwala, A.~Joulin, and
  I.~Misra, ``Imagebind: One embedding space to bind them all,'' in
  \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
  Recognition}, 2023, pp. 15\,180--15\,190.

\bibitem{mutex}
R.~Shah, R.~Mart{\'\i}n-Mart{\'\i}n, and Y.~Zhu, ``Mutex: Learning unified
  policies from multimodal task specifications,'' \emph{arXiv preprint
  arXiv:2309.14320}, 2023.

\bibitem{siamese_av}
Y.-B. Lin and G.~Bertasius, ``Siamese vision transformers are scalable
  audio-visual learners,'' \emph{arXiv preprint arXiv:2403.19638}, 2024.

\bibitem{unified_av}
S.~Mo and P.~Morgado, ``A unified audio-visual learning framework for
  localization, separation, and recognition,'' in \emph{International
  Conference on Machine Learning}.\hskip 1em plus 0.5em minus 0.4em\relax PMLR,
  2023, pp. 25\,006--25\,017.

\bibitem{learning_li}
H.~You, L.~Zhou, B.~Xiao, N.~Codella, Y.~Cheng, R.~Xu, S.-F. Chang, and
  L.~Yuan, ``Learning visual representation from modality-shared contrastive
  language-image pre-training,'' in \emph{European Conference on Computer
  Vision}.\hskip 1em plus 0.5em minus 0.4em\relax Springer, 2022, pp. 69--87.

\bibitem{adamw}
I.~Loshchilov and F.~Hutter, ``Decoupled weight decay regularization,'' in
  \emph{International Conference on Learning Representations}, 2018.

\bibitem{rgbt210}
C.~Li, N.~Zhao, Y.~Lu, C.~Zhu, and J.~Tang, ``Weighted sparse representation
  regularized graph learning for rgb-t object tracking,'' in \emph{Proceedings
  of the 25th ACM international conference on Multimedia}, 2017, pp.
  1856--1864.

\bibitem{gmmt}
Z.~Tang, T.~Xu, X.~Wu, X.-F. Zhu, and J.~Kittler, ``Generative-based fusion
  mechanism for multi-modal tracking,'' in \emph{Proceedings of the AAAI
  Conference on Artificial Intelligence}, vol.~38, no.~6, 2024, pp. 5189--5197.

\bibitem{transt}
X.~Chen, B.~Yan, J.~Zhu, D.~Wang, X.~Yang, and H.~Lu, ``Transformer tracking,''
  in \emph{Proceedings of the IEEE/CVF conference on computer vision and
  pattern recognition}, 2021, pp. 8126--8135.

\bibitem{stark}
B.~Yan, H.~Peng, J.~Fu, D.~Wang, and H.~Lu, ``Learning spatio-temporal
  transformer for visual tracking,'' in \emph{Proceedings of the IEEE/CVF
  international conference on computer vision}, 2021, pp. 10\,448--10\,457.

\bibitem{aiatrack}
S.~Gao, C.~Zhou, C.~Ma, X.~Wang, and J.~Yuan, ``Aiatrack: Attention in
  attention for transformer visual tracking,'' in \emph{European Conference on
  Computer Vision}.\hskip 1em plus 0.5em minus 0.4em\relax Springer, 2022, pp.
  146--164.

\bibitem{spt}
X.-F. Zhu, T.~Xu, Z.~Tang, Z.~Wu, H.~Liu, X.~Yang, X.-J. Wu, and J.~Kittler,
  ``Rgbd1k: A large-scale dataset and benchmark for rgb-d object tracking,'' in
  \emph{Proceedings of the AAAI Conference on Artificial Intelligence},
  vol.~37, no.~3, 2023, pp. 3870--3878.

\bibitem{focal_loss}
T.-Y. Lin, P.~Goyal, R.~Girshick, K.~He, and P.~Doll{\'a}r, ``Focal loss for
  dense object detection,'' in \emph{Proceedings of the IEEE international
  conference on computer vision}, 2017, pp. 2980--2988.

\bibitem{giou}
H.~Rezatofighi, N.~Tsoi, J.~Gwak, A.~Sadeghian, I.~D. Reid, and S.~Savarese,
  ``Generalized intersection over union: {A} metric and a loss for bounding box
  regression,'' in \emph{{CVPR}}.\hskip 1em plus 0.5em minus 0.4em\relax
  Computer Vision Foundation / {IEEE}, 2019, pp. 658--666.

% \bibitem{tokenLearn}
% M.~Ryoo, A.~Piergiovanni, A.~Arnab, M.~Dehghani, and A.~Angelova,
%   ``\BIBforeignlanguage{en-US}{Tokenlearner: What can 8 learned tokens do for
%   images and videos?}'' \emph{\BIBforeignlanguage{en-US}{arXiv: Computer Vision
%   and Pattern Recognition,arXiv: Computer Vision and Pattern Recognition}}, Jun
%   2021.
\bibitem{det}
S.~Yan, J.~Yang, J.~K{\"{a}}pyl{\"{a}}, F.~Zheng, A.~Leonardis, and
  J.~K{\"{a}}m{\"{a}}r{\"{a}}inen, ``Depthtrack: Unveiling the power of {RGBD}
  tracking,'' in \emph{{ICCV}}.\hskip 1em plus 0.5em minus 0.4em\relax {IEEE},
  2021, pp. 10\,705--10\,713.
\bibitem{FFTrack}
X.~Hu, B.~Zhong, Q.~Liang, S.~Zhang, N.~Li, X.~Li, and R.~Ji, ``Transformer
  tracking via frequency fusion,'' \emph{{IEEE} Trans. Circuits Syst. Video
  Technol.}, vol.~34, no.~2, pp. 1020--1031, 2024.
% \bibitem{ecmd}
% T.~Zhang, H.~Guo, Q.~Jiao, Q.~Zhang, and J.~Han,
%   ``\BIBforeignlanguage{en-US}{Efficient rgb-t tracking via cross-modality
%   distillation}.''

% \bibitem{vatal}
% Y.~Song, C.~Ma, X.~Wu, L.~Gong, L.~Bao, W.~Zuo, C.~Shen, R.~W.~H. Lau, and
%   M.~Yang, ``{VITAL:} visual tracking via adversarial learning,'' in
%   \emph{{CVPR}}.\hskip 1em plus 0.5em minus 0.4em\relax Computer Vision
%   Foundation / {IEEE} Computer Society, 2018, pp. 8990--8999.

% \bibitem{prdimp50}
% Z.~Feng, L.~Yan, Y.~Xia, and B.~Xiao, ``Multi-task probabilistic regression
%   with overlap maximization for visual tracking,'' \emph{{IEEE} Trans. Circuits
%   Syst. Video Technol.}, vol.~33, no.~12, pp. 7554--7564, 2023.

% \bibitem{apfnet}
% Y.~Xiao, M.~Yang, C.~Li, L.~Liu, and J.~Tang, ``Attribute-based progressive
%   fusion network for {RGBT} tracking,'' in \emph{{AAAI}}.\hskip 1em plus 0.5em
%   minus 0.4em\relax {AAAI} Press, 2022, pp. 2831--2838.


\bibitem{CDTrack}
X.~Zheng, H.~Cui, and X.~Lu, ``Multiple source domain adaptation for multiple
  object tracking in satellite video,'' \emph{IEEE Transactions on Geoscience
  and Remote Sensing}, vol.~61, pp. 1--11, 2023.

\bibitem{evptrack}
L.~Shi, B.~Zhong, Q.~Liang, N.~Li, S.~Zhang, and X.~Li, ``Explicit visual
  prompts for visual object tracking,'' in \emph{{AAAI}}.\hskip 1em plus 0.5em
  minus 0.4em\relax {AAAI} Press, 2024, pp. 4838--4846.

\bibitem{aqatrack}
J.~Xie, B.~Zhong, Z.~Mo, S.~Zhang, L.~Shi, S.~Song, and R.~Ji, ``Autoregressive
  queries for adaptive tracking with spatio-temporal transformers,'' in
  \emph{CVPR}, 2024, pp. 19\,300--19\,309.

\bibitem{clip}
A.~Radford, J.~Kim, C.~Hallacy, A.~Ramesh, G.~Goh, Agarwal, and et~al.,
  ``\BIBforeignlanguage{en-US}{Learning transferable visual models from natural
  language supervision},'' \emph{\BIBforeignlanguage{en-US}{Cornell University
  - arXiv,Cornell University - arXiv}}, Feb 2021.

\bibitem{mctrack}
X.~Hu, B.~Zhong, Q.~Liang, S.~Zhang, N.~Li, and X.~Li, ``Towards modalities
  correlation for rgb-t tracking,'' \emph{IEEE Transactions on Circuits and
  Systems for Video Technology}, 2024.


\bibitem{SPT}
X.~Zhu, T.~Xu, Z.~Tang, Z.~Wu, H.~Liu, X.~Yang, X.~Wu, and J.~Kittler,
  ``{RGBD1K:} {A} large-scale dataset and benchmark for {RGB-D} object
  tracking,'' in \emph{{AAAI}}.\hskip 1em plus 0.5em minus 0.4em\relax {AAAI}
  Press, 2023, pp. 3870--3878.

\bibitem{siamban}
Z.~Chen, B.~Zhong, G.~Li, S.~Zhang, R.~Ji, Z.~Tang, and X.~Li, ``Siamban:
  Target-aware tracking with siamese box adaptive network,'' \emph{IEEE
  Transactions on Pattern Analysis and Machine Intelligence}, vol.~45, no.~4,
  pp. 5158--5173, 2022.

\bibitem{dualevent_mmht}
H.~Sun, R.~Liu, W.~Cai, J.~Wang, Y.~Wang, H.~Tang, Y.~Cui, D.~Yao, and D.~Guo,
  ``Reliable object tracking by multimodal hybrid feature extraction and
  transformer-based fusion,'' \emph{arXiv preprint arXiv:2405.17903}, 2024.
  
\bibitem{dualrgbd_depthrefiner}
S.~Lai, D.~Wang, and H.~Lu, ``Depthrefiner: Adapting rgb trackers to rgbd
  scenes via depth-fused refinement,'' in \emph{2024 IEEE International
  Conference on Multimedia and Expo (ICME)}.\hskip 1em plus 0.5em minus
  0.4em\relax IEEE, 2024, pp. 1--6.


\bibitem{CAFormer}
Y.~Xiao, J.~Zhao, A.~Lu, C.~Li, Y.~Lin, B.~Yin, and C.~Liu, ``Cross-modulated
  attention transformer for rgbt tracking,'' \emph{arXiv preprint
  arXiv:2408.02222}, 2024.

\bibitem{AINet}
A.~Lu, W.~Wang, C.~Li, J.~Tang, and B.~Luo, ``Rgbt tracking via all-layer
  multimodal interactions with progressive fusion mamba,'' \emph{arXiv preprint
  arXiv:2408.08827}, 2024.


  

\end{thebibliography}




% \subsection*{Basic format for books:}\vspace*{-12pt}
% \def\refname{}
% \begin{thebibliography}{34}
% \item[] J. K. Author, ``Title of chapter in the book,'' in {\em Title of His Published Book}, xth ed. City of Publisher, (only U.S. State), Country: Abbrev. of Publisher, year, ch. x, sec. x, pp. xxx--xxx.
% \end{thebibliography}

% \subsection*{Examples:}
% \def\refname{}
% \begin{thebibliography}{34}\vspace*{-12pt}

% \bibitem{}G. O. Young, ``Synthetic structure of industrial plastics,'' in {\em Plastics},\break 2nd ed., vol. 3, J. Peters, Ed. New York, NY, USA: McGraw-Hill, 1964,\break pp. 15--64.

% \bibitem{}W.-K. Chen, {\it Linear Networks and Systems}. Belmont, CA, USA: Wadsworth, 1993, pp. 123--135.

% \end{thebibliography}

% \subsection*{Basic format for periodicals:}\vspace*{-12pt}

% \begin{thebibliography}{34}
% \item[]
% J. K. Author, ``Name of paper,'' {\it Abbrev. Title of Periodical}, vol. {\it x},\break   no. {\it x}, pp. xxx--xxx, Abbrev. Month, year, DOI. \href{https://dx.doi.org/10.1109.XXX.123456}{10.1109.XXX.123456}.
% \end{thebibliography}


% \subsection*{Examples:}\vspace*{-12pt}

% \begin{thebibliography}{34}
% \setcounter{enumiv}{2}

% \bibitem{}J. U. Duncombe, ``Infrared navigation Part I: An assessment of feasibility,'' {\em IEEE Trans. Electron Devices}, vol. ED-11, no. 1, pp. 34--39,\break Jan. 1959, 10.1109/TED.2016.2628402.

% \bibitem{}E. P. Wigner, ``Theory of traveling-wave optical laser,''
% {\em Phys. Rev.},  vol.\break 134, pp. A635--A646, Dec. 1965. DOI. \href{https://dx.doi.org/10.1109.XXX.123456}{10.1109.XXX.123456}.

% \bibitem{}E. H. Miller, ``A note on reflector arrays,'' {\em IEEE Trans. Antennas Propagat.}, to be published.
% \end{thebibliography}


% \subsection*{Basic format for reports:}\vspace*{-12pt}
% \begin{thebibliography}{34}
% \item[]
% J. K. Author, ``Title of report,'' Abbrev. Name of Co., City of Co., Abbrev. State, Country, Rep. xxx, year.
% \end{thebibliography}

% \subsection*{Examples:}\vspace*{-12pt}
% \begin{thebibliography}{34}
% \setcounter{enumiv}{5}

% \bibitem{} E. E. Reber, R. L. Michell, and C. J. Carter, ``Oxygen absorption in the earth’s atmosphere,'' Aerospace Corp., Los Angeles, CA, USA, Tech. Rep. TR-0200 (4230-46)-3, Nov. 1988.

% \bibitem{} J. H. Davis and J. R. Cogdell, ``Calibration program for the 16-foot antenna,'' Elect. Eng. Res. Lab., Univ. Texas, Austin, TX, USA, Tech. Memo. NGL-006-69-3, Nov. 15, 1987.
% \end{thebibliography}

% \subsection*{Basic format for handbooks:}\vspace*{-12pt}
% \begin{thebibliography}{34}
% \item[]
% {\em Name of Manual/Handbook}, x ed., Abbrev. Name of Co., City of Co., Abbrev. State, Country, year, pp. xxx--xxx.
% \end{thebibliography}

% \subsection*{Examples:}\vspace*{-12pt}

% \begin{thebibliography}{34}
% \setcounter{enumiv}{7}

% \bibitem{} {\em Transmission Systems for Communications}, 3rd ed., Western Electric Co., Winston-Salem, NC, USA, 1985, pp. 44--60.

% \bibitem{} {\em Motorola Semiconductor Data Manual}, Motorola Semiconductor Products Inc., Phoenix, AZ, USA, 1989.
% \end{thebibliography}

% \subsection*{Basic format for books (when available online):}\vspace*{-12pt}
% \begin{thebibliography}{34}
% \item[]
% J. K. Author, ``Title of chapter in the book,'' in {\em Title of Published Book}, xth ed. City of Publisher, State, Country: Abbrev. of Publisher, year, ch. x, sec. x, pp. xxx--xxx. [Online]. Available: http://www.web.com 
% \end{thebibliography}


% \subsection*{Examples:}\vspace*{-12pt}

% \begin{thebibliography}{34}
% \setcounter{enumiv}{9}

% \bibitem{}G. O. Young, ``Synthetic structure of industrial plastics,'' in Plastics, vol. 3, Polymers of Hexadromicon, J. Peters, Ed., 2nd ed. New York, NY, USA: McGraw-Hill, 1964, pp. 15--64. [Online]. Available: http://www.bookref.com. 

% \bibitem{} {\em The Founders Constitution}, Philip B. Kurland and Ralph Lerner, eds., Chicago, IL, USA: Univ. Chicago Press, 1987. [Online]. Available: http://press-pubs.uchicago.edu/founders/

% \bibitem{} The Terahertz Wave eBook. ZOmega Terahertz Corp., 2014. [Online]. Available: http://dl.z-thz.com/eBook/zomega\_ebook\_pdf\_1206\_sr.pdf. Accessed on: May 19, 2014. 

% \bibitem{} Philip B. Kurland and Ralph Lerner, eds., {\em The Founders Constitution}. Chicago, IL, USA: Univ. of Chicago Press, 1987, Accessed on: Feb. 28, 2010, [Online] Available: http://press-pubs.uchicago.edu/founders/ 
% \end{thebibliography}

% \subsection*{Basic format for journals (when available online):}\vspace*{-12pt}
% \begin{thebibliography}{34}


% \item[] J. K. Author, ``Name of paper,'' {\em Abbrev. Title of Periodical}, vol. x, no. x, pp. xxx--xxx, Abbrev. Month, year. Accessed on: Month, Day, year, DOI: 10.1109.XXX.123456, [Online].
% \end{thebibliography}

% \subsection*{Examples:}\vspace*{-12pt}

% \begin{thebibliography}{34}
% \setcounter{enumiv}{13}

% \bibitem{}J. S. Turner, ``New directions in communications,'' {\em IEEE J. Sel. Areas Commun.}, vol. 13, no. 1, pp. 11--23, Jan. 1995. 

% \bibitem{} W. P. Risk, G. S. Kino, and H. J. Shaw, ``Fiber-optic frequency shifter using a surface acoustic wave incident at an oblique angle,'' {\em Opt. Lett.}, vol. 11, no. 2, pp. 115--117, Feb. 1986.

% \bibitem{} P. Kopyt {\em et al.}, ``Electric properties of graphene-based conductive layers from DC up to terahertz range,'' {\em IEEE THz Sci. Technol.}, to be published. DOI: \href{https://dx.doi.org/10.1109.XXX.123456}{10.1109/TTHZ.2016.2544142}.
% \end{thebibliography}

% \subsection*{Basic format for papers presented at conferences (when available online):}\vspace*{-12pt}
% \begin{thebibliography}{34}
% \item[] J.K. Author. (year, month). Title. presented at abbrev. conference title. [Type of Medium]. Available: site/path/file
% \end{thebibliography}

% \subsection*{Example:}\vspace*{-12pt}

% \begin{thebibliography}{34}
% \setcounter{enumiv}{16}

% \bibitem{}PROCESS Corporation, Boston, MA, USA. Intranets: Internet technologies deployed behind the firewall for corporate productivity. Presented at INET96 Annual Meeting. [Online]. Available: http://home.process.com/Intranets/wp2.htp
% \end{thebibliography}

% \subsection*{Basic format for reports  and  handbooks (when available online):}\vspace*{-12pt}
% \begin{thebibliography}{34}
% \item[] J. K. Author. ``Title of report,'' Company. City, State, Country. Rep. no., (optional: vol./issue), Date. [Online] Available: site/path/file 
% \end{thebibliography}

% \subsection*{Examples:}\vspace*{-12pt}

% \begin{thebibliography}{34}
% \setcounter{enumiv}{17}

% \bibitem{}R. J. Hijmans and J. van Etten, ``Raster: Geographic analysis and modeling with raster data,'' R Package Version 2.0-12, Jan. 12, 2012. [Online]. Available: http://CRAN.R-project.org/package=raster 

% \bibitem{}Teralyzer. Lytera UG, Kirchhain, Germany [Online]. Available: http://www.lytera.de/Terahertz\_THz\_Spectroscopy.php?id=home, Accessed on: Jun. 5, 2014.
% \end{thebibliography}

% \subsection*{Basic format for computer programs and electronic documents (when available online):}\vspace*{-12pt}
% \begin{thebibliography}{34}
% \item[] Legislative body. Number of Congress, Session. (year, month day). {\em Number of bill or resolution, Title}. [Type of medium]. Available: site/path/file

% \item[] {\em NOTE:} ISO recommends that capitalization follow the accepted practice for the language or script in which the information is given.
% \end{thebibliography}

% \subsection*{Example:}\vspace*{-12pt}

% \begin{thebibliography}{34}
% \setcounter{enumiv}{19}

% \bibitem{}U. S. House. 102nd Congress, 1st Session. (1991, Jan. 11). {\em H. Con. Res. 1, Sense of the Congress on Approval of Military Action}. [Online]. Available: LEXIS Library: GENFED File: BILLS 
% \end{thebibliography}

% \subsection*{Basic format for patents (when available online):}\vspace*{-12pt}
% \begin{thebibliography}{34}
% \item[] Name of the invention, by inventor’s name. (year, month day). Patent Number [Type of medium]. Available:site/path/file
% \end{thebibliography}


% \subsection*{Example:}\vspace*{-12pt}

% \begin{thebibliography}{34}
% \setcounter{enumiv}{20}

% \bibitem{}Musical tooth brush with mirror, by L. M. R. Brooks. (1992, May 19). Patent D 326 189
% [Online]. Available: NEXIS Library: LEXPAT File:   DES 
% \end{thebibliography}


% \subsection*{Basic format for conference proceedings (published):}\vspace*{-12pt}
% \begin{thebibliography}{34}
% \item[] J. K. Author, ``Title of paper,'' in {\em Abbreviated Name of Conf.}, City of Conf., Abbrev. State (if given), Country, year, pp. xxx--xxx.
% \end{thebibliography}

% \subsection*{Example:}\vspace*{-12pt}

% \begin{thebibliography}{34}
% \setcounter{enumiv}{21}

% \bibitem{}D. B. Payne and J. R. Stern, ``Wavelength-switched passively coupled single-mode optical network,'' in {\em Proc. IOOC-ECOC}, Boston, MA, USA, 1985,
% pp. 585--590. 

% \end{thebibliography}

% \subsection*{Example for papers presented at conferences (unpublished):}\vspace*{-12pt}

% \begin{thebibliography}{34}
% \setcounter{enumiv}{22}

% \bibitem{}D. E behard and E. Voges, ``Digital single sideband detection for inter ferometric sensors,'' presented at the {\em 2nd Int. Conf. Optical Fiber Sensors}, Stuttgart, Germany, Jan. 2--5, 1984.
% \end{thebibliography}

% \subsection*{Basic format for patents:}\vspace*{-12pt}
% \begin{thebibliography}{34}
% \item[] J. K. Author, ``Title of patent,'' U.S. Patent x xxx--xxx, Abbrev. Month, day, year.
% \end{thebibliography}


% \subsection*{Example:}\vspace*{-12pt}

% \begin{thebibliography}{34}
% \setcounter{enumiv}{23}

% \bibitem{}G. Brandli and M. Dick, ``Alternating current fed power supply,'' U.S. Patent 4 084 217, Nov. 4, 1978.
% \end{thebibliography}

% \subsection*{Basic format for theses (M.S.) and dissertations (Ph.D.):}\vspace*{-12pt}
% \begin{thebibliography}{34}
% \item[a)] J. K. Author, ``Title of thesis,'' M.S. thesis, Abbrev. Dept., Abbrev. Univ., City of Univ., Abbrev. State, year.

% \item[b)] J. K. Author, ``Title of dissertation,'' Ph.D. dissertation, Abbrev. Dept., Abbrev. Univ., City of Univ., Abbrev. State, year.
% \end{thebibliography}

% \subsection*{Examples:}\vspace*{-12pt}

% \begin{thebibliography}{34}
% \setcounter{enumiv}{24}

% \bibitem{}J. O. Williams, ``Narrow-band analyzer,'' Ph.D. dissertation, Dept. Elect. Eng., Harvard Univ., Cambridge, MA, USA, 1993.

% \bibitem{}N. Kawasaki, ``Parametric study of thermal and chemical nonequilibrium nozzle flow,'' M.S. thesis, Dept. Electron. Eng., Osaka Univ., Osaka, Japan, 1993.
% \end{thebibliography}

% \subsection*{Basic format for the most common types of unpublished references:}\vspace*{-12pt}
% \begin{thebibliography}{34}
% \item[a)] J. K. Author, private communication, Abbrev. Month, year.

% \item[b)] J. K. Author, ``Title of paper,'' unpublished.

% \item[c)] J. K. Author, ``Title of paper,'' to be published.
% \end{thebibliography}

% \subsection*{Examples:}\vspace*{-18pt}

% \begin{thebibliography}{34}
% \setcounter{enumiv}{26}

% \bibitem{}A. Harrison, private communication, May 1995.

% \bibitem{}B. Smith, ``An approach to graphs of linear forms,'' unpublished.

% \bibitem{}A. Brahms, ``Representation error for real numbers in binary computer arithmetic,'' IEEE Computer Group Repository, Paper R-67-85.
% \end{thebibliography}\vspace*{-6pt}

% \subsection*{Basic formats for standards:}\vspace*{-18pt}
% \begin{thebibliography}{34}
% \item[a)] {\em Title of Standard}, Standard number, date.

% \item[b)] {\em Title of Standard}, Standard number, Corporate author, location, date.
% \end{thebibliography}\vspace*{-6pt}

% \subsection*{Examples:}\vspace*{-18pt}

% \begin{thebibliography}{34}
% \setcounter{enumiv}{29}
% \bibitem{}IEEE Criteria for Class IE Electric Systems, IEEE Standard 308, 1969.

% \bibitem{} Letter Symbols for Quantities, ANSI Standard Y10.5-1968.
% \end{thebibliography}\vspace*{-6pt}

% \subsection*{Article number in reference examples:}\vspace*{-18pt}

% \begin{thebibliography}{34}

% \setcounter{enumiv}{31}

% \bibitem{}R. Fardel, M. Nagel, F. Nuesch, T. Lippert, and A. Wokaun, ``Fabrication of organic light emitting diode pixels by laser-assisted forward transfer,'' {\em Appl. Phys. Lett.}, vol. 91, no. 6, Aug. 2007, Art. no. 061103. 

% \bibitem{} J. Zhang and N. Tansu, ``Optical gain and laser characteristics of InGaN quantum wells on ternary InGaN substrates,'' {\em IEEE Photon. J.}, vol. 5, no. 2, Apr. 2013, Art. no. 2600111.  \href{https://dx.doi.org/10.1109.XXX.123456}{10.1109.XXX.123456}.
% \end{thebibliography}\vspace*{-6pt}

% \subsection*{Example when using et al.:}\vspace*{-18pt}

% \begin{thebibliography}{34}
% \setcounter{enumiv}{33}

% \bibitem{}S. Azodolmolky {\em et al.}, ``Experimental demonstration of an impairment aware network planning and operation tool for transparent/translucent optical networks,'' {\em J. Lightw. Technol.}, vol. 29, no. 4, pp. 439--448, Sep. 2011.
% \end{thebibliography}


% \begin{IEEEbiography}{First A. Author}{\space}(M'76--SM'81--F'87) and all authors may include biographies. Biographies are often not included in conference-related papers. This author became a Member (M) of IEEE in 1976, a Senior Member (SM) in 1981, and a Fellow (F) in 1987. The first paragraph may contain a place and/or date of birth (list place, then date). Next, the author’s educational background is listed. The degrees should be listed with type of degree in what field, which institution, city, state, and country, and year the degree was earned. The author's major field of study should be lower-cased.

% The second paragraph uses the pronoun of the person (he or she) and not the author's last name. It lists military and work experience, including summer and fellowship jobs. Job titles are capitalized. The current job must have a location; previous positions may be listed without one. Information concerning previous publications may be included. Try not to list more than three books or published articles. The format for listing publishers of a book within the biography is: title of book (publisher name, year) similar to a reference. Current and previous research interests end the paragraph.

% The third paragraph begins with the author's title and last name (e.g., Dr. Smith, Prof. Jones, Mr. Kajor, Ms. Hunter). List any memberships in professional societies other than the IEEE. Finally, list any awards and work for IEEE committees and publications. If a photograph is provided, it should be of good quality, and professional-looking. Following are two examples of an author’s biography.
% \end{IEEEbiography}

% \begin{IEEEbiography}{Second B. Author}{\space}was born in Greenwich Village, New York, NY, USA in 1977. He received the B.S. and M.S. degrees in aerospace engineering from the University of Virginia, Charlottesville, in 2001 and the Ph.D. degree in mechanical engineering from Drexel University, Philadelphia, PA, in 2008.

%     From 2001 to 2004, he was a Research Assistant with the Princeton Plasma Physics Laboratory. Since 2009, he has been an Assistant Professor with the Mechanical Engineering Department, Texas A\&M University, College Station. He is the author of three books, more than 150 articles, and more than 70 inventions. His research interests include high-pressure and high-density nonthermal plasma discharge processes and applications, microscale plasma discharges, discharges in liquids, spectroscopic diagnostics, plasma propulsion, and innovation plasma applications. He is an Associate Editor of the journal {\it Earth}, {\it Moon}, {\it Planets}, and holds two patents. 

%    Dr. Author was a recipient of the International Association of Geomagnetism and Aeronomy Young Scientist Award for Excellence in 2008, and the IEEE Electromagnetic Compatibility Society Best Symposium Paper Award in 2011. 
% \end{IEEEbiography}


% \begin{IEEEbiography}{Third C. Author, Jr.}{\space}(M’87) received the B.S. degree in mechanical engineering from National Chung Cheng University, Chiayi, Taiwan, in 2004 and the M.S. degree in mechanical engineering from National Tsing Hua University, Hsinchu, Taiwan, in 2006. He is currently pursuing the Ph.D. degree in mechanical engineering at Texas A\&M University, College Station, TX, USA.

%     From 2008 to 2009, he was a Research Assistant with the Institute of Physics, Academia Sinica, Tapei, Taiwan. His research interest includes\vadjust{\vfill\pagebreak} the development of surface processing and biological/medical treatment techniques using nonthermal atmospheric pressure plasmas, fundamental study of plasma sources, and fabrication of micro- or nanostructured surfaces. 

%    Mr. Author’s awards and honors include the Frew Fellowship (Australian Academy of Science), the I. I. Rabi Prize (APS), the European Frequency and Time Forum Award, the Carl Zeiss Research Award, the William F. Meggers Award and the Adolph Lomb Medal (OSA).
% \end{IEEEbiography}


\end{document}
