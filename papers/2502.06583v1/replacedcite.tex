\section{Related Works}
\subsection{Unified Multi-modal Tracking}
Recent RGB-based object trackers____ achieved significant advancements in large-scale benchmark tests____. However, the unstable reliability of RGB causes these types of trackers to struggle with complex scenarios such as low-light conditions. Meanwhile, with the development of multi-modal imaging technology and the reduction in costs, an increasing number of multi-modal datasets, such as RGB-T____, RGB-E____, and RGB-D____, provide possibilities for developing more robust trackers. 
Several works explored how to effectively integrate RGB with another modality. Specifically, TBSI____ model used a pair of template features as a mediator to bridge the relationship between TIR search features and RGB search features. BAT____ froze the backbone network and used adapters to transfer features from one modality to another, achieving adaptive fusion between RGB and TIR. STNet____ combines event images and RGB images from both spatial and temporal perspectives. 
Besides some works have begun exploring unified multi-modal tracking architectures. Among them, ViPT____ was an early adopter of the dominant-auxiliary modality method to fine-tune multiple multi-modal tracking tasks. OneTracker____ utilized large-scale benchmark training of the foundation tracker and incorporated auxiliary branches into the trained model to provide visual cues. On the other hand, Un-Track____ achieved unified training for multi-modal tasks through low-rank factorization and reconstruction techniques. 
However, these methods still exhibit gaps compared to models specifically designed for single multi-modal____ tasks.
% The main reason is the adoption of a dominant-auxiliary modalities modeling approach, making the models more sensitive to features from the dominant modality while insufficiently attending to features from the auxiliary modalities, thus failing to effectively align multi-modal features.

\begin{figure*}
    \centering
    \includegraphics[height=9.5cm]{fig/structure.pdf}
    \caption{The overall structure of APTrack. APTrack is composed of shared embedding, shared transformer block, AMI and Head. The method of modal processing in this model is completely consistent, and there is no need for extra processing for a certain model, which makes the modal features can be aligned adaptively. In addition, AMI can transfer the advantages of modalities to each other.
 }
  \label{fig:struct} 
\end{figure*}


\subsection{Adaptive Perception}
Recent research____ has shown that through extensive training on vast amounts of datasets, models can acquire the capability to align multiple modalities. However, due to the scarcity of multi-modal tracking datasets, it is not feasible to rely on large-scale data for training.
Therefore, recently unified multi-modal tracker____ typically rely on the weights of pre-trained RGB-based tracker, optimizing multi-modal task performance by aligning auxiliary modality features with RGB features. However, simply mapping auxiliary modalities to the RGB latent space is insufficient to fully harness their potential, making it challenging to effectively perceive and leverage the advantages of multi-modality. Furthermore, in the modeling part of its shared weights encoder, SDSTrack____ has a bias in the asymmetrical adapter modeling of the X modality and focuses on multitask fine-tuning, while lacking exploration of the application of adaptive perception in multi-modal tasks under uniform parameters. Moreover, research____  has shown that using shared-weight local networks for multi-modal feature perception provides significant advantages, particularly in consistent modeling at certain stages. In RGB-T trackers____, this shared feature extraction network has shown a significant performance improvement by virtue of adaptive perception. Based on this observation, we pose a critical question: Can adaptive perception further enhance the performance of unified multi-modal trackers? To explore this, we investigate a strategy that avoids fixing a dominant modality and adopts a consistent modeling approach, aiming to improve the performance of unified multi-modal trackers without the need for separate fine-tuning for each task. In other words, rather than focusing on a single task, our goal is to focus on improving the performance of the unified parameter model to enable its easy deployment in diverse scenarios.

% Therefore, a common practice is to fine-tune tracking models pretrained on RGB for multi-modal tasks. 
% Recent methods____ aim to fully leverage the potential of pretraining on RGB by typically freezing the parameters of the RGB extraction network. They then add additional modules to extract auxiliary modality features and fuse them into RGB. These method unilaterally aligns auxiliary modalities to RGB to ensure the effectiveness of the model in multi-modal tasks. However, simply mapping auxiliary modalities to the potential space of RGB does not fully exploit the potential of auxiliary modalities. Meanwhile, 
% recent studies____ have demonstrated the effectiveness of shared-weight feature partial networks in perceiving multi-modal features. However, this approach has not been fully explored in multi-modal tracking frameworks. Inspired by these findings, we further investigated the potential of a consistent modeling strategy for achieving unified multi-modal tracking through adaptive perception. 

% some studies have demonstrated the effectiveness of methods that share weights across modalities for features alignment. 
% Furthermore, the alignment between multi-modal features becomes more challenging due to the frozen RGB feature extraction network, leading to the model's inability to fully utilize multi-modal features. 

 % By maximizing the distance between different semantic features in the potential space while minimizing the distance between the same semantic features, the model can better align multi-modal features in the potential space. 
 % By exploring fully symmetric modeling of multi-modal methods, we have demonstrated that well-designed modality alignment strategies can effectively achieve unified multi-modal representation learning. This unified model surpasses algorithms designed for specific multi-modal tasks in both fine-tuning for joint training.