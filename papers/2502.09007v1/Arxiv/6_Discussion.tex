\section{Discussion}
\label{discussion}

\textbf{Appliance to different models and dataset}
Even though we only evaluate the proposed \sysname framework for Transformer models and datasets in this study, the framework is equally applicable to various memory-bound applications such as deep neural networks (DNN), recommendation systems, and other workloads that include GEMM or GEMV. Unlike previous PIM designs that use memory as a static device, our framework can achieve the highest energy efficiency with the proposed reconfigurable pull-down driver to dynamically control the memory operation, considering the properties of the workload assigned to the PIM macro. In other words, by using the workload configurations to be assigned to the PIM macro and the configurations of the memory and processing unit as input for retention-aware scheduling, our framework can find the optimal operating conditions to achieve maximum energy efficiency.


\textbf{Scalability of the \sysname framework}
In our evaluation, we explore various memory configurations by adjusting the number of subarrays per bank while maintaining a total memory capacity. Notably, our framework is also able to accommodate any total memory capacity and configuration of subarrays and banks because our approach includes a data mapping to the PIM macro, ensuring that operations can be processed in parallel and subarrays are controlled equally. Such mapping is a common consideration in PIM macro design, ensuring that memories and processing units share the same control signals and input data within the macro.

%Such mapping is a common consideration in PIM macro design, ensuring that, within the macro, memories and processing units share the same control signals and broadcast input data.

\textbf{Estimation with different processing unit}
Our experiments assess two types of processing units: bit-serial and bit-parallel operators. The \sysname framework can be extended to other operators by incorporating the operating power of the operators as input in the energy modeling during the scheduling phase. However, for operators that process both input and weight in a bit-parallel manner, their placement in each subarray presents challenges due to its high area and power overhead. Instead, these types of operators are typically configured so that they are shared across multiple subarrays, such as at the bank level. This arrangement allows data from various subarrays to be multiplexed and directed to the shared operator. By integrating these practical constraints into our energy modeling and hardware configuration, we can identify optimal operating conditions in the same manner.
