


\section{Introduction}
\label{introduction}

In the era of artificial intelligence (AI), Transformer has made a significant impact over convolutional neural networks (CNN) by demonstrating its performance across a variety of applications, including generative models ~\cite{radford2019language}, natural language processing ~\cite{devlin2018bert}, and image processing ~\cite{liu2022video}. Although transformer-based models provide remarkable results, many studies show that the excessive amount of parameters causes inefficiency when computed in the traditional von Neumann computer architecture. Therefore, many processing-in-memory (PIM) based accelerators have been proposed to accelerate Transformer: \cite{zhou2022transpim, tu2022trancim,kim2022overview, PIM_TRANS_01, PIM_TRANS_02, PIM_TRANS_03, PIM_TRANS_04}.

By placing the processing unit in/near the memory, the high latency and power consumption caused by data movement can be significantly reduced, making PIM superior for accelerating data-intensive applications. 
By leveraging the benefits of PIM, various fully customized designs based on SRAM have been proposed with its high reliability~\cite{kim2020z, heo2022t, kim2023sp, SRAM_PIM_01, SRAM_PIM_02, SRAM_PIM_03, SRAM_PIM_04, SRAM_PIM_05, SRAM_PIM_06}. However, due to the increasing demand for higher memory capacity with increasing model sizes, PIM designs using the cell with higher density, such as embedded DRAM (eDRAM), have been recently proposed \cite{kim2023dynaplasia, xie2022gain, eDRAM_PIM_01, eDRAM_PIM_02, eDRAM_PIM_03} to provide better area and power efficiency compared to SRAM-based approach.

PIM achieves high throughput by executing multiple PIM macros in parallel. As PIM macros carry out most of the operations, they account for most of the power consumption, about 70\%, as shown in the power breakdown of \cite{tu2022trancim}. Consequently, designing an energy-efficient PIM macro is vital for boosting overall energy efficiency. To accomplish this, previous works mainly focus on optimizing the computational part. To mitigate the area and power overhead that a processing unit imposes on the memory, they have introduced various techniques in computation such as zero-skipping \cite{kim2020z}, approximate computing \cite{wang2022dimc}, and optimized adder tree \cite{chih202116}. However, the importance of memory design, which consumes the most power in PIM operation, has been rather neglected.

\begin{figure}[t]
\centering
\includegraphics[width=0.48\textwidth]{Arxiv/Figure/Introduction.pdf}
\caption{(a) PIM Macro (b) Power Consumption Breakdown}
\vspace{-0.1in}
\label{figure1}
\end{figure}

Figure~\ref{figure1} (a) presents a PIM macro designed to accelerate general matrix-matrix multiplication (GEMM) or general matrix-vector multiplication (GEMV). This macro includes an input buffer, an output buffer, a memory, a 1-bit multiplier, an adder tree, and an accumulator. The multiplier and adder tree facilitate the multiply-accumulate (MAC) operations, and the accumulator merges the resulting partial sum. Figure~\ref{figure1} (b) shows the power consumption breakdown for each unit. As the breakdown shows, memory access consumes 71.98\% of the total power during an operation. This finding underscores the importance of optimizing memory access power to develop an energy-efficient PIM macro.

In this work, we propose \sysname, an energy optimization framework for eDRAM-based PIM. %consisting of a scheduling phase and an execution phase. 
First, we make two pivotal observations to attain high energy efficiency in eDRAM-based PIM: 1) energy consumption of memory access is dominant in PIM; and 2) a trade-off exists among read bitline (RBL) voltage swing, sense amplifier power, and retention time. We find that optimizing memory access power without considering a specific use case is an inefficient approach. Leveraging these insights, we propose a novel reconfigurable eDRAM and retention-aware scheduling. Through the \sysname's retention-aware scheduling, our framework identifies the optimal tiling scheme and memory operation. Then, by controlling the memory access pattern and memory operation based on the outcomes of the scheduling, the power and energy consumption of memory in PIM operation is fully optimized. 

With these approaches, the \sysname framework achieves up to 2.66$\times$, 3.05$\times$, and 8.16$\times$ higher energy efficiency than the worst case, Neural Cache~\cite{eckert2018neural}, and eDRAM baseline for various Transformer models. Our proposed retention-aware scheduler and controller require less than 3.5\% area overhead and 1\% energy overhead, promising effective solutions for the development of eDRAM-based PIM. We summarize the key contributions of our work below.
\begin{figure}[t]
\centering
\includegraphics[width=0.48\textwidth]{Arxiv/Figure/Background.pdf}
\caption{(a) Memory and 2T eDRAM Cell Structure (b) Power Breakdown of eDRAM Access}
%\vspace{-0.1in}
\label{figure2}
\end{figure}

\begin{itemize}
    \item We first analyze how the use case affects energy consumption of PIM and discuss the trade-off involved in the memory operation.
    \item We propose an energy optimization framework for eDRAM-based PIM. The \sysname framework fully optimizes the energy consumption of memory access, which is the main source of energy consumption in PIM architecture.
    \item We develop retention-aware scheduling that can identify the most energy-efficient tiling scheme and memory operation.
    \item We propose a novel reconfigurable eDRAM allowing optimal memory control tailored to the actual use case.
\end{itemize}

