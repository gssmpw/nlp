\begin{abstract}
Although Large Language Models (LLMs) succeed in human-guided conversations such as instruction following and question answering, the potential of LLM-guided conversations—where LLMs direct the discourse and steer the conversation's objectives—remains under-explored. In this study, we first characterize LLM-guided conversation into three fundamental components: (\textit{i}) \textit{Goal Navigation}; (\textit{ii}) \textit{Context Management}; (\textit{iii}) \textit{Empathetic Engagement}, and propose \methodname as an installation. We then implement an interviewing environment for the evaluation of LLM-guided conversation.
Specifically, various topics are involved in this environment for comprehensive interviewing evaluation, resulting in around 1.4k turns of utterances, 184k tokens, and over 200 events mentioned during the interviewing for each chatbot evaluation.
We compare \methodname with 6 state-of-the-art LLMs such as GPT-4o and Llama-3-70b-Instruct, from the perspective of interviewing quality, and autobiography generation quality. For automatic evaluation, we derive user proxies from multiple autobiographies and employ LLM-as-a-judge to score LLM behaviors. We further conduct a human-involved experiment by employing 45 human participants to chat with \methodname and baselines. We then collect human feedback, preferences, and ratings regarding the qualities of conversation and autobiography. Experimental results indicate that \methodname significantly outperforms baseline LLMs in automatic evaluation and achieves consistent leading performances in human ratings.

\end{abstract}