
\section{Experiments for Automatic Evaluation}\label{sec:automatic_eval_exp}
\input{tables/merged_conv_autobio}

\subsection{Experimental Settings}
\noindent\textbf{User Proxy.} We utilize GPT-4-turbo to simulate users for evaluating \methodname in autobiography interviewing. Three LLM user proxies are implemented based on popular autobiographies: \textit{``A Promised Land''} by Barack Obama, \textit{``Jane Eyre: An Autobiography''}, and \textit{``An Autobiography by Catherine Helen Spence''}. They are assigned to role-play the corresponding main character in the autobiography and respond to questions by referencing the autobiography using the Retrieval-Augmented Generation (RAG) approach. See~\cref{appendix:user_proxy} for more details.


\noindent\textbf{Evaluation.} Each chatbot will engage in conversations with all user proxies across 23 interview topics (\cref{appendix:interview_protocol}). The evaluation is three-fold:

\noindent\textbf{(1) Interviewing Quality} measures the capability of LLMs to explore users' major events and life experiences and their ability to document these experiences accurately (\cref{section:interviewing_eval}).
    
\noindent\textbf{(2) Conversation Quality} evaluates whether the responses from the LLM chatbot are comforting and engaging (\cref{section:conversation_eval}).
    
\noindent\textbf{(3) Autobiography Generation:} measures the quality of the generated autobiography, such as insightfulness and narrativity (\cref{section:autobio_eval})

\noindent\textbf{Baseline.} To evaluate the design of \methodname, we employ state-of-the-art LLMs and prompt them to be autobiography interviewers.
For a fair comparison, baseline agents are also equipped with basic goal navigation and context management functions. Please refer to~\cref{appendix:baseline_prompts} for how baseline agents are built. The backbone LLMs for \methodname is fixed to GPT-4o, with the same generative hyperparameters as the baselines. We consider both commercial LLMs, e.g., GPT-4 and GPT-4o~\cite{achiam2023gpt}, and open-source LLMs, e.g., Llama-3-70b-Instruct~\cite{llama3}, Mixtral-8x22B-Instruct~\cite{jiang2024mixtral}, and Qwen2-72b-Instruct~\cite{bai2023qwen}. 


\subsection{Interviewing Quality Evaluation}\label{section:interviewing_eval}
We denote by $E_{intw}=\{e_1, e_2, \cdots\}$ the events extracted during interviewing conversation. 
We denote by $E_{GT}=\{e_1, e_2, \cdots\}$ the events directly extracted from the original. For both \methodname and baselines, $E_{intw}$ are obtained by prompting LLMs to extract events from conversation history (please refer to ~\cref{appendix:interviewing_coverage} for more details):

\noindent\textbf{Interviewing Coverage ($coverage$)} is calculated by the \underline{\textit{date-intersection}} between $E_{intw}$ and $E_{GT}$:

    \small
    $$
    coverage = \frac{|E_{intw} \cap E_{GT}|}{|E_{GT}|} \times 100\%,
    $$
    \normalsize
    where $e_i \in E_{intw} \cap E_{GT} $ if $e_i \in E_{intw}$ and $\exists \,\, e_j \in E_{GT}$ that has the same date as $e_i$, and $|\cdot|$ is the number of elements. 100\% $coverage$ indicates that all the important dates in the user's life are at least mentioned during the interview.
    
\begin{figure*}[t]
    \centering
    \includegraphics[width=0.9\textwidth]{figures/new_ablation_study.pdf}
    \vspace{-3mm}
    \caption{Ablation study of \methodname: how empathetic engagement affects users' (a) positive and (b) negative emotional distributions, (c) statistical results on the number of valid conversation rounds, and (d) the benefits of the MGE in goal navigation.}
    \label{fig:ablation_study}
    \vspace{-5mm}
\end{figure*}

\noindent\textbf{Correctness.} We define the $Precision$ as the percentage of extracted events that are being verified as correct:
    \small
    $$
    Precision = \frac{|E_{correct}|}{|E_{intw}|} \times 100\%, \,\, E_{correct} \subset E_{intw} 
    $$
    \normalsize
    Please refer to~\cref{appendix:interviewing_coverage} for the definition of a correct event. 
    For comprehensive evaluation, we also adopt $Recall$ and $F1$ as evaluation metrics.


 The results of Interviewing Coverage and Correctness are presented in~\cref{tab:memory_explore,tab:memory_explore_janeeyre}, where \methodname shows a significant advantage in Interviewing Coverage over the baselines. This indicates that VIP and MGE are effective techniques for the goal navigation of LLMs in the interviewing framework. For Correctness, \methodname also outperforms baseline agents. Moreover, the $Recall$ and $F1$ indicate that \methodname not only maintains high accuracy for documentation but also extracts more memory events than baselines. The statistics of the interviewing environment are summarized in~\cref{tab:stats}. 

\subsection{Conversation Quality}\label{section:conversation_eval}

Inspired by human evaluation metrics of therapy chatbots from~\citet{Wang2023EnhancingTC}, we design three conversation quality metrics: (\textit{i}) \textbf{\textit{Fluency}}; (\textit{ii}) \textbf{\textit{Identification}}; (\textit{iii}) \textbf{\textit{Comforting}}. We utilize the popular LLM-as-a-judge~\cite{zheng2024judging} evaluation to make GPT-4 decide which conversation is better. Then, we calculate the \textbf{win rate (WR)} and \textbf{loss rate (LR)} of \methodname against baselines. See~\ref{appendix:conv_eval} for the evaluation protocol and prompt templates.

The results in~\cref{tab:conversation_eval} show that \methodname significantly outperforms most baselines in GPT-4-as-a-judge evaluations. With human examinations, we find that baseline agents often resort to simple greetings or summaries, e.g., \textit{``Your commitment to sharing experiences and insights that inspire action and change is truly admirable.}''. Instead of proficiently steering the dialogue to complete the interview, these \underline{\textit{repetitive utterance}} happen multiple times in a session with a baseline agent (\cref{appendix:invalid_repetitive_conv}). In contrast, with our goal navigation module, \methodname provides substantial content at each round of conversation.


\subsection{Autobiography Generation Evaluation}\label{section:autobio_eval}
We follow popular memo evaluations from~\citet{quoraRateCritique, 10.1093/actrade/9780199669240.001.0001, jbp:/content/journals/10.1075/ni.21.2.08smo, 10.1159/000100939} and design three metrics of generated autobiography: \textbf{\textit{Insightfulness}}, \textbf{\textit{Narrativity}}, and \textbf{\textit{Emotional Impact}} (prompt templates can be found in~\cref{appendix:autobio_eval}). Leveraging the same LLM-as-a-judge evaluation protocol in~\cref{section:conversation_eval}, we found that the autobiography generated by \methodname is more favorable than that of baseline agents. 
Examples of generated autobiography are presented in~\cref{appendix:autobio_examples}. 

\subsection{Ablation Study}

\noindent\textbf{Empathetic Engagement.} We study how the Empathetic Engagement (EE) module (\cref{sec:empathetic_engagement}) affects the emotion distribution of user responses. We compare how the intensity of emotions (both positive emotions~\cref{fig:ablation_study}(a) and negative emotions~\cref{fig:ablation_study}(b)) changed when the EE module is enabled and disabled. It is shown that the EE module effectively enhances the user's positive emotions while mitigating negative emotions, indicating that express strategy and emotional sensitivity foster a more positive emotion for users.



\noindent\textbf{Valid Rounds in Conversation.} As outlined in~\cref{section:conversation_eval}, the lack of autonomy in LLMs leads to repetitive responses. We manually count 10 conversation sessions for each chatbot and identify those conversations become repetitive or diverge into irrelevant or nonsensical content. The valid round percentages are calculated as $\frac{\#total\,\,rounds - \#invalid\,\,rounds}{\#total\,\,rounds}$. As shown in~\cref{fig:ablation_study} (c), all baseline models, especially Qwen2-72b-Instruct, show over 50\% meaningless repeats. However, \methodname with a goal navigation module offering diverse and detailed interview questions, has an extensive range of topics and is less prone to repetition.

\noindent\textbf{Conversation Dynamics.} In~\cref{fig:ablation_study} (d), we count the memory events extracted and questions extrapolated at different conversation rounds. Generally, the MGE module identifies around 100 events and extrapolates nearly 40 questions for the LLM's follow-up. This highlights MGE's effectiveness in event management and goal navigation.

