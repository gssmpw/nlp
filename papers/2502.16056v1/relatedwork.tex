\section{Related work}
\label{sec:related_work}


\paragraph{Recent Developments in Neural Causal Discovery}
\label{par:score_based}


%\mg{28.01 new version}

% We evaluated recent neural causal discovery methods such as: NO-TEARS, NO-BEARS, NO-CURL, GRAN-DAG, SCORE, DAGMA, DCDFG, DCDI, DiBS, BayesDAG, SDCD. 
We would like to highlight four recent approaches to neural causal discovery: DCDI~\citep{dcdi}, SDCD~\citep{sdcd}, DiBS~\citep{dibs}, and BayesDAG~\citep{bayesdag}, which in our view effectively represent the major developments in neural causal discovery from  past years. DCDI represents the evolution of NO-TEARS-based methods like GRAN-DAG~\citep{notears, grandag}, improving upon the original by separating structural and functional parameters and incorporating interventional data. SDCD unifies and advances various acyclicity constraints proposed in methods like NO-BEARS~\citep{nobears} and DAGMA~\citep{dagma}, demonstrating superior performance compared to SCORE~\citep{score} and DCDFG~\citep{dcdfg}. For Bayesian approaches, we chose DiBS, which incorporates NO-TEARS regularization in its prior, and BayesDAG, which builds on NO-CURL's DAG parametrization~\citep{nocurl} using MCMC optimization. 

All these approaches use a continuous representation of the graph structure, enforcing a differentiable acyclicity constraint to ensure the result is a DAG. The primary objective is to maximize $\log p_\theta(X|\mathcal{G})$, that is the log-likelihood of the data given the graph while incorporating regularization terms to control graph complexity.
The discovery procedure comprises two parts: fitting functional approximators and structure search, which are usually done in parallel.
% \piotrm{17.02 training -> 'discovery'}\piotrm{17.02 rewrite} % to maximize compute efficiency. 

% \molko{27.01 We may not need that!!!
% This paragraph requires rephasing - we aim to show that there have been a multitude of work towards optimizations and archtectural improvements but all methods use the same functional approximators (no improvements here).}
% During the preliminary phase, we considered the following methods NO-TEARS~\citep{notears}, NO-BEARS~\citep{nobears}, NO-CURL~\citep{nocurl}, GRAN-DAG~\citep{grandag}, SCORE~\citep{score}, DAGMA~\citep{dagma}, DCDFG~\citep{dcdfg}, DCDI~\citep{dcdi}, DiBS~\citep{dibs}, BayesDAG~\citep{bayesdag}, SDCD~\citep{sdcd}, from which we chose DCDI, SDCD, DiBS and BayesDAG. Below we explain why the included ones cover non-included methods.

% NO-TEARS is the first method to use augmented Lagrangian and differentiable constraints to enforce DAGness. However, the suggested formulation entangles functional and structural parameters, making NO-TEARS applicable only to linear models or restricted neural networks. The NO-TEARS method was improved in GRAN-DAG (introduces separate adjacency matrix and sampling based on Gumbel softmax) and then in DCDI (accounts for interventional data). We chose to use DCDI as it is the most developed method in this line of work and has clean implementation.

% An interesting line of work shows articles introducing methods such as NO-BEARS and DAGMA, that were focused on improving the acyclicity constraint introduced in NO-TEARS, all proposed constraints were unified in the SDCD paper, and a new constraint was proposed, that was shown to perform the best. Additionally, SDCD is compared against SCORE and DCDFG again presenting better performance.

% The two other methods are from the class of Bayesian approaches. DiBS method is selected as a Bayesian approach that uses classic NO-TEARS-based regularization embedded in its prior. The BayesDAG is based on the NO-CURL parametrization of DAGs and provides improvements to the optimization pipeline (uses MCMC instead of SVGD).

% We argue that this selection of four methods summarizes various research directions and improvements explored in neural causal discovery over the last four years and well represents the spectrum of existing approaches.
% \paragraph{Properties of Bayesian Networks and the Faithfulness assumption} 

\paragraph{The Faithfulness Property} 
% \piotrm{17.02 condition vs assumption}
% \molko{Lambda strong property, assumption or condition? Can we skip faithfulness?}
%The properties of distributions generated by Bayesian Networks with relation to the underlying graph have been extensively studied. The most common assumption relating the two is the faithfulness assumption. 
The so-called faithfulness property is a fundamental assumption commonly used by causal discovery methods \citep{pearl, dcdi}. 
% It states that $d$-separation in a DAG holds if and only if conditional independencies in the distribution:
\begin{equation}
    \forall_{a,b \in V} \forall_{S \subseteq V \setminus \{a ,b\}}X_a \perp\!\!\!\perp X_b | X_S \iff a \perp_\mathcal{G} b | S.
\end{equation}
where  $\perp\!\!\!\perp$ denotes conditional independence and $\perp_\mathcal{G}$ denotes $d$-separation. For more information on d-separation please refer to Appendix~\ref{appendix:d_separation}.
% \piotrm{17.02 <-- this definition needs to be written properly. What is $\perp_\mathcal{G}$}
The faithfulness property can be violated when multiple causal paths cancel each other (see example in Appendix~\ref{appendix:faith_example}). While unfaithful distributions associated with a given DAG have measure zero in the space of possible distributions~\citep{boeken2025bayesiannetworkstypicallyfaithful}, we can encounter distributions where causal relationships are arbitrarily weak. Without infinite samples, such relationships remain undetectable. 

This observation motivated~\citealp{zhang2003faithfulness} to introduce the $\lambda$-strong faithfulness assumption. % for linear systems. 
A distribution $P$ is said to be $\lambda$-strong faithful to a DAG $\mathcal{G}$ if:

\begin{equation}\label{eq:lambda_faith}
    \forall_{a,b \in V} \forall_{S \subseteq V \setminus \{a ,b\}} |\rho_P(X_a, X_b | X_S)| > \lambda \iff a \not\perp_\mathcal{G} b | S,
\end{equation}

where $\rho_P(X_a, X_b | X_S)$ denotes a partial \mg{20.02 should we write 'linear'?}\molko{21.02 I think they do not really specify that it is linear. Though maybe from the context it can be deduced? I would vote for not specifying unless we have to} correlation coefficient and $\lambda \in (0,1)$\footnote{When $\lambda=0$, $\lambda$ this reduces to the standard faithfulness assumption.}.  For linear systems, this assumption ensures uniform consistency of the PC algorithm with $\lambda \propto 1 / \sqrt{n}$ \mg{can we explain it in one sentence? What doeas it mean? Because I am not sure }where the number of nodes $p = |V|$ is fixed and sample size $n \rightarrow \infty.$ Therefore, $\lambda$ can serve as a notion of the difficulty of the causal discovery task. 

% Additionally, \citet[Section 3.2]{zhang2003faithfulness} note that when a distribution is nearly unfaithful, small changes in parameters can disrupt its dependence structure. In this sense, $\lambda$ in the $\lambda$-Strong-Faithfulness condition provides a rough measure of how sensitive a distribution is to such changes. \piotrm{17.02 <-- do we need this? It hard to parse, and not informaiive?}

Notably ~\citep{geometry_of_faithfulness} proved that, in case of linear SCMs, for any fixed $\lambda > 0$, the fraction of $\lambda$-strong faithful distributions decreases exponentially with graph size and density, suggesting fundamental limitations in causal discovery on large graphs. \piotrm{17.02 rephrase. } \piotrm{17.02 important: how does this depend on $\lambda$.}

While above mentioned work provide theoretical results for linear SCMs and the PC algorithm. So far little has been shown regarding nonlinear functions and contemporary neural network approaches. We aim to breach the gap with our experimental contributions.

For a fixed distribution $P$ and associated with it graph $\mathcal{G}$ there can be multiple $\lambda$ that satisfy the Equation~\ref{eq:lambda_faith}. Therefore, in the remainder of the paper we will denote $\lambda$ as the maximal threshold satisfying the equation. Additionally, as we are working with nonlinear data, we use Spearman correlation coefficient, which we will denote $\rho_P$ with a slight abuse of notation. Specifically, for a given distribution $P$ associated with graphs $\mathcal{G}$ we define:
\begin{multline}\label{eq:lambda_def}
    \lambda = \max\{t: \forall_{a,b \in V} \forall_{S \subseteq V \setminus \{a ,b\}} \\ |\rho_P(X_a, X_b | X_S)| > t \iff a \not\perp_\mathcal{G} b | S\}.
\end{multline}

% \piotrm{17.02 I'm mathematically biased. I'd prefer to see $\forall$ etc}


% \molko{New narration:
% \begin{enumerate}
%     \item 1) We begin nonlinear case analysis by providing method to apprximate lambda. 2) The $\hat{\lambda}$ behaves as expected and described by Uhler on non linear (connected) ER graphs.
%     \item 1) in order to analyse convergeence rate of ncd methods we design a robust approach that minimizes aproximation errors 2) We justify robustness (text, no exps)
%     3) we show that number of required samples depends on lambda
%     4) we provide case study of estimation error, we show tnere is statistically signifficant error in score modelling (we atrribute it to limited data)
%     5) we show that the method improves signifficantly when provided larger dataset
%     \item We conclude that because number of lambda faithfull distibutions vanishes quickly and lambda is connected to the number of samples required to recover the true graph the hight-dimensional effectivenes of the current ncd paradigm seems rather unreaslitic (unless real world data has substantially different characteristic that random NNs)
%     \item (no changes reqruied) We show that the above metioned observations apply to contemporary NCD methods on medium sized graphs. 2) Even medium sized graphs cause problems which highlights the limitaions for scaling this paradighm to larger graphs.
% \end{enumerate}
% }