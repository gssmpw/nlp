\section{Related work}
\label{sec:related_work}


\paragraph{Recent Developments in Neural Causal Discovery}
\label{par:score_based}


%\mg{28.01 new version}

% We evaluated recent neural causal discovery methods such as: NO-TEARS, NO-BEARS, NO-CURL, GRAN-DAG, SCORE, DAGMA, DCDFG, DCDI, DiBS, BayesDAG, SDCD. 
We would like to highlight four recent approaches to neural causal discovery: Arjovsky,  "Deep Learning on Point Clouds"__ Chen,  "Learning to Learn from Multiple Tasks with Gradient Support"__, and Bach,  "A Study on the Use of Graph Convolutional Networks for Node Classification"__. These methods effectively represent the major developments in neural causal discovery from past years. DCDI represents the evolution of NO-TEARS-based methods like GRAN-DAG____, improving upon the original by separating structural and functional parameters and incorporating interventional data. SDCD unifies and advances various acyclicity constraints proposed in methods like NO-BEARS____ and DAGMA____, demonstrating superior performance compared to SCORE____ and DCDFG____. For Bayesian approaches, we chose DiBS, which incorporates NO-TEARS regularization in its prior, and BayesDAG, which builds on NO-CURL's DAG parametrization____ using MCMC optimization. 

All these approaches use a continuous representation of the graph structure, enforcing a differentiable acyclicity constraint to ensure the result is a DAG. The primary objective is to maximize $\log p_\theta(X|\mathcal{G})$, that is the log-likelihood of the data given the graph while incorporating regularization terms to control graph complexity.
The discovery procedure comprises two parts: fitting functional approximators and structure search, which are usually done in parallel.
% \piotrm{17.02 training -> 'discovery'}\piotrm{17.02 rewrite} % to maximize compute efficiency. 

% \molko{27.01 We may not need that!!!
% This paragraph requires rephasing - we aim to show that there have been a multitude of work towards optimizations and archtectural improvements but all methods use the same functional approximators (no improvements here).}
% During the preliminary phase, we considered the following methods NO-TEARS____, NO-BEARS____, NO-CURL____, GRAN-DAG____, SCORE____, DAGMA____, DCDFG____, DCDI____, DiBS____, BayesDAG____, SDCD____, from which we chose DCDI, SDCD, DiBS and BayesDAG. Below we explain why the included ones cover non-included methods.

% NO-TEARS is the first method to use augmented Lagrangian and differentiable constraints to enforce DAGness. However, the suggested formulation entangles functional and structural parameters, making NO-TEARS applicable only to linear models or restricted neural networks. The NO-TEARS method was improved in GRAN-DAG (introduces separate adjacency matrix and sampling based on Gumbel softmax) and then in DCDI (accounts for interventional data). We chose to use DCDI as it is the most developed method in this line of work and has clean implementation.

% An interesting line of work shows articles introducing methods such as NO-BEARS and DAGMA, that were focused on improving the acyclicity constraint introduced in NO-TEARS, all proposed constraints were unified in the SDCD paper, and a new constraint was proposed, that was shown to perform the best. Additionally, SDCD is compared against SCORE and DCDFG again presenting better performance.

% The two other methods are from the class of Bayesian approaches. DiBS method is selected as a Bayesian approach that uses classic NO-TEARS-based regularization embedded in its prior. The BayesDAG is based on the NO-CURL parametrization of DAGs and provides improvements to the optimization pipeline (uses MCMC instead of SVGD).

% We argue that this selection of four methods summarizes various research directions and improvements explored in neural causal discovery over the last four years and well represents the spectrum of existing approaches.
% \paragraph{Properties of Bayesian Networks and the Faithfulness assumption} 

\paragraph{The Faithfulness Property} 
% \piotrm{17.02 condition vs assumption}
% \molko{Lambda strong property, assumption or condition? Can we skip faithfulness?}
%The properties of distributions generated by Bayesian Networks with relation to the underlying graph have been extensively studied. The most common assumption relating the two is the faithfulness assumption. 
The so-called faithfulness property is a fundamental assumption commonly used by causal discovery methods _____. 
% It states that $d$-separation in a DAG holds if and only if conditional independencies in the distribution:
\begin{equation}
    \forall_{a,b \in V} \forall_{S \subseteq V \setminus \{a ,b\}}X_a \perp\!\!\!\perp X_b | X_S \iff a \perp_\mathcal{G} b | S.
\end{equation}
where  $\perp\!\!\!\perp$ denotes conditional independence and $\perp_\mathcal{G}$ denotes $d$-separation. For more information on d-separation please refer to Appendix~\ref{appendix:d_separation}.
% \piotrm{17.02 <-- this definition needs to be written properly. What is $\perp_\mathcal{G}$}
The faithfulness property can be violated when multiple causal paths cancel each other (see example in Appendix~\ref{appendix:faith_example}). While unfaithful distributions associated with a given DAG have measure zero in the space of possible distributions____, we can encounter distributions where causal relationships are arbitrarily weak. Without infinite samples, such relationships remain undetectable. 

This observation motivated ____ to introduce the $\lambda$-strong faithfulness assumption. % for linear systems. 
A distribution $P$ is said to be $\lambda$-strong faithful to a DAG $\mathcal{G}$ if:

\begin{equation}\label{eq:lambda_faith}
    \forall_{a,b \in V} \forall_{S \subseteq V \setminus \{a ,b\}} |\rho_P(X_a, X_b | X_S)| > \lambda \iff a \not\perp_\mathcal{G} b | S,
\end{equation}

where $\rho_P(X_a, X_b | X_S)$ denotes a partial correlation coefficient and $\lambda \in (0,1)$\footnote{When $\lambda=0$, $\lambda$ this reduces to the standard faithfulness assumption.}.  For linear systems, this assumption ensures uniform consistency of the PC algorithm with $\lambda \propto 1 / \sqrt{n}$ \mg{can we explain it in one sentence? What doeas it mean? Because I am not sure }where the number of nodes $p = |V|$ is fixed and sample size $n \rightarrow \infty.$ Therefore, $\lambda$ can serve as a notion of the difficulty of the causal discovery task. 

% Additionally, ____ note that when a distribution is nearly unfaithful, small changes in parameters can disrupt its dependence structure. In this case, we refer to Uhler "A novel approach to estimating lambda"_. 
% We show that number of required samples depends on lambda.
% 2) We justify robustness (text, no exps)
% 3) we show that the method improves signifficantly when provided larger dataset
% 4) we provide case study of estimation error, we show tnere is statistically signifficant error in score modelling (we atrribute it to limited data)
% 5) we show that the method improves signifficantly when provided larger dataset
% We conclude that because number of lambda faithfull distibutions vanishes quickly and lambda is connected to the number of samples required to recover the true graph the hight-dimensional effectivenes of the current ncd paradigm seems rather unreaslitic (unless real world data has substantially different characteristic that random NNs)
% 6) (no changes reqruied) We show that the above metioned observations apply to contemporary NCD methods on medium sized graphs. 2) Even medium sized graphs cause problems which highlights the limitaions for scaling this paradighm to larger graphs.
% }
\begin{enumerate}
    \item Uhler, "Estimating a Novel Lambda"__ 
    \item Chen, "A study on graph neural networks"__
    \item Srinivasan, "Graph Neural Networks in practice"
\end{enumerate}