% Use this file for citations not found in the ACL Anthology (contained in "anthology.bib").

@inproceedings{verma-etal-2023-evaluating,
    title = "Evaluating Paraphrastic Robustness in Textual Entailment Models",
    author = "Verma, Dhruv  and
      Lal, Yash Kumar  and
      Sinha, Shreyashee  and
      Van Durme, Benjamin  and
      Poliak, Adam",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.acl-short.76/",
    doi = "10.18653/v1/2023.acl-short.76",
    pages = "880--892",
    abstract = "We present PaRTE, a collection of 1,126 pairs of Recognizing Textual Entailment (RTE) examples to evaluate whether models are robust to paraphrasing. We posit that if RTE models understand language, their predictions should be consistent across inputs that share the same meaning. We use the evaluation set to determine if RTE models' predictions change when examples are paraphrased. In our experiments, contemporary models change their predictions on 8-16{\%} of paraphrased examples, indicating that there is still room for improvement."
}


@inproceedings{DBLP:conf/icml/KandpalDRWR23,
  author       = {Nikhil Kandpal and
                  Haikang Deng and
                  Adam Roberts and
                  Eric Wallace and
                  Colin Raffel},
  editor       = {Andreas Krause and
                  Emma Brunskill and
                  Kyunghyun Cho and
                  Barbara Engelhardt and
                  Sivan Sabato and
                  Jonathan Scarlett},
  title        = {Large Language Models Struggle to Learn Long-Tail Knowledge},
  booktitle    = {International Conference on Machine Learning, {ICML} 2023, 23-29 July
                  2023, Honolulu, Hawaii, {USA}},
  series       = {Proceedings of Machine Learning Research},
  volume       = {202},
  pages        = {15696--15707},
  publisher    = {{PMLR}},
  year         = {2023},
  url          = {https://proceedings.mlr.press/v202/kandpal23a.html},
  timestamp    = {Mon, 28 Aug 2023 17:23:08 +0200},
  biburl       = {https://dblp.org/rec/conf/icml/KandpalDRWR23.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{petroni,
    title = "Language Models as Knowledge Bases?",
    author = {Petroni, Fabio  and
      Rockt{\"a}schel, Tim  and
      Riedel, Sebastian  and
      Lewis, Patrick  and
      Bakhtin, Anton  and
      Wu, Yuxiang  and
      Miller, Alexander},
    editor = "Inui, Kentaro  and
      Jiang, Jing  and
      Ng, Vincent  and
      Wan, Xiaojun",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-1250",
    doi = "10.18653/v1/D19-1250",
    pages = "2463--2473",
    abstract = "Recent progress in pretraining language models on large textual corpora led to a surge of improvements for downstream NLP tasks. Whilst learning linguistic knowledge, these models may also be storing relational knowledge present in the training data, and may be able to answer queries structured as {``}fill-in-the-blank{''} cloze statements. Language models have many advantages over structured knowledge bases: they require no schema engineering, allow practitioners to query about an open class of relations, are easy to extend to more data, and require no human supervision to train. We present an in-depth analysis of the relational knowledge already present (without fine-tuning) in a wide range of state-of-the-art pretrained language models. We find that (i) without fine-tuning, BERT contains relational knowledge competitive with traditional NLP methods that have some access to oracle knowledge, (ii) BERT also does remarkably well on open-domain question answering against a supervised baseline, and (iii) certain types of factual knowledge are learned much more readily than others by standard language model pretraining approaches. The surprisingly strong ability of these models to recall factual knowledge without any fine-tuning demonstrates their potential as unsupervised open-domain QA systems. The code to reproduce our analysis is available at \url{https://github.com/facebookresearch/LAMA}.",
}


@inproceedings{triviaqa,
    title = "{T}rivia{QA}: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension",
    author = "Joshi, Mandar  and
      Choi, Eunsol  and
      Weld, Daniel  and
      Zettlemoyer, Luke",
    editor = "Barzilay, Regina  and
      Kan, Min-Yen",
    booktitle = "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2017",
    address = "Vancouver, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P17-1147",
    doi = "10.18653/v1/P17-1147",
    pages = "1601--1611",
    abstract = "We present TriviaQA, a challenging reading comprehension dataset containing over 650K question-answer-evidence triples. TriviaQA includes 95K question-answer pairs authored by trivia enthusiasts and independently gathered evidence documents, six per question on average, that provide high quality distant supervision for answering the questions. We show that, in comparison to other recently introduced large-scale datasets, TriviaQA (1) has relatively complex, compositional questions, (2) has considerable syntactic and lexical variability between questions and corresponding answer-evidence sentences, and (3) requires more cross sentence reasoning to find answers. We also present two baseline algorithms: a feature-based classifier and a state-of-the-art neural network, that performs well on SQuAD reading comprehension. Neither approach comes close to human performance (23{\%} and 40{\%} vs. 80{\%}), suggesting that TriviaQA is a challenging testbed that is worth significant future study.",
}


@inproceedings{kassner-etal-2021-multilingual,
    title = "Multilingual {LAMA}: Investigating Knowledge in Multilingual Pretrained Language Models",
    author = {Kassner, Nora  and
      Dufter, Philipp  and
      Sch{\"u}tze, Hinrich},
    editor = "Merlo, Paola  and
      Tiedemann, Jorg  and
      Tsarfaty, Reut",
    booktitle = "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume",
    month = apr,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.eacl-main.284",
    doi = "10.18653/v1/2021.eacl-main.284",
    pages = "3250--3258",
    abstract = "Recently, it has been found that monolingual English language models can be used as knowledge bases. Instead of structural knowledge base queries, masked sentences such as {``}Paris is the capital of [MASK]{''} are used as probes. We translate the established benchmarks TREx and GoogleRE into 53 languages. Working with mBERT, we investigate three questions. (i) Can mBERT be used as a multilingual knowledge base? Most prior work only considers English. Extending research to multiple languages is important for diversity and accessibility. (ii) Is mBERT{'}s performance as knowledge base language-independent or does it vary from language to language? (iii) A multilingual model is trained on more text, e.g., mBERT is trained on 104 Wikipedias. Can mBERT leverage this for better performance? We find that using mBERT as a knowledge base yields varying performance across languages and pooling predictions across languages improves performance. Conversely, mBERT exhibits a language bias; e.g., when queried in Italian, it tends to predict Italy as the country of origin.",
}

@inproceedings{elsahar-etal-2018-rex,
    title = "{T}-{RE}x: A Large Scale Alignment of Natural Language with Knowledge Base Triples",
    author = "Elsahar, Hady  and
      Vougiouklis, Pavlos  and
      Remaci, Arslen  and
      Gravier, Christophe  and
      Hare, Jonathon  and
      Laforest, Frederique  and
      Simperl, Elena",
    editor = "Calzolari, Nicoletta  and
      Choukri, Khalid  and
      Cieri, Christopher  and
      Declerck, Thierry  and
      Goggi, Sara  and
      Hasida, Koiti  and
      Isahara, Hitoshi  and
      Maegaard, Bente  and
      Mariani, Joseph  and
      Mazo, H{\'e}l{\`e}ne  and
      Moreno, Asuncion  and
      Odijk, Jan  and
      Piperidis, Stelios  and
      Tokunaga, Takenobu",
    booktitle = "Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018)",
    month = may,
    year = "2018",
    address = "Miyazaki, Japan",
    publisher = "European Language Resources Association (ELRA)",
    url = "https://aclanthology.org/L18-1544",
}


@inproceedings{DBLP:conf/aaai/YinJY024,
  author       = {Xunjian Yin and
                  Jin Jiang and
                  Liming Yang and
                  Xiaojun Wan},
  editor       = {Michael J. Wooldridge and
                  Jennifer G. Dy and
                  Sriraam Natarajan},
  title        = {History Matters: Temporal Knowledge Editing in Large Language Model},
  booktitle    = {Thirty-Eighth {AAAI} Conference on Artificial Intelligence, {AAAI}
                  2024, Thirty-Sixth Conference on Innovative Applications of Artificial
                  Intelligence, {IAAI} 2024, Fourteenth Symposium on Educational Advances
                  in Artificial Intelligence, {EAAI} 2014, February 20-27, 2024, Vancouver,
                  Canada},
  pages        = {19413--19421},
  publisher    = {{AAAI} Press},
  year         = {2024},
  url          = {https://doi.org/10.1609/aaai.v38i17.29912},
  doi          = {10.1609/AAAI.V38I17.29912},
  timestamp    = {Tue, 02 Apr 2024 16:32:09 +0200},
  biburl       = {https://dblp.org/rec/conf/aaai/YinJY024.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@misc{dyknow,
      title={DyKnow: Dynamically Verifying Time-Sensitive Factual Knowledge in LLMs}, 
      author={Seyed Mahed Mousavi and Simone Alghisi and Giuseppe Riccardi},
      year={2024},
      eprint={2404.08700},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2404.08700}, 
}

@inproceedings{jain-etal-2020-temporal,
    title = "{T}emporal {K}nowledge {B}ase {C}ompletion: {N}ew {A}lgorithms and {E}valuation {P}rotocols",
    author = "Jain, Prachi  and
      Rathi, Sushant  and
      {Mausam}  and
      Chakrabarti, Soumen",
    editor = "Webber, Bonnie  and
      Cohn, Trevor  and
      He, Yulan  and
      Liu, Yang",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-main.305",
    doi = "10.18653/v1/2020.emnlp-main.305",
    pages = "3733--3747",
    abstract = "Research on temporal knowledge bases, which associate a relational fact (s,r,o) with a validity time period (or time instant), is in its early days. Our work considers predicting missing entities (link prediction) and missing time intervals (time prediction) as joint Temporal Knowledge Base Completion (TKBC) tasks, and presents TIMEPLEX, a novel TKBC method, in which entities, relations and, time are all embedded in a uniform, compatible space. TIMEPLEX exploits the recurrent nature of some facts/events and temporal interactions between pairs of relations, yielding state-of-the-art results on both prediction tasks. We also find that existing TKBC models heavily overestimate link prediction performance due to imperfect evaluation mechanisms. In response, we propose improved TKBC evaluation protocols for both link and time prediction tasks, dealing with subtle issues that arise from the partial overlap of time intervals in gold instances and system predictions.",
}




@article{paraphrase1,
  author       = {Harsh Raj and
                  Domenic Rosati and
                  Subhabrata Majumdar},
  title        = {Measuring Reliability of Large Language Models through Semantic Consistency},
  journal      = {CoRR},
  volume       = {abs/2211.05853},
  year         = {2022},
  url          = {https://doi.org/10.48550/arXiv.2211.05853},
  doi          = {10.48550/ARXIV.2211.05853},
  eprinttype    = {arXiv},
  eprint       = {2211.05853},
  timestamp    = {Tue, 15 Nov 2022 15:45:12 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2211-05853.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}



@inproceedings{cc_paraphrase1,
  author       = {Wee Chung Gan and
                  Hwee Tou Ng},
  editor       = {Anna Korhonen and
                  David R. Traum and
                  Llu{\'{\i}}s M{\`{a}}rquez},
  title        = {Improving the Robustness of Question Answering Systems to Question
                  Paraphrasing},
  booktitle    = {Proceedings of the 57th Conference of the Association for Computational
                  Linguistics, {ACL} 2019, Florence, Italy, July 28- August 2, 2019,
                  Volume 1: Long Papers},
  pages        = {6065--6075},
  publisher    = {Association for Computational Linguistics},
  year         = {2019},
  url          = {https://doi.org/10.18653/v1/p19-1610},
  doi          = {10.18653/V1/P19-1610},
  timestamp    = {Fri, 06 Aug 2021 00:41:06 +0200},
  biburl       = {https://dblp.org/rec/conf/acl/GanN19.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}



@inproceedings{cc_paraphrase2,
  author       = {Paulo Alting von Geusau and
                  Peter Bloem},
  editor       = {Mitra Baratchi and
                  Lu Cao and
                  Walter A. Kosters and
                  Jefrey Lijffijt and
                  Jan N. van Rijn and
                  Frank W. Takes},
  title        = {Evaluating the Robustness of Question-Answering Models to Paraphrased
                  Questions},
  booktitle    = {Artificial Intelligence and Machine Learning - 32nd Benelux Conference,
                  BNAIC/Benelearn 2020, Leiden, The Netherlands, November 19-20, 2020,
                  Revised Selected Papers},
  series       = {Communications in Computer and Information Science},
  volume       = {1398},
  pages        = {1--14},
  publisher    = {Springer},
  year         = {2020},
  url          = {https://doi.org/10.1007/978-3-030-76640-5\_1},
  doi          = {10.1007/978-3-030-76640-5\_1},
  timestamp    = {Sat, 30 Sep 2023 09:35:39 +0200},
  biburl       = {https://dblp.org/rec/conf/bnaic/GeusauB20.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}







@inproceedings{robust_typo,
  author       = {Tim Hagen and
                  Harrisen Scells and
                  Martin Potthast},
  editor       = {Yaser Al{-}Onaizan and
                  Mohit Bansal and
                  Yun{-}Nung Chen},
  title        = {Revisiting Query Variation Robustness of Transformer Models},
  booktitle    = {Findings of the Association for Computational Linguistics: {EMNLP}
                  2024, Miami, Florida, USA, November 12-16, 2024},
  pages        = {4283--4296},
  publisher    = {Association for Computational Linguistics},
  year         = {2024},
  url          = {https://aclanthology.org/2024.findings-emnlp.248},
  timestamp    = {Mon, 18 Nov 2024 09:05:59 +0100},
  biburl       = {https://dblp.org/rec/conf/emnlp/HagenSP24.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}



@INPROCEEDINGS{cc_paraphrase3,
  author={Matsuno, Takumi and Tsuchiya, Masatoshi},
  booktitle={2023 10th International Conference on Advanced Informatics: Concept, Theory and Application (ICAICTA)}, 
  title={Evaluating the Robustness of Question Answering Model Against Context Variations}, 
  year={2023},
  volume={},
  number={},
  pages={1-6},
  keywords={Training;Annotations;Machine learning;Encyclopedias;Data models;Robustness;Question answering (information retrieval);Question Answering;Answerability;Model Evaluation},
  doi={10.1109/ICAICTA59291.2023.10390252}}





@inproceedings{cc_paraphrase4,
  author       = {Ishani Mondal and
                  Abhilasha Sancheti},
  title        = {On the robustness of Chatgpt under input perturbations for Named Entity
                  Recognition Task},
  booktitle    = {The Second Tiny Papers Track at {ICLR} 2024, Tiny Papers @ {ICLR}
                  2024, Vienna, Austria, May 11, 2024},
  publisher    = {OpenReview.net},
  year         = {2024},
  url          = {https://openreview.net/forum?id=cyN5Ck1RFT},
  timestamp    = {Fri, 26 Jul 2024 13:58:33 +0200},
  biburl       = {https://dblp.org/rec/conf/iclr/MondalS24.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{
cc_paraphrase5,
title={Measuring Reliability of Large Language Models through Semantic Consistency},
author={Harsh Raj and Domenic Rosati and Subhabrata Majumdar},
booktitle={NeurIPS ML Safety Workshop},
year={2022},
url={https://openreview.net/forum?id=SgbpddeEV-C}
}




@article{pararel,
  author       = {Yanai Elazar and
                  Nora Kassner and
                  Shauli Ravfogel and
                  Abhilasha Ravichander and
                  Eduard H. Hovy and
                  Hinrich Sch{\"{u}}tze and
                  Yoav Goldberg},
  title        = {Measuring and Improving Consistency in Pretrained Language Models},
  journal      = {Trans. Assoc. Comput. Linguistics},
  volume       = {9},
  pages        = {1012--1031},
  year         = {2021},
  url          = {https://doi.org/10.1162/tacl\_a\_00410},
  doi          = {10.1162/TACL\_A\_00410},
  timestamp    = {Wed, 19 Jun 2024 17:28:03 +0200},
  biburl       = {https://dblp.org/rec/journals/tacl/ElazarKRRHSG21.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}



@inproceedings{karr,
  author       = {Qingxiu Dong and
                  Jingjing Xu and
                  Lingpeng Kong and
                  Zhifang Sui and
                  Lei Li},
  editor       = {Alice Oh and
                  Tristan Naumann and
                  Amir Globerson and
                  Kate Saenko and
                  Moritz Hardt and
                  Sergey Levine},
  title        = {Statistical Knowledge Assessment for Large Language Models},
  booktitle    = {Advances in Neural Information Processing Systems 36: Annual Conference
                  on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans,
                  LA, USA, December 10 - 16, 2023},
  year         = {2023},
  url          = {http://papers.nips.cc/paper\_files/paper/2023/hash/5f0a4cd23e1c6eedd3edebba674ab877-Abstract-Conference.html},
  timestamp    = {Tue, 12 Nov 2024 16:30:41 +0100},
  biburl       = {https://dblp.org/rec/conf/nips/DongXKSL23.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@inproceedings{robust_negation,
    title = "Negated and Misprimed Probes for Pretrained Language Models: Birds Can Talk, But Cannot Fly",
    author = {Kassner, Nora  and
      Sch{\"u}tze, Hinrich},
    editor = "Jurafsky, Dan  and
      Chai, Joyce  and
      Schluter, Natalie  and
      Tetreault, Joel",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.698",
    doi = "10.18653/v1/2020.acl-main.698",
    pages = "7811--7818",
    abstract = "Building on Petroni et al. 2019, we propose two new probing tasks analyzing factual knowledge stored in Pretrained Language Models (PLMs). (1) Negation. We find that PLMs do not distinguish between negated ({`}{`}Birds cannot [MASK]{''}) and non-negated ({`}{`}Birds can [MASK]{''}) cloze questions. (2) Mispriming. Inspired by priming methods in human psychology, we add {``}misprimes{''} to cloze questions ({`}{`}Talk? Birds can [MASK]{''}). We find that PLMs are easily distracted by misprimes. These results suggest that PLMs still have a long way to go to adequately learn human-like factual knowledge.",
}


@inproceedings{outdated1_set_the_clock,
  title={Set the Clock: Temporal Alignment of Pretrained Language Models},
  author={Bowen Zhao and Zander Brumbaugh and Yizhong Wang and Hanna Hajishirzi and Noah A. Smith},
  booktitle={Annual Meeting of the Association for Computational Linguistics},
  year={2024},
  url={https://api.semanticscholar.org/CorpusID:268033102}
}


@inproceedings{mindthegap,
  title={Mind the Gap: Assessing Temporal Generalization in Neural Language Models},
  author={Angeliki Lazaridou and Adhiguna Kuncoro and Elena Gribovskaya and Devang Agrawal and Adam Liska and Tayfun Terzi and Mai Gim{\'e}nez and Cyprien de Masson d'Autume and Tom{\'a}s Kocisk{\'y} and Sebastian Ruder and Dani Yogatama and Kris Cao and Susannah Young and Phil Blunsom},
  booktitle={Neural Information Processing Systems},
  year={2021},
  url={https://api.semanticscholar.org/CorpusID:239886013}
}

@inproceedings{tempreason1_mentaqa,
  title={MenatQA: A New Dataset for Testing the Temporal Comprehension and Reasoning Abilities of Large Language Models},
  author={Yifan Wei and Yisong Su and Huanhuan Ma and Xiaoyan Yu and Fangyu Lei and Yuanzhe Zhang and Jun Zhao and Kang Liu},
  booktitle={Conference on Empirical Methods in Natural Language Processing},
  year={2023},
  url={https://api.semanticscholar.org/CorpusID:263831019}
}


@inproceedings{tempreason1_bench_temp_reason,
  title={Towards Benchmarking and Improving the Temporal Reasoning Capability of Large Language Models},
  author={Qingyu Tan and Hwee Tou Ng and Lidong Bing},
  booktitle={Annual Meeting of the Association for Computational Linguistics},
  year={2023},
  url={https://api.semanticscholar.org/CorpusID:259165281}
}



@inproceedings{tempreason_timequestions,
  author       = {Wenhu Chen and
                  Xinyi Wang and
                  William Yang Wang},
  editor       = {Joaquin Vanschoren and
                  Sai{-}Kit Yeung},
  title        = {A Dataset for Answering Time-Sensitive Questions},
  booktitle    = {Proceedings of the Neural Information Processing Systems Track on
                  Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks 2021, December
                  2021, virtual},
  year         = {2021},
  url          = {https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/hash/1f0e3dad99908345f7439f8ffabdffc4-Abstract-round2.html},
  timestamp    = {Thu, 04 Jul 2024 21:52:56 +0200},
  biburl       = {https://dblp.org/rec/conf/nips/ChenWWW21.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}





@article{know_what_language_models_know,
  author       = {Zhengbao Jiang and
                  Frank F. Xu and
                  Jun Araki and
                  Graham Neubig},
  title        = {How Can We Know What Language Models Know},
  journal      = {Trans. Assoc. Comput. Linguistics},
  volume       = {8},
  pages        = {423--438},
  year         = {2020},
  url          = {https://doi.org/10.1162/tacl\_a\_00324},
  doi          = {10.1162/TACL\_A\_00324},
  timestamp    = {Wed, 19 Jun 2024 17:28:03 +0200},
  biburl       = {https://dblp.org/rec/journals/tacl/JiangXAN20.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}





@inproceedings{head_to_tail,
  author       = {Kai Sun and
                  Yifan Ethan Xu and
                  Hanwen Zha and
                  Yue Liu and
                  Xin Luna Dong},
  editor       = {Kevin Duh and
                  Helena G{\'{o}}mez{-}Adorno and
                  Steven Bethard},
  title        = {Head-to-Tail: How Knowledgeable are Large Language Models (LLMs)?
                  {A.K.A.} Will LLMs Replace Knowledge Graphs?},
  booktitle    = {Proceedings of the 2024 Conference of the North American Chapter of
                  the Association for Computational Linguistics: Human Language Technologies
                  (Volume 1: Long Papers), {NAACL} 2024, Mexico City, Mexico, June 16-21,
                  2024},
  pages        = {311--325},
  publisher    = {Association for Computational Linguistics},
  year         = {2024},
  url          = {https://doi.org/10.18653/v1/2024.naacl-long.18},
  doi          = {10.18653/V1/2024.NAACL-LONG.18},
  timestamp    = {Thu, 29 Aug 2024 17:13:57 +0200},
  biburl       = {https://dblp.org/rec/conf/naacl/SunXZLD24.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}





@article{templama,
  author       = {Bhuwan Dhingra and
                  Jeremy R. Cole and
                  Julian Martin Eisenschlos and
                  Daniel Gillick and
                  Jacob Eisenstein and
                  William W. Cohen},
  title        = {Time-Aware Language Models as Temporal Knowledge Bases},
  journal      = {Trans. Assoc. Comput. Linguistics},
  volume       = {10},
  pages        = {257--273},
  year         = {2022},
  url          = {https://doi.org/10.1162/tacl\_a\_00459},
  doi          = {10.1162/TACL\_A\_00459},
  timestamp    = {Fri, 21 Jun 2024 08:19:15 +0200},
  biburl       = {https://dblp.org/rec/journals/tacl/DhingraCEGEC22.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}



@inproceedings{zhang-etal-2023-large,
    title = "How Do Large Language Models Capture the Ever-changing World Knowledge? A Review of Recent Advances",
    author = "Zhang, Zihan  and
      Fang, Meng  and
      Chen, Ling  and
      Namazi-Rad, Mohammad-Reza  and
      Wang, Jun",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.emnlp-main.516",
    doi = "10.18653/v1/2023.emnlp-main.516",
    pages = "8289--8311",
    abstract = "Although large language models (LLMs) are impressive in solving various tasks, they can quickly be outdated after deployment. Maintaining their up-to-date status is a pressing concern in the current era. This paper provides a comprehensive review of recent advances in aligning deployed LLMs with the ever-changing world knowledge. We categorize research works systemically and provide in-depth comparisons and discussions. We also discuss existing challenges and highlight future directions to facilitate research in this field.",
}




@inproceedings{melo,
  author       = {Lang Yu and
                  Qin Chen and
                  Jie Zhou and
                  Liang He},
  editor       = {Michael J. Wooldridge and
                  Jennifer G. Dy and
                  Sriraam Natarajan},
  title        = {{MELO:} Enhancing Model Editing with Neuron-Indexed Dynamic LoRA},
  booktitle    = {Thirty-Eighth {AAAI} Conference on Artificial Intelligence, {AAAI}
                  2024, Thirty-Sixth Conference on Innovative Applications of Artificial
                  Intelligence, {IAAI} 2024, Fourteenth Symposium on Educational Advances
                  in Artificial Intelligence, {EAAI} 2014, February 20-27, 2024, Vancouver,
                  Canada},
  pages        = {19449--19457},
  publisher    = {{AAAI} Press},
  year         = {2024},
  url          = {https://doi.org/10.1609/aaai.v38i17.29916},
  doi          = {10.1609/AAAI.V38I17.29916},
  timestamp    = {Sun, 06 Oct 2024 20:55:17 +0200},
  biburl       = {https://dblp.org/rec/conf/aaai/YuCZH24.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}





@article{rag,
  author       = {Ori Ram and
                  Yoav Levine and
                  Itay Dalmedigos and
                  Dor Muhlgay and
                  Amnon Shashua and
                  Kevin Leyton{-}Brown and
                  Yoav Shoham},
  title        = {In-Context Retrieval-Augmented Language Models},
  journal      = {Trans. Assoc. Comput. Linguistics},
  volume       = {11},
  pages        = {1316--1331},
  year         = {2023},
  url          = {https://doi.org/10.1162/tacl\_a\_00605},
  doi          = {10.1162/TACL\_A\_00605},
  timestamp    = {Wed, 19 Jun 2024 17:28:03 +0200},
  biburl       = {https://dblp.org/rec/journals/tacl/RamLDMSLS23.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}





@inproceedings{rome,
  author       = {Kevin Meng and
                  David Bau and
                  Alex Andonian and
                  Yonatan Belinkov},
  editor       = {Sanmi Koyejo and
                  S. Mohamed and
                  A. Agarwal and
                  Danielle Belgrave and
                  K. Cho and
                  A. Oh},
  title        = {Locating and Editing Factual Associations in {GPT}},
  booktitle    = {Advances in Neural Information Processing Systems 35: Annual Conference
                  on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans,
                  LA, USA, November 28 - December 9, 2022},
  year         = {2022},
  url          = {http://papers.nips.cc/paper\_files/paper/2022/hash/6f1d43d5a82a37e89b0665b33bf3a182-Abstract-Conference.html},
  timestamp    = {Mon, 08 Jan 2024 16:31:36 +0100},
  biburl       = {https://dblp.org/rec/conf/nips/MengBAB22.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}





@inproceedings{grace,
  author       = {Tom Hartvigsen and
                  Swami Sankaranarayanan and
                  Hamid Palangi and
                  Yoon Kim and
                  Marzyeh Ghassemi},
  editor       = {Alice Oh and
                  Tristan Naumann and
                  Amir Globerson and
                  Kate Saenko and
                  Moritz Hardt and
                  Sergey Levine},
  title        = {Aging with {GRACE:} Lifelong Model Editing with Discrete Key-Value
                  Adaptors},
  booktitle    = {Advances in Neural Information Processing Systems 36: Annual Conference
                  on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans,
                  LA, USA, December 10 - 16, 2023},
  year         = {2023},
  url          = {http://papers.nips.cc/paper\_files/paper/2023/hash/95b6e2ff961580e03c0a662a63a71812-Abstract-Conference.html},
  timestamp    = {Fri, 01 Mar 2024 16:26:20 +0100},
  biburl       = {https://dblp.org/rec/conf/nips/HartvigsenSPKG23.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}



@inproceedings{streaming_qa,
  author       = {Adam Liska and
                  Tom{\'{a}}s Kocisk{\'{y}} and
                  Elena Gribovskaya and
                  Tayfun Terzi and
                  Eren Sezener and
                  Devang Agrawal and
                  Cyprien de Masson d'Autume and
                  Tim Scholtes and
                  Manzil Zaheer and
                  Susannah Young and
                  Ellen Gilsenan{-}McMahon and
                  Sophia Austin and
                  Phil Blunsom and
                  Angeliki Lazaridou},
  editor       = {Kamalika Chaudhuri and
                  Stefanie Jegelka and
                  Le Song and
                  Csaba Szepesv{\'{a}}ri and
                  Gang Niu and
                  Sivan Sabato},
  title        = {StreamingQA: {A} Benchmark for Adaptation to New Knowledge over Time
                  in Question Answering Models},
  booktitle    = {International Conference on Machine Learning, {ICML} 2022, 17-23 July
                  2022, Baltimore, Maryland, {USA}},
  series       = {Proceedings of Machine Learning Research},
  volume       = {162},
  pages        = {13604--13622},
  publisher    = {{PMLR}},
  year         = {2022},
  url          = {https://proceedings.mlr.press/v162/liska22a.html},
  timestamp    = {Tue, 12 Jul 2022 17:36:52 +0200},
  biburl       = {https://dblp.org/rec/conf/icml/LiskaKGTSAdSZYG22.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}



@inproceedings{outdated2_carpediem,
  author       = {Yujin Kim and
                  Jaehong Yoon and
                  Seonghyeon Ye and
                  Sangmin Bae and
                  Namgyu Ho and
                  Sung Ju Hwang and
                  Se{-}Young Yun},
  editor       = {Kevin Duh and
                  Helena G{\'{o}}mez{-}Adorno and
                  Steven Bethard},
  title        = {Carpe diem: On the Evaluation of World Knowledge in Lifelong Language
                  Models},
  booktitle    = {Proceedings of the 2024 Conference of the North American Chapter of
                  the Association for Computational Linguistics: Human Language Technologies
                  (Volume 1: Long Papers), {NAACL} 2024, Mexico City, Mexico, June 16-21,
                  2024},
  pages        = {5401--5415},
  publisher    = {Association for Computational Linguistics},
  year         = {2024},
  url          = {https://doi.org/10.18653/v1/2024.naacl-long.302},
  doi          = {10.18653/V1/2024.NAACL-LONG.302},
  timestamp    = {Thu, 29 Aug 2024 17:13:57 +0200},
  biburl       = {https://dblp.org/rec/conf/naacl/KimYYBHHY24.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}




@inproceedings{outdated3_signal,
  author       = {Katerina Margatina and
                  Shuai Wang and
                  Yogarshi Vyas and
                  Neha Anna John and
                  Yassine Benajiba and
                  Miguel Ballesteros},
  editor       = {Andreas Vlachos and
                  Isabelle Augenstein},
  title        = {Dynamic Benchmarking of Masked Language Models on Temporal Concept
                  Drift with Multiple Views},
  booktitle    = {Proceedings of the 17th Conference of the European Chapter of the
                  Association for Computational Linguistics, {EACL} 2023, Dubrovnik,
                  Croatia, May 2-6, 2023},
  pages        = {2873--2890},
  publisher    = {Association for Computational Linguistics},
  year         = {2023},
  url          = {https://doi.org/10.18653/v1/2023.eacl-main.211},
  doi          = {10.18653/V1/2023.EACL-MAIN.211},
  timestamp    = {Thu, 05 Oct 2023 18:04:58 +0200},
  biburl       = {https://dblp.org/rec/conf/eacl/MargatinaWVJBB23.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}



@inproceedings{outdated4_realtime,
  author       = {Jungo Kasai and
                  Keisuke Sakaguchi and
                  Yoichi Takahashi and
                  Ronan Le Bras and
                  Akari Asai and
                  Xinyan Yu and
                  Dragomir Radev and
                  Noah A. Smith and
                  Yejin Choi and
                  Kentaro Inui},
  editor       = {Alice Oh and
                  Tristan Naumann and
                  Amir Globerson and
                  Kate Saenko and
                  Moritz Hardt and
                  Sergey Levine},
  title        = {RealTime {QA:} What's the Answer Right Now?},
  booktitle    = {Advances in Neural Information Processing Systems 36: Annual Conference
                  on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans,
                  LA, USA, December 10 - 16, 2023},
  year         = {2023},
  url          = {http://papers.nips.cc/paper\_files/paper/2023/hash/9941624ef7f867a502732b5154d30cb7-Abstract-Datasets\_and\_Benchmarks.html},
  timestamp    = {Fri, 01 Mar 2024 16:26:20 +0100},
  biburl       = {https://dblp.org/rec/conf/nips/KasaiST0A0RS0I23.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}



@inproceedings{outdated5_dyknow,
  author       = {Seyed Mahed Mousavi and
                  Simone Alghisi and
                  Giuseppe Riccardi},
  editor       = {Yaser Al{-}Onaizan and
                  Mohit Bansal and
                  Yun{-}Nung Chen},
  title        = {DyKnow: Dynamically Verifying Time-Sensitive Factual Knowledge in
                  LLMs},
  booktitle    = {Findings of the Association for Computational Linguistics: {EMNLP}
                  2024, Miami, Florida, USA, November 12-16, 2024},
  pages        = {8014--8029},
  publisher    = {Association for Computational Linguistics},
  year         = {2024},
  url          = {https://aclanthology.org/2024.findings-emnlp.471},
  timestamp    = {Mon, 18 Nov 2024 09:05:59 +0100},
  biburl       = {https://dblp.org/rec/conf/emnlp/MousaviAR24.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@article{allen_temp_logic,
  title={Maintaining knowledge about temporal intervals},
  author={James F. Allen},
  journal={Commun. ACM},
  year={1983},
  volume={26},
  pages={832-843},
  url={https://api.semanticscholar.org/CorpusID:16729000}
}


@misc{mistral,
      title={Mistral 7B}, 
      author={Albert Q. Jiang and Alexandre Sablayrolles and Arthur Mensch and Chris Bamford and Devendra Singh Chaplot and Diego de las Casas and Florian Bressand and Gianna Lengyel and Guillaume Lample and Lucile Saulnier and Lélio Renard Lavaud and Marie-Anne Lachaux and Pierre Stock and Teven Le Scao and Thibaut Lavril and Thomas Wang and Timothée Lacroix and William El Sayed},
      year={2023},
      eprint={2310.06825},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2310.06825}, 
}

@inproceedings{openelm,
title = {OpenELM: An Efficient Language Model Family with Open Training and Inference Framework},
booktitle = {ICML Workshop},
author = {Sachin Mehta and Mohammad Sekhavat and Qingqing Cao and Max Horton and Yanzi Jin and Frank Sun and Iman Mirzadeh and Mahyar Najibikohnehshahri and Dmitry Belenko and Peter Zatloukal and Mohammad Rastegari},
year = {2024},
URL = {https://arxiv.org/abs/2404.14619}
}


@misc{gemma2,
      title={Gemma 2: Improving Open Language Models at a Practical Size}, 
      author={Gemma Team and Morgane Riviere and Shreya Pathak et el.},
      year={2024},
      eprint={2408.00118},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2408.00118}, 
}

@misc{llama3,
      title={The Llama 3 Herd of Models}, 
      author={Aaron Grattafiori and Abhimanyu Dubey and Abhinav Jauhri et al.},
      year={2024},
      eprint={2407.21783},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2407.21783}, 
}



@article{wikidata,
  author       = {Denny Vrandecic and
                  Markus Kr{\"{o}}tzsch},
  title        = {Wikidata: a free collaborative knowledgebase},
  journal      = {Commun. {ACM}},
  volume       = {57},
  number       = {10},
  pages        = {78--85},
  year         = {2014},
  url          = {https://doi.org/10.1145/2629489},
  doi          = {10.1145/2629489},
  timestamp    = {Wed, 14 Nov 2018 10:22:37 +0100},
  biburl       = {https://dblp.org/rec/journals/cacm/VrandecicK14.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}



@inproceedings{ammar-khodja-etal-2025-factual,
    title = "Factual Knowledge Assessment of Language Models Using Distractors",
    author = "Ammar Khodja, Hichem  and
      Ait gueni ssaid, Abderrahmane  and
      Bechet, Frederic  and
      Brabant, Quentin  and
      Nasr, Alexis  and
      Lecorv{\'e}, Gw{\'e}nol{\'e}",
    editor = "Rambow, Owen  and
      Wanner, Leo  and
      Apidianaki, Marianna  and
      Al-Khalifa, Hend  and
      Eugenio, Barbara Di  and
      Schockaert, Steven",
    booktitle = "Proceedings of the 31st International Conference on Computational Linguistics",
    month = jan,
    year = "2025",
    address = "Abu Dhabi, UAE",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2025.coling-main.537/",
    pages = "8043--8056",
    abstract = "Language models encode extensive factual knowledge within their parameters. The accurate assessment of this knowledge is crucial for understanding and improving these models. In the literature, factual knowledge assessment often relies on cloze sentences, which can lead to erroneous conclusions due to the complexity of natural language (out-of-subject continuations, the existence of many correct answers and the several ways of expressing them). In this paper, we introduce a new interpretable knowledge assessment method that mitigates these issues by leveraging distractors{---}incorrect but plausible alternatives to the correct answer. We propose several strategies for retrieving distractors and determine the most effective one through experimentation. Our method is evaluated against existing approaches, demonstrating solid alignment with human judgment and stronger robustness to verbalization artifacts. The code and data to reproduce our experiments are available on GitHub."
}

@inproceedings{lyu-etal-2024-beyond,
    title = "Beyond Probabilities: Unveiling the Misalignment in Evaluating Large Language Models",
    author = "Lyu, Chenyang  and
      Wu, Minghao  and
      Aji, Alham",
    editor = "Li, Sha  and
      Li, Manling  and
      Zhang, Michael JQ  and
      Choi, Eunsol  and
      Geva, Mor  and
      Hase, Peter  and
      Ji, Heng",
    booktitle = "Proceedings of the 1st Workshop on Towards Knowledgeable Language Models (KnowLLM 2024)",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.knowllm-1.10/",
    doi = "10.18653/v1/2024.knowllm-1.10",
    pages = "109--131",
    abstract = "Large Language Models (LLMs) have demonstrated remarkable capabilities across various applications, fundamentally reshaping the landscape of natural language processing (NLP) research. However, recent evaluation frameworks often rely on the output probabilities of LLMs for predictions, primarily due to computational constraints, diverging from real-world LLM usage scenarios. While widely employed, the efficacy of these probability-based evaluation strategies remains an open research question. This study aims to scrutinize the validity of such probability-based evaluation methods within the context of using LLMs for Multiple Choice Questions (MCQs), highlighting their inherent limitations. Our empirical investigation reveals that the prevalent probability-based evaluation method inadequately aligns with generation-based prediction. Furthermore, current evaluation frameworks typically assess LLMs through predictive tasks based on output probabilities rather than directly generating responses, owing to computational limitations. We illustrate that these probability-based approaches do not effectively correspond with generative predictions. The outcomes of our study can enhance the understanding of LLM evaluation methodologies and provide insights for future research in this domain."
}




@inproceedings{DBLP:conf/nips/DongXKSL23,
  author       = {Qingxiu Dong and
                  Jingjing Xu and
                  Lingpeng Kong and
                  Zhifang Sui and
                  Lei Li},
  editor       = {Alice Oh and
                  Tristan Naumann and
                  Amir Globerson and
                  Kate Saenko and
                  Moritz Hardt and
                  Sergey Levine},
  title        = {Statistical Knowledge Assessment for Large Language Models},
  booktitle    = {Advances in Neural Information Processing Systems 36: Annual Conference
                  on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans,
                  LA, USA, December 10 - 16, 2023},
  year         = {2023},
  url          = {http://papers.nips.cc/paper\_files/paper/2023/hash/5f0a4cd23e1c6eedd3edebba674ab877-Abstract-Conference.html},
  timestamp    = {Tue, 12 Nov 2024 16:30:41 +0100},
  biburl       = {https://dblp.org/rec/conf/nips/DongXKSL23.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{ammar-khodja-etal-2024-wikifactdiff,
    title = "{W}iki{F}act{D}iff: A Large, Realistic, and Temporally Adaptable Dataset for Atomic Factual Knowledge Update in Causal Language Models",
    author = "Ammar Khodja, Hichem  and
      B{\'e}chet, Fr{\'e}d{\'e}ric  and
      Brabant, Quentin  and
      Nasr, Alexis  and
      Lecorv{\'e}, Gw{\'e}nol{\'e}",
    editor = "Calzolari, Nicoletta  and
      Kan, Min-Yen  and
      Hoste, Veronique  and
      Lenci, Alessandro  and
      Sakti, Sakriani  and
      Xue, Nianwen",
    booktitle = "Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)",
    month = may,
    year = "2024",
    address = "Torino, Italia",
    publisher = "ELRA and ICCL",
    url = "https://aclanthology.org/2024.lrec-main.1532/",
    pages = "17614--17624",
    abstract = "The factuality of large language model (LLMs) tends to decay over time since events posterior to their training are {\textquotedblleft}unknown{\textquotedblright} to them. One way to keep models up-to-date could be factual update: the task of inserting, replacing, or removing certain simple (atomic) facts within the model. To study this task, we present WikiFactDiff, a dataset that describes the evolution of factual knowledge between two dates as a collection of simple facts divided into three categories: new, obsolete, and static. We describe several update scenarios arising from various combinations of these three types of basic update. The facts are represented by subject-relation-object triples; indeed, WikiFactDiff was constructed by comparing the state of the Wikidata knowledge base at 4 January 2021 and 27 February 2023. Those fact are accompanied by verbalization templates and cloze tests that enable running update algorithms and their evaluation metrics. Contrary to other datasets, such as zsRE and CounterFact, WikiFactDiff constitutes a realistic update setting that involves various update scenarios, including replacements, archival, and new entity insertions. We also present an evaluation of existing update algorithms on WikiFactDiff."
}

@inproceedings{kang-choi-2023-impact,
    title = "Impact of Co-occurrence on Factual Knowledge of Large Language Models",
    author = "Kang, Cheongwoong  and
      Choi, Jaesik",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2023",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.findings-emnlp.518/",
    doi = "10.18653/v1/2023.findings-emnlp.518",
    pages = "7721--7735",
    abstract = "Large language models (LLMs) often make factually incorrect responses despite their success in various applications. In this paper, we hypothesize that relying heavily on simple co-occurrence statistics of the pre-training corpora is one of the main factors that cause factual errors. Our results reveal that LLMs are vulnerable to the co-occurrence bias, defined as preferring frequently co-occurred words over the correct answer. Consequently, LLMs struggle to recall facts whose subject and object rarely co-occur in the pre-training dataset although they are seen during finetuning. We show that co-occurrence bias remains despite scaling up model sizes or finetuning. Therefore, we suggest finetuning on a debiased dataset to mitigate the bias by filtering out biased samples whose subject-object co-occurrence count is high. Although debiased finetuning allows LLMs to memorize rare facts in the training set, it is not effective in recalling rare facts unseen during finetuning. Further research in mitigation will help build reliable language models by preventing potential errors. The code is available at https://github.com/CheongWoong/impact{\_}of{\_}cooccurrence."
}


@inproceedings{zhang-choi-2021-situatedqa,
    title = "{S}ituated{QA}: Incorporating Extra-Linguistic Contexts into {QA}",
    author = "Zhang, Michael  and
      Choi, Eunsol",
    editor = "Moens, Marie-Francine  and
      Huang, Xuanjing  and
      Specia, Lucia  and
      Yih, Scott Wen-tau",
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2021",
    address = "Online and Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.emnlp-main.586/",
    doi = "10.18653/v1/2021.emnlp-main.586",
    pages = "7371--7387",
    abstract = "Answers to the same question may change depending on the extra-linguistic contexts (when and where the question was asked). To study this challenge, we introduce SituatedQA, an open-retrieval QA dataset where systems must produce the correct answer to a question given the temporal or geographical context. To construct SituatedQA, we first identify such questions in existing QA datasets. We find that a significant proportion of information seeking questions have context-dependent answers (e.g. roughly 16.5{\%} of NQ-Open). For such context-dependent questions, we then crowdsource alternative contexts and their corresponding answers. Our study shows that existing models struggle with producing answers that are frequently updated or from uncommon locations. We further quantify how existing models, which are trained on data collected in the past, fail to generalize to answering questions asked in the present, even when provided with an updated evidence corpus (a roughly 15 point drop in accuracy). Our analysis suggests that open-retrieval QA benchmarks should incorporate extra-linguistic context to stay relevant globally and in the future. Our data, code, and datasheet are available at \url{https://situatedqa.github.io/}."
}
@inproceedings{chu-etal-2024-timebench,
    title = "{T}ime{B}ench: A Comprehensive Evaluation of Temporal Reasoning Abilities in Large Language Models",
    author = "Chu, Zheng  and
      Chen, Jingchang  and
      Chen, Qianglong  and
      Yu, Weijiang  and
      Wang, Haotian  and
      Liu, Ming  and
      Qin, Bing",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.acl-long.66/",
    doi = "10.18653/v1/2024.acl-long.66",
    pages = "1204--1228",
    abstract = "Grasping the concept of time is a fundamental facet of human cognition, indispensable for truly comprehending the intricacies of the world.Previous studies typically focus on specific aspects of time, lacking a comprehensive temporal reasoning benchmark.To address this, we propose TimeBench, a comprehensive hierarchical temporal reasoning benchmark that covers a broad spectrum of temporal reasoning phenomena.TimeBench provides a thorough evaluation for investigating the temporal reasoning capabilities of large language models.We conduct extensive experiments on GPT-4, LLaMA2, and other popular LLMs under various settings.Our experimental results indicate a significant performance gap between the state-of-the-art LLMs and humans, highlighting that there is still a considerable distance to cover in temporal reasoning.Besides, LLMs exhibit capability discrepancies across different reasoning categories.Furthermore, we thoroughly analyze the impact of multiple aspects on temporal reasoning and emphasize the associated challenges.We aspire for TimeBench to serve as a comprehensive benchmark, fostering research in temporal reasoning.Code and data are available at https://github.com/zchuz/TimeBench."
}

@misc{fatemi2024testtimebenchmarkevaluating,
      title={Test of Time: A Benchmark for Evaluating LLMs on Temporal Reasoning}, 
      author={Bahare Fatemi and Mehran Kazemi and Anton Tsitsulin and Karishma Malkan and Jinyeong Yim and John Palowitch and Sungyong Seo and Jonathan Halcrow and Bryan Perozzi},
      year={2024},
      eprint={2406.09170},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2406.09170}, 
}

@inproceedings{xiong-etal-2024-large,
    title = "Large Language Models Can Learn Temporal Reasoning",
    author = "Xiong, Siheng  and
      Payani, Ali  and
      Kompella, Ramana  and
      Fekri, Faramarz",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.acl-long.563/",
    doi = "10.18653/v1/2024.acl-long.563",
    pages = "10452--10470",
    abstract = "While large language models (LLMs) have demonstrated remarkable reasoning capabilities, they are not without their flaws and inaccuracies. Recent studies have introduced various methods to mitigate these limitations. Temporal reasoning (TR), in particular, presents a significant challenge for LLMs due to its reliance on diverse temporal concepts and intricate temporal logic. In this paper, we propose TG-LLM, a novel framework towards language-based TR. Instead of reasoning over the original context, we adopt a latent representation, temporal graph (TG) that enhances the learning of TR. A synthetic dataset (TGQA), which is fully controllable and requires minimal supervision, is constructed for fine-tuning LLMs on this text-to-TG translation task. We confirmed in experiments that the capability of TG translation learned on our dataset can be transferred to other TR tasks and benchmarks. On top of that, we teach LLM to perform deliberate reasoning over the TGs via Chain-of-Thought (CoT) bootstrapping and graph data augmentation. We observed that those strategies, which maintain a balance between usefulness and diversity, bring more reliable CoTs and final results than the vanilla CoT distillation."
}


@inproceedings{su-etal-2024-living,
    title = "Living in the Moment: Can Large Language Models Grasp Co-Temporal Reasoning?",
    author = "Su, Zhaochen  and
      Li, Juntao  and
      Zhang, Jun  and
      Zhu, Tong  and
      Qu, Xiaoye  and
      Zhou, Pan  and
      Bowen, Yan  and
      Cheng, Yu  and
      Zhang, Min",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.acl-long.703/",
    doi = "10.18653/v1/2024.acl-long.703",
    pages = "13014--13033",
    abstract = "Temporal reasoning is fundamental for large language models (LLMs) to comprehend the world. Current temporal reasoning datasets are limited to questions about single or isolated events, falling short in mirroring the realistic temporal characteristics involving concurrent nature and intricate temporal interconnections. In this paper, we introduce CoTempQA, a comprehensive co-temporal Question Answering (QA) benchmark containing four co-temporal scenarios (Equal, Overlap, During, Mix) with 4,748 samples for evaluating the co-temporal comprehension and reasoning abilities of LLMs. Our extensive experiments reveal a significant gap between the performance of current LLMs and human-level reasoning on CoTempQA tasks. Even when enhanced with Chain of Thought (CoT) methodologies, models consistently struggle with our task. In our preliminary exploration, we discovered that mathematical reasoning plays a significant role in handling co-temporal events and proposed a strategy to boost LLMs' co-temporal reasoning from a mathematical perspective. We hope that our CoTempQA datasets will encourage further advancements in improving the co-temporal reasoning capabilities of LLMs."
}
