% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[preprint]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}
\usepackage{amsmath}
\usepackage{dsfont}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{tcolorbox}
\usepackage{url}
\usepackage{hyperref}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{bbm}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

\newcommand{\blank}{\_\_\_\_}
\newcommand{\ie}{i.e., }
\newcommand{\eg}{e.g., }

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}
\usepackage{pifont}
\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%

\newcommand{\khi}{\mbox{\Large$\chi$}}
\usepackage{array}

\newcommand{\qb}[1]{\textcolor{purple}{#1}}

\newcolumntype{H}{@{}>{\lrbox0}l<{\endlrbox}}
% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{Language Models Struggle to Achieve a Consistent Temporal Representation of Facts}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a separate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

\author{
 \textbf{Hichem Ammar Khodja\textsuperscript{1,2}},
 \textbf{Frédéric Béchet\textsuperscript{2,3}},
 \textbf{Quentin Brabant\textsuperscript{1}},
\\
 \textbf{Alexis Nasr\textsuperscript{2}},
 \textbf{Gwénolé Lecorvé\textsuperscript{1}}
\\
 \textsuperscript{1}Orange - \textit{Lannion, France},\\
 \textsuperscript{2}Aix Marseille Université, CNRS, LIS, UMR 7020 - \textit{Marseille, France},\\
 \textsuperscript{3}International Laboratory on Learning Systems (ILLS - IRL2020 CNRS)
\\
 \small{
   \textbf{Correspondence:} \texttt{\{hichem.ammarkhodja, quentin.brabant, gwenole.lecorve\}@orange.com},
 }\\
 \small{
 \texttt{\{frederic.bechet, alexis.nasr\}@lis-lab.fr}
 }
}


%\author{
%  \textbf{First Author\textsuperscript{1}},
%  \textbf{Second Author\textsuperscript{1,2}},
%  \textbf{Third T. Author\textsuperscript{1}},
%  \textbf{Fourth Author\textsuperscript{1}},
%\\
%  \textbf{Fifth Author\textsuperscript{1,2}},
%  \textbf{Sixth Author\textsuperscript{1}},
%  \textbf{Seventh Author\textsuperscript{1}},
%  \textbf{Eighth Author \textsuperscript{1,2,3,4}},
%\\
%  \textbf{Ninth Author\textsuperscript{1}},
%  \textbf{Tenth Author\textsuperscript{1}},
%  \textbf{Eleventh E. Author\textsuperscript{1,2,3,4,5}},
%  \textbf{Twelfth Author\textsuperscript{1}},
%\\
%  \textbf{Thirteenth Author\textsuperscript{3}},
%  \textbf{Fourteenth F. Author\textsuperscript{2,4}},
%  \textbf{Fifteenth Author\textsuperscript{1}},
%  \textbf{Sixteenth Author\textsuperscript{1}},
%\\
%  \textbf{Seventeenth S. Author\textsuperscript{4,5}},
%  \textbf{Eighteenth Author\textsuperscript{3,4}},
%  \textbf{Nineteenth N. Author\textsuperscript{2,5}},
%  \textbf{Twentieth Author\textsuperscript{1}}
%\\
%\\
%  \textsuperscript{1}Affiliation 1,
%  \textsuperscript{2}Affiliation 2,
%  \textsuperscript{3}Affiliation 3,
%  \textsuperscript{4}Affiliation 4,
%  \textsuperscript{5}Affiliation 5
%\\
%  \small{
%    \textbf{Correspondence:} \href{mailto:email@domain}{email@domain}
%  }
%}

\begin{document}
\maketitle
\begin{abstract}
Language Models (LMs) have shown substantial improvements in handling factual knowledge, yet their capability to consistently represent temporal facts, which are valid only within specific timeframes, remains underexplored. To investigate this, we introduce TimeStress, a novel dataset comprising 521K statements on 2003 of the most popular temporal facts in Wikidata. Each statement contextualizes a fact with correct and incorrect dates across three precisions (Day, Month, Year). This setup allows us to evaluate LMs' ability to discern between correct and incorrect temporal statements based on their probability of being generated. We assess 18 LMs across various architectures using two metrics: the win rate, indicating how often correct dates outperform incorrect ones, and robustness, reflecting consistent performance across all dates. Our findings reveal that while some LMs achieve a win rate exceeding 80\%, robustness remains low, with the best model achieving only 6\%. Furthermore, robust knowledge at one date precision does not reliably transfer to others, highlighting a significant generalization gap. These results underscore the struggle of LMs to maintain a consistent temporal representation, supporting their limitations as reliable sources of temporal knowledge. We provide all data and code for further research.
\end{abstract}

\section{Introduction}

% FINAL VERSION

A language model (LM) that correctly completes the cloze sentence \texttt{``The capital of France is \blank''} with \texttt{Paris} demonstrates that it has stored the fact somewhere in its parameters. However, merely recalling a fact is insufficient: to be reliable, the LM must also maintain a consistent and robust representation of that fact, especially when the fact is temporally contextualized. While previous research has focused on testing LMs against textual perturbations such as paraphrasing \cite{pararel}, alias substitution \cite{karr}, typographical errors \cite{robust_typo}, and negation \cite{robust_negation}, the temporal dimension of factual knowledge is less studied.

In this paper, we focus on the robustness of LMs in handling \emph{temporal facts}, facts that are valid only within specific timeframes. As data material for this study, we introduce \textbf{TimeStress}, a carefully designed dataset comprising 521K English statements about 2,003 of the most popular temporal facts from the Wikidata knowledge base \cite{wikidata}.
These statements are temporally contextualized questions followed by their answer, such as: ``In 2011, who was the president of the USA? Barack Obama'', which corresponds to the fact (Barack Obama, president of, USA), which is valid from January 20, 2009 to January 20, 2017.


\begin{figure}[t!]
    \centering
    \includegraphics[width=1\linewidth]{figs/robustness_illustration.pdf}
    \caption{The language model's consistency on a fact is assessed by asking it to differentiate between a set of correct and incorrect statements. The temporal context is varied on two dimensions: its position on the timeline (row 1 and 2) and its precision (row 1 and 3).}
    \label{fig:robustness_illustration}
\end{figure}
For each fact, TimeStress contains statements with various dates, on three precision levels: \textit{Year}, \textit{Month}, \textit{Day}. Some of these dates are correct (i.e, they belong to the validity period of the fact), and some are not. To enable the comparison between date precision levels, each fact has an equal number of correct and incorrect dates across precisions levels. All questions were generated using GPT-4o, and their quality was checked manually on a random sample. 

In our approach, language models are evaluated by assessing their ability to assign higher probabilities to correct statements over incorrect ones. Combining this approahc with TimeStress enables the evaluation of the consistency of the temporal representation of facts by varying the temporal context on two crucial dimensions: their \textbf{position on the timeline} and their \textbf{precision}, as it is portrayed with an example in Figure \ref{fig:robustness_illustration}.

In our experiments, we evaluated 18 language models from various families and sizes, including both pretrained and instruction-tuned models. We observed that, on average, the log-probabilities for statements in the TimeStress dataset peak when the date is correct and gradually decrease as the date moves away from the validity period, suggesting that LMs have a basic understanding of temporality. However, despite some models performing well in identifying correct statements (with accuracy rates exceeding 80\%), they manage to consistently discern between correct and incorrect statements in only 6\% of the time for the best model. In addition, even when models almost perfectly recognize facts, they often fail on ''easy" incorrect dates that are far outside the validity period. Additionally, when a fact is recognized at one level of date precision, this knowledge often does not transfer to other precisions. These findings lead us to conclude that \textbf{LMs currently struggle to achieve a consistent representation of temporal knowledge} and, therefore, are not yet reliable for storing temporal facts.

In the following sections, we first discuss related work (Section \ref{sec:related_work}). Then, we introduce the terminology used throughout the paper (Section \ref{sec:terminology}). Next, we detail the creation process of the TimeStress dataset and highlight its key properties (Section \ref{sec:timestress}). We then describe the experiments we conducted along with an analysis of their results (Section \ref{sec:experimentation}).


\section{Related work}
\label{sec:related_work}

\paragraph{Robustness of Factual Knowledge in LMs.} It has been demonstrated that LMs store a significant amount of factual information \cite{petroni, know_what_language_models_know, head_to_tail}. However, many studies indicate that this acquired knowledge often lacks robustness when faced with text perturbations. For example, \citet{robust_negation} highlighted the limitations of pretrained LMs in adapting to negation in questions, leading to contradictory answers. Robustness to paraphrasing and minor typos has also been extensively studied \cite{cc_paraphrase1, cc_paraphrase2, cc_paraphrase3, cc_paraphrase4, cc_paraphrase5}. Notably, \citet{pararel} and \citet{paraphrase1} found that LMs produce different answers for semantically equivalent factual queries, and \citet{robust_typo} discovered that recent embedding models are negatively impacted by minor typos that preserve the original semantics. Many of these studies were conducted on older generations of LMs. But more recently, \citet{verma-etal-2023-evaluating} demonstrated that the consistency of recent LMs for the task of text entailment is now high for more recent models. However, the robustness of temporality representation in these models remains unexplored.

\paragraph{Temporal Reasoning in LMs.} Many studies investigated the temporal reasoning capabilities of LMs \cite{zhang-choi-2021-situatedqa, chu-etal-2024-timebench,tempreason1_mentaqa,fatemi2024testtimebenchmarkevaluating, templama, xiong-etal-2024-large, su-etal-2024-living}. Notably, the works by \citet{tempreason_timequestions} and \citet{tempreason1_bench_temp_reason} are related to ours. They each proposed datasets where LMs are tasked with relating two timeframes (\eg determining which period precedes the other), a timeframe and a fact (\eg assessing if a fact is valid at a given period), and, in the former study, two facts (\eg determining which fact precedes the other). However, these studies do not measure consistency, as they evaluate LMs' performance by averaging the number of correct answers across many tests with one test per fact. Consistency, in contrast, is achieved when a model systematically passes all tests for a given fact. Also, one advantage of our approach over these studies is its compatibility with non-functional relations. Indeed, because they use generation-based metrics (Exact Match and/or F1-Score) to measure the LMs' answer quality, it is difficult to judge the model when there are many correct answers. Finally, studying how resilient is a temporal representation of facts across several date precisions was not explored.

% \paragraph{Temporality Datasets and Benchmarks.} SituatedQA \cite{zhang-choi-2021-situatedqa} . Also, several evaluation datasets were proposed to detect outdated facts in LMs \cite{outdated1_set_the_clock, outdated2_carpediem, outdated3_signal, outdated4_realtime, outdated5_dyknow}

% \paragraph{Temporal alignment of knowledge in LMs.} Because factual knowledge is comprised of a large portion of temporal facts that are constantly evolving, studies were performed to see how LMs adapt to this shift. As expected, LMs were shown to be unable to predict future facts \cite{mindthegap} which implies the necessity to adapt them to maintain their alignment with current knowledge. To solve this problem, continual learning methods \cite{streaming_qa} as well as special pretraining techniques were proposed, such as jointly modeling text and its associated timestamp which facilitates the acquisition of temporal knowledge \cite{templama}; knowledge editing techniques \cite{rome, grace, melo, zhang-etal-2023-large}; or simply externalizing the knowledge source using retrieval-augmented generation \cite{rag}. In parallel, several evaluation datasets were proposed to detect outdated facts in LMs \cite{outdated1_set_the_clock, outdated2_carpediem, outdated3_signal, outdated4_realtime, outdated5_dyknow}.

\section{Terminology}
\label{sec:terminology}
In knowledge representation, facts are typically represented as relations
between entities \cite{petroni, elsahar-etal-2018-rex}, often using the common RDF format, which expresses facts as triples: (subject, relation, object), or $(s, r, o)$ for short. When dealing with temporal facts, this representation is extended to include the validity period, resulting in a quintuple format, similarly to other works \cite{DBLP:conf/aaai/YinJY024, jain-etal-2020-temporal, tempreason1_bench_temp_reason}. A quintuple $(s, r, o, a, b)$ indicates that the subject $s$ is connected to the object $o$ via the relation $r$ during the time span from date $a$ to date $b$. \textit{Temporal facts} and \textit{quintuples} are used interchangeably in this document.

It is worth noting that
all dates, including $a$ and $b$, are imprecise to some degree and can thus be seen as time intervals \cite{allen_temp_logic}.
Consequently, given a quintuple $(s, r, o, a, b)$, a date can be assigned one of the 3 possible classes: \textbf{Correct} if the date is unambiguously within the validity period $[a,b]$ (\eg 2020 is in [2019 January, 2021]), \textbf{Incorrect} if the date is unambiguously outside of it (\eg 1 January 2019 is outside of [February 2019, 2020]), and \textbf{Transitional} if it overlaps with $a$, $b$, or both (\eg 2021 is transitional in the period [8 August 2021, 3 April 2023]).

Throughout this document, we refer to a \textbf{statement} as a text that begins with a date context, whether it is correct, incorrect, or transitional (e.g., \textit{"In 2020," "On February 1, 2022"}), followed by a question and its answer directly after the question mark (?).
An example would be \textit{"In 2011, who was the president of the USA? Barack Obama"}.


\section{TimeStress Dataset}
\label{sec:timestress}
This section provides a comprehensive description of the TimeStress dataset. A sample is available in Table \ref{tab:verb_sample}. We begin by detailing the construction process, followed by an outline of its key properties at the end of the section.

\subsection{Creation Process}

The creation of TimeStress involves three primary steps: the collection of quintuples from Wikidata, question generation, and date selection. 

\subsubsection{Quintuple Collection}
%First, t
The process begins with the collection of over 2.1 million temporal facts (quintuples) from the preprocessed version of Wikidata in \citet{ammar-khodja-etal-2025-factual}. These quintuples include at least a start date and non-literal objects (no quantities nor dates as objects). To refine the dataset, quintuples are filtered to remove duplicates with different validity periods, simplifying result analysis\footnote{This allows the assumption that all dates outside the validity period are incorrect}. Additionally, quintuples lacking an end date are excluded, as they are assumed to still be valid.
In addition, only quintuples with a validity period exceeding three years are retained to guarantee a minimal number of correct and incorrect \textit{year} dates.

Next, a popularity
score is computed for each quintuple $(s,r,o,a,b)$, based on the monthly human visits on the Wikipedia articles of $s$ and $o$, as in \citet{ammar-khodja-etal-2024-wikifactdiff}.
Quintuples with a similarity score below an arbitrary threshold are then filtered out, in order to retain approximately 2,000 quintuples.

\subsubsection{Verbalization of Quintuples and Quality Assessment}
Verbalization of quintuples into natural language questions is conducted using GPT-4o. The adapted prompt instructs the model to generate four linguistically diverse questions for each tuple, adhering to guidelines such as using past tense and being concise. The main prompt involves filling placeholders with data from Wikidata, and GPT-4o generates four questions and answers for each quintuple. Afterward, one of the four questions is picked randomly, the date is removed from it, and it is verified that the answer corresponds to the quintuple's object.
The result is a generic question that can be used to create statement for different dates, for instance: ``In [DATE], who was the president of the USA? Barack Obama''.

The quality of the generated questions is analyzed to eliminate incorrect entries. Initially, 53 facts failed to be verbalized, and 64 questions incorrectly used the subject as the answer. These were removed, resulting in 2,003 facts and 8,012 questions. A manual evaluation confirmed the high quality of the questions, with only 1 out of 50 samples being incorrect, yielding a Wilson's 95\% Confidence Interval of [0.85, 0.99].

\subsubsection{Date Selection and Statement Generation}
For each quintuple, a generic question is used to generate several statements with various dates on the \textit{year}, \textit{month} and \textit{day} precision levels. The range of considered dates
%For test generation, the range of tested dates
is defined as $m \pm 5l$, where $m$ is the midpoint of the validity period, and $l$ is its length.
%Dates are
For the \text{year} level, the range is
scanned with a step size of $0.05 \times l$ to capture unique years.
Then, dates with the \textit{month} level are chosen by picking a random month for each date of the \textit{year} level, and dates in the \textit{day} level are chosen by picking a random day for each date in the \textit{month} level.
%, and random months and days are selected for finer precision.
This creates a hierarchical relationship among dates of different precisions (e.g., \textit{2020}, \textit{2020-03}, \textit{2020-03-24}), allowing for meaningful comparisons between date precisions. Then, dates are classified as correct, incorrect, or transitional as defined in Section \ref{sec:terminology}. Transitional dates can be absent in \textit{month} and \textit{day} precisions while being present in \textit{year} precision, due to scanning steps skipping them\footnote{This is caused by the interval nature of dates combined with the possibility that the validity period's boundaries are of different precisions.}. This is problematic because it implies that the number of correct and incorrect dates can be different across precisions, unfairly advantaging  precisions with a lower number of dates, particularly on the robustness metric. To address this, \textit{month} and \textit{day} precision dates associated with a transitional \textit{year} dates are removed from the set of correct or incorrect dates. 

\subsection{Key properties}

The resulting dataset, TimeStress, comprises over 521K statements generated from 2,003 temporal facts, averaging 11.27 correct dates and 74.14 incorrect dates per fact. It includes 1,883 unique entities, 1,385 unique subjects, 1,113 unique objects, and a various set of 86 relations.

This dataset is characterized by high-quality statements that are both coherent and linguistically diverse. These statements are free of typos, consistently use past tense, and exclude future dates beyond 2020, thereby preventing nonsensical questions such as "In 2052, who was the president of the USA?". The focus on popular facts is crucial for our study's focus and for evaluating knowledge transfer between date precisions, a task that becomes challenging if language models lack familiarity with the facts\footnote{Knowledge transfer between date precisions is investigated on facts "known'' by LMs which is problematic if they do not know most fact from TimeStress.}. For each fact, all dates outside the validity period are incorrect, each date precision maintains an equal number of correct and incorrect dates, and dates of similar year but different precisions are organized hierarchically, both combined make the comparison across date precisions sound. Furthermore, in the worst-case scenario across all facts in TimeStress, the number of correct and incorrect dates is sufficiently large to make it nearly impossible for a random model to differentiate them perfectly by chance.

A detailed version of this section is provided in Appendix \ref{appendix:timestress}.

\begin{table*}[t!]
    \centering
\resizebox{\linewidth}{!}{
\begin{tabular}{p{8.6cm}llHp{8cm}}
\toprule
\textbf{Temporal fact} & \textbf{Date} & \textbf{Correct?} & \textbf{Statement} & \textbf{Statement} \\
\midrule
(Betty Ford, spouse, Gerald Ford, start=1948-10-15, end=2006-12-26) & 1983-03-21 & Correct & On March 21, 1983, who was the spouse of Betty Ford? & On March 21, 1983, who was the spouse of Betty Ford? Gerald Ford \\
\midrule
(Beirut, country, Ottoman Empire, start=1520, end=1918) & 1759-05 & Correct & In May 1759, to which sovereign state did Beirut belong? & In May 1759, to which sovereign state did Beirut belong? Ottoman Empire \\
\midrule
(Jimmy Butler, member of sports team, Chicago Bulls, start=2011, end=2017-06-22) & 1989-06-17 & Incorrect & On June 17, 1989, which basketball team did Jimmy Butler belong to? & On June 17, 1989, which basketball team did Jimmy Butler belong to? Chicago Bulls \\
\midrule
(Samarkand, country, Soviet Union, start=1922-12-30, end=1991-08-31) & 1789-03-31 & Incorrect & On March 31, 1789, what was the sovereign state of Samarkand? & On March 31, 1789, what was the sovereign state of Samarkand? Soviet Union \\
\midrule
(United States of America, head of government, Andrew Johnson, start=1865-04-15, end=1869-03-04) & 1865 & Transitional & In 1865, who served as the head of government for the United States of America? & In 1865, who served as the head of government for the United States of America? Andrew Johnson \\
\midrule
(Chris Evans, unmarried partner, Minka Kelly, start=2007-05, end=2014-10) & 2014 & Transitional & In 2014, who was Chris Evans romantically involved with? & In 2014, who was Chris Evans romantically involved with? Minka Kelly \\
\bottomrule
\end{tabular}}
    \caption{
    Random sample of two statements per date class (Correct, Incorrect, Transitional). Each row contains the corresponding date and temporal fact.
    }
    \label{tab:verb_sample}
\end{table*}

\section{Experimentation}
\label{sec:experimentation}

This section contains the experimentation
%regarding the evaluation of
for evaluating
the consistency of temporality representation of facts by LMs, using the TimeStress dataset. Temporal consistency in LMs is investigated by varying the statements' temporal context on two dimensions: their position on the timeline and their precision. This part is composed of two subsections. In the first one, we show that LMs have at least a basic
%understanding
representation
of temporality and achieve a high accuracy in differentiating correct and incorrect statements. Then, we show that while accuracy is high, LMs struggle to reach perfect discerning. Furthermore, even when the accuracy is high, LMs fail on easy incorrect dates (far from the validity period). In the second subsection, we show that knowledge transfer between date precisions is lacking in all studied LMs.


Experiments are performed on 18 language models of different families and sizes, including pretrained and instruction-tuned models. They are listed below (each model has an instruction-tuned equivalent)\footnote{These models were collected from \href{https://huggingface.co}{huggingface.co}}:
\begin{itemize}
    \item \textbf{Mistral}: \texttt{Mistral-Nemo-Base-2407}, \texttt{Mistral-7B-v0.3} \cite{mistral}
    \item \textbf{OpenELM}: \texttt{OpenELM-\{450M, 3B\}} \cite{openelm}
    \item \textbf{Gemma2}: \texttt{gemma-2-\{2b, 9b, 27b\}} \cite{gemma2}
    \item \textbf{Llama3.1}: \texttt{Llama-3.1-\{8B, 70B\}} \cite{llama3}
\end{itemize}

\subsection{Evaluation of the Consistency of Language Models}
For a given temporal fact $f=(s,r,o,a,b)$, we consider all its associated statements in TimeStress.
For each of these statement, we look at the probability of generating the answer, given the rest of the statement (until the question mark). We denote this probability by $P(o|f,d)$, since the used statement depends only on the fact $f$ and the considered date $d$, and the answer corresponds to $o$.
We evaluate the model on the basis that it should assign higher probabilities to $o$
when the date is correct compared to when they are incorrect.

For result analysis, we propose two metrics: \textit{win rate} and \textit{robustness}. The \textit{win rate} simply computes the average win rate of correct dates over incorrect dates for a given fact and on a given date precision. On the other hand, the \textit{robustness} metric verifies that correct dates systematically win over incorrect dates. These metrics are defined precisely in what follows.

\subsubsection{Metrics Definitions}
Consider a temporal fact $f=(s,r,o,a,b)$. We define $C_g(f)$ and $I_g(f)$ as the sets of correct and incorrect dates of precision $g\in \{Year, Month, Day\}$ associated to $f$ in TimeStress.
Then, for a correct date $d$ and an incorrect date $d'$, we define a boolean function $\chi$ to determine whether the considered LM favors $d$ over $d'$ as:
\begin{equation}
\chi(f, d, d') = \mathbbm{1}[P(o | f, d) > P(o | f, d')]
\end{equation}
%
% \begin{equation}
% \chi(f, d, d') = P(ans \mid q, d) > P(ans \mid q, d')
% \end{equation}
%
\noindent Then, the \textit{win rate} metric is defined as:
\begin{equation}
    \mathcal{W}_g(f) = \frac{1}{N} \sum_{(d, d')\ \in\ C_g(f) \times I_g(f)} \chi(f,d,d')
\end{equation}

\noindent where $N = |C_g(f)||I_g(f)|$.
%
In addition, we define the \textit{global win rate} noted $\mathcal{W}_G$ which is the win rate computed across all precisions and is equal to $\mathcal{W}_G(f) = \frac{1}{3}[ \mathcal{W}_{Day}(f) + \mathcal{W}_{Month}(f) + \mathcal{W}_{Year}(f)]$.
%
Finally, the robustness metric is defined as follows:
\begin{equation}
\mathcal{R}_g(f) = 
\begin{cases} 
1, & \text{if } \mathcal{W}_g(f) = 1 \\
0, & \text{otherwise}
\end{cases}
\end{equation}
%
Similarly to win rate, we define the \textit{global robustness} $\mathcal{R}_G(f)$ which is equal to 1 if $\mathcal{W}_G(f) = 1$ else it is equal to 0. 

Lastly, the relative position of a date $d$ to the validity period $[a,b]$ is noted $\alpha$, which is defined as $\alpha = \frac{d-m}{b-a}$ where $m$ is the middle of the validity period $m=\frac{a+b}{2}$. When $|\alpha| < \frac{1}{2}$, it means that the date is within the validity period, if $|\alpha| > \frac{1}{2}$, it means it is outside of it, and when $|\alpha| = \frac{1}{2}$, it means that the date is at the frontier of the validity period, i.e. $d$ is a transitional date.

\subsubsection{Main Results}
\paragraph{Do LMs have a basic understanding of temporality?} Figure \ref{fig:alpha_vs_logprob} illustrates the average $log \ P(o | f, d)$ with respect to the date's relative position to the validity period of facts. The average probability is computed across all facts in the TimeStress dataset and across all studied language models. The results indicate that \textbf{language models have
%an understanding
some representation
of the temporality of facts}, because the probability peaks during the validity period and shows a symmetrical trend around its midpoint. This is further supported by the clear negative correlation between the distance to the validity period and the log-probability values. The log-probabilities values clearly align with the validity periods of temporal facts, which indicates that language models are converging towards a truthful representation of facts. We note that the probability assigned to transitional dates (neither correct nor incorrect) is significantly higher than for other dates, including correct ones. We hypothesize that this is because transitional dates are likely more prevalent in the training dataset, as events like the start or end of a term are more frequently mentioned than during the term itself. Additionally, the start transitional date tends to have a higher probability than the end transitional date, suggesting that start dates are more prevalent in the training data.

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{plots/alpha_vs_logprob_bar_classic.pdf}
    \caption{The evolution of $log\ P(o \mid f,d)$ with respect to the relative distance of the date to the validity period $\alpha$ averaged across all facts in TimeStress, on the date precision \textit{Year}, averaged across all LMs.}
    \label{fig:alpha_vs_logprob}
\end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[width=1\linewidth]{plots/lenient_classic.pdf}
    \caption{The average $\mathcal{W}(f)$ over all the facts $f$ in TimeStress for each LM. The error bars represent 95\% confidence intervals.}
    \label{fig:win_rate}
\end{figure}

\paragraph{Robustness Evaluation.} Figure \ref{fig:win_rate} presents the average win rate for the TimeStress dataset facts across all studied language models (LMs) and three time precisions: Day, Month, and Year, in addition to the global win rate metric noted \textit{Global}. The results indicate that larger LMs achieve higher win rates, with even smaller models performing above random chance. The win rate consistently improves with model size, with Llama-3.1-70B-Instruct surpassing an 80\% win rate. 
However, the average robustness across all facts shows that this win rate rarely reaches a perfect score of 1 (robust knowledge). Notably, the best model, gemma-2-27b-it, achieves only about a 10\% robustness rate when considering \textit{year} precision, and this drops to 6\% when evaluating all date precisions using the global robustness score $\mathcal{R}_G$. Most other models do not exceed a 2\% global robustness score. Instruction-tuned models perform better than their pretrained counterpart most of the time; the most significant gap between the two is with the Llama-3.1-70B-Instruct, which is $2.7\times$ more robust than Llama-3.1-70B, suggesting that the training data and possibly training procedure significantly impact temporal robustness.
Additionally, early signs of failing knowledge transfer between date precisions are evident due to the substantial gap between individual precision robustness scores and the global score. This issue is explored in greater detail in Section~\ref{sec:granularity_generalization}.

\paragraph{Relative Position of Incorrect Dates.} Analyzing the relative positions of incorrect dates that were favored over correct dates, on the 5 most robust LMs (cf. Figure \ref{fig:robust}), when the win rate is very high ($\geq$ 95\%), reveals that many of these incorrect dates are not near the validity period as expected. Instead, they often occur far from it (Figure \ref{fig:pos_incorrect_dates}). Specifically, language models fail to achieve robustness due to dates with a distance of $|\alpha| \geq 1$ in 36\% of cases. This proportion decreases to 19\% for $|\alpha| \geq 2$ and 12\% for $|\alpha| \geq 3$. The last value is still large, given the proximity of the win rate to 100\% and the large distance of these dates from the validity period, indicating that language models are vulnerable to easy incorrect dates, and make mistakes that humans would probably not make when they almost perfectly know the target fact.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.9\linewidth]{plots/incorrect_date_position_noninstruct_classic.pdf}
    \caption{Relative positions of incorrect dates favored over correct dates, when the win rate exceeds 95\%, on the 5 most robust models. Inputs to LMs are not formatted as instructions.}
    \label{fig:pos_incorrect_dates}
\end{figure}

\paragraph{Instruction Questions.} The performance of instruction-tuned models might be underestimated because they are fine-tuned on instructions, yet evaluated using raw text. To address this, win rate and robustness scores were recalculated using instructions by applying each instruction-tuned LM's chat template\footnote{The chat template is applied as follows: \texttt{``USER: [DATE+QUESTION] ASSISTANT:[ANSWER]''}. There is no special token at the end.}. Figure \ref{fig:inst_vs_noninst_inst_vs_noninst_query_robust} compares robustness scores computed using raw text versus instructions. While instructions can enhance performance for some LMs (e.g., Llama-3.1-70B-Instruct), they generally reduce it (e.g., gemma-2-27b-it), and overall robustness scores remain very low. Similarly, the win rate significantly decreases with the introduction of instructions except for Llama-3.1-70B. The relative positions of incorrect dates for high win rate facts are similar to non-instruction settings and LMs still fail on easy incorrect dates. Therefore, the missing instruction format does not explain the poor robustness of LMs.

\begin{figure}[t]
    \centering
    \includegraphics[width=1\linewidth]{plots/robust_classic.pdf}
    \caption{The average $\mathcal{R}$ over all the facts in TimeStress for each LM. The error bars represent 95\% CIs.}
    \label{fig:robust}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.98\linewidth]{plots/inst_vs_noninst_inst_vs_noninst_query_robust_global_classic.pdf}
    \caption{Average $\mathcal{R}_G$ across all facts in TimeStress when the query is in the instruction format (orange) compared to when it is free-form (blue). The error bars represent 95\% confidence intervals.}
    \label{fig:inst_vs_noninst_inst_vs_noninst_query_robust}
\end{figure}

\subsection{Knowledge Transfer between Date Precisions}
\label{sec:granularity_generalization}
This section examines the ability of language models to transfer knowledge of a fact across different levels of temporal precision. TimeStress allows for comparisons between two date precisions because the three studied precisions have the same number of correct and incorrect dates across all temporal facts. The only difference between two precisions is the addition of a random month or day, which does not affect validity going from a lower to a higher precision. If a date is incorrect for the entire year, it remains incorrect for all dates within that year; the same logic applies to correct dates.

\paragraph{Main Experiment.} We retain all facts known at one level of date precision on non-instruction questions for the 5 most robust LMs (cf. Figure \ref{fig:robust}). We then calculate the proportion of facts known at precision A, given that they are known at precision B. The results are presented in Figure \ref{fig:gran_gen}.

The raw text Figure \ref{fig:gran_gen_raw} shows that, on average, language models were not able to generalize their knowledge to other precisions 38\% of the time, which is surprisingly high given their perfect win rate score on the start precision. It shows that there is still improvements to be made. The performance between LMs vary. For instance, for the most robust model gemma-2-27b-it, the Year to Month transition is successful in 70±6\% of cases (A=Month, B=Year) and other transitions' success rates vary between 63±7\% and 80±6\%. For instance, The LM with the highest win rate Llama-3.1-70B-Instruct performs worse and its success rates vary from 39±8\% to 65±10\%. The rest of LMs also do not exhibit perfect transition for any pair of precisions. Instruction format (Figure \ref{fig:gran_gen_inst}) improves generalization especially from lower to higher date precisions (\eg A=Day, B=Year). In fact, the average success rates increases from 62\% to 68\% when using instruction format. Generalization of individual LMs can be found in Appendix~\ref{appendix:generalization_matrices}.

\begin{figure}[t]
    \centering
    \begin{subfigure}[h]{0.6\linewidth}
        \centering
        \includegraphics[width=\linewidth]{plots/gran_vs_smallcomb_noninst_alllms_classic.pdf}
        \caption{Raw text}
        \label{fig:gran_gen_raw}
    \end{subfigure}
    \begin{subfigure}[h]{0.6\linewidth}
        \centering
        \includegraphics[width=\linewidth]{plots/gran_vs_smallcomb_inst_alllms_classic.pdf}
        \caption{Instruction format}
        \label{fig:gran_gen_inst}
    \end{subfigure}
    \caption{Average success rate of knowledge transfer between pairs of date precisions over the 5 most robust LMs (with 95\% confidence intervals).}
    \label{fig:gran_gen}
\end{figure}

\paragraph{Explanation Prompts.}
For exploration purposes, we investigate if the inclusion of explanations of temporal concepts in the prompt could help the LMs to better generalize knowledge between date precisions. To assess this, two prompts that were prefixed to each statements in TimeStress were used. The first one explains the hierarchical nature of dates (\ie a year is composed of months and a month is composed of days), while the second one is more direct and explains how knowledge about a temporal fact can be generalized based on the hierarchical nature of dates (\ie If a fact is correct on a whole year then it is correct on all dates within this year). We recomputed the generalization between precision pairs on the same 5 LMs used to compute the previous generalization matrices. The two explanation prompts improved raw format generalization from 62\% to 68\%. However, no substantial gain over not using explanation prompt was noticed when using straightforward instruction format.

\subsection{Findings Summary}
The combination of (1) LMs converging towards a representation of temporal facts that accurately reflects their real validity periods, as supported by the results in Figures \ref{fig:alpha_vs_logprob} and \ref{fig:win_rate}, (2) the inability of LMs to achieve a perfect win rate (Figure \ref{fig:robust}), (3) LMs fail to achieve robustness due to easy incorrect dates distracting them (Figure \ref{fig:pos_incorrect_dates}), and (4) facts perfectly known on one date precision do not necessarily transfer to other precisions (Figure \ref{fig:gran_gen}), highlights that \textbf{LMs struggle to achieve a consistent temporal representation of facts}. It is worth mentioning that since the studied temporal facts are popular, the performances computed in this section are likely to be an upper bound of the performance of LMs on the general population of facts, knowing the strong relation between knowledge popularity and the probability that it is learned by LMs \cite{DBLP:conf/icml/KandpalDRWR23,kang-choi-2023-impact}.

\section{Conclusion}

This study evaluated the ability of LMs to consistently represent temporal facts using the TimeStress dataset, which includes over 521K statements about 2,003 popular temporal facts. The findings indicate that while LMs demonstrate a basic understanding of temporality by aligning their probability distributions with the validity periods of facts, they struggle with consistency. Despite achieving high win rates, often exceeding 80\%, their robustness remains low, with the best model achieving only a 6\% global robustness score. This highlights a significant gap between average and consistent performance. Additionally, LMs frequently fail on "easy" incorrect dates that are far from the validity period. Furthermore, LMs struggle to transfer knowledge across different date precisions, suggesting a lack of coherent temporal representation. In conclusion, while LMs can store and recall factual information, they struggle to achieve a consistent temporal representation of facts. Future work should focus on enhancing temporal reasoning capabilities to make LMs more reliable for temporal knowledge.


\section{Limitations}
The study evaluates language models (LMs) using a probability-based approach to assess their understanding of temporal facts. Although this method may not fully capture the models' performance in text generation scenarios, it allows for precise probing of specific non-functional relationships where multiple correct answers exist. This is more challenging with generation-based metrics, as LMs can produce unexpected or off-topic responses. Moreover, previous research has shown that probability-based metrics correlate reasonably well with actual model performance in knowledge assessment contexts, where the model is expected to generate specific entities \cite{karr, lyu-etal-2024-beyond}, which is closely related with our approach.

The TimeStress dataset consists of English statements, which might limit the applicability of our findings to other languages due to eventual linguistic differences that can affect temporal understanding. However, future research can easily broaden the scope by adapting the GPT-4o prompt used for verbalizing facts to target additional languages, thus enhancing the study's cross-linguistic validity.

\bibliography{custom}

\appendix

\section{TimeStress: Data collection details}
\label{appendix:timestress}
This section provides a detailed description of the construction of the TimeStress dataset. Before discussing the collection process, we outline the key properties of TimeStress. Firstly, the dataset focuses on past facts valid strictly before 2021, ensuring they are past events for all language models (LMs) under study. TimeStress includes high-quality verbalizations that are coherent with the facts and exhibit linguistic diversity to avoid bias from limited question variety. Verbalizations are carefully curated to minimize typos, use past tense verbs, and exclude future dates beyond 2020 to prevent nonsensical questions, such as "In 2052, who was the president of the USA?" The dataset encompasses a diverse set of 86 relations to mitigate bias from a narrow range. The targeted facts are popular, which is crucial for our study's focus and essential for evaluating knowledge generalization across different date precisions, a task that becomes challenging if LMs lack familiarity with the facts. For a given fact, all dates outside the validity period are considered incorrect. Also, to ensure fairness, each date precision has an equal number of correct and incorrect dates across all facts. Finally, the number of correct and incorrect dates is large enough to make it almost impossible for a random model to know robustly a fact by chance. 

The creation process of the TimeStress dataset was meticulously designed to fulfill the previously outlined properties, thereby effectively supporting the claims of this paper. This process involves three primary steps. First, an initial collection of 2105 popular temporal facts is sourced from Wikidata for inclusion in TimeStress. Second, questions are generated from these quintuples using GPT-4o, accompanied by a quality assessment to ensure the questions are of high quality and filter out low quality questions. Finally, for each fact, the correct and incorrect dates are identified and integrated into the questions to produce complete verbalizations.

\subsection{Quintuple collection process}
The quintuple collection process begins with the collection of the preprocessed version of Wikidata provided by \cite{ammar-khodja-etal-2025-factual}. This source also provides a popularity measure for entities based on the number of human visits to Wikipedia articles over the past 12 months. This measure is used to define the popularity of a quintuple, calculated as the geometric mean of the popularity of its object and subject. Appendix \ref{appendix:pop_vs_robust} provides a straightforward demonstration of the effectiveness of this popularity measure, illustrating that the likelihood of language models (LMs) knowing a fact increases with its popularity. This finding aligns with previous research \cite{DBLP:conf/icml/KandpalDRWR23, kang-choi-2023-impact}. 

Initially, all quintuples with at least a start date and whose objects are not literals, such as quantities and dates, are collected, totaling over 2.1 million quintuples. Quintuples are then filtered to remove any $(s,r,o,a,b)$ where another quintuple $(s,r,o,a',b')$ exists with a different validity period $[a',b']$, allowing for the assumption that all dates outside $[a',b']$ are incorrect, which simplifies result analysis. This step eliminates a negligible amount of quintuples (6.23\%). Additionally, quintuples with no end date, which are assumed to still be valid, are removed. Only those quintuples with a popularity measure of at least 90,000\footnote{This amount was found by gradually decreasing the threshold starting from 150,000 by step of 10,000 until the number of facts retrieved exceeded 2,000.} and a validity period exceeding three years are retained. The final result is a dataset comprising the 2,098 most popular facts in Wikidata, featuring 1,910 unique entities, 1,435 unique subjects, 1,151 unique objects, and 86 relations, making it a well-diversified set of temporal facts.

\subsection{Quintuple verbalization}

The process of verbalizing quintuples into natural language questions is conducted using GPT-4o. The verbalization prompt, adapted from \cite{ammar-khodja-etal-2024-wikifactdiff} (Appendix B), was modified to generate questions rather than declarative sentences. The adapted system prompt instructs GPT-4o to take a tuple (subject, relation, object, time) and generate four linguistically diverse questions. For example, given the input (British India, capital, Kolkata, 1929), a possible question could be: "In 1929, what was the capital of British India? Kolkata". The questions must adhere to specific guidelines: they should be in the past tense, start with the year followed by a comma, and conclude with the answer. The focus of the questions should be on the object, and they should be straightforward and concise, avoiding any details that might simplify the answer. The adapted system prompt is as follows:

\scriptsize
\begin{tcolorbox}[width=\columnwidth,colback=white]
You are an advanced knowledge verbalization system.\\
You take as input a knowledge quadruple (subject, relation, object, time) and generate a list of 4 linguistically diverse questions on the quadruple.\\
For example, the input could be : (British India, capital, Kolkata, 1929) and one of your questions may be : "In 1929, what was the capital of British India? Kolkata.".\\

All the questions you generate must be in past tense because the facts are not valid anymore.\\
The questions must always start with the year, then a comma, then the question itself, and then finally the answer.\\
The questions must always be asked on the object.\\
The questions must be straightforward and concise.\\
The questions must not contain details that could make them easier to answer.\\

Examples of questions:\\
- (Jimmy Butler, member of sports team, Chicago Bulls, 2014) --> "In 2014, which team did Jimmy Butler play for? Chicago Bulls."\\
- (Philippines, head of state, Emilio Aguinaldo, 1900) --> "In 1900, who was the head of state of Philippines? Emilio Aguinaldo."\\
- (Coretta Scott King, spouse, Martin Luther King Jr., 1960) --> "In 1960, who was Coretta Scott King married to? Martin Luther King Jr."\\
- (European Union, currency, pound sterling, 2002) --> "In 2002, what was one of the currencies of the European Union? Pound sterling."\\
\end{tcolorbox}
\normalsize


And here is the main prompt:

\scriptsize
\begin{tcolorbox}[width=\columnwidth,colback=white]
Here is the knowledge quadruple to verbalize: ([SUBJECT], [RELATION], [OBJECT], [YEAR]).\\

Due to the ambiguity that could arise from the provided labels, here is their meaning:\\
- (subject) "[SUBJECT]" : "[SUBJECT\_DESC]"\\
- (relation) "[RELATION]" : "[RELATION\_DESC]"\\
- (object) "[OBJECT]" : "[OBJECT\_DESC]"\\

Finally, here is an example where the relation "[RELATION]" is employed : ([EXAMPLE\_SUBJECT], [RELATION], [EXAMPLE\_OBJECT]).\\
\end{tcolorbox}
\normalsize

The main prompt involves filling placeholders [SUBJECT], [RELATION], [OBJECT], [SUBJECT\_DESC], [RELATION\_DESC], and [OBJECT\_DESC] with corresponding labels and descriptions from Wikidata. An example of the relation is also retrieved from Wikidata using the \textit{Wikidata property example (P1855)} relation. If no example is available, the final line of the prompt is omitted. The [YEAR] is selected as the midpoint of the validity period of the quintuple. GPT-4o then generates four questions and answers for each quintuple. Afterward, the date is removed from the question, and it is verified that the answer corresponds to the object.

\subsection{Quality of the Generated Questions}

The quality of the verbalizations was analyzed to identify and eliminate incorrect entries. Initially, out of the 2,098 facts intended for verbalization, 53 failed to be verbalized, and 64 questions incorrectly used the subject as the answer instead of the object. These erroneous cases were removed from the dataset, resulting in a total of 2,003 facts and $2003 \times 4 = 8012$ questions. To further ensure quality, a manual evaluation was conducted on a random sample of 50 generated questions.

The evaluation revealed that only 1 out of the 50 verbalizations was incorrect, while the remaining questions were accurately constructed (Wilson's 95\% Confidence Interval = [0.85, 0.99])\footnote{This confidence interval was calculated with finite-population correction.}. These results demonstrate the high quality of the questions in our dataset.

Finally, each fact was randomly assigned one of its four associated questions.

\subsection{Test Generation}
Arithmetic between dates is involved in this section. It is worth noting that all operations between dates are performed on the midpoint of the date (because dates are intervals). For example when $a + b$ is computed, the result is the midpoint of $a$ added to the midpoint of $b$. The highest precision a midpoint can be is the \textit{day} precision. This is done get our way around the interval nature of dates.

For each quintuple, the range of tested dates is defined as $m \pm 5l$, where $m$ is the midpoint of the validity period $(a+b)/2$, and $l$ is the length of the validity period $b-a$. To determine the dates with year precision to be included in TimeStress, we perform a scan starting from the midpoint and extending to the extremities with a step size of $0.05 \times l$. This step size is chosen to limit the maximum number of correct and incorrect dates to the reasonable values 21 and 180, respectively. For month precision, a random month is selected from the year, and for day precision, a random day is chosen from the month. This creates a hierarchical relationship among dates of different precisions (\eg \textit{2020}, \textit{2020-03}, \textit{2020-03-24}), allowing for meaningful comparisons in terms of win rate and robustness metrics, as they share the same year and month where applicable. All dates are categorized as correct, incorrect, or transitional.

Despite this setup, a fact might have varying numbers of correct and incorrect dates across precisions due to transitional dates, which may be absent in higher precisions if the scan skips them. This can happen because of minor perturbations caused by bissextile years. This discrepancy could bias the performance, particularly favoring year precision in robustness metrics, which are calculated on fewer tests. To address this, month and day precision dates associated with transitional year precision dates are removed from the correct and incorrect sets and assigned to a special class called \textbf{Discarded}.

Finally, the dates are converted into text and prefixed to the questions to create verbalizations for each date at each precision level for every fact.

The resulting dataset, named \textbf{TimeStress}, comprises 521K verbalizations generated from 2,003 temporal facts. On average, it includes 11.27 correct dates and 74.14 incorrect dates, featuring 1,883 unique entities, 1,385 unique subjects, 1,113 unique objects, and 86 relations. A random sample of TimeStress is presented in Table \ref{tab:timestress_sample}.

\begin{table*}
\resizebox{\linewidth}{!}{

\begin{tabular}{p{10cm}p{8cm}l}
\toprule
\textbf{Quintuple} & \textbf{Statement} & \textbf{Is correct?} \\
\midrule
(Alexander Graham Bell, country of citizenship, United States of America, start=1882, end=1922) & In July 1734, what was Alexander Graham Bell's country of citizenship? United States of America & Incorrect \\
\midrule
(Lauren Bacall, spouse, Jason Robards, start=1961-07-04, end=1969-09-10) & In July 1984, who was the spouse of Lauren Bacall? Jason Robards & Incorrect \\
\midrule
(Vatican City, head of state, John Paul II, start=1978-10-16, end=2005-04-02) & In July 2006, who held the highest authority in Vatican City? John Paul II & Incorrect \\
\midrule
(Gareth Barry, member of sports team, Manchester City F.C., start=2009, end=2014) & In July 2020, which football team included Gareth Barry as a player? Manchester City F.C. & Incorrect \\
\midrule
(Pierce Brosnan, spouse, Cassandra Harris, start=1980, end=1991) & In 1954, who did Pierce Brosnan have as his wife? Cassandra Harris & Incorrect \\
\midrule
(Metallica, has part, Jason Newsted, start=1987, end=2001-01-17) & In 1971, who was included in Metallica's lineup? Jason Newsted & Incorrect \\
\midrule
(Eliza Dushku, unmarried partner, Rick Fox, start=2009, end=2014) & In 2003, who was Eliza Dushku in a relationship with? Rick Fox & Incorrect \\
\midrule
(United Kingdom, head of state, George VI, start=1936-12-11, end=1952-02-06) & On July 1, 1892, who served as the king of the United Kingdom? George VI & Incorrect \\
\midrule
(Linda Lee Cadwell, spouse, Bruce Lee, start=1964, end=1973-07-20) & In 1929, who was the spouse of Linda Lee Cadwell? Bruce Lee & Incorrect \\
\midrule
(George Harrison, part of, The Beatles, start=1960, end=1970) & On July 2, 1971, what was the name of the band that George Harrison was associated with? The Beatles & Incorrect \\
\midrule
(Philippines, head of state, Corazon Aquino, start=1986-02-25, end=1992-06-30) & On July 2, 1969, who served as the leader of the Philippines? Corazon Aquino & Incorrect \\
\midrule
(Jawaharlal Nehru, position held, Prime minister of India, start=1947-08-15, end=1964-05-27) & In 1985, what position did Jawaharlal Nehru hold? Prime Minister of India & Incorrect \\
\midrule
(Vienna, country, Austria-Hungary, start=1867-03-30, end=1918-11-11) & In July 1769, which country did Vienna belong to? Austria-Hungary & Incorrect \\
\midrule
(Mileva Marić, spouse, Albert Einstein, start=1903, end=1919) & In July 1907, who was Mileva Marić married to? Albert Einstein & Correct \\
\midrule
(Mayte Garcia, spouse, Prince, start=1996, end=2000) & In July 1979, who was the spouse of Mayte Garcia? Prince & Incorrect \\
\midrule
(Abkhazia, country, Soviet Union, start=1921, end=1991) & In July 1956, which country did Abkhazia belong to? Soviet Union & Correct \\
\midrule
(Georgia, member of, Commonwealth of Independent States, start=1993-12-03, end=2009-08-18) & In 1930, what group included Georgia as a member? Commonwealth of Independent States & Incorrect \\
\midrule
(Abraham Lincoln, member of political party, Whig Party, start=1834, end=1854) & In 1808, which political party was Abraham Lincoln a member of? Whig Party & Incorrect \\
\midrule
(Wales, located in the administrative territorial entity, Kingdom of England, start=1284, end=1707-04-30) & On July 1, 1072, which territorial entity included Wales? Kingdom of England & Incorrect \\
\midrule
(Frédéric Chopin, residence, Paris, start=1831, end=1849) & On July 2, 1847, which city was home to Frédéric Chopin? Paris & Correct \\
\bottomrule
\end{tabular}

}
\caption{Random sample of TimeStress.}\label{tab:timestress_sample}
\end{table*}

\section{Computation of Conditional Probability In Language Models}
Since we compute conditional probabilities extensively in our experiments, it is important that these computations are rigorously implemented.

Because different tokenizers split a text differently, we need a universal algorithm to compute as best as possible the probability of generating a text given a prompt, even with the possibility that the end of the prompt is the middle of a token. 

\noindent Here is the high-level steps we used to compute $P(A \mid B)$ where $A$ and $B$ are strings of characters:
\begin{enumerate}
    \item Tokenize $A+B$ into a sequence $s=(t_1, t_2, \dots, t_n)$\footnote{$+$ means string concatenation.}.
    \item Find the smallest sequence of tokens $(t_k, \dots, t_n)$ in $s$ that contains $B$ starting from the end
    \item Compute $P(t_k, \dots, t_n \mid t_1, \dots, t_{k-1})$ which can be done directly through the logits outputed by the LM.
\end{enumerate}

Other aspects need to be taken into account such as the automatic addition of special tokens by the tokenizer. A detailed implementation of this method (the function \texttt{LanguageModel.credibility\_text}) that takes care of these details is available in the source code.

\section{Relation between model size and robustness}
The relation between the model size and the global robustness metric $\mathcal{R}_G$ (Figure \ref{fig:numparams_vs_robust}) shows as expected that the larger the model, the more robust it is. A noticeable improvement is noted when a model is instruction-tuned which is probably due to the fact that a large portion of is the most efficient with respect to the ''number of parameters to robustness" ratio.

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{plots/numparams_vs_robust_classic.pdf}
    \caption{Relation between the number of parameters in a language model and its robustness. Pretrained models are represented as straight lines, while instruction-tuned models are represented with dotted lines.}
    \label{fig:numparams_vs_robust}
\end{figure}

\section{Relation between Fact Popularity and Robustness}
\label{appendix:pop_vs_robust}
Figure \ref{fig:pop_vs_robust} illustrates the relationship between the popularity of a fact and the robustness metric, averaged across all studied language models (see the list in Section \ref{sec:experimentation}). Popularity is defined as the median number of human visits to the associated Wikipedia article for an entity over the year 2020. The popularity of a fact is calculated as the geometric mean of the popularity of its subject and object. Our findings indicate that more popular facts are more likely to be robustly known by language models. This is expected, as popularity is likely correlated with the frequency of a fact's appearance in the dataset, which biases the model towards memorizing these facts during training. This result supports the validity of using this popularity measure, as the correlation between popularity and knowledge has been observed in previous studies \cite{kang-choi-2023-impact, DBLP:conf/icml/KandpalDRWR23}.

\begin{figure}[t!]
    \centering
    % \includegraphics[width=0.30\linewidth]{figs/pop_sub_vs_robust.pdf}
    % \includegraphics[width=0.30\linewidth]{figs/pop_obj_vs_robust.pdf}
    \includegraphics[width=\linewidth]{plots/pop_fact_vs_robust_classic.pdf}
    \caption{Relation of fact popularity to the robustness metric.}
    \label{fig:pop_vs_robust}
\end{figure}

\section{Pretrained vs. Intruction-tuned Language Models}
\label{appendix:inst_vs_noninst}

Pretrained models and their respective instruction-tuned version are compared using matches between their win rate $\mathcal{W}$ score on each fact (Figure \ref{fig:inst_vs_noninst}). Instruction-tuned models are clearly more knowledgeable than pretrained models on questions using win rate as a metric. As of robustness, the vast majority of matches resulted in ties.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.5\textwidth]{plots/inst_vs_noninst_lenient_classic.pdf}
    \caption{Comparison of the performance of pretrained LMs and their respective instruction-tuned version. The global win rate score $\mathcal{W}_G$ of the pretrained and instruction-tuned model are computed on each fact and determine from that the proportion of cases where the instruction-tuned model's score is higher (green) or the pretrained model's score is higher (blue), and the proportion of ties (orange).}
    \label{fig:inst_vs_noninst}
\end{figure}

\section{Knowledge Generalization between Temporal Precisions}

\subsection{Success Rate Matrices}
\label{appendix:generalization_matrices}
In Section \ref{sec:granularity_generalization}, we explored the capacity of language models to generalize their temporal knowledge from one temporal precision to another. We provided two matrices (one for questions in instruction format and one for questions in raw format) that contain the generalization rate between each pair of temporal precisions averaged over the 5 most robust LMs. Here, the generalization rate matrix of individual models are displayed in Figure \ref{fig:gran_gen_all}.

\begin{figure*}
    \centering
    \begin{subfigure}[h]{0.195\linewidth}
        \centering
        \includegraphics[width=\linewidth]{plots/gran_vs_smallcomb_noninst_google_gemma-2-27b-it_classic.pdf}
    \end{subfigure}
    \begin{subfigure}[h]{0.195\linewidth}
        \centering
        \includegraphics[width=\linewidth]{plots/gran_vs_smallcomb_noninst_google_gemma-2-9b-it_classic.pdf}
    \end{subfigure}
    \begin{subfigure}[h]{0.195\linewidth}
        \centering
        \includegraphics[width=\linewidth]{plots/gran_vs_smallcomb_noninst_meta-llama_Llama-3.1-70B-Instruct_classic.pdf}
    \end{subfigure}
    \begin{subfigure}[h]{0.195\linewidth}
        \centering
        \includegraphics[width=\linewidth]{plots/gran_vs_smallcomb_noninst_mistralai_Mistral-Nemo-Instruct-2407_classic.pdf}
    \end{subfigure}
    \begin{subfigure}[h]{0.195\linewidth}
        \centering
        \includegraphics[width=\linewidth]{plots/gran_vs_smallcomb_noninst_mistralai_Mistral-7B-Instruct-v0.3_classic.pdf}
    \end{subfigure}
    
    \begin{subfigure}[h]{0.195\linewidth}
        \centering
        \includegraphics[width=\linewidth]{plots/gran_vs_smallcomb_inst_google_gemma-2-27b-it_classic.pdf}
        \caption{gemma-2-27b-it}
    \end{subfigure}
    \begin{subfigure}[h]{0.195\linewidth}
        \centering
        \includegraphics[width=\linewidth]{plots/gran_vs_smallcomb_noninst_google_gemma-2-9b-it_classic.pdf}
        \caption{gemma-2-9b-it}
    \end{subfigure}
    \begin{subfigure}[h]{0.195\linewidth}
        \centering
        \includegraphics[width=\linewidth]{plots/gran_vs_smallcomb_inst_meta-llama_Llama-3.1-70B-Instruct_classic.pdf}
        \caption{Llama-3.1-70B-Instruct}
    \end{subfigure}
    \begin{subfigure}[h]{0.195\linewidth}
        \centering
        \includegraphics[width=\linewidth]{plots/gran_vs_smallcomb_inst_mistralai_Mistral-Nemo-Instruct-2407_classic.pdf}
        \caption{Mistral-Nemo-Instruct-2407}
    \end{subfigure}
    \begin{subfigure}[h]{0.195\linewidth}
        \centering
        \includegraphics[width=\linewidth]{plots/gran_vs_smallcomb_inst_mistralai_Mistral-7B-Instruct-v0.3_classic.pdf}
        \caption{Mistral-7B-Instruct-v0.3}
    \end{subfigure}
    \caption{Generalization matrices between date precision pairs of the 5 most robust LMs. In the first row, questions are presented to LMs in a raw format, and in the second, they are presented in an instruction format.}
    \label{fig:gran_gen_all}
\end{figure*}

\subsection{Explanation Prompts}
\label{appendix:explanation_prompts}
We investigated in Section \ref{sec:granularity_generalization} if the inclusion of explanations of temporal concepts in the prompt could help the LMs to better generalize knowledge between date precisions. Two prompts that were prefixed to each statements in TimeStress were used:

\noindent\textbf{Prompt 1:}
\scriptsize
\begin{tcolorbox}[width=\columnwidth,colback=white]
A date is a specific point in time, expressed through a year, a month, and a day. A year is divided into months, and a month is divided into days. Answer the following question.
\end{tcolorbox}
\normalsize

\noindent\textbf{Prompt 2:}
\scriptsize
\begin{tcolorbox}[width=\columnwidth,colback=white]
A date is a specific point in time. If a fact is valid for a specific year, it holds true for all dates within that year. If a fact is valid for a specific month of a specific year, it holds true for all dates within that month. Answer the following question.
\end{tcolorbox}
\normalsize

The first one explains the hierarchical nature of dates while the second one is more direct and explains how knowledge about a temporal fact can be generalized based on the hierarchical nature of dates.

Figure \ref{fig:gran_gen_explanation_prompts} shows the average generalization matrices across the same 5 models as in Figure \ref{fig:gran_gen}, using raw text and instruction format.

\begin{figure*}
    \begin{subfigure}[h]{0.245\linewidth}
        \centering
        \includegraphics[width=\linewidth]{plots/gran_vs_smallcomb_noninst_alllms_explain_date.pdf}
        \caption{Prompt 1, raw text}
    \end{subfigure}
    \begin{subfigure}[h]{0.245\linewidth}
        \centering
        \includegraphics[width=\linewidth]{plots/gran_vs_smallcomb_inst_alllms_explain_date.pdf}
        \caption{Prompt 1, instruction format}
    \end{subfigure}
    \begin{subfigure}[h]{0.245\linewidth}
        \centering
        \includegraphics[width=\linewidth]{plots/gran_vs_smallcomb_noninst_alllms_explain_granularity_generalization.pdf}
        \caption{Prompt 2, raw text}
    \end{subfigure}
    \begin{subfigure}[h]{0.245\linewidth}
        \centering
        \includegraphics[width=\linewidth]{plots/gran_vs_smallcomb_inst_alllms_explain_granularity_generalization.pdf}
        \caption{Prompt 2, instruction format}
    \end{subfigure}
    \centering
    \begin{subfigure}[h]{0.245\linewidth}
        \centering
        \includegraphics[width=\linewidth]{plots/gran_vs_smallcomb_noninst_alllms_classic.pdf}
        \caption{No prompt, raw text}
    \end{subfigure}
    \begin{subfigure}[h]{0.245\linewidth}
        \centering
        \includegraphics[width=\linewidth]{plots/gran_vs_smallcomb_inst_alllms_classic.pdf}
        \caption{No prompt, instruction format}
    \end{subfigure}
    \caption{Effect of adding explanation of temporal concepts through prompting (cf. Appendix \ref{appendix:explanation_prompts})}
    \label{fig:gran_gen_explanation_prompts}
    
\end{figure*}

\section{Supplementary Results}
\label{appendix:additional_results}

\begin{itemize}
    \item Figure \ref{fig:incorrect_date_position_instruct_classic} shows the relative positions of incorrect dates favored over correct dates, when the win rate exceeds 95\%, on the 5 most robust models and using instruction format. It shows , similarly to raw format, that LMs often fail to achieve robustness because they are distracted by easy incorrect dates. Specifically, language models fail to achieve robustness due to dates with a distance of $|\alpha| \geq 1$ in 40\% of cases. This proportion decreases to 23\% for $|\alpha| \geq 2$ and 15\% for $|\alpha| \geq 3$.

    \item Figure \ref{fig:alpha_vs_logprob_detailed} show the evolution of $log P (o | f, d)$ with respect to the relative distance of the date to the validity period $\alpha$, which is equivalent to Figure \ref{fig:alpha_vs_logprob} but with higher details.

    \item Figure \ref{fig:top_rels} show the relations that were the most known robustly on average by studied LMs (raw statements format).
\end{itemize}

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{plots/incorrect_date_position_instruct_classic.pdf}
    \caption{Relative positions of incorrect dates favored over correct dates, when the win rate exceeds 95\%, on the 5 most robust models. Inputs to LMs are formatted as instructions.}
    \label{fig:incorrect_date_position_instruct_classic}
\end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{plots/alpha_vs_logprob_classic.pdf}
    \caption{the evolution of $log P (o | f, d)$ with respect to the relative distance of the date to the validity period $\alpha$. Each points is an average over many data points.}
    \label{fig:alpha_vs_logprob_detailed}
\end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{plots/relation_vs_robust_classic.pdf}
    \caption{The top-10 most known relations in TimeStress on average by studied LMs.}
    \label{fig:top_rels}
\end{figure}

\end{document}
