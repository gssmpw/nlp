\section{Related work}
\label{sec:related_work}

\paragraph{Robustness of Factual Knowledge in LMs.} It has been demonstrated that LMs store a significant amount of factual information \cite{petroni, know_what_language_models_know, head_to_tail}. However, many studies indicate that this acquired knowledge often lacks robustness when faced with text perturbations. For example, \citet{robust_negation} highlighted the limitations of pretrained LMs in adapting to negation in questions, leading to contradictory answers. Robustness to paraphrasing and minor typos has also been extensively studied \cite{cc_paraphrase1, cc_paraphrase2, cc_paraphrase3, cc_paraphrase4, cc_paraphrase5}. Notably, \citet{pararel} and \citet{paraphrase1} found that LMs produce different answers for semantically equivalent factual queries, and \citet{robust_typo} discovered that recent embedding models are negatively impacted by minor typos that preserve the original semantics. Many of these studies were conducted on older generations of LMs. But more recently, \citet{verma-etal-2023-evaluating} demonstrated that the consistency of recent LMs for the task of text entailment is now high for more recent models. However, the robustness of temporality representation in these models remains unexplored.

\paragraph{Temporal Reasoning in LMs.} Many studies investigated the temporal reasoning capabilities of LMs \cite{zhang-choi-2021-situatedqa, chu-etal-2024-timebench,tempreason1_mentaqa,fatemi2024testtimebenchmarkevaluating, templama, xiong-etal-2024-large, su-etal-2024-living}. Notably, the works by \citet{tempreason_timequestions} and \citet{tempreason1_bench_temp_reason} are related to ours. They each proposed datasets where LMs are tasked with relating two timeframes (\eg determining which period precedes the other), a timeframe and a fact (\eg assessing if a fact is valid at a given period), and, in the former study, two facts (\eg determining which fact precedes the other). However, these studies do not measure consistency, as they evaluate LMs' performance by averaging the number of correct answers across many tests with one test per fact. Consistency, in contrast, is achieved when a model systematically passes all tests for a given fact. Also, one advantage of our approach over these studies is its compatibility with non-functional relations. Indeed, because they use generation-based metrics (Exact Match and/or F1-Score) to measure the LMs' answer quality, it is difficult to judge the model when there are many correct answers. Finally, studying how resilient is a temporal representation of facts across several date precisions was not explored.

% \paragraph{Temporality Datasets and Benchmarks.} SituatedQA \cite{zhang-choi-2021-situatedqa} . Also, several evaluation datasets were proposed to detect outdated facts in LMs \cite{outdated1_set_the_clock, outdated2_carpediem, outdated3_signal, outdated4_realtime, outdated5_dyknow}

% \paragraph{Temporal alignment of knowledge in LMs.} Because factual knowledge is comprised of a large portion of temporal facts that are constantly evolving, studies were performed to see how LMs adapt to this shift. As expected, LMs were shown to be unable to predict future facts \cite{mindthegap} which implies the necessity to adapt them to maintain their alignment with current knowledge. To solve this problem, continual learning methods \cite{streaming_qa} as well as special pretraining techniques were proposed, such as jointly modeling text and its associated timestamp which facilitates the acquisition of temporal knowledge \cite{templama}; knowledge editing techniques \cite{rome, grace, melo, zhang-etal-2023-large}; or simply externalizing the knowledge source using retrieval-augmented generation \cite{rag}. In parallel, several evaluation datasets were proposed to detect outdated facts in LMs \cite{outdated1_set_the_clock, outdated2_carpediem, outdated3_signal, outdated4_realtime, outdated5_dyknow}.