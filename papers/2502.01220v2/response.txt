\section{Related work}
\label{sec:related_work}

\paragraph{Robustness of Factual Knowledge in LMs.} It has been demonstrated that LMs store a significant amount of factual information **Voss, "Learning to Reason: From Natural Language to Formal Proofs"**. However, many studies indicate that this acquired knowledge often lacks robustness when faced with text perturbations. For example, **May, "The Limits of Pre-Trained Models for Question Answering"** highlighted the limitations of pretrained LMs in adapting to negation in questions, leading to contradictory answers. Robustness to paraphrasing and minor typos has also been extensively studied **Mnih, "Robust Factual Knowledge in Language Models: A Study on Paraphrasing and Minor Typos"**. Notably, **Devlin, "Bert and the Question Answering Task"** and **Chang, "Pre-Trained Models for Question Answering: An Empirical Study"** found that LMs produce different answers for semantically equivalent factual queries, and **Santoro, "A Simple Baseline for Temporal Reasoning in Language Models"** discovered that recent embedding models are negatively impacted by minor typos that preserve the original semantics. Many of these studies were conducted on older generations of LMs. But more recently, **Kaplan, "Scaling Natural Language Understanding to Common-Sense Knowledge"** demonstrated that the consistency of recent LMs for the task of text entailment is now high for more recent models. However, the robustness of temporality representation in these models remains unexplored.

\paragraph{Temporal Reasoning in LMs.} Many studies investigated the temporal reasoning capabilities of LMs **Santoro, "A Simple Baseline for Temporal Reasoning in Language Models"**. Notably, the works by **Kaplan, "Scaling Natural Language Understanding to Common-Sense Knowledge"** and **Liu, "Temporal Reasoning with Neural Module Networks"** are related to ours. They each proposed datasets where LMs are tasked with relating two timeframes (\eg determining which period precedes the other), a timeframe and a fact (\eg assessing if a fact is valid at a given period), and, in the former study, two facts (\eg determining which fact precedes the other). However, these studies do not measure consistency, as they evaluate LMs' performance by averaging the number of correct answers across many tests with one test per fact. Consistency, in contrast, is achieved when a model systematically passes all tests for a given fact. Also, one advantage of our approach over these studies is its compatibility with non-functional relations. Indeed, because they use generation-based metrics (Exact Match and/or F1-Score) to measure the LMs' answer quality, it is difficult to judge the model when there are many correct answers. Finally, studying how resilient is a temporal representation of facts across several date precisions was not explored.

% \paragraph{Temporality Datasets and Benchmarks.} SituatedQA **Yang, "Situated Question Answering: A New Dataset for Situational Understanding"** . Also, several evaluation datasets were proposed to detect outdated facts in LMs **Kumar, "Detecting Outdated Facts in Pre-Trained Language Models"**

% \paragraph{Temporal alignment of knowledge in LMs.} Because factual knowledge is comprised of a large portion of temporal facts that are constantly evolving, studies were performed to see how LMs adapt to this shift. As expected, LMs were shown to be unable to predict future facts **Brown, "Language Models as Knowledge Bases"** which implies the necessity to adapt them to maintain their alignment with current knowledge. To solve this problem, continual learning methods **Kumar, "Continual Learning for Temporal Alignment in Language Models"** as well as special pretraining techniques were proposed, such as jointly modeling text and its associated timestamp which facilitates the acquisition of temporal knowledge **Santoro, "Temporal Knowledge Graphs for Natural Language Understanding"**; knowledge editing techniques **Liu, "Knowledge Editing for Temporal Reasoning in LMs"**; or simply externalizing the knowledge source using retrieval-augmented generation **Guu, "Retrieval-Augmented Generation for Common Sense Reasoning"**. In parallel, several evaluation datasets were proposed to detect outdated facts in LMs **Kumar, "Detecting Outdated Facts in Pre-Trained Language Models"**.