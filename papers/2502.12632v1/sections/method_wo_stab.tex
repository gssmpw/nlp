\input{figures/concept}
\section{\sname: \lname}
\label{sec:method}
We first formalize our problem as follows. Consider a dataset $\mathcal{D}$, where each data $(\bx, \bc) \in \mathcal{D}$ consists of a video $\bx$ and corresponding conditions $\bc$ (\eg, text captions). Our goal is to train a model using $\mathcal{D}$ to learn a model distribution $p_{\text{model}}(\bx | \bc)$ that matches a ground-truth conditional distribution $p_{\text{data}} (\bx | \bc)$. In particular, we are interested in the situation where each $\bx$ is a \emph{long} video, where target video lengths are much larger than conventional choices of $<$100 for both training and inference~\citep{he2022lvdm}.

To efficiently model the distribution of long videos, we adopt ``memory'' that encodes previous long context in our latent diffusion transformer. Specifically, we aim to train a single model capable of: (a) encoding previous context of the long video as a compact memory latent vector, and (b) generating a future clip conditioned on the memory and a given condition $\bc$. 

In the rest of this section, we explain our \lname (\sname) in detail. In Section~\ref{subsec:ldm}, we provide a brief overview of latent diffusion models. In Section~\ref{subsec:obj}, we describe how we formulate the problem and how we design a training objective. Finally, in Section~\ref{subsec:arch}, we explain the architecture that we used in our framework. %\jon{can delete this if we need space}

\textbf{Notation.}
We write a sequence of vectors $[\bx^{a} \ldots, \bx^{b}]$ with $a<b$ as $\bx^{a:b}$. 

\subsection{Latent diffusion models}
\label{subsec:ldm}
In order to generate data, 
diffusion models learn the \emph{reverse} process of a
%\jon{destructive?}
forward diffusion, where the forward diffusion diffuses a data $\bx_{0} \sim p_{\text{data}}(\bx)$ to a (simple) prior distribution $\bx_{T} \sim \mathcal{N}(\mathbf{0}, \sigma_{\mathrm{max}} \mathbf{I})$ (with pre-defined $\sigma_{\mathrm{max}}>0$) with the following stochastic differential equation (SDE):
\begin{align}
    d\bx = \mathbf{f} (\bx, t) dt + g(t) d\mathbf{w},
    \label{eq:forwardsde}
\end{align}
where $\mathbf{f}$, $g$, and $\mathbf{w}$ are pre-defined drift coefficient, diffusion coefficient, and standard Wiener process (respectively) with $t \in [0, T]$ and pre-defined $T>0$. 
With this forward process, data sampling can be done with the following reverse SDE of Eq.~\eqref{eq:forwardsde}: 
\begin{align}
    d\bx = \Big[ \mathbf{f} (\bx, t) - \frac{1}{2} g(t)^2 \nabla_{\bx} \log p_t (\bx) \Big] dt + g(t) d\mathbf{\bar{w}},
\end{align}
where $\mathbf{\bar{w}}$ is a standard reverse-time Wiener process, and $\nabla_{\bx} \log p_t(\bx)$ is a score function of the marginal density from Eq.~\eqref{eq:forwardsde} at time $t$.
% \jon{in the above eqn, $p_t(x)$ is undefined.  maybe a good place to define it is directly after eqn 1 and say that it has the property $p_0$ is the data distribution and $p_T$ is the prior. and probably good to say here that nabla log pt is the score function}
\citet{song2021scorebased} shows there exists a \emph{probability flow ordinary differential equation (PF ODE)}
whose marginal $p_t (\bx)$ is identical for the SDE with $t \in [0, T]$:
\begin{align}
    d\bx = \Big[ \mathbf{f} (\bx, t) - \frac{1}{2} g(t)^2 \nabla_{\bx} \log p_t (\bx) \Big] dt.
    \label{eq:pfode}
\end{align}
Following previous diffusion model methods~\citep{lee2024dreamflow,zheng2024fast}, we use the setup in EDM~\citep{karras2022edm} with $\mathbf{f} (\bx, t) \coloneqq \mathbf{0}$, $g(t)\coloneqq \sqrt{2\dot\sigma(t)\sigma(t)}$ and a decreasing noise schedule $\sigma: [0, T] \to \mathbb{R}_{+}$. In this case, the PF ODE in Eq.~\eqref{eq:pfode} can be written:
\begin{align}
    d\bx = -\dot\sigma(t)\sigma(t)\nabla_{\bx}\log p (\bx; \sigma(t)) dt,
\end{align}
where we denote $p(\bx; \sigma)$ be
the smoothed distribution by adding i.i.d Gaussian noise $\bm{\epsilon}\sim\mathcal{N}(\mathbf{0}, \sigma \mathbf{I})$ with standard deviation $\sigma > 0$. To learn the score function $\nabla_{\bx}\log p (\bx; \sigma(t))$, we train a denoising network $D_{\bm{\theta}} (\bx, t)$ with the denoising score matching (DSM)~\citep{song2019generative} objective for all $t \in [0, T]$:
\begin{align*}
\mathbb{E}_{\bx, \bm{\epsilon}, t}\Big[ \lambda({t}) || D_{\bm{\theta}}(\bx_0 + \bm{\epsilon}, t)  - \mathbf{\bx}_0||_2^2 \Big],
\end{align*}
where $\lambda(\cdot)$ assigns a non-negative weight.

However, training $D_{\bm{\theta}}$ directly with raw high-dimensional $\bx$ is computation and memory expensive. To solve this problem, latent diffusion models~\citep{rombach2021highresolution} first learn a 
lower dimensional latent representation of $\bx$ by training an autoencoder (with encoder $F(\bx) = \bz$ and 
decoder $G(\bz) = \bx)$ to reconstruct $\bx$ from 
the low-dimensional vector $\bz$, and then train $D_{\bm{\theta}}$ to generate in this latent space instead.
Specifically, latent diffusion models use the following denoising objective defined in the latent space:
\begin{align*}
    \mathbb{E}_{\bx, \bm{\epsilon}, t}\Big[ \lambda(t) || D_{\bm{\theta}}(\bz_0 + \bm{\epsilon}, t)  - \mathbf{\bz}_0||_2^2 \Big],
\end{align*}
where $\bz_0=F(\bx_0)$. After training the model in latent space, we sample a latent vector $\bz$ through an ODE/SDE solver~\citep{song2021denoising,song2021scorebased,karras2022edm} and then decode the result using $G$ to generate
a final sample.

\subsection{Modeling long sequences via blockwise latent diffusion}
\label{subsec:obj}
\textbf{Autoencoder.}
Given a long video $\bx^{1:NL} \in \mathbb{R}^{NL \times H \times W \times 3}$ with a resolution $H \times W$ and a length $NL>0$, we first divide it to $N$ segments $\bx^{(i-1)L+1:iL}$ for $1\leq i \leq N$. This is because to avoid encoder and decoder to compute the entire very long sequence at once to reduce memory and computation constraints. After that, we encode and decode $m<N$ segments joint at a time; for $1 \leq i \leq N/m$, we encode and $\bx^{(i-1)mL+1:imL}$ as a latent vector $\bz^{im:(i+1)m}$ and decode it as:
\begin{align*}
    \bz^{im:(i+1)m}\coloneqq F(\bx^{(i-1)mL+1:imL}) \in \mathbb{R}^{m\cdot l \times h \times w \times c},\quad G(\bz^{im:(i+1)m})\approx \bx^{(i-1)mL+1:imL},
\end{align*}
where $F(\cdot)$ is an encoder network that maps the original video segments to the corresponding latent vectors with a spatial downsampling factor $d_s = H/h = W/w > 1$ and a temporal downsampling factor $d_l = L/l > 1$, and $G(\cdot)$ is a decoder network. We use these latent segments $\bz^{1}, \ldots, \bz^{n}$ of the original $\bx$ obtained from the autoencoder for modeling the long video distribution.

\textbf{Diffusion model.}
We now directly model $p(\bz^{1:L}|\bc)$ with the blockwise-autoregressive modeling with $\bz^{1}, \ldots, \bz^{N}$ of a long video $\bx^{1:L}$, namely $p (\bz^{1:L} | \bc) = \prod_{n=0}^{N-1} p(\bz^{n+1} | \bz^{1:n}, \bc)$ with $\bz^{1:0}\coloneqq \mathbf{0}$,
% \begin{align}
%     p (\bz^{1:L} | \bc) = \prod_{n=0}^{N-1} p(\bz^{n+1} | \bz^{1:n}, \bc), \,\, \text{where}\,\, \bz^{1:0} \coloneqq \mathbf{0},
% \end{align}
where we model all of $p(\bz^{n+1} | \bz^{1:n}, \bc)$ for $0\leq n \leq N-1$ using a single diffusion model $D_{\bm{\theta}}$.

However, if $N$ is large, $\bz^{1:n}$ becomes very high-dimensional, so using $\bz^{1:n}$ directly as a condition to $D_{\bm{\theta}}$ can easily require extreme memory and computation costs. To mitigate this bottleneck, we instead introduce a \emph {fixed-size} hidden state $\bh^{i} \coloneqq [\bh^{i}_1, \ldots, \bh^{i}_{d}]$ recurrently computed from $D_{\bm{\theta}}$ as a memory vector to encode previous contexts $\bz^{1:n}$, where $d>0$ is a number of hidden states that are used as memory vectors (\ie, $i$ is a segment index and $d$ refers to the number of layers of the model). 
%Hence, our diffusion model $D_{\bm{\theta}}(\bz_t^{n+1}, t;\,\bh^{n}, \bc)$ is trained to denoise $\bz_t^{n+1}$ using $t$, $\bc$, and the memory vector $\bh^n$ that amortizes $\bz^{1:n}$.

Specifically, for $1 \leq i \leq n$, we compute $\bh^n$ with the following recurrent mechanism:
\begin{align}
    \bh^{i} = \mathrm{HiddenState}\big(D_{\bm{\theta}} (\bz_0^{i}, 0;\, \small{\mathtt{sg}}(\bh^{i-1}), \bc)\big), \,\, \bh^{0} = [\mathbf{0}, \ldots, \mathbf{0}],
\end{align}
where $\small{\mathtt{sg}}$ denotes a stop-grad operation. Note that we use a clean segment $\bz^{i}$ without noise so we set $t=0$ here, which has not been used in conventional diffusion model training~\citep{ho2021denoising,song2021denoising,song2021scorebased}.

To sum up, we train $D_{\bm{\theta}}$ with the following denoising autoencoder objective with the memory $\bh^{n}$:
\begin{align}
    \mathbb{E}_{(\bx_0, \bc), \bm{\epsilon}, t, n} 
    \Big[
    \lambda(t)||D_{\bm{\theta}}(\bz_t^{n+1}, t;\, \bh^{n}, \bc) - \bz_0||_2^2 
    \Big],
\label{eq:pseudo-obj}
\end{align}
where $(\bx_0, \bc)$ is sampled from the video dataset $\mathcal{D}$, $\epsilon \sim p(\bm{\epsilon})$, $t \sim [1, T]$, and $n \sim p(n)$ with pre-defined prior distributions $p(\bm{\epsilon})$ and $p(n)$. Note that we do not use stop-grad operation to $\bh^{n}$ in Eq.~\eqref{eq:pseudo-obj}; as we mentioned, diffusion model training uses $t \geq 1$ in the common diffusion model objective because they only consider noisy inputs but our memory vector computation uses a clean sample ($t=0$), which cannot be optimized without a backpropagation to $\bh^{n}$. 
%\textcolor{magenta}{Note that while we use $t=0$ for previous latents and $t\geq1$ for the current segment, but one can assign any different latent timesteps prior to the current timestep depending on how we formulate this distribution learning problem.}\jon{this is not really understandable to me}
Moreover, recall that we use the stop-grad operation in the computation of $\bh^{n}$, so the memory cost does not increase rapidly with respect to the number of segments used in training.

\textbf{Inference.}
After training, we synthesize a long video by autoregressively generating short video clips. Specifically, we start from generating a first segment $\bz^{1}$ conditioned on $\bc$, and then iteratively generate $\bz^{n+1}$ for $n>0$ by computing memory $\bh^{n}$ and performing conditional generation from $\bh^{n}$ and $\bc$. We provide the detailed algorithm in Appendix~\ref{appen:sampling}.

\input{figures/model}
\subsection{Architecture}
\label{subsec:arch}

We now discuss specific architectural designs that we used, but note that the general approach outlined above can apply broadly to many choices of autoencoder and diffusion model architectures.

\textbf{Autoencoder.}
Similar to the encoding scheme used in a recent latent video diffusion, W.A.L.T~\citep{gupta2023photorealistic}, we use a
causal 3D CNN encoder-decoder architecture for the video autoencoder based on the recent MAGVIT-2 tokenizer~\citep{yu2024language}. We train the autoencoder with a sum of pixel-level reconstruction loss (\eg, mean-squared error), perceptual loss (\eg, LPIPS~\citep{zhang2018perceptual}), and adversarial loss~\citep{goodfellow2014generative} similar to existing image and video latent diffusion model approaches. Recall that both training and inference are not done directly on long videos; they are done after splitting long videos into short segments.

\textbf{Diffusion model.}
As outlined in Figure~\ref{fig:model}, we design our model architecture upon recent diffusion transformer (DiT) architecture~\citep{Peebles2022DiT}. Thus, given a latent vector $\bz^n \in \mathbb{R}^{l \times h \times w \times c}$ of a video clip $\bx^n$, we patchify it with a patch size $p_l \times p_s \times p_s$ to have a flattened latent vector $\mathtt{patchify}(\bz^n) \in \mathbb{R}^{(lhw / p_lp_s^2) \times c}$ with a sequence length $lhw / p_lp_s^2$ and use it as inputs to the model. In particular, we choose W.A.L.T~\citep{gupta2023photorealistic} as backbone, a variant of DiT for videos by introducing compute-efficient spatiotemporal window attention instead of full attention between large number of video patches.

To enable training with long videos with DiT architectures, we introduce a memory-augmented attention layer and insert this layer to every beginning of the Transformer block. Specifically, we design this layer as a cross-attention layer between the previous memory latent vector and the current hidden state, similar to memory-augmented attention~\citep{dai2019transformer} in language domain. Hence, query, key, and value of a $d$-th memory layer with the segment $\bz^{n}$ and the memory latent vector $\bh^{n-1}$ become:
\begin{align*}
    \text{query}\coloneqq \bh_{d}^{n},\quad\text{key} \coloneqq [\bh_{d}^{n-1}, \bh_{d}^{n}],\quad \,\,\text{value}\coloneqq [\bh_{d}^{n-1}, \bh_{d}^{n}], \quad \bh_{d}^{n-1}, \bh_{d}^{n} \in \mathbb{R}^{(hw / p_s^2) \times (l/p_l) \times c'},
\end{align*}
where $c'>0$ denotes the hidden dimension of the model and $\bh_{d}^{n-1}, \bh_{d}^{n}$ are \emph{reshaped} as a sequence length $l/p_l$ and a batch dimension size $hw/p_s^2$, similar to space-time factorized attention in previous video Transformers~\citep{arnab2021vivit,bertasius2021space}. Thus, memory-augmented attentions are only computed together with each of $l/p_l$ patches that have the same spatial location (\ie, temporal attention in video transformers). Because the computation of attention is restricted only to the sample spatial locations, the computation increase from our attention layers does not become significant; as the former has $O(L^2HW)$ computation complexity but the latter has the $O((LHW)^2)$ computation complexity.
We also use relative positional encoding that is widely used in transformer model to handle longer context.
Finally, recall that we build our architecture on W.A.L.T, but this memory-augmented layer idea can be applied to any video diffusion transformer architectures, such as \citep{lu2023vdt,ma2024latte}. We provide an detailed illustration of the architecture combined with W.A.L.T in Appendix~\ref{appen:archi}.