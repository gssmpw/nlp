\vspace{-0.1in}
\section{Introduction}
\label{sec:intro}
Diffusion models (DMs)~\citep{ho2021denoising,song2021scorebased} provide a scalable approach to training high quality generative models and have been shown in recent years to apply widely across a variety of data domains including images~\citep{dhariwal2021diffusion,karras2022edm}, audio~\citep{kong2020diffwave,lakhotia2021generative}, 3D shapes~\citep{lee2024dreamflow, luo2021diffusion, zeng2022lion}, and more. Inspired by these successes, a number of DM-based approaches have also been proposed for generating videos~\citep{blattmann2023align,gupta2023photorealistic,ho2022video,bar2024lumiere} and even enabling complex zero-shot text-to-video generation \citep{gupta2023photorealistic,videoworldsimulators2024}.

\input{figures/concept}

In video generation, a central challenge and key desiderata is to achieve \emph{any-length} generation where the model is capable of producing a video of arbitrary length conditioned on its understanding of the previous context. Many state-of-the-art existing video DM frameworks are \emph{prima facie}, only capable of generating videos of fixed temporal length (\eg, in the form of a fixed shape 3D RGB tensor \citep{ho2022video}) and generally limited from 2-10 seconds. A standard ``naive’’ approach for addressing any-length video generation is to use autoregressive generation where new frames are generated conditioned on a small number of previously generated video frames (rather than a long context). For example, given a 4-second base model, one might autoregressively generate 2 new seconds of footage at a time, conditioning on the previous 2 seconds. Thus, this autoregressive approach can generate compelling videos for longer extents than the base model (especially for very high-quality generation models). However, it also has clear limitations. By not conditioning on long temporal extents, these models often struggle to achieve long-term consistency.  Moreover, small errors during generation can easily cause error propagation~\citep{ruhe2024rolling}, resulting in a rapid quality drop with the frame index.

To summarize, there are two key subchallenges entailed by the any-length video generation problem:
\begin{itemize}[leftmargin=*,itemsep=0mm]
    \item \textbf{long-term contextual understanding}: allowing the model to condition on long temporal extents while staying within memory limitations and computational constraints
    \item \textbf{long-term stability}: generating long videos without frame quality degradation over time.
\end{itemize}
Solving both of these issues is a critical next step for video generation, and indeed, there have been a number of recent works that generate stable video using long context~\citep{harvey2022flexible,yan2023temporally}. However, they have been limited to simple datasets and fail to be stable for complex, realistic datasets. 


\vspace{0.02in}
\noindent\textbf{Our approach.}
In this paper, we tackle any-length video generation by achieving the aforementioned twin goals of long-term contextual understanding and long-term stability. Toward this end, we introduce a new latent DM specialized for long videos, coined \emph{\sname Diffusion}: \lname (\sname for short); see Figure~\ref{fig:concept} for an illustration. For long-term contextual understanding, we introduce a new diffusion transformer architecture that enables blockwise autoregressive generation of long videos. Motivated by recent language models developed for handling long sequences~(\eg, \citep{dai2019transformer}), we design our model by adding recurrent temporal attention layers into an existing diffusion transformer architecture, which computes the attention between the current segment and the hidden states of previous segments (context). Hence, hidden states of contextual segments act as a ``memory'' to encode context and is used to generate future segments. Consequently, in contrast to conventional DMs, our model can behave as both a DM for generating a video clip and as an encoder to encode previous contexts (clean inputs) as memory.

To mitigate error accumulation at inference (\ie, to achieve long-term stability), we propose a training technique that
encourages our trained model to be robust with 
respect to noisy memory vectors by applying noise augmentation to the memory vector at training time. Moreover, we carefully design our model optimization to mitigate the rapid increase of
computation and memory costs with respect to video length, enabling the training to be done on very long videos. 
With these components, \sname outperforms previous state-of-the-art results on popular long video generation and prediction benchmarks \citep{yu2023video,yan2023temporally} while using about $2\times$ fewer parameters than the prior best baselines.\footnote{We mainly focus on videos, but ideas can be similarly applied to other domain like PDE or climate data~\citep{ruhe2024rolling}.}


\vspace{0.02in}
\noindent{The main contributions of our paper are:}

\begin{itemize}[leftmargin=*,itemsep=0mm]
    \item We propose a novel latent diffusion model \emph{\sname Diffusion}: \lname (or just \sname), which can generate and be trained on long videos and addresses the aforementioned shortcomings. 
    \item We propose several techniques for training our architecture on long videos, resulting in a robust model both capable of being a denoiser and an encoder of previous contexts. We also ensure training can be performed without expensive memory requirements.
    \item We show the strengths of \sname 
    on long video generation/prediction benchmarks. For instance, \sname achieves an FVD~\citep{unterthiner2018towards} of 220.4 in unconditional 128-frame video generation on UCF-101~\citep{soomro2012ucf101}, 66.0\% better than the previous state-of-the-art of 648.4. Also, on a challenging realistic dataset, Kinetics-600~\citep{kay2017kinetics}, \sname exceeds the prior best long video prediction FVD as 799$\to$392. 
    %We also validate \sname's capability in long-term contextual understanding and stability.
    \item We also qualitatively validate \sname's capability in long-term contextual understanding and stability.
    In particular, we show that \sname shows a strong length generalization: \sname generates long videos when trained on an open-world short text-to-video dataset (up to 4-5 seconds 
    per clip), producing >120 seconds at 8 fps without suffering from significant frame quality degradation.
    %compared with existing methods for (training-free) long text-to-video generation.
\end{itemize}
