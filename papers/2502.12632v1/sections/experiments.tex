\input{figures/main_qual_ucf101}

\section{Experiments}
We validate the performance of \sname and the effect of the proposed components through extensive experiments. In particular, we investigate the following questions:
\begin{itemize}[leftmargin=*,itemsep=0mm]
\item Can \sname generate or predict videos with a long horizon with understanding? (Table~\ref{tab:ucf}, \ref{tab:kinetics}, Figure~\ref{fig:main_qual_ucf101}, \ref{fig:main_qual_k600})
%\item Can \sname do challenging zero-shot long text-to-video generation? (Figure~\ref{fig:main_qual_t2v})
\item Do the proposed components (recurrent memory, training for long-term stability) contribute to the final performance of images and videos (Table~\ref{tabs:ablation})?
\item Is the frame quality of generated video maintained across frame indices? (Figure~\ref{fig:analysis}) 
\end{itemize}

\vspace{0.02in}
\noindent\textbf{Note.} Although we achieve strong performance on popular benchmarks, the focus of our paper is more on long-term stability rather than per-frame fidelity. Thus, the size of our models is relatively compact, and we do not train separate super-resolution modules to increase frame quality.



\subsection{Setup}
\label{subsec:setup}

We explain some important setups in this section. We include more details in Appendix~\ref{appen:setup} and \ref{appen:baselines}.

\vspace{0.02in}
\noindent\textbf{Datasets.}
We use long video generation or prediction benchmarks~\citep{skorokhodov2021stylegan, yan2023temporally} to evaluate our method. Specifically, we use UCF-101~\citep{soomro2012ucf101} for unconditional video \emph{generation} with a length of 128 frames and use Kinetics-600~\citep{kay2017kinetics} for long video \emph{prediction} to predict 80 frames conditioned on 20 frames. We choose these video datasets as they contain diverse and complex motions and thus can demonstrate the scalability of \sname to generalize to real-world complex videos. For text-to-video generation, we train on 89M text-short-video
pairs (up to 37 frames) and
970M text-image pairs from public internet and internal
sources. All datasets are center-cropped and resized to 128$\times$128 resolution.

\vspace{0.02in}
\noindent\textbf{Training details.}
All models are trained with the AdamW~\citep{loshchilov2018decoupled} optimizer with a learning rate of 5$e$-5. For our autoencoder, we use the same architecture and configuration as W.A.L.T~\citep{gupta2023photorealistic}. We also use similar diffusion model configurations that W.A.L.T used: specifically, we use the W.A.L.T~XL-config for UCF-101 and W.A.L.T~L-config for Kinetics-600 experiments. We also use the W.A.L.T~L-config for text-to-video experiments. For the memory noise scale $\sigma_{\mathrm{mem}}$, we use 0.1 in all experiments.

\vspace{0.02in}
\noindent\textbf{Metrics.}
We mainly use the Fr\`echet Video Distance (FVD~\citep{unterthiner2018towards}; lower is better) to evaluate the quality of generated videos, following recent works~\citep{yu2023video,skorokhodov2021stylegan,kim2024hybrid}. On Kinetics-600, we also measure Peak signal-to-noise ratio (PSNR; higher is better), SSIM (higher is better), and perceptual metric (LPIPS \citep{zhang2018perceptual}; lower is better) between ground-truth video frames and predicted frames to measure how well the prediction resembles the ground-truth. 

\vspace{0.02in}
\noindent\textbf{Baselines.}
We use existing recent video generation and prediction methods as baselines that are capable of long video generation. Specifically, we use MoCoGAN~\citep{tulyakov2018mocogan}, MoCoGAN-HD~\citep{tian2021good}, DIGAN~\citep{yu2022digan}, StyleGAN-V~\citep{skorokhodov2021stylegan}, PVDM~\citep{yu2023video}, and HVDM~\citep{kim2024hybrid} as baselines for long video generation. For the video prediction task, we consider the following baselines: Perceiver AR~\citep{hawthorne2022general}, Latent FDM~\citep{harvey2022flexible}, and TECO~\citep{yan2023temporally}, which are recent generation methods designed for handling long videos with long-term understanding.

\input{figures/main_qual_k600}
\input{figures/error_propagation}
\input{figures/main_qual_t2v}
 
\subsection{Long-term contextual understanding}
%Figure~\ref{fig:main_qual_ucf101} visualizes \sname's results when trained on UCF-101.
First, Figure~\ref{fig:main_qual_ucf101} on UCF-101 shows that our method does understand long-term dependencies through memory latent vectors: in the video of two people fencing (last row), the first person on the left disappears entirely but is able to reappear in the last generated frames.
This is also demonstrated in Figure~\ref{fig:main_qual_k600} where we train on Kinetics-600. For instance, in the second video, there is an orange roof on the left side of the given prefix frames. The roof is covered by the camel's head for a few dozen frames before reappearing in the last predictions. Without a long understanding of the context, the model cannot remember these details, and the final predicted segment cannot contain this roof. 
%Also, in the third video, the yellow chair appears in the first few frames and then becomes mostly hidden with the camera movement. Without long-term understanding, the model cannot understand the yellow lines on the legs of the chair and cannot reconstruct this detail when the camera returns to its original angle. 
%However, MALT exactly reconstructs such a chair leg in the last predicted frames. 

Note that this is possible due to the fundamental difference between predominant autoregressive methods and \sname: the former is ``explicitly'' autoregressive, while \sname is not. Specifically, prior works generate video frames conditioned on the previous explicit context (\eg,, explicit in the sense of being conditioned on the last 8-16 RGB frames), whereas our method autoregressively generates video frames with respect to a ``latent memory'', which is capable of capturing much longer context in an efficient manner. This allows our model to have an extremely long receptive field without blowing up memory requirements.


\subsection{Long-term stability}
We also quantitatively analyze long term generation stability by visualizing FVD plots of the recent long-video generation training scheme, TECO, and \sname in Figure~\ref{fig:analysis}. Here, FVD values are computed by dividing the predicted video frames into multiple segments of 16-frame video clips and separately measuring FVD values using video clips at each specific interval. As shown in this figure, \sname generates higher-quality videos (\ie, FVD values are always better at any given interval) but also suffers less from error accumulation issues than the previous state-of-the-art TECO. Specifically, the FVD difference between the first and the last FVD value of \sname is smaller (174.3) than TECO (225.5) even if the FVD difference of \sname is measured with 2$\times$ more number of segments (\ie, \sname generates 2$\times$ more video frames). This indicates the frame quality of \sname drops slower than TECO, highlighting the robustness of \sname to error accumulation. 

We also compare the T2V generation results from recent training-free long T2V methods and MALT.\footnote{There are numerous factors that affect the frame quality, to name a few, model size, text caption fidelity, and video-text alignment. Thus, an apples-to-apples comparison of videos based on frame fidelity is difficult because each method uses design choices for aforementioned factors: Thus, we focus on comparing long-term stability of generated video frames.} 
To compare this, we train a relatively small model (545M) compared to existing T2V models that usually have more than 2B parameters \citep{blattmann2023align,gupta2023photorealistic,ho2022imagen}. 
As shown in Figure~\ref{fig:main_qual_t2v}, \sname shows reasonable T2V generation on complex text prompts and more importantly, generated frames are consistent and stable over a long temporal horizon. We provide more visualizations in Appendix~\ref{appen:more_qual}: we show that \sname can be generalized to generate up to 2 minute videos with 8 fps.

\input{tables/ucf_kinetics_merged}

\subsection{System-level comparison}
Table~\ref{tab:ucf} and \ref{tab:kinetics} summarize long video generation and prediction results on UCF-101 and Kinetics-600 datasets, respectively. On UCF-101, \sname achieves an FVD score (lower is better) of 220.4, significantly outperforming existing video generation methods including unconditional latent video diffusion models capable of long video generation, such as PVDM~\citep{yu2023video} and HVDM~\citep{kim2024hybrid}. Similarly, on a long video prediction task on Kinetics-600, \sname shows much better performance than the previous state-of-the-art method, TECO, on all four metrics despite have $\sim 2\times $ fewer parameters (440M for \sname and 1.1B for TECO).

\input{tables/ablation}

% \input{tables/ucf}
% \input{tables/kinetics}
\subsection{Ablation studies}
\label{subsec:ablation}

We conduct ablation studies and report in Table~\ref{tabs:ablation}. We also perform error propagation analysis and summarize the result in Figure~\ref{fig:analysis}. For further analysis, see Appendix~\ref{appen:ablation}.

\vspace{0.02in}
\noindent\textbf{Memory design.}
As shown in Table~\ref{tabs:ablation}, a conditional model that only uses the last segment (``Last only'') shows worse performance than other models that use longer contexts, indicating the importance of long-term contextual understanding. Moreover, our recurrent and compact memory design (``Recurrent'') shows comparable performance compared to the kv-cache that uses the entire context without any compression, which speaks to the effectiveness of \sname.

\vspace{0.02in}
\noindent\textbf{Training objective.}
The model trained without noise augmentation and correlated prior distribution shows worse SSIM and PSNR (\eg, PSNR gets worse from 15.4 to 14.7), exhibiting the frame quality drop across the frame indices. It demonstrates the importance of our training techniques that use noise augmentation to the memory latent vector and correlated prior distribution for stabilizing the frame quality. 

