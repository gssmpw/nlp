\section{\sname Diffusion with \lname}
\label{sec:method}
Consider a dataset $\mathcal{D}$, where each example $(\bx, \bc) \in \mathcal{D}$ consists of a video $\bx$ and corresponding conditions $\bc$ (\eg, text captions). Our goal is to train a model using $\mathcal{D}$ to learn a model distribution $p_{\text{model}}(\bx | \bc)$ that matches a ground-truth conditional distribution $p_{\text{data}} (\bx | \bc)$. In particular, we are interested in the situation where each $\bx$ is a \emph{long} video, and video lengths are much larger than conventional methods that often use $\sim$20-128 frames for both training and inference~\citep{he2022lvdm}.

To efficiently model long video distribution, we adopt a ``memory'' that encodes previous long context in our latent diffusion transformer. Specifically, we aim to train a single model capable of: (a) encoding previous context of the long video as a compact memory latent vector, and (b) generating a future clip conditioned on the memory and $\bc$. 

In the rest of this section, we explain \sname Diffusion in detail. In Section~\ref{subsec:ldm}, we provide a brief overview of latent diffusion models. In Section~\ref{subsec:obj}, we describe problem formulation and how we design a training objective. Finally, in Section~\ref{subsec:arch}, we explain the architecture that we used.

\vspace{0.02in}
\noindent\textbf{Notation.}
We write a sequence of vectors $[\bx^{a} \ldots, \bx^{b}]$ with $a<b$ as $\bx^{a:b}$. 

\subsection{Latent diffusion models}
\label{subsec:ldm}
In order to generate data, 
diffusion models learn the \emph{reverse} process of a forward diffusion, where the forward diffusion diffuses an 
example $\bx_{0} \sim p_{\text{data}}(\bx)$ to a (simple) prior distribution $\bx_{T} \sim \mathcal{N}(\mathbf{0}, \sigma_{\mathrm{max}}^2 \mathbf{I})$ (with pre-defined $\sigma_{\mathrm{max}}>0$) with the following stochastic differential equation (SDE):
\begin{align}
    d\bx = \mathbf{f} (\bx, t) dt + g(t) d\mathbf{w},
    \label{eq:forwardsde}
\end{align}
where $\mathbf{f}$, $g$, $\mathbf{w}$ are pre-defined drift and diffusion coefficients, and standard Wiener process (respectively) with $t \in [0, T]$ with $T>0$. 
With this forward process, data sampling can be done with the following reverse SDE of Eq.~\eqref{eq:forwardsde}: 
\begin{align}
    d\bx = \Big[ \mathbf{f} (\bx, t) - \frac{1}{2} g(t)^2 \nabla_{\bx} \log p_t (\bx) \Big] dt + g(t) d\mathbf{\bar{w}},
\end{align}
where $\mathbf{\bar{w}}$ is a standard reverse-time Wiener process, and $\nabla_{\bx} \log p_t(\bx)$ is a score function of the marginal density from Eq.~\eqref{eq:forwardsde} at time $t$.
\citet{song2021scorebased} shows there exists a \emph{probability flow ordinary differential equation (PF ODE)}
whose marginal $p_t (\bx)$ is identical for the SDE:
\begin{align}
    d\bx = \Big[ \mathbf{f} (\bx, t) - \frac{1}{2} g(t)^2 \nabla_{\bx} \log p_t (\bx) \Big] dt.
    \label{eq:pfode}
\end{align}
Following previous diffusion model methods~\citep{lee2024dreamflow,zheng2024fast}, we use the setup in EDM~\citep{karras2022edm} with $\mathbf{f} (\bx, t) \coloneqq \mathbf{0}$, $g(t)\coloneqq \sqrt{2\dot\sigma(t)\sigma(t)}$ and a decreasing noise schedule $\sigma: [0, T] \to \mathbb{R}_{+}$. In this case, the PF ODE in Eq.~\eqref{eq:pfode} can be written:
\begin{align}
    d\bx = -\dot\sigma(t)\sigma(t)\nabla_{\bx}\log p (\bx; \sigma(t)) dt,
\end{align}
where  $p(\bx; \sigma)$ is
the smoothed distribution by adding i.i.d Gaussian noise $\bm{\epsilon}\sim\mathcal{N}(\mathbf{0}, \sigma^2 \mathbf{I})$ with standard deviation $\sigma > 0$. To learn the score function $\nabla_{\bx}\log p (\bx; \sigma(t))$, we train a denoising network $D_{\bm{\theta}} (\bx, t)$ with the denoising score matching (DSM)~\citep{song2019generative} objective for all $t \in [0, T]$:
\begin{align*}
\mathbb{E}_{\bx, \bm{\epsilon}, t}\Big[ \lambda({t}) || D_{\bm{\theta}}(\bx_0 + \bm{\epsilon}, t)  - \mathbf{\bx}_0||_2^2 \Big],
\end{align*}
where $\lambda(\cdot)$ assigns a non-negative weight.

However, training $D_{\bm{\theta}}$ directly with raw high-dimensional $\bx$ is computation and memory expensive. To solve this problem, latent diffusion models~\citep{rombach2021highresolution} first learn a 
lower dimensional latent representation of $\bx$ by training an autoencoder (with encoder $F(\bx) = \bz$ and 
decoder $G(\bz) = \bx)$ to reconstruct $\bx$ from 
the low-dimensional vector $\bz$, and then train $D_{\bm{\theta}}$ to generate in this latent space instead.
Specifically, latent diffusion models use the following denoising objective defined in the latent space:
\begin{align*}
    \mathbb{E}_{\bx, \bm{\epsilon}, t}\Big[ \lambda(t) || D_{\bm{\theta}}(\bz_0 + \bm{\epsilon}, t)  - \mathbf{\bz}_0||_2^2 \Big],
\end{align*}
where $\bz_0=F(\bx_0)$. After training the model in latent space, we sample a latent vector $\bz$ through an ODE or SDE solver~\citep{song2021denoising,song2021scorebased,karras2022edm} starting from random noises and then decode the result using $G$ to generate
a final sample.

\subsection{Modeling long videos via blockwise diffusion}
\label{subsec:obj}

Given a ``long'' video $\bx^{1:S} \in \mathbb{R}^{S \times H \times W \times 3}$ with a resolution $H \times W$ and number of frames $S>0$, we divide the video into $N$ \emph{segments} of length $L$: $\bx^{(i-1)L+1:iL}$ for $1\leq i \leq N$ (thus $S=NL$). \sname will autoregressively generate
these segments one-by-one.

\vspace{0.02in}
\noindent\textbf{Autoencoder.}
Following the standard latent diffusion approach, we would like to be able to encode and decode 
videos using a trained autoencoder, which is typically more lightweight 
compared to the diffusion model.  However for very long videos, even running the autoencoder
can be infeasible---thus we encode and decode videos in \emph{chunks} of $m<N$ contiguous segments at a time.  Specifically, for $1 \leq i \leq N/m$, we encode $\bx^{(i-1)mL+1:imL}$ as a latent vector $\bz^{im:(i+1)m}$ and decode it as:
\begin{gather*}
    \bz^{im:(i+1)m}\coloneqq F(\bx^{(i-1)mL+1:imL}) \in \mathbb{R}^{m\cdot l \times h \times w \times c}, \\
    G(\bz^{im:(i+1)m})\approx \bx^{(i-1)mL+1:imL},
\end{gather*}
where $F(\cdot)$ is an encoder network that maps the original video segments to their corresponding latent vectors with a spatial downsampling factor $d_s = H/h = W/w > 1$ and a temporal downsampling factor $d_l = L/l > 1$, and $G(\cdot)$ is a decoder network. We use these latent segments $\bz^{1}, \ldots, \bz^{n}$ of the original $\bx$ obtained from the autoencoder for modeling the long video distribution.

\vspace{0.02in}
\noindent\textbf{Diffusion model.}
We now directly model the joint distribution of latent segments,  $p(\bz^{1:N}|\bc)$, autoregressively as 
\begin{align}
p (\bz^{1:N} | \bc) = \prod_{n=0}^{N-1} p(\bz^{n+1} | \bz^{1:n}, \bc) \text{with}\,\,\, \bz^{1:0}\coloneqq \mathbf{0}
\end{align}
where we learn all 
$p(\bz^{n+1} | \bz^{1:n}, \bc)$ for $0\leq n \leq N-1$ using a single diffusion model $D_{\bm{\theta}}$.

A na\"ive approach would be to use $\bz^{1:n}$ directly as a condition to the model; however, if the number of segments, $N$, is large, $\bz^{1:n}$ easily becomes extremely high-dimensional and prohibitive with respect to memory and compute. To mitigate this bottleneck, we instead introduce a \emph {fixed-size} hidden state $\bh^{i} \coloneqq [\bh^{i}_1, \ldots, \bh^{i}_{d}]$ recurrently computed from $D_{\bm{\theta}}$. This hidden state serves as a memory vector to encode the context (i.e., the previous sequence of segments) $\bz^{1:n}$, where $d>0$ is a number of hidden states that are used as memory vectors (\ie, $i$ is a segment index and $d$ refers to the number of layers of the model). 

Specifically, for $1 \leq i \leq n$, we compute $\bh^n$ with the following recurrent mechanism:
\begin{equation}
    \bh^{i} = \mathrm{HiddenState}\big(D_{\bm{\theta}} (\bz_0^{i}, 0;\, \small{\mathtt{sg}}(\bh^{i-1}), \bc)\big),
\end{equation}
where $\bh^{0} = [\mathbf{0}, \ldots, \mathbf{0}]$ and $\small{\mathtt{sg}}$ denotes a stop-grad operation.
Note that we use a clean segment $\bz^{i}$ without added noise so we set $t=0$ here, which has not been used in conventional diffusion model training~\citep{ho2021denoising,song2021denoising,song2021scorebased}.

\input{figures/model}

To sum up, we train $D_{\bm{\theta}}$ with the following denoising autoencoder objective with the memory $\bh^{n}$:
\begin{align}
    \mathbb{E}
    \Big[
    \lambda(t)||D_{\bm{\theta}}(\bz_t^{n+1}, t;\, \bh^{n}, \bc) - \bz_0^{n+1}||_2^2 
    \Big],
\label{eq:pseudo-obj}
\end{align}
where each data is sampled from the video dataset $\mathcal{D}$, $\epsilon \sim p(\bm{\epsilon})$, $t \sim [1, T]$, and $n \sim P(n)$ with pre-defined prior distributions $p(\bm{\epsilon})$ and $P(n)$. 
Specifically, we set $P(n)$ as $P(n$ {$=$} $0)=\nicefrac{1}{2}$ and $P(n$ {$=$} $k)=\nicefrac{1}{2(N-1)}$ for $k>0$, as generating videos without memory (\ie, $n=0$) is more difficult than continuation with a given memory vector (\ie, $n>0$).
Note that in Eq.~\eqref{eq:pseudo-obj} we do not use the stop-grad operation on $\bh^{n}$ itself; as we mentioned, diffusion model training uses $t \geq 1$ in the common diffusion model objective because they only consider noisy inputs but our memory vector computation uses a clean sample ($t=0$), which cannot be optimized without a backpropagation to $\bh^{n}$. 
Instead, we use the stop-grad operation on the $\bh^{i}$ for $i<n$ which are used to compute $\bh^{n}$ in
order to reduce memory requirements with respect to the number of segments used during training.

\vspace{0.02in}
\noindent\textbf{Training for long term stability.}
Unfortunately, the video quality from the model $D_{\bm{\theta}}$ trained with Eq.~\eqref{eq:pseudo-obj} is usually not satisfactory, because of frame quality degradation caused by error accumulation. We hypothesize that the reason why this happens is because there exists a discrepancy between training and inference: in training, we use ground-truth latent vector $\bz^{n}$ for computation of the memory $\bh^{n}$, but at inference, the model instead uses generated latent vectors, 
where small errors during generation can compound over a long sequence of segments.

To mitigate this discrepancy, we use a noisy version of $\bh^{n}$ at training time, denoted by $\tilde{\bh}^{n}$,  where
\begin{gather}
    \tilde{\bh}^{n} \coloneqq \mathrm{HiddenState}\big(D_{\bm{\theta}} (\bz^{n} + \bm{\xi}, 0;\, \small{\mathtt{sg}}(\bh^{n-1}), \bc)\big),\\ \text{where}\,\,\bh^{0} = [\mathbf{0}, \ldots, \mathbf{0}],
\end{gather}
and $\bm{\xi} \sim p(\bm{\xi})$ with a pre-defined prior distribution $p(\bm{\xi})$. Since $\tilde{\bh}^{n}$ is computed with $\bh^{n-1}$ and a \emph{noisy} latent vector $\bz^{n} + \bm{\xi}$, the model is trained to be robust to small errors and reduces the train-test discrepancy between the memory computed at training and inference. 

Summing up all of these components, our final training objective $\mathcal{L}(\bm{\theta})$ becomes:
\begin{align}
    \mathcal{L}(\bm{\theta}) \coloneqq \mathbb{E} 
    \Big[
    \lambda(t)|| D_{\bm{\theta}}(\bz_t^{n+1}, t;\, \tilde{\bh}^{n}, \bc) - \bz_0^{n+1}||_2^2 
    \Big],
\label{eq:obj}
\end{align}
with prior distributions $p(\bm{\epsilon}), p(n), p(\bm{\xi})$.

For $p(\bm{\epsilon})$, we use a progressively correlated Gaussian distribution proposed in \citet{ge2023preserve} to further mitigate error accumulation. Next, we set $p(\bm{\xi})$ to be a Gaussian $\mathcal{N}(\mathbf{0}$, $\sigma_{\mathrm{mem}}^2\mathbf{I})$ with small $\sigma_{\mathrm{mem}}>0$.

\vspace{0.02in}
\noindent\textbf{Inference.}
After training, we synthesize a long video by autoregressively generating one segment at a time. Specifically, we start from generating a first segment $\bz^{1}$ conditioned on $\bc$, and then iteratively generate $\bz^{n+1}$ for $n>0$ by computing memory $\bh^{n}$ and performing conditional generation from $\bh^{n}$ and $\bc$. We provide detailed pseudocode in Appendix~\ref{appen:sampling}.

\subsection{Architecture}
\label{subsec:arch}

\textbf{Autoencoder.}
Similar to the encoding scheme used in a recent latent video diffusion model, W.A.L.T \citep{gupta2023photorealistic}, we use a
causal 3D CNN encoder-decoder architecture for the video autoencoder based on the  
MAGVIT-2 tokenizer~\citep{yu2024language} without quantization (so that latent vectors lie in a continuous space). We train the autoencoder with a sum of pixel-level reconstruction loss (\eg, mean-squared error), perceptual loss (\eg, LPIPS~\citep{zhang2018perceptual}), and adversarial loss~\citep{goodfellow2014generative} similar to prior image and video generation methods. Recall that both training and inference are not done directly on long videos; they are done after splitting long videos into short segments.

\vspace{0.02in}
\noindent\textbf{Diffusion model.}
As outlined in Figure~\ref{fig:model}, our model architecture is based on the recent diffusion transformer (DiT) architecture~\citep{Peebles2022DiT, ma2024latte, yu2024representation}. 
Thus, given a latent vector $\bz^n \in \mathbb{R}^{l \times h \times w \times c}$ of a video clip $\bx^n$, we patchify it with patch size $p_l \times p_s \times p_s$ to form a flattened latent vector $\mathtt{patchify}(\bz^n) \in \mathbb{R}^{(lhw / p_lp_s^2) \times c}$ with a sequence length $lhw / p_lp_s^2$ and use it as inputs to the model. In particular, we choose W.A.L.T~\citep{gupta2023photorealistic} as backbone, a variant of DiT which employs efficient spatiotemporal windowed attention instead of full attention between large numbers of video patches.

To enable training with long videos with DiT architectures, we introduce a memory-augmented attention layer and insert this layer to the beginning of every Transformer block. Specifically, we design this layer as a cross-attention layer between the previous memory latent vector and the current hidden state, similar to memory-augmented attention~\citep{dai2019transformer} in the language domain. Hence, query, key, and value of a $d$-th memory layer with the segment $\bz^{n}$ and the memory latent vector $\bh^{n-1}$ become:
\begin{gather*}
    \text{query}\coloneqq \bh_{d}^{n},\,\,\text{key} \coloneqq [\bh_{d}^{n-1}, \bh_{d}^{n}],\,\,\text{value}\coloneqq [\bh_{d}^{n-1}, \bh_{d}^{n}], \\ \bh_{d}^{n-1}, \bh_{d}^{n} \in \mathbb{R}^{(hw / p_s^2) \times (l/p_l) \times c'},
\end{gather*}
where $c'$ denotes the hidden dimension of the model and $\bh_{d}^{n-1}, \bh_{d}^{n}$ are \emph{reshaped} as a sequence length $l/p_l$ and a batch dimension size $hw/p_s^2$, similar to previous space-time factorized attention~\citep{bertasius2021space}. 
We also use relative positional encodings that are widely used to handle longer context length.

With this formulation, memory-augmented attentions are only computed together with each of $l/p_l$ patches that have the same spatial location (\ie, temporal attention in video transformers~\citep{bertasius2021space}). This increase is not significant because the computation of attention is restricted only to the sample spatial locations. Our 
memory augmented attentions have $O(L^2HW)$ computational complexity whereas full attention would scale as $O((LHW)^2)$.

Finally, recall that we build our architecture on W.A.L.T, but our general approach of using memory-augmented latent transformers can be applied more broadly to any video diffusion transformer architectures, such as \citet{lu2023vdt} and \citet{ma2024latte}. We provide a detailed illustration of the architecture combined with W.A.L.T in Appendix~\ref{appen:archi}.