\section{Related Work}

We provide a brief discussion with important relevant literature. We provide more detailed discussion in Appendix~\ref{appen:related}. 

\vspace{0.02in}
\noindent\textbf{Video diffusion models.}
Inspired by the remarkable success of image diffusion models~\citep{rombach2021highresolution,chen2023pixart}, many
recent approaches have attempted to solve the challenging problem of video generation through diffusion models by extending ideas and architectures
from the image domain \citep{blattmann2023align,gupta2023photorealistic,ho2022video,harvey2022flexible,he2022lvdm,ge2023preserve,lu2023vdt,ma2024latte,ho2022imagen,singer2022make,voleti2022mcvd,weng2023art,yin2023nuwa,yu2023video,yu2024efficient,zhou2022magicvideo}. Since memory and computation requirements increase 
dramatically due to the cubic complexity of videos as RGB pixels, most works have focused on efficient methods to generate videos via diffusion models. One avenue to achieve efficiency has been to train compact latent representations specialized for videos and training latent diffusion models in such spaces~\citep{gupta2023photorealistic,he2022lvdm,yu2023video,yu2024efficient}. Other works have designed efficient 
model architectures to reduce computation in handling video data~\citep{gupta2023photorealistic,bar2024lumiere,yu2024efficient}. Remarkably, with these efforts, very recent works have shown diffusion models can generate high-resolution (1080p) and quite long (up to 1 minute) videos if they use massive number of videos as data with an extremely large model \citep{veo,videoworldsimulators2024}. Our method builds on both strands of prior work, leading to an
efficient architecture specialized for and trained on \emph{long} videos.

\vspace{0.02in}
\noindent\textbf{Long video generation.}
Many works have attempted to solve long video generation in different directions. First, there exist several works that interpret videos as compactly parameterized continuous functions of time (neural fields; \citep{sitzmann2020implicit}) and train GANs \citep{goodfellow2014generative} to generate such function parameters \citep{yu2022digan,skorokhodov2021stylegan}. These works have shown potential to generate arbitrarily long and smooth videos but are difficult to scale up with complex datasets due to the mode collapse problem of GANs~\citep{srivastava2017veegan}. Another line of work has proposed an autoregressive approach to generate frames conditioned on previous context~\citep{blattmann2023align,gupta2023photorealistic,yan2023temporally,weng2023art,ge2022long,villegas2023phenaki,yan2021videogpt,kondratyuk2023videopoet}. However, due to the high-dimensionality of videos, these approaches condition on a small temporal window of previous frames. They also often suffer from error propagation leading to low-quality frames for longer
temporal horizons~\citep{huang2023video}, particularly on complex datasets.
Several recent works have introduced training (or training-free) method to mitigate this issue \citep{ruhe2024rolling, kim2024fifo, chen2024diffusion, xie2024progressive,lu2024freelong}, but they still do not have long conteuxutal understanding capability.
Other methods have proposed hierarchical generation to progressively generate long videos by interpolating between previously synthesized frames~\citep{yin2023nuwa,huang2023video}. However, if the target video length is extremely long, they need to generate very sparse video frames with very little local correlation, which is challenging, and they are also limited to generating frames up to the training length. \sname can be categorized as a diffusion based
autoregressive approach; however, unlike the above approaches that condition on previously generated frames, we condition on memory, which is capable of capturing information going significantly further back in time, and is robust to error propagation when generating extremely long videos.

\vspace{0.02in}
\noindent\textbf{Transformers for handling long context.}
Recent works primarily from the large language models (LLMs) literature have proposed different ideas to handle extremely long contexts using transformers~\citep{vaswani2017attention}. 
First, several works have introduced efficient attention mechanisms~\citep{wang2020linformer,kitaev2020reformer,beltagy2020longformer,choromanski2020rethinking} that reduce their quadratic complexity and thereby can handle longer contexts better. Next, some approaches have attempted to adopt recurrence in transformers~\citep{dai2019transformer,bessonov2023recurrent,bulatov2022recurrent,bulatov2024beyond,hutchins2022block,peng2023rwkv}, showing great potential to understand long contexts, sometimes even demonstrating their effectiveness on extremely long sequences (1M) \citep{bulatov2024beyond}.
Some works introduce a method to cache previous keys and values from previous contexts (known as \emph{KV-caches})~\citep{wu2022memorizing,adnan2024keyformer,pope2023efficiently}, which are often combined with a relative positional encoding generalizable to a longer sequence than the training length~\citep{su2024roformer}. Finally, there exist some approaches that implement
efficient attention using hardware optimizations \citep{dao2022flashattention,liu2023ring}. Our method fits into this larger family of approaches by incorporating the recurrence technique into recent transformer-based diffusion
approaches~\citep{gupta2023photorealistic,Peebles2022DiT} to generate long sequences. %We believe that further exploring the connection between these other techniques and long video generation would likely be a fruitful direction.

