\section{Limitations and Negative Social Impact}
\label{appen:social_impact}


While \sname shows strong performance in long video generation benchmarks, it is as-yet tested only with a relatively small model.
Considering the scalability of diffusion transformer architectures in text-to-image~\citep{chen2023pixart} or text-to-video generation~\citep{gupta2023photorealistic}, we expect the performance of \sname to show a similar trend given more data and larger models.
Moreover, considering that our text-to-video model is trained on short videos (up to 37 frames), training \sname on large-scale text and long-video paired datasets is a natural next direction.
Finally, one can combine our approach with other techniques for sequence data, such as \citet{ruhe2024rolling}, or consider recent techniques in LLMs to further expand the context window and to leverage its usefulness~\citep{liu2023ring}.

\begin{figure}[ht!]
    \centering    
    \includegraphics[width=.4\textwidth]{assets/supple_figure_t2v_fail_1.pdf}
    \captionof{figure}{\textbf{Failure case} from \sname. We visualize the video frames with a stride of 5. Each video frame has a 128$\times$128 resolution. Text prompts used for generating videos are denoted in the below per each visualization. We visualize first 400 frames.} 
\label{fig:t2v_failure}
\end{figure}
\vspace{0.02in}
\noindent\textbf{Failure cases.}
For text-to-video generation, our model sometimes does not align with the text prompt: for instance, in Figure~\ref{fig:t2v_failure}, the generated video frames do not have ``ukiyo-e'' style, but they are rather photorealistic. Moreover, while \sname shows strong length generalization longer than the training length (\eg, 50 seconds consistent generation in Figure~\ref{fig:main_qual_t2v}), it sometimes fails to generalize and produce uncorrelated frames to the previous context, as shown in the last two frames in Figure~\ref{fig:t2v_failure}. We hypothesize this is because of the small model size and short video length that we used, and we strongly believe training a larger model on long video datasets can solve this issue, considering our superior performance on Kinetics-600 and UCF-101, which are directly trained on long videos.

\vspace{0.02in}
\noindent\textbf{Long-context understanding.}
While \sname shows superior performance in long video generation with efficiency, there might be better diffusion model architecture and formulation to handle long sequences better with large window sizes, and exploring this should be interesting future work. In particular, one may focus on an approach to model extremely long (\eg, 1, 1-hour movies) with diffusion models, which might be suboptimal with \sname because we always assume the fixed-size memory latent vector. One may consider borrowing ideas in recent LLMs into diffusion model contexts, like we have done in this paper, to apply recurrent mechanisms to diffusion models. 

%Our problem of interest, video generation, can provide various real-world applications, including content-creations for designers~\citep{villegas2023phenaki}, similar to the impact of recent text-to-image diffusion models~\citep{brooks2023instructpix2pix, meng2022sdedit}. At the same moment, one should be aware of the opposite aspect of video generation, as it may unexpectedly result in generating sensitive content (\eg, Deepfake~\citep{guera2018deepfake}), as well as generating videos that violate copyright To prevent this, one should first carefully curate video data for training not to include inappropriate contents, and similar to language domain, one should investigate a difference between generated data and real data and develop a detection framework~\citep{mitchell2023detectgpt}.
