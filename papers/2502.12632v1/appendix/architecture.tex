\section{Architecture Illustration}
\label{appen:archi}

Figure~\ref{fig:detailed_architecture} visualizes the \sname architecture in detail as applied to the existing W.A.L.T~\citep{gupta2023photorealistic} architecture. W.A.L.T is a variant of diffusion transformer (DiT)~\citep{Peebles2022DiT} that uses repeating spatiotemporal and spatial window attention layers instead of full attention. As illustrated in this figure, \sname adds memory attention layers at the beginning of each spatiotemporal window attention layer and spatial window attention layer. Here, window attention layers are similar to the original DiT that uses adaptive instance normalization (AdaIN), where we use LoRA~\cite{hu2021lora} for parametrization of MLP proposed by W.A.L.T to reduce the number of parameters without performance degradation. For memory attention layer, we simply add a cross attention layer without AdaIN.