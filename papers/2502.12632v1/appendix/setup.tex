\section{Setup}
\label{appen:setup}

\subsection{Datasets}
\label{appen:dataset}
\textbf{UCF-101.}
UCF-101 \citep{soomro2012ucf101} is a video dataset widely used for evaluation of video generation methods. Is consists of 101 classes of different human actions, where each video has 320$\times$240 resolution frames. The dataset includes 13,320 videos with 9,537 training split and 3,783 test split. Following the previous recent video generation literature we only use train split and test split for evaluation~\citep{yu2023magvit, singer2022make}. We center-cropped each video and resized it into $128\times128$ resolution. We use first 128 frames of videos for training and evaluation.

\vspace{0.02in}
\noindent\textbf{Kinetics-600.} Kinetics-600~\citep{kay2017kinetics} is a large-scale complex video dataset consisting of 600 action categories. It contains about 480,000 videos in total, where they are divided into 390,000, 30,000, and 60,000 videos for train, validation, and test splits (respectively). Following the previous video prediction benchmarks~\citep{gupta2023photorealistic,yan2023temporally,yu2023magvit}, we center-crop each video frame and resize each video frame as 128$\times$128 resolution. Due to the different tokenizer used in our method, we use 17 frames for condition (a bit different from TECO~\citep{yan2023temporally} that uses 20 frames for condition) and but predict 80 frames same as TECO. We use train split for train and validation split for evaluation.

\vspace{0.02in}
\textbf{T2V dataset.}
We use 89M text-short-video pairs (up to 37 frames) and 970M text-image pairs from public and internal sources. All datasets are center-cropped and resized to 128$\times$128 resolution. For videos, we sample the frames with a fps of 8.

\subsection{Hyperparameters}
We mostly follow the same hyperparameter setup to W.A.L.T~\citep{gupta2023photorealistic}. Specifically, we provide the detailed hyperparameter values in Table~\ref{tab:hyperparam}.

\begin{table}[!ht]
    \caption{Hyperparameter setup.}
    \centering
    \resizebox{!}{.6\linewidth}{%
    \begin{tabular}{l c c c}
        \toprule
         & {UCF-101} & {Kinetics-600} & T2V  \\
        \midrule
        \textbf{Autoencoder} \\
        Input dim. & 17$\times$128$\times$128$\times$3 & 17$\times$128$\times$128$\times$3 & 17$\times$128$\times$128$\times$3 \\
        $d_s$, $d_t$ & {8, 4} & {8, 4} & {8, 4} \\
        Channel dim. & 128 & 128 & 128 \\
        Channel multiplier & {1, 2, 2, 4} & {1, 2, 2, 4} & {1, 2, 2, 4} \\
        Training duration & 2,000 epochs & 270,000 steps & 1,000,000 steps \\ 
        Batch size & 256 & 256 & 256 \\
        lr scheduler & Cosine & Cosine & Cosine \\
        Optimizer & Adam & Adam & Adam \\
        \midrule
        \textbf{Diffusion model} \\
        Input dim. & 5$\times$16$\times$16 & 5$\times$16$\times$16 & 5$\times$16$\times$16 \\
        Num. layers & 28 & 24 & 24 \\
        Hidden dim. & 1,152 & 1,024 & 1,024 \\
        Num. heads & 16 & 16 & 16 \\ 
        $\sigma_{\mathrm{mem}}$ & 0.1 & 0.1 & 0.1 \\
        $N$ & 7 & 5 & 2 \\
        Training duration & 120,000 steps & 270,000 steps & 700,000 steps \\ 
        Batch size & 256 & 256 & 256 \\ 
        lr scheduler & Cosine & Cosine & Cosine \\
        Optimizer & AdamW & AdamW & AdamW \\
        lr & 0.0005 & 0.0005 &  0.0002 \\
        \midrule
        \textbf{Diffusion} \\
        Diffusion steps & 1000 & 1000 & 1000 \\
        Noise schedule & Linear & Linear & Linear  \\
        $\beta_0$ & 0.0001 & 0.0001 & 0.0001 \\
        $\beta_T$ & 0.02 & 0.02 & 0.02 \\
        Training objective & v-prediction & v-prediction & v-prediction \\
        Sampler & DDIM~\citep{song2021denoising} & DDIM~\citep{song2021denoising} & DDIM~\citep{song2021denoising} \\
        Sampling steps & 50 & 50 & 50 \\
        Guidance & - & - & \checkmark \\
        \bottomrule
         & 
    \end{tabular}
    }
    \label{tab:hyperparam}
    \vspace{-0.2in}
\end{table}

\subsection{Metrics}
\label{appen:metrics}
For Fr\'echet video distance (FVD; \citet{unterthiner2018towards}), we use the same protocol used in previous popular video generation works~\citep{yu2022digan, tulyakov2018mocogan}; we use a pretrained I3D network~\citep{tran2015learning} to compute feature for evaluating statistics to compute FVD. For UCF-101, we use 2,048 real and fame samples following the common practice in previous video generation literature~\citep{yu2022digan, skorokhodov2021adversarial,yu2023video}. For Kinetics-600, we use 256 samples for FVD evaluation and average the values of 4 runs, following the setup in TECO~\citep{yan2023temporally}. For other metrics (PSNR, SSIM, LPIPS) used in evaluation on Kinetics-600, we also follow the evaluation setup of TECO: we compute them in per-frame manner between predicted frames and ground-truth frames, and then average them.

\subsection{Training resources}
All experiments are conducted with Google Cloud TPU v5e 16$\times$16 instances, where each chip has a 16GB HBM2 capacity. Note that training can be done on devices with relative small memory (less than 16GB) because of our efficient training scheme design.

\label{appen:hyper}
