\section{More Discussion with Related Work}
\label{appen:related}

\textbf{Diffusion model for sequential data.}
We emphasize that although we primarily explore video data in this paper, our method is not limited to video data. There exists multiple works that try to generate general sequential data (\eg, climate data, PDE data, audio, \emph{etc}.) through diffusion models. For instance, \citet{ruhe2024rolling} formulates a diffusion process specialized for sequential data; they propose denoising process with a \emph{sliding window}, where the noise scale of elements within a window is set differently by considering uncertainty of each element. Another line of work~\citep{ge2023preserve,lu2024improve} have tried to solve this problem by exploring temporal correlation of Gaussian noise in diffusion process with \emph{fixed-size} sequences. \citet{lippe2023pderefiner} tries to solve PDE through diffusion model based on autoregressive rollout, and demonstrates high-frequency details can be predicted better than existing PDE solvers. Our method primarily focuses on generating video data, where each element (\ie, video frame) is already quite high-dimensional and thus mitigating error propagation and ensuring large context window size is more difficult than other temporal data. Therefore, we believe our method can be extending to other forms of sequential data listed above. 

\vspace{0.02in}
\noindent\textbf{Video generation.}
There are many works to tackle the problem of video generation. First, there are approaches that uses generative adversarial network (GAN;~\citet{goodfellow2014generative}) for modeling video distribution \citep{tulyakov2018mocogan,yu2022digan,skorokhodov2021stylegan,tian2021good,acharya2018towards,clark2019adversarial,fox2021stylevideogan,gordon2021latent,kahembwe2020lower,munoz2021temporal,saito2017temporal,vondrick2016generating,yushchenko2019markov}. They usually extend popular image GAN architectures (\eg, StyleGAN~\citep{karras2020analyzing}) by considering an additional temporal axis. Another line of works encode videos in discrete token space using popular vector-quantized autoencoders~\citep{yu2024language,van2017neural,yu2022vectorquantized} and learn video distribution in this latent space, either with autoregressive transformers \citep{yu2024language,ge2022long,yan2021videogpt,kalchbrenner2017video,rakhimov2020latent,weissenborn2020scaling} or masked generative transformers \citep{yoo2023towards,yu2023magvit}. Finally, many of recent works propose diffusion-based approach for generating videos~\citep{ho2022video,harvey2022flexible,lu2023vdt,singer2022make,weng2023art,hoppe2022diffusion,yang2022diffusion}, and some of them have attempt to exploit knowledge learned from large-scale image datasets \citep{blattmann2023align,he2022lvdm,ge2023preserve,ho2022imagen,singer2022make,wang2023videofactory,an2023latent} by fine-tuning image diffusion models or do image-video joint training. Our model is also categorized as diffusion-based approach to synthesize long videos.

\vspace{0.02in}
\noindent\textbf{Video prediction.}
As our model has a capability of both video generation and prediction, it also has a close relationship to existing video prediction methods \citep{srivastava2015unsupervised,finn2016unsupervised,denton2017unsupervised,babaeizadeh2018stochastic,denton2018stochastic,lee2018stochastic,villegas2019high,kumar2020videoflow,franceschi2020stochastic,luc2020transformation,lee2021revisiting,seo2022autoregressive}. These methods usually use given frames as condition to the model and generate future frames via popular deep generative models, such as GANs, diffusion models, and autoregressive  long video prediction~\citep{harvey2022flexible,yan2023temporally}, in contrast to previous works that usually focus on predicting a few video frames. Our method also shares a similar goal to predict video frames with a long horizon.

\twocolumn[{
\centering
    \includegraphics[width=.84\linewidth]{assets/detailed_architecture.pdf}
    \captionof{figure}{\textbf{Detailed architecture illustration} of \sname if it is incoporated with W.A.L.T~\citep{gupta2023photorealistic}. 
    } 
    \label{fig:detailed_architecture}
    \vspace{0.2in}
}]
