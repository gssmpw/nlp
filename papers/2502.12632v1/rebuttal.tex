\documentclass[10pt,twocolumn,letterpaper]{article}
\usepackage[rebuttal]{cvpr}

% Include other packages here, before hyperref.
% Include other packages here, before hyperref.
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{xcolor}

\usepackage{adjustbox}
\usepackage{wrapfig}
\usepackage{colortbl}
\usepackage{nicefrac}

\newcommand{\ms}[1]{\tiny{$\pm$#1}}
% Import additional packages in the preamble file, before hyperref
\input{preamble}

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\definecolor{cvprblue}{rgb}{0.21,0.49,0.74}
\usepackage[pagebackref,breaklinks,colorlinks,allcolors=cvprblue]{hyperref}

% If you wish to avoid re-using figure, table, and equation numbers from
% the main paper, please uncomment the following and change the numbers
% appropriately.
%\setcounter{figure}{2}
%\setcounter{table}{1}
%\setcounter{equation}{2}

% If you wish to avoid re-using reference numbers from the main paper,
% please uncomment the following and change the counter value to the
% number of references you have in the main paper (here, 100).
%\makeatletter
%\apptocmd{\thebibliography}{\global\c@NAT@ctr 100\relax}{}{}
%\makeatother

%%%%%%%%% PAPER ID  - PLEASE UPDATE
\def\paperID{2384} % *** Enter the Paper ID here
\def\confName{CVPR}
\def\confYear{2025}

\definecolor{blizzardblue}{rgb}{0.73, 0.96, 0.99}
\definecolor{deepgreen}{rgb}{0.0, 0.6, 0.2}
\definecolor{pinegreen}{rgb}{0.0, 0.47, 0.44}
\definecolor{cornellred}{rgb}{0.7, 0.11, 0.11}
\definecolor{cadmiumgreen}{rgb}{0.0, 0.42, 0.24}
\definecolor{spirodiscoball}{rgb}{0.06, 0.75, 0.99}

\newcommand{\one}{\textcolor{orange}{\textbf{abPW}}}
\newcommand{\two}{\textcolor{deepgreen}{\textbf{QdjH}}}
\newcommand{\three}{\textcolor{blue}{\textbf{k6dE}}}

\hypersetup{
  linkcolor = cornellred,
  citecolor  = cadmiumgreen,
  colorlinks = true,
}

\newcommand{\sh}{\color{magenta}}
\newcommand{\jon}{\color{magenta}}



\begin{document}

%%%%%%%%% TITLE - PLEASE UPDATE
\title{MALT Diffusion: Memory-Augmented Latent Transformers for Any-Length Video Generation}  % **** Enter the paper title here
\maketitle
\thispagestyle{empty}

\noindent We thank all the reviewers for reviewing our manuscript and pointing out the positive aspects: it is well-written (\one, \three), presents an effective (\textbf{All}) and novel method (\one, \two), targets an interesting problem (\three). Below we denote major and minor concerns as ``M'' and ``m'', respectively, and we write diffusion model as DM.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\vspace{0.02in}
\noindent
\textbf{[\one-M1] Evaluation on shorter videos: degradation?}
To show our modules do not downgrade quality or motion, we evaluate MALT for short video prediction. Due to  limited resources, we conduct experiments on Kinetics following the setup in [1]. MALT achieves FVD=5.8, which is comparable with recent video DMs like [1] (FVD=5.2). 

\vspace{0.02in}
\noindent
\textbf{[\one-M2, \three-M5] T2V evaluation (\eg, human evaluation, video-text alignment, VBench).}
\emph{The intent of our work is not to go head to head against the latest T2V models},
which  among other things would require a significantly larger model size, and significantly higher
dataset scale/quality (c.f. Footnote 2, L510) and training at higher resolutions.  These are all issues orthogonal to long term stability which we isolate as the focus of our submission.  Moreover, even if we were to train our long term model in such a scaled up regime, current evaluation metrics like text-video alignment or Vbench focus on relatively short clip fidelity and have not been proven as reliable metrics in a long video regime.
Thus several long video generation approaches [3] have provided only qualitative comparisons on T2V and have done their quantitative evaluation on popular long video benchmarks (\eg, UCF-101) --- we have followed this approach.
We believe that our T2V results are a good proof-of-concept to show what might be possible, but we are happy to remove them if the reviewers insist, as we believe that the benefits of our method are adequately shown in the UCF-101 and Kinetics experiments and are done at scales that are reproducible in academic labs.

% We remark that our goal is \emph{not} to train a better T2V model; we explore a DM training scheme scalable to extremely long videos to achieve long-term stability and contextual understanding. Evaluation metrics like text-video alignment or Vbench that usually focus on relatively short clip fidelity are \emph{not} suitable to evaluate these two goals investigated in our paper. Moreover, as mentioned in footnote 2 (L510), T2V generation quality is significantly affected by other design choices, like model size and dataset scale/quality. 
% Thus, several long video generation approaches [3] have provided only qualitative comparison on T2V and done quantitative evaluation on popular long video benchmarks (\eg, UCF-101).
% Similarly, we performed extensive qualitative and quantitative analyses on popular UCF-101 and Kinetics, and we showed the potential of MALT for training large-scale T2V models by qualitatively showing the long-term stability of generated results.
% {Thus, We believe that our T2V results are a good proof-of-concept to show what might be possible, but we are happy to remove them if the reviewers insist, as we believe that the benefits of our method are adequately shown in the UCF-101 and Kinetics experiments and are done at scales that are reproducible in academic labs.}

\vspace{0.02in}
\noindent
\textbf{[\one-m1, \two-M2] Clarifications for equations.}
$\mathrm{HiddenState}$ in Eq. (6) and (8) indicates a sequence of hidden states from transformer blocks. Also, we will add labels to all equations and will use $:=$ only with new definitions. 


\vspace{0.02in}
\noindent
\textbf{[\one-m2] Add FreeLong.}
Thank you! We will add discussion and comparison with FreeLong in the revision.
%Thanks for the suggestion. We will incorporate it to the manuscript with qualitative comparison in the experiment section.

\vspace{0.02in}
\noindent
\textbf{[\two-M1] Architecture clarity.}
Thanks for the suggestion. In the revision, we will emphasize our new technical contributions (segment-wise diffusion, memory vector design) by separating them as an independent subsection. 

\vspace{0.02in}
\noindent
\textbf{[\two-M3] Missing ablation study on memory design.}
We \emph{have already} performed such an experiment and reported the result in Table 2. First, by comparing with the ``Last only'' model that uses only the last segment as a condition, we show providing longer context through memory improves generation. Moreover, by comparing with the ``kv-cache'' model, a conditional model that provides the entire context without compression, we show the efficiency of our compact design through the recurrent mechanism. 

% \vspace{0.02in}
% \noindent
% \textbf{[\two-M4] Model Complexity and Efficiency.}
% Regarding running time, naive DiTs scale quadratically
% in the length of the video but MALT scales linearly.  Regarding 
% memory requirements, MALT requires a fixed amount of memory
% while naive DiT or WALT memory requirements grow with the 
% length of the video and thus have a maximum length they
% can process. We will add it in the revision.

\vspace{0.02in}
\noindent
\textbf{[\two-M4, \three-M1] Training complexity and model efficiency.}
While MALT needs to wait for the computation of the previous segment; it is not slower yet rather scales more efficiently. Specifically, since we aim at \emph{direct training} of DiTs on long videos, complexity should be compared with a training scheme that feeds entire long videos at once. Assuming the video length is $NL$ with $N$ segments of length $L$, MALT requires $N$ forward operations where each operation at the $n$-th segment requires fixed $O(L^2)$ complexity (due to our memory design), resulting in $O(NL^2)$ complexity in total, where the latter one scales with $O(N^2L^2)$. More importantly, \emph{MALT requires a fixed amount of memory} while naive DiT memory requirements grow with the video length and thus have a maximum length they can process. We will add this in the revision.


\vspace{0.02in}
\noindent
\textbf{[\three-M2] Confusion: $\mathbf{z}_0$ in training objective (Eq. (7)).}
Thanks for pointing this out. As you mentioned, $\mathbf{z}_0$ should be $\mathbf{z}_0^{n+1}$. Specifically, we train a model to predict the next segment $\mathbf{z}_0^{n+1}$ based on the memory $\mathbf{h}^{n}$ that amortizes previous $n$ segments. We will fix related typos in the revision.

\vspace{0.02in}
\noindent
\textbf{[\three-M3] On using pre-trained video VAEs.}
We retrained video tokenizers on UCF-101 and Kinetics for a fair comparison with baselines that have trained their video VAEs from scratch. For T2V experiments, this is a good point; using pretrained video VAEs would be a promising and interesting approach to reduce the training cost.


\vspace{0.02in}
\noindent
\textbf{[\three-M4] 128$\times$128 resolution is not convincing.}
While there are several large-scale T2V models for generating high-resolution videos, we politely disagree that training on 128$\times$128 resolution is not convincing. Due to the tremendous computing resources to train DMs on \emph{long videos},  128$\times$128 resolution is still widely used in academic research. For instance, on UCF-101 and Kinetics, recent studies [1, 2] also have used $128\times128$ resolution and we strictly follow the same setup. For T2V generation, we follow the base model training protocol in W.A.L.T [2] that trains the model on 128$\times$128 resolution. Thus, similar to W.A.L.T, we believe that our model can be easily extended to generate higher-resolution videos by fine-tuning this base model on higher-resolution frames [2] or training additional upsampling modules [4] and we leave it for a future work.

\vspace{0.02in}
\noindent
\textbf{[\three-m1] Issue in L135.} We will fix in the revision. 
\maketitle
\thispagestyle{empty}

%%%%%%%%% REFERENCES
\vspace{-0.04in}
\noindent
-----------------------------------------------------------------------
\vspace{-0.02in}

{
\small
\vspace{-0.02in}
\noindent
[1] Rolling Diffusion Models, 2024

\noindent
[2] Photorealistic Video Generation with Diffusion Models, 2024

\noindent
[3] FIFO-Diffusion: Generating Infinite Videos from Text without Training, 2024

\noindent
[4] A Noise Prior for Video Diffusion Models, 2023
}


\end{document}
