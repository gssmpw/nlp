% Template for ISBI paper; to be used with:
%          spconf.sty  - ICASSP/ICIP LaTeX style file, and
%          IEEEbib.bst - IEEE bibliography style file.
% --------------------------------------------------------------------------
\documentclass{article}
\usepackage{spconf,amsmath,graphicx}

% It's fine to compress itemized lists if you used them in the
% manuscript
\usepackage{enumitem}
\usepackage{amsmath,graphicx}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{color}
\usepackage{url}
\usepackage{caption}
\usepackage{multirow}
\usepackage{textcomp}
\usepackage{booktabs}
\usepackage{multicol}
\usepackage{algorithm2e}
\usepackage{colortbl} % in the preamble
\usepackage{todonotes}
 \usepackage[nolist,nohyperlinks]{acronym}
\newcommand{\en}{\enspace} 
\newcommand{\bm}[1]{ \mathbf #1} 
%\newcommand{\it}[1]{\textit} 
% Example definitions.
% --------------------
\def\x{{\mathbf x}}
\def\L{{\cal L}}
\newcommand{\loss}{\ensuremath{\mathcal{L}}}
\newcommand{\lbce}{\ensuremath{\loss_\text{BCE}}\xspace}
\newcommand{\liou}{\ensuremath{\loss_\text{IoU}}\xspace}
\newcommand{\lDUAL}{\ensuremath{\loss_\text{DUAL}}\xspace}
\newcommand{\glmsrf}{\ensuremath{\loss_\text{GMSRF}}\xspace}
\usepackage[acronym]{glossaries}
\acrodef{mIoU}{mean intersection over union}
\acrodef{DSC}{dice coefficient}
\acrodef{CT}{computed tomography}

\setlist{nosep, leftmargin=14pt}

\usepackage{mwe} % to get dummy images

% Example definitions.
% --------------------
\def\x{{\mathbf x}}
\def\L{{\cal L}}

% Title.
% ------
\title{Long Range Sequential Modeling Necessary for Colorectal Tumor Segmentation?}
%
% Single address.
% ---------------
\name{Abhishek Srivastava, Koushik Biswas, Ulas Bagci}
\address{Author Affiliation(s)}
%
% For example:
% ------------
%\address{School\\
%	Department\\
%	Address}
%
% Two addresses (uncomment and modify for two-address case).
% ----------------------------------------------------------
%\twoauthors
%  {A. Author-one, B. Author-two\sthanks{Some author footnote.}}
%	{School A-B\\
%	Department A-B\\
%	Address A-B}
%  {C. Author-three, D. Author-four\sthanks{The fourth author performed the work
%	while at ...}}
%	{School C-D\\
%	Department C-D\\
%	Address C-D}
%
% More than two addresses
% -----------------------
% \name{Author Name$^{\star \dagger}$ \qquad Author Name$^{\star}$ \qquad Author Name$^{\dagger}$}
%
% \address{$^{\star}$ Affiliation Number One \\
%     $^{\dagger}$}Affiliation Number Two
%
\begin{document}
%\ninept
%



\maketitle
\def\x{{\mathbf x}}
\def\L{{\cal L}}
%
\begin{abstract}
3D medical image segmentation is pivotal in computer-aided diagnosis, especially for accurately segmenting tumors, which aids in diagnosis, treatment planning, and monitoring disease progression. Manual segmentation, while precise, is time-intensive and impractical for high-volume or urgent clinical environments. Automated segmentation reduces subjective variability and supports consistent analysis of colorectal tumors, which is critical for early-stage detection, significantly enhancing survival rates. Advances in deep learning, particularly with CNNs and Transformers, have significantly improved 3D segmentation accuracy by capturing both local and long-range dependencies. Notable architectures like Lkd-net, UX-Net, and MSRF-Net have extended receptive fields, while Transformer-based models, including UNETR and SwinUNETR, have demonstrated enhanced global context modeling. However, traditional self-attention mechanisms encounter computational limitations, motivating the development of Mamba, which uses state space models to efficiently model long-range dependencies. In this work, we introduce MambaOutUnet and a new colorectal cancer segmentation dataset (CRC204) to systematically compare these local and global token interaction models. Our findings suggest that robust local token interactions can outperform many long-range modeling techniques, proposing a potential shift in 3D tumor segmentation research.
\end{abstract}
%
\begin{IEEEkeywords}
Deep learning, generalization, multi-scale feature fusion, colonoscopy
% segmentation, polyp segmentation, multi-scale fusion, attention, generalization
\end{IEEEkeywords}
\section{Introduction}
\label{sec:intro}
3D medical image segmentation is a crucial component in computer-aided diagnosis, as precise segmentation can significantly reduce the diagnostic workload for clinicians across various conditions. Within this domain, tumor segmentation in medical imaging is critical to healthcare as a requisite for diagnosis, treatment planning, and monitoring tumor progression. Accurate delineation of tumors within 3D medical images facilitates clinicians' detailed insights into tumor morphology, assesses growth or response to therapy, and even guides surgical or radiotherapeutic interventions~\cite{minaee2021image,clark2013cancer}. While manual segmentation is precise, it is time-consuming and requires extensive domain expertise, making it unfeasible in settings with high image volumes or in time-sensitive clinical environments. 
CRC imaging data, particularly from modalities such as computed tomography (CT) and magnetic resonance imaging (MRI), often contain complex anatomical details that complicate manual segmentation. Automated segmentation, by removing subjective variability and reducing time demands, supports consistent and reproducible analysis of colorectal tumors, enabling more reliable data for clinical decisions. Early-stage detection of CRC has been shown to significantly increase survival rates, yet CRC is often diagnosed at later stages due to its subtle and variable manifestations in imaging~\cite{yao2022deepcrc}.

To address these limitations, automated segmentation methods have been developed, demonstrating substantial progress due to advancements in deep learning. Convolutional neural networks (CNNs) and, more recently, Transformers, have proven to be effective for 3D image segmentation~\cite{ronneberger2015u}, with CNNs capturing local features efficiently and Transformers modeling long-range dependencies~\cite{dosovitskiy2020image}. These methods have enabled faster, more accurate tumor delineation, improving decision-making and ultimately enhancing patient outcomes. 

Enhancing the model's receptive field within the 3D space is one of the key components to improve model segmentation performance. Lkd-net~\cite{luo2023lkd} modeled a wider range of features using their large-kernel convolution layer. Similarly, the lightweight 3D UX-Net~\cite{lee20223d} utilized kernel sizes starting from (\(7 \times 7 \times 7\)) to achieve large receptive fields. MSRF-Net~\cite{srivastava2021msrf} obtained diverse feature maps combining different receptive fields utilizing their multi-scale feature fusion block. However, modeling global relationships is challenging for CNNs, owing to their inherent inductive bias and their hierarchical nature makes modeling relationships between distant tokens inefficient. However, when the vision transformer~\cite{dosovitskiy2020image} was conceptualized, it drew attention from the community due to its inherent ability to model global token interactions~\cite{xing2022nestedformer,xing2024hybrid,yang2024vivim}. UNETR~\cite{hatamizadeh2022unetr} used ViT in their encoder to leverage its advantage for medical image segmentation, while, SwinUNETR [5] utilizes the Swin Transformer [14] to extract multi-scale features, enhancing segmentation accuracy~\cite{hatamizadeh2021swin}. However, the self-attention mechanism~\cite{vaswani2017attention} suffered from quadratic complexity bottleneck which resulted in significant computational demands. Resolution to this issue came to light when Mamba~\cite{gu2023mamba}, derived from state space models~\cite{kalman1960}, came into existence. Mamba can model long-range dependencies by incorporating a selection mechanism and a hardware-optimized algorithm. U-Mamba~\cite{ma2024u} integrates Mamba into nn-UNet~\cite{isensee2021nnu}, Vision Mamba~\cite{zhu2024vision} included bidirectional SSMs in a Vim block for improved global visual context modeling. SegMamba~\cite{xing2024segmamba} proposed a tri-oriented Mamba (ToM) module for enhanced 3D feature modeling.
However, trending research in the direction of increasing long-range modeling capacities for 3D medical image segmentation might be useful but still has limitations in some use cases. MambaOut~\cite{yu2024mambaout} studies these limitations comprehensively on different computer vision tasks. In this paper, we provide a comprehensive comparison of these different local- and global token modeling mechanisms on our newly proposed Colorectal Cancer Segmentation 204 (CRC204) dataset and instantiate a new methodology MambaOutUnet to provide strong evidence for the hypothesis introduced in~\cite{yu2024mambaout}. Additionally, we pose the community the following questions 1.) Is long-range modeling necessary for tumor segmentation? 2.) Should we focus on devising methodologies with enhanced capacity to model local token interactions for more robust 3D tumor segmentation performance? 

We also proposed the UNet variations of State-of-the-art mamba-based architectures in this paper, and along with current 3D segmentation architectures in the literature, we provide a comprehensive analysis of these methods on our CRC204 dataset. The performance of MambaOutUnet whose encoder is boosted by gated convolutional layers~\cite{dauphin2017language}
shows that channel mixing is only required to beat most of the existing computationally heavy long-range modeling techniques, and provides a new perspective on current 3D medical image segmentation research.
\iffalse
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure*}[!t]
    \centering
    \includegraphics[height=0.32\textwidth]{Figures/GMSRF_FULL_new.jpeg}
    \caption{The proposed GMSRF-Net architecture, a) The GMSRF-Net architecture (left), b) The GMSRF module (right)}
    \label{fig:GMSRF-Net}
\end{figure*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\fi
%\section{Materials and method}
\begin{table*}[!t]
\centering
\footnotesize
\setlength{\tabcolsep}{18pt}
\renewcommand{\arraystretch}{1.1}
\caption{Quantitative Comparison on the CTS 204 and Synapse multi-organ CT dataset (BTCV). We report the Dice Score (DSC), Mean Intersection over Union (mIoU), and Normalized Surface Distance (NSD)}
\begin{tabular}{@{}l|ccc|ccc@{}}
\toprule
\multirow{2}{*}{\textbf{Method}} & \multicolumn{3}{c|}{\textbf{CTS 204}} & \multicolumn{3}{c}{\textbf{BTCV}} \\
 & \textbf{DSC} & \textbf{mIoU} & \textbf{NSD} & \textbf{DSC} & \textbf{mIoU} & \textbf{NSD} \\ 
\hline
UNet & \textcolor{magenta}{0.5007} & \textcolor{magenta}{0.3839} & \textcolor{magenta}{0.6116} & \textcolor{magenta}{0.8217} & \textcolor{magenta}{0.6982} & \textcolor{magenta}{0.9320} \\ \hline
NNUNet & 0.4842 & 0.3619 & 0.6073 & 0.7775 & 0.6351 & 0.9244 \\ \hline
SwinUnet & 0.4602 & 0.3388 & 0.5060 & 0.8023 & 0.6718 & 0.8982 \\ \hline
VisMix & 0.3612 & 0.2566 & 0.4663 & 0.7436 & 0.5906 & 0.8025 \\ \hline
SegMamba & 0.3990 & 0.2982 & 0.5256 & 0.8184 & 0.7124 & 0.9158 \\ \hline
SegHydra & 0.3687 & 0.2728 & 0.5972 & 0.7917 & 0.6803 & 0.8823 \\ \hline
MambaOutUNet & \textbf{0.5203} & \textbf{0.3950} & \textbf{0.6139} & \textbf{0.8338} & \textbf{0.7345} & \textbf{0.9397} \\ 
\bottomrule
\end{tabular}
\label{tab:result_combined}
\end{table*}
\iffalse
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{table*}[!t]
\centering
\footnotesize
\setlength{\tabcolsep}{400pt}
\caption{Quantitative Comparison on the CTS 204 and Synapse multi-organ CT dataset (BTCV). We report the Dice Score (DSC), Mean Intersection over Union (mIoU), and Normalized Surface Distance (NSD)}
%\begin{tabular}{@{}l|ccccc|ccccc@{}}
\begin{tabular}{@{}l|p{2cm}p{2cm}p{2cm}|p{2cm}p{2cm}p{2cm}@{}}
\toprule
\multirow{2}{*}{\textbf{Method}} & \multicolumn{5}{c|}{\textbf{CTS 204}} & \multicolumn{5}{c}{\textbf{BTCV}} \\
 & \textbf{DSC} & \textbf{mIoU} & \textbf{NSD} & \textbf{ASD} & \textbf{HD95} & \textbf{DSC} & \textbf{mIoU} & \textbf{NSD} & \textbf{ASD} & \textbf{HD95} \\ 
\hline
UNet~\cite{ronneberger2015u} & \textcolor{magenta}{0.5007} & \textcolor{magenta}{0.3839} & \textcolor{magenta}{0.6116} & 37.2800 & 59.5921 & 0.8217 & 0.6982 & 0.9320 & 2.9506 & 39.6404 \\ \hline
NNUNet~\cite{isensee2021nnu} & 0.4842 & 0.3619 & 0.6073 & 40.6262 & 57.1466 & 0.7775 & 0.6351 & 0.9244 & 3.4849 & 33.6257 \\ \hline
SwinUnet~\cite{hatamizadeh2021swin} & 0.4602 & 0.3388 & 0.5060 & 63.8752 & 172.4032 & 0.8023 & 0.6718 & 0.8982 & 2.4188 & 62.4951 \\ \hline
MambaSwinUnet & 0.3612 & 0.2566 & 0.4663 & 59.3459 & 74.2021 & 0.7436 & 0.5906 & 0.8025 & 11.8268 & 107.3043 \\ \hline
SegMamba~\cite{xing2024segmamba} & 0.3990 & 0.2982 & 0.5256 & 47.7523 & 66.1386 & 0.8184 & 0.7124 & 0.9158 & 1.39601 & 48.4698 \\ \hline
SegHydra & 0.3687 & 0.2728 & 0.5972 & 14.1302 & 39.6611 & 0.7917 & 0.6803 & 0.8823 & 3.1242 & 74.2332 \\ \hline
MambaOutUNet & \textbf{0.5203} & \textbf{0.3950} & \textbf{0.6139} & 49.3902 & \textbf{83.5807} & 0.8338 & 0.7345 & 0.9397 & 0.9391 & 24.9705 \\ 
\bottomrule
\end{tabular}
\label{tab:result_combined}
\end{table*}
\fi

\begin{figure*}[!t]
    \centering
    \includegraphics[width=0.9\textwidth]{1.png}
    \caption{Qualitative comparison of MambaOutUNet with other established and proposed baselines.}
    \label{fig:qualitative}
    \vspace{-5mm}
\end{figure*}
\section{Dataset}
We have publicly released a new colorectal tumor segmentation dataset termed "CTS-204". The dataset was collected from Marmara University Pendik Research and Education Hospital in Turkey and includes CT scans from 204 patients undergoing treatment for colon cancer. All sensitive patient information has been de-identified. Two experienced radiation oncologists annotated each volume in consensus. All the CT scans were acquired with tube voltage, 120 Kv, pitch, 0.85-1.2, Z-axis spacing from 3.0 mm to 3.75 mm, with a median of 3.75 mm. Additionally, to validate the performance of our MambaOut method and our hypothesis that long-range sequence modeling still useful for segmentation problems where global token modeling is informative we include \textit{Synapse multi-organ segmentation dataset}\footnote{https://www.synapse.org/\#!Synapse:syn3193805/wiki/217789}.Experiments on the Synapse multi-organ segmentation dataset followed the same training-testing splits as used in~\cite{chen2021transunet}.
\section{Methods}
In this section, we describe the architecture of our three proposed baselines: Mamba-SwinUnet,SegHydra, and MambaOutUnet. Assessing the impact of long range sequence modeling through Mamba requires a fair comparison with SegMamba~\cite{xing2024segmamba}. We ensure this by following the same architecture as proposed in SegMamba~\cite{xing2024segmamba} while invoking necessary architectural changes in the \textbf{encoder} for each proposed technique. The architecture of Mamba-SwinUnet,SegHydra, and MambaOutUnet along with the Tri-oriented spatial Mamba (TSMamba) block and the convolutional layer incorporated 3D decoder will be introduced in the following section. 
%For a fair comparison with SegMamba~\cite{xing2024segmamba}, we use the thier  1.) tri-orientated spatial Mamba (TSMamba) blocks for modeling global
%information at different scales and 2) a 3D decoder based on the convolution layer for predicting segmentation results. The encoder is changed with respect to the mamba variant being incorporated. In the following section we will describe the Tri-oriented spatial mamba block, the decoder, followed by how each architecture influences the encoder.
\subsection{Encoder}
The 3D input volume \( CT \in \mathbb{R}^{C \times D \times H \times W} \) is initially passed through the encoder stem comprising a depth-wise convolutional layer with a kernel size of \(7 \times 7 \times 7\), padding of \(3 \times 3 \times 3\), and stride of \(2 \times 2 \times 2\). The resultant \( x_0 \in \mathbb{R}^{48 \times \frac{D}{2} \times \frac{H}{2} \times \frac{W}{2}} \) tensor is sent to the subsequent TSMamba Blocks and downsampling layers.
\subsubsection{ Tri-orientated Spatial Mamba (TSMamba) Block}
TSMamba blocks facilitate global token modeling through Mamba and their hierarchal nature in the UNet allows modeling of multi-scale features. Global token modeling and capturing multi-scale features are conducive to obtaining precise segmentation maps. Specifically, the input feature in $lth$ layer of TSMamba block are first passed through the Gated Spatial Convolutional module. This module consists of two covolutional blocks each consisting of normalization, convolution and non-linear activation layer. The two convolutional layers have kernel size of \(3 \times 3 \times 3\) and \(1 \times 1 \times 1\), respectively. The resultant outputs undergo a element wise multiplication so as to regulate the amount of informative spatial features and suppress irrelevant features, similar to a geted attention mechanism(see Equation~\ref{eq:gsc_def}, where C is the convolutional layers and x are the input feature maps)
\begin{equation}
    \text{GSC}(x) = x + C_{3 \times 3 \times 3} \left( C_{3 \times 3 \times 3}(x) \cdot C_{1 \times 1 \times 1}(x) \right), \label{eq:gsc_def}
\end{equation}
Since Mamba layers performs token-to-token modeling in one direction the input 3D features are flattened into three different sequences corresponding to the axial, coronal, and sagittal plane(see Equation~\ref{eq:tri_orientated_mamba}).
\begin{equation}
\text{ToM}(x) = \text{Mamba}(x_{\text{axial}}) + \text{Mamba}(x_{\text{coronal}}) + \text{Mamba}(x_{\text{sagittal}})
\label{eq:tri_orientated_mamba}
\end{equation}
Subsequently, a multi-layer perceptron (MLP) is used to perform channel mixing and layer normalization is used to further model feature representations. The information flow can be summarized as:
\begin{align}
    \hat{x}^l &= \text{GSC}(x^l), \label{eq:gsc} \\
    \tilde{x}^l &= \text{ToM} \left( \text{LN} \left( \hat{x}^l \right) \right) + \hat{x}^l, \label{eq:tom} \\
    x^{l+1} &= \text{MLP} \left( \text{LN} \left( \tilde{x}^l \right) \right) + \tilde{x}^l, \label{eq:mlp}
\end{align}
\subsection{Decoder and Feature-level Uncertainty Estimation (FUE)}
The encoder generates multi-scale features, which carry uncertainty information \cite{zhao2023uncertainty,xing2023diff}. Features of lower uncertainty are stabilized using this module which embedded within a skip connection. At each feature scale the uncertainty \( u_i \) for the $i'th$ scale is computed as:
\begin{equation}
    u_i = -\bar{x}_i \log(\bar{x}_i), \quad \text{where} \quad \bar{x}_i = \sigma \left( \frac{1}{C_i} \sum_{c=1}^{C_i} x_{ic} \right), \label{eq:fue_uncertainty}
\end{equation}
where \( \bar{x}_i \) is the normalized mean feature. This uncertainty measurement allows for refined multi-scale feature representation.
Consequently, the final \(i\)-th scale feature \( \tilde{x}_i \) is obtained by integrating the uncertainty information, as shown below:
\begin{equation}
    \tilde{x}_i = x_i + x_i \cdot (1 - u_i), \label{eq:fue_final}
\end{equation}
where \( \tilde{x}_i \) represents the uncertainty-enhanced feature at the \(i\)-th scale. The decoder progressively refines feature maps from the bottleneck layer using a series of upscaling blocks. Each decoder block doubles the spatial resolution and combines multi-scale features from the encoder at each level \( x^i \). The transformation in each block is defined as:
\begin{equation}
D_x = \text{ReLU} \left( \text{Norm} \left( \text{Conv}_{3 \times 3} \left( \text{ConvT}(D_{x-1}) \oplus \text{FUE}x^i \right) \right) \right)
\label{eq:decoder_block}
\end{equation}
where \( D_0 \) is the output from the first decoder block. Here, \(\text{ConvT}\) represents a 3x3 transposed convolution with stride 2, \(\text{Norm}\) denotes instance normalization, and \(\oplus\) is the concatenation operation. The final segmentation head uses a convolutional layer on \( D_1 \) to produce the output with the required number of class channels.
\subsubsection{SegHydra}
% LaTeX code for the Hydra model with quasiseparable matrix mixer in a structured format with labeled equations

%\section{Hydra Model with Quasiseparable Matrix Mixer}
%Mamba-2~\cite{dao2024transformers}

q%uasiseparable matrix have unrestrained parameters in thier diagnals allowing additonal mathematical expressivity over addition based bidirectional semiseperable matrices (Bidirectional mamba)
%\subsection{What it is}
Hydra is a sequence model that incorporates a \textbf{quasiseparable matrix mixer} for effective bidirectional sequence processing. Quasiseparable matrices offer an ideal structure for sequence modeling by generalizing low-rank mixers in linear attention and semiseparable matrices in state-space models (SSMs). This structure provides enhanced computational efficiency while preserving high model expressivity.

%\subsection{How it works}
The quasiseparable matrix structure allows bidirectional sequence mixing holding an advantage over SSMs for its application in non-causal scenarios. SSMs rely on semiseperable matrices for sequence modeling~\cite{dao} and can overcome this limitation by using two SSMs for forward and backward sequence modeling, combining thier features through addition/concatenation. In contrast, Hydra uses quasiseperable matrices to allow non-causal modeling posting three advantages a.) better representation power, b.) sub-quadratic matrix multiplication, c.) lower parameters as compared to alternatives.
%extending traditional SSMs for use in non-causal scenarios. Hydra achieves this without relying on prior ad-hoc approaches, such as combining forward and backward models with element-wise addition or concatenation. Instead, Hydra directly utilizes quasiseparable matrices for a more theoretically grounded bidirectional modeling.
%The quasiseparable matrix structure enables bidirectional sequence mixing, extending traditional SSMs for use in non-causal scenarios. Hydra achieves this without relying on prior ad-hoc approaches, such as combining forward and backward models with element-wise addition or concatenation. Instead, Hydra directly utilizes quasiseparable matrices for a more theoretically grounded bidirectional modeling.
%\subsection{Mathematical Formulation}
A matrix \( M \) is defined as \( N \)-quasiseparable if its elements \( m_{ij} \) satisfy the following conditions:

\begin{equation}
    m_{ij} = 
    \begin{cases} 
      \overrightarrow{c_i}^T \, \overrightarrow{A_{i:j}} \, \overrightarrow{b_j}, & \text{if } i > j \\
      \delta_i, & \text{if } i = j \\
      \overleftarrow{c_j}^T \, \overleftarrow{A_{j:i}} \, \overleftarrow{b_i}, & \text{if } i < j
   \end{cases} \label{eq:quasi_matrix_definition}
\end{equation}

Here, \( \delta_i \) is a scalar, and \( \overrightarrow{b_j}, \overrightarrow{c_i}, \overleftarrow{b_i}, \overleftarrow{c_j} \in \mathbb{R}^{N \times 1} \), with \( A_i \in \mathbb{R}^{N \times N} \). This formulation permits bidirectional processing by including non-zero entries in the upper triangular section.
%\subsection{Efficient Matrix Multiplication}
Tri-oriented Mamba block in SegMamba, processes flattened sequence across three different views of the same 3D input volume through mamba blocks. We reduce parametric complexity and could increase the efficiency of long range modeling by using the quasiseparable matrix multiplication to provide an more effective and efficient alternative while maintaining other aspects of the encoder-decoder architecture described earlier. This operation in Hydra can be efficiently computed by decomposing it into operations involving semiseparable matrices. 
\begin{equation}
    \text{QS}(x_l) = \text{shift}(\text{SS}(x_l)) + \text{flip}(\text{shift}(\text{SS}(\text{flip}(x_l)))) + Dx_l, \label{eq:quasi_multiplication}
\end{equation}

where:
\begin{itemize}
    \item \( \text{QS}(x_l) \): quasiseparable operation on input \( x_l \),
    \item \( \text{SS}(x_l) \): semiseparable operation on \( x_l \),
    \item \( \text{flip}(x_l) \): reverses \( x_l \),
    \item \( \text{shift}(x_l) \): shifts \( x_l \) right by one position with zero-padding at the start, and
    \item \( D = \text{diag}(\delta_1, \ldots, \delta_L) \): diagonal matrix with parameters \( \delta_i \).
\end{itemize}

This decomposition enables Hydra to leverage efficient linear-time semiseparable matrix multiplications, compatible with various SSMs, for high-performance sequence modeling.
\subsubsection{Vision Mamba Block with Swin Transformer Integration}
% LaTeX code for describing the Vision Mamba block integrated with Swin Transformer in a Mamba-based architecture
The MambaVision~\cite{hatamizadeh2024mambavision} emperically showed the advantage of combining Mamba layers succeeded by the self-attention mechanism. Mamba’s autoregressive structure, while effective for sequence modeling, presents limitations in tasks like segmentation and object detection that benefit from a global receptive field. Specifically:
1. Image pixels do not exhibit strict sequential dependencies, making Mamba’s step-wise approach less efficient for spatial data.
2. Autoregressive processing limits the model’s ability to capture global context in a single pass, essential for many vision tasks.
However, using self-attention following Mamba layers for 3D volumes can be computationally heavy and not feasible to train. To address these challenges, we propose a restructured Mamba-SwinTransformer block that integrates Swin-Transformer blocks~\cite{hatamizadeh2021swin} for improved spatial context representation. In our design, we augment each Mamba block with a Swin Transformer layer after every Mamba processing layer. This hybrid structure balances Mamba’s sequential strengths with the Swin Transformer's ability to capture global and local spatial dependencies more effectively.
For the \( l \)-th Vision Mamba block, the computation is given by:

\begin{equation}
    x_{l} = \text{Swin}(\text{LN}(\text{Mamba}(x_l))) + x_l, \label{eq:vision_mamba_swin}
\end{equation}
where:
\begin{itemize}
    \item \( \text{Mamba}(x_l) \): processes the input \( x_l \) to capture sequential dependencies,
    \item \( \text{Swin}(x) \): applies the Swin Transformer block to model spatial context, improving receptive field coverage, and
    \item \( x_l \): the original input feature at layer \( l \).
\end{itemize}

This hybrid Mamba-SwinUnet approach combines the Mamba model’s capability of sequential feature extraction with the Swin Transformer's global spatial learning, enhancing both local and global feature understanding.
\subsubsection{MambaOutUNet}
% LaTeX code for describing the integration of Gated Convolutional Layer in Mamba Block
%\section{Gated Convolutional Layer in Mamba Block Meta-Architecture}
%\subsection{What it is}
To empirically validate our hypothesis, we follow the protocol set in MambaOut~\cite{yu2024mambaout}, the Mamba block is analyzed alongside the Gated CNN block~\cite{dauphin2017language}. 
%we analyze the Mamba block [25] alongside the Gated CNN block [18]. Both structures simplify the MetaFormer’s token mixer and combine it with an MLP layer, similar to the MetaNeXt architecture [93]. 
%\subsection{How it works}
Given input \( X \in \mathbb{R}^{N \times D} \), the Mamba block meta-architecture integrates token mixing, represented by a token mixer, with normalization and an MLP. In this setup:
- \( \text{Norm}(\cdot) \) denotes the normalization layer.
- \( \text{TokenMixer}(\cdot) \) performs token mixing, enhancing spatial feature extraction.
%- Learnable weights \( W_1 \in \mathbb{R}^{D \times rD} \), \( W_2 \in \mathbb{R}^{D \times rD} \), and \( W_3 \in \mathbb{R}^{rD \times D} \) are used in the MLP layers.
For token mixing, the Gated CNN and Mamba differ as follows:
\begin{equation}
    \text{TokenMixer}_{\text{GatedCNN}}(x) = \text{Conv}(x), \label{eq:token_mixer_gatedcnn}
\end{equation}
\begin{equation}
    \text{TokenMixer}_{\text{Mamba}}(x) = \text{SSM}(\sigma(\text{Conv}(x))), \label{eq:token_mixer_mamba}
\end{equation}
where \( \sigma \) is the activation function and SSM represents the sequential state-space model for improved token mixing.
%\subsection{Application}
To isolate the impact of SSM, we introduce a simplified methodology, MambaOut, based on the Gated CNN block without SSM. This setup enables a focused evaluation of the Mamba block's contribution to visual recognition tasks. ,Specifically the Gated CNN token mixer employs a depthwise convolution w



\section{Experiments}
CTS-204 was split into 3 splits, training, validation and testing which had 163,20,21 cases, respectively. The relevant baselines and all proposed architectures were trained on CTS-204 for 200 epochs for the same learning rate and validation interval. Similarly, the BTCV dataset was used and all models were run on it for 400 epochs on a single Tesla A100 GPU. For a fair performance this was done. Following~\cite{pitfallmetric} we use three metrics Dice Score, MeanIoU, and owing to the requirement of accurate boundary delination in radiation oncolocy we used normalized Dice Score To evaluate the method.


\subsection{Results and discussion}
To validate our hypotheses, we conducted experiments on two datasets: the CTS 204 dataset and the BTCV multi-organ segmentation dataset. The results of these experiments are summarized in Tables \ref{tab:result_combined}. From Table~\ref{tab:result1}, we can observe that the MambaOutUNet achieves the Dice Score (DSC), mean Intersection over Union (mIoU), and normalized surface dice (NSD) of 0.5203, 0.6971, and 0.6971 outperforming the second best benchmarks by 1.96\%,1.11\%,0.23\%, respectively. Moreover, a performance gain of 15.19\% and 9.68\% is observed over the Tri-directional mamba equipped SegMamba~\cite{xing2024segmamba}, this can be attributed to the fact that even though segmentation is essentially a \textit{long-sequence task}, tumor segmentation requires focus on immediate locality structure surrounding cancerous tissues for accurate delineation, thus, mamba, transformer and other long-range sequence modeling techniques might not be useful for that extent. It should also be noted that while Hydra's quasisemiseperable matrix mixing technique holds various advantages over bidirectional mamba, it is outperformed by SegMamba on both CTS-204 and BTCV, suggesting that, despite their claims~\cite{hwang2024hydra}, inherent RNN nature of SSMs limits modeling of non-causal features. Methods incorporating SSMs and transformer architectures, while beneficial in contexts requiring long-range dependencies, do not provide the same level of performance for localized segmentation tasks. The focus on global context in these models may lead to unnecessary complexity without yielding proportional improvements in accuracy.
However, From Table~\ref{tab:result_combined} and Figure~\ref{fig:qualitative} we can observe that on BTCV multi-organ segmentation challenge, where the region-of-interest has high variance in size and structure, long range sequencing is desirable, Mamba and transformer based models show competitive performance. Therefore, even though long range sequencing might be desirable in medical image segmentation and is worthwhile exploring, there must also be a need to study these new architectures equipped with Mamba and self-attention variants under the light of small ROI, complex structure datasets like the introduced CTS-204. To this extent, MambaOutUNet can serve as a useful baseline to asses the performance gain obtained through long-range sequencing. We would also like to highlight that there should be an increased effort to design architectures specifically for more efficient local token modeling to capture the small, complex and difficult to observe ROIs like colorectal tumors in medical image analysis..
%In Table \ref{tab:resultw}, we further analyze the performance of MambaOutUNet on the BTCV dataset, where it achieves a DSC of \(0.8338\), indicating competitive performance compared to other state-of-the-art methods, including SwinUNet and SegMamba. These results highlight the versatility of the MambaOut architecture, demonstrating its capability to maintain robust segmentation performance across different types of data.
%The performance of MambaOutUNet in the CRC 204 dataset reflects the utility of gated convolutional layers in extracting relevant features for tumor segmentation. Given that colorectal cancer segmentation or tumor segmentation primarily involves localized regions, the traditional focus on long-range contextual dependencies as provided by SSMs and transformers is less critical. 
%This distinction is vital: while SSMs and transformer models have shown effectiveness in long-range sequence tasks, their benefits diminish in scenarios like colorectal cancer segmentation where precise, localized predictions are paramount. The results affirm that a simpler architecture relying solely on gated convolutions can yield superior results without the need for complex contextual modeling.
%Moreover, the competitive performance observed with MambaOutUNet on the BTCV dataset supports our conclusion that both transformers and Mamba can be effective for longer-range sequencing tasks. However, for colorectal cancer segmentation, MambaOut demonstrates that simplicity in design can lead to significant improvements in accuracy and efficiency.

%In Table \ref{tab:result1}, we observe that the MambaOutUNet achieves a Dice score (DSC) of \(0.5203\), which outperforms the second-best method, SegMamba, by \(0.1213\) points. This improvement indicates that the localized processing capabilities of gated convolutional layers in MambaOut are highly effective for segmenting small areas of tumors, essential for accurate colorectal cancer diagnosis. Conversely, Methods incorporating SSMs and transformer architectures, while beneficial in contexts requiring long-range dependencies, do not provide the same level of performance for localized segmentation tasks. The focus on global context in these models may lead to unnecessary complexity without yielding proportional improvements in accuracy.
\section{CONCLUSION}
In this paper, we release CTS-204, a new colorectal tumor segmentation dataset with 204 distinct cases. We demonstrate the effectiveness of our MambaOut architecture for colorectal tumor segmentation which provides critical insights and a robust framework for future research in tumor segmentation. Our findings suggest a promising direction for further investigation into segmentation methodologies that prioritize performance on small-scale features. Additionally, we critically analyze segmentation architectures embedded with recent long range sequencing techniques and reassess their performance on two different medical image segmentation, which empirically supports our hypothesis. Moreover, we establish the importance of MambaOutUnet as a baseline architecture for future 3D volumetric segmentation research. 

%In summary, we have introduced a novel colorectal cancer segmentation dataset alongside the MambaOut architecture, which provides critical insights and a robust framework for future research in tumor segmentation. Our findings emphasize the importance of localized feature extraction and suggest a promising direction for further investigation into segmentation methodologies that prioritize performance on small-scale features. This research paves the way for enhancing accuracy in tumor detection and can guide subsequent advancements in medical imaging technologies.


%\small{\subsubsection*{COMPLIANCE WITH ETHICAL STANDARDS}} This research study was conducted retrospectively using human subject data made available in open access by Kvasir-SEG and CVC-ClinicDB. Ethical approval was not required.
%% \small{\subsubsection*{ACKNOWLEDGEMENTS}}
%\vfill
%\pagebreak
\bibliographystyle{IEEEbib}
\bibliography{strings,refs}

\end{document}
