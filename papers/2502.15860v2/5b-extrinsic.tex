\subsection{Scenario 1: Baseline}

\input{tables/sc-1-baseline-best}

Table~\ref{t:results-s1-b} shows the development and test set accuracies for BERT-based classifier trained on different portions of the training split of authentic data in scenario 1.
The results illustrate the impact of training data size on the performance of a BERT-based classifier.
As expected, model accuracy on both the development and test sets improves as the portion of authentic training data increases. The improvement is most pronounced between 20\% and 50\% of the data, where test accuracy jumps by nearly 6\%, indicating that a significant portion of the model's learning occurs with moderate data availability. However, gains diminish beyond 80\%, with only a marginal increase of 1.1 percentage points when scaling from 80\% to 100\%. This suggests diminishing returns in model performance as more data is added.%, a common trend in deep learning models where initial data increments yield larger benefits than later ones.
%The relatively small standard deviations indicate stable performance across different random seeds, reinforcing the reliability of these trends.


\subsection{Scenario 2: LLMs as a Classifier}

Table~\ref{t:results-s2} presents the accuracy of Llama 3.3 70B, GPT-4o and Grok-2-1212 models using GE and GF prompts on both development and test sets in Scenario 2. We evaluate both GE and GF prompts on the development set and test the winning prompts on the test set.
GPT-4o achieves the highest accuracy regardless of the prompt used, %with GE prompts, %demonstrating its superior generalization, 
while Grok performs the lowest.
The lower performance of GF prompts for two LLMs, particularly for Grok, highlights the importance of prompt design for model accuracy.

To decide which prompt to evaluate on test data for GPT-4o, we use the consistent performance trend with the other LLMs as a tie breaker.
Comparing the GE test set results in Table~\ref{t:results-s2} with those in
Table~\ref{t:results-s1-b}, GPT-4o with the GE prompt reaches the highest test accuracy, outperforming the BERT-based model by 1.6\%.
However, despite the performance advantage, LLMs come with significant computational costs, making them less practical for real-time applications on resource-constrained devices.
% JW: The following 2 sentences (1 LaTeX code line) could be commented out if space is tight
In contrast, the BERT-based classifier is a lightweight model optimized for efficiency, making it a more suitable choice for scenarios where speed and scalability are crucial. Since our industry partner aims to deploy CB detection on children's mobile devices, we prefer using the BERT-based classifier due to its lightweight nature, which allows it to run efficiently on mobile hardware.
This trade-off highlights the importance of balancing accuracy with %feasibility,
application requirements
as a small accuracy decrease of 1.6\% can be acceptable in exchange for a model that is faster, more accessible, and capable of running on mobile devices.
% without requiring constant server access.

\begin{table}[ht]
\centering
\begin{tabular}{llrr}
%\hline
\textbf{LLM} & \textbf{Prompt} & \textbf{Dev. Set} & \textbf{Test Set} \\
\hline
Llama & GE & 82.9\% & 80.4\% \\
Llama & GF & 82.0\% & -\\
\hline
GPT & GE & \textbf{85.6\%} & \textbf{83.1\%} \\
GPT & GF & \textbf{85.6\%} & - \\
\hline
Grok & GE & 81.3\% & 78.6\% \\
Grok & GF & 75.9\% & - \\
\hline
\end{tabular}
\caption{Accuracy of different LLMs with GE and GF prompts on development and Test sets in scenario 2: Using LLM as a CB Classifier. Dev. = development, Llama 3.3 with 70 billion parameters}
\label{t:results-s2}
\end{table}



\subsection{Scenario 3: Fully Synthetic Data}  

\input{tables/sc-3-synthetic-single-3}
%\input{tables/sc-3-synthetic-wide-3}

Table \ref{t:results-s3-s-3} presents the development and test set accuracy for scenario 3: BERT-based classifiers trained on synthetic datasets generated by Llama, GPT, and Grok. The results indicate that Llama consistently outperforms the other two models across different dataset sizes. GPT follows closely, while Grok exhibits the weakest performance, particularly on the test set, where its accuracy is significantly lower than that of the other models. This suggests that Llama-generated synthetic data is of higher quality and more beneficial for training the classifier, despite the lower label quality observed in scenario 2.

Examining the impact of increasing the size of synthetic data, the results show a general trend of performance improvement for Llama and Grok, but little impact for GPT.
%These results suggest that increasing synthetic data size enhances performance for some models, like Llama and Grok, but has little effect on GPT.
This indicates that the effectiveness of synthetic data scaling may depend on the LLM and the quality of the data.

Comparing the training of BERT on authentic data (Table~\ref{t:results-s1-b}) with its training on synthetic data (Table~\ref{t:results-s3-s-3}), the performance of Llama-generated synthetic data without any human-created data is notable  (75.8\% accuracy vs.\ 81.5\%).
%that, while authentic data yields the highest accuracy (81.5\%), achieving 75.8\% with â€”without relying on any human-created data, is still notable. 
The 5.7\% accuracy gap highlights the trade-off between performance and the reduced costs and ethical complexities associated with human-annotated CB datasets.
For sensitive and ethically challenging tasks, %, such as CB detection,
synthetic data can present a valuable alternative, minimizing human involvement while maintaining competitive model performance.
The results emphasize that high-quality synthetic data from advanced LLMs, such as Llama, can serve as a practical supplement or even a partial substitute, particularly in scenarios where authentic data collection is difficult or impractical.

\subsection{Scenario 4: Synthetic Labels for Unlabeled Authentic Data}

\input{tables/sc-4-silver-comb-3}
Table~\ref{t:results-s4-c} shows that among the different LLMs used for synthetic annotation, Llama 3.3 produces the most useful labels, allowing BERT to achieve a test accuracy of 79.1\%, closely approaching the human-labeled benchmark.
This is despite the fact that the results in scenario 2 above show that the GPT-4o labels better match human labels than Llama 3.3's labels.
In scenario 4,
GPT-4o follows with a slightly lower accuracy of 77.9\%, while Grok lags behind at 75.2\%. The difference in performance suggests that not all synthetic labels are of equal quality, and the choice of LLM for data annotation can impact model performance. Comparing the results in Table \ref{t:results-s1-b} with those in Table \ref{t:results-s1-a} shows that BERT trained on 100\% authentic human-labeled data achieves a test accuracy of 81.5\%, while the best-performing synthetic labels, produced by Llama 3.3, yield 79.1\%. This small gap of just 2.4 percentage points highlights that synthetic labels can %closely approximate human annotations, making them
be
a viable substitute
to human labels
while significantly reducing the cost, ethical concerns, and legal restrictions associated with human annotation, particularly for sensitive or harmful content.


%Commented for industry track \subsection{Scenario 6} 
  
% \input{tables/sc-6-augmented-all-3}
% Table~\ref{t:results-s6-all-3} shows the  Development and test set accuracies in scenario 6. We investigate two key aspects: (1) The comparison of different LLMs (GPT, Llama, and Grok) for augmentation, (2) The minimum amount of authentic data required to achieve comparable results to using the full authentic dataset.


% When comparing the potential of different LLMs for data augmentation, considering the rows with 100\% authentic data, all three models initially show similar test accuracies. However, regardless of the LLM used, adding synthetic data consistently leads to improvements in performance, with one exception: GPT at the 100\%-60\% authentic-synthetic combination, where its accuracy slightly decreases. GPT demonstrates the most significant gains, reaching up to 82.2\% accuracy with 40\% synthetic data. An interesting observation is that, despite Grok being significantly weaker than both GPT and Llama in scenario 3 in Table \ref{t:results-s4-all-3}, with a maximum accuracy of only 52.7\%, it still performs well in data augmentation, providing valuable improvements to the overall model performance. 

% Analyzing Table~\ref{t:results-s6-all-3}, it is evident that introducing synthetic data allows the models to achieve comparable performance to using the full (100\%) authentic dataset with a reduced amount of authentic data. For instance, at a 50\% authentic-to-synthetic data ratio, all three models show test accuracies (79.8\%), closely matching the 80.9\% accuracy obtained with the full authentic dataset. This suggests that using synthetic data, especially in balanced ratios (50\% authentic and 50\% synthetic), can significantly reduce the amount of authentic data required to maintain high model performance. Therefore, a 50\% authentic data ratio appears to be the minimum threshold at which comparable results to using 100\% authentic data can be achieved.










