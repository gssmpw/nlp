\input{tables/ie-1-intrinsic-metrics-merged} 

% Commented for industry tack Table~\ref{t:intrinsic-metrics-merged} shows the results of the intrinsic evaluation across % these
% four categories for each dataset. In terms of lexical and linguistic characteristics, the synthetic datasets show significant differences from WhatsApp. Both Grok-2 and Llama-3.3-70B-Instruct generate longer messages compared to WhatsApp dataset. Meanwhile, GPT is closer but still produces longer messages. The Type-Token Ratio (TTR) is lower for all synthetic datasets compared to WhatsApp, indicating less lexical diversity in the synthetic data. This suggests that the synthetic data may lack the richness in vocabulary typically found in real-world WhatsApp conversations.

% In the content and cyberbullying indicators category, the synthetic datasets show varying levels of harmful content, with Grok-2 standing out by producing the highest percentage of harmful messages (65.27\%) and toxicity (2.37\%). This high level of harmful content in Grok can be valuable for detecting extreme cases of CB. While Llama (17.19\%) and GPT (6.57\%) have much lower harmful content, these datasets
% may
% still present valuable data for detecting subtler instances of CB, with Llama's content being more balanced and closely resembling real-world interactions. 

% In terms of sentiment scores, Llama and GPT are more similar to WhatsApp overall. Both Llama and GPT have a slightly higher percentage of positive messages compared to WhatsApp, but their neutral and negative sentiment distributions are fairly close to WhatsApp’s. On the other hand, Grok’s sentiment distribution deviates more significantly from WhatsApp dataset. Grok-2's dataset has a much higher proportion of negative messages, which suggests that it over-represents hostile or aggressive tones. Its positive sentiment is similar to WhatsApp datset, but the higher negative sentiment creates a more polarized emotional tone in the data. This makes Grok-2's dataset less similar to WhatsApp dataset in overall sentiment, but it could still be useful for detecting more extreme negative interactions, such as severe CB cases.

% In terms of dialogue act distribution, the statement and accept/reject ratios are the most critical for CB detection. Llama-3.3-70B's dataset is the most similar to WhatsApp dataset, making Llama-3.3-70B the best choice for generating synthetic data that closely mirrors real-world conversations. GPT-4o and Grok-2 have higher statement percentages but lower accept/reject rates, making them useful for augmenting data but less similar to WhatsApp datsset compared to Llama.



