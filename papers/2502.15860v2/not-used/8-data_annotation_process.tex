
To generate synthetic labels for unlabeled data, we employ a data annotation pipeline leveraging Large Language Models (LLMs), specifically using the Meta-Llama-3-70B-Instruct model. The data annotation process automates the labeling of large datasets.

\subsection{Annotation Pipeline}
The data annotation pipeline operates as follows:

\subsubsection{Input Data}
Unlabeled text messages from authentic conversations are provided as input.

\subsubsection{Contextualization}
For each message, the last \textit{kkk} messages in the conversation are retrieved to provide context. This helps the LLM understand the flow of the conversation and make more accurate annotations.

\subsubsection{Prompt Construction}
A prompt is constructed containing the moderation criteria, the context, and the message to be annotated. The prompt instructs the LLM to analyze the text and determine whether it contains any form of cyberbullying or harmful content.

\subsubsection{LLM Inference}
The LLM processes the prompt and generates an annotation, providing detailed reasoning for its classification. It outputs the final answer as a tuple in the format:
\begin{center}
\texttt{('label', confidence)},
\end{center}
where \texttt{'label'} is either \texttt{'Harm'} or \texttt{'No Harm'}, and \texttt{confidence} is a number between 0 and 1.

\subsubsection{Error Handling}
The pipeline includes mechanisms to handle potential errors such as tokenization issues, inference errors, and parsing failures. These are logged for further analysis.
