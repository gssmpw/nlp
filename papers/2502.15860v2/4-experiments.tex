\subsection{Dataset Description} 

%As previously stated, the
The
primary objective of our study is to detect CB among children.
Therefore, it is essential to utilize a dataset specifically tailored for this purpose.
The WhatsApp (WA) dataset \cite{verma-etal-2023-leveraging, sprugnoli-etal-2018-creating} is a publicly available, fully annotated dataset %specifically
focused on pre-adolescents and we use the English version of this dataset for our experiments.
The WA dataset was constructed through role-play activities in WA groups, each containing approximately 10 students.
The roles are:
    cyberbully (2 students),
    cyberbully assistants (3–4 students),
    victim assistants (3–4 students) and
    a victim.
Conversations are initiated using one of four
predefined cases describing situations in which CB may occur \cite{sprugnoli-etal-2018-creating}.
Appendix Table~\ref{t:whatsapp-scenarios} shows one of these cases as an
example.\footnote{Note
    that in the cited paper, these cases are referred to
    as ``scenarios.''
    However, we use the term ``cases'' to avoid confusion with
    the scenarios discussed above.}
that addressed common teenage issues.
%, such as gender stereotypes in sports or the spread of embarrassing videos. 
%Appendix Table~\ref{t:whatsapp-scenarios} presents an example of one case and the type of problem used to initiate the conversations. Details of the remaining three cases can be found in ~\cite{sprugnoli-etal-2018-creating}.
%After creating conversations via role-playing, m
Messages in the conversation are annotated manually by experts following the fine-grained framework
%adapted from the "Guidelines for the Fine-Grained Analysis of Cyberbullying" 
of \newcite{van-hee-et-al-2015-guidelines}.
% The annotations categorized harmful expressions into detailed types, such as General Insults, Body Shame, Sexism, Racism and Body Shame. 
%In this study, we aim to classify messages as either harmful or harmless. %Therefore, instead of using fine-grained labeling,
However, for this study, we binarise the labels, mapping
all categories of CB to the ``harmful''
label.\footnote{We
    deviate from \newcite{verma-etal-2023-leveraging} in not excluding
    harmful messages of the victim from the ``harmful'' class.
}
Table~\ref{t:dataset-stat} shows the statistics of the WA dataset.

\begin{table}
\centering
\begin{tabular}{lrrc}
\textbf{Split}  & \textbf{Size} & \textbf{Harmful}  & \textbf{\%} \\
\hline     
Training        &  1,314        &   398    &  30.3 \\ 
Validation      &    439        &   133    &  30.3 \\ 
Test            &    439        &   133    &  30.3 \\     
\hline
Total           &  2,192        &   664    &  30.3 \\ 
\hline
\end{tabular}
\caption{WA dataset: number of messages and fraction of harmful messages}
\label{t:dataset-stat}
\end{table}


% \subsection{Data Preprocessing}

% TODO: Give overview what ``merge-data.py'' does. Say that we start with non=-public pre-processes data received from KV.



\subsection{Large Language Models}

We employ three LLMs for our experiments: 
%GPT, LlamA, and Grok. Specifically, we use
GPT-4o-Feb-2025 \cite{openai-2024-gpt},
Llama-3.3-70B-Instruct \cite{meta-2024-llama33}\footnote{We include a small amount
    of synthetic data produced by Llama3 from early experiments,
    see Appendix Table~\ref{t:llama-versions}.
    For labeling with Llama, version 3.3 has been used throughout.
} and Grok-2-latest-Feb-2025 \cite{xai-2024-grok}.


\subsection{Generating Synthetic Data and Labels}

%Prompt Engineering involves crafting and refining the instructions provided to the LLM to elicit the desired outputs. 
Our prompt design process for all scenarios begins with a simple initial prompt, which is % iteratively
refined over multiple rounds of trial and error.
In this iterative approach, we make gradual improvements, with adjustments made based on the quality and relevance of the responses generated by the LLM on the development set.
% JW: below two sentences already covered in section 3
%In this paper, LLMs are utilized for two primary tasks in the context of CB detection: (1) generating synthetic data and (2) generating synthetic labels.
%More specifically, in Scenarios 2 and 4, the LLM is used to label authentic data. In Scenario 3, the LLM is first used to generate synthetic data, and then it is applied again to label the created synthetic data.
% The subsections below detail the prompt design strategies employed for these two tasks.


\subsubsection{Designing Prompts for Synthetic Label Generation}

%In this section, we describe the prompts designed to instruct an LLM to label unlabeled input data for the task of CB harm detection.
In the task of labelling the messages of a conversation as either harmful or not, the input data consists of a list of messages and the possible labels are ``harmful'' and ``harmless''.
We simplify the task by not considering context (previous messages) and classifying each message in isolation.\footnote{Future work can consider how to best include the conversation context into the prompt.}
%Harmful messages show instances of CB, while harmless messages are normal, safe communications.
%As described in Section 3 --- We explore two approaches to prompt design: (1) guideline-free (GF) and  (2) guideline-enriched (GE).
In the GF approach (Section~\ref{s:m:sc2}), the LLM is simply instructed to label messages as ``harmful'' or ``harmless'' for the task of CB detection, without providing additional guidelines. In the GE approach, the LLM is supplied with detailed instructions for labeling messages. These instructions are adapted from the annotation guidelines~\cite{van-hee-et-al-2015-guidelines} originally used by human annotators for labeling authentic data.
For this study, since the test set is derived from the WA dataset, we utilize the same guideline employed by human annotators to label this dataset.
A copy of the guidelines can be found in Appendix Table~\ref{t:whatsapp-guideline}.
Our prompts are shown in Appendix Table~\ref{t:prompt}
%in the Appendix shows the prompts used to generate synthetic labels
for both the GE and GF approaches.


\subsubsection{Designing Prompts for Synthetic Data Generation}

%In this section, we present the prompts designed to guide an LLM in generating synthetic data for CB detection. Specifically, the synthetic data w
We aim to produce synthetic data that consists of conversations between participants, where the dialogue should include instances of CB (or at least harmful messages that could be CB if the threats are repeated and intended to cause harm).
% JW: The following sentence is off-topic and repeats what is explained elsewhere.
% To train the CB classifier, we require labeled messages. Therefore, we first use the LLM to generate synthetic conversation and subsequently use the LLM to label the messages in the conversation.
To make the generated data more relevant to the test data, each prompt includes one of the four predefined ``cases'' and ``problem types'' that where used by \newcite{sprugnoli-etal-2018-creating} in the creation of the WA dataset to guide the students' role-playing.
%We used the cases defined in the WA dataset to generate synthetic conversations.
%By providing the system with the predefined "cases" and "problem types" from the WA dataset, we prompted it to create conversations based on these inputs. %In the zero-shot setting, the language model was not given any examples. In the few-shot setting, we randomly selected conversations from the validation set of the WA dataset and provided them as examples to the LLM to guide the generation process. T
Appendix Table~\ref{t:prompt} presents the prompts employed for generating synthetic data.


\subsection{Amount of Synthetic Data}

For a more meaningful comparison of results with the authentic data, we sample subsets of the generated data to match the size of the authentic data for each CB case.
We also sample subsets of certain percentages of these sizes
from 100\% to 200\%.
%\footnote{The appendix includes results for up to 600\%.} %% that's for data-augmentation moved to a separate paper
Since the test set is never synthetic, we measure sizes relative to the concatenation of training and validation data, i.e.\ $1314+439=1753$ messages correspond to 100\%. 
The per-case target sizes for 100\% are
% deduplicate.py get_wa_target_sizes_nodedup_80_cl()
 863 (case A),
 462 (case B),
 103 (case C) and
 325 (case D).
 

\subsection{Authentic Unlabelled Data}

In scenario 4, we remove the gold labels from the training and validation section of the authentic labelled data of scenario 1 to produce the unlabelled data
to maximise comparability of the data and to limit changes to the labels that are
annotated by the LLM in this scenario.
%
In scenarios 3 and 4, the validation set with synthetic labels is used for model selection and early stopping, but validation set results in Section~\ref{s:results} will use the gold labels
to measure performance.\footnote{Accuracy
    on synthetic validation data often exceeds 90\% in our experiments,
    suggesting the that the synthetic data is less diverse than
    the authentic data for which validation accuracy usually is
    below 83\%.
}

%\subsection{Data Annotation Process}
%\input{8-data_annotation_process}
%\subsection{Training Configurations and Hyperparameters}

