In recent years, the use of LLMs for synthetic data generation has gained significant traction due to its efficiency, scalability, and cost-effectiveness compared to human-annotated datasets.
%Given that LLMs demonstrate human-like behavior in natural language understanding, many studies have explored their potential in generating synthetic datasets for various NLP tasks. 
\newcite{he-etal-2021-generate, he-etal-2022-generate} generate
synthetic data for knowledge distillation, self-training, and few-shot learning tasks, which is then annotated using state-of-the-art classifiers.
\newcite{bonifacio-etal-2022-inpars} employ LLMs
to generate labeled data in a few-shot manner for information retrieval.
%The synthetic data is subsequently used to fine-tune smaller retrieval models, which are later used to rerank search results from an initial retrieval system. 
\newcite{yoo-etal-2021-gpt3mix} generate
augmented text samples by selecting a few sentences from task-specific training data and embedding them into prompts.
\newcite{anaby-etal-2020-donot} fine-tune a pre-trained language model
on a small labeled dataset. The fine-tuned model is then used to generate new labeled text samples.
\newcite{meng-etal-2022-generating} employ an LLM
to generate class-conditioned texts based on label-descriptive prompts for a classification task.


While the aforementioned studies focus on general NLP tasks, some research
%specifically 
explores synthetic data generation for specific domains. % such as medical and
harmful-content detection.
\newcite{wang-etal-2024-notechat} introduce NoteChat, a multi-agent framework designed to generate synthetic patient-physician conversations from clinical documents.
\newcite{ghanadian-etal-2024-socially} create socially aware synthetic datasets for suicidal ideation detection.
%The authors first extract key social factors associated with suicidal ideation from psychology literature, then they generate synthetic datasets using zero-shot and few-shot learning techniques. These datasets are subsequently used to fine-tune classifiers, which are evaluated on both authentic and synthetic-labeled datasets.
Previous studies exploring the use of synthetic data have reported mixed results on whether LLM-generated synthetic data can effectively train models to perform at a level comparable to those trained on authentic data \cite{li-etal-2023-synthetic}.

There also is research %specifically focusing
on generating semi-synthetic data for CB detection.
\newcite{ejaz-etal-2024-multi} generate a semi-synthetic CB dataset by creating synthetic users. %A peerness matrix quantifies relationships between users. 
Aggressive and non-aggressive messages are sourced from existing authentic datasets and randomly assigned to user interactions.
CB instances are labelled using a threshold-based approach, flagging interactions as CB if they meet predefined criteria for peerness, intent to harm and message repetition.
This study creates a dataset, trying to simulate real CB behaviors, where the users are semi-synthetic, and the messages are generated using a combination of existing authentic CB datasets and synthetic data.
%Pérez et al.
\newcite{perez-etal-2024-generation} generate
%% JW@AK: I created a bib entry. Is this the correct one?
%% An NLP-assisted Bayesian time-series analysis for prevalence of Twitter cyberbullying during the COVID-19 pandemic
%% https://link.springer.com/article/10.1007/s13278-023-01053-4- AK: Corrected. I changed the referecne.
a synthetic CB dataset using a Bayesian Network (BN) model
\cite{pearl-1988-probabilistic} trained on real-world survey data collected from minors. To generate synthetic data, the Bayesian Model Sampling algorithm was applied to the trained BN, producing risk profiles for synthetic agents.
The dataset consists of synthetic players with demographic attributes and responses to 15 binary questions related to cyber risk.
%This dataset represents synthetic users' responses to 15 CB-related binary questions.

In contrast to these works, our study aims to explore the use of LLMs to generate a fully synthetic CB dataset or to label existing authentic but unlabeled CB datasets.
Our work uniquely applies LLM-generated data to CB detection, an area where data collection is both ethically and logistically challenging.
By assessing the effectiveness of LLM-generated datasets in comparison to authentic data,
%in the task of training light-weight BERT-based classifiers,
we provide new insights into the viability of LLMs in training models for sensitive tasks like CB detection. %, where the availability of diverse and annotated data is critical.

%\subsection{Cyberbullying Detection}

% \cite{Verma-etal-2023-leveraging}

% \subsection{Synthetic Data for NLP}

% \cite{li-etal-2023-synthetic}

% + Wang et al. (2023). Authentic Dialogue Generation to Improve Youth’s Awareness of Cybergrooming for Online Safety.

% + \cite{ghanadian-etal-2024-socially} and citations in their section II.C

% + \cite{kaddour-etal-2023-challenges}'s section 3.11

% + Wang et al. (2023). GPT-NER: Named Entity Recognition via Large Language Models. \url{https://arxiv.org/abs/2304.10428}
% add ``self-verification'', in which the GPT model reviews entity decisions.

% +  Anaby-Tavor et al. (2020). Do Not Have Enough Data? Deep Learning to the Rescue!
% The Thirty-Fourth AAAI Conference on Artificial Intelligence (AAAI-20)
% \url{https://doi.org/10.1609/aaai.v34i05.6233}
% in which synthetic data and labels are generated simultaneously by an LLM and the synthetic data is then filtered for quality by a base classifier trained on authentic data before training a new classifier.

% Li et al. (2020). Complementary Auxiliary Classifiers for Label-Conditional Text Generation. Proceedings of the AAAI Conference on Artificial Intelligence.
% \url{https://doi.org/10.1609/aaai.v34i05.6346}

% + papers on open-domain conversation generation?

