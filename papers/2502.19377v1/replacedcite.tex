\section{Related Work}
The easiest way of training a model for \ac{CO} is to assume the existence of ground truth solutions to the \ac{CO} problems and train in a supervised fashion.
    ____ leverage the fact that many \ac{CO} problems ask to identify a certain subset of the input (e.g.\ minimum-k-cut) or a permutation of the input (e.g.\ \ac{TSP}).
    They introduce an LSTM-based sequence-to-sequence architecture where the elements of the output sequence are positions in the input sequence, which they name pointer network.
    ____ recognize that most common \ac{CO} problems have a natural graph representation.
    In their work, a \ac{GNN} predicts an approximate solution as a heatmap, which is then decoded into a valid solution to the \ac{CO} problem using beam search.
    ____ follow a neural algorithmic reasoning approach to learn to imitate \ac{CO} solvers.
    The model is pre-trained on simple algorithms and then fine-tuned on difficult ones.
    Finally, ____ uses graph-based denoising diffusion to generate high-quality solutions.
    However, these supervised approaches aren't applicable to such cases where calculating exact solutions for the training problems is not feasible.

    Several approaches have used \acf{RL} to remove this dependence on a labeled dataset.
    ____ use a pointer network____, but train it with \ac{RL}.
    ____ use the \ac{GNN} autoregressively to predict which node should be added next to the solution set and repeat that process until a valid solution is reached.
    ____ formulate the \ac{CO} problem as a Markov decision process, then use an algorithm similar to AlphaGo Zero____ to solve it autoregressively.
    ____ propose a modification to the \ac{RL} training process for autoregressive methods that improves sample efficiency.
    However, the use of the score function estimator in many of these methods leads to high-variance gradients, which makes training more difficult.


    Other self-supervised approaches that do not rely on reinforcement learning include ____, which use a contrastive loss instead.
    ____ focus on quadratic and polynomial unconstrained binary optimization, which many \ac{CO} problems can be formulated as.
    This allows them to make use of a self-supervised loss function specific to these two problem families.
    ____ also concentrate on quadratic unconstrained binary optimization, but employ a diffusion-based approach similar to ____.
    Similarly, ____ formulate a self-supervised loss function for a comprehensive class of \ac{CO} problems.
    ____ use an LSTM-based architecture to solve binary maximum constraint satisfaction problems, which many \ac{CO} problems can be formulated as.
    Since these approaches focus on certain families of \ac{CO} problems, they aren't general to \ac{CO} as a whole.

    
    ____ use a \ac{GNN} as a heuristic for the branch-and-bound algorithm, which guarantees exact solutions at the expense of a comparatively longer running time.
    The \ac{GNN} is trained via imitation learning from a known high-quality but slow heuristic.
    ____ compare some of the paradigms introduced in other papers in structured experiments.
    There have been two lines of work to address the problem of backpropagating through combinatorial optimization problems.
    Firstly, if we have a set of optimal solutions given as training data, we can use supervised learning to train the GNN to output adjacency matrices as close as possible to the optimal solutions ____.
    This is often called ``predict, then optimize''.
    Secondly, there are several methods to backpropagate through a non-differentiable \ac{CO} algorithm, such as ____. Related to our work is decision-focused learning, which has developed several methods to backpropagate through CO solvers ____.