\documentclass{article}


\usepackage{microtype}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{bm} 
\usepackage{subcaption}
\usepackage[printonlyused]{acronym}
\usepackage{hyperref}


\newcommand{\theHalgorithm}{\arabic{algorithm}}


\usepackage[accepted]{icml2025}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage[capitalize,noabbrev]{cleveref}
\usepackage{xcolor}

\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}


\usepackage[textsize=tiny]{todonotes}



\newacro{PBGE}[PBGE]{preference-based gradient estimation}
\newacro{GNN}[GNN]{graph neural network}
\newacro{CO}[CO]{combinatorial optimization}
\newacro{TSP}[TSP]{travelling salesman problem}
\newacro{RL}[RL]{reinforcement learning}
\newacro{MLP}[MLP]{multi-layer perceptron}
\newacro{I-MLE}[I-MLE]{implicit maximum likelihood estimator}
\newacro{LLM}[LLM]{large language model}



\def\gG{{\mathcal{G}}}
\def\gM{{\mathcal{M}}}
\def\gP{{\mathcal{P}}}
\def\gS{{\mathcal{S}}}
\def\gT{{\mathcal{T}}}



\renewcommand{\algorithmiccomment}[1]{{\color{gray}// #1}}


\newif\ifshowcomment
\newcommand{\mn}[1]{\ifshowcomment{{\textcolor{Bittersweet}{\textbf{[MN:} {#1}\textbf{]}}}}\fi}
\newcommand{\am}[1]{\ifshowcomment{{\textcolor{Emerald}{\textbf{[AM:} {#1}\textbf{]}}}}\fi}
\newcommand{\ub}[1]{\ifshowcomment{{\textcolor{Tan}{\textbf{[UB:} {#1}\textbf{]}}}}\fi}
\newcommand{\ts}[1]{\ifshowcomment{{\textcolor{Rhodamine}{\textbf{[TS:} {#1}\textbf{]}}}}\fi}


\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}






\begin{document}


\twocolumn[
\icmltitle{Preference-Based Gradient Estimation for ML-Based Approximate Combinatorial Optimization}



\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
    \icmlauthor{Arman Mielke}{etas,stuttgart,imprs}
    \icmlauthor{Uwe Bauknecht}{etas}
    \icmlauthor{Thilo Strauss}{xjtlu}
    \icmlauthor{Mathias Niepert}{stuttgart,nec,imprs}
\end{icmlauthorlist}

\icmlaffiliation{etas}{ETAS Research, Stuttgart, Germany}
\icmlaffiliation{stuttgart}{Computer Science Department, University of Stuttgart, Germany}
\icmlaffiliation{imprs}{Max Planck Research School for Intelligent Systems (IMPRS-IS)}
\icmlaffiliation{xjtlu}{Xi'an Jiaotong-Liverpool University, School of AI and Advanced Computing, China}
\icmlaffiliation{nec}{NEC Laboratories Europe}

\icmlcorrespondingauthor{Arman Mielke}{arman.mielke@etas.com}




\icmlkeywords{Combinatorial Optimization, Combinatorial Optimisation, Gradient Estimation, Preference Learning, Graph Neural Networks, Travelling Salesman Problem, Traveling Salesman Problem, TSP, Minimum k-Cut, Machine Learning, ICML}

\vskip 0.3in
]









\printAffiliationsAndNotice{}  










\begin{abstract}
    \Ac{CO} problems arise in a wide range of fields from medicine to logistics and manufacturing.
    While exact solutions are often not necessary, many applications require finding high-quality solutions quickly.
    For this purpose, we propose a data-driven approach to improve existing non-learned approximation algorithms for \ac{CO}.
    We parameterize the approximation algorithm and train a \ac{GNN} to predict parameter values that lead to the best possible solutions.
    Our pipeline is trained end-to-end in a self-supervised fashion using gradient estimation, treating the approximation algorithm as a black box.
    We propose a novel gradient estimation scheme for this purpose, which we call \emph{preference-based gradient estimation}.
    Our approach combines the benefits of the neural network and the non-learned approximation algorithm:
    The \ac{GNN} leverages the information from the dataset to allow the approximation algorithm to find better solutions, while the approximation algorithm guarantees that the solution is feasible.
    We validate our approach on two well-known combinatorial optimization problems, the \acl{TSP} and the minimum $k$-cut problem, and show that our method is competitive with state of the art learned \ac{CO} solvers.
\end{abstract}


\section{Introduction}

    Traditional algorithms for \acf{CO} often focus on improving worst-case performance.
    However, this worst case may occur rarely or never in real life.
    Data-driven approaches can help focus on the problem instances that do occur in practice.
    Previous work has used neural networks to solve \ac{CO} problems \citep{co-with-dl-survey}.
    \Acfp{GNN} are usually used in this role, since most \ac{CO} problems are either naturally formulated on graphs or admit simple graph formulations.
    Since neural networks cannot output discrete solutions to \ac{CO} problems directly, generic algorithms like Monte Carlo tree search, beam search, or sampling are often used to decode the \ac{GNN}'s continuous outputs into a solution to the \ac{CO} problem.
    However, these methods are impractical to use during training because of their prohibitively long runtime.
    Omitting them during training means the usage of the \ac{GNN} differs significantly between training and testing.
    \citet{position--rethinking-post-hoc-search-tsp} argue that this inconsistency leads to uncertain performance during testing.

    In order to avoid such drawbacks, we instead use the \ac{GNN} to augment an existing, non-learned approximation algorithm for a given \ac{CO} problem.
    We parameterize the approximation algorithm and let the \ac{GNN} estimate the parameters based on the input graph.
    By selecting an approximation algorithm that is fast enough to be used during training, we can use the same overall pipeline during training and testing, addressing the concern raised by \citet{position--rethinking-post-hoc-search-tsp}.
    Our approach combines the advantages of the \ac{GNN} and the traditional, non-learned approximation algorithm, and allows them to cancel out each other's weaknesses.
    The \ac{GNN} leverages a data-driven approach to achieve good performance on the problem instances that occur frequently.
    However, it cannot output solutions to the \ac{CO} problem directly.
    The \ac{CO} approximation algorithm solves this by transforming the parameters predicted by the \ac{GNN} into a feasible solution.
    On its own, the approximation algorithm cannot leverage the information in the dataset and it therefore performs comparatively poorly in practice, but this is offset by the \ac{GNN}.

    Since the \ac{CO} approximation algorithm's output is discrete, the algorithm is not differentiable.
    We therefore use gradient estimation to allow us to backpropagate through the approximation algorithm during training.
    We experiment using several existing gradient estimators, but we also introduce a new gradient estimation scheme, which improves over existing approaches for this purpose.
    We call our new scheme \emph{\acf{PBGE}}.
    Our training setup allows for fully self-supervised training without the use of pre-computed optimal solutions to the \ac{CO} problems.

    In summary, our contributions include:
    \begin{enumerate}
        \item A self-supervised method for training a \ac{GNN} for \ac{CO} problems;
        \item A novel gradient estimation scheme, \ac{PBGE}, for backpropagating through \ac{CO} approximation algorithms; and
        \item Extensive experimental evaluation of (1) and (2) on two common \ac{CO} problems.
    \end{enumerate}
    Our implementation and data will be made public as soon as the paper is accepted\footnote{The code was submitted as supplementary material for the reviewers and will be published once the paper is accepted}.


\section{Related Work}

    
    
    

    The easiest way of training a model for \ac{CO} is to assume the existence of ground truth solutions to the \ac{CO} problems and train in a supervised fashion.
    \citet{pointer-networks} leverage the fact that many \ac{CO} problems ask to identify a certain subset of the input (e.g.\ minimum-k-cut) or a permutation of the input (e.g.\ \ac{TSP}).
    They introduce an LSTM-based sequence-to-sequence architecture where the elements of the output sequence are positions in the input sequence, which they name pointer network.
    \citet{a-note-on-learning-algorithms-for-quadratic-assignment-with-gnns, an-efficient-gcn-technique-for-the-tsp} recognize that most common \ac{CO} problems have a natural graph representation.
    In their work, a \ac{GNN} predicts an approximate solution as a heatmap, which is then decoded into a valid solution to the \ac{CO} problem using beam search.
    \citet{neural-algorithmic-reasoning-for-co} follow a neural algorithmic reasoning approach to learn to imitate \ac{CO} solvers.
    The model is pre-trained on simple algorithms and then fine-tuned on difficult ones.
    Finally, \citet{difusco} uses graph-based denoising diffusion to generate high-quality solutions.
    However, these supervised approaches aren't applicable to such cases where calculating exact solutions for the training problems is not feasible.

    Several approaches have used \acf{RL} to remove this dependence on a labeled dataset.
    \citet{neural-co-with-rl} use a pointer network~\citep{pointer-networks}, but train it with \ac{RL}.
    \citet{learning-co-algorithms-over-graphs, attention-learn-to-solve-routing-problems} use the \ac{GNN} autoregressively to predict which node should be added next to the solution set and repeat that process until a valid solution is reached.
    \citet{solving-np-hard-problems-on-graphs-with-extended-alphago-zero} formulate the \ac{CO} problem as a Markov decision process, then use an algorithm similar to AlphaGo Zero~\citep{alpha-go-zero} to solve it autoregressively.
    \citet{symmetric-replay-training--sample-efficiency-rl-co} propose a modification to the \ac{RL} training process for autoregressive methods that improves sample efficiency.
    However, the use of the score function estimator in many of these methods leads to high-variance gradients, which makes training more difficult.


    Other self-supervised approaches that do not rely on reinforcement learning include \citet{augment-with-care}, which use a contrastive loss instead.
    \citet{co-with-physics-inspired-gnns} focus on quadratic and polynomial unconstrained binary optimization, which many \ac{CO} problems can be formulated as.
    This allows them to make use of a self-supervised loss function specific to these two problem families.
    \citet{diffusion-model-framework-for-unsupervised-nco} also concentrate on quadratic unconstrained binary optimization, but employ a diffusion-based approach similar to \citet{difusco}.
    Similarly, \citet{erdos-goes-neural, tackling-prevalent-conditions-in-unsupervised-co} formulate a self-supervised loss function for a comprehensive class of \ac{CO} problems.
    \citet{gnns-for-maximum-constraint-satisfaction} use an LSTM-based architecture to solve binary maximum constraint satisfaction problems, which many \ac{CO} problems can be formulated as.
    Since these approaches focus on certain families of \ac{CO} problems, they aren't general to \ac{CO} as a whole.

    
    \citet{exact-co-with-gcns} use a \ac{GNN} as a heuristic for the branch-and-bound algorithm, which guarantees exact solutions at the expense of a comparatively longer running time.
    The \ac{GNN} is trained via imitation learning from a known high-quality but slow heuristic.
    \citet{learning-tsp-requires-rethinking-generalization} compare some of the paradigms introduced in other papers in structured experiments.
    There have been two lines of work to address the problem of backpropagating through combinatorial optimization problems.
    Firstly, if we have a set of optimal solutions given as training data, we can use supervised learning to train the GNN to output adjacency matrices as close as possible to the optimal solutions \cite{elmachtoub2022smart}.
    This is often called ``predict, then optimize''.
    Secondly, there are several methods to backpropagate through a non-differentiable \ac{CO} algorithm, such as \citet{imle, adaptive-imle, differentiation-of-blackbox-co-solvers}. Related to our work is decision-focused learning, which has developed several methods to backpropagate through CO solvers \cite{DBLP:journals/corr/abs-2307-13565}.


\section{Background}

    \subsection{Combinatorial Optimization Problems}
    \label{sec:background-co-problems}

        A \acf{CO} problem asks us, given a discrete set $M$ and an objective function $J_\text{CO}: M \longrightarrow \mathbb{R}$, to find the minimum
        \[
            \min_{x \in M} J_\text{CO}(x).
        \]
        Of course, maximization problems can be turned into minimization problems by inverting the sign.
        Since finding the exact global optimum is often not necessary in practice, this paper focuses on finding approximate solutions.

        To illustrate our approach, we will refer to specific problems in combinatorial optimization as follows.
        
        \paragraph{Minimum $k$-cut problem.}
        We are given a connected, undirected graph $\gG = (V, E, w)$ with edge weights $w: E \longrightarrow \mathbb{R}_{> 0}$, as well as a desired number of clusters $k \in \mathbb{N}, 2 \leq k \leq |V|$.
        The goal is to find a set of edges $C \subseteq E$ with minimal total weight whose removal leaves $k$ connected components.
        This set is called a \emph{minimum $k$-cut}.

        Formally, we are optimizing
        \[
            \min_{C \subseteq E} \sum_{e \in C} w(e),
        \]
        \vspace{-2.5ex}
        \[
            \text{s.t.\ graph } (V, E \backslash C) \text{ has } k \text{ connected components}
        \]

        A commonly used approximation algorithm for solving the minimum $k$-cut problem is the Karger--Stein algorithm \cite{karger-stein}, which we describe in appendix~\ref{sec:algorithms-minimum-k-cut}.

        \paragraph{\Acf{TSP}.}
        The version of the \ac{TSP} most commonly experimented on in related literature~\citep{attention-learn-to-solve-routing-problems, an-efficient-gcn-technique-for-the-tsp, learning-tsp-requires-rethinking-generalization} is called Euclidean \ac{TSP}.
        We are given a complete, undirected graph $\gG = (V, E, w)$ whose nodes $V$ lie at points in the unit square, and whose edge weights $w$ are the Euclidean distances between the respective points.
        The goal is to find a minimum-weight Hamiltonian cycle, i.e. a cycle that visits every node exactly once and where the sum of the weights of edges that are traversed in the cycle is minimal.

        A well-known probabilistic approximation algorithm for solving the \ac{TSP} is the random insertion algorithm \cite{random-insertion}, which we describe in appendix~\ref{sec:algorithms-random-insertion}.



    \subsection{Residual Gated Graph Convnets}

        We use residual gated graph convnets \citep{residual-gated-graph-convnets}, but adapt them to include edge features $\bm{e}_{ij}^l$ and a dense attention map $\bm{\eta}_{ij}^l$ following \cite{an-efficient-gcn-technique-for-the-tsp}.
        The input node features $\bm{x}_i^0$ and edge features $\bm{e}_{ij}^0$ are first pre-processed using a single-layer \acs{MLP} for each of the two.
        Each further layer is computed as follows:
        \begin{align*}
            \bm{x}_i^{l+1} &= \bm{x}_i^l + \mathrm{ReLU} \Bigg( \mathrm{BN} \bigg( W_1^l \bm{x}_i^l + \sum_{j \in \mathcal{N}_i} \bm{\eta}_{ij}^l \odot W_2^l \bm{x}_j^l \bigg) \Bigg) \\
            &\hspace{1em} \text{ with } \bm{\eta}_{ij}^l = \frac{\sigma \big( \bm{e}_{ij}^l \big)}{\sum_{j' \in \mathcal{N}_i} \sigma \big( \bm{e}_{ij'}^l + \varepsilon \big)} \in \mathbb{R}^d, \\
            \bm{e}_{ij}^{l+1} &= \bm{e}_{ij}^l + \mathrm{ReLU} \Bigg( \mathrm{BN} \bigg( W_3^l \bm{e}_{ij}^l + W_4^l \bm{x}_i^l + W_5^l \bm{x}_j^l \bigg) \Bigg),
        \end{align*}
        where $W_1^l, \dots, W_5^l \in \mathbb{R}^{d \times d}$ are learnable weights, $d$ is the hidden dimension, $\mathrm{ReLU}$ is the rectified linear unit, $\mathrm{BN}$ is batch normalization, $\sigma = \frac{e^x}{1 + e^x}$ is the element-wise sigmoid function, and $\varepsilon$ is an arbitrary small value.
        $\odot$ denotes the Hadamard product, and $\mathcal{N}_i$ denotes the set of nodes that are adjacent to $i$.

        The final edge-level output is calculated from the last layer's edge features $\bm{e}_{ij}^l$ using another \acs{MLP}.
        For the remainder of the paper, $f(\gG) \in \mathbb{R}^{|E|}$ refers to applying this \ac{GNN} on a graph $\gG$.



    \subsection{REINFORCE}
    \label{sec:reinforce}

        Given a  function $J$ and a parameterized probability distribution $p_\theta(x)$, the REINFORCE algorithm \citep{reinforce}, also known as the score function estimator, estimates the true gradient as follows:
        \[
            \nabla_\theta \mathbb{E}_{y \sim p_\theta(x)} \big[ J(y) \big]
            = \mathbb{E}_{y \sim p_\theta(x)} \big[ J(y) \nabla_\theta \log p_\theta(x) \big]
        \]
        \[
            \approx \frac{1}{S} \sum_{i=1}^S J(y_i) \nabla_\theta \log p_\theta (x_i), \quad y_i \sim p_\theta(x_i)
        \]
        While this estimator is applicable even for discrete distributions and is unbiased, it suffers from large variance.
        \acf{I-MLE} \citep{imle} is another gradient estimator that produces biased estimates with smaller variance.


    \begin{figure*}
        \centering
        \includegraphics[width=0.8\textwidth]{figures/approach-overview.pdf}
        \caption{
            Overview of the training process.
            In the forward pass, the \ac{GNN} produces edge scores that are used as parameters for a \ac{CO} approximation algorithm.
            The approximation algorithm operates on these parameters by using them to scale the graph's edge weights and running an off-the-shelf approximation algorithm on the modified graph to obtain a solution $\hat{\bm{y}}$.
            Since the CO approximation algorithm is not differentiable in general, a gradient estimation scheme is used in the backward pass.
            We propose a novel scheme for this purpose, \acf{PBGE}.
            Note that there is no explicit loss function, since the gradient is estimated directly from the solution $\hat{\bm{y}}$.
        }
        \label{fig:approach-overview}
    \end{figure*}

\section{Problem Statement}
\label{sec:problem-statement}

    We consider \ac{CO} problems on graphs with a linear objective function $J_\text{CO}$ and a probabilistic approximation algorithm $h(\hat{\bm{y}} \mid \gG)$.
    The approximation algorithm takes as input a graph $\gG = (V, E, w)$ with nodes $V$, edges $E$ and edge weights $w: E \longrightarrow \mathbb{R}_{> 0}$, and returns (samples) a potentially suboptimal solution $\hat{\bm{y}}$.
    For instance, we might have the minimum $k$-cut problem as defined in \autoref{sec:background-co-problems}.

    We now want to use a GNN $f_{\bm{\theta}}$ parameterized by $\bm{\theta}$ applied to the input graphs $\gG$ to compute an updated graph $\gG' = f_{\bm{\theta}}(\gG)$ such that the probabilistic approximation algorithm when applied to this new graph is improved in expectation.

    Hence, we want to solve the following optimization problem:
    \[
        \min_{\bm{\theta}} \mathbb{E}_{\hat{\bm{y}} \sim h(\hat{\bm{y}} \mid f_{\bm{\theta}}(\gG))} \big[ J_\text{CO}(\hat{\bm{y}}) \big].
    \]

   For each input graph $\gG$, $h(\hat{\bm{y}} \mid f_{\bm{\theta}}(\gG))$ is a discrete probability distribution (due to the assumption that $h$ is probabilistic) parameterized by $\bm{\theta}$.
    The main challenge for solving this optimization problem is that the (discrete) approximation algorithms are typically not differentiable functions and that optimal solutions are prohibitively expensive to obtain as training data.
    Moreover, we assume that the approximation algorithm is a black box. This means that while we can sample from the probability distribution defined by it, we cannot calculate a probability for a given sample.

    Note that if the approximation algorithm available to us is deterministic, we can convert it into a probabilistic approximation algorithm by simply adding noise to the input, as demonstrated by e.g.\ \citet{imle}.
    For instance, this can take the form of adding noise from a Gumbel distribution to the edge weights of the input graph.



    With this paper, we propose \ac{PBGE}, a new approach based on preference-based learning. It estimates gradients by contrasting better and worse solutions, similar to preference-based learning in the context of large language models. 



\section{Method}
\label{sec:method}



    \autoref{fig:approach-overview} shows an overview of our approach.
    We augment an existing probabilistic approximation algorithm for a given \ac{CO} problem using a \ac{GNN}.
    The \ac{GNN} receives the problem graph as input and produces a prior score for each edge.
    These scores are used as additional input alongside the graph for a parameterized version of an off-the-shelf \ac{CO} approximation algorithm, which then produces a solution to the \ac{CO} problem.

    This same pipeline is used both during training and testing.
    Since the approximation algorithm is not differentiable in general, we use gradient estimation to obtain the gradients with respect to the \ac{GNN}'s output.
    Existing gradient estimation schemes such as REINFORCE or \ac{I-MLE} can be used for this, as introduced in \autoref{sec:reinforce}.
    We propose a new scheme for this purpose: \acf{PBGE}.
    The remainder of this section elaborates on our proposed training procedure as well as \ac{PBGE}.



    \subsection{Parameterizing Approximation Algorithms}

        A given \ac{CO} approximation algorithm that takes a problem graph as input can be parameterized by modifying the input graph, then running the approximation algorithm on the modified graph.

        We calculate the modified graph $\gG'$ by using the \acs{GNN}'s output to change the edge weights.
        Assume there is an arbitrary but fixed ordering of edges.
        The model outputs a prior score for each edge, $\bm{s} = f_{\bm{\theta}}(\gG) \in \mathbb{R}^{|E|}$.
        A high score for a given edge is interpreted to mean that the respective edge should belong to the solution set with a higher probability mass.
        The approximation algorithms we use prefer including edges of low weight in the solution set. Therefore, we scale down the weights of edges that received high scores.
        Specifically, the edge weights are multiplied with $1 - \sigma(\bm{s})$, where $\sigma$ is the element-wise sigmoid function.
        By running the \ac{CO} approximation algorithm on this modified graph, we parameterize the approximation algorithm using the \acs{GNN}'s output scores $\bm{s}$.
        In the formulas in the remainder of this paper, $h(\hat{\bm{y}} \mid \gG, \bm{s})$ represents the probability distribution defined by a probabilistic \ac{CO} approximation algorithm parameterized in this way.
        It samples and outputs a vector $\hat{\bm{y}} \in \{ 0, 1 \}^{|E|}$ that represents a solution to the \ac{CO} problem, such as a TSP tour or $k$-cut.
        A value of 1 in this vector indicates that the corresponding edge is in the solution set.



    \subsection{Preference-Based Gradient Estimation (PBGE)}
    \label{sec:preference-based-gradient-estimation}

        In preference learning, a training instance consists of an input and a pair of possible outputs.
        The supervision signal is an annotation indicating that one of the outputs $\bm{y}_w$ is of higher quality than the other output $\bm{y}_l$.
        We can construct a similar setup for \acl{CO} by leveraging a pre-existing probabilistic \ac{CO} approximation algorithm $h$.
        Sampling from the approximation algorithm multiple times likely yields two solutions $\hat{\bm{y}}_1, \hat{\bm{y}}_2 \sim h(\hat{\bm{y}} \mid \gG, \bm{s})$ of different quality for a given problem instance $\gG \sim \mathcal{D}$ from dataset $\mathcal{D}$.
        These solutions can easily be ranked by applying the \ac{CO} problem's objective function%
        \footnote{Note that $J_\mathrm{CO}$ depends on a graph $\gG$ and a predicted solution $\hat{\bm{y}}$, though we omit the graph parameter for readability}
        $J_\mathrm{CO}$.
        This means assigning $\hat{\bm{y}}_w$ and $\hat{\bm{y}}_l$ such that $J_\mathrm{CO}(\hat{\bm{y}}_w) < J_\mathrm{CO}(\hat{\bm{y}}_l)$.

        We now propose the following preference-based loss function:
        \begin{align}
            \begin{split}
                \label{eq:initial-preference-learning-loss}
                \mathcal{L}(\mathcal{D}, \bm{s}) =
                &\; \mathbb{E}_{\hat{\bm{y}}_w, \hat{\bm{y}}_l \sim h(\hat{\bm{y}} \mid \gG, \bm{s}), \: \gG \sim D} \\
                &\left[
                    d(\hat{\bm{y}}_w, \hat{\bm{y}}_l)
                    \log \left( \frac{
                        h(\hat{\bm{y}}_w \mid \gG, \bm{s})
                    }{
                        h(\hat{\bm{y}}_l \mid \gG, \bm{s})
                    } \right)
                \right].
            \end{split}
        \end{align}
        Here, $d(\hat{\bm{y}}_w, \hat{\bm{y}}_l)$ is a scaling factor%
        \footnote{The graph $\gG$ is also a parameter to $d$, but we omit it in the notation for readability}.
        As we will see later, its purpose is to scale the gradients based on the distance between the objective values of the losing and winning solutions.

        Since we treat the \ac{CO} approximation algorithm as a black box, we cannot calculate the probabilities $h(\hat{\bm{y}}_w \mid \gG, \bm{s})$ and $h(\hat{\bm{y}}_l \mid \gG, \bm{s})$ directly.
        We therefore introduce a proxy distribution $\pi(\hat{\bm{y}} \mid \gG, \bm{s}) \approx h(\hat{\bm{y}} \mid \gG, \bm{s})$ for which we can obtain  probabilities directly.
        For all approximation algorithms we use in this paper, a high prior score in $\bm{s}$ for a certain edge increases the probability of this edge being included in the output $\hat{\bm{y}}$.
        This motivates the use of an exponential family distribution to model the proxy distribution $\pi$ for $h$:
        \begin{equation}
            \label{eq:pi-definition}
            \pi(\hat{\bm{y}} \mid \gG, \bm{s}) = \frac{\exp(- \langle \hat{\bm{y}}, \bm{s} \rangle)}{\sum_{\bm{y}' \in \mathcal{C}} \exp(- \langle \bm{y}', \bm{s} \rangle)},
        \end{equation}
        where $\langle \cdot, \cdot \rangle$ is the inner product and $\mathcal{C}$ is the set of all solutions to the \ac{CO} problem.

        Replacing $h$ with $\pi$ in \autoref{eq:initial-preference-learning-loss} and inserting \autoref{eq:pi-definition} simplifies the loss function to
        \begin{align}
            \begin{split}
                \mathcal{L}(\mathcal{D}, \bm{s}) =
                &\; \mathbb{E}_{\bm{\hat{y}}_w, \bm{\hat{y}}_l \sim h(\hat{\bm{y}} \mid \gG, \bm{s}), \: \gG \sim D} \\
                &\Big[
                    d(\hat{\bm{y}}_w, \hat{\bm{y}}_l)
                    \big( \langle \bm{\hat{y}}_l, \bm{s} \rangle - \langle \bm{\hat{y}}_w, \bm{s} \rangle \big)
                \Big].
            \end{split}
        \end{align}
        Now, the gradient of this expectation w.r.t. $\bm{s}$ is 
        \begin{align}
            \begin{split}
                \label{eq:loss-derivative}
                \nabla_{\bm{s}} \mathcal{L}(\mathcal{D}, \bm{s}) =
                &\; \mathbb{E}_{\bm{\hat{y}}_w, \bm{\hat{y}}_l \sim h(\hat{\bm{y}} \mid \gG, \bm{s}), \: \gG \sim D} \\
                &\big[
                    d(\hat{\bm{y}}_w, \hat{\bm{y}}_l)
                    (\bm{\hat{y}}_l - \bm{\hat{y}}_w)
                \big],
            \end{split}
        \end{align}
        whose single-sample Monte Carlo estimate can be written as
        \begin{align}
            \begin{split}
                \nabla_{\bm{s}} \mathcal{L}(\gG, \bm{s})
                &\approx d(\hat{\bm{y}}_w, \hat{\bm{y}}_l) (\hat{\bm{y}}_l - \hat{\bm{y}}_w), \\
                &\quad \text{where } \hat{\bm{y}}_w, \hat{\bm{y}}_l \sim h(\hat{\bm{y}} \mid \gG, \bm{s}).
            \end{split}
        \end{align}
        Intuitively, the gradient is $-1$ at a certain edge if that edge is in the better solution, but not in the worse solution.
        A gradient of $-1$ raises the \ac{GNN}'s output score, meaning that the \ac{GNN} will be nudged towards including this edge in its solution.
        Similarly, a gradient of $1$ means that the corresponding edge was in the worse solution, but not in the better solution.
        A positive gradient nudges the \ac{GNN}'s output down, so it pushes the \ac{GNN} towards not including this edge.

        This gradient is similar to the ones used for preference learning with \acp{LLM} \cite{direct-preference-optimization, meng2024simpo}:
        the gradient nudges the model to increase the likelihood of the better solution and to decrease the likelihood of the worse solution,
        and a scaling factor is used to weight important gradients more highly.
        Unlike the preference learning setting used with \acp{LLM}, we not only know which solution in a pair is better, but we can measure the quality of each solution exactly using the objective function.
        This means that we don't need to rely on human annotators to rank pairs of examples.
        On top of this, we can leverage the objective function to more easily compute a suitable scaling factor.



        \paragraph{Gradient scaling.}
        If the solutions $\hat{\bm{y}}_w$ and $\hat{\bm{y}}_l$ are of similar quality, we do not want to strongly nudge the \ac{GNN} towards either solution.
        We therefore opt for scaling the gradient with the relative gap in quality between the two solutions,
        \[
            d(\hat{\bm{y}}_w, \hat{\bm{y}}_l) = \frac{J_\text{CO}(\hat{\bm{y}}_l)}{J_\text{CO}(\hat{\bm{y}}_w)} - 1.
        \]
        Note that this is always positive, because $J_\text{CO}(\hat{\bm{y}}_l) > J_\text{CO}(\hat{\bm{y}}_w)$.
        Using this scaling factor puts a larger emphasis on pairs of solutions where the difference in quality is large.
        In particular, if the two solutions are of the same quality, the gradient is set to zero, so we do not nudge the \ac{GNN} towards either solution.
        The scaled gradient is
        \[
            \nabla_{\bm{s}} \mathcal{L}(\gG, \bm{s})
            \approx \left( \frac{J_\text{CO}(\hat{\bm{y}}_l)}{J_\text{CO}(\hat{\bm{y}}_w)} - 1 \right)
            \big( \hat{\bm{y}}_l - \hat{\bm{y}}_w \big).
        \]

        The variance of the gradient can be decreased estimating the expectation in \autoref{eq:loss-derivative} with more than one sample.
        However, this requires sampling two additional solutions from the approximation algorithm for each additional pair.
        We can increase the number of pairs more efficiently by first sampling a pool of solutions from the approximation algorithm, then constructing pairs from this pool.
        Since we want a gradient that nudges the model towards the best known solution, we form the pairs by combining the best solution from the pool with each of the weaker solutions.

        In practice, the accuracy of the gradients depends heavily on the quality of the best found solution $\hat{\bm{y}}_w$.
        In our experiments, we noticed that early during training, the \ac{GNN} cannot yet output good enough scores to consistently find reasonable $\hat{\bm{y}}_w$.
        To remedy this, we also run the approximation algorithm on the unmodified graph and add the resulting solutions to the pool from which the pairs are generated.

        The complete training procedure is described in Algorithm~\ref{alg:training-with-pbge}.

        \begin{algorithm}[t!]
            \caption{One training iteration with \ac{PBGE}}
            \label{alg:training-with-pbge}
            \begin{algorithmic}
                \STATE $\bm{s} \gets f_{\bm{\theta}}(\gG)$
                    \quad \COMMENT{Evaluate the GNN}
                \medskip
                \STATE \COMMENT{Sample $n$ solutions from approximation algorithm $h$, guided by GNN output $\bm{s}$, and $m$ solutions without guidance from $\bm{s}$}
                \STATE $\hat{\bm{y}}_1, \dots, \hat{\bm{y}}_n \sim h(\hat{\bm{y}} \mid \gG, \bm{s})$
                \STATE $\hat{\bm{y}}_{n+1}, \dots, \hat{\bm{y}}_{n+m} \sim h(\hat{\bm{y}} \mid \gG)$
                \medskip
                \STATE $\hat{\bm{y}}_w \gets \argmin_{\hat{\bm{y}} \in \{\hat{\bm{y}}_1, \dots, \hat{\bm{y}}_{n+m}\}} J_\mathrm{CO}(\hat{\bm{y}})$
                \STATE $\nabla_{\bm{s}} \mathcal{L} \gets \bm{0}$
                \medskip
                \STATE \COMMENT{Create pairs with $\hat{\bm{y}}_w$ and each other solution}
                \FOR{$\hat{\bm{y}}_l \in \{\hat{\bm{y}}_1, \dots, \hat{\bm{y}}_{n+m}\}$}
                    \STATE $\nabla_{\bm{s}} \mathcal{L} \gets \nabla_{\bm{s}} \mathcal{L} + \left( \frac{J_\text{CO}(\hat{\bm{y}}_l)}{J_\text{CO}(\hat{\bm{y}}_w)} - 1 \right) \hspace{-0.3em} \big( \hat{\bm{y}}_l - \hat{\bm{y}}_w \big)$
                \ENDFOR
                \medskip
                \STATE Backpropagate gradient $\nabla_{\bm{s}} \mathcal{L}$
            \end{algorithmic}
        \end{algorithm}




    \subsection{Decoding at Test Time}

        Regardless of how the model was trained, at test time, the model's output needs to be converted (decoded) to a solution to the \ac{CO} problem.
        This can simply be done by running the \ac{CO} approximation algorithm with the model's output as input, as described in \autoref{sec:problem-statement}.
        The solution can be improved by running a probabilistic \ac{CO} approximation algorithm repeatedly and using the best solution found as final output.
        Other decoders (like beam search) are also available, but haven't shown promising results in preliminary experiments.

        In the case of the Karger--Stein algorithm, we noticed empirically that simply modifying the input graph can lead to degenerate behavior during testing.
        The Karger--Stein algorithm uses the graph's edge weights in two places:
        (1) when sampling an edge for contraction and
        (2) when comparing the cuts that resulted from different recursion arms.
        We noticed that the performance of our overall method can be improved when using a model trained with the setting described in \autoref{sec:preference-based-gradient-estimation} by using the modified edge weights for the first case and the original edge weights for the second case.
        Intuitively, if the GNN makes a mistake when scaling the edge weights, using the original edge weights for comparing cuts can allow the Karger--Stein algorithm to find the optimal cut regardless.

        For the algorithms we use in our experiments, we prove the following theoretical result, which shows that we can turn an approximation algorithm into an exact algorithm if we find an optimal modified input graph for Karger-Stein or random insertion. 

        \begin{theorem}
            Let $h$ be the Karger-Stein algorithm for the minimum $k$-cut problem or the random insertion algorithm for the \ac{TSP}.
            Let $\bm{y} \in \{0, 1\}^{|E|}$ be an optimal solution to the respective problem on graph $\gG$.
            Then,
            \[
                \lim_{\sigma(\bm{s}) \to \bm{y}} h(\bm{y} \mid \gG, \bm{s}) = 1.
            \]
        \end{theorem}

        \vspace{-1.5ex}
        In other words, in the limit, the approximation algorithm guided by $\bm{s}$ always finds optimal solution $\bm{y}$.
        The proofs can be found in \autoref{sec:proofs}.




\section{Experiments}

    We validate our approach on two well-known combinatorial optimization problems: the \acf{TSP} and the minimum $k$-cut problem. For both problems, we synthetically generate problem instances and establish baselines as reference.


        \begin{table*}
        \small
            \centering
            \begin{tabular}{lcc@{\hskip 2em}cc}
                \toprule
                                                  & \multicolumn{2}{c}{Unweighted graphs \hspace*{2em}} & \multicolumn{2}{c}{NOIgen+ \hspace*{2em}} \\
                Method                            & Single run & Best out of 3 runs & Single run & Best out of 3 runs \\
                \midrule
                \textbf{Non-learned}              &              &                                 &                                 \\
                \quad Karger--Stein        & 3.66\%$\pm$0.18 (330ms) & 0.59\%$\pm$0.05 (971ms) & 10.84\%$\pm$0.40 (352ms) & 0.48\%$\pm$0.11 (1.00s) \\
                \midrule
                \textbf{Supervised}                &         &                &                &                \\
                \quad (BCE loss)                   & 0.28\%$\pm$0.06 (402ms) & 0.03\%$\pm$0.02 (1.05s) & 0.38\%$\pm$0.06 (415ms) & 0.05\%$\pm$0.04 (1.04s) \\
                \quad (I-MLE)                      & 1.64\%$\pm$0.09 (415ms) & 0.15\%$\pm$0.05 (1.03s) & 2.49\%$\pm$0.10 (435ms) & 0.26\%$\pm$0.07 (1.08s) \\
                \textbf{Self-supervised}           &     &                &                &                \\
                \quad (I-MLE)                      & 3.47\%$\pm$0.14 (402ms) & 0.51\%$\pm$0.06 (1.01s) & 7.12\%$\pm$0.35 (444ms) & 0.39\%$\pm$0.06 (1.10s) \\
                \quad (PBGE, \textbf{ours})        & 0.39\%$\pm$0.05 (398ms) & 0.06\%$\pm$0.04 (1.01s) & 0.57\%$\pm$0.06 (439ms) & 0.08\%$\pm$0.05 (1.08s) \\
                \bottomrule
            \end{tabular}
            \caption{
                Minimum $k$-cut optimality gaps on graphs with 100 nodes, using Karger-Stein as decoder.
                Mean $\pm$ standard deviation were calculated over five evaluation runs on the same model parameters.
                In the supervised and self-supervised rows, the parentheses indicate how the \ac{GNN} was trained.
                In the columns labelled ``Best out of 3 runs'', the Karger--Stein algorithm is run three times on the same GNN outputs, and the best result is used.
            }
            \label{tab:minimum-k-cut-optimality-gaps}
        \end{table*}
        

    \subsection{Problem Instance Generation}
    \label{sec:graph-generation}

        For minimum $k$-cut, we use the established graph generator \emph{NOIgen}~\citep{noigen}.
        Since it relies on dramatically scaling down the weights of edges that are in the minimum $k$-cut in order to avoid trivial solutions, it makes it easy for a \ac{GNN} to identify the correct edges.
        To make the graphs more challenging, we extend NOIgen to also use graph structure to avoid trivial solutions, which allows us to scale down edge weights less dramatically.
        We call this improved graph generator \emph{NOIgen+}.
        We also generate unweighted graphs that only rely on graph structure to prevent trivial solutions.
        Graphs for the \ac{TSP} are generated according to the established method described in \citet{attention-learn-to-solve-routing-problems, an-efficient-gcn-technique-for-the-tsp, learning-tsp-requires-rethinking-generalization}.
        Please refer to appendix~\ref{sec:graph-generation-appendix} for further details.

        \begin{figure}[t!]
            \begin{subfigure}{\linewidth}
                \includegraphics[width=\textwidth]{figures/karger-minimum-k-cuts-found-after-runs--on-simple-graph-plus--n-100--k-2--max-cut-value-30--line.pdf}
                \caption{On graphs without edge weights and 100 nodes.}
                \label{fig:karger-stein-num-runs-until-minimum-found-n-100}
            \end{subfigure}
            \hfill
            \begin{subfigure}{\linewidth}
                \includegraphics[width=\textwidth]{figures/karger-minimum-k-cuts-found-after-runs--on-simple-graph-plus--n-200--k-2--max-cut-value-30--line.pdf}
                \caption{On graphs without edge weights and 200 nodes.}
                \label{fig:karger-stein-num-runs-until-minimum-found-generalising-to-n-200}
            \end{subfigure}

            \caption{
                The number of graphs for which the minimum $k$-cut was found after a given number of Karger-Stein runs.
                For example, for Karger-Stein without a \ac{GNN} on graphs with 100 nodes (\autoref{fig:karger-stein-num-runs-until-minimum-found-n-100}), two Karger-Stein runs suffice to find the minimum $k$-cut for 83\% of graphs in the validation set.
                The ideal algorithm, which always finds the minimum $k$-cut on the first attempt, would have simply a horizontal line at 100\%.
            }
            \label{fig:karger-stein-num-runs-until-minimum-found}
        \end{figure}
        



    \subsection{Baselines}
    \label{sec:baselines}

        Since we assume a setting without access to ground truth solutions to the \ac{CO} problems, our primary baselines are gradient estimation schemes for unsupervised training.
        We set the loss to
        \[
            \mathcal{L}(\mathcal{D}, \bm{s}) = \mathbb{E}_{\hat{\bm{y}} \sim h(\hat{\bm{y}} \mid \gG, \bm{s}), \: \gG \sim D}
            \big[ J_\mathrm{CO}(\hat{\bm{y}}) \big]
        \]
        and estimate $\nabla_{\bm{s}} \mathcal{L}(\mathcal{D}, \bm{s})$ using REINFORCE \citep{reinforce} in the case of \ac{TSP} and \ac{I-MLE} \citep{imle} in the case of the minimum $k$-cut problem.
        Additionally, for \ac{TSP}, we also compare against a simple baseline that runs random insertion on the input graph 20 times and treats the best solution found as ground truth for a binary cross-entropy loss.
        We call this baseline ``Best-of-20''.

        For additional context, we train models in a supervised fashion using an edge-level binary cross-entropy loss comparing the \acs{GNN}'s output scores $\bm{s}$ with a pre-calculated ground truth solution $\bm{y}$.
        For minimum $k$-cut, we also train using \ac{I-MLE}, comparing the approximation algorithm's output $\hat{\bm{y}}$ with $\bm{y}$ using a Hamming loss.
        See \autoref{sec:baseline-details} for details on these baselines.



    \subsection{Minimum \texorpdfstring{$k$}{k}-Cut}

        We evaluate our method on the minimum $k$-cut problem, using the Karger--Stein algorithm as a base.
        \autoref{tab:minimum-k-cut-optimality-gaps} shows optimality gaps of the unmodified Karger--Stein algorithm, as well as several versions of our method.
        Each version augments the Karger--Stein algorithm with a \ac{GNN}, and they differ by how the \ac{GNN} was trained.
        Note that when augmenting the Karger--Stein algorithm with a \ac{GNN} trained with PBGE, the optimality gap improves by an order of magnitude.
        On top of this, even though it didn't use any ground truth solutions during training, the \ac{GNN} trained with \ac{PBGE} comes close to matching the \ac{GNN} trained supervised with a binary cross entropy loss.

        In practice, the most important metric is the number of runs it takes for Karger--Stein to find the optimal $k$-cut.
        If this number is low, we can run Karger--Stein a small number of times and be reasonably certain that the minimum $k$-cut was found.
        \autoref{fig:karger-stein-num-runs-until-minimum-found} shows for how many graphs the minimum $k$-cut is found in a set number of runs, comparing the unmodified Karger--Stein algorithm with two versions that were augmented using a \ac{GNN}.
        On both datasets, the augmented Karger--Stein algorithm needs much fewer runs to find the minimum $k$-cut, almost always finding it on the first attempt.
        Again, the \ac{GNN} trained self-supervised with PBGE comes close to matching supervised performance.



    \subsection{Travelling Salesman Problem (TSP)}



        \begin{table*}[t!]
        \small
            \centering
            \begin{tabular}{llll}
                \toprule
                Method (decoder in parentheses)            & $n = 20$        & $n = 50$        & $n = 100$       \\
                \midrule
                \textbf{Self-supervised}                   &                 &                 &                 \\
                This work                                  &                 &                 &                 \\
                \quad REINFORCE (random ins., 20 runs)       & 8.02\%$\pm$0.07 (769ms)          & 23.94\%$\pm$0.11 (11.62s) & 52.61\%$\pm$0.42 (1.38m) \\
                \quad REINFORCE (random ins., 100 runs)      & 4.75\%$\pm$0.03 (3.64s)          & 11.23\%$\pm$0.08 (57.85s) & 31.09\%$\pm$0.26 (7.53m) \\
                \quad Best-of-20 (random ins., 20 runs)      & 0.39\%$\pm$0.03 (787ms)          & 11.97\%$\pm$0.69 (11.03s)       & --              \\
                \quad Best-of-20 (random ins., 100 runs)     & 0.10\%$\pm$0.01 (3.70s)          & 5.28\%$\pm$0.79 (52.82s) & --              \\
                \quad PBGE \textbf{(ours)} (random ins., 20 runs)     & 0.18\%$\pm$0.01 (763ms)          & 2.36\%$\pm$0.02 (11.11s) & 5.15\%$\pm$0.04 (1.43m) \\
                \quad PBGE \textbf{(ours)} (random ins., 100 runs)    & 0.05\%$\pm$0.01 (3.73s) & 1.14\%$\pm$0.03 (53.98s) & 3.67\%$\pm$0.05 (7.44m) \\
                \citet{learning-co-algorithms-over-graphs} &                &                 &                 \\
                \quad (greedy)                             & 1.42\%*         & 5.16\%*         & 7.03\%*         \\
                \citet{attention-learn-to-solve-routing-problems} &                &                 &                 \\
                \quad (greedy)                             & 0.34\%*         & 1.76\%* (2s)    & 4.53\%* (6s)    \\
                \quad (sampling 1280 times)                & 0.08\%* (5m)    & 0.52\%* (24m)   & 2.26\%* (1h)    \\
                \midrule
                \textbf{Supervised}                  &                 &                 &                 \\
                This work                                  &                 &                 &                 \\
                \quad BCE loss (random insertion, 20 runs)       & 0.15\%$\pm$0.01 (787ms)          & 0.94\%$\pm$0.03 (10.92s) & 2.86\%$\pm$0.04 (1.47m) \\
                \quad BCE loss (random insertion, 100 runs)      & 0.04\%$\pm$0.00 (3.75s)          & 0.58\%$\pm$0.02 (54.79s) & 1.74\%$\pm$0.03 (7.35m) \\
                \citet{an-efficient-gcn-technique-for-the-tsp} &             &                 &                 \\
                \quad (greedy)                             & 0.60\%* (6s)    & 3.10\%* (55s)   & 8.38\%* (6m)    \\
                \quad (beam search, beam width 1280)            & 0.10\%* (20s)   & 0.26\%* (2m)    & 2.11\%* (10m)   \\
                \quad (beam search, width 1280 + heuristic)       & 0.01\%* (12m)   & 0.01\%* (18m)   & 1.39\%* (40m)   \\
                \citet{difusco}                            &                 &                 &                 \\
                \quad (greedy)                             &                 & 0.10\%*         & 0.24\%*         \\
                \quad (sampling 16 times)                  &                 & 0.00\%*         & 0.00\%*         \\
                \midrule
                \textbf{Non-learned approximation algorithms} &              &                 &                 \\
                Christofides                               & 8.72\%$\pm$0.00 (45ms) & 11.07\%$\pm$0.00 (685ms) & 11.86\%$\pm$0.00 (4.45s) \\
                Random Insertion                           & 4.47\%$\pm$0.04 (41ms) & 7.64\%$\pm$0.12 (575ms)  & 9.63\%$\pm$0.11 (4.34s) \\
                Farthest Insertion                         & 2.38\%$\pm$0.00 (57ms) & 5.50\%$\pm$0.00 (909ms)  & 7.58\%$\pm$0.00 (5.68s) \\
                LKH3                                       & 0.00\%* (18s)          & 0.00\%* (5m)             & 0.00\%* (21m)   \\
                \midrule
                \textbf{Exact solvers}                     &                 &                 &                 \\
                Concorde~\citep{concorde}                  & 0.00\%* (1m)    & 0.00\%* (2m)    & 0.00\%* (3m)    \\
                Gurobi                                     & 0.00\%* (7s)    & 0.00\%* (2m)    & 0.00\%* (17m)   \\
                \bottomrule
            \end{tabular}
            \caption{
                TSP optimality gaps, with mean $\pm$ standard deviation, calculated over five evaluation runs on the same model parameters.
                Results marked with * are values obtained from the indicated papers and therefore do not include standard deviations.
            }
            \label{tab:tsp-optimality-gaps}
        \end{table*}

        \autoref{tab:tsp-optimality-gaps} shows the optimality gaps of our approach and its variants.
        All of our models were trained using random insertion as the \ac{CO} approximation algorithm.
        For \ac{PBGE}, we sampled 10 solutions from $h(\hat{\bm{y}} \mid \gG, \bm{s})$ and 10 solutions from $h(\hat{\bm{y}} \mid \gG)$.

        The decoder used at test time is listed after the name of the respective method in parentheses.
        Greedy search starts from an arbitrary node, and follows the edge with the highest score to select the next node.
        This process of greedily following the best edge is repeated until each node has been visited once.
        To ensure that the resulting tour is valid, edges that lead to nodes that have already been visited are excluded.
        Beam search also starts with an arbitrary node, then explores the $b$ edges with the highest scores.
        This gives us $b$ partial solutions.
        In each iteration, each partial solution is expanded at its last node, and out of the resulting paths, the $b$ best partial solutions are kept.
        As before, edges that would lead to invalid tours are ignored.
        The parameter $b$ is called the \emph{beam width}, and beam search with $b = 1$ corresponds to greedy search.
        Sampling simply refers to sampling multiple solutions and using the best one.




\section{Conclusion}

    We introduced a method to improve existing approximation algorithms for \ac{CO} using \ac{GNN}s.
    The \ac{GNN} predicts parameters, which are used as input for the non-learned approximation algorithm to produce a high-quality solution to the \ac{CO} problem.
    The GNN is trained based on the \ac{CO} problem's downstream objective, without the need for labelled data.
    To achieve this, we used gradient estimation to backpropagate through the approximation algorithm.
    We proposed a novel gradient estimation scheme for this purpose, which we called \acf{PBGE}.

    \paragraph{Limitations and future work.}
    Incorporating a \ac{CO} approximation algorithm during training means that the training process is more computationally intensive compared to competing approaches.
    This also means that an existing approximation algorithm is required for our approach.
    We only experimented on \ac{CO} problems for which solutions can be represented in terms of the graph's edges.
    While extending our approach to other kinds of \ac{CO} problems is theoretically possible, we leave this for future work.
    We also plan to incorporate the ability to formulate additional constraints.



\section*{Acknowledgements}

    We thank Aneesh Barthakur, Matteo Palmas and Roman Freiberg for fruitful discussions.
    The authors thank the International Max Planck Research School for Intelligent Systems (IMPRS-IS) for supporting Arman Mielke.
    The contributions of Thilo Strauss were carried out while employed with ETAS Research.


\section*{Impact Statement}

    This paper presents work whose goal is to advance the field of Machine Learning.
    There are many potential societal consequences of our work, none which we feel must be specifically highlighted here.



\bibliography{references}
\bibliographystyle{icml2025}




\newpage
\appendix
\onecolumn  



\section{Algorithms for Specific Combinatorial Optimization Problems}

    \subsection{Minimum \texorpdfstring{$k$}{k}-Cut Problem}
    \label{sec:algorithms-minimum-k-cut}

        \subsubsection{Karger's Algorithm}
            Karger's algorithm~\citep{karger} is a Monte Carlo algorithm for the minimum $k$-cut problem.

            The algorithm is based on the \emph{contraction operation}:
            An edge $e = \{ x, y \}$ is contracted by merging its nodes $x$ and $y$ into a new node $xy$.
            For clarity, we will call the node that results from this merger a meta-node.
            Every edge that was incident to exactly one of the two merged nodes is now altered to instead be incident to the meta-node $xy$:
            An edge $\{ x, z \}$ or $\{ y, z \}$ becomes $\{ xy, z \}$.
            This may result in parallel edges, meaning that the resulting graph is a multigraph.
            All edges $\{ x, y \}$ are removed, so that the resulting multigraph contains no self-loops.

            Karger's algorithm works by repeatedly sampling an edge, where the probability of each edge is proportional to its weight, then contracting that edge.
            This is repeated until there are only $k$ nodes left.
            Each of these remaining $k$ meta-nodes represents a connected component in the original graph, with each node of the original graph that was subsequently been merged into that meta-node belonging to this connected component.
            Any edge in the original graph that spans between two connected components is cut.

            Since this algorithm is not guaranteed to find the minimum $k$-cut, a common strategy is to run the algorithm repeatedly and use the smallest found cut as the final result.


        \subsubsection{Karger--Stein Algorithm}

            The Karger--Stein algorithm~\citep{karger-stein} is a recursive version of Karger's algorithm, shown in Algorithm~\ref{alg:karger-stein}.

            \begin{algorithm}[h!]
                \caption{Karger--Stein algorithm}
                \label{alg:karger-stein}
                \begin{algorithmic}
                    \STATE \textsc{Karger-Stein}
                    \STATE \textbf{Input:} connected, undirected graph $\gG = (V, E, w)$
                    \IF{$|V| \leq 6$}
                        \STATE \textbf{return} $\textsc{Contract}(\gG, 2)$
                    \ELSE
                        \STATE target $t \gets \left\lceil \displaystyle\frac{|V|}{\sqrt{2}} + 1 \right\rceil$
                        \STATE $\gG_1 \gets \textsc{Contract}(\gG, t)$
                        \STATE $\gG_2 \gets \textsc{Contract}(\gG, t)$
                        \STATE \textbf{return} $\min \! \big\{ \textsc{Karger-Stein}(\gG_1), \; \textsc{Karger-Stein}(\gG_2) \big\}$ \COMMENT{Return the lower-weight cut}
                    \ENDIF
                    \bigskip
                    \STATE \textsc{Contract}
                    \STATE \textbf{Input:} connected, undirected graph $\gG = (V, E, w)$, target number of nodes $t$
                    \WHILE{$|V| > t$}
                        \STATE $\gG \gets$ sample edge in $\gG$ and contract it
                    \ENDWHILE
                    \STATE \textbf{return} $\gG$
                \end{algorithmic}
            \end{algorithm}



    \subsection{Random Insertion for the Travelling Salesman Problem}
    \label{sec:algorithms-random-insertion}

        The random insertion algorithm~\citep{random-insertion} is a Monte Carlo algorithm for the \ac{TSP}.
        
        Since a Hamiltonian cycles to a given graph $\gG = (V, E)$ is required to contain every $v \in V$ exactly once, it is a straightforward approach to iteratively sample and remove nodes from $V$ until it is empty.
        The random insertion algorithm, as suggested by Karg and Thompson, begins by selecting two nodes $s,t \in V$ at random and adds the edges $(s,t)$ and $(t,s)$ to an initial cycle.
        In order to extend the cycle to include all nodes, the algorithm now samples a node $v \in V \setminus \{s,t\}$ and selects the edges $(x,v)$ and $(v,y)$ such that $x$ and $y$ are already part of the partial cycle with $x \neq y$ and such that the sum of the metric distances of $(x,v)$ and $(v,y)$ is minimal.
        
        The cycles obtained in this way are at most $(\lceil \log_2 |V| \rceil + 1)$ times longer than the optimal cycle \cite{insertion-algorithms-approximation-ratio}.

        The algorithm is summarized in Algorithm~\ref{alg:random-insertion}

            \begin{algorithm}[h!]
                \caption{Random insertion}
                \label{alg:random-insertion}
                \begin{algorithmic}
                    \STATE \textbf{Input:} connected, undirected graph $\gG = (V, E, w)$
                    \STATE $\mathcal{T} \gets$ \ac{TSP} tour consisting of one random node
                    \STATE $v \gets$ sample node in $V$ that is not part of $\mathcal{T}$ yet
                    \STATE $\mathcal{T} \gets$ insert $v$ into $\mathcal{T}$ to form a loop of two nodes

                    \FOR{$i \in \{1, \ldots, |V|\}$}
                        \STATE $v \gets$ sample node in $V$ that is not part of $\mathcal{T}$ yet
                        \STATE $\mathcal{T} \gets$ insert $v$ into $\mathcal{T}$ at the point in the tour $\mathcal{T}$ where it increases the tour's length by the least amount
                               \am{can describe this more precisely using formulas (look at \cite{insertion-algorithms-approximation-ratio})}
                    \ENDFOR
                    \STATE \textbf{return} $\gG$
                \end{algorithmic}
            \end{algorithm}

        \am{TODO}




\section{Proofs}
\label{sec:proofs}

    We assume that the edges $E$ of a graph $\gG = (V, E, w)$ are in an arbitrary but fixed order.
    This means that the scores assigned to the edges by a \ac{GNN} can be represented as a vector $\bm{s} \in \mathbb{R}^{|E|}$.
    We use $\bm{s}[e]$ to denote the score assigned to a specific edge $e \in E$.
    A subset of edges $\hat{\bm{y}}$ can be represented as $\hat{\bm{y}} \in \{0, 1\}^{|E|}$, where a 1 indicates that the respective edge is in the set.
    For notational simplicity, we will still write $e \in \hat{\bm{y}}$ for edges that are in this subset.

    A probabilistic \ac{CO} approximation algorithm for an edge subset problem defines a probability distribution over the subsets of edges.
    We use $h(\hat{\bm{y}} \mid \gG, \bm{s})$ to denote the probability that the output is edge subset $\hat{\bm{y}}$ given input graph $\gG$ and edge scores $\bm{s}$, as described in \autoref{sec:method}.

    We use $\sigma$ to denote the element-wise sigmoid function $\sigma(x) = \frac{e^x}{1 + e^x}$.

    \subsection{Probability of Finding the Optimal Minimum $k$-cut}

        Let $\gG = (V, E)$ be an undirected graph, $n = |V|$, and $m = |E|$.

        During the first $i - 1$ iterations of the modified Karger's algorithm, some edges have been merged away.
        Let $E_i$ be the set of edges that are left at iteration $i$ (this means $E_1 = E$).
        The probability that a specific edge $e$ is selected for contraction at iteration $i$ is
        \[
            p_i(e) = \frac{1 - \sigma(\bm{s}[e])}{\sum_{e' \in E_i} \big( 1 - \sigma(\bm{s}[e']) \big)}.
        \]

        \begin{theorem}
            \label{theorem:1}
            Let $h(\hat{\bm{y}} \mid \gG, \bm{s})$ be Karger's algorithm steered by scores $\bm{s}$, and let $\bm{y} \in \{0, 1\}^{|E|}$ be a minimum $k$-cut on $\gG = (V, E)$. Then,
            \[
                \lim_{\sigma(\bm{s}) \to \bm{y}} h(\bm{y} \mid \gG, \bm{s}) = 1.
            \]
        \end{theorem}
        In other words, in the limit, Karger's algorithm steered by $\bm{s}$ always finds optimal solution $\bm{y}$.

        \begin{proof}
            Assuming that no edge in $\bm{y}$ has been contracted yet, the probability that the edge that is selected for contraction in iteration $i$ is in $\bm{y}$ is
            \[
                p_i(e \in \bm{y})
                = \sum_{e \in \bm{y}} p_i(e)
                = \frac{\sum_{e \in \bm{y}} \big( 1 - \sigma(\bm{s}[e]) \big)}{\sum_{e' \in E_i} \big( 1 - \sigma(\bm{s}[e']) \big)}
            \]

            Karger's algorithm outputs a given cut $\bm{y}$ if and only if no edge in $\bm{y}$ is contracted by the algorithm
            (see~\citet{karger-stein}, lemma 2.1).
            Let $k$ be the parameter for minimum $k$-cut, i.e.\ the number of desired connected components.
            Karger's algorithm will always terminate after $|V| - k$ contraction steps.
            The probability that no edge in $\bm{y}$ is contracted during the $|V| - k$ contraction steps is
            \[
                h(\bm{y} \mid \gG, \bm{s})
                = \prod_{i=1}^{|V|-k} \big(1 - p_i(e \in \bm{y}) \big)
                = \prod_{i=1}^{|V|-k} \left(1 - \frac{\sum_{e \in \bm{y}} \big( 1 - \sigma(\bm{s}[e]) \big)}{\sum_{e' \in E_i} \big( 1 - \sigma(\bm{s}[e']) \big)} \right)
            \]

            In the numerator, $\lim_{\sigma(s) \to \bm{y}} \sum_{e \in \bm{y}} \big( 1 - \sigma(\bm{s}[e]) \big) = 0$, since all $\sigma(\bm{s}[e])$ go to 1.
            The denominator is greater than zero, because there is at least one edge left that is not in $\bm{y}$, otherwise the algorithm would be finished.
            Since the fraction goes towards zero, all terms of the product go towards 1 and $\lim_{\sigma(\bm{s}) \to \bm{y}} h(\bm{y} \mid \gG, \bm{s}) = 1$.
        \end{proof}

        The result can trivially be extended to the Karger--Stein algorithm, a variant of Karger's algorithm, and to weighted graphs $\gG(V, E, w)$.



    \subsection{Probability of Finding the Optimal TSP-Tour}

        Let $\gG = (V, E, w)$ be an undirected graph.
        For a given TSP tour $\hat{\bm{y}} \in \{0, 1\}^{|E|}$, let $J(\hat{\bm{y}})$ be the length of the tour:
        \[
            J(\hat{\bm{y}}) = \sum_{e \in \hat{\bm{y}}} w(e)
        \]
        Let $h(\hat{\bm{y}} \mid \gG)$ be an insertion algorithm, such as random insertion \cite{random-insertion}.
        Let $h(\hat{\bm{y}} \mid \gG, \bm{s})$ be that algorithm steered by the GNN output scores $\bm{s}$.

        \begin{theorem}
            \label{theorem:insertion-algorithm-optimal-tour}
            Let $\bm{y} \in \{0, 1\}^{|E|}$ be a minimal length TSP tour on graph $\gG$.
            Then,
            \[
                \lim_{\sigma(\bm{s}) \to \bm{y}} h(\bm{y} \mid \gG, \bm{s}) = 1.
            \]
        \end{theorem}
        In other words, in the limit, the insertion algorithm steered by $\bm{s}$ always finds optimal solution $\bm{y}$.

        \begin{proof}
            We know that the solution produced by any insertion algorithm $h(\hat{\bm{y}} \mid \gG)$ is within $\lceil \log(|V|) \rceil + 1$ of the optimal solution~\cite{insertion-algorithms-approximation-ratio}, i.e.
            \[
                J(\hat{\bm{y}}) \leq \big( \lceil \log(|V|) \rceil + 1 \big) \: J(\bm{y}) \qquad \textup{for } \hat{\bm{y}} \sim h(\hat{\bm{y}} \mid \gG).
            \]

            Let $\gG'$ be the graph that the insertion algorithm receives as input after modification through $\bm{s}$, i.e.\ $h(\hat{\bm{y}} \mid \gG, \bm{s}) = h(\hat{\bm{y}} \mid \gG')$.
            As $\sigma(\bm{s})$ approaches $\bm{y}$, the weights in $\gG'$ of edges in $\bm{y}$ approach 0.
            Meanwhile, the weights of the other edges approach their original weights from $\gG$, which are greater than 0.
            This means that the length of the tour $\bm{y}$ in $\gG'$ approaches 0, while the lengths of any other tour in $\gG'$ approach values greater than 0.

            This means that the maximum length of tours that the insertion algorithm can find approaches 0, and, as soon as the lengths of the suboptimal tours in $\gG'$ are far enough away from 0, $\bm{y}$ is the only tour left that the insertion algorithm can find.
        \end{proof}




\section{Details Regarding Baselines}
\label{sec:baseline-details}

    \subsection{Supervised Training with Binary Cross Entropy Loss}

        The task is treated as an edge-level binary classification task.
        The network is trained using a binary cross entropy loss:
        \begin{align*}
            \hat{y} &= \sigma \big( f_\theta(\gG) \big) \\
            \mathcal{L}_\text{supervised BCE}(\gG, y) &= \text{BCE}(\hat{y}, y) = \sum_{i=1}^{|E|} y_i \log \hat{y}_i + (1 - y_i) \log(1 - \hat{y}_i)
        \end{align*}
        where $f_\theta$ is a \ac{GNN}, $\gG$ is the input graph,
        $y\in \{ 0, 1 \}^{|E|}$ is the ground truth solution and $\hat{y} \in (0, 1)^{|E|}$ is the predicted solution.

        A ground truth label of 1 represents that an edge belongs to a minimum $k$-cut or a \ac{TSP} tour.
        



    \subsection{REINFORCE}

        \am{since in our case the probability distribution is the approximation algorithm, we should use $x$ instead of $\theta$ and use $y$ where we use $x$ at the moment}

        Recall that the REINFORCE algorithm, also known as the score function estimator, calculates
        \[
            \nabla_\theta \mathbb{E}_{y \sim p_\theta(x)} \big[ J(y) \big]
            = \mathbb{E}_{y \sim p_\theta(x)} \big[ J(y) \nabla_\theta \log p_\theta(x) \big]
        \]
        where $J$ is an objective function, and $p_\theta(x)$ is a probability distribution parameterized by $\theta$.

        We assume that $p_\theta$ is a discrete constrained exponential family distribution, i.e.
        \[
            p_\theta(x) =
            \begin{cases}
                \displaystyle\frac{\exp \big( \langle x, \theta \rangle \big)}{\sum_{x'} \exp \big( \langle x', \theta \rangle \big)} & \text{if } x \text{ satisfies the constraints} \\
                0 & \text{otherwise}
            \end{cases}
        \]
        \am{TODO: clarify what is meant by $x$ satisfies the constraints. symbol for constraint set is $\mathcal{C}$}

        For valid $x$,
        \[
            \log p_\theta(x) = \langle x, \theta \rangle - A(\theta),
        \]
        where $A(\theta)$ is the log-partition function
        \[
            A(\theta) = \log \left( \sum_{x' \in \mathcal{C}} \exp \big( \langle x', \theta \rangle \big) \right).
        \]
        Since $\nabla_\theta A(\theta) = \mathbb{E}_{y \sim p_\theta(x)}[y]$, we get
        \[
            \nabla_\theta \log p_\theta(x) = x - \mathbb{E}_{y \sim p_\theta(x)}[y].
        \]
        Inserting this into the REINFORCE formula gives us
        \[
            \nabla_\theta \mathbb{E}_{y \sim p_\theta(x)} \big[ J(y) \big] = \mathbb{E}_{y \sim p_\theta(x)} \bigg[ J(y) \Big( y - \mathbb{E}_{y' \sim p_\theta(x)}[y'] \Big) \bigg].
        \]

        Using the Gumbel-max trick, we sample from $p_\theta$ by sampling $\varepsilon \sim \mathrm{Gumbel}(0, 1)$, then calculating $y := h(\theta + \varepsilon)$.

        Estimating the outer expectation by sampling once and the inner expectation by sampling $N$ times, we arrive at Algorithm~\ref{alg:reinforce}.

        \begin{algorithm}[h!]
            \caption{REINFORCE}
            \label{alg:reinforce}
            \begin{algorithmic}
                \STATE \textbf{Input:} distribution parameter $\theta$
                \STATE $\varepsilon \sim \mathrm{Gumbel}(0, 1)$
                \STATE $y \gets h(\theta + \varepsilon)$
                \STATE $\nabla_\theta J(y) \gets J(y) \left( y - \displaystyle\frac{1}{N} \sum_{i=1}^N y_i \right)$ \\
                    \hspace{1.5cm} where $\varepsilon_i \sim \mathrm{Gumbel}(0, 1)$ \\
                    \hspace{2.93cm} $y_i \gets h(\theta + \varepsilon_i)$
                \STATE \textbf{return} $\nabla_\theta J(y)$
            \end{algorithmic}
        \end{algorithm}



    \subsection{Implicit Maximum Likelihood Estimator (I-MLE)}
    \label{sec:imle-details}

        \am{use another symbol instead of $\theta$, we're already using that for the GNN weights}

        \acused{I-MLE}
        \Ac{I-MLE} \citep{imle} allows estimating gradients with respect to the parameters of discrete exponential family distributions.
        This can be used to backpropagate through \ac{CO} solvers as follows.
        In the forward pass, perturb the input $\bm{\theta} \in \mathbb{R}^n$ to the \ac{CO} solver using noise $\bm{\epsilon} \sim \rho(\bm{\epsilon})$ sampled from a suitable noise distribution.
        Then run the \ac{CO} solver on the perturbed input $\bm{\theta} + \bm{\epsilon}$, obtaining output $\bm{z}$.

        In the backward pass, assume we know the gradient of the loss w.r.t.\ to $\bm{z}$, $\nabla_{\bm{z}} \mathcal{L}$.
        First, obtain a modified input $\bm{\theta}'$ for which we can expect better outputs compared to $\bm{\theta}$.
        One generally applicable option suggested by \cite{imle} is $\bm{\theta}' = \bm{\theta} - \lambda \nabla_{\bm{z}} \mathcal{L}$, where $\lambda$ is a hyperparameter.
        Using the same noise $\bm{\epsilon}$ as in the forward pass, perturb $\bm{\theta}'$ and run the \ac{CO} solver on $\bm{\theta}' + \bm{\epsilon}$, obtaining $\bm{z}'$.
        Finally, return the estimated gradient $\nabla_{\bm{\theta}} \mathcal{L} \approx \bm{z} - \bm{z}'$.

        This produces biased gradient estimates, but with much smaller variance than REINFORCE.
        To further reduce variance, this procedure can be repeated $S$ times, sampling new noise $\bm{\epsilon}_i \sim \rho(\bm{\epsilon}_i)$ each time and averaging the results.

        \subsubsection{Supervised Training with \acs{I-MLE}}

            The outputs of the \ac{GNN} are used to steer a \ac{CO} approximation algorithm.
            This approximation algorithm outputs a solution to the \ac{CO} problem, which can be compared to the ground truth solution using a Hamming loss.
            During backpropagation, I-MLE~\citep{imle} is used to estimate the gradient of the loss with respect to the \ac{GNN}'s output.
            This setting has no practical benefit over the simple supervised training using a binary cross entropy loss, but it serves to measure the effectiveness of \ac{I-MLE}.
    
            The training procedure works as follows.
            \begin{align*}
                s &= \sigma \big( f_\theta(\gG) \big) \\
                \hat{y} &= h(\gG, 1 - s) \\
                \mathcal{L}_\text{supervised I-MLE}(\gG, y) &= \frac{1}{|E|} \sum_{i=1}^{|E|} \hat{y}_i (1 - y_i) + (1 - \hat{y}_i) y_i
            \end{align*}
            Note that here, $\hat{y} \in \{ 0, 1 \}^{|E|}$ is guaranteed to be a valid solution to the given \ac{CO} problem.
            \ac{I-MLE} is used to estimate $\frac{\mathrm{d} \mathcal{L}}{\mathrm{d} (1 - s)}$.

        \subsubsection{I-MLE Target Distribution}

            We're using a custom target distribution for \ac{I-MLE} in the supervised setting.
            This target distribution is similar to the target distribution for \ac{CO} problems presented in~\citep{imle}.
            The idea behind it is to recover the ground truth label from the loss, which is possible when using the Hamming loss:
            \begin{align*}
                \ell(\hat{y}, y)                                   &= \hat{y} (1 - y) + (1 - \hat{y}) y \\
                \frac{\mathrm{d}}{\mathrm{d} \hat{y}} \ell(\hat{y}, y) &= 1 - 2y \\
                y &= \frac{1 - \frac{\mathrm{d}}{\mathrm{d} \hat{y}} \ell(\hat{y}, y)}{2}
            \end{align*}
            The best value for $\theta$ is $1 - y$ (we have to invert it because the input to the approximation algorithm is inverted)%
            \footnote{Using $\theta$ in the same sense as \cite{imle}. In our case, $\theta = 1 - s$}.
            With this we arrive at the following target distribution:
            \[
                \theta'
                = 1 - y
                = \frac{1 + \frac{\mathrm{d}}{\mathrm{d} \hat{y}} \mathcal{L}}{2}
            \]


        \subsubsection{Self-supervised Training with I-MLE}
            Since we're using a \ac{CO} approximation algorithm that guarantees that its outputs are valid solutions to the \ac{CO} problem, we can use the \ac{CO} problem's objective function as a loss directly instead of the supervised Hamming loss.
            The ground truth labels are therefore no longer required.
    
            In the case of minimum $k$-cut, the size of the cut is used as loss function.
            For TSP, the length of the tour is used.
    
            \begin{align*}
                s &= \sigma \big( f_\theta(\gG) \big) \\
                \hat{y} &= h(\gG, 1 - s) \\
                \mathcal{L}_\text{self-supervised I-MLE}(\gG) &= J_\text{CO}(\hat{y}) \\
            \end{align*}
            where $J_\text{CO}$ is the objective function of the \ac{CO} problem.
            Note that the \ac{CO} problem's constraints don't explicitly appear here, because the \ac{CO} approximation algorithm already guarantees that the constraints are met.
    
            As before, \ac{I-MLE} is used to estimate $\frac{\mathrm{d} \mathcal{L}}{\mathrm{d} (s - 1)}$.
            In this setting, the general-purpose target distribution for \ac{I-MLE} presented in~\citep{imle} is used.
    



\section{Graph Generation}
\label{sec:graph-generation-appendix}

    \subsection{Minimum \texorpdfstring{$k$}{k}-Cut}
    \label{sec:graph-generation-appendix-minimum-k-cut}

        Many commonly used graph generators create graphs with low-degree nodes.
        These graphs contain trivial solutions to the minimum $k$-cut problem in which $k - 1$ connected components only contain one node, and one connected component contains all of the remaining nodes.
        When creating a dataset for minimum $k$-cut, care must therefore be taken to avoid graphs with low-degree nodes.

        \paragraph{Graphs without edge weights.}
        A simple method to generate graphs with meaningful solutions to the minimum $k$-cut problem is as follows.
        Create $k$ fully connected subgraphs of random sizes within a given range.
        Then, add a random number of edges between random nodes of different subgraphs while ensuring that the resulting graph is connected.
        An additional benefit of this method is that, if the number of edges added between subgraphs is smaller than the number of nodes in the smallest subgraph by at least two, then the minimum $k$-cut is known from the construction:
        the cut consists of exactly the edges that were added between subgraphs.
        However, since all graphs generated this way consist of fully connected subgraphs, these problem instances are limited in diversity.

        The range of possible problem instances can be improved by generating graphs of varying density.
        Start by assigning nodes to $k$ subgraphs of random sizes within a given range.
        Then, add a random number of edges that connect nodes of different subgraphs.
        For each subgraph, add edges between random nodes within the same subgraph until all nodes have a higher degree than the number of edges between subgraphs.
        As long as there are enough edges between subgraphs, the minimum $k$-cut very likely consists of the edges between subgraphs.
        The minimum node degree and therefore the density of the graph depends on the number of edges between subgraphs and therefore on the size of the minimum $k$-cut.

        \paragraph{Graphs with edge weights.}
        For minimum $k$-cut graphs with edge weights, a graph generator commonly called NOIgen~\citep{noigen} (named after the initials of the authors) is often used.
        NOIgen works by first creating a specified number of nodes and adding edges between random nodes until a specified density is reached (sometimes, a Hamilton path is created first to ensure that the graph is connected).
        The weights of the edges are chosen uniformly at random.
        Finally, the nodes are randomly divided into $k$ subgraphs.
        The weights of edges that connect nodes of different subgraphs are scaled down by a fixed factor.

        When testing traditional, non-learned algorithms, the scaling factor is sometimes chosen to be very small, such that the minimum $k$-cut is very likely to consist of the edges between subgraphs~\citep{experimental-study-of-minimum-cut-algorithms}.
        However, this makes the problem trivially easy for GNNs, which can learn that a very low edge weight corresponds to an edge belonging to the minimum $k$-cut.
        This allows the GNN to disregard the graph structure and therefore circumvent the challenging part of the problem.
        On the other hand, if the weights of edges between subgraphs are not scaled down enough, the generated graph might have a trivial solution that simply cuts out $k - 1$ nodes.

        To combat this problem, we modify NOIgen by controlling not just the weights of edges between subgraphs, but also the number of edges between subgraphs.
        We add a parameter that specifies which fraction of edges is generated between subgraphs (as opposed to within the same subgraph).
        Ensuring that there are few enough edges between subgraphs allows for a milder downscaling of their edge weights without introducing a trivial solution.
        This in turn prevents the GNN from inferring whether an edge belongs to the minimum $k$-cut simply from its weight.

        The minimum $k$-cut in these graphs usually consists of the edges between subgraphs, but this is not guaranteed.
        The ground truth solution is therefore calculated separately to make sure that it reflects the optimal cut, as described in \autoref{sec:calculating-ground-truth-labels-appendix}.
        Another benefit of calculating them separately is that we can set the number of subgraphs to a different number than $k$, generating more interesting graphs.



    \subsection{Travelling Salesman Problem}

        Instances of Euclidean TSP are commonly generated with this simple algorithm:
        \begin{enumerate}
            \item Create a fully connected graph with $n$ nodes
            \item For each node, draw a position in the unit square uniformly at random and assign it as node features
            \item Calculate the distances between the nodes and assign them as edge features
        \end{enumerate}



    \subsection{Calculating Ground Truth Labels for Supervised Training}
    \label{sec:calculating-ground-truth-labels-appendix}

        In general, ground truth labels are generated using a traditional (i.e.\ non-learned) algorithm.
        In some settings, the graph can be constructed such that the ground truth solution can be obtained simultaneously from the same construction process, in which case running the traditional algorithm is not necessary.
        \begin{itemize}
            \item Minimum k-cut: For graphs without edge weights, the graphs can be constructed with known ground truth solutions.
            See \autoref{sec:graph-generation-appendix-minimum-k-cut} for details.
            For graphs with edge weights, the Karger--Stein algorithm~\citep{karger-stein} is run 100 times,
            and the smallest cut found is treated as the ground truth minimum cut.
            \item TSP: The well-established Concorde solver~\citep{concorde}, which guarantees optimal solutions, is used to generate ground truth labels.
        \end{itemize}


\end{document}
