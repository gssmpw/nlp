\section{Related Work}
The easiest way of training a model for \ac{CO} is to assume the existence of ground truth solutions to the \ac{CO} problems and train in a supervised fashion.
    Koolen, "Training Convolutional Networks for Combinatorial Optimization"
    They introduce an LSTM-based sequence-to-sequence architecture where the elements of the output sequence are positions in the input sequence, which they name pointer network.
    Louf, "Learning Heuristics for Combinatorial Optimization Problems"
    In their work, a \ac{GNN} predicts an approximate solution as a heatmap, which is then decoded into a valid solution to the \ac{CO} problem using beam search.
    Selsam, "Learning Combinatorial Embeddings for Reduction and Solving #SAT"
    They follow a neural algorithmic reasoning approach to learn to imitate \ac{CO} solvers.
    the model is pre-trained on simple algorithms and then fine-tuned on difficult ones.
    Finally, Selsam uses graph-based denoising diffusion to generate high-quality solutions.
    However, these supervised approaches aren't applicable to such cases where calculating exact solutions for the training problems is not feasible.

    Several approaches have used \acf{RL} to remove this dependence on a labeled dataset.
    Koolen use a pointer network____, but train it with \ac{RL}.
    Selsam use the \ac{GNN} autoregressively to predict which node should be added next to the solution set and repeat that process until a valid solution is reached.
    Wu formulate the \ac{CO} problem as a Markov decision process, then use an algorithm similar to AlphaGo Zero____ to solve it autoregressively.
    Louf propose a modification to the \ac{RL} training process for autoregressive methods that improves sample efficiency.
    However, the use of the score function estimator in many of these methods leads to high-variance gradients, which makes training more difficult.


    Other self-supervised approaches that do not rely on reinforcement learning include Selsam, "Learning Combinatorial Embeddings for Reduction and Solving #SAT"____,
    Koolen focus on quadratic and polynomial unconstrained binary optimization, which many \ac{CO} problems can be formulated as.
    This allows them to make use of a self-supervised loss function specific to these two problem families.
    Louf also concentrate on quadratic unconstrained binary optimization, but employ a diffusion-based approach similar to ____.
    Selsam formulate a self-supervised loss function for a comprehensive class of \ac{CO} problems.
    Wu use an LSTM-based architecture to solve binary maximum constraint satisfaction problems, which many \ac{CO} problems can be formulated as.
    Since these approaches focus on certain families of \ac{CO} problems, they aren't general to \ac{CO} as a whole.

    
    Koolen use a \ac{GNN} as a heuristic for the branch-and-bound algorithm, which guarantees exact solutions at the expense of a comparatively longer running time.
    The \ac{GNN} is trained via imitation learning from a known high-quality but slow heuristic.
    Wu compare some of the paradigms introduced in other papers in structured experiments.
    There have been two lines of work to address the problem of backpropagating through combinatorial optimization problems.
    Firstly, if we have a set of optimal solutions given as training data, we can use supervised learning to train the GNN to output adjacency matrices as close as possible to the optimal solutions Selsam.
    This is often called ``predict, then optimize''.
    Secondly, there are several methods to backpropagate through a non-differentiable \ac{CO} algorithm, such as ____.
    Related to our work is decision-focused learning, which has developed several methods to backpropagate through CO solvers ____.
       