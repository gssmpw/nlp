\section{Introduction}
\label{sec:intro}

The success of deep neural networks (DNNs) is largely fueled by large-scale, accurately labeled datasets \cite{lecun2015deep,zong2024dirichlet}. However, acquiring such data is often expensive and time-consuming primarily due to the labor-intensive nature of manual labeling \cite{ren2021survey,10.1007/978-3-031-73390-1_8}. To save labeling costs, recent research has focused on developing effective model training techniques that leverage limited and insufficiently labeled data \citep{zhou2018brief,van2020survey,ren2021survey}. Among them, active learning (AL) has emerged as a popular framework, which iteratively selects the most informative examples from the unlabeled data pool and queries their labels from the Oracle \cite{settles2009active,ren2021survey, huang2021asynchronous}.

\begin{figure}[t]
  \centering
   \includegraphics[width=0.97\linewidth]{figs/motivation.pdf}
   \caption{Dataset: CIFAR-10; mismatch ratio: 40\%. Our motivation: in open-set scenarios, querying examples with low epistemic uncertainty yields high query precision, but the overall information content is low, resulting in poor model performance. Focusing on examples with high aleatoric uncertainty also leads to poor performance, as the model's assessment of this uncertainty becomes meaningless for open-set examples. Nevertheless, an effective combination of the two can lead to a superior outcome.}
   \label{fig:motivation}
\end{figure}

Existing AL methods can be categorized into three types based on their sampling strategies: uncertainty-based \cite{balcan2007margin,holub2008entropy,yoo2019learning,10.1007/978-3-031-73390-1_8}, diversity-based \cite{nguyen2004active,sener2017active,xie2023active}, and hybrid strategies \cite{huang2010active,ash2019deep,safaei2024entropic}. Most of these methods operate under the closed set assumption, which posits that the classes in the unlabeled data match those in the labeled data. However, in real-world scenarios, it is challenging and often costly to ensure that no open-set classes are present in the unlabeled data. Meanwhile, several studies \cite{du2021contrastive,ning2022active,park2022meta,safaei2024entropic} have shown that these methods perform poorly when open-set classes are involved, as such examples tend to receive uncertain model predictions and exhibit distinct features. Therefore, developing effective AL methods for open-world scenarios, where open-set classes exist, is of significant importance.


Recently, this emerging research problem has garnered considerable attention \cite{du2021contrastive,ning2022active,park2022meta,safaei2024entropic,10.1007/978-3-031-73390-1_8}. For instance, in LfOSA \cite{ning2022active}, the authors formulate this problem as open-set annotation (OSA) and utilize queried unknown class examples to train a $(C+1)$-class detector for rejecting open-set examples and focusing more on selecting known class ones. Two recent methods, EOAL \cite{safaei2024entropic} and BUAL \cite{10.1007/978-3-031-73390-1_8}, adopt a structure similar to LfOSA, with EOAL aimed at improving the recognition of known class examples, while BUAL focuses more on sampling highly uncertain examples. Despite demonstrating high query precision, our findings reveal that these methods can not achieve satisfactory test accuracy and struggle to identify the most informative examples.



%For instance, CCAL \cite{du2021contrastive} and MQNet \cite{park2022meta} leverage self-supervised learning to utilize and filter unknown class examples (MQNet additionally employs meta-learning). However, they fail to fully exploit labeled examples from unknown classes and incur high training costs, which have been shown to deliver suboptimal performance \cite{10.1007/978-3-031-73390-1_8}.


%EOAL \cite{safaei2024entropic} and BUAL \cite{10.1007/978-3-031-73390-1_8} adopt a structure similar to LfOSA, but focus on sampling diverse examples and highly uncertain examples, respectively.

%LfOSA \cite{ning2022active} formulates this problem as an open-set annotation (OSA) task and utilizes queried unknown class examples to train a $C+1$ classifier for rejecting unknown class examples and focusing more on selecting known class ones. EOAL \cite{safaei2024entropic} and BUAL \cite{10.1007/978-3-031-73390-1_8} adopt a structure similar to LfOSA, but focus on sampling diverse examples and highly uncertain examples, respectively. However, in this paper, we find that although these methods demonstrate high query precision, they do not achieve satisfactory test accuracy, suggesting that they fail to identify the most informative examples. In contrast, CCAL \cite{du2021contrastive} and MQNet \cite{park2022meta} adopt entirely different architectures, leveraging self-supervised learning to utilize and filter unknown class examples (MQNet additionally employing meta-learning). However, they fail to fully exploit unknown class examples and incur high training costs, which have been shown to deliver suboptimal performance \cite{10.1007/978-3-031-73390-1_8}.


To analyze the reasons behind their failure, we review the concept of uncertainty quantification\footnote{A detailed introduction with intuitive illustrations is in \cref{sec:uq}. }. Epistemic uncertainty refers to a measure that remains high for instances not previously encountered and decreases when these instances are included in the training \cite{kendall2017uncertainties,smith2018understanding}. In contrast, aleatoric uncertainty is characterized by elevated values in ambiguous examples \cite{kendall2017uncertainties,smith2018understanding}. In closed-set settings, examples with high epistemic uncertainty tend to reside in low-density regions of the representation space, while those with high aleatoric uncertainty may appear around the decision boundary due to ambiguous features. Both types are potential targets for our queries. However, in open-set scenarios, examples with high epistemic uncertainty are likely to be open-set instances, and aleatoric uncertainty is meaningful only in closed-set contexts, as it reflects the ambiguity between different observable classes\footnote{Essentially, the class probability $p(y|x)=\frac{p(x,y)}{p(x)} $ is meaningful only when $p(x)\ne 0$, i.e., $x$ must be an observing example from a known class.} \cite{kendall2017uncertainties,mukhoti2023deep}. Therefore, selecting examples with low epistemic uncertainty (to ensure a closed set) and high aleatoric uncertainty is a more reasonable choice. However, LfOSA and EOAL focus on querying examples with low epistemic uncertainty, while BUAL prioritizes querying those with high aleatoric uncertainty. This may be the potential reason for their failure.

%%We propose first selecting examples with low epistemic uncertainty to form a smaller candidate set, ensuring closed-set properties, which makes aleatoric uncertainty meaningful. Then, within the candidate set, we query examples with high aleatoric uncertainty.
%As shown in Figure \ref{fig:motivation}, we can observe that focusing solely on either epistemic or aleatoric uncertainty leads to suboptimal performance in open-world scenarios (see Figure \ref{fig:motivation}). 

%To validate this, we present the results in Figure \ref{fig
%}. As shown, focusing solely on either epistemic or aleatoric uncertainty in an open-world scenario results in suboptimal performance. However, combining both leads to a significant improvement. Inspired by this, we propose \textbf{E}nergy-based \textbf{A}ctive \textbf{O}pen-set \textbf{A}nnotation (EAOA), an approach that effectively queries the most informative examples by considering both types of uncertainty. EAOA maintains two networks: a $(C+1)$-class detector and a $C$-class target classifier, with the following contributions:

To validate this, we present the results in Figure \ref{fig:motivation}. As shown, focusing solely on either epistemic or aleatoric uncertainty in an open-world scenario results in suboptimal performance. However, effectively combining both leads to a significant improvement. Inspired by this, we propose \textbf{E}nergy-based \textbf{A}ctive \textbf{O}pen-set \textbf{A}nnotation (EAOA), an approach that effectively queries the most informative examples by considering both types of uncertainty. EAOA maintains two networks: a $(C+1)$-class detector and a $C$-class target classifier, with the following contributions:
\begin{itemize}
    \item An energy-based epistemic uncertainty measure is designed for the detector, expressed as the free energy score on known classes minus that on the unknown class. This measure integrates both learning-based and data-driven perspectives, enabling reliable uncertainty assessment in data-limited scenarios.

    \item An energy-based aleatoric uncertainty measure is proposed for the target classifier, defined as the free energy score on all classes minus that on secondary classes.

    \item A coarse-to-fine querying strategy is proposed. It first selects examples with low epistemic uncertainty to form a smaller candidate set, ensuring closed-set properties, which makes aleatoric uncertainty meaningful. Then, it queries examples with high aleatoric uncertainty within this set, with the candidate set size adaptively adjusted through a novel target-driven strategy.

    \item A margin-based energy loss is introduced for the detector training, aimed at maximizing the free energy score on known classes while minimizing that on the unknown class, thereby enhancing the unknown class detection.

    \item Extensive experiments show that EAOA outperforms current state-of-the-art methods in test accuracy, query precision, and training efficiency.

\end{itemize}





