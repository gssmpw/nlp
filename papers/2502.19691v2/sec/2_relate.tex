\section{Related Work}
\label{sec:related}


\textbf{Active learning (AL)} has garnered great research interest as a primary framework for reducing labeling costs by querying the most informative examples for model training. AL's query methods can be categorized based on their data sources into three types: query-synthesizing \cite{mahapatra2018efficient,mayer2020adversarial,zhu2017generative}, stream-based \cite{fang2017learning,narr2016stream}, and pool-based approaches \cite{balcan2007margin,holub2008entropy,huang2021asynchronous,du2021contrastive,10.1007/978-3-031-73390-1_8}. Among these, pool-based methods are currently the mainstream, operating under the assumption of a large pool of available unlabeled data, from which a subset of examples is selected for annotation in each AL round. These query methods can be further divided into three categories: 1) uncertainty-based strategies \cite{li2006confidence,balcan2007margin,holub2008entropy,yoo2019learning}, which select instances for which labeling is least certain; 2) diversity-based strategies \cite{nguyen2004active,sener2017active,xie2023active}, which query instances that are most representative or exhibit the greatest feature diversity; and 3) hybrid strategies \cite{huang2010active,ash2019deep,safaei2024entropic}, which combine both to achieve better performance.


\textbf{Open-set recognition (OSR)} refers to a system's ability to differentiate between data types it has encountered during training (in-distribution (ID) data) and those it has not previously seen (out-of-distribution (OOD) data). Earlier studies employed traditional machine learning techniques such as support vector machines \cite{jain2014multi,scheirer2014probability}, extreme value theory (EVT) \cite{zhang2016sparse}, nearest class mean classifier \cite{bendale2015towards}, and nearest neighbor \cite{mendes2017nearest}. Recently, there has been growing interest in using generative models to learn representation spaces focused exclusively on known examples \cite{oza2019c2ae,sun2020conditional,zhang2020hybrid,perera2020generative}. Other techniques often aim to simulate unknown examples, providing a more intuitive approach to OSR \cite{ge2017generative,neal2018open,chen2020learning,zhou2021learning,chen2021adversarial}. However, simply applying these methods in AL scenarios under the open-world assumption often leads to failure for two main reasons. First, the recognition performance of these methods heavily relies on the classifier's effectiveness; when the classifier underperforms—a common occurrence in AL scenarios—the overall performance can decline significantly \cite{vaze2021open}. Second, some genuine unknown class examples are inevitably labeled during the labeling process, and OSR methods may not effectively utilize them.


\begin{figure*}[t]
  \centering
   \includegraphics[width=0.99\linewidth]{figs/framework.pdf}

   \caption{The framework of EAOA. It consists of three general steps: model training, example selection, and Oracle labeling. In the model training phase, a detector is trained to assess epistemic uncertainty (EU) from both learning-based and data-driven perspectives, along with a target classifier to evaluate aleatoric uncertainty (AU) based on class confusion. In the example selection phase, $kb$ examples with the lowest EU scores are chosen first, followed by querying $b$ examples with the highest AU scores, where $k$ is adaptively adjusted based on the target precision. In the Oracle labeling phase, the queried examples are assigned labels, and all relevant data pools are updated accordingly.}
   \label{fig:framework}
\end{figure*}

\textbf{Active open-set annotation (AOSA)} refers to AL tasks under open-world scenarios, which aligns more closely with practical application scenarios and has become a research hotspot in recent years \cite{du2021contrastive,ning2022active,park2022meta,safaei2024entropic,10.1007/978-3-031-73390-1_8}. CCAL \cite{du2021contrastive} and MQNet \cite{park2022meta} employ contrastive learning and established metrics respectively to assess sample purity and informativeness, utilizing heuristic and meta-learning approaches, respectively, to achieve a balance. However, by not fully leveraging labeled unknown class examples, they provide inadequate assessments, leading to poorer model performance. LfOSA \cite{ning2022active} incorporates labeled unknown class examples to train an additional $(C+1)$-class classifier (a.k.a, detector), using the maximum activation values (MAVs) to identify known class examples. EOAL \cite{safaei2024entropic} enhances the detector by adding an additional binary classifier head and uses entropy values, calculated separately for known and unknown classes, to identify known class examples. BUAL \cite{10.1007/978-3-031-73390-1_8} defines positive uncertainty and negative uncertainty respectively, and utilizes the detector's OOD probabilities to balance, aiming to identify highly uncertain examples. However, as previously noted, these methods fail to select examples with both low epistemic uncertainty and high aleatoric uncertainty, resulting in suboptimal performance.