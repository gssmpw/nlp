
\begin{figure*}[!h]
\centering
   \includegraphics[width=0.98\linewidth]{figs/c10_v2.pdf}
   \includegraphics[width=0.98\linewidth]{figs/c100_v2.pdf}
   \includegraphics[width=0.98\linewidth]{figs/tiny_v2.pdf}
\caption{Test accuracy comparison on CIFAR-10, CIFAR-100, and Tiny-ImageNet.}
	\label{fig:acc_cifar10}
\end{figure*}


\section{Experiments}
\label{sec:experiments}



\begin{figure*}[!h]
\centering
   \includegraphics[width=0.97\linewidth]{figs/c10_query_v2.pdf}
   \includegraphics[width=0.97\linewidth]{figs/c100_query_v2.pdf}
   \includegraphics[width=0.97\linewidth]{figs/tiny_query_v2.pdf}
\caption{Query precision comparison on CIFAR-10, CIFAR-100, and Tiny-ImageNet.}
	\label{fig:query_cifar10}
\end{figure*}


%Ablation results on CIFAR-100 with a mismatch ratio of 40\%. The \textbf{left} validates the effectiveness of each method component, while the \textbf{middle} and \textbf{right} assess the sensitivity of the hyperparameter $K$ for reverse k-NN and the energy loss weight $\lambda_e$. ``Part 1" represents energy-based epistemic uncertainty, ``Part 2" indicates energy-based aleatoric uncertainty, and ``Ours" denotes their combination. ``mQP" and ``mDA" are mean query precision and mean detector test accuracy across all AL rounds, respectively.


\subsection{Implementation Details}

\textbf{Datasets.} We validate the effectiveness of our method on three benchmark datasets: CIFAR-10 \cite{krizhevsky2009learning}, CIFAR-100 \cite{krizhevsky2009learning}, and Tiny-ImageNet \cite{yao2015tiny}, with category counts of 10, 100, and 200, respectively. To perform active open-set annotation (AOSA), we create their open-set versions by randomly selecting a subset of classes as known according to the specified mismatch ratio, while the remaining classes are treated as unknown. The mismatch ratio is defined as the proportion of known classes in the total number of classes. For CIFAR-10 and CIFAR-100, we set the mismatch ratios to 20\%, 30\%, and 40\%. For Tiny-ImageNet, we set the ratios to 10\%, 15\%, and 20\%, making it more challenging.

\textbf{Training details.} Initially, we randomly select 1\%, 8\%, and 8\% of known class examples from CIFAR-10, CIFAR-100, and Tiny-ImageNet, respectively, to construct the labeled dataset. The active learning (AL) process consists of 10 rounds, with 1,500 examples queried in each round.
For all experiments, we choose ResNet-18 \cite{he2016deep} as the base model and train it by SGD \cite{zinkevich2010parallelized} optimizer with momentum 0.9, weight decay 5e-4, and batch size 128 for 200 epochs. The initial learning rate is set to 0.01 and is reduced by a factor of 10 every 60 epochs. We repeat all experiments three times on GeForce RTX 3090 GPUs and record the average results for three random seeds  ($seed = 1, 2, 3$). We generally set the values of $K$, $tP$, $k_1$, $a$, $z$, $m_{kno}$, $m_{unk}$, and $\lambda_e$ to 250, 0.6, 5, 1, 0.05, -25, -7, and 0.01, respectively, and these values generalize well across datasets.


%In the paper, $tP$, $k_1$, $a$, and $b$ are set to 0.6, 5, 1, and 0.05 for all settings, respectively.

\textbf{Baselines.} We consider the following methods as baselines: Random, Uncertainty, Certainty, Coreset, BADGE, CCAL, MQNet, EOAL, and BUAL. Among them, EOAL and BUAL are currently state-of-the-art (SOTA). A detailed overview of these methods is provided in \cref{sec:baselines}.


%\textbf{Baselines.} We consider the following AL methods as baselines: 1) Random, which selects instances at random; 2) Uncertainty, which selects instances with the highest entropy of predictions; 3) Certainty, which selects instances with the lowest entropy of predictions; 4) Coreset, which uses the concept of core-set selection to choose diverse instances; 5) BADGE, which selects instances by considering both uncertainty and diversity in the gradient via k-means++ clustering; 6) CCAL, which employs contrastive learning to extract the semantic and distinctive scores of examples for instance querying; 7) MQNet, which balances the purity score and informativeness score to select instances through meta-learning; 8) LfOSA, which selects instances based on maximum activation value produced by the $C+1$ class detector; 8) EOAL, which queries instances by calculating the entropy of examples in both known and unknown classes; 9) BUAL, which queries instances by adaptively combining the uncertainty obtained from positive and negative classifiers trained in different ways. Among these methods, EOAL and BUAL are currently state-of-the-art (SOTA).


\subsection{Performance Comparison} 

Figure \ref{fig:acc_cifar10} displays the test accuracy of various methods on CIFAR-10, CIFAR-100, and Tiny-ImageNet, varying with the number of AL rounds. Figure \ref{fig:query_cifar10} presents scatter plots of the average query precision across all rounds and the final round test accuracy for each method on CIFAR-10, CIFAR-100, and Tiny-ImageNet.
Here, the query precision refers to the proportion of queried known class examples to the total number of queried examples in each round.



As shown in Figure \ref{fig:acc_cifar10}, our method achieves optimal test accuracy across all datasets and mismatch ratios, and in most AL rounds, the curve of our method completely overlaps with those of other methods, demonstrating its superiority. In Figure \ref{fig:query_cifar10}, our method achieves optimal final round test accuracy and mean query precision in most scenarios, particularly on the challenging Tiny-ImageNet dataset, demonstrating its strong recognition capabilities.
Compared to the existing SOTA methods, BUAL and EOAL, our method ensures that queried examples exhibit low epistemic uncertainty while maintaining high aleatoric uncertainty. The significant test performance improvement over them validates the effectiveness of our proposed framework, suggesting that the examples selected by our method are more informative. All methods that employ a $(C+1)$-class detector to leverage labeled unknown class examples—BUAL, EOAL, LfOSA, and our method—exhibit significant performance advantages over the remaining methods, both in test performance and recognition capability. Although CCAL and MQNet consider both sample purity and informativeness, they fail to achieve an effective balance and adopt inadequate measurement metrics. Certainty shows similar recognition performance to Random, supporting that entropy, i.e., a measure of aleatoric uncertainty, is only meaningful in closed-set scenarios. Traditional AL methods, Uncertainty, Coreset, and BADGE, are significantly hindered by open-set examples, as these examples are prone to receive low-confidence predictions and exhibit diverse features, thus leading to poorer performance.



\begin{figure*}[!h]
	\centering
	
	\includegraphics[width=0.29\linewidth]{figs/aba_component_v2.pdf}
	%\includegraphics[width=5.75cm]{figs/aba_reverseK.pdf}
	\includegraphics[width=0.29\linewidth]{figs/aba_energyW.pdf}
        \includegraphics[width=0.41\linewidth]{figs/time_aba.pdf}
	
	\caption{Ablation results on CIFAR-100 with a mismatch ratio of 40\%. 1) The \textbf{left} validates the effectiveness of each method component. ``Part 1" represents energy-based epistemic uncertainty and ``Part 2" indicates energy-based aleatoric uncertainty. ``mQP" denotes mean query precision across all AL rounds. 2) The \textbf{middle} assesses the sensitivity of the energy loss weight $\lambda_e$. ``mDA" denotes mean detector test accuracy across all AL rounds. 3) The \textbf{right} shows the runtime comparison. The numbers on the bar chart correspond to mQP scores.}
	\label{fig:three_aba}
\end{figure*}

\begin{figure}[!h]
  \centering
   %\includegraphics[width=1\linewidth]{figs/TR_aba.png}
   \includegraphics[width=0.975\linewidth]{figs/TR_aba_v2.png}

   \caption{Ablation results for target precision $tP$ on CIFAR-10 (\textbf{Left}) and CIFAR-100 (\textbf{Right}). ``MR" denotes mismatch ratio. ``Best" indicates the top-performing method in the comparisons.}
   \label{fig:TR_aba}
\end{figure}

\subsection{Ablation Studies}

\textbf{Effect of each component.} Figure \ref{fig:three_aba} (left) shows the ablation results to validate the effectiveness of each component in our method. Our proposed energy-based epistemic uncertainty (``Part 1") utilizes information from labeled unknown class examples, leading to significant improvements in both recognition and test performance compared to free energy alone. The proposed energy-based aleatoric uncertainty (``Part 2") performs poorly on its own, as aleatoric uncertainty is only meaningful in closed-set scenarios. However, when combined with ``Part 1", it shows significant improvement, validating the superiority of the entire framework. Additionally, removing the data-driven epistemic uncertainty score or the margin-based energy loss leads to a decline in performance, confirming their necessity.

  
\textbf{Hyper-parameter sensitivity.} Figure \ref{fig:TR_aba} presents the ablation results for the hyperparameter target query precision $tP$ on CIFAR-10 and CIFAR-100, with values set to [0.4, 0.5, 0.6, 0.7, 0.8]. Overall, the fluctuations in test performance are minimal, and as $tP$ increases, the method's recognition performance improves. 
%Figure \ref{fig:three_aba} (middle) shows the impact of the hyperparameter $K$ for reverse k-NN on the method under a 40\% mismatch ratio on CIFAR-100, with values set to [150, 200, 250, 300, 350]. Our method demonstrates high robustness to changes in this parameter.
Figure \ref{fig:three_aba} (middle) displays the ablation results for the energy loss weight $\lambda_e$ on CIFAR-100 with a 40\% mismatch ratio, using values of [0.001, 0.005, 0.01, 0.05, 0.1]. A higher loss weight can impede model training, leading to reduced detector accuracy and reliability. Conversely, a lower loss weight fails to effectively separate the energy distributions of known and unknown class examples, adversely affecting the detector's recognition performance. %In this paper, $eW$ is set to 0.01 for all settings.

The ablation results for additional hyper-parameters are provided in \cref{sec:aas}, which can demonstrate their ability to generalize effectively across different datasets.

%\begin{figure}[!h]
%  \centering
%   \includegraphics[width=1\linewidth]{figs/time_aba.pdf}

%   \caption{Runtime comparison on CIFAR-100 with a 40\% mismatch ratio. The numbers on the bar chart indicate the mean query precision across all AL rounds for each method.}
%   \label{fig:time_aba}
%\end{figure}


\textbf{Runtime comparison.} Figure \ref{fig:three_aba} (right) presents the running times and mean query precision of various methods on CIFAR-100 with a mismatch ratio of 40\%. A higher mean query precision indicates that a larger number of examples are involved in training, often resulting in longer training times. Traditional AL methods, characterized by lower mean query precision and the training of a single model, generally have shorter training times. Notably, our method has the shortest runtime among all AOSA methods, achieving the highest test accuracy and maintaining a very high recognition rate.


%\begin{figure*}
%	\centering
	
%	\includegraphics[width=5.75cm]{figs/c10_n2.pdf}
%	\includegraphics[width=5.75cm]{figs/c100_n20.pdf}
%	\includegraphics[width=5.75cm]{figs/c100_n20.pdf}
	
%	\includegraphics[width=5.75cm]{figs/c10_n3.pdf}
%	\includegraphics[width=5.75cm]{figs/c100_n30.pdf}
%	\includegraphics[width=5.75cm]{figs/c100_n30.pdf}
	
%	\includegraphics[width=5.75cm]{figs/c10_n4.pdf}
%	\includegraphics[width=5.75cm]{figs/c100_n40.pdf}
%	\includegraphics[width=5.75cm]{figs/c100_n40.pdf}
	
%	\caption{Accuracy comparison on CIFAR-10 (first column), CIFAR-100 (second column), and Tiny-ImageNet (third column). The ratio of known class examples to the total number of examples is fixed at 0.2 (first row), 0.3 (second row), and 0.4 (third row) for each dataset.}
%	\label{fig.acc}
%\end{figure*}

