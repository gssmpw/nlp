\clearpage
\setcounter{page}{1}
\maketitlesupplementary

\section{Uncertainty Quantification}
\label{sec:uq}

\begin{figure}[t]
  \centering
   \includegraphics[width=1\linewidth]{figs/AU_EU.pdf}
   \caption{Intuitive examples of aleatoric and epistemic uncertainty in dog-wolf binary classification.}
   \label{fig:AU&EU}
\end{figure}

In deep learning, epistemic uncertainty and aleatoric uncertainty represent two distinct types of uncertainty, commonly used to describe the various sources of uncertainty in a model's predictions:
\begin{itemize}
    \item \textbf{Epistemic uncertainty:} 
    \begin{itemize}
    \item  This type of uncertainty arises from a model's lack of knowledge, often due to insufficient training data or the model's complexity. It reflects the model's incomplete or uncertain understanding of the task and can generally be reduced or eliminated with more data or a more effective model. 
    \item  For example, in a deep neural network, if there is little data available for certain classes or the model has not been trained sufficiently, the model may be highly uncertain in its predictions for certain examples.
    \item  This uncertainty is generally reducible, as it can be mitigated by adding more training data or improving the model architecture.
    \item  In Figure \ref{fig:AU&EU}, the ``Bulldog" and ``Arctic Wolf" exhibit significant feature differences from the ``Dog" and ``Wolf" in the training set, leading to higher epistemic uncertainty. After these examples are incorporated into model training, predictive performance on them improves, thereby reducing their epistemic uncertainty.
    \end{itemize}

    \item \textbf{Aleatoric uncertainty:} 
    \begin{itemize}
    \item  This type of uncertainty stems from inherent noise or variability in the data, i.e., the intrinsic randomness or uncontrollable factors within the data. 
    \item  For instance, in image classification, factors like feature confusion, lighting conditions, or object occlusion may lead to instability in the model's predictions.
    \item  This uncertainty is generally irreducible because it originates from the intrinsic properties of the data, not from issues with the model or training process.
    
    \item In Figure \ref{fig:AU&EU}, the ``Seppala Siberian Sleddog" resembles the ``Wolf" in appearance but belongs to the ``Dog" class, leading to higher aleatoric uncertainty. Due to feature confusion, incorporating these examples into model training may not substantially improve performance or reduce their aleatoric uncertainty.
    \end{itemize}
\end{itemize}


\section{Label-Wise Free Energy}
\label{sec:lwfe}

EBMs define the probability distribution in multi-label settings through the logits as:
\begin{equation}
\begin{aligned}
    p\left ( y_c|x \right ) &=\frac{e ^{-E\left ( x,y_c \right ) }}{\int_{y}e^{-E\left ( x,y_c \right ) } }=\frac{e ^{-E\left ( x,y_c \right ) }}{ e ^{-E\left ( x,y_c \right )}+e ^{-E\left ( x,-y_c \right )}}\\&=\frac{e^{-E\left ( x,y_c \right )+E\left ( x,-y_c \right )   } }{1+e^{-E\left ( x,y_c \right )+E\left ( x,-y_c \right )   } }=\frac{e^{f_{y_c}\left ( x \right ) }}{1+e^{f_{y_c}\left ( x \right ) } } \\&=\frac{e ^{-E\left ( x,y_c \right ) }}{e ^{-E\left ( x\right ) }}
\end{aligned}
\end{equation}
where $y_c=1$ indicates that instance $x$ belongs to the $c$-th class while $y_c=-1$ indicates not, $f_{y_c}\left ( x \right )$ denotes predicted logit of the model $f$ for instance $x$ regarding the $c$-th class, and $E_{y_c}\left ( x \right ) =-\log_{}{\left ( 1+e^{f_{y_c}\left ( x \right ) } \right ) } $ is the label-wise free energy for instance $x$ on class $y_c$.







\section{The Pseudocode of EAOA}
\label{sec:pesudocode}

The pseudocode of EAOA is summarized in Algorithm \ref{alg:1}.


\begin{algorithm}
    \caption{The EAOA algorithm}
    \label{alg:1}
    \textbf{Input:} Labeled data pool $\mathcal{D}_{L} = \mathcal{D}_{L}^{kno}\cup \mathcal{D}_{L}^{unk}$, unlabeled data pool $\mathcal{D}_{U}$, detector $f_{\theta_D}$, target classifier $f_{\theta_C}$, query budget $b$, dynamic factor $k_t$, and target precision $tP$. \\
    \textbf{Process: (The $t$-th AL round) }
    \begin{algorithmic}[1]
        \STATE \textit{\# Detector training}
        \STATE Update $\theta_D$ by minimizing $\mathcal{L} _{detector}$ in Eq. \eqref{l_detector} using all labeled examples from $\mathcal{D}_{L}$.

        \STATE \textit{\# Epistemic uncertainty estimating}
        \STATE Extract logit outputs and features from $f_{\theta_D}$ for examples in $\mathcal{D}_{L}$ and $\mathcal{D}_{U}$, respectively.
        \STATE Based on model outputs, estimate the learning-based epistemic uncertainty score for each example in $\mathcal{D}_{U}$ using Eq. \eqref{eq1} and Remark \ref{remark1}.
        \STATE Based on feature similarity, find K-nearest neighbors in $\mathcal{D}_{U}$ for each example in $\mathcal{D}_{L}$, and obtain reverse neighbors by class in $\mathcal{D}_{L}$ for each example in $\mathcal{D}_{U}$.
        \STATE Estimate data-centric epistemic uncertainty score for each example in $\mathcal{D}_{U}$ using Eq. \eqref{eq7} and Remark \ref{remark1}.
        \STATE For each example, combine the two scores into one final epistemic uncertainty score using GMM and Eq. \eqref{eq8}.
        
        \STATE \textit{\# Target classifier training}
        \STATE Update $\theta_C$ by minimizing $\mathcal{L} _{classifier}$ in Eq. \eqref{l_classifier} using all known class labeled examples from $\mathcal{D}_{L}^{kno}$.
        
        \STATE \textit{\# Aleatoric uncertainty estimating}
        \STATE Extract logit outputs from $f_{\theta_C}$ for examples in $\mathcal{D}_{U}$.
        \STATE Estimate aleatoric uncertainty score for each example in $\mathcal{D}_{U}$ using Remark \ref{remark2}.

        \STATE \textit{\# Active sampling}
        
        \STATE $k_tb$ examples with the lowest epistemic uncertainty scores are selected first to form a candidate query set.
        \STATE $b$ examples with the highest aleatoric uncertainty scores are then queried to form the final query set $X^{query}$.

        \STATE \textit{\# Oracle labeling}
        \STATE Query labels from Oracle and obtain $X_{kno}^{query}$, $X_{unk}^{query}$, and query precision $rP=\left | \frac{X_{kno}^{query}}{X^{query}}  \right | $.
        \STATE Update $k_t$ to $k_{t+1}$ using Eq. \eqref{k1} based on $tP - rP$.
        \STATE Update corresponding data pools: $\mathcal{D}_{U} = \mathcal{D}_{U} - X^{query}$, $\mathcal{D}_{L}^{kno}=\mathcal{D}_{L}^{kno}\cup X_{kno}^{query}$, and $\mathcal{D}_{L}^{unk}=\mathcal{D}_{L}^{unk}\cup X_{unk}^{query}$.
        
    \end{algorithmic}
    \textbf{Output:}  $\mathcal{D}_{L}$, $\mathcal{D}_{U}$, $\theta_D$, $\theta_C$, and $k_{t+1}$ for next round.
\end{algorithm}


\section{Comparing Methods}
\label{sec:baselines}

We consider the following AL methods as baselines: 
\begin{itemize}
\item Random, which selects instances at random; 

\item Uncertainty, which selects instances with the highest entropy of predictions; 

\item Certainty, which selects instances with the lowest entropy of predictions; 

\item Coreset, which uses the concept of core-set selection to choose diverse instances; 

\item BADGE, which selects instances by considering both uncertainty and diversity in the gradient via k-means++ clustering; 

\item CCAL, which employs contrastive learning to extract the semantic and distinctive scores of examples for instance querying; 

\item MQNet, which balances the purity score and informativeness score to select instances through meta-learning; 

\item LfOSA, which selects instances based on the maximum activation value produced by the $(C+1)$-class detector; 

\item EOAL, which queries instances by calculating the entropy of examples in both known and unknown classes; 

\item BUAL, which queries instances by adaptively combining the uncertainty obtained from positive and negative classifiers trained in different ways. 
\end{itemize}
Among these methods, EOAL and BUAL are currently state-of-the-art.

\section{Additional Ablation Studies}
\label{sec:aas}


\begin{figure}[!h]
  \centering
   \includegraphics[width=1\linewidth]{figs/TR_aba_K.png}

   \caption{Ablation results for $K$ in reverse k-NN on CIFAR-10 (\textbf{Left}) and CIFAR-100 (\textbf{Right}). ``MR" denotes mismatch ratio. ``Best" indicates the top-performing method in the comparisons.}
   \label{fig:TR_aba_k}
\end{figure}

\begin{figure}[!h]
  \centering
   \includegraphics[width=1\linewidth]{figs/TR_aba_m_kno.png}
   \includegraphics[width=1\linewidth]{figs/TR_aba_m_unk.png}

   \caption{Ablation results for $m_{kno}$ and $m_{unk}$ in margin-based energy loss on CIFAR-10 (\textbf{Left}) and CIFAR-100 (\textbf{Right}). }
   \label{fig:TR_aba_m_kno_unk}
\end{figure}

Figure \ref{fig:TR_aba_k} illustrates the effect of the hyperparameter $K$ in reverse k-NN on EAOA's performance, with values set to [150, 200, 250, 300, 350]. 
Figure \ref{fig:TR_aba_m_kno_unk} presents the influence of the known class margin $m_{kno}$ and the unknown class margin $m_{unk}$ in margin-based energy loss $\mathcal{L} _{energy}$ on EAOA's performance, with values set to [-29, -27, -25, -23, -21] for $m_{kno}$ and [-11, -9, -7, -5, -3] for $m_{unk}$.
While the optimal value of $K$, $m_{kno}$, and $m_{unk}$ differ across different settings, their overall performance remains relatively stable compared to the top-performing method in the comparisons, with $K=250$, $m_{kno}=-25$, and $m_{unk}=-7$ consistently achieving strong results.



%Our method demonstrates high robustness to changes in this parameter.





\begin{figure}[!h]
  \centering
   \includegraphics[width=1\linewidth]{figs/TR_aba_k1.png}
   \includegraphics[width=1\linewidth]{figs/TR_aba_a1.png}
   \includegraphics[width=1\linewidth]{figs/TR_aba_b.png}

   \caption{Ablation results for $k_1$, $a$ and $z$ in target-driven adaptive sampling strategy on CIFAR-10 (\textbf{Left}) and CIFAR-100 (\textbf{Right}).}
   \label{fig:TR_aba_k1}
\end{figure}

Figure \ref{fig:TR_aba_k1} shows the impact of initial round $k_1$, variation amplitude $a$, and triggering threshold $z$ in Eq. \ref{k1} on EAOA's performance, with values set to [-3, -5, -7] for $k_1$, [0.5, 1, 1.5] for $a$, and [0.025, 0.05, 0.075] for $z$. An excessively large $k_1$ value may lead to initial rounds that prioritize aleatoric uncertainty, beneficial for lower mismatch ratios. Conversely, a small $k_1$ value emphasizes epistemic uncertainty, making it suitable for higher mismatch ratios. Here, $k_1=5$ consistently delivers strong performance across various datasets. In practical applications, prior knowledge about the dataset can be used to further adjust its value. For hyperparameters $a$ and $z$, their values are simply set to 1 and 0.05 (ensuring no adjustments are triggered when the difference between target and actual query precision is within $\pm $0.05, or a range of 0.1) respectively to simplify parameter tuning. Although the parameter selection here is intuitive, the results in Figure \ref{fig:TR_aba_k1} confirm its suitability.



%\section{Rationale}
%\label{sec:rationale}
% 
%Having the supplementary compiled together with the main paper means that:
% 
%\begin{itemize}
%\item The supplementary can back-reference sections of the main paper, for example, we can refer to \cref{sec:intro};
%\item The main paper can forward reference sub-sections within the supplementary explicitly (e.g. referring to a particular experiment); 
%\item When submitted to arXiv, the supplementary will already included at the end of the paper.
%\end{itemize}
% 
%To split the supplementary pages from the main paper, you can use \href{https://support.apple.com/en-ca/guide/preview/prvw11793/mac#:~:text=Delete%20a%20page%20from%20a,or%20choose%20Edit%20%3E%20Delete).}{Preview (on macOS)}, \href{https://www.adobe.com/acrobat/how-to/delete-pages-from-pdf.html#:~:text=Choose%20%E2%80%9CTools%E2%80%9D%20%3E%20%E2%80%9COrganize,or%20pages%20from%20the%20file.}{Adobe Acrobat} (on all OSs), as well as \href{https://superuser.com/questions/517986/is-it-possible-to-delete-some-pages-of-a-pdf-document}{command line tools}.



%\begin{figure}[!h]
%  \centering
%   \includegraphics[width=1\linewidth]{figs/TR_aba_v2.png}

%   \caption{TR aba v2}
%   \label{fig:TR_aba_v2}
%\end{figure}



