\documentclass[10pt,twocolumn,letterpaper]{article}
\usepackage[rebuttal]{cvpr}

% Include other packages here, before hyperref.
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{multirow}
% Import additional packages in the preamble file, before hyperref
\input{preamble}

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\definecolor{cvprblue}{rgb}{0.21,0.49,0.74}
\usepackage[pagebackref,breaklinks,colorlinks,allcolors=cvprblue]{hyperref}

% If you wish to avoid re-using figure, table, and equation numbers from
% the main paper, please uncomment the following and change the numbers
% appropriately.
%\setcounter{figure}{2}
%\setcounter{table}{1}
%\setcounter{equation}{2}

% If you wish to avoid re-using reference numbers from the main paper,
% please uncomment the following and change the counter value to the
% number of references you have in the main paper (here, 100).
%\makeatletter
%\apptocmd{\thebibliography}{\global\c@NAT@ctr 100\relax}{}{}
%\makeatother

%%%%%%%%% PAPER ID  - PLEASE UPDATE
\def\paperID{4223} % *** Enter the Paper ID here
\def\confName{CVPR}
\def\confYear{2025}

\begin{document}

%%%%%%%%% TITLE - PLEASE UPDATE
\title{Rethinking Epistemic and Aleatoric Uncertainty for Active Open-Set Annotation: An Energy-Based Approach}  % **** Enter the paper title here

\maketitle
\thispagestyle{empty}
\appendix

%%%%%%%%% BODY TEXT - ENTER YOUR RESPONSE BELOW

%We thank all reviewers for their valuable and constructive comments. Below we address the reviewers' comments in detail.

% Epistemic uncertainty is defined by combining the gap between the C+1 class prediction and the highest known-class prediction score.
\noindent \textcolor{red}{\textbf{Reviewer snsj: Q1.}} \textbf{Epistemic uncertainty (EU) is the gap between the C+1 class prediction and the highest known-class prediction score.} This appears to be a misunderstanding. EU is actually the difference between the energy score on all known classes (the first C classes) and that on the unknown class (the (C+1)-th class).


% Aleatoric uncertainty is represented as the gap between the top-2 prediction scores of the in-distribution classifier.
%\noindent \textcolor{red}{\textbf{Q2.}} \textbf{Aleatoric uncertainty (AU) is the gap between the top-2 in-distribution classifier prediction scores.} AU is approximately the discrepancy between the logit of the optimal label and the logit of the sub-optimal label.

%AU is actually the difference between the energy score of all classes and that of all suboptimal classes (except the most confident class). 


\noindent \textcolor{red}{\textbf{Q2.}} \textbf{Ambiguity in the definition of uncertainty measures.} While EU and aleatoric uncertainty (AU) are defined similarly, they differ fundamentally. Approximately, EU is used to measure $p(x)$, while AU is used to measure $p(y_{top1}|x) - p(y_{top2}|x)$. We will make this clear.

\noindent \textcolor{red}{\textbf{Q3.}} \textbf{What are the benefits of reformulating the method in terms of energy?} Uncertainty quantification, including EU and AU, plays a central role in active learning (AL). In Bayesian models, both are easy to measure, with EU typically regarded as more important [14]. However, in deep neural networks, EU is harder to estimate, leading existing methods to focus more on AU. Motivated by the definition of free energy and the characteristics of open-set AL tasks, this paper attempts to estimate both types of uncertainty in terms of energy. Extensive experiments demonstrate the feasibility and effectiveness of this attempt.


% The method does not exhibit clear distinctions from existing approaches.
\noindent \textcolor{red}{\textbf{Q4.}} \textbf{The method doesn't clearly differ from existing approaches.} Our core contribution is recognizing the need to simultaneously consider both uncertainties, with AU being effective only under the closed-set assumption. Inspired by free energy theory, we propose two corresponding uncertainty measures and an adaptive query strategy. To better assess EU, we introduce a data-centric logit formulation. To highlight the prediction difference between known and unknown class examples, we propose a margin-based energy loss. These are all novel contributions. While some methods also rely on \textit{feature similarity}, \textit{prediction gaps}, or \textit{in-distribution uncertainty}, our approach differs significantly in its implementation and has clear advantages. We will detail the differences with PAL (NeurIPS'23) in the revision.

%\textbf{The method doesn't clearly differ from existing approaches.} Our core contribution is recognizing the need to simultaneously consider both uncertainties, with AU being effective only under the closed-set assumption. Inspired by free energy theory, we propose two uncertainty measures and an adaptive query strategy. To better assess EU, we introduce a data-centric logit formulation. To highlight the prediction difference between known and unknown classes, we propose a margin-based energy loss. These contributions are novel. While some methods also rely on \textit{feature similarity}, \textit{prediction gaps}, or \textit{in-distribution uncertainty}, our approach differs significantly in both implementation and advantages.


\noindent \textcolor{red}{\textbf{Q5.}} \textbf{There might be a substantial overlap between samples with high scores for both measures.} Examples with high EU scores are in low-density regions of the representation space, while those with high AU scores are near the decision boundary. Some examples may lie in both, so you're right. Our approach tends to query examples that are in high-density regions and near the decision boundary, as the boundary here is more reliable. This offers another perspective on why AU works only under the closed-set property.

%\textbf{There might be a substantial overlap between samples with high scores for both measures.} Examples with high EU scores lie in low-density regions of the representation space, while those with high AU scores are near the decision boundary. Some examples may fall into both categories, so you're correct. Our approach tends to query examples in high-density regions near the decision boundary, as the boundary is more reliable. This provides another perspective on why AU works only under the closed-set assumption.

%This offers another perspective on our approach’s motivation.

\noindent \textcolor{red}{\textbf{Q6.}} \textbf{The correlation between EU and AU.} Based on the two uncertainty measures, examples can be categorized into four regions: (1) low EU and low AU, (2) low EU and high AU, (3) high EU and low AU, and (4) high EU and high AU. Region (1) examples are easy and already well-learned by the model. Region (2) examples are high-information known class examples. Regions (3) and (4) are more likely to contain unknown class examples, as the decision boundary (or the AU score) in these regions is unreliable.




\noindent \textcolor{blue}{\textbf{Reviewer jjWB: Q1.}} \textbf{If it consistently selects the same classes as known, such as classes 2 and 7 in CIFAR-10, this may introduce bias.} In the experiments, we followed the settings of EOAL, LfOSA, and BUAL, selecting fixed categories as known classes. To address your concern, we randomly chose a subset of categories as known classes each time, with partial results on CIFAR-100 shown below:


\begin{table}[h]
\centering
%\caption{Test accuracy of C-FreeMix with varying $\lambda_2$.}
\vspace{-6pt}
\label{tab:gamma-trends}
\resizebox{.91\columnwidth}{!}{\begin{tabular}{c|ccc}
     \toprule
          CIFAR-100 & 20\% & 30\% & 40\% \\
        \midrule
        EOAL & $71.45\pm2.21$ & $65.32\pm3.42$ & $62.24\pm4.04$ \\
        \midrule
        BUAL & $72.40\pm2.51$ & $67.30\pm4.20$ & $62.94\pm3.06$\\
        \midrule
        Ours & $\textbf{73.05}\pm2.93$ & $\textbf{68.60}\pm3.71$ & $\textbf{64.44}\pm3.69$\\
        \bottomrule
\end{tabular}}
\vspace{-6pt}
\end{table}


%Considering a transformer-based backbone model might yield different insights into the method’s effectiveness.
\noindent \textcolor{blue}{\textbf{Q2.}} \textbf{A transformer-based model might yield different insights into the method’s effectiveness.} We changed the base model to ViT-B, and partial results are shown below:


\begin{table}[h]
\centering
%\caption{Test accuracy of C-FreeMix with varying $\lambda_2$.}
\vspace{-6pt}
\label{tab:gamma-trends}
\resizebox{.91\columnwidth}{!}{\begin{tabular}{c|ccc}
     \toprule
          CIFAR-100 & 20\% & 30\% & 40\% \\
        \midrule
        EOAL & $58.98\pm0.98$ & $50.25\pm0.24$ & $46.55\pm0.42$ \\
        \midrule
        BUAL & $60.60\pm0.82$ & $49.80\pm1.13$ & $46.07\pm0.65$\\
        \midrule
        Ours & $\textbf{61.03}\pm0.66$ & $\textbf{50.93}\pm0.47$ & $\textbf{47.37}\pm0.55$\\
        \bottomrule
\end{tabular}}
\vspace{-6pt}
\end{table}

%How the process of randomly selecting known classes' portions to total classes affects the performance of the proposed method.
\noindent \textcolor{blue}{\textbf{Q3.}} \textbf{How the ratio of known classes to total classes affects the performance of the proposed method.} The chosen ratio values are highly challenging. Benefiting from the proposed adaptive sampling strategy, our method is not sensitive to changes in this ratio. We will add more discussion.

%\textbf{How the ratio of known classes to total classes affects the performance of the proposed method.} The chosen ratio values are highly challenging. Thanks to the proposed adaptive sampling strategy, our method is not sensitive to changes in this ratio. We will include further discussion.



\noindent \textcolor{magenta}{\textbf{Reviewer SJgA: Q1.}} \textbf{The method involves multiple components that may complicate its implementation and scalability.} Unlike previous methods, this paper introduces a novel energy framework, inevitably bringing in some new components. However, these components are not complex, as demonstrated by the faster runtime of our method. Also, they are well-modularized and can be used independently.

%\textbf{The method involves multiple components that may complicate its implementation and scalability.} Unlike previous methods, this paper introduces a novel energy framework, which inevitably adds new components. However, these components are not complex, as demonstrated by the faster runtime of our method. Furthermore, they are well-modularized and can be used independently.

%Unlike previous methods, this paper introduces a novel energy framework, bringing in new but simple components. The faster runtime of our method confirms this, and the components are well-modularized for independent use.

%Unlike previous methods, this paper adopts a novel energy framework, which inevitably introduces a series of new components. However, these components are not complex, as evidenced by the faster runtime of our method. Moreover, they are well modularized and can be used independently.

%Unlike previous methods, our approach is based on energy-based models, thus introducing additional components.

%We agree the method has multiple components, but each is designed to address specific aspects of the problem, contributing to its overall effectiveness. To ensure scalability, we've modularized them for independent optimization. We believe the benefits outweigh the complexity and will provide guidelines to facilitate real-world application.

%We agree that the method involves multiple components, but each component was designed to tackle specific aspects of the problem and contribute to the overall effectiveness of the approach. To address scalability, we have modularized the components to ensure that each part can be independently optimized and scaled. We believe that the benefits outweigh the complexity, and we will provide detailed implementation guidelines to ensure that the method can be efficiently applied in real-world scenarios.


% The strategy of distinguishing whether the unlabeled data belongs to known classes or unknown classes has been extensively used in open-set active learning. It is suggested to emphasize the contribution ‘adaptive adjustment of the candidate set size’ rather than the ‘coarse-to-fine querying strategy’.
\noindent \textcolor{magenta}{\textbf{Q2.}} \textbf{It is suggested to emphasize the contribution ‘adaptive adjustment of the candidate set size’.} Thank you for the insightful comment. We will adjust accordingly.

%\textbf{It is suggested to emphasize the contribution ‘adaptive adjustment of the candidate set size’.} Thank you for your insightful comment. We will make the necessary adjustments accordingly.

%Some OOD-related experiments can be added in the appendix to demonstrate the effectiveness of the proposed epistemic uncertainty.
\noindent \textcolor{magenta}{\textbf{Q3.}} \textbf{Adding OOD-related experiments can demonstrate the effectiveness of the proposed epistemic uncertainty (EU).} We modify LfOSA, BUAL, EOAL, and our method to compare their AUROC for unknown class recognition in the first and last active learning (AL) rounds on CIFAR-100 with a 30\% mismatch ratio. The results are shown below:

%We compare the AUROC of EU and some other methods for unknown class recognition in the first and last active learning rounds on CIFAR-100 with a 30\% mismatch ratio.


%We present the AUROC comparison of EU and several methods for unknown class recognition in the first and last active learning rounds on CIFAR-100 with a 30\% mismatch ratio.

\begin{table}[h]
\centering
%\caption{Test accuracy of C-FreeMix with varying $\lambda_2$.}
\vspace{-6pt}
\label{tab:gamma-trends}
\resizebox{.91\columnwidth}{!}{\begin{tabular}{c|cccc}
     \toprule
           & LfOSA & BUAL & EOAL & Ours (EU) \\
        \midrule
        First AL round & 54.61 & 50.57 & 55.57 & \textbf{60.23}\\
        \midrule
        Last AL round & 60.73 & 51.26 & 65.76 & \textbf{71.29}\\
        \bottomrule
\end{tabular}}
\vspace{-6pt}
\end{table}


%%%%%%%%% REFERENCES
%{
%    \small
%    \bibliographystyle{ieeenat_fullname}
%    \bibliography{main}
%}

\end{document}
