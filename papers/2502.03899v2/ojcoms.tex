\documentclass{IEEEoj}
\usepackage[table,xcdraw]{xcolor}
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx,color}
\usepackage{textcomp}
\usepackage{glossaries}
\usepackage{multirow}
\usepackage{url}
\usepackage{comment}
\usepackage[utf8]{inputenc}
\usepackage{pgfplots}
\usepackage{afterpage}
\usepackage{orcidlink}
\usepackage{ulem} 
\usepackage{soul}
\pgfplotsset{compat=newest}
\usepackage{tikz}
%\tikzset{external/export=false}
\usetikzlibrary{external,patterns,shapes.arrows}
%\tikzexternalize[prefix=IEEE_OJCOMS-template-LaTex_202401/tikz/]
\DeclareUnicodeCharacter{2212}{−}
\usepgfplotslibrary{groupplots,dateplot}
\usepackage{adjustbox}


\newcommand{\rev}[1]{\textcolor{black}{#1}}



\input{files/glossary}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\AtBeginDocument{\definecolor{ojcolor}{cmyk}{0.93,0.59,0.15,0.02}}
\def\OJlogo{\vspace{-4pt}\hskip-4pt}
\bibliographystyle{ieeetr}

\begin{document}
\title{A Slicing Model for Transport Networks with Traffic Burst Control and QoS Compliance for Traffic Flows}

\author{Aitor Encinas-Alonso\IEEEauthorrefmark{1}\orcidlink{0009-0000-1290-8815}, Carlos M. Lentisco\IEEEauthorrefmark{1}\orcidlink{0000-0002-7444-0872}, Ignacio Soto\IEEEauthorrefmark{1}\orcidlink{0000-0002-7421-3733 }, Luis Bellido\IEEEauthorrefmark{1}\orcidlink{0000-0001-9591-0928}, David Fernandez\IEEEauthorrefmark{1}\orcidlink{0000-0002-2172-9162}
}
\affil{Departamento de Ingeniería de Sistemas Telemáticos, ETSI Telecomunicación, Universidad Politécnica de Madrid, Spain}
\corresp{CORRESPONDING AUTHOR: Aitor Encinas-Alonso (e-mail: aitor.encinas.alonso@alumnos.upm.es).}
\authornote{This work was partially supported by the Remote Driver project (TSI-065100-2022-003) funded by the Ministerio de Asuntos Económicos y Transformación Digital (Gobierno de España) and the European Union's Horizon Europe research and innovation program under Grant Agreement No. 101097122 (ACROSS).}
\markboth{Preparation of Papers for IEEE OPEN JOURNALS}{Author \textit{et al.}}


\begin{abstract}
Network slicing has emerged as a key network technology, providing network operators with the means to offer virtual networks to vertical users over a single physical network infrastructure. Recent research has resulted mainly in techniques for managing and deploying network slices, but the implementation of network slices on a real physical transport network infrastructure has received much less attention. Standardization bodies, such as the \gls{ietf}, have provided some implementation recommendations. Still, there is a lack of mechanisms to implement network slices capable of handling traffic bursts while simultaneously meeting the \gls{qos} requirements of the traffic flows associated with the slices. In this paper, we propose a novel fine-grained resource control mechanism to implement transport network slices that meet traffic \gls{qos} requirements while both accepting limited traffic bursts, and enabling efficient bandwidth sharing within and across slices. The mechanism is executed at the edge of the transport network. The proposed model aligns with current standards on network slicing and has been tested on an experimental platform. Using this platform, we have conducted an extensive experimental campaign that demonstrates that our proposal can effectively control traffic bursts generated within the network slices while maximizing bandwidth utilization across the network.
\end{abstract}

\begin{IEEEkeywords}
Network Slicing, Quality of Service, Communication System Traffic Control,  Mobile Communication, Transport Network.
\end{IEEEkeywords}

\maketitle

\section{Introduction}
\label{sec:intro}
Although the deployment of commercial 5G networks is a reality in many countries worldwide, the most advanced features proposed in the \gls{3gpp} 5G standards are still under investigation. Among them, network slicing promises to revolutionize the way in which network infrastructures are exploited. With network slicing, a network infrastructure can be shared to provide different and isolated network services~\cite{3GPP2024_1}. Network slicing allows network operators to provide different communication services with tailored quality levels. Examples are \gls{urllc}, \gls{embb}, or \gls{miot}. Network slicing techniques can be applied to the different segments that compose a mobile network: the \gls{ran}, the \gls{tn}, and \gls{cn}. While the \gls{ran} and \gls{cn} fall under the domain of \gls{3gpp}, the \gls{tn} belongs to the \gls{ietf}'s domain. A mobile network operator may predefine a set of network slices, allowing external customers to subscribe to the communication services that best suit their applications. Additionally, operators can enable customers to define their own network slices based on specific needs. Vertical industries are expected to be one of the main beneficiaries of network slicing, obtaining customized connectivity services through a shared network infrastructure. 

Up to this date, the state of the art has focused on the problem of allocating network resources to different network slices efficiently, especially in the \gls{ran} segment of the network. Regarding the transport network, the \gls{ietf} has defined models for controlling the allocation of resources to different network slices, but these models do not provide the ability to limit the impact on the \gls{qos} of the traffic associated with services delivered over such slices. 
%\rev{In the literature, network slicing models have been defined that provide mechanisms to guarantee bandwidth and delay for slices, with some proposals even including mechanisms for burst traffic control. However, none of these models integrate all these capabilities in a cohesive manner. This paper advances in this direction by proposing \gls{hctns}, a model for transport network slicing that efficiently manage the available bandwidth while fulfilling service \gls{qos} requirements. The  results demonstrate that our proposal achieves better outcomes in reducing the delay of flows associated with slices and enhances resource utilization when these are shared among different slices. Furthermore, it integrates a burst traffic control mechanism in a unified manner.} This paper advances in this direction by proposing \gls{hctns}, a network slicing model for transport networks that efficiently manage the available bandwidth while fulfilling service \gls{qos} requirements. 

Our proposal is applied to a scenario where standard network slices such as \gls{urllc} and \gls{embb} 
are sharing the physical network infrastructure with a network slice for \gls{tod}. \gls{tod} is an advanced automotive industry use case that enables drivers to operate vehicles remotely~\cite{Amador2022}. 

%\sout{\rev{Originally designed to assist autonomous driving in situations where vehicles cannot make decisions independently, \gls{tod} has gained interest from sectors such as public and logistic transportation as a standalone service.}}

In our target scenario, a \gls{tod} service provider signs a contract with an operator of a public mobile network to hire a network slice for the \gls{tod} service. Within the \gls{tod} service, various traffic flows with different \gls{qos} requirements (video, telemetry, and commands) are generated. Consequently, the network slice for \gls{tod} must be capable of providing differentiated traffic treatment for each type of traffic flow within the slice. 


The main contributions of this work are:
\begin{itemize}

\item The design of a flexible slicing model for transport networks, named \gls{hctns}, aligned with current \gls{ietf} standards, enhancing bandwidth sharing between traffic classes and slices while providing explicit control over how bandwidth is allocated. The model also ensures consistent \gls{qos} behavior for all traffic accepted in the \gls{tn} within the same traffic class, or, in its absence, within the same slice.

\item \gls{hctns} incorporates a novel traffic policer that optimizes the utilization of available bandwidth both across slices and within individual slices. 

\item A new traffic burst control mechanism that enables the network to accommodate traffic bursts while managing their impact on the \gls{qos} experienced by the different traffic flows in the \gls{tn} through configurable parameters.

\item A performance evaluation, based on an implemented prototype, showing that by applying our proposal it is possible to meet the strict \gls{qos} requirements of services such as \gls{tod}. The results confirm the advantages of \gls{hctns} in ensuring that all traffic flows within any network slice adhere to the defined \gls{qos} requirements. \rev{Moreover, the experiments show that our proposal outperforms the 5G transport network slicing model that is being standardized by the IETF and a reference proposal in the literature.}
\end{itemize}

The remainder of the paper is organized as follows. Section~\ref{sec:soa} reviews the related work and highlights the contributions of this paper to the state of the art.
Section~\ref{sec:background} describes the IETF 5G transport network slicing model, which serves as the reference model for our proposed work. 
Section~\ref{sec:proposal} describes the \gls{hctns} network slicing and QoS model.
Section~\ref{sec:eval} presents our testbed with a simplified version of our proposed network slicing model for 5G transport networks, configured to support teleoperated driving services sharing the same physical network with other typical and constrained slices. This section is also dedicated to the evaluation of the proposal. Finally,  Section~\ref{sec:conclusion} summarizes the main conclusions of our work. 

\rev{A summary of key acronyms used in the article is presented in Table~\ref{table:acronyms}.}

\begin{table}[tbh]
\centering
\begin{tabular}{ |l l| } 
\hline
\rowcolor{lightgray} Acronym & Definition \\
\hline
\hline
5QI & 5G Quality Indicator \\
BE & Best Effort \\
CBS & Committed Burst Size \\
CIR & Committed Information Rate \\
DRR & Deficit Round Robin \\
DSCP & Differentiated Services Code Point \\
eMBB & enhanced Mobile Broadband \\
HCTNS & Hierarchically Controlled Transport Network Slicing \\
HTB & Hierarchical Token Bucket \\
PBS & Peak Burst Size \\
PDB & Packet Delay Budget \\
PE & Provider Edge \\
PHB & Per-Hop-Behaviour \\
PIR & Peak Information Rate \\
PQ & Priority Queue \\
S-NSSAI & Single - Network Slice Selection Assistance Information \\
SDP & Service Demarcation Point \\
SLA & Service Level Agreement \\
SR & Segment Routing \\
SRH & Segment Router Header \\
TC & Traffic Control \\
TE & Traffic Engineering \\
TN & Transport Network \\
ToD & Teleoperated Driving \\
trTCM & two rate Three Color Marker \\
TS & Technical Specification \\
UPF & User Plane Function \\
URLLC & Ultra Reliable Low Latency Communications \\
VLAN & Virtual Local Area Network \\
VNX & Virtual Networks over LinuX \\
WFQ & Weighted Fair Queuing \\
\hline
\end{tabular}
\vspace*{0.25cm}
\caption{\rev{Summary of important acronyms.}}
\label{table:acronyms}
\end{table}

\section{Related Work}
\label{sec:soa}
\subsection{Network Slicing Standards}
\label{subsec:soa-standards}
The \gls{3gpp} has done a great deal of standardization work on network slicing and \gls{qos} in 5G. \gls{ts} 23.501 \cite{3GPP2023_1} defines the overall 5G system architecture, including a framework providing \gls{qos} to network flows and an identification mechanism for managing network slices. 

\gls{ts} 28.530~\cite{3GPP2024_1} defines the general concepts and definitions for network slicing, as well as the phases that compose the lifecyle of a network slice instance. \gls{ts} 23.502~\cite{3GPP2023_2} describes the procedures to define the \gls{qos} policies in the 5G network functions and to connect mobile terminals to slices. \gls{tr} 28.801~\cite{3GPP2018} defines network slice management functions. Management operations and procedures for provisioning network slices are defined in \gls{ts} 28.533~\cite{3GPP2024_2}.

The \protect\gls{gsma}~\cite{GSMA} has defined generic network slices templates (GST) that contain a set of attributes that can characterize a type of network slice or service. A \gls{nest} is obtained by assigning specific values to the fields in the GST. 3GPP network slice management functions build a network slice based on a provided \gls{nest}.

The above standards define control and management plane mechanisms required for the administration and orchestration of network slices. However, they do not specify how network slices can be implemented in the data plane. A topic neither addressed by the above standards is how to define an end-to-end network slice across the \gls{ran}, \gls{tn}, and core networks. 

The \gls{ietf} has made progress in this area by defining: (1) an orchestration and management framework that enables the inter-operation between 5G network slices in non-\gls{3gpp} \glspl{tn} that connect with network slices defined within a \gls{3gpp} domain~\cite{draft-ietf-teas-5g-network-slice-application}; and (2) how to identify a \gls{3gpp} network slice for associating it to a \gls{tn} slice and how the \glspl{5qi}, which define levels of \gls{qos} for the traffic flows, are translated to the \gls{tn} classes in the non-\gls{3gpp} domain~\cite{draft-ietf-teas-5g-ns-ip-mpls}~\cite{draft-cbs-teas-5qi-to-dscp-mapping}. By means of this identification, the \gls{tn} is capable of treating these traffic flows in the \gls{tn} with the expected level of \gls{qos}. This makes it possible to create end-to-end slices across all the network segments. 


Network slice management, coordination and signaling between \gls{3gpp} and \gls{ietf} control elements are also necessary and partially defined in \cite{rfc9543, draft-ietf-teas-5g-network-slice-application}, but they are out of the scope of this paper.

\subsection{Network Slicing Literature}
\label{subsec:soa-literature}
There is extensive literature covering the application of network slicing techniques across the different segments of the network. Wang et al.~\cite{Wang2019} followed an approach based on queuing disciplines implemented over a software-defined network based on P4. They proposed a hierarchical queuing structure that has two levels. The first one is composed of eight round robin queues that ensure a proper load balance between the processed traffic. In the second level, each round robin queue is connected to four priority queues. In this way, it is possible to limit the delay of different traffic classes in each round robin queue. The problem with this approach is that it is not aligned with the model proposed by \gls{ietf} for \glspl{tn}, for instance, because it does not consider a policer controlling the traffic entering the network.

Other works~\cite{Bosk2021},~\cite{Gajic2022} proposed using traffic shaping mechanisms based on \gls{htb} for providing each network slice with a bandwidth guarantee and allowing a network slice to consume unused bandwidth by other slices. The proposed traffic shaping mechanism provides traffic isolation between network slices that are sharing the network infrastructure. Raussi \textit{et al.}~\cite{Raussi2023} also proposed the use of traffic shaping mechanisms based on \glspl{htb} to improve the reliability of communications in smart grids scenarios with wired connections. Lin et al.~\cite{Lin2021} implemented \gls{qos} framework over a P4-based network composed of four functional blocks: a classifier, a marker, a policer and a packet scheduler based on priority queues. In this work, all traffic marked as \textit{``green"} from all of the slices is enqueued into the same priority queue, so, it is not possible to control the delay of the \textit{``green"} traffic of each slice. Additionally, traffic marked as \textit{``yellow"} is enqueued in a lower priority queue, so it does not meet the latency requirements. \rev{Chen \textit{et al.} proposal~\cite{Chen2022}  uses two priority queues, one for the \textit{``green"} traffic coming from all the slices, and a lower priority one for \textit{``yellow"} traffic from all the slices and for non-sliced or best-effort traffic (this is a similar arrangement to~\cite{Lin2021}). The lower priority queue is further divided into four queues using a \gls{drr} to separate different types of flows, which allows to control the sharing of the available bandwidth beyond the one guaranteed to the network slices. This proposal meets the bandwidth slice requirements and has a mechanism to control the sharing of the additional bandwidth available, but because all the slices share the same queue, bandwidth sharing among them is not controlled and delay constraints are not addressed. In general, proposals that use \textit{``yellow"} traffic to enable bandwidth sharing have the drawback that losses and delay for such traffic cannot be controlled.}

\rev{Huin \textit{et al.}~\cite{huin} and Martin \textit{et al.}~\cite{martin} have followed a different approach, where network resources are exclusively dedicated to each slice. While this approach provides a high level of isolation between slices, it reduces network utilization and efficiency by preventing the sharing of unused bandwidth from one slice with others, potentially leading to under-utilization of resources.}

None of the above works addressed the problem of control the delay of different types of traffic flows. On the other hand, \rev{Chang \textit{et al.}\rev{~\cite{chang-5growth, Chang2021}} proposed a network slicing technique at the data link layer (L2) that is able to meet the latency and bandwidth requirements of different network slices by using a queuing system based on priority queues and Active Queue Management (AQM). The proposed model is implemented over a network of P4-programmable data plane switches, an approach that is still not aligned with \gls{ietf} priorities. Besides, a worst-case scenario with bursty traffic, as proposed in this paper, has also not been analyzed either.}

Baba et al. ~\cite{Baba2019} analyzed the impact of micro-bursts on the \gls{qos} of the 5G network, comparing the case of using a priority queuing scheduler versus a \gls{wfq} scheduler. However, they did not propose a solution to limit this impact.

Regarding vehicular networks, several works~\cite{Cui2022,Ndikumana2023,Cui2023,Cui2024,Zamfirescu2024} focused on the management of resources on the radio interface to support network slicing. The work in~\cite{Khan2021}, in addition to studying slicing techniques in the radio interface, also explored how to apply network slicing in the core of the network, and proposed the use of priority queues to achieve a latency in mission critical traffic lower than in best effort traffic. For the specific case of \gls{tod}, the work in~\cite{Campolo2017,Campolo2018} identified the need to define a network slice capable of meeting the strict \gls{qos} requirements of the service. 

\rev{None of the existing works in the state of the art align with the slicing model defined by the \gls{ietf} for 5G \glspl{tn}. Additionally, they fail to account for multiple flows with different \gls{qos} requirements per slice or to address how to limit delays while guaranteeing bandwidth requirements in worst-case scenarios under bursty traffic conditions. The solution proposed in this paper makes it possible to satisfy both the bandwidth and the latency requirements of \glspl{tn} slices and incorporate traffic burst control for worst-case scenarios.} 

\section{Background: The IETF Network Slice Model}
\label{sec:background}
\rev{This section provides an overview of the key features of the slicing model being defined by the \gls{ietf} for transport networks.}

%Transport networks are provider networks that interconnect two sites or different network domains. As transport networks typically carry very high volumes of traffic from diverse sources and destinations, delivering end-to-end network slicing while maintaining \gls{qos} requires the implementation of mechanisms that uphold the network slicing and \gls{qos} requirements demanded. To achieve this, it is essential to establish \glspl{sla} between the transport network and the other domains. In a transport network, it is not always feasible to maintain the same level of granularity in traffic handling as within a specific network domain. In a 5G network, many different network slices may exist, each containing multiple traffic flows with specific \gls{qos} characteristics, identified by a \gls{5qi} value. While some \gls{5qi} values are standardized, others are network-specific. To ensure scalability, transport networks often aggregate slices and traffic flows with similar \gls{qos} characteristics, maintaining \gls{qos} but with reduced granularity. In this way, the \gls{ietf} has defined a network slice model in~\cite{rfc9543, draft-ietf-teas-5g-ns-ip-mpls} for transport networks. We refer to this solution as \gls{ietf} model. This model addresses the scalability challenge by proposing mapping models between 5G network slices and transport network slices, as well as aggregating \gls{qos} flows into \gls{tn} \gls{qos} classes. Moreover, the \gls{ietf} has identified some mechanisms for transport network slice realization. 

The slicing model defined by the \gls{ietf}~\cite{rfc9543, draft-ietf-teas-5g-ns-ip-mpls} is composed of three parts. First, the model defines how the \gls{3gpp} 5G network slice identifier, namely the \gls{snssai}, which cannot be used in the \gls{ietf} transport network domain, can be identified in this domain by using L2/L3 header fields such as the VLAN Identifier or the MPLS label. The second part of the model focuses on how the \gls{qos} indicators used in the 5G network are mapped to values of the DSCP field of the IP header to mark the traffic in the transport network, with concrete recommendations for this mapping process given in~\cite{draft-cbs-teas-5qi-to-dscp-mapping}. Finally, the model defines how the data plane traffic is treated in the transport network so that the \gls{qos} requirements of the flows are correspondingly satisfied. \rev{Our proposal, described in Section~\ref{sec:proposal}, focuses on network realization, but we  also describe the other features considered to be part of our model}.

An \gls{ietf} network slice is defined between a set of \glspl{sdp}, that is, points of attachment for customers connecting their corresponding network slices. An \gls{sdp} has a unique identifier in the provider network scope, for instance, an IP address, a VLAN tag, an MPLS label, or an interface/port number. The network slice provides connectivity between the \glspl{sdp} and satisfies the latency and bandwidth requirements agreed between the network slice provider and the network slice consumer in the \gls{sla}. 

Below, we explain the components of the \gls{ietf} network slice model in more detail.

\subsection{Network Slicing Identification}
\label{subsec:background-nsi}
End-to-end network slices in the \gls{3gpp} domain are identified by \glspl{snssai} that are not visible in the \gls{ietf} domain. \mbox{\glspl{snssai}} are managed in the \gls{ran} and core segments of the 5G network, for example, to apply differentiated treatment in terms of \gls{qos} to the slices defined in the network. But, since the \gls{snssai} is not visible in the \gls{ietf} domain, different mechanisms have been proposed for the \gls{tn} to be able to identify the network slice that is associated to the incoming data traffic. These mechanisms, known as hand-off methods~\cite{draft-ietf-teas-5g-ns-ip-mpls, draft-ietf-teas-5g-network-slice-application}, are based on the use of L2, L3, or L4 identifiers. The main hand-off methods are detailed below:

\begin{itemize}
    \item \gls{vlan} hand-off. A \gls{vlan} tag is added to all the traffic exchanged between the transport network and the \gls{ran} or core networks. The VLAN ID is used by the \gls{pe} transport network nodes to identify the 5G network slice. Each \gls{sdp} is represented by a \gls{vlan} ID (or double \gls{vlan} with QinQ) and each \gls{vlan} represents a separated logical interface on the \glspl{pe}. The VLAN ID is only used to identify the 5G network slice, so the VLAN tag is removed by the \gls{pe} routers when forwarding the data traffic within the transport network.

    %A \gls{vlan} tag is added to all the traffic exchanged between the transport network and the \gls{ran} or core networks. The VLAN ID is used by the \gls{pe} transport network nodes to identify the 5G network slice. The \gls{pe} routers remove the VLAN tag when forwarding the data traffic within the transport network.
        
    \item IP hand-off (IPv4 or IPv6). It consists on establishing associations between \glspl{snssai} and source/destination IP addresses. The association can be done in three different ways: (1) the \gls{snssai} is associated to the IP address allocated to the gNB or the \gls{upf}; (2) the \gls{snssai} is associated to a range of IP addresses allocated to a set of gNBs and \glspl{upf}; (3) the \gls{snssai} is associated to a subset of bits of an IP address, or (4) the \gls{snssai} is associated to a \gls{dscp} value. 
    \item \gls{mpls} Label hand-off. \gls{mpls} labels are used by the \gls{pe} nodes to infer the identification of the 5G network slice.
    \item \gls{udp} source port hand-off. \gls{udp} source ports carried over \gls{gtp}-U tunnels are used to identify the 5G network slice. 
\end{itemize}

Regardless of the identification method used, it is also necessary to map 5G network slices to \gls{tn} slices. In \cite{draft-ietf-teas-5g-ns-ip-mpls}, the \gls{ietf} proposes three alternatives for making this mapping

\subsection{QoS Flow Identification}
\label{subsec:background-qfi}

As in the previous case, the \gls{3gpp} \gls{qos} identifiers are only visible in the \gls{3gpp} domain, which makes it necessary to translate them in the \gls{ietf} domain for providing different \gls{qos} treatments in the \glspl{tn} for each \gls{3gpp} 5G slice or for each 5G \gls{qos} flow within each \gls{3gpp} 5G slice. 

Each \gls{qos} flow in the \gls{3gpp} domain, either sent from the \gls{upf} to the \gls{ran} or vice versa, traverses the \gls{tn} through a \gls{gtp} tunnel, and it is identified by a \gls{qfi} that is carried in the \gls{gtp} header. Using these \glspl{qfi}, gNBs and \glspl{upf} apply the \gls{qos} policies defined in the \gls{3gpp} domain. A \gls{qfi} is associated with a \gls{5qi} that indicates the \gls{qos} characteristics of the traffic flow, such as the \gls{pdb} or the priority level. Packets from \gls{qos} flows are marked by using the \gls{dscp} field of the IP header. The \gls{ietf} describes an example of how the mapping between a \gls{5qi} and a \gls{dscp} can be done~\cite{draft-cbs-teas-5qi-to-dscp-mapping}. A \gls{5qi} can take multiple values, as has been defined in TS.23.501~\cite{3GPP2023_1}, and, in practice, \glspl{5qi} with similar characteristics are mapped to the same \gls{dscp} value. Packets received on the ingress ports of the \gls{pe} routers of the transport network are marked with these \gls{dscp} values. These marks enable the \gls{pe} routers to classify the incoming traffic and associate it with a \gls{tn} \gls{qos} class, so traffic can be treated according to the characteristics defined by the corresponding \glspl{5qi}. This association is defined by a label in the headers added to the packets when transported through a tunnel in the transport network, commonly referred to as the \gls{tn} \gls{qos} mark. These tunnels can be implemented over MPLS networks (and in this case, the Traffic Class field is used) or over IPv6 (\gls{dscp} values are used in this case). In the \gls{ietf} model, eight transport network classes have been considered, as typical router hardware supports up to eight traffic queues per port. These \gls{tn} \gls{qos} classes determine a \gls{phb} enforced to packets in each \gls{tn} node. From the eight available transport network classes, according to \cite{draft-cbs-teas-5qi-to-dscp-mapping}, four are reserved for the data plane traffic and the remaining four for control plane traffic. Consequently, data traffic is grouped into four \gls{5qi} categories: 

\begin{itemize}
    \item Group 1: delay-critical traffic with \gls{gbr}, allowing packet losses between $10^{-6}$ and $10^{-4}$).
    \item Group 2: traffic with moderated delay and varying packet loss levels.
    \item Group 3: other \gls{gbr} traffic not included in Groups 1 or 2.  
    \item Group 4: \gls{5qi} values assigned for non-\gls{gbr} traffic.
\end{itemize}

 For describing our proposal, the above groups will be referred as \gls{tn} \gls{qos} class A, \gls{tn} \gls{qos} class B, \gls{tn} \gls{qos} class C, and \gls{tn} \gls{qos} class D, respectively. 


\subsection{IETF Network Slice Realization}
\label{subsec:background-realization}
An important part of the \gls{ietf} model described in~\cite{draft-ietf-teas-5g-ns-ip-mpls} is the realization of the transport network slices. Implementing a network slice in the \gls{ietf} domain requires combining different network mechanisms. The mechanisms identified in the \gls{ietf} draft are the following: L2VPN and/or L3VPN service instances for logical separation of the slices; fine-grained resource control for enforcing the bandwidth contract at the edges of the provider network for each \gls{qos} flow and slice; coarse-grained resource control for applying \gls{qos} mechanisms to flows aggregated in traffic classes within the transport network; and capacity planning/management mechanisms ensuring that enough capacity is available across the transport network for the network slices. Below, a more \rev{detailed} description of these mechanisms is provided.

\subsubsection{L2VPN and/or L3VPN for slices isolation}
L2VPN and/or L3VPN service instances might be deployed for achieving logical separation of slices. This results in an additional outer header due to the packet encapsulation carried out in the border nodes hosting the services instances, i.e., in the \glspl{pe}. This also provides a clear separation between the \gls{dscp} value that is used to identify the \gls{qos} in traffic received from the \gls{3gpp} domain and the \gls{tn} \gls{qos} mark used to determine the \gls{qos} in the \gls{tn}. These mechanisms might also be used to deploy different underlay transport paths optimized according to the \rev{latency and bandwidth requirements specified in the} \glspl{sla} of different 5G \gls{qos} flows. For example, for traffic with low latency requirements, a path with fewer hops would be preferable.

It is important to note that the \gls{tn} \gls{qos} classes and the underlay transport path have different targets. The \gls{tn} \gls{qos} class defines the \gls{phb} enforced to packets that transit the \gls{tn}, whereas the underlay transport determines the overall path taken by packets based on the operator’s requirements. These underlay transports can be realized through various mechanisms, such as \gls{rsvpte} or \gls{srte} tunnels.

\subsubsection{Fine-grained resource control at the edge of the TN}

This mechanism works as an admission control that enforces the bandwidth contract at the edge of the provider network by using traffic policers. A \gls{pe} receiving traffic from the 5G network, i.e., an ingress \gls{pe}, enforces a rate limitation policy to guarantee bandwidth per slice and per traffic class within each slice. Since resources are controlled for each traffic class and/or slice, this approach is referred to as fine-grained resource control. The \gls{ietf} describes a model (known as \textit{5QI-aware model}) in which the traffic policer is implemented by means of a hierarchical model. On the one hand, the \gls{pe} includes a traffic policer per slice (\textit{slice policer}). This traffic policer follows a two-rate three-color rate limiter approach. Traffic exceeding the \gls{pir} (maximum bitrate) is dropped. Traffic under the \gls{cir} is marked as ``green", and traffic between the \gls{cir} and \gls{pir} is marked as ``yellow''. In this way ``yellow'' traffic will be dropped in case of congestion. On the other hand, the \gls{pe} includes a traffic policer per \gls{qos} class. This traffic policer follows a single-rate two-color rate limiter approach, where only a \gls{cir} is defined and all traffic exceeding this \gls{cir} is dropped. The class policer does not apply any rate limitation to the best effort traffic, allowing this type of traffic to use the bandwidth in the slice that is not used by the \gls{qos} classes.

A \gls{pe} forwarding traffic to the 5G network, i.e., an egress \gls{pe}, may optionally incorporate a hierarchical scheduler or shaper that ensures the guaranteed bandwidth for each \gls{cir} slice and for each \gls{cir} \gls{qos} class defined.

\rev{In this paper, we propose a novel fine-grained resource control based on a \gls{htb} scheduler composed of three levels (Section~\ref{sec:proposal}.\ref{subsec:proposal-finegraine}).}

\subsubsection{Coarse-grained resource control in the transit links}

For the transit nodes of the transport network, the \gls{ietf} proposes a simplified traffic treatment mechanism based on the combination of priority queues and \gls{wfq} or \gls{drr} queues. As a result, there is no fine-grained resource control at the transit nodes as they do not perform traffic treatments per traffic class. Instead, they handle data traffic following a DiffServ model that supports up to eight transport network \gls{qos} classes (as explained in Section~\ref{sec:background}.\ref{subsec:background-qfi}). All the slices defined in the \gls{3gpp} domain are mapped to a \gls{tn} \gls{qos} class in the \gls{ietf} domain. In scenarios where the number of classes or slices is greater than the number of \gls{tn} \gls{qos} classes, it is possible to aggregate multiple slices or classes into a single \gls{tn} \gls{qos} class.  

Typically, traffic classes with strict latency requirements are associated with priority queues, while others are linked to \gls{wfq} or \gls{drr} non-priority queues. Non-priority queues are configured so that their weights are adjusted according to the \gls{qos} requirements of the classes sharing a \gls{tn} \gls{qos} class. With this coarse-grained resource control, traffic from best effort classes or traffic marked as ``yellow'' can be discarded during network congestion events while maintaining the \gls{qos} of the traffic flows in other classes.

This flat \gls{qos} model, based on traffic scheduling/prioritization, entails a different \gls{qos} treatment which is enforced in all \gls{tn} nodes at transit links to apply the same \gls{phb} for all packets that belong to each \gls{tn} \gls{qos} class.

\subsubsection{Capacity planning/management}
\gls{te} mechanisms, based on technologies such as \gls{rsvpte} or \gls{srte} tunnels, are employed to ensure that sufficient capacity is available along the transport network for all the accepted slices. This is a topic that is not covered in this paper. 

\section{HCTNS Model}
\label{sec:proposal}
\subsection{General View}
\label{subsec:general-view}

\begin{figure*}[th]
\vspace*{-0.12cm}
\centerline{\includegraphics[width=1.58\columnwidth]{figures/general-view_v2.pdf}}
\caption{General view of the \gls{hctns} model.} % detallar un poquito más
\label{general-view}
\end{figure*}

A general view of the network slicing model proposed in this paper is illustrated in Fig.~\ref{general-view}, which also shows where our proposal for the ingress \textit{fine-grained resource control} is placed. The top of the figure shows a simplified view of a 5G transport network, illustrating how data traffic is double-encapsulated into two tunnels: GTP and IPv6. The bottom of the figure shows the protocol stack for packets transmitted in the uplink direction.

Our design for the fine-grained ingress resource control addresses some limitations we have identified in the \gls{ietf} model (described in Section~\ref{sec:background}.\ref{subsec:background-realization}): (1) Inflexible and inefficient bandwidth sharing between different 5G \gls{qos} classes and slices. The \gls{ietf} model allows unused bandwidth in a transport network class to be used only by the best-effort traffic. In contrast, \gls{hctns} improves resource utilization between 5G \gls{qos} classes and between slices because the bandwidth not consumed by a traffic class is made available for use by other traffic classes in the same slice or by other network slices. To address this, we propose a three-level hierarchical ingress policing mechanism, including a new top-level policer, referred to as \textit{global policer} (1st level), followed by \textit{slice policers} (2nd level), and \textit{class policers} (3rd level). Furthermore, we propose two mechanisms to determine how the bandwidth is shared between the 5G \gls{qos} classes of a slice and between the slices. (2) Lack of a traffic burst control mechanism. Some services may need a traffic burst control for meeting their applications demands. However, traffic bursts, as it will be demonstrated in Section~\ref{sec:eval}, can increase packet delays and compromise the \gls{qos} levels agreed in the \glspl{sla} associated with the classes and slices. For instance, packet delay in classes associated to \gls{tn} \gls{qos} class B may be increased when there are traffic bursts in the traffic classes that are assigned to \gls{tn} \gls{qos} class A. To address this, \gls{hctns} incorporates a mechanism to control traffic bursts. This mechanism is based on the definition of two new parameters at the class policers. The three-level structure of the \gls{htb} that we propose controls the maximum accepted rate, i.e., global \gls{cir}, while also guaranteeing the \glspl{cir} per class and slice even in worst-case networks scenarios with bursty traffic. (3) In the \gls{ietf} model, ``yellow'' traffic (the traffic of a class that arrives at the transport network's ingress at a rate between the \gls{cir} and the \gls{pir}) can cause network congestion, and may be discarded at transit nodes of the transport network. In contrast, our approach ensures that all traffic admitted by the policers and forwarded through the transport network is treated with the same \gls{qos} guarantees, without causing congestion, and with no packet losses. 

\rev{\gls{hctns}, in addition to our proposed \textit{fine-grained ingress resource control} component, incorporates some of the features of the slicing model defined by the \gls{ietf} (Section~\ref{sec:background}). These features are the following:}

\begin{itemize}

    \item Network Slice Identification (Section~\ref{sec:background}.\ref{subsec:background-nsi}): \gls{hctns} assumes the use of \gls{vlan} tags to identify network slices in the border between the \gls{3gpp} and \gls{ietf} domains. As shown in Fig.~\ref{general-view},  the Ethernet frame transmitted by the gNB to the ingress \gls{pe} router includes the 802.1Q extension header, which carries the VLAN identifier of the slice. Any other type of the hand-off mechanisms \rev{ described in Section~\ref{sec:background}.\ref{subsec:background-nsi}}, could also be applied. 
    
    \item \gls{qos} Flow Identification (Section~\ref{sec:background}.\ref{subsec:background-qfi}): \gls{hctns} adopts the \gls{ietf} \gls{5qi}-aware model, where the \glspl{5qi} values used in the \gls{3gpp} network are mapped to \gls{dscp} values in the transport network. As shown in Fig.~\ref{general-view}, the packet transmitted by the gNB to the UPF,  whether IPv4 or IPv6 (IPv4 in this case) carries the \gls{dscp} value corresponding to the \gls{5qi} of the traffic flow of a slice. The ingress \gls{pe} router uses this \gls{dscp} value to perform an additional mapping to a \gls{tn} \gls{qos} class. This mapping is reflected in the IPv6 header that encapsulates the IPv4 packet transmitted by the gNB. Flows with similar \gls{qos} requirements (i.e., \gls{5qi} values that ensure a comparable level of \gls{qos}) can be assigned to the same \gls{tn} \gls{qos} class.  
    
    \item Network Slice Realization (Section~\ref{sec:background}.\ref{subsec:background-realization}): \gls{hctns} considers the deployment of \glspl{l3vpn} based on IPv6 tunnels for: (1) the traffic isolation among slices, and (2) the transport of the \gls{tn} \gls{qos} class identifier, which is included in the \gls{dscp} field of the IPv6 packet header that is encapsulating the IPv4 packet. IPv6 tunnels are assumed to be implemented by using \gls{sr}. As shown in Fig.~\ref{general-view}, IPv6 packets exiting the ingress \gls{pe} router carry the \gls{dscp} identifying the \gls{tn} \gls{qos} class and a \gls{srh}, which is used to configure the path followed by packets in the underlying network. The implementation of IPv6 tunnels may also take into account capacity and planning/methods, something that falls outside the scope of this paper and that may be addressed in future steps of the investigation. Regarding coarse-grained resource control, our model is closely aligned with the model proposed by the \gls{ietf}, which is based on priority and \gls{drr} queues.     
    
\end{itemize}

\rev{Although the paper focuses on 5G transport networks, HCTNS has been designed based on the IETF framework, allowing the proposed network slicing model to be applied to any IP or MPLS network. In the case of MPLS networks, instead of using IPv6 tunnels and the DSCP field (which identifies the TN QoS Class for Coarse-grained Resource Control), MPLS tunnels (e.g., VPLS) and the MPLS Traffic Class field would be employed for the same purpose.}

A detailed description of the fine-grained resource control mechanism proposed is explained below.  

\subsection{A New Model for Fine-Grained Resource Control}
\label{subsec:proposal-finegraine}

\textit{Fine-grained resource control} is implemented in the ingress and egress \gls{pe} routers of the transport network, which handle, respectively,  various data plane functions in their input and output ports. As Fig.~\ref{ingress_operations} illustrates, the input port of a \gls{pe} is responsible for classifying the incoming packets according to the values of the VLAN tag and the \gls{dscp} header fields. This classification identifies the network slice and its associated \gls{qos} requirements.  Based on these identifiers, packets are enqueued into a \gls{htb} queuing system that implements our proposed three-level hierarchical ingress policing mechanism. Additionally, the identifiers are then used to map the packet to the corresponding \gls{tn} \gls{qos} class. As Fig.~\ref{ingress_operations} shows, the output port is responsible for sending the traffic over a IPv6 tunnel that carries its own \gls{dscp} mark. This marking is used by the coarse-grained resource control mechanism to identify the \gls{tn} \gls{qos} class and apply the appropiate \gls{phb} treatment. Packets are then enqueued into a queuing system based on priority and \gls{drr} queues, ensuring they are processed according to the \gls{qos} level defined for their \gls{tn} \gls{qos} class. 

\begin{figure}[t]
\centerline{\includegraphics[width=0.85\columnwidth]{figures/ingress_PE_operation_flow_v3.pdf}}
\caption{Input and output port processing at the \gls{pe} ingress node.}
\label{ingress_operations}
\end{figure}

The three-level hierarchical ingress policing mechanism proposed in this paper, implemented using the \gls{htb} rate-limiting algorithm, is illustrated in Fig.~\ref{htb_logical}.  The \gls{htb} allows organizing traffic into a hierarchy of classes (or nodes), with each class ensuring its bandwidth guarantees. As shown in Fig.~\ref{htb_logical}, only the leaf nodes of the \gls{htb} tree have an associated queue, where packets can wait to be transmitted to an output port of the \gls{pe} router. The tree-based hierarchical structure of the \gls{htb} allows classes under the same parent node to share bandwidth not consumed by the other classes. This is a key advantage of our proposal. The \gls{ietf} in~\cite{draft-ietf-teas-5g-ns-ip-mpls} employs a two-level hierarchy (slice and class) for its policing mechanism, which makes bandwidth sharing between slices and 5G \gls{qos} classes inefficient and without \gls{qos} guarantees. In the \gls{ietf} model, unused bandwidth can only be used by the best-effort class within the same slice. 

\begin{figure}[b]
\centerline{\includegraphics[width=0.5\textwidth]{figures/htb_logical.pdf}}
\caption{Three-level hierarchical ingress policing mechanism.}
\label{htb_logical}
\end{figure}

Fig.~\ref{htb_logical} shows the global policer at the root of the \gls{htb} tree. The global policer limits the total amount of traffic allowed to pass through the ingress \gls{pe} router, defined by the \gls{cir} parameter. In our proposal, packets exceeding the \gls{cir} limit are discarded, making \gls{cir} and \gls{pir} equivalent in this case. All traffic admitted to traverse the transport network will be treated as ``green" by the transit nodes, and will not be dropped, as we will explain later. The global policer is also important to handle the bursty traffic, since it controls that, on average, the total maximum capacity is not exceeded. 

\begin{figure*}[hbtp]
\vspace*{-0.1cm}
\centerline{\includegraphics[width=0.9\textwidth]{figures/htb_physical.pdf}}
\caption{\gls{htb} operation diagram.}
\label{htb_physical}
\end{figure*}

The second level of the \gls{htb} hierarchy is occupied by slice policers, which enforce rate limiting at the slice level. These policers are defined by two parameters: \gls{cir} and \gls{pir}, which, in this case, may not be equivalent. The \gls{cir} represents the guaranteed bitrate of the slice, while the \gls{pir} defines its maximum. The sum of the \gls{cir} for all the slices must not exceed the \gls{cir} of the global policer. Regarding bandwidth sharing, any slice can make use of the bandwidth not consumed by other slices. The maximum bandwidth a slice can borrow from the root node is equal to its \gls{pir}-\gls{cir}. Slices where the \gls{cir} is equal to the \gls{pir} are not allowed to borrow bandwidth, making this an interesting method for controlling bandwidth consumption.  

The third level of the \gls{htb} hierarchy is occupied by class policers, which are used by slices transporting various \gls{qos} flows with different \gls{qos} requirements (e.g., slice 1 and 2 in Fig.~\ref{htb_logical}). In network slices where all traffic flows receive the same \gls{qos} treatment, only the global and slice policers are applied. Class policers are defined by four parameters: \gls{cir}, \gls{pir}, \gls{cbs}, and \gls{pbs}. As explained later, the \gls{cbs} and the \gls{pbs} parameters play a key role in controlling the bursty traffic of the slices. On the other hand, the sum of the \gls{cir} values for all classes must not exceed the \gls{cir} of the corresponding slice policer. Besides, any class can make use of the bandwidth not consumed by other classes within the same slice (i.e., under the same parent node).

Our bandwidth sharing mechanism allows all slices and classes to borrow excess available bandwidth equally. However, \gls{hctns} also introduces two more advanced methods to specify how excess or unused bandwidth can be shared between slices and, within a slice, among different \gls{qos} classes. These methods define how bandwidth is distributed when several nodes under the same parent node in the \gls{htb} try to obtain bandwidth from that parent node at the same time. The methods are implemented by controlling the distribution of tokens that is used to assign bandwidth in the \gls{htb} as we will explain later.  

\begin{itemize}
    \item Weight/Quantum-based bandwidth sharing. The available bandwidth is shared among slices and \gls{qos} classes based on weights (relative) or quantums (fix amount of bytes) assigned to the slice and/or class policers. When defined at the slice level, the average bandwidth a slice $\theta$ can borrow from the global available bandwidth (assuming that all the slices have packets to transmit), is calculated as: $B_\theta = B_r \cdot \frac{w_\theta}{\sum_{i=1}^{N} w_i}$, where $N$ is the total number of slices, $B_r$ is the global available bandwidth, $w_\theta$ is the weight/quantum assigned to slice $\theta$ and $w_i$ the weight/quantum assigned to the \textit{i}-th slice. When defined at both the slice and the class levels, the average bandwidth a class $\phi$ within a slice $\theta$ can borrow from the global available bandwidth  (assuming that all classes within the same slice have packets to transmit) is calculated as: $B_{\theta, \phi} = B_r \cdot \frac{w_\theta}{\sum_{i=1}^{N} w_i} \cdot \frac{w_\phi}{\sum_{j=1}^{C} w_j}$, where C is the total number of classes within the slice $\theta$, $w_\phi$ the weigh/quantum assigned to slice $\phi$ and $w_j$ the weight/quantum assigned to the \textit{j}-th class within the slice $\theta$. 
    \item Priority-based bandwidth sharing. In this method, each slice and/or \gls{qos} class is assigned a priority value, with lower values indicating  higher priority. The available global bandwidth is shared based on these priority levels. When defined at the slice level, slices with higher priority are granted bandwidth first, either until the bandwidth is exhausted or the slices have reached their \gls{pir} limit. When defined at both the slice and class levels, the bandwidth each class within a slice can borrow from the global available bandwidth depends on both the priority of the slice and the priority of the class within that slice. 
\end{itemize}

These methods can be combined in various ways. For instance, the weight-based method may be applied at the slice level while the priority-based method may be applied at the class level. Another option is to use both methods at both levels, as they are not mutually exclusive. Nevertheless, regardless of the bandwidth sharing method employed, it is important to note that the \gls{cir} of each slice and \gls{qos} class is always guaranteed because we are sharing only the available excess bandwidth.


The use of a \gls{htb} on the ingress port of the \gls{pe} router does not impact packet delay, despite the typical behavior of \glspl{htb}. The packet scheduler of the \gls{htb} first dequeues all traffic under the \gls{cir} limit for each leaf node, and after that, traffic between \gls{cir} and \gls{pir}. This causes traffic between \gls{cir} and \gls{pir} to wait for transmission, potentially violating the service delay requirements. However, in our proposal, the \gls{htb} operates on the \gls{pe} ingress interface, where packets traverse the switching fabric at a very high speed. As a result, packet delay is negligible when processed by our \gls{htb}-based fine-grained resource control mechanism.

The control of traffic bursts, i.e., traffic received at an unlimited rate for a bounded time, is a key feature of our proposal. Each class policer is defined by two parameters that regulate traffic bursts for the \gls{qos} flows: the \gls{cbs} and the \gls{pbs}. The \gls{cbs} indicates the maximum number of bits that can be transmitted above the \gls{cir} during a burst period, while the \gls{pbs} indicates the maximum number of bits allowed to be transmitted above the \gls{pir} during the same period. To better illustrate how bandwidth sharing and traffic burst control works in our proposal, Fig.~\ref{htb_physical} presents the internal structure of the proposed three-level \gls{htb}. 

Fig.~\ref{htb_physical} shows that each policer is associated with two token buckets. The bucket on the right, hereinafter referred to as \textit{\gls{cir}-bucket} is filled with tokens at the \gls{cir} rate, ensuring a guaranteed bandwidth. The bucket on the left, hereinafter referred to as \textit{\gls{pir}-bucket}, is filled with tokens at the \gls{pir} rate, ensuring that traffic does not exceeds a maximum bandwidth. The size of each bucket is determined by the \gls{cbs} and \gls{pbs} parameters. Buckets cannot store more tokens than their capacity (\gls{cbs} or \gls{pbs}) allows, so the tokens received when the buckets are full are discarded. Having this in mind, an \gls{htb} works as follows. If a packet arrives in the queue but there are no tokens available in the \textit{\gls{pir}-bucket}, the maximum bandwidth has been reached and the packet must wait in the queue until new tokens become available. If the \textit{\gls{pir}-bucket} is not empty, the \textit{\gls{cir}-bucket} is checked. If there are tokens available in the \textit{\gls{cir}-bucket}, the packet is within the \gls{cir} limit, and can be transmitted to the switching fabric, consuming a token from both buckets. Tokens consumed in the buckets of a child node must also be deducted from the parent nodes. For example, when a packet consumes tokens from the buckets associated with a class policer, tokens are also consumed from the buckets associated with the slice and global policers. In case there are no tokens available in the class policer \textit{\gls{cir}-bucket}, the child node requests tokens to the parent (slice) node, which first checks its \textit{\gls{pir}-bucket}. If no tokens are available in the \textit{\gls{pir}-bucket} the slice's maximum bandwidth has been reached and the packet must wait in the queue. If the \textit{\gls{pir}-bucket} is not empty, and there are tokens available in the slice's \textit{\gls{cir}-bucket}, the child node can borrow bandwidth from the parent (slice) node and the packet can be transmitted to the switching fabric (tokens are consumed in the slice and in the global level).  If no tokens are found at the slice level (the slice \textit{\gls{cir}-bucket} is empty), the slice policer requests tokens to the global policer. In summary, if the child node and its ancestors have no tokens for the packet transmission, the packet waits in the queue. However, if tokens are available in the \gls{htb} tree, tokens are consumed at different levels to maintain bandwidth control at both the slice and the global levels. The \gls{htb} queues have a limited size, and when they are full, packets begin to be dropped. This behavior enables the enforcement of the policying function.

In the following, we discuss how the \gls{htb} can be configured to control traffic bursts. Traffic burst control is effectively disabled in a bucket when it is sized to store just the tokens needed to transmit a single packet. If the traffic burst control is set in the \textit{\gls{cir}-bucket}, but not in the \textit{\gls{pir}-bucket}, packets can be dequeued at most at the \gls{pir} rate. This is because packet transmissions require the simultaneous consumption of tokens from both the \gls{cir} and the \gls{pir} buckets, and the \textit{\gls{pir}-bucket} is only storing tokens for transmitting one packet. In contrast, if the traffic burst control is set in the \textit{\gls{pir}-bucket} but not in the \textit{\gls{cir}-bucket}, the child node may request additional tokens to its parent node, as Fig.~\ref{htb_physical} shows. However, this does not guarantee the transmission of the traffic burst either. Finally, if the traffic burst control is set in both the \gls{cir} and \gls{pir} buckets, packets can be dequeued immediately as long as there are tokens available in both buckets, regardless of the availability of tokens in the buckets of the parent nodes. This is the configuration we have selected to control traffic bursts in our proposal, with the \gls{pbs} and \gls{cbs} set to the same value. Using this configuration, the \gls{pe} router can accept traffic bursts during a period that depends on the size of the buckets. After that burst period, a new burst will only be accepted if packets arrive at the ingress \gls{pe} port at a lower rate than the \gls{cir} rate, so that the \textit{\gls{cir}-bucket} can be refilled, and at a lower rate than the \gls{pir} rate, so that the \textit{\gls{pir}-bucket} can also be refilled. 

Our approach consists on enabling traffic burst control exclusively in leaf nodes, which implies specifying values for \gls{cbs} and \gls{pbs} only in the buckets associated with classes. To be able to accept traffic bursts, defining traffic burst control in the class policers is mandatory. Otherwise, even if the buckets associated with the slice and the global policer have tokens, a packet from a \gls{qos} class received at a rate higher than the class policer \gls{pir} would not be dequeued until a new token is received in the buckets associated with the class policer. The values of \gls{cbs} and \gls{pbs} will control the burst size allowed per \gls{qos} class. When the burst size is smaller than the \gls{pbs}/\gls{cbs}, there are sufficient tokens to forward all packets in the burst to the switching fabric at the rate they are received. However, if the burst size exceeds the \gls{pbs}/\gls{cbs}, only part of the burst is forwarded at the rate packets are received. This forwarding ends when both buckets are depleted. The remainder of the burst will be enqueued in the \gls{htb} queue until it reaches its capacity and will be transmitted at the \gls{cir} rate defined by the class policer. Any additional packets beyond the queue’s limit will be discarded.

As shown in Fig.~\ref{htb_physical}, packet transmission is allowed if there are tokens in the class policer, regardless the availability of tokens in the parent nodes. However, when packet transmission is allowed, tokens are consumed from the policers at all levels. This may result, when sending traffic bursts, in the parent nodes not having enough tokens in their buckets. In such cases, the token counters in the \gls{htb} parent nodes can take negative values, which can be considered as ''tokens to be paid''. Nodes with ''negative'' tokens cannot share tokens with their child classes, until the counters return to positive values. During this period, traffic will only flow at the \gls{cir} rate of the leaf nodes. This ensures that while the global maximum rate of the parent node may be temporally exceeded, it is eventually compensated to maintain the average maximum rate of the parent node within the defined limit. 

\begin{figure}[b]
\centerline{\includegraphics[width=0.9\columnwidth]{figures/egress_v2.pdf}}
\caption{Input and output port processing at the \gls{pe} egress node.}
\label{egress_operation_flow}
\end{figure}

As Fig.~\ref{egress_operation_flow} shows, a fine-grained resource control is also implemented in the \gls{pe} egress routers. The input port of the \gls{pe} router terminates the IPv6 tunnel, forwarding packets to the output ports of the \gls{pe} based on their IP destination address. On the physical output ports, various logical interfaces are defined with the VLAN-tagged mode enabled to carry tags identifying the network slices in the \gls{3gpp} domain. The output port applies a fine-grained resource control using a hierarchical two-level \gls{drr} queuing system. We have adopted the same model proposed by the \gls{ietf} in~\cite{draft-ietf-teas-5g-ns-ip-mpls}, and shown in Fig.~\ref{egress_queue_scheme}. The first \gls{drr} level is used to differentiate the 5G \gls{qos} classes within the same network slice, while the second \gls{drr} level differentiates the slices. 

\begin{figure}[t]
\centerline{\includegraphics[width=0.8\columnwidth]{figures/egress_PE_queue_scheme.png}}
\caption{\rev{Hierarchical} \gls{drr} queuing system, as proposed by the \gls{ietf}.}
\label{egress_queue_scheme}
\end{figure}

The \gls{ietf} resource control mechanism can maintain the \gls{cir} of 5G slices and their associated traffic classes even in scenarios with network congestion. However, thanks to the global policer proposed in \gls{hctns}, if a slice sends traffic above its \gls{cir} it is mainly because there is unused bandwidth available from other slices. So, by applying \gls{hctns}, the 5G slices are allowed to consume more bandwidth without causing network congestion. 

Although the model has been described considering uplink communications, the concepts described here are also valid for downlink traffic. 

\subsection{Configuring the Coarse-Grained Resource Control}
\label{subsec:proposal-coarsegrained}  

The coarse-grained resource control is applied to the output ports of the \gls{pe} and P routers, as shown in Figs.~\ref{ingress_operations} and~\ref{P_operation_flow}. The output port of the \gls{pe} router first encapsulates packets in IPv6 tunnels based on \gls{sr}, adding a mark to the \gls{dscp} value in the IPv6 header that identifies the \gls{tn} class associated with the packet. A classifier uses this \gls{dscp} mark to enqueue the packets of each \gls{tn} class into the corresponding priority/\gls{drr} queue for the coarse-grained resource control. This allows traffic flows in the \gls{tn} to be treated according to their delay and bandwidth requirements. The output port of the P router works similarly but does not tunnel the packets, as P routers only forward traffic within the underlay network. Forwarding is done according to the \gls{srh} of the packet (referred to as \gls{srh} processing in Fig.~\ref{P_operation_flow}), which determines the next IPv6 hop in the underlay network. 

\begin{figure}[tb]
\centerline{\includegraphics[width=1\columnwidth]{figures/P_operation_flow_v2.pdf}}
\caption{Input and output port processing at the P transit node.}
\label{P_operation_flow}
\end{figure}

Fig.~\ref{transit_queues} shows the coarse-grained resource control (on the right) and its relationship with the fine-grained resource control (on the left). As an example, Fig.~\ref{transit_queues} shows the slice $S3$ as the highest priority slice. This slice has no associated classes, and therefore, does not have any child nodes in the \gls{htb} tree. Traffic from slice $S3$, marked with \gls{dscp} 6, enters the priority queue in the coarse-grained resource control. Slice $S1$ has three classes associated ($S1_{C1}$, $S1_{C2}$, and $S1_{C3})$, with traffic class $S1_{C1}$ marked with \gls{dscp} 3, $S1_{C2}$ marked with \gls{dscp} 2, and $S1_{C3}$ marked with \gls{dscp} 0. Similarly, slice $S2$ has two associated classes ($S2_{C1}$ and $S2_{C2}$), marked with \gls{dscp} 3 and 2 respectively. As shown in Fig.~\ref{transit_queues}, packets are associated to a queue in the \gls{drr} queuing system based on the \gls{dscp} value of the IP header, enabling the differentiation of traffic classes with different levels of \gls{qos}. Traffic from these classes must wait in the corresponding \gls{drr} queues until the priority queue is empty.

\begin{figure*}[bth]
\centerline{\includegraphics[width=2\columnwidth]{figures/transit-queues.png}}
\caption{Queuing scheme at transit links for data plane traffic, based on the \gls{ietf} proposal \cite{draft-cbs-teas-5qi-to-dscp-mapping}.}
\label{transit_queues}
\end{figure*} 


\section{Performance Evaluation}
\label{sec:eval}
\subsection{Experimental Platform}

To evaluate \gls{hctns}, we have developed a platform that combines physical and virtual network equipment. The platform's software has been published in open access\footnote{\label{repo}https://github.com/giros-dit/net-slicing-emulator} for community use. The top of Fig.~\ref{platform} shows a conceptual view of the network scenario, consisting of a gNB and a \gls{upf} connected via a transport network. Terminals connect to standard 5G slices such as \gls{embb} or \gls{urllc}, as well as to more innovative slices, such as a \gls{tod} slice. The 5G network is interconnected via a simplified version of the transport network, consisting of three nodes: an ingress, a transit, and an egress \gls{pe} router. As shown in the middle part of Fig.~\ref{platform}, this network scenario has been deployed on two physical PCs connected via four communication links. 

\begin{figure}[t]
\centerline{\includegraphics[width=0.95\columnwidth]{figures/lab.pdf}}
\caption{Experimental platform for transport network slicing.}
\label{platform}
\end{figure}

PC1 is equipped with an Intel Core i7-8700 processor with 6 cores (clock speed 3.2\,GHz), 32\,GB of RAM, and a 512\,GB SSD. As Fig.~\ref{platform} shows, PC1 connects to PC2 by using a PCI Express Gigabit Ethernet network card with a single port and three USB-Ethernet adapters. PC2 is equipped with an Intel Core i7-4790 processor with 4 cores (clock speed 3.6\,GHz), 32\,GB of RAM, and a 256\,GB SSD. In this case, PC2 connects to PC1 using PCI Express Gigabit Ethernet network card with four Ethernet Ports. Both PCs run the Ubuntu 22.04 Linux distribution as their operating system. The network adapters can be configured flexibly to emulate links with capacities of 10\,Mbps, 100\,Mbps or 1\,Gbps, using Ethtool\footnote{https://linux.die.net/man/8/ethtool}. This approach was chosen instead of deploying the scenario in a fully virtualized environment, because setting virtual links speeds is more challenging and less realistic. To facilitate experiments, the communication links capacities in the testbed are scaled down compared to those in a real transport network~\cite{UC3M2018}. Additionally, to conduct  saturation tests on the input port of the \gls{pe} router, the link between PE1 and P has been set to 100\,Mbps, which is lower capacity than the rest of the links in the platform.

Within each PC, a virtual scenario has been deployed using the network virtualization tool \gls{vnx}\footnote{https://web.dit.upm.es/vnxwiki/index.php/Main\_Page}. The \gls{vnx} scenario on PC1, see Fig.~\ref{platform}, includes the P router and several hosts and servers, defined as LinuX Containers (LXC) running Ubuntu 22.04. These hosts and servers act as traffic sources and sinks for the traffic in our experiments. The \gls{vnx} scenario on PC1 also includes two Linux bridges emulating a gNB and a \gls{upf} 5G router. On the other hand, the \gls{vnx} scenario on PC2 includes the ingress and egress \gls{pe} routers. Using this setup, synthetic traffic was generated with the Iperf\footnote{https://iperf.fr/} tool to measure the \gls{qos} level of the traffic flows when the global available bandwidth is shared among different slices. 

Our experimental campaign focuses on analyzing the \gls{qos} levels achieved by traffic flows of these slices in the uplink direction, i.e., traffic transmitted from the gNB to the \gls{upf} router, although a similar analysis for downlink traffic would yield comparable results. This focus on the uplink is due to a particular interest in understanding how a \gls{tod} slice may be defined, as initially analyzed in~\cite{wimob}. The \gls{tod} slice is more bandwidth demanding in the uplink than in the downlink, as vehicles transmit video signals and telemetry data to a remote operations center. 

\begin{table}[t]
\vspace*{0.1cm}
\centering
\begin{tabular}{|c|c|c|c|c|}
\hline
Slice                 & Traffic Class & 5QI                                                   & DSCP & TN QoS Class \\ \hline
URLLC                 & URLLC         & 82                                                    & 46   & A            \\ \hline
\multirow{2}{*}{ToD}  & Video         & 130*                                                  & 38*  & B            \\ \cline{2-5} 
                      & Telemetry     & 131*                                                  & 28*  & C            \\ \hline
\multirow{2}{*}{eMBB} & VC            & 2                                                     & 28   & C            \\ \cline{2-5} 
                      & BE            & 9 (default)                                           & 0    & D            \\ \hline
\end{tabular}
\vspace*{0.3cm}
\caption{Slices and traffic classes in the experimental campaign.}
\label{tab:slices}
\end{table}


\begin{figure*}[t]
\vspace{-0.1cm}
\centering
\includegraphics[width=1.8\columnwidth]{figures/htb-lab2.pdf}
\vspace*{0.5em}
\caption{Fine-grained resource control implementation in PE1 and PE2.}
\label{htb-lab}
\end{figure*}

At the logical level, we have three network slices in our experiments: the mentioned \gls{tod} slice, and an \gls{embb} and a \gls{urllc} slice. We define two traffic classes in the \gls{tod} slice: video and telemetry classes. The \gls{embb} slice has two associated traffic classes: Best Effort (BE) and Video Conferencing (VC). In contrast, the \gls{urllc} slice has no associated traffic classes. Table~\ref{tab:slices} summarizes this information and indicates a possible mapping between the \glspl{5qi} values that specify the \gls{qos} characteristics of the 5G network slices, the \gls{dscp} values, and their corresponding \gls{tn} \gls{qos} classes. This mapping is relevant for defining how the data traffic will be treated by the fine-and-coarse grained resource control mechanisms in our experiments. \glspl{5qi} values for the \gls{embb} and \gls{urllc} slices are obtained from~\cite{3GPP2023_1}, while their associated \gls{dscp} values and \gls{tn} \gls{qos} classes are derived from~\cite{draft-cbs-teas-5qi-to-dscp-mapping}. Since the \gls{tod} slice is not defined in the above documents, we propose new \glspl{5qi} values for the \gls{tod} video and telemetry traffic, along with a mapping of these \glspl{5qi} to \glspl{dscp} and \gls{tn} \gls{qos} classes.

Both the fine-grained and the coarse-grained mechanisms have been implemented using the Linux \gls{tc} tool\footnote{https://man7.org/linux/man-pages/man8/tc.8.html}. In the following, we provide implementation details of the \gls{hctns} model. For more details, please refer to our open-access code repository\footref{repo}. 

As described in Section~\ref{sec:proposal}, \gls{hctns} uses a three-level hierarchical \gls{htb} rate-limiting algorithm for fine-grained resource control at the ingress \gls{pe}1. In our platform, this \gls{htb} is implemented with the \gls{tc} \textit{htb qdisc} queuing discipline, which has been used to hierarchically organize the slices and traffic classes, as shown in Fig.~\ref{htb-lab}. \rev{\gls{tc} \textit{htb qdisc} was originally designed for shaping mechanisms applied to output interfaces,  and it cannot be directly applied to input interfaces, which is a limitation of the TC tool. To implement our proposed three-level hierarchical ingress policing mechanism, we have defined an Intermediate Functional Block (IFB) logical interface in the \gls{pe} router, enabling the redirection of incoming traffic to this logical interface and treating it as an output port. This logical interface is not the actual output port of the PE1 router, but an intermediary interface for handling ingress traffic according to the defined policing mechanism. Using TC, we have configured the different policers (class, slice, and global) for the three types of experiments conducted, as specified in Tables~\ref{tab:expa-params}, \ref{tab:expb-params} and \ref{tab:expc-params}, which define the CIR, PIR, CBS, and PBS values that determine the behavior of \gls{hctns} at the ingress interface of PE1.}


The token-sharing methods described in Section~\ref{sec:proposal}.\ref{subsec:proposal-finegraine} (Weight/Quantum-based and Priority-based) are configured in Linux \gls{htb} by using the \textit{quantum} and \textit{prio} parameters. \rev{However, the Linux \gls{htb} implementation has two differences with the proposed \gls{htb} model described in Subsection \ref{sec:proposal}.\ref{subsec:proposal-finegraine}. On the one hand, \gls{tc} allows those parameters to be defined only at the leaf nodes of the \gls{htb} tree. The description of the experiments will indicate how these parameters have been configured in our platform. On the other hand, the Linux \gls{htb} implementation does not distribute the excess bandwidth proportionally among the active classes. When quantum values are used to distribute the excess bandwidth, traffic classes that consume bandwidth 
not used by other classes within the same slice receive less proportion of the excess bandwidth.}

To ensure that the traffic generated by the hosts in our platform (see Fig.~\ref{platform}) is queued according to the corresponding traffic class in the \gls{htb}, \gls{tc} \textit{traffic filters} have been used. These filters emulate, to some extent, the classifiers of the transport network routers. In the current version of the platform, traffic classification is based on the source IP address of the packets. Similarly, we have defined \textit{traffic filters} to ensure that traffic exiting the logical IFB interface is correctly enqueued in the priority and \gls{drr} queues of the coarse-grained resource control associated with the output port of the \gls{pe} router.

The \gls{pe} egress router implementation differs slightly from \gls{pe}1, as the fine-grained resource control for outgoing traffic is based on a hierarchical \gls{drr} queuing discipline following the \gls{ietf} model (Section~\ref{sec:proposal}.\ref{subsec:proposal-finegraine}). For this, we use the \textit{\gls{drr} qdisc}, with three sub-classes, one per slice. The \gls{embb} and \gls{tod} slices  have a second level in the hierarchy for their corresponding traffic classes, as shown in Fig.~\ref{htb-lab} (on the right).

Coarse-grained resource control is implemented using the \gls{tc} \textit{PRIO qdisc} queuing discipline. With \gls{tc} \textit{PRIO qdisc} we have defined two classes with two priority levels, assigning higher priority to the \gls{urllc} slice over the \gls{embb} and \gls{tod} slices. As a result, \gls{embb} and \gls{tod} packets must wait for transmission if \gls{urllc} packets are present in the corresponding queue. The \gls{urllc} traffic is handled in the transport network using the \gls{tn} \gls{qos} class A. The lowest priority class is managed using a \textit{\gls{drr} qdisc}, with three sub-classes corresponding to \gls{tn} \gls{qos} classes B, C, and D. Table~\ref{tab:slices} shows the \gls{tn} \gls{qos} classes associated with each traffic class defined in our experiments. Each \gls{drr} queue is configured with a \textit{quantum}, which specifies the amount of bytes that can be dequeued from a class before the packet scheduler moves to the next class. As it will be described later, we have varied the values of the quantums associated with the \gls{drr} queues to analyze its impact on the network performance. 

Packets transmitted from the output port of the \gls{pe}1 router are sent to the P router at 100\,Mbps (see Fig.~\ref{platform}), the maximum global bandwidth allowed by the telco-operator in our emulated network. However, the link between the P router with the \gls{pe}2 router has a speed of 1\,Gbps, resulting in no queuing delay at either the P or \gls{pe}2 routers. 

\subsection{Test}
    \label{sec:test}

\rev{For validation and insight into the behavior of our proposed network slicing model, we conducted an experimental campaign comparing \gls{hctns} with the slicing models defined by the \gls{ietf} and the proposal in Lin \textit{et al.}~\cite{Lin2021}. Based on the related work (Section~\ref{sec:soa}.\ref{subsec:soa-literature}), we selected the model proposed by Lin \textit{et al.} because similar approaches are commonly used in the literature to implement network slices in transport networks. The campaign is composed of three experiments: \textit{Experiment A}, \textit{Experiment B} and \textit{Experiment C}}. 

\rev{\textit{Experiment A} compares the performance of the \gls{ietf} model~\cite{rfc9543, draft-ietf-teas-5g-ns-ip-mpls}, the state-of-the-art model \cite{Lin2021}, and \gls{hctns}} by analyzing the evolution of latency, bandwidth, packet losses, and number of packets waiting in the queuing system associated with the output port of the \gls{pe}1 router. \rev{This is done while generating UDP traffic at a constant bitrate of 100 Mbps for all defined traffic classes.} In this experiment, we also evaluate a combination of the proposed quantum and priority based bandwidth sharing mechanisms described in Section~\ref{sec:proposal}.\ref{subsec:proposal-finegraine}. Traffic burst control is disabled. 

\textit{Experiment B} also compares \gls{hctns} and the \gls{ietf} network slicing models but with a focus on analyzing the impact of coarse-grained resource control configuration on the bandwidth consumed by the telemetry and video traffic classes of the \gls{tod} slice. As in the previous experiment, traffic burst control is disabled and UDP traffic has been generated at a constant bitrate \rev{(100\,Mbps)}. \rev{In this experiment, the model in \cite{Lin2021} is excluded from the analysis because it cannot incorporate different coarse-grained resource control configurations.} 

Finally, \textit{Experiment C} demonstrates the operation of the traffic burst control mechanism in \gls{hctns}, highlighting the impact of traffic bursts on latency experienced by packets from different traffic classes and slices. Under these conditions, we analyze how the configuration of coarse-grained resource control affects the latency experienced by packets of the different traffic classes. \rev{In~\cite{Lin2021}, traffic bursts are not considered. However, the policer the~\cite{Lin2021} model uses to split \textit{``green"} and \textit{``yellow"} traffic has parameters to accept traffic bursts. Therefore, we have configured them to compare the performance in the presence of bursts with \gls{hctns}}. 

\rev{The packet size was set to 1538 bytes (including link and physical headers) in all experiments. Since the analyzed models, including our proposal, manage rate in byte/s (not in packet/s), packet size, apart from the overhead, should not affect the behavior of the models or their performance, although we plan to carry out more experiments in our future work to further corroborate this.}

\input{files/table-expa}

\rev{\subsubsection{Experiment A: Comparison between IETF, \cite{Lin2021} and HCTNS models}}

For this experiment, the fine-grained and coarse-grained resource control mechanisms of the \gls{pe}1 router in our emulated \gls{tn} scenario were configured using the parameters and values listed in Table~\ref{tab:expa-params}. This table shows some differences between \rev{the \cite{Lin2021} model,} the \gls{ietf} model and our proposed model. The class policers defined by the \gls{ietf} only allow the configuration of the \gls{cir}. For slices that do not have best-effort traffic classes (such as the \gls{tod} slice), the \gls{pir} setting in slice policers is not applicable, as traffic classes can only transmit at their \gls{cir}. The \gls{ietf} model does not define a global policer, which is also reflected in Table~\ref{tab:expa-params}. \rev{Since the model in \cite{Lin2021} considers only slices and not different \gls{qos} flows within them, we conducted experiments by treating each flow as a separate slice, where a \gls{cir} and \gls{pir} can be defined for each slice. Moreover,  \cite{Lin2021} does not implement a hierarchical policing at the \gls{tn} ingress, as \gls{ietf} and \gls{hctns} do. Instead, it uses a single level mechanism based on a \gls{trtcm} per slice. Furthermore, \cite{Lin2021} implements a queuing system at the output ports of the \gls{tn} routers composed of two priority queues. In contrast, \gls{hctns} and the \gls{ietf} model use a queuing system composed of a priority queue and three \gls{drr} queues, each corresponding to a different \gls{tn} \gls{qos} class. Consequently, we assume that the highest priority queue (HPQ) of \cite{Lin2021} corresponds to \gls{tn} \gls{qos} class A, and the lowest priority queue (LPQ) to \gls{tn} \gls{qos} class B.}

This experiment lasts 100\,s and is divided into six time intervals. In each interval, different UDP traffic flows of 100\,Mbps, representing different traffic classes, are transmitted over the transport network. This configuration has allowed us to evaluate the performance of our solution under diverse conditions. The traffic flows active in each interval are as follows:
 
\begin{itemize}
    \item Interval 1 (from \( t = 0 \, \text{s} \) to \( t = 20 \, \text{s} \)): BE traffic.
    \item Interval 2 (from \( t = 20 \, \text{s} \) to \( t = 40 \, \text{s} \)): BE, video, and telemetry traffic.
    \item Interval 3 (from \( t = 40 \, \text{s} \) to \( t = 60 \, \text{s} \)): All traffic classes are active.
    \item Interval 4 (from \( t = 60 \, \text{s} \) to \( t = 70 \, \text{s} \)): All traffic classes except \gls{tod} video are active
    \item Interval 5 (from \( t = 70 \, \text{s} \) to \( t = 80 \, \text{s} \)): BE, telemetry, and \gls{urllc} traffic.
    \item Interval 6 (from \( t = 80 \, \text{s} \) to the end of the experiment): BE traffic, as in the initial state.
\end{itemize}

\afterpage{
%% PRIMER EXPERIMENTO
\begin{figure*}[h!]
    \centering
    % LEYENDA
        \vspace{-2em}
        \includegraphics[width=\textwidth, trim=50 10 50 10, clip]{figures/results/ExperimentA/legend_5g.pdf}
    % PRIMERA FILA
    \begin{minipage}[t]{0.3\textwidth}
        \centering
        \input{figures/results/ExperimentA/IETF/bandwidth_behaviour}
        \hspace*{3.35em}
        \centering \small (a) IETF: Bandwidth Evolution.
    \end{minipage}
    \vspace*{0.25em}
    \hfill
    \begin{minipage}[t]{0.3\textwidth}
        \centering
        \input{figures/results/ExperimentA/SoA/bandwidth_behaviour}
        \hspace*{3.5em}
        \centering \small (b) \cite{Lin2021}: Bandwidth Evolution.
    \end{minipage}
    \vspace*{0.25em}
    \hfill
    \begin{minipage}[t]{0.3\textwidth}
        \centering
        \input{figures/results/ExperimentA/Proposed/bandwidth_behaviour}
        \hspace*{3.35em}
        \centering \small (c) \gls{hctns}: Bandwidth Evolution.
    \end{minipage}
    \vspace*{0.25em}

    % SEGUNDA FILA
    \begin{minipage}[t]{0.3\textwidth}
        \centering
        \input{figures/results/ExperimentA/IETF/maximum_latency_behaviour}
        \hspace*{3.35em}
        \centering \small (d) IETF: Latency Evolution.
    \end{minipage}
    \vspace*{0.25em}
    \hfill
    \begin{minipage}[t]{0.3\textwidth}
        \centering
        \input{figures/results/ExperimentA/SoA/maximum_latency_behaviour}
        \hspace*{5em}
        \centering \small (e) \cite{Lin2021}: Latency Evolution.
    \end{minipage}
    \vspace*{0.25em}
    \hfill
    \begin{minipage}[t]{0.3\textwidth} % Cuarta subfigura
        \centering
        \input{figures/results/ExperimentA/Proposed/maximum_latency_behaviour}
        \hspace*{2.75em}
        \centering \small (f) \gls{hctns}: Latency Evolution.
    \end{minipage}
    \vspace*{0.25em}

    % SEGUNDA LEYENDA
    \vspace{0.5em} 
    \includegraphics[width=0.8\textwidth]{figures/results/ExperimentA/legend_tn.pdf}
    \vspace{0.3em}
    
    % TERCERA FILA
    \begin{minipage}[t]{0.3\textwidth}
        \centering
        \input{figures/results/ExperimentA/IETF/packet_loss}
        \hspace*{0.5em}   
        \centering \small (g) IETF: Packet Loss in output PE port.
    \end{minipage}
    \vspace*{0.25cm}
    \hfill
    \begin{minipage}[t]{0.3\textwidth}
        \centering
        \input{figures/results/ExperimentA/SoA/packet_loss}
        \hspace*{1.75em}
        \centering \small (h) \cite{Lin2021}: Packet Loss in output PE port.
    \end{minipage}
    \vspace*{0.25cm}
    \hfill
    \begin{minipage}[t]{0.3\textwidth} % Cuarta subfigura
        \centering
        \input{figures/results/ExperimentA/Proposed/packet_loss}
        \hspace*{0.75em}
        \centering \small (i) \gls{hctns}: Packet Loss in output PE port.
    \end{minipage}
    \vspace*{0.25cm}
    % CUARTA FILA
    \begin{minipage}[t]{0.3\textwidth}
        \centering
        \input{figures/results/ExperimentA/IETF/queue_size}
        \hspace*{0.4em}   
        \centering \small (j) IETF: Packets Queued in output PE port.
    \end{minipage}
    \hfill
    \begin{minipage}[t]{0.3\textwidth}
        \centering
        \input{figures/results/ExperimentA/SoA/queue_size}
        \hspace*{0.5em}
        \centering \small (k) \cite{Lin2021}: Packets Queued in output PE port.
    \end{minipage}
    \hfill
    \begin{minipage}[t]{0.3\textwidth} % Cuarta subfigura
        \centering
        \input{figures/results/ExperimentA/Proposed/queue_size}
        \hspace*{-0.2cm}
        \centering \small (l) \gls{hctns}: Packets Queued in output PE port.
    \end{minipage}
    \caption{Experiment A: \gls{ietf} and \cite{Lin2021} network slicing models versus \gls{hctns}.}
    \label{fig:expa-ietf-proposal}
\end{figure*}
}

Fig.~\ref{fig:expa-ietf-proposal} presents the results obtained with the \gls{ietf} network slicing model alongside the results achieved with~\rev{the \cite{Lin2021} model and} \gls{hctns}. As discussed below, significant differences in the \gls{qos} provided to the traffic flows are observed between the three approaches.  

In \textit{Interval 1}, only BE traffic is active, fully utilizing the available bandwidth (100\,Mbps of the link between the PE1 and the P routers). All traffic is admitted by the ingress policers in all the models (Figures~\ref{fig:expa-ietf-proposal}a, \ref{fig:expa-ietf-proposal}b and \ref{fig:expa-ietf-proposal}c). Since the accepted BE traffic does not exceed the link capacity, packet delay remains low (Figures~\ref{fig:expa-ietf-proposal}d, \ref{fig:expa-ietf-proposal}e and \ref{fig:expa-ietf-proposal}f), and no packets are lost (Figures~\ref{fig:expa-ietf-proposal}g, \ref{fig:expa-ietf-proposal}h  and \ref{fig:expa-ietf-proposal}i) or enqueued (Figures~\ref{fig:expa-ietf-proposal}j, \ref{fig:expa-ietf-proposal}k and \ref{fig:expa-ietf-proposal}l) in the output port of \gls{pe}1. For this interval, all models show the same behavior.

In \textit{Interval 2}, BE traffic from the \gls{embb} slice, along with video and telemetry traffic from the \gls{tod} slice simultaneously enter the transport network.
\rev{The \gls{ietf} and \gls{hctns} models, by utilizing hierarchical policers, not only ensure the configured \gls{cir} for each traffic flow but also guarantee it at the slice level. Consequently, even though BE traffic does not have explicit bandwidth guarantees, it belongs to the \gls{embb} slice, which has a \gls{cir} of 52.8\,Mbps. If no other traffic in the slice is using that bandwidth, BE traffic can take advantage of the slice's guaranteed bandwidth.
In the \gls{ietf} model, traffic classes with bandwidth guarantees cannot exceed their assigned \gls{cir}. As a result, video traffic is limited to 32\,Mbps, aligning with the \gls{cir} values defined in Table~\ref{tab:expa-params}. Similarly, telemetry traffic is restricted to 4 Mbps. However, the 100\,Mbps of BE traffic are accepted by the policer, therefore, the remaining bandwidth is fully utilized by BE traffic (achieving 64 Mbps of the bandwidth in the link between the PE1 and the P routers). However, in the \gls{hctns} model, all the active flows can consume part of the excess bandwidth exceeding their respective \glspl{cir}. \rev{The \gls{htb} Linux tool implementation, based on a \gls{drr} scheduler that we configured with equal quantum values for all leaf nodes, assigns the excess bandwidth, in this case, as follows: the telemetry and video classes receive an additional 5.6 Mbps, while the BE traffic, which is set without a \gls{cir}, consume the 52.8 Mbps allocated to the \gls{embb} slice.} On the other hand, in the~\cite{Lin2021} model, BE traffic is treated as an independent slice without any bandwidth guarantees. Consequently, in the \cite{Lin2021} model, traffic flows can secure their \gls{cir} bandwidth (the policer marks this traffic as ``green") and other traffic (BE and ``yellow" traffic, i.e., the traffic above the \gls{cir} and under the \gls{pir} of other flows) compete for the remaining available bandwidth, achieving a total of 54\,Mbps for video, 26\,Mbps for telemetry, and 20\,Mbps for BE traffic.}

The coarse-grained resource control plays a crucial role in achieving these results. Packet loss in the \gls{ietf} model occurs when traffic admitted by the ingress policer attempts to access a \gls{drr} queue that is already full, as shown in Fig.~\ref{fig:expa-ietf-proposal}j, in the egress port of \gls{pe}1. \rev{The same occurs with the \cite{Lin2021} model, where BE and ``yellow" packets wait in the lower priority queue, as shown in Fig.~\ref{fig:expa-ietf-proposal}k}. In contrast, the operation of the global policer in \gls{hctns} prevents packets from waiting in the output port queues of the \gls{pe}1 (Fig.~\ref{fig:expa-ietf-proposal}l). This is achieved because our fine-grained resource control mechanism only accepts traffic that can be processed within the transport network with \gls{qos} guarantees.

 As shown in Table~\ref{tab:expa-params}, the same quantums values were configured for the \gls{drr} queues of the \gls{ietf} and \gls{hctns} models, corresponding to \gls{tn} classes B, C, and D. This configuration does not apply to \gls{tn} class A, as this traffic class utilizes the priority queue within the coarse-grained resource control queuing system, \rev{neither to the queues implemented in the~\cite{Lin2021} model, since~\cite{Lin2021} uses two priority queues.} For the quantum values used in this experiment, and without traffic of \gls{tn} \gls{qos} class A, the packet scheduler can serve packets from the \gls{drr} queues at an average transmission rate of 33.3\,Mbps for each of the \gls{tn} \gls{qos} classes B, C and D, when there are packets of all these \gls{tn} classes waiting in the queues to be transmitted.  

During \textit{Interval 2} of the Experiment A, the \gls{ietf} \gls{drr} queue corresponding to the \gls{tn} \gls{qos} class B exclusively receives video traffic from the \gls{tod} slice at a rate of 32\,Mbps, which explains why packets from this flow are neither dropped nor enqueued (Figs.~\ref{fig:expa-ietf-proposal}g and~\ref{fig:expa-ietf-proposal}j). Similarly, the \gls{drr} queue assigned to \gls{tn} \gls{qos} class C receives telemetry traffic at a rate of 4\,Mbps, a rate that is also below the threshold of 33.3\,Mbps. The problem arises with BE traffic, which is enqueued in the \gls{drr} for the \gls{tn} class D. In this case, BE traffic is received at a rate of 100\,Mbps. As previously mentioned, when all \gls{drr} queues contain packets waiting for transmission, the packet scheduler can only serve packets at an average rate of 33.3\,Mbps per queue. However, since the video and telemetry rates remain below their thresholds, the BE traffic can utilize the remaining available bandwidth, achieving a transmission rate of 64\,Mbps. As BE traffic is admitted by the \gls{ietf} ingress policer at a rate of 100\,Mbps, which exceeds the available 64\,Mbps, packet losses occur, and some packets experience queuing delays. \rev{On the other hand, the \cite{Lin2021} model forwards all the ``green" traffic (i.e., traffic under \gls{cir}) to the highest priority queue. Since the sum of the \gls{cir} values of the current traffic, 34\,Mbps in this interval, is lower than the link capacity, packets are not enqueued nor dropped. However, this model forwards all ``yellow'' traffic (i.e., between \gls{cir} and \gls{pir} rate) and the BE traffic to the lowest priority queue. As a consequence, more traffic arrives at the lowest priority queue than it can handle, leading to queue filling and packet losses, as shown in Figs.~\ref{fig:expa-ietf-proposal}k and ~\ref{fig:expa-ietf-proposal}h}. In contrast to both the IETF and the~\cite{Lin2021} models, \gls{hctns} includes a global policer that limits the total traffic entering the \gls{tn}. This mechanism ensures that traffic does not exceed the global available bandwidth at the output of the \gls{pe}1 router. As a result, packets are not enqueued nor dropped, as illustrated in Figs.~\ref{fig:expa-ietf-proposal}l and ~\ref{fig:expa-ietf-proposal}i.

Queuing delays determine the packet delay measurements shown in Figs.~\ref{fig:expa-ietf-proposal}d, \ref{fig:expa-ietf-proposal}e and \ref{fig:expa-ietf-proposal}f. During \textit{Interval 2}, in the \gls{ietf} model, packets from the \gls{tod} slice are not significantly affected by queuing delays, because traffic is generated at rates below the threshold of 33.3\,Mbps \rev{and there is not other traffic in their associated \gls{tn} \gls{qos} classes}. However, BE traffic is accumulated (also discarded) in the \gls{drr} queue of the \gls{tn} class D. \rev{In the \cite{Lin2021} model, since all flows have packets marked as ``yellow'' by the ingress policer, the maximum latency experienced by a flow is determined by the waiting time of the lowest priority queue, leading to similar latencies for all flows as BE traffic, around 195\,ms.} In contrast, in our proposed model, packet delay is influenced by the waiting time in the queue associated with the input port. This waiting time depends on the token arrival rate defined for each traffic class, i.e., the speed at which tokens are received in the \textit{\gls{cir}} and \textit{\gls{pir}} buckets defined in Section~\ref{sec:proposal}.\ref{subsec:proposal-finegraine}. As soon as there are tokens available in these buckets, the packet waiting in the queue of the \gls{htb} can be transmitted to the switching fabric of the \gls{pe} router. 

\textit{Interval 3} is particularly insightful for comparing our proposal with the other models. During this time interval, all traffic classes from the defined slices are transmitting traffic to the transport network. Unlike \textit{Interval 2}, for the \gls{ietf} and \gls{hctns} models, the Video Conferencing (VC) and the BE traffic classes are now sharing the network resources allocated to the \gls{embb} slice. However, VC traffic requires \gls{qos} guarantees, unlike the BE traffic class. The \gls{ietf} policer allows VC traffic to be transmitted at its \gls{cir} (52.8\,Mbps as shown in Table.~\ref{tab:expa-params}), leaving  47.2\,Mbps of remaining bandwidth of the slice available for BE traffic. 

During this interval, traffic from the \gls{urllc} slice is also generated at 100\,Mbps. Although the \gls{cir} of the \gls{urllc} slice policer is set to 1.2\,Mbps (Table~\ref{tab:expa-params}), Fig.~\ref{fig:expa-ietf-proposal}a reveals that, in the \gls{ietf} model, more bandwidth than this limit is being processed in the transport network (around 25\,Mbps). This occurs because traffic exceeding the \gls{cir} but under the \gls{pir} undergoes a process of ''de-prioritization'', according to the \gls{ietf} terminology. In practical terms, these packets will be treated as if they belonged to the BE traffic class, entering the \gls{drr} queue associated with the \gls{tn} \gls{qos} class D. This severely impacts the \gls{urllc} traffic, as shown in Fig.~\ref{fig:expa-ietf-proposal}d. Since a portion of the \gls{urllc} traffic is being enqueued in the \gls{drr} queue of the \gls{tn} \gls{qos} class D, the maximum packet delay for this traffic class rises to levels (more than 350\,ms) that make the \gls{urllc} slice unusable. Fig.~\ref{fig:expa-ietf-proposal}d also illustrates that BE traffic experiences similar delays, as it is enqueued in the same queue. Another problem arising from this situation is that, as more packets access the same \gls{drr} queue, the number  of discarded packets increases. From $t=40$ to $t=60$ there is a significant increase in the number of packets lost.

The queue associated with \gls{tn} \gls{qos} class C, which is assigned to the VC and telemetry traffic classes, also overflows during this interval in the \gls{ietf} model. This is explained because, when all the \gls{drr} queues contain packets waiting for transmission and there is also \gls{urllc} traffic (\gls{tn} class A), the packet scheduler delivers $\frac{100-1.2}{3}$\,Mbps$=32.9$\,Mbps per \gls{tn} \gls{qos} class using the \gls{drr} queues. Fig.~\ref{fig:expa-ietf-proposal}g shows that packets from the \gls{tn} \gls{qos} class C are dropped in the corresponding \gls{drr} queue, which is further confirmed by Fig.~\ref{fig:expa-ietf-proposal}j, where the queue is shown to be full. The same happens for \gls{tn} \gls{qos} class D. As a consequence, the guaranteed bandwidth for the \gls{embb} and \gls{tod} slices, which is 52.8\,Mbps and 36\,Mbps respectively (see Table \ref{tab:expa-params}), is not respected and the agreed bandwidth specified in the \glspl{sla} is violated.

Regarding the packet delay experienced during this time interval in the \gls{ietf} model, Fig.~\ref{fig:expa-ietf-proposal}d shows worse results compared to \textit{Interval 2}. This is because more traffic is sharing the \gls{drr} queues associated with the \gls{tn} \gls{qos} classes than in \textit{Interval 2}. The \gls{tn} \gls{qos} class A only receives \gls{urllc} traffic, but as explained earlier, a portion of this traffic is treated as best effort, leading to unacceptable delay levels. \gls{tn} \gls{qos} class B receives only video generated by the \gls{tod} slice. Since this video is generated at a rate of 32\,Mbps, below the 32.9\,Mbps threshold, the packet delay for this type of traffic remains very low during this interval. However, both \gls{tn} \gls{qos} classes C and D receive traffic at an aggregated rate that exceeds the 32.9\,Mbps limit, resulting in increased delays for the corresponding traffic classes. 

\rev{During this interval, the~\cite{Lin2021} model improves the bandwidth performance of the \gls{ietf} model but not its latency performance. As shown in Fig.~\ref{fig:expa-ietf-proposal}b, the model processes more bandwidth than the configured \gls{cir} values (Table~\ref{tab:expa-params}) for all flows. However, similar to the \gls{ietf} model, traffic exceeding the \gls{cir} but below the \gls{pir} undergoes a process of "de-prioritization," where packets are marked as ``yellow" and directed to the lowest-priority queue, negatively impacting their latencies. This occurs because these packets from all flows are enqueued in the same low priority queue.}

\rev{Since ``green" packets of all the flows are forwarded to the highest priority queue, this queue utilizes 90\,Mbps (i.e., the sum of the \gls{cir} values for each traffic class), leaving only 10\,Mbps for the lowest-priority queue. Additionally, due to the ingress policing mechanism, all traffic between the \gls{cir} and \gls{pir} is accepted and marked as ``yellow," to be discarded later if necessary. Consequently, the lowest priority queue receives traffic at the sum of the \gls{pir}-\gls{cir} values for \gls{urllc}, video, telemetry, and VC traffic, as well as best-effort (BE) traffic, reaching an arrival rate of 410\,Mbps and saturating the queue. In \textit{Interval 2}, the lowest priority queue has 66\,Mbps available for transmission with packets arriving at 264\,Mbps, whereas in \textit{Interval 3}, the lower-priority queue has only 10\,Mbps for transmission while receiving traffic at 410\,Mbps. This results in a substantial increase in packet losses and latency, making the ``yellow" traffic of all flows unusable and reaching levels exceeding 1200\,ms, as shown in Figs.~\ref{fig:expa-ietf-proposal}h and \ref{fig:expa-ietf-proposal}e.} 

In contrast, \gls{hctns} incorporates a global policer that limits the traffic admitted at the transport network's ingress, ensuring that the accepted traffic rate does not exceed the capacity of the link connecting the PE1 and P routers (100\,Mbps). The proposed three-level \gls{htb} efficiently shares bandwidth among slices, and further distributes it within slices according to their \gls{cir} and \gls{pir} values. As shown in Fig.~\ref{fig:expa-ietf-proposal}c, each traffic class is provided with more than its guaranteed bandwidth. For instance, VC traffic is transmitted at 54.3\,Mbps, exceeding the \gls{cir} of 52.8\,Mbps. The results obtained by our model in terms of latency (Fig.~\ref{fig:expa-ietf-proposal}f) are remarkable, with values approximately two orders of magnitude lower than those observed with the \gls{ietf} \rev{and the~\cite{Lin2021} models}. For example, the packet delay perceived by the \gls{urllc} packets is approximately 7\,ms when using our proposal, compared to over 350\,ms with the \gls{ietf} model \rev{and 1230\,ms with the~\cite{Lin2021} model}. This improvement is due to absence of queuing delays in the \gls{drr} queues associated with the output port of the \gls{pe}. In \gls{hctns}, packet delay under saturation scenarios is primarily influenced by the token arrival rate for each traffic class. 

During \textit{Interval 4}, all traffic classes remain active except the \gls{tod} video. In the \gls{ietf} model, this results in the \gls{drr} queue associated with the \gls{tn} \gls{qos} class B becoming empty. Consequently, the bandwidth previously allocated to this traffic flow is now available to be shared between \gls{tn} \gls{qos} classes C and D. As a result, the packet scheduler can serve packets from the two remaining \gls{drr} queues at a rate higher than 32.9\,Mbps, in particular, at 49.4\,Mbps. This allows packets waiting in the \gls{drr} queues associated with \gls{tn} \gls{qos} classes C and D to be dequeued more quickly. Fig.~\ref{fig:expa-ietf-proposal}d illustrates the resulting reduction in latency in the \gls{ietf} model for all active traffic classes, which decreases to approximately to 200\,ms.

\rev{In the~\cite{Lin2021} model, unlike the \gls{ietf} and \gls{hctns} models, the bandwidth released by the video traffic is not directly consumed by the telemetry traffic, since the~\cite{Lin2021} model treats telemetry and video traffic as separated slices. The bandwidth below the \gls{cir} released by the video is freed in the higher priority queue and added to the lower priority queue. Moreover, the bandwidth between the \gls{cir} and the \gls{pir} released by the video also becomes available in the lower priority queue and can be leveraged by ``yellow'' traffic from other traffic flows. However, traffic continues to arrive at a higher rate than the lower priority queue can transmit now, i.e., 42\,Mbps. As a result, the queue remains full, although packet loss rates and latencies decrease.}

\rev{In \gls{hctns}, since the \gls{tod} video flow is deactivated, the remaining traffic classes gain a slightly larger share of the available bandwidth. The total \gls{cir} across all defined traffic classes in this interval amounts to 58\,Mbps, leaving 42\,Mbps for redistribution by the global policer among all active flows. Therefore, in this interval, telemetry traffic, which belongs to the \gls{tod} slice, consumes the \gls{cir} in the slice released by the video, reaching 36\,Mbps, while \gls{urllc}, VC, and BE traffic consume the rest of the bandwidth, reaching 4.8\,Mbps, 3.6\,Mbps, and 55.5\,Mbps, respectively. }

During \textit{Interval 5}, the VC traffic flow is deactivated, causing the \gls{drr} queue associated with the \gls{tn} \gls{qos} class C in the \gls{ietf} model to become empty (Fig.~\ref{fig:expa-ietf-proposal}j), as it only receives packets from the telemetry traffic class. Consequently, packet delay for telemetry packets decreases notably during this interval, as shown in Fig.~\ref{fig:expa-ietf-proposal}d. Additionally, the number of packets lost for \gls{tn} \gls{qos} class C stops increasing, as Fig.~\ref{fig:expa-ietf-proposal}g shows. In contrast, traffic continues to arrive at \gls{tn} \gls{qos} class D faster than it can be transmitted, causing the corresponding \gls{drr} queue to remain full. This explains the ongoing enqueuing and packet loss observed for \gls{tn} \gls{qos} class D. \rev{In the \cite{Lin2021} model, the lowest priority queue gains more bandwidth that is shared between the BE and the ``yellow" traffic of the \gls{urllc} and telemetry flows, increasing their bandwidths as shown in Fig.~\ref{fig:expa-ietf-proposal}b. However, traffic continues to arrive at a higher rate than the lower priority queue can transmit now, 294.8\,Mbps versus 94.8\,Mbps. As a result, the queue remains full, but packet loss rates and latencies decrease, as shown in Figs.~\ref{fig:expa-ietf-proposal}h and \ref{fig:expa-ietf-proposal}e.} In \gls{hctns}, the active classes achieve a larger share of bandwidth without packet losses or added delay.  

Finally, \textit{Interval 6} returns to the initial state, where only BE traffic is active in the experiment. In the \gls{ietf} model, this traffic encounters the \gls{drr} queue associated with \gls{tn} \gls{qos} class D already full of packets. Fig.~\ref{fig:expa-ietf-proposal}j shows that packets from this class remain enqueued during this interval. This results in a larger latency than in \textit{Interval 1}. However, Fig.~\ref{fig:expa-ietf-proposal}e indicates that no additional packets are lost from this traffic class. This is because BE traffic is being received at the transmission rate. The results achieved in terms of bandwidth are consistent with those observed during \textit{Interval 1}. \rev{Similarly to the \gls{ietf} model, in the \cite{Lin2021} model, BE traffic encounters the lower priority queue already full of packets, as Fig.~\ref{fig:expa-ietf-proposal}k shows, resulting in a higher experienced latency than in \textit{Interval 1}. Nevertheless, Fig.~\ref{fig:expa-ietf-proposal}h indicates that no additional packets are lost. This is because BE traffic is received at the transmission rate available in the lower priority queue. The results achieved in terms of bandwidth are also consistent with those observed during \textit{Interval 1}}. In \gls{hctns}, the results in terms of both bandwidth and latency are the same as in \textit{Interval 1}.

An important conclusion from this experiment is that, \rev{unlike the \gls{ietf} model, the~\cite{Lin2021} model and \gls{hctns} comply with the \glspl{cir} defined (Table~\ref{tab:expa-params}) in all the intervals.  However, the results obtained with the~\cite{Lin2021} model show that the traffic above the \gls{cir} experiments large latencies, while our model ensures that all the traffic from the slices is treated with the expected \gls{qos} level. Moreover, our model allows that the bandwidth unused by any traffic class from a slice to be shared among the other classes. In contrast, the \gls{ietf} model restricts the sharing of unused bandwidth in the slice to the best-effort classes only, and the~\cite{Lin2021} model does not distinguish between slices and classes, treating each flow as a separated slice.} Moreover, our proposed model provides flexibility for network operators to configure the bandwidth sharing mechanism according to their specific needs, enabling adaptation to the slices defined in their network scenarios.

The results described above were obtained using the Linux \gls{htb} bandwidth sharing mechanism with all leaf nodes quantum set to the same value (the packet size). However, \gls{hctns}, unlike the other models, introduces two alternative methods for bandwidth sharing within the ingress policer: Weighted/Quantum and Priority based. As previously mentioned, the Linux \gls{tc} tool currently allows configuring these parameters only at the child nodes of the \gls{htb} tree. Our proposal extends this capability to any node of the tree. To evaluate the potential utility of these alternative bandwidth sharing methods, we conducted an additional experiment leveraging the capabilities of the Linux \gls{tc} tool. Table~\ref{tab:expa-params-sharing} shows the priority and quantums values set for the defined traffic classes in our network scenario. The results, depicted in Fig.~\ref{other-bw-config}, show the bandwidth evolution over the same timeline as in the previous experiment. 

\begin{table}[t]
\vspace{0.5em}
%\centering
\hspace*{6em}
\resizebox{0.33\textwidth}{!}{%
\begin{tabular}{cccll}
\cline{1-3}
\multicolumn{1}{|c|}{\cellcolor[HTML]{FFFFFF}\textbf{Traffic Class}} & \multicolumn{1}{c|}{\cellcolor[HTML]{FFFFFF}\textbf{PRIO}} & \multicolumn{1}{c|}{\cellcolor[HTML]{FFFFFF}\textbf{Quantum}} &  &  \\ \cline{1-3}
\multicolumn{1}{|c|}{\cellcolor[HTML]{FFFFFF}URLLC}                  & \multicolumn{1}{c|}{\cellcolor[HTML]{FFFFFF}0}             & \multicolumn{1}{c|}{\cellcolor[HTML]{FFFFFF}18456 B}           &  &  \\ \cline{1-3}
\multicolumn{1}{|c|}{\cellcolor[HTML]{FFFFFF}Video}                  & \multicolumn{1}{c|}{\cellcolor[HTML]{FFFFFF}0}             & \multicolumn{1}{c|}{\cellcolor[HTML]{FFFFFF}15380 B}           &  &  \\ \cline{1-3}
\multicolumn{1}{|c|}{\cellcolor[HTML]{FFFFFF}Telemetry}              & \multicolumn{1}{c|}{\cellcolor[HTML]{FFFFFF}0}             & \multicolumn{1}{c|}{\cellcolor[HTML]{FFFFFF}3076 B}            &  &  \\ \cline{1-3}
\multicolumn{1}{|c|}{\cellcolor[HTML]{FFFFFF}VC}                     & \multicolumn{1}{c|}{\cellcolor[HTML]{FFFFFF}7}             & \multicolumn{1}{c|}{\cellcolor[HTML]{FFFFFF}12304 B}           &  &  \\ \cline{1-3}
\multicolumn{1}{|c|}{\cellcolor[HTML]{FFFFFF}BE}                     & \multicolumn{1}{c|}{\cellcolor[HTML]{FFFFFF}7}             & \multicolumn{1}{c|}{\cellcolor[HTML]{FFFFFF}6152 B}            &  &  \\ \cline{1-3}
\multicolumn{1}{l}{}                                                 & \multicolumn{1}{l}{}                                       & \multicolumn{1}{l}{}                                          &  &  
\end{tabular}%
}
\vspace*{0.5em}
\caption{New bandwidth sharing configuration in the ingress policer.}
\label{tab:expa-params-sharing}
\vspace*{-0.75em}
\end{table}

\begin{figure}[t]
    \vspace*{-0.75em}
    \centering
    \includegraphics[width=0.47\textwidth, trim=210 10 210 10, clip]{figures/results/ExperimentA/legend_5g_half.pdf}
    \centering
    \begin{minipage}[t]{0.45\textwidth} % Primer subfigura
        \centering
        \input{figures/results/ExperimentA/Proposed/bandwidth_behaviour_alternative_sharing}
    \end{minipage}
    \vspace{-0.35cm}
    \caption{Bandwidth Behaviour with Table~\ref{tab:expa-params-sharing} configuration.}
    \label{other-bw-config}
\end{figure}

During \textit{Interval 1}, when only BE traffic is active, the bandwidth evolution matches that shown in Fig.~\ref{fig:expa-ietf-proposal}b. In \textit{Interval 2}, however, differences appear. Traffic classes from the \gls{tod} slice are given higher priority than those from the \gls{embb} slice. As a result, when tokens are available in the global policer, \gls{tod} slice traffic classes are prioritized for consuming the remaining bandwidth. Additionally, the \gls{tod} video traffic class is assigned a quantum value five times higher than that of the telemetry traffic class, enabling video traffic to borrow up to five times more bandwidth than telemetry traffic. Nonetheless, the \gls{cir} set for all of the traffic classes and slices is guaranteed. For instance, while BE traffic cannot borrow bandwidth due to its low priority, the \gls{cir} of the \gls{embb} slice is still respected.

During \textit{Interval 3}, all traffic classes have their \gls{cir} guaranteed. However, as shown in Table~\ref{tab:expa-params-sharing}, the \gls{urllc} and the \gls{tod} traffic classes are assigned higher priority. This ensures that any unused bandwidth is exclusively shared between these two classes according to the configured quantum values. In this scenario, since BE traffic does not have a guaranteed rate and is assigned the lowest priority, it does not consume any bandwidth, with packets from this class being denied access by the ingress policer.

In \textit{Interval 4}, the telemetry class takes over the bandwidth initially allocated to the \gls{tod} slice, while the \gls{urllc} traffic takes over the remaining available bandwidth. During the \textit{Interval 5}, when the \gls{tod} video and \gls{embb} VC traffic classes are deactivated, the active traffic flows obtain the \gls{cir} allocated to their slices, with the \gls{urllc} traffic consuming the remaining bandwidth. Finally, in \textit{Interval 6}, where only BE traffic is active in the \gls{tn}, the behavior mirrors that of \textit{Interval 1}.

The results demonstrate that, regardless of the bandwidth sharing method used, the global policer ensures that traffic is transmitted to the \gls{tn} without exceeding the global available bandwidth. This prevents packets from waiting in the \gls{drr} and priority queues of the coarse-grained resource control mechanism, while also avoiding packet losses, the same as in Figs.~\ref{fig:expa-ietf-proposal}f and \ref{fig:expa-ietf-proposal}h.

\input{files/table-expb}

\subsubsection{Experiment B: Teleoperated Driving Slice Performance}
\label{expb}

In this experiment, we analyze the impact of coarse-grained resource control configuration on the bandwidth consumption and packet losses of the telemetry and video traffic classes within the ToD slice. The \gls{pe}1 router of our network scenario has been configured using the parameters and values listed in Table~\ref{tab:expb-params}. While the configuration of the fine-grained resource control mechanism remains unchanged, four different configurations of the coarse-grained resource control mechanism are defined in this experiment. The \gls{ietf} network slicing model and \gls{hctns} are compared. 

%% PRIMER EXPERIMENTO
\begin{figure*}[!h]
    \centering
    % LEYENDA
        \vspace*{-2.5em}
        \hspace*{-0.8cm}\includegraphics[width=1\textwidth, trim=0 10 0 10, clip]{figures/results/ExperimentB/legend_video.pdf}
    % PRIMERA FILA
    \begin{minipage}[t]{0.4\textwidth} % Primer subfigura
        \centering
        \input{figures/results/ExperimentB/IETF/video_bandwidth}\vspace*{-0.1cm}
        \vspace*{-0.2cm} \centering \small (a) IETF: Video Bandwidth Evolution.
    \end{minipage}
    \vspace*{0.2cm}
    \hspace{1.5cm}
    \begin{minipage}[t]{0.4\textwidth} % Segunda subfigura
        \centering
        \input{figures/results/ExperimentB/Proposed/video_bandwidth}
        \vspace*{-0.2cm} \centering \small (b) \gls{hctns}: Video Bandwidth Evolution.
    \end{minipage}
    \vspace*{0.2cm}

    % SEGUNDA FILA
    \begin{minipage}[t]{0.4\textwidth}
        \centering
        %
        \input{figures/results/ExperimentB/IETF/video_pl}
        \vspace*{-0.2cm} \centering \small (c) IETF: Video Packet Loss in output PE port.
    \end{minipage}
    \vspace*{0.2cm}
    \hspace{1.5cm}
    \begin{minipage}[t]{0.4\textwidth} % Cuarta subfigura
        \centering
        \input{figures/results/ExperimentB/Proposed/video_pl}
        \vspace*{-0.2cm} \centering \small (d) \gls{hctns}: Video Packet Loss in output PE port.
    \end{minipage}
    \vspace*{0.2cm}

    % SEGUNDA LEYENDA
    \hspace*{-0.8cm}\includegraphics[width=1\textwidth, trim=0 15 0 10, clip]{figures/results/ExperimentB/legend_telemetry.pdf}
    % TERCERA FILA
    \begin{minipage}[t]{0.4\textwidth}
        \centering
        \input{figures/results/ExperimentB/IETF/telemetry_bandwidth}
        \centering \small (e) IETF: Telemetry Bandwidth Evolution.
    \end{minipage}
    \vspace*{0.2cm}
    \hspace{1.5cm}
    \begin{minipage}[t]{0.4\textwidth} % Cuarta subfigura
        \centering
        \input{figures/results/ExperimentB/Proposed/telemetry_bandwidth}
        \centering \small (f) \gls{hctns}: Telemetry Bandwidth Evolution.
    \end{minipage}
    \vspace*{0.2cm}
    % CUARTA FILA
    \begin{minipage}[t]{0.4\textwidth}
        \centering
        \input{figures/results/ExperimentB/IETF/telemetry_pl}
        \centering \small (g) IETF: Telemetry Packets Queued in output PE port.
    \end{minipage}
    \hspace{1.5cm}
    \begin{minipage}[t]{0.4\textwidth} % Cuarta subfigura
        \centering
        \input{figures/results/ExperimentB/Proposed/telemetry_pl}
        \centering \small (h) \gls{hctns}: Telemetry Packets Queued in output PE port.
    \end{minipage}
    \vspace*{0.1cm}
    \caption{Experiment B: \gls{ietf} versus our proposed network slicing model.}
    \label{expb-ietf-proposal}
\end{figure*}

This experiment also spans 100\,s and it is divided into the same six time intervals defined for Experiment A. First, we analyze the performance of the \gls{ietf} model. As illustrated in Fig.~\ref{expb-ietf-proposal}a, with the third and fourth coarse-grained configurations, the \gls{tod} video traffic flow achieves its guaranteed rate throughout the entire experiment. In contrast, with the first two configurations, this guarantee is not met due to packet losses occurring at the output port of \gls{pe}1, as shown in Fig.~\ref{expb-ietf-proposal}c. This is caused by the low quantum value assigned to the queue of the \gls{tn} \gls{qos} class B (i.e., the \gls{tn} \gls{qos} class assigned to video) in the first two configurations. This quantum is significantly lower than the values assigned to the other \gls{drr} queues, resulting in a reduced transmission rate at the output port for the \gls{tn} \gls{qos} class B queue. Consequently, when there is traffic in the other \gls{drr} queues, more packets arrive in the \gls{drr} queue associated with \gls{tn} \gls{qos} class B than can be transmitted, leading to an accumulation of packets, overflow of the queue, and packet losses.

For telemetry traffic, with the \gls{ietf} model, as shown in Fig.~\ref{expb-ietf-proposal}e, with the second and fourth coarse-grained configurations, the telemetry traffic achieves its guaranteed rate throughout the entire experiment. However, with the first and third configurations, this guarantee is not met due to packet losses occurring at the output port of \gls{pe}1, as shown in Fig.~\ref{expb-ietf-proposal}g. The same reasons given in the previous point apply to this case. The quantum values set in the second and fourth configurations favor the transmission of the \gls{tn} \gls{qos} class C (i.e., the \gls{tn} \gls{qos} class associated to telemetry). 

These results highlight the importance of properly configuring the coarse-grained resource control parameters in transport network routers when employing the \gls{ietf} network slicing model. In contrast, the slice model proposed in this paper guarantees bandwidth levels for both video and telemetry traffic (Figs.~\ref{expb-ietf-proposal}b and~\ref{expb-ietf-proposal}f), while preventing packet losses (Figs.~\ref{expb-ietf-proposal}d and~\ref{expb-ietf-proposal}h). This is particularly crucial for a service like \gls{tod}, which demands high-reliable communications with guaranteed bandwidth to ensure road safety. \gls{hctns} demonstrates robustness and independence from the coarse-grained resource control configuration under normal traffic conditions (without traffic bursts), ensuring that traffic classes receive the \gls{qos} level they require. Experiment C illustrates how the proposed traffic burst control mechanism performs under different network scenarios.

\subsubsection{Experiment C: Traffic Burst Control}

This experiment analyzes the impact of traffic bursts on the latency experienced by packets from different traffic classes and slices. The \gls{ietf} model does not incorporate this function, so results only show the performance of \rev{the \cite{Lin2021} model and} our proposed traffic control burst mechanism for \gls{hctns}. For this purpose, the \gls{pe}1 router of our network scenario has been configured using the parameters and values listed in Table~\ref{tab:expc-params}. Unlike Experiment A, the fine-grained resource control mechanism in this experiment incorporates traffic burst control, with the \gls{cbs} and the \gls{pbs} parameters defined for both models. \rev{Although the work in~\cite{Lin2021} does not address traffic burst control, it mentions that the \gls{trtcm} mechanism they use has two parameters, \gls{cbs} and \gls{pbs}, that can be used to control traffic bursts. For Experiment C, we have configured these parameters in our implementation of the~\cite{Lin2021} model.  Table~\ref{tab:expc-params} shows some differences between the \gls{cir} and \gls{pir} parameters configured for \gls{hctns} and the~\cite{Lin2021} model. To make the closest comparison between the two models, since \gls{hctns} treats all the traffic accepted with the same \gls{qos}, we have configured the burst in \cite{Lin2021} using only the \gls{cbs} parameter, in order to treat all burst traffic as ``green". The \gls{pbs} values remain at their default setting, i.e, the maximum packet size. As   Table~\ref{tab:expc-params} shows, the \gls{cbs} and \gls{pbs} parameters have not been specified for BE traffic, which means that the bucket sizes can only store enough tokens to allow the transmission of a single packet. This is because BE traffic does not have a guaranteed rate.} Additionally, Table~\ref{tab:expc-params} indicates that two coarse-grained resource control configurations have been applied in our model to analyze how traffic burst control behaves under these two configurations. \rev{In \cite{Lin2021}, only two priority queues are used, so no other parameter configurations are possible.}

\input{files/table-expc}   

This experiment lasts 60 seconds and is divided into the three following time intervals: 

\begin{enumerate}
    \item Interval 1 (from \( t = 0 \, \text{s} \) to \( t = 10 \, \text{s} \)): All traffic classes are active, but there are no traffic bursts during this interval. 
    \item Interval 2 (from \( t = 10 \, \text{s} \) to \( t = 45 \, \text{s} \)): All traffic classes are active and transmitting at a constant rate. However, traffic bursts are introduced for all classes: \gls{urllc} generates a burst every second, the \gls{tod} video every two seconds, and both \gls{tod} telemetry and \gls{embb} VC every five seconds.
    \item Interval 3 (from \( t = 45 \, \text{s} \) to \( t = 60 \, \text{s} \)): the same conditions as Interval 2, except that the traffic burst from \gls{urllc} is no longer present.
\end{enumerate}

\rev{All flows have a background constant rate that is significantly below their guaranteed rate, except for BE traffic, which is transmitted at 100\,Mbps. This approach ensures that the buckets have time to refill, allowing multiple bursts per class to be observed rather than just one. Moreover, throughout the experiment, all generated bursts have a uniform size of 100\,KB. The traffic generated for this experiment is shown in Fig. \ref{expc-conf}a.}

The results are presented in Figs.~\ref{expc-conf}b, \ref{expc-conf}c, \ref{expc-conf}d and \ref{expc-conf}e, illustrating  the \gls{qos} achieved by the traffic flows in terms of both bandwidth and latency with both models. The \textit{Conf1} configuration shown in Table~\ref{tab:expc-params} for the coarse-grained resource control has been used in the \gls{hctns} model to obtain these results.

\afterpage{
\begin{figure*}[!h]
    \vspace*{-2em}
    \centering
    \includegraphics[width=1\textwidth, trim=50 10 50 10, clip]{figures/results/ExperimentA/legend_5g.pdf}
    \begin{minipage}[t]{0.45\textwidth} % Primer subfigura
        \centering
        \input{figures/results/ExperimentC/Proposed/bandwidth_sent}
        \centering \small (a) Experiment C Traffic Generation.
    \end{minipage}
    \vspace*{0.4cm}
    \hfill
    \begin{minipage}[t]{0.45\textwidth} 
        \centering
        \input{figures/results/ExperimentC/SoA/bandwidth_received}
        \centering \small (b) \cite{Lin2021}: Bandwidth Received Under Burst Traffic.
    \end{minipage}
    \vspace*{0.4cm}
    \begin{minipage}[t]{0.45\textwidth} 
        \centering
        \input{figures/results/ExperimentC/Proposed/bandwidth_received}
        \centering \small (c) \gls{hctns}: Bandwidth Received Under Burst Traffic.
    \end{minipage}
    \vspace*{0.2cm}
    \hfill
    \begin{minipage}[t]{0.45\textwidth} 
        \centering
        \input{figures/results/ExperimentC/SoA/max_lat}
        \centering \small (d) \cite{Lin2021}: Maximum Latency Under Burst Traffic.
    \end{minipage}
    \vspace*{0.2cm}
    \begin{minipage}[t]{0.45\textwidth}
        \centering
        \input{figures/results/ExperimentC/Proposed/max_lat}
        \centering \small (e) \gls{hctns}: Maximum Latency Under Burst Traffic (\textit{Conf1}).
    \end{minipage}
    \hfill
    \begin{minipage}[t]{0.45\textwidth}
        \centering
        \input{figures/results/ExperimentC/Proposed/max_lat_conf2}
        \centering \small (f) \gls{hctns}: Maximum Latency Under Burst Traffic (\textit{Conf2}).
    \end{minipage}
    \caption{Experiment C: \gls{hctns} and \cite{Lin2021} model performance under burst traffic.}
    \label{expc-conf}
\end{figure*}
}

\rev{During \textit{Interval 1}, traffic classes transmit at a constant rate without generating traffic bursts. As depicted in Figs.~\ref{expc-conf}b and \ref{expc-conf}c, both models show similar bandwidth behavior, with BE traffic using all the remaining bandwidth not consumed by other traffic classes. However, the latency behavior (Figs.~\ref{expc-conf}d and~\ref{expc-conf}e) for the BE traffic differs, since the~\cite{Lin2021} model, in addition to accepting background traffic from other flows, admits 100\,Mbps of BE traffic. As a result, the low priority queue is congested and BE packets experiment a high latency. In contrast, since \gls{hctns} incorporates a global policer that limits the traffic admitted at the transport network's ingress, it ensures that the accepted traffic does not exceed the queues' capacity. This results in low latency values, similar to the rest of the flows.}

During \textit{Interval 2}, traffic bursts are generated for all defined traffic classes. \rev{In the~\cite{Lin2021} model, the bursts accepted from all traffics are classified as ``green", being forwarded to the highest priority queue. As the priority queue is not saturated, these packets are transmitted. When bursts are accepted, the highest priority queue uses a higher transmission rate, which reduces the transmission rate of the lower priority queue, causing the downward spikes in bandwidth in Fig.~\ref{expc-conf}b.} In \gls{hctns}, a very close bandwidth behavior (Fig.~\ref{expc-conf}c) is observed, accepting a similar amount of burst packers. In this case, the configured \gls{cbs} and \gls{pbs} values allow the defined amount of bytes to ingress the \gls{tn}, while the excess traffic is queued in the \gls{htb} ingress queue. If this queue becomes full, any additional packets are discarded. As explained in Section~\ref{sec:proposal}.\ref{subsec:proposal-finegraine}, when traffic burst control is enabled and bursts are accepted by the class policers (or the slice policer for \gls{urllc} traffic), both the slice and global policers may temporally have ``negative'' tokens. Consequently, BE traffic is not admitted by the ingress policer until the slice policer's bucket recovers the tokens it owed restoring a positive balance. Until the global policer's bucket is not fully recovered, BE traffic cannot borrow bandwidth beyond the \gls{embb} slice's \gls{cir}. As a result, during burst periods, the bandwidth available to the BE traffic shows also temporary downward spikes. This mechanism ensures that, while packets arriving at rates exceeding the \gls{cir} defined in the global policer may be temporally accepted, the average bandwidth will remain within the defined limits. 

\rev{As shown in Fig.~\ref{expc-conf}d, traffic bursts impact the latency of all flows when using the~\cite{Lin2021} model. This is because the accepted burst packets (``green") are forwarded to the highest priority queue, regardless of the traffic flow to which they belong. As a result, all burst traffic from the flows undergoes similar treatment. For example, all  traffic flows experience a similar maximum latency,  around 15\,ms. Packets marked as ``yellow" are forwarded to the lowest priority queue, which is congested. Since BE traffic continuously enters the lowest priority queue at 100\,Mbps and the lowest priority queue cannot transmit a packet if there are packets in the highest priority queue, most ``yellow" packets from the burst will be discarded due to queue overflow. For the same reason, ``yellow" packets, such as those from \gls{urllc} traffic, experience latencies exceeding 130 ms, which is unacceptable.}

In \gls{hctns}, as illustrated in Fig.~\ref{expc-conf}e, traffic bursts also have a significant impact on the latency. While the average rate at which packets arrive at the \gls{pe}1 input port does not exceed the output port capacity, bursts temporarily surpass this capacity. This causes packets to be enqueued at the \gls{pe}1 output port, resulting in delays. However, since the average accepted rate remains below the global policer's \gls{cir}, which is adjusted to the available network capacity, packets do not accumulate over time in these queues. Therefore, it is crucial to properly dimension the queue sizes to handle bursty traffic effectively, preventing packet loss. 

Since \gls{urllc} traffic is assigned to \gls{tn} \gls{qos} class A, which has a priority queue, its latency remains unaffected  by traffic bursts from the other \gls{tn} \gls{qos} classes. However, \gls{urllc} bursts impact the delay in the \gls{drr} queues, as these queues must wait for the priority queues to empty before transmitting packets. Additionally, the latency experienced by traffic classes in \gls{tn} \gls{qos} classes with \gls{drr} queues is also influenced by the bursts in other \gls{drr} queues. For example, when bursts from the telemetry and VC traffic classes are transmitted, the latency experienced by BE and \gls{tod} video traffic increases. As will be explained below, different coarse-grained resource control configurations can be applied to manage and control this latency. \rev{Unlike \cite{Lin2021}, all traffic accepted by the policers from the same \gls{tn} \gls{qos} class receives the same treatment, achieving \gls{urllc}, video, telemetry, and VC latencies of under 9\,ms, 13\,ms, 21\,ms, and 21\,ms, respectively. The BE queue is not saturated, so latency values remain within these ranges.}

\rev{In \textit{Interval 3}, when there are no \gls{urllc} traffic bursts, the latency perceived by the other traffic classes decreases in both models. In the~\cite{Lin2021} model, fewer packets are queued in the highest priority queue, which reduces the waiting time for packets arriving at this queue. As a result, the maximum latency values achieved are around 12\,ms. Additionally, although some packets are marked as ``yellow'', the saturation of the lowest priority queue prevents any ``yellow" packets from entering the buffer. This explains why the latency spikes have disappeared. However, as the lowest priority queue remains saturated, the latency perceived by BE traffic does not change.}

\rev{In \gls{hctns}, the latency experienced by traffic classes assigned to \gls{tn} \gls{qos} classes with an allocated \gls{drr} queue is still influenced by bursts from other \gls{drr} queues. However, the latency values decrease to under 8\,ms for video and under 15\,ms for telemetry and VC. Since the BE queue is not saturated, its latency values also remain within these ranges.}

In \gls{hctns}, as expected, all the traffic classes sharing a \gls{tn} \gls{qos} class experiment similar latency values, due to the \gls{phb} applied at the output port through the coarse-grained resource control configuration. 

Fig.~\ref{expc-conf}f shows how different coarse-grained resource control configurations (quantum values)  affect latencies experienced by the traffic classes. The \textit{Conf2} configuration, as shown in Table~\ref{tab:expc-params}, was used to obtain the following results. 

This new configuration of the coarse-grained resource control mechanism allows the \gls{tn} \gls{qos} class B (which carries \gls{tod} video) to have a higher transmission rate, compared to the previous configuration. As a result, its queued packets are transmitted earlier, reducing the latency experienced. For example, a maximum latency of 12.89\,ms was measured in the experiment shown in Fig.~\ref{expc-conf}e, compared to 10.69\,ms in the experiment shown in Fig.~\ref{expc-conf}f. The other \gls{tn} \gls{qos} classes now have a lower transmission rate than in the previous coarse-grained resource control configuration, which leads to increased latency for those traffic classes.

\rev{Despite both models achieving similar bandwidth results, in \gls{hctns}, by adjusting the quantum values in the coarse-grained resource control configuration, the latency experienced by the traffic of a \gls{tn} \gls{qos} class can be controlled. As observed throughout all the experiments, when using \gls{hctns}, packets experience queuing delays under burst traffic conditions. However, even in these cases, unlike with the~\cite{Lin2021} model, this latency can be controlled by \gls{hctns}. In \gls{hctns}, the latency of \gls{urllc} traffic is solely influenced by the traffic bursts of this \gls{tn} \gls{qos} class, while the latency in the remaining traffic classes depends not only on the accepted \gls{urllc} traffic bursts, but also on the bursts accepted by other \gls{tn} \gls{qos} classes. In contrast, in the~\cite{Lin2021} model, the latency of each traffic flow is influenced by the bursts of other traffic flows, including BE traffic, when some of its packets are classified as ``yellow". This causes the~\cite{Lin2021} model to perform worse than \gls{hctns}.}

\section{Conclusions}
\label{sec:conclusion}

In this paper, we have proposed \gls{hctns}, a slicing model  that employs a three-level hierarchical \gls{htb} to implement a fine-grained resource control mechanism at the ingress node of the transport network. This mechanism is combined with coarse-grained resource control in the transport network nodes to allow meeting the bandwidth and latency requirements of services defined within the slices, even in worst-case scenarios and with bursty traffic, while achieving a high level of network bandwidth sharing both within and across slices. One of the strengths of \gls{hctns} is that all the traffic admitted to be processed in the transport network is treated with \gls{qos} guarantees, not only the traffic under the guaranteed bitrate but also traffic exceeding it (up to certain limit), and even for limited traffic bursts.

\rev{In real deployments, each network segment operates independently as an administrative domain, meaning that end-to-end performance depends on the specific traffic management solutions implemented in each segment. HCTNS ensures that the latency, bandwidth, and packet loss requirements defined in the SLAs are met within the transport network, contributing to their fulfillment end-to-end. While HCTNS does not impact the performance of the RAN or core network, it is essential for maintaining service guarantees in the transport network and, by extension, across the entire system.}

\rev{An experimental platform combining virtual and physical network elements has allowed us to verify that \gls{hctns} outperforms not only the model being standardized by the \gls{ietf}, but also a state-of-the-art reference approach, in terms of bandwidth utilization, latency and traffic burst control.}  



As future steps for this research, we believe that centralized policies are needed to guarantee end-to-end delay bounds in the transport network. In addition, models are required to translate these policies into configurations for data plane functions in the transport network equipment. This includes the establishment and configuration of IPv6 tunnels based on \gls{sr} with routing algorithms driven by \gls{qos} metrics.  


\bibliography{ojcoms}

\input{files/biography}

\end{document}