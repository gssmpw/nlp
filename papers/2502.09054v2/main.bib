@inproceedings{Tortorella2001,
  author    = {F. Tortorella},
  title     = {An optimal reject rule for binary classifiers},
  booktitle = {Lecture Notes in Computer Science},
  volume    = {1876},
  pages     = {611--620},
  year      = {2001}
}



@misc{chen2023,
      title={FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance}, 
      author={Lingjiao Chen and Matei Zaharia and James Zou},
      year={2023},
      eprint={2305.05176},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2305.05176}, 
}

@misc{madaan2024,
      title={AutoMix: Automatically Mixing Language Models}, 
      author={Pranjal Aggarwal and Aman Madaan and Ankit Anand and Srividya Pranavi Potharaju and Swaroop Mishra and Pei Zhou and Aditya Gupta and Dheeraj Rajagopal and Karthik Kappaganthu and Yiming Yang and Shyam Upadhyay and Manaal Faruqui and Mausam},
      year={2024},
      eprint={2310.12963},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2310.12963}, 
}

@misc{yue2024,
      title={Large Language Model Cascades with Mixture of Thoughts Representations for Cost-efficient Reasoning}, 
      author={Murong Yue and Jie Zhao and Min Zhang and Liang Du and Ziyu Yao},
      year={2024},
      eprint={2310.03094},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2310.03094}, 
}

@misc{zellinger2025,
      title={Rational Tuning of LLM Cascades via Probabilistic Modeling}, 
      author={Michael J. Zellinger and Matt Thomson},
      year={2025},
      eprint={2501.09345},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2501.09345}, 
}

@article{elyaniv2010,
  title={On the Foundations of Noise-free Selective Classification},
  author={El-Yaniv, Ran and Wiener, Yair},
  journal={Journal of Machine Learning Research},
  volume={11},
  pages={1605--1641},
  year={2010},
  month={May},
  publisher={JMLR.org}
}

@misc{geifman2017,
      title={Selective Classification for Deep Neural Networks}, 
      author={Yonatan Geifman and Ran El-Yaniv},
      year={2017},
      eprint={1705.08500},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1705.08500}, 
}

@article{chow1957,
  title={An optimum character recognition system using decision function},
  author={Chow, C K},
  journal={IEEE Transactions on Computers},
  volume={6},
  number={4},
  pages={247--254},
  year={1957},
  publisher={IEEE}
}

@article{chow1970,
  title={On optimum recognition error and reject trade-off},
  author={Chow, C K},
  journal={IEEE Transactions on Information Theory},
  volume={16},
  pages={41--36},
  year={1970},
  publisher={IEEE}
}

@inproceedings{yoshikawa2023,
    title = "Selective-{LAMA}: Selective Prediction for Confidence-Aware Evaluation of Language Models",
    author = "Yoshikawa, Hiyori  and
      Okazaki, Naoaki",
    editor = "Vlachos, Andreas  and
      Augenstein, Isabelle",
    booktitle = "Findings of the Association for Computational Linguistics: EACL 2023",
    month = may,
    year = "2023",
    address = "Dubrovnik, Croatia",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.findings-eacl.150/",
    doi = "10.18653/v1/2023.findings-eacl.150",
    pages = "2017--2028",
    abstract = "Recent studies have suggested that neural language models learn and store a large amount of facts and commonsense knowledge from training data. The ability of language models to restore such knowledge is often evaluated via zero-shot cloze-style QA tasks. However, such evaluations rely only on prediction accuracy without punishing the systems for their mistakes, e.g., simply guessing or hallucinating likely answers. Selective prediction is a more informative evaluation framework that takes the confidence of predictions into account. Under the selective prediction setting, a model is evaluated not only by the number of correct predictions, but also by the ability to filter out dubious predictions by estimating the confidence of individual predictions. Such confidence-aware evaluation is crucial for determining whether to trust zero-shot predictions of language models. In this paper, we apply the selective prediction setting to an existing benchmark, LAMA probe, and conduct extensive experiments with recent neural language models and different confidence functions. We empirically show that our Selective-LAMA evaluation is more robust to the effect of simple guesses than the conventional accuracy-based evaluation. Our evaluation reveals the importance of the choice of confidence functions by showing that simply relying on token probabilities is not always the best choice. Further analysis shows that various confidence functions exhibit different preferences over predicted tokens for a given context."
}

@inproceedings{xin2021,
    title = "The Art of Abstention: Selective Prediction and Error Regularization for Natural Language Processing",
    author = "Xin, Ji  and
      Tang, Raphael  and
      Yu, Yaoliang  and
      Lin, Jimmy",
    editor = "Zong, Chengqing  and
      Xia, Fei  and
      Li, Wenjie  and
      Navigli, Roberto",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-long.84/",
    doi = "10.18653/v1/2021.acl-long.84",
    pages = "1040--1051",
    abstract = "In selective prediction, a classifier is allowed to abstain from making predictions on low-confidence examples. Though this setting is interesting and important, selective prediction has rarely been examined in natural language processing (NLP) tasks. To fill this void in the literature, we study in this paper selective prediction for NLP, comparing different models and confidence estimators. We further propose a simple error regularization trick that improves confidence estimation without substantially increasing the computation budget. We show that recent pre-trained transformer models simultaneously improve both model accuracy and confidence estimation effectiveness. We also find that our proposed regularization improves confidence estimation and can be applied to other relevant scenarios, such as using classifier cascades for accuracy{--}efficiency trade-offs. Source code for this paper can be found at \url{https://github.com/castorini/transformers-selective}."
}

@article{farquhar2024,
  title={Detecting hallucinations in large language models using semantic entropy},
  author={Farquhar, Shaun and Kossen, Jannik and Kuhn, Laurent and others},
  journal={Nature},
  volume={630},
  pages={625--630},
  year={2024},
  doi={10.1038/s41586-024-07421-0},
  publisher={Nature Publishing Group}
}

@misc{manakul2023,
      title={SelfCheckGPT: Zero-Resource Black-Box Hallucination Detection for Generative Large Language Models}, 
      author={Potsawee Manakul and Adian Liusie and Mark J. F. Gales},
      year={2023},
      eprint={2303.08896},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2303.08896}, 
}

@misc{lin2024,
      title={Generating with Confidence: Uncertainty Quantification for Black-box Large Language Models}, 
      author={Zhen Lin and Shubhendu Trivedi and Jimeng Sun},
      year={2024},
      eprint={2305.19187},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2305.19187}, 
}

@misc{jitkrittum2024,
      title={When Does Confidence-Based Cascade Deferral Suffice?}, 
      author={Wittawat Jitkrittum and Neha Gupta and Aditya Krishna Menon and Harikrishna Narasimhan and Ankit Singh Rawat and Sanjiv Kumar},
      year={2024},
      eprint={2307.02764},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2307.02764}, 
}

@misc{hendrycks2017,
      title={A Baseline for Detecting Misclassified and Out-of-Distribution Examples in Neural Networks}, 
      author={Dan Hendrycks and Kevin Gimpel},
      year={2018},
      eprint={1610.02136},
      archivePrefix={arXiv},
      primaryClass={cs.NE},
      url={https://arxiv.org/abs/1610.02136}, 
}

@misc{kadavath2022,
      title={Language Models (Mostly) Know What They Know}, 
      author={Saurav Kadavath and Tom Conerly and Amanda Askell and Tom Henighan and Dawn Drain and Ethan Perez and Nicholas Schiefer and Zac Hatfield-Dodds and Nova DasSarma and Eli Tran-Johnson and Scott Johnston and Sheer El-Showk and Andy Jones and Nelson Elhage and Tristan Hume and Anna Chen and Yuntao Bai and Sam Bowman and Stanislav Fort and Deep Ganguli and Danny Hernandez and Josh Jacobson and Jackson Kernion and Shauna Kravec and Liane Lovitt and Kamal Ndousse and Catherine Olsson and Sam Ringer and Dario Amodei and Tom Brown and Jack Clark and Nicholas Joseph and Ben Mann and Sam McCandlish and Chris Olah and Jared Kaplan},
      year={2022},
      eprint={2207.05221},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2207.05221}, 
}

@inproceedings{cortes2016,
  title={Learning with rejection},
  author={Cortes, Corinna and DeSalvo, Giulia and Mohri, Mehryar},
  booktitle={Algorithmic Learning Theory: 27th International Conference, ALT 2016},
  pages={67--82},
  year={2016},
  publisher={Springer International Publishing},
  address={Bari, Italy},
  month={October}
}

@ARTICLE{scipy2020,
  author  = {Virtanen, Pauli and Gommers, Ralf and Oliphant, Travis E. and
            Haberland, Matt and Reddy, Tyler and Cournapeau, David and
            Burovski, Evgeni and Peterson, Pearu and Weckesser, Warren and
            Bright, Jonathan and {van der Walt}, St{\'e}fan J. and
            Brett, Matthew and Wilson, Joshua and Millman, K. Jarrod and
            Mayorov, Nikolay and Nelson, Andrew R. J. and Jones, Eric and
            Kern, Robert and Larson, Eric and Carey, C J and
            Polat, {\.I}lhan and Feng, Yu and Moore, Eric W. and
            {VanderPlas}, Jake and Laxalde, Denis and Perktold, Josef and
            Cimrman, Robert and Henriksen, Ian and Quintero, E. A. and
            Harris, Charles R. and Archibald, Anne M. and
            Ribeiro, Ant{\^o}nio H. and Pedregosa, Fabian and
            {van Mulbregt}, Paul and {SciPy 1.0 Contributors}},
  title   = {{{SciPy} 1.0: Fundamental Algorithms for Scientific
            Computing in Python}},
  journal = {Nature Methods},
  year    = {2020},
  volume  = {17},
  pages   = {261--272},
  adsurl  = {https://rdcu.be/b08Wh},
  doi     = {10.1038/s41592-019-0686-2},
}

@misc{liu2023,
      title={G-Eval: NLG Evaluation using GPT-4 with Better Human Alignment}, 
      author={Yang Liu and Dan Iter and Yichong Xu and Shuohang Wang and Ruochen Xu and Chenguang Zhu},
      year={2023},
      eprint={2303.16634},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2303.16634}, 
}

@inproceedings{
kag2023,
title={Efficient Edge Inference by Selective Query},
author={Anil Kag and Igor Fedorov and Aditya Gangrade and Paul Whatmough and Venkatesh Saligrama},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=jpR98ZdIm2q}
}