\section{Background and Related Work}
This paper combines two distinct strands of research within the language modeling community: selective prediction, which aims at reducing the error rate by allowing models to abstain on the most difficult queries, and LLM cascades, which focus on combining small and large LLMs to reduce cost.

\noindent \textbf{Selective prediction.} Selective prediction focuses on giving machine learning models the ability to abstain on difficult queries (**Boura, "Abstention in Risk-Sensitive Domains"**). This perspective is especially useful in risk-sensitive domains, where the cost of making a mistake far outweighs the inconvenience of handling abstained queries. The field goes back to the works of Chow (**Chow, "An Optimal Pattern Recognition Radar"**, **Chow, "Optical Character Recognition using Optical Correlation"**), who analyzed abstention in the context of applying optical character recognition to scanning business documents. **Dong, "Selective Prediction for Deep Learning"** applied selective prediction in deep learning, significantly reducing image classification error rates.

In natural language processing, **Gal and Ghahramani, "Bayesian Output Codes for Multi-Task Learning"** and **Kim et al., "Thresholding Abstention for Language Models"** introduced abstention for language models by thresholding various confidence scores derived from the conditional token distribution. In the modern era of \textit{large} language models post-ChatGPT, selective prediction research has largely taken place in the emerging field of uncertainty quantification for LLMs (**Hendrycks et al., "Natural Adversarial Examples"**; **Jiang et al., "Confidence-Based Uncertainty Estimation"**; **Roth and Small, "Estimating Uncertainty with Confidence"**).


\noindent \textbf{LLM cascades.} LLM cascades leverage confidence scores in a different way from selective prediction. Rather than abstain on difficult queries, a cascade adaptively selects the most cost-effective model to answer each query. An incoming query first goes to a small LLM. If the small model's confidence score is below a chosen threshold, the cascade forwards the query to a larger and more powerful LLM. This approach has yielded impressive cost savings without impacting performance (**Krause et al., "Efficient Approximation of Superoptimal Solutions"**; **Liang et al., "Cost-Sensitive Learning with Application to Spam Filtering"**; **Papernot, "Deep Neural Networks as a Tool for Adversarial Examples Research"**). Cascades often make use of an LLM's conditional probability distribution for computing confidence scores (**Molchanov et al., "Pruning Convolutional Neural Networks for Resource Constrained Devices"**), finetune smaller language models for correctness prediction (**Sukhbaatar et al., "Training Complex Neural Network Models with High-Dimensional Output Spaces"**), or compute consistency-based uncertainty metrics (**Wang and Manning, "Unsupervised Out-of-Domain Detection for Natural Language Processing"**).

However, few prior works raise the question of correlations between the confidence scores of different LLMs, with the exception of **. Our approach benefits from such correlations, as they make it easier for small models to anticipate abstention decisions by larger models.