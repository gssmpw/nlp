\section{Background and Related Work}
This paper combines two distinct strands of research within the language modeling community: selective prediction, which aims at reducing the error rate by allowing models to abstain on the most difficult queries, and LLM cascades, which focus on combining small and large LLMs to reduce cost.

\noindent \textbf{Selective prediction.} Selective prediction focuses on giving machine learning models the ability to abstain on difficult queries (\cite{elyaniv2010}). This perspective is especially useful in risk-sensitive domains, where the cost of making a mistake far outweighs the inconvenience of handling abstained queries. The field goes back to the works of Chow (\cite{chow1957}, \cite{chow1970}), who analyzed abstention in the context of applying optical character recognition to scanning business documents. \cite{geifman2017} applied selective prediction in deep learning, significantly reducing image classification error rates.

In natural language processing, \cite{xin2021} and \cite{yoshikawa2023} introduced abstention for language models by thresholding various confidence scores derived from the conditional token distribution. In the modern era of \textit{large} language models post-ChatGPT, selective prediction research has largely taken place in the emerging field of uncertainty quantification for LLMs (\citealp{manakul2023}; \citealp{farquhar2024}; \citealp{lin2024}). 


\noindent \textbf{LLM cascades.} LLM cascades leverage confidence scores in a different way from selective prediction. Rather than abstain on difficult queries, a cascade adaptively selects the most cost-effective model to answer each query. An incoming query first goes to a small LLM. If the small model's confidence score is below a chosen threshold, the cascade forwards the query to a larger and more powerful LLM. This approach has yielded impressive cost savings without impacting performance (\citealp{chen2023}; \cite{madaan2024}; \citealp{yue2024}). Cascades often make use of an LLM's conditional probability distribution for computing confidence scores (\citealp{jitkrittum2024}), finetune smaller language models for correctness prediction (\citealp{chen2023}), or compute consistency-based uncertainty metrics (\citealp{yue2024}).

However, few prior works raise the question of correlations between the confidence scores of different LLMs, with the exception of \cite{zellinger2025}. Our approach benefits from such correlations, as they make it easier for small models to anticipate abstention decisions by larger models.