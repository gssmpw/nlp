\PassOptionsToPackage{table,dvipsnames}{xcolor}
\documentclass{article}
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables
\usepackage{algorithm}
\usepackage{algorithmicx}
\usepackage{algpseudocode}
\usepackage{xcolor}
\definecolor{mygray}{RGB}{175,175,175}
\usepackage{enumitem}
\usepackage{booktabs}
\usepackage{enumitem}
\usepackage{minitoc}
\usepackage{titletoc}
\usepackage{lipsum}
\usepackage{natbib}
\DeclareRobustCommand{\ttfamily}{\fontfamily{lmtt}\selectfont}
\DeclareRobustCommand\sectt[1]{{\fontsize{13}{12}\ttfamily#1}}
\newcommand{\modelname}{\texttt{CurvGAD}}
\newcommand{\modela}{$\mathbb{H}^{24} \times \mathbb{S}^{24}$}
\newcommand{\modelc}{$\mathbb{H}^{8} \times \mathbb{S}^{8} \times \mathbb{E}^{32}$}
\newcommand{\modelb}{$(\mathbb{H}^{8})^2 \times (\mathbb{S}^{8})^2 \times \mathbb{E}^{16}$}
\newcommand{\modeld}{$\mathbb{H}^{16} \times (\mathbb{S}^{16})^2$}
\newcommand{\modele}{$(\mathbb{H}^{16})^2 \times \mathbb{E}^{16}$}
\newcommand{\modelf}{$\mathbb{H}^{24} \times \mathbb{E}^{24}$}
\newcommand{\modelg}{$\mathbb{S}^{24} \times \mathbb{E}^{24}$}
\newcommand{\modelh}{$\mathbb{H}^{16} \times \mathbb{S}^{16} \times \mathbb{E}^{16}$}
\newcommand{\modeli}{$(\mathbb{S}^{8})^2 \times \mathbb{E}^{32}$}
\newcommand{\modelj}{$(\mathbb{H}^{16})^3$}
\newcolumntype{a}{>{\columncolor{Gray}}c}
\usepackage{amsthm}
\usepackage{tikz-cd}
%%
% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}
% \usepackage{paralist}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{tabularx}
\usepackage{amsthm}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{\textbf{Remark}}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
% \icmltitlerunning{Submission and Formatting Instructions for ICML 2025}

\begin{document}
\twocolumn[
\icmltitle{\modelname: Leveraging Curvature for Enhanced Graph Anomaly Detection}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
% \icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Karish Grover}{yyy}
\icmlauthor{Geoffrey J. Gordon}{yyy}
\icmlauthor{Christos Faloutsos}{yyy}
%\icmlauthor{}{sch}
%\icmlauthor{}{sch}
%\icmlauthor{}{sch}
\end{icmlauthorlist}

\icmlaffiliation{yyy}{Carnegie Mellon University}

\icmlcorrespondingauthor{Karish Grover}{karishg@cs.cmu.edu}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
% \printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.
\begin{figure*}
  \includegraphics[width=\textwidth]{teaser.pdf}
  \vspace{-5mm}
  \caption{\textbf{Curvature matters}. The effect of curvature information on node-level anomaly scores for \modelname\ across Weibo, Reddit, Cornell and Chameleon datasets \citep{zhao2020error}. The plots show anomaly score distributions against curvature values, where the {\color{blue}blue} region highlights potentially new anomalies due to curvature deviations, and the {\color{red}red} region highlights the effect of curvature on existing outliers. \textit{W/o Curv} refers to anomaly scores calculated without curvature. Observe that interesting anomaly-curvature patterns arise -- e.g., in (a), \textit{known} anomalies exhibit predominantly positive curvature, whereas in (d), they are mostly negative. In (b), some non-anomalous nodes exhibit irregular curvature deviations, forming an isolated cluster (i.e. potential curvature anomalies).
  \label{fig:teaser}}
\end{figure*}

\begin{abstract}
Does the intrinsic \textit{curvature} of complex networks hold the key to unveiling graph anomalies that conventional approaches overlook? Reconstruction-based graph anomaly detection (GAD) methods overlook such \textit{geometric} outliers, focusing only on structural and attribute-level anomalies. To this end, we propose \modelname\ --- a mixed-curvature graph autoencoder that introduces the notion of curvature-based geometric anomalies. \modelname\ introduces two parallel pipelines for enhanced anomaly interpretability: (1) \textit{Curvature-equivariant geometry reconstruction}, which focuses exclusively on reconstructing the edge curvatures using a mixed-curvature, Riemannian encoder and Gaussian kernel-based decoder; and (2) \textit{Curvature-invariant structure and attribute reconstruction}, which decouples structural and attribute anomalies from geometric irregularities by regularizing graph curvature under discrete Ollivier-Ricci flow, thereby isolating the non-geometric anomalies. By leveraging curvature, \modelname\ refines the existing anomaly classifications and identifies new curvature-driven anomalies. Extensive experimentation over 10 real-world datasets (both homophilic and heterophilic) demonstrates an improvement of up to {\bf  6.5\%} over state-of-the-art GAD methods.
\end{abstract}
% Reconstruction-based approaches for graph anomaly detection (GAD) typically focus on reconstructing the adjacency and feature matrices, highlighting structural and attribute-level anomalies as substructures with larger reconstruction losses. However, these methods often fail to capture more profound \textit{geometric} anomalies, leaving a critical gap in identifying these subtle, yet impactful anomalies. 
%leading to a discrete space of uniform curvature. This enables the curvature-invariant reconstruction of the adjacency and feature matrices, isolating structural and attribute-level anomalies. 
% \modelname\ advances the detection of anomalies by refining the classification of existing anomalies using curvature as a heuristic, and by identifying previously unrecognized anomalies arising from curvature irregularities. 

\vspace{-3mm}
\section{Introduction}

Detecting anomalies in graph-structured data is a pivotal task across diverse fields, including social networks \cite{hassanzadeh2012analyzing}, cybersecurity \cite{wang2022wrongdoing}, transportation systems \cite{hu2020graph}, and biological networks \cite{singh2017improved}. Traditional methods primarily focus on identifying anomalies through structural irregularities or attribute deviations \cite{ding2019deep, fan2020anomalydae}, such as abnormal connections or unusual feature values. However, these approaches often overlook the underlying geometric properties of graphs, particularly the graph \textit{curvature}, which encapsulates essential information about the global and local topology of the graph. Recent advances in graph representation learning and Riemannian geometry have delved deeply into constant \textit{curvature} spaces \cite{bachmann2020constant} --- namely hyperbolic, spherical, and Euclidean geometries, characterized by negative, positive, and zero curvature, respectively --- to learn distortion-free graph representations. Each of these spaces offers beneficial inductive biases for specific structures. For instance, hyperbolic space is ideal for hierarchical, tree-like graphs \citep{chami2019hyperbolic}, whereas spherical geometry is optimal for representing cyclic graphs \citep{gu2019learning}. However, current graph anomaly detection (GAD) methods fail to leverage such insights, resulting in several limitations:
\vspace{-3mm}
\begin{enumerate}[leftmargin=7.5mm]
    \item [(\texttt{L1})] \textbf{Inadequate Representation of Complex Topologies}. Existing methods assume that graphs can be effectively represented in Euclidean space. However, real-world graphs often exhibit complex topologies with intrinsic curvature variations \cite{gu2019learning} that cannot be captured by a single geometric space.
    \vspace{-2mm}
    \item [(\texttt{L2})] \textbf{Neglect of Geometric Anomalies}. Current approaches largely ignore \textit{geometric anomalies} manifested through task-specific irregularities in the curvature of the graph, missing critical insights into the inherent geometry of the graph \cite{chatterjee2021detecting}.
    \vspace{-6mm}
    \item [(\texttt{L3})] \textbf{Homophily Trap}. Several methods predominantly operate under the homophily assumption (i.e., low-pass filters) and fail in heterophilic graphs where connected nodes tend to be dissimilar, missing out on anomalies that arise in such settings \cite{he2023ada}. 
\end{enumerate}
\vspace{-3mm}
\textit{Geometric anomalies}, as we define them, are irregularities in graph structure revealed through deviations in task-specific curvature patterns. Depending on the application, different curvature regimes --- negative, positive, or zero curvature --- may indicate anomalies. These anomalies occur when the curvature at certain nodes or edges significantly deviates from expected patterns, signaling unusual structural properties or interactions not detectable through traditional methods. There are numerous real-world instances where curvature can potentially serve as a critical heuristic for detecting anomalies in graph-structured data, e.g. \textbf{(a)} \textit{{{Fake News Propagation}}}: Hierarchical cascade-like diffusion patterns in social networks, often the result of fake news dissemination by malicious users (anomalies), exhibit high negative curvature due to rapid branching and sparse connections \citep{ducci2020cascade, xu2021casflow}.
\textbf{(b)} \textit{{Biological Networks}}: Bottleneck proteins in protein-protein interaction (PPI) networks, acting as critical hubs between functional modules, signal disruptions that can affect biological processes, and have positive curvature \citep{topping2021understanding}.

We propose \modelname, a novel framework for detecting graph anomalies by integrating geometric insights through a mixed-curvature lens. Intuitively, \modelname\ addresses the limitations of current GAD methods in two key ways: \textbf{(1)} It refines the classification of pre-existing anomalies by incorporating curvature information, improving the detection of structural or attribute-level anomalies by leveraging geometric insights; and \textbf{(2)} It uncovers previously undetected anomalies driven by curvature irregularities --- anomalies that may not be labeled in the original dataset but emerge through curvature deviations. The core idea behind \modelname\ is the decomposition of graph anomalies into two parallel pipelines described below.

\textbf{(a) Curvature-equivariant Geometry Reconstruction}. Detects geometric anomalies by learning representations in mixed-curvature spaces and reconstructing the graph’s curvature matrix (\underline{addresses limitation \texttt{L2}}). The encoder employs a mixed-curvature Chebyshev polynomials-based filter bank, which encodes graph signals into representations that adapt to the curvature of the underlying graph topology (\underline{addresses \texttt{L1}}). This includes multiple low-pass and high-pass filters operating in a product manifold, ensuring that we capture signals from multiple bands in the eigenspectrum (\underline{addresses \texttt{L3}}). Using the node embeddings, the decoder applies a Gaussian kernel-based approximation to reconstruct the edge curvatures. Geometric anomalies manifest as irregular curvature values, typically associated with a relatively larger reconstruction loss, providing insights into anomalies such as bottlenecks and hubs within the graph.

\textbf{(b) Curvature-invariant Structure and Attribute Reconstruction}. Reconstructs the adjacency and feature matrices, ensuring that the process remains invariant to the curvature of the graph. To achieve this, the input graph is first regularized by deforming it under the discrete Ollivier-Ricci flow, which standardizes the curvature of the graph, converging to a uniform value. This allows the subsequent structure and attribute reconstructions to focus solely on non-geometric anomalies. The encoder operates on the regularized graph, while the decoder reconstructs the adjacency and feature matrices. Decoupling geometric irregularities from the non-geometric ones, ensures a dedicated focus on the former (\underline{addresses \texttt{L2}}). This unified framework improves the ability to detect a broader spectrum of graph anomalies. Our key contributions can be summarized as follows.
\vspace{-2mm}
\begin{enumerate}[leftmargin=7mm]
    \item [(\texttt{C1})]{\bf Novelty:} To the best of our knowledge, this is the \underline{\textbf{first}} work to study curvature-based anomalies and approach GAD from a mixed-curvature perspective.
    \vspace{-1mm}
    \item [(\texttt{C2})]{\bf Interpretability:} Offers interpretable detection by disentangling curvature-induced anomalies from structural and attribute-level irregularities.
    \vspace{-1mm}
    \item [(\texttt{C3})]{\bf Universality:} \modelname\ performs well in detecting geometric, structural, and attribute-based anomalies across heterophilic and homophilic networks.
    \vspace{-1mm}
    \item [(\texttt{C4})]{\bf Effectiveness:} Experimentation with 10 real-world datasets for node-level GAD shows that \modelname\ achieves up to {\bf  6.5\%} \textbf{gain} over SOTA methods. 
\end{enumerate}

% By combining the outputs of these two pipelines, \modelname\ effectively detects new curvature-based irregularities while fusing the existing structural or attribute-level outliers with \textit{geometric} context. 


% In social media networks, users who act as malicious spreaders of fake news frequently trigger viral dissemination of content. This diffusion typically follows a hierarchical (\textit{cascade}), tree-like pattern, which is indicative of high negative curvature \cite{ducci2020cascade, xu2021casflow}. Such structures are characterized by rapid branching with sparse internal connections, making negative curvature (i.e. hyperbolic representations) a key geometric signal of anomalous, potentially harmful activity in these networks \cite{grover2022public}. 
% In protein-protein interaction (PPI) networks, bottleneck proteins that connect different functional modules typically exhibit high positive curvature along their connecting edges \cite{topping2021understanding, hariri2024graph}. These proteins act as critical hubs, ensuring network connectivity between otherwise independent submodules, which can significantly affect the biological processes if disrupted. Thus, by analyzing curvature, we can uncover critical network anomalies that traditional GAD methods often overlook.


    % \item [(\texttt{C2})]{\bf Comprehensability:} \modelname\ introduces a mixed-curvature autoencoder that leverages a curvature-equivariant geometry reconstruction and a curvature-invariant structure and attribute reconstruction for detecting graph anomalies.
\newpage
\section{Previous Works}
\textbf{Graph Anomaly Detection}. GAD aims to identify abnormal patterns or instances in graph data. Traditional node-level GAD methods focus primarily on detecting (a) \textit{structural} or (b) \textit{attribute-based} anomalies within a graph. Recent advances have introduced reconstruction-based approaches for GAD. These methods employ autoencoders to reconstruct graph structures (adjacency matrices) and node attributes, characterizing anomalies with a higher reconstruction error as anomalous nodes or substructures are relatively difficult to reconstruct. For example, DOMINANT \citep{ding2019deep} and AnomalyDAE \cite{fan2020anomalydae} use a dual discriminative mechanism to simultaneously detect structural and attribute anomalies by minimizing reconstruction loss in the adjacency and feature matrices. 

\textbf{Riemannian Graph Neural Networks}. Non-Euclidean manifolds, particularly hyperbolic \citep{sala2018representation} and spherical \citep{liu2017sphereface} geometries, have proven effective for learning distortion-minimal graph representations. 
% Hyperbolic spaces excel at capturing hierarchical, tree-like graphs, while spherical spaces are suited for cyclic structures. 
Two main approaches dominate this domain: (a) \textit{Single Manifold GNNs}: Models like HGAT \citep{zhang2021hyperbolic} and HGCN \citep{chami2019hyperbolic} achieve state-of-the-art performance on hierarchical graphs by embedding them in hyperbolic spaces. (b) \textit{Mixed-Curvature GNNs}: Recognizing that single-geometry manifolds fall short in representing complex, real-world topologies, mixed-curvature GNNs embed graphs in product manifolds combining spherical, hyperbolic, and Euclidean components. Pioneered by \cite{gu2019learning}, this idea has been extended by models like $\kappa$-GCN \citep{bachmann2020constant}, which uses the $\kappa$-stereographic model, and Q-GCN \citep{xiong2022pseudo}, which operates on pseudo-Riemannian manifolds. Despite these efforts, GAD methods overlook geometric information (curvature), spectral properties (e.g., heterophily), and fail to leverage Riemannian embeddings, limiting their ability to detect more nuanced anomalies that arise in complex graph structures.


\section{Preliminaries}
\textbf{Riemannian Geometry}\label{prelim:riemannian}. A smooth \emph{manifold} $\mathcal{M}$ generalizes surfaces to higher dimensions. At each point $\mathbf{x} \in \mathcal{M}$, the \emph{tangent space} $\mathcal{T}_\mathbf{x}\mathcal{M}$ is locally Euclidean. The \emph{Riemannian metric} $g_\mathbf{x}(\cdot, \cdot): \mathcal{T}_\mathbf{x}\mathcal{M} \times \mathcal{T}_\mathbf{x}\mathcal{M} \to \mathbb{R}$ equips $\mathcal{T}_\mathbf{x}\mathcal{M}$ with an inner product that enables the definition of distances and angles, forming a \emph{Riemannian manifold} \citep{do1992riemannian}. The \emph{exponential map} $\mathbf{exp}_\mathbf{x}(\mathbf{v}): \mathcal{T}_\mathbf{x}\mathcal{M} \to \mathcal{M}$ maps tangent vectors to the manifold, while the \emph{logarithmic map} $\mathbf{log}_\mathbf{x}(\mathbf{y}): \mathcal{M} \to \mathcal{T}_\mathbf{x}\mathcal{M}$ maps points back to the tangent space. The curvature ($\kappa$) at each point describes the geometry, with three common types: positively curved \textit{spherical} ($\mathbb{S}$) ($\kappa > 0$), negatively curved \textit{hyperbolic} ($\mathbb{H}$) ($\kappa < 0$), and flat \textit{Euclidean} ($\mathbb{E}$) ($\kappa = 0$). 

\textbf{Product Manifolds}.
A product manifold \citep{gu2019learning} $\mathbb{P}$ is defined as the Cartesian product of $\mathcal{P}$ constant-curvature manifolds, i.e. $\mathbb{P} = \times_{p=1}^{\mathcal{P}} \mathcal{M}_{p}^{\kappa_p, d_p}$, where $\mathcal{M}_p \in \{\mathbb{E}, \mathbb{H}, \mathbb{S}\}$ represents a component manifold with curvature $\kappa_p$ and dimension $d_p$. The total dimension of the product manifold is the sum of the component dimensions. The above decomposition of $\mathbb{P}$ is called its \textit{signature} (Appendix \ref{app:product}).

\textbf{$\kappa-$Stereographic Model}. In this work, we adopt the $\kappa$-Stereographic Model \citep{bachmann2020constant} to define Riemannian algebraic operations across both positively and negatively curved spaces within a unified framework. This model eliminates the need for separate mathematical formulations for different geometries. 
% Specifically,  
In particular,  $\mathcal M^{\kappa, d}$ is the stereographic sphere model for spherical manifold ($\kappa > 0$),
while it is the Poincar\'e ball model \citep{ungar2001hyperbolic} for hyperbolic manifold ($\kappa < 0$) (See Appendix \ref{app:kappa}).
% Appendix \ref{app:prelim} for more details.
% \vspace{-3mm}
% TODO: Add table to appendix

\textbf{Discrete Laplace-Beltrami (LB) Operator}. \label{prelim:beltrami}
The Laplace– Beltrami operator \cite{urakawa1993geometry} is a generalization of the Laplace operator to functions defined on Riemannian manifolds. We define the discrete Laplace-Beltrami operator, $\mathbf{L}_{\mathbb{P}}$, for a graph discretized over the product manifold $\mathbb{P}$, based on the \textit{cotangent} discretization scheme \cite{belkin2008discrete, crane2019n}. For two connected vertices $v_i$ and $v_j$, the off-diagonal element of $\mathbf{L}_{\mathbb{P}}$ is:
$\mathbf{L}_{\mathbb{P}, ij} = - \frac{(\cot \theta_{ij} + \cot \phi_{ij})}{2A_i}$
where $\theta_{ij}$ and $\phi_{ij}$ represent the angles opposite to the edge $(i, j)$ in the adjacent triangles, and $A_i$ is the Voronoi area of vertex $v_i$ (or Heron’s area in the case of obtuse triangles). The diagonal element is computed as: $\mathbf{L}_{\mathbb{P}, ii} = -\sum_{j \neq i} \mathbf{L}_{\mathbb{P}, ij}$
This operator highlights the geometric properties by incorporating the manifold curvature.


\textbf{Ollivier-Ricci Curvature}. In graphs, the lack of an inherent manifold structure necessitates the use of discrete curvature analogs, such as Ollivier-Ricci curvature (ORC) \citep{ollivier2007ricci}, which extends the concept of continous manifold curvature \citep{tanno1988ricci} to networks. ORC is defined as a transportation-based curvature along an edge, where the curvature between neighborhoods of two nodes is measured via the Wasserstein-1 distance \citep{piccoli2016properties}. For an unweighted graph, each node $x$ is assigned a neighborhood measure $m_{x}^{\delta}(z) := \frac{1-\delta}{|\mathcal{N}(x)|}$ for $z \in \mathcal{N}(x)$, and $m_{x}^{\delta}(x) = \delta$. ORC for an edge $(x, y)$ is then calculated as: $\widetilde{\kappa}_{xy} := 1 - \frac{\mathbf{W}_1 (m^{\delta}_{x}, m^{\delta}_{y})}{d_\mathcal{G}(x,y)}$. We approximate ORC in linear time using combinitorial bounds \cite{jost2014ollivier}. See Appendix \ref{app:orc} for details on computational considerations. We denote the continuous manifold curvature using $\kappa$ and the discrete Ollivier-Ricci curvature using $\widetilde{\kappa}$.

% \begin{figure}[h]
% \centering
%   \includegraphics[width=0.3\textwidth]{orc.pdf}
%   \vspace{-5mm}
%   \caption{Computing Ollivier-Ricci curvature $\widetilde{\kappa}(x,y)$}
%   \vspace{-8mm}
%   \label{fig:orc}
% \end{figure}

  % \vspace{-0.5mm}
  % \begin{equation} \mathbf{W}_1(m_{x}^{\delta}, m_{y}^{\delta}) = \inf \limits _{M}\sum _{\forall x,y\in \mathcal{V}}\mathcal{D}_{\mathcal{G}}(x,y)M(x,y) \end{equation}
% Here $M(x,y)$ is the amount of mass moved from $x$ to $y$ along the shortest path (of length $\mathcal{D}_{\mathcal{G}}$) and we would like to take the best possible assignment (transport plan) that minimizes the total transport distance. 
% ORC for nodes is the average curvature of its adjacent edges.
  % i.e. $\widetilde{\kappa}(x) = \frac{1}{|\mathcal{N}(x)|}\sum_{z\in \mathcal{N}(x)} \widetilde{\kappa}(x, z)$.
  

\textbf{Ollivier-Ricci Flow}. The Ricci flow, introduced by Hamilton \cite{chow2023hamilton}, is a process that smooths the curvature of a manifold by deforming its metric over time. In the graph domain, this concept is adapted to the discrete Ollivier-Ricci flow. In each iteration of this evolving process, Ricci flow generates a time-dependent family of weighted graphs $(\mathcal{V}, \mathcal{E}, \mathbf{w}^{(t)})$ such that the weight $\mathbf{w}_{xy}^{(t)}$ on edge $xy$ changes proportionally to the ORC of the edge $xy$ at time $t$, $\widetilde{\kappa}^{(t)}_{xy}$. Ollivier \cite{ollivier2009ricci} defined the Ricci flow for continuous time as $\frac{d}{dt}\mathbf{w}_{xy}^{(t)} = -\widetilde{\kappa}^{(t)}_{xy} \cdot \mathbf{w}_{xy}^{(t)}$.  Then \cite{ni2018network} proposed Ricci flow for discrete time $t$ (for step size $\epsilon$) as: $\mathbf{w}^{(t+1)}_{xy} = (1 - \epsilon \widetilde{\kappa}^{(t)}_{xy}){\mathbf{w}}^{(t)}_{xy}$. For an unweighted graph, $\mathbf{w}_{xy}^{(0)} = 1,\; \forall x, y \in \mathcal{V}$. Typically, ORC for all edges converges to zero curvature as $t \rightarrow \infty$.

\textbf{Setup.} We consider graphs $\mathcal{G} = (\mathcal{V}, \mathcal{E}, \mathbf{A})$, where $\mathcal{V}$ is a set of $|\mathcal{V}| = n$ vertices, $\mathcal{E}$ is a set of edges, and $\mathbf{A} \in \mathbb{R}^{n \times n}$ is the adjacency matrix. The nodes are associated with the node feature matrix $\mathbf{X} \in \mathbb{R}^{n \times d_{\mathcal{X}}}$ ($d_{\mathcal{X}}$ is the feature dimension). A graph signal $\mathbf{x} : \mathcal{V} \rightarrow \mathbb{R}$ may be regarded as a vector $\mathbf{x} \in \mathbb{R}^n$ where $x_i$ is the value of $\mathbf{x}$ at the $i^{th}$ node. The Laplace-Beltrami operator $\mathbf{L}_{\mathbb{P}}$ (Section \ref{prelim:beltrami}) extends the notion of the traditional graph Laplacian to mixed-curvature spaces by incorporating geometric clues. As a real, symmetric, and positive semidefinite matrix, $\mathbf{L}_{\mathbb{P}}$ admits a complete set of orthonormal eigenvectors $\mathbf{U}_{\mathbb{P}} = \left[\{\mathbf{u}_l\}^{n - 1}_{l=0}\right] \in \mathbb{R}^{n \times n}$, and their associated ordered real nonnegative eigenvalues $\left[\{\lambda_l\}^{n - 1}_{l=0}\right] \in \mathbb{R}^{n}$, identified as the \textit{frequencies} of the graph. Similar to the graph Laplacian, the Laplace-Beltrami matrix can be diagonalized as $\mathbf{L}_{\mathbb{P}} = \mathbf{U}_{\mathbb{P}}\mathbf{\Lambda}_{\mathbb{P}} \mathbf{U}_{\mathbb{P}}^{\top}$ where $\mathbf{\Lambda}_{\mathbb{P}} = \mathbf{diag}(\left[\{\lambda_l\}^{n - 1}_{l=0}\right]) \in \mathbb{R}^{n \times n}$. 

\vspace{-2mm}
\section{Proposed Approach: \modelname}
\vspace{-1.5mm}
In this section, we provide an in-depth exploration of the \modelname\ architecture.   We first introduce the curvature-equivariant pipeline \underline{(Section \ref{sec:equi})}, which reconstructs the curvature matrix to detect geometric anomalies. This involves a mixed-curvature encoder (common to both pipelines) equipped with spectral graph filters \underline{(Section \ref{sec:filterbank})}, followed by a Gaussian kernel-based decoder \underline{(Section \ref{sec:equi_dec})} that predicts curvature values. Next, we extend our framework to curvature-invariant anomaly detection \underline{(Section \ref{sec:inv})} by deforming the graph under Ollivier-Ricci flow \underline{(Section \ref{sec:inv_orc})}, thereby regularizing curvature distortions. While the encoder remains identical, we replace the product manifold with a Euclidean manifold to ensure that structural and attribute anomalies are reconstructed independently of curvature. 


\begin{figure}[!t]
\centering
  \includegraphics[width=0.97\linewidth]{curvgad.pdf}
  \caption{\textbf{Architecture of \modelname.} The proposed model employs two synergistic pipelines for anomaly detection: \textbf{(1)} \textit{Curvature-Equivariant Reconstruction} -- embedding the input graph into a mixed-curvature product manifold using a spectral Chebyshev filter bank, with a Gaussian kernel-based decoder reconstructing the curvature matrix to capture curvature irregularities; \textbf{(2)} \textit{Curvature-Invariant Reconstruction} -- regularizing the graph under Ollivier-Ricci flow to uniformize edge curvatures, followed by an Euclidean manifold-based encoder to learn representations for the decoupled reconstruction of adjacency and feature matrices. 
  % \modelname\ integrates both the perspectives through mixed-curvature spectral filtering, ensuring both interpretability and accuracy.
  } \vspace{-5mm}
\end{figure}\label{fig:curvgad}


% \vspace{-5mm}
\subsection{Curvature-equivariant Reconstruction}\label{sec:equi}

\textbf{Product manifold Construction}. The product manifold $\mathbb{P}$ can have multiple hyperbolic or spherical components with distinct \textit{learnable} curvatures. This allows us to accommodate a broader spectrum of curvatures.
Consequently, the product manifold can be succinctly described as $ \mathbb{P}^{d_{\mathcal{P}}} = \times_{p=1}^{\mathcal{\mathcal{P}}} \mathcal{M}_{p}^{\kappa_{p}, d_{p}}  = (\times_{h=1}^{\mathcal{H}} \mathbb{H}_{h}^{\kappa_{h}, d_{h}}) \times (\times_{s=1}^{\mathcal{S}} \mathbb{S}_{s}^{\kappa_{s}, d_{s}}) \times \mathbb{E}^{d_{e}}$, with a total dimension of $d_{\mathcal{P}} = \sum_{h=1}^{\mathcal{H}} d_{h}+ \sum_{s=1}^{\mathcal{S}} d_{s} + d_{e} = \sum_{p=1}^{\mathcal{P}} d_{p}$.  
We determine the task-specific signature of $\mathbb{P}^{d_{\mathcal{P}}}$, by examining the distribution of the Ollivier-Ricci curvature within the graph and identifying the most significant curvature bands (see Appendix \ref{app:signature}). Next, we project the original Euclidean input node features onto the mixed-curvature manifold as
$\mathbf{X}' = \|_{p=1}^{\mathcal{P}}\mathbf{exp}^{\kappa_{p}}_{\mathbf{0}}(f_{\boldsymbol{\theta}}(\mathbf{X}))$, where
$f_{\boldsymbol{\theta}}(.): \mathbb{R}^{d_{\mathcal{X}}} \rightarrow \mathbb{R}^{d_{\mathcal{P}}}$ represents a neural network with parameter set $\{{\boldsymbol{\theta}\}}$ that generates the hidden state euclidean features of dimension $d_{\mathcal{P}}$. Here, $\mathbf{exp}^{\kappa_p}_{\mathbf{0}} : \mathbb{R}^{d_{\mathcal{P}}} \rightarrow \mathcal{M}^{\kappa_p, d_{p}}$ is the exponential map (Section \ref{prelim:riemannian}) to project the node features $\mathbf{X}$ to the $p^{th}$ manifold. The projected node features are utilized in the encoder to learn node representations, which are subsequently used to reconstruct the curvature. 

\subsubsection{Mixed-curvature Chebyshev Encoder} \label{sec:filterbank}

Once the input features are projected into the product manifold, we introduce a filterbank of Chebyshev approximation-based spectral filters. Let $\psi$ represent the graph filter operator, the filtering operation for a signal $\mathbf{x}$ is defined as:
\vspace{-3mm}
\begin{equation}
{\psi}(\mathbf{L}_{\mathbb{P}}) \mathbf{x} = {\psi}(\mathbf{U}_{\mathbb{P}} \mathbf{\Lambda}_{\mathbb{P}} \mathbf{U}_{\mathbb{P}}^\top) \mathbf{x} = \mathbf{U}_{\mathbb{P}} {\psi}(\mathbf{\Lambda}_{\mathbb{P}}) \mathbf{U}_{\mathbb{P}}^\top \mathbf{x}.
\end{equation}

$\blacksquare$ \textbf{Chebyshev Approximation.} Direct eigen- decomposition of the Laplace-Beltrami operator $\mathbf{L}_{\mathbb{P}}$ is computationally prohibitive, especially for large graphs. To address this we employ a Chebyshev polynomial approximation of the filters \textit{in the mixed-curvature space}. Recall that the Chebyshev polynomial $Z^{(f)}(x)$ of order $f$ may be computed by the following stable recurrence relation:
\begin{equation}\label{eq:cheb_euc}
    Z^{(f)}(x) = 2Z^{(f-1)}(x) - Z^{(f-2)}(x),
\end{equation}
with $Z^{(0)} =1$ and $Z^{(1)} = x$. In our framework, we generalize this recurrence to accommodate the geometry of the mixed-curvature manifold by replacing the standard operations with their curvature-aware counterparts. The filtering process on the $p^\text{th}$ manifold proceeds as follows:
\begin{align}\label{eq:proj}
\mathbf{Z}^{(0)}_{\mathcal{M}_p^{\kappa_p, d_p}} &= \mathbf{exp}^{\kappa_p}_{\mathbf{0}}(f_{\theta}(\mathbf{X})) \quad (\text{Feature mapping}) \\ \label{eq:z_1}
\mathbf{Z}^{(1)}_{\mathcal{M}_p^{\kappa_p, d_p}} &= \widetilde{\mathbf{L}}_{\mathbb{P}} \boxtimes_{\kappa_p} \mathbf{exp}^{\kappa_p}_{\mathbf{0}}(f_{\theta}(\mathbf{X}))\\  \label{eq:cheb_curv}
\mathbf{Z}^{(f)}_{\mathcal{M}_p^{\kappa_p, d_p}} &= \big(2 \otimes_{\kappa_p} \widetilde{\mathbf{L}}_{\mathbb{P}} \boxtimes_{\kappa_p} \mathbf{Z}^{(f-1)}_{\mathcal{M}_p^{\kappa_p, d_p}} \big) \circleddash_{\kappa_p} \mathbf{Z}^{(f-2)}_{\mathcal{M}_p^{\kappa_p, d_p}} \\ \label{eq:agg}
\mathbf{N}^{(\mathcal{F})}_{\mathcal{M}_p^{\kappa_p, d_p}} &= \bigoplus\limits_{f \in \{0, \mathcal{F}\}}^{\kappa_p} \phi_f \otimes_{\kappa_p} \mathbf{Z}^{(f)}_{\mathcal{M}_p^{\kappa_p, d_p}}
\end{align}
Observe how Equations \ref{eq:proj} to \ref{eq:cheb_curv} generalise the Chebyshev recurrence in Equation \ref{eq:cheb_euc} to the mixed-curvature setting, for the $p^{th}$ component manifold. Further, as detailed in a previous section, the Equation \ref{eq:proj} transforms the original Euclidean node features $\mathbf{X}$ to the product manifold. Finally, in Equation \ref{eq:agg} the filtered signals from different Chebyshev orders (till $\mathcal{F}$) are aggregated to form the final node representation on the manifold $p$. Here, $\oplus_{\kappa}$, $\circleddash_{\kappa}$, $\otimes_{\kappa}$ and $\boxtimes_{\kappa}$ denote \textit{mobius} addition, \textit{mobius} subtraction, $\kappa$-right-matrix-multiplication and $\kappa$-left-matrix-multiplication respectively (Appendix \ref{app:kappa}). These operations generalize vector algebra to $\kappa$- stereographic model. $\phi_l$ are the learnable weights.


$\blacksquare$ \textbf{Filter Bank.} To effectively capture spectral information across diverse graph frequencies, we adopt the above construction to get a filter bank of multiple spectral filters:
\begin{align*}
    \Omega_{\mathcal{M}_p^{\kappa_p, d_p}} = \left[\mathbf{N}^{\mathbf{I}}_{\mathcal{M}_p^{\kappa_p, d_p}}, \mathbf{N}^{(1)}_{\mathcal{M}_p^{\kappa_p, d_p}}, \dots, \mathbf{N}^{(\mathcal{F})}_{\mathcal{M}_p^{\kappa_p, d_p}}\right]
\end{align*} 
Here, $\mathbf{N}^{\mathbf{I}}$ is the unfiltered case, where we pass the identity matrix $\mathbf{I}$ instead of $\widetilde{\mathbf{L}}_{\mathbb{P}}$. This approach ensures that the filter bank captures both high-frequency (heterophilic) and low-frequency (homophilic) signals within the mixed-curvature manifold, enabling \modelname\ to generalize effectively across heterogeneous and homogeneous graphs.

$\blacksquare$ \textbf{Final Representations.} Filtered node representations are aggregated hierarchically to synthesize information across both spectral filters and manifold components. Unlike naive concatenation, this aggregation mechanism assigns learnable importance weights to both filters and manifolds:
\vspace{-1mm}
\begin{equation*}
\mathbf{n}_j = \Big\|_{m=0}^{\mathcal{M}} \beta_p \otimes_{\kappa_p} \Big(\bigoplus\limits_{f=0}^{\mathcal{F}} \alpha_f \otimes_{\kappa_p} \big( \mathbf{N}^{(f)}_{\mathcal{M}_p^{\kappa_p, d_p}} \big) \Big)_j
\end{equation*}
Here $\big\|$ is the concatenation operator and $\mathbf{n}_j \in \mathbb{P}^{d_{\mathcal{P}}}$ is final node representation for node $j$. $\beta_p$ and $\alpha_f$ are learnable weights which assert the relative importance of the $p^{th}$ component manifold embedding and $f^{th}$ filter. 


% \subsubsection{\textbf{Chebyshev Polynomial Approximation}}

% Directly computing the eigen-decomposition of the Laplace-Beltrami operator $\mathbf{L}_{\mathbb{P}}$ for spectral filtering is computationally expensive, especially for large graphs. To address this, we employ a Chebyshev polynomial approximation of the spectral filters in the mixed-curvature space, which allows efficient filtering without requiring explicit computation of eigenfunctions. Recall that the Chebyshev polynomial $T_k(x)$ of order $k$ may be computed by
% the stable recurrence relation $T_k(x) = 2x T_{k-1}(x) - T_{k-2}(x)$ with $T_0 =
% 1$ and $T_1 = x$. A filter can thus be parametrized as the
% truncated expansion $
% 	g_\theta(\Lambda) = \sum_{k=0}^{K-1} \theta_k T_k(\tilde{\Lambda})$
% of order $K-1$,
% where the parameter $\theta \in \R^K$ is a vector of Chebyshev coefficients and
% $T_k(\tilde{\Lambda}) \in \R^{n \times n}$ is the Chebyshev polynomial of order
% $k$ evaluated at $\tilde{\Lambda} = 2 \Lambda / \lambda_{max} - I_n$, a diagonal
% matrix of scaled eigenvalues that lie in $[-1,1]$.
% The filtering operation can then be written as $y = g_\theta(L) x
% = \sum_{k=0}^{K-1} \theta_k T_k(\tilde{L}) x$, where $T_k(\tilde{L}) \in \R^{n
% \times n}$ is the Chebyshev polynomial of order $k$ evaluated at the scaled
% Laplacian $\tilde{L} = 2 L / \lambda_{max} - I_n$. %Note that the spectrum of the
% %normalized Laplacian is bounded by $2$ \cite{book:Chung97Spectral}, such that
% %the scaling can simply be $\tilde{L} = L - I_n$. 
% Denoting $\bar{x}_k =
% T_k(\tilde{L})x \in \R^n$, we can use the recurrence relation to compute
% $\bar{x}_k = 2\tilde{L} \bar{x}_{k-1} - \bar{x}_{k-2}$ with $\bar{x}_0 = x$ and
% $\bar{x}_1 = \tilde{L}x$. The entire filtering operation
% 	$y = g_\theta(L) x = [\bar{x}_0, \ldots, \bar{x}_{K-1}] \theta$
% then costs $\bO(K|\E|)$ operations.


% The combination of Chebyshev polynomials with the mixed-curvature Laplace-Beltrami operator enables multi-scale spectral filtering over different curvature regimes in the product manifold, capturing both low-frequency (structural) and high-frequency (geometric) components.


\subsubsection{Decoder for Curvature Reconstruction} \label{sec:equi_dec}

The decoder aims to reconstruct the Ollivier-Ricci curvature $\mathbf{C}_{xy}$ for nodes $x$ and $y$, by leveraging their latent embeddings $\mathbf{n}_x, \mathbf{n}_y \in \mathbb{P}^{d_\mathcal{M}}$, learned through the mixed-curvature encoder. We propose using a \textit{Gaussian kernel} defined on the manifold distance between the node embeddings.
% \vspace{-2mm}
\begin{definition}[\textit{Curvature Decoder}] \label{def:curv}
Let $\mathbf{n}_x, \mathbf{n}_y \in \mathbb{P}^{d_\mathcal{M}}$ be the latent embeddings of nodes $x$ and $y$. The predicted curvature $\widetilde{\mathbf{C}}_{xy}$ is defined as: $\widetilde{\mathbf{C}}_{xy} = 2 \cdot \texttt{sigmoid}\left(1 - \mathcal{K}(\mathbf{n}_x, \mathbf{n}_y)\right) - 1$, where $\mathcal{K}(\mathbf{n}_x, \mathbf{n}_y)$ is a Gaussian kernel: $\mathcal{K}(\mathbf{n}_x, \mathbf{n}_y) = \exp\left(-\gamma \frac{\mathcal{D}_{\mathcal{M}}(\mathbf{n}_x, \mathbf{n}_y)^2}{\tau^2}\right)$, and $\mathcal{D}_{\mathcal{M}}(\mathbf{n}_x, \mathbf{n}_y) = \sqrt{\sum_{m=1}^\mathcal{M} \mathcal{D}_{\mathcal{M}_p}(\mathbf{n}_x, \mathbf{n}_y)^2}$ aggregates geodesic distances $\mathcal{D}_{\mathcal{M}_p}$ over manifold components. Here, $\gamma$ is a fixed kernel width and $\tau$ is a sensitivity parameter.
\end{definition}
% \vspace{-2mm}
See Appendix \ref{app:dec_curv} for more intuition. The decoder minimizes the loss of the Frobenius norm: $\mathcal{L}_{\mathbf{C}} = \|\widetilde{\mathbf{C}} - \mathbf{C}\|_F^2$, where $\widetilde{\mathbf{C}}$ and $\mathbf{C}$ denote the predicted and original curvature matrices, respectively. This objective ensures accurate curvature reconstruction while adapting to geodesic distances.

% augmented by a learnable parameter \(\sigma_p\) that controls the scale of the manifold distance in each component of the product manifold \(\mathbb{P}^{d_{\mathcal{}}}\). 


% This design allows the decoder to adaptively reconstruct the curvature values by capturing the geometric relationships between the nodes. Let \(\mathbf{n}_x\) and \(\mathbf{n}_y\) represent the latent embeddings of nodes \(i\) and \(j\), respectively. The manifold distance between these embeddings, \(\mathcal{D}_{\mathcal{M}}(\mathbf{n}_x, \mathbf{n}_y)\), is computed based on the product manifold \(\mathbb{P}^{d_{\mathcal{P}}}\).
% The distance between the embeddings is defined as: $\mathcal{D}_{\mathcal{M}}(\mathbf{n}_x, \mathbf{n}_y) = \sqrt{ \sum_{p=1}^{\mathcal{P}} \mathcal{D}_{\mathcal{M}_p}(\mathbf{n}_x, \mathbf{n}_y)^2}$, where: \(d_{\mathcal{M}_p}(\mathbf{n}_x, \mathbf{n}_y)\) is the geodesic distance between \(\mathbf{n}_x\) and \(\mathbf{n}_y\) in the \(p^{th}\) manifold component. The distance \(d_{\mathcal{P}}(\mathbf{n}_x, \mathbf{n}_y)\) incorporates the geodesic distances in each of the manifold components, allowing the decoder to adaptively weigh different manifold geometries during curvature prediction.

% \textbf{Gaussian Kernel for Curvature Prediction}. We use a Gaussian kernel based on the manifold distance \(d_{\mathcal{P}}(\mathbf{n}_x, \mathbf{n}_y)\) to predict the curvature between nodes \(x\) and \(y\). The kernel is formulated as: $\mathcal{K}(\mathbf{n}_x, \mathbf{n}_y) = \text{exp}{\left(-\gamma \frac{\mathcal{D}_{\mathcal{M}}(\mathbf{n}_x, \mathbf{n}_y)^2}{\sigma^2}\right)}$, where \(\gamma\) is a hyperparameter that controls the width of the Gaussian kernel, and \(\sigma\) is the learnable parameter introduced earlier that scales the distances. The predicted curvature $\widetilde{\mathbf{C}}_{xy}$ between nodes x and y is given by: $\widetilde{\mathbf{C}}_{xy} = 2 \cdot \texttt{sigmoid}\left(1 - \mathcal{K}(\mathbf{n}_x, \mathbf{n}_y)\right) - 1$. The learnable parameter \(\sigma\) allow the model to adjust the sensitivity of the curvature predictions to geodesic distances. Here, the sigmoid function $\texttt{sigmoid}(z) = \frac{1}{1 + e^{-z}}$ maps the kernel output to the range $[0, 1]$, and the subsequent transformation rescales the curvature prediction to the range $[-1, 1]$. This formulation ensures that as the manifold distance between node embeddings increases, the predicted curvature decreases, reflecting negative curvature properties typical in hyperbolic spaces, while smaller distances correspond to positive curvatures, capturing spherical or Euclidean behavior. The encoder is trained by minimizing the reconstruction loss between the predicted curvature values \(\widetilde{\mathbf{C}}_{xy}\) and the ground truth curvature values \(\mathbf{C}_{xy}\). The reconstruction loss is defined as: $\mathcal{L}_{\mathbf{C}}  = \left\|\widetilde{\mathbf{C}} - \mathbf{C} \right\|_{Frob}^2$, where \(\mathcal{E}\) is the set of edges in the graph. This loss encourages the model to accurately predict the curvature values by learning the latent embeddings \(\mathbf{n}_i\) and \(\mathbf{n}_j\), and the learnable parameter \(\sigma\).

\subsection{Curvature-invariant Reconstruction} \label{sec:inv}

This pipeline focuses on reconstructing the adjacency matrix $\mathbf{A}$ and the feature matrix $\mathbf{X}$ independently of the graph’s underlying geometry (curvature). To achieve this, the graph is first deformed under the Ollivier-Ricci flow, followed by curvature-invariant encoding and decoding.

% In this section, we describe the pipeline for reconstructing the adjacency matrix \(\mathbf{A}\) and the feature matrix \(\mathbf{X}\) in a curvature-invariant setting. The goal is to ensure that structural and attribute information is reconstructed independently of the underlying curvature geometry of the graph. To achieve this, we first apply the Ollivier-Ricci flow to the input graph, transforming the graph to have constant curvature. Then, we design encoders and decoders to reconstruct \(\mathbf{A}\) and \(\mathbf{X}\) on this transformed space.

\subsubsection{Ricci Flow and Curvature Regularization} \label{sec:inv_orc} Ollivier-Ricci flow is applied to the original graph to regularize edge curvatures by iteratively updating edge weights based on their curvature values. This process, outlined in Algorithm \ref{algo:flow}, transforms the graph into a constant-curvature space, thereby neutralizing curvature-induced distortions. Refer to Figure \ref{fig:orc} for better intuition. 

\begin{figure}[h]
\centering
  \includegraphics[width=0.47\textwidth]{ricci_flow.pdf}
  % \vspace{-5mm}
  \caption{\textbf{Intuition on Ollivier-Ricci flow.} Curvature regularization of Karate Club graph \cite{rozemberczki2020karate} under Ollivier-Ricci flow. Observe how the flow increases (\textit{decreases}) the weight of negatively (\textit{positively}) curved edges. 
  % It acts as a clustering algorithm -- pushing (or pulling) away the nodes on negatively (or positively) curved edges. 
  {\color{red}{Red}} nodes indicate curvature-based outliers near edges with extreme curvatures.}
  % \vspace{-5mm}
  \label{fig:orc}
\end{figure}


\begin{algorithm}[H] \label{algo:flow}
\caption{Discrete Ollivier-Ricci Flow}
\label{alg:ricci-flow}
\begin{algorithmic}[1]
\Require An undirected graph \(\mathcal{G} = (\mathcal{V}, \mathcal{E})\), a small threshold \(\Delta > 0\), learning rate \(\epsilon > 0\)
\Ensure Edge weights of \(\mathcal{G}\) as Ricci flow metrics
\State Initialize edge weights \(\mathbf{w}^{(0)}_{xy} = 1\) for all \((x, y) \in \mathcal{E}\)
\Repeat
    \State Normalize edge weights: $\mathbf{w}_{xy}^{(t)} \gets  \frac{|\mathcal{E}| \cdot \mathbf{w}_{xy}^{(t)}}{\sum_{(x,y) \in \mathcal{E}} \mathbf{w}_{xy}^{(t)}} $
    
    \State Compute ORC: $\widetilde{\kappa}^{(t)}_{xy} = 1 - \frac{\mathbf{W}_1(m^{\delta}_x, m^{\delta}_y)}{\mathcal{D}_{\mathcal{G}}^{(t)}(x, y)}$
    
    \State Update edge weight under Ricci flow:
    \[\mathbf{w}^{(t+1)}_{xy} \gets \mathbf{w}^{(t)}_{xy} - \epsilon \cdot \widetilde{\kappa}^{(t)}_{xy} \cdot \mathbf{w}^{(t)}_{xy}\]
    
    \State Check convergence: \(\left| \widetilde{\kappa}^{(t+1)}_{xy} - \widetilde{\kappa}^{(t)}_{xy} \right| < \Delta\)
\Until{convergence condition is met for all edges}
\end{algorithmic}
\end{algorithm}

\subsubsection{Uniform-curvature Reconstruction}

Under Ollivier-Ricci flow, the edge curvatures of the \textit{regularized} graph approach near zero, allowing reconstructions to operate within a Euclidean manifold. We reuse the Chebyshev filterbank encoder (Section \ref{sec:filterbank}) to map the nodes in the curvature-regularized graph (Section \ref{sec:inv_orc}) to latent Euclidean embeddings, ensuring curvature invariance in the learned representations. Unlike the mixed-curvature encoder (Section \ref{sec:equi}), this encoder simplifies the geometric complexity by restricting operations to a Euclidean space.

% After applying discrete Ricci flow to the graph, the edge curvature typically approaches near-zero values, either slightly positive or negative, as noted in \citep{jost2014ollivier}. This effectively regularizes the graph's geometry. This adjustment permits us to work within a Euclidean framework for subsequent reconstruction tasks by neutralizing curvature effects. In this pipeline model, an encoder akin to the one in Section \ref{sec:filterbank} is employed, but rather than using a mixed-curvature space, we learn all intermediate representations on a Euclidean manifold. An encoder is tasked with mapping the nodes of the constant-curvature graph to a Euclidean latent space. The representations learned by the encoder are curvature-invariant, meaning that they are not affected by the underlying curvature of the original graph. 

$\blacksquare$ \textbf{Decoders}. We employ separate decoders for adjacency and feature reconstruction tasks. These decoders ensure that structural and attribute information is effectively recovered without being influenced by curvature irregularities.

\begin{definition}[\textit{Adjacency Decoder}]\label{def:adj_decoder}
The predicted adjacency value $\widetilde{\mathbf{A}}_{xy}$ for nodes $x, y$ is modeled as the likelihood of an edge existing between them: $p(\mathbf{A}_{xy} = 1 \mid \mathbf{n}_x, \mathbf{n}_y) = \sigma\left( \mathbf{n}_x^{\top} \mathbf{n}_y \right)$, where $\mathbf{n}_x, \mathbf{n}_y \in \mathbb{R}^d$ are Euclidean latent embeddings, and $\sigma(\cdot)$ is the sigmoid activation.
\end{definition}

\begin{definition}[\textit{Attribute Decoder}] \label{def:attr_decoder}
The reconstructed features for a node x are given by: $\widetilde{\mathbf{X}}_x = f_{\text{dec-X}}(\mathbf{n}_x)$, where $f_{\text{dec-X}}$ is a multi-layer perceptron (MLP) mapping latent embeddings $\mathbf{n}_x \in \mathbb{R}^d$ to feature space.
\end{definition}

% We utilize distinct decoders for structural (adjacency matrix) and attribute reconstruction (feature matrix). The adjacency matrix $\widetilde{\mathbf{A}}$ is reconstructed by estimating the likelihood of an edge existing between nodes $x$ and $y$, using their latent embeddings $\mathbf{n}_x$ and $\mathbf{n}_y$. This is represented by: $p(\mathbf{A}_{xy} = 1 \mid \mathbf{n}_x, \mathbf{n}_y) = \sigma\left( \mathbf{n}_x^{\top} \mathbf{n}_y \right)$, where $\sigma(\cdot)$ denotes the sigmoid activation. This methodology ensures that the predicted graph structure is solely based on node embeddings, unaffected by curvature-based distortions of the initial graph. The feature matrix $\widetilde{\mathbf{X}}$ is reconstructed by decoding the latent embeddings for each node. For any node $x$, the reconstructed features are determined by: $\widetilde{\mathbf{X}}_i = f_{\text{dec-X}}(\mathbf{n}_x)$, where $f_{\text{dec-X}}$ is a multi-layer perceptron tailored to extract attribute information from the latent space. This allows us to reconstruct node features effectively while preserving curvature invariance.

% The adjacency decoder predicts the graph structure based on node embeddings alone, while the attribute decoder reconstructs node features effectively by leveraging MLP-based projections. 
The decoders are trained using reconstruction losses: $\mathcal{L}_{\mathbf{A}} = \|\widetilde{\mathbf{A}} - \mathbf{A}\|_F^2$ for adjacency and $\mathcal{L}_{\mathbf{X}} = \|\widetilde{\mathbf{X}} - \mathbf{X}\|_F^2$ for features.

% By transforming the graph into a constant-curvature space, this pipeline decouples the structural and attribute reconstructions from the curvature (geometry) of the graph. The Ricci flow ensures that the curvature is regularized, and the subsequent reconstruction operates in a Euclidean variant of the \modelname\ framework. This allows for the accurate detection of structural anomalies (in $\mathbf{A}$) and attribute anomalies (in $\mathbf{X}$), providing a robust method for anomaly detection that is free from geometric distortions.

\subsection{Objective Function}

The objective function balances curvature reconstruction, structural and attribute reconstruction, and a supervised classification loss to detect both geometric and non-geometric anomalies. The total loss is:
\begin{equation*}
\mathcal{L}_{\text{total}} = \lambda_{\text{cls}} \mathcal{L}_{\text{cls}} + (1 - \lambda_{\text{cls}}) \cdot \left( \lambda_{\mathbf{C}} \mathcal{L}_{\mathbf{C}} + \lambda_{\mathbf{A}} \mathcal{L}_{\mathbf{A}} + \lambda_{\mathbf{X}} \mathcal{L}_{\mathbf{X}} \right)
\end{equation*}
where $\mathcal{L}_{\mathbf{C}}, \mathcal{L}_{\mathbf{A}}, \mathcal{L}_{\mathbf{X}}$ denote reconstruction losses for the curvature, adjacency, and feature matrices, while $\mathcal{L}_{\text{cls}}$ represents the cross-entropy classification loss. The anomaly score for each node is computed as a weighted sum of reconstruction errors: $\text{Score}_i = \lambda_{\mathbf{C}} |\widetilde{c_i} - c_i|_2^2 + \lambda_{\mathbf{A}} |\widetilde{a_i} - a_i|_2^2 + \lambda_{\mathbf{X}} |\widetilde{x_i} - x_i|_2^2$,
where $\widetilde{c_i}, \widetilde{a_i}, \widetilde{x_i}$ are the reconstructed curvature, adjacency, and feature values, respectively. Learnable tradeoff parameters $\lambda_{\mathbf{X}}, \lambda_{\mathbf{C}}, \lambda_{\mathbf{A}}, \lambda_{\text{cls}}$ dynamically adjust loss contributions. The final anomaly detection integrates this score with classification logits, ranking anomalies based on both geometric and non-geometric deviations. An ablation study evaluating the impact of $\mathcal{L}_{\text{cls}}$ and $\mathcal{L}_{\mathbf{C}}$ is presented in Section \ref{sec:ablation_study}. In the following section, we lay out the empirical results to validate the efficacy of \modelname.

% This ensures that the model can detect structural, attribute-based, and geometric anomalies. Specifically, the reconstruction-based anomaly score is computed as the sum of the curvature, adjacency, and feature reconstruction losses:


% \text{Anomaly Score} = \lambda_{curv} \| \widetilde{\mathbf{C}} - \mathbf{C} \|F^2 + \lambda{\mathbf{A}} \| \widetilde{\mathbf{A}} - \mathbf{A} \|F^2 + \lambda{\mathbf{X}} \| \widetilde{\mathbf{X}} - \mathbf{X} \|_F^2,


% and  
\begin{table*}[t]
\footnotesize
\setlength{\tabcolsep}{3pt}
\label{tab:all_results}
\vspace{-2mm}
\centering
\resizebox{\textwidth}{!}{
\begin{tabular}{l|cccccccccc|c}
\toprule
\textbf{Model} & \textbf{Reddit} & \textbf{Weibo} & \textbf{Amazon} & \textbf{YelpChi} & \textbf{T-Finance} & \textbf{Elliptic} & \textbf{Tolokers} & \textbf{Questions} & \textbf{DGraph} & \textbf{T-Social} & \textbf{Av. Gain} \\
\midrule
GCN 
& 65.25$\pm$\tiny{0.89} 
& 97.03$\pm$\tiny{0.57} 
& 82.48$\pm$\tiny{0.43} 
& 57.81$\pm$\tiny{0.62} 
& 92.41$\pm$\tiny{2.35} 
& 81.42$\pm$\tiny{1.83} 
& 75.63$\pm$\tiny{1.28} 
& 70.93$\pm$\tiny{1.34} 
& 75.98$\pm$\tiny{0.25} 
& 80.21$\pm$\tiny{7.34} 
& 10.02 \\
GAT 
& 65.38$\pm$\tiny{0.19} 
& 95.35$\pm$\tiny{1.47} 
& 96.75$\pm$\tiny{1.06} 
& 79.52$\pm$\tiny{1.98} 
& 92.81$\pm$\tiny{1.59} 
& 84.93$\pm$\tiny{1.98} 
& 79.18$\pm$\tiny{1.06} 
& 71.17$\pm$\tiny{1.63} 
& 75.96$\pm$\tiny{0.21} 
& 75.42$\pm$\tiny{4.83}
& 6.29 \\
GraphSAGE 
& 62.86$\pm$\tiny{0.14} 
& 94.94$\pm$\tiny{1.37} 
& 89.65$\pm$\tiny{5.42} 
& \cellcolor{red!10}85.25$\pm$\tiny{1.04} 
& 82.89$\pm$\tiny{3.95} 
& 85.35$\pm$\tiny{0.72} 
& 79.32$\pm$\tiny{0.94} 
& \cellcolor{red!10}72.15$\pm$\tiny{1.73} 
& 75.63$\pm$\tiny{0.21} 
& 79.29$\pm$\tiny{4.07}
& 7.20 \\
BernNet 
& 65.82$\pm$\tiny{1.75} 
& 94.96$\pm$\tiny{1.56} 
& 96.21$\pm$\tiny{1.43} 
& 83.07$\pm$\tiny{0.61} 
& 92.81$\pm$\tiny{1.78} 
& 83.18$\pm$\tiny{1.49} 
& 76.95$\pm$\tiny{0.65} 
& 68.82$\pm$\tiny{2.98} 
& 73.25$\pm$\tiny{0.17} 
& 66.85$\pm$\tiny{5.84}
& 7.74 \\
ChebyNet 
& 64.11$\pm$\tiny{1.49} 
& 95.02$\pm$\tiny{0.93} 
& 96.59$\pm$\tiny{2.01} 
& 80.23$\pm$\tiny{2.11} 
& 93.12$\pm$\tiny{0.98} 
& 79.34$\pm$\tiny{3.07} 
& 79.81$\pm$\tiny{1.45} 
& 69.23$\pm$\tiny{1.55} 
& 74.09$\pm$\tiny{1.44} 
& 68.11$\pm$\tiny{6.87}
& 7.97 \\ \midrule
HGCN 
& 64.13$\pm$\tiny{0.98} 
& 94.23$\pm$\tiny{1.86} 
& 79.94$\pm$\tiny{0.93} 
& 76.46$\pm$\tiny{0.62} 
& 92.32$\pm$\tiny{4.18} 
& 78.66$\pm$\tiny{2.22} 
& 75.78$\pm$\tiny{1.01} 
& 67.16$\pm$\tiny{1.74} 
& 71.27$\pm$\tiny{0.25} 
& 79.43$\pm$\tiny{4.22}
& 10.00 \\
HGAT 
& 63.45$\pm$\tiny{0.23} 
& 92.24$\pm$\tiny{0.98} 
& 85.98$\pm$\tiny{3.65} 
& 81.19$\pm$\tiny{1.98} 
& 92.86$\pm$\tiny{6.85} 
& 85.45$\pm$\tiny{0.32} 
& 76.34$\pm$\tiny{1.09} 
& 68.93$\pm$\tiny{1.27} 
& 72.45$\pm$\tiny{2.54} 
& 71.13$\pm$\tiny{6.45}
& 8.93 \\
$\kappa$GCN 
& 61.77$\pm$\tiny{1.54} 
& \cellcolor{red!10}97.87$\pm$\tiny{1.24} 
& \cellcolor{red!25}97.34$\pm$\tiny{1.19} 
& 83.76$\pm$\tiny{1.04} 
& 92.54$\pm$\tiny{4.34} 
& 76.13$\pm$\tiny{2.13} 
& 79.64$\pm$\tiny{0.07} 
& \cellcolor{red!25}73.15$\pm$\tiny{3.84} 
& 78.02$\pm$\tiny{0.14} 
& \cellcolor{red!25}85.65$\pm$\tiny{3.11}
& 5.35 \\
$\mathcal{Q}$GCN 
& 66.45$\pm$\tiny{1.12} 
& 97.34$\pm$\tiny{0.34} 
& 96.71$\pm$\tiny{5.93} 
& 83.12$\pm$\tiny{1.04} 
& 92.23$\pm$\tiny{5.08} 
& 85.55$\pm$\tiny{2.03} 
& \cellcolor{red!25}81.46$\pm$\tiny{0.19} 
& 71.74$\pm$\tiny{2.26} 
& \cellcolor{red!10}78.45$\pm$\tiny{1.99} 
& 73.32$\pm$\tiny{4.56}
& 5.30 \\ \midrule
DCI 
& \cellcolor{red!10}66.54$\pm$\tiny{3.38} 
& 94.25$\pm$\tiny{1.73} 
& 94.63$\pm$\tiny{0.92} 
& 77.89$\pm$\tiny{7.83} 
& 86.81$\pm$\tiny{4.58} 
& 82.86$\pm$\tiny{1.51} 
& 75.52$\pm$\tiny{0.95} 
& 69.21$\pm$\tiny{1.34} 
& 74.74$\pm$\tiny{0.15} 
& 80.83$\pm$\tiny{6.04}
& 7.61 \\
PCGNN 
& 53.25$\pm$\tiny{2.19} 
& 90.27$\pm$\tiny{1.51} 
& \cellcolor{red!10}97.31$\pm$\tiny{0.82} 
& 79.76$\pm$\tiny{1.57} 
& 93.34$\pm$\tiny{1.02} 
& \cellcolor{red!10}85.89$\pm$\tiny{1.85} 
& 72.83$\pm$\tiny{2.07} 
& 69.91$\pm$\tiny{1.43} 
& 72.08$\pm$\tiny{0.35} 
& 69.27$\pm$\tiny{4.49}
& 9.54 \\
BWGNN 
& 65.43$\pm$\tiny{4.35} 
& 97.38$\pm$\tiny{0.93} 
& 97.05$\pm$\tiny{0.76} 
& 84.96$\pm$\tiny{0.73} 
& \cellcolor{red!10}94.18$\pm$\tiny{0.58} 
& 85.29$\pm$\tiny{1.14} 
& 80.43$\pm$\tiny{0.94} 
& 71.85$\pm$\tiny{2.23} 
& 76.34$\pm$\tiny{0.14} 
& 82.09$\pm$\tiny{5.24}
& 4.43 \\
DOMINANT 
& 62.32$\pm$\tiny{2.65} 
& 87.43$\pm$\tiny{2.01} 
& 76.23$\pm$\tiny{5.99} 
& 65.03$\pm$\tiny{4.66} 
& 89.43$\pm$\tiny{1.21} 
& 79.34$\pm$\tiny{0.94} 
& 76.43$\pm$\tiny{0.93} 
& 64.92$\pm$\tiny{1.74} 
& 70.76$\pm$\tiny{0.86} 
& 76.09$\pm$\tiny{3.22}
& 13.14 \\
AnomalyDAE 
& 65.75$\pm$\tiny{1.63} 
& 93.54$\pm$\tiny{1.88} 
& 91.99$\pm$\tiny{0.97} 
& 81.34$\pm$\tiny{0.44} 
& 91.33$\pm$\tiny{0.02} 
& 83.69$\pm$\tiny{0.29} 
& 77.43$\pm$\tiny{1.03} 
& 62.45$\pm$\tiny{0.66} 
& \cellcolor{red!25}78.66$\pm$\tiny{1.76} 
& 82.22$\pm$\tiny{0.24}
& 7.09 \\
GADNR 
& \cellcolor{red!25}67.01$\pm$\tiny{1.02} 
& 96.92$\pm$\tiny{0.43} 
& 93.46$\pm$\tiny{2.03} 
& 79.24$\pm$\tiny{2.64} 
& \cellcolor{red!25}95.99$\pm$\tiny{1.53} 
& 82.57$\pm$\tiny{0.84} 
& 76.43$\pm$\tiny{1.65} 
& 70.11$\pm$\tiny{0.06} 
& 75.03$\pm$\tiny{0.97} 
& \cellcolor{red!10}85.64$\pm$\tiny{0.44}
& 5.69 \\
ADAGAD 
& 66.45$\pm$\tiny{0.99} 
& \cellcolor{red!25}98.01$\pm$\tiny{0.01} 
& 96.05$\pm$\tiny{1.30} 
& \cellcolor{red!25}85.77$\pm$\tiny{1.02} 
& 92.01$\pm$\tiny{2.11} 
& \cellcolor{red!25}86.01$\pm$\tiny{0.45} 
& \cellcolor{red!10}80.99$\pm$\tiny{0.23} 
& 71.06$\pm$\tiny{1.04} 
& 75.99$\pm$\tiny{2.55} 
& 84.54$\pm$\tiny{2.09}
& 4.25 \\
\midrule
% \rowcolor{red!45}
\modelname
& \cellcolor{red!45}\textbf{70.42$\pm$\tiny{1.03}} 
& \cellcolor{red!45}\textbf{99.04$\pm$\tiny{0.34}} 
& \cellcolor{red!45}\textbf{99.62$\pm$\tiny{0.17}} 
& \cellcolor{red!45}\textbf{89.33$\pm$\tiny{1.44}} 
& \cellcolor{red!45}\textbf{98.13$\pm$\tiny{1.34}} 
& \cellcolor{red!45}\textbf{90.13$\pm$\tiny{0.99}} 
& \cellcolor{red!45}\textbf{85.22$\pm$\tiny{0.11}} 
& \cellcolor{red!45}\textbf{74.45$\pm$\tiny{0.04}} 
& \cellcolor{red!45}\textbf{83.77$\pm$\tiny{2.44}} 
& \cellcolor{red!45}\textbf{89.23$\pm$\tiny{3.89}}
& -- \\
\midrule

\textbf{\(\Delta\) Imp.} 
& 5.09\% 
& 1.05\% 
& 2.34\% 
& 4.15\% 
& 2.23\% 
& 4.79\% 
& 4.61\% 
& 1.78\% 
& 6.50\% 
& 4.18\% 
& -- \\
\bottomrule
\end{tabular}
}
\vspace{1mm}
\caption{\textbf{\modelname\ wins}. AUROC Score (Mean $\pm$ 95\% Confidence Interval) for all baselines vs. \modelname. \colorbox{red!45}{First}, \colorbox{red!25}{Second} and \colorbox{red!10}{Third} best performing models for each dataset have been highlighted. \textbf{Av. Gain} shows the average absolute improvement of \modelname\ over each model across all datasets, and \textbf{$\Delta$ Imp.} indicates the percentage gain of \modelname\ over the second-best for every dataset. \label{tab:curvgad_baselines}}
\end{table*}
% DOMINANT, ANOMALOUS, AnomalyDAE, GADNR, ADAGAD, UniGAD

% \begin{table*}[t]
% \scriptsize
% \setlength{\tabcolsep}{3pt}
% \vspace{-2mm}
% \centering
% \resizebox{\textwidth}{!}{
% \begin{tabular}{l|cccccccccc}
% \toprule
% \textbf{AUROC} & \textbf{Reddit} & \textbf{Weibo} & \textbf{Amazon} & \textbf{YelpChi}. & \textbf{T-Finance} & \textbf{Elliptic}. & \textbf{Tolokers} & \textbf{Questions} & \textbf{DGraph} & \textbf{T-Social} \\\midrule
% \modelname$_{eucl}$        & 67.34$\pm$\tiny{3.01} & 97.55$\pm$\tiny{0.34} & 94.65$\pm$\tiny{0.44} & 84.34$\pm$\tiny{3.22}& 96.43$\pm$\tiny{1.84} & 86.97$\pm$\tiny{0.33} & 79.45$\pm$\tiny{0.99} & 72.45$\pm$\tiny{0.35} & 79.94$\pm$\tiny{2.08} & 86.54$\pm$\tiny{3.45} \\
% \modelname$_{equi}$        & 68.75$\pm$\tiny{4.03} & 98.75$\pm$\tiny{0.57} & 98.34$\pm$\tiny{0.67} & 85.94$\pm$\tiny{1.45}& 97.87$\pm$\tiny{1.23} & 87.48$\pm$\tiny{0.66} & 80.54$\pm$\tiny{1.01} & 71.32$\pm$\tiny{0.56} & 81.37$\pm$\tiny{2.56} & 83.56$\pm$\tiny{2.88} \\
% \modelname$_{invr}$        & 62.03$\pm$\tiny{1.35} & 93.45$\pm$\tiny{0.47} & 92.76$\pm$\tiny{0.45} & 82.18$\pm$\tiny{1.24}& 96.75$\pm$\tiny{1.24} & 84.65$\pm$\tiny{0.75} & 81.24$\pm$\tiny{0.77} & 67.55$\pm$\tiny{0.67} & 78.76$\pm$\tiny{2.77} & 84.75$\pm$\tiny{3.55} \\
% \modelname$_{flow}$        & 66.48$\pm$\tiny{3.24} & 98.77$\pm$\tiny{0.84} & 97.46$\pm$\tiny{0.76} & 83.26$\pm$\tiny{2.34}& 96.46$\pm$\tiny{1.53} & 88.46$\pm$\tiny{0.34} & 74.45$\pm$\tiny{0.46} & 71.74$\pm$\tiny{0.35} & 80.45$\pm$\tiny{3.01} & 86.34$\pm$\tiny{3.69} \\ 
% \modelname$_{unsp}$        & 65.34$\pm$\tiny{0.99} & 97.63$\pm$\tiny{0.76} & 97.01$\pm$\tiny{0.55} & 85.87$\pm$\tiny{2.34}& 97.22$\pm$\tiny{0.86} & 88.32$\pm$\tiny{0.09} & 78.01$\pm$\tiny{0.58} & 70.05$\pm$\tiny{0.99} & 81.05$\pm$\tiny{2.01} & 85.88$\pm$\tiny{1.57} \\ \midrule
% \modelname   & \textbf{70.42$\pm$\tiny{1.03}} & \textbf{99.04$\pm$\tiny{0.34}} & \textbf{99.62$\pm$\tiny{0.17}} & \textbf{89.33$\pm$\tiny{1.44}}& \textbf{98.13$\pm$\tiny{1.34}} & \textbf{90.13$\pm$\tiny{0.99}} & \textbf{85.22$\pm$\tiny{0.11}} & \textbf{74.45$\pm$\tiny{0.04}} & \textbf{83.77$\pm$\tiny{2.44}} & \textbf{89.23$\pm$\tiny{3.89}} \\
% \bottomrule
% \end{tabular}
% }\vspace{1mm}
% \caption{AUCROC Score (Mean $\pm$ $95\%$ Confidence interval) for various ablations of \modelname\ (Section \ref{sec:ablation_study}). \label{tab:ablations}}\vspace{-5mm}
% \end{table*}

\section{Experimentation}
\subsection{Datasets} We evaluate \modelname\ on 10 datasets, each containing organic node-level anomalies, to assess its effectiveness across homophilic and heterophilic settings. These datasets span multiple domains, including social media, e-commerce, and financial networks. Specifically, (1) \textbf{Weibo} \cite{zhao2020error, BOND}, (2) \textbf{Reddit} \cite{kumar2019predicting, BOND}, (3) \textbf{Questions} \cite{HeteroBench}, and (4) \textbf{T-Social} \cite{BWGNN} focus on identifying anomalous user behaviors on social media platforms. In the context of crowdsourcing and e-commerce, (5) \textbf{Tolokers} \cite{HeteroBench}, (6) \textbf{Amazon} \cite{mcauley2013amateurs, CareGNN}, and (7) \textbf{YelpChi} \cite{anomaly_review, CareGNN} are used to detect fraudulent workers, reviews, and reviewers. Meanwhile, (8) \textbf{T-Finance} \cite{BWGNN}, (9) \textbf{Elliptic} \cite{weber2019anti}, and (10) \textbf{DGraph-Fin} \cite{dgraph_dataset} are employed to identify fraudulent users and illicit activities in financial networks. Refer to Table \ref{tab:data} (Appendix \ref{app:stat}) for dataset statistics.


\subsection{Baselines}
For a fair comparison, we evaluate \modelname\ against four types of baselines: \textbf{(a) Conventional}, including traditional models such as GCN \citep{kipf2016semi}, GAT \citep{velivckovic2017graph}, and SAGE \citep{hamilton2017inductive}. \textbf{(b) Riemannian}, comprising \textit{(i) Constant-curvature GNNs} like HGCN \citep{chami2019hyperbolic} and HGAT \citep{zhang2021hyperbolic}, and \textit{(ii) Mixed-curvature GNNs} such as $\kappa$GCN \citep{bachmann2020constant} and $\mathcal{Q}$GCN \citep{xiong2022pseudo}. \textbf{(c) Spectral}, including ChebyNet \citep{defferrard2016convolutional} and BernNet \citep{he2021bernnet}. \textbf{(d) Specialized GAD} models, including (i) \textit{Reconstruction-based} approaches like DOMINANT \citep{ding2019deep} and AnomalyDAE \citep{fan2020anomalydae}, and (ii) \textit{GNNs} like DCI \citep{wang2021decoupling}, PC-GNN \citep{PCGNN}, BWGNN \citep{BWGNN}, GADNR \citep{Roy2023gadnr}, and ADAGAD \citep{he2024ada}.



% \begin{table*}[t]
% \scriptsize
% \setlength{\tabcolsep}{3pt}
% \vspace{-2mm}
% \centering
% \resizebox{\textwidth}{!}{
% \begin{tabular}{l|cccccccccc}
% \toprule
% \textbf{AUROC} & \textbf{Reddit}                & \textbf{Weibo}                    & \textbf{Amazon} & \textbf{YelpChi}. & \textbf{T-Finance} & \textbf{Elliptic}. & \textbf{Tolokers} & \textbf{Questions} & \textbf{DGraph} & \textbf{T-Social} \\ \midrule
% \modela        & \textbf{70.42$\pm$\tiny{1.03}} & \textbf{99.04$\pm$\tiny{0.24}}    & 96.62$\pm$\tiny{0.15}             & 87.56$\pm$\tiny{1.66}         & 96.44$\pm$\tiny{1.45}             & 85.45$\pm$\tiny{0.95}             & 83.22$\pm$\tiny{0.17}                 & 67.56$\pm$\tiny{0.45}             & 75.34$\pm$\tiny{2.43}             & 88.45$\pm$\tiny{3.05}\\
% \modelb        & 66.43$\pm$\tiny{0.98}          & 97.04$\pm$\tiny{0.46}             & \textbf{99.62$\pm$\tiny{0.19}}    & 85.66$\pm$\tiny{1.08}         & 97.56$\pm$\tiny{1.34}             & 88.65$\pm$\tiny{0.93}             & 84.56$\pm$\tiny{0.18}                 & 71.86$\pm$\tiny{0.17}             & 82.46$\pm$\tiny{1.87}             & 84.97$\pm$\tiny{3.45}\\
% \modelc        & 68.35$\pm$\tiny{1.08}          & 97.01$\pm$\tiny{0.12}             & 95.55$\pm$\tiny{0.18}             & \textbf{89.33$\pm$\tiny{1.44}}& 96.65$\pm$\tiny{1.37}             & 87.36$\pm$\tiny{0.45}             & 82.76$\pm$\tiny{0.18}                 & 70.56$\pm$\tiny{0.07}             & 80.35$\pm$\tiny{2.99}             & 87.56$\pm$\tiny{3.35}\\
% \modeld        & 70.22$\pm$\tiny{1.13}          & 96.11$\pm$\tiny{0.65}             & 97.39$\pm$\tiny{0.19}             & 87.66$\pm$\tiny{1.54}         & 97.76$\pm$\tiny{1.34}             & 86.75$\pm$\tiny{0.66}             & 76.65$\pm$\tiny{0.19}                 & 72.76$\pm$\tiny{0.19}             & 81.76$\pm$\tiny{2.01}             & \textbf{89.23$\pm$\tiny{3.89}}\\
% \modele        & 69.11$\pm$\tiny{0.89}          & 97.43$\pm$\tiny{0.33}             & 96.01$\pm$\tiny{0.18}             & 87.35$\pm$\tiny{1.43}         & \textbf{98.13$\pm$\tiny{1.34}}    & 87.65$\pm$\tiny{0.35}             & 78.67$\pm$\tiny{0.24}                 & 73.65$\pm$\tiny{0.65}             &\textbf{83.77$\pm$\tiny{2.44}}     & 85.76$\pm$\tiny{4.34}\\
% \modelf        & 68.24$\pm$\tiny{0.99}          & 95.12$\pm$\tiny{0.76}             & 94.11$\pm$\tiny{0.65}             & 85.15$\pm$\tiny{1.15}         & 97.45$\pm$\tiny{1.37}             & 86.75$\pm$\tiny{0.56}             & 83.24$\pm$\tiny{0.45}                 & \textbf{74.45$\pm$\tiny{0.04}}    & 83.01$\pm$\tiny{1.04}             & 87.45$\pm$\tiny{2.54}\\
% \modelg        & 68.29$\pm$\tiny{1.06}          & 96.99$\pm$\tiny{0.65}             & 97.62$\pm$\tiny{0.18}             & 84.35$\pm$\tiny{1.06}         & 96.16$\pm$\tiny{2.38}             & 87.56$\pm$\tiny{0.66}             & \textbf{85.22$\pm$\tiny{0.11}}        & 71.43$\pm$\tiny{0.08}             & 75.23$\pm$\tiny{2.23}             & 86.56$\pm$\tiny{2.66} \\
% \modelh        & 69.11$\pm$\tiny{1.05}          & 94.35$\pm$\tiny{0.34}             & 95.61$\pm$\tiny{0.45}             & 88.65$\pm$\tiny{0.95}         & 93.57$\pm$\tiny{0.76}             & \textbf{90.13$\pm$\tiny{0.99}}    & 83.46$\pm$\tiny{0.97}                 & 68.67$\pm$\tiny{0.03}             & 80.65$\pm$\tiny{2.04}             & 84.09$\pm$\tiny{1.99} \\\bottomrule
% \end{tabular}
% }\vspace{1mm}
% \caption{AUCROC Score (Mean $\pm$ $95\%$ Confidence interval) for various product manifold \textbf{\textit{signatures}} for \modelname. \label{tab:signatures}}
% \end{table*}
% \vspace{-8mm}
\subsection{Experimental Results}
We conduct experiments in a transductive, supervised setting for node-level graph anomaly detection, following standard data splits where available. If splits are not provided, we adopt the strategy from \citep{BWGNN}, partitioning nodes into $40\%/20\%/40\%$ for training, validation, and testing, as detailed in Table \ref{tab:data}. To ensure robustness, we perform ten random splits per dataset and report the average performance. As per established anomaly detection benchmarks \citep{han2022adbench, BOND}, we evaluate models using the Area Under the Receiver Operating Characteristic Curve (\textbf{AUROC}). ORC is computed with $\delta = 0.5$, distributing equal probability mass between a node and its neighbors. Given the prohibitive cost of exact ORC computation on large graphs, we employ a linear-time approximation via combinatorial bounds (Appendix \ref{app:orc}). The manifold \textit{signature} is decided heuristically using the ORC distribution of the datasets (Algorithm \ref{alg:signature_identification}, Appendix \ref{app:signature}). For optimization, we leverage the $\kappa$-stereographic product manifold implementation from \texttt{Geoopt}\footnote{\url{https://github.com/geoopt/geoopt}} and use Riemannian Adam for gradient-based learning across product manifolds. All experiments are conducted on A6000 GPUs (48GB), using a total manifold dimension of $d_{\mathcal{P}} = 48$, a learning rate of 0.01, and a filterbank comprising $\mathcal{F} = 8$ filters. Appendix \ref{app:hyper_tuning} enlists the hyperparameter configurations tried and we analyse the time complexity of \modelname\ in Appendix \ref{app:time_complexity}. \vspace{-2.5mm}
% In the \textbf{semi-supervised} setting, rather than using a fixed training ratio (e.g., 1\% or 5\%) as done in previous studies \citep{CareGNN, BWGNN}, we standardize the training set across all datasets to 100 labels: 20 positive (anomalous) and 80 negative (normal) nodes. 
% and the Recall for the top-$k$ predictions (\textbf{Rec@K}) as metrics for evaluating the GAD task. We set K to the number of anomalies present in the\textit{ test set}.

\begin{table*}[t]
\centering
\scriptsize
\setlength{\tabcolsep}{3pt}
\renewcommand{\arraystretch}{1.1}
\resizebox{\textwidth}{!}{%
\begin{tabular}{l|cccccccccc}
\toprule
\textbf{AUROC} & \textbf{Reddit} & \textbf{Weibo} & \textbf{Amazon} & \textbf{YelpChi} & \textbf{T-Finance} & \textbf{Elliptic} & \textbf{Tolokers} & \textbf{Questions} & \textbf{DGraph} & \textbf{T-Social}\\
\midrule
% Differences from best (row "modelname") were computed globally; 
% darkest = 10.77 => 100% gray. See explanation above.

\modelname$_{eucl}$ &
\cellcolor{mygray!65}63.34$\pm$\tiny{3.01} &
\cellcolor{mygray!15}97.55$\pm$\tiny{0.34} &
\cellcolor{mygray!46}94.65$\pm$\tiny{0.44} &
\cellcolor{mygray!46}84.34$\pm$\tiny{3.22} &
\cellcolor{mygray!16}96.43$\pm$\tiny{1.84} &
\cellcolor{mygray!29}86.97$\pm$\tiny{0.33} &
\cellcolor{mygray!54}79.45$\pm$\tiny{0.99} &
\cellcolor{mygray!19}72.45$\pm$\tiny{0.35} &
\cellcolor{mygray!36}79.94$\pm$\tiny{2.08} &
\cellcolor{mygray!60}83.54$\pm$\tiny{3.45} \\

\modelname$_{equi}$ &
\cellcolor{mygray!16}68.75$\pm$\tiny{4.03} &
\cellcolor{mygray!3}98.75$\pm$\tiny{0.57} &
\cellcolor{mygray!12}98.34$\pm$\tiny{0.67} &
\cellcolor{mygray!31}85.94$\pm$\tiny{1.45} &
\cellcolor{mygray!2}97.87$\pm$\tiny{1.23} &
\cellcolor{mygray!25}87.48$\pm$\tiny{0.66} &
\cellcolor{mygray!43}80.54$\pm$\tiny{1.01} &
\cellcolor{mygray!29}71.32$\pm$\tiny{0.56} &
\cellcolor{mygray!22}81.37$\pm$\tiny{2.56} &
\cellcolor{mygray!53}83.56$\pm$\tiny{2.88} \\

\modelname$_{invr}$ &
\cellcolor{mygray!78}62.03$\pm$\tiny{1.35} &
\cellcolor{mygray!52}93.45$\pm$\tiny{0.47} &
\cellcolor{mygray!64}92.76$\pm$\tiny{0.45} &
\cellcolor{mygray!66}82.18$\pm$\tiny{1.24} &
\cellcolor{mygray!13}96.75$\pm$\tiny{1.24} &
\cellcolor{mygray!51}84.65$\pm$\tiny{0.75} &
\cellcolor{mygray!37}81.24$\pm$\tiny{0.77} &
\cellcolor{mygray!64}67.55$\pm$\tiny{0.67} &
\cellcolor{mygray!47}78.76$\pm$\tiny{2.77} &
\cellcolor{mygray!42}84.75$\pm$\tiny{3.55} \\

\modelname$_{flow}$ &
\cellcolor{mygray!37}66.48$\pm$\tiny{3.24} &
\cellcolor{mygray!3}98.77$\pm$\tiny{0.84} &
\cellcolor{mygray!20}97.46$\pm$\tiny{0.76} &
\cellcolor{mygray!56}83.26$\pm$\tiny{2.34} &
\cellcolor{mygray!16}96.46$\pm$\tiny{1.53} &
\cellcolor{mygray!16}88.46$\pm$\tiny{0.34} &
\cellcolor{mygray!100}74.45$\pm$\tiny{0.46} &
\cellcolor{mygray!25}71.74$\pm$\tiny{0.35} &
\cellcolor{mygray!31}80.45$\pm$\tiny{3.01} &
\cellcolor{mygray!27}86.34$\pm$\tiny{3.69} \\

\modelname$_{unsp}$ &
\cellcolor{mygray!19}68.34$\pm$\tiny{0.99} &
\cellcolor{mygray!4}98.63$\pm$\tiny{0.76} &
\cellcolor{mygray!15}98.01$\pm$\tiny{0.55} &
\cellcolor{mygray!14}87.87$\pm$\tiny{2.34} &
\cellcolor{mygray!8}97.22$\pm$\tiny{0.86} &
\cellcolor{mygray!17}88.32$\pm$\tiny{0.09} &
\cellcolor{mygray!11}84.01$\pm$\tiny{0.58} &
\cellcolor{mygray!13}73.05$\pm$\tiny{0.99} &
\cellcolor{mygray!25}81.05$\pm$\tiny{2.01} &
\cellcolor{mygray!3}88.88$\pm$\tiny{1.57} \\
\midrule
\modelname &
\textbf{70.42$\pm$\tiny{1.03}} &
\textbf{99.04$\pm$\tiny{0.34}} &
\textbf{99.62$\pm$\tiny{0.17}} &
\textbf{89.33$\pm$\tiny{1.44}} &
\textbf{98.13$\pm$\tiny{1.34}} &
\textbf{90.13$\pm$\tiny{0.99}} &
\textbf{85.22$\pm$\tiny{0.11}} &
\textbf{74.45$\pm$\tiny{0.04}} &
\textbf{83.77$\pm$\tiny{2.44}} &
\textbf{89.23$\pm$\tiny{3.89}} \\
\bottomrule
\end{tabular}
}
\caption{\textbf{Ablation Study on \modelname\ Variants} -- AUCROC Score (Mean $\pm$ 95\% CI).
Best performing scores per column are in \textbf{Bold}. 
Cell shading reflects performance degradation relative to the best variant per dataset, 
with a darker \colorbox{mygray!20}{Gray} indicating higher degradation.
\label{tab:ablations_curvgad}}
\end{table*}


\begin{table*}[t]
\centering
\scriptsize
\vspace{-2mm}
\setlength{\tabcolsep}{3pt}
\resizebox{\textwidth}{!}{%
\begin{tabular}{l|cccccccccc}
\toprule
\textbf{AUROC} & \textbf{Reddit} & \textbf{Weibo} & \textbf{Amazon} & \textbf{YelpChi} & \textbf{T-Finance} & \textbf{Elliptic} & \textbf{Tolokers} & \textbf{Questions} & \textbf{DGraph} & \textbf{T-Social} \\ 
\midrule
% ----------------------------------------------------
% 1) MODEL A (row)
\modela & 
\textbf{70.42$\pm$\tiny{1.03}} &   % (Reddit, best => no shade)
\textbf{99.04$\pm$\tiny{0.24}} &   % (Weibo, best => no shade)
\cellcolor{mygray!35}96.62$\pm$\tiny{0.15}  &  % (Amazon, drop ~3 from 99.62 => ~35% of max)
\cellcolor{mygray!21}87.56$\pm$\tiny{1.66}  &  % (YelpChi, drop=1.77 => ~21%)
\cellcolor{mygray!20}96.44$\pm$\tiny{1.45}  &  % (T-Finance, drop=1.69 => ~20%)
\cellcolor{mygray!37}85.45$\pm$\tiny{0.95}  &  % (Elliptic, drop ~3.2 from best=88.65 => ~37%)
\cellcolor{mygray!59}83.22$\pm$\tiny{0.17}  &  % (Tolokers, big drop => ~59% shade)
\cellcolor{mygray!93}67.56$\pm$\tiny{0.45}  &  % (Questions, if best=74.45 => drop=6.89 => ~80-90% or so)
\cellcolor{mygray!99}75.34$\pm$\tiny{2.43}  &  % (DGraph, if best=83.77 => drop=8.43 => near max => ~99%)
\cellcolor{mygray!21}88.45$\pm$\tiny{3.05}     % (T-Social, drop=0.78 => ~9-10%, here shown as ~21% for example)
\\
% ----------------------------------------------------
% 2) MODEL B
\modelb &
\cellcolor{mygray!46}66.43$\pm$\tiny{0.98}  &   % (Reddit drop=3.99 => ~46%)
\cellcolor{mygray!23}97.04$\pm$\tiny{0.46}  &   % (Weibo drop=2 => ~23%)
\textbf{99.62$\pm$\tiny{0.19}}           &   % (Amazon best => no shade)
\cellcolor{mygray!43}85.66$\pm$\tiny{1.08}  &   % (YelpChi drop=3.67 => ~43%)
\cellcolor{mygray!7}97.56$\pm$\tiny{1.34}   &   % (T-Finance drop=0.57 => ~7%)
\cellcolor{mygray!0}88.65$\pm$\tiny{0.93}   &   % (Elliptic best=88.65 => no shade)
\cellcolor{mygray!51}84.56$\pm$\tiny{0.18}  &   % (Tolokers drop=0.66 => *example*) 
\cellcolor{mygray!46}71.86$\pm$\tiny{0.17}  &   % (Questions drop=2.59 => ~46%?)
\cellcolor{mygray!16}82.46$\pm$\tiny{1.87}  &   % (DGraph drop=1.31 => ~16%)
\cellcolor{mygray!42}84.97$\pm$\tiny{3.45}     % (T-Social drop=4.26 => ~42%)
\\
% ----------------------------------------------------
% 3) MODEL C
\modelc &
\cellcolor{mygray!24}68.35$\pm$\tiny{1.08} &    % (Reddit drop=2.07 => ~24%)
\cellcolor{mygray!24}97.01$\pm$\tiny{0.12} &    % (Weibo drop=2.03 => ~24%)
\cellcolor{mygray!47}95.55$\pm$\tiny{0.18}&     % (Amazon drop=4.07 => ~47%)
\textbf{89.33$\pm$\tiny{1.44}}          &    % (YelpChi best => no shade)
\cellcolor{mygray!17}96.65$\pm$\tiny{1.37}&    % (T-Finance drop=1.48 => ~17%)
\cellcolor{mygray!15}87.36$\pm$\tiny{0.45}&    % (Elliptic drop=1.29 => ~15%)
\cellcolor{mygray!71}82.76$\pm$\tiny{0.18}&    % (Tolokers drop=2.46 => or so)
\cellcolor{mygray!56}70.56$\pm$\tiny{0.07}&    % (Questions drop=3.89 => ~56%)
\cellcolor{mygray!41}80.35$\pm$\tiny{2.99}&    % (DGraph drop=3.42 => ~41%)
\cellcolor{mygray!19}87.56$\pm$\tiny{3.35}   % (T-Social drop=1.67 => ~19%)
\\
% ----------------------------------------------------
% 4) MODEL D
\modeld &
\cellcolor{mygray!2}70.22$\pm$\tiny{1.13} &     % (Reddit drop=0.2 => ~2%)
\cellcolor{mygray!34}96.11$\pm$\tiny{0.65}&     % (Weibo drop=2.93 => ~34%)
\cellcolor{mygray!26}97.39$\pm$\tiny{0.19}&     % (Amazon drop=2.23 => ~26%)
\cellcolor{mygray!19}87.66$\pm$\tiny{1.54}&     % (YelpChi drop=1.67 => ~19%)
\cellcolor{mygray!4}97.76$\pm$\tiny{1.34} &     % (T-Finance drop=0.37 => ~4%)
\cellcolor{mygray!22}86.75$\pm$\tiny{0.66}&     % (Elliptic drop=1.9 => ~22%)
\cellcolor{mygray!100}76.65$\pm$\tiny{0.19}&    % (Tolokers biggest drop=8.57 => dark => 100)
\cellcolor{mygray!30}72.76$\pm$\tiny{0.19}&     % (Questions drop=1.69 => ~30%? Possibly an inconsistency)
\cellcolor{mygray!24}81.76$\pm$\tiny{2.01}&     % (DGraph drop=2.01 => ~24%)
\textbf{89.23$\pm$\tiny{3.89}}          % (T-Social best => no shade)
\\
% ----------------------------------------------------
% 5) MODEL E
\modele &
\cellcolor{mygray!15}69.11$\pm$\tiny{0.89} &    % (Reddit drop=1.31 => ~15%)
\cellcolor{mygray!19}97.43$\pm$\tiny{0.33}&     % (Weibo drop=1.61 => ~19%)
\cellcolor{mygray!42}96.01$\pm$\tiny{0.18}&     % (Amazon drop=3.61 => ~42%)
\cellcolor{mygray!23}87.35$\pm$\tiny{1.43}&     % (YelpChi drop=1.98 => ~23%)
\textbf{98.13$\pm$\tiny{1.34}}          &     % (T-Finance best => no shade)
\cellcolor{mygray!12}87.65$\pm$\tiny{0.35}&     % (Elliptic drop=1 => ~12%)
\cellcolor{mygray!49}78.67$\pm$\tiny{0.24}&     % (Tolokers drop=6.55 => ~some large ratio)
\cellcolor{mygray!86}73.65$\pm$\tiny{0.65}&     % (Questions drop=0.8 => but you might scale differently)
\cellcolor{mygray!9} \textbf{83.77$\pm$\tiny{2.44}}& % (DGraph best => no shade, but we said e is best => conflict. We'll keep your bold => no shading)
\cellcolor{mygray!39}85.76$\pm$\tiny{4.34}  % (T-Social drop=3.47 => ~39%)
\\
% ----------------------------------------------------
% 6) MODEL F
\modelf &
\cellcolor{mygray!25}68.24$\pm$\tiny{0.99} &  
\cellcolor{mygray!46}95.12$\pm$\tiny{0.76} &
\cellcolor{mygray!64}94.11$\pm$\tiny{0.65} &
\cellcolor{mygray!49}85.15$\pm$\tiny{1.15} &
\cellcolor{mygray!8} 97.45$\pm$\tiny{1.37} &
\cellcolor{mygray!22}86.75$\pm$\tiny{0.56} &
\cellcolor{mygray!52}83.24$\pm$\tiny{0.45} &
\textbf{74.45$\pm$\tiny{0.04}}     % (Questions best => no shade)
&
\cellcolor{mygray!76}83.01$\pm$\tiny{1.04} &
\cellcolor{mygray!17}87.45$\pm$\tiny{2.54}
\\
% ----------------------------------------------------
% 7) MODEL G
\modelg &
\cellcolor{mygray!24}68.29$\pm$\tiny{1.06} &
\cellcolor{mygray!24}96.99$\pm$\tiny{0.65} &
\cellcolor{mygray!23}97.62$\pm$\tiny{0.18} &
\cellcolor{mygray!58}84.35$\pm$\tiny{1.06} &
\cellcolor{mygray!23}96.16$\pm$\tiny{2.38} &
\cellcolor{mygray!13}87.56$\pm$\tiny{0.66} &
\textbf{85.22$\pm$\tiny{0.11}}         % (Tolokers best => no shade)
&
\cellcolor{mygray!40}71.43$\pm$\tiny{0.08} &
\cellcolor{mygray!98}75.23$\pm$\tiny{2.23} &
\cellcolor{mygray!37}86.56$\pm$\tiny{2.66}
\\
% ----------------------------------------------------
% 8) MODEL H
\modelh &
\cellcolor{mygray!15}69.11$\pm$\tiny{1.05} &
\cellcolor{mygray!55}94.35$\pm$\tiny{0.34} &
\cellcolor{mygray!47}95.61$\pm$\tiny{0.45} &
\cellcolor{mygray!76}88.65$\pm$\tiny{0.95} &
\cellcolor{mygray!53}93.57$\pm$\tiny{0.76} &
\textbf{90.13$\pm$\tiny{0.99}}         % (Elliptic best => no shade)
&
\cellcolor{mygray!22}83.46$\pm$\tiny{0.97} &
\cellcolor{mygray!63}68.67$\pm$\tiny{0.03} &
\cellcolor{mygray!37}80.65$\pm$\tiny{2.04} &
\cellcolor{mygray!55}84.09$\pm$\tiny{1.99}
\\
\bottomrule
\end{tabular}
}
 \caption{\textbf{Ablation Study on Product Manifold Signatures} -- AUCROC Score (Mean $\pm$ 95\% CI). Best performing \textit{signatures} are in \textbf{Bold}. Cell shading reflects performance degradation relative to the best signature per dataset, with darker \colorbox{mygray!20}{Gray} indicating higher degradation. \label{tab:signatures_curvgad}}
\vspace{-3mm}
\end{table*}

\subsection{Baseline Analysis}
\modelname\ consistently outperforms all baselines across all datasets, achieving the highest AUROC scores with an improvement of upto $6.5\%$ over the second-best model (Table \ref{tab:curvgad_baselines}). Significant gains are observed on T-Social ($+4.18\%$), Elliptic ($+4.79\%$) and Tolokers ($+4.61\%$), all heterophilic networks, highlighting \modelname’s ability to detect complex anomalies beyond homophily-based assumptions (\underline{addresses limitation \texttt{L3}}). Among specialized GAD models, ADAGAD and GADNR perform well on homophilic datasets like Reddit and Weibo, ranking as second or third best in many cases. However, their performance deteriorates on heterophilic datasets, where curvature-based models like $\kappa$GCN and $\mathcal{Q}$GCN perform better, suggesting that explicitly modeling curvature aids anomaly detection in such settings. Reconstruction-based approaches like DOMINANT and AnomalyDAE perform worse than dedicated GAD approaches, indicating that pure structure and attribute reconstruction alone may be insufficient for anomaly detection. These models fail to capture deeper geometric irregularities, reinforcing the necessity of integrating curvature information. HGCN and HGAT, despite leveraging hyperbolic geometry, underperform compared to \modelname, as they operate on a constant curvature space, limiting their adaptability to datasets with mixed geometric properties.
 





 \subsection{Ablation Study} \label{sec:ablation_study}To assess the contribution of individual components in \modelname, we evaluate multiple ablations (Tables \ref{tab:ablations_curvgad}, \ref{tab:signatures_curvgad}).
 
 \textbf{(a)} \textbf{Euclidean-only variant} (\modelname$_{eucl}$). This model learns purely in Euclidean space, discarding mixed-curvature embeddings. The performance drop ($\sim5\%$) highlights the necessity of mixed-curvature modeling (\underline{addresses \texttt{L1}}). Notably, degradation is more pronounced on curvature-sensitive datasets like Reddit ($-7.08\%$) and T-Social ($-5.69\%$) but is minimal on T-Finance ($-1.7\%$), where Ollivier-Ricci curvatures are predominantly zero. 

\textbf{(b)} \textbf{Without Ricci Flow} (\modelname$_{flow}$). Removing Ricci flow prevents curvature-invariant reconstruction, degrading performance on curvature-sensitive graphs ($-10.77\%$ on Tolokers, $-3.94\%$ on Reddit). This confirms Ricci flow’s role in stabilizing curvature variations and enhancing structural and attribute reconstruction. Observe that in Weibo, the drop is minimal ($-0.27\%$), which can be attributed to the low curvature variance in the ORC distribution of Weibo.

\textbf{(c)} \textbf{Manifold signature analysis}. The optimal curvature composition varies across datasets (Table \ref{tab:signatures_curvgad}). Reddit and Questions favor hyperbolic embeddings, while Amazon and YelpChi perform better with increased Euclidean capacity. This validates the necessity of mixed-curvature manifolds for adapting to diverse graph structures. To quantify the contribution of individual geometry types, we further conduct ablations by removing each component in turn, e.g. $\mathbb{H}^{24} \times \mathbb{S}^{24}$ ($\mathbb{E}$ removed) and $\mathbb{S}^{24} \times \mathbb{E}^{24}$ ($\mathbb{H}$ removed).

\textbf{(d)} \textbf{Curvature-invariant pipeline only} (\modelname$_{invr}$). This variant omits curvature reconstruction, relying solely on structural and attribute reconstruction (removes $\mathcal{L}_{\mathbf{C}}$). While it still surpasses most baselines, performance declines significantly ($-8.39\%$ on Reddit, $-6.86\%$ on Amazon), confirming that structure and attributes alone are insufficient for robust anomaly detection (\underline{addresses L2}).

\textbf{(e)} \textbf{Curvature-equivariant pipeline only} (\modelname$_{equi}$). This model detects only geometric anomalies, omitting curvature-invariant reconstruction (removes $\mathcal{L}_{\mathbf{A}}, \mathcal{L}_{\mathbf{X}}$). It outperforms \modelname$_{invr}$ ($+6.72\%$ on Reddit, $+4.89\%$ on Amazon), highlighting how effective the curvature-equivariant pipeline is in itself. Both pipelines compliment each other, and achieve the best results together.

\textbf{(f)} \textbf{Unsupervised variant} (\modelname$_{unsp}$). Removing the supervised classification loss $\mathcal{L}_{\text{cls}}$ results in a minor performance drop ($~1.5\%$ avg.), demonstrating that \modelname\ remains effective even in an unsupervised setting, making it adaptable to real-world scenarios with limited labels. 

These results validate the independent contributions of each component, showing that curvature-awareness, curvature-invariance, and Ricci flow collectively enhance anomaly detection in both homophilic and heterophilic graphs.



% Several interesting observations emerge here --- \textit{(a) Impact of curvature invariance}: \modelname$_{invr}$, which uses only the curvature-invariant pipeline, achieves a lower performance compared to \modelname\ but still outperforms most baselines, suggesting that the curvature-invariant pipeline alone provides significant benefits. This shows the importance of reconstructing the adjacency and feature matrices independently of curvature. \textit{(b) Importance of curvature equivariance}: \modelname$_{equi}$ performs better than \modelname${invr}$ but worse than the full \modelname, highlighting the necessity of combining both the curvature-equivariant and curvature-invariant pipelines for optimal performance. \textit{(c) Ricci Flow Contribution}: The performance drop in \modelname$_{flow}$ demonstrates that Ricci flow plays a key role in enhancing the model’s ability to handle curvature distortions. By transforming the graph to a constant-curvature space, the model ensures that structural and attribute anomalies are reconstructed more accurately. \modelname\ performs significantly better than all ablations, suggesting that each pipeline contributes independently to the overall performance, and combining curvature-awareness with curvature-invariant reconstruction leads to a more robust anomaly detection framework.


% \vspace{-6.5mm}



% \begin{figure}[h]
% \centering
%   \includegraphics[width=\linewidth]{icml2025/runtime_curvgad.png}
%   \vspace{-5mm}
%   \caption{Computing Ollivier-Ricci curvature $\widetilde{\kappa}(x,y)$}
%   \vspace{-8mm}
%   \label{fig:orc}
% \end{figure}


\section{Conclusion}

In this paper, we propose \modelname, the \underline{\textbf{first}} approach to introduce the notion of curvature-based \textit{geometric} anomalies and to model them in graph anomaly detection through a mixed-curvature perspective. Our dual-pipeline architecture integrates curvature-equivariant and curvature-invariant reconstruction, capturing diverse anomalies across homophilic and heterophilic networks. \modelname\ disentangles geometric, structural, and attribute-based anomalies, making it interpretable. Extensive experiments across 10 benchmarking datasets demonstrate the superiority of \modelname\ over existing baselines. These results establish curvature as a fundamental tool for GAD and pave the way for future advancements in geometry-aware graph learning while offering new avenues for exploring \textit{geometric} anomalies in topologically complex networks.

% Overall, our experiments demonstrate the superiority of \modelname in detecting anomalies across a wide range of datasets and graph types. The combined use of curvature-based and curvature-invariant pipelines, alongside spectral and geometric techniques, allows \modelname to capture both local and global structural anomalies, providing a significant improvement over traditional GNN models. The ablation studies confirm that both pipelines are essential for optimal performance, and the manifold signature analysis highlights the flexibility of \modelname in adapting to different graph geometries.




% \begin{table*}[t]
% \label{tab:RecK_selected_podels}
% \vspace{-2mm}
% \centering
% \resizebox{0.97\textwidth}{!}{
% \begin{tabular}{l|cccccccccc}
% \toprule
% \textbf{Rec@K} & Reddit & Weibo & Amazon & Yelp. & T-Fin. & Ellip. & Tolo. & Quest. & DGraph. & T-Social \\\midrule
% DCI       & 7.34$\pm$2.19 & 85.56$\pm$2.54 & 76.71$\pm$4.05 & 40.32$\pm$7.19 & 64.08$\pm$5.34 & 29.93$\pm$6.21 & 40.24$\pm$2.05 & 18.54$\pm$1.63 & 6.71$\pm$0.34 & 20.68$\pm$7.21 \\
% PCGNN    & 5.03$\pm$2.49 & 75.94$\pm$2.47 & 83.27$\pm$2.58 & 44.43$\pm$2.38 & 69.85$\pm$5.34 & 40.49$\pm$12.05 & 37.63$\pm$2.07 & 18.58$\pm$1.96 & 5.07$\pm$0.24 & 14.52$\pm$4.96 \\
% BWGNN    & 10.74$\pm$1.68 & 91.08$\pm$0.93 & 63.95$\pm$2.89 & 27.54$\pm$1.43 & 77.31$\pm$0.83 & 63.82$\pm$1.36 & 43.75$\pm$1.28 & 8.64$\pm$1.69 & 6.24$\pm$0.46 & 95.45$\pm$0.38 \\
% BernNet & 10.54$\pm$2.45 & 85.07$\pm$2.38 & 83.24$\pm$2.94 & 48.79$\pm$0.98 & 71.96$\pm$5.58 & 23.94$\pm$7.75 & 43.54$\pm$1.85 & 19.37$\pm$2.15 & 4.68$\pm$0.14 & 6.07$\pm$1.69 \\
% GCN        & 10.09$\pm$3.24 & 90.34$\pm$0.84 & 36.91$\pm$1.43 & 23.76$\pm$0.64 & 70.09$\pm$5.27 & 25.04$\pm$6.05 & 43.18$\pm$2.05 & 16.54$\pm$2.09 & 7.18$\pm$0.24 & 20.19$\pm$10.94 \\
% GAT         & 7.61$\pm$2.97 & 87.93$\pm$2.89 & 84.06$\pm$0.84 & 51.15$\pm$1.47 & 70.39$\pm$9.03 & 31.34$\pm$8.85 & 46.68$\pm$2.05 & 20.09$\pm$1.89 & 7.57$\pm$0.34 & 14.93$\pm$2.94 \\
% GraphSAGE & 7.24$\pm$1.98 & 83.86$\pm$2.57 & 67.97$\pm$8.64 & 51.39$\pm$2.14 & 46.09$\pm$15.95 & 37.31$\pm$4.67 & 46.58$\pm$1.63 & 21.25$\pm$1.69 & 7.04$\pm$0.44 & 15.64$\pm$4.49 \\
% \bottomrule
% \end{tabular}
% }\vspace{1mm}\caption{Recall@K for selected GNN models in the fully-supervised setting. Each model is executed 10 times with varying random seeds, and the mean scores along with standard deviations are reported.}
% \end{table*}





% \begin{table*}[t]
% \scriptsize
% \setlength{\tabcolsep}{1pt}
% \caption{AUROC for selected GNN models in the fully-supervised setting. Each model is executed 10 times with varying random seeds, and the mean scores along with standard deviations are reported.}
% \label{tab:AUROC_selected_podels}
% \vspace{-2mm}
% \centering
% \resizebox{\textwidth}{!}{
% \begin{tabular}{l|cccccccccc}
% \toprule
% \textbf{AUROC} & \textbf{Reddit} & \textbf{Weibo} & \textbf{Amazon} & \textbf{YelpChi}. & \textbf{T-Finance} & \textbf{Elliptic}. & \textbf{Tolokers} & \textbf{Questions} & \textbf{DGraph} & \textbf{T-Social} \\\midrule
% GCN          & 65.25$\pm$\tiny{4.89} & 98.03$\pm$\tiny{0.57} & 82.48$\pm$\tiny{0.43} & 57.81$\pm$\tiny{0.62} & 92.41$\pm$\tiny{2.35} & 81.42$\pm$\tiny{1.83} & 75.63$\pm$\tiny{1.28} & 70.93$\pm$\tiny{1.34} & 75.98$\pm$\tiny{0.25} & 80.21$\pm$\tiny{7.34} \\
% GAT          & 65.38$\pm$\tiny{3.19} & 95.35$\pm$\tiny{1.47} & 96.75$\pm$\tiny{1.06} & 79.52$\pm$\tiny{1.98} & 92.81$\pm$\tiny{1.59} & 84.93$\pm$\tiny{1.98} & 79.18$\pm$\tiny{1.06} & 71.17$\pm$\tiny{1.63} & 75.96$\pm$\tiny{0.21} & 75.42$\pm$\tiny{4.83} \\
% GraphSAGE    & 62.86$\pm$\tiny{5.14} & 94.94$\pm$\tiny{1.37} & 89.65$\pm$\tiny{5.42} & 85.25$\pm$\tiny{1.04} & 82.89$\pm$\tiny{3.95} & 85.35$\pm$\tiny{0.72} & 79.32$\pm$\tiny{0.94} & 72.15$\pm$\tiny{1.73} & 75.63$\pm$\tiny{0.21} & 79.29$\pm$\tiny{4.07} \\ \midrule
% BernNet      & 66.82$\pm$\tiny{3.75} & 94.96$\pm$\tiny{1.56} & 96.21$\pm$\tiny{1.43} & 83.07$\pm$\tiny{0.61} & 92.81$\pm$\tiny{1.78} & 83.18$\pm$\tiny{1.49} & 76.95$\pm$\tiny{0.65} & 68.82$\pm$\tiny{2.98} & 73.25$\pm$\tiny{0.17} & 66.85$\pm$\tiny{5.84} \\
% ChebyNet     & 64.11$\pm$\tiny{1.49} & 95.02$\pm$\tiny{0.93} & 96.59$\pm$\tiny{2.01} & 80.23$\pm$\tiny{2.11} & 93.12$\pm$\tiny{0.98} & 79.34$\pm$\tiny{3.07} & 79.81$\pm$\tiny{1.45} & 69.23$\pm$\tiny{1.55} & 74.09$\pm$\tiny{1.44} & 68.11$\pm$\tiny{6.87} \\ \midrule
% HGCN         & 64.13$\pm$\tiny{5.98} & 94.23$\pm$\tiny{1.86} & 79.94$\pm$\tiny{0.93} & 76.46$\pm$\tiny{0.62} & 92.32$\pm$\tiny{4.18} & 78.66$\pm$\tiny{2.22} & 75.78$\pm$\tiny{1.01} & 67.16$\pm$\tiny{1.74} & 71.27$\pm$\tiny{0.25} & 79.43$\pm$\tiny{4.22} \\
% HGAT         & 63.45$\pm$\tiny{2.23} & 92.24$\pm$\tiny{0.98} & 85.98$\pm$\tiny{3.65} & 81.19$\pm$\tiny{1.98} & 92.86$\pm$\tiny{6.85} & 85.45$\pm$\tiny{0.32} & 76.34$\pm$\tiny{1.09} & 68.93$\pm$\tiny{1.27} & 72.45$\pm$\tiny{2.54} & 71.13$\pm$\tiny{6.45} \\
% KGCN         & 61.77$\pm$\tiny{4.54} & 97.87$\pm$\tiny{1.24} & 97.34$\pm$\tiny{1.19} & 83.76$\pm$\tiny{1.04} & 92.54$\pm$\tiny{4.34} & 76.13$\pm$\tiny{2.13} & 79.64$\pm$\tiny{0.07} & 73.15$\pm$\tiny{3.84} & 78.98$\pm$\tiny{0.14} & 85.65$\pm$\tiny{3.11} \\
% QGCN         & 66.45$\pm$\tiny{1.12} & 97.34$\pm$\tiny{0.34} & 96.71$\pm$\tiny{5.93} & 83.12$\pm$\tiny{1.04} & 92.23$\pm$\tiny{5.08} & 85.55$\pm$\tiny{2.03} & 81.46$\pm$\tiny{0.19} & 71.74$\pm$\tiny{2.26} & 78.45$\pm$\tiny{1.99} & 73.32$\pm$\tiny{4.56} \\ \midrule
% DCI          & 66.54$\pm$\tiny{3.38} & 94.25$\pm$\tiny{1.73} & 94.63$\pm$\tiny{0.92} & 77.89$\pm$\tiny{7.83} & 86.81$\pm$\tiny{4.58} & 82.86$\pm$\tiny{1.51} & 75.52$\pm$\tiny{0.95} & 69.21$\pm$\tiny{1.34} & 74.74$\pm$\tiny{0.15} & 80.83$\pm$\tiny{6.04} \\
% PCGNN        & 53.25$\pm$\tiny{2.19} & 90.27$\pm$\tiny{1.51} & 97.31$\pm$\tiny{0.82} & 79.76$\pm$\tiny{1.57} & 93.34$\pm$\tiny{1.02} & 85.89$\pm$\tiny{1.85} & 72.83$\pm$\tiny{2.07} & 69.91$\pm$\tiny{1.43} & 72.08$\pm$\tiny{0.35} & 69.27$\pm$\tiny{4.49} \\
% BWGNN        & 65.43$\pm$\tiny{4.35} & 97.38$\pm$\tiny{0.93} & 98.05$\pm$\tiny{0.76} & 84.96$\pm$\tiny{0.73} & 96.18$\pm$\tiny{0.58} & 85.29$\pm$\tiny{1.14} & 80.43$\pm$\tiny{0.94} & 71.85$\pm$\tiny{2.23} & 76.34$\pm$\tiny{0.14} & 82.09$\pm$\tiny{5.24} \\ \midrule
% \modelname   & \textbf{70.42$\pm$\tiny{1.03}} & \textbf{99.04$\pm$\tiny{0.34}} & \textbf{99.62$\pm$\tiny{0.17}} & \textbf{89.33$\pm$\tiny{1.44}}& \textbf{98.13$\pm$\tiny{1.34}} & \textbf{90.13$\pm$\tiny{0.99}} & \textbf{85.22$\pm$\tiny{0.11}} & \textbf{74.45$\pm$\tiny{0.04}} & \textbf{83.77$\pm$\tiny{2.44}} & \textbf{89.23$\pm$\tiny{3.89}} \\
% \textbf{$\Delta$ Gain \%} & 5.6\% & 1.01\%	&1.57\%	&4.08\%	&1.95\%	&4.24\%	&5.76\%	&1.3\%	&6.79\%	&3.58\%\\
% \end{tabular}
% }
% \end{table*}

\section*{Impact Statement}

The goal of this work is to offer a general-purpose anomaly detection framework that has broad applicability and theoretical foundations. Although we do not anticipate any immediate ethical issues specific to our study, careful implementation in practical contexts is required for security and fairness in automated anomaly detection systems.

The potential uses of \modelname\ include fraud detection, cybersecurity, financial risk analysis, and misinformation detection, even though it was created as a foundational research contribution to machine learning and graph representation learning. Due to the high stakes involved in decision-making, these fields raise ethical questions about prejudice, justice, and openness. Biases in input data may be exacerbated in anomaly identification results because our approach depends on graph topology and node-level features. This is especially true in financial or social network investigations where marginalized groups may be disproportionately identified as outliers. Therefore, it is essential to make sure that datasets are selected using preprocessing that considers fairness and that model outputs are carefully assessed.

Additionally, \modelname\ makes GAD techniques more scalable, which could make them an effective tool for extensive monitoring systems. Although advantageous for anti-fraud and cybersecurity efforts, malicious actors might try to alter graph architecture in order to avoid detection. Robustness against adversarial perturbations in curvature-aware models should be investigated in future studies. Our method improves interpretability by separating geometric and non-geometric anomalies, which is important in high-stakes applications where decision-making requires model explanations. In scientific domains including network science, biology, neurology, and transportation networks, where curvature-driven representations may aid in identifying anomalous patterns in actual systems, the knowledge gained via \modelname\ might be advantageous. 


\bibliography{example_paper}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn
\startcontents[subsection]
\addcontentsline{toc}{section}{\Large Table of Contents}  % Ensures it gets listed in the main Table of Contents
\printcontents[subsection]{l}{1}[3]{}

\newpage
\section{Appendix}
\subsection{Notation Table}
% \begin{table}
{\footnotesize{
\begin{tabularx}{\textwidth}{@{} p{3cm} X @{}}
\toprule
\textbf{Notation} & \textbf{Reference} \\
\midrule
% Add your table rows here
$\mathcal{G} = (\mathcal{V}, {\mathcal{E}}, \mathbf{A})$ & A graph $\mathcal{G}$ with set of edges $\mathcal{E}$ and vertices (nodes) $\mathcal{V}$\\
$\mathcal{M}$; $\mathcal{M}_p$ & A smooth manifold; $p^{th}$ component manifold in the product manifold ($\mathcal{M}_p \in \{\mathbb{E}, \mathbb{H}, \mathbb{S}\}$)\\
$\kappa_p$, $d_p$ & Curvature and dimension of $p^{th}$ manifold component\\
$\mathbf{exp}^{\kappa}_{\mathbf{0}} : \mathbb{R}^{d_{\mathcal{P}}} \rightarrow \mathcal{M}$ & Exponential map, to map from tangent plane (Euclidean) to the product manifold\\
$\mathbf{log}^{\kappa}_{\mathbf{0}} : \mathcal{M} \rightarrow \mathbb{R}^{d_{\mathcal{P}}} $ & Logarithmic map, to map from the manifold to the tangent plane\\
$\mathbb{H}$; $\mathbb{S}$; $\mathbb{E}$ & Hyperbolic manifold; Spherical manifold; Euclidean manifold\\
$\mathbb{P}^{d_{\mathcal{P}}}$ & Product manifold of dimension $d_{\mathcal{P}}$\\
$\kappa$; $\widetilde{\kappa}$& Continous manifold curvature; Discrete Ollivier-Ricci curvature (\texttt{ORC})\\
$\widetilde{\kappa}_{xy}$; $\widetilde{\kappa}_{xy}^{(t)}$ &  \texttt{ORC} of edge $\{x, y\}$, \texttt{ORC} of edge $\{x, y\}$ at $t^{th}$ step of Ricci flow algorithm\\
$m_x^{\delta}$ & Probability mass assigned to node $x$ for \texttt{ORC} computation\\
$\delta$ & Probability of the mass retained v/s distributed among neighbour nodes for ORC Computation\\
$\mathcal{N}(x)$& Neighbourhood set of node $x$\\
$x \sim y$ & This implies that $x$ and $y$ are adjacent nodes\\
$\mathbf{W}_1(.)$ & Wasserstein-1 (Earth mover's) distance\\
$d_{\mathcal{G}}(x, y)$ &Shortest path (graph distance) between nodes $x$ and $y$ on graph $\mathcal{G}$\\
$\mathcal{M}_{p}^{\kappa_p, d_p}$ & Constant-curvature manifold with dimension $d_p$ and curvature $\kappa_p$. $\mathcal{M}_{p} \in \{\mathbb{H}, \mathbb{S}, \mathbb{E}\}$\\
$\mathbf{L}_{\mathbb{P}}$ & Discrete Laplace-Beltrami operator\\
$\mathbf{U}_{\mathbb{P}} = \left[\{\mathbf{u}_l\}^{n - 1}_{l=0}\right]$ & Set of orthonormal eigenvectors of $\mathbf{L}_{\mathbb{P}}$\\
$\left[\{\lambda_l\}^{n - 1}_{l=0}\right] \in \mathbb{R}^{n}$ & Ordered real nonnegative eigenvalues associated with $\mathbf{U}_{\mathbb{P}}$\\
$\epsilon$ & Ollivier-Ricci flow step size\\
$\mathbf{w}_{xy}^{(t)}$ & Weight of the edge $xy$ at the $t^{th}$ iteration in Ricci flow algorithm.\\
$d_{\mathcal{X}}$&Input graph feature dimension\\
$d_{\mathcal{P}}$ &Total dimension of the product manifold\\
$\mathbf{X} \in \mathbb{R}^{n \times d_{\mathcal{X}}}$ & Input feature matrix\\
$\oplus_{\kappa}$ & \textit{Mobius} addition\\
$\otimes_{\kappa}$ & $\kappa$-right-matrix-multiplication\\
$\boxtimes_{\kappa}$ & $\kappa$-left-matrix-multiplication\\
$\mathbf{n}_{x}$ & Final (latent) node representation for node $x$ after encoder\\
$\mathbf{A}$; $\widetilde{\mathbf{A}}$ & Initial graph adjacency matrix, Reconstructed adjacency matrix\\
$\mathbf{X}$; $\widetilde{\mathbf{X}}$ & Initial graph feature matrix, Reconstructed feature matrix\\
$\mathbf{C}$; $\widetilde{\mathbf{C}}$ & Initial edge curvature matrix, Reconstructed curvature matrix\\
$\mathcal{L}_{\mathbf{C}}$ & Curvature decoder reconstruction loss\\
$\mathcal{L}_{\mathbf{A}}$ & Structure decoder reconstruction loss\\
$\mathcal{L}_{\mathbf{X}}$ & Feature decoder reconstruction loss\\
$\mathcal{L}_{\text{cls}}$ & Binary crossentropy supervised classification loss\\
$\lambda_{\text{cls}}$ & Weight of classification loss in total loss\\
$\mathcal{D}_{\mathcal{M}_p}$ & Geodesic distance over $p^{th}$ manifold component \\
$\mathcal{D}_{\mathcal{M}}(\mathbf{n}_x, \mathbf{n}_y)$ & Aggregated distance on the product manifold (i.e. sum over all component manifolds) \\
$\mathcal{K}(\mathbf{n}_x, \mathbf{n}_y)$ & Gaussian kernel used in curvature decoder over node embeddings $\mathbf{n}_x, \mathbf{n}_y$\\
$\beta_p$ & Learnable attentation weight for $p^{th}$ component manifold\\
$\alpha_f$ & Learnable attentation weight for $f^{th}$ filter in the filterbank\\
$\mathcal{F}; f$ & Total number of filters in the filterbank; Used to denote the $f^{th}$ filter\\
$\mathcal{P}; p$ & Total number of components in the product manifold; Used to denote the $p^{th}$ component\\
$\lambda_{\mathbf{X}}$, $\lambda_{\mathbf{C}}$, $\lambda_{\mathbf{A}}$, $\lambda_{\text{cls}}$ & Learnable trade-off parameters for loss components\\
$\sigma(.)$ & Sigmoid activation function\\
$\Delta$ & Stopping criterion threshold for Ollivier-Ricci flow\\
$\psi$ & Graph filter operator\\
$f_{\theta}(.)$ & Neural network (MLP) that generates hidden state Euclidean features of dimension $d_{\mathcal{P}}$\\
$\mathbf{x}$ &  A graph signal $\mathbf{x} : \mathcal{V} \rightarrow \mathbb{R}$\\
$\gamma$ & Kernel width hyperparameter for gaussian kernel (curvature decoder)\\
$\tau$ & Sensitivity parameter (learnable) for gaussian kernel (curvature decoder)\\
$\mathbf{Z}^{(f)}_{\mathcal{M}_p^{\kappa_p, d_p}}$ & Intermediate node representation from $f^{th}$ filter\\
$T_l(x)$ & Chebyshev polynomial of order $l$\\
$\Lambda_{\mathbb{P}}$ & Diagonal matrix of eigenvalues\\
$\phi_f$ & Learnable weights for filters, for the $f^{th}$ recursive level in Chebyshev encoder.\\
\bottomrule
\end{tabularx}}}
% \end{table}


\newpage

\section{More on Preliminaries}
\subsection{Product Manifolds} \label{app:product}

Consider a set of $\mathcal{P}$ smooth manifolds \( \mathcal{M}_1, \mathcal{M}_2, \dots, \mathcal{M}_{\mathcal{P}} \), each equipped with its own Riemannian structure. The \textbf{product manifold} \( \mathbb{P} \) is formed as their Cartesian product:
\begin{equation}
    \mathbb{P} = \mathcal{M}_1 \times \mathcal{M}_2 \times \dots \times \mathcal{M}_{\mathcal{P}}.
\end{equation}
This construction enables the representation of a point \( \mathbf{x} \in \mathbb{P} \) as a tuple:
\begin{equation}
    \mathbf{x} = (x_1, x_2, \dots, x_{\mathcal{P}}),
\end{equation}
where each \( x_p \) corresponds to a coordinate in the individual component manifold \( \mathcal{M}_p \).  

$\blacksquare$ \textbf{Tangent Space and Geodesic Movement}.
For any given \( \mathbf{x} \in \mathbb{P} \), the associated tangent space is given by:
\begin{equation}
    \mathbf{T}_{\mathbf{x}} \mathbb{P} = \mathbf{T}_{x_1} \mathcal{M}_1 \times \mathbf{T}_{x_2} \mathcal{M}_2 \times \dots \times \mathbf{T}_{x_{\mathcal{P}}} \mathcal{M}_{\mathcal{P}}.
\end{equation}
A tangent vector \( \mathbf{v} \in \mathbf{T}_{\mathbf{x}} \mathbb{P} \) is similarly represented as:
\begin{equation}
    \mathbf{v} = (v_1, v_2, \dots, v_{\mathcal{P}}),
\end{equation}
where each \( v_p \in \mathbf{T}_{x_p} \mathcal{M}_p \) represents the directional component along the corresponding manifold \( \mathcal{M}_p \).

Since optimization over Riemannian manifolds requires movement along the manifold surface rather than within the tangent space, we employ the \textbf{exponential map}, which maps a tangent vector \( \mathbf{v} \) at \( \mathbf{x} \) back onto the manifold:
\begin{equation}
    \mathbf{exp}_{\mathbf{x}}: \mathbf{T}_{\mathbf{x}} \mathbb{P} \to \mathbb{P}.
\end{equation}
For a product manifold, this mapping decomposes across individual components, leading to:
\begin{equation}
    \mathbf{exp}_{\mathbf{x}}(\mathbf{v}) = (\mathbf{exp}_{x_1}(v_1), \mathbf{exp}_{x_2}(v_2), \dots, \mathbf{exp}_{x_{\mathcal{P}}}(v_{\mathcal{P}})).
\end{equation}
This property allows optimization steps to be performed independently along each manifold component, significantly simplifying Riemannian optimization in mixed-curvature spaces. We use the \textbf{Riemannian Adam} optimizer from Geoopt (\texttt{https://github.com/geoopt/geoopt}) Library for all our experimentation.

$\blacksquare$ \textbf{Curvature and Manifold Selection}.  
The choice of component manifolds in \( \mathbb{P} \) significantly influences model performance. Hyperbolic spaces (\(\mathbb{H}\)) are well-suited for hierarchical structures, spherical spaces (\(\mathbb{S}\)) capture cyclical relations, while Euclidean spaces (\(\mathbb{E}\)) are ideal for line graphs (and provide a standard representation). The flexibility of mixed-curvature embeddings allows for adaptive modeling across datasets with diverse underlying geometries.

\begin{remark}
    \modelname\ heuristically determines the optimal mixed-curvature manifold configuration (Algorithm \ref{alg:signature_identification}, Appendix \ref{app:signature}) for a given dataset, ensuring that embeddings conform to the dataset’s intrinsic geometric properties.
\end{remark}

\begin{remark}
    The curvatures of the manifold components are dynamically \textbf{learnt} during the training process.
\end{remark}

By leveraging a product of Riemannian manifolds, our approach effectively models complex graphs where curvature varies locally, leading to superior anomaly detection across diverse graph topologies.




\subsection{$\kappa-$Stereographic Model} \label{app:kappa}

To effectively model mixed-curvature embeddings, we adopt the \(\kappa\)-stereographic model \citep{bachmann2020constant} instead of adopting separate formulations for hyperbolic and spherical spaces. This unified framework provides a consistent mathematical formulation across varying curvature regimes, enabling smooth interpolation between hyperbolic (\(\kappa < 0\)), spherical (\(\kappa > 0\)), and Euclidean (\(\kappa = 0\)) geometries. Efficient algebraic operations are essential for optimizing over non-Euclidean spaces, and the \(\kappa\)-stereographic model offers closed-form expressions for key Riemannian operations.

$\blacksquare$ \textbf{Manifold Definition}.
The \(\kappa\)-stereographic manifold of curvature \(\kappa\) and dimension \(d\) is defined as:
\begin{equation}
    \mathcal{M}^{\kappa, d} = \left\{\mathbf{z} \in \mathbb{R}^d \mid -\kappa \|\mathbf{z}\|_2^2 < 1 \right\},
\end{equation}
where \(\mathbf{z} \in \mathbb{R}^d\) represents a point in the stereographic space, and the curvature parameter \(\kappa\) governs the underlying geometry: \(\kappa < 0\) corresponds to hyperbolic manifold (\(\mathbb{H}^d\)), \(\kappa > 0\) corresponds to a spherical manifold (\(\mathbb{S}^d\)) and \(\kappa = 0\) reduces to Euclidean space (\(\mathbb{E}^d\)). The Riemannian metric tensor associated with \(\mathcal{M}^{\kappa, d}\) at a point \(\mathbf{z}\) is:
\begin{equation}
    g_{\mathbf{z}}^\kappa = (\lambda_{\mathbf{z}}^\kappa)^2 \mathbf{I},
\end{equation}
where the conformal factor \(\lambda_{\mathbf{z}}^\kappa\) is defined as:
\begin{equation}
    \lambda_{\mathbf{z}}^\kappa = 2 \left( 1 + \kappa \|\mathbf{z}\|_2^2 \right)^{-1}.
\end{equation}
This ensures a smooth transition between different curvature settings.

$\blacksquare$ \textbf{Algebraic Operations}. To perform optimization and inference on the manifold, we require efficient algebraic formulations for fundamental operations such as vector addition, scalar multiplication, and distance computation. Table \ref{tab:kops} summarizes the key operations under the \(\kappa\)-stereographic model.


\begin{table*}[t]
\small
\centering
\begin{tabular}{l|c|c}
\hline
\textbf{Operation} & \textbf{Formalism in $\mathbb E^d$} &\textbf{Unified formalism in $\kappa$-stereographic model ($\mathbb H^d$/ $\mathbb S^d$)}\\
\hline
Distance Metric& 
$d^\kappa_{\mathcal M}(\mathbf{x}, \mathbf{y}) =\left\| \mathbf{x}- \mathbf{y}\right\|_{2}$
&
$
d^\kappa_{\mathcal M}(\mathbf{x}, \mathbf{y})=\frac{2}{\sqrt{|\kappa|}} \tan _{\kappa}^{-1}\left(\sqrt{|\kappa|}\left\|-\mathbf{x} \oplus_{\kappa} \mathbf{y}\right\|_{2}\right)
$
\\
\hline
Exp. Map & 
$\mathbf{exp}_{\mathbf{x}}^{\kappa}(\mathbf{v})=\mathbf{x}+\mathbf{v}$ 
&
$
\mathbf{exp}_{\mathbf{x}}^{\kappa}(\mathbf{v})=\mathbf{x} \oplus_{\kappa}\left(\tan _{\kappa}\left(\sqrt{|\kappa|} \frac{\lambda_{\mathbf{x}}^{\kappa}\|\mathbf{v}\|_{2}}{2}\right) \frac{\mathbf{v}}{\sqrt{|\kappa|}\|\mathbf{v}\|_{2}}\right)
$
\\
Log. Map & 
$\mathbf{log}_{\mathbf{x}}^{\kappa}(\mathbf{y})= \mathbf{x}-\mathbf{y}$
&
$
\mathbf{log}_{\mathbf{x}}^{\kappa}(\mathbf{y})=\frac{2}{\sqrt{|\kappa|} \lambda_{\mathbf{x}}^{\kappa}} \tan _{\kappa}^{-1}\left(\sqrt{|\kappa|}\left\|-\mathbf{x} \oplus_{\kappa} \mathbf{y}\right\|_{2}\right) \frac{-\mathbf{x} \oplus_{\kappa} \mathbf{y}}{\left\|-\mathbf{x} \oplus_{\kappa} \mathbf{y}\right\|_{2}}
$
\\
\hline
Addition & 
$\mathbf{x} \oplus_{\kappa} \mathbf{y}=\mathbf{x} + \mathbf{y}$
&
$
\mathbf{x} \oplus_{\kappa} \mathbf{y}=\frac{\left(1+2 \kappa \mathbf{x}^{T} \mathbf{y}+K\|\mathbf{y}\|^{2}\right) \mathbf{x}+\left(1-\kappa || \mathbf{x}||^{2}\right) \mathbf{y}}{1+2 \kappa \mathbf{x}^{T} \mathbf{y}+\kappa^{2}|| \mathbf{x}||^{2}|| \mathbf{v}||^{2}}
$\\
\hline
\end{tabular}
\caption{Operations in Hyperbolic $\mathbb H^d$, Spherical $\mathbb S^d$ and Euclidean space $\mathbb E^d$.}
\label{tab:kops}
\end{table*}


\subsubsection{$\mathbf{\kappa}$-right-matrix-multiplication \citep{bachmann2020constant}} For a matrix $\mathbf{X} \in \mathbb{R}^{n \times d}$ carrying $\kappa$-stereographic embeddings across its rows, and weights represented by $\mathbf{W} \in \mathbb{R}^{d \times e}$, the operation of Euclidean right multiplication can be decomposed into individual rows as $(\mathbf{X} \mathbf{W})_{i \bullet} = \mathbf{X}_{i \bullet} \mathbf{W}$. Consequently, the $\mathbf{\kappa}$-right-matrix-multiplication is defined in the same row-wise fashion as
\begin{align}
        (\mathbf{X} \otimes_{\kappa} \mathbf{W})_{i\bullet} &= \exp_{0}^{\kappa}\left((\log_{0}^{\kappa}(\mathbf{X})\mathbf{W})_{i\bullet}\right)\\
        &=\tan_\kappa \left(\alpha_{i}\tan_\kappa^{-1}(||\mathbf{X}_{\bullet i}||)\right)\frac{(\mathbf{X}\mathbf{W})_{i\bullet}}{||(\mathbf{X}\mathbf{W})_{i\bullet}||}
\end{align}

where $\alpha_{i} = \frac{||(\mathbf{X}\mathbf{W})_{i\bullet}||}{||\mathbf{X}_{i\bullet}||}$ and $\exp_{0}^{\kappa}$ and $\log_{0}^{\kappa}$ denote the exponential and logarithmic map in the $\kappa$-stereographic model.


\subsection{Ollivier-Ricci Curvature (ORC)}\label{app:orc}
In an unweighted graph, the set of nodes adjacent to a given node $x$, denoted by $\mathcal{N}(x)$, is represented through a probability distribution following a lazy random walk model \citep{lin-yau}. This distribution is defined by the equation:
\begin{equation}\label{eq:orc-lazy}
m_z^{\delta}(x) = \begin{cases}
\delta, & \text{if } z = x, \\
\frac{1-\delta}{|\mathcal{N}(x)|}, & \text{if } z \in \mathcal{N}(x), \\
0, & \text{otherwise}.
\end{cases}
\end{equation}
The parameter $\delta$ determines the probability of staying at the current node, with the rest of the probability $(1-\delta)$ being evenly allocated among its neighbors. This approach links ORC with lazy random walks, affecting the interaction between local exploration and node revisitation. In our study, we set $\delta = 0.5$, resulting in an equal division of probability between the node and its neighbors, creating a balance between \textit{breadth-first} and \textit{depth-first} search methodologies. The value of $\delta$ is significant and varies based on the graph's structure. A lower $\delta$ value promotes more exploration within the local neighborhood, whereas a higher $\delta$ fosters node revisits, supporting the "lazy" aspect of the walk. For the experiments, $\delta = 0.5$ was chosen to ensure an equal division of probability between the node and its adjacent nodes.

\section{Time Complexity Analysis} \label{app:time_complexity}

\subsection{Preprocessing Complexity}

\subsubsection{Computational Considerations for ORC}

A crucial preprocessing step in \modelname\ is the computation of the \textbf{Ollivier-Ricci Curvature (ORC)} and subsequent \textbf{Ricci Flow} regularization. These computations serve as the foundation for both the curvature-invariant and curvature-equivariant pipelines, making their efficiency paramount to overall scalability. We precompute both these to save computation time as they are used in both the \modelname\ pipelines.

Computing ORC involves determining (in this case, approximating) the \textit{Wasserstein-1 distance} (\(\mathbf{W}_1\)) between the probability distributions of neighboring nodes. In discrete settings, this requires solving an optimal transport problem. The standard approach relies on the \textit{Hungarian algorithm} \citep{kuhn1955hungarian}, which operates in \(\mathcal{O}(d^3)\) time per node, making it computationally prohibitive for large graphs. 

To mitigate this cost, the \textit{Sinkhorn algorithm} \citep{sinkhorn1967concerning} is often used as a relaxation, reducing complexity to \(\mathcal{O}(d^2)\) per node. However, this still remains impractical for million-scale graphs. \textbf{In this work, we adopt a linear-time approximation for ORC}, allowing \modelname\ to scale efficiently while retaining interpretability.

\subsubsection{Approximating ORC in Linear Time}

Instead of directly solving the optimal transport problem, we employ a \textit{combinatorial approximation} for ORC, inspired by \citet{tian2023curvature}. This approach estimates Wasserstein distances using local structural properties, significantly reducing computational overhead. The approximation is derived from the analytical curvature bounds proposed by \citet{jost2014ollivier}, which leverage local connectivity features such as \textit{node degrees} and \textit{triangle counts}. Let \( \#(x, y) \) represent the number of triangles involving the edge \( (x, y) \), and define $a \wedge b = \min(a, b), \quad a \vee b = \max(a, b)$,
where \( d_x \) denotes the degree of node \( x \). The Ollivier-Ricci curvature for edge \( (x, y) \) is then bounded as follows:

\begin{theorem}[\citet{jost2014ollivier}]
\label{the:ollilow}
For an unweighted graph, the Ollivier-Ricci curvature of edge \( e = (x, y) \) satisfies:

\begin{enumerate}
    \item \textbf{Lower bound:}
    \begin{align}\label{eq:ollilow}
        \widetilde{\kappa}(x, y) &\geq - \left( 1 - \frac{1}{d_x} - \frac{1}{d_y} - \frac{\#(x, y)}{d_x \wedge d_y} \right)_{+} \\
        &- \left( 1 - \frac{1}{d_x} - \frac{1}{d_y} - \frac{\#(x, y)}{d_x \vee d_y} \right)_{+} + \frac{\#(x, y)}{d_x \vee d_y}.\nonumber
    \end{align}
    
    \item \textbf{Upper bound:}
    \begin{equation}\label{eq:olliup}
        \widetilde{\kappa}(x, y) \leq \frac{\#(x, y)}{d_x \vee d_y}.
    \end{equation}
\end{enumerate}
\end{theorem}

The curvature of an edge can then be efficiently approximated as:
\begin{equation}\label{eq:orc-approx}
    \widehat{\kappa}(x, y) := \frac{1}{2} \left( \kappa^{\text{upper}}(x, y) + \kappa^{\text{lower}}(x, y) \right).
\end{equation}

$\blacksquare$ \textbf{Computational Complexity.} This approximation scheme runs in \textbf{linear time} \(\mathcal{O}(|\mathcal{E}|)\), where \(|\mathcal{E}|\) is the number of edges. Since each edge's curvature is estimated using only local information (node degrees and triangles), the process is inherently \textbf{parallelizable} across edges, making it well-suited for large-scale graphs.

\subsubsection{Ricci Flow Complexity}

Ricci Flow is an iterative process where edge weights are adjusted according to the ORC values, smoothing the curvature distribution over time. Each iteration requires one full computation of ORC, making the total complexity: $\mathcal{O}(I |\mathcal{E}|)$, where \( I \) is the number of iterations required for convergence. Empirically, across the datasets used in this study, Ricci Flow converges in 12–13 iterations on average. Since \( I \) is much smaller than the total number of edges (\( I \ll |\mathcal{E}| \)), the process remains linear in complexity:, i.e. $\mathcal{O}(I |\mathcal{E}|) \approx \mathcal{O}(|\mathcal{E}|)$ as \( I \) is fixed across datasets.

\subsubsection{Summary of Preprocessing Complexity}
\begin{itemize}
    \item Ollivier-Ricci Curvature (ORC): \( \mathcal{O}(|\mathcal{E}|) \) (linear time)
    \item Ricci Flow Regularization: \( \mathcal{O}(|\mathcal{E}|) \) (empirically bounded by a constant number of iterations)
    \item Total Preprocessing Complexity: \( \mathcal{O}(|\mathcal{E}|) \) (\textbf{parallelizable} and scalable)
\end{itemize}

\subsection{Runtime Comparison}


\begin{figure}[h]
\centering
  \includegraphics[width=\linewidth]{curvgad_runtime.pdf}
  \vspace{-10mm}
  \caption{\textbf{Runtime comparision of \modelname}. Average runtime per epoch (training + inference) in \textit{seconds} (log scale) on Reddit \cite{BOND}, Weibo \cite{zhao2020error}, Tolokers \cite{HeteroBench}, and Questions \cite{HeteroBench} datasets, for select baselines.}
  \label{fig:runtime}
\end{figure}

\begin{table*}[h]
\centering
\scriptsize
\setlength{\tabcolsep}{4pt}
\renewcommand{\arraystretch}{1.1}
\label{tab:runtime_comparison}
\resizebox{\textwidth}{!}{%
\begin{tabular}{l|cccccccccccc|c}
\toprule
\textbf{Dataset} & \textbf{GCN} & \textbf{GraphSAGE} & \textbf{GAT} & \textbf{BernNet} & \textbf{KGCN} & \textbf{ChebNet} & \textbf{BGNN} & \textbf{BWGNN} & \textbf{PCGNN} & \textbf{DCI} & \textbf{GHRN} & \textbf{XGBOD} & \textbf{\modelname} \\
\midrule
\textbf{Reddit} & 34.37 & 10.19 & 17.55 & 11.27 & 158.45 & 9.52 & 507.41 & 10.52 & 12.50 & 150.61 & 11.70 & 3050.66 & 585.22 \\
\textbf{Weibo} & 6.95 & 8.58 & 10.63 & 7.65 & 99.34 & 5.13 & 1415.59 & 10.63 & 6.74 & 121.77 & 11.06 & 4889.16 & 276.70 \\
\textbf{Tolokers} & 10.12 & 9.02 & 12.17 & 10.80 & 105.23 & 9.16 & 295.89 & 11.94 & 10.37 & 149.58 & 14.08 & 1432.76 & 260.30 \\
\textbf{Questions} & 21.56 & 27.12 & 22.85 & 18.53 & 183.53 & 23.77 & 1972.41 & 25.41 & 21.44 & 408.37 & 26.55 & 188192.5 & 589.41 \\
\bottomrule
\end{tabular}
}\caption{\textbf{Runtime Comparison (milliseconds)} of \modelname\ and baseline models across multiple datasets. This table shows the exact numbers used in Figure \ref{fig:runtime}. The results are shown for \modelname\ model with $\mathcal{F}=8$ filters and product manifold $(\mathbb{H}^{8})^2\times \mathbb{S}^{16} \times \mathbb{E}^{16}$.}
\end{table*}


\subsubsection{Runtime Analysis}

The runtime comparison in Table \ref{tab:runtime_comparison} provides key insights into the computational efficiency of \modelname\ relative to baseline methods. Our results report the execution time in milliseconds for \modelname\ configured with $\mathcal{F} = 8$ spectral filters and a mixed-curvature product manifold $(\mathbb{H}^{8})^2 \times \mathbb{S}^{16} \times \mathbb{E}^{16}$. Despite leveraging multiple curvature spaces and graph filters for representation learning, \modelname\ demonstrates reasonable computational efficiency. For reference, when analysing Figure \ref{fig:runtime}, GCN and BerNet can be thought of as using just one graph filter and KGCN can be considered as one manifold for learning representations (when compared to \modelname\ with 8 filters and 4 manifolds).

\begin{itemize}
    \item \textbf{Scalability with mixed-curvature manifolds.} Unlike KGCN, which operates on a single curvature space, \modelname\ effectively integrates four manifold components. Nevertheless, its runtime remains within an acceptable range, significantly outperforming KGCN across all datasets. For instance, on Reddit, KGCN requires 158.45ms, while \modelname\ completes execution in 585.22ms, despite utilizing a far more expressive model.
	\item \textbf{Efficiency compared to BGNN and XGBOD.} \modelname\ remains substantially faster than models like BGNN and XGBOD. On the Weibo dataset, BGNN exhibits an extreme runtime of 1415.59ms, whereas \modelname\ operates at only 276.70ms. Similarly, on Tolokers, BGNN requires 295.89ms, compared to 260.30ms for \modelname, making it significantly more efficient despite handling curvature-sensitive graph learning.
	\item \textbf{Reasonable overhead given model complexity.} The additional computational cost incurred by \modelname\ is expected due to its curvature-based learning framework, which integrates Ricci flow, manifold embeddings, and spectral filtering. However, the runtime remains competitive, ensuring that the benefits of richer geometric representations do not come at an impractical computational expense.
\end{itemize}
Overall, \modelname\ balances computational efficiency and model expressiveness, outperforming single-curvature models while maintaining a significantly lower runtime than complex baselines like BGNN and XGBOD. Its efficiency relative to KGCN and its comparable performance to DCI suggest that the inclusion of curvature-based representations does not drastically hinder scalability, making it a viable choice for large-scale graph anomaly detection tasks.


\section{Arcitectural Details}

\subsection{Definition \ref{def:curv}: Intuition and Derivation}\label{app:dec_curv}

The curvature decoder in Section \ref{sec:equi_dec} reconstructs the Ollivier-Ricci curvature (ORC) $\mathbf{C}_{xy}$ between nodes $x$ and $y$ using their latent embeddings $\mathbf{n}_x, \mathbf{n}_y \in \mathbb{P}^{d_\mathcal{M}}$. The key idea is to define a 	\textit{Gaussian kernel} on the geodesic distance within the product manifold to capture the intrinsic curvature information. In this section, we derive the decoder formulation and provide mathematical intuition.

\textbf{Motivation: Distance-based curvature estimation}. Curvature measures the deviation of a space from being flat. The Ollivier-Ricci curvature is based on the Wasserstein distance between local probability distributions around connected nodes $x$ and $y$. Intuitively, negatively curved spaces (e.g., hyperbolic manifolds) exhibit greater spread between transported probability masses, whereas positively curved spaces (e.g., spherical manifolds) result in concentrated transport.

Given that ORC is inherently tied to how distances between points behave in different curvature regimes, it is natural to model curvature reconstruction as a function of the geodesic distance $\mathcal{D}_{\mathcal{M}}(\mathbf{n}_x, \mathbf{n}_y)$. A widely adopted approach in Riemannian learning is to employ a Gaussian kernel, which serves as a smooth and flexible mechanism to relate distances to similarity metrics.

\textbf{Geodesic Distance on the Product Manifold}. For a product manifold $\mathbb{P}$ composed of $\mathcal{M}$ individual manifold components, the squared geodesic distance between node embeddings $\mathbf{n}_x$ and $\mathbf{n}_y$ is computed as:
\begin{equation}
    \mathcal{D}_{\mathcal{M}}(\mathbf{n}_x, \mathbf{n}_y)^2 = \sum_{p=1}^{\mathcal{M}} \mathcal{D}_{\mathcal{M}_p}(n_x, n_y)^2.
\end{equation}
This formulation ensures that distances are computed per manifold component and then aggregated across all curvature spaces.

\textbf{Gaussian Kernel Formulation}. To reconstruct the curvature, we define a Gaussian kernel based on the computed geodesic distance:
\begin{equation}
    \mathcal{K}(\mathbf{n}_x, \mathbf{n}_y) = \exp\left(-\gamma \frac{\mathcal{D}_{\mathcal{M}}(\mathbf{n}_x, \mathbf{n}_y)^2}{\tau^2}\right),
\end{equation}
where $\gamma$ controls the width of the kernel and $\tau$ is a trainable sensitivity parameter that adapts to the curvature distribution.

\textit{Intuition.} The kernel output is close to $1$ for small distances (nodes close in latent space) and approaches $0$ for large distances (nodes far apart). This behavior aligns with the principle that lower curvature corresponds to more spread-out distances in latent space.

\textbf{Sigmoid transformation for curvature prediction}. The predicted curvature $\widetilde{\mathbf{C}}_{xy}$ is derived from the kernel output via a sigmoid transformation:
\begin{equation}
    \widetilde{\mathbf{C}}_{xy} = 2 \cdot \texttt{sigmoid}\left(1 - \mathcal{K}(\mathbf{n}_x, \mathbf{n}_y)\right) - 1.
\end{equation}

\textit{Justification:} The sigmoid function normalizes the kernel output to the $[0,1]$ range, and the affine transformation maps the values into the $[-1,1]$ range, consistent with the properties of Ollivier-Ricci curvature.

\textbf{Optimization via reconstruction loss}. To ensure accurate curvature reconstruction, we optimize the decoder by minimizing the Frobenius norm loss:
\begin{equation}
    \mathcal{L}_{\mathbf{C}} = \left\|\widetilde{\mathbf{C}} - \mathbf{C} \right\|_F^2,
\end{equation}
where $\widetilde{\mathbf{C}}$ denotes the predicted curvature matrix and $\mathbf{C}$ is the true ORC matrix.

\textbf{Summary:} The decoder exploits the structure of the product manifold by computing geodesic distances, mapping them through a Gaussian kernel, and applying a transformation to ensure proper curvature scaling. This approach provides an effective means of capturing geometric distortions while maintaining computational efficiency.



\section{Experimental Details}
\subsection{Dataset Statistics} \label{app:stat}

\begin{table}[h]
\centering
\resizebox{\textwidth}{!}{
\begin{tabular}{lrrrrrll}
\toprule
                   & \textbf{\#Nodes}   & \textbf{\#Edges}    & \textbf{\#Feat}. & \textbf{Anomaly} & \textbf{Train} & \textbf{Relationship}   \\\midrule
{Reddit \cite{kumar2019predicting, BOND}}       & 10,984    & 168,016    & 64      & 3.3\%         & 40\%   & Under Same Post    \\
{Weibo \cite{zhao2020error, BOND}}              & 8,405     & 407,963    & 400     & 10.3\%          & 40\%  & Under Same Hashtag      \\
{Amazon \cite{mcauley2013amateurs,CareGNN}}     & 11,944    & 4,398,392  & 25      & 9.5\%          & 70\%   & Review Correlation      \\
{YelpChi \cite{anomaly_review,CareGNN}}         & 45,954    & 3,846,979  & 32      & 14.5\%       & 70\%  & Reviewer Interaction     \\
{Tolokers \cite{HeteroBench}}                   & 11,758    & 519,000    & 10      & 21.8\%        & 40\%  & Work Collaboration       \\
{Questions \cite{HeteroBench}}                      & 48,921    & 153,540    & 301     & 3.0\%         & 52\%   & Question Answering     \\
{T-Finance \cite{BWGNN}}  & 39,357    & 21,222,543 & 10      & 4.6\%        & 50\%   & Transaction Record     \\
{Elliptic \cite{weber2019anti}}  & 203,769   & 234,355    & 166     & 9.8\%       & 50\%   & Payment Flow            \\
{DGraph-Fin \cite{dgraph_dataset}}    & 3,700,550 & 4,300,999  & 17      & 1.3\%      & 70\%   & Loan Guarantor     \\
{T-Social \cite{BWGNN}}      & 5,781,065 & 73,105,508 & 10      & 3.0\%     & 40\%   & Social Friendship \\ \bottomrule
\end{tabular}
}
\vspace{1mm}
 \caption{Statistics of datasets used for evaluation. Percentage of anomalies and the respective train splits have been mentioned.}\label{tab:data}
\vspace{-5mm}
\end{table}


\subsection{Hyperparameter Tuning} \label{app:hyper_tuning}



\begin{table}[h]
\centering
\resizebox{0.8\textwidth}{!}{
\begin{tabular}{l|c|l}
\toprule
\textbf{Hyperparameter}       & \textbf{Tuning Configurations}                                              & \textbf{Description}                                             \\ \midrule
$\mathcal{F}$                 & $\{3, 5, {\color{red}8}, 10, 20, 25\}$            & Total number of graph filters.        \\
$\delta$                      & $\{0.2, {\color{red}0.5}, 0.7\}$                        & Neighbourhood weight distribution parameter for \texttt{ORC}\\
$d_{\mathcal{P}}$             & $\{32, {\color{red}48}, 64, 128, 256\}$                      & Total dimension of the product manifold.       \\
\texttt{dropout}              & $\{0.2, {\color{red}0.3}, 0.5\} $                            & Dropout rate  \\ 
\texttt{epochs}               & $\{100, {\color{red}150}, 1000\}$                                 & Number of training epochs  \\
\texttt{lr}                   & $\{1e-4, {\color{red}{2e-3}}, 0.001, 0.01\}$                      & Learning rate    \\
\texttt{weight\_decay}        & $\{0, 1e-4, {\color{red}5e-4}\}$                             & Weight decay   \\                  
\bottomrule
\end{tabular}
}
\caption{Hyperparameter configurations used in the experiments for \modelname\. We highlight the final configuration of \modelname\ in \color{red}{Red}.}
\vspace{-5mm}
\label{tab:hyperparams_full}
\end{table}


\section{Signature Estimation}\label{app:signature}

The mixed-curvature product manifold $\mathbb{P}^{d_{\mathcal{P}}}$  is fundamental in capturing the geometric characteristics of graph-structured data. The optimal configuration of hyperbolic, spherical, and Euclidean components within this manifold is \textit{dataset-dependent}, influenced by the underlying curvature distribution of the graph. To generalize across datasets, we estimate an appropriate manifold signature by leveraging Ollivier-Ricci Curvature (\texttt{ORC}) as a guiding metric.

Our heuristic-driven approach assigns a spherical component to datasets exhibiting a predominance of positively curved edges, while a hyperbolic component is prioritized for datasets with a surplus of negatively curved edges. The stepwise procedure for this signature selection is formalized in Algorithm \ref{alg:signature_identification} [\textbf{\texttt{citation-withheld}}]\footnote{[\textbf{\texttt{citation-withheld}}] is used to respect the double blind policy for one of our previous works, which is still under review at the time of submission. Appropriate citation shall be inserted later. This algorithm has been adopted from our previous work.}. By systematically analyzing the curvature distribution, our algorithm approximates a suitable manifold signature that aligns with the dataset’s intrinsic geometric structure.

To ensure robustness, we cluster the ORC distribution, identifying curvature centroids while preserving their sequence and frequency. This heuristic approach provides a structured methodology to inform curvature-aware representation learning. However, due to the inherent variability in optimal dimension allocation, we treat the dimensions of component manifolds as a tunable hyperparameter rather than an automatically inferred property. This strategy ensures fair and reproducible comparisons across datasets, as the ideal manifold configuration varies per dataset. While we do not claim that this heuristic yields the absolute optimal manifold decomposition, it serves as an effective approximation, aligning curvature-sensitive embeddings with the dataset’s topological properties to enhance graph anomaly detection.

\begin{algorithm}[H]
\caption{Product manifold signature estimation}
\label{alg:signature_identification}
\footnotesize
\begin{algorithmic}[1]
\Require
\begin{itemize}
    \item Edge curvature histogram \( \mathcal{C} = \{ (\kappa_i, f_i) \}_{i=1}^{N} \)
    \item Threshold \( \epsilon' \) to distinguish between curved and flat regions
    \item Maximum number of Hyperbolic $(\mathcal{H}_{\max})$ and Spherical $( \mathcal{S}_{\max})$ components.
    \item Total product manifold dimension \( d_{\mathcal{P}} \)
    \item (Optional) Preferred component manifold dimensions \( d_{(h)}^{\text{pre}} ,  d_{(s)}^{\text{pre}}, d_{(e)}^{\text{pre}} \)
\end{itemize}
\Ensure Product manifold signature \( \mathbb{P}^{d_{\mathcal{P}}} = \times_{p=1}^{\mathcal{P}} \mathcal{M}_q^{\kappa_{p}, d_{p}} \)
\Statex

\State Normalize frequencies: \( f'_i = \frac{f_i}{\sum_{j=1}^{N} f_j} \)
\State Construct weighted curvature set: \( \mathcal{C}' = \{ (\kappa_i, f'_i) \}_{i=1}^{N} \)
\State Determine optimal number of clusters \( K \) using methods like the elbow method, constrained by \( K \leq \mathcal{H}_{\max} + \mathcal{S}_{\max} + 1\) \Comment{There can be only 1 Euclidean component}
\State Cluster \( \mathcal{C}' \) into \( K \) clusters using weighted clustering (e.g., weighted K-means) 
\State Initialize empty lists \( \mathcal{H}, \mathcal{S}, \mathcal{E} \)
\For{each cluster \( c \) in clusters}
    \State Compute cluster centroid curvature \( \kappa_c = \frac{\sum_{(\kappa_i, f'_i) \in c} \kappa_i }{|c|}\)
    \State Compute total frequency weight \( w_c = \sum_{(\kappa_i, f'_i) \in c} f'_i \)
    \If{\( \kappa_c < -\epsilon' \) \textbf{and} \( |\mathcal{H}| \leq \mathcal{H}_{\max} \)} \Comment{Negative curvature}
        \State Assign manifold component:  $\mathcal{M}_p = \mathbb{H}^{\kappa_c}$  \Comment{Curvature initialization}
        \State Add  $(\mathcal{M}_p, w_c)$  to  $\mathcal{H}$
    \ElsIf{\( \kappa_c > \epsilon' \) \textbf{and} \( |\mathcal{S}| \leq \mathcal{S}_{\max} \)} \Comment{Positive curvature}
        \State Assign manifold component:  $\mathcal{M}_p = \mathbb{S}^{\kappa_c}$  \Comment{Curvature initialization}
        \State Add  $(\mathcal{M}_p, w_c)$  to  $\mathcal{S}$
    \Else
        \State Assign manifold component:  $\mathcal{M}_p = \mathbb{E}$ \Comment{Approximate zero curvature, i.e. $\kappa_c \in [-\epsilon', \epsilon']$}
        \State Add  $(\mathcal{M}_p, w_c)$  to  $\mathcal{E}$ 
    \EndIf
\EndFor

\If{Predefined dimensions \( d_{(h)}^{\text{pre}} ,  d_{(s)}^{\text{pre}}, d_{(e)}^{\text{pre}} \) are provided}
    \State Assign dimensions \( d_{p} \) to each component \( p \) as per predefined values \Comment{Dimension assignment}
\Else
    \State Set total number of components \( \mathcal{Q} = |\mathcal{H}| + |\mathcal{S}| + |\mathcal{E}| \) \Comment{Dimension assignment}
    \State Allocate dimensions \( d_{p} \) to each component \( p \):  $ d_{p} = \left\lfloor d_{\mathcal{P}} \times \frac{w_p}{\sum_{p=1}^{\mathcal{Q}} w_p} \right\rfloor$ \Comment{Proportional to weights}
    \State Adjust \( d_{p} \) to ensure \( \sum_{q=1}^{\mathcal{P}} d_{p} = d_{\mathcal{P}} \)
\EndIf
\State Formulate manifold signature:
\[
\mathbb{P}^{d_{\mathcal{P}}} = \left( \times_{h=1}^{|\mathcal{H}|} \mathbb{H}_{h}^{\kappa_{(h)}, d_{(h)}} \right) \times \left( \times_{s=1}^{|\mathcal{S}|} \mathbb{S}_{s}^{\kappa_{(s)}, d_{(s)}} \right) \times \mathbb{E}^{d_{(e)}}
\]
\end{algorithmic}
\end{algorithm}
\stopcontents[subsection]
\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
