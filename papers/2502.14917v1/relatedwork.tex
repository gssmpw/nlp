\section{Related Works}
\label{submission}

\subsection{Multimodal Large Language Models}

Recent advancements have seen the creation of MLLMs. Flamingo \cite{alayrac2022flamingo} and BLIP2 \cite{li2023blip} align visual features with the embedding space of LLM through gated attention and Q-Former, while LLaVA \cite{liu2024improved} and MiniGPT4 \cite{zhu2023minigpt} use Multilayer Perceptrons (MLPs) to combine pretrained vision module with LLM backbone. Additionally, some studies have attempted to extend modality interaction to video and audio. Video-LLaVA \cite{lin2023video} employs a LanguageBind encoder to pre-align different visual features to the text space, facilitating joint training for images and videos. Video-Llama \cite{zhang2023video} achieves joint processing of visual and auditory signals in video data by integrating pretrained visual and audio encoder into the LLM.

\subsection{Autonomous Driving With Multimodal Large Language Models}

MLLMs have demonstrated the potential to understand traffic scenes, optimize driving decisions, and fundamentally improve human-vehicle interactions. Compared to traditional AD perception systems, MLLMs offer a new paradigm, leveraging their inherent few-shot learning capabilities to rapidly learn from vast amounts of multimodal data, thereby providing richer supervision sources. PromptTrack \cite{wu2023language} integrates cross-modal features into language prompts as semantic cues, combined with MLLM for 3D detection and tracking.  Talk2BEV \cite{choudhary2023talk2bev} combines BEV images with language prompts, using MLLM for audiovisual integration in AD. For end-to-end driving, MLLMs also exhibit better interpretability and trustworthiness. DriveGPT4 \cite{xu2024drivegpt4} pioneers using MLLM to transform sensor data and instructions into control signals and text responses. RAG-Driver \cite{yuan2024rag} proposes a retrieval-augmented MLLM that generates driving behavior justifications and predicts control signals by retrieving expert demonstrations. DriveVLM \cite{tian2024drivevlm} integrates the cognitive chain module into the MLLM, enabling driving scene description and motion planning. However, Existing research has yet to align MLLMs with the implicit cognitive chain of human driving, enabling reasoning from combined global and local scene understanding to behavior, trajectory, and control commands, limiting cross-scene generalization and human-consensus driving.

\subsection{Visual Question Answering Datasets}

To support the efficient training of MLLMs, the design of large-scale VQA datasets has become a research hotspot. Currently, various VQA datasets exist, including image-based datasets such as CLEVR \cite{johnson2017clevr}, VQA2.0 \cite{goyal2017making}, and EQA \cite{das2018embodied}, as well as video-based datasets like TVQA \cite{lei2018tvqa}, TGIF-QA \cite{jang2017tgif}, and ActivityNet-QA \cite{yu2019activitynet}. For ImageQA, early studies \cite{johnson2017clevr,fukui2016multimodal} attempted to fuse image features extracted by Convolutional Neural Networks (CNNs) with question encodings, which were then fed into decoders for answer generation. Recently, Transformer-based models \cite{tan2019lxmert,zhang2021vinvl} have achieved state-of-the-art performance in ImageQA tasks. Through attention networks, some studies have effectively captured the intrinsic relationships between temporal context and spatial features in video frames. 3D QA is a novel task in the VQA domain, focusing on answering questions about 3D-view scenes, requiring models to understand the geometric structures and spatial relationships of objects. Recently, many 3D QA datasets have been constructed, such as 3DQA \cite{ye2022visatlas}, ScanQA \cite{azuma2022scanqa}, and SQA3D \cite{ma2022sqa3d}. Despite significant progress in the VQA community, challenges remain when dealing with complex traffic scenes involving multimodal, multi-view, and multi-frame contexts. Moreover, the AD field currently lacks comprehensive VQA driving datasets.