\section{Related Works}
\label{submission}

\subsection{Multimodal Large Language Models}

Recent advancements have seen the creation of MLLMs. Flamingo ____ and BLIP2 ____ align visual features with the embedding space of LLM through gated attention and Q-Former, while LLaVA ____ and MiniGPT4 ____ use Multilayer Perceptrons (MLPs) to combine pretrained vision module with LLM backbone. Additionally, some studies have attempted to extend modality interaction to video and audio. Video-LLaVA ____ employs a LanguageBind encoder to pre-align different visual features to the text space, facilitating joint training for images and videos. Video-Llama ____ achieves joint processing of visual and auditory signals in video data by integrating pretrained visual and audio encoder into the LLM.

\subsection{Autonomous Driving With Multimodal Large Language Models}

MLLMs have demonstrated the potential to understand traffic scenes, optimize driving decisions, and fundamentally improve human-vehicle interactions. Compared to traditional AD perception systems, MLLMs offer a new paradigm, leveraging their inherent few-shot learning capabilities to rapidly learn from vast amounts of multimodal data, thereby providing richer supervision sources. PromptTrack ____ integrates cross-modal features into language prompts as semantic cues, combined with MLLM for 3D detection and tracking.  Talk2BEV ____ combines BEV images with language prompts, using MLLM for audiovisual integration in AD. For end-to-end driving, MLLMs also exhibit better interpretability and trustworthiness. DriveGPT4 ____ pioneers using MLLM to transform sensor data and instructions into control signals and text responses. RAG-Driver ____ proposes a retrieval-augmented MLLM that generates driving behavior justifications and predicts control signals by retrieving expert demonstrations. DriveVLM ____ integrates the cognitive chain module into the MLLM, enabling driving scene description and motion planning. However, Existing research has yet to align MLLMs with the implicit cognitive chain of human driving, enabling reasoning from combined global and local scene understanding to behavior, trajectory, and control commands, limiting cross-scene generalization and human-consensus driving.

\subsection{Visual Question Answering Datasets}

To support the efficient training of MLLMs, the design of large-scale VQA datasets has become a research hotspot. Currently, various VQA datasets exist, including image-based datasets such as CLEVR ____, VQA2.0 ____, and EQA ____, as well as video-based datasets like TVQA ____, TGIF-QA ____, and ActivityNet-QA ____. For ImageQA, early studies ____ attempted to fuse image features extracted by Convolutional Neural Networks (CNNs) with question encodings, which were then fed into decoders for answer generation. Recently, Transformer-based models ____ have achieved state-of-the-art performance in ImageQA tasks. Through attention networks, some studies have effectively captured the intrinsic relationships between temporal context and spatial features in video frames. 3D QA is a novel task in the VQA domain, focusing on answering questions about 3D-view scenes, requiring models to understand the geometric structures and spatial relationships of objects. Recently, many 3D QA datasets have been constructed, such as 3DQA ____, ScanQA ____, and SQA3D ____. Despite significant progress in the VQA community, challenges remain when dealing with complex traffic scenes involving multimodal, multi-view, and multi-frame contexts. Moreover, the AD field currently lacks comprehensive VQA driving datasets.