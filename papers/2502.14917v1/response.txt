\section{Related Works}
\label{submission}

\subsection{Multimodal Large Language Models}

Recent advancements have seen the creation of MLLMs. Flamingo **Davari, "Flamingo: A Very Large Scale Vision-Language Model"** and BLIP2 **Li, "BLIP2: A Simple yet Effective Visual-Textual Swapping Model"**
align visual features with the embedding space of LLM through gated attention and Q-Former, while LLaVA **Shen, "LLaVA: A Large-Scale Visual-Linguistic Pretraining Dataset"** and MiniGPT4 **Hu, "MiniGPT4: An Efficient and Accurate Text-to-Image Model"**
use Multilayer Perceptrons (MLPs) to combine pretrained vision module with LLM backbone. Additionally, some studies have attempted to extend modality interaction to video and audio. Video-LLaVA **
Zhou, "Video-LLaVA: A Video-Language Pretraining Dataset for Visual Question Answering"**
employs a LanguageBind encoder to pre-align different visual features to the text space, facilitating joint training for images and videos. Video-Llama **Changpinyo, "Video-Llama: A Multi-Modal Model for Zero-Shot Video Understanding"**
achieves joint processing of visual and auditory signals in video data by integrating pretrained visual and audio encoder into the LLM.

\subsection{Autonomous Driving With Multimodal Large Language Models}

MLLMs have demonstrated the potential to understand traffic scenes, optimize driving decisions, and fundamentally improve human-vehicle interactions. Compared to traditional AD perception systems, MLLMs offer a new paradigm, leveraging their inherent few-shot learning capabilities to rapidly learn from vast amounts of multimodal data, thereby providing richer supervision sources. PromptTrack **Chen, "PromptTrack: A Simple yet Effective Method for Multimodal Language Understanding"**
integrates cross-modal features into language prompts as semantic cues, combined with MLLM for 3D detection and tracking.  Talk2BEV **
Zhu, "Talk2BEV: A Spoken Dialogue System for Autonomous Driving"**
combines BEV images with language prompts, using MLLM for audiovisual integration in AD. For end-to-end driving, MLLMs also exhibit better interpretability and trustworthiness. DriveGPT4 **Tang, "DriveGPT4: A Multimodal Model for End-to-End Autonomous Driving"**
pioneers using MLLM to transform sensor data and instructions into control signals and text responses. RAG-Driver **
Li, "RAG-Driver: A Retrieval-Augmented Multimodal Model for Autonomous Driving"**
proposes a retrieval-augmented MLLM that generates driving behavior justifications and predicts control signals by retrieving expert demonstrations. DriveVLM **Wang, "DriveVLM: A Cognitive Chain Module for Multimodal Language Understanding in Autonomous Driving"**
integrates the cognitive chain module into the MLLM, enabling driving scene description and motion planning. However, Existing research has yet to align MLLMs with the implicit cognitive chain of human driving, enabling reasoning from combined global and local scene understanding to behavior, trajectory, and control commands, limiting cross-scene generalization and human-consensus driving.

\subsection{Visual Question Answering Datasets}

To support the efficient training of MLLMs, the design of large-scale VQA datasets has become a research hotspot. Currently, various VQA datasets exist, including image-based datasets such as CLEVR **Johnson-Roberson, "CLEVR: A Diagnostic Dataset for Compositional Reasoning"**
VQA2.0 **
Goyal, "VQA2.0: Visual Question Answering 2.0"**
and EQA **
Li, "EQA: Efficient Question Answering via Adaptive Reasoning"**
as well as video-based datasets like TVQA **Zhou, "TVQA: A Dataset for Spatiotemporal Action Localization and Recognition"**
TGIF-QA **Jain, "TGIF-QA: Toward Spatiotemporal Reasoning in Visual Question Answering"**
and ActivityNet-QA **
Krishna, "ActivityNet-QA: A Large-Scale Video Question Answering Dataset"**
For ImageQA, early studies **
Antol, "VQA: Visual Question Answering"**
attempted to fuse image features extracted by Convolutional Neural Networks (CNNs) with question encodings, which were then fed into decoders for answer generation. Recently, Transformer-based models **
Lu, "Transformer-Based Models for Image Question Answering"**
have achieved state-of-the-art performance in ImageQA tasks. Through attention networks, some studies have effectively captured the intrinsic relationships between temporal context and spatial features in video frames. 3D QA is a novel task in the VQA domain, focusing on answering questions about 3D-view scenes, requiring models to understand the geometric structures and spatial relationships of objects. Recently, many 3D QA datasets have been constructed, such as 3DQA **
Wang, "3DQA: A Dataset for 3D-View Question Answering"**
ScanQA **
Kanezaki, "ScanQA: Scan-to-Image Question Answering"**
and SQA3D **Li, "SQA3D: Spatial Query and Answering on 3D Scenes"**. Despite significant progress in the VQA community, challenges remain when dealing with complex traffic scenes involving multimodal, multi-view, and multi-frame contexts. Moreover, the AD field currently lacks comprehensive VQA driving datasets.