[
  {
    "index": 0,
    "papers": [
      {
        "key": "alayrac2022flamingo",
        "author": "Alayrac, Jean-Baptiste and Donahue, Jeff and Luc, Pauline and Miech, Antoine and Barr, Iain and Hasson, Yana and Lenc, Karel and Mensch, Arthur and Millican, Katherine and Reynolds, Malcolm and others",
        "title": "Flamingo: a visual language model for few-shot learning"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "li2023blip",
        "author": "Li, Junnan and Li, Dongxu and Savarese, Silvio and Hoi, Steven",
        "title": "Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "liu2024improved",
        "author": "Liu, Haotian and Li, Chunyuan and Li, Yuheng and Lee, Yong Jae",
        "title": "Improved baselines with visual instruction tuning"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "zhu2023minigpt",
        "author": "Zhu, Deyao and Chen, Jun and Shen, Xiaoqian and Li, Xiang and Elhoseiny, Mohamed",
        "title": "Minigpt-4: Enhancing vision-language understanding with advanced large language models"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "lin2023video",
        "author": "Lin, Bin and Ye, Yang and Zhu, Bin and Cui, Jiaxi and Ning, Munan and Jin, Peng and Yuan, Li",
        "title": "Video-llava: Learning united visual representation by alignment before projection"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "zhang2023video",
        "author": "Zhang, Hang and Li, Xin and Bing, Lidong",
        "title": "Video-llama: An instruction-tuned audio-visual language model for video understanding"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "wu2023language",
        "author": "Wu, Dongming and Han, Wencheng and Wang, Tiancai and Liu, Yingfei and Zhang, Xiangyu and Shen, Jianbing",
        "title": "Language prompt for autonomous driving"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "choudhary2023talk2bev",
        "author": "Choudhary, Tushar and Dewangan, Vikrant and Chandhok, Shivam and Priyadarshan, Shubham and Jain, Anushka and Singh, Arun K and Srivastava, Siddharth and Jatavallabhula, Krishna Murthy and Krishna, K Madhava",
        "title": "Talk2bev: Language-enhanced bird's-eye view maps for autonomous driving"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "xu2024drivegpt4",
        "author": "Xu, Zhenhua and Zhang, Yujia and Xie, Enze and Zhao, Zhen and Guo, Yong and Wong, Kwan-Yee K and Li, Zhenguo and Zhao, Hengshuang",
        "title": "Drivegpt4: Interpretable end-to-end autonomous driving via large language model"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "yuan2024rag",
        "author": "Yuan, Jianhao and Sun, Shuyang and Omeiza, Daniel and Zhao, Bo and Newman, Paul and Kunze, Lars and Gadd, Matthew",
        "title": "Rag-driver: Generalisable driving explanations with retrieval-augmented in-context learning in multi-modal large language model"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "tian2024drivevlm",
        "author": "Tian, Xiaoyu and Gu, Junru and Li, Bailin and Liu, Yicheng and Wang, Yang and Zhao, Zhiyong and Zhan, Kun and Jia, Peng and Lang, Xianpeng and Zhao, Hang",
        "title": "Drivevlm: The convergence of autonomous driving and large vision-language models"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "johnson2017clevr",
        "author": "Johnson, Justin and Hariharan, Bharath and Van Der Maaten, Laurens and Fei-Fei, Li and Lawrence Zitnick, C and Girshick, Ross",
        "title": "Clevr: A diagnostic dataset for compositional language and elementary visual reasoning"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "goyal2017making",
        "author": "Goyal, Yash and Khot, Tejas and Summers-Stay, Douglas and Batra, Dhruv and Parikh, Devi",
        "title": "Making the v in vqa matter: Elevating the role of image understanding in visual question answering"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "das2018embodied",
        "author": "Das, Abhishek and Datta, Samyak and Gkioxari, Georgia and Lee, Stefan and Parikh, Devi and Batra, Dhruv",
        "title": "Embodied question answering"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "lei2018tvqa",
        "author": "Lei, Jie and Yu, Licheng and Bansal, Mohit and Berg, Tamara L",
        "title": "Tvqa: Localized, compositional video question answering"
      }
    ]
  },
  {
    "index": 15,
    "papers": [
      {
        "key": "jang2017tgif",
        "author": "Jang, Yunseok and Song, Yale and Yu, Youngjae and Kim, Youngjin and Kim, Gunhee",
        "title": "Tgif-qa: Toward spatio-temporal reasoning in visual question answering"
      }
    ]
  },
  {
    "index": 16,
    "papers": [
      {
        "key": "yu2019activitynet",
        "author": "Yu, Zhou and Xu, Dejing and Yu, Jun and Yu, Ting and Zhao, Zhou and Zhuang, Yueting and Tao, Dacheng",
        "title": "Activitynet-qa: A dataset for understanding complex web videos via question answering"
      }
    ]
  },
  {
    "index": 17,
    "papers": [
      {
        "key": "johnson2017clevr",
        "author": "Johnson, Justin and Hariharan, Bharath and Van Der Maaten, Laurens and Fei-Fei, Li and Lawrence Zitnick, C and Girshick, Ross",
        "title": "Clevr: A diagnostic dataset for compositional language and elementary visual reasoning"
      },
      {
        "key": "fukui2016multimodal",
        "author": "Fukui, Akira and Park, Dong Huk and Yang, Daylen and Rohrbach, Anna and Darrell, Trevor and Rohrbach, Marcus",
        "title": "Multimodal compact bilinear pooling for visual question answering and visual grounding"
      }
    ]
  },
  {
    "index": 18,
    "papers": [
      {
        "key": "tan2019lxmert",
        "author": "Tan, Hao and Bansal, Mohit",
        "title": "Lxmert: Learning cross-modality encoder representations from transformers"
      },
      {
        "key": "zhang2021vinvl",
        "author": "Zhang, Pengchuan and Li, Xiujun and Hu, Xiaowei and Yang, Jianwei and Zhang, Lei and Wang, Lijuan and Choi, Yejin and Gao, Jianfeng",
        "title": "Vinvl: Revisiting visual representations in vision-language models"
      }
    ]
  },
  {
    "index": 19,
    "papers": [
      {
        "key": "ye2022visatlas",
        "author": "Ye, Yilin and Huang, Rong and Zeng, Wei",
        "title": "VISAtlas: An image-based exploration and query system for large visualization collections via neural image embedding"
      }
    ]
  },
  {
    "index": 20,
    "papers": [
      {
        "key": "azuma2022scanqa",
        "author": "Azuma, Daichi and Miyanishi, Taiki and Kurita, Shuhei and Kawanabe, Motoaki",
        "title": "Scanqa: 3d question answering for spatial scene understanding"
      }
    ]
  },
  {
    "index": 21,
    "papers": [
      {
        "key": "ma2022sqa3d",
        "author": "Ma, Xiaojian and Yong, Silong and Zheng, Zilong and Li, Qing and Liang, Yitao and Zhu, Song-Chun and Huang, Siyuan",
        "title": "Sqa3d: Situated question answering in 3d scenes"
      }
    ]
  }
]