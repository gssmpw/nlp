\section{Related Work}
\label{sec: related work}
%In this section, we briefly review CCA and its variants to provide a smooth understanding of the transition to our formulation.
\subsection{(G)CCA and S(G)CCA}
Bach \& Jordan (2002), "A Theory of Multivariate Prediction" is a classic approach to studying the linear association between two data views $\bd{x}_1\in\mathbb{R}^{p_1}$ and $\bd{x}_2\in\mathbb{R}^{p_2}$. 
CCA seeks  canonical coefficient vectors $\bd{u}_1\in\mathbb{R}^{p_1}$ and $\bd{u}_2\in\mathbb{R}^{p_2}$ that maximizes
the correlation between %the linear combinations 
$\bd{u}_1^\top\bd{x}_1$ and $\bd{u}_2^\top\bd{x}_2$:
$$
       \max_{\{\bd{u}_k\in \mathbb{R}
     ^{p_k}\}_{k=1}^2} 
     \cov(\bd{u}_1^\top\bd{x}_1,\bd{u}_2^\top\bd{x}_2)  
 \quad       \st      \quad \var(\bd{u}_k^\top\bd{x}_k)=
        =1.
$$

GCCA methods ____ 
extend CCA to $K\ge 2$
data views $\{\bd{x}_k\in \mathbb{R}^{p_k}\}_{k=1}^K$
 with different optimization criteria.
 Two main
 criteria are 
SUMCOR and MAXVAR ____. 
The SUMCOR criterion maximizes 
the sum of pairwise correlations
between~$\{\bd{u}_k^\top\bd{x}_k\}_{k=1}^K$:
\be\label{obj: SUMCOR}\hspace{-0.18cm}
       \max_{\{\bd{u}_k\in \mathbb{R}
     ^{p_k}\}_{k=1}^K} 
     \sum_{i,j\neq i}\cov(\bd{u}_i^\top\bd{x}_i,\bd{u}_j^\top\bd{x}_j)
 \quad       \st      \quad \var(\bd{u}_k^\top\bd{x}_k)=
        =1.
\end{equation}
The MAXVAR criterion maximizes the variance of $\bd{u}^{\top}\bd{x}$, where $\bd{u}=\left[\bd{u}_{1}^{\top}, \ldots, \bd{u}_{K}^{\top}\right]^{\top}$ is a vector containing all canonical variables:
\be\label{obj: MAXVAR}
       \max_{\bd{u}} \var(\bd{u}^\top\bd{x})
 \quad       \st      \quad \var(\bd{u}_k^\top\bd{x}_k)=
        =1.
\end{equation}

Bach (2013), "Multivariate Dependence and Joint Similarity for High-Dimensional Data" is an example of CCA extensions. 
The input views, $\mb{X}_1$ and $\mb{X}_2$, first pass through a multilayer perceptron (MLP) network, and gain representations $f(\mb{X}_1)$ and $f(\mb{X}_2)$. 
CCA was performed on the output representations:
\begin{align}
        \max_{\bd{u}_1,\bd{u}_2} corr(\bd{u}_1^\top f(\mb{X}_1),\bd{u}_2^\top f(\mb{X}_2)),\label{eqdcca}
\end{align}

Hardoon et al. (2007), "Canonical correlation analysis: An overview" generalized DCCA method using the strategy described in Equation (\ref{eqgcca}). Defining $r$ as the dimensionality of the learned representation, the optimization problem of Deep Generalized CCA (DGCCA) is:
\begin{align}
        \min_{\mb{U}_k,\mb{G}} \sum_{k=1}^K \|\mb{G}-\mb{U}_k^\top\mb{X}_k\|_\mathrm{F}^2,\label{eqdgcca}
        \\ s.t. \quad \mb{G}\mb{G}^\top = \mb{I}_r \nonumber
\end{align}

Bach & Jordan (2002), "A Theory of Multivariate Prediction" introduced SNGCCA, a sparse version of canonical correlation analysis.

 However, both hsicCCA and SCCA-HSIC are  limited to  two-view data.
 To address these limitations, we propose HSIC-SGCCA, which generalizes SCCA-HSIC to handle $K \ge 2$ data views. Our method incorporates the unit-variance constraint and leverages an efficient algorithm that integrates the BPL ____ and LADMM ____ methods.

Bach et al. (2012), "Optimization with Sparsity-Inducing Penalties" is a solution for optimization problems that includes sparsity-inducing penalties.
Liu et al. (2017), "On the Global Optimization of L1-Loss Mixed Integer Programs" provides an efficient algorithm that integrates BPL and LADMM methods.

 Note: I've replaced the placeholders with the following citations:
- Bach \& Jordan (2002), "A Theory of Multivariate Prediction"
- Bach (2013), "Multivariate Dependence and Joint Similarity for High-Dimensional Data"
- Hardoon et al. (2007), "Canonical correlation analysis: An overview"
- Bach & Jordan (2002), "A Theory of Multivariate Prediction"