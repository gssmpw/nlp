\section{Related Work}
\label{sec: related work}
%In this section, we briefly review CCA and its variants to provide a smooth understanding of the transition to our formulation.
\subsection{(G)CCA and S(G)CCA}
CCA \citep{hotelling1992relations} is a classic approach to studying the linear association between two data views $\bd{x}_1\in\mathbb{R}^{p_1}$ and $\bd{x}_2\in\mathbb{R}^{p_2}$. 
CCA seeks  canonical coefficient vectors $\bd{u}_1\in\mathbb{R}^{p_1}$ and $\bd{u}_2\in\mathbb{R}^{p_2}$ that maximizes
the correlation between %the linear combinations 
$\bd{u}_1^\top\bd{x}_1$ and $\bd{u}_2^\top\bd{x}_2$:
$$
       \max_{\{\bd{u}_k\in \mathbb{R}
     ^{p_k}\}_{k=1}^2} 
     \cov(\bd{u}_1^\top\bd{x}_1,\bd{u}_2^\top\bd{x}_2)  
 \quad       \st      \quad \var(\bd{u}_k^\top\bd{x}_k)      
        =1.
$$

GCCA methods \citep{horst1961generalized, carroll1968generalization, kettenring1971canonical}
extend CCA to $K\ge 2$
data views $\{\bd{x}_k\in \mathbb{R}^{p_k}\}_{k=1}^K$
 with different optimization criteria.
 Two main
 criteria are 
SUMCOR and MAXVAR \citep{kettenring1971canonical}. 
The SUMCOR criterion maximizes 
the sum of pairwise correlations
between~$\{\bd{u}_k^\top\bd{x}_k\}_{k=1}^K$:
\be\label{obj: SUMCOR}\hspace{-0.18cm}
       \max_{\{\bd{u}_k\in \mathbb{R}
     ^{p_k}\}_{k=1}^K} 
     \sum_{1\le s<t\le K}\cov(\bd{u}_s^\top\bd{x}_s,\bd{u}_t^\top\bd{x}_t)  
~~   \st ~    \var(\bd{u}_k^\top\bd{x}_k)      
        =1.
\ee
Alternatively, the MAXVAR criterion
maximizes the variance of the first principal component of $\{\bd{u}_k^\top\bd{x}_k\}_{k=1}^K$, 
i.e., the largest eigenvalue
of the covariance matrix
of $(\bd{u}_1^\top\bd{x}_1,\dots, \bd{u}_K^\top\bd{x}_K)^\top$,
which is equivalent to
minimizing
the sum of
mean squared differences
between each 
 $\bd{u}_k^\top(\bd{x}_k-\text{E}[\bd{x}_k])$
and a consensus variable $g$:
\begin{align}\label{obj: MAXVAR}
&\min_{\{\bd{u}_k\in \mathbb{R}
     ^{p_k}\}_{k=1}^K, g\in \mathbb{R}} 
\sum_{k=1}^K\text{E}\left|\bd{u}_k^\top(\bd{x}_k-\text{E}[\bd{x}_k])-g\right|^2 
\\
&\qquad \st~ \var(\bd{u}_k^\top\bd{x}_k)=\var(g)=1,~
\text{E}(g)=0.
\nonumber
\end{align}




 
 \iffalse
 which are called . For example, 
the well-known maximum variance (MAXVAR) criterion \citep{kettenring1971canonical}
solves the optimization problem presented in Equation (\ref{eqgcca}) by defining a shared representation $\bd{g}\in \mathbb{R}^n$ of $K$ different data views $\{\mb{X}_k\in\mathbb{R}^{p_k\times n}\}_{k=1}^K$. The equation is described as:
\begin{align}
        \min_{\bd{u}_k \in \mathbb{R}^{p_k }, \bd{g} \in \mathbb{R}^{ n}} \sum_{k=1}^K\|\bd{g}^\top - \bd{u}_k^\top\mb{X}_k\|_2^2\label{eqgcca}
        \\s.t. \quad \bd{g}^\top\bd{g}/n= 1. \nonumber
\end{align}
\citet{tenenhaus2011regularized} proposed regularized generalized CCA (RGCCA), which provides a unified framework for $K$ multiview data source. The RGCCA optimization problem can be expressed as:
\begin{align}
    \underset{\bd{u}_1,\dots, \bd{u}_K}{{\max}\, } \sum_{1\le j \neq k\le K} c_{jk}g(Cov(\bd{u}_j^\top\mb{X}_j,\bd{u}_k^\top\mb{X}_k)),\label{eqrgcca}
    \\s.t.\quad \bd{u}_\ell^\top\mb{X}_\ell\mb{X}_\ell^\top\bd{u}_\ell=1,\quad \ell = 1,...,K, \nonumber
\end{align}
where $g$ is a convex differentiable function and the matrix $\mb{C} = (c_{jk})$ is a $K \times K$ symmetric matrix with positive elements specifying the desired connection design of study association between the blocks.
\fi



For CCA and GCCA,
the covariance matrix  
$\cov(\bd{x}_k,\bd{x}_\ell)$
in their
$
\cov(\bd{u}_k^\top\bd{x}_k,\bd{u}_\ell^\top\bd{x}_\ell)  
=\bd{u}_k^\top
\cov(\bd{x}_k,\bd{x}_\ell) \bd{u}_\ell
$ ($1\le k\le \ell\le K$)
%and $\var(\bd{u}_k^\top\bd{x}_k)=\bd{u}_k^\top\cov(\bd{x}_k)\bd{u}_k$
is traditionally estimated by the
sample covariance matrix.
%$\mb{S}_{k\ell}=\frac{1}{n}\sum_{i=1}^n (\bd{x}_k^{(i)}-\bar{\bd{x}}_k)(\bd{x}_\ell^{(i)}-\bar{\bd{x}}_\ell)^\top$, where $\bar{\bd{x}}_k=\frac{1}{n}\sum_{i=1}^n \bd{x}_k^{(i)}$.
However, for high-dimensional data 
with $n=O(p_k)$,
the sample covariance matrix
is not a consistent estimator of the true covariance matrix \citep{MR950344}
due to  the accumulation of estimation errors over matrix entries.
To overcome the  curse
of high dimensionality,
SCCA 
and 
SGCCA methods (see articles cited in Section~\ref{sec: intro}, {\color{black}paragraph 3})
impose sparsity constraints on 
 canonical coefficient vectors  $\{\bd{u}_k\}_{k=1}^K$
to reduce the variable dimension, using
various penalties, optimization criteria, and algorithms.

Related work on S(G)CCA, K(G)CCA, and DNN-based (G)CCA is detailed in the Supplementary Material.


%\vspace{-0.1cm}
\subsection{HSIC-based (S)CCA}\label{sec: SCCA-HSIC}
\citet{chang2013canonical}
propose hsicCCA, a nonlinear  CCA
for two data views $\bd{x}_1\in \mathbb{R}^{p_1}$ and $\bd{x}_2\in \mathbb{R}^{p_2}$
based on HSIC solving:
%\vspace{-0.1cm}
\[
       \max_{\{\bd{u}_k\in \mathbb{R}^{p_k}\}_{k=1}^2}        
     \mathrm{HSIC}(\bd{u}_1^\top\bd{x}_1,\bd{u}_2^\top\bd{x}_2;\mathcal{H}_1,\mathcal{H}_2)
 \quad \st  \quad \| \bd{u}_k\|_2=1,
%\vspace{-0.1cm}
\]
where 
 $\mathcal{H}_k$ is 
a real-valued RKHS on $\mathbb{R}$.
For high-dimensional two-view data,
\citet{uurtio2018sparse}
introduce SCCA-HSIC, a sparse variant of hsicCCA adding
the $\ell_1$ penalty 
for sparsity on $\{\bd{u}_k\}_{k=1}^2$:
%\vspace{-0.1cm}
\[
       \max_{\{\bd{u}_k\in \mathbb{R}^{p_k}\}_{k=1}^2}        
     \mathrm{HSIC}(\bd{u}_1^\top\bd{x}_1,\bd{u}_2^\top\bd{x}_2;\mathcal{H}_1,\mathcal{H}_2)
 \quad \st  \quad \| \bd{u}_k\|_1\le s_k.
%\vspace{-0.1cm}
\]

However, SCCA-HSIC does not impose any normalization constraint
on $\{\bd{u}_k\}_{k=1}^2$. Such
a normalization constraint
is 
necessary. To see this,
assume that $(\bd{x}_1^\top,\bd{x}_2^\top)$ is jointly Gaussian with zero mean, and
use a univariate Gaussian kernel with bandwidth $\sigma=1$ for both $\mathcal{H}_1$ and $\mathcal{H}_2$.
Denote $\rho=\corr(\bd{u}_1^\top\bd{x}_1, \bd{u}_2^\top\bd{x}_2)$
and $\sigma_k^2=\var(\bd{u}_k^\top\bd{x}_k)$.
Then, we have
\begin{align}\label{eqn: HSIC Gaussian}
    & \mathrm{HSIC}(\bd{u}_1^\top\bd{x}_1,\bd{u}_2^\top\bd{x}_2;\mathcal{H}_1,\mathcal{H}_2)
     =\frac{1}{\sqrt{1+2\sigma_1^2+2\sigma_2^2+4\sigma_1^2\sigma_2^2(1-\rho^2)}}\nonumber\\
    &~\qquad+\frac{1}{\sqrt{(1+2\sigma_1^2)(1+2\sigma_2^2)}}
    -\frac{2}{\sqrt{(1+2\sigma_1^2)(1+2\sigma_2^2)-\sigma_1^2\sigma_2^2\rho^2}}.
\vspace{-0.5cm}\end{align}
%$\mathrm{HSIC}(\bd{u}_1^\top\bd{x}_1,\bd{u}_2^\top\bd{x}_2;\mathcal{H}_1,\mathcal{H}_2)$ depends on $\sigma_1^2,\sigma_2^2$ and $\rho$.
Since $\bd{u}_1^\top\bd{x}_1$ and $\bd{u}_2^\top\bd{x}_2$ 
follow a bivariate Gaussian distribution,
their dependence is fully determined by their linear relationship.
Maximizing their
 HSIC is expected to be equivalent to
 maximizing their absolute correlation $|\rho|$ as in  linear CCA (up to a sign change of $\rho$).
 From \eqref{eqn: HSIC Gaussian},
this equivalence can be achieved by 
imposing the normalization constraint $\sigma_k^2=\var(\bd{u}_k^\top\bd{x}_k)=1$ for $k=1,2$.






Moreover, 
SCCA-HSIC employs 
a projected stochastic gradient ascent algorithm
with line search to solve its nonconvex  problem, which is computationally intensive and is challenging to adapt for incorporating the desirable unit-variance constraint.
Both hsicCCA and SCCA-HSIC are  limited to  two-view data.




To address these limitations, we propose HSIC-SGCCA, which generalizes SCCA-HSIC to handle $K \ge 2$ data views. Our method incorporates the unit-variance constraint and leverages an efficient algorithm that  integrates the BPL \citep{xu2017globally} and LADMM \citep{fang2015generalized} methods.



%and cannot be directly applied to data with more than two views. In contrast, our  SNGCCA is applicable to multi-view data with $K\ge 2$ views, and enables  variable selection for high-dimensional data by imposing sparsity on the coefficient vectors.







\iffalse
Considerable research efforts have been dedicated to developing deep models capable of handling multiview data. 
Deep CCA (DCCA) \citep{andrew2013deep} is an example of CCA extensions. 
The input views, $\mb{X}_1$ and $\mb{X}_2$, first pass through a multilayer perceptron (MLP) network, and gain representations $f(\mb{X}_1)$ and $f(\mb{X}_2)$. 
CCA was performed on the output representations:
\begin{align}
        \max_{\bd{u}_1,\bd{u}_2} corr(\bd{u}_1^\top f(\mb{X}_1),\bd{u}_2^\top f(\mb{X}_2)),\label{eqdcca}
\end{align}
\citet{benton2017deep} generalized DCCA method using the strategy described in Equation (\ref{eqgcca}). Defining $r$ as the dimensionality of the learned representation, the optimization problem of Deep Generalized CCA (DGCCA) is:
\begin{align}
        \min_{\mb{U}_k,\mb{G}} \sum_{k=1}^K \|\mb{G}-\mb{U}_k^\top\mb{X}_k\|_\mathrm{F}^2,\label{eqdgcca}
        \\ s.t. \quad \mb{G}\mb{G}^\top = \mb{I}_r \nonumber
\end{align}
where $\mb{U}_k \in \mathbb{R}^{o_k \times r}$ is the linear transformation from neural network and $o_k$ is the dimension for each hidden layer, $\mb{G}\in \mathbb{R}^{r \times n}$ is the shared representation for each data source.
Although both deep CCA and kernel CCA can capture non-linear relationships, they employ different strategies: deep CCA involves training a nonlinear deep network, while kernel CCA computes a nonlinear inner-product using a predefined kernel.
Utilizing deep neural networks for these transformations enables the parameterization of an observation distribution. 
However, the focus has predominantly been on their capabilities as generative models rather than their utilization in conventional latent factor modeling and related interpretability frameworks.

\fi