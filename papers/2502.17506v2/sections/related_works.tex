\noindent \textbf{LLMs for Molecules.}
Leveraging the extensive body of literature and string-based molecular representations such as SMILES, language models (LMs) have been successfully applied to molecular sciences. 
Inspired by the masked language modeling approach used in BERT training \citep{devlin2018bert}, KV-PLM \citep{zeng2022deep} introduces a method to train LMs by reconstructing masked SMILES and textual data. 
Similarly, MolT5 \citep{edwards2022translation} adopts the ``replace corrupted spans" objective \citep{raffel2020exploring} for pre-training on both SMILES strings and textual data, followed by fine-tuning for downstream tasks such as molecule captioning and generation. 
Building on this foundation, \citet{pei2023biot5} and \citet{christofidellis2023unifying} extend MolT5 with additional pre-training tasks, including protein FASTA reconstruction and chemical reaction prediction.
% , further advancing the capabilities of LMs in molecular science.
Furthermore, GIMLET \cite{zhao2023gimlet}, Mol-Instructions \cite{fang2023mol}, and MolecularGPT \cite{liu2024moleculargpt} adopt instruction tuning \cite{zhang2023instruction} to improve generalization across a wide range of molecular tasks. 
While these approaches demonstrate enhanced versatility, they still rely on expensive fine-tuning processes to enable molecule-specific tasks or to incorporate new data.

% \subsection{LLM Agents}
\noindent \textbf{LLM Agents for Science.}
An LLM agent is a system that leverages LLMs to interact with users or other systems, perform tasks, and make decisions autonomously \cite{wang2024survey}. 
Recently, LLM agents have attracted significant interest in scientific applications and biomedical discovery~\cite{gao2024empowering}, with applications including literature search~\cite{lala2023paperqa}, experiment design~\cite{roohani2024biodiscoveryagent}, and hypothesis generation~\cite{wang-etal-2024-scimon}, among others. In particular, agents focusing on drug discovery applications have emerged. Systems like ChemCrow \cite{bran2023chemcrow}, CACTUS \cite{mcnaughton2024cactus}, and Coscientist \cite{boiko2023autonomous} focus on automating cheminformatics tasks and experiments, streamlining computational and experimental pipelines. Other works leverage agent-based orchestration of tools and data to accelerate specific aspects of scientific workflows, such as search~\cite{odonoghue2023bioplanner} or design~\cite{ghafarollahi2024protagents}. In contrast to existing works, we investigate an agent-based framework that can effectively incorporate external knowledge to improve general molecular QA. This could be used either independently or as part of a larger system for automated drug discovery~\cite{tom2024self}.


% showcase the capability for autonomous ideation and experimentation by integrating with domain-specific tools. 
% Recently, interest has shifted beyond single-agent frameworks, 
% employing multiple agents to engage collaboratively in planning, discussions, and decision-making, reflecting the cooperative dynamics of human teamwork in problem-solving tasks \cite{guo2024large}.
% For instance, MDAgents \cite{kim2024mdagents} and KGRevion \cite{su2024knowledge} leverage collaborative interactions among agents within the medical domain. 
% In this work, we adapt the LLM multi-agent framework, specifically for the field of drug discovery.


% \subsection{Multi-Agent Collaborations for Drug Discovery}
\noindent \textbf{Multi-Agent Collaborations for Drug Discovery.}
Only a limited number of studies have explored multi-agent frameworks in the context of drug discovery. DrugAgent~\cite{inoue2024drugagent} introduces a multi-agent framework integrating multiple external data sources but is limited to predicting drug-target interaction scores.
Another study with the same name employs an agentic framework for automating machine learning programming for drug discovery tasks~\cite{liu2024drugagent}. 
In contrast, our work seeks to tackle a diverse array of drug discovery tasks, enabling applicability across a wide variety of use cases.
We provide additional related works on the knowledge graph in Appendix \ref{app: Related Works}.



