We assess the effectiveness of \proposed by conducting a range of drug discovery tasks, including 
property-specific molecular captioning (Section \ref{exp:Molecular Captioning Task}),
drug-target prediction (Section \ref{exp:Protein Target Prediction Task}),
and drug toxicity prediction (Section \ref{exp:Drug Toxicity Prediction Task}).
% , and single-cell perturbation prediction (Appendix \ref{app: additional experiments}).
% These experiments span a broad range of drug discovery applications, datasets, and task types.

\textbf{Implementation Details.}
In all experiments, we utilize GPT-4o mini through the OpenAI API for each agent.
We use PrimeKG \cite{chandak2023building} as the KG, PubChem \cite{kim2021pubchem} as an annotation database, and MolT5 \cite{edwards2022translation} as an external captioning tool.
Additional implementation details and agent templates can be found in Appendix \ref{app: Implementation Details} and \ref{app: agent templates}, respectively.

\subsection{Property-Specific Molecular Captioning Task
}
\label{exp:Molecular Captioning Task}
Earlier studies on molecular captioning tasks have primarily focused on generating general descriptions of molecules without targeting specific areas of interest, raising concerns about their practical applicability in real-world drug discovery tasks.
Indeed, the usefulness of a molecular description is often task-dependent, and scientists may be interested in detailed explanations of specific characteristics of a molecule rather than a general description~\cite{guo2024moltailor,edwards2024molcap}.
Hence, in this paper, we introduce \emph{property-specific molecular captioning}, where the model is required to generate a description for a given molecule customized to a particular area of interest.


% new drug discovery task called property-specific molecular captioning, which emphasizes generating captions for a molecule that are customized to a particular area of interest.

\textbf{Datasets.}
We leverage four widely recognized molecular property prediction datasets from the MoleculeNet benchmark \cite{wu2018moleculenet}: \textbf{BBBP}, \textbf{Sider}, \textbf{ClinTox}, and \textbf{BACE}.
We provide further details on the datasets in Appendix \ref{app: Datasets}.

\textbf{Methods Compared.}
We consider different baseline approaches. 
First, we compare recent molecular captioning methods designed to generate general descriptions of molecules, including MolT5~\cite{edwards2022translation}, LlasMol \cite{yu2024llasmol}, and BioT5~\cite{pei2023biot5}. Furthermore, we assess general-purpose LLMs, namely GPT-4o mini and GPT-4o.
% Finally, we compare GNN-based models, which map chemical structures to target properties and are commonly used for molecular property prediction. 
% Although lacking the flexibility of LLM-based frameworks, these methods achieved state-of-the-art performance across many tasks and are routinely used in applications~\cite{wu2018moleculenet}. 
% In particular, pre-trained GNNs have shown remarkable performance, and 
Finally, we consider two GNNs pre-trained with different methodologies: GraphMVP \cite{liu2021pre} and MoleculeSTM \cite{liu2023multi}. 
We provide further details on the baseline models in Appendix~\ref{app: Baselines}.

\input{tables/mol_cap}
\textbf{Evaluation Protocol.}
Although property-specific captions are practical, no ground truth property-specific captions exist for individual molecules, rendering traditional text generation evaluation methods inapplicable.
Thus, in line with recent works~\cite{xu2024llm,guo2024moltailor,edwards2024molcap}, we assess whether the generated captions can drive a classification model that categorizes molecules based on their properties.
Specifically, for a generated \texttt{caption} and the \texttt{SMILES} representation of the target molecule, we concatenate them using a \texttt{[CLS]} token, forming \texttt{SMILES[CLS]caption}, and fine-tune a SciBERT \cite{beltagy2019scibert} model for property prediction.
The ``Only SMILES" model utilizes only the SMILES string as input for the SciBERT classifier.
For baseline GNNs, we convert the SMILES into a molecular graph and provide it to the model.
For all the experiments, we use a scaffold splitting strategy to simulate realistic distribution shifts (train/validation/test data split as 80/10/10\%), following previous works~\cite{liu2023multi}.
We perform five independent fine-tuning runs of SciBERT (or GNN baselines) and report the mean and standard deviation of the AUROC.

\textbf{Experimental Results.}
Table~\ref{tab: mol cap} summarizes the results. We note the following observations:
\textbf{1)} While domain-specific models outperform general-purpose LLMs, their performance remains suboptimal, occasionally falling behind the ``Only SMILES" approach. This means that the generated captions occasionally reduce model performance compared to using only the SMILES representation of the molecule. This aligns with previous work that found that general descriptors may lack property-specific relevance~\cite{guo2024moltailor,edwards2024molcap}.
\textbf{2)} On the other hand, \proposed-generated captions consistently outperform all the baseline captioners and successfully improve over ``Only SMILES" across all datasets.
We attribute this improvement to the ability of \proposed to draw on external biochemical knowledge to ground its generation and its task-specificity. % which allows for interest-specific captioning of the target molecule, unlike domain-specific LLMs that can only produce general captions for the molecule.
\textbf{3)} Moreover, \proposed consistently outperforms pre-trained GNN baselines, except on the BACE dataset. 
Interestingly, this is also the only dataset for which the ``Only SMILES'' baseline falls short compared to GNN models, thus 
% Unlike other datasets where the ``Only SMILES" approach achieves performance comparable to GNN models, it falls significantly behind GNNs on the Bace dataset,
highlighting the critical role of 2D topological and 3D geometric information in this case.
This paves the way for future research on injecting essential aspects of molecules, such as topological and geometric information, into LLM understanding.

\input{tables/protein_target}

\subsection{Drug-Target Prediction Task}
\label{exp:Protein Target Prediction Task}

Accurately predicting a drug's protein target is essential for understanding its mechanism of action and optimizing its therapeutic efficacy while minimizing off-target effects \cite{santos2017comprehensive,batool2019structure}.
Here, we evaluate the models' ability to accurately identify which proteins a given molecule is most likely to activate or inhibit. 
% Furthermore, we leverage this task to assess the flexible question-answering abilities of \proposed.

\textbf{Datasets.} We use molecular targets present in the Drug Repurposing Hub~\cite{corsello2017drug}, DrugBank~\cite{wishart2018drugbank}, and STITCH v5.0~\cite{szklarczyk2016stitch}, as preprocessed in \citealp{zheng2023chempert}, including 13,688 molecules in total (details are presented in Appendix \ref{app: Datasets}). 


\textbf{Methods Compared.}
We evaluate two pre-trained GNNs, GraphMVP and MoleculeSTM, along with two general-purpose LLMs—GPT-4o mini and GPT-4o, and the domain-specific language model Galactica \cite{taylor2022galactica} (details are presented in Appendix \ref{app: Baselines}).

\textbf{Evaluation Protocol.}
We assess the performance of LLMs in a zero-shot setting. 
Specifically, for a given target molecule, we prompt the LLMs to generate the top 5 proteins that the molecule is most likely to activate or inhibit. The precision is then calculated by determining whether the generated proteins match the correct answers.
As baseline GNNs cannot perform this task without training in a zero-shot setting, we fine-tune them in a few-shot setting using 10\% of the data. 
For domain-specific LMs, we also present fine-tuning results  on the specific task. 
To better assess generalization power, we separately report the performance on the test set for molecules present/not present in the external databases (``Overlap''/``No Overlap'').

\input{tables/toxic}


\textbf{Experimental Results.}
Table~\ref{tab: protein target} summarizes the results. We observe the following:
\textbf{1)} \proposed~outperforms all the baselines, with a higher likelihood of correctly identifying proteins activated/inhibited by the input molecule. 
\textbf{2)} Importantly, the superiority of \proposed is confirmed for molecules not present in the caption database or knowledge graph (Table \ref{tab: protein target} (b)), showcasing \proposed's ability to leverage external knowledge to generalize to novel molecules.
This outcome underscores the potential of \proposed~for exploring entirely new molecular spaces.
\textbf{3)}~We observe that domain-specific fine-tuned models, such as Galactica, GIMLET, and MolecularGPT, could not generate five protein targets in a zero-shot setting when prompted to do so, as they are limited to providing answers based on their fine-tuning instruction dataset. By specifically fine-tuning Galactica on the task, we were able to answer the specific question, outperforming general-purpose LLMs in most experiments, but results were still inferior to \proposed.
This further highlights the flexibility of \proposed, which leverages the zero-shot abilities of general-purpose LLMs in its architecture. %, enabling genuine interactions between scientists and the AI model through its flexibility.
% We provide further details on the baseline models in Appendix \ref{app: Baselines}.



\begin{figure}[t]
    \centering
    \includegraphics[width=0.9\linewidth]{imgs/model_analysis.pdf}
    \vspace{-1ex}
    \caption{\textbf{Ablation studies.} \textbf{(a)} On model components. \textbf{(b)} On external knowledge.}
    \label{fig: model analysis}
    \vspace{-2ex}
\end{figure}

\begin{figure*}[t]
\vspace{-8pt}
    \centering
\includegraphics[width=0.9\linewidth]{imgs/Qualitative.pdf}
\vspace{-2ex}
    \caption{\textbf{Example of collaboration between agents in \proposed} (on the drug-target prediction task). Red represents adrenergic receptors, yellow represents histamine receptors, and green represents dopamine receptors. The full version is available in Appendix \ref{app: additional experiments}.
    % The BioRel Agent identifies associations with benign prostatic hyperplasia (BPH) while the DrugRel Agent proposes other complementary pathways, highlighting the synergies of the agentic approach.
    %Case studies. We ask \proposed~to identify the top 5 proteins that the molecule COc1ccccc1N1CCN(CCCCNC(=O)c2ccc3ccccc3c2)CC1 is most likely to activate.
    }
    \label{fig: case studies}
    \vspace{-8pt}
\end{figure*}

% \subsection{Experimental Setup}
% \textbf{Implementation Details.}

\vspace{-1ex}
\subsection{Drug Toxicity Prediction Task}
\label{exp:Drug Toxicity Prediction Task}
Accurate predictions of drug toxicity are crucial to ensure patient safety and minimize the risk of adverse effects during drug development~\cite{basile2019artificial}.
Here, we evaluate the models' ability to predict the toxicity of a target molecule from its SMILES-based structural description.

\textbf{Datasets.}
We use four datasets to comprehensively evaluate the performance of \proposed~for drug toxicity prediction tasks: \textbf{hERG} \cite{wang2016admet}, \textbf{DILI} \cite{xu2015deep}, \textbf{Skin} \cite{alves2015predicting}, \textbf{Carcinogens} \cite{lagunin2009computer} datasets (details are presented in Appendix \ref{app: Datasets}).

\textbf{Methods Compared.}
We compare five domain-specific LLMs—Galactica 125M, Galactica 1.3B, Galactica 6.7B \cite{taylor2022galactica}, LlasMol \cite{yu2024llasmol}, and GIMLET \cite{zhao2023gimlet} alongside two general-purpose LLMs, GPT-4o and GPT-4o mini (details in Appendix \ref{app: Baselines}).

\textbf{Evaluation Protocol.}
As all the methods compared are foundation models, we evaluate their performance in a zero-shot setting. 
Specifically, given a SMILES-based structural description of the target molecule and a task description, the model outputs whether the molecule possesses the target property (binary classification).
Using the text-formatted output generated by each model, we compute the Macro-F1 score \cite{opitz2019macro} as the evaluation metric.

\textbf{Experimental Results.}
Table~\ref{tab: toxicity} summarizes the results.
\textbf{1)} Comparing Galactica models, we observe that performance improves as the model size increases. However, this trend is not confirmed for  GPT-4o and GPT-4o mini. %, GPT-4o mini achieves better average performance.
This suggests that larger general-purpose models do not necessarily excel in domain-specific tasks, a trend also reported in prior research \cite{edwards2024molcap}. 
These findings emphasize the necessity of domain-specific training in combination with model scaling. %emphasize that model scale becomes significant when the model is specifically trained for the domain, underscoring the importance of domain-specific training over sheer model size.
\textbf{2)} Meanwhile, \proposed outperforms all the baselines (average score across datasets),  without requiring domain-specific training by effectively incorporating external knowledge into general-purpose LLMs.
This highlights the potential of integrating external knowledge at inference time to improve molecular understanding, acting as an alternative (or complementary approach) to expensive domain-specific model fine-tuning.
% This highlights the potential of external knowledge integration as a viable alternative to extensive domain-specific model training for improving task performance.


\subsection{Ablation studies}

\textbf{Model Components Ablations.}
In Figure~\ref{fig: model analysis} (a), we report the results of ablations on the components of~\proposed. We observe:
\textbf{1)}~\emph{The knowledge graph and the molecular annotations are important and complementary data sources}, as shown by the lower performance of the models with only Molecular Understanding or Only Knowledge Graph team available (``Only MU'', ``Only KG''). 
\textbf{2)}~\emph{Dynamically selecting the relevant data sources with our Planning Team leads to better performance}, leveraging their complementarity, as suggested by the lower performance of the ``No Planning''.
\textbf{3)} 
\emph{The distributed architecture of the multi-agent system is a more effective way of processing the retrieved information}. In ``Only Planning'', we concatenate all the relevant data sources directly into the prompt of the Prediction Agent, bypassing the preprocessing and report generation of the KG and MU teams. Our results show the limitations of a single model in handling heterogeneous biological data sources. More ablations are presented in Appendix~\ref{app: additional ablation studies}.




\textbf{External Knowledge Ablations.}
To further assess the impact of external knowledge on model performance, we evaluate the model after progressively pruning the available databases and present our results in Figure \ref{fig: model analysis} (b). We observe the following:
\textbf{1)} \emph{Model performance depends on external knowledge size}, validating the key role of the external knowledge to the framework.  \textbf{2)} Interestingly, \emph{we do not observe any performance plateau}, indicating that further expanding the external knowledge could provide additional performance improvements.
\textbf{3)} From the bar plots, \emph{i.e.,} ``No CT (No Captioning Tool)" and ``Use KG (Call Knowledge Graph Team)," we observe that as the amount of external knowledge grows, the planning team increasingly depends on it. 
This indicates that \proposed~actively leverages external knowledge more effectively during the decision-making process when such knowledge is more abundant.
More analysis on external knowledge usage is given in Appendix \ref{app: additional external knowledge analysis}.

\vspace{-1ex}
\subsection{Case Studies}
% In Figure \ref{fig: case studies}, we provide an example of agents collaboration in~\proposed to identify the top 5 proteins a query molecule is most likely to activate.
% We initially observe that the BioRel Agent summarizes the knowledge graph, identifying links between the drugs and the disease benign prostatic hyperplasia (BPH) \cite{roehrborn2005benign}. 
% However, it does not provide connections to the target proteins due to the absence of such information in the knowledge graph. In contrast, the DrugRel Agent leverages its internal knowledge to propose alpha-1 adrenergic receptors as potential protein targets. 
% Interestingly, we find that alpha-1 adrenergic receptors are indeed recognized as candidate genes associated with the severity of BPH~\cite{klotsman2004case}, effectively addressing the missing information in the knowledge graph.
% This highlights the complementary nature of the agents in \proposed, where the integration of diverse information from each agent is key to its success. 
% Furthermore, the model's ability to incorporate and synthesize evidence from multiple agents provides exceptional interpretability, enhancing trust and understanding in the predictions made by \proposed.
In Figure \ref{fig: case studies}, we provide an example of agent collaboration in~\proposed to identify the top 5 proteins a query molecule is most likely to activate.
First, the BioRel Agent extracts from the knowledge graph that the anchor drug, Naftopidil, is indicated for benign prostatic hyperplasia (BPH), pointing to likely activation of pathways related to that condition. 
The DrugRel Agent then complements these findings by 
\textbf{1)} connecting BPH with alpha-1 adrenergic receptors using its internal knowledge (which is confirmed in the literature~\cite{klotsman2004case}), and 
\textbf{2)} analyzing the related drugs in the knowledge graph (\emph{e.g.,} Hydroxyzine and Clopamine), inferring potential interaction with histamine and dopamine receptors. 
Finally, the MU agent integrates these findings with its own evidence to provide a summarized report of the likely activated proteins. 
This example highlights the complementary nature of the agents in \proposed, where the integration of diverse information from each agent is key to its success. 
The distributed architecture of the model, grounded in real data sources, also leads to increased interpretability and reliability of the generated answers.
Additional case studies illustrating the collaborative behavior of agents in \proposed are provided in Appendix~\ref{app: additional case studies}.

%In this section, we present case studies illustrating how the agents in our system collaborate with one another. In Figure \ref{fig: case studies}, we ask \proposed~to identify the top 5 proteins the target molecule is most likely to activate.
%We initially observe that the BioRel Agent summarizes the knowledge graph, identifying links between the drugs and the disease benign prostatic hyperplasia (BPH) \cite{roehrborn2005benign}. 
%However, it does not provide connections to the target proteins due to the absence of such information in the knowledge graph. In contrast, the DrugRel Agent leverages its internal knowledge to propose alpha-1 adrenergic receptors as potential protein targets. 
%Interestingly, we find that alpha-1 adrenergic receptors are indeed recognized as candidate genes associated with the severity of BPH~\cite{klotsman2004case}, effectively addressing the missing information in the knowledge graph.
%This highlights the complementary nature of the agents in \proposed, where the integration of diverse information from each agent is key to its success. 
%Furthermore, the model's ability to incorporate and synthesize evidence from multiple agents provides exceptional interpretability, enhancing trust and understanding in the predictions made by \proposed.
%Additional case studies illustrating the collaborative behavior of agents are provided in the Appendix \ref{app: additional experiments}.

\vspace{-2ex}
\section{Conclusion}
\vspace{-1ex}
In this work, we introduced \proposed, a multi-agent framework for molecular question-answering that dynamically retrieves and integrates external knowledge to support various drug discovery tasks. 
We showcased its flexibility and effectiveness across multiple tasks, outperforming both general-purpose and domain-specific LLMs as well as standard deep learning methods, without requiring expensive domain-specific fine-tuning.
Our analyses highlighted the complementarity of external knowledge sources, internal LLM reasoning, and multi-agent orchestration. \proposed's chain of messages also provides insight into its decision-making process and the role of different data sources, fostering more interpretable scientist-AI interactions.

%Moreover, as shown in our case studies, examining \proposed's chain of messages provides insight into its decision-making process and the role of different data sources, fostering more interpretable scientist-AI interactions. 

\textbf{Future Work.}
Our findings revealed a strong correlation between external knowledge size and system performance, with no observed plateaus, suggesting opportunities in scaling up external data. Additionally, beyond serving as a standalone tool, \proposed~could also be leveraged as a component of more complex agentic workflows, for example, combining computational and experimental systems~\cite{tom2024self}, which will be the subject of future work.
% Given its modularity and generality, \proposed can be easily extended to incorporate additional databases and tools.
% Beyond serving as a standalone tool, \proposed can also be leveraged as a component of more complex agentic workflows, for example, combining computational and experimental systems~\cite{tom2024self}, which will be the subject of future work.
%By leveraging the collaboration of multiple LLM agents, \proposed~dynamically retrieves information from diverse knowledge bases, contextualizes query molecules, and integrates relevant evidence to generate responses. Extensive experimental results highlight the effectiveness of \proposed, with its explainability and flexibility paving the way for genuine interaction between scientists and AI in the drug discovery process.



% for future work:
% These results pave the way for future research on improving the integration of essential aspects of molecules, such as topological and geometric information, into LLMs understanding.



% \input{tables/ref_drugs}