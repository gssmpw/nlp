\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{subfiles}
\usepackage{multirow}
\usepackage{booktabs} % for professional tables
\usepackage{enumitem}
\usepackage{xcolor}
\usepackage{pifont}
\usepackage{hyperref}
\usepackage{subcaption}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{natbib}

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2024} with \usepackage[nohyperref]{icml2024} above.

\newcommand{\cmark}{\textcolor{blue}{\ding{51}}}%
\newcommand{\xmark}{\textcolor{red}{\ding{55}}}%
\newcommand{\ehsan}[1]{\noindent{\textcolor{olive}{\{{\bf Ehsan:} \em #1\}}}}
% \newcommand{\proposed}{\textsf{DrugAgents}}

\usepackage{xspace}

\newcommand{\proposed}{\mbox{\textsf{CLADD}}\@\xspace}

% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025}

% For theorems and such

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}

% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{RAG-Enhanced Collaborative LLM Agents for Drug Discovery}
% \icmltitlerunning{RAG-Enhanced Collaborative LLM Agents for Drug Discovery}

\begin{document}

\twocolumn[
% \icmltitle{TeamPharma: Collaborative LLM Agents for Drug Discovery}
\icmltitle{RAG-Enhanced Collaborative LLM Agents for Drug Discovery}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2024
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{$\dagger$}

\begin{icmlauthorlist}
\icmlauthor{Namkyeong Lee}{kaist,equal}
\icmlauthor{Edward De Brouwer}{genentech}
\icmlauthor{Ehsan Hajiramezanali}{genentech}
\icmlauthor{Tommaso Biancalani}{genentech}
\\
\icmlauthor{Chanyoung Park}{kaist}
\icmlauthor{Gabriele Scalia}{genentech}
%\icmlauthor{}{sch}
%\icmlauthor{}{sch}
\end{icmlauthorlist}

\icmlaffiliation{kaist}{KAIST}
\icmlaffiliation{genentech}{Genentech}

\icmlcorrespondingauthor{Gabriele Scalia}{scalia.gabriele@gene.com}
\icmlcorrespondingauthor{Chanyoung Park}{cy.park@kaist.ac.kr}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
\printAffiliationsAndNotice{} % otherwise use the standard text.

% \begin{abstract}
% Recently, large language models (LLMs) have become increasingly popular in the field of drug discovery, often relying on the costly process of fine-tuning due to the abundant data available in the field. 
% However, this expensive fine-tuning strategy hinders the synergistic power of recent progress in LLMs and biological research. 
% In this paper, we propose \proposed, a general multi-agent collaborative framework that can be used for various drug discovery tasks.
% % In particular, \proposed~consists of three specialized teams: the Planning Team, responsible for selecting appropriate data sources; the Molecule Understanding Team, dedicated to analyzing the target molecule using structural data and captions from external databases; and the Molecule Relation Analysis Team, which investigates the structural and biological relationship between the target molecule and related drugs.
% In particular, \proposed~consists of three specialized teams: 
% the Planning Team, responsible for selecting appropriate data sources; 
% the Molecular Relationship Analysis Team, which investigates the structural and biological relationship between the target molecule and related drugs; and 
% the Molecule Understanding Team, dedicated to analyzing the target molecule using structural data and captions from external databases.
% This approach closely mirrors the operations of real-world drug discovery teams, which focus not only on analyzing the target molecule itself but also on understanding its relationships with other drugs.
% Extensive experiments demonstrate the superiority of \proposed~in various drug discovery tasks, achieving effective results without the need for expensive fine-tuning processes.
% Our code is publicly available at \textcolor{magenta}{\url{https://github.com/Namkyeong/DrugAgent}}.
% \end{abstract}

\begin{abstract}

%Recent advances in large language models (LLMs) have shown great potential to accelerate and enhance drug discovery. However, the specialized nature of biochemical data often requires expensive domain-specific fine-tuning, hindering the applicability of general-purpose LLMs for addressing cutting-edge drug discovery questions.Indeed, the continuous influx of new scientific knowledge from experiments and research requires systems that can integrate new insights promptly.To address this challenge, we investigate retrieval-augmented generation (RAG)-empowered LLMs tailored to drug discovery tasks. We propose \proposed, a multi-agent collaborative RAG framework leveraging generalist LLMs for molecular question-answering. The framework employs specialized LLM agents to identify relevant data sources, contextualize query molecules by integrating external knowledge, and summarize relevant evidence to generate accurate responses.  Importantly, \proposed addresses key challenges stemming from applying RAG-based workflows to biochemical data, including data heterogeneity, ambiguity, and multi-source integration. \proposed can effectively leverage new data without fine-tuning, and we demonstrate its flexibility and superior performance across diverse drug discovery tasks.

%- Edward suggestion - 

% Recent advances in large language models (LLMs) have shown great potential to accelerate drug discovery.
% However, the specialized nature of biochemical data has hitherto prevented a direct usage of general purpose LLMs for cutting-edge drug discovery questions, requiring instead expensive domain-specific fine-tuning. The speed of progress in biology and chemistry has only exacerbated that problem, prompting regular rounds of fine-tuning to keep the model up-to-date with the latest scientific advances.
% In this work, we investigate whether agentic systems empowered with retrieval augmented generation (RAG) can effectively provide an efficient way to solve drug discovery problems, by relying on general purposes LLMs and directly connecting to up-to-date data sources. To that end, we introduce~\proposed, a dedicated multi-agent system that can retrieve and reason upon data from molecular property databases and biomedical knowledge graphs. For a given task, our framework automatically identifies relevant data sources, integrates the evidence, and provides informed predictions. Through extensive benchmarking, we show that our approach outperforms dedicated language models fine-tuned on drug discovery, as well as state-of-the-art graph neural networks.

% Recent advances in large language models (LLMs) have shown great potential to accelerate drug discovery.
% However, the specialized nature of biochemical data has hitherto prevented a direct usage of general purpose LLMs for cutting-edge drug discovery questions, requiring instead expensive domain-specific fine-tuning.
% The speed of progress in the field of AI and biochemistry has exacerbated that problem, requiring regular rounds of fine-tuning to keep the model up-to-date with the latest scientific advances, which is impractical.
% To this end, we investigate whether agentic systems empowered with retrieval augmented generation (RAG) can effectively provide an efficient way to solve drug discovery problems, by relying on general purposes LLMs and directly connecting to up-to-date data sources.
% Specifically, we introduce~\proposed, a dedicated team-based multi-agent system that can automatically identify relevant data sources, integrate the evidence, and provide informed predictions with collaborative teamwork.
% Extensive experiments highlight the superiority of \proposed~in various drug discovery tasks, showcasing exceptional flexibility and interpretability that enables genuine interaction with the user.

Recent advances in large language models (LLMs) have shown great potential to accelerate drug discovery. However, the
specialized nature of biochemical data often necessitates costly domain-specific fine-tuning, posing critical challenges. 
First, it hinders the application of more flexible general-purpose LLMs in cutting-edge drug discovery tasks. 
More importantly, it impedes the rapid integration of the vast amounts of scientific data continuously generated through experiments and research.
To investigate these challenges, we propose \proposed, a retrieval-augmented generation (RAG)-empowered agentic system tailored to drug discovery tasks. Through the collaboration of multiple LLM agents, \proposed dynamically retrieves information from biomedical knowledge bases, contextualizes query molecules, and integrates relevant evidence to generate responses --- all without the need for domain-specific fine-tuning. 
Crucially, we tackle key obstacles in applying RAG workflows to biochemical data, including data heterogeneity, ambiguity, and multi-source integration.
We demonstrate the flexibility and effectiveness of this framework across a variety of drug discovery tasks, showing that it outperforms general-purpose and domain-specific LLMs as well as traditional deep learning approaches.
% The source code for \proposed~is available at~\url{https://anonymous.4open.science/r/CLADD-0281}.


\end{abstract}

\section{Introduction}
\label{sec: Introduction}
%\subfile{sections/introduction}
\subfile{sections/introduction_v2}

\section{Related Work}
\label{sec: related_works}
\subfile{sections/related_works}

% \section{Preliminaries}
% \label{sec: Preliminaries}
% \subfile{sections/preliminaries}

\section{Methodology}
\label{sec: Methodology}
\subfile{sections/methodology}

\section{Experiments}
\label{sec: Experiments}
\subfile{sections/experiments}

% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocite
% \nocite{langley00}

\clearpage
% \section*{Impact Statement}
% This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here.
\bibliography{icml}
\bibliographystyle{icml2025}

\clearpage

\onecolumn
\appendix

This is an appendix for the paper \textbf{RAG-Enhanced Collaborative LLM Agents for Drug Discovery}.

\section{Additional Related Works}
\label{app: Related Works}
\subfile{appendix/related_works}

\section{Preliminaries}
\label{app: Preliminaries}
\subfile{sections/preliminaries}

\section{Datasets}
\label{app: Datasets}
\subfile{appendix/dataset}

\section{Baselines Setup}
\label{app: Baselines}
\subfile{appendix/baselines}


\section{Implementation Details}
\label{app: Implementation Details}
\subfile{appendix/implementation}


\section{Additional Experimental Results}
\label{app: additional experiments}
\subfile{appendix/experiments}

\clearpage
\section{Agent Templates}
\label{app: agent templates}
In this section, we provide the templates for each agent used in Section \ref{sec: Methodology}.
% Templates for each agent described in Section \ref{sec: Methodology} are provided.

\input{tables/prompt_caption_evaluator}
\input{tables/prompt_kg_evaluator}
\input{tables/prompt_bra}
\input{tables/prompt_sra}
\input{tables/prompt_tma}
\input{tables/prompt_task_agent}

\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
