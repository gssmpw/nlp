In this section, we provide additional experimental results that can supplement our experimental results in Section \ref{sec: Experiments}.

% \subsection{Single Cell Perturbation Task}

% This task evaluates the ability of models to predict how cells respond to small molecule drug perturbations by estimating transcriptional responses given an input molecule in a specific cell type. The ability to assess the biological effects of molecular interventions offers a key opportunity to uncover cellular functions and causal links and has the potential to significantly advance biomedical research~\cite{siplex}.
% However, these experiments are costly, labor-intensive, and often impractical for high-throughput transcriptomic screening across different cell types and tissues~\citep{siplex},  highlighting the need for AI models to streamline and accelerate the process.

% In this study, similar to the drug-target prediction task~ (Section~\ref{exp:Protein Target Prediction Task}),  we formalize this task as a general question-answering problem, showcasing the flexibility of \proposed.

% \input{tables/app_perturbation}

% \textbf{Datasets.} We use the dataset curated in \citet{open-problems-single-cell-perturbations}, which includes single-cell perturbational outcomes for 144 compounds from LINCS~\cite{subramanian2017next} across different cell types. In particular, single-cell gene expression profiles after 24 hours of treatment with a specific molecule are reported, and differential expression (DE) analysis is included. We use these values to identify the top-$k$ genes up-regulated in each cell type applying a given molecule. 

% \textbf{Methods Compared.}
% We evaluate two general-purpose LLMs—GPT-4o mini and GPT-4o.

% \textbf{Evaluation Protocol.}
% We evaluate the performance of LLMs in a zero-shot setting. Specifically, for a given target molecule, we prompt the LLMs to \textit{generate the top 5 genes up-regulated in each cell type after applying the target molecule}.
% We then calculate the precision by counting how many of the correct answers are present in the LLMs' outputs.

% \textbf{Experimental Results.}
% In Table \ref{app tab: perturbation}, we observe that LLMs face difficulties in identifying genes up-regulated after applying the target molecule, highlighting the complexity of this task.
% We observe that GPT-4o always outputs \textit{``I'm sorry, but I can't provide predictions for gene expression changes based on the chemical structure of a compound without experimental data or a specific computational model trained for this purpose."}.
% Despite the task's complexity, we observe that \proposed~consistently outperforms GPT-4o mini, further confirming the improvement given by our RAG-based system.




\subsection{Additional Ablation Studies}
\label{app: additional ablation studies}
In Table \ref{app tab: ablations}, we conduct a model analysis by removing one component of the model at a time. We have the following observations:
\textbf{1)} By comparing ``Only Expert Annotation'' and ``Only Generated Caption'', we observe that relying solely on expert annotations yields significantly better performance. This highlights the critical importance of human-generated annotations over machine-generated captions.
\textbf{2)} Among the three agents—DrugRel Agent, BioRel Agent, and MU Agent—we could not determine a clear superiority in their relative importance, as the agent leading to the best performance by itself varied by task (Activation or Inhibition). 
\textbf{3)} Overall, we observe a decline in performance when any single component of \proposed~is removed, emphasizing the significance of each component.

Moreover, we perform additional ablation studies in the property-specific molecule captioning tasks in Figure \ref{app fig: ablations captioning}.
We observe that including all components (i.e., \proposed) leads to best performance except for the BACE dataset.
This is because, as illustrated in Figure \ref{app fig: data planning}, the BACE dataset contains minimal relevant information in both the annotation database and the knowledge graph. 
Consequently, the model derives minimal benefit from external knowledge, highlighting the critical role of having relevant external information.


\begin{figure}[h]
    \centering
    \begin{minipage}{0.54\linewidth}
        \input{tables/app_ablations}
    \end{minipage}
    \begin{minipage}{0.45\linewidth}
        \centering
        \vspace{4ex}
        \includegraphics[width=0.95\linewidth]{imgs/ablations_captioning.pdf}
        \vspace{-2ex}
        \caption{Ablation studies in the property-specific molecular captioning task.}
        \label{app fig: ablations captioning}
    \end{minipage}
\end{figure}




\subsection{Additional External Knowledge Analysis}
\label{app: additional external knowledge analysis}
In Figure \ref{app fig: model analysis}, we analyze how external knowledge is used during the decision-making process for drug-target prediction tasks.
We have the following observations:
\textbf{1)} As shown in Figure \ref{app fig: model analysis} (a) and (b), the average length of human descriptions is considerably longer in the ``Correct" case, and the number of retrieved 2-hop paths is notably higher in the ``Correct" case. 
This highlights the importance of having external information that is both high quality and abundant.
\textbf{2)} On the other hand, although we anticipated a higher proportion of 2-hop paths containing Gene/Protein entities in the ``Correct" case, no significant difference was observed between the ``Correct" and ``Incorrect" cases in Figure \ref{app fig: model analysis} (c) and (d).
From these results, we argue that \proposed's performance is not solely reliant on retrieving external information that is directly linked to the correct answer, given that external information can be further processed and contextualized by the agents, also integrating different sources of evidence.
\begin{figure}[h]
    \centering
    \includegraphics[width=0.95\linewidth]{imgs/model_analysis_chempert.pdf}
    \caption{External knowledge analysis results. (a) The average length of retrieved human descriptions, (b) the average number of retrieved 2-hop paths in the knowledge graph, and (c) the proportion of entity types in 2-hop paths for correct and incorrect cases.}
    \label{app fig: model analysis}
\end{figure}

In Figure \ref{app fig: data planning}, we examine how the Planning Team determines the use of the captioning tool and collaborates with the Knowledge Graph Team based on the datasets.
We observed that, in most cases, the KG was used for more than 50\% of the query molecules, with the BACE and Skin Reaction datasets as significant exceptions. 
Furthermore, we observed that the BACE and hERG datasets lacked corresponding annotations for all query molecules.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\linewidth]{imgs/data_planning.pdf}
    \caption{Planning team decision analysis based on different datasets. ``No CT" signifies that the planning team has decided not to utilize the captioning tool, while ``Use KG" indicates that the planning team intends to involve the Knowledge Graph Team.}
    \label{app fig: data planning}
\end{figure}

\clearpage
\subsection{Additional Case Studies}
\label{app: additional case studies}
In this section, we provide additional case studies to analyze the behavior \proposed.
In Figure \ref{app fig: qualitative}, we observe that all three agents consistently predict dopamine-related and serotonin-related proteins as targets.
Based on the reports, Prediction Agent prioritizes these proteins over Cytochrome P450-related enzymes in their predictions.
Thus, we argue that our system efficiently prioritizes relevant information based on consensus, functioning similarly to a majority voting system.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.99\linewidth]{imgs/Qualitative_app1.pdf}
    \caption{Additional case studies. Red represents dopamine-related proteins, yellow represents serotonin-related proteins, and green represents Cytochrome P450-related enzymes.}
    \label{app fig: qualitative}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.99\linewidth]{imgs/Qualitative_full.pdf}
    \caption{Full version of Figure \ref{fig: case studies}.}
    \label{app fig: qualitative full}
\end{figure}


% \begin{figure}[h]
%     \centering
%     \includegraphics[width=0.99\linewidth]{imgs/Qualitative_app2.pdf}
%     \caption{Additional case studies. Yellow represents serotonin-related proteins and red representes dopamine-related proteins.}
%     \label{app fig: qualitative2}
% \end{figure}