In this section, we provide further details on the baselines we used in Section \ref{sec: Experiments}.
For all baseline models, we utilize the pre-trained checkpoints provided by the authors of the original papers.
\input{tables/baselines}

\begin{itemize}[leftmargin=.1in]
\item \textbf{Galactica} \cite{taylor2022galactica} is a large language model designed to store, integrate, and reason over scientific knowledge. The authors demonstrate Galactica's capabilities in simple molecule understanding tasks, such as predicting IUPAC names and performing binary classification for molecular property prediction.
We also fine-tune Galactica for the Drug-Target Prediction task described in Section \ref{sec: Experiments}, using molecules linked to more than five proteins for activation and inhibition. 
For fine-tuning, we search for the optimal hyperparameters by experimenting with learning rates of $\{1e-3, 1e-4, 1e-5, 1e-6\}$ and epochs of $\{50, 100, 150, 200\}$, reporting the best performance achieved.

\item \textbf{GIMLET} \cite{zhao2023gimlet} introduces a unified approach to leveraging language models for both graph and text data. The authors aim to enhance the generalization ability of language models for molecular property prediction through instruction tuning.
\item \textbf{LlaSMol} \cite{yu2024llasmol} presents a large-scale, comprehensive, and high-quality dataset designed for instruction tuning of large language models. This dataset includes tasks such as name conversion, molecule description, property prediction, and chemical reaction prediction, and it is used to fine-tune different open-source LLMs.
% \item \textbf{MolecularGPT} \cite{liu2024moleculargpt} introduces a few-shot approach for molecular property prediction by using the target properties of structurally similar molecules as examples.
\end{itemize}