\noindent \textbf{LLMs with Knowledge Graphs.}
While large language models (LLMs) have been successfully adapted to numerous domains, they have faced criticism for their lack of factual accuracy. Specifically, LLMs often struggle to recall reliable facts and are prone to hallucinations~\cite{ji2023survey}, which can be a bottleneck for scientific applications.
A promising approach to mitigate these issues is the integration of external knowledge sources, such as knowledge graphs (KGs), into LLMs during the generation process. 
For instance, \citet{baek2023knowledge} proposes a method where relevant triplets are retrieved from KGs based on the input query. These triplets are then verbalized and provided as additional input to the LLM, enhancing its factual grounding and accuracy.
KG-Rank~\cite{yang2024kg} focuses on medical question-answering, leveraging a medical knowledge graph to match terms in the question and expand them. DALK~\cite{li2024dalk} leverages an LLM to construct an Alzheimer's disease-specific KG, which is then used to enhance the accuracy and relevance of LLM-generated responses.
Although these methods retrieve entities from KGs that are related to those in the query, the virtually infinite number of potential molecules of interest in drug discovery, combined with the limited domain expertise of general-purpose LLMs, makes it challenging to directly apply existing techniques to molecular question-answering.