\section{Related Work}
%In this subsection, we highlight the most closely related concurrent contributions to contextualize our work.
%\paragraph{Differential privacy and Statistics / Learning.}
%The task of estimating various quantities under differential privacy has garnered growing interest over the past decade. A selection of references includes, but is not limited to 
%\cite{wasserman2010statistical,barber2014privacy,diakonikolas2015differentially,karwa2017finite,bun2019privatehypothesis,bun2021privatehypothesis,kamath2019highdimensional,biswas2020coinpress,kamath2020heavytailed,acharya2021differentially,lalanne:thesis,adenali2021unbounded,cai2021cost,brown2021covariance,cai2021cost,kamath2022improved,lalanne2023private,lalanne2022private,lalanne2024privatedensity,singhal2023polynomial,kamath2023biasvarianceprivacy,kamath2023new}. Notably, it is possible to perform advanced private learning tasks such as deep learning with a technique called DP-SDG for \emph{differentially private stochastic gradient descent} \cite{abadi2016deep}. The method that we will present in this article is typically aimed at being a \emph{data-dependent} regulizer for this framework penalizing with discrepencies of prescribed empirical measures.
\paragraph{Differential Privacy and Optimal Transport}
Our analysis aligns with the work of \cite{rakotomamonjy2021DPslicedWasserstein}, which extends the ideas from \cite{harder2021dp}—originally applied to the Maximum Mean Discrepancy (MMD)—to the sliced Wasserstein loss. This work establishes privacy guarantees for the value of the sliced Wasserstein distance. However, the privacy guarantees are insufficient for training models privately, except in simple scenarios such as the generative model proposed in \cite{harder2021dp}. In contrast, our work is significantly broader in scope, and adapts to a wider range of problems, as discussed in \cref{remark:comparison}. \cite{Liu_Yu} follow the same line of \cite{rakotomamonjy2021DPslicedWasserstein}, extending their methodology to an alternative definition of the sliced Wasserstein distance.  In a different vein, other existing works develop task-specific private methodologies leveraging optimal transport. The sliced Wasserstein distance has been applied in data generation by \cite{segag2023gradientFlow} from a different approach based on gradient flows. \cite{tien2019DPOTdomainAdapt} tackled differentially private domain adaptation with optimal transport by perturbing the optimal coupling between noisy data. Recently, \cite{xian2024DPfairRegression} proposed a post-processing method based on the Wasserstein barycenter of private histogram estimators of conditional densities to obtain a fair and private regressor. Beyond these approaches, optimal transport has also been explored in novel privacy paradigms unrelated to our work \cite{pierquin2024Pufferfish,Kawamoto2019localObfuscation,yang2024wassersteinDP}.
\paragraph{Fairness in Machine Learning.}
Fairness in machine learning has emerged as a critical area of research, driven  by the growing recognition of its societal impact and the ethical implications of algorithmic decision-making. Additionally, regulatory frameworks such as the General Data Protection Regulation (GDPR) and the recent European AI Act\footnote{\url{https://artificialintelligenceact.eu/}} mandate stringent measures to identify and mitigate bias in AI systems, emphasizing the need for fair and private methodologies in machine learning. Unfairness arises when certain variables, often referred to as sensible variable, systematically bias the behavior of an algorithm against specific groups of individuals, leading to disparate outcomes. This field of research has received a growing attention over the last few years as pointed out in the following papers and references therein \cite{chouldechova2020snapshot,dwork2012fairness,oneto2020fairness,wang2022brief,barocas2018fairness,besse2022survey}.

The Wasserstein distance offers a compelling framework for addressing fairness, as it provides a principled way to quantify discrepancies between the distributions of different subgroups. Moreover, as stated first in \cite{feldman2015certifying}, then in \cite{gouic2020projection} or \cite{chzhen2020fair}, Wasserstein distance between the conditional distributions of the algorithm  for each group, is the natural  measure to quantify the cost of ensuring fairness of the algorithm, defined as algorithms exhibiting the same behavior for each group. Hence optimal transport based methods are commonly used to  assess and mitigate distributional biases, paving the way for more equitable algorithmic decision-making. We refer, for instance, to the previously mentioned references \cite{silvia2020general,gordaliza2019obtainingFairness} and references therein.


\paragraph{Differential Privacy and Fair Learning.}

The interplay between fairness and differential privacy has received significant attention in recent years. A comprehensive review of this topic in decision and learning problems is provided in \cite{fioretto2022DPandfairness}. Within the learning framework, research has progressed in various directions. From a theoretical standpoint, despite the early work of \cite{cummings2019compatibility} demonstrating inherent incompatibilities between exact fairness and differential privacy, \cite{mangold2023boundedimpact} recently presented promising theoretical results indicating that fairness is not severely compromised by privacy in classification tasks. Another research direction has focused on studying the disparate impacts on model accuracy introduced by private training of algorithms. This phenomenon was first observed in \cite{bagdasaryan2019disparateimpact} and has been extensively studied in subsequent works \cite{farrand2020neithePrivateNorFair,tran21fairnesslens,xu2021removingDisparateImpact,esipova2023gradientMisalignment}. A third line of research aims to develop models that are both private and fair. Private and fair classification models have been proposed using in-processing and post-processing techniques across various scenarios in \cite{xu2019achievingDPandFairness,jagielski2019differentially,ding2020differentially,lowi2022SDPfairLearning,yaghini2023learning,Ghoukasian2024DPFairBinaryClassif}. A recent comparison of these works can be found in \cite{Ghoukasian2024DPFairBinaryClassif}. 
In the topic of fair and private regression, the only available work is the aforementioned post-processing method of \cite{xian2024DPfairRegression}, which is limited to one-dimensional case.