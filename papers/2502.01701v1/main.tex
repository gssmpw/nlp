%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables
\usepackage{mathrsfs}


% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
%\usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
%\usepackage[accepted]{icml2025}
\usepackage[preprint]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{dsfont}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}

\newcommand{\R}[0]{\mathbb{R}}
\newcommand{\C}[0]{\mathbb{C}}
\newcommand{\N}[0]{\mathbb{N}}
\newcommand{\Z}[0]{\mathbb{Z}}
\newcommand{\Q}[0]{\mathbb{Q}}
\newcommand{\Prob}[0]{\mathbb{P}}
\newcommand{\prob}[0]{\mathbb{p}}
\newcommand{\E}[0]{\mathbb{E}}
\newcommand{\V}[0]{\mathbb{V}}
\newcommand{\Ind}[0]{\mathds{1}}
\newcommand{\argmin}[0]{\operatorname{argmin}}
\newcommand{\argmax}[0]{\operatorname{argmax}}
\newcommand{\eqdef}{\vcentcolon=}
\newcommand{\defeq}{=\vcentcolon}
\def\P{\mathbb{P}}
\def\one{\mathbb{1}}
\newcommand\mech[1]{{#1}}
\newcommand\vect[1]{\mathbf{#1}}
\newcommand\dom[1]{\operatorname{dom}\left({#1}\right)}
\newcommand\codom[1]{\operatorname{codom}\left({#1}\right)}
\newcommand\ninf[0]{n \rightarrow +\infty}
\newcommand\conv[1]{\overset{}{\underset{#1}{\longrightarrow}}}
\newcommand\distconv[1]{\overset{\mathcal{L}}{\underset{#1}{\longrightarrow}}}
\newcommand\probconv[1]{\overset{\Prob}{\underset{#1}{\longrightarrow}}}
\newcommand\asconv[1]{\overset{\text{a.s.}}{\underset{#1}{\longrightarrow}}}
\newcommand{\iid}{\stackrel{{\text{i.i.d.}}}{\sim}}
\newcommand\tv[2]{\mathrm{TV}\left( {#1}, {#2} \right)}
\newcommand\kl[2]{\mathrm{KL}\left( \left. {#1}\right\Vert {#2} \right)}
\newcommand\renyi[3]{\text{D}_{#1}\left( \left. {#2}\right\Vert {#3} \right)}
\newcommand\ham[2]{d_\mathrm{ham}\left( {#1}, {#2} \right)}
\newcommand\hamext[1]{d_\mathrm{ham}\left( {#1}\right)}
\newcommand\intslice[2]{\left\{ {#1}, \dots,  {#2} \right\}}
\newcommand\p[1]{\left( {#1}\right)}
\newcommand\ceil[1]{\left\lceil {#1}\right\rceil}
\newcommand\floor[1]{\left\lfloor {#1}\right\rfloor}
\newcommand\card[1]{\# \left( {#1}\right)}
\newcommand\myop{\ensuremath\mathrel{\raisebox{1pt}{\stackunder{$<$}{\rotatebox{-27}{\resizebox{7pt}{2pt}{$\sim$}}}}}}
\newenvironment{sproof}{%
  \renewcommand{\proofname}{Sketch of proof}\proof}{\endproof}
\newcommand\set[1]{\mathcal{#1}}
\newcommand\proj[0]{\operatorname{Proj}}

\newcommand\green[1]{\textcolor{ao(english)}{#1}}
\newcommand\red[1]{\textcolor{red}{#1}}
\newcommand\orange[1]{\textcolor{orange}{#1}}
\newcommand\blue[1]{\textcolor{blue}{#1}}
\newcommand\purple[1]{\textcolor{purple}{#1}}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Submission and Formatting Instructions for ICML 2025}

\begin{document}

\twocolumn[
\icmltitle{Learning with Differentially Private (Sliced) Wasserstein Gradients}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{David Rodríguez-Vítores}{aff3}
\icmlauthor{Clément  Lalanne}{aff1}
\icmlauthor{Jean-Michel Loubes}{aff1,aff2}
\end{icmlauthorlist}

\icmlaffiliation{aff1}{Institut de Mathématiques de Toulouse, UMR5219, Université de Toulouse, CNRS, UPS, F-31062 Toulouse Cedex 9, France}
\icmlaffiliation{aff2}{Inria, France}
\icmlaffiliation{aff3}{Universidad de Valladolid and IMUVA, Valladolid, Spain}

\icmlcorrespondingauthor{Clément Lalanne}{clement.lalanne@math.univ-toulouse.fr}
\icmlcorrespondingauthor{David Rodríguez-Vítores}{david.rodriguez.vitores@uva.es}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in]


% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
In this work, we introduce a novel framework for privately optimizing objectives that rely on Wasserstein distances between data-dependent empirical measures. Our main theoretical contribution is, based on an explicit formulation of the Wasserstein gradient in a fully discrete setting, a control on the sensitivity of this gradient to individual data points, allowing strong privacy guarantees at minimal utility cost. Building on these insights, we develop a deep learning approach that incorporates gradient and activations clipping, originally designed for DP training of problems with a finite-sum structure. We further demonstrate that privacy accounting methods extend to Wasserstein-based objectives, facilitating large-scale private training. Empirical results confirm that our framework effectively balances accuracy and privacy, offering a theoretically sound solution for privacy-preserving machine learning tasks relying on optimal transport distances such as Wasserstein distance or sliced-Wasserstein distance.
\end{abstract}

\section{Introduction}


Optimal transport distances have been shown to be a powerful tool for measuring discrepancies between distributions in learning problems. Given two probabilities $P$ and $Q$  in $\mathbb R^d$, the Wasserstein distance $W_p(P,Q)$ is defined as the $p^{th}$ root of the cost associated to the optimal transport plan, i.e., 
\begin{equation*}
    W_p(P,Q)= \Bigl( \inf_{\pi\in \Pi(P,Q)}\  \int_{\mathbb R^d} \|x-y\|_2^p d\pi(x,y) \Bigr)^{1/p}
\end{equation*}
where $\Pi(P,Q)$ represents the set of probabilities in the product space with marginals $P$ and $Q$. Except mentioned otherwise, this article will look at the specific case of $W_2$. Its straightforward geometric interpretation makes it an effective tool for comparing distributions even when the supports do not align, offering a significant advantage over other widely used metrics and divergences.  Thus, it has been successfully applied in a number of areas, including generative models \cite{arjovski2017WGAN}, representation learning \cite{toltstikhin2018WAE}, domain adaptation \cite{courty2017OTDomainAdapt} and fairness \cite{gordaliza2019obtainingFairness,risser2022tacklingAlgorithm,de2024transport,jiang2020wasserstein,chzhen2020fair,gaucher2023fair}. 

To tackle the curse of dimensionality in its computations, two main alternatives have been explored, namely, approximating the OT cost by an entropic regularization, as proposed in~\cite{cuturi2013sinkhorn}, or leveraging the use the Wasserstein distance between one-dimensional projections.

Indeed, denoting by, for any probability distribution $P$ on $\R$, $F_P$ its cumulative distribution function (CDF) 
and $F_P^{-1}$ its quantile function, which is defined as the generalized inverse of $F_P$,
then the $W_2$ Wasserstein distance satisfies $W_2(P, Q) = \| F_P^{-1} - F_Q^{-1}\|_{L^2((0, 1))}
$
for any probability measures $P, Q$ on $\R$.
This perspective has inspired a variety of distance surrogates that incorporate one-dimensional projections. In this work, we center our attention on the sliced Wasserstein distance \cite{rabin2011wassersteinBarycenter,bonnel2015slicedRadon}, defined as
\begin{equation*}
    SW_2(P,Q)= \Bigl(  \int_{\mathbb S^{d-1}} W_2^2(\operatorname{Pr}_\vartheta \# P, \operatorname{Pr}_\vartheta  \# Q) d\mu(\vartheta) \Bigr)^{1/2} \;,
\end{equation*}
where $\#$ is the \emph{push forward} operation of a measure by a measurable mapping, $\operatorname{Pr}_\vartheta$ the projection along the direction of $\vartheta$, and $\mu$ denotes the uniform measure on the unit sphere $\mathbb S^{d-1}$. Note that the integral on the sphere may be approximated by Monte-Carlo methods.
A substantial body of research has demonstrated the effectiveness of the sliced Wasserstein distance as a discrepancy measure for generative modeling \cite{deshpande2018generativelModelling,wu2019slicedGenerativeModels}, representation learning \cite{Kolouri2018SWAE}, domain adaptation \cite{Chen2019slicedWasserteinDiscrepancy} and fairness \cite{risser2022tacklingAlgorithm}. 

In parallel, analyzing statistics derived from real user data introduces new challenges, particularly regarding privacy. It is well-established that releasing statistics based on such data, without proper safeguards, can lead to severe consequences \cite{narayanan2006break,backstrom2007wherefore,fredrikson2015model,dinur2003revealing,homer2008resolving,loukides2010disclosure,narayanan2008robust,sweeney2000simple,wagner2018technical,sweeney2002k}.

To address these issues, differential privacy \cite{dwork2006calibrating} has emerged as the leading standard for privacy protection. Differential privacy incorporates randomness into the computation process, ensuring that the estimator relies not only on the dataset, but also on an additional source of randomness. This mechanism obscures the influence of individual data points, safeguarding user privacy. Prominent organizations like the US Census Bureau~\citep{abowd2018us}, Google~\citep{erlingsson2014rappor}, Apple~\citep{thakurta2017learning}, and Microsoft~\citep{ding2017collecting} have adopted this approach. Notably, an extended body of literature studies the interplay between privacy and learning / statistics \cite{wasserman2010statistical,barber2014privacy,diakonikolas2015differentially,karwa2017finite,bun2019privatehypothesis,bun2021privatehypothesis,kamath2019highdimensional,biswas2020coinpress,kamath2020heavytailed,acharya2021differentially,lalanne:thesis,adenali2021unbounded,cai2021cost,brown2021covariance,cai2021cost,kamath2022improved,lalanne2023private,lalanne2022private,lalanne2024privatedensity,singhal2023polynomial,kamath2023biasvarianceprivacy,kamath2023new}.



\subsection{Contributions}

The main contribution of this  work is to present a framework to privately optimize problems involving Wasserstein distances between data-dependent empirical measures. 
%The applications of this method range for instance from fairness to domain adaptation, representation learning and data generation, depending on the use cases. 
This general contribution can be split into as follows.


    \textbf{1) A tight sensitivity analysis leading to privacy at low cost.}
    Despite $W_2$ not enjoying the typical finite sum structure (e.g. $\text{loss} = \frac{1}{n} \sum_{i = 1}^n g_\theta(x_i)$), we prove that its gradient has a decomposition that is favorable for privacy analysis and that is compatible with standard autodifferentiation frameworks \cite{tensorflow2015-whitepaper,NEURIPS2019_9015,jax2018github}.
    We prove in \Cref{sec:SensitivityPrivacy} that the sensitivity of this gradient (i.e. how much the gradients are allowed to change when changing on individual's data) roughly vanishes as $\frac{1}{n}$ where $n$ is the sample size. The implications of this observation are that it is possible to leverage classical tools in differential privacy to obtain privacy at a vanishing cost in tasks utilizing those gradients.

    \textbf{2) A deep learning framework.}
    As is often the case with differential privacy, the privacy analysis typically assumes that a prescribed set of data-dependent quantities are bounded. In practice, this is often not the case, and one has to resort to the use of clipping \cite{abadi2016deep}, which leads to biases \cite{kamath2023biasvarianceprivacy} in the estimation procedure. Despite the problem not enjoying a finite-sum structure, we show in \Cref{sec:DeepLearningFramework} that similar tricks are applicable to Wasserstein gradients. In addition to that, \Cref{sec:DeepLearningFramework} also demonstrates that privacy accounting \cite{abadi2016deep,dong2019gaussian} is still applicable to Wasserstein gradients, allowing for deep learning and scalable applications.



\subsection{Related Work}

%In this subsection, we highlight the most closely related concurrent contributions to contextualize our work.
%\paragraph{Differential privacy and Statistics / Learning.}
%The task of estimating various quantities under differential privacy has garnered growing interest over the past decade. A selection of references includes, but is not limited to 
%\cite{wasserman2010statistical,barber2014privacy,diakonikolas2015differentially,karwa2017finite,bun2019privatehypothesis,bun2021privatehypothesis,kamath2019highdimensional,biswas2020coinpress,kamath2020heavytailed,acharya2021differentially,lalanne:thesis,adenali2021unbounded,cai2021cost,brown2021covariance,cai2021cost,kamath2022improved,lalanne2023private,lalanne2022private,lalanne2024privatedensity,singhal2023polynomial,kamath2023biasvarianceprivacy,kamath2023new}. Notably, it is possible to perform advanced private learning tasks such as deep learning with a technique called DP-SDG for \emph{differentially private stochastic gradient descent} \cite{abadi2016deep}. The method that we will present in this article is typically aimed at being a \emph{data-dependent} regulizer for this framework penalizing with discrepencies of prescribed empirical measures.
\paragraph{Differential Privacy and Optimal Transport}
Our analysis aligns with the work of \cite{rakotomamonjy2021DPslicedWasserstein}, which extends the ideas from \cite{harder2021dp}—originally applied to the Maximum Mean Discrepancy (MMD)—to the sliced Wasserstein loss. This work establishes privacy guarantees for the value of the sliced Wasserstein distance. However, the privacy guarantees are insufficient for training models privately, except in simple scenarios such as the generative model proposed in \cite{harder2021dp}. In contrast, our work is significantly broader in scope, and adapts to a wider range of problems, as discussed in \cref{remark:comparison}. \cite{Liu_Yu} follow the same line of \cite{rakotomamonjy2021DPslicedWasserstein}, extending their methodology to an alternative definition of the sliced Wasserstein distance.  In a different vein, other existing works develop task-specific private methodologies leveraging optimal transport. The sliced Wasserstein distance has been applied in data generation by \cite{segag2023gradientFlow} from a different approach based on gradient flows. \cite{tien2019DPOTdomainAdapt} tackled differentially private domain adaptation with optimal transport by perturbing the optimal coupling between noisy data. Recently, \cite{xian2024DPfairRegression} proposed a post-processing method based on the Wasserstein barycenter of private histogram estimators of conditional densities to obtain a fair and private regressor. Beyond these approaches, optimal transport has also been explored in novel privacy paradigms unrelated to our work \cite{pierquin2024Pufferfish,Kawamoto2019localObfuscation,yang2024wassersteinDP}.
\paragraph{Fairness in Machine Learning.}
Fairness in machine learning has emerged as a critical area of research, driven  by the growing recognition of its societal impact and the ethical implications of algorithmic decision-making. Additionally, regulatory frameworks such as the General Data Protection Regulation (GDPR) and the recent European AI Act\footnote{\url{https://artificialintelligenceact.eu/}} mandate stringent measures to identify and mitigate bias in AI systems, emphasizing the need for fair and private methodologies in machine learning. Unfairness arises when certain variables, often referred to as sensible variable, systematically bias the behavior of an algorithm against specific groups of individuals, leading to disparate outcomes. This field of research has received a growing attention over the last few years as pointed out in the following papers and references therein \cite{chouldechova2020snapshot,dwork2012fairness,oneto2020fairness,wang2022brief,barocas2018fairness,besse2022survey}.

The Wasserstein distance offers a compelling framework for addressing fairness, as it provides a principled way to quantify discrepancies between the distributions of different subgroups. Moreover, as stated first in \cite{feldman2015certifying}, then in \cite{gouic2020projection} or \cite{chzhen2020fair}, Wasserstein distance between the conditional distributions of the algorithm  for each group, is the natural  measure to quantify the cost of ensuring fairness of the algorithm, defined as algorithms exhibiting the same behavior for each group. Hence optimal transport based methods are commonly used to  assess and mitigate distributional biases, paving the way for more equitable algorithmic decision-making. We refer, for instance, to the previously mentioned references \cite{silvia2020general,gordaliza2019obtainingFairness} and references therein.


\paragraph{Differential Privacy and Fair Learning.}

The interplay between fairness and differential privacy has received significant attention in recent years. A comprehensive review of this topic in decision and learning problems is provided in \cite{fioretto2022DPandfairness}. Within the learning framework, research has progressed in various directions. From a theoretical standpoint, despite the early work of \cite{cummings2019compatibility} demonstrating inherent incompatibilities between exact fairness and differential privacy, \cite{mangold2023boundedimpact} recently presented promising theoretical results indicating that fairness is not severely compromised by privacy in classification tasks. Another research direction has focused on studying the disparate impacts on model accuracy introduced by private training of algorithms. This phenomenon was first observed in \cite{bagdasaryan2019disparateimpact} and has been extensively studied in subsequent works \cite{farrand2020neithePrivateNorFair,tran21fairnesslens,xu2021removingDisparateImpact,esipova2023gradientMisalignment}. A third line of research aims to develop models that are both private and fair. Private and fair classification models have been proposed using in-processing and post-processing techniques across various scenarios in \cite{xu2019achievingDPandFairness,jagielski2019differentially,ding2020differentially,lowi2022SDPfairLearning,yaghini2023learning,Ghoukasian2024DPFairBinaryClassif}. A recent comparison of these works can be found in \cite{Ghoukasian2024DPFairBinaryClassif}. 
In the topic of fair and private regression, the only available work is the aforementioned post-processing method of \cite{xian2024DPfairRegression}, which is limited to one-dimensional case. 


\section{Differential Privacy}


Differential privacy \cite{dwork2006calibrating} starts with fixing a dataset space $\set{D}$, the space in which we expect the dataset to live, and a neighboring relation $\sim$ on $\set{D}$. For $\vect{D}, \vect{\Tilde D} \in \set{D}$, we write $\vect{D} \sim \vect{\Tilde D}$ when $\vect{D}$ and $\vect{\Tilde D}$ are \emph{neighbors} (see below). Differential privacy then imposes that a \emph{randomized} mechanism (i.e. a conditional kernel of probabilities) $\mech{M} : \set{D} \rightarrow \set{O}$ makes 
$\mech{M}(\vect{D})$ hard to discriminate (in a statistical sense) from $\mech{M}(\vect{\Tilde D})$ for any pair of neighbors $\vect{D} \sim \vect{\Tilde D}$.

The neighboring relation $\sim$ is \emph{application specific} and is usually either the \emph{addition / deletion} relation ($\vect{D}$ and $\vect{\Tilde D}$ are neighbors iff one can be obtained from the other by adding or removing the data of one individual from the dataset) or the \emph{replacement} relation ($\vect{D}$ and $\vect{\Tilde D}$ are neighbors iff one can be obtained from the other by changing the data of one individual from either dataset).  In general, it is useful to the reader to understand
$\vect{D} \sim \vect{\Tilde D}$ as : ``The difference between $\vect D$ and $\vect{\Tilde D}$ only comes
from one individual’s data”

In our paper, due to the splitting of the data into separate categories in the Wasserstein distance, and because of potential asymmetry that may arise in their  treatments, we will occasionally employ modified definitions of neighboring relations, which can be encompassed within the following family, indexed by the number of classes $k\geq 1$. Note that the case $k=1$ coincides with the usual \textit{replacement} relation. 

\begin{definition}{(k-end neighboring relation)} \label{definition:neighboring} Let $\mathcal D  = \mathcal D_1^{n_1} \times \ldots \times \mathcal D_k^{n_k}$ be the set of partitioned datasets with sizes $n_1,\ldots,n_k \geq 1$. Given two datasets $\vect D = (\vect{D^1},\ldots,\vect{D^k})$, $\vect{\Tilde D} =  (\vect{\Tilde D^1},\ldots,\vect{\Tilde D^k}) \in \mathcal D$, we say that $\vect D \sim_k \vect{\Tilde D}$
if there exist and index $j\in[k]$ such that $\vect{D^i}= (d^i_1,\ldots,d_{n_i}^i)$ and $\vect{\Tilde D^i}= (\Tilde d^i_1,\ldots,\Tilde d_{n_i}^i)$ coincide up to a
permutation of the elements if $i\neq j$, and up to a permutation and a replacement of one of
the $d_l^i$'s by any element in $\mathcal D_i$ if $i=j$.
\end{definition}


The historic definition of differential privacy \cite{dwork2006calibrating,dwork2006differentialPrivacy,dwork2006our} with $(\epsilon,\delta)$ reads:

\begin{definition}[{$(\epsilon, \delta)$-DP \cite{dwork2006our}}]
\label{definitionConcentratedDifferentialPrivacy}
    A randomized mechanism $\mech{M} : \set{D} \rightarrow \set{O}$ is $(\epsilon, \delta)$-differentially private ($(\epsilon, \delta)$-DP) if $\forall \ \vect{D}\sim \vect{\Tilde D}$, and $\forall$ measurable $S \subset \set{O}$,
\begin{equation*}
 \Prob \p{M(\vect{D}) \in S} \leq e^{\epsilon} \Prob \p{M(\vect{\Tilde D}) \in S} + \delta \;,
\end{equation*}
where the randomness is taken on $M$ only.
\end{definition}
A ubiquitous building block for building private mechanisms is the so-called \emph{Gaussian mechanism} which consists in adding independent Gaussian noise to the output of a deterministic mapping. Quantifying the privacy of this (now randomized) mechanism then boils down to controlling the variations of the deterministic mapping on neighboring datasets, as captured by the following lemma.
\begin{lemma}[Privacy of the Gaussian mechanism (Corollary of Theorem 2.7, Corollary 3.3 and Corollary 2.13 in \cite{dong2019gaussian})]
\label{factProvacyGaussianMechanism}
    Given a deterministic function $h$ mapping a dataset to a quantity in $\R^{d'}$, one can define the $l_2$-sensitivity of $h$ as
    \begin{equation*}
\begin{aligned}
    \Delta_2 h \eqdef \sup_{\vect{D}\sim \vect{\Tilde D}} \big\| h (\vect{D})  - h (\vect{\Tilde D})\big\|_2 \;.
\end{aligned}
\end{equation*}
When this quantity is finite, for any $\sigma > 0$, the Gaussian mechanism defined as 
\begin{equation*}
    \begin{aligned}
        \vect{D} \mapsto h(\vect{D}) + \sigma \mathcal{N}(0, I_{d'}) \;,
    \end{aligned}
\end{equation*}
is $(\epsilon, \delta(\epsilon))$-DP for any $\epsilon \geq 0$ where, by noting $\mu = \frac{\Delta_2 h}{\sigma}$,
\begin{equation*}
    \delta(\epsilon) = \Phi \Bigl( - \frac{\epsilon}{\mu} + \frac{\mu}{2}\Bigr) - e^{\epsilon} \Phi \Bigl( - \frac{\epsilon}{\mu} - \frac{\mu}{2}\Bigr) \;,
\end{equation*}
where $\Phi$ denotes the standard normal CDF.
\end{lemma}
A deterministic query $h$ is thus usually considered easy to privatize with the Gaussian mechanism when its sensitivity decreases with the sample size.
In particular, \Cref{sec:SensitivityPrivacy} proves that the Wasserstein gradients enjoy such property, motivating the methods presented in this article.

In addition, private mechanisms are stable under composition (which means that is possible to quantify the privacy of sequential private accesses to a dataset) \cite{dwork2014algorithmic,dong2019gaussian}, privacy is amplified by subsampling \cite{DBLP:journals/corr/abs-2210-00597}, and private mechanisms are stable under post-processing \cite{dwork2014algorithmic,dong2019gaussian}. 


\section{Wasserstein Gradients and Applications}
\label{sec:WassersteinGradients}

The key to obtain appropriate privacy guarantees in this work involves deriving a concise and tractable closed-form expression for the squared Wasserstein distance between one-dimensional empirical distributions.

%\subsection{The  weighted quadratic formula}
In the following, given sample of observations $x_i \in \mathbb{R}$ for $i\in[n]\eqdef \{1,\ldots,n\}$, %real values \(\{x_1,\dots,x_n\}\), 
we denote its order statistics by 
%\(x_{(1)},x_{(2)},\dots,x_{(n)}\), where  
$ x_{(1)} \leq x_{(2)} \leq \cdots \leq x_{(n)}.$
Given two discrete probabilities on the real line $P_\vect{U} = \frac{1}{n}\sum_{i=1}^n \delta_{U_i}$ and $P_\vect{V}=\frac{1}{m}\sum_{j=1}^m \delta_{V_j}$, using the characterization of $W_2^2(P_{\vect U},P_{\vect V})$ in terms of quantile functions, it follows that if we define the weights 
\begin{equation*}
    R_{i,j}= \lambda \Bigl(\Bigl(\frac{i-1}{n},\frac{i}{n}\Bigr]\cap\Bigl(\frac{j-1}{m},\frac{j}{m}\Bigr]\Bigr), \ i\in [n],j\in[m],
\end{equation*}
where $\lambda$ denotes the Lebesgue measure, and consider the rank permutations $\sigma,\tau$ such that $U_i = U_{(\sigma(i))}$ for each $i \in [n]$ and $V_j = V_{(\tau(j))}$ for each $j\in[m]$, then 

\begin{proposition}\label{lemma:expression_W2} With the above notation,
   \begin{align*} 
   W_2^2(P_{\vect U},P_{\vect V})  = \sum_{i=1}^{n} \sum_{j=1}^{m} R_{\sigma(i),\tau(j)}(U_{i}-V_{j} )^2     .
\end{align*}
\end{proposition}

Proposition~\ref{lemma:expression_W2} allows us to express the Wasserstein distance as a sum of squared differences multiplied by some weights $R_{\sigma(i),\tau(j)}$, which depend only on the rank permutations. Thus, if we are interested in the partial derivative with respect to $U_i$, it is well defined, provided that its rank $\sigma(i)$ remains unchanged in a neighborhood of $U_i$. 
\begin{proposition}
    \label{corollary:FormulaGradientsGeneral}
    With all the previous definitions, 
    $W_2^2(P_{\vect U},P_{\vect V})$ is differentiable as a function of $\vect U=(U_1,\ldots,U_n)$ in the set of points verifying $U_{(1)}<\ldots<U_{(n)}$, and its gradient is given by
    \begin{equation}\label{eq:gradient_U}
        \nabla_{U}W_2^2(P_{\vect U},P_{\vect V}) = \Bigl( 2\sum_{j=1}^m R_{\sigma(i),\tau(j)} (U_i-V_j)\Bigr)_{i\in[n]}
    \end{equation}
    Similarly, as a function of $\vect V=(V_1,\ldots,V_m)$, $W_2^2(P_{\vect U},P_{\vect V})$  is differentiable in the set of points verifying $V_{(1)}<\ldots<V_{(m)}$, and 
    \begin{equation}\label{eq:gradient_V}
        \nabla_{V}W_2^2(P_{\vect U},P_{\vect V}) = \Bigl( 2\sum_{i=1}^n R_{\sigma(i),\tau(j)} (V_j-U_i)\Bigr)_{j\in[m]}
    \end{equation}
\end{proposition}

This result offers a straightforward alternative to the empirical approximation of the Wasserstein gradient between absolutely continuous measures presented in \cite{risser2022tacklingAlgorithm}, and generalizes the gradient formula used in \cite{bonnel2015slicedRadon} and \cite{tanguy2023propertiesDiscreteSliced} to distributions with different sample sizes.  From a practical perspective, the lack of differentiability when some of the points coincide is not a significant concern. In such rare cases, the rank permutation is not unique. Selecting one of these permutations, equations \eqref{eq:gradient_U} and \eqref{eq:gradient_V} allows to compute (incorrect) gradients, take a step, and continue. It should be noted that this approach has been implicitly assumed in previous papers \cite{deshpande2018generativelModelling, Kolouri2018SWAE} relying on automatic differentiation with satisfactory empirical results.
With a slight abuse of notation, we will use the term \textit{gradient} in the following sections to denote the values in Equations \eqref{eq:gradient_U} and \eqref{eq:gradient_V}. Even outside the set of differentiability points, we will still be able to obtain privacy guarantees, as detailed in the next sections.

\section{A Private Surrogate for Wasserstein Gradients}\label{sec:SensitivityPrivacy}

Assume that we have samples   $\vect{X} = (x_1,\ldots,x_n)\in \mathcal X^n$, $\vect{Z}=(z_1,\dots,z_m)\in \mathcal Z^m$, and denote by $P_{\vect X}$ and $P_{\vect Z}$ the empirical distributions associated with $\vect X$ and $\vect Z$. This section explains how to apply previous result to the case where $U_i=g_\theta(x_i)$ and $V_j = h_\theta(z_j)$ are the outputs of machine learning models, to  privatize the quantity $\nabla_\theta W_2^2 \eqdef \nabla_\theta W_2^2 ( g_\theta \# P_{\vect X}, h_\theta \# P_{\vect Z})$. For clarity of presentation and due to its importance in various applications, we present the  analysis for the one-dimensional Wasserstein distance, assuming $ g_\theta(x), h_\theta(z) \in \mathbb{R}$. The extension to the sliced case is simple, as explained in \cref{remark:extension_sliced}. 

The gradient $\nabla_\theta W_2^2$ is commonly used for a large variety of applications in machine learning, including representation and fairness, when trying to optimize the parameters $\theta$ of two functions $g_{\theta}$ and $h_{\theta}$ to minimize the distance between their corresponding empirical distributions $\frac{1}{n} \sum_{i=1}^n \delta_{g_{\theta}(x_i)}$ and $\frac{1}{m} \sum_{j=1}^m \delta_{h_{\theta}(z_j)}$, using (stochastic) gradient descent optimization algorithms. \Cref{corollary:FormulaGradientsGeneral} and the chain rule under suitable assumptions give the following feasible expression, which will be used throughout all the paper
\begin{equation*}
\begin{aligned}
        \nabla_{\theta}W_2^2  
        &= 2 \sum_{i=1}^n \sum_{j=1}^m R_{\sigma(i),\tau(j)} (g_{\theta}(x_i)-h_{\theta}(z_j)) \nabla_{\theta} g_{\theta}(x_i) \\
        &+ 2 \sum_{i=1}^n \sum_{j=1}^m R_{\sigma(i),\tau(j)} (h_{\theta}(z_j)-g_{\theta}(x_i)) \nabla_{\theta} h_{\theta}(Z_j)\;.
\end{aligned}
\end{equation*}
When estimating the previous gradient,
and depending on the application, we may want either $\vect{X}$ or the $\vect{Z}$ to be private (e.g. when trying to match sensitive data to a reference known distribution), or both to be private (e.g. when working with sensitive data coming from two different groups). The first case being symmetric in the $\vect X$ and the $\vect Z$, we will always assume that either only $\vect X$ is private, or both  $\vect X$ and $\vect Z$ are private. \Cref{definition:neighboring} provides suitable neighboring relations to establish privacy guarantees in both cases. In the following, we will bound the sensitivity of $\nabla_{\theta}W_2^2$,
both as a function of $\vect X$, with $\sim_1$ in $\mathcal D =\mathcal X^n$, and as a function of $(\vect X,\vect Z)$, with $\sim_2$ in $\mathcal D =\mathcal X^n \times \mathcal Z^m$. Our main result can be stated as follows.

\begin{theorem}\label{theorem:gradient_sensitivity}
    With all the previous notation, assume that there exists constants $M,L_1,L_2\geq 0$ such that for each $\theta\in\Theta$, $x\in\mathcal X$  and $z\in\mathcal Z$,
    \begin{enumerate}
        \item \label{assumption1}$|g_\theta(x)|\leq M$, $|h_\theta(z)|\leq M$ .
        \item \label{assumption2}$\|\nabla_\theta g_\theta(x)\|_2\leq L_1$, $\|\nabla_\theta h_\theta(z)\|_2\leq L_2$. \quad {\rm Then}
    \end{enumerate}  
    \begin{itemize}
        \item[(a)]  Under the neighboring relation $\sim_1$ in $\mathcal D = \mathcal X^n$, if we define $\Phi_\theta(\vect X)= \nabla_{\theta} W_2^2(g_\theta\# P_{\vect X}, h_\theta\# P_{\vect Z})$, then
$$ \Delta \Phi_\theta   \leq  4M\frac{3L_1+L_2}{n} \ .$$
\item[(b)] Under the neighboring relation $\sim_2$ in $\mathcal D = \mathcal X^n\times \mathcal Z^m$, if we define $\Psi_\theta(\vect X, \vect Z)= \nabla_{\theta} W_2^2(g_\theta\# P_{\vect X}, h_\theta\# P_{\vect Z})$, then
$$ \Delta \Psi_\theta   \leq  4M\max\Bigl\{ \frac{3L_1+L_2}{n}  , \frac{L_1+3L_2}{m}  \Bigr\}\ .$$
    \end{itemize}
   

\end{theorem}

Assumption \ref{assumption1} is a uniform boundedness condition. Assumption \ref{assumption2} is verified as soon as  $g_\theta$ and $h_\theta$ are Lipschitz with respect to the parameter $\theta$. The main advantage of this bound is that it adapts to many different situations. This theorem (and its sliced version, Remark \ref{remark:extension_sliced}) covers 

    $\bullet$  \textit{Data generation:} $g_\theta(x)=x$, $\vect Z$ samples from a reference distribution. \cite{deshpande2018generativelModelling}\\
      $\bullet$ \textit{Representation learning:} 
    $h_\theta(z)=z$, $\vect Z$ samples from a reference distribution. \cite{Kolouri2018SWAE} \\
  $\bullet$    \textit{Domain adaptation:} $\vect Z$ available public data and $h_\theta(z)=z$ or $h_\theta=g_\theta$. \cite{Chen2019slicedWasserteinDiscrepancy}
  
In all previous applications, privacy guarantees are only required with respect to $\vect X$. In the data generation problem, $g_\theta(x)=x$ does not depend on $\theta$, and therefore $L_1=0$. Similarly, in the representation learning problem presented, $L_2=0$, and we can use (a) to obtain suitable privacy guarantees. 
\begin{remark}
\label{remark:extension_sliced}
\cref{theorem:gradient_sensitivity_sliced} in Appendix \ref{append:slice} extends Theorem~\ref{theorem:gradient_sensitivity} to the multidimensional setting $g_\theta(x),h_\theta(z)\in \mathbb R^d$, using the sliced Wasserstein distance $SW_{2}^2$ or its Monte-Carlo approximation. In any case, \cref{theorem:gradient_sensitivity_sliced} follows directly from \cref{theorem:gradient_sensitivity} and the fact that the sliced gradient is an average (in the form of an integral of a sum) of the gradients $\nabla_{\theta} W_2^2( (\operatorname{Pr}_\vartheta \# g_\theta)\# P_{\vect X}, (\operatorname{Pr}_\vartheta \# h_\theta)\# P_{\vect Z})$.  The conclusions of \cref{theorem:gradient_sensitivity_sliced} remain identical to  \cref{theorem:gradient_sensitivity}, but now require uniform control over $\vartheta$ of the bounds in Assumptions \eqref{assumption1} and \eqref{assumption2} for the projected functions $\operatorname{Pr}_\vartheta \# g_\theta, \operatorname{Pr}_\vartheta \# h_\theta$. To achieve this, Assumption~\ref{assumption1} is replaced by $\|g_\theta(x)\|_2\leq M$, $\|h_\theta(x)\|_2\leq M$ and Assumption \ref{assumption2} by $\|\mathcal J_\theta g_\theta(x)\|_2\leq L_1$, $\|\mathcal J_\theta h_\theta(z)\|_2\leq L_2$, where $\|\cdot \|_2$ denotes here the spectral norm of a matrix. Note that, for $d=1$,  $\mathcal J_\theta g_\theta = \nabla_\theta g_\theta$ and the spectral norm coincides with the 2-norm.
\end{remark}


\begin{remark}\label{remark:comparison}
Our work significantly broadens the applicability of the method in \cite{rakotomamonjy2021DPslicedWasserstein}. Assuming $h_\theta = I_d$, by the chain rule, and with a slight abuse of notation,
%(denoting $\vect Z=P_{\vect Z}$ and $g_\theta(\vect X)$ the empirical distribution $g_\theta \# P_{\vect X}$ or the vector ($g_\theta(x_1),\ldots,g_\theta(x_n))$, as convenient)
\begin{equation*} \nabla_\theta SW_2^2(g_\theta(\vect X), \vect{Z} ) = {\nabla_{g_\theta(\vect X)} SW_2^2(g_\theta(\vect X), \vect{Z} )}\mathcal J_\theta g_\theta(\vect X)\end{equation*} The approach in \cite{rakotomamonjy2021DPslicedWasserstein} provides privacy guarantees only for the first term in the decomposition. Therefore, privacy guarantees can only be derived in cases where the trained function $g_\theta$ is not directly applied to private data, allowing the second term to be ignored. In other words, privacy guarantees can only be given with respect to $\vect Z$, see \cref{additional_exp}. As a result, their training procedure is valid only for the simple data generation model outlined above.
\end{remark}

\section{A Framework for Deep Learning}
\label{sec:DeepLearningFramework}

This section explains how to adapt the methods presented above into a deep-learning framework where it is typically not possible to guarantee a priori that the gradients and the activations are bounded, and where one typically needs to run many iterations in a batched setting.

\subsection{Inner-Clipping of the Gradients}

Directly applying \Cref{theorem:gradient_sensitivity} to general deep learning models is typically infeasible, as the required boundedness conditions are not satisfied. As a solution, we propose to introduce three hyperparameters $M \geq 0$, $L_1 \geq 0$, and $L_2 \geq 0$, and to use as a proxy for ${\nabla}_{\theta}W_2^2$ the following quantity (named $\nabla^{M,L_1,L_2}_{\theta}W_2^2$) :
\begin{equation}\label{eq:gradient_approx}
\begin{aligned}
        & 2 \sum_{i=1}^n \sum_{j=1}^m R_{\sigma(i),\tau(j)} (U_i - V_j) \proj_{L_1} \p{ \nabla_{\theta}  g_{\theta}(x_i)} \\
        &+ 2 \sum_{i=1}^n \sum_{j=1}^m R_{\sigma(i),\tau(j)} (V_j - U_i) \proj_{L_2} \p{\nabla_{\theta}  h_{\theta}(z_j)}
\end{aligned}
\end{equation}
where for all i and j, we have $U_i = \proj_M \p{g_{\theta}(x_i)}$  and $V_j = \proj_M \p{h_{\theta}(z_j)}.$ This technique is known as \emph{``clipping"} and was historically introduced as a preprocessing of the gradients for problems with a finite-sum structure \cite{abadi2016deep}. For Wasserstein gradients, note that we also need to clip the \emph{activations}. Now, \Cref{theorem:gradient_sensitivity}  applies and one may add noise to this quantity to make it private with the Gaussian mechanism.

\subsection{Amplification by Subsampling}
In deep-learning, sub-sampling is often a necessity because of the size of the datasets. With differential privacy, it allows to leverage a property called \emph{privacy amplification by subsampling}. Since such property varies depending on the neighboring relation, we formalize it in the following lemma with the conventions of this article.

\begin{lemma}[Privacy amplification by subsampling] \label{lemma:subsampling}
    Let $n_1' \leq n_1, \dots, n_k' \leq n_k$. If a mechanism $M_{\text{batch}}$ is $(\epsilon, \delta)$-DP on $\mathcal D_1^{n_1'} \times \ldots \times \mathcal D_k^{n_k'}$, the mechanism $M$ that 
    (i) selects $n_{i}'$ among the $n_i$ points in each category without replacement, and then
    (ii) applies $M_{\text{batch}}$ to the sampled dataset, is $(\epsilon', \delta')$-DP on $\mathcal D_1^{n_1} \times \ldots \times \mathcal D_k^{n_k}$ where $\epsilon' = \ln \p{1 + p\p{e^{\epsilon} - 1}} , \quad \delta' = p \delta$ 
    and $p = \max \p{\frac{n_{1}'}{n_1}, \dots,  \frac{n_k'}{n_k}}$.
\end{lemma}

\subsection{Privacy Accountanting}
In the influential article \cite{abadi2016deep}, the authors introduce the \emph{moment accountant} method, a framework for quantifying the privacy of a composition of subsampled Gaussian mechanisms. We now detail why similar methods \cite{dong2019gaussian} are applicable to Wasserstein gradients.

\begin{algorithm}[h]
\label{alg:sequentialgradients}
   \caption{Sequential Computation of Subsampled Wasserstein Noisy Gradients}
\begin{algorithmic}
   \FOR{$t=1$ {\bfseries to} $T$}
   \STATE - Selects $n_{i}'$ among the $n_i$ points in each category without replacement.
   \STATE - Compute$\hat \nabla_{\theta}W_2^2 \eqdef {\nabla}^{M,L_1,L_2}_{\theta}W_2^2 + \sigma \mathcal{N}(0, I_d)$  on the subsampled dataset 
    \STATE - Publish $\hat{\nabla}_{\theta}W_2^2$.
    \STATE - Wait for the optimizer to update $\theta$.
    \ENDFOR
\end{algorithmic}
\end{algorithm}

Since our neighboring relation is based on the replacement relation and since we use subsampling with fixed batch size and without replacement, the classical moment accountant method \cite{abadi2016deep} does not apply. We thus turn to the accounting techniques of \cite{dong2019gaussian} that build on the theory of $f$-differential privacy and that are more suited to this scenario.
Using the notations of \cite{dong2019gaussian}, and denoting by $\Delta$ the sensitivity of $\nabla^{M,L_1,L_2}_{\theta}W_2^2$ (which is controlled by \Cref{theorem:gradient_sensitivity}), $\hat{\nabla}_{\theta}W_2^2$ is $\frac{\Delta}{\sigma}$-GDP (for \emph{Gaussian Differential Privacy}) ignoring the subsampling step.
In order to account for subsampling, one would like to apply Theorem 4.2 in \cite{dong2019gaussian}. This is not possible since this article uses a different neighboring relation and a different form of subsampling. However, we can notice that we can substitute Lemma 4.4 in the proof of Theorem 4.2 in \cite{dong2019gaussian} by our \Cref{lemma:subsampling} and the rest of the proof follows. We thus get that the overall procedure described by Algorithm 1 is $C_p(G_{(\sigma/\Delta)^{-1}})^{\otimes T}$-DP where $p = \max (n_{1}'/{n_1}, \dots, {n_k'}/{n_k})$ with the formalism of $f$-differential privacy \cite{dong2019gaussian}.

Finally, this writing now fits the framework of privacy accounting in the limit regime of Section 5.2 of \cite{dong2019gaussian}. We use this accountant in the experiments using the implementation of \cite{opacus}.

\begin{remark}
As in the one-dimensional case, clipped approximations of $\nabla_\theta SW_2^2$ (or its Monte-Carlo approximation) satisfying the assumptions of \cref{remark:extension_sliced} can be defined. In this case, we need to clip the spectral norm of the Jacobian matrix $\mathcal J_\theta g_\theta(x_i)$, which requires a matrix decomposition. For simplicity, we adopted in our experiments a suboptimal naive approach, based on clipping each component by $L/\sqrt d$, which ensures $L$-bounded spectral norm, as detailed in \cref{remark:degraded_bound}. Naturally, Algorithm 1 also applies in the sliced setting.
\end{remark}


\section{Bias Mitigation with privacy guarantee}\label{section:fairness}

Previous result enables to obtain a suitable framework to perform private  bias mitigation by   a fairness penalization.  Assume that we have a dataset $ \vect D$ with $n$ samples $(x_i,a_i, y_i)$ or $(x_i,a_i)$, where $x_i$ are the non-sensitive attributes, $a_i\in \{0,1\}$ is the sensitive attribute and $y_i$ the response variable, only available in supervised problem. Typical machine learning algorithms are trained minimizing the empirical risk for a given loss function $\ell$,
\begin{equation}\label{loss:ERM}
    \min_{\theta} \mathscr L(g_\theta) \eqdef \min_{\theta} \frac{1}{n} \sum_{i=1}^n \ell(g_\theta(x_i)) 
\end{equation}
where $\ell(g_\theta(x_i))$ is a shorthand for $\ell(g_\theta(x_i), y_i)$ in supervised problems, and for $\ell(g_\theta(x_i), x_i)$ in unsupervised problems. This particular finite-sum structure of the loss function is translated to the gradient, and as long as $\|\nabla_\theta \ell(g_\theta(x))\|_2 \leq C$ for all $x\in \mathcal X$, the sensitivity of $\nabla_\theta \mathscr L(g_\theta)$ is bounded by $2C/n$ for the substitution relation $\sim_1$, as well as for any k-end neighboring relation $\sim_k$. This bound allow us to benefit from large sample sizes to obtain private gradients with minimal amount of noise. With this great generality, we will discuss how different fairness notions can be favored during training by adding different penalization terms on the loss function $\mathscr L(g_\theta)$, while preserving  sensitivity bounds allowing for strong privacy guarantees. We present this section for the general case of the sliced Wasserstein distance, note that the case $d=1$ agrees with the one-dimensional Wasserstein distance.

$\bullet$ \textit{Statistical Parity (SP):} Statistical parity corresponds to the situation where  the algorithmic decision does not depend on the sensitive variable.  Statistical parity is thus satisfied if  $ \mathcal L( g_\theta(X) | A= 0 ) = \mathcal L ( g_\theta(X) | A=1 )
    $. Given our data, if we define $\vect X_j = (x_i: a_i=j)$, $n_j = \textnormal{length}(\vect X_j)$ for $j=0,1$, statistical parity can be favored by minimizing   
    \begin{equation}\label{eq:loss_SP}
    \begin{gathered}
    \mathscr L^{SP}_\alpha(g_\theta) = (1-\alpha)\mathscr L(g_\theta)  + \alpha {SW}_2^2\Bigl(  g_\theta \# P_{\vect X_0}, g_\theta \# P_{\vect X_1} \Bigr)  
    \end{gathered}
    \end{equation}
    where $\alpha \in [0,1]$ measures the weight of each part in the optimization. In order to establish privacy guarantees, we need to assume that $n_0$ and $n_1$ are fixed.
    
$\bullet$ \textit{Equality of Odds (EO):} Beyond guaranteeing the same decision for all, which is not suitable in some cases where the sensitive variable impacts the decision, bias mitigation may require that the model performs with the same accuracy for all groups, often referred to as equality of odds. We focus only in the supervised case, where $y_i$ is available and takes values in $\{0,\ldots,R-1\}$. In this case, equality of odds is verified if 
$ \mathcal L( g_\theta(X) | A= 0, Y = k ) = \mathcal L ( g_\theta(X) | A=1 , Y = k)$ for all $k \in \{0,\ldots,R-1\}$. With the same ideas as before, if we define $\vect X_{j,k} = (x_i: a_i=j,y_i=k)$, $n_{j,k} = \textnormal{length}(\vect X_{j,k})$ for $j \in \{0,1\}, k\in\{0,\ldots,R-1\}$,
equality of odds bias mitigation can be enforced by training with loss $\mathscr L^{EO}_\alpha(g_\theta)$
\begin{equation}\label{eq:loss_EOO}
    = (1-\alpha)\mathscr L(g_\theta) + \frac{\alpha}{R} \sum_{k=1}^K {SW}_2^2\Bigl(  g_\theta \# P_{\vect X_{0,k}}, g_\theta \# P_{\vect X_{1,k}} \Bigr) 
\end{equation}
To obtain privacy guarantees, now we need to impose that the values $n_{j,k}$ are fixed. 
    \begin{theorem} \label{th:fairandprivate}
        In both cases,  under the assumptions that $g_\theta$ verifies {$\|g_\theta(x)\|_2 \leq M$, $\|\mathcal J_\theta g_\theta(x)\|_2 \leq L$} and $\|\nabla_\theta \ell(g_\theta(x))\|_2 \leq C$, we obtain that

    $\bullet$ For SP, under $\sim_2$, the sensitivity of  $\nabla_\theta \mathscr L^{SP}_\alpha(g_\theta)$ {or its MC approximation} is bounded by     
    \begin{equation}\label{eq:sensitivity_SP}
     (1-\alpha) \frac{2C}{n} + \alpha \frac{16 M L }{\min \{n_0,n_1\}} \ .
\end{equation}        
    $\bullet $ For EO, under the relation $\sim_{2R}$, the sensitivity of $\nabla_\theta \mathscr L^{EO}_\alpha(g_\theta)$  {or its MC approximation} is bounded by     
    \begin{equation}\label{eq:sensitivity_EO}
     (1-\alpha) \frac{2C}{n} + \frac{\alpha}{R} \frac{16 M L }{\min_{j,k} \{n_{j,k}\}} \ .
    \end{equation} 
    \end{theorem}
    
\begin{remark}
Our privacy guarantees in the fairness framework are built upon the knowledge of class sizes. The importance of controlling these sizes has been previously recognized. For example, \cite{lowi2022SDPfairLearning} imposes a restriction on the minimum proportion of elements in each class, while \cite{Ghoukasian2024DPFairBinaryClassif} and \cite{xian2024DPfairRegression} derive privacy guarantees that degrade with smaller class sizes. Conceptually, our framework for establishing privacy guarantees is very sound. Even though an attacker might learn  some information about the number of individuals in each class used during training, they cannot distinguish between the outputs of two datasets $\vect D$ and $\vect{\Tilde D}$ differing only in one individual from the same class.
\end{remark}   
    
\section{Numerical Illustrations}
To highlight the efficiency and versatility of our method, we simulate biased data as explained in Appendix \ref{appendix:experiments}, and explore the properties of our bias in-processing mitigation in three different scenarios, starting with the simpler but illustrative well-known problem of fair and private classification, then presenting two completely novel problems, namely, multidimensional fair and private regression, and fair and private representation learning. In all the experiments, the model optimizes \eqref{eq:loss_SP} or \eqref{eq:loss_EOO} (recall that  $SW_2^2 = W_2^2$ if $d=1$), following the DP-SGD methodology explained in \cref{sec:DeepLearningFramework}, with clipping constant $C>0$ for the individual gradients in \eqref{loss:ERM}, and inner clipping constants $M,L>0$ for the Wasserstein gradient approximation \eqref{eq:gradient_approx}. \cref{th:fairandprivate} enable us to compute the privacy budget obtained after $T$ iterations of DP-SGD. In particular, in all the experiments, we fix the number of iterations $T$ and the value of $\delta$, and compute the required noise to obtain $(\epsilon,\delta)$-DP after $T$ iterations, for different values of the privacy budget $\epsilon$ and the weight $\alpha\in[0,1]$. Batch sizes are $n'_j \approx n_j/5$ minimizing \eqref{eq:loss_SP}, and $n'_{j,k} \approx n_{j,k}/5$ minimizing \eqref{eq:loss_EOO}.  Additional details about each experiment are presented in \cref{appendix:experiments}. 

\textbf{Classification.} A decision rule is a function $g_\theta$ mapping each $x$ to the predicted probability $g_\theta(x)\in (0,1)$. The classification rule is given by $G_\theta(x) = I(g_\theta(x)>1/2)$.  
Many authors propose to mitigate not only the mean but the whole distribution of the predicted probabilities $g_\theta(x) \in (0,1)$ as in \cite{risser2022tacklingAlgorithm}, \cite{gouic2020projection} or \cite{chzhen2020fair}. In our example, $g_\theta$ is a neural network with one layer and a sigmoid activation function, and we define $\ell$ as the the binary cross-entropy loss function. Results are shown in Figures \ref{fig:di_small} and \ref{fig:eoo_small}. 
Above each graph, we can see the noise required to achieve the privacy budget in the fixed number of iterations, the weighted training loss value and the value of each term, and the accuracy on test data, together with specific fairness measures for each case, detailed in \cref{appendix:experiments}. Two main conclusions can be drawn. First, the Wasserstein penalization mitigates biases as $\alpha$ increases. Second, adding privacy does not significantly alter the results of the optimization. This can be seen from the loss curve, presented in Figure \ref{fig:losses_main}.
\begin{figure}[h]
    \centering
\includegraphics[width=1\linewidth]{di_small.pdf}
    \caption{Histogram of $g_\theta(x)$ for the classification model minimizing \eqref{eq:loss_SP}, weight $\alpha \in \{0,0.75\}$ and privacy budget  $\epsilon\in \{\infty,1\}$.} 
    \label{fig:di_small}
\end{figure}


\begin{figure}[h]
    \centering
\includegraphics[width=1\linewidth]{eoo_small.pdf}
    \caption{Histogram of $g_\theta(x)$ for the classification model minimizing \eqref{eq:loss_EOO}, weight $\alpha \in \{0,0.75\}$ and privacy budget  $\epsilon\in \{\infty,1 \}$.} 
    \label{fig:eoo_small}
\end{figure}


\textbf{Fair representation learning.} The aim is to privately learn fair encoder-decoder maps. To achieve this, we train an autoencoder privately, with $\theta= (\theta_a,\theta_b)$, encoder $\varphi_\theta =\varphi_{\theta_a}$, bi-dimensional latent space, and decoder $\psi_\theta = \psi_{\theta_b}$, minimizing a version of \eqref{eq:loss_SP}, where statistical parity penalization is imposed on the latent space, and $l(\psi_\theta(\varphi_\theta(x)))=\|\psi_\theta(\varphi_\theta(x))-x\|^2$. Figure \ref{fig:autoencoder_small} shows the results obtained, increasing values of $\alpha$ reduce the discrepancy between the conditional distributions in the latent space. In addition,  Figures \ref{fig:autoencoder_small} and \ref{fig:losses_main} show that privacy does not have a significative effect on the optimization.  

\textbf{Regression.} In our generating mechanism, the label $Y\in \{0,1\}$ is defined as a  set indicator function of a continuous response $Y^C \in [0,1]\times[0,1]$. We train a two-layer neural network privately with statistical parity penalization and $l(g_\theta(x),y)=\|g_\theta(x)-y\|^2$. See \cref{appendix:experiments}. \\
\begin{figure}[h!]
    \centering
\includegraphics[width=1\linewidth]{autoencoder_small.pdf}
    \caption{Encoded values  $\varphi_\theta(x)$ for the autoencoder model minimizing \eqref{eq:loss_SP}, weight $\alpha \in \{0,0.75\}$, privacy budget  $\epsilon\in \{\infty,1\}$.}  
    \label{fig:autoencoder_small}
\end{figure}
\begin{figure}[h!]
    \centering
    \includegraphics[width=1\linewidth]{losses_main.pdf}
    \caption{From left to right, training losses of Figures \ref{fig:di_small}, \ref{fig:eoo_small} and \ref{fig:autoencoder_small} for $\alpha=0.75$ and different values of $\epsilon$.}
    \label{fig:losses_main}
\end{figure}

\section{Conclusion}
In this work, we have provided a novel and practical method to ensure Differential Privacy for (sliced) Wasserstein gradients. By embedding DP guarantees into these gradients, we can preserve their statistical utility while ensuring that the training process does not inadvertently expose sensitive data. We tackle not only the constraint of statistical parity but also the Equality of Odds constraint to guarantee a fair accuracy for all. This synergy is crucial in fairness-sensitive high risk domains as denoted in the AI European Act (e.g., healthcare, criminal justice, access to public resources), where models must balance the dual imperatives of privacy preservation and equitable performance across subgroups. Our methodology is versatile and can is also useful to many other applications in Machine Learning such as representation learning or data generation, i.e., in all tasks where a Sliced Wasserstein metric is involved. 

This work opens many research directions.  Our results do not generalize to other Wasserstein losses, such as $W_p$. We prove in Appendix~\ref{counterexample}  that, in general, it is not possible to bound the sensitivity of the gradient of $W_p$, for general $p \geq 1$, by a factor that decreases approximately as $1/n$. Yet we believe that the generalization of our method to $W_p^p$ can be tackled and is worth of interest. A natural extension to the privacy of the computation of sliced Wasserstein barycenters is also left for future research.

\section*{Acknowledgements}

This paper has been partially funded by the Agence Nationale de la Recherche under grant ANR-23-CE23-0029 Regul-IA. The research leading to these results received funding from MCIN/AEI/10.13039/501100011033/FEDER under Grant Agreement Number PID2021-128314NB-I00. The authors also acknowledge the support of the AI Cluster ANITI (ANR-19-PI3A-0004). 

%\newpage

\section*{ Impact Statement}

This paper presents work whose goal is to advance the field
 of Privacy-Preserving and Fair Machine Learning. This work is theoretical. There are many potential societal consequences of our work, none which we feel must be
 specifically highlighted here.


\bibliography{biblio}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn

\section{Proofs}

\subsection{Proofs of \Cref{sec:WassersteinGradients}}

\begin{proof}[Proof of Proposition \ref{lemma:expression_W2}]

If we denote by $F$ and $G$ the distribution functions of $P_{\vect U}$ and $P_{\vect V}$, we know that 
\begin{align*}  W_2^2(P_{\vect U},P_{\vect V}) & = \int_0^1 (F^{-1}(t)-G^{-1}(t))^2dt \\
& = \int_0^1 \Bigl(\sum_{i=1}^{n} U_{(i)} I\Bigl(\frac{i-1}{n}<t\leq \frac{i}{n}\Bigr) -\sum_{j=1}^{m} V_{(j)} I\Bigl(\frac{j-1}{m}<t\leq \frac{j}{n}\Bigr) \Bigr)^2dt \\
& = \sum_{i=1}^{n} \sum_{j=1}^{m} (U_{(i)}-V_{(j)} )^2 \int_0^1  I\Bigl(\frac{i-1}{n}<t\leq \frac{i}{n},\frac{j-1}{m}<t\leq \frac{j}{n}\Bigr) dt \\
& = \sum_{i=1}^{n} \sum_{j=1}^{m} (U_{(i)}-V_{(j)} )^2   R_{i,j}  \\
&= \sum_{i=1}^{n} \sum_{j=1}^{m} (U_{(\sigma(i))}-V_{(\sigma(j))} )^2   R_{\sigma(i),\sigma(j)}\\
&= \sum_{i=1}^{n} \sum_{j=1}^{m} (U_{i}-V_{j} )^2   R_{\sigma(i),\sigma(j)} \ ,
\end{align*}
where the third equality follows from the fact that exactly one element in each sum is different from 0, and the fifth equality follows from reindexing the sum.\end{proof}
\subsection{Proofs of \Cref{sec:SensitivityPrivacy}}

\begin{proof}[Proof of Theorem \ref{theorem:gradient_sensitivity}]
First of all, note that (b) follows 
immediately from (a) and the definition of the neighboring relation $\sim_2$ in $\mathcal X^n\times \mathcal Z^m$. Consider two neighboring datasets $\vect X\sim \vect{\Tilde X}$ under the substitution  relation. We can assume without loss of generality that the datasets differ on the first observation $\Tilde x_1\neq x_1$. For ease of notation, denote $\vect{\Tilde{X}} = \{ \Tilde x_i\}_{i=1}^n$, even though $\Tilde x_i=x_i$ for $i\neq 1$. Along this proof, we will define $U_i:=g_\theta(x_i)$ and $\Tilde U_i := g_\theta(\Tilde x_i)$ for each  $i\in[n]$, and  $V_j:=h_\theta(z_j)$ for $j\in [m]$. Again, $U_i=\Tilde U_i$ for every $i\neq 1$. Define now the rank permutations $\sigma,\Tilde \sigma$ and $\tau$ such that
\begin{eqnarray*}
    U_i &= U_{(\sigma(i))}\ ,  \quad \ &i \in [n] \ ,\\
     \Tilde U_i &= \Tilde U_{(\Tilde \sigma(i))}\ ,  \quad \ &i \in [n] \ ,\\
      V_j &= V_{(\tau(j))}\ ,  \quad \ &j \in [m]\ .
\end{eqnarray*}
Denote $\vect U=(U_1,\ldots,U_n)$ and $\vect V=(V_1,\ldots,V_m)$. Corollary \ref{corollary:FormulaGradientsGeneral} ensures if we define  $P_{\vect{U}}= g_\theta \# P_{\vect{X}} =\frac{1}{n}\sum_{i=1}^n\delta_{U_i}$ and $P_{\vect V}= h_\theta \# P_{\vect Z}=\frac{1}{m}\sum_{i=1}^m \delta_{V_j}$ , then
\begin{align*}
    \nabla_{U,V} W_2^2(P_{\vect U}, P_{\vect V}) = \Biggl( \Bigl( 2\sum_{j=1}^m R_{\sigma(i),\tau(j)} (U_i-V_j)\Bigr)_{i\in[n]}, \Bigl( 2\sum_{i=1}^n R_{\sigma(i),\tau(j)} (V_j-U_i)\Bigr)_{j\in[m]}\Biggr) \ \in \mathbb R^{n+m} \ .
\end{align*}
Applying the chain rule, we obtain that 
\begin{align*}
    \nabla_\theta W_2^2(g_\theta\# P_{\vect{X}}, h_\theta \# P_{\vect{Z}})& = 2\sum_{i=1}^{n} \sum_{j=1}^{m} R_{\sigma(i),\tau(j)} (U_{i}-V_{j} ) \nabla_\theta g_\theta(x_i) + 2\sum_{j=1}^{m}\sum_{i=1}^{n}  R_{\sigma(i),\tau(j)} (V_{j}-U_{i} ) \nabla_\theta h_\theta(z_j)
\end{align*}
Similarly, for the dataset $\vect{\Tilde X}$ we get
\begin{align*}    
    \nabla_\theta W_2^2(g_\theta\# P_{\vect{\Tilde X}}, h_\theta \# P_{\vect{Z}})& = 2\sum_{i=1}^{n} \sum_{j=1}^{m} R_{\Tilde \sigma(i),\tau(j)} (\Tilde U_i-V_{j} ) \nabla_\theta g_\theta(\Tilde x_i) + 2\sum_{j=1}^{m}\sum_{i=1}^{n}  R_{\Tilde \sigma(i),\tau(j)} (V_{j}-\Tilde U_i ) \nabla_\theta h_\theta(z_j)
\end{align*}

Therefore, 
\begin{align}
    \|\nabla_\theta W_2^2&(g_\theta\# P_{\vect{X}}, h_\theta \# P_{\vect{Z}})- \nabla_\theta W_2^2(g_\theta\# P_{\vect{\Tilde X}}, h_\theta \# P_{\vect{Z}}) \|_2 \leq \notag \\
    &\leq  2\ \Big\| \sum_{i=1}^{n} \sum_{j=1}^{m} R_{\sigma(i),\tau(j)} (U_{i}-V_{j} ) \nabla_\theta g_\theta(x_i) - \sum_{i=1}^{n} \sum_{j=1}^{m} R_{\Tilde \sigma(i),\tau(j)} (\Tilde U_i-V_{j} ) \nabla_\theta g_\theta(\Tilde x_i) \Big\|_2  \label{eq:term1}\\
    & +2\  \Big \| \sum_{j=1}^{m}\sum_{i=1}^{n}  R_{\sigma(i),\tau(j)} (V_{j}-U_{i} ) \nabla_\theta h_\theta(z_j) - \sum_{j=1}^{m}\sum_{i=1}^{n}  R_{\Tilde \sigma(i),\tau(j)} (V_{j}-\Tilde U_i ) \nabla_\theta h_\theta(z_j) \Big\|_2\label{eq:term2}
\end{align}

The term \eqref{eq:term2} is easier to bound, since the values inside $\nabla_\theta h_\theta(\cdot)$ coincide. First, note that 
\begin{align}\label{eq:propertyR}
    \sum_{j=1}^{m} R_{i,j} = \frac{1}{n} \ ,  \ \forall \ i\in [n] \quad \textnormal{and} \quad 
     \sum_{i=1}^{n} R_{i,j} = \frac{1}{m} \ ,\ \forall\  j\in [m] \ ,
\end{align}
The triangular inequality, the assumption $\|\nabla_\theta h_\theta(z)\|\leq L_2$ for every $z,\theta$ and the previous property allow us to derive the following inequalities
\begin{align}
    (\ref{eq:term2})  &=2\  \Big \| \sum_{j=1}^{m}V_{j} \nabla_\theta h_\theta(z_j)\Bigl( \sum_{i=1}^{n}  R_{\sigma(i),\tau(j)} -\sum_{i=1}^{n}  R_{\Tilde \sigma(i),\tau(j)}\Bigr) - \sum_{j=1}^{m} \nabla_\theta h_\theta(z_j)\Bigl( \sum_{i=1}^{n}  U_iR_{\sigma(i),\tau(j)} -\sum_{i=1}^{n}  \Tilde U_i R_{\Tilde \sigma(i),\tau(j)}\Bigr)\Big\|_2 \notag\\
   &\leq 2 \sum_{j=1}^{m} \Bigl\|\nabla_\theta h_\theta(z_j)\Bigl( \sum_{i=1}^{n}  U_iR_{\sigma(i),\tau(j)} -\sum_{i=1}^{n}  \Tilde U_i R_{\Tilde \sigma(i),\tau(j)}\Bigr)\Big\|_2 \notag \\
   &\leq 2L_2\sum_{j=1}^{m} \Bigl| \sum_{i=1}^{n}  U_iR_{\sigma(i),\tau(j)} -\sum_{i=1}^{n}  \Tilde U_i R_{\Tilde \sigma(i),\tau(j)}\Big|\notag \\
   &=  2 L_2\sum_{j=1}^{m} \Bigl| \sum_{i=1}^{n}  U_{(i)}R_{i,\tau(j)} -\sum_{i=1}^{n}  \Tilde U_{(i)} R_{i,\tau(j)}\Big| \notag\\
   &= 2L_2 \sum_{j=1}^{m} \Bigl| \sum_{i=1}^{n}  R_{i,\tau(j)}(U_{(i)} - \Tilde U_{(i)})\Big|  \label{eq:last_line_ineq}
\end{align}
where the last lines follows from $U_i=U_{(\sigma(i))}$, $\Tilde U_i=\Tilde U_{(\Tilde \sigma(i))}$ and reindexing the sum. Since $U_i=\Tilde U_i$ for every $i\neq 1$, we know that
\begin{itemize}
    \item If $U_1\geq \Tilde U_1$, then $U_{(i)}\geq \Tilde U_{(i)}$ for every $i \in [n]$.
    \item If $U_1< \Tilde U_1$, then $U_{(i)}\leq \Tilde U_{(i)}$ for every $i\in[n]$.
\end{itemize}
This monotonicity property and the fact that $R_{i,j}\geq 0$ for every $i,j$ ensures that
\begin{align*}
    (\ref{eq:last_line_ineq})  &= 2L_2\ \Bigl| \sum_{j=1}^{m}  \sum_{i=1}^{n}  R_{i,\tau(j)}(U_{(i)} - \Tilde U_{(i)})\Big| \\
    &= 2L_2\ \Bigl|  \sum_{i=1}^{n} (U_{(i)} - \Tilde U_{(i)}) \sum_{j=1}^{m} 
 R_{i,\tau(j)}\Big| \\
 &=  \frac{2L_2}{n}\ \Bigl|  \sum_{i=1}^{n} (U_{(i)} - \Tilde U_{(i)}) \Big| \\
 &  = \frac{2L_2}{n} \Bigl|  \sum_{i=1}^{n} (U_{i} - \Tilde U_i) \Big| \\
 &= \frac{2L_2}{n} \bigr|U_{1} - \Tilde U_{1}\big| \\
 &\leq \frac{4L_2M}{n}
\end{align*}
By the triangular inequality, the term \eqref{eq:term1} can be bounded as follows
\begin{align}
   (\ref{eq:term1}) &\leq  2\  \Big \| \sum_{i=1}^{n} \nabla_\theta g_\theta(x_i)U_i\sum_{j=1}^{m} R_{\sigma(i),\tau(j)}   - \sum_{i=1}^{n} \nabla_\theta g_\theta(\Tilde x_i)\Tilde U_i\sum_{j=1}^{m} R_{\Tilde \sigma(i),\tau(j)} \Big\|_2 \label{eq:decomp1}\\
   & \quad +2\  \Big \| \nabla_\theta g_\theta(x_1)\sum_{j=1}^{m} R_{\sigma(1),\tau(j)} V_j  -  \nabla_\theta g_\theta(\Tilde x_1)\sum_{j=1}^{m} R_{\Tilde \sigma(1),\tau(j)}V_j \Big\|_2 \label{eq:decomp2} \\
   & \quad +  2\  \Big \| \sum_{i=2}^{n} \nabla_\theta g_\theta(x_i)\sum_{j=1}^{m} V_j (R_{\sigma(i),\tau(j)}   -  R_{\Tilde \sigma(i),\tau(j)} )\Big\|_2 \label{eq:decomp3}
\end{align}
We can bound independently each term in the decomposition, 
\begin{align}
    (\ref{eq:decomp1}) &= \frac{2}{n} \Big\| \sum_{i=1}^{n} \nabla_\theta g_\theta(x_i)U_i   -  \nabla_\theta g_\theta(\Tilde x_i)\Tilde U_i \Big\|_2 \notag \\
    &=\frac{2}{n}
 \Big\| \nabla_\theta g_\theta(x_1)U_1   - \nabla_\theta g_\theta(\Tilde x_1)\Tilde U_1 \Big\|_2 \notag \\
 &\leq \frac{2}{n}\Bigl(|U_1|\|\nabla_\theta g_\theta(x_1)\|_2 +|\Tilde U_1|\|\nabla_\theta g_\theta(\Tilde x_1)\|_2 \Bigr) \notag \\
 &\leq \frac{4L_1M}{n}\notag \\ 
 (\ref{eq:decomp2}) &\leq 2L_1M\Bigl(\sum_{j=1}^{m} R_{\sigma(1),\tau(j)}+\sum_{j=1}^{m} R_{\Tilde \sigma(1),\tau(j)}\Bigr)\notag \\
 &= \frac{4L_1M}{n}\notag \\
 (\ref{eq:decomp3})& \leq  2L_1\   \sum_{i=2}^{n} \Big |\sum_{j=1}^{m} V_j (R_{\sigma(i),\tau(j)}   -  R_{\Tilde \sigma(i),\tau(j)} )\Big| \notag\\
 &=2L_1\   \sum_{i=2}^{n} \Big |\sum_{j=1}^{m} V_{(j)} (R_{\sigma(i),j)}   -  R_{\Tilde \sigma(i),j} )\Big| \label{eq:last_inequality_2}
 \end{align}
 The last equality is a simple consequence of $V_j=V_{(\tau(j))}$ and reindexing the sum. To bound the last expression, it is useful to see that all the terms $\sum_{j=1}^{m} V_{(j)} (R_{\sigma(i),j)}   -  R_{\Tilde \sigma(i),j} )$ have the same sign, for $i=2,\ldots,n$. This will follow from the relationship between the permutations $\sigma$ and $\Tilde \sigma$. For instance, if $\Tilde \sigma(1)<\sigma(1)$, it follows that
 \begin{enumerate}
     \item[a)] \textit{$\Tilde \sigma(i)\geq \sigma(i)$ for every $i\geq 2$}.    
     Remember that $\sigma(i)$ denotes the position of $U_i$ in the ordered statistic $(U_{(1)},\ldots,U_{(n)})$, and $\Tilde \sigma(i)$ denotes the position of $\Tilde U_i$ in the ordered statistic $(\Tilde U_{(1)},\ldots,\Tilde U_{(n)}')$. Recall also that $\Tilde U_i=U_i$ for every $i\geq 2$. Therefore, $\Tilde \sigma(1)<\sigma(1)$ implies that $\Tilde U_1 < U_1$, and 
     \begin{itemize}
         \item If $\sigma(i)<\Tilde \sigma(1)$, then $ \Tilde \sigma(i)=\sigma(i)$.
         \item If $\sigma(i)=\Tilde \sigma(1)$, then $ \Tilde \sigma(i)=\sigma(i)$ if $U_i<\Tilde U_1$, and $ \Tilde \sigma(i)=\sigma(i)+1$ if $U_i>\Tilde U_1$.
          \item If $\Tilde \sigma(1)<\sigma(i)<\sigma(1)$, then $ \Tilde \sigma(i)=\sigma(i)+1$.
           \item If $\sigma(i)>\sigma(1)$, then $ \Tilde \sigma(i)=\sigma(i)$.
     \end{itemize}
     \item[b)] \textit{$\sum_{j=1}^{m} V_{(j)} (R_{\sigma(i),j)}   -  R_{\Tilde \sigma(i),j} ) \leq 0$ for every $i=2,\ldots,n$}. If we denote by $G$ the empirical distribution function of $V_1,\ldots,V_{m}$, then by definition of $R_{i,j}$,
     \begin{align*}
         \sum_{j=1}^{m} V_{(j)} &(R_{\sigma(i),j)}   -  R_{\Tilde \sigma(i),j} ) = \\
         &=\sum_{j=1}^{m} V_{(j)} \Biggl(\int_{\frac{\sigma(i)-1}{n}}^{\frac{\sigma(i)}{n}} I\Bigl( \frac{j-1}{m}<t\leq\frac{j}{m}\Bigr)dt-\int_{\frac{\Tilde \sigma(i)-1}{n}}^{\frac{\Tilde \sigma(i)}{n}} I\Bigl( \frac{j-1}{m}<t\leq\frac{j}{m}\Bigr)dt\Biggr)\\
         &= \int_{\frac{\sigma(i)-1}{n}}^{\frac{\sigma(i)}{n}}\sum_{j=1}^{m} V_{(j)}  I\Bigl( \frac{j-1}{m}<t\leq\frac{j}{m}\Bigr)dt-\int_{\frac{\Tilde \sigma(i)-1}{n}}^{\frac{\Tilde \sigma(i)}{n}}\sum_{j=1}^{m} V_{(j)}  I\Bigl( \frac{j-1}{m}<t\leq\frac{j}{m}\Bigr)dt\\
         &= \int_{\frac{\sigma(i)-1}{n}}^{\frac{\sigma(i)}{n}} G^{-1}(t)dt-\int_{\frac{\Tilde \sigma(i)-1}{n}}^{\frac{\Tilde \sigma(i)}{n}}G^{-1}(t)dt \\
         &= \int_{\frac{\sigma(i)-1}{n}}^{\frac{\sigma(i)}{n}} G^{-1}(t)-G^{-1}\Bigl(t+\frac{\Tilde \sigma(i)-\sigma(i)}{n}\Bigr)dt \leq 0
     \end{align*}
     for every $i=2,\ldots,n$, where the last bound is consequence of $(a)$ and the monotonicity of $G^{-1}$.
 \end{enumerate}
 Similarly, if $\Tilde \sigma(1)>\sigma(1)$, then $\Tilde \sigma(i)\leq\sigma(i)$ for every $i\geq 2$, which implies  $\sum_{j=1}^{m} V_{(j)} (R_{\sigma(i),j)}   -  R_{\Tilde \sigma(i),j} ) \geq 0$. Finally, the case $\Tilde \sigma(1)=\sigma(1)$ is trivial, since this implies $\Tilde \sigma=\sigma$. Therefore, in any of the cases, the sign property implies that 
 \begin{align*}
     (\ref{eq:last_inequality_2})& =  2L_1\  \Big | \sum_{i=2}^{n} \sum_{j=1}^{m} V_{(j)} (R_{\sigma(i),j)}   -  R_{\Tilde \sigma(i),j} )\Big| \\
     &= 2L_1\  \Big |  \sum_{j=1}^{m} V_{(j)} \sum_{i=2}^{n}(R_{\sigma(i),j)}   -  R_{\Tilde \sigma(i),j} )\Big| \\
     &= 2L_1\  \Big |  \sum_{j=1}^{m} V_{(j)} \sum_{i=1}^{n}(R_{\sigma(i),j)}   -  R_{\Tilde \sigma(i),j} ) - \sum_{j=1}^{m} V_{(j)} (R_{\sigma(1),j)}   -  R_{\Tilde \sigma(1),j} )\Big| \\
     &=  2L_1\  \Big |  \sum_{j=1}^{m} V_{(j)} (R_{\sigma(1),j)}   -  R_{\Tilde \sigma(1),j} )\Big| \\
     &\leq 2L_1M \Bigl( \sum_{j=1}^{m} R_{\sigma(1),\tau(j)}+\sum_{j=1}^{m} R_{\Tilde \sigma(1),\tau(j)}\Bigr) \\
     &= \frac{4L_1M}{n}
 \end{align*}
 Putting everything together, we can conclude that, 
\begin{align*}
    \|\nabla_\theta W_2^2&(g_\theta\# P_{\vect{X}}, h_\theta \# P_{\vect{Z}})- \nabla_\theta W_2^2(g_\theta\# P_{\vect{\Tilde X}}, h_\theta \# P_{\vect{Z}}) \|_2 \leq \frac{12L_1M}{n} + \frac{4L_2 M}{n} \ .
\end{align*}
\end{proof}

\subsection{Extension to the sliced Wasserstein distance} \label{append:slice}
As pointed out in Remark \ref{remark:extension_sliced}, the results of this paper can be extended to higher dimensions by considering the sliced Wasserstein distance. Assume that $g_\theta(x),h_\theta(x)\in\mathbb R^d$. Following the notation of \cref{sec:SensitivityPrivacy}, we are interested now in bounding the sensitivity of the gradient of the (squared) sliced Wasserstein distance between the distributions $g_\theta\# P_{\vect{X}}$ and $h_\theta\# P_{\vect{Z}}$ in $\mathbb R^d$,  defined as 
\begin{align*}
    {SW}_{2}^2(&g_\theta\# P_{\vect{X}},h_\theta \# P_{\vect{Z}}) = \int_{\mathbb S^{d-1} }W_2^2\Bigl(\operatorname{Pr}_{\vartheta}\# (g_\theta\# P_{\vect{X}}),\operatorname{Pr}_{\vartheta}\# (h_\theta \# P_{\vect{Z}})\Bigr) d\mu(\vartheta) \ ,
\end{align*}
where $\mu$ represents the uniform measure on $\mathbb S^{d-1}$, the unit sphere of $\mathbb R^d$. From a practical standpoint, we are mainly interested in the study of the gradient of its Monte-Carlo approximation given by $k$ i.i.d. random directions $\vartheta_1,\ldots,\vartheta_k\in\mathbb S^{d-1}$,
\begin{align*}
{SW}_{2,k}^2(&g_\theta\# P_{\vect{X}},h_\theta \# P_{\vect{Z}}) = \frac{1}{k} \sum_{l=1}^k W_2^2\Bigl(\operatorname{Pr}_{\vartheta_l}\# (g_\theta\# P_{\vect{X}}),\operatorname{Pr}_{\vartheta_l}\# (h_\theta \# P_{\vect{Z}})\Bigr) \ .
\end{align*}
As in the proof of \cref{theorem:gradient_sensitivity}, it suffices to bound the sensitivity of the gradient with respect to the substitution neighboring relation $\vect X \sim_1 \vect{\Tilde X}$. If we define $\Phi(\vect X)= \nabla_\theta {SW}_{2}^2(g_\theta\# P_{\vect{X}},h_\theta \# P_{\vect{Z}})$ and $\Phi_\vartheta(\vect X)= \nabla_\theta W_2^2(\operatorname{Pr}_{\vartheta}\# (g_\theta\# P_{\vect{X}}),\operatorname{Pr}_{\vartheta}\# (h_\theta \# P_{\vect{Z}}))$, by the chain rule and the same reasoning as in the proof of Theorem 1 in \cite{bonnel2015slicedRadon}, we know that under suitable smoothness assumptions, in the set of non-repeated points $\Gamma =\{\theta: g_\theta(x_i)\neq g_\theta(x_j) , h_\theta(z_i)\neq h_\theta(z_j) \textnormal{ for }i\neq j\} $,
\begin{equation*}
\begin{aligned}
    \Phi(\vect X) &= \int_{\mathbb S^{d-1}} \Phi_{\vartheta}(\vect X) d\mu(\vartheta) 
\end{aligned}
\end{equation*}
As in \cref{sec:WassersteinGradients}, we can define the \textit{gradient} $\Phi(\vect X)$ by this expression, even outside the set of differentiability points $\Gamma$, and provide privacy guarantees for every point.  Similarly, if we consider the Monte-Carlo approximation of the gradient $\Phi(\vect X)= \nabla_\theta {SW}_{2,k}^2(g_\theta\# P_{\vect{X}},h_\theta \# P_{\vect{Z}})$, it follows that
$\Phi(\vect X) = \frac{1}{k}\sum_{l=1}^k \Phi_{\vartheta}(\vect X)$. In any case, we can conclude that 
\begin{equation*}
\begin{aligned}
    \Delta \Phi = \sup_{\vect X\sim \vect{\Tilde X}} \|\Phi(\vect X)-\Phi(\vect{\Tilde X})\|_2\leq \sup_{\vartheta\in\mathbb S^{d-1}} \Delta \Phi_\vartheta 
\end{aligned}
\end{equation*}

The sensitivity of $\Phi_\vartheta$ can be controlled with the one-dimensional results in Section \ref{sec:SensitivityPrivacy}. Note that if we define $ g_\theta^\vartheta(x)= \vartheta^T g_\theta(x)$ and  $ h_\theta^\vartheta(x)= \vartheta^T h_\theta(z)$, then $\Phi_\vartheta(\vect X)= \nabla_\theta W_2^2(g_\theta^\vartheta\# P_{\vect{X}}, h_\theta^\vartheta \# P_{\vect{Z}})$, and we can conclude 

$$ \Delta \Phi_\vartheta   \leq  \frac{12 L_1M}{n} +  \frac{4 L_2M}{n} $$
provided that:
\begin{enumerate}
        \item $|g_\theta^\vartheta(x)| =|\vartheta^T g_\theta(x)|\leq M$, $|h_\theta^\vartheta(x)|=|\vartheta^T h_\theta(z)|\leq M$ .
        \item $\|\nabla_\theta g_\theta^\vartheta(x)\|_2 =  \|\vartheta^T \mathcal J_\theta g_\theta(x)\|_2  \leq L_1$,  $\|\vartheta^T \mathcal J_\eta h_\theta(z)\|_2 \leq L_2$.
\end{enumerate} 
In particular, both inequalities are verified uniformly in $\vartheta$ if we impose the following, more natural conditions: 
\begin{enumerate}
\item $\|g_\theta^\vartheta(x)\|_2\leq M$, $\|h_\theta(z)\|_2\leq M$
 \item $\| \mathcal J_\theta g_\theta(x)\|_2= \sup_{\|\eta\|_2=1} \| J_\theta g_\theta(x) \eta \|_2  \leq L_1$, $\| \mathcal J_\theta h_\theta(z)\|_2= \sup_{\|\eta\|_2=1} \| J_\theta h_\theta(z) \eta \|_2  \leq L_2$.
\end{enumerate} 
 The second assumption implies that for every $\vartheta\in\mathbb S^{d-1}$ and $x$, 
 \begin{equation*}
     \begin{aligned}
         \|\vartheta^T\mathcal J_\theta g_\theta(v_j)\|_2 
         &= \vartheta^T\mathcal J_\theta g_\theta(x) \frac{\vartheta^T\mathcal J_\theta g_\theta(x)}{\|\vartheta^T\mathcal J_\theta g_\theta(x)\|_2} \\
         &\leq \|\vartheta\|_2 \Big\| J_\theta g_\theta(x) \frac{\vartheta^T\mathcal J_\theta g_\theta(x)}{\|\vartheta^T\mathcal J_\theta g_\theta(x)\|_2}\Big\|_2 \\
         &\leq L_1 \; ,
     \end{aligned}
 \end{equation*}
and similarly for $h_\theta$. As in the one dimensional setting, the second assumption is verified if $g_\theta$ and $h_\theta$ are $L_1$-Lipschitz and  $L_2$-Lipschitz with respect to $\theta$. To see this, note that if $\|\eta\|_2=1$, by the Lipschitz condition,
\begin{align*}
     \|\mathcal J_\theta g_\theta(x) \eta \|_2 &= \Big\| \lim_{t\rightarrow0} \frac{g_{\theta+t\eta}(x)-g_\theta(x)}{t} \Big\| \leq L_1\|\eta\|_2 = L_1
\end{align*}
Therefore, Theorem \ref{theorem:gradient_sensitivity} can be extended to the multidimensional setting with the sliced Wasserstein distance as follows:
\begin{theorem}\label{theorem:gradient_sensitivity_sliced}
    With all the previous notation, assume that there exists constants $M,L_1,L_2\geq 0$ such that for each $\theta\in\Theta$, $x\in\mathcal X$ and $z\in\mathcal Z$,
    \begin{enumerate}
        \item $\|g_\theta(x)\|\leq M$, $\|h_\theta(z)\|_2\leq M$ .
      \item $\| \mathcal J_\theta g_\theta(x)\|_2= \sup_{\|\eta\|_2=1} \| J_\theta g_\theta(x) \eta \|_2  \leq L_1$, $\| \mathcal J_\theta h_\theta(z)\|_2= \sup_{\|\eta\|_2=1} \| J_\theta h_\theta(z) \eta \|_2  \leq L_2$.
    \end{enumerate} 
    Then, 
 \begin{itemize}
\item[(a)]  Under neighboring relation $\sim_1$ in $\mathcal D = \mathcal X^n$, if we define $\Phi_\theta(\vect X)$ as $\nabla_{\theta} SW_{2}^2(g_\theta\# P_{\vect X}, h_\theta\# P_{\vect Z})$ or its Monte-Carlo approximation $\nabla_{\theta} SW_{2,k}^2(g_\theta\# P_{\vect X}, h_\theta\# P_{\vect Z})$  then
$$ \Delta \Phi_\theta   \leq  4M\frac{3L_1+L_2}{n} \ .$$
\item[(b)] Under neighboring relation $\sim_2$ in $\mathcal D = \mathcal X^n\times \mathcal Z^m$, if we define $\Psi_\theta(\vect X, \vect Z)$ as $\nabla_{\theta} SW_{2}^2(g_\theta\# P_{\vect X}, h_\theta\# P_{\vect Z})$ or its Monte-Carlo approximation $\nabla_{\theta} SW_{2,k}^2(g_\theta\# P_{\vect X}, h_\theta\# P_{\vect Z})$, then
$$ \Delta \Psi_\theta   \leq  4M\max\Bigl\{ \frac{3L_1+L_2}{n}  , \frac{L_1+3L_2}{m}  \Bigr\}\ .$$
    \end{itemize}
   

\end{theorem}

\begin{remark}\label{remark:degraded_bound}
From a computational point of view, if we want to define a clipped approximation  $\mathcal J^{L_1}_\theta g_\theta(x_i)$  of $\mathcal J_\theta g_\theta(x_i)$ that verifies Assumption 2 in Theorem \ref{theorem:gradient_sensitivity_sliced}, this might be done by clipping the eigenvalues of the singular value decomposition of $\mathcal J_\theta g_\theta(x_i)$. This should be done at each step, for each $x_i$ in the batch. To simplify the computation and enable easy parallelization, we have adopted a suboptimal, naive alternative approach. If $g_\theta = (g_\theta^1,\ldots, g_\theta^d)$, and we define 

\begin{equation*}
    \mathcal J^{L_1}_\theta g_\theta(x_i) = \left( \begin{array}{c}
         \operatorname{clip}_{\frac{L_1}{\sqrt{d}}} (\nabla_\theta g_\theta^1)  \\
          \vdots \\
            \operatorname{clip}_{\frac{L_1}{\sqrt{d}}} (\nabla_\theta g_\theta^d)
    \end{array}\right) \ ,
\end{equation*}
then it is straightforward to see that  $\| \mathcal J^{L_1}_\theta g_\theta(x)\|_2= \sup_{\|\eta\|_2=1} \| J^{L_1}_\theta g_\theta(x) \eta \|_2  \leq  L_1$.
\end{remark}


\subsection{Other Proofs}
\begin{proof}[Proof of Lemma~\ref{lemma:subsampling}]
    See the proof of Theorem 29 in \cite{DBLP:journals/corr/abs-2210-00597} which gives the result up to a minor adaptation. The term $\max \p{\frac{n_{1}'}{n_1}, \dots,  \frac{n_k'}{n_k}}$ indeed comes from considering the worst case analysis depending on which category the differing point is in.
\end{proof}

\begin{proof}[Proof of Theorem~\ref{th:fairandprivate}]
    Formally, with the notation of \Cref{definition:neighboring}, define for the first part $\mathcal D = \mathcal D_0^{n_0}\times \mathcal D_1^{n_1}$, where $\mathcal D_j = \mathcal X \times \mathcal Y \times \{j\}$ in the supervised case, and  $\mathcal D_j = \mathcal X  \times \{j\}$ in the unsupervised case, for $j=0,1$. Applying \cref{theorem:gradient_sensitivity_sliced} with $g_\theta = h_\theta$, we can bound the sensitivity of $\nabla_\theta \mathscr L^{SP}_\alpha(g_\theta)$ by \eqref{eq:sensitivity_SP}.
    
    For the second part, consider $\mathcal D =\prod_{j,k} \mathcal D_{j,k}^{n_{j,k}}$, where $\mathcal D_{j,k} = \mathcal X \times \{j\} \times \{k\}$, for $j \in \{0,1\}, k\in\{0,\ldots,R-1\}$. Under the relation $\sim_{2R}$, given two neighboring datasets, all the terms except one are the same in the sum in \eqref{eq:loss_SP}, and similarly for the gradient expression. More precisely,  under the assumptions of the theorem, with the notation adopted in \cref{section:fairness},
    \begin{align*}
        &\sup_{\vect D \sim_{2R} 
 \vect{\Tilde D}} \Big\|  \frac{1}{R} \sum_{k=1}^K \nabla_\theta{SW}_2^2\Bigl(  g_\theta \# P_{\vect X_{0,k}}, g_\theta \# P_{\vect X_{1,k}} \Bigr) -  \frac{1}{R} \sum_{k=1}^K \nabla_\theta{SW}_2^2\Bigl(  g_\theta \# P_{\vect{\Tilde X}_{0,k}}, g_\theta \# P_{\vect{\Tilde X}_{1,k}} \Bigr) \Big\|_2 \\
  &\leq \frac{1}{R}   \sup_{\vect D \sim_{2R} 
 \vect{\Tilde D}} \sum_{k=1}^K \Big\|  \nabla_\theta{SW}_2^2\Bigl(  g_\theta \# P_{\vect X_{0,k}}, g_\theta \# P_{\vect X_{1,k}} \Bigr) -   \nabla_\theta{SW}_2^2\Bigl(  g_\theta \# P_{\vect{\Tilde X}_{0,k}}, g_\theta \# P_{\vect{\Tilde X}_{1,k}} \Bigr) \Big\|_2 \\     
   &\leq \frac{1}{R} \max_{k=0,\ldots,R-1} \sup_{(\vect X_{0,k},\vect X_{1,k}) \sim_{2} 
 (\vect{\Tilde X}_{0,k},\vect{\Tilde X}_{1,k})} \Big\|  \nabla_\theta{SW}_2^2\Bigl(  g_\theta \# P_{\vect X_{0,k}}, g_\theta \# P_{\vect X_{1,k}} \Bigr) -\nabla_\theta{SW}_2^2\Bigl(  g_\theta \# P_{\vect{\Tilde X}_{0,k}}, g_\theta \# P_{\vect{\Tilde X}_{1,k}} \Bigr) \Big\|_2 \\  
 &\leq \frac{1}{R}\max_{k=0\ldots,R-1} \frac{16ML}{\min \{n_{0,k},n_{1,k}\}} = \frac{1}{R}\frac{16ML}{\min_{j,k} \{n_{j,k}\}} 
    \end{align*}
which implies \eqref{eq:sensitivity_EO}.
    \end{proof}




\section{Additional details on the fairness  experiments}\label{appendix:experiments}

%\subsection{Data generation}
In order to demonstrate the versatility of our methodology for imposing fairness in different scenarios, we use an illustrative model to simulate bias in algorithmic decision-making.
Note that we do not provide comparisons with other application-specific methodologies, as our approach is highly general and does not include the statistical, convergence, or fairness guarantees that may be described by other methods, see \cite{xu2019achievingDPandFairness,jagielski2019differentially,ding2020differentially,lowi2022SDPfairLearning,yaghini2023learning,Ghoukasian2024DPFairBinaryClassif} for the fair and private classification problem, or \cite{xian2024DPfairRegression} for fair and private one-dimensional regression.  Yet we provide, to our knowledge, the first method to handle  novel problems such as multidimensional fair and private regression, or fair and private representation learning.

We consider $\vect D =\{(x_i,a_i,y_i)\}_{i=1}^n$ i.i.d. samples with the same distribution as $(X,A,Y^C,Y)$, where $X$ denotes the features, $A$ is the sensitive variable, $Y^C=(Y^C_1,Y^C_2)$ is a continuous response variable and $Y$ is a discrete version of $Y^C$, related by

\begin{enumerate}
    \item $Y^C \sim U([0,1]\times [0,1])$
    \item $Y = I\Bigl(Y^C_2 >1 - Y^C_1 \Bigr)$ 
    \item $A = BY + (1-B)(1-Y)$, where $B$ is a Bernoulli variable of parameter $p$ independent of Y.
    \item $X_{core} = [ \underbrace{Y^C,\ldots,Y^C}_{d_{core}/2 \  times}] + N\Bigl(0,\sigma^2_{core}\ I_{d_{core}}\Bigr)$,  $\quad X_{spurious} = [ \underbrace{A,\ldots,A}_{d_{sp} \  times}] +  N\Bigl(0,\sigma^2_{sp}\ I_{d_{sp}}\Bigr)$
    \item $X = [X_{core},X_{sp}]$
\end{enumerate}

Therefore, this generated data consists in a response variable $Y_C$, which is correlated with the sensitive attribute $A$. The features $X$ can be divided into two parts: a first part $X_{core}$ which is a noisy transformation of $Y_C$, and a second spurious part $X_{sp}$ which is a noisy version of $A$. If $p$ is close to 1, most of the cases verify $A=Y$ and therefore, the decision of the algorithm relies highly  on the sensitive variable $A$. Bias in the algorithmic decision is created when the sensitive variable $A$ is not aligned with the decision. When $Y \neq A$, the learning task is more complicated since while $X_{core}$ is correlated with $Y$, the spurious part pushes towards the bad decision. This setting reproduces the characteristics of some of the main biases present in many data sets, for instance,  \cite{adult_2} or \cite{statlog_(german_credit_data)_144}  in supervised learning. We will explore this problem in different situations, and we will show how penalized models with our Wasserstein losses can help to alleviate the unfairness of these models, according to different notions, while preserving privacy guarantees. In all our experiments we will consider $n=30000$, $p=0.7$, $d_{core}=d_{sp}=8$, $\sigma^2_{core} = 1/5$, $\sigma^2_{sp} = 2/5$. \\

All  models are trained with DP-SGD as explained in \cref{sec:DeepLearningFramework}, with clipping constant $C>0$ for the individual gradients in \eqref{loss:ERM}, as usual in DP-SGD, and inner clipping constants $M,L>0$ for the Wasserstein gradient approximation \eqref{eq:gradient_approx} or its sliced version. In the latter case, all the experiments use the naive clipping procedure explained in \cref{remark:degraded_bound}. \cref{th:fairandprivate} and the procedure described in Section~\ref{sec:DeepLearningFramework}, enable us to compute the privacy budget obtained after $T$ iterations of DP-SGD. In particular, in all the experiments, we fix the number of iterations $T$ and the value of $\delta$, and compute the required noise to obtain $(\epsilon,\delta)$-DP after $T$ iterations, for different values of the privacy budget $\epsilon$ and the weight $\alpha\in[0,1]$ in the penalized loss functions \eqref{eq:loss_SP} and \eqref{eq:loss_EOO}. Batch sizes are $n'_j \approx n_j/5$ when minimizing \eqref{eq:loss_SP}, and $n'_{j,k} \approx n_{j,k}/5$ when minimizing \eqref{eq:loss_EOO}, where the approximation is related to internal parallelization of the gradient computations in the code.  \\

Following the notation of the main text, denote $\vect X_j = (x_i: a_i=j)$, $n_j = \textnormal{length}(\vect X_j)$ for $j=0,1$, and $\vect X_{j,k} = (x_i: a_i=j,y_i=k)$, $n_{j,k} = \textnormal{length}(\vect X_{j,k})$ for $j,k  \in \{0,1\}$. Given our data generation procedure, we know that $\mathbb E(n_j)=n/2$, $\mathbb E(n_{j,j}) = pn/2$ and $\mathbb E(n_{j,1-j}) = (1-p)n/2$ for $j\in\{0,1\}$.\\ 


\subsection{Classification}
First, we consider the problem of predicting the label $Y$ as a function of $X$. Our decision rule is based on logistic regression, where the function $g_\theta$ maps each $x_i$ to the predicted probability $g_\theta(x_i)\in (0,1)$. The classification rule is given by $G_\theta(x) = I(g_\theta(x)>1/2)$. $g_\theta$ is defined as a neural network with one layer and a sigmoid activation function, and it is trained with DP-SGD and a binary cross-entropy loss function, denoted by $\ell_{bce}$. We have analyzed fairness using two of the most common notions.

\begin{itemize}
    \item \textit{Statistical parity:} Statistical parity corresponds to the situation where  the algorithmic decision does not depend on the sensitive variable. It is usually measured by the Disparate Impact, defined as \begin{equation}\label{eq:disparate:impact}
        DI(G_\theta) = \frac{\mathbb P (G_\theta(X) = 1 | A = 0)}{\mathbb P (G_\theta(X) = 1 | A = 1)} \ .
    \end{equation}
    Enforcing statistical parity by enforcing independence between $G_\theta(X)$ and $A$ often produces unstable solutions as discussed in \cite{krco2023mitigating} or \cite{barrainkua2024uncertainty}, hence many authors propose to mitigate not only the mean but the whole distribution of the predicted probabilities $g_\theta(X) \in (0,1)$ as in \cite{risser2022tacklingAlgorithm}, \cite{gouic2020projection} or \cite{chzhen2020fair}. Statistical parity is thus satisfied if  $ \mathcal L( g_\theta(X) | A = 0 ) = \mathcal L ( g_\theta(X) | A = 1 )
    $. In our discrete setting, statistical parity can be favored by minimizing   
    \begin{equation}\label{eq:loss_SP_appendix}
    \mathscr L^{SP}_\alpha(g_\theta) = (1-\alpha) \frac{1}{n}\sum_{i=1}^n \ell_{bce}(g_\theta(x_i),y_i)  + \alpha W_2^2\Bigl(g_\theta \# P_{\vect X_0} , g_\theta \# P_{\vect X_1}\Bigr)  
    \end{equation}
    
    In Figure \ref{fig:di} we present the results obtained for different values of the weight $\alpha$ and the privacy budget $\epsilon$, when we fix $\delta = 0.1/n$,  number of iterations $T=500$, clipping constants $C=5$, $M=L=1$ and learning rate $=0.05$. For every pair of $\alpha$ and $\epsilon$, we plot the histograms of the distribution of the predicted probabilities $g_\theta(X)|A=0$ and $g_\theta(X)| A=1$. Above each graph, we can see the noise required to achieve the privacy budget in the fixed number of iterations, the weighted training loss value obtained and the specific values of each term in the loss, and the accuracy and disparate impact of $G_\theta$ on test data.  Two main conclusions can be drawn from Figure \ref{fig:di}. First, it confirms that the Wasserstein penalization approach mitigates the unfair biases present in the data set. We can see that, for increasing values of $\alpha$, the histograms of the scores conditioned by the value of the sensitive variable get closer, leading to a progressive reduction of biases, as seen in the decreasing values of the disparate impact, albeit at the expense of accuracy, as expected. The second important conclusion is that adding privacy does not significantly alter the results of the optimization. For different privacy budgets $\epsilon$, both the histogram and the computed measures do not change much across the rows. Moreover, Figure \ref{fig:di_losses} shows the training loss curve of the optimization for each value of $\alpha$ considered when $\epsilon$ varies. Low values of $\epsilon$ lead to noisy versions of the loss curve, but very close to the non-private version.
   

\begin{figure}[h!]
    \centering
\includegraphics[width=1\linewidth]{di.pdf}
    \caption{Histogram of the predicted probabilities $g_\theta(X)$ of the model conditioned by the sensitive attribute $A=0,1$ in the training set. $\theta$ is the parameter obtained after 500 iterations of DP-SGD minimizing \ref{eq:loss_SP_appendix} for the different values of $\alpha$ (columns), with different privacy budgets $\epsilon$ (rows) for $\delta = 0.1/n$ fixed. The parameters of the optimization are the learning rate $= 0.05$, clipping values $C=5, \ M=1, \ L=1$, batch sizes $n_0'\approx n_0/5, n_1' \approx n_1/5$. Above each graph we indicate the noise added at each step of DP-SGD to obtain the desired privacy level, the value of the loss \ref{eq:loss_SP_appendix} in the training procedure, together with the individual value of the classification loss (CL) and the distributional Wasserstein loss (W). Last line includes accuracy (ACC) and disparate impact (DI) of the classification rule $G_\theta$ computed with independent test data set.}
    \label{fig:di}
\end{figure}

\end{itemize}

\begin{figure}[h!]
    \centering
    \includegraphics[width=\linewidth]{di_losses.pdf}
    \caption{Training loss curve for the experiment of Figure \ref{fig:di}. Each graph represents the training loss \eqref{eq:loss_SP_appendix}  for a fixed value of $\alpha$ along the iterations of DP-SGD, for the different privacy budgets of the experiments. }
    \label{fig:di_losses}
\end{figure}
  
\begin{itemize}
        \item \textit{Equality of odds:} Beyond guaranteeing the same decision for all, which is not suitable in some cases where the sensitive variable impacts the decision, bias mitigation may require that the model performs with the same accuracy for all groups, often referred to as equality of odds. Usual measures of this bias for a classification rule $G_\theta$ are computed using the two following indexes:
    \begin{align}\label{eq:equality_odds_index}
         EO_1(G_\theta)& = \frac{\mathbb P (G_\theta(X) = 1 | A = 0, Y=1)}{\mathbb P (G_\theta(X) = 1 | A = 1, Y=1)} \ . \\
         EO_0(G_\theta)& = \frac{\mathbb P (G_\theta(X) = 1 | A = 0, Y=0)}{\mathbb P (G_\theta(X) = 1 | A = 1, Y=0)} \ . 
    \end{align}
    With the same ideas as before, Equality of Odds bias mitigation can be enforced for the distribution of the predicted probabilities $g_\theta$ by enforcing that $ \mathcal L( g_\theta(X) | A= 0, Y = j ) = \mathcal L ( g_\theta(X) | A=1 , Y = j)$ for $j = 0, 1$. For this, we train the model with the penalized loss 
   \begin{equation}\label{eq:loss_EOO_appendix}
        \mathscr L^{EOO}_\alpha(g_\theta) = (1-\alpha) \frac{1}{n}\sum_{i=1}^n \ell_{bce}(g_\theta(x_i),y_i)  + \frac{\alpha}{2} W_2^2\Bigl(g_\theta \# P_{\vect X_{0,0}} , g_\theta \# P_{\vect X_{1,0}}\Bigr) +\frac{\alpha}{2} W_2^2\Bigl(g_\theta \# P_{\vect X_{0,1}} , g_\theta \# P_{\vect X_{1,1}}\Bigr) \ .
   \end{equation}


Figure \ref{fig:eoo} shows the results of training the model minimizing \ref{eq:loss_EOO} for different values of $\alpha$ and the privacy budget $\epsilon$, with fixed $\delta = 0.1/n$, number of iterations $T=500$, clipping constants $C=5$, $M=L=1$ and learning rate $=0.05$. The histogram of the distribution of the predicted probabilities, $g_\theta(X)|A=1,Y=j$ versus $g_\theta(X)|A=0,Y=j$, illustrates the model's capability to minimize discrepancies between the distributions as $\alpha$ increases, as shown by the values of $EO_0,EO_1$. From Figure \ref{fig:eoo}, we can also observe that private training has minimal impact on the model's fit across all values of $\alpha$. Similarly, it does not significantly affect the learning loss curve during optimization, as shown in Figure \ref{fig:eoo_losses}.



\end{itemize}


\begin{figure}[h]
    \centering
\includegraphics[width=1\linewidth]{eoo.pdf}
    \caption{Histogram of the predicted probabilities $g_\theta(X)$ of the model conditioned by the sensitive attribute $A=0,1$ and the label $Y=0,1$ in the training set. $\theta$ is the parameter obtained after 500 iterations of DP-SGD minimizing \eqref{eq:loss_EOO_appendix} for the different values of $\alpha$ (columns), with different privacy budgets $\epsilon$ (rows) for $\delta = 0.1/n$ fixed. The parameters of the optimization are the learning rate $= 0.05$, clipping values $C=5, \ M=1, \ L=1$, batch sizes $n_0'\approx n_0/5, n_1' \approx n_1/5$. Above each graph we indicate the noise added at each step of DP-SGD to obtain the desired privacy level, the value of the loss \eqref{eq:loss_EOO_appendix} in the training procedure, together with the individual value of the classification loss (L) and the  Wasserstein loss (W). Last line includes accuracy, $EO_0$ and $EO_1$ indexes computed with test data set.}
    \label{fig:eoo}
\end{figure}




\begin{figure}[h!]
    \centering
    \includegraphics[width=0.88\linewidth]{eoo_losses.pdf}
    \caption{Training loss curve for the experiment of Figure \ref{fig:eoo}. Each graph represents the training loss \eqref{eq:loss_EOO_appendix} for a fixed value of $\alpha$ along the iterations of DP-SGD, for the different privacy budgets of the experiments. }
    \label{fig:eoo_losses}
\end{figure}

\subsection{Regression}
 In our generating mechanism, the label $Y\in \{0,1\}$ is defined as a  set indicator function of a continuous response $Y^C \in [0,1]\times[0,1]$. From the data-generating process, it is easy to derive the distribution of $Y_C$ conditioned by the sensitive attribute. If $T_0$ denotes the triangle with vertices $(0,0),(0,1),(1,0)$ and $T_1$ the triangles with vertices $(0,1),(1,1),(1,0)$, the we know that $Y^C|A=j$ follows a mixture of the uniform distributions on $T_0$ and $T_1$, with weight $p$ in $T_0$ and $(1-p)$ in $T_1$ if $A=0$, and vice versa if $A=1$. The aim of this experiment is to perform  private and bi-dimensional fair regression over $Y_C$, which has never been considered before in the literature. To simplify our clipping bounds, we have centered our data to obtain a distribution in $[-1/2,1/2]\times[-1/2,1/2]$, and we have trained a two-layer neural network with hidden dimension 64, sigmoid activation function in the last layer,  with the output centered by subtracting  $(1/2,1/2)$, and minimizing the loss
 \begin{align}\label{eq:loss_regression}
    \mathscr L^{SP}_\alpha(g_\theta) = (1-\alpha) \frac{1}{n}\sum_{i=1}^n \|g_\theta(x_i)- y_i^C\|_2^2    + \alpha SW_2^2\Bigl(g_\theta \# P_{\vect X_0} , g_\theta \# P_{\vect X_1}\Bigr)  
\end{align}

Figure \ref{fig:regression} shows the results of this experiment for different values of $\alpha$ and the privacy budget $\epsilon$, with fixed $\delta = 0.1/n$, number of iterations $T=1000$, clipping constants $C=10$, $M=1/\sqrt{2}$, $L=\sqrt{2}$, learning rate $=0.05$ and number of projections in the Monte Carlo approximation $= 50$. From the visual inspection of the plots, we can appreciate that our statistical parity penalization helps to reduce the differences between the distributions of the predicted values. To aid visual inspection, we provide the values of the number of points over the diagonal for each class $A=0$ and $A=1$. If $g_\theta(x) = (g^1_\theta(x),g^2_\theta(x))$

$$ OD_0 = \frac{ \# \{X: g^2_\theta(X)>-g^1_\theta(X), A = 0\}}{n_0}$$
$$ OD_1 = \frac{ \# \{X: g^2_\theta(X)>-g^1_\theta(X), A = 1\}}{n_1}$$
 
Finally, \cref{fig:regression_losses} shows the convergence of the loss curve for the different values of $\alpha$ and $\epsilon$. As in the previous examples, the private loss curves are simply noisy versions of the non-private ones.

\begin{figure}[h!]
    \centering
\includegraphics[width=1\linewidth]{regression.pdf}
    \caption{Plot of $g_\theta(X)$ conditioned by the sensitive attribute $A=0,1$ in the training set. $\theta$ is the parameter obtained after 1000 iterations of DP-SGD minimizing \eqref{eq:loss_regression} for the different values of $\alpha$ (columns), with different privacy budgets $\epsilon$ (rows) for $\delta = 0.1/n$ fixed. The parameters of the optimization are the learning rate $= 0.05$, clipping values $C=10, \ M=1/\sqrt{2}, \ L=\sqrt{2}$, batch sizes $n_0'\approx n_0/5, n_1' \approx n_1/5$, number of random preojections $=50$. Above each graph we indicate the noise added at each step of DP-SGD to obtain the desired privacy level, the value of the loss \eqref{eq:loss_regression} in the training procedure, together with the individual value of the regression loss (RL) and the  sliced Wasserstein loss (SW). Last line includes accuracy, $OD_0$ and $OD_1$.}
    \label{fig:regression}
\end{figure}



\begin{figure}[h!]
    \centering
    \includegraphics[width=0.6\linewidth]{regression_losses.pdf}
    \caption{Training loss curve for the experiment of Figure \ref{fig:regression}. Each graph represents the training loss for a fixed value of $\alpha$ in \eqref{eq:loss_regression} along the iterations of DP-SGD, for the different privacy budgets of the experiments. }
    \label{fig:regression_losses}
\end{figure}





\subsection{Representation learning.} Finally, we present another completely novel application of our procedure: fair representation learning. Using the same data as before, the objective is to privately learn an encoder $\varphi_{\theta_a}$ and a decoder $\psi_{\theta_b}$ minimizing the reconstruction mean squared error of the reconstructed values, penalized with the sliced Wasserstein distance to alleviate unfairness present in the data. For simplicity, we denote $\theta=(\theta_a,\theta_b)$, $\varphi_{\theta}= \varphi_{\theta_a}$ and $\psi_\theta = \psi_{\theta_b}$. In our example, the encoder and decoder are defined as fully connected neural networks with two layers, hidden dimension 62 and bi-dimensional latent space, and we look for an encoded representation enhancing statistical parity by minimizing 
\begin{align}\label{eq:loss_autoencoder}
    \mathscr L^{SP}_\alpha(\varphi_\theta,\psi_\theta) = (1-\alpha) \frac{1}{n}\sum_{i=1}^n \|\psi_\theta (\varphi_\theta(x_i))-x_i\|_2^2 +\alpha SW_2^2\Bigl(\varphi_\theta \# P_{\vect X_0} , \varphi_\theta \# P_{\vect X_1}\Bigr)  \ .
\end{align}


\begin{figure}[h!]
    \centering
\includegraphics[width=1\linewidth]{autoencoder.pdf}
    \caption{Plot of the latent space, $\varphi_\theta(X)$ conditioned by the sensitive attribute $A=0,1$ in the training set. $\theta$ is the parameter obtained after 500 iterations of DP-SGD minimizing \eqref{eq:loss_autoencoder} for the different values of $\alpha$ (columns), with different privacy budgets $\epsilon$ (rows) for $\delta = 0.1/n$ fixed. The parameters of the optimization are the learning rate $= 0.01$, clipping values $C=10, \ M=2, \ L=\sqrt{2}$, batch sizes $n_0'\approx n_0/5, n_1' \approx n_1/5$ and number of projections $=50$. Above each plot we indicate the noise added at each step of DP-SGD to obtain the desired privacy level, the value of the loss \ref{eq:loss_autoencoder} in the training procedure, together with the individual value of the reconstruction loss (RL) and the sliced Wasserstein loss (SW). Last line includes the reconstruction loss on the core variables ($RL_1$), accuracy and disparate impact on test data.}
    \label{fig:autoencoder}
\end{figure}

As usual, \cref{fig:autoencoder} presents the result of training this model for different values of $\alpha$ and $\epsilon$, with fixed $\delta=0.1/n$, number of iterations $T=500$ iterations, clipping values $C=10,M=2,L=\sqrt{2}$, learning rate $0.01$ and number of projections in the Monte Carlo approximation $= 50$. Over each plot, we can see the noise introduced to achieve the required privacy level, the weighted and individual values of the loss during training, and other comparative measures computed with an independent test sample. First, $RL_c$ denotes the reconstruction loss in the core part $X_{core}$, i.e. the first eight variables of $X$. The rest of the variables $X_{sp}$ are just a noisy version of $A$. Thus, $RL_c$ provides a measure of the error in the reconstruction loss for the relevant part of the data, and Figure \ref{fig:autoencoder} shows that for increasing values of $\alpha$, even though the reconstruction loss increases significantly, the reconstruction associated with the core part is not affected much. The other measures computed on the test data are the accuracy and disparate impact of a simple logistic regression model trained on the encoded representation of a portion (60\%) of the test data and evaluated on the remaining (40\%). We observe that increasing values of $\alpha$ lead to values of the disparate impact index closer to 1, at the expense of a decrease in accuracy.  Finally, we can infer from Figures \ref{fig:autoencoder} and \ref{fig:autoencoder_losses} that privacy doesn't affect much to the results of the optimization. 





\begin{figure}[h!]
    \centering
    \includegraphics[width=0.88\linewidth]{autoencoder_losses.pdf}
    \caption{Training loss curve for the experiment of Figure \ref{fig:autoencoder}. Each graph represents the training loss \eqref{eq:loss_autoencoder} for a fixed value of $\alpha$  along the iterations of DP-SGD, for the different privacy budgets of the experiments. }
    \label{fig:autoencoder_losses}
\end{figure}

\section{Additional Experiments} \label{additional_exp}

To demonstrate the ability of our method to privately learn distributions in a deep learning scenario, we trained a neural network to approximate the distribution of a variable $Z$ by applying a transformation $g_\theta$ to another variable $X$. Specifically, we considered $n = 100000$ samples of $Z$ drawn from the uniform distribution on a circle with radius $3/4$, and equal number of samples of $X$ drawn from the standard Gaussian distribution in $\mathbb{R}^2$. The function $g_\theta$ is defined as a fully connected neural network with an input dimension 2, three hidden layers with dimensions $(128, 64, 64)$, and an output dimension 2. Figure \ref{fig:gradient_flow} shows the evolution of the matching problem at different training steps. Thanks to \cref{theorem:gradient_sensitivity_sliced}, our methodology provides privacy guarantees for both the fixed variable $Z$ and the \textit{trained} variable $X$, in the sense that $g_\theta$ is applied to $X$. Above each plot, we can see the iteration number, the value of the loss, and the privacy budget $\epsilon$ for both $X$ and $Z$, at each training step. The optimization parameters are $\delta = 0.1/n$,  batch size$ = 10,000$, learning rate $= 0.0075$, number of projections in the Monte Carlo approximation $= 50$, clipping values $M = 1$ and $L = 2\sqrt{2}$ (imposed using the suboptimal approach described in \cref{remark:degraded_bound}). To ensure more stable results, once we have privatized the gradient by adding noise, we clip the gradient again to improve the method's stability. Note that this step preserves privacy due to the post-processing property. In comparison with the approach of \cite{rakotomamonjy2021DPslicedWasserstein}, which can only provide privacy guarantees with respect to the \textit{non-trained} variable $Z$, our method provides privacy guarantees for both variables. Although privacy with respect to $Z$ might be sufficient in data generation, this example highlights the limitations of \cite{rakotomamonjy2021DPslicedWasserstein}, as their procedure cannot be applied in any situation where \textit{training} is required on private data.



\begin{figure}[h!]
    \centering
\includegraphics[width=1\linewidth]{circles.pdf} 
\caption{Data generation experiment in \cref{additional_exp}. Samples from $X$ are represented in blue, samples from $Z$ in orange. Above each graph, we can see the iteration, the value of the loss, and privacy budgets w.r.t. the variables $X$ and $Z$.}
\label{fig:gradient_flow}
\end{figure}

 
\section{Counterexample for general cost functions} \label{counterexample}

In this section, we demonstrate that we cannot bound in general the sensitivity of the gradient if we use the Wasserstein loss function $W_p$, for general $p\geq 1$. Following the notation of Theorem \ref{theorem:gradient_sensitivity}, we denote by $\vect X = \{x_1,\ldots,x_n\} \subset \mathcal X^n$ the private dataset, $\vect Z=\{z_1,\ldots, z_n\} \subset \mathbb R$ the non-private dataset, and $P_{\vect X}, P_{\vect Z}$ the associated empirical distributions. Given $g_\theta: \mathcal X \rightarrow \mathbb R$ depending on the parameter $\theta$, and considering $h_\theta=I_d$, we study the sensitivity of $\Phi_\theta(\vect X)= \nabla_{\theta} W_p(g_\theta\# P_{\vect X}, P_{\vect Z})$, for the particular values of

\begin{itemize}
    \item $\vect X =\{x_1,\ldots,x_n\}$ with $x_i=\frac{i}{n}, \ i \in [n]$
     \item $\vect{\Tilde X} =\{\Tilde x_1,\ldots,\Tilde x_n\}$ with $\Tilde x_i=\frac{i-1}{n}, \ i \in [n]$
     \item $\vect Z=\{z_1,\ldots, z_n\}$, with $z_i = \frac{2i-1}{2n}, \ i\in [n].$
     \item $g_\theta(x) = x + \theta$
\end{itemize}

For this particular choice, $\vect X\sim_1\vect{\Tilde X}$, and  Assumption \ref{assumption1} and \ref{assumption2} in Theorem \ref{theorem:gradient_sensitivity} are verified for certain constants (once we restrict the domain of $\theta$). From the quantile representation, it is easy to compute 

\begin{align*}
     W_p(g_\theta\# P_{\vect X}, P_{\vect Z}) =& \Bigl(\frac{1}{n} \sum_{i=1}^n | x_i + \theta - z_i|^p \Bigr)^{1/p} =  \Bigl(\frac{1}{n} \sum_{i=1}^n \Big|\frac{1}{2n} + \theta \Big|^p \Bigr)^{1/p} = \left\{ \begin{array}{cc}
       \frac{1}{2n} + \theta   & \textnormal{if}\  \theta>-\frac{1}{2n} \\
        - \theta -\frac{1}{2n}   &  \textnormal{if}\ \theta\leq -\frac{1}{2n} 
     \end{array}\right. \\
    W_p(g_\theta\# P_{\vect{\Tilde X}}, P_{\vect Z}) =& \Bigl(\frac{1}{n} \sum_{i=1}^n | \Tilde x_i + \theta - z_i|^p \Bigr)^{1/p} =  \Bigl(\frac{1}{n} \sum_{i=1}^n \Big|-\frac{1}{2n} + \theta \Big|^p \Bigr)^{1/p}  = \left\{ \begin{array}{cc}
       -\frac{1}{2n} + \theta   & \textnormal{if}\  \theta>\frac{1}{2n} \\
         \theta -\frac{1}{2n}   &  \textnormal{if}\ \theta\leq \frac{1}{2n} 
     \end{array}\right.
\end{align*}

Therefore, by setting $\theta = 0$, we observe that the derivatives are $ \Phi_0(\vect X) = 1$ and $ \Phi_0(\vect{\Tilde X}) = -1$. Consequently, $ \Delta \Phi_\theta \geq 2$, indicating that the sensitivity does not decrease with the sample size $n$.


\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
