\section{Related Work}
Over the years, a range of benchmarks have been introduced to advance research in automated fact-checking. Early benchmarks such as FEVER Stevenson et al., "Automatic Fact-Checking: A Multi-Dataset Benchmark" and FEVEROUS Guo et al., "FEVEROUS: An Enhanced Dataset for Fact-Checking with Open-Domain Evidence Retrieval" have been widely adopted, providing large-scale datasets for claim verification. These benchmarks include retrieval corpora but are based on synthetic claims generated from Wikipedia content, limiting retrieval to a constrained domain of Wikipedia articles. While these datasets have driven significant progress, they fail to reflect the complexity and diversity of real-world claims and evidence.

More recently, datasets like ClaimDecomp Guo et al., "ClaimDecomp: A Dataset for Fact-Checking with Evidence Retrieval" and QABriefs Chen et al., "QABriefs: Question Answering over Briefs for Fact-Verification" have emerged, leveraging real-world fact-checks conducted by professional fact-checkers. These datasets introduce claim decomposition, where complex claims are broken down into sub-questions to facilitate evidence retrieval and verification. However, their retrieval process remains limited to pre-verified justification documents, which simplifies the task and does not simulate realistic retrieval challenges. Similarly, AVeriTeC Guo et al., "AVeriTec: An Enhanced Dataset for Fact-Checking with Open-Domain Evidence Retrieval" incorporates human-authored questions paired with search-engine-retrieved answers, offering a step toward open-domain retrieval. Nevertheless, it lacks a dedicated retrieval corpus, restricting its ability to evaluate retrieval performance comprehensively.

Other benchmarks, such as QuanTemp Guo et al., "QuanTemp: A Benchmark for Numerical Fact-Checking" , focus on numerical fact-checking and their primary focus is on numerical reasoning based verification, and retrieval is not directly evaluated. 

Despite these advancements, current benchmarks either rely on constrained retrieval settings or pre-verified evidence, failing to fully capture the challenges of open-domain, real-world retrieval where evidence must be identified from diverse, unstructured sources. Addressing these gaps is the primary goal of this paper.