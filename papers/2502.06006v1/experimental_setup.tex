

\vspace{-0.7em}
\section{Experimental Setup}
 We evaluate a range of state-of-the-art retrieval and re-ranking approaches on \name{},  with two V100S-PCIE-32GB GPUs. 

\textbf{Lexical and Sparse retrieval}: We employ ElasticSearch's BM25 implementation due to low latency and ease of use~\cite{fogelberg2023search}. We also employ SPLADEV2 \cite{SPLADEv2} which is a neural model employing query and document sparse expansions.

\textbf{Dense retrieval}: DPR ~\cite{karpukhin-etal-2020-dense} comprises a query, document bi-encoder model, and we employ the open source model trained on multiple Question Answering datasets \textit{facebook-dpr-question-encoder-multiset-base}. We also include ANCE~\cite{ance} in our evaluation, which is a bi-encoder model that samples hard negatives through Nearest Neighbour search over the corpus index, yielding better negatives. We employ \textit{msmarco-roberta-base-ance-first} trained on MS-MARCO \cite{bajaj2018ms}. Tas-b \cite{tas-b} is a bi-encoder trained using supervision from a cross-encoder. We benchmark the recent \textit{snowflake-arctic-embed-s} \cite{merrick2024embeddingclusteringdataimprove} which improves contrastive pre-training through semantic clustering-based source stratification. 

\textbf{Late-Interaction Models}: We implement the late-interaction model ColBERTv2 \cite{colbert,santhanam-etal-2022-colbertv2} in \name{}. It is a late-interaction model that employs a cross-attention-based MaxSim operation betweeen query and document token representations.

\textbf{Re-rankers:} We employ several state-of-the-art cross-encoder models including Large Language Models (LLMs) for re-ranking. We employ ColBERTV2 in re-ranker mode due to better relevance estimate from late-interaction. We evaluate BERT based cross-encoders like \textit{cross-encoder/mmarco-mMiniLMv2-L12-H384-v1} and \textit{cross-encoder/msmarco-MiniLM-L12-en-de-v1} that have shown to achieve impressive performance on BEIR \cite{beir} benchmark. We also benchmark more recent re-rankers like \textit{jinaai/jina-reranker-v2-base-multilingual} which employs flash-attention. We also evaluate LLM based re-rankers like \textit{Alibaba-NLP/gte-multilingual-reranker-base}

\textbf{Metrics:}  We choose Normalized Discounted Cumulative Gain (nDCG@k) and Recall@k as the primary metrics for our results. 
%This is because nDCG is a rank-aware metric that is well-suited for binary relevance judgments.
\vspace{-1.2em}

