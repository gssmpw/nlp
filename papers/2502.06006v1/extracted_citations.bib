@inproceedings{10.1145/3626772.3657874,
author = {V, Venktesh and Anand, Abhijit and Anand, Avishek and Setty, Vinay},
title = {QuanTemp: A real-world open-domain benchmark for fact-checking numerical claims},
year = {2024},
isbn = {9798400704314},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},


abstract = {With the growth of misinformation on the web, automated fact checking has garnered immense interest for detecting growing misinformation and disinformation. Current systems have made significant advancements in handling synthetic claims sourced from Wikipedia, and noteworthy progress has been achieved in addressing real-world claims that are verified by fact-checking organizations as well. We compile and release QuanTemp, a diverse, multi-domain dataset focused exclusively on numerical claims, encompassing comparative, statistical, interval, and temporal aspects, with detailed metadata and an accompanying evidence collection. This addresses the challenge of verifying real-world numerical claims, which are complex and often lack precise information, a gap not filled by existing works that mainly focus on synthetic claims. We evaluate and quantify these gaps in existing solutions for the task of verifying numerical claims. We also evaluate claim decomposition based methods, numerical understanding based natural language inference (NLI) models and our best baselines achieves a macro-F1 of 58.32. This demonstrates that QuanTemp serves as a challenging evaluation set for numerical claim verification.},
booktitle = {Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {650â€“660},
numpages = {11},
keywords = {claim decomposition, fact-checking, numerical claims},
location = {Washington DC, USA},
series = {SIGIR '24}
}

@inproceedings{QABriefs,
    title = "Generating Fact Checking Briefs",
    author = "Fan, Angela  and
      Piktus, Aleksandra  and
      Petroni, Fabio  and
      Wenzek, Guillaume  and
      Saeidi, Marzieh  and
      Vlachos, Andreas  and
      Bordes, Antoine  and
      Riedel, Sebastian",
    editor = "Webber, Bonnie  and
      Cohn, Trevor  and
      He, Yulan  and
      Liu, Yang",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    
    
    pages = "7147--7161",
    abstract = "Fact checking at scale is difficult{---}while the number of active fact checking websites is growing, it remains too small for the needs of the contemporary media ecosystem. However, despite good intentions, contributions from volunteers are often error-prone, and thus in practice restricted to claim detection. We investigate how to increase the accuracy and efficiency of fact checking by providing information about the claim before performing the check, in the form of natural language briefs. We investigate passage-based briefs, containing a relevant passage from Wikipedia, entity-centric ones consisting of Wikipedia pages of mentioned entities, and Question-Answering Briefs, with questions decomposing the claim, and their answers. To produce QABriefs, we develop QABriefer, a model that generates a set of questions conditioned on the claim, searches the web for evidence, and generates answers. To train its components, we introduce QABriefDataset We show that fact checking with briefs {---} in particular QABriefs {---} increases the accuracy of crowdworkers by 10{\%} while slightly decreasing the time taken. For volunteer (unpaid) fact checkers, QABriefs slightly increase accuracy and reduce the time required by around 20{\%}.",
}

@misc{aly2021feverous,
      title={FEVEROUS: Fact Extraction and VERification Over Unstructured and Structured information}, 
      author={Rami Aly and Zhijiang Guo and Michael Schlichtkrull and James Thorne and Andreas Vlachos and Christos Christodoulopoulos and Oana Cocarascu and Arpit Mittal},
      year={2021},
      eprint={2106.05707},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{claimdecomp,
      title={Generating Literal and Implied Subquestions to Fact-check Complex Claims}, 
      author={Jifan Chen and Aniruddh Sriram and Eunsol Choi and Greg Durrett},
      year={2022},
      eprint={2205.06938},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{schlichtkrull2023averitec,
  title={AVeriTeC: A dataset for real-world claim verification with evidence from the web},
  author={Schlichtkrull, Michael and Guo, Zhijiang and Vlachos, Andreas},
  journal={arXiv preprint arXiv:2305.13117},
  year={2023}
}

@inproceedings{thorne-etal-2018-fever,
    title = "{FEVER}: a Large-scale Dataset for Fact Extraction and {VER}ification",
    author = "Thorne, James  and
      Vlachos, Andreas  and
      Christodoulopoulos, Christos  and
      Mittal, Arpit",
    booktitle = "Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)",
    month = jun,
    year = "2018",
    address = "New Orleans, Louisiana",
    publisher = "Association for Computational Linguistics",
    
    
    pages = "809--819",
    abstract = "In this paper we introduce a new publicly available dataset for verification against textual sources, FEVER: Fact Extraction and VERification. It consists of 185,445 claims generated by altering sentences extracted from Wikipedia and subsequently verified without knowledge of the sentence they were derived from. The claims are classified as Supported, Refuted or NotEnoughInfo by annotators achieving 0.6841 in Fleiss kappa. For the first two classes, the annotators also recorded the sentence(s) forming the necessary evidence for their judgment. To characterize the challenge of the dataset presented, we develop a pipeline approach and compare it to suitably designed oracles. The best accuracy we achieve on labeling a claim accompanied by the correct evidence is 31.87{\%}, while if we ignore the evidence we achieve 50.91{\%}. Thus we believe that FEVER is a challenging testbed that will help stimulate progress on claim verification against textual sources.",
}

