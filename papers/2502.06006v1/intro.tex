\section{Introduction}
The rapid spread of information through digital media has increased the need for accurate, automated fact-checking to combat misinformation. Automated fact-checking systems verify claims by retrieving relevant evidence from vast, often unstructured web data~\cite{guo2022survey,schlichtkrull2023averitec}. A key challenge lies in the retrieval component, which traditionally prioritizes similarity. However, as misinformation becomes more complex, verifying multifaceted claims often requires reasoning from indirectly related evidence. For instance, validating a claim about vaccine safety may involve documents on production processes, clinical trials, or regulatory approvals. This complexity highlights the need for retrieval methods and benchmarks that go beyond traditional approaches.

Existing fact-checking benchmarks fall short of addressing real-world retrieval challenges. Popular datasets like FEVER~\cite{thorne-etal-2018-fever} and FEVEROUS~\cite{aly2021feverous} rely on synthetic claims, with retrieval limited to Wikipedia. While recent works such as ClaimDecomp~\cite{claimdecomp}, QABriefs~\cite{QABriefs}, and AVeriTeC~\cite{schlichtkrull2023averitec} incorporate fact-checks from professional fact-checkers, they constrain retrieval to pre-verified justification documents or lack dedicated retrieval corpora. This highlights the need for benchmarks that better simulate open-domain, real-world retrieval scenarios critical for advancing fact-checking systems.

In this paper, we present a novel retrieval benchmark rooted in real-world fact-checking data to address existing gaps. Our benchmark is derived from Factiverse production logs, incorporating evidence retrieval data and human annotations from an operational fact-checking system~\cite{10.1145/3626772.3657663,10.1145/3626772.3661361}. By leveraging data collected under genuine production conditions, we aim to provide a benchmark that accurately represents the unpredictable and multifaceted nature of real-world fact-checking. This dataset allows us to evaluate how retrieval models handle diverse information sources, incomplete knowledge contexts, partially relevant evidence, and claims that require subtle reasoning beyond simple factoid matching. Moreover, as most existing fact-checking systems rely on off-the-shelf retrievers pre-trained on datasets from other domains and used in a zero-shot setting~\cite{sriram-etal-2024-contrastive,gupta-etal-2022-dialfact}, it becomes essential to assess their generalization performance under these realistic constraints, where fine-tuning is often infeasible. In this paper, we adopt the same zero-shot setting to ensure our evaluation aligns with real-world usage scenarios.

Our contributions are threefold. First, we provide a real-world retrieval benchmark based on actual usage logs and human annotations, filling a critical gap in fact-checking research. Second, we conduct an extensive evaluation of state-of-the-art retrieval models (zero-shot), examining their ability to handle complex claims in authentic conditions. Finally, we offer insights and recommendations for designing retrieval systems that are not only effective in controlled settings but robust enough for real-world fact-checking, where the retrieval process must often synthesize indirect and inferential evidence. Through our work, we aim to foster the development of fact-checking systems that can better meet the challenges of todayâ€™s information ecosystem, supporting fact-checkers and automated systems alike in their mission to mitigate misinformation.
% \vspace{-0.6em}