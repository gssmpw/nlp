\section{Related Work}

Over the years, a range of benchmarks have been introduced to advance research in automated fact-checking. Early benchmarks such as FEVER~\cite{thorne-etal-2018-fever} and FEVEROUS~\cite{aly2021feverous} have been widely adopted, providing large-scale datasets for claim verification. These benchmarks include retrieval corpora but are based on synthetic claims generated from Wikipedia content, limiting retrieval to a constrained domain of Wikipedia articles. While these datasets have driven significant progress, they fail to reflect the complexity and diversity of real-world claims and evidence.

More recently, datasets like ClaimDecomp~\cite{claimdecomp} and QABriefs~\cite{QABriefs} have emerged, leveraging real-world fact-checks conducted by professional fact-checkers. These datasets introduce claim decomposition, where complex claims are broken down into sub-questions to facilitate evidence retrieval and verification. However, their retrieval process remains limited to pre-verified justification documents, which simplifies the task and does not simulate realistic retrieval challenges. Similarly, AVeriTeC~\cite{schlichtkrull2023averitec} incorporates human-authored questions paired with search-engine-retrieved answers, offering a step toward open-domain retrieval. Nevertheless, it lacks a dedicated retrieval corpus, restricting its ability to evaluate retrieval performance comprehensively.

Other benchmarks, such as QuanTemp~\cite{10.1145/3626772.3657874}, focus on numerical fact-checking and their primary focus is on numerical reasoning based verification, and retrieval is not directly evaluated. 

Despite these advancements, current benchmarks either rely on constrained retrieval settings or pre-verified evidence, failing to fully capture the challenges of open-domain, real-world retrieval where evidence must be identified from diverse, unstructured sources. Addressing these gaps is the primary goal of this paper.