\begin{table*}[!ht]
    \centering
    \small
    \begin{tabular}{lccccccccc}
    \toprule
     \textbf{Method}& \multicolumn{1}{c}{nDcG@5} &\multicolumn{1}{c}{Recall@5} &\multicolumn{1}{c}{nDcG@10} & \multicolumn{1}{c}{Recall@10} & \multicolumn{1}{c}{nDcG@100} & \multicolumn{1}{c}{Recal@100} \\
   % &\multicolumn{1}{c|}{n@10} & \multicolumn{1}{c}{RR}&\multicolumn{1}{c|}{n@10} & \multicolumn{1}{c}{RR}&\multicolumn{1}{c|}{n@10} & \multicolumn{1}{c}{RR}&\multicolumn{1}{c|}{n@10} & \multicolumn{1}{c}{RR} &\multicolumn{1}{c|}{n@10} & \multicolumn{1}{c}{RR} &\multicolumn{1}{c|}{n@10} & \multicolumn{1}{c}{RR} &\multicolumn{1}{c|}{n@10} & \multicolumn{1}{c}{RR} \\
     % & cover-EM & cover-EM& EM& EM&EM \\
     \midrule

     

    \midrule
    
    \textbf{Lexical} & & \\
            BM25 \cite{bm25} & 0.288 & 0.253 &0.345 & 0.421& 0.475&0.779 \\
          


     \midrule
     
      \textbf{Sparse} & & \\
            SPLADEV2 \cite{SPLADEv2} & 0.287 & 0.231& 0.336 & 0.384 & 0.473 & 0.783 \\
     \midrule
      \textbf{Dense} & & \\
            Stella-en-v5 & 0.090 & 0.065 & 0.098 & 0.110 & 0.158 & 0.314 \\
            DPR \cite{karpukhin-etal-2020-dense} & 0.247 &  0.219 & 0.292 & 0.356 & 0.404 & 0.670    \\
            ANCE \cite{ance} & 0.246 & 0.184  & 0.289 & 0.327 & 0.419 & 0.691\\
      


                  %      MDR & & \\
                        tas-b \cite{tas-b} & 0.289 & 0.232 & 0.336 & 0.399 & 0.468 & 0.771 \\
                        MPNet \cite{mpnet} & 0.290 & 0.250 & 0.327& 0.388 & 0.464& 0.767 \\
       Contriever \cite{contriever} & 0.299& 0.249 & 0.346 & 0.406 & 0.471 & 0.760\\
       % multilingual-minilm (Factiverse) & &  & 0.240 & 0.312 & 0.392 & 0.729 \\
       %        xlm-roberta (Factiverse) & &  & 0.271  & 0.319 & 0.415& 0.720 \\
    COlBERTV2 \cite{santhanam-etal-2022-colbertv2} & 0.252 &  0.230 & 0.325 & 0.419 & 0.465 & 0.789  \\
    Snowflake-arctic-embed-s \cite{merrick2024embeddingclusteringdataimprove} & \textbf{0.367} \up{27.43} & \textbf{0.302} \up{19.37} & \textbf{0.420} \up{21.74} & \textbf{0.480} \up{14.01} & \textbf{0.529} \up{11.37} & \textbf{0.795} \up{2.05} \\
\midrule
\textbf{Re-Ranker} \\
BM25 + & \\
 - ColBERTV2 &0.265 & 0.253 &  0.333 & 0.424 & 0.464 & 0.759 \\
- MARCO-MiniLM-H384 & 0.293 & 0.247     & 0.349 & 0.408 & 0.485 & 0.779\\

% - MSMARCO-electra-base  & 0.282 & 0.233& 0.327 & 0.379 & 0.473  & 0.779 \\
 - MARCO-MiniLM-en-de & 0.278 & 0.264 & 0.342 & 0.426 & 0.479  & 0.779 \\
- bge-reranker-base & 0.222 & 0.205 & 0.301 & 0.398 & 0.452 & 0.779\\
- bge-ranker-v2 & 0.252 & 0.235 & 0.312 & 0.399 & 0.460 & 0.779 \\
- Jina-reranker-v2 & 0.270 & 0.245 & 0.336 & 0.421 & 0.474 & 0.779\\
- gte-multilingual & \underline{0.308} \up{6.04}& \underline{0.277} \up{9.49} & \underline{0.368} \up{6.67} & \underline{0.437} \up{3.80}& \underline{0.496} \up{4.42} & \underline{0.779} \\

% Snowflake-arctic-embed-s &  \\
% -  MARCO-MiniLM-H384 \\
% -  gte-multilingual & 0.316 & 0.290 \\

\midrule
% \textbf{LLM+Retrieval} \\
%         Decompose-retrieve & &&&&& \\
%     \midrule

    % \textbf{Re-Ranking} & & & & & & & \\
    % BM25 + CE & & & & & & & \\
    % \bottomrule
%          \textbf{Semi-Oracle} & & & & & & & & & & \\
%         ClaimOnly & 33.03& 39.57& 36.31 & 58.15 & 33.81& 48.61& 25.70 & 23.99&28.55 & 63.79 & 7.95 & 33.42 &  43.70  \\


% \programfc{} & 38.57& 42.49 & 37.12  & 50.66& 35.22& 45.76& 33.43&32.50 & 32.95 & 55.11 & 25.44 & 37.83 & 43.79 \\
%     \claimdecomp{}&33.43 & 39.78 & 35.04&  55.49 & 33.93 & 48.53& 34.37& 33.50&29.48 & 63.11& 10.85 & 34.48& 44.19  \\
%      \numdecomp{}& 33.81& 39.46 &33.57 & 53.45& 34.18& 47.00& 35.23&34.23 &29.11 & 60.29 & 13.90 &34.43& 43.24\\
    \end{tabular}
    \caption{Retrieval results on \name{}, nDCG@10 across datasets. The best results are in bold and the second best results are underlined with \% improvements indicated by \up{} over the baseline BM25 being specified in brackets. }
     \vspace{-1em}
    \label{tab:main_result}
\end{table*}