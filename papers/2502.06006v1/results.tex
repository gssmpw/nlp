
\section{Results and  Analysis}
In this section, we analyze how retrieval and re-ranking approaches perform on the \name{} benchmark (Table \ref{tab:main_result}). 

\textbf{Lexical and Sparse retrievers}: We observe that lexical models like BM25 and learned sparse retrievers like SPLADEV2 perform surprisingly well on \name{} benchmark, as shown in Table \ref{tab:main_result}. They outperform several dense retrievers, proving to be strong baselines.

\textbf{Classical dense retrieval models fail on OOD data}: Since models like DPR, Contriever are frequently used in a zero-shot fashion for fact-checking tasks \cite{gupta-etal-2022-dialfact,sriram-etal-2024-contrastive}, we evaluate their performance on \name{}. We observe that all dense retrievers have sub-par performance when compared to BM25. This could primarily be caused by domain shift from pre-training data to claims in \name{}. Additionally, these models might also be incapable of handling such complex real-world claims as models like DPR are trained on datasets mostly containing factoid queries \cite{sriram-etal-2024-contrastive}.  We observe that DPR which has been trained on QA datasets performs worse than other dense retrievers like Contriever or tas-b which are pre-trained on MS-MARCO. We observe that Contriever is closer in performance compared to BM25 compared to other dense retrievers.


\textbf{Retrieval models with strong stratification based training objectives generalize better}:
From Table \ref{tab:main_result}, we observe that snowflake-arctic-embed-s outperforms lexical sparse and other state-of-the-art dense retrieval models  as measured by nDCG@k and Recall@k demonstrating better generalization performance. We hypothesize that this is primarily due to the semantic clustering based source stratification \cite{merrick2024embeddingclusteringdataimprove} based training objective. This objective is inspired by the cluster hypothesis \cite{clustering_hypothesis} and further builds upon the topic aware sampling philosophy of tas-b \cite{tas-b} and the hard negative mining aspect of ANCE \cite{ance} leading to superior performance. We posit that this training regime causes \textit{snowflake-arctic-embed-s} to retrieve topically and contextually relevant evidence for the claim while ignoring topically similar hard negatives. 


\textbf{LLM-Based Re-Rankers Show Superior Generalization}: After retrieving the top-100 documents using BM25, we re-rank them with a variety of models, as shown in Table \ref{tab:main_result}. Among these, the LLM-based re-ranker \textit{gte-multilingual-reranker-base} outperforms other cross-encoder models pre-trained on MSMARCO. We attribute this performance to the architecture of mGTE and the extensive pre-training of \textit{gte-multilingual} \cite{zhang-etal-2024-mgte} on data sourced from a wide range of tasks and domains, reducing source bias. This multi-domain pre-training enhances its generalization capabilities, enabling it to perform effectively across diverse retrieval scenarios.

\vspace{-1em}
\section{Conclusion}
In this work, we introduce \name{} for benchmarking of retrieval for fact-checking in an open-domain setup. \name{} is a collection of high quality real-world claims along with high quality relevance assessments from users. We evaluate a wide range of retrieval and re-ranking models and observe lexical and sparse retrievers to be strong baselines. We also observe that strong training objectives lead to impressive generalization performance. We also release our library which is easily extendable to new retrievers and re-rankers.
\section{Acknowledgements}
This work is partly funded by the Research Council of Norway project EXPLAIN (grant no:
337133).