\begin{figure}
    \centering
    \includegraphics[width=0.9\linewidth]{figures/factiverseeditor.png}
    \caption{Factiverse fact-check editor with feedback mechanism. Example for the claim ``Keto diet can cure cancer.''}
    \label{fig:fceditor}
    \vspace{-10pt}
\end{figure}

\section{Data Curation}
We employ the Factiverse Fact-Check Editor~\cite{10.1145/3626772.3657663}, a web-based tool designed for automated fact-checking with a built-in feedback mechanism. As shown in Figure \ref{fig:fceditor}, users input claims they wish to fact-check, and the system aggregates evidence retrieved from multiple search engines, such as Google and Bing, ensuring comprehensive coverage of web content. The editor utilizes query generation and evidence filtering to retrieve the most relevant documents~\cite{10.1145/3626772.3661361,10.1145/3627673.3679985}. Users can then provide detailed feedback on two aspects: the relevance of the evidence to the claim and the correctness of the stance, whether the claim is supported or refuted.

To ensure high-quality labels for this benchmark, we focus on feedback collected from domain experts, specifically professional fact-checkers and individuals with journalism backgrounds, who verify claims using Factiverse’s production deployment. The claims are organically generated by users as they engage in tasks covering a variety of topics, including politics, healthcare, and the economy. The feedback was collected over the period spanning 2021 to 2023.

The annotators were provided with the following instructions and examples: \textbf{Evidence is deemed relevant to a claim if the extracted snippet can help verify the claim’s veracity.} For example, for the claim ``Keto diet can cure cancer,'' a document discussing the general benefits of the ``keto diet'' without addressing its effect on cancer would be considered irrelevant. On the other hand, a scientific study examining the ``ketogenic diet as a treatment and prevention strategy for cancer'' (as illustrated in Figure~\ref{fig:fceditor}), even if it disproves the claim, would be considered relevant.

\begin{figure}
\begin{minipage}{0.22\textwidth}
\captionof{table}{Topical distribution}
\label{tab:topical}
\begin{tabular}{lr}
\hline
\textbf{Topic} & \textbf{Count}  \\

\midrule
Politics & 26 \\
Economy & 24 \\
Health & 21 \\
Law & 6 \\
Climate & 8 \\
Education & 3 \\
Other &  12\\

\bottomrule
\end{tabular}\end{minipage}
\begin{minipage}{0.22\textwidth}
\captionof{table}{Numerical  taxonomy}
\label{tab:taxonomy}
\begin{tabular}{lr}
\hline
\textbf{Topic} & \textbf{Count}  \\

\midrule
Statistical & 40 \\
Temporal & 27 \\
Comparative & 8 \\

\bottomrule
\end{tabular}
\end{minipage}

\end{figure}

\input{tables/retrieval_results}
\vspace{-0.6em}

\subsection{Benchmark Statistics}
 The benchmark comprises, \textbf{1413} claim-evidence  pair relevance annotations with a total of \textbf{90047} documents in the corpus collection and 100 claims with an average of \textbf{13.89} documents / relevance assesments per query. We provide fine-grained topical analysis in Table \ref{tab:topical}. We observe that a number of claims in the benchmark are \textit{quantitative or temporal} in nature, as shown in Table \ref{tab:taxonomy}. We also observe that many claims are \textit{compositional} comprising multiple aspects, making \name{} a challenging retrieval benchmark. We performed a qualitative meta-analysis of relevance assignments with the help of two researchers who were asked to annotate as ``1" when they deem the relevance assessment to be correct else ``0". The annotators found the relevance assessments to be of high quality (\textbf{88.03\%} was deemed to be correct) with a high agreement of \textbf{0.946} as indicated by Cohen's kappa.