% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
% \usepackage[review]{acl}
\usepackage[preprint]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{xspace}
\usepackage{subfigure}
% \usepackage{minted}
\usepackage{multirow}
\usepackage{cleveref}
\usepackage{amssymb}
\usepackage{booktabs} 
\usepackage{url}
\usepackage{lscape}
\usepackage{afterpage}
\usepackage{rotating}
% \usepackage[table,xcdraw]{xcolor}
\usepackage{ulem}
\usepackage{diagbox}
\usepackage{listings}

\lstset{
  frame=lines,                    % 绘制上下横线
  breaklines=true,                % 自动换行
  basicstyle=\scriptsize\ttfamily, % 设置字体大小为 \scriptsize，并使用等宽字体
  breakindent=0cm,                % 换行时不缩进
  postbreak=\mbox{}               % 换行符号为空（即不显示标记）
}



\newcommand{\framework}{\texttt{\textbf{DPT-Agent}}\xspace}



% If the title and author information does not fit in the area allocated, uncomment the following
%
% \setlength\titlebox{5cm}
%
% and set <dim> to something 5cm or larger.

% \title{Understand Human Policy: \\Leveraging Theory of Mind in LLM-based Agent \\for Real-time Human-AI Collaboration}
\title{Leveraging Dual Process Theory in Language Agent Framework \\for Real-time Simultaneous Human-AI Collaboration}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a separate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}






\author{
 \textbf{Shao Zhang\textsuperscript{1}\thanks{Equal Contribution}},
 \textbf{Xihuai Wang\textsuperscript{1}\footnotemark[1]\thanks{Work done while interning at Meituan.}},
 \textbf{Wenhao Zhang\textsuperscript{1}},
 \textbf{Chaoran Li\textsuperscript{1}}, 
\\
 \textbf{Junru Song\textsuperscript{1}},
 \textbf{Tingyu Li\textsuperscript{1}},
 \textbf{Lin Qiu\textsuperscript{2}},
 \textbf{Xuezhi Cao\textsuperscript{2}},
 \textbf{Xunliang Cai\textsuperscript{2}},
\\
 \textbf{Wen Yao\textsuperscript{3}},
 \textbf{Weinan Zhang\textsuperscript{1}},
 \textbf{Xinbing Wang\textsuperscript{1}},
 \textbf{Ying Wen\textsuperscript{1}\thanks{~Corresponding Author: \href{mailto:ying.wen@sjtu.edu.cn}{ying.wen@sjtu.edu.cn}. } }
\\
\\
 \textsuperscript{1}Shanghai Jiao Tong University,
 \textsuperscript{2}Meituan,
 \textsuperscript{3}Intelligent Game and Decision Laboratory
\\
 % \small{
 %   \textbf{Correspondence: Ying Wen} \href{mailto:ying.wen@sjtu.edu.cn}{ying.wen@sjtu.edu.cn}
 }

\begin{document}
\maketitle
\begin{abstract}


Agents built on large language models (LLMs) have excelled in turn-by-turn human-AI collaboration but struggle with simultaneous tasks requiring real-time interaction. Latency issues and the challenge of inferring variable human strategies hinder their ability to make autonomous decisions without explicit instructions.
Through experiments with current independent \textit{System 1} and \textit{System 2} methods, we validate the necessity of using Dual Process Theory (DPT) in real-time tasks.
We propose \framework, a novel language agent framework that integrates \textit{System 1} and \textit{System 2} for efficient real-time simultaneous human-AI collaboration.
\framework's \textit{System 1} uses a Finite-state Machine (FSM) and code-as-policy for fast, intuitive, and controllable decision-making. 
\framework's \textit{System 2} integrates Theory of Mind (ToM) and asynchronous reflection to infer human intentions and perform reasoning-based autonomous decisions.
We demonstrate the effectiveness of \framework through further experiments with rule-based agents and human collaborators, showing significant improvements over mainstream LLM-based frameworks.
\framework can effectively help LLMs convert correct slow thinking and reasoning into executable actions, thereby improving performance.
To the best of our knowledge, \framework is the first language agent framework that achieves successful real-time simultaneous human-AI collaboration autonomously. 
Code of \framework can be found in \url{https://github.com/sjtu-marl/DPT-Agent}.
\end{abstract}

\section{Introduction}

Large language models (LLMs) have revolutionized generalization capabilities and interaction methods, driving the application of human-AI collaboration in real-world tasks.
LLM-based agents have already been successfully applied to many collaborative tasks with humans, such as writing \cite{wan2024felt} and coding \cite{prather2024widening}, where humans and the agents interact turn-by-turn.
However, many collaborative tasks in shared workspaces require entities involved in the collaboration to cooperate simultaneously in the environment \cite{vildan2024auto,collaborationawareness1992}. 
Unlike turn-by-turn collaborative tasks, the simultaneous collaboration tasks that are time-sensitive require real-time responses to partners and interaction with the environment \cite{shao2024collaborative,gong2024mindagent}, as well as reasoning about dynamically changing human partners' strategies and environments \cite{wang2024zsc}.
Such simultaneous human-AI collaboration tasks present two challenges for LLM-based agents: \textbf{real-time responsiveness and autonomous collaboration adapted to humans}.

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/intro.pdf}
    \caption{How \framework Collaborates with Human Simultaneously.}
    \label{fig:intro}
\end{figure}

The real-time responsiveness issues faced by LLMs in inference time have been widely discussed.
Larger models with stronger reasoning capabilities often suffer from significant latency \cite{zhou2024survey}, making it difficult for them to respond quickly to dynamic changes in human interactions and environments in highly real-time scenarios.
The combination of fast and slow thinking using \textit{System 1} and \textit{System 2} based on Dual Process Theory (DPT) \cite{kahneman2011thinking,evans2013dual} has already been applied to address real-time issues via the combination of large and small models in language agent frameworks \cite{liu2024slow}.
However, this method still cannot resolve the contradiction between latency and performance fundamentally, as it uses small models as \textit{System 1}.


The agent frameworks designed for collaborating with humans also face challenges of insufficient autonomy and difficulty in adapting to human strategy variability.
Agents in the shared workspace tasks are regarded as independent collaborators joining the partnership \cite{dafoe2021cooperative}. 
However, most collaborative agent frameworks still require human input to output actions or strategies \cite{liu2024slow,guan2023efficient}, failing to collaborate with humans autonomously. 
Furthermore, humans in shared workspace tasks might perceive and engage with agents like how they interact with human partners for fostering collaboration like inferring agents' intentions to adjust strategies \cite{zhang2024mutual}, which further enhances the challenge of simultaneous human-AI collaboration.
Researchers also point out that LLMs are still limited in their ability to adapt to dynamic human strategy changes \cite{DBLP:conf/acl/ZhangTWW0HTLZ024}, making it difficult to transition reasoning into decision-making for effective adaptation \cite{riemer2024can}.

To address these challenges, we propose \framework, which leverages Dual Process Theory (DPT) to integrate FSM-based \textit{System 1} and LLM-driven \textit{System 2}, as shown in \Cref{fig:intro}.
Based on the intuitive thinking and fast decision-making characteristics of \textit{System 1}, we use a Finite-state Machine (FSM) for low-level action decision-making and execution, while employing a code-as-policy \cite{DBLP:conf/icra/LiangHXXHIFZ23} approach to enable \textit{System 2}'s slow thinking to guide and control fast decisions.
For slow thinking (\textit{System 2}), we design a Theory of Mind (ToM) mechanism for actively inferring human intentions and reflecting on environmental feedback based on how humans infer the partners and situations in shared workspace collaboration \cite{krych2007think}.
We also further improve the performance of the reflection mechanism with an asynchronous design to achieve better efficiency in self-evolution.


Building on the shared workspace task environment which is a hard version of Overcooked from \citet{zhang2024mutual}, we further develop a real-time simultaneous human-AI collaboration environment with new layouts and conduct multiple experiments in single agent setup, with rule-based agent and real humans.
We aim to understand:
1) \framework's capability in real-time tasks, 2) \framework's capability in collaboration, and 3) \framework's performance in collaboration with humans simultaneously.

In the experiments collaborating with rule-based agents, \framework outperforms strong language agent frameworks.
In reasoning models that suffer from extremely high latency due to long thinking processes, \framework framework further demonstrates its ability to effectively convert thinking into action and improve performance.
When collaborating with real humans, \framework also outperforms these baselines in both subjective and objective results, showing the significant improvement brought by asynchronous reflection and ToM module to infer humans. 

In summary, our contributions are as follows: 
\begin{itemize}
\vspace{-5pt}
    \item We experimentally analyze LLMs independently as \textit{System 1} and \textit{System 2} in real-time tasks, highlighting the challenge of the trade-off between performance and latency.
    \vspace{-5pt}
    \item We propose \framework that integrates FSM-based \textit{System 1} for fast and intuitive decision-making and LLM-driven \textit{System 2} for deliberate and analytical reasoning, effectively balancing latency and performance.
    \vspace{-5pt}
    \item We conduct extensive experiments with rule-based agents and human participants, demonstrating that \framework outperforms existing language agent frameworks in real-time simultaneous human-AI collaboration.
\end{itemize}

To the best of our knowledge, \framework is the first agent framework that can achieve successful real-time simultaneous human-AI collaboration autonomously in the hard version of Overcooked, which is one step closer to real-world application.



\section{Related Works}

\paragraph{Dual Process Theory (DPT).}
Dual Process Theory (DPT) \cite{evans2013dual} refers to human cognition operates through two distinct systems: \textit{System 1}, which is fast, automatic, and intuitive, and \textit{System 2}, which is slower, deliberate, and analytical \cite{kahneman2011thinking}.
DPT explains how humans think during the perception-decision process. 
The ability to effectively integrate \textit{System 1} and \textit{System 2} helps humans accomplish complex perception and decision-making tasks.
Numerous LLM-based reasoning frameworks also utilized DPT to facilitate human-related interactions like dialogue \cite{he-etal-2024-planning} and mitigate latency issues via using a small model as \textit{System 1} \cite{liu2024slow}.
Many current agent frameworks use \textit{System 2}-based approaches to assist with planning and decision-making \cite{yu2024distilling,DBLP:conf/acl/ZhangTWW0HTLZ024}, such as chain-of-thought (CoT) \cite{wei2022chain}, ReAct \cite{yao2022react}, and Reflexion \cite{shinn2024reflexion}.
\framework is inspired by DPT, further alleviating latency issues in \textit{System 1} and endowing the agent with greater autonomy and adaptability to humans in the design of \textit{System 2}.


\paragraph{Simultaneous Human-AI Collaboration.}
Most tasks related to LLMs in human-AI collaboration research pose lower demands on real-time responsiveness, such as task-oriented dialogue systems \cite{yi2024survey} and word-guessing \cite{zahra2021direction}, where players take actions turn-by-turn.
However, collaborative tasks in the real world are often simultaneous, requiring real-time reasoning, which presents latency challenges for many LLM-based frameworks \cite{DBLP:conf/icra/LiangHXXHIFZ23}.
Another significant challenge of simultaneous collaborative tasks is adapting to humans, who are unfamiliar partners not encountered during training \cite{wang2024zsc,Yang23Cole,carroll2019utility,zhang2024proagent}. 
Theory of Mind (ToM) \cite{neil2018tom,baron1985does} has been introduced to enhance reasoning in human-AI collaborative scenarios \cite{wester2024theory}.
However, studies have pointed out that LLMs fail to achieve functional ToM \cite{riemer2024can}, where reasoning cannot be effectively implemented in decision-making processes.
To adapt to humans, \framework integrates DPT and ToM to support the entire process from perception to reasoning and decision-making, achieving functional ToM while ensuring real-time performance.

\section{Why We Need Dual Process Theory?}

To understand the necessity of DPT in real-time simultaneous human-AI collaboration, we first examine the real-time responsiveness and task completion capabilities of using large language models (LLMs) independently as \textit{System 1} and \textit{System 2} agents.

In the Overcooked environment \citep{zhang2024mutual}, we employ a single-agent setup, \textit{Counter Circuit} (shown on the left in \Cref{fig:overcookedlayout}), to compare the performance of typical LLM-based \textit{System 1}-only agents using mainstream LLMs of varying sizes with that of an FSM-based agent. Additionally, we include the DeepSeek-R1 series reasoning model \cite{guo2025deepseek} and OpenAI's o3-mini, which incorporate \textit{System 2} capabilities with long CoT as agents for this task.

To evaluate the performance of the \textit{System 1}-only agents in real-time task completion, we assess action output latency, task score, and score efficiency. Each model is evaluated over 20 runs, using the same game introduction prompt (\Cref{app:gameprompt}), instruction prompt (\Cref{app:instruct}), and output prompt (\Cref{app:act-prompt}).

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/exp1-1.pdf}
    \caption{\textbf{LLM as Independent \textit{System 1} and \textit{System 2} in Overcooked.} Mean score means the inter-quartile mean score of 20 games. We define score efficiency as the average score gained per macro action. The size of each model's circle represents latency, which is the time taken from the request to the output of a macro action. }
    \label{fig:exp1}
\end{figure}

As shown in \Cref{fig:exp1}, with detailed data provided in \Cref{app:exp1}, as independent \textit{System 1}, models with fewer than 20B parameters excel in latency but often have near-zero score efficiency, indicating fast responses but ineffective actions.
Since missed orders lead to score deductions, some high-score-efficiency models with high latency still score below zero.
The models that can balance capability in generating scoring actions with low latency perform better.
When the reasoning models use long CoT as the \textit{System 2}, despite their stronger reasoning capabilities, their performance presents even lower score efficiency and overall scores compared to many smaller models functioning as \textit{System 1}.
Additionally, all agents perform worse than the FSM agent.

These results show that LLM-based independent \textit{System 1} and \textit{System 2} agents struggle with low-latency models lacking capability and high-capability models suffering from excessive latency.
This phenomenon highlights the need for a framework to integrate \textit{System 1} and \textit{System 2}, balancing capability and latency in real-time tasks.


\section{\framework Framework}
To enable real-time responsiveness and seamless autonomous collaboration that aligns with human cognitive processes, we propose \textbf{D}ual \textbf{P}rocess \textbf{T}heory \textbf{Agent} framework (\framework). 
\framework integrates both \textit{System 1}, which facilitates fast, intuitive decision-making, and \textit{System 2}, which supports deliberate, analytical reasoning. 

\paragraph{Formulation.} We model real-time simultaneous human-AI collaboration as a two-agent decentralized Markov decision process (DEC-MDP) \cite{bernstein2002complexity}. The framework is defined by the tuple $\langle\mathcal{S}, \{\mathcal{A}^i\}, \{\mathcal{A}^h\}, \rho, \mathcal{P}, r\rangle$ where $\mathcal{S}$ is the state space, $\mathcal{A}^i$ and $\mathcal{A}^h$ denote the agent's and human's action spaces, $\rho:\mathcal{S}\to[0,1]$ is the initial state distribution, $\mathcal{P}:\mathcal{S}\times\mathcal{A}\times\mathcal{S}\to[0,1]$ governs transitions with $\mathcal{A} = \mathcal{A}^i \times \mathcal{A}^h$ as the joint action space, and $r:\mathcal{S}\times\mathcal{A}\to\mathbb{R}$ is the reward function. At each timestep $t$, the agent executes $a^i_t \in \mathcal{A}^i$ while the human performs $a^h_t \in \mathcal{A}^h$ simultaneously, inducing the joint action $a_t = (a^i_t, a^h_t)$ that drives state transitions through $\mathcal{P}(s_{t+1}|s_t, a_t)$. We further develop modular formulations for \framework in the following sections.

\begin{figure*}
\vspace{-10pt}
    \centering
    \includegraphics[width=\linewidth]{figures/framework.pdf}
    \caption{\textbf{\framework Framework.} 
 In \textit{System 2}, the historical states from the history buffer periodically trigger the ToM module to infer human behaviors. The reflection module then analyzes the belief output from the ToM module, along with game score feedback and other historical state information, to summarize its own behaviors and generate guidelines. 
Within \textit{System 1}, the code-as-policy generator utilizes the current state, belief and guidelines to generate code-as-policy when necessary, enabling control over the FSM. When no specific input is provided, the FSM continues operating autonomously, generating macro actions to ensure the agent maintains continuous action output, thereby guaranteeing real-time responsiveness in simultaneous collaboration.}
    \label{fig:framework}
\end{figure*}

\subsection{\textit{System 2}: Deliberate and Analytical Reasoning}

When facing complex situations, humans often rely on \textit{System 2} to process large amounts of information to aid decision-making. 
Inspired by this process, we designed \textit{System 2} for \framework, integrating environmental feedback for Theory of Mind and self-evolution-based inference, which aims to enable advanced reasoning and planning while dynamically adapting to human partners.
We also refine the reflection mechanism \cite{shinn2024reflexion} by using asynchronous reflection to facilitate efficient and flexible self-evolution of strategies.

\subsubsection{Theory of Mind for Inferring Human }

Equipped with the Theory of Mind (ToM) capability, individuals can infer others' mental states as beliefs by analyzing their actions and communication history, allowing them to understand and anticipate their behaviors \cite{premack1978does}.
In the context of ToM, belief refers to an individual's perception of events, which subsequently shapes their actions \cite{baron1985does,neil2018tom,wen2018probabilistic}.
We develop a Theory of Mind module that enables the agent to construct a belief about the human, encompassing aspects such as tendencies, conventions, and plans, based on observed human behaviors.
The belief output from the ToM module influences the strategy by guiding both the strategy reflection in \textit{System 2} and the decision-making in \textit{System 1}.

To formulate the ToM process, we denote the history from time-step $0$ to time-step $t$ of the game that the agent perceives as $\tau_{0:t} = \{(s_{0}, a_{0}^{i}, a_{0}^{h}, r_{0}), \ldots, (s_{t}, a_{t}^{i}, a_{t}^{h}, r_{t})
\}$.
The Theory of Mind module takes in the history $\tau_{0:t}$, summarizes the history, infers the conventions and tendencies of the human, and explains how the agent's policy can be adjusted to coordinate better with the human player. 
The Theory of Mind module outputs the belief in natural language, as shown in \Cref{fig:framework}. 
The $n$-th ToM process execution can be formalized as $b^{n} = \text{LLM}\left(\tau_{0:t_{n}}, b^{n-1}\right)$, where $b^{n-1}$ is the $n-1$-th generated belief and $t_{n}$ is the time-step when the $n$-th belief inference is performed.

\subsubsection{Asynchronous Reflection for Self-evolution}
The Asynchronous Reflection module enables the agent to improve its policy in such a long-horizon interaction process for higher performance.
We design the ``Behavior Guideline,'' where the agent maintains and iteratively updates language guidelines for the self-evolution of the current policy, based on the generated belief about the human partner and the game history.
The Asynchronous Reflection module proceeds asynchronously with the decision-making process and allows real-time responsiveness to be handled by \textit{System 1}, enabling the reflection process to focus on thinking without worrying about decision delays, thus facilitating more thorough self-evolution.
The $m$-th Reflection process execution can be formalized as $g^{m} = \text{LLM}\left(\tau_{0:t_{m}}, b^{n},g^{m-1}\right)$, where $b^{n}$ is the latest inferred belief about human, $g^{m}$ is the ``Behavior Guideline'' that is updated $m$ times.

Given the modular formulation of the ToM and Asynchronous Reflection modules, we derive the formulation of the whole \textit{System 2} process as a policy $\pi^{\text{S2}}: \mathcal{T} \times \mathcal{B} \times \mathcal{G} \mapsto \mathcal{B} \times \mathcal{G}$, where $\mathcal{T} = \left\{ \tau_{0:t} = (s_0, a_0, \dots) \mid s_t \in \mathcal{S}, a_t \in \mathcal{A}, t = 0, \dots \right\}$ is the space of the game history. 
The \textit{System 2} policy $\pi^{\text{S2}}$ iteratively updates the belief about the human player and the behavior guidelines given the game history, which can be denoted as $b^{n}, g^{m} = \text{LLM}(\tau_{0, \max(t_n, t_m)}, b^{n-1}, g^{m-1})$.

\subsection{\textit{System 1}: Fast and Intuitive Decision Making}

In time-sensitive tasks, humans typically rely on \textit{System 1} to make intuitive decisions without engaging in complex reasoning and keep asynchronous reasoning while taking action. 
Inspired by this process, we implement \textit{System 1} in \framework by combining a code-as-policy generator and Finite-state Machine (FSM)  to enable intuitive and rapid decision-making.
The code-as-policy approach also establishes a decision pipeline from \textit{System 2} to \textit{System 1}, which allows \textit{System 2} to influence and refine actions.


\subsubsection{Code-as-Policy Generator}

To enhance the performance of the agent, we designed the code-as-policy generator to effectively bridge the gap between \textit{System 2}'s guidelines and inferred beliefs, and \textit{System 1}'s rapid decision-making. 
By incorporating \textit{System 2}'s reasoning into the decision pipeline, we ensure that the agent can leverage \textit{System 2}'s reasoning abilities to gradually transform \textit{System 2}'s inferences into actionable decisions within an episode. 

The Code-as-policy generator takes in the history, guidelines and inferred beliefs, and outputs executable code that consists of task-completing rules and modifies the logic of the Finite-state Machine, which is detailed in \cref{sssec: fsm}.
This process allows \textit{System 1} to refine its intuitive responses with thoughts derived from \textit{System 2}, thus enhancing the agent's overall decision-making capabilities in dynamic environments.

The policy generation process of Code-as-policy generator at time-step $t$ can be formalized as $c_{t} = \text{LLM}\left(\tau_{t-\lambda:t}, b^{n}, g^{m}\right)$, where $b^{n}$ and $g^{m}$ represents the latest belief about human and the latest guidelines respectively, and $\lambda$ is the interval the Code-as-policy generator executes.
% which is $25$ in our experiment.



\subsubsection{Finite-state Machine \& Action Executor}\label{sssec: fsm}

To implement rapid response in \textit{system 1}, we adopt the Finite-state Machine (FSM) method \citep{russell2016artificial}, which is a widely used computational model that enables structured and efficient decision-making by transitioning between pre-defined states based on inputs. 
In \framework, we leverage FSM to facilitate fast and intuitive decision-making by defining each state as a specific agent context or situation. 
State transitions are triggered by environment dynamics, allowing the agent to adapt efficiently without relying on external LLM responses. 

When LLM generates code-as-policy, the executable code changes the pre-defined logics of FSM and thus facilitates the adaption to human and performance improvement.
The FSM takes in the code-as-policy and game states, and outputs macro actions, denoted as $ma$, which are high-level combinations of atomic actions for specific targets.
For example, in Overcooked, macro actions include food ingredients preparation, food assembling and food serving.  
The generated macro actions are sent to an action executor for conversion into atomic actions that can be directly executed in the environment. 
The action executor employs script policies, ensuring smooth and efficient execution.
Upon receiving a macro action, the action executor selects an appropriate execution plan and performs path planning to determine the necessary atomic actions using the A* algorithm \cite{hart1968formal}. 
The Detailed design and implementation of the FSM is provided in \Cref{app:fsm}.
These processes can be formalized as $ma_t = \text{FSM}(c_t, s_t)$ and $a_{t}^{i} = \text{Executor}\left(ma_{t}\right)~$.

Given the formulation of these modules, we derive the formulation of the whole \textit{System 1} process as $\pi^{\text{S1}}: \mathcal{S} \times \mathcal{B} \times \mathcal{G}\mapsto \mathcal{A}$.
At time-step $t$, $\pi^{\text{S1}}$ generates executable atomic action $a_t = \text{Executor}(\text{FSM}(\text{LLM}(\tau_{t-\lambda}, b^{n}, g^{m}), s_t))$.


\section{Experimental Design}

In this section, we introduce the new real-time simultaneous human-AI collaboration environment and tasks we designed based on \citet{zhang2024mutual} and our experimental setup.
Specifically, we aim to understand: 1) \framework's capability in real-time tasks, 2) \framework's capability in collaboration, and 3) \framework's performance when collaborating with humans simultaneously.


\subsection{Overcooked Challenge for Real-time Simultaneous Human-AI Collaboration}
To effectively evaluate the performance of \framework in real-time simultaneous human-AI collaboration, we implement the real-time shared workspace environment proposed by \citet{zhang2024mutual}, using a challenging version of Overcooked based on the original Overcooked game \cite{carroll2019utility,strouse2021fcp,Yang23Cole,li2024tackling,yu23hsp,gymcooking}. In our experiments, we introduce a new layout.
As shown in \Cref{fig:overcookedlayout}, we adopt the basic layout, referred to as \textit{New Counter Circuit}, from \citet{zhang2024mutual} and design a new layout, named \textit{New Asymmetric Advantages}, building on the original Overcooked AI environment \cite{carroll2019utility}. The implementation is based on the gym-cooking environment \cite{gymcooking}.
In the real-time settings, each timestep corresponds to 0.25 seconds in the real world. 
Time-sensitive elements within the environment, such as overcooked beef and expiring orders, underscore the importance of timely task execution. 
Additionally, layout conflicts and the complexity of the burger-making process emphasize the critical role of collaboration. Further details about the environment and tasks can be found in \Cref{app:env}.


\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/env.png}
    \caption{\textbf{Two Layouts in Overcooked Challenge for Real-time Simultaneous Human-AI Collaboration.} Left is Map 1 - \textit{New Counter Circuit} with brief introduction of the item and game mechanism. Right is Map 2 - \textit{New Asymmetric Advantages} }
    \label{fig:overcookedlayout}
\end{figure}

\begin{figure*}[htp]
    \centering
    \subfigure[ReAct.]{
    \includegraphics[height=0.22\linewidth]{figures/exp1-2-react.pdf}
    \label{fig:exp1-2react}
    }
    \subfigure[Reflexion.]{
    \includegraphics[height=0.22\linewidth]{figures/exp1-2-reflexion.pdf}
    \label{fig:exp1-2refexion}
    }
    \subfigure[\framework w/o ToM.]{
    \includegraphics[height=0.22\linewidth]{figures/exp1-2-dpt.pdf}
    \label{fig:exp1-2dpt}
    }
    \caption{\textbf{Results of LLM with ReAct, Reflexion and DPT-Agent w/o ToM in the Single Agent Game.} }
    \label{fig:exp1-2}
\end{figure*}

\subsection{Experimental Setup}

Based on the Overcooked challenge, we set up three series of experiments to validate the effectiveness of \framework using the commonly adopted ReAct \cite{yao2022react} and Reflexion \cite{shinn2024reflexion} framework.
We first compare \framework with baselines in a single agent setting to understand the \framework's capability of a real-time task.
Next, we use three specialized rule-based agents as partners to evaluate the simultaneous collaboration capability of \framework. 
Finally, we conduct human-involved experiments to compare baseline frameworks with \framework in collaboration with real humans.
The baseline frameworks in experiments are implemented in a manner that ensures a fair comparison via using the same output way of code-as-policy with \framework.
Based on this implementation, the ReAct and Reflexion become \textit{System 1} + \textit{System 2} frameworks.
The implementation details can be found in \Cref{app:imple,app:baselines,app:gameprompt}.
All the open-source models used in experiments are deployed locally with NVIDIA A800-SXM4-80GB and NVIDIA H100-80GB-HBM3 for the best latency performance.
Model deployment details can be found in \Cref{app:exp1,app:exp2}.
For close-source models, we use the original API.
All the models' temperature is set to 0.
The whole experiment cost 517.5 A800 GPU hours, 228 H100 GPU hours and \$735 in API in total.
For reliability, all the experiments are repeated 20 runs and reported as the inter-quartile mean and the standard error.
The details of the metrics used in experiments can be found in \Cref{app:metrics}.



\paragraph{Capability in Real-time Task.}
We first consider the real-time performance and task completion capability of \framework in a single agent setting.
In the single-agent setup \textit{Counter Circuit}, we compare the ReAct and Reflexion implemented as \textit{System 1} + \textit{System 2} frameworks with \framework w/o ToM in score, latency and score efficiency.

\paragraph{Capability in Simultaneous Collaboration.}
Expanding on the previous experiment, we use rule-based agents as partners to evaluate the performance of \framework in simultaneous collaboration tasks. We employ three specialized rule-based agents: one for beef preparation, one for lettuce preparation, and one for burger assembly.
In Map 1, we compare ReAct and Reflexion implemented as \textit{System 1} + \textit{System 2} frameworks and \framework w/o ToM with \framework driven by 11 high-performing LLMs on the same map as the previous experiment in a two-player setting. 






\paragraph{Real-time Simultaneous Collaboration Experiments with Human.}
To evaluate \framework's capabilities of collaborating with humans, we conduct experiments with 71 university students. To balance response latency and capability, all frameworks are powered by GPT-4o-mini. We enhance all baselines by incorporating an FSM-based \textit{System 1} and perform an ablation study to assess the ToM module's impact.
We compare ReAct + FSM-based System 1, Reflexion + FSM-based System 1, \framework, and \framework w/o ToM on two cooperative two-player maps. Participants are split into two groups, each playing on a different map. Within each group, every participant plays two games, each lasting 500 timesteps, with each of the four agents in random order.
We also collect subjective human preferences. 
Detailed participant demographics, experiment implementation, instructions, recruitment, and payment information are provided in \Cref{app:humanexp}.


\section{Results}
In this section, we present the results of experiments and analyze \framework's effectiveness in real-time simultaneous human-AI collaboration. 

\subsection{Capability in Real-time Task}

As shown in \Cref{fig:exp1-2react,fig:exp1-2refexion} (detailed data in \Cref{app:exp1}), under ReAct and Reflexion framework, the score efficiency of most models has significantly improved compared with when they functioned as independent \textit{System 1} (\Cref{fig:exp1}).
However, the score of many models has declined with an increase in latency due to more complex \textit{System 2} reasoning. 
Low-latency models, like Qwen2.5-14b, still struggle with capability issues, failing to achieve higher final scores despite good score efficiency. 
Further comparison of the performance of \framework in \Cref{fig:exp1-2dpt} reveals that inference models with high latency and larger models get a significant improvement. \framework can help these high latency models convert the high score efficiency and reasoning capability to scores, which demonstrates the effectiveness of \framework in real-time tasks.






\begin{table*}
\vspace{-5pt}
\resizebox{\linewidth}{!}{
\begin{tabular}{lcccccccc}
\toprule
\multicolumn{1}{c}{\multirow{3}{*}{\diagbox[width=3.5cm]{\textbf{Model}}{\textbf{Framework}}}} &
  \multicolumn{4}{c}{\textbf{Score}} &
  \multicolumn{4}{c}{\textbf{Agent Contribution Rate}} \\
  \cmidrule(lr){2-5}
  \cmidrule(lr){6-9}  
 & \multirow{2}{*}{ReAct} & \multirow{2}{*}{Reflexion} & \framework & \multirow{2}{*}{\framework}     & \multirow{2}{*}{ReAct}   & \multirow{2}{*}{Reflexion}     & \framework    & \multirow{2}{*}{\framework}   \\
                &     &        & w/o ToM   &     &    &      &  w/o ToM   &    \\
\midrule
\textbf{o3-mini-high}    & -43.00(0.93)	&-42.00(0.85)&\textbf{65.83(5.66)}&	\uline{55.17(4.84)} &0.00(0.00)&	0.00(0.00)&	0.68(0.02)&	\textbf{0.72(0.01)}   \\			
\textbf{o3-mini-medium}    &-10.00(6.94)	&4.83(7.63)	&\uline{56.50(7.07)}&	\textbf{60.00(6.07)} &0.60(0.05)&	0.62(0.02)&	0.56(0.04)&	\textbf{0.68(0.03)}   \\
\textbf{o3-mini-low}    & 7.00(7.491)	&33.50(7.06)	&\uline{44.83(9.74)}&	\textbf{51.33(8.67)} &0.60(0.05)&	0.62(0.02)&	0.56(0.04)&	\textbf{0.68(0.03)}   \\
\textbf{GPT-4o }     & 35.67(9.62)&\uline{39.17(8.43)}&18.67(8.50)&\textbf{39.50(8.63) } &0.60(0.02)&	0.61(0.02)&	0.60(0.05)&	\textbf{0.69(0.04) }\\
\textbf{GPT-4o-mini}   & -6.58(5.37)&5.58(7.53)&\uline{50.00(5.27)}&\textbf{52.92(6.34)}  &0.27(0.07)&	0.46(0.06)&	0.66(0.02)&	\textbf{0.67(0.02) } \\
\textbf{Qwen-Max}   &30.50(6.58)	&21.17(6.23)&	\uline{51.50(9.27)}	&\textbf{53.83(7.33)}  &0.59(0.03)&	0.60(0.03)&	0.68(0.04)	&\textbf{0.70(0.03)} \\
\textbf{Claude 3.5 Haiku}     & 29.50(5.63)	&24.83(6.58)	&\textbf{43.17(8.01)}	&\uline{41.50(7.69)}  &0.62(0.04)&	0.58(0.03)&	0.67(0.03)&	\textbf{0.70(0.03)}\\
\midrule
\textbf{DeepSeek-R1-671b}     &20.67(5.47)	&21.00(6.83)&\uline{56.67(5.13)}	&\textbf{74.33(5.33) }& 0.61(0.01)&	0.59(0.01)&\textbf{0.69(0.02)}&	\textbf{0.69(0.01)}\\
\textbf{DeepSeek-R1-70b}     &33.83(6.77)&-2.67(5.98)&\uline{51.00(6.08)}&\textbf{61.50(6.40)} &0.57(0.01)&	0.55(0.05)	&\textbf{0.69(0.02)	}&0.66(0.02)\\
\textbf{DeepSeek-R1-32b}     &37.33(8.51)&23.33(7.46)&\textbf{45.50(6.39)}&\uline{38.83(8.51)} &	0.56(0.02)&	0.53(0.03)	&0.67(0.02)&	\textbf{0.69(0.05)}\\
\textbf{DeepSeek-R1-14b}     &-8.50(3.88)&12.00(8.51)&\uline{40.33(7.73)}&\textbf{43.17(8.54)} &0.52(0.02)	&0.48(0.02)	&0.68(0.03)&\textbf{0.71(0.03)}\\
\textbf{DeepSeek-V3 }  &29.17(8.24)&	33.33(7.76)&	\textbf{70.33(5.28)}	&\uline{61.83(5.86)}&   0.60(0.03)	&0.58(0.02)&	\textbf{0.74(0.01)	}&\textbf{0.74(0.02)}  \\
\textbf{DeepSeek-V2.5}   &	-6.00(5.23)	&12.33(4.83)&\textbf{ 31.50(6.58)}&	\uline{23.50(8.44)}&0.25(0.02)	&0.47(0.04)&\textbf{0.64(0.04)}&	0.60(0.04)	\\
\textbf{QwQ-32b}      & 8.80(1.04)	&7.28(2.47)&\textbf{14.50(3.04)}	&\uline{12.67(1.21)}	 &	0.58(0.03)	&0.00(0.03)	&0.64(0.02)	&\textbf{0.70(0.02)} \\
\textbf{Qwen2.5-72b}      & 18.03(4.69)&\textbf{48.67(5.68)}&18.83(5.51)&\uline{32.08(5.17)} &	\textbf{0.75(0.01)}&	0.58(0.01)	&0.67(0.04)&	0.67(0.03)  \\
\textbf{Llama3.3-70b}      & 27.97(5.68)&-15.58(5.28)&\textbf{30.75(3.86)}&\uline{28.08(6.68)}    &0.74(0.03)&	0.54(0.05)&	\textbf{0.85(0.02)}	&0.75(0.05)  \\
\textbf{Mixtral-8x22b}    & 20.17(6.30)	&\uline{24.67(6.07)}&	24.00(6.10)	& \textbf{26.83(5.79)}&  0.54(0.03)&	0.54(0.03)	&\textbf{0.70(0.06)	}&0.60(0.03)  \\
\midrule
\midrule
\textbf{Overall}     & 11.77(6.31)	&15.48(6.16)	&\uline{44.23(6.60)}	&\textbf{46.63(6.88)	}  	&0.52(0.03)&	0.49(0.03)
&0.68(0.03)&	\textbf{0.69(0.03)}	 \\
\bottomrule
\end{tabular}
}
\caption{\textbf{Performance with Standard Errors of Experiments Collaborating with Rule-based Agents.}}\label{tab:exp2}
\vspace{-10pt}
\end{table*}










\begin{table}[htp]
\centering
\resizebox{\linewidth}{!}{
\begin{tabular}{@{}lcccc@{}}
\toprule
\multicolumn{5}{c}{\textbf{Map 1 - \textit{New Counter Circuit}}}                                                                               \\ \midrule
\textbf{Frameworks} & ReAct          & Reflexion         & \framework w/o ToM     & \framework \\ \midrule
\textbf{Mean Score} & 99.03(9.86)  &    97.78(7.23)    &      103.19(7.06)     &   \textbf{111.53(5.42)}       \\
\textbf{Agent CR} & 0.51(0.03)  &    0.53(0.03)    &     0.62(0.02)     &   \textbf{0.62(0.02)}   \\
\bottomrule \toprule
\multicolumn{5}{c}{\textbf{Map 2 - \textit{New Asymmetric Advantages}}}                                                                               \\\midrule
\textbf{Frameworks} &  ReAct          & Reflexion         & \framework w/o ToM     & \framework \\\midrule
\textbf{Mean Score} &115.00(9.28)  &   119.67(10.54)  &      152.03(8.13)     &   \textbf{160.63(7.97)}       \\ 
\textbf{Agent CR} & 0.49(0.04)  &    0.51(0.03)    &     \textbf{0.62(0.02)}     &   0.59(0.03)  \\
\bottomrule
\end{tabular}
}
\caption{\textbf{Performance with Standard Errors of Experiments with Humans.} Agent CR refers to Agent Contribution Rate.} 
\label{tab:exp3}
\vspace{-10pt}
\end{table}


\subsection{Capability in Simultaneous Collaboration}

As shown in \Cref{tab:exp2}, \framework achieved the best performance across the majority of models, especially on the widely recognized general-purpose SOTA models like GPT-4o. 
This phenomenon aligns with the conclusions from the experiments in single-agent settings, where larger models can overcome the latency limitations and achieve better performance with the help of \framework. 
Such performance improvements are more noticeable in the reasoning model series of GPT o3-mini and DeepSeek-R1.
\framework framework can help reasoning models, which require long periods of thinking, overcome the latency and effectively transition from thinking to action.
Additionally, when facing rule-based agents that can only perform a single task, \framework can maintain a high contribution rate.
For some models like Llama3.3-70b, \framework w/o ToM outperforms the complete \framework, which may be closely related to the model's ToM capabilities. 
We provide detailed results and case analyses of different partners in \Cref{app:case-study}.





\subsection{Experiments with Real Humans}

After data validation, we have 68 valid data points in total: 36 of Map 1 and 32 of Map 2.
The data validation details are in \Cref{app:humanexp}. 
As shown in \Cref{tab:exp3}, \framework achieves the highest scores in both Map 1 and Map 2 when collaborating with humans. 
\framework w/o ToM also outperforms ReAct and Reflexion, confirming the effectiveness of asynchronous reflection.
Moreover, the ToM module also brought a significant score improvement in collaborating with humans, confirming that incorporating human belief reasoning into \textit{System 2} can foster better collaboration.
Regarding human perception (\Cref{tab:human-rank}), \framework ranks highest in Map 1, with the most participants recognizing its collaborative abilities. 
Interestingly, in Map 2, \framework w/o ToM surpasses \framework in both cooperation and preference ranking with a higher agent contribution rate, which may refer to the human preference for partners who work more.







\begin{table}[htp]
    % \vspace{-10pt}
    \centering
    \resizebox{\linewidth}{!}{
\begin{tabular}{cccccc}
\toprule
  \multirow{2}{*}{\textbf{Layouts}} & \multirow{2}{*}{\textbf{Perception}} & \multirow{2}{*}{ReAct} & \multirow{2}{*}{Reflexion} & \framework& \multirow{2}{*}{\framework}\\
& &&&w/o ToM &\\
\midrule
\multirow{2}{*}{\textbf{Map 1} }       & Cooperation     & \uline{88} & 79 &86 & \textbf{107} \\
\cmidrule{2-6}
     &Preference &  88   & 80  &  \uline{91}  & \textbf{101}  \\
\midrule
\multirow{2}{*}{\textbf{Map 2} }    & Cooperation            & 65 & 78 & \textbf{94} & \uline{83} \\
\cmidrule{3-6}
 &Preference     &      63  &   73  & \textbf{95}       &  \uline{89}   \\
\bottomrule
\end{tabular}
        }
          
        \caption{\textbf{Borda Count Result of Humans' Perceived Subjective Ranking.} In our Borda count, the first place receives 4 points, and each subsequent rank decreases by one point.}
        \label{tab:human-rank}
  \vspace{-10pt}
\end{table}
\section{Discussion and Future Works}



The experiment results illustrate the complex interplay between latency, capability, and collaboration in real-time tasks. 
\framework shows the capability to address this issue by effectively balancing latency and capability, enabling high-latency models to convert score efficiency and reasoning capability into better outcomes.
Moreover, the significant improvement observed when incorporating ToM into \framework during human collaboration confirms the value of human-like reasoning in enhancing task performance. 
This insight emphasizes the importance of integrating cognitive abilities, like ToM, to optimize human-agent interactions in real-world applications.
Interestingly, the absence of ToM in \framework outperformed the complete \framework in some cases, suggesting the models' lack of ToM capabilities, which might influence the effectiveness of \framework. 
For future work, the integration approach of \framework with FSM holds great potential for integrating LLMs into existing FSMs in the real world, offering the possibility of supporting more simultaneous human-AI collaboration scenarios to achieve stronger capabilities and promote better cooperation.



\section{Conclusion}
In this paper, we propose \framework, a language agent framework for the challenges of real-time responsiveness and autonomous adaption to humans in real-time simultaneous human-AI collaboration tasks. 
Inspired by DPT, \framework combines FSM-based \textit{System 1} for rapid decision-making with a \textit{System 2} driven by LLMs for deeper reasoning.
The single-agent experiments and experiments with rule-based agents highlight that \framework has the capability in real-time tasks and simultaneous collaboration.
Moreover, the performance of \framework in human experiments marks a significant advancement, offering a more autonomous and adaptive framework in simultaneous human-AI collaboration. 
We open-source both the method and the environment to foster future research and advancements in simultaneous human-AI collaboration.
To the best of our knowledge, \framework is the first agent framework to achieve autonomous and simultaneous collaboration with humans in real time, making it a major step forward in language agents for human-AI collaboration.



\section*{Limitations}


\framework has already made breakthrough progress in the task of simultaneous Human-AI collaboration, providing a solid foundation for designing more complex agent frameworks in the future. 
However, \framework still has significant room for improvement. 
First, providing guidance and code-as-policy to \framework FSM-driven \textit{System 1} remains a major challenge for many models with weaker capabilities, especially small models. 
Many models are still limited by errors in their output, which cannot be verified and thus lead to invalid policies.
Secondly, using lambda functions to control the FSM still has a certain lack of flexibility. However, given the current limitations of model capabilities, it might be hard for models to directly output valid state machine code.
And since ToM ability is a complex higher-order reasoning capability, it imposes high demands on the model itself. 
This limitation makes it more likely for ToM failures to occur when \framework is applied to smaller models.
Big models with strong reasoning capabilities suffer from high latency, which reduces the timeliness of reasoning, which is another limitation of \framework, making it challenging to consistently outperform FSM across different models.
Our current experiments are still conducted on a small scale, and since the human subjects are all university students, there may be potential biases. 
Conducting larger-scale experiments in the future will help deepen our understanding of simultaneous human-AI collaboration.


% Bibliography entries for the entire Anthology, followed by custom entries
%\bibliography{anthology,custom}
% Custom bibliography entries only
\bibliography{dpt}


\clearpage
\appendix


\section{Environment Details}\label{app:env}

We implement the environment from \cite{zhang2024mutual} based on overcooked-ai\footnote{\url{https://github.com/HumanCompatibleAI/overcooked_ai}, MIT License} \cite{carroll2019utility} and gym-cooking\footnote{\url{https://github.com/rosewang2008/gym-cooking}, MIT License} \cite{gymcooking}.

\paragraph{State.} Both the agent and the human have full access to the game states and each other's actions.
Players can directly see the status of all items in the game interface, such as the location where items are placed and their current state (e.g., beef cooking in a pan).
Players can also view the remaining game time and current score through the information displayed. 
The remaining time for each order, the progress of chopping lettuce, the process of cooking beef, and the process of extinguishing a fire are shown through progress bars.
All actions taken by teammates, the teammates' location, and the items they are holding are fully visible to each other.


\paragraph{Action.} In this environment, the actions that the human and the agent can take to control the chefs include moving up, down, left, and right, as well as ``interact''. 
All activities such as picking up items, serving dishes, and extinguishing fires are considered as ``interact'' actions.
The specific interaction rules are illustrated in Figure \ref{fig:diff_level}. 
We denote the actions to control the chefs as $\mathcal{A}^{\text{control}}$.
The agent and the human share the same $\mathcal{A}^{\text{control}}$.

\begin{figure}[h]
    \centering
    \subfigure[LettuceBurger]{
    \includegraphics[width=0.4\linewidth]{figures/lettuceburger.png}
    \label{fig:vegan}
    }
    % \hfill
    \subfigure[BeefBurger]{
    \includegraphics[width=0.4\linewidth]{figures/beefburger.png}
    \label{fig:meat}
    }
    \hfill
    \subfigure[BeefLettuceBurger]{
    \includegraphics[width=0.4\linewidth]{figures/beeflettuceburger.png}
    \label{fig:allinone}
    }
    % \hfill
    \subfigure[Overcooked Beef]{
    \includegraphics[width=0.4\linewidth]{figures/fire.png}
    \label{fig:fire}
    }
    \caption{\textbf{Game Mechanism from \citet{zhang2024mutual}.} (a), (b), and (c) are the rules for preparing and serving burgers. (d) demonstrates the mechanism of overcooked beef and the rules for handling the fire caused by overcooked beef.}
    \label{fig:diff_level}
\end{figure}

\paragraph{Reward.} The scores for completing the three different types of orders vary and serving the wrong burger or missing an order will result in a penalty. The specific rewards are detailed in \Cref{tab:reward}.

\paragraph{Timesteps.} In the environment implementation, one timestep is 0.25 sencond in the real world. At most one action can be executed at each time step.

\begin{table}[]
    \centering

        \resizebox{0.9\linewidth}{!}{
    \begin{tabular}{l|c}
    \toprule
        \textbf{Event} & \textbf{Rewards} \\
        \midrule
        Serve a LettuceBurger & +15\\
        Serve a BeefBurger & +20\\
        Serve a BeefLettuceBurger & +25\\
        Serve a Wrong Burger (or Something not a Burger) & -10\\
        Miss an order & -10\\
\bottomrule
    \end{tabular}
}
        \caption{\textbf{Rewards in Game.}}
    \label{tab:reward}
\end{table}

\subsection{FSM in Overcooked}
\label{app:fsm}

Follow the \citet{zhang2024mutual}, the macro actions included are summarized below.
\begin{itemize}
    \item \textbf{Prepare:}  
    \begin{itemize}
        \item \textbf{Valid Objects:} ``Beef'', ``Lettuce'', ``Bread''
        \item \textbf{Function:} Prepare an appointed ingredient until it can be used to assemble.
    \end{itemize}
    
    \item \textbf{Assemble:}  
    \begin{itemize}
        \item \textbf{Valid Objects:} ``BeefBurger'', ``LettuceBurger'', ``BeefLettuceBurger''
        \item \textbf{Function:} Assemble an appointed burger if all necessary ingredients are ready.
    \end{itemize}
    
    \item \textbf{Pass on:}  
    \begin{itemize}
        \item \textbf{Valid Objects:} ``Plate'', ``Bread''
        \item \textbf{Function:} Put the object onto the center counters to deliver it to the partner.
    \end{itemize}
    
    \item \textbf{Serve:}  
    \begin{itemize}
        \item \textbf{Valid Objects:} ``BeefBurger'', ``LettuceBurger'', ``BeefLettuceBurger''
        \item \textbf{Function:} Deliver an assembled burger to the customer.
    \end{itemize}
    
    \item \textbf{Putout Fire:}  
    \begin{itemize}
        \item \textbf{Valid Objects:} -
        \item \textbf{Function:} Pick up the fire extinguisher and put out the fire, if any.
    \end{itemize}
\end{itemize}


\onecolumn
\section{General Game Prompt of All Experiments}\label{app:gameprompt}

\input{prompts/game_prompt}

\section{\framework Implementation in Overcooked}\label{app:imple}

\onecolumn

\subsection{Instruction Prompt}\label{app:instruct}

\input{prompts/instructions}

\subsection{Prompts of \framework}\label{app:dpt-prompt}
\subsubsection{Prompts of Code-as-Policy Generator}\label{app:cap-prompt}

\input{prompts/dpt-cap}

\subsubsection{Prompts of Policy Reflection}\label{app:pr-promt}

\input{prompts/dpt-reflection}

\subsubsection{Prompts of Theory of Mind Module}\label{app:tom-prompt}

\input{prompts/dpt-tom}


\section{Implementation of Act, ReAct and Reflexion Frameworks}\label{app:baselines}
ReAct \cite{yao2022react} is a framework that integrates reasoning and acting by allowing agents to plan, interpret environments, and interact dynamically to improve decision-making.
Reflexion \cite{shinn2024reflexion} is a framework that enhances language model agents by enabling self-reflection, allowing them to learn from past mistakes, refine their reasoning, and iteratively improve decision-making in future tasks.
We use Act to name the LLM as Indenpendent System 1.

We implement Act, ReAct and Reflexion in Overcooked challenge. The three frameworks use the same prompt in the instruction part with \framework in \Cref{app:instruct}. We outline the specific differences in the output prompts for the three frameworks below.

\subsection{Output Prompts of Act}\label{app:act-prompt}
\input{prompts/act}

\subsection{Output Prompts of ReAct}\label{app:react-prompt}


\input{prompts/react}


\subsection{Output Prompts of Reflexion}\label{app:refl-prompt}

\input{prompts/reflexion}

\twocolumn
\begin{table*}[t]
    \renewcommand{\arraystretch}{1.5} % 行高
    \centering
    \resizebox{\linewidth}{!}{
    \begin{tabular}{l|l}
    \toprule
    \textbf{Key Events}  & \textbf{Key Actions} \\
    \midrule
    Cook Beef & \textcircled{1} Get Beef from station Put onto Pan \\
    Use Beef & \textcircled{1} Plate well-done Beef from Pan \\
    Prepare Lettuce & \textcircled{1} Get lettuce from station \textcircled{2} Put onto Cutboard \textcircled{3} Chop Lettuce \\
    \multirow{2}{*}{Use Lettuce}      & 
                                        \textcircled{1} Plate Lettuce Done from Cutboard \textcircled{2} Plate Lettuce Done from Counter \textcircled{3} Put onto Plate with BeefBurger                     
                                        \\
                                        &
                                        \textcircled{4} Put onto Plate with Bread \textcircled{5} Put Lettuce onto Plate \textcircled{6} Put Lettuce onto Plate with Beef
                                        \\
    \multirow{2}{*}{Use Bread} &         
                                        \textcircled{1} Get Bread from Station \textcircled{2} Plate Bread from Counter \textcircled{3} Put onto Plate with BeefLettuce
                                        \\
                                        & 
                                        \textcircled{4} Put onto Plate with Lettuce \textcircled{5} Put Bread onto Plate \textcircled{6} Put Bread onto Plate with Beef
                                        \\

         Use Plate & \textcircled{1} Get Plate from Station \\
                  Serve & \textcircled{1} Deliver Burger \\
    \bottomrule
    \end{tabular}

    }
    \caption{\textbf{The mapping from key event to key actions from \citet{zhang2024mutual}.}}\label{tab:event_action}
    
\end{table*}

\section{Metrics}\label{app:metrics}

Metrics we used in experiments include Atom Action Occupy,  Failure Missed, Failure Wrong Serve, Score Efficiency, Agent Contribution Rate, the total game score, and latency in second.

\paragraph{Atom Action Occupy.} The percentage of total time spent by in-game agents performing actions. $t_{atomic}$ refers to the number of time steps that have atomic action. $t_{total}$ refers to the total number of time steps.
\begin{equation}
    \text{Atom Action Occupy} = \frac{t_{atomic}}{t_{total}} 
\end{equation}

\paragraph{Failure Missed.} The number of orders missed of each games.

\paragraph{Failure Wrong Serve} The number of incorrect orders made by agents.

\paragraph{Score Efficiency.} The average score gained per macro action being executed.
$S_{total\_gain}$ refers to score gained and is excluding penalty points.
$\text{MA}_e$ refers to the number of macro action (MA) being executed.
\begin{equation}
    \text{Score Efficiency} = \frac{S_{total\_gain}}{\text{MA}_e}
\end{equation}

\paragraph{Latency.} The time in second that from the request to the output of a macro action or a code-as-policy output.

\paragraph{Agent Contribution Rate.} A concept from \citet{zhang2024mutual} to demonstrate the agent's contribution in each order based on the overcook environment. 

Below are the definition from \citet{zhang2024mutual}:
Key task events $KE$ are defined to track which team member completes specific tasks in Overcooked. Based on the burger-making process, each of the three burger types involves a set of essential, non-repeatable events. For instance, preparing a BeefBurger requires completing five key events: Cooking Beef, Using Beef, Using Bread, Using a Plate, and Serving. Each of these key events occurs only once. The completion of these events is triggered by specific “interact” actions, which are referred to as Key Actions.
The key actions mapping with the key events are in \Cref{tab:event_action}.
Each key event completed by a player is counted once as their contribution to the overall performance. 
Since these key events are non-repeatable, we can determine each player's contribution by tracking the key events they complete while preparing each successfully served burger.
We define the agent's contribution ratio $CR^{i}$ as:
$CR^{i} = \frac{KE^{i}}{KE^{i}+KE^{h}} \times 100\%$, where $KE^{i}$ and $KE^{h}$ represent the key events completed by the agent and the human respectively.



\section{Details of the LLM as Independent \textit{System 1} and \textit{System 2} Experiments .}\label{app:exp-p}


\subsection{Models and Deployment}
In this series of experiments, we use 8 different model series including GPT \cite{openai2024gpt4omini}, Qwen \cite{qwen2.5}, Llama \cite{touvron2023llama}, Phi \cite{abdin2024phi4}, Gemma \cite{team2024gemma}, Mistral \cite{mistralAI}, DeepSeek \cite{liu2024deepseek} and DeepSeek-R1 \cite{guo2025deepseek}:

\textbf{GPT Series}: GPT-4o, GPT-4o-mini and o3-mini

 \textbf{Qwen Series}: Qwen2.5 with 5 different sizes including 3b, 7b, 14b, 32b and 72b (Lisence: Apache license 2.0)
 
\textbf{Llama Series}: Llama3.1-8b, Llama3.2-3b and Llama3.3-70b (Lisence: llama)

\textbf{Phi Series}: Phi-3.5-3.8b and Phi-4-14b (Lisence: MIT)

\textbf{Gemma Series}: Gemma2 with 3 different sizes including 2b, 9b and 27b (Lisence: gemma)

\textbf{Mistral Series}: Ministral with 2 different sizes including 3b and 8b, Mistral-nemo-12b, Mistral-small-24b and Mixtral-8x22b (Lisence: mistral)

\textbf{DeepSeek Series}: DeepSeek-V2-16b and DeepSeek-V2.5 (Lisence: MIT)

\textbf{DeepSeek-R1 Series}: DeepSeek-R1 with 5 different sizes including 7b, 8b, 14b, 32b and 70b (Lisence: MIT)

All the open-source models are locally deployed with NVIDIA A800-SXM4-80GB through ollama \cite{ollama}, with the number of cards used determined by the model size.
For DeepSeek-R1 series in Long CoT, we deploy via llama.cpp \cite{llama.cpp} for customizing structured output.
The GPT series models use native API calls to conduct experiments.
The experiments use 26.3 A800-SXM4-80GB GPU hours for open-source models and \$ 35 in OpenAI API cost.
All models had their temperature parameter set to 0, while the remaining parameters were kept at their default values.

\subsection{Detailed Results}

We list the data from \Cref{fig:exp1} in \Cref{tab:exp1-1} and provided more detailed metrics. 
Metrics include Atom Action Occupy, Failure Missed, Failure Wrong Serve), Score Efficiency, the total game score, and latency in second.



\section{Details of Capability in Real-time Task Experiments .} \label{app:exp1}

\subsection{Models and Deployment}


In this series of experiments, we used 5 different model series including GPT \cite{openai2024gpt4omini}, Qwen \cite{qwen2.5}, Llama \cite{touvron2023llama}, Mistral \cite{mistralAI} and DeepSeek \cite{guo2025deepseek}.

\textbf{GPT Series}: GPT-4o, GPT-4o-mini and o3-mini

\textbf{Qwen Series}: Qwen2.5 with 3 different sizes including 14b, 32b and 72b (Lisence: Apache license 2.0)

\textbf{Llama Series}: Llama3.3-70b  (Lisence: llama)

\textbf{Mistral Series}: Mistral-nemo-12b, Mistral-small-24b and Mixtral-8x22b (Lisence: mistral)

\textbf{DeepSeek Series}: DeepSeek-R1-Distill-Llama-70B and DeepSeek-V2.5 (Lisence: MIT)


All the open-source models are locally deployed with NVIDIA A800-SXM4-80GB through vLLM \cite{kwon2023efficient}, with the number of cards used determined by the model size.
For DeepSeek-R1-70b, we use 8 NVIDIA H100-80GB-HBM3 for deployment through vLLM \cite{kwon2023efficient}.
The GPT series models use native API calls to conduct experiments.
The experiments use 140.3 A800-SXM4-80GB GPU hours and 17.5 H100-80GB-HBM3 GPU hours for open-source models and \$ 100 in OpenAI API cost.
All models had their temperature parameter set to 0, while the remaining parameters were kept at their default values.

\subsection{Detailed Results}


We list the data from \Cref{fig:exp1-2} in \Cref{tab:exp1-2-react,tab:exp1-2-reflexion,tab:exp1-2-dpt} and provided more detailed metrics. 
Metrics include Atom Action Occupy, Failure Missed, Failure Wrong Serve), Score Efficiency, the total game score, and latency in second.


\onecolumn
\begin{sidewaystable}
    \centering
    \resizebox{0.6\paperheight}{!}{
    \begin{tabular}{lcccccc}
    \toprule
\multicolumn{1}{c}{\multirow{2}{*}{\diagbox[width=3.5cm]{\textbf{Model}}{\textbf{Metrics}}}} & \textbf{Atom Action Occupy } & \textbf{Failure Missed} & \textbf{Failure Wrong Serve}& \textbf{Score Efficiency} & \textbf{Score} & \textbf{Latency}\\
& - & Times & Times&  Score/Marco Action&  &  Second \\
\midrule
\textbf{FSM}  & \textbf{0.90}  &3.00 & \textbf{0.00}  & \textbf{5.59}  & \textbf{65.00}  & \textbf{0.00(0.00)} \\\midrule
\midrule
\multicolumn{7}{c}{LLM as Independent \textit{System 1}}\\
\midrule
\textbf{GPT-4o}  & 0.83(0.00)  & \textbf{2.70(0.18)}  & 0.00(0.05)  & \uline{2.00(0.10)}  & 46.00(4.00)  & 0.91(0.03) \\
\textbf{GPT-4o-mini}  & 0.85(0.01)  & 3.40(0.22)  & 0.00(0.10)  & 1.43(0.10)  & 24.50(5.97)  & 0.78(0.03) \\
\midrule
\textbf{DeepSeek-V2-16b}  & 0.00(0.00)  & 4.00(0.00)  & \textbf{0.00(0.00)}  & 0.00(0.00)  & -40.00(0.00)  & 1.62(0.00) \\
\textbf{DeepSeek-V2.5-236b}  & 0.52(0.00)  & 3.00(0.00)  & \textbf{0.00(0.00)}  & 1.39(0.01)  & -5.00(0.00)  & 6.51(0.03) \\
\midrule
\textbf{Gemma2-2b}  & \uline{0.00(0.00)}  & 4.00(0.00)  & \textbf{0.00(0.00)}  & 0.00(0.00)  & -40.00(0.00)  & 0.92(0.01) \\
\textbf{Gemma2-9b}  & 0.77(0.00)  & 4.10(0.11)  & 0.10(0.11)  & 0.49(0.06)  & -24.00(4.25)  &1.44(0.01) \\
\textbf{Gemma2-27b}  & 0.74(0.00)  & 4.00(0.09)  & {1.00(0.07)}  & 0.74(0.06)  & -30.00(2.56)  & 2.26(0.02) \\
\midrule
\textbf{Llama3.1-8b } & 0.82(0.00)  & 4.30(0.11)  & \textbf{0.00(0.00)}  & 0.00(0.00)  & -43.00(1.12)  & 1.07(0.02) \\
\textbf{Llama3.2-3b}  & 0.80(0.00)  & 4.00(0.00)  & \textbf{0.00(0.00)}  & 0.00(0.00)  & -40.00(0.00)  & 0.64(0.01) \\
\textbf{Llama3.3-70b} & 0.54(0.00)  & 4.00(0.00)  & 4.00(0.00)  & 1.40(0.3)  & -5.00(0.75)  & 4.38(0.02) \\
\midrule
\textbf{Ministral-3b}  & 0.00(0.00)  & 4.00(0.00)  & \textbf{0.00(0.00)}  & 0.00(0.00)  & -40.00(0.00)  & \uline{0.62(0.00)} \\
\textbf{Ministral-8b}  & 0.75(0.03)  & 5.00(0.08)  & \textbf{0.00(0.00)}  & 0.64(0.05)  & -25.00(1.06)  & 1.03(3.15) \\
\textbf{Mistral-nemo-12b}  & 0.72(0.02)  & 4.00(0.05)  & {1.00(0.08)}  & 1.27(0.11)  & 2.50(2.55)  & 1.29(0.33) \\
\textbf{Mistral-small-24b} & 0.70(0.01)  & 5.30(0.21)  & {0.50(0.11)}  & 0.31(0.10)  & -47.50(5.57)  & 1.86(0.04) \\
\textbf{Mixtral-8x22b}& 0.72(0.00)  & 4.00(0.11)  & {0.40(0.11)}  & 1.27(0.11)  & -12.50(4.20)  & 2.38(0.02) \\
\midrule
\textbf{Phi-3.5-3.8b}  & 0.75(0.00)  & 4.00(0.09)  & \textbf{0.00(0.00)}  & 0.00(0.00)  & -40.00(0.92)  & 1.00(0.03) \\
\textbf{Phi-4-14b}  & 0.81(0.00)  & 5.00(0.55)  & \textbf{0.00(0.00)}  & 0.00(0.03)  & -500.00(1.00)  & 1.51(0.01) \\
\midrule
\textbf{Qwen2.5-3b}  & 0.93(0.03)  & 5.00(0.00)  & \textbf{0.00(0.00)}  & 0.00(0.00)  & -50.00(0.00)  & 0.90(0.29) \\
\textbf{Qwen2.5-7b}  & 0.78(0.03)  & 5.00(0.45)  & \textbf{0.00(0.00)}  & 0.00(0.00)  & -50.00(4.47)  & 2.10(0.48) \\
\textbf{Qwen2.5-14b}  & 0.76(0.00)  & 3.50(0.11)  & {0.00(0.10)}  & \uline{2.29(0.12)}  & 31.50(4.21)  & 1.59(0.03) \\
\textbf{Qwen2.5-32b}  & 0.63(0.02)  & 3.00(0.12)  &\textbf{0.00(0.00)}  & 1.59(0.07)  & 18.00(3.62)  & 2.67(0.52) \\
\textbf{Qwen2.5-72b}  & 0.56(0.00)  & 4.00(0.24)  & 0.90(0.11)  &0.16(0.18)  & -45.00(5.26)  & 4.56(0.04) \\
\midrule
\multicolumn{7}{c}{LLM as Independent \textit{System 2}}\\
\midrule
\textbf{o3-mini}  & 0.51(0.01)  & 4.00(0.10)  & {0.00(0.09)}  & 0.94(0.13)  & -20.50(3.82)  & 5.10(0.30) \\
\midrule
\textbf{DeepSeek-R1-14b} & 0.67(0.02)  & 4.00(0.11)  & {1.80(0.29)}  & 0.47(0.10)  & -45.00(6.68)  & 2.20(0.01) \\
\textbf{DeepSeek-R1-32b}& 0.54(0.01)  & 3.10(0.11)  & {0.10(0.11)}  & 0.80(0.12)  & -13.00(4.19)  & 3.86(0.02) \\
\textbf{DeepSeek-R1-70b} & 0.45(0.00)  & 5.00(0.07)  & \textbf{0.00(0.00)}  & 0.38(0.10)  & -42.50(2.31)  & 6.69(0.03) \\
\textbf{DeepSeek-R1-7b} & 0.00(0.00)  & 4.00(0.00)  & \textbf{0.00(0.00)}  & 0.00(0.00)  & -40.00(0.00)  & 1.38(0.01) \\
\textbf{DeepSeek-R1-8b} & 0.00(0.00)  & 4.00(0.05)  & \textbf{0.00(0.00)}  & 0.00(0.03)  & -40.00(2.23)  & 1.36(0.01) \\
        \bottomrule
    \end{tabular}
}
    \caption{Performance of Different Models in LLM as Independent \textit{System 1} and \textit{System 2} Experiments.}
    \label{tab:exp1-1}
\end{sidewaystable}

\clearpage
\twocolumn
\begin{sidewaystable}
    \centering
    \resizebox{0.55\paperheight}{!}{
    \begin{tabular}{lcccccc}
        \toprule
\multicolumn{1}{c}{\multirow{2}{*}{\diagbox[width=3.5cm]{\textbf{Model}}{\textbf{Metrics}}}} & \textbf{Atom Action Occupy } & \textbf{Failure Missed} & \textbf{Failure Wrong Serve}& \textbf{Score Efficiency} & \textbf{Score} & \textbf{Latency}\\
        & - & Time & Time&  Score/Marco Action&  &  Second \\
        \midrule
 \textbf{GPT-4o}  & 0.87(0.02)  & 3.70(0.15)  & 0.00(0.10)  & 3.08(0.30)  & 21.00(7.01)  & 7.10(0.29) \\
\textbf{GPT-4o-mini}  & 0.90(0.03)  & 4.00(0.12)  & 0.00(0.16) & 0.60(0.28)  & -28.50(6.23)  & 3.06(0.07) \\
 \textbf{o3-mini-low}  &    0.70(0.02) &  3.60(0.18) &0.00(0.08)  & 2.51(0.25)  &5.50(5.86)  & 8.64(0.27) \\
\midrule
\textbf{DeepSeek-V2.5-236b}  &  0.63(0.02) &  4.30(0.15) & 0.00(0.05)  &1.72(0.24)  &-21.50(3.56)  & 6.45(0.18)\\
\textbf{DeepSeek-R1-70b} &  0.67(0.02) & 4.00(0.10) & 0.70(0.11)  & 1.48(0.17)  &-17.00(4.32)  & 7.79(0.20)\\
\textbf{DeepSeek-R1-32b}   & 0.70(0.03)           & 4.00(0.13)  & 0.00(0.13) & 1.49(0.18) & -15.50(4.51)  & 5.77(0.18)  \\
\textbf{DeepSeek-R1-14b}   & 0.87(0.02)  & 4.00(0.12)  &0.00(0.12) & 2.67(0.19)  & -7.00(4.94)  & 2.91(0.03)  \\
\midrule
\textbf{Llama3.3-70b} &    0.82(0.01) &  4.00(0.12) & \textbf{0.00(0.00)}  & 2.86(0.16)  &20.00(4.21)  & 5.44(0.05)\\
\midrule
\textbf{Mistral-nemo-12b} &    0.64(0.01) &  4.50(0.17) & \textbf{0.00(0.00)}  & 2.40(0.13)  &-10.00(3.31)  & \textbf{1.10(0.03)}\\
\textbf{Mistral-small-24b}   &  \textbf{0.94(0.01)} &  3.00(0.11) & \textbf{0.00(0.00)}  & \textbf{4.63(0.20)}  &\textbf{59.50(5.04)}  & 2.69(0.02) \\
\textbf{Mixtral-8x22b}  &  0.88(0.02) & 3.40(0.15) & \textbf{0.00(0.00)}  & 1.73(0.22)  &-5.00(5.23)  & 5.56(0.10) \\
\midrule
\textbf{Qwen2.5-14b}   & 0.90(0.02)&  3.50(0.18) & 0.20(0.18)  & 1.98(0.21)  &-5.00(5.31)  & 1.55(0.03)\\
\textbf{Qwen2.5-32b}  & 0.87(0.00)  & 4.00(0.05)  & \textbf{0.00(0.00)}  & 2.94(0.02)  & 10.00(0.50)  & 1.93(0.04) \\
\textbf{Qwen2.5-72b}  &  0.83(0.03) &  \textbf{2.90(0.12)} & 0.00(0.05)  & 2.71(0.09)  &16.50(3.22)  & 4.60(0.09) \\
\textbf{QwQ-32b}  & 0.78(0.01)  & 3.20(0.11)  & \textbf{0.00(0.00)}  & 2.46(0.12)  & 8.00(2.77)  & 10.75(0.24) \\

        \bottomrule
    \end{tabular}}
    \caption{Performance of Different Models - ReAct }
    \label{tab:exp1-2-react}
\end{sidewaystable}





\begin{sidewaystable}
    \centering
    \resizebox{0.55\paperheight}{!}{
    \begin{tabular}{lcccccc}
\toprule
\multicolumn{1}{c}{\multirow{2}{*}{\diagbox[width=3.5cm]{\textbf{Model}}{\textbf{Metrics}}}} & \textbf{Atom Action Occupy } & \textbf{Failure Missed} & \textbf{Failure Wrong Serve}& \textbf{Score Efficiency} & \textbf{Score} & \textbf{Latency}\\
& - & Times & Times&  Score/Marco Action&  &  Second \\
\midrule
\textbf{GPT-4o}  & 0.80(0.02)  & 4.00(0.16)  & 0.00(0.09)  & 2.14(0.17)  & -1.50(3.78)  & 7.49(0.27) \\
\textbf{GPT-4o-mini}  & \textbf{0.91(0.01)}  & 4.00(0.07)  & \textbf{0.00(0.00)}  & 0.00(0.14)  & -40.00(2.17)  & 3.11(0.08) \\
\textbf{o3-mini}  &  0.78(0.02) &  4.00(0.18) & 0.30(0.11)  & 1.78(0.26)  &-16.50(7.12)  & 8.86(0.23) \\
\midrule
\textbf{DeepSeek-V2.5}   &  0.52(0.01) &  4.22(0.13) & \textbf{0.00(0.00)}  & 1.24(0.18)  &-25.56(2.91)  & 7.64(0.16)\\
\textbf{DeepSeek-R1-70b}  &  0.66(0.01) &  4.00(0.14) & 0.00(0.08)  & 1.44(0.19)  &-20.00(4.79)  & 7.78(0.17) \\
\textbf{DeepSeek-R1-32b}   & 0.79(0.02)           & 4.60(0.11)  & 0.30(0.17) & 0.90(0.21) & -37.50(4.77)  & 7.39(0.11)  \\
\textbf{DeepSeek-R1-14b}   & 0.80(0.03)           & 3.7(0.15)  &0.00(0.07) & 1.93(0.22)  & -10.50(4.12)  & 4.01(0.11)  \\

\midrule
\textbf{Llama3.3-70b}   &  0.65(0.02) &  \textbf{3.00(0.09)} & \textbf{0.00(0.00)}  & \textbf{3.25(0.19)}  &\textbf{20.00(4.47)}  & 5.20(0.06) \\
\midrule
\textbf{Mistral-nemo-12b}   &  0.00(0.00) &  4.00(0.00) & \textbf{0.00(0.00)}  & 0.00(0.00) & -40.00(0.00)  & \textbf{1.60(0.02)} \\
\textbf{Mistral-small-24b}   &  0.90(0.01) &  \textbf{3.00(0.09)} & 0.00(0.10)  & 1.43(0.03)  &-5.00(3.63)  & 3.11(0.05) \\
\textbf{Mixtral-8x22b}   &  0.84(0.02) &  3.80(0.18) & \textbf{0.00(0.00)}  & 2.44(0.20)  & 0.50(4.33)  & 5.58(0.23) \\
\midrule
\textbf{Qwen2.5-14b}   &  \textbf{0.91(0.02)} &  4.00(0.10) & 0.00(0.10)  & 2.44(0.24)  &-4.00(4.45)  & 1.87(0.05) \\
\textbf{Qwen2.5-32b}  & 0.00(0.00)  & 4.00(0.00)  & \textbf{0.00(0.00)}  & 0.00(0.00)  & -40.00(0.00)  & 2.93(0.05) \\
\textbf{Qwen2.5-72b}  & 0.69(0.05)  & \uline{5.00(0.09)}  & \textbf{0.00(0.00)}  & 1.47(0.09)  & -25.00(2.76)  & 4.66(0.05) \\
\textbf{QwQ-32b}  & 0.00(0.01)  & 5.00(0.00)  & \textbf{0.00(0.00)}  & 0.00(0.11)  & -50.00(0.75)  & 7.75(0.11) \\
\bottomrule
    \end{tabular}}
        \caption{Performance of Different Models - Reflexion}
    \label{tab:exp1-2-reflexion}
\end{sidewaystable}



\clearpage
  \onecolumn

\begin{sidewaystable}
    \centering
    \resizebox{0.7\paperheight}{!}{
    \begin{tabular}{lcccccc}
        \toprule
       \multicolumn{1}{c}{\multirow{2}{*}{\diagbox[width=3.5cm]{\textbf{Model}}{\textbf{Metrics}}}} & \textbf{Atom Action Occupy } & \textbf{Failure Missed} & \textbf{Failure Wrong Serve}& \textbf{Score Efficiency} & \textbf{Score} & \textbf{Latency}\\
        & - & Times & Times&  Score/Marco Action&  &  Second \\
        \midrule
\textbf{GPT-4o}            & 0.91(0.00)           & 3.60(0.14)  & 0.00(0.05) & 3.05(0.24)  & 20.50(5.41)  & 5.08(0.15) \\
\textbf{GPT-4o-mini}       & 0.93(0.00)           & 3.60(0.11)  & \textbf{0.00(0.00)} & 3.50(0.23)  & 21.00(4.47)  & 2.13(0.01) \\
\textbf{o3-mini}           & 0.90(0.00)           & 3.00(0.18)  & \textbf{0.00(0.00)} & 3.68(0.19)  & 37.50(4.81)  & 7.03(0.28) \\
\midrule
\textbf{DeepSeek-V2.5}     & 0.93(0.00)           & 3.00(0.11)  &\textbf{ 0.00(0.00)} & 3.40(0.14)  &31.50(3.40)  & 4.73(0.11) \\
\textbf{DeepSeek-R1-70b}   & 0.90(0.01)           & \textbf{2.30(0.17)}  & \textbf{0.00(0.00)} & \textbf{4.19(0.15)}  & \textbf{60.00(4.35)}  & 9.09(0.26)  \\
\textbf{DeepSeek-R1-32b}   & 0.93(0.00)           & 2.80(0.21)  & \textbf{0.00(0.00)} & 3.35(0.27) & 39.50(7.68)  & 6.58(0.25)  \\
\textbf{DeepSeek-R1-14b}   & 0.91(0.01)           & 3.40(0.15)  &0.00(0.50) & 3.04(0.21)  & 23.00(5.42)  & 3.87(0.07)  \\
\midrule
\textbf{Llama3.3-70b}      & 0.94(0.00)           & 4.00(0.13)  & 0.00(0.05) & 1.82(0.34)  & -10.00(6.46) & 2.28(0.10)  \\
\midrule
\textbf{Mistral-nemo-12b}  & 0.91(0.01)           & 3.30(0.13)  & \textbf{0.00(0.00)} & 3.49(0.21)  & 30.00(5.20)  & 1.31(0.03)  \\
\textbf{Mistral-small-24b} & 0.91(0.00)           & 4.00(0.09)  & \textbf{0.00(0.00)} & 2.05(0.17)  & -1.50(3.63)  & 3.61(0.31)  \\
\textbf{Mixtral-8x22b}     & \textbf{0.95(0.01)}  & 4.00(0.00)  & \textbf{0.00(0.00)} & 2.70(0.20)  & 0.00(15.00)  & 4.21(0.17) \\
\midrule
\textbf{Qwen2.5-14b}       & 0.94(0.00)  & 4.00(0.05)  & \textbf{0.00(0.00)} & 2.68(0.22)  & 1.50(4.11)  & \textbf{1.18(0.02)}  \\
\textbf{Qwen2.5-32b}       & 0.93(0.00)           & 4.00(0.13)  & \textbf{0.00(0.00)} & 2.26(0.13)  & 1.00(3.83)  & 1.65(0.03)  \\
\textbf{Qwen2.5-72b}       & 0.94(0.01)           & 3.70(0.18)  &\textbf{ 0.00(0.00)} & 2.66(0.21) & 11.00(4.88)  & 3.01(0.12)   \\
\textbf{QwQ-32b}       & 0.90(0.00)           & 2.70(0.17)  &\textbf{ 0.00(0.00)} & 3.90(0.15) & 51.00(4.74)  & 14.96(0.78)   \\
        \bottomrule
    \end{tabular}
    }
        \caption{Performance of Different Models - \framework.}
    \label{tab:exp1-2-dpt}
\end{sidewaystable}



\clearpage
\twocolumn


\begin{table*}[htp]
\resizebox{\linewidth}{!}{
\begin{tabular}{lcccccccc}
\toprule
\multicolumn{1}{c}{\multirow{3}{*}{\diagbox[width=3.5cm]{\textbf{Model}}{\textbf{Framework}}}} &
  \multicolumn{4}{c}{\textbf{Score Efficiency}} &
  \multicolumn{4}{c}{\textbf{Latency}} \\
  \cmidrule(lr){2-5}
  \cmidrule(lr){6-9}
  
\multicolumn{1}{c}{}                               & \multirow{2}{*}{ReAct}    & \multirow{2}{*}{Reflexion}       & \framework    & \multirow{2}{*}{\framework}     & \multirow{2}{*}{ReAct}   & \multirow{2}{*}{Reflexion}     & \framework    & \multirow{2}{*}{\framework}   \\
\multicolumn{1}{c}{}           &     &        & w/o ToM   &   &    &      &  w/o ToM   &    \\
\midrule
\textbf{o3-mini-high} & 0.00(0.00)	&0.00(0.00)&	\textbf{5.66(0.21)}	&5.33(0.18) &39.01(2.82)	&39.47(2.43)	&\textbf{34.77(4.37)}&35.96(4.91)\\
\textbf{o3-mini-medium} & 2.67(0.38)	&3.59(0.39)	&5.16(0.28)&\textbf{5.23(0.24)} &28.07(2.42)	&26.73(3.93)	&\textbf{22.24(1.39)}&24.05(2.81)\\
\textbf{o3-mini-low} & 3.20(0.34)	&4.18(0.34)&	\textbf{4.28(0.43)}	&4.60(0.35) &10.78(1.40)	&10.58(0.80)	&\textbf{7.34(0.37)}&7.68(0.38)\\
\textbf{GPT-4o }  & 4.26(0.42)	&3.86(0.34)	&3.43(0.42)&	\textbf{4.46(0.39)}&	6.63(7.53)	&6.81(0.24)&	4.92(1.32)	&\textbf{4.91(1.41)}\\
\textbf{GPT-4o-mini}   & 3.95(0.52)	&4.64(0.66)	&5.03(0.28)&	\textbf{5.33(0.33)} &  2.93(0.77)	&3.15(1.27)	&2.09(1.09)	
&\textbf{2.08(0.58)} \\
\textbf{Qwen-Max}   &4.56(0.39)&	4.03(0.28)	&4.83(0.45)	&\textbf{5.09(0.31)} &8.29(0.14)&	10.30(0.21)&	5.90(0.11)&	\textbf{5.89(0.10)} \\
\textbf{Claude 3.5 Haiku}    & 4.04(0.30)	&3.65(0.31)&	\textbf{4.67(0.39)}&	4.47(0.34)  &5.74(0.06)	&7.47(0.11)&	\textbf{5.21(0.05)}&	5.25(0.06)\\
% \textbf{Claude 3.5 Sonnet}  &  & -& - & - & - &  &- & - & - & -  \\
% \textbf{Gemini 2.0 Flash}  &  \textbf{58.33}   & -& - & - & - & \textbf{0.67}  &- & - & - & -   \\
\midrule
\textbf{DeepSeek-R1-671b}  &4.52(0.25)	&4.47(0.37)
&	4.90(0.21)	&\textbf{5.27(0.19)}	 &\textbf{ 31.31(2.17)}	&41.66(2.45)&	38.89(3.70)	&34.63(2.30)\\
\textbf{DeepSeek-R1-70b}  &3.66(0.25)&	2.25(0.27)	&4.64(0.25)&	\textbf{4.92(0.24)} &7.82(0.17)&	\textbf{7.39(0.14)}	&10.30(0.36)&	10.13(0.34)\\
\textbf{DeepSeek-R1-32b}  &3.64(0.29)&	3.27(0.29)	&\textbf{4.31(0.26)}&	4.04(0.38) &5.75(0.09)	&6.77(0.13)&	5.24(0.08)&	\textbf{5.11(0.13)} \\
\textbf{DeepSeek-R1-14b}  &3.16(0.22)&	3.16(0.43)	&\textbf{4.33(0.36)}	&4.29(0.38) & \textbf{3.06(0.06)}	&3.44(0.09)&	3.88(0.088)	&3.57(0.06)\\
\textbf{DeepSeek-V3 }   &4.78(0.39)&	5.03(0.38)	&\textbf{6.00(0.18)}	&5.66(0.25)&   7.54(0.15)&8.86(0.15)	&\textbf{1.92(0.04)}	&2.41(0.10)  \\
\textbf{DeepSeek-V2.5}  &	2.29(0.26)	&3.43(0.29)	&\textbf{4.24(0.40)	}&3.61(0.42)&  4.88(0.07)	&5.35(0.08)	&\textbf{4.06(0.10)	}&4.49(0.07)	\\
\textbf{QwQ-32b} & 3.92(0.24)	&0.00(0.21)&\textbf{4.64(0.25)}	&4.29(0.25)	&	8.80(1.04)	&\textbf{7.28(2.47)}&14.50(3.04)	&12.67(1.21)	 \\
\textbf{Qwen2.5-72b} & 4.44(0.16)&	\textbf{5.11(0.29)}	&3.25(0.27)	&4.51(0.29)&	4.34(0.06)	&4.83(0.11)	&\textbf{3.81(0.10)}	&4.62(0.11) \\
\textbf{Llama3.3-70b}  & 4.44(0.37)	&2.01(0.28)	&\textbf{4.08(0.19)}&	3.89(0.32)&    4.53(0.08)&	5.34(0.11)	&\textbf{2.30(0.08)}	&2.90(0.09) \\
\textbf{Mixtral-8x22b}   &  3.58(0.30)&	4.01(0.32)	&\textbf{4.63(0.41)}&	4.38(0.43)&  5.20(0.18)	&5.19(0.22)	&\textbf{4.53(0.14)}&	5.31(0.19)  \\
\midrule
\midrule
\textbf{Overall}     &3.59(0.30)&	3.33(0.32)&	4.59(0.31)	&\textbf{4.67(0.31)	}  	&10.86(1.13)&	11.80(0.88)&10.11(0.97)	&\textbf{10.10(0.87)}\\
\bottomrule
\end{tabular}
}
\caption{\textbf{Results with Standard Errors of Experiments - Collaborating with Rule-based Agents.}}\label{tab:exp2-d}
\end{table*}




	



\section{Details of Capability in Simultaneous Collaboration Experiments} \label{app:exp2}




\subsection{Models and Deployment}

In this series of experiments, we used 5 different model series including GPT \cite{openai2024gpt4omini}, Claude \cite{anthropic_claude35}, Qwen \cite{qwen2.5}, Llama \cite{touvron2023llama}, Mistral \cite{mistralAI} and DeepSeek \cite{guo2025deepseek}.

\textbf{GPT Series}: GPT-4o, GPT-4o-mini and o3-mini

\textbf{Claude Series}: Claude-3.5-haiku 


\textbf{Qwen Series}: Qwen2.5-72b  (Lisence: Apache license 2.0) and Qwen-Max

\textbf{Llama Series}: Llama3.3-70b  (Lisence: llama)

\textbf{Mistral Series}: Mixtral-8x22b  (Lisence: mistral)

\textbf{DeepSeek Series}: DeepSeek-R1-671b, DeepSeek-V2.5 and DeepSeek-V3 (Lisence: MIT)


All the open-source models are locally deployed with NVIDIA A800-SXM4-80GB through vLLM \cite{kwon2023efficient}, with the number of cards used determined by the model size.
For DeepSeek-R1-70b and DeepSeek-V3, we use 8 and 16 NVIDIA H100 80GB HBM3 for deployment through vLLM \cite{kwon2023efficient}.
The GPT series, Claude-3.5-haiku and Qwen-Max use native API calls to conduct experiments.
The experiments use 350.9 A800-SXM4-80GB GPU hours and 210.5 H100-80GB-HBM3 GPU hours for open-source models, \$ 400 in OpenAI API cost, \$ 80 in Claude API cost and \$ 120 in Qwen API cost.
All models had their temperature parameter set to 0, while the remaining parameters were kept at their default values.



% \subsection{Additional Results}

\subsection{Other Metrics}
We list the more metrics \Cref{tab:exp2-d} including Score Efficiency and latency in second.

\subsection{Case Study}\label{app:case-study}

\subsubsection{Example of \framework when playing with Beef Agent.}
Below is the ToM module output of \framework in the 78th timestep, driven by DeepSeek-R1-70b. It demonstrates that \framework's ToM mechanism effectively helps the agent infer that its partner specializes in preparing beef.
\begin{lstlisting}
Inferred Human Behavior:
- The human player focuses on beef preparation and distribution, frequently passing well-cooked beef to the counter.
- The agent should complement this by handling lettuce preparation, bread management, and burger assembly.
- The human's consistent passing of beef indicates a preference for handling meat, so the agent should focus on other tasks to optimize teamwork.
- The agent should pass prepared lettuce and bread to the counter to enable the human to assemble burgers efficiently.
- By dividing tasks based on the human's focus, the agent can improve overall efficiency and ensure orders are completed on time.
\end{lstlisting}

\subsubsection{Example of \framework when playing with Assemble \& Serve Agent.}
Below is the Reflection and ToM module output of \framework in the 266th timestep, driven by o3-mini-low. It demonstrates that \framework's ToM mechanism effectively helps the agent infer that its partner specializes in burger assembly.
\begin{lstlisting}
Behavior Guidelines:
Focus on avoiding overcooking by closely monitoring beef cooking times and switching to fresh ingredients if needed. Prioritize orders with minimal remaining time to prevent missed orders. Avoid repetitive actions like serving beefburgers without verifying order demand. Delegate tasks by quickly preparing ingredients and passing them to the human player, minimizing counter clutter and idle time.

Inference on Human Player Behavior Pattern:
The human player prioritizes urgent beef-based orders and rapid assembly, often focusing on assembling and serving ready-to-go items. Their approach suggests a preference for quick, order-focused actions, emphasizing the need for prepped ingredients. The agent should support this by preparing well-cooked beef and promptly passing completed ingredients to ensure smoother coordination.
\end{lstlisting}





\section{Details of Human Experiments }\label{app:humanexp}



\subsection{Procedure}
We recruited 71 participants from the university through its internal social platform. Each participant received a compensation of 50 RMB for their participation. To enhance engagement and attentiveness, we provided performance-based bonuses. Participants within each group were ranked based on their self-play performance and their performance across four different agent games. The top 25\% in playing with each agent and self-play can receive an additional bonus of 3 RMB with a maximum possible bonus of 15 RMB.

The experiment was conducted online, where participants completed the tasks on a designated webpage using a computer with a keyboard. Each session lasted approximately 40 minutes. Participants controlled the chef using the arrow keys and interacted with objects by pressing the spacebar. The entire experimental process was recorded, with playback support available for data validation.

Participants were randomly assigned to one of two maps. Within a group, each participant interacted with four different agents, playing two games per agent, resulting in a total of eight games in random order. To examine whether participants could infer the agents' capabilities, they were not informed of the agent types but were only made aware that the experiment involved four types of agents, differentiated by color.

Before beginning the experiment, all participants completed an informed consent form (\Cref{fig:statement}) and read instructions detailing the game rules and operations. Following the instructions, they first participated in a non-scored trial to familiarize themselves with the environment, rules, and controls. This was followed by a scored trial to assist with data validation.

In the formal experiment, after each game, participants were asked to rank the agents based on their collaborative capabilities and personal preference. Upon completing all eight games, they filled out an additional questionnaire, where we collected their perceptions of task load and their intended level of task engagement.

The experiment use \$ 30 in OpenAI API cost.

\subsection{Participants}
A total of 71 participants participated in the study. 
The first and second authors of the article independently validated all collected data. 
This validation included checking data completeness (e.g., whether participants completed all the experiments) and reviewing the recorded playbacks to identify any abnormal actions (e.g., instances where participants did not engage in any cooperative behavior). 
After data validation, we excluded any data with anomalies, including passive participation and missing data, resulting in 68 valid participants (M = 36, F = 32, and Others = 0, ages between 18 and 31). Group 1 (Map 1) has 36 valid data points and Group 2 (Map 2) has 32 valid data points.





\begin{figure*}
    \centering
    \includegraphics[width=0.8\linewidth]{figures/statement.png}
    \caption{Experiment Statement.}
    \label{fig:statement}
\end{figure*}

\end{document}
