@inproceedings{DBLP:conf/acl/ZhangTWW0HTLZ024,
  author       = {Wenqi Zhang and
                  Ke Tang and
                  Hai Wu and
                  Mengna Wang and
                  Yongliang Shen and
                  Guiyang Hou and
                  Zeqi Tan and
                  Peng Li and
                  Yueting Zhuang and
                  Weiming Lu},
  editor       = {Lun{-}Wei Ku and
                  Andre Martins and
                  Vivek Srikumar},
  title        = {Agent-Pro: Learning to Evolve via Policy-Level Reflection and Optimization},
  booktitle    = {Proceedings of the 62nd Annual Meeting of the Association for Computational
                  Linguistics (Volume 1: Long Papers), {ACL} 2024, Bangkok, Thailand,
                  August 11-16, 2024},
  pages        = {5348--5375},
  publisher    = {Association for Computational Linguistics},
  year         = {2024},
  url          = {https://aclanthology.org/2024.acl-long.292},
  timestamp    = {Mon, 26 Aug 2024 16:40:52 +0200},
  biburl       = {https://dblp.org/rec/conf/acl/ZhangTWW0HTLZ024.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{DBLP:conf/icra/LiangHXXHIFZ23,
  author       = {Jacky Liang and
                  Wenlong Huang and
                  Fei Xia and
                  Peng Xu and
                  Karol Hausman and
                  Brian Ichter and
                  Pete Florence and
                  Andy Zeng},
  title        = {Code as Policies: Language Model Programs for Embodied Control},
  booktitle    = {{IEEE} International Conference on Robotics and Automation, {ICRA}
                  2023, London, UK, May 29 - June 2, 2023},
  pages        = {9493--9500},
  publisher    = {{IEEE}},
  year         = {2023},
  url          = {https://doi.org/10.1109/ICRA48891.2023.10160591},
  doi          = {10.1109/ICRA48891.2023.10160591},
  timestamp    = {Tue, 08 Aug 2023 10:24:29 +0200},
  biburl       = {https://dblp.org/rec/conf/icra/LiangHXXHIFZ23.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{Yang23Cole,
  author       = {Yang Li and
                  Shao Zhang and
                  Jichen Sun and
                  Yali Du and
                  Ying Wen and
                  Xinbing Wang and
                  Wei Pan},
  title        = {Cooperative Open-ended Learning Framework for Zero-Shot Coordination},
  booktitle    = {{ICML}},
  series       = {Proceedings of Machine Learning Research},
  volume       = {202},
  pages        = {20470--20484},
  publisher    = {{PMLR}},
  year         = {2023}
}

@article{baron1985does,
  title={Does the autistic child have a “theory of mind”?},
  author={Baron-Cohen, Simon and Leslie, Alan M and Frith, Uta},
  journal={Cognition},
  volume={21},
  number={1},
  pages={37--46},
  year={1985},
  publisher={Elsevier}
}

@article{carroll2019utility,
  title={On the utility of learning about humans for human-ai coordination},
  author={Carroll, Micah and Shah, Rohin and Ho, Mark K and Griffiths, Tom and Seshia, Sanjit and Abbeel, Pieter and Dragan, Anca},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@article{evans2013dual,
  title={Dual-process theories of higher cognition: Advancing the debate},
  author={Evans, Jonathan St BT and Stanovich, Keith E},
  journal={Perspectives on psychological science},
  volume={8},
  number={3},
  pages={223--241},
  year={2013},
  publisher={Sage Publications Sage CA: Los Angeles, CA}
}

@inproceedings{he-etal-2024-planning,
    title = "Planning Like Human: A Dual-process Framework for Dialogue Planning",
    author = "He, Tao  and
      Liao, Lizi  and
      Cao, Yixin  and
      Liu, Yuanxing  and
      Liu, Ming  and
      Chen, Zerui  and
      Qin, Bing",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.acl-long.262/",
    doi = "10.18653/v1/2024.acl-long.262",
    pages = "4768--4791",
    abstract = "In proactive dialogue, the challenge lies not just in generating responses but in steering conversations toward predetermined goals, a task where Large Language Models (LLMs) typically struggle due to their reactive nature. Traditional approaches to enhance dialogue planning in LLMs, ranging from elaborate prompt engineering to the integration of policy networks, either face efficiency issues or deliver suboptimal performance. Inspired by the dual-process theory in psychology, which identifies two distinct modes of thinking{---}intuitive (fast) and analytical (slow), we propose the Dual-Process Dialogue Planning (DPDP) framework. DPDP embodies this theory through two complementary planning systems: an instinctive policy model for familiar contexts and a deliberative Monte Carlo Tree Search (MCTS) mechanism for complex, novel scenarios. This dual strategy is further coupled with a novel two-stage training regimen: offline Reinforcement Learning for robust initial policy model formation followed by MCTS-enhanced on-the-fly learning, which ensures a dynamic balance between efficiency and strategic depth. Our empirical evaluations across diverse dialogue tasks affirm DPDP`s superiority in achieving both high-quality dialogues and operational efficiency, outpacing existing methods."
}

@article{kahneman2011thinking,
  title={Thinking, fast and slow},
  author={Kahneman, Daniel},
  journal={Farrar, Straus and Giroux},
  year={2011}
}

@inproceedings{liu2024slow,
author = {Liu, Jijia and Yu, Chao and Gao, Jiaxuan and Xie, Yuqing and Liao, Qingmin and Wu, Yi and Wang, Yu},
title = {LLM-Powered Hierarchical Language Agent for Real-time Human-AI Coordination},
year = {2024},
isbn = {9798400704864},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {AI agents powered by Large Language Models (LLMs) have made significant advances, enabling them to assist humans in diverse complex tasks and leading to a revolution in human-AI coordination. LLM-powered agents typically require invoking LLM APIs and employing artificially designed complex prompts, which results in high inference latency. While this paradigm works well in scenarios with minimal interactive demands, such as code generation, it is unsuitable for highly interactive and real-time applications, such as gaming. Traditional gaming AI often employs small models or reactive policies, enabling fast inference but offering limited task completion and interaction abilities. In this work, we consider Overcooked as our testbed where players could communicate with natural language and cooperate to serve orders. We propose a Hierarchical Language Agent (HLA) for human-AI coordination that provides both strong reasoning abilities while keeping real-time execution. In particular, HLA adopts a hierarchical framework and comprises three modules: a proficient LLM, referred to as Slow Mind, for intention reasoning and language interaction, a lightweight LLM, referred to as Fast Mind, for generating macro actions, and a reactive policy, referred to as Executor, for transforming macro actions into atomic actions. Human studies show that HLA outperforms other baseline agents, including slow-mind-only agents and fast-mind-only agents, with stronger cooperation abilities, faster responses, and more consistent language communications.},
booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
pages = {1219–1228},
numpages = {10},
keywords = {hierarchical reasoning and planning, language agents, large language models, real-time human-ai coordination},
location = {Auckland, New Zealand},
series = {AAMAS '24}
}

@InProceedings{neil2018tom,
  title = 	 {Machine Theory of Mind},
  author =       {Rabinowitz, Neil and Perbet, Frank and Song, Francis and Zhang, Chiyuan and Eslami, S. M. Ali and Botvinick, Matthew},
  booktitle = 	 {Proceedings of the 35th International Conference on Machine Learning},
  pages = 	 {4218--4227},
  year = 	 {2018},
  editor = 	 {Dy, Jennifer and Krause, Andreas},
  volume = 	 {80},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {10--15 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v80/rabinowitz18a/rabinowitz18a.pdf},
  url = 	 {https://proceedings.mlr.press/v80/rabinowitz18a.html},
  abstract = 	 {Theory of mind (ToM) broadly refers to humans’ ability to represent the mental states of others, including their desires, beliefs, and intentions. We design a Theory of Mind neural network {–} a ToMnet {–} which uses meta-learning to build such models of the agents it encounters. The ToMnet learns a strong prior model for agents’ future behaviour, and, using only a small number of behavioural observations, can bootstrap to richer predictions about agents’ characteristics and mental states. We apply the ToMnet to agents behaving in simple gridworld environments, showing that it learns to model random, algorithmic, and deep RL agents from varied populations, and that it passes classic ToM tasks such as the "Sally-Anne" test of recognising that others can hold false beliefs about the world.}
}

@article{riemer2024can,
  title={Can Large Language Models Adapt to Other Agents In-Context?},
  author={Riemer, Matthew and Ashktorab, Zahra and Bouneffouf, Djallel and Das, Payel and Liu, Miao and Weisz, Justin D and Campbell, Murray},
  journal={arXiv preprint arXiv:2412.19726},
  year={2024}
}

@article{shinn2024reflexion,
  title={Reflexion: Language agents with verbal reinforcement learning},
  author={Shinn, Noah and Cassano, Federico and Gopinath, Ashwin and Narasimhan, Karthik and Yao, Shunyu},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@inproceedings{wang2024zsc,
  title={Zsc-eval: An evaluation toolkit and benchmark for multi-agent zero-shot coordination},
  author={Wang, Xihuai and Zhang, Shao and Zhang, Wenhao and Dong, Wentao and Chen, Jingxiao and Wen, Ying and Zhang, Weinan},
  booktitle={The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track},
  year={2024}
}

@article{wei2022chain,
  title={Chain-of-thought prompting elicits reasoning in large language models},
  author={Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Xia, Fei and Chi, Ed and Le, Quoc V and Zhou, Denny and others},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={24824--24837},
  year={2022}
}

@inproceedings{wester2024theory,
  title={Theory of Mind and Self-Presentation in Human-LLM Interactions},
  author={Wester, Joel and Jacobsen, Rune M{\o}berg and de Jong, Sander and Als, Naja Kathrine Kollerup and Djern{\ae}s, Helena B{\o}jer and van Berkel, Niels},
  booktitle={Adjunct Proceedings of the ACM SIGCHI Conference on Human Factors in Computing Systems},
  year={2024}
}

@article{yao2022react,
  title={React: Synergizing reasoning and acting in language models},
  author={Yao, Shunyu and Zhao, Jeffrey and Yu, Dian and Du, Nan and Shafran, Izhak and Narasimhan, Karthik and Cao, Yuan},
  journal={arXiv preprint arXiv:2210.03629},
  year={2022}
}

@article{yi2024survey,
  title={A Survey on Recent Advances in LLM-Based Multi-turn Dialogue Systems},
  author={Yi, Zihao and Ouyang, Jiarui and Liu, Yuwen and Liao, Tianhao and Xu, Zhe and Shen, Ying},
  journal={arXiv preprint arXiv:2402.18013},
  year={2024}
}

@article{yu2024distilling,
  title={Distilling system 2 into system 1},
  author={Yu, Ping and Xu, Jing and Weston, Jason and Kulikov, Ilia},
  journal={arXiv preprint arXiv:2407.06023},
  year={2024}
}

@inproceedings{zahra2021direction,
author = {Ashktorab, Zahra and Dugan, Casey and Johnson, James and Pan, Qian and Zhang, Wei and Kumaravel, Sadhana and Campbell, Murray},
title = {Effects of Communication Directionality and AI Agent Differences in Human-AI Interaction},
year = {2021},
isbn = {9781450380966},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411764.3445256},
doi = {10.1145/3411764.3445256},
abstract = {In Human-AI collaborative settings that are inherently interactive, direction of communication plays a role in how users perceive their AI partners. In an AI-driven cooperative game with partially observable information, players (be it the AI or the human player) require their actions to be interpreted accurately by the other player to yield a successful outcome. In this paper, we investigate social perceptions of AI agents with various directions of communication in a cooperative game setting. We measure subjective social perceptions (rapport, intelligence, and likeability) of participants towards their partners when participants believe they are playing with an AI or with a human and the nature of the communication (responsiveness and leading roles). We ran a large scale study on Mechanical Turk (n=199) of this collaborative game and find significant differences in gameplay outcome and social perception across different AI agents, different directions of communication and when the agent is perceived to be an AI/Human. We find that the bias against the AI that has been demonstrated in prior studies varies with the direction of the communication and with the AI agent.},
booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},
articleno = {238},
numpages = {15},
keywords = {social perception, human-AI interaction, games, collaboration},
location = {<conf-loc>, <city>Yokohama</city>, <country>Japan</country>, </conf-loc>},
series = {CHI '21}
}

@inproceedings{zhang2024proagent,
  title={ProAgent: building proactive cooperative agents with large language models},
  author={Zhang, Ceyao and Yang, Kaijie and Hu, Siyi and Wang, Zihao and Li, Guanghe and Sun, Yihang and Zhang, Cheng and Zhang, Zhaowei and Liu, Anji and Zhu, Song-Chun and others},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={38},
  number={16},
  pages={17591--17599},
  year={2024}
}

