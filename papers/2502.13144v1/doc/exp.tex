\section{Experiments}
\subsection{Experimental Settings}
\boldparagraph{Dataset and Benchmark.} We collect $2000h$ of expert human driving demonstrations in the real physical world. 
We get ground-truths of maps and agents in these driving demonstrations through a low-cost automated annotation pipeline. We use the map and agent labels as supervision for the first-stage perception pre-training. For the second-stage planning pre-training, we use the odometry information of the ego vehicle as supervision.
For the third-stage reinforced post-training,  we select 4305 critical dense-traffic clips of high collision risks from collected driving demonstrations and reconstruct these clips into 3DGS environments.
Of these, 3968 3DGS environments are used for RL training, and the other 337 3DGS environments are used as closed-loop evaluation benchmarks. 

\boldparagraph{Metric.} 
We evaluate the performance of the AD policy using nine key metrics. Dynamic Collision Ratio (DCR) and Static Collision Ratio (SCR) quantify the frequency of collisions with dynamic and static obstacles, respectively, with their sum represented as the Collision Ratio (CR). Positional Deviation Ratio (PDR) measures the ego vehicle’s deviation from the expert trajectory with respect to position, while Heading Deviation Ratio (HDR) evaluates the ego vehicle’s consistency to the expert trajectory with respect to the forward direction. The overall deviation is quantified by the Deviation Ratio (DR), defined as the sum of PDR and HDR. Average Deviation Distance (ADD) quantifies the mean closest distance between the ego vehicle and the expert trajectory before any collisions or deviations occur. Additionally, Longitudinal Jerk (Long. Jerk) and Lateral Jerk (Lat. Jerk) assess driving smoothness by measuring acceleration changes in the longitudinal and lateral directions. CR, DCR, and SCR mainly reflect the policy's safety, and ADD reflects the trajectory consistency between the AD policy and human drivers.

\begin{table*}[h]
    \centering
{
\begin{tabular}{lccccccccccc}
    \toprule
    Method  & CR$\downarrow$ & DCR$\downarrow$ & SCR$\downarrow$ & DR$\downarrow$ & PDR$\downarrow$ & HDR$\downarrow$ &ADD$\downarrow$ & Long. Jerk$\downarrow$ & Lat. Jerk$\downarrow$  \\
    \midrule
    VAD~\cite{vad} & 0.335 & 0.273     & 0.062  & 0.314 & 0.255  & 0.059   & 0.304 & 5.284 & 0.550 \\
    GenAD~\cite{zheng2024genad} & 0.341 & 0.299    & 0.042  & 0.291 & 0.160  & 0.131  & 0.265 & 11.37 & 0.320 \\
    VADv2~\cite{vadv2}  & 0.270   &0.240  &0.030  & 0.243 &0.139  &0.104  & 0.273 & 7.782 &0.171  \\
     \thename  & 0.089 & 0.080 & 0.009 & 0.063 & 0.042 & 0.021  & 0.257 & 4.495 & 0.082 \\
    \bottomrule
\end{tabular}
}
    \caption{\textbf{Closed-loop quantitative comparisons with other IL-based methods on the 3DGS dense-traffic evaluation benchmark.}}
    \label{tab:main}
\end{table*}

\begin{figure*}[h]
\centering
\vspace{1mm}
\includegraphics[width=0.98\textwidth]{fig/vs1-2.pdf} 
\vspace{2mm}
\caption{\textbf{Closed-loop qualitative comparisons between the IL-only policy and \thename{} in  3DGS environments.} Rows 1-2 correspond to Yield to Pedestrians. Rows 3-4 correspond to Unprotected Left-turn.}
\label{fig:vis1}
\end{figure*}

\subsection{Ablation Study}
To evaluate the impact of different design choices in RAD,  we conduct three ablation studies. These studies examine the balance between RL and IL, the role of different reward sources, and the effect of auxiliary objectives.

\boldparagraph{RL-IL Ratio Analysis.}  
We first analyze the effect of different RL-to-IL step mixing ratios (Tab.~\ref{tab:ratio}). A pure IL policy (0:1) results in the highest CR (0.229) but the lowest ADD (0.238), indicating strong trajectory consistency but poor safety. In contrast, a pure RL policy (1:0) significantly reduces CR (0.143) but increases ADD (0.345), suggesting improved safety at the cost of trajectory deviation. The best balance is achieved at a 4:1 ratio, which results in the lowest CR (0.089) while maintaining a relatively low ADD (0.257). Further increasing RL dominance (e.g., 8:1) leads to a deteriorated ADD (0.323) and higher jerk, implying reduced trajectory smoothness.

\boldparagraph{Reward Source Analysis.}  
We analyze the influence of different reward components (Tab.~\ref{tab:reward_ablation}). Policies trained with only partial reward terms (e.g., ID 1, 2, 3, 4, 5) exhibit higher collision rates (CR) compared to the full reward setup (ID 6), which achieves the lowest CR (0.089) while maintaining a stable ADD (0.257). This demonstrates that a well-balanced reward function, incorporating all reward terms, effectively enhances both safety and trajectory consistency. Among the partial reward configurations, ID 2, which omits the dynamic collision reward term, exhibits the highest CR (0.238), indicating that the absence of this term significantly impairs the model's ability to avoid dynamic obstacles, resulting in a higher collision rate.

\boldparagraph{Auxiliary Objective Analysis.}  
Finally, we examine the impact of auxiliary objectives (Tab.~\ref{tab:auxiliary_ablation}). Compared to the full auxiliary setup (ID 8), omitting any auxiliary objective increases CR, with a significant rise observed when all auxiliary objectives are removed. This highlights their collective role in enhancing safety. Notably, ID 1, which retains all auxiliary objectives but excludes the PPO objective, results in a CR of 0.187. This value is higher than that of ID 8, indicating that while auxiliary objectives help reduce collisions, they are most effective when combined with the PPO objective.

Our ablation studies highlight the importance of combining RL and IL, using a comprehensive reward function, and implementing structured auxiliary objectives. The optimal RL-IL ratio (4:1) and the full reward and auxiliary setups consistently yield the lowest CR while maintaining stable ADD, ensuring both safety and trajectory consistency.

\subsection{Comparisons with Existing Methods}
As presented in Tab.~\ref{tab:main}, we compare \thename{} with other end-to-end autonomous driving methods in the proposed 3DGS-based closed-loop evaluation. For fair comparisons, all the methods are trained with the same amount of human driving demonstrations. The 3DGS environments for the RL training in \thename{} are also based on these data.
\thename{} achieves better performance compared to IL-based methods in most metrics. Especially in terms of CR, \thename{} achieves $3\times$ lower collision rate, demonstrating that RL helps the AD policy learn general collision avoidance ability.


\subsection{Qualitative Comparisons}
We provide qualitative comparisons between the IL-only AD policy (without reinforced post-training) and \thename{}, as shown in Fig.~\ref{fig:vis1}.
The IL-only method struggles in dynamic environments, frequently failing to avoid collisions with moving obstacles or manage complex traffic situations. In contrast, RAD consistently performs well, effectively avoiding dynamic obstacles and handling challenging tasks. These results highlight the benefits of closed-loop training in the hybrid method, which enables better handling of dynamic environments. Additional visualizations are included in the Appendix (Fig.~\ref{fig:more-vis}). 