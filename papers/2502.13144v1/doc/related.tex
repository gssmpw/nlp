
\section{Related Work}
\begin{figure*}[ht]
\centering
\includegraphics[width=0.98\textwidth]{fig/framework-2.pdf} 
\caption{\textbf{Overall framework of \thename.}  \thename{} takes a three-stage training paradigm.
In the perception pre-training, ground-truths of map and agent are used to guide instance-level tokens to encode corresponding information. In the planning pre-training stage, large-scale driving demonstrations are used to initialize the action distribution. In the reinforced post-training stage, RL and IL synergistically fine-tune the AD policy. }
\label{fig:framework}
\end{figure*}
\boldparagraph{Dynamic Scene Reconstruction.}
Implicit neural representations have dominated novel view synthesis and dynamic scene reconstruction, with methods like UniSim~\cite{UniSim}, MARS~\cite{MARS}, and NeuRAD~\cite{NeuRAD} leveraging neural scene graphs to enable structured scene decomposition. However, these approaches rely on implicit representations, leading to slow rendering speeds that limit their practicality in real-time applications. In contrast, 3D Gaussian Splatting (3DGS)~\cite{3dgs} has emerged as an efficient alternative, offering significantly faster rendering while maintaining high visual fidelity. Recent works have explored its potential for dynamic scene reconstruction, particularly in autonomous driving scenarios. StreetGaussians~\cite{StreetGaussians}, DrivingGaussians~\cite{DrivingGaussian}, and HUGSIM~\cite{hugsim} have demonstrated the effectiveness of Gaussian-based representations in modeling urban environments. These methods achieve superior rendering performance while maintaining controllability by explicitly decomposing scenes into structured components. However, these works primarily leverage 3DGS for closed-loop evaluation. In this work, we incorporate 3DGS into an RL training framework.


\boldparagraph{End-to-End Autonomous Driving.}  Learning-based planning has shown great potential recently due to its data-driven nature and impressive performance with increasing amounts of data. UniAD~\cite{hu2023planning} demonstrates the potential of end-to-end autonomous driving by integrating multiple perception tasks to enhance planning performance. VAD~\cite{jiang2023vad} further explores the use of compact vectorized scene representations to improve efficiency. A series of works~\cite{li2024enhancing,paradrive,zheng2024genad,li2024ego,wang2024driving,gu2024producing,chen2025ppad,transfuser} also adopt the single-trajectory planning paradigm and further enhance planning performance. VADv2~\cite{vadv2} shifts the paradigm towards multi-mode planning by modeling the probabilistic distribution of a planning vocabulary. Hydra-MDP~\cite{hydramdp} improves the scoring mechanism of VADv2 by introducing additional supervision from a rule-based scorer. SparseDrive~\cite{sun2024sparsedrive} explores an alternative BEV-free solution. DiffusionDrive~\cite{diffusiondrive} proposes a truncated diffusion policy that denoises an anchored Gaussian distribution to a multi-mode driving action distribution. Most of the end-to-end methods follow the data-driven IL-training paradigm. In this work, we present an RL-training paradigm based on 3DGS.

\boldparagraph{Reinforcement Learning.}
Reinforcement Learning is a promising technique that has not been fully explored. AlphaGo~\cite{alphago} and AlphaGo Zero~\cite{alphago-zero} have demonstrated the power of Reinforcement Learning in the game of Go. Recently, OpenAI O1~\cite{openai-o1} and Deepseek-R1~\cite{deepseek-r1} have leveraged Reinforcement Learning to develop reasoning abilities. Several studies have also applied Reinforcement Learning in autonomous driving~\cite{toromanoff2020end, chen2021learning, roach, ILNotEnough, GUMP}. However, these studies are based on non-photorealistic simulators (such as CARLA~\cite{carla}) or do not involve end-to-end driving algorithms, as they require perfect perception results as input. To the best of our knowledge, \thename{} is the first work to train an end-to-end AD agent using Reinforcement Learning in a photorealistic 3DGS environment.