\section{Introduction}
\label{sec:introduction}
\begin{figure}[ht]
\centering
\vspace{1mm}
\includegraphics[width=1.0\linewidth]{fig/teaser-2.pdf} 
\caption{\textbf{Different training paradigms of end-to-end autonomous driving (AD).}  
}
\label{fig:teaser-2}
\end{figure}
End-to-end autonomous driving (AD) is currently a trending topic in both academia and industry. It replaces a modularized pipeline with a holistic one by directly mapping sensory inputs to driving actions, offering advantages in system simplicity and generalization ability.  
Most existing end-to-end AD algorithms~\cite{uniad,vad,zheng2024genad,paradrive,hydramdp,vadv2,sun2024sparsedrive,diffusiondrive} follow the Imitation Learning (IL) paradigm, which trains a neural network to mimic human driving behavior. However, despite their simplicity, IL-based methods face significant challenges in real-world deployment.  

One key issue is \textbf{causal confusion}. IL trains a network to replicate human driving policies by learning from demonstrations.  
However, this paradigm primarily captures correlations rather than causal relationships between observations (states) and actions.  
As a result, IL-trained policies may struggle to identify the true causal factors behind planning decisions, leading to shortcut learning~\cite{shortcut},  
\eg, merely extrapolating future trajectories from historical ones~\cite{egomlp,admlp}.  
Furthermore, since IL training data predominantly consists of common driving behaviors and does not adequately cover long-tailed distributions,  
IL-trained policies tend to converge to trivial solutions, lacking sufficient sensitivity to safety-critical events such as collisions.  

Another major challenge is the \textbf{gap between open-loop training and closed-loop deployment}.  
IL policies are trained in an open-loop manner using well-distributed driving demonstrations.  
However, real-world driving is a closed-loop process where minor trajectory errors at each step accumulate over time,  
leading to compounding errors and out-of-distribution scenarios.  
IL-trained policies often struggle in these unseen situations, raising concerns about their robustness.  

A straightforward solution to these problems is to perform closed-loop Reinforcement Learning (RL) training,  
which requires a driving environment that can interact with the AD policy.  
However, using real-world driving environments for closed-loop training poses prohibitive safety risks and operational costs.  
Simulated driving environments with sensor data simulation capabilities~\cite{carla,carsim} (which are required for end-to-end AD)  
are typically built on game engines~\cite{unreal,unity} but fail to provide realistic sensor simulation results.  

In this work, we establish a 3DGS-based~\cite{3dgs} closed-loop RL training paradigm.  
Leveraging 3DGS techniques, we construct a photorealistic digital replica of the real world,  
where the AD policy can extensively explore the state space and learn to handle out-of-distribution situations  
through large-scale trial and error. To ensure effective responses to safety-critical events  
and a better understanding of real-world causations, we design specialized safety-related rewards.  
However, RL training presents several critical challenges, which this paper addresses.  

One significant challenge is the \textbf{Human Alignment Problem}.  
The exploration process in RL can lead to policies that deviate from human-like behavior,  
disrupting the smoothness of the action sequence.  
To address this, we incorporate imitation learning as a regularization term during RL training,  
helping to maintain similarity to human driving behavior.  
As illustrated in Fig.~\ref{fig:teaser-2}, RL and IL work together to optimize the AD policy:  
RL enhances IL by addressing causation and the open-loop gap, while IL improves RL by ensuring better human alignment.  

Another major challenge is the \textbf{Sparse Reward Problem}.  
RL often suffers from sparse rewards and slow convergence.  
To alleviate this issue, we introduce dense auxiliary objectives related to collisions and deviations,  
which help constrain the full action distribution.  
Additionally, we streamline and decouple the action space to reduce the exploration cost associated with RL.  

To validate the effectiveness of our approach, we construct a closed-loop evaluation benchmark comprising diverse, unseen 3DGS environments. Our method, \texttt{\thename}, outperforms IL-based approaches across most closed-loop metrics, notably achieving a collision rate that is $3\times$ lower.

The contributions of this work are summarized as follows:
\begin{itemize}
    \item We propose the first 3DGS-based RL framework for training end-to-end AD policy. The reward, action space, optimization objective, and interaction mechanism are specially designed to enhance training efficiency and effectiveness.
    \item We combine RL and IL to synergistically optimize the AD policy. RL complements IL by modeling the causations and narrowing the open-loop gap, while IL complements RL in terms of human alignment.
    \item We validate the effectiveness of \thename{} on a closed-loop evaluation benchmark consisting of diverse, unseen 3DGS environments. \thename{} achieves stronger performance in closed-loop evaluation, particularly a $3\times$ lower collision rate, compared to IL-based methods.
\end{itemize}
