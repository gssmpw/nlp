\section{Scheduling Quantum Queries}
\label{sec:schedule}

The previous section examined the architectural design and hardware implementation of Fat-Tree QRAM. Optimizing query performance, however, also depends on efficient QPU-QRAM collaboration at the compiler level, particularly in scheduling query requests. This section introduces a latency-optimal scheduling algorithm for Fat-Tree QRAM and explores its full utilization by analyzing intrinsic quantum algorithm structures.

\subsection{Increasing Utilization of a Shared QRAM}

Fig.~\ref{fig:fullpip} illustrates that $\log(N)$ queries can be efficiently pipelined in $O(\log(N))$ circuit layers if executed consecutively. However, real algorithms may not issue queries uniformly, as seen in Fig.~\ref{fig:algopip}, which incorporates processing stages occupying $d$ circuit layers between queries. If queries occur at regular intervals of depth $d$, QRAM utilization will sometimes be below 1.

State-of-the-art QRAMs serve requests sequentially, yielding binary utilization (0 or 1). In contrast, a capacity-$N$ Fat-Tree QRAM pipelines $\log(N)$ queries, allowing utilization to vary between 0 and 1, enabling additional queries during QPU processing intervals. This suggests Fat-Tree QRAM can accommodate $\log(N)+d$ distributed QPUs rather than just $\log(N)$. Understanding algorithmic structures is key to maximizing QRAM utilization, especially in shared QRAM architectures interacting with multiple QPUs, further explored in Sec.~\ref{sec:results}.

\subsection{Offline and Online Query Scheduling}

The above discussion considers offline scheduling, where query intervals are predetermined. In practice, shared QRAM must handle online query requests, making scheduling more complex as QRAM lacks prior knowledge of QPU activity, and queries arrive at random intervals.

Using a greedy exchange proof, we demonstrate that First-In-First-Out (FIFO) scheduling minimizes total query latency for both offline and online cases (proof in Sec.~\ref{subsec:greedyproof}). Assume an optimal schedule deviating from FIFO, where a later-requested query is processed first. Swapping these queries to align with the request order does not worsen latency. Repeatedly applying this swap to all out-of-order query pairs transforms the schedule into FIFO while maintaining non-increasing latency. Thus, FIFO scheduling is optimal, ensuring minimal total latency.


\begin{figure*}[t]
     \centering
         \centering
         \includegraphics[width=0.7\textwidth, trim=0 1.5em 0 0]{Figures/algpip.pdf}
         \caption{Algorithm execution and query scheduling diagram with Fat-Tree QRAM. Every single query requires $10n-1$ circuit layers to finish for address width $n$, followed by $d$ circuit layers of QPU processing before the next query. In this example, QRAM is underutilized; that is, additional queries can be pipelined.}
         \label{fig:algopip}
\end{figure*}


