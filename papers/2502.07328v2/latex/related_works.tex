AI music generation has evolved rapidly with techniques such as autoregressive~\cite{agostinelli2023musiclm,c:23,ziv2024masked}, diffusion-based ~\cite{c:24,huang2023noise2music,li2024jen} and GAN-based ~\cite{dong2018musegan,li2021inco} producing high-quality music. Some of the works include adapter-based settings which proved effective for music editing and inpainting~\cite{lin2024arrange, zhang2024instruct}. Moreover,~\citet{lan2024musicongen} used adapters for rhythm and chord conditioning. ~\citet{tan2020automated} showcased how visual emotions from images can be effectively translated into music using deep learning techniques.

Drawing inspiration~\citet{c:27}, which systematically analyzes the under-representation of languages spoken by the global majority, we conduct a survey of the datasets and research papers on music generation.

\subsection{Data Collection}
To get our initial pool of papers, we implemented an efficient, automated data collection method.

We employed a multi-stage, keyword-based selection method, leveraging the Scholarly package~\citet{cholewiak2021scholarly} to gather approximately 5000 papers. This included up to 1000 papers per query, using broad search terms such as “music,” “music generation,” “non-Western music,” “MIDI,” and “symbolic music.” We then refined our selection by focusing on papers presented at 10 major conferences including \textit{IJCAI, AAAI, ICML, EURASIP, EUSIPCO, ISMIR, NeurIPS, SMC, NIME} and \textit{ICASSP}, chosen based on their popularity and prestige in the area of computational processing of music, narrowing our pool to around 800 papers. Conferences such as ISMIR and NIME specialize in music information retrieval and musical expression, frequently showcasing work related to generative AI. Additionally, conferences like ICASSP, AAAI, and NeurIPS are known for their focus on cutting-edge AI technologies, such as GANs and transformers, which are crucial for music generation.

\subsubsection{Dataset Papers}
To identify papers proposing datasets, we read through the title and abstract of each paper. This led to a set of  \textbf{152} papers proposing new datasets with a total of \textbf{1 million+} hours of music. These datasets were manually annotated for the region and genres covered, total hours of music data, and whether the dataset is annotated with other details (such as, instruments, genre, and style). Papers that directly provided details of the distribution of data points across genres and regions, were analyzed with the already available statistics. Unfortunately, several datasets did not offer substantial details necessary for our study. If such a dataset had more than \textit{\textbf{10,000}} hours of audio data, we analyzed each sound file's metadata to collect genre and region information. However, when the genre and region were not explicitly mentioned in either the paper or the metadata, we did not make any assumptions; thus, \textbf{7.9\%} of the datasets totaling\textbf{ 5,772} hours were excluded from our analysis. 

\subsection{Findings}
Our findings are summarized in Figure \ref{fig:dataset_infographic}. The results reveal an almost complete omission of musical genres from non-Western countries, especially those from the Global South. Approximately 94\% of the total hours in available datasets are dedicated to music from the Western world, while only 5.7\% are devoted to \textit{South Asian, Middle Eastern, Oceanian, Central Asian, Latin American}, and \textit{African} music combined. This imbalance is likely to cause poor-quality music generation for genres from the Global South. For detailed analysis, please refer to Appendix \ref{appendix:research_landscape}.
\input{tables/paper_table}

% \section{Related Works}

%AI-based music generation has garnered significant %attention, with recent advancements primarily focusing %on Transformer and Diffusion-based strategies for %generating high-quality music with varying levels of %control over musical elements.

%\noindent \textbf{Transformer based generative models} %: MusicGen () utilizes simple codebook interleaving %strategies to generate high quality music in stereo. %MAGNET () proposes a Transformer based non-%autoregressive architecture using a  single-stage %encoder and inference time rescoring of audio tokens to %speed up music generation.

%\noindent \textbf{Diffusion based generative models}
%Moûsai () uses a latent-diffusion model that can %generate longer audio clips in real time through text-%audio binding. Mustango () - is a controllable %diffusion based model with fine grained control over %the musical properties such as chord , key, beat, tempo % etc to guide the generation. However,
%such properties may not universal accross all cultures %and genres or may exist in some other forms.
%%
% Long version

% \section{Related Work}

% AI-based music generation has garnered significant attention, with recent advancements primarily focusing on Transformer and Diffusion-based strategies for generating high-quality music with varying levels of control over musical elements.

% \noindent \textbf{Transformer-based Generative Models}: MusicGen utilizes a codebook interleaving strategy that leverages the transformer architecture to generate coherent, high-fidelity stereo music by predicting audio tokens in a hierarchical manner. This approach allows the model to efficiently handle large music sequences while maintaining audio quality.

% MAGNET, another prominent model, introduces a non-autoregressive Transformer-based architecture for music generation. Unlike traditional autoregressive models that generate music token-by-token, MAGNET adopts a single-stage encoder and employs inference-time rescoring of predicted audio tokens.

% \noindent \textbf{Diffusion-based Generative Models}: Diffusion models, originally developed for generating high-resolution images, have shown great promise in audio generation. Moûsai applies a latent-diffusion model for music generation, where the model generates long audio clips by learning the intricate relationships between text descriptions and audio features. By binding text and audio representations, Moûsai is capable of creating contextually relevant and extended music pieces in real-time, which can adapt to specific prompts or genres.

% Mustango takes diffusion-based models a step further by introducing a controllable music generation framework. It provides users with fine-grained control over musical attributes such as chord progressions, key signatures, tempo, and other structural elements to guide the generation. This makes it particularly useful for tasks requiring precise musical composition. However, it is worth noting that these properties may not be universal across different musical traditions. For instance, non-Western music often follows different theoretical frameworks, and features like chord and key may either not exist or may be represented in alternative forms.


% \noindent \textbf{Music auto encoders}. \textit{Example}. Work 1 has done compression of music into quantized representation similar to VQ-VAE~\cite{}
% \noindent \textbf{Transformer based generative models}. \textit{Example}. MusicGen has adapted the transformer model based on similarity of linguistics and music~\cite{}\\
% \noindent \textbf{Diffusion based generative models}. \textit{Example.} Mustango has scaled vast amount of music data into diffusion model and achieved something\\
