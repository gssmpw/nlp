In this paper, for the first time, we systematically explored and established the skewed distribution of musical genres from around the world in datasets used for training Music-Language Models. Non-Western musical traditions are severely underrerpresented which naturally leads to disparate performance across genres in these models. We also demonstrate that PEFT-based techniques vary in effectiveness across different genres and models, further aggravating the challenges of overcoming the data scarcity problem.

As generative models continue to gain traction in the field of music generation and are expected to be used even more widely in the coming years, the misrepresentation and under-representation of the musical genres of the ``global majority" poses a significant threat to the inclusion of musical cultures from around the world. The skewed distribution in datasets, reflected in model outputs, can lead to several issues, including cultural homogenization, reinforcement of Western culture dominance \cite{crawford2016}, misrepresentation of musical styles, and most importantly, gradual decline leading to the disappearance of many musical genres~\cite{tan-2021,decolon_pop,intercontinental2023ai}. Therefore, it is critically important to prioritize the creation of inclusive music datasets and models, with an emphasis on under-represented musical genres.

%In the global scenario of AI-driven music generation, there exists a significant bias towards Western music genres. This leads to the under-representation of many non-Western musical traditions and potential homogenization of AI generated music. The introduction of adapter-based techniques tries to address this issue by allowing for efficient cultural adaptation of under-represented low resource genres like Hindustani Classical \& Turkish Makam. Mustango and MusicGen finetuned models both show scope for improvement in different genres upon finetuning showing that parameter efficient finetuning approach works on models but is difficult to extrapolate it to all genres and scenarios. 
%Our work also demonstrates the importance of pre-training data, as it forms the foundation upon which fine-tuning can enhance audio fidelity. A more diverse global model trained to cover a wide range of genres, would ensure the inclusion of musical traditions from across the globe, important for the preservation and celebration of global musical traditions.
%Through this work, we aim to broaden the horizons of music generation technology, making it more representative of the world's rich and diverse musical heritage. Our findings not only contribute to the field of computational efficiency but also raise important questions about cultural representation in AI models. By embracing cultural diversity in AI systems, we can create more inclusive and culturally sensitive technologies that respect and celebrate the world's multifaceted artistic expressions.