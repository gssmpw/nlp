% \vspace{-5pt}
\input{tables/objective_results_table}
\begin{figure*}[!t]
    \centering
    % First Image
    \includegraphics[width=\textwidth]{graphs/iaa_all.png} % Replace with your image file
    \caption{Inter Annotator Agreement metrics for Hindustani Classical \& Turkish Makam.}
    \label{iaa_evaluation}
\end{figure*}
We evaluated four models, Mustango Baseline (MTB), Mustangto Fine-tuned (MTF), MusicGen Baseline (MGB), and MusicGen Finetuned (MGF), on two genres using both objective metrics and human evaluation, providing both objective and subjective insights into model performance. 
\subsection{Automatic Metrics}
We sample 400 audio samples from the test set to form our test prompt corpus. For capturing the distance between generated audio and the test corpus we compute Fréchet Audio Distance(FAD), Fréchet Distance(FD) and Kullback-Leibler(KL) with Sigmoid activation. We utilize the AudioLDM~\cite{liu2023audioldm} toolkit for implementation of FAD, FD, and KL, with distributions computed using PANN-CNN14~\cite{Kong_Cao_Iqbal_Wang_Wang_Plumbley_2020} as the backbone model for extracting features for each audio sample. 

% The results are summarized in 
% Table \ref{tab:objective_model_metrics}. 

% The results for models trained on Hindustani Classical music are shown in Table~\ref{tab:objective_model_metrics}. Note that both finetuned version of MusicGen and Mustango outperform baseline models, achieving better performance across all the metrics. We see that finetuned Mustango results show substantial improvement whereas MusicGen performance remains close to its baseline, showing limited gains. This suggests that Mustango effectively learns and adapts to the dataset. ... (something that Mustango better than MusicGen for peft)

 Table~\ref{tab:objective_model_metrics} presents the performance metrics for models trained on Hindustani Classical music. Both finetuned versions of MusicGen and Mustango demonstrate superior performance compared to their baseline counterparts across all evaluated metrics. Notably, Mustango exhibits significant improvement after finetuning, whereas MusicGen shows only marginal gains. This disparity suggests that Mustango possesses a greater capacity for dataset-specific adaptation.

Adapter finetuning for Mustango model better incorporates domain-specific nuances of Hindustani Classical music, resulting in generated outputs that more closely align with the target style.

% Table~\ref{tab:objective_model_metrics} also shows observations for Turkish Makam. We can clearly see that it has same performance except for FD, where Mustango baseline performs on the same level as MusicGen baseline. However, its performance improves remarkably upon finetuning, surpassing all other models.

Table~\ref{tab:objective_model_metrics} additionally presents results for Turkish Makam generation. The performance trends mirror those observed in Hindustani Classical music. Mustango demonstrates strong improvement with one notable exception: the PSNR metric. For PSNR, the baseline Mustango model performs better.

\subsection{Human Evaluation}
To complement our objective metrics, we designed a rigorous human evaluation process, recognizing the crucial role of human perception in assessing music quality and authenticity. We begin by generating prompts for drawing audio inferences from the models based on Bloom's taxonomy criteria. Then we present the outputs to human judges to compare them in an arena setup~\cite{chiang2024chatbot, tts-arena}.  

We divided our process into two phases. In first phase, two annotators independently judged a portion of the same set of data points. This allowed us to compute inter-annotator agreement, a crucial measure of evaluation reliability. Disagreements were systematically discussed and resolved, refining our evaluation criteria. In phase two, annotators transitioned to single annotations per data point continuing evaluation of the rest audios. Finally, we compute ELO ratings of the models based on second phase annotations. 

\subsubsection{Material}

We introduce novel evaluation criteria based on Bloom's Taxonomy to assess a model's understanding of musical elements in text and their alignment with the generated audio using arena-style evaluation.

We evaluate the models under three conditions: \textbf{recall}, \textbf{analysis}, and \textbf{creativity}, by manually generating 10 prompts in each category~(Table \ref{tab:queries}).
\begin{itemize}
\setlength{\parskip}{0pt}
    \item \textbf{Recall}: Tests the model’s ability to reproduce combinations of melody, instrument, and rhythm from the fine-tuning data, testing effective memorization and recall.

     % \item \textbf{Recall} designed to evaluate how well a model can incorporate key musical elements from the provided prompt into its generated output.

    \item \textbf{Analysis}, We create novel combinations by substituting melodies, rhythms, or instruments, testing the model's adaptability beyond the training data.

    % \item \textbf{Analysis} designed to assess a model's ability to generate music that intelligently combines familiar cultural elements in novel ways.
    
    \item \textbf{Creativity}, We combine genres, blending melodies, rhythms, and instruments across styles to test the model's integration of underrepresented and over-represented genres.
    
    % \textbf{Creativity}, we modify these prompts by combining different genres and blending elements —such as melodic lines, rhythmic patterns, or instruments— across various genres, testing the model's ability to integrate its newly acquired knowledge of underrepresented genres with its existing understanding of over-represented genres.

    % \item \textbf{Analysis} designed to assess a model's ability to generate music that intelligently integrates newly acquired knowledge of underrepresented genres with existing overrepresented ones.
    
\end{itemize}

\noindent For each case, we generate model responses from all models, creating 120 total music samples. Since Mustango generates 10-second inferences at 16kHz, we process MusicGen outputs by clipping them to 10 seconds and downsampling to 16kHz to ensure uniform evaluation conditions.

\subsubsection{Method}

\begin{figure*}[!t]
    \centering
    % First Image
    \includegraphics[width=0.98\textwidth]{graphs/elo.png} % Replace with your image file
    \caption{Scaled ELO ratings for each model in Hindustani Classical and Turkish Makam music. Categories include OA: Overall Assessment, Inst.: Instrument Accuracy, MC: Melody Capture, RC: Rhythm Capture, and CR: Creativity. Ratings are provided for all query types and individual query categories.}
    \label{fig:results_evaluation}
\end{figure*}

We decided to go for a comparative evaluation of pieces instead of absolute judgments of pieces in isolation to control the subjectivity so that, the shorter, lower-sampling-rate music clips, are more effectively evaluated through comparison. For each comparison, the user receives a reference prompt and two anonymous audio samples (with the comparisons ordered randomly), followed by five comparative evaluation questions comparing the two audio generations on each criterion: Overall Aesthetic\textit{(OA)}, Instrument Accuracy\textit{(Inst.)}, Rhythm Capture\textit{(RC)}, Melody Capture\textit{(MC)}, and Creativity\textit{(CR)} since we provide these entities in the prompt and we are trying to assess the alignment of the text to the audio generated. For each criteria, we provide the annotator with 7 options: A $\gg$ B, A $>$ B, A = B, A $<$ B, A $\ll$ B, None, and Not Applicable (NA). Please refer to Appendix~\ref{appendix:annotation_det} for questions.

\textbf{In first phase,} we conduct four types of comparisons: \textit{baseline} vs. \textit{baseline},\textit{ baseline }vs.\textit{ fine-tuned} (for both models), and \textit{fine-tuned} vs. \textit{fine-tuned}. We request two avid listeners of each genre, who are aware of the nuances but not themselves professional musicians(demography details in Appendix \ref{appendix:annotator_demographics}), to annotate these samples. The annotation process begins by evaluating 36 comparisons for each genre—9 generations from each model per genre—compared across all models based on the five evaluation criteria. After the completion of first phase we compute the \textbf{Inter-Annotator Agreement(IAA)}, using \textbf{distance} and \textbf{direction-based kappa} scores. The distance-based Kappa quantifies the absolute differences in annotations by both the annotators whereas direction-based Kappa assesses consistency in preference order rather than the extent of preference. Detailed kappa-score calculation methods are provided in the Appendix \ref{appendix:kappa_score_compute}. 

% <FOLD THE FOLLOWING PARA INTO THE ABOVE. REFER TO THE SCREENSHOT IN THE APPENDIX>

% <INTRODUCE SUBSUBSECTION ON PHASE I AND pHASE 2 ANNOTATION. PRESENT MOTIVATION AS IAA.
% DESCRIBE THE ANNOTATOR AS TWO AVID LISTERNERS OF THE GENRES, WHO ARE AWARE OF THE NUANCES, BUT NOT THEMSELVES PROFESSIONAL MUSICIANS
% THEN DESCRIBE THE COMBINATIONS GENERATED, NUMBER OF SAMPLES JUDGED. SAY THAT THE ORDER OF PRESENTATION WAS RANDOM, AND THE ANNOTATORS DIDN'T KNOW WHICH PIECE WAS GENERATED BY WHICH MODEL.

% INTRODUCE PHASE ii - AGAIN WITH MOTIVATION (SEE ABOVE, i HAVE WRITTEN THE OUTLINE OF THE MOTIVATION IN THE BEGINNING OF THIS SECTION). THEN DESCRIBE AGAIN THE NEW COMBINATIONS AND THE REASON FOR CHOOSING THOSE (AS IN ONE PARTICULAR CASE WAS DROPPED AS THE DIFFERENCE WAS CLEAR BETWEEN THOSE MODELS). 

% <FOLD THE FOLLOWING PARA INTO THE ABOVE DESCRIPTIONS>

Figure \ref{iaa_evaluation} presents the kappa score and average agreement for each criterion.
In evaluating Hindustani Classical and Turkish Makam music, these metrics reveal distinct patterns across assessment criteria. As we can see from the figure, \textit{OA} scores range from 0.40 to 0.67, while \textit{Inst.} shows high agreement, with scores up to 0.89 due to its objectivity (Figure \ref{iaa_evaluation}). \textit{MC} achieves moderate agreement, and \textit{RC} scores are generally lower, particularly for Hindustani Classical (0.19 and 0.21), likely due to the complexity of rhythmic patterns within the short 10-second evaluation span. \textit{CR} consistently records the lowest scores, reflecting the subjective nature of this criterion(For more genre-wise and query-type-wise details refer to Appendix\ref{appendix:iaa}). After phase-I, we ask the annotators to discuss and re-annotate music samples for Inst. and RC criteria where disagreement is higher.

\textbf{In the second phase,} for Hindustani Classical, we removed the MGB vs MGF comparison since the trend made it clear that MGB is better with agreement from both the annotators. For Turkish Makam, we removed MTF vs MTB since MTB proved to be better(see Figure \ref{fig:results_evaluation}). After filtering we are left with 3 sets of comparisons, with 7 queries for each model, across 3 query types leading to 63 more comparisons for each genre. The annotations from both rounds are combined to compute each model’s ELO rating.

\subsubsection{ELO Ratings}
After comparing the model outputs, we compute ELO ratings (usually used to calculate the relative skill levels of players in a two-player game) for each model across all query types for each evaluation criterion.
For each criterion, we consider a single annotation as a single match between the models. If the annotator marks it as NA, then we omit it from the calculation, if A$=$B or None is marked we consider it as a draw and A$\gg$B, A$>$ is considered a win for A and vice-versa for the remaining cases. The details of computing ELO ratings are given in Appendix \ref{appendix:elo}.

The normalized ELO ratings are shown in Figure~\ref{fig:results_evaluation}. \textbf{For Hindustani Classical music}, over all queries, MTF outperforms all models. Interestingly, MGB is better than MTF, but MGF is judged least favourably, implying that while fine-tuning significantly improves Mustango, MusicGen's performance regresses considerably. These trends hold for all aspects (melody, rhythm, instrument) except creativity. The trends are most prominent for Recall queries, but also hold for Analysis queries, but completely reverses for creativity queries, where MTF performs the worst while MTB performs the best. Qualitative analysis of the generated pieces confirm this finding and shows that there was a strong effect of adaptation on Mustango which led to knowledge attrition and resultant poor performance on creative queries, which required the model to utilize previous knowledge.

For Turkish Makam music, MTF regresses significantly from MTB, for types of queries as well as on all aspects. While MGF performs slightly better than MGB on all queries for overall rating, the trends are not consistent across different aspects, or different types of queries. In fact, for Analysis queries, even MusicGen's performance significantly regresses for all aspects on finetuning. Thus, we can conclude that the PEFT technique explored here did not help boost the performance of the models for Turkish Makam music. 

%ELO ratings reveal that MusicGen-finetuned scores highest in Overall (1530) and Instrument (1542) categories, while MusicGen-baseline leads in Rhythm, Melody, and Creativity. For analysis queries, MusicGen-finetuned’s performance drops, with Mustango-baseline outperforming across all cases. For creative prompts, both Mustango and MusicGen baselines perform best. \textbf{MusicGen-finetuned excels in core categories, MusicGen-baseline shows versatility, and Mustango-baseline captures genre nuances, while Mustango-finetuned fails to improve upon the baseline.}


