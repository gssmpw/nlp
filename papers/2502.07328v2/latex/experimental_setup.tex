For our genre-adaptation experiments, we selected two distinct non-Western genres — Hindustani Classical~\cite{jairazbhoy1971rāgs} and Turkish Makam~\cite{signell2008makam} — both significantly underrepresented in music generation research and datasets, and two open source models -- MusicGen~\cite{c:23} and Mustango~\cite{melechovsky-etal-2024-mustango}. We begin by describing the dataset creation, followed by prompt generation, the models, adapter architectures and finally, the training process.

\subsection{Dataset Creation}
% First describe the structure of training data required ... a textual prompt and a audio - and the size of the audio...
% We need 30-second audio samples with accompanying text prompts to train the music generation models. MusicGen requires these samples at a 32 kHz sampling rate, while Mustango requires a 16 kHz sampling rate.


Our study necessitated diverse corpus of non-Western music with detailed metadata. The Dunya~\cite{porter-2013} which is part of the CompMusic project~\cite{serra2014creating}, emerged as the ideal choice, offering an extensive collection of over 1,300 hours of music across multiple non-Western genres. This corpus includes Carnatic, Hindustani, Turkish Makam, Beijing opera, and Arab Andalusian music, providing a broad spectrum of cultural music. We focused specifically on Hindustani Classical and Turkish Makam genres as both genres possess complex culturally specific melodic and rhythmic structures different from Western music \& we had easier access to listeners familiar with Indian and Turkish music. For Hindustani Classical, we chose the MTG Saraga~\cite{srinivasamurthy2021saraga} annotated dataset which is built on CompMusic offering 50 hours of audio. For Turkish Makam, we use the Dunya dataset API for accessing the metadata and audio samples leading to 405 hours of audio. 

To ensure consistency and manage computational resources effectively, we implemented several pre-processing steps. We standardized the audio sample length by truncating longer recordings to 30 seconds. We utilized the accompanying metadata from the Dunya corpus without modification. These descriptions, rich in genre-specific details, served as valuable inputs for creating prompt templates. Finally, to accommodate the differing requirements of our chosen models, we performed audio resampling. Specifically, for MusicGen we resampled the audio to a 32 kHz sampling rate and for Mustango 16 kHz sampling rate.

The metadata from the dataset provides genre-specific information for each audio clip, including three key details critical to our study: melodic line, rhythmic pattern, and instrumentation. For the melodic line, we extracted the \textit{raga} (a melodic framework in Hindustani Classical music) and \textit{Makam} (a system of melodic modes in Turkish music). For rhythmic patterns, we identified \textit{laya} (tempo) in Indian music and \textit{usul} (a sequence of rhythmic strokes) in Turkish music. Additionally, we extracted the meta-data for the instruments (including voice) played in each audio sample. Details of the dataset can be found in Appendix \ref{appendix:dataset_details}.

% Audio clips and corresponding metadata for Hindustani Classical and Turkish Makam genres were sourced from Dunya \cite{porter-2013}. The Dunya corpus is a collection of diverse music genres, including Carnatic, Hindustani, Turkish-makam, Beijing Opera, and Arab-Andalusian. The total size of the corpus is more than 1300 hours of music available. The corpus has 305 hours of audio \& metadata for Hindustani Classical \& 420 hours of audio \& metadata for Turkish Makam. The metadata provides genre-specific information for each audio clip, including three key details critical to our study: melodic line, rhythmic pattern, and instrumentation. For the melodic line, we extracted the \textit{raga} (a melodic framework in Hindustani Classical music) and \textit{makam} (a system of melodic modes in Turkish music). For rhythmic patterns, we identified \textit{laya} (tempo) in Indian music and \textit{usul} (a sequence of rhythmic strokes) in Turkish music. Additionally, we extracted the meta-data for the instruments (including voice) played in each audio sample. 


\begin{figure*}[!t]
    \centering
    \includegraphics[width=1\linewidth]{graphs/model_settings.pdf}
    \caption{Mustango \& MusicGen settings for low-resource fine-tuning.}
    \label{fig:architecture}
\end{figure*}
% The original audio clips, in stereo format at 44.1 kHz, are segmented into 30-second chunks to streamline model training. To meet model specifications, we down-sample these audio segments to both 16 kHz and 32 kHz and convert them to mono audio. This adjustment ensures compatibility with the models' training requirements. To capture essential metadata across each segment, we clip audio from 20\% to 95\% of its length that helps minimize the chances of excluding entities referenced in the metadata that might be present outside the clipped sections.

After pre-processing, we collected a total of 23.24 hours of audio for Hindustani Classical music and 121.16 hours for Turkish Makam music. The dataset was then divided using an 80-20\% split for training and testing, allowing us to evaluate the final model performance effectively. We ensured that audio clips for training and testing come from different songs to prevent distribution overlap in train and test. This split resulted in 18.91 hours of Hindustani Classical music and 97.23 hours of Turkish Makam music for training, and the remaining portions reserved for testing. 


\subsection{Prompt Generation}


To create effective prompts for model training, we created three distinct templates that describe each musical piece based on sample metadata from the selected genres. 

\begin{table}[!t]
\centering
\small
\setlength{\tabcolsep}{6pt} % Adjust column padding for compactness
\begin{tabularx}{\columnwidth}{lX} % Limit table width to column width
\toprule
\textbf{Query Type} & \textbf{Example} \\
\midrule
\textbf{Recall} & Imagine a traditional $\bigstar$ \textbf{Makam} performance that brings together $\triangleright$ Clarinet, $\triangleright$ Darbuka, $\triangleright$ Kanun, $\triangleright$ Oud, $\triangleright$ Voice, $\sharp$ Aksak makam, and $\flat$ Hicaz usul, flowing effortlessly. \\ \midrule
\textbf{Analysis} & Imagine a traditional $\bigstar$ \textbf{Makam} performance that brings together $\triangleright$ Tanbur, $\triangleright$ Oud, $\triangleright$ Cello, with the flowing essence of $\sharp$ Aksak makam and $\flat$ Fahte usul, flowing effortlessly. \\ \midrule
\textbf{Creativity} & Imagine a modern $\bigstar$ \textbf{Western Electronic Dance Music (EDM)} performance infused with the soulful sound of $\triangleright$ Tanbur, rich vocals blending with $\sharp$ Acem makam and $\flat$ Fahte usul. \\
\bottomrule
\end{tabularx}
\caption{Recall, Analysis \& Creativity Queries: Recall uses known combinations, while Analysis introduces novel combinations to test analytical capability and Creativity introduces cross-genre combinations. Refer to Section \ref{result}. \textbf{Genre}:$\bigstar$, melodic line:$\sharp$, rhythmic pattern:$\flat$, and instrumentation:$\triangleright$.}
\label{tab:queries}
\end{table}

For each audio sample, we randomly selected one of the three templates and populated it with relevant metadata attributes as shown in Table \ref{tab:queries}.

\noindent This process ensures that each prompt captures the unique musical elements of the sample. By maintaining this metadata-specific structure across prompts, we help the model learn to identify and respond to key attributes within each genre, enabling it to generate more accurate and culturally informed outputs during training.

\subsection{Models}
We utilize two state-of-the-art models, MusicGen~\cite{c:23} and Mustango~\cite{melechovsky-etal-2024-mustango}, to explore cross-genre adaptation. MusicGen is a transformer-based model, while Mustango integrates both diffusion and transformer architectures. We introduce adapters~\cite{pfeiffer-etal-2020-adapterhub} that enable low-resource fine-tuning. We also considered Moûsai \cite{c:24} and MusicLM \cite{agostinelli2023musiclm}, but Moûsai and MusicLM lack open-source training codes.

\subsubsection{MusicGen}
In MusicGen, we enhance the model with an additional 2 million parameters by integrating \textbf{Bottleneck Residual Adapter} after the transformer decoder within the MusicGen architecture after thorough experimentation with other placements. The total parameter count of MusicGen is 2 billion, making the adapter only 0.1\% of the total size. The adapter, as shown in Figure \ref{fig:architecture}, consists of a linear layer that compresses the embedding to a very small dimension, followed by a non-linear activation and projection back to the original size.

MusicGen leverages the Encodec~\cite{defossez2022highfi} framework, which compresses audio into latent representations. These latent representations are processed through a transformer model, which generates new music based on input prompts. By placing adapters at the end of the decoder, we achieve a lightweight adaptation mechanism that enhances the model’s ability to generate music in specific styles or regions, such as Hindustani Classical music and Makam, without modifying the fundamental Encodec structure.
\vspace{-1pt}
\subsubsection{Mustango}
In Mustango, we enhance the model with an additional 2 million parameters, which represents only 0.1\% of the model's total parameter count, by integrating a \textbf{Bottleneck Residual Adapter}. 

While Mustango supports chord and beat embeddings, we opted not to use them here due to the distinct focus of Hindustani Classical and Turkish Makam on melodic lines rather than harmonic progressions. Unlike Western classical music, these genres feature complex, rhythms with accents often within a single beat, making fixed beat and chord embeddings difficult to apply.

The adaptation process in Mustango begins with the FLAN-T5~\cite{flan-t5} model, which converts the input text into embeddings. These embeddings are then incorporated into the UNet architecture~\cite{10.1007/978-3-319-24574-4_28} through a cross-attention mechanism, aligning the text and audio components. To refine this process, a Bottleneck Residual Adapter with convolution layers is incorporated into the up-sampling, middle, and down-sampling blocks of the UNet, positioned immediately after the cross-attention block at the end of each stage (Figure \ref{fig:architecture}). The adapters reduce channel dimensions by a factor of 8, using a kernel size of 1 and GeLU activation after the down-projection layers to introduce non-linearity. Various adapter configurations and placements were explored to preserve the musical structure while adapting stylistic elements, with this setup yielding the best output quality. This design facilitates cultural adaptation while preserving computational efficiency. 

\subsection{Training settings}
For MusicGen fine-tuning, we used two RTX A6000 GPUs over a period of around 10 hours. The adapter block was fine-tuned, using the AdamW~\cite{Loshchilov2017DecoupledWD} optimizer with a learning rate of 5e-5 and a weight decay of 0.05 using MSE based Reconstruction Loss. The training spanned 20 epochs, with a patience threshold of 5 epochs for early stopping based on validation loss. We utilized a batch size of 4 and applied gradient clipping with a maximum norm of 1.0. The training data was split into 90\% for training and 10\% for validation.

For Mustango model fine-tuning, we used one RTX A6000 GPU over a period of 12 hours.  The adapter block was fine-tuned, using the AdamW optimizer with a learning rate of 4.5e-5 and a weight decay of 0.01 using MSE based Reconstruction Loss. The training spanned 25 epochs for both genres, with a patience threshold of 5 epochs for early stopping based on validation loss. The training data was split into 80\% for training and 20\% for validation. 