\appendix % Start of the appendix

% \section{Appendix A: Current Research Scenario Study Details}
% This is some additional information in Appendix A.

% \section{Appendix B: Methodology Details}
% This is optional. If any details need to be mentioned. Just adding a heading for reference purposes.
\section{Research Landscape} \label{appendix:research_landscape}
\begin{table}[htbp]
\centering
\resizebox{\columnwidth}{!}{
\begin{tabular}{lrr}
\toprule
\textbf{Region} & \textbf{Papers (count)} & \textbf{Duration (hrs.)} \\ \midrule 
\textbf{European}    & $66$ & $\mathbf{6127.92}$ \\ \midrule
East Asian           & $\mathbf{71}$ & $2746.73$ \\ 
South Asian          & $1$ & $88.78$ \\
Central Asian        & $0$ & $57.01$ \\ \midrule
American             & $72$ & $921.84$ \\ 
Latin American       & $5$ & $323.25$ \\ \midrule
Oceania              & $3$ & $41.99$ \\ \midrule
African              & $0$ & $27.50$ \\ \midrule
Middle Eastern       & $5$ & $37.86$ \\ \bottomrule
\end{tabular}}
\caption{Distribution of Papers and Duration in hours by Region.}
\label{tab:region_table}
\end{table}

\subsection{Genre Distribution Analysis}

Genre-wise analysis we can observe in Table \ref{tab:genre_table} and that \textit{Pop} music forms 200K+ hours of the corpus while \textit{Folk} music constitutes only 20K hours of the corpus which is a 10 times between the two as shown in Table \ref{tab:genre_table}. \textit{Pop, Rock, Classical} and \textit{Electronic} music genres each constitute more than 10\% of the total corpus and more than 100K hours in the total corpus. As shown in Figure \ref{fig:dataset_infographic}, \textit{Pop} music has the highest (19.3\%) representation followed by \textit{Rock}~(17.4\%) and \textit{Classical}~(13.5\%) genres. \textit{Country, hip-hop}\textit{, blues }and \textit{jazz} have a moderate~(more than 5\%) representation in the corpus. \textit{Folk} and \textit{experimental} music contribute to only 2.1\% of the corpus. The other genres receive minimal attention(\textbf{$\leq 1\%$}) which includes music for Children, \textit{Indie-music}, and region-specific genres.
 
\subsection{Regional Distribution Analysis}
In region-wise analysis, from analyzing the research space we find that more than 6k hours of music in the research belongs to \textit{European} music and only 28 hours of music belong to \textit{African} music as shown in Table \ref{tab:region_table}. \textit{European}, \textit{East Asian} and\textit{ American} music are well explored forming 84.5\% of the corpus. On the other hand, \textit{South Asian, Middle Eastern, Central Asian} and \textit{African} music each contribute less than 1\% to the whole corpus as depicted in Figure \ref{fig:dataset_infographic}.

\begin{table}[htbp]
\centering
\resizebox{0.9\columnwidth}{!}{
\begin{tabular}{lrr}
\toprule
\textbf{Genre} & \textbf{Papers (count)} & \textbf{Duration (hrs.)} \\ \midrule
\textbf{Pop} & \textbf{24} & \textbf{206.89} \\
Rock & 7 & 186.67 \\
Electronic & 36 & 140.25 \\ \midrule
Classical & 91 & 144.64 \\
Country & - & 95.77 \\
Hip-hop & 3 & 64.35 \\ \midrule
Jazz & 15 & 60.62 \\
Blues & - & 64.01 \\
Easy Listening & 2 & 74.39 \\ \midrule
Folk & 3 & 22.802 \\ 
Experimental & 26 & 11.310 \\
Others & 15 & 0.94 \\
\bottomrule
\end{tabular}}
\caption{Distribution of Hours and Papers by Genre. Duration (Dur.) is represented as 10\textsuperscript{3} hours.}
\label{tab:genre_table}
\end{table}

\section{Annotation Details} \label{appendix:annotation_det}
We asked annotators to choose between two audio samples, based on their preference, to select which better represents the prompted culture in both the inter-annotator agreement scenario and human evaluation. For both inter-annotator agreement and human evaluation, we relied on the same set of questions outlined below.
\begin{itemize}
\setlength{\parskip}{0pt}
    \item Overall, which piece do you like more?
    \item Which piece captures the instrument (if mentioned the prompt) better?
    \item Which piece captures the melodic line/scale (if mentioned the prompt) better?
    \item Which piece captures the rhythm/tempo (if mentioned the prompt) better?
    \item Which piece is more creative (ignore audio quality while answering this question)?
\end{itemize}

\section{Evaluation of Inter Annotator Agreement Results} \label{appendix:iaa}
\input{tables/iaa_table}
The inter-annotator agreement (IAA) results, measured using Cohen's Kappa, reveal interesting patterns across genres, metrics, and query types. 

In Table~\ref{tab:inter_annotator_agreement} Turkish Makam consistently showed higher agreement (0.57-0.67) than Hindustani Classical (0.40-0.60), suggesting potentially clearer structural elements. This trend is particularly pronounced in Rhythm (RC) annotations, where Turkish Makam exhibits substantially higher agreement (0.58-0.76) compared to Hindustani Classical (0.19-0.21).

% Across both genres, distance-based metrics yielded higher agreement than direction-based metrics. This indicates that annotators find it easier to agree on relative magnitudes rather than precise directional changes in musical elements. The difference is most noticeable in Creative Response (CR) for Hindustani Classical, where agreement jumps from 0.02 (direction) to 0.25 (distance).

Instrument identification (Inst.) showed high agreement across both genres (0.57-0.89), with Hindustani Classical scoring particularly well (0.89 for direction). Creativity (CR) exhibited the lowest overall agreement (0.02-0.55), reflecting the inherent subjectivity in assessing creativity.

% This suggests that distinguishing instruments is a more objective task, less prone to individual interpretation.

Examining query types in Table~\ref{tab:inter_annotator_agreement_query} reveals that Recall queries generally yielded higher agreement, particularly in Turkish Makam (0.74-0.75). This indicates strong consistency in factual recall tasks. Analysis queries showed mixed results, with some categories in Hindustani Classical even showing negative agreement, pointing to potential confusion or divergent interpretations in analytical tasks. Interestingly, Creativity queries showed perfect agreement (1.0) in Melody for both genres, suggesting a strong consensus in perceiving creative aspects of melody.

\section{Evaluation of Human Evaluation Results} \label{appendix:human_eval_result}
\input{tables/subjective_results_table}

The human evaluation results in Table~\ref{tab:overall_model_metrics} and Table~\ref{tab:overall_model_metrics_query}, measured using ELO ratings, also reveal intriguing patterns across genres, models, and query types. Comparing the two genres, we observe distinct performance profiles for each model. In Hindustani Classical Music, the Mustango finetuned model emerges as the clear leader (OA: 1577), outperforming other models across most categories, particularly excelling in Melodic Contour (MC: 1623). This suggests a strong grasp of the melodic structures specific to Hindustani music. Conversely, for Turkish Makam, the MusicGen finetuned model takes the lead (OA: 1597), with both MusicGen and Mustango baseline models also performing well.

% Interestingly, the Mustango finetuned model, which excels in Hindustani Classical, performs poorly in Turkish Makam (OA: 1337). This stark contrast indicates a potential overfitting to Hindustani Classical characteristics, highlighting the challenge of creating models that generalize well across diverse musical traditions.

The MusicGen Baseline shows remarkable consistency across both genres, often scoring above 1500 in various categories. This suggests a robust general understanding of musical elements that transcends genre boundaries. The Mustango Baseline, while competitive, generally scores lower than MusicGen Baseline, especially in Hindustani Classical Music.

Finetuning yields mixed results across the two models. For Mustango, it significantly improves performance in Hindustani Classical but drastically reduces its effectiveness in Turkish Makam. Conversely, MusicGen's finetuning slightly lowers its performance in Hindustani Classical but enhances it in Turkish Makam. This divergence underscores the complexity of adapting models to specific musical traditions without losing generalizability.

Examining performance across query types reveals further insights. In Recall queries for Hindustani Classical, Mustango finetuned significantly outperforms other models (OA: 1630), particularly in Melody (1668). For Turkish Makam, MusicGen Baseline leads in Recall queries (OA: 1512), with MusicGen finetuned close behind (OA: 1530). This suggests that finetuning can enhance a model's ability to accurately reproduce genre-specific musical elements.

Creativity queries yield particularly interesting results, with baseline models outperforming their finetuned counterparts in both genres. In Hindustani Classical, Mustango Baseline leads (OA: 1553), while in Turkish Makam, it shares the top position with MusicGen Baseline (OA: 1508 and 1566 respectively). This suggests that finetuning, while beneficial for recall and analysis, might constrain the model's creative capabilities.

\section{Kappa Score Computation} \label{appendix:kappa_score_compute}
\begin{table}[h!]
    \centering
    \[
    \begin{array}{c|ccccc}
        & \text{A$\gg$B} & \text{A$>$B} & \text{A$=$B} & \text{A$<$B} & \text{A$\ll$B} \\ \hline
    \text{A$\gg$B} & 1 & 1 & 0.33 & 0 & 0 \\
    \text{A$>$B} & 1 & 1 & 0.67 & 0.33 & 0 \\
    \text{A$=$B} & 0.33 & 0.67 & 1 & 0.67 & 0.33 \\
    \text{A$<$B} & 0 & 0.33 & 0.67 & 1 & 1 \\
    \text{A$\ll$B} & 0 & 0 & 0.33 & 1 & 1 \\
    \end{array}
    \]
    \caption{Matrix representation of distance-based agreement score for Inter Annotator Agreement. Column represents Annotator-1's preference and Row represents Annotator-2's preference.}
    \label{tab:iaa_distance}
\end{table}
\subsection{Distance-based Computation Matrix}
The \textbf{distance-based Kappa} quantifies the absolute differences in annotations by both the annotators. Each option(except None \& NA) is assigned a value between 2 to -2 in order; A $\gg$ B, A $>$ B, A = B, A $<$ B, A $\ll$ B. After assigning values we calculate absolute distances between annotator preferences while excluding all cases which are annotated None or NA by annotators. The distance values are clipped to a maximum of 3, with agreement computed as follows : 
\begin{equation*}
    p_o^{i} = \frac{3 - d}{3}
\end{equation*}

Table \ref{tab:iaa_distance} represents the annotator preferences and the agreement score for each combination.

\subsection{Direction-based Computation Matrix}

\begin{table}[h!]
    \centering
    \[
    \begin{array}{c|ccccc}
        & \text{A$\gg$B} & \text{A$>$B} & \text{A$=$B} & \text{A$<$B} & \text{A$\ll$B} \\ \hline
    \text{A$\gg$B} & 1 & 1 & 1 & 0 & 0 \\
    \text{A$>$B} & 1 & 1 & 1 & 0 & 0 \\
    \text{A$=$B} & 1 & 1 & 1 & 1 & 1 \\
    \text{A$<$B} & 0 & 0 & 1 & 1 & 1 \\
    \text{A$\ll$B} & 0 & 0 & 1 & 1 & 1 \\
    \end{array}
    \]
    \caption{Matrix representation of direction-based agreement score for Inter Annotator Agreement. Column represents Annotator-1's preference and Row represents Annotator-2's preference.}
    \label{tab:iaa_direction}
\end{table}


The \textbf{direction-based Kappa} assesses consistency in preference order rather than the extent of preference. A disagreement is defined as only when the preference orders are reversed between the two annotators (i.e., when one annotator chooses A$<$B or A$\ll$B and the other annotator chooses B$<$A or B$\ll$A) Agreement is calculated as follows : 
\begin{equation*}
    p_o^{i} = 1 - d
\end{equation*}

Table \ref{tab:iaa_direction} shows the agreement scores between different annotations.


Observed agreement is averaged per criterion, with expected agreement(\(p_e\)) and Kappa(\(\kappa\)) calculated as follows:
\begin{equation*}
\kappa = \frac{\frac{1}{n} \sum_{i=1}^{n} p_o^i - p_e}{1 - p_e}
\end{equation*}



\section{Dataset Details} \label{appendix:dataset_details}
For Hindustani Classical, the dataset includes five instrument types—sarangi, harmonium, tabla, violin, and tanpura—along with voice. It comprises 41 ragas across two laya types: Madhya and Vilambit.

For Turkish Makam, the dataset features 15 makam-specific instruments, including the oud, tanbur, ney, davul, clarinet, kös, kudüm, yaylı tanbur, tef, kanun, zurna, bendir, darbuka, classical kemençe, rebab, and çevgen. It encompasses 93 different makams and 63 distinct usuls.


\section{ELO Ratings Computation} \label{appendix:elo}
For phase I, the total number of evaluations are 36 by each annotator and we consider each annotation as a single match. In Phase II, 63 additional annotations are conducted making a total of 135 matches for computing the ELO ratings. For every match the new rating : 
\begin{equation*}
R_i = R_i + K * (S_i - E_i)
\end{equation*}
$R_i$: Player's current Elo rating.\newline
$K$: Weighting factor that determines how much a single game affects the rating. \newline
$S_i$: Outcome of the game for the player: 1 for a win, 0.5 for a draw, and 0 for a loss.

\begin{equation*}
    E_i = \frac{1}{(1 + 10^{\frac{(R_j - R_i)}{400}})}
    , E_j = \frac{1}{(1 + 10^{\frac{(R_i - R_j)}{400}})}
\end{equation*}
$R_i$: Player- 1 current Elo rating.\newline
$R_j$: Player- 2 current Elo rating.\newline
$E_i$: Expected Elo rating of Player-1.

We use a K value of 15 for calculations due to the limited number of matches; a higher K would disproportionately weight each match and skew the ELO ratings.


\section{Annotation Tool} \label{appendix:annotation_tool}

\begin{figure*}
    \centering
    \begin{minipage}{\textwidth}
        \centering
        \includegraphics[width=1\linewidth]{graphs/labelstudio-screenshot2.png}
        \label{fig:enter-label-1}
    \end{minipage}
    \vspace{1em}  % Adjust this value to control vertical spacing between figures
    \begin{minipage}{\textwidth}
        \centering
        \includegraphics[width=1\linewidth]{graphs/labelstudio-screenshot.png}
        \caption{Screenshots of Label Studio, annotation tool for Inter Annotator Agreement and ELO rating comparison}
        \label{fig:annotation_tool}
    \end{minipage}
\end{figure*}

We deployed LabelStudio, a versatile and user-friendly annotation tool, on HuggingFace Spaces. Figure~\ref{fig:annotation_tool} provides a visual representation of our annotation tool interface, illustrating the layout and features that our human evaluators used to assess the generated music samples.

\section{Annotator Demographics} 
\label{appendix:annotator_demographics}
The annotators for our music generation task using adapter models include three individuals from India and one from Uzbekistan, all of whom are avid music listeners with diverse cultural backgrounds and a keen interest in music technology.

