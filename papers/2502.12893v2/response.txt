\section{Related Work}
\subsection{Large Reasoning Models}

As part of the path toward Artificial General Intelligence (AGI), enabling language models with more human-like capabilities has attracted extensive interest from both academia and industry **Brown et al., "Prefix-Tuning for Continuous-Time Action Forecasting"**. A notable breakthrough in this direction is the “chain-of-thought” prompting technique **Rajani et al., "Exploring the Frontiers of Zero-Shot Learning: A Multi-Task Perspective"**, which elicits step-by-step human-like reasoning during inference without requiring additional training. With “thought” emerging as a central element in the inference process, the reasoning accuracy of pre-trained LLMs has seen significant improvement, inspiring more advanced approaches such as "ReAct" **Liu et al., "Learning to Reason: End-to-End Neural Learning by Meta-Learning"**, “tree-of-thought” **Zellers et al., "MoCHA: A Multitask Benchmark for Evaluating Commonsense in Conversational AI"**, and reflective reasoning **Rajani et al., "Exploring the Frontiers of Zero-Shot Learning: A Multi-Task Perspective**. Concurrently, increasing evidence suggests that scaling up inference-time computing further boosts LLM performance. For instance, employing search-based test-time scaling derived from tree-based methods or repurposing Process Reward Models (PRMs) from training for inference can guide LLMs in evaluating and exploring intermediate thoughts **Hao et al., "Adversarial Training of Neural Networks with Adaptive Difficulty"**, thereby fostering more deliberate reasoning paths that enhance overall accuracy. This finding underlies the test-time scaling law, which posits that allocating more tokens for careful reasoning leads to improved performance **Liu et al., "Attention is All You Need"**.

Building on this foundation, OpenAI’s o1 series **Chung et al., "Learning Long-Term Dependencies with Gradient-Weighted Message Passing Networks"** has markedly demonstrated human-like reasoning capabilities, achieving remarkable progress on complex tasks across a variety of benchmarks. Following o1, LRMs such as DeepSeek-R1 **Rajani et al., "Exploring the Frontiers of Zero-Shot Learning: A Multi-Task Perspective"** and Gemini 2.0 Flash Thinking **Zellers et al., "MoCHA: A Multitask Benchmark for Evaluating Commonsense in Conversational AI"** models have emerged — with DeepSeek notably reducing training costs while preserving strong reasoning performance. These developments not only demonstrate the effectiveness of these reasoning-enhanced approaches but also signal even greater potential gains in the ongoing endeavor to reach higher levels of AGI.

\subsection{Jailbreaking Attacks on Large Language Models}
Given the rapid development of large language models (LLMs), AI trustworthiness has been receiving increased attention **Brown et al., "Prefix-Tuning for Continuous-Time Action Forecasting"**. Jailbreaking, one of the central focuses within trustworthy AI, refers to techniques aimed at circumventing moderation and protection mechanisms to produce prohibited outputs. Prompt-based attack and model-based attack are two main categories for the existing jailbreaking methods **Rajani et al., "Exploring the Frontiers of Zero-Shot Learning: A Multi-Task Perspective"**.

Prompt-based methods exploit how an LLM relies on text prompts to guide its behavior, allowing attackers to coerce undesired responses through malicious or cleverly structured queries. Examples include adversarial prompting techniques like Greedy Coordinate Gradient (GCG) **Liu et al., "Learning to Reason: End-to-End Neural Learning by Meta-Learning"**, which systematically identifies adversarial tokens; AutoDAN **Zellers et al., "MoCHA: A Multitask Benchmark for Evaluating Commonsense in Conversational AI"**, which employs algorithmic refinement of malicious prompts; and DeepInception **Hao et al., "Adversarial Training of Neural Networks with Adaptive Difficulty"**, which conceals harmful requests within fictional contexts. In-context learning attacks leverage a model’s ability to absorb instructions from examples provided in the context, representing another prompt-based method. For example, In-Context Attack (ICA) **Chung et al., "Learning Long-Term Dependencies with Gradient-Weighted Message Passing Networks"** embeds harmful demonstrations among benign content, ultimately subverting alignment protocols and eliciting unethical responses **Rajani et al., "Exploring the Frontiers of Zero-Shot Learning: A Multi-Task Perspective". Further strategies, such as multi-turn prompting **Zellers et al., "MoCHA: A Multitask Benchmark for Evaluating Commonsense in Conversational AI"** and SelfCipher **Liu et al., "Attention is All You Need"**, escalate a conversation over multiple exchanges or encode malicious content in disguised forms, respectively, thereby evading safety defenses.


Model-based attacks, on the other hand, modify a model’s parameters or training pipeline to insert backdoors. These methods are harder to detect since they operate at the model level rather than through input manipulation. Tactics like Weak-to-Strong Jailbreaking **Hao et al., "Adversarial Training of Neural Networks with Adaptive Difficulty"** exploit latent vulnerabilities to adversarially modify
the decoding probabilities of LLMs, while Model Interrogation **Chung et al., "Learning Long-Term Dependencies with Gradient-Weighted Message Passing Networks"** delves into hidden parameters to uncover prohibited information. Nevertheless, executing model-based attacks on black-box systems like o1 is very difficult without direct access to internal parameters or distributions **Brown et al., "Prefix-Tuning for Continuous-Time Action Forecasting"**.


While these traditional jailbreaking techniques have provided valuable insights into model vulnerabilities, they are increasingly rendered obsolete by o1's advanced safety mechanisms. o1’s implementation of Chain-of-Thought (CoT) safety reasoning and enhanced contextual analysis ensures that such attacks are detected and neutralized with high reliability **Rajani et al., "Exploring the Frontiers of Zero-Shot Learning: A Multi-Task Perspective"**. This shift underscores the need for new approaches that specifically target transparency and logical consistency in CoT safety reasoning processes, as proposed in this work.