\section{Related Work}
\subsection{Large Reasoning Models}

As part of the path toward Artificial General Intelligence (AGI), enabling language models with more human-like capabilities has attracted extensive interest from both academia and industry \citep{llama3modelcard,anil2023palm,openai-chatgpt,GPT4report,team2023gemini,NEURIPS2024_0939f13f,joren2024sufficient,zhang2024mllm, zhang2024artist, yao2022react,liu2024visual,zhang-etal-2023-reaugkd,openai2024o3mini,guo2025deepseek,zhong2024evaluationopenaio1opportunities}. A notable breakthrough in this direction is the “chain-of-thought” prompting technique \citep{wei2023chainofthoughtpromptingelicitsreasoning}, which elicits step-by-step human-like reasoning during inference without requiring additional training. With “thought” emerging as a central element in the inference process, the reasoning accuracy of pre-trained LLMs has seen significant improvement, inspiring more advanced approaches such as "ReAct" \citep{yao2022react}, “tree-of-thought” \citep{yao2024tree}, and reflective reasoning \citep{Renze2024SelfReflectionIL,zeng2024perceivereflectplandesigning}. Concurrently, increasing evidence suggests that scaling up inference-time computing further boosts LLM performance. For instance, employing search-based test-time scaling derived from tree-based methods or repurposing Process Reward Models (PRMs) from training for inference can guide LLMs in evaluating and exploring intermediate thoughts \citep{zhang2024restmcts,openai2024openaio1card}, thereby fostering more deliberate reasoning paths that enhance overall accuracy. This finding underlies the test-time scaling law, which posits that allocating more tokens for careful reasoning leads to improved performance \citep{openai2024openaio1card}.

Building on this foundation, OpenAI’s o1 series \citep{zhong2024evaluationopenaio1opportunities} has markedly demonstrated human-like reasoning capabilities, achieving remarkable progress on complex tasks across a variety of benchmarks. Following o1, LRMs such as DeepSeek-R1 \citep{guo2025deepseek} and Gemini 2.0 Flash Thinking \citep{ deepmind2024FlashThinking} models have emerged — with DeepSeek notably reducing training costs while preserving strong reasoning performance. These developments not only demonstrate the effectiveness of these reasoning-enhanced approaches but also signal even greater potential gains in the ongoing endeavor to reach higher levels of AGI.

\subsection{Jailbreaking Attacks on Large Language Models}
Given the rapid development of large language models (LLMs), AI trustworthiness has been receiving increased attention \cite{zou2023universaltransferableadversarialattacks,peng2024jailbreakingmitigationvulnerabilitieslarge,zhao2024weaktostrongjailbreakinglargelanguage,guan2024deliberative,openai2024openaio1card,yao2024federated,zhang2024min,federatedgpt,kuo2025proactive}. Jailbreaking, one of the central focuses within trustworthy AI, refers to techniques aimed at circumventing moderation and protection mechanisms to produce prohibited outputs. Prompt-based attack and model-based attack are two main categories for the existing jailbreaking methods \citep{peng2024jailbreakingmitigationvulnerabilitieslarge,yi2024jailbreakattacksdefenseslarge}. 


Prompt-based methods exploit how an LLM relies on text prompts to guide its behavior, allowing attackers to coerce undesired responses through malicious or cleverly structured queries. Examples include adversarial prompting techniques like Greedy Coordinate Gradient (GCG) \citep{zou2023universaltransferableadversarialattacks}, which systematically identifies adversarial tokens; AutoDAN \citep{liu2024autodangeneratingstealthyjailbreak}, which employs algorithmic refinement of malicious prompts; and DeepInception \citep{li2024deepinceptionhypnotizelargelanguage}, which conceals harmful requests within fictional contexts. In-context learning attacks leverage a model’s ability to absorb instructions from examples provided in the context, representing another prompt-based method. For example, In-Context Attack (ICA) \citep{Wei2023JailbreakAG} embeds harmful demonstrations among benign content, ultimately subverting alignment protocols and eliciting unethical responses \citep{Wei2023JailbreakAG}. Further strategies, such as multi-turn prompting \citep{ren2024derail} and SelfCipher \citep{yuan2024gpt4smartsafestealthy}, escalate a conversation over multiple exchanges or encode malicious content in disguised forms, respectively, thereby evading safety defenses.


Model-based attacks, on the other hand, modify a model’s parameters or training pipeline to insert backdoors. These methods are harder to detect since they operate at the model level rather than through input manipulation. Tactics like Weak-to-Strong Jailbreaking \citep{zhao2024weaktostrongjailbreakinglargelanguage} exploit latent vulnerabilities to adversarially modify
the decoding probabilities of LLMs, while Model Interrogation \citep{liu2024makingaskanswerjailbreaking} delves into hidden parameters to uncover prohibited information. Nevertheless, executing model-based attacks on black-box systems like o1 is very difficult without direct access to internal parameters or distributions \cite{zhao2024weaktostrongjailbreakinglargelanguage,liu2024makingaskanswerjailbreaking}. 




While these traditional jailbreaking techniques have provided valuable insights into model vulnerabilities, they are increasingly rendered obsolete by o1's advanced safety mechanisms. o1’s implementation of Chain-of-Thought (CoT) safety reasoning and enhanced contextual analysis ensures that such attacks are detected and neutralized with high reliability \citep{openai2024openaio1card,openai2024o3mini,guan2024deliberative}. This shift underscores the need for new approaches that specifically target transparency and logical consistency in CoT safety reasoning processes, as proposed in this work.