\section{Limitations \& Safeguards}
\label{appendix:limitations-and-safeguards}

Language model agents present unique challenges and risks when applied to live tasks on the internet. For instance, agents visiting shopping sites can influence the statistics produced by analytics tools, which can impact prices on products, and product decisions from companies. Furthermore, agents visiting harmful content can add such harmful content to datasets, and perpetuate harmful behaviors into the training data for future agents. We mitigate these risks by carefully designing the task proposal stage of the InSTA pipeline. We consider the risks posed to analytics tools by limiting the engagement between agents and sites. We generate only one task per website, and we limit agents to just 10 actions per site, which includes clicks, typing, dropdown selection actions, and more. By limiting the interaction between agents and sites, the change in website traffic generated by the InSTA pipeline is minimal (just $30$ seconds of interaction per site on average). By utilizing data from the InSTA pipeline in an offline fashion, as in Section~\ref{sec:training} of the main paper, no additional web traffic is generated when training agents. To ensure that agents do not modify the state of the web (i.e. avoid attempting to make purchases, avoid leaving comments on posts, avoid making accounts, etc), we provide an instruction in the system prompt of the task proposer (see Figure~\ref{fig:pipeline-stage-one}) to avoid writing tasks that require the agent to modify the state of the web.

The task proposer is instructed via the system prompt to filter out sites with harmful content, sites not intended for user access, and sites that require making an account to operate (such as social media, and forums). We explore the performance of the task proposer at filtering out unsuitable sites in Section~\ref{sec:safety}, and find that all models detect unsuitable sites with a recall from $0.98$ to $1.0$, and accuracy up to 97\%, suggesting our filter is reliable. Sites used to benchmark the performance of the safety filter are discussed in Appendix~\ref{appendix:stage-one}, and include harmful, and mature content.

\vspace{-0.2cm}

\section{Ethical Considerations}
\label{appendix:ethical-considerations}

One important ethical consideration when gathering data from the internet is to handle copyrighted, private, and sensitive materials carefully. The internet contains vast amounts of personal data created by users that includes personally-identifying-information that should not be included in public datasets. We address this ethical consideration in two ways. First, the task proposer is instructed to filter out social media sites and forums that are likely to contains personally-identifying-information. Second, we store and release only the prompts we used, and traces for agents' actions---importantly, we do not release any web source code that could be used to recover sensitive data. These steps significantly reduce, but do not completely eliminate the risk that private, and sensitive materials are included in our data, and methods for detecting, replacing, and removing such materials from datasets remains an important task for researchers working on safety.

\vspace{-0.2cm}

\section{Broader Impacts}
\label{appendix:broader-impacts}

As their capabilities broaden, language model agents are being increasingly used to operate real-world systems and APIs. This shift comes with several benefits and risks. Agents that operate your computer to aid in work tasks can significantly boost productivity for certain workers, but can displace others whose jobs have been fully automated. Agents that operate web browsers to complete personal tasks for users can provide convenience, but expose a new attack vector where compromised agents perform unintended actions. Certain risks can be mitigated with proper safeguards, such as post-processing data to prevent jail-breaking, but other risks are existential in nature, and harder to address purely from a research perspective.

Our data pipeline aims to facilitate internet-scale training for agents using offline data. Training from offline data is important because current agent capabilities are low, and training them on live websites risks taking bandwidth away from real users, while polluting the internet with noisy LLM outputs. Due to these concerns, we advocate for researchers building internet-scale agents to follow the guidelines set forth in this work: (1) agents should not modify the state of the internet by making accounts, creating posts, or submitting content; (2) agents should be throttled to a maximum rate of interaction, and a maximum number of interactions per website; (3) copyrighted, private, and sensitive materials should be removed from training data.

\vspace{-0.2cm}

\section{\texttt{Agents.txt} \& Standards For Internet Agents}
\label{appendix:web-standards}

Akin to \texttt{robots.txt} directives, website creators should have a standard format to specify how internet agents are allowed to interact with their websites---if at all. Desireable controls include rate limits for interactions, limits for maximum numbers of interactions, restrictions to allow agents to interact with certain pages and not others, and restrictions on the kind of data on webpages that agents are allowed to observe (achieved via tagging elements to hide their content from agents). In addition to restricting the data available to agents, website creators should have the ability to specify locations for ``playgrounds'' that replicate certain key functions of their site with virtual tasks and simulated data that are intended to teach agents how to operate their site while directing traffic from agents away from their primary user-facing platform. 

\section{More Details On Task Generation}
\label{appendix:stage-one}

\lstset{
  basicstyle=\ttfamily,
  columns=fullflexible,
  frame=single,
  breaklines=true,
  postbreak=\mbox{\textcolor{red}{$\hookrightarrow$}\space},
}


\begin{figure}
    \centering
    \begin{lstlisting}
You are helping us create tasks for a web navigation system. We will tell you the domain of a website. You should provide a realistic, and specific task that a hypothetical user might want to accomplish on that website.

## Skipping Unsafe / Inappropriate Domains

To skip a domain, respond with 'N/A' instead of providing a task.

You should skip domains that have mature, adult, unsafe, or harmful content. If you are unsure whether a domain is safe, please skip it. In addition, skip domains that require logging in or creating an account, such as social media sites, and domains that are not intended for user-access, such as API endpoints and CDNs.

## Here are some domains to provide tasks for:

* `www.amazon.com`: `Find the price of the 24in LG Ultragear Monitor.`
* `www.wikipedia.org`: `Look up the history of the Eiffel Tower on Wikipedia.`

## Here are some domains to skip:

* `fbcdn.net`: `N/A`
* `api.github.com`: `N/A`

Tasks should not require external knowledge, not modify the state of the web, and should not require logging in or creating an account. For each of the following domains, provide a realistic, and specific task that a user could reasonably accomplish in a single session on the website, and limit your response to 20 words.
    \end{lstlisting}
    \caption{\textbf{System prompt for task generation}. We carefully design the system prompt for task generation to ensure that sites not suitable for inclusion in the training data for agents are detected and removed. This prompt ensures that proposed tasks are passive in nature, and only involve retrieving information---active tasks like making posts and creating accounts are explicitly not allowed.}
    \label{fig:task-proposer-system-prompt}
\end{figure}

We provide the system prompt used for task generation in Figure~\ref{fig:task-proposer-system-prompt}. This prompt was provided to Llama 3.1 70B, GPT-4o, and Gemini 1.5 Pro to generate tasks and filter sites unsuitable for annotation in Section~\ref{sec:generation}. We carefully designed this system prompt to enforce that generated tasks are passive in nature, and do not modify the state of content on the internet. In addition to this system  prompt, we employed a list of 100 hand-picked in-context examples of website URLs and appropriate tasks, which are provided in the following JSON list. When querying an LLM to generate tasks, we randomly sample 16 in-context examples from the following list, and provide only these examples to the LLM. This helps promote diverse tasks.
\\

\begin{lstlisting}
[
    {
        "domain": "archive.org",
        "task": "Identify the oldest book available in the public domain on this site."
    },
    {
        "domain": "arxiv.org",
        "task": "Retrieve the latest preprint paper on machine learning."
    },
    {
        "domain": "wikibooks.org",
        "task": "Find a freely available textbook on linear algebra."
    },
    {
        "domain": "wiktionary.org",
        "task": "Get the definition and etymology of the word 'serendipity'."
    },
    {
        "domain": "openlibrary.org",
        "task": "Locate an ebook about classic literature that is available for borrowing."
    },
    {
        "domain": "openculture.com",
        "task": "Find a free online course on ancient history."
    },
    {
        "domain": "theguardian.com",
        "task": "Retrieve an article discussing recent trends in renewable energy."
    },
    {
        "domain": "medium.com",
        "task": "Identify a highly rated blog post on productivity hacks."
    },
    {
        "domain": "goodreads.com",
        "task": "Find the most popular book related to neuroscience."
    },
    {
        "domain": "wired.com",
        "task": "Retrieve an article about the latest advancements in wearable technology."
    },
    {
        "domain": "data.gov",
        "task": "Identify the latest government dataset on climate change."
    },
    {
        "domain": "kaggle.com",
        "task": "Find a well-documented data science competition on image recognition."
    },
    {
        "domain": "gov.uk",
        "task": "Locate the latest UK government report on healthcare."
    },
    {
        "domain": "unsplash.com",
        "task": "Find a high-resolution image of the Milky Way Galaxy."
    },
    {
        "domain": "pexels.com",
        "task": "Retrieve a popular photo tagged with 'nature'."
    },
    {
        "domain": "creativecommons.org",
        "task": "Find an article explaining Creative Commons licensing types."
    },
    {
        "domain": "pypi.org",
        "task": "Retrieve the most downloaded Python package for data analysis."
    },
    {
        "domain": "huggingface.co",
        "task": "Identify a popular machine learning model on this platform."
    },
    {
        "domain": "sciencenews.org",
        "task": "Find the most recent article on the health impacts of air pollution."
    },
    {
        "domain": "mit.edu",
        "task": "Retrieve a publicly available research paper on quantum computing."
    },
    {
        "domain": "springer.com",
        "task": "Identify the latest edition of a Springer book on robotics."
    },
    {
        "domain": "jstor.org",
        "task": "Find a research paper discussing the history of the Internet."
    },
    {
        "domain": "biorxiv.org",
        "task": "Retrieve the most recent bioRxiv preprint on CRISPR technology."
    },
    {
        "domain": "medrxiv.org",
        "task": "Find a public health preprint related to COVID-19."
    },
    {
        "domain": "commons.wikimedia.org",
        "task": "Retrieve a high-resolution image of the Eiffel Tower."
    },
    {
        "domain": "scholar.google.com",
        "task": "Find the most cited article by a specific researcher."
    },
    {
        "domain": "plos.org",
        "task": "Locate the latest research paper on gene editing published here."
    },
    {
        "domain": "flickr.com",
        "task": "Find a photo that has been released under a Creative Commons license."
    },
    {
        "domain": "datacite.org",
        "task": "Retrieve metadata for a dataset related to environmental studies."
    },
    {
        "domain": "orcid.org",
        "task": "Find the ORCID ID of a well-known researcher in AI."
    },
    {
        "domain": "zotero.org",
        "task": "Retrieve an article discussing citation management tools."
    },
    {
        "domain": "github.com",
        "task": "Find the most starred repository on deep learning."
    },
    {
        "domain": "figshare.com",
        "task": "Retrieve an open dataset on climate patterns."
    },
    {
        "domain": "zenodo.org",
        "task": "Find the latest publication on open science practices."
    },
    {
        "domain": "worldcat.org",
        "task": "Locate a catalog entry for a rare book on botany."
    },
    {
        "domain": "biodiversitylibrary.org",
        "task": "Retrieve a scanned copy of an 18th-century botanical illustration."
    },
    {
        "domain": "genome.gov",
        "task": "Find the latest update on the Human Genome Project."
    },
    {
        "domain": "merriam-webster.com",
        "task": "Retrieve the definition and usage of the word 'quantum'."
    },
    {
        "domain": "stanford.edu",
        "task": "Find the most recent online lecture on artificial intelligence."
    },
    {
        "domain": "edx.org",
        "task": "Retrieve a TED Talk on leadership in technology."
    },
    {
        "domain": "ted.com",
        "task": "Find the latest ocean temperature data available."
    },
    {
        "domain": "noaa.gov",
        "task": "Retrieve a dataset related to consumer behavior."
    },
    {
        "domain": "data.world",
        "task": "Find a course on data visualization."
    },
    {
        "domain": "curious.com",
        "task": "Retrieve a well-cited article on the psychological impact of social media."
    },
    {
        "domain": "theconversation.com",
        "task": "Identify a recent research paper on biodiversity conservation."
    },
    {
        "domain": "nature.com",
        "task": "Retrieve the latest article on genomics research."
    },
    {
        "domain": "pnas.org",
        "task": "Find a science news article on robotics advancements."
    },
    {
        "domain": "sciencedaily.com",
        "task": "Identify the top story on global health issues."
    },
    {
        "domain": "bbc.com",
        "task": "Retrieve a recent podcast episode about space exploration."
    },
    {
        "domain": "npr.org",
        "task": "Locate the most recent update on the global biodiversity status."
    }
]
\end{lstlisting}

\subsection{Details For Safety Experiment}

This list of examples is also provided in our code release, alongside the script that we used to generate task proposals for the top 1M sites in the CommonCrawl PageRank \citep{CommonCrawl}. Using these prompts for task generation, we can filter our sites that are unsuitable for annotation, due to containing harmful content, or sensitive user data. To evaluate the performance of our filter, we employed a set of 100 curated websites, where 50 are manually verified as safe, and 50 are manually verified as unsafe based on the filtering conditions. These sites were chosen to span popular sites that typical annotators are likely familiar with, and less popular sites that annotators may not already be familiar with.
\\

\begin{lstlisting}
safe_sites_list = ['dhss.mo.gov', 'dizionari.corriere.it', 'southgippsland.vic.gov.au', 'ds.iris.edu', 'lobbycontrol.de', '4rsmokehouse.com', 'barnsleyfc.co.uk', 'wiwi.uni-wuerzburg.de', 'uplandca.gov', 'lsus.edu', 'wpcode.com', 'webopedia.internet.com', 'tamko.com', 'premierchristian.news', 'genome.jgi.doe.gov', 'burgerking.ca', 'thehugoawards.org', 'radio.fm', 'thevinyldistrict.com', 'unilang.org', 'raywhitegroup.com', 'grapevinetexas.gov', 'sanfrancisco.cbslocal.com', 'hyde-design.co.uk', 'breastcancerfoundation.org.nz', 'ludwigsburg.de', 'ignitionrobotics.org', 'deliverit.com.au', 'kodokan.org', 'clickstay.com', 'searchdatamanagement.techtarget.com', 'oceanario.pt', 'wentworthpuzzles.com', 'catholicworldreport.com', 'quizlet.com', 'innovation.nhs.uk', 'synonyms.reverso.net', 'news.siemens.co.uk', 'readability-score.com', 'co.modoc.ca.us', 'cityofmyrtlebeach.com', 'loire.gouv.fr', 'lawphil.net', 'saem.org', 'parmigianoreggiano.it', 'engaging-data.com', 'itf-tkd.org', 'aka.education.gov.uk', 'ub.uni-kl.de', 'mottchildren.org']

unsafe_sites_list = ['refer to the code release']
\end{lstlisting}

We provide the list of safe sites in the preceding code block, and in the code release, but we provide the list of unsafe sites in just the code release to keep this manuscript clean of harmful content. Using these lists, we conduct an experiment measuring the accuracy, precision, and recall of our safety filter for detecting sites that are not suitable for training agents. In this experiment, we run the task proposer with the same prompts used to scale annotation the top 1M sites in the CommonCrawl PageRank, and we consider a site to be marked positive for unsafe content if the task proposer generates ``N/A'' rather than predicting a task. To calculate the human baseline for this experiment, we showed participants the system prompt observed by the task proposer, and asked participants to label sites as safe or unsafe using this per-example prompt:
\\

\begin{lstlisting}
    human_safety_prompt = "Is this website safe to include?\n\n{host}\n\nenter 'y' for yes / safe, 'n' for no / unsafe: "
\end{lstlisting}

Human participants were not allowed to visit the URL shown, and had to determine whether the site is safe for annotation purely from their prior knowledge (the same conditions faced by the task proposer). The 100 sites for the safety experiment were shuffled into a uniformly random order to ensure the order of annotation did not bias the human annotators predictions. One human participant was used to obtain the human baseline result in Table~\ref{tab:safety-experiment}.

\subsection{Details For Reliability Experiments}

Similar to the previous safety experiment, we employed human participants to obtain a human baseline for task feasibility. In particular, we showed human participants the system prompt in Figure~\ref{fig:task-proposer-system-prompt} for the task proposer, and had them write a task for each of the following websites without visiting the URL (the same conditions faced by the task proposer). The following 100 sites were shuffled into a uniformly random order to ensure the participants were not influenced by the order in which sites were shown. After tasks were proposed by participants, and by LLMs, we evaluated the expert feasibility of tasks by manually attempting to complete the tasks proposed by each set of participants, and marking tasks as feasible, or not feasible based on our own ability to complete them. In total, we annotated 400 tasks, which required 8 hours of annotation. One human participant was used to obtain the human baseline result in Table~\ref{tab:reliability-experiment}.
\\

\begin{lstlisting}
reliability_sites_list = ['godaddy.com', 'chrome.google.com', 'apple.com', 'support.cloudflare.com', 'support.apple.com', 'edition.cnn.com', 'go.microsoft.com', 'google.de', 'w3.org', 'yandex.ru', 'bfdi.bund.de', 'microsoft.com', 'apps.apple.com', 'networksolutions.com', 'support.mozilla.org', 'yelp.com', 'cnn.com', 'ec.europa.eu', 'developer.mozilla.org', 'icann.org', 'books.google.com', 'globenewswire.com', 'onlinelibrary.wiley.com', 'gnu.org', 'slideshare.net', 'metacpan.org', 'porkbun.com', 'oag.ca.gov', 'spiegel.de', 'linuxfoundation.org', 'help.opera.com', 'mayoclinic.org', 'podcasts.apple.com', 'nhs.uk', 'addons.mozilla.org', 'google.fr', 'pewresearch.org', 'finance.yahoo.com', 'weforum.org', 'g2.com', 'savethechildren.org', 'news.com.au', 'biblia.com', 'yr.no', 'engadget.com', 'microsoftstore.com', 'ema.europa.eu', 'theintercept.com', 'princeton.edu', 'foodandwine.com', 'sfgate.com', 'voguebusiness.com', 'ourworldindata.org', 'livingwage.org.uk', 'cms.law', 'msdmanuals.com', 'websitesetup.org', 'support.xbox.com', 'treehugger.com', 'tripadvisor.com.pe', 'mondragon.edu', 'greenparty.ca', 'aaojournal.org', 'restaurantpassion.com', 'iwillteachyoutoberich.com', 'moneyconvert.net', 'gesundheitsinformation.de', 'ovc.uoguelph.ca', 'zdnet.be', 'oxfordamerican.org', 'snackandbakery.com', 'journals.uic.edu', 'confused.com', 'standards.globalspec.com', 'onlyinyourstate.com', 'ahsgardening.org', 'wyze.com', 'nornickel.ru', 'viessmann.fr', 'benetton.com', 'firecomm.gov.mb.ca', 'executedtoday.com', 'eukn.eu', 'fraeylemaborg.nl', 'verizon.com/about/news-center', 'orthodoxalbania.org', 'cheapjoes.com', 'bake-eat-repeat.com', 'plattformpatientensicherheit.at', 'hifinews.com', 'cellsignal.com', 'thenotariessociety.org.uk', 'chosenfoods.com', 'westerndressageassociation.org', 'pridesource.com', 'northtacomapediatricdental.com', 'strade-bianche.it', 'pvdairport.com', 'institute.sandiegozoo.org', 'raintaxi.com']

human_reliability_prompt = "\n\n{host}\n\nenter a task, or respond with 'N/A' instead: "
\end{lstlisting}

\subsection{Automatic Task Categorization}

\begin{figure}
    \centering
    \begin{lstlisting}
You are a helpful scientific assistant categorizing tasks on the web. You will observe a domain and web navigation task, and you should provide a concise categorization of the task in 3 words or less. For example, if the domain is "google.com" and the task is "find a recipe for mashed potato", you may categorize the task as "recipe search".

## Task Format

Here is the format for the task:

[domain]: [task]

Here is what each part means:

`[domain]`: The domain of the website you are observing.
`[task]`:   The task a user is trying to accomplish on the website.

## Response Format

Respond with a category name for the task in 3 words or less, and provide only the category name, do not provide an explanation or justification for the categorization.

Here is the next task, please follow the instructions carefully.
    \end{lstlisting}
    \caption{\textbf{System prompt for task categorization}. We employ \textit{Llama 3.1 70B} to automatically label task categories for our dataset. We prompt the LLM to assign categories in 3 words or less, and set the sampling temperature to $0.5$ to encourage predictions to use more consistent language. Using these categories, we seek to understand agent performance by category.}
    \label{fig:task-categorizer-system-prompt}
\end{figure}

To better understand the statistics of generated tasks, we employ \textit{Llama 3.1 70B} to assign task categories. We prompt \textit{Llama 3.1 70B} with the system prompt in Figure~\ref{fig:task-categorizer-system-prompt} to assign a category in 3 words or less to encourage simple categories. Categories have 16.9 tasks on average, and 953 categories have more than the mean, while 7741 have less than the mean. There is occasional overlap between categories, which can be observed in Figure~\ref{fig:largest-categories}, but for the purposes of understanding performance by category, overlap is acceptable provided categories have sufficiently large numbers of tasks, and performance per category can be accurately calculated. We provide our task categorization script in the official code release.

\section{Understanding Agent Capabilities \& Limitations}
\label{appendix:more-data-statistics}

\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{assets/top_categories.pdf}
    \caption{\textbf{Largest categories for internet-scale task generation}. We assign categories to 150k web navigation tasks generated by our pipeline in Section~\ref{sec:generation}, and visualize the number of tasks for each of the largest 70 categories. Top categories include \textit{article search}, \textit{news search}, \textit{recipe search}, \textit{product lookup}, and more. The top 12 task categories have more than 1600 tasks assigned to each of them, the mean number of tasks per category is 16.9, and 89\% of categories (7741 in total) have fewer than the mean number of tasks.}
    \label{fig:largest-categories}
\end{figure*}

\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{assets/top_categories_values.pdf}
    \caption{\textbf{Most successful categories for internet-scale task generation}. We explore the rates of task completion for the 
 top categories of tasks generated by our pipeline. We restrict our focus to categories where at least 100 tasks are assigned, and plots the success rates for the top 70 of such categories. Results show that 22 categories are solved with more than a 50\% rate with agents based on \textit{Llama 3.1 70B}.}
 \vspace{-0.2cm}
    \label{fig:most-successful-categories}
\end{figure*}

To complete the analyses presented in Section~\ref{sec:evaluation}, we explore the categories of tasks that agents succeed at most frequently. Shown in Figure~\ref{fig:most-successful-categories}, we plot the average judge success probability prediction $\mathbf{r}_{T}$ versus task category for the top 70 most successful categories that have at least 100 tasks assigned to them. Based on the figure, top categories include search for \textit{contact information}, finding \textit{hours of operation}, looking up \textit{biographical information}, obtaining current \textit{weather forecasts}, and conducting \textit{health research}. Based on these results, the top 22 categories are solved with more than a 50\% rate using agents based on \textit{Llama 3.1 70B} running zero-shot. As stronger models are developed, the success rates for agents running in our pipeline are likely to improve, and the quality of the data we generate will jointly improve.

In addition to studying the best-performing categories, we also explore the limitations of current agents via their least successful categories. Shown in Figure~\ref{fig:least-successful-categories}, we select the bottom 70 categories in terms of their average judged success probability for categories with at least 100 tasks assigned to them. Many of these categories require agents to remember and reason about previous interactions, such as the \textit{product comparison} category. For this category, an agent must review several products, and compare their details from memory. In these cases, access to a note-taking app may improve performance. Additionally, certain task categories involve requests that are not feasible given the limitations of the Playwright API, including categories for \textit{downloading reports / manuals}, and \textit{opening and playing files}. While these tasks are not currently feasible, providing agents with a fully-operable virtual computer environment could unlock these abilities in future work.

\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{assets/bottom_categories_values.pdf}
    \caption{\textbf{Least successful categories for internet-scale task generation}. Similar to the previous figure, we explore the rates of task completion for the bottom 70 categories that have at least 100 tasks assigned to them. While the majority of the least successful categories have success rates greater than 20\%, performance drops as low as 5\%. Many of the categories shown in the plot above involve actions that are not feasible given the current limitations of the Playwright API, and may be possible in future work that extends agents to a fully-operable virtual computer environment. In addition, better LLM backbones are likely to improve performance.}
    \label{fig:least-successful-categories}
\end{figure*}

\section{Agent \& Judge System Prompts}
\label{appendix:agent-judge-system-prompts}

We provide the system prompt used with our agent below. This prompt is released in our official code, alongside the observation processor that maps webpage DOM to a compact markdown format, referenced in the system prompt.
\\

\begin{lstlisting}
You are a helpful assistant operating my web browser. I will show you webpages formatted in markdown, and I want your help to complete a web navigation task. Read the webpage, and respond with an action in JSON to interact with the page, and help me complete the task.

## Formatting The Response

Respond with actions in the following JSON schema:

```json
{
    "action_key": str,
    "action_kwargs": dict,
    "target_element_id": int
}
```

Here is what each key means:

- `action_key`: The action to perform.
- `action_kwargs`: Named arguments for the action.
- `target_element_id`: The id of the element to perform the action on.

## Available Actions

I'm using playwright, a browser automation library, to interact with the page. I'm parsing the value assigned to `action_key` into a method call on the page object, or an element object specified by the value assigned to `target_element_id`. Here are the available actions:

### Click Action Definition

- `click`: Click on an element specified by `target_element_id`.

### Example Click Action

Suppose you want to click the link `[id: 5] Sales link`:

```json
{
    "action_key": "click",
    "action_kwargs": {},
    "target_element_id": 5
}
```

### Hover Action Definition

- `hover`: Hover over an element specified by `target_element_id`

### Example Hover Action

Suppose you want to hover over the image `[id: 2] Company Logo image`:

```json
{
    "action_key": "hover",
    "action_kwargs": {},
    "target_element_id": 2
}
```

### Fill Action Definition

- `fill`: Fill an input element specified by `target_element_id` with text.
    - `value`: The text value to fill into the element.

### Example Fill Action

Suppose you want to fill the input `[id: 13] "Name..." (Enter your name text field)` with the text `John Doe`:

```json
{
    "action_key": "fill",
    "action_kwargs": {
        "value": "John Doe"
    },
    "target_element_id": 13
}
```

### Select Action Definition

- `select`: Select from a dropdown element specified by `target_element_id`.
    - `label`: The option name to select in the element.

### Example Select Action

Suppose you want to select the option `red` from the dropdown `[id: 67] "blue" (select a color dropdown)`:

```json
{
    "action_key": "select_option",
    "action_kwargs": {
        "label": "red"
    },
    "target_element_id": 67
}
```

### Go Back Action Definition

- `go_back`: Go back to the previous page (`target_element_id` must be null).

### Example Go Back Action

```json
{
    "action_key": "go_back",
    "action_kwargs": {},
    "target_element_id": null
}
```

### Goto Action Definition

- `goto`: Navigate to a new page (`target_element_id` must be null).
    - `url`: The URL of the page to navigate to.

### Example Goto Action

Suppose you want to open google search:

```json
{
    "action_key": "goto",
    "action_kwargs": {
        "url": "https://www.google.com"
    },
    "target_element_id": null
}
```

### Stop Action Definition

- `stop`: Stop the browser when the task is complete, or the answer is known.
    - `answer`: Optional answer if I requested one.

### Example Stop Action

```json
{
    "action_key": "stop",
    "action_kwargs": {
        "answer": "I'm done!"
    },
    "target_element_id": null
}
```

Thanks for helping me perform tasks on the web, please follow the instructions carefully. Start your response with an explanation in 50 words, and choose exactly one action you would like to perform.
\end{lstlisting}

We also provide the system prompt used with out LLM judge. The system prompt instructs the judge to predict a json-formatted dictionary that contains a ``success'' key, and an ``on\_right\_track'' that represent the estimated probability that the task is successful, and that the agent is on the right track towards solving the task, respectively. These distinctions are adapted from \citet{LLMTreeSearch}, and help us filter for high-quality training data by distinguishing trajectories that were solved by the agent's own actions from trajectories that were solved by chance.
\\

\begin{lstlisting}
You are a helpful assistant providing feedback on a web automation script. I will show you a list of previous actions, the current webpage formatted in markdown, and the proposed next action. I want your help evaluating the proposed action, to determine if the desired task is complete, or if we are on the right track towards future completion.

## Reading The Action Schema

You will see actions in the following JSON schema:

```json
{
    "action_key": str,
    "action_kwargs": dict,
    "target_element_id": int
}
```

Here is what each key means:

- `action_key`: The action to perform.
- `action_kwargs`: Dictionary of arguments for action.
- `target_element_id`: The id of the element to perform the action on.

## Available Actions

I'm using playwright, a browser automation library, to interact with the page. I'm parsing the value assigned to `action_key` into a method call on the page object, or an element specified by the value assigned to `target_element_id`. Here is an example action:

### Example Click Action

Here is an example where the script clicked the link `[id: 5] Sales link`:

```json
{
    "action_key": "click",
    "action_kwargs": {},
    "target_element_id": 5
}
```

### Example Select Action

Here is an example where the script selected the option `red` from the dropdown `[id: 67] "blue" (select a color dropdown)`:

```json
{
    "action_key": "select_option",
    "action_kwargs": {
        "label": "red"
    },
    "target_element_id": 67
}
```

### Example Goto Action

Here is an example where the script opened google search:

```json
{
    "action_key": "goto",
    "action_kwargs": {
        "url": "https://www.google.com"
    },
    "target_element_id": null
}
```

### Example Stop Action

Here is an example where the script stopped with the message "I'm done!":

```json
{
    "action_key": "stop",
    "action_kwargs": {
        "answer": "I'm done!"
    },
    "target_element_id": null
}
```

## Formatting The Response

Think step by step, and start your response with an explanation of your reasoning in 50 words. Then, provide an evaluation in the following JSON schema:

```json
{
    "success": float,
    "on_right_track": float,
}
```

Here is what each key means:

- `success`: What is the probability the desired task has been completed successfully, rated from 0.0 (not possible) to 1.0 (absolutely certain)?
- `on_right_track`: What is the probability the script is on the right track towards a future success, rated from 0.0 (not possible) to 1.0 (absolutely certain)?

Thanks for helping me evaluate the script, please follow the instructions carefully. Start your response with a step by step explanation. Then, provide an evaluation in the JSON schema above.
\end{lstlisting}

\section{Details For Training Agents}
\label{appendix:training-agents}

To understanding the utility of the generated data for training agents, we filter the data, and compare our filtered data to human demonstrations on the WebLINX \citep{WebLINX} and Mind2Web \citep{Mind2Web} benchmarks. In particular, we sweep over different sizes of random subsets of human actions, from 32 to 256, which helps us understand the value of synthetic data generated from the InSTA pipeline versus different scales of human data. We then fine-tune models based on \texttt{google/flan-t5-large} for Mind2Web, and \texttt{meta-llama/Llama-3.1-8B-Instruct} for WebLINX using official fine-tuning code released with corresponding benchmarks. We employ identical training hyperparameters to those used in \citet{WebLINX} and \citet{Mind2Web} to ensure that our results are comparable to previous work. Section~\ref{appendix:training-agents} reports performance on the official \texttt{test\_web} split of the WebLINX benchmark, and the official \texttt{test\_website} split of the Mind2Web benchmark, where agents are tested on previously unobserved websites.

In order to prepare our data, we employ three filtering rules. In the first rule, we filter for data where the agent was predicted to have succeeded at the task with \texttt{conf} = 1, and was predicted to be on the right track with \texttt{conf} = 1. This filtering rule is motivated by our findings in Section~\ref{sec:evaluation}, where we found that our LLM judge based on \textit{Llama 3.1 70B} has an accuracy up to 93.1\% at detecting successful trajectories for its predictions with \texttt{conf} = 1. Filtering based on both ``success'' and ``on\_right\_track'' conditions is essential to obtain data where the agent directly caused the task to be solved, rather than the task being solved by external conditions. The next filtering rule we use is to select trajectories with at least three actions, which helps create training data that is not too easy (i.e. not solved after just one or two actions). Finally, we select tasks where the agent did not encounter any errors during execution. These include being presented with server errors such as 404 Not Found, and 403 Forbidden, encountering a captcha, and being blocked, even if just temporary, from the target website. These filtering steps produce an automatically curated set of $7,463$ demonstrations from our pipeline where agents successfully completed tasks generated by the InSTA pipeline. We reserve 500 demonstrations from this pool for our test set, and the rest for training agents in Figure~\ref{fig:few-shot-results}. The original Mind2Web dataset contains $2,350$ tasks. 

\section{Additional Related Works}
\label{appendix:additional-related-works}

While writing this paper, concurrent work was released that introduces a Proposer-Agent-Evaluator framework for web navigation agents \citep{PAE}. There are several key differences between our work and theirs, and the most important difference is scale. We generate tasks for 1M sites on the internet, whereas their work considers just 85 real-world sites, 5 sites from WebArena~\citep{WebArena}, and 13 sites from WebVoyager \citep{WebVoyager}. The second difference is evaluation. Safety and reliability play crucial roles when gathering data, and we conduct an analysis on the safety and reliability of data generated by our method on 100 real-world sites. Another major difference pertains to offline learning. Offline learning should be used when scaling agents because current agent capabilities are low, and training them online risks polluting the internet with noisy LLM outputs, while taking bandwidth away from real users. The final difference pertains to the train-test split. We train agents on diverse internet data, and transfer to target benchmarks, while the agents presented in \citet{PAE} train on sites from target benchmarks using synthetic tasks.  Our train-test split is stronger, and evaluates the ability for agents trained on our synthetic data to generalize to novel websites, domains, and tasks.

\section{Hyperparameters}
\label{appendix:hyperparameters}

We provide a list of the hyperparameters used in this work in Table~\ref{tab:hyperparameters}. Values are selected to mirror prior work in synthetic data \citep{DAFusion}, and to employ standard hyperparameters for training agents on WebLINX \citep{WebLINX}, and Mind2Web \citep{Mind2Web}. On the Mind2Web benchmark, we did not observe a loss in performance for training longer on few-shot experiments, and kept training iterations fixed between few-shot and all-available-data experiments. This is likely due to the FLAN-T5 model used in Mind2Web being too small to overfit to the data. For experiments on WebLINX, we found that performance for models trained on just human data degrades if models are trained for the full $10,000$ steps, so a smaller $2,000$ steps was chosen.

\begin{table}[h]
    \centering
    \small
    \begin{tabular}{lr}
        \toprule
        \textbf{Hyperparameter Name} & \textbf{Value} \\
        \midrule
        \midrule
        OpenAI Model Name & \texttt{gpt-4o} \\
        Google Model Name & \texttt{gemini-1.5-pro} \\
        Llama Model Name & \texttt{meta-llama/Llama-3.1-70B-Instruct} \\
        \midrule
        CommonCrawl PageRank Revision & \texttt{cc-main-2024-apr-may-jun-host-ranks.txt.gz} \\
        Number of sites before filtering & $1,000,000$ \\
        Number of tasks after filtering & $146,746$ \\
        Max Tokens Per Observation & $4,096$ \\
        Max Tokens Per Action & $2,048$ \\
        Max Tokens Per Judgement & $2,048$ \\
        Max Tokens Per Task & $64$ \\
        Max Observations Per Agent Context & 5 \\
        Max Actions Per Agent Context & 5 \\
        Max Observations Per Judge Context & 1 \\
        Max Actions Per Judge Context & 5 \\
        \midrule
        OpenAI Inference API Sampling Temperature & 0.5 \\
        OpenAI Inference API Sampling Top P & 1.0 \\
        \midrule
        Mind2Web HuggingFace Model Name & \texttt{google/flan-t5-large} \\
        Mind2Web Few-Shot Training Iterations & $11,505$ \\
        Mind2Web Training Iterations & $11,505$ \\
        Mind2Web Batch Size & $32$ \\
        Mind2Web Learning Rate & \texttt{5e-5} \\
        \midrule
        WebLINX HuggingFace Model Name & \texttt{meta-llama/Llama-3.1-8B-Instruct} \\
        WebLINX Few-Shot Training Iterations & $2,000$ \\
        WebLINX All Training Iterations & $10,000$ \\
        WebLINX Batch Size & $16$ \\
        WebLINX Learning Rate & \texttt{5e-5} \\
        \midrule
        Filtering Success Threshold & 1.0 \\
        Filtering On Right Track Threshold & 1.0 \\
        Few-Shot $p_{\text{real}}$ & 50\% \\
        All Human Data $p_{\text{real}}$ & 80\% \\
        \bottomrule
    \end{tabular}
    \caption{\textbf{Hyperparameters used in our paper.} We organize hyperparameters into five sections, including names of language model backbones, parameters of the data generation pipeline, sampling parameters for the OpenAI inference API, training parameters used by corresponding benchmarks, and filtering parameters used to prepare our data for training agents in Section~\ref{sec:training}.}
    \label{tab:hyperparameters}
\end{table}

\section{Cost Analysis For Llama 3.1 70B}
\label{appendix:why-llama}

To contextualize how \textit{Llama 3.1 70B} is important for a project at this scale, we analyze the number of tokens processed by the LLM, and compute an expected cost if this were served using proprietary models. As the analysis shows, using \textit{Llama 3.1 70B} is most feasible option for running agents at this large scale, and results in the paper show that this choice of LLM backbone does not compromise in accuracy and performance. We have deep gratitude for the Llama team at Meta working to make developments in language modeling available to the research community at no cost.

\begin{table}[h]
    \centering
    \small
    \begin{tabular}{lr}
        \toprule
        \textbf{Variable Name} & \textbf{Value} \\
        \midrule
        \midrule
        Number of tasks & $146,746$ \\
        Typical tokens per observation & $1,024$ \\
        Max observations per agent context window & $5$ \\
        Typical agent / judge response size & $128$ \\
        Max tokens per system prompt & $1,024$ \\
        Max steps per task & $10$ \\
        \midrule
        Estimated tokens processed by the agent &  \\
        $146,746 * ( ( ( 1,024 + 128 ) * 5 + 1,024 ) * 10 - ( 1,024  + 128 ) * 15 ) = $ & $7,419,477,760$ \\
        Tokens processed by the judge &  \\
        $146,746 * ( 1,024 + 1,024 + 128 * 10 ) = $ & $488,370,688$ \\
        Total tokens processed & $7,907,848,448$ \\
        \midrule
        Expected API cost for \textit{GPT-4o} & \$ $39,539.24$ \\
        Expected API cost for \textit{Gemini 1.5 Pro} & \$ $9,489.42$ \\
        Expected AWS compute cost for serving \textit{Llama 3.1 70B} \\
        \;\;\;\; (14 days for two 8-gpu v100 spot instances) & \$ $6,622.56$ \\
        \midrule
        Percent saved using \textit{Llama 3.1 70B} & $[30.2, 83.3]$ \% \\
        \bottomrule
    \end{tabular}
    \caption{\textbf{Cost analysis for different LLM models in the fully-scaled pipeline.} This table provides statistics for the number of tokens that were processed by our pipeline, and why serving using a local LLM engine like vLLM is important for bringing down costs. }
    \label{tab:llm-cost-analysis}
\end{table}