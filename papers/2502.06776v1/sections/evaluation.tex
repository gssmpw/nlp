In the next stage, we begin to scale LLM agents to diverse web navigation tasks across the web. Shown in Figure~\ref{fig:pipeline-stage-two}, we initialize a web browsing environment to the URL provided to the task proposer in Section~\ref{sec:generation}, and run a language model agent to complete tasks by generating function calls in the Playwright API. For evaluation, current efforts typically use human-written constraints based on the final URL or page state \citep{WebArena,VisualWebArena,WebShop,WorkArena}, but it can be difficult to scale these. Recall from Figure~\ref{fig:reliability-experiment-pagerank} that human annotators are less reliable for sites lower in the PageRank, where their familiarity is reduced. Results in section~\ref{sec:generation} showed that language models beat humans in safety and reliability for task generation. As we begin to scale agents to diverse websites from the internet, can we replace human-written criteria with language model judgments for efficient evaluation? Their robustness remains an important unresolved question, and current work only considers language model judges for a limited set of popular websites \citep{WebVoyager}.
We begin by validating the robustness of language models for evaluating diverse internet tasks.

\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{assets/icml2025_insta_pipeline_stage_two.pdf}
    \vspace{-0.3cm}
    \caption{\small \textbf{Automatic evaluation for agents with language model judges.} Building on the large and diverse set of tasks generated by the pipeline, we employ pretrained language models to attempt and evaluate web navigation tasks. We dispatch language model agents to perform tasks by making calls to the Playwright API. We then employ language model judges to evaluate the trajectories.}
    \vspace{-0.3cm}
    \label{fig:pipeline-stage-two}
\end{figure*}

\subsection{Evaluation With Language Models}
\label{sec:robust-evaluation}

We model the process of evaluating trajectories from agents as a classification problem, where the goal is to estimate the probability $\mathbf{r}_{T}$ that a task $\mathbf{c}$ is solved by the final timestep. Conditioned on a system prompt $\mathbf{y}_{\text{sys}}$, a task $\mathbf{c}$, the history of previous actions $\mathbf{a}_{1}, \cdots, \mathbf{a}_{T}$ and the final observation from a web browsing environment $\mathbf{s}_{T}$ formatted in text, we generate a completion using a language model and employ a function $f^{\;\text{text} \to \text{val}} (\cdot)$ to parse the estimated probability.

\begin{equation}\label{eqn:value-function}
    \mathbf{r}_{T} = f^{\;\text{text} \to \text{val}} \big( \; \text{LLM} ([ \; \mathbf{y}_{\text{sys}}, \mathbf{c}, \mathbf{a}_{1}, \cdots, \mathbf{a}_{T}, \text{Enc} ( \mathbf{s}_{T} ) \; ]) \; \big)
\end{equation}

\vspace{0.3cm}

Building on the sites used to measure reliability in Section~\ref{sec:reliability}, we conduct an experiment to measure the accuracy of language models to detect successful web navigation trajectories. We run agents on tasks generated by Llama 3.1 70B for the 100 sites in Section~\ref{sec:reliability}, and prompt language models to estimate the probability that tasks are solved $\mathbf{r}_{T}$ based on Equation~\ref{eqn:value-function}. We then conduct human evaluations for the trajectories and manually assign binary success labels. Accuracy is calculated by applying a threshold to the predictions $\mathbf{r}_{T} > 0.5$ to assign classes and tracking the rate at which the predictions agree with human labels. To understand robustness for sites of varying popularity, we report the accuracy of language models versus the PageRank of the corresponding sites. Similarly, to understand the ability of language models to judge their own uncertainty, we report their accuracy versus their prediction confidence, given by \texttt{conf} = $2 \cdot | \mathbf{r}_{T} - 1/2 |$ (twice the total variation distance from the uniform distribution to the predicted distribution). Results are shown in Figure~\ref{fig:robust-evaluation}.

\begin{wrapfigure}{r}{0.48\textwidth}
    \centering
    \vspace{-0.5cm}
    \includegraphics[width=\linewidth]{assets/judge_robustness_accuracy.pdf}
    \vspace{-0.5cm}
    \caption{\small \textbf{Language models are robust evaluators.} We measure the accuracy of language models for detecting successful trajectories, and find that accuracy remains stable relative to PageRank values (\textit{left plot}). As models become more confident, their accuracy improves (\textit{right plot}), suggesting confidence is a useful proxy for the reliability of their predictions.}
    \label{fig:robust-evaluation}
\end{wrapfigure}

\paragraph{Understanding The Results.} Language models are \textit{robust evaluators for web navigation tasks}. Accuracy remains stable relative to PageRank values, suggesting that language models are effective for sites that typical human annotators are less familiar with. The best results are obtained with an evaluator based on \textit{GPT-4o}, which attains an accuracy of $82.6\%$, compared to $81.7\%$ for \textit{Llama 3.1 70B}, and $78.0\%$ for \textit{Gemini 1.5 Pro}. Although the accuracy is robust to PageRank, the accuracy is highly informed by confidence. Language models show improved accuracy as their confidence improves, suggesting that they can effectively determine when their predictions are reliable. When considering predictions with \texttt{conf} = 1, the \textit{Llama 3.1 70B} evaluator displays a compelling $93.1\%$ accuracy, 0.87 precision, and 0.82 recall for detecting successful trajectories. Now that we can efficiently and accurately judge trajectories, we can begin to scale language model agents to diverse internet tasks and track their success. Harnessing this judge, we can study the current abilities and shortcomings of language model agents spanning 150k websites.

\begin{wrapfigure}{r}{0.48\textwidth}
    \centering
    \vspace{-1.5cm}
    \includegraphics[width=\linewidth]{assets/values.pdf}
    \vspace{-0.7cm}
    \caption{\small \textbf{Scaling LLM agents to 150k live sites.} We run agents based on Llama 3.1 70B to complete tasks generated by our pipeline. We estimate success probabilities using a language model evaluator (\textit{left plot}), and estimate probabilities agents are on the right track (\textit{right plot}). 16.7\% of tasks are estimated to be successful with \texttt{conf} = 1, and the spread of probabilities suggests data spans many difficulties.}
    \label{fig:data-statistics}
    \vspace{-0.2cm}
\end{wrapfigure}

\subsection{Scaling To 150,000 Agents}
\label{sec:scaling-agents} 

We scale language model agents to 150k live sites in diverse domains across the Internet and attempt to complete 150k web navigation tasks generated by our pipeline. Shown in Figure~\ref{fig:data-statistics}, we evaluate the trajectories using a \textit{Llama 3.1 70B} judge and run agents based on \textit{Llama 3.1 70B}, selected because this model demonstrates high accuracy in Figure~\ref{fig:robust-evaluation}, and running currently available propriety models would be prohibitively expensive at this scale (see Appendix~\ref{appendix:why-llama} for a cost analysis with different LLMs). We find that agents solve 16.7\% of tasks with a model confidence of \texttt{conf} = 1. Furthermore, we observe that 35k tasks are judged to be on the right track with a confidence of \texttt{conf} = 1, suggesting that these could be solved if a larger computing budget was allocated. The spread along the x-axis in both plots in Figure~\ref{fig:data-statistics} suggests that our tasks cover a broad range of difficulties, and working to solve them presents an opportunity to improve the capabilities of LLM agents. We observe that, when judging success, our evaluator tends to prefer binary predictions with high confidence values, suggesting that this subset of predictions is accurate based on Figure~\ref{fig:robust-evaluation} results. Analyses for the agents that produced Figure~\ref{fig:data-statistics} are presented in Appendix~\ref{appendix:more-data-statistics}.

%\vspace{0.7cm}

Assembling the stages of the InSTA pipeline, we first generated tasks for 1M diverse sites across the Internet, we then dispatched language model agents to complete tasks for 150k sites marked as safe, and finally we evaluated the trajectories using a language model judge. Equipped with this large and diverse source of data, we seek to understand the utility of the data for \textit{training} agents.