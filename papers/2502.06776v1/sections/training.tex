We compare the performance of agents trained on data from the InSTA pipeline to agents trained on human demonstrations from WebLINX \citep{WebLINX} and Mind2Web \citep{Mind2Web}, two recent and popular benchmarks for web navigation. Recent works that mix synthetic data with real data control the real data sampling probability in the batch $p_{\text{real}}$ independently from data size \citep{DAFusion}. We employ $p_{\text{real}} = 0.5$ in few-shot experiments and $p_{\text{real}} = 0.8$ otherwise. Shown in Figure~\ref{fig:data-statistics}, our data have a wide spread in performance, so we apply several filtering rules to select high-quality training data. First, we require the evaluator to return \texttt{conf} = 1 that the task was successfully completed, and that the agent was on the right track (this selects data where the actions are reliable, and directly caused the task to be solved). Second, we filter data where the trajectory contains at least three actions. Third, we remove data where the agent encountered any type of server error, was presented with a captcha, or was blocked at any point. These steps produce $7,463$ high-quality demonstrations in which agents successfully completed tasks on diverse websites. We sample 500 demonstrations uniformly at random from this pool to create a diverse test set, and employ the remaining $6,963$ demonstrations to train agents on a mix of real and synthetic data.

\subsection{Improving Data-Efficiency}
\label{sec:few-shot}

\begin{wrapfigure}{r}{0.48\textwidth}
    \centering
    \vspace{-0.8cm}
    \includegraphics[width=\linewidth]{assets/few_shot_results_weblinx_mind2web.pdf}
    \vspace{-0.3cm}
    \caption{\small \textbf{Data from InSTA improves efficiency.} Language model agents trained on mixtures of our data and human demonstrations scale faster than agents trained on human data. In a setting with 32 human actions, adding our data improves \textit{Step Accuracy} by +89.5\% relative to human data for Mind2Web, and +122.1\% relative to human data for WebLINX.}
    \vspace{-0.2cm}
    \label{fig:few-shot-results}
\end{wrapfigure}

In a data-limited setting derived from WebLINX \citep{WebLINX} and Mind2Web \citep{Mind2Web}, agents trained on our data \textit{scale faster with increasing data size} than human data alone. Without requiring laborious human annotations, the data produced by our pipeline leads to improvements on Mind2Web that range from +89.5\% in \textit{Step Accuracy} (the rate at which the correct element is selected and the correct action is performed on that element) with 32 human actions, to +77.5\% with 64 human actions, +13.8\% with 128 human actions, and +12.1\% with 256 human actions. For WebLINX, our data improves by +122.1\% with 32 human actions, and +24.6\% with 64 human actions, and +6.2\% for 128 human actions. Adding our data is comparable in performance gained to doubling the amount of human data from 32 to 64 actions. Performance on the original test sets for Mind2Web and WebLINX appears to saturate as the amount of human data increases, but these benchmark only test agent capabilities for a limited set of 150 popular sites.

\subsection{Improving Generalization} 
\label{sec:generalization}

\begin{wrapfigure}{r}{0.48\textwidth}
    \centering
    \vspace{-1.0cm}
    \includegraphics[width=\linewidth]{assets/diverse_results_weblinx_mind2web.pdf}
    \vspace{-0.3cm}
    \caption{\small \textbf{Our data improves generalization.} We train agents with all human data from the WebLINX and Mind2Web training sets, and resulting agents struggle to generalize to more diverse test data. Adding our data improves generalization by +149.0\% for WebLINX, and +156.3\% for Mind2Web.}
    \vspace{-0.3cm}
    \label{fig:generalization-results}
\end{wrapfigure}

To understand how agents trained on data from our pipeline generalize to diverse real-world sites, we construct a more diverse test set than Mind2Web and WebLINX using 500 held-out demonstrations produced by our pipeline. Shown in Figure~\ref{fig:generalization-results}, we train agents using all human data in the training sets for WebLINX and Mind2Web, and compare the performance with agents trained on 80\% human data, and 20\% data from our pipeline. Agents trained with our data achieve comparable performance to agents trained purely on human data on the official test sets for the WebLINX and Mind2Web benchmarks, suggesting that when enough human data are available, synthetic data may not be necessary. However, when evaluated in a more diverse test set that includes 500 sites not considered by existing benchmarks, agents trained purely on existing human data struggle to generalize. Training with our data improves generalization to these sites by +149.0\% for WebLINX agents, and +156.3\% for Mind2Web agents, with the largest gains in generalization \textit{Step Accuracy} appearing for harder tasks.