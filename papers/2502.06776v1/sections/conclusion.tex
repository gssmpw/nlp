We present a pipeline that unlocks new scaling potential for language model agents by generating internet-scale web navigation data for diverse sites. We employ pretrained language models to generate, attempt, and evaluate web navigation tasks at scales beyond what humans can manually annotate. Our results show that language models outperform human annotators at detecting safe websites, and proposing feasible web navigation tasks. In addition, we find that language models can judge successful web navigation trajectories with an accuracy up to 93.1\% for their most confident predictions. We scale task generation to the top 1M sites in the CommonCrawl PageRank, and test agents on 150k live web navigation tasks. Agents based on \textit{Llama 3.1 70B} solve 16.7\% of these tasks zero-shot. In data-limited settings derived from Mind2Web and WebLINX, we improve \textit{Step Accuracy} by up to +89.5\% and +122.1\% respectively for agents trained on mixtures of data from our pipeline, and human data. When training agents with all available human data from these benchmarks, agents fail to generalize to diverse real sites, and adding our data improves their generalization by +149.0\% for WebLINX and +156.3\% for Mind2Web. 

Our work reveals several exciting directions for future work. First, our work can be scaled further. The latest CommonCrawl release contains data for more than 300 million sites, suggesting another \textit{1,000} times more data than we explored could be available for training agents by scaling the pipeline. In addition, our language model judge was employed offline, and its high accuracy suggests that it could be used to guide an online algorithm. Finally, we considered primarily text-based agents in this work, and our pipeline could be extended to generate data for multimodal tasks. 