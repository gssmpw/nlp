\paragraph{Language Model Agents.} There is an emerging paradigm in modern NLP using language models \citep{GPT2,GPT3,Llama,Llama2} as the backbone for agents \citep{LanguageModelAgents}. These models show impressive reasoning capabilities \citep{SparksAGI,EvaluateO1,CanLRMsPlan} that allow them to generalize to downstream applications, such as web navigation, where text formats differ significantly from their training data. Search algorithms provide a secondary axis to improve the reasoning capabilities of language model agents \citep{TreeOfThoughts,GraphOfThoughts,LLMTreeSearch,LanguageAgentTreeSearch} by providing an explicit algorithmic scaffold and allowing test-time compute to improve reasoning steps \citep{ScalingTestTimeCompute,EvaluateO1}. Although most of the work focuses on running language models as zero-shot agents, fine-tuning language models to improve their effectiveness as agents is becoming popular \citep{AgentQ,AgentTuning,AppAgent,CogAgent,LLMAgentSurvey1,LLMAgentSurvey2} as target benchmarks are becoming more difficult for zero-shot language models.

\paragraph{Agent Pipelines.} There are a growing number of agent pipelines aimed at fine-tuning language models to improve their effectiveness as agents \citep{AgentInstruct,AgentTuning,AgentQ,FireAct,SynatraSyntheticData}. However, driven by the limited data available, many such works train on data with significant overlap with their test environment, either with different tasks for the same environment configuration as the test setting \citep{Mind2Web}, or the same tasks \citep{AgentQ}. We instead consider a setting where tasks and environment configurations are entirely separate between training and testing, creating a strong train-test split that follows recommended practice. This presents a challenge: the web navigation data for training LLM agents is limited \citep{Mind2Web,WebLINX}. We address this challenge by increasing the scale of data collection and better coverage of the distribution of real-world sites. We train on diverse tasks generated by our pipeline and successfully transfer agents trained on our data to downstream benchmarks while maintaining a strong train-test split. Our training procedure resembles a modified FireAct \citep{FireAct}, where language models jointly propose and evaluate tasks for agents.

\paragraph{Agent Datasets.} The majority of datasets for training web navigation agents rely on human annotators to create tasks \citep{WebArena,VisualWebArena,AndroidInTheWild}, and provide demonstrations \citep{Mind2Web,WebLINX,AndroidInTheWild,ScribeAgent}. This approach has limits, as the breadth and diversity of tasks researchers can manually curate are dwarfed by the volume of sites on the Internet. There are more than three-hundred million sites on the Internet according to \citet{CommonCrawl}, and existing datasets are limited to about 150 popular sites that human annotators are already familiar with \citep{Mind2Web,WebLINX,ScribeAgent}. There are hypothetical \textit{1,000,000} times more data that could be available if we can efficiently harness this previously untapped resource. However, most sites are relatively obscure and human annotators are unreliable for sites they are not already familiar with. Finding suitable annotators becomes impractical at this massive scale, so we adapt language models to propose, attempt, and evaluate web navigation tasks. Although we are not the first to consider synthetic data for training agents \citep{RetrieveAndTransform,SynatraSyntheticData,LLMSyntheticMathData,LLMSyntheticPreferenceData}, we have developed a key approach to harness internet-scale data efficiently.

\paragraph{Language Model Judges.} Core to our pipeline is a language model evaluator. Using language models to judge the correctness of responses is becoming popular to improve accuracy for LLMs \citep{LLMAsJudgeSurvey}, and applications include verifying reasoning steps \citep{GenerativeVerifiers}, rejection sampling \citep{ScalingTestTimeCompute,BestOfNSpeculativeRejection}, prioritizing frontier nodes in search algorithms \citep{LanguageAgentTreeSearch,LLMTreeSearch}, filtering out harmful responses \citep{LlamaGuard}, providing feedback for response improvement \citep{SelfRefine,Refiner,LLMSelfImprove,TextGrad}, and providing ratings for alignment \citep{RLAIF,RLHF}. Our use of language models to evaluate agent tasks is inspired by the generative verifier in \citet{GenerativeVerifiers}, and modified from the multimodal verifier in \citet{WebVoyager}, where our language model predicts a confidence score that a task is solved, which is used to identify successful attempts.