The predominant approach to training LLM-based web navigation agents is to collect human demonstrations on a set of manually curated websites and tasks \citep{Mind2Web,WebArena,AgentQ,VisualWebArena,VisualWebBench,WebLINX,AndroidInTheWild}. Human data can be laborious to collect and becomes costly to scale as the breadth of skills that users require from language model agents grows. There are more than three-hundred million sites on the Internet \citep{CommonCrawl}, and the range of sites that researchers can manually prepare for human annotation represents a tiny fraction of the Internet. The key problem is that human data can become unreliable at scale. Human-written web navigation tasks are highly effective for popular sites, but reliability drops for sites with lower popularity due to annotators' lack of familiarity. For these sites with lower popularity, which represent the majority of sites on the internet, human-written web navigation tasks are feasible only 40\% of the time (discussed in Section~\ref{sec:generation}), requiring a costly manual verification step. For the same pool of sites, language models improve feasibility rates to more than 80\%.
There is a growing need to automate data pipelines for the next generation of agents trained at internet scale. We address a key challenge by reducing the dependency on human data in the agent pipeline.
We develop an automatic data pipeline that aims to facilitate Internet-Scale Training for Agents, which we shorten to InSTA, a pipeline that relies on synthetic web navigation tasks proposed, attempted, and evaluated by language models.

\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{assets/icml2025_insta_pipeline_main.pdf}
    \vspace{-0.3cm}
    \caption{\small \textbf{Overview of the proposed agent pipeline.} We develop a pipeline for training web navigation agents at internet scale using tasks proposed, attempted, and evaluated by pretrained large language models. We generate 150k diverse tasks across 1M internet sites. Code for our data generation pipeline, and traces for agent rollouts will be available on our website: \href{https://data-for-agents.github.io}{data-for-agents.github.io}.}
    \vspace{-0.3cm}
    \label{fig:pipeline-main}
\end{figure*}

Our method operates in three stages. In the first stage, we employ a language model to propose candidate web navigation tasks for an agent to perform across 150k live sites on the Internet. Current works are limited to 200 popular sites \citep{WebLINX,AndroidInTheWild,Mind2Web} that human annotators are likely to be familiar with. Language models help us scale to \textit{1,000} times more sites than current efforts, with better coverage of real-world sites. One major consideration when scaling up training for agents is safety: building safe agents requires that we avoid sites with harmful, unsafe, or dangerous content. We evaluated the ability of language models to detect such content and aggressively filtered out 85\% candidates from the initial 1M sites to 150k sites that are judged safe by language models. These models succeed at detecting safe content with an accuracy of~97\%, compared to~75\% human accuracy. With tasks generated across a safe and diverse set of websites, we proceed to run language model agents to attempt the generated tasks.

In the second stage of the pipeline, a language model agent attempts to complete tasks using a web browser. We provide the entire Playwright API to the agent, which operates the browser by generating function calls in the Playwright API, a standard choice in recent web navigation benchmarks \citep{BrowserGym,AnyTool,WorkArena}. While existing LLM agents can be directly applied to our tasks, measuring their success presents another challenge: sites organize data in bespoke formats. Previous efforts circumvent this by enlisting human labelers to review attempts manually and judge their success \citep{WebLINX,Mind2Web}, or by designing sites for benchmarking where formats are consistent \citep{WebArena,VisualWebArena}. However, neither approach scales efficiently, and human annotators may not be reliable for sites they are not familiar with in the tail of the distribution. In the third stage of the pipeline, we scale the evaluation using language models. We employ LLMs to judge \citep{VerifyStepByStep} whether a task is solved by the final timestep and obtain an accuracy of up to~93.1\% in detecting successful trajectories for the most confident predictions. Llama-3.1-70B-Instruct solves 16.7\% of zero-shot tasks with a judge confidence of \texttt{conf} = 1. In data-limited settings derived from Mind2Web and WebLINX, we improve \textit{Step Accuracy} by up to +89.5\% and +122.1\%, respectively, when training agents on mixtures of data from our pipeline and human data. When training agents with all human data from these benchmarks, agents fail to generalize to diverse real sites, and adding our data improves their generalization by +149.0\% for WebLINX and +156.3\% for Mind2Web.

Our work develops InSTA, an automated data pipeline that aims to facilitate Internet-scale training for agents. Driven by pretrained language models, we generate web navigation tasks for 150k live websites, run LLM agents, and judge their success. By removing humans from the agent pipeline, we can scale and extend coverage towards sites where human data is less reliable. We present an analysis to improve safety and show that our data can outperform human demonstrations for training agents. Code for the pipeline will be available at: \href{https://data-for-agents.github.io}{data-for-agents.github.io}.