\section{Related Work}
\label{sec:relatedwork}
\vspace{-0.02in}
This section reviews energy anomaly detection in FL and discussed adversarial attacks.

\subsection{Energy Anomaly Detection and Federated Learning}
\label{sub_sec:AnomalyFL}
% \vspace{-0.05in}

Anomaly detection in energy has been an active research area due to its critical importance in detecting sub-optimal performance, device malfunction, and abnormal behavior, thereby contributing to energy efficiency. Several studies  \cite{AnomalyDNN, ChalaDNN2020, somu2021deep, srCNN2021} proposed anomaly detection based on Deep Neural Networks (DNN) due to their ability to model complex relationships. Recently, LSTMs have gained popularity in the energy domain due to their ability to capture temporal dependencies \cite{ChalaDNN2020, somu2021deep}. 

In the Natural Language Processing (NLP) domain, Transformer has been renowned for its attention mechanism and the generative capabilities utilized in diverse applications such as ChatGPT \cite{achiam2023gpt}. Consequently, Transformer has been adapted in various energy data studies \cite{nazir2023forecasting, zhang2021power}. Energy forecasting is commonly used with Transformers in self-supervised anomaly detection solutions. Such solutions examine the difference between the actual and predicted values: if this difference exceeds a threshold, the sample is deemed abnormal. Zhang \etal \cite{zhang2021power} combined Transformer and K-means clustering to forecast energy consumption and detect anomalies. Nazir \etal \cite{nazir2023forecasting} also presented a Transformer-based solution focusing on energy forecasting. As LSTMs and Transformers have dominated energy anomaly detection and forecasting, our study considered these two architectures in the FL setting. 

These energy forecasting/anomaly detection techniques typically train models centrally where the data sharing raises privacy and security risks. Due to its data privacy-enhancing capabilities and distributed nature, FL has been gaining popularity for various tasks in the energy domain. Fekri \etal \cite{DistributedLoad2022} proposed a distributed load forecasting method based on FL which takes advantage of LSTM as the base learner. The same group further advanced FL-based forecasting to enable asynchronous learning in the presence of non-IDD data by introducing a novel aggregation technique \cite{AsynchronousFL}. Similarly, Sater \etal \cite{Sater2020AFL} also integrated LSTM but combined it with multi-task learning for anomaly detection in smart buildings. The approach proposed by Jithish \etal \cite{jithish2023distributed} for anomaly detection in smart grids is also based on FL. They considered the diversity of ML models as the base learning, including LSTM, Gated Recurrent Neural Networks (GRU), and Vanilla RNN. 

The studies employing FL in the energy field made great strides in improving energy prediction and anomaly detection by enabling model training without sharing raw data. Nevertheless, they did not consider the possible presence of malicious clients in the federation. Consequently, we address this gap by examining the vulnerability of FL-based anomaly detection techniques to adversarial attacks.

\subsection{Adversarial Attacks}
\label{sub_sec:AdversarialAttack}
% \vspace{-0.05in}

Adversarial attacks are designed to deceive the ML model, leading to incorrect classifications. Often examined in computer vision, adversarial samples are created by altering the images so they still look normal to the human eye but lead the model to incorrect predictions. Goodfellow \etal \cite{goodfellow2014explaining} proposed the Fast Gradient Sign Method (FGSM) that utilizes the gradients of a neural network to craft adversarial input image samples. In the FGSM approach, the gradient is calculated only once. To extend this, Kurakin \etal \cite{kurakin2017adversarial} introduced the Basic Iterative Method (BIM), an iterative approach that adds perturbations to the input data. Similarly, Madry \etal \cite{madry2017towards} introduced another iterative attack named Projected Gradient Descent (PGD). PGD uses random initialization and a projection step which iteratively alters the input to improve the generated samples. However, these algorithms were designed for the computer vision domain, adding perturbations to images.

Considering the transferability of the mentioned attack models, Fawaz \etal \cite{fawaz2019adversarial} emphasized that adversarial attacks have not been thoroughly explored for time series classification. Thus, they perturbed time series data using FGSM and BIM and compared the effectiveness of the attack models. Mode \etal \cite{mode2020adversarial} also considered adversarial attacks on multivariate time-series data and adapted adversarial attacks from the image domain, such as FGSM and BIM, to deep learning regression models for multivariate time series forecasting. However, these studies  \cite{fawaz2019adversarial, mode2020adversarial} only considered centralized ML.  

Overall, adversarial attacks were mostly considered in the vision domain, with a few works investigating time-series \cite{fawaz2019adversarial, mode2020adversarial} but in the centralized setting. Furthermore, Bondok \etal \cite{bondok2023novel} studied attacks in an FL setting; however, they focused on theft detection while our study focuses on generic anomaly detection. Nevertheless, there is a need to understand how adversarial attacks affect anomaly detection in the FL setting. To address this gap, our work investigates the vulnerability of FL-based anomaly detection models, specifically LSTM and Transformer, to adversarial attacks of various strengths and compares their vulnerability to that of centralized training.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%