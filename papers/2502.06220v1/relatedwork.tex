\section{Related Work}
\subsection{SAM-based Applications in the Medical Field}
Due to the powerful segmentation performance and generalization ability of the SAM, many works have applied it to image segmentation tasks in different domains. However, the performance of SAM in medical domain is more limited due to its lack of medical specific knowledge and facing challenges such as low image contrast, fuzzy tissue boundaries and tiny lesion regions in medical images. And in order to solve these challenges and apply SAM to the medical field, some methods have been proposed nowadays, SAMed \cite{zhang2023customizedsegmentmodelmedical} utilizes a low-rank (Lora) strategy to fine-tune the local parameters of SAM, which reduces the computational overhead and makes it more adaptable to the medical field. MedSAM \cite{WOS:001148371500004}, through a large amount of data and freezing the image encoder and the prompt Encoder, the Mask Decoder was completely fine-tuned. AutoSAM \cite{INSPEC:23394927} changed the original prompter encoder structure of SAM and explored three unprompted prediction headers. Med-SA \cite{wu2023medicalsamadapteradapting} configured adapter correlation network layers for each of the three parts that make up the SAM and only fine-tuned it locally. SAMUS \cite{INSPEC:23780196} introduced a CNN branch and also used cross-branching attention to improve the performance of SAM in the medical domain.

\subsection{Parameter Effective Fine Tuning (PEFT) }
Parameter Efficient Fine-Tuning (PEFT) is a strategy for efficiently fine-tuning models by selectively fine-tuning a small fraction (typically less than 5\% of the total parameters) of the model parameters while keeping the majority of the parameters frozen, which avoids catastrophic forgetting and minimizes computational and storage requirements. PEFT includes adapter-based techniques, selective parameter tuning, prompt-driven fine-tuning, and Low Rank Adaptation (LoRA). In the adapter paradigm \cite{WOS:000684034302095}, small layers of trainable parameters are added to each layer of a pre-trained model, enabling the model to adapt to a new task without significantly increasing the number of parameters. Some approaches \cite{guo2021parameterefficienttransferlearningdiff} proposed a selective fine-tuning strategy, which reduces the cost of fine-tuning by updating the parameters of only a few layers, while maintaining high task performance. Prompt tuning \cite{WOS:000855966303015} \cite{WOS:000698679200153} proposes tuning the model's behavior by using cues as additional inputs or inserting cues in the middle layer of the model, respectively. LoRA \cite{hu2021lora} introduced trainable low-rank matrices in the transformer layer for weight updating.

The PEFT technique has also been shown to be effective in the field of computer vision (CV). Recent studies have shown that adaptive techniques can be easily applied to a variety of downstream computer vision tasks \cite{article} \cite{chen2022adaptformeradaptingvisiontransformers}. Therefore, we believe that the adaptive approach is the best choice for introducing SAM into the medical field. We foresee that this concise and powerful method will bring broader possibilities for the development of basic medical models.

\begin{figure*}[!t]
    \centering
    \includegraphics[width = 16cm]{fig2.pdf}
\caption{Overview of the proposed FunduSAM consists of Adapter layers, Spatial Attention Module and Channel Attention Module in CBAM}
    \label{fig2_framework}
\end{figure*}