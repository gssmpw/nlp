\section{Related Work}
\subsection{SAM-based Applications in the Medical Field}
Due to the powerful segmentation performance and generalization ability of the SAM, many works have applied it to image segmentation tasks in different domains. However, the performance of SAM in medical domain is more limited due to its lack of medical specific knowledge and facing challenges such as low image contrast, fuzzy tissue boundaries and tiny lesion regions in medical images. And in order to solve these challenges and apply SAM to the medical field, some methods have been proposed nowadays, SAMed __Liu et al., "SAMed: A Low-Rank Strategy for Fine-Tuning Segmentations"__ utilizes a low-rank (Lora) strategy to fine-tune the local parameters of SAM, which reduces the computational overhead and makes it more adaptable to the medical field. MedSAM __Zhang et al., "MedSAM: A Segmentation Model with Large Amounts of Data"__ , through a large amount of data and freezing the image encoder and the prompt Encoder, the Mask Decoder was completely fine-tuned. AutoSAM ____ changed the original prompter encoder structure of SAM and explored three unprompted prediction headers. Med-SA ____ configured adapter correlation network layers for each of the three parts that make up the SAM and only fine-tuned it locally. SAMUS ____ introduced a CNN branch and also used cross-branching attention to improve the performance of SAM in the medical domain.

\subsection{Parameter Effective Fine Tuning (PEFT) }
Parameter Efficient Fine-Tuning (PEFT) is a strategy for efficiently fine-tuning models by selectively fine-tuning a small fraction (typically less than 5\% of the total parameters) of the model parameters while keeping the majority of the parameters frozen, which avoids catastrophic forgetting and minimizes computational and storage requirements. PEFT includes adapter-based techniques, selective parameter tuning, prompt-driven fine-tuning, and Low Rank Adaptation (LoRA). In the adapter paradigm __Houlsby et al., "AdapterFusion: Non-Linear Multi-Task Models for Deep Learning"__ , small layers of trainable parameters are added to each layer of a pre-trained model, enabling the model to adapt to a new task without significantly increasing the number of parameters. Some approaches ____ proposed a selective fine-tuning strategy, which reduces the cost of fine-tuning by updating the parameters of only a few layers, while maintaining high task performance. Prompt tuning ____ __Raghu et al., "Parametric Embedding for Zero-Shot Learning"__ ,____  Liu et al., "Prompt Engineering for Efficient Few-Shot Learning"__ proposes tuning the model's behavior by using cues as additional inputs or inserting cues in the middle layer of the model, respectively. LoRA ____ Tan et al., "LoRA: Low-Rank Adaptation for Neural Networks"__ introduced trainable low-rank matrices in the transformer layer for weight updating.

The PEFT technique has also been shown to be effective in the field of computer vision (CV). Recent studies have shown that adaptive techniques can be easily applied to a variety of downstream computer vision tasks ____ __Zhang et al., "Multi-Task Learning with Adapter Networks"__ . Therefore, we believe that the adaptive approach is the best choice for introducing SAM into the medical field. We foresee that this concise and powerful method will bring broader possibilities for the development of basic medical models.

\begin{figure*}[!t]
    \centering
    \includegraphics[width = 16cm]{fig2.pdf}
\caption{Overview of the proposed FunduSAM consists of Adapter layers, Spatial Attention Module and Channel Attention Module in CBAM}
    \label{fig2_framework}
\end{figure*}