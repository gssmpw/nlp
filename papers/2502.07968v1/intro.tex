\section{Introduction}

%%%%%%In recent years, it is becoming more and more crucial to develop the capability of machine learning models in dealing with tasks that follow a different distribution from training data.
In recent years, it has become increasingly crucial to develop machine learning models that can handle tasks with test data distributions differing from training data, commonly referred to as out-of-distribution (OOD) generalization~\citep{mansour2009domain,blanchard2011generalizing,muandet2013domain,beery2018recognition,recht2019imagenet,su2019one}. Such disparities, termed as \textit{distribution shifts}, can substantially undermine the %%%%%%performance\tz{decrease the performance sounds strange. change to "impact"?} 
efficacy of the empirical risk minimization (ERM) paradigm, which presumes consistency in data distribution across training and test phases~\citep{quinonero2008dataset,lazer2014parable,zhang2018mixup}. While there exist numerous works~{\citep{hu2018does,krueger2021out,chang2020invariant,sagawa2020distributionally,koh2021wilds}} on OOD generalization for i.i.d. (independent and identically distributed) data (e.g., images), few have focused on graph-structured data, despite the prevalence of distribution shifts in real-world graphs~\citep{fakhraei2015collective, gui2022good,yu2023mind}. For instance, in citation networks~\citep{hu2020open}, the distribution of paper topics (i.e., node labels) may considerably change over time, leading to differences in graph structures. However, Graph Neural Networks (GNNs)~\citep{kipf2017semi,hamilton2017inductive, xu2018powerful, zhou2020graph, you2020design}, despite being the de facto choice to model graphs, often fall short in addressing the challenge of distribution shifts on OOD graph data~\citep{albadawy2018deep,dai2018dark,li2022out,tan2022graph,zhang2024survey}.
%%%%%%This is because the citation relationships between papers (i.e., graph structures) can experience significant changes~\citep{yang2016revisiting}\tz{I cannot see the causal relationship here. I feel like topic change will lead to structure change. How about change to something like: "which also lead to the change in graph structure, i.e., the citation relationships, over time."}. 
%%%%%%%Overlooking this distribution shift, the learned Graph Neural Networks (GNNs) 
%%%%%%%%%thus resulting in performance degradation on OOD data. 
%%%%%%As a result, the learned Graph Neural Networks (GNNs) will inevitably encounter performance degradation on OOD data during inference, due to the domain shifts between training data and test data~\citep{dai2018dark,albadawy2018deep}.


% Invariant learning assumes that the information of each instance for prediction includes two parts, i.e., invariant part whose relationship with the label is stable across different environments, and variant part whose relationship with the label can change across different environments
Existing works for OOD generalization primarily focus on identifying \textit{invariant relationships} across diverse data distributions, generally referred to as \textit{environments} (or domains)~\citep{arjovsky2019invariant,chang2020invariant,ahuja2020invariant}. They typically
aim to identify this invariant relationship, which maps input invariant features to the outputs (e.g., labels), through robust optimization or learning an invariant feature space~\citep{creager2021environment,krueger2021out}. Regarding OOD generalization on graphs, existing works~\citep{chen2022learning,miao2022interpretable,chen2023does,tan2023virtual,wang2024enhancing} primarily aim to identify an invariant subgraph $G_c$ from a given graph $G$ for predictions. 
However, such an extraction strategy could be subpar for completely capturing the invariant information, which could be mixed with spurious information in a graph and could not be distinctly separated~\citep{bevilacqua2021size}. 
%Due to the discrete nature of graph data, the extracted subgraphs may not authentically present the invariant information~\citep{bevilacqua2021size}. 
% zhao2019learning,,liu2022graph,gui2023joint
For example, due to the complicated interactions (as edges) of atoms (as nodes) in a molecule graph, extracting a node may inevitably incorporate both invariant and spurious information, thus failing to achieve a precise invariant subgraph~\citep{gui2022good}.
%when the invariant subgraph $G_c$ and the spurious subgraph $G_s=G\setminus G_c$ are closely related, a discrete extraction may result in either the incorporation of spurious information or loss of invariant information. Due to the complex structures in various graphs, the identification of such distinct invariant subgraphs can be more challenging
Concretely, the strategy of extracting (discrete) structures may not extract invariant information on graphs. 

To deal with this, we propose an innovative framework, named Generative Risk Minimization (GRM), to fully exploit invariant information on graphs.  Different from the distinct extraction of invariant subgraphs used in existing works~\citep{chen2023does,
chen2022learning}, the core idea of GRM is
to generate the invariant subgraphs in a continuous manner. 
In particular, the generated invariant subgraph preserves the same set of nodes as the input $G$, while possessing continuous edge weights and node representations. 
This design allows us to flexibly preserve the invariant information without the need for extracting discrete structures, which could potentially lead to loss of invariant information. To ensure that the generated subgraphs contain sufficient invariant information, our proposed GRM framework involves two objectives: (1)~generation objective, which aims to generate precise subgraphs with continuous edge weights and node representations, and (2) invariant objective, which ensures the independence of the invariant subgraph and the domains.
%and 3) spurious objective, which reduces the environment-related spurious information in the invariant subgraph.

Although the above two objectives are straightforward and intuitive, it is challenging to directly optimize them, especially when the domain labels are unavailable~\citep{wudiscovering}. Therefore, we transform our GRM objective into three correlated losses that could be used for optimization, based on our theoretical analysis. In particular, our GRM framework achieves two attractive properties for OOD generalization. (1) Maximally involving invariant information. We introduce a variational approximation of the latent causal variable $Z$ to both the generation objective and the invariant objective. Our derivation results ensure that the learned latent representation of $Z$ involves minimal loss of invariant information. 
(2) Minimally involving spurious information. %As the domain labels are generally not observed in practice, 
%existing works typically propose to generate new environments or infer the underlying environment labels to impose the independence of the invariant subgraph and the environments. Nevertheless, 
Our GRM framework could directly minimize the mutual information between the invariant subgraph and the domains when combined with our generation objective, based on our theoretical analysis. The derived loss forces the generator to focus less on domain-related information and thereby reduces the incorporation of spurious information. 
In summary, our contributions are as follows:
\begin{itemize}[leftmargin=0.5cm] % yc: change to remove large indents.
    \item We develop the Generative Risk Minimization (GRM) framework, a novel approach that aims to generate invariant subgraphs for graph OOD generalization.
    \item We provide a theoretical analysis that ensures the effectiveness of the generated subgraphs and sheds light on the rationales of GRM and its validity in graph OOD generalization tasks.
    \item We evaluate GRM through extensive experiments on various real-world datasets that cover multiple types of distribution shifts. The results validate the superiority of GRM over state-of-the-art baselines.
\end{itemize}






\section{Preliminaries}

\begin{wrapfigure}{r}{0.45\textwidth}
    % \vspace{-0.15in}
\centering
\scalebox{1}{
        \includegraphics[width=0.99\linewidth]{model_ood.png}}
                            % \vspace{-0.2in}
        \caption{The SCMs with distribution shift (left) and without distribution shifts (right).}            \label{fig:SCM}
                    \vspace{-0.05in}
\end{wrapfigure}
In this section, we provide the formulation for our studied graph OOD generalization problem. 
%Notably, our framework is capable of both node and graph classification tasks.
We start by representing a graph (or a local subgraph of a node in node-level tasks) as $G=(\mathcal{V},\mathcal{E},\bX)$, where $\mathcal{V}$ and $\mathcal{E}$ are the node set and the edge set, respectively. Moreover, $\mathbf{X}\in\mathbb{R}^{|\mathcal{V}|\times d_x}$ is a feature matrix, where the $j$-th row vector ($d_x$-dimensional) represents the attribute of the $j$-th node. 
%Notice that in graph OOD generalization, each domain typically refers to a single graph~\citep{wuhandling,gui2022good}. %%%%%with a potentially massive number of nodes. 
We can define the distribution of a graph and its label from domain $D_i$ as $(G,Y)\sim P(G,Y|D_i)$, where $Y\in\mathcal{Y}$ is the label of $G$. Here $\mathcal{Y}$ is the label space shared across domains. We further denote the training and test domains (i.e., graphs) as $\mathcal{D}_{tr}=\{D_1,D_2, \dotsc, D_{|\mathcal{D}_{tr}|}\}$ and $\mathcal{D}_{te}=\{D_1,D_2, \dotsc, D_{|\mathcal{D}_{te}|}\}$, respectively. Generally, existing works for OOD generalization on graphs primarily rely on the Structural Causal Models (SCMs), as shown in Fig.~\ref{fig:SCM}, to interpret distribution shifts on graphs~\citep{chen2022learning,chen2023does}. Specifically, the observed graphs \( G \) and labels \( Y \) are affected by the latent causal variable \( Z \) and a spurious variable \( S \), which decide the underlying invariant subgraph \( G_c \) and the spurious subgraph \( G_s \), respectively. As the spurious variable $S$ is related to the domain $D$, existing works aim to identify the invariant subgraph $G_c$ for the precise prediction of its label $Y$, without the effect of $S$. Specifically, the goal is to develop an invariant GNN, represented as \( \mathcal{M}:= f_c \circ g \). This model comprises: 1) an extractor \( g: \mathcal{G} \to \mathcal{G}_c \) that identifies the invariant subgraph \( G_{c} \), and 2) a classifier \( f:  \mathcal{G}_c \to \mathcal{Y} \) that predicts the label \( Y \) using the extracted \( G_{c} \), where \( \mathcal{G}_c \) denotes the space of subgraphs within \( G \). 
%In this manner, the learning objectives for \( f_c \) and \( g \) are formulated as follows:
%\begin{equation}
%    \max _{f_c, g} I(\widehat{G}_c ; Y),\ \  \st\ \  \widehat{G}_c \perp D,\ \widehat{G}_c=g(G).
%\end{equation}
However, the extractor $g$ in existing works could only output a discrete invariant subgraph $\widehat{G}_c$, which is a subset of edges and nodes in the input graph $G$. As a result, the invariant information and spurious information cannot be entirely separated when a node or edge consists of both types of information. 



\begin{comment}
    

\subsection{GRM Objective}
In this subsection, we formulate the three objectives of GRM for optimization. 


\mypara{Generation Objective.}
The optimization objective of our generator $g$ is to maximize the log-likelihood term $\log P(\widehat{G}_c|G,E)$, which is the probability of the generated adaptive subgraph $\widehat{G}_c$, conditioned on the input graph $G$ and the environment $E$. 


\mypara{Invariant Objective.}
To maximally capture the invariant information in the generated invariant subgraphs $\widehat{G}_c$, we propose to maximize the mutual information between $\widehat{G}_c$ and its label $Y$, i.e., $I(\widehat{G}_c ; Y)$. This objective is derived from the assumption that the invariant subgraph should be predictive of its label under different environments.

\mypara{Spurious Objective.}
To ensure the independence of the invariant subgraph $\widehat{G}_c$ and the environment $E$, it is imperative to reduce their correlations, generally represented as $\widehat{G}_c \perp E$. As the environment labels are typically unavailable, existing works have resorted to creating more environments or inferring environment labels. In our GRM framework, we propose to reduce the mutual information between $\widehat{G}_c$ and $E$. 

\mypara{Overall GRM Objective.}
The main idea of GRM is to generate invariant subgraphs such that they preserve crucial label-relevant information for classification while reducing the correlations between them and the environments. 

In this manner, our proposed objective is as follows:
\begin{equation}
 \max_{f_c, g}\ \mathbb{E}\left[ \log P(\widehat{G}_c|G,E) \right] +I(\widehat{G}_c ; Y) - I(\widehat{G}_c ; E)
\end{equation}

\subsection{Lower Bound Derivation of GRM Objective}
 

\mypara{Latent Causal Variable Z}
Although our GRM objective is straightforward for enhancing OOD generalization, it is difficult to directly use it to guide the generation of invariant subgraphs $\widehat{G}_c$ from $G$. Thus, we propose to leverage the latent causal variable $Z$ that is decisive for the input graph $G$ and the label $Y$, as illustrated in Fig.~\ref{fig:scm}. 
In this manner,  

Notably, the direct optimization of $I(\widehat{G}_c ; E)$ is still infeasible due to the absence of environment labels. Nevertheless, we could derive an alternative loss when combining this objective with the generation objective.
\subsection{Optimization of GRM}
\end{comment}


%Use generators, reasons: modifying data is more stable, easy to incorporate domain embeddings

%Two challenges: domain embedding achieve, and lack of ground truth

%while disregarding the spurious relationships that are specific within different domains~\citep{gulrajanisearch}.  



%\begin{itemize}
%    \item Motivation: New domains are not suitable for our GNN/classifier. Therefore, we hope to generate domain-specific augmentations of input nodes for classification (in contrast to invariant methods). Our method is good for cases with little invariant information across domains.
%    \item Method: We use VGAE to generate augmentations of input nodes, while incorporating domain embeddings.
%    \item Experiments: We use baselines and datasets from EERM. We may consider additional experiments that magnify input variance among domains, to validate our method.
%\end{itemize}
