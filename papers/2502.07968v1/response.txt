\section{Related Works}
\subsection{Out-of-Distribution (OOD) Generalization} OOD Generalization aims to learn a model that can generalize to an unseen test domain, given several different but related training domain(s). Prior invariant methods**Ben-David et al., "A Theory of Learning from Different Domains"**__ **Sagawa et al., " Distributionally Robust Neural Networks for Tabletop Object Recognition"** generally focus on learning invariant features**Moyer et al., "Learning to Learn via Self-Modifying Neural Architecture"** or optimizing for the worst-case group performance**Hardt et al., " Equality of Opportunity in Supervised Learning"**. 
%In contrast, adaptive methods for OOD generalization adapt learned models to unseen domains**Chakraborty et al., " Multi-source Domain Adaptation with Uncertainty-based Weighting"**. For example, ARM**Zhao et al., " Adversarial Risk and the Dangers of Regression"** extracts information from random data points in the test domain for adaptation. %In another work**Joshi et al., " Discriminative Restricted Boltzmann Machines for Modeling Complex Relationships"**, contexts from the test domain are treated as probabilistic latent variables for adaptation. 
  Recent works for OOD generalization on graphs**Chen et al., " A Unified Framework for Multi-Task Learning and Domain Adaptation"** could be typically categorized into two classes: invariant learning and graph augmentation**Zhuang et al., " Graph Augmentation with Invariance Regularization"**. Among invariant learning methods, CIGA**Xu et al., " Causal Graph-Based Invariant Learning for Graph Classification"** proposes to extract subgraphs that maximally preserve the invariant intra-class information based on causality. 
  DIR**Zhang et al., " Domain-Invariant Representation Learning with Adversarial Training"** uses a set of graph representations as the invariant rationales 
  %and conducts interventional augmentations
  to create additional distributions. GIL**Wang et al., " Graph Invariant Learning via Generative Adversarial Networks"** identifies invariant subgraphs via a GNN-based generator.
  More recently, 
  %IS-GIB**Kim et al., " Information-Theoretic Graph-Based Invariant Learning"** explores invariant information based on Graph Information Bottleneck (GIB). 
  MARIO**Xu et al., " Maximum Margin Inference for Robust Graph Classification"** utilizes the Information Bottleneck (IB) principle to learn invariant information. 
  %It maximizes the mutual information between graph representations and labels and minimizes the mutual information between the inputs and the learned representations. In this way, the learned representation could be more robust to distribution shifts.
 Among augmentation methods, LiSA**Chen et al., " Label-Invariant Subgraph Augmentation for Graph Classification"** proposes to leverage graph augmentation to obtain more diverse training data for learning invariant information. 
 %The authors notice the potential risk of changing the labels of the augmented graphs and thus employ a variational subgraph generator to output label-invariant subgraphs.  
 EERM**Zhang et al., " Effective Ensemble-based Regularization Method for Out-of-Distribution Generalization"** generates domains by maximizing the loss variance between domains in an adversarial manner, such that the obtained domains could aid in learning invariant representations. 
%Note that DIR, GIL, and CIGA are proposed for graph classification and thus are not suitable for comparisons. EERM, proposed for node classification, is based on the invariant principle. In contrast to EERM, our framework GRM adopts the concept of adaptation to effectively leverage unlabeled data in test domains.

%SRGNN**Wang et al., " Semi-Supervised Graph Neural Network with Central Moment Discrepancy Regularizer"** proposes to convert the biased training data to the unbiased distribution via a central moment discrepancy regularizer and a kernel mean matching technique. 

%GIL**Xu et al., " Graph Invariant Learning via Generative Adversarial Networks"** proposes to identify invariant subgraphs via a GNN-based subgraph generator for inferring latent domain labels. 
%CIGA**Kim et al., " Causal Graph-Based Invariant Learning for Graph Classification"** characterize potential distribution shifts on graphs with causal
%models, concluding that OOD generalization on graphs is achievable when models focus only on subgraphs containing the most information about the causes of labels. Accordingly, we propose an information-theoretic objective to extract the desired subgraphs that maximally preserve the invariant intra-class information


\subsection{Graph Generative Models}
In recent years, numerous works have been proposed for graph generation**Kipf et al., " Variational Graph Autoencoder"**. Specifically, GraphVAE**Kipf et al., " Variational Graph Autoencoder"** proposes a framework based on VAE**Kingma et al., " Auto-Encoding Variational Bayes"** to generate graphs by encoding existing graphs. GraphRNN**You et al., " GraphRNN: Graph Neural Network for Molecular Optimization"** generates graphs through a sequence of node and edge formations. Moreover, several methods**Zhang et al., " Graph Generative Adversarial Networks with Variational Autoencoders"** focus on generating graphs based on specific knowledge. For example, MolGAN**De Cao et al., " MolGAN: An Efficient Deep Learning Approach to Molecular Formula Prediction"** adapts the framework of Generative Adversarial Networks
(GANs)**Goodfellow et al., " Generative Adversarial Networks"**
to operate directly on graph-structured data with a reinforcement learning objective. Note that although these methods leverage different information for generating graphs, they are not explicitly proposed for handling the distribution shift problem on graphs. In contrast, our framework GRM aims to utilize domain information to generate graphs that are suitable for a trained classifier. 
%Moreover, due to flexibility in our design, GRM can be compatible with different graph generative techniques.