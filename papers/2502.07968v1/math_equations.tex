
\documentclass[10pt]{article} % For LaTeX2e
\usepackage{tmlr}
% If accepted, instead use the following line for the camera-ready submission:
% \usepackage[accepted]{tmlr}
% To de-anonymize and remove mentions to TMLR (for example for posting to preprint servers), instead use the following:
%\usepackage[preprint]{tmlr}


\usepackage{hyperref}
\usepackage{url}
\usepackage{graphicx}
\usepackage{dsfont}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{booktabs}

\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{multirow}


\usepackage{algorithm}
\usepackage{comment}
\usepackage{amsmath}
\usepackage{bm}
\usepackage{amsfonts}
\usepackage{amsthm}
\usepackage{graphicx}
\usepackage{enumitem}
\usepackage{booktabs}
\usepackage{array}
\usepackage{siunitx}  % For aligning numbers at the decimal point
\usepackage{algpseudocode}
% \usepackage{sub}
% \usepackage{algorithmic}
\usepackage{multirow}
\usepackage{subcaption}
\usepackage{bm}
%\usepackage{bbm}
%\usepackage[switch]{lineno}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{prop}[theorem]{Proposition}
\newtheorem{property}[theorem]{Property}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{remark}{Remark}
\newtheorem{definition}{Definition}
\newtheorem{assumption}{Assumption}
\usepackage{pifont}
\usepackage{wrapfig}

\usepackage{comment}
\usepackage{tikz}
%\usepackage{pgfplots}
\usepackage{xcolor}
%\usepackage{wrapfig}
\usepackage{url}
\usepackage{inconsolata}
\usepackage{eurosym}
\usepackage{todonotes} 
\usepackage{subcaption}
\usepackage{amssymb}
\usepackage{lipsum}
\usepackage{amsmath}
\usepackage{enumitem}
\usepackage{array}
\usepackage{longtable}
\usepackage{makecell}
\usepackage{xspace}
\usepackage{tikz}
\usepackage{xcolor,colortbl}
\usepackage[most]{tcolorbox}
\usepackage{arydshln}
\usepackage{adjustbox}
\newcommand{\dline}{\hdashline[0.5pt/1pt]}
\usepackage{tikz}
\usepackage[tikz]{bclogo}
\usepackage{pgfplots}
\pgfplotsset{width=1.0\columnwidth}
\usepackage{multirow}
% \usepackage{fontawesome}
%\usepackage[fixed]{fontawesome5}
\usepackage{makecell}
\usepackage{pifont}
%\usepackage{bbm}
\usepackage{rotating}
\usepackage{tablefootnote}
\usepackage{soul}
\usepackage{booktabs}
\usepackage{tikz}
\usepackage[tikz]{bclogo}
\usepackage{pgfplotstable}
\usepackage{mdframed}
\usepackage{graphicx}
\usepackage{verbatim}
\usepackage{tokcycle}
\usepackage{fancyvrb,fvextra}
% \usepackage{subfigure}
\usepackage{filecontents}
\usepackage{subcaption}
\usepackage{tablefootnote}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{url}
%\usepackage{hyperref}
%\usepackage{subfig}
\usepackage{comment}
% \usepackage{amsfonts,amssymb}
\usepackage{amsfonts}
\usepackage{color, soul}
\usepackage{mathrsfs}
\usepackage{cleveref}
\usepackage{multirow}
\usepackage{float}
%\usepackage{wrapfig}
\usepackage{amsthm}
\usepackage{bm}
% \usepackage[title]{appendix}
%\usepackage{bbm}
\usepackage{algorithm}
\usepackage{algorithmicx}
\usepackage{algpseudocode}
\usepackage{colortbl}
\usepackage{enumitem}
\usepackage{color}
% \usepackage{flushend}
%\usepackage{balance}
%\usepackage{stfloats}
\usepackage{makecell}
%\usepackage{bbold}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{fontawesome}
% \newcommand{\yd}[1]{\stepcounter{todocounter}
%   {\color{blue} YD: #1}}
\newcommand{\yd}[1]{\textcolor{blue}{\textbf{yd: #1}}}
\newcommand{\sw}[1]{\textcolor{red}{\textbf{#1}}}
\newcommand{\yj}[1]{\textcolor{purple}{[yj: #1]}}

\usepackage{xcolor}         % colors

\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%
\def \mL{\mathcal L}
\def \T{\mathbb T}
\def \R{\mathbb R}



\def \st{\;\text{s.t.}\;}
\def\etal{et al.\;}
\newcommand{\bX}{\mathbf{X}}
\newcommand{\bU}{\mathbf{U}}
\newcommand{\bV}{\mathbf{V}}
\newcommand{\bu}{\mathbf{u}}
\newcommand{\bv}{\mathbf{v}}
\newcommand{\bL}{\mathbf{L}}
\newcommand{\bD}{\mathbf{D}}
\newcommand{\bA}{\mathbf{A}}
\newcommand{\bY}{\mathbf{Y}}
\newcommand{\bZ}{\mathbf{Z}}
\newcommand{\bb}{\mathbf{b}}
\newcommand{\bs}{\mathbf{s}}
\newcommand{\bB}{\mathbf{B}}
\newcommand{\bS}{\mathbf{S}}
\newcommand{\bW}{\mathbf{W}}
\newcommand{\bH}{\mathbf{H}}
\newcommand{\bh}{\mathbf{h}}
\newcommand{\bO}{\mathbf{O}}

\newcommand{\bx}{\mathbf{x}}
\newcommand{\by}{\mathbf{y}}
\newcommand{\bz}{\mathbf{z}}


%\newcommand{\p}{\mathbf{p}}
\newcommand{\bsig}{\mathbf{\sigma}}
\newcommand{\bSig}{\mathbf{\Sigma}}
% norms
\newcommand{\NM}[2]{\| #1 \|_{#2} }  
\newcommand{\SO}[1]{\mathcal{P}_{\mathbf{\Omega}}(#1)}
\newcommand{\Tr}[1]{\text{tr}( #1 ) }

\newcommand{\Diag}[1]{\text{Diag}(#1)}
\newcommand{\mypara}[1]{{\smallskip \noindent \bf #1}\hspace{0.1in}}
\usepackage{array}
\newcolumntype{L}[1]{>{\raggedright\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}
\newcolumntype{C}[1]{>{\centering\let\newline  \\\arraybackslash\hspace{0pt}}m{#1}}
\newcolumntype{R}[1]{>{\raggedleft\let\newline \\\arraybackslash\hspace{0pt}}m{#1}}
\newcommand\norm[1]{\left\lVert#1\right\rVert}
% Replace "require" and "ensure" with "input" and "output" in algorithmic package of LaTeX
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}
\newcommand{\colorann}[3]{\textcolor{#1}{${}^{#2}[$#3$]$}}
\newcommand{\JL}[1]{\colorann{red}{JL}{#1}}
\newcommand{\red}{\color{red}}
\newcommand{\zt}[1]{\colorann{blue}{zt}{#1}}
\newcommand{\rc}[1]{\colorann{brown}{rc}{#1}}
%\newcommand{\sw}[1]{\colorann{red}{sw}{#1}}
\newcommand{\zy}[1]{\colorann{cyan}{zy}{#1}}

\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}

%\usepackage{ bbold }
\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\argmin}{argmin}

\newtheorem{innercustomthm}{Theorem}
\newenvironment{customthm}[1]
  {\renewcommand\theinnercustomthm{#1}\innercustomthm}
  {\endinnercustomthm}

%
% These are are recommended to typeset listings but not required. See the subsubsection on listing. Remove this block if you don't have listings in your paper.
\usepackage{newfloat}
\usepackage{listings}

\title{Generative Risk Minimization for Out-of-Distribution \\ Generalization on Graphs}

% Authors must not appear in the submitted version. They should be hidden
% as long as the tmlr package is used without the [accepted] or [preprint] options.
% Non-anonymous submissions will be rejected without review.

\author{\name Xingbo Fu \email xf3av@virginia.edu \\
        \addr Department of Electrical and Computer Engineering \\
        University of Virginia
        \AND
        \name Song Wang \email sw3wv@virginia.edu \\
        \addr Department of Electrical and Computer Engineering \\
        University of Virginia
        \AND
        \name Yushun Dong \email yd6eb@virginia.edu \\
        \addr Department of Electrical and Computer Engineering \\
        University of Virginia
        \AND
        \name Binchi Zhang \email epb6gw@virginia.edu \\
        \addr Department of Electrical and Computer Engineering \\
        University of Virginia
        \AND
        \name Chen Chen \email zrh6du@virginia.edu \\
        \addr Department of Electrical and Computer Engineering \\
        University of Virginia
        \AND
        \name Jundong Li \email jundong@virginia.edu\\
        \addr Department of Electrical and Computer Engineering \\
        University of Virginia
      }

% The \author macro works with any number of authors. Use \AND 
% to separate the names and addresses of multiple authors.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\def\month{10}  % Insert correct month for camera-ready version
\def\year{2024} % Insert correct year for camera-ready version
\def\openreview{\url{https://openreview.net/forum?id=mVAp0eDfyR}} % Insert correct link to OpenReview for camera-ready version


\begin{document}


\maketitle

\documentclass[12pt]{article}
\usepackage{amsmath, amssymb, amsfonts}
\usepackage{graphicx}

\title{Learning User and Ad Representation}
\author{Rong Jin}
\date{November 23, 2024}

\begin{document}

\maketitle

\section*{Introduction}
Let $\mathbf{Z}_k \in \mathbb{R}^d$ be the vector representation of user $i_k$ interacting with ad $j_k$ for event $k$. Our goal is to learn a vector representation for user $i_k$, denoted by $\mathbf{u}_{i_k}$, and a vector representation for ad $j_k$, denoted by $\mathbf{v}_{j_k}$, that can well approximate the user-ad joint representation $\mathbf{Z}_k$.

\section*{Optimization Problem}
The joint interaction representation $\mathbf{Z}_k$ can be reformulated as a matrix of size $d_u \times d_v$, where $d = d_u \times d_v$. We approximate $\mathbf{Z}_k$ using a rank-one matrix $\mathbf{u}_{i_k}\mathbf{v}_{j_k}^\top$. This leads to the following optimization problem:
\[
\min_{\mathbf{U} \in \mathbb{R}^{d_u \times n_u}, \mathbf{V} \in \mathbb{R}^{d_v \times n_v}} L(\mathbf{U}, \mathbf{V}) = \frac{1}{2N} \sum_{k=1}^N \left\|\mathbf{u}_{i_k}\mathbf{v}_{j_k}^\top - \mathbf{Z}_k\right\|_F^2
\]
Here, $\mathbf{U}$ and $\mathbf{V}$ are matrices containing the vector representations of users and ads, respectively, and $N$ is the number of training examples.

\subsection*{Gradient Descent Updates}
Using gradient descent, the updates for $\mathbf{u}_{i_k}$ and $\mathbf{v}_{j_k}$ are:
\[
\mathbf{u}_{i_k} \leftarrow \mathbf{u}_{i_k} - \eta \left((\mathbf{u}_{i_k}\mathbf{v}_{j_k}^\top - \mathbf{Z}_k)\mathbf{v}_{j_k}\right)
\]
\[
\mathbf{v}_{j_k} \leftarrow \mathbf{v}_{j_k} - \eta \left((\mathbf{v}_{j_k}\mathbf{u}_{i_k}^\top - \mathbf{Z}_k^\top)\mathbf{u}_{i_k}\right)
\]

\subsubsection*{Explanation}
The gradient updates above minimize the difference between the predicted interaction $\mathbf{u}_{i_k}\mathbf{v}_{j_k}^\top$ and the true interaction $\mathbf{Z}_k$. The learning rate $\eta$ controls the step size during each iteration.

To ensure regularization and prevent overfitting, a small constant $\gamma > 0$ is added to the weight decay factor:
\[
\mathbf{u}_{i_k} \leftarrow (1 - \eta(\gamma + \|\mathbf{v}_{j_k}\|^2))\mathbf{u}_{i_k} + \eta\mathbf{Z}_k\mathbf{v}_{j_k}
\]
\[
\mathbf{v}_{j_k} \leftarrow (1 - \eta(\gamma + \|\mathbf{u}_{i_k}\|^2))\mathbf{v}_{j_k} + \eta\mathbf{Z}_k^\top\mathbf{u}_{i_k}
\]

\section*{Special Case}
Assuming a constant vector $\bar{\mathbf{v}}$ for ad embeddings leads to:
\[
\mathbf{u}_i = \eta \sum_{k=1}^{|S_u^i|} (1 - \eta \gamma)^{|S_u^i|-k} \mathbf{Z}_{j_{ik}}
\]
\[
\bar{\mathbf{u}} = \eta^2 \bar{\mathbf{F}}\bar{\mathbf{G}}\bar{\mathbf{u}}
\]

\subsubsection*{Explanation}
Here, $\bar{\mathbf{u}}$ must be the largest singular vector of $\bar{\mathbf{F}}\bar{\mathbf{G}}$. This formulation simplifies computation and provides a basis for estimating user embeddings.

\section*{Algorithm 1: Estimating User Embedding}
The algorithm iteratively updates user and ad embeddings, assuming a constant counterpart vector. It is detailed as follows:

\begin{enumerate}
    \item Initialize $\mathbf{F}_i$ and $\mathbf{G}_j$ for all users and ads.
    \item Update $\mathbf{F}_i$ and $\mathbf{G}_j$ using interaction data.
    \item Compute $\bar{\mathbf{u}}$ and $\bar{\mathbf{v}}$ as the largest eigenvectors of aggregated matrices.
    \item Update embeddings iteratively.
\end{enumerate}

\section*{Algorithm 2: Alternating Estimation}
To improve on Algorithm 1, the alternating estimation method iterates between user and ad embedding updates, removing the assumption of constant vectors.

\section*{Extension to Rank-K Approximation}
To generalize, we introduce rank-$K$ representations $\mathbf{A}_i$ and $\mathbf{B}_i$, leading to:
\[
\min_{\mathbf{A}, \mathbf{B}} L(\mathbf{A}, \mathbf{B}) = \frac{1}{2N} \sum_{k=1}^N \left\|\mathbf{A}_{i_k}\mathbf{B}_{j_k}^\top - \mathbf{Z}_k\right\|_F^2
\]

Gradient updates for rank-$K$:
\[
\mathbf{A}_{i_k} \leftarrow (1 - \eta(\gamma + \|\mathbf{B}_{j_k}\|^2))\mathbf{A}_{i_k} + \eta\mathbf{Z}_k\mathbf{B}_{j_k}
\]
\[
\mathbf{B}_{j_k} \leftarrow (1 - \eta(\gamma + \|\mathbf{A}_{i_k}\|^2))\mathbf{B}_{j_k} + \eta\mathbf{Z}_k^\top\mathbf{A}_{i_k}
\]

\subsubsection*{Explanation}
The rank-$K$ approximation captures richer interactions by introducing multiple representations for users and ads.



Here is the LaTeX representation of the given text:

\subsection{Notations}

Let $A$ and $B$ denote the matrices containing user embeddings from tables A and B, respectively. Here, we use $A$ to represent active users and $B$ to represent inactive users.

\begin{itemize}
    \item $n$: The number of users in both tables.
    \item $d$: The dimensionality of the user embeddings.
    \item $k$: The desired dimensionality of the augmented embeddings.
\end{itemize}

We first perform concatenation of the embeddings from tables A and B. The concatenated matrix can be represented as:

$$C = \begin{bmatrix} A \\ B \end{bmatrix} \in \mathbb{R}^{2n \times d}$$

where $;$ denotes vertical concatenation.

The SVD of the concatenated matrix $C$ can be written as:

$$C = U \Sigma V^T$$

where:

\begin{itemize}
    \item $U \in \mathbb{R}^{2n \times 2n}$: An orthogonal matrix containing the left-singular vectors.
    \item $\Sigma \in \mathbb{R}^{2n \times d}$: A diagonal matrix containing the singular values.
    \item $V \in \mathbb{R}^{d \times d}$: An orthogonal matrix containing the right-singular vectors.
\end{itemize}

With the decomposed results, we further aim to select top $k$ singular values and their corresponding singular vectors. In particular, we take the first $k$ columns of $U$, $\Sigma$, and $V$. This can be represented as:
$$U_k = U_{:, :k} \in \mathbb{R}^{2n \times k}$$
$$s_k = \text{diag}(\Sigma_{:k, :k}) \in \mathbb{R}^k$$
$$V_k = V_{:k, :} \in \mathbb{R}^{k \times d}$$

With these results, we propose further to project the high-quality embeddings from table A onto selected singular vectors. The process is described as follows:

$$E = U_k \cdot \text{Diag}(s_k) \cdot V_k \cdot A$$

\end{document}





\end{document}
