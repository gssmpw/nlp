\section{Related Work}
\label{sec:related-works}
\subsection{Novel View Synthesis}
Novel view synthesis is a foundational task in the computer vision and graphics, which aims to generate unseen views of a scene from a given set of images.
% Many methods have been designed to solve this problem by posing it as 3D geometry based rendering, where point clouds~\cite{point_differentiable,point_nfs}, mesh~\cite{worldsheet,FVS,SVS}, planes~\cite{automatci_photo_pop_up,tour_into_the_picture} and multi-plane images~\cite{MINE,single_view_mpi,stereo_magnification}, \etal
Numerous methods have been developed to address this problem by approaching it as 3D geometry-based rendering, such as using meshes~\cite{worldsheet,FVS,SVS}, MPI~\cite{MINE,single_view_mpi,stereo_magnification}, point clouds~\cite{point_differentiable,point_nfs}, etc.
% planes~\cite{automatci_photo_pop_up,tour_into_the_picture}, 


\begin{figure*}[!t]
    \centering
    \includegraphics[width=1.0\linewidth]{figures/overview-v7.png}
    %\caption{\textbf{Overview.} Given a set of images, our method obtains both camera intrinsics and extrinsics, as well as a 3DGS model. First, we obtain the initial camera parameters, global track points from image correspondences and monodepth with reprojection loss. Then we incorporate the global track information and select Gaussian kernels associated with track points. We jointly optimize the parameters $K$, $T_{cw}$, 3DGS through multi-view geometric consistency $L_{t2d}$, $L_{t3d}$, $L_{scale}$ and photometric consistency $L_1$, $L_{D-SSIM}$.}
    \caption{\textbf{Overview.} Given a set of images, our method obtains both camera intrinsics and extrinsics, as well as a 3DGS model. During the initialization, we extract the global tracks, and initialize camera parameters and Gaussians from image correspondences and monodepth with reprojection loss. We determine Gaussian kernels with recovered 3D track points, and then jointly optimize the parameters $K$, $T_{cw}$, 3DGS through the proposed global track constraints (i.e., $L_{t2d}$, $L_{t3d}$, and $L_{scale}$) and original photometric losses (i.e., $L_1$ and $L_{D-SSIM}$).}
    \label{fig:overview}
\end{figure*}

Recently, Neural Radiance Fields (NeRF)~\cite{2020NeRF} provide a novel solution to this problem by representing scenes as implicit radiance fields using neural networks, achieving photo-realistic rendering quality. Although having some works in improving efficiency~\cite{instant_nerf2022, lin2022enerf}, the time-consuming training and rendering still limit its practicality.
Alternatively, 3D Gaussian Splatting (3DGS)~\cite{3DGS2023} models the scene as explicit Gaussian kernels, with differentiable splatting for rendering. Its improved real-time rendering performance, lower storage and efficiency, quickly attract more attentions.
% Different from NeRF-based methods which need MLPs to model the scene and huge computational cost for rendering, 3DGS has stronger real-time performance, higher storage and computational efficiency, benefits from its explicit representation and gradient backpropagation.

\subsection{Optimizing Camera Poses in NeRFs and 3DGS}
Although NeRF and 3DGS can provide impressive scene representation, these methods all need accurate camera parameters (both intrinsic and extrinsic) as additional inputs, which are mostly obtained by COLMAP~\cite{colmap2016}.
% This strong reliance on COLMAP significantly limits their use in real-world applications, so optimizing the camera parameters during the scene training becomes crucial.
When the prior is inaccurate or unknown, accurately estimating camera parameters and scene representations becomes crucial.

% In early works, only photometric constraints are used for scene training and camera pose estimation. 
% iNeRF~\cite{iNerf2021} optimizes the camera poses based on a pre-trained NeRF model.
% NeRFmm~\cite{wang2021nerfmm} introduce a joint optimization process, which estimates the camera poses and trains NeRF model jointly.
% BARF~\cite{barf2021} and GARF~\cite{2022GARF} provide new positional encoding strategy to handle with the gradient inconsistency issue of positional embedding and yield promising results.
% However, they achieve satisfactory optimization results when only the pose initialization is quite closed to the ground-truth, as the photometric constrains can only improve the quality of camera estimation within a small range.
% Later, more prior information of geometry and correspondence, \ie monocular depth and feature matching, are introduced into joint optimisation to enhance the capability of camera poses estimation.
% SC-NeRF~\cite{SCNeRF2021} minimizes a projected ray distance loss based on correspondence of adjacent frames.
% NoPe-NeRF~\cite{bian2022nopenerf} chooses monocular depth maps as geometric priors, and defines undistorted depth loss and relative pose constraints for joint optimization.
In earlier studies, scene training and camera pose estimation relied solely on photometric constraints. iNeRF~\cite{iNerf2021} refines the camera poses using a pre-trained NeRF model. NeRFmm~\cite{wang2021nerfmm} introduces a joint optimization approach that simultaneously estimates camera poses and trains the NeRF model. BARF~\cite{barf2021} and GARF~\cite{2022GARF} propose a new positional encoding strategy to address the gradient inconsistency issues in positional embedding, achieving promising results. However, these methods only yield satisfactory optimization when the initial pose is very close to the ground truth, as photometric constraints alone can only enhance camera estimation quality within a limited range. Subsequently, 
% additional prior information on geometry and correspondence, such as monocular depth and feature matching, has been incorporated into joint optimization to improve the accuracy of camera pose estimation. 
SC-NeRF~\cite{SCNeRF2021} minimizes a projected ray distance loss based on correspondence between adjacent frames. NoPe-NeRF~\cite{bian2022nopenerf} utilizes monocular depth maps as geometric priors and defines undistorted depth loss and relative pose constraints.

% With regard to 3D Gaussian Splatting, CF-3DGS~\cite{CF-3DGS-2024} also leverages mono-depth information to constrain the optimization of local 3DGS for relative pose estimation and later learn a global 3DGS progressively in a sequential manner.
% InstantSplat~\cite{fan2024instantsplat} focus on sparse view scenes, first use DUSt3R~\cite{dust3r2024cvpr} to generate a set of densely covered and pixel-aligned points for 3D Gaussian initialization, then introduce a parallel grid partitioning strategy in joint optimization to speed up.
% % Jiang et al.~\cite{Jiang_2024sig} proposed to build the scene continuously and progressively, to next unregistered frame, they use registration and adjustment to adjust the previous registered camera poses and align unregistered monocular depths, later refine the joint model by matching detected correspondences in screen-space coordinates.
% \gjh{Jiang et al.~\cite{Jiang_2024sig} also implemented an incremental approach for reconstructing camera poses and scenes. Initially, they perform feature matching between the current image and the image rendered by a differentiable surface renderer. They then construct matching point errors, depth errors, and photometric errors to achieve the registration and adjustment of the current image. Finally, based on the depth map, the pixels of the current image are projected as new 3D Gaussians. However, this method still exhibits limitations when dealing with complex scenes and unordered images.}
% % CG-3DGS~\cite{sun2024correspondenceguidedsfmfree3dgaussian} follows CF-3DGS, first construct a coarse point cloud from mono-depth maps to train a 3DGS model, then progressively estimate camera poses based on this pre-trained model by constraining the correspondences between rendering view and ground-truth.
% \gjh{Similarly, CG-3DGS~\cite{sun2024correspondenceguidedsfmfree3dgaussian} first utilizes monocular depth estimation and the camera parameters from the first frame to initialize a set of 3D Gaussians. It then progressively estimates camera poses based on this pre-trained model by constraining the correspondences between the rendered views and the ground truth.}
% % Free-SurGS~\cite{freesurgs2024} matches the projection flow derived from 3D Gaussians with optical flow to estimate the poses, to compensate for the limitations of photometric loss.
% \gjh{Free-SurGS~\cite{freesurgs2024} introduces the first SfM-free 3DGS approach for surgical scene reconstruction. Due to the challenges posed by weak textures and photometric inconsistencies in surgical scenes, Free-SurGS achieves pose estimation by minimizing the flow loss between the projection flow and the optical flow. Subsequently, it keeps the camera pose fixed and optimizes the scene representation by minimizing the photometric loss, depth loss and flow loss.}
% \gjh{However, most current works assume camera intrinsics are known and primarily focus on optimizing camera poses. Additionally, these methods typically rely on sequentially ordered image inputs and incrementally optimize camera parameters and scene representation. This inevitably leads to drift errors, preventing the achievement of globally consistent results. Our work aims to address these issues.}

Regarding 3D Gaussian Splatting, CF-3DGS~\cite{CF-3DGS-2024} utilizes mono-depth information to refine the optimization of local 3DGS for relative pose estimation and subsequently learns a global 3DGS in a sequential manner. InstantSplat~\cite{fan2024instantsplat} targets sparse view scenes, initially employing DUSt3R~\cite{dust3r2024cvpr} to create a densely covered, pixel-aligned point set for initializing 3D Gaussian models, and then implements a parallel grid partitioning strategy to accelerate joint optimization. Jiang \etal~\cite{Jiang_2024sig} develops an incremental method for reconstructing camera poses and scenes, but it struggles with complex scenes and unordered images. 
% Similarly, CG-3DGS~\cite{sun2024correspondenceguidedsfmfree3dgaussian} progressively estimates camera poses using a pre-trained model by aligning the correspondences between rendered views and actual scenes. Free-SurGS~\cite{freesurgs2024} pioneers an SfM-free 3DGS method for reconstructing surgical scenes, overcoming challenges such as weak textures and photometric inconsistencies by minimizing the discrepancy between projection flow and optical flow.
%\pb{SF-3DGS-HT~\cite{ji2024sfmfree3dgaussiansplatting} introduced VFI into training as additional photometric constraints. They separated the whole scene into several local 3DGS models and then merged them hierarchically, which leads to a significant improvement on simple and dense view scenes.}
HT-3DGS~\cite{ji2024sfmfree3dgaussiansplatting} interpolates frames for training and splits the scene into local clips, using a hierarchical strategy to build 3DGS model. It works well for simple scenes, but fails with dramatic motions due to unstable interpolation and low efficiency.
% {While effective for simple scenes, it struggles with dramatic motion due to unstable view interpolation and suffers from low computational efficiency.}

However, most existing methods generally depend on sequentially ordered image inputs and incrementally optimize camera parameters and 3DGS, which often leads to drift errors and hinders achieving globally consistent results. Our work seeks to overcome these limitations.
