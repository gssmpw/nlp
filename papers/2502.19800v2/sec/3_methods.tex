% \section{Preliminaries}
%First, we review the 3DGS definition and the training loss function. Then, we theoretically derive the gradient of the loss function with respect to the camera intrinsics, in preparation for the joint optimization of camera parameters and 3DGS.

\section{3D Gaussian Splatting}\label{subsec:3dgs}
3DGS models a scene using a set of 3D anisotropic Gaussians. Each Gaussian is parameterized by a centroid $\mu\in\mathbb{R}^{3}$, a quaternion factor $q\in\mathbb{R}^{4}$, a scale factor $s\in\mathbb{R}^{3}$, spherical harmonics (SH) coefficients of color $c\in\mathbb{R}^{k}$, and opacity $\alpha\in\mathbb{R}$. 
Donating the rotation matrix of quaternion $q$ and scale matrix of $s$ by $R\in\mathbb{R}^{3\times 3}$ and $S=\mathrm{diag}(s)$, the covariance matrix $\Sigma$ and Gaussian function $G(x)$ are:
\begin{equation}\label{eq:cov}
    \Sigma=RSS^\top R^\top, G(x)=\exp{(-\frac{1}{2}(x-\mu)^\top\Sigma^{-1}(x-\mu))}.
\end{equation}
% \begin{equation}\label{eq:cov}
% \begin{aligned}
%      \Sigma&=RSS^\top R^\top, \\
%      G(x)&=\exp{(-\frac{1}{2}(x-\mu)^\top\Sigma^{-1}(x-\mu))}.
% \end{aligned}
% \end{equation}
Denoting projection matrix $T_{cw}=[R_{cw}|t_{cw}]$, which transforms points from the \textit{world} to \textit{camera} coordinate space, an image rendered from the specified view can be obtained as follows. First, the covariance matrix in camera coordinates $\Sigma^{\mathrm{2D}}$ is obtained by approximating the projection of 3D Gaussian in pixel coordinates, and can be expressed as:
\begin{equation}\label{eq:cov-2d}
    \Sigma^{\mathrm{2D}}=JR_{cw}\Sigma R_{cw}^\top J^\top,
\end{equation}
where $J$ is the Jacobian of the affine approximation of the projective transformation. The final rendered color $\hat{C}$ can be denoted as the alpha-blending of $N$ ordered Gaussians:
\begin{equation}
    \label{eq:3dgs-color}    
    \hat{C}=\sum_{i}^{N}c_{i}\alpha_{i}\prod_{j}^{i-1}(1-\alpha_{j}),
\end{equation}
where $c_{i}$ and $\alpha_{i}$ are the color and opacity of the Gaussians. 
Similarly, the depth of the scene preceived of a pixel is,
\begin{equation}
    \label{eq:3dgs-depth}
    \hat{D}=\sum_{i}^{N}d_{i}\alpha_{i}\prod_{j}^{i-1}(1-\alpha_{j}),
\end{equation}
where $d_i$ denotes the z-axis coordinate for the transformed Gaussian centers in the camera space.

Usually, the parameters of 3D Gaussians are optimized by rendering and comparing the rendered images with the ground-truths. The loss function $\mathcal{L}$ is defined as follows:
\begin{equation}\label{eq:loss_3dgs}
    \mathcal{L} = (1-\lambda)L_{1} + \lambda L_{\mathrm{D-SSIM}}.
\end{equation}
Typically, 3D Gaussians are initialized with Structure from Motion (SfM) point clouds obtained from the input images. 
% Please refer to \cite{3DGS2023} for more details.


\section{Method}
\textbf{Overview.}
% \lb{Given a set of images $\mathcal{I}=\{I_{i}\}_{i=1}^{M}$, the extrinsic matrix and the intrinsic matrix for each image $I_{i}$ are denoted by $T_{cw,i}$ and $K$, respectively.
% Our method aims to simultaneously estimate camera intrinsics and extrinsics, as well as generate a 3D Gaussian Splatting (3DGS) model, as shown in Fig.~\ref{fig:overview}. We leverage the global track constraint to explicitly capture and enforce multi-view geometric consistency, which is the fundamental assumption for both camera parameter estimation and 3DGS model training.}
Given a set of images $\mathcal{I}=\{I_{i}\}_{i=1}^{M}$, with unknown extrinsic matrix $T_{cw,i}$ at each view and unknown intrinsic matrix denoted by $K$, our method aims to build a 3D Gaussian Splatting (3DGS) model while simultaneously estimating both the extrinsic and intrinsic matrices, as shown in Fig.~\ref{fig:overview}. To achieve this goal, our key approach is to leverage the global track constraint to explicitly capture and enforce multi-view geometric consistency, which serves as the foundation for accurately estimating both the 3DGS model and the camera parameters.
Specifically, during initialization, we construct Maximum Spanning Tree (Sec.~\ref{subsec:init}) based on 2D matched feature points and extract global tracks. Then we initialize both the camera parameters and subsequent 3D Gaussians with the estimated 3D track points.
Building on this, we propose an effective joint optimization method with three loss terms: 2D track loss, 3D track loss, and scale loss. 
% The 2D track loss minimizes reprojection errors on the image plane, while the 3D track loss reduces backprojection errors within the 3D scene, ensuring multi-view geometric consistency.
The 2D and 3D track losses are minimized to ensure multi-view geometric consistency.
The scale loss constrains the track Gaussians remain aligned with the scene's surface while preserving the expressive capability of the 3DGS model.
% \cs{To ensure that all parameters in the joint optimization process are differentiable, we also derived and implemented the differentiable components of the camera intrinsic parameters.}
% \lb{Since the entire optimization process is differentiable with respect to both camera intrinsics and extrinsics, we leverage the chain rule to achieve seamless joint optimization of the camera parameters and the 3DGS model.}
% To ensure differentiability in the joint optimization process, 
We derive and implement the differentiable components of the camera parameters, including both the extrinsic and intrinsic matrices. This allows us to apply the chain rule, enabling seamless joint optimization of the 3DGS model and the camera parameters.

%Given a set of images $\mathcal{I}=\{I_{i}\}_{i=1}^{M}$, the extrinsic matrix for each image $I_{i}$ and the intrinsic matrix are denoted by $T_{cw,i}$ and $K$, respectively. Our method aims to simultaneously obtain both camera intrinsics and extrinsics, as well as a 3DGS model, as demonstrated in Fig.~\ref{fig:overview}. Due to the incorporation of additional variables, i.e., camera parameters, we enhance the original 3DGS with several key designs. 
%\pb{First, inspired by COLMAP,  we incorporate global track information for the images and select the Gaussian kernels associated with the track points. We use reprojection loss to further constrain the parameters of 3DGS and camera, through multi-view geometric consistency. To incorporate the computation of the reprojection errors without affecting the original role of remaining 3D Gaussians, we require that the tracked Gaussians are automatically scaled to an infinitely small size, distributed near the actual surface of the scene. Next, we propose an efficient joint optimization framework for training. As the optimized intrinsic is also important for the improvement of novel view synthesis and to make the whole program differentiable, we provide the closed formula for the gradients of both camera parameters.}


\subsection{Initialization}\label{subsec:init}

%\pb{Before our joint optimization framework, we extract global track information from inputs. As providing better initial values for the camera parameters and Gaussians helps to accelerate convergence and avoid falling into local suboptimal solutions, those parameters are initialized based on the extracted information.}

\textbf{Global Tracks.}
We begin by extracting 2D feature points $\{p_{i}\}$ from each image $I$ and computing feature matches across all images using off-the-shelf algorithms~\cite{detone2018superpoint,sarlin20superglue}. To organize these matches into global tracks, we first construct a Maximum Spanning Tree (MST) using Kruskal's algorithm, where the node represents each image and the weight of each edge is determined by the number of feature matching pairs between two images. By traversing MST and the feature points, we use the Union-Find algorithm to extract global track $\mathcal{P}$ and remove short tracks for robustness.

\textbf{Camera Parameters.}
We assume all cameras share a standard pinhole model with no distortion, and the principal point locates at the center of the image, then the intrinsic matrix $K$ of camera is:
\begin{equation}\label{eq:intrinsic}
    K=\begin{bmatrix}
        f_{x} & 0 & c_{x} \\
        0 & f_{y} & c_{y} \\
        0 & 0 & 1
    \end{bmatrix},
\end{equation}
where $(c_{x},c_{y})$ is the principal point and $(f_{x},f_{y})$ is focal length. 
Empirically, we initialize the focal length with a field of view (FoV) of $60^\circ$ as:
\begin{equation}
    f_{x}=f_{y}=\frac{\sqrt{c_{x}^{2}+c_{y}^{2}}}{\tan(\mathrm{FoV}/2)}.
\end{equation}
For each edge $(i,j)$ in the MST, we leverage the off-the-shelf monocular depth maps of images (i.e., DPT~\cite{dpt2021iccv}), and convert correspondences $p_i$ and $p_j$  to the point clouds. Then we define a reprojection loss to optimize the associated transformation as follows:
\begin{equation}\label{eq:loss-reproj}
    L_{reproj}(i,j)=\|K \cdot (T_{ji} \cdot K^{-1} \cdot p_{i}) - p_{j}\|,
\end{equation}
where $T_{ji}$ is the associated transformation from $i$ to $j$. By minimizing the overall reprojection loss for all pairs, we can roughly obtain the initial camera's intrinsics and extrinsics.

\textbf{3D Gaussians.}
We initialize the 3D Gaussians by 3D track points.
We extract a set of tracks $\mathcal{P}$, where each element $(P,\{p_{i}\}_{i=1}^{l})\in\mathcal{P}$ represents a 3D track point $P$ and its corresponding matching points $\{p_{i}\}_{i=1}^{l}$ associated with the training images. The 3D track point $P$ is initialized as the centroid of the transformed projections of $\{p_i\}$:
\begin{equation}
    P=\frac{1}{l}\sum_{i=1}^{l}K^{-1}p_i.
\label{eq:avg_tracking_points}
\end{equation}
Notably, we use track points solely to initialize 3D Gaussians, as their positions will be refined by global optimization and constraints to accurately represent object surfaces.
%Points not constrained have errors from depth estimation, keeping them distant from the actual surface, making them unsuitable for initializing 3D Gaussians.
%\pb{Note that, we use track points solely to initialize 3D Gaussians, because their positions are already refined through above estimation to represent the scene.}

\subsection{Joint Optimization}
% Since the camera's intrinsics and extrinsics need to be optimized simultaneously with the parameters of 3D Gaussians, the difficulty of training is significantly increased. The original loss function based on image alone is insufficient to constrain the camera parameters. To address this, we consider introducing multi-view geometric consistency from matching point tracking trajectories to enhance the joint optimization capability.
% We use the centroids of the initialized 3D Gaussians to maintain the 3D tracking points, and require that these initial 3D Gaussians will not be deleted in the subsequent optimization process. The advantage of this is that we do not affect the original role of the 3D Gaussians, and at the same time take the initialized 3D Gaussians as virtual spatial 3D points which can be applied for evaluating the accuracy of camera parameters by computing the projection errors on each image.
% Since the camera's intrinsics and extrinsics need to be optimized simultaneously with the parameters of 3D Gaussians, the training complexity increases significantly. The original loss function (Eq.~\ref{eq:loss_3dgs}), which relies solely on photometric data, is inadequate for constraining the camera parameters. To address this issue, we propose incorporating multi-view geometric consistency through the tracking of matching point trajectories, thereby enhancing the joint optimization capability. \TODO{The causal relationship here is a bit odd. Our starting point isn't that we introduce global track constraints just because we need to optimize the camera parameters.}

%\pb{Later to make the whole framework differentiable, we provide the formula of gradients for camera extrinsic and intrinsic parameters.}
%Notably, we utilize the centroids of the initialized 3D Gaussians to maintain the 3D tracking points, ensuring that these initial 3D Gaussians are preserved throughout the optimization process. This approach allows us to maintain the original function of the 3D Gaussians while using them as virtual spatial 3D points. These points can be used to evaluate the accuracy of the camera parameters by calculating the projection errors in each image. In order to achieve the goal, we define two additional constraints as follows.

\subsubsection{Global Track Constraints}
%\pb{To our knowledge, existing methods\TODO{Cite} which considered track infomation as geometric constraints in joint optimization, always first evaluated the depth of all 2D track points and then measured the distance of 3D points by projecting the depth point cloud into world space with camera parameters. The cost of evaluation in each training stage is quite expensive, so most works only focus on sparse view cases, where is capable to train on small size of track point cloud. One trivial solution to make these constraints work for normal cases, is setting the 3D track point cloud as new parameters of joint optimization. \TODO{(Why not?)}}
% \lb{
% We leverage the global tracks obtained during initialization to enforce multi-view geometric consistency during joint optimization in both the 2D image domain and 3D space. Our design is intuitive and addresses two key aspects.
% On one hand, the reprojections of the 3D track points on each image should closely align with the original 2D image feature points. This ensures that reprojection relationships are preserved throughout the optimization process.
% On the other hand, the backprojection of the matched 2D feature points from each training image should remain close to their corresponding 3D track points in the scene. This ensures that the 3DGS model maintains spatial consistency across all input images.
% Based on these two considerations, we introduce the \textbf{2D track loss} and \textbf{3D track loss}, respectively.}
% \lb{
% Since the 3D track points in the scene lie on the surface of the object, while 3D Gaussians are not good at modeling specific geometric points. To address this limitation, we utilize the centroids of the initialized 3D Gaussians to represent the 3D track points by introducing the \textbf{scale loss} to constrain sizes of those Gaussians. This approach allows us to maintain the original function of the 3D Gaussians while using them as virtual spatial 3D points. These points can then be used to evaluate the accuracy of the camera parameters by computing projection errors in each image and assessing geometric consistency through reprojection errors in the scene.
% }
We leverage the global tracks obtained during initialization to enforce multi-view geometric consistency in joint optimization, both in 2D and 3D space. Our approach is intuitive and focuses on two key aspects. First, the reprojections of the 3D track points onto each image should closely match the original 2D feature points, ensuring that reprojection relationships are preserved throughout the optimization process. Second, the backprojection of the matched 2D feature points with 3DGS rendering depth from each training image should remain near their corresponding 3D track points in the scene, ensuring spatial consistency across all input images. Based on these two considerations, we introduce the \textbf{2D track loss} and \textbf{3D track loss}, respectively. Since the 3D track points in the scene should lie on the surface of the object, 
% and the general 3D Gaussians are not ideal for modeling specific geometric points, 
we address this by using the centroids of the initialized 3D Gaussians to represent the 3D track points. To achieve this, we introduce a \textbf{scale loss} to constrain the sizes of these Gaussians. This approach allows us to preserve the original function of the 3D Gaussians while treating them as virtual 3D spatial points. These points serve as the key elements for optimizing camera parameters and enhancing the global geometric consistency.

\textbf{2D Track Loss.}
We reproject the 3D track points $P$ (Eq.~\ref{eq:avg_tracking_points}) into the corresponding images using the associated camera parameters and compute the reprojection loss, which will be summed to calculate the total 2D track loss:
%\begin{equation}
%    L_{proj}(P)=\sum_{i=1}^{l}\|p_{i}-K\cdot T_{cw,i} \cdot P\|, L_{t2d}=\sum_{P\in \mathcal{P}}L_{proj}(P).
%\end{equation}
\begin{equation}
    L_{t2d} = \sum_{P\in\mathcal{P}}\frac{1}{l}{\sum_{i=1}^{l}\|p_{i}-K\cdot T_{cw,i} \cdot P\|}.
\end{equation}

\textbf{3D Track Loss.}
We backproject the 2D feature points into 3D scene using the rendered depth and camera parameter associated with each point. The backprojection error is then computed with respect to the 3D tracked point $P$. Then errors are aggregated to calculate the overall 3D track loss:
\begin{equation}
    L_{t3d}=\sum_{P\in\mathcal{P}}\frac{1}{l}{\sum_{i=1}^{l}\|d(p_i) \cdot T_{cw,i}^{-1} \cdot K^{-1} \cdot p_{i} - P\|},
\end{equation}
where $d(p_i)$ denotes the depth perceived from $p_i$ according to Eq.~\ref{eq:3dgs-depth}. 
% Importantly, this loss functions differently with the 2D track loss.  
% at first glance, this loss might seem similar to the 2D track loss mentioned above, but in essence, there is a significant difference. 
% The 2D track loss only involves updating the camera parameters, whereas the 3D track loss requires the rendered depth values during computation. This indirectly ties the optimization of the 3D track loss to the optimization of the 3DGS model. 
Note, the 2D track loss relates to the track Gaussians that are mainly used for the optimization of camera parameters, whereas the 3D track loss requires the 3DGS rendered depth values during computation. This indirectly ties the optimization of the 3D track loss to the optimization of the 3DGS model and enhances the capability of multi-view geometric consistency. They are fundamentally different.

\textbf{Scale Loss.} 
% In fact, 3D tracking points are all located on the surface of scene objects. Since we use the centroids of 3D Gaussians to maintain the 3D tracking points, it is necessary to regularize the scale of these Gaussians and ensure that their centroids are as close to the real surface of the scene as possible. This ensures that the positions of 3D tracking points and projection errors are precise. 
In fact, 3D tracking points reside on object surfaces in the scene. By using 3D Gaussian centroids for tracking, it's crucial to regulate their scale and align centroids closely with the actual object surfaces, ensuring accuracy in tracking point positions and minimizing projection errors.
To achieve this, we incorporate a scale loss function to constrain the scale $S_{track}$ of these Gaussians $G_{track}$:
\begin{equation}
    L_{scale}=\sum_{S\in S_{track}}\|\max(S)\|.
\end{equation}

\textbf{Overall Objectives.} Combined with Eq.~\ref{eq:loss_3dgs}, our joint optimization can be formulated as:
\begin{equation}
\begin{aligned}
    \mathcal{L} =&\lambda_{1} L_{1}+\lambda_{\mathrm{D-SSIM}}L_{\mathrm{D-SSIM}}\\
                 &+\lambda_{t2d}L_{t2d}+\lambda_{t3d}L_{t3d}+\lambda_{scale}L_{scale}.
    \label{eq:overall}
\end{aligned}
\end{equation}

\subsubsection{Optimizing Camera Parameters}
To optimize the camera parameters of 3D Gaussians simultaneously, the gradient of the loss function $\mathcal{L}$ respect to the camera parameters are needed. We derive these gradients accordingly, where the gradient of extrinsic parameters is:
\begin{equation}
    \frac{\partial \mathcal{L}}{\partial T_{cw}}=\frac{\partial \mathcal{L}}{\partial t}q^\top,
\end{equation}
where $q=[\mu,1]^T$ and $t=T_{cw}q=[t_{x},t_{y},t_{z},t_{w}]^T$. 
Further, let $(\mu^{'},\Sigma^{'})$ be the 2D projection of the centroid and covariance $(\mu, \Sigma)$, the gradient of $\mathcal{L}$ respect to focal length $F=(f_{x},f_{y})$ can be computed, where $T=JR_{cw}$, as:
\begin{equation}
    \left\{
    \begin{aligned}
        & \frac{\partial \mathcal{L}}{\partial f_{x}} = \frac{t_x}{t_z}\frac{\partial \mathcal{L}}{\partial \mu_{x}^{'}} + <\frac{\partial \mathcal{L}}{\partial T}{R_{cw}^\top},\frac{\partial J}{\partial f_{x}}>, \\
        & \frac{\partial \mathcal{L}}{\partial f_{y}} = \frac{t_y}{t_z}\frac{\partial \mathcal{L}}{\partial \mu_{y}^{'}} + <\frac{\partial \mathcal{L}}{\partial T}{R_{cw}^\top},\frac{\partial J}{\partial f_{y}}>. \\
    \end{aligned}
    \right.
    \label{eq:grad_intrinsic}
\end{equation}
Please refer to the supplementary materials for more details.


% \subsection{Gradients of Camera Parameters}
% % Given a set of images $\mathcal{I}=\{I_{i}\}_{i=1}^{M}$, each image $I_{i}$ is associated with the camera parameters $T_{cw}$ and $K$. $T_{cw}$ is the camera extrinsic matrix and $K$ is the intrinsic matrix. 
% % $(\mathcal{T},K)$, where $\mathcal{T}=\{T_{i}\}_{i=1}^{M}$ 


% To optimize the camera parameters of 3D Gaussians simultaneously, the gradient of the loss function $\mathcal{L}$ respect to the camera parameters are needed. 
% % Following the notations in~\cite{ye2024gsplatopensourcelibrarygaussian}, the extrinsic matrix $T_{cw}$ is defined as:
% % \begin{equation}
% %     \label{eq:T&P}
% %     T_{cw}=\begin{bmatrix}R_{cw}&t_{cw}\\0&1\end{bmatrix}.
% % \end{equation}
% % The gradient of the loss $\mathcal{L}$ with respect to the extrinsic parameters is formulated as:
% For extrinsic parameters, the gradient is formulated as:
% \begin{equation}
%     \frac{\partial \mathcal{L}}{\partial T_{cw}}=\frac{\partial \mathcal{L}}{\partial t}q^\top,
% \end{equation}
% where $q=[\mu,1]^T$ and $t=T_{cw}q=[t_{x},t_{y},t_{z},t_{w}]^T$. 

% Further, let $(\mu^{'},\Sigma^{'})$ be the 2D projection of the centroid and covariance $(\mu, \Sigma)$, then the gradient of focal length $F$ can be computed by the chain rule as:
% \begin{equation}
%     \frac{\partial \mathcal{L}}{\partial F}=\frac{\partial \mathcal{L}}{\partial \mu^{'}} \frac{\partial \mu^{'}}{\partial F} + \frac{\partial \mathcal{L}}{\partial \Sigma^{'}} \frac{\partial \Sigma^{'}}{\partial F} .
% \end{equation}
% Finally, the loss function $\mathcal{L}$ with respect to the camera intrinsics $(f_{x},f_{y})$, where $T=JR_{cw}$, as:
% \begin{equation}
%     \left\{
%     \begin{aligned}
%         & \frac{\partial \mathcal{L}}{\partial f_{x}} = \frac{t_x}{t_z}\frac{\partial \mathcal{L}}{\partial \mu_{x}^{'}} + <\frac{\partial \mathcal{L}}{\partial T}{R_{cw}^\top},\frac{\partial J}{\partial f_{x}}>, \\
%         & \frac{\partial \mathcal{L}}{\partial f_{y}} = \frac{t_y}{t_z}\frac{\partial \mathcal{L}}{\partial \mu_{y}^{'}} + <\frac{\partial \mathcal{L}}{\partial T}{R_{cw}^\top},\frac{\partial J}{\partial f_{y}}>. \\
%     \end{aligned}
%     \right.
%     \label{eq:grad_intrinsic}
% \end{equation}
% % Where $<\cdot>$ is the Frobenius inner product operator. 
% For more detailed derivation, please check the supplementary materials. 
% % Although we adopt the loss function of Eq.~\ref{eq:loss_3dgs}, 
% % during training the traditional 3DGS,
% We are not limited to the specific form of the loss function when deriving the gradients. It is only necessary to ensure that the loss function is differentiable.
