\section{Introduction}
\label{sec:intro}

% Given a set of images of a 3D scene and the corresponding camera parameters (intrinsic and extrinsic), 3D Gaussian Splatting (3DGS) can explicitly represent the scene with a set of 3D Gaussians and synthesize high-quality images from novel views. Due to its high training efficiency and synthesis quality, 3DGS has been widely used in various tasks, such as reconstruction, editing, AR/VR, and more. However, the training effect of 3DGS depends heavily on the accurate pre-computed camera poses (i.e., camera extrinsics) and camera focal lengths (i.e., camera intrinsics). These parameters are usually estimated by running the Structure-from-Motion (SfM) library COLMAP. This preprocessing step is not only time-consuming but also affects the training effect of 3DGS, especially when the camera motion is complex.
Given a collection of images from a 3D scene along with the corresponding camera intrinsic and extrinsic parameters, 3D Gaussian Splatting (3DGS)~\cite{3DGS2023} can effectively represent the scene with a series of 3D Gaussians, and generate high-quality images from novel viewpoints. Due to its efficiency in training and superior performance in testing, 3DGS has become popular for a variety of applications including reconstruction, editing, and AR/VR etc. However, the effectiveness of 3DGS training relies on accurately pre-determined camera poses (i.e., camera extrinsics) and camera focal lengths (i.e., camera intrinsics). These parameters are typically derived using COLMAP~\cite{colmap2016} in advance. This preprocessing step is not only time-consuming but also impacts the training performance of 3DGS, particularly when dealing with complex camera movements and scenes.

% \begin{figure*}[!t]
%     \centering
%     % \includegraphics[width=1.0\linewidth]{figures/figure_1col.jpg}
%     \includegraphics[width=1.0\linewidth]{figures/scgs-teaser-v3.png}
%     \caption{\textbf{Teaser.}}
%     \label{fig:teaser}
% \end{figure*}

Recent COLMAP-Free approaches~\cite{iNerf2021,SCNeRF2021,bian2022nopenerf,tracknerf2024eccv,fan2024instantsplat,CF-3DGS-2024,ji2024sfmfree3dgaussiansplatting} have tried to address this by adding local constraints. However, it limits these methods to handling only simpler scenes. They typically assume that the input data are sequential and that the focal length is known. When faced with more complex scenarios with very complicated camera movements, these methods generally perform poorly.

% \pb{Recent studies~\cite{iNerf2021,SCNeRF2021,bian2022nopenerf,fan2024instantsplat,CF-3DGS-2024,ji2024sfmfree3dgaussiansplatting} have tried to relax the input requirements. }They typically assume that the input data is sequential and the focal length of the camera is given~\cite{bian2022nopenerf,CF-3DGS-2024}.
% The relative transformations between adjacent images are then estimated using explicit 3D expressions. The camera poses and 3DGS are then optimized in a progressive or alternating iterative manner. 
% The advantage is that the camera poses estimation is integrated with 3DGS, however, the camera intrinsics still need to be provided. Additionally, these methods do not take into account the inherent connection between camera poses optimization and 3DGS training, which are decoupled during the training process. 
% Differently, we aim to optimize both the camera intrinsic and extrinsic parameters, with unordered images, by exploring the relationship between camera parameters and 3DGS training. \TODO{Replaced with the motivation of global track.}
% \TODO{The advantage of these methods is that the camera pose estimation is integrated with 3DGS, but the camera intrinsics still need to be provided. Moreover, this approach does not unify the learning of camera parameters and 3D Gaussian parameters, and the training results of 3DGS are significantly affected by the accuracy of the relative pose estimation.}

% To address this problem, we propose, for the first time, a joint optimization method to train 3DGS from a set of images \textbf{without both camera intrinsics and extrinsics}. To be specific, unlike previous works that only update camera poses during training, we also theoretically derive the gradients of focal length in order to simultaneously optimize the camera intrinsics by back-propagation, thereby unifying the learning process for both camera parameters and 3D Gaussians. Additionally, in order to enhance the stability of training, we further incorporate the global track information and select the 3D Gaussians that correspond to each track. Note that the tracked 3D Gaussians are dynamically adjusted in size during the training process, degrading to a very small size and infinitely approaching spatial points, which will be distributed near the actual surface. By leveraging the reprojection loss, the positions of which and camera parameters will be updated to satisfy the requirements. Moreover, the remaining 3D Gaussians still function as before, and all 3D Gaussians will be aggregated and constrained by the loss function. With this hybrid representation, we seamlessly integrate the optimization of camera parameters with 3DGS training.
To address this problem, we introduce global track information to globally constrain geometric consistency and innovatively integrate it into 3DGS. Specifically, we select the 3D Gaussians that correspond to each track, where the track Gaussians are dynamically adjusted in size during the training process, shrinking to an infinitesimally small size, and approaching spatial points that are distributed near the surface. Using our novel designed 2D and 3D track losses, the reprojection and backprojection errors are explicitly minimized. Moreover, the remaining 3D Gaussians continue to function as before, and all 3D Gaussians are aggregated and constrained by the loss function. Additionally, for the first time, we theoretically derive the focal length gradients to achieve full differentiability of the pipeline, eliminating the pre-computation of all the necessary camera parameters, including both the intrinsics and extrinsics. It enables to unify the learning process for both camera parameters and 3DGS.

% We first theoretically derive the gradients of the camera parameters to be back-propagated during 3DGS training, thereby unifying the learning process for both camera parameters and 3D Gaussian parameters. To achieve joint optimization, in the initialization phase, we use explicit feature matching between images to initialize the camera parameters and a set of 3D Gaussians. During the joint training phase, we impose trajectory and scale constraints on these Gaussians, driving them to degenerate into feature points of the 3D scene, while other 3D Gaussians focus on representing the scene accurately from input images. This maintains multi-view geometric consistency and ensures precise camera parameter estimation. The benefit of this method is that the camera parameter estimation is fully integrated with the 3DGS training process, resulting in a more robust 3DGS.

% We have conducted extensive evaluations on both public benchmark datasets and our synthesized virtual datasets. Compared with the previous methods, our method only takes a set of images as inputs and achieves the state-of-the-art performance in both camera parameters estimation and novel view synthesis.

% \bj{We have conducted extensive evaluations on both public benchmark datasets and our synthetic datasets with significant camera movements. In comparison, our method achieves SOTA performance on both camera parameters estimation and novel view synthesis.}

% The code and datasets will be available upon acceptance. 
In summary, our contributions are as follows:
\begin{itemize}
    % \item We theoretically derive the gradients of the camera parameters for back-propagation during 3DGS training, including both camera intrinsics and extrinsics, thereby unifying the learning process of camera parameters with 3D Gaussian training.
    % \item For the first time, we propose a joint optimization method for 3D Gaussians and camera parameters. By initializing a set of 3D Gaussians and applying trajectory and scale constraints, multi-view geometric consistency is applied for camera parameter estimation and results in a robust 3DGS.
    % \item On both public and synthetic datasets, our method surpasses previous methods requiring camera insintrics. It achieves state-of-the-art (SOTA) performance on novel view synthesis.
    
    % \item We theoretically derive the gradients of the focal length in relation to 3DGS training to update the camera intrinsics, thereby eliminating any prior information about the camera parameters for 3DGS training.
    % \item To our knowledge, for the first time, we introduce a joint optimization approach for camera parameters and 3DGS. We achieve this by initializing a set of 3D Gaussians and enforce trajectory and scale constraints, which allows us to apply multi-view consistency and reprojection loss to estimate camera parameters, resulting in a robust 3DGS.
    % \item On both public and synthetic datasets, our approach outperforms previous methods that require camera intrinsics, and achieves SOTA performance on novel view synthesis.

    \item We are the first to propose integrating track information with 3DGS, using global geometric constraints to simultaneously optimize camera parameters and 3DGS. To achieve this, we introduce 2D and 3D track losses to constrain reprojection and backprojection errors.
    \item We propose a joint optimization framework, where for the first time, we derive the gradient of the camera intrinsics. Without relying on any known camera parameters, we achieve full differentiability for the entire pipeline, seamlessly integrating camera parameters estimation, including both intrinsics and extrinsics, with 3DGS training.
    \item On both \textit{challenging} public and synthetic datasets, our approach outperforms previous methods on both camera parameters estimation and novel view synthesis.
\end{itemize}

% \TODO{double-check size of tracked 3D Gaussians}
