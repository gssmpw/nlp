% \begin{abstract}
% % The ABSTRACT is to be in fully justified italicized text, at the top of the left-hand column, below the author and affiliation information.
% % Use the word ``Abstract'' as the title, in 12-point Times, boldface type, centered relative to the column, initially capitalized.
% % The abstract is to be in 10-point, single-spaced type.
% % Leave two blank lines after the Abstract, then begin the main text.
% % Look at previous \confName abstracts to get a feel for style and length.
% % Although 3D Gaussian Splatting (3DGS) has achieved remarkable progress in scene reconstruction and novel view synthesis, it heavily relies on accurate pre-computed camera poses (i.e., camera extrinsics) and camera focal lengths (i.e., camera intrinsics). To address this limitation, many works have attempted to obtain 3D Gaussians without camera poses, but camera intrinsics are still required. 
% While 3D Gaussian Splatting (3DGS) has made significant progress in scene reconstruction and novel view synthesis, it still heavily relies on accurately pre-computed camera intrinsics and extrinsics, such as focal length and camera poses. In order to mitigate this dependency, the previous efforts have focused on optimizing 3DGS without the need for camera poses, yet camera intrinsics remain necessary. To further loose the requirement, we propose a joint optimization method to train 3DGS from an image collection without requiring either camera intrinsics or extrinsics. 
% % To achieve the goal, we introduce several key design choices during the training of 3DGS. Theoretically, we additionally derive the gradients of the camera intrinsics, allowing the values of the intrinsic parameters to be also updated during the back-propagation. 
% To achieve this goal, we introduce several key improvements during the joint training of 3DGS. We theoretically derive the gradient of the camera intrinsics, allowing the camera intrinsics to be optimized simultaneously during training.
% Moreover, we integrate global track information and select the Gaussian kernels associated with each track, which will be trained and automatically rescaled to an infinitesimally small size, closely approximating surface points, and focusing on enforcing multi-view consistency and minimizing reprojection errors, while the remaining kernels continue to serve their original roles. This hybrid training strategy nicely unifies the camera parameters estimation and 3DGS training.
% % Moreover, we incorporate the global track information and apply extra training strategies on the filtered Gaussian kernels that correspond to each track, which coherently unifies the camera parameters estimation with the 3DGS training and leads to enhanced multi-view consistency and stability.
% % in this paper, we propose a joint optimization method to train 3DGS from an image collection without requiring either camera intrinsics or extrinsics. To overcome these challenges, our approach initializes a set of 3D Gaussians using relative pose initialization with explicit feature matching among input images and applies trajectory and scale constraints on these Gaussians for further training. This enhances the camera parameter estimation, leading to more robust 3DGS by maintaining multi-view geometric consistency. Moreover, we theoretically derive the gradients of the camera parameters to be back-propagated during 3DGS training, thereby unifying the camera parameter estimation with the 3DGS training process.
% Extensive evaluations demonstrate that the proposed method achieves state-of-the-art (SOTA) performance on both public and synthetic datasets. Codes and datasets will be released upon acceptance.
% % Our estimated camera parameters also outperform those of other methods on the synthetic dataset.
% %Although 3DGS has made significant breakthroughs in both speed and quality for novel view synthesis tasks, training a 3DGS task still requires high-precision camera parameters provided by offline COLMAP. To address this issue, recent researchers have proposed methods for jointly optimizing 3DGS and camera parameters. However, current methods primarily rely on monocular depth estimation or neural networks to recover camera parameters. These methods solely explores the local relationships of images, fails to recover the accurate camera parameters in the complex scenarios.In this work, we propose a global bundle adjustment (BA) optimization method based on image correspondence relationships. During the initialization phase, we construct a maximum spanning tree (MST) from matching points to obtain spatial adjacency relationships between cameras. And we use monocular depth(DPT) and the matching points to recover the initial camera parameters. Then, within the joint optimization process with 3DGS, we select some 3DGSs as the anchors to track,and establish correspondence relationships with image to recover high-precision camera parameters.We are the first to propose a method for jointly estimating camera parameters and novel view synthesis by tracking 3DGS. Our method achieves state-of-the-art results in both parameter estimation accuracy and novel view synthesis tasks. To validate the effectiveness of our approach, we not only tested on public benchmark datasets but also on a synthetically constructed dataset. And our method achieved excellent results in all tests. 
% \end{abstract}

\begin{abstract}
% \bj{While 3D Gaussian Splatting (3DGS) has made significant strides in scene reconstruction and novel view synthesis, it still heavily depends on accurately pre-computed camera intrinsics and extrinsics, such as focal length and camera poses. However, in practice, obtaining precise camera parameters is both time-consuming and prone to noise. To reduce this reliance, previous COLMAP-Free methods mainly depend on local constraints to optimize the camera pose, which often perform poorly in complex scenarios. In contrast, inspired by the bundle adjustment approach, we propose TrackGS to globally constrain multi-view geometry by incorporating track information. We introduce feature tracks across all views and select the Gaussians associated with each track. These kernels are trained and automatically rescaled to an infinitesimally small size, closely approximating surface points, which helps enforce geometry consistency while minimizing both reprojection and backprojection errors, with the remaining Gaussians maintaining their original roles. Additionally, we derive the gradient of the intrinsic matrix, for the first time, and seamlessly unify camera parameters estimation with 3DGS training. Thanks to our joint optimization strategy, extensive evaluations show that our method achieves SOTA performance on both challenging public and synthetic datasets with severe camera movement. Code and datasets will be released.}
While 3D Gaussian Splatting (3DGS) has advanced ability on novel view synthesis, it still depends on accurate pre-computaed camera parameters, which are hard to obtain and prone to noise. Previous COLMAP-Free methods optimize camera poses using local constraints, but they often struggle in complex scenarios. To address this, we introduce TrackGS, which incorporates feature tracks to globally constrain multi-view geometry. We select the Gaussians associated with each track, which will be trained and rescaled to an infinitesimally small size to guarantee the spatial accuracy. We also propose minimizing both reprojection and backprojection errors for better geometric consistency. Moreover, by deriving the gradient of intrinsics, we unify camera parameter estimation with 3DGS training into a joint optimization framework, achieving SOTA performance on challenging datasets with severe camera movements. Code and datasets will be released.
\end{abstract}