\clearpage
\setcounter{page}{1}
\maketitlesupplementary

\section{Derivation of Camera Intrinsic Parameters}
\label{sec:focal-div}
With the output image width and height $(w,h)$, as well as the near and far clipping planes $(n,f)$, the extrinsic matrix $T_{cw}$ and the projection matrix $P$, representing the transformation from camera space to normalized clip space, are denoted as follows:
\begin{equation}\label{eq:supp-T&P}
    T_{cw}=\begin{bmatrix}R_{cw}&t_{cw}\\0&1\end{bmatrix}, \\
    P=\begin{bmatrix}
        \frac{2f_{x}}{w} & 0 & 0 & 0 \\
        0 & \frac{2f_{y}}{h} & 0 & 0 \\
        0 & 0 & \frac{f+n}{f-n} & \frac{-2fn}{f-n} \\
        0 & 0 & 1 & 0 \\
    \end{bmatrix}.
\end{equation}
%
Fig.~\ref{fig:focal-div} demonstrates the computational graph of parameters, which are related to camera intrinsics. In our discussion, the focal length is $F=(f_{x},f_{y})$ and the principal point is $(c_{x},c_{y})$.
For a 3D Gaussian parameterized by its mean $\mu\in\mathbb{R}^3$ and covariance $\Sigma\in\mathbb{R}^{3\times 3}$, the loss $\mathcal{L}$ is formulated by its 2D projected mean $\mu^{'}$ and covariance $\sigma^{'}$.
We convert the mean $\mu$ into $t=(t_{x},t_{y},t_{z},t_{w})\in \mathbb{R}^4$ in camera coordinates, $t^{'}=(t_{x}^{'},t_{y}^{'},t_{z}^{'},t_{w}^{'})\in\mathbb{R}^4$ in normalized coordinates (NDC),
% ND coordinates\TODO{ND?refer Mathematical Supplement for the gsplat Library, I think is NDC}, 
and finally $\mu^{'}\in\mathbb{R}^{2}$ in pixel coordinates as follows:
\begin{equation}\label{eq:supp-t&mu}
    t=T_{cw}\begin{bmatrix}\mu & 1\end{bmatrix}^\top,
    t^{'}=Pt,
    \mu^{'}=\begin{bmatrix}
        \frac{1}{2}(\frac{w\cdot t_{x}^{'}}{t_{w}^{'}}+1)+c_{x} \\
        \frac{1}{2}(\frac{h\cdot t_{y}^{'}}{t_{w}^{'}}+1)+c_{y}
    \end{bmatrix}.
\end{equation}
%
Notice that the projection of a 3D Gaussian does not result in a 2D Gaussian, the projection of $\Sigma$ to pixel coordinates is approximated with a first-order Taylor expansion at $t$ in camera space, then the affine transform $J\in\mathbb{R}^{2\times 3}$ and the 2D covariance $\Sigma^{'}\in\mathbb{R}^{2\times 2}$~\cite{2002EWA} are:
\begin{equation}\label{eq:supp-J&sigma}
    J=\begin{bmatrix}
        \frac{f_{x}}{t_{z}} & 0 & -\frac{f_{x}\cdot t_{x}}{t_{z}^{2}}\\
        0 & \frac{f_{y}}{t_{z}} & -\frac{f_{y}\cdot t_{y}}{t_{z}^{2}}
    \end{bmatrix},
    \Sigma^{'}=J R_{cw} \Sigma R_{cw}^\top J^\top.
\end{equation}
%
Given the gradients of $\mathcal{L}$ with respect to 2D mean $\mu^{'}$ and covariance $\Sigma^{'}$, we can back-propagate the gradient of focal length $F$ as:
\begin{equation}\label{eq:supp-gradF}
    \frac{\partial \mathcal{L}}{\partial F}=\frac{\partial \mathcal{L}}{\partial \mu^{'}} \frac{\partial \mu^{'}}{\partial F} + \frac{\partial \mathcal{L}}{\partial \Sigma^{'}}\frac{\partial \Sigma^{'}}{\partial F}.
\end{equation}

First, we compute the gradient contribution of 2D mean $\mu^{'}$ to focal length $F$, $\frac{\partial \mu^{'}}{\partial F}$ can be obtained by the chain rule:
\begin{equation}\label{eq:supp-gradF-mu}
\begin{aligned}
    \frac{\partial \mathcal{\mu^{'}}}{\partial F} & =  \frac{\partial \mu^{'}}{\partial t^{'}} \frac{\partial t^{'}}{\partial F}\\
     &=\begin{bmatrix}
         \frac{w}{2 t_{w}^{'}} & 0 & 0 & -\frac{w t_{x}^{'}}{(t_{w}^{'})^{2}} \\
        0 & \frac{h}{2 t_{w}^{'}} & 0 & -\frac{h t_{y}^{'}}{(t_{w}^{'})^{2}}
     \end{bmatrix}
     \begin{bmatrix}
         \frac{2 t_{x}}{w} & 0 & 0 & 0 \\ 0 & \frac{2 t_{y}}{h} & 0 & 0
     \end{bmatrix}^\top\\
     &=\begin{bmatrix}
         \frac{t_{x}}{t_{z}} & 0 \\
         0 & \frac{t_{y}}{t_{z}}
     \end{bmatrix},
\end{aligned}
\end{equation}
where $t_{w}^{'}=t_{z}$ from Eq.~\ref{eq:supp-t&mu}.

\begin{figure}[t]
  \centering
   \includegraphics[width=0.8\linewidth]{figures/focal-div-v2.png}
   % \caption{focal-div notations.}
   \caption{\textbf{Computational graph of parameters}.}
   \label{fig:focal-div}
\end{figure}

Then, for the second part of Eq.~\ref{eq:supp-gradF}, we use another parameter $J$ to compute this component, which means 
$\frac{\partial \mathcal{L}}{\partial \Sigma^{'}}\frac{\partial \Sigma^{'}}{\partial F}=\frac{\partial \mathcal{L}}{\partial J}\frac{\partial J}{\partial F}$.
\textit{gsplat}~\cite{ye2024gsplatopensourcelibrarygaussian} obtained the gradient of $\mathcal{L}$ to the affine transform $J$ through $T=J R_{cw}\in \mathbb{R}^{2\times 3}$ as:
\begin{equation}\label{eq:supp-gradJ-L}
    \partial \mathcal{L}=<\frac{\partial \mathcal{L}}{\partial T}{R_{cw}^\top},\partial J>,\mathrm{where}\frac{\partial \mathcal{L}}{\partial T}=\frac{\partial \mathcal{L}}{\partial \Sigma^{'}}T\Sigma^\top+{\frac{\partial \mathcal{L}}{\partial \Sigma^{'}}}^\top T\Sigma,
\end{equation}
with the gradient of $J$ to the focal length $F$ as:
\begin{equation}\label{eq:supp-gradF-J}
    \frac{\partial J}{\partial f_{x}}=\begin{bmatrix}
        \frac{1}{t_{z}} & 0 & -\frac{t_{x}}{t_{z}^{2}} \\
        0 & 0 & 0
    \end{bmatrix},
    \frac{\partial J}{\partial f_{y}}=\begin{bmatrix}
        0 & 0 & 0 \\
        0 & \frac{1}{t_{z}} & -\frac{t_{y}}{t_{z}^{2}}
    \end{bmatrix}.
\end{equation}

Finally, the gradient of loss $\mathcal{L}$ with respect to focal length $F$ in Eq.~\ref{eq:supp-gradF} is formulated as:
\begin{equation}
    \left\{
    \begin{aligned}
        & \frac{\partial \mathcal{L}}{\partial f_{x}} = \frac{t_x}{t_z}\frac{\partial \mathcal{L}}{\partial \mu_{x}^{'}} + <\frac{\partial \mathcal{L}}{\partial T}{R_{cw}^\top},\frac{\partial J}{\partial f_{x}}>, \\
        & \frac{\partial \mathcal{L}}{\partial f_{y}} = \frac{t_y}{t_z}\frac{\partial \mathcal{L}}{\partial \mu_{y}^{'}} + <\frac{\partial \mathcal{L}}{\partial T}{R_{cw}^\top},\frac{\partial J}{\partial f_{y}}>. \\
    \end{aligned}
    \right.
\end{equation}


% \bj{Initializaiton of MST: In order to join the camera parameter estimation with the 3D Gaussian Splatting (3DGS) training and obtain robust results in complex scenes, such as those with large view changes, we delicately initialize both the camera intrinsic and extrinsic parameters, as well as the 3D Gaussians. First, we apply off-the-shelf explicit feature extraction (SuperPoint~\cite{detone2018superpoint}) and matching (SuperGlue~\cite{sarlin20superglue}) to establish the association relationships between images. Then, we construct a Maximum Spanning Tree (MST) using Kruskal's method, from a weighted graph where the weight of each edge is the number of matching pairs. Subsequently, we initialize the camera intrinsics and extrinsics based on the relative reprojection error of adjacent images in the MST. The initial 3D Gaussians are placed at the spatial positions of the feature matching points.

% As the mono-depth maps are inherently distorted and lack global consistency, we introduce scale and shift variables $(\gamma_i, \eta_{i})$ for each map $d_{i}$ to approximate the global \textit{absolute} depth $d^{*}_{i}=\gamma_{i} d_{i} + \eta_{i}$. 
% \TODO{More details about the undistorted depth map.}
% Moreover, we define a regularization term to prevent negative depth values:
% \begin{equation}
%     L_{depth}(d_i)=\max(-d_i+\epsilon_{depth}, 0),
% \end{equation}
% where $\epsilon_{depth}=1e^{-3}$.
% Finally, we initial camera intrinsic and extrinsic parameters by optimizing the following problem:
% \begin{equation}
%     (T,K)=\argmin_{T,K,(\gamma, \eta)}\sum_{(i,j)\in \mathrm{MST}}L_{reproj}(i,j) + \sum_{i}L_{depth}(d_i).
% \end{equation}
% }

\begin{figure*}[ht]
  \centering
   % \includegraphics[width=\linewidth]{figures/scgs-suppl-all-gt-poses.png}
   \includegraphics[width=\linewidth]{figures/scgs-suppl-all-gt-poses-lowRes.png}
   \caption{\textbf{Ground-truth camera trajectory for Tanks and Temples and CO3D-V2}.}
   \label{fig:suppl-gt-poses}
\end{figure*}
\newpage
\section{Implementation Details}
We provide more details about the datasets and training procedure in following sections.
\subsection{Dataset}
We select three datasets for training and evaluation including existing datasets Tanks and Temples, CO3D-V2, and a virtual Synthetic dataset created by ourself.
Tab.~\ref{table:suppl-dataset-details} shows the details of the scenes in all three datasets, where $Max.\ rot$ is the maximum relative rotation angle between any two frames and the $Avg.\ adj.\ rot$ donates the average relative rotation angle between two adjacent frames. The later represents the magnitude of the relative angle change in a sequence. In comparison, notice that the frame changes in Tanks and Temples are quite small, and our Synthetic datasets are more complex than CO3D-V2, although the $Max.\ rot$ of both datasets is 180 degrees. The visualization of these camera trajectories is shown in Fig.~\ref{fig:suppl-gt-poses}.

\textbf{Synthetic Dataset.}
% Obtaining the ground truth camera parameters from image sequences is quite challenging. Although existing dataset use COLMAP to obtain the camera parameters, there still exists inaccuracies. For more accurate comparison, we create the Synthetic Dataset using Blender~\cite{blender2018}. The Synthetic Dataset is comprised of four indoor scenes. The camera motion of \textit{classroom} (180 frames) and \textit{lego\_c2} (75 frames) is object-centric, the camera motion of \textit{livingroom} (150 frames) and \textit{bedroom} (180 frame) is roaming. In both type of scenes, the camera movement spans greater than 360 degrees and is rather complex. The visualization of the camera trajectory is shown in Fig.\ref{fig:suppl-pose-synthetic}. 
Extracting accurate camera parameters from image sequences is challenging. While existing datasets employ COLMAP to derive these parameters, inaccuracies remain. To facilitate more precise comparisons, we have created a Synthetic Dataset using Blender~\cite{blender2018}. This dataset includes four indoor scenes. The camera movements in the \textit{classroom} (180 frames) and \textit{lego\_c2} (75 frames) scenes are object-centric, whereas those in the \textit{livingroom} (150 frames) and \textit{bedroom} (180 frames) scenes involve roaming. In both types of scenes, the camera navigates complex paths extending over 360 degrees. The visualization of the camera trajectories is illustrated in Fig. \ref{fig:suppl-pose-synthetic}.

\begin{table}[!ht]
    \centering
    \resizebox{\linewidth}{!}{
    \begin{tabular}{c c ccccc} \hline
        & {Scenes}                  & {Type}        & \makecell{Seq.\\length}      & \makecell{Frame\\ rate}      & \makecell{Max.\\ rot (deg)}      & \makecell{Avg. adj. \\rot (deg)}\\ \hline
    \multirow{8}{*}{\rotatebox{90}{Tanks and Temples}}    
        & Church                    &indoor         &400                &30                 &37.3       &0.09   \\
        & Barn                      &outdoor        &150                &10                 &47.5       &0.32   \\
        & Museum                    &indoor         &100                &10                 &76.2       &0.76   \\
        & Family                    &outdoor        &200                &30                 &35.4       &0.13   \\
        & Horse                     &outdoor        &120                &20                 &39.0       &0.32   \\
        & Ballroom                  &indoor         &150                &20                 &30.3       &0.20   \\
        & Francis                   &outdoor        &150                &10                 &47.5       &0.32   \\
        & Ignatius                  &outdoor        &120                &20                 &26.0       &0.22   \\ \hline
    \multirow{8}{*}{\rotatebox{90}{CO3D-V2}}  
        & {34\_1403\_4393}          &indoor         &202                &30                 &180.0      &1.53   \\
        & {46\_2587\_7531}          &indoor         &202                &30                 &180.0      &1.60   \\
        & {106\_12648\_23157}       &outdoor        &202                &30                 &180.0      &1.33   \\
        & {110\_13051\_23361}       &indoor         &202                &30                 &71.6       &0.99   \\
        & {245\_26182\_52130}       &indoor         &202                &30                 &180.0      &1.40   \\
        & {407\_54965\_106262}      &indoor         &202                &30                 &180.0      &1.46   \\
        & {415\_57112\_110099}      &outdoor        &202                &30                 &180.0      &1.60   \\
        & {429\_60388\_117059}      &outdoor        &202                &30                 &180.0      &3.11   \\ \hline
    \multirow{4}{*}{\rotatebox{90}{Synthetic}}
        & classroom                 &indoor         &180                &0                  &180.0      &3.84   \\
        & {lego\_c2}                &indoor         &75                 &0                  &180.0      &3.77   \\
        & livingroom                &indoor         &150                &0                  &180.0      &2.63   \\
        & bedroom                   &indoor         &180                &0                  &180.0      &2.33   \\ \hline
    \end{tabular}
    }
    \caption{\textbf{Details of the selected sequences.} }
    \label{table:suppl-dataset-details}
\end{table}


\begin{algorithm}
\caption{Initialization}\label{alg:suppl-init}
\begin{algorithmic}
\State $\mathcal{I}=\{I_{i}\}_{i=1}^{M} \gets$ {Input images}
\State DPT $\gets$ {Monocular Depth Estimation Model}
\State $\{p_{i}\} \gets \mathrm{SuperPoint}(\mathcal{I})$ \Comment{extract feature}
\State $(p_{i},p_{j})\gets \mathrm{SuperGlue}(\{p_{i}\})$ \Comment{image matching}
\State MST $\gets$ Kruskal Algorithm    \Comment{construct MST}
\State $\mathcal{P}=(P,\{p_{i}\}_{i=1}^{l})\gets \mathrm{Track(MST)}$ 
\State \Comment{extract track points}
\Statex\hrule
\Loop   \Comment{Loop 100 iterations}
    \For{edge $(i,j)$ in MST}
        \State $L_{reproj}(i,j)=\|K \cdot (T_{ji} \cdot K^{-1} \cdot p_{i}) - p_{j}\|$ 
        \State $L_{reproj}+=L_{reproj}(i,j)$
    \EndFor
    \State $K,\{T_{ji}\}\gets \min L_{reproj}$ 
\EndLoop\Comment{Optimize $T_{ji}, K$}
% \Statex
% \hrule
\For {$(P,\{p_{i}\})$ in $\mathcal{P}$}
    \State $d_{i} \gets \mathrm{DPT}(p_{i})$
    \State $P \gets \mathrm{InitTrackPoint}(K,\{d_i\},\{p_{i}\})$ 

\EndFor \Comment{Init Track Points}
\State $T_{cw}=\prod_{(i,j)}T_{ji}$ \Comment{Init camera extrinsic}
\Statex
\hrule
\State \textbf{return} $K,T_{cw},P$
\end{algorithmic}
\end{algorithm}

\begin{algorithm}
\caption{Joint optimization}\label{alg:suppl-joint}
\begin{algorithmic}
\State $lr\gets lr_{\mu},lr_q,lr_s,lr_{\alpha}$ \Comment{Initial lr of 3D Gaussians}
\State $lr_{pos}\gets lr_{R_{cw}},lr_{T_{cw}}$ \Comment{Initial lr of camera pose}
\State $G_{track}\gets \mathrm{Gaussian}(P)$ \Comment{Initial track 3D Gaussians}
\Statex
\hrule
\Procedure{WarmUp:}{}
    \State $\mathrm{step}=0$
    \While{$\mathrm{step < 500}$}
        \State $\hat{I}_i,\hat{p}_i \gets \mathrm{Rasterize}(G_{track},T_{cw,i},K)$
        \State $L_i \gets \mathcal{L}(I_i,p_i,\hat{I}_i,\hat{p}_i)$
        \State $L \gets \sum_i^M{L_i}$ 
        \State $G_{track},T_{cw},K \gets \mathrm{Adam}(\nabla{L})$ 
        \State \Comment{Update track Gaussians and camera param}
        \State $lr \gets \mathrm{schedule}(lr)$ \Comment{Update Gaussian lr}
        \State $lr_{pos} \gets \mathrm{schedule}(lr_{pos})$ \Comment{Update pose lr}
        \If{$\mathrm{step}<100$}
            \State $lr_{focal}=0.0$
        \Else
            \State $lr_{focal}=\max(1e^{-4},5e^{-3}*(1.0-\frac{\mathrm{step}}{500}))$
        \EndIf \Comment{Update focal lr}
        \State $\mathrm{step} \gets \mathrm{step} + 1$
    \EndWhile
\EndProcedure
\Statex
\hrule
\Procedure{Joint 3DGS:}{}
\State $\mathrm{step}=0$
\State $\tilde{\mathcal{I}} \gets \mathrm{Queue(shuffle}(\mathcal{I}))$ \Comment{Shuffle images}
\While{$\mathrm{step}<30000$}
    \State $I_i \gets \mathrm{QuePopLeft}(\tilde{\mathcal{I}})$ \Comment{Pop first elem in Que}
    \State $G \gets G_{track}+G_{normal}$
    \State $\hat{I}_i,\hat{p}_i \gets \mathrm{Rasterize}(G,T_{cw,i},K)$
    \State $L_i\gets \mathcal{L}(I_i,p_i,\hat{I}_i,\hat{p}_i)$
    \State $G\gets \mathrm{Adam}(\nabla{L_i})$ \Comment{Update Gaussians}
    \State $lr$ $\leftarrow$ $schedule(lr)$ \Comment{Update Gaussians lr}
    \If{$\tilde{\mathcal{I}}==\emptyset$}
        \State $L\gets \sum_i^M{L_i}$ 
        \State $T_{cw},K \gets \mathrm{Adam}(\nabla{L})$ 
        \State \Comment{Update all cameras param}
        \State $lr_{pos},lr_{focal}\gets \mathrm{schedule}(lr_{pos},lr_{focal})$ 
        \State \Comment{Update all cameras lr}
        \State $\tilde{\mathcal{I}} \gets \mathrm{Queue(shuffle}(\mathcal{I}))$ \Comment{Shuffle images}
    \EndIf
    \For{all $(\mu,\sum,c,\alpha)$ in G}
        \If{$\nabla_{p}L<\tau_p$} \Comment{Densification}
            \State {$\mathrm{SplitGaussians}(\mu,\sum,c,\alpha)$}
            \State {$\mathrm{CloneGaussians}(\mu,\sum,c,\alpha)$}
        \EndIf
        \If{$\alpha < \epsilon$ or IsTooLarge$(\mu,\sum)$}
            \State $\mathrm{RemoveGaussian}(G_{normal})$ \Comment{Pruning}
        \EndIf
    \EndFor \Comment{Adaptive control of Gaussians}
    \State $\mathrm{step} \gets \mathrm{step} + 1$
\EndWhile
\EndProcedure
\end{algorithmic}
\end{algorithm}

% \TODO{check if the parameters in Alg.2 are consistent with the paper.}

\subsection{Training Details}

\textbf{Initialization.}
% \sout{Providing better initial values for the camera parameters and Gaussians helps to accelerate convergence and avoid falling into local suboptimal solutions. For generality, we construct a Maximum Spanning Tree (MST) whose edge weights represent the number of corresponding points of two images, where we can acquire image-level matching pairs and point-level correspondences to extract global track. Then the camera parameters and 3D Gaussians are initialized as follows.}
To provide the initial values of camera parameters and 3D Gaussians, we first obtain mono-depth maps of images $\mathcal{I}$ by DPT~\cite{dpt2021iccv}. We extract the feature points $\{p_{i}\}$ of each image $I$ with SuperPoint~\cite{detone2018superpoint} and compute the feature matches among all images with SuperGlue~\cite{sarlin20superglue}. Then we construct the Maximum Spanning Tree (MST) by Kruskalâ€™s algorithm, where the node represents each image and the weight of each edge is determined by the number of feature matching pairs between two images. We extract the set of tracks $\mathcal{P}$ from the MST, where each element $(P,\{p_{i}\})\in \mathcal{P}$ is a 3D track point $P$ and its corresponding matching points $\{p_{i}\}$ associated with the training images. Later on, we define a reprojection loss for the edges of MST, and minimize this objective to optimize the camera intrinsic $K$ and the relative camera extrinsic matrix $T_{ji}$. This optimization procedure is set as 100 steps in our experiment. Finally, we obtain the initialization of the location of 3D track points $P$, the camera intrinsic $K$ and extrinsic matrix $T_{cw}$. The whole algorithm of initialization is summarized in Alg. \ref{alg:suppl-init}.

\begin{table}[h]
\label{tab:lr}
\centering
% \vspace{-1.6em}
\resizebox{\columnwidth}{!}{
\begin{tabular}{c|c|c|c|c|c}
\hline
$lr_{\mu}$ & $lr_q$ & $lr_s$ & $lr_{\alpha}$ & $lr_{R_{cw}}$ & $lr_{T_{cw}}$ \\ \hline
$1.6 * xyz\_scale * 1e^{-2}$ & $1e^{-3}$ & $5e^{-3}$ & $5e^{-2}$ & $5e^{-3}$ & $1e^{-2}$ \\ \hline
\end{tabular}
}
% \vspace{-1.6em}
\end{table}
\textbf{Joint optimization.}
% The joint optimization process of the 3DGS and camera is comprehensively detailed in Algorithm \ref{alg:suppl-joint}. Because 
As detailed in Alg.~\ref{alg:suppl-joint}, since the initial camera poses and tracking points are very noisy, before training of novel view synthesis, we take a warmup to get more precise parameters. 
Here, we draw on the concept of global bundle adjustment, first optimizing these parameters through RGB loss and track loss over 500 epochs. The initial learning rate for each variable is set as above.

For the learning rate of $\mu$, considering the global scale of scene, we introduce the bounding sphere radius of the initial point clouds $xyz\_scale$ as a parameter. Additionally, the learning rates for both $\mu$ and the camera parameters are decayed using the \textit{ExponentialLR} mechanism. The remaining learning rates are kept constant.
Moreover, the leanring rate for the focal length is set differentially: it is set to 0 during the initial 100 epochs, meaning that only the camera's pose and the initial 3DGS will be optimized. After the first 100 epochs, it decreases according to the following formulation:
\begin{equation}
\resizebox{\columnwidth}{!}{%
    $lr_{focal} = \left\{
\begin{array}{rl}
0.0 &, step \leq 100 \\
\max(1e^{-4},5e^{-3}*(1.0-step/500)) &, step > 100
\end{array} \right.$%
}
\end{equation}
During warmup, our goal is to achieve a better geometric initialization, at which point the weights of the RGB loss and track loss are all set to $1.0$. 
% Moreover, the learning rate for the focal length is set differently: it is 0 during the initial 100 epochs and then linearly decreases from $5e^{-3}$ to $1e^{-4}$ according to the training step.

\begin{table*}[!ht]
    \centering
    \resizebox{\linewidth}{!}{
    \begin{tabular}{c|ccccc|ccccc|ccccc|ccccc} \hline
    \multirow{2}{*}{Scene}  & \multicolumn{5}{c}{classroom}         & \multicolumn{5}{c}{{lego\_c2}}        & \multicolumn{5}{c}{livingroom}        & \multicolumn{5}{c}{bedroom} \\ \cline{2-21}
                            & PSNR  & SSIM  & LPIPS & ATE   & FoV           & PSNR  & SSIM  & LPIPS & ATE   & FoV           & PSNR  & SSIM  & LPIPS & ATE   & FoV               & PSNR  & SSIM  & LPIPS & ATE   & FoV \\ \hline
    % GT+3DGS                 &   36.27&   0.94&   0.15&   0      &   0       &   29.28&   0.89&   0.15&   0      &   0       &   33.87&   0.88&   0.27&   0      &   0           &   33.38&   0.95&   0.12&   0      &  0    \\
    COLMAP+3DGS             &   35.81&   0.94&   0.15&   0.00023&   0.993   &   28.77&   0.88&   0.15&   0.00019&   0.021   &   32.74&   0.87&   0.27&   0.00014&   0.0291      &   31.73&   0.94&   0.13&   0.00023&  0.042\\
    w.o. 2D Track Loss      &   24.08&   0.78&   0.40&   0.00973&   1.668   &   17.60&   0.35&   0.42&   0.01969&   3.013   &   20.82&   0.62&   0.45&   0.01735&   3.3760      &   10.22&   0.50&   0.60&   0.03404&  2.413\\
    w.o. 3D Track Loss      &   35.99&   0.94&   0.14&   0.00012&   0.083   &   29.33&   0.90&   0.13&   0.00012&   0.031   &   33.24&   0.88&   0.26&   0.00021&   0.1150      &   30.97&   0.93&   0.14&   0.01056&  0.021\\
    w.o. Scale Loss         &   33.27&   0.87&   0.14&   0.01800&   0.066   &   25.04&   0.71&   0.16&   0.00016&   0.041   &   30.74&   0.83&   0.26&   0.00024&   0.1360      &   29.75&   0.91&   0.14&   0.01111&  0.031\\
    Ours                    &   36.26&   0.95&   0.13&   0.00008&   0.012   &   29.36&   0.90&   0.12&   0.00011&   0.031   &   33.52&   0.88&   0.24&   0.00009&   0.0121      &   31.17&   0.93&   0.13&   0.00044&  0.003\\ \hline
    \end{tabular}
    }
    \caption{\textbf{Ablatation study on different losses.} }
    \label{table:suppl-abla-details}
\end{table*}

During the warmup, we do not perform clone, split, and prune operations on the Gaussian kernels to ensure better geometric constraints on the track points. Afterwards, we will clone new Gaussian kernels from those associated with the track points and apply the same training strategy as the original 3DGS (including clone, split, and prune). However, the tracked Gaussians still need to be preserved without pruning, and should be constrained by a scale loss. The learning rate of $\mu$ and camera parameters continue to decay using \textit{ExponentialLR} from the end of the warmup and the other learning rates still remain constant. For optimization of the camera parameters,we update the camera parameters after calculating the loss of all cameras like bundle adjustment (BA). Considering the limitations of GPU resources, we optimize the Gaussians separately for each camera.
% \pb{add description about updating camera parameters and Gaussians.Those are not updated in the same step.}
% Through the ingenious design, our method has achieved excellent results on both public and synthetic dataset.


\section{Additional Experiments and Results}
We present additional results of novel view synthesis and camera parameter estimation by our method and other baselines, including NoPe-NeRF and CF-3DGS, on Tanks and Temples, CO3D-V2, and Synthetic Dataset.
% , which are not exhibited in our main paper.

\subsection{Novel View Synthesis}
% We show the additional results of novel view synthesis on Tanks and Temples in Fig \ref{fig:suppl-render-T&T}, CO3D-V2 in Fig \ref{fig:suppl-render-co3dv2} and our virtual Synthetic Dataset in Fig \ref{fig:suppl-render-synthetic}, which are evaluated by the same evaluation procedure mentioned in the main paper. Compared with other baselines, our method provides more photo-realistic rendering images, benefits from the high quality rendering ability of 3DGS model and the accuracy of camera parameter estimation by our joint optimization.
As shown in Fig.~\ref{fig:suppl-render-T&T}, \ref{fig:suppl-render-synthetic} and ~\ref{fig:suppl-render-co3dv2}, which are evaluated by the same rules as mentioned in the main paper, our method outperforms other baselines by rendering more photo-realistic images, which benefits from the high quality rendering ability of 3DGS model and the accurate camera parameters estimated by our joint optimization.

\subsection{Camera Parameter Estimation}
% As the camera motions of the scenes in Tanks and Temples are small and mostly linear, the estimated camera parameters are quite accurate on all baselines as mentioned in main paper. We only provide the comparison results on CO3D-V2 in Fig \ref{fig:suppl-pose-co3dv2} and our Synthetic Dataset in Fig \ref{fig:suppl-pose-synthetic} between our method and CF-3DGS. In these scene with large camera motions, our method outperforms baseline significantly, especially on Synthetic datasets which contains more complex camera motion and scenes.
The camera movements in the scenes from Tanks and Temples are minimal and predominantly linear, which results in highly accurate estimated camera parameters across all baselines, as detailed in the main paper. We have included comparison results only for CO3D-V2 in Fig.~\ref{fig:suppl-pose-co3dv2} and for Synthetic Dataset in Fig.~\ref{fig:suppl-pose-synthetic}, comparing our method with CF-3DGS. Our method significantly outperforms the baseline, particularly on the Synthetic Datasets, which involve large camera motions and complex scene.

\subsection{Rendering Trajectory}
% To demonstrate more visualization results of novel view synthesis, we also generate some videos of continuous camera motion using these datasets.
To better illustrate the results of novel view synthesis, we have also created several videos showcasing continuous camera motion using these datasets. Fig.~\ref{fig:suppl-render-nvs} shows novel view synthesis results on scenes from the datasets, in which novel views are sampled from new camera trajectories.
%\pb{We provide some of the results in Fig \ref{fig:suppl-render-nvs}, and more sequential results in the video.}

\subsection{Ablation Study}
To exhibit the effectiveness of different losses in our joint optimization, we ablate each loss of the algorithm on synthetic dataset, since it has ground-truth camera parameters. Tab.~\ref{table:suppl-abla-details} reports the synthesis quality and camera parameter errors accross different variants on our synthetic dataset. 

\begin{figure*}[!t]
  \centering
%  \includegraphics[width=\linewidth]{figures/scgs-suppl-render-T&T-v1.png}
  \includegraphics[width=\linewidth]{figures/scgs-suppl-render-T_T-v1-lowRes.png}
   \caption{\textbf{Comparison of novel view synthesis results on Tanks and Temples.}}
   \label{fig:suppl-render-T&T}
\end{figure*}

\begin{figure*}[!t]
  \centering
 % \includegraphics[width=\linewidth]{figures/scgs-suppl-render-syn-v1.png}
   \includegraphics[width=\linewidth]{figures/scgs-suppl-render-syn-v1-lowRes.png}
   \caption{\textbf{Comparison of novel view synthesis results on Synthetic dataset.}}
   \label{fig:suppl-render-synthetic}
\end{figure*}

\begin{figure*}[t]
  % \centering
%  \includegraphics[width=\linewidth]{figures/scgs-suppl-poses-co3dv2-v1.png}
   \includegraphics[width=\linewidth]{figures/scgs-suppl-poses-co3dv2-v1-lowRes.png}
   \caption{\textbf{Comparison of pose estimation on CO3D-V2.}}
   \label{fig:suppl-pose-co3dv2}
\end{figure*}

\begin{figure*}[!t]
  \centering
  % \includegraphics[width=\linewidth]{figures/scgs-suppl-render-co3dv2-v2.png}
   \includegraphics[width=0.5\linewidth]{figures/scgs-suppl-render-co3dv2-v2-lowRes.png}
   \caption{\textbf{Comparison of novel view synthesis on CO3D-V2.}}
   \label{fig:suppl-render-co3dv2}
\end{figure*}

\begin{figure*}[!t]
  \centering
  % \includegraphics[width=\linewidth]{figures/scgs-supp-poses-syn-v1.png}
  \includegraphics[width=0.5\linewidth]{figures/scgs-supp-poses-syn-v1-lowRes.png}
   \caption{\textbf{Comparison of pose estimation on Synthetic dataset.}}
   \label{fig:suppl-pose-synthetic}
\end{figure*}

\begin{figure*}[!t]
  \centering
  % \includegraphics[width=\linewidth]{figures/scgs-suppl-render-nvs-v1.png}
  \includegraphics[width=\linewidth]{figures/scgs-suppl-render-nvs-v1-lowRes.png}
   \caption{\textbf{Novel view synthesis on a new camera trajectory.} We demonstrate the rendering images and the associated depth maps on scenes from three datasets, where the view points are uniformly sampled on a \textbf{new} camera trajectory.}
   \label{fig:suppl-render-nvs}
\end{figure*}