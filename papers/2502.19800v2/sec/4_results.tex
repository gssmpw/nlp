\section{Experiments}\label{sec:results}

\subsection{Experimental Setup}
\textbf{Datasets.}
We conduct experiments on two real-world datasets, i.e., \emph{Tanks and Temples}~\cite{Tanks_Temples} and \emph{CO3D-V2}~\cite{co3d}, and a \emph{Synthetic Dataset} created by ourselves.
\textbf{Tanks and Temples}, adapted from NoPe-NeRF~\cite{bian2022nopenerf}, is used for novel view synthesis and pose estimation. It features 8 scenes with mild view changes, both indoors and outdoors.
% : As a benchmark dataset for novel view synthesis and pose estimation, we directly adapt it from NoPe-NeRF~\cite{bian2022nopenerf}. It includes 8 scenes with mild view changes located both indoors and outdoors.
\textbf{CO3D-V2} includes thousands of videos of various objects. Following the CF-3DGS~\cite{CF-3DGS-2024}, we select 8 scenes with significant camera movements to demonstrate our robustness.
% : It consists of thousands of videos shot around different objects. Similar to CF-3DGS~\cite{CF-3DGS-2024}, we select 8 scenes, all involving large camera motions. We follow their experimental setup to show the robustness of our method.
\textbf{Synthetic Dataset}
comprises 4 scenes with about 150 frames each created using Blender~\cite{blender2018}, showcasing complex roaming and object-centric camera motions. It's used to assess camera parameter estimation, providing ground truth for intrinsic and extrinsic parameters. For additional details on the synthetic dataset, see the supplementary material.

\textbf{Note that}, the Tanks and Temples dataset is a relatively simple test case for this task, as the camera motion in this scene is mostly linear or involves small movements, resulting in a very limited solution space for pose estimation. In contrast, the CO3D-V2 and our synthetic datasets contain much more complex camera trajectories. Therefore, as follows, we will emphasis more on the results of such challenging scenes, as they pose a higher level of difficulty.

% : We construct a virtual dataset consisting of 4 scenes using Blender~\cite{blender2018}. Each scene contains about 150 frames, including roaming and object-centric camera motions. Compared with CO3D-V2, the camera motion is more complex. We apply it to evaluate camera parameter estimation results as it contains the ground truth intrinsic and extrinsic parameters of each frame.
% \TODO{See more details about the synthetic dataset collection+statistics in the supplementary.}

\textbf{Metrics.}
We use standard evaluation metrics, including PSNR, SSIM~\cite{SSIM}, and LPIPS~\cite{LPIPS} to evaluate the quality of novel view synthesis. For pose estimation, we rely on the Absolute Trajectory Error (ATE) and Relative Pose Error (RPE)~\cite{barf2021,bian2022nopenerf,CF-3DGS-2024}. $\mathrm{RPE}_r$ and $\mathrm{RPE}_t$ are utilized to measure the accuracy of rotation and translation, respectively. To ensure the metrics are comparable on the same scale, we align the camera poses using Umeyama's method~\cite{Umeyama} for both estimation and evaluation. For camera focal length, we convert it to the field of view (FoV) and calculate the angular error, following~\cite{zhu2023tame}.

\textbf{Implementation Details.}
Our implementation is primarily based on \emph{gsplat}~\cite{ye2024gsplatopensourcelibrarygaussian}, an accelerated 3DGS library. We implement joint optimization by backpropagating the gradient of camera parameters through modifications to the CUDA operator in the library. All parameters are optimized using Adam optimizer.
For initialization, we optimize the relative pose between frames, and focal length. Then, the parameters of 3DGS, the absolute poses of cameras, and focal length are optimized. The camera pose is represented as a combination of an axis-angle representation $\mathrm{q} \in \mathfrak{so}(3)$ and a translation vector $\mathrm{t}\in\mathbb{R}^3$.
% During the initialization phase, we train the model with constant learning rates: $5e^{-3}$ for relative rotation, $1e^{-2}$ for relative translation, $5e^{-3}$ for camera focal length, and $1e^{-2}$ for depth distortion. In the joint optimization phase, we follow the same protocol as the original 3DGS~\cite{3DGS2023} for Gaussians but use different schedulers to decay the learning rates for camera pose and focal length. Specifically, for camera poses, the learning rate is exponentially decreased from $5e^{-3}$ to $5e^{-5}$, and for camera focal length, it is exponentially decreased from $5e^{-3}$ to $1e^{-5}$.
All experiments are conducted on a single RTX 4090 GPU with 24GB VRAM.

% \lb{[MOVE TO SUPPL.? START-]
% In practice, adopting a naive training strategy fails to yield reasonable results, because the initial camera poses and tracking points are very noisy and they are interdependent. Directly optimizing the loss function can lead to unstable outcomes and difficulty in convergence. To overcome the challenges, we have designed a joint optimization approach that fundamentally aids in better training.

% To warm up, before training for novel view synthesis, it is necessary to enhance the accuracy of the initial 3DGS and camera parameters. Here, we draw on the concept of global bundle adjustment, first optimizing these parameters through RGB loss and track loss over 500 epochs. The initial learning rate for each variable (Sec.~\ref{subsec:3dgs}) is set as follows: 
% \begin{table}[h]
% \label{tab:lr}
% \centering
% \vspace{-1.6em}
% \resizebox{\columnwidth}{!}{
% \begin{tabular}{c|c|c|c|c|c}
% \hline
% $lr_{\mu}$ & $lr_q$ & $lr_s$ & $lr_{\alpha}$ & $lr_{R_{cw}}$ & $lr_{T_{cw}}$ \\ \hline
% $1.6 * xyz\_scale * 1e^{-2}$ & $1e^{-3}$ & $5e^{-3}$ & $5e^{-2}$ & $5e^{-3}$ & $1e^{-2}$ \\ \hline
% \end{tabular}
% }
% \vspace{-1.6em}
% \end{table}

% For the learning rate of $\mu$, considering the global scale of scene, we introduce the bounding sphere radius of the initial point clouds $xyz\_scale$ as a parameter. Additionally, the learning rates for both $\mu$ and the camera parameters are decayed using the \textit{ExponentialLR} mechanism. Other learning rates remain constant. Moreover, the leanring rate for the focal length is set differentially: it is set to 0 during the initial 100 epochs, meaning that only the camera's pose and the initial 3DGS will be optimized. After the first 100 epochs, it decreases according to the following formulation:
% \begin{equation}
% \resizebox{\columnwidth}{!}{%
%     $lr_{focal} = \left\{
% \begin{array}{rl}
% 0.0 &, step \leq 100 \\
% \max(1e^{-4},5e^{-3}*(1.0-step/500)) &, step > 100
% \end{array} \right.$%
% }
% \end{equation}
% During warmup, our goal is to achieve a better geometric initialization, at which point the weights of the RGB loss and track loss are all set to $1.0$. Note that, we do not perform clone, split, and delete operations on the Gaussian kernels to ensure better geometric constraints on the track points.
% [-END; MOVE TO SUPPL.?]
% }

\begin{figure*}[!t]
  \centering
   % \includegraphics[width=\linewidth]{figures/scgs-render-T_T-v5.png}
   \includegraphics[width=\linewidth]{figures/scgs-render-T_T-v5-loWRes.png}
   \caption{\textbf{Qualitative comparison for novel view synthesis on Tanks and Temples.} We achieve better rendering results on details.}
   \label{fig:render-T&T}
\end{figure*}

\begin{figure}[!t]
  \centering
   % \includegraphics[width=\linewidth]{figures/scgs-render-T&T-v4.png}
   \includegraphics[width=0.9\linewidth]{figures/scgs-HT-CF-Ours.png}
   \caption{\textbf{Qualitative comparison for novel view synthesis on CO3D-V2.} We achieve the best rendering results on details.}
   \label{fig:render-HT-CF-Ours}
\end{figure}

\begin{table}[!t]
    \centering
    \resizebox{\linewidth}{!}{
    \begin{tabular}{c c cccc cccc cccc ccc} \hline
        & \multirow{2}{*}{Scenes} & \multicolumn{3}{c}{Ours}  & & \multicolumn{3}{c}{HT-3DGS} & & \multicolumn{3}{c}{CF-3DGS} & & \multicolumn{3}{c}{NoPe-NeRF}\\ \cline{3-5}\cline{7-9}\cline{11-13}\cline{15-17}
        &                         & PSNR$\uparrow$ & SSIM$\uparrow$ & LPIPS$\downarrow$ & & PSNR & SSIM & LPIPS & & PSNR & SSIM & LPIPS & & PSNR & SSIM & LPIPS  \\ \hline
    \multirow{9}{*}{\rotatebox{90}{Tanks and Temples}}
        & Church                  & 29.39& 0.92& 0.10   & & 31.34& 0.94&  0.08  & & 30.23 & 0.93 & 0.11    & & 25.17 & 0.73 & 0.39    \\
        & Barn                    & 31.98& 0.94& 0.08   & & 34.95& 0.97&  0.05  & & 31.23 & 0.90 & 0.10    & & 26.35 & 0.69 & 0.44    \\
        & Museum                  & 31.92& 0.94& 0.08   & & 31.59& 0.95&  0.08  & & 29.91 & 0.91 & 0.11    & & 26.77 & 0.76 & 0.35    \\
        & Family                  & 32.22& 0.95& 0.09   & & 34.17& 0.97&  0.05  & & 31.27 & 0.94 & 0.07    & & 26.01 & 0.74 & 0.41    \\
        & Horse                   & 30.33& 0.94& 0.09   & & 35.82& 0.98&  0.03  & & 33.94 & 0.96 & 0.05    & & 27.64 & 0.84 & 0.26    \\
        & Ballroom                & 35.03& 0.97& 0.03   & & 34.12& 0.97&  0.04  & & 32.47 & 0.96 & 0.07    & & 25.33 & 0.72 & 0.38    \\
        & Francis                 & 33.39& 0.92& 0.15   & & 34.09& 0.93&  0.13  & & 32.72 & 0.91 & 0.14    & & 29.48 & 0.80 & 0.38    \\
        & Ignatius                & 29.25& 0.90& 0.11   & & 31.64& 0.95&  0.06  & & 28.43 & 0.90 & 0.09    & & 23.96 & 0.61 & 0.47    \\ \cline{2-17}
        & mean                    & 31.68& 0.94& 0.09   & & \cellcolor{gray!40}33.53& \cellcolor{gray!40}0.96&  \cellcolor{gray!40}0.07  & & 31.28 & 0.93 & 0.09    & & 26.34 & 0.74 & 0.39    \\ \hline
     \multirow{9}{*}{\rotatebox{90}{CO3D-V2}}  
        & {34\_1403\_4393}          & 28.68 & 0.88 & 0.21   & & 32.52& 0.93& 0.14   & & 27.75 & 0.86 & 0.20     & & 28.62 & 0.80 & 0.35    \\
        & {46\_2587\_7531}          & 31.83 & 0.92 & 0.12   & & 30.65& 0.91& 0.13   & & 25.44 & 0.80 & 0.21     & & 25.30 & 0.73 & 0.46    \\
        & {106\_12648\_23157}       & 26.18 & 0.83 & 0.19   & & 23.43& 0.73& 0.28   & & 22.14 & 0.64 & 0.34     & & 20.41 & 0.46 & 0.58    \\
        & {110\_13051\_23361}       & 33.44 & 0.94 & 0.11   & & 29.95& 0.87& 0.19   & & 29.69 & 0.89 & 0.29     & & 26.86 & 0.73 & 0.47    \\
        & {245\_26182\_52130}       & 33.82 & 0.93 & 0.20   & & 28.59& 0.87& 0.27   & & 27.24 & 0.85 & 0.30     & & 25.05 & 0.80 & 0.49    \\
        & {407\_54965\_106262}      & 28.73 & 0.86 & 0.39   & & -    & -   & -      & & 27.80 & 0.84 & 0.35     & & 25.53 & 0.83 & 0.58    \\
        & {415\_57112\_110099}      & 30.37 & 0.88 & 0.22   & & 27.23& 0.78& 0.30   & & 22.14 & 0.64 & 0.34     & & 20.41 & 0.46 & 0.58    \\
        & {429\_60388\_117059}      & 25.70 & 0.70 & 0.35   & & -    & -   & -      & & 24.44 & 0.68 & 0.36     & & 22.19 & 0.62 & 0.56    \\ \cline{2-17}
        & mean                      & \cellcolor{gray!40}29.84 & \cellcolor{gray!40}0.87 & \cellcolor{gray!40}0.22   & & 28.73& 0.85& 0.22   & & 25.83 & 0.78 & 0.30     & & 24.30 & 0.68 & 0.51   \\ \hline
    \end{tabular}
    }
    \caption{\textbf{Novel view synthesis results on Tanks and Temples and CO3D-V2.} Each baseline method is trained with its public code under the original settings and evaluated with the same evaluation protocol. The best results are gray background.}
    \label{table:render-T&T-co3dv2}
    % \vspace{-5pt}
\end{table}

%In the subsequent training phase, 
During the training phase, we will clone new Gaussians from those associated with the track points and apply the same training strategy as the original 3DGS (including clone, split, and delete). Note that the track Gaussians still need to be preserved and constrained by a scale loss. 
%At this time, we 
We use Eq.~\ref{eq:overall} for training and set $\lambda_{1}=0.8$, $\lambda_{\mathrm{D-SSIM}}=0.2$, $\lambda_{t2d}=0.01$, $\lambda_{t3d}=0.01$, $\lambda_{scale}=0.01$.
% \lb{[MOVE TO SUPPL.]The learning rate of $\mu$ and camera parameters continue decay with \textit{ExponentialLR} from the end of the warm-up and the other learning rates still remain constant.[MOVE TO SUPPL.]}

% \TODO{Do we need to specify the value of lambda such as $\lambda_{rgb}$, and learning rate for each parameters?} 

\begin{figure}[!t]
  \centering
   % \includegraphics[width=\linewidth]{figures/scgs-render-co3dv2-v4.png}
   \includegraphics[width=0.9\linewidth]{figures/scgs-render-co3dv2-v4-lowRes.png}
   \caption{\textbf{Qualitative comparison for novel view synthesis and camera pose estimation on CO3D-V2.} Benefit from the accuracy of the camera pose estimation, the rendering quality of novel view synthesis obtained by our method is higher than CF-3DGS.}
   \label{fig:render-co3dv2}
\end{figure}

\subsection{Experimental Results and Analysis}

% In this subsection, we compare our method with two baselines, including CF-3DGS~\cite{CF-3DGS-2024} and NoPe-NeRF~\cite{bian2022nopenerf}, on both novel view synthesis and camera pose estimation.
\textbf{Novel View Synthesis.}
Unlike the standard settings where the camera poses of test views are given, we need to first obtain the camera poses of the test views for rendering. Following the same protocol as CF-3DGS~\cite{CF-3DGS-2024}, we obtain the camera poses of the test views by minimizing the photometric error between the synthesized images and the test views using the pre-trained 3DGS model. We apply the same procedure to all baseline methods to maintain a consistent bias for a fair comparison.
%Unlike the colmap where the camera paramters of testing views are given, we need to first obtain the camera parameters of test views for rendering.Inspired by CF-3DGS\cite{CF-3DGS-2024}, we freeze the pre-trained 3DGS model that trained on the training views, while optimizing testing viewsâ€™ camera poses via minimizing the photometric error between the synthesised images and the test views.And we assume that all cameras share the same intrinsic parameters, so during testing, we directly load the camera intrinsics from the trained checkpoint and keep them freezed.

We report the results on Tanks and Temples and CO3D-V2 in Tab.~\ref{table:render-T&T-co3dv2}. Our method consistently outperforms NoPe-NeRF across all metrics and slightly outperforms CF-3DGS in overall performance on Tanks and Temples. 
On CO3D-V2, our results are better than those of all baselines. 
Interestingly, compared to HT-3DGS~\cite{ji2024sfmfree3dgaussiansplatting}, our method achieves significantly higher PSNR on the CO3D-V2, which involves large camera motion, but performs worse on the Tanks and Temples, which features smooth motion trajectories. We attribute this to HT-3DGS's reliance on video frame interpolation, which can be unstable under large camera motion.
As shown in Fig.~\ref{fig:render-T&T}, the images synthesized by our method are clearer than those obtained by other methods, as evidenced by higher scores in terms of SSIM and LPIPS, as detailed in Tab.~\ref{table:render-T&T-co3dv2}.
As illustrated in Fig.~\ref{fig:render-HT-CF-Ours} and ~\ref{fig:render-co3dv2}, the advantages of our algorithm are well demonstrated, especially with large camera motions. Due to global joint optimization, multi-view geometric consistency is better maintained in the trained 3DGS model, leading to high-quality images. 
% with fine details.

For further comparison, we evaluated our method on the Synthetic Dataset, which features extremely complex camera motions. One result is shown in the bottom-right of Fig.~\ref{fig:teaser}. In this case, the camera not only moves in multiple circles around the object but also changes significantly in the vertical direction. Our synthesized image from the novel view remains clear and sharp, whereas the CF-3DGS result is blurry with obvious artifacts.

%Table \ref{table:render-T&T} show the results of our method and other baselines on \emph{Tanks and Temples}. We also show the qualitative results in Fig \ref{fig:render-T&T}. We think that the scenes and camera motion in Tanks and Temples are relatively simple, so compared with other methods, our results do not show a significant advantage.So we evaluate our method on \emph{co3dv2}.The qualitative results is show in Fig \ref{fig:render-co3dv2}.
%The quantitative results is show in Tab\ref{table:render-co3dv2}.Benefiting from our joint optimization method,the images synthesized through our approach are significantly sharper and clearer than those produced by the other methods,as evidenced by the notably higher scores in terms of PSNR, SSIM and LPIPS.

% \begin{table}[!t]
%     \centering
%     \resizebox{\linewidth}{!}{
%     \begin{tabular}{c c cccc cccc cccc ccc} \hline
%         & \multirow{2}{*}{Scenes} & \multicolumn{3}{c}{Ours}  & & \multicolumn{3}{c}{HT-3DGS} & & \multicolumn{3}{c}{CF-3DGS} & & \multicolumn{3}{c}{NoPe-NeRF}\\ \cline{3-5}\cline{7-9}\cline{11-13}\cline{15-17}
%         &                         & $\mathrm{RPE}_{t}\downarrow$ & $\mathrm{RPE}_{r}\downarrow$ & ATE$\downarrow$ & & $\mathrm{RPE}_{t}$ & $\mathrm{RPE}_{r}$ & ATE & & $\mathrm{RPE}_{t}$ & $\mathrm{RPE}_{r}$ & ATE & & $\mathrm{RPE}_{t}$ & $\mathrm{RPE}_{r}$ & ATE  \\ \hline
%     \multirow{9}{*}{\rotatebox{90}{Tanks and Temples}}
%         & Church                  & 0.007& 0.013& 0.002   & & & &  & & 0.008 & 0.018 & 0.002    & & 0.034 & 0.008 & 0.008      \\
%         & Barn                    & 0.007& 0.021& 0.001    & & & &  & & 0.034 & 0.034 & 0.003    & & 0.046 & 0.032 & 0.004      \\
%         & Museum                  & 0.029& 0.115& 0.003    & & & &  & & 0.052 & 0.215 & 0.005    & & 0.207 & 0.202 & 0.020      \\
%         & Family                  & 0.013& 0.021& 0.001    & & & &  & & 0.022 & 0.024 & 0.002    & & 0.047 & 0.015 & 0.001      \\
%         & Horse                   & 0.082& 0.036& 0.002    & & & &  & & 0.112 & 0.057 & 0.003    & & 0.179 & 0.017 & 0.003      \\
%         & Ballroom                & 0.014& 0.013& 0.001   & & & &  & & 0.037 & 0.024 & 0.003    & & 0.041 & 0.018 & 0.002      \\
%         & Francis                 & 0.010& 0.084& 0.003    & & & &  & & 0.029 & 0.154 & 0.006    & & 0.057 & 0.009 & 0.005      \\
%         & Ignatius                & 0.020& 0.031& 0.003    & & & &  & & 0.033 & 0.032 & 0.005    & & 0.026 & 0.005 & 0.002  \\ \cline{2-17}
%         & mean                    & 0.023& 0.042& 0.002    & & & &  & & 0.041 & 0.069 & 0.004    & & 0.080 & 0.038 & 0.006  \\ \hline
%      \multirow{9}{*}{\rotatebox{90}{CO3D-V2}}  
%         & {34\_1403\_4393}          & 0.099 & 0.605 & 0.009     & & 0.041& 0.170&  0.009    & & 0.505 & 0.211 & 0.009     & & 0.591 & 1.313 & 0.053    \\
%         & {46\_2587\_7531}          & 0.013 & 0.080 & 0.001     & & 0.025& 0.275& 0.004     & & 0.095 & 0.447 & 0.009     & & 0.426 & 4.226 & 0.023    \\
%         & {106\_12648\_23157}       & 0.009 & 0.076 & 0.001     & & 0.045& 0.282& 0.014     & & 0.094 & 0.360 & 0.008     & & 0.387 & 1.312 & 0.049    \\
%         & {110\_13051\_23361}       & 0.012 & 0.052 & 0.001     & & 0.093& 0.331& 0.020     & & 0.140 & 0.401 & 0.021     & & 0.400 & 1.966 & 0.046    \\
%         & {245\_26182\_52130}       & 0.005 & 0.029 & 0.001     & & 0.064& 0.438& 0.017     & & 0.239 & 0.472 & 0.017     & & 0.587 & 1.867 & 0.038    \\
%         & {407\_54965\_106262}      & 0.062 & 0.461 & 0.011     & & -    & -    & -         & & 0.310 & 0.243 & 0.008     & & 0.553 & 4.685 & 0.057    \\
%         & {415\_57112\_110099}      & 0.004 & 0.024 & 0.001     & & 0.049& 0.351& 0.024     & & 0.110 & 0.424 & 0.014     & & 0.326 & 1.919 & 0.054    \\
%         & {429\_60388\_117059}      & 0.052 & 0.454 & 0.009     & & -    & -    & -         & & 0.134 & 0.542 & 0.018     & & 0.398 & 2.914 & 0.055    \\ \cline{2-17}
%         & mean                      & 0.032 & 0.222 & 0.004     & & 0.053& 0.308& 0.017     & & 0.203 & 0.388 & 0.013     & & 0.459 & 2.525 & 0.047    \\ \hline
%     \end{tabular}
%     }
%     \caption{\textbf{Quantitative comparison of pose accuracy on Tanks and Temples and CO3D-V2.} Note that, we use COLMAP poses as the ground truth. The unit of $\mathrm{RPE}_{r}$ is in degrees, ATE is in the ground truth scale and $\mathrm{RPE}_{t}$ is scaled by 100. The best results are highlighted in bold. \TODO{highlight}}
%     \label{table:pose-T&T-co3dv2}
% \end{table}

\begin{table}[!t]
    \centering
    \resizebox{\linewidth}{!}{
    \begin{tabular}{c c cccc cccc cccc ccc} \hline
        & \multirow{2}{*}{Scenes} & \multicolumn{3}{c}{Ours}  & & \multicolumn{3}{c}{HT-3DGS} & & \multicolumn{3}{c}{CF-3DGS} & & \multicolumn{3}{c}{NoPe-NeRF}\\ \cline{3-5}\cline{7-9}\cline{11-13}\cline{15-17}
        &                         & $\mathrm{RPE}_{t}\downarrow$ & $\mathrm{RPE}_{r}\downarrow$ & ATE$\downarrow$ & & $\mathrm{RPE}_{t}$ & $\mathrm{RPE}_{r}$ & ATE & & $\mathrm{RPE}_{t}$ & $\mathrm{RPE}_{r}$ & ATE & & $\mathrm{RPE}_{t}$ & $\mathrm{RPE}_{r}$ & ATE  \\ \hline
     \multirow{9}{*}{\rotatebox{90}{CO3D-V2}}  
        & {34\_1403\_4393}          & 0.099 & 0.605 & 0.009     & & 0.041& 0.170&  0.009    & & 0.505 & 0.211 & 0.009     & & 0.591 & 1.313 & 0.053    \\
        & {46\_2587\_7531}          & 0.013 & 0.080 & 0.001     & & 0.025& 0.275& 0.004     & & 0.095 & 0.447 & 0.009     & & 0.426 & 4.226 & 0.023    \\
        & {106\_12648\_23157}       & 0.009 & 0.076 & 0.001     & & 0.045& 0.282& 0.014     & & 0.094 & 0.360 & 0.008     & & 0.387 & 1.312 & 0.049    \\
        & {110\_13051\_23361}       & 0.012 & 0.052 & 0.001     & & 0.093& 0.331& 0.020     & & 0.140 & 0.401 & 0.021     & & 0.400 & 1.966 & 0.046    \\
        & {245\_26182\_52130}       & 0.005 & 0.029 & 0.001     & & 0.064& 0.438& 0.017     & & 0.239 & 0.472 & 0.017     & & 0.587 & 1.867 & 0.038    \\
        & {407\_54965\_106262}      & 0.062 & 0.461 & 0.011     & & -    & -    & -         & & 0.310 & 0.243 & 0.008     & & 0.553 & 4.685 & 0.057    \\
        & {415\_57112\_110099}      & 0.004 & 0.024 & 0.001     & & 0.049& 0.351& 0.024     & & 0.110 & 0.424 & 0.014     & & 0.326 & 1.919 & 0.054    \\
        & {429\_60388\_117059}      & 0.052 & 0.454 & 0.009     & & -    & -    & -         & & 0.134 & 0.542 & 0.018     & & 0.398 & 2.914 & 0.055    \\ \cline{2-17}
        & mean                      & \cellcolor{gray!40}0.032 & \cellcolor{gray!40}0.222 & \cellcolor{gray!40}0.004     & & 0.053& 0.308& 0.017     & & 0.203 & 0.388 & 0.013     & & 0.459 & 2.525 & 0.047    \\ \hline
    \end{tabular}
    }
    \caption{\textbf{Quantitative comparison of pose accuracy on CO3D-V2.} Note that, we use COLMAP poses as the ground truth. The unit of $\mathrm{RPE}_{r}$ is in degrees, ATE is in the ground truth scale and $\mathrm{RPE}_{t}$ is scaled by 100. The best results are gray background.}
    \label{table:pose-T&T-co3dv2}
\end{table}

\begin{table}[!t]
    \centering
    \resizebox{\linewidth}{!}{
    \begin{tabular}{c cccc cccc cccc} \hline
    \multirow{2}{*}{Scenes} & \multicolumn{3}{c}{Ours} & & \multicolumn{3}{c}{CF-3DGS} & & \multicolumn{3}{c}{COLMAP+3DGS}\\ \cline{2-4}\cline{6-8}\cline{10-12}
                            & PSNR$\uparrow$ & SSIM$\uparrow$ & LPIPS$\downarrow$ & & PSNR & SSIM & LPIPS & & PSNR & SSIM & LPIPS \\ \hline
    classroom       & \cellcolor{gray!40}36.26 & \cellcolor{gray!40}0.94 & \cellcolor{gray!40}0.15    & & 19.69 & 0.69 & 0.46         & & 35.81 & 0.94 & 0.15 \\
    {lego\_c2}      & \cellcolor{gray!40}29.36 & \cellcolor{gray!40}0.90 & \cellcolor{gray!40}0.12    & & 15.93 & 0.31 & 0.55         & & 28.77 & 0.88 & 0.15 \\
    livingroom      & \cellcolor{gray!40}33.52 & \cellcolor{gray!40}0.88 & \cellcolor{gray!40}0.24    & & 16.63 & 0.57 & 0.57         & & 32.74 & 0.87 & 0.27 \\
    bedroom         & 31.17 & 0.93 & \cellcolor{gray!40}0.13    & & 16.98 & 0.65 & 0.45         & & \cellcolor{gray!40}31.73 & \cellcolor{gray!40}0.94 & 0.13 \\   \hline
    % mean     & \textbf{32.38} & \textbf{0.91} & \textbf{0.16}  && & &     & & 32.26 & 0.90 & 0.18  \\ \hline
    \end{tabular}
    }
    \caption{\textbf{Novel view synthesis results on our Synthetic dataset.} We show the quantitative results using our method, CF-3DGS and COLMAP+3DGS. The best results are gray background.}
    \label{table:render-Synthetic}
\end{table}

\textbf{Camera Parameter Estimation.}
First, we compare the camera pose estimation with baseline methods. In the comparison, our method only assumes that the camera focal length is fixed, while others additionally input the camera focal length. The estimated camera poses are analyzed by Procrusts as in CF-3DGS and compared with the ground-truth of training views.
%The quantitative results of camera pose estimation on Tanks and Temples and CO3D-V2 datasets are summarized in Tab.~\ref{table:pose-T&T-co3dv2}
The quantitative results of camera pose estimation on CO3D-V2 datasets are summarized in Tab.~\ref{table:pose-T&T-co3dv2}\footnote{Due to the hierarchical training of HT-3DGS, it needs significant computational resources and causes OOM errors in our experimental setup. We report results from the paper, only covering partial scenes from CO3D-V2.}. 
%On the Tanks and Temples dataset, where the camera motion is relatively mild, we achieved comparable results to CF-3DGS and NoPe-NeRF. However, on the CO3D-V2 dataset, our results are significantly better than those of all methods. 
The results show that our estimated camera parameters achieve the smallest error among all methods, with the Absolute Trajectory Error (ATE) being only one-fourth of that of the second-best method.
This demonstrates that our algorithm excels in scenes with complex camera motions. Compared to the baselines, the global tracking information we use eliminates accumulated errors, leading to more accurate camera pose estimation. Additionally, joint optimization enhances the stability of the estimation results. 
%The learned camera poses are post-processed by Procrustes analysis as in \cite{bian2022nopenerf,CF-3DGS-2024} and compared with the ground-truth poses of training views. The quantitative results of camera pose estimation are summarized in \ref{table:pose-T&T}. Due to use of global track information to eliminate accumulated errors,regardless of whether it is on Tanks and Temples dataset or co3dv2 dataset, our method achieved the highest pos accuracy.

Next, we evaluate camera parameter estimation on our Synthetic Dataset. Tab.~\ref{table:pose-Synthetic} shows the estimation errors from CF-3DGS, COLMAP, and ours. Note that CF-3DGS uses the camera FoV estimated by COLMAP. We find that our estimated camera FoVs and poses are comparable to those of COLMAP, and the camera pose error is 10 times smaller than that of CF-3DGS. Fig.~\ref{fig:render-phase} visualizes our estimated poses in different stages. Thanks to the joint optimization based on global track and the back-propagation of the gradient of the camera parameters, our approach is able to combine these two tasks, thereby reducing the input requirements.

\textbf{Algorithm Efficiency.}
%All experiments are conducted on a single RTX 4090 GPU. The processing times for CF-3DGS and NoPe-NeRF, averaged over all scenes, are approximately 1.5 hour and 20 hours, respectively. Since we optimize all parameters, including those from the camera, simultaneously, our approach takes less than 1 hour and uses less than 12GB of GPU memory on average.
All experiments were conducted on a single RTX 4090 GPU. On average across all scenes, the processing times for CF-3DGS and NoPe-NeRF are approximately 1.5 hours and 4 hours, respectively. HT-3DGS reports a runtime of around 4 hours on a professional-grade GPU. In contrast, our approach takes less than 1 hour and uses less than 12GB of GPU memory on average, as we optimize all parameters, including camera parameters.

%All experiments are conducted on a NVIDIA 4090 GPU.Since we optimize the parameters of all cameras and 3DGS at the same time,in our experiments the average processing time is less than 1 hour, and the GPU memory usage is less than 12GB.

%\textbf{Synthetic scenes}
%In order to compare the accuracy of the camera parameters, we also constructed a virtual dataset with ground truth using Blender.
%Additionally we compare ours parameters with colmap\cite{colmap2016}.The results of novel view synthesis is show in Tab.\ref{table:render-Synthetic}.The qualitative results is show in Fig.\ref{fig:render-Synthetic}. We also evaluated the accuracy of the camera intrinsic parameter, focal length, and the extrinsic parameters,camera pos.The results of camera parameters in show in Tab.\ref{table:pose-Synthetic}.


% \begin{table}[!t]
%     \centering
%     \resizebox{\linewidth}{!}{
%     \begin{tabular}{c cccc cccc ccc} \hline
%     \multirow{2}{*}{Scenes} & \multicolumn{3}{c}{Ours} & & \multicolumn{3}{c}{CF-3DGS} & & \multicolumn{3}{c}{NoPe-NeRF}\\ \cline{2-4}\cline{6-8}\cline{10-12}
%                             & PSNR$\uparrow$ & SSIM$\uparrow$ & LPIPS$\downarrow$ & & PSNR & SSIM & LPIPS & & PSNR & SSIM & LPIPS \\ \hline
%     {34\_1403\_4393}        & \textbf{28.68} & \textbf{0.88} & 0.21   & & 27.75 & 0.86 & \textbf{0.20}     & & 28.62 & 0.80 & 0.35    \\
%     {46\_2587\_7531}        & \textbf{31.83} & \textbf{0.92} & \textbf{0.12}  & & 25.44 & 0.80 & 0.21     & & 25.30 & 0.73 & 0.46    \\
%     {106\_12648\_23157}     & \textbf{26.18} & \textbf{0.83} & \textbf{0.19}   & & 22.14 & 0.64 & 0.34     & & 20.41 & 0.46 & 0.58    \\
%     {110\_13051\_23361}     & \textbf{33.44} & \textbf{0.94} & \textbf{0.11}   & & 29.69 & 0.89 & 0.29     & & 26.86 & 0.73 & 0.47    \\
%     {245\_26182\_52130}     & \textbf{33.82} & \textbf{0.93} & \textbf{0.20}   & & 27.24 & 0.85 & 0.30     & & 25.05 & 0.80 & 0.49    \\
%     {407\_54965\_106262}    & \textbf{28.73} & \textbf{0.86} & 0.39  & & 27.80 & 0.84 & \textbf{0.35}     & & 25.53 & 0.83 & 0.58    \\
%     {415\_57112\_110099}    & \textbf{30.37} & \textbf{0.88} & \textbf{0.22}   & & 22.14 & 0.64 & 0.34     & & 20.41 & 0.46 & 0.58    \\
%     {429\_60388\_117059}    & \textbf{25.70} & \textbf{0.70} & \textbf{0.35}  & & 24.44 & 0.68 & 0.36     & & 22.19 & 0.62 & 0.56    \\ \hline
%     mean                    & \textbf{29.84} & \textbf{0.87} & \textbf{0.22}   & & 25.83 & 0.775 & 0.30    & & 24.30 & 0.68 & 0.51   \\ \hline
%     \end{tabular}
%     }
%     \caption{\textbf{Novel view synthesis results on CO3D-V2.} All rendering metrics of our method are better than other baselines. The best results are highlighted in bold.}
%     \label{table:render-co3dv2}
% \end{table}

% \begin{table}[!t]
%     \centering
%     \resizebox{\linewidth}{!}{
%     \begin{tabular}{c cccc cccc ccc} \hline
%     \multirow{2}{*}{Scenes} & \multicolumn{3}{c}{Ours} & & \multicolumn{3}{c}{CF-3DGS} & & \multicolumn{3}{c}{NoPe-NeRF}\\ \cline{2-4}\cline{6-8}\cline{10-12}
%                             & $\mathrm{RPE}_{t}\downarrow$ & $\mathrm{RPE}_{r}\downarrow$ & ATE$\downarrow$   & & $\mathrm{RPE}_{t}$ & $\mathrm{RPE}_{r}$ & ATE  & & $\mathrm{RPE}_{t}$ & $\mathrm{RPE}_{r}$ & ATE \\ \hline
%     {34\_1403\_4393}        & \textbf{0.099} & 0.605 & \textbf{0.0088}    & & 0.505 & \textbf{0.211} & 0.009     & & 0.591 & 1.313 & 0.053    \\
%     {46\_2587\_7531}        & \textbf{0.013} & \textbf{0.080} & \textbf{0.0005}  & & 0.095 & 0.447 & 0.009     & & 0.426 & 4.226 & 0.023    \\
%     {106\_12648\_23157}     & \textbf{0.009} & \textbf{0.076} & \textbf{0.0009}    & & 0.094 & 0.360 & 0.008     & & 0.387 & 1.312 & 0.049    \\
%     {110\_13051\_23361}     & \textbf{0.012} & \textbf{0.052} & \textbf{0.0002}    & & 0.140 & 0.401 & 0.021     & & 0.400 & 1.966 & 0.046    \\
%     {245\_26182\_52130}     & \textbf{0.005} & \textbf{0.029} & \textbf{0.0001}    & & 0.239 & 0.472 & 0.017     & & 0.587 & 1.867 & 0.038    \\
%     {407\_54965\_106262}    & \textbf{0.062} & 0.461 & 0.0107   & & 0.310 & \textbf{0.243} & \textbf{0.008}     & & 0.553 & 4.685 & 0.057    \\
%     {415\_57112\_110099}    & \textbf{0.004} & \textbf{0.024} & \textbf{0.0001}    & & 0.110 & 0.424 & 0.014     & & 0.326 & 1.919 & 0.054    \\
%     {429\_60388\_117059}    & \textbf{0.052} & \textbf{0.454} & \textbf{0.0092}  & & 0.134 & 0.542 & 0.018     & & 0.398 & 2.914 & 0.055    \\ \hline
%     mean                    & \textbf{0.032} & \textbf{0.222} & \textbf{0.0038}  & & 0.203 & 0.388 & 0.013     & & 0.459 & 2.525 & 0.047    \\ \hline
%     \end{tabular}
%     }
%     \caption{\textbf{Quantitative comparison of pose accuracy on CO3D-V2.} We use COLMAP poses as the ground truth. The best results are highlighted in bold. \gjh{The decimal precision in ATE is inconsistent}}
%     \label{table:pose-co3dv2}
% \end{table}


\begin{table*}[!t]
    \centering
    \resizebox{\linewidth}{!}{
    \begin{tabular}{c ccccc ccccc ccccc cccc} \hline
    \multirow{2}{*}{Scenes} & \multicolumn{4}{c}{classroom} & & \multicolumn{4}{c}{{lego\_c2}} & & \multicolumn{4}{c}{livingroom} & & \multicolumn{4}{c}{bedroom}\\ \cline{2-5}\cline{7-10}\cline{12-15}\cline{17-20}
    & FoV($^\circ$)$\downarrow$ & $\mathrm{RPE}_{t}\downarrow$ & $\mathrm{RPE}_{r}\downarrow$ & ATE$\downarrow$ & & FoV($^\circ$) & $\mathrm{RPE}_{t}$ & $\mathrm{RPE}_{r}$ & ATE & & FoV($^\circ$) & $\mathrm{RPE}_{t}$ & $\mathrm{RPE}_{r}$ & ATE & & FoV($^\circ$) & $\mathrm{RPE}_{t}$ & $\mathrm{RPE}_{r}$ & ATE \\ \hline
    CF-3DGS     & 0.993 & 0.588 & 2.436 & 0.07412    & & 0.021 & 1.1265 & 4.946 & 0.10795   & & 0.029 & 0.425 & 2.104 & 0.07653  & & 0.042 & 0.366 & 1.103 & 0.06284 \\
    COLMAP+3DGS & 0.993 & 0.004 & 0.018 & \cellcolor{gray!40}0.00007     & & \cellcolor{gray!40}0.021 & 0.009 & 0.026 & 0.00019   & & 0.029 & 0.008 & 0.026 & 0.00014   & & 0.042 & \cellcolor{gray!40}0.009 & \cellcolor{gray!40}0.035 & \cellcolor{gray!40}0.00023 \\
    Ours        & \cellcolor{gray!40}0.012 & \cellcolor{gray!40}0.002 & \cellcolor{gray!40}0.013 & 0.00008     & & 0.031 & \cellcolor{gray!40}0.002 & \cellcolor{gray!40}0.015 & \cellcolor{gray!40}0.00011   & & \cellcolor{gray!40}0.012 & \cellcolor{gray!40}0.002 & \cellcolor{gray!40}0.013 & \cellcolor{gray!40}0.00009   & & \cellcolor{gray!40}0.003 & 0.013 & 0.062 & 0.00044 \\\hline
    % mean & \textbf{0.063} & 0.015 & 0.073 & 0.00121 & & & & & & & 0.271 & \textbf{0.007} & \textbf{0.026} & \textbf{0.00016} \\
    % \hline
    \end{tabular}
    }
    \caption{\textbf{Quantitative comparison of parameter accuracy on our Synthetic dataset.} We convert the estimated camera intrinsics focal to FoV and perform the errors of FoV with ground truth (provided by our synthetic datasets) on our method, CF-3DGS and COLMAP+3DGS. As CF-3DGS requires the camera intrinsic parameters as fixed inputs, we set them the same as COLMAP+3DGS.}
    \label{table:pose-Synthetic}
\end{table*}

\begin{figure}[!t]
  \centering
  % \includegraphics[width=\linewidth]{figures/scgs-diff-poses.png}
  \includegraphics[width=\linewidth]{figures/scgs-diff-poses-lowRes.png}
   \caption{\textbf{The trajectory of initial stage and joint stage}. Our joint stage significantly improved the accuracy of camera pose.}
   \label{fig:render-phase}
\end{figure}

\subsection{Ablation Study}
\textbf{Effectiveness of Different Losses.}
We ablate each loss of the algorithm on synthetic dataset, since it has ground-truth camera parameters. Tab.~\ref{tab:ablation-all} reports the average synthesis quality and camera parameter errors across different algorithm variants (see supplementary material for details). In order to better show the role of each loss, we construct variants of the three schemes and remove them one by one.
First, we can find that any variant of our algorithm (Variant 3, 4, 5, 6) is better than the CF-3DGS method (Variant 3) in synthesis quality and absolute camera position.
Second, from Variant 3, it shows that 2D track loss plays a crucial role in the entire joint optimization. When 2D track loss is not used, compared with the final method (Variant 3 vs. 6), there is a significant decrease in synthesis quality (18.18 vs. 32.58), and the camera parameter error is significantly larger (2.617 vs. 0.015). This shows that the reprojection error constrained by global consistency can significantly enhance the camera parameter estimation, thereby improving the 3DGS training effect and improving the new perspective synthesis ability.
In addition, the results of Variant 4 vs. 6 show that 3D track loss can further enhance the geometric consistency of 3DGS. When 3D track loss is used, the PSNR of the novel view synthesis can be further improved by 0.2 dB, and the absolute position error of the camera can be reduced by an order of magnitude.
From the experiment, we find that when scale loss is not used, Variant 5 has obvious degradation in all results of the entire scene. This is related to the fact that our method achieves explicit tracking by limiting the size of the tracked Gaussian and using it as a virtual spatial 3D point.
Tab.~\ref{tab:ablation-gaussian-size} further illustrates the role of scale loss in constraining the size of the tracked Gaussian. It can be seen that after using scale loss, the centroid of the Gaussian is an order of magnitude closer to the surface than those without it. 

We further analyze the role of track losses~(2D\&3D) and scale loss on the CO3D-V2 in Tab.~\ref{table:ablation-trackloss-scaleloss}. 
Additionally, we visualize the camera poses and rendered images with and without track losses in Fig.~\ref{fig:ablation-wotrack}. Our observations indicate that global tracks are crucial for improving both novel view synthesis and pose estimation, as the tracks enforce multi-view geometric consistency during 3DGS training.


\iffalse
\textbf{Effectiveness of 2D Track Loss.}\TODO{merge}
We first ablate the 2D track loss from the joint optimization in Eq.~\ref{eq:overall}. We use the camera parameters from the local initialization and DPT depth to obtain the initial 3D Gaussians. Then, we train a standard 3DGS model using the camera parameters and the initial 3D Gaussians, but without the track information. We report the synthesis quality and pose error with and without the track loss in Tab.~\ref{table:ablation-trackloss-scaleloss}. We also visualize the camera poses and the render image w./w.o. the track loss in Fig.~\ref{fig:ablation-wotrack}. We observe that the global tracks are essential for enhancing both novel view synthesis and pose estimation, as the tracks maintain multi-view geometric consistency during 3DGS training.

\textbf{Effectiveness of 3D Track Loss.} \TODO{merge}
\lb{We ablate the 3D track loss on our synthetic dataset with ground-truth camera poses and high-quality images. First, we train 3DGS using only original photometric consistency with ground-truth camera parameters, approximating the upper bound of the original method. We then ablate the 3D track loss from the joint optimization (Eq.~\ref{eq:overall}) and compare it with COLMAP-initialized 3DGS. Results in Tab.~\ref{table:ablation-track3d-loss} show that COLMAP performs worst, followed by our joint optimization without 3D track loss. The method with 3D track loss ranks second, slightly below GT+3DGS, proving its effectiveness in improving multi-view geometric consistency despite modest numerical gains.}

\textbf{Effectiveness of Scale Loss.\TODO{merge}}
Scale loss regularizes the size of the Gaussians used for tracking, with the centroids serving as spatial points for maintaining the multi-view consistency of the 3DGS model.
% To confirm its impact on performance, we remove the scale loss from Eq.~\ref{eq:overall}. 
Tab.~\ref{table:ablation-trackloss-scaleloss} reports the results of novel view synthesis and pose estimation with and without the scale loss. We observe that the scale loss plays an essential role in both tasks, as removing it results in a degradation of the results across almost all scenes in CO3D-V2 dataset.
%Because we select some 3dgs to track,we hypothesis that the 3dgs should be located on the geometric surface.To confirm the effectiveness of the scale loss,we remove it from the global joint optimization.The results for novel view synthesis and pose accuracy are shown as w.o.Scale in Tab.\ref{table:ablation-scaleloss}.We observe that scale loss shows improvements in metrics for novel view synthesis and pose estimation.
\fi

\iffalse
\begin{table}[!t]
    \centering
    \resizebox{\linewidth}{!}{
    \begin{tabular}{c ccc c ccc c ccc}\hline
    \multirow{2}{*}{scenes}  & \multicolumn{3}{c}{w.o. 2D Track}  & & \multicolumn{3}{c}{w.o. Scale}     & & \multicolumn{3}{c}{Ours}\\ \cline{2-4}\cline{6-8}\cline{10-12}
                            & PSNR$\uparrow$ & SSIM$\uparrow$ & ATE$\downarrow$ & & PSNR & SSIM & ATE   & & PSNR & SSIM & ATE  \\ \hline
    classroom      & 24.08 & 0.78  & 0.0179 & & 33.27 & 0.87  & 0.0110 & & 36.26 & 0.95  & 0.0088\\
    lego\_c2       & 17.60 & 0.35  & 0.0110 & & 25.04 & 0.71  & 0.0006 & & 29.36 & 0.90  & 0.0005\\
    livingroom     & 20.82 & 0.62  & 0.0026 & & 30.74 & 0.63  & 0.0014 & & 33.52 & 0.88  & 0.0009\\
    bedroom        & 20.22 & 0.50  & 0.0208 & & 29.75 & 0.91  & 0.0002 & & 31.17 & 0.93  & 0.0002 \\    \hline
    mean           & 26.10 & 0.73  & 0.0119 & & 28.38 & 0.80  & 0.0042      & & \textbf{29.84} & \textbf{0.87} & \textbf{0.0038} \\
    \hline
    \end{tabular}
    }
    \caption{\textbf{Effectiveness of 2D Track Loss and Scale Loss.} Performance on both novel view synthesis and camera pose estimation.\TODO{combine with Tab.~\ref{table:ablation-track3d-loss}}}
    \label{table:ablation-trackloss-scaleloss}
\end{table}
\fi

\begin{table}[]
    \centering
    \resizebox{\linewidth}{!}{
        \begin{tabular}{c l c c c c c}\hline
             ID & Variant       & PSNR$\uparrow$ & SSIM$\uparrow$  & LPIPS$\downarrow$ & ATE$\downarrow$ & FoV$\downarrow$ \\ \hline
             1  & COLMAP + 3DGS & 32.26          & 0.91            & 0.18              & 0.00020         & 0.271 \\ 
             2  & CF-3DGS       & 17.30          & 0.55            & 0.51              & 0.08036         & 0.271 \\ \hline
             3  & w.o. 2D track & 18.18	        & 0.56            & 0.47              & 0.02020         & 2.617 \\
             4  & w.o. 3D track & 32.38          & 0.91            & 0.17              & 0.00275         & 0.063 \\
             5  & w.o. scale    & 29.70          & 0.83            & 0.18              & 0.00738         & 0.069 \\
             6  & Ours          & \cellcolor{gray!40}32.58          & \cellcolor{gray!40}0.92            & \cellcolor{gray!40}0.16              & \cellcolor{gray!40}0.00018         & \cellcolor{gray!40}0.015 \\ \hline
        \end{tabular}
    }
    \caption{\textbf{Ablation study on different losses.}}
    \label{tab:ablation-all}
\end{table}

\begin{table}[]
    \centering
    \resizebox{0.75\linewidth}{!}{
        \begin{tabular}{lcccc}
        \hline
                    & {classroom}  & {lego\_c2}   & {livingroom} & {bedroom} \\ \hline
         w. scale   & 7.97$e^{-5}$ & 3.48$e^{-3}$ & 9.03$e^{-4}$ & 3.08$e^{-3}$ \\ 
         w.o. scale & 1.26$e^{-4}$ & 8.56$e^{-3}$ & 3.10$e^{-2}$ & 2.98$e^{-2}$ \\ \hline
    \end{tabular}
    }
    \caption{\textbf{Effectiveness of scale loss in regulating Gaussian sizes.}}
    \label{tab:ablation-gaussian-size}
\end{table}

\begin{table}[!t]
    \centering
    \resizebox{\linewidth}{!}{
    \begin{tabular}{c ccc c ccc c ccc}\hline
    \multirow{2}{*}{scenes}  & \multicolumn{3}{c}{w.o. 2D\&3D track}  & & \multicolumn{3}{c}{w.o. scale}     & & \multicolumn{3}{c}{Ours}\\ \cline{2-4}\cline{6-8}\cline{10-12}
                            & PSNR$\uparrow$ & SSIM$\uparrow$ & ATE$\downarrow$ & & PSNR & SSIM & ATE   & & PSNR & SSIM & ATE  \\ \hline
    34\_1403\_4393      & 28.23 & 0.86  & 0.0179        & & 28.80 & 0.87  & 0.0110      & & 28.68 & 0.88  & 0.0088\\
    46\_2587\_7531      & 30.10 & 0.90  & 0.0110        & & 31.64 & 0.91  & 0.0006      & & 31.83 & 0.92  & 0.0005\\
    106\_12648\_23157   & 19.21 & 0.47  & 0.0026        & & 21.97 & 0.63  & 0.0014      & & 26.18 & 0.83  & 0.0009\\
    110\_13051\_23361   & 25.13 & 0.73  & 0.0208        & & 33.04 & 0.94  & 0.0002      & & 33.44 & 0.94  & 0.0002 \\
    245\_26182\_52130   & 30.38 & 0.80  & 0.0099        & & 34.00 & 0.93  & 0.0001      & & 33.82 & 0.93  & 0.0001 \\
    407\_54965\_106262  & 27.35 & 0.83  & 0.0180        & & 29.20 & 0.86  & 0.0090      & & 28.73 & 0.86  & 0.0100\\
    415\_57112\_110099  & 23.79 & 0.62  & 0.0041        & & 23.49 & 0.61  & 0.0014      & & 30.37 & 0.88  & 0.0010 \\
    429\_60388\_117059  & 24.57 & 0.62  & 0.0108        & & 24.91 & 0.65  & 0.0096      & & 25.70 & 0.70  & 0.0092 \\    \hline
    mean                & 26.10 & 0.73  & 0.0119        & & 28.38 & 0.80  & 0.0042      & & \cellcolor{gray!40}29.84 & \cellcolor{gray!40}0.87 & \cellcolor{gray!40}0.0038 \\
    \hline
    \end{tabular}
    }
    \caption{\textbf{Effectiveness of track losses and scale loss.} Performance on both novel view synthesis and camera pose estimation.}
    \label{table:ablation-trackloss-scaleloss}
\end{table}

\iffalse
\begin{table}[!t]
    \centering
    \resizebox{\linewidth}{!}{
        \begin{tabular}{c cccc cccc cccc cccc} \hline
        \multirow{2}{*}{Scenes} & \multicolumn{3}{c}{Ours} & & \multicolumn{3}{c}{w.o. 3D Track} & & \multicolumn{3}{c}{COLMAP-3DGS} & & \multicolumn{3}{c}{GT+3DGS}\\ \cline{2-4}\cline{6-8}\cline{10-12}\cline{14-16}
                                & PSNR$\uparrow$ & SSIM$\uparrow$ & LPIPS$\downarrow$ & & PSNR & SSIM & LPIPS & & PSNR & SSIM & LPIPS & & PSNR & SSIM & LPIPS \\ \hline
        classroom    & 36.26 & 0.95 & 0.13 && 35.99 & 0.94 & 0.14 & & 35.81 & 0.94 & 0.15 & & \textbf{36.27} & 0.94 & 0.15 \\
        {lego\_c2}   & \textbf{29.36} & 0.90 & 0.13 && 29.33 & 0.90 & 0.13 & & 28.77 & 0.88 & 0.15 & & 29.28 & 0.89 & 0.15 \\
        livingroom  & 33.52 & 0.88 & 0.24 && 33.24 & 0.88 & 0.26 & & 32.74 & 0.87 & 0.27   & & \textbf{33.87} & 0.88 & 0.27 \\
        bedroom  & 31.17 & 0.93 & 0.13 && 30.97 & 0.93 & 0.14 & & 31.73 & 0.94 & 0.13 & & \textbf{33.38} & 0.95 & 0.12 \\   \hline
        \end{tabular}
    }
    \caption{\textbf{Effectiveness of 3D Track Loss.} Performance on novel view synthesis on the synthetic dataset.\TODO{add ATE, combine with Tab.~\ref{table:ablation-trackloss-scaleloss}}}
    \label{table:ablation-track3d-loss}
\end{table}
\fi

\begin{table}[!t]
    \centering
    \resizebox{0.75\linewidth}{!}{
    \begin{tabular}{c ccc c ccc}\hline
    \multirow{2}{*}{scenes}  & \multicolumn{3}{c}{Ours}  & & \multicolumn{3}{c}{COLMAP+3DGS}\\ \cline{2-4}\cline{6-8}
                            & PSNR$\uparrow$ & SSIM$\uparrow$ & LPIPS$\downarrow$ & & PSNR & SSIM & LPIPS \\ \hline
    Church  & 29.39 & 0.92 & 0.10 & & 29.93 & 0.93 & 0.09  \\
    Barn    & 31.98 & 0.94 & 0.08 & & 31.08 & 0.95 & 0.07  \\
    Museum  & 31.92 & 0.94 & 0.08 & & 34.47 & 0.96 & 0.05  \\
    Family  & 32.22 & 0.95 & 0.08 & & 27.93 & 0.92 & 0.11  \\
    Horse   & 30.33 & 0.94 & 0.09 & & 20.91 & 0.77 & 0.23 \\
    Ballroom & 35.03 & 0.97 & 0.03 & & 34.48 & 0.96 & 0.04 \\
    Francis & 33.39 & 0.92 & 0.15 & & 32.64 & 0.92 & 0.15 \\
    Ignatius & 29.25 & 0.90 & 0.11 & & 30.20 & 0.93 & 0.08 \\
    \hline
    mean & \cellcolor{gray!40}31.68 & \cellcolor{gray!40}0.94 & \cellcolor{gray!40}0.09 & & 30.20 & 0.92 & 0.10 \\
    \hline
    \end{tabular}
    }
    \caption{\textbf{Comparison to 3DGS trained with COLMAP poses.}}
    \label{table:ablation-colmap}
\end{table}


% \begin{table}[!t]
%     \centering
%     \resizebox{\linewidth}{!}{
%     \begin{tabular}{c ccc c ccc}\hline
%     \multirow{2}{*}{scenes}  & \multicolumn{3}{c}{w.o. Scale}  & & \multicolumn{3}{c}{Ours}\\ \cline{2-4}\cline{6-8}
%                             & PSNR$\uparrow$ & SSIM$\uparrow$ & ATE$\downarrow$ & & PSNR & SSIM & ATE  \\ \hline
%     34\_1403\_4393      & 28.80 & 0.87  & 0.0110       & & 28.68 & 0.88  & 0.0088\\
%     46\_2587\_7531      & 31.64 & 0.91  & 0.0006       & & 31.83 & 0.92  & 0.0005\\
%     106\_12648\_23157   & 21.97 & 0.63  & 0.0014       & & 26.18 & 0.83  & 0.0009\\
%     110\_13051\_23361   & 33.04 & 0.94  & 0.0002       & & 33.44 & 0.94  & 0.0002 \\
%     245\_26182\_52130   & 34.00 & 0.93  & 0.0001       & & 33.82 & 0.93  & 0.0001 \\
%     407\_54965\_106262  & 29.20 & 0.86  & 0.0090       & & 28.73 & 0.86  & 0.0100\\
%     415\_57112\_110099  & 23.49 & 0.61  & 0.0014       & & 30.37 & 0.88  & 0.0010 \\
%     429\_60388\_117059  & 24.91 & 0.65  & 0.0096       & & 25.70 & 0.70  & 0.0092 \\    \hline
%     mean                & 28.38 & 0.80  & 0.0042       & & \textbf{29.84} & \textbf{0.87} & \textbf{0.0038}\\
%     \hline
%     \end{tabular}
%     }
%     \caption{\textbf{Effectiveness of Scale loss.} Performance on both novel view synthesis and camera pose estimation. The best results are highlighted in bold.}
%     \label{table:ablation-scaleloss}
% \end{table}

\textbf{Effectiveness of Intrinsic Optimization.}
Accurate camera intrinsics resolve scale ambiguity in 3DGS models, leading to improved novel view synthesis performance. As shown in Tab.~\ref{table:pose-Synthetic}, our method produces more accurate camera intrinsics (i.e., FoV) compared to COLMAP with original 3DGS. We also performed an ablation study with a fixed camera FoV of $60^\circ$ and without further optimization. The results, shown in Tab.~\ref{tab:ablation-intrinsic}, indicate a 21.4\% average decrease in PSNR, due to inaccurate camera intrinsics affecting pose estimation and introducing scale ambiguity.
\begin{table}[!t]
    \centering
     \resizebox{\linewidth}{!}{
    \begin{tabular}{c|cccc|c}
        \hline
            & {classroom} & {lego\_c2} & {livingroom} & {bedroom} & mean\\
        \hline
        Ours & 36.26 & 29.36 & 33.52 & 31.17 & \cellcolor{gray!40}32.58 \\
        COLMAP+3DGS & 35.81 & 28.77 & 32.74 & 31.73 & 32.26 \\
        Ours with fixed FoV (60\degree) & 34.76 & 20.96 & 26.24 & 21.20 & 25.79\\
        \hline
    \end{tabular}
    }
    \caption{\textbf{Effectiveness of intrinsic optimization.}}
    \label{tab:ablation-intrinsic}
\end{table}


\textbf{Comparison with COLMAP-Assisted 3DGS.}
We compare the synthesis quality from novel views generated by our method against the original 3DGS, where the camera intrinsics and extrinsics are estimated using COLMAP on the Tanks and Temples dataset. Tab.~\ref{table:ablation-colmap} shows that our method achieves results that slightly outperform the 3DGS model trained with COLMAP-assisted poses across all scenes. Unlike the original 3DGS, which uses a fixed camera pose for training, our method seamlessly integrates 3DGS training with camera parameter estimation, allowing the two tasks to complement each other and ultimately achieve high-quality novel view synthesis. Besides, on a sequence with low-texture areas, COLMAP fails to estimate correct poses, which results in artifacts as shown in Fig.~\ref{fig:render-Synthetic}.
%We further analyze the novel view synthesis quality of the 3dgs trained with our learned parameters compared to COLMAP parameters on the Tanks and Temples dataset. This evaluation allows us to indirectly compare the accuracy of the estimated parameters based on their impact on novel view synthesis. Specifically, in Tab.~\ref{table:ablation-colmap}, we achieve results that are comparable to COLMAP across all scenes, and in some cases, even outperform COLMAP. The main reason for this, as we analyzed, is that COLMAP uses traditional algorithms for parameter estimation, which do not perform well in challenging scenarios. In contrast, our proposed joint learning method seamlessly integrating with 3DGS and camera parameter optimization,not only ensures better rendering result quality but also enhances the accuracy of camera parameter estimation.

\begin{figure}[!t]
  \centering
  % \fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
  % \includegraphics[width=\linewidth]{figures/scgs-colmap-fail.png}
  \includegraphics[width=\linewidth]{figures/scgs-colmap-fail-lowRes.png}
   \caption{\textbf{COLMAP failure case on Synthetic dataset.}}
   \label{fig:render-Synthetic}
\end{figure}

\begin{figure}[!t]
  \centering
  % \includegraphics[width=\linewidth]{figures/scgs-track-loss.png}
  \includegraphics[width=\linewidth]{figures/scgs-track-loss-lowRes.png}
   %\caption{\textbf{Novel view synthesis and pose accuracy on CO3D-V2 for ablation of track losses.} The result without track loss appears blurry in novel view synthesis.}
   \caption{\textbf{Visualization of ablation study on track losses in CO3D-V2.} The result without track loss appears blurry in novel view synthesis.}
   \label{fig:ablation-wotrack}
\end{figure}

% \begin{table}[!t]
%     \centering
%     \resizebox{\linewidth}{!}{
%     \begin{tabular}{c cccc ccc} \hline
%     \multirow{2}{*}{Scenes} & \multicolumn{3}{c}{Ours} & & \multicolumn{3}{c}{COLMAP+3DGS}  \\ \cline{2-4}\cline{6-8}
%                             & PSNR$\uparrow$ & SSIM$\uparrow$ & LPIPS$\downarrow$  & & PSNR$\uparrow$ & SSIM$\uparrow$ & LPIPS$\downarrow$  \\ \hline
%     classroom               & \textbf{36.8865} & \textbf{0.9466} & \textbf{0.138} & & 36.46095 & 0.942800 & 0.147312    \\
%     bedroom                  & & &   & & & &    \\
%     scene1                  & & &   & & & &    \\
%     scene2                  & & &   & & & &    \\
%     mean                    & & &   & & & &    \\ \hline
%     \end{tabular}
%     }
%     \caption{\textbf{Qualitative comparison of novel view synthesis on our Synthetic dataset with COLMAP+3DGS.} }
%     \label{table:ablation-render-colmap}
% \end{table}

% % TODO:focalæ€Žä¹ˆæ¯”è¾ƒ
% \begin{table}[!t]
%     \centering
%     \resizebox{\linewidth}{!}{
%     \begin{tabular}{c cccc ccc} \hline
%     \multirow{2}{*}{Scenes} & \multicolumn{3}{c}{Ours} & & \multicolumn{3}{c}{COLMAP}  \\ \cline{2-4}\cline{6-8}
%                             & $\mathrm{RPE}_{t}\downarrow$ & $\mathrm{RPE}_{r}\downarrow$ & ATE$\downarrow$ & & $\mathrm{RPE}_{t}$ & $\mathrm{RPE}_{r}$ & ATE \\ \hline
%     classroom                & \textbf{0.0023} & \textbf{0.0136} & 0.00011 &  & 0.0042 & 0.0182 & \textbf{0.00007}   \\
%     bedroom                  & & &   & & & &    \\
%     scene1                  & & &   & & & &    \\
%     scene2                  & & &   & & & &    \\
%     mean                    & & &   & & & &    \\ \hline
%     \end{tabular}
%     }
%     \caption{\textbf{Qualitative comparison of parameter accuracy on our Synthetic dataset with COLMAP.} }
%     \label{table:ablation-pose-colmap}
% \end{table}

% \begin{table*}[!t]
%     \centering
%     \resizebox{\linewidth}{!}{
%     \begin{tabular}{c cccc cccc cccc ccc} \hline
%     \multirow{2}{*}{Scenes} & \multicolumn{3}{c}{Ours} & & \multicolumn{3}{c}{CG-3DGS} & & \multicolumn{3}{c}{CF-3DGS} & & \multicolumn{3}{c}{NoPe-NeRF}\\ \cline{2-4}\cline{6-8}\cline{10-12}\cline{14-16}
%                             & PSNR$\uparrow$ & SSIM$\uparrow$ & LPIPS$\downarrow$ & & PSNR & SSIM & LPIPS & & PSNR & SSIM & LPIPS & & PSNR & SSIM & LPIPS \\ \hline
%     Church                  & 29.13& 0.91& 0.11 &  & \textbf{32.14} & 0.96 & 0.08    &  & 30.23 & 0.93 & 0.11    & & 25.17 & 0.73 & 0.39    \\
%     Barn                    & 31.43& 0.94& 0.09 &  & \textbf{33.19} & 0.94 & 0.07    &  & 31.23 & 0.90 & 0.10    & & 26.35 & 0.69 & 0.44    \\
%     Museum                  & 30.88& 0.94& 0.08 &  & \textbf{31.62} & 0.94 & 0.08    &  & 29.91 & 0.91 & 0.11    & & 26.77 & 0.76 & 0.35    \\
%     Family                  & 31.88& 0.94& 0.09 &  & \textbf{34.80} & 0.97 & 0.04    &  & 31.27 & 0.94 & 0.07    & & 26.01 & 0.74 & 0.41    \\
%     Horse                   & 30.27& 0.94& 0.08 &  & \textbf{35.45} & 0.97 & 0.04    &  & 33.94 & 0.96 & 0.05    & & 27.64 & 0.84 & 0.26    \\
%     Ballroom                & 34.57& 0.97& 0.03 &  & 33.91 & 0.97 & 0.04    &  & 32.47 & 0.96 & 0.07    & & 25.33 & 0.72 & 0.38    \\
%     Francis                 & 33.14& 0.92& 0.15 &  & \textbf{33.80} & 0.92 & 0.13    &  & 32.72 & 0.91 & 0.14    & & 29.48 & 0.80 & 0.38    \\
%     Ignatius                & 29.28& 0.9& 0.11 &  & \textbf{31.14} & 0.94 & 0.06    &  & 28.43 & 0.90 & 0.09    & & 23.96 & 0.61 & 0.47    \\ \hline
%     mean                    & 31.32& 0.93& 0.09 &  & \textbf{33.26} & \textbf{0.95} & \textbf{0.07}    &  & 31.28 & 0.93 & 0.09    & & 26.34 & 0.74 & 0.39    \\ \hline
%     \end{tabular}
%     }
%     \caption{\textbf{Qualitative comparison of novel view synthesis on Tanks and Temples.} Each baseline method is trained with its public code under the original settings and evaluated with the same evaluation protocol. The best results are highlighted in bold.}
%     \label{table:render-T&T}
% \end{table*}

% \begin{table*}[!t]
%     \centering
%     \resizebox{\linewidth}{!}{
%     \begin{tabular}{c cccc cccc cccc ccc} \hline
%     \multirow{2}{*}{Scenes} & \multicolumn{3}{c}{Ours} & & \multicolumn{3}{c}{CG-3DGS} & & \multicolumn{3}{c}{CF-3DGS} & & \multicolumn{3}{c}{NoPe-NeRF}\\ \cline{2-4}\cline{6-8}\cline{10-12}\cline{14-16}
%                             & $\mathrm{RPE}_{t}\downarrow$ & $\mathrm{RPE}_{r}\downarrow$ & ATE$\downarrow$ & & $\mathrm{RPE}_{t}$ & $\mathrm{RPE}_{r}$ & ATE & & $\mathrm{RPE}_{t}$ & $\mathrm{RPE}_{r}$ & ATE & & $\mathrm{RPE}_{t}$ & $\mathrm{RPE}_{r}$ & ATE \\ \hline
%     Church                  & 0.0071& 0.0126& 0.0023 & & \textbf{0.006} & 0.016 & \textbf{0.001}   & & 0.008 & 0.018 & 0.002    & & 0.034 & \textbf{0.008} & 0.008      \\
%     Barn                    & 0.0071& 0.0208& 0.0013 & & 0.029 & 0.030 & 0.002   & & 0.034 & 0.034 & 0.003    & & 0.046 & 0.032 & 0.004      \\
%     Museum                  & 0.0291& 0.1154& 0.003  & & 0.047 & 0.203 & 0.004   & & 0.052 & 0.215 & 0.005    & & 0.207 & 0.202 & 0.020      \\
%     Family                  & 0.0125& 0.021& 0.0017  & & 0.024 & 0.020 & \textbf{0.001}   & & 0.022 & 0.024 & 0.002    & & 0.047 & \textbf{0.015} & \textbf{0.001}      \\
%     Horse                   & 0.0822& 0.0361& 0.0016 & & 0.109 & 0.053 & 0.003   & & 0.112 & 0.057 & 0.003    & & 0.179 & \textbf{0.017} & 0.003      \\
%     Ballroom                & 0.0139& 0.0127& 0.0003 & & 0.033 & 0.020 & 0.003   & & 0.037 & 0.024 & 0.003    & & 0.041 & 0.018 & 0.002      \\
%     Francis                 & 0.0102& 0.0844& 0.0029 & & 0.026 & 0.147 & 0.005   & & 0.029 & 0.154 & 0.006    & & 0.057 & \textbf{0.009} & 0.005      \\
%     Ignatius                & 0.0195& 0.0307& 0.0032 & & 0.027 & 0.012 & 0.003   & & 0.033 & 0.032 & 0.005    & & 0.026 & \textbf{0.005} & \textbf{0.002}  \\ \hline
%     mean                    & 0.0227& 0.0417125& 0.0020375 & & 0.037 & 0.063 & 0.003   & & 0.041 & 0.069 & 0.004    & & 0.080 & \textbf{0.038} & 0.006  \\ \hline
%     \end{tabular}
%     }
%     \caption{\textbf{Qualitative comparison of pose accuracy on Tanks and Temples.} Note that, we use COLMAP poses as the ground truth. The unit of $RPE_{r}$ is in degrees, ATE is in the ground truth scale and $RPE_{t}$ is scaled by 100. The best results are highlighted in bold.}
%     \label{table:pose-T&T}
% \end{table*}

% \begin{table*}[!t]
%     \centering
%     \resizebox{\textwidth}{!}
%     {
%     \begin{tabular}{l cccc cccc cccc cccc ccc} \hline
%     \multirow{2}{*}{Methods} & \multicolumn{3}{c}{110\_13051\_23361} & & \multicolumn{3}{c}{415\_57112\_110099} & & \multicolumn{3}{c}{106\_12648\_23157} & & \multicolumn{3}{c}{245\_26182\_52130} & & \multicolumn{3}{c}{34\_1403\_4393} \\ \cline{2-4}\cline{6-8}\cline{10-12}\cline{14-16}\cline{18-20}
%                 & PSNR$\uparrow$ & SSIM$\uparrow$ & LPIPS$\downarrow$   & &  PSNR & SSIM & LPIPS   & &  PSNR & SSIM & LPIPS  & &  PSNR & SSIM & LPIPS   & &  PSNR & SSIM & LPIPS  \\   \hline
%     NoPe-NeRF   & 26.86 & 0.73 & 0.47  & & 24.78 & 0.64 & 0.55    & & 20.41 & 0.46 & 0.58     & & 25.05 & 0.80 & 0.49     & & 28.62 & 0.80 & 0.35\\
%     CF-3DGS     & 29.69 & 0.89 & 0.29  & & 26.21 & 0.73 & 0.32    & & 22.14 & 0.64 & 0.34     & & 27.24 & 0.85 & 0.30     & & 27.75 & 0.86 & \textbf{0.20}\\
%     Ours        & \textbf{33.44} & \textbf{0.94} & \textbf{0.11}  & & \textbf{30.37} & \textbf{0.88} & \textbf{0.22}    & & \textbf{26.18} & \textbf{0.83} & \textbf{0.19}     & & \textbf{33.82} & \textbf{0.93} & \textbf{0.20}     & & \textbf{28.68} & \textbf{0.88} & 0.21\\ \hline
%     \end{tabular}
%     }
%     \caption{\textbf{Comparison of novel view synthesis on CO3D-V2.}}
%     \label{table:render-co3dv2}
% \end{table*}
% \begin{table*}[!t]
%     \centering
%     \resizebox{\textwidth}{!}
%     {
%     \begin{tabular}{l cccc cccc cccc cccc ccc} \hline
%     \multirow{2}{*}{Methods} & \multicolumn{3}{c}{110\_13051\_23361} & & \multicolumn{3}{c}{415\_57112\_110099} & & \multicolumn{3}{c}{106\_12648\_23157} & & \multicolumn{3}{c}{245\_26182\_52130} & & \multicolumn{3}{c}{34\_1403\_4393} \\ \cline{2-4}\cline{6-8}\cline{10-12}\cline{14-16}\cline{18-20}
%                 & $\mathrm{RPE}_{t}\downarrow$ & $\mathrm{RPE}_{r}\downarrow$ & ATE$\downarrow$   & & $\mathrm{RPE}_{t}$ & $\mathrm{RPE}_{r}$ & ATE  & & $\mathrm{RPE}_{t}$ & $\mathrm{RPE}_{r}$ & ATE & & $\mathrm{RPE}_{t}$ & $\mathrm{RPE}_{r}$ & ATE  & & $\mathrm{RPE}_{t}$ & $\mathrm{RPE}_{r}$ & ATE  \\   \hline
%     NoPe-NeRF   & 0.400 & 1.966 & 0.046   & & 0.326 & 1.919 & 0.054    & & 0.387 & 1.312 & 0.049     & & 0.587 & 1.867 & 0.038     & & 0.591 & 1.313 & 0.053\\
%     CF-3DGS     & 0.140 & 0.401 & 0.021   & & 0.110 & 0.424 & 0.014    & & 0.094 & 0.360 & 0.008     & & 0.239 & 0.472 & 0.017     & & 0.505 & \textbf{0.211} & 0.009\\
%     Ours        & \textbf{0.012} & \textbf{0.052} & \textbf{0.0002}  & & \textbf{0.004} & \textbf{0.024} & \textbf{0.0001}   & & \textbf{0.009} & \textbf{0.076} & \textbf{0.0009}    & & \textbf{0.005} & \textbf{0.029} & \textbf{0.0001}    & & \textbf{0.099} & 0.605 & \textbf{0.0088} \\ \hline
%     \end{tabular}
%     }
%     \caption{\textbf{Comparison of pose estimation on CO3D-V2.}}
%     \label{table:pose-co3dv2}
% \end{table*}

% \begin{table}[!t]
%     \centering
%     \resizebox{\linewidth}{!}{
%     \begin{tabular}{c ccccccc c ccccccc}\hline
%     \multirow{2}{*}{scenes}  & \multicolumn{7}{c}{w.o. Track}  & & \multicolumn{7}{c}{Ours}\\ \cline{2-4}\cline{6-8}\cline{10-12}\cline{14-16}
%                             & PSNR$\uparrow$ & SSIM$\uparrow$ & LPIPS$\downarrow$ & & $RPE_t\downarrow$ & $RPE_r\downarrow$ & ATE$\downarrow$ & & PSNR & SSIM & LPIPS & & $RPE_t$ & $RPE_r$ & ATE  \\ \hline
%     Church  & 28.78 & 0.91 & 0.11 & &0.007 &0.016 &0.002 & & 29.24 & 0.91 & 0.11 & & 0.007 & 0.013 & 0.002  \\
%     Barn    & 29.82 & 0.88 & 0.11 & & 0.036 &0.047 &0.002 & &31.93 & 0.94 & 0.08 & & 0.007 & 0.021 & 0.001  \\
%     Museum  & 29.18 & 0.91 & 0.10  & &0.032 &0.164 &0.005 & & 30.92 & 0.93 & 0.08 & &0.029 & 0.115 & 0.003  \\
%     Family  & 30.83 & 0.92 & 0.12 & &0.016 &0.019 &0.003 & & 31.85 & 0.94 & 0.09 & & 0.013 & 0.019 & 0.002  \\
%     Horse  & 30.64 & 0.94 & 0.09 & &0.037 &0.038 &0.002 & & 29.88 & 0.94 & 0.09 & & 0.082 & 0.038 & 0.002  \\
%     Ballroom & 33.47 & 0.96 &0.04 & &0.017 &0.016 &0.001 & & 34.71 & 0.97 & 0.04 & & 0.013 & 0.014 & 0.001 \\
%     Francis & 32.73 & 0.91 & 0.16 & & 0.013 & 0.102 & 0.004 & & 33.13 & 0.92 & 0.15 & & 0.010 & 0.084 & 0.003 \\
%     Ignatius & 27.20 & 0.88 & 0.12 & & 0.020 & 0.028 & 0.003 & & 29.08 & 0.90 & 0.11 & & 0.020 & 0.031 & 0.003 \\
%     \hline
%     mean & 30.33 & 0.91 & 0.11 & & \textbf{0.022} & 0.054 & 0.003 & & \textbf{31.34} & \textbf{0.93} & \textbf{0.09} & & 0.023 & \textbf{0.042} & \textbf{0.002} \\
%     \hline
%     \end{tabular}
%     }
%     \caption{\textbf{Effectiveness of Track loss on Tanks.}}
%     \label{table:ablation-trackloss-tt}
% \end{table}

% \begin{table}[!t]
%     \centering
%     \resizebox{\linewidth}{!}{
%     \begin{tabular}{c ccccccc c ccccccc}\hline
%     \multirow{2}{*}{scenes}  & \multicolumn{7}{c}{w.o. Track}  & & \multicolumn{7}{c}{Ours}\\ \cline{2-4}\cline{6-8}\cline{10-12}\cline{14-16}
%                             & PSNR$\uparrow$ & SSIM$\uparrow$ & LPIPS$\downarrow$ & & $RPE_t\downarrow$ & $RPE_r\downarrow$ & ATE$\downarrow$ & & PSNR & SSIM & LPIPS & & $RPE_t$ & $RPE_r$ & ATE  \\ \hline
%     34\_1403\_4393 & 28.23 & 0.86 & 0.20 & & 0.138 & 0.530 & 0.0179 & & 28.68 & 0.88 & 0.21 & & 0.099 & 0.605 & 0.0088\\
%     46\_2587\_7531 & 30.10 & 0.90 & 0.15 && 0.028 & 0.220 & 0.0110 && 31.83 & 0.92 & 0.12 && 0.013 & 0.080 & 0.0005\\
%     106\_12648\_23157 & 19.21 & 0.47 &0.35 & & 0.047 & 0.264 & 0.0026 & &
%     26.18 & 0.83 & 0.19 & & 0.009 & 0.076 & 0.0009\\
%     110\_13051\_23361 & 25.13& 0.73 & 0.26 & & 0.221 & 0.815 & 0.0208 & & 33.44 & 0.94 & 0.11 & & 0.012 & 0.052 & 0.0002 \\
%     245\_26182\_52130 & 30.38 & 0.8 & 0.28 & & 0.066 & 0.625 & 0.0099 & & 33.82 & 0.93 & 0.20 & & 0.005 & 0.029 & 0.0001 \\
%     407\_54965\_106262 & 27.35 & 0.83 & 0.42 & & 0.325 & 1.056 & 0.0180 && 28.73 & 0.86 & 0.39 & & 0.062 & 0.461 & 0.010\\
%     415\_57112\_110099 & 23.79 & 0.62 & 0.37 & & 0.069 & 0.171 & 0.0041 & & 30.37 & 0.88 & 0.22 & & 0.004 & 0.024 & 0.001 \\
%     429\_60388\_117059 & 24.57 &0.62 & 0.39 && 0.055 & 0.510 & 0.0108 && 25.70 & 0.70 & 0.35 && 0.052 & 0.454 & 0.0092 \\
%     \hline
%     mean & 26.10 & 0.73 & 0.30 & & 0.120 & 0.524 & 0.0119 & & \textbf{29.84} & \textbf{0.87} & \textbf{0.22} & & \textbf{0.032} & \textbf{0.12} & \textbf{0.0038} \\
%     \hline
%     \end{tabular}
%     }
%     \caption{\textbf{Effectiveness of Track loss.}}
%     \label{table:ablation-trackloss}
% \end{table}

% \begin{table}[!t]
%     \centering
%     \resizebox{\linewidth}{!}{
%     \begin{tabular}{c ccccccc c ccccccc}\hline
%     \multirow{2}{*}{scenes}  & \multicolumn{7}{c}{w.o. Scale}  & & \multicolumn{7}{c}{Ours}\\ \cline{2-4}\cline{6-8}\cline{10-12}\cline{14-16}
%                             & PSNR$\uparrow$ & SSIM$\uparrow$ & LPIPS$\downarrow$ & & $RPE_t\downarrow$ & $RPE_r\downarrow$ & ATE$\downarrow$ & & PSNR & SSIM & LPIPS & & $RPE_t$ & $RPE_r$ & ATE  \\ \hline
%     34\_1403\_4393 & 28.80 & 0.87 & 0.21 & & 0.127 & 0.607 & 0.0110 & & 28.68 & 0.88 & 0.21 & & 0.099 & 0.605 & 0.0088\\
%     46\_2587\_7531 & 31.64 & 0.91 & 0.12 && 0.014 & 0.086 & 0.0006 && 31.83 & 0.92 & 0.12 && 0.013 & 0.080 & 0.0005\\
%     106\_12648\_23157 & 21.97 & 0.63 & 0.24 & & 0.051 & 0.173 & 0.0014 & &
%     26.18 & 0.83 & 0.19 & & 0.009 & 0.076 & 0.0009\\
%     110\_13051\_23361 & 33.04 & 0.94 & 0.11 & & 0.012 & 0.052 & 0.0002 & & 33.44 & 0.94 & 0.11 & & 0.012 & 0.052 & 0.0002 \\
%     245\_26182\_52130 & 34.00 & 0.93 & 0.18 & & 0.006 & 0.034 & 0.0001 & & 33.82 & 0.93 & 0.20 & & 0.005 & 0.029 & 0.0001 \\
%     407\_54965\_106262 & 29.20 & 0.86 & 0.38 & & 0.065 & 0.419 & 0.0090 && 28.73 & 0.86 & 0.39 & & 0.062 & 0.461 & 0.010\\
%     415\_57112\_110099 & 23.49 & 0.61 & 0.31 & &0.071 & 0.226 & 0.0014 & & 30.37 & 0.88 & 0.22 & & 0.004 & 0.024 & 0.001 \\
%     429\_60388\_117059 & 24.91 &0.65 & 0.35 && 0.060 & 0.503 & 0.0096 && 25.70 & 0.70 & 0.35 && 0.052 & 0.454 & 0.0092 \\
%     \hline
%     mean & 28.38 & 0.80 & 0.24 & & 0.051 & 0.262 & 0.0042 & & \textbf{29.84} & \textbf{0.87} & \textbf{0.22} & & \textbf{0.032} & \textbf{0.12} & \textbf{0.0038}\\
%     \hline
%     \end{tabular}
%     }
%     \caption{\textbf{Effectiveness of Scale loss.}}
%     \label{table:ablation-scaleloss}
% \end{table}
