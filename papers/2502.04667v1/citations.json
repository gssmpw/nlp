[
  {
    "index": 0,
    "papers": [
      {
        "key": "vaswani2017attention",
        "author": "Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, \\L ukasz and Polosukhin, Illia",
        "title": "Attention is All you Need"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "wei2022chain",
        "author": "Jason Wei and Xuezhi Wang and Dale Schuurmans and Maarten Bosma and brian ichter and Fei Xia and Ed H. Chi and Quoc V Le and Denny Zhou",
        "title": "Chain of Thought Prompting Elicits Reasoning in Large Language Models"
      },
      {
        "key": "zhou2023leasttomost",
        "author": "Denny Zhou and Nathanael Sch{\\\"a}rli and Le Hou and Jason Wei and Nathan Scales and Xuezhi Wang and Dale Schuurmans and Claire Cui and Olivier Bousquet and Quoc V Le and Ed H. Chi",
        "title": "Least-to-Most Prompting Enables Complex Reasoning in Large Language Models"
      },
      {
        "key": "khot2023decomposedpromptingmodularapproach",
        "author": "Tushar Khot and Harsh Trivedi and Matthew Finlayson and Yao Fu and Kyle Richardson and Peter Clark and Ashish Sabharwal",
        "title": "Decomposed Prompting: A Modular Approach for Solving Complex Tasks"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "yue2023mammothbuildingmathgeneralist",
        "author": "Xiang Yue and Xingwei Qu and Ge Zhang and Yao Fu and Wenhao Huang and Huan Sun and Yu Su and Wenhu Chen",
        "title": "MAmmoTH: Building Math Generalist Models through Hybrid Instruction Tuning"
      },
      {
        "key": "yu2024metamath",
        "author": "Longhui Yu and Weisen Jiang and Han Shi and Jincheng YU and Zhengying Liu and Yu Zhang and James Kwok and Zhenguo Li and Adrian Weller and Weiyang Liu",
        "title": "MetaMath: Bootstrap Your Own Mathematical Questions for Large Language Models"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "wang-etal-2024-math",
        "author": "Wang, Peiyi  and\nLi, Lei  and\nShao, Zhihong  and\nXu, Runxin  and\nDai, Damai  and\nLi, Yifei  and\nChen, Deli  and\nWu, Yu  and\nSui, Zhifang",
        "title": "Math-Shepherd: Verify and Reinforce {LLM}s Step-by-step without Human Annotations"
      },
      {
        "key": "havrilla2024teaching",
        "author": "Alexander Havrilla and Yuqing Du and Sharath Chandra Raparthy and Christoforos Nalmpantis and Jane Dwivedi-Yu and Eric Hambro and Sainbayar Sukhbaatar and Roberta Raileanu",
        "title": "Teaching Large Language Models to Reason with Reinforcement Learning"
      },
      {
        "key": "shao2024deepseekmathpushinglimitsmathematical",
        "author": "Zhihong Shao and Peiyi Wang and Qihao Zhu and Runxin Xu and Junxiao Song and Xiao Bi and Haowei Zhang and Mingchuan Zhang and Y. K. Li and Y. Wu and Daya Guo",
        "title": "DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models"
      },
      {
        "key": "yu2024flowreasoningtrainingllmsdivergent",
        "author": "Fangxu Yu and Lai Jiang and Haoqiang Kang and Shibo Hao and Lianhui Qin",
        "title": "Flow of Reasoning:Training LLMs for Divergent Problem Solving with Minimal Examples"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "feng2023towards",
        "author": "Guhao Feng and Bohang Zhang and Yuntian Gu and Haotian Ye and Di He and Liwei Wang",
        "title": "Towards Revealing the Mystery behind Chain of Thought: A Theoretical Perspective"
      },
      {
        "key": "merrill2024expressivepowertransformerschain",
        "author": "William Merrill and Ashish Sabharwal",
        "title": "The Expressive Power of Transformers with Chain of Thought"
      },
      {
        "key": "li2024chain",
        "author": "Zhiyuan Li and Hong Liu and Denny Zhou and Tengyu Ma",
        "title": "Chain of Thought Empowers Transformers to Solve Inherently Serial Problems"
      },
      {
        "key": "prabhakar2024decipheringfactorsinfluencingefficacy",
        "author": "Akshara Prabhakar and Thomas L. Griffiths and R. Thomas McCoy",
        "title": "Deciphering the Factors Influencing the Efficacy of Chain-of-Thought: Probability, Memorization, and Noisy Reasoning"
      },
      {
        "key": "yin2025enhancinggeneralizationchainthought",
        "author": "Maxwell J. Yin and Dingyi Jiang and Yongbing Chen and Boyu Wang and Charles Ling",
        "title": "Enhancing Generalization in Chain of Thought Reasoning for Smaller Models"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "feng2023towards",
        "author": "Guhao Feng and Bohang Zhang and Yuntian Gu and Haotian Ye and Di He and Liwei Wang",
        "title": "Towards Revealing the Mystery behind Chain of Thought: A Theoretical Perspective"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "pmlr-v80-lake18a",
        "author": "Lake, Brenden and Baroni, Marco",
        "title": "Generalization without Systematicity: On the Compositional Skills of Sequence-to-Sequence Recurrent Networks"
      },
      {
        "key": "onoe-etal-2023-lms",
        "author": "Onoe, Yasumasa  and\nZhang, Michael  and\nPadmanabhan, Shankar  and\nDurrett, Greg  and\nChoi, Eunsol",
        "title": "Can {LM}s Learn New Entities from Descriptions? Challenges in Propagating Injected Knowledge"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "zhong-etal-2023-mquake",
        "author": "Zhong, Zexuan  and\nWu, Zhengxuan  and\nManning, Christopher  and\nPotts, Christopher  and\nChen, Danqi",
        "title": "{MQ}u{AKE}: Assessing Knowledge Editing in Language Models via Multi-Hop Questions"
      },
      {
        "key": "10.1162/tacl_a_00644",
        "author": "Cohen, Roi and Biran, Eden and Yoran, Ori and Globerson, Amir and Geva, Mor",
        "title": "Evaluating the Ripple Effects of Knowledge Editing in Language Models"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "stolfo-etal-2023-mechanistic",
        "author": "Stolfo, Alessandro  and\nBelinkov, Yonatan  and\nSachan, Mrinmaya",
        "title": "A Mechanistic Interpretation of Arithmetic Reasoning in Language Models using Causal Mediation Analysis"
      },
      {
        "key": "nanda2023progress",
        "author": "Neel Nanda and Lawrence Chan and Tom Lieberum and Jess Smith and Jacob Steinhardt",
        "title": "Progress measures for grokking via mechanistic interpretability"
      },
      {
        "key": "conmy2023towards",
        "author": "Arthur Conmy and Augustine N. Mavor-Parker and Aengus Lynch and Stefan Heimersheim and Adri{\\`a} Garriga-Alonso",
        "title": "Towards Automated Circuit Discovery for Mechanistic Interpretability"
      },
      {
        "key": "brinkmann-etal-2024-mechanistic",
        "author": "Brinkmann, Jannik  and\nSheshadri, Abhay  and\nLevoso, Victor  and\nSwoboda, Paul  and\nBartelt, Christian",
        "title": "A Mechanistic Analysis of a Transformer Trained on a Symbolic Multi-Step Reasoning Task"
      },
      {
        "key": "li-etal-2024-understanding",
        "author": "Li, Zhaoyi  and\nJiang, Gangwei  and\nXie, Hong  and\nSong, Linqi  and\nLian, Defu  and\nWei, Ying",
        "title": "Understanding and Patching Compositional Reasoning in {LLM}s"
      },
      {
        "key": "rai-yao-2024-investigation",
        "author": "Rai, Daking  and\nYao, Ziyu",
        "title": "An Investigation of Neuron Activation as a Unified Lens to Explain Chain-of-Thought Eliciting Arithmetic Reasoning of {LLM}s"
      },
      {
        "key": "yao2024knowledge",
        "author": "Yunzhi Yao and Ningyu Zhang and Zekun Xi and Mengru Wang and Ziwen Xu and Shumin Deng and Huajun Chen",
        "title": "Knowledge Circuits in Pretrained Transformers"
      },
      {
        "key": "wang2024grokking",
        "author": "Boshi Wang and Xiang Yue and Yu Su and Huan Sun",
        "title": "Grokking of Implicit Reasoning in Transformers: A Mechanistic Journey to the Edge of Generalization"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "yang2024largelanguagemodelslatently",
        "author": "Sohee Yang and Elena Gribovskaya and Nora Kassner and Mor Geva and Sebastian Riedel",
        "title": "Do Large Language Models Latently Perform Multi-Hop Reasoning?"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "biran2024hoppinglateexploringlimitations",
        "author": "Eden Biran and Daniela Gottesman and Sohee Yang and Mor Geva and Amir Globerson",
        "title": "Hopping Too Late: Exploring the Limitations of Large Language Models on Multi-Hop Queries"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "deng2023implicitchainthoughtreasoning",
        "author": "Yuntian Deng and Kiran Prasad and Roland Fernandez and Paul Smolensky and Vishrav Chaudhary and Stuart Shieber",
        "title": "Implicit Chain of Thought Reasoning via Knowledge Distillation"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "deng2024explicitcotimplicitcot",
        "author": "Yuntian Deng and Yejin Choi and Stuart Shieber",
        "title": "From Explicit CoT to Implicit CoT: Learning to Internalize CoT Step by Step"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "giannou2023looped",
        "author": "Angeliki Giannou and Shashank Rajput and Jy-yong Sohn and Kangwook Lee and Jason D Lee and Dimitris Papailiopoulos",
        "title": "Looped Transformers as Programmable Computers"
      },
      {
        "key": "cabannes2024iteration",
        "author": "Vivien Cabannes and Charles Arnal and Wassim Bouaziz and Xingyu Alice Yang and Francois Charton and Julia Kempe",
        "title": "Iteration Head: A Mechanistic Study of Chain-of-Thought"
      },
      {
        "key": "fan2024loopedtransformerslengthgeneralization",
        "author": "Ying Fan and Yilun Du and Kannan Ramchandran and Kangwook Lee",
        "title": "Looped Transformers for Length Generalization"
      }
    ]
  }
]