%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

\usepackage[most]{tcolorbox}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Unveiling the Mechanisms of Explicit CoT Training: How Chain-of-Thought Enhances Reasoning Generalization}

\begin{document}

\twocolumn[
\icmltitle{Unveiling the Mechanisms of Explicit CoT Training: \\How Chain-of-Thought Enhances Reasoning Generalization}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Xinhao Yao}{sch1}
\icmlauthor{Ruifeng Ren}{sch1}
\icmlauthor{Yun Liao}{sch2}
\icmlauthor{Yong Liu}{sch1}
%\icmlauthor{}{sch}
%\icmlauthor{}{sch}
%\icmlauthor{}{sch}
\end{icmlauthorlist}


\icmlaffiliation{sch1}{Gaoling School of Artificial Intelligence, Renmin University of China, Beijing, China}
\icmlaffiliation{sch2}{College of Artificial Intelligence, Tianjin
University of Science and Technology, Tianjin, China}

\icmlcorrespondingauthor{Yong Liu}{liuyonggsai@ruc.edu.cn}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
% \icmlkeywords{Machine Learning, CoT}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
% \printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.
\begin{abstract}
Training large language models (LLMs) with high-quality Chain-of-Thought (CoT) annotations has become a widely adopted strategy due to its significant enhancement of reasoning capabilities. To fully comprehend this approach, two questions naturally arise: (Q1) What advantages does training with CoT offer compared to training without CoT? (Q2) If there are advantages, what are the underlying mechanisms of explicit CoT training? Analyzing the advantages and mechanisms of CoT training is challenging due to the many factors involved. To address this, we conduct a detailed analysis using clear and controllable data distributions and, for the first time, reveal that CoT training offers the following advantages: (1) Training with CoT markedly improves reasoning generalization, extending it from in-distribution (ID) to both ID and out-of-distribution (OOD) scenarios, while also speeding up convergence; (2) Even when training with CoT includes a certain range of erroneous reasoning steps, it still enables the model to learn reasoning patterns, leading to systematic generalization. We further explore the underlying mechanisms from a circuit perspective: (1) The data distribution (e.g., ratio $\lambda$ and pattern) plays a crucial role in influencing the model's systematic generalization; (2) CoT training (with two-hop facts) internalizes reasoning into a two-stage generalizing circuit, where the number of stages corresponds to the explicit reasoning steps during training. Our findings elucidate the mechanisms underlying explicit CoT training and offer critical insights into tuning strategies for LLMs to achieve robust generalization\footnote{The code is available at \url{https://github.com/chen123CtrlS/TCotMechanism}}.
\end{abstract}
\begin{figure}[ht]
\vskip -0.2in
\begin{center}
\begin{minipage}{0.23\textwidth}
    \centering
    \includegraphics[width=\textwidth]{pics/Training_without_CoT.pdf}  % 第一张图
\end{minipage}
\hfill
\begin{minipage}{0.23\textwidth}
    \centering
    \includegraphics[width=\textwidth]{ pics/Training_with_CoT.pdf}  % 第二张图
\end{minipage}
\caption{The model's accuracy on training and testing two-hop reasoning facts during optimization (\textbf{Left}: Training without CoT, \textbf{Right}: Training with CoT). The figure illustrates the accuracy on training data, in-distribution (ID) testing data and out-of-distribution (OOD) testing data as a function of optimization steps. It is clear that by incorporating explicit CoT training, (1) the convergence speed is significantly accelerated,
and (2)  the generalization
ability is enhanced, transitioning from ID-only generalization to
both ID and OOD generalization. See Section \ref{generalization} for more details.}
\label{figure:cmp cot w/o}
\end{center}
\vskip -0.2in
\end{figure}
\section{Introduction}
Training large language models (LLMs) to generate solutions step by step during the training phase has gained significant attention and is progressively becoming a widely adopted practice in the industry \citep{yue2023mammothbuildingmathgeneralist,yu2024metamath,wang-etal-2024-math,havrilla2024teaching,shao2024deepseekmathpushinglimitsmathematical,yu2024flowreasoningtrainingllmsdivergent,kim2023the,hsieh-etal-2023-distilling,ho-etal-2023-large,openai2024,deepseekai2025deepseekr1incentivizingreasoningcapability}. For instance, OpenAI has advanced AI customization with the introduction of reinforcement fine-tuning (RFT) for its O1 models, unveiled on the second day of the “12 Days of OpenAI” livestream series \citep{openai2024}. A key component of RFT/ReFT \citep{trung-etal-2024-reft} is conducting supervised fine-tuning (SFT) using Chain-of-Thought (CoT) annotations \citep{wei2022chain}. In the DeepSeek-R1 model \citep{deepseekai2025deepseekr1incentivizingreasoningcapability}, a small amount of long CoT cold-start data is incorporated to tune the model as the initial reinforcement learning (RL) actor. However, to gain a comprehensive understanding of the strategy of training with CoT, two key questions need to be addressed:
\begin{itemize}
    \item \textbf{Q1: }What advantages does training with CoT offer compared to training without CoT?
    \item \textbf{Q2: }If there are advantages, what are the underlying mechanisms of explicit CoT training?
\end{itemize}
Given the numerous factors involved in the actual training process, analyzing the advantages and underlying mechanisms of explicit CoT training presents significant challenges. To address this, we perform a detailed analysis using clear and controllable data distributions and uncover the following intriguing phenomena:

$\bullet$ \textbf{The Advantages of Training with CoT.} 
 (i) Compared to training without CoT, training with CoT significantly enhances reasoning generalization, extending it from in-distribution (ID)-only to both ID and (out-of-distribution) OOD scenarios (indicating systematic generalization), while also accelerating the convergence speed (Figure \ref{figure:cmp cot w/o}, Section \ref{generalization}). (ii) Even when training with CoT includes a certain range of erroneous reasoning steps, it still enables the model to learn reasoning patterns, leading to systematic generalization (Figures \ref{figure:only_b_noise} and \ref{figure:both_noise}, Section \ref{noise}). This highlights that data quality is more important than the method itself. The primary bottleneck in training lies in collecting complex long CoT solutions, with some errors being acceptable.
 
$\bullet$ \textbf{The Internal Mechanisms of Training with CoT.} (i) The crucial factors of
data distribution (e.g., ratio $\lambda$ and pattern) in characterizing the model’s systematic generalization (Section \ref{muti-hop}). In other words, a model that has only encountered two-hop data during CoT training cannot directly generalize to three-hop cases, it needs to have encountered the relevant patterns. (ii) Through Logit Lens and Causal Tracing experiments, we find that CoT training (with two-hop facts) internalizes reasoning steps into the model, forming a two-stage generalizing
circuit. The number of reasoning circuit stages matches the number of explicit reasoning steps during the training process (Section \ref{sec:circuit}).

\textbf{A Summary of Our Analyses.} 
In Section \ref{pre}, we define multi-step reasoning, training data, CoT methods, and generalization evaluation using symbols.  Our analyses in Sections \ref{generalization} and \ref{sec:circuit} correspond to advantages and mechanisms, respectively.  We further extend our analyses to distributions with errors present in the reasoning process (Section \ref{noise}) and verify the insights remain valid for more complex architectures on real-world data (Section \ref{real}). As far as we are aware, our work is the first to explore the benefits of CoT training in controlled experiments and provide a circuit-based explanation of the mechanisms underlying CoT training. These findings offer valuable insights into CoT and tuning strategies for LLMs to achieve robust generalization.

\section{Related Work} 
\textbf{Chain-of-Thought Reasoning and Training.} Research has demonstrated that incorporating an intermediate reasoning process in language before producing the final output significantly enhances performance, especially for transformers \citep{vaswani2017attention} with advanced generation capabilities. This includes prompting LLMs \citep{wei2022chain,zhou2023leasttomost,khot2023decomposedpromptingmodularapproach}, or training LLMs to generate reasoning chains, either through supervised fine-tuning \citep{yue2023mammothbuildingmathgeneralist,yu2024metamath} or reinforcement learning \citep{wang-etal-2024-math,havrilla2024teaching,shao2024deepseekmathpushinglimitsmathematical,yu2024flowreasoningtrainingllmsdivergent}. \textit{However, the specific advantages of CoT training remain an open question, which is the focus of our study}. There are also theoretical analyses that have shown the usefulness of CoT from the perspective of model capabilities (e.g., expressivity) \citep{feng2023towards,merrill2024expressivepowertransformerschain,li2024chain,prabhakar2024decipheringfactorsinfluencingefficacy,yin2025enhancinggeneralizationchainthought}. For instance, by employing CoT, the effective depth of the transformer increases as the generated outputs are fed back into the input \citep{feng2023towards}. \textit{These analyses, along with the practice effectiveness of CoT, motivate our exploration of the core mechanisms underlying explicit CoT training. Our findings suggest that a two-layer transformer may be sufficient for learning generalizing circuits through CoT training, which may explain the origin of CoT's expressivity during the training phase.}

\textbf{Latent Reasoning in Language Models.} The investigation of latent multi-hop abilities of LLMs could also have significant implications for areas such as generalization \citep{pmlr-v80-lake18a,onoe-etal-2023-lms} and model editing \citep{zhong-etal-2023-mquake,10.1162/tacl_a_00644}. To enhance the latent reasoning capabilities of LLMs, it is crucial to first understand the internal mechanisms by which LLMs effectively handle two-hop facts using latent multi-hop reasoning, which is also our focus. While a precise latent reasoning pathway has been found in controlled experiments \citep{stolfo-etal-2023-mechanistic,nanda2023progress,conmy2023towards,brinkmann-etal-2024-mechanistic,li-etal-2024-understanding,rai-yao-2024-investigation,yao2024knowledge,wang2024grokking}, it has not been thoroughly investigated in large pre-trained models. To fill this gap, \citet{yang2024largelanguagemodelslatently} construct a dataset of two-hop reasoning problems and discovered that it is possible to recover the intermediate variable from the hidden representations. \citet{biran2024hoppinglateexploringlimitations} identify a sequential latent reasoning pathway in LLMs, where the first-hop query is initially resolved into the bridge entity, which is then used to answer the second hop, they further propose to intervene the latent reasoning by “back-patching” the hidden representation. \textit{Nevertheless, these studies focus only on CoT prompting and do not consider CoT training.} Recently, it has also been found that one can “internalize” the CoT reasoning into latent reasoning in transformers with knowledge distillation \citep{deng2023implicitchainthoughtreasoning} or a special training curriculum that gradually shortens
CoT \citep{deng2024explicitcotimplicitcot}. Loop transformers \citep{giannou2023looped,cabannes2024iteration,fan2024loopedtransformerslengthgeneralization} have been proposed to solve algorithmic tasks. \textit{However, these works focus more on innovations in training methods and algorithms, without fully analyzing the underlying mechanisms of CoT training. This is precisely what we aim to uncover}.
\newpage
\section{Preliminaries and General Setup}\label{pre}
\textbf{Atomic and Multi-Hop Facts.} The atomic (one-hop) fact  
  describes two entities and the relationship between them, which can be represented as a triplet $(e_1,r_1,e_2)$. For example, “The United States’ capital is Washington, D.C.” More specifically, $e_1$ refers to the head entity (e.g., the United States), 
$r$ is the relation (e.g., Capital), and $e_2$ refers to the tail entity (e.g., Washington, D.C.). Based on atomic facts, a two-hop fact can be derived by combining two atomic facts, such as $(e_1,r_1,e_2)\oplus(e_2,r_2,e_3)\Longrightarrow (e_1,r_1,r_2,e_3)$, where $e_2$ serves as a bridge entity. Similarly and recursively, a multi-hop fact can be constructed from more atomic facts, resulting in $(e_1,r_1,e_2)\oplus(e_2,r_2,e_3)\oplus...\oplus(e_n,r_n,e_{n+1})\Longrightarrow (e_1,r_1,r_2,...,r_n,e_{n+1})$, where $n$ is the number of steps. In real-world scenarios, $e_2$ corresponds to the intermediate inference in a two-hop fact, while $e_2,e_3,...,e_{n}$ are intermediate results in more complex multi-step reasoning.

\textbf{Training Data.} Firstly, following the work of \citet{wang2024grokking}, we define the sets of entities $\mathcal{E}$ and relations $\mathcal{R}$, from which the set of atomic facts is constructed as $ \tilde{S} = \{(e_1, r_1, e_2)| e_1,e_2 \in \mathcal{E}, r_1 \in \mathcal{R}\}$. Our training set of atomic facts, denoted as $S$, is sampled from $\tilde{S}$ (i.e., $S \subset \tilde{S}$). Then, we partition $S$ into two subsets, $S_{\text{ID}}$ and $S_{\text{OOD}}$,  which are used to form two sets of two-hop facts, $S_{\text{ID}}^{(2)}$ and $S_{\text{OOD}}^{(2)}$, where $S_{\text{ID}}^{(2)}=\{(e_1,r_1,r_2,e_3)|(e_1,r_1,e_2),(e_2,r_2,e_3)\in S_{\text{ID}}, e_1 \neq e_3\}$, and $S_{\text{OOD}}^{(2)}$ is formed in a similar manner.  To evaluate the model's in-distribution (ID) and out-of-distribution (OOD) generalization ability, the training dataset $T$ excludes $S_{\text{ID}_{\text{test}}}^{(2)},S_{\text{OOD}}^{(2)}$, that is, $T = S_{\text{ID}}  \cup  S_{\text{OOD}} \cup S_{\text{ID}_{\text{train}}}^{(2)}$, where $S_{\text{ID}_{\text{train}}}^{(2)} \cup S_{\text{ID}_{\text{test}}}^{(2)} = S_{\text{ID}}^{(2)}$. Let $\lambda=|S_{\text{ID}_{\text{train}}}^{(2)}|/|S_{\text{ID}}|$, where $|\text{set}|$ denotes the number of samples in the set. Notice that $S_{\text{ID}}\cap S_{\text{OOD}}=\emptyset$, so $S_{\text{ID}}^{(2)}\cap S_{\text{OOD}}^{(2)}=\emptyset$ and the relation $r_1,r_2$ in $S_{\text{ID}}^{(2)}$ will not appear in $S_{\text{OOD}}^{(2)}$.

\textbf{Training without/with CoT.} For atomic facts $(S=S_{\text{ID}} \cup S_{\text{OOD}})$, training and evaluation are performed by having the model predict the final tail entity ($e_1, r_1 \xrightarrow{\text{predict}} \hat{e}_2, \forall (e_1,r_1,e_2)\in S$, $\hat{e_2}$ is the prediction of $e_2$, $\text{input} \xrightarrow{\text{predict}}\text{output}$). As for two-hop facts, we consider whether to use CoT annotations during training ($(e_1,r_1,r_2,e_3)\in S_{\text{ID}_{\text{train}}}^{(2)}$). (1) Training without CoT: $e_1, r_1, r_2 \xrightarrow{\text{predict}} \hat{e}_3$, only predict the final tail entity $e_3$. (2) Training with CoT: $e_1, r_1, r_2 \xrightarrow{\text{predict}} \hat{e_2} \text{ and } e_1, r_1, r_2, \hat{e_2} \xrightarrow{\text{predict}} \hat{e_3}$, predict both the bridge entity $e_2$ and the final tail entiy $e_3$. 

\textbf{ID/OOD Evaluation.} To better evaluate the generalization capacity of the model, we assess its performance on both ID and OOD data. (1) ID generalization aims to determine whether the model has correctly learned the latent patterns by evaluating its ability to complete previously unseen two-hop facts $S_{\text{ID}_{\text{test}}}^{(2)}$. (2) OOD generalization aims to assess the systematicity \citep{lake2018generalization} acquired by the model, specifically its ability to apply learned patterns to knowledge irrespective of its distribution. This is done by testing the model on facts $S_{\text{OOD}}^{(2)}$. If the model performs well on ID data, it may have memorized or learned patterns present in the training data $T$. \textbf{However, strong performance on OOD data indicates that the model has learned the latent patterns, as $T$ contains only atomic facts} $S_{\text{OOD}}$, \textbf{not} $S_{\text{OOD}}^{(2)}$. Representative examples illustrating the distribution difference between ID and OOD data are in Appendix \ref{app:example}.

\section{Systematic Composition Generalization}\label{generalization}
Our investigations focus on composition, where a model needs to “chain” different pieces of
facts, as stated in Section \ref{pre}. Although explicit verbalizations of reasoning steps (e.g., chain-of-thought rationales) can enhance task performance \citep{lake2018generalization,wei2022chain,wang2022iteratively,zelikman2022star,liu2023crystal}, they are not available during large-scale (pre-)training, which is the stage in which the model's core capabilities are developed \citep{li2020train,zhou2023lima}. Prior work has extensively studied whether transformer-based language models can perform implicit composition, with consistently negative results reported \citep{press2023measuring,yang2024largelanguagemodelslatently}. Specifically, there exists a “compositionality gap” \citep{press2023measuring}, i.e.,
the frequency at which the model knows all the underlying basic facts but fails to compose them,
which is considerable across different LLMs and does not decrease as models scale. To be more precise, \citet{wang2024grokking} reveal that transformers are capable of learning to perform implicit reasoning in ID generalization, but not in OOD generalization (same in Figure \ref{figure:cmp cot w/o}(left)). It naturally raises the question: \textbf{How would the generalization ability be affected if we use explicit reasoning steps in models during training?} (answer to \textbf{Q1:} What advantages does training with CoT offer compared to training without CoT?)

\subsection{Setup}\label{setup} 
The model we employ is a standard decoder-only transformer as in GPT-2 \citep{radford2019language}, with a configuration of 8 layers, 768 hidden dimensions, and 12 attention heads, and the tokenization is done by having a unique token for each entity/relation for convenience\footnote{Details regarding the model, tokenization and optimization we used are provided in Appendix \ref{app:train details}.}.

Moreover, in order to control the data distribution, we utilize the data-generation process introduced in Section \ref{pre}. Specifically, $S$ is constructed with 
$|\mathcal{E}|=2000$ entities and $|\mathcal{R}|=200$ relations (Details in Appendix \ref{app:data_process}). 
 One-hop (atomic) facts correspond to the $(e_1,r_1,e_2)$ triplets in $S$. These triplets are partitioned into two disjoint sets, that is, $S_{\text{ID}}$ (95\%) and $S_{\text{OOD}}$ (5\%), which are used to deduce the ID and OOD two-hop facts ($S_{\text{ID}}^{(2)},S_{\text{OOD}}^{(2)}$). The candidate set of $\lambda$ is $\{0.001,0.9,1.8,3.6,7.2,12.6\}$.
 
\subsection{CoT Training Boosts Reasoning Generalization} \label{sec:com results}
As shown in Figure \ref{figure:cmp cot w/o}, we demonstrate the model’s accuracy on the training and testing two-hop facts throughout optimization, with $\lambda=7.2$. 

(1) Training without CoT (Figure \ref{figure:cmp cot w/o} (left)). We observe the same phenomenon (known as grokking \citep{power2022grokking}) as \citet{wang2024grokking}, namely that the model can generalize well to ID testing examples $S_{\text{ID}_{\text{test}}}^{(2)}$, but high performance is only achieved after extensive training, far beyond the point of overfitting. Furthermore, even after training for millions of optimization steps, there is still no sign of OOD generalization ($S_{\text{OOD}}^{(2)}$), indicating that it is a case of delayed generalization lacking systematicity. The model may have memorized or learned patterns present in the training data.

(2) Training with CoT (Figure \ref{figure:cmp cot w/o} (right)). With CoT annotations, the convergence speed on the training set is accelerated, and the model achieves high test performance much earlier in the training process, particularly for ID testing examples. The accuracy on the ID test dataset $S_{\text{ID}_{\text{test}}}^{(2)}$ reaches near-perfect levels after approximately 4,000 optimization steps, indicating a significant improvement in generalization compared to training without CoT. The OOD generalization ($S_{\text{OOD}}^{(2)}$) also shows notable improvement, highlighting that CoT prompting training plays a crucial role in enhancing generalization not only within the distribution but also out-of-distribution, albeit with varying degrees of effectiveness. 
Therefore, by incorporating explicit reasoning steps during training, the model's generalization ability is enhanced, evolving from nonsystematic to systematic generalization. The model has learned the underlying patterns.

\begin{tcolorbox}[colback=white!5, colframe=blue!15!white]
\textbf{Insight 1.} \\ Compared to training without CoT, training with CoT significantly boosts reasoning generalization, expanding it from ID-only to both ID and OOD scenarios, while also accelerating convergence speed.
\end{tcolorbox}

\subsection{Exploration of Influencing Components}\label{ablation} We further conduct an ablation study to assess the impact of the following components on CoT during training. (1) Ratio $\lambda$ between two-hop facts and one-hop facts in training set. (2) Model scales of the from-scratch trained model. (3) The training set size, which scales linearly with $|\mathcal{E}|$. Notice that what we present here is the accuracy on the OOD test dataset ($S_{\text{OOD}}^{(2)}$).
\begin{figure}[ht]
\vskip 0.in
\begin{center}
\begin{minipage}{0.23\textwidth}
    \centering
    \includegraphics[width=\textwidth]{ pics/Effect_of_the_ratio_ood.pdf}  % 第一张图
\end{minipage}
\hfill
\begin{minipage}{0.23\textwidth}
    \centering
    \includegraphics[width=\textwidth]{ pics/Effect_of_the_model_size.pdf}  % 第二张图
\end{minipage}
\caption{The speed of reasoning generalization on OOD testing set $S_{\text{OOD}}^{(2)}$. \textbf{Left}: The effect of the two-hop facts/one-hop facts ratio $\lambda$, with the model having 8 layers as in Section \ref{setup}. The ratio $\lambda$ correlates with OOD generalization speed. \textbf{Right}: The effect of different model scales (layers:2,4,8) across different $\lambda$ (3.6,7.2,12.6). Larger models converge in fewer optimization steps, but have no qualitative changes on the learned behaviors.}
\label{figure:ratio lambda}
\end{center}
\vskip -0.3in
\end{figure}

\textbf{An appropriate value of $\lambda$ can accelerate convergence.}
The Figure \ref{figure:ratio lambda} (left) shows the OOD test
accuracy across different $\lambda$. It could be seen that the ratio $\lambda$ strongly correlates with the speed of generalization. More interestingly, a small ratio can accelerate the improvement of OOD generalization enhanced by CoT training, reducing the need for extended training. However, a smaller ratio is not always better, as the model may fail to learn the relevant rules. We further analyze the ratio $(>0)$ effect in Section \ref{muti-hop}. Moreover, a key feature of the  reinforcement fine-tuning (RFT, used in OpenAI O1 models \citep{openai2024}) is enabling organizations to train models using reinforcement learning to tackle domain-specific tasks with minimal data, this is consistent with our findings.

\textbf{The impact of different model scales/layers and training set size.} We run the experiments with model layer $\in\{2,4,8\}$ and $\lambda\in\{3.6,7.2,12.6\}$, other settings are same to Section \ref{setup}. Overall, it can be observed that scaling up the model does not qualitatively alter its generalization behavior. The primary trend is that larger models converge in fewer optimization steps, which aligns with previous findings \citep{li2020train,tirumala2022memorization,wang2024grokking}. For the effect of training set size ($|\mathcal{E}|$), our results align with \citep{wang2024grokking}: when fixing the ratio $\lambda$, the training set size does not
qualitatively affect the model’s generalization.

\subsection{Two-Hop to Muti-Hop}\label{muti-hop}
In Sections \ref{setup} to \ref{ablation}, we primarily focus on two-hop facts. In this subsection, we shift our focus to multi-hop scenarios: Can a model that has only encountered two-hop facts during the CoT training phase generalize to three-hop facts\footnote{It can naturally extend to multi-hop scenarios once thoroughly analyzed.}?

\textbf{Experiment Setup.} The rule of (three-hop) composition is $(h,r_1,b_1)\oplus(b_1,r_2,b_2)\oplus(b_2,r_3,t)\Longrightarrow (h,r_1,r_2,r_3,t)$, $\forall h,b_1,b_2,t \in \mathcal{E}, \forall r_1,r_2,r_3 \in \mathcal{R}$. In CoT training, we only use one-hop/two-hop facts ($h, r \xrightarrow{\text{predict}} t$; $h, r_1,r_2 \xrightarrow{\text{predict}} b_1,b_2$), and we test whether the model can generalize to reasoning in three-hop facts ($h, r_1,r_2,r_3\xrightarrow{\text{predict}} b_1,b_2,t$). Other settings are same in Section \ref{setup}.

\textbf{Results.} A model that has only encountered two-hop data during CoT training cannot directly generalize to three-hop scenarios. However, when a certain amount of three-hop data is added to the training set, the model can quickly generalize (model needs to have encountered the relevant patterns). On the other hand, when we artificially split a three-hop fact into two two-hop facts for testing, the model is also able to generalize effectively. In other words, we test $h, r_1,r_2\xrightarrow{\text{predict}} b_1,b_2$ and $b_1, r_2,r_3\xrightarrow{\text{predict}} b_2,t$ separately, and when both are correct, we consider $h, r_1,r_2,r_3\xrightarrow{\text{predict}} b_1,b_2,t$ to be correct. These align with \citep{cabannes2024iteration}: CoT relates to reproducing reasoning patterns that appear in the training set.

\begin{tcolorbox}[colback=white!5, colframe=blue!15!white]
\textbf{Insight 2.} \\
When the intermediate reasoning results (data patterns) from explicit CoT training are highly aligned with or closely match the intermediate reasoning required for final inference during testing, the model's generalization ability is significantly enhanced. This could explain why the researchers behind DeepSeek-R1 \citep{deepseekai2025deepseekr1incentivizingreasoningcapability} construct and collect a small amount of \textit{long CoT} data to fine-tune the model in the \textit{Cold Start} phase.
\end{tcolorbox}

\textbf{Summary.} Up to this point, we have demonstrated that incorporating explicit CoT training in controlled experiments significantly enhances the reasoning generalization ability, shifting it from ID-only generalization to encompassing both ID and OOD generalization. The crucial factors of \textbf{data distribution} (e.g., ratio and pattern) in characterizing the model's  systematic generalization. However, the internal mechanisms driving these improvements remain unclear, which we will further investigate in the next section (anwser to \textbf{Q2:} If there are advantages, what are the underlying
mechanisms of explicit CoT training?).

\section{Two-Stage Generalizing Circuit}\label{sec:circuit}
\begin{figure}[ht]
\vskip -0.in
\begin{center}
\centerline{\includegraphics[width=1.0\columnwidth]{ pics/circuit_layer8.pdf}}
\caption{The two-stage generalizing circuit (layer:8) for two-hop facts. We use logit lens
to interpret individual states, and use causal tracing to measure the strength of connections between states. The output of first stage is autoregressively used for the second stage.}
\label{circuit_layer8}
\end{center}
\vskip -0.3in
\end{figure}
We analyze the inner workings of the model throughout generalization via two prevalent approaches: logit lens \citep{nostalgebraist2020interpreting} and causal tracing \citep{pearl2009causality}. We apply our analysis to the setting in Section \ref{setup}, and we also show the layer-wise gradients change in Appendix \ref{app:gradient}.

\textbf{Logit Lens \& Causal Tracing.} Logit lens is a widely used for inspecting hidden states of LLMs \citep{dar2023analyzing,geva2023dissecting,katz2023visit,sakarvadia2023memory,yang2024largelanguagemodelslatently,wang2024grokking}. The key idea of logit lens is thus to  interpret hidden states in middle layers of LLMs via projecting them into the output vocabulary space with the model output embedding matrix. We adopt the recent practice \citep{yang2024largelanguagemodelslatently,wang2024grokking}  where the activation first goes through the transformer’s final normalization layer before multiplying with the output embedding. For causal tracing, the transformer can be interpreted as a causal graph \citep{pearl2009causality} that propagates information from the input to the output via a network of intermediate states, enabling various causal analyses of its internal computations \citep{vig2020investigating,meng2022locating,wang2023interpretability,hanna2023gpt2,feng2024binding,wang2024grokking}. Specifically, during the normal run, we intervene on the state of interest by replacing its activation with the activation from the perturbed run (See method details in Appendix \ref{app:causal}). We then proceed with the remaining computations and measure whether the target state (the top-1 token through the logit lens) is altered. For simplicity, we denote a hidden state by $E[\text{layer index, token position}]$.

\textbf{Two-Stage Generalizing Circuit.} We perform a set of logit lens and causal tracing experiments after training with CoT. Figure \ref{circuit_layer8} illustrates the discovered generalizing circuit, which represents the causal computational pathways after the 8-layer model achieves two-hop out-of-distribution (OOD) generalization. We also plot the 2-layer model circuit in Appendix \ref{app:2_circuit}, demonstrating that a two-layer transformer is sufficient to learn generalizing circuits from CoT training\footnote{By
employing CoT, the effective depth of the transformer increases because the generated outputs are looped
back to the input \citep{feng2023towards}.}. Specifically, we identify a highly interpretable causal graph consisting of states in layers 0, $l$, and 8, where weak nodes and connections have been pruned\footnote{If perturbing a node does not alter the target state (top-1 token through the logit lens), we prune the node.}. The circuit exhibits two stages, consistent with the explicit reasoning steps in models during training. Here, we use $(h,r_1,b)\oplus(b,r_2,t)\Longrightarrow (h,r_1,r_2,t), \forall h,b,t \in \mathcal{E}, \forall r_1,r_2 \in \mathcal{R}$ to represent two-hop facts, in order to more clearly illustrate the circuits.

(1) In the \textbf{first-hop stage}, layer $l$ splits the circuit into lower and upper parts: the lower parts retrieve the first-hop fact from the input $h,r_1,r_2$, store the bridge entity ($b$) in state $E[l,r_2]$; the upper parts pass the information of $b$ to output state $E[8,r_2]$ through the residual connections. Since the data distribution is controllable, $l$ can be precisely located (3 for $S_{\text{ID}_{\text{test}}}^{(2)}$, 5 for $S_{\text{OOD}}^{(2)}$). 

(2) In the \textbf{second-hop stage}, the auto-regressive model uses the $b$ generated in the first-hop stage. This stage omits the $h,r_1$ and processes the second-hop from the input $h,r_1,r_2,b$ to store the tail $t$ to the output state $E[8,b]$.

\textbf{Explaining the Systematic Generalization.} 

(1) The two-stage generalizing circuit indicates that training with CoT can internalize the reasoning steps into the model. This is also why, as shown in Section \ref{sec:com results}, the model generalizes well on OOD test data under CoT training. 

(2) The circuit consists of two stages, aligning with the explicit reasoning steps in models during training. Therefore, in Section \ref{muti-hop}, the model encountered two-hop data during CoT training cannot directly generalize to three-hop scenarios. 

(3) In the circuit, the bridge entity ($b$) can be extracted from the middle layer hidden states, which shares with prior findings \citep{wang2024grokking,biran2024hoppinglateexploringlimitations, li-etal-2024-understanding,ramesh2024compositional}. For example, \citet{biran2024hoppinglateexploringlimitations} discover that the bridge entity is resolved in the
early layers of the model. Then, only after
this resolution, the two-hop query is solved in
the later layers. Furthermore, they find that in incorrect cases, the resolution generally occurs at higher layers compared to correct cases. This is consistent with our finding that for ID, $l=3$, and for OOD, $l=5$ (OOD generalization is more challenging than ID generalization). 

\begin{tcolorbox}[colback=white!5, colframe=blue!15!white]
\textbf{Insight 3.} \\
CoT training internalizes reasoning steps, with the number of reasoning circuit stages matching the number of explicit reasoning steps during training.
\end{tcolorbox}

\section{More General Experience}
At a high level, our study so far paves the way for a deeper understanding and enhancement of the transformer's generalization abilities through CoT training on controlled data distributions. However, real-world training data distributions are often more complex. In this section, we extend our analysis to distributions with errors present in the reasoning process (Section \ref{noise}) and show that the insights from Section \ref{generalization} hold in more complex scenarios, as explored in Section \ref{real}.
\subsection{CoT Training with Noise}\label{noise}
\begin{figure}[ht]
\vskip 0.in
\begin{center}
\begin{minipage}{0.23\textwidth}
    \centering
    \includegraphics[width=\textwidth]{ pics/only_b_noiseiid.pdf}  
\end{minipage}
\hfill
\begin{minipage}{0.23\textwidth}
    \centering
    \includegraphics[width=\textwidth]{ pics/only_b_noiseood.pdf}  
\end{minipage}
\caption{
The impact of only the second-hop noise on ID and OOD generalization. The trend of accuracy changes on two test sets (\textbf{Left}: $S_{\text{ID}_{\text{test}}}^{(2)}$, \textbf{Right}: $S_{\text{OOD}}^{(2)}$) during the optimization process at different noise ratios ($\xi \in \{0.05,0.2,0.4,0.6,0.8\}$). Noise has a significant impact on the final performance on both ID and OOD test data. However, the generalization trends for ID and OOD differ under noisy conditions. See main text for details.} 
\label{figure:only_b_noise}
\end{center}
\vskip -0.1in
\end{figure}

\begin{figure}[ht]
\vskip -0.1in
\begin{center}
\begin{minipage}{0.23\textwidth}
    \centering
    \includegraphics[width=\textwidth]{ pics/both.noise.0.05.pdf}  
\end{minipage}
\hfill
\begin{minipage}{0.23\textwidth}
    \centering
    \includegraphics[width=\textwidth]{ pics/both.noise.0.1.pdf}  
\end{minipage}
\begin{minipage}{0.23\textwidth}
    \centering
    \includegraphics[width=\textwidth]{ pics/both.noise.0.2.pdf}  
\end{minipage}
\hfill
\begin{minipage}{0.23\textwidth}
    \centering
    \includegraphics[width=\textwidth]{ pics/both.noise.0.4.pdf}  
\end{minipage}
\caption{
The model's accuracy on training and testing two-hop reasoning facts at different noise ratios (both hops are noisy). This figure compares the results for $\xi$ values of $0.05, 0.1, 0.2$, and $0.4$. } 
\label{figure:both_noise}
\end{center}
\vskip -0.2in
\end{figure}
\textbf{Method.} We aim to analyze the robustness of the systematic generalization gained through CoT training under noisy training data. We choose the same datasets in Section \ref{sec:com results}, and a 2-layer model stated in Section \ref{setup}. We introduce noise to the $S_{\text{ID}_{\text{train}}}^{(2)}$ by randomly selecting a valid entity (the gold training target is $h,r_1,r_2,b,t$, similar in Section \ref{sec:circuit}): (1) Only the second hop is noisy, $h,r_1,r_2,b,t_{\text{noise}}$; (2) Both hops are noisy, $h,r_1,r_2,b_{\text{noise}},t_{\text{noise}}$. Note that the noise ratio is denoted as $\xi$, we explore the impact of different $\xi$.
\begin{table}[ht]
\caption{The model’s accuracy (100\%) on all reasoning facts at different noise ratios (only the second hop is noisy). This table compares the results for $\xi$ values of 0.05, 0.2, 0.4, 0.6 and 0.8 with 20,000 optimization steps in Figure \ref{figure:only_b_noise}.}
\label{table:only_t_noise}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{lccccr}
\toprule
$\xi$ & $S_{\text{ID}}$ & $S_{\text{OOD}}$ & $S_{\text{ID}_{\text{train}}}^{(2)}$ & $S_{\text{ID}_{\text{test}}}^{(2)}$ &$S_{\text{OOD}}^{(2)}$\\
\midrule
0.05    & 99.9 & 100.0 & 99.3 & 99.0 & 97.8 \\
0.2    & 100.0 & 99.9 & 98.9 & 95.1 & 77.6 \\
0.4    & 100.0 & 99.9 & 98.0 & 86.6 & 49.1  \\
0.6  &  100.0 & 99.5 & 96.8 &70.4 & 30.9 \\
0.8 & 100.0 &  99.2 & 97.3 & 42.6 & 14.8 \\
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table}
\begin{table}[ht]
\caption{The model’s accuracy (100\%) on all reasoning facts at different noise ratios (both the first and the second hops are noisy). This table compares the results for $\xi$ values of 0.05, 0.1, 0.2 and 0.4 with 20,000 optimization steps in Figure \ref{figure:both_noise}.}
\label{table:both_noise}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{lccccr}
\toprule
$\xi$ & $S_{\text{ID}}$ & $S_{\text{OOD}}$ & $S_{\text{ID}_{\text{train}}}^{(2)}$ & $S_{\text{ID}_{\text{test}}}^{(2)}$ &$S_{\text{OOD}}^{(2)}$\\
\midrule
0.05    & 98.3 & 99.7 & 93.7 & 55.4 & 53.7 \\
0.1  &  97.5 & 97.4 & 85.4 &29.8 & 27.1 \\
0.2    & 95.0 & 92.2 & 73.6 & 11.5 & 8.4 \\
0.4    & 84.5 & 72.4 & 65.2 & 2.1 & 0.3  \\
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table}

\textbf{Results.} We analyze different $\xi$ (noise ratio) candidate sets for the two situations: $\{0.05,0.2,0.4,0.6,0.8\}$ for only the second hop is noisy and $\{0.05,0.1,0.2,0.4\}$ for both hops are noisy. The comparison results are as follows:

(1) Figure \ref{figure:only_b_noise} and Table \ref{table:only_t_noise} clearly demonstrate the impact of only the second-hop noise on ID and OOD generalization ($S_{\text{ID}_{\text{test}}}^{(2)}$ and $S_{\text{OOD}}^{(2)}$). Overall, under CoT training conditions, the model is still able to achieve systematic generalization from noisy training data, but its generalization ability decreases as the noise ratio increases. More specifically, as training progresses, OOD generalization initially remains constant and then increases, while ID generalization first increases and then decreases. The decrease in ID generalization corresponds to the increase in OOD generalization. However, the final performance of both ID and OOD generalization decreases as the ratio increases. In particular, when the noise ratio ($\xi<0.2$) is relatively small, the model is almost unaffected, demonstrating the robustness of CoT training. Furthermore, we also use the method in Section \ref{sec:circuit} to examine the circuits. Since we only add noise in the second hop, the circuit in the first-hop stage is learned relatively well, while the circuit in the second-hop stage is more heavily affected by noise. 

(2) Figure \ref{figure:both_noise} and Table \ref{table:both_noise} show a comparison of the results for both
hop noise $\xi$ values of $0.05, 0.1, 0.2$, and $0.4$. Adding noise to both hops has a much stronger suppressive effect on the model's generalization compared to adding noise only to the second hop. A noise ratio greater than 0.2 is enough to nearly eliminate both ID and OOD generalization ability.

(3)  Table \ref{table:only_t_noise} and Table \ref{table:both_noise} demonstrate that adding noise to both hops exerts a significantly stronger suppressive effect on the model’s generalization compared to introducing noise solely to the second hop. When the noise ratio is 0.2, the accuracy of $S_{\text{ID}_{\text{test}}}^{(2)}$ in Table \ref{table:only_t_noise} is 95.1\%, and the accuracy of $S_{\text{OOD}}^{(2)}$ is 77.6\%. However, in Table \ref{table:both_noise}, the accuracies of $S_{\text{ID}_{\text{test}}}^{(2)}$ and $S_{\text{OOD}}^{(2)}$ are only 11.5\% and 8.4\%. This indicates that the harm caused by CoT training data with completely incorrect reasoning steps is enormous.

(4) To sum up, even with noisy training data, CoT training can still enable the model to achieve systematic generalization when the noise is within a certain range\footnote{\citet{deepseekai2025deepseekr1incentivizingreasoningcapability} collect about 600k long CoT training samples. Errors during the process are acceptable, as it is impossible for humans to manually write 600k high-quality solutions. Moreover, through reinforcement learning, OpenAI O1 \citep{openai2024o1} learns to hone its chain of thought and refine the strategies it uses. It learns to recognize and correct its mistakes.}. Especially when the noise ratio is small, these noisy data can still help the model learn generalization circuits. Moreover, 
we further analyze the connection between noisy samples and the samples with prediction errors from the model, and give the impact of only the first-hop is noisy in Appendix \ref{app:noise}.
\begin{tcolorbox}[colback=white!5, colframe=blue!15!white]
\textbf{Insight 4.} \\
CoT training still enables systematic generalization with noisy data, highlighting that data quality outweighs the method itself. The training bottleneck lies in collecting or synthesizing complex \textit{long CoT} solutions, with some errors being acceptable.
\end{tcolorbox}
\subsection{Realistic Data Verification}\label{real}
\textbf{Experiment Setup.} We extend our experiments to Llama2-7B \citep{touvron2023llama} to verify whether the insights (that the model can achieve systematic composition generalization through CoT training) hold true for real-world datasets. We use the dataset provided by \citep{biran2024hoppinglateexploringlimitations}, which contains 82,020 two-hop queries based on data from Wikidata \citep{Wikidata}. To exclude the influence of the model's inherent knowledge, we filter the data and split the training and testing set, the filtering process can be seen in Appendix \ref{app:filter}. When it is about to training and testing data:

(1) The training set is made up of 6,442 cases where the model correctly answers both the two-hop query and the first hop. 

(2) The testing set includes 2,320 cases where the model correctly answers both the first and second hop in isolation, but fails to answer the full two-hop query. 

We force the model to only output the answers to both hops, for example, the model input is “the capital of the country of the base of OpenAI is” and the model target is “the United States. Washington, D.C.” To be more specific, the first hop of the example is “the country of the base of OpenAI is the United States.” and the second hop of the example is “the capital of the United States is Washington, D.C.”. Due to resource limitations, we fine-tune the model for 100 epochs on the training set using LoRA \citep{lora} (rank=32) and test it on the testing set. The framework and hardware information we used is provided in Appendix \ref{app:train details}.

\textbf{Results.} As shown in Figure \ref{figure:train_real}, the model's testing loss on the real-world dataset decreases along with the training loss, indicating that CoT training does indeed provide the model with some generalization ability. In more detail, we examined the outputs of the model after fine-tuning. Excitingly, when the model receives the input “the capital of the country of citizenship of Jenna Jameson is” (in testing set), the first token it outputs is “Washington.”, which is also the first token of “Washington, D.C”. These verify our analysis insights in Section \ref{generalization} and \ref{sec:circuit}.

\section{Discussion}
\textbf{Conclusion.} This paper reveals the core mechanisms underlying Chain-of-Thought (CoT) training by illustrating how systematic composition generalization emerges in transformers through explicit CoT training in controlled and interpretable settings. Specifically: 

(1) Compared to training without CoT, CoT training significantly enhances reasoning generalization, expanding it from in-distribution (ID)-only generalization to encompass both ID and out-of-distribution (OOD) scenarios. 

(2) Through Logit Lens and Causal Tracing experiments, we find that CoT training (with two-hop facts) internalizes reasoning steps into the transformers, forming a two-stage generalization circuit. However, the model's reasoning ability is constrained by the complexity of the training data, as it struggles to generalize from two-hop to three-hop cases. This suggests that CoT primarily reproduces reasoning patterns present in the training set. 

(3) We further extend our analysis to distributions with errors present in the reasoning process, demonstrating that CoT training can still enable the model to achieve systematic generalization when the noise remains within a certain range. The structure of such noisy data can contribute to the generalization circuits. 

Interestingly, our work also highlights the bottleneck of training with CoT: training data distribution (ratio $\lambda$ and pattern) plays a key role in guiding the model towards the implementation of generalizing circuits. The model needs to have encountered the relevant patterns (especially the number of CoT steps) during the training process. This may explain why the researchers of DeepSeek-R1 \citep{deepseekai2025deepseekr1incentivizingreasoningcapability} construct and collect a small amount of \textit{long CoT} data to fine-tune the model in the \textit{Cold Start} phase. Our findings provide critical insights into tuning strategies for large language models (LLMs) to achieve robust generalization.

\textbf{Limitation and Future Work.} 

(1) Although our bottom-up work provides valuable insights for practical applications, a key limitation of our work is that the experiments and analyses are based on synthetic data, which may not fully capture the complexities of real-world datasets and tasks. While some of our conclusions have also been validated in models like Llama2-7B \citep{touvron2023llama}, further verification on a wider range of models is necessary to bridge the gap between our theoretical understanding and practical application. 

(2) Our analysis is currently limited to the use of natural language. In the future, we aim to explore the potential of LLM reasoning in an unrestricted latent space, specifically through approaches such as training large language models to reason in a continuous latent space \citep{hao2024traininglargelanguagemodels}.

(3) Common interpretability methods project weights and hidden states obtained from the forward pass onto the models' vocabularies \citep{nostalgebraist2020interpreting,geva-etal-2021-transformer,geva-etal-2022-lm,ma-etal-2023-chain,katz-belinkov-2023-visit}, helping to uncover how information flows within Transformer-based LMs, such as the “Logit Lens” we use.  Back propagation has been playing a major role in interpreting deep learning models and multiple lines of study aggregate the gradients to provide explainability \citep{sanyal-ren-2021-discretized,chefer2022optimizing,miglani-etal-2023-using}. A recent method, “Backward Lens” \citep{katz-etal-2024-backward}, projects LM gradients onto the vocabulary space to capture the backward information flow. This provides us with a new perspective to analyze the mechanisms underlying CoT training.

% \subsection{Tables}

% You may also want to include tables that summarize material. Like
% figures, these should be centered, legible, and numbered consecutively.
% However, place the title \emph{above} the table with at least
% 0.1~inches of space before the title and the same after it, as in
% \cref{sample-table}. The table title should be set in 9~point
% type and centered unless it runs two or more lines, in which case it
% should be flush left.

% % Note use of \abovespace and \belowspace to get reasonable spacing
% % above and below tabular lines.

% \begin{table}[t]
% \caption{Classification accuracies for naive Bayes and flexible
% Bayes on various data sets.}
% \label{sample-table}
% \vskip 0.15in
% \begin{center}
% \begin{small}
% \begin{sc}
% \begin{tabular}{lcccr}
% \toprule
% Data set & Naive & Flexible & Better? \\
% \midrule
% Breast    & 95.9$\pm$ 0.2& 96.7$\pm$ 0.2& $\surd$ \\
% Cleveland & 83.3$\pm$ 0.6& 80.0$\pm$ 0.6& $\times$\\
% Glass2    & 61.9$\pm$ 1.4& 83.8$\pm$ 0.7& $\surd$ \\
% Credit    & 74.8$\pm$ 0.5& 78.3$\pm$ 0.6&         \\
% Horse     & 73.3$\pm$ 0.9& 69.7$\pm$ 1.0& $\times$\\
% Meta      & 67.1$\pm$ 0.6& 76.5$\pm$ 0.5& $\surd$ \\
% Pima      & 75.1$\pm$ 0.6& 73.9$\pm$ 0.5&         \\
% Vehicle   & 44.9$\pm$ 0.6& 61.5$\pm$ 0.4& $\surd$ \\
% \bottomrule
% \end{tabular}
% \end{sc}
% \end{small}
% \end{center}
% \vskip -0.1in
% \end{table}

% Tables contain textual material, whereas figures contain graphical material.
% Specify the contents of each row and column in the table's topmost
% row. Again, you may float tables to a column's top or bottom, and set
% wide tables across both columns. Place two-column tables at the
% top or bottom of the page.



\section*{Impact Statement}
This study presents the advantages gained from training with CoT and sheds light on the mechanisms underlying explicit CoT training. Our goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none of which we feel must be specifically highlighted here. However, future research could use the insights we provided to design effective strategies for fine-tuning LMs. We hope such cases will be used to develop better and safer models.



% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocite
% \nocite{langley00}
\newpage
\bibliography{example_paper}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn
\section{Example with Distribution Differences} \label{app:example}
The key distinction between ID and OOD data lies in whether their distributions are the same. We provide a more prominent example with distribution differences here. The training set is composed of $S_{\text{ID}}$, $S_{\text{OOD}}$ and $S_{\text{ID}_{\text{train}}}^{(2)}$.

\textbf{$S_{\text{ID}}$:} \\
$(\text{Paris}, \text{CapitalOf}, \text{France}),(\text{France}, \text{LocatedIn}, \text{Europe}),(\text{Berlin}, \text{CapitalOf}, \text{Germany}),(\text{Germany}, \text{LocatedIn}, \text{Europe}).$

\textbf{$S_{\text{OOD}}$:}\\
$(\text{Lima}, \text{CapitalOf}, \text{Peru}),$
$(\text{Peru}, \text{HasNaturalFeature}, \text{Andes Mountains})$.

\textbf{$S_{\text{ID}_{\text{train}}}^{(2)}$,} a uniformly random subset of the inferred facts derived from $S_{\text{ID}}$:\\
$(\text{Paris}, \text{CapitalOfCountryLocatedIn}, \text{Europe})$.

\textbf{$S_{\text{ID}_{\text{test}}}^{(2)}$,} previously unseen inferred facts derived from $S_{\text{ID}}$ (ID generalization):\\
$(\text{Berlin}, \text{CapitalOfCountryLocatedIn}, \text{Europe})$.

\textbf{$S_{\text{OOD}}^{(2)}$,} previously unseen inferred facts derived from $S_{\text{OOD}}$ (OOD generalization):\\
$(\text{Lima}, \text{CapitalOfCountryWithNaturalFeature}, \text{Andes Mountains})$.

\textcolor{blue}{\textbf{Key Changes.}} This OOD data setup requires the model to tackle greater knowledge distribution differences and reasoning challenges during evaluation.

Change in Relation Types: New relation types, such as "HasNaturalFeature," are introduced in the OOD data.

Complexity of Reasoning Paths: Reasoning paths involving natural features are added, requiring the model not only to reason but also to generalize to new types of knowledge.


\section{Training Details}\label{app:train details}
\textbf{Model.} The model we employ is a standard decoder-only transformer as in GPT-2 \citep{radford2019language}. We use 8-layer for Figure \ref{figure:cmp cot w/o} and Figure \ref{figure:ratio lambda} (left), while 2,4,8-layer for Figure \ref{figure:ratio lambda} (right) and 2-layer for Figure \ref{figure:only_b_noise} and \ref{figure:both_noise}. The other hyperparameters are consistent with those described in Section \ref{setup}.

\textbf{Tokenization.} In our main content (Section \ref{generalization}), we assign a unique token to each relation/entity by default. This is because \citet{wang2024grokking} find that different tokenizations affect the results in rather expected ways, and do not influence
the reasoning generalization findings. We also  validate our findings in the real-world entity-tokenization in Section \ref{real}.

\textbf{Optimization.} Optimization is done by AdamW \citep{loshchilov2019decoupled} with learning rate $10^{-4}$, batch size $512$, weight decay $0.1$ and $2000$ warm-up steps. Notably, models are trained for a large number of epochs/steps beyond the point where training performance saturates. All runs were done with PyTorch \citep{paszke2019pytorch} and Huggingface Transformers \citep{wolf2020transformers} on NVIDIA GeForce RTX 3090.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{More Method Details}
\subsection{Causality Analysis}\label{app:causal}
We follow the recent practice \citep{wang2023interpretability}, where the causal tracing process consists of three steps (for every stage in Section \ref{sec:circuit}):

(1) In the normal run, we capture the model's hidden state activations for a regular input $(h,r_1,r_2)/(h,r_1,r_2,b)$. Notably, as the model maintains perfect training accuracy throughout the CoT training process, the final prediction invariably aligns with the ground truth\footnote{For simplicity, when we refer to a state as a token, we are indicating the top-ranked token of that state as determined by the logit lens.} —— bridge entity $b$ for the first-hop stage and tail entity $t$ for the second-hop stage.

(2) During the perturbed run\footnote{ For the perturbation, prior work has explored
adding noise to the input \citep{meng2022locating} and replacing key tokens with semantically close ones \citep{vig2020investigating,feng2024binding}. We
adopt token replacement which avoids unnecessary distribution shifts \citep{zhang2024towards}.}, the model is given a slightly modified input that alters its prediction, and the hidden state activations are recorded once again. Specifically, for the hidden state of interest, we modify the input token at the corresponding position by substituting it with a random alternative of the same type (e.g., $r_1\rightarrow r_1^{'}$ ), which results in a different target prediction (e.g., $b\rightarrow b^{'},t\rightarrow t^{'}$ ).

(3) Intervention. During the normal run, we intervene the activation of the state of interest by substituting it with its corresponding activation from the perturbed run. After completing the remaining computations, we check whether the target state (determined as the top-1 token via the logit lens) has changed. If perturbing a node does not alter the target state (top-1 token
through the logit lens), we prune the node.

\subsection{Realistic Data Filtering Process}\label{app:filter}
Referring to \citep{biran2024hoppinglateexploringlimitations}, we aim to filter out cases where no latent reasoning occurs. Given a two hop query $((h,r_1,b),(b,r_2,t)$ we test two prompts constructed to detect and filter out cases where the model performs reasoning shortcuts \citep{xu-etal-2022-model,wang-etal-2023-causal,ju-etal-2024-investigating}. (1) The first prompt is $((,r_1,b),(b,r_2,t)$ (i.e.,
the query without $h$), aimed at filtering out cases
where the model predicts generally popular entities. (2) The second prompt is $((h,,b),(b,r_2,t)$ (i.e.,
the query without $r_1$), aimed at filtering out
cases where the model predicts correctly due to a
high correlation between $h$ and $t$. We perform this filtering for the model (Llama2-7B(-chat)) using greedy decoding creating a subset of the dataset. 

We are specifically interested in the model's reasoning performance, therefore, we further generate two dataset subsets: (1) The first subset is made up of 6,442 cases where the model correctly answers both the two-hop query and the first hop. (2) The second subset includes 2,320 cases where the
model correctly answers both the first and second
hop in isolation, but fails to answer the full two-hop
query. Notice that we only use the first subset for CoT training and the second subset for generalization evaluation.

\section{More Experience Results}
\subsection{Two-Layer Model Circuit}\label{app:2_circuit}
\begin{figure}[ht]
\vskip -0.in
\begin{center}
\centerline{\includegraphics[width=0.95\columnwidth]{ pics/circuit_layer2.pdf}}
\caption{The two-stage generalizing circuit (layer:2) for two-hop facts. We use logit lens
to interpret individual states, and use causal tracing to measure the strength of connections between states. \textbf{It is evident that a two-layer model can learn generalizing circuits from CoT training}. This aligns with \citet{cabannes2024iteration}, who describe how a certain distribution of weights within the first two attention layers of a transformer, referred to as an “iteration head,” enables a transformer to solve iterative tasks with CoT reasoning with relative ease. We identify CoT reasoning circuits in a transformer with only 2 layers.}
\label{figure:circuit_layer2}
\end{center}
\vskip -0.2in
\end{figure}
\subsection{Noise Discussion}\label{app:noise}
In Section \ref{noise}, we demonstrated that adding noise to both hops exerts a significantly stronger suppressive effect on the model's generalization compared to introducing noise solely to the second hop. Here, we further investigate the impact of scenarios where only the first hop is noisy. We introduce noise to $S_{\text{ID}_{\text{train}}}^{(2)}$ by randomly selecting a valid entity, where the gold training target is $h, r_1, r_2, b, t$. In this case, only the first hop is noisy, resulting in $h, r_1, r_2, b_{\text{noise}}, t$.

\textbf{Only the First-Hop is Noisy.} Table \ref{table:only_b_noise} highlights that as $\xi$ increases, performance generally declines across all datasets, with the most significant drops observed in $S_{\text{ID}_{\text{test}}}^{(2)}$ and $S_{\text{OOD}}^{(2)}$ at higher noise levels. For instance, when $\xi=0.4$, the accuracy for $S_{\text{OOD}}^{(2)}$ falls to $0.3\%$, indicating a substantial degradation in model generalization under noisy conditions. This degradation indicates that errors in intermediate steps within the CoT training data have a greater impact. However, the model still retains a certain level of generalization capability under low noise conditions.

\begin{table}[ht]
\caption{The model’s accuracy (100\%) on all reasoning facts at different noise ratios (only the first hop is noisy). This table compares the results for $\xi$ values of 0, 0.05, 0.1, 0.2, and 0.4. Regarding the optimization (20,000 steps) and model settings, we remain consistent with Section \ref{noise}. All values are averaged over 3 random seeds.}
\label{table:only_b_noise}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{lccccr}
\toprule
$\xi$ & $S_{\text{ID}}$ & $S_{\text{OOD}}$ & $S_{\text{ID}_{\text{train}}}^{(2)}$ & $S_{\text{ID}_{\text{test}}}^{(2)}$ &$S_{\text{OOD}}^{(2)}$\\
\midrule
0  &  100.0 & 99.9 & 99.9 &100.0 & 99.4 \\
0.05    & 98.4 & 99.4 & 94.3 & 53.3 & 51.3 \\
0.1 & 97.9 &  98.2 & 86.3 & 29.7 & 29.5\\
0.2    & 97.0 & 94.5 & 74.9 & 10.6 & 7.5 \\
0.4    & 89.6 & 77.0 & 68.7 & 1.8 & 0.3         \\
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table}

\textbf{Noisy Samples Affect the Probability Distribution of the Output Vocabulary Space.}
As shown in Table \ref{table:only_b_noise}, adding noise to $S_{\text{ID}_{\text{train}}}^{(2)}$ can even impact the model's basic knowledge ($S_{\text{ID}}$ and $S_{\text{OOD}}$). We further analyze the relationship between noisy samples and the samples with prediction errors from the model. Namely, we use a sample 
$\textcolor{blue}{e_{1567},r_{32}},r_{23}\xrightarrow{\text{gold label}}\textcolor{blue}{e_{1002}},e_{404}$ to validate the model after CoT training with $\xi=0.05$. The model’s erroneous output is $\textcolor{blue}{e_{1567},r_{32}},r_{23}\xrightarrow{\text{wrong predict}}\textcolor{blue}{e_{1640}},e_{665}$, and we identify the noisy sample in the training set as: $\textcolor{blue}{e_{1567},r_{32}},r_{132}\xrightarrow{\text{noisy label}}\textcolor{blue}{e_{1640}},e_{851}$. We check the probability distribution of the output vocabulary space: (1) For one-hop fact $\textcolor{blue}{e_{1567},r_{32}} (\xrightarrow{\text{gold label}}\textcolor{blue}{e_{1002}}),$ 
the token with the highest output probability is $e_{1002}$, followed by $e_{1640}$ as the second highest. Adding excessive noise can bias the previously learned one-hop facts. (2) For two-hop fact $\textcolor{blue}{e_{1567},r_{32}},\ , (\xrightarrow{\text{gold label}}\textcolor{blue}{e_{1002}},\ ),$ the token with the highest output probability is $e_{1640}$, followed by $e_{1002}$ as the second highest. The structure of such noisy data can indeed contribute to the learning of generalization performance. However, the addition of noise can negatively impact the reasoning of related facts.


\subsection{Realistic Data Loss}
\begin{figure}[ht]
\vskip -0.2in
\begin{center}
\centerline{\includegraphics[width=0.35\columnwidth]{ pics/Training_real_data.pdf}}
\caption{The training loss and testing loss on Llama2-7B(-chat) of experience in Section \ref{real}.}
\label{figure:train_real}
\end{center}
\vskip -0.2in
\end{figure}

\subsection{Layer-Wise Gradients Change}\label{app:gradient}
\begin{figure}[ht]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=1.0\columnwidth]{ pics/weight_change.png}}
\caption{The nuclear norm of gradients across different layers when training with CoT. This figure aligns with the Figure \ref{figure:ratio lambda} (right), we plot the setting with $\text{layer}=4,\lambda=7.2$. The checkpoint=4 means the optimization steps=4,000, when the OOD test accuracy begins to grow. The ID generalization phase $(0-4)$ leads to larger gradients across layers compared to the OOD generalization phase $(5-8)$. This aligns with the results \citep{li2024happenedllmslayerstrained}, indicating that slow thinking with CoT results in smaller gradients and enables the gradients to effectively distinguish between correct and irrelevant reasoning paths.}
\label{figure:gradient}
\end{center}
\vskip -0.2in
\end{figure}

\textbf{Nuclear Norm:} The nuclear norm of gradient ($G$) is defined as the $l_1$ norm of the singular values, which reflects the sparsity of the spectrum and serves as a convex surrogate of the matrix rank. Hence, it does not only quantify the gradient magnitude but also the concentration of the spectrum on its top few singular values, which is vital to understand the gradient patterns in each layer, i.e., $||G||_{\text{nuclear}}=\sum_{j=1}^{\min(m,n)}|\sigma_j|$, $\sigma_j (j=1,...,)$  are singular values of $G$.

\subsection{Data Generation Process} \label{app:data_process}
Specifically, for one-hop facts, a random knowledge graph $\mathcal{G}$ is constructed with 
$|\mathcal{E}|$ entities and $|\mathcal{R}|=200$ relations. Each entity, acting as the head ($h$), is linked to 20 unique relations, with each relation connecting to another randomly selected entity serving as the tail ($t$). One-hop facts correspond to the $(h,r,t)$ triplets in $\mathcal{G}$. $S \subset \mathcal{G}\subset \tilde{S}$.
These triplets are partitioned into two disjoint sets: $S_{\text{ID}}$ (95\%) and $S_{\text{OOD}}$ (5\%), which are used to deduce the ID/OOD two-hop facts ($    \forall h,b,t \in \mathcal{E}, \forall r_1,r_2 \in \mathcal{R}$): $(h,r_1,b)\oplus(b,r_2,t)\Longrightarrow (h,r_1,r_2,t).$

\section{More Related Work}
\textbf{Chain-of-Thought Reasoning.} Research has demonstrated that incorporating an intermediate reasoning process in language before producing the final output significantly enhances performance, especially for transformers \citep{vaswani2017attention} with advanced generation capabilities. This includes prompting LLMs \citep{wei2022chain,zhou2023leasttomost,khot2023decomposedpromptingmodularapproach}, or training LLMs to generate reasoning chains, either with supervised fine-tuning \citep{yue2023mammothbuildingmathgeneralist,yu2024metamath} or reinforcement learning \citep{wang-etal-2024-math,havrilla2024teaching,shao2024deepseekmathpushinglimitsmathematical,yu2024flowreasoningtrainingllmsdivergent}. There are also theoretical analyses have shown the usefulness of CoT from the perspective of model expressivity and generalization \citep{feng2023towards,merrill2024expressivepowertransformerschain,li2024chain,prabhakar2024decipheringfactorsinfluencingefficacy,yin2025enhancinggeneralizationchainthought}. By employing CoT, the effective depth of the transformer increases as the generated outputs are fed back into the input \citep{feng2023towards}. This aligns with our findings, suggesting that a two-layer transformer is sufficient for learning generalizing circuits through CoT training. While CoT has proven effective for certain tasks, its autoregressive generation nature makes it difficult to replicate human reasoning on more complex problems \citep{lecun2022path,hao-etal-2023-reasoning}, which often require planning and search \citep{xie2023selfevaluation,yao2023tree,hao2024llm,lehnert2024beyond,gandhi2024stream,su2024dualformercontrollablefastslow,li2024happenedllmslayerstrained}. These analyses, along with the practice effectiveness of CoT, motivate our exploration of the core mechanisms underlying explicit CoT training.

\textbf{Latent Reasoning in Language Models.} The investigation of latent multi-hop abilities of LLMs could also have significant implications for areas such as generalization \citep{pmlr-v80-lake18a,onoe-etal-2023-lms} and model editing \citep{zhong-etal-2023-mquake,10.1162/tacl_a_00644}. To enhance the latent reasoning capabilities of LLMs, it is crucial to first understand the internal mechanisms by which LLMs effectively handle two-hop facts using latent multi-hop reasoning, which is also our focus. While a precise latent reasoning pathway has been found in controlled experiments \citep{stolfo-etal-2023-mechanistic,nanda2023progress,conmy2023towards,brinkmann-etal-2024-mechanistic,li-etal-2024-understanding,rai-yao-2024-investigation,yao2024knowledge,wang2024grokking}, it has not been thoroughly investigated in large pre-trained models. To fill this gap, \citet{yang2024largelanguagemodelslatently} construct a dataset of two-hop reasoning problems and discovered that it is possible to recover the intermediate variable from the hidden representations. \citet{biran2024hoppinglateexploringlimitations} identify a sequential latent reasoning pathway in LLMs, where the first hop query is initially resolved into the bridge entity which is then used to answer the second hop, they further propose to intervene the latent reasoning by “back-patching” the
hidden representation. \citet{shalev2024distributionalreasoningllmsparallel}  discover parallel latent reasoning paths in LLMs. \citet{ghandehariounpatchscopes} introduce a framework called “Patchscopes”  to explain LLMs internal representations in natural language. Recently, it has also been found that one can “internalize” the CoT reasoning into latent reasoning in the transformer with knowledge distillation \citep{deng2023implicitchainthoughtreasoning} or a special training curriculum which gradually shortens
CoT \citep{deng2024explicitcotimplicitcot}. Loop transformers \citep{giannou2023looped,cabannes2024iteration,fan2024loopedtransformerslengthgeneralization} have been proposed to solve algorithmic tasks. These have some similarities to the two-stage generalizing circuit we present.
\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
