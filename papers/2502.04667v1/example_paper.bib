%  The following are used
@misc{openai2024,
  author = {OpenAI},
  title = {12 Days of OpenAI},
  year = {2024},
  howpublished = {\url{https://openai.com/12-days/}},
}
@misc{openai2024o1,
  author = {OpenAI},
  title = {Learning to reason with LLMs},
  year = {2024},
  howpublished = {\url{https://openai.com/index/learning-to-reason-with-llms/}},
}
@inproceedings{
wang2024grokking,
title={Grokking of Implicit Reasoning in Transformers: A Mechanistic Journey to the Edge of Generalization},
author={Boshi Wang and Xiang Yue and Yu Su and Huan Sun},
booktitle={Advances in Neural Information Processing Systems},
year={2024},
}

@inproceedings{lake2018generalization,
  author    = {Brenden Lake and Marco Baroni},
  title     = {Generalization without systematicity: On the compositional skills of sequence-to-sequence recurrent networks},
  booktitle = {Proceedings of the International Conference on Machine Learning},
  pages     = {2873--2882},
  year      = {2018},
}

@inproceedings{
wei2022chain,
title={Chain of Thought Prompting Elicits Reasoning in Large Language Models},
author={Jason Wei and Xuezhi Wang and Dale Schuurmans and Maarten Bosma and brian ichter and Fei Xia and Ed H. Chi and Quoc V Le and Denny Zhou},
booktitle={Advances in Neural Information Processing Systems},
year={2022},
}

@inproceedings{wang2022iteratively,
  author    = {Boshi Wang and Xiang Deng and Huan Sun},
  title     = {Iteratively prompt pre-trained language models for chain of thought},
  booktitle = {Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing},
  pages     = {2714--2730},
  year      = {2022},
}
@inproceedings{
zelikman2022star,
title={{ST}aR: Bootstrapping Reasoning With Reasoning},
author={Eric Zelikman and Yuhuai Wu and Jesse Mu and Noah Goodman},
booktitle={Advances in Neural Information Processing Systems},
year={2022},
}
@inproceedings{liu2023crystal,
  author    = {Jiacheng Liu and Ramakanth Pasunuru and Hannaneh Hajishirzi and Yejin Choi and Asli Celikyilmaz},
  title     = {Crystal: Introspective reasoners reinforced with self-feedback},
  booktitle = {Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing},
  pages     = {11557--11572},
  year      = {2023},
}
@inproceedings{sun2023recitation,
  author    = {Zhiqing Sun and Xuezhi Wang and Yi Tay and Yiming Yang and Denny Zhou},
  title     = {Recitation-augmented language models},
  booktitle = {International Conference on Learning Representations},
  year      = {2023}
}
@inproceedings{zhou2023lima,
  author    = {Chunting Zhou and Pengfei Liu and Puxin Xu and Srini Iyer and Jiao Sun and Yuning Mao and Xuezhe Ma and Avia Efrat and Ping Yu and LILI YU and Susan Zhang and Gargi Ghosh and Mike Lewis and Luke Zettlemoyer and Omer Levy},
  title     = {LIMA: Less is more for alignment},
booktitle={Advances in Neural Information Processing Systems},
  year      = {2023}
}
@inproceedings{li2020train,
  author    = {Zhuohan Li and Eric Wallace and Sheng Shen and Kevin Lin and Kurt Keutzer and Dan Klein and Joey Gonzalez},
  title     = {Train big, then compress: Rethinking model size for efficient training and inference of transformers},
  booktitle = {Proceedings of the 37th International Conference on Machine Learning},
  pages     = {5958--5968},
  year      = {2020},
}


@inproceedings{press2023measuring,
  author    = {Ofir Press and Muru Zhang and Sewon Min and Ludwig Schmidt and Noah Smith and Mike Lewis},
  title     = {Measuring and narrowing the compositionality gap in language models},
  booktitle = {Findings of the Association for Computational Linguistics: EMNLP 2023},
  pages     = {5687--5711},
  year      = {2023},
}
@misc{yang2024largelanguagemodelslatently,
      title={Do Large Language Models Latently Perform Multi-Hop Reasoning?}, 
      author={Sohee Yang and Elena Gribovskaya and Nora Kassner and Mor Geva and Sebastian Riedel},
      year={2024},
      eprint={2402.16837},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2402.16837}, 
}
@article{radford2019language,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  journal={OpenAI blog},
  volume={1},
  number={8},
  pages={9},
  year={2019}
}
@misc{power2022grokking,
      title={Grokking: Generalization Beyond Overfitting on Small Algorithmic Datasets}, 
      author={Alethea Power and Yuri Burda and Harri Edwards and Igor Babuschkin and Vedant Misra},
      year={2022},
      eprint={2201.02177},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2201.02177}, 
}
@inproceedings{
tirumala2022memorization,
title={Memorization Without Overfitting:  Analyzing the Training Dynamics of Large Language Models},
author={Kushal Tirumala and Aram H. Markosyan and Luke Zettlemoyer and Armen Aghajanyan},
booktitle={Advances in Neural Information Processing Systems},
year={2022},
}
@inproceedings{loshchilov2019decoupled,
  title        = {Decoupled weight decay regularization},
  author       = {Ilya Loshchilov and Frank Hutter},
  booktitle    = {International Conference on Learning Representations},
  year         = {2019}
}

@inproceedings{paszke2019pytorch,
  title        = {PyTorch: An Imperative Style, High-Performance Deep Learning Library},
  author       = {Adam Paszke and Sam Gross and Francisco Massa and Adam Lerer and James Bradbury and Gregory Chanan and Trevor Killeen and Zeming Lin and Natalia Gimelshein and Luca Antiga and others},
  booktitle    = {Advances in Neural Information Processing Systems},
  year         = {2019}
}

@inproceedings{wolf2020transformers,
  title        = {Transformers: State-of-the-Art Natural Language Processing},
  author       = {Thomas Wolf and Lysandre Debut and Victor Sanh and Julien Chaumond and Clement Delangue and Anthony Moi and Pierric Cistac and Tim Rault and Remi Louf and Morgan Funtowicz and Joe Davison and Sam Shleifer and Patrick von Platen and Clara Ma and Yacine Jernite and Julien Plu and Canwen Xu and Teven Le Scao and Sylvain Gugger and Mariama Drame and Quentin Lhoest and Alexander Rush},
  booktitle    = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations},
  pages        = {38--45},
  year         = {2020},
}

@inproceedings{
cabannes2024iteration,
title={Iteration Head: A Mechanistic Study of Chain-of-Thought},
author={Vivien Cabannes and Charles Arnal and Wassim Bouaziz and Xingyu Alice Yang and Francois Charton and Julia Kempe},
booktitle    = {Advances in Neural Information Processing Systems},
year={2024},
}

@misc{nostalgebraist2020interpreting,
  title={Interpreting GPT: The Logit Lens},
  author={Nostalgebraist},
  year={2020},
  journal={},
}

@book{pearl2009causality,
  title={Causality: Models, Reasoning, and Inference},
  author={Pearl, Judea},
  year={2009},
  publisher={Cambridge University Press},
  address={Cambridge},
  isbn={9780521426085}
}


@inproceedings{dar2023analyzing,
  title={Analyzing Transformers in Embedding Space},
  author={Dar, Guy and Geva, Mor and Gupta, Ankit and Berant, Jonathan},
  booktitle={Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  year={2023},
  pages={16124--16170},
}
@inproceedings{geva2023dissecting,
  title={Dissecting Recall of Factual Associations in Auto-Regressive Language Models},
  author={Geva, Mor and Bastings, Jasmijn and Filippova, Katja and Globerson, Amir},
  booktitle={Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing},
  year={2023},
  pages={12216--12235},
}
@inproceedings{katz2023visit,
  title={VISIT: Visualizing and Interpreting the Semantic Information Flow of Transformers},
  author={Katz, Shahar and Belinkov, Yonatan},
  booktitle={Findings of the Empirical Methods in Natural Language Processing},
  year={2023},
  pages={14094--14113},
}
@inproceedings{sakarvadia2023memory,
  title={Memory Injections: Correcting Multi-Hop Reasoning Failures During Inference in Transformer-Based Language Models},
  author={Sakarvadia, Mansi and Ajith, Aswathy and Khan, Arham and Grzenda, Daniel and Hudson, Nathaniel and Bauer, Andr{\'e} and Chard, Kyle and Foster, Ian},
  booktitle={Proceedings of the 6th BlackboxNLP Workshop: Analyzing and Interpreting Neural Networks for NLP},
  year={2023},
  pages={342--356},
}


@inproceedings{vig2020investigating,
  title={Investigating Gender Bias in Language Models Using Causal Mediation Analysis},
  author={Vig, Jesse and Gehrmann, Sebastian and Belinkov, Yonatan and Qian, Sharon and Nevo, Daniel and Singer, Yaron and Shieber, Stuart},
  booktitle={Advances in Neural Information Processing Systems},
  volume={33},
  pages={12388--12401},
  year={2020},
}
@inproceedings{wang2023interpretability,
  title={Interpretability in the Wild: A Circuit for Indirect Object Identification in GPT-2 Small},
  author={Wang, Kevin Ro and Variengien, Alexandre and Conmy, Arthur and Shlegeris, Buck and Steinhardt, Jacob},
  booktitle={International Conference on Learning Representations},
  year={2023},
}
@inproceedings{meng2022locating,
  title={Locating and Editing Factual Associations in GPT},
  author={Meng, Kevin and Bau, David and Andonian, Alex J and Belinkov, Yonatan},
  booktitle={Advances in Neural Information Processing Systems},
  year={2022},
}
@inproceedings{hanna2023gpt2,
  title={How Does GPT-2 Compute Greater-Than?: Interpreting Mathematical Abilities in a Pre-Trained Language Model},
  author={Hanna, Michael and Liu, Ollie and Variengien, Alexandre},
  booktitle={Advances in Neural Information Processing Systems},
    year={2023}
}
@inproceedings{feng2024binding,
  title={How Do Language Models Bind Entities in Context?},
  author={Feng, Jiahai and Steinhardt, Jacob},
  booktitle={International Conference on Learning Representations},
  year={2024},
}
@inproceedings{
    biran2024hoppinglateexploringlimitations,
      title={Hopping Too Late: Exploring the Limitations of Large Language Models on Multi-Hop Queries}, 
      author={Eden Biran and Daniela Gottesman and Sohee Yang and Mor Geva and Amir Globerson},
      year={2024},
  booktitle={Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing},
}
@inproceedings{li-etal-2024-understanding,
    title = {Understanding and Patching Compositional Reasoning in {LLM}s},
    author = {Li, Zhaoyi  and
      Jiang, Gangwei  and
      Xie, Hong  and
      Song, Linqi  and
      Lian, Defu  and
      Wei, Ying},
    editor = {Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek},
    booktitle = {Findings of the Association for Computational Linguistics},
    year = {2024},
    pages = {9668--9688},
}
@inproceedings{
ramesh2024compositional,
title={Compositional Capabilities of Autoregressive Transformers: A Study on Synthetic, Interpretable Tasks},
author={Rahul Ramesh and Ekdeep Singh Lubana and Mikail Khona and Robert P. Dick and Hidenori Tanaka},
booktitle={International Conference on Machine Learning},
year={2024},
}

@article{touvron2023llama,
  title={Llama: Open and efficient foundation language models},
  author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timothée and Rozière, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others},
  journal={arXiv preprint arXiv:2302.13971},
  year={2023}
}

@article{Wikidata,
author = {Vrande\v{c}i\'{c}, Denny and Kr\"{o}tzsch, Markus},
title = {Wikidata: a free collaborative knowledgebase},
year = {2014},
journal = {Commun. ACM},
}

@inproceedings{lora,
      title={LoRA: Low-Rank Adaptation of Large Language Models}, 
      author={Edward J. Hu and Yelong Shen and Phillip Wallis and Zeyuan Allen-Zhu and Yuanzhi Li and Shean Wang and Lu Wang and Weizhu Chen},
      year={2022},
  booktitle={International Conference on Learning Representations},
}

@inproceedings{vaswani2017attention,
 author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, \L ukasz and Polosukhin, Illia},
 booktitle = {Advances in Neural Information Processing Systems},
 title = {Attention is All you Need},
 year = {2017}
}


@inproceedings{
zhang2024towards,
title={Towards Best Practices of Activation Patching in Language Models: Metrics and Methods},
author={Fred Zhang and Neel Nanda},
booktitle={International Conference on Learning Representations},
year={2024},
}


@inproceedings{xu-etal-2022-model,
    title = "Does Your Model Classify Entities Reasonably? Diagnosing and Mitigating Spurious Correlations in Entity Typing",
    author = "Xu, Nan  and
      Wang, Fei  and
      Li, Bangzheng  and
      Dong, Mingtao  and
      Chen, Muhao",
    booktitle = "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
    year = "2022",
    pages = "8642--8658",
}

@inproceedings{wang-etal-2023-causal,
    title = "A Causal View of Entity Bias in (Large) Language Models",
    author = "Wang, Fei  and
      Mo, Wenjie  and
      Wang, Yiwei  and
      Zhou, Wenxuan  and
      Chen, Muhao",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2023",
    year = "2023",
    pages = "15173--15184",
}

@inproceedings{ju-etal-2024-investigating,
    title = "Investigating Multi-Hop Factual Shortcuts in Knowledge Editing of Large Language Models",
    author = "Ju, Tianjie  and
      Chen, Yijin  and
      Yuan, Xinwei  and
      Zhang, Zhuosheng  and
      Du, Wei  and
      Zheng, Yubin  and
      Liu, Gongshen",
    booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    year = "2024",
    pages = "8987--9001",
}

@misc{deepseekai2025deepseekr1incentivizingreasoningcapability,
      title={DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning}, 
      author={DeepSeek-AI and Daya Guo and Dejian Yang and Haowei Zhang and Junxiao Song and Ruoyu Zhang and Runxin Xu and Qihao Zhu and Shirong Ma and Peiyi Wang and Xiao Bi and Xiaokang Zhang and Xingkai Yu and Yu Wu and Z. F. Wu and Zhibin Gou and Zhihong Shao and Zhuoshu Li and Ziyi Gao and Aixin Liu and Bing Xue and Bingxuan Wang and Bochao Wu and Bei Feng and Chengda Lu and Chenggang Zhao and Chengqi Deng and Chenyu Zhang and Chong Ruan and Damai Dai and Deli Chen and Dongjie Ji and Erhang Li and Fangyun Lin and Fucong Dai and Fuli Luo and Guangbo Hao and Guanting Chen and Guowei Li and H. Zhang and Han Bao and Hanwei Xu and Haocheng Wang and Honghui Ding and Huajian Xin and Huazuo Gao and Hui Qu and Hui Li and Jianzhong Guo and Jiashi Li and Jiawei Wang and Jingchang Chen and Jingyang Yuan and Junjie Qiu and Junlong Li and J. L. Cai and Jiaqi Ni and Jian Liang and Jin Chen and Kai Dong and Kai Hu and Kaige Gao and Kang Guan and Kexin Huang and Kuai Yu and Lean Wang and Lecong Zhang and Liang Zhao and Litong Wang and Liyue Zhang and Lei Xu and Leyi Xia and Mingchuan Zhang and Minghua Zhang and Minghui Tang and Meng Li and Miaojun Wang and Mingming Li and Ning Tian and Panpan Huang and Peng Zhang and Qiancheng Wang and Qinyu Chen and Qiushi Du and Ruiqi Ge and Ruisong Zhang and Ruizhe Pan and Runji Wang and R. J. Chen and R. L. Jin and Ruyi Chen and Shanghao Lu and Shangyan Zhou and Shanhuang Chen and Shengfeng Ye and Shiyu Wang and Shuiping Yu and Shunfeng Zhou and Shuting Pan and S. S. Li and Shuang Zhou and Shaoqing Wu and Shengfeng Ye and Tao Yun and Tian Pei and Tianyu Sun and T. Wang and Wangding Zeng and Wanjia Zhao and Wen Liu and Wenfeng Liang and Wenjun Gao and Wenqin Yu and Wentao Zhang and W. L. Xiao and Wei An and Xiaodong Liu and Xiaohan Wang and Xiaokang Chen and Xiaotao Nie and Xin Cheng and Xin Liu and Xin Xie and Xingchao Liu and Xinyu Yang and Xinyuan Li and Xuecheng Su and Xuheng Lin and X. Q. Li and Xiangyue Jin and Xiaojin Shen and Xiaosha Chen and Xiaowen Sun and Xiaoxiang Wang and Xinnan Song and Xinyi Zhou and Xianzu Wang and Xinxia Shan and Y. K. Li and Y. Q. Wang and Y. X. Wei and Yang Zhang and Yanhong Xu and Yao Li and Yao Zhao and Yaofeng Sun and Yaohui Wang and Yi Yu and Yichao Zhang and Yifan Shi and Yiliang Xiong and Ying He and Yishi Piao and Yisong Wang and Yixuan Tan and Yiyang Ma and Yiyuan Liu and Yongqiang Guo and Yuan Ou and Yuduan Wang and Yue Gong and Yuheng Zou and Yujia He and Yunfan Xiong and Yuxiang Luo and Yuxiang You and Yuxuan Liu and Yuyang Zhou and Y. X. Zhu and Yanhong Xu and Yanping Huang and Yaohui Li and Yi Zheng and Yuchen Zhu and Yunxian Ma and Ying Tang and Yukun Zha and Yuting Yan and Z. Z. Ren and Zehui Ren and Zhangli Sha and Zhe Fu and Zhean Xu and Zhenda Xie and Zhengyan Zhang and Zhewen Hao and Zhicheng Ma and Zhigang Yan and Zhiyu Wu and Zihui Gu and Zijia Zhu and Zijun Liu and Zilin Li and Ziwei Xie and Ziyang Song and Zizheng Pan and Zhen Huang and Zhipeng Xu and Zhongyu Zhang and Zhen Zhang},
      year={2025},
      eprint={2501.12948},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2501.12948}, 
}
@misc{li2024happenedllmslayerstrained,
      title={What Happened in LLMs Layers when Trained for Fast vs. Slow Thinking: A Gradient Perspective}, 
      author={Ming Li and Yanhong Li and Tianyi Zhou},
      year={2024},
      eprint={2410.23743},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2410.23743}, 
}

@inproceedings{
feng2023towards,
title={Towards Revealing the Mystery behind Chain of Thought: A Theoretical Perspective},
author={Guhao Feng and Bohang Zhang and Yuntian Gu and Haotian Ye and Di He and Liwei Wang},
booktitle={Advances in Neural Information Processing Systems},
year={2023},
}

% intro
@misc{yue2023mammothbuildingmathgeneralist,
      title={MAmmoTH: Building Math Generalist Models through Hybrid Instruction Tuning}, 
      author={Xiang Yue and Xingwei Qu and Ge Zhang and Yao Fu and Wenhao Huang and Huan Sun and Yu Su and Wenhu Chen},
      year={2023},
      eprint={2309.05653},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2309.05653}, 
}
@inproceedings{
yu2024metamath,
title={MetaMath: Bootstrap Your Own Mathematical Questions for Large Language Models},
author={Longhui Yu and Weisen Jiang and Han Shi and Jincheng YU and Zhengying Liu and Yu Zhang and James Kwok and Zhenguo Li and Adrian Weller and Weiyang Liu},
booktitle={International Conference on Learning Representations},
year={2024},
}
@inproceedings{wang-etal-2024-math,
    title = "Math-Shepherd: Verify and Reinforce {LLM}s Step-by-step without Human Annotations",
    author = "Wang, Peiyi  and
      Li, Lei  and
      Shao, Zhihong  and
      Xu, Runxin  and
      Dai, Damai  and
      Li, Yifei  and
      Chen, Deli  and
      Wu, Yu  and
      Sui, Zhifang",
    booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    year = "2024",
    pages = "9426--9439",
}
@inproceedings{
havrilla2024teaching,
title={Teaching Large Language Models to Reason with Reinforcement Learning},
author={Alexander Havrilla and Yuqing Du and Sharath Chandra Raparthy and Christoforos Nalmpantis and Jane Dwivedi-Yu and Eric Hambro and Sainbayar Sukhbaatar and Roberta Raileanu},
booktitle={AI for Math Workshop @ ICML 2024},
year={2024},
}
@misc{shao2024deepseekmathpushinglimitsmathematical,
      title={DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models}, 
      author={Zhihong Shao and Peiyi Wang and Qihao Zhu and Runxin Xu and Junxiao Song and Xiao Bi and Haowei Zhang and Mingchuan Zhang and Y. K. Li and Y. Wu and Daya Guo},
      year={2024},
      eprint={2402.03300},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2402.03300}, 
}
@misc{yu2024flowreasoningtrainingllmsdivergent,
      title={Flow of Reasoning:Training LLMs for Divergent Problem Solving with Minimal Examples}, 
      author={Fangxu Yu and Lai Jiang and Haoqiang Kang and Shibo Hao and Lianhui Qin},
      year={2024},
      eprint={2406.05673},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2406.05673}, 
}
@inproceedings{
kim2023the,
title={The CoT Collection: Improving Zero-shot and Few-shot Learning of Language Models via Chain-of-Thought Fine-Tuning},
author={Seungone Kim and Se June Joo and Doyoung Kim and Joel Jang and Seonghyeon Ye and Jamin Shin and Minjoon Seo},
booktitle={The 2023 Conference on Empirical Methods in Natural Language Processing},
year={2023},
}
@inproceedings{hsieh-etal-2023-distilling,
    title = "Distilling Step-by-Step! Outperforming Larger Language Models with Less Training Data and Smaller Model Sizes",
    author = "Hsieh, Cheng-Yu  and
      Li, Chun-Liang  and
      Yeh, Chih-kuan  and
      Nakhost, Hootan  and
      Fujii, Yasuhisa  and
      Ratner, Alex  and
      Krishna, Ranjay  and
      Lee, Chen-Yu  and
      Pfister, Tomas",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2023",
    year = "2023",
    pages = "8003--8017",
}
@inproceedings{ho-etal-2023-large,
    title = "Large Language Models Are Reasoning Teachers",
    author = "Ho, Namgyu  and
      Schmid, Laura  and
      Yun, Se-Young",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    year = "2023",
    pages = "14852--14882",
}

@inproceedings{trung-etal-2024-reft,
    title = "{R}e{FT}: Reasoning with Reinforced Fine-Tuning",
    author = "Trung, Luong  and
      Zhang, Xinbo  and
      Jie, Zhanming  and
      Sun, Peng  and
      Jin, Xiaoran  and
      Li, Hang",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    year = "2024",
    pages = "7601--7614",
}
@misc{hao2024traininglargelanguagemodels,
      title={Training Large Language Models to Reason in a Continuous Latent Space}, 
      author={Shibo Hao and Sainbayar Sukhbaatar and DiJia Su and Xian Li and Zhiting Hu and Jason Weston and Yuandong Tian},
      year={2024},
      eprint={2412.06769},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2412.06769}, 
}

@inproceedings{
zhou2023leasttomost,
title={Least-to-Most Prompting Enables Complex Reasoning in Large Language Models},
author={Denny Zhou and Nathanael Sch{\"a}rli and Le Hou and Jason Wei and Nathan Scales and Xuezhi Wang and Dale Schuurmans and Claire Cui and Olivier Bousquet and Quoc V Le and Ed H. Chi},
booktitle={International Conference on Learning Representations },
year={2023},
}
@inproceedings{khot2023decomposedpromptingmodularapproach,
      title={Decomposed Prompting: A Modular Approach for Solving Complex Tasks}, 
      author={Tushar Khot and Harsh Trivedi and Matthew Finlayson and Yao Fu and Kyle Richardson and Peter Clark and Ashish Sabharwal},
booktitle={International Conference on Learning Representations },
year={2023},
}



@misc{yin2025enhancinggeneralizationchainthought,
      title={Enhancing Generalization in Chain of Thought Reasoning for Smaller Models}, 
      author={Maxwell J. Yin and Dingyi Jiang and Yongbing Chen and Boyu Wang and Charles Ling},
      year={2025},
      eprint={2501.09804},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2501.09804}, 
}
@misc{prabhakar2024decipheringfactorsinfluencingefficacy,
      title={Deciphering the Factors Influencing the Efficacy of Chain-of-Thought: Probability, Memorization, and Noisy Reasoning}, 
      author={Akshara Prabhakar and Thomas L. Griffiths and R. Thomas McCoy},
      year={2024},
      eprint={2407.01687},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2407.01687}, 
}

@inproceedings{merrill2024expressivepowertransformerschain,
      title={The Expressive Power of Transformers with Chain of Thought}, 
      author={William Merrill and Ashish Sabharwal},
      year={2024},
    booktitle={International Conference on Learning Representations },
}
@inproceedings{
li2024chain,
title={Chain of Thought Empowers Transformers to Solve Inherently Serial Problems},
author={Zhiyuan Li and Hong Liu and Denny Zhou and Tengyu Ma},
booktitle={International Conference on Learning Representations},
year={2024},
}

@inproceedings{hao-etal-2023-reasoning,
    title = "Reasoning with Language Model is Planning with World Model",
    author = "Hao, Shibo  and
      Gu, Yi  and
      Ma, Haodi  and
      Hong, Joshua  and
      Wang, Zhen  and
      Wang, Daisy  and
      Hu, Zhiting",
    booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
    year = "2023",
    pages = "8154--8173",
}
@article{lecun2022path,
  author    = {Yann LeCun},
  title     = {A Path Towards Autonomous Machine Intelligence Version 0.9.2},
  journal   = {Open Review},
  volume    = {62},
  number    = {1},
  pages     = {1--62},
  year      = {2022},
  note      = {2022-06-27}
}

@inproceedings{
xie2023selfevaluation,
title={Self-Evaluation Guided Beam Search for Reasoning},
author={Yuxi Xie and Kenji Kawaguchi and Yiran Zhao and Xu Zhao and Min-Yen Kan and Junxian He and Qizhe Xie},
booktitle={Advances in Neural Information Processing Systems},
year={2023},
}
@inproceedings{
yao2023tree,
title={Tree of Thoughts: Deliberate Problem Solving with Large Language Models},
author={Shunyu Yao and Dian Yu and Jeffrey Zhao and Izhak Shafran and Thomas L. Griffiths and Yuan Cao and Karthik R Narasimhan},
booktitle={Advances in Neural Information Processing Systems},
year={2023},
}
@inproceedings{
hao2024llm,
title={{LLM} Reasoners: New Evaluation, Library, and Analysis of Step-by-Step Reasoning with Large Language Models},
author={Shibo Hao and Yi Gu and Haotian Luo and Tianyang Liu and Xiyan Shao and Xinyuan Wang and Shuhua Xie and Haodi Ma and Adithya Samavedhi and Qiyue Gao and Zhen Wang and Zhiting Hu},
booktitle={First Conference on Language Modeling},
year={2024},
}
@inproceedings{
lehnert2024beyond,
title={Beyond A*: Better {LLM} planning via Search Dynamics Bootstrapping},
author={Lucas Lehnert and Sainbayar Sukhbaatar and Paul McVay and Michael Rabbat and Yuandong Tian},
booktitle={ICLR 2024 Workshop on Large Language Model (LLM) Agents},
year={2024},
}
@inproceedings{
gandhi2024stream,
title={Stream of Search (SoS): Learning to Search in Language},
author={Kanishk Gandhi and Denise H J Lee and Gabriel Grand and Muxin Liu and Winson Cheng and Archit Sharma and Noah Goodman},
booktitle={First Conference on Language Modeling},
year={2024},
}
@misc{su2024dualformercontrollablefastslow,
      title={Dualformer: Controllable Fast and Slow Thinking by Learning with Randomized Reasoning Traces}, 
      author={DiJia Su and Sainbayar Sukhbaatar and Michael Rabbat and Yuandong Tian and Qinqing Zheng},
      year={2024},
      eprint={2410.09918},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2410.09918}, 
}



@InProceedings{pmlr-v80-lake18a,
  title = 	 {Generalization without Systematicity: On the Compositional Skills of Sequence-to-Sequence Recurrent Networks},
  author =       {Lake, Brenden and Baroni, Marco},
  booktitle = 	 {Proceedings of the 35th International Conference on Machine Learning},
  pages = 	 {2873--2882},
  year = 	 {2018},
}
@inproceedings{onoe-etal-2023-lms,
    title = "Can {LM}s Learn New Entities from Descriptions? Challenges in Propagating Injected Knowledge",
    author = "Onoe, Yasumasa  and
      Zhang, Michael  and
      Padmanabhan, Shankar  and
      Durrett, Greg  and
      Choi, Eunsol",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    year = "2023",
    pages = "5469--5485",
}
@inproceedings{zhong-etal-2023-mquake,
    title = "{MQ}u{AKE}: Assessing Knowledge Editing in Language Models via Multi-Hop Questions",
    author = "Zhong, Zexuan  and
      Wu, Zhengxuan  and
      Manning, Christopher  and
      Potts, Christopher  and
      Chen, Danqi",
    booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
    year = "2023",
    pages = "15686--15702",
}
@article{10.1162/tacl_a_00644,
    author = {Cohen, Roi and Biran, Eden and Yoran, Ori and Globerson, Amir and Geva, Mor},
    title = {Evaluating the Ripple Effects of Knowledge Editing in Language Models},
    journal = {Transactions of the Association for Computational Linguistics},
    volume = {12},
    pages = {283-298},
    year = {2024},
}

@inproceedings{brinkmann-etal-2024-mechanistic,
    title = "A Mechanistic Analysis of a Transformer Trained on a Symbolic Multi-Step Reasoning Task",
    author = "Brinkmann, Jannik  and
      Sheshadri, Abhay  and
      Levoso, Victor  and
      Swoboda, Paul  and
      Bartelt, Christian",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2024",
    year = "2024",
    pages = "4082--4102",
}

@inproceedings{stolfo-etal-2023-mechanistic,
    title = "A Mechanistic Interpretation of Arithmetic Reasoning in Language Models using Causal Mediation Analysis",
    author = "Stolfo, Alessandro  and
      Belinkov, Yonatan  and
      Sachan, Mrinmaya",
    booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
    year = "2023",
    pages = "7035--7052",
}
@inproceedings{rai-yao-2024-investigation,
    title = "An Investigation of Neuron Activation as a Unified Lens to Explain Chain-of-Thought Eliciting Arithmetic Reasoning of {LLM}s",
    author = "Rai, Daking  and
      Yao, Ziyu",
    booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    year = "2024",
    pages = "7174--7193",
}
@inproceedings{
yao2024knowledge,
title={Knowledge Circuits in Pretrained Transformers},
author={Yunzhi Yao and Ningyu Zhang and Zekun Xi and Mengru Wang and Ziwen Xu and Shumin Deng and Huajun Chen},
booktitle={Advances in Neural Information Processing Systems},
year={2024},
}

@inproceedings{
nanda2023progress,
title={Progress measures for grokking via mechanistic interpretability},
author={Neel Nanda and Lawrence Chan and Tom Lieberum and Jess Smith and Jacob Steinhardt},
booktitle={International Conference on Learning Representations },
year={2023},
}
@inproceedings{
conmy2023towards,
title={Towards Automated Circuit Discovery for Mechanistic Interpretability},
author={Arthur Conmy and Augustine N. Mavor-Parker and Aengus Lynch and Stefan Heimersheim and Adri{\`a} Garriga-Alonso},
booktitle={Advances in Neural Information Processing Systems},
year={2023},
}
@misc{shalev2024distributionalreasoningllmsparallel,
      title={Distributional reasoning in LLMs: Parallel reasoning processes in multi-hop reasoning}, 
      author={Yuval Shalev and Amir Feder and Ariel Goldstein},
      year={2024},
      eprint={2406.13858},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2406.13858}, 
}
@inproceedings{ghandehariounpatchscopes,
  title={Patchscopes: A Unifying Framework for Inspecting Hidden Representations of Language Models},
  author={Ghandeharioun, Asma and Caciularu, Avi and Pearce, Adam and Dixon, Lucas and Geva, Mor},
  booktitle={International Conference on Machine Learning},
year={2024}
}
@misc{deng2023implicitchainthoughtreasoning,
      title={Implicit Chain of Thought Reasoning via Knowledge Distillation}, 
      author={Yuntian Deng and Kiran Prasad and Roland Fernandez and Paul Smolensky and Vishrav Chaudhary and Stuart Shieber},
      year={2023},
      eprint={2311.01460},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2311.01460}, 
}
@misc{deng2024explicitcotimplicitcot,
      title={From Explicit CoT to Implicit CoT: Learning to Internalize CoT Step by Step}, 
      author={Yuntian Deng and Yejin Choi and Stuart Shieber},
      year={2024},
      eprint={2405.14838},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2405.14838}, 
}
@inproceedings{giannou2023looped,
  author    = {Angeliki Giannou and Shashank Rajput and Jy-yong Sohn and Kangwook Lee and Jason D Lee and Dimitris Papailiopoulos},
  title     = {Looped Transformers as Programmable Computers},
  booktitle = {Proceedings of the International Conference on Machine Learning},
  pages     = {11398--11442},
  year      = {2023},
}
@misc{fan2024loopedtransformerslengthgeneralization,
      title={Looped Transformers for Length Generalization}, 
      author={Ying Fan and Yilun Du and Kannan Ramchandran and Kangwook Lee},
      year={2024},
      eprint={2409.15647},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2409.15647}, 
}

@inproceedings{katz-etal-2024-backward,
    title = "Backward Lens: Projecting Language Model Gradients into the Vocabulary Space",
    author = "Katz, Shahar  and
      Belinkov, Yonatan  and
      Geva, Mor  and
      Wolf, Lior",
    booktitle = "Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing",
    year = "2024",
    pages = "2390--2422",
}

@inproceedings{geva-etal-2022-lm,
    title = "{LM}-Debugger: An Interactive Tool for Inspection and Intervention in Transformer-Based Language Models",
    author = "Geva, Mor  and
      Caciularu, Avi  and
      Dar, Guy  and
      Roit, Paul  and
      Sadde, Shoval  and
      Shlain, Micah  and
      Tamir, Bar  and
      Goldberg, Yoav",
    booktitle = "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing: System Demonstrations",
    year = "2022",
    pages = "12--21",
}
@inproceedings{ma-etal-2023-chain,
    title = "Chain-of-Skills: A Configurable Model for Open-Domain Question Answering",
    author = "Ma, Kaixin  and
      Cheng, Hao  and
      Zhang, Yu  and
      Liu, Xiaodong  and
      Nyberg, Eric  and
      Gao, Jianfeng",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    year = "2023",
    pages = "1599--1618",
}
@inproceedings{katz-belinkov-2023-visit,
    title = "{VISIT}: Visualizing and Interpreting the Semantic Information Flow of Transformers",
    author = "Katz, Shahar  and
      Belinkov, Yonatan",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2023",
    year = "2023",
    pages = "14094--14113",
}
@inproceedings{geva-etal-2021-transformer,
    title = "Transformer Feed-Forward Layers Are Key-Value Memories",
    author = "Geva, Mor  and
      Schuster, Roei  and
      Berant, Jonathan  and
      Levy, Omer",
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
    year = "2021",
    pages = "5484--5495",
}

@inproceedings{sanyal-ren-2021-discretized,
    title = "Discretized Integrated Gradients for Explaining Language Models",
    author = "Sanyal, Soumya  and
      Ren, Xiang",
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
    year = "2021",
    pages = "10285--10299",
}
@inproceedings{
chefer2022optimizing,
title={Optimizing Relevance Maps of Vision Transformers Improves Robustness},
author={Hila Chefer and Idan Schwartz and Lior Wolf},
booktitle={Advances in Neural Information Processing Systems},
editor={Alice H. Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho},
year={2022},
}
@inproceedings{miglani-etal-2023-using,
    title = "Using Captum to Explain Generative Language Models",
    author = "Miglani, Vivek  and
      Yang, Aobo  and
      Markosyan, Aram  and
      Garcia-Olano, Diego  and
      Kokhlikyan, Narine",
    booktitle = "Proceedings of the 3rd Workshop for Natural Language Processing Open Source Software (NLP-OSS 2023)",
    month = dec,
    year = "2023",
    pages = "165--173",
}