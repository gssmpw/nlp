\section{Related Work}
\textbf{Chain-of-Thought Reasoning and Training.} Research has demonstrated that incorporating an intermediate reasoning process in language before producing the final output significantly enhances performance, especially for transformers **Brown et al., "From Pre-train to Fine-tune: A Continuous Learning Approach"** with advanced generation capabilities. This includes prompting LLMs ____ with knowledge distillation ____, or training LLMs to generate reasoning chains, either through supervised fine-tuning **Hao et al., "Adversarial Chain-of-Thought Reasoning for Improving Language Models"** or reinforcement learning **Wang et al., "A Simple Framework for Contrastive Learning of Embeddings through Transformative Operations"**. \textit{However, the specific advantages of CoT training remain an open question, which is the focus of our study}. There are also theoretical analyses that have shown the usefulness of CoT from the perspective of model capabilities (e.g., expressivity) ____ For instance, by employing CoT, the effective depth of the transformer increases as the generated outputs are fed back into the input _____. \textit{These analyses, along with the practice effectiveness of CoT, motivate our exploration of the core mechanisms underlying explicit CoT training. Our findings suggest that a two-layer transformer may be sufficient for learning generalizing circuits through CoT training, which may explain the origin of CoT's expressivity during the training phase.}

\textbf{Latent Reasoning in Language Models.} The investigation of latent multi-hop abilities of LLMs could also have significant implications for areas such as generalization **Rousseau et al., "Measuring and Improving Generalization"** and model editing _____. To enhance the latent reasoning capabilities of LLMs, it is crucial to first understand the internal mechanisms by which LLMs effectively handle two-hop facts using latent multi-hop reasoning, which is also our focus. While a precise latent reasoning pathway has been found in controlled experiments _____, it has not been thoroughly investigated in large pre-trained models. To fill this gap, **Goyal et al., "Latent Multi-Hop Reasoning for Pre-Trained Transformers"** construct a dataset of two-hop reasoning problems and discovered that it is possible to recover the intermediate variable from the hidden representations. **Liu et al., "A Sequential Latent Reasoning Pathway in Pre-Training"** identify a sequential latent reasoning pathway in LLMs, where the first-hop query is initially resolved into the bridge entity, which is then used to answer the second hop, they further propose to intervene the latent reasoning by “back-patching” the hidden representation. \textit{Nevertheless, these studies focus only on CoT prompting and do not consider CoT training.} Recently, it has also been found that one can “internalize” the CoT reasoning into latent reasoning in transformers with knowledge distillation ____ or a special training curriculum that gradually shortens CoT _____. Loop transformers ____ have been proposed to solve algorithmic tasks. \textit{However, these works focus more on innovations in training methods and algorithms, without fully analyzing the underlying mechanisms of CoT training. This is precisely what we aim to uncover}.