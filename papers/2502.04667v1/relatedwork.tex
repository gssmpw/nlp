\section{Related Work}
\textbf{Chain-of-Thought Reasoning and Training.} Research has demonstrated that incorporating an intermediate reasoning process in language before producing the final output significantly enhances performance, especially for transformers \citep{vaswani2017attention} with advanced generation capabilities. This includes prompting LLMs \citep{wei2022chain,zhou2023leasttomost,khot2023decomposedpromptingmodularapproach}, or training LLMs to generate reasoning chains, either through supervised fine-tuning \citep{yue2023mammothbuildingmathgeneralist,yu2024metamath} or reinforcement learning \citep{wang-etal-2024-math,havrilla2024teaching,shao2024deepseekmathpushinglimitsmathematical,yu2024flowreasoningtrainingllmsdivergent}. \textit{However, the specific advantages of CoT training remain an open question, which is the focus of our study}. There are also theoretical analyses that have shown the usefulness of CoT from the perspective of model capabilities (e.g., expressivity) \citep{feng2023towards,merrill2024expressivepowertransformerschain,li2024chain,prabhakar2024decipheringfactorsinfluencingefficacy,yin2025enhancinggeneralizationchainthought}. For instance, by employing CoT, the effective depth of the transformer increases as the generated outputs are fed back into the input \citep{feng2023towards}. \textit{These analyses, along with the practice effectiveness of CoT, motivate our exploration of the core mechanisms underlying explicit CoT training. Our findings suggest that a two-layer transformer may be sufficient for learning generalizing circuits through CoT training, which may explain the origin of CoT's expressivity during the training phase.}

\textbf{Latent Reasoning in Language Models.} The investigation of latent multi-hop abilities of LLMs could also have significant implications for areas such as generalization \citep{pmlr-v80-lake18a,onoe-etal-2023-lms} and model editing \citep{zhong-etal-2023-mquake,10.1162/tacl_a_00644}. To enhance the latent reasoning capabilities of LLMs, it is crucial to first understand the internal mechanisms by which LLMs effectively handle two-hop facts using latent multi-hop reasoning, which is also our focus. While a precise latent reasoning pathway has been found in controlled experiments \citep{stolfo-etal-2023-mechanistic,nanda2023progress,conmy2023towards,brinkmann-etal-2024-mechanistic,li-etal-2024-understanding,rai-yao-2024-investigation,yao2024knowledge,wang2024grokking}, it has not been thoroughly investigated in large pre-trained models. To fill this gap, \citet{yang2024largelanguagemodelslatently} construct a dataset of two-hop reasoning problems and discovered that it is possible to recover the intermediate variable from the hidden representations. \citet{biran2024hoppinglateexploringlimitations} identify a sequential latent reasoning pathway in LLMs, where the first-hop query is initially resolved into the bridge entity, which is then used to answer the second hop, they further propose to intervene the latent reasoning by “back-patching” the hidden representation. \textit{Nevertheless, these studies focus only on CoT prompting and do not consider CoT training.} Recently, it has also been found that one can “internalize” the CoT reasoning into latent reasoning in transformers with knowledge distillation \citep{deng2023implicitchainthoughtreasoning} or a special training curriculum that gradually shortens
CoT \citep{deng2024explicitcotimplicitcot}. Loop transformers \citep{giannou2023looped,cabannes2024iteration,fan2024loopedtransformerslengthgeneralization} have been proposed to solve algorithmic tasks. \textit{However, these works focus more on innovations in training methods and algorithms, without fully analyzing the underlying mechanisms of CoT training. This is precisely what we aim to uncover}.
\newpage