\section{Related Work}
\textbf{Chain-of-Thought Reasoning and Training.} Research has demonstrated that incorporating an intermediate reasoning process in language before producing the final output significantly enhances performance, especially for transformers ____ with advanced generation capabilities. This includes prompting LLMs ____, or training LLMs to generate reasoning chains, either through supervised fine-tuning ____ or reinforcement learning ____. \textit{However, the specific advantages of CoT training remain an open question, which is the focus of our study}. There are also theoretical analyses that have shown the usefulness of CoT from the perspective of model capabilities (e.g., expressivity) ____. For instance, by employing CoT, the effective depth of the transformer increases as the generated outputs are fed back into the input ____. \textit{These analyses, along with the practice effectiveness of CoT, motivate our exploration of the core mechanisms underlying explicit CoT training. Our findings suggest that a two-layer transformer may be sufficient for learning generalizing circuits through CoT training, which may explain the origin of CoT's expressivity during the training phase.}

\textbf{Latent Reasoning in Language Models.} The investigation of latent multi-hop abilities of LLMs could also have significant implications for areas such as generalization ____ and model editing ____. To enhance the latent reasoning capabilities of LLMs, it is crucial to first understand the internal mechanisms by which LLMs effectively handle two-hop facts using latent multi-hop reasoning, which is also our focus. While a precise latent reasoning pathway has been found in controlled experiments ____, it has not been thoroughly investigated in large pre-trained models. To fill this gap, ____ construct a dataset of two-hop reasoning problems and discovered that it is possible to recover the intermediate variable from the hidden representations. ____ identify a sequential latent reasoning pathway in LLMs, where the first-hop query is initially resolved into the bridge entity, which is then used to answer the second hop, they further propose to intervene the latent reasoning by “back-patching” the hidden representation. \textit{Nevertheless, these studies focus only on CoT prompting and do not consider CoT training.} Recently, it has also been found that one can “internalize” the CoT reasoning into latent reasoning in transformers with knowledge distillation ____ or a special training curriculum that gradually shortens
CoT ____. Loop transformers ____ have been proposed to solve algorithmic tasks. \textit{However, these works focus more on innovations in training methods and algorithms, without fully analyzing the underlying mechanisms of CoT training. This is precisely what we aim to uncover}.
\newpage