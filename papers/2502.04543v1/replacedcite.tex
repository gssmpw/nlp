\section{Related Work}
\paragraph{$\Phi$-regret} The $\phi$-regret we study is an instance-dependent version of a better-known concept called the $\Phi$-regret, due to ____. With respect to any class $\Phi\subset\calS(d)$ of action modification rules, the $\Phi$-regret is defined as the supremum of $\reg_T(\phi)$ over all $\phi\in\Phi$. Due to its generality unifying external, internal and swap regret, further developments have been presented in numerous works afterwards, particularly regarding its connection to various equilibrium concepts in game theory ____. 

Technically, we build on the well-known reduction from swap regret to external regret on the extended domain $\calS(d)$, pioneered by ____ and further developed by ____. In a conceptually different manner, one may also analyze the swap regret through the \emph{subsequence regret} ____, and unifying algorithms have been proposed based on certain multi-objective formulations of online learning ____ related to the $L_\infty$-norm \emph{Blackwell approachability} of the negative orthant ____. The key idea here is to convert the subsequence regret to the regret of a ``meta'' LEA algorithm that reweighs different subsequences. In particular, utilizing the meta algorithm of ____, the approach of ____ can achieve an instance-dependent refinement of the $\tilde O(\sqrt{T})$ internal regret bound.

A recent breakthrough of ____ and ____ showed that the $d$-dependence of the time-averaged $\tilde O(\sqrt{d/T})$ swap regret bound can be improved exponentially, at the price of an exponentially worse dependence on $T$. This is orthogonal to the regime of $T\gg d$ we consider, but very intriguingly, their algorithms are also based on some sort of multi-resolution analysis. Closer to our setting, they showed that the $\tilde O(\sqrt{dT})$ swap regret is optimal in the distributional version of LEA with $T\gg d$, answering an open problem from earlier works. 

\paragraph{Features and sparsity} Our techniques are inspired by recent advances in \emph{dynamic regret} minimization, which is the hardest regret notion that compares to an arbitrary sequence of predictions selected in hindsight. ____ presented a reduction from dynamic regret to external regret on the extended domain $\R^T$, which is reminiscent of the swap-to-external reduction discussed above; also see ____. Central to this technique is the use of features to associate the complexity of a hypothesis to the sparsity of its linear representations. This suggests viewing the features of ____ as the canonical matrix basis, and our main conceptual takeaway is that the Haar wavelet, introduced to online learning by ____, produces matrix features that are better-aligned with the inherent structure of the external and internal regret. 

\paragraph{Technical challenges} We now highlight the technical challenges of this work along the lines of the above discussion. Regardless of the choice of features, the dynamic regret bound of ____ is $\tilde O(\sqrt{T}\cdot\sqrt{T})$ in the worst case, where one of the $\sqrt{T}$ is the iconic rate of online linear optimization, and the other one comes from the dimensionality of the extended domain $\R^T$. By default, it means the analogous approach for our problem would result in the suboptimal $\tilde O(\sqrt{d^2}\cdot\sqrt{T})$ swap regret. The key subtlety here is that the surrogate gradients from the swap-to-external reduction are well-structured, such that one could use \emph{``first-order'' gradient-adaptivity} to further shave off a $\sqrt{d}$ factor ____. But this does not hold true for arbitrary features anymore. 

Regarding this issue, we show that the Haar wavelet is particularly ``compatible'' with the gradient structure from the swap-to-external reduction, such that the optimal $\tilde O(\sqrt{dT})$ swap regret bound can still be recovered after incorporating specific Haar-wavelet-type matrix features. Our approach also involves a complexity-preserving augmentation of the hypothesis class $\calS(d)$ (Section~\ref{subsection:preprocessing}), as well as a projection technique that enforces the constraint $\calS(d)$ (Section~\ref{subsection:constraint}). Both are nontrivial constructions absent from ____.