\section{Related Work}
\paragraph{$\Phi$-regret} The $\phi$-regret we study is an instance-dependent version of a better-known concept called the $\Phi$-regret, due to \cite{greenwald2003general}. With respect to any class $\Phi\subset\calS(d)$ of action modification rules, the $\Phi$-regret is defined as the supremum of $\reg_T(\phi)$ over all $\phi\in\Phi$. Due to its generality unifying external, internal and swap regret, further developments have been presented in numerous works afterwards, particularly regarding its connection to various equilibrium concepts in game theory \citep{stoltz2007learning,rakhlin2011online,piliouras2022evolutionary,bernasconi2023constrained,cai2024tractable,zhang2024efficient}. 

Technically, we build on the well-known reduction from swap regret to external regret on the extended domain $\calS(d)$, pioneered by \cite{stoltz2005internal,blum2007external,gordon2008no} and further developed by \cite{ito2020tight}. In a conceptually different manner, one may also analyze the swap regret through the \emph{subsequence regret} \citep{lehrer2003wide,roth2023learning}, and unifying algorithms have been proposed based on certain multi-objective formulations of online learning \citep{lee2022online,haghtalab2023calibrated,noarov2023high} related to the $L_\infty$-norm \emph{Blackwell approachability} of the negative orthant \citep{blackwell1956analog,perchet2015exponential,shimkin2016online}. The key idea here is to convert the subsequence regret to the regret of a ``meta'' LEA algorithm that reweighs different subsequences. In particular, utilizing the meta algorithm of \cite{chen2021impossible}, the approach of \cite{roth2023learning} can achieve an instance-dependent refinement of the $\tilde O(\sqrt{T})$ internal regret bound.

A recent breakthrough of \cite{dagan2024external} and \cite{peng2024fast} showed that the $d$-dependence of the time-averaged $\tilde O(\sqrt{d/T})$ swap regret bound can be improved exponentially, at the price of an exponentially worse dependence on $T$. This is orthogonal to the regime of $T\gg d$ we consider, but very intriguingly, their algorithms are also based on some sort of multi-resolution analysis. Closer to our setting, they showed that the $\tilde O(\sqrt{dT})$ swap regret is optimal in the distributional version of LEA with $T\gg d$, answering an open problem from earlier works. 

\paragraph{Features and sparsity} Our techniques are inspired by recent advances in \emph{dynamic regret} minimization, which is the hardest regret notion that compares to an arbitrary sequence of predictions selected in hindsight. \cite{zhang2023unconstrained} presented a reduction from dynamic regret to external regret on the extended domain $\R^T$, which is reminiscent of the swap-to-external reduction discussed above; also see \citep{jacobsen2024equivalence}. Central to this technique is the use of features to associate the complexity of a hypothesis to the sparsity of its linear representations. This suggests viewing the features of \citep{blum2007external} as the canonical matrix basis, and our main conceptual takeaway is that the Haar wavelet, introduced to online learning by \cite{baby2019online}, produces matrix features that are better-aligned with the inherent structure of the external and internal regret. 

\paragraph{Technical challenges} We now highlight the technical challenges of this work along the lines of the above discussion. Regardless of the choice of features, the dynamic regret bound of \cite{zhang2023unconstrained} is $\tilde O(\sqrt{T}\cdot\sqrt{T})$ in the worst case, where one of the $\sqrt{T}$ is the iconic rate of online linear optimization, and the other one comes from the dimensionality of the extended domain $\R^T$. By default, it means the analogous approach for our problem would result in the suboptimal $\tilde O(\sqrt{d^2}\cdot\sqrt{T})$ swap regret. The key subtlety here is that the surrogate gradients from the swap-to-external reduction are well-structured, such that one could use \emph{``first-order'' gradient-adaptivity} to further shave off a $\sqrt{d}$ factor \citep{blum2007external}. But this does not hold true for arbitrary features anymore. 

Regarding this issue, we show that the Haar wavelet is particularly ``compatible'' with the gradient structure from the swap-to-external reduction, such that the optimal $\tilde O(\sqrt{dT})$ swap regret bound can still be recovered after incorporating specific Haar-wavelet-type matrix features. Our approach also involves a complexity-preserving augmentation of the hypothesis class $\calS(d)$ (Section~\ref{subsection:preprocessing}), as well as a projection technique that enforces the constraint $\calS(d)$ (Section~\ref{subsection:constraint}). Both are nontrivial constructions absent from \citep{zhang2023unconstrained}.