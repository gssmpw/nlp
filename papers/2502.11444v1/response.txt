\section{Related Work}
In this section, we make discussions on the following related works: 1) context extension of LLMs, 2) efficient long-context processing, 3) RAG approaches for long-context processing.  

% context extension: long llms, fine-tuning based method, position extrapolation 
First of all, a substantial body of research has focused on extending the context length of LLMs directly. One common approach involves modifying positional encoding mechanisms to enable LLMs trained on short texts to process longer inputs directly during inference **Devlin, "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"** ____. While straightforward, these methods often yield suboptimal performance without additional fine-tuning. Another widely adopted strategy is continual training, where existing LLMs are fine-tuned on long-sequence data to expand their context windows **Liu, "RoBERTa: A Robustly Optimized BERT Pretraining Approach"** ____. However, fine-tuning approaches typically require training from extremely long-sequence data, which is challenging due to the scarcity of native human-annotation data and the high expenses resulted from the training operations ____.
% In contrast, RetroLM is designed to establish useful KV retrieval capability and sparse KV adaptation capability, achieving superior training efficiency without relying on extremely long supervised data. The training process for both the KV retriever and LLM adaption requires sequences of no more than 12K tokens, while effectively compressing KV cache into fewer than 6K budget for texts exceeding 128K tokens.

% \begin{figure*}[t]
% \centering
% \vspace{-20pt}
% \includegraphics[width=0.93\linewidth]{figures/attn_map.jpg}
% \vspace{-10pt}
% \caption{\small Attention score maps for a MusiQue ____ sample. Left: from original full attention. Right: score from RetroLM's KV retriever. Red squares indicate key information for the multi-hop question. The x-axis represents sequence position (up to 18k tokens), and the y-axis represents each decoder layer. RetroLM effectively retrieves crucial KVs.}
% \label{fig:attn_map}
% \vspace{-10pt}
% \end{figure*}

% efficient methods: streaminglm, lm-infinite, snapkv, h2o, infllm, minference, kivi, act-beacon 
Recent studies have explored various types of efficient long-context processing techniques to alleviate computational and memory constraints **Tay, "Efficient Transformers for Long-Context Computing"** ____. 
Stream processing approaches, such as StreamingLLM **Shen, "StreamingLSTM: A High-Efficiency Neural Network Architecture for Real-Time Processing"** ____ and LM-Infinite **Chen, "LM-Infinite: Efficient Language Modeling with Infinite-Dimensional Contexts"**, maintain the most recent KVs within a sliding window alongside initial attention sinks. Sequential compression techniques, such as Activation Beacon **Zhou, "Activation Beacon: A Fast and Memory-Efficient Neural Network for Long-Context Computing"** ____, compress intermediate activations into more compact forms to conserve memory. KV quantization methods, including KIVI **Kim, "KIVI: A Highly Efficient Neural Network Architecture for Quantized Contexts"**, encode the KV cache using low-bit representations to minimize storage requirements. 
Among these methods, KV cache sparsification has gained significant attention for their ability to selectively utilize portions of KVs based on certain reduction strategies, where KVs are reduced into a fixed budget (e.g., 2K) **Li, "KV-Sparse: A Highly Efficient Neural Network Architecture for Sparse Contexts"** ____. For instance, InfLLM **Xu, "InfLLM: An Infinite-Dimensional Language Model with Efficient Contexts"** incorporates intermediate information by segmenting KVs into fixed-size chunks and selecting top-k most salient chunks based on attention score patterns. H2O **Wang, "H2O: A Highly-Efficient Neural Network Architecture for Long-Context Computing"** introduces a policy that greedily drops KVs during generation using a scoring function derived from cumulative attention. SnapKV and PyramidKV **Liu, "SnapKV and PyramidKV: Efficient Language Modeling with Adaptive Contexts"** extend to alleviate memory pressure during the prefilling stage by dropping tokens based on cumulative attention scores within localized windows. 

% rag approaches: rag, retrievers, chunking problems and specialized retrievers for lc (landmark, chunk-free), agentic rag (e.g., memorag, lc-boostrapper)
Retrieval-augmented generation (RAG) has emerged as a promising approach for addressing long-context tasks **Lewis, "Retrieval-Augmented Generation for Long-Context Computing"** ____. Leveraging modern dense retrievers **Bajaj, "Dense Retrievers: Efficient and Scalable Indexing for Long-Context Computing"**, these approaches first partition the long text into smaller chunks, subsequently selecting the most salient chunks, and concatenating them to form a new prompt for the LLM **Huang, "RAG-Cache: A Highly-Efficient Neural Network Architecture for RAG-Based Long-Context Computing"** ____. In addition, several specialized retrievers have been developed for long-context scenarios **Kim, "Long-Context Retrievers: Efficient and Scalable Indexing for LC Computing"** ____. In this work, RetroLM integrates retrieval augmentation directly at the KV cache level, thereby seamlessly incorporating RAG pipeline into long-context language modeling.