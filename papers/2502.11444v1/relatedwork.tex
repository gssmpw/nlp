\section{Related Work}
In this section, we make discussions on the following related works: 1) context extension of LLMs, 2) efficient long-context processing, 3) RAG approaches for long-context processing.  

% context extension: long llms, fine-tuning based method, position extrapolation 
First of all, a substantial body of research has focused on extending the context length of LLMs directly. One common approach involves modifying positional encoding mechanisms to enable LLMs trained on short texts to process longer inputs directly during inference \citep{chen2023extending, peng2023yarn, ding2024longrope}. While straightforward, these methods often yield suboptimal performance without additional fine-tuning. Another widely adopted strategy is continual training, where existing LLMs are fine-tuned on long-sequence data to expand their context windows \citep{li2023long, chen2023longlora, mohtashami2023landmark, xiong2023effective}. However, fine-tuning approaches typically require training from extremely long-sequence data, which is challenging due to the scarcity of native human-annotation data and the high expenses resulted from the training operations \citep{fu2024data, gao2024train}. 
% In contrast, RetroLM is designed to establish useful KV retrieval capability and sparse KV adaptation capability, achieving superior training efficiency without relying on extremely long supervised data. The training process for both the KV retriever and LLM adaption requires sequences of no more than 12K tokens, while effectively compressing KV cache into fewer than 6K budget for texts exceeding 128K tokens.

% \begin{figure*}[t]
% \centering
% \vspace{-20pt}
% \includegraphics[width=0.93\linewidth]{figures/attn_map.jpg}
% \vspace{-10pt}
% \caption{\small Attention score maps for a MusiQue \citep{bai2023longbench} sample. Left: from original full attention. Right: score from RetroLM's KV retriever. Red squares indicate key information for the multi-hop question. The x-axis represents sequence position (up to 18k tokens), and the y-axis represents each decoder layer. RetroLM effectively retrieves crucial KVs.}
% \label{fig:attn_map}
% \vspace{-10pt}
% \end{figure*}

% efficient methods: streaminglm, lm-infinite, snapkv, h2o, infllm, minference, kivi, act-beacon 
Recent studies have explored various types of efficient long-context processing techniques to alleviate computational and memory constraints \citep{sun2024shadowkv, liao2024e2llm, yang2024pyramidinfer}.
Stream processing approaches, such as StreamingLLM \citep{streamingllm} and LM-Infinite \citep{lminfinite}, maintain the most recent KVs within a sliding window alongside initial attention sinks. Sequential compression techniques, such as Activation Beacon \citep{zhang2024soaring}, compress intermediate activations into more compact forms to conserve memory. KV quantization methods, including KIVI \citep{liu2024kivi}, encode the KV cache using low-bit representations to minimize storage requirements. 
Among these methods, KV cache sparsification has gained significant attention for their ability to selectively utilize portions of KVs based on certain reduction strategies, where KVs are reduced into a fixed budget (e.g., 2K) \citep{xu2024think, quest, huang2024locret, liu2024retrievalattention, shi2024discovering}. For instance, InfLLM \citep{infllm} incorporates intermediate information by segmenting KVs into fixed-size chunks and selecting top-k most salient chunks based on attention score patterns. H2O \cite{h2o} introduces a policy that greedily drops KVs during generation using a scoring function derived from cumulative attention. SnapKV and PyramidKV \citep{snapkv, pyramidkv} extend to alleviate memory pressure during the prefilling stage by dropping tokens based on cumulative attention scores within localized windows. 

% rag approaches: rag, retrievers, chunking problems and specialized retrievers for lc (landmark, chunk-free), agentic rag (e.g., memorag, lc-boostrapper)
Retrieval-augmented generation (RAG) has emerged as a promising approach for addressing long-context tasks \citep{xu2023retrieval, li2024long, yue2024inference}. Leveraging modern dense retrievers \citep{karpukhin2020dense, xiao2023c}, these approaches first partition the long text into smaller chunks, subsequently selecting the most salient chunks, and concatenating them to form a new prompt for the LLM \citep{zhao2024retrieval}. In addition, several specialized retrievers have been developed for long-context scenarios \citep{luo2024bge, gunther2023jina}. In this work, RetroLM integrates retrieval augmentation directly at the KV cache level, thereby seamlessly incorporating RAG pipeline into long-context language modeling.