@article{team2024gemini,
  title={Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context},
  author={Team, Gemini and Georgiev, Petko and Lei, Ving Ian and Burnell, Ryan and Bai, Libin and Gulati, Anmol and Tanzer, Garrett and Vincent, Damien and Pan, Zhufeng and Wang, Shibo and others},
  journal={arXiv preprint arXiv:2403.05530},
  year={2024}
}

@article{caciularu2023peek,
  title={Peek across: Improving multi-document modeling via cross-document question-answering},
  author={Caciularu, Avi and Peters, Matthew E and Goldberger, Jacob and Dagan, Ido and Cohan, Arman},
  journal={arXiv preprint arXiv:2305.15387},
  year={2023}
}

@article{bai2023longbench,
  title={Longbench: A bilingual, multitask benchmark for long context understanding},
  author={Bai, Yushi and Lv, Xin and Zhang, Jiajie and Lyu, Hongchang and Tang, Jiankai and Huang, Zhidian and Du, Zhengxiao and Liu, Xiao and Zeng, Aohan and Hou, Lei and others},
  journal={arXiv preprint arXiv:2308.14508},
  year={2023}
}

@inproceedings{infbench,
  title={InfBench: Extending long context evaluation beyond 100k tokens},
  author={Zhang, Xinrong and Chen, Yingfa and Hu, Shengding and Xu, Zihang and Chen, Junhao and Hao, Moo and Han, Xu and Thai, Zhen and Wang, Shuo and Liu, Zhiyuan and others},
  booktitle={Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={15262--15277},
  year={2024}
}

@article{hsieh2024ruler,
  title={RULER: What's the Real Context Size of Your Long-Context Language Models?},
  author={Hsieh, Cheng-Ping and Sun, Simeng and Kriman, Samuel and Acharya, Shantanu and Rekesh, Dima and Jia, Fei and Zhang, Yang and Ginsburg, Boris},
  journal={arXiv preprint arXiv:2404.06654},
  year={2024}
}

@article{an2024training,
  title={Training-free long-context scaling of large language models},
  author={An, Chenxin and Huang, Fei and Zhang, Jun and Gong, Shansan and Qiu, Xipeng and Zhou, Chang and Kong, Lingpeng},
  journal={arXiv preprint arXiv:2402.17463},
  year={2024}
}

@article{gur2023real,
  title={A real-world webagent with planning, long context understanding, and program synthesis},
  author={Gur, Izzeddin and Furuta, Hiroki and Huang, Austin and Safdari, Mustafa and Matsuo, Yutaka and Eck, Douglas and Faust, Aleksandra},
  journal={arXiv preprint arXiv:2307.12856},
  year={2023}
}

@misc{openai2024learning,
  title={Learning to Reason with LLMs},
  author={{OpenAI}},
  howpublished={\url{https://openai.com/index/learning-to-reason-with-llms/}},
  year={2024},
  note={Accessed: 2024-12-18}
}

@article{jin2024long,
  title={Long-Context LLMs Meet RAG: Overcoming Challenges for Long Inputs in RAG},
  author={Jin, Bowen and Yoon, Jinsung and Han, Jiawei and Arik, Sercan O},
  journal={arXiv preprint arXiv:2410.05983},
  year={2024}
}

@article{streamingllm,
  title={Efficient streaming language models with attention sinks},
  author={Xiao, Guangxuan and Tian, Yuandong and Chen, Beidi and Han, Song and Lewis, Mike},
  journal={arXiv preprint arXiv:2309.17453},
  year={2023}
}

@article{yao2024sirllm,
  title={SirLLM: Streaming infinite retentive LLM},
  author={Yao, Yao and Li, Zuchao and Zhao, Hai},
  journal={arXiv preprint arXiv:2405.12528},
  year={2024}
}

@article{lminfinite,
  title={Lm-infinite: Simple on-the-fly length generalization for large language models},
  author={Han, Chi and Wang, Qifan and Xiong, Wenhan and Chen, Yu and Ji, Heng and Wang, Sinong},
  journal={arXiv preprint arXiv:2308.16137},
  year={2023}
}

@article{infllm,
  title={Infllm: Unveiling the intrinsic capacity of llms for understanding extremely long sequences with training-free memory},
  author={Xiao, Chaojun and Zhang, Pengle and Han, Xu and Xiao, Guangxuan and Lin, Yankai and Zhang, Zhengyan and Liu, Zhiyuan and Han, Song and Sun, Maosong},
  journal={arXiv preprint arXiv:2402.04617},
  year={2024}
}

@article{h2o,
  title={H2o: Heavy-hitter oracle for efficient generative inference of large language models},
  author={Zhang, Zhenyu and Sheng, Ying and Zhou, Tianyi and Chen, Tianlong and Zheng, Lianmin and Cai, Ruisi and Song, Zhao and Tian, Yuandong and R{\'e}, Christopher and Barrett, Clark and others},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  pages={34661--34710},
  year={2023}
}

@article{snapkv,
  title={Snapkv: Llm knows what you are looking for before generation},
  author={Li, Yuhong and Huang, Yingbing and Yang, Bowen and Venkitesh, Bharat and Locatelli, Acyr and Ye, Hanchen and Cai, Tianle and Lewis, Patrick and Chen, Deming},
  journal={arXiv preprint arXiv:2404.14469},
  year={2024}
}

@article{pyramidkv,
  title={Pyramidkv: Dynamic kv cache compression based on pyramidal information funneling},
  author={Cai, Zefan and Zhang, Yichi and Gao, Bofei and Liu, Yuliang and Liu, Tianyu and Lu, Keming and Xiong, Wayne and Dong, Yue and Chang, Baobao and Hu, Junjie and others},
  journal={arXiv preprint arXiv:2406.02069},
  year={2024}
}

@article{shi2024keep,
  title={Keep the Cost Down: A Review on Methods to Optimize LLM's KV-Cache Consumption},
  author={Shi, Luohe and Zhang, Hongyi and Yao, Yao and Li, Zuchao and Zhao, Hai},
  journal={arXiv preprint arXiv:2407.18003},
  year={2024}
}

@article{chen2023extending,
  title={Extending context window of large language models via positional interpolation},
  author={Chen, Shouyuan and Wong, Sherman and Chen, Liangjian and Tian, Yuandong},
  journal={arXiv preprint arXiv:2306.15595},
  year={2023}
}

@article{peng2023yarn,
  title={Yarn: Efficient context window extension of large language models},
  author={Peng, Bowen and Quesnelle, Jeffrey and Fan, Honglu and Shippole, Enrico},
  journal={arXiv preprint arXiv:2309.00071},
  year={2023}
}

@inproceedings{li2023long,
  title={How Long Can Context Length of Open-Source LLMs truly Promise?},
  author={Li, Dacheng and Shao, Rulin and Xie, Anze and Sheng, Ying and Zheng, Lianmin and Gonzalez, Joseph and Stoica, Ion and Ma, Xuezhe and Zhang, Hao},
  booktitle={NeurIPS 2023 Workshop on Instruction Tuning and Instruction Following},
  year={2023}
}

@article{chen2023longlora,
  title={Longlora: Efficient fine-tuning of long-context large language models},
  author={Chen, Yukang and Qian, Shengju and Tang, Haotian and Lai, Xin and Liu, Zhijian and Han, Song and Jia, Jiaya},
  journal={arXiv preprint arXiv:2309.12307},
  year={2023}
}

@article{ding2024longrope,
  title={Longrope: Extending llm context window beyond 2 million tokens},
  author={Ding, Yiran and Zhang, Li Lyna and Zhang, Chengruidong and Xu, Yuanyuan and Shang, Ning and Xu, Jiahang and Yang, Fan and Yang, Mao},
  journal={arXiv preprint arXiv:2402.13753},
  year={2024}
}

@article{mohtashami2023landmark,
  title={Landmark attention: Random-access infinite context length for transformers},
  author={Mohtashami, Amirkeivan and Jaggi, Martin},
  journal={arXiv preprint arXiv:2305.16300},
  year={2023}
}

@article{gao2024train,
  title={How to train long-context language models (effectively)},
  author={Gao, Tianyu and Wettig, Alexander and Yen, Howard and Chen, Danqi},
  journal={arXiv preprint arXiv:2410.02660},
  year={2024}
}

@article{fu2024data,
  title={Data engineering for scaling language models to 128k context},
  author={Fu, Yao and Panda, Rameswar and Niu, Xinyao and Yue, Xiang and Hajishirzi, Hannaneh and Kim, Yoon and Peng, Hao},
  journal={arXiv preprint arXiv:2402.10171},
  year={2024}
}

@article{xiong2023effective,
  title={Effective long-context scaling of foundation models},
  author={Xiong, Wenhan and Liu, Jingyu and Molybog, Igor and Zhang, Hejia and Bhargava, Prajjwal and Hou, Rui and Martin, Louis and Rungta, Rashi and Sankararaman, Karthik Abinav and Oguz, Barlas and others},
  journal={arXiv preprint arXiv:2309.16039},
  year={2023}
}

@article{li2024scbench,
  title={SCBench: A KV Cache-Centric Analysis of Long-Context Methods},
  author={Li, Yucheng and Jiang, Huiqiang and Wu, Qianhui and Luo, Xufang and Ahn, Surin and Zhang, Chengruidong and Abdi, Amir H and Li, Dongsheng and Gao, Jianfeng and Yang, Yuqing and others},
  journal={arXiv preprint arXiv:2412.10319},
  year={2024}
}

@article{tang2024ltri,
  title={Ltri-LLM: Streaming Long Context Inference for LLMs with Training-Free Dynamic Triangular Attention Pattern},
  author={Tang, Hongyin and Xiu, Di and Wang, Lanrui and Geng, Xiurui and Wang, Jingang and Cai, Xunliang},
  journal={arXiv preprint arXiv:2412.04757},
  year={2024}
}

@article{huang2024locret,
  title={Locret: Enhancing Eviction in Long-Context LLM Inference with Trained Retaining Heads},
  author={Huang, Yuxiang and Yuan, Binhang and Han, Xu and Xiao, Chaojun and Liu, Zhiyuan},
  journal={arXiv preprint arXiv:2410.01805},
  year={2024}
}

@article{liu2024retrievalattention,
  title={Retrievalattention: Accelerating long-context llm inference via vector retrieval},
  author={Liu, Di and Chen, Meng and Lu, Baotong and Jiang, Huiqiang and Han, Zhenhua and Zhang, Qianxi and Chen, Qi and Zhang, Chengruidong and Ding, Bailu and Zhang, Kai and others},
  journal={arXiv preprint arXiv:2409.10516},
  year={2024}
}

@article{xiao2024duoattention,
  title={Duoattention: Efficient long-context llm inference with retrieval and streaming heads},
  author={Xiao, Guangxuan and Tang, Jiaming and Zuo, Jingwei and Guo, Junxian and Yang, Shang and Tang, Haotian and Fu, Yao and Han, Song},
  journal={arXiv preprint arXiv:2410.10819},
  year={2024}
}

@article{sun2024shadowkv,
  title={ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM Inference},
  author={Sun, Hanshi and Chang, Li-Wen and Bao, Wenlei and Zheng, Size and Zheng, Ningxin and Liu, Xin and Dong, Harry and Chi, Yuejie and Chen, Beidi},
  journal={arXiv preprint arXiv:2410.21465},
  year={2024}
}

@article{wang2024model,
  title={Model tells you where to merge: Adaptive kv cache merging for llms on long-context tasks},
  author={Wang, Zheng and Jin, Boxiao and Yu, Zhongzhi and Zhang, Minjia},
  journal={arXiv preprint arXiv:2407.08454},
  year={2024}
}

@article{shi2024discovering,
  title={Discovering the gems in early layers: Accelerating long-context llms with 1000x input token reduction},
  author={Shi, Zhenmei and Ming, Yifei and Nguyen, Xuan-Phi and Liang, Yingyu and Joty, Shafiq},
  journal={arXiv preprint arXiv:2409.17422},
  year={2024}
}

@article{shen2023slimpajama,
  title={Slimpajama-dc: Understanding data combinations for llm training},
  author={Shen, Zhiqiang and Tao, Tianhua and Ma, Liqun and Neiswanger, Willie and Liu, Zhengzhong and Wang, Hongyi and Tan, Bowen and Hestness, Joel and Vassilieva, Natalia and Soboleva, Daria and others},
  journal={arXiv preprint arXiv:2309.10818},
  year={2023}
}

@article{karpukhin2020dense,
  title={Dense passage retrieval for open-domain question answering},
  author={Karpukhin, Vladimir and O{\u{g}}uz, Barlas and Min, Sewon and Lewis, Patrick and Wu, Ledell and Edunov, Sergey and Chen, Danqi and Yih, Wen-tau},
  journal={arXiv preprint arXiv:2004.04906},
  year={2020}
}

@article{luo2024bge,
  title={BGE Landmark Embedding: A Chunking-Free Embedding Method For Retrieval Augmented Long-Context Large Language Models},
  author={Luo, Kun and Liu, Zheng and Xiao, Shitao and Liu, Kang},
  journal={arXiv preprint arXiv:2402.11573},
  year={2024}
}

@article{xiao2023c,
  title={C-pack: Packaged resources to advance general chinese embedding},
  author={Xiao, Shitao and Liu, Zheng and Zhang, Peitian and Muennighof, Niklas},
  journal={arXiv preprint arXiv:2309.07597},
  year={2023}
}

@article{beltagy2020longformer,
  title={Longformer: The long-document transformer},
  author={Beltagy, Iz and Peters, Matthew E and Cohan, Arman},
  journal={arXiv preprint arXiv:2004.05150},
  year={2020}
}

@article{quest,
  title={Quest: Query-Aware Sparsity for Efficient Long-Context LLM Inference},
  author={Tang, Jiaming and Zhao, Yilong and Zhu, Kan and Xiao, Guangxuan and Kasikci, Baris and Han, Song},
  journal={arXiv preprint arXiv:2406.10774},
  year={2024}
}

@article{robertson2009probabilistic,
  title={The probabilistic relevance framework: BM25 and beyond},
  author={Robertson, Stephen and Zaragoza, Hugo and others},
  journal={Foundations and Trends{\textregistered} in Information Retrieval},
  volume={3},
  number={4},
  pages={333--389},
  year={2009},
  publisher={Now Publishers, Inc.}
}

@inproceedings{ma2024fine,
  title={Fine-tuning llama for multi-stage text retrieval},
  author={Ma, Xueguang and Wang, Liang and Yang, Nan and Wei, Furu and Lin, Jimmy},
  booktitle={Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval},
  pages={2421--2425},
  year={2024}
}

@article{cai2024lococo,
  title={LoCoCo: Dropping In Convolutions for Long Context Compression},
  author={Cai, Ruisi and Tian, Yuandong and Wang, Zhangyang and Chen, Beidi},
  journal={arXiv preprint arXiv:2406.05317},
  year={2024}
}

@article{dao2023flashattention,
  title={Flashattention-2: Faster attention with better parallelism and work partitioning},
  author={Dao, Tri},
  journal={arXiv preprint arXiv:2307.08691},
  year={2023}
}

@inproceedings{rajbhandari2020zero,
  title={Zero: Memory optimizations toward training trillion parameter models},
  author={Rajbhandari, Samyam and Rasley, Jeff and Ruwase, Olatunji and He, Yuxiong},
  booktitle={SC20: International Conference for High Performance Computing, Networking, Storage and Analysis},
  pages={1--16},
  year={2020},
  organization={IEEE}
}

@article{chen2016training,
  title={Training deep nets with sublinear memory cost},
  author={Chen, Tianqi and Xu, Bing and Zhang, Chiyuan and Guestrin, Carlos},
  journal={arXiv preprint arXiv:1604.06174},
  year={2016}
}

@article{zhang2024soaring,
  title={Soaring from 4k to 400k: Extending llmâ€™s context with activation beacon},
  author={Zhang, Peitian and Liu, Zheng and Xiao, Shitao and Shao, Ninglu and Ye, Qiwei and Dou, Zhicheng},
  journal={arXiv preprint arXiv:2401.03462},
  volume={2},
  number={3},
  pages={5},
  year={2024}
}

@article{liu2024kivi,
  title={Kivi: A tuning-free asymmetric 2bit quantization for kv cache},
  author={Liu, Zirui and Yuan, Jiayi and Jin, Hongye and Zhong, Shaochen and Xu, Zhaozhuo and Braverman, Vladimir and Chen, Beidi and Hu, Xia},
  journal={arXiv preprint arXiv:2402.02750},
  year={2024}
}

@article{kilburn1962one,
  title={One-level storage system},
  author={Kilburn, Tom and Edwards, David BG and Lanigan, Michael J and Sumner, Frank H},
  journal={IRE Transactions on Electronic Computers},
  number={2},
  pages={223--235},
  year={1962},
  publisher={IEEE}
}

@inproceedings{kwon2023efficient,
  title={Efficient memory management for large language model serving with pagedattention},
  author={Kwon, Woosuk and Li, Zhuohan and Zhuang, Siyuan and Sheng, Ying and Zheng, Lianmin and Yu, Cody Hao and Gonzalez, Joseph and Zhang, Hao and Stoica, Ion},
  booktitle={Proceedings of the 29th Symposium on Operating Systems Principles},
  pages={611--626},
  year={2023}
}

@article{ge2023model,
  title={Model tells you what to discard: Adaptive kv cache compression for llms},
  author={Ge, Suyu and Zhang, Yunan and Liu, Liyuan and Zhang, Minjia and Han, Jiawei and Gao, Jianfeng},
  journal={arXiv preprint arXiv:2310.01801},
  year={2023}
}

@article{liao2024e2llm,
  title={E2LLM: Encoder Elongated Large Language Models for Long-Context Understanding and Reasoning},
  author={Liao, Zihan and Wang, Jun and Yu, Hang and Wei, Lingxiao and Li, Jianguo and Zhang, Wei},
  journal={arXiv preprint arXiv:2409.06679},
  year={2024}
}

@article{xu2024think,
  title={Think: Thinner key cache by query-driven pruning},
  author={Xu, Yuhui and Jie, Zhanming and Dong, Hanze and Wang, Lei and Lu, Xudong and Zhou, Aojun and Saha, Amrita and Xiong, Caiming and Sahoo, Doyen},
  journal={arXiv preprint arXiv:2407.21018},
  year={2024}
}

@article{yang2024pyramidinfer,
  title={PyramidInfer: Pyramid KV Cache Compression for High-throughput LLM Inference},
  author={Yang, Dongjie and Han, XiaoDong and Gao, Yan and Hu, Yao and Zhang, Shilin and Zhao, Hai},
  journal={arXiv preprint arXiv:2405.12532},
  year={2024}
}