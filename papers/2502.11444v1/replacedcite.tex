\section{Related Work}
In this section, we make discussions on the following related works: 1) context extension of LLMs, 2) efficient long-context processing, 3) RAG approaches for long-context processing.  

% context extension: long llms, fine-tuning based method, position extrapolation 
First of all, a substantial body of research has focused on extending the context length of LLMs directly. One common approach involves modifying positional encoding mechanisms to enable LLMs trained on short texts to process longer inputs directly during inference ____. While straightforward, these methods often yield suboptimal performance without additional fine-tuning. Another widely adopted strategy is continual training, where existing LLMs are fine-tuned on long-sequence data to expand their context windows ____. However, fine-tuning approaches typically require training from extremely long-sequence data, which is challenging due to the scarcity of native human-annotation data and the high expenses resulted from the training operations ____. 
% In contrast, RetroLM is designed to establish useful KV retrieval capability and sparse KV adaptation capability, achieving superior training efficiency without relying on extremely long supervised data. The training process for both the KV retriever and LLM adaption requires sequences of no more than 12K tokens, while effectively compressing KV cache into fewer than 6K budget for texts exceeding 128K tokens.

% \begin{figure*}[t]
% \centering
% \vspace{-20pt}
% \includegraphics[width=0.93\linewidth]{figures/attn_map.jpg}
% \vspace{-10pt}
% \caption{\small Attention score maps for a MusiQue ____ sample. Left: from original full attention. Right: score from RetroLM's KV retriever. Red squares indicate key information for the multi-hop question. The x-axis represents sequence position (up to 18k tokens), and the y-axis represents each decoder layer. RetroLM effectively retrieves crucial KVs.}
% \label{fig:attn_map}
% \vspace{-10pt}
% \end{figure*}

% efficient methods: streaminglm, lm-infinite, snapkv, h2o, infllm, minference, kivi, act-beacon 
Recent studies have explored various types of efficient long-context processing techniques to alleviate computational and memory constraints ____.
Stream processing approaches, such as StreamingLLM ____ and LM-Infinite ____, maintain the most recent KVs within a sliding window alongside initial attention sinks. Sequential compression techniques, such as Activation Beacon ____, compress intermediate activations into more compact forms to conserve memory. KV quantization methods, including KIVI ____, encode the KV cache using low-bit representations to minimize storage requirements. 
Among these methods, KV cache sparsification has gained significant attention for their ability to selectively utilize portions of KVs based on certain reduction strategies, where KVs are reduced into a fixed budget (e.g., 2K) ____. For instance, InfLLM ____ incorporates intermediate information by segmenting KVs into fixed-size chunks and selecting top-k most salient chunks based on attention score patterns. H2O ____ introduces a policy that greedily drops KVs during generation using a scoring function derived from cumulative attention. SnapKV and PyramidKV ____ extend to alleviate memory pressure during the prefilling stage by dropping tokens based on cumulative attention scores within localized windows. 

% rag approaches: rag, retrievers, chunking problems and specialized retrievers for lc (landmark, chunk-free), agentic rag (e.g., memorag, lc-boostrapper)
Retrieval-augmented generation (RAG) has emerged as a promising approach for addressing long-context tasks ____. Leveraging modern dense retrievers ____, these approaches first partition the long text into smaller chunks, subsequently selecting the most salient chunks, and concatenating them to form a new prompt for the LLM ____. In addition, several specialized retrievers have been developed for long-context scenarios ____. In this work, RetroLM integrates retrieval augmentation directly at the KV cache level, thereby seamlessly incorporating RAG pipeline into long-context language modeling.