[
  {
    "index": 0,
    "papers": [
      {
        "key": "chen2023extending",
        "author": "Chen, Shouyuan and Wong, Sherman and Chen, Liangjian and Tian, Yuandong",
        "title": "Extending context window of large language models via positional interpolation"
      },
      {
        "key": "peng2023yarn",
        "author": "Peng, Bowen and Quesnelle, Jeffrey and Fan, Honglu and Shippole, Enrico",
        "title": "Yarn: Efficient context window extension of large language models"
      },
      {
        "key": "ding2024longrope",
        "author": "Ding, Yiran and Zhang, Li Lyna and Zhang, Chengruidong and Xu, Yuanyuan and Shang, Ning and Xu, Jiahang and Yang, Fan and Yang, Mao",
        "title": "Longrope: Extending llm context window beyond 2 million tokens"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "li2023long",
        "author": "Li, Dacheng and Shao, Rulin and Xie, Anze and Sheng, Ying and Zheng, Lianmin and Gonzalez, Joseph and Stoica, Ion and Ma, Xuezhe and Zhang, Hao",
        "title": "How Long Can Context Length of Open-Source LLMs truly Promise?"
      },
      {
        "key": "chen2023longlora",
        "author": "Chen, Yukang and Qian, Shengju and Tang, Haotian and Lai, Xin and Liu, Zhijian and Han, Song and Jia, Jiaya",
        "title": "Longlora: Efficient fine-tuning of long-context large language models"
      },
      {
        "key": "mohtashami2023landmark",
        "author": "Mohtashami, Amirkeivan and Jaggi, Martin",
        "title": "Landmark attention: Random-access infinite context length for transformers"
      },
      {
        "key": "xiong2023effective",
        "author": "Xiong, Wenhan and Liu, Jingyu and Molybog, Igor and Zhang, Hejia and Bhargava, Prajjwal and Hou, Rui and Martin, Louis and Rungta, Rashi and Sankararaman, Karthik Abinav and Oguz, Barlas and others",
        "title": "Effective long-context scaling of foundation models"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "fu2024data",
        "author": "Fu, Yao and Panda, Rameswar and Niu, Xinyao and Yue, Xiang and Hajishirzi, Hannaneh and Kim, Yoon and Peng, Hao",
        "title": "Data engineering for scaling language models to 128k context"
      },
      {
        "key": "gao2024train",
        "author": "Gao, Tianyu and Wettig, Alexander and Yen, Howard and Chen, Danqi",
        "title": "How to train long-context language models (effectively)"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "bai2023longbench",
        "author": "Bai, Yushi and Lv, Xin and Zhang, Jiajie and Lyu, Hongchang and Tang, Jiankai and Huang, Zhidian and Du, Zhengxiao and Liu, Xiao and Zeng, Aohan and Hou, Lei and others",
        "title": "Longbench: A bilingual, multitask benchmark for long context understanding"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "sun2024shadowkv",
        "author": "Sun, Hanshi and Chang, Li-Wen and Bao, Wenlei and Zheng, Size and Zheng, Ningxin and Liu, Xin and Dong, Harry and Chi, Yuejie and Chen, Beidi",
        "title": "ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM Inference"
      },
      {
        "key": "liao2024e2llm",
        "author": "Liao, Zihan and Wang, Jun and Yu, Hang and Wei, Lingxiao and Li, Jianguo and Zhang, Wei",
        "title": "E2LLM: Encoder Elongated Large Language Models for Long-Context Understanding and Reasoning"
      },
      {
        "key": "yang2024pyramidinfer",
        "author": "Yang, Dongjie and Han, XiaoDong and Gao, Yan and Hu, Yao and Zhang, Shilin and Zhao, Hai",
        "title": "PyramidInfer: Pyramid KV Cache Compression for High-throughput LLM Inference"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "streamingllm",
        "author": "Xiao, Guangxuan and Tian, Yuandong and Chen, Beidi and Han, Song and Lewis, Mike",
        "title": "Efficient streaming language models with attention sinks"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "lminfinite",
        "author": "Han, Chi and Wang, Qifan and Xiong, Wenhan and Chen, Yu and Ji, Heng and Wang, Sinong",
        "title": "Lm-infinite: Simple on-the-fly length generalization for large language models"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "zhang2024soaring",
        "author": "Zhang, Peitian and Liu, Zheng and Xiao, Shitao and Shao, Ninglu and Ye, Qiwei and Dou, Zhicheng",
        "title": "Soaring from 4k to 400k: Extending llm\u2019s context with activation beacon"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "liu2024kivi",
        "author": "Liu, Zirui and Yuan, Jiayi and Jin, Hongye and Zhong, Shaochen and Xu, Zhaozhuo and Braverman, Vladimir and Chen, Beidi and Hu, Xia",
        "title": "Kivi: A tuning-free asymmetric 2bit quantization for kv cache"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "xu2024think",
        "author": "Xu, Yuhui and Jie, Zhanming and Dong, Hanze and Wang, Lei and Lu, Xudong and Zhou, Aojun and Saha, Amrita and Xiong, Caiming and Sahoo, Doyen",
        "title": "Think: Thinner key cache by query-driven pruning"
      },
      {
        "key": "quest",
        "author": "Tang, Jiaming and Zhao, Yilong and Zhu, Kan and Xiao, Guangxuan and Kasikci, Baris and Han, Song",
        "title": "Quest: Query-Aware Sparsity for Efficient Long-Context LLM Inference"
      },
      {
        "key": "huang2024locret",
        "author": "Huang, Yuxiang and Yuan, Binhang and Han, Xu and Xiao, Chaojun and Liu, Zhiyuan",
        "title": "Locret: Enhancing Eviction in Long-Context LLM Inference with Trained Retaining Heads"
      },
      {
        "key": "liu2024retrievalattention",
        "author": "Liu, Di and Chen, Meng and Lu, Baotong and Jiang, Huiqiang and Han, Zhenhua and Zhang, Qianxi and Chen, Qi and Zhang, Chengruidong and Ding, Bailu and Zhang, Kai and others",
        "title": "Retrievalattention: Accelerating long-context llm inference via vector retrieval"
      },
      {
        "key": "shi2024discovering",
        "author": "Shi, Zhenmei and Ming, Yifei and Nguyen, Xuan-Phi and Liang, Yingyu and Joty, Shafiq",
        "title": "Discovering the gems in early layers: Accelerating long-context llms with 1000x input token reduction"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "infllm",
        "author": "Xiao, Chaojun and Zhang, Pengle and Han, Xu and Xiao, Guangxuan and Lin, Yankai and Zhang, Zhengyan and Liu, Zhiyuan and Han, Song and Sun, Maosong",
        "title": "Infllm: Unveiling the intrinsic capacity of llms for understanding extremely long sequences with training-free memory"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "h2o",
        "author": "Zhang, Zhenyu and Sheng, Ying and Zhou, Tianyi and Chen, Tianlong and Zheng, Lianmin and Cai, Ruisi and Song, Zhao and Tian, Yuandong and R{\\'e}, Christopher and Barrett, Clark and others",
        "title": "H2o: Heavy-hitter oracle for efficient generative inference of large language models"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "snapkv",
        "author": "Li, Yuhong and Huang, Yingbing and Yang, Bowen and Venkitesh, Bharat and Locatelli, Acyr and Ye, Hanchen and Cai, Tianle and Lewis, Patrick and Chen, Deming",
        "title": "Snapkv: Llm knows what you are looking for before generation"
      },
      {
        "key": "pyramidkv",
        "author": "Cai, Zefan and Zhang, Yichi and Gao, Bofei and Liu, Yuliang and Liu, Tianyu and Lu, Keming and Xiong, Wayne and Dong, Yue and Chang, Baobao and Hu, Junjie and others",
        "title": "Pyramidkv: Dynamic kv cache compression based on pyramidal information funneling"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "xu2023retrieval",
        "author": "Xu, Peng and Ping, Wei and Wu, Xianchao and McAfee, Lawrence and Zhu, Chen and Liu, Zihan and Subramanian, Sandeep and Bakhturina, Evelina and Shoeybi, Mohammad and Catanzaro, Bryan",
        "title": "Retrieval meets long context large language models"
      },
      {
        "key": "li2024long",
        "author": "Li, Xinze and Cao, Yixin and Ma, Yubo and Sun, Aixin",
        "title": "Long Context vs. RAG for LLMs: An Evaluation and Revisits"
      },
      {
        "key": "yue2024inference",
        "author": "Yue, Zhenrui and Zhuang, Honglei and Bai, Aijun and Hui, Kai and Jagerman, Rolf and Zeng, Hansi and Qin, Zhen and Wang, Dong and Wang, Xuanhui and Bendersky, Michael",
        "title": "Inference scaling for long-context retrieval augmented generation"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "karpukhin2020dense",
        "author": "Karpukhin, Vladimir and O{\\u{g}}uz, Barlas and Min, Sewon and Lewis, Patrick and Wu, Ledell and Edunov, Sergey and Chen, Danqi and Yih, Wen-tau",
        "title": "Dense passage retrieval for open-domain question answering"
      },
      {
        "key": "xiao2023c",
        "author": "Xiao, Shitao and Liu, Zheng and Zhang, Peitian and Muennighof, Niklas",
        "title": "C-pack: Packaged resources to advance general chinese embedding"
      }
    ]
  },
  {
    "index": 15,
    "papers": [
      {
        "key": "zhao2024retrieval",
        "author": "Zhao, Penghao and Zhang, Hailin and Yu, Qinhan and Wang, Zhengren and Geng, Yunteng and Fu, Fangcheng and Yang, Ling and Zhang, Wentao and Cui, Bin",
        "title": "Retrieval-augmented generation for ai-generated content: A survey"
      }
    ]
  },
  {
    "index": 16,
    "papers": [
      {
        "key": "luo2024bge",
        "author": "Luo, Kun and Liu, Zheng and Xiao, Shitao and Liu, Kang",
        "title": "BGE Landmark Embedding: A Chunking-Free Embedding Method For Retrieval Augmented Long-Context Large Language Models"
      },
      {
        "key": "gunther2023jina",
        "author": "G{\\\"u}nther, Michael and Ong, Jackmin and Mohr, Isabelle and Abdessalem, Alaeddine and Abel, Tanguy and Akram, Mohammad Kalim and Guzman, Susana and Mastrapas, Georgios and Sturua, Saba and Wang, Bo and others",
        "title": "Jina embeddings 2: 8192-token general-purpose text embeddings for long documents"
      }
    ]
  }
]