This section outlines the results of the two experiments outlined in Section 4. The overarching objective of the experiments is to demonstrate whether models using different sets of stylistic and the proposed social-monetisation features are able to detect ‘real-world’ fake news (achieved by using the Facebook URLs dataset for evaluation) and comparing them to state-of-the-art approaches relying on token-representations and LLMs. 

\subsection{Experiment 1: Generalisability of Token-Representations \& LLMs}

Experiment 1 aimed to address RQ1 by examining the generalisation capabilities of token representations (BoW, TF-IDF, Word2Vec, and BERT, as detailed in Section 4.2.1) combined with different machine learning models, as well as evaluating the performance of the large language model LLaMa under zero-shot, few-shot, and fine-tuning configurations.

\subsubsection{Token-Representation Results}

\begin{table}[ht]
    \centering
    \caption{Token-Representations Baseline Results}
     \label{tab:tr-base}   
     \begin{adjustbox}{width=0.7\textwidth}
    \begin{tabular}{|l|l|l|l|l|l|l|} \hline  
      \textbf{Features}&\textbf{Model}&  \textbf{Acc.}&  \textbf{Prec.}&  \textbf{Rec.}&  \textbf{Spec.}& \textbf{F1}\\ \hline
      \multirow{6}{*}{BoW}&Logistic Regression&  0.98&  0.97&  0.99&  0.97& 0.98\\   
      & Decision Tree&  0.96&  0.95&  0.96&  0.95& 0.96\\   
      &SVM&  0.91&  0.85&  0.99&  0.84& 0.92\\   
      &Gradient Boosting&  0.97&  0.96&  0.99&  0.96& 0.97\\   
      &Random Forest&  0.99&  0.97&  1.00&  0.97& 0.99\\   
      &Neural Network&  0.99&  0.99&  0.98&  0.98& 0.99\\ \hline  
      \multirow{6}{*}{TF-IDF}&Logistic Regression
&  0.98&  0.96&  1.00&  0.96& 0.98\\ 
      &Decision Tree
&  0.95&  0.95&  0.96&  0.95& 0.95\\   
      &SVM
&  0.97&  0.94&  1.00&  0.94& 0.97\\   
      &Gradient Boosting
& 0.98& 0.96& 0.99& 0.97&0.98\\   
      &Random Forest
& 0.99& 0.97& 1.00& 0.97&0.99\\   
      &Neural Network& 0.99& 0.98& 1.00& 0.97&0.99\\ \hline  
      \multirow{6}{*}{Word2Vec}&Logistic Regression
& 0.89& 0.86& 0.93& 0.86&0.90\\   
      &Decision Tree
& 0.87& 0.85& 0.88& 0.88&0.86\\   
      &SVM
& 0.91& 0.86& 0.96& 0.85&0.91\\   
      &Gradient Boosting
& 0.95& 0.94& 0.97& 0.94&0.95\\   
      &Random Forest
& 0.95& 0.92& 0.98& 0.92&0.95 \\   
      &Neural Network& 0.88& 0.91& 0.86& 0.83&0.88 \\
      &LSTM& 0.88& 0.91& 0.84& 0.91&0.87 \\ \hline  
      \multirow{6}{*}{BERT}&Logistic Regression
& 0.88& 0.86& 0.90& 0.86&0.88 \\   
      &Decision Tree
& 0.81& 0.82& 0.80& 0.82&0.81 \\   
      &SVM
& 0.90& 0.89& 0.92& 0.89&0.90 \\   
      &Gradient Boosting
& 0.84& 0.83& 0.85& 0.83&0.84 \\   
      &Random Forest
& 0.84& 0.82& 0.86& 0.82&0.84 \\   
      &Neural Network& 0.85& 0.89& 0.81& 0.83&0.84 \\ 
      &LSTM& 0.99& 0.99& 0.99& 0.99&0.99\\
      &Fine-Tuned BERT& 1.0& 1.0& 1.0& 1.0&1.0\\ \hline 
    \end{tabular}
    \end{adjustbox}

\end{table}

As seen in Table \ref{tab:tr-base}, under K-fold test conditions, token-representations trained and tested on the NELA dataset exhibit high performance across several models, achieving a mean accuracy of 0.93, with a range between 0.79 and 1.0. These results align with those commonly reported in the literature and validate the effectiveness of the feature extraction and modeling methods when applied in controlled conditions.

Among the tested approaches, Fine-Tuned BERT and LSTMs demonstrated the highest performance, achieving near-perfect metrics. Fine-Tuned BERT reached 1.0 accuracy, precision, recall, specificity, and F1 score, while LSTMs trained on BERT embeddings achieved 0.99 across all metrics. However, such results may reflect the model's ability to memorise patterns specific to the training data rather than a genuine ability to generalise beyond the dataset.

Traditional feature extraction techniques such as BoW and TF-IDF also achieved strong results, with accuracies up to 0.99. Their reliance on high-frequency or significant terms may have contributed to their robust performance within the dataset. This aligns with our observations in Section 2.4 regarding biases within commonly used fake news datasets, where certain terms may strongly correlate with specific classes. Specifically, Logistic Regression outperformed SVM when using the BoW representation, likely due to its ability to leverage the sparse, linearly separable nature of BoW features. In contrast, the SVM with an RBF kernel may not have been optimally suited for this representation, as the kernel is designed for capturing non-linear relationships, which may not align with BoW’s characteristics \citep{Colas2007}. This mismatch could partly explain the relatively poorer performance of SVM in this context.


In contrast to BoW and TF-IDF, Word2Vec and standard BERT-base embeddings performed slightly worse, with accuracies ranging from 0.81 to 0.95. These models may have struggled with the added noise in the textual data or the more complex representations introduced by their embeddings. In the case of Word2Vec, despite using a relevant model pre-trained on Google News, the performance may have been hindered by out-of-vocabulary (OOV) words. Unlike BoW and TF-IDF, which construct their vocabularies directly from the dataset and therefore capture all words within that domain, Word2Vec relies on a fixed vocabulary from its pre-training corpus. Fake news datasets often contain domain-specific terms, slang, or creative language use that may not be present in the pre-trained Word2Vec vocabulary. As a result, OOV words are either ignored or mapped to suboptimal representations, leading to a potential loss of crucial information. This limitation reduces the model’s ability to capture dataset-specific keywords and patterns, whereas simpler methods like BoW and TF-IDF, by leveraging dataset-dependent vocabularies, retain the ability to represent all terms present in the text.

While these results highlight the apparent effectiveness of both traditional and deep learning approaches in controlled settings, they must be interpreted cautiously. The high performance observed here may not translate to real-world applications or unseen datasets, as will be discussed in the cross-dataset results.

\begin{table}[ht]
    \centering
    \caption{Token-Representations Cross-Dataset Results}
     \label{tab:tr-xd}   
     \begin{adjustbox}{width=0.7\textwidth}
   \begin{tabular}{|l|l|l|l|l|l|l|} \hline  
      \textbf{Features}&\textbf{Model}&  \textbf{Acc.}&  \textbf{Prec.}&  \textbf{Rec.}&  \textbf{Spec.}& \textbf{F1}\\ \hline
      \multirow{6}{*}{BoW}&Logistic Regression&  0.66&  0.68&  0.60&  0.72& 0.64
\\   
      & Decision Tree&  0.64&  0.66&  0.57&  0.71& 0.61
\\   
      &SVM&  0.61&  0.58&  0.78&  0.44& 0.67
\\   
      &Gradient Boosting&  0.63&  0.60&  0.76&  0.50& 0.67
\\   
      &Random Forest&  0.61&  0.57&  0.88&  0.34& 0.69
\\   
      &Neural Network&  0.68&  0.72&  0.58&  0.78& 0.65
\\ \hline  
      \multirow{6}{*}{TF-IDF}&Logistic Regression
&  0.68&  0.68&  0.71&  0.66& 0.69
\\ 
      &Decision Tree
&  0.64&  0.67&  0.58&  0.71& 0.62
\\   
      &SVM
&  0.68&  0.66&  0.74&  0.62& 0.70
\\   
      &Gradient Boosting
& 0.64& 0.62& 0.76& 0.53&0.68
\\   
      &Random Forest
& 0.64& 0.60& 0.86& 0.42&0.70
\\   
      &Neural Network& 0.70& 0.74& 0.63& 0.78&0.68
\\ \hline  
      \multirow{6}{*}{Word2Vec}&Logistic Regression
& 0.67& 0.70& 0.57& 0.76&0.63
\\   
      &Decision Tree
& 0.60& 0.62& 0.56& 0.66&0.59
\\   
      &SVM
& 0.65& 0.68& 0.57& 0.73&0.62
\\   
      &Gradient Boosting
& 0.65& 0.67& 0.58& 0.72&0.62
\\   
      &Random Forest
& 0.65& 0.66& 0.62& 0.67&0.64
\\   
      &Neural Network& 0.66& 0.70& 0.55& 0.77&0.61\\ 
      &LSTM& 0.62& 0.64& 0.54& 0.7&0.59\\\hline  
      \multirow{6}{*}{BERT}&Logistic Regression
& 0.65& 0.67& 0.60& 0.70&0.63
\\   
      &Decision Tree
& 0.60& 0.62& 0.53& 0.67&0.57
\\   
      &SVM
& 0.66& 0.69& 0.59& 0.74&0.64
\\   
      &Gradient Boosting
& 0.63& 0.64& 0.61& 0.66&0.62
\\   
      &Random Forest
& 0.62& 0.62& 0.62& 0.62&0.62
\\   
      &Neural Network& 0.66& 0.69& 0.56& 0.75&0.62\\ 
      &LSTM& 0.68& 0.75& 0.55& 0.81&0.63\\
      &Fine-Tuned BERT& 0.68& 0.75& 0.53& 0.82&0.62\\
      \hline 
    \end{tabular}
    \end{adjustbox}

\end{table}

The results from the cross-dataset testing (Table \ref{tab:tr-xd}) reveal a significant reduction in performance across all models and feature sets compared to the K-fold test conditions discussed in the previous section. While token-representation models achieved high accuracy within the NELA dataset (Section \ref{tab:tr-base}), their ability to generalise across datasets is markedly lower. On average, models experienced a 0.28 drop in accuracy, underscoring the challenge of applying these approaches to unseen data.

Among the tested feature sets, Fine-Tuned BERT and LSTM models, which previously excelled in the controlled K-fold setting, demonstrated similar vulnerabilities to generalisation issues. Fine-Tuned BERT achieved an accuracy of 0.68, precision of 0.75, recall of 0.53, specificity of 0.82, and an F1 score of 0.62. While these metrics are comparable to other models in cross-dataset conditions, they represent a stark decline from its perfect performance in K-fold testing. Similarly, the LSTM trained on BERT embeddings exhibited similar limitations, achieving a slightly lower accuracy of 0.62 and an imbalanced recall and specificity, suggesting a difficulty in maintaining consistency across datasets.

Traditional feature sets such as BoW and TF-IDF also showed substantial performance degradation, with accuracy ranging between 0.61 and 0.7. While Logistic Regression and SVM models trained on these features demonstrated relatively balanced recall and specificity, their overall drop in accuracy highlights the limitations of token-representation methods when applied to different datasets. Word2Vec and BERT, which already underperformed compared to BoW and TF-IDF in K-fold testing, fared similarly poorly under cross-dataset conditions, suggesting that they may be more sensitive to noise in the training data.

Neural networks trained on TF-IDF features achieved the highest accuracy under cross-dataset conditions (0.7), but their poor recall indicates that they struggled to correctly identify the positive (true news) class. This trend, observed across other neural network models, suggests potential overfitting to the NELA dataset, even with the inclusion of the early-stopping hyperparameter. In contrast, SVM and Logistic Regression models trained on TF-IDF features exhibited more balanced recall and specificity, making them arguably better candidates for generalisation despite their slightly lower overall accuracy.

These findings underscore a key limitation of token-representation-based models: while they can achieve high performance in controlled conditions, their generalisability to unseen datasets remains limited. This highlights the need for exploring alternative feature sets beyond the text, such as stylistic or social-monetisation features, which may offer more robust solutions for addressing dataset variability and bias. Additionally, the large imbalances observed between precision, recall, and specificity across models suggest the influence of underlying differences in how these methods respond to dataset-specific patterns, further complicating generalisability.

\subsection{LLaMa Results}

This section evaluates the performance of LLaMa 3.2-1B in detecting fake news, focusing on its zero-shot, few-shot, and fine-tuned configurations. Unlike traditional algorithms relying on static token representations, LLaMa leverages its pre-trained, context-aware transformer architecture to adapt dynamically to the task.

\begin{table}[!h]
    \centering
    \resizebox{0.6\textwidth}{!}{%
    \renewcommand{\arraystretch}{1.2}
    \begin{tabular}{|l|c|c|c|c|c|}
        \hline
        \textbf{Config.} & \textbf{Acc.} & \textbf{Prec.} & \textbf{Rec.} & \textbf{Spec.} & \textbf{F1} \\ \hline
        Zero-Shot & 0.53 & 0.55 & 0.39 & 0.68 & 0.46 \\ 
        Few-Shot  & 0.60 & 0.65 & 0.42 & 0.77 & 0.51 \\ 
        Fine-Tuned & 1 & 1 & 1 & 1.00 & 1 \\ \hline
    \end{tabular}}
    \caption{Performance of LLaMa configurations on the NELA dataset.}
    \label{tab:llama_nela_results}
\end{table}

The results presented in Table \ref{tab:llama_nela_results} reveal the performance of LLaMa 3.2-1B under zero-shot, few-shot, and fine-tuned configurations on the NELA dataset.

In the zero-shot configuration, LLaMa achieved an accuracy of only 0.53, with precision at 0.55 and a low recall of 0.39. The specificity was 0.68, and the F1-score stood at 0.46. These metrics are barely above random chance, indicating that the model struggled to effectively detect fake news without any task-specific training. Despite large language models often performing well in zero-shot settings for other tasks, LLaMa's performance here suggests that its pre-trained knowledge does not generalise well to the nuances of fake news detection.

The few-shot configuration showed a modest improvement, with accuracy increasing to 0.6, precision to 0.65, and recall to 0.42. Specificity improved to 0.77, and the F1-score to 0.51. While these results are slightly better than the zero-shot configuration, they remain unsatisfactory for practical applications. The limited enhancement implies that providing a small number of labeled examples was insufficient for the model to grasp the complex patterns associated with fake news, which often involve subtle linguistic cues and context-dependent nuances.

In stark contrast, the fine-tuned configuration achieved perfect scores across all metrics, with accuracy, precision, recall, specificity, and F1-score all at 1.00. While this suggests that the model can perform exceptionally well when extensively trained on the task-specific data, such flawless performance is unusual and may indicate overfitting to the NELA dataset. Overfitting reduces the model's ability to generalise to new, unseen data, limiting its practical utility in real-world scenarios where fake news can vary widely in form and content.

These findings underscore a critical limitation of large language models like LLaMa in the context of fake news detection. Despite their strong performance in zero-shot and few-shot settings on more general language tasks, these models do not perform well on the task of fake news detection without substantial task-specific training.

\begin{table}[!h]
    \centering
    \resizebox{0.8\textwidth}{!}{%
    \renewcommand{\arraystretch}{1.2}
    \begin{tabular}{|l|c|c|c|c|c|}
        \hline
        \textbf{Configuration} & \textbf{Accuracy} & \textbf{Precision} & \textbf{Recall} & \textbf{Specificity} & \textbf{F1} \\ \hline
        Zero-Shot & 0.50 & 0.50 & 0.40 & 0.61 & 0.44 \\ 
        Few-Shot  & 0.55 & 0.57 & 0.37 & 0.72 & 0.45 \\ 
        Fine-Tuned & 0.71 & 0.90 & 0.47 & 0.95 & 0.62 \\ \hline
    \end{tabular}}
    \caption{Performance of LLaMa configurations on the Facebook URLs dataset.}
    \label{tab:llama_facebook_results}
\end{table}

This issue is demonstrated further in regards to the Facebook URLs dataset, where Table \ref{tab:llama_facebook_results} highlights the cross-dataset performance of LLaMa in these three configurations. These findings provide insights into the model's ability to generalise when tested on unseen, manually labelled data.

In the zero-shot configuration, LLaMa achieves an accuracy of 0.5, with precision and recall at 0.5 and 0.4, respectively. The specificity is moderately better at 0.61, and the F1-score stands at 0.44. These results indicate that the model performs at near-random levels when applied to the external dataset without any task-specific training. While large language models are often effective in zero-shot scenarios for general tasks, LLaMa's performance here underscores the difficulty of adapting pre-trained knowledge to the domain-specific challenges of fake news detection, particularly when confronted with nuanced and diverse real-world data.

The few-shot configuration using examples from the NELA dataset shows a marginal improvement over zero-shot performance. Accuracy increases to 0.55, precision rises to 0.57, and specificity improves to 0.72. However, recall remains low at 0.37, and the F1-score only improves slightly to 0.45. These results suggest that while a small number of labeled examples from the training dataset provided some task-specific guidance, they were insufficient for LLaMa to effectively generalise to the Facebook dataset. This limited improvement highlights the challenges of adapting models trained on coarsely labelled datasets, such as NELA, to manually curated datasets with more nuanced distinctions.

In the fine-tuned configuration of LLaMA trained on the NELA dataset, the model demonstrates a notable improvement, achieving an accuracy of 0.71, precision of 0.9 for identifying fake news (the negative class), and specificity of 0.95. However, recall remains relatively low at 0.47, resulting in an F1-score of 0.62. While the accuracy is higher compared to token-representations, the high specificity suggests the model is effective at identifying fake news, but the low recall coupled with high precision indicates it is conservative in predicting true news, potentially overlooking many true news instances. Similar imbalanced metrics can be observed in token-based approaches, suggesting that both LLMs and token-representations face challenges in achieving balanced performance across classes, likely due to dataset-specific biases and the inherent difficulty of the classification task.


\subsection{Experiment 2: Generalisability of Stylistic \& Social Monetisation \\Features}

The second experiment targeted RQ2 and RQ3 and aimed to determine whether the stylistic features suggested in the literature and the social-monetisation features introduced in this study are more generalisable than the token-level representations tested in Section 5.1. As detailed in Section 4.3.1, the following groups of stylistic features were evaluated: Fernandez; Abonizio; LIWC; NELA; and modified NELA. Each of these groups was tested with and without the proposed social monetisation features identified in Section 4.3.2. A K-fold test was first performed with the same splits as in the first experiment, using the NELA dataset to provide a baseline for comparison and the Facebook dataset to perform a cross-dataset test for each model trained in each fold.

\begin{table}[ht]
    \begin{adjustwidth}{-.5in}{-.5in} 
    \centering
    \caption{Stylistic Features \& S-M Features Baseline Results}
    \label{tab:style-base}
    \begin{adjustbox}{width=1.15\textwidth}
    \begin{tabular}{|l|l|c|c|c|c|c|c|c|c|c|c|l|}
        \hline
        \multirow{2}{*}{\textbf{Feature-Set}} & \multirow{2}{*}{\textbf{Model}} & \multicolumn{5}{c|}{\textbf{Without proposed S-M Features}} & \multicolumn{5}{c|}{\textbf{With proposed S-M Features}} & \multirow{2}{*}{\textbf{p-value}}\\ \cline{3-12} 
         &  & \textbf{Acc.} & \textbf{Prec.} & \textbf{Rec.} & \textbf{Spec.} & \textbf{F1} & \textbf{Acc.} & \textbf{Prec.} & \textbf{Rec.} & \textbf{Spec.} & \textbf{F1}   &\\ \hline
        \multirow{6}{*}{Fernandez} & Logistic Regression & 0.83& 0.80& 0.89& 0.75& 0.84& 0.84& 0.81& 0.90& 0.78& 0.85 &{\cellcolor[gray]{.8}} \textless0.001\\ 
         & Decision Tree & 0.88& 0.88& 0.88& 0.87& 0.88& 0.93& 0.93& 0.93& 0.93& 0.93 &{\cellcolor[gray]{.8}} \textless0.001\\ 
         & SVM & 0.90& 0.86& 0.95& 0.84& 0.91& 0.92& 0.89& 0.96& 0.87& 0.93 &{\cellcolor[gray]{.8}} \textless0.001\\ 
         & Gradient Boosting & 0.89& 0.87& 0.93& 0.85& 0.90& 0.93& 0.92& 0.96& 0.91& 0.94&{\cellcolor[gray]{.8}} \textless0.001\\ 
         & Random Forest & 0.93& 0.91& 0.96& 0.90& 0.94& 0.96& 0.95& 0.98& 0.95& 0.97&{\cellcolor[gray]{.8}} \textless0.001\\ 
         & Neural Network & 0.89& 0.87& 0.92& 0.85& 0.90& 0.93& 0.92& 0.95& 0.91& 0.93&{\cellcolor[gray]{.8}} \textless0.001\\ \hline
        \multirow{6}{*}{Abonizio} & Logistic Regression & 0.78& 0.77& 0.82& 0.73& 0.79& 0.78& 0.77& 0.82& 0.74& 0.79 &{\cellcolor[gray]{.8}} 0.5678\\ 
         & Decision Tree & 0.85& 0.86& 0.86& 0.85& 0.86& 0.92& 0.92& 0.92& 0.91& 0.92 &{\cellcolor[gray]{.8}} \textless0.001\\ 
         & SVM & 0.91& 0.89& 0.94& 0.88& 0.91& 0.94& 0.93& 0.97& 0.92& 0.95&{\cellcolor[gray]{.8}} \textless0.001\\ 
         & Gradient Boosting & 0.86& 0.85& 0.90& 0.82& 0.87& 0.93& 0.91& 0.96& 0.90& 0.93&{\cellcolor[gray]{.8}} \textless0.001\\ 
         & Random Forest & 0.93& 0.92& 0.95& 0.91& 0.94& 0.98& 0.97& 0.99& 0.97& 0.98&{\cellcolor[gray]{.8}} \textless0.001\\ 
         & Neural Network & 0.90& 0.90& 0.92& 0.88& 0.91& 0.94& 0.94& 0.96& 0.93& 0.95&{\cellcolor[gray]{.8}} \textless0.001\\ \hline
        \multirow{6}{*}{LIWC}& Logistic Regression & 0.91& 0.90& 0.92& 0.89& 0.91& 0.91& 0.91& 0.92& 0.90& 0.92 & {\cellcolor[gray]{.8}} 0.2017\\ 
         & Decision Tree & 0.88& 0.88& 0.88& 0.88& 0.88& 0.90& 0.91& 0.90& 0.91& 0.91&{\cellcolor[gray]{.8}} \textless0.001\\ 
         & SVM & 0.97& 0.96& 0.98& 0.95& 0.97& 0.97& 0.96& 0.98& 0.96& 0.97&{\cellcolor[gray]{.8}} 0.03\\ 
         & Gradient Boosting & 0.94& 0.92& 0.96& 0.91& 0.94& 0.95& 0.94& 0.97& 0.93& 0.95&{\cellcolor[gray]{.8}} \textless0.001\\ 
         & Random Forest & 0.95& 0.93& 0.98& 0.92& 0.95& 0.96& 0.94& 0.99& 0.94& 0.96&{\cellcolor[gray]{.8}} \textless0.001\\ 
         & Neural Network & 0.95& 0.95& 0.96& 0.94& 0.95& 0.96& 0.95& 0.96& 0.95& 0.96&{\cellcolor[gray]{.8}} 0.03\\ \hline
        \multirow{6}{*}{\shortstack[l]{NELA Feature\\Extractor}}& Logistic Regression & 0.85& 0.85& 0.88& 0.83& 0.86& 0.86& 0.85& 0.88& 0.84& 0.87& {\cellcolor[gray]{.8}} 0.023\\ 
         & Decision Tree & 0.85& 0.86& 0.85& 0.85& 0.86& 0.89& 0.89& 0.89& 0.89& 0.89&{\cellcolor[gray]{.8}} \textless0.001\\ 
         & SVM & 0.94& 0.92& 0.96& 0.91& 0.94& 0.95& 0.93& 0.97& 0.92& 0.95&{\cellcolor[gray]{.8}} \textless0.001\\ 
         & Gradient Boosting & 0.90& 0.88& 0.93& 0.86& 0.91& 0.93& 0.91& 0.96& 0.89& 0.93&{\cellcolor[gray]{.8}} \textless0.001\\ 
         & Random Forest & 0.93& 0.90& 0.97& 0.88& 0.93& 0.95& 0.93& 0.98& 0.92& 0.95&{\cellcolor[gray]{.8}} \textless0.001\\ 
         & Neural Network & 0.92& 0.91& 0.93& 0.91& 0.92& 0.94& 0.94& 0.95& 0.93& 0.94&{\cellcolor[gray]{.8}} \textless0.001\\ \hline
        \multirow{6}{*}{\shortstack[l]{Modified NELA \\Feature Extractor}}& Logistic Regression & 0.87& 0.87& 0.89& 0.86& 0.88& 0.88& 0.88& 0.89& 0.86& 0.88&{\cellcolor[gray]{.8}} 0.023\\ 
         & Decision Tree & 0.88& 0.89& 0.88& 0.88& 0.88& 0.90& 0.91& 0.91& 0.90& 0.91&{\cellcolor[gray]{.8}} \textless0.001\\ 
         & SVM & 0.96& 0.95& 0.98& 0.94& 0.96& 0.97& 0.96& 0.98& 0.95& 0.97&{\cellcolor[gray]{.8}} \textless0.001\\ 
         & Gradient Boosting & 0.92& 0.90& 0.94& 0.89& 0.92& 0.94& 0.93& 0.96& 0.92& 0.94&{\cellcolor[gray]{.8}} \textless0.001\\ 
         & Random Forest & 0.95& 0.93& 0.97& 0.92& 0.95& 0.97& 0.95& 0.98& 0.94& 0.97&{\cellcolor[gray]{.8}} \textless0.001\\ 
         & Neural Network & 0.94& 0.94& 0.95& 0.94& 0.95& 0.95& 0.95& 0.96& 0.95& 0.96&{\cellcolor[gray]{.8}} \textless0.001\\ \hline
    \end{tabular}
    \end{adjustbox}
    \end{adjustwidth}
\end{table}

As can be seen from Table \ref{tab:style-base}, in K-fold cross-validation test conditions the selected stylistic features performed comparably to the token-representations (see Table \ref{tab:tr-base}). Across the different groups of stylistic features and machine learning algorithms, the mean accuracy was 90\% with a range between 78\% and 98\%. From this test, it can be seen that that Logistic Regression models using the Fernandez and Abonizio feature-sets excluding the proposed social monetisation features perform least well with the Random Forest trained on the Abonizio feature-group performing the best. Furthermore, with the inclusion of the proposed features, each feature group performed marginally better than without them. This is supported by the results of the Mann-Whitney U-test, whereby the p-values across the majority of models indicate a statistically significant increase in mean accuracy in models using the proposed features compared to those that exclude these features. Additionally, the modified NELA features also performed slightly better compared to the original NELA feature-set both with and without the proposed features. These results provide support for the use of the proposed features as well as the modification of the original NELA feature-set. 

\begin{table}[ht]
    \begin{adjustwidth}{-.5in}{-.5in} 
    \centering
    \caption{Stylistic Features \& S-M Features Cross-Dataset Results}
    \label{tab:style-xd}
    \begin{adjustbox}{width=1.15\textwidth}
    \begin{tabular}{|l|l|c|c|c|c|c|c|c|c|c|c|l|}
        \hline
        \multirow{2}{*}{\textbf{Feature-Set}} & \multirow{2}{*}{\textbf{Model}} & \multicolumn{5}{c|}{\textbf{Without proposed S-M Features}} & \multicolumn{5}{c|}{\textbf{With proposed S-M Features}} & \multirow{2}{*}{\textbf{p-value}}\\ \cline{3-12} 
         &  & \textbf{Acc.} & \textbf{Prec.} & \textbf{Rec.} & \textbf{Spec.} & \textbf{F1} & \textbf{Acc.} & \textbf{Prec.} & \textbf{Rec.} & \textbf{Spec.} & \textbf{F1}   &\\ \hline
        \multirow{6}{*}{Fernandez} 
& Logistic Regression & 0.66& 0.66& 0.67& 0.65& 0.67& 0.68& 0.67& 0.70& 0.66& 0.68&{\cellcolor[gray]{.8}} \textless0.001\\     
& Decision Tree & 0.66& 0.65& 0.67& 0.65& 0.66& 0.68& 0.69& 0.64& 0.71& 0.66&{\cellcolor[gray]{.8}} \textless0.001\\ 
& SVM & 0.68& 0.66& 0.71& 0.64& 0.69& 0.69& 0.69& 0.71& 0.68& 0.70&{\cellcolor[gray]{.8}} \textless0.001\\ 
& Gradient Boosting & 0.73& 0.73& 0.73& 0.73& 0.73& 0.74& 0.75& 0.72& 0.76& 0.74&{\cellcolor[gray]{.8}} \textless0.001\\ 
& Random Forest & 0.73& 0.76& 0.67& 0.79& 0.71& 0.74& 0.76& 0.70& 0.78& 0.72&{\cellcolor[gray]{.8}} \textless0.001\\ 
& Neural Network & 0.70& 0.70& 0.69& 0.71& 0.69& 0.71& 0.71& 0.69& 0.72& 0.70&{\cellcolor[gray]{.8}} 0.4446\\ \hline
        \multirow{6}{*}{Abonizio} 
& Logistic Regression & 0.59& 0.59& 0.62& 0.56& 0.60& 0.62& 0.61& 0.66& 0.57& 0.63&{\cellcolor[gray]{.8}} \textless0.001\\ 
& Decision Tree & 0.57& 0.57& 0.56& 0.58& 0.57& 0.60& 0.60& 0.60& 0.59& 0.60&{\cellcolor[gray]{.8}} \textless0.001\\ 
& SVM & 0.63& 0.63& 0.62& 0.64& 0.62& 0.65& 0.66& 0.64& 0.67& 0.65&{\cellcolor[gray]{.8}} \textless0.001\\ 
& Gradient Boosting & 0.61& 0.60& 0.64& 0.58& 0.62& 0.65& 0.65& 0.67& 0.63& 0.66&{\cellcolor[gray]{.8}} \textless0.001\\ 
& Random Forest & 0.64& 0.65& 0.61& 0.67& 0.63& 0.67& 0.68& 0.67& 0.68& 0.67&{\cellcolor[gray]{.8}} \textless0.001\\ 
& Neural Network & 0.60& 0.60& 0.59& 0.61& 0.60& 0.64& 0.64& 0.61& 0.66& 0.63&{\cellcolor[gray]{.8}} \textless0.001\\ \hline
        \multirow{6}{*}{LIWC}
& Logistic Regression & 0.69& 0.71& 0.65& 0.73& 0.68& 0.67& 0.69& 0.62& 0.72& 0.65&{\cellcolor[gray]{.8}}  N/A\\ 
& Decision Tree & 0.62& 0.62& 0.63& 0.61& 0.62& 0.62& 0.62& 0.63& 0.61& 0.62&{\cellcolor[gray]{.8}} 0.3564\\ 
& SVM & 0.67& 0.68& 0.65& 0.70& 0.66& 0.68& 0.68& 0.67& 0.68& 0.67&{\cellcolor[gray]{.8}} 0.02\\ 
& Gradient Boosting & 0.69& 0.70& 0.65& 0.72& 0.68& 0.74& 0.74& 0.72& 0.75& 0.73&{\cellcolor[gray]{.8}} \textless0.001\\ 
& Random Forest & 0.67& 0.67& 0.69& 0.66& 0.68& 0.69& 0.69& 0.68& 0.69& 0.69&{\cellcolor[gray]{.8}} \textless0.01\\ 
& Neural Network & 0.68& 0.69& 0.64& 0.71& 0.66& 0.69& 0.71& 0.63& 0.74& 0.67&{\cellcolor[gray]{.8}} 0.0319\\ \hline
        \multirow{6}{*}{\shortstack[l]{NELA Feature\\Extractor}}
& Logistic Regression & 0.65& 0.65& 0.63& 0.67& 0.64& 0.68& 0.69& 0.65& 0.71& 0.67& {\cellcolor[gray]{.8}} \textless0.001\\ 
& Decision Tree & 0.61& 0.61& 0.58& 0.63& 0.60& 0.62& 0.63& 0.61& 0.64& 0.62&{\cellcolor[gray]{.8}} 0.0258\\ 
& SVM & 0.63& 0.63& 0.61& 0.65& 0.62& 0.65& 0.66& 0.61& 0.69& 0.63&{\cellcolor[gray]{.8}} \textless0.001\\ 
& Gradient Boosting & 0.69& 0.72& 0.64& 0.75& 0.68& 0.70& 0.71& 0.67& 0.73& 0.69&{\cellcolor[gray]{.8}} 0.1312\\ 
& Random Forest & 0.66& 0.68& 0.62& 0.70& 0.65& 0.68& 0.69& 0.64& 0.72& 0.66&{\cellcolor[gray]{.8}} \textless0.001\\ 
& Neural Network & 0.63& 0.64& 0.60& 0.67& 0.62& 0.65& 0.67& 0.61& 0.70& 0.64&{\cellcolor[gray]{.8}} \textless0.001\\ \hline
        \multirow{6}{*}{\shortstack[l]{Modified NELA\\Feature Extractor}}
& Logistic Regression & 0.67& 0.68& 0.63& 0.70& 0.65& 0.71& 0.73& 0.67& 0.75& 0.70&{\cellcolor[gray]{.8}} \textless0.001\\ 
& Decision Tree & 0.64& 0.64& 0.63& 0.65& 0.64& 0.66& 0.66& 0.65& 0.66& 0.66&{\cellcolor[gray]{.8}} \textless0.001\\ 
& SVM & 0.66& 0.66& 0.65& 0.67& 0.66& 0.69& 0.70& 0.64& 0.73& 0.67&{\cellcolor[gray]{.8}} \textless0.001\\ 
& Gradient Boosting & 0.70& 0.70& 0.70& 0.71& 0.70& 0.75& 0.76& 0.72& 0.77& 0.74&{\cellcolor[gray]{.8}} \textless0.001\\ 
& Random Forest & 0.69& 0.70& 0.67& 0.72& 0.69& 0.75& 0.76& 0.73& 0.77& 0.74&{\cellcolor[gray]{.8}} \textless0.001\\ 
         & Neural Network & 0.64& 0.65& 0.62& 0.67& 0.63& 0.69& 0.70& 0.64& 0.73& 0.67&{\cellcolor[gray]{.8}} \textless0.001\\ \hline
    \end{tabular}
    \end{adjustbox}
    \end{adjustwidth}
\end{table}

In terms of generalisability, Table \ref{tab:style-xd} shows most models utilising stylistic features showed slightly better performance compared to LLaMa and models using token representations in the cross-dataset test, with an average drop in accuracy between the baseline and cross-dataset test of 24\%. While not all models trained on stylistic features outperformed token-based models or LLaMa, they consistently demonstrated a more balanced performance in recall and specificity, effectively addressing key limitations observed in models relying on token representations or LLaMa. Some models also demonstrated superior cross-dataset accuracy compared to the best performing models in the previous experiment, the fine-tuned LLaMa model, as well as the best performing model trained on token-representations (the Neural Network trained on TF-IDF features). Among those models trained without the proposed social-monetisation features, both Gradient Boosting and Random Forest algorithms trained on the Fernandez feature-set exhibited higher mean accuracy than LLaMa or any models trained using token-representations. The number of models displaying higher mean accuracy than those trained on token representations/LLaMa increased after integrating the proposed social-monetisation features, suggesting that the proposed features are relevant to producing more generalisable fake news detection models. Models incorporating these features and outperforming both token-representations and LLaMa include Gradient Boosting, and Random Forest trained on the modified NELA feature-set; Gradient Boosting trained on LIWC; and Gradient Boosting and Random Forest trained on the Fernandez feature-set. The Mann-Whitney U-test provides statistical support that the proposed features have a significant contribution in terms of improving the generalisability of these models. 

\subsection{Analysis with Permutation Feature Importance}

Experiment 2 presented evidence that the proposed social monetisation features contribute to producing more generalisable models. Permutation Feature Importance (PFI) analysis will further assess the impact of these features on the generalisability of the model. To prevent redundancy, the most successful model was selected for this analysis, which was Gradient Boosting trained on the modified NELA feature set, due to its higher mean accuracy (75\%) in cross-dataset conditions compared to other models. Although Random Forest trained on the same feature set demonstrates similar superior mean accuracy, Gradient Boosting was preferred due to its better performance across the other feature sets when compared to Random Forest. PFI was implemented by training the model on the NELA dataset and calculating the feature importance on both an unseen portion of the NELA dataset and a random balanced sample of the Facebook URLs dataset. This allows us to observe the features that are relevant to both models, and therefore what features can be considered the most generalisable between the coarsely labelled NELA dataset and the manually-labelled Facebook URLs dataset. 

Figures \ref{fig:baseline-pfi} and \ref{fig:external-pfi} display the feature importance plots for the Gradient Boosting models used in fake news detection, highlighting features that contribute meaningfully to model predictions. Due to the nature of the Gradient Boosting algorithm, certain features with `zero' importance were excluded from the plots. This exclusion likely results from the algorithm's tendency to select only one feature among highly correlated ones, thereby focusing on features with distinct positive or negative impacts on model performance.

\begin{table}[!h]
    \centering
    \resizebox{0.8\textwidth}{!}{ 
    \renewcommand{\arraystretch}{0.9}
    \small
    \begin{tabular}{|l|p{8cm}|}
        \hline
        \textbf{Feature} & \textbf{Description} \\ \hline
         ads              & Number of advertisements \\ 
         all\_caps        & Words written entirely in uppercase \\ 
         ampersand        & Frequency of ampersand characters (\&)\\ 
         at               & Frequency of the ``at" symbol (@) \\ 
         avg\_wordlen     & Average length of words \\ 
         CD               & Cardinal numbers  \\ 
         coleman\_liau\_index & Readability metric indicating grade level \\ 
         dollar           & Dollar signs (\$) \\ 
         exclamation      & Exclamation marks (!) \\ 
         ext\_total       & Total number of external links \\ 
         fb               & Presence of Facebook-related content \\ 
         FW               & Foreign words \\ 
         IngroupVirtue    & Words conveying positive group associations \\ 
         JJR              & Comparative adjectives (e.g., better) \\ 
         JJS              & Superlative adjectives (e.g., best) \\ 
         NNP              & Singular proper nouns \\ 
         NNPS             & Plural proper nouns \\ 
         percentage       & Percentage signs (\%) \\ 
         POS              & Part-of-speech tags \\ 
         PurityVice       & Words indicating impurity or moral vice \\ 
         question         & Question marks (?) \\ 
         RP               & Particles\\ 
         single\_quote    & Single quotation marks (') \\ 
         stops            & Stop words\\ 
         TO               & Infinitive marker ``to" \\ 
         ttr              & Type-token ratio (lexical diversity) \\ 
         twit             & Presence of Twitter-related content \\ 
         vadneu           & Neutral valence in sentiment analysis \\ 
         vadpos           & Positive valence in sentiment analysis \\ 
         VB               & Base form verbs \\ 
         VBN              & Past participle verbs \\ 
         WDT              & Wh-determiners (e.g., which) \\ 
         word\_count      & Total number of words\\ \hline
    \end{tabular}}
    \caption{Relevant features to both datasets}
    \label{tab:positive-feature-rankings}
\end{table}


In examining these plots, we can identify 33 features (Table \ref{tab:positive-feature-rankings}) that hold relevance across both datasets, including all four proposed social-monetisation features. This overlap provides additional evidence supporting the viability of social-monetisation features in enhancing the generalisability of fake news detection models. Notably, the ‘ads’ feature ranks highly in both datasets, reinforcing the idea that a key motivation for creating disinformation is often profit through advertising. This high ranking for `ads' aligns with findings in fake news literature that connect monetisation tactics, such as heavy ad placement, with disinformation. Additionally, the Facebook feature ranks highly in both datasets, indicating the prominent role social media platforms play in the dissemination of fake news. The consistent relevance of these features suggests that economic incentives, captured through social-monetisation indicators like advertisements and Facebook links, are significant drivers of disinformation. This aligns with prior research that highlights the exploitation of digital platforms for financial gain as a core characteristic of fake news.

From a stylistic perspective, exclamation marks consistently rank as the most important feature in both datasets, with ‘all caps’ words also ranking prominently. These features are frequently associated with fake news, particularly in sensationalist headlines or emotionally charged content. This emphasis on exclamations and capitalised words aligns with prior research that links these stylistic cues to disinformation. Additionally, features like ‘CD’ (cardinal numbers) and ‘single quotes’ also show high importance in both datasets, which could reflect the tendency of fake news content to use specific numbers or quotations for added emphasis or perceived authority.

These findings underscore the value of both social-monetisation and stylistic features in identifying fake news. The strong presence of social-monetisation features, combined with stylistic cues like exclamations and all-caps text, suggests that these elements are integral to creating and detecting disinformation. Together, they enhance model accuracy and contribute to the broader objective of building more generalisable fake news detection models.

\begin{table}[h]
\centering
\caption{Reduced Feature-Set Results}
\label{tab:study2-reducedresults}
\vspace{5pt}
\begin{adjustbox}{width=\textwidth}
\begin{tabular}{|l|c|c|c|c|c|c|c|c|c|c|}
\cline{2-11}
\multicolumn{1}{c|}{} & \multicolumn{5}{|c|}{\textbf{Original Feature-Set}} & \multicolumn{5}{c|}{\textbf{Reduced Feature-Set}} \\
\hline
\multicolumn{1}{|l|}{\textbf{Test}} & \textbf{Acc.} & \textbf{Prec.} & \textbf{Rec.} & \textbf{Spec.} & \textbf{F1} & \textbf{Acc.} & \textbf{Prec.} & \textbf{Rec.} & \textbf{Spec.} & \textbf{F1} \\
\hline
\textbf{K-Fold Test} & 0.94 & 0.93 & 0.96 & 0.92 & 0.94 & 0.91 & 0.89 & 0.94 & 0.89 & 0.91 \\
\textbf{Cross-Dataset Test} & 0.75 & 0.76 & 0.72 & 0.77 & 0.74 & 0.76 & 0.78 & 0.73 & 0.79 & 0.75 \\
\hline
\end{tabular}
\end{adjustbox}
\end{table}

Further analysis, involving the repetition of K-Fold cross-validation and cross-dataset testing using the 33 features that demonstrated positive feature importance across both datasets, revealed a slight decrease in K-Fold testing performance but slight improvements in cross-dataset testing on the Facebook URLs dataset. Specifically, accuracy, recall, and F1 score increased by 0.01, while precision and specificity each improved by 0.02. These findings indicate that the reduced feature set, while slightly compromising K-Fold testing performance, enhances generalisability when applied to external datasets. This underscores the value of prioritising features with consistent positive importance across datasets.

Compared to word embeddings such as Word2Vec and BERT, the reduced set of stylistic features offers notable advantages in terms of computational efficiency. Word embeddings typically require significant resources for both feature extraction and model training, particularly when fine-tuning pre-trained models on large datasets. In contrast, the streamlined stylistic feature set demands less computational overhead, enabling faster training and evaluation while maintaining competitive performance.

These results highlight the practical and efficient nature of stylistic features for real-world applications, where resource constraints and model scalability are critical considerations. By balancing performance, generalisability, and efficiency, the reduced feature set provides a compelling alternative to computationally intensive word embedding approaches.
