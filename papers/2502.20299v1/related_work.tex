The challenge of detecting fake news has prompted extensive research across various domains, leading to the development of numerous models and approaches. This section reviews key studies that have explored token-level representations, stylistic features, and multimodal approaches to enhance the accuracy and generalisability of fake news detection models

\subsection{Overview}

Current approaches to fake news detection largely focus on using a variety of supervised ML algorithms and features. Typically, such approaches are tested using either holdout testing or cross-validation, using an unseen portion of the datasets on which they were trained. Overall, current approaches achieve accuracies of ~80\% on average, with many achieving significantly higher results \citep{Hoy2021}. The features that are used in such approaches can broadly be categorised as either: (i) content-based, which are features derived from things such as article text, title, publisher and images; (ii) socially-based, which are features derived from social networks data (typically from X/Twitter), such as the relationships between users who share fake news and their profiles; and (iii) feature fusion, which includes a combination of these features \citep{Xie2020}. Of these groups, content-based features are overwhelmingly the most popular, with many methods having a particular focus on features derived from the article text. As social media companies such as X/Twitter become more restrictive in the access provided to their social networks through their APIs \citep{Blakey2024}, it is likely that content-based features will remain the most dominant approach in the field of fake news detection, making this category of approach the most fruitful to study. 

\subsection{Content-Based Textual Features}

Of the sub-categories of content-based features, textual features are the most prominently used. These can be broadly divided into the following sub-categories:

\begin{itemize}
    \item \textbf{Token-Level Representations:} These convert words into numerical vectors using methods like Bag of Words (BoW), TF-IDF, and word embeddings such as Word2Vec and BERT \citep{Thota2018}. BoW and TF-IDF represent text data as vectors based on word frequency and importance, respectively, while word embeddings like Word2Vec and BERT capture semantic relationships and contextual information between words. Studies such as \cite{Kaur2020} and \citep{Poddar2019} have shown high accuracies using these methods with various machine learning models, demonstrating their effectiveness in the field of fake news detection under hold-out testing conditions

    \item \textbf{Stylistic Features:} These include statistical features (e.g., average word length, sentence complexity, part-of-speech tags) and psycholinguistic features (e.g., sentiment analysis scores) generated by analysing the text corpus. Tools like Linguistic Inquiry Word Count (LIWC) produce such features, encompassing linguistic aspects and psycholinguistic processes. These features provide insights into the author's writing style and emotional tone, enhancing the analysis of text data \citep{Gravanis2019,Spezzano2021}. \cite{Fernandez2019} demonstrated strong performance using stylistic features, achieving up to 94.2\% accuracy in their experiments, highlighting the importance of stylistic analysis in improving model performance
\end{itemize}

Studies may also explore the combination of token-level representations and stylistic features to leverage the strengths of both approaches. Research combining these features have demonstrated strong performance in fake news detection. For instance, \cite{ngada2020fake} demonstrated over 95\% accuracy on the Kaggle `fake and real news' dataset across six different classification algorithms by integrating token-level and stylistic features. Similarly, Verma et al. (2021) showed that such combined features can generalise well across four datasets. However, these generalisation results stand in significant contrast to the wider literature \citep{Hoy2022,Gautam2020,Blackledge2021,Janicka20191089,Alnabhan2024,Liu2024}, suggesting a likely overlap between the datasets used, particularly in experiments leveraging a Kaggle dataset, where data collection methodologies are not often described and combining datasets for training and testing is a common practice. Such overlap may artificially inflate performance metrics and does not reflect the challenges of generalising to entirely unseen data. These findings underscore the need for careful dataset selection and validation to ensure meaningful evaluations of generalisability.

Beyond traditional token-based and stylistic methods, Large Language Models (LLMs) such as GPT and LLaMA represent a distinct advancement in natural language processing. Unlike embedding-based approaches like BERT, which focus on generating contextual embeddings for token-level representations, LLMs leverage extensive pretraining on vast datasets to perform text classification tasks holistically. These models excel in zero-shot and few-shot learning scenarios, demonstrating strong performance across tasks such as sentiment analysis and summarisation without requiring extensive fine-tuning \citep{kojima2022large}.

Preliminary evidence suggests, however, that LLMs struggle to achieve similar success in the fake news detection domain. Unlike their strong performance on tasks such as summarisation or sentiment analysis, their results in fake news detection have been consistently poor. Recent studies, including \cite{pavlyshenko2023analysis}, highlight significant differences in performance between LLMs and specialised NLP classifiers that leverage token-based features, with the latter consistently outperforming LLMs in this domain.

\subsection{Multimodal Appraoaches using Textual Features}

Multimodal approaches leverage various types of data, including textual features, to enhance the performance of fake news detection models. By combining different data sources, these approaches aim to create more comprehensive and accurate models

One strategy involves combining LIWC with other features. Tools like LIWC are often used in conjunction with additional data to improve model accuracy. \cite{Ahmad2020} and \cite{Shu2019} demonstrated that integrating LIWC features with user profile data results in high accuracies across various datasets. This combination allows models to capture not only the linguistic and psycholinguistic aspects of the text but also contextual information from user profiles

Another strategy is the integration of text and image data. \cite{Spezzano2021} highlighted the benefits of combining textual features with image data. By integrating LIWC features with visual information, their study showed significant improvements in model performance. This approach leverages the strengths of both textual and visual analysis, providing a more holistic view of the content

\subsection{Current Issues in Fake News Detection}

Despite positive outcomes in many studies, issues remain with datasets used for training fake news detection models, particularly those relying on token representations. Dataset size is a critical issue, as collecting and accurately labeling a large number of news articles is challenging \citep{DUlizia2021}. To manage this, articles are often labeled based on their publisher as a proxy for accuracy, which can introduce topical biases \citep{Torabi2019}. This can lead to models that perform well in hold-out test conditions but struggle to generalise outside of the training dataset \citep{Suprem2022}

Limited studies have explored the generalisability of these models. \cite{Gautam2020} observed a 39\% accuracy drop when models trained on political news were tested on celebrity news. Similarly, \cite{Castelo2019} found comparable accuracy drops across different news domains using small datasets of fewer than 500 articles. Multimodal approaches, like those examined by \cite{Liu2024}, combining text and image features, showed some cross-domain generalisation but significant accuracy drops to around 55\% under cross-dataset conditions. Notably, embeddings from more recent large language models such as GPT-4 also suffer from an inability to generalise when trained on these coarsely labelled datasets, as demonstrated by \cite{Alnabhan2024}

\cite{Hoy2022} focused on four political news datasets to assess generalisability within a single domain. They found significant accuracy drops and sensitivity to biases in token-representation models. Interestingly, while stylistic features also struggled with generalisability, they performed more consistently across datasets, suggesting lower sensitivity to dataset biases

This motivates further exploration of stylistic features, forming the foundation of the research presented in this paper. Studies by \cite{Shu2019BeyondCont} and \cite{Spezzano2021} support incorporating supplementary features beyond the text, arguing that combining different feature categories outperforms single-category approaches. Similarly, \cite{Liu2024} highlights the need to explore additional features to create more generalisable models, especially in cross-dataset conditions


\subsection{Proposed Features}
Given this evidence, this study seeks to propose a set of four novel features with the goal of producing more generalisable fake news detection models. These features are outlined as follows:

\begin{itemize}
    \item \textbf{Frequency of Ads:} One of the primary motivations behind the creation and dissemination of fake news is financial gain through advertising. According to \cite{Allcott2017}, fake news websites often rely on sensationalist and misleading content to attract high volumes of traffic, which in turn increases their advertising revenue. These sites typically feature a large number of advertisements, as their business model is heavily reliant on generating ad impressions and clicks. Therefore, the number of adverts associated with a given article could be a significant indicator of fake news. Articles that contain an unusually high number of ads may be designed to maximise revenue rather than to provide factual information, making this a critical feature to include in fake news detection models

    \item \textbf{External Links:} Similar to advertising, the prevalence of external links in an article can also be an indicator of fake news, especially when these links are intended for affiliate marketing purposes. Fake news articles often include numerous external links that direct readers to other sites, which can generate affiliate income for the publisher each time a link is clicked. This tactic is particularly common in misinformation related to healthcare and other high-interest topics, as noted by \cite{Rehman2022}

    \item \textbf{Social Media Share Links:} The role of social media in the spread of fake news is well-established, with platforms like Facebook and X/Twitter being primary channels for misinformation dissemination. One of the mechanisms that facilitate this spread is the use of visual cues, such as share buttons, which prompt habitual behaviour in social media users \citep{Ceylan2023}. When users encounter these visual cues, they are more likely to share the content without critically evaluating its veracity. Including 'call to action' links that lead to social media platforms in the analysis is essential, as these links can significantly amplify the reach of fake news articles. By encouraging readers to share content on social media, these articles can quickly go viral, spreading misinformation at an unprecedented rate. Therefore, factoring in Facebook and X/Twitter links is expected to be important in identifying articles that are designed to exploit social media behaviour for rapid dissemination. It is important to note these social media features are distinct from others seen in the literature, which typically focus on user profiles and relationships between tweets and users
\end{itemize}

This novel group of features shall be characterised as ‘social-monetisation’ features 
