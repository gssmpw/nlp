Fake news has been a topic of interest since the term was popularised at the time of Trump’s 2016 Presidential Election bid. It is typically characterised as content that appears to be news but is intentionally misleading for the purposes of generating profit through advertising or exerting political influence \citep{Allcott2017}. In recent times it has maintained its relevance in public discourse, particularly with the rise of generative text models that are capable of generating large amounts of misinformation quickly and easily \citep{Xu2023}. Detecting fake news before dissemination is crucial to uphold information integrity and maintaining public trust in media and institutions. In democratic societies, fake news can manipulate public opinion, sway elections, and undermine governance \citep{Morgan2018}. Moreover, false information about health crises, emergencies, or scientific discoveries can endanger public safety \citep{Nelson2020}. Socially, it deepens divisions, fuels polarisation, and exacerbates societal tensions \citep{Olan2024}. Therefore, effective detection and mitigation of fake news not only protects individuals from harm but also upholds the essential principles of truth, transparency, and responsible communication necessary for a well-functioning society.

Researchers in the field of Computer Science have aimed to address this problem, often  using Natural Language Processing (NLP) and Machine Learning (ML) classification techniques. While such approaches report good results under cross-validation and holdout test conditions, evidence suggests that current approaches struggle to generalise, particularly when relying on token-representation methods such as Bag-of-Words (BoW), Term Frequency - Inverse Document Frequency (TF-IDF) and Bidirectional Encoder Representations from Transformers (BERT). This issue is further exacerbated by the use of coarsely labelled datasets, commonly used in the literature, that label articles based on their publisher rather than individually through manual fact-checking. While Large Language Models (LLMs) have emerged as transformative tools in NLP, enabling sophisticated and context-aware text analysis, their application to fake news detection remains limited. Given these issues, investigating the issue of generalisability in both token-based and LLM-based fake news detection models is the primary focus of this study.

Evidence of the poor generalisability of fake news detection models using token-representations can be found in \cite{Alnabhan2024}, which explored the use of deep learning models and embeddings such as Glove and BERT. Results from this study demonstrate that models achieve high accuracies under holdout test conditions (around 99\%) but suffer a drop in accuracy of around 30\% when evaluated on several different datasets. Similar results are also observed in \cite{Hoy2022}, where models trained on one political news dataset, struggle to generalise to other political news datasets (also observing a ~30\% drop in accuracy). \cite{Hoy2022} additionally demonstrate, through the use of the Local Interpretable Model-agnostic Explanations (LIME) package in Python, that token-representations were more sensitive to topical biases within the datasets on which the models were trained, while stylistic features, though still performing poorly in terms of generalisability, demonstrated less sensitivity to such topical biases. These findings also indicate the need for more robust testing methodologies and datasets to ensure that models perform as expected in the real-world. 

Motivated by these findings, this study seeks to enhance the generalisability of fake news detection models by identifying a set of features that exhibit reduced sensitivity to biases present within datasets. Building on preliminary findings hinting at the lower sensitivity of stylistic features to these biases, these features will serve as the basis for further exploration. Additionally, aligning with approaches observed in existing literature, the study will propose additional features independent to the article text. This exploration aims to determine if these features contribute to the development of more generalisable fake news detection models—that is, whether they perform well when tested on different datasets— compared to the current approaches that largely rely on token-representations. Owing to the limited evidence of the efficacy of LLMs in this domain, the also study includes an evaluation of LLaMa to provide a basis for comparison against other approaches in addressing these challenges. Notably, this study employs the Facebook URLs dataset as the external validation set to evaluate generalisability; this dataset is not publicly available and is unique in that it has been manually labelled by an external fact-checking organisation, making it more representative benchmark than other datasets used in the literature. 

To meet the aims outlined above, this paper will commence with an exploration of the relevant literature; in particular, Section 2 will discuss studies that have focused on the use of stylistic features and their efficacy, arguing that the use of stylistic features may be more effective in producing more generalisable models that are less sensitive to topical biases. Building on this exploration, three research questions pertaining to this study will be formulated. Section 4 will discuss the methodology of the experimental work designed to address the research questions. It will commence with the methods underpinning the experiments, including the data collection process, feature-sets, training and testing methodology, machine learning algorithms and evaluation metrics chosen for the experiments. Section 5 will detail the results of the experiments. Section 6 will discuss these results in relation to existing research, motivating the paper’s conclusions and proposed directions of future work, which are outlined in Section 7.

