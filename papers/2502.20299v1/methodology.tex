\begin{figure}[h]
\centering
\includegraphics[width=\linewidth]{ExperimentOverview.PNG}
\caption{Study Overview}
\label{fig:overview}
\end{figure}

This section details the methodology used to address the research questions outlined in Section 3. The study conducted two experiments (summarised in Figure 1) to achieve this. 

The first experiment aimed to evaluate the generalisability of commonly used token-representations, including Bag of Words (BoW), TF-IDF, Word2Vec and BERT, as well as the Large Language Model `LLaMA', addressing RQ1. Models using token-representations were trained on the NELA 2020-21 dataset and their performance evaluated using K-fold cross-validation. For LLaMA, the zero-shot configuration relied on its pre-trained knowledge and structured prompts, while the few-shot configuration incorporated labelled examples from the NELA dataset to guide classification. The fine-tuned LLaMA model followed a methodology similar to the token-based approaches, enabling a direct comparison of generalisability across techniques. Models trained/fine-tuned on the NELA dataset were then evaluated using external validation on the Facebook URLs dataset. By evaluating token-representations and LLMs in this manner, the study established a baseline for comparison with other feature-sets used in the second experiment

The second experiment focused on assessing the generalisability of five groups of stylistic features and evaluating the impact of newly proposed social-monetisation features. This experiment addressed two research questions: the effectiveness of stylistic features (RQ2) and the improvement in model performance through the inclusion of social-monetisation features (RQ3). Similar to the first experiment, models were trained using the NELA 2020-21 dataset, initially evaluated using K-fold cross-validation, before being externally evaluated on the Facebook URLs dataset to assess generalisability. These tests were conducted, both with the inclusion and exclusion of the proposed social-monetisation features. This was done to determine if the proposed features resulted in a statistically significant improvement in generalisability performance using the Mann-Whitney U-Test. 

The following subsections elaborate on the methodology. Section 4.1 outlines the data collection process and resulting datasets that were used in the experiments. Section 4.2 to 4.3 describe the features that were extracted from these datasets in relation to Experiment 1 and Experiment 2 respectively. Section 4.4 outlines the machine learning algorithms that were used in these two experiments and how they were trained and tested. 

\subsection{Data Collection \& Processing}

This section outlines the datasets and data extraction methods used. Owing to the nature of the proposed social-monetisation features, the dataset required the source URL of the articles to facilitate the extraction of these features. \cite{Capuano2023}'s systematic review lists several datasets used in content-based fake news detection. However, out of the 19 datasets reviewed, only three — FakeNewsNet, Buzzfeed, and Celebrity fake news—include the article's source URL. These datasets are relatively small, which limits the likelihood of producing a generalisable model. To develop a more comprehensive and reliable model, a larger dataset is necessary. Therefore, the NELA series of datasets was chosen for its large size and inclusion of article URLs, providing a more extensive and diverse data source for training. Using a dataset of this size also ensures that a significant number of articles can be extracted to compensate for pages that are no longer available. While not as frequently used in the literature, a number of studies make use of this dataset including \cite{Horne2020}; \cite{Raj2023} and \cite{Raza2022}

The latest iterations of this dataset released in March 2023, NELA 2020 and 2021, were chosen for this study. Each dataset contains over a million articles from various sources and are coarsely labelled, with each article's legitimacy derived from its source's aggregated label from seven assessment sites: Media Bias Fact Check, Pew Research Center, Wikipedia, OpenSources, AllSides, Buzzfeed News, and Politifact. The labels are categorised as unreliable, mixed, and reliable. For this study, only `unreliable' and `reliable' labels were used, excluding the `mixed' label to align with the binary labels in the external validation dataset

\begin{figure}[h!]
\centering
\includegraphics[scale=0.7]{Articlespersourcer3.PNG}
\caption{Articles per Source Prior to Extraction}
\label{fig:art-per-src-pre}
\end{figure}

\begin{figure}[h!]
\centering
\includegraphics[scale=0.7]{Articlespersource-filteredr3.PNG}
\caption{Articles per Source Post Extraction}
\label{fig:art-per-src-post}
\end{figure}

The combined NELA 2020-21 dataset includes 3,635,636 records from 525 unique sources. After joining the labels file and excluding the `mixed' category, the dataset consists of 1,013,808 `true' and 551,051 `fake' articles from 224 sources. To prevent any single source from dominating the training set \ref{fig:art-per-src-pre}, the number of URLs extracted from each source was reduced using the 1st quartile as a threshold (285 articles per source), resulting in a final set of 22,230 `true' and 25,650 `fake' articles from 168 sources (Figure \ref{fig:art-per-src-post}).

The Facebook URLs Dataset was chosen as an external validation dataset owing to its unique position as a dataset collected in a ‘real-world’ context and granular labelling by a third-party fact-checking organisation. Its individual article labels provide a robust standard for assessing model accuracy and practical applicability in fake news detection. This stands in contrast to commonly used datasets in the field, which often employ coarse labels based on article publishers, potentially misrepresenting the true nature of fake news. By using a coarsely labelled dataset for training and a manually labelled dataset for testing, the aim is to demonstrate that despite the limitations of coarsely labelled datasets, meaningful features can still be extracted to develop robust models applicable in real-world scenarios

The Facebook URLs dataset contains over 38 million URLs shared on Facebook since January 1, 2017, with 35,924 records identified as fake news. The dataset is protected with differential privacy, ensuring no information can be gathered regarding individuals \citep{Messing2020}. Given its restricted accessibility and limited usage in prior studies, this research represents one of the few to utilise the Facebook URLs Dataset for fake news classification, following a study by \cite{Barnabo2022}. The dataset initially comprised 28,271 fake and 7,653 true records, with non-English articles filtered out based on `US' and `UK' values in the `Public Shares Top Country' field, resulting in 14,354 fake and 1,468 true records. To enhance dataset quality, URLs referring to Tweets and videos were excluded. Class balancing was implemented during experimentation. Due to its size, the Facebook URLs Dataset served as a test set for cross-dataset testing, complementing the larger training datasets to bolster the model's generalisability and validate its performance in diverse real-world scenarios

In order to extract the raw textual data from the URLs in these datasets, the BeautifulSoup library was used. As many webpages in these datasets may no longer be available, particularly in relation to ‘fake’ news pages, initial extraction was attempted through the use of the Wayback Machine API (Internet Archive). This was done to increase the likelihood of extracting a webpage with a complete article and not a splash page indicating the article had since been deleted. In instances where webpages were not available in this archive, a final extraction attempt was made directly from the webpage using the URL provided in the dataset to account for cases where webpages may not yet have been added to the Internet Archive. If through these methods a complete article was not extracted, the URL would be excluded from the resulting dataset. 

In cases where full articles were available, rather than attempt to accurately extract only the text pertaining to the news articles from these URLs, all textual elements are extracted from the body of the webpage. While this may introduce additional noise to the feature-sets, it was a deliberate choice. Websites have different layouts, styles and coding structures, making it challenging to consistently and accurately extract only the article text. It is argued that models that extract all textual elements from the webpage body are more adaptable to the varying structures and formats of webpages and, as such, have the potential to be more robust and scalable across a wider range of online content. Following this data extraction phase, pages returning \textless3KB of data were excluded, as it was observed that pages with less than this amount of data had typically had their articles removed. The resulting datasets are summarised in Table \ref{tab:Dataset Statistics}:

\begin{table}[h]
\centering
\caption{Dataset Statistics}
\label{tab:Dataset Statistics}
\vspace{5pt}
\begin{tabular}{|l|c|c|}
\cline{2-3}
\multicolumn{1}{c|}{} & \textbf{NELA 2020-21} & \textbf{Facebook URLs Dataset} \\
\multicolumn{1}{c|}{} & \textbf{(Training Dataset)} & \textbf{(External Validation)} \\
\hline
\textbf{Fake} & 10,529 & 5,355 \\
\textbf{True} & 10,487 & 798 \\
\hline
\end{tabular}
\end{table}

\subsection{Experiment 1 Features: Token-Representations \& LLMs}

This section outlines the features to be used in the first experiment, addressing RQ1. This experiment aims to address RQ1, by exploring how well models using token-representations generalise between two different datasets of the same topic, using the NELA and Facebook datasets. An overview of the procedure followed in this experiment is provided in Figure \ref{fig:exp1}. In this section, each of the token-representations used in this experiment and the libraries used in extracting these features from the datasets are outlined. The results of this experiment are presented in Section 5.1. 

\begin{figure}[ht]
\centering
\includegraphics[scale=0.37]{Experiment1Flowchartr3.PNG}
\caption{Experiment 1 - Token-Representations Overview}
\label{fig:exp1}
\end{figure}

\subsubsection{Token-Representations}

The token-representations chosen for this study are motivated by the systematic review by \cite{Capuano2023}, which identified the following token-representations among the most popular in the literature for content-based fake news detection:

\begin{itemize}
    \item \textbf{Bag-of-Words (BoW)} converts text into fixed-length vectors based on word frequencies. It does not take into account word-order. In this study, BoW was implemented with SKLearn's CountVectorizer with a max\_features parameter of 10,000 words. 

    \item \textbf{TF-IDF} enhances BoW by considering word frequency relative to the dataset, capturing token significance. This technique, therefore, better captures the significance of tokens compared to BoW. However, like BoW of words it fails to account for word-order. TF-IDF is among the most popular feature extraction approaches, as identified by \cite{Capuano2023}, owing to its simplicity and focus on informative tokens. In this study, TF-IDF was implemented with SKLearn's TFIDFVectorizer and max\_features set to 10,000 words

    \item \textbf{Word2Vec} generates word embeddings by training a neural network to predict target words from their context, capturing semantic meaning but ignoring word order (similar to the previous two techniques). It is one of the most popular feature extraction methods, identified in approximately 25\% of studies in an ongoing systematic review by the authors. For this study, a pre-trained Word2Vec model trained on Google News data was implemented using the Zeugma library

    \item \textbf{BERT}, similar to Word2Vec, also generates word-embeddings but with the distinct advantage of being context-dependent thus allowing for unique representations of words morphologically similar words. BERT achieves this through a novel approach of training at the sub-word level, encoding word-positions and training on tasks such as Masked-Language-Modelling (MLM) and Next-Sentence Prediction (NSP). Although other, more advanced,  transformer-based models like GPT-4 exist, they are still much less well-established in the literature for this specific task. Therefore, BERT has been chosen for this study In this study, BERT was implemented using the SentenceTransformers library using the pre-trained ‘bert-base-uncased’ model. Additionally, a fine-tuned version of this model trained on the NELA dataset was employed to further optimise performance for the fake news detection task.
\end{itemize}

In the case of the BoW and TF-IDF approaches, the following steps were taken to remove any unwanted noise from the text: (i) converting the text to lowercase to ensure all words were treated uniformly; (ii) lemmatising the text; and (iii) removing punctuation, URLs, Twitter handles, extra-whitespace and stop words. Word2Vec and BERT did not undergo the above three steps  because these techniques require contextual information in order to generate their embeddings. 

\subsubsection{LLMs}

Building on the foundation of traditional token-based features, this study also incorporates the large language model \textbf{LLaMa 3.2-1B}, a transformer-based model with 1 billion parameters. Chosen for its robust natural language understanding and computational efficiency, the model provides a strong baseline for evaluating advanced detection techniques.

LLaMa was utilised in three configurations: zero-shot, few-shot, and fine-tuning. In the zero-shot configuration, the model leveraged its pre-trained knowledge without task-specific training, providing an initial benchmark for its capabilities. Few-shot learning introduced a small number of labeled examples to guide predictions, while fine-tuning adapted the model comprehensively by training it on labeled datasets.

To operationalise LLaMa in zero-shot and few-shot learning, structured prompts were designed to align with the task requirements. The system prompt defined the classification task and response format, while user prompts provided the input text for classification. For example:

\begin{verbatim} 
{"role": "system", "content": 
"Classify the text as `fake' news or `real' news. 
Respond only with `fake' or `real'."} 

{"role": "user", "content": 
"Text: {INSERT ARTICLE TEXT} 
\n Is this `fake' or real' news?"} 
\end{verbatim}

In the few-shot configuration, a series of four labelled examples (two from each class: fake and real) were provided as prompts to demonstrate the classification task to the model. The subsequent prompts iteratively processed the remaining dataset, excluding the initial examples, to classify each instance while leveraging in-context learning for improved performance. This approach was adopted to utilise the model’s capability to learn task-specific patterns from minimal examples while preserving token space for processing longer texts effectively.

For fine-tuning, \textit{Low-Rank Adaptation (LoRA)} and \textit{Parameter-Efficient Fine-Tuning (PEFT)} were employed \citep{pavlyshenko2023analysis}. LoRA targeted the transformer’s query (\texttt{q\_proj}) and value (\texttt{v\_proj}) projection layers with a rank of 8, alpha scaling of 32, and dropout of 0.1, enabling efficient learning of task-specific patterns. PEFT further optimized this process, allowing the model to retain its pre-trained knowledge while adapting to the binary classification task.

The fine-tuning process included tokenizing input texts to a maximum length of 512 tokens, configuring training with a learning rate of $2 \times 10^{-5}$, weight decay of 0.01, and three epochs, and managing the process using the Hugging Face \texttt{Trainer}. This configuration complemented the zero-shot and few-shot setups by embedding task-specific knowledge directly into the model, enabling scalable and efficient fake news detection.


\subsection{Experiment 2 Features: Stylistic \& Proposed Social-Monetisation \\Features}

The following section outlines the stylistic features that were used in addressing RQ2 Experiment 2 follows a similar structure to Experiment 1 and evaluates the generalisability of five groups of stylistic features proposed by previous research and compares it against the results of Experiment 1, which explored the generalisability of token-representations. Then, it addresses RQ3 by exploring whether the four social-monetisation features proposed by this study improve generalisability. An overview of the procedure followed in Experiment 2 is shown in Figure \ref{fig:exp2}. The stylistic features and social monetisation features used in the experiment are presented in Sections 4.3.1 and 4.3.2, respectively. The results of this experiment are described in Section 5.2.

\begin{figure}[ht]
\centering
\includegraphics[scale=0.37]{Experiment2Flowchartr3.PNG}
\caption{Experiment 2 - Stylistic Features Overview}
\label{fig:exp2}
\end{figure}

\subsubsection{Selection of Stylistic Features}

Five groups of stylistic features, proposed in the literature, are evaluated in this study. Each feature group varies in complexity, with the first group focusing solely on linguistics, while subsequent feature-sets progressively incorporate additional groups such as psycholinguistics and document complexity. Due to the diverse scales of many features within these groups, we applied StandardScaler to ensure uniform treatment by machine learning algorithms. The selection of these five groups was motivated by their inclusion in the limited number of studies exploring generalisability of fake news detection models (with the exception of the NELA feature-set which was chosen owing to the use of the NELA dataset in this study). However, these studies only used coarsely labelled datasets for external validation. As such, we aimed to observe their performance using real-world data from the Facebook URLs dataset, underscoring the relevance of these features in practical applications of fake news detection. The complete table detailing these features is provided in the Appendix

\paragraph{Group 1: Fernandez and Devaraj Stylistic Features}

This study employed the collection of 34 linguistic attributes (referred to as ‘Linguistic Dimension’ and ‘Punctuation Cues’) that demonstrated the highest efficacy in classifying fake news, as determined through a sequence of tests outlined by \cite{Fernandez2019}. The inclusion of these features was motivated by \cite{Hoy2022} which used these features in providing preliminary evidence that stylistic features have the potential to be more generalisable than token-representations. But both studies relied on coarsely labelled datasets The two groups of features can be summarised as follows:

\begin{itemize}
    \item \textbf{Linguistic Dimensions:} Based on the Linguistic Dimensions of LIWC, this category aims to capture the complexity of news through inclusion of features such as word-per-sentence, average word size and type-token ratio (a measure of lexical variety) as well as the different types of words used such as the ratio of adjectives, nouns, verbs and named-entities.
    \item \textbf{Punctuation Cues:} Focuses solely on the different types of punctuation used relative to all punctuation in a given article.
\end{itemize}

As such linguistic features are a staple in NLP, the other groups of features described below also include similar groups of features. A comprehensive list of these features is provided in Table \ref{tab:fernandez}

\paragraph{Group 2: Abonizio Features}

This study leverages 21 features organised into three groups: complexity, stylometric and psychological. The inclusion of these features, similar to the previous group, is motivated by their use in another generalisability test on coarsely labelled datasets in \cite{Abonizio2020} Similar groups of features can also be found in \citep{Paschalides2019,Garg2022,Reis2019} thus motivating their inclusion in this study.

The ‘complexity’ and ‘stylometric’ features overlap with some of the features used in the previous ‘Fernandez’ feature-set however it should be noted that the Abonizio feature-set is not as granular. However, unlike the Fernandez feature-set, the Abonizio feature-set does extend to include a psychological category, capturing the sentiment analysis score of a given article.  A list of these features is provided in Table \ref{tab:abonizio} 

\paragraph{Group 3: Linguistic Inquiry Word Count (LIWC)}

As previously mentioned in Section 2, LIWC is a dictionary-based approach comprising of linguistic elements, punctuation characteristics as well as psycholinguistic features organised in a number of categories. These categories can be summarised as follows: 

\begin{itemize}
    \item \textbf{Summary Variables:} Aim to summarise the features from the below three categories and attempt to capture document complexity and psychological features.
    
    \item \textbf{Linguistic Dimensions:} Aims to capture different types of words such as pronouns, verbs and adjectives as well as words denoting grammatical person and numbers. 

    \item \textbf{Psychological Processes:} captures a number of psychological words relating to sentiment (such as ‘good’ and ‘bad’) as well as words relating to cognition (e.g. ‘know’ and ‘think’) and social processes (such as ‘love’ and ‘fight’).   

    \item \textbf{Expanded Dictionary:} captures a range of different words relating to a number of different topics such as culture and lifestyle as well as temporal words such as ‘when’, ‘now’ and ‘then’. 

\end{itemize}

The total number of features in this feature-set amount to 118. Similar to the previous feature groups, LIWC is used in a generalisability study by \cite{Perez-Rosas2018} which observed the performance of LIWC trained on the FakeNewsAMT dataset and tested Celebrity news datasets and vice versa. A number of other studies also leverage these features, thus further justifying their inclusion \citep{Ahmad2020,Spezzano2021,Shu2019BeyondCont} An exhaustive list of these features is provided in Table \ref{tab:LIWC}

\paragraph{Group 4: NELA Feature Extractor}

The NELA feature extractor is a tool hosted on GitHub designed by the authors of the NELA dataset which has been used throughout this study. This therefore motivated the inclusion of these features in this study. It includes a rich, hand-crafted feature-set of 91 features which can be summarised into the following categories:  

\begin{itemize}
    
    \item \textbf{Style:} Largely similar to those from the previous three studies, focussing on POS tags

    \item \textbf{Complexity:} Similar to the ‘Linguistic Dimensions’ and ‘Complexity’ categories of the Abonizio feature-set, this category of features aims to capture how complex an article is through analysing lexical diversity, reading-difficulty metrics and the average length of words and sentences

    \item \textbf{Bias:} Based on \cite{Recasens2013} work, this category of features aims to capture the subjectivity of the text by identifying the number of hedges, factives, assertives, implicatives, and opinion words.

    \item \textbf{Affect:} Relying on VADER sentiment analysis, this category aims to capture the emotion and sentiment of the text

    \item \textbf{Moral:} The objective of this feature category is to encompass the ethical content present in a text, and it is built upon the principles of Moral Foundation Theory (MFT) introduced by \cite{GRAHAM201355}.\cite{Lin2018} subsequently expanded upon this theory and developed a lexicon specifically designed for assessing the moral aspects of text. This feature group employs the lexicon established by \cite{Lin2018} to gauge the morality of the text under consideration.

    \item \textbf{Event:} Aims to capture words relating to dates, times and locations.

\end{itemize}

A full list of these features is provided in Table \ref{tab:NELA}

\paragraph{Group 5: Modified NELA Features}
Through the use of the NELA Feature Extractor, it was noted that a number of the features were either duplicated or returning zero values, in particular, when attempting to extract punctuation. As such, the NELA Feature Extractor was modified to remediate this and to include additional punctuation such as ‘\#’, ‘@’, ‘£’, ‘\$’, ‘\&’ and ‘\%’. The normalisation was also adjusted depending on the features. For example, rather than scaling the punctuation based on the word-count of an article, punctuation was scaled based on the total number of punctuation items. 

\subsubsection{Proposed `Social-Monetisation' Features}

As motivated in Section 2.5, a number of additional novel features, categorised as ‘social-monetisation’ features, were investigated to complement the groups of stylistic features selected for this study. These were: (i) the number of advertisements; (ii) the number of external links; (iii) the number of links to Facebook; and (iv) number of links to Twitter/X.  The number of ads was extracted through the use of EasyList, an open-source project that compiles a list of the most popular adblocking filters. Using this list enables searching the webpage’s LXML tree and counting the frequency of various ads. External links were identified through a combination of extracting ‘hrefs’ in the webpages and comparing their domains to the host domain using the ‘tldextract’ library. Links the domains of which did not match the host domain were used to calculate the frequency of external links. Links that pointed to Facebook and Twitter/X were each counted separately. 

\subsection{Machine Learning Algorithms}

As this study prioritises the exploration of stylistic features for generalisable fake news detection, less emphasis has been put on exploring the effect of different machine learning algorithms and their respective hyperparameters. However, for completeness and to offer an opportunity for comparison to the literature, a number of machine learning algorithms including Logistic Regression, SVM, Gradient Boosting, Decision Trees, Random Forest, and a feed-forward neural network (FFNN) are employed. Each of these algorithms was implemented using default hyperparameters in SKLearn. The exception to this was the neural network where default hyperparameters are not available and, as such, a shallow Sequential model was used with a single hidden layer of 10 neurons, a sigmoid activation layer compiled with binary cross-entropy loss and the Adam optimiser. To protect against overfitting, the EarlyStopping hyperparameter was set to stop training if the loss function did not improve by 0.01.

Additionally, a Long Short-Term Memory (LSTM) network was employed to process word embeddings generated by BERT and Word2Vec. LSTMs are well-suited for capturing sequential dependencies in embeddings, providing a deeper understanding of contextual relationships within the text. This approach was not applied to Bag-of-Words or TF-IDF features, as these methods represent text as sparse matrices, lacking sequential and contextual information, rendering them unsuitable for use with LSTMs. This distinction highlights the effort to align algorithms with feature sets that best leverage their strengths for fake news detection. The LSTM architecture used in this study consisted of an LSTM layer with 128 hidden units, designed to model complex temporal dependencies and contextual patterns in the data. A dropout rate of 40\% was applied to the LSTM's output to reduce the risk of overfitting. Finally, a fully connected layer was used to map the final hidden state to two output classes: fake and real news.

\subsection{Training/Testing Methodology}

The training and testing methodology in this study integrates K-fold cross-validation, external validation, and tailored evaluation techniques for large language models (LLMs) to assess performance and generalisability in fake news detection. For traditional machine learning algorithms and feature sets, K-fold cross-validation is employed on the NELA dataset, partitioning it into 10 equal folds. Models are trained on 9 folds and validated on the remaining fold across iterations, ensuring robust performance estimates with a fixed random state (set to 42). External  validation evaluates the generalisability of models by testing those trained on each NELA fold against 500 randomly sampled articles per class from the Facebook dataset, addressing class imbalance. Key evaluation metrics such as Accuracy, Precision, Recall, Specificity and F1-Score are used to assess model performance in distinguishing between true news' and fake news'. By evaluating the models using external validation after each fold.

For LLaMa zero-shot and few-shot models, a distinct methodology was adopted due to its pre-trained nature. In the zero-shot configuration, the model leveraged its existing knowledge without any task-specific training, relying entirely on structured prompts to classify text from both the entirety of the NELA dataset and 5 random balanced samples of 500 Facebook URLs dataset. In the few-shot configuration, the model was guided by labelled examples from the NELA dataset only to reflect the goals of this study to train on a coarsely labelled dataset but test on a manually labelled dataset. The fine-tuned LLaMa model followed a similar methodology to the above traditional methods.

In addition to the Mann-Whitney U-test,  Permutation Feature Importance (PFI) was used to pinpoint the stylistic and proposed social monetisation features that positively contributed to a more generalisable model. PFI works by shuffling a random feature, thereby disrupting the relationship between that feature and the target variable. By repeating this process for all features in the dataset and observing the effects on model performance, the method reveals how much the model depends on each feature
