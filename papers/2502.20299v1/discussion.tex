The motivation for this study stemmed from previous findings indicating poor generalisability of current fake news detection approaches. Current approaches often rely on token-representations and coarsely labelled datasets, that is, datasets that use the article’s publisher as a proxy for labelling articles as ‘fake’ or ‘true’. Building on findings that suggest stylistic features are less sensitive to biases in datasets, this study aimed to produce a generalisable model, trained on a coarsely labelled dataset (NELA) that performs well on real-world data (Facebook URLs Dataset). Additionally, the study proposed four social-monetisation features, and investigated the effectiveness of these novel features in producing more generalisable models

The first contribution of this study, relates to RQ1, confirming the issue of poor generalisability of models that use token-representations. Unlike previous studies that have examined generalisability across coarsely labelled datasets, this research focuses on 'real-world' data that has been manually fact-checked. Experiment 1 demonstrated that common token-representations (including BoW, TF-IDF, Word2Vec and BERT) suffer a similar drop in accuracy on the real-world data in the Facebook URLs dataset as other studies in the literature exploring generalisability between coarsely labelled datasets \citep{Silva2020,Lakshmanarao20193125,Kresnakova2019,Smitha2020}. There was also a large degree of variation in recall and specificity in these models, notably the BoW and TF-IDF Random Forest models and the BoW SVM model, which produced significantly higher values for recall compared to specificity. Additionally, the Word2Vec Logistic Regression model produced significantly higher specificity than recall. All these models provided similar levels of accuracy. Further exploration of LLMs in this study, specifically using the fine-tuned LLaMA model, revealed similar issues regarding generalisability and metric imbalances. The fine-tuned LLaMA model achieved an accuracy of 0.71 on the manually fact-checked Facebook URLs dataset, outperforming token-representation models in this regard. Additionally, it demonstrated high specificity (0.95), indicating strong performance in detecting fake news. However, recall was significantly lower (0.47) coupled with high precision (0.9), suggesting that while the model excelled at identifying fake news, it was conservative when classifying instances of true news. The zero-shot and few-shot configurations of LLaMA further highlighted its limitations in this domain. In the zero-shot configuration, the model performed poorly, failing to generalise effectively without task-specific fine-tuning. Similarly, the few-shot configuration, despite providing some task-specific context through labelled examples, showed substantial performance gaps, with accuracy and recall lagging behind those of the fine-tuned configuration. These results suggest that LLaMA, like other LLMs \citep{pavlyshenko2023analysis, kumar2024silver}, struggles to achieve robust performance in fake news detection with and without extensive adaptation to the task. This imbalance between specificity and recall across all LLaMA configurations mirrors the trends observed in token-representation models, underscoring the inherent difficulty in balancing these metrics across different approaches. These findings highlight the challenges of achieving generalisability and balanced performance when training models on coarsely labelled datasets commonly used throughout the literature.

Further reflection on the performance of these models also brings into focus an issue in current fake news research: there is, as yet, no consensus on whether to optimise for recall (accurate detection of true news) or specificity (accurate detection of fake news). It could be argued that optimising for recall is desirable as it may be critical to avoid censoring true news unintentionally, even if it is at the expense of misclassifying fake news. This approach prioritises legitimate information being freely disseminated, which is crucial in maintaining the integrity of open communication and the right to free speech. However, a high recall could lead to the spread of more false information, which can undermine public trust and have serious societal consequences Conversely, it could be argued that optimising for specificity is desirable, as capturing all instances of fake news is important, even if it is at the expense of misclassifying true news. This approach focuses on minimising the harm caused by misinformation, which can sway public opinion, impact elections, and incite unrest. However, excessive misclassification of true news could lead to censorship, limiting the diversity of viewpoints and potentially suppressing important information. Balancing these priorities is essential for developing effective and ethical fake news detection systems. As such, it is recommended that models aim to produce a similar ratio of false positives to false negatives, to ensure both classes of news are treated equally by the model.

The second contribution of this study pertains to RQ2, empirically validating that stylistic features can lead to more generalisable models. Experiment 2 provides evidence that while stylistic features did not significantly outperform token representations in terms of accuracy, they maintained more balanced recall and specificity. This balance suggests that models using stylistic features are less prone to producing false positives and false negatives, making them more suitable for generalisable fake news detection. Additionally, the resilience of stylistic features against potential biases related to specific topics and concept drift further contributes to their balanced performance \citep{Przybya2020}. As such, this may contribute to the model performing well across other datasets not utilised in this study. From a feature engineering and interpretability standpoint, stylistic features offer a straightforward and transparent means of identifying the specific features influencing the model (as demonstrated in the PFI analysis), unlike complex token representations \citep{Qiao2020}

The third contribution of this paper relates to the proposed social-\\monetisation features, used in addressing RQ3. These features, as demonstrated in Experiment 2, produced a statistically significant increase in accuracy under cross-dataset testing conditions on the manually labelled Facebook URLs dataset. The Random Forest and Gradient Boosting models, in particular, achieved a mean accuracy of 75\%, maintaining balanced specificity and recall. These results underscore the benefit of multimodal approaches that utilise features outside the article text, producing more robust and generalisable models. Similar to stylistic features, the proposed social-monetisation features can also be considered robust in regard to potential topical biases in datasets and concept drift. The feature importance analysis also demonstrates how these features, in particular the frequency of ads, positively contribute to producing more generalisable models.

The fourth contribution of this study relates to the PFI analysis, which demonstrated that a simplified feature-set performs comparably to the original comprehensive feature-set. Owing to the positive impact of the proposed social-monetisation features in the original model, this simplified model further underscores the utility of these novel features. The study also demonstrates the utility of this simplified feature-set alongside traditional machine learning algorithms such as Gradient Boosting. This simplified feature-set contributes to the efficiency in retraining of models, as well as extracting features for classifying unseen data, which is crucial for keeping pace with the constantly evolving news landscape. This is in contrast to fine-tuning large language models (LLMs), which can be time consuming and computationally expensive. 

Overall, this study makes significant contributions to the field of fake news detection by tackling the often-overlooked issue of generalisability. It highlights the limitations of training token-representations on coarsely labelled datasets, demonstrates the balanced performance provided by stylistic features, and introduces novel social-monetisation features that significantly improve model performance. These findings support the value of multimodal approaches in fake news detection and provides a foundation for future research to further enhance the robustness and applicability of fake news detection models in real-world scenarios