\section{Literature Review}
\subsection{Time Series Representation Learning}

Time series representation learning methods can be categorized based on the backbone into five groups: MLPs, RNNs, CNNs, GNNs, and Transformers. 

Inspired by the efficient performance of autoregressive models, 
MLPs 
such as DLinear____ demonstrate excellent performance. However, these methods often require additional design to capture time-wise dependency effectively. 
RNNs____ are naturally suitable for modeling sequential data, while they suffer from issues such as gradient vanishing due to recurrent structure and struggle to learn relationships between multivariate variables. 
CNNs, unlike RNNs, are less prone to gradient vanishing and excel at capturing the local patterns in time series. However, they often require stacking multiple convolutional layers to learn global futures, as seen in TCN____, which results in a significant training time cost. 
GNNs____ abstract variables as nodes and establish edges between multivariate variables, learning spatial dependencies through GCN____. However, this approach relies on message passing to capture global features, and shows less scalable than Transformers. 
Leveraging the self-attention mechanism, Transformers are particularly adept at learning long-term temporal dependencies and complex multivariate correlations. 
Transformers can be categorized into three categories according to different types of tokenization. 
Point-wise methods____ learn correlations between time steps but become computationally expensive for long sequences.
Series-wise methods____ pay attention to model multivariate dependencies by tokenization, but struggle with complex temporal patterns.
Patch-wise methods____ adjust patch sizes for flexibility across different time series, making them more adaptable to different types of time series data.
These methods have demonstrated certain advantages in specific tasks, and our work also adopts a patch-based approach in light curve processing.

\subsection{Time Series Analysis based on PLMs} 

Among time series large models, aside from MOMENT____ and Chronos____ which are trained from scratch using big time series data, most approaches are adaptations of existing PLMs. According to modification methods, these approaches can be sorted into three types:

\textbf{Fine-tuning.} Studies like UniTime____ and OFA____ unfreeze a portion of parameters, while others, including TEMPO____ and LLM4TS____ leverage Parameter-Efficient Fine-Tuning (PEFT) methods, such as LoRA____, adapt to new data by increasing trainable parameters without disrupting the existing structure of the large model. 


\textbf{Alignment.} PLMs trained on text data need to be aligned with time series data in the same data space. Based on the object of modification, these approaches can be divided into two categories. The first approach fine-tunes PLMs with time series data to map the model parameters into time series data space, as seen in LLM4TS____. The other approach maps the time series vector into text space, as demonstrated in TIME-LLM____, which employs multi-head attention mechanisms to achieve mapping.

\textbf{Prompt-learning.} Studies such as UniTime____ incorporate text prompts, while TEST____ employs the combination of trainable vectors and textual token embeddings to improve performance.



Although these methods generally perform well on specific tasks, they struggle to simultaneously handle multi-task time series analysis and text feature extraction.