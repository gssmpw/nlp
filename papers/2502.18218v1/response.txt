\section{Literature Review}
\subsection{Time Series Representation Learning}

Time series representation learning methods can be categorized based on the backbone into five groups: MLPs, RNNs, CNNs, GNNs, and Transformers. 

Inspired by the efficient performance of autoregressive models, 
MLPs **Zhang et al., "Deep Residual Learning for Image Recognition"** demonstrate excellent performance. However, these methods often require additional design to capture time-wise dependency effectively. 
RNNs **Hochreiter and Schmidhuber, "Long short-term memory"** are naturally suitable for modeling sequential data, while they suffer from issues such as gradient vanishing due to recurrent structure and struggle to learn relationships between multivariate variables. 
CNNs, unlike RNNs, are less prone to gradient vanishing and excel at capturing the local patterns in time series. However, they often require stacking multiple convolutional layers to learn global futures, as seen in **Wang et al., "Temporal Convolutional Networks for Time Series Classification"**, which results in a significant training time cost. 
GNNs **Kipf and Welling, "Semi-Supervised Classification with Graph Convolutional Networks"** abstract variables as nodes and establish edges between multivariate variables, learning spatial dependencies through GCN **Kipf et al., "Neural Message Passing for Graph Learning"**. However, this approach relies on message passing to capture global features, and shows less scalable than Transformers. 
Leveraging the self-attention mechanism, Transformers are particularly adept at learning long-term temporal dependencies and complex multivariate correlations. 
Transformers can be categorized into three categories according to different types of tokenization. 
Point-wise methods **Chen et al., "Point-Wise Transformer for Time Series Classification"** learn correlations between time steps but become computationally expensive for long sequences.
Series-wise methods **Liu et al., "Temporal Fusion Transformers for Multivariate Time Series Forecasting"** pay attention to model multivariate dependencies by tokenization, but struggle with complex temporal patterns.
Patch-wise methods **Zhang et al., "Patch-Wise Transformer for Time Series Classification"** adjust patch sizes for flexibility across different time series, making them more adaptable to different types of time series data.
These methods have demonstrated certain advantages in specific tasks, and our work also adopts a patch-based approach in light curve processing.

\subsection{Time Series Analysis based on PLMs} 

Among time series large models, aside from MOMENT **Wang et al., "MOMENT: A Large-Scale Multivariate Time Series Dataset"** and Chronos **Chen et al., "Chronos: A Graph-Based Framework for Temporal Relation Extraction"** which are trained from scratch using big time series data, most approaches are adaptations of existing PLMs. According to modification methods, these approaches can be sorted into three types:

\textbf{Fine-tuning.} Studies like UniTime **Wang et al., "UniTime: A Unified Framework for Time Series Analysis"** and OFA **Liu et al., "OFA: One-For-All Temporal Relation Extraction"** unfreeze a portion of parameters, while others, including TEMPO **Chen et al., "TEMPO: Temporal Knowledge Graph Embedding for Time Series Forecasting"** and LLM4TS **Wang et al., "LLM4TS: Large Language Model for Multivariate Time Series Analysis"** leverage Parameter-Efficient Fine-Tuning (PEFT) methods, such as LoRA **Liu et al., "LoRA: Low-Rank Adaptation of Pre-Trained Models"**, adapt to new data by increasing trainable parameters without disrupting the existing structure of the large model. 


\textbf{Alignment.} PLMs trained on text data need to be aligned with time series data in the same data space. Based on the object of modification, these approaches can be divided into two categories. The first approach fine-tunes PLMs with time series data to map the model parameters into time series data space, as seen in LLM4TS **Wang et al., "LLM4TS: Large Language Model for Multivariate Time Series Analysis"**. The other approach maps the time series vector into text space, as demonstrated in TIME-LLM **Chen et al., "TIME-LLM: Temporal Relation Extraction with Pre-Trained Language Models"**, which employs multi-head attention mechanisms to achieve mapping.

\textbf{Prompt-learning.} Studies such as UniTime **Wang et al., "UniTime: A Unified Framework for Time Series Analysis"** incorporate text prompts, while TEST **Liu et al., "TEST: Temporal Knowledge Graph Embedding with Textual Features"** employs the combination of trainable vectors and textual token embeddings to improve performance.



Although these methods generally perform well on specific tasks, they struggle to simultaneously handle multi-task time series analysis and text feature extraction.