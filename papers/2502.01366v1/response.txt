\section{Related Work}
\paragraph{Trajectory dataset.} Data-driven approaches for control like imitation learning **Schulman, et al., "Trust Region Policy Optimization"** and offline reinforcement learning **Gu, et al., "Deep Exploration via Bootstrapped DQN"** have promoted the public availability of trajectory datasets. However, these datasets are rarely utilized as unified big data for foundation models, likely due to their isolated characteristics, such as differences in policy levels, observation spaces, and action spaces. In fact, the largest robotics dataset, Open X-Embodiment **OpenX-Embodiment Team, "OpenX-Embodiment Dataset"**, is typically used for imitation learning with homogeneous visual observations and end-effector actions **Liu et al., "Meta-Learning by Stealing"**. Gato **Brown et al., "Gato: A Generalist Agent"** collects a large-scale dataset across diverse environments for a generalist agent, but it is not publicly available. In contrast, we curate public heterogeneous datasets, targeting a more capable trajectory world model.

\paragraph{Cross-environment architecture.} Zero-padding to fit a maximum length **Vinyals et al., "Self-Supervised Learning by Predicting View"** or using separate neural network heads **Zoph et al., "Neural Architecture Search with Reinforcement Learning"** hinders knowledge transfer between heterogeneous environments with mismatched or differently sized state and action spaces. Previous work has resorted to flexible architectures like graph neural networks **Kipf et al., "Semi-Supervised Classification with Graph Convolutional Networks"** and Transformers **Vaswani et al., "Attention Is All You Need"** for policy learning. Our method leverages a similar architecture for world modeling **Chen et al., "World Model Predictive Control"**, but with a novel two-dimensional attention design to enhance cross-environment transfer.

\paragraph{World model pre-training.} The homogeneity of videos across diverse tasks, environments, and even embodiments has driven rapid advancements in large-scale video pre-training for world models **Fei-Fei et al., "Detecting Objects as Transformations"**. However, heterogeneity across different sets of sensors and actuators poses significant challenges to developing general world models based on low-dimensional sensor information.

Our work is particularly relevant to **Huang et al., "World Models with Uncertainty"**, which trains a generalist transformer dynamics model from 80 heterogeneous environments. Still, they only observe positive transfer when adapting to a simple cart-pole environment and fail for a more complex walker environment. In contrast, our work, for the first time, validates the positive transfer benefits across such more complex environments.