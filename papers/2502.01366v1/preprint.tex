
\documentclass{article}

\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{float}
\usepackage{wrapfig}
\usepackage{booktabs} %

\usepackage{hyperref}


\newcommand{\theHalgorithm}{\arabic{algorithm}}

\usepackage[accepted]{icml2025}


\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{multirow} 
\usepackage{bm}

\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage[capitalize,noabbrev]{cleveref}

\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

\usepackage[textsize=tiny]{todonotes}


\icmltitlerunning{Trajectory World Models for Heterogeneous Environments}


\begin{document}


\twocolumn[
\icmltitle{Trajectory World Models for Heterogeneous Environments}



\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Shaofeng Yin}{equal,bnrist,zhili}
\icmlauthor{Jialong Wu}{equal,bnrist}
\icmlauthor{Siqiao Huang}{bnrist,iiis}
\icmlauthor{Xingjian Su}{bnrist}
\icmlauthor{Xu He}{huawei}
\icmlauthor{Jianye Hao}{tju,huawei}
\icmlauthor{Mingsheng Long}{bnrist}
\end{icmlauthorlist}

\icmlaffiliation{bnrist}{School of Software, BNRist, Tsinghua University.}
\icmlaffiliation{zhili}{Zhili College, Tsinghua University.}
\icmlaffiliation{iiis}{IIIS, Tsinghua University.}
\icmlaffiliation{huawei}{Huawei Noah's Ark Lab.}
\icmlaffiliation{tju}{College of Intelligence and Computing, Tianjin University.

Shaofeng Yin $<$ysf22@mails.tsinghua.edu.cn$>$. Jialong Wu $<$wujialong0229@gmail.com$>$}

\icmlcorrespondingauthor{Mingsheng Long}{mingsheng@tsinghua.edu.cn}

\icmlkeywords{World Models, Pre-training, Heterogeneous Environments}

\vskip 0.3in
]



\printAffiliationsAndNotice{\icmlEqualContribution} %
\begin{abstract}

Heterogeneity in sensors and actuators across environments poses a significant challenge to building large-scale pre-trained world models on top of this low-dimensional sensor information. In this work, we explore pre-training world models for heterogeneous environments by addressing key transfer barriers in both data diversity and model flexibility. We introduce UniTraj, a unified dataset comprising over one million trajectories from 80 environments, designed to scale data while preserving critical diversity. Additionally, we propose TrajWorld, a novel architecture capable of flexibly handling varying sensor and actuator information and capturing environment dynamics in-context. Pre-training TrajWorld on UniTraj demonstrates significant improvements in transition prediction and achieves a new state-of-the-art for off-policy evaluation. To the best of our knowledge, this work, for the first time, demonstrates the transfer benefits of world models across heterogeneous and complex control environments.
\end{abstract}

\section{Introduction}

World models \cite{ha2018recurrent, lecun2022path} have made remarkable progress in addressing sequential decision-making problems \cite{hafner2019dream, schrittwieser2020mastering, hansen2023td}. Trained on trajectory data, these models can simulate environments and are leveraged to either evaluate complex actions \cite{chua2018deep, ebert2018visual, tian2023control} or optimize policies \cite{janner2019trust, kurutach2018model}. However, existing methods often learn world models \textit{tabula rasa}, relying on data from a single, specific environment. This limits their ability to generalize to out-of-distribution transitions, demanding a substantial number of costly interactions with the environment.

In recent years, machine learning has been revolutionized by foundation models pre-trained on large-scale, diverse data \cite{achiam2023gpt, oquab2023dinov2, kirillov2023segment}. General world models have also been realized through pre-training, enabled by the \textit{homogeneity} present within massive and diverse datasets of specific modalities, such as text \cite{wang2024can, gu2024your}, images \cite{zhou2024dino}, and videos \cite{seo2022reinforcement, wu2024pre, wu2024ivideogpt, agarwal2025cosmos}. However, a unique challenge of world models from Internet AI is commonly overlooked or circumvented: the \textit{heterogeneity} inherent in sensor and actuator information, which means proprioceptive data, such as joint positions and velocities, as well as optional target positions, vary significantly across environments. Failing to properly address this heterogeneity can result in no transfer, or even negative transfer.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.95\linewidth]{img/pred_aggregate.pdf}
    \vspace{-12pt}
    \caption{Aggregated transition prediction error (MAE) across 75 train-test dataset pairs, comparing MLP Ensemble \cite{chua2018deep}, TDM \cite{schubert2023generalist}, and proposed TrajWorld, with and without pre-training on UniTraj dataset. Y-axis at log scale.}
    \label{fig:aggregate}
\end{figure}


\begin{figure*}[t]
    \centering
    \includegraphics[width=0.84\linewidth]{img/concept2.pdf}
    \vspace{-10pt}
    \caption{Illustration of pre-training a world model from heterogeneous environments, with each environment labeled by its state and action dimensions. A Trajectory World Model, designed for flexibility in handling divergent state and action definitions, demonstrates effective positive transfer across distinct, heterogeneous and complex control environments.
    }
    \label{fig:concept}
\end{figure*}

We argue that no modality in world models should be left behind, including essential sensor information represented as low-dimensional vectors. In this work, we take a first step to bridge this gap by exploring the potential of pre-training a world model to extract shared knowledge from trajectories across heterogeneous environments (illustrated in Figure~\ref{fig:concept}). To this end, it is essential to overcome the transfer barriers from both data and model architecture perspectives.

\paragraph{Scaling data.} To achieve strong generalization through pre-training, access to vast and diverse data is essential \cite{team2021open}. While scaling data is straightforward, the real challenge lies in scaling data while preserving diversity. Diversity in our work has two key aspects. First, it refers to the data sources, i.e., the environments from which the data is collected. Second, it concerns the data properties, specifically the distribution of the data itself. Even within the same environment, different policies at various levels can produce significantly different data distributions. To tackle these challenges, we curate the UniTraj dataset, including over one million trajectories collected from various distributions from 80 heterogeneous environments. By scaling data while maintaining these diversities, we ensure that the model focuses on the core knowledge shared across environments, thereby enabling successful transferability.

\vspace{-5pt}
\paragraph{Flexible architecture.} Previous approaches often address size variations in state and action spaces by applying zero-padding to match a maximum length \cite{yu2020meta, hansen2023td} or employing separate input and output heads for each environment \cite{wang2024scaling, d2024sharing}. However, zero-padding imposes a dimension limit and adds training overheads, while the separate head approach requires training new heads for new environments, hindering zero-shot transfer. A truly capable model for heterogeneous environments requires a more flexible architecture. To address this, we propose the Trajectory World Model (TrajWorld), a novel architecture that integrates interleaved variate and temporal attention mechanisms. It is enabled to naturally accommodate varying numbers of sensors and actuators through variate attention and, more importantly, to capture their relationships in-context through temporal attention. This in-context learning capability goes beyond learning specific environment dynamics and thus enhances the model's generalizability across environments.



By pre-training our flexible TrajWorld architecture on the diverse and massive UniTraj dataset, we demonstrate, for the first time, the transfer benefits of world models across heterogeneous and complex control environments. Fine-tuning TrajWorld on 15 datasets from three previously unseen environments \cite{fu2020d4rl} significantly reduces transition prediction errors for both in-distribution and out-of-distribution actions (as shown in Figure~\ref{fig:aggregate}). This improved predictive accuracy also translates to our state-of-the-art performance on off-policy evaluation (OPE) tasks \cite{fu2021benchmarks}, enabling the offline evaluation and selection of a set of complex policies for best performance. 

The main contributions can be summarized as follows:
\vspace{-5pt}
\begin{itemize}
    \setlength\itemsep{0em}
    \item We investigate an under-explored world model pre-training paradigm across heterogeneous environments.
    \item We curate UniTraj, a unified trajectory dataset, enabling large-scale pre-training of world models.
    \item We propose TrajWorld, a novel architecture to facilitate transfer between heterogeneous environments.
    \item For the first time, our experiments demonstrate positive world model transfer across diverse and complex environments, achieving significant improvements in both transition prediction and policy evaluation.
\end{itemize}









\begin{table*}[t]

    \caption{Statistics for six components of the UniTraj dataset. The checkmark (\checkmark) represents a dataset collected or curated by ourselves. 
    }
    \label{tab:unitraj}
    \vspace{5pt}
    \centering
    \small
    \begin{tabular}{llcccccl}
    \toprule
    & Dataset     & \#Env.     & \#Episodes   & \#Steps    & State dim.           & Action dim.          & Characteristic           \\ \midrule
     & ExORL \cite{yarats2022don}       &  4 &  541,336                    &  330,985,000                    &  $5\sim 78$                    &       $1\sim 12$               & Exploratory       \\
     & RL Unplugged \cite{gulcehre2020rl} &  7   &     5,841               &            5,841,000          &           $5\sim 67$           &            $1\sim 21$          & Experience replay \\
     & JAT  \cite{gallouedec2024jack}    &    5    &       50,000               &      26,238,954                &      $4\sim 23$                &      $1\sim 7$                & Expert            \\
     & DB-1  \cite{wen2022realization}   &   58     &             290         &       19,320               &        $5\sim 67$              &           $1\sim 21$           & Expert; Diversity         \\
     & TD-MPC2  \cite{hansen2023td}   &  30  &    672,000                 & 336,000,000 & $3\sim 24$ & $1\sim 6$ & Experience replay \\
    \checkmark & Modular RL \cite{huang2020one}  & 20  &        37,199             & 19,996,902 & $7\sim 23$ & $1\sim 6$ & Experience replay \\  \midrule
    \checkmark &  \textbf{UniTraj} (Ours)  & 80  &             1,306,666        & 719,081,176 & $3\sim 78$ & $1\sim 21$ & \textbf{Omnifarious} \\ \bottomrule
    \end{tabular}
\end{table*}


\section{Problem Formulation}

An environment is typically described by a Markov decision process (MDP) $\mathcal{M}=\{\mathcal{S}, \mathcal{A}, P, r, \mu\}$, specified by the state space $\mathcal{S}$ (of sensors), the action space $\mathcal{A}$ (of actuators), the transition function $P: \mathcal{S}\times \mathcal{A}\rightarrow \Delta(\mathcal{S})$, the reward function $r: \mathcal{S}\times \mathcal{A} \rightarrow \mathbb{R}$, and the initial state distribution $\mu \in \Delta(\mathcal{S})$.

Given an MDP, a trajectory of length $T$:
\begin{equation}
\tau=\left(s_0, a_0, r_1, s_1, \cdots, a_{T-2}, r_{T-1}, s_{T-1}\right),
\label{eq:traj}
\end{equation}
is recorded as interactions between the environment and an agent, according the following protocol: starting from an initial state $ s_0 \sim \mu $, at each discrete time step $t = 0, 1, \dots$, the agent performs an action $a_t \in \mathcal{A}$ according to its policy, receives an immediate reward $ r_{t+1} = r(s_t, a_t) $, and observes the next state after transition $ s_{t+1} \sim P(s_t, a_t) $.

A world model $p_\theta(s_{t+1}, r_{t+1} | s_t, a_t)$, or more generally $p_\theta(s_{t+1}, r_{t+1} | s_{1:t}, a_{1:t})$, learns its parameter $\theta$ from a dataset of recorded trajectories $\mathcal{D} = \{\tau_i\}$ to approximate the underlying transition probability and reward function, thus serving as an alternative of the environment.

\vspace{-5pt}
\paragraph{Our work.} While most literature learns a world model on target environment $\mathcal{M}^t$ from scratch, we investigate an under-explored paradigm of pre-training a world model from a family of heterogeneous\footnote{We use the term ``heterogeneous" to highlight that different environments not only feature varying transition and reward functions but also, more challengingly, possess distinct state and action spaces tied to unique sets of sensors and actuators.} environments $\{\mathcal{M}^1, \mathcal{M}^2, \dots, \mathcal{M}^K\}$. Through learning from mixed trajectory data $\{\mathcal{D}^1, \mathcal{D}^2, \dots, \mathcal{D}^K\}$, we obtain a good starting-point of the model $\theta_0$, ready for either zero-shot generalization to unseen $\mathcal{M}^t$ or fine-tuning to obtain a world model of $\mathcal{M}^t$ with strong generalization given limited data. We elaborate on the intuition behind this paradigm in Section~\ref{sec:intuition}.

\section{UniTraj Dataset}

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.95\linewidth]{img/architect.pdf}
    \caption{Architecture of Trajectory World Models. A trajectory is first flattened into scalars, organized into two dimensions by timesteps and variates (each variate corresponds to a single dimension in the state, action, and reward), and then discretized into categorical representations. A Transformer with interleaved temporal and variate attentions processes the inputs to predict the categorical distribution for the next timestep autoregressively. Layer normalizations and residual connections are omitted for simplicity.}
    \label{fig:architecture}
\end{figure*}


We introduce UniTraj, a large-scale unified trajectory dataset from heterogeneous environments, to support the pre-training of a trajectory world model. To ensure diversity, we merge five publicly available datasets with different characteristics. To further enhance diversity, we also by ourselves collect the training buffer of agents from a set of diverse morphologies \cite{huang2020one}. As a result, UniTraj occupies a total of 1.3M trajectories (or 719M steps) from 80 distinct environments, as summarized in Table~\ref{tab:unitraj}. A detailed list of dataset information can be found in Appendix~\ref{app:dataset}.

Beyond its unprecedented scale, the collected UniTraj represents diversity in several aspects:


\vspace{-5pt}
\paragraph{Environment diversity.} 

UniTraj encompasses a wide range of control environments. These include not only widely-used environments from the DeepMind Control Suite (DMC) \cite{tassa2018DeepMind} and OpenAI Gym \cite{brockman2016openai}, but also customized embodiments and tasks proposed in Modular RL and TD-MPC2. Notably, we purposely exclude all trajectories from the HalfCheetah, Hopper, and Walker2D environment of OpenAI Gym, which are held out as our downstream test environments.

\vspace{-5pt}
\paragraph{Distribution diversity.} 
The dataset contains data collected from various distributions, resulting from different collection methods and policies. Specifically, data from RL Unplugged, TD-MPC2, and Modular RL are gathered by recording the training agent's replay buffer, while JAT and DB-1 data are collected through expert policies rollouts. Additionally, ExORL data are collected by storing the transitions from running unsupervised exploration algorithms \cite{laskin2021urlb}. The policies cover a range of approaches, including a wide range of reinforcement learning algorithms (e.g., D4PG \cite{barthmaron2018distributed}, PPO \cite{schulman2017proximal}) and state-of-the-art model predictive control algorithms, TD-MPC2.

By scaling up the dataset while preserving diversity, we empower the model with the potential to generalize across varied environments. 

\section{Trajectory World Models}

In this section, we first explain the intuition behind the proposed Trajectory World Models (TrajWorld) (Section~\ref{sec:intuition}), then provide a detailed overview of the architecture implementation (Section~\ref{sec:architecture}), and conclude with a discussion of the pre-training and fine-tuning paradigm (Section~\ref{sec:general}).

\subsection{Intuition}
\label{sec:intuition}


To address the challenges of heterogeneity and promote knowledge transfer, we make three key observations: 



\vspace{-5pt}
\paragraph{Rediscovering homogeneity in scalars.} While heterogeneity often arises in differently sized vector information, there exists an inherent homogeneity at the scalar level. Each variate---a single scalar dimension in the state, action, or reward---represents a fundamental quantity with its own physical meanings of the environment, e.g., position or torque of a single joint, and can be consistently modeled, regardless of the shape of the whole vector information. This insight leads to our design choice: \textit{Instead of treating vector information as a whole, we break it into the scalar level for processing and prediction.}


\vspace{-5pt}
\paragraph{Identifying environment through historical context.}
Unlike single-environment scenarios with fixed state and action definitions, in our setting, variants can represent different quantities across environments despite the same index in the vector. While environment IDs are typically included as inputs to distinguish environments, we instead leverage the in-context learning ability of Transformers \cite{brown2020language}: history transitions can provide the context needed for the model to infer relationships between variants. This makes pre-training even more critical. By exposing the model to diverse data across environments, we encourage it to learn ``how to learn environment dynamics,"---a more generalizable knowledge---rather than solely focusing on specific environments. This ability is demonstrated in Section~\ref{sec:zero-shot}, where our pre-trained model has satisfactory zero-shot performance. In summary, \textit{we provide historical context instead of environment identities, guiding the model to learn to infer dynamics through context}.


\vspace{-5pt}
\paragraph{Inductive bias for two-dimensional representations.} So far, our modeling for heterogeneous dynamics involves two dimensions: one focuses on capturing the relationships among variants, and the other models how actions drive transitions from the current state to the next. Instead of using simple one-dimensional attention over flattened sequences, explicitly modeling these two dimensions has the potential to enhance transferability in downstream tasks, as it guides the model to learn in a more structured and systematic manner. This is supported by empirical results in Section~\ref{sec:exp_pred}. In short, \textit{we use a two-way attention mechanism instead of one-dimensional attention on sequences.}









\subsection{Architecture}
\label{sec:architecture}

Building on the above intuitions, we realize a Transformer-based architecture for TrajWorld (see Figure~\ref{fig:architecture}).

\paragraph{Scalarization.} To exploit the inherent homogeneity at the scalar level, we flatten a trajectory $\tau$ (Equation~\eqref{eq:traj}) from the spaces $\mathcal{S} \subset\mathbb{R}^{m}, \mathcal{A} \subset \mathbb{R}^{n}$ into a two-dimensional representation organized by timesteps and variates:
\begin{equation}
    X=\begin{pmatrix}
        s_0^{(1)}&\cdots &s_0^{(m)} & r_0 & a_0^{(1)}&\cdots &a_0^{(n)} \\
        \vdots&\ddots& \vdots & \vdots & \vdots& \ddots &\vdots \\
        s_{T-1}^{(1)}&\cdots &s_{T-1}^{(m)} & r_{T-1} & a_{T-1}^{(1)}&\cdots &a_{T-1}^{(n)}  
    \end{pmatrix},
\end{equation}
where $s_t^{(i)}$ denotes the $i$-th dimension of $s_t$. Padding is applied to $r_0$ and $a_{T-1}$ as zeros. This transformation converts heterogeneous trajectories of varying lengths and dimensions into matrices $X \in \mathbb{R}^{T \times M}$, where $M = m + n + 1$, which can be flexibly processed by the attention mechanism.



\paragraph{Discretization and embeddings.} Transformers excel in processing discrete inputs, so we further convert scalars into categorical representations. For each variate $s^{(i)}$ or $a^{(i)}$, we define $B$ uniform bins with boundaries $b_0 < b_1 < \dots < b_{B}$, where $b_0$ and $b_B$ represent the minimum and maximum values of the variate in the training data. Scalars are then mapped to these bins using one-hot encoding or Gaussian histograms \cite{imani2018improving, farebrother2024stop}. 

The resulting discrete representation $Q \in [0, 1]^{T\times M \times B}$ is linearly projected to match the Transformer's hidden size $d$. Additionally, we apply three learned embeddings---\textit{timestep-embedding} (TE), \textit{variate embedding} (VE), and \textit{prediction embedding} (PE)---to capture timestep indices, variate identities, and whether a variate is a target for prediction. Formally, for each $i\in [T]$ and $j \in [M]$:
\begin{equation} 
    Z_{ij}^0 = W_{\text{in}} Q_{ij} + \text{TE}(i) + \text{VE}(j) + \text{PE}(\mathbf{1}[j \leq m+1]).
\end{equation}




\paragraph{Interleaved temporal-variate attentions.} The input $ Z^0 \in \mathbb{R}^{T\times M \times d} $ is processed through a series of $L$ transformer blocks, adapted for the two-dimensional input structure. In each block $l=1,\dots, L$, we first apply \textit{temporal attention}, processing each variate independently:
\begin{align}
    U_{1:T, j}^{l}=\text{CausalAttention}(Z_{1:T, j}^{l-1}), \quad \forall j \in [M],
\end{align}
followed by a feedforward network (FFN): $\hat{U}^{l} = \text{FFN}(U^l)$.
Afterwards, \textit{variate attention} is applied at each timestep:
\begin{align}
    V_{i, 1:M}^{l}=\text{Attention}(\hat{U}_{i, 1:M}^{l}), \quad \forall i \in [T].
\end{align}
Since there are no causal dependencies between variates at the same timestep, no causal mask is applied during variate attention. Finally, another FFN is applied: $Z^{l} = \text{FFN}(V^l)$.

Through interleaved temporal and variate attentions, each entry in our model efficiently aggregates information from all variates across all previous timesteps. As previously discussed, this enables the model to infer environment dynamics in-context for transition prediction.










\paragraph{Prediction and objective.} A linear prediction head, followed by a softmax operation, produces the prediction distribution $P = \text{Softmax}(W_\text{out}Z^{L})\in [0,1]^{T\times M \times B}$. Our model is trained using a next-step prediction objective to match the categorical representation of the inputs:
\begin{equation}
    \mathcal{L}(P,Q)=-\sum_{i=1}^{T-1}\sum_{j=1}^{m+1}\sum_{k=1}^B Q_{i+1, j, k} \log P_{i, j, k}.
\end{equation}
During inference, the next-step prediction can be obtained by sampling from or taking the expectation of the predicted categorical distribution over bin centers.

\subsection{Towards a General Trajectory World Model}
\label{sec:general}


We pre-train a general Trajectory World Model on offline datasets from diverse environments. This same pre-trained model can then be applied to all downstream tasks for fine-tuning. Thanks to the Transformer's flexible architecture design and in-context learning capabilities, the pre-trained knowledge becomes more transferable, benefiting a wide range of heterogeneous and complex control environments.













\section{Experiments}




In this section, we test the following hypotheses:

\begin{itemize}
\vspace{-5pt}
    \item Large-scale trajectory pre-training can generalize effectively and even enable zero-shot generalization, contrary to the common belief. (Section~\ref{sec:zero-shot})
    \item TrajWorld outperforms alternative architectures for transition prediction when transferring dynamics knowledge to new environments. (Section~\ref{sec:exp_pred})
\end{itemize}

\begin{figure}[H]
    \centering
    \begin{subfigure}{0.48\textwidth}
        \centering
        \small
        \begin{tabular}{lcc}
        \toprule
        \multirow{2}{*}{Method} & \multicolumn{2}{c}{Prediction Error (MSE)}  \\ \cmidrule{2-3} 
                                & Pendulum          & Walker2D             \\ \midrule
        Last-step Mirroring         & $1.3 \times 10^{-3}$ & $1.7 \times 10^{-2}$ \\
        TrajWorld (w/o history) & $1.7 \times 10^{-5}$ & $2.1 \times 10^{-3}$ \\
        TrajWorld (w/ history)  & $2.9 \times 10^{-6}$ & $7.2 \times 10^{-4}$ \\ \bottomrule
        \end{tabular}
        \caption{Environment parameters transfer.}
        \label{fig:zero_shot_gravity}
    \end{subfigure}
    \begin{subfigure}{0.45\textwidth}
        \centering
        {\vspace{5pt}
        \includegraphics[width=1.0\linewidth]{img/cart2pole.pdf}
        \vspace{-17pt}}
        \caption{Cross-environment transfer. }
        \label{fig:zero_shot_two_pole}
    \end{subfigure}
    \vspace{-8pt}
    \caption{Zero-shot generalization. (a) Mean squared error of zero-shot transition predictions in modified Gym Pendulum (holdout gravity) and Walker2D (holdout friction etc.). (b) TrajWorld's zero-shot predictions for two Cart-2-Pole trajectories, which share 10 context steps but diverge due to differing subsequent actions. }
    \vspace{5pt}
\end{figure}

\begin{itemize}
    \item TrajWorld leverages the general dynamics knowledge acquired from pre-training to improve performance in downstream tasks. (Section~\ref{sec:exp_ope})
\end{itemize}
\vspace{-5pt}

\begin{figure*}[t]
    \centering
    \includegraphics[width=\linewidth]{img/pred.pdf}
    \vspace{-15pt}
    \caption{Mean absolute errors (MAE) of transition prediction for TrajWorld, with and without pre-training (PT), across different train-test dataset pairs. Each subplot corresponds to a distinct training dataset, with the test datasets shown on the x-axis (r=random, m-r=medium-replay, m=medium, m-e=medium-expert, e=expert). Error bars represent the standard deviation across three random seeds.}
    \label{fig:prediction}
\end{figure*}


\subsection{Zero-shot Generalization}
\label{sec:zero-shot}




We first demonstrate that through in-context learning ability, TrajWorld exhibits favorable generalization across heterogeneous environments, which differ not only in their transition dynamics but also in state and action spaces.

\paragraph{Environment parameter transfer.} We pre-train a TrajWorld model on data from Gym Pendulum environments with varying gravity values and evaluate its transition prediction error on holdout gravity values. As shown in Table \ref{fig:zero_shot_gravity}, TrajWorld achieves significantly lower prediction error in zero-shot settings compared to a naive baseline that simply mimics the last timestep. Moreover, the performance of TrajWorld deteriorates noticeably when historical information is excluded, highlighting the critical role of contexts for the model to effectively infer environment parameters. The results are consistent in a similar experiment conducted on Gym Walker2D, where friction, mass, etc., are varied.

\paragraph{Cross-environment transfer.} We further find that TrajWorld, when trained on the large-scale UniTraj dataset, is also capable of zero-shot generalizing to unseen environments, Cart-2-Pole and Cart-3-Pole from DMC (Figure \ref{fig:zero_shot_two_pole} and \ref{fig:cart-3-pole}). Specifically, TrajWorld successfully infers the influence of the action value (pushing force) on the state dimension (cart position) and accurately predicts the outcomes for different action sequences performed subsequently.



\subsection{Transition Prediction}
\label{sec:exp_pred}

We then evaluate how different world models benefit from pre-training for transition prediction, particularly for out-of-distribution queries, when fine-tuned to more complex, standard environments.

\vspace{-5pt}
\paragraph{Setup.} We use datasets of three environments---HalfCheetah, Hopper, and Walker2D---from D4RL \cite{fu2020d4rl} as our testbed. Each environment in D4RL is provided with five datasets of different distributions from policies of varying performance levels. We train world models in each of the fifteen datasets and test prediction errors of states and rewards across all five datasets under the same environment, resulting in 75 train-test dataset pairs. 



\vspace{-5pt}
\paragraph{Baselines.}




We compare our approach against two baselines: an ensemble of MLPs \cite{chua2018deep}, widely adopted for dynamics modeling, and TDM \cite{schubert2023generalist}, which is similar to our model but uses one-dimensional attention. Each baseline is evaluated both for training from scratch and fine-tuning pre-trained ones on the same UniTraj dataset as TrajWorld. To enable pre-training, we pad the state and action vectors with zeros to match the same dimensionality for MLP. Additionally, we include our model trained from scratch for comparison.

\vspace{-5pt}
\paragraph{Results.}

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.9\linewidth]{img/ope_results.pdf}
    \caption{Overall off-policy evaluation (OPE) results across 15 datasets of 3 environments, averaged across three random seeds.}
    \label{fig:ope}
\end{figure*}

Figure \ref{fig:aggregate} presents the aggregated mean absolute error of 75 train-test dataset pairs for various models. TrajWorld outperforms all baselines, highlighting the effectiveness of its pre-training strategy and architecture design. Notably, MLP Ensemble with pre-training performs worse than its non-pre-trained counterpart, emphasizing the importance of careful model design for world modeling across heterogeneous environments. While TDM also benefits significantly from pre-training, it still lags behind TrajWorld. This is likely because TDM naively treats everything as a 1D sequence, neglecting the unique 
problem structures. In contrast, TrajWorld explicitly models variate relationships and temporal transitions, leveraging different facets of dynamics knowledge from the pre-training. Moreover, TDM predicts variants sequentially, which may accumulate errors and lead to less accurate results.

In Figure \ref{fig:prediction}, we further show detailed prediction error results for TrajWorld compared to its non-pre-trained counterparts. In 12 out of 15 training datasets, fine-tuned TrajWorld achieves a lower average prediction error across 5 test datasets, further validating the effectiveness of pre-training. 
Moreover, the transfer benefits are evident in both in-distribution and out-of-distribution scenarios.


\subsection{Off-Policy Evaluation}
\label{sec:exp_ope}

Off-policy evaluation (OPE) estimates the value of a target policy using an offline transition dataset collected by a separate behavior policy. It is commonly used to select the most performant policy from a set of candidates when online evaluation is too costly to be practical. This task provides an ideal evaluation scenario for world models,  as value estimation can be acquired by rolling out the target policy within the learned world model. 


\paragraph{Setup.}  We adopt the DOPE benchmark \cite{fu2021benchmarks} over various D4RL environments. The tasks in this benchmark are particularly challenging, as the target policies are of different levels and may differ significantly from the behavior policy. To perform well on these tasks, the world model must generalize well across all possible state-action distributions. Evaluation metrics include \textit{mean absolute error} comparing estimated vs. ground-truth policy values, \textit{rank correlation} between estimated and actual policy rankings, and \textit{Regret@1} measuring accuracy in selecting the best policy, as detailed in Appendix \ref{app:ope_metric}.

\paragraph{Baselines.} In addition to the MLP Ensemble and TDM models mentioned earlier, we compare our approach against several other baselines. Notably, Energy-based Transition Models (ETM) \cite{chen2024offline} currently sets the state-of-the-art on this benchmark, outperforming prior methods by a significant margin. We also include the classical methods from the original DOPE paper \cite{fu2021benchmarks} for a more comprehensive comparison.



\paragraph{Results.} Figure \ref{fig:ope} shows that TrajWorld significantly improves OPE compared to its non-pre-trained variant and outperforms all baselines in both average normalized absolute error and rank correlation. TrajWorld slightly underperforms on Regret@1, likely due to bounded reward prediction (see discussion in Appendix~\ref{app:discussion}). Consistent with Section \ref{sec:exp_pred}, MLP Ensemble with pre-training suffers from negative transfer, showing a notable drop in performance compared to the non-pre-trained model. Although TDM also benefits from pre-training, it does not reach the same level of performance as TrajWorld. We attribute this to the same reason discussed in Section~\ref{sec:exp_pred}.


\subsection{Analysis}

\paragraph{Few-shot adaptation.} TrajWorld presents pre-training benefits in few-shot scenarios. In Figure \ref{fig:fewshot}, we show the prediction error across varying levels of data scarcity and compare TrajWorld with and without pre-training. These results highlight that the advantages of pre-training become increasingly pronounced as data becomes more limited.

\paragraph{Effects of pre-training dataset.} To further investigate which aspects contribute to the pre-training benefits, we train TrajWorld on a modified UniTraj dataset with the Modular RL and TD-MPC2 data removed. Despite this modification, TrajWorld still demonstrates significant pre-training advantages (Appendix~\ref{app:dataset_ablation}). This highlights that the benefits of pre-training are derived from the diversity of the entire UniTraj dataset, rather than relying exclusively on data from domains that are relatively similar to target environments.




\paragraph{Discretization visualization.} We use t-SNE \cite{JMLR:v9:vandermaaten08a} to visualize the linear weights of our model's prediction head for each category. The mapped weights exhibit strong continuity in Figure \ref{fig:tsne}. Since the output categories' indices are aligned with the bins in increasing order, this indicates that our model has learned the ordering of bins shared by variants, despite being trained via an unordered classification objective. This suggests the model's potential for fine interpolation between existing bins and extrapolation to unseen ranges of variant values.

\begin{figure*}[t]
    \centering
     \begin{subfigure}[b]{0.266\textwidth}
        \centering
        \includegraphics[width=1.0\linewidth]{img/fewshot.pdf}
        \vspace{-20pt}
        \caption{Few-shot adaptation.}
        \label{fig:fewshot}
    \end{subfigure}
     \begin{subfigure}[b]{0.266\textwidth}
        \centering
        \includegraphics[width=1.0\linewidth]{img/tsne.pdf}
        \vspace{-20pt}
        \caption{Discretization visualization.}
        \label{fig:tsne}
    \end{subfigure}
    \begin{subfigure}[b]{0.4085\textwidth}
        \centering
        \includegraphics[width=1.0\linewidth]{img/attention.pdf}
        \vspace{-20pt}
        \caption{Variate attention visualization.}
        \label{fig:attention_map_walker}
    \end{subfigure}
    {
    \vspace{-8pt}
    \caption{Model analysis. (a) Downstream prediction error of TrajWorld under varying data scarcity levels. (b) t-SNE visualization of the linear weights in the modelâ€™s prediction head. (c) Variate attention map from the third layer of TrajWorld fine-tuned on Walker2D.} 
    }
\end{figure*}

\paragraph{Variate attention visualization.}
\label{analysis_attention}
We visualize the variate attention maps of our fine-tuned model in the Walker2D environment, whose states are ordered with joint positions first, followed by their velocities. As shown in Figure \ref{fig:attention_map_walker}, the attention map exhibits prominent diagonal patterns that focus on the corresponding joint's position and velocity, suggesting the model's understanding of each variate's semantics. Additionally, the strong attention between neighboring variate, such as physically linked joints, further confirms the model's grasp of joint relationships. We also observe notable attention patterns between states and actions, and these additional results are available in Appendix \ref{app:attention_add}.




\section{Related Work}

\paragraph{Trajectory dataset.} Data-driven approaches for control like imitation learning \cite{florence2022implicit, shafiullah2022behavior, gallouedec2024jack} and offline reinforcement learning \cite{fu2020d4rl, rafailov2024d5rl, gulcehre2020rl, qin2022neorl} have promoted the public availability of trajectory datasets. However, these datasets are rarely utilized as unified big data for foundation models, likely due to their isolated characteristics, such as differences in policy levels, observation spaces, and action spaces. In fact, the largest robotics dataset, Open X-Embodiment \cite{o2024open}, is typically used for imitation learning with homogeneous visual observations and end-effector actions \cite{team2024octo, kim2024openvla}. Gato \cite{reed2022generalist} collects a large-scale dataset across diverse environments for a generalist agent, but it is not publicly available. In contrast, we curate public heterogeneous datasets, targeting a more capable trajectory world model.

\paragraph{Cross-environment architecture.} Zero-padding to fit a maximum length \cite{yu2020meta, hansen2023td, schmied2024learning, seo2022reinforcement} or using separate neural network heads \cite{wang2024scaling, d2024sharing} hinders knowledge transfer between heterogeneous environments with mismatched or differently sized state and action spaces. Previous work has resorted to flexible architectures like graph neural networks \cite{huang2020one, kurin2020my} and Transformers \cite{gupta2022metamorph, hong2021structure} for policy learning. Our method leverages a similar architecture for world modeling \cite{janner2021offline, zhang2021autoregressive}, but with a novel two-dimensional attention design to enhance cross-environment transfer.

\paragraph{World model pre-training.} The homogeneity of videos across diverse tasks, environments, and even embodiments has driven rapid advancements in large-scale video pre-training for world models \cite{seo2022reinforcement, wu2024pre, wu2024ivideogpt, ye2024latent, cheang2024gr}. However, heterogeneity across different sets of sensors and actuators poses significant challenges to developing general world models based on low-dimensional sensor information.

Our work is particularly relevant to \citet{schubert2023generalist}, which trains a generalist transformer dynamics model from 80 heterogeneous environments. Still, they only observe positive transfer when adapting to a simple cart-pole environment and fail for a more complex walker environment. In contrast, our work, for the first time, validates the positive transfer benefits across such more complex environments.







\section{Conclusion}

We address the challenge of building large-scale pre-trained world models for heterogeneous environments with distinct sensors, actuators, and dynamics. Our contributions include UniTraj, a dataset of over one million trajectories from 80 environments, and TrajWorld, a flexible architecture for cross-environment transfer. Pre-training TrajWorld on UniTraj achieves superior results in transition prediction and off-policy evaluation, demonstrating the first successful transfer of world models across complex control environments. 

\paragraph{Limitations and future work.} While this work takes a successful first step, there is significant room for further study. Despite the strong practical performance, one limitation of our architecture is that the discretization scheme constrains predictions to a fixed range, making it theoretically difficult to model extremely out-of-distribution transitions beyond these bounds. Additionally, our model, designed for scalable pre-training, has a larger capacity compared to classic MLPs, which poses challenges in model calibration \cite{guo2017calibration}, particularly in scenarios where uncertainty quantification is critical, such as offline RL \cite{yu2020mopo}. This increased complexity also comes with additional computational costs. For future work, we envision that pre-training multimodal world models incorporating both visual and proprioceptive observations could lead to models with a deeper understanding of the physical world.



















































































\newpage
\section*{Impact Statement}


This paper presents work whose goal is to advance the field of 
Machine Learning. There are many potential societal consequences 
of our work, none which we feel must be specifically highlighted here.




\bibliography{example_paper}
\bibliographystyle{icml2025}


\newpage
\appendix
\onecolumn

\section{UniTraj Dataset Details}
\label{app:dataset}

\subsection{Overview of UniTraj Components}
In this part, we provide a brief overview of each component of the UniTraj dataset. 

\paragraph{ExORL \cite{yarats2022don}.} Exploratory Data for Offline RL (ExORL) follows a two-step data collection protocol. First, data is generated in reward-free environments using unsupervised exploration strategies \cite{laskin2021urlb}. Next, this data is relabeled with either a standard or hand-designed reward function specific to each environment's task. This procedure leads to data with broader state-action space coverage, which benefits generalization-demanding scenarios like offline RL.

\paragraph{RL Unplugged \cite{gulcehre2020rl}.} We incorporate RL Unplugged's dataset from the DeepMind Control Suite domains. Most of the data collected in this domain are generated by recording D4PG's training runs \cite{barthmaron2018distributed}, while Manipulator insert ball and Manipulator insert peg's data is collected using V-MPO \cite{song2019vmpo}.

\paragraph{JAT \cite{gallouedec2024jack}.} We utilize Jack of All Trades (JAT)'s released dataset, which is collected using expert RL agent's rollouts. These agents are trained using asynchronous PPO \cite{schulman2017proximal}, following the Sample Factory implementation \cite{petrenko2020sample}. Specifically, we only use the subset of the dataset that was collected in the OpenAI Gym environments, excluding data collected in Walker2D, HalfCheetah, and Hopper.

\paragraph{DB-1 \cite{wen2022realization}.} The dataset for Digital Brain-1 (DB-1), a reproduction of Gato \cite{reed2022generalist}, also consists solely of expert policy rollouts. Although the released dataset contains only five expert episodes per domain, it spans multiple environments, including various DeepMind Control Suite environments and custom ones from Modular RL.

\paragraph{TD-MPC2 \cite{hansen2023td}.} TD-MPC2 is a state-of-the-art model-based RL algorithm. We include released data from single-task TD-MPC2 agents' replay buffers, collected from DeepMind Control Suite environments.

\paragraph{Modular RL \cite{huang2020one}.} The Modular RL environments introduced by \citet{huang2020one} feature customizable embodiments with varying limb and joint configurations. We collected the data on these environments by ourselves. Specifically, we used the provided XML files to define different embodiment structures and followed the original reward function designs. We ran the TD3 algorithm \cite{fujimoto2018addressing} and stored all episodes until the policy began to converge. The hyperparameters for TD3 are kept consistent with the default settings provided in the official repository repository\footnote{\url{https://github.com/sfujim/TD3}}.

\subsection{List of Environments}

The curated UniTraj dataset spans a diverse range of environments from multiple sources, including DeepMind Control Suite, OpenAI Gym, and various customized environments. In Table \ref{tab:untraj_env}, we provide a detailed list of environments used in each component of UniTraj.

\begin{table}[tb]
    \centering
    \begin{tabular}{cl}
    \toprule
    \multicolumn{1}{c}{\textbf{Component}} & \multicolumn{1}{c}{\textbf{Environments}} \\
    \midrule
       ExORL  & Cartpole, Jaco, Quadruped, Walker$^\dagger$\\
    \hline
       RL Unplugged & Cartpole, Fish, Humanoid, Manipulator, Walker$^\dagger$\\ 
    \hline
       JAT & Double Pendulum$^*$, Pendulum$^*$, Pusher$^*$, Reacher$^*$, Swimmer$^*$ \\
    \hline
       \multirow{12}{*}{DB-1} & Acrobat, Ball In Cup, Cartpole, Cheetah-2-back, 
Cheetah-2-front, \\
& Cheetah-3-back, 
Cheetah-3-balanced, 
Cheetah-3-front, \\
& Cheetah-4-allback, 
Cheetah-4-allfront, 
Cheetah-4-back, 
Cheetah-4-front, \\
& Cheetah-5-back, 
Cheetah-5-balanced, 
Cheetah-5-front, 
Cheetah-6-back, \\
& Cheetah-6-front, Finger, Fish, Hopper$^\dagger$, 
Hopper-3, 
Hopper-5, 
Humanoid, \\
& Humanoid-2d-7-left-arm, 
Humanoid-2d-7-left-leg, 
Humanoid-2d-7-lower-arms, \\
& Humanoid-2d-7-right-arm, 
Humanoid-2d-7-right-leg, 
Humanoid-2d-8-left-knee, \\
& Humanoid-2d-8-right-knee, 
Humanoid-2d-9-full, \\
& Manipulator, Reacher, Swimmer6, Swimmer15, Walker$^\dagger$, \\
& Walker-2-flipped, 
Walker-2-main, 
Walker-3-flipped, 
Walker-3-main, \\
& Walker-4-flipped, 
Walker-4-main, 
Walker-5-flipped, \\
& Walker-5-main, 
Walker-6-flipped, 
Walker-6-main
\\
\hline
       \multirow{2}{*}{TD-MPC2} & Acrobot, Ball In Cup, Cartpole, Cheetah$^\dagger$, Finger, Fish, Hopper$^\dagger$,  \\
       & Pendulum$^\dagger$, Reacher$^\dagger$, Walker$^\dagger$ \\
\hline
       \multirow{5}{*}{Modular RL} & Cheetah-2-back, 
Cheetah-2-front, 
Cheetah-3-back, 
Cheetah-3-balanced, \\
& Cheetah-4-allback, 
Cheetah-4-back, 
Cheetah-4-front, 
Cheetah-5-back, \\ 
& Cheetah-5-balanced, 
Cheetah-5-front, 
Cheetah-6-back, 
Cheetah-6-front, \\
& Hopper-3,
Hopper-5, 
Walker-2-flipped, 
Walker-3-flipped, \\
& Walker-4-flipped, 
Walker-5-flipped, 
Walker-6-flipped, 
Walker-7-flipped \\
    \bottomrule
    \end{tabular}
    \caption{A detailed list of environments used in the UniTraj dataset. For environments sharing the same name, we mark those from OpenAI Gym with an asterisk ($*$) and those from DeepMind Control Suite with a dagger ($\dagger$). Notably, the Gym Hopper, Walker2D, and HalfCheetah environments used for evaluating our methods and baselines differ from their DeepMind Control Suite counterparts, exhibiting variations in state/action definitions and environment parameters.}
    \label{tab:untraj_env}
\end{table}


\section{Experimental Details}
\label{app:implementation}

\subsection{Model Implementation}

\paragraph{TrajWorld.}
For discretization, as described in Section~\ref{sec:architecture}, we can employ two methods: one-hot encoding and Gaussian histograms. Specifically, the Gaussian histogram method is utilized for input discretization, while the one-hot encoding is applied for target discretization. Compared to one-hot encoding, Gaussian histograms provide a more fine-grained representation of value information. While we can also use Gaussian histograms for target discretization, one-hot encoding is more suitable for uncertainty quantization in future applications such as offline RL. This is because two Gaussian distributions with the same standard derivation can yield different entropy when discretized into histograms.

For prediction, each bin $[b_{i-1}, b_{i}]$ is represented by its center $c_i = (b_{i-1} + b_{i}) / 2$. Given the predicted bin probability $p_i$, the output value distribution can be expressed as $P(X=x) = \sum_{i=1}^B p_i \mathbf{1}(x=c_i)$ or $P(X=x) = \sum_{i=1}^B p_i \mathbf{1}(b_{i-1} < x \leq b_i) / (b_i-b_{i-1})$. We use the former for simplicity.

When pre-training with data from heterogeneous environments, for practical reasons, each batch is made up of data from a single environment.

We provide the hyperparameters used in pre-training and fine-tuning in Table \ref{tab:TrajWorld_hp}. On transition prediction and OPE experiments, the environment-specific models trained from scratch use the same set of hyperparameters as fine-tuning.



\begin{table}[tb]
    \centering
    \begin{tabular}{ccc}
    \toprule
         & \textbf{Hyperparameter} & \textbf{Value} \\
    \midrule
        \multirow{8}{*}{Architecture} 
        & Input discretization & Gauss-hist \\
        & Target discretization & One-hot \\
        & Transformer blocks number & 6 \\
        & Attention heads number & 4 \\
        & Transformer context length & 20 \\
        & Hidden dimension & 256 \\
        & MLP hidden & [1024,256] \\
        & MLP activation & GeLU \\
        \midrule
        \multirow{8}{*}{Pre-training} & Total gradient steps & 1M \\
        & Batch size & 64 \\
        & Learning rate & $1 \times 10^{-4}$ \\
        & Dropout rate & 0.05 \\
        & Optimizer & Adam \\
        & Weight decay & $1 \times 10^{-5}$ \\
        & Gradient clip norm & 0.25 \\
        & Scheduler & Warmup cosine decay \\
        & Scheduler warmup steps & 10000 \\
        \midrule
        \multirow{11}{*}{Fine-tuning} & Total max gradient steps & 1.5M \\
        & Max epochs & 300 \\
        & Steps per epoch & 5000 \\
        & Batch size & 64 \\
        & Learning rate & $1 \times 10^{-5}$ \\
        & Dropout rate & 0.05 \\
        & Optimizer & Adam \\
        & Weight decay & $1 \times 10^{-5}$ \\
        & Gradient clip norm & 0.25 \\
        & Scheduler & Warmup cosine decay \\
        & Scheduler warmup steps & 10000 \\
    \bottomrule
    \end{tabular}
    \caption{Hyperparameters for TrajWorld.}
    \label{tab:TrajWorld_hp}
\end{table}

\paragraph{Baseline: Transformer Dynamics Model (TDM).}

TDM \cite{schubert2023generalist} does not provide an official implementation. To enable a fair comparison, we adapt our TrajWorld implementation to reproduce TDM while maintaining consistency in discretization and embedding methods. Furthermore, when trained using a cross-entropy loss, we mask actions and require the model to only predict the next states and rewards---unlike the TDM paper, where all variates are predicted. During inference, the model predicts each scalar dimension of the state sequentially, followed by setting each scalar of the action (e.g., provided by the policy in off-policy evaluation) one at a time. The hyperparameters for pre-training and fine-tuning are kept consistent with those used in TrajWorld (Table \ref{tab:TrajWorld_hp}), except for the batch size for pre-training. Due to GPU memory constraints, the batch size for pre-training, originally set to 64, is reduced to 16. Like TrajWorld, we use the same hyperparameters as fine-tuning for environment-specific models trained from scratch.

\paragraph{Baseline: MLP Ensemble.}

Following prior work \cite{chua2018deep, janner2019trust, yu2020mopo}, we train an ensemble of transition models, parameterized as a diagonal Gaussian distribution of the next state and reward, implemented using MLPs. These models are trained with bootstrapped training samples, and optimized via negative log-likelihood. After training, we select an elite subset of models based on validation loss, and during inference, a model from this subset is randomly sampled for predictions. For pre-training on heterogeneous environments, we implement the MLP Ensemble baseline by padding each state vector to 90 dimensions and each action vector to 30 dimensions, resulting in a 120-dimensional input to the MLP. The model outputs the distribution over a 91-dimensional vector (90 for the next state and 1 for the reward). To ensure a fair comparison with other methods, we match the parameter count of the ensemble to TrajWorld, and no environment identities are provided to this baseline. The hyperparameters are listed in Table \ref{tab:hyperparam_mlp}. Environment-specific models trained from scratch use the same hyperparameters as in fine-tuning.

\begin{table}[ht]
    \centering
    \begin{tabular}{ccc}
    \toprule
        & \textbf{Hyperparameter} & \textbf{Value} \\
    \midrule
        \multirow{3}{*}{Architecture}
        & MLP hidden & [640, 640, 640, 640] \\
        & Ensemble number & 7\\
        & Ensemble elite Number & 5\\
    \midrule
        \multirow{4}{*}{Pre-training} & Total gradient steps & 1M \\
        & Batch size & 256 \\
        & Learning rate & $1\times10^{-4}$ \\
        & Optimizer & Adam \\
    \midrule
        \multirow{6}{*}{Fine-tuning} & Total max gradient steps & 1.5M \\
        & Max epochs & 300 \\
        & Steps per epoch & 5000 \\
        & Batch size & 256 \\
        & Learning rate & $1\times10^{-5}$ \\
        & Optimizer & Adam \\
    \bottomrule
    \end{tabular}
    \caption{Hyperparameters for MLP Ensemble.}
    \label{tab:hyperparam_mlp}
\end{table}


\subsection{Zero-Shot Generalization}

\subsubsection{Environment Parameter Transfer}
\paragraph{Pendulum.}
We pre-train the TrajWorld model on 60 Gym Pendulum environments, where the gravity values range from $8$ m/s$^2$ to $12$ m/s$^2$. The pre-training dataset is collected by running the TD3 algorithm \cite{fujimoto2018addressing} and storing all episodes until the policy converges. For evaluation, we use five holdout environments with gravity values between $6.5 $m/s$^2$ and $7.5 $m/s$^2$, collecting data in the same manner as the training datasets. The zero-shot results are reported as the average prediction error on these holdout datasets.

\paragraph{Walker2D.}
We pre-train a four-layer TrajWorld model using 45 training datasets provided by MACAW \cite{mitchell2021offline} and evaluate it on a separate dataset also from MACAW. The datasets in MACAW are collected under varying physical conditions, including differences in body mass, friction, damping, and inertia.

\subsubsection{Cross-Environment Transfer}
\label{app:zero-shot}
We evaluate the model pre-trained on UniTraj by performing a ten-step rollout in the Cart-2-Pole and Cart-3-Pole environment from the DeepMind Control Suite. The rollout is conditioned on a history of ten prior timesteps. After this initial context, actions are applied in a simple predefined manner: either continuously pushing to the right ($a=0.5$) or to the left ($a=-0.5$). The action repeat for the Cart-2-Pole and Cart-3-Pole environment is set to 4.

\subsection{Transition Prediction}
The model is trained on a dataset using this dataset's training set and tested on five test datasets that come from the same environment. The evaluation for each test set is based on the model's prediction error across the entire test dataset. We use the Mean Absolute Error (MAE) as the evaluation metric. The prediction of TrajWorld is done by maintaining a history context window of 19 to predict the 20th state and reward.

In Figure \ref{fig:aggregate}, the prediction errors for each train-test dataset pair are normalized by dividing them by the MAE of the TrajWorld model without pre-training. The final result is then obtained by averaging across all environments.

\subsection{Off-Policy Evaluation}

\subsubsection{Implementation: Model-Based OPE}

Given a world model, the most direct method for off-policy evaluation (OPE) is Monte Carlo policy evaluation. This involves starting from a set of initial states, performing policy rollouts within the learned model, and averaging the accumulated rewards to estimate the policy value. The procedure is summarized in Algorithm \ref{alg:direct_rollout}.

In practice, we use a discount factor of $\gamma=0.995$ and a horizon length of $h=2000$. The number of samples $N$ is set such that each trajectoryâ€™s initial state from the behavior dataset is used exactly once, resulting in approximately $N \approx 1000$. We use KV cache to accelerate the rollouts of our TrajWorld.

\begin{algorithm}[tb]
\caption{Model-Based OPE}
\label{alg:direct_rollout}
\begin{algorithmic}
    \STATE \textbf{Input:} learned world model $P_\theta(s_{t+1}, r_{t+1}|s_{t},a_{t})$, test policy $\pi$, samples number $N$, initial state distribution $S_0$, discount factor $\gamma$, horizon length $h$. 
    \FOR{$i = 1$ to $N$}
        \STATE $R_i \leftarrow 0$
        \STATE Sample initial state $s_0 \sim S_0$
        \FOR{$t = 0$ to $h-1$}
            \STATE $a_t \sim \pi(\cdot | s_t)$
            \STATE $s_{t+1}, r_{t+1} \sim P_\theta(\cdot | s_t, a_t)$
            \STATE $R_i \leftarrow R_i + \gamma^t r_{t+1}$
        \ENDFOR
    \ENDFOR
    \STATE \textbf{Return} $\hat{V}(\pi)=\frac{1}{N} \sum_{i=1}^{N} R_i$
\end{algorithmic}
\end{algorithm}

\subsubsection{Baselines}

We primarily compare against model-based OPE with \textbf{Energy-based Transition Models (ETM)} \cite{chen2024offline}, a strong baseline that significantly outperforms previous methods and represents state-of-the-art on the DOPE benchmark \cite{fu2021benchmarks}.

We also include five classic OPE methods as baselines from the DOPE benchmark: \textbf{Fitted Q-Evaluation (FQE)} \citep{le2019batch}, \textbf{Doubly Robust (DR)} \citep{jiang2016doubly}, \textbf{Importance Sampling (IS)}, \citep{kostrikov2020statistical} \textbf{DICE} \citep{yang2020off}, and \textbf{Variational Power Method (VPM)} \citep{wen2020batch}.

 

\subsubsection{Metrics}
\label{app:ope_metric}
We adopt the evaluation metrics used in the DOPE benchmark.  

\paragraph{Mean Absolute Error.} The absolute error quantifies the deviation between the true value and the estimated value of a policy, defined as:  
\begin{equation}
\text{AbsErr} = |V^\pi - \hat{V}^\pi|,
\end{equation}
where \(V^\pi\) represents the true value of the policy, and \(\hat{V}^\pi\) denotes its estimated value. The Mean Absolute Error (MAE) is computed as the average absolute error across all evaluated policies. To aggregate results, these values are normalized by the difference between the maximum and minimum true policy values.

\paragraph{Rank correlation.} Rank correlation, also known as Spearmanâ€™s rank correlation coefficient (\(\rho\)), measures the ordinal correlation between the estimated policy values and their true values. It is given by:  
\begin{equation}
\text{RankCorr} = \frac{\text{Cov}(V^\pi_{1:N}, \hat{V}^\pi_{1:N})}{\sigma(V^\pi_{1:N}) \sigma(\hat{V}^\pi_{1:N})},
\end{equation}
where $1:N$ represents the indices of the evaluated policies.

\paragraph{Regret@$k$.} Regret@$k$ quantifies the performance gap between the actual best policy and the best policy selected from the top-$k$ candidates (ranked by estimated values). It is formally defined as:
\begin{equation}
\text{Regret@}k = \max_{i \in 1:N} V^\pi_i - \max_{j \in \text{topk}(1:N)} V^\pi_j
\end{equation}
where \(\text{topk}(1:N)\) denotes the indices of the top \(k\) policies based on estimated values \(\hat{V}^\pi\). In our experiments, we specifically use \textbf{normalized Regret@1} as the evaluation metric.

\subsection{Computational Cost}

Our implementation, built upon JAX \cite{jax2018github}, benefits from significant computational efficiency. Both pre-training and fine-tuning of the TrajWorld model can be conducted on a single 24GB NVIDIA RTX 4090 GPU. For comparison, the computational cost for 1.5M training steps in our implementations of the MLP Ensemble, TDM, and TrajWorld is $1.5$, $36$, and $28$ hours, respectively. This highlight that TrajWorld achieves strong performance with lower computational cost than TDM.



\section{Extended Experimental Results}
\label{app:exp}

\subsection{Detailed Prediction Error for Baselines}

We report the prediction error for MLP Ensemble and TDM in Figure \ref{fig:pred_mlp} and \ref{fig:pred_tt}, respectively.


\begin{figure}[p]
    \centering
    \includegraphics[width=1\linewidth]{img/pred_mlp.pdf}
    \caption{Mean absolute errors (MAE) of transition prediction for MLP Ensemble, with and without pre-training (PT), across different train-test dataset pairs. Each subplot corresponds to a distinct training dataset, with the test datasets shown on the x-axis (r=random, m-r=medium-replay, m=medium, m-e=medium-expert, e=expert). Error bars represent the standard deviation across three random seeds.}
    \label{fig:pred_mlp}
\end{figure}

\begin{figure}[p]
    \centering
    \includegraphics[width=1\linewidth]{img/pred_tt.pdf}
    \caption{ Mean absolute errors (MAE) of transition prediction for TDM, with and without pre-training (PT), across different train-test dataset pairs. Each subplot corresponds to a distinct training dataset, with the test datasets shown on the x-axis (r=random, m-r=medium-replay, m=medium, m-e=medium-expert, e=expert). Error bars represent the standard deviation across three random seeds.}
    \label{fig:pred_tt}
\end{figure}

\subsection{Quantitative Results for Off-Policy Evaluation}
\label{app:ope}

We report the raw absolute error, rank correlation and regret@1 for each OPE method and task in Table \ref{tab:ope_main}.

\begin{table}[p]
    \centering

    \begin{subtable}[b]{1\textwidth}
    \centering
    
    \resizebox{\linewidth}{!}{%
    \begin{tabular}{llccccccc}
    \toprule
    \textbf{Env.} & \textbf{Level} & ETM & MLP (w/o PT) & MLP (w/ PT) & TDM (w/o PT) & TDM (w/ PT) & TW (w/o PT) & TW (w/ PT)\\
    \midrule
    
    \multirow{5}{*}{\textbf{Hopper}}
    & random & 236 Â± 15 & 245 Â± 9 & 307 Â± 15 & 79 Â± 19 & 160 Â± 17 & 259 Â± 27 & \textbf{98} Â± 1 \\
    & medium & \textbf{47} Â± 21 & 149 Â± 30 & 181 Â± 18 & 140 Â± 10 & 145 Â± 7 & 81 Â± 11 & 127 Â± 9 \\
    & m-replay & \textbf{29} Â± 8 & \textbf{24} Â± 5 & 33 Â± 2 & 38 Â± 13 & 56 Â± 13 & 60 Â± 7 & 73 Â± 6 \\
    & m-expert & 32 Â± 4 & 87 Â± 35 & 173 Â± 15 & 116 Â± 21 & 79 Â± 3 & \textbf{48} Â± 7 & 69 Â± 8 \\
    & expert & 71 Â± 16 & 167 Â± 36 & 218 Â± 29 & 283 Â± 8 & 100 Â± 2 & 105 Â± 31 & \textbf{42} Â± 2 \\
    
    \midrule
    \multirow{5}{*}{\textbf{Walker2D}}
     & random & 339 Â± 10 & 356 Â± 4 & 372 Â± 3 & 291 Â± 40 & \textbf{264} Â± 9 & 312 Â± 19 & \textbf{269} Â± 1 \\
& medium & 159 Â± 13 & 181 Â± 10 & 371 Â± 9 & 104 Â± 22 & 123 Â± 12 & \textbf{61} Â± 6 & 101 Â± 7 \\
& m-replay & 132 Â± 31 & 131 Â± 8 & 313 Â± 15 & 143 Â± 52 & 147 Â± 3 & \textbf{54} Â± 12 & 182 Â± 10 \\
& m-expert & 152 Â± 9 & 210 Â± 47 & 340 Â± 19 & 87 Â± 24 & 137 Â± 17 & \textbf{60} Â± 11 & \textbf{72} Â± 7 \\
& expert & 364 Â± 7 & 344 Â± 20 & 368 Â± 15 & 403 Â± 141 & 458 Â± 19 & 272 Â± 124 & \textbf{100} Â± 2 \\
    
    \midrule
    \multirow{5}{*}{\textbf{Halfcheetah}}
     & random & \textbf{842} Â± 42 & 965 Â± 2 & 1137 Â± 27 & 1079 Â± 11 & 1050 Â± 4 & 1028 Â± 17 & 1059 Â± 7 \\
& medium & 655 Â± 114 & 734 Â± 24 & 973 Â± 91 & 1435 Â± 54 & 1312 Â± 21 & 568 Â± 23 & \textbf{444} Â± 4 \\
& m-replay & 727 Â± 119 & 712 Â± 59 & 993 Â± 41 & 927 Â± 261 & 730 Â± 25 & \textbf{540} Â± 45 & \textbf{540} Â± 16 \\
& m-expert & 689 Â± 203 & 692 Â± 65 & 1117 Â± 90 & 923 Â± 98 & 1319 Â± 23 & 809 Â± 150 & \textbf{528} Â± 10 \\
& expert & 758 Â± 116 & 973 Â± 175 & 1243 Â± 36 & 1273 Â± 158 & \textbf{646} Â± 50 & 1013 Â± 246 & 841 Â± 14 \\
    
    \bottomrule
    \end{tabular}
    }
    
    \caption{Raw absolute error}
    \label{tab:ope_main_abs}
    \end{subtable}

    
    \begin{subtable}[b]{1\textwidth}
    \centering
    
    \resizebox{\linewidth}{!}{%
    \begin{tabular}{llccccccc}
    \toprule
    \textbf{Env.} & \textbf{Level} & ETM & MLP (w/o PT) & MLP (w/ PT) & TDM (w/o PT) & TDM (w/ PT) & TW (w/o PT) & TW (w/ PT)\\
    \midrule
    
    \multirow{5}{*}{\textbf{Hopper}}
     & random random & 0.61 Â± 0.15 & 0.65 Â± 0.17 & 0.43 Â± 0.09 & \textbf{0.90} Â± 0.05 & 0.82 Â± 0.05 & 0.56 Â± 0.23 & 0.81 Â± 0.01 \\
& medium & \textbf{0.94} Â± 0.04 & 0.81 Â± 0.05 & 0.72 Â± 0.03 & 0.64 Â± 0.10 & 0.46 Â± 0.07 & 0.81 Â± 0.06 & 0.31 Â± 0.10 \\
& m-replay & \textbf{0.97} Â± 0.02 & 0.99 Â± 0.00 & \textbf{0.98} Â± 0.00 & 0.96 Â± 0.01 & 0.89 Â± 0.04 & 0.86 Â± 0.05 & 0.61 Â± 0.36 \\
& m-expert & \textbf{0.95} Â± 0.01 & \textbf{0.90} Â± 0.09 & 0.79 Â± 0.05 & 0.55 Â± 0.32 & 0.86 Â± 0.01 & 0.93 Â± 0.01 & 0.87 Â± 0.02 \\
& expert & 0.85 Â± 0.05 & 0.62 Â± 0.07 & 0.42 Â± 0.09 & -0.34 Â± 0.17 & 0.78 Â± 0.04 & 0.86 Â± 0.04 & \textbf{0.95} Â± 0.00 \\
    
    \midrule
    \multirow{5}{*}{\textbf{Walker2D}}
     & random & -0.12 Â± 0.32 & 0.75 Â± 0.03 & 0.58 Â± 0.17 & 0.73 Â± 0.10 & 0.79 Â± 0.02 & 0.67 Â± 0.01 & \textbf{0.78} Â± 0.00 \\
& medium & 0.78 Â± 0.12 & 0.90 Â± 0.03 & 0.44 Â± 0.12 & 0.86 Â± 0.04 & 0.91 Â± 0.02 & \textbf{0.95} Â± 0.01 & \textbf{0.94} Â± 0.00 \\
& m-replay & 0.77 Â± 0.10 & \textbf{0.95} Â± 0.01 & 0.72 Â± 0.08 & 0.88 Â± 0.03 & 0.93 Â± 0.02 & \textbf{0.97} Â± 0.02 & 0.77 Â± 0.01 \\
& m-expert & 0.67 Â± 0.14 & 0.92 Â± 0.02 & 0.74 Â± 0.06 & 0.91 Â± 0.04 & 0.79 Â± 0.06 & \textbf{0.95} Â± 0.01 & \textbf{0.96} Â± 0.01 \\
& expert & 0.54 Â± 0.11 & 0.36 Â± 0.11 & 0.11 Â± 0.42 & 0.36 Â± 0.42 & 0.80 Â± 0.01 & 0.59 Â± 0.30 & \textbf{0.94} Â± 0.01 \\
    
    \midrule
    \multirow{5}{*}{\textbf{Halfcheetah}}
     & random & 0.76 Â± 0.10 & 0.90 Â± 0.01 & \textbf{0.84} Â± 0.12 & 0.93 Â± 0.00 & 0.90 Â± 0.00 & 0.91 Â± 0.00 &\textbf{0.94} Â± 0.00 \\
& medium & 0.78 Â± 0.12 & 0.93 Â± 0.01 & 0.93 Â± 0.01 & -0.29 Â± 0.38 & 0.14 Â± 0.08 & 0.96 Â± 0.00 & \textbf{0.98} Â± 0.00 \\
& m-replay & 0.77 Â± 0.10 & 0.90 Â± 0.00 & 0.88 Â± 0.02 & \textbf{0.93} Â± 0.03 & 0.86 Â± 0.02 & \textbf{0.93} Â± 0.00 & \textbf{0.93} Â± 0.01 \\
& m-expert & 0.91 Â± 0.03 & \textbf{0.96} Â± 0.02 & 0.90 Â± 0.00 & 0.75 Â± 0.10 & 0.27 Â± 0.07 & 0.91 Â± 0.06 & \textbf{0.97} Â± 0.00 \\
& expert & 0.81 Â± 0.10 & \textbf{0.90} Â± 0.06 & 0.24 Â± 0.44 & 0.24 Â± 0.20 & 0.81 Â± 0.02 & 0.49 Â± 0.26 & \textbf{0.94} Â± 0.01 \\
    
    \bottomrule
    
    \end{tabular}
    }
    \caption{Rank correlation}
    \label{tab:ope_main_rank}
    \end{subtable}

    \begin{subtable}[b]{1\textwidth}
    \centering
    
    \resizebox{\linewidth}{!}{%
    \begin{tabular}{llccccccc}
    \toprule
    \textbf{Env.} & \textbf{Level} & ETM & MLP Ens.(w/o PT) & MLP Ens.(w/ PT) & TDM (w/o PT) & TDM (w/ PT) & TW (w/o PT) & TW (w/ PT)\\
    \midrule
    
    \multirow{5}{*}{\textbf{Hopper}}
     & random & \textbf{0.20} Â± 0.10 & \textbf{0.30} Â± 0.28 & 0.62 Â± 0.00 & \textbf{0.20} Â± 0.15 & \textbf{0.25} Â± 0.08 & \textbf{0.39} Â± 0.32 & 0.37 Â± 0.00 \\
    & medium & \textbf{0.05} Â± 0.04 & \textbf{0.03} Â± 0.04 & \textbf{0.05} Â± 0.04 & \textbf{0.17 }Â± 0.17 & 0.16 Â± 0.00 & 0.22 Â± 0.13 & \textbf{0.11} Â± 0.06 \\
    & m-replay & \textbf{0.00} Â± 0.00 & 0.08 Â± 0.00 & 0.08 Â± 0.00 & 0.09 Â± 0.02 & 0.08 Â± 0.00 & 0.15 Â± 0.01 & 0.14 Â± 0.00 \\
    & m-expert & \textbf{0.08} Â± 0.07 & \textbf{0.09} Â± 0.02 & \textbf{0.12} Â± 0.21 & 0.37 Â± 0.00 & \textbf{0.08} Â± 0.00 & 0.15 Â± 0.13 & \textbf{0.08} Â± 0.00 \\
    & expert & \textbf{0.08} Â± 0.02 & \textbf{0.17} Â± 0.17 & \textbf{0.17} Â± 0.17 & 0.68 Â± 0.55 & \textbf{0.08} Â± 0.00 & 0.12 Â± 0.15 & \textbf{0.10} Â± 0.02 \\
    
    \midrule
    \multirow{5}{*}{\textbf{Walker2D}}
     & random & 0.16 Â± 0.09 & \textbf{0.05} Â± 0.01 & 0.41 Â± 0.40 & 0.10 Â± 0.04 & \textbf{0.05} Â± 0.00 & 0.17 Â± 0.00 & 0.08 Â± 0.00 \\
& medium & \textbf{0.00} Â± 0.00 & 0.08 Â± 0.00 & 0.28 Â± 0.00 & 0.08 Â± 0.07 & 0.12 Â± 0.00 & \textbf{0.04} Â± 0.07 & 0.08 Â± 0.00 \\
& m-replay & \textbf{0.00} Â± 0.00 & 0.07 Â± 0.02 & 0.19 Â± 0.16 & 0.08 Â± 0.04 & 0.10 Â± 0.06 & \textbf{0.03} Â± 0.05 & \textbf{0.00} Â± 0.00 \\
& m-expert & \textbf{0.03} Â± 0.02 & 0.08 Â± 0.00 & \textbf{0.09} Â± 0.16 & \textbf{0.06} Â± 0.10 & 0.12 Â± 0.00 & \textbf{0.05} Â± 0.05 & 0.08 Â± 0.00 \\
& expert & \textbf{0.05} Â± 0.05 & \textbf{0.19} Â± 0.16 & \textbf{0.19} Â± 0.26 & \textbf{0.12} Â± 0.14 & 0.28 Â± 0.00 & \textbf{0.28} Â± 0.28 & \textbf{0.17} Â± 0.10 \\
    
    \midrule
    \multirow{5}{*}{\textbf{Halfcheetah}}
     & random & 0.20 Â± 0.10 & 0.15 Â± 0.10 & 0.11 Â± 0.12 & \textbf{0.04} Â± 0.00 & \textbf{0.09} Â± 0.08 & \textbf{0.14} Â± 0.10 & \textbf{0.03} Â± 0.02 \\
& medium & \textbf{0.08} Â± 0.08 & 0.18 Â± 0.00 & 0.17 Â± 0.02 & 0.70 Â± 0.52 & 0.23 Â± 0.07 & \textbf{0.12} Â± 0.11 & \textbf{0.12} Â± 0.02 \\
& m-replay & \textbf{0.16} Â± 0.12 & \textbf{0.15} Â± 0.10 & \textbf{0.23} Â± 0.07 & \textbf{0.16} Â± 0.05 & 0.25 Â± 0.00 & \textbf{0.18} Â± 0.00 & \textbf{0.18} Â± 0.00 \\
& m-expert & 0.11 Â± 0.10 & \textbf{0.06} Â± 0.11 & 0.18 Â± 0.00 & 0.16 Â± 0.05 & 0.37 Â± 0.00 & 0.14 Â± 0.13 & \textbf{0.00} Â± 0.00 \\
& expert & 0.12 Â± 0.07 & 0.16 Â± 0.02 & \textbf{0.04} Â± 0.00 & 0.27 Â± 0.10 & 0.14 Â± 0.00 & 0.18 Â± 0.03 & \textbf{0.20} Â± 0.19 \\
    
    \bottomrule
    \end{tabular}
    }
    \caption{Regret@1}
    \label{tab:ope_main_regret}
    \end{subtable}
    \caption{Quantitative results of all model-based methods (TW=TrajWorld) for OPE, averaged over 3 seeds.}
    \label{tab:ope_main}
\end{table}

\subsection{Off-Policy Evaluation with Pre-trained Models on Parameter-Variant Environments.}

In addition to the zero-shot prediction error reported in Section \ref{sec:zero-shot}, we further investigate our four-layer model pre-trained on Walker2D with variant friction, mass, etc. Specifically, we evaluate the model's performance by fine-tuning it and testing it on downstream off-policy evaluation tasks on standard Walker2D. The results are summarized in Table \ref{tab:additional_ope_results}. This provides additional evidence, beyond the zero-shot prediction error, demonstrating that TrajWorld exhibits strong capability for transfer to environments with varying parameters.

\begin{table}[H]
\centering

    \begin{tabular}{llcc}
    \toprule
    \textbf{Env.} & \textbf{Level} & TrajWorld (w/o PT) & TrajWorld (w/ PT)\\
    \midrule
    \multirow{5}{*}{Walker2D}
     & random   & 262  Â± 34   & \textbf{76}   Â± 6    \\
     & medium   & 68   Â± 2    & \textbf{40}   Â± 4    \\
     & m-replay & 71   Â± 11   & \textbf{46}   Â± 1    \\
     & m-expert & \textbf{49}   Â±  1   & 76   Â± 1    \\
     & expert   & 281  Â±  8   & \textbf{186}  Â± 1    \\
    
    \bottomrule
    \end{tabular}
\caption{Raw absolute error of off-policy evaluation for a four-layer TrajWorld model trained from scratch compared to a model fine-tuned from a pre-trained version on the Walker2D dataset with variant environment parameters with holdout onest, averaged over two seeds.}
\label{tab:additional_ope_results}
\end{table}









\subsection{Additional Zero-shot Cross-Environment Transfer}

\begin{wrapfigure}{r}{0.4\textwidth} 
\vspace{-30pt}
    \centering
    \includegraphics[width=\linewidth]{img/cart3pole.pdf}
    \vspace{-20pt}
    \caption{TrajWorld's zero-shot predictions for two Cart-3-Pole trajectories, which share 10 context steps but diverge due to differing subsequent actions.}
    \label{fig:cart-3-pole}
    \vspace{-25pt}
\end{wrapfigure}

We also test TrajWorld's zero-shot prediction on the more challenging Cart-3-pole environment, which has an 11-dimensional state space. Surprisingly, TrajWorld can still give cart's position predictions roughly aligned with the ground truth, despite not seeing this embodiment before. The action sequence is depicted in Section~\ref{app:zero-shot}.

\subsection{Additional Variate Attention Visualization}
\label{app:attention_add}

We present the variate attention maps of our TrajWorld model across all six layers, comparing a fine-tuned model and a model trained from scratch, in Figures \ref{fig:attention_all} and \ref{fig:attention_all_scr}.

For the fine-tuned model, in the early layers (Layer 0 and 1), attention is more scattered and less structured, likely capturing broad and low-level features. In contrast, later layers (Layer 4 and 5) exhibit more focused attention, suggesting the model is concentrating on specific relationships or entities. The prominent diagonal patterns and neighboring attentions discussed in Section \ref{analysis_attention} can also be clearly observed in Layer 2. Additionally, diagonal patterns linking joint velocities and actions appear in Layers 1 and 2. Such diagonal patterns are also observed in the attention maps of the model trained from scratch.

A notable difference between the attention maps of the fine-tuned model and the model trained from scratch is the earlier emergence of diagonal patterns in the layers of the model trained from scratch. Specifically, while the first two layers of the fine-tuned model exhibit more scattered and less interpretable attention, the scratch-trained model immediately begins capturing structured diagonal patterns, particularly between positions and velocities, as well as velocities and actions. This probably suggests that pre-training changes the model's behavior. The model without pre-training tends to focus on environment-specific patterns and more localized features for prediction. In contrast, the fine-tuned model seems to dedicate its first two layers to extracting more semantically meaningful and generalizable features, encouraging the model to perform inference through in-context learning from these environment-agnostic representations.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\linewidth]{img/attention_map_walker.png}
    \vspace{-10pt}
    \caption{Variate attention maps of our pre-trained TrajWorld Model, fine-tuned under Walker2D environment. }
    \label{fig:attention_all}
\end{figure}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\linewidth]{img/attention_map_scratch.png}
    \vspace{-10pt}
    \caption{Variate attention maps of our TrajWorld Model in the Walker2D environment, trained from scratch. }
    \label{fig:attention_all_scr}
\end{figure}

\subsection{Ablation Study on Pre-training Dataset}
\label{app:dataset_ablation}

To investigate the contributions of different components of the UniTraj dataset to the pre-training process, we conduct an ablation study by training a four-layer TrajWorld model on a modified version of the UniTraj dataset, excluding two data sources more closely aligned with the target environments: Modular RL and TD-MPC2. The results presented in Table \ref{tab:effects_of_pt_dataset} indicate that the advantages of pre-training stem from the diversity encompassed within the complete UniTraj dataset, rather than relying solely on data from domains closely resembling the target environments.

\begin{table}[H]
    \centering
     \begin{subtable}[b]{1\textwidth}
            \centering
                \begin{tabular}{llcc}
                    \toprule
                    \textbf{Env.} & \textbf{Level} & TrajWorld (w/o PT) & TrajWorld (w/ PT)\\
                    \midrule
                     Halfcheetah & medium   & \textbf{491}  Â± 94   & \textbf{468} Â± 40    \\
                     Walker2D & medium   & 88   Â± 21    & \textbf{54}   Â± 2    \\
                     Walker2D & expert & 141   Â± 15   & \textbf{116}   Â± 12    \\
                    \bottomrule
                \end{tabular}
        \caption{Raw absolute error}
    \end{subtable}
     \begin{subtable}[b]{1\textwidth}
            \centering
            \begin{tabular}{llcc}
                \toprule
                \textbf{Env.} & \textbf{Level} & TrajWorld (w/o PT) & TrajWorld (w/ PT)\\
                \midrule
                 Halfcheetah & medium   & 0.95  Â± 0.00   & \textbf{0.97}  Â± 0.01    \\
                 Walker2D & medium   & 0.93   Â± 0.01    & \textbf{0.97}   Â± 0.02   \\
                 Walker2D & expert & 0.56   Â± 0.20   & \textbf{0.81}  Â± 0.04    \\
                \bottomrule
            \end{tabular}
        \caption{Rank Correlation}
    \end{subtable}
    \begin{subtable}[b]{1\textwidth}
                \centering
                \begin{tabular}{llcc}
                    \toprule
                    \textbf{Env.} & \textbf{Level} & TrajWorld (w/o PT) & TrajWorld (w/ PT)\\
                    \midrule
                     Halfcheetah & medium   & 0.18 Â± 0.00   & \textbf{0.05} Â± 0.07    \\
                     Walker2D & medium   & 0.04   Â± 0.06    & \textbf{0.00}   Â± 0.00    \\
                     Walker2D & expert & 0.34   Â± 0.31   & \textbf{0.16}   Â± 0.16       \\
                    \bottomrule
                \end{tabular}
        \caption{Regret@1}
    \end{subtable}
    {
    \vspace{-20pt}
    \caption{OPE results for a four-layer TrajWorld model trained from scratch compared to a model fine-tuned from a pre-trained version on the ablation dataset, averaged over two seeds.}
    \label{tab:effects_of_pt_dataset}
    }
\end{table}


\section{Extended Discussion}
\label{app:discussion}

\paragraph{Bounded prediction.} Our discretization scheme (Section \ref{sec:architecture}) has the drawback that it can only represent variate values within the bounded range $[b_0, b_B]$, restricted by the maximum and minimum in training data. This can lead to inaccurate predictions. For example, a model trained on trajectories from low-performing policies, may underestimate the reward of a high-rewarding transition. This may explain why our model slightly underperforms in Regret@1 for off-policy evaluation tasks. Since all variates share the same bin embeddings, a promising way to address this issue is to simply extend the value range of bins beyond the observed data limits for variates with narrow coverages. Although the model would not have encountered those out-of-range values for a specific variate during training, we hypothesize it could extrapolate similarly to regression models (e.g., MLPs), leveraging learned bin ordering shared with other variates. This hypothesis is supported by the bin continuity observed in Figure~\ref{fig:tsne}. Further exploration and improvement of this approach are left for future work.











\end{document}
