\documentclass[12pt,english]{article}
%------------------------------------------------%
\usepackage[sort&compress,numbers]{natbib}

\usepackage{hyperref}
\hypersetup{
    unicode = false,
    pdftoolbar = true,
    pdfmenubar = true,
    pdffitwindow = true,
    pdftitle = {JOGOD2100161},
    pdfauthor = {JOGO},
    pdfsubject = {Review},
    pdfnewwindow = true,
    pdfkeywords = {Nan},
    colorlinks = true,
    linkcolor = blue,
    citecolor = blue,
    filecolor = black,
    urlcolor = blue,
    breaklinks = true
}

\usepackage[english]{babel}
\usepackage[T1]{fontenc}
\usepackage[latin1]{inputenc}
\usepackage{amsfonts}
\usepackage{dsfont}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{multirow}
\usepackage{epsfig}
\usepackage{soul}
\usepackage{colortbl}
\usepackage{geometry}
\usepackage{times}
\usepackage{graphicx}
\usepackage{pdfpages}
%================================================================
\usepackage{mathrsfs}
\usepackage{frcursive}
\usepackage{bm}
%================================================================
\usepackage[ruled, vlined, linesnumbered]{algorithm2e}
\usepackage{color}
\usepackage{booktabs}
%================================================================

\definecolor{gris3}{rgb}{0.3,0.3,0.3}

\definecolor{Green}{rgb}{0,.6,0}
\definecolor{Blue}{rgb}{0,0,1}
%----------------------------------------------------------------
\definecolor{Red}{rgb}{1,0,0}
\definecolor{Gray}{rgb}{0.2,0.2,0.2}
\definecolor{Maroon}{rgb}{0.6,0.05,0.03}

\newcommand{\bl}{\color{black}} % For 1st revision
\newcommand{\bbl}{\color{blue}} % For 2nd revision
\newcommand{\rd}{\color{red}}

\geometry{letterpaper, tmargin=3cm, bmargin=3cm, lmargin=2.3cm, rmargin=2.3cm}
\setlength{\tabcolsep}{2.5pt}

%=======================TikZ===========================================
\usepackage{tikz}
\usetikzlibrary{shapes}
\usetikzlibrary{fit, arrows, calc, positioning}
\usetikzlibrary{arrows, automata}
\usetikzlibrary{positioning}
\usetikzlibrary{shadows}

\tikzstyle{c} = [rectangle, rounded corners, draw]
\tikzset{box/.style={draw,rectangle,rounded  corners=0pt, fill=gray!15, minimum  width=3cm, minimum  height=2cm}}
\tikzset{loz/.style={draw,diamond, aspect=1.5, fill=gray!15}}

\tikzstyle{rnd}=[circle,fill=blue!25,minimum  width=5em]

%%%%


\newcommand{\C}{\mathbb{C}}
\newcommand{\normax}{{\left\lVert Ax\right\rVert}_{2}}
\newcommand{\U}{\mathcal{U}}
\newcommand{\Ln}{L_{n,\alpha,\gamma}}
\newcommand{\An}{A_{n,\epsilon}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\Esp}{\mathbb{E}}
\newcommand{\rn}{\mathbb{R}^n}
\newcommand{\rnn}{\mathbb{R}^{n\times n}}
\newcommand{\s}{\mathbb{S}}
\newcommand{\snA}{s_n(A)}
\newcommand{\sn}{\mathbb{S}^{n-1}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\V}{\mathcal{V}}
\newcommand{\D}{\mathcal{D}}
\newcommand{\B}{\mathcal{B}}
\newcommand{\dx}{\Delta x}
\newcommand{\F}{\mathcal{F}}
\newcommand{\cov}{\operatorname{cov}}
\newcommand{\levy}{\mathcal{L}}
\newcommand{\n}{\mathcal{N}}
\newcommand{\ON}{\mathcal{O}_N}
\newcommand{\hh}{\mathcal{H}}
\newcommand{\X}{\mathcal{X}}
\newcommand{\lun}{\mathbb{L}^1(\Omega,\mathcal{G},\mathbb{P})}
\newcommand{\lp}{{\mathbb{L}}^p(\Omega,\mathcal{G},\mathbb{P})}
\newcommand{\ldeux}{{\mathbb{L}}^2(\Omega,\mathcal{G},\mathbb{P})}
\newcommand{\Y}{\mathcal{Y}}
\newcommand{\pb}{\mathbb{P}}
\newcommand{\M}{\mathcal{M}}
\newcommand{\comp}{{Comp}(\delta,\rho)}
\newcommand{\incomp}{{Incomp}(\delta,\rho)}
\newcommand{\sparse}{{Sparse}(\delta)}
\newcommand{\faibl}{\rightharpoonup}
\newcommand{\dplExp}{\delta^k}
\newcommand{\DpExp}{\Delta^k}
\newcommand{\DkExp}{\Delta^k}
\newcommand{\DmExp}{\Delta^k_m}
\newcommand{\dmExp}{\delta^k_m}
\newcommand{\dkExp}{\delta^k}
\newcommand{\epr}{{\epsilon'}}
\newcommand{\ttk}{\theta^k}
\newcommand{\Tk}{\Theta^k}
\newcommand{\sk}{s^k}
\newcommand{\Sk}{S^k} 
\newcommand{\fe}{f_\Theta}
\newcommand{\Mk}{\mathcal{M}^k}
\newcommand{\fok}{f^k_0}
\newcommand{\fsk}{f^k_s}
\newcommand{\dmaxExp}{\delta_{\max}}
\newcommand{\Fok}{F^k_0}
\newcommand{\Fsk}{F^k_s}
\newcommand{\ef}{\varepsilon_{f,p}} 
\newcommand{\ir}{\mathds{1}_{S^R}}   
\newcommand{\isucc}{\mathds{1}_{S}}   
\newcommand{\iu}{\mathds{1}_{S^{\bar{R}}}}
\newcommand{\iunsuc}{\mathds{1}_{{\bar{S}}^C}}
\newcommand{\iunc}{\mathds{1}_{{\bar{S}}^{\bar{C}}}}
\newcommand{\ijk}{\mathds{1}_{J_k}}
\newcommand{\iji}{\mathds{1}_{J_i}}
\newcommand{\ijkc}{\mathds{1}_{\bar{J_k}}}
\newcommand{\isuccC}{\mathds{1}_{\text{Succ}^c}}
\newcommand{\isbar}{\mathds{1}_{\bar{S}}} 
\newcommand{\ds}{\displaystyle}
%======================================================================


\newcommand{\esp}{\mathbb{E}}
%======================================================================
\newcommand{\accolade}[1]{\left\lbrace#1\right\rbrace}
\newcommand{\abs}[1]{\left\lvert#1\right\rvert}
\newcommand{\E}[1]{\mathbb{E}\left(#1\right)}
\newcommand{\pr}[1]{\mathbb{P}\left(#1\right)}
\newcommand{\var}[1]{\mathbb{V}\left(#1\right)}
\newcommand{\lcd}[1]{\text{LCD}_{\alpha,\gamma}\left(#1\right)}
\newcommand{\vol}[1]{\text{vol}\left(#1\right)}
\newcommand{\vect}[1]{\text{vect}\lbrace#1\rbrace}
\newcommand{\norme}[1]{\left\lVert#1\right\rVert}
\newcommand{\scal}[2]{\left\langle#1,#2\right\rangle}
\newcommand{\dist}[2]{d\left(#1,#2\right)}
\newcommand{\Dist}[2]{\text{dist}\left(#1,#2\right)}
\newcommand{\fct}[2]{#1\left(#2\right)}
\newcommand{\normp}[1]{{\left\lVert#1\right\rVert}_{p}}
\newcommand{\norms}[1]{{\left\lVert#1\right\rVert}_{\dot{H}^{s}(\R^{d})}}
\newcommand{\normhs}[1]{{\left\lVert#1\right\rVert}_{H^{s}(\R^{d})}}
\newcommand{\norminf}[1]{{\left\lVert#1\right\rVert}}
\newcommand{\normhi}[1]{{\left\lVert#1\right\rVert}_{H^{1}(\R^{d})}}
\newcommand{\normlii}[1]{{\left\lVert#1\right\rVert}_{L^{2}(\R^{d})}}
\newcommand{\ird}[2]{{\left(\int_{\R^{d}}{#1}\right)}^{\frac{1}{#2}}}
\newcommand{\comb}[2]{\begin{pmatrix}
		{#1}\\
		{#2}
\end{pmatrix}}
\newcommand{\irdom}[3]{{\left(\int_{#1}{#2}\right)}^{\frac{1}{#3}}}
\newcommand{\petito}[1]{o\mathopen{}\left(#1\right)}
\newcommand{\grando}[1]{O\mathopen{}\left(#1\right)}
\newcommand\indep{\protect\mathpalette{\protect\independenT}{\perp}}
\def\independenT#1#2{\mathrel{\rlap{$#1#2$}\mkern2mu{#1#2}}}
\newcommand{\indepdt}{\rotatebox[origin=c]{90}{$\models$}}
\newcommand{\interff}[2]{\intervalle{[}{#1}{#2}{]}}
\newcommand{\interof}[2]{\intervalle{]}{#1}{#2}{]}}
\newcommand{\interfo}[2]{\intervalle{[}{#1}{#2}{[}}
\newcommand{\interoo}[2]{\intervalle{]}{#1}{#2}{[}}

\newcommand{\calL}{{\cal L}}
\newcommand{\T}{\mathrm{T}}
\def\real{\mathbb{R}}
\DeclareMathOperator{\NLP}{NLP}
\DeclareMathOperator{\MILP}{MILP}
\DeclareMathOperator{\doe}{DoE}
\newcommand{\youssef}[1]{\textcolor{red}{Youssef: #1}}

\newcommand{\revised}[1]{\textcolor{red}{#1}}
\newcommand{\comment}[1]{\textit{#1}}

\newcommand{\answer}[1]{\textcolor{blue}{#1}}

\newcommand{\toanswer}[1]{\textcolor{red}{To do: #1}}
\begin{document}
\begin{center} \Large
High-dimensional efficient global optimization using both random and supervised embeddings 
\\[6pt] 
\large R\'emy Priem, Youssef Diouane, Nathalie Bartoli, Sylvain Dubreuil and Paul Saves
\\[6pt] \textit{AIAA Journal}
\\[6pt] \today
\\[12pt]\Large \bf Response to reviewers for Submission ID 2023-08-J063488
\end{center}

We are very grateful to both reviewers for their careful reading 
of the paper and for their  insightful feedbacks. 
For the reviewers' convenience, the changes are highlighted in \revised{red} in 
the revised manuscript. All references, equation numbers, etc, correspond to the revised version of the manuscript.

\section*{Reply to Reviewer 1}
%The Editors particularly desire your specific comments on technical content, overall value, relevancy, accuracy of computed results or experimental data, and revisions needed for conciseness, clarity, and/or completeness.
This paper proposes a new approach for solving optimisation problems with limited objective function evaluation budgets, and also applied to problems of different numbers of design variables. This is achieved by embedding a dimension reduction strategy (using subspace projection) into a surrogate modelling optimisation strategy (termed Bayesian optimisation framework). In summary, I find the paper well written and mostly complete. The method is well described. The investigations as presented show that the method works well but much more work is required here to compare the method on a wider range of problems. With some changes and expansion of the results, the paper would be suitable for publication in AIAA Journal. 



{\color{blue} %Response:
Dear reviewer, we would like to thank you for your thorough comments and feedback. Your comments and
feedback were very valuable for us to improve the quality and clarity of our paper. Please find our
responses to your comments along with the corresponding revisions made in the revised manuscript
below. 
%Please note that the corresponding changes have been highlighted in red within the revised manuscript.
}
\subsection*{Specific comments}
\begin{enumerate}

\item The term "High-dimensional" in the title of the paper is only one aspect of the work. Another interesting aspect that would be worth teasing out in the abstract (and perhaps in the title also, though this is not absolutely necessary) is that the method is also applied to problems where there is a limited budget for objective function evaluations. The authors test their problem on 100D problems using a few hundred cost evaluations, which is impressive, so this is worth noting in the abstract.


{\color{blue} %Response:
We thank the reviewer for this positive comment. Following the reviewer suggestion we add a sentence in the abstract to highlight this point. 
}

   \item P2 - sentence "In this was, it is not possible...". It's more accurate to say "it is not desirable" since it is easily possible to do MDAO with low-fidelity models.
    
    {\color{blue} %Response: 
    Fixed. Please see the  paragraph P2 of the introduction (page 2). }
    \item P2 - final sentence "This means that no...". This is factually incorrect, particularly the inclusion of evolutionary computing in the statement. If only the output values were available, then evolutionary computing would be an obvious choice. Please update this sentence to be correct.
 {\color{blue} We thank the reviewer for their comment. Indeed, evolutionary computing methods can be also suitable for blackbox optimization. However, such methods often require a large number functions evaluation which is not possible to afford when we target to solve expensive-to-evaluate blackbox optimization problem such as those related to aircraft design. We clarified our claim in the revised version of the paper; we mention now explicitly the reason for which evolutionary based optimization methods  is not adapted to our setting. Please see the  paragraph P2 of the introduction (page 2). }

       \item The method is well described, however, I find the justification on the choice of surrogate approach to be lacking. Currently the paper just presents the dimension reduction approach but does not discuss or elaborate further on why that method was chosen, or what the alternatives could be. For example, why not use POD or PCA?
    
    {\color{blue} The main reason behind the choice of working with Gaussian process surrogates is motivated by the fact that, contrarily to  deterministic surrogates (e.g., PCA and POD), it gives both a prediction of the quantity of interest and the associate uncertainty. This uncertainty prediction is important because it is the keystone that allow as Bayesian optimization to explore the design space and target better global minima. We clarified this points on the introduction of the revised version of the manuscript. 
    }
    
        
    \item  P6 - Please provide the formulation for the transfer matrix, A, as I couldn't see to find it in the paper. This is key to reproducibility of the method. 
    
{\color{blue} First, we stress that our proposed approach is fairly general and allows to incorporate any embedding (supervised or not). In this work, we have implemented two type of transfer matrices using the PLS and the MGP technique. Following the review suggestion, we have added a new paragraph with implementation details specifically on the PLS and MGP formulations (see Section F pages 11-12, on the revised manuscript). For more information on KPLS and MGP, we point also the reader to \citet{hellandStructurePartialLeast1988} for the PLS and \citet{GarnettActiveLearningLinear2014} for the MGP method.}

%The transfer matrix is provided by the dimension reduction method depending on the design of experiments.
%In this way, it is not possible to provide it. The optimization methodology described in this paper can be used with any dimension reduction method.

%{\color{olive}Furthermore, we briefly introduced the transfer matrix formulations of KPLS and MGP in the appendix.}For more information on KPLS and MGP, the reader can look at the following cited papers : Helland, I. S., "On the Structure of Partial Least Squares Regression," Communications in statistics-Simulation and Computation, Vol. 17, 1988, pp. 581-607 and Garnett, R., Osborne, M., and Hennig, P., "Active Learning of Linear Embeddings for Gaussian Processes," Proceedings of the 30th Conference on Uncertainty in Artificial Intelligence (UAI 2014), 2014, p. 10.}
    
       \item On the results, at first glance the presented method appears to do well, and the authors are praised for comparing to other, state-of-the-art methods. However, there are some shortcomings to the results as presented, and need addressing before publication:
       
       \begin{itemize}
       \item[(a)] The optimum objective value f*=1.1, and looking at fig 5, it looks like the presented method is only able to get to about 4 for the 10D case and about 8 for the 100D case. One could therefore argue that in fact, the method has not performed well. I understand that you have approached this with limiting the number of function evaluations, but more discussion is needed. Specifically, I would recommend that the authors perform a study on the effect of objective function evaluation budget e.g. 8e2, 8e3, 8e4, 8e5. This would give the reader a better idea of the performance across a wider number of conditions.

         {\color{blue} Reaching the global minimum of the modified Branin optimization problem (using a limited budget) is very hard, particularly in high dimensions. 
          We clarified this point in the revised version of the paper, see paragraph P3 in page 15.
         %{\color{red}Youssef: I guess here, we need to include a sentence to clarify this point in the revised paper.}
         
         
         Regarding the second part of the reviewer comment, we stress that Bayesian optimization is not a method of choice when a wider number of function evaluations is available. Typically, in such case, one does better to use evolutionary based optimization methods. For instance, in typical aircraft design optimization problems, a simulation  of the blackbox might take few minutes to compute, in this case, using 8e5 blackbox evaluations would take years to end the optimization process. We clarified this point in the revised version of the paper, both in the title and in the abstract as well.
         }
       \item[(b)] Also, it is not enough to study one function and draw conclusions on this. A wider number of functions should be studied to determine statistically significant results. There is plenty of work on expensive cost functions in the IEEE CEC competitions (e.g. https://github.com/P-N-Suganthan/CEC2018) so the authors should present results for a full suite of the test functions and use statistical tests following guidelines presented in literature \href{https://doi.org/10.1016/j.swevo.2020.100665}, to compare methods.

          {\color{blue}
          We understand the reviewer suggestion. However, the test beds  CEC/GECCO (which are very popular in the evolutionary community) are not very used in the context of Bayesian optimization where only a very limited budget is allowed. We believe also that solving practical (real world) problems is very important. For this reason, we have decided to include in our comparison two test problems: a very challenging analytical optimization problem and a practical one.
           The modified Branin (analytical) and the rover  (real world application) problems are reference problems in the Bayesian optimization community. We make the choice of keeping only those test problems because we believe that those problems are very challenging and very complex (highly non linear with multiple local minima). The second reason, for choosing to not include many test problems is to allow a detailed analysis of the different numerical aspects of the proposed methods. Additionally, we have included in our comparisons, different state-of-the-art BO methods, namely Binois et al~\cite{binoisChoiceLowdimensionalDomain2020} and Nayebi et al.~\cite{nayebiFrameworkBayesianOptimization2019}. 
          
         % In fact, in this paper, we challenge our algorithm on a real world problem, name ROVER, which is not the case for Binois et al. [13] and Nayebi et al. [27]. In this way, it seems acceptable to evaluate our framework with 3 functions. 
     } 
       \end{itemize}
   
\end{enumerate}

\section*{Reply to Reviewer 2}

This paper introduces an approach for Bayesian optimization that is suitable for high-dimensional optimization problems that are common in aerospace engineering.  Specifically, a linear embedding and adaptive learning procedure is introduced.  The method is demonstrated on a number of problems, including two analytic optimization problems and a Rover path planning problem.  

Overall, I think this approach is interesting, and it seems to perform well.  However, the connection to aerospace engineering seems pretty thin at the moment.  I think this could be appropriate for AIAA J, but would require a major revision to address these issues.


    {\color{blue} Dear reviewer, we would like to thank you for your thorough comments and feedbacks. Your suggestions were very valuable for us to improve the quality and clarity of our paper. Please find our
responses to your comments along with the corresponding revisions made in the revised manuscript
below. We note that the corresponding changes have been highlighted in red within the revised
manuscript.}


\begin{itemize}
    \item The examples are not as relevant for aerospace engineering as they could be.  Could you demonstrate this for an additional optimization problem that is more relevant, like airfoil shape design optimization?  Ideally something with some physics or geometric constraints.
    
    {\color{blue} Indeed, the current tested problems are not directly related to aerospace engineering test-cases (which by definition are constrained).
    At this stage, the  methodology proposed in this paper for unconstrained optimization problems, including constrained is far not yet trivial. We believe that tackling large-scale unconstrained blackbox optimization problems is the first step toward tackling solving realistic aerospace problems. More investigations  are required to include constraints in our framework. We clarified this point in the perspectives of the revised version of the paper.}
    
    \item The conclusion is too brief.  What are limitations, caveats? Future directions are only very briefly mentioned.  These should all be expanded, and connected back to problems in aerospace engineering
    
    {\color{blue} Following the suggestion of the reviewer, we elaborate more in the revised version of the paper on the limitations and the perspectives related to the proposed approach, see paragraph P2 in the conclusion section.}



    \item The introduction needs to be connected more to aerospace engineering, specifically the types of optimization problems and challenges that are common.  I believe a more extensive literature survey would also be helpful here.  

    {\color{blue} Following the review suggestion. We improved the presentation of the application context of the paper and showed better the connection with the aerospace setting. See the first paragraph in the introduction of the revised version of the paper.}

    \item  For a new optimization method, it is critical that the code is provided to test and reproduce these results.  Perhaps I missed it, but I couldn't find it in the current manuscript.  I don't believe a methods paper like this can be accepted without an associated open code base.

    
    {\color{blue} In this work, we propose a new framework to handle high-dimensional problems using supervised embeddings. The framework's main implementation is based on the coupling of classical Bayesian optimization solvers (e.g., EGO) and the use of advanced supervised embeddings (e.g., PLS and MGP). The code for all these components (EGO optimizer and the embeddings PLS and MGP) is made publicly available in the open-source surrogate modeling toolbox (SMT)~\cite{bouhlelPythonSurrogateModeling2019,SMT2023}. The associated GitHub repository\footnote{\href{https://github.com/SMTorg/smt}{https://github.com/SMTorg/smt}} is also listed at the end of Section IV.}
    \end{itemize}

\bibliographystyle{elsarticle-harv}
\bibliography{Biblio.bib}

\includepdf[pages=-]{HDBO_TEST.pdf}

\end{document}