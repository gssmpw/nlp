%\documentclass[conf]{new-aiaa}
\documentclass[journal]{new-aiaa} %for journal papers
\usepackage[utf8]{inputenc}

\usepackage{algorithm}
\usepackage{algorithmic}

\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{mathtools}
\usepackage[version=4]{mhchem}
\usepackage{siunitx}
\usepackage{longtable,tabularx}
\setlength\LTleft{0pt} 
\usepackage{bm}

\usepackage{subfig}
\usepackage{multicol}
\usepackage{multirow}
 
%%% Commandes perso 
\newcommand{\x}{\bm{x}}
\newcommand{\dthetai}[1]{\ensuremath{\frac{\partial #1}{\partial {\theta_i^{(l)}}}}}
\newcommand{\dthetaj}[1]{\ensuremath{\frac{\partial #1}{\partial {\theta_j^{(l)}}}}}
\newcommand{\dthetaij}[1]{\ensuremath{\frac{\partial^2 #1}{\partial{\theta_j^{(l)}} \partial {\theta_i^{(l)}}}}}
\newcommand{\As}{{\mathcal{A}}}
\newcommand{\A}{\bm{A}}
\DeclareMathOperator{\conf}{conf}
\DeclareMathOperator{\Tr}{Tr}
\DeclareMathOperator{\Err}{Err}
\DeclareMathOperator{\diff}{diff}
\DeclareMathOperator{\vect}{vect}

% algorithm
\newcommand{\algorithmicinput}{\textbf{input}}
\newcommand{\algorithmicoutput}{\textbf{output}}
\newcommand{\algorithmicin}{\textbf{in }}
\newcommand{\INPUT}{\item[\algorithmicinput]}
\newcommand{\OUTPUT}{\item[\algorithmicoutput]}
\newcommand{\IN}{\algorithmicin}


% \renewcommand{\thefootnote}{\Alph{footnote}}

%%%% Review
\newcommand{\changecolor}{redOrange}
\newcommand{\change}[1]{{\color{\changecolor} {#1}}}
\newcommand{\remove}[1]{{\color{\changecolor} {\sout{#1}}}}
\newcommand{\removeeq}[1]{{\color{\changecolor} {\cancel{#1}}}} 
 
% \title{High-dimensional efficient global optimization {\color{black}of expensive-to-evaluate blackboxes} using both random and supervised embeddings}
\title{High-dimensional {\color{black}Bayesian} optimization using both random and linear embeddings}


\author{R\'emy Priem\footnote{Artificial Intelligence Engineer for Aeronautical Systems, Technical Unit, remy.priem@intradef.gouv.fr}}
\affil{DGA Maîtrise de l'Information, Bruz, France}
\author{Youssef Diouane\footnote{Professor, Mathematics and Industrial Engineering Department, youssef.diouane@polymtl.ca, AIAA MDO TC Member.}}
\affil{GERAD \& Polytechnique Montr\'eal, Montreal, QC, Canada}
\author{Nathalie Bartoli\footnote{Senior researcher, Information Processing and Systems Department, nathalie.bartoli@onera.fr, AIAA MDO TC Member.}, Sylvain Dubreuil\footnote{Researcher, Information Processing and Systems Department, sylvain.dubreuil@onera.fr, AIAA Member.}, Paul Saves\footnote{Post-doctoral fellow, Information Processing and Systems Department, paul.saves@onera.fr, Student AIAA Member.}} 
\affil{DTIS, ONERA, Université de Toulouse, Toulouse, France}
\affil{Fédération ENAC ISAE-SUPAERO ONERA, Université de Toulouse, 31000, Toulouse, France}


%\author{Sylvain Dubreuil\footnote{Researcher, Information Processing and Systems Department, sylvain.dubreuil@onera.fr, AIAA Member.}}
%\affil{ONERA, DTIS, Université de Toulouse, Toulouse, France}
%\author{Paul Saves\footnote{PhD Student, Information Processing and Systems Department, paul.saves@onera.fr, Student AIAA Member.}}
%\affil{ONERA, DTIS, Université de Toulouse, Toulouse, France}
%\affil{ISAE-SUPAERO, Université de Toulouse, Toulouse, 31055 Cedex 4, France}

\begin{document}

 
\maketitle

\begin{abstract}
Bayesian optimization (BO) is one of the most powerful strategies to solve {\color{black}computationally expensive-to-evaluate} blackbox optimization problems.
However, BO methods are conventionally used for optimization problems of small dimension because of the curse of dimensionality.
In this paper, to solve high dimensional optimization problems, we propose to incorporate linear embedding subspaces of small dimension to efficiently perform the optimization. An adaptive learning strategy for these linear embeddings is carried out in conjunction with the optimization.
The resulting BO method, named EGORSE, combines in an adaptive way both random and supervised linear embeddings.
EGORSE has been compared to state-of-the-art algorithms and tested on academic examples with a number of design variables ranging from 10 to 600.
The obtained results show the high potential of EGORSE to solve high-dimensional blackbox optimization problems, both in terms of CPU time and {\color{black}{limited}} number of calls to the expensive blackbox {\color{black}{simulation}}.
\end{abstract}

\section{Nomenclature}

{\renewcommand\arraystretch{1.0}
\noindent\begin{longtable*}{@{}l @{\quad=\quad} l@{}}
$\Omega$ & design space \\
$d$ & dimension of the design space \\
$f$ & objective function \\
$\bm{x}$ & vector of design variables\\
$\mu$ & prior mean function \\
$k$ & covariance kernel\\
$\mathcal{D}^{(l)}$ & design of experiments of $l$ sampled points \\
$y$ & output of the objective function \\
$\mathcal{N}$ & Gaussian distribution \\
$\hat\sigma^{(l)}$ & variance function of a Gaussian process conditioned by $l$ sampled points \\
$\hat\mu^{(l)}$ & mean function of a Gaussian process conditioned by $l$ sampled points \\
$\bm{\theta}^{(l)}$ & hyper-parameters of a Gaussian process conditioned by $l$ sampled points \\
$\alpha^{(l)}$ & acquisition function \\
$y_{min}^{(l)}$ & minimum output of the objective function in a set of $l$ sampled points\\
$f_{\mathcal{A}}$ & objective function reduced in the linear subspace $\mathcal{A}$ \\
$\bm{A}$ & transfer matrix \\
$\mathcal{A}$ & linear subspace \\
$\mathcal{B}$ & hyper-cube in the linear subspace \\
$T$ & number of dimension reduction methods \\
$\mathcal{R}$ & dimension reduction method \\
$\gamma$ & reverse application \\
$\bm{u}$ & vector of design variables in the linear subspace \\
$g$ & constraint in the linear subspace \\
\end{longtable*}}

\input{Intro}

\input{review}

\input{HDBO}

\input{tests}

\input{conclu}

%\input{appendix}


\section*{Acknowledgments}
This work is part of the activities of ONERA - ISAE - ENAC joint research group.
The authors would like to express their special thanks to Nina Moello for her test on the implementation of the log-likelihood gradient and Hessian in the opensource Python toolbox SMT\footnote{https://github.com/SMTorg/smt} and for her implementation of the HESBO algorithm in our computer environment.

\bibliography{main}

\end{document}
