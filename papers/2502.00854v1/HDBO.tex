\section{Supervised linear embeddings for BO}
\label{sec:EGFORSE}
\subsection{Method description}
\label{ssec:outline}

In this paper, the objective function is supposed to depend only on the effective dimensions 
%meaning 
$d_e \ll d$ where we typically assume that it exists a function $f_{\As}: \mathbb{R}^{d_e} \mapsto \mathbb{R}$ such as $f_{\As}(\A\x) = f(\x)$ with $\A \in \mathbb{R}^{d_e \times d}$, $\As = \left\{ \bm{u} = \A\x \ ; \ \forall \x \in \Omega \right\}$ and\ $\Omega = [-1,1]^d$~\cite{WangBatchedHighdimensionalBayesian2018,binoisChoiceLowdimensionalDomain2020}.
The idea is then to perform the optimization procedure in the reduced linear subspace $\As$ so that the number of hyper-parameters to estimate and dimension of the design space are reduced to $d_e$ instead of  $d$.
This allows to build the inexpensive GPs and will ease the acquisition function optimization.
Using a subspace (based on $\A$) for the optimization requires finding the effective dimension of the reduced design space $\mathcal{B} \subset \mathbb{R}^{d_e}$ as well as the backward application $\gamma: \mathcal{B} \mapsto \Omega$.

The proposed method focuses on the definition of the optimization problem when a linear subspace is used as well as on efficient construction procedure of such embedding subspaces.  
Most existing HDBO methods rely on random linear subspaces  meaning that no information is used to incorporate a priori information from the optimization problem within the embedding space which may slow down the optimization process. 
In this work, a recursive search, with $T\in \mathbb{N}$ supervised reduction dimension methods, is performed to find supervised linear subspace so that the most important search directions for the exploration of the objective function are included. 
%E. g., 
Using an initial DoE, one can use a \textit{Partial Least Squares} (PLS) regression \cite{hellandStructurePartialLeast1988} to build such linear embedding prior to the optimization process.
Furthermore, the new search design of the optimization problem (within the linear embedding subspace) is a necessary step.
Most methods rely on a classic optimization problem formulation that may limit the process performance due to very restricted new design space.
Here, once an appropriate linear subspace is found, the optimization problem is turned into a constrained optimization problem to limit the computational cost of the algorithm. The constrained optimization problem can be solved using a classical CBO method~\cite{frazierTutorialBayesianOptimization2018,priemOptimisationBayesienneSous2020,ShahriariTakingHumanOut2016,bartoliAdaptiveModelingStrategy2019,priemUpperTrustBound2020c}.
%algorithm named \textit{Super Efficient Global Optimization with Mixture Of Experts} (SEGOMOE)~\cite{bartoliAdaptiveModelingStrategy2019,priemUpperTrustBound2020c}. 

\subsection{Definition of the reduced search spaces}

To define the optimization problem in the low dimensional spaces, transfer matrices from the initial to the low dimensional spaces and optimization domains must be defined. 
The definition of these transfer matrices relies on a set of $T\in\mathbb{N}$ dimension reduction methods.
For each of the $T$ dimension reduction methods $\mathcal{R}^{(t)}$, the transfer matrix $\A^{(t)} \in \mathbb{R}^{d_e \times d}$ is build using $\mathcal{R}^{(t)}$ where $t \in \{1,\ldots,T\}$.
In this way, we propose to use supervised dimension reduction algorithms, like the PLS~\cite{hellandStructurePartialLeast1988}, to guide the optimization process through highly varying linear subspaces of the objective function.
This procedure allows to tackle the issue of~\citet{binoisChoiceLowdimensionalDomain2020,WangBatchedHighdimensionalBayesian2018} by relying on random linear subspaces defined with random Gaussian transfer matrices.
In their works, the optimization can be performed in a subspace in which the objective function is not varying, meaning the optimum of the objective function could not be discovered.
In the BO framework, the optimization process is usually performed in an hypercube. 
However $\As^{(t)}$ is not an hypercube.
As $\Omega = [-1,1]^d$, it is possible to compute $\mathcal{B}^{(t)} \subset \mathbb{R}^{d_e}$~\cite{binoisChoiceLowdimensionalDomain2020} the smallest hypercube containing all points of $\As^{(t)}$ such as 
\begin{equation}
    \mathcal{B}^{(t)} = \left[-\sum\limits_{i=1}^d\left| A^{(t)}_{1,i} \right|, \sum\limits_{i=1}^d\left| A^{(t)}_{1,i} \right| \right] \times \cdots \times \left[-\sum\limits_{i=1}^d\left| A^{(t)}_{d_e,i} \right|, \sum\limits_{i=1}^d\left| A^{(t)}_{d_e,i} \right| \right].
    \label{eq:borne_sub}
\end{equation}
Performing the optimization process in $\mathcal{B}^{(t)}$ leads to define a backward application from $\mathcal{B}^{(t)}$ to $\Omega$ to compute the objective function on the desired point. 
% Thus, no point $\x \in \Omega$, solution of the problem $$\x=\left[\A^{(t)}\right]^{+}\bm{u}$$ 
% with {$\left[\A^{(t)}\right]^{+} = \left[\A^{(t)}\right]^\top \left[ \A^{(t)} \left[\A^{(t)}\right]^\top \right]^{-1}$} the pseudo-inverse of $\A^{(t)}$, for all $\bm{u} \in \mathbb{R}^{d_e}$, is forgotten in the optimization process.
% In fact, $\As^{(t)} \subset \mathcal{B}^{(t)}$.
% However, $\left[\A^{(t)}\right]^{+}$ implies that some points $\bm{u} \in \mathcal{B}^{(t)}$ can not provide $\x = \left[\A^{(t)}\right]^{+} \bm{u}$ with $\x \in \Omega$.
% This property is actually used to consider objective function values on the closer edge of the $\mathcal{B}^{(t)}$, image by $\left[\A^{(t)}\right]^{+}$ in $\Omega$.
% This transformation, introduced in the following, is realized thanks to a backward application from $\mathcal{B}^{(t)}$ to $\Omega$.

\subsection{The backward application}
\label{ssec:reverse}

We use the backward application introduced by~\cite{binoisChoiceLowdimensionalDomain2020}. Namely, a bijective application $\gamma_B^{(t)} : \As^{(t)} \subset \mathbb{R}^d_e \mapsto \Omega \subset \mathbb{R}^d$ such that
\begin{equation}
    \gamma_B^{(t)}(\bm{u}) = \arg \min\limits_{\x \in \Omega} \left\{ \left\| \x - \left[\A^{(t)}\right]^{+} \bm{u} \right\|^2, \ \text{s.c. } \ \A^{(t)}\x = \bm{u}  \right\},
    \label{eq:quad_prob}
\end{equation}
where {$\left[\A^{(t)}\right]^{+} = \left[\A^{(t)}\right]^\top \left[ \A^{(t)} \left[\A^{(t)}\right]^\top \right]^{-1}$} the pseudo-inverse of $\A^{(t)}$. 
This problem requires to solve a quadratic optimization problem.
As $\As^{(t)} \subset \mathcal{B}^{(t)}$, $\gamma_B^{(t)}$ defines an injection from $\mathcal{B}^{(t)}$ to $\Omega$, meaning some points of $\mathcal{B}^{(t)}$ do not have any image in $\Omega$.
For instance, Figure~\ref{fig:backpropa} displays $\Omega \subset \mathbb{R}^{10}$ projected in a domain $\mathcal{B}^{(t)} \subset \mathbb{R}^2$.
The $\As^{(t)}$ domain is 
%produced 
in white while the points in the black domain do not have any image in $\Omega$ by $\gamma_B^{(t)}$.
\begin{figure}[ht!]
    \centering
    \includegraphics[width=0.5\textwidth]{Figures/emb/Back_proj.pdf}
    \caption{Illustration of $\mathcal{B}^{(t)}$ and $\As^{(t)}$. In black, the points of $\mathcal{B}^{(t)}$ without image in $\Omega$ by $\gamma_B^{(t)}$; in white, the points of $\As^{(t)}$ corresponding to the points of $\mathcal{B}^{(t)}$ with an image in $\Omega$ by $\gamma_B^{(t)}$.}
    \label{fig:backpropa}
\end{figure}

The method developed by~\cite{binoisChoiceLowdimensionalDomain2020} is using the $\gamma_B^{(t)}$ even if such backward application is not defined all over $\mathcal{B}^{(t)}$ by itself.
In fact, the function $f^{(t)}(\bm{u})=f\left(\gamma_B^{(t)}(\bm{u})\right)$ is only defined on $\As^{(t)}$, meaning the optimization problem in the linear subspace is given by:
\begin{equation}
    \min_{\bm{u}\in\As^{(t)}}\left\{f^{(t)}(\bm{u})\right\}.
    \label{eq:rrembo_pb}
\end{equation}
However, standard BO algorithms are only solving optimization problems with an objective function defined on an hypercube. \citet{binoisChoiceLowdimensionalDomain2020} used  $\mathcal{B}^{(t)}$, see~\eqref{eq:borne_sub}, the smallest hypercube containing $\As^{(t)}$.
This way, some points reachable by the BO algorithm might not be evaluated on the objective function as they do not have any image in $\Omega$ using $\gamma_B^{(t)}$.
To fix this issue, an additional modification of the EI acquisition function is thus introduced in \cite{binoisChoiceLowdimensionalDomain2020}. In fact, over $\As^{(t)}$, the acquisition function $\alpha^{(l)}_{EI_{ext}}(\bm{u}) = \alpha^{(l)}_{EI}(\bm{u})$~\eqref{eq:EI} while on $\mathcal{B}^{(t)} \backslash \As^{(t)}$, the acquisition function $\alpha^{(l)}_{EI_{ext}}(\bm{u}) = -\| \bm{u} \|_2$.
As the EI acquisition function is positive (i.e., lower bounded), the optimization of the acquisition function provides a point of $\As^{(t)}$. This quadratic problem~\eqref{eq:quad_prob} must be solved at each acquisition function evaluation to know if $\bm{u}$ belongs to $\As^{(t)}$.
Depending on the number of evaluations of the $\alpha^{(l)}_{EI_{ext}}$ acquisition function, an optimization process performed with the RREMBO algorithm can be expensive in CPU time.
Actually the cost in CPU time grows with the number of design variables $d$.
Figure~\ref{fig:proj_pb:proj} shows the objective function of 10 design variables projected in a linear subspace of 2 dimension.
\begin{figure}[!hbt]
    \centering
    \subfloat[Projected objective function. \label{fig:proj_pb:proj}]{\includegraphics[width=0.5\textwidth]{Figures/emb/REMBO_Iter.png}}
    \subfloat[${\alpha^{(l)}_{EI_{ext}}}$. \label{fig:proj_pb:ei}]{\includegraphics[width=0.5\textwidth]{Figures/emb/REMBO_Iner.png}}
    \caption{Projection of an objective function of 10 design variables into a linear subspace of 2 dimensions and the corresponding ${\alpha^{(l)}_{EI_{ext}}}$ acquisition function. The grey area is the unfeasible domain, the green squares are DoE points of $\As^{(t)}$, the red squares are DoE points of $\mathcal{B}^{(t)} \backslash \As^{(t)}$ and the green star is a solution of the optimization sub-problem.}
    \label{fig:proj_pb}
\end{figure}
The grey area shows that an important part of $\mathcal{B}^{(t)}$ does not have any image in $\Omega$. 
In Figure~\ref{fig:proj_pb:ei}, one can see the grey domain is replaced by negative values increasing towards the center of $\mathcal{B}^{(t)}$ allowing convergence to points having an image by $\gamma_B^{(t)}$ in $\Omega$.

\subsection{The optimization problem}
\label{ssec:sub_prob}

Section~\ref{ssec:reverse} shows that the backward application $\gamma_B^{(t)}$ is bijective from $\As^{(t)}$ to $\Omega$ and injective from $\mathcal{B}^{(t)}$ to $\Omega$. 
Thus some points of $\mathcal{B}^{(t)}$ do not have image by $\gamma_B^{(t)}$ in $\Omega$. 
To avoid points of $\mathcal{B}^{(t)} \backslash \As^{(t)}$, we define an optimization problem with a constraint which is feasible on $\As^{(t)}$ and unfeasible on $\mathcal{B}^{(t)} \backslash \As^{(t)}$.
For the value of the constraint when $\bm{u} \not\in \As^{(t)}$, we chose the opposite of the 2-norm of $\bm{u}$ .
The constraint value is negative and tends to zero when $\bm{u}$ is getting closer to $\As^{(t)}$ in norm.
To define the constraint value in the feasible zone, ones can rely on~\eqref{eq:borne_sub} and on $\Omega = [-1,1]^d$.
The equation shows that points on the edge of $\mathcal{B}^{(t)}$, having an image in $\Omega$ by $\gamma_B^{(t)}$, are in the corners of $\Omega$. 
The corners of a domain are considered to be the points whose components are $-1$ or $1$.
However, the corners of the domain are the farthest points from the center $x_c = \bm{0} \in \mathbb{R}^d$ and have the same norm. 
All the images from points of $\mathcal{B}^{(t)}$ by $\gamma_B^{(t)}$ have a lower norm than the corners of $\Omega$. 
Hence the constraint of the optimization problem is defined as the difference between the norm of the corners $\Omega$ and the norm of $\gamma_{B}^{(t)}(\bm{u})$ when $\bm{u} \in \As^{(t)}$.
Thus, the constraint value tends to zero when $\bm{u}$ is getting closer to $\mathcal{B}^{(t)} \backslash \As^{(t)}$.
Eventually, the constraint function is normalized to provide values in $[-1,1]$.
This normalization on the bounds of the constraint function balances the importance of the feasible and unfeasible domain in the optimization process.

To summarize, the constraint is given by $g^{(t)}(\bm{u}) \geq 0$ where:
\begin{equation}
    g^{(t)}(\bm{u}) = \left\{
    \begin{tabular}{ll}
        $1 - \frac{\left\| \gamma_B^{(t)}\left(\bm{u}\right)\right\|^2_2}{d}$ &  if $\bm{u} \in \mathcal{A}^{(t)}$\\
        $-\left\|\bm{u}^{\A^{(t)}}\right\|^2_2$ &  otherwise
    \end{tabular}
    \right.
    \label{eq:it_cst}
\end{equation}
with $u^{\A^{(t)}}_i = u_i / \sum\limits_{j=1}^d\left| A^{(t)}_{i,j} \right|$.
The terms $u^{\A^{(t)}}_i$ and $u_i$ are respectively the $i^{\text{th}}$ components of the $\bm{u}^{\A^{(t)}}$ and $\bm{u}$ vectors.
One remarks that $f^{(t)}(\bm{u}) = f\left(\gamma_B^{(t)}(\bm{u})\right)$ function is not defined all over $\mathcal{B}^{(t)}$. 
That is why one seeks to give a value to $f^{(t)}$ on $\mathcal{B}^{(t)} \backslash \As^{(t)}$ to obtain a function not only defined on $\mathcal{B}^{(t)}$.
The extension of $f^{(t)}$ on $\mathcal{B}^{(t)} \backslash \As^{(t)}$ is given by $f^{(t)}(\bm{u}) = f\left(\gamma_W^{(t)}(\bm{u})\right)$ with  
\begin{equation}
     \gamma_W^{(t)}(\bm{u}) \in \arg \min\limits_{\x \in \Omega} \| \x - \left[\A^{(t)}\right]^{+} \bm{u} \|^2.
    \label{eq:quad_prob_wang}
\end{equation}
Indeed, the $\gamma_W^{(t)}$ application exists for all points $\bm{u} \in \mathcal{B}^{(t)}$ although it can provide the same $\x \in \Omega$ for different $\bm{u} \in \mathcal{B}^{(t)}$.
These points of $\Omega$ are moreover reachable with points of $\As^{(t)}$ using the $\gamma_B^{(t)}$ backward application.
Thus, one of the objective function minima is necessarily in $\As^{(t)}$.
Eventually, the objective function $f^{(t)}$ is given on $\mathcal{B}^{(t)}$ by 
\begin{equation}
    f^{(t)}(\bm{u}) = \left\{
    \begin{tabular}{ll}
        $f\left(\gamma_B^{(t)}\left(\bm{u}\right)\right)$    &  if $\bm{u} \in \mathcal{A}^{(t)}$\\
        $f\left(\gamma_W^{(t)}\left(\bm{u}\right)\right)$  &  otherwise
    \end{tabular}
    \right.
    \label{eq:it_obj}
\end{equation}
In fact, one solves the following constrained optimization problem in the $\mathcal{B}^{(t)}$ hypercube:
\begin{equation}
    \min\limits_{\bm{u} \in \mathcal{B}^{(t)}} \left\{ f^{(t)}(\bm{u}) \quad \text{s.c.} \quad g^{(t)}(\bm{u}) \geq 0 \right\}.
    \label{eq:it_pb}
\end{equation}
where $f^{(t)}(\bm{u})$ and $g^{(t)}(\bm{u})$ are respectively defined by Eq.~\eqref{eq:it_obj} and Eq.~\eqref{eq:it_cst}.
To solve this problem, a standard CBO algorithm~\cite{frazierTutorialBayesianOptimization2018,priemOptimisationBayesienneSous2020,ShahriariTakingHumanOut2016}, like SEGOMOE~\cite{bartoliAdaptiveModelingStrategy2019,priemUpperTrustBound2020c} can be used.
Figure~\ref{fig:it_pb:pb} shows the optimization problem~\eqref{eq:it_pb} for the $10$ design variable function given in Figure~\ref{fig:proj_pb:proj} and projected in a 2 dimensional linear subspace. 
\begin{figure}[!htb]
    \centering
    \subfloat[Optimization problem \eqref{eq:it_pb}. \label{fig:it_pb:pb}]{\includegraphics[width=0.5\textwidth]{Figures/EGORSE_Iter.png}}
    \subfloat[SEGOMOE optimization sub-problem associated to problem~\eqref{eq:it_pb}. \label{fig:it_pb:spb}]{\includegraphics[width=0.5\textwidth]{Figures/EGORSE_Iner.png}}
    \caption{An optimization problem~\eqref{eq:it_pb} in a 2 dimensional linear subspace and the associated SEGOMOE optimization sub-problem. The grey area is the unfeasible domain, the green squares are DoE points of $\As^{(t)}$, the red squares are DoE points of $\mathcal{B}^{(t)} \backslash \As^{(t)}$ and the green star is a solution of the optimization sub-problem.}
    \label{fig:it_pb}
\end{figure}
One sees that the grey unfeasible area corresponds to the grey area of Figure~\ref{fig:proj_pb:proj}.
Figure~\ref{fig:it_pb:spb} shows the optimization sub-problem of the SEGOMOE algorithm using the EI acquisition function where the green and red squares are points of the initial DoE.
The predicted unfeasible zone, in grey in Figure~\ref{fig:it_pb:spb}, contains the unfeasible points of the initial DoE. 
Recall, with SEGOMOE, only the mean of the GP modeling the constraint is used to defined the feasible zones.
On the contrary of RREMBO no quadratic problem solving is needed to solve the optimization sub-problem. 
In fact, the introduced process avoids the quadratic problem solving in the optimization sub-problem by defining functions existing all over $\mathcal{B}^{(t)}$, which is not the case of RREMBO (see Section~\ref{ssec:reverse}).
The quadratic problem is only solved when the objective function is called at each iteration of the SEGOMOE algorithm.
Thus, the introduced EGORSE algorithm should be faster in CPU time than RREMBO.

\subsection{Adaptive learning of the linear subspace}

To ease the convergence process, we use a linear subspace discovered by supervised dimension reduction methods, i.e. taking points of the considered domain and the associated function values as inputs.
In our case, linear dimension reduction methods are used to provide the transfer matrix $\A^{(t)} \in \mathbb{R}^{d \times d_e}$ from $\Omega$ to $\As^{(t)}$.
 In order to find the minimum of the objective function, we solve the optimization problem~\eqref{eq:it_pb} with a CBO algorithm~\cite{frazierTutorialBayesianOptimization2018,ShahriariTakingHumanOut2016,priemOptimisationBayesienneSous2020} in $\mathcal{B}^{(t)}$ generated by the $\A^{(t)}$ matrix with a maximum of \texttt{max\_nb\_it\_sub} iterations.
However, in larger space, the linear subspace approximation can lack of accuracy if the number of points used by the dimension reduction methods is not sufficient.
To improve the generated subspace, the previous process is iterated using all the evaluated points during the previous iterations.
Note that points outside of the subspace $\As^{(t)}$ are added to provide information which is non biased by the subspace selection.
In that way, points coming from an optimization performed in a subspace defined by other dimension reduction methods can be used.
For instance, a transfer matrix defined by a random Gaussian distribution is chosen. 
The convergence properties, given by~\citet{binoisChoiceLowdimensionalDomain2020}, are thus preserved. 
Finally, we 
%propose to 
generalize this approach by using several dimension reduction methods, which can be unsupervised (like random Gaussian transfer matrices or hash tables~\cite{binoisChoiceLowdimensionalDomain2020,nayebiFrameworkBayesianOptimization2019}), or supervised (like PLS or MGP~\cite{hellandStructurePartialLeast1988,GarnettActiveLearningLinear2014}), to consider their advantages. 
The overall process of EGORSE is finally given by Algorithm~\ref{alg:EGORSE} whereas the flow chart of the method is described by the \textit{eXtended Design Structure Matrix} (XDSM) \cite{lambeExtensionsDesignStructure2012} diagram of Figure~\ref{fig:xdsm}.
\begin{algorithm}[!hbtp]
     \begin{algorithmic}[1]
        \INPUT{: an objective function $f$, an initial {DoE} $\mathcal{D}_f^{(0)}$, a maximum number of iterations \mbox{max\_nb\_it}, a maximum number of iterations by subspace max\_nb\_it\_sub, a number of active directions $d_e$, a list $\mathcal{R} = \left\{\mathcal{R}^{(1)},\ldots,\mathcal{R}^{(T)}\right\}$ of $T \in \mathbb{N}^+$ supervised or unsupervised dimension reduction methods\;}
        \FOR{$i = 0$ \TO \mbox{max\_nb\_it} - 1}
            \FOR{$t = 1$ \TO $T$}
                \STATE {Build $\A^{(t)} \in \mathbb{R}^{d_e \times d}$ with $\mathcal{R}^{(t)}$ using or not $\mathcal{D}_f^{(i)}$}
                \STATE {Define $\mathcal{B}^{(t)}$ (see Eq.~\eqref{eq:borne_sub})}
                \STATE {Build $f^{(t)}$ (see Eq.~\eqref{eq:it_obj})}
                \STATE {Build $g^{(t)}$ (seeEq.~\eqref{eq:it_cst})}
                \STATE {Solve the optimization problem
                $\min\limits_{\bm{u} \in \mathcal{B}^{(t)}} \left\{ f^{(t)}(\bm{u}) \quad \text{s.c.} \quad g^{(t)}(\bm{u}) \geq 0 \right\}$
                with a CBO algorithm using \mbox{max\_nb\_it\_sub} maximum iterations.}
            \ENDFOR
            \STATE {$\mathcal{D}_f^{(i+1)} = \mathcal{D}_f^{(i)} \cup \{\text{Points already evaluated on $f$ at iteration $i$}\}$}
        \ENDFOR
        \OUTPUT{: The best point regarding the value of $f$ in  $\mathcal{D}_f^{(\text{max\_nb\_it})}$\;}
    \end{algorithmic}
    \caption{The EGORSE process.}
    \label{alg:EGORSE}
\end{algorithm}
\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{Figures/EGORSE_XDSM_v1.pdf}
    \caption{An XDSM diagram of the EGORSE framework.}
    \label{fig:xdsm}
\end{figure}
To conclude, the EGORSE algorithm includes three main contributions:
\begin{itemize}
    \item The restriction of the number of quadratic problems solved with a new formulation of the subspace optimization problem.
    \item The use of supervised dimension reduction methods promoting the search in highly varying directions of the objective function in $\Omega$.
    \item The adaptive learning of the linear subspace using all the evaluated points.
\end{itemize}

{\color{black}
\subsection{Efficient supervised embeddings}

Building an efficient supervised embedding (i.e., the transfer matrix $A^{(t)}$) plays a key role in the EGORSE framework. In this work, we consider two well-known supervised embedding techniques: the \textit{Partial Least-Squares (PLS)}~\cite{hellandStructurePartialLeast1988} and the \textit{Marginal Gaussian Process (MGP)}~\cite{GarnettActiveLearningLinear2014} based methods. In this section, we recall briefly the main features of the PLS and MGP methods. To simplify the notations, we will omit the iteration indices $(t)$ in this section; we will refer to the transfer matrix using within EGORSE by $A$ and to the current DoE by $\mathcal{D}_f$. 
These two embeddings PLS and MGP coupled with GP are available in the SMT toolbox~\cite{bouhlelPythonSurrogateModeling2019,SMT2023}\footnote{\href{https://github.com/SMTorg/smt}{https://github.com/SMTorg/smt}}.


\paragraph{On the PLS transfer matrix ( $\A_{\mbox{PLS}}$).}
The PLS~\cite{hellandStructurePartialLeast1988} method searches for the most important $d_e$ orthonormal directions $\bm{a}_{(i)} \in \mathbb{R}^d$, $i \in \{1,\ldots,d_e\}$ of $\Omega$ in regards of the influence of (related to the DoE $\mathcal{D}_f$) the inputs \mbox{$\bm{X}^{(l)}_{(0)} = \left[\left[\x^{(0)}\right]^\top,\ldots,\left[\x^{(l)}\right]^\top \right]^\top \in \mathbb{R}^{d \times l}$} on the outputs $\bm{Y}^{(l)}_{f,(0)} = \bm{Y}^{(l)}_f \in \mathbb{R}^l$.
These directions are recursively given by: 
\begin{gather*}
	\bm{a}'_{(i)} \in \arg \max_{\bm{a}' \in \mathbb{R}^d} \left\{ \bm{a}'^\top \left[\bm{X}^{(l)}_{(i)}\right]^\top \bm{Y}^{(l)}_{f,(i)} \left[\bm{Y}^{(l)}_{s,(i)}\right]^\top \bm{X}^{(l)}_{(i)} \bm{a}' \ , \ \bm{a}'^\top \bm{a}' = 1 \right\}, \label{eq:kpls_opt}\\
	\bm{t}_{(i)} = \bm{X}^{(l)}_{(i)} \bm{a}'_{(i)}, \quad
	\bm{p}_{(i)} = \left[\bm{X}^{(l)}_{(i)}\right]^\top \bm{t}_{(i)} , \quad c_{(i)} = \left[\bm{Y}^{(l)}_{s,(i)}\right]^\top \bm{t}_{(i)} \\ \bm{X}^{(l)}_{(i+1)} = \bm{X}^{(l)}_{(i)} - \bm{t}_{(i)} \bm{p}_{(i)}, \quad
	\bm{Y}^{(l)}_{f,(i+1)} = \bm{Y}^{(l)}_{f,(i)} - c_{(i)} \bm{t}_{(i)}.
 %\\ \A' = [\bm{a}'_{(1)},\ldots,\bm{a}'_{(d_e)}], \quad \bm{P} = [\bm{p}'_{(1)},\ldots,\bm{p}'_{(d_e)}], \quad
	%\A = \A' (\bm{P}^\top \A')^{-1},
\end{gather*}
where $\bm{X}^{(l)}_{(i+1)} \in \mathbb{R}^{d \times l}$ and $\bm{Y}^{(l)}_{f,(i+1)} \in \mathbb{R}^d$ are the residuals of the projection of $\bm{X}^{(l)}_{(i)}$ and $\bm{Y}^{(l)}_{f,(i)}$ on the $\text{i}^{\text{th}}$ principal component $\bm{t}_{(i)} \in \mathbb{R}^d$.
Finally, $\bm{p}_{(i)} \in \mathbb{R}^d$ and $c_{(i)} \in \mathbb{R}$ are respectively the regression coefficients of $\bm{X}^{(l)}_{(i)}$ and $\bm{Y}^{(l)}_{s,(i)}$ on the $\text{i}^{\text{th}}$ principal component $\bm{t}_{(i)}$ for $i \in \{1,\ldots,d_e\}$.
In fact, the square of the covariance between $\bm{a}'_{(i)}$ and $\left[\bm{X}^{(l)}_{(i)}\right]^\top \bm{Y}^{(l)}_{f,(i)}$ is recursively maximized. In this case, the PLS transfer matrix is given by:
$$
\A_{\mbox{PLS}} = \A' (\bm{P}^\top \A')^{-1},
$$
where $\A' = [\bm{a}'_{(1)},\ldots,\bm{a}'_{(d_e)}] $ and $
	\bm{P} = [\bm{p}'_{(1)},\ldots,\bm{p}'_{(d_e)}]$. 


\paragraph{On the MGP transfer matrix ($\A_{\mbox{MGP}}$).}
With the MGP~\cite{GarnettActiveLearningLinear2014}, the matrix is considered as the realization of a Gaussian distribution \mbox{$\mathbb{P}(A) = \mathcal{N}\left(\A_{\mbox{MGP}}, \bm{\Sigma}_{\mbox{MGP}} \right)$} where $\A_{\mbox{MGP}}$ and $\bm{\Sigma}_{\mbox{MGP}}$ are respectively the prior mean and the covariance matrix.
The density function of $\mathbb{P}(A)$ is noted $p(\A)$.  Then, the transfer matrix $\A_{\mbox{MGP}}$ used when referring to the MGP dimension reduction method is given as the maximum of the  posterior probability law of $\A$ with respect to the DoE $\mathcal{D}_f$, i.e.,
$$
\A_{\mbox{MGP}} \in \arg \max_{A}  p\left(\A | \mathcal{D}_f\right):= p(\A) \cdot \mathcal{L}\left(\bm{Y}_f,\A\right),
$$
 where  $\mathbb{P}\left(\A | \mathcal{D}_f\right)$ represents the density function of $\mathbb{P}\left(\A | \mathcal{D}_f\right)$ and $\mathcal{L}$ the likelihood. The covariance matrix $\bm{\Sigma}_{\mbox{MGP}}$ is estimated by the inverse of the logarithm of $p\left(\A | \mathcal{D}_f\right)$ Hessian  matrix evaluated at $\A_{\mbox{MGP}}$
\begin{equation*}
            \hat{\bm{\Sigma}}^{-1}_{\mbox{MGP}}  = - \nabla^2 \log p\left(\A_{\mbox{MGP}} | \mathcal{D}_f\right),
\end{equation*}
where $\nabla^2$ is the Hessian operator with respect to $\A$. The remaining details on the MGP can be found in~\cite{GarnettActiveLearningLinear2014}.
}
