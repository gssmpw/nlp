\section{A sensitivity analysis over \texttt{EGORSE} hyper-parameters}
\label{sec:sensitivity-anal}
\subsection{Implementation details}

\texttt{EGORSE} method is implemented with Python 3.8. A CBO process (see Section~\ref{ssec:sub_prob}) is performed at each iteration of the \texttt{EGORSE} algorithm
using SEGO~\cite{SasenaExplorationmetamodelingsampling2002}  (from the SEGOMOE toolbox~\cite{bartoliAdaptiveModelingStrategy2019}).
The SEGOMOE toolbox uses the SMT~\cite{bouhlelPythonSurrogateModeling2019}, a Python package used to build the GP model.
In the CBO process, the EI acquisition function is optimized in two steps. 
First, a good starting point is found by solving the sub-optimization problem~\eqref{eq:iner_opt} with the ISRES~\cite{runarsson2005search} from the NLOPT~\cite{johnson2014nlopt} Python toolbox.
ISRES is an evolutionary optimization algorithm able to solve multi-modal optimization problems with equality and inequality constraints.
The algorithm explores the domain to find an optimal area maximizing the acquisition function and respecting the feasibility criteria.
However, such algorithm needs many function evaluations to converge.
To limit this number of evaluations, the solution provided by ISRES is refined with a gradient based optimization algorithm as the analytical derivatives of the acquisition function and the feasibility criteria are easily available from the GP approximations and provided as outputs from SMT~\cite{bouhlelPythonSurrogateModeling2019}.
The gradient based algorithm used is SNOPT~\cite{Gillsnopt2005} from the PyOptSparse~\cite{Perezpyopt2012} Python toolbox whose initial guess is given by ISRES.
To solve the quadratic problem~\eqref{eq:quad_prob}, the CVXOPT~\cite{andersen2013cvxopt} Python toolbox is chosen.
A point $\bm{u}\in\mathcal{B}_B^{(t)}$ is considered to belong to $\As^{(t)}$ if the CVXOPT optimization status '\verb'optimal'' is reached meaning that the quadratic problem has a solution.

\subsection{On the setting of \texttt{EGORSE} hyper-parameters}
\label{sssec:hyperparam}

\texttt{EGORSE} is controlled by a set of hyper-parameters. To select the value of these hyper-parameters, a parametric study is performed on two optimization problems.
The hyper-parameters considered in this study are the following.
\begin{itemize}
    \item \textbf{The number of points in the initial DoE}. It impacts the supervised dimension reduction methods. On the overall \texttt{EGORSE} versions, three sizes of initial DoE are tested: 5 points, $d$ points and $2d$ points where $d$ is the number of design variables.
    \item \textbf{The supervised dimension reduction method.} It changes the behavior of the algorithm favoring different directions of the domain.
    The PLS~\cite{hellandStructurePartialLeast1988} and MGP~\cite{gardnerDiscoveringExploitingAdditive2017} methods are considered in this study. 
    The PLS method is coming from the scikit-learn~\cite{scikit-learn} Python toolbox. 
    The MGP method is implemented within SMT~\cite{bouhlelPythonSurrogateModeling2019}.
    \item \textbf{The unsupervised dimension reduction method} relies on a random Gaussian transfer matrix like in~\citet{binoisChoiceLowdimensionalDomain2020} work or on hash table like in~\citet{nayebiFrameworkBayesianOptimization2019} work.
\end{itemize}
In what comes next, the following 6 possible variants of \texttt{EGORSE} will be tested and compared:
\begin{enumerate}
    \item \texttt{EGORSE Gaussian}: it uses Gaussian random transfer matrix.
    \item  \texttt{EGORSE Hash}: it uses random matrix defined by Hash table.
    \item  \texttt{EGORSE PLS}: it uses transfer matrix defined by the PLS method.
    \item  \texttt{EGORSE PLS+Gaussian}: it uses Gaussian random transfer matrix and transfer matrix defined by the PLS.
    \item  \texttt{EGORSE MGP}: it uses transfer matrix defined by the MGP method.
    \item  \texttt{EGORSE MGP+Gaussian}: it uses Gaussian random transfer matrix and transfer matrix defined by the MGP.
\end{enumerate}
\subsection{Study on two analytical problems}
\subsubsection{Definition of the two problems}

The considered class of problems is an adjustment of the Modified Branin (MB) problem~\cite{ParrInfillsamplingcriteria2012} whose number of design variables is artificially increased.
This problem is commonly used in the literature~\cite{binoisChoiceLowdimensionalDomain2020,WangBayesianoptimizationbillion2016,nayebiFrameworkBayesianOptimization2019} and it is  defined as follows:
\begin{equation}
    \min_{\bm{u} \in \Omega_1}{f_1(\bm{u})},
\end{equation}
where $\Omega_1 = [-5,10] \times [0,15]$ and
\begin{equation}
    f_1(\bm{u}) = \left[ \left( u_2 - \frac{5.1u_1^2}{4\pi^2} + \frac{5u_1}{\pi} - 6 \right)^2 +\left( 10 - \frac{10}{8\pi} \right) \cos{(u_1) + 1} \right] + \frac{5u_1 + 25}{15}.
\end{equation}
The modified version of the Branin problem is selected because it count three local minima including a global one.
The value of the global optimum is about ${\text{MB\_$d$}}_{min} = 1.1$.
Furthermore, the problem is normalized to have $\bm{u} \in [-1,1]^2$.
To artificially increase the number of design variables, a random matrix $\A_d \in \mathbb{R}^{2 \times d}$ is generated such that for all $\x \in [-1,1]^d$, $\A_d \x = \bm{u}$ belongs to $[-1,1]^2$.
An objective function MB\_$d$, where $d$ is the number of design variables, is defined such that $\text{MB\_$d$}(\x) = f_1(\A_d\x)$.
Eventually, we solve the following optimization problem:
\begin{equation}
    \min\limits_{x \in [-1,1]^d} \text{MB\_$d$}(\x) = \min\limits_{x \in [-1,1]^d} f_1(\A_d\x).
\end{equation}
In the following numerical experiments are conducted on two functions of respective dimension 10 and 100. These two test functions are denoted $\text{MB\_$10$}$ and $\text{MB\_$100$}$.

\subsubsection{Convergence plots}
\label{sssec:conv}

To study the \texttt{EGORSE} hyper-parameter impact, convergence plots are obtained for both problems.
Ten independent optimizations are thus performed on each of the problems, for each of the \texttt{EGORSE} versions, using 10 initial DoE.
The number of iterations is imposed to 10 and the number of evaluations by sub-space optimization is set to $20d_e$ and $d_e=2$. 
For  \texttt{EGORSE PLS, \texttt{EGORSE} MGP, \texttt{EGORSE} Hash and \texttt{EGORSE} Gaussian}, the number of iterations is doubled to keep a fixed number of evaluations, i.e. 800 evaluations per run.
Indeed, at each iteration, the problem is evaluated only $20d_e$ for \texttt{EGORSE} composed of a unique dimension reduction method against $2\times20d_e$ for \texttt{EGORSE} composed of two dimension reduction methods.  
All of the \texttt{EGORSE} versions are then compared displaying the evolution of the mean and standard deviation on the $10$ optimizations of the lower value of the objective function evaluated with respect to the number of evaluations. 
For readability, the standard deviation is displayed with a reduction factor of four. 

\subsubsection{Results analysis}
\label{sssec:impact}

In this section, the convergence robustness and speed of the six \texttt{EGORSE} versions are analyzed with convergence plots (see Section~\ref{sssec:conv}). {\textcolor{black}{Reaching the global minimum (value of 1) of the modified Branin optimization problem in high dimension~\cite{ParrInfillsamplingcriteria2012}  is very hard specially with a limited budget of function evaluations, so the the main purpose here is to compare the convergence speed and the quality of the best value found by  each method.}} 
The convergence plots of the considered \texttt{EGORSE} versions for the MB\_10 problems are displayed in Figure~\ref{fig:EGORSE_MB10}.
\begin{figure}[!htbp]
    \centering
    \subfloat[DoE of 5 points.\label{fig:EGORSE_MB10_5}]{\includegraphics[width=0.50\textwidth]{Figures/CV/CV_MB_10_EGORSE_5.pdf}}
    \subfloat[DoE of 10 points.\label{fig:EGORSE_MB10_d}]{\includegraphics[width=0.50\textwidth]{Figures/CV/CV_MB_10_EGORSE_D.pdf}} \\
    \subfloat[DoE of 20 points.\label{fig:EGORSE_MB10_2d}]{\includegraphics[width=0.50\textwidth]{Figures/CV/CV_MB_10_EGORSE_2D.pdf}}
    \subfloat[Best versions for each DoE size. \label{fig:EGORSE_MB10_best}]{\includegraphics[width=0.50\textwidth]{Figures/CV/CV_MB_10_EGORSE_ALL.pdf}}
    \caption{Convergence plots of 6 versions of \texttt{EGORSE} applied on the MB\_10 problem. The grey vertical line shows the size of the initial DoE.}
    \label{fig:EGORSE_MB10}
\end{figure}
In Figure~\ref{fig:EGORSE_MB10_5}, one can see that the \texttt{EGORSE Gaussian} algorithm offers the best performance in term of convergence speed and robustness for an initial DoE of 5 points. 
However, Figures~\ref{fig:EGORSE_MB10_d} and~\ref{fig:EGORSE_MB10_2d} show that \texttt{EGORSE PLS+Gaussian} outperforms \texttt{EGORSE Gaussian} in term of convergence speed and robustness for larger initial DoE. 
Figure~\ref{fig:EGORSE_MB10_best} finally compares these different versions and they are hardly distinguishable for the MB\_10 problem. 
Thus, all the six \texttt{EGORSE} algorithms seem to provide equivalent performance for low dimensional problems.
Figure~\ref{fig:EGORSE_MB100} displays the convergence plots of the six \texttt{EGORSE} versions applied to the MB\_100 problem.
\begin{figure}[!htbp]
    \centering
    \subfloat[DoE of 5 points.\label{fig:EGORSE_MB100_5}]{\includegraphics[width=0.50\textwidth]{Figures/CV/CV_MB_100_EGORSE_5.pdf}}
    \subfloat[DoE of $d$ points.\label{fig:EGORSE_MB100_d}]{\includegraphics[width=0.50\textwidth]{Figures/CV/CV_MB_100_EGORSE_D.pdf}} \\
    \subfloat[DoE of $2d$ points.\label{fig:EGORSE_MB100_2d}]{\includegraphics[width=0.50\textwidth]{Figures/CV/CV_MB_100_EGORSE_2D.pdf}}
    \subfloat[Best versions for each DoE size. \label{fig:EGORSE_MB100_best}]{\includegraphics[width=0.50\textwidth]{Figures/CV/CV_MB_100_EGORSE_ALL.pdf}}
    \caption{Convergence plots of 6 versions of \texttt{EGORSE} applied on the MB\_100 problem. The grey vertical line shows the size of the initial DoE.}
    \label{fig:EGORSE_MB100}
\end{figure}
One can see that the different \texttt{EGORSE} versions are much more distinguishable on Figure~\ref{fig:EGORSE_MB100}. 
In fact, Figures~\ref{fig:EGORSE_MB100_5},~\ref{fig:EGORSE_MB100_d} and~\ref{fig:EGORSE_MB100_2d} show that \texttt{EGORSE PLS+Gaussian} provides the best convergence speed and robustness trade-off for all the initial DoE sizes tested.
To select the best number of initial DoE points, Figure~\ref{fig:EGORSE_MB100_best} displays the convergence plots of  \texttt{EGORSE PLS+Gaussian} for the three tested number of points in the initial DoE.
One can easily see that  \texttt{EGORSE PLS+Gaussian} with an initial DoE of $d$ points  provides the best performance in term of convergence speed and robustness.
in terms of evaluations number, the use of an initial DoE of $d$ points allows the algorithm to explore the domain in an interesting direction more quickly than a $2d$ points initial DoE.  
On the contrary, using an initial DoE of 5 points forces the algorithm to seek for the best direction for a long time.

To conclude, choosing the PLS and Gaussian dimension reduction methods with an initial DoE of $d$ points seems the most suitable to obtain the best performance of \texttt{EGORSE}.
Nevertheless, \texttt{EGORSE} capabilities have to be compared with HDBO algorithms to validate its usefulness as it is proposed in the following.

\section{Comparison with state-of-the-art HDBO methods}
\label{sec:num}
\subsection{BO algorithms and setup details}
\label{ssec:setup_all}

\texttt{EGORSE} is now compared to the following state-of-the-art algorithms:
\begin{itemize}
    \item \texttt{TuRBO}~\cite{erikssonScalableGlobalOptimization2019a}: a HDBO algorithm using trust regions to favor the exploitation of the DoE data.
    Tests are performed with the TuRBO\footnote{https://github.com/uber-research/TuRBO}~\cite{erikssonScalableGlobalOptimization2019a} Python toolbox.
    \item   \texttt{EGO-KPLS}~\cite{BouhlelEfficientglobaloptimization2018}: an HDBO method relying on the reduction of the number of GP hyper-parameters.
    This allows to speed up the GP building. 
    The SEGOMOE~\cite{bartoliAdaptiveModelingStrategy2019} Python toolbox is used.
    All the hyper-parameters of this algorithm are the default ones.
    The number of principal components for the KPLS model is set to two.
    \item  \texttt{RREMBO}~\cite{binoisChoiceLowdimensionalDomain2020}: a HDBO method using the random Gaussian transfer matrix to reduce the number of dimensions of the optimization problem through random embedding.
     \texttt{RREMBO}\footnote{https://github.com/mbinois/RRembo} implementation of this algorithm is used.
    The parameters are also set by default.
    \item   \texttt{HESBO}~\cite{nayebiFrameworkBayesianOptimization2019}: a HDBO algorithm using Hash tables to generate the transfer matrix and construct so called hashing-enhanced subspaces. 
    We use the \texttt{HESBO}\footnote{https://github.com/aminnayebi/HesBO} Python toolbox with the default parameters.
\end{itemize}
For \texttt{EGORSE}, the version showing the best performance in term of convergence speed and robustness in Section~\ref{sssec:impact} is selected, i.e.  \texttt{{\texttt{EGORSE}} PLS + Gaussian} with an initial DoE of $d$ points.
To achieve this comparison, $10$ optimizations for each problem and  for each studied method are completed to analyze the statistical behavior of these BO algorithms.
Because of the different strategies implemented in the previously introduced algorithms, a specific test plan must be adopted for each of them.
\begin{itemize}
    \item \texttt{EGORSE}: The test plan of Section~\ref{sssec:conv} is implemented.
    \item  \texttt{TuRBO}: Five trust regions are used with a maximum of $800$ evaluations of the objective function. Note that it is not possible to provide the same initial DoE used for \texttt{EGORSE} in  \texttt{TuRBO}. The number of points generated at the beginning of the algorithm is thus imposed to $d$. The EI acquisition function is chosen.
     \item  \texttt{EGO-KPLS}: The optimization is performed with a maximum evaluation number of $800$, with the initial DoE used in \texttt{EGORSE} and with the EI acquisition function. 
     \item  \texttt{RREMBO \& HESBO}: $20$ optimizations of $20d_e$ evaluations are performed for each of the initial DoE for  \texttt{RREMBO} and  \texttt{HESBO}. These $20$ optimizations are then concatenated and considered as a unique optimization process. The number of effective directions are imposed to $d_e=2$ and the acquisition function is EI.  \texttt{RREMBO} and  \texttt{HESBO} are equivalent to \texttt{EGORSE} without supervised dimension reduction method.
\end{itemize}
\subsection{Results analysis}
\label{ssec:bo_mb:res}

In this section, a comparison between \texttt{EGORSE} and the four studied algorithms is performed in term of robustness, convergence speed both in CPU time and in number of iterations.
Figure~\ref{fig:ALL} provides the iteration convergence plots, as introduced in Section~\ref{sssec:conv}, and the time convergence plots drawing the evolution of the means of the best discovered function values against the CPU time. 
\begin{figure}[!hbtp]
    \centering
    \subfloat[MB\_10 iteration convergence plot. \label{fig:ALL:mb10_nb}]{\includegraphics[width=0.50\textwidth]{Figures/CV/CV_MB_10_ALL_NBCall.pdf}}
    \subfloat[MB\_100 iteration convergence plot. \label{fig:ALL:mb100_nb}]{\includegraphics[width=0.50\textwidth]{Figures/CV/CV_MB_100_ALL_NBCall.pdf}} \\
    \subfloat[MB\_10 time convergence plot. \label{fig:ALL:mb10_time}]{\includegraphics[width=0.50\textwidth]{Figures/CV/CV_MB_10_ALL_Time.pdf}}
    \subfloat[MB\_100 time convergence plot. \label{fig:ALL:mb100_time}]{\includegraphics[width=0.50\textwidth]{Figures/CV/CV_MB_100_ALL_Time.pdf}}
    \caption{Iteration and time convergence plots for 5 HDBO algorithms on the MB\_10 and MB\_100 problems. The grey vertical line shows the size of the initial DoE.}
    \label{fig:ALL}
\end{figure}

Figure~\ref{fig:ALL:mb10_nb} shows that  \texttt{TuRBO} and  \texttt{EGO-KPLS} are converging the fastest and with a low standard deviation.
Moreover, the convergence plots of  \texttt{EGORSE},  \texttt{RREMBO} and  \texttt{HESBO} are hardly distinguishable. 
Figure~\ref{fig:ALL:mb100_nb} displays that  \texttt{EGO-KPLS} converges the fastest to the lowest values with a low standard deviation.
 \texttt{TuRBO} is also providing good performance even if it converges slower than  \texttt{EGO-KPLS}. 
Regarding the three methods using dimension reduction procedure,  \texttt{EGORSE} is converging to the lowest value with a relatively low standard deviation.
The good performance of  \texttt{EGO-KPLS} and  \texttt{TuRBO} is certainly due to the ability of these algorithms to search all over $\Omega$, which is not the case for other methods.
However, when the dimension of $\Omega$ increases, the ability to search all over $\Omega$ becomes a drawback. In fact, a complete search in $\Omega$ is intractable in time is this case.

Figures~\ref{fig:ALL:mb10_time} and~\ref{fig:ALL:mb100_time} depict the convergence CPU time necessary to obtain the regarded value.
First, the RREMBO, TuRBO and EGO-KPLS complete the optimization procedure in more than 8 hours on the MB\_100 problem against an hour on the MB\_10 problem.
This suggests that  \texttt{RREMBO},  \texttt{TuRBO} and  \texttt{EGO-KPLS} are intractable in time for larger problems.
Then, one can easily see that \texttt{EGORSE} is converging the fastest in CPU time than the other algorithms on the MB\_100 problem.
In fact, the computation time needed to find the enrichment point is much lower than the one for  \texttt{TuRBO},  \texttt{EGO-KPLS} and  \texttt{RREMBO}.
This was sought in the definition of the enrichment sub-problem introduced in Section~\ref{ssec:sub_prob}.
Finally,  \texttt{EGORSE} is converging to a lower value than  \texttt{HESBO} in a similar amount of time. 
Thus,  \texttt{EGORSE} seems more interesting to solve HDBO problems than the studied algorithms.
Note that only  \texttt{HESBO} and  \texttt{EGORSE} are able to perform an optimization procedure on HDBO problems.

To conclude this section, several sets of hyper-parameters of  \texttt{EGORSE} have been tested on two problems of dimension 10 and 100. 
The analysis of the obtained results has shown that  \texttt{EGORSE PLS+Gaussian} with an initial DoE of $d$ points is performing the best. 
A comparison of  \texttt{EGORSE} with HDBO algorithms has also been carried out. 
It has pointed out that  \texttt{TuRBO},  \texttt{EGO-KPLS} and  \texttt{RREMBO} are intractable in time for HDBO problems.
Furthermore,  \texttt{EGORSE} has appeared to be the most suitable to solve HDBO problem efficiently. 

\section{Evaluation of \texttt{EGORSE} on a high dimensional planning optimization}
\label{sec:rover}

In this section, the \texttt{EGORSE} algorithm is evaluated to find an optimal path planning problem using 600 design optimization variables. 
\subsection{Problem definition and implementation details}

The Rover\_600 path planning problems relies on the same idea than MB\_d problems except that the objective function is a adjustment of the Rover\_60~\cite{WangBatchedHighdimensionalBayesian2018} problem.
It consists on a robot routing from a starting point $x_{start}$ to a goal point $x_{goal}$ in a forest. 
The robot trajectory is a spline defined by 30 control points. 
These points, including the starting and the goal ones, are the design variables of the optimization problem that belong to $\Omega=[0,1]^{60}$.
The objective function is minimal when the robot follows the shortest trajectory without meeting a tree.
The minimum of the function is $f_{min} = -5$.
Figure~\ref{fig:rover_dom} gives an example of trajectory.
\begin{figure}[!htp]
    \centering
    \includegraphics[width=0.8\textwidth]{Figures/rover_domain.pdf}
    \caption{Example of a robot trajectory in a forest.}
    \label{fig:rover_dom}
\end{figure}

To increase the number of design variables, the problem is normalized in $\Omega=[-1,1]^{60}$, a random matrix $\A_d \in \mathbb{R}^{60  \times 600}$ is generated such that all $\x \in [-1,1]^d$, $\A_d \x = \bm{u} \in [-1,1]^{60}$.
An objective function Rover\_600, where $d=600$ is the number of design variables, is defined such that $\text{Rover\_600}(\x) = \text{Rover\_60}(\A_d\x)$.
Eventually, we solve the following optimization problem:
\begin{equation}
    \min\limits_{x \in [-1,1]^600} \text{Rover\_600}(\x) = \min\limits_{x \in [-1,1]^{60}} \text{Rover\_60}(\A_d\x).
\end{equation}

%\subsection{BO algorithms and setup details}
%\label{sssec:detail_Rover}
We note that, in our tests, \texttt{TuRBO},  \texttt{EGO-KPLS} and  \texttt{RREMBO} algorithms are not included anymore in this comparison as they are intractable in time for optimization problems with more than a hundred design variables (see Section~\ref{ssec:bo_mb:res}). So, only \texttt{EGORSE PLS+Gaussian} is compared to \texttt{HESBO}. The same test plan as in Section~\ref{ssec:setup_all} is used with $200$ optimizations (instead of $20$) of $20d_e$ evaluations.

\subsection{Result analysis}
Time and iteration convergence plots of  \texttt{EGORSE} and  \texttt{HESBO} are depicted in Figure~\ref{fig:Rover}.
\begin{figure}[!hbt]
    \centering
    \subfloat[Time convergence plot. \label{fig:Rover:Time}]{\includegraphics[width=0.50\textwidth]{Figures/CV/CV_Rover_600_ALL_Time.pdf}}
    \subfloat[Iteration convergence plot. \label{fig:Rover:nbCall}]{\includegraphics[width=0.50\textwidth]{Figures/CV/CV_Rover_600_ALL_NBCall.pdf}}
    \caption{CPU time convergence plots of  \texttt{HESBO, RREMBO, TuRBO, EGO-KPSL} on the Rover\_600 problem with an initial DoE of $d$ points. The grey vertical line shows the size of the initial DoE.}
    \label{fig:Rover}
\end{figure}
Figure~\ref{fig:Rover:nbCall} clearly shows that  \texttt{EGORSE} is converging fast to the lowest objective value with  a very low standard deviation. 
However, the obtained objective value is larger than the known optimal one (i.e. $f_{min}=-5$).
This is due to two main reasons that may be addressed in further research:
\begin{itemize}
    \item The number of effective directions used in  \texttt{EGORSE} (i.e. $d_e=2$) is much lower than the actual number of effective directions (i.e. $d_e=60$). The effective search space is not covering the space in which Rover\_600 is varying.
    \item The dimension reduction method PLS is global. The local variations of the function, in which the global optimum can be located, are thus deleted. Even if this problem is tackled by searching in randomly generated subspace,  \texttt{EGORSE} cannot provide better results. 
\end{itemize}
Figure~\ref{fig:Rover:Time} shows that  \texttt{HESBO} is performing the optimization procedure faster than  \texttt{EGORSE}.
In fact,  \texttt{HESBO} does not solve any quadratic problem at each iteration.
However, one can see that the time difference is not significant.