\section{Introduction}

Conditional generative models are of significant importance for many scientific and industrial applications. 
Of these, the class of flow-based models and score-based diffusion models has recently shown a particularly impressive performance \citep{lipman2022flow, esser2024scaling, dhariwal2021diffusion, ho2022classifier}.  
Although impressive, current methods suffer from long training and sampling times. 
To this end, in this work, we tap into a non-utilized property of conditional flow-based models: the ability to design a non-trivial prior distribution for conditional flow-based models based on the input condition. In particular, for class-conditional generation and text-to-image generation, we propose a \emph{robust} method for constructing a conditional flow-based generative model using an informative
condition-specific prior distribution fitted to the conditional modes (e.g., classes) of the target distribution. 
By better modeling the 
prior distribution, 
we aim to improve the efficiency, both at training and at inference, of conditional generation via flow matching, thus achieving high quality results with fewer sampling steps. 



Given an input variable (e.g., a class or text prompt), current flow-based and score-based diffusion models 
combine the input condition with intermediate representations in a learnable manner. 
However, crucially, these models are still trained to transform a generic unimodal noise distribution to the different modes of the target data distribution.
In some formulations, such as score-based diffusion \citep{ho2020denoising, song2020score, sohl2015deep}, the use of a Gaussian source density is intrinsically connected to the process constructing the transformation. In others, such as flow matching \citep{lipman2022flow, liu2022flow, albergo2022building}, a Gaussian source is not required, but is often chosen as a default for convenience.
Consequently, in these settings, the prior distribution bears little or no resemblance to the target, and hence every point in the initial source distribution can be mapped to every point in every mode in the target distribution, corresponding to a given condition. This means that the average distance between pairs of source-target points is fairly large. %

In the unconditional setting, recent works~\citep{pooladian2023multisample, tong2023improving}, show that starting from a source (noise) data point that is close to the target data sample, during training, results in straighter probability flow lines, fewer sampling steps at test time, and faster training time. This is in comparison to the non-specific random pairing between the distributions typically used for training flow-based and score-based models. 
This suggests that finding a strategy to 
minimize the average distance between source and target points could result in a similar benefit. Our work aims to construct this by constructing a \emph{condition-specific} source distribution by leveraging the input condition. 

\input{figures/teaser/teaser}
We, therefore, propose a novel paradigm for designing an informative condition-specific prior distribution for a flow-based conditional generative model. While in this work, we choose to work on \emph{flow matching}, our approach can also be incorporated in other generative models, supporting arbitrary prior distributions.
In the first step, we embed the input condition $c$ to a point $x_c$ lying in data space (which can be a latent one). 
For a discrete set of classes, this is done by averaging training samples corresponding to a given class $c$ in the data space. In the continuous case, such as text-to-image, we first choose a meaningful embedding for the input condition $c$ (\emph{e.g.} CLIP~\cite{radford2021learning}).
Given a training sample $x_c$ and the corresponding conditional embedding $e_c$, we train a deterministic mapper function 
that projects $e_c$ to $x_c$ lying in data space. This results in an ``average" data point of all samples $x$ corresponding to the condition $c$. 
To enable stochastic mapping, we then map samples from a 
parametric conditional distribution centered on $x_c$ to the conditional target distribution $\rho_1(x|c)$. 

While our approach can be implemented with any parametric conditional distribution, in our experiments, we chose to utilize a Gaussian Mixture Model (GMM). Specifically, the mean of each Gaussian is the ``average" conditional data point. In the discrete condition case, each prior-Gaussian's covariance is estimated directly from the class-dependent training data, while for the continuous setting, it is fixed as a hyperparameter. 
Further, while the data space can be arbitrary, we choose it as the latent space of a pre-trained variational autoencoder (VAE). These choices are derived from the following desirable properties: (i). 
One can easily sample from a GMM, (ii). Class conditional information can be directly represented by a GMM, with each Gaussian corresponding to a conditional mode.  (iii). We find empirically (see Sec.~\ref{sec:prior_distribution}), for real-world distributions (ImageNet~\cite{deng2009imagenet} and MS-COCO~\cite{lin2014microsoft}), that the average distance between pairs of samples from the prior and data distributions (\emph{i.e.} the \emph{transport cost}) is much smaller than the unimodal Gaussian alternative (as in \cite{lipman2022flow, pooladian2023multisample}). Moreover, previous works~\cite{jiang2017variationaldeepembeddingunsupervised, Bull_2021, hinton2006reducing} has shown that applying a GMM in a VAE space can be highly effective for clustering, suggesting that it can act as a suitable prior distribution. 
An illustration of our approach, for a simple setting consisting of eight Gaussians, each representing a different class, is shown in Fig.~\ref{fig:teaser}.


To validate our approach, we first formulate flow matching from our conditional prior distribution (CPD) and show that our formulation results in low global truncation errors.
Next, we consider a toy setting with a known analytical target distribution and illustrate our method's advantage in efficiency and quality. 
For real-world datasets, we consider both the MS-COCO (text-to-image generation) and ImageNet-64 datasets (class conditioned generation). Compared to other flow-based (CondOT~\cite{lipman2022flow}, BatchOT~\cite{pooladian2023multisample}) and diffusion (DDPM~\cite{Ho2020DDPM}) based models, our approach allows for faster training and sampling, as well as for a significantly improved generated image quality and diversity, evaluated using FID and KID, and alignment to the input text, evaluated using CLIP score. 
