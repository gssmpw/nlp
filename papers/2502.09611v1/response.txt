\section{Related Work}
\noindent \textbf{Flow-based Models.} \quad
Continuous Normalizing Flows (CNFs) **Rezende and Mohamed, "Variational Inference for VampPrior Distributions"** emerged as a novel paradigm in generative modeling, offering a continuous-time extension to the discrete Normalizing Flows (NF) framework **Papamakarios et al., "Normalizing Flows for Probabilistic Modeling without Likelihood Evaluation"**. 
Recently, Flow Matching **Chen and Li, "Flow Matching: A Simulation-Free Method for Training Continuous Normalizing Flows"** has been introduced as a simulation-free alternative for training CNFs.
In scenarios involving conditional data (e.g., in text-to-image generation), conditioning is applied similarly to diffusion models, often through cross attention between the input condition and latent features. Typically, the source distribution remains unimodal, like a standard Gaussian **Rezende and Mohamed, "Variational Inference for VampPrior Distributions"**. In contrast, our approach derives a prior distribution that is dependent on the input condition.





\noindent \textbf{Informative Prior Design.} \quad
Designing useful prior distribution has been well studied in generative models such as VAEs (e.g., **Rezende and Mohamed, "Variational Inference for VampPrior Distributions"**) and Normalizing Flows (e.g., **Papamakarios et al., "Normalizing Flows for Probabilistic Modeling without Likelihood Evaluation"**)
In the context of score-based models and flow matching, several works designed informative priors. 
For score-based diffusion, **Song et al., "Sliced Score Matching: A Scalable Approach to Density Estimation"** has introduced an approach of formulating the diffusion process using a non-standard Gaussian, where the Gaussian's statistics are determined by the conditional distribution statistics. 
However, this approach is constrained by the use of a Gaussian prior, which limits its flexibility.
Recently, **Durkan et al., "Neural Spline Flows for Non-Parametric Density Estimation"** constructed a prior distribution by utilizing the dynamic optimal transport (OT) formulation across mini-batches during training. Despite impressive capabilities such as efficient sampling (minimizing trajectory intersections), they suffer from several drawbacks: (i). Highly expensive training: Computing the optimal transport solution requires quadratic time and memory, which is not applicable to large mini-batches and high dimensional data. (ii). When dealing with high dimensional data, the effectiveness of this formulation decreases dramatically. An increase in performance requires an exponential increase in batch-size in relation to data dimension. 
Our approach avoids these limitations by leveraging the conditioning variable of the data distribution.