@article{lipman2022flow,
  title={Flow matching for generative modeling},
  author={Lipman, Yaron and Chen, Ricky TQ and Ben-Hamu, Heli and Nickel, Maximilian and Le, Matt},
  journal={arXiv preprint arXiv:2210.02747},
  year={2022}
}

@article{liu2022flow,
  title={Flow straight and fast: Learning to generate and transfer data with rectified flow},
  author={Liu, Xingchao and Gong, Chengyue and Liu, Qiang},
  journal={arXiv preprint arXiv:2209.03003},
  year={2022}
}

@article{albergo2022building,
  title={Building normalizing flows with stochastic interpolants},
  author={Albergo, Michael S and Vanden-Eijnden, Eric},
  journal={arXiv preprint arXiv:2209.15571},
  year={2022}
}

@article{lee2021priorgrad,
  title={Priorgrad: Improving conditional denoising diffusion models with data-dependent adaptive prior},
  author={Lee, Sang-gil and Kim, Heeseung and Shin, Chaehun and Tan, Xu and Liu, Chang and Meng, Qi and Qin, Tao and Chen, Wei and Yoon, Sungroh and Liu, Tie-Yan},
  journal={arXiv preprint arXiv:2106.06406},
  year={2021}
}

@article{albergo2023stochastic,
  title={Stochastic interpolants with data-dependent couplings},
  author={Albergo, Michael S and Goldstein, Mark and Boffi, Nicholas M and Ranganath, Rajesh and Vanden-Eijnden, Eric},
  journal={arXiv preprint arXiv:2310.03725},
  year={2023}
}

@article{tong2023improving,
  title={Improving and generalizing flow-based generative models with minibatch optimal transport},
  author={Tong, Alexander and Malkin, Nikolay and Huguet, Guillaume and Zhang, Yanlei and Rector-Brooks, Jarrid and Fatras, Kilian and Wolf, Guy and Bengio, Yoshua},
  journal={arXiv preprint arXiv:2302.00482},
  year={2023}
}

@article{pooladian2023multisample,
  title={Multisample flow matching: Straightening flows with minibatch couplings},
  author={Pooladian, Aram-Alexandre and Ben-Hamu, Heli and Domingo-Enrich, Carles and Amos, Brandon and Lipman, Yaron and Chen, Ricky TQ},
  journal={arXiv preprint arXiv:2304.14772},
  year={2023}
}

@inproceedings{sohl2015deep,
  title={Deep unsupervised learning using nonequilibrium thermodynamics},
  author={Sohl-Dickstein, Jascha and Weiss, Eric and Maheswaranathan, Niru and Ganguli, Surya},
  booktitle={International conference on machine learning},
  pages={2256--2265},
  year={2015},
  organization={PMLR}
}

@article{ho2020denoising,
  title={Denoising diffusion probabilistic models},
  author={Ho, Jonathan and Jain, Ajay and Abbeel, Pieter},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={6840--6851},
  year={2020}
}

@article{lin2014microsoft,
  title={Microsoft COCO: Common objects in context},
  author={Lin, Tsung-Yi and Maire, Michael and Belongie, Serge and Hays, James and Perona, Pietro and Ramanan, Deva and Doll{\'a}r, Piotr and Zitnick, C Lawrence},
  journal={European conference on computer vision},
  pages={740--755},
  year={2014},
  publisher={Springer}
}

@inproceedings{deng2009imagenet,
  title={ImageNet: A large-scale hierarchical image database},
  author={Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
  booktitle={2009 IEEE conference on computer vision and pattern recognition},
  pages={248--255},
  year={2009},
  organization={Ieee}
}

@article{song2020score,
  title={Score-based generative modeling through stochastic differential equations},
  author={Song, Yang and Sohl-Dickstein, Jascha and Kingma, Diederik P and Kumar, Abhishek and Ermon, Stefano and Poole, Ben},
  journal={arXiv preprint arXiv:2011.13456},
  year={2020}
}

@inproceedings{esser2024scaling,
  title={Scaling rectified flow transformers for high-resolution image synthesis},
  author={Esser, Patrick and Kulal, Sumith and Blattmann, Andreas and Entezari, Rahim and M{\"u}ller, Jonas and Saini, Harry and Levi, Yam and Lorenz, Dominik and Sauer, Axel and Boesel, Frederic and others},
  booktitle={Forty-first International Conference on Machine Learning},
  year={2024}
}

@article{dhariwal2021diffusion,
  title={Diffusion models beat gans on image synthesis},
  author={Dhariwal, Prafulla and Nichol, Alexander},
  journal={Advances in neural information processing systems},
  volume={34},
  pages={8780--8794},
  year={2021}
}

@article{ho2022classifier,
  title={Classifier-free diffusion guidance},
  author={Ho, Jonathan and Salimans, Tim},
  journal={arXiv preprint arXiv:2207.12598},
  year={2022}
}

@misc{chen2019neural,
      title={Neural Ordinary Differential Equations}, 
      author={Ricky T. Q. Chen and Yulia Rubanova and Jesse Bettencourt and David Duvenaud},
      year={2019},
      eprint={1806.07366},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{kobyzev2020normalizing,
  title={Normalizing flows: An introduction and review of current methods},
  author={Kobyzev, Ivan and Prince, Simon JD and Brubaker, Marcus A},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  volume={43},
  number={11},
  pages={3964--3979},
  year={2020},
  publisher={IEEE}
}

@article{papamakarios2021normalizing,
  title={Normalizing flows for probabilistic modeling and inference},
  author={Papamakarios, George and Nalisnick, Eric and Rezende, Danilo Jimenez and Mohamed, Shakir and Lakshminarayanan, Balaji},
  journal={Journal of Machine Learning Research},
  volume={22},
  number={57},
  pages={1--64},
  year={2021}
}

@article{McCann1997ACP,
  title={A Convexity Principle for Interacting Gases},
  author={Robert J. McCann},
  journal={Advances in Mathematics},
  year={1997},
  volume={128},
  pages={153-179},
  url={https://api.semanticscholar.org/CorpusID:123005604}
}

@misc{mirza2014conditionalgenerativeadversarialnets,
      title={Conditional Generative Adversarial Nets}, 
      author={Mehdi Mirza and Simon Osindero},
      year={2014},
      eprint={1411.1784},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1411.1784}, 
}

@article{sohn2015learning,
  title={Learning structured output representation using deep conditional generative models},
  author={Sohn, Kihyuk and Lee, Honglak and Yan, Xinchen},
  journal={Advances in neural information processing systems},
  volume={28},
  year={2015}
}

@misc{isobe2024extendedflowmatchingmethod,
      title={Extended Flow Matching: a Method of Conditional Generation with Generalized Continuity Equation}, 
      author={Noboru Isobe and Masanori Koyama and Jinzhe Zhang and Kohei Hayashi and Kenji Fukumizu},
      year={2024},
      eprint={2402.18839},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2402.18839}, 
}

@misc{dao2023flowmatchinglatentspace,
      title={Flow Matching in Latent Space}, 
      author={Quan Dao and Hao Phung and Binh Nguyen and Anh Tran},
      year={2023},
      eprint={2307.08698},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2307.08698}, 
}

@misc{zheng2023guidedflowsgenerativemodeling,
      title={Guided Flows for Generative Modeling and Decision Making}, 
      author={Qinqing Zheng and Matt Le and Neta Shaul and Yaron Lipman and Aditya Grover and Ricky T. Q. Chen},
      year={2023},
      eprint={2311.13443},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2311.13443}, 
}

@misc{atanackovic2024metaflowmatchingintegrating,
      title={Meta Flow Matching: Integrating Vector Fields on the Wasserstein Manifold}, 
      author={Lazar Atanackovic and Xi Zhang and Brandon Amos and Mathieu Blanchette and Leo J. Lee and Yoshua Bengio and Alexander Tong and Kirill Neklyudov},
      year={2024},
      eprint={2408.14608},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2408.14608}, 
}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% RAANAN THREW STUFF HERE %%%%%%%%%%%%%%%%


@article{ho2022video,
title={Video diffusion models},
author={Ho, Jonathan and Salimans, Tim and Gritsenko, Alexey and Chan, William and Norouzi, Mohammad and Fleet, David J},
journal={arXiv:2204.03458},
year={2022}}
}

@article{Tong2024mini,
title={Improving and generalizing flow-based generative models with minibatch optimal transport},
author={Alexander Tong and Kilian FATRAS and Nikolay Malkin and Guillaume Huguet and Yanlei Zhang and Jarrid Rector-Brooks and Guy Wolf and Yoshua Bengio},
journal={Transactions on Machine Learning Research},
issn={2835-8856},
year={2024}
}

@InProceedings{Johnson2016Ploss,
author="Johnson, Justin
and Alahi, Alexandre
and Fei-Fei, Li",
editor="Leibe, Bastian
and Matas, Jiri
and Sebe, Nicu
and Welling, Max",
title="Perceptual Losses for Real-Time Style Transfer and Super-Resolution",
booktitle="Computer Vision -- ECCV 2016",
year="2016",
publisher="Springer International Publishing",
address="Cham",
pages="694--711",
abstract="We consider image transformation problems, where an input image is transformed into an output image. Recent methods for such problems typically train feed-forward convolutional neural networks using a per-pixel loss between the output and ground-truth images. Parallel work has shown that high-quality images can be generated by defining and optimizing perceptual loss functions based on high-level features extracted from pretrained networks. We combine the benefits of both approaches, and propose the use of perceptual loss functions for training feed-forward networks for image transformation tasks. We show results on image style transfer, where a feed-forward network is trained to solve the optimization problem proposed by Gatys et al.Â in real-time. Compared to the optimization-based method, our network gives similar qualitative results but is three orders of magnitude faster. We also experiment with single-image super-resolution, where replacing a per-pixel loss with a perceptual loss gives visually pleasing results.",
isbn="978-3-319-46475-6"
}



@inproceedings{Tolstikhin2018WAE,
  added-at = {2019-04-04T00:00:00.000+0200},
  author = {Tolstikhin, Ilya O. and Bousquet, Olivier and Gelly, Sylvain and Schölkopf, Bernhard},
  biburl = {https://www.bibsonomy.org/bibtex/2fb987f0c663734450f09b7d754179b65/dblp},
  booktitle = {ICLR},
  crossref = {conf/iclr/2018},
  ee = {https://openreview.net/forum?id=HkL7n1-0b},
  interhash = {9c4d07a9ac1c4a8faca996469be9fab1},
  intrahash = {fb987f0c663734450f09b7d754179b65},
  keywords = {dblp},
  publisher = {OpenReview.net},
  timestamp = {2019-04-05T11:38:59.000+0200},
  title = {Wasserstein Auto-Encoders.},
  url = {http://dblp.uni-trier.de/db/conf/iclr/iclr2018.html#TolstikhinBGS18},
  year = {2018}
}


@inproceedings{Beyer1999NN,
  author = {Beyer, Kevin S. and Goldstein, Jonathan and Ramakrishnan, Raghu and Shaft, Uri},
  booktitle = {ICDT},
  editor = {Beeri, Catriel and Buneman, Peter},
  isbn = {3-540-65452-6},
  pages = {217-235},
  publisher = {Springer},
  series = {Lecture Notes in Computer Science},
  title = {When Is ''Nearest Neighbor'' Meaningful?},
  volume = {1540},
  year = {1999}
}

@inproceedings{Sauer2021Proj,
author = {Sauer, Axel and Chitta, Kashyap and M\"{u}ller, Jens and Geiger, Andreas},
title = {Projected GANs converge faster},
year = {2024},
isbn = {9781713845393},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Generative Adversarial Networks (GANs) produce high-quality images but are challenging to train. They need careful regularization, vast amounts of compute, and expensive hyper-parameter sweeps. We make significant headway on these issues by projecting generated and real samples into a fixed, pretrained feature space. Motivated by the finding that the discriminator cannot fully exploit features from deeper layers of the pretrained model, we propose a more effective strategy that mixes features across channels and resolutions. Our Projected GAN improves image quality, sample efficiency, and convergence speed. It is further compatible with resolutions of up to one Megapixel and advances the state-of-the-art Fr\'{e}chet Inception Distance (FID) on twenty-two benchmark datasets. Importantly, Projected GANs match the previously lowest FIDs up to 40 times faster, cutting the wall-clock time from 5 days to less than 3 hours given the same computational resources.},
booktitle = {Proceedings of the 35th International Conference on Neural Information Processing Systems},
articleno = {1337},
numpages = {13},
series = {NIPS '21}
}

@inproceedings{Esser2O21VQGAN,
  author = {Esser, Patrick and Rombach, Robin and Ommer, Björn},
  biburl = {https://www.bibsonomy.org/bibtex/2f9dccb191e8f56160323a27f6d77f8f3/dblp},
  booktitle = {CVPR},
  interhash = {085a60f932248b2846118c73a0babd15},
  intrahash = {f9dccb191e8f56160323a27f6d77f8f3},
  keywords = {dblp},
  pages = {12873-12883},
  publisher = {Computer Vision Foundation / IEEE},
  timestamp = {2024-04-09T20:53:51.000+0200},
  title = {Taming Transformers for High-Resolution Image Synthesis.},
  url = {http://dblp.uni-trier.de/db/conf/cvpr/cvpr2021.html#EsserRO21},
  year = {2021}
}


@inproceedings{Sauer2022StyleGAN,
author = {Sauer, Axel and Schwarz, Katja and Geiger, Andreas},
title = {StyleGAN-XL: Scaling StyleGAN to Large Diverse Datasets},
year = {2022},
isbn = {9781450393379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
doi = {10.1145/3528233.3530738},
booktitle = {ACM SIGGRAPH 2022 Conference Proceedings},
articleno = {49},
numpages = {10},
keywords = {Generative Adversarial Networks, Image Editing, Image Synthesis, Pretrained Models},
location = {Vancouver, BC, Canada},
series = {SIGGRAPH '22}
}

@inproceedings{Altschuler2017sink,
author = {Altschuler, Jason and Weed, Jonathan and Rigollet, Philippe},
title = {Near-linear time approximation algorithms for optimal transport via Sinkhorn iteration},
year = {2017},
isbn = {9781510860964},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Computing optimal transport distances such as the earth mover's distance is a fundamental problem in machine learning, statistics, and computer vision. Despite the recent introduction of several algorithms with good empirical performance, it is unknown whether general optimal transport distances can be approximated in near-linear time. This paper demonstrates that this ambitious goal is in fact achieved by Cuturi's Sinkhorn Distances. This result relies on a new analysis of Sinkhorn iterations, which also directly suggests a new greedy coordinate descent algorithm GREENKHORN with the same theoretical guarantees. Numerical simulations illustrate that GREENKHORN significantly outperforms the classical SINKHORN algorithm in practice.},
booktitle = {Proceedings of the 31st International Conference on Neural Information Processing Systems},
pages = {1961–1971},
numpages = {11},
location = {Long Beach, California, USA},
series = {NIPS'17}
}

@InProceedings{Lee2023VAEtraj,
  title = {Minimizing Trajectory Curvature of {ODE}-based Generative Models},
  author =       {Lee, Sangyun and Kim, Beomsu and Ye, Jong Chul},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  pages = {18957--18973},
  year = {2023},
  editor = {Krause, Andreas and Brunskill, Emma and Cho, Kyunghyun and Engelhardt, Barbara and Sabato, Sivan and Scarlett, Jonathan},
  volume = {202},
  series = {Proceedings of Machine Learning Research},
  month = {23--29 Jul},
  publisher =    {PMLR},
  pdf = {https://proceedings.mlr.press/v202/lee23j/lee23j.pdf},
  url = {https://proceedings.mlr.press/v202/lee23j.html},
  abstract = {Recent ODE/SDE-based generative models, such as diffusion models, rectified flows, and flow matching, define a generative process as a time reversal of a fixed forward process. Even though these models show impressive performance on large-scale datasets, numerical simulation requires multiple evaluations of a neural network, leading to a slow sampling speed. We attribute the reason to the high curvature of the learned generative trajectories, as it is directly related to the truncation error of a numerical solver. Based on the relationship between the forward process and the curvature, here we present an efficient method of training the forward process to minimize the curvature of generative trajectories without any ODE/SDE simulation. Experiments show that our method achieves a lower curvature than previous models and, therefore, decreased sampling costs while maintaining competitive performance. Code is available at https://github.com/sangyun884/fast-ode.}
}


@article{Flamary21POT,
author = {Flamary, R\'{e}mi and Courty, Nicolas and Gramfort, Alexandre and Alaya, Mokhtar Z. and Boisbunon, Aur\'{e}lie and Chambon, Stanislas and Chapel, Laetitia and Corenflos, Adrien and Fatras, Kilian and Fournier, Nemo and Gautheron, L\'{e}o and Gayraud, Nathalie T.H. and Janati, Hicham and Rakotomamonjy, Alain and Redko, Ievgen and Rolet, Antoine and Schutz, Antony and Seguy, Vivien and Sutherland, Danica J. and Tavenard, Romain and Tong, Alexander and Vayer, Titouan},
title = {POT: Python optimal transport},
year = {2021},
issue_date = {January 2021},
publisher = {JMLR.org},
volume = {22},
number = {1},
issn = {1532-4435},
abstract = {Optimal transport has recently been reintroduced to the machine learning community thanks in part to novel efficient optimization procedures allowing for medium to large scale applications. We propose a Python toolbox that implements several key optimal transport ideas for the machine learning community. The toolbox contains implementations of a number of founding works of OT for machine learning such as Sinkhorn algorithm and Wasserstein barycenters, but also provides generic solvers that can be used for conducting novel fundamental research. This toolbox, named POT for Python Optimal Transport, is open source with an MIT license.},
journal = {J. Mach. Learn. Res.},
month = {jan},
articleno = {78},
numpages = {8},
keywords = {optimal transport, divergence, optimization, domain adaptation}
}

@inproceedings{Pooladian2023mini,
  author       = {Aram{-}Alexandre Pooladian and
                  Heli Ben{-}Hamu and
                  Carles Domingo{-}Enrich and
                  Brandon Amos and
                  Yaron Lipman and
                  Ricky T. Q. Chen},
  editor       = {Andreas Krause and
                  Emma Brunskill and
                  Kyunghyun Cho and
                  Barbara Engelhardt and
                  Sivan Sabato and
                  Jonathan Scarlett},
  title        = {Multisample Flow Matching: Straightening Flows with Minibatch Couplings},
  booktitle    = {International Conference on Machine Learning, {ICML} 2023, 23-29 July
                  2023, Honolulu, Hawaii, {USA}},
  series       = {Proceedings of Machine Learning Research},
  volume       = {202},
  pages        = {28100--28127},
  publisher    = {{PMLR}},
  year         = {2023},
  url          = {https://proceedings.mlr.press/v202/pooladian23a.html},
  timestamp    = {Mon, 28 Aug 2023 17:23:08 +0200},
  biburl       = {https://dblp.org/rec/conf/icml/PooladianBDALC23.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{Nichol2022text2img,
  author       = {Alexander Quinn Nichol and
                  Prafulla Dhariwal and
                  Aditya Ramesh and
                  Pranav Shyam and
                  Pamela Mishkin and
                  Bob McGrew and
                  Ilya Sutskever and
                  Mark Chen},
  editor       = {Kamalika Chaudhuri and
                  Stefanie Jegelka and
                  Le Song and
                  Csaba Szepesv{\'{a}}ri and
                  Gang Niu and
                  Sivan Sabato},
  title        = {{GLIDE:} Towards Photorealistic Image Generation and Editing with
                  Text-Guided Diffusion Models},
  booktitle    = {International Conference on Machine Learning, {ICML} 2022, 17-23 July
                  2022, Baltimore, Maryland, {USA}},
  series       = {Proceedings of Machine Learning Research},
  volume       = {162},
  pages        = {16784--16804},
  publisher    = {{PMLR}},
  year         = {2022},
  url          = {https://proceedings.mlr.press/v162/nichol22a.html},
  timestamp    = {Thu, 25 May 2023 10:38:31 +0200},
  biburl       = {https://dblp.org/rec/conf/icml/NicholDRSMMSC22.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{Ramesh2022text2img,
  title={Hierarchical Text-Conditional Image Generation with CLIP Latents},
  author={Aditya Ramesh and Prafulla Dhariwal and Alex Nichol and Casey Chu and Mark Chen},
  journal={ArXiv},
  year={2022},
  volume={abs/2204.06125},
  url={https://api.semanticscholar.org/CorpusID:248097655}
}

@inproceedings{Zhou2024SiD,
  title={Score identity Distillation: Exponentially Fast Distillation of Pretrained Diffusion Models for One-Step Generation},
  author={Mingyuan Zhou and Huangjie Zheng and Zhendong Wang and Mingzhang Yin and Hai Huang},
  booktitle={International Conference on Machine Learning},
  url={https://arxiv.org/abs/2404.04057},
  url_code={https://github.com/mingyuanzhou/SiD},
  year={2024}
}


@inproceedings{zhang2018perceptual,
  title={The Unreasonable Effectiveness of Deep Features as a Perceptual Metric},
  author={Zhang, Richard and Isola, Phillip and Efros, Alexei A and Shechtman, Eli and Wang, Oliver},
  booktitle={CVPR},
  year={2018}
}

@inproceedings{Ho2020DDPM,
 author = {Ho, Jonathan and Jain, Ajay and Abbeel, Pieter},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {6840--6851},
 publisher = {Curran Associates, Inc.},
 title = {Denoising Diffusion Probabilistic Models},
 url = {https://proceedings.neurips.cc/paper_files/paper/2020/file/4c5bcfec8584af0d967f1ab10179ca4b-Paper.pdf},
 volume = {33},
 year = {2020}
}


@inproceedings{StyleGAND2D,
author = {Kang, Minguk and Shim, Woohyeon and Cho, Minsu and Park, Jaesik},
title = {Rebooting ACGAN: auxiliary classifier GANs with stable training},
year = {2024},
isbn = {9781713845393},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
booktitle = {Proceedings of the 35th International Conference on Neural Information Processing Systems},
articleno = {1800},
numpages = {14},
series = {NIPS '21}
}


@inproceedings{StyleGANADA,
author = {Karras, Tero and Aittala, Miika and Hellsten, Janne and Laine, Samuli and Lehtinen, Jaakko and Aila, Timo},
title = {Training generative adversarial networks with limited data},
year = {2020},
isbn = {9781713829546},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
booktitle = {Proceedings of the 34th International Conference on Neural Information Processing Systems},
articleno = {1015},
numpages = {11},
location = {Vancouver, BC, Canada},
series = {NIPS '20}
}


@inproceedings{Lu2023CMGAN,
title={{CM}-{GAN}: Stabilizing {GAN} Training with Consistency Models},
author={Haoye Lu and Yiwei Lu and Dihong Jiang and Spencer Ryan Szabados and Sun Sun and Yaoliang Yu},
booktitle={ICML 2023 Workshop on Structured Probabilistic Inference {\&} Generative Modeling},
year={2023},
url={https://openreview.net/forum?id=Uh2WwUyiAv}
}


@inproceedings{Song2021NCSN,
 author = {Song, Yang and Ermon, Stefano},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Generative Modeling by Estimating Gradients of the Data Distribution},
 url = {https://proceedings.neurips.cc/paper_files/paper/2019/file/3001ef257407d5a371a96dcd947c7d93-Paper.pdf},
 volume = {32},
 year = {2019}
}

@inproceedings{Song2021SMLD,
  author       = {Yang Song and
                  Jascha Sohl{-}Dickstein and
                  Diederik P. Kingma and
                  Abhishek Kumar and
                  Stefano Ermon and
                  Ben Poole},
  title        = {Score-Based Generative Modeling through Stochastic Differential Equations},
  booktitle    = {9th International Conference on Learning Representations, {ICLR} 2021,
                  Virtual Event, Austria, May 3-7, 2021},
  publisher    = {OpenReview.net},
  year         = {2021},
  url          = {https://openreview.net/forum?id=PxTIG12RRHS},
  timestamp    = {Wed, 23 Jun 2021 17:36:39 +0200},
  biburl       = {https://dblp.org/rec/conf/iclr/0011SKKEP21.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{Albergo2023stointrp,
  author       = {Michael S. Albergo and
                  Eric Vanden{-}Eijnden},
  title        = {Building Normalizing Flows with Stochastic Interpolants},
  booktitle    = {The Eleventh International Conference on Learning Representations,
                  {ICLR} 2023, Kigali, Rwanda, May 1-5, 2023},
  publisher    = {OpenReview.net},
  year         = {2023},
  url          = {https://openreview.net/pdf?id=li7qeBbCR1t},
  timestamp    = {Fri, 30 Jun 2023 14:55:53 +0200},
  biburl       = {https://dblp.org/rec/conf/iclr/AlbergoV23.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@inproceedings{Jabri23RIN,
author = {Jabri, Allan and Fleet, David J. and Chen, Ting},
title = {Scalable adaptive computation for iterative generation},
year = {2023},
publisher = {JMLR.org},
booktitle = {Proceedings of the 40th International Conference on Machine Learning},
articleno = {594},
numpages = {21},
location = {Honolulu, Hawaii, USA},
series = {ICML'23}
}


@article {Allen2013fitting,
      author = "Allan J.  Clarke and Stephen  Van Gorder",
      title = "On Fitting a Straight Line to Data when the Noise in Both Variables Is Unknown",
      journal = "Journal of Atmospheric and Oceanic Technology",
      year = "2013",
      publisher = "American Meteorological Society",
      address = "Boston MA, USA",
      volume = "30",
      number = "1",
      doi = "10.1175/JTECH-D-12-00067.1",
      pages=      "151 - 158",
      url = "https://journals.ametsoc.org/view/journals/atot/30/1/jtech-d-12-00067_1.xml"
}

@inproceedings{Tero2022EDM,
author = {Karras, Tero and Aittala, Miika and Laine, Samuli and Aila, Timo},
title = {Elucidating the design space of diffusion-based generative models},
year = {2022},
isbn = {9781713871088},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We argue that the theory and practice of diffusion-based generative models are currently unnecessarily convoluted and seek to remedy the situation by presenting a design space that clearly separates the concrete design choices. This lets us identify several changes to both the sampling and training processes, as well as preconditioning of the score networks. Together, our improvements yield new state-of-the-art FID of 1.79 for CIFAR-10 in a class-conditional setting and 1.97 in an unconditional setting, with much faster sampling (35 network evaluations per image) than prior designs. To further demonstrate their modular nature, we show that our design changes dramatically improve both the efficiency and quality obtainable with pre-trained score networks from previous work, including improving the FID of a previously trained ImageNet-64 model from 2.07 to near-SOTA 1.55, and after re-training with our proposed improvements to a new SOTA of 1.36.},
booktitle = {Proceedings of the 36th International Conference on Neural Information Processing Systems},
articleno = {1926},
numpages = {13},
location = {, New Orleans, LA, USA, },
series = {NIPS '22}
}


@inproceedings{liu2024instaflow,
title={InstaFlow: One Step is Enough for High-Quality Diffusion-Based Text-to-Image Generation},
author={Xingchao Liu and Xiwen Zhang and Jianzhu Ma and Jian Peng and qiang liu},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=1k4yZbbDqX}
}

@article{Luhman2021knowdistl,
  author       = {Eric Luhman and
                  Troy Luhman},
  title        = {Knowledge Distillation in Iterative Generative Models for Improved
                  Sampling Speed},
  journal      = {CoRR},
  volume       = {abs/2101.02388},
  year         = {2021},
  url          = {https://arxiv.org/abs/2101.02388},
  eprinttype    = {arXiv},
  eprint       = {2101.02388},
  timestamp    = {Thu, 21 Jan 2021 14:42:30 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2101-02388.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{radford2021learning,
  title={Learning transferable visual models from natural language supervision},
  author={Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and others},
  booktitle={International conference on machine learning},
  pages={8748--8763},
  year={2021},
  organization={PMLR}
}

@inproceedings{Lipman2023flow,
title={Flow Matching for Generative Modeling},
author={Yaron Lipman and Ricky T. Q. Chen and Heli Ben-Hamu and Maximilian Nickel and Matthew Le},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=PqvMRDCJT9t}
}


@inproceedings{Liu2023RectFlow,
  author       = {Xingchao Liu and
                  Chengyue Gong and
                  Qiang Liu},
  title        = {Flow Straight and Fast: Learning to Generate and Transfer Data with
                  Rectified Flow},
  booktitle    = {The Eleventh International Conference on Learning Representations,
                  {ICLR} 2023, Kigali, Rwanda, May 1-5, 2023},
  publisher    = {OpenReview.net},
  year         = {2023},
  url          = {https://openreview.net/pdf?id=XVjTT1nw5z},
  timestamp    = {Fri, 30 Jun 2023 14:55:53 +0200},
  biburl       = {https://dblp.org/rec/conf/iclr/LiuG023.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{Kim2024CTM,
title={Consistency Trajectory Models: Learning Probability Flow {ODE} Trajectory of Diffusion},
author={Dongjun Kim and Chieh-Hsin Lai and Wei-Hsiang Liao and Naoki Murata and Yuhta Takida and Toshimitsu Uesaka and Yutong He and Yuki Mitsufuji and Stefano Ermon},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=ymjI8feDTD}
}

@article{Bull_2021,
   title={Probabilistic Inference for Structural Health Monitoring: New Modes of Learning from Data},
   volume={7},
   ISSN={2376-7642},
   url={http://dx.doi.org/10.1061/AJRUA6.0001106},
   DOI={10.1061/ajrua6.0001106},
   number={1},
   journal={ASCE-ASME Journal of Risk and Uncertainty in Engineering Systems, Part A: Civil Engineering},
   publisher={American Society of Civil Engineers (ASCE)},
   author={Bull, Lawrence A. and Gardner, Paul and Rogers, Timothy J. and Cross, Elizabeth J. and Dervilis, Nikolaos and Worden, Keith},
   year={2021},
   month=mar }

@misc{jiang2017variationaldeepembeddingunsupervised,
      title={Variational Deep Embedding: An Unsupervised and Generative Approach to Clustering}, 
      author={Zhuxi Jiang and Yin Zheng and Huachun Tan and Bangsheng Tang and Hanning Zhou},
      year={2017},
      eprint={1611.05148},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/1611.05148}, 
}

@inproceedings{Song23CM,
author = {Song, Yang and Dhariwal, Prafulla and Chen, Mark and Sutskever, Ilya},
title = {Consistency models},
year = {2023},
publisher = {JMLR.org},
abstract = {Diffusion models have significantly advanced the fields of image, audio, and video generation, but they depend on an iterative sampling process that causes slow generation. To overcome this limitation, we propose consistency models, a new family of models that generate high quality samples by directly mapping noise to data. They support fast one-step generation by design, while still allowing multistep sampling to trade compute for sample quality. They also support zero-shot data editing, such as image inpainting, colorization, and super-resolution, without requiring explicit training on these tasks. Consistency models can be trained either by distilling pre-trained diffusion models, or as standalone generative models altogether. Through extensive experiments, we demonstrate that they outperform existing distillation techniques for diffusion models in one- and few-step sampling, achieving the new state-of-the-art FID of 3.55 on CIFAR-10 and 6.20 on ImageNet 64 \texttimes{} 64 for one-step generation. When trained in isolation, consistency models become a new family of generative models that can outperform existing one-step, non-adversarial generative models on standard benchmarks such as CIFAR- 10, ImageNet 64 \texttimes{} 64 and LSUN 256 \texttimes{} 256.},
booktitle = {Proceedings of the 40th International Conference on Machine Learning},
articleno = {1335},
numpages = {42},
location = {<conf-loc>, <city>Honolulu</city>, <state>Hawaii</state>, <country>USA</country>, </conf-loc>},
series = {ICML'23}
}


@inproceedings{Dhariwal2021beatgan,
 author = {Dhariwal, Prafulla and Nichol, Alexander},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {8780--8794},
 publisher = {Curran Associates, Inc.},
 title = {Diffusion Models Beat GANs on Image Synthesis},
 url = {https://proceedings.neurips.cc/paper_files/paper/2021/file/49ad23d1ec9fa4bd8d77d02681df5cfa-Paper.pdf},
 volume = {34},
 year = {2021}
}

@inproceedings{Salimans2022PD,
  author       = {Tim Salimans and
                  Jonathan Ho},
  title        = {Progressive Distillation for Fast Sampling of Diffusion Models},
  booktitle    = {The Tenth International Conference on Learning Representations, {ICLR}
                  2022, Virtual Event, April 25-29, 2022},
  publisher    = {OpenReview.net},
  year         = {2022},
  url          = {https://openreview.net/forum?id=TIdIXIpzhoI},
  timestamp    = {Sat, 20 Aug 2022 01:15:42 +0200},
  biburl       = {https://dblp.org/rec/conf/iclr/SalimansH22.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{Saharia2024,
author = {Saharia, Chitwan and Chan, William and Saxena, Saurabh and Lit, Lala and Whang, Jay and Denton, Emily and Ghasemipour, Seyed Kamyar Seyed and Ayan, Burcu Karagol and Mahdavi, S. Sara and Gontijo-Lopes, Raphael and Salimans, Tim and Ho, Jonathan and Fleet, David J and Norouzi, Mohammad},
title = {Photorealistic text-to-image diffusion models with deep language understanding},
year = {2024},
isbn = {9781713871088},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We present Imagen, a text-to-image diffusion model with an unprecedented degree of photorealism and a deep level of language understanding. Imagen builds on the power of large transformer language models in understanding text and hinges on the strength of diffusion models in high-fidelity image generation. Our key discovery is that generic large language models (e.g. T5), pretrained on text-only corpora, are surprisingly effective at encoding text for image synthesis: increasing the size of the language model in Imagen boosts both sample fidelity and image-text alignment much more than increasing the size of the image diffusion model. Imagen achieves a new state-of-the-art FID score of 7.27 on the COCO dataset, without ever training on COCO, and human raters find Imagen samples to be on par with the COCO data itself in image-text alignment. To assess text-to-image models in greater depth, we introduce DrawBench, a comprehensive and challenging benchmark for text-to-image models. With DrawBench, we compare Imagen with recent methods including VQ-GAN+CLIP, Latent Diffusion Models, GLIDE and DALL-E 2, and find that human raters prefer Imagen over other models in side-by-side comparisons, both in terms of sample quality and image-text alignment.},
booktitle = {Proceedings of the 36th International Conference on Neural Information Processing Systems},
articleno = {2643},
numpages = {16},
location = {<conf-loc>, <city>New Orleans</city>, <state>LA</state>, <country>USA</country>, </conf-loc>},
series = {NIPS '22}
}

@inproceedings{KingmaW2013VAE,
  author       = {Diederik P. Kingma and
                  Max Welling},
  editor       = {Yoshua Bengio and
                  Yann LeCun},
  title        = {Auto-Encoding Variational Bayes},
  booktitle    = {2nd International Conference on Learning Representations, {ICLR} 2014,
                  Banff, AB, Canada, April 14-16, 2014, Conference Track Proceedings},
  year         = {2014},
  url          = {http://arxiv.org/abs/1312.6114},
  timestamp    = {Thu, 04 Apr 2019 13:20:07 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/KingmaW13.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{hinton2006reducing,
  title={Reducing the dimensionality of data with neural networks},
  author={Hinton, Geoffrey E and Salakhutdinov, Ruslan R},
  journal={science},
  volume={313},
  number={5786},
  pages={504--507},
  year={2006},
  publisher={American Association for the Advancement of Science}
}

@misc{Liu2024sora,
      title={Sora: A Review on Background, Technology, Limitations, and Opportunities of Large Vision Models}, 
      author={Yixin Liu and Kai Zhang and Yuan Li and Zhiling Yan and Chujie Gao and Ruoxi Chen and Zhengqing Yuan and Yue Huang and Hanchi Sun and Jianfeng Gao and Lifang He and Lichao Sun},
      year={2024},
      eprint={2402.17177},
      archivePrefix={arXiv},
      primaryClass={id='cs.CV' full_name='Computer Vision and Pattern Recognition' is_active=True alt_name=None in_archive='cs' is_general=False description='Covers image processing, computer vision, pattern recognition, and scene understanding. Roughly includes material in ACM Subject Classes I.2.10, I.4, and I.5.'}
}


@INPROCEEDINGS{Hoang2020modecollapse,
  author={Thanh-Tung, Hoang and Tran, Truyen},
  booktitle={2020 International Joint Conference on Neural Networks (IJCNN)}, 
  title={Catastrophic forgetting and mode collapse in GANs}, 
  year={2020},
  volume={},
  number={},
  pages={1-10},
  keywords={Gallium nitride;Task analysis;Training;Generators;Convergence;Generative adversarial networks;Neural networks;GANs;generative;catastrophic forgetting;mode collapse},
  doi={10.1109/IJCNN48605.2020.9207181}}

@inproceedings{srivastava2017veegan,
  title={VEEGAN: Reducing mode collapse in GANs using implicit variational learning},
  author={Srivastava, Akash and Valkov, Lazar and Russell, Chris and Gutmann, Michael U and Sutton, Charles},
  booktitle={Advances in neural information processing systems},
  pages={3308--3318},
  year={2017}
}

@inproceedings{larsen2016autoencoding,
  title={Autoencoding beyond pixels using a learned similarity metric},
  author={Larsen, Anders Boesen Lindbo and Sønderby, Søren Kaae and Larochelle, Hugo and Winther, Ole},
  booktitle={International Conference on Machine Learning},
  pages={1558--1566},
  year={2016},
  organization={PMLR}
}

@inproceedings{rombach2022high,
  added-at = {2022-09-05T11:03:26.000+0200},
  author = {Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Bj{\"o}rn},
  biburl = {https://www.bibsonomy.org/bibtex/28253f81df661643c915d38d2e317d17d/tobias.koopmann},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  interhash = {e54e035bdfef24c40a2133cbe99ac3bb},
  intrahash = {8253f81df661643c915d38d2e317d17d},
  keywords = {readinglist},
  pages = {10684--10695},
  timestamp = {2022-09-05T11:03:26.000+0200},
  title = {High-resolution image synthesis with latent diffusion models},
  year = 2022
}

@article{Allan2013noisefit,
      author = "Allan J.  Clarke and Stephen  Van Gorder",
      title = "On Fitting a Straight Line to Data when the Noise in Both Variables Is Unknown",
      journal = "Journal of Atmospheric and Oceanic Technology",
      year = "2013",
      publisher = "American Meteorological Society",
      address = "Boston MA, USA",
      volume = "30",
      number = "1",
      doi = "10.1175/JTECH-D-12-00067.1",
      pages=      "151 - 158",
      url = "https://journals.ametsoc.org/view/journals/atot/30/1/jtech-d-12-00067_1.xml"
}

@inproceedings{Eric2023flow,
  author       = {Michael S. Albergo and
                  Eric Vanden{-}Eijnden},
  title        = {Building Normalizing Flows with Stochastic Interpolants},
  booktitle    = {The Eleventh International Conference on Learning Representations,
                  {ICLR} 2023, Kigali, Rwanda, May 1-5, 2023},
  publisher    = {OpenReview.net},
  year         = {2023},
  url          = {https://openreview.net/pdf?id=li7qeBbCR1t},
  timestamp    = {Fri, 30 Jun 2023 14:55:53 +0200},
  biburl       = {https://dblp.org/rec/conf/iclr/AlbergoV23.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{Song2021implicit,
  author       = {Jiaming Song and
                  Chenlin Meng and
                  Stefano Ermon},
  title        = {Denoising Diffusion Implicit Models},
  booktitle    = {9th International Conference on Learning Representations, {ICLR} 2021,
                  Virtual Event, Austria, May 3-7, 2021},
  publisher    = {OpenReview.net},
  year         = {2021},
  url          = {https://openreview.net/forum?id=St1giarCHLP},
  timestamp    = {Wed, 23 Jun 2021 17:36:39 +0200},
  biburl       = {https://dblp.org/rec/conf/iclr/SongME21.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{Kim2021GuidedTTSAD,
  title={Guided-TTS: A Diffusion Model for Text-to-Speech via Classifier Guidance},
  author={Heeseung Kim and Sungwon Kim and Sungroh Yoon},
  booktitle={International Conference on Machine Learning},
  year={2021},
  url={https://api.semanticscholar.org/CorpusID:246430592}
}

@misc{kong2021diffwave,
      title={DiffWave: A Versatile Diffusion Model for Audio Synthesis}, 
      author={Zhifeng Kong and Wei Ping and Jiaji Huang and Kexin Zhao and Bryan Catanzaro},
      year={2021},
      eprint={2009.09761},
      archivePrefix={arXiv},
      primaryClass={id='eess.AS' full_name='Audio and Speech Processing' is_active=True alt_name=None in_archive='eess' is_general=False description='Theory and methods for processing signals representing audio, speech, and language, and their applications. This includes analysis, synthesis, enhancement, transformation, classification and interpretation of such signals as well as the design, development, and evaluation of associated signal processing systems. Machine learning and pattern analysis applied to any of the above areas is also welcome.  Specific topics of interest include: auditory modeling and hearing aids; acoustic beamforming and source localization; classification of acoustic scenes; speaker separation; active noise control and echo cancellation; enhancement; de-reverberation; bioacoustics; music signals analysis, synthesis and modification; music information retrieval;  audio for multimedia and joint audio-video processing; spoken and written language modeling, segmentation, tagging, parsing, understanding, and translation; text mining; speech production, perception, and psychoacoustics; speech analysis, synthesis, and perceptual modeling and coding; robust speech recognition; speaker recognition and characterization; deep learning, online learning, and graphical models applied to speech, audio, and language signals; and implementation aspects ranging from system architecture to fast algorithms.'}
}

@misc{chen2020wavegrad,
      title={WaveGrad: Estimating Gradients for Waveform Generation}, 
      author={Nanxin Chen and Yu Zhang and Heiga Zen and Ron J. Weiss and Mohammad Norouzi and William Chan},
      year={2020},
      eprint={2009.00713},
      archivePrefix={arXiv},
      primaryClass={id='eess.AS' full_name='Audio and Speech Processing' is_active=True alt_name=None in_archive='eess' is_general=False description='Theory and methods for processing signals representing audio, speech, and language, and their applications. This includes analysis, synthesis, enhancement, transformation, classification and interpretation of such signals as well as the design, development, and evaluation of associated signal processing systems. Machine learning and pattern analysis applied to any of the above areas is also welcome.  Specific topics of interest include: auditory modeling and hearing aids; acoustic beamforming and source localization; classification of acoustic scenes; speaker separation; active noise control and echo cancellation; enhancement; de-reverberation; bioacoustics; music signals analysis, synthesis and modification; music information retrieval;  audio for multimedia and joint audio-video processing; spoken and written language modeling, segmentation, tagging, parsing, understanding, and translation; text mining; speech production, perception, and psychoacoustics; speech analysis, synthesis, and perceptual modeling and coding; robust speech recognition; speaker recognition and characterization; deep learning, online learning, and graphical models applied to speech, audio, and language signals; and implementation aspects ranging from system architecture to fast algorithms.'}
}

@misc{popov2021gradtts,
      title={Grad-TTS: A Diffusion Probabilistic Model for Text-to-Speech}, 
      author={Vadim Popov and Ivan Vovk and Vladimir Gogoryan and Tasnima Sadekova and Mikhail Kudinov},
      year={2021},
      eprint={2105.06337},
      archivePrefix={arXiv},
      primaryClass={id='cs.LG' full_name='Machine Learning' is_active=True alt_name=None in_archive='cs' is_general=False description='Papers on all aspects of machine learning research (supervised, unsupervised, reinforcement learning, bandit problems, and so on) including also robustness, explanation, fairness, and methodology. cs.LG is also an appropriate primary category for applications of machine learning methods.'}
}

@inproceedings{Chen2018CNF,
author = {Chen, Ricky T. Q. and Rubanova, Yulia and Bettencourt, Jesse and Duvenaud, David},
title = {Neural ordinary differential equations},
year = {2018},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We introduce a new family of deep neural network models. Instead of specifying a discrete sequence of hidden layers, we parameterize the derivative of the hidden state using a neural network. The output of the network is computed using a black-box differential equation solver. These continuous-depth models have constant memory cost, adapt their evaluation strategy to each input, and can explicitly trade numerical precision for speed. We demonstrate these properties in continuous-depth residual networks and continuous-time latent variable models. We also construct continuous normalizing flows, a generative model that can train by maximum likelihood, without partitioning or ordering the data dimensions. For training, we show how to scalably backpropagate through any ODE solver, without access to its internal operations. This allows end-to-end training of ODEs within larger models.},
booktitle = {Proceedings of the 32nd International Conference on Neural Information Processing Systems},
pages = {6572–6583},
numpages = {12},
location = {Montr\'{e}al, Canada},
series = {NIPS'18}
}

@book{kendall1973book,
  title={The Advanced Theory of Statistics. Vol. 2: Inference and: Relationsship},
  author={Kendall, M.G. and Stuart, A.},
  url={https://books.google.co.il/books?id=elabQwAACAAJ},
  year={1973},
  publisher={Griffin}
}

@inproceedings{singer2023makeavideo,
title={Make-A-Video: Text-to-Video Generation without Text-Video Data},
author={Uriel Singer and Adam Polyak and Thomas Hayes and Xi Yin and Jie An and Songyang Zhang and Qiyuan Hu and Harry Yang and Oron Ashual and Oran Gafni and Devi Parikh and Sonal Gupta and Yaniv Taigman},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=nJfylDvgzlq}
}


@inproceedings{Goodfellow2014GAN,
 author = {Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Generative Adversarial Nets},
 url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/5ca3e9b122f61f8f06494c97b1afccf3-Paper.pdf},
 volume = {27},
 year = {2014}
}

@article{Pascal2011connection,
author = {Vincent, Pascal},
title = {A connection between score matching and denoising autoencoders},
year = {2011},
issue_date = {July 2011},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
volume = {23},
number = {7},
issn = {0899-7667},
doi = {10.1162/NECO_a_00142},
abstract = {Denoising autoencoders have been previously shown to be competitive alternatives to restricted Boltzmann machines for unsupervised pretraining of each layer of a deep architecture. We show that a simple denoising autoencoder training criterion is equivalent to matching the score (with respect to the data) of a specific energy-based model to that of a nonparametric Parzen density estimator of the data. This yields several useful insights. It defines a proper probabilistic model for the denoising autoencoder technique, which makes it in principle possible to sample from them or rank examples by their energy. It suggests a different way to apply score matching that is related to learning to denoise and does not require computing second derivatives. It justifies the use of tied weights between the encoder and decoder and suggests ways to extend the success of denoising autoencoders to a larger family of energy-based models.},
journal = {Neural Comput.},
month = {jul},
pages = {1661–1674},
numpages = {14}
}


@InProceedings{Nichol21improved,
  title = 	 {Improved Denoising Diffusion Probabilistic Models},
  author =       {Nichol, Alexander Quinn and Dhariwal, Prafulla},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {8162--8171},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/nichol21a/nichol21a.pdf},
  url = 	 {https://proceedings.mlr.press/v139/nichol21a.html},
  abstract = 	 {Denoising diffusion probabilistic models (DDPM) are a class of generative models which have recently been shown to produce excellent samples. We show that with a few simple modifications, DDPMs can also achieve competitive log-likelihoods while maintaining high sample quality. Additionally, we find that learning variances of the reverse diffusion process allows sampling with an order of magnitude fewer forward passes with a negligible difference in sample quality, which is important for the practical deployment of these models. We additionally use precision and recall to compare how well DDPMs and GANs cover the target distribution. Finally, we show that the sample quality and likelihood of these models scale smoothly with model capacity and training compute, making them easily scalable. We release our code and pre-trained models at https://github.com/openai/improved-diffusion.}
}



@InProceedings{sohl2015thermo,
  title = 	 {Deep Unsupervised Learning using Nonequilibrium Thermodynamics},
  author = 	 {Sohl-Dickstein, Jascha and Weiss, Eric and Maheswaranathan, Niru and Ganguli, Surya},
  booktitle = 	 {Proceedings of the 32nd International Conference on Machine Learning},
  pages = 	 {2256--2265},
  year = 	 {2015},
  editor = 	 {Bach, Francis and Blei, David},
  volume = 	 {37},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Lille, France},
  month = 	 {07--09 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v37/sohl-dickstein15.pdf},
  url = 	 {https://proceedings.mlr.press/v37/sohl-dickstein15.html},
  abstract = 	 {A central problem in machine learning involves modeling complex data-sets using highly flexible families of probability distributions in which learning, sampling, inference, and evaluation are still analytically or computationally tractable. Here, we develop an approach that simultaneously achieves both flexibility and tractability. The essential idea, inspired by non-equilibrium statistical physics, is to systematically and slowly destroy structure in a data distribution through an iterative forward diffusion process. We then learn a reverse diffusion process that restores structure in data, yielding a highly flexible and tractable generative model of the data. This approach allows us to rapidly learn, sample from, and evaluate probabilities in deep generative models with thousands of layers or time steps, as well as to compute conditional and posterior probabilities under the learned model. We additionally release an open source reference implementation of the algorithm.}
}

@inproceedings{song2019sliced,
  author    = {Yang Song and
               Sahaj Garg and
               Jiaxin Shi and
               Stefano Ermon},
  title     = {Sliced Score Matching: {A} Scalable Approach to Density and Score
               Estimation},
  booktitle = {Proceedings of the Thirty-Fifth Conference on Uncertainty in Artificial
               Intelligence, {UAI} 2019, Tel Aviv, Israel, July 22-25, 2019},
  pages     = {204},
  year      = {2019},
  url       = {http://auai.org/uai2019/proceedings/papers/204.pdf},
}

@inproceedings{song2020improved,
  author    = {Yang Song and Stefano Ermon},
  editor    = {Hugo Larochelle and
               Marc'Aurelio Ranzato and
               Raia Hadsell and
               Maria{-}Florina Balcan and
               Hsuan{-}Tien Lin},
  title     = {Improved Techniques for Training Score-Based Generative Models},
  booktitle = {Advances in Neural Information Processing Systems 33: Annual Conference
               on Neural Information Processing Systems 2020, NeurIPS 2020, December
               6-12, 2020, virtual},
  year      = {2020}
}

@inproceedings{
biggan,
title={Large Scale {GAN} Training for High Fidelity Natural Image Synthesis},
author={Andrew Brock and Jeff Donahue and Karen Simonyan},
booktitle={International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=B1xsqj09Fm},
}

@inproceedings{
gu2023boot,
title={{BOOT}: Data-free Distillation of Denoising Diffusion Models with Bootstrapping},
author={Jiatao Gu and Shuangfei Zhai and Yizhe Zhang and Lingjie Liu and Joshua M. Susskind},
booktitle={ICML 2023 Workshop on Structured Probabilistic Inference {\&} Generative Modeling},
year={2023},
url={https://openreview.net/forum?id=ZeM7S01Xi8}
}

@inproceedings{zhao2020diffaugment,
  title={Differentiable Augmentation for Data-Efficient GAN Training},
  author={Zhao, Shengyu and Liu, Zhijian and Lin, Ji and Zhu, Jun-Yan and Han, Song},
  booktitle={Conference on Neural Information Processing Systems (NeurIPS)},
  year={2020}
}

@inproceedings{
song2024improved,
title={Improved Techniques for Training Consistency Models},
author={Yang Song and Prafulla Dhariwal},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=WNzy9bRDvG}
}

@inproceedings{yin2024onestepdmd,
      title={One-step Diffusion with Distribution Matching Distillation},
      author={Yin, Tianwei and Gharbi, Micha{\"e}l and Zhang, Richard and Shechtman, Eli and Durand, Fr{\'e}do and Freeman, William T and Park, Taesung},
      booktitle={CVPR},
      year={2024}
    }


@inproceedings{
liu2024generative,
title={Generative Pre-training for Speech with Flow Matching},
author={Alexander H. Liu and Matthew Le and Apoorv Vyas and Bowen Shi and Andros Tjandra and Wei-Ning Hsu},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=KpoQSgxbKH}
}

@article{IPreview,
author = {Fortuin, Vincent},
title = {Priors in Bayesian Deep Learning: A Review},
journal = {International Statistical Review},
volume = {90},
number = {3},
pages = {563-591},
keywords = {Bayesian deep learning, Bayesian learning, deep learning, priors},
doi = {https://doi.org/10.1111/insr.12502},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/insr.12502},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1111/insr.12502},
abstract = {Summary While the choice of prior is one of the most critical parts of the Bayesian inference workflow, recent Bayesian deep learning models have often fallen back on vague priors, such as standard Gaussians. In this review, we highlight the importance of prior choices for Bayesian deep learning and present an overview of different priors that have been proposed for (deep) Gaussian processes, variational autoencoders and Bayesian neural networks. We also outline different methods of learning priors for these models from data. We hope to motivate practitioners in Bayesian deep learning to think more carefully about the prior specification for their models and to provide them with some inspiration in this regard.},
year = {2022}
}

@book{suli2003introduction,
  title={An introduction to numerical analysis},
  author={S{\"u}li, Endre and Mayers, David F},
  year={2003},
  publisher={Cambridge university press}
}

@misc{oord2018neuraldiscreterepresentationlearning,
      title={Neural Discrete Representation Learning}, 
      author={Aaron van den Oord and Oriol Vinyals and Koray Kavukcuoglu},
      year={2018},
      eprint={1711.00937},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1711.00937}, 
}

@misc{torchdiffeq,
	author={Chen, Ricky T. Q.},
	title={torchdiffeq},
	year={2018},
	url={https://github.com/rtqichen/torchdiffeq},
}

@misc{kingma2017adammethodstochasticoptimization,
      title={Adam: A Method for Stochastic Optimization}, 
      author={Diederik P. Kingma and Jimmy Ba},
      year={2017},
      eprint={1412.6980},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1412.6980}, 
}

@misc{ronneberger2015unetconvolutionalnetworksbiomedical,
      title={U-Net: Convolutional Networks for Biomedical Image Segmentation}, 
      author={Olaf Ronneberger and Philipp Fischer and Thomas Brox},
      year={2015},
      eprint={1505.04597},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/1505.04597}, 
}

@misc{von-platen-etal-2022-diffusers,
  author = {Patrick von Platen and Suraj Patil and Anton Lozhkov and Pedro Cuenca and Nathan Lambert and Kashif Rasul and Mishig Davaadorj and Dhruv Nair and Sayak Paul and William Berman and Yiyi Xu and Steven Liu and Thomas Wolf},
  title = {Diffusers: State-of-the-art diffusion models},
  year = {2022},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/huggingface/diffusers}}
}

@misc{bińkowski2021demystifyingmmdgans,
      title={Demystifying MMD GANs}, 
      author={Mikołaj Bińkowski and Danica J. Sutherland and Michael Arbel and Arthur Gretton},
      year={2021},
      eprint={1801.01401},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/1801.01401}, 
}

@misc{heusel2018ganstrainedtimescaleupdate,
      title={GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium}, 
      author={Martin Heusel and Hubert Ramsauer and Thomas Unterthiner and Bernhard Nessler and Sepp Hochreiter},
      year={2018},
      eprint={1706.08500},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1706.08500}, 
}

@misc{hessel2022clipscorereferencefreeevaluationmetric,
      title={CLIPScore: A Reference-free Evaluation Metric for Image Captioning}, 
      author={Jack Hessel and Ari Holtzman and Maxwell Forbes and Ronan Le Bras and Yejin Choi},
      year={2022},
      eprint={2104.08718},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2104.08718}, 
}

@misc{vaswani2023attentionneed,
      title={Attention Is All You Need}, 
      author={Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
      year={2023},
      eprint={1706.03762},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1706.03762}, 
}

@inproceedings{izmailov2020semi,
  title={Semi-supervised learning with normalizing flows},
  author={Izmailov, Pavel and Kirichenko, Polina and Finzi, Marc and Wilson, Andrew Gordon},
  booktitle={International conference on machine learning},
  pages={4615--4630},
  year={2020},
  organization={PMLR}
}

@article{dilokthanakul2016deep,
  title={Deep unsupervised clustering with gaussian mixture variational autoencoders},
  author={Dilokthanakul, Nat and Mediano, Pedro AM and Garnelo, Marta and Lee, Matthew CH and Salimbeni, Hugh and Arulkumaran, Kai and Shanahan, Murray},
  journal={arXiv preprint arXiv:1611.02648},
  year={2016}
}

@Manual{gillespie2025datasauRus,
  title = {datasauRus: Datasets from the Datasaurus Dozen},
  author = {Colin Gillespie and Steph Locke and Rhian Davies and Lucy {D'Agostino McGowan}},
  year = {2025},
  note = {R package version 0.1.9,
    https://jumpingrivers.github.io/datasauRus/},
  url = {https://github.com/jumpingrivers/datasauRus},
}