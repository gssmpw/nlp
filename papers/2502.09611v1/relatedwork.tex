\section{Related Work}
\noindent \textbf{Flow-based Models.} \quad
Continuous Normalizing Flows (CNFs) \citep{chen2019neural} emerged as a novel paradigm in generative modeling, offering a continuous-time extension to the discrete Normalizing Flows (NF) framework \citep{kobyzev2020normalizing, papamakarios2021normalizing}. 
Recently, Flow Matching \citep{lipman2022flow, liu2022flow, albergo2022building} has been introduced as a simulation-free alternative for training CNFs.
In scenarios involving conditional data (e.g., in text-to-image generation), conditioning is applied similarly to diffusion models, often through cross attention between the input condition and latent features. Typically, the source distribution remains unimodal, like a standard Gaussian ~\citep{liu2024generative}. In contrast, our approach derives a prior distribution that is dependent on the input condition.





\noindent \textbf{Informative Prior Design.} \quad
Designing useful prior distribution has been well studied in generative models such as VAEs (e.g., \cite{dilokthanakul2016deep}) and Normalizing Flows (e.g., \cite{izmailov2020semi})
In the context of score-based models and flow matching, several works designed informative priors. 
For score-based diffusion, ~\cite{lee2021priorgrad} has introduced an approach of formulating the diffusion process using a non-standard Gaussian, where the Gaussian's statistics are determined by the conditional distribution statistics. 
However, this approach is constrained by the use of a Gaussian prior, which limits its flexibility.
Recently, ~\cite{pooladian2023multisample, tong2023improving} constructed a prior distribution by utilizing the dynamic optimal transport (OT) formulation across mini-batches during training. Despite impressive capabilities such as efficient sampling (minimizing trajectory intersections), they suffer from several drawbacks: (i). Highly expensive training: Computing the optimal transport solution requires quadratic time and memory, which is not applicable to large mini-batches and high dimensional data. (ii). When dealing with high dimensional data, the effectiveness of this formulation decreases dramatically. An increase in performance requires an exponential increase in batch-size in relation to data dimension. 
Our approach avoids these limitations by leveraging the conditioning variable of the data distribution.