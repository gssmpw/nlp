\section{Related Work}
\noindent \textbf{Flow-based Models.} \quad
Continuous Normalizing Flows (CNFs) ____ emerged as a novel paradigm in generative modeling, offering a continuous-time extension to the discrete Normalizing Flows (NF) framework ____. 
Recently, Flow Matching ____ has been introduced as a simulation-free alternative for training CNFs.
In scenarios involving conditional data (e.g., in text-to-image generation), conditioning is applied similarly to diffusion models, often through cross attention between the input condition and latent features. Typically, the source distribution remains unimodal, like a standard Gaussian ____. In contrast, our approach derives a prior distribution that is dependent on the input condition.





\noindent \textbf{Informative Prior Design.} \quad
Designing useful prior distribution has been well studied in generative models such as VAEs (e.g., ____) and Normalizing Flows (e.g., ____)
In the context of score-based models and flow matching, several works designed informative priors. 
For score-based diffusion, ____ has introduced an approach of formulating the diffusion process using a non-standard Gaussian, where the Gaussian's statistics are determined by the conditional distribution statistics. 
However, this approach is constrained by the use of a Gaussian prior, which limits its flexibility.
Recently, ____ constructed a prior distribution by utilizing the dynamic optimal transport (OT) formulation across mini-batches during training. Despite impressive capabilities such as efficient sampling (minimizing trajectory intersections), they suffer from several drawbacks: (i). Highly expensive training: Computing the optimal transport solution requires quadratic time and memory, which is not applicable to large mini-batches and high dimensional data. (ii). When dealing with high dimensional data, the effectiveness of this formulation decreases dramatically. An increase in performance requires an exponential increase in batch-size in relation to data dimension. 
Our approach avoids these limitations by leveraging the conditioning variable of the data distribution.