\input{figures/2d_intuition_trajectory/2d_intuition_trajectory}
\input{figures/2d_intuition_NFE/2d_intuition_NFE}
\input{figures/toy_multimodal/toy_multimodal}
\input{figures/real_world_numerics/real_world_numerics}

\section{Experiments}

We begin by validating our approach on a 2D toy example.
Then, for two real-world datasets, we evaluate our approach on class-conditional and text-conditional image generation.

\subsection{Toy Examples}
\label{sec:toy_example}

We begin by considering the setting in which the prior distribution is a mixture of isotropic Gaussians (GMM), where each Gaussian's mean represents the center of a class (we set the standard deviation to $0.2$). The target distribution consists of 2D squares with the same center as the Gaussian's mean in the source distribution and with a width and height of $0.2$, representing a large square.   
We compare our method to class-conditional flow matching (with OT paths), where each conditional sample can be generated from each Gaussian in the prior distribution. 

In Fig.~\ref{fig:2d_intuition_trajectory}, we consider the trajectory from the prior to the target distribution. By starting from a more informative conditional prior, our method converges more quickly and results in a better fitting of the target distribution. 
In Fig.~\ref{fig:2d_intuition_NFE}(a), we consider the resulting samples for the different NFEs.
NFE indicates the number of function evaluation is used using a discrete Euler solver.
Our method better aligns with the target distribution with fewer number of steps. 

In Fig.~\ref{fig:2d_intuition_NFE}(b), we consider the model's ability to generalize to new classes not seen during training, akin to text-to-image generation's setting. Training on only a subset of the classes our model exhibits generalization to new classes at test time. 

In Fig.~\ref{fig:multimodal}, we evaluate the method in the case where classes are not uni-modal and there are intersections in the prior distribution, following data from VLines of the Datasaurus Dozen~\cite{gillespie2025datasauRus}. We present generated samples from a model trained using CondOT (\emph{a}) alongside samples from our model (\emph{b}, \emph{c}). The maximum mean discrepancy (MMD) computed on this data is 0.084 for CondOT, while we achieve an improved MMD of 0.072.






\subsection{Real World Setting}


\noindent \textbf{Datasets and Latent Representation Space.} \quad
For the class-conditioned setting, we consider the ImageNet-64 dataset~\cite{deng2009imagenet}, which includes more than 1.28M training images and 50k validation images, categorized into 1k object classes, all resized to $64\times64$ pixels.  For the text-to-image setting, we consider the 2017 split of the MS-COCO dataset~\cite{lin2014microsoft}, which consists of 330,000 images annotated with 80 object categories and over 2.5 million labeled instances. We use the standard split of 118k images for training, 5k for validation, and 41k for testing. We compute all our metrics on the ImageNet-64 validation set and the MS-COCO validation set. We perform flow matching in the latent representation of a pre-trained variational auto-encoder ~\cite{oord2018neuraldiscreterepresentationlearning}. 



\input{figures/real_world_numerics/nfe_convergence}
\input{figures/coco_generation/projector_output_diversity}
\input{figures/coco_generation/NFE_coco}





\subsubsection{Quantitative Results}
\label{sec:results_quantitative}





For a fair comparison, we evaluate our method in comparison to baselines using the same architecture, training scheme, and latent representation, as detailed above. We compare our method to standard class-conditioned or text-conditioned flow matching with OT paths ~\cite{lipman2022flow} which we denote CondOT, where the source distribution is chosen to be a standard Gaussian. We also consider BatchOT \cite{pooladian2023multisample}, which 
constructed a prior distribution by utilizing the dynamic optimal transport (OT) formulation across mini-batches
during training. Lastly, we consider Denoising Diffusion Probabilistic Models (DDPM) ~\cite{ho2020denoising}. To evaluate image quality, we consider the KID ~\cite{bi≈Ñkowski2021demystifyingmmdgans} and FID ~\cite{heusel2018ganstrainedtimescaleupdate} scores commonly used in literature. We also consider the CLIP score to evaluate the alignment of generated images to the input text or class, using the standard setting, as in ~\cite{hessel2022clipscorereferencefreeevaluationmetric}.



\noindent \textbf{Overall Performance.} \quad
We evaluate the FID, KID and CLIP similarity metrics for various NFE values (as defined above), which is indicative of the sampling speed. 
In Fig.~\ref{fig:real_world_numerics}(a) and Fig.~\ref{fig:real_world_numerics}(b), we perform this evaluation for our method and the baseline methods, for ImageNet-64 (class conditioned generation) and for MS-COCO (text-to-image generation), respectively. 
As can be seen, our method obtains superior results across all scores for both ImageNet-64 and MS-COCO. For ImageNet-64, already, at 15 NFEs our method achieves almost full convergence, whereas baseline methods achieve such convergence at much higher NFEs. This is especially true for FID, where our method converges at 15 NFEs, and baseline methods only achieve such performance at 30 NFEs. A similar behavior occurs for MS-COCO at 20 NFEs. We note that when considering NFEs for MS-COCO, we consider the pass in the mapper $\gP_\theta$ to be marginal due to the small size of the the mapper in relation to the velocity $v_\theta$, see Appendix ~\ref{sec:implementation_details}.




\noindent \textbf{Training Convergence Speed.} \quad By starting from our conditional prior distribution, training paths are on average shorter, and so our method should also converge more quickly at training. To evaluate this,
in Fig.~\ref{fig:nfe_convergence}, we consider the FID obtained at each epoch as well as the number of function evaluations (NFE) required for an adaptive solver to reach a pre-defined numerical tolerance, for a model trained on MS-COCO. Specifically,
FID is computed using an Euler sampler with a constant number of function evaluations, NFE=20. As for the adaptive sampler, we use the \texttt{dopri5} sampler with \texttt{atol=rtol=1e-5} from the \texttt{torchdiffeq} ~\citep{torchdiffeq} library.  Our method results in lower NFEs and superior FID, for every training epoch. %



\noindent \textbf{Qualitative Results.} \quad
In Fig.~\ref{fig:projector_output_diversity}, we provide a visualization of our results for a model trained on MS-COCO. We show, for four different text prompts: (a). The sample corresponding to the text in the conditional source distribution, which is used as the center of Gaussian corresponding to the text prompt. (b). Six randomly generated samples from the learned target distribution conditioned on the text prompt. As can be seen, the conditional source distribution samples resemble `an average' image corresponding to the text, while generated samples display diversity and realism. In the appendix, 
we also provide a diverse set of images generated by our method, in comparison to flow matching. 

In Fig.~\ref{fig:nfe_coco}, we consider, for a model trained on MS-COCO and a specific prompt, a visualization of our results for different NFEs, illustrating the sample quality for varying numbers of sampling steps. As can be seen, our method already produces highly realistic samples at NFE=15. 



\input{figures/ablation_study}
\noindent \textbf{Ablation Study.} \quad
In the continuous setting, as in MS-COCO, our method requires choosing the hyperparameter $\sigma$, the standard deviation of each Gaussian. 
In Tab.~\ref{tab:ablation_table}, we report the FID, KID, and CLIP similarity values for different values of $\sigma$. As can be seen, our method results in best performance when $\sigma=0.7$, we believe that a relative large $\sigma$ is necessary to allow a richer conditional prior due to the complex nature of the conditional image distribution. 
We also consider the case where our mapper $\gP_\theta$ takes as input a bag-of-words encoding instead of a CLIP encoding showing the importance of an expressive condition representation. As can be seen, performance drops significantly. 













