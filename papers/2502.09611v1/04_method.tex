 \section{Method}
\label{sec:method}











Given a set $\{x_{1_i},c_i\}_{i=1}^m$ of input samples and their corresponding conditioning states, our goal is to construct a flow-matching model that samples from $q(x_1|c)$ that start from our conditional prior distribution (CPD). 

\subsection{Flow Matching from Conditional Prior Distribution}
\label{sec:conditional_fm_joint}

We generalize the framework of  Sec.~\ref{sec:flow_matching} to a construction that uses an arbitrary conditional joint distribution of $\rho(x_0, x_1, c)$ which satisfy the marginal constraints:
\begin{equation*}
\label{eq:conditional_marginal}
    \int \rho(x_0, x_1, c)dx_0 = q(x_1, c),  \int \rho(x_0, x_1, c)dx_1dc = p(x_0)
\end{equation*}
Then, building on flow matching, we propose to modify the conditional probability path so that at $t=0$, we define:
\begin{equation}
    \rho_0(x_0|x_1, c) = p(x_0|x_1, c) 
\end{equation}
where $p(x_0|x_1, c)$ is the conditional distribution $\frac{\rho(x_0, x_1, c)}{q(x_1, c)}$. 
Using this construction, we satisfy the boundary condition of Eq.~\ref{eq:boundary_conditions}: 
\begin{align}
    \rho_0(x_0) &= \int\rho_0(x_0|x_1, c)q(x_1, c)dx_1dc  \\
                &=  \int p(x_0|x_1, c)dx_1dc = p(x_0)
\end{align}




The conditional probability path $\rho_t(x|x_1, c)$ does not need to be explicitly formulated. Instead, only its corresponding conditional vector field $u_t(x|x_1, c)$ needs to be defined such that points $x_0$ drawn from the conditional prior distribution $\rho_0(x_0|x_1, c) $, reach $x_1$ at $t=1$, i.e., reach distribution $\rho_1(x|x_1, c) = \delta(x - x_1)$.  We thus purpose the \emph{Conditional Generation Joint FM} $\gL_{\rm cgjfm}(\theta)$ objective:
\begin{equation}\label{eq:conditionl_joint_cfm}
    \mathbb{E}_{t\sim \mathcal{U}(0,1), q(x_0,x_1,c)} \|v_\theta(t, x, c) - u_t(x | x_1, c)\|^2
\end{equation}
where $x = \psi_t(x_0|x_1,c)$.
Training only involves sampling from $q(x_0,x_1,c)$ and does not require explicitly defining the densities $q(x_0,x_1,c)$ and $\rho_t(x|x_1,c)$.
We note that this objective is reduced to the CGFM objective Eq.~\ref{eq_conditional_generative_fm_objective} when $q(x_0,x_1,c) = q(x_1, c)p(x_0)$.

\subsection{Conditional Prior Distribution}
\label{sec:prior_distribution}

We now describe our choice of a condition-specific prior distribution. 
When choosing a conditional prior distribution we want to adhere to the following design principles:
(i) \emph{Easy to sample}: can be efficiently sampled from.
(ii) Well represents the target conditional modes. 
We design a condition-specific prior distribution based on a parametric \emph{Mixture Model} where each mode of the mixture is correlated to a specific conditional distribution $p(x_1|c)$. 
Specifically, we choose the prior distribution to be the following, \emph{easy to sample}, \emph{Gaussian Mixture Model} (GMM):
\begin{equation}\label{eq:gmm_formula}
    p_0 = \mathrm{GMM}(\gN(\mu_i, \Sigma_i)_{i=1}^n, \pi)
\end{equation}

where $\pi\in\R^n$ is a probability vector associated with the number of conditions $n$ (could be $\infty$) and $\mu_i, \Sigma_i$ are parameters determined by the conditional distribution $q(x_1|c_i)$ statistics, \emph{i.e.} 
 \begin{equation}\label{eq:gmm_parameters}
     \mu_i = \E[x_1|c_i], \quad \Sigma_i = \mathrm{cov}[x_1|c_i]
 \end{equation}
To sample from the marginal distribution $p(x_0|x_1, c_i)$, we sample from the cluster $\gN(\mu_i, \Sigma_i)$ associated with the condition $c_i$.

\noindent \textbf{Obtaining a Lower Global Truncation Error.} \quad 
Our CPD fits a GMM to the data distribution in a favorable setting, where the association between samples and clusters is given. 
\input{figures/wasserstein_distance}
In this process, we fit a dedicated Gaussian distribution to data points with the same condition. If the latter are close to being unimodal, this approximation is expected to be tight, in terms of the average distances between samples from the condition data mode and the fitted Gaussian. 
Tab.~\ref{tab:wasserstein_table} provides the average distances between pairs of samples from the prior and data distributions (i.e. the \emph{transport cost}) of CondOT~\cite{lipman2022flow}, BatchOT~\cite{pooladian2023multisample} and our CPD over the ImageNet-64~\cite{deng2009imagenet} and MS-COCO~\cite{lin2014microsoft} datasets. 
As expected, BatchOT which minimizes this exact measure within mini-batches, obtains better scores than the naÃ¯ve pairing used in CondOT, while our CPD, which approximates the data using a GMM exploits the conditioning available in these datasets, and obtains considerably lower average distances.

As noted in \cite{pooladian2023multisample}, lower transport cost is generally associated with straighter flow trajectories, more efficient sampling and lower training time. We want to substantiate this claim from the viewpoint of cumulative errors in numerical integration.
Sampling from flow-based models consists of solving a time-dependent ODE of the form $\dot{x}_t =u_t(x_t)$, where $u_t$ is the velocity field. This equation is solved by the following integral $x_t = \int_{0}^t u_s(x_s)ds$, where the initial condition $x_0 $ is sampled from the prior distribution. Numerical integration over discrete time steps accumulate an error at each step $n$ which is known as the \emph{local truncation error $\tau_n$}, which accumulates into what is known as the \emph{global truncation error $e_n$}.  This error is bounded by ~\cite{suli2003introduction}
\begin{equation}
    |e_n| \leq \frac{max_j\tau_j}{hL}\big(e^{L(t_n-t_0)} - 1\big)
\end{equation}\label{eq:truncation_error_bound} 
where $h$ is the step size and $L$ is the Lipschitz constant of the velocity $u_t$. 
Accordingly, the distance between the endpoints of a path $\Delta = |x_1  - x_0|$  is given by $|\int_0^1 u_s(x_s)ds|$ which can be interpreted as the magnitude of the average velocity along the path $x_t$. Hence, the longer the path $\Delta$ is, the larger the integrated flow vector field $u_t$ is.
For example, if we scale a path uniformly by a factor $C>1$, i.e., $x_t \rightarrow C(x_t)$, we get,  $\frac{d}{dt}C(x_t) = C(u_t)$ in which case the Lipschitz constant $L$ is also multiplied by $C$.

By shortening the distance between the prior and and data distribution, as our CPD does, we lower the integration errors which permits the use of coarser integration steps, which in turn yield smaller global errors. Thus, our construction allows for fewer integration steps during sampling.

\subsubsection{Construction}


Next, we explain how we construct $p_0$ (Eq.~\ref{eq:gmm_formula}) for both the discrete case (e.g., class conditional generation) and continuous case (e.g., text conditional generation). 

\noindent \textbf{Discrete Condition.} \quad
In the setup of discrete conditional generation, we are given data $\{x_{1_i}, c_i\}_{i=1}^m$ where there are a finite set of conditions $c_i$.
We approximate the statistics of Eq.~\ref{eq:gmm_parameters} using the training data statistics. That is, we compute the mean and covariance matrix of each class (potentially in some latent represntation of a pretrained auto-encoder).  Since the classes at inference time are the same as in training, we use the same statistics at inference. 

\noindent \textbf{Continuous Condition.} \quad
While in the discrete case we can directly approximate the statistics in Eq.~\ref{eq:gmm_parameters} from the training data, in the continuous case (\emph{e.g.} text-conditional) we need to find those statistics also for conditions that were not seen during training. To this end, we first consider a joint representation space for training samples $\{x_{1_i}, c_i\}_{i=1}^m$, which represents the semantic distances between the conditions $c_i$ and the samples $x_{1_i}$. In the setting where $c_i$ is text, we choose a pretrained CLIP embedding. 
$c_i$ is then mapped to this representation space, and then mapped to the 
data space (which could be a latent representation of an auto-encoder), using a learned mapper $\gP_\theta$. 
Specifically, $\gP_\theta$ is trained to minimize the objective:
\begin{equation}
    \gL_{\rm prior}(\theta) = \mathbb{E}_{q(x_1,c)} \|\gP_\theta(E(c)) - x_1\|^2_2.
\end{equation}
where $E$ is the pre-trained mapping to the joint condition-sample space (e.g. CLIP). $\gP_\theta$ can be seen as approximating $\E[x_1|c]$, which is used as the mean for the condition specific Gaussian.  
At inference, where new conditions (e.g., texts) may appear, we first encode the condition $c_i$ to the joint representation space (e.g., CLIP) followed by $\gP_\theta$. This mapping provides us with the center $\mu_i$ of each Gaussian. %
We also define $\Sigma_i = \sigma_i^2\mathrm{I}$ where $\sigma_i$ is a hyper-parameter, ablated in Sec.~\ref{sec:results_quantitative} 

\subsection{Training and Inference}

Given the prior $p_0$ (either using the data statistics or by training $\gP_\theta$), for each condition $c$, we have its associated Gaussian parameters $\mu_c$ and $\Sigma_c$. The map $\psi_t(x|x_1,c)$ must be defined in order to minimize Eq.~\ref{eq:conditionl_joint_cfm} above. This corresponds to the interpolating maps between this Gaussian at $t=0$ and a small Gaussian around $x_1$ at $t=1$, defined by:
\begin{align}
    \psi_{t}(x|x_1,c) &= \sigma_t(x_1,c)x + \mu_t(x_1,c), \\ 
    \sigma_t(x_1,c) &= t (\sigma_{\min} \mathrm{I}) + (1-t)\Sigma_{c}^{1/2}, \quad \text{and} \\
    \mu_t(x_1,c) &= t x_1 + (1-t) \mu_c.
\end{align}
This results in the following target flow vector field 
\begin{equation*}
    u_t(\psi_{t}(x|x_1,c)) = \frac{d}{dt}\psi_t (x|x_1,c)  =   \big(\sigma_{\min}  \mathrm{I} - \Sigma_c^{1/2}\big)x +  x_1 - \mu_c.
\end{equation*}

During inference we are given a condition $c$ and want to sample from $q(x_1|c)$. Similarly to the training, we sample $x_0\sim p(x_0|c)$ and solve the ODE 
\begin{equation}
    \frac{d}{dt} \psi_t(x) = v_\theta \left(t, \psi_t(x), c \right), \quad \psi_0(x) = x_0
\end{equation}
Training and implementation details are in the appendix.







