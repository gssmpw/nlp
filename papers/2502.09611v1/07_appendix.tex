\appendix

\section{Additional Quantitative Results}
\input{figures/real_world_numerics/table_metrics_nfe}
In Tab.\ref{tab:nfe_results}, we present additional metrics (FID, KID, and CLIP-Score) for ImageNet-64 with 15 NFEs. We compare the performance of CondOT~\cite{lipman2022flow}, BatchOT~\cite{pooladian2023multisample} and  DDPM~\cite{ho2020denoising}.
As shown, our model delivers significant improvements over the baselines.
\section{Visual Results}
\label{sec:visuals_appendix}

\input{figures/coco_generation/non_curated_samples}

In Fig.~\ref{fig:non_curated_coco}, we provide additional visual results for our method in comparison to standard flow matching for a model trained on MS-COCO.

\section{Implementation Details}
\label{sec:implementation_details}
\input{figures/hyperparameters}
We report the hyper-parameters used in Table ~\ref{tab:hyper-params}. All models were trained using the Adam optimizer ~\cite{kingma2017adammethodstochasticoptimization} with the following parameters: $\beta_1 = 0.9$, $\beta_2=0.999$, weight decay = 0.0, and $\epsilon = 1e{-8}$. 
All methods we trained (\emph{i.e.} Ours, CondOT, BatchOT, DDPM) using  identical architectures, specifically, the standard Unet ~\cite{ronneberger2015unetconvolutionalnetworksbiomedical} architecture from the \texttt{diffusers} ~\cite{von-platen-etal-2022-diffusers} library with the same number of parameters ($872M$) for the the same number of Epochs (see Table \ref{tab:hyper-params} for details). For all methods and datasets, we utilize a pre-trained Auto-Encoder ~\cite{oord2018neuraldiscreterepresentationlearning} and perform the flow/diffusion in its latent space.

In the case of text-to-image generation, we encode the text prompt using a pre-trained CLIP network and pass to the velocity $v_\theta$ using the standard UNet condition mechanism. In the class-conditional setting, we create the prompt `an image of a $\langle class \rangle$' and use it for the same conditioning scheme as in text conditional generation.

For the mapper $\gP_\theta$ from Sec~\ref{sec:prior_distribution} we use a network consisting a linear layer and 2 ResNet blocks with $11M$ parameters.

When using an adaptive step size sampler, we use \texttt{dopri5} with \texttt{atol=rtol=1e-5} from the \texttt{torchdiffeq} ~\citep{torchdiffeq} library.

Regarding the toy example Sec.~\ref{sec:toy_example}, we use a 4 layer MLP with ReLU activation as the velocity $v_\theta$. In this setup, we incorporate the condition by using positional embedding ~\cite{vaswani2023attentionneed} on the mean of each conditional mode and pass it to the velocity $v_\theta$ by concatenating it to its input. 


