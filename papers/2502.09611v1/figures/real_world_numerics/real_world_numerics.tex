\begin{figure*}[h!]
\centering

\begin{tabular}{ccc}
\includegraphics[width=0.31\textwidth]{figures/real_world_numerics/fid_nfe_imagenet.png}  &
\includegraphics[width=0.31\textwidth]{figures/real_world_numerics/kid_nfe_imagenet.png}  &
\includegraphics[width=0.31\textwidth]{figures/real_world_numerics/clip_nfe_imagenet.png}  \\
\multicolumn{3}{c}{\emph{(a)}}  \\
\includegraphics[width=0.31\textwidth, height=0.20\textwidth]{figures/real_world_numerics/fid_nfe_coco.png}  &
\includegraphics[width=0.31\textwidth, height=0.20\textwidth]{figures/real_world_numerics/kid_nfe_coco.png}  &
\includegraphics[width=0.31\textwidth, height=0.20\textwidth]{figures/real_world_numerics/clip_nfe_coco.png}  \\
\multicolumn{3}{c}{\emph{(b)}}  \\
\end{tabular}
\vskip -0.1in
\caption{\textbf{Numerical evaluation.} \emph{(a)} We compare our method to class conditional flow matching using optimal transport paths (CondOT) ~\cite{lipman2022flow}, BatchOT~\cite{pooladian2023multisample}, and DDPM~\cite{Ho2020DDPM}, on the ImageNet-64 dataset. We consider the FID score (LHS), KID score (Middle) and CLIP score (RHS). \emph{(b)}. As in \emph{(a)} but for text-to-image generation on the MS-COCO dataset. 
As can be seen our method exhibit significant improvement per NFE, especially for low NFEs. For example, for 15 NFEs, on ImageNet-64 and MS-COCO we get \textbf{FID of 13.62} and \textbf{FID of 18.05} respectively, while baselines do not surpass FID of 16.10 and FID of 28.32 respectively for the same NFEs. We consider up to 40 NFE steps and note that DDPM converges to a superior result given more steps. }

\label{fig:real_world_numerics}
\vspace{-0.3cm}
\end{figure*}
