\section{Related Work}
\subsection{Segment Anything Model}
Segment Anything Model (SAM)____ is a category-agnostic interactive segmentation model trained on a large-scale SA-1B dataset containing over 1 billion masks. SAM shows powerful zero-shot capabilities to new objects without additional training by taking user-provided prompts consisting of points, bounding boxes, or coarse masks. Due to its generality, SAM has been applied to a variety of research____. As depicted in \cref{fig:2}(a), SAM comprises three modules: an image encoder, a prompt encoder, and a mask decoder. The image encoder is a Vision Transformer____ backbone to extract image embeddings. The prompt encoder generates prompt embeddings from geometric prompts such as points and boxes. The mask decoder is a Transformer-based decoder____ that outputs a class-agnostic mask from image embeddings and prompt embeddings. While SAM can segment any object in zero-shot, it requires user-provided prompts for each target image, meaning human interaction and knowledge of the target image are needed. Furthermore, SAM does not output the class labels for each mask, limiting its usability in real-world applications.

\subsection{Few-shot Segmentation Model with SAM}
Few-shot segmentation models____ aim to segment objects in target images belonging to the same category as annotated reference images. Specifically, SAM-based few-shot segmentation models____ only take a few reference image-mask pairs as prompts, instead of user-provided prompts for each target image. This allows leveraging the richness of large-scale foundational model SAM while addressing SAM's weaknesses: user interaction and class-agnostic masks.

SAM-based few-shot segmentation models can be mainly classified into two types: training-free models____ that input geometric prompts derived from annotated reference images into SAM's prompt encoder, and meta-learning models____ that introduce the new SAM's prompt encoder and input prompt embeddings into SAM's mask decoder. Training-free models generate geometric prompts of target objects from pixel-level correlations between annotated reference images and target images without user interaction. For example, as shown in \cref{fig:2}(b), PerSAM____ selects the most positive and negative points from pixel-level correlations to input into SAM as prompts, and segments target objects without training. However, the performance of training-free models heavily depends on the quality of pseudo masks generated from pixel-level correlations, leading to incorrect prompts and reduced accuracy. Additionally, training-free models limit their scalability because SAM only accepts points, boxes, and masks as prompts.

Meta-learning models generate prompt embeddings for SAM derived from annotated reference images instead of geometric prompts via a meta-learning procedure. As depicted in \cref{fig:2}(c), VRP-SAM____ introduces a novel prompt encoder, the VRP encoder, which outputs prompt embeddings from annotated reference and target images. These prompt embeddings are then input into SAM's mask decoder, creating a scalable few-shot segmentation model capable of predicting unseen classes not included in the training data. In this study, as shown in \cref{fig:2}(d), we introduce a novel SAM's prompt encoder (VLP encoder) that leverages a vision-language model (VLM) to capture both visual and semantic information for higher segmentation accuracy.

\subsection{Multimodal Vision-Language Model}
Multimodal vision-language models (VLM) embed images and text in a unified embedding space, and various VLMs have been released____. A representative model, CLIP____, is a foundational model learned through contrastive learning____ on 400 million image-text pairs. Its zero-shot capability has been widely utilized in a variety of computer vision tasks____. However, for object detection and image segmentation tasks, pixel-text matching rather than CLS token-text matching like CLIP is required to capture object-level features. VLM with pixel-text matching____ can capture spatial and semantic information of objects from each patch-text relationship. Notably, CLIP Surgery____ enhances the class attention map (CAM)____ of CLIP to create high-performance pixel-text correlations (attention maps) without additional training. This model retains the benefits of CLIP's large-scale foundational model while providing image-text embeddings that capture object-level semantic features. Therefore, in this study, we employ CLIP Surgery as VLM and CLIP as a comparison.
%---------------------------------------------------------------------

%---------------------------------------------------------------------